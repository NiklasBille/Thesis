>>> Starting run for dataset: freesolv
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running SCAFF configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.6.yml on cuda:0
Running SCAFF configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.7.yml on cuda:1
Running SCAFF configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.8.yml on cuda:2
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.8.yml --runseed 4 --device cuda:2
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.7.yml --runseed 4 --device cuda:1
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.6.yml --runseed 4 --device cuda:0
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.6.yml --runseed 5 --device cuda:0
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.7.yml --runseed 5 --device cuda:1
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.8.yml --runseed 5 --device cuda:2
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.6.yml --runseed 6 --device cuda:0
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.7.yml --runseed 6 --device cuda:1
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.8.yml --runseed 6 --device cuda:2
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/freesolv/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/freesolv/scaff/train_prop=0.6/freesolv_scaff_5_26-05_10-18-47  ]
[ Using Seed :  5  ]
[ Using device :  cuda:0  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 17.202527046203613
RMSE train: 4.701440	val: 5.808304	test: 8.166009
MAE train: 3.907863	val: 5.040431	test: 6.659166

Epoch: 2
Loss: 16.40054225921631
RMSE train: 5.183101	val: 5.805896	test: 8.122282
MAE train: 4.338710	val: 5.065500	test: 6.689602

Epoch: 3
Loss: 14.798081398010254
RMSE train: 5.087822	val: 5.707110	test: 7.894960
MAE train: 4.253726	val: 4.985242	test: 6.450615

Epoch: 4
Loss: 14.270646095275879
RMSE train: 4.744882	val: 5.570468	test: 7.592547
MAE train: 3.950542	val: 4.870355	test: 6.122758

Epoch: 5
Loss: 12.678380966186523
RMSE train: 4.391960	val: 5.428740	test: 7.268624
MAE train: 3.638454	val: 4.754003	test: 5.782697

Epoch: 6
Loss: 11.519891262054443
RMSE train: 4.043725	val: 5.241795	test: 6.936275
MAE train: 3.333454	val: 4.594104	test: 5.432246

Epoch: 7
Loss: 11.387932777404785
RMSE train: 3.701715	val: 4.968572	test: 6.575079
MAE train: 3.059064	val: 4.345958	test: 5.115455

Epoch: 8
Loss: 9.855218410491943
RMSE train: 3.401859	val: 4.606402	test: 6.210037
MAE train: 2.824515	val: 3.993889	test: 4.801336

Epoch: 9
Loss: 9.478909969329834
RMSE train: 3.199139	val: 4.263495	test: 5.928591
MAE train: 2.668999	val: 3.661073	test: 4.574994

Epoch: 10
Loss: 8.582519054412842
RMSE train: 3.084179	val: 4.117055	test: 5.696700
MAE train: 2.604279	val: 3.494796	test: 4.369576

Epoch: 11
Loss: 7.432490110397339
RMSE train: 3.097695	val: 4.621876	test: 5.639283
MAE train: 2.653543	val: 4.188962	test: 4.319472

Epoch: 12
Loss: 7.629828929901123
RMSE train: 3.162660	val: 4.645275	test: 5.552443
MAE train: 2.722165	val: 4.257881	test: 4.217841

Epoch: 13
Loss: 7.487372636795044
RMSE train: 3.166070	val: 4.475527	test: 5.455546
MAE train: 2.726243	val: 4.096592	test: 4.152593

Epoch: 14
Loss: 7.158205986022949
RMSE train: 3.140332	val: 4.273178	test: 5.453201
MAE train: 2.722222	val: 3.894545	test: 4.169034

Epoch: 15
Loss: 6.644716262817383
RMSE train: 3.091363	val: 4.061728	test: 5.463850
MAE train: 2.692304	val: 3.685461	test: 4.188880

Epoch: 16
Loss: 6.316022634506226
RMSE train: 3.028737	val: 3.909689	test: 5.489195
MAE train: 2.649018	val: 3.536889	test: 4.210046

Epoch: 17
Loss: 5.9124579429626465
RMSE train: 2.965629	val: 3.692869	test: 5.461903
MAE train: 2.603113	val: 3.317394	test: 4.179821

Epoch: 18
Loss: 5.402947425842285
RMSE train: 2.860070	val: 3.383682	test: 5.434499
MAE train: 2.509952	val: 2.981386	test: 4.148909

Epoch: 19
Loss: 5.028075456619263
RMSE train: 2.746808	val: 3.125865	test: 5.340534
MAE train: 2.400799	val: 2.715694	test: 4.041399

Epoch: 20
Loss: 5.172419309616089
RMSE train: 2.638802	val: 3.144244	test: 5.244899
MAE train: 2.299263	val: 2.757484	test: 3.933794

Epoch: 21
Loss: 4.779807090759277
RMSE train: 2.554810	val: 3.176204	test: 5.177303
MAE train: 2.223350	val: 2.815011	test: 3.863327

Epoch: 22
Loss: 4.499081373214722
RMSE train: 2.506436	val: 3.195134	test: 5.128620
MAE train: 2.174660	val: 2.844885	test: 3.799401Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/freesolv/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/freesolv/scaff/train_prop=0.6/freesolv_scaff_4_26-05_10-18-47  ]
[ Using Seed :  4  ]
[ Using device :  cuda:0  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 17.917302131652832
RMSE train: 3.863672	val: 5.614779	test: 8.456019
MAE train: 3.004781	val: 4.746434	test: 6.781792

Epoch: 2
Loss: 15.498872756958008
RMSE train: 3.969025	val: 5.551673	test: 8.382781
MAE train: 3.132385	val: 4.721571	test: 6.717410

Epoch: 3
Loss: 14.979270458221436
RMSE train: 4.083188	val: 5.508461	test: 8.320128
MAE train: 3.261257	val: 4.702994	test: 6.699060

Epoch: 4
Loss: 14.144861698150635
RMSE train: 4.035489	val: 5.453320	test: 8.176832
MAE train: 3.250093	val: 4.676788	test: 6.547402

Epoch: 5
Loss: 13.048326969146729
RMSE train: 3.889193	val: 5.387855	test: 7.951127
MAE train: 3.149834	val: 4.645768	test: 6.294205

Epoch: 6
Loss: 11.863218307495117
RMSE train: 3.640957	val: 5.287563	test: 7.704018
MAE train: 2.959308	val: 4.580816	test: 6.075139

Epoch: 7
Loss: 10.372374534606934
RMSE train: 3.406561	val: 5.180941	test: 7.436198
MAE train: 2.769892	val: 4.513721	test: 5.856482

Epoch: 8
Loss: 9.977538108825684
RMSE train: 3.226551	val: 5.053792	test: 7.202663
MAE train: 2.625608	val: 4.418489	test: 5.728782

Epoch: 9
Loss: 9.680540084838867
RMSE train: 3.129304	val: 4.870608	test: 6.835178
MAE train: 2.569867	val: 4.249475	test: 5.356746

Epoch: 10
Loss: 8.483867406845093
RMSE train: 3.148367	val: 4.773731	test: 6.491354
MAE train: 2.624286	val: 4.173298	test: 4.928388

Epoch: 11
Loss: 8.445077657699585
RMSE train: 3.240853	val: 4.862039	test: 6.355724
MAE train: 2.734914	val: 4.316183	test: 4.747675

Epoch: 12
Loss: 7.5349578857421875
RMSE train: 3.257430	val: 4.972466	test: 6.260814
MAE train: 2.770706	val: 4.476209	test: 4.621698

Epoch: 13
Loss: 7.212130069732666
RMSE train: 3.258827	val: 4.990428	test: 6.241579
MAE train: 2.789764	val: 4.521010	test: 4.646057

Epoch: 14
Loss: 6.627695083618164
RMSE train: 3.203552	val: 4.885421	test: 6.200979
MAE train: 2.749780	val: 4.419322	test: 4.642697

Epoch: 15
Loss: 6.6884496212005615
RMSE train: 3.096944	val: 4.679102	test: 6.129166
MAE train: 2.662147	val: 4.204367	test: 4.580792

Epoch: 16
Loss: 6.752434492111206
RMSE train: 2.983356	val: 4.442250	test: 6.037056
MAE train: 2.568849	val: 3.954059	test: 4.484273

Epoch: 17
Loss: 5.9851768016815186
RMSE train: 2.884498	val: 4.263221	test: 5.972687
MAE train: 2.488950	val: 3.771643	test: 4.407956

Epoch: 18
Loss: 6.022743463516235
RMSE train: 2.794438	val: 4.087206	test: 5.903746
MAE train: 2.409291	val: 3.595645	test: 4.329681

Epoch: 19
Loss: 4.99803626537323
RMSE train: 2.685659	val: 4.016255	test: 5.825391
MAE train: 2.295663	val: 3.539878	test: 4.250135

Epoch: 20
Loss: 5.736779689788818
RMSE train: 2.631481	val: 3.928310	test: 5.743613
MAE train: 2.239645	val: 3.468231	test: 4.206335

Epoch: 21
Loss: 4.990142583847046
RMSE train: 2.541685	val: 3.761335	test: 5.627191
MAE train: 2.158625	val: 3.314589	test: 4.143719

Epoch: 22
Loss: 4.637014865875244
RMSE train: 2.438073	val: 3.510839	test: 5.455725
MAE train: 2.080925	val: 3.061961	test: 4.017464Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/freesolv/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/freesolv/scaff/train_prop=0.6/freesolv_scaff_6_26-05_10-18-47  ]
[ Using Seed :  6  ]
[ Using device :  cuda:0  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 15.831287384033203
RMSE train: 3.963302	val: 5.538846	test: 7.800316
MAE train: 3.166020	val: 4.689124	test: 6.141202

Epoch: 2
Loss: 14.808084964752197
RMSE train: 3.883638	val: 5.480321	test: 7.666654
MAE train: 3.114033	val: 4.655198	test: 6.018461

Epoch: 3
Loss: 13.1277174949646
RMSE train: 3.736422	val: 5.417724	test: 7.489054
MAE train: 2.999884	val: 4.621560	test: 5.882290

Epoch: 4
Loss: 12.831688404083252
RMSE train: 3.526672	val: 5.346804	test: 7.245613
MAE train: 2.836352	val: 4.592416	test: 5.670773

Epoch: 5
Loss: 11.961541652679443
RMSE train: 3.273128	val: 5.234918	test: 6.951766
MAE train: 2.649249	val: 4.517708	test: 5.419836

Epoch: 6
Loss: 11.151431560516357
RMSE train: 3.010958	val: 5.141983	test: 6.600879
MAE train: 2.443905	val: 4.463143	test: 5.093348

Epoch: 7
Loss: 9.336272716522217
RMSE train: 2.773696	val: 4.852200	test: 6.200893
MAE train: 2.257308	val: 4.154918	test: 4.711080

Epoch: 8
Loss: 9.446680545806885
RMSE train: 2.655018	val: 4.510270	test: 5.880140
MAE train: 2.170127	val: 3.784740	test: 4.403331

Epoch: 9
Loss: 8.583203792572021
RMSE train: 2.663305	val: 4.543861	test: 5.784508
MAE train: 2.193715	val: 3.861620	test: 4.298175

Epoch: 10
Loss: 8.014731168746948
RMSE train: 2.681667	val: 4.619602	test: 5.737261
MAE train: 2.213068	val: 3.995999	test: 4.285763

Epoch: 11
Loss: 8.112531185150146
RMSE train: 2.673377	val: 4.574261	test: 5.656403
MAE train: 2.202936	val: 3.998527	test: 4.205371

Epoch: 12
Loss: 7.907001256942749
RMSE train: 2.675446	val: 4.502296	test: 5.614004
MAE train: 2.201773	val: 3.981927	test: 4.163609

Epoch: 13
Loss: 7.006887435913086
RMSE train: 2.668418	val: 4.366099	test: 5.584709
MAE train: 2.199049	val: 3.875212	test: 4.126919

Epoch: 14
Loss: 6.263910293579102
RMSE train: 2.666597	val: 4.161318	test: 5.565831
MAE train: 2.204452	val: 3.668605	test: 4.097195

Epoch: 15
Loss: 6.130066871643066
RMSE train: 2.654977	val: 3.919833	test: 5.559761
MAE train: 2.208207	val: 3.420978	test: 4.088254

Epoch: 16
Loss: 5.623003721237183
RMSE train: 2.649337	val: 3.720792	test: 5.546533
MAE train: 2.222306	val: 3.213247	test: 4.071822

Epoch: 17
Loss: 5.937528848648071
RMSE train: 2.682607	val: 3.549933	test: 5.559515
MAE train: 2.278973	val: 3.017142	test: 4.063114

Epoch: 18
Loss: 5.2994067668914795
RMSE train: 2.673255	val: 3.391130	test: 5.532483
MAE train: 2.289977	val: 2.835258	test: 4.042890

Epoch: 19
Loss: 4.984710693359375
RMSE train: 2.643007	val: 3.243252	test: 5.484474
MAE train: 2.279023	val: 2.675996	test: 4.042080

Epoch: 20
Loss: 5.086764574050903
RMSE train: 2.646538	val: 3.221794	test: 5.499539
MAE train: 2.300781	val: 2.659072	test: 4.095909

Epoch: 21
Loss: 5.023493528366089
RMSE train: 2.669519	val: 3.254680	test: 5.489624
MAE train: 2.339915	val: 2.721513	test: 4.122114

Epoch: 22
Loss: 4.400911569595337
RMSE train: 2.651638	val: 3.098442	test: 5.422518
MAE train: 2.342028	val: 2.557634	test: 4.049646Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/freesolv/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/freesolv/scaff/train_prop=0.7/freesolv_scaff_6_26-05_10-18-47  ]
[ Using Seed :  6  ]
[ Using device :  cuda:1  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 25.478808403015137
RMSE train: 4.914669	val: 5.557199	test: 6.659486
MAE train: 3.722449	val: 4.716652	test: 5.488738

Epoch: 2
Loss: 23.816906929016113
RMSE train: 4.801127	val: 5.494043	test: 6.452323
MAE train: 3.660919	val: 4.674196	test: 5.305730

Epoch: 3
Loss: 21.914794921875
RMSE train: 4.561176	val: 5.437671	test: 6.209689
MAE train: 3.489327	val: 4.644540	test: 5.132076

Epoch: 4
Loss: 20.014634132385254
RMSE train: 4.280597	val: 5.405337	test: 5.931739
MAE train: 3.313885	val: 4.650503	test: 4.964096

Epoch: 5
Loss: 18.69456672668457
RMSE train: 3.951825	val: 5.361782	test: 5.602593
MAE train: 3.131660	val: 4.651180	test: 4.760007

Epoch: 6
Loss: 16.67029857635498
RMSE train: 3.598958	val: 5.339422	test: 5.346675
MAE train: 2.939087	val: 4.675669	test: 4.573221

Epoch: 7
Loss: 15.013664245605469
RMSE train: 3.319116	val: 5.372107	test: 5.209820
MAE train: 2.745851	val: 4.760830	test: 4.407179

Epoch: 8
Loss: 13.424605369567871
RMSE train: 3.222624	val: 5.420210	test: 5.192503
MAE train: 2.669081	val: 4.866057	test: 4.391830

Epoch: 9
Loss: 12.491267204284668
RMSE train: 3.256175	val: 5.452373	test: 5.293055
MAE train: 2.706657	val: 4.952401	test: 4.600786

Epoch: 10
Loss: 12.414356708526611
RMSE train: 3.250540	val: 5.435303	test: 5.346696
MAE train: 2.727009	val: 4.973217	test: 4.693653

Epoch: 11
Loss: 12.254479885101318
RMSE train: 3.220678	val: 5.400838	test: 5.288112
MAE train: 2.732570	val: 4.960548	test: 4.646461

Epoch: 12
Loss: 11.075836181640625
RMSE train: 3.211076	val: 5.194875	test: 5.011237
MAE train: 2.742332	val: 4.758017	test: 4.325339

Epoch: 13
Loss: 10.554700374603271
RMSE train: 3.250785	val: 4.972657	test: 4.696236
MAE train: 2.803496	val: 4.522561	test: 3.850308

Epoch: 14
Loss: 9.979099750518799
RMSE train: 3.350834	val: 5.212267	test: 4.776685
MAE train: 2.908286	val: 4.788222	test: 3.947369

Epoch: 15
Loss: 9.849853992462158
RMSE train: 3.471299	val: 5.567293	test: 4.992614
MAE train: 3.012913	val: 5.162645	test: 4.130252

Epoch: 16
Loss: 9.362535953521729
RMSE train: 3.577499	val: 5.754559	test: 5.155820
MAE train: 3.101120	val: 5.354248	test: 4.286218

Epoch: 17
Loss: 8.808167934417725
RMSE train: 3.595183	val: 5.742062	test: 5.205177
MAE train: 3.118165	val: 5.344657	test: 4.358789

Epoch: 18
Loss: 8.263661623001099
RMSE train: 3.535351	val: 5.605024	test: 5.158847
MAE train: 3.070523	val: 5.214581	test: 4.326505

Epoch: 19
Loss: 7.842372894287109
RMSE train: 3.440413	val: 5.370340	test: 5.077312
MAE train: 3.003297	val: 4.990703	test: 4.261880

Epoch: 20
Loss: 7.90376877784729
RMSE train: 3.348569	val: 4.974214	test: 4.934287
MAE train: 2.941910	val: 4.598215	test: 4.138197

Epoch: 21
Loss: 7.522637128829956
RMSE train: 3.234734	val: 4.482898	test: 4.740788
MAE train: 2.852284	val: 4.100338	test: 3.957511

Epoch: 22
Loss: 7.579986572265625
RMSE train: 3.158218	val: 4.144092	test: 4.569107
MAE train: 2.793910	val: 3.741559	test: 3.800331Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/freesolv/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/freesolv/scaff/train_prop=0.7/freesolv_scaff_4_26-05_10-18-47  ]
[ Using Seed :  4  ]
[ Using device :  cuda:1  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 26.637914657592773
RMSE train: 4.962408	val: 5.652599	test: 7.172713
MAE train: 3.616029	val: 4.781411	test: 6.173257

Epoch: 2
Loss: 24.73297119140625
RMSE train: 4.905999	val: 5.648384	test: 6.883417
MAE train: 3.632333	val: 4.812059	test: 5.856721

Epoch: 3
Loss: 24.044533729553223
RMSE train: 4.745166	val: 5.601732	test: 6.617550
MAE train: 3.523955	val: 4.788003	test: 5.591285

Epoch: 4
Loss: 22.64819622039795
RMSE train: 4.530822	val: 5.579932	test: 6.339102
MAE train: 3.392747	val: 4.800251	test: 5.370853

Epoch: 5
Loss: 20.611271858215332
RMSE train: 4.283587	val: 5.590813	test: 6.090267
MAE train: 3.274328	val: 4.852888	test: 5.218490

Epoch: 6
Loss: 19.083776473999023
RMSE train: 3.990789	val: 5.601657	test: 5.841807
MAE train: 3.119951	val: 4.906584	test: 5.069972

Epoch: 7
Loss: 16.73100996017456
RMSE train: 3.724061	val: 5.663945	test: 5.625686
MAE train: 2.971528	val: 5.024525	test: 4.934445

Epoch: 8
Loss: 14.720256805419922
RMSE train: 3.513545	val: 5.740226	test: 5.360178
MAE train: 2.855380	val: 5.157605	test: 4.745053

Epoch: 9
Loss: 14.25326919555664
RMSE train: 3.387124	val: 5.787825	test: 5.130865
MAE train: 2.783119	val: 5.254659	test: 4.472031

Epoch: 10
Loss: 13.821476936340332
RMSE train: 3.370044	val: 5.801998	test: 5.083809
MAE train: 2.773886	val: 5.313631	test: 4.429463

Epoch: 11
Loss: 13.521753311157227
RMSE train: 3.377149	val: 5.820807	test: 5.129362
MAE train: 2.803027	val: 5.371233	test: 4.440799

Epoch: 12
Loss: 12.018929481506348
RMSE train: 3.372888	val: 5.800746	test: 5.065980
MAE train: 2.827380	val: 5.379320	test: 4.360592

Epoch: 13
Loss: 11.266181945800781
RMSE train: 3.379291	val: 5.693265	test: 4.971675
MAE train: 2.877136	val: 5.295494	test: 4.319301

Epoch: 14
Loss: 11.295598983764648
RMSE train: 3.414872	val: 5.565548	test: 4.824483
MAE train: 2.950373	val: 5.166345	test: 4.164382

Epoch: 15
Loss: 10.119239330291748
RMSE train: 3.448045	val: 5.391161	test: 4.629265
MAE train: 3.007283	val: 4.973358	test: 3.959959

Epoch: 16
Loss: 10.037871837615967
RMSE train: 3.515431	val: 5.333380	test: 4.553686
MAE train: 3.075474	val: 4.911831	test: 3.899088

Epoch: 17
Loss: 10.051129341125488
RMSE train: 3.571195	val: 5.300194	test: 4.545162
MAE train: 3.138018	val: 4.888775	test: 3.880499

Epoch: 18
Loss: 9.18427848815918
RMSE train: 3.592775	val: 5.303516	test: 4.622490
MAE train: 3.163540	val: 4.916792	test: 3.940462

Epoch: 19
Loss: 9.348804473876953
RMSE train: 3.601067	val: 5.348399	test: 4.753008
MAE train: 3.173311	val: 4.990191	test: 4.046932

Epoch: 20
Loss: 8.451030015945435
RMSE train: 3.571584	val: 5.358484	test: 4.838052
MAE train: 3.144059	val: 5.023393	test: 4.199323

Epoch: 21
Loss: 8.318880558013916
RMSE train: 3.530355	val: 5.261214	test: 4.829861
MAE train: 3.112839	val: 4.936191	test: 4.234774

Epoch: 22
Loss: 7.952929973602295
RMSE train: 3.448914	val: 5.103534	test: 4.754463
MAE train: 3.049941	val: 4.781522	test: 4.167415Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/freesolv/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/freesolv/scaff/train_prop=0.7/freesolv_scaff_5_26-05_10-18-47  ]
[ Using Seed :  5  ]
[ Using device :  cuda:1  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 26.07930088043213
RMSE train: 5.451015	val: 5.844589	test: 6.975128
MAE train: 4.333742	val: 5.076761	test: 6.007624

Epoch: 2
Loss: 24.124794960021973
RMSE train: 5.817343	val: 5.858271	test: 6.994456
MAE train: 4.776574	val: 5.118564	test: 6.077581

Epoch: 3
Loss: 22.9558048248291
RMSE train: 5.965007	val: 5.825232	test: 6.900864
MAE train: 4.929790	val: 5.106321	test: 6.012131

Epoch: 4
Loss: 20.937042236328125
RMSE train: 5.768094	val: 5.786679	test: 6.649574
MAE train: 4.760916	val: 5.097079	test: 5.823626

Epoch: 5
Loss: 19.127400398254395
RMSE train: 5.496076	val: 5.776563	test: 6.291559
MAE train: 4.541057	val: 5.120530	test: 5.481755

Epoch: 6
Loss: 18.217862129211426
RMSE train: 5.213532	val: 5.799077	test: 5.897892
MAE train: 4.327124	val: 5.175106	test: 5.056156

Epoch: 7
Loss: 16.59737205505371
RMSE train: 4.890136	val: 5.863850	test: 5.628316
MAE train: 4.102244	val: 5.274304	test: 4.756194

Epoch: 8
Loss: 14.842848777770996
RMSE train: 4.438749	val: 5.901663	test: 5.436716
MAE train: 3.762546	val: 5.351605	test: 4.486640

Epoch: 9
Loss: 14.402688980102539
RMSE train: 4.017950	val: 5.835676	test: 5.246542
MAE train: 3.415269	val: 5.331159	test: 4.225593

Epoch: 10
Loss: 13.797462463378906
RMSE train: 3.837272	val: 5.654748	test: 5.100971
MAE train: 3.234685	val: 5.193263	test: 4.133607

Epoch: 11
Loss: 12.925179958343506
RMSE train: 3.713075	val: 5.378108	test: 4.899323
MAE train: 3.138016	val: 4.945765	test: 3.995136

Epoch: 12
Loss: 12.251460552215576
RMSE train: 3.602740	val: 5.085956	test: 4.716952
MAE train: 3.100999	val: 4.661101	test: 3.853282

Epoch: 13
Loss: 11.41219425201416
RMSE train: 3.530289	val: 4.843250	test: 4.553394
MAE train: 3.080444	val: 4.409341	test: 3.735052

Epoch: 14
Loss: 10.536071300506592
RMSE train: 3.504054	val: 4.773038	test: 4.469733
MAE train: 3.087272	val: 4.337042	test: 3.688341

Epoch: 15
Loss: 10.181512355804443
RMSE train: 3.549448	val: 4.786125	test: 4.492191
MAE train: 3.159888	val: 4.353058	test: 3.754955

Epoch: 16
Loss: 9.588646411895752
RMSE train: 3.630187	val: 4.890266	test: 4.599141
MAE train: 3.265074	val: 4.469069	test: 3.904721

Epoch: 17
Loss: 9.280112266540527
RMSE train: 3.698694	val: 5.021335	test: 4.763716
MAE train: 3.334038	val: 4.612410	test: 4.112524

Epoch: 18
Loss: 9.117832660675049
RMSE train: 3.697099	val: 5.073285	test: 4.811760
MAE train: 3.326036	val: 4.673213	test: 4.179057

Epoch: 19
Loss: 8.47667646408081
RMSE train: 3.665796	val: 5.073383	test: 4.759136
MAE train: 3.290104	val: 4.675236	test: 4.113559

Epoch: 20
Loss: 7.652867794036865
RMSE train: 3.609343	val: 5.092323	test: 4.711025
MAE train: 3.226730	val: 4.703999	test: 4.055297

Epoch: 21
Loss: 7.7714338302612305
RMSE train: 3.573741	val: 5.134223	test: 4.680363
MAE train: 3.199195	val: 4.751533	test: 4.009355

Epoch: 22
Loss: 7.689842939376831
RMSE train: 3.558309	val: 5.045329	test: 4.622050
MAE train: 3.196615	val: 4.662968	test: 3.955825Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/freesolv/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/freesolv/scaff/train_prop=0.8/freesolv_scaff_6_26-05_10-18-47  ]
[ Using Seed :  6  ]
[ Using device :  cuda:2  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 20.234868049621582
RMSE train: 4.419019	val: 8.339573	test: 6.353675
MAE train: 3.538620	val: 6.377543	test: 5.417513

Epoch: 2
Loss: 19.099644660949707
RMSE train: 4.364151	val: 8.253962	test: 6.439282
MAE train: 3.510750	val: 6.311208	test: 5.516439

Epoch: 3
Loss: 18.11079502105713
RMSE train: 4.188977	val: 8.071976	test: 6.384726
MAE train: 3.360693	val: 6.174431	test: 5.469497

Epoch: 4
Loss: 17.05916118621826
RMSE train: 4.009453	val: 7.843819	test: 6.262072
MAE train: 3.216933	val: 5.992234	test: 5.353212

Epoch: 5
Loss: 15.592849254608154
RMSE train: 3.802220	val: 7.554855	test: 6.126685
MAE train: 3.065775	val: 5.768388	test: 5.216360

Epoch: 6
Loss: 14.133426189422607
RMSE train: 3.548235	val: 7.209794	test: 6.022775
MAE train: 2.897544	val: 5.507660	test: 5.116089

Epoch: 7
Loss: 12.980513572692871
RMSE train: 3.301127	val: 6.838957	test: 5.972622
MAE train: 2.748373	val: 5.233558	test: 5.071160

Epoch: 8
Loss: 12.049928665161133
RMSE train: 3.057063	val: 6.419039	test: 5.954706
MAE train: 2.587420	val: 4.946644	test: 5.059482

Epoch: 9
Loss: 11.332897186279297
RMSE train: 2.896541	val: 6.059744	test: 5.956811
MAE train: 2.434294	val: 4.723481	test: 5.102576

Epoch: 10
Loss: 11.251480102539062
RMSE train: 2.818419	val: 5.842401	test: 5.915371
MAE train: 2.350777	val: 4.599245	test: 5.147669

Epoch: 11
Loss: 10.185586929321289
RMSE train: 2.852845	val: 5.891731	test: 5.924593
MAE train: 2.408206	val: 4.735789	test: 5.185574

Epoch: 12
Loss: 9.683964252471924
RMSE train: 2.909390	val: 6.056528	test: 5.911625
MAE train: 2.499270	val: 4.982028	test: 5.197753

Epoch: 13
Loss: 9.539618492126465
RMSE train: 2.987242	val: 6.276479	test: 5.906721
MAE train: 2.607803	val: 5.232890	test: 5.199642

Epoch: 14
Loss: 8.959511041641235
RMSE train: 3.035672	val: 6.422558	test: 5.871860
MAE train: 2.669650	val: 5.399537	test: 5.147701

Epoch: 15
Loss: 8.782291412353516
RMSE train: 3.014748	val: 6.452118	test: 5.847787
MAE train: 2.641823	val: 5.433888	test: 5.112959

Epoch: 16
Loss: 8.012115955352783
RMSE train: 2.990420	val: 6.371144	test: 5.819110
MAE train: 2.620156	val: 5.383170	test: 5.089190

Epoch: 17
Loss: 7.813625335693359
RMSE train: 2.977192	val: 6.273089	test: 5.808313
MAE train: 2.621060	val: 5.313931	test: 5.094886

Epoch: 18
Loss: 7.362698554992676
RMSE train: 3.013511	val: 6.252365	test: 5.841156
MAE train: 2.678535	val: 5.317178	test: 5.146120

Epoch: 19
Loss: 6.923004865646362
RMSE train: 3.008663	val: 6.224868	test: 5.809401
MAE train: 2.696268	val: 5.304912	test: 5.125199

Epoch: 20
Loss: 6.294309854507446
RMSE train: 3.004395	val: 6.244971	test: 5.799357
MAE train: 2.705627	val: 5.338477	test: 5.110893

Epoch: 21
Loss: 6.128342151641846
RMSE train: 3.011011	val: 6.226707	test: 5.723254
MAE train: 2.716623	val: 5.323537	test: 5.026868

Epoch: 22
Loss: 6.058534145355225
RMSE train: 2.990018	val: 6.191282	test: 5.598258
MAE train: 2.688805	val: 5.277382	test: 4.907654Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/freesolv/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/freesolv/scaff/train_prop=0.8/freesolv_scaff_5_26-05_10-18-47  ]
[ Using Seed :  5  ]
[ Using device :  cuda:2  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 21.056096076965332
RMSE train: 5.116389	val: 8.593554	test: 6.998773
MAE train: 4.309756	val: 6.749035	test: 6.177153

Epoch: 2
Loss: 19.54881763458252
RMSE train: 5.397704	val: 8.598611	test: 7.117944
MAE train: 4.594501	val: 6.861059	test: 6.301976

Epoch: 3
Loss: 18.541428565979004
RMSE train: 5.410191	val: 8.444158	test: 7.000708
MAE train: 4.593472	val: 6.735595	test: 6.169442

Epoch: 4
Loss: 17.301130294799805
RMSE train: 5.212499	val: 8.183275	test: 6.792918
MAE train: 4.397051	val: 6.458540	test: 5.934653

Epoch: 5
Loss: 16.50622320175171
RMSE train: 4.883346	val: 7.942247	test: 6.646771
MAE train: 4.101717	val: 6.211331	test: 5.782295

Epoch: 6
Loss: 15.171558856964111
RMSE train: 4.515805	val: 7.750188	test: 6.548928
MAE train: 3.794275	val: 6.037293	test: 5.703985

Epoch: 7
Loss: 14.576389789581299
RMSE train: 4.263164	val: 7.647988	test: 6.500751
MAE train: 3.598746	val: 6.012111	test: 5.674233

Epoch: 8
Loss: 13.493584156036377
RMSE train: 4.142083	val: 7.573306	test: 6.494703
MAE train: 3.535253	val: 6.052575	test: 5.715493

Epoch: 9
Loss: 13.334139823913574
RMSE train: 4.114567	val: 7.519797	test: 6.397006
MAE train: 3.549079	val: 6.121299	test: 5.651513

Epoch: 10
Loss: 11.78775930404663
RMSE train: 4.053499	val: 7.374933	test: 6.218291
MAE train: 3.520976	val: 6.099943	test: 5.519066

Epoch: 11
Loss: 11.834356307983398
RMSE train: 3.893696	val: 7.116328	test: 6.021221
MAE train: 3.401038	val: 5.950642	test: 5.346718

Epoch: 12
Loss: 10.889190196990967
RMSE train: 3.666764	val: 6.676913	test: 5.732207
MAE train: 3.206763	val: 5.557562	test: 5.059287

Epoch: 13
Loss: 10.42688512802124
RMSE train: 3.405270	val: 6.129275	test: 5.406296
MAE train: 2.958407	val: 5.001977	test: 4.737724

Epoch: 14
Loss: 9.545791149139404
RMSE train: 3.180129	val: 5.620208	test: 5.128315
MAE train: 2.741209	val: 4.450238	test: 4.456895

Epoch: 15
Loss: 9.018331527709961
RMSE train: 2.999396	val: 5.334288	test: 4.949195
MAE train: 2.564634	val: 4.116552	test: 4.247984

Epoch: 16
Loss: 8.650936126708984
RMSE train: 2.946365	val: 5.329080	test: 4.941302
MAE train: 2.525785	val: 4.140643	test: 4.217147

Epoch: 17
Loss: 7.991583347320557
RMSE train: 2.984629	val: 5.463853	test: 5.045832
MAE train: 2.586293	val: 4.349842	test: 4.309190

Epoch: 18
Loss: 7.754264831542969
RMSE train: 3.046359	val: 5.605183	test: 5.168683
MAE train: 2.663588	val: 4.553238	test: 4.438693

Epoch: 19
Loss: 7.59151029586792
RMSE train: 3.071563	val: 5.701217	test: 5.267138
MAE train: 2.704660	val: 4.690772	test: 4.575816

Epoch: 20
Loss: 7.182331800460815
RMSE train: 3.045904	val: 5.685693	test: 5.260263
MAE train: 2.682578	val: 4.697255	test: 4.593282

Epoch: 21
Loss: 6.619697570800781
RMSE train: 3.004626	val: 5.684136	test: 5.219595
MAE train: 2.636727	val: 4.721121	test: 4.572539

Epoch: 22
Loss: 6.275853633880615
RMSE train: 2.949377	val: 5.699714	test: 5.194948
MAE train: 2.587253	val: 4.729699	test: 4.539296Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/freesolv/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/freesolv/scaff/train_prop=0.8/freesolv_scaff_4_26-05_10-18-47  ]
[ Using Seed :  4  ]
[ Using device :  cuda:2  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 21.20197296142578
RMSE train: 4.553543	val: 8.999517	test: 7.085163
MAE train: 3.629096	val: 6.916874	test: 6.245158

Epoch: 2
Loss: 20.109695434570312
RMSE train: 4.548959	val: 8.934888	test: 7.104087
MAE train: 3.666957	val: 6.867706	test: 6.256744

Epoch: 3
Loss: 19.077220916748047
RMSE train: 4.512399	val: 8.934503	test: 7.155492
MAE train: 3.664230	val: 6.958122	test: 6.308835

Epoch: 4
Loss: 17.976847648620605
RMSE train: 4.435309	val: 8.925561	test: 7.209461
MAE train: 3.622686	val: 7.063493	test: 6.347774

Epoch: 5
Loss: 16.676668167114258
RMSE train: 4.307756	val: 8.884646	test: 7.245330
MAE train: 3.540686	val: 7.136872	test: 6.375819

Epoch: 6
Loss: 15.549558639526367
RMSE train: 4.111240	val: 8.799992	test: 7.260348
MAE train: 3.407511	val: 7.188135	test: 6.393283

Epoch: 7
Loss: 14.239709377288818
RMSE train: 3.829766	val: 8.554028	test: 7.151306
MAE train: 3.210743	val: 7.079957	test: 6.293867

Epoch: 8
Loss: 13.03789472579956
RMSE train: 3.503532	val: 8.162366	test: 6.967426
MAE train: 2.981018	val: 6.818658	test: 6.107491

Epoch: 9
Loss: 12.184255599975586
RMSE train: 3.208908	val: 7.747528	test: 6.718656
MAE train: 2.745910	val: 6.464589	test: 5.848524

Epoch: 10
Loss: 11.428013324737549
RMSE train: 3.012257	val: 7.348934	test: 6.409759
MAE train: 2.584165	val: 6.054461	test: 5.550254

Epoch: 11
Loss: 10.819309711456299
RMSE train: 2.948447	val: 7.136527	test: 6.194461
MAE train: 2.544420	val: 5.896665	test: 5.354306

Epoch: 12
Loss: 11.014606475830078
RMSE train: 2.880008	val: 7.052384	test: 5.977407
MAE train: 2.496431	val: 5.948355	test: 5.185093

Epoch: 13
Loss: 9.8458833694458
RMSE train: 2.915077	val: 7.027452	test: 5.835058
MAE train: 2.559946	val: 6.047802	test: 5.054554

Epoch: 14
Loss: 9.650461673736572
RMSE train: 2.947295	val: 6.968080	test: 5.703354
MAE train: 2.601871	val: 6.086078	test: 4.929875

Epoch: 15
Loss: 9.260108947753906
RMSE train: 2.978294	val: 6.936205	test: 5.607617
MAE train: 2.625969	val: 6.089381	test: 4.849634

Epoch: 16
Loss: 8.791780471801758
RMSE train: 3.028993	val: 6.960768	test: 5.551254
MAE train: 2.674052	val: 6.129800	test: 4.800304

Epoch: 17
Loss: 8.419682264328003
RMSE train: 3.030185	val: 6.936171	test: 5.459227
MAE train: 2.677845	val: 6.107830	test: 4.701659

Epoch: 18
Loss: 7.763916969299316
RMSE train: 3.009257	val: 6.905564	test: 5.375498
MAE train: 2.666286	val: 6.071617	test: 4.603711

Epoch: 19
Loss: 7.649778842926025
RMSE train: 2.929750	val: 6.734926	test: 5.236140
MAE train: 2.591919	val: 5.886191	test: 4.451108

Epoch: 20
Loss: 7.125959396362305
RMSE train: 2.851921	val: 6.584554	test: 5.108398
MAE train: 2.517509	val: 5.694930	test: 4.317405

Epoch: 21
Loss: 6.812332630157471
RMSE train: 2.773475	val: 6.446170	test: 4.985637
MAE train: 2.446109	val: 5.534572	test: 4.206515

Epoch: 22
Loss: 6.520740270614624
RMSE train: 2.725303	val: 6.339185	test: 4.920232
MAE train: 2.403411	val: 5.431169	test: 4.148998

Epoch: 23
Loss: 4.63045334815979
RMSE train: 2.377274	val: 3.284604	test: 5.343171
MAE train: 2.035990	val: 2.829573	test: 3.927751

Epoch: 24
Loss: 4.350730895996094
RMSE train: 2.336172	val: 3.109454	test: 5.270397
MAE train: 2.006473	val: 2.649612	test: 3.873187

Epoch: 25
Loss: 4.04102087020874
RMSE train: 2.300042	val: 3.002942	test: 5.210302
MAE train: 1.976623	val: 2.549333	test: 3.854734

Epoch: 26
Loss: 3.869894862174988
RMSE train: 2.253611	val: 2.978580	test: 5.165373
MAE train: 1.933378	val: 2.545754	test: 3.835440

Epoch: 27
Loss: 3.7593164443969727
RMSE train: 2.233750	val: 3.057857	test: 5.192743
MAE train: 1.898001	val: 2.631827	test: 3.861903

Epoch: 28
Loss: 3.7133405208587646
RMSE train: 2.218425	val: 3.054387	test: 5.167222
MAE train: 1.874732	val: 2.622664	test: 3.840040

Epoch: 29
Loss: 3.182656764984131
RMSE train: 2.160608	val: 2.943808	test: 5.082022
MAE train: 1.811713	val: 2.508267	test: 3.761560

Epoch: 30
Loss: 3.1438037157058716
RMSE train: 2.096999	val: 2.724343	test: 4.995949
MAE train: 1.756607	val: 2.271576	test: 3.675408

Epoch: 31
Loss: 3.1386855840682983
RMSE train: 2.011794	val: 2.517230	test: 4.906942
MAE train: 1.687674	val: 2.041378	test: 3.589895

Epoch: 32
Loss: 2.717776656150818
RMSE train: 1.882558	val: 2.361485	test: 4.787284
MAE train: 1.583847	val: 1.863788	test: 3.502990

Epoch: 33
Loss: 2.7719531059265137
RMSE train: 1.790481	val: 2.295536	test: 4.757721
MAE train: 1.507721	val: 1.786990	test: 3.509653

Epoch: 34
Loss: 2.824266195297241
RMSE train: 1.732214	val: 2.228809	test: 4.765375
MAE train: 1.448297	val: 1.719032	test: 3.563243

Epoch: 35
Loss: 2.1651278734207153
RMSE train: 1.649249	val: 2.105950	test: 4.726013
MAE train: 1.366968	val: 1.587052	test: 3.547233

Epoch: 36
Loss: 2.2989578247070312
RMSE train: 1.637488	val: 2.057607	test: 4.701754
MAE train: 1.353665	val: 1.521565	test: 3.543628

Epoch: 37
Loss: 2.4705147743225098
RMSE train: 1.671974	val: 2.022459	test: 4.712633
MAE train: 1.379111	val: 1.469881	test: 3.565776

Epoch: 38
Loss: 2.1988200545310974
RMSE train: 1.753252	val: 2.029496	test: 4.769243
MAE train: 1.434088	val: 1.464698	test: 3.609127

Epoch: 39
Loss: 2.1475716829299927
RMSE train: 1.749554	val: 2.017614	test: 4.736258
MAE train: 1.408409	val: 1.431639	test: 3.554758

Epoch: 40
Loss: 2.0622421503067017
RMSE train: 1.709986	val: 1.947313	test: 4.723679
MAE train: 1.350302	val: 1.342330	test: 3.510789

Epoch: 41
Loss: 1.831541895866394
RMSE train: 1.597927	val: 1.894539	test: 4.691726
MAE train: 1.242329	val: 1.285170	test: 3.432790

Epoch: 42
Loss: 2.0033541917800903
RMSE train: 1.480183	val: 1.885551	test: 4.673808
MAE train: 1.135151	val: 1.270457	test: 3.408793

Epoch: 43
Loss: 1.7460344433784485
RMSE train: 1.335279	val: 1.805355	test: 4.585416
MAE train: 1.018110	val: 1.199276	test: 3.354739

Epoch: 44
Loss: 1.9459033608436584
RMSE train: 1.265635	val: 1.719122	test: 4.524184
MAE train: 0.976147	val: 1.140923	test: 3.327912

Epoch: 45
Loss: 1.5198103785514832
RMSE train: 1.192138	val: 1.634145	test: 4.405393
MAE train: 0.947134	val: 1.101867	test: 3.248541

Epoch: 46
Loss: 1.5161495804786682
RMSE train: 1.179052	val: 1.601054	test: 4.332081
MAE train: 0.955758	val: 1.091538	test: 3.182307

Epoch: 47
Loss: 1.5038681626319885
RMSE train: 1.206274	val: 1.588473	test: 4.312362
MAE train: 0.987759	val: 1.092211	test: 3.161617

Epoch: 48
Loss: 1.9036251306533813
RMSE train: 1.254787	val: 1.601767	test: 4.360884
MAE train: 1.014964	val: 1.118431	test: 3.183488

Epoch: 49
Loss: 1.1583752036094666
RMSE train: 1.222575	val: 1.606802	test: 4.409973
MAE train: 0.977196	val: 1.127403	test: 3.200889

Epoch: 50
Loss: 1.3745303750038147
RMSE train: 1.174720	val: 1.600887	test: 4.428128
MAE train: 0.934314	val: 1.125063	test: 3.203492

Epoch: 51
Loss: 1.1996471881866455
RMSE train: 1.113822	val: 1.589324	test: 4.386679
MAE train: 0.883750	val: 1.119418	test: 3.165161

Epoch: 52
Loss: 1.7086483836174011
RMSE train: 1.118417	val: 1.592419	test: 4.348260
MAE train: 0.884733	val: 1.125667	test: 3.144580

Epoch: 53
Loss: 1.0157995223999023
RMSE train: 1.109017	val: 1.613198	test: 4.280483
MAE train: 0.870888	val: 1.133376	test: 3.102792

Epoch: 54
Loss: 1.0768189430236816
RMSE train: 1.090396	val: 1.622864	test: 4.264362
MAE train: 0.852981	val: 1.141073	test: 3.095405

Epoch: 55
Loss: 0.9723263680934906
RMSE train: 1.089766	val: 1.613201	test: 4.285476
MAE train: 0.845804	val: 1.137572	test: 3.109742

Epoch: 56
Loss: 1.1870099306106567
RMSE train: 1.073852	val: 1.610206	test: 4.307031
MAE train: 0.829303	val: 1.139124	test: 3.141760

Epoch: 57
Loss: 1.2231812477111816
RMSE train: 1.042155	val: 1.623004	test: 4.347734
MAE train: 0.798018	val: 1.161990	test: 3.164039

Epoch: 58
Loss: 1.3592382073402405
RMSE train: 1.025450	val: 1.636684	test: 4.375994
MAE train: 0.777258	val: 1.189974	test: 3.180086

Epoch: 59
Loss: 1.211049199104309
RMSE train: 1.013283	val: 1.611191	test: 4.367599
MAE train: 0.776452	val: 1.159547	test: 3.174673

Epoch: 60
Loss: 1.1228291690349579
RMSE train: 0.994348	val: 1.613619	test: 4.323399
MAE train: 0.762926	val: 1.165904	test: 3.115103

Epoch: 61
Loss: 1.2343709468841553
RMSE train: 0.989698	val: 1.649070	test: 4.269829
MAE train: 0.760441	val: 1.227649	test: 3.057223

Epoch: 62
Loss: 1.1855497658252716
RMSE train: 1.028120	val: 1.722359	test: 4.225903
MAE train: 0.782031	val: 1.326938	test: 3.030830

Epoch: 63
Loss: 0.983077883720398
RMSE train: 1.012387	val: 1.711201	test: 4.209808
MAE train: 0.772214	val: 1.318826	test: 3.021793

Epoch: 64
Loss: 1.0181837975978851
RMSE train: 0.964038	val: 1.756798	test: 4.166366
MAE train: 0.739569	val: 1.361078	test: 2.994242

Epoch: 65
Loss: 1.0269414186477661
RMSE train: 0.915579	val: 1.832887	test: 4.148784
MAE train: 0.698818	val: 1.427216	test: 2.987120

Epoch: 66
Loss: 1.079636573791504
RMSE train: 0.876082	val: 1.837906	test: 4.141431
MAE train: 0.671827	val: 1.425555	test: 2.982908

Epoch: 67
Loss: 1.414093017578125
RMSE train: 0.816847	val: 1.772487	test: 4.150371
MAE train: 0.627805	val: 1.368355	test: 2.987520

Epoch: 68
Loss: 1.1678102016448975
RMSE train: 0.777208	val: 1.742625	test: 4.128954
MAE train: 0.583470	val: 1.340107	test: 2.967581

Epoch: 69
Loss: 1.2560172975063324
RMSE train: 0.791840	val: 1.736110	test: 4.166200
MAE train: 0.584239	val: 1.337202	test: 3.002442

Epoch: 70
Loss: 0.8015975058078766
RMSE train: 0.883070	val: 1.781482	test: 4.266814
MAE train: 0.624207	val: 1.389476	test: 3.080417

Epoch: 71
Loss: 0.971608430147171
RMSE train: 0.974174	val: 1.850511	test: 4.334486
MAE train: 0.664657	val: 1.467379	test: 3.121965

Epoch: 72
Loss: 1.013112485408783
RMSE train: 0.973164	val: 1.837349	test: 4.335796
MAE train: 0.672103	val: 1.460801	test: 3.120891

Epoch: 73
Loss: 1.0895243287086487
RMSE train: 0.922799	val: 1.819925	test: 4.256610
MAE train: 0.659481	val: 1.445613	test: 3.072569

Epoch: 74
Loss: 0.8912488520145416
RMSE train: 0.847041	val: 1.811200	test: 4.174934
MAE train: 0.627568	val: 1.428020	test: 3.006527

Epoch: 75
Loss: 0.9139759540557861
RMSE train: 0.738162	val: 1.865499	test: 4.068981
MAE train: 0.564257	val: 1.457996	test: 2.937615

Epoch: 76
Loss: 1.0367121696472168
RMSE train: 0.700509	val: 1.831990	test: 4.023373
MAE train: 0.539677	val: 1.414823	test: 2.901381

Epoch: 77
Loss: 0.8492750227451324
RMSE train: 0.714511	val: 1.818485	test: 3.990229
MAE train: 0.552819	val: 1.394768	test: 2.866138

Epoch: 78
Loss: 0.9626065790653229
RMSE train: 0.753286	val: 1.830422	test: 4.000718
MAE train: 0.582826	val: 1.400389	test: 2.860255

Epoch: 79
Loss: 0.8990583717823029
RMSE train: 0.844023	val: 1.803140	test: 4.115684
MAE train: 0.636972	val: 1.380681	test: 2.932009

Epoch: 80
Loss: 0.9056548476219177
RMSE train: 0.925018	val: 1.811562	test: 4.194916
MAE train: 0.679328	val: 1.397511	test: 2.984780

Epoch: 81
Loss: 1.0677838623523712
RMSE train: 0.980693	val: 1.759190	test: 4.270595
MAE train: 0.705676	val: 1.355735	test: 3.051370

Epoch: 82
Loss: 0.9439018964767456
RMSE train: 0.935598	val: 1.636444	test: 4.294658
MAE train: 0.681475	val: 1.237097	test: 3.074046

Epoch: 83
Loss: 0.9111400842666626
RMSE train: 0.896797	val: 1.594522	test: 4.307127
MAE train: 0.657287	val: 1.200402	test: 3.094370

Epoch: 23
Loss: 4.366589784622192
RMSE train: 2.439517	val: 3.087928	test: 5.028771
MAE train: 2.111121	val: 2.731951	test: 3.698432

Epoch: 24
Loss: 3.9008867740631104
RMSE train: 2.370796	val: 2.898046	test: 4.929193
MAE train: 2.055639	val: 2.526077	test: 3.623478

Epoch: 25
Loss: 3.6440900564193726
RMSE train: 2.306310	val: 2.593791	test: 4.807492
MAE train: 2.019187	val: 2.180482	test: 3.542378

Epoch: 26
Loss: 3.524009346961975
RMSE train: 2.293876	val: 2.496552	test: 4.743355
MAE train: 2.023188	val: 2.070602	test: 3.507029

Epoch: 27
Loss: 3.375687003135681
RMSE train: 2.278081	val: 2.441589	test: 4.763154
MAE train: 2.017844	val: 2.006107	test: 3.496764

Epoch: 28
Loss: 3.264198422431946
RMSE train: 2.221039	val: 2.450548	test: 4.795616
MAE train: 1.961914	val: 2.017473	test: 3.517083

Epoch: 29
Loss: 3.0242937803268433
RMSE train: 2.139149	val: 2.503224	test: 4.828510
MAE train: 1.869458	val: 2.080904	test: 3.542388

Epoch: 30
Loss: 2.725066661834717
RMSE train: 2.108600	val: 2.623571	test: 4.900122
MAE train: 1.821225	val: 2.195686	test: 3.606022

Epoch: 31
Loss: 3.261518120765686
RMSE train: 2.079864	val: 2.589990	test: 4.928738
MAE train: 1.779839	val: 2.145061	test: 3.628549

Epoch: 32
Loss: 3.303234338760376
RMSE train: 1.996533	val: 2.482227	test: 4.890468
MAE train: 1.686402	val: 2.022155	test: 3.592284

Epoch: 33
Loss: 2.4877923727035522
RMSE train: 1.901631	val: 2.294793	test: 4.796613
MAE train: 1.586811	val: 1.813255	test: 3.513143

Epoch: 34
Loss: 2.630261540412903
RMSE train: 1.829045	val: 2.085513	test: 4.698979
MAE train: 1.513378	val: 1.558679	test: 3.423205

Epoch: 35
Loss: 2.2308002710342407
RMSE train: 1.780191	val: 1.921023	test: 4.583198
MAE train: 1.461786	val: 1.372202	test: 3.340136

Epoch: 36
Loss: 2.1610310077667236
RMSE train: 1.733227	val: 1.794979	test: 4.498021
MAE train: 1.422192	val: 1.239901	test: 3.292740

Epoch: 37
Loss: 2.3749444484710693
RMSE train: 1.711000	val: 1.739275	test: 4.458550
MAE train: 1.402320	val: 1.198254	test: 3.263971

Epoch: 38
Loss: 2.111885190010071
RMSE train: 1.667294	val: 1.701672	test: 4.455908
MAE train: 1.359142	val: 1.183455	test: 3.250055

Epoch: 39
Loss: 1.830416202545166
RMSE train: 1.608392	val: 1.669325	test: 4.470894
MAE train: 1.307837	val: 1.181475	test: 3.246201

Epoch: 40
Loss: 1.8028595447540283
RMSE train: 1.495031	val: 1.668299	test: 4.462984
MAE train: 1.206082	val: 1.206044	test: 3.240639

Epoch: 41
Loss: 1.7176581025123596
RMSE train: 1.381349	val: 1.741345	test: 4.440810
MAE train: 1.107463	val: 1.275097	test: 3.238114

Epoch: 42
Loss: 2.070965051651001
RMSE train: 1.338161	val: 1.834329	test: 4.437089
MAE train: 1.074497	val: 1.355066	test: 3.243852

Epoch: 43
Loss: 1.700066328048706
RMSE train: 1.295117	val: 1.922750	test: 4.393444
MAE train: 1.045605	val: 1.432199	test: 3.210444

Epoch: 44
Loss: 1.7802542448043823
RMSE train: 1.291404	val: 1.870685	test: 4.341541
MAE train: 1.050835	val: 1.405832	test: 3.157225

Epoch: 45
Loss: 1.971966028213501
RMSE train: 1.350437	val: 1.823977	test: 4.282637
MAE train: 1.093742	val: 1.375051	test: 3.105551

Epoch: 46
Loss: 1.5594750046730042
RMSE train: 1.324442	val: 1.741238	test: 4.238930
MAE train: 1.061118	val: 1.289891	test: 3.065854

Epoch: 47
Loss: 1.4616708159446716
RMSE train: 1.201403	val: 1.705076	test: 4.178427
MAE train: 0.965173	val: 1.256181	test: 3.004102

Epoch: 48
Loss: 1.3951361179351807
RMSE train: 1.125737	val: 1.660674	test: 4.139834
MAE train: 0.891704	val: 1.224582	test: 2.984685

Epoch: 49
Loss: 1.2370725870132446
RMSE train: 1.053538	val: 1.690400	test: 4.080735
MAE train: 0.820049	val: 1.264535	test: 2.945064

Epoch: 50
Loss: 1.274404227733612
RMSE train: 0.999028	val: 1.785956	test: 4.000714
MAE train: 0.771651	val: 1.356207	test: 2.884827

Epoch: 51
Loss: 1.410871684551239
RMSE train: 0.995294	val: 1.826981	test: 3.961540
MAE train: 0.773765	val: 1.388859	test: 2.842535

Epoch: 52
Loss: 1.174070954322815
RMSE train: 0.989047	val: 1.944678	test: 3.917203
MAE train: 0.766038	val: 1.474373	test: 2.797742

Epoch: 53
Loss: 1.4914482831954956
RMSE train: 0.975466	val: 1.815211	test: 3.931015
MAE train: 0.754012	val: 1.356769	test: 2.812729

Epoch: 54
Loss: 1.3655205368995667
RMSE train: 0.958423	val: 1.653808	test: 3.936846
MAE train: 0.733898	val: 1.214493	test: 2.810212

Epoch: 55
Loss: 1.2760364413261414
RMSE train: 0.952980	val: 1.591396	test: 3.958634
MAE train: 0.722823	val: 1.140104	test: 2.838048

Epoch: 56
Loss: 1.2450945973396301
RMSE train: 0.963265	val: 1.575589	test: 4.001583
MAE train: 0.727660	val: 1.098122	test: 2.847707

Epoch: 57
Loss: 1.1296290755271912
RMSE train: 0.999758	val: 1.576664	test: 4.050799
MAE train: 0.757930	val: 1.093937	test: 2.862143

Epoch: 58
Loss: 1.253224492073059
RMSE train: 1.028491	val: 1.569858	test: 4.059292
MAE train: 0.786476	val: 1.125187	test: 2.844626

Epoch: 59
Loss: 1.0152133703231812
RMSE train: 1.065743	val: 1.573982	test: 4.054720
MAE train: 0.821715	val: 1.167336	test: 2.830996

Epoch: 60
Loss: 1.1586456894874573
RMSE train: 0.988284	val: 1.558015	test: 4.032052
MAE train: 0.761558	val: 1.177737	test: 2.815939

Epoch: 61
Loss: 1.228675901889801
RMSE train: 0.893783	val: 1.570023	test: 4.038422
MAE train: 0.686202	val: 1.218688	test: 2.838019

Epoch: 62
Loss: 1.1491451263427734
RMSE train: 0.865049	val: 1.576295	test: 4.067601
MAE train: 0.653247	val: 1.225078	test: 2.869084

Epoch: 63
Loss: 1.1141760349273682
RMSE train: 0.864207	val: 1.613844	test: 4.051045
MAE train: 0.652595	val: 1.256040	test: 2.863355

Epoch: 64
Loss: 1.0703245997428894
RMSE train: 0.891445	val: 1.654910	test: 4.046067
MAE train: 0.677626	val: 1.295757	test: 2.865687

Epoch: 65
Loss: 0.9075594544410706
RMSE train: 0.931206	val: 1.727984	test: 4.040149
MAE train: 0.698409	val: 1.351844	test: 2.864702

Epoch: 66
Loss: 0.9743205904960632
RMSE train: 0.965362	val: 1.795381	test: 4.067284
MAE train: 0.718106	val: 1.406464	test: 2.904347

Epoch: 67
Loss: 0.9450172781944275
RMSE train: 0.950557	val: 1.825275	test: 4.055554
MAE train: 0.709489	val: 1.429035	test: 2.905556

Epoch: 68
Loss: 1.0274682939052582
RMSE train: 0.880795	val: 1.866880	test: 4.035788
MAE train: 0.666449	val: 1.460358	test: 2.886555

Epoch: 69
Loss: 1.1523508429527283
RMSE train: 0.824949	val: 1.846024	test: 4.019547
MAE train: 0.631310	val: 1.437734	test: 2.874553

Epoch: 70
Loss: 1.0230414271354675
RMSE train: 0.796765	val: 1.872898	test: 4.006423
MAE train: 0.606827	val: 1.444936	test: 2.862705

Epoch: 71
Loss: 0.9050991833209991
RMSE train: 0.791211	val: 2.048860	test: 3.997525
MAE train: 0.608258	val: 1.577603	test: 2.868640

Epoch: 72
Loss: 1.1170524954795837
RMSE train: 0.853039	val: 2.045391	test: 4.027596
MAE train: 0.657793	val: 1.584947	test: 2.897132

Epoch: 73
Loss: 1.016127586364746
RMSE train: 0.925688	val: 2.007475	test: 4.111773
MAE train: 0.715712	val: 1.545051	test: 2.966410

Epoch: 74
Loss: 1.007572889328003
RMSE train: 1.007340	val: 1.916091	test: 4.193881
MAE train: 0.779504	val: 1.456443	test: 3.025801

Epoch: 75
Loss: 0.9636320471763611
RMSE train: 1.041332	val: 1.840054	test: 4.267680
MAE train: 0.802946	val: 1.384010	test: 3.072103

Epoch: 76
Loss: 1.2718968987464905
RMSE train: 1.062316	val: 1.825779	test: 4.318844
MAE train: 0.819011	val: 1.371204	test: 3.113128

Epoch: 77
Loss: 1.0984753966331482
RMSE train: 1.040338	val: 1.912047	test: 4.310943
MAE train: 0.804169	val: 1.454500	test: 3.110942

Epoch: 78
Loss: 1.0251798033714294
RMSE train: 0.981912	val: 2.092938	test: 4.242058
MAE train: 0.754043	val: 1.602808	test: 3.059658

Epoch: 79
Loss: 0.9973592758178711
RMSE train: 0.936228	val: 2.231961	test: 4.142889
MAE train: 0.702193	val: 1.717375	test: 2.979187

Epoch: 80
Loss: 0.8445081412792206
RMSE train: 0.911357	val: 2.195842	test: 4.129988
MAE train: 0.676593	val: 1.717231	test: 2.973208

Epoch: 81
Loss: 0.9703033566474915
RMSE train: 0.877732	val: 2.097213	test: 4.137340
MAE train: 0.654616	val: 1.667589	test: 2.980394

Epoch: 82
Loss: 0.8861604034900665
RMSE train: 0.817766	val: 1.958936	test: 4.155936
MAE train: 0.608679	val: 1.587086	test: 2.979296

Epoch: 83
Loss: 0.97644904255867
RMSE train: 0.762930	val: 1.831195	test: 4.185378
MAE train: 0.568639	val: 1.489125	test: 2.992909

Epoch: 23
Loss: 4.323615312576294
RMSE train: 2.659877	val: 3.016868	test: 5.426197
MAE train: 2.368167	val: 2.486698	test: 4.045393

Epoch: 24
Loss: 3.9717016220092773
RMSE train: 2.656370	val: 2.958298	test: 5.429358
MAE train: 2.369916	val: 2.433312	test: 4.059428

Epoch: 25
Loss: 4.040629148483276
RMSE train: 2.671944	val: 3.045927	test: 5.462443
MAE train: 2.374966	val: 2.566666	test: 4.097538

Epoch: 26
Loss: 3.7206069231033325
RMSE train: 2.621349	val: 3.087075	test: 5.423102
MAE train: 2.319325	val: 2.632706	test: 4.052062

Epoch: 27
Loss: 3.2903560400009155
RMSE train: 2.544936	val: 2.910406	test: 5.338377
MAE train: 2.254533	val: 2.437208	test: 3.971148

Epoch: 28
Loss: 3.154845356941223
RMSE train: 2.459754	val: 2.776448	test: 5.231091
MAE train: 2.175603	val: 2.293113	test: 3.851358

Epoch: 29
Loss: 2.9243897199630737
RMSE train: 2.370007	val: 2.793476	test: 5.186820
MAE train: 2.086590	val: 2.308677	test: 3.792843

Epoch: 30
Loss: 2.8569703102111816
RMSE train: 2.246233	val: 2.605375	test: 5.094052
MAE train: 1.973005	val: 2.086588	test: 3.765017

Epoch: 31
Loss: 2.8302911520004272
RMSE train: 2.192254	val: 2.505498	test: 5.074783
MAE train: 1.927181	val: 1.958848	test: 3.798908

Epoch: 32
Loss: 2.921396017074585
RMSE train: 2.189267	val: 2.463666	test: 5.102050
MAE train: 1.920901	val: 1.891683	test: 3.812695

Epoch: 33
Loss: 2.766108989715576
RMSE train: 2.126620	val: 2.354828	test: 5.024552
MAE train: 1.863248	val: 1.777351	test: 3.741615

Epoch: 34
Loss: 2.160924196243286
RMSE train: 2.050724	val: 2.335984	test: 4.961557
MAE train: 1.785134	val: 1.789804	test: 3.657420

Epoch: 35
Loss: 2.0269691944122314
RMSE train: 1.921044	val: 2.335153	test: 4.912685
MAE train: 1.642586	val: 1.822087	test: 3.608813

Epoch: 36
Loss: 2.381847381591797
RMSE train: 1.761893	val: 2.203449	test: 4.824908
MAE train: 1.492242	val: 1.694145	test: 3.541236

Epoch: 37
Loss: 2.6766130924224854
RMSE train: 1.680690	val: 2.007176	test: 4.719748
MAE train: 1.442209	val: 1.515988	test: 3.425361

Epoch: 38
Loss: 1.9345905184745789
RMSE train: 1.592475	val: 1.872445	test: 4.609633
MAE train: 1.370906	val: 1.382079	test: 3.296738

Epoch: 39
Loss: 1.9427902698516846
RMSE train: 1.601485	val: 1.816929	test: 4.577892
MAE train: 1.384482	val: 1.334158	test: 3.223295

Epoch: 40
Loss: 2.219192624092102
RMSE train: 1.627989	val: 1.809482	test: 4.675923
MAE train: 1.391813	val: 1.298471	test: 3.290707

Epoch: 41
Loss: 1.8018168807029724
RMSE train: 1.617381	val: 1.907280	test: 4.833238
MAE train: 1.352423	val: 1.322329	test: 3.423427

Epoch: 42
Loss: 1.7893644571304321
RMSE train: 1.639704	val: 2.010430	test: 4.927153
MAE train: 1.338550	val: 1.378560	test: 3.493461

Epoch: 43
Loss: 1.4682939052581787
RMSE train: 1.610140	val: 1.970419	test: 4.922742
MAE train: 1.288065	val: 1.346148	test: 3.478877

Epoch: 44
Loss: 1.6556479334831238
RMSE train: 1.543417	val: 1.878100	test: 4.824513
MAE train: 1.232256	val: 1.305690	test: 3.437045

Epoch: 45
Loss: 1.4669686257839203
RMSE train: 1.438827	val: 1.849075	test: 4.726341
MAE train: 1.145866	val: 1.326757	test: 3.417846

Epoch: 46
Loss: 1.3552270531654358
RMSE train: 1.293042	val: 1.870240	test: 4.617108
MAE train: 1.030219	val: 1.365130	test: 3.410304

Epoch: 47
Loss: 1.4119213819503784
RMSE train: 1.154305	val: 1.901266	test: 4.591408
MAE train: 0.919580	val: 1.385869	test: 3.480119

Epoch: 48
Loss: 1.7686666250228882
RMSE train: 1.084856	val: 1.908427	test: 4.562067
MAE train: 0.864773	val: 1.388439	test: 3.466285

Epoch: 49
Loss: 1.271119773387909
RMSE train: 1.057314	val: 1.946463	test: 4.510505
MAE train: 0.847236	val: 1.425714	test: 3.403010

Epoch: 50
Loss: 1.3705310821533203
RMSE train: 1.052957	val: 1.973032	test: 4.482626
MAE train: 0.848276	val: 1.459508	test: 3.353418

Epoch: 51
Loss: 1.4124400615692139
RMSE train: 1.129474	val: 1.933328	test: 4.477547
MAE train: 0.903717	val: 1.427124	test: 3.263295

Epoch: 52
Loss: 1.2477372884750366
RMSE train: 1.249151	val: 1.965180	test: 4.447100
MAE train: 0.998828	val: 1.457886	test: 3.179861

Epoch: 53
Loss: 1.1983027458190918
RMSE train: 1.296755	val: 2.006550	test: 4.427076
MAE train: 1.033990	val: 1.487702	test: 3.153240

Epoch: 54
Loss: 1.132161796092987
RMSE train: 1.292033	val: 2.032539	test: 4.432588
MAE train: 1.017956	val: 1.496745	test: 3.167706

Epoch: 55
Loss: 1.2061993479728699
RMSE train: 1.258960	val: 2.026678	test: 4.423736
MAE train: 0.976905	val: 1.486143	test: 3.182720

Epoch: 56
Loss: 1.0264636278152466
RMSE train: 1.159987	val: 2.051143	test: 4.370523
MAE train: 0.889256	val: 1.505955	test: 3.171394

Epoch: 57
Loss: 1.4421197175979614
RMSE train: 1.124452	val: 2.168739	test: 4.246372
MAE train: 0.871385	val: 1.596363	test: 3.087740

Epoch: 58
Loss: 1.2032510042190552
RMSE train: 1.084510	val: 2.333178	test: 4.117525
MAE train: 0.847826	val: 1.735824	test: 2.997654

Epoch: 59
Loss: 1.1433220505714417
RMSE train: 1.084436	val: 2.422769	test: 4.062978
MAE train: 0.855215	val: 1.811494	test: 2.953814

Epoch: 60
Loss: 0.8991638720035553
RMSE train: 1.076913	val: 2.356480	test: 4.061719
MAE train: 0.857401	val: 1.763622	test: 2.950693

Epoch: 61
Loss: 1.2172064185142517
RMSE train: 1.093477	val: 2.301043	test: 4.104152
MAE train: 0.870929	val: 1.740794	test: 2.966830

Epoch: 62
Loss: 1.1991818249225616
RMSE train: 1.070836	val: 2.322740	test: 4.183404
MAE train: 0.843913	val: 1.784683	test: 3.013595

Epoch: 63
Loss: 1.140453815460205
RMSE train: 1.012567	val: 2.262360	test: 4.282475
MAE train: 0.787750	val: 1.753625	test: 3.085673

Epoch: 64
Loss: 0.8969078660011292
RMSE train: 0.959456	val: 2.138316	test: 4.391462
MAE train: 0.734682	val: 1.649767	test: 3.184324

Epoch: 65
Loss: 1.3303622007369995
RMSE train: 0.930301	val: 2.134913	test: 4.394984
MAE train: 0.705854	val: 1.644517	test: 3.197968

Epoch: 66
Loss: 1.322243720293045
RMSE train: 0.953451	val: 2.328990	test: 4.253051
MAE train: 0.746132	val: 1.807986	test: 3.081900

Epoch: 67
Loss: 0.9667006731033325
RMSE train: 0.972848	val: 2.591842	test: 4.076397
MAE train: 0.773145	val: 2.004446	test: 2.941124

Epoch: 68
Loss: 1.088705837726593
RMSE train: 1.018212	val: 2.591239	test: 4.005447
MAE train: 0.814058	val: 2.008903	test: 2.876239

Epoch: 69
Loss: 1.2493691742420197
RMSE train: 0.991512	val: 2.545017	test: 3.992118
MAE train: 0.791844	val: 1.943781	test: 2.870962

Epoch: 70
Loss: 0.8606133460998535
RMSE train: 0.963073	val: 2.353659	test: 4.060637
MAE train: 0.769865	val: 1.782379	test: 2.922589

Epoch: 71
Loss: 1.0497760474681854
RMSE train: 0.991726	val: 2.187793	test: 4.167926
MAE train: 0.772645	val: 1.670169	test: 3.000854

Epoch: 72
Loss: 1.0011458694934845
RMSE train: 1.024104	val: 2.116360	test: 4.214885
MAE train: 0.780815	val: 1.630324	test: 3.032594

Epoch: 73
Loss: 0.8519776463508606
RMSE train: 1.073020	val: 2.074787	test: 4.230001
MAE train: 0.811909	val: 1.610321	test: 3.043352

Epoch: 74
Loss: 1.4206171035766602
RMSE train: 1.102770	val: 2.151969	test: 4.176734
MAE train: 0.836974	val: 1.683245	test: 3.012020

Epoch: 75
Loss: 0.8721424043178558
RMSE train: 1.171046	val: 2.124366	test: 4.189972
MAE train: 0.885784	val: 1.680809	test: 3.031202

Epoch: 76
Loss: 0.976480096578598
RMSE train: 1.144378	val: 2.044096	test: 4.207350
MAE train: 0.872404	val: 1.608386	test: 3.053649

Epoch: 77
Loss: 1.1461193561553955
RMSE train: 1.051977	val: 1.932125	test: 4.226871
MAE train: 0.791824	val: 1.502480	test: 3.057513

Epoch: 78
Loss: 1.0761246979236603
RMSE train: 0.979854	val: 1.896352	test: 4.213484
MAE train: 0.727657	val: 1.460642	test: 3.032948

Epoch: 79
Loss: 0.8788472414016724
RMSE train: 0.882652	val: 1.895425	test: 4.193642
MAE train: 0.649226	val: 1.456365	test: 3.005376

Epoch: 80
Loss: 0.9297463595867157
RMSE train: 0.790184	val: 2.047558	test: 4.132869
MAE train: 0.580956	val: 1.584677	test: 2.974014

Epoch: 81
Loss: 0.965080976486206
RMSE train: 0.760737	val: 2.250718	test: 4.084818
MAE train: 0.559041	val: 1.753468	test: 2.950567

Epoch: 82
Loss: 0.8174368143081665
RMSE train: 0.781833	val: 2.382232	test: 4.063924
MAE train: 0.580508	val: 1.874257	test: 2.935578

Epoch: 83
Loss: 0.9502906203269958
RMSE train: 0.848303	val: 2.383218	test: 4.088308
MAE train: 0.642031	val: 1.862838	test: 2.958757

Epoch: 23
Loss: 6.718259334564209
RMSE train: 3.080925	val: 3.923320	test: 4.431994
MAE train: 2.729580	val: 3.510408	test: 3.666420

Epoch: 24
Loss: 6.483410835266113
RMSE train: 3.051241	val: 3.826426	test: 4.350202
MAE train: 2.703063	val: 3.414285	test: 3.585111

Epoch: 25
Loss: 6.284624338150024
RMSE train: 3.045775	val: 3.819486	test: 4.331100
MAE train: 2.692527	val: 3.418476	test: 3.546283

Epoch: 26
Loss: 6.1335694789886475
RMSE train: 3.051104	val: 3.794368	test: 4.314633
MAE train: 2.691250	val: 3.398156	test: 3.517599

Epoch: 27
Loss: 5.462379693984985
RMSE train: 3.027187	val: 3.772129	test: 4.328176
MAE train: 2.664626	val: 3.385331	test: 3.522188

Epoch: 28
Loss: 5.096259832382202
RMSE train: 2.952506	val: 3.699300	test: 4.264725
MAE train: 2.596191	val: 3.309612	test: 3.451826

Epoch: 29
Loss: 4.818030834197998
RMSE train: 2.876646	val: 3.595630	test: 4.202586
MAE train: 2.532912	val: 3.193383	test: 3.389480

Epoch: 30
Loss: 4.584916830062866
RMSE train: 2.767894	val: 3.461216	test: 4.108087
MAE train: 2.435486	val: 3.038996	test: 3.285991

Epoch: 31
Loss: 4.9084227085113525
RMSE train: 2.690745	val: 3.279153	test: 4.030702
MAE train: 2.369509	val: 2.826412	test: 3.204469

Epoch: 32
Loss: 4.036038756370544
RMSE train: 2.641949	val: 3.121723	test: 3.957723
MAE train: 2.329765	val: 2.638482	test: 3.139231

Epoch: 33
Loss: 4.281048059463501
RMSE train: 2.594706	val: 3.040676	test: 3.936145
MAE train: 2.284611	val: 2.544853	test: 3.129548

Epoch: 34
Loss: 4.014576315879822
RMSE train: 2.544675	val: 2.985101	test: 3.928860
MAE train: 2.223160	val: 2.488299	test: 3.156172

Epoch: 35
Loss: 3.3810651302337646
RMSE train: 2.473660	val: 2.948771	test: 3.907349
MAE train: 2.135552	val: 2.456804	test: 3.143199

Epoch: 36
Loss: 3.6258416175842285
RMSE train: 2.398795	val: 2.896374	test: 3.847539
MAE train: 2.063471	val: 2.392409	test: 3.072834

Epoch: 37
Loss: 3.494680166244507
RMSE train: 2.320679	val: 2.753327	test: 3.690895
MAE train: 1.994426	val: 2.196991	test: 2.885868

Epoch: 38
Loss: 2.9281965494155884
RMSE train: 2.253684	val: 2.644585	test: 3.544143
MAE train: 1.934338	val: 2.040394	test: 2.738713

Epoch: 39
Loss: 3.0859118700027466
RMSE train: 2.176459	val: 2.533642	test: 3.458083
MAE train: 1.858422	val: 1.904123	test: 2.663793

Epoch: 40
Loss: 3.090694308280945
RMSE train: 2.092546	val: 2.445153	test: 3.389831
MAE train: 1.782009	val: 1.818282	test: 2.645725

Epoch: 41
Loss: 2.8020790815353394
RMSE train: 1.994349	val: 2.457030	test: 3.356208
MAE train: 1.673849	val: 1.846319	test: 2.625681

Epoch: 42
Loss: 2.494821310043335
RMSE train: 1.884806	val: 2.446157	test: 3.316812
MAE train: 1.543905	val: 1.858185	test: 2.587060

Epoch: 43
Loss: 2.169178783893585
RMSE train: 1.784398	val: 2.365529	test: 3.237406
MAE train: 1.446109	val: 1.775524	test: 2.500631

Epoch: 44
Loss: 2.2870458364486694
RMSE train: 1.713570	val: 2.237679	test: 3.133682
MAE train: 1.405877	val: 1.644529	test: 2.407483

Epoch: 45
Loss: 2.220040798187256
RMSE train: 1.668586	val: 2.127600	test: 3.058476
MAE train: 1.387245	val: 1.529875	test: 2.354155

Epoch: 46
Loss: 2.109821915626526
RMSE train: 1.637407	val: 2.051637	test: 2.993552
MAE train: 1.376982	val: 1.467951	test: 2.304282

Epoch: 47
Loss: 2.055552840232849
RMSE train: 1.626249	val: 2.018224	test: 2.969303
MAE train: 1.367504	val: 1.424858	test: 2.278419

Epoch: 48
Loss: 2.009035348892212
RMSE train: 1.605775	val: 2.030002	test: 2.995260
MAE train: 1.329519	val: 1.402401	test: 2.296996

Epoch: 49
Loss: 1.950910210609436
RMSE train: 1.600889	val: 2.055320	test: 3.024103
MAE train: 1.303611	val: 1.410733	test: 2.334839

Epoch: 50
Loss: 1.8196490406990051
RMSE train: 1.563872	val: 2.036080	test: 3.014178
MAE train: 1.256678	val: 1.394983	test: 2.346938

Epoch: 51
Loss: 1.6482225060462952
RMSE train: 1.533922	val: 1.982418	test: 2.968042
MAE train: 1.221351	val: 1.350699	test: 2.306552

Epoch: 52
Loss: 1.8445497751235962
RMSE train: 1.497508	val: 1.868343	test: 2.871663
MAE train: 1.202641	val: 1.264071	test: 2.218022

Epoch: 53
Loss: 1.4966469407081604
RMSE train: 1.419736	val: 1.786409	test: 2.770267
MAE train: 1.135414	val: 1.232620	test: 2.121654

Epoch: 54
Loss: 1.700355887413025
RMSE train: 1.326588	val: 1.760640	test: 2.695116
MAE train: 1.043214	val: 1.234852	test: 2.038242

Epoch: 55
Loss: 1.3456586599349976
RMSE train: 1.244097	val: 1.744463	test: 2.641397
MAE train: 0.958241	val: 1.207529	test: 1.974565

Epoch: 56
Loss: 1.544789969921112
RMSE train: 1.209446	val: 1.760985	test: 2.652646
MAE train: 0.915917	val: 1.174749	test: 1.980073

Epoch: 57
Loss: 1.3305767178535461
RMSE train: 1.156911	val: 1.773189	test: 2.662637
MAE train: 0.865077	val: 1.180241	test: 2.006673

Epoch: 58
Loss: 1.5373459458351135
RMSE train: 1.127631	val: 1.746903	test: 2.641491
MAE train: 0.837370	val: 1.189055	test: 1.986180

Epoch: 59
Loss: 1.5467285513877869
RMSE train: 1.091997	val: 1.738640	test: 2.594657
MAE train: 0.818116	val: 1.232583	test: 1.963103

Epoch: 60
Loss: 1.2214834690093994
RMSE train: 1.056403	val: 1.770848	test: 2.563841
MAE train: 0.799019	val: 1.299175	test: 1.940144

Epoch: 61
Loss: 1.306047797203064
RMSE train: 1.070970	val: 1.770720	test: 2.572431
MAE train: 0.805709	val: 1.281105	test: 1.940568

Epoch: 62
Loss: 1.5571702122688293
RMSE train: 1.085951	val: 1.766275	test: 2.564293
MAE train: 0.822758	val: 1.270729	test: 1.926176

Epoch: 63
Loss: 1.1645598411560059
RMSE train: 1.038673	val: 1.751000	test: 2.542298
MAE train: 0.769779	val: 1.249834	test: 1.929161

Epoch: 64
Loss: 1.3443379998207092
RMSE train: 1.011100	val: 1.753037	test: 2.534969
MAE train: 0.728016	val: 1.264178	test: 1.929898

Epoch: 65
Loss: 1.6525976657867432
RMSE train: 1.024584	val: 1.775492	test: 2.562302
MAE train: 0.734223	val: 1.308688	test: 1.951574

Epoch: 66
Loss: 1.2523558139801025
RMSE train: 1.049280	val: 1.793664	test: 2.572532
MAE train: 0.752984	val: 1.346677	test: 1.978898

Epoch: 67
Loss: 1.260268211364746
RMSE train: 1.026800	val: 1.806168	test: 2.604445
MAE train: 0.752546	val: 1.363925	test: 2.035654

Epoch: 68
Loss: 1.0093267261981964
RMSE train: 1.010134	val: 1.850180	test: 2.619323
MAE train: 0.742447	val: 1.410335	test: 2.046737

Epoch: 69
Loss: 1.081468105316162
RMSE train: 0.995377	val: 1.835005	test: 2.649699
MAE train: 0.728886	val: 1.388320	test: 2.067938

Epoch: 70
Loss: 1.1198185682296753
RMSE train: 0.996753	val: 1.802173	test: 2.673676
MAE train: 0.720181	val: 1.344837	test: 2.092598

Epoch: 71
Loss: 1.0867777466773987
RMSE train: 0.972624	val: 1.780082	test: 2.663949
MAE train: 0.694975	val: 1.319171	test: 2.088941

Epoch: 72
Loss: 1.2506725192070007
RMSE train: 0.892140	val: 1.790484	test: 2.663432
MAE train: 0.643196	val: 1.350838	test: 2.099299

Epoch: 73
Loss: 1.1966240406036377
RMSE train: 0.841042	val: 1.868989	test: 2.672730
MAE train: 0.611804	val: 1.449222	test: 2.102253

Epoch: 74
Loss: 1.216398537158966
RMSE train: 0.831032	val: 1.973803	test: 2.704645
MAE train: 0.607979	val: 1.546496	test: 2.121208

Epoch: 75
Loss: 1.1018028259277344
RMSE train: 0.835628	val: 1.973231	test: 2.703182
MAE train: 0.618237	val: 1.540841	test: 2.129126

Epoch: 76
Loss: 1.063271164894104
RMSE train: 0.854128	val: 1.960960	test: 2.695170
MAE train: 0.636367	val: 1.527547	test: 2.131204

Epoch: 77
Loss: 1.1841359734535217
RMSE train: 0.904264	val: 1.883320	test: 2.640213
MAE train: 0.671594	val: 1.446770	test: 2.087924

Epoch: 78
Loss: 1.0789287984371185
RMSE train: 0.926723	val: 1.832843	test: 2.612800
MAE train: 0.679407	val: 1.387990	test: 2.041640

Epoch: 79
Loss: 1.069267213344574
RMSE train: 0.916208	val: 1.812476	test: 2.606109
MAE train: 0.664976	val: 1.371732	test: 2.030047

Epoch: 80
Loss: 1.0871806144714355
RMSE train: 0.955878	val: 1.811581	test: 2.573003
MAE train: 0.683369	val: 1.391135	test: 1.996582

Epoch: 81
Loss: 0.9704931080341339
RMSE train: 0.956512	val: 1.843153	test: 2.557126
MAE train: 0.675772	val: 1.441779	test: 1.978028

Epoch: 82
Loss: 1.193278729915619
RMSE train: 0.903441	val: 1.861233	test: 2.574632
MAE train: 0.639564	val: 1.459921	test: 1.995559

Epoch: 83
Loss: 0.9343728125095367
RMSE train: 0.854728	val: 1.819745	test: 2.592897
MAE train: 0.605896	val: 1.406219	test: 2.021456

Epoch: 23
Loss: 7.6342103481292725
RMSE train: 3.556494	val: 4.796781	test: 4.547482
MAE train: 3.209804	val: 4.398254	test: 3.900624

Epoch: 24
Loss: 6.8170647621154785
RMSE train: 3.484228	val: 4.517950	test: 4.412963
MAE train: 3.148954	val: 4.108736	test: 3.781775

Epoch: 25
Loss: 6.370371103286743
RMSE train: 3.371812	val: 4.153998	test: 4.230911
MAE train: 3.046397	val: 3.720660	test: 3.615358

Epoch: 26
Loss: 5.97912335395813
RMSE train: 3.257665	val: 3.841998	test: 4.036861
MAE train: 2.941933	val: 3.379354	test: 3.419116

Epoch: 27
Loss: 5.700729608535767
RMSE train: 3.141755	val: 3.633948	test: 3.856858
MAE train: 2.829634	val: 3.148967	test: 3.208500

Epoch: 28
Loss: 5.252854108810425
RMSE train: 3.020161	val: 3.488174	test: 3.718105
MAE train: 2.710903	val: 3.003254	test: 3.069149

Epoch: 29
Loss: 5.12451696395874
RMSE train: 2.938082	val: 3.364899	test: 3.632713
MAE train: 2.628346	val: 2.884907	test: 2.981731

Epoch: 30
Loss: 5.171253204345703
RMSE train: 2.874937	val: 3.237745	test: 3.554461
MAE train: 2.566967	val: 2.758068	test: 2.885066

Epoch: 31
Loss: 4.978823304176331
RMSE train: 2.777478	val: 3.105383	test: 3.500775
MAE train: 2.464057	val: 2.624347	test: 2.842131

Epoch: 32
Loss: 4.942450523376465
RMSE train: 2.645761	val: 2.979291	test: 3.402240
MAE train: 2.328186	val: 2.495443	test: 2.735278

Epoch: 33
Loss: 4.361820459365845
RMSE train: 2.488636	val: 2.866643	test: 3.282328
MAE train: 2.169351	val: 2.382959	test: 2.585097

Epoch: 34
Loss: 4.385047554969788
RMSE train: 2.384419	val: 2.801802	test: 3.237651
MAE train: 2.059305	val: 2.317480	test: 2.531917

Epoch: 35
Loss: 3.8030641078948975
RMSE train: 2.340456	val: 2.730227	test: 3.216886
MAE train: 2.015520	val: 2.241239	test: 2.526237

Epoch: 36
Loss: 3.598947048187256
RMSE train: 2.320801	val: 2.620548	test: 3.149643
MAE train: 2.003391	val: 2.105926	test: 2.463989

Epoch: 37
Loss: 3.380490303039551
RMSE train: 2.304978	val: 2.518259	test: 3.115237
MAE train: 1.992992	val: 1.980057	test: 2.449566

Epoch: 38
Loss: 3.3361823558807373
RMSE train: 2.264187	val: 2.412025	test: 3.090018
MAE train: 1.953201	val: 1.850855	test: 2.443006

Epoch: 39
Loss: 3.093496561050415
RMSE train: 2.169174	val: 2.344450	test: 3.063737
MAE train: 1.858299	val: 1.776658	test: 2.431367

Epoch: 40
Loss: 3.082473874092102
RMSE train: 2.095559	val: 2.334172	test: 3.042119
MAE train: 1.790869	val: 1.775083	test: 2.406364

Epoch: 41
Loss: 2.690301299095154
RMSE train: 2.023866	val: 2.338313	test: 2.970209
MAE train: 1.722215	val: 1.791141	test: 2.320639

Epoch: 42
Loss: 3.1976370215415955
RMSE train: 1.977415	val: 2.383572	test: 2.927920
MAE train: 1.677004	val: 1.848642	test: 2.276081

Epoch: 43
Loss: 2.4780861139297485
RMSE train: 1.956797	val: 2.388354	test: 2.887650
MAE train: 1.666163	val: 1.860648	test: 2.238433

Epoch: 44
Loss: 2.2767555117607117
RMSE train: 1.968657	val: 2.337812	test: 2.874923
MAE train: 1.685801	val: 1.805679	test: 2.238375

Epoch: 45
Loss: 2.30679714679718
RMSE train: 1.962306	val: 2.301725	test: 2.862442
MAE train: 1.674093	val: 1.759149	test: 2.230918

Epoch: 46
Loss: 2.2611334323883057
RMSE train: 1.894663	val: 2.243399	test: 2.834682
MAE train: 1.600519	val: 1.681599	test: 2.206844

Epoch: 47
Loss: 2.1248821020126343
RMSE train: 1.791308	val: 2.144534	test: 2.772030
MAE train: 1.496884	val: 1.559963	test: 2.151428

Epoch: 48
Loss: 1.8977620005607605
RMSE train: 1.757533	val: 2.101723	test: 2.738131
MAE train: 1.456939	val: 1.495425	test: 2.134606

Epoch: 49
Loss: 2.4594661593437195
RMSE train: 1.611203	val: 2.025747	test: 2.658861
MAE train: 1.310674	val: 1.408944	test: 2.058894

Epoch: 50
Loss: 1.6667850613594055
RMSE train: 1.487912	val: 2.004246	test: 2.613847
MAE train: 1.179199	val: 1.380764	test: 2.014409

Epoch: 51
Loss: 1.830921232700348
RMSE train: 1.389868	val: 1.947604	test: 2.572040
MAE train: 1.072772	val: 1.322669	test: 1.970042

Epoch: 52
Loss: 1.5051352977752686
RMSE train: 1.365101	val: 1.910145	test: 2.565983
MAE train: 1.040191	val: 1.287873	test: 1.955550

Epoch: 53
Loss: 1.474095106124878
RMSE train: 1.371831	val: 1.848760	test: 2.558235
MAE train: 1.059432	val: 1.238255	test: 1.941244

Epoch: 54
Loss: 1.8183525800704956
RMSE train: 1.354860	val: 1.798469	test: 2.550256
MAE train: 1.058200	val: 1.196508	test: 1.930551

Epoch: 55
Loss: 1.478890597820282
RMSE train: 1.319086	val: 1.745339	test: 2.522474
MAE train: 1.037597	val: 1.158452	test: 1.899076

Epoch: 56
Loss: 1.4636810421943665
RMSE train: 1.221963	val: 1.706851	test: 2.517537
MAE train: 0.942639	val: 1.130322	test: 1.899492

Epoch: 57
Loss: 1.5596750974655151
RMSE train: 1.197148	val: 1.739217	test: 2.544438
MAE train: 0.905191	val: 1.148399	test: 1.923025

Epoch: 58
Loss: 1.5784831643104553
RMSE train: 1.201938	val: 1.779189	test: 2.575322
MAE train: 0.907229	val: 1.175639	test: 1.947021

Epoch: 59
Loss: 1.509539783000946
RMSE train: 1.215934	val: 1.735942	test: 2.556948
MAE train: 0.913804	val: 1.138115	test: 1.939063

Epoch: 60
Loss: 1.386195719242096
RMSE train: 1.234513	val: 1.685614	test: 2.536525
MAE train: 0.936763	val: 1.106809	test: 1.932662

Epoch: 61
Loss: 1.576776146888733
RMSE train: 1.234404	val: 1.633539	test: 2.503144
MAE train: 0.934137	val: 1.077297	test: 1.898952

Epoch: 62
Loss: 1.5841724872589111
RMSE train: 1.162605	val: 1.604549	test: 2.448399
MAE train: 0.864188	val: 1.058440	test: 1.840118

Epoch: 63
Loss: 1.3319052457809448
RMSE train: 1.066769	val: 1.627652	test: 2.424428
MAE train: 0.767624	val: 1.078821	test: 1.801990

Epoch: 64
Loss: 1.2664734423160553
RMSE train: 0.974941	val: 1.596077	test: 2.389801
MAE train: 0.692957	val: 1.064720	test: 1.789442

Epoch: 65
Loss: 1.4000553488731384
RMSE train: 0.947442	val: 1.563991	test: 2.360341
MAE train: 0.681852	val: 1.047863	test: 1.795534

Epoch: 66
Loss: 1.1805489659309387
RMSE train: 0.977826	val: 1.545961	test: 2.348648
MAE train: 0.718372	val: 1.041061	test: 1.812669

Epoch: 67
Loss: 1.3007825016975403
RMSE train: 1.013297	val: 1.552691	test: 2.362180
MAE train: 0.754068	val: 1.047239	test: 1.835391

Epoch: 68
Loss: 1.355090320110321
RMSE train: 1.020018	val: 1.581309	test: 2.385434
MAE train: 0.753943	val: 1.057216	test: 1.866413

Epoch: 69
Loss: 1.2496237754821777
RMSE train: 1.037554	val: 1.598001	test: 2.395840
MAE train: 0.762844	val: 1.075682	test: 1.883627

Epoch: 70
Loss: 1.8100223541259766
RMSE train: 1.105799	val: 1.654167	test: 2.414728
MAE train: 0.815733	val: 1.118408	test: 1.856474

Epoch: 71
Loss: 1.1069583296775818
RMSE train: 1.145340	val: 1.670447	test: 2.419840
MAE train: 0.846085	val: 1.127425	test: 1.832278

Epoch: 72
Loss: 1.2647326588630676
RMSE train: 1.157029	val: 1.689202	test: 2.407608
MAE train: 0.849457	val: 1.140576	test: 1.793144

Epoch: 73
Loss: 1.158591389656067
RMSE train: 1.146055	val: 1.669975	test: 2.366715
MAE train: 0.856095	val: 1.121783	test: 1.756042

Epoch: 74
Loss: 1.1964255571365356
RMSE train: 1.076638	val: 1.668489	test: 2.364482
MAE train: 0.805361	val: 1.122469	test: 1.759889

Epoch: 75
Loss: 1.3062326908111572
RMSE train: 0.984841	val: 1.697415	test: 2.411939
MAE train: 0.732284	val: 1.167562	test: 1.814333

Epoch: 76
Loss: 1.210977554321289
RMSE train: 0.965367	val: 1.720727	test: 2.493151
MAE train: 0.711680	val: 1.197544	test: 1.900257

Epoch: 77
Loss: 1.1454300284385681
RMSE train: 1.019128	val: 1.727325	test: 2.549189
MAE train: 0.750678	val: 1.179343	test: 1.946550

Epoch: 78
Loss: 1.0955746173858643
RMSE train: 1.071337	val: 1.704111	test: 2.556310
MAE train: 0.796676	val: 1.126191	test: 1.938748

Epoch: 79
Loss: 1.2669954895973206
RMSE train: 1.107752	val: 1.720584	test: 2.524827
MAE train: 0.824368	val: 1.128488	test: 1.892300

Epoch: 80
Loss: 1.1381161510944366
RMSE train: 1.132131	val: 1.774148	test: 2.518460
MAE train: 0.841113	val: 1.174215	test: 1.867966

Epoch: 81
Loss: 1.2386530041694641
RMSE train: 1.144791	val: 1.830943	test: 2.517967
MAE train: 0.843478	val: 1.231752	test: 1.853568

Epoch: 82
Loss: 1.025486171245575
RMSE train: 1.159327	val: 1.838466	test: 2.506973
MAE train: 0.845977	val: 1.251326	test: 1.819757

Epoch: 83
Loss: 1.1184388399124146
RMSE train: 1.111024	val: 1.758890	test: 2.468737
MAE train: 0.805415	val: 1.184457	test: 1.781170

Epoch: 23
Loss: 7.530688047409058
RMSE train: 3.388802	val: 4.828209	test: 4.608180
MAE train: 3.015338	val: 4.502512	test: 4.017858

Epoch: 24
Loss: 7.136397123336792
RMSE train: 3.340332	val: 4.509128	test: 4.436463
MAE train: 2.998385	val: 4.168145	test: 3.841709

Epoch: 25
Loss: 6.599354982376099
RMSE train: 3.312210	val: 4.293440	test: 4.298785
MAE train: 2.987422	val: 3.941144	test: 3.706226

Epoch: 26
Loss: 6.544433832168579
RMSE train: 3.327586	val: 4.193593	test: 4.176469
MAE train: 3.014833	val: 3.836292	test: 3.560820

Epoch: 27
Loss: 5.929227590560913
RMSE train: 3.309468	val: 4.042657	test: 3.995501
MAE train: 3.006023	val: 3.681595	test: 3.336974

Epoch: 28
Loss: 6.040480613708496
RMSE train: 3.248845	val: 3.880511	test: 3.859986
MAE train: 2.954246	val: 3.519310	test: 3.208867

Epoch: 29
Loss: 5.8695104122161865
RMSE train: 3.132880	val: 3.701469	test: 3.733606
MAE train: 2.838818	val: 3.338075	test: 3.086355

Epoch: 30
Loss: 5.199605941772461
RMSE train: 3.061271	val: 3.598161	test: 3.671272
MAE train: 2.768215	val: 3.236897	test: 3.006289

Epoch: 31
Loss: 5.058307409286499
RMSE train: 2.941695	val: 3.550594	test: 3.658968
MAE train: 2.651222	val: 3.198644	test: 2.996553

Epoch: 32
Loss: 5.140113592147827
RMSE train: 2.878887	val: 3.558725	test: 3.705923
MAE train: 2.582729	val: 3.216940	test: 3.044450

Epoch: 33
Loss: 4.830053806304932
RMSE train: 2.780607	val: 3.576483	test: 3.715895
MAE train: 2.474843	val: 3.243484	test: 3.042739

Epoch: 34
Loss: 4.436389446258545
RMSE train: 2.719930	val: 3.505132	test: 3.645137
MAE train: 2.417341	val: 3.169761	test: 2.966720

Epoch: 35
Loss: 4.2284015417099
RMSE train: 2.676635	val: 3.311531	test: 3.472707
MAE train: 2.384571	val: 2.954270	test: 2.782452

Epoch: 36
Loss: 3.9321136474609375
RMSE train: 2.603160	val: 3.010239	test: 3.222344
MAE train: 2.324983	val: 2.603372	test: 2.533356

Epoch: 37
Loss: 3.711539626121521
RMSE train: 2.522108	val: 2.732124	test: 3.082630
MAE train: 2.247684	val: 2.274107	test: 2.396841

Epoch: 38
Loss: 3.5231717824935913
RMSE train: 2.417223	val: 2.576904	test: 3.080424
MAE train: 2.146085	val: 2.101901	test: 2.423411

Epoch: 39
Loss: 3.2221189737319946
RMSE train: 2.349657	val: 2.593951	test: 3.157630
MAE train: 2.073022	val: 2.135519	test: 2.537470

Epoch: 40
Loss: 3.13763689994812
RMSE train: 2.263974	val: 2.588836	test: 3.153166
MAE train: 1.981453	val: 2.136361	test: 2.530929

Epoch: 41
Loss: 2.9858345985412598
RMSE train: 2.183291	val: 2.518498	test: 3.062558
MAE train: 1.903654	val: 2.054483	test: 2.420090

Epoch: 42
Loss: 2.9434003829956055
RMSE train: 2.105700	val: 2.393500	test: 2.913968
MAE train: 1.828869	val: 1.907649	test: 2.246874

Epoch: 43
Loss: 2.8265089988708496
RMSE train: 2.071199	val: 2.225678	test: 2.769046
MAE train: 1.788199	val: 1.699522	test: 2.085067

Epoch: 44
Loss: 2.6609575748443604
RMSE train: 1.994040	val: 2.129532	test: 2.663301
MAE train: 1.708383	val: 1.583786	test: 1.962194

Epoch: 45
Loss: 2.4733227491378784
RMSE train: 1.902714	val: 2.083646	test: 2.679014
MAE train: 1.620242	val: 1.550220	test: 2.008821

Epoch: 46
Loss: 2.1954442262649536
RMSE train: 1.805358	val: 2.030795	test: 2.718167
MAE train: 1.527512	val: 1.502577	test: 2.088313

Epoch: 47
Loss: 2.4911696910858154
RMSE train: 1.747597	val: 1.987612	test: 2.754481
MAE train: 1.477059	val: 1.459910	test: 2.140634

Epoch: 48
Loss: 2.027099370956421
RMSE train: 1.760975	val: 1.917611	test: 2.779064
MAE train: 1.495430	val: 1.400012	test: 2.189152

Epoch: 49
Loss: 1.824008047580719
RMSE train: 1.764978	val: 1.837721	test: 2.749365
MAE train: 1.493641	val: 1.322100	test: 2.159251

Epoch: 50
Loss: 1.7427846193313599
RMSE train: 1.753021	val: 1.789577	test: 2.688584
MAE train: 1.467386	val: 1.275351	test: 2.075393

Epoch: 51
Loss: 1.6547223925590515
RMSE train: 1.697421	val: 1.748673	test: 2.624924
MAE train: 1.410019	val: 1.245707	test: 2.014283

Epoch: 52
Loss: 1.7598291635513306
RMSE train: 1.604549	val: 1.717283	test: 2.596916
MAE train: 1.320227	val: 1.222796	test: 1.997897

Epoch: 53
Loss: 1.6385521292686462
RMSE train: 1.498135	val: 1.704977	test: 2.586299
MAE train: 1.204788	val: 1.221837	test: 1.996426

Epoch: 54
Loss: 1.955057144165039
RMSE train: 1.367967	val: 1.686931	test: 2.558905
MAE train: 1.092999	val: 1.213527	test: 1.969230

Epoch: 55
Loss: 1.6521868705749512
RMSE train: 1.354509	val: 1.681340	test: 2.544909
MAE train: 1.084999	val: 1.205772	test: 1.955775

Epoch: 56
Loss: 1.6414344906806946
RMSE train: 1.322733	val: 1.658292	test: 2.528582
MAE train: 1.060456	val: 1.175403	test: 1.942069

Epoch: 57
Loss: 1.956455111503601
RMSE train: 1.331481	val: 1.617083	test: 2.521886
MAE train: 1.054068	val: 1.126795	test: 1.934215

Epoch: 58
Loss: 1.6379008889198303
RMSE train: 1.301511	val: 1.595347	test: 2.508945
MAE train: 1.026761	val: 1.124772	test: 1.915161

Epoch: 59
Loss: 1.4965986013412476
RMSE train: 1.290958	val: 1.592976	test: 2.521422
MAE train: 1.014645	val: 1.135857	test: 1.906796

Epoch: 60
Loss: 1.6791885495185852
RMSE train: 1.281624	val: 1.600913	test: 2.539343
MAE train: 0.994783	val: 1.127865	test: 1.895206

Epoch: 61
Loss: 1.2498072981834412
RMSE train: 1.211540	val: 1.624029	test: 2.558064
MAE train: 0.940813	val: 1.119642	test: 1.896928

Epoch: 62
Loss: 1.4413276314735413
RMSE train: 1.186143	val: 1.640879	test: 2.563291
MAE train: 0.919141	val: 1.127098	test: 1.882615

Epoch: 63
Loss: 1.4805848002433777
RMSE train: 1.152274	val: 1.632175	test: 2.560789
MAE train: 0.882773	val: 1.120369	test: 1.854786

Epoch: 64
Loss: 1.382460355758667
RMSE train: 1.070134	val: 1.578460	test: 2.525491
MAE train: 0.816970	val: 1.092212	test: 1.854933

Epoch: 65
Loss: 1.3346368074417114
RMSE train: 1.011699	val: 1.582912	test: 2.495827
MAE train: 0.759956	val: 1.141670	test: 1.869453

Epoch: 66
Loss: 1.1956160068511963
RMSE train: 1.010166	val: 1.604527	test: 2.471560
MAE train: 0.739319	val: 1.191554	test: 1.866520

Epoch: 67
Loss: 1.3119648694992065
RMSE train: 1.035533	val: 1.647064	test: 2.476934
MAE train: 0.753290	val: 1.259565	test: 1.905714

Epoch: 68
Loss: 1.2903355360031128
RMSE train: 1.060975	val: 1.651102	test: 2.474456
MAE train: 0.770094	val: 1.260181	test: 1.898347

Epoch: 69
Loss: 1.3188297748565674
RMSE train: 1.101600	val: 1.630176	test: 2.476124
MAE train: 0.794963	val: 1.209588	test: 1.864009

Epoch: 70
Loss: 1.1181702017784119
RMSE train: 1.104255	val: 1.611685	test: 2.509759
MAE train: 0.804981	val: 1.142007	test: 1.872907

Epoch: 71
Loss: 1.2792978584766388
RMSE train: 1.097387	val: 1.638669	test: 2.581138
MAE train: 0.800293	val: 1.156033	test: 1.884274

Epoch: 72
Loss: 1.2272650003433228
RMSE train: 1.113531	val: 1.657872	test: 2.638433
MAE train: 0.809776	val: 1.179580	test: 1.940682

Epoch: 73
Loss: 1.3886310458183289
RMSE train: 1.097680	val: 1.665401	test: 2.650753
MAE train: 0.790765	val: 1.191469	test: 1.958675

Epoch: 74
Loss: 1.1508090496063232
RMSE train: 1.048578	val: 1.644851	test: 2.638059
MAE train: 0.757918	val: 1.181420	test: 1.971038

Epoch: 75
Loss: 1.4670403599739075
RMSE train: 1.028988	val: 1.641718	test: 2.602246
MAE train: 0.741226	val: 1.198288	test: 1.976736

Epoch: 76
Loss: 1.3279115557670593
RMSE train: 1.030293	val: 1.655762	test: 2.589132
MAE train: 0.742823	val: 1.244951	test: 2.010261

Epoch: 77
Loss: 1.248328447341919
RMSE train: 1.022441	val: 1.631787	test: 2.600793
MAE train: 0.735860	val: 1.213811	test: 2.015824

Epoch: 78
Loss: 1.179217278957367
RMSE train: 0.984715	val: 1.605989	test: 2.609284
MAE train: 0.714788	val: 1.166503	test: 1.989290

Epoch: 79
Loss: 1.159118413925171
RMSE train: 0.954197	val: 1.588511	test: 2.626847
MAE train: 0.696162	val: 1.129503	test: 1.977256

Epoch: 80
Loss: 1.1412399411201477
RMSE train: 0.993411	val: 1.583087	test: 2.638415
MAE train: 0.732978	val: 1.117232	test: 1.943011

Epoch: 81
Loss: 1.0295227766036987
RMSE train: 1.062085	val: 1.587922	test: 2.661672
MAE train: 0.789348	val: 1.139966	test: 1.924318

Epoch: 82
Loss: 1.0290562212467194
RMSE train: 1.053998	val: 1.584880	test: 2.709925
MAE train: 0.780754	val: 1.129861	test: 1.935621

Epoch: 83
Loss: 1.027633249759674
RMSE train: 1.044818	val: 1.599931	test: 2.747881
MAE train: 0.759393	val: 1.150321	test: 1.966193

Epoch: 84
Loss: 0.8154372274875641
RMSE train: 0.918906	val: 2.272540	test: 4.150530
MAE train: 0.697986	val: 1.760906	test: 3.025583

Epoch: 85
Loss: 0.9449474811553955
RMSE train: 0.940128	val: 2.169961	test: 4.191847
MAE train: 0.712752	val: 1.663803	test: 3.065015

Epoch: 86
Loss: 0.9061431288719177
RMSE train: 0.898003	val: 2.147062	test: 4.211912
MAE train: 0.685153	val: 1.638004	test: 3.070365

Epoch: 87
Loss: 0.9641988575458527
RMSE train: 0.861161	val: 2.125670	test: 4.247490
MAE train: 0.653905	val: 1.629982	test: 3.083417

Epoch: 88
Loss: 0.7902138829231262
RMSE train: 0.847333	val: 2.061629	test: 4.267529
MAE train: 0.639840	val: 1.598172	test: 3.084124

Epoch: 89
Loss: 0.9048758745193481
RMSE train: 0.865588	val: 2.083164	test: 4.247479
MAE train: 0.660302	val: 1.617635	test: 3.059326

Epoch: 90
Loss: 0.7847873568534851
RMSE train: 0.836958	val: 2.090034	test: 4.198282
MAE train: 0.641221	val: 1.620790	test: 3.020407

Epoch: 91
Loss: 0.9442390203475952
RMSE train: 0.823767	val: 2.140577	test: 4.157848
MAE train: 0.635022	val: 1.645261	test: 2.991189

Epoch: 92
Loss: 0.9879337549209595
RMSE train: 0.872960	val: 2.099641	test: 4.196527
MAE train: 0.673674	val: 1.617547	test: 3.034231

Epoch: 93
Loss: 0.7680931687355042
RMSE train: 0.938991	val: 2.063984	test: 4.247179
MAE train: 0.718020	val: 1.597337	test: 3.087743

Epoch: 94
Loss: 0.8354185223579407
RMSE train: 0.947872	val: 2.077819	test: 4.292310
MAE train: 0.711148	val: 1.603371	test: 3.133678

Epoch: 95
Loss: 0.8217496871948242
RMSE train: 0.966526	val: 2.146817	test: 4.280664
MAE train: 0.728549	val: 1.650592	test: 3.138420

Epoch: 96
Loss: 0.7940621674060822
RMSE train: 0.959004	val: 2.128175	test: 4.264102
MAE train: 0.729260	val: 1.633051	test: 3.134952

Epoch: 97
Loss: 0.5750986337661743
RMSE train: 0.912362	val: 2.080049	test: 4.239816
MAE train: 0.702260	val: 1.594008	test: 3.109982

Epoch: 98
Loss: 0.7228860557079315
RMSE train: 0.881918	val: 1.994621	test: 4.228828
MAE train: 0.680926	val: 1.529934	test: 3.094924

Epoch: 99
Loss: 0.7835589051246643
RMSE train: 0.862969	val: 1.984518	test: 4.208164
MAE train: 0.665978	val: 1.523021	test: 3.069334

Epoch: 100
Loss: 0.9248824417591095
RMSE train: 0.876245	val: 2.011470	test: 4.226197
MAE train: 0.669836	val: 1.559280	test: 3.072331

Epoch: 101
Loss: 0.7826243340969086
RMSE train: 0.911473	val: 2.065735	test: 4.251289
MAE train: 0.682826	val: 1.626453	test: 3.076921

Epoch: 102
Loss: 0.708835244178772
RMSE train: 0.918436	val: 2.095342	test: 4.257961
MAE train: 0.672371	val: 1.659232	test: 3.062170

Epoch: 103
Loss: 0.6989892423152924
RMSE train: 0.917879	val: 2.076824	test: 4.295453
MAE train: 0.668631	val: 1.635581	test: 3.078720

Epoch: 104
Loss: 0.7627103924751282
RMSE train: 0.882351	val: 2.034682	test: 4.314251
MAE train: 0.656079	val: 1.585723	test: 3.090640

Epoch: 105
Loss: 0.8013230264186859
RMSE train: 0.894728	val: 1.989374	test: 4.340819
MAE train: 0.673248	val: 1.543544	test: 3.117110

Epoch: 106
Loss: 0.7546753585338593
RMSE train: 0.881241	val: 2.064687	test: 4.286354
MAE train: 0.667441	val: 1.596814	test: 3.100657

Epoch: 107
Loss: 0.8141302466392517
RMSE train: 0.884752	val: 2.165293	test: 4.270443
MAE train: 0.666683	val: 1.689604	test: 3.117422

Epoch: 108
Loss: 0.7744617760181427
RMSE train: 0.871307	val: 2.312061	test: 4.205216
MAE train: 0.648944	val: 1.818099	test: 3.086421

Epoch: 109
Loss: 0.7454228699207306
RMSE train: 0.883432	val: 2.441782	test: 4.166208
MAE train: 0.659609	val: 1.936042	test: 3.071291

Epoch: 110
Loss: 0.7704253494739532
RMSE train: 0.890121	val: 2.344712	test: 4.195283
MAE train: 0.662234	val: 1.860580	test: 3.094196

Epoch: 111
Loss: 0.7121837139129639
RMSE train: 0.818993	val: 2.242947	test: 4.181830
MAE train: 0.617774	val: 1.759158	test: 3.082938

Epoch: 112
Loss: 0.7452461123466492
RMSE train: 0.782230	val: 2.131562	test: 4.179313
MAE train: 0.593158	val: 1.661881	test: 3.074459

Epoch: 113
Loss: 0.808170735836029
RMSE train: 0.812896	val: 1.980967	test: 4.174193
MAE train: 0.621641	val: 1.545751	test: 3.080524

Epoch: 114
Loss: 0.7515750527381897
RMSE train: 0.859981	val: 1.932372	test: 4.187666
MAE train: 0.653886	val: 1.512878	test: 3.104325

Epoch: 115
Loss: 0.6731009483337402
RMSE train: 0.908751	val: 1.934614	test: 4.189334
MAE train: 0.685306	val: 1.527434	test: 3.117139

Epoch: 116
Loss: 1.0826756060123444
RMSE train: 0.956798	val: 1.933742	test: 4.207302
MAE train: 0.722666	val: 1.539293	test: 3.125378

Epoch: 117
Loss: 0.7004436254501343
RMSE train: 0.917370	val: 1.961728	test: 4.155524
MAE train: 0.696599	val: 1.567484	test: 3.070642

Epoch: 118
Loss: 0.6885451078414917
RMSE train: 0.897815	val: 1.938191	test: 4.142320
MAE train: 0.680368	val: 1.544326	test: 3.055231

Epoch: 119
Loss: 0.8085281252861023
RMSE train: 0.803013	val: 1.989188	test: 4.085801
MAE train: 0.607855	val: 1.574490	test: 2.997896

Epoch: 120
Loss: 0.5006838440895081
RMSE train: 0.741068	val: 2.019483	test: 4.073683
MAE train: 0.562105	val: 1.594635	test: 2.982167

Epoch: 121
Loss: 0.5436663627624512
RMSE train: 0.716860	val: 1.990093	test: 4.104610
MAE train: 0.545788	val: 1.577508	test: 3.007622

Early stopping
Best (RMSE):	 train: 1.627989	val: 1.809482	test: 4.675923
Best (MAE):	 train: 1.391813	val: 1.298471	test: 3.290707


Epoch: 84
Loss: 0.9001858234405518
RMSE train: 0.748847	val: 1.767896	test: 4.201843
MAE train: 0.563862	val: 1.440943	test: 3.004638

Epoch: 85
Loss: 0.8939349353313446
RMSE train: 0.760460	val: 1.735880	test: 4.230445
MAE train: 0.576126	val: 1.416045	test: 3.034825

Epoch: 86
Loss: 0.8606507480144501
RMSE train: 0.795743	val: 1.754215	test: 4.242388
MAE train: 0.604161	val: 1.419832	test: 3.049906

Epoch: 87
Loss: 0.8781870007514954
RMSE train: 0.805421	val: 1.791251	test: 4.223836
MAE train: 0.612643	val: 1.419650	test: 3.043697

Epoch: 88
Loss: 0.8149523437023163
RMSE train: 0.788692	val: 1.911780	test: 4.173370
MAE train: 0.600492	val: 1.479147	test: 3.021447

Epoch: 89
Loss: 0.8065719306468964
RMSE train: 0.742423	val: 2.041398	test: 4.105681
MAE train: 0.572335	val: 1.547865	test: 2.981543

Epoch: 90
Loss: 0.6977707147598267
RMSE train: 0.706062	val: 2.027604	test: 4.073547
MAE train: 0.550040	val: 1.520411	test: 2.960650

Epoch: 91
Loss: 0.8655174076557159
RMSE train: 0.693017	val: 1.985824	test: 4.040378
MAE train: 0.535325	val: 1.478546	test: 2.927794

Epoch: 92
Loss: 0.8188458383083344
RMSE train: 0.723733	val: 1.850918	test: 4.077787
MAE train: 0.561797	val: 1.380820	test: 2.939631

Epoch: 93
Loss: 1.3304010033607483
RMSE train: 0.764034	val: 1.688853	test: 4.155414
MAE train: 0.593287	val: 1.269806	test: 2.978288

Epoch: 94
Loss: 0.7881860136985779
RMSE train: 0.748109	val: 1.619676	test: 4.162302
MAE train: 0.575101	val: 1.227600	test: 2.973007

Epoch: 95
Loss: 0.9739074110984802
RMSE train: 0.771199	val: 1.564540	test: 4.169565
MAE train: 0.599946	val: 1.187842	test: 2.979879

Epoch: 96
Loss: 0.7500489950180054
RMSE train: 0.786730	val: 1.570219	test: 4.161025
MAE train: 0.615930	val: 1.194843	test: 2.974007

Epoch: 97
Loss: 0.9541536271572113
RMSE train: 0.811071	val: 1.572243	test: 4.211524
MAE train: 0.634592	val: 1.192881	test: 3.008066

Epoch: 98
Loss: 0.8389868438243866
RMSE train: 0.802768	val: 1.590842	test: 4.247787
MAE train: 0.621387	val: 1.210553	test: 3.037368

Epoch: 99
Loss: 1.0618892312049866
RMSE train: 0.754290	val: 1.680001	test: 4.238894
MAE train: 0.583542	val: 1.295771	test: 3.034286

Epoch: 100
Loss: 0.8050821423530579
RMSE train: 0.716597	val: 1.792484	test: 4.233754
MAE train: 0.555140	val: 1.390926	test: 3.031502

Epoch: 101
Loss: 0.6102287173271179
RMSE train: 0.663353	val: 1.954930	test: 4.194631
MAE train: 0.513508	val: 1.523081	test: 3.001236

Epoch: 102
Loss: 0.7560363709926605
RMSE train: 0.643620	val: 2.005400	test: 4.170005
MAE train: 0.498347	val: 1.566203	test: 2.985116

Epoch: 103
Loss: 0.8785935044288635
RMSE train: 0.634126	val: 2.085060	test: 4.116165
MAE train: 0.493228	val: 1.628101	test: 2.952036

Epoch: 104
Loss: 0.7588444352149963
RMSE train: 0.630606	val: 2.120700	test: 4.101062
MAE train: 0.489949	val: 1.643931	test: 2.947121

Epoch: 105
Loss: 0.6430160105228424
RMSE train: 0.654836	val: 2.076032	test: 4.130240
MAE train: 0.507055	val: 1.596646	test: 2.968446

Epoch: 106
Loss: 0.7518070340156555
RMSE train: 0.668489	val: 2.122018	test: 4.148745
MAE train: 0.520699	val: 1.618911	test: 2.989951

Epoch: 107
Loss: 0.5870340764522552
RMSE train: 0.673677	val: 2.100816	test: 4.133608
MAE train: 0.526272	val: 1.612219	test: 2.979494

Epoch: 108
Loss: 0.8723646700382233
RMSE train: 0.671755	val: 1.990626	test: 4.154265
MAE train: 0.528753	val: 1.548043	test: 2.999320

Epoch: 109
Loss: 0.9175535440444946
RMSE train: 0.660401	val: 2.029960	test: 4.168532
MAE train: 0.516762	val: 1.608612	test: 3.010126

Epoch: 110
Loss: 0.855740487575531
RMSE train: 0.659543	val: 2.167969	test: 4.151163
MAE train: 0.505262	val: 1.749976	test: 2.999062

Epoch: 111
Loss: 0.7089457511901855
RMSE train: 0.648842	val: 2.224860	test: 4.158922
MAE train: 0.492104	val: 1.789573	test: 3.014651

Epoch: 112
Loss: 0.6634236574172974
RMSE train: 0.637816	val: 2.142889	test: 4.153622
MAE train: 0.484774	val: 1.712681	test: 3.014960

Epoch: 113
Loss: 0.6849755346775055
RMSE train: 0.655045	val: 1.972524	test: 4.162506
MAE train: 0.501071	val: 1.560496	test: 3.023082

Epoch: 114
Loss: 0.7782318294048309
RMSE train: 0.711371	val: 1.793540	test: 4.182344
MAE train: 0.553134	val: 1.404510	test: 3.036159

Epoch: 115
Loss: 0.8509167432785034
RMSE train: 0.776568	val: 1.681004	test: 4.184101
MAE train: 0.608423	val: 1.300102	test: 3.025195

Epoch: 116
Loss: 0.6728442013263702
RMSE train: 0.806132	val: 1.579710	test: 4.205176
MAE train: 0.635088	val: 1.212765	test: 3.037309

Epoch: 117
Loss: 0.6859488785266876
RMSE train: 0.745131	val: 1.558930	test: 4.203286
MAE train: 0.597499	val: 1.199815	test: 3.044636

Epoch: 118
Loss: 0.8312980234622955
RMSE train: 0.689555	val: 1.569063	test: 4.222242
MAE train: 0.551705	val: 1.208816	test: 3.060029

Epoch: 119
Loss: 0.724238783121109
RMSE train: 0.665473	val: 1.582664	test: 4.197757
MAE train: 0.529877	val: 1.214996	test: 3.044056

Epoch: 120
Loss: 0.5682063102722168
RMSE train: 0.651339	val: 1.596980	test: 4.157518
MAE train: 0.523053	val: 1.218935	test: 3.011341

Epoch: 121
Loss: 0.6850379705429077
RMSE train: 0.682186	val: 1.570419	test: 4.149627
MAE train: 0.538376	val: 1.187999	test: 2.994012

Early stopping
Best (RMSE):	 train: 0.988284	val: 1.558015	test: 4.032052
Best (MAE):	 train: 0.761558	val: 1.177737	test: 2.815939


Epoch: 23
Loss: 5.549623012542725
RMSE train: 2.959744	val: 6.099805	test: 5.452618
MAE train: 2.653742	val: 5.163392	test: 4.769843

Epoch: 24
Loss: 5.294961929321289
RMSE train: 2.895066	val: 5.968007	test: 5.306028
MAE train: 2.598030	val: 5.004970	test: 4.624561

Epoch: 25
Loss: 4.988894462585449
RMSE train: 2.837571	val: 5.858621	test: 5.202390
MAE train: 2.549437	val: 4.859655	test: 4.514592

Epoch: 26
Loss: 4.674878120422363
RMSE train: 2.743403	val: 5.753083	test: 5.084443
MAE train: 2.463185	val: 4.717954	test: 4.382795

Epoch: 27
Loss: 4.4750659465789795
RMSE train: 2.712592	val: 5.729344	test: 5.023255
MAE train: 2.435958	val: 4.650962	test: 4.314410

Epoch: 28
Loss: 4.217185139656067
RMSE train: 2.693939	val: 5.734376	test: 4.981255
MAE train: 2.412512	val: 4.636949	test: 4.272943

Epoch: 29
Loss: 4.196302652359009
RMSE train: 2.712837	val: 5.789661	test: 4.956617
MAE train: 2.414315	val: 4.675176	test: 4.255771

Epoch: 30
Loss: 3.842435836791992
RMSE train: 2.629283	val: 5.719302	test: 4.832988
MAE train: 2.320866	val: 4.586055	test: 4.126576

Epoch: 31
Loss: 4.021886825561523
RMSE train: 2.484963	val: 5.587618	test: 4.682627
MAE train: 2.181957	val: 4.435057	test: 3.970157

Epoch: 32
Loss: 3.38686740398407
RMSE train: 2.291546	val: 5.390529	test: 4.508783
MAE train: 1.997072	val: 4.204204	test: 3.769526

Epoch: 33
Loss: 3.173053026199341
RMSE train: 2.123460	val: 5.215044	test: 4.366870
MAE train: 1.837616	val: 3.987746	test: 3.592464

Epoch: 34
Loss: 3.234735608100891
RMSE train: 1.999618	val: 5.080118	test: 4.269641
MAE train: 1.721039	val: 3.796911	test: 3.477560

Epoch: 35
Loss: 2.9857534170150757
RMSE train: 1.928828	val: 5.045293	test: 4.231186
MAE train: 1.649170	val: 3.745450	test: 3.450181

Epoch: 36
Loss: 2.8772300481796265
RMSE train: 1.888446	val: 5.071373	test: 4.212312
MAE train: 1.604100	val: 3.768759	test: 3.435689

Epoch: 37
Loss: 2.4123587608337402
RMSE train: 1.854633	val: 5.068251	test: 4.178227
MAE train: 1.566060	val: 3.781358	test: 3.402229

Epoch: 38
Loss: 2.2316012382507324
RMSE train: 1.816660	val: 5.058154	test: 4.146139
MAE train: 1.527292	val: 3.782711	test: 3.372977

Epoch: 39
Loss: 2.170311391353607
RMSE train: 1.754106	val: 4.983364	test: 4.101604
MAE train: 1.472513	val: 3.710764	test: 3.348421

Epoch: 40
Loss: 2.2054011821746826
RMSE train: 1.651492	val: 4.844257	test: 4.009511
MAE train: 1.377245	val: 3.581963	test: 3.263539

Epoch: 41
Loss: 1.9267297983169556
RMSE train: 1.544809	val: 4.717892	test: 3.881906
MAE train: 1.278531	val: 3.452032	test: 3.135936

Epoch: 42
Loss: 1.7786628007888794
RMSE train: 1.458198	val: 4.631758	test: 3.776990
MAE train: 1.199209	val: 3.357897	test: 3.019980

Epoch: 43
Loss: 2.0061095356941223
RMSE train: 1.402212	val: 4.586046	test: 3.674290
MAE train: 1.135177	val: 3.269221	test: 2.887203

Epoch: 44
Loss: 1.772011637687683
RMSE train: 1.390175	val: 4.593964	test: 3.614521
MAE train: 1.109691	val: 3.230047	test: 2.789277

Epoch: 45
Loss: 1.6941173672676086
RMSE train: 1.408349	val: 4.598164	test: 3.597462
MAE train: 1.111502	val: 3.208945	test: 2.741870

Epoch: 46
Loss: 1.6028363108634949
RMSE train: 1.401440	val: 4.546180	test: 3.575525
MAE train: 1.099105	val: 3.184384	test: 2.724702

Epoch: 47
Loss: 1.6103068590164185
RMSE train: 1.359299	val: 4.475871	test: 3.534892
MAE train: 1.057519	val: 3.137303	test: 2.678491

Epoch: 48
Loss: 1.6870844960212708
RMSE train: 1.333178	val: 4.460228	test: 3.491639
MAE train: 1.039877	val: 3.121644	test: 2.633205

Epoch: 49
Loss: 1.4919352531433105
RMSE train: 1.307356	val: 4.476493	test: 3.480253
MAE train: 1.008221	val: 3.137473	test: 2.643148

Epoch: 50
Loss: 1.366030752658844
RMSE train: 1.225630	val: 4.384426	test: 3.410024
MAE train: 0.930513	val: 3.048562	test: 2.573437

Epoch: 51
Loss: 1.551957368850708
RMSE train: 1.172857	val: 4.356881	test: 3.393333
MAE train: 0.890456	val: 3.012617	test: 2.528031

Epoch: 52
Loss: 1.3724090456962585
RMSE train: 1.114867	val: 4.318050	test: 3.366482
MAE train: 0.841530	val: 2.979267	test: 2.487617

Epoch: 53
Loss: 1.4322015047073364
RMSE train: 1.063639	val: 4.263168	test: 3.307379
MAE train: 0.797629	val: 2.948145	test: 2.437971

Epoch: 54
Loss: 1.2459976077079773
RMSE train: 1.029119	val: 4.239452	test: 3.257995
MAE train: 0.759967	val: 2.932080	test: 2.393049

Epoch: 55
Loss: 1.4492707252502441
RMSE train: 1.006706	val: 4.287024	test: 3.245798
MAE train: 0.750982	val: 2.990123	test: 2.375474

Epoch: 56
Loss: 1.052198588848114
RMSE train: 0.993588	val: 4.308970	test: 3.238725
MAE train: 0.745996	val: 3.029016	test: 2.371040

Epoch: 57
Loss: 1.2915520668029785
RMSE train: 0.945979	val: 4.267984	test: 3.203397
MAE train: 0.708160	val: 3.017130	test: 2.345202

Epoch: 58
Loss: 1.0548025369644165
RMSE train: 0.895233	val: 4.209150	test: 3.172575
MAE train: 0.667508	val: 2.979325	test: 2.323210

Epoch: 59
Loss: 1.2366389036178589
RMSE train: 0.869517	val: 4.174386	test: 3.153430
MAE train: 0.649335	val: 2.949353	test: 2.308573

Epoch: 60
Loss: 1.1441564559936523
RMSE train: 0.861655	val: 4.162082	test: 3.152471
MAE train: 0.643015	val: 2.910634	test: 2.292577

Epoch: 61
Loss: 1.189824253320694
RMSE train: 0.870164	val: 4.168564	test: 3.148440
MAE train: 0.649182	val: 2.885653	test: 2.283773

Epoch: 62
Loss: 1.1675684750080109
RMSE train: 0.883624	val: 4.203600	test: 3.132700
MAE train: 0.659458	val: 2.909500	test: 2.265145

Epoch: 63
Loss: 1.1812989115715027
RMSE train: 0.874125	val: 4.202785	test: 3.092416
MAE train: 0.651229	val: 2.920078	test: 2.239408

Epoch: 64
Loss: 1.1812912821769714
RMSE train: 0.865495	val: 4.177979	test: 3.072932
MAE train: 0.644905	val: 2.917363	test: 2.247476

Epoch: 65
Loss: 1.0944043397903442
RMSE train: 0.870992	val: 4.206430	test: 3.071190
MAE train: 0.646567	val: 2.918405	test: 2.232115

Epoch: 66
Loss: 1.0844099521636963
RMSE train: 0.829274	val: 4.206257	test: 3.054408
MAE train: 0.609274	val: 2.892318	test: 2.202524

Epoch: 67
Loss: 1.1237135529518127
RMSE train: 0.808420	val: 4.235552	test: 3.044243
MAE train: 0.594512	val: 2.894125	test: 2.188456

Epoch: 68
Loss: 1.027238368988037
RMSE train: 0.792665	val: 4.246378	test: 3.007686
MAE train: 0.591295	val: 2.882291	test: 2.159695

Epoch: 69
Loss: 1.2475528717041016
RMSE train: 0.808014	val: 4.261141	test: 3.011477
MAE train: 0.603662	val: 2.903714	test: 2.183815

Epoch: 70
Loss: 1.0640989542007446
RMSE train: 0.832639	val: 4.281355	test: 3.025316
MAE train: 0.613408	val: 2.917594	test: 2.224891

Epoch: 71
Loss: 1.0678856372833252
RMSE train: 0.851015	val: 4.270567	test: 3.044563
MAE train: 0.625494	val: 2.942302	test: 2.257475

Epoch: 72
Loss: 1.0477526783943176
RMSE train: 0.811359	val: 4.222729	test: 3.024363
MAE train: 0.600883	val: 2.932112	test: 2.229549

Epoch: 73
Loss: 1.050966203212738
RMSE train: 0.760597	val: 4.163938	test: 3.003667
MAE train: 0.568970	val: 2.924051	test: 2.198570

Epoch: 74
Loss: 0.924194872379303
RMSE train: 0.719529	val: 4.097740	test: 2.977465
MAE train: 0.529771	val: 2.903782	test: 2.172715

Epoch: 75
Loss: 0.943562239408493
RMSE train: 0.739560	val: 4.118801	test: 2.983456
MAE train: 0.540529	val: 2.911422	test: 2.159632

Epoch: 76
Loss: 0.8991996645927429
RMSE train: 0.772355	val: 4.179104	test: 2.993548
MAE train: 0.566279	val: 2.947359	test: 2.180189

Epoch: 77
Loss: 1.0333508849143982
RMSE train: 0.844722	val: 4.283000	test: 3.018970
MAE train: 0.623584	val: 3.000682	test: 2.203423

Epoch: 78
Loss: 1.0420862436294556
RMSE train: 0.910703	val: 4.352240	test: 3.046018
MAE train: 0.674603	val: 3.025275	test: 2.207780

Epoch: 79
Loss: 0.9023851454257965
RMSE train: 0.962443	val: 4.417331	test: 3.082170
MAE train: 0.706999	val: 3.061817	test: 2.222897

Epoch: 80
Loss: 0.9723890721797943
RMSE train: 0.973267	val: 4.409014	test: 3.087358
MAE train: 0.714189	val: 3.059154	test: 2.218292

Epoch: 81
Loss: 1.0646387338638306
RMSE train: 0.917408	val: 4.335838	test: 3.070334
MAE train: 0.673312	val: 3.032858	test: 2.212362

Epoch: 82
Loss: 0.9622311294078827
RMSE train: 0.847071	val: 4.237409	test: 3.036849
MAE train: 0.622456	val: 2.980454	test: 2.191580

Epoch: 83
Loss: 0.9886260032653809
RMSE train: 0.807110	val: 4.164717	test: 3.015737
MAE train: 0.599505	val: 2.920585	test: 2.168854

Epoch: 84
Loss: 0.9498404860496521
RMSE train: 0.862952	val: 1.577570	test: 4.278598
MAE train: 0.637359	val: 1.185133	test: 3.081459

Epoch: 85
Loss: 0.8420726656913757
RMSE train: 0.811940	val: 1.587224	test: 4.241511
MAE train: 0.607221	val: 1.201695	test: 3.062564

Epoch: 86
Loss: 0.862127810716629
RMSE train: 0.769372	val: 1.620235	test: 4.238956
MAE train: 0.581085	val: 1.239985	test: 3.071378

Epoch: 87
Loss: 0.7560413479804993
RMSE train: 0.757707	val: 1.732568	test: 4.179162
MAE train: 0.569712	val: 1.358258	test: 3.016765

Epoch: 88
Loss: 0.7648161947727203
RMSE train: 0.773440	val: 1.800848	test: 4.169991
MAE train: 0.572525	val: 1.427532	test: 2.985129

Epoch: 89
Loss: 0.9678477048873901
RMSE train: 0.796940	val: 1.701995	test: 4.211416
MAE train: 0.596831	val: 1.327594	test: 3.015490

Epoch: 90
Loss: 0.836631178855896
RMSE train: 0.770446	val: 1.614798	test: 4.184287
MAE train: 0.581185	val: 1.231296	test: 2.994551

Epoch: 91
Loss: 0.7750074863433838
RMSE train: 0.742747	val: 1.575148	test: 4.133563
MAE train: 0.558482	val: 1.184650	test: 2.956644

Epoch: 92
Loss: 0.6917402446269989
RMSE train: 0.736211	val: 1.605975	test: 4.091184
MAE train: 0.559858	val: 1.210781	test: 2.940064

Epoch: 93
Loss: 0.8762010931968689
RMSE train: 0.739118	val: 1.779016	test: 4.034846
MAE train: 0.560302	val: 1.369707	test: 2.914355

Epoch: 94
Loss: 0.7068263590335846
RMSE train: 0.797764	val: 1.981323	test: 4.052224
MAE train: 0.581881	val: 1.559673	test: 2.920462

Epoch: 95
Loss: 0.850756973028183
RMSE train: 0.849119	val: 2.069065	test: 4.121967
MAE train: 0.596814	val: 1.640444	test: 2.958077

Epoch: 96
Loss: 0.8419969379901886
RMSE train: 0.881465	val: 1.975705	test: 4.204501
MAE train: 0.618913	val: 1.563121	test: 3.007931

Epoch: 97
Loss: 0.7030728161334991
RMSE train: 0.914388	val: 1.919756	test: 4.231598
MAE train: 0.655065	val: 1.507548	test: 3.027900

Epoch: 98
Loss: 0.757874071598053
RMSE train: 0.909988	val: 1.891789	test: 4.175755
MAE train: 0.661868	val: 1.472076	test: 2.990566

Epoch: 99
Loss: 0.6766851544380188
RMSE train: 0.904077	val: 1.900513	test: 4.142054
MAE train: 0.665251	val: 1.470535	test: 2.968492

Epoch: 100
Loss: 0.7353234589099884
RMSE train: 0.877762	val: 1.960012	test: 4.074876
MAE train: 0.647347	val: 1.511071	test: 2.927719

Epoch: 101
Loss: 0.7106645405292511
RMSE train: 0.837705	val: 2.075645	test: 4.020739
MAE train: 0.622852	val: 1.606181	test: 2.898993

Epoch: 102
Loss: 0.737691342830658
RMSE train: 0.803788	val: 2.121469	test: 4.038013
MAE train: 0.604226	val: 1.647745	test: 2.913479

Epoch: 103
Loss: 0.747119814157486
RMSE train: 0.778107	val: 2.043294	test: 4.088671
MAE train: 0.590359	val: 1.580788	test: 2.953317

Epoch: 104
Loss: 0.826633870601654
RMSE train: 0.760811	val: 1.888099	test: 4.140274
MAE train: 0.578384	val: 1.454926	test: 2.990217

Epoch: 105
Loss: 0.7827316224575043
RMSE train: 0.784100	val: 1.792701	test: 4.209079
MAE train: 0.581466	val: 1.398958	test: 3.032443

Epoch: 106
Loss: 0.6886849999427795
RMSE train: 0.819156	val: 1.739762	test: 4.239025
MAE train: 0.601295	val: 1.374999	test: 3.044446

Epoch: 107
Loss: 0.8009102642536163
RMSE train: 0.863378	val: 1.733863	test: 4.281983
MAE train: 0.627210	val: 1.396056	test: 3.095005

Epoch: 108
Loss: 0.9217309355735779
RMSE train: 0.812002	val: 1.773436	test: 4.236416
MAE train: 0.586460	val: 1.448468	test: 3.091429

Epoch: 109
Loss: 0.6771684288978577
RMSE train: 0.795476	val: 1.796053	test: 4.241193
MAE train: 0.577978	val: 1.472266	test: 3.135453

Epoch: 110
Loss: 0.6195973455905914
RMSE train: 0.798433	val: 1.822777	test: 4.214249
MAE train: 0.590585	val: 1.490986	test: 3.135052

Epoch: 111
Loss: 0.7123007774353027
RMSE train: 0.813023	val: 1.822708	test: 4.178405
MAE train: 0.612218	val: 1.480295	test: 3.130000

Epoch: 112
Loss: 0.7129133641719818
RMSE train: 0.848971	val: 1.719070	test: 4.192665
MAE train: 0.643111	val: 1.365278	test: 3.156411

Epoch: 113
Loss: 0.6877586543560028
RMSE train: 0.852950	val: 1.655755	test: 4.168750
MAE train: 0.648853	val: 1.280879	test: 3.126745

Epoch: 114
Loss: 0.6379703879356384
RMSE train: 0.850218	val: 1.598151	test: 4.167963
MAE train: 0.643127	val: 1.218918	test: 3.096592

Epoch: 115
Loss: 0.6634032428264618
RMSE train: 0.802630	val: 1.590341	test: 4.107257
MAE train: 0.598931	val: 1.228004	test: 3.014806

Epoch: 116
Loss: 0.9156526923179626
RMSE train: 0.769405	val: 1.664969	test: 4.053334
MAE train: 0.570682	val: 1.324345	test: 2.961993

Epoch: 117
Loss: 0.811950147151947
RMSE train: 0.674208	val: 1.727674	test: 3.971117
MAE train: 0.501734	val: 1.376444	test: 2.879645

Epoch: 118
Loss: 0.7970202267169952
RMSE train: 0.631077	val: 1.750304	test: 3.963306
MAE train: 0.474411	val: 1.380609	test: 2.871515

Epoch: 119
Loss: 0.7053607702255249
RMSE train: 0.626120	val: 1.734880	test: 3.993914
MAE train: 0.469885	val: 1.355750	test: 2.889292

Epoch: 120
Loss: 0.6410993933677673
RMSE train: 0.603785	val: 1.751649	test: 3.968454
MAE train: 0.454264	val: 1.363651	test: 2.865116

Epoch: 121
Loss: 0.6474395096302032
RMSE train: 0.608122	val: 1.787244	test: 3.972964
MAE train: 0.459605	val: 1.398203	test: 2.866451

Epoch: 122
Loss: 0.8255581259727478
RMSE train: 0.643581	val: 1.767203	test: 4.011988
MAE train: 0.493816	val: 1.385157	test: 2.895288

Epoch: 123
Loss: 0.5207816660404205
RMSE train: 0.643964	val: 1.772807	test: 3.971260
MAE train: 0.492353	val: 1.398548	test: 2.872722

Epoch: 124
Loss: 0.6901925802230835
RMSE train: 0.649330	val: 1.712633	test: 3.990650
MAE train: 0.496391	val: 1.355706	test: 2.900087

Epoch: 125
Loss: 0.6519862115383148
RMSE train: 0.651544	val: 1.691145	test: 3.979757
MAE train: 0.499280	val: 1.341459	test: 2.906137

Epoch: 126
Loss: 0.6063899993896484
RMSE train: 0.687358	val: 1.681627	test: 3.962640
MAE train: 0.533065	val: 1.332860	test: 2.898218

Early stopping
Best (RMSE):	 train: 0.742747	val: 1.575148	test: 4.133563
Best (MAE):	 train: 0.558482	val: 1.184650	test: 2.956644


Epoch: 23
Loss: 6.227498292922974
RMSE train: 2.848872	val: 5.723747	test: 5.165779
MAE train: 2.501745	val: 4.734286	test: 4.492405

Epoch: 24
Loss: 5.824389696121216
RMSE train: 2.731864	val: 5.684222	test: 5.076748
MAE train: 2.398608	val: 4.660157	test: 4.363337

Epoch: 25
Loss: 5.425910711288452
RMSE train: 2.633329	val: 5.630857	test: 4.983642
MAE train: 2.310330	val: 4.568478	test: 4.245415

Epoch: 26
Loss: 5.0388312339782715
RMSE train: 2.549680	val: 5.553164	test: 4.885415
MAE train: 2.227948	val: 4.473359	test: 4.129680

Epoch: 27
Loss: 5.0178046226501465
RMSE train: 2.518855	val: 5.492786	test: 4.817830
MAE train: 2.206099	val: 4.411626	test: 4.058625

Epoch: 28
Loss: 4.712810516357422
RMSE train: 2.483118	val: 5.389300	test: 4.742148
MAE train: 2.177296	val: 4.294741	test: 3.982472

Epoch: 29
Loss: 4.380943298339844
RMSE train: 2.455923	val: 5.296152	test: 4.662137
MAE train: 2.158193	val: 4.206075	test: 3.907277

Epoch: 30
Loss: 4.12570321559906
RMSE train: 2.416633	val: 5.211410	test: 4.596148
MAE train: 2.115344	val: 4.117557	test: 3.846003

Epoch: 31
Loss: 3.5777047872543335
RMSE train: 2.352708	val: 5.123355	test: 4.522869
MAE train: 2.055017	val: 4.004519	test: 3.770913

Epoch: 32
Loss: 3.673061490058899
RMSE train: 2.278515	val: 5.051918	test: 4.449673
MAE train: 1.984847	val: 3.919990	test: 3.687274

Epoch: 33
Loss: 3.5150744915008545
RMSE train: 2.200199	val: 4.996083	test: 4.348708
MAE train: 1.899420	val: 3.834607	test: 3.578909

Epoch: 34
Loss: 3.1674020290374756
RMSE train: 2.156849	val: 4.991025	test: 4.310481
MAE train: 1.852021	val: 3.828298	test: 3.543606

Epoch: 35
Loss: 3.0382351875305176
RMSE train: 2.075202	val: 4.956288	test: 4.263015
MAE train: 1.773407	val: 3.782698	test: 3.491682

Epoch: 36
Loss: 2.83163321018219
RMSE train: 2.004388	val: 4.937287	test: 4.250122
MAE train: 1.700230	val: 3.751160	test: 3.478414

Epoch: 37
Loss: 2.6635830402374268
RMSE train: 1.955633	val: 4.955521	test: 4.273922
MAE train: 1.653626	val: 3.752913	test: 3.505670

Epoch: 38
Loss: 2.6972038745880127
RMSE train: 1.869580	val: 4.952145	test: 4.253698
MAE train: 1.573146	val: 3.747699	test: 3.478648

Epoch: 39
Loss: 2.653243899345398
RMSE train: 1.756204	val: 4.894331	test: 4.185480
MAE train: 1.462040	val: 3.676745	test: 3.391941

Epoch: 40
Loss: 2.120793581008911
RMSE train: 1.646163	val: 4.811504	test: 4.101087
MAE train: 1.349552	val: 3.583557	test: 3.288567

Epoch: 41
Loss: 2.0301467180252075
RMSE train: 1.562263	val: 4.706957	test: 4.009438
MAE train: 1.258506	val: 3.453403	test: 3.179338

Epoch: 42
Loss: 2.0602325797080994
RMSE train: 1.536586	val: 4.610956	test: 3.938633
MAE train: 1.229244	val: 3.333327	test: 3.099939

Epoch: 43
Loss: 1.866649568080902
RMSE train: 1.552106	val: 4.618846	test: 3.942685
MAE train: 1.244295	val: 3.338781	test: 3.104133

Epoch: 44
Loss: 1.6844440698623657
RMSE train: 1.584527	val: 4.647999	test: 3.962643
MAE train: 1.277315	val: 3.386040	test: 3.128449

Epoch: 45
Loss: 1.6864556074142456
RMSE train: 1.590438	val: 4.661723	test: 3.973218
MAE train: 1.290894	val: 3.422957	test: 3.136688

Epoch: 46
Loss: 1.4594061374664307
RMSE train: 1.564729	val: 4.631142	test: 3.934623
MAE train: 1.262519	val: 3.377508	test: 3.083829

Epoch: 47
Loss: 1.6880649328231812
RMSE train: 1.477201	val: 4.564160	test: 3.861438
MAE train: 1.166221	val: 3.276093	test: 3.006491

Epoch: 48
Loss: 1.446926236152649
RMSE train: 1.436111	val: 4.530557	test: 3.842682
MAE train: 1.120814	val: 3.243023	test: 2.983396

Epoch: 49
Loss: 1.4673709869384766
RMSE train: 1.361118	val: 4.467233	test: 3.812782
MAE train: 1.051522	val: 3.234212	test: 2.959866

Epoch: 50
Loss: 1.3254104256629944
RMSE train: 1.298954	val: 4.392141	test: 3.783166
MAE train: 1.000085	val: 3.188324	test: 2.937041

Epoch: 51
Loss: 1.3958399295806885
RMSE train: 1.263725	val: 4.335549	test: 3.781868
MAE train: 0.977977	val: 3.146794	test: 2.940098

Epoch: 52
Loss: 1.3783641457557678
RMSE train: 1.253704	val: 4.354225	test: 3.778294
MAE train: 0.974488	val: 3.218301	test: 2.944822

Epoch: 53
Loss: 1.4401891827583313
RMSE train: 1.261081	val: 4.404049	test: 3.796997
MAE train: 0.985496	val: 3.311643	test: 2.980282

Epoch: 54
Loss: 1.2394952774047852
RMSE train: 1.189391	val: 4.370639	test: 3.724654
MAE train: 0.926761	val: 3.273581	test: 2.911405

Epoch: 55
Loss: 1.4245709776878357
RMSE train: 1.089993	val: 4.270235	test: 3.608521
MAE train: 0.837066	val: 3.133537	test: 2.785466

Epoch: 56
Loss: 1.188353955745697
RMSE train: 1.010622	val: 4.201597	test: 3.521411
MAE train: 0.765511	val: 3.026573	test: 2.673536

Epoch: 57
Loss: 1.3266810178756714
RMSE train: 0.949369	val: 4.162513	test: 3.483507
MAE train: 0.711074	val: 2.959796	test: 2.626137

Epoch: 58
Loss: 1.3301045894622803
RMSE train: 0.893601	val: 4.131155	test: 3.447213
MAE train: 0.662730	val: 2.912566	test: 2.595780

Epoch: 59
Loss: 1.2211484909057617
RMSE train: 0.870901	val: 4.125617	test: 3.414055
MAE train: 0.634478	val: 2.902220	test: 2.556829

Epoch: 60
Loss: 1.2214557528495789
RMSE train: 0.840954	val: 4.089922	test: 3.372964
MAE train: 0.614987	val: 2.873799	test: 2.515537

Epoch: 61
Loss: 1.0519751906394958
RMSE train: 0.844963	val: 4.094262	test: 3.331448
MAE train: 0.627758	val: 2.872101	test: 2.458130

Epoch: 62
Loss: 1.1387585699558258
RMSE train: 0.903164	val: 4.136966	test: 3.329440
MAE train: 0.674228	val: 2.942675	test: 2.459311

Epoch: 63
Loss: 1.1041130423545837
RMSE train: 0.926953	val: 4.152534	test: 3.347762
MAE train: 0.691071	val: 2.966673	test: 2.473259

Epoch: 64
Loss: 1.2504398822784424
RMSE train: 0.881797	val: 4.149949	test: 3.376706
MAE train: 0.644967	val: 3.001584	test: 2.506376

Epoch: 65
Loss: 1.0770047903060913
RMSE train: 0.865936	val: 4.161641	test: 3.400427
MAE train: 0.637515	val: 3.018913	test: 2.547801

Epoch: 66
Loss: 1.06078040599823
RMSE train: 0.840949	val: 4.154874	test: 3.392715
MAE train: 0.623543	val: 2.966628	test: 2.547928

Epoch: 67
Loss: 1.1193004846572876
RMSE train: 0.817470	val: 4.149183	test: 3.370134
MAE train: 0.603797	val: 2.938228	test: 2.518624

Epoch: 68
Loss: 1.125951111316681
RMSE train: 0.806538	val: 4.138731	test: 3.368293
MAE train: 0.594377	val: 2.903957	test: 2.508407

Epoch: 69
Loss: 1.1426193714141846
RMSE train: 0.841497	val: 4.196495	test: 3.406476
MAE train: 0.621639	val: 3.005777	test: 2.544862

Epoch: 70
Loss: 1.1487873792648315
RMSE train: 0.934241	val: 4.319986	test: 3.483786
MAE train: 0.688624	val: 3.189762	test: 2.609678

Epoch: 71
Loss: 1.0424225330352783
RMSE train: 0.972057	val: 4.398741	test: 3.532266
MAE train: 0.714104	val: 3.285610	test: 2.661274

Epoch: 72
Loss: 1.0857966542243958
RMSE train: 0.950259	val: 4.350234	test: 3.512380
MAE train: 0.703017	val: 3.241018	test: 2.644021

Epoch: 73
Loss: 0.9416526257991791
RMSE train: 0.898177	val: 4.245105	test: 3.488875
MAE train: 0.678471	val: 3.124114	test: 2.631412

Epoch: 74
Loss: 1.2056377530097961
RMSE train: 0.868446	val: 4.179371	test: 3.507224
MAE train: 0.663216	val: 3.007616	test: 2.659932

Epoch: 75
Loss: 1.0254172384738922
RMSE train: 0.855302	val: 4.132740	test: 3.539257
MAE train: 0.655838	val: 2.927789	test: 2.704850

Epoch: 76
Loss: 1.001380056142807
RMSE train: 0.889713	val: 4.140079	test: 3.576682
MAE train: 0.677938	val: 2.937303	test: 2.755630

Epoch: 77
Loss: 1.220154881477356
RMSE train: 0.924117	val: 4.175985	test: 3.536194
MAE train: 0.698615	val: 2.985494	test: 2.715472

Epoch: 78
Loss: 0.9028685688972473
RMSE train: 0.979950	val: 4.231417	test: 3.476217
MAE train: 0.718999	val: 3.028968	test: 2.644946

Epoch: 79
Loss: 1.1338959336280823
RMSE train: 0.997233	val: 4.245462	test: 3.424421
MAE train: 0.706998	val: 3.040196	test: 2.591676

Epoch: 80
Loss: 1.0389868021011353
RMSE train: 0.949925	val: 4.162010	test: 3.378837
MAE train: 0.672624	val: 2.974248	test: 2.555331

Epoch: 81
Loss: 1.0731855034828186
RMSE train: 0.881923	val: 4.034337	test: 3.335715
MAE train: 0.644516	val: 2.892475	test: 2.539359

Epoch: 82
Loss: 1.2674935460090637
RMSE train: 0.849662	val: 3.963619	test: 3.297031
MAE train: 0.631687	val: 2.833736	test: 2.530759

Epoch: 83
Loss: 0.974847823381424
RMSE train: 0.801853	val: 3.897493	test: 3.235911
MAE train: 0.593860	val: 2.761741	test: 2.464658

Epoch: 23
Loss: 6.348367214202881
RMSE train: 2.688545	val: 6.276520	test: 4.890059
MAE train: 2.372238	val: 5.362675	test: 4.114466

Epoch: 24
Loss: 6.138476610183716
RMSE train: 2.606563	val: 6.141392	test: 4.829108
MAE train: 2.292557	val: 5.233090	test: 4.076900

Epoch: 25
Loss: 5.721550464630127
RMSE train: 2.519854	val: 6.023679	test: 4.778832
MAE train: 2.207013	val: 5.123539	test: 4.056241

Epoch: 26
Loss: 5.416784763336182
RMSE train: 2.422816	val: 5.877745	test: 4.732657
MAE train: 2.106756	val: 4.995707	test: 4.027533

Epoch: 27
Loss: 5.228135824203491
RMSE train: 2.348261	val: 5.778396	test: 4.709927
MAE train: 2.031621	val: 4.880822	test: 4.002349

Epoch: 28
Loss: 4.905421495437622
RMSE train: 2.295183	val: 5.765910	test: 4.695567
MAE train: 1.980927	val: 4.840640	test: 3.979647

Epoch: 29
Loss: 4.713513135910034
RMSE train: 2.266796	val: 5.747586	test: 4.681862
MAE train: 1.953925	val: 4.803109	test: 3.954098

Epoch: 30
Loss: 4.470478534698486
RMSE train: 2.249528	val: 5.774602	test: 4.670208
MAE train: 1.937893	val: 4.801564	test: 3.913544

Epoch: 31
Loss: 4.101536750793457
RMSE train: 2.216700	val: 5.763708	test: 4.641281
MAE train: 1.909153	val: 4.797768	test: 3.869815

Epoch: 32
Loss: 3.8950822353363037
RMSE train: 2.198705	val: 5.765403	test: 4.618135
MAE train: 1.889357	val: 4.809815	test: 3.847092

Epoch: 33
Loss: 3.585570216178894
RMSE train: 2.175416	val: 5.743819	test: 4.584399
MAE train: 1.868772	val: 4.826931	test: 3.816908

Epoch: 34
Loss: 3.351755976676941
RMSE train: 2.126735	val: 5.654261	test: 4.504621
MAE train: 1.821005	val: 4.756518	test: 3.749341

Epoch: 35
Loss: 3.2254626750946045
RMSE train: 2.052996	val: 5.500072	test: 4.380477
MAE train: 1.748779	val: 4.576184	test: 3.635383

Epoch: 36
Loss: 3.122777223587036
RMSE train: 1.988859	val: 5.335249	test: 4.262608
MAE train: 1.686639	val: 4.378310	test: 3.519516

Epoch: 37
Loss: 2.780603051185608
RMSE train: 1.960357	val: 5.208360	test: 4.178893
MAE train: 1.661170	val: 4.219002	test: 3.428905

Epoch: 38
Loss: 2.729269027709961
RMSE train: 1.925954	val: 5.126301	test: 4.131052
MAE train: 1.631628	val: 4.092937	test: 3.366075

Epoch: 39
Loss: 2.527913808822632
RMSE train: 1.851484	val: 5.042647	test: 4.063514
MAE train: 1.554959	val: 3.923389	test: 3.294281

Epoch: 40
Loss: 2.3450512886047363
RMSE train: 1.756812	val: 4.952587	test: 3.993545
MAE train: 1.454275	val: 3.729265	test: 3.223173

Epoch: 41
Loss: 2.2627408504486084
RMSE train: 1.643544	val: 4.870946	test: 3.917780
MAE train: 1.332255	val: 3.553735	test: 3.131134

Epoch: 42
Loss: 2.2562363147735596
RMSE train: 1.588894	val: 4.846045	test: 3.865668
MAE train: 1.268745	val: 3.471523	test: 3.072685

Epoch: 43
Loss: 2.078888773918152
RMSE train: 1.523423	val: 4.756841	test: 3.774246
MAE train: 1.205092	val: 3.352781	test: 2.980764

Epoch: 44
Loss: 2.0558342337608337
RMSE train: 1.439634	val: 4.628712	test: 3.662457
MAE train: 1.127972	val: 3.211808	test: 2.863073

Epoch: 45
Loss: 1.8843633532524109
RMSE train: 1.338614	val: 4.466246	test: 3.553167
MAE train: 1.039664	val: 3.067110	test: 2.759867

Epoch: 46
Loss: 1.7673653364181519
RMSE train: 1.298819	val: 4.341206	test: 3.494374
MAE train: 1.013902	val: 2.982983	test: 2.722298

Epoch: 47
Loss: 1.8525325655937195
RMSE train: 1.224262	val: 4.302663	test: 3.478348
MAE train: 0.944445	val: 2.915249	test: 2.705429

Epoch: 48
Loss: 1.6116315126419067
RMSE train: 1.240365	val: 4.377792	test: 3.510539
MAE train: 0.942659	val: 2.935146	test: 2.717999

Epoch: 49
Loss: 1.6486263275146484
RMSE train: 1.266968	val: 4.440253	test: 3.534970
MAE train: 0.955657	val: 2.960799	test: 2.723394

Epoch: 50
Loss: 1.4498327374458313
RMSE train: 1.259474	val: 4.421157	test: 3.501858
MAE train: 0.955611	val: 2.959378	test: 2.687965

Epoch: 51
Loss: 1.6483843922615051
RMSE train: 1.229086	val: 4.344684	test: 3.437599
MAE train: 0.928301	val: 2.913692	test: 2.628136

Epoch: 52
Loss: 1.3731701374053955
RMSE train: 1.164940	val: 4.210218	test: 3.362028
MAE train: 0.878453	val: 2.828345	test: 2.566340

Epoch: 53
Loss: 1.1805251836776733
RMSE train: 1.118087	val: 4.127746	test: 3.316732
MAE train: 0.846679	val: 2.773855	test: 2.526150

Epoch: 54
Loss: 1.3694008588790894
RMSE train: 1.057092	val: 4.021742	test: 3.252489
MAE train: 0.802723	val: 2.687001	test: 2.481029

Epoch: 55
Loss: 1.2490087151527405
RMSE train: 1.053258	val: 4.017338	test: 3.233318
MAE train: 0.799094	val: 2.663328	test: 2.469846

Epoch: 56
Loss: 1.4639538526535034
RMSE train: 1.057262	val: 4.094064	test: 3.258297
MAE train: 0.809265	val: 2.725052	test: 2.485704

Epoch: 57
Loss: 1.1424617171287537
RMSE train: 1.081192	val: 4.216101	test: 3.286315
MAE train: 0.813382	val: 2.790944	test: 2.501930

Epoch: 58
Loss: 1.1542413830757141
RMSE train: 1.070939	val: 4.271259	test: 3.293778
MAE train: 0.789409	val: 2.857607	test: 2.510804

Epoch: 59
Loss: 1.3017863035202026
RMSE train: 1.013621	val: 4.215854	test: 3.263803
MAE train: 0.736416	val: 2.826478	test: 2.480216

Epoch: 60
Loss: 1.2359800338745117
RMSE train: 0.937113	val: 4.103952	test: 3.200578
MAE train: 0.671137	val: 2.762151	test: 2.403261

Epoch: 61
Loss: 1.2057920098304749
RMSE train: 0.878768	val: 3.981273	test: 3.144347
MAE train: 0.616596	val: 2.677861	test: 2.344859

Epoch: 62
Loss: 1.0487489104270935
RMSE train: 0.840797	val: 3.873878	test: 3.091717
MAE train: 0.590251	val: 2.617973	test: 2.322517

Epoch: 63
Loss: 1.3038787841796875
RMSE train: 0.827934	val: 3.833880	test: 3.064802
MAE train: 0.589055	val: 2.585382	test: 2.324617

Epoch: 64
Loss: 1.2646325826644897
RMSE train: 0.847402	val: 3.854669	test: 3.057451
MAE train: 0.611105	val: 2.623135	test: 2.330948

Epoch: 65
Loss: 1.1837548613548279
RMSE train: 0.852203	val: 3.883887	test: 3.068191
MAE train: 0.617432	val: 2.659539	test: 2.343525

Epoch: 66
Loss: 1.0349636673927307
RMSE train: 0.825657	val: 3.887748	test: 3.088559
MAE train: 0.593473	val: 2.626663	test: 2.346429

Epoch: 67
Loss: 1.1710633635520935
RMSE train: 0.815780	val: 3.857880	test: 3.081188
MAE train: 0.590853	val: 2.526872	test: 2.323785

Epoch: 68
Loss: 1.1025285720825195
RMSE train: 0.815316	val: 3.817510	test: 3.053869
MAE train: 0.590529	val: 2.480822	test: 2.300081

Epoch: 69
Loss: 1.2637932896614075
RMSE train: 0.800225	val: 3.782602	test: 3.057193
MAE train: 0.571774	val: 2.451015	test: 2.302900

Epoch: 70
Loss: 1.04556405544281
RMSE train: 0.831218	val: 3.801115	test: 3.071463
MAE train: 0.594181	val: 2.470675	test: 2.314843

Epoch: 71
Loss: 1.001536786556244
RMSE train: 0.828919	val: 3.809812	test: 3.077911
MAE train: 0.588603	val: 2.513673	test: 2.335108

Epoch: 72
Loss: 0.9138720631599426
RMSE train: 0.840966	val: 3.844518	test: 3.066097
MAE train: 0.600617	val: 2.560912	test: 2.325895

Epoch: 73
Loss: 1.009756624698639
RMSE train: 0.863659	val: 3.875066	test: 3.042716
MAE train: 0.630135	val: 2.564778	test: 2.295132

Epoch: 74
Loss: 0.9523444473743439
RMSE train: 0.892121	val: 3.928769	test: 3.027323
MAE train: 0.656915	val: 2.608859	test: 2.277358

Epoch: 75
Loss: 0.9860044419765472
RMSE train: 0.862226	val: 3.955252	test: 3.032126
MAE train: 0.636346	val: 2.653921	test: 2.293453

Epoch: 76
Loss: 0.9579021632671356
RMSE train: 0.845719	val: 3.979719	test: 3.038845
MAE train: 0.625225	val: 2.707541	test: 2.318530

Epoch: 77
Loss: 0.8906534016132355
RMSE train: 0.846886	val: 3.969230	test: 3.030246
MAE train: 0.627416	val: 2.719511	test: 2.312010

Epoch: 78
Loss: 1.0296582877635956
RMSE train: 0.852459	val: 3.989255	test: 3.051852
MAE train: 0.645507	val: 2.722027	test: 2.320729

Epoch: 79
Loss: 0.9617912769317627
RMSE train: 0.869676	val: 4.031916	test: 3.080061
MAE train: 0.654309	val: 2.731204	test: 2.311210

Epoch: 80
Loss: 1.0082646012306213
RMSE train: 0.892393	val: 4.054989	test: 3.096026
MAE train: 0.665475	val: 2.741816	test: 2.304231

Epoch: 81
Loss: 0.8469928205013275
RMSE train: 0.913200	val: 4.062752	test: 3.110685
MAE train: 0.681816	val: 2.750358	test: 2.316132

Epoch: 82
Loss: 1.054741382598877
RMSE train: 0.905053	val: 4.048664	test: 3.123606
MAE train: 0.660754	val: 2.753433	test: 2.326715

Epoch: 83
Loss: 1.027444839477539
RMSE train: 0.867327	val: 3.986476	test: 3.112981
MAE train: 0.620982	val: 2.711997	test: 2.325983All runs completed.


Epoch: 84
Loss: 0.933886706829071
RMSE train: 0.771915	val: 3.883278	test: 3.210552
MAE train: 0.569124	val: 2.745999	test: 2.429170

Epoch: 85
Loss: 1.0625637769699097
RMSE train: 0.734493	val: 3.912416	test: 3.178051
MAE train: 0.540505	val: 2.753575	test: 2.382080

Epoch: 86
Loss: 0.9203311204910278
RMSE train: 0.724105	val: 3.967922	test: 3.160316
MAE train: 0.537293	val: 2.786471	test: 2.372313

Epoch: 87
Loss: 1.074174404144287
RMSE train: 0.698123	val: 4.017132	test: 3.158994
MAE train: 0.518392	val: 2.829325	test: 2.376176

Epoch: 88
Loss: 0.8620310425758362
RMSE train: 0.698937	val: 4.051145	test: 3.152407
MAE train: 0.523951	val: 2.862392	test: 2.383725

Epoch: 89
Loss: 0.9858013391494751
RMSE train: 0.695688	val: 4.063239	test: 3.148987
MAE train: 0.521732	val: 2.874387	test: 2.389607

Epoch: 90
Loss: 0.9718677997589111
RMSE train: 0.649389	val: 4.033687	test: 3.122180
MAE train: 0.487024	val: 2.802913	test: 2.360125

Epoch: 91
Loss: 0.9628666043281555
RMSE train: 0.624309	val: 4.010822	test: 3.100914
MAE train: 0.465279	val: 2.696040	test: 2.321921

Epoch: 92
Loss: 0.9311221837997437
RMSE train: 0.652445	val: 4.021962	test: 3.116847
MAE train: 0.494016	val: 2.654253	test: 2.323579

Epoch: 93
Loss: 0.8262550532817841
RMSE train: 0.637248	val: 4.060919	test: 3.199526
MAE train: 0.477383	val: 2.713927	test: 2.407902

Epoch: 94
Loss: 0.9811842739582062
RMSE train: 0.723286	val: 4.128713	test: 3.321547
MAE train: 0.547233	val: 2.841331	test: 2.536683

Epoch: 95
Loss: 0.8871453702449799
RMSE train: 0.828639	val: 4.200453	test: 3.393403
MAE train: 0.616954	val: 2.950556	test: 2.599686

Epoch: 96
Loss: 0.9253560304641724
RMSE train: 0.901593	val: 4.259945	test: 3.416579
MAE train: 0.655343	val: 3.033322	test: 2.625124

Epoch: 97
Loss: 1.085154950618744
RMSE train: 0.903371	val: 4.237143	test: 3.397192
MAE train: 0.650285	val: 3.037934	test: 2.594027

Epoch: 98
Loss: 0.8518571853637695
RMSE train: 0.814997	val: 4.137197	test: 3.330344
MAE train: 0.589122	val: 2.963590	test: 2.515216

Epoch: 99
Loss: 0.9289227724075317
RMSE train: 0.722762	val: 4.066166	test: 3.284190
MAE train: 0.527212	val: 2.894350	test: 2.471315

Epoch: 100
Loss: 0.8678938448429108
RMSE train: 0.707034	val: 4.068415	test: 3.295512
MAE train: 0.517906	val: 2.863208	test: 2.504335

Epoch: 101
Loss: 1.0307170450687408
RMSE train: 0.767752	val: 4.110772	test: 3.332134
MAE train: 0.563106	val: 2.905206	test: 2.521233

Epoch: 102
Loss: 0.9678912162780762
RMSE train: 0.826754	val: 4.150932	test: 3.352543
MAE train: 0.617459	val: 2.920524	test: 2.542766

Epoch: 103
Loss: 0.821924477815628
RMSE train: 0.867158	val: 4.183040	test: 3.357583
MAE train: 0.647410	val: 2.913068	test: 2.549445

Epoch: 104
Loss: 0.8204613924026489
RMSE train: 0.821555	val: 4.160264	test: 3.357330
MAE train: 0.611193	val: 2.892003	test: 2.549862

Epoch: 105
Loss: 0.8273041546344757
RMSE train: 0.771985	val: 4.110404	test: 3.357295
MAE train: 0.568775	val: 2.844006	test: 2.561753

Epoch: 106
Loss: 0.8736744523048401
RMSE train: 0.754499	val: 4.081029	test: 3.376055
MAE train: 0.552218	val: 2.824967	test: 2.605444

Epoch: 107
Loss: 0.8418393731117249
RMSE train: 0.747754	val: 4.064368	test: 3.373452
MAE train: 0.546215	val: 2.824580	test: 2.602849

Epoch: 108
Loss: 0.7349667847156525
RMSE train: 0.761224	val: 4.068666	test: 3.384125
MAE train: 0.555264	val: 2.843149	test: 2.615874

Epoch: 109
Loss: 0.7719156444072723
RMSE train: 0.771070	val: 4.055788	test: 3.383108
MAE train: 0.562561	val: 2.850447	test: 2.629112

Epoch: 110
Loss: 0.9432756900787354
RMSE train: 0.737123	val: 4.024391	test: 3.347977
MAE train: 0.537916	val: 2.871648	test: 2.604565

Epoch: 111
Loss: 0.7860652208328247
RMSE train: 0.731734	val: 4.011221	test: 3.311487
MAE train: 0.534300	val: 2.870108	test: 2.576124

Epoch: 112
Loss: 0.8234768211841583
RMSE train: 0.745955	val: 4.034983	test: 3.318519
MAE train: 0.548387	val: 2.893046	test: 2.587880

Epoch: 113
Loss: 0.8436278700828552
RMSE train: 0.768425	val: 4.059937	test: 3.341592
MAE train: 0.570530	val: 2.912657	test: 2.613495

Epoch: 114
Loss: 0.723285973072052
RMSE train: 0.754068	val: 4.064672	test: 3.348629
MAE train: 0.564639	val: 2.920885	test: 2.628999

Epoch: 115
Loss: 0.7948872745037079
RMSE train: 0.741938	val: 4.073201	test: 3.369877
MAE train: 0.555919	val: 2.921563	test: 2.650993

Epoch: 116
Loss: 0.8030301034450531
RMSE train: 0.723512	val: 4.051846	test: 3.378499
MAE train: 0.541745	val: 2.844913	test: 2.651240

Epoch: 117
Loss: 0.7724825441837311
RMSE train: 0.734163	val: 4.057905	test: 3.383938
MAE train: 0.543424	val: 2.792293	test: 2.647717

Epoch: 118
Loss: 0.762956827878952
RMSE train: 0.756680	val: 4.072341	test: 3.409810
MAE train: 0.563704	val: 2.769754	test: 2.666820

Epoch: 119
Loss: 0.7727459073066711
RMSE train: 0.803817	val: 4.090013	test: 3.448356
MAE train: 0.603618	val: 2.781788	test: 2.715132

Epoch: 120
Loss: 0.7094918489456177
RMSE train: 0.841200	val: 4.088903	test: 3.466227
MAE train: 0.635827	val: 2.767228	test: 2.728839

Epoch: 121
Loss: 0.8060390949249268
RMSE train: 0.845197	val: 4.098367	test: 3.453720
MAE train: 0.648222	val: 2.779847	test: 2.694236

Early stopping
Best (RMSE):	 train: 0.771915	val: 3.883278	test: 3.210552
Best (MAE):	 train: 0.569124	val: 2.745999	test: 2.429170


Epoch: 84
Loss: 1.1301138401031494
RMSE train: 1.044449	val: 1.656888	test: 2.429035
MAE train: 0.759061	val: 1.094686	test: 1.746985

Epoch: 85
Loss: 1.0712648034095764
RMSE train: 0.985433	val: 1.585767	test: 2.399660
MAE train: 0.726779	val: 1.045584	test: 1.747194

Epoch: 86
Loss: 1.048400342464447
RMSE train: 0.947506	val: 1.519237	test: 2.375084
MAE train: 0.709940	val: 1.032565	test: 1.787080

Epoch: 87
Loss: 1.1287968754768372
RMSE train: 0.946814	val: 1.512680	test: 2.354687
MAE train: 0.717718	val: 1.036852	test: 1.795224

Epoch: 88
Loss: 1.1970625519752502
RMSE train: 0.944756	val: 1.540136	test: 2.333760
MAE train: 0.721885	val: 1.044902	test: 1.769736

Epoch: 89
Loss: 0.8433198630809784
RMSE train: 0.955895	val: 1.591928	test: 2.319202
MAE train: 0.728442	val: 1.071853	test: 1.745728

Epoch: 90
Loss: 0.9793129563331604
RMSE train: 0.938916	val: 1.597637	test: 2.282848
MAE train: 0.709817	val: 1.080732	test: 1.744815

Epoch: 91
Loss: 1.1747628450393677
RMSE train: 0.910080	val: 1.598955	test: 2.279644
MAE train: 0.679061	val: 1.086497	test: 1.757621

Epoch: 92
Loss: 0.9958776533603668
RMSE train: 0.858524	val: 1.559582	test: 2.280660
MAE train: 0.640432	val: 1.072848	test: 1.782146

Epoch: 93
Loss: 1.0654513835906982
RMSE train: 0.824183	val: 1.503241	test: 2.282823
MAE train: 0.609430	val: 1.065066	test: 1.794258

Epoch: 94
Loss: 0.9677677154541016
RMSE train: 0.750197	val: 1.446111	test: 2.258342
MAE train: 0.548457	val: 1.078758	test: 1.809693

Epoch: 95
Loss: 1.097434163093567
RMSE train: 0.703407	val: 1.439190	test: 2.244834
MAE train: 0.508846	val: 1.094666	test: 1.802120

Epoch: 96
Loss: 0.9687399864196777
RMSE train: 0.703452	val: 1.428717	test: 2.237165
MAE train: 0.510562	val: 1.076020	test: 1.786305

Epoch: 97
Loss: 1.5052499175071716
RMSE train: 0.738732	val: 1.409583	test: 2.243519
MAE train: 0.540408	val: 1.030767	test: 1.766520

Epoch: 98
Loss: 1.0254037082195282
RMSE train: 0.768645	val: 1.413957	test: 2.300524
MAE train: 0.565111	val: 0.994771	test: 1.788082

Epoch: 99
Loss: 0.9280140399932861
RMSE train: 0.787551	val: 1.437530	test: 2.347332
MAE train: 0.587817	val: 0.996792	test: 1.841675

Epoch: 100
Loss: 0.9164540767669678
RMSE train: 0.844709	val: 1.463496	test: 2.357240
MAE train: 0.628254	val: 1.008636	test: 1.842505

Epoch: 101
Loss: 0.8292105197906494
RMSE train: 0.896055	val: 1.471692	test: 2.330790
MAE train: 0.659483	val: 1.015458	test: 1.829186

Epoch: 102
Loss: 1.2473172545433044
RMSE train: 0.947021	val: 1.518199	test: 2.320501
MAE train: 0.696282	val: 1.033605	test: 1.806212

Epoch: 103
Loss: 1.1890270113945007
RMSE train: 0.920948	val: 1.571076	test: 2.349410
MAE train: 0.684389	val: 1.046475	test: 1.824748

Epoch: 104
Loss: 0.911127358675003
RMSE train: 0.925381	val: 1.575251	test: 2.352843
MAE train: 0.684919	val: 1.042295	test: 1.827096

Epoch: 105
Loss: 1.1320413947105408
RMSE train: 0.939709	val: 1.542850	test: 2.324422
MAE train: 0.702412	val: 1.021244	test: 1.791981

Epoch: 106
Loss: 1.3355555534362793
RMSE train: 0.894471	val: 1.498620	test: 2.281459
MAE train: 0.662777	val: 1.003011	test: 1.757175

Epoch: 107
Loss: 0.9495288133621216
RMSE train: 0.792773	val: 1.452637	test: 2.245217
MAE train: 0.586638	val: 0.986487	test: 1.739097

Epoch: 108
Loss: 0.9440810680389404
RMSE train: 0.768862	val: 1.464261	test: 2.256092
MAE train: 0.571215	val: 0.991903	test: 1.756938

Epoch: 109
Loss: 1.1314260959625244
RMSE train: 0.793749	val: 1.492314	test: 2.281129
MAE train: 0.594528	val: 1.005408	test: 1.775827

Epoch: 110
Loss: 1.2903814315795898
RMSE train: 0.864473	val: 1.528607	test: 2.329844
MAE train: 0.657140	val: 1.036959	test: 1.819969

Epoch: 111
Loss: 1.133619487285614
RMSE train: 0.889549	val: 1.568279	test: 2.352801
MAE train: 0.680029	val: 1.066301	test: 1.843284

Epoch: 112
Loss: 1.1933626532554626
RMSE train: 0.879579	val: 1.554837	test: 2.316244
MAE train: 0.672688	val: 1.049749	test: 1.807642

Epoch: 113
Loss: 0.9903247654438019
RMSE train: 0.856523	val: 1.526465	test: 2.268564
MAE train: 0.644000	val: 1.039178	test: 1.769107

Epoch: 114
Loss: 0.9135652184486389
RMSE train: 0.833867	val: 1.518075	test: 2.249397
MAE train: 0.623248	val: 1.059123	test: 1.753654

Epoch: 115
Loss: 0.9103566110134125
RMSE train: 0.806540	val: 1.504742	test: 2.226114
MAE train: 0.600206	val: 1.077210	test: 1.732451

Epoch: 116
Loss: 0.8600414097309113
RMSE train: 0.783741	val: 1.505077	test: 2.230322
MAE train: 0.588079	val: 1.090630	test: 1.742224

Epoch: 117
Loss: 0.9685599207878113
RMSE train: 0.799844	val: 1.535988	test: 2.234409
MAE train: 0.598330	val: 1.109909	test: 1.721033

Epoch: 118
Loss: 0.9254043698310852
RMSE train: 0.849669	val: 1.584952	test: 2.247999
MAE train: 0.625978	val: 1.137596	test: 1.700397

Epoch: 119
Loss: 0.7974343001842499
RMSE train: 0.826527	val: 1.567181	test: 2.210152
MAE train: 0.606435	val: 1.126644	test: 1.666292

Epoch: 120
Loss: 0.9729592800140381
RMSE train: 0.816526	val: 1.568057	test: 2.206230
MAE train: 0.599450	val: 1.119505	test: 1.661261

Epoch: 121
Loss: 0.9722511172294617
RMSE train: 0.805443	val: 1.535329	test: 2.210229
MAE train: 0.583616	val: 1.090503	test: 1.698444

Epoch: 122
Loss: 0.9497017860412598
RMSE train: 0.789846	val: 1.513216	test: 2.211172
MAE train: 0.571492	val: 1.068426	test: 1.703233

Epoch: 123
Loss: 0.9339767098426819
RMSE train: 0.791146	val: 1.499204	test: 2.229798
MAE train: 0.579471	val: 1.054899	test: 1.701127

Epoch: 124
Loss: 0.9399053454399109
RMSE train: 0.818471	val: 1.513440	test: 2.245357
MAE train: 0.605216	val: 1.080112	test: 1.708993

Epoch: 125
Loss: 0.7861737906932831
RMSE train: 0.810883	val: 1.532024	test: 2.245082
MAE train: 0.603876	val: 1.098250	test: 1.703869

Epoch: 126
Loss: 0.8490791618824005
RMSE train: 0.798854	val: 1.545730	test: 2.230043
MAE train: 0.600044	val: 1.113341	test: 1.685876

Epoch: 127
Loss: 0.8400937914848328
RMSE train: 0.788772	val: 1.554439	test: 2.211801
MAE train: 0.595664	val: 1.116115	test: 1.687408

Epoch: 128
Loss: 0.9833178520202637
RMSE train: 0.812672	val: 1.566215	test: 2.204567
MAE train: 0.616797	val: 1.116852	test: 1.675008

Epoch: 129
Loss: 0.7991187274456024
RMSE train: 0.811563	val: 1.585365	test: 2.211337
MAE train: 0.617278	val: 1.124088	test: 1.674181

Epoch: 130
Loss: 1.1286621689796448
RMSE train: 0.754871	val: 1.563823	test: 2.214806
MAE train: 0.577064	val: 1.101836	test: 1.685610

Epoch: 131
Loss: 0.7169976234436035
RMSE train: 0.755397	val: 1.561991	test: 2.209232
MAE train: 0.578803	val: 1.090527	test: 1.671926

Epoch: 132
Loss: 0.8063980340957642
RMSE train: 0.732391	val: 1.509821	test: 2.180755
MAE train: 0.554903	val: 1.053895	test: 1.652446

Early stopping
Best (RMSE):	 train: 0.738732	val: 1.409583	test: 2.243519
Best (MAE):	 train: 0.540408	val: 1.030767	test: 1.766520


Epoch: 84
Loss: 0.9137367308139801
RMSE train: 0.845139	val: 3.958671	test: 3.123679
MAE train: 0.601907	val: 2.703882	test: 2.350894

Epoch: 85
Loss: 0.9693345427513123
RMSE train: 0.805828	val: 3.934802	test: 3.137038
MAE train: 0.569213	val: 2.654303	test: 2.374012

Epoch: 86
Loss: 0.8428014516830444
RMSE train: 0.788759	val: 3.924501	test: 3.160518
MAE train: 0.557443	val: 2.621369	test: 2.403641

Epoch: 87
Loss: 0.9478423595428467
RMSE train: 0.737940	val: 3.885854	test: 3.144163
MAE train: 0.524485	val: 2.610793	test: 2.419131

Epoch: 88
Loss: 0.8524636626243591
RMSE train: 0.685790	val: 3.809750	test: 3.111758
MAE train: 0.491781	val: 2.550012	test: 2.408567

Epoch: 89
Loss: 0.8873068690299988
RMSE train: 0.667166	val: 3.742598	test: 3.061088
MAE train: 0.488478	val: 2.472839	test: 2.375894

Epoch: 90
Loss: 0.9840947985649109
RMSE train: 0.695394	val: 3.738993	test: 3.041337
MAE train: 0.517662	val: 2.454602	test: 2.358711

Epoch: 91
Loss: 0.8945520222187042
RMSE train: 0.734553	val: 3.832981	test: 3.085265
MAE train: 0.543206	val: 2.539269	test: 2.370941

Epoch: 92
Loss: 0.8024676740169525
RMSE train: 0.769403	val: 3.936369	test: 3.133086
MAE train: 0.559971	val: 2.638202	test: 2.385642

Epoch: 93
Loss: 0.8582606315612793
RMSE train: 0.780330	val: 3.966020	test: 3.161838
MAE train: 0.563423	val: 2.641294	test: 2.375132

Epoch: 94
Loss: 0.8330044448375702
RMSE train: 0.752373	val: 3.940971	test: 3.162437
MAE train: 0.533759	val: 2.614093	test: 2.350286

Epoch: 95
Loss: 0.8412762582302094
RMSE train: 0.710005	val: 3.860128	test: 3.118469
MAE train: 0.514727	val: 2.601404	test: 2.349145

Epoch: 96
Loss: 0.8104875385761261
RMSE train: 0.716648	val: 3.843536	test: 3.099871
MAE train: 0.524920	val: 2.617997	test: 2.367964

Epoch: 97
Loss: 1.0040526986122131
RMSE train: 0.732269	val: 3.871204	test: 3.089064
MAE train: 0.536769	val: 2.627023	test: 2.333014

Epoch: 98
Loss: 0.8729124069213867
RMSE train: 0.761262	val: 3.953912	test: 3.105912
MAE train: 0.567968	val: 2.678930	test: 2.318524

Epoch: 99
Loss: 0.9743894934654236
RMSE train: 0.820297	val: 4.037960	test: 3.114620
MAE train: 0.607733	val: 2.740213	test: 2.328040

Epoch: 100
Loss: 0.8511137962341309
RMSE train: 0.848125	val: 4.068497	test: 3.083735
MAE train: 0.612504	val: 2.728627	test: 2.275697

Epoch: 101
Loss: 0.8842452466487885
RMSE train: 0.842881	val: 4.042741	test: 3.064157
MAE train: 0.596102	val: 2.680606	test: 2.259453

Epoch: 102
Loss: 0.8911016583442688
RMSE train: 0.799306	val: 3.961325	test: 3.043904
MAE train: 0.563560	val: 2.617987	test: 2.265507

Epoch: 103
Loss: 0.8376276195049286
RMSE train: 0.716559	val: 3.867637	test: 3.008199
MAE train: 0.512947	val: 2.561209	test: 2.235078

Epoch: 104
Loss: 0.8853879868984222
RMSE train: 0.662130	val: 3.822244	test: 3.010371
MAE train: 0.473615	val: 2.514618	test: 2.242021

Epoch: 105
Loss: 0.839526891708374
RMSE train: 0.696878	val: 3.825454	test: 3.014495
MAE train: 0.499924	val: 2.527886	test: 2.264364

Epoch: 106
Loss: 0.8174998760223389
RMSE train: 0.713066	val: 3.786563	test: 3.007889
MAE train: 0.514293	val: 2.497582	test: 2.260892

Epoch: 107
Loss: 0.8167934417724609
RMSE train: 0.707695	val: 3.749901	test: 3.006123
MAE train: 0.515526	val: 2.452886	test: 2.255322

Epoch: 108
Loss: 0.892041027545929
RMSE train: 0.705720	val: 3.765655	test: 3.020420
MAE train: 0.516122	val: 2.501666	test: 2.276487

Epoch: 109
Loss: 0.8170954287052155
RMSE train: 0.738583	val: 3.819370	test: 3.002773
MAE train: 0.541246	val: 2.624299	test: 2.290461

Epoch: 110
Loss: 0.77278271317482
RMSE train: 0.718226	val: 3.817026	test: 3.005259
MAE train: 0.514635	val: 2.611591	test: 2.282320

Epoch: 111
Loss: 0.848402738571167
RMSE train: 0.679588	val: 3.809544	test: 3.024748
MAE train: 0.480129	val: 2.605463	test: 2.298702

Epoch: 112
Loss: 0.7203872203826904
RMSE train: 0.664679	val: 3.801706	test: 3.018059
MAE train: 0.467536	val: 2.586211	test: 2.289478

Epoch: 113
Loss: 0.8059620261192322
RMSE train: 0.665717	val: 3.804334	test: 2.990776
MAE train: 0.477453	val: 2.582144	test: 2.269685

Epoch: 114
Loss: 0.8713465929031372
RMSE train: 0.678109	val: 3.849087	test: 2.974763
MAE train: 0.499120	val: 2.606123	test: 2.253673

Epoch: 115
Loss: 0.730247288942337
RMSE train: 0.662451	val: 3.850660	test: 2.968538
MAE train: 0.492976	val: 2.600529	test: 2.262308

Epoch: 116
Loss: 0.77142533659935
RMSE train: 0.663598	val: 3.882260	test: 2.974343
MAE train: 0.490299	val: 2.615024	test: 2.258351

Epoch: 117
Loss: 0.6647846698760986
RMSE train: 0.678279	val: 3.904682	test: 2.986722
MAE train: 0.490451	val: 2.648343	test: 2.260128

Epoch: 118
Loss: 0.6649059057235718
RMSE train: 0.719393	val: 3.949107	test: 2.992572
MAE train: 0.512068	val: 2.660165	test: 2.229374

Epoch: 119
Loss: 0.6692265570163727
RMSE train: 0.706911	val: 3.931540	test: 2.973780
MAE train: 0.508014	val: 2.638385	test: 2.214768

Epoch: 120
Loss: 0.6982999742031097
RMSE train: 0.689048	val: 3.912043	test: 2.965563
MAE train: 0.504785	val: 2.621662	test: 2.221153

Epoch: 121
Loss: 0.7990928590297699
RMSE train: 0.675914	val: 3.868568	test: 2.962633
MAE train: 0.493829	val: 2.559692	test: 2.210884

Epoch: 122
Loss: 0.592786580324173
RMSE train: 0.673863	val: 3.837458	test: 2.947949
MAE train: 0.492664	val: 2.494493	test: 2.172101

Epoch: 123
Loss: 0.8564141094684601
RMSE train: 0.678855	val: 3.851036	test: 2.924548
MAE train: 0.504667	val: 2.505660	test: 2.137046

Epoch: 124
Loss: 0.7389647364616394
RMSE train: 0.722512	val: 3.929238	test: 2.903243
MAE train: 0.540484	val: 2.561868	test: 2.120942

Epoch: 125
Loss: 0.7663854658603668
RMSE train: 0.788803	val: 4.035736	test: 2.923839
MAE train: 0.571414	val: 2.677250	test: 2.158614

Early stopping
Best (RMSE):	 train: 0.695394	val: 3.738993	test: 3.041337
Best (MAE):	 train: 0.517662	val: 2.454602	test: 2.358711


Epoch: 84
Loss: 0.8848299980163574
RMSE train: 0.847461	val: 1.765445	test: 2.600595
MAE train: 0.606456	val: 1.332880	test: 2.024553

Epoch: 85
Loss: 1.2766900658607483
RMSE train: 0.865064	val: 1.743158	test: 2.618643
MAE train: 0.623401	val: 1.293114	test: 2.037684

Epoch: 86
Loss: 1.2746533751487732
RMSE train: 0.923019	val: 1.763736	test: 2.642242
MAE train: 0.666540	val: 1.317473	test: 2.054166

Epoch: 87
Loss: 1.111040621995926
RMSE train: 0.934148	val: 1.823350	test: 2.651218
MAE train: 0.679834	val: 1.391516	test: 2.063716

Epoch: 88
Loss: 1.0784401893615723
RMSE train: 0.907347	val: 1.828272	test: 2.679631
MAE train: 0.660288	val: 1.391466	test: 2.098261

Epoch: 89
Loss: 1.1105031371116638
RMSE train: 0.868823	val: 1.858132	test: 2.653960
MAE train: 0.633314	val: 1.414464	test: 2.093858

Epoch: 90
Loss: 0.863136500120163
RMSE train: 0.859641	val: 1.856518	test: 2.636514
MAE train: 0.620966	val: 1.405169	test: 2.100340

Epoch: 91
Loss: 1.1229219436645508
RMSE train: 0.839053	val: 1.853248	test: 2.639609
MAE train: 0.603010	val: 1.400177	test: 2.110433

Epoch: 92
Loss: 1.262424886226654
RMSE train: 0.801179	val: 1.800293	test: 2.618683
MAE train: 0.572599	val: 1.357092	test: 2.092839

Epoch: 93
Loss: 1.2658701539039612
RMSE train: 0.789901	val: 1.764381	test: 2.579330
MAE train: 0.562931	val: 1.331134	test: 2.054077

Epoch: 94
Loss: 1.01473730802536
RMSE train: 0.778106	val: 1.730915	test: 2.554226
MAE train: 0.548212	val: 1.303131	test: 2.019254

Epoch: 95
Loss: 1.0852250456809998
RMSE train: 0.790879	val: 1.713851	test: 2.542909
MAE train: 0.541200	val: 1.292999	test: 1.998791

Epoch: 96
Loss: 1.013828068971634
RMSE train: 0.795563	val: 1.718107	test: 2.549143
MAE train: 0.545987	val: 1.306054	test: 1.985139

Epoch: 97
Loss: 1.1652106642723083
RMSE train: 0.803721	val: 1.728119	test: 2.603479
MAE train: 0.558218	val: 1.315721	test: 2.023151

Epoch: 98
Loss: 1.0325660109519958
RMSE train: 0.802913	val: 1.754868	test: 2.626680
MAE train: 0.576505	val: 1.335637	test: 2.045560

Epoch: 99
Loss: 1.1011843085289001
RMSE train: 0.808100	val: 1.728333	test: 2.659301
MAE train: 0.589942	val: 1.299062	test: 2.093266

Epoch: 100
Loss: 1.054450899362564
RMSE train: 0.824393	val: 1.685950	test: 2.658909
MAE train: 0.606127	val: 1.245707	test: 2.120963

Epoch: 101
Loss: 1.013571172952652
RMSE train: 0.836649	val: 1.672538	test: 2.664005
MAE train: 0.607499	val: 1.219890	test: 2.141115

Epoch: 102
Loss: 0.8773394227027893
RMSE train: 0.847507	val: 1.659386	test: 2.666711
MAE train: 0.610619	val: 1.206518	test: 2.141934

Epoch: 103
Loss: 0.9350615739822388
RMSE train: 0.822881	val: 1.649096	test: 2.664227
MAE train: 0.589845	val: 1.192223	test: 2.138716

Epoch: 104
Loss: 0.963470995426178
RMSE train: 0.828089	val: 1.650803	test: 2.610539
MAE train: 0.590358	val: 1.198510	test: 2.079359

Epoch: 105
Loss: 1.0106006860733032
RMSE train: 0.797624	val: 1.663665	test: 2.590858
MAE train: 0.582607	val: 1.218472	test: 2.049412

Epoch: 106
Loss: 0.8380138278007507
RMSE train: 0.761415	val: 1.664117	test: 2.653579
MAE train: 0.556310	val: 1.220881	test: 2.081031

Epoch: 107
Loss: 0.7913040816783905
RMSE train: 0.775819	val: 1.665923	test: 2.706339
MAE train: 0.568414	val: 1.223170	test: 2.115248

Epoch: 108
Loss: 0.8989748954772949
RMSE train: 0.808566	val: 1.664732	test: 2.701546
MAE train: 0.589014	val: 1.216694	test: 2.126426

Epoch: 109
Loss: 0.8321492671966553
RMSE train: 0.838750	val: 1.653536	test: 2.680741
MAE train: 0.614704	val: 1.205001	test: 2.129568

Epoch: 110
Loss: 1.0037006735801697
RMSE train: 0.919393	val: 1.639717	test: 2.623309
MAE train: 0.668800	val: 1.192575	test: 2.095621

Epoch: 111
Loss: 0.9463832378387451
RMSE train: 0.933381	val: 1.622838	test: 2.615025
MAE train: 0.671072	val: 1.178061	test: 2.076483

Epoch: 112
Loss: 1.3029348850250244
RMSE train: 0.878906	val: 1.596555	test: 2.639586
MAE train: 0.634039	val: 1.167154	test: 2.077091

Epoch: 113
Loss: 1.0119145512580872
RMSE train: 0.738692	val: 1.570581	test: 2.652858
MAE train: 0.529827	val: 1.161379	test: 2.099390

Epoch: 114
Loss: 0.9489290714263916
RMSE train: 0.703743	val: 1.600876	test: 2.731911
MAE train: 0.494269	val: 1.207962	test: 2.144341

Epoch: 115
Loss: 0.9774903655052185
RMSE train: 0.726851	val: 1.650528	test: 2.772370
MAE train: 0.507944	val: 1.268474	test: 2.158212

Epoch: 116
Loss: 0.9296525418758392
RMSE train: 0.756574	val: 1.682260	test: 2.735747
MAE train: 0.541997	val: 1.291091	test: 2.134509

Epoch: 117
Loss: 0.7522911429405212
RMSE train: 0.820532	val: 1.673869	test: 2.680742
MAE train: 0.596618	val: 1.254136	test: 2.110901

Epoch: 118
Loss: 0.8866262137889862
RMSE train: 0.881800	val: 1.622881	test: 2.676404
MAE train: 0.648401	val: 1.184804	test: 2.103675

Epoch: 119
Loss: 0.8821207582950592
RMSE train: 0.909336	val: 1.620422	test: 2.693015
MAE train: 0.667041	val: 1.163560	test: 2.126670

Epoch: 120
Loss: 0.8389557003974915
RMSE train: 0.876525	val: 1.615923	test: 2.717543
MAE train: 0.642619	val: 1.155118	test: 2.141088

Epoch: 121
Loss: 0.8144344389438629
RMSE train: 0.806508	val: 1.598696	test: 2.687028
MAE train: 0.587487	val: 1.178110	test: 2.103074

Epoch: 122
Loss: 0.8604467511177063
RMSE train: 0.743277	val: 1.627407	test: 2.632012
MAE train: 0.536069	val: 1.265843	test: 2.055329

Epoch: 123
Loss: 0.7068269848823547
RMSE train: 0.747564	val: 1.669066	test: 2.596226
MAE train: 0.526990	val: 1.333627	test: 2.010077

Epoch: 124
Loss: 0.792694628238678
RMSE train: 0.775970	val: 1.674535	test: 2.581486
MAE train: 0.539440	val: 1.342704	test: 1.983309

Epoch: 125
Loss: 0.8914218544960022
RMSE train: 0.765973	val: 1.636684	test: 2.608278
MAE train: 0.550054	val: 1.284118	test: 2.008278

Epoch: 126
Loss: 0.8290521502494812
RMSE train: 0.748568	val: 1.616093	test: 2.668132
MAE train: 0.550091	val: 1.230531	test: 2.076597

Epoch: 127
Loss: 0.8298777043819427
RMSE train: 0.693784	val: 1.601383	test: 2.733257
MAE train: 0.504184	val: 1.193484	test: 2.143627

Epoch: 128
Loss: 0.8952421247959137
RMSE train: 0.665934	val: 1.600504	test: 2.770784
MAE train: 0.476308	val: 1.189681	test: 2.163530

Epoch: 129
Loss: 0.7830771207809448
RMSE train: 0.627140	val: 1.630827	test: 2.748027
MAE train: 0.444692	val: 1.235095	test: 2.152967

Epoch: 130
Loss: 0.9008162319660187
RMSE train: 0.634290	val: 1.651736	test: 2.636603
MAE train: 0.448882	val: 1.266824	test: 2.085030

Epoch: 131
Loss: 1.0692473351955414
RMSE train: 0.660694	val: 1.626647	test: 2.584387
MAE train: 0.462765	val: 1.242722	test: 2.052304

Epoch: 132
Loss: 0.87481689453125
RMSE train: 0.712709	val: 1.586633	test: 2.579192
MAE train: 0.505952	val: 1.189497	test: 2.031020

Epoch: 133
Loss: 1.2538806200027466
RMSE train: 0.758151	val: 1.581863	test: 2.604790
MAE train: 0.550086	val: 1.157111	test: 2.046755

Epoch: 134
Loss: 0.8929052650928497
RMSE train: 0.738240	val: 1.599872	test: 2.690337
MAE train: 0.553328	val: 1.153415	test: 2.109602

Epoch: 135
Loss: 0.7214105725288391
RMSE train: 0.771239	val: 1.628849	test: 2.779252
MAE train: 0.581624	val: 1.158748	test: 2.168167

Epoch: 136
Loss: 0.7000855803489685
RMSE train: 0.828430	val: 1.605845	test: 2.818444
MAE train: 0.634479	val: 1.130760	test: 2.176518

Epoch: 137
Loss: 1.0026387870311737
RMSE train: 0.818498	val: 1.571444	test: 2.727797
MAE train: 0.623208	val: 1.118884	test: 2.106089

Epoch: 138
Loss: 0.7750697433948517
RMSE train: 0.786540	val: 1.575869	test: 2.677966
MAE train: 0.589515	val: 1.148591	test: 2.049583

Epoch: 139
Loss: 0.6173775792121887
RMSE train: 0.792450	val: 1.583960	test: 2.655875
MAE train: 0.572112	val: 1.188446	test: 2.026254

Epoch: 140
Loss: 0.7206216156482697
RMSE train: 0.773587	val: 1.573007	test: 2.643573
MAE train: 0.539460	val: 1.189365	test: 2.021973

Epoch: 141
Loss: 0.8885783553123474
RMSE train: 0.747225	val: 1.579360	test: 2.617641
MAE train: 0.525228	val: 1.207269	test: 2.011063

Epoch: 142
Loss: 0.8588422536849976
RMSE train: 0.644570	val: 1.598192	test: 2.570618
MAE train: 0.460487	val: 1.229725	test: 1.993374

Epoch: 143
Loss: 0.671649694442749
RMSE train: 0.539811	val: 1.624037	test: 2.608352
MAE train: 0.394451	val: 1.254773	test: 2.040154

Epoch: 144
Loss: 0.8309799432754517
RMSE train: 0.555771	val: 1.605308	test: 2.584975
MAE train: 0.398828	val: 1.227563	test: 2.035586

Epoch: 145
Loss: 0.663505494594574
RMSE train: 0.608016	val: 1.575436	test: 2.542073
MAE train: 0.451588	val: 1.187472	test: 2.000060

Epoch: 146
Loss: 0.698673814535141
RMSE train: 0.644236	val: 1.576853	test: 2.520373
MAE train: 0.478074	val: 1.189504	test: 1.963366

Epoch: 147
Loss: 0.8376589417457581
RMSE train: 0.635096	val: 1.627942	test: 2.518873
MAE train: 0.469271	val: 1.258970	test: 1.956748

Epoch: 148
Loss: 0.8130519390106201
RMSE train: 0.639291	val: 1.728328	test: 2.510434
MAE train: 0.465707	val: 1.371379	test: 1.968932

Early stopping
Best (RMSE):	 train: 0.738692	val: 1.570581	test: 2.652858
Best (MAE):	 train: 0.529827	val: 1.161379	test: 2.099390


Epoch: 84
Loss: 1.2278953194618225
RMSE train: 1.040977	val: 1.607367	test: 2.763477
MAE train: 0.757152	val: 1.153511	test: 1.986654

Epoch: 85
Loss: 1.2228271961212158
RMSE train: 1.007013	val: 1.600487	test: 2.761838
MAE train: 0.733157	val: 1.134378	test: 1.970805

Epoch: 86
Loss: 1.0822028517723083
RMSE train: 1.000201	val: 1.604287	test: 2.759562
MAE train: 0.736765	val: 1.126161	test: 1.944829

Epoch: 87
Loss: 0.9532995223999023
RMSE train: 0.980692	val: 1.606059	test: 2.741605
MAE train: 0.722648	val: 1.133894	test: 1.933037

Epoch: 88
Loss: 0.9043807089328766
RMSE train: 0.954429	val: 1.614495	test: 2.704873
MAE train: 0.695131	val: 1.162146	test: 1.912503

Epoch: 89
Loss: 1.0646218657493591
RMSE train: 0.953856	val: 1.632466	test: 2.672317
MAE train: 0.691757	val: 1.199505	test: 1.882697

Epoch: 90
Loss: 1.2987506985664368
RMSE train: 0.897987	val: 1.630049	test: 2.674831
MAE train: 0.645063	val: 1.203767	test: 1.877675

Epoch: 91
Loss: 0.9673312604427338
RMSE train: 0.906286	val: 1.609993	test: 2.709192
MAE train: 0.639275	val: 1.164221	test: 1.887338

Epoch: 92
Loss: 0.8883163928985596
RMSE train: 0.924189	val: 1.601877	test: 2.739054
MAE train: 0.641323	val: 1.144377	test: 1.909102

Epoch: 93
Loss: 0.8563232719898224
RMSE train: 0.908206	val: 1.584827	test: 2.725753
MAE train: 0.638321	val: 1.134427	test: 1.889881

Epoch: 94
Loss: 1.096391201019287
RMSE train: 0.885763	val: 1.578462	test: 2.702532
MAE train: 0.636934	val: 1.131005	test: 1.879959

Epoch: 95
Loss: 0.9958407282829285
RMSE train: 0.872708	val: 1.590770	test: 2.643543
MAE train: 0.633365	val: 1.154352	test: 1.845206

Epoch: 96
Loss: 0.8639091551303864
RMSE train: 0.890032	val: 1.584107	test: 2.618615
MAE train: 0.644055	val: 1.139366	test: 1.833884

Epoch: 97
Loss: 0.9230360686779022
RMSE train: 0.876951	val: 1.588237	test: 2.593014
MAE train: 0.633986	val: 1.143703	test: 1.824441

Epoch: 98
Loss: 0.9760935008525848
RMSE train: 0.849357	val: 1.585677	test: 2.591690
MAE train: 0.607120	val: 1.137286	test: 1.829549

Epoch: 99
Loss: 0.9024451971054077
RMSE train: 0.868641	val: 1.588160	test: 2.614175
MAE train: 0.615499	val: 1.130814	test: 1.837485

Epoch: 100
Loss: 0.845268189907074
RMSE train: 0.880459	val: 1.584776	test: 2.644229
MAE train: 0.628834	val: 1.129116	test: 1.866714

Epoch: 101
Loss: 0.881656140089035
RMSE train: 0.862144	val: 1.571152	test: 2.633918
MAE train: 0.621085	val: 1.127899	test: 1.869933

Epoch: 102
Loss: 0.8160003125667572
RMSE train: 0.866758	val: 1.581791	test: 2.620241
MAE train: 0.628208	val: 1.160669	test: 1.903157

Epoch: 103
Loss: 1.0879821479320526
RMSE train: 0.854337	val: 1.579528	test: 2.618334
MAE train: 0.614671	val: 1.169275	test: 1.917128

Epoch: 104
Loss: 0.7831744849681854
RMSE train: 0.868941	val: 1.575133	test: 2.616070
MAE train: 0.619641	val: 1.155811	test: 1.912229

Epoch: 105
Loss: 1.0114631950855255
RMSE train: 0.866443	val: 1.573549	test: 2.662175
MAE train: 0.611436	val: 1.143356	test: 1.941043

Epoch: 106
Loss: 0.8186374008655548
RMSE train: 0.871449	val: 1.570346	test: 2.688161
MAE train: 0.611947	val: 1.142924	test: 1.975835

Epoch: 107
Loss: 0.8251270055770874
RMSE train: 0.874219	val: 1.563879	test: 2.691200
MAE train: 0.611911	val: 1.153459	test: 1.998981

Epoch: 108
Loss: 0.8931764960289001
RMSE train: 0.876379	val: 1.566795	test: 2.688391
MAE train: 0.613215	val: 1.166644	test: 1.996224

Epoch: 109
Loss: 0.9727934300899506
RMSE train: 0.883167	val: 1.569570	test: 2.667140
MAE train: 0.617440	val: 1.172734	test: 1.959196

Epoch: 110
Loss: 1.201779544353485
RMSE train: 0.878528	val: 1.576072	test: 2.671043
MAE train: 0.614228	val: 1.179005	test: 1.937505

Epoch: 111
Loss: 0.8975502252578735
RMSE train: 0.928193	val: 1.604928	test: 2.657203
MAE train: 0.659028	val: 1.214436	test: 1.884377

Epoch: 112
Loss: 0.9449919164180756
RMSE train: 0.943880	val: 1.630790	test: 2.648365
MAE train: 0.674297	val: 1.242786	test: 1.847550

Epoch: 113
Loss: 0.7285315990447998
RMSE train: 0.921314	val: 1.640910	test: 2.641508
MAE train: 0.661116	val: 1.256011	test: 1.849160

Epoch: 114
Loss: 0.9278002679347992
RMSE train: 0.914694	val: 1.607681	test: 2.604696
MAE train: 0.661203	val: 1.213913	test: 1.846629

Epoch: 115
Loss: 0.8798578083515167
RMSE train: 0.966553	val: 1.575008	test: 2.597945
MAE train: 0.694962	val: 1.173432	test: 1.833144

Epoch: 116
Loss: 0.8099928498268127
RMSE train: 1.018699	val: 1.553064	test: 2.636777
MAE train: 0.721105	val: 1.122232	test: 1.839893

Epoch: 117
Loss: 0.849473387002945
RMSE train: 1.051871	val: 1.555733	test: 2.680141
MAE train: 0.723588	val: 1.104199	test: 1.849568

Epoch: 118
Loss: 0.9792144894599915
RMSE train: 0.945738	val: 1.559131	test: 2.689574
MAE train: 0.664653	val: 1.105977	test: 1.849673

Epoch: 119
Loss: 0.8486076891422272
RMSE train: 0.837178	val: 1.574584	test: 2.692500
MAE train: 0.606022	val: 1.128002	test: 1.882242

Epoch: 120
Loss: 0.7897239029407501
RMSE train: 0.805746	val: 1.600607	test: 2.703326
MAE train: 0.592690	val: 1.165868	test: 1.894134

Epoch: 121
Loss: 0.9356914758682251
RMSE train: 0.788030	val: 1.618867	test: 2.722575
MAE train: 0.583970	val: 1.195287	test: 1.925768

Epoch: 122
Loss: 0.8225687444210052
RMSE train: 0.817359	val: 1.632999	test: 2.697441
MAE train: 0.608817	val: 1.219573	test: 1.896309

Epoch: 123
Loss: 0.8188762366771698
RMSE train: 0.866669	val: 1.632571	test: 2.685734
MAE train: 0.639588	val: 1.224174	test: 1.879982

Epoch: 124
Loss: 0.6742874383926392
RMSE train: 0.867179	val: 1.626909	test: 2.688833
MAE train: 0.628988	val: 1.225582	test: 1.882378

Epoch: 125
Loss: 0.8135546147823334
RMSE train: 0.856734	val: 1.651176	test: 2.676518
MAE train: 0.610999	val: 1.261323	test: 1.874476

Epoch: 126
Loss: 0.9332680702209473
RMSE train: 0.831819	val: 1.651252	test: 2.673605
MAE train: 0.584434	val: 1.270935	test: 1.860979

Epoch: 127
Loss: 0.7744536995887756
RMSE train: 0.759068	val: 1.691151	test: 2.679821
MAE train: 0.533523	val: 1.313108	test: 1.885520

Epoch: 128
Loss: 0.7973468899726868
RMSE train: 0.698153	val: 1.734378	test: 2.698957
MAE train: 0.484040	val: 1.355909	test: 1.937178

Epoch: 129
Loss: 0.8773233592510223
RMSE train: 0.697323	val: 1.722145	test: 2.728110
MAE train: 0.487443	val: 1.337332	test: 1.999213

Epoch: 130
Loss: 0.8745081126689911
RMSE train: 0.736145	val: 1.666475	test: 2.688157
MAE train: 0.525213	val: 1.282282	test: 1.949835

Epoch: 131
Loss: 0.7461249828338623
RMSE train: 0.838942	val: 1.604739	test: 2.664463
MAE train: 0.606433	val: 1.217361	test: 1.874443

Epoch: 132
Loss: 1.0199478268623352
RMSE train: 0.931175	val: 1.575786	test: 2.660013
MAE train: 0.657224	val: 1.188954	test: 1.834896

Epoch: 133
Loss: 0.8916247189044952
RMSE train: 0.972001	val: 1.577868	test: 2.638963
MAE train: 0.668779	val: 1.203691	test: 1.813245

Epoch: 134
Loss: 0.9135859310626984
RMSE train: 0.911100	val: 1.620911	test: 2.609834
MAE train: 0.620199	val: 1.263091	test: 1.817778

Epoch: 135
Loss: 0.928057849407196
RMSE train: 0.814597	val: 1.705117	test: 2.611583
MAE train: 0.570512	val: 1.342564	test: 1.900596

Epoch: 136
Loss: 0.8229504823684692
RMSE train: 0.791990	val: 1.726750	test: 2.638825
MAE train: 0.573267	val: 1.357842	test: 1.943107

Epoch: 137
Loss: 0.8940390944480896
RMSE train: 0.817600	val: 1.655256	test: 2.658167
MAE train: 0.600095	val: 1.280284	test: 1.966249

Epoch: 138
Loss: 0.7864438891410828
RMSE train: 0.813199	val: 1.558521	test: 2.683682
MAE train: 0.591912	val: 1.164055	test: 1.969463

Epoch: 139
Loss: 1.0072274208068848
RMSE train: 0.820883	val: 1.535299	test: 2.684081
MAE train: 0.587321	val: 1.139570	test: 1.974756

Epoch: 140
Loss: 0.8323400020599365
RMSE train: 0.806658	val: 1.572744	test: 2.641941
MAE train: 0.571151	val: 1.191219	test: 1.950775

Epoch: 141
Loss: 1.2266936898231506
RMSE train: 0.817625	val: 1.619119	test: 2.620917
MAE train: 0.570121	val: 1.245043	test: 1.939161

Epoch: 142
Loss: 1.0472816824913025
RMSE train: 0.826330	val: 1.602929	test: 2.609985
MAE train: 0.571847	val: 1.230562	test: 1.912900

Epoch: 143
Loss: 0.7571147680282593
RMSE train: 0.831900	val: 1.564525	test: 2.637333
MAE train: 0.576809	val: 1.188508	test: 1.920658

Epoch: 144
Loss: 0.7958638966083527
RMSE train: 0.862523	val: 1.539955	test: 2.653846

Epoch: 84
Loss: 1.0302894413471222
RMSE train: 0.829514	val: 4.156270	test: 3.035863
MAE train: 0.622199	val: 2.900714	test: 2.185800

Epoch: 85
Loss: 0.9717861413955688
RMSE train: 0.825947	val: 4.109033	test: 3.049686
MAE train: 0.626150	val: 2.873553	test: 2.222533

Epoch: 86
Loss: 0.8809167444705963
RMSE train: 0.879626	val: 4.138384	test: 3.099562
MAE train: 0.671636	val: 2.893823	test: 2.283040

Epoch: 87
Loss: 0.9566179811954498
RMSE train: 0.944018	val: 4.230378	test: 3.137502
MAE train: 0.720799	val: 2.968763	test: 2.321449

Epoch: 88
Loss: 0.820137232542038
RMSE train: 0.995191	val: 4.343900	test: 3.163535
MAE train: 0.760748	val: 3.045963	test: 2.334248

Epoch: 89
Loss: 0.9751091599464417
RMSE train: 0.981915	val: 4.390253	test: 3.144125
MAE train: 0.743776	val: 3.080004	test: 2.310430

Epoch: 90
Loss: 1.0760736465454102
RMSE train: 0.904816	val: 4.346090	test: 3.084001
MAE train: 0.675637	val: 3.021078	test: 2.233274

Epoch: 91
Loss: 0.8825850188732147
RMSE train: 0.823206	val: 4.274294	test: 3.028866
MAE train: 0.607110	val: 2.956289	test: 2.164825

Epoch: 92
Loss: 0.8797780573368073
RMSE train: 0.738725	val: 4.173229	test: 2.993450
MAE train: 0.547411	val: 2.906811	test: 2.132454

Epoch: 93
Loss: 0.7543498575687408
RMSE train: 0.667045	val: 4.087844	test: 2.986552
MAE train: 0.500180	val: 2.889898	test: 2.145236

Epoch: 94
Loss: 0.8190864324569702
RMSE train: 0.677097	val: 4.085335	test: 3.002264
MAE train: 0.514688	val: 2.907887	test: 2.175287

Epoch: 95
Loss: 0.8841372728347778
RMSE train: 0.726779	val: 4.144629	test: 3.047638
MAE train: 0.557383	val: 2.960535	test: 2.224145

Epoch: 96
Loss: 0.8484598100185394
RMSE train: 0.782157	val: 4.206284	test: 3.091457
MAE train: 0.596546	val: 3.004351	test: 2.257983

Epoch: 97
Loss: 0.7839482724666595
RMSE train: 0.823075	val: 4.266640	test: 3.105160
MAE train: 0.623214	val: 3.034965	test: 2.248068

Epoch: 98
Loss: 0.8492003977298737
RMSE train: 0.780842	val: 4.218379	test: 3.070146
MAE train: 0.584910	val: 2.996705	test: 2.199510

Epoch: 99
Loss: 0.7977246344089508
RMSE train: 0.711184	val: 4.132154	test: 3.018454
MAE train: 0.528960	val: 2.919344	test: 2.168065

Epoch: 100
Loss: 0.8217684030532837
RMSE train: 0.661830	val: 4.078130	test: 2.991698
MAE train: 0.490301	val: 2.867611	test: 2.159598

Epoch: 101
Loss: 0.7895101010799408
RMSE train: 0.651456	val: 4.063690	test: 2.988186
MAE train: 0.481167	val: 2.842331	test: 2.156886

Epoch: 102
Loss: 0.8039361834526062
RMSE train: 0.696491	val: 4.112897	test: 3.002400
MAE train: 0.526826	val: 2.859534	test: 2.154139

Epoch: 103
Loss: 0.9250202178955078
RMSE train: 0.734630	val: 4.169641	test: 3.018208
MAE train: 0.564754	val: 2.892013	test: 2.170104

Epoch: 104
Loss: 0.7763113677501678
RMSE train: 0.744671	val: 4.175668	test: 3.015537
MAE train: 0.569721	val: 2.889904	test: 2.163820

Epoch: 105
Loss: 0.7568119168281555
RMSE train: 0.748992	val: 4.154171	test: 3.024609
MAE train: 0.574800	val: 2.874428	test: 2.155462

Epoch: 106
Loss: 0.8538247346878052
RMSE train: 0.773431	val: 4.154051	test: 3.029326
MAE train: 0.583009	val: 2.865708	test: 2.157398

Epoch: 107
Loss: 0.7629415392875671
RMSE train: 0.774928	val: 4.162118	test: 3.012749
MAE train: 0.577448	val: 2.874412	test: 2.152463

Epoch: 108
Loss: 0.6881644129753113
RMSE train: 0.753165	val: 4.157761	test: 2.997251
MAE train: 0.564798	val: 2.882306	test: 2.153532

Epoch: 109
Loss: 0.883804589509964
RMSE train: 0.781902	val: 4.187396	test: 3.004603
MAE train: 0.590627	val: 2.907491	test: 2.163936

Epoch: 110
Loss: 0.8200522661209106
RMSE train: 0.825934	val: 4.247573	test: 3.031297
MAE train: 0.620211	val: 2.928725	test: 2.181883

Epoch: 111
Loss: 0.8195522725582123
RMSE train: 0.881352	val: 4.337944	test: 3.053887
MAE train: 0.660910	val: 2.959207	test: 2.181858

Epoch: 112
Loss: 0.8403190076351166
RMSE train: 0.888002	val: 4.366751	test: 3.048463
MAE train: 0.665336	val: 2.960680	test: 2.171401

Epoch: 113
Loss: 0.8074414730072021
RMSE train: 0.864860	val: 4.341389	test: 3.036488
MAE train: 0.642858	val: 2.952685	test: 2.175535

Epoch: 114
Loss: 0.8339069783687592
RMSE train: 0.795678	val: 4.268225	test: 3.009728
MAE train: 0.589300	val: 2.931178	test: 2.172387

Epoch: 115
Loss: 0.6500613391399384
RMSE train: 0.700569	val: 4.172609	test: 2.965660
MAE train: 0.522327	val: 2.907125	test: 2.154201

Epoch: 116
Loss: 0.8472374677658081
RMSE train: 0.623058	val: 4.129328	test: 2.943503
MAE train: 0.467768	val: 2.888773	test: 2.125903

Epoch: 117
Loss: 0.8388156294822693
RMSE train: 0.628283	val: 4.153294	test: 2.947530
MAE train: 0.469721	val: 2.888673	test: 2.111975

Epoch: 118
Loss: 0.8030889928340912
RMSE train: 0.724400	val: 4.234300	test: 2.987617
MAE train: 0.549146	val: 2.912271	test: 2.116174

Epoch: 119
Loss: 0.7168787121772766
RMSE train: 0.753829	val: 4.271400	test: 3.003261
MAE train: 0.574990	val: 2.929268	test: 2.137148

Epoch: 120
Loss: 0.7845460176467896
RMSE train: 0.714535	val: 4.199682	test: 2.988139
MAE train: 0.549993	val: 2.888026	test: 2.161780

Epoch: 121
Loss: 0.7417310476303101
RMSE train: 0.691852	val: 4.089111	test: 2.961361
MAE train: 0.532036	val: 2.817399	test: 2.175476

Epoch: 122
Loss: 0.7828832268714905
RMSE train: 0.691437	val: 4.054427	test: 2.959770
MAE train: 0.531452	val: 2.795942	test: 2.180487

Epoch: 123
Loss: 0.6677591800689697
RMSE train: 0.697184	val: 4.092544	test: 2.973269
MAE train: 0.534802	val: 2.817480	test: 2.164645

Epoch: 124
Loss: 0.6755157113075256
RMSE train: 0.779175	val: 4.169601	test: 3.015886
MAE train: 0.592908	val: 2.862771	test: 2.160638

Epoch: 125
Loss: 0.7929979860782623
RMSE train: 0.842960	val: 4.217050	test: 3.034494
MAE train: 0.636491	val: 2.901647	test: 2.171703

Epoch: 126
Loss: 0.7324905395507812
RMSE train: 0.815458	val: 4.201105	test: 3.013250
MAE train: 0.612963	val: 2.910117	test: 2.169860

Epoch: 127
Loss: 0.6398876905441284
RMSE train: 0.791144	val: 4.161669	test: 2.969752
MAE train: 0.598647	val: 2.885071	test: 2.155786

Epoch: 128
Loss: 0.8681541681289673
RMSE train: 0.829930	val: 4.172401	test: 2.939072
MAE train: 0.621204	val: 2.861825	test: 2.131882

Epoch: 129
Loss: 0.7285455465316772
RMSE train: 0.811545	val: 4.200792	test: 2.924390
MAE train: 0.600504	val: 2.856425	test: 2.083902

Epoch: 130
Loss: 0.6985348463058472
RMSE train: 0.810649	val: 4.216009	test: 2.915846
MAE train: 0.601115	val: 2.859826	test: 2.043019

Epoch: 131
Loss: 0.7417119741439819
RMSE train: 0.764776	val: 4.157494	test: 2.889561
MAE train: 0.579826	val: 2.829622	test: 2.010801

Epoch: 132
Loss: 0.8138837218284607
RMSE train: 0.649772	val: 4.035622	test: 2.846133
MAE train: 0.502853	val: 2.770443	test: 1.986983

Epoch: 133
Loss: 0.6194010674953461
RMSE train: 0.608112	val: 4.009489	test: 2.849181
MAE train: 0.474601	val: 2.778971	test: 2.008799

Epoch: 134
Loss: 0.6923784017562866
RMSE train: 0.625588	val: 4.034079	test: 2.867964
MAE train: 0.491333	val: 2.806232	test: 2.049069

Epoch: 135
Loss: 0.6643199026584625
RMSE train: 0.663692	val: 4.076861	test: 2.884435
MAE train: 0.513522	val: 2.828860	test: 2.042352

Epoch: 136
Loss: 0.7439665198326111
RMSE train: 0.749942	val: 4.150677	test: 2.912388
MAE train: 0.561843	val: 2.864508	test: 2.034941

Epoch: 137
Loss: 0.6887668371200562
RMSE train: 0.848549	val: 4.233998	test: 2.946374
MAE train: 0.626091	val: 2.906788	test: 2.053139

Epoch: 138
Loss: 0.6243389844894409
RMSE train: 0.810480	val: 4.200094	test: 2.933746
MAE train: 0.594468	val: 2.892791	test: 2.054388

Epoch: 139
Loss: 0.7824638187885284
RMSE train: 0.742494	val: 4.149625	test: 2.925319
MAE train: 0.548971	val: 2.884597	test: 2.070944

Epoch: 140
Loss: 0.6793689131736755
RMSE train: 0.717189	val: 4.108163	test: 2.924837
MAE train: 0.538183	val: 2.880751	test: 2.092062

Epoch: 141
Loss: 0.7694402933120728
RMSE train: 0.686682	val: 4.062163	test: 2.918478
MAE train: 0.519402	val: 2.842911	test: 2.106155

Epoch: 142
Loss: 0.7673702239990234
RMSE train: 0.683495	val: 4.072297	test: 2.937341
MAE train: 0.517637	val: 2.819722	test: 2.113416

Epoch: 143
Loss: 0.6031649708747864
RMSE train: 0.695439	val: 4.083957	test: 2.948421
MAE train: 0.522929	val: 2.807184	test: 2.107633

Epoch: 144
Loss: 0.6796557605266571
RMSE train: 0.667165	val: 4.050838	test: 2.931684
MAE train: 0.615339	val: 1.152288	test: 1.919356

Epoch: 145
Loss: 0.7710320055484772
RMSE train: 0.874944	val: 1.528626	test: 2.666427
MAE train: 0.633511	val: 1.112696	test: 1.907827

Epoch: 146
Loss: 0.7840761542320251
RMSE train: 0.839277	val: 1.528238	test: 2.673451
MAE train: 0.618169	val: 1.112678	test: 1.921907

Epoch: 147
Loss: 0.8242297172546387
RMSE train: 0.841830	val: 1.540913	test: 2.634181
MAE train: 0.618907	val: 1.155749	test: 1.887185

Epoch: 148
Loss: 0.7062918245792389
RMSE train: 0.864003	val: 1.579941	test: 2.623848
MAE train: 0.625867	val: 1.239044	test: 1.867979

Epoch: 149
Loss: 1.2122911512851715
RMSE train: 0.877144	val: 1.623496	test: 2.639624
MAE train: 0.640420	val: 1.290602	test: 1.891752

Epoch: 150
Loss: 0.7273486852645874
RMSE train: 0.830401	val: 1.583873	test: 2.645674
MAE train: 0.621173	val: 1.220536	test: 1.922106

Epoch: 151
Loss: 0.705351322889328
RMSE train: 0.810420	val: 1.552573	test: 2.649663
MAE train: 0.612916	val: 1.158272	test: 1.937711

Epoch: 152
Loss: 0.6756552755832672
RMSE train: 0.798328	val: 1.568739	test: 2.630001
MAE train: 0.604737	val: 1.170207	test: 1.918414

Epoch: 153
Loss: 0.7409902215003967
RMSE train: 0.791963	val: 1.602693	test: 2.617443
MAE train: 0.606968	val: 1.205192	test: 1.908130

Epoch: 154
Loss: 0.861783355474472
RMSE train: 0.806798	val: 1.629947	test: 2.613806
MAE train: 0.605938	val: 1.238998	test: 1.889763

Epoch: 155
Loss: 0.7462103068828583
RMSE train: 0.846026	val: 1.605926	test: 2.613594
MAE train: 0.615467	val: 1.213084	test: 1.847529

Epoch: 156
Loss: 0.7347169518470764
RMSE train: 0.853260	val: 1.613080	test: 2.621220
MAE train: 0.617133	val: 1.226857	test: 1.850079

Epoch: 157
Loss: 0.6897239983081818
RMSE train: 0.816565	val: 1.650012	test: 2.616133
MAE train: 0.589582	val: 1.283798	test: 1.891058

Epoch: 158
Loss: 0.7445182204246521
RMSE train: 0.730951	val: 1.696255	test: 2.644835
MAE train: 0.528495	val: 1.328061	test: 1.950902

Epoch: 159
Loss: 0.9697243869304657
RMSE train: 0.672537	val: 1.741879	test: 2.668374
MAE train: 0.471560	val: 1.364707	test: 1.983531

Epoch: 160
Loss: 0.620741993188858
RMSE train: 0.675992	val: 1.734386	test: 2.646060
MAE train: 0.466208	val: 1.358155	test: 1.959306

Epoch: 161
Loss: 0.7016893327236176
RMSE train: 0.731304	val: 1.626076	test: 2.629157
MAE train: 0.508950	val: 1.243883	test: 1.917454

Epoch: 162
Loss: 0.8306837975978851
RMSE train: 0.797224	val: 1.565049	test: 2.646136
MAE train: 0.557355	val: 1.169062	test: 1.901807

Epoch: 163
Loss: 0.707260251045227
RMSE train: 0.852724	val: 1.566462	test: 2.643076
MAE train: 0.597050	val: 1.167449	test: 1.891163

Epoch: 164
Loss: 0.6700163185596466
RMSE train: 0.846188	val: 1.603253	test: 2.626755
MAE train: 0.598341	val: 1.211955	test: 1.894357

Epoch: 165
Loss: 0.731168657541275
RMSE train: 0.802560	val: 1.659927	test: 2.615453
MAE train: 0.576023	val: 1.272739	test: 1.918358

Epoch: 166
Loss: 0.9968900084495544
RMSE train: 0.770861	val: 1.675084	test: 2.640535
MAE train: 0.566512	val: 1.287506	test: 1.950243

Epoch: 167
Loss: 0.6051869690418243
RMSE train: 0.774987	val: 1.620735	test: 2.639348
MAE train: 0.578029	val: 1.229266	test: 1.928779

Epoch: 168
Loss: 0.6044307351112366
RMSE train: 0.785182	val: 1.585278	test: 2.623001
MAE train: 0.583369	val: 1.187864	test: 1.875587

Epoch: 169
Loss: 0.6273883581161499
RMSE train: 0.758068	val: 1.602383	test: 2.579953
MAE train: 0.544515	val: 1.219903	test: 1.829750

Epoch: 170
Loss: 0.7958808541297913
RMSE train: 0.693438	val: 1.696538	test: 2.534413
MAE train: 0.488519	val: 1.336904	test: 1.836581

Epoch: 171
Loss: 0.5696278512477875
RMSE train: 0.665638	val: 1.722911	test: 2.525713
MAE train: 0.467669	val: 1.360945	test: 1.863992

Epoch: 172
Loss: 0.64258673787117
RMSE train: 0.662562	val: 1.675213	test: 2.555294
MAE train: 0.474013	val: 1.309106	test: 1.917118

Epoch: 173
Loss: 0.6172269284725189
RMSE train: 0.704580	val: 1.609238	test: 2.588163
MAE train: 0.510369	val: 1.241392	test: 1.957519

Epoch: 174
Loss: 0.7650282979011536
RMSE train: 0.757738	val: 1.546356	test: 2.578877
MAE train: 0.553950	val: 1.177760	test: 1.931645

Epoch: 175
Loss: 0.6258088052272797
RMSE train: 0.776475	val: 1.523486	test: 2.550038
MAE train: 0.561070	val: 1.164636	test: 1.888032

Epoch: 176
Loss: 0.7781203091144562
RMSE train: 0.783675	val: 1.548166	test: 2.527599
MAE train: 0.554897	val: 1.207719	test: 1.851308

Epoch: 177
Loss: 0.5508323013782501
RMSE train: 0.719389	val: 1.598494	test: 2.539717
MAE train: 0.513863	val: 1.274058	test: 1.857214

Epoch: 178
Loss: 0.6781623959541321
RMSE train: 0.644969	val: 1.632099	test: 2.582610
MAE train: 0.473419	val: 1.299858	test: 1.905231

Epoch: 179
Loss: 0.5840521454811096
RMSE train: 0.600502	val: 1.628847	test: 2.618637
MAE train: 0.435052	val: 1.279737	test: 1.931102

Epoch: 180
Loss: 0.5579274147748947
RMSE train: 0.622221	val: 1.584329	test: 2.596861
MAE train: 0.452547	val: 1.209775	test: 1.896278

Epoch: 181
Loss: 0.77470463514328
RMSE train: 0.671898	val: 1.556358	test: 2.587656
MAE train: 0.489486	val: 1.164238	test: 1.856795

Epoch: 182
Loss: 0.661149650812149
RMSE train: 0.728127	val: 1.571432	test: 2.565886
MAE train: 0.540163	val: 1.190936	test: 1.812830

Epoch: 183
Loss: 0.5924376845359802
RMSE train: 0.776262	val: 1.645417	test: 2.543544
MAE train: 0.569175	val: 1.292488	test: 1.780797

Epoch: 184
Loss: 0.9116550981998444
RMSE train: 0.780182	val: 1.736840	test: 2.552628
MAE train: 0.565601	val: 1.393559	test: 1.818447

Epoch: 185
Loss: 0.6481732428073883
RMSE train: 0.745449	val: 1.663123	test: 2.588002
MAE train: 0.547882	val: 1.312640	test: 1.865195

Epoch: 186
Loss: 0.5100566297769547
RMSE train: 0.750606	val: 1.556822	test: 2.631711
MAE train: 0.558320	val: 1.190716	test: 1.886828

Epoch: 187
Loss: 0.7717249989509583
RMSE train: 0.775125	val: 1.526184	test: 2.640329
MAE train: 0.564116	val: 1.127085	test: 1.888315

Epoch: 188
Loss: 0.6949599087238312
RMSE train: 0.770491	val: 1.520159	test: 2.605352
MAE train: 0.543777	val: 1.127926	test: 1.840810

Epoch: 189
Loss: 0.5428505539894104
RMSE train: 0.759780	val: 1.549684	test: 2.533138
MAE train: 0.521792	val: 1.205388	test: 1.778883

Epoch: 190
Loss: 0.5449755191802979
RMSE train: 0.726656	val: 1.631990	test: 2.487198
MAE train: 0.498109	val: 1.316900	test: 1.781576

Epoch: 191
Loss: 0.637267529964447
RMSE train: 0.676461	val: 1.669455	test: 2.492309
MAE train: 0.476384	val: 1.358160	test: 1.825204

Epoch: 192
Loss: 0.5842980444431305
RMSE train: 0.660490	val: 1.607352	test: 2.523013
MAE train: 0.475935	val: 1.274939	test: 1.867706

Epoch: 193
Loss: 0.5264923125505447
RMSE train: 0.662717	val: 1.565491	test: 2.561771
MAE train: 0.485959	val: 1.221949	test: 1.921492

Epoch: 194
Loss: 0.5889820158481598
RMSE train: 0.690021	val: 1.546753	test: 2.577734
MAE train: 0.511532	val: 1.193147	test: 1.929424

Epoch: 195
Loss: 0.6102086305618286
RMSE train: 0.687101	val: 1.571124	test: 2.571862
MAE train: 0.512857	val: 1.227099	test: 1.915198

Epoch: 196
Loss: 0.6184129118919373
RMSE train: 0.714708	val: 1.616948	test: 2.546862
MAE train: 0.523368	val: 1.286839	test: 1.864975

Epoch: 197
Loss: 0.6092924773693085
RMSE train: 0.726274	val: 1.640981	test: 2.557393
MAE train: 0.529002	val: 1.317869	test: 1.850571

Epoch: 198
Loss: 0.5193637013435364
RMSE train: 0.732237	val: 1.657366	test: 2.571506
MAE train: 0.537421	val: 1.339495	test: 1.850492

Epoch: 199
Loss: 0.7096640169620514
RMSE train: 0.747864	val: 1.630339	test: 2.576278
MAE train: 0.547093	val: 1.303900	test: 1.862195

Epoch: 200
Loss: 0.4993009567260742
RMSE train: 0.758478	val: 1.587053	test: 2.571010
MAE train: 0.545593	val: 1.238686	test: 1.874740

Epoch: 201
Loss: 0.953081488609314
RMSE train: 0.756105	val: 1.572223	test: 2.549201
MAE train: 0.541006	val: 1.217713	test: 1.868137

Epoch: 202
Loss: 0.5604948997497559
RMSE train: 0.763754	val: 1.574020	test: 2.526752
MAE train: 0.548342	val: 1.234987	test: 1.841769

Epoch: 203
Loss: 0.6008521020412445
RMSE train: 0.739696	val: 1.580646	test: 2.519855
MAE train: 0.545442	val: 1.255702	test: 1.864101

Epoch: 204
Loss: 0.6266538798809052
RMSE train: 0.720849	val: 1.576055	test: 2.521185
MAE train: 0.538436	val: 1.256360	test: 1.877191
MAE train: 0.505530	val: 2.777359	test: 2.091983

Epoch: 145
Loss: 0.850972443819046
RMSE train: 0.644015	val: 4.054159	test: 2.918777
MAE train: 0.498447	val: 2.787143	test: 2.089683

Epoch: 146
Loss: 0.6413135826587677
RMSE train: 0.640481	val: 4.048567	test: 2.910681
MAE train: 0.498224	val: 2.783766	test: 2.091966

Epoch: 147
Loss: 0.8115143179893494
RMSE train: 0.694675	val: 4.076570	test: 2.933686
MAE train: 0.540977	val: 2.813898	test: 2.105096

Epoch: 148
Loss: 0.6507471799850464
RMSE train: 0.706433	val: 4.070224	test: 2.928473
MAE train: 0.543824	val: 2.815966	test: 2.084596

Epoch: 149
Loss: 0.77750363945961
RMSE train: 0.683172	val: 4.024783	test: 2.890872
MAE train: 0.521656	val: 2.785492	test: 2.050181

Epoch: 150
Loss: 0.5413498878479004
RMSE train: 0.673537	val: 3.993890	test: 2.863957
MAE train: 0.503784	val: 2.773508	test: 2.033968

Epoch: 151
Loss: 0.6360287368297577
RMSE train: 0.685608	val: 3.995928	test: 2.852188
MAE train: 0.512818	val: 2.759833	test: 2.034049

Epoch: 152
Loss: 0.7883246839046478
RMSE train: 0.674686	val: 3.992247	test: 2.841239
MAE train: 0.512950	val: 2.729803	test: 2.022699

Epoch: 153
Loss: 0.6436851024627686
RMSE train: 0.689363	val: 4.014413	test: 2.842591
MAE train: 0.525312	val: 2.719384	test: 2.015139

Epoch: 154
Loss: 0.6916437745094299
RMSE train: 0.703810	val: 4.031657	test: 2.836846
MAE train: 0.535367	val: 2.696180	test: 1.999895

Epoch: 155
Loss: 0.6533391773700714
RMSE train: 0.702795	val: 4.051507	test: 2.827795
MAE train: 0.532239	val: 2.679201	test: 1.978842

Epoch: 156
Loss: 0.6224953532218933
RMSE train: 0.705917	val: 4.067302	test: 2.824188
MAE train: 0.533934	val: 2.684158	test: 1.984624

Epoch: 157
Loss: 0.5514101088047028
RMSE train: 0.680627	val: 4.049548	test: 2.814222
MAE train: 0.509526	val: 2.680350	test: 1.993151

Epoch: 158
Loss: 0.6284940242767334
RMSE train: 0.654248	val: 4.034940	test: 2.813260
MAE train: 0.491963	val: 2.687389	test: 1.994868

Epoch: 159
Loss: 0.5124867558479309
RMSE train: 0.655310	val: 4.052532	test: 2.836511
MAE train: 0.500093	val: 2.715452	test: 2.020911

Epoch: 160
Loss: 0.6299226880073547
RMSE train: 0.673656	val: 4.094322	test: 2.873060
MAE train: 0.523288	val: 2.767672	test: 2.055797

Epoch: 161
Loss: 0.5376234650611877
RMSE train: 0.727509	val: 4.146518	test: 2.928833
MAE train: 0.573003	val: 2.822807	test: 2.110948

Epoch: 162
Loss: 0.6728847324848175
RMSE train: 0.759788	val: 4.180054	test: 2.965730
MAE train: 0.599853	val: 2.861389	test: 2.144445

Epoch: 163
Loss: 0.5762464553117752
RMSE train: 0.741632	val: 4.145586	test: 2.958163
MAE train: 0.583790	val: 2.852220	test: 2.142450

Epoch: 164
Loss: 0.5720946937799454
RMSE train: 0.694724	val: 4.069110	test: 2.925154
MAE train: 0.538994	val: 2.801365	test: 2.113752

Epoch: 165
Loss: 0.5797875821590424
RMSE train: 0.648453	val: 3.992676	test: 2.885221
MAE train: 0.495421	val: 2.750099	test: 2.088200

Epoch: 166
Loss: 0.5309144258499146
RMSE train: 0.645723	val: 3.977660	test: 2.853384
MAE train: 0.490485	val: 2.732570	test: 2.049870

Epoch: 167
Loss: 0.6959126591682434
RMSE train: 0.663966	val: 4.036761	test: 2.846115
MAE train: 0.503805	val: 2.747095	test: 2.023088

Epoch: 168
Loss: 0.6306216716766357
RMSE train: 0.715970	val: 4.101283	test: 2.859835
MAE train: 0.541885	val: 2.757587	test: 2.006532

Epoch: 169
Loss: 0.5686492025852203
RMSE train: 0.700629	val: 4.104015	test: 2.836032
MAE train: 0.536765	val: 2.741645	test: 1.975680

Epoch: 170
Loss: 0.6546331942081451
RMSE train: 0.701921	val: 4.123915	test: 2.828507
MAE train: 0.542308	val: 2.748427	test: 1.953711

Epoch: 171
Loss: 0.5896719396114349
RMSE train: 0.674255	val: 4.111447	test: 2.817342
MAE train: 0.525471	val: 2.751373	test: 1.948976

Epoch: 172
Loss: 0.5124899595975876
RMSE train: 0.607024	val: 4.053275	test: 2.800245
MAE train: 0.481444	val: 2.710879	test: 1.948137

Epoch: 173
Loss: 0.5821459591388702
RMSE train: 0.590781	val: 4.038343	test: 2.822248
MAE train: 0.472313	val: 2.699893	test: 1.969505

Epoch: 174
Loss: 0.4339175820350647
RMSE train: 0.587827	val: 4.029836	test: 2.844449
MAE train: 0.471186	val: 2.690621	test: 1.996036

Epoch: 175
Loss: 0.6126990020275116
RMSE train: 0.631743	val: 4.085979	test: 2.876112
MAE train: 0.502416	val: 2.735735	test: 2.008009

Epoch: 176
Loss: 0.5654324591159821
RMSE train: 0.671591	val: 4.124784	test: 2.892487
MAE train: 0.532831	val: 2.786483	test: 2.032153

Epoch: 177
Loss: 0.5025430172681808
RMSE train: 0.676475	val: 4.135187	test: 2.889700
MAE train: 0.525394	val: 2.809359	test: 2.036880

Epoch: 178
Loss: 0.6202105581760406
RMSE train: 0.699258	val: 4.193760	test: 2.910630
MAE train: 0.529805	val: 2.849557	test: 2.052997

Epoch: 179
Loss: 0.6667259931564331
RMSE train: 0.720455	val: 4.255249	test: 2.939476
MAE train: 0.543491	val: 2.887097	test: 2.077459

Epoch: 180
Loss: 0.6193460822105408
RMSE train: 0.719772	val: 4.299824	test: 2.968602
MAE train: 0.547674	val: 2.930705	test: 2.102393

Epoch: 181
Loss: 0.5597211420536041
RMSE train: 0.630767	val: 4.232661	test: 2.947149
MAE train: 0.493023	val: 2.900728	test: 2.095468

Epoch: 182
Loss: 0.5978915989398956
RMSE train: 0.572221	val: 4.147349	test: 2.915333
MAE train: 0.457967	val: 2.845220	test: 2.076969

Epoch: 183
Loss: 0.667065441608429
RMSE train: 0.563183	val: 4.086993	test: 2.893270
MAE train: 0.456021	val: 2.804039	test: 2.069918

Epoch: 184
Loss: 0.57436203956604
RMSE train: 0.588217	val: 4.088428	test: 2.890901
MAE train: 0.479160	val: 2.799486	test: 2.072430

Epoch: 185
Loss: 0.552805632352829
RMSE train: 0.613630	val: 4.096442	test: 2.889999
MAE train: 0.497626	val: 2.789579	test: 2.074192

Epoch: 186
Loss: 0.49614381790161133
RMSE train: 0.681963	val: 4.168806	test: 2.920657
MAE train: 0.548174	val: 2.821889	test: 2.079873

Epoch: 187
Loss: 0.46389807760715485
RMSE train: 0.729327	val: 4.210949	test: 2.944131
MAE train: 0.571183	val: 2.845956	test: 2.070250

Epoch: 188
Loss: 0.5631399303674698
RMSE train: 0.663967	val: 4.154413	test: 2.896945
MAE train: 0.516118	val: 2.814636	test: 2.037466

Epoch: 189
Loss: 0.5460789799690247
RMSE train: 0.613939	val: 4.097884	test: 2.859328
MAE train: 0.481348	val: 2.774981	test: 2.000383

Epoch: 190
Loss: 0.5301738977432251
RMSE train: 0.604656	val: 4.095476	test: 2.854243
MAE train: 0.470134	val: 2.766921	test: 1.985442

Epoch: 191
Loss: 0.487049400806427
RMSE train: 0.599114	val: 4.093465	test: 2.846062
MAE train: 0.463371	val: 2.758835	test: 1.975800

Epoch: 192
Loss: 0.5271070152521133
RMSE train: 0.642373	val: 4.134840	test: 2.879306
MAE train: 0.488948	val: 2.788407	test: 1.993462

Epoch: 193
Loss: 0.475682869553566
RMSE train: 0.672069	val: 4.151951	test: 2.903025
MAE train: 0.511065	val: 2.817751	test: 2.025040

Epoch: 194
Loss: 0.48323503136634827
RMSE train: 0.657353	val: 4.137994	test: 2.904470
MAE train: 0.504720	val: 2.822044	test: 2.054548

Epoch: 195
Loss: 0.4872622489929199
RMSE train: 0.675548	val: 4.157743	test: 2.925771
MAE train: 0.527530	val: 2.837883	test: 2.084324

Epoch: 196
Loss: 0.5658927112817764
RMSE train: 0.686089	val: 4.162344	test: 2.927444
MAE train: 0.535688	val: 2.832748	test: 2.083933

Epoch: 197
Loss: 0.5630321800708771
RMSE train: 0.717016	val: 4.197875	test: 2.929086
MAE train: 0.550251	val: 2.851372	test: 2.074242

Epoch: 198
Loss: 0.6017770171165466
RMSE train: 0.718008	val: 4.215653	test: 2.911549
MAE train: 0.543977	val: 2.865227	test: 2.062745

Epoch: 199
Loss: 0.4245092123746872
RMSE train: 0.702016	val: 4.230943	test: 2.872679
MAE train: 0.524761	val: 2.872266	test: 2.041418

Epoch: 200
Loss: 0.5049989372491837
RMSE train: 0.668104	val: 4.207847	test: 2.845487
MAE train: 0.504802	val: 2.849792	test: 2.017685

Epoch: 201
Loss: 0.6594689190387726
RMSE train: 0.700867	val: 4.225216	test: 2.849886
MAE train: 0.539409	val: 2.867085	test: 1.993671

Early stopping
Best (RMSE):	 train: 0.645723	val: 3.977660	test: 2.853384
Best (MAE):	 train: 0.490485	val: 2.732570	test: 2.049870
All runs completed.


Epoch: 205
Loss: 0.6084279417991638
RMSE train: 0.698915	val: 1.588080	test: 2.542119
MAE train: 0.534815	val: 1.266443	test: 1.909480

Epoch: 206
Loss: 0.8585242033004761
RMSE train: 0.700251	val: 1.616458	test: 2.586583
MAE train: 0.538520	val: 1.277730	test: 1.937132

Epoch: 207
Loss: 0.8868609666824341
RMSE train: 0.770836	val: 1.646777	test: 2.572547
MAE train: 0.588502	val: 1.297000	test: 1.886947

Epoch: 208
Loss: 0.534672737121582
RMSE train: 0.752800	val: 1.649481	test: 2.568693
MAE train: 0.572512	val: 1.283367	test: 1.858145

Epoch: 209
Loss: 0.6515669524669647
RMSE train: 0.715841	val: 1.655750	test: 2.569520
MAE train: 0.544596	val: 1.275774	test: 1.827762

Epoch: 210
Loss: 0.5281955301761627
RMSE train: 0.659669	val: 1.698035	test: 2.571514
MAE train: 0.494581	val: 1.318769	test: 1.813318

Epoch: 211
Loss: 0.5651844441890717
RMSE train: 0.644339	val: 1.733851	test: 2.563847
MAE train: 0.468956	val: 1.361479	test: 1.799764

Epoch: 212
Loss: 0.8332720398902893
RMSE train: 0.657858	val: 1.799512	test: 2.562356
MAE train: 0.464005	val: 1.429269	test: 1.808327

Epoch: 213
Loss: 0.7606193423271179
RMSE train: 0.768025	val: 1.734506	test: 2.560554
MAE train: 0.533315	val: 1.370160	test: 1.796723

Epoch: 214
Loss: 0.610637903213501
RMSE train: 0.817551	val: 1.631154	test: 2.578408
MAE train: 0.581789	val: 1.258049	test: 1.813913

Epoch: 215
Loss: 0.7012285590171814
RMSE train: 0.803946	val: 1.587249	test: 2.584244
MAE train: 0.592467	val: 1.204887	test: 1.842312

Epoch: 216
Loss: 0.6156957745552063
RMSE train: 0.759005	val: 1.567374	test: 2.565533
MAE train: 0.559926	val: 1.180864	test: 1.839223

Epoch: 217
Loss: 0.6478430330753326
RMSE train: 0.673517	val: 1.581211	test: 2.535756
MAE train: 0.492743	val: 1.208212	test: 1.844166

Epoch: 218
Loss: 0.5045941919088364
RMSE train: 0.666308	val: 1.569226	test: 2.521683
MAE train: 0.481921	val: 1.193566	test: 1.826148

Epoch: 219
Loss: 0.5609842240810394
RMSE train: 0.671969	val: 1.578668	test: 2.510328
MAE train: 0.481472	val: 1.207070	test: 1.807789

Epoch: 220
Loss: 0.47003644704818726
RMSE train: 0.715635	val: 1.568591	test: 2.497095
MAE train: 0.504484	val: 1.203102	test: 1.778517

Epoch: 221
Loss: 0.909940779209137
RMSE train: 0.727515	val: 1.600147	test: 2.484166
MAE train: 0.504539	val: 1.247108	test: 1.773866

Epoch: 222
Loss: 0.487484335899353
RMSE train: 0.749989	val: 1.611162	test: 2.483466
MAE train: 0.520464	val: 1.266257	test: 1.768474

Epoch: 223
Loss: 0.5739867389202118
RMSE train: 0.683829	val: 1.616291	test: 2.496563
MAE train: 0.501454	val: 1.261435	test: 1.817372

Early stopping
Best (RMSE):	 train: 0.770491	val: 1.520159	test: 2.605352
Best (MAE):	 train: 0.543777	val: 1.127926	test: 1.840810
All runs completed.
