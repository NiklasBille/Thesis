>>> Starting run for dataset: bace
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running RANDOM configs_static_noise_experiments/GraphCL/bace/noise=0.0.yml on cuda:0
Running RANDOM configs_static_noise_experiments/GraphCL/bace/noise=0.05.yml on cuda:1
Running RANDOM configs_static_noise_experiments/GraphCL/bace/noise=0.1.yml on cuda:2
Running RANDOM configs_static_noise_experiments/GraphCL/bace/noise=0.2.yml on cuda:3
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.1.yml --runseed 4 --device cuda:2
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.0.yml --runseed 4 --device cuda:0
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.1.yml --runseed 5 --device cuda:2
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.0.yml --runseed 5 --device cuda:0
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.1.yml --runseed 6 --device cuda:2
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.0.yml --runseed 6 --device cuda:0
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.2.yml --runseed 4 --device cuda:3
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.2.yml --runseed 5 --device cuda:3
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.2.yml --runseed 6 --device cuda:3
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.05.yml --runseed 4 --device cuda:1
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.05.yml --runseed 5 --device cuda:1
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.05.yml --runseed 6 --device cuda:1
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: bace
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/bace/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/bace/noise=0.0/bace_scaff_5_26-05_11-28-51  ]
[ Using Seed :  5  ]
[ Using device :  cuda:0  ]
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6903216163401306
ROC train: 0.672223	val: 0.682051	test: 0.517823
PRC train: 0.561840	val: 0.924732	test: 0.531492

Epoch: 2
Loss: 0.6426270539187323
ROC train: 0.753188	val: 0.712454	test: 0.585290
PRC train: 0.641972	val: 0.930976	test: 0.581503

Epoch: 3
Loss: 0.5858100399894464
ROC train: 0.818553	val: 0.687179	test: 0.639541
PRC train: 0.723000	val: 0.932767	test: 0.623123

Epoch: 4
Loss: 0.5359082097614162
ROC train: 0.846744	val: 0.689377	test: 0.675535
PRC train: 0.752013	val: 0.931765	test: 0.660369

Epoch: 5
Loss: 0.5156310511515815
ROC train: 0.856136	val: 0.688278	test: 0.691532
PRC train: 0.776078	val: 0.930603	test: 0.682391

Epoch: 6
Loss: 0.4791463236106012
ROC train: 0.869541	val: 0.693040	test: 0.717267
PRC train: 0.802163	val: 0.934832	test: 0.698859

Epoch: 7
Loss: 0.4695580947438239
ROC train: 0.884372	val: 0.681685	test: 0.746479
PRC train: 0.814832	val: 0.937885	test: 0.737012

Epoch: 8
Loss: 0.459447189413547
ROC train: 0.890868	val: 0.696337	test: 0.745783
PRC train: 0.822485	val: 0.941889	test: 0.745844

Epoch: 9
Loss: 0.439559218953986
ROC train: 0.893042	val: 0.703663	test: 0.747174
PRC train: 0.827457	val: 0.942472	test: 0.760256

Epoch: 10
Loss: 0.43631746088582324
ROC train: 0.895634	val: 0.684615	test: 0.760216
PRC train: 0.828855	val: 0.938578	test: 0.776836

Epoch: 11
Loss: 0.4352704411886855
ROC train: 0.899957	val: 0.679853	test: 0.764041
PRC train: 0.835028	val: 0.937961	test: 0.776619

Epoch: 12
Loss: 0.42343887730054136
ROC train: 0.905654	val: 0.696337	test: 0.756042
PRC train: 0.844059	val: 0.941472	test: 0.772924

Epoch: 13
Loss: 0.40819642441324466
ROC train: 0.907917	val: 0.692308	test: 0.759694
PRC train: 0.848580	val: 0.939949	test: 0.771643

Epoch: 14
Loss: 0.3939615800687605
ROC train: 0.912591	val: 0.686813	test: 0.767171
PRC train: 0.853885	val: 0.939265	test: 0.773103

Epoch: 15
Loss: 0.4020218114448064
ROC train: 0.914763	val: 0.670696	test: 0.768388
PRC train: 0.861055	val: 0.934046	test: 0.778778

Epoch: 16
Loss: 0.38698798874704726
ROC train: 0.917931	val: 0.668498	test: 0.769431
PRC train: 0.864483	val: 0.933382	test: 0.775056

Epoch: 17
Loss: 0.3726583529100991
ROC train: 0.918596	val: 0.687912	test: 0.757781
PRC train: 0.865877	val: 0.937005	test: 0.767743

Epoch: 18
Loss: 0.39197001471848136
ROC train: 0.919463	val: 0.687179	test: 0.747174
PRC train: 0.869577	val: 0.937391	test: 0.759298

Epoch: 19
Loss: 0.3718430833864559
ROC train: 0.922594	val: 0.679487	test: 0.766475
PRC train: 0.874937	val: 0.932148	test: 0.763247

Epoch: 20
Loss: 0.393990948588527
ROC train: 0.927357	val: 0.672527	test: 0.763346
PRC train: 0.881431	val: 0.930959	test: 0.767645

Epoch: 21
Loss: 0.38316505140605583
ROC train: 0.927754	val: 0.671062	test: 0.768562
PRC train: 0.884251	val: 0.930109	test: 0.782014

Epoch: 22
Loss: 0.3587821404043393
ROC train: 0.927988	val: 0.680220	test: 0.773605
PRC train: 0.884364	val: 0.934201	test: 0.787743

Epoch: 23
Loss: 0.3721955731398272
ROC train: 0.932180	val: 0.686813	test: 0.777778
PRC train: 0.888896	val: 0.932895	test: 0.789994

Epoch: 24
Loss: 0.37056156843705124
ROC train: 0.932026	val: 0.677289	test: 0.774822
PRC train: 0.888457	val: 0.927169	test: 0.783187

Epoch: 25
Loss: 0.35764886088536774
ROC train: 0.934081	val: 0.686081	test: 0.762476
PRC train: 0.891265	val: 0.929290	test: 0.779686

Epoch: 26
Loss: 0.3651045634195484
ROC train: 0.936478	val: 0.673626	test: 0.773778
PRC train: 0.897669	val: 0.924176	test: 0.786778

Epoch: 27
Loss: 0.36371579036924845
ROC train: 0.939051	val: 0.647253	test: 0.775517
PRC train: 0.902864	val: 0.916201	test: 0.777254

Epoch: 28
Loss: 0.35566342434856874
ROC train: 0.939906	val: 0.646154	test: 0.766128
PRC train: 0.905709	val: 0.915526	test: 0.767588

Epoch: 29
Loss: 0.3401942666676914
ROC train: 0.940565	val: 0.656044	test: 0.749957
PRC train: 0.907018	val: 0.918362	test: 0.749958

Epoch: 30
Loss: 0.33911634638148297
ROC train: 0.942763	val: 0.641758	test: 0.763172
PRC train: 0.909511	val: 0.914989	test: 0.763316

Epoch: 31
Loss: 0.34014442917082566
ROC train: 0.943813	val: 0.667033	test: 0.772214
PRC train: 0.909596	val: 0.920103	test: 0.766506

Epoch: 32
Loss: 0.3432277034808093
ROC train: 0.942751	val: 0.684982	test: 0.753434
PRC train: 0.908474	val: 0.925804	test: 0.743610

Epoch: 33
Loss: 0.3334631682599067
ROC train: 0.945759	val: 0.671429	test: 0.745436Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: bace
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/bace/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/bace/noise=0.0/bace_scaff_6_26-05_11-28-51  ]
[ Using Seed :  6  ]
[ Using device :  cuda:0  ]
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6857142523983691
ROC train: 0.680420	val: 0.614652	test: 0.593636
PRC train: 0.591016	val: 0.912524	test: 0.563322

Epoch: 2
Loss: 0.6364860687670886
ROC train: 0.735468	val: 0.583150	test: 0.613980
PRC train: 0.638770	val: 0.901588	test: 0.600224

Epoch: 3
Loss: 0.5874637744147222
ROC train: 0.814015	val: 0.593407	test: 0.683359
PRC train: 0.713731	val: 0.906968	test: 0.655900

Epoch: 4
Loss: 0.5597078650892879
ROC train: 0.846481	val: 0.626007	test: 0.733612
PRC train: 0.756573	val: 0.914521	test: 0.736513

Epoch: 5
Loss: 0.5065654418716904
ROC train: 0.859889	val: 0.599267	test: 0.774648
PRC train: 0.774100	val: 0.909386	test: 0.742491

Epoch: 6
Loss: 0.4829697346169818
ROC train: 0.873536	val: 0.624908	test: 0.733090
PRC train: 0.789435	val: 0.913086	test: 0.719188

Epoch: 7
Loss: 0.4640201193633876
ROC train: 0.882003	val: 0.655678	test: 0.769605
PRC train: 0.807810	val: 0.925890	test: 0.737080

Epoch: 8
Loss: 0.44978884206288755
ROC train: 0.895046	val: 0.670330	test: 0.785776
PRC train: 0.826143	val: 0.930480	test: 0.778107

Epoch: 9
Loss: 0.44227763400541126
ROC train: 0.901073	val: 0.667033	test: 0.793775
PRC train: 0.825635	val: 0.927091	test: 0.779255

Epoch: 10
Loss: 0.4183551012946821
ROC train: 0.906761	val: 0.671062	test: 0.793949
PRC train: 0.838584	val: 0.933207	test: 0.789912

Epoch: 11
Loss: 0.41517865062431153
ROC train: 0.910317	val: 0.665201	test: 0.793253
PRC train: 0.843669	val: 0.934271	test: 0.797712

Epoch: 12
Loss: 0.39980669052308015
ROC train: 0.912925	val: 0.658608	test: 0.778647
PRC train: 0.847577	val: 0.931967	test: 0.771850

Epoch: 13
Loss: 0.38802249757218477
ROC train: 0.914309	val: 0.686813	test: 0.783690
PRC train: 0.853743	val: 0.939008	test: 0.768240

Epoch: 14
Loss: 0.3920914842376854
ROC train: 0.920551	val: 0.680952	test: 0.798296
PRC train: 0.863923	val: 0.937939	test: 0.780057

Epoch: 15
Loss: 0.39869060919607663
ROC train: 0.922009	val: 0.676190	test: 0.785950
PRC train: 0.864619	val: 0.936265	test: 0.778216

Epoch: 16
Loss: 0.38746262188568487
ROC train: 0.921618	val: 0.689011	test: 0.786472
PRC train: 0.864710	val: 0.937789	test: 0.770231

Epoch: 17
Loss: 0.3800649499923315
ROC train: 0.924341	val: 0.709158	test: 0.765432
PRC train: 0.873222	val: 0.942644	test: 0.748094

Epoch: 18
Loss: 0.383466922428585
ROC train: 0.926978	val: 0.686813	test: 0.764737
PRC train: 0.878699	val: 0.938089	test: 0.754816

Epoch: 19
Loss: 0.3702900706886462
ROC train: 0.929669	val: 0.680586	test: 0.789776
PRC train: 0.884061	val: 0.936912	test: 0.774881

Epoch: 20
Loss: 0.37762949510890464
ROC train: 0.929289	val: 0.680220	test: 0.793775
PRC train: 0.882487	val: 0.935620	test: 0.772190

Epoch: 21
Loss: 0.3629881787828248
ROC train: 0.931084	val: 0.686447	test: 0.774126
PRC train: 0.887227	val: 0.935960	test: 0.758642

Epoch: 22
Loss: 0.37104095867632914
ROC train: 0.930776	val: 0.685714	test: 0.760563
PRC train: 0.886736	val: 0.933239	test: 0.737475

Epoch: 23
Loss: 0.3715342903301643
ROC train: 0.933662	val: 0.683883	test: 0.763519
PRC train: 0.891970	val: 0.934885	test: 0.738788

Epoch: 24
Loss: 0.34741049821432624
ROC train: 0.936904	val: 0.676557	test: 0.778473
PRC train: 0.898593	val: 0.931611	test: 0.754825

Epoch: 25
Loss: 0.36182460844109876
ROC train: 0.940616	val: 0.671062	test: 0.787515
PRC train: 0.904836	val: 0.930680	test: 0.761859

Epoch: 26
Loss: 0.34075112491297943
ROC train: 0.941002	val: 0.668132	test: 0.772561
PRC train: 0.905544	val: 0.932473	test: 0.755835

Epoch: 27
Loss: 0.3399027718331711
ROC train: 0.941196	val: 0.683150	test: 0.772387
PRC train: 0.906405	val: 0.934197	test: 0.759915

Epoch: 28
Loss: 0.33609829890147813
ROC train: 0.940297	val: 0.674725	test: 0.761954
PRC train: 0.905615	val: 0.930453	test: 0.748626

Epoch: 29
Loss: 0.33967529735896357
ROC train: 0.943576	val: 0.676923	test: 0.746653
PRC train: 0.908044	val: 0.931099	test: 0.744979

Epoch: 30
Loss: 0.33985647480064785
ROC train: 0.943459	val: 0.687546	test: 0.741089
PRC train: 0.909706	val: 0.933706	test: 0.744620

Epoch: 31
Loss: 0.3508511340448364
ROC train: 0.945234	val: 0.669597	test: 0.759172
PRC train: 0.913933	val: 0.928934	test: 0.755781

Epoch: 32
Loss: 0.33713356363190333
ROC train: 0.947489	val: 0.669231	test: 0.773952
PRC train: 0.917602	val: 0.929730	test: 0.753775

Epoch: 33
Loss: 0.34312846304280287
ROC train: 0.946418	val: 0.688645	test: 0.762998Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: bace
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/bace/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/bace/noise=0.0/bace_scaff_4_26-05_11-28-51  ]
[ Using Seed :  4  ]
[ Using device :  cuda:0  ]
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6808801266676602
ROC train: 0.700753	val: 0.604396	test: 0.633455
PRC train: 0.600300	val: 0.895934	test: 0.584159

Epoch: 2
Loss: 0.6264195809908251
ROC train: 0.797272	val: 0.679853	test: 0.665623
PRC train: 0.694868	val: 0.926534	test: 0.616881

Epoch: 3
Loss: 0.5700793488401874
ROC train: 0.825351	val: 0.682418	test: 0.689445
PRC train: 0.723984	val: 0.929061	test: 0.659277

Epoch: 4
Loss: 0.5253503271841616
ROC train: 0.853088	val: 0.671795	test: 0.712050
PRC train: 0.748025	val: 0.922224	test: 0.691922

Epoch: 5
Loss: 0.4921588930297158
ROC train: 0.865360	val: 0.667033	test: 0.730482
PRC train: 0.758878	val: 0.921944	test: 0.710099

Epoch: 6
Loss: 0.4793087919927849
ROC train: 0.877771	val: 0.662637	test: 0.744218
PRC train: 0.776000	val: 0.926328	test: 0.726594

Epoch: 7
Loss: 0.4590699130206997
ROC train: 0.886898	val: 0.672527	test: 0.733438
PRC train: 0.788597	val: 0.932328	test: 0.731328

Epoch: 8
Loss: 0.4304449477460248
ROC train: 0.894155	val: 0.695971	test: 0.729091
PRC train: 0.804381	val: 0.940045	test: 0.731886

Epoch: 9
Loss: 0.42838462560680934
ROC train: 0.900211	val: 0.715018	test: 0.732742
PRC train: 0.826163	val: 0.946042	test: 0.743850

Epoch: 10
Loss: 0.41968057475922993
ROC train: 0.908059	val: 0.707692	test: 0.735524
PRC train: 0.843756	val: 0.944972	test: 0.745379

Epoch: 11
Loss: 0.41489779750516087
ROC train: 0.910850	val: 0.676557	test: 0.740045
PRC train: 0.853146	val: 0.939475	test: 0.747376

Epoch: 12
Loss: 0.4132340144697477
ROC train: 0.912032	val: 0.678022	test: 0.729438
PRC train: 0.851227	val: 0.937591	test: 0.731223

Epoch: 13
Loss: 0.4067529484743809
ROC train: 0.916835	val: 0.695238	test: 0.732394
PRC train: 0.862600	val: 0.941498	test: 0.736475

Epoch: 14
Loss: 0.40950299445765886
ROC train: 0.916547	val: 0.703297	test: 0.736741
PRC train: 0.861304	val: 0.942946	test: 0.736277

Epoch: 15
Loss: 0.3837008904108713
ROC train: 0.918188	val: 0.692674	test: 0.724744
PRC train: 0.863468	val: 0.942666	test: 0.712463

Epoch: 16
Loss: 0.39701296664707275
ROC train: 0.921356	val: 0.696337	test: 0.725265
PRC train: 0.868072	val: 0.943205	test: 0.713144

Epoch: 17
Loss: 0.3854536905144489
ROC train: 0.923941	val: 0.681685	test: 0.726135
PRC train: 0.870398	val: 0.939748	test: 0.724960

Epoch: 18
Loss: 0.38340730530432265
ROC train: 0.924158	val: 0.693040	test: 0.708225
PRC train: 0.867942	val: 0.942002	test: 0.705457

Epoch: 19
Loss: 0.387697156921086
ROC train: 0.925400	val: 0.690476	test: 0.716223
PRC train: 0.875650	val: 0.940262	test: 0.704382

Epoch: 20
Loss: 0.3697425964116357
ROC train: 0.923408	val: 0.697436	test: 0.729264
PRC train: 0.877793	val: 0.938895	test: 0.710169

Epoch: 21
Loss: 0.3534083017573535
ROC train: 0.927346	val: 0.710623	test: 0.717788
PRC train: 0.882721	val: 0.941523	test: 0.701596

Epoch: 22
Loss: 0.3656146075793063
ROC train: 0.934284	val: 0.680952	test: 0.727178
PRC train: 0.892220	val: 0.937063	test: 0.711124

Epoch: 23
Loss: 0.34957534030895043
ROC train: 0.937777	val: 0.671429	test: 0.725613
PRC train: 0.897679	val: 0.935149	test: 0.715968

Epoch: 24
Loss: 0.36130804670506605
ROC train: 0.938590	val: 0.685714	test: 0.707181
PRC train: 0.899970	val: 0.936618	test: 0.699038

Epoch: 25
Loss: 0.35565800637518125
ROC train: 0.936698	val: 0.708059	test: 0.703356
PRC train: 0.898449	val: 0.940140	test: 0.696701

Epoch: 26
Loss: 0.3409334991281921
ROC train: 0.941164	val: 0.704396	test: 0.714484
PRC train: 0.905569	val: 0.941049	test: 0.721199

Epoch: 27
Loss: 0.36381135150136534
ROC train: 0.942292	val: 0.693407	test: 0.696922
PRC train: 0.907595	val: 0.939315	test: 0.712510

Epoch: 28
Loss: 0.3424431107245851
ROC train: 0.943693	val: 0.702564	test: 0.695010
PRC train: 0.908668	val: 0.940597	test: 0.708098

Epoch: 29
Loss: 0.3496717108430515
ROC train: 0.942346	val: 0.706960	test: 0.703008
PRC train: 0.906641	val: 0.941790	test: 0.703283

Epoch: 30
Loss: 0.33956855514900075
ROC train: 0.943368	val: 0.697802	test: 0.711181
PRC train: 0.908782	val: 0.939919	test: 0.708399

Epoch: 31
Loss: 0.3280354760247171
ROC train: 0.944135	val: 0.675092	test: 0.699357
PRC train: 0.912030	val: 0.933887	test: 0.688161

Epoch: 32
Loss: 0.33552517305342955
ROC train: 0.944378	val: 0.669597	test: 0.688924
PRC train: 0.913311	val: 0.931233	test: 0.675768

Epoch: 33
Loss: 0.33654734647819623
ROC train: 0.948559	val: 0.695604	test: 0.700226Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: bace
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/bace/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/bace/noise=0.05/bace_scaff_4_26-05_11-28-51  ]
[ Using Seed :  4  ]
[ Using device :  cuda:1  ]
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6908163030142442
ROC train: 0.614529	val: 0.582418	test: 0.596418
PRC train: 0.499557	val: 0.886038	test: 0.584999

Epoch: 2
Loss: 0.6515832047911954
ROC train: 0.683576	val: 0.664103	test: 0.632412
PRC train: 0.577330	val: 0.918265	test: 0.626806

Epoch: 3
Loss: 0.6257486909009321
ROC train: 0.723313	val: 0.660806	test: 0.635194
PRC train: 0.623383	val: 0.922519	test: 0.640367

Epoch: 4
Loss: 0.6050449193078056
ROC train: 0.760371	val: 0.676923	test: 0.656408
PRC train: 0.660705	val: 0.928340	test: 0.667687

Epoch: 5
Loss: 0.5831787383978169
ROC train: 0.787549	val: 0.653480	test: 0.672057
PRC train: 0.694182	val: 0.925379	test: 0.685361

Epoch: 6
Loss: 0.5658164850763348
ROC train: 0.822765	val: 0.650183	test: 0.685968
PRC train: 0.735392	val: 0.924092	test: 0.691777

Epoch: 7
Loss: 0.5436133137767079
ROC train: 0.842600	val: 0.636630	test: 0.697096
PRC train: 0.763365	val: 0.910456	test: 0.697310

Epoch: 8
Loss: 0.5205484167427509
ROC train: 0.859181	val: 0.609524	test: 0.736915
PRC train: 0.785305	val: 0.904337	test: 0.734024

Epoch: 9
Loss: 0.5190776077173063
ROC train: 0.874572	val: 0.587179	test: 0.741784
PRC train: 0.808695	val: 0.898941	test: 0.742914

Epoch: 10
Loss: 0.4999019295573709
ROC train: 0.884546	val: 0.602198	test: 0.743175
PRC train: 0.823885	val: 0.912284	test: 0.736586

Epoch: 11
Loss: 0.47346059008829444
ROC train: 0.893299	val: 0.601832	test: 0.735350
PRC train: 0.836705	val: 0.912056	test: 0.730841

Epoch: 12
Loss: 0.46120964237595163
ROC train: 0.904432	val: 0.655311	test: 0.730482
PRC train: 0.850249	val: 0.926557	test: 0.738988

Epoch: 13
Loss: 0.45155600737348706
ROC train: 0.912586	val: 0.656044	test: 0.735524
PRC train: 0.860910	val: 0.928131	test: 0.739530

Epoch: 14
Loss: 0.42132142064081235
ROC train: 0.921047	val: 0.646886	test: 0.752739
PRC train: 0.872447	val: 0.927402	test: 0.753411

Epoch: 15
Loss: 0.42032528039538664
ROC train: 0.927922	val: 0.646886	test: 0.755347
PRC train: 0.883320	val: 0.922281	test: 0.764429

Epoch: 16
Loss: 0.4107269553890399
ROC train: 0.933057	val: 0.657509	test: 0.754477
PRC train: 0.892149	val: 0.925938	test: 0.766349

Epoch: 17
Loss: 0.41221861198860504
ROC train: 0.941347	val: 0.660440	test: 0.727352
PRC train: 0.905263	val: 0.926660	test: 0.744256

Epoch: 18
Loss: 0.3804095635305541
ROC train: 0.946647	val: 0.668132	test: 0.729091
PRC train: 0.914567	val: 0.927898	test: 0.741977

Epoch: 19
Loss: 0.3700081096957548
ROC train: 0.949666	val: 0.660806	test: 0.748565
PRC train: 0.920626	val: 0.923634	test: 0.758804

Epoch: 20
Loss: 0.36027568406204696
ROC train: 0.954546	val: 0.662271	test: 0.752217
PRC train: 0.928880	val: 0.925746	test: 0.752388

Epoch: 21
Loss: 0.36789757493998565
ROC train: 0.957500	val: 0.639194	test: 0.762650
PRC train: 0.933201	val: 0.918969	test: 0.761366

Epoch: 22
Loss: 0.3539246811088479
ROC train: 0.962751	val: 0.622711	test: 0.735698
PRC train: 0.940843	val: 0.913454	test: 0.747710

Epoch: 23
Loss: 0.37167679834477646
ROC train: 0.968636	val: 0.623810	test: 0.725787
PRC train: 0.950315	val: 0.914388	test: 0.737227

Epoch: 24
Loss: 0.34512943087219405
ROC train: 0.970910	val: 0.647985	test: 0.746827
PRC train: 0.954486	val: 0.921821	test: 0.754062

Epoch: 25
Loss: 0.340588812201511
ROC train: 0.971504	val: 0.632601	test: 0.761781
PRC train: 0.955036	val: 0.919315	test: 0.767283

Epoch: 26
Loss: 0.31464621964725004
ROC train: 0.973359	val: 0.632601	test: 0.755869
PRC train: 0.957319	val: 0.918659	test: 0.753797

Epoch: 27
Loss: 0.3311851342254336
ROC train: 0.976678	val: 0.633700	test: 0.732394
PRC train: 0.963838	val: 0.916812	test: 0.728147

Epoch: 28
Loss: 0.31641739157642923
ROC train: 0.978607	val: 0.633700	test: 0.724744
PRC train: 0.967507	val: 0.915858	test: 0.734854

Epoch: 29
Loss: 0.2985841400743449
ROC train: 0.981244	val: 0.622711	test: 0.748218
PRC train: 0.971094	val: 0.912700	test: 0.749802

Epoch: 30
Loss: 0.2849313115605544
ROC train: 0.981784	val: 0.606227	test: 0.763693
PRC train: 0.972135	val: 0.911474	test: 0.765088

Epoch: 31
Loss: 0.29680225423105305
ROC train: 0.982038	val: 0.638462	test: 0.740741
PRC train: 0.972493	val: 0.917818	test: 0.736869

Epoch: 32
Loss: 0.27954173496037443
ROC train: 0.986133	val: 0.643590	test: 0.725787Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: bace
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/bace/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/bace/noise=0.05/bace_scaff_5_26-05_11-28-51  ]
[ Using Seed :  5  ]
[ Using device :  cuda:1  ]
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.676915779724539
ROC train: 0.670756	val: 0.642491	test: 0.718310
PRC train: 0.539306	val: 0.910660	test: 0.708568

Epoch: 2
Loss: 0.6347694340741273
ROC train: 0.740023	val: 0.651648	test: 0.720744
PRC train: 0.616912	val: 0.909706	test: 0.694458

Epoch: 3
Loss: 0.6053551194277642
ROC train: 0.789546	val: 0.669963	test: 0.727873
PRC train: 0.667563	val: 0.919081	test: 0.703122

Epoch: 4
Loss: 0.5738160554733702
ROC train: 0.826824	val: 0.644689	test: 0.732047
PRC train: 0.712869	val: 0.915250	test: 0.705212

Epoch: 5
Loss: 0.5386140233798999
ROC train: 0.848479	val: 0.574359	test: 0.736915
PRC train: 0.736291	val: 0.893468	test: 0.712109

Epoch: 6
Loss: 0.5115210993724871
ROC train: 0.861490	val: 0.581319	test: 0.721788
PRC train: 0.753802	val: 0.880753	test: 0.709343

Epoch: 7
Loss: 0.5042778216113458
ROC train: 0.869101	val: 0.629670	test: 0.721614
PRC train: 0.765776	val: 0.898232	test: 0.711195

Epoch: 8
Loss: 0.48232196513459513
ROC train: 0.890748	val: 0.656044	test: 0.761433
PRC train: 0.810116	val: 0.931777	test: 0.744028

Epoch: 9
Loss: 0.47277505420249566
ROC train: 0.902443	val: 0.651648	test: 0.766475
PRC train: 0.835637	val: 0.930141	test: 0.755912

Epoch: 10
Loss: 0.4633180376460106
ROC train: 0.907494	val: 0.645421	test: 0.771866
PRC train: 0.848132	val: 0.928147	test: 0.763150

Epoch: 11
Loss: 0.4351145631561729
ROC train: 0.916150	val: 0.638828	test: 0.764563
PRC train: 0.862662	val: 0.926370	test: 0.766853

Epoch: 12
Loss: 0.43089563189014013
ROC train: 0.921849	val: 0.653480	test: 0.753260
PRC train: 0.869430	val: 0.931160	test: 0.756514

Epoch: 13
Loss: 0.42221809489167966
ROC train: 0.926898	val: 0.642491	test: 0.771692
PRC train: 0.881809	val: 0.931821	test: 0.771144

Epoch: 14
Loss: 0.4261655345848981
ROC train: 0.932286	val: 0.655311	test: 0.774822
PRC train: 0.889024	val: 0.932591	test: 0.771157

Epoch: 15
Loss: 0.4007969198549695
ROC train: 0.934732	val: 0.668132	test: 0.762824
PRC train: 0.893000	val: 0.936741	test: 0.753580

Epoch: 16
Loss: 0.3800805415530854
ROC train: 0.939070	val: 0.663370	test: 0.762476
PRC train: 0.899047	val: 0.936734	test: 0.747117

Epoch: 17
Loss: 0.3768653990355663
ROC train: 0.947671	val: 0.680586	test: 0.783168
PRC train: 0.915350	val: 0.940150	test: 0.765480

Epoch: 18
Loss: 0.3677260548699787
ROC train: 0.954949	val: 0.650916	test: 0.779517
PRC train: 0.926667	val: 0.933761	test: 0.776642

Epoch: 19
Loss: 0.3527011585596761
ROC train: 0.956330	val: 0.634432	test: 0.760216
PRC train: 0.928075	val: 0.929447	test: 0.765538

Epoch: 20
Loss: 0.3687419393510876
ROC train: 0.960488	val: 0.660440	test: 0.774126
PRC train: 0.937479	val: 0.932443	test: 0.777359

Epoch: 21
Loss: 0.34885307557307327
ROC train: 0.962871	val: 0.671795	test: 0.781082
PRC train: 0.941900	val: 0.933057	test: 0.773282

Epoch: 22
Loss: 0.3402451295346858
ROC train: 0.968410	val: 0.652015	test: 0.786124
PRC train: 0.950706	val: 0.927575	test: 0.759128

Epoch: 23
Loss: 0.31385495655472817
ROC train: 0.969806	val: 0.626374	test: 0.773778
PRC train: 0.952340	val: 0.923461	test: 0.744999

Epoch: 24
Loss: 0.31782638015639164
ROC train: 0.971761	val: 0.635531	test: 0.765432
PRC train: 0.955037	val: 0.926872	test: 0.742270

Epoch: 25
Loss: 0.30295539297675267
ROC train: 0.974050	val: 0.643590	test: 0.761781
PRC train: 0.959000	val: 0.929422	test: 0.749559

Epoch: 26
Loss: 0.2985211168700621
ROC train: 0.978159	val: 0.643590	test: 0.765258
PRC train: 0.966218	val: 0.930026	test: 0.747084

Epoch: 27
Loss: 0.31243379650359265
ROC train: 0.982374	val: 0.657875	test: 0.766128
PRC train: 0.972054	val: 0.934448	test: 0.743424

Epoch: 28
Loss: 0.28687676642624205
ROC train: 0.982988	val: 0.649451	test: 0.769953
PRC train: 0.973511	val: 0.932583	test: 0.749549

Epoch: 29
Loss: 0.28829241331812494
ROC train: 0.983662	val: 0.676557	test: 0.749435
PRC train: 0.974290	val: 0.939874	test: 0.746751

Epoch: 30
Loss: 0.28585759100815833
ROC train: 0.976162	val: 0.660073	test: 0.749609
PRC train: 0.960340	val: 0.931720	test: 0.750387

Epoch: 31
Loss: 0.2865872857585033
ROC train: 0.984175	val: 0.663370	test: 0.745609
PRC train: 0.974701	val: 0.933923	test: 0.750210

Epoch: 32
Loss: 0.24896299980437192
ROC train: 0.988333	val: 0.636630	test: 0.765780Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:3
  input_data_dir: 
  dataset: bace
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/bace/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/bace/noise=0.2/bace_scaff_4_26-05_11-28-51  ]
[ Using Seed :  4  ]
[ Using device :  cuda:3  ]
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6946647413540039
ROC train: 0.584386	val: 0.536996	test: 0.660581
PRC train: 0.450161	val: 0.839882	test: 0.622139

Epoch: 2
Loss: 0.6658041423516703
ROC train: 0.638245	val: 0.556777	test: 0.701617
PRC train: 0.480866	val: 0.872566	test: 0.697310

Epoch: 3
Loss: 0.6548409837672315
ROC train: 0.666610	val: 0.598901	test: 0.714484
PRC train: 0.523787	val: 0.898045	test: 0.744089

Epoch: 4
Loss: 0.6375365862101762
ROC train: 0.692894	val: 0.695238	test: 0.705095
PRC train: 0.573155	val: 0.934506	test: 0.734165

Epoch: 5
Loss: 0.6226227100993379
ROC train: 0.727052	val: 0.694872	test: 0.694836
PRC train: 0.620304	val: 0.935200	test: 0.725466

Epoch: 6
Loss: 0.6147317276419847
ROC train: 0.765888	val: 0.701832	test: 0.704573
PRC train: 0.671371	val: 0.937997	test: 0.742278

Epoch: 7
Loss: 0.5971424170263879
ROC train: 0.796224	val: 0.683150	test: 0.714311
PRC train: 0.712615	val: 0.928966	test: 0.737662

Epoch: 8
Loss: 0.5822450150267389
ROC train: 0.811795	val: 0.667033	test: 0.705443
PRC train: 0.732901	val: 0.919500	test: 0.713531

Epoch: 9
Loss: 0.561974789739121
ROC train: 0.826758	val: 0.654212	test: 0.725091
PRC train: 0.753071	val: 0.918174	test: 0.742822

Epoch: 10
Loss: 0.5312003936211853
ROC train: 0.836487	val: 0.636996	test: 0.719701
PRC train: 0.768899	val: 0.907221	test: 0.744412

Epoch: 11
Loss: 0.5387152613820589
ROC train: 0.860186	val: 0.620147	test: 0.720223
PRC train: 0.796681	val: 0.896921	test: 0.734834

Epoch: 12
Loss: 0.5220805151777513
ROC train: 0.873938	val: 0.583150	test: 0.738306
PRC train: 0.820066	val: 0.883391	test: 0.759550

Epoch: 13
Loss: 0.5031664646740852
ROC train: 0.893696	val: 0.585714	test: 0.730829
PRC train: 0.844062	val: 0.883359	test: 0.754204

Epoch: 14
Loss: 0.4881112748913436
ROC train: 0.910596	val: 0.599267	test: 0.714658
PRC train: 0.864112	val: 0.887644	test: 0.729697

Epoch: 15
Loss: 0.47899625148810243
ROC train: 0.921975	val: 0.595238	test: 0.735176
PRC train: 0.882313	val: 0.882669	test: 0.741244

Epoch: 16
Loss: 0.462650051284208
ROC train: 0.929315	val: 0.571429	test: 0.733264
PRC train: 0.899338	val: 0.865465	test: 0.751818

Epoch: 17
Loss: 0.4613144471574603
ROC train: 0.942246	val: 0.561538	test: 0.740045
PRC train: 0.920466	val: 0.858412	test: 0.760820

Epoch: 18
Loss: 0.42608469397724924
ROC train: 0.945945	val: 0.583516	test: 0.735003
PRC train: 0.925018	val: 0.864884	test: 0.751339

Epoch: 19
Loss: 0.42544540034610223
ROC train: 0.952697	val: 0.602564	test: 0.719527
PRC train: 0.932340	val: 0.890746	test: 0.733312

Epoch: 20
Loss: 0.40624422304065444
ROC train: 0.961772	val: 0.623810	test: 0.730134
PRC train: 0.944789	val: 0.907211	test: 0.739244

Epoch: 21
Loss: 0.3890785744817793
ROC train: 0.970753	val: 0.619414	test: 0.777082
PRC train: 0.957864	val: 0.901947	test: 0.784401

Epoch: 22
Loss: 0.36135773044212827
ROC train: 0.976444	val: 0.595604	test: 0.782473
PRC train: 0.965585	val: 0.877144	test: 0.786569

Epoch: 23
Loss: 0.3705128910063179
ROC train: 0.977922	val: 0.573993	test: 0.770475
PRC train: 0.967046	val: 0.868773	test: 0.790833

Epoch: 24
Loss: 0.3609710263055036
ROC train: 0.980257	val: 0.553846	test: 0.782473
PRC train: 0.970887	val: 0.857281	test: 0.801567

Epoch: 25
Loss: 0.34069401263773197
ROC train: 0.979372	val: 0.593040	test: 0.792384
PRC train: 0.969110	val: 0.869670	test: 0.788562

Epoch: 26
Loss: 0.33942380876623224
ROC train: 0.982138	val: 0.624908	test: 0.773952
PRC train: 0.972649	val: 0.900912	test: 0.773298

Epoch: 27
Loss: 0.34050958226093486
ROC train: 0.987820	val: 0.613187	test: 0.770822
PRC train: 0.981131	val: 0.895116	test: 0.766498

Epoch: 28
Loss: 0.3160422591501087
ROC train: 0.990151	val: 0.563370	test: 0.784385
PRC train: 0.984883	val: 0.849628	test: 0.800117

Epoch: 29
Loss: 0.2960880250260528
ROC train: 0.991361	val: 0.578022	test: 0.788211
PRC train: 0.986824	val: 0.857934	test: 0.805296

Epoch: 30
Loss: 0.30532491752573965
ROC train: 0.995126	val: 0.587546	test: 0.785776
PRC train: 0.992541	val: 0.863865	test: 0.808142

Epoch: 31
Loss: 0.29241160704430763
ROC train: 0.995166	val: 0.577656	test: 0.776213
PRC train: 0.992856	val: 0.860590	test: 0.798598

Epoch: 32
Loss: 0.2556018580759939
ROC train: 0.996162	val: 0.591575	test: 0.766823Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: bace
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/bace/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/bace/noise=0.1/bace_scaff_5_26-05_11-28-51  ]
[ Using Seed :  5  ]
[ Using device :  cuda:2  ]
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6788925223028843
ROC train: 0.660057	val: 0.577656	test: 0.636933
PRC train: 0.529424	val: 0.898946	test: 0.617514

Epoch: 2
Loss: 0.6461560771563989
ROC train: 0.715853	val: 0.610623	test: 0.651539
PRC train: 0.588296	val: 0.903494	test: 0.643992

Epoch: 3
Loss: 0.6210893143847483
ROC train: 0.746281	val: 0.648718	test: 0.655190
PRC train: 0.621009	val: 0.909086	test: 0.648204

Epoch: 4
Loss: 0.601674980285211
ROC train: 0.774546	val: 0.627106	test: 0.681968
PRC train: 0.651775	val: 0.908059	test: 0.677206

Epoch: 5
Loss: 0.586327636243001
ROC train: 0.796969	val: 0.590110	test: 0.715006
PRC train: 0.676987	val: 0.903449	test: 0.697922

Epoch: 6
Loss: 0.5675865750263202
ROC train: 0.817263	val: 0.551282	test: 0.717440
PRC train: 0.704081	val: 0.891615	test: 0.704709

Epoch: 7
Loss: 0.5548649226813667
ROC train: 0.840398	val: 0.592308	test: 0.717093
PRC train: 0.737587	val: 0.900770	test: 0.698556

Epoch: 8
Loss: 0.5397878812036303
ROC train: 0.865168	val: 0.624542	test: 0.720049
PRC train: 0.776628	val: 0.912986	test: 0.685516

Epoch: 9
Loss: 0.5161741396371278
ROC train: 0.879247	val: 0.626740	test: 0.712050
PRC train: 0.800056	val: 0.917095	test: 0.678876

Epoch: 10
Loss: 0.5130666554310626
ROC train: 0.891441	val: 0.653480	test: 0.722135
PRC train: 0.821628	val: 0.925758	test: 0.685715

Epoch: 11
Loss: 0.49703541304833954
ROC train: 0.901096	val: 0.602198	test: 0.731873
PRC train: 0.836831	val: 0.915830	test: 0.705066

Epoch: 12
Loss: 0.4942564251521378
ROC train: 0.906895	val: 0.576190	test: 0.730829
PRC train: 0.846907	val: 0.902816	test: 0.711096

Epoch: 13
Loss: 0.45851096548433173
ROC train: 0.918716	val: 0.633333	test: 0.740567
PRC train: 0.868500	val: 0.929084	test: 0.705242

Epoch: 14
Loss: 0.4532078296426816
ROC train: 0.927283	val: 0.652015	test: 0.737959
PRC train: 0.883341	val: 0.933708	test: 0.708557

Epoch: 15
Loss: 0.44279565701449497
ROC train: 0.930508	val: 0.641758	test: 0.732220
PRC train: 0.891221	val: 0.926814	test: 0.722124

Epoch: 16
Loss: 0.4305353142188654
ROC train: 0.943288	val: 0.636996	test: 0.731873
PRC train: 0.906914	val: 0.931699	test: 0.711283

Epoch: 17
Loss: 0.4250473471127044
ROC train: 0.952480	val: 0.633333	test: 0.744392
PRC train: 0.921709	val: 0.930397	test: 0.721326

Epoch: 18
Loss: 0.4095342802862496
ROC train: 0.953739	val: 0.665201	test: 0.744045
PRC train: 0.926612	val: 0.936054	test: 0.732465

Epoch: 19
Loss: 0.4021339740588994
ROC train: 0.950682	val: 0.649451	test: 0.738480
PRC train: 0.920193	val: 0.932182	test: 0.734371

Epoch: 20
Loss: 0.39603483437266646
ROC train: 0.963319	val: 0.658608	test: 0.763519
PRC train: 0.941830	val: 0.934133	test: 0.746511

Epoch: 21
Loss: 0.35926158116577034
ROC train: 0.968082	val: 0.658974	test: 0.776213
PRC train: 0.950003	val: 0.934606	test: 0.754150

Epoch: 22
Loss: 0.34945653516276043
ROC train: 0.970622	val: 0.635897	test: 0.776734
PRC train: 0.953561	val: 0.929547	test: 0.759198

Epoch: 23
Loss: 0.3180020681362671
ROC train: 0.975243	val: 0.632967	test: 0.771518
PRC train: 0.960159	val: 0.928407	test: 0.758744

Epoch: 24
Loss: 0.33025409993421395
ROC train: 0.978556	val: 0.628205	test: 0.790819
PRC train: 0.964914	val: 0.929133	test: 0.776208

Epoch: 25
Loss: 0.3168227015971442
ROC train: 0.980685	val: 0.616484	test: 0.802817
PRC train: 0.968176	val: 0.926031	test: 0.786533

Epoch: 26
Loss: 0.2842795407757476
ROC train: 0.984187	val: 0.628571	test: 0.800209
PRC train: 0.974859	val: 0.928010	test: 0.791548

Epoch: 27
Loss: 0.31596461368159917
ROC train: 0.989866	val: 0.649451	test: 0.791862
PRC train: 0.983977	val: 0.934376	test: 0.784680

Epoch: 28
Loss: 0.2697599158915317
ROC train: 0.989906	val: 0.640659	test: 0.782820
PRC train: 0.984689	val: 0.930962	test: 0.779757

Epoch: 29
Loss: 0.2842650696010042
ROC train: 0.992158	val: 0.638095	test: 0.773778
PRC train: 0.988212	val: 0.931209	test: 0.766136

Epoch: 30
Loss: 0.30111266296864453
ROC train: 0.990791	val: 0.581685	test: 0.760911
PRC train: 0.985913	val: 0.918350	test: 0.764697

Epoch: 31
Loss: 0.2830413305932383
ROC train: 0.993057	val: 0.585348	test: 0.777430
PRC train: 0.988918	val: 0.917427	test: 0.777634

Epoch: 32
Loss: 0.273425849470903
ROC train: 0.993656	val: 0.669597	test: 0.797948Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: bace
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/bace/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/bace/noise=0.05/bace_scaff_6_26-05_11-28-51  ]
[ Using Seed :  6  ]
[ Using device :  cuda:1  ]
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6836701789732176
ROC train: 0.637540	val: 0.538462	test: 0.604417
PRC train: 0.514355	val: 0.864327	test: 0.605614

Epoch: 2
Loss: 0.650323990135694
ROC train: 0.718108	val: 0.517216	test: 0.644236
PRC train: 0.619269	val: 0.878735	test: 0.641545

Epoch: 3
Loss: 0.6131790877306186
ROC train: 0.758359	val: 0.538828	test: 0.671535
PRC train: 0.660163	val: 0.885007	test: 0.658577

Epoch: 4
Loss: 0.5993385077318045
ROC train: 0.792751	val: 0.574359	test: 0.694836
PRC train: 0.694286	val: 0.889247	test: 0.667063

Epoch: 5
Loss: 0.5569234429238674
ROC train: 0.827626	val: 0.592674	test: 0.754999
PRC train: 0.732744	val: 0.898132	test: 0.744738

Epoch: 6
Loss: 0.5459038413494254
ROC train: 0.846484	val: 0.589744	test: 0.774126
PRC train: 0.753435	val: 0.899098	test: 0.769401

Epoch: 7
Loss: 0.5199183394981661
ROC train: 0.865933	val: 0.593407	test: 0.785950
PRC train: 0.776588	val: 0.894615	test: 0.789195

Epoch: 8
Loss: 0.48505717815670596
ROC train: 0.872540	val: 0.609890	test: 0.782820
PRC train: 0.785373	val: 0.891470	test: 0.782495

Epoch: 9
Loss: 0.49903227914497494
ROC train: 0.889578	val: 0.592308	test: 0.774126
PRC train: 0.811710	val: 0.898423	test: 0.769221

Epoch: 10
Loss: 0.47515767070613874
ROC train: 0.897691	val: 0.612088	test: 0.752217
PRC train: 0.829956	val: 0.912572	test: 0.758801

Epoch: 11
Loss: 0.45654595164944556
ROC train: 0.905117	val: 0.633333	test: 0.746827
PRC train: 0.842533	val: 0.921343	test: 0.750235

Epoch: 12
Loss: 0.45458485364572604
ROC train: 0.917451	val: 0.637729	test: 0.760737
PRC train: 0.863170	val: 0.924601	test: 0.774522

Epoch: 13
Loss: 0.4214072580936704
ROC train: 0.925186	val: 0.653846	test: 0.793775
PRC train: 0.881349	val: 0.928935	test: 0.801163

Epoch: 14
Loss: 0.4144580059745211
ROC train: 0.933376	val: 0.664835	test: 0.798470
PRC train: 0.892780	val: 0.932531	test: 0.805653

Epoch: 15
Loss: 0.4010523635731145
ROC train: 0.937888	val: 0.664103	test: 0.778995
PRC train: 0.899664	val: 0.932175	test: 0.785186

Epoch: 16
Loss: 0.3754283615313643
ROC train: 0.944563	val: 0.660440	test: 0.785429
PRC train: 0.907624	val: 0.930978	test: 0.804914

Epoch: 17
Loss: 0.4071772592479241
ROC train: 0.945354	val: 0.653846	test: 0.785776
PRC train: 0.911065	val: 0.929233	test: 0.800172

Epoch: 18
Loss: 0.37120235774071475
ROC train: 0.952001	val: 0.668498	test: 0.792906
PRC train: 0.924205	val: 0.934329	test: 0.786078

Epoch: 19
Loss: 0.37251700193082266
ROC train: 0.957423	val: 0.668132	test: 0.767345
PRC train: 0.930342	val: 0.935405	test: 0.760797

Epoch: 20
Loss: 0.3470204696377436
ROC train: 0.960696	val: 0.653846	test: 0.760563
PRC train: 0.936625	val: 0.931295	test: 0.758660

Epoch: 21
Loss: 0.3584793834177523
ROC train: 0.965128	val: 0.661905	test: 0.794471
PRC train: 0.944260	val: 0.934113	test: 0.791015

Epoch: 22
Loss: 0.32724894697460194
ROC train: 0.963687	val: 0.653480	test: 0.799165
PRC train: 0.942336	val: 0.930134	test: 0.804478

Epoch: 23
Loss: 0.3312734790956261
ROC train: 0.969118	val: 0.661905	test: 0.786124
PRC train: 0.949147	val: 0.932922	test: 0.797497

Epoch: 24
Loss: 0.33847735061831025
ROC train: 0.964552	val: 0.638462	test: 0.744566
PRC train: 0.942007	val: 0.927728	test: 0.749665

Epoch: 25
Loss: 0.3278480201781152
ROC train: 0.971501	val: 0.636264	test: 0.767693
PRC train: 0.953909	val: 0.927299	test: 0.763837

Epoch: 26
Loss: 0.31423686414329766
ROC train: 0.975525	val: 0.636996	test: 0.775517
PRC train: 0.958491	val: 0.926087	test: 0.779958

Epoch: 27
Loss: 0.29155368589258357
ROC train: 0.975274	val: 0.666300	test: 0.751521
PRC train: 0.955828	val: 0.932973	test: 0.766635

Epoch: 28
Loss: 0.3161036860355766
ROC train: 0.980382	val: 0.678755	test: 0.769779
PRC train: 0.965124	val: 0.936974	test: 0.775703

Epoch: 29
Loss: 0.30548387500195445
ROC train: 0.983330	val: 0.676190	test: 0.769953
PRC train: 0.968756	val: 0.936080	test: 0.761181

Epoch: 30
Loss: 0.29229991781797093
ROC train: 0.986592	val: 0.682784	test: 0.794471
PRC train: 0.974228	val: 0.938676	test: 0.790452

Epoch: 31
Loss: 0.29185097664462173
ROC train: 0.985771	val: 0.672161	test: 0.793949
PRC train: 0.973957	val: 0.935996	test: 0.791472

Epoch: 32
Loss: 0.2643201994132767
ROC train: 0.985322	val: 0.662637	test: 0.787167Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:3
  input_data_dir: 
  dataset: bace
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/bace/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/bace/noise=0.2/bace_scaff_5_26-05_11-28-51  ]
[ Using Seed :  5  ]
[ Using device :  cuda:3  ]
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6793079538003164
ROC train: 0.596130	val: 0.589744	test: 0.668927
PRC train: 0.472261	val: 0.898409	test: 0.644326

Epoch: 2
Loss: 0.6625057427748475
ROC train: 0.642169	val: 0.584615	test: 0.653799
PRC train: 0.512904	val: 0.891373	test: 0.620365

Epoch: 3
Loss: 0.6505212584106083
ROC train: 0.674569	val: 0.612088	test: 0.630151
PRC train: 0.549409	val: 0.896598	test: 0.602817

Epoch: 4
Loss: 0.6402507694811355
ROC train: 0.711618	val: 0.603297	test: 0.603721
PRC train: 0.592494	val: 0.896575	test: 0.597021

Epoch: 5
Loss: 0.6301333963586732
ROC train: 0.740445	val: 0.620879	test: 0.589984
PRC train: 0.630071	val: 0.899828	test: 0.580753

Epoch: 6
Loss: 0.6175853829637117
ROC train: 0.765845	val: 0.630037	test: 0.584768
PRC train: 0.668359	val: 0.901680	test: 0.557965

Epoch: 7
Loss: 0.5996450130364154
ROC train: 0.793921	val: 0.630403	test: 0.588941
PRC train: 0.704109	val: 0.901102	test: 0.568526

Epoch: 8
Loss: 0.589339594152338
ROC train: 0.815596	val: 0.615385	test: 0.596592
PRC train: 0.735336	val: 0.900348	test: 0.579661

Epoch: 9
Loss: 0.571734393830807
ROC train: 0.835057	val: 0.607692	test: 0.604417
PRC train: 0.760204	val: 0.898376	test: 0.568202

Epoch: 10
Loss: 0.5660329921005289
ROC train: 0.853205	val: 0.631136	test: 0.619371
PRC train: 0.781556	val: 0.906892	test: 0.570541

Epoch: 11
Loss: 0.5506560625853536
ROC train: 0.868193	val: 0.603297	test: 0.625630
PRC train: 0.799277	val: 0.899519	test: 0.585242

Epoch: 12
Loss: 0.5308056086279919
ROC train: 0.879729	val: 0.599267	test: 0.594679
PRC train: 0.812053	val: 0.892373	test: 0.568130

Epoch: 13
Loss: 0.523422274867355
ROC train: 0.897006	val: 0.615751	test: 0.613806
PRC train: 0.836450	val: 0.898264	test: 0.575122

Epoch: 14
Loss: 0.48727544221774066
ROC train: 0.910331	val: 0.643223	test: 0.649278
PRC train: 0.862778	val: 0.910891	test: 0.591053

Epoch: 15
Loss: 0.5097139486081563
ROC train: 0.931612	val: 0.673993	test: 0.673796
PRC train: 0.892214	val: 0.919246	test: 0.621976

Epoch: 16
Loss: 0.46298678492041045
ROC train: 0.941130	val: 0.683883	test: 0.703356
PRC train: 0.909816	val: 0.925235	test: 0.645506

Epoch: 17
Loss: 0.4537359203170972
ROC train: 0.948896	val: 0.672527	test: 0.720396
PRC train: 0.921964	val: 0.925192	test: 0.657678

Epoch: 18
Loss: 0.44306808612348336
ROC train: 0.958251	val: 0.658974	test: 0.716397
PRC train: 0.935160	val: 0.920467	test: 0.662245

Epoch: 19
Loss: 0.42124293685236563
ROC train: 0.962078	val: 0.639560	test: 0.722831
PRC train: 0.939909	val: 0.918109	test: 0.670912

Epoch: 20
Loss: 0.41371597232810553
ROC train: 0.967332	val: 0.649084	test: 0.744740
PRC train: 0.949024	val: 0.917763	test: 0.699628

Epoch: 21
Loss: 0.40539689832190084
ROC train: 0.975106	val: 0.656044	test: 0.745436
PRC train: 0.962135	val: 0.917026	test: 0.677810

Epoch: 22
Loss: 0.36166561992277535
ROC train: 0.976347	val: 0.636264	test: 0.753434
PRC train: 0.965717	val: 0.914376	test: 0.689859

Epoch: 23
Loss: 0.3925734657932086
ROC train: 0.984506	val: 0.649084	test: 0.760389
PRC train: 0.977324	val: 0.912726	test: 0.714808

Epoch: 24
Loss: 0.36934580535657474
ROC train: 0.988136	val: 0.651648	test: 0.751000
PRC train: 0.982366	val: 0.919109	test: 0.729940

Epoch: 25
Loss: 0.33985444323583053
ROC train: 0.990688	val: 0.644689	test: 0.753608
PRC train: 0.986397	val: 0.916949	test: 0.738308

Epoch: 26
Loss: 0.33368695092988765
ROC train: 0.991133	val: 0.637729	test: 0.753608
PRC train: 0.987667	val: 0.915722	test: 0.732624

Epoch: 27
Loss: 0.32299591209067235
ROC train: 0.993048	val: 0.623077	test: 0.740219
PRC train: 0.990395	val: 0.902423	test: 0.715983

Epoch: 28
Loss: 0.3171849847143238
ROC train: 0.995531	val: 0.639927	test: 0.743523
PRC train: 0.993710	val: 0.910030	test: 0.705052

Epoch: 29
Loss: 0.2926734741645502
ROC train: 0.995294	val: 0.636996	test: 0.747696
PRC train: 0.993110	val: 0.908045	test: 0.704855

Epoch: 30
Loss: 0.28772865457573327
ROC train: 0.996461	val: 0.641026	test: 0.747001
PRC train: 0.994788	val: 0.912106	test: 0.711346

Epoch: 31
Loss: 0.2871416367486813
ROC train: 0.998533	val: 0.636264	test: 0.749087
PRC train: 0.997852	val: 0.909265	test: 0.726132

Epoch: 32
Loss: 0.2745600847282855
ROC train: 0.999061	val: 0.651282	test: 0.766649Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: bace
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/bace/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/bace/noise=0.1/bace_scaff_6_26-05_11-28-51  ]
[ Using Seed :  6  ]
[ Using device :  cuda:2  ]
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6922728119101199
ROC train: 0.597383	val: 0.526740	test: 0.581638
PRC train: 0.475371	val: 0.885593	test: 0.566618

Epoch: 2
Loss: 0.6673272784203158
ROC train: 0.684598	val: 0.558974	test: 0.666841
PRC train: 0.567874	val: 0.890917	test: 0.660643

Epoch: 3
Loss: 0.6372271625812747
ROC train: 0.718490	val: 0.533333	test: 0.665449
PRC train: 0.614457	val: 0.888071	test: 0.657793

Epoch: 4
Loss: 0.6295943435258575
ROC train: 0.741872	val: 0.534066	test: 0.664406
PRC train: 0.642317	val: 0.885008	test: 0.662812

Epoch: 5
Loss: 0.6063520567854608
ROC train: 0.768739	val: 0.543223	test: 0.680230
PRC train: 0.669442	val: 0.884554	test: 0.662202

Epoch: 6
Loss: 0.5855464973850986
ROC train: 0.797100	val: 0.562637	test: 0.691706
PRC train: 0.698639	val: 0.887352	test: 0.679230

Epoch: 7
Loss: 0.563929286215049
ROC train: 0.824980	val: 0.559707	test: 0.707703
PRC train: 0.727512	val: 0.886001	test: 0.708832

Epoch: 8
Loss: 0.5474162938839336
ROC train: 0.846544	val: 0.553846	test: 0.731351
PRC train: 0.754310	val: 0.880913	test: 0.726346

Epoch: 9
Loss: 0.5301617799548038
ROC train: 0.859894	val: 0.545788	test: 0.745957
PRC train: 0.776092	val: 0.878195	test: 0.739522

Epoch: 10
Loss: 0.5103200632224578
ROC train: 0.881781	val: 0.575092	test: 0.754825
PRC train: 0.810273	val: 0.886251	test: 0.754794

Epoch: 11
Loss: 0.5005113466396557
ROC train: 0.900919	val: 0.597070	test: 0.756390
PRC train: 0.835841	val: 0.895774	test: 0.751236

Epoch: 12
Loss: 0.48558018691802934
ROC train: 0.908079	val: 0.602198	test: 0.744566
PRC train: 0.842711	val: 0.897465	test: 0.737255

Epoch: 13
Loss: 0.46069153278624836
ROC train: 0.923079	val: 0.578022	test: 0.769084
PRC train: 0.866173	val: 0.892411	test: 0.753369

Epoch: 14
Loss: 0.45179468048535776
ROC train: 0.932705	val: 0.591575	test: 0.785429
PRC train: 0.886267	val: 0.894812	test: 0.785306

Epoch: 15
Loss: 0.4322237008810081
ROC train: 0.937954	val: 0.565934	test: 0.765780
PRC train: 0.899048	val: 0.885949	test: 0.771563

Epoch: 16
Loss: 0.4344543786385232
ROC train: 0.939906	val: 0.582784	test: 0.742132
PRC train: 0.903535	val: 0.891255	test: 0.753367

Epoch: 17
Loss: 0.415761579112222
ROC train: 0.946709	val: 0.610989	test: 0.754999
PRC train: 0.913891	val: 0.908532	test: 0.745640

Epoch: 18
Loss: 0.3925332579713364
ROC train: 0.959326	val: 0.601099	test: 0.775517
PRC train: 0.934619	val: 0.905074	test: 0.769541

Epoch: 19
Loss: 0.37959803503252587
ROC train: 0.966487	val: 0.614286	test: 0.767866
PRC train: 0.945424	val: 0.914404	test: 0.755313

Epoch: 20
Loss: 0.35007915646673904
ROC train: 0.968062	val: 0.638828	test: 0.765954
PRC train: 0.949939	val: 0.921319	test: 0.752217

Epoch: 21
Loss: 0.3374339466248791
ROC train: 0.965745	val: 0.624908	test: 0.769258
PRC train: 0.945679	val: 0.915908	test: 0.739460

Epoch: 22
Loss: 0.3441933065055817
ROC train: 0.976216	val: 0.600733	test: 0.777604
PRC train: 0.962686	val: 0.903984	test: 0.776380

Epoch: 23
Loss: 0.3379997117374446
ROC train: 0.981592	val: 0.614652	test: 0.802991
PRC train: 0.970262	val: 0.913299	test: 0.807187

Epoch: 24
Loss: 0.3112235403495512
ROC train: 0.985168	val: 0.607326	test: 0.784733
PRC train: 0.976979	val: 0.912943	test: 0.794639

Epoch: 25
Loss: 0.3268337183758382
ROC train: 0.987417	val: 0.616484	test: 0.781082
PRC train: 0.980105	val: 0.919727	test: 0.773187

Epoch: 26
Loss: 0.3076928222562625
ROC train: 0.990448	val: 0.623810	test: 0.786298
PRC train: 0.983607	val: 0.922485	test: 0.774078

Epoch: 27
Loss: 0.2981297888989975
ROC train: 0.991264	val: 0.639194	test: 0.789602
PRC train: 0.985196	val: 0.924663	test: 0.784632

Epoch: 28
Loss: 0.2707263248505745
ROC train: 0.992323	val: 0.625275	test: 0.803339
PRC train: 0.987342	val: 0.920815	test: 0.790515

Epoch: 29
Loss: 0.2714644825899409
ROC train: 0.993876	val: 0.619780	test: 0.804730
PRC train: 0.990166	val: 0.918850	test: 0.795958

Epoch: 30
Loss: 0.2505941498723247
ROC train: 0.995248	val: 0.645421	test: 0.812380
PRC train: 0.992394	val: 0.929194	test: 0.810776

Epoch: 31
Loss: 0.2671797693092185
ROC train: 0.984852	val: 0.678388	test: 0.767693
PRC train: 0.978328	val: 0.937343	test: 0.746428

Epoch: 32
Loss: 0.2458197935927795
ROC train: 0.994441	val: 0.661538	test: 0.783342Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: bace
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/bace/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/bace/noise=0.1/bace_scaff_4_26-05_11-28-51  ]
[ Using Seed :  4  ]
[ Using device :  cuda:2  ]
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6947723453273795
ROC train: 0.589623	val: 0.536630	test: 0.629456
PRC train: 0.461121	val: 0.855021	test: 0.647383

Epoch: 2
Loss: 0.6653116012101776
ROC train: 0.673042	val: 0.565568	test: 0.719353
PRC train: 0.531370	val: 0.892186	test: 0.741031

Epoch: 3
Loss: 0.6426540374214218
ROC train: 0.714192	val: 0.626740	test: 0.707355
PRC train: 0.599718	val: 0.923374	test: 0.727163

Epoch: 4
Loss: 0.6219427418619975
ROC train: 0.744800	val: 0.646886	test: 0.679012
PRC train: 0.650482	val: 0.927444	test: 0.694276

Epoch: 5
Loss: 0.6001382366389351
ROC train: 0.776564	val: 0.663736	test: 0.692575
PRC train: 0.695503	val: 0.930350	test: 0.701615

Epoch: 6
Loss: 0.5845217171378676
ROC train: 0.801147	val: 0.645788	test: 0.702834
PRC train: 0.717887	val: 0.922341	test: 0.713584

Epoch: 7
Loss: 0.5703870491997699
ROC train: 0.822260	val: 0.621978	test: 0.717440
PRC train: 0.737555	val: 0.910682	test: 0.720133

Epoch: 8
Loss: 0.545204786013127
ROC train: 0.840382	val: 0.613919	test: 0.722831
PRC train: 0.758402	val: 0.906655	test: 0.711963

Epoch: 9
Loss: 0.5446731279228109
ROC train: 0.852083	val: 0.600366	test: 0.725961
PRC train: 0.774059	val: 0.902934	test: 0.712460

Epoch: 10
Loss: 0.5269351167166165
ROC train: 0.865251	val: 0.605861	test: 0.731177
PRC train: 0.791092	val: 0.899443	test: 0.723191

Epoch: 11
Loss: 0.4948131317043659
ROC train: 0.883687	val: 0.600000	test: 0.743871
PRC train: 0.816257	val: 0.896407	test: 0.723322

Epoch: 12
Loss: 0.49828831950141783
ROC train: 0.897098	val: 0.588645	test: 0.742480
PRC train: 0.838560	val: 0.897644	test: 0.718634

Epoch: 13
Loss: 0.48053192319835586
ROC train: 0.910499	val: 0.595238	test: 0.736915
PRC train: 0.859860	val: 0.897906	test: 0.719732

Epoch: 14
Loss: 0.4450489803316498
ROC train: 0.924107	val: 0.612454	test: 0.731873
PRC train: 0.880217	val: 0.901624	test: 0.714019

Epoch: 15
Loss: 0.42395938471324934
ROC train: 0.933399	val: 0.575824	test: 0.752043
PRC train: 0.895967	val: 0.891314	test: 0.733249

Epoch: 16
Loss: 0.4321276704730181
ROC train: 0.945539	val: 0.590476	test: 0.749609
PRC train: 0.916532	val: 0.897018	test: 0.724446

Epoch: 17
Loss: 0.4318325599607201
ROC train: 0.954960	val: 0.603297	test: 0.737959
PRC train: 0.931097	val: 0.905318	test: 0.720991

Epoch: 18
Loss: 0.4024753065099448
ROC train: 0.959655	val: 0.623810	test: 0.743523
PRC train: 0.939824	val: 0.913024	test: 0.709044

Epoch: 19
Loss: 0.3921587211501389
ROC train: 0.962774	val: 0.610623	test: 0.746131
PRC train: 0.944144	val: 0.908168	test: 0.703792

Epoch: 20
Loss: 0.38305350314496495
ROC train: 0.965825	val: 0.624908	test: 0.741784
PRC train: 0.946965	val: 0.907162	test: 0.703994

Epoch: 21
Loss: 0.36695025383646057
ROC train: 0.968807	val: 0.620879	test: 0.736220
PRC train: 0.951884	val: 0.906700	test: 0.700374

Epoch: 22
Loss: 0.3489153338680782
ROC train: 0.974763	val: 0.613919	test: 0.725961
PRC train: 0.961356	val: 0.905265	test: 0.703274

Epoch: 23
Loss: 0.3294515161756905
ROC train: 0.980528	val: 0.578022	test: 0.742132
PRC train: 0.969930	val: 0.886686	test: 0.729842

Epoch: 24
Loss: 0.34624305869889005
ROC train: 0.983282	val: 0.568864	test: 0.748739
PRC train: 0.973712	val: 0.874437	test: 0.748175

Epoch: 25
Loss: 0.32100788358513316
ROC train: 0.984986	val: 0.589377	test: 0.737263
PRC train: 0.977466	val: 0.883148	test: 0.723381

Epoch: 26
Loss: 0.2992509820068453
ROC train: 0.986555	val: 0.600733	test: 0.719875
PRC train: 0.979762	val: 0.891451	test: 0.703459

Epoch: 27
Loss: 0.3020388755703108
ROC train: 0.991635	val: 0.564469	test: 0.720918
PRC train: 0.987372	val: 0.878801	test: 0.703946

Epoch: 28
Loss: 0.2832276578410581
ROC train: 0.991595	val: 0.574725	test: 0.711702
PRC train: 0.987412	val: 0.893543	test: 0.702258

Epoch: 29
Loss: 0.2860777436748053
ROC train: 0.989632	val: 0.580220	test: 0.707007
PRC train: 0.984554	val: 0.892620	test: 0.702764

Epoch: 30
Loss: 0.2629822996046913
ROC train: 0.991455	val: 0.567766	test: 0.708920
PRC train: 0.986946	val: 0.871441	test: 0.707833

Epoch: 31
Loss: 0.2733897859305599
ROC train: 0.995037	val: 0.573993	test: 0.718310
PRC train: 0.992582	val: 0.889078	test: 0.706475

Epoch: 32
Loss: 0.26020999341438145
ROC train: 0.996159	val: 0.579853	test: 0.736394Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:3
  input_data_dir: 
  dataset: bace
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/bace/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/bace/noise=0.2/bace_scaff_6_26-05_11-28-51  ]
[ Using Seed :  6  ]
[ Using device :  cuda:3  ]
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6917223880586499
ROC train: 0.548967	val: 0.402930	test: 0.592766
PRC train: 0.434552	val: 0.831005	test: 0.574511

Epoch: 2
Loss: 0.671540244208434
ROC train: 0.615428	val: 0.484615	test: 0.643714
PRC train: 0.480809	val: 0.865824	test: 0.618312

Epoch: 3
Loss: 0.6540481650737269
ROC train: 0.655967	val: 0.520513	test: 0.642149
PRC train: 0.515144	val: 0.885152	test: 0.618017

Epoch: 4
Loss: 0.6514705398678677
ROC train: 0.683936	val: 0.524542	test: 0.626674
PRC train: 0.545692	val: 0.889457	test: 0.621204

Epoch: 5
Loss: 0.6374194564548376
ROC train: 0.708259	val: 0.553846	test: 0.675535
PRC train: 0.575473	val: 0.895976	test: 0.656402

Epoch: 6
Loss: 0.626633642326345
ROC train: 0.736952	val: 0.557143	test: 0.664058
PRC train: 0.613290	val: 0.894586	test: 0.641128

Epoch: 7
Loss: 0.6080350675265749
ROC train: 0.757058	val: 0.561905	test: 0.641628
PRC train: 0.644792	val: 0.894211	test: 0.623784

Epoch: 8
Loss: 0.5928836386779134
ROC train: 0.776527	val: 0.581319	test: 0.644931
PRC train: 0.673164	val: 0.895611	test: 0.632273

Epoch: 9
Loss: 0.5786630341171491
ROC train: 0.794144	val: 0.601832	test: 0.645801
PRC train: 0.692371	val: 0.897670	test: 0.634054

Epoch: 10
Loss: 0.5728991587930309
ROC train: 0.818736	val: 0.601099	test: 0.664232
PRC train: 0.722309	val: 0.898543	test: 0.649294

Epoch: 11
Loss: 0.56085006939608
ROC train: 0.842100	val: 0.605128	test: 0.682316
PRC train: 0.753417	val: 0.900018	test: 0.668674

Epoch: 12
Loss: 0.5543800439588422
ROC train: 0.856296	val: 0.614652	test: 0.672057
PRC train: 0.776950	val: 0.901158	test: 0.661107

Epoch: 13
Loss: 0.5321197278937223
ROC train: 0.882588	val: 0.619414	test: 0.678838
PRC train: 0.820000	val: 0.912107	test: 0.671033

Epoch: 14
Loss: 0.5279944614576451
ROC train: 0.901270	val: 0.626740	test: 0.683359
PRC train: 0.851893	val: 0.913469	test: 0.685689

Epoch: 15
Loss: 0.4990136452771651
ROC train: 0.913582	val: 0.639194	test: 0.703008
PRC train: 0.869384	val: 0.922823	test: 0.711392

Epoch: 16
Loss: 0.4760994974126843
ROC train: 0.926538	val: 0.643223	test: 0.714484
PRC train: 0.883734	val: 0.926280	test: 0.715501

Epoch: 17
Loss: 0.4902483249938442
ROC train: 0.931695	val: 0.657875	test: 0.710485
PRC train: 0.893889	val: 0.930080	test: 0.700704

Epoch: 18
Loss: 0.45472179381515493
ROC train: 0.943119	val: 0.659707	test: 0.689619
PRC train: 0.912501	val: 0.922912	test: 0.681013

Epoch: 19
Loss: 0.45286526987574904
ROC train: 0.947474	val: 0.702564	test: 0.682664
PRC train: 0.918757	val: 0.937717	test: 0.680777

Epoch: 20
Loss: 0.40866183739454043
ROC train: 0.958699	val: 0.708425	test: 0.695531
PRC train: 0.934735	val: 0.943476	test: 0.697897

Epoch: 21
Loss: 0.4116155713654372
ROC train: 0.963696	val: 0.673260	test: 0.695531
PRC train: 0.945248	val: 0.932001	test: 0.688288

Epoch: 22
Loss: 0.38622873202614305
ROC train: 0.966815	val: 0.636996	test: 0.701791
PRC train: 0.948988	val: 0.920977	test: 0.695423

Epoch: 23
Loss: 0.4141464523498737
ROC train: 0.968493	val: 0.627839	test: 0.713963
PRC train: 0.948197	val: 0.916062	test: 0.671263

Epoch: 24
Loss: 0.3678229493134654
ROC train: 0.974381	val: 0.686081	test: 0.729612
PRC train: 0.959780	val: 0.935751	test: 0.711092

Epoch: 25
Loss: 0.375963235509187
ROC train: 0.980999	val: 0.664103	test: 0.747001
PRC train: 0.970548	val: 0.929199	test: 0.732884

Epoch: 26
Loss: 0.3377417818259587
ROC train: 0.985283	val: 0.643956	test: 0.732047
PRC train: 0.977011	val: 0.915760	test: 0.702529

Epoch: 27
Loss: 0.3420427524791522
ROC train: 0.989024	val: 0.639927	test: 0.723179
PRC train: 0.982641	val: 0.913422	test: 0.690699

Epoch: 28
Loss: 0.34409851937761815
ROC train: 0.987648	val: 0.647619	test: 0.724570
PRC train: 0.979617	val: 0.919391	test: 0.717816

Epoch: 29
Loss: 0.31043111408623814
ROC train: 0.987583	val: 0.682784	test: 0.731177
PRC train: 0.981940	val: 0.934061	test: 0.730831

Epoch: 30
Loss: 0.30739221146891643
ROC train: 0.992471	val: 0.616850	test: 0.736915
PRC train: 0.988791	val: 0.907207	test: 0.697388

Epoch: 31
Loss: 0.31945670188552044
ROC train: 0.993122	val: 0.593773	test: 0.758477
PRC train: 0.989899	val: 0.902016	test: 0.724406

Epoch: 32
Loss: 0.3178614735561134
ROC train: 0.994586	val: 0.587912	test: 0.770649
PRC train: 0.915727	val: 0.932413	test: 0.739660

Epoch: 34
Loss: 0.3366088431392198
ROC train: 0.943382	val: 0.676190	test: 0.756564
PRC train: 0.910265	val: 0.930596	test: 0.735519

Epoch: 35
Loss: 0.3335706924080276
ROC train: 0.949155	val: 0.668498	test: 0.749609
PRC train: 0.917287	val: 0.930984	test: 0.736902

Epoch: 36
Loss: 0.32802478758924264
ROC train: 0.949991	val: 0.681685	test: 0.757607
PRC train: 0.919854	val: 0.933239	test: 0.743139

Epoch: 37
Loss: 0.34077027288259354
ROC train: 0.950873	val: 0.684982	test: 0.763346
PRC train: 0.922949	val: 0.930974	test: 0.738786

Epoch: 38
Loss: 0.3314103023064877
ROC train: 0.953944	val: 0.668498	test: 0.772040
PRC train: 0.928322	val: 0.927375	test: 0.757408

Epoch: 39
Loss: 0.3262114948141658
ROC train: 0.955579	val: 0.659341	test: 0.758998
PRC train: 0.931422	val: 0.925880	test: 0.745900

Epoch: 40
Loss: 0.31279590708565214
ROC train: 0.956635	val: 0.651282	test: 0.764737
PRC train: 0.932569	val: 0.924705	test: 0.742977

Epoch: 41
Loss: 0.3398225074182491
ROC train: 0.955123	val: 0.663004	test: 0.775170
PRC train: 0.930590	val: 0.927433	test: 0.764867

Epoch: 42
Loss: 0.32159622221504625
ROC train: 0.956632	val: 0.679487	test: 0.776734
PRC train: 0.932362	val: 0.929344	test: 0.761347

Epoch: 43
Loss: 0.31295843062468043
ROC train: 0.957337	val: 0.680586	test: 0.773257
PRC train: 0.933045	val: 0.927862	test: 0.750003

Epoch: 44
Loss: 0.30690159200364603
ROC train: 0.959349	val: 0.662271	test: 0.761781
PRC train: 0.935572	val: 0.925337	test: 0.742945

Epoch: 45
Loss: 0.2784491924686917
ROC train: 0.960665	val: 0.675824	test: 0.747522
PRC train: 0.937649	val: 0.930591	test: 0.737587

Epoch: 46
Loss: 0.3019692299335448
ROC train: 0.959244	val: 0.677289	test: 0.737437
PRC train: 0.934290	val: 0.931192	test: 0.733542

Epoch: 47
Loss: 0.2985805013917532
ROC train: 0.961107	val: 0.677289	test: 0.718832
PRC train: 0.938441	val: 0.930257	test: 0.708213

Epoch: 48
Loss: 0.2976200985092089
ROC train: 0.962491	val: 0.665201	test: 0.727526
PRC train: 0.943401	val: 0.926210	test: 0.715160

Epoch: 49
Loss: 0.30998922175156685
ROC train: 0.964446	val: 0.659341	test: 0.737959
PRC train: 0.946735	val: 0.925978	test: 0.726749

Epoch: 50
Loss: 0.30063730761854335
ROC train: 0.962734	val: 0.667399	test: 0.722483
PRC train: 0.943535	val: 0.925383	test: 0.713714

Epoch: 51
Loss: 0.28751061034825415
ROC train: 0.964646	val: 0.656044	test: 0.735176
PRC train: 0.946061	val: 0.921352	test: 0.724124

Epoch: 52
Loss: 0.28518395875912395
ROC train: 0.966998	val: 0.645055	test: 0.748218
PRC train: 0.949717	val: 0.921187	test: 0.741582

Epoch: 53
Loss: 0.28429373491439475
ROC train: 0.967232	val: 0.658242	test: 0.732916
PRC train: 0.949275	val: 0.923467	test: 0.731218

Epoch: 54
Loss: 0.29443791508705053
ROC train: 0.967429	val: 0.678022	test: 0.748739
PRC train: 0.949464	val: 0.931309	test: 0.742596

Epoch: 55
Loss: 0.2946571456943984
ROC train: 0.965739	val: 0.669597	test: 0.751000
PRC train: 0.945186	val: 0.930505	test: 0.740803

Epoch: 56
Loss: 0.30047894877031595
ROC train: 0.968610	val: 0.671429	test: 0.729438
PRC train: 0.950939	val: 0.929042	test: 0.721504

Epoch: 57
Loss: 0.29405236881297
ROC train: 0.967026	val: 0.669597	test: 0.720049
PRC train: 0.948188	val: 0.926811	test: 0.711425

Epoch: 58
Loss: 0.27774263031510993
ROC train: 0.964746	val: 0.680586	test: 0.721961
PRC train: 0.944251	val: 0.930563	test: 0.720024

Epoch: 59
Loss: 0.2679614151446084
ROC train: 0.965745	val: 0.688278	test: 0.721266
PRC train: 0.946841	val: 0.933698	test: 0.714947

Epoch: 60
Loss: 0.27334911709505
ROC train: 0.969269	val: 0.695971	test: 0.722657
PRC train: 0.952471	val: 0.936877	test: 0.722146

Epoch: 61
Loss: 0.2792328046053068
ROC train: 0.970762	val: 0.672161	test: 0.724744
PRC train: 0.955672	val: 0.931919	test: 0.723445

Epoch: 62
Loss: 0.27773116915046314
ROC train: 0.969943	val: 0.671062	test: 0.713267
PRC train: 0.953875	val: 0.929335	test: 0.719023

Epoch: 63
Loss: 0.2761092009573209
ROC train: 0.971735	val: 0.673626	test: 0.710311
PRC train: 0.956998	val: 0.928155	test: 0.717111

Epoch: 64
Loss: 0.27450745518720754
ROC train: 0.973014	val: 0.672161	test: 0.731351
PRC train: 0.959885	val: 0.929999	test: 0.725185

Epoch: 65
Loss: 0.27571485961226655
ROC train: 0.973057	val: 0.680220	test: 0.750826
PRC train: 0.958506	val: 0.929207	test: 0.747671

Epoch: 66
Loss: 0.2715636775122664
ROC train: 0.973687	val: 0.686447	test: 0.723526
PRC train: 0.959577	val: 0.931089	test: 0.717015

Epoch: 67
Loss: 0.27228788537824433
ROC train: 0.975197	val: 0.671795	test: 0.726135
PRC train: 0.961563	val: 0.930855	test: 0.725282

Epoch: 68
Loss: 0.27874715346321205
ROC train: 0.973884	val: 0.690476	test: 0.721614
PRC train: 0.959840	val: 0.932777	test: 0.727687

Epoch: 69
Loss: 0.26414851467523953
ROC train: 0.975000	val: 0.682784	test: 0.725265
PRC train: 0.961439	val: 0.933782	test: 0.721210

Epoch: 70
Loss: 0.26091730600688956
ROC train: 0.975582	val: 0.687546	test: 0.720049
PRC train: 0.962125	val: 0.935944	test: 0.720032

Epoch: 71
Loss: 0.24880672107846866
ROC train: 0.974826	val: 0.669963	test: 0.697966
PRC train: 0.959923	val: 0.931144	test: 0.708241

Epoch: 72
Loss: 0.2445698141530183
ROC train: 0.977460	val: 0.673993	test: 0.724744
PRC train: 0.965330	val: 0.930872	test: 0.728346

Epoch: 73
Loss: 0.25481176437141473
ROC train: 0.977403	val: 0.665568	test: 0.728221
PRC train: 0.966100	val: 0.926090	test: 0.722445

Epoch: 74
Loss: 0.2528762081930406
ROC train: 0.978410	val: 0.675824	test: 0.724396
PRC train: 0.967441	val: 0.926694	test: 0.712165

Epoch: 75
Loss: 0.25209154941616
ROC train: 0.976450	val: 0.686813	test: 0.719875
PRC train: 0.964597	val: 0.930375	test: 0.692849

Epoch: 76
Loss: 0.2653060187944515
ROC train: 0.975631	val: 0.674725	test: 0.738132
PRC train: 0.962787	val: 0.931300	test: 0.726965

Epoch: 77
Loss: 0.27698226423890854
ROC train: 0.977063	val: 0.650916	test: 0.743175
PRC train: 0.966354	val: 0.922502	test: 0.737528

Epoch: 78
Loss: 0.2352537677697069
ROC train: 0.979535	val: 0.661538	test: 0.723352
PRC train: 0.970052	val: 0.924362	test: 0.717645

Epoch: 79
Loss: 0.2670208622324424
ROC train: 0.979267	val: 0.644322	test: 0.716919
PRC train: 0.967856	val: 0.921528	test: 0.712521

Epoch: 80
Loss: 0.2558967408201259
ROC train: 0.978770	val: 0.643223	test: 0.733785
PRC train: 0.966454	val: 0.922449	test: 0.728177

Epoch: 81
Loss: 0.24750337284632376
ROC train: 0.979455	val: 0.675092	test: 0.728221
PRC train: 0.968355	val: 0.932552	test: 0.722097

Epoch: 82
Loss: 0.25188359403097493
ROC train: 0.979335	val: 0.686081	test: 0.711007
PRC train: 0.968871	val: 0.934667	test: 0.701175

Epoch: 83
Loss: 0.2534045067287039
ROC train: 0.980263	val: 0.671062	test: 0.716223
PRC train: 0.970232	val: 0.928720	test: 0.702712

Epoch: 84
Loss: 0.2490806783545902
ROC train: 0.981787	val: 0.662637	test: 0.714658
PRC train: 0.973105	val: 0.923392	test: 0.699585

Epoch: 85
Loss: 0.26108158569356
ROC train: 0.980899	val: 0.677656	test: 0.705443
PRC train: 0.971800	val: 0.929726	test: 0.691019

Epoch: 86
Loss: 0.2418043099187118
ROC train: 0.978881	val: 0.683516	test: 0.716571
PRC train: 0.969014	val: 0.930146	test: 0.703608

Epoch: 87
Loss: 0.23552253784572025
ROC train: 0.981998	val: 0.682418	test: 0.717788
PRC train: 0.973304	val: 0.931730	test: 0.707903

Epoch: 88
Loss: 0.2406696879284504
ROC train: 0.982939	val: 0.663004	test: 0.717614
PRC train: 0.975032	val: 0.928753	test: 0.702636

Epoch: 89
Loss: 0.22826766578355953
ROC train: 0.982295	val: 0.662271	test: 0.715354
PRC train: 0.973993	val: 0.930718	test: 0.704936

Epoch: 90
Loss: 0.22984950456606423
ROC train: 0.980782	val: 0.691209	test: 0.712224
PRC train: 0.971014	val: 0.934127	test: 0.710342

Epoch: 91
Loss: 0.22047273748045426
ROC train: 0.981544	val: 0.689011	test: 0.713267
PRC train: 0.972111	val: 0.935902	test: 0.700199

Epoch: 92
Loss: 0.2295224219410496
ROC train: 0.983590	val: 0.673260	test: 0.721266
PRC train: 0.975552	val: 0.931367	test: 0.704191

Epoch: 93
Loss: 0.23178606622014475
ROC train: 0.984840	val: 0.673626	test: 0.724744
PRC train: 0.977700	val: 0.929683	test: 0.707952

Epoch: 94
Loss: 0.21828954367397504
ROC train: 0.981878	val: 0.676557	test: 0.717788
PRC train: 0.912510	val: 0.922292	test: 0.727395

Epoch: 34
Loss: 0.3194460753896712
ROC train: 0.946618	val: 0.647619	test: 0.753434
PRC train: 0.914387	val: 0.917151	test: 0.733443

Epoch: 35
Loss: 0.3461297608653898
ROC train: 0.948562	val: 0.657143	test: 0.759172
PRC train: 0.918005	val: 0.915539	test: 0.760562

Epoch: 36
Loss: 0.34475315291314496
ROC train: 0.950842	val: 0.643956	test: 0.772561
PRC train: 0.922301	val: 0.912726	test: 0.768362

Epoch: 37
Loss: 0.31542677630080995
ROC train: 0.947771	val: 0.663736	test: 0.765954
PRC train: 0.917913	val: 0.919493	test: 0.752860

Epoch: 38
Loss: 0.3317280706280144
ROC train: 0.950362	val: 0.669597	test: 0.770996
PRC train: 0.922775	val: 0.923194	test: 0.762192

Epoch: 39
Loss: 0.3152865041073681
ROC train: 0.951527	val: 0.668498	test: 0.774648
PRC train: 0.924169	val: 0.924445	test: 0.761912

Epoch: 40
Loss: 0.31544030521117616
ROC train: 0.950850	val: 0.668864	test: 0.765432
PRC train: 0.923696	val: 0.924081	test: 0.750927

Epoch: 41
Loss: 0.3495438917389482
ROC train: 0.954135	val: 0.658974	test: 0.757955
PRC train: 0.928613	val: 0.922218	test: 0.748270

Epoch: 42
Loss: 0.3108172878932849
ROC train: 0.955271	val: 0.647985	test: 0.767519
PRC train: 0.932046	val: 0.919917	test: 0.765037

Epoch: 43
Loss: 0.3122438432018432
ROC train: 0.956564	val: 0.634799	test: 0.775343
PRC train: 0.933748	val: 0.913304	test: 0.766465

Epoch: 44
Loss: 0.31091604350085944
ROC train: 0.957628	val: 0.631136	test: 0.765432
PRC train: 0.934385	val: 0.910724	test: 0.757792

Epoch: 45
Loss: 0.30880411809705394
ROC train: 0.955788	val: 0.653846	test: 0.746131
PRC train: 0.933382	val: 0.919903	test: 0.741803

Epoch: 46
Loss: 0.3003429035961037
ROC train: 0.960086	val: 0.633700	test: 0.749609
PRC train: 0.938420	val: 0.907962	test: 0.742431

Epoch: 47
Loss: 0.31154535281145407
ROC train: 0.961881	val: 0.621978	test: 0.754651
PRC train: 0.941421	val: 0.904619	test: 0.732724

Epoch: 48
Loss: 0.3112991642076114
ROC train: 0.961076	val: 0.623077	test: 0.744740
PRC train: 0.940518	val: 0.905665	test: 0.724899

Epoch: 49
Loss: 0.2960357212804737
ROC train: 0.958955	val: 0.643223	test: 0.748913
PRC train: 0.938123	val: 0.909026	test: 0.739779

Epoch: 50
Loss: 0.2907491511289054
ROC train: 0.959432	val: 0.646154	test: 0.733264
PRC train: 0.939244	val: 0.916114	test: 0.727814

Epoch: 51
Loss: 0.3145239093686064
ROC train: 0.958430	val: 0.643956	test: 0.720223
PRC train: 0.936343	val: 0.917792	test: 0.718966

Epoch: 52
Loss: 0.2903115678810667
ROC train: 0.963211	val: 0.620513	test: 0.757781
PRC train: 0.944281	val: 0.911545	test: 0.748905

Epoch: 53
Loss: 0.28514857942890426
ROC train: 0.963627	val: 0.622344	test: 0.754130
PRC train: 0.944659	val: 0.910563	test: 0.744235

Epoch: 54
Loss: 0.2909678571623958
ROC train: 0.965417	val: 0.620879	test: 0.748739
PRC train: 0.947183	val: 0.909071	test: 0.741248

Epoch: 55
Loss: 0.2991612182880981
ROC train: 0.965351	val: 0.641392	test: 0.747174
PRC train: 0.947921	val: 0.913937	test: 0.738880

Epoch: 56
Loss: 0.26782702623889415
ROC train: 0.965260	val: 0.654579	test: 0.739176
PRC train: 0.948311	val: 0.917590	test: 0.733179

Epoch: 57
Loss: 0.28120285388553584
ROC train: 0.966025	val: 0.650183	test: 0.736046
PRC train: 0.948701	val: 0.915906	test: 0.733779

Epoch: 58
Loss: 0.2728874182432043
ROC train: 0.966998	val: 0.635897	test: 0.742132
PRC train: 0.949487	val: 0.912145	test: 0.742719

Epoch: 59
Loss: 0.2762037811483162
ROC train: 0.968248	val: 0.646520	test: 0.750130
PRC train: 0.953265	val: 0.915228	test: 0.753930

Epoch: 60
Loss: 0.27682176846979384
ROC train: 0.965688	val: 0.654579	test: 0.741262
PRC train: 0.948605	val: 0.916762	test: 0.739742

Epoch: 61
Loss: 0.28030913351002695
ROC train: 0.966641	val: 0.636996	test: 0.738828
PRC train: 0.949202	val: 0.914486	test: 0.741501

Epoch: 62
Loss: 0.2770892995120404
ROC train: 0.969555	val: 0.604396	test: 0.743001
PRC train: 0.954656	val: 0.903559	test: 0.744227

Epoch: 63
Loss: 0.2861790964634191
ROC train: 0.967409	val: 0.615018	test: 0.724222
PRC train: 0.950659	val: 0.904275	test: 0.729947

Epoch: 64
Loss: 0.284282693980205
ROC train: 0.970511	val: 0.643956	test: 0.725961
PRC train: 0.956289	val: 0.916091	test: 0.727970

Epoch: 65
Loss: 0.2809286823178903
ROC train: 0.973154	val: 0.644322	test: 0.740741
PRC train: 0.960764	val: 0.917741	test: 0.750545

Epoch: 66
Loss: 0.2689893016089701
ROC train: 0.972497	val: 0.616484	test: 0.734829
PRC train: 0.959954	val: 0.909032	test: 0.748763

Epoch: 67
Loss: 0.2590160993184475
ROC train: 0.970029	val: 0.649084	test: 0.718658
PRC train: 0.955745	val: 0.913902	test: 0.721988

Epoch: 68
Loss: 0.2556023341427251
ROC train: 0.968430	val: 0.650183	test: 0.713441
PRC train: 0.952368	val: 0.910929	test: 0.717850

Epoch: 69
Loss: 0.271209598573471
ROC train: 0.973496	val: 0.618315	test: 0.727178
PRC train: 0.960623	val: 0.901342	test: 0.723741

Epoch: 70
Loss: 0.2695791238025079
ROC train: 0.975083	val: 0.603297	test: 0.743349
PRC train: 0.962143	val: 0.898416	test: 0.734478

Epoch: 71
Loss: 0.26834698249525407
ROC train: 0.973764	val: 0.619414	test: 0.735698
PRC train: 0.960897	val: 0.901229	test: 0.731082

Epoch: 72
Loss: 0.2757700466832964
ROC train: 0.970362	val: 0.641026	test: 0.724744
PRC train: 0.958054	val: 0.908664	test: 0.719577

Epoch: 73
Loss: 0.2698552972442482
ROC train: 0.972494	val: 0.635531	test: 0.708746
PRC train: 0.960481	val: 0.910077	test: 0.709644

Epoch: 74
Loss: 0.2631820430930996
ROC train: 0.971955	val: 0.627106	test: 0.705964
PRC train: 0.959866	val: 0.909691	test: 0.705521

Epoch: 75
Loss: 0.26228326025701865
ROC train: 0.971949	val: 0.635897	test: 0.707703
PRC train: 0.961194	val: 0.912142	test: 0.702907

Epoch: 76
Loss: 0.27983391142538033
ROC train: 0.970562	val: 0.627473	test: 0.706834
PRC train: 0.958610	val: 0.911301	test: 0.700796

Epoch: 77
Loss: 0.2648096831633824
ROC train: 0.975023	val: 0.611722	test: 0.722831
PRC train: 0.964738	val: 0.911407	test: 0.713376

Epoch: 78
Loss: 0.24519259397190293
ROC train: 0.977314	val: 0.623810	test: 0.733264
PRC train: 0.967112	val: 0.910832	test: 0.722438

Epoch: 79
Loss: 0.2643155412819853
ROC train: 0.975591	val: 0.642491	test: 0.744566
PRC train: 0.964681	val: 0.914492	test: 0.743183

Epoch: 80
Loss: 0.25665507402136767
ROC train: 0.975711	val: 0.647619	test: 0.724396
PRC train: 0.964673	val: 0.916091	test: 0.733375

Epoch: 81
Loss: 0.24854617173310428
ROC train: 0.975357	val: 0.645788	test: 0.713441
PRC train: 0.963906	val: 0.914716	test: 0.721424

Epoch: 82
Loss: 0.237231241543754
ROC train: 0.977854	val: 0.623443	test: 0.715006
PRC train: 0.967318	val: 0.904499	test: 0.718716

Epoch: 83
Loss: 0.24862290241753918
ROC train: 0.976436	val: 0.621612	test: 0.725091
PRC train: 0.965255	val: 0.906433	test: 0.725978

Epoch: 84
Loss: 0.2493701795077437
ROC train: 0.977568	val: 0.632234	test: 0.729786
PRC train: 0.966892	val: 0.910499	test: 0.721573

Epoch: 85
Loss: 0.2517172989541945
ROC train: 0.979209	val: 0.638462	test: 0.721266
PRC train: 0.970931	val: 0.913127	test: 0.720802

Epoch: 86
Loss: 0.25237259751708613
ROC train: 0.980108	val: 0.642491	test: 0.719875
PRC train: 0.971711	val: 0.908375	test: 0.722387

Epoch: 87
Loss: 0.24276492902132504
ROC train: 0.979147	val: 0.646886	test: 0.719527
PRC train: 0.969627	val: 0.909307	test: 0.718831

Epoch: 88
Loss: 0.24644079125165597
ROC train: 0.980174	val: 0.642857	test: 0.716049
PRC train: 0.971525	val: 0.915699	test: 0.722013

Epoch: 89
Loss: 0.2379215585686425
ROC train: 0.979409	val: 0.654212	test: 0.718136
PRC train: 0.969510	val: 0.919645	test: 0.726636

Epoch: 90
Loss: 0.2379451076060833
ROC train: 0.979070	val: 0.660806	test: 0.712919
PRC train: 0.969056	val: 0.919332	test: 0.718819

Epoch: 91
Loss: 0.2340051909357809
ROC train: 0.981518	val: 0.626374	test: 0.711876
PRC train: 0.973845	val: 0.905419	test: 0.726934

Epoch: 92
Loss: 0.24152520631221247
ROC train: 0.980091	val: 0.623077	test: 0.701269
PRC train: 0.971645	val: 0.902459	test: 0.711166

Epoch: 93
Loss: 0.23282751394165477
ROC train: 0.981943	val: 0.624542	test: 0.711181
PRC train: 0.974351	val: 0.891712	test: 0.709821

Epoch: 94
Loss: 0.2328234765865001
ROC train: 0.983741	val: 0.624908	test: 0.732394
PRC train: 0.919595	val: 0.938459	test: 0.703260

Epoch: 34
Loss: 0.3251275096348619
ROC train: 0.950502	val: 0.689377	test: 0.691010
PRC train: 0.922877	val: 0.937218	test: 0.698389

Epoch: 35
Loss: 0.3106739946879229
ROC train: 0.952434	val: 0.679487	test: 0.697444
PRC train: 0.924724	val: 0.934233	test: 0.706796

Epoch: 36
Loss: 0.31119123215509664
ROC train: 0.951747	val: 0.672527	test: 0.713789
PRC train: 0.923614	val: 0.933844	test: 0.717445

Epoch: 37
Loss: 0.31582213249137847
ROC train: 0.951199	val: 0.684249	test: 0.707529
PRC train: 0.923104	val: 0.936262	test: 0.711110

Epoch: 38
Loss: 0.3173391684302391
ROC train: 0.952232	val: 0.699634	test: 0.700226
PRC train: 0.924252	val: 0.939186	test: 0.704971

Epoch: 39
Loss: 0.31795748035607796
ROC train: 0.951136	val: 0.691209	test: 0.695705
PRC train: 0.923795	val: 0.937544	test: 0.699776

Epoch: 40
Loss: 0.3099280589824368
ROC train: 0.950205	val: 0.701465	test: 0.702313
PRC train: 0.923671	val: 0.938399	test: 0.705527

Epoch: 41
Loss: 0.2939963558095739
ROC train: 0.952860	val: 0.698535	test: 0.707703
PRC train: 0.926146	val: 0.935356	test: 0.703743

Epoch: 42
Loss: 0.3192535198716602
ROC train: 0.954024	val: 0.678022	test: 0.717614
PRC train: 0.926127	val: 0.932219	test: 0.724467

Epoch: 43
Loss: 0.3114290932202203
ROC train: 0.954101	val: 0.668132	test: 0.733785
PRC train: 0.926462	val: 0.928725	test: 0.731654

Epoch: 44
Loss: 0.31382929523347114
ROC train: 0.953884	val: 0.662271	test: 0.721614
PRC train: 0.926915	val: 0.924796	test: 0.717733

Epoch: 45
Loss: 0.31182623077186616
ROC train: 0.957491	val: 0.663004	test: 0.696053
PRC train: 0.933610	val: 0.926678	test: 0.705643

Epoch: 46
Loss: 0.3050126199808451
ROC train: 0.955870	val: 0.657143	test: 0.675187
PRC train: 0.931184	val: 0.925974	test: 0.688948

Epoch: 47
Loss: 0.29709623412206837
ROC train: 0.960194	val: 0.672161	test: 0.689793
PRC train: 0.937702	val: 0.926564	test: 0.704346

Epoch: 48
Loss: 0.28846996123354696
ROC train: 0.958850	val: 0.669963	test: 0.693445
PRC train: 0.935014	val: 0.928496	test: 0.709619

Epoch: 49
Loss: 0.2990953895956922
ROC train: 0.958753	val: 0.664103	test: 0.684750
PRC train: 0.934591	val: 0.926569	test: 0.696661

Epoch: 50
Loss: 0.3072557667022953
ROC train: 0.960753	val: 0.671062	test: 0.687359
PRC train: 0.937871	val: 0.927005	test: 0.698927

Epoch: 51
Loss: 0.29107216856272516
ROC train: 0.961040	val: 0.678022	test: 0.685446
PRC train: 0.939919	val: 0.928311	test: 0.699973

Epoch: 52
Loss: 0.3024085622264639
ROC train: 0.961909	val: 0.680586	test: 0.683012
PRC train: 0.942348	val: 0.929098	test: 0.712127

Epoch: 53
Loss: 0.2892479023162444
ROC train: 0.964244	val: 0.691941	test: 0.703008
PRC train: 0.945678	val: 0.930830	test: 0.723543

Epoch: 54
Loss: 0.308104561682621
ROC train: 0.963259	val: 0.672527	test: 0.694488
PRC train: 0.942673	val: 0.927760	test: 0.703918

Epoch: 55
Loss: 0.2803166923576043
ROC train: 0.964983	val: 0.688278	test: 0.695879
PRC train: 0.944067	val: 0.932158	test: 0.700440

Epoch: 56
Loss: 0.28512662848921566
ROC train: 0.964880	val: 0.691209	test: 0.686489
PRC train: 0.943638	val: 0.930782	test: 0.699122

Epoch: 57
Loss: 0.27980753850791695
ROC train: 0.967785	val: 0.676923	test: 0.684403
PRC train: 0.949237	val: 0.928144	test: 0.694955

Epoch: 58
Loss: 0.26277173156605427
ROC train: 0.966989	val: 0.679487	test: 0.680230
PRC train: 0.947958	val: 0.929821	test: 0.688001

Epoch: 59
Loss: 0.27529589826850026
ROC train: 0.968168	val: 0.684615	test: 0.679012
PRC train: 0.949919	val: 0.932811	test: 0.681614

Epoch: 60
Loss: 0.2727580639051136
ROC train: 0.971196	val: 0.694139	test: 0.696053
PRC train: 0.955531	val: 0.935750	test: 0.696980

Epoch: 61
Loss: 0.2651112762540071
ROC train: 0.970873	val: 0.690110	test: 0.701095
PRC train: 0.955018	val: 0.933891	test: 0.706710

Epoch: 62
Loss: 0.2675275481595188
ROC train: 0.967951	val: 0.695971	test: 0.710137
PRC train: 0.950867	val: 0.934601	test: 0.726307

Epoch: 63
Loss: 0.2760207721019919
ROC train: 0.967939	val: 0.684982	test: 0.707355
PRC train: 0.951737	val: 0.930525	test: 0.721847

Epoch: 64
Loss: 0.27781792996168836
ROC train: 0.971692	val: 0.686813	test: 0.702834
PRC train: 0.957512	val: 0.931503	test: 0.716788

Epoch: 65
Loss: 0.2700411718012508
ROC train: 0.972080	val: 0.693773	test: 0.707181
PRC train: 0.957003	val: 0.932720	test: 0.717646

Epoch: 66
Loss: 0.2628136098212338
ROC train: 0.972360	val: 0.682418	test: 0.713267
PRC train: 0.956732	val: 0.931167	test: 0.724444

Epoch: 67
Loss: 0.2654859725262691
ROC train: 0.971387	val: 0.698168	test: 0.721092
PRC train: 0.955314	val: 0.935284	test: 0.737291

Epoch: 68
Loss: 0.25811561115968573
ROC train: 0.971955	val: 0.709890	test: 0.708051
PRC train: 0.956043	val: 0.939370	test: 0.724426

Epoch: 69
Loss: 0.25336885739781384
ROC train: 0.971881	val: 0.713187	test: 0.696748
PRC train: 0.956778	val: 0.942070	test: 0.713165

Epoch: 70
Loss: 0.2553150621004987
ROC train: 0.969846	val: 0.706593	test: 0.699878
PRC train: 0.954833	val: 0.938477	test: 0.714984

Epoch: 71
Loss: 0.26246985561647135
ROC train: 0.972217	val: 0.698535	test: 0.708572
PRC train: 0.957288	val: 0.933597	test: 0.717162

Epoch: 72
Loss: 0.257297168040721
ROC train: 0.973864	val: 0.686813	test: 0.698487
PRC train: 0.959491	val: 0.926982	test: 0.704458

Epoch: 73
Loss: 0.2670355884709902
ROC train: 0.970188	val: 0.688645	test: 0.685968
PRC train: 0.952711	val: 0.928642	test: 0.699343

Epoch: 74
Loss: 0.2581129029433522
ROC train: 0.973950	val: 0.697802	test: 0.713441
PRC train: 0.960273	val: 0.932324	test: 0.731183

Epoch: 75
Loss: 0.26026790475138023
ROC train: 0.976749	val: 0.678022	test: 0.720570
PRC train: 0.964300	val: 0.926585	test: 0.731671

Epoch: 76
Loss: 0.2585416819015813
ROC train: 0.970908	val: 0.684249	test: 0.707877
PRC train: 0.956185	val: 0.929784	test: 0.723098

Epoch: 77
Loss: 0.24276921400606827
ROC train: 0.975511	val: 0.693773	test: 0.712224
PRC train: 0.962941	val: 0.932762	test: 0.730841

Epoch: 78
Loss: 0.25505168180193116
ROC train: 0.976741	val: 0.673260	test: 0.700052
PRC train: 0.964881	val: 0.927328	test: 0.707385

Epoch: 79
Loss: 0.26894857034366865
ROC train: 0.978450	val: 0.678022	test: 0.706834
PRC train: 0.966878	val: 0.926346	test: 0.726808

Epoch: 80
Loss: 0.24234571505751434
ROC train: 0.974418	val: 0.666300	test: 0.713441
PRC train: 0.961113	val: 0.922286	test: 0.725787

Epoch: 81
Loss: 0.2541067143171441
ROC train: 0.973476	val: 0.677656	test: 0.708399
PRC train: 0.959875	val: 0.925876	test: 0.729494

Epoch: 82
Loss: 0.23006608269412268
ROC train: 0.975791	val: 0.701099	test: 0.720049
PRC train: 0.963188	val: 0.929199	test: 0.738917

Epoch: 83
Loss: 0.24615754670040926
ROC train: 0.978753	val: 0.691941	test: 0.709442
PRC train: 0.967254	val: 0.926657	test: 0.729770

Epoch: 84
Loss: 0.24934037515630575
ROC train: 0.978981	val: 0.678022	test: 0.697270
PRC train: 0.968412	val: 0.927668	test: 0.703602

Epoch: 85
Loss: 0.24389303170457177
ROC train: 0.976316	val: 0.676190	test: 0.691532
PRC train: 0.964420	val: 0.926440	test: 0.695308

Epoch: 86
Loss: 0.25088621151452684
ROC train: 0.976744	val: 0.682051	test: 0.704399
PRC train: 0.964659	val: 0.926856	test: 0.723680

Epoch: 87
Loss: 0.23992993700546167
ROC train: 0.977340	val: 0.674725	test: 0.707703
PRC train: 0.965587	val: 0.925351	test: 0.731151

Epoch: 88
Loss: 0.24841340433548562
ROC train: 0.980411	val: 0.663004	test: 0.702660
PRC train: 0.969860	val: 0.924769	test: 0.720445

Epoch: 89
Loss: 0.24949473882735376
ROC train: 0.980765	val: 0.665201	test: 0.692054
PRC train: 0.970848	val: 0.926093	test: 0.698034

Epoch: 90
Loss: 0.2389415536303187
ROC train: 0.977220	val: 0.659341	test: 0.694140
PRC train: 0.963616	val: 0.921337	test: 0.703130

Epoch: 91
Loss: 0.2156484575702652
ROC train: 0.977038	val: 0.662637	test: 0.689793
PRC train: 0.963905	val: 0.921411	test: 0.692758

Epoch: 92
Loss: 0.23562779918058033
ROC train: 0.981193	val: 0.668864	test: 0.687359
PRC train: 0.971347	val: 0.922362	test: 0.688573

Epoch: 93
Loss: 0.22560783868078743
ROC train: 0.981641	val: 0.671062	test: 0.677447
PRC train: 0.973023	val: 0.925253	test: 0.683658

Epoch: 94
Loss: 0.24048332912252
ROC train: 0.980793	val: 0.675824	test: 0.686663
PRC train: 0.994212	val: 0.864460	test: 0.792059

Epoch: 33
Loss: 0.29509771538274143
ROC train: 0.995682	val: 0.586447	test: 0.751348
PRC train: 0.993429	val: 0.863297	test: 0.752823

Epoch: 34
Loss: 0.2503669567462884
ROC train: 0.997814	val: 0.564469	test: 0.770649
PRC train: 0.996589	val: 0.852511	test: 0.764875

Epoch: 35
Loss: 0.26856223110687544
ROC train: 0.998607	val: 0.549084	test: 0.759694
PRC train: 0.997786	val: 0.845718	test: 0.770862

Epoch: 36
Loss: 0.25053893574485375
ROC train: 0.997608	val: 0.579487	test: 0.716919
PRC train: 0.996362	val: 0.857826	test: 0.747231

Epoch: 37
Loss: 0.22734019545788745
ROC train: 0.998379	val: 0.561172	test: 0.756216
PRC train: 0.997534	val: 0.860581	test: 0.770151

Epoch: 38
Loss: 0.2464357008761151
ROC train: 0.999364	val: 0.532234	test: 0.767171
PRC train: 0.999016	val: 0.853991	test: 0.780374

Epoch: 39
Loss: 0.21632075988170554
ROC train: 0.999272	val: 0.548718	test: 0.740915
PRC train: 0.998863	val: 0.858556	test: 0.759490

Epoch: 40
Loss: 0.2103888508855833
ROC train: 0.999287	val: 0.568498	test: 0.745262
PRC train: 0.998856	val: 0.878971	test: 0.747751

Epoch: 41
Loss: 0.20250048775861287
ROC train: 0.999655	val: 0.573626	test: 0.757607
PRC train: 0.999452	val: 0.880171	test: 0.739771

Epoch: 42
Loss: 0.2210300161414796
ROC train: 0.999737	val: 0.571062	test: 0.769084
PRC train: 0.999592	val: 0.865645	test: 0.757248

Epoch: 43
Loss: 0.197249467160367
ROC train: 0.999786	val: 0.585714	test: 0.771170
PRC train: 0.999678	val: 0.866867	test: 0.770762

Epoch: 44
Loss: 0.17111599498092375
ROC train: 0.999889	val: 0.587546	test: 0.769953
PRC train: 0.999832	val: 0.859944	test: 0.772175

Epoch: 45
Loss: 0.1580460652564984
ROC train: 0.999763	val: 0.597436	test: 0.761607
PRC train: 0.999643	val: 0.878293	test: 0.751493

Epoch: 46
Loss: 0.18552695733830707
ROC train: 0.999860	val: 0.601465	test: 0.768562
PRC train: 0.999787	val: 0.879461	test: 0.759021

Epoch: 47
Loss: 0.18077902290664155
ROC train: 0.999823	val: 0.627839	test: 0.736741
PRC train: 0.999735	val: 0.897621	test: 0.739640

Epoch: 48
Loss: 0.1521990558248412
ROC train: 0.999766	val: 0.639560	test: 0.719701
PRC train: 0.999656	val: 0.910088	test: 0.722082

Epoch: 49
Loss: 0.1441092714592107
ROC train: 0.999994	val: 0.604762	test: 0.745609
PRC train: 0.999991	val: 0.897313	test: 0.742934

Epoch: 50
Loss: 0.1624185774759008
ROC train: 1.000000	val: 0.578022	test: 0.765780
PRC train: 1.000000	val: 0.878338	test: 0.773965

Epoch: 51
Loss: 0.14549454811882373
ROC train: 0.999994	val: 0.567033	test: 0.770127
PRC train: 0.999991	val: 0.876181	test: 0.777555

Epoch: 52
Loss: 0.13763148603020442
ROC train: 0.999949	val: 0.560806	test: 0.771344
PRC train: 0.999923	val: 0.864052	test: 0.790381

Epoch: 53
Loss: 0.13759829631446482
ROC train: 0.999971	val: 0.545055	test: 0.770127
PRC train: 0.999957	val: 0.853768	test: 0.792302

Epoch: 54
Loss: 0.16156282312283926
ROC train: 1.000000	val: 0.549817	test: 0.764910
PRC train: 1.000000	val: 0.875007	test: 0.753178

Epoch: 55
Loss: 0.11234766005362617
ROC train: 1.000000	val: 0.580586	test: 0.748565
PRC train: 1.000000	val: 0.894878	test: 0.735466

Epoch: 56
Loss: 0.1203843673237311
ROC train: 1.000000	val: 0.600366	test: 0.741089
PRC train: 1.000000	val: 0.905375	test: 0.725790

Epoch: 57
Loss: 0.13319580826377908
ROC train: 0.999997	val: 0.594505	test: 0.741784
PRC train: 0.999996	val: 0.889211	test: 0.726537

Epoch: 58
Loss: 0.13172778236197918
ROC train: 0.999971	val: 0.590842	test: 0.748392
PRC train: 0.999957	val: 0.873155	test: 0.758855

Epoch: 59
Loss: 0.11260020749125277
ROC train: 0.999997	val: 0.591575	test: 0.755521
PRC train: 0.999996	val: 0.890586	test: 0.753343

Epoch: 60
Loss: 0.13763258800192754
ROC train: 1.000000	val: 0.553480	test: 0.758477
PRC train: 1.000000	val: 0.865696	test: 0.756373

Epoch: 61
Loss: 0.11268652265782415
ROC train: 1.000000	val: 0.538828	test: 0.739871
PRC train: 1.000000	val: 0.846234	test: 0.755689

Epoch: 62
Loss: 0.12415968120407442
ROC train: 1.000000	val: 0.510623	test: 0.752217
PRC train: 1.000000	val: 0.845393	test: 0.778513

Epoch: 63
Loss: 0.11446665673713996
ROC train: 0.999963	val: 0.536264	test: 0.756564
PRC train: 0.999943	val: 0.870279	test: 0.766997

Epoch: 64
Loss: 0.1034646484612968
ROC train: 0.999974	val: 0.550549	test: 0.744566
PRC train: 0.999961	val: 0.866215	test: 0.761326

Epoch: 65
Loss: 0.10684290549171352
ROC train: 1.000000	val: 0.541758	test: 0.741436
PRC train: 1.000000	val: 0.853407	test: 0.753076

Epoch: 66
Loss: 0.11192672388017748
ROC train: 1.000000	val: 0.529670	test: 0.749435
PRC train: 1.000000	val: 0.846893	test: 0.754434

Epoch: 67
Loss: 0.10343544860152332
ROC train: 1.000000	val: 0.535897	test: 0.765954
PRC train: 1.000000	val: 0.852884	test: 0.773995

Epoch: 68
Loss: 0.10722933349257864
ROC train: 1.000000	val: 0.536264	test: 0.780908
PRC train: 1.000000	val: 0.856070	test: 0.789231

Epoch: 69
Loss: 0.09931425741622361
ROC train: 1.000000	val: 0.555678	test: 0.773952
PRC train: 1.000000	val: 0.862743	test: 0.778501

Epoch: 70
Loss: 0.10483913963681704
ROC train: 1.000000	val: 0.569597	test: 0.752913
PRC train: 1.000000	val: 0.872977	test: 0.739966

Epoch: 71
Loss: 0.08601924371906819
ROC train: 1.000000	val: 0.557875	test: 0.748913
PRC train: 1.000000	val: 0.872771	test: 0.736131

Epoch: 72
Loss: 0.10339267630716238
ROC train: 1.000000	val: 0.554579	test: 0.750478
PRC train: 1.000000	val: 0.871105	test: 0.734017

Epoch: 73
Loss: 0.10292934037715155
ROC train: 1.000000	val: 0.535165	test: 0.762302
PRC train: 1.000000	val: 0.853431	test: 0.755527

Epoch: 74
Loss: 0.095297895730804
ROC train: 1.000000	val: 0.572161	test: 0.745436
PRC train: 1.000000	val: 0.869480	test: 0.739176

Epoch: 75
Loss: 0.07373942218062085
ROC train: 1.000000	val: 0.577656	test: 0.734133
PRC train: 1.000000	val: 0.875685	test: 0.736222

Epoch: 76
Loss: 0.08289036393292533
ROC train: 0.999923	val: 0.556777	test: 0.743697
PRC train: 0.999880	val: 0.857226	test: 0.743875

Epoch: 77
Loss: 0.07984619066793137
ROC train: 0.999994	val: 0.574359	test: 0.757086
PRC train: 0.999991	val: 0.869541	test: 0.750132

Epoch: 78
Loss: 0.06802674870201174
ROC train: 1.000000	val: 0.581685	test: 0.765606
PRC train: 1.000000	val: 0.872872	test: 0.746277

Epoch: 79
Loss: 0.0696425530986203
ROC train: 1.000000	val: 0.571062	test: 0.768736
PRC train: 1.000000	val: 0.855201	test: 0.760137

Epoch: 80
Loss: 0.0688628410537537
ROC train: 1.000000	val: 0.575092	test: 0.753608
PRC train: 1.000000	val: 0.854662	test: 0.764204

Epoch: 81
Loss: 0.07723524027874529
ROC train: 1.000000	val: 0.556044	test: 0.749435
PRC train: 1.000000	val: 0.848097	test: 0.758082

Epoch: 82
Loss: 0.08288373623730595
ROC train: 1.000000	val: 0.544689	test: 0.764041
PRC train: 1.000000	val: 0.855287	test: 0.765761

Epoch: 83
Loss: 0.05431943247659503
ROC train: 1.000000	val: 0.549451	test: 0.757086
PRC train: 1.000000	val: 0.861164	test: 0.758945

Epoch: 84
Loss: 0.07742106732003019
ROC train: 1.000000	val: 0.545421	test: 0.766475
PRC train: 1.000000	val: 0.840461	test: 0.770189

Epoch: 85
Loss: 0.0725699298181138
ROC train: 1.000000	val: 0.555311	test: 0.766649
PRC train: 1.000000	val: 0.857275	test: 0.757492

Epoch: 86
Loss: 0.05729044968196848
ROC train: 1.000000	val: 0.587179	test: 0.741089
PRC train: 1.000000	val: 0.873428	test: 0.744332

Epoch: 87
Loss: 0.07318965554434366
ROC train: 1.000000	val: 0.612088	test: 0.734829
PRC train: 1.000000	val: 0.883828	test: 0.744855

Epoch: 88
Loss: 0.06546591297891637
ROC train: 1.000000	val: 0.608791	test: 0.750130
PRC train: 1.000000	val: 0.896321	test: 0.752919

Epoch: 89
Loss: 0.06726986041396318
ROC train: 1.000000	val: 0.596337	test: 0.759172
PRC train: 1.000000	val: 0.890833	test: 0.760135

Epoch: 90
Loss: 0.0498724603056651
ROC train: 1.000000	val: 0.590842	test: 0.750652
PRC train: 1.000000	val: 0.877656	test: 0.749699

Epoch: 91
Loss: 0.0734896856383129
ROC train: 1.000000	val: 0.555678	test: 0.731003
PRC train: 1.000000	val: 0.848544	test: 0.735725

Epoch: 92
Loss: 0.06389244967138405
ROC train: 1.000000	val: 0.534799	test: 0.736046
PRC train: 1.000000	val: 0.842664	test: 0.729732

Epoch: 93
Loss: 0.06601742544723926
ROC train: 1.000000	val: 0.545055	test: 0.737263
PRC train: 0.979201	val: 0.915426	test: 0.724144

Epoch: 33
Loss: 0.2829306520294037
ROC train: 0.988624	val: 0.638462	test: 0.728569
PRC train: 0.982548	val: 0.913904	test: 0.727930

Epoch: 34
Loss: 0.2627778049968323
ROC train: 0.989840	val: 0.647985	test: 0.726830
PRC train: 0.984394	val: 0.917304	test: 0.720805

Epoch: 35
Loss: 0.28647095744677115
ROC train: 0.990186	val: 0.653480	test: 0.744218
PRC train: 0.984763	val: 0.915852	test: 0.737549

Epoch: 36
Loss: 0.28112269036114695
ROC train: 0.990548	val: 0.654945	test: 0.763346
PRC train: 0.985936	val: 0.917156	test: 0.757439

Epoch: 37
Loss: 0.2456338696983808
ROC train: 0.991667	val: 0.635531	test: 0.745609
PRC train: 0.986800	val: 0.907563	test: 0.740358

Epoch: 38
Loss: 0.24556777796616522
ROC train: 0.990605	val: 0.625275	test: 0.743349
PRC train: 0.985600	val: 0.903079	test: 0.737335

Epoch: 39
Loss: 0.2515969062197863
ROC train: 0.990499	val: 0.628205	test: 0.763867
PRC train: 0.985182	val: 0.906846	test: 0.747575

Epoch: 40
Loss: 0.25317131911815427
ROC train: 0.992686	val: 0.633333	test: 0.754130
PRC train: 0.988753	val: 0.910273	test: 0.738696

Epoch: 41
Loss: 0.23872607330007542
ROC train: 0.993102	val: 0.598901	test: 0.749087
PRC train: 0.989689	val: 0.900425	test: 0.749804

Epoch: 42
Loss: 0.23966670150086028
ROC train: 0.994415	val: 0.617949	test: 0.734307
PRC train: 0.991461	val: 0.906572	test: 0.735576

Epoch: 43
Loss: 0.22663487580310754
ROC train: 0.994170	val: 0.647619	test: 0.744740
PRC train: 0.990776	val: 0.912506	test: 0.739929

Epoch: 44
Loss: 0.19525919177109377
ROC train: 0.996239	val: 0.617216	test: 0.762476
PRC train: 0.994075	val: 0.902333	test: 0.754554

Epoch: 45
Loss: 0.20405760026774727
ROC train: 0.996775	val: 0.617216	test: 0.768214
PRC train: 0.995001	val: 0.906412	test: 0.762290

Epoch: 46
Loss: 0.22204343440407937
ROC train: 0.997314	val: 0.634799	test: 0.772735
PRC train: 0.995860	val: 0.911471	test: 0.764818

Epoch: 47
Loss: 0.2081424390778542
ROC train: 0.997297	val: 0.639560	test: 0.772214
PRC train: 0.995729	val: 0.912719	test: 0.759091

Epoch: 48
Loss: 0.16985401047669807
ROC train: 0.997657	val: 0.624908	test: 0.775691
PRC train: 0.996364	val: 0.906543	test: 0.764177

Epoch: 49
Loss: 0.17491439501443073
ROC train: 0.997897	val: 0.630037	test: 0.770475
PRC train: 0.996830	val: 0.907982	test: 0.756316

Epoch: 50
Loss: 0.19696869036910325
ROC train: 0.998008	val: 0.645788	test: 0.769953
PRC train: 0.996844	val: 0.910688	test: 0.761908

Epoch: 51
Loss: 0.1634194557453553
ROC train: 0.997757	val: 0.650549	test: 0.776213
PRC train: 0.996437	val: 0.911441	test: 0.768770

Epoch: 52
Loss: 0.18174559054184122
ROC train: 0.998276	val: 0.622711	test: 0.760911
PRC train: 0.997266	val: 0.899400	test: 0.748022

Epoch: 53
Loss: 0.17818159491848834
ROC train: 0.998938	val: 0.623810	test: 0.753086
PRC train: 0.998345	val: 0.895619	test: 0.743368

Epoch: 54
Loss: 0.16512815596774463
ROC train: 0.999269	val: 0.627839	test: 0.762824
PRC train: 0.998857	val: 0.903027	test: 0.756974

Epoch: 55
Loss: 0.17165962362438558
ROC train: 0.999255	val: 0.636630	test: 0.762302
PRC train: 0.998820	val: 0.909312	test: 0.747495

Epoch: 56
Loss: 0.16543554098041344
ROC train: 0.999232	val: 0.632601	test: 0.756390
PRC train: 0.998763	val: 0.909100	test: 0.734237

Epoch: 57
Loss: 0.16272035902158105
ROC train: 0.998587	val: 0.635897	test: 0.750478
PRC train: 0.997700	val: 0.910888	test: 0.725579

Epoch: 58
Loss: 0.1546863910585316
ROC train: 0.998519	val: 0.646886	test: 0.750826
PRC train: 0.997583	val: 0.913698	test: 0.723188

Epoch: 59
Loss: 0.17792647418397728
ROC train: 0.998662	val: 0.659707	test: 0.758998
PRC train: 0.997893	val: 0.916571	test: 0.736954

Epoch: 60
Loss: 0.1602795550835488
ROC train: 0.999466	val: 0.630403	test: 0.766997
PRC train: 0.999156	val: 0.909020	test: 0.746395

Epoch: 61
Loss: 0.17293772827986378
ROC train: 0.999389	val: 0.617949	test: 0.752043
PRC train: 0.999057	val: 0.905118	test: 0.743916

Epoch: 62
Loss: 0.1411953753680401
ROC train: 0.999155	val: 0.648352	test: 0.741784
PRC train: 0.998629	val: 0.917071	test: 0.727990

Epoch: 63
Loss: 0.15694880591042465
ROC train: 0.999307	val: 0.658608	test: 0.752739
PRC train: 0.998848	val: 0.921212	test: 0.742942

Epoch: 64
Loss: 0.15159904878508126
ROC train: 0.999358	val: 0.641026	test: 0.756390
PRC train: 0.998983	val: 0.917321	test: 0.746050

Epoch: 65
Loss: 0.1256987685243867
ROC train: 0.999349	val: 0.630037	test: 0.761607
PRC train: 0.998981	val: 0.914565	test: 0.760030

Epoch: 66
Loss: 0.1457922668978277
ROC train: 0.999555	val: 0.651282	test: 0.764389
PRC train: 0.999299	val: 0.917163	test: 0.753661

Epoch: 67
Loss: 0.1452134876322125
ROC train: 0.999872	val: 0.657143	test: 0.764389
PRC train: 0.999803	val: 0.917823	test: 0.747117

Epoch: 68
Loss: 0.14761010485008402
ROC train: 0.999892	val: 0.649817	test: 0.758477
PRC train: 0.999834	val: 0.913273	test: 0.748924

Epoch: 69
Loss: 0.1328167632557744
ROC train: 0.999686	val: 0.653480	test: 0.752217
PRC train: 0.999519	val: 0.915898	test: 0.746769

Epoch: 70
Loss: 0.12750153533432237
ROC train: 0.999615	val: 0.642491	test: 0.753434
PRC train: 0.999409	val: 0.910727	test: 0.746798

Epoch: 71
Loss: 0.11654940350244096
ROC train: 0.999860	val: 0.636996	test: 0.764910
PRC train: 0.999788	val: 0.908751	test: 0.756165

Epoch: 72
Loss: 0.14172541753193285
ROC train: 0.999957	val: 0.648718	test: 0.757433
PRC train: 0.999935	val: 0.913837	test: 0.748064

Epoch: 73
Loss: 0.12493371814917172
ROC train: 0.999894	val: 0.646886	test: 0.738132
PRC train: 0.999840	val: 0.914227	test: 0.731198

Epoch: 74
Loss: 0.1135624984570115
ROC train: 0.999912	val: 0.620513	test: 0.759172
PRC train: 0.999867	val: 0.903796	test: 0.747756

Epoch: 75
Loss: 0.13478459755980696
ROC train: 0.999906	val: 0.641392	test: 0.756564
PRC train: 0.999859	val: 0.907961	test: 0.739865

Epoch: 76
Loss: 0.11840928394187848
ROC train: 0.999735	val: 0.652747	test: 0.737785
PRC train: 0.999595	val: 0.912839	test: 0.731631

Epoch: 77
Loss: 0.11137891620522815
ROC train: 0.999923	val: 0.656777	test: 0.765780
PRC train: 0.999882	val: 0.913593	test: 0.774128

Epoch: 78
Loss: 0.1137605485449861
ROC train: 0.999943	val: 0.649451	test: 0.774474
PRC train: 0.999912	val: 0.911134	test: 0.789443

Epoch: 79
Loss: 0.09117915496026505
ROC train: 0.999954	val: 0.639927	test: 0.768040
PRC train: 0.999930	val: 0.905524	test: 0.774981

Epoch: 80
Loss: 0.10599090500797706
ROC train: 0.999969	val: 0.646154	test: 0.758303
PRC train: 0.999952	val: 0.910490	test: 0.759849

Epoch: 81
Loss: 0.10822850350875486
ROC train: 0.999969	val: 0.657875	test: 0.743175
PRC train: 0.999952	val: 0.914404	test: 0.738032

Epoch: 82
Loss: 0.1072136388246466
ROC train: 0.999926	val: 0.659707	test: 0.740393
PRC train: 0.999888	val: 0.914156	test: 0.732563

Epoch: 83
Loss: 0.11677234209398549
ROC train: 0.999991	val: 0.627839	test: 0.717788
PRC train: 0.999987	val: 0.906151	test: 0.721375

Epoch: 84
Loss: 0.09455891217084442
ROC train: 0.999943	val: 0.617582	test: 0.725613
PRC train: 0.999912	val: 0.901230	test: 0.734525

Epoch: 85
Loss: 0.10086693679019706
ROC train: 0.999786	val: 0.656777	test: 0.763693
PRC train: 0.999682	val: 0.910043	test: 0.759769

Epoch: 86
Loss: 0.11534035963101566
ROC train: 1.000000	val: 0.642125	test: 0.743697
PRC train: 1.000000	val: 0.907461	test: 0.737370

Epoch: 87
Loss: 0.09934140827263707
ROC train: 1.000000	val: 0.649084	test: 0.723005
PRC train: 1.000000	val: 0.911735	test: 0.716124

Epoch: 88
Loss: 0.07405330389420742
ROC train: 1.000000	val: 0.648352	test: 0.736915
PRC train: 1.000000	val: 0.912499	test: 0.729937

Epoch: 89
Loss: 0.0856754564397237
ROC train: 0.999991	val: 0.634799	test: 0.745783
PRC train: 0.999987	val: 0.907693	test: 0.733902

Epoch: 90
Loss: 0.07941948568183455
ROC train: 0.999960	val: 0.656044	test: 0.755521
PRC train: 0.999939	val: 0.915218	test: 0.733679

Epoch: 91
Loss: 0.104178236672327
ROC train: 1.000000	val: 0.669963	test: 0.767345
PRC train: 1.000000	val: 0.921411	test: 0.753077

Epoch: 92
Loss: 0.09277238328443829
ROC train: 0.999997	val: 0.650183	test: 0.761954
PRC train: 0.999996	val: 0.916635	test: 0.756277

Epoch: 93
Loss: 0.09247170410041852
ROC train: 0.999920	val: 0.626740	test: 0.732394
PRC train: 0.982820	val: 0.923456	test: 0.764542

Epoch: 33
Loss: 0.27923487930648494
ROC train: 0.990611	val: 0.652747	test: 0.771170
PRC train: 0.985840	val: 0.925092	test: 0.766819

Epoch: 34
Loss: 0.25132646705590456
ROC train: 0.991005	val: 0.661905	test: 0.743871
PRC train: 0.987059	val: 0.928945	test: 0.740184

Epoch: 35
Loss: 0.2664317912614933
ROC train: 0.990674	val: 0.654212	test: 0.760389
PRC train: 0.986211	val: 0.925720	test: 0.760173

Epoch: 36
Loss: 0.25529772182495614
ROC train: 0.988761	val: 0.656044	test: 0.758825
PRC train: 0.983087	val: 0.928526	test: 0.759085

Epoch: 37
Loss: 0.2366843714651942
ROC train: 0.988593	val: 0.701099	test: 0.727873
PRC train: 0.982877	val: 0.943301	test: 0.735130

Epoch: 38
Loss: 0.2286512278927514
ROC train: 0.991341	val: 0.692308	test: 0.753782
PRC train: 0.986628	val: 0.942088	test: 0.754498

Epoch: 39
Loss: 0.24290940786702545
ROC train: 0.993964	val: 0.681685	test: 0.781951
PRC train: 0.990531	val: 0.937910	test: 0.766381

Epoch: 40
Loss: 0.23019885877191187
ROC train: 0.994229	val: 0.669597	test: 0.790993
PRC train: 0.990848	val: 0.933826	test: 0.764066

Epoch: 41
Loss: 0.21637781229424227
ROC train: 0.994872	val: 0.698535	test: 0.779690
PRC train: 0.991822	val: 0.942406	test: 0.767073

Epoch: 42
Loss: 0.23357565741857655
ROC train: 0.995748	val: 0.693773	test: 0.792036
PRC train: 0.993302	val: 0.941236	test: 0.780060

Epoch: 43
Loss: 0.21677845139758073
ROC train: 0.996276	val: 0.657509	test: 0.787167
PRC train: 0.994273	val: 0.931424	test: 0.779262

Epoch: 44
Loss: 0.1910603604549848
ROC train: 0.996084	val: 0.648352	test: 0.738306
PRC train: 0.994035	val: 0.929158	test: 0.733864

Epoch: 45
Loss: 0.19355435109345062
ROC train: 0.996601	val: 0.640293	test: 0.769953
PRC train: 0.994729	val: 0.927934	test: 0.763952

Epoch: 46
Loss: 0.20218465989302628
ROC train: 0.997295	val: 0.672161	test: 0.758129
PRC train: 0.995551	val: 0.937510	test: 0.754947

Epoch: 47
Loss: 0.18687815356455934
ROC train: 0.997272	val: 0.689744	test: 0.757260
PRC train: 0.995463	val: 0.942333	test: 0.758916

Epoch: 48
Loss: 0.16232619365715142
ROC train: 0.997797	val: 0.652747	test: 0.785603
PRC train: 0.996278	val: 0.932939	test: 0.779103

Epoch: 49
Loss: 0.18402360086757155
ROC train: 0.998416	val: 0.671062	test: 0.783516
PRC train: 0.997532	val: 0.938143	test: 0.780761

Epoch: 50
Loss: 0.17401143197487662
ROC train: 0.997574	val: 0.687912	test: 0.724917
PRC train: 0.996351	val: 0.941947	test: 0.731598

Epoch: 51
Loss: 0.177986543983175
ROC train: 0.997985	val: 0.669963	test: 0.747001
PRC train: 0.997005	val: 0.936151	test: 0.748586

Epoch: 52
Loss: 0.14997334391862893
ROC train: 0.999084	val: 0.685348	test: 0.755347
PRC train: 0.998637	val: 0.940418	test: 0.736297

Epoch: 53
Loss: 0.1766318136042015
ROC train: 0.999272	val: 0.684615	test: 0.775691
PRC train: 0.998885	val: 0.936640	test: 0.757391

Epoch: 54
Loss: 0.16425372692691123
ROC train: 0.998311	val: 0.683516	test: 0.784907
PRC train: 0.997507	val: 0.936122	test: 0.766708

Epoch: 55
Loss: 0.1769384727635874
ROC train: 0.999324	val: 0.691941	test: 0.779343
PRC train: 0.998966	val: 0.939739	test: 0.755745

Epoch: 56
Loss: 0.15986831034435361
ROC train: 0.997554	val: 0.686813	test: 0.734481
PRC train: 0.996192	val: 0.940688	test: 0.712808

Epoch: 57
Loss: 0.14748579804948833
ROC train: 0.998208	val: 0.674725	test: 0.752565
PRC train: 0.997236	val: 0.935516	test: 0.737086

Epoch: 58
Loss: 0.15901077570260858
ROC train: 0.999404	val: 0.684249	test: 0.752043
PRC train: 0.999083	val: 0.939759	test: 0.737701

Epoch: 59
Loss: 0.15419902177596218
ROC train: 0.999712	val: 0.661172	test: 0.775691
PRC train: 0.999567	val: 0.933492	test: 0.752415

Epoch: 60
Loss: 0.15522827699009017
ROC train: 0.999854	val: 0.682784	test: 0.771344
PRC train: 0.999782	val: 0.939091	test: 0.746457

Epoch: 61
Loss: 0.14202427202235962
ROC train: 0.999678	val: 0.716850	test: 0.744914
PRC train: 0.999504	val: 0.948996	test: 0.715355

Epoch: 62
Loss: 0.12658774400658326
ROC train: 0.999726	val: 0.693040	test: 0.755869
PRC train: 0.999583	val: 0.943664	test: 0.745767

Epoch: 63
Loss: 0.13186885481127816
ROC train: 0.999880	val: 0.662271	test: 0.772909
PRC train: 0.999818	val: 0.936696	test: 0.765591

Epoch: 64
Loss: 0.14560369692574113
ROC train: 0.999797	val: 0.650916	test: 0.766128
PRC train: 0.999694	val: 0.933995	test: 0.765218

Epoch: 65
Loss: 0.12614535466164173
ROC train: 0.999195	val: 0.672527	test: 0.772561
PRC train: 0.998784	val: 0.937071	test: 0.772898

Epoch: 66
Loss: 0.13564887657532787
ROC train: 0.999949	val: 0.694872	test: 0.770822
PRC train: 0.999922	val: 0.943188	test: 0.761309

Epoch: 67
Loss: 0.12613037910075425
ROC train: 0.999840	val: 0.710623	test: 0.733090
PRC train: 0.999755	val: 0.947371	test: 0.701408

Epoch: 68
Loss: 0.11558121045045881
ROC train: 0.999786	val: 0.663736	test: 0.770301
PRC train: 0.999675	val: 0.932586	test: 0.752939

Epoch: 69
Loss: 0.11367780599294534
ROC train: 0.999974	val: 0.651648	test: 0.788037
PRC train: 0.999961	val: 0.932945	test: 0.776774

Epoch: 70
Loss: 0.12120143633378437
ROC train: 0.999940	val: 0.675458	test: 0.770301
PRC train: 0.999909	val: 0.940022	test: 0.759093

Epoch: 71
Loss: 0.12627536474946457
ROC train: 0.999795	val: 0.661538	test: 0.774996
PRC train: 0.999693	val: 0.935698	test: 0.763016

Epoch: 72
Loss: 0.1138516543120848
ROC train: 0.999957	val: 0.636630	test: 0.789428
PRC train: 0.999935	val: 0.928042	test: 0.768522

Epoch: 73
Loss: 0.120130924724609
ROC train: 0.999929	val: 0.629670	test: 0.784385
PRC train: 0.999892	val: 0.927530	test: 0.766833

Epoch: 74
Loss: 0.09979101920153041
ROC train: 0.999969	val: 0.625641	test: 0.774474
PRC train: 0.999952	val: 0.926455	test: 0.762657

Epoch: 75
Loss: 0.1045856976553877
ROC train: 0.999983	val: 0.662637	test: 0.760389
PRC train: 0.999974	val: 0.936508	test: 0.749608

Epoch: 76
Loss: 0.11833888853496277
ROC train: 0.999991	val: 0.679121	test: 0.778647
PRC train: 0.999987	val: 0.940367	test: 0.770184

Epoch: 77
Loss: 0.08459728255450001
ROC train: 0.999977	val: 0.660440	test: 0.794297
PRC train: 0.999965	val: 0.935955	test: 0.783529

Epoch: 78
Loss: 0.09407305669398687
ROC train: 0.999909	val: 0.668864	test: 0.781255
PRC train: 0.999857	val: 0.937196	test: 0.761211

Epoch: 79
Loss: 0.08753528674611204
ROC train: 0.999954	val: 0.654579	test: 0.780908
PRC train: 0.999930	val: 0.934367	test: 0.761518

Epoch: 80
Loss: 0.10412212952290315
ROC train: 0.999991	val: 0.643956	test: 0.793949
PRC train: 0.999987	val: 0.930508	test: 0.775149

Epoch: 81
Loss: 0.0877349299688908
ROC train: 0.999989	val: 0.659341	test: 0.797600
PRC train: 0.999983	val: 0.934164	test: 0.779504

Epoch: 82
Loss: 0.09651079934427054
ROC train: 1.000000	val: 0.673993	test: 0.788559
PRC train: 1.000000	val: 0.939231	test: 0.769986

Epoch: 83
Loss: 0.10327202512794084
ROC train: 1.000000	val: 0.684982	test: 0.764389
PRC train: 1.000000	val: 0.942851	test: 0.752054

Epoch: 84
Loss: 0.08399868513268852
ROC train: 0.999966	val: 0.678388	test: 0.746305
PRC train: 0.999948	val: 0.938730	test: 0.733023

Epoch: 85
Loss: 0.10281875684104909
ROC train: 0.999892	val: 0.677656	test: 0.767693
PRC train: 0.999835	val: 0.938427	test: 0.755522

Epoch: 86
Loss: 0.09730976777674664
ROC train: 0.999874	val: 0.673626	test: 0.775865
PRC train: 0.999808	val: 0.938998	test: 0.769448

Epoch: 87
Loss: 0.09455689787938222
ROC train: 1.000000	val: 0.629304	test: 0.789428
PRC train: 1.000000	val: 0.929255	test: 0.785611

Epoch: 88
Loss: 0.07769445927367509
ROC train: 1.000000	val: 0.645788	test: 0.778299
PRC train: 1.000000	val: 0.932215	test: 0.766977

Epoch: 89
Loss: 0.0977511492964124
ROC train: 0.999989	val: 0.646886	test: 0.772040
PRC train: 0.999983	val: 0.931061	test: 0.757168

Epoch: 90
Loss: 0.07238851860392756
ROC train: 0.999991	val: 0.623810	test: 0.773431
PRC train: 0.999987	val: 0.925362	test: 0.763109

Epoch: 91
Loss: 0.07830355007396159
ROC train: 0.999997	val: 0.650183	test: 0.756390
PRC train: 0.999996	val: 0.933175	test: 0.740719

Epoch: 92
Loss: 0.09525534001209937
ROC train: 0.999994	val: 0.647985	test: 0.776561
PRC train: 0.999991	val: 0.932936	test: 0.755745

Epoch: 93
Loss: 0.08607969780099525
ROC train: 0.999997	val: 0.623443	test: 0.784559
PRC train: 0.973953	val: 0.932955	test: 0.781867

Epoch: 33
Loss: 0.27287583694331125
ROC train: 0.987135	val: 0.654579	test: 0.778995
PRC train: 0.977451	val: 0.931377	test: 0.763114

Epoch: 34
Loss: 0.27000380091397314
ROC train: 0.989800	val: 0.653846	test: 0.773257
PRC train: 0.981652	val: 0.931457	test: 0.769821

Epoch: 35
Loss: 0.235191067276756
ROC train: 0.990351	val: 0.652381	test: 0.765606
PRC train: 0.982417	val: 0.930685	test: 0.761658

Epoch: 36
Loss: 0.2343047895275971
ROC train: 0.991233	val: 0.664469	test: 0.762824
PRC train: 0.984220	val: 0.934544	test: 0.768035

Epoch: 37
Loss: 0.2594876151239288
ROC train: 0.991735	val: 0.691941	test: 0.766128
PRC train: 0.984876	val: 0.942045	test: 0.776387

Epoch: 38
Loss: 0.23832849768235462
ROC train: 0.991096	val: 0.689011	test: 0.756738
PRC train: 0.984729	val: 0.939802	test: 0.750500

Epoch: 39
Loss: 0.2392787670597885
ROC train: 0.988596	val: 0.673260	test: 0.755173
PRC train: 0.981660	val: 0.935755	test: 0.747855

Epoch: 40
Loss: 0.22138401905445915
ROC train: 0.990711	val: 0.686081	test: 0.754130
PRC train: 0.984068	val: 0.938803	test: 0.748792

Epoch: 41
Loss: 0.2385295738055539
ROC train: 0.991789	val: 0.671429	test: 0.766649
PRC train: 0.986729	val: 0.934841	test: 0.777985

Epoch: 42
Loss: 0.22044103698145792
ROC train: 0.992854	val: 0.655678	test: 0.777778
PRC train: 0.988409	val: 0.930719	test: 0.797543

Epoch: 43
Loss: 0.20783539349680077
ROC train: 0.995397	val: 0.664469	test: 0.773605
PRC train: 0.992574	val: 0.934248	test: 0.793190

Epoch: 44
Loss: 0.2154025028853499
ROC train: 0.994766	val: 0.675092	test: 0.763346
PRC train: 0.992193	val: 0.936514	test: 0.769295

Epoch: 45
Loss: 0.21972162334230888
ROC train: 0.995648	val: 0.640293	test: 0.776387
PRC train: 0.992964	val: 0.928443	test: 0.769043

Epoch: 46
Loss: 0.20435276985458092
ROC train: 0.995599	val: 0.643223	test: 0.782646
PRC train: 0.992484	val: 0.928508	test: 0.768819

Epoch: 47
Loss: 0.19451159168783955
ROC train: 0.997169	val: 0.668132	test: 0.804556
PRC train: 0.995684	val: 0.933886	test: 0.794693

Epoch: 48
Loss: 0.17426392988856004
ROC train: 0.998419	val: 0.701465	test: 0.806295
PRC train: 0.997304	val: 0.943581	test: 0.795394

Epoch: 49
Loss: 0.1930206220648567
ROC train: 0.998193	val: 0.690476	test: 0.793427
PRC train: 0.996727	val: 0.940217	test: 0.786138

Epoch: 50
Loss: 0.1815101617106431
ROC train: 0.997235	val: 0.672161	test: 0.770996
PRC train: 0.995665	val: 0.935874	test: 0.758888

Epoch: 51
Loss: 0.19382990174260223
ROC train: 0.996969	val: 0.642125	test: 0.768040
PRC train: 0.995379	val: 0.928174	test: 0.763962

Epoch: 52
Loss: 0.18821751483634577
ROC train: 0.998256	val: 0.646520	test: 0.757607
PRC train: 0.997319	val: 0.929613	test: 0.753870

Epoch: 53
Loss: 0.17684438290577373
ROC train: 0.998051	val: 0.653846	test: 0.767519
PRC train: 0.997107	val: 0.929964	test: 0.770311

Epoch: 54
Loss: 0.1555935672692781
ROC train: 0.998553	val: 0.682784	test: 0.793253
PRC train: 0.997579	val: 0.937853	test: 0.790479

Epoch: 55
Loss: 0.16993815610615737
ROC train: 0.998861	val: 0.722344	test: 0.789776
PRC train: 0.998135	val: 0.946371	test: 0.776929

Epoch: 56
Loss: 0.1627553228794926
ROC train: 0.999386	val: 0.708059	test: 0.793775
PRC train: 0.999057	val: 0.943422	test: 0.770654

Epoch: 57
Loss: 0.17228414277662993
ROC train: 0.998584	val: 0.716117	test: 0.784907
PRC train: 0.997729	val: 0.945681	test: 0.765944

Epoch: 58
Loss: 0.16301063844691457
ROC train: 0.999247	val: 0.686447	test: 0.794818
PRC train: 0.998691	val: 0.938933	test: 0.772417

Epoch: 59
Loss: 0.1505944712790275
ROC train: 0.999224	val: 0.664469	test: 0.775865
PRC train: 0.998769	val: 0.932424	test: 0.759828

Epoch: 60
Loss: 0.14322868143235107
ROC train: 0.999521	val: 0.680952	test: 0.777952
PRC train: 0.999269	val: 0.937322	test: 0.764327

Epoch: 61
Loss: 0.12459686415261344
ROC train: 0.999749	val: 0.696703	test: 0.797253
PRC train: 0.999619	val: 0.941794	test: 0.796941

Epoch: 62
Loss: 0.1194526411796295
ROC train: 0.999803	val: 0.705861	test: 0.801078
PRC train: 0.999702	val: 0.944216	test: 0.799271

Epoch: 63
Loss: 0.1317842675464153
ROC train: 0.999829	val: 0.698535	test: 0.781255
PRC train: 0.999742	val: 0.942142	test: 0.780770

Epoch: 64
Loss: 0.13837773240844536
ROC train: 0.999832	val: 0.707692	test: 0.778126
PRC train: 0.999751	val: 0.944259	test: 0.779181

Epoch: 65
Loss: 0.13484414083260657
ROC train: 0.999615	val: 0.698168	test: 0.776387
PRC train: 0.999403	val: 0.942359	test: 0.773287

Epoch: 66
Loss: 0.12075468844499282
ROC train: 0.998690	val: 0.700000	test: 0.776561
PRC train: 0.997966	val: 0.941174	test: 0.765937

Epoch: 67
Loss: 0.13304856141886195
ROC train: 0.999857	val: 0.684615	test: 0.791167
PRC train: 0.999781	val: 0.939470	test: 0.782797

Epoch: 68
Loss: 0.12118458018205813
ROC train: 0.999840	val: 0.682418	test: 0.783864
PRC train: 0.999755	val: 0.938899	test: 0.778451

Epoch: 69
Loss: 0.12597972366952753
ROC train: 0.999578	val: 0.673993	test: 0.795688
PRC train: 0.999321	val: 0.937730	test: 0.784725

Epoch: 70
Loss: 0.11860969294946948
ROC train: 0.999712	val: 0.667766	test: 0.770822
PRC train: 0.999554	val: 0.934474	test: 0.756639

Epoch: 71
Loss: 0.11827405176176473
ROC train: 0.999546	val: 0.682784	test: 0.757086
PRC train: 0.999349	val: 0.937374	test: 0.747052

Epoch: 72
Loss: 0.12458695003391976
ROC train: 0.999806	val: 0.696703	test: 0.768736
PRC train: 0.999698	val: 0.939976	test: 0.752156

Epoch: 73
Loss: 0.11040051339659947
ROC train: 0.999880	val: 0.721978	test: 0.773431
PRC train: 0.999823	val: 0.947469	test: 0.752540

Epoch: 74
Loss: 0.10449131220347478
ROC train: 1.000000	val: 0.702564	test: 0.785081
PRC train: 1.000000	val: 0.944637	test: 0.774370

Epoch: 75
Loss: 0.10233104873071472
ROC train: 1.000000	val: 0.690110	test: 0.782299
PRC train: 1.000000	val: 0.942060	test: 0.765490

Epoch: 76
Loss: 0.10146230448115068
ROC train: 0.999997	val: 0.678755	test: 0.784038
PRC train: 0.999996	val: 0.938405	test: 0.767278

Epoch: 77
Loss: 0.11595656012541282
ROC train: 0.999986	val: 0.692308	test: 0.779343
PRC train: 0.999978	val: 0.941236	test: 0.768114

Epoch: 78
Loss: 0.08688959967934608
ROC train: 0.999989	val: 0.704762	test: 0.768910
PRC train: 0.999983	val: 0.943644	test: 0.752045

Epoch: 79
Loss: 0.0826159088868321
ROC train: 1.000000	val: 0.687179	test: 0.777778
PRC train: 1.000000	val: 0.938607	test: 0.762812

Epoch: 80
Loss: 0.08346248476861952
ROC train: 1.000000	val: 0.668498	test: 0.773431
PRC train: 1.000000	val: 0.933801	test: 0.769743

Epoch: 81
Loss: 0.09614649122513749
ROC train: 0.999986	val: 0.672161	test: 0.777082
PRC train: 0.999978	val: 0.935126	test: 0.760086

Epoch: 82
Loss: 0.10450943436082538
ROC train: 0.999994	val: 0.661905	test: 0.767866
PRC train: 0.999991	val: 0.932590	test: 0.748316

Epoch: 83
Loss: 0.08500500851435647
ROC train: 0.999991	val: 0.662271	test: 0.766128
PRC train: 0.999987	val: 0.933398	test: 0.743368

Epoch: 84
Loss: 0.08071928106701605
ROC train: 0.999969	val: 0.667766	test: 0.762128
PRC train: 0.999952	val: 0.934701	test: 0.738336

Epoch: 85
Loss: 0.09429897742730692
ROC train: 0.999986	val: 0.702930	test: 0.766302
PRC train: 0.999978	val: 0.942216	test: 0.750239

Epoch: 86
Loss: 0.09614656704499323
ROC train: 0.999837	val: 0.735897	test: 0.770127
PRC train: 0.999738	val: 0.950712	test: 0.772470

Epoch: 87
Loss: 0.08636488424082656
ROC train: 0.999994	val: 0.732601	test: 0.768388
PRC train: 0.999991	val: 0.950247	test: 0.771490

Epoch: 88
Loss: 0.08522901077238866
ROC train: 1.000000	val: 0.732967	test: 0.783342
PRC train: 1.000000	val: 0.951055	test: 0.784180

Epoch: 89
Loss: 0.061496727257686234
ROC train: 1.000000	val: 0.716850	test: 0.778995
PRC train: 1.000000	val: 0.947759	test: 0.772707

Epoch: 90
Loss: 0.08636064981792677
ROC train: 0.999997	val: 0.711722	test: 0.786994
PRC train: 0.999996	val: 0.946013	test: 0.786899

Epoch: 91
Loss: 0.08749607786927213
ROC train: 1.000000	val: 0.696337	test: 0.792210
PRC train: 1.000000	val: 0.941133	test: 0.797103

Epoch: 92
Loss: 0.08242001822032323
ROC train: 1.000000	val: 0.667766	test: 0.784211
PRC train: 1.000000	val: 0.933172	test: 0.778261

Epoch: 93
Loss: 0.07112066968808148
ROC train: 0.999989	val: 0.676190	test: 0.781951
PRC train: 0.989905	val: 0.936885	test: 0.790878

Epoch: 33
Loss: 0.27929790936094234
ROC train: 0.994110	val: 0.684982	test: 0.795514
PRC train: 0.990550	val: 0.938517	test: 0.792894

Epoch: 34
Loss: 0.2582354566582187
ROC train: 0.996958	val: 0.637729	test: 0.797427
PRC train: 0.995417	val: 0.929744	test: 0.790081

Epoch: 35
Loss: 0.2427457689999765
ROC train: 0.996533	val: 0.602930	test: 0.757955
PRC train: 0.994862	val: 0.917725	test: 0.762323

Epoch: 36
Loss: 0.24279982277725193
ROC train: 0.996561	val: 0.600000	test: 0.762128
PRC train: 0.994928	val: 0.920833	test: 0.769358

Epoch: 37
Loss: 0.24480846754265198
ROC train: 0.998099	val: 0.637729	test: 0.759346
PRC train: 0.997196	val: 0.930610	test: 0.758586

Epoch: 38
Loss: 0.23613305052438482
ROC train: 0.997891	val: 0.672894	test: 0.781951
PRC train: 0.996903	val: 0.938816	test: 0.779800

Epoch: 39
Loss: 0.2402404911555068
ROC train: 0.997352	val: 0.675092	test: 0.789602
PRC train: 0.996068	val: 0.938914	test: 0.790546

Epoch: 40
Loss: 0.2086168818159238
ROC train: 0.998216	val: 0.638462	test: 0.788385
PRC train: 0.997381	val: 0.931308	test: 0.789030

Epoch: 41
Loss: 0.2006008213159573
ROC train: 0.999192	val: 0.627839	test: 0.767866
PRC train: 0.998759	val: 0.928761	test: 0.769071

Epoch: 42
Loss: 0.22066072089236616
ROC train: 0.999469	val: 0.625641	test: 0.772561
PRC train: 0.999175	val: 0.928725	test: 0.769489

Epoch: 43
Loss: 0.1790636604445141
ROC train: 0.999563	val: 0.643956	test: 0.774822
PRC train: 0.999304	val: 0.932700	test: 0.771955

Epoch: 44
Loss: 0.15712700477662794
ROC train: 0.999461	val: 0.653480	test: 0.762128
PRC train: 0.999134	val: 0.935876	test: 0.766127

Epoch: 45
Loss: 0.17792118030553744
ROC train: 0.999692	val: 0.652747	test: 0.784038
PRC train: 0.999515	val: 0.935504	test: 0.785294

Epoch: 46
Loss: 0.1679058637648653
ROC train: 0.999757	val: 0.639927	test: 0.782994
PRC train: 0.999629	val: 0.931032	test: 0.779611

Epoch: 47
Loss: 0.17989868103891102
ROC train: 0.999720	val: 0.639560	test: 0.766475
PRC train: 0.999569	val: 0.929947	test: 0.771819

Epoch: 48
Loss: 0.14367669847946102
ROC train: 0.999809	val: 0.657509	test: 0.775691
PRC train: 0.999704	val: 0.936618	test: 0.778311

Epoch: 49
Loss: 0.16697889111574254
ROC train: 0.999860	val: 0.661905	test: 0.798296
PRC train: 0.999787	val: 0.937603	test: 0.792987

Epoch: 50
Loss: 0.17052284290580857
ROC train: 0.999780	val: 0.656777	test: 0.797427
PRC train: 0.999666	val: 0.935745	test: 0.794873

Epoch: 51
Loss: 0.151362789111905
ROC train: 0.999906	val: 0.654579	test: 0.790993
PRC train: 0.999858	val: 0.935867	test: 0.796834

Epoch: 52
Loss: 0.14017777433569084
ROC train: 0.999951	val: 0.653480	test: 0.789254
PRC train: 0.999927	val: 0.934316	test: 0.791372

Epoch: 53
Loss: 0.15684488982676875
ROC train: 0.999969	val: 0.632967	test: 0.779343
PRC train: 0.999952	val: 0.930191	test: 0.782188

Epoch: 54
Loss: 0.1590153926303251
ROC train: 0.999923	val: 0.635531	test: 0.744566
PRC train: 0.999883	val: 0.931284	test: 0.745053

Epoch: 55
Loss: 0.15924923655745804
ROC train: 0.999786	val: 0.649084	test: 0.778995
PRC train: 0.999674	val: 0.933253	test: 0.767973

Epoch: 56
Loss: 0.15364714296352275
ROC train: 0.999820	val: 0.640659	test: 0.785603
PRC train: 0.999728	val: 0.932071	test: 0.784491

Epoch: 57
Loss: 0.13088597746670502
ROC train: 0.999906	val: 0.620879	test: 0.778299
PRC train: 0.999858	val: 0.927782	test: 0.781458

Epoch: 58
Loss: 0.12737758536995786
ROC train: 0.999989	val: 0.627839	test: 0.762824
PRC train: 0.999983	val: 0.931148	test: 0.762413

Epoch: 59
Loss: 0.13080272764317918
ROC train: 0.999880	val: 0.647985	test: 0.757955
PRC train: 0.999815	val: 0.932410	test: 0.767086

Epoch: 60
Loss: 0.1281306172796357
ROC train: 0.999983	val: 0.652747	test: 0.764041
PRC train: 0.999974	val: 0.930613	test: 0.771134

Epoch: 61
Loss: 0.11887339758717211
ROC train: 1.000000	val: 0.664103	test: 0.770649
PRC train: 1.000000	val: 0.935503	test: 0.772303

Epoch: 62
Loss: 0.1358894951515876
ROC train: 0.999983	val: 0.667399	test: 0.780212
PRC train: 0.999974	val: 0.936946	test: 0.786901

Epoch: 63
Loss: 0.10984051726082045
ROC train: 0.999897	val: 0.659341	test: 0.776908
PRC train: 0.999840	val: 0.935051	test: 0.790558

Epoch: 64
Loss: 0.12559207537185121
ROC train: 0.999897	val: 0.637729	test: 0.737263
PRC train: 0.999845	val: 0.932006	test: 0.748389

Epoch: 65
Loss: 0.10967459687517264
ROC train: 1.000000	val: 0.644689	test: 0.777430
PRC train: 1.000000	val: 0.931529	test: 0.785793

Epoch: 66
Loss: 0.10327489094634741
ROC train: 0.999994	val: 0.662637	test: 0.779517
PRC train: 0.999991	val: 0.932975	test: 0.775352

Epoch: 67
Loss: 0.13838281877842512
ROC train: 0.999997	val: 0.669963	test: 0.782473
PRC train: 0.999996	val: 0.936285	test: 0.796181

Epoch: 68
Loss: 0.09280221760881495
ROC train: 0.999997	val: 0.644689	test: 0.771692
PRC train: 0.999996	val: 0.931960	test: 0.781092

Epoch: 69
Loss: 0.11204683121063093
ROC train: 1.000000	val: 0.639560	test: 0.769431
PRC train: 1.000000	val: 0.930132	test: 0.776802

Epoch: 70
Loss: 0.09474762302681343
ROC train: 1.000000	val: 0.637729	test: 0.772735
PRC train: 1.000000	val: 0.929565	test: 0.781449

Epoch: 71
Loss: 0.09304255557101115
ROC train: 1.000000	val: 0.649817	test: 0.783168
PRC train: 1.000000	val: 0.933395	test: 0.790657

Epoch: 72
Loss: 0.09702878766501959
ROC train: 0.999994	val: 0.679853	test: 0.774126
PRC train: 0.999991	val: 0.941344	test: 0.785375

Epoch: 73
Loss: 0.08404678723066664
ROC train: 0.999991	val: 0.695971	test: 0.763346
PRC train: 0.999987	val: 0.943638	test: 0.774324

Epoch: 74
Loss: 0.09689179978878167
ROC train: 0.999997	val: 0.689377	test: 0.774648
PRC train: 0.999996	val: 0.942104	test: 0.777603

Epoch: 75
Loss: 0.09938336620439517
ROC train: 1.000000	val: 0.673626	test: 0.790471
PRC train: 1.000000	val: 0.939147	test: 0.781948

Epoch: 76
Loss: 0.09688112789605585
ROC train: 1.000000	val: 0.661905	test: 0.804208
PRC train: 1.000000	val: 0.936027	test: 0.799149

Epoch: 77
Loss: 0.07108376537288538
ROC train: 0.999997	val: 0.636264	test: 0.786820
PRC train: 0.999996	val: 0.927710	test: 0.790711

Epoch: 78
Loss: 0.07731284076553575
ROC train: 1.000000	val: 0.632601	test: 0.778473
PRC train: 1.000000	val: 0.928853	test: 0.786656

Epoch: 79
Loss: 0.06865273436633065
ROC train: 1.000000	val: 0.672161	test: 0.781603
PRC train: 1.000000	val: 0.938926	test: 0.781891

Epoch: 80
Loss: 0.0942698312057218
ROC train: 1.000000	val: 0.687179	test: 0.764737
PRC train: 1.000000	val: 0.942231	test: 0.768394

Epoch: 81
Loss: 0.04640533413065414
ROC train: 0.999997	val: 0.676557	test: 0.767693
PRC train: 0.999996	val: 0.938492	test: 0.769123

Epoch: 82
Loss: 0.08849606558964193
ROC train: 1.000000	val: 0.674725	test: 0.762128
PRC train: 1.000000	val: 0.937469	test: 0.764020

Epoch: 83
Loss: 0.0683903443785135
ROC train: 1.000000	val: 0.662637	test: 0.767345
PRC train: 1.000000	val: 0.936001	test: 0.773266

Epoch: 84
Loss: 0.05641661989528081
ROC train: 1.000000	val: 0.661172	test: 0.779343
PRC train: 1.000000	val: 0.936170	test: 0.781732

Epoch: 85
Loss: 0.07739104380404292
ROC train: 0.999997	val: 0.672894	test: 0.783690
PRC train: 0.999996	val: 0.937809	test: 0.792077

Epoch: 86
Loss: 0.0766393133653039
ROC train: 1.000000	val: 0.665934	test: 0.767519
PRC train: 1.000000	val: 0.936841	test: 0.767774

Epoch: 87
Loss: 0.09493246704331035
ROC train: 1.000000	val: 0.659707	test: 0.781255
PRC train: 1.000000	val: 0.933311	test: 0.785637

Epoch: 88
Loss: 0.0946492591327492
ROC train: 1.000000	val: 0.652747	test: 0.779864
PRC train: 1.000000	val: 0.930906	test: 0.788615

Epoch: 89
Loss: 0.0763862818044371
ROC train: 1.000000	val: 0.645788	test: 0.762998
PRC train: 1.000000	val: 0.927961	test: 0.773832

Epoch: 90
Loss: 0.07538894984336308
ROC train: 1.000000	val: 0.654579	test: 0.737959
PRC train: 1.000000	val: 0.931758	test: 0.745992

Epoch: 91
Loss: 0.07022974238222632
ROC train: 1.000000	val: 0.666667	test: 0.745783
PRC train: 1.000000	val: 0.936018	test: 0.750826

Epoch: 92
Loss: 0.06931439857227664
ROC train: 1.000000	val: 0.687179	test: 0.766649
PRC train: 1.000000	val: 0.943476	test: 0.766246

Epoch: 93
Loss: 0.06921216923522812
ROC train: 1.000000	val: 0.704029	test: 0.794644
PRC train: 0.998621	val: 0.919302	test: 0.733790

Epoch: 33
Loss: 0.2676466165907266
ROC train: 0.998953	val: 0.649817	test: 0.766475
PRC train: 0.998435	val: 0.918071	test: 0.722708

Epoch: 34
Loss: 0.26979213571292976
ROC train: 0.999244	val: 0.632601	test: 0.739524
PRC train: 0.998874	val: 0.912918	test: 0.694099

Epoch: 35
Loss: 0.25463947087922273
ROC train: 0.999435	val: 0.632601	test: 0.736915
PRC train: 0.999179	val: 0.905805	test: 0.700820

Epoch: 36
Loss: 0.24085301013350446
ROC train: 0.999329	val: 0.648718	test: 0.755347
PRC train: 0.999008	val: 0.913134	test: 0.701881

Epoch: 37
Loss: 0.24541131599410032
ROC train: 0.999064	val: 0.649817	test: 0.745262
PRC train: 0.998509	val: 0.918994	test: 0.690392

Epoch: 38
Loss: 0.2257060369387442
ROC train: 0.999715	val: 0.643590	test: 0.735003
PRC train: 0.999553	val: 0.916424	test: 0.691028

Epoch: 39
Loss: 0.19849533642798484
ROC train: 0.999943	val: 0.648718	test: 0.730656
PRC train: 0.999914	val: 0.916865	test: 0.695534

Epoch: 40
Loss: 0.20465798121303128
ROC train: 0.999943	val: 0.657143	test: 0.733438
PRC train: 0.999915	val: 0.921966	test: 0.682737

Epoch: 41
Loss: 0.2172705149321522
ROC train: 0.999966	val: 0.660073	test: 0.741610
PRC train: 0.999948	val: 0.921883	test: 0.685980

Epoch: 42
Loss: 0.19148374246578617
ROC train: 0.999971	val: 0.653846	test: 0.732220
PRC train: 0.999957	val: 0.913179	test: 0.705285

Epoch: 43
Loss: 0.18480238840713828
ROC train: 0.999849	val: 0.658974	test: 0.722831
PRC train: 0.999771	val: 0.924595	test: 0.687682

Epoch: 44
Loss: 0.15711020010738713
ROC train: 0.999926	val: 0.620147	test: 0.712572
PRC train: 0.999888	val: 0.911553	test: 0.702344

Epoch: 45
Loss: 0.1768951337285256
ROC train: 0.999852	val: 0.587179	test: 0.707877
PRC train: 0.999769	val: 0.893434	test: 0.696772

Epoch: 46
Loss: 0.18129471635118585
ROC train: 0.999806	val: 0.571429	test: 0.717440
PRC train: 0.999698	val: 0.892370	test: 0.682487

Epoch: 47
Loss: 0.16836115678570313
ROC train: 0.999769	val: 0.578022	test: 0.706834
PRC train: 0.999621	val: 0.895363	test: 0.695957

Epoch: 48
Loss: 0.15438911664495328
ROC train: 0.999766	val: 0.612454	test: 0.706834
PRC train: 0.999640	val: 0.908814	test: 0.706497

Epoch: 49
Loss: 0.14849926303820307
ROC train: 0.999874	val: 0.619414	test: 0.719005
PRC train: 0.999807	val: 0.911088	test: 0.716888

Epoch: 50
Loss: 0.1390128897153883
ROC train: 0.999980	val: 0.611355	test: 0.729612
PRC train: 0.999970	val: 0.906417	test: 0.721203

Epoch: 51
Loss: 0.1621498774365624
ROC train: 0.999997	val: 0.627839	test: 0.730482
PRC train: 0.999996	val: 0.911337	test: 0.694844

Epoch: 52
Loss: 0.15989332727572284
ROC train: 0.999997	val: 0.652381	test: 0.737785
PRC train: 0.999996	val: 0.924655	test: 0.677955

Epoch: 53
Loss: 0.15396103615546047
ROC train: 0.999917	val: 0.670696	test: 0.745957
PRC train: 0.999881	val: 0.923905	test: 0.683717

Epoch: 54
Loss: 0.15389712999292798
ROC train: 0.999989	val: 0.649451	test: 0.741784
PRC train: 0.999983	val: 0.912973	test: 0.696915

Epoch: 55
Loss: 0.15243333230683148
ROC train: 1.000000	val: 0.626374	test: 0.721961
PRC train: 1.000000	val: 0.911802	test: 0.703131

Epoch: 56
Loss: 0.1514422239913143
ROC train: 1.000000	val: 0.628938	test: 0.713267
PRC train: 1.000000	val: 0.921701	test: 0.709406

Epoch: 57
Loss: 0.12863995899859101
ROC train: 1.000000	val: 0.654579	test: 0.731873
PRC train: 1.000000	val: 0.925734	test: 0.700550

Epoch: 58
Loss: 0.12770593967394575
ROC train: 1.000000	val: 0.664103	test: 0.727352
PRC train: 1.000000	val: 0.923544	test: 0.693274

Epoch: 59
Loss: 0.1197872787012765
ROC train: 1.000000	val: 0.652015	test: 0.743697
PRC train: 1.000000	val: 0.921376	test: 0.695746

Epoch: 60
Loss: 0.12634158137072313
ROC train: 1.000000	val: 0.619048	test: 0.744045
PRC train: 1.000000	val: 0.911943	test: 0.693594

Epoch: 61
Loss: 0.11366234570047647
ROC train: 1.000000	val: 0.594139	test: 0.745436
PRC train: 1.000000	val: 0.899604	test: 0.704307

Epoch: 62
Loss: 0.12633115747188092
ROC train: 1.000000	val: 0.618681	test: 0.753434
PRC train: 1.000000	val: 0.912267	test: 0.699103

Epoch: 63
Loss: 0.12441614580221187
ROC train: 1.000000	val: 0.656410	test: 0.760389
PRC train: 1.000000	val: 0.923176	test: 0.699353

Epoch: 64
Loss: 0.09935277035183585
ROC train: 1.000000	val: 0.669231	test: 0.756390
PRC train: 1.000000	val: 0.927792	test: 0.692391

Epoch: 65
Loss: 0.11323446792645989
ROC train: 1.000000	val: 0.661172	test: 0.746827
PRC train: 1.000000	val: 0.922072	test: 0.689162

Epoch: 66
Loss: 0.09067655900544358
ROC train: 0.999989	val: 0.662637	test: 0.739176
PRC train: 0.999983	val: 0.922603	test: 0.693658

Epoch: 67
Loss: 0.09250500667245419
ROC train: 1.000000	val: 0.669963	test: 0.766128
PRC train: 1.000000	val: 0.931481	test: 0.709592

Epoch: 68
Loss: 0.09344571529228649
ROC train: 1.000000	val: 0.642491	test: 0.755173
PRC train: 1.000000	val: 0.924552	test: 0.713162

Epoch: 69
Loss: 0.1084619716487171
ROC train: 1.000000	val: 0.639194	test: 0.754477
PRC train: 1.000000	val: 0.922822	test: 0.699175

Epoch: 70
Loss: 0.08546748564529333
ROC train: 1.000000	val: 0.631502	test: 0.748739
PRC train: 1.000000	val: 0.918286	test: 0.696837

Epoch: 71
Loss: 0.08599550852737643
ROC train: 1.000000	val: 0.637363	test: 0.740915
PRC train: 1.000000	val: 0.910885	test: 0.704837

Epoch: 72
Loss: 0.10872252311230723
ROC train: 1.000000	val: 0.656777	test: 0.744566
PRC train: 1.000000	val: 0.913331	test: 0.697695

Epoch: 73
Loss: 0.07641097427721062
ROC train: 1.000000	val: 0.687179	test: 0.752391
PRC train: 1.000000	val: 0.931668	test: 0.699531

Epoch: 74
Loss: 0.0950001908838861
ROC train: 1.000000	val: 0.673626	test: 0.732916
PRC train: 1.000000	val: 0.928239	test: 0.684885

Epoch: 75
Loss: 0.07823941367665459
ROC train: 1.000000	val: 0.635897	test: 0.719701
PRC train: 1.000000	val: 0.907847	test: 0.670710

Epoch: 76
Loss: 0.07310502257975518
ROC train: 1.000000	val: 0.632967	test: 0.719005
PRC train: 1.000000	val: 0.904035	test: 0.659030

Epoch: 77
Loss: 0.09701861650706238
ROC train: 1.000000	val: 0.633333	test: 0.723005
PRC train: 1.000000	val: 0.909790	test: 0.669028

Epoch: 78
Loss: 0.06770991031650693
ROC train: 1.000000	val: 0.634799	test: 0.722831
PRC train: 1.000000	val: 0.910069	test: 0.678372

Epoch: 79
Loss: 0.08742945640244802
ROC train: 1.000000	val: 0.640659	test: 0.731351
PRC train: 1.000000	val: 0.913878	test: 0.691990

Epoch: 80
Loss: 0.08414039749771132
ROC train: 1.000000	val: 0.644689	test: 0.731525
PRC train: 1.000000	val: 0.915017	test: 0.695279

Epoch: 81
Loss: 0.06674847636320594
ROC train: 1.000000	val: 0.652747	test: 0.732047
PRC train: 1.000000	val: 0.921334	test: 0.690300

Epoch: 82
Loss: 0.06227829202612525
ROC train: 1.000000	val: 0.652015	test: 0.732220
PRC train: 1.000000	val: 0.924525	test: 0.696255

Epoch: 83
Loss: 0.06800348604690165
ROC train: 1.000000	val: 0.643590	test: 0.734307
PRC train: 1.000000	val: 0.921222	test: 0.704585

Epoch: 84
Loss: 0.0764906678661254
ROC train: 1.000000	val: 0.630769	test: 0.721266
PRC train: 1.000000	val: 0.917346	test: 0.706562

Epoch: 85
Loss: 0.06662757929793786
ROC train: 1.000000	val: 0.621612	test: 0.710137
PRC train: 1.000000	val: 0.915741	test: 0.694886

Epoch: 86
Loss: 0.060415556138927594
ROC train: 1.000000	val: 0.622344	test: 0.708399
PRC train: 1.000000	val: 0.915847	test: 0.684275

Epoch: 87
Loss: 0.07586025974617865
ROC train: 1.000000	val: 0.621978	test: 0.707355
PRC train: 1.000000	val: 0.912167	test: 0.676508

Epoch: 88
Loss: 0.07844308637837243
ROC train: 1.000000	val: 0.634066	test: 0.709790
PRC train: 1.000000	val: 0.916298	test: 0.684287

Epoch: 89
Loss: 0.0669820856482887
ROC train: 1.000000	val: 0.647619	test: 0.708746
PRC train: 1.000000	val: 0.922475	test: 0.680337

Epoch: 90
Loss: 0.08059557326188277
ROC train: 1.000000	val: 0.632234	test: 0.698661
PRC train: 1.000000	val: 0.918097	test: 0.690105

Epoch: 91
Loss: 0.061430278593651286
ROC train: 1.000000	val: 0.628938	test: 0.686663
PRC train: 1.000000	val: 0.919783	test: 0.684368

Epoch: 92
Loss: 0.06752274303454127
ROC train: 1.000000	val: 0.638095	test: 0.701617
PRC train: 1.000000	val: 0.921857	test: 0.685192

Epoch: 93
Loss: 0.07333237103499778
ROC train: 1.000000	val: 0.644322	test: 0.711181
PRC train: 0.991923	val: 0.900299	test: 0.735137

Epoch: 33
Loss: 0.2946774772496318
ROC train: 0.997286	val: 0.619414	test: 0.755347
PRC train: 0.995674	val: 0.912633	test: 0.744940

Epoch: 34
Loss: 0.26342600935489213
ROC train: 0.997260	val: 0.635165	test: 0.743697
PRC train: 0.995740	val: 0.915060	test: 0.726999

Epoch: 35
Loss: 0.2841058934794217
ROC train: 0.997577	val: 0.610623	test: 0.752217
PRC train: 0.996349	val: 0.908044	test: 0.761754

Epoch: 36
Loss: 0.23287060210952407
ROC train: 0.998787	val: 0.631502	test: 0.758998
PRC train: 0.998225	val: 0.901651	test: 0.758626

Epoch: 37
Loss: 0.2553968529699585
ROC train: 0.998102	val: 0.646886	test: 0.771344
PRC train: 0.997390	val: 0.915881	test: 0.774421

Epoch: 38
Loss: 0.23794558901100799
ROC train: 0.997803	val: 0.644689	test: 0.760216
PRC train: 0.996715	val: 0.922221	test: 0.765051

Epoch: 39
Loss: 0.22098001790324467
ROC train: 0.998533	val: 0.631868	test: 0.742306
PRC train: 0.997926	val: 0.915963	test: 0.760188

Epoch: 40
Loss: 0.22688392752752087
ROC train: 0.998699	val: 0.641758	test: 0.727004
PRC train: 0.998341	val: 0.917557	test: 0.745711

Epoch: 41
Loss: 0.21086471929542755
ROC train: 0.998876	val: 0.617582	test: 0.738828
PRC train: 0.998551	val: 0.912953	test: 0.753239

Epoch: 42
Loss: 0.20070045783991755
ROC train: 0.999712	val: 0.637363	test: 0.768388
PRC train: 0.999572	val: 0.918102	test: 0.773502

Epoch: 43
Loss: 0.18254621926478345
ROC train: 0.999737	val: 0.650183	test: 0.776908
PRC train: 0.999591	val: 0.920211	test: 0.784239

Epoch: 44
Loss: 0.1939237264987176
ROC train: 0.999883	val: 0.674359	test: 0.756912
PRC train: 0.999823	val: 0.927437	test: 0.759437

Epoch: 45
Loss: 0.1765279859038317
ROC train: 0.999832	val: 0.680952	test: 0.749261
PRC train: 0.999735	val: 0.927709	test: 0.733977

Epoch: 46
Loss: 0.18134393856293704
ROC train: 0.999834	val: 0.668864	test: 0.748044
PRC train: 0.999739	val: 0.920239	test: 0.728745

Epoch: 47
Loss: 0.18177175693570508
ROC train: 0.999874	val: 0.654579	test: 0.764910
PRC train: 0.999807	val: 0.917171	test: 0.778366

Epoch: 48
Loss: 0.19013732264003175
ROC train: 0.999857	val: 0.644689	test: 0.759694
PRC train: 0.999795	val: 0.915603	test: 0.767152

Epoch: 49
Loss: 0.1544230059626468
ROC train: 0.999603	val: 0.670330	test: 0.757086
PRC train: 0.999412	val: 0.915419	test: 0.752836

Epoch: 50
Loss: 0.16229580698283336
ROC train: 0.999820	val: 0.671429	test: 0.784211
PRC train: 0.999729	val: 0.919041	test: 0.786906

Epoch: 51
Loss: 0.1845953945032166
ROC train: 0.999923	val: 0.676923	test: 0.771518
PRC train: 0.999885	val: 0.924195	test: 0.760650

Epoch: 52
Loss: 0.18336527303827818
ROC train: 0.999432	val: 0.643956	test: 0.738654
PRC train: 0.999199	val: 0.914317	test: 0.704449

Epoch: 53
Loss: 0.15250578126223074
ROC train: 0.999834	val: 0.656410	test: 0.765780
PRC train: 0.999761	val: 0.910020	test: 0.764911

Epoch: 54
Loss: 0.1380290279235225
ROC train: 0.999983	val: 0.667033	test: 0.771170
PRC train: 0.999974	val: 0.912513	test: 0.757137

Epoch: 55
Loss: 0.1502195688117362
ROC train: 1.000000	val: 0.672894	test: 0.767171
PRC train: 1.000000	val: 0.914396	test: 0.747515

Epoch: 56
Loss: 0.15484866558797677
ROC train: 0.999980	val: 0.668864	test: 0.764563
PRC train: 0.999970	val: 0.924354	test: 0.757749

Epoch: 57
Loss: 0.1421273927666871
ROC train: 0.999997	val: 0.676923	test: 0.771344
PRC train: 0.999996	val: 0.929586	test: 0.779221

Epoch: 58
Loss: 0.14825665650231007
ROC train: 1.000000	val: 0.663004	test: 0.777778
PRC train: 1.000000	val: 0.919628	test: 0.791016

Epoch: 59
Loss: 0.13705799824277493
ROC train: 1.000000	val: 0.673993	test: 0.784211
PRC train: 1.000000	val: 0.917411	test: 0.790452

Epoch: 60
Loss: 0.12869678121393066
ROC train: 0.999994	val: 0.693773	test: 0.784559
PRC train: 0.999991	val: 0.928702	test: 0.794161

Epoch: 61
Loss: 0.12071409399398503
ROC train: 1.000000	val: 0.684615	test: 0.792558
PRC train: 1.000000	val: 0.922996	test: 0.800147

Epoch: 62
Loss: 0.11309851528760255
ROC train: 1.000000	val: 0.668864	test: 0.800730
PRC train: 1.000000	val: 0.915560	test: 0.803850

Epoch: 63
Loss: 0.13709614440229373
ROC train: 1.000000	val: 0.653846	test: 0.786472
PRC train: 1.000000	val: 0.908659	test: 0.786761

Epoch: 64
Loss: 0.10789587230155022
ROC train: 1.000000	val: 0.661905	test: 0.777778
PRC train: 1.000000	val: 0.917745	test: 0.777445

Epoch: 65
Loss: 0.11568364326219054
ROC train: 0.999989	val: 0.679853	test: 0.777430
PRC train: 0.999983	val: 0.923278	test: 0.784563

Epoch: 66
Loss: 0.11288353151658086
ROC train: 0.999963	val: 0.682784	test: 0.769258
PRC train: 0.999943	val: 0.920397	test: 0.771718

Epoch: 67
Loss: 0.13361681850344764
ROC train: 0.999989	val: 0.665934	test: 0.770822
PRC train: 0.999983	val: 0.909878	test: 0.776459

Epoch: 68
Loss: 0.10462423205920712
ROC train: 1.000000	val: 0.646520	test: 0.794123
PRC train: 1.000000	val: 0.910116	test: 0.792303

Epoch: 69
Loss: 0.09722157704623183
ROC train: 0.999974	val: 0.649817	test: 0.776908
PRC train: 0.999961	val: 0.917822	test: 0.740465

Epoch: 70
Loss: 0.11081179866115438
ROC train: 1.000000	val: 0.651648	test: 0.794818
PRC train: 1.000000	val: 0.916730	test: 0.786567

Epoch: 71
Loss: 0.09449407108275465
ROC train: 1.000000	val: 0.637729	test: 0.780038
PRC train: 1.000000	val: 0.911776	test: 0.800420

Epoch: 72
Loss: 0.07355696047652376
ROC train: 1.000000	val: 0.642857	test: 0.779343
PRC train: 1.000000	val: 0.906095	test: 0.783173

Epoch: 73
Loss: 0.09814498594887458
ROC train: 1.000000	val: 0.630403	test: 0.791862
PRC train: 1.000000	val: 0.893481	test: 0.776144

Epoch: 74
Loss: 0.11563866335113564
ROC train: 1.000000	val: 0.641392	test: 0.793775
PRC train: 1.000000	val: 0.894565	test: 0.798465

Epoch: 75
Loss: 0.11624338662987262
ROC train: 1.000000	val: 0.645421	test: 0.786298
PRC train: 1.000000	val: 0.900756	test: 0.793210

Epoch: 76
Loss: 0.09881492903134274
ROC train: 1.000000	val: 0.645055	test: 0.780734
PRC train: 1.000000	val: 0.901551	test: 0.782208

Epoch: 77
Loss: 0.07957538457845001
ROC train: 1.000000	val: 0.641026	test: 0.789602
PRC train: 1.000000	val: 0.907472	test: 0.785882

Epoch: 78
Loss: 0.09999879433194309
ROC train: 1.000000	val: 0.610989	test: 0.791688
PRC train: 1.000000	val: 0.899586	test: 0.783007

Epoch: 79
Loss: 0.07101932731577112
ROC train: 1.000000	val: 0.626007	test: 0.780386
PRC train: 1.000000	val: 0.898808	test: 0.759831

Epoch: 80
Loss: 0.06647364738622087
ROC train: 1.000000	val: 0.649084	test: 0.782299
PRC train: 1.000000	val: 0.906694	test: 0.783009

Epoch: 81
Loss: 0.07720997165264512
ROC train: 1.000000	val: 0.672894	test: 0.786124
PRC train: 1.000000	val: 0.914649	test: 0.795276

Epoch: 82
Loss: 0.06653849166119519
ROC train: 1.000000	val: 0.673993	test: 0.789776
PRC train: 1.000000	val: 0.915680	test: 0.799888

Epoch: 83
Loss: 0.07352213335782956
ROC train: 1.000000	val: 0.673993	test: 0.791862
PRC train: 1.000000	val: 0.912989	test: 0.801740

Epoch: 84
Loss: 0.07909545266142531
ROC train: 1.000000	val: 0.667033	test: 0.794992
PRC train: 1.000000	val: 0.904032	test: 0.801194

Epoch: 85
Loss: 0.06303538464697833
ROC train: 1.000000	val: 0.661538	test: 0.787689
PRC train: 1.000000	val: 0.904386	test: 0.786483

Epoch: 86
Loss: 0.08769275227093827
ROC train: 1.000000	val: 0.681685	test: 0.778126
PRC train: 1.000000	val: 0.911468	test: 0.786546

Epoch: 87
Loss: 0.08172788422773998
ROC train: 1.000000	val: 0.679121	test: 0.770996
PRC train: 1.000000	val: 0.913607	test: 0.782320

Epoch: 88
Loss: 0.0635746406702868
ROC train: 1.000000	val: 0.669597	test: 0.780212
PRC train: 1.000000	val: 0.910190	test: 0.787574

Epoch: 89
Loss: 0.0635412660333045
ROC train: 1.000000	val: 0.661538	test: 0.780734
PRC train: 1.000000	val: 0.909174	test: 0.784392

Epoch: 90
Loss: 0.07227429634001623
ROC train: 1.000000	val: 0.649817	test: 0.781951
PRC train: 1.000000	val: 0.905480	test: 0.778525

Epoch: 91
Loss: 0.052647532203510164
ROC train: 1.000000	val: 0.647253	test: 0.794471
PRC train: 1.000000	val: 0.908430	test: 0.789944

Epoch: 92
Loss: 0.05525346138718712
ROC train: 1.000000	val: 0.653480	test: 0.798296
PRC train: 1.000000	val: 0.915220	test: 0.808324

Epoch: 93
Loss: 0.06801135235315366
ROC train: 1.000000	val: 0.652747	test: 0.805947
PRC train: 0.991375	val: 0.934328	test: 0.760931

Epoch: 33
Loss: 0.2520664103893385
ROC train: 0.997660	val: 0.597070	test: 0.795514
PRC train: 0.996328	val: 0.911299	test: 0.787352

Epoch: 34
Loss: 0.23664023827601705
ROC train: 0.997905	val: 0.613553	test: 0.791515
PRC train: 0.996745	val: 0.919820	test: 0.781906

Epoch: 35
Loss: 0.21437311870346862
ROC train: 0.998199	val: 0.599634	test: 0.782994
PRC train: 0.997183	val: 0.915443	test: 0.772651

Epoch: 36
Loss: 0.22367147040331004
ROC train: 0.998624	val: 0.569963	test: 0.783516
PRC train: 0.997889	val: 0.906782	test: 0.779205

Epoch: 37
Loss: 0.20801066116473832
ROC train: 0.998936	val: 0.569597	test: 0.785255
PRC train: 0.998366	val: 0.908188	test: 0.781520

Epoch: 38
Loss: 0.20576792983155978
ROC train: 0.996949	val: 0.588645	test: 0.763346
PRC train: 0.995199	val: 0.918152	test: 0.748927

Epoch: 39
Loss: 0.20380269377050492
ROC train: 0.997089	val: 0.575092	test: 0.776039
PRC train: 0.995582	val: 0.915634	test: 0.741300

Epoch: 40
Loss: 0.18344351444403897
ROC train: 0.999515	val: 0.598901	test: 0.785603
PRC train: 0.999252	val: 0.919797	test: 0.778226

Epoch: 41
Loss: 0.1722347549723442
ROC train: 0.999078	val: 0.578388	test: 0.767693
PRC train: 0.998593	val: 0.910325	test: 0.756051

Epoch: 42
Loss: 0.19459007857770938
ROC train: 0.998975	val: 0.608425	test: 0.765954
PRC train: 0.998429	val: 0.921633	test: 0.729684

Epoch: 43
Loss: 0.16921032548731232
ROC train: 0.998019	val: 0.613919	test: 0.739524
PRC train: 0.997058	val: 0.924396	test: 0.702964

Epoch: 44
Loss: 0.1736006900526037
ROC train: 0.999672	val: 0.605861	test: 0.746827
PRC train: 0.999510	val: 0.923462	test: 0.743939

Epoch: 45
Loss: 0.1590193189066747
ROC train: 0.999803	val: 0.577289	test: 0.759868
PRC train: 0.999723	val: 0.910923	test: 0.747742

Epoch: 46
Loss: 0.15703536627079537
ROC train: 0.999526	val: 0.583883	test: 0.753086
PRC train: 0.999334	val: 0.912915	test: 0.736500

Epoch: 47
Loss: 0.16782835013908345
ROC train: 0.999512	val: 0.603663	test: 0.743523
PRC train: 0.999247	val: 0.917383	test: 0.737887

Epoch: 48
Loss: 0.1335458260392909
ROC train: 0.999424	val: 0.607692	test: 0.758651
PRC train: 0.999040	val: 0.918852	test: 0.757015

Epoch: 49
Loss: 0.16287721146394563
ROC train: 0.999934	val: 0.613919	test: 0.770475
PRC train: 0.999901	val: 0.919747	test: 0.769160

Epoch: 50
Loss: 0.13393816295469146
ROC train: 0.999977	val: 0.624908	test: 0.770649
PRC train: 0.999966	val: 0.923585	test: 0.764387

Epoch: 51
Loss: 0.14994121509641747
ROC train: 0.999923	val: 0.633333	test: 0.770301
PRC train: 0.999885	val: 0.926804	test: 0.766917

Epoch: 52
Loss: 0.1358137991417888
ROC train: 0.999626	val: 0.600000	test: 0.753086
PRC train: 0.999433	val: 0.919562	test: 0.739080

Epoch: 53
Loss: 0.12854184788411738
ROC train: 0.999663	val: 0.595971	test: 0.750130
PRC train: 0.999470	val: 0.916767	test: 0.742010

Epoch: 54
Loss: 0.13584641027908323
ROC train: 0.999854	val: 0.572527	test: 0.741958
PRC train: 0.999783	val: 0.908226	test: 0.729562

Epoch: 55
Loss: 0.12563658138894448
ROC train: 0.999932	val: 0.550916	test: 0.766997
PRC train: 0.999899	val: 0.900518	test: 0.751096

Epoch: 56
Loss: 0.140885644470838
ROC train: 0.999809	val: 0.557509	test: 0.766302
PRC train: 0.999713	val: 0.897709	test: 0.774917

Epoch: 57
Loss: 0.12931063836103407
ROC train: 0.999963	val: 0.564103	test: 0.770822
PRC train: 0.999944	val: 0.905884	test: 0.774286

Epoch: 58
Loss: 0.12196613511433656
ROC train: 0.999640	val: 0.536264	test: 0.774126
PRC train: 0.999449	val: 0.899162	test: 0.765085

Epoch: 59
Loss: 0.11297737895959707
ROC train: 0.999843	val: 0.560806	test: 0.767345
PRC train: 0.999760	val: 0.900065	test: 0.768121

Epoch: 60
Loss: 0.08808821221658694
ROC train: 0.999914	val: 0.612088	test: 0.769953
PRC train: 0.999871	val: 0.920195	test: 0.765662

Epoch: 61
Loss: 0.09973589468580579
ROC train: 0.999983	val: 0.645055	test: 0.779343
PRC train: 0.999974	val: 0.931971	test: 0.750880

Epoch: 62
Loss: 0.09359351512067822
ROC train: 0.999997	val: 0.644689	test: 0.787341
PRC train: 0.999996	val: 0.931614	test: 0.768214

Epoch: 63
Loss: 0.09749954054832682
ROC train: 1.000000	val: 0.628938	test: 0.792036
PRC train: 1.000000	val: 0.925827	test: 0.779375

Epoch: 64
Loss: 0.09291396509314712
ROC train: 0.999980	val: 0.624176	test: 0.790645
PRC train: 0.999970	val: 0.925517	test: 0.779185

Epoch: 65
Loss: 0.10450504371148459
ROC train: 1.000000	val: 0.597802	test: 0.768388
PRC train: 1.000000	val: 0.918607	test: 0.755328

Epoch: 66
Loss: 0.11929904334569552
ROC train: 1.000000	val: 0.569963	test: 0.750478
PRC train: 1.000000	val: 0.911519	test: 0.749174

Epoch: 67
Loss: 0.0986001007454489
ROC train: 1.000000	val: 0.575824	test: 0.754999
PRC train: 1.000000	val: 0.909486	test: 0.755500

Epoch: 68
Loss: 0.10312588611765254
ROC train: 0.999977	val: 0.632967	test: 0.759172
PRC train: 0.999965	val: 0.929089	test: 0.745756

Epoch: 69
Loss: 0.10134454491965854
ROC train: 0.999991	val: 0.613553	test: 0.763693
PRC train: 0.999987	val: 0.922809	test: 0.757263

Epoch: 70
Loss: 0.09627415417311243
ROC train: 0.999986	val: 0.588278	test: 0.767519
PRC train: 0.999978	val: 0.915704	test: 0.755271

Epoch: 71
Loss: 0.09067620689572868
ROC train: 0.999994	val: 0.604029	test: 0.766475
PRC train: 0.999991	val: 0.920614	test: 0.759656

Epoch: 72
Loss: 0.07985848476477955
ROC train: 1.000000	val: 0.622344	test: 0.769084
PRC train: 1.000000	val: 0.925854	test: 0.761321

Epoch: 73
Loss: 0.07678974354620294
ROC train: 0.999994	val: 0.635531	test: 0.762650
PRC train: 0.999991	val: 0.928064	test: 0.751907

Epoch: 74
Loss: 0.08278283806531081
ROC train: 1.000000	val: 0.632234	test: 0.761433
PRC train: 1.000000	val: 0.926415	test: 0.741462

Epoch: 75
Loss: 0.07901133492909376
ROC train: 1.000000	val: 0.616117	test: 0.762128
PRC train: 1.000000	val: 0.921708	test: 0.740646

Epoch: 76
Loss: 0.07881052991001382
ROC train: 1.000000	val: 0.600733	test: 0.754130
PRC train: 1.000000	val: 0.915768	test: 0.741855

Epoch: 77
Loss: 0.0832614238510723
ROC train: 0.999991	val: 0.621978	test: 0.738654
PRC train: 0.999987	val: 0.923708	test: 0.703259

Epoch: 78
Loss: 0.07519412936748583
ROC train: 0.999986	val: 0.601465	test: 0.736220
PRC train: 0.999978	val: 0.917887	test: 0.711406

Epoch: 79
Loss: 0.10028107689523898
ROC train: 1.000000	val: 0.597802	test: 0.746827
PRC train: 1.000000	val: 0.914751	test: 0.731725

Epoch: 80
Loss: 0.07407459575321221
ROC train: 0.999989	val: 0.570696	test: 0.744392
PRC train: 0.999983	val: 0.907050	test: 0.738125

Epoch: 81
Loss: 0.08846019266838848
ROC train: 0.999983	val: 0.570696	test: 0.714484
PRC train: 0.999974	val: 0.908191	test: 0.716075

Epoch: 82
Loss: 0.10033943267191033
ROC train: 0.999977	val: 0.577289	test: 0.713267
PRC train: 0.999966	val: 0.912162	test: 0.703938

Epoch: 83
Loss: 0.09232603099600545
ROC train: 1.000000	val: 0.599267	test: 0.758651
PRC train: 1.000000	val: 0.916738	test: 0.739356

Epoch: 84
Loss: 0.07935290694697192
ROC train: 1.000000	val: 0.565934	test: 0.774648
PRC train: 1.000000	val: 0.899997	test: 0.762001

Epoch: 85
Loss: 0.07477167665876124
ROC train: 1.000000	val: 0.536996	test: 0.772387
PRC train: 1.000000	val: 0.892389	test: 0.763619

Epoch: 86
Loss: 0.08084463230799481
ROC train: 1.000000	val: 0.548352	test: 0.779517
PRC train: 1.000000	val: 0.901001	test: 0.768450

Epoch: 87
Loss: 0.07641346985135725
ROC train: 1.000000	val: 0.574725	test: 0.784559
PRC train: 1.000000	val: 0.909867	test: 0.770477

Epoch: 88
Loss: 0.05559586996893371
ROC train: 1.000000	val: 0.590110	test: 0.795340
PRC train: 1.000000	val: 0.916998	test: 0.781507

Epoch: 89
Loss: 0.06233222347478766
ROC train: 1.000000	val: 0.579487	test: 0.792732
PRC train: 1.000000	val: 0.912650	test: 0.780905

Epoch: 90
Loss: 0.05372731664585595
ROC train: 1.000000	val: 0.568132	test: 0.781603
PRC train: 1.000000	val: 0.908506	test: 0.767346

Epoch: 91
Loss: 0.06368713920481868
ROC train: 1.000000	val: 0.564469	test: 0.777082
PRC train: 1.000000	val: 0.907966	test: 0.759476

Epoch: 92
Loss: 0.06035922078851761
ROC train: 1.000000	val: 0.582784	test: 0.772040
PRC train: 1.000000	val: 0.913443	test: 0.757467

Epoch: 93
Loss: 0.05001458574642784
ROC train: 1.000000	val: 0.621612	test: 0.763693
PRC train: 0.994286	val: 0.894603	test: 0.732550

Epoch: 33
Loss: 0.2438183425678308
ROC train: 0.995788	val: 0.586813	test: 0.743523
PRC train: 0.993821	val: 0.897498	test: 0.740845

Epoch: 34
Loss: 0.24493474937004237
ROC train: 0.996807	val: 0.596703	test: 0.714658
PRC train: 0.995246	val: 0.898617	test: 0.704874

Epoch: 35
Loss: 0.24812324039207603
ROC train: 0.997255	val: 0.582418	test: 0.684750
PRC train: 0.995888	val: 0.893641	test: 0.684949

Epoch: 36
Loss: 0.2199778733584637
ROC train: 0.997534	val: 0.597070	test: 0.663537
PRC train: 0.996465	val: 0.902983	test: 0.657842

Epoch: 37
Loss: 0.2195719644000227
ROC train: 0.998801	val: 0.605861	test: 0.659711
PRC train: 0.998226	val: 0.907338	test: 0.655235

Epoch: 38
Loss: 0.20928373256743535
ROC train: 0.998502	val: 0.621978	test: 0.685446
PRC train: 0.997696	val: 0.911033	test: 0.696384

Epoch: 39
Loss: 0.2263712693952587
ROC train: 0.998467	val: 0.639927	test: 0.717788
PRC train: 0.997740	val: 0.913371	test: 0.706117

Epoch: 40
Loss: 0.19418097362016443
ROC train: 0.998633	val: 0.624908	test: 0.743349
PRC train: 0.997903	val: 0.911594	test: 0.731122

Epoch: 41
Loss: 0.19051077834561517
ROC train: 0.998984	val: 0.632967	test: 0.730656
PRC train: 0.998433	val: 0.912868	test: 0.723825

Epoch: 42
Loss: 0.19552495159967287
ROC train: 0.999269	val: 0.632234	test: 0.703182
PRC train: 0.998885	val: 0.912359	test: 0.694934

Epoch: 43
Loss: 0.1943100386161416
ROC train: 0.999352	val: 0.628205	test: 0.693619
PRC train: 0.999022	val: 0.910683	test: 0.694626

Epoch: 44
Loss: 0.16575306949644356
ROC train: 0.999412	val: 0.609158	test: 0.701269
PRC train: 0.999120	val: 0.903564	test: 0.702386

Epoch: 45
Loss: 0.17052507339374412
ROC train: 0.999463	val: 0.602564	test: 0.695531
PRC train: 0.999213	val: 0.902831	test: 0.704793

Epoch: 46
Loss: 0.1465313317012374
ROC train: 0.999866	val: 0.620513	test: 0.711181
PRC train: 0.999802	val: 0.909665	test: 0.721895

Epoch: 47
Loss: 0.13826290769086422
ROC train: 0.999777	val: 0.636996	test: 0.708225
PRC train: 0.999672	val: 0.917673	test: 0.710864

Epoch: 48
Loss: 0.14331303246254198
ROC train: 0.999720	val: 0.644322	test: 0.713789
PRC train: 0.999602	val: 0.920625	test: 0.720835

Epoch: 49
Loss: 0.16549575163821584
ROC train: 0.999971	val: 0.609890	test: 0.731873
PRC train: 0.999957	val: 0.903216	test: 0.737575

Epoch: 50
Loss: 0.159816287752042
ROC train: 0.999951	val: 0.602198	test: 0.724222
PRC train: 0.999928	val: 0.899317	test: 0.736607

Epoch: 51
Loss: 0.1384542503363432
ROC train: 0.999923	val: 0.589011	test: 0.708746
PRC train: 0.999887	val: 0.899597	test: 0.716328

Epoch: 52
Loss: 0.15286245936000475
ROC train: 0.999951	val: 0.578388	test: 0.723005
PRC train: 0.999927	val: 0.888627	test: 0.732010

Epoch: 53
Loss: 0.13618308364876014
ROC train: 0.999926	val: 0.597802	test: 0.718136
PRC train: 0.999889	val: 0.897831	test: 0.716800

Epoch: 54
Loss: 0.1526964326896751
ROC train: 0.999880	val: 0.610256	test: 0.714137
PRC train: 0.999819	val: 0.903168	test: 0.696652

Epoch: 55
Loss: 0.14159568067562492
ROC train: 1.000000	val: 0.598901	test: 0.720570
PRC train: 1.000000	val: 0.892383	test: 0.710703

Epoch: 56
Loss: 0.12124547574886324
ROC train: 0.999869	val: 0.590842	test: 0.703008
PRC train: 0.999794	val: 0.893219	test: 0.687846

Epoch: 57
Loss: 0.13584124526852495
ROC train: 0.999963	val: 0.593407	test: 0.703704
PRC train: 0.999943	val: 0.900206	test: 0.677061

Epoch: 58
Loss: 0.1368169565758275
ROC train: 0.999994	val: 0.602930	test: 0.688750
PRC train: 0.999991	val: 0.901732	test: 0.674310

Epoch: 59
Loss: 0.1319101417697194
ROC train: 1.000000	val: 0.617949	test: 0.687533
PRC train: 1.000000	val: 0.894798	test: 0.676755

Epoch: 60
Loss: 0.1375455048137228
ROC train: 0.999989	val: 0.641026	test: 0.695183
PRC train: 0.999983	val: 0.906287	test: 0.685813

Epoch: 61
Loss: 0.09534553101958711
ROC train: 0.999920	val: 0.641392	test: 0.705269
PRC train: 0.999878	val: 0.908498	test: 0.707282

Epoch: 62
Loss: 0.09201057318604922
ROC train: 1.000000	val: 0.622344	test: 0.713963
PRC train: 1.000000	val: 0.900216	test: 0.722326

Epoch: 63
Loss: 0.11439115200914123
ROC train: 1.000000	val: 0.586447	test: 0.715180
PRC train: 1.000000	val: 0.889183	test: 0.723107

Epoch: 64
Loss: 0.11873904161891832
ROC train: 1.000000	val: 0.599267	test: 0.696401
PRC train: 1.000000	val: 0.898138	test: 0.702282

Epoch: 65
Loss: 0.08559614649063334
ROC train: 1.000000	val: 0.613919	test: 0.709268
PRC train: 1.000000	val: 0.903325	test: 0.709347

Epoch: 66
Loss: 0.11286448614387663
ROC train: 1.000000	val: 0.620147	test: 0.708572
PRC train: 1.000000	val: 0.910166	test: 0.710013

Epoch: 67
Loss: 0.10354558085436087
ROC train: 1.000000	val: 0.608425	test: 0.701269
PRC train: 1.000000	val: 0.908168	test: 0.710229

Epoch: 68
Loss: 0.09992594852211965
ROC train: 0.999994	val: 0.592674	test: 0.677621
PRC train: 0.999991	val: 0.896590	test: 0.697279

Epoch: 69
Loss: 0.09156835776408148
ROC train: 0.999997	val: 0.602564	test: 0.676926
PRC train: 0.999996	val: 0.895575	test: 0.696247

Epoch: 70
Loss: 0.08481584539528894
ROC train: 1.000000	val: 0.608791	test: 0.693966
PRC train: 1.000000	val: 0.898081	test: 0.701355

Epoch: 71
Loss: 0.10045974711247758
ROC train: 0.999980	val: 0.622711	test: 0.710833
PRC train: 0.999970	val: 0.906705	test: 0.704468

Epoch: 72
Loss: 0.11923100775633284
ROC train: 0.999994	val: 0.615018	test: 0.713615
PRC train: 0.999991	val: 0.901143	test: 0.711758

Epoch: 73
Loss: 0.09462651304037104
ROC train: 0.999997	val: 0.589744	test: 0.714137
PRC train: 0.999996	val: 0.887623	test: 0.716652

Epoch: 74
Loss: 0.08927545105841155
ROC train: 0.999951	val: 0.594139	test: 0.694140
PRC train: 0.999928	val: 0.896618	test: 0.693503

Epoch: 75
Loss: 0.083068334616731
ROC train: 0.999920	val: 0.625641	test: 0.698313
PRC train: 0.999883	val: 0.915094	test: 0.699451

Epoch: 76
Loss: 0.0894298189435195
ROC train: 0.999980	val: 0.653480	test: 0.720223
PRC train: 0.999970	val: 0.922357	test: 0.719107

Epoch: 77
Loss: 0.08288518226238964
ROC train: 1.000000	val: 0.648718	test: 0.716397
PRC train: 1.000000	val: 0.921489	test: 0.708948

Epoch: 78
Loss: 0.07389684763935284
ROC train: 1.000000	val: 0.628571	test: 0.692227
PRC train: 1.000000	val: 0.912150	test: 0.696327

Epoch: 79
Loss: 0.09091641572348264
ROC train: 0.999997	val: 0.615751	test: 0.676926
PRC train: 0.999996	val: 0.906878	test: 0.691321

Epoch: 80
Loss: 0.06714210785281532
ROC train: 0.999997	val: 0.634066	test: 0.680925
PRC train: 0.999996	val: 0.914543	test: 0.694419

Epoch: 81
Loss: 0.08856955732445526
ROC train: 1.000000	val: 0.651648	test: 0.699878
PRC train: 1.000000	val: 0.918948	test: 0.697336

Epoch: 82
Loss: 0.07792696200055635
ROC train: 1.000000	val: 0.628571	test: 0.709963
PRC train: 1.000000	val: 0.903596	test: 0.692691

Epoch: 83
Loss: 0.07217321642177141
ROC train: 1.000000	val: 0.602930	test: 0.691010
PRC train: 1.000000	val: 0.893021	test: 0.684333

Epoch: 84
Loss: 0.06658238975970301
ROC train: 1.000000	val: 0.584982	test: 0.659537
PRC train: 1.000000	val: 0.889799	test: 0.667146

Epoch: 85
Loss: 0.06109543454700768
ROC train: 1.000000	val: 0.611722	test: 0.666145
PRC train: 1.000000	val: 0.906805	test: 0.665469

Epoch: 86
Loss: 0.07509296845791483
ROC train: 1.000000	val: 0.623077	test: 0.679012
PRC train: 1.000000	val: 0.908178	test: 0.685499

Epoch: 87
Loss: 0.06483741897157488
ROC train: 1.000000	val: 0.629304	test: 0.681968
PRC train: 1.000000	val: 0.904777	test: 0.687152

Epoch: 88
Loss: 0.06321078733664029
ROC train: 1.000000	val: 0.626007	test: 0.693097
PRC train: 1.000000	val: 0.902495	test: 0.695036

Epoch: 89
Loss: 0.06536987368700589
ROC train: 1.000000	val: 0.634432	test: 0.688576
PRC train: 1.000000	val: 0.912699	test: 0.684152

Epoch: 90
Loss: 0.05928590845457919
ROC train: 1.000000	val: 0.630769	test: 0.677100
PRC train: 1.000000	val: 0.913989	test: 0.672260

Epoch: 91
Loss: 0.06849342982740723
ROC train: 1.000000	val: 0.610256	test: 0.688228
PRC train: 1.000000	val: 0.904238	test: 0.700228

Epoch: 92
Loss: 0.06045138975473642
ROC train: 0.999997	val: 0.597436	test: 0.697270
PRC train: 0.999996	val: 0.899636	test: 0.712001

Epoch: 93
Loss: 0.046683047456417714
ROC train: 1.000000	val: 0.598535	test: 0.677795
PRC train: 0.976712	val: 0.896051	test: 0.728076

Epoch: 95
Loss: 0.22494148239152595
ROC train: 0.983082	val: 0.633700	test: 0.733264
PRC train: 0.975743	val: 0.902347	test: 0.737955

Epoch: 96
Loss: 0.2323922833898429
ROC train: 0.981367	val: 0.622711	test: 0.736220
PRC train: 0.973075	val: 0.901206	test: 0.743455

Epoch: 97
Loss: 0.21816646860892877
ROC train: 0.981301	val: 0.629304	test: 0.731699
PRC train: 0.973200	val: 0.900880	test: 0.737085

Epoch: 98
Loss: 0.23399376798587807
ROC train: 0.983114	val: 0.634799	test: 0.732568
PRC train: 0.976076	val: 0.911267	test: 0.730715

Epoch: 99
Loss: 0.2248418541744701
ROC train: 0.983402	val: 0.637363	test: 0.733785
PRC train: 0.976551	val: 0.908550	test: 0.733156

Epoch: 100
Loss: 0.22881389483449627
ROC train: 0.981852	val: 0.641758	test: 0.725961
PRC train: 0.974248	val: 0.911632	test: 0.720431

Epoch: 101
Loss: 0.2403223569703557
ROC train: 0.983467	val: 0.631502	test: 0.737611
PRC train: 0.976263	val: 0.913275	test: 0.729544

Epoch: 102
Loss: 0.22106230777224387
ROC train: 0.982985	val: 0.617949	test: 0.734133
PRC train: 0.975444	val: 0.907934	test: 0.731658

Epoch: 103
Loss: 0.22393158653348336
ROC train: 0.982369	val: 0.620513	test: 0.731525
PRC train: 0.974536	val: 0.904772	test: 0.725127

Epoch: 104
Loss: 0.22876124532759126
ROC train: 0.981513	val: 0.647253	test: 0.698835
PRC train: 0.973318	val: 0.909400	test: 0.696551

Epoch: 105
Loss: 0.21579124684958334
ROC train: 0.981735	val: 0.635897	test: 0.680056
PRC train: 0.974493	val: 0.905676	test: 0.678507

Epoch: 106
Loss: 0.22664837747041347
ROC train: 0.985420	val: 0.615751	test: 0.706138
PRC train: 0.979387	val: 0.904953	test: 0.694238

Epoch: 107
Loss: 0.23030646586330494
ROC train: 0.983439	val: 0.604029	test: 0.722309
PRC train: 0.976307	val: 0.901180	test: 0.714981

Epoch: 108
Loss: 0.21563734771841608
ROC train: 0.983973	val: 0.628571	test: 0.721614
PRC train: 0.976784	val: 0.906433	test: 0.714021

Epoch: 109
Loss: 0.21229732920313588
ROC train: 0.985009	val: 0.637729	test: 0.715180
PRC train: 0.978577	val: 0.908857	test: 0.721040

Epoch: 110
Loss: 0.23529930508382493
ROC train: 0.985103	val: 0.645421	test: 0.715528
PRC train: 0.978815	val: 0.915596	test: 0.716304

Epoch: 111
Loss: 0.21922786033336586
ROC train: 0.983487	val: 0.649817	test: 0.716049
PRC train: 0.976520	val: 0.913489	test: 0.693998

Epoch: 112
Loss: 0.21594252740749117
ROC train: 0.986473	val: 0.658608	test: 0.720918
PRC train: 0.980727	val: 0.916723	test: 0.714956

Epoch: 113
Loss: 0.19952872787717696
ROC train: 0.986096	val: 0.645055	test: 0.729960
PRC train: 0.979724	val: 0.913115	test: 0.729471

Epoch: 114
Loss: 0.20938691838090012
ROC train: 0.986635	val: 0.649817	test: 0.715006
PRC train: 0.980657	val: 0.917237	test: 0.714035

Epoch: 115
Loss: 0.21243614320811002
ROC train: 0.986658	val: 0.653480	test: 0.694314
PRC train: 0.981006	val: 0.917438	test: 0.690703

Epoch: 116
Loss: 0.20831276519761577
ROC train: 0.986336	val: 0.673260	test: 0.686142
PRC train: 0.980671	val: 0.923672	test: 0.677312

Epoch: 117
Loss: 0.21288867357466817
ROC train: 0.987637	val: 0.639194	test: 0.717093
PRC train: 0.982213	val: 0.917081	test: 0.706050

Epoch: 118
Loss: 0.20043669738668823
ROC train: 0.984700	val: 0.626740	test: 0.708572
PRC train: 0.977300	val: 0.909801	test: 0.693948

Epoch: 119
Loss: 0.21093108586709297
ROC train: 0.985648	val: 0.656777	test: 0.708572
PRC train: 0.979494	val: 0.916088	test: 0.702691

Epoch: 120
Loss: 0.20728746396519707
ROC train: 0.987768	val: 0.657143	test: 0.711702
PRC train: 0.982918	val: 0.917828	test: 0.707303

Early stopping
Best (ROC):	 train: 0.753188	val: 0.712454	test: 0.585290
Best (PRC):	 train: 0.641972	val: 0.930976	test: 0.581503

PRC train: 0.971378	val: 0.929617	test: 0.698048

Epoch: 95
Loss: 0.22533274632929712
ROC train: 0.981273	val: 0.672161	test: 0.718310
PRC train: 0.971997	val: 0.927938	test: 0.728789

Epoch: 96
Loss: 0.2184035290441134
ROC train: 0.979635	val: 0.686081	test: 0.712050
PRC train: 0.968905	val: 0.927346	test: 0.724973

Epoch: 97
Loss: 0.24395862502585164
ROC train: 0.981978	val: 0.682418	test: 0.712572
PRC train: 0.972958	val: 0.927786	test: 0.724898

Epoch: 98
Loss: 0.23478053101650267
ROC train: 0.982834	val: 0.674725	test: 0.715354
PRC train: 0.973737	val: 0.925499	test: 0.725582

Epoch: 99
Loss: 0.2334960841284784
ROC train: 0.982392	val: 0.677289	test: 0.711355
PRC train: 0.973139	val: 0.928117	test: 0.723952

Epoch: 100
Loss: 0.22670048966248207
ROC train: 0.977471	val: 0.665201	test: 0.711355
PRC train: 0.965320	val: 0.922866	test: 0.718807

Epoch: 101
Loss: 0.21432199244440703
ROC train: 0.980565	val: 0.672161	test: 0.715006
PRC train: 0.970483	val: 0.924710	test: 0.729123

Epoch: 102
Loss: 0.23339363114526446
ROC train: 0.981293	val: 0.687179	test: 0.711181
PRC train: 0.971988	val: 0.931988	test: 0.726534

Epoch: 103
Loss: 0.22955807848307508
ROC train: 0.983111	val: 0.692308	test: 0.709442
PRC train: 0.974545	val: 0.930817	test: 0.717559

Epoch: 104
Loss: 0.2269078504631883
ROC train: 0.982449	val: 0.684249	test: 0.701617
PRC train: 0.973752	val: 0.928785	test: 0.713864

Epoch: 105
Loss: 0.21734968698027055
ROC train: 0.982651	val: 0.682418	test: 0.707529
PRC train: 0.973782	val: 0.927251	test: 0.722964

Epoch: 106
Loss: 0.2153183420543677
ROC train: 0.982226	val: 0.658974	test: 0.706312
PRC train: 0.973160	val: 0.916054	test: 0.717020

Epoch: 107
Loss: 0.22122495659264355
ROC train: 0.980908	val: 0.673260	test: 0.698313
PRC train: 0.970842	val: 0.922899	test: 0.706687

Epoch: 108
Loss: 0.2163449405242975
ROC train: 0.978724	val: 0.679487	test: 0.697966
PRC train: 0.967859	val: 0.925243	test: 0.706551

Epoch: 109
Loss: 0.19864894979152664
ROC train: 0.981584	val: 0.689377	test: 0.696227
PRC train: 0.971875	val: 0.930854	test: 0.702873

Epoch: 110
Loss: 0.22247445389639645
ROC train: 0.984535	val: 0.690476	test: 0.699878
PRC train: 0.976452	val: 0.933286	test: 0.700394

Epoch: 111
Loss: 0.21401126127264228
ROC train: 0.985171	val: 0.679487	test: 0.694488
PRC train: 0.977458	val: 0.930415	test: 0.692831

Epoch: 112
Loss: 0.220953708960961
ROC train: 0.985120	val: 0.696337	test: 0.695183
PRC train: 0.977342	val: 0.932615	test: 0.693604

Epoch: 113
Loss: 0.23248814549001878
ROC train: 0.983764	val: 0.677656	test: 0.693271
PRC train: 0.975310	val: 0.926790	test: 0.699196

Epoch: 114
Loss: 0.211191825040701
ROC train: 0.984424	val: 0.657509	test: 0.707007
PRC train: 0.976536	val: 0.918142	test: 0.715338

Epoch: 115
Loss: 0.23067784070895275
ROC train: 0.986695	val: 0.678388	test: 0.708572
PRC train: 0.980005	val: 0.927539	test: 0.707994

Epoch: 116
Loss: 0.20542233344027547
ROC train: 0.986598	val: 0.694505	test: 0.695531
PRC train: 0.979971	val: 0.933064	test: 0.689203

Epoch: 117
Loss: 0.2073769344906183
ROC train: 0.985451	val: 0.692674	test: 0.683012
PRC train: 0.978130	val: 0.932773	test: 0.676303

Epoch: 118
Loss: 0.20743065395927576
ROC train: 0.985171	val: 0.689011	test: 0.704921
PRC train: 0.977591	val: 0.931666	test: 0.706644

Epoch: 119
Loss: 0.209248714438156
ROC train: 0.986179	val: 0.694505	test: 0.721266
PRC train: 0.978918	val: 0.933857	test: 0.725481

Epoch: 120
Loss: 0.2142136388235444
ROC train: 0.986313	val: 0.679487	test: 0.719179
PRC train: 0.979530	val: 0.928711	test: 0.715003

Early stopping
Best (ROC):	 train: 0.900211	val: 0.715018	test: 0.732742
Best (PRC):	 train: 0.826163	val: 0.946042	test: 0.743850

PRC train: 1.000000	val: 0.842267	test: 0.743854

Epoch: 94
Loss: 0.050045935727211076
ROC train: 1.000000	val: 0.562637	test: 0.756390
PRC train: 1.000000	val: 0.848715	test: 0.766284

Epoch: 95
Loss: 0.06643619588576302
ROC train: 1.000000	val: 0.576557	test: 0.759346
PRC train: 1.000000	val: 0.873613	test: 0.754536

Epoch: 96
Loss: 0.06733225491938168
ROC train: 1.000000	val: 0.565201	test: 0.741610
PRC train: 1.000000	val: 0.870505	test: 0.750008

Epoch: 97
Loss: 0.06527764469503569
ROC train: 1.000000	val: 0.542491	test: 0.744218
PRC train: 1.000000	val: 0.858001	test: 0.743117

Epoch: 98
Loss: 0.06645360355387553
ROC train: 1.000000	val: 0.563370	test: 0.753608
PRC train: 1.000000	val: 0.865182	test: 0.741495

Epoch: 99
Loss: 0.049858071050728946
ROC train: 1.000000	val: 0.579121	test: 0.751521
PRC train: 1.000000	val: 0.872024	test: 0.737915

Epoch: 100
Loss: 0.07299918949165121
ROC train: 1.000000	val: 0.563004	test: 0.736394
PRC train: 1.000000	val: 0.855364	test: 0.741995

Epoch: 101
Loss: 0.059501111469446845
ROC train: 1.000000	val: 0.539194	test: 0.741610
PRC train: 1.000000	val: 0.840398	test: 0.766106

Epoch: 102
Loss: 0.050404583422040586
ROC train: 1.000000	val: 0.543223	test: 0.745262
PRC train: 1.000000	val: 0.841733	test: 0.760850

Epoch: 103
Loss: 0.05579047010808197
ROC train: 1.000000	val: 0.559341	test: 0.733612
PRC train: 1.000000	val: 0.865435	test: 0.735687

Epoch: 104
Loss: 0.055165076335007744
ROC train: 1.000000	val: 0.583150	test: 0.724222
PRC train: 1.000000	val: 0.878192	test: 0.726516

Epoch: 105
Loss: 0.046753314319687767
ROC train: 1.000000	val: 0.586081	test: 0.750130
PRC train: 1.000000	val: 0.886492	test: 0.744465

Epoch: 106
Loss: 0.04843713708487872
ROC train: 1.000000	val: 0.567766	test: 0.758998
PRC train: 1.000000	val: 0.887814	test: 0.734044

Epoch: 107
Loss: 0.0455738982094166
ROC train: 1.000000	val: 0.548718	test: 0.768736
PRC train: 1.000000	val: 0.877637	test: 0.737529

Epoch: 108
Loss: 0.055203575638231486
ROC train: 1.000000	val: 0.516484	test: 0.751869
PRC train: 1.000000	val: 0.844798	test: 0.756427

Epoch: 109
Loss: 0.05160399848734352
ROC train: 1.000000	val: 0.575092	test: 0.733612
PRC train: 1.000000	val: 0.864514	test: 0.754308

Epoch: 110
Loss: 0.05159731456145701
ROC train: 1.000000	val: 0.623443	test: 0.718832
PRC train: 1.000000	val: 0.888203	test: 0.742941

Epoch: 111
Loss: 0.056678243383190254
ROC train: 1.000000	val: 0.616850	test: 0.735003
PRC train: 1.000000	val: 0.895553	test: 0.743218

Epoch: 112
Loss: 0.05097300154438209
ROC train: 1.000000	val: 0.572161	test: 0.767693
PRC train: 1.000000	val: 0.877734	test: 0.755407

Epoch: 113
Loss: 0.04092321928861739
ROC train: 1.000000	val: 0.543956	test: 0.762476
PRC train: 1.000000	val: 0.848846	test: 0.776314

Epoch: 114
Loss: 0.03601110541045122
ROC train: 1.000000	val: 0.516484	test: 0.744045
PRC train: 1.000000	val: 0.838093	test: 0.777451

Epoch: 115
Loss: 0.03626617411485702
ROC train: 1.000000	val: 0.507692	test: 0.764041
PRC train: 1.000000	val: 0.837440	test: 0.783430

Epoch: 116
Loss: 0.030489839113178818
ROC train: 1.000000	val: 0.513553	test: 0.779690
PRC train: 1.000000	val: 0.839980	test: 0.782367

Epoch: 117
Loss: 0.04522776868273255
ROC train: 1.000000	val: 0.543590	test: 0.780212
PRC train: 1.000000	val: 0.861412	test: 0.759397

Epoch: 118
Loss: 0.03697306329601985
ROC train: 1.000000	val: 0.555311	test: 0.773605
PRC train: 1.000000	val: 0.871479	test: 0.745519

Epoch: 119
Loss: 0.04591947150572605
ROC train: 1.000000	val: 0.556410	test: 0.775343
PRC train: 1.000000	val: 0.865671	test: 0.745220

Epoch: 120
Loss: 0.041618079115900215
ROC train: 1.000000	val: 0.560806	test: 0.776561
PRC train: 1.000000	val: 0.865074	test: 0.748100

Early stopping
Best (ROC):	 train: 0.765888	val: 0.701832	test: 0.704573
Best (PRC):	 train: 0.671371	val: 0.937997	test: 0.742278

PRC train: 0.999876	val: 0.911299	test: 0.732474

Epoch: 94
Loss: 0.07689670946478597
ROC train: 1.000000	val: 0.636630	test: 0.727178
PRC train: 1.000000	val: 0.912305	test: 0.728134

Epoch: 95
Loss: 0.08498448064588517
ROC train: 0.999991	val: 0.645788	test: 0.754130
PRC train: 0.999987	val: 0.914189	test: 0.742360

Epoch: 96
Loss: 0.08744392149660855
ROC train: 1.000000	val: 0.648718	test: 0.758651
PRC train: 1.000000	val: 0.913362	test: 0.756124

Epoch: 97
Loss: 0.10347541295644638
ROC train: 0.999991	val: 0.672894	test: 0.762128
PRC train: 0.999987	val: 0.923370	test: 0.756362

Epoch: 98
Loss: 0.07518894394216039
ROC train: 0.999974	val: 0.667399	test: 0.770301
PRC train: 0.999961	val: 0.921702	test: 0.764755

Epoch: 99
Loss: 0.0854707489122742
ROC train: 0.999957	val: 0.663736	test: 0.754651
PRC train: 0.999934	val: 0.917093	test: 0.748679

Epoch: 100
Loss: 0.07393626871188548
ROC train: 1.000000	val: 0.644689	test: 0.743349
PRC train: 1.000000	val: 0.911538	test: 0.738350

Epoch: 101
Loss: 0.08111072007880418
ROC train: 1.000000	val: 0.646154	test: 0.737785
PRC train: 1.000000	val: 0.914017	test: 0.737095

Epoch: 102
Loss: 0.06885064902783151
ROC train: 1.000000	val: 0.626374	test: 0.729786
PRC train: 1.000000	val: 0.911285	test: 0.735340

Epoch: 103
Loss: 0.06041638431852862
ROC train: 1.000000	val: 0.633700	test: 0.733959
PRC train: 1.000000	val: 0.913082	test: 0.742118

Epoch: 104
Loss: 0.07249456223309374
ROC train: 0.999994	val: 0.635897	test: 0.736568
PRC train: 0.999991	val: 0.911735	test: 0.731241

Epoch: 105
Loss: 0.08142648153985521
ROC train: 0.999986	val: 0.629304	test: 0.741784
PRC train: 0.999978	val: 0.908216	test: 0.739021

Epoch: 106
Loss: 0.08277062196622073
ROC train: 1.000000	val: 0.638462	test: 0.755347
PRC train: 1.000000	val: 0.912270	test: 0.758749

Epoch: 107
Loss: 0.0686276910942439
ROC train: 1.000000	val: 0.647619	test: 0.727004
PRC train: 1.000000	val: 0.914777	test: 0.733681

Epoch: 108
Loss: 0.06066541538754124
ROC train: 1.000000	val: 0.652747	test: 0.728569
PRC train: 1.000000	val: 0.914769	test: 0.734443

Epoch: 109
Loss: 0.06734732193843716
ROC train: 1.000000	val: 0.639194	test: 0.749783
PRC train: 1.000000	val: 0.912551	test: 0.751451

Epoch: 110
Loss: 0.08437167258254918
ROC train: 1.000000	val: 0.641392	test: 0.747001
PRC train: 1.000000	val: 0.913505	test: 0.751772

Epoch: 111
Loss: 0.07568159249378789
ROC train: 0.999997	val: 0.636630	test: 0.735350
PRC train: 0.999996	val: 0.914540	test: 0.734114

Epoch: 112
Loss: 0.06869068829830653
ROC train: 0.999983	val: 0.621978	test: 0.729786
PRC train: 0.999974	val: 0.909900	test: 0.731839

Epoch: 113
Loss: 0.08482784043773885
ROC train: 0.999997	val: 0.630037	test: 0.739697
PRC train: 0.999996	val: 0.913084	test: 0.733951

Epoch: 114
Loss: 0.07505964830626559
ROC train: 1.000000	val: 0.660073	test: 0.760563
PRC train: 1.000000	val: 0.916724	test: 0.750974

Epoch: 115
Loss: 0.07243223539074228
ROC train: 0.999994	val: 0.668498	test: 0.761607
PRC train: 0.999991	val: 0.918132	test: 0.751666

Epoch: 116
Loss: 0.06439548773817341
ROC train: 1.000000	val: 0.639194	test: 0.764041
PRC train: 1.000000	val: 0.911502	test: 0.760304

Epoch: 117
Loss: 0.062444555541634104
ROC train: 1.000000	val: 0.626740	test: 0.757260
PRC train: 1.000000	val: 0.908220	test: 0.751027

Epoch: 118
Loss: 0.06155415051552896
ROC train: 0.999929	val: 0.647619	test: 0.740915
PRC train: 0.999893	val: 0.913514	test: 0.733061

Epoch: 119
Loss: 0.05223417477172414
ROC train: 0.999977	val: 0.650183	test: 0.733612
PRC train: 0.999965	val: 0.914422	test: 0.717924

Epoch: 120
Loss: 0.0724734576051059
ROC train: 1.000000	val: 0.637363	test: 0.743175
PRC train: 1.000000	val: 0.913130	test: 0.739819

Early stopping
Best (ROC):	 train: 0.760371	val: 0.676923	test: 0.656408
Best (PRC):	 train: 0.660705	val: 0.928340	test: 0.667687

PRC train: 0.999996	val: 0.927970	test: 0.760088

Epoch: 94
Loss: 0.07307147417431914
ROC train: 0.999989	val: 0.626740	test: 0.783864
PRC train: 0.999983	val: 0.929150	test: 0.770757

Epoch: 95
Loss: 0.08684659039742361
ROC train: 0.999977	val: 0.661172	test: 0.770649
PRC train: 0.999966	val: 0.935737	test: 0.761067

Epoch: 96
Loss: 0.07362446412226084
ROC train: 1.000000	val: 0.683516	test: 0.761781
PRC train: 1.000000	val: 0.941895	test: 0.745692

Epoch: 97
Loss: 0.08203847476510492
ROC train: 0.999997	val: 0.659341	test: 0.762650
PRC train: 0.999996	val: 0.935715	test: 0.756057

Epoch: 98
Loss: 0.051676597352708964
ROC train: 0.999994	val: 0.644689	test: 0.771344
PRC train: 0.999991	val: 0.931367	test: 0.759752

Epoch: 99
Loss: 0.07236435792618072
ROC train: 1.000000	val: 0.652381	test: 0.757260
PRC train: 1.000000	val: 0.934956	test: 0.749296

Epoch: 100
Loss: 0.0703860832984475
ROC train: 1.000000	val: 0.658974	test: 0.739350
PRC train: 1.000000	val: 0.936732	test: 0.733868

Epoch: 101
Loss: 0.09682232810753688
ROC train: 1.000000	val: 0.639927	test: 0.775517
PRC train: 1.000000	val: 0.930769	test: 0.760464

Epoch: 102
Loss: 0.08397333474963431
ROC train: 0.999997	val: 0.621612	test: 0.798818
PRC train: 0.999996	val: 0.924714	test: 0.761388

Epoch: 103
Loss: 0.07920095914327603
ROC train: 1.000000	val: 0.623443	test: 0.782820
PRC train: 1.000000	val: 0.923193	test: 0.741734

Epoch: 104
Loss: 0.07251922261719833
ROC train: 0.999997	val: 0.658974	test: 0.766128
PRC train: 0.999996	val: 0.932898	test: 0.736492

Epoch: 105
Loss: 0.06921378889682964
ROC train: 0.999994	val: 0.678022	test: 0.768388
PRC train: 0.999991	val: 0.939134	test: 0.755193

Epoch: 106
Loss: 0.059510466410865194
ROC train: 1.000000	val: 0.672894	test: 0.783864
PRC train: 1.000000	val: 0.939767	test: 0.777403

Epoch: 107
Loss: 0.06716108246305233
ROC train: 0.999923	val: 0.649084	test: 0.786646
PRC train: 0.999882	val: 0.932990	test: 0.780015

Epoch: 108
Loss: 0.06721779419568194
ROC train: 1.000000	val: 0.658608	test: 0.792384
PRC train: 1.000000	val: 0.935865	test: 0.782585

Epoch: 109
Loss: 0.049363126551108104
ROC train: 0.999997	val: 0.669597	test: 0.789602
PRC train: 0.999996	val: 0.938646	test: 0.773934

Epoch: 110
Loss: 0.07888952416432127
ROC train: 1.000000	val: 0.662271	test: 0.771692
PRC train: 1.000000	val: 0.937445	test: 0.754127

Epoch: 111
Loss: 0.052430625086563466
ROC train: 1.000000	val: 0.678022	test: 0.756564
PRC train: 1.000000	val: 0.941344	test: 0.744083

Epoch: 112
Loss: 0.05846773236488843
ROC train: 1.000000	val: 0.679853	test: 0.744914
PRC train: 1.000000	val: 0.941294	test: 0.743539

Epoch: 113
Loss: 0.06520748357081704
ROC train: 0.999994	val: 0.678755	test: 0.730134
PRC train: 0.999991	val: 0.940798	test: 0.730338

Epoch: 114
Loss: 0.05959718791051607
ROC train: 1.000000	val: 0.663736	test: 0.742827
PRC train: 1.000000	val: 0.937544	test: 0.744641

Epoch: 115
Loss: 0.04796363120557222
ROC train: 1.000000	val: 0.650916	test: 0.772214
PRC train: 1.000000	val: 0.934148	test: 0.762793

Epoch: 116
Loss: 0.06540494760987889
ROC train: 1.000000	val: 0.658974	test: 0.782125
PRC train: 1.000000	val: 0.935795	test: 0.767413

Epoch: 117
Loss: 0.06465294113898779
ROC train: 1.000000	val: 0.673993	test: 0.786472
PRC train: 1.000000	val: 0.938395	test: 0.767139

Epoch: 118
Loss: 0.05263340707062573
ROC train: 1.000000	val: 0.680220	test: 0.788037
PRC train: 1.000000	val: 0.939354	test: 0.764665

Epoch: 119
Loss: 0.03322180440202131
ROC train: 1.000000	val: 0.670696	test: 0.802991
PRC train: 1.000000	val: 0.937281	test: 0.779657

Epoch: 120
Loss: 0.0372668058938367
ROC train: 1.000000	val: 0.654579	test: 0.806816
PRC train: 1.000000	val: 0.932746	test: 0.783435

Early stopping
Best (ROC):	 train: 0.999678	val: 0.716850	test: 0.744914
Best (PRC):	 train: 0.999504	val: 0.948996	test: 0.715355

PRC train: 1.000000	val: 0.924405	test: 0.700259

Epoch: 94
Loss: 0.05064995610241417
ROC train: 1.000000	val: 0.649451	test: 0.689619
PRC train: 1.000000	val: 0.926780	test: 0.679773

Epoch: 95
Loss: 0.06427734554631531
ROC train: 1.000000	val: 0.621612	test: 0.716397
PRC train: 1.000000	val: 0.917593	test: 0.714555

Epoch: 96
Loss: 0.05359037138996751
ROC train: 1.000000	val: 0.610989	test: 0.729091
PRC train: 1.000000	val: 0.913647	test: 0.716724

Epoch: 97
Loss: 0.05649994694273454
ROC train: 0.999994	val: 0.612454	test: 0.720570
PRC train: 0.999991	val: 0.910428	test: 0.709445

Epoch: 98
Loss: 0.06288596555233783
ROC train: 1.000000	val: 0.606960	test: 0.725787
PRC train: 1.000000	val: 0.910798	test: 0.694208

Epoch: 99
Loss: 0.05968713811228747
ROC train: 1.000000	val: 0.622344	test: 0.724222
PRC train: 1.000000	val: 0.911416	test: 0.676718

Epoch: 100
Loss: 0.05486473568342787
ROC train: 1.000000	val: 0.636630	test: 0.731003
PRC train: 1.000000	val: 0.914422	test: 0.692736

Epoch: 101
Loss: 0.059313062505185
ROC train: 1.000000	val: 0.653846	test: 0.733525
PRC train: 1.000000	val: 0.919456	test: 0.683718

Epoch: 102
Loss: 0.03401449693439817
ROC train: 1.000000	val: 0.666300	test: 0.732568
PRC train: 1.000000	val: 0.925804	test: 0.671629

Epoch: 103
Loss: 0.06528304552546545
ROC train: 1.000000	val: 0.664469	test: 0.732742
PRC train: 1.000000	val: 0.924940	test: 0.673191

Epoch: 104
Loss: 0.04554174447236555
ROC train: 1.000000	val: 0.649451	test: 0.724744
PRC train: 1.000000	val: 0.920255	test: 0.673184

Epoch: 105
Loss: 0.04806063369084844
ROC train: 1.000000	val: 0.645055	test: 0.725265
PRC train: 1.000000	val: 0.920981	test: 0.677339

Epoch: 106
Loss: 0.047587470573771065
ROC train: 1.000000	val: 0.638095	test: 0.723352
PRC train: 1.000000	val: 0.921252	test: 0.688427

Epoch: 107
Loss: 0.05312564223682359
ROC train: 1.000000	val: 0.641392	test: 0.727526
PRC train: 1.000000	val: 0.919857	test: 0.685395

Epoch: 108
Loss: 0.037366708862012606
ROC train: 1.000000	val: 0.651648	test: 0.732394
PRC train: 1.000000	val: 0.920347	test: 0.688013

Epoch: 109
Loss: 0.029121109547278045
ROC train: 1.000000	val: 0.656410	test: 0.729612
PRC train: 1.000000	val: 0.920368	test: 0.687777

Epoch: 110
Loss: 0.03870273872685466
ROC train: 1.000000	val: 0.649817	test: 0.719875
PRC train: 1.000000	val: 0.918513	test: 0.685707

Epoch: 111
Loss: 0.03922089542749584
ROC train: 1.000000	val: 0.646520	test: 0.711181
PRC train: 1.000000	val: 0.915635	test: 0.685810

Epoch: 112
Loss: 0.05536318777272947
ROC train: 1.000000	val: 0.665568	test: 0.723179
PRC train: 1.000000	val: 0.921114	test: 0.683194

Epoch: 113
Loss: 0.04583122521530297
ROC train: 1.000000	val: 0.652747	test: 0.721092
PRC train: 1.000000	val: 0.918240	test: 0.677512

Epoch: 114
Loss: 0.06287467927433185
ROC train: 1.000000	val: 0.643590	test: 0.703704
PRC train: 1.000000	val: 0.912108	test: 0.658384

Epoch: 115
Loss: 0.04908363897187372
ROC train: 1.000000	val: 0.627473	test: 0.701095
PRC train: 1.000000	val: 0.905449	test: 0.656160

Epoch: 116
Loss: 0.05194423610836228
ROC train: 1.000000	val: 0.655311	test: 0.733438
PRC train: 1.000000	val: 0.917443	test: 0.671209

Epoch: 117
Loss: 0.03141918571776527
ROC train: 1.000000	val: 0.655311	test: 0.736220
PRC train: 1.000000	val: 0.917569	test: 0.664918

Epoch: 118
Loss: 0.03723404764396848
ROC train: 1.000000	val: 0.668498	test: 0.742132
PRC train: 1.000000	val: 0.923773	test: 0.676510

Epoch: 119
Loss: 0.039536916362240465
ROC train: 1.000000	val: 0.669231	test: 0.735524
PRC train: 1.000000	val: 0.922600	test: 0.684452

Epoch: 120
Loss: 0.04411988861604475
ROC train: 1.000000	val: 0.658242	test: 0.707181
PRC train: 1.000000	val: 0.916323	test: 0.669033

Early stopping
Best (ROC):	 train: 1.000000	val: 0.687179	test: 0.752391
Best (PRC):	 train: 1.000000	val: 0.931668	test: 0.699531

PRC train: 1.000000	val: 0.924466	test: 0.748107

Epoch: 94
Loss: 0.05738552473137779
ROC train: 1.000000	val: 0.610623	test: 0.761781
PRC train: 1.000000	val: 0.921511	test: 0.751488

Epoch: 95
Loss: 0.055643326806359104
ROC train: 1.000000	val: 0.589377	test: 0.762650
PRC train: 1.000000	val: 0.913362	test: 0.754774

Epoch: 96
Loss: 0.04532370019864381
ROC train: 1.000000	val: 0.579487	test: 0.768388
PRC train: 1.000000	val: 0.911150	test: 0.763097

Epoch: 97
Loss: 0.03986339857973732
ROC train: 1.000000	val: 0.613919	test: 0.763693
PRC train: 1.000000	val: 0.923803	test: 0.750837

Epoch: 98
Loss: 0.051351137158519335
ROC train: 1.000000	val: 0.617582	test: 0.745088
PRC train: 1.000000	val: 0.925899	test: 0.709898

Epoch: 99
Loss: 0.05434821624224441
ROC train: 1.000000	val: 0.605495	test: 0.742132
PRC train: 1.000000	val: 0.921644	test: 0.725917

Epoch: 100
Loss: 0.04910828646972039
ROC train: 1.000000	val: 0.582418	test: 0.744218
PRC train: 1.000000	val: 0.911199	test: 0.740189

Epoch: 101
Loss: 0.05805025702489065
ROC train: 1.000000	val: 0.579853	test: 0.747696
PRC train: 1.000000	val: 0.911451	test: 0.736080

Epoch: 102
Loss: 0.05366612534369906
ROC train: 0.999983	val: 0.608791	test: 0.745088
PRC train: 0.999974	val: 0.920941	test: 0.710512

Epoch: 103
Loss: 0.05113107471669729
ROC train: 1.000000	val: 0.597436	test: 0.751000
PRC train: 1.000000	val: 0.917203	test: 0.735037

Epoch: 104
Loss: 0.047310236965129
ROC train: 1.000000	val: 0.565934	test: 0.753782
PRC train: 1.000000	val: 0.905797	test: 0.749264

Epoch: 105
Loss: 0.040080604908903306
ROC train: 1.000000	val: 0.579487	test: 0.769605
PRC train: 1.000000	val: 0.910804	test: 0.764138

Epoch: 106
Loss: 0.05668378132966637
ROC train: 1.000000	val: 0.583883	test: 0.781777
PRC train: 1.000000	val: 0.912417	test: 0.769160

Epoch: 107
Loss: 0.045988743402649246
ROC train: 1.000000	val: 0.574359	test: 0.774822
PRC train: 1.000000	val: 0.908794	test: 0.759241

Epoch: 108
Loss: 0.03461800823753598
ROC train: 1.000000	val: 0.586081	test: 0.761954
PRC train: 1.000000	val: 0.912319	test: 0.745958

Epoch: 109
Loss: 0.04727972368393002
ROC train: 0.999997	val: 0.613919	test: 0.764041
PRC train: 0.999996	val: 0.921043	test: 0.736540

Epoch: 110
Loss: 0.040187982617708985
ROC train: 1.000000	val: 0.590110	test: 0.758651
PRC train: 1.000000	val: 0.915434	test: 0.720017

Epoch: 111
Loss: 0.052496489720842955
ROC train: 1.000000	val: 0.595971	test: 0.771344
PRC train: 1.000000	val: 0.917118	test: 0.761355

Epoch: 112
Loss: 0.027775514075768126
ROC train: 1.000000	val: 0.599267	test: 0.764563
PRC train: 1.000000	val: 0.917335	test: 0.763456

Epoch: 113
Loss: 0.05554262706397003
ROC train: 1.000000	val: 0.646886	test: 0.779690
PRC train: 1.000000	val: 0.931214	test: 0.770420

Epoch: 114
Loss: 0.044834928186657054
ROC train: 1.000000	val: 0.654945	test: 0.775517
PRC train: 1.000000	val: 0.932049	test: 0.760156

Epoch: 115
Loss: 0.06095454629150776
ROC train: 1.000000	val: 0.621978	test: 0.773952
PRC train: 1.000000	val: 0.924346	test: 0.766598

Epoch: 116
Loss: 0.042804455859877924
ROC train: 0.999994	val: 0.575824	test: 0.770649
PRC train: 0.999991	val: 0.911040	test: 0.766381

Epoch: 117
Loss: 0.06470555986806978
ROC train: 0.999997	val: 0.552381	test: 0.769431
PRC train: 0.999996	val: 0.902713	test: 0.765485

Epoch: 118
Loss: 0.03610305389546816
ROC train: 1.000000	val: 0.584982	test: 0.770127
PRC train: 1.000000	val: 0.914162	test: 0.755895

Epoch: 119
Loss: 0.05001301134909962
ROC train: 1.000000	val: 0.582418	test: 0.770996
PRC train: 1.000000	val: 0.912201	test: 0.749334

Epoch: 120
Loss: 0.036517465037800845
ROC train: 1.000000	val: 0.547619	test: 0.775691
PRC train: 1.000000	val: 0.901090	test: 0.747541

Early stopping
Best (ROC):	 train: 0.984852	val: 0.678388	test: 0.767693
Best (PRC):	 train: 0.978328	val: 0.937343	test: 0.746428

PRC train: 1.000000	val: 0.901013	test: 0.694665

Epoch: 94
Loss: 0.07004065836560701
ROC train: 1.000000	val: 0.615751	test: 0.684750
PRC train: 1.000000	val: 0.906013	test: 0.700101

Epoch: 95
Loss: 0.04724572144448491
ROC train: 1.000000	val: 0.627839	test: 0.704399
PRC train: 1.000000	val: 0.901081	test: 0.702401

Epoch: 96
Loss: 0.05439464779773577
ROC train: 1.000000	val: 0.622344	test: 0.705790
PRC train: 1.000000	val: 0.898351	test: 0.701335

Epoch: 97
Loss: 0.07651623157654755
ROC train: 1.000000	val: 0.607692	test: 0.704747
PRC train: 1.000000	val: 0.891124	test: 0.700728

Epoch: 98
Loss: 0.06642254798879928
ROC train: 0.999997	val: 0.628205	test: 0.685620
PRC train: 0.999996	val: 0.898195	test: 0.687591

Epoch: 99
Loss: 0.0715726428133473
ROC train: 1.000000	val: 0.649451	test: 0.691880
PRC train: 1.000000	val: 0.910777	test: 0.691355

Epoch: 100
Loss: 0.060282942909036207
ROC train: 1.000000	val: 0.638462	test: 0.681968
PRC train: 1.000000	val: 0.907376	test: 0.690982

Epoch: 101
Loss: 0.06273659855936249
ROC train: 1.000000	val: 0.613919	test: 0.660755
PRC train: 1.000000	val: 0.900251	test: 0.681549

Epoch: 102
Loss: 0.05768297025799533
ROC train: 1.000000	val: 0.609890	test: 0.675882
PRC train: 1.000000	val: 0.901657	test: 0.681502

Epoch: 103
Loss: 0.05758696591300984
ROC train: 1.000000	val: 0.600000	test: 0.694836
PRC train: 1.000000	val: 0.897127	test: 0.693318

Epoch: 104
Loss: 0.0439669370416198
ROC train: 1.000000	val: 0.593040	test: 0.689967
PRC train: 1.000000	val: 0.890020	test: 0.691384

Epoch: 105
Loss: 0.06735280763023818
ROC train: 1.000000	val: 0.581319	test: 0.682142
PRC train: 1.000000	val: 0.886067	test: 0.694035

Epoch: 106
Loss: 0.06403736103719196
ROC train: 1.000000	val: 0.610623	test: 0.673448
PRC train: 1.000000	val: 0.902144	test: 0.683562

Epoch: 107
Loss: 0.07364886197438758
ROC train: 1.000000	val: 0.639194	test: 0.697444
PRC train: 1.000000	val: 0.916055	test: 0.696059

Epoch: 108
Loss: 0.046886142578804926
ROC train: 1.000000	val: 0.634799	test: 0.715702
PRC train: 1.000000	val: 0.913752	test: 0.709565

Epoch: 109
Loss: 0.04854424035809966
ROC train: 1.000000	val: 0.613919	test: 0.719179
PRC train: 1.000000	val: 0.900865	test: 0.718148

Epoch: 110
Loss: 0.05903323545106949
ROC train: 1.000000	val: 0.609158	test: 0.708572
PRC train: 1.000000	val: 0.900016	test: 0.705846

Epoch: 111
Loss: 0.05600904598198207
ROC train: 1.000000	val: 0.622711	test: 0.711355
PRC train: 1.000000	val: 0.905561	test: 0.706815

Epoch: 112
Loss: 0.034816330898734864
ROC train: 1.000000	val: 0.631136	test: 0.709442
PRC train: 1.000000	val: 0.910539	test: 0.707442

Epoch: 113
Loss: 0.05429598863704503
ROC train: 1.000000	val: 0.632234	test: 0.704921
PRC train: 1.000000	val: 0.906664	test: 0.706774

Epoch: 114
Loss: 0.041584534254422365
ROC train: 1.000000	val: 0.632234	test: 0.701965
PRC train: 1.000000	val: 0.902080	test: 0.705072

Epoch: 115
Loss: 0.03770829151716544
ROC train: 1.000000	val: 0.631136	test: 0.686142
PRC train: 1.000000	val: 0.900871	test: 0.688532

Epoch: 116
Loss: 0.04795096466988217
ROC train: 1.000000	val: 0.628571	test: 0.674665
PRC train: 1.000000	val: 0.902021	test: 0.673523

Epoch: 117
Loss: 0.053039647854577696
ROC train: 1.000000	val: 0.620513	test: 0.699878
PRC train: 1.000000	val: 0.896441	test: 0.703051

Epoch: 118
Loss: 0.0313784711641582
ROC train: 1.000000	val: 0.617582	test: 0.722309
PRC train: 1.000000	val: 0.898660	test: 0.720048

Epoch: 119
Loss: 0.041781398375044916
ROC train: 1.000000	val: 0.601832	test: 0.703530
PRC train: 1.000000	val: 0.893555	test: 0.716079

Epoch: 120
Loss: 0.0358832837085805
ROC train: 1.000000	val: 0.603663	test: 0.678665
PRC train: 1.000000	val: 0.899488	test: 0.686880

Early stopping
Best (ROC):	 train: 0.776564	val: 0.663736	test: 0.692575
Best (PRC):	 train: 0.695503	val: 0.930350	test: 0.701615

PRC train: 0.972411	val: 0.932629	test: 0.706514

Epoch: 95
Loss: 0.2417624540877473
ROC train: 0.982902	val: 0.665568	test: 0.713963
PRC train: 0.974553	val: 0.928304	test: 0.694843

Epoch: 96
Loss: 0.2246531913687959
ROC train: 0.985420	val: 0.661905	test: 0.701269
PRC train: 0.978675	val: 0.926526	test: 0.672389

Epoch: 97
Loss: 0.20839262969288747
ROC train: 0.986039	val: 0.679853	test: 0.712224
PRC train: 0.979244	val: 0.928938	test: 0.685735

Epoch: 98
Loss: 0.21110928540680002
ROC train: 0.985362	val: 0.668498	test: 0.715702
PRC train: 0.977933	val: 0.927315	test: 0.701112

Epoch: 99
Loss: 0.2004730366128788
ROC train: 0.985639	val: 0.673993	test: 0.715180
PRC train: 0.978970	val: 0.931576	test: 0.695892

Epoch: 100
Loss: 0.2191722053397996
ROC train: 0.985511	val: 0.690110	test: 0.716919
PRC train: 0.978471	val: 0.935514	test: 0.705196

Epoch: 101
Loss: 0.22627649389985138
ROC train: 0.985260	val: 0.690842	test: 0.720223
PRC train: 0.978170	val: 0.934423	test: 0.717237

Epoch: 102
Loss: 0.22083566714244696
ROC train: 0.986824	val: 0.682784	test: 0.705790
PRC train: 0.981036	val: 0.936071	test: 0.695698

Epoch: 103
Loss: 0.19473392291953853
ROC train: 0.986084	val: 0.707326	test: 0.693271
PRC train: 0.979766	val: 0.940606	test: 0.678729

Epoch: 104
Loss: 0.2118046069391161
ROC train: 0.985950	val: 0.715385	test: 0.704747
PRC train: 0.979285	val: 0.944528	test: 0.687888

Epoch: 105
Loss: 0.23278759166265792
ROC train: 0.987568	val: 0.688645	test: 0.715006
PRC train: 0.981658	val: 0.937129	test: 0.698059

Epoch: 106
Loss: 0.20249502881525094
ROC train: 0.987041	val: 0.698168	test: 0.700400
PRC train: 0.981082	val: 0.934212	test: 0.683838

Epoch: 107
Loss: 0.23771857440390437
ROC train: 0.987380	val: 0.704396	test: 0.711181
PRC train: 0.981886	val: 0.936489	test: 0.692563

Epoch: 108
Loss: 0.22129425170314634
ROC train: 0.985668	val: 0.684249	test: 0.729612
PRC train: 0.979114	val: 0.928468	test: 0.709558

Epoch: 109
Loss: 0.2078482927886065
ROC train: 0.985374	val: 0.708791	test: 0.730482
PRC train: 0.978710	val: 0.934163	test: 0.722873

Epoch: 110
Loss: 0.23025719027598454
ROC train: 0.984298	val: 0.702198	test: 0.726308
PRC train: 0.976857	val: 0.934133	test: 0.723979

Epoch: 111
Loss: 0.22393614682359625
ROC train: 0.986139	val: 0.697436	test: 0.716397
PRC train: 0.980356	val: 0.934373	test: 0.710102

Epoch: 112
Loss: 0.2132859479746699
ROC train: 0.985482	val: 0.709158	test: 0.719179
PRC train: 0.978767	val: 0.934315	test: 0.710303

Epoch: 113
Loss: 0.21171064042128443
ROC train: 0.986404	val: 0.713919	test: 0.719005
PRC train: 0.980018	val: 0.939873	test: 0.705485

Epoch: 114
Loss: 0.21237175508360587
ROC train: 0.988408	val: 0.711722	test: 0.719179
PRC train: 0.983379	val: 0.940583	test: 0.707835

Epoch: 115
Loss: 0.19723841018572957
ROC train: 0.988950	val: 0.695604	test: 0.724222
PRC train: 0.983881	val: 0.938314	test: 0.714554

Epoch: 116
Loss: 0.1975861608839129
ROC train: 0.988205	val: 0.708791	test: 0.720396
PRC train: 0.982602	val: 0.939816	test: 0.706941

Epoch: 117
Loss: 0.2094069953342515
ROC train: 0.988370	val: 0.694139	test: 0.713963
PRC train: 0.982988	val: 0.934263	test: 0.700069

Epoch: 118
Loss: 0.21554933266603826
ROC train: 0.987694	val: 0.701099	test: 0.697966
PRC train: 0.981858	val: 0.934201	test: 0.685651

Epoch: 119
Loss: 0.19474483811223053
ROC train: 0.988268	val: 0.698535	test: 0.719005
PRC train: 0.981773	val: 0.933622	test: 0.707813

Epoch: 120
Loss: 0.20804568915545438
ROC train: 0.990180	val: 0.682418	test: 0.717440
PRC train: 0.986045	val: 0.927646	test: 0.681200

Epoch: 121
Loss: 0.20039910610770204
ROC train: 0.988396	val: 0.678022	test: 0.719875
PRC train: 0.983379	val: 0.917548	test: 0.665041

Epoch: 122
Loss: 0.21809600666854373
ROC train: 0.986898	val: 0.691209	test: 0.720570
PRC train: 0.980404	val: 0.932001	test: 0.691169

Epoch: 123
Loss: 0.21042854562533692
ROC train: 0.987791	val: 0.675458	test: 0.723179
PRC train: 0.981833	val: 0.930337	test: 0.706158

Epoch: 124
Loss: 0.20570990062924993
ROC train: 0.989603	val: 0.675458	test: 0.716397
PRC train: 0.984488	val: 0.924710	test: 0.690070

Epoch: 125
Loss: 0.20765348729358585
ROC train: 0.989172	val: 0.672527	test: 0.714832
PRC train: 0.984129	val: 0.922553	test: 0.676486

Epoch: 126
Loss: 0.18495321959706992
ROC train: 0.989081	val: 0.667766	test: 0.712746
PRC train: 0.984415	val: 0.925083	test: 0.678448

Epoch: 127
Loss: 0.1851069730091511
ROC train: 0.989906	val: 0.698168	test: 0.714137
PRC train: 0.985328	val: 0.932388	test: 0.690720

Epoch: 128
Loss: 0.19792121540146007
ROC train: 0.989526	val: 0.701099	test: 0.702834
PRC train: 0.984240	val: 0.934168	test: 0.682528

Epoch: 129
Loss: 0.18368143396884265
ROC train: 0.990508	val: 0.683150	test: 0.709268
PRC train: 0.986402	val: 0.930696	test: 0.684311

Epoch: 130
Loss: 0.20068359892692386
ROC train: 0.988947	val: 0.679121	test: 0.709616
PRC train: 0.984390	val: 0.929104	test: 0.695163

Epoch: 131
Loss: 0.19113485466170418
ROC train: 0.989752	val: 0.687179	test: 0.710311
PRC train: 0.985725	val: 0.932764	test: 0.693477

Epoch: 132
Loss: 0.17555758590094625
ROC train: 0.991213	val: 0.682784	test: 0.716397
PRC train: 0.987562	val: 0.933339	test: 0.697724

Epoch: 133
Loss: 0.18818257998775817
ROC train: 0.990548	val: 0.675824	test: 0.710833
PRC train: 0.987144	val: 0.932641	test: 0.691605

Epoch: 134
Loss: 0.2028329497088371
ROC train: 0.990568	val: 0.700733	test: 0.712572
PRC train: 0.986795	val: 0.935099	test: 0.695867

Epoch: 135
Loss: 0.19061727810667203
ROC train: 0.989064	val: 0.679853	test: 0.707703
PRC train: 0.984615	val: 0.927083	test: 0.685752

Epoch: 136
Loss: 0.19113950878454694
ROC train: 0.991073	val: 0.690110	test: 0.702313
PRC train: 0.987305	val: 0.929087	test: 0.675672

Epoch: 137
Loss: 0.18422150764504658
ROC train: 0.992129	val: 0.681319	test: 0.713093
PRC train: 0.989067	val: 0.926234	test: 0.671577

Epoch: 138
Loss: 0.18089775536666614
ROC train: 0.992075	val: 0.688645	test: 0.712572
PRC train: 0.988975	val: 0.932015	test: 0.669682

Epoch: 139
Loss: 0.17740368558870956
ROC train: 0.990225	val: 0.697802	test: 0.706312
PRC train: 0.985822	val: 0.935188	test: 0.669407

Early stopping
Best (ROC):	 train: 0.985950	val: 0.715385	test: 0.704747
Best (PRC):	 train: 0.979285	val: 0.944528	test: 0.687888
All runs completed.

PRC train: 1.000000	val: 0.945312	test: 0.793718

Epoch: 94
Loss: 0.04758881916863907
ROC train: 1.000000	val: 0.703297	test: 0.783864
PRC train: 1.000000	val: 0.943002	test: 0.787362

Epoch: 95
Loss: 0.0773287000710435
ROC train: 1.000000	val: 0.679121	test: 0.763346
PRC train: 1.000000	val: 0.936401	test: 0.763043

Epoch: 96
Loss: 0.054017719354980454
ROC train: 1.000000	val: 0.652381	test: 0.744392
PRC train: 1.000000	val: 0.933273	test: 0.748308

Epoch: 97
Loss: 0.06453287722121868
ROC train: 1.000000	val: 0.646520	test: 0.734307
PRC train: 1.000000	val: 0.933238	test: 0.737540

Epoch: 98
Loss: 0.046698256673955586
ROC train: 0.999997	val: 0.678022	test: 0.745436
PRC train: 0.999996	val: 0.939148	test: 0.753834

Epoch: 99
Loss: 0.05320168863226763
ROC train: 1.000000	val: 0.685714	test: 0.761607
PRC train: 1.000000	val: 0.940385	test: 0.768174

Epoch: 100
Loss: 0.047940891065502624
ROC train: 1.000000	val: 0.688278	test: 0.764389
PRC train: 1.000000	val: 0.940967	test: 0.767213

Epoch: 101
Loss: 0.04976877769841294
ROC train: 1.000000	val: 0.686447	test: 0.751521
PRC train: 1.000000	val: 0.941212	test: 0.751990

Epoch: 102
Loss: 0.05604124381031694
ROC train: 1.000000	val: 0.673626	test: 0.753956
PRC train: 1.000000	val: 0.939680	test: 0.765413

Epoch: 103
Loss: 0.05103755444443425
ROC train: 1.000000	val: 0.669597	test: 0.760389
PRC train: 1.000000	val: 0.937797	test: 0.768211

Epoch: 104
Loss: 0.05515969048397963
ROC train: 1.000000	val: 0.678755	test: 0.787863
PRC train: 1.000000	val: 0.939517	test: 0.793868

Epoch: 105
Loss: 0.052422611652916284
ROC train: 1.000000	val: 0.686813	test: 0.787863
PRC train: 1.000000	val: 0.939416	test: 0.788476

Epoch: 106
Loss: 0.056383045358562256
ROC train: 1.000000	val: 0.668498	test: 0.776561
PRC train: 1.000000	val: 0.935670	test: 0.773929

Epoch: 107
Loss: 0.04679398784686147
ROC train: 1.000000	val: 0.643956	test: 0.764389
PRC train: 1.000000	val: 0.928888	test: 0.765689

Epoch: 108
Loss: 0.055545208518066144
ROC train: 1.000000	val: 0.642857	test: 0.755347
PRC train: 1.000000	val: 0.927373	test: 0.762117

Epoch: 109
Loss: 0.04386388577703414
ROC train: 1.000000	val: 0.672527	test: 0.754304
PRC train: 1.000000	val: 0.935884	test: 0.767050

Epoch: 110
Loss: 0.0421570848765431
ROC train: 1.000000	val: 0.673260	test: 0.740219
PRC train: 1.000000	val: 0.935223	test: 0.755924

Epoch: 111
Loss: 0.06076705131135347
ROC train: 1.000000	val: 0.658974	test: 0.732047
PRC train: 1.000000	val: 0.934652	test: 0.745083

Epoch: 112
Loss: 0.037037482910019064
ROC train: 1.000000	val: 0.662637	test: 0.733090
PRC train: 1.000000	val: 0.935792	test: 0.734057

Epoch: 113
Loss: 0.06744833094857543
ROC train: 1.000000	val: 0.669231	test: 0.726830
PRC train: 1.000000	val: 0.937758	test: 0.730942

Epoch: 114
Loss: 0.046008409718584845
ROC train: 1.000000	val: 0.692674	test: 0.735524
PRC train: 1.000000	val: 0.940343	test: 0.740850

Epoch: 115
Loss: 0.05804093396138871
ROC train: 1.000000	val: 0.685714	test: 0.729091
PRC train: 1.000000	val: 0.940027	test: 0.736358

Epoch: 116
Loss: 0.04641727939160031
ROC train: 1.000000	val: 0.667766	test: 0.730308
PRC train: 1.000000	val: 0.937327	test: 0.740768

Epoch: 117
Loss: 0.04255351263181656
ROC train: 1.000000	val: 0.682051	test: 0.764563
PRC train: 1.000000	val: 0.940802	test: 0.769691

Epoch: 118
Loss: 0.05958355209173296
ROC train: 1.000000	val: 0.697070	test: 0.777778
PRC train: 1.000000	val: 0.944134	test: 0.773752

Epoch: 119
Loss: 0.03774291061445551
ROC train: 1.000000	val: 0.691575	test: 0.772387
PRC train: 1.000000	val: 0.943270	test: 0.767369

Epoch: 120
Loss: 0.03975395756928428
ROC train: 1.000000	val: 0.684982	test: 0.768388
PRC train: 1.000000	val: 0.941635	test: 0.764701

Epoch: 121
Loss: 0.04882021484190681
ROC train: 1.000000	val: 0.668498	test: 0.765084
PRC train: 1.000000	val: 0.938122	test: 0.765585

Epoch: 122
Loss: 0.05281332774198194
ROC train: 1.000000	val: 0.666300	test: 0.785255
PRC train: 1.000000	val: 0.936809	test: 0.782836

Epoch: 123
Loss: 0.04360036588890109
ROC train: 1.000000	val: 0.659341	test: 0.791341
PRC train: 1.000000	val: 0.934732	test: 0.788885

Epoch: 124
Loss: 0.03277562007024809
ROC train: 1.000000	val: 0.639927	test: 0.791167
PRC train: 1.000000	val: 0.929361	test: 0.789378

Epoch: 125
Loss: 0.043950462886265175
ROC train: 1.000000	val: 0.653846	test: 0.790297
PRC train: 1.000000	val: 0.931247	test: 0.786468

Epoch: 126
Loss: 0.03671386674913148
ROC train: 1.000000	val: 0.668864	test: 0.784907
PRC train: 1.000000	val: 0.933915	test: 0.784784

Epoch: 127
Loss: 0.04147325599058552
ROC train: 1.000000	val: 0.665934	test: 0.773778
PRC train: 1.000000	val: 0.934251	test: 0.779176

Epoch: 128
Loss: 0.04679108183878554
ROC train: 1.000000	val: 0.651282	test: 0.768040
PRC train: 1.000000	val: 0.930970	test: 0.775351

Early stopping
Best (ROC):	 train: 1.000000	val: 0.704029	test: 0.794644
Best (PRC):	 train: 1.000000	val: 0.945312	test: 0.793718
All runs completed.

PRC train: 0.999983	val: 0.935559	test: 0.763882

Epoch: 94
Loss: 0.09061668021478504
ROC train: 0.999974	val: 0.704029	test: 0.787341
PRC train: 0.999962	val: 0.939343	test: 0.758564

Epoch: 95
Loss: 0.07763655287789298
ROC train: 1.000000	val: 0.701465	test: 0.804556
PRC train: 1.000000	val: 0.941322	test: 0.788975

Epoch: 96
Loss: 0.05996127610242995
ROC train: 1.000000	val: 0.709890	test: 0.802991
PRC train: 1.000000	val: 0.944055	test: 0.794214

Epoch: 97
Loss: 0.06070439879812656
ROC train: 1.000000	val: 0.700733	test: 0.803165
PRC train: 1.000000	val: 0.942836	test: 0.792283

Epoch: 98
Loss: 0.07214815258931473
ROC train: 1.000000	val: 0.694505	test: 0.803165
PRC train: 1.000000	val: 0.940897	test: 0.795968

Epoch: 99
Loss: 0.0651631334555016
ROC train: 1.000000	val: 0.710256	test: 0.792558
PRC train: 1.000000	val: 0.944190	test: 0.782644

Epoch: 100
Loss: 0.07333413652018102
ROC train: 1.000000	val: 0.702564	test: 0.773083
PRC train: 1.000000	val: 0.943855	test: 0.770164

Epoch: 101
Loss: 0.07834754338016174
ROC train: 1.000000	val: 0.712088	test: 0.778473
PRC train: 1.000000	val: 0.946577	test: 0.776345

Epoch: 102
Loss: 0.057310793690715275
ROC train: 1.000000	val: 0.717949	test: 0.796731
PRC train: 1.000000	val: 0.947157	test: 0.799909

Epoch: 103
Loss: 0.07523996274830538
ROC train: 1.000000	val: 0.733333	test: 0.796383
PRC train: 1.000000	val: 0.949902	test: 0.790058

Epoch: 104
Loss: 0.07756296764854045
ROC train: 0.999971	val: 0.743223	test: 0.780560
PRC train: 0.999956	val: 0.951902	test: 0.774957

Epoch: 105
Loss: 0.06058918118615334
ROC train: 0.999989	val: 0.747985	test: 0.790297
PRC train: 0.999983	val: 0.953708	test: 0.799823

Epoch: 106
Loss: 0.06060102489868282
ROC train: 1.000000	val: 0.739194	test: 0.801252
PRC train: 1.000000	val: 0.951619	test: 0.808866

Epoch: 107
Loss: 0.06477264808322489
ROC train: 1.000000	val: 0.710623	test: 0.793427
PRC train: 1.000000	val: 0.944629	test: 0.799200

Epoch: 108
Loss: 0.045600142896131235
ROC train: 1.000000	val: 0.701832	test: 0.760389
PRC train: 1.000000	val: 0.942239	test: 0.760290

Epoch: 109
Loss: 0.06066289976695264
ROC train: 1.000000	val: 0.710989	test: 0.762824
PRC train: 1.000000	val: 0.944592	test: 0.767760

Epoch: 110
Loss: 0.059611909643719305
ROC train: 1.000000	val: 0.702198	test: 0.768388
PRC train: 1.000000	val: 0.942982	test: 0.768609

Epoch: 111
Loss: 0.04612690072073659
ROC train: 1.000000	val: 0.698535	test: 0.783690
PRC train: 1.000000	val: 0.942497	test: 0.769708

Epoch: 112
Loss: 0.05350165581883348
ROC train: 1.000000	val: 0.700733	test: 0.793775
PRC train: 1.000000	val: 0.942331	test: 0.770343

Epoch: 113
Loss: 0.053392860661693
ROC train: 1.000000	val: 0.724176	test: 0.807338
PRC train: 1.000000	val: 0.946452	test: 0.793152

Epoch: 114
Loss: 0.05379866673226776
ROC train: 1.000000	val: 0.738462	test: 0.790993
PRC train: 1.000000	val: 0.952011	test: 0.787325

Epoch: 115
Loss: 0.06841430599105572
ROC train: 1.000000	val: 0.722344	test: 0.774996
PRC train: 1.000000	val: 0.948924	test: 0.771247

Epoch: 116
Loss: 0.04313671567985668
ROC train: 1.000000	val: 0.712821	test: 0.766823
PRC train: 1.000000	val: 0.945285	test: 0.771387

Epoch: 117
Loss: 0.06065796988406998
ROC train: 0.999994	val: 0.712454	test: 0.780734
PRC train: 0.999991	val: 0.946306	test: 0.774183

Epoch: 118
Loss: 0.07048909919358012
ROC train: 1.000000	val: 0.708059	test: 0.776734
PRC train: 1.000000	val: 0.944890	test: 0.751584

Epoch: 119
Loss: 0.06069678263432353
ROC train: 1.000000	val: 0.708791	test: 0.772387
PRC train: 1.000000	val: 0.944683	test: 0.745766

Epoch: 120
Loss: 0.04480657441989102
ROC train: 1.000000	val: 0.719780	test: 0.774648
PRC train: 1.000000	val: 0.946627	test: 0.752191

Epoch: 121
Loss: 0.05584190002757609
ROC train: 0.999997	val: 0.713919	test: 0.787167
PRC train: 0.999996	val: 0.945023	test: 0.763959

Epoch: 122
Loss: 0.057822802927652475
ROC train: 0.999997	val: 0.679487	test: 0.781777
PRC train: 0.999996	val: 0.936285	test: 0.752975

Epoch: 123
Loss: 0.052199833968342636
ROC train: 0.999991	val: 0.700366	test: 0.788211
PRC train: 0.999987	val: 0.941393	test: 0.770794

Epoch: 124
Loss: 0.05210322181955292
ROC train: 0.999986	val: 0.718681	test: 0.796731
PRC train: 0.999978	val: 0.945055	test: 0.777958

Epoch: 125
Loss: 0.048670521752740926
ROC train: 0.999986	val: 0.700733	test: 0.797079
PRC train: 0.999978	val: 0.940776	test: 0.784529

Epoch: 126
Loss: 0.047180301924064603
ROC train: 1.000000	val: 0.682418	test: 0.781255
PRC train: 1.000000	val: 0.936984	test: 0.759035

Epoch: 127
Loss: 0.06605478171581662
ROC train: 0.999977	val: 0.686447	test: 0.764041
PRC train: 0.999966	val: 0.938477	test: 0.749499

Epoch: 128
Loss: 0.050328571989632996
ROC train: 1.000000	val: 0.677656	test: 0.804034
PRC train: 1.000000	val: 0.935720	test: 0.796724

Epoch: 129
Loss: 0.05847088770625253
ROC train: 1.000000	val: 0.693773	test: 0.808033
PRC train: 1.000000	val: 0.939757	test: 0.790344

Epoch: 130
Loss: 0.05658504249846669
ROC train: 1.000000	val: 0.724542	test: 0.797774
PRC train: 1.000000	val: 0.947417	test: 0.770123

Epoch: 131
Loss: 0.06656742540348758
ROC train: 0.999986	val: 0.720147	test: 0.781951
PRC train: 0.999978	val: 0.946093	test: 0.757074

Epoch: 132
Loss: 0.07051956528014484
ROC train: 0.999974	val: 0.705861	test: 0.784559
PRC train: 0.999961	val: 0.942140	test: 0.773240

Epoch: 133
Loss: 0.06553460251324354
ROC train: 1.000000	val: 0.696337	test: 0.795688
PRC train: 1.000000	val: 0.940467	test: 0.787775

Epoch: 134
Loss: 0.0471698542204145
ROC train: 1.000000	val: 0.696703	test: 0.796557
PRC train: 1.000000	val: 0.941210	test: 0.795182

Epoch: 135
Loss: 0.04254800187105402
ROC train: 1.000000	val: 0.710989	test: 0.791688
PRC train: 1.000000	val: 0.944790	test: 0.794927

Epoch: 136
Loss: 0.0591558358877603
ROC train: 1.000000	val: 0.702930	test: 0.765954
PRC train: 1.000000	val: 0.944130	test: 0.778718

Epoch: 137
Loss: 0.05545345923928348
ROC train: 1.000000	val: 0.715018	test: 0.765084
PRC train: 1.000000	val: 0.947289	test: 0.781831

Epoch: 138
Loss: 0.04548569407506996
ROC train: 1.000000	val: 0.740659	test: 0.782820
PRC train: 1.000000	val: 0.952485	test: 0.798252

Epoch: 139
Loss: 0.04425093125000743
ROC train: 0.999991	val: 0.732967	test: 0.778995
PRC train: 0.999987	val: 0.951440	test: 0.795418

Epoch: 140
Loss: 0.04881752155783957
ROC train: 1.000000	val: 0.701465	test: 0.777952
PRC train: 1.000000	val: 0.944244	test: 0.781574

Early stopping
Best (ROC):	 train: 0.999989	val: 0.747985	test: 0.790297
Best (PRC):	 train: 0.999983	val: 0.953708	test: 0.799823
All runs completed.

PRC train: 1.000000	val: 0.909886	test: 0.813294

Epoch: 94
Loss: 0.055628331137565545
ROC train: 1.000000	val: 0.657875	test: 0.813772
PRC train: 1.000000	val: 0.911024	test: 0.808658

Epoch: 95
Loss: 0.04680998298987492
ROC train: 1.000000	val: 0.670330	test: 0.804556
PRC train: 1.000000	val: 0.916415	test: 0.799164

Epoch: 96
Loss: 0.05051717348484462
ROC train: 1.000000	val: 0.673260	test: 0.791167
PRC train: 1.000000	val: 0.918916	test: 0.778886

Epoch: 97
Loss: 0.06714533682542985
ROC train: 1.000000	val: 0.661905	test: 0.779343
PRC train: 1.000000	val: 0.914903	test: 0.759012

Epoch: 98
Loss: 0.07006197819652785
ROC train: 1.000000	val: 0.656777	test: 0.785255
PRC train: 1.000000	val: 0.908973	test: 0.790204

Epoch: 99
Loss: 0.05273617681443549
ROC train: 1.000000	val: 0.671062	test: 0.786994
PRC train: 1.000000	val: 0.913384	test: 0.803090

Epoch: 100
Loss: 0.05207225340270812
ROC train: 1.000000	val: 0.675824	test: 0.791862
PRC train: 1.000000	val: 0.921060	test: 0.803545

Epoch: 101
Loss: 0.05512993860138053
ROC train: 1.000000	val: 0.666300	test: 0.805251
PRC train: 1.000000	val: 0.919775	test: 0.809619

Epoch: 102
Loss: 0.05205123283072686
ROC train: 1.000000	val: 0.666667	test: 0.801947
PRC train: 1.000000	val: 0.921994	test: 0.802855

Epoch: 103
Loss: 0.07321035821020619
ROC train: 1.000000	val: 0.679121	test: 0.776734
PRC train: 1.000000	val: 0.926784	test: 0.751214

Epoch: 104
Loss: 0.04696638644250829
ROC train: 1.000000	val: 0.686813	test: 0.762476
PRC train: 1.000000	val: 0.928675	test: 0.739451

Epoch: 105
Loss: 0.053293872970022596
ROC train: 1.000000	val: 0.689011	test: 0.758825
PRC train: 1.000000	val: 0.923012	test: 0.757990

Epoch: 106
Loss: 0.07344147090757364
ROC train: 1.000000	val: 0.695238	test: 0.760042
PRC train: 1.000000	val: 0.925588	test: 0.766905

Epoch: 107
Loss: 0.04837192261126596
ROC train: 1.000000	val: 0.692674	test: 0.754304
PRC train: 1.000000	val: 0.926096	test: 0.744696

Epoch: 108
Loss: 0.0513669634365004
ROC train: 1.000000	val: 0.697802	test: 0.762650
PRC train: 1.000000	val: 0.931004	test: 0.765739

Epoch: 109
Loss: 0.04698280047594566
ROC train: 1.000000	val: 0.707326	test: 0.765954
PRC train: 1.000000	val: 0.932726	test: 0.770141

Epoch: 110
Loss: 0.053980250844559284
ROC train: 1.000000	val: 0.692308	test: 0.773431
PRC train: 1.000000	val: 0.924294	test: 0.764635

Epoch: 111
Loss: 0.044367857242100475
ROC train: 1.000000	val: 0.686813	test: 0.771170
PRC train: 1.000000	val: 0.917948	test: 0.748744

Epoch: 112
Loss: 0.05581698653911445
ROC train: 1.000000	val: 0.677289	test: 0.775865
PRC train: 1.000000	val: 0.910033	test: 0.751661

Epoch: 113
Loss: 0.061597911604169796
ROC train: 1.000000	val: 0.664103	test: 0.783516
PRC train: 1.000000	val: 0.906794	test: 0.772735

Epoch: 114
Loss: 0.05290727362619543
ROC train: 1.000000	val: 0.666300	test: 0.785950
PRC train: 1.000000	val: 0.911586	test: 0.791539

Epoch: 115
Loss: 0.06407963659355817
ROC train: 1.000000	val: 0.658242	test: 0.783516
PRC train: 1.000000	val: 0.918759	test: 0.796889

Epoch: 116
Loss: 0.07031202451061389
ROC train: 1.000000	val: 0.663004	test: 0.801600
PRC train: 1.000000	val: 0.920667	test: 0.806502

Epoch: 117
Loss: 0.04420266620973062
ROC train: 1.000000	val: 0.682418	test: 0.776561
PRC train: 1.000000	val: 0.923096	test: 0.766495

Epoch: 118
Loss: 0.050484579280821076
ROC train: 1.000000	val: 0.703663	test: 0.776734
PRC train: 1.000000	val: 0.929000	test: 0.779498

Epoch: 119
Loss: 0.033235734574040894
ROC train: 1.000000	val: 0.721245	test: 0.790297
PRC train: 1.000000	val: 0.936992	test: 0.794280

Epoch: 120
Loss: 0.03586848013305596
ROC train: 1.000000	val: 0.725275	test: 0.794297
PRC train: 1.000000	val: 0.939034	test: 0.801369

Epoch: 121
Loss: 0.051034705007578306
ROC train: 1.000000	val: 0.703297	test: 0.793253
PRC train: 1.000000	val: 0.932794	test: 0.782805

Epoch: 122
Loss: 0.04668499012405229
ROC train: 1.000000	val: 0.701099	test: 0.788906
PRC train: 1.000000	val: 0.932635	test: 0.764882

Epoch: 123
Loss: 0.0403154670252988
ROC train: 1.000000	val: 0.693773	test: 0.800035
PRC train: 1.000000	val: 0.933592	test: 0.783335

Epoch: 124
Loss: 0.044273147773217766
ROC train: 1.000000	val: 0.695238	test: 0.793253
PRC train: 1.000000	val: 0.930955	test: 0.802206

Epoch: 125
Loss: 0.0500069514729982
ROC train: 1.000000	val: 0.673260	test: 0.801947
PRC train: 1.000000	val: 0.916397	test: 0.808854

Epoch: 126
Loss: 0.03903907242192935
ROC train: 1.000000	val: 0.662271	test: 0.792210
PRC train: 1.000000	val: 0.907738	test: 0.786049

Epoch: 127
Loss: 0.050810270613125263
ROC train: 1.000000	val: 0.662271	test: 0.799861
PRC train: 1.000000	val: 0.910109	test: 0.799531

Epoch: 128
Loss: 0.0394692393620603
ROC train: 1.000000	val: 0.661172	test: 0.814815
PRC train: 1.000000	val: 0.911824	test: 0.820733

Epoch: 129
Loss: 0.0325870558552374
ROC train: 1.000000	val: 0.663004	test: 0.816554
PRC train: 1.000000	val: 0.915471	test: 0.817981

Epoch: 130
Loss: 0.03492193861513491
ROC train: 1.000000	val: 0.672527	test: 0.811859
PRC train: 1.000000	val: 0.916742	test: 0.808581

Epoch: 131
Loss: 0.04363953994202955
ROC train: 1.000000	val: 0.658242	test: 0.822292
PRC train: 1.000000	val: 0.914644	test: 0.815632

Epoch: 132
Loss: 0.04576918360842878
ROC train: 1.000000	val: 0.658242	test: 0.791515
PRC train: 1.000000	val: 0.911571	test: 0.763271

Epoch: 133
Loss: 0.033283197642410695
ROC train: 1.000000	val: 0.666667	test: 0.780386
PRC train: 1.000000	val: 0.909317	test: 0.747770

Epoch: 134
Loss: 0.044923406055160806
ROC train: 1.000000	val: 0.664469	test: 0.792036
PRC train: 1.000000	val: 0.915057	test: 0.791623

Epoch: 135
Loss: 0.034715481971478154
ROC train: 1.000000	val: 0.648352	test: 0.795862
PRC train: 1.000000	val: 0.915159	test: 0.797618

Epoch: 136
Loss: 0.03820592891800197
ROC train: 1.000000	val: 0.647985	test: 0.816206
PRC train: 1.000000	val: 0.910275	test: 0.819205

Epoch: 137
Loss: 0.036876896457227075
ROC train: 1.000000	val: 0.660806	test: 0.814989
PRC train: 1.000000	val: 0.910801	test: 0.803952

Epoch: 138
Loss: 0.0329198617731132
ROC train: 1.000000	val: 0.670330	test: 0.804382
PRC train: 1.000000	val: 0.915862	test: 0.796689

Epoch: 139
Loss: 0.03619981692347042
ROC train: 1.000000	val: 0.700366	test: 0.797427
PRC train: 1.000000	val: 0.930888	test: 0.808799

Epoch: 140
Loss: 0.030738777434284485
ROC train: 1.000000	val: 0.703297	test: 0.782473
PRC train: 1.000000	val: 0.931169	test: 0.796778

Epoch: 141
Loss: 0.043671332992785306
ROC train: 1.000000	val: 0.705128	test: 0.777778
PRC train: 1.000000	val: 0.932170	test: 0.790829

Epoch: 142
Loss: 0.04765305506657582
ROC train: 1.000000	val: 0.703297	test: 0.787863
PRC train: 1.000000	val: 0.932033	test: 0.791838

Epoch: 143
Loss: 0.043483357152609606
ROC train: 1.000000	val: 0.681685	test: 0.791688
PRC train: 1.000000	val: 0.916074	test: 0.796990

Epoch: 144
Loss: 0.03057728645772164
ROC train: 1.000000	val: 0.671429	test: 0.797079
PRC train: 1.000000	val: 0.911358	test: 0.804732

Epoch: 145
Loss: 0.034761687777171146
ROC train: 1.000000	val: 0.673260	test: 0.803686
PRC train: 1.000000	val: 0.914154	test: 0.801438

Epoch: 146
Loss: 0.05244945445619318
ROC train: 1.000000	val: 0.679121	test: 0.807164
PRC train: 1.000000	val: 0.917162	test: 0.805543

Epoch: 147
Loss: 0.026103708045643575
ROC train: 1.000000	val: 0.673626	test: 0.800556
PRC train: 1.000000	val: 0.916524	test: 0.794804

Epoch: 148
Loss: 0.03558060504177861
ROC train: 1.000000	val: 0.676557	test: 0.797253
PRC train: 1.000000	val: 0.911217	test: 0.780993

Epoch: 149
Loss: 0.02768566808399926
ROC train: 1.000000	val: 0.676557	test: 0.788037
PRC train: 1.000000	val: 0.909783	test: 0.773373

Epoch: 150
Loss: 0.04656222769397233
ROC train: 1.000000	val: 0.669231	test: 0.796383
PRC train: 1.000000	val: 0.909953	test: 0.797701

Epoch: 151
Loss: 0.0475904493253831
ROC train: 1.000000	val: 0.664103	test: 0.798818
PRC train: 1.000000	val: 0.906971	test: 0.805266

Epoch: 152
Loss: 0.03842565093206633
ROC train: 1.000000	val: 0.657143	test: 0.785081
PRC train: 1.000000	val: 0.903737	test: 0.787988

Epoch: 153
Loss: 0.04751945574936546
ROC train: 1.000000	val: 0.659707	test: 0.789254
PRC train: 1.000000	val: 0.905864	test: 0.794518

Epoch: 154
Loss: 0.034596972200742974
ROC train: 1.000000	val: 0.649451	test: 0.800383
PRC train: 1.000000	val: 0.903018	test: 0.803850

Epoch: 155
Loss: 0.029919009382245555
ROC train: 1.000000	val: 0.651648	test: 0.801426
PRC train: 1.000000	val: 0.904872	test: 0.804206

Early stopping
Best (ROC):	 train: 1.000000	val: 0.725275	test: 0.794297
Best (PRC):	 train: 1.000000	val: 0.939034	test: 0.801369
All runs completed.
