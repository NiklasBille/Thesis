>>> Starting run for dataset: esol
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running RANDOM configs_split_experiments/GraphMVP/esol/random/train_prop=0.6.yml on cuda:0
Running RANDOM configs_split_experiments/GraphMVP/esol/random/train_prop=0.7.yml on cuda:1
Running RANDOM configs_split_experiments/GraphMVP/esol/random/train_prop=0.8.yml on cuda:2
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/esol/random/train_prop=0.6.yml --runseed 4 --device cuda:0
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/esol/random/train_prop=0.8.yml --runseed 4 --device cuda:2
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/esol/random/train_prop=0.7.yml --runseed 4 --device cuda:1
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/esol/random/train_prop=0.8.yml --runseed 5 --device cuda:2
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/esol/random/train_prop=0.6.yml --runseed 5 --device cuda:0
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/esol/random/train_prop=0.7.yml --runseed 5 --device cuda:1
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/esol/random/train_prop=0.8.yml --runseed 6 --device cuda:2Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/esol/random/train_prop=0.6.yml --runseed 6 --device cuda:0

Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/esol/random/train_prop=0.7.yml --runseed 6 --device cuda:1
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/esol/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/esol/random/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/esol/random/train_prop=0.6/esol_random_6_26-05_11-02-53  ]
[ Using Seed :  6  ]
[ Using device :  cuda:0  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 13.68503475189209
RMSE train: 3.580964	val: 3.448059	test: 3.398058
MAE train: 3.032127	val: 2.936554	test: 2.878225

Epoch: 2
Loss: 12.664024035135904
RMSE train: 3.399784	val: 3.281526	test: 3.207595
MAE train: 2.867696	val: 2.797077	test: 2.702762

Epoch: 3
Loss: 11.65092945098877
RMSE train: 3.264284	val: 3.194611	test: 3.063189
MAE train: 2.731764	val: 2.714016	test: 2.555479

Epoch: 4
Loss: 10.551813125610352
RMSE train: 3.070349	val: 3.060193	test: 2.853490
MAE train: 2.526914	val: 2.557588	test: 2.347899

Epoch: 5
Loss: 9.810911178588867
RMSE train: 2.777304	val: 2.787917	test: 2.562370
MAE train: 2.253240	val: 2.293264	test: 2.080082

Epoch: 6
Loss: 9.183428764343262
RMSE train: 2.512169	val: 2.516272	test: 2.314471
MAE train: 2.016482	val: 2.051485	test: 1.852347

Epoch: 7
Loss: 8.50029436747233
RMSE train: 2.370731	val: 2.369269	test: 2.200377
MAE train: 1.905195	val: 1.935890	test: 1.761329

Epoch: 8
Loss: 8.007975260416666
RMSE train: 2.482661	val: 2.478450	test: 2.341570
MAE train: 2.038438	val: 2.050246	test: 1.928351

Epoch: 9
Loss: 7.548231442769368
RMSE train: 2.479004	val: 2.472681	test: 2.356507
MAE train: 2.027830	val: 2.041682	test: 1.933723

Epoch: 10
Loss: 7.008416493733724
RMSE train: 2.395364	val: 2.385876	test: 2.276413
MAE train: 1.941161	val: 1.950195	test: 1.842397

Epoch: 11
Loss: 6.468223889668782
RMSE train: 2.370668	val: 2.359431	test: 2.253389
MAE train: 1.935682	val: 1.941887	test: 1.828040

Epoch: 12
Loss: 6.1470638910929365
RMSE train: 2.389960	val: 2.372959	test: 2.277372
MAE train: 1.988811	val: 1.990467	test: 1.877775

Epoch: 13
Loss: 5.51128355662028
RMSE train: 2.270648	val: 2.256388	test: 2.160841
MAE train: 1.895667	val: 1.890901	test: 1.784712

Epoch: 14
Loss: 5.239731788635254
RMSE train: 2.123824	val: 2.112285	test: 2.014797
MAE train: 1.755454	val: 1.746169	test: 1.644903

Epoch: 15
Loss: 4.617917378743489
RMSE train: 2.072623	val: 2.066070	test: 1.969144
MAE train: 1.700455	val: 1.692815	test: 1.593095

Epoch: 16
Loss: 4.28229554494222
RMSE train: 2.095512	val: 2.092620	test: 1.996376
MAE train: 1.738670	val: 1.727273	test: 1.627090

Epoch: 17
Loss: 3.7562195460001626
RMSE train: 1.983937	val: 1.985108	test: 1.890389
MAE train: 1.647033	val: 1.639286	test: 1.534972

Epoch: 18
Loss: 3.4559895992279053
RMSE train: 1.875091	val: 1.879578	test: 1.788602
MAE train: 1.566250	val: 1.564413	test: 1.459288

Epoch: 19
Loss: 3.0129504998524985
RMSE train: 1.767504	val: 1.767390	test: 1.696232
MAE train: 1.477445	val: 1.476619	test: 1.388413

Epoch: 20
Loss: 2.616621255874634
RMSE train: 1.545388	val: 1.569882	test: 1.479285
MAE train: 1.236203	val: 1.246895	test: 1.152820

Epoch: 21
Loss: 2.3208235104878745
RMSE train: 1.392367	val: 1.434414	test: 1.330206
MAE train: 1.076436	val: 1.116688	test: 1.008490

Epoch: 22
Loss: 2.1354600191116333
RMSE train: 1.318938	val: 1.323959	test: 1.274521
MAE train: 1.065230	val: 1.053847	test: 1.017060

Epoch: 23
Loss: 1.9042473236719768Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/esol/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/esol/random/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/esol/random/train_prop=0.6/esol_random_4_26-05_11-02-53  ]
[ Using Seed :  4  ]
[ Using device :  cuda:0  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 13.57213020324707
RMSE train: 3.703500	val: 3.581153	test: 3.555263
MAE train: 3.151783	val: 3.075350	test: 3.054301

Epoch: 2
Loss: 12.30455462137858
RMSE train: 3.479393	val: 3.385008	test: 3.319928
MAE train: 2.990941	val: 2.954424	test: 2.875782

Epoch: 3
Loss: 11.114358901977539
RMSE train: 3.365702	val: 3.324970	test: 3.197175
MAE train: 2.913097	val: 2.927746	test: 2.788696

Epoch: 4
Loss: 10.235244115193685
RMSE train: 3.267855	val: 3.275408	test: 3.079478
MAE train: 2.810563	val: 2.864857	test: 2.669766

Epoch: 5
Loss: 9.576169967651367
RMSE train: 3.122246	val: 3.142391	test: 2.919631
MAE train: 2.652694	val: 2.714147	test: 2.495717

Epoch: 6
Loss: 8.857024828592936
RMSE train: 2.926555	val: 2.925269	test: 2.728807
MAE train: 2.459175	val: 2.482792	test: 2.282687

Epoch: 7
Loss: 8.247071584065756
RMSE train: 2.734603	val: 2.704312	test: 2.548926
MAE train: 2.282024	val: 2.260787	test: 2.104601

Epoch: 8
Loss: 7.760157426198323
RMSE train: 2.589464	val: 2.537921	test: 2.419756
MAE train: 2.154158	val: 2.115818	test: 1.985826

Epoch: 9
Loss: 7.308572451273601
RMSE train: 2.544271	val: 2.477793	test: 2.392813
MAE train: 2.113984	val: 2.068192	test: 1.959879

Epoch: 10
Loss: 6.722459634145101
RMSE train: 2.555765	val: 2.491621	test: 2.415888
MAE train: 2.131350	val: 2.092807	test: 1.988805

Epoch: 11
Loss: 6.177760124206543
RMSE train: 2.545528	val: 2.493767	test: 2.404864
MAE train: 2.137262	val: 2.106512	test: 1.992041

Epoch: 12
Loss: 5.823786576588948
RMSE train: 2.517129	val: 2.482719	test: 2.377308
MAE train: 2.116741	val: 2.094370	test: 1.971106

Epoch: 13
Loss: 5.431056181589763
RMSE train: 2.379756	val: 2.365909	test: 2.244810
MAE train: 2.013863	val: 2.016017	test: 1.888799

Epoch: 14
Loss: 4.8630631764729815
RMSE train: 2.268651	val: 2.263102	test: 2.140120
MAE train: 1.910494	val: 1.917014	test: 1.792835

Epoch: 15
Loss: 4.454548994700114
RMSE train: 2.213091	val: 2.196084	test: 2.095676
MAE train: 1.846934	val: 1.834430	test: 1.729085

Epoch: 16
Loss: 3.8846885363260903
RMSE train: 2.070952	val: 2.041413	test: 1.969050
MAE train: 1.707866	val: 1.686107	test: 1.604720

Epoch: 17
Loss: 3.7057345708211265
RMSE train: 1.892513	val: 1.866185	test: 1.795038
MAE train: 1.547717	val: 1.528297	test: 1.450460

Epoch: 18
Loss: 3.2066641648610434
RMSE train: 1.752679	val: 1.736957	test: 1.659138
MAE train: 1.439418	val: 1.423698	test: 1.344519

Epoch: 19
Loss: 2.8194793065389
RMSE train: 1.673819	val: 1.658561	test: 1.582927
MAE train: 1.359409	val: 1.339796	test: 1.268850

Epoch: 20
Loss: 2.6083516279856362
RMSE train: 1.639309	val: 1.623814	test: 1.549594
MAE train: 1.349524	val: 1.335645	test: 1.266196

Epoch: 21
Loss: 2.3860088189442954
RMSE train: 1.705372	val: 1.678952	test: 1.615093
MAE train: 1.403461	val: 1.377445	test: 1.322550

Epoch: 22
Loss: 2.0017128785451255
RMSE train: 1.643694	val: 1.635348	test: 1.560957
MAE train: 1.363463	val: 1.353456	test: 1.277621

Epoch: 23
Loss: 1.7423830429712932Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/esol/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/esol/random/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/esol/random/train_prop=0.6/esol_random_5_26-05_11-02-53  ]
[ Using Seed :  5  ]
[ Using device :  cuda:0  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 14.324878374735514
RMSE train: 3.606644	val: 3.491741	test: 3.439423
MAE train: 3.108140	val: 3.045059	test: 2.971651

Epoch: 2
Loss: 12.767555872599283
RMSE train: 3.443537	val: 3.368150	test: 3.271496
MAE train: 2.969955	val: 2.957101	test: 2.835724

Epoch: 3
Loss: 11.787726084391275
RMSE train: 3.326422	val: 3.300758	test: 3.147393
MAE train: 2.843800	val: 2.874923	test: 2.706621

Epoch: 4
Loss: 10.889676729838053
RMSE train: 3.246657	val: 3.255256	test: 3.041770
MAE train: 2.763354	val: 2.827604	test: 2.611635

Epoch: 5
Loss: 10.337766965230307
RMSE train: 3.212985	val: 3.226373	test: 3.003918
MAE train: 2.751162	val: 2.813951	test: 2.581225

Epoch: 6
Loss: 9.56679884592692
RMSE train: 3.078539	val: 3.077553	test: 2.884443
MAE train: 2.621746	val: 2.660924	test: 2.453513

Epoch: 7
Loss: 8.931859334309896
RMSE train: 2.890954	val: 2.875153	test: 2.718098
MAE train: 2.444549	val: 2.458633	test: 2.301461

Epoch: 8
Loss: 8.519700368245443
RMSE train: 2.788176	val: 2.756906	test: 2.639215
MAE train: 2.356178	val: 2.356559	test: 2.231811

Epoch: 9
Loss: 7.802573998769124
RMSE train: 2.734085	val: 2.690693	test: 2.599940
MAE train: 2.298887	val: 2.286407	test: 2.182092

Epoch: 10
Loss: 7.111262798309326
RMSE train: 2.704154	val: 2.659667	test: 2.577615
MAE train: 2.273537	val: 2.252074	test: 2.155498

Epoch: 11
Loss: 6.759592056274414
RMSE train: 2.679754	val: 2.645558	test: 2.548157
MAE train: 2.276045	val: 2.254730	test: 2.143392

Epoch: 12
Loss: 6.358085473378499
RMSE train: 2.638091	val: 2.612492	test: 2.504305
MAE train: 2.254728	val: 2.235109	test: 2.115243

Epoch: 13
Loss: 5.8010891278584795
RMSE train: 2.551308	val: 2.531419	test: 2.419242
MAE train: 2.180887	val: 2.164323	test: 2.044984

Epoch: 14
Loss: 5.3228715260823565
RMSE train: 2.380851	val: 2.360398	test: 2.257972
MAE train: 2.025742	val: 2.007514	test: 1.896833

Epoch: 15
Loss: 5.011360804239909
RMSE train: 2.273624	val: 2.252147	test: 2.161483
MAE train: 1.919981	val: 1.896664	test: 1.797809

Epoch: 16
Loss: 4.400232474009196
RMSE train: 2.212416	val: 2.194296	test: 2.109009
MAE train: 1.857812	val: 1.831983	test: 1.743061

Epoch: 17
Loss: 4.187707742055257
RMSE train: 2.134590	val: 2.125052	test: 2.035863
MAE train: 1.811358	val: 1.796249	test: 1.701102

Epoch: 18
Loss: 3.5667062600453696
RMSE train: 2.033233	val: 2.016132	test: 1.939960
MAE train: 1.719598	val: 1.698405	test: 1.615072

Epoch: 19
Loss: 3.20825203259786
RMSE train: 1.954223	val: 1.933038	test: 1.866748
MAE train: 1.634675	val: 1.613023	test: 1.537163

Epoch: 20
Loss: 2.9039559364318848
RMSE train: 1.905621	val: 1.894161	test: 1.818837
MAE train: 1.588939	val: 1.578355	test: 1.494400

Epoch: 21
Loss: 2.48470401763916
RMSE train: 1.778644	val: 1.782717	test: 1.695864
MAE train: 1.502873	val: 1.502565	test: 1.409327

Epoch: 22
Loss: 2.260585149129232
RMSE train: 1.721338	val: 1.721198	test: 1.657189
MAE train: 1.436170	val: 1.424654	test: 1.361527

Epoch: 23
Loss: 1.9888705015182495Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/esol/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/esol/random/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/esol/random/train_prop=0.8/esol_random_5_26-05_11-02-53  ]
[ Using Seed :  5  ]
[ Using device :  cuda:2  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 13.692152738571167
RMSE train: 3.532667	val: 3.551229	test: 3.204262
MAE train: 3.056532	val: 3.088513	test: 2.755541

Epoch: 2
Loss: 11.96371603012085
RMSE train: 3.385710	val: 3.357580	test: 3.051026
MAE train: 2.931922	val: 2.918186	test: 2.638841

Epoch: 3
Loss: 10.901531457901001
RMSE train: 3.195545	val: 3.122749	test: 2.827418
MAE train: 2.718782	val: 2.665114	test: 2.416648

Epoch: 4
Loss: 9.87619686126709
RMSE train: 2.985740	val: 2.901635	test: 2.619907
MAE train: 2.515516	val: 2.445288	test: 2.186997

Epoch: 5
Loss: 8.91833233833313
RMSE train: 2.770081	val: 2.689837	test: 2.458560
MAE train: 2.323083	val: 2.264149	test: 2.047081

Epoch: 6
Loss: 8.2140132188797
RMSE train: 2.680380	val: 2.648783	test: 2.409409
MAE train: 2.267028	val: 2.264153	test: 2.014536

Epoch: 7
Loss: 7.458340048789978
RMSE train: 2.619792	val: 2.650717	test: 2.353540
MAE train: 2.201668	val: 2.246780	test: 1.948553

Epoch: 8
Loss: 6.679323077201843
RMSE train: 2.498035	val: 2.508718	test: 2.249806
MAE train: 2.098281	val: 2.108558	test: 1.866401

Epoch: 9
Loss: 6.018353581428528
RMSE train: 2.483857	val: 2.497074	test: 2.224271
MAE train: 2.079850	val: 2.072372	test: 1.836132

Epoch: 10
Loss: 5.840821385383606
RMSE train: 2.417592	val: 2.425327	test: 2.172789
MAE train: 2.043782	val: 2.017645	test: 1.810004

Epoch: 11
Loss: 4.898867547512054
RMSE train: 2.274339	val: 2.276122	test: 2.051579
MAE train: 1.917436	val: 1.899616	test: 1.715350

Epoch: 12
Loss: 4.58437192440033
RMSE train: 2.188641	val: 2.175355	test: 1.983518
MAE train: 1.823057	val: 1.806931	test: 1.644473

Epoch: 13
Loss: 3.987268328666687
RMSE train: 2.064858	val: 2.041861	test: 1.872098
MAE train: 1.710030	val: 1.674728	test: 1.544501

Epoch: 14
Loss: 3.377641022205353
RMSE train: 1.923458	val: 1.888726	test: 1.754834
MAE train: 1.609113	val: 1.557601	test: 1.467818

Epoch: 15
Loss: 2.8905186653137207
RMSE train: 1.849121	val: 1.855784	test: 1.664122
MAE train: 1.528801	val: 1.516316	test: 1.372616

Epoch: 16
Loss: 2.5323439240455627
RMSE train: 1.738944	val: 1.768304	test: 1.570218
MAE train: 1.427943	val: 1.428712	test: 1.289836

Epoch: 17
Loss: 2.155272901058197
RMSE train: 1.576230	val: 1.615961	test: 1.443581
MAE train: 1.290793	val: 1.299690	test: 1.183975

Epoch: 18
Loss: 1.7913209199905396
RMSE train: 1.357278	val: 1.369513	test: 1.275572
MAE train: 1.104876	val: 1.085773	test: 1.051166

Epoch: 19
Loss: 1.5939003825187683
RMSE train: 1.245046	val: 1.271104	test: 1.185686
MAE train: 0.982946	val: 1.007055	test: 0.945092

Epoch: 20
Loss: 1.297931581735611
RMSE train: 1.211942	val: 1.270970	test: 1.163302
MAE train: 0.953841	val: 1.014136	test: 0.918057

Epoch: 21
Loss: 1.130034789443016
RMSE train: 1.188793	val: 1.223090	test: 1.136572
MAE train: 0.944701	val: 0.969015	test: 0.896305

Epoch: 22
Loss: 0.9895077496767044
RMSE train: 1.010706	val: 1.074002	test: 1.026410
MAE train: 0.803257	val: 0.829178	test: 0.842838

Epoch: 23
Loss: 0.8714677691459656Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/esol/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/esol/random/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/esol/random/train_prop=0.7/esol_random_6_26-05_11-02-53  ]
[ Using Seed :  6  ]
[ Using device :  cuda:1  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 12.358302593231201
RMSE train: 3.477113	val: 3.530219	test: 3.272404
MAE train: 2.936438	val: 3.021601	test: 2.777103

Epoch: 2
Loss: 11.738175868988037
RMSE train: 3.238255	val: 3.322350	test: 3.010319
MAE train: 2.720851	val: 2.831584	test: 2.527969

Epoch: 3
Loss: 11.052485704421997
RMSE train: 2.997947	val: 3.103095	test: 2.735970
MAE train: 2.492200	val: 2.616966	test: 2.275966

Epoch: 4
Loss: 8.80276644229889
RMSE train: 2.831020	val: 2.947225	test: 2.552207
MAE train: 2.313011	val: 2.455281	test: 2.072875

Epoch: 5
Loss: 8.868488311767578
RMSE train: 2.651742	val: 2.758224	test: 2.401692
MAE train: 2.150069	val: 2.290195	test: 1.918512

Epoch: 6
Loss: 8.307358741760254
RMSE train: 2.429682	val: 2.504222	test: 2.218828
MAE train: 1.955185	val: 2.068203	test: 1.757027

Epoch: 7
Loss: 7.939880013465881
RMSE train: 2.208390	val: 2.268464	test: 2.001636
MAE train: 1.736452	val: 1.828719	test: 1.538333

Epoch: 8
Loss: 6.992278456687927
RMSE train: 2.091721	val: 2.137491	test: 1.886231
MAE train: 1.621345	val: 1.701538	test: 1.411331

Epoch: 9
Loss: 6.502241253852844
RMSE train: 2.088933	val: 2.118738	test: 1.875129
MAE train: 1.623378	val: 1.680159	test: 1.408239

Epoch: 10
Loss: 5.560245752334595
RMSE train: 2.016923	val: 2.042761	test: 1.817248
MAE train: 1.592557	val: 1.625478	test: 1.400847

Epoch: 11
Loss: 5.491611480712891
RMSE train: 2.017347	val: 2.056239	test: 1.825533
MAE train: 1.637786	val: 1.655892	test: 1.463240

Epoch: 12
Loss: 4.031169354915619
RMSE train: 2.078762	val: 2.158267	test: 1.906893
MAE train: 1.717012	val: 1.793544	test: 1.561484

Epoch: 13
Loss: 3.7293590307235718
RMSE train: 1.946822	val: 2.063162	test: 1.790031
MAE train: 1.563987	val: 1.696325	test: 1.414445

Epoch: 14
Loss: 3.5070573687553406
RMSE train: 1.735711	val: 1.861504	test: 1.589850
MAE train: 1.330793	val: 1.461151	test: 1.182066

Epoch: 15
Loss: 2.792236238718033
RMSE train: 1.587388	val: 1.682843	test: 1.466676
MAE train: 1.211013	val: 1.305469	test: 1.099679

Epoch: 16
Loss: 2.480875015258789
RMSE train: 1.491882	val: 1.542811	test: 1.387210
MAE train: 1.154767	val: 1.221007	test: 1.056761

Epoch: 17
Loss: 2.216677874326706
RMSE train: 1.234870	val: 1.234805	test: 1.136698
MAE train: 0.950501	val: 0.966322	test: 0.876349

Epoch: 18
Loss: 2.8928772807121277
RMSE train: 1.293159	val: 1.323926	test: 1.176323
MAE train: 0.995315	val: 1.031178	test: 0.871968

Epoch: 19
Loss: 1.5067210048437119
RMSE train: 1.213177	val: 1.268982	test: 1.110628
MAE train: 0.929343	val: 0.995934	test: 0.819961

Epoch: 20
Loss: 1.2462220340967178
RMSE train: 1.141654	val: 1.232767	test: 1.087900
MAE train: 0.861898	val: 0.945599	test: 0.799498

Epoch: 21
Loss: 1.2852861285209656
RMSE train: 0.953803	val: 1.029954	test: 0.947861
MAE train: 0.719270	val: 0.777698	test: 0.688589

Epoch: 22
Loss: 1.2436893284320831
RMSE train: 1.098557	val: 1.147704	test: 1.058315
MAE train: 0.855072	val: 0.917780	test: 0.813392

Epoch: 23
Loss: 1.3499779552221298Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/esol/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/esol/random/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/esol/random/train_prop=0.7/esol_random_5_26-05_11-02-53  ]
[ Using Seed :  5  ]
[ Using device :  cuda:1  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 13.4487464427948
RMSE train: 3.513319	val: 3.574042	test: 3.325195
MAE train: 3.030169	val: 3.119373	test: 2.891462

Epoch: 2
Loss: 12.928200006484985
RMSE train: 3.371393	val: 3.468294	test: 3.160517
MAE train: 2.914178	val: 3.033950	test: 2.748556

Epoch: 3
Loss: 10.587411165237427
RMSE train: 3.280922	val: 3.376725	test: 3.028915
MAE train: 2.827440	val: 2.954546	test: 2.637369

Epoch: 4
Loss: 9.97551679611206
RMSE train: 3.190101	val: 3.273230	test: 2.917900
MAE train: 2.733604	val: 2.851886	test: 2.523929

Epoch: 5
Loss: 8.940261483192444
RMSE train: 2.980304	val: 3.043111	test: 2.719491
MAE train: 2.512148	val: 2.607121	test: 2.291544

Epoch: 6
Loss: 8.349531531333923
RMSE train: 2.721851	val: 2.764821	test: 2.489439
MAE train: 2.268061	val: 2.347521	test: 2.081671

Epoch: 7
Loss: 7.057990312576294
RMSE train: 2.550106	val: 2.579948	test: 2.342840
MAE train: 2.136913	val: 2.200644	test: 1.968043

Epoch: 8
Loss: 6.807546138763428
RMSE train: 2.497560	val: 2.522705	test: 2.311202
MAE train: 2.109214	val: 2.168956	test: 1.949147

Epoch: 9
Loss: 6.7339417934417725
RMSE train: 2.375391	val: 2.390843	test: 2.208453
MAE train: 2.001210	val: 2.054980	test: 1.853274

Epoch: 10
Loss: 5.722429037094116
RMSE train: 2.243227	val: 2.266079	test: 2.090929
MAE train: 1.883267	val: 1.932399	test: 1.750425

Epoch: 11
Loss: 5.547596335411072
RMSE train: 2.166021	val: 2.190681	test: 2.014822
MAE train: 1.805417	val: 1.840999	test: 1.666134

Epoch: 12
Loss: 4.229416310787201
RMSE train: 2.076132	val: 2.107101	test: 1.936827
MAE train: 1.710209	val: 1.744010	test: 1.585630

Epoch: 13
Loss: 4.0961819887161255
RMSE train: 2.144561	val: 2.187107	test: 2.031682
MAE train: 1.785779	val: 1.843484	test: 1.694559

Epoch: 14
Loss: 4.253488481044769
RMSE train: 2.142048	val: 2.208972	test: 2.023910
MAE train: 1.777570	val: 1.858077	test: 1.674104

Epoch: 15
Loss: 3.0843491554260254
RMSE train: 1.897939	val: 1.961911	test: 1.787868
MAE train: 1.564532	val: 1.636495	test: 1.477051

Epoch: 16
Loss: 2.679408013820648
RMSE train: 1.729911	val: 1.770429	test: 1.635404
MAE train: 1.437144	val: 1.487065	test: 1.368488

Epoch: 17
Loss: 2.798523187637329
RMSE train: 1.672903	val: 1.714942	test: 1.580854
MAE train: 1.391364	val: 1.451322	test: 1.332038

Epoch: 18
Loss: 2.313742905855179
RMSE train: 1.468039	val: 1.522422	test: 1.393818
MAE train: 1.174400	val: 1.232723	test: 1.140359

Epoch: 19
Loss: 1.9335797727108002
RMSE train: 1.242356	val: 1.271928	test: 1.183521
MAE train: 0.959752	val: 0.984307	test: 0.930907

Epoch: 20
Loss: 1.6614764630794525
RMSE train: 1.176460	val: 1.192727	test: 1.142185
MAE train: 0.919592	val: 0.931271	test: 0.900871

Epoch: 21
Loss: 1.554477483034134
RMSE train: 1.111957	val: 1.163322	test: 1.090734
MAE train: 0.862371	val: 0.909315	test: 0.865268

Epoch: 22
Loss: 1.2254688143730164
RMSE train: 0.897285	val: 0.970462	test: 0.924975
MAE train: 0.666200	val: 0.747046	test: 0.705800

Epoch: 23
Loss: 1.3808858692646027Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/esol/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/esol/random/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/esol/random/train_prop=0.7/esol_random_4_26-05_11-02-53  ]
[ Using Seed :  4  ]
[ Using device :  cuda:1  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 12.942556858062744
RMSE train: 3.584239	val: 3.659915	test: 3.418519
MAE train: 3.052492	val: 3.162457	test: 2.945436

Epoch: 2
Loss: 13.350836038589478
RMSE train: 3.397879	val: 3.502227	test: 3.188756
MAE train: 2.937286	val: 3.074137	test: 2.774322

Epoch: 3
Loss: 10.053568124771118
RMSE train: 3.259905	val: 3.382815	test: 3.011220
MAE train: 2.825573	val: 2.985342	test: 2.607979

Epoch: 4
Loss: 9.144784212112427
RMSE train: 3.055256	val: 3.159748	test: 2.768580
MAE train: 2.611004	val: 2.758489	test: 2.359733

Epoch: 5
Loss: 7.913028955459595
RMSE train: 2.799097	val: 2.860742	test: 2.529619
MAE train: 2.341393	val: 2.451289	test: 2.083279

Epoch: 6
Loss: 8.08359432220459
RMSE train: 2.565134	val: 2.581763	test: 2.323846
MAE train: 2.125893	val: 2.187856	test: 1.903299

Epoch: 7
Loss: 6.73425829410553
RMSE train: 2.464499	val: 2.474943	test: 2.246605
MAE train: 2.024930	val: 2.069375	test: 1.835838

Epoch: 8
Loss: 6.1752647161483765
RMSE train: 2.395088	val: 2.400552	test: 2.196256
MAE train: 1.945868	val: 1.980153	test: 1.766494

Epoch: 9
Loss: 5.676891446113586
RMSE train: 2.177805	val: 2.154498	test: 1.991391
MAE train: 1.774879	val: 1.782144	test: 1.621723

Epoch: 10
Loss: 5.127571702003479
RMSE train: 2.049937	val: 2.012224	test: 1.902946
MAE train: 1.714713	val: 1.703752	test: 1.595011

Epoch: 11
Loss: 5.054108023643494
RMSE train: 2.114523	val: 2.112665	test: 1.991793
MAE train: 1.770869	val: 1.778911	test: 1.644050

Epoch: 12
Loss: 5.098742127418518
RMSE train: 1.971616	val: 1.994911	test: 1.862948
MAE train: 1.650668	val: 1.675717	test: 1.551489

Epoch: 13
Loss: 3.2300615906715393
RMSE train: 1.648450	val: 1.646090	test: 1.538465
MAE train: 1.345859	val: 1.357644	test: 1.259001

Epoch: 14
Loss: 3.166570544242859
RMSE train: 1.620003	val: 1.612534	test: 1.494671
MAE train: 1.313334	val: 1.332771	test: 1.215127

Epoch: 15
Loss: 3.17531955242157
RMSE train: 1.482506	val: 1.477755	test: 1.361612
MAE train: 1.171371	val: 1.199614	test: 1.077781

Epoch: 16
Loss: 2.6067338585853577
RMSE train: 1.220739	val: 1.217442	test: 1.149020
MAE train: 0.942079	val: 0.960049	test: 0.870136

Epoch: 17
Loss: 2.028031438589096
RMSE train: 1.281959	val: 1.293989	test: 1.243371
MAE train: 1.004302	val: 1.037623	test: 0.976014

Epoch: 18
Loss: 1.954733818769455
RMSE train: 1.353639	val: 1.375165	test: 1.297414
MAE train: 1.075538	val: 1.119018	test: 1.029570

Epoch: 19
Loss: 1.6627951264381409
RMSE train: 1.318383	val: 1.310960	test: 1.230777
MAE train: 1.087067	val: 1.112834	test: 1.022860

Epoch: 20
Loss: 1.8889654874801636
RMSE train: 1.410517	val: 1.440735	test: 1.299339
MAE train: 1.108973	val: 1.164895	test: 1.025164

Epoch: 21
Loss: 1.3293500244617462
RMSE train: 1.347579	val: 1.363791	test: 1.269684
MAE train: 1.023319	val: 1.077858	test: 0.975682

Epoch: 22
Loss: 1.151993989944458
RMSE train: 1.131214	val: 1.116211	test: 1.100768
MAE train: 0.858059	val: 0.885570	test: 0.839452

Epoch: 23
Loss: 1.3058196902275085Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/esol/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/esol/random/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/esol/random/train_prop=0.8/esol_random_6_26-05_11-02-53  ]
[ Using Seed :  6  ]
[ Using device :  cuda:2  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 13.202501058578491
RMSE train: 3.489544	val: 3.532300	test: 3.116980
MAE train: 2.960027	val: 3.020266	test: 2.619591

Epoch: 2
Loss: 11.735097885131836
RMSE train: 3.264218	val: 3.271292	test: 2.873141
MAE train: 2.746524	val: 2.751488	test: 2.387933

Epoch: 3
Loss: 10.362945556640625
RMSE train: 2.990046	val: 2.933454	test: 2.572886
MAE train: 2.451515	val: 2.405479	test: 2.099332

Epoch: 4
Loss: 9.531757831573486
RMSE train: 2.603903	val: 2.516839	test: 2.232675
MAE train: 2.095124	val: 2.023491	test: 1.792563

Epoch: 5
Loss: 8.54301655292511
RMSE train: 2.351661	val: 2.248637	test: 2.065992
MAE train: 1.898291	val: 1.808666	test: 1.666886

Epoch: 6
Loss: 7.74343204498291
RMSE train: 2.409323	val: 2.344899	test: 2.139023
MAE train: 1.958036	val: 1.917920	test: 1.730190

Epoch: 7
Loss: 7.185832262039185
RMSE train: 2.416268	val: 2.390664	test: 2.134516
MAE train: 1.950525	val: 1.931001	test: 1.700933

Epoch: 8
Loss: 6.592986702919006
RMSE train: 2.347075	val: 2.332212	test: 2.067449
MAE train: 1.912039	val: 1.875586	test: 1.673344

Epoch: 9
Loss: 5.893651366233826
RMSE train: 2.339313	val: 2.317825	test: 2.084054
MAE train: 1.950320	val: 1.904926	test: 1.732378

Epoch: 10
Loss: 5.075778007507324
RMSE train: 2.224000	val: 2.185729	test: 2.009786
MAE train: 1.854959	val: 1.807426	test: 1.668648

Epoch: 11
Loss: 4.72543478012085
RMSE train: 2.094645	val: 2.053434	test: 1.893550
MAE train: 1.701100	val: 1.646707	test: 1.529544

Epoch: 12
Loss: 4.086288332939148
RMSE train: 1.989184	val: 1.961038	test: 1.799498
MAE train: 1.642288	val: 1.612411	test: 1.472690

Epoch: 13
Loss: 3.655068039894104
RMSE train: 1.821008	val: 1.790721	test: 1.639441
MAE train: 1.485588	val: 1.439346	test: 1.315317

Epoch: 14
Loss: 3.162684679031372
RMSE train: 1.806990	val: 1.808543	test: 1.593220
MAE train: 1.474891	val: 1.452824	test: 1.278706

Epoch: 15
Loss: 2.646288573741913
RMSE train: 1.688680	val: 1.695990	test: 1.491205
MAE train: 1.378774	val: 1.370559	test: 1.201913

Epoch: 16
Loss: 2.3544223308563232
RMSE train: 1.475196	val: 1.466778	test: 1.339854
MAE train: 1.197152	val: 1.161271	test: 1.080459

Epoch: 17
Loss: 2.0321887731552124
RMSE train: 1.290227	val: 1.322915	test: 1.151753
MAE train: 0.984165	val: 0.970184	test: 0.874804

Epoch: 18
Loss: 1.5650497376918793
RMSE train: 1.062924	val: 1.101119	test: 0.967139
MAE train: 0.781345	val: 0.787458	test: 0.737049

Epoch: 19
Loss: 1.2740478366613388
RMSE train: 0.989347	val: 1.018572	test: 0.922580
MAE train: 0.742852	val: 0.747813	test: 0.722974

Epoch: 20
Loss: 1.2370967268943787
RMSE train: 0.981863	val: 1.022246	test: 0.911967
MAE train: 0.739233	val: 0.756846	test: 0.707750

Epoch: 21
Loss: 1.013808399438858
RMSE train: 1.001786	val: 1.045297	test: 0.938757
MAE train: 0.764865	val: 0.806045	test: 0.727500

Epoch: 22
Loss: 0.8623237013816833
RMSE train: 0.995195	val: 1.001385	test: 0.940785
MAE train: 0.758364	val: 0.736725	test: 0.736468

Epoch: 23
Loss: 0.8436644822359085Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/esol/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/esol/random/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/esol/random/train_prop=0.8/esol_random_4_26-05_11-02-53  ]
[ Using Seed :  4  ]
[ Using device :  cuda:2  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 12.964437484741211
RMSE train: 3.614733	val: 3.628593	test: 3.339899
MAE train: 3.094289	val: 3.170627	test: 2.842563

Epoch: 2
Loss: 11.52287220954895
RMSE train: 3.398724	val: 3.379846	test: 3.097202
MAE train: 2.949701	val: 2.954019	test: 2.682011

Epoch: 3
Loss: 10.081639289855957
RMSE train: 3.302906	val: 3.235332	test: 2.962939
MAE train: 2.859021	val: 2.817925	test: 2.563979

Epoch: 4
Loss: 9.059993505477905
RMSE train: 3.018272	val: 2.950736	test: 2.641395
MAE train: 2.555440	val: 2.507396	test: 2.206796

Epoch: 5
Loss: 8.431079268455505
RMSE train: 2.731782	val: 2.687392	test: 2.383771
MAE train: 2.278616	val: 2.236060	test: 1.973655

Epoch: 6
Loss: 7.707471489906311
RMSE train: 2.596725	val: 2.567090	test: 2.288606
MAE train: 2.158475	val: 2.126838	test: 1.893365

Epoch: 7
Loss: 6.942959427833557
RMSE train: 2.537693	val: 2.535149	test: 2.249681
MAE train: 2.100644	val: 2.084046	test: 1.848718

Epoch: 8
Loss: 6.392381548881531
RMSE train: 2.481625	val: 2.478290	test: 2.207772
MAE train: 2.055389	val: 2.032685	test: 1.811050

Epoch: 9
Loss: 5.811639428138733
RMSE train: 2.379644	val: 2.355776	test: 2.123135
MAE train: 1.985531	val: 1.937200	test: 1.754178

Epoch: 10
Loss: 5.190823793411255
RMSE train: 2.309403	val: 2.257352	test: 2.074965
MAE train: 1.939255	val: 1.863426	test: 1.729802

Epoch: 11
Loss: 4.423188805580139
RMSE train: 2.267432	val: 2.240969	test: 2.026153
MAE train: 1.884349	val: 1.835304	test: 1.676623

Epoch: 12
Loss: 4.042845368385315
RMSE train: 1.999545	val: 1.970285	test: 1.797080
MAE train: 1.655546	val: 1.600911	test: 1.493701

Epoch: 13
Loss: 3.661766469478607
RMSE train: 1.806953	val: 1.748492	test: 1.631109
MAE train: 1.481364	val: 1.395639	test: 1.346505

Epoch: 14
Loss: 3.051830470561981
RMSE train: 1.820634	val: 1.794840	test: 1.639001
MAE train: 1.495057	val: 1.447477	test: 1.349418

Epoch: 15
Loss: 2.5773907899856567
RMSE train: 1.649684	val: 1.611066	test: 1.507801
MAE train: 1.358703	val: 1.291123	test: 1.260795

Epoch: 16
Loss: 2.1823141872882843
RMSE train: 1.565341	val: 1.506025	test: 1.448042
MAE train: 1.296632	val: 1.202013	test: 1.213960

Epoch: 17
Loss: 1.7890493869781494
RMSE train: 1.580960	val: 1.597302	test: 1.454769
MAE train: 1.298065	val: 1.286730	test: 1.214533

Epoch: 18
Loss: 1.6175734996795654
RMSE train: 1.426927	val: 1.473076	test: 1.305302
MAE train: 1.152843	val: 1.172151	test: 1.075125

Epoch: 19
Loss: 1.3697790503501892
RMSE train: 1.261978	val: 1.284816	test: 1.155492
MAE train: 0.995921	val: 0.989910	test: 0.931429

Epoch: 20
Loss: 1.141933113336563
RMSE train: 1.147455	val: 1.226548	test: 1.072743
MAE train: 0.913157	val: 0.959292	test: 0.874155

Epoch: 21
Loss: 1.029367908835411
RMSE train: 1.129609	val: 1.234181	test: 1.058082
MAE train: 0.885291	val: 0.962781	test: 0.841696

Epoch: 22
Loss: 0.8263144940137863
RMSE train: 0.911109	val: 0.996445	test: 0.873545
MAE train: 0.689495	val: 0.715467	test: 0.676191

Epoch: 23
Loss: 0.8069920539855957
RMSE train: 1.602546	val: 1.604781	test: 1.530300
MAE train: 1.324509	val: 1.311888	test: 1.236002

Epoch: 24
Loss: 1.4510873953501384
RMSE train: 1.496398	val: 1.500015	test: 1.443729
MAE train: 1.234000	val: 1.216808	test: 1.163125

Epoch: 25
Loss: 1.4095005989074707
RMSE train: 1.498629	val: 1.500968	test: 1.457505
MAE train: 1.225478	val: 1.205122	test: 1.174034

Epoch: 26
Loss: 1.1838401556015015
RMSE train: 1.317245	val: 1.344084	test: 1.281668
MAE train: 1.098594	val: 1.102431	test: 1.047544

Epoch: 27
Loss: 1.072546660900116
RMSE train: 1.248186	val: 1.284936	test: 1.213803
MAE train: 1.016350	val: 1.037902	test: 0.977581

Epoch: 28
Loss: 0.8620922168095907
RMSE train: 1.118273	val: 1.157177	test: 1.105434
MAE train: 0.883240	val: 0.889418	test: 0.865014

Epoch: 29
Loss: 0.8927419185638428
RMSE train: 0.822951	val: 0.884768	test: 0.883255
MAE train: 0.622819	val: 0.650640	test: 0.665027

Epoch: 30
Loss: 0.7692679564158121
RMSE train: 0.893258	val: 0.930513	test: 0.930591
MAE train: 0.673267	val: 0.682264	test: 0.710733

Epoch: 31
Loss: 0.6934214035669962
RMSE train: 1.053166	val: 1.099464	test: 1.056891
MAE train: 0.798349	val: 0.829527	test: 0.815986

Epoch: 32
Loss: 0.6950689355532328
RMSE train: 0.954041	val: 1.024731	test: 0.975060
MAE train: 0.732605	val: 0.766355	test: 0.755751

Epoch: 33
Loss: 0.6229530572891235
RMSE train: 0.898800	val: 0.966890	test: 0.963060
MAE train: 0.680895	val: 0.709861	test: 0.740333

Epoch: 34
Loss: 0.6436295509338379
RMSE train: 0.732777	val: 0.843866	test: 0.832366
MAE train: 0.552880	val: 0.600548	test: 0.630669

Epoch: 35
Loss: 0.6344538728396097
RMSE train: 0.671453	val: 0.814900	test: 0.790773
MAE train: 0.491712	val: 0.579340	test: 0.577038

Epoch: 36
Loss: 0.6158925096193949
RMSE train: 0.737701	val: 0.855470	test: 0.875976
MAE train: 0.543104	val: 0.613147	test: 0.661334

Epoch: 37
Loss: 0.5762580831845602
RMSE train: 0.685439	val: 0.809878	test: 0.813667
MAE train: 0.502503	val: 0.578053	test: 0.617391

Epoch: 38
Loss: 0.5719252725442251
RMSE train: 0.703862	val: 0.822723	test: 0.806360
MAE train: 0.533970	val: 0.603663	test: 0.598200

Epoch: 39
Loss: 0.6233132680257162
RMSE train: 0.903424	val: 0.953211	test: 0.984975
MAE train: 0.679218	val: 0.696674	test: 0.760324

Epoch: 40
Loss: 0.6388542056083679
RMSE train: 0.739280	val: 0.838292	test: 0.854840
MAE train: 0.559990	val: 0.608474	test: 0.651626

Epoch: 41
Loss: 0.5651870469252268
RMSE train: 0.616992	val: 0.795816	test: 0.758550
MAE train: 0.467216	val: 0.586621	test: 0.556419

Epoch: 42
Loss: 0.5563568671544393
RMSE train: 0.674915	val: 0.824220	test: 0.793697
MAE train: 0.507786	val: 0.583393	test: 0.601809

Epoch: 43
Loss: 0.5533886551856995
RMSE train: 0.637992	val: 0.793802	test: 0.787758
MAE train: 0.474129	val: 0.560627	test: 0.597942

Epoch: 44
Loss: 0.4994972050189972
RMSE train: 0.560500	val: 0.744234	test: 0.728959
MAE train: 0.414526	val: 0.518278	test: 0.553257

Epoch: 45
Loss: 0.5836560527483622
RMSE train: 0.647185	val: 0.810327	test: 0.764334
MAE train: 0.477407	val: 0.578699	test: 0.576170

Epoch: 46
Loss: 0.5228910446166992
RMSE train: 0.711870	val: 0.860432	test: 0.803393
MAE train: 0.532231	val: 0.624096	test: 0.607514

Epoch: 47
Loss: 0.5130021373430887
RMSE train: 0.697171	val: 0.822015	test: 0.791331
MAE train: 0.528629	val: 0.598480	test: 0.596019

Epoch: 48
Loss: 0.534901758035024
RMSE train: 0.726723	val: 0.835987	test: 0.800536
MAE train: 0.559579	val: 0.618927	test: 0.603096

Epoch: 49
Loss: 0.4330326020717621
RMSE train: 0.774317	val: 0.885288	test: 0.842308
MAE train: 0.595818	val: 0.646177	test: 0.633961

Epoch: 50
Loss: 0.49964133898417157
RMSE train: 0.768467	val: 0.897891	test: 0.869727
MAE train: 0.574985	val: 0.645229	test: 0.648523

Epoch: 51
Loss: 0.5149580538272858
RMSE train: 0.607485	val: 0.788554	test: 0.763964
MAE train: 0.447439	val: 0.545835	test: 0.562217

Epoch: 52
Loss: 0.4922657211621602
RMSE train: 0.592451	val: 0.775050	test: 0.760955
MAE train: 0.434951	val: 0.540426	test: 0.549635

Epoch: 53
Loss: 0.5024954179922739
RMSE train: 0.607296	val: 0.772195	test: 0.758466
MAE train: 0.443090	val: 0.535063	test: 0.555372

Epoch: 54
Loss: 0.5270209312438965
RMSE train: 0.653503	val: 0.801265	test: 0.785840
MAE train: 0.482258	val: 0.565940	test: 0.581546

Epoch: 55
Loss: 0.5096751153469086
RMSE train: 0.603587	val: 0.767200	test: 0.773237
MAE train: 0.446288	val: 0.536887	test: 0.581815

Epoch: 56
Loss: 0.5205116073290507
RMSE train: 0.548906	val: 0.737873	test: 0.752419
MAE train: 0.405376	val: 0.516243	test: 0.563096

Epoch: 57
Loss: 0.43823549151420593
RMSE train: 0.630882	val: 0.800892	test: 0.781344
MAE train: 0.470013	val: 0.568151	test: 0.579347

Epoch: 58
Loss: 0.47553834319114685
RMSE train: 0.780649	val: 0.914392	test: 0.879045
MAE train: 0.581136	val: 0.663018	test: 0.665541

Epoch: 59
Loss: 0.47951390345891315
RMSE train: 0.761340	val: 0.903960	test: 0.859323
MAE train: 0.567704	val: 0.657674	test: 0.652749

Epoch: 60
Loss: 0.4595194657643636
RMSE train: 0.767057	val: 0.899952	test: 0.861938
MAE train: 0.574749	val: 0.655780	test: 0.653738

Epoch: 61
Loss: 0.43603528539339703
RMSE train: 0.759394	val: 0.896562	test: 0.867953
MAE train: 0.569840	val: 0.647776	test: 0.655809

Epoch: 62
Loss: 0.42439571022987366
RMSE train: 0.647806	val: 0.816886	test: 0.801618
MAE train: 0.475832	val: 0.571378	test: 0.598150

Epoch: 63
Loss: 0.43499547243118286
RMSE train: 0.545149	val: 0.758089	test: 0.748464
MAE train: 0.404016	val: 0.530781	test: 0.549712

Epoch: 64
Loss: 0.41500115394592285
RMSE train: 0.505895	val: 0.734620	test: 0.738835
MAE train: 0.378221	val: 0.516456	test: 0.539082

Epoch: 65
Loss: 0.430111567179362
RMSE train: 0.575431	val: 0.782940	test: 0.771424
MAE train: 0.425434	val: 0.553229	test: 0.563687

Epoch: 66
Loss: 0.41152559717496234
RMSE train: 0.575145	val: 0.783981	test: 0.747865
MAE train: 0.425287	val: 0.558852	test: 0.553105

Epoch: 67
Loss: 0.4106612801551819
RMSE train: 0.595579	val: 0.790072	test: 0.769823
MAE train: 0.445591	val: 0.557077	test: 0.572762

Epoch: 68
Loss: 0.41275834043820697
RMSE train: 0.479868	val: 0.721228	test: 0.712097
MAE train: 0.358015	val: 0.501290	test: 0.519093

Epoch: 69
Loss: 0.4091125826040904
RMSE train: 0.442641	val: 0.705841	test: 0.702065
MAE train: 0.331325	val: 0.496954	test: 0.509557

Epoch: 70
Loss: 0.438041885693868
RMSE train: 0.539207	val: 0.766127	test: 0.751230
MAE train: 0.401152	val: 0.535022	test: 0.553237

Epoch: 71
Loss: 0.43530821800231934
RMSE train: 0.562788	val: 0.783236	test: 0.748307
MAE train: 0.414363	val: 0.542897	test: 0.552211

Epoch: 72
Loss: 0.4038309355576833
RMSE train: 0.593076	val: 0.811277	test: 0.736732
MAE train: 0.442004	val: 0.584559	test: 0.537845

Epoch: 73
Loss: 0.3805541495482127
RMSE train: 0.626810	val: 0.837095	test: 0.774657
MAE train: 0.466013	val: 0.597841	test: 0.572268

Epoch: 74
Loss: 0.3997852106889089
RMSE train: 0.596656	val: 0.812773	test: 0.786482
MAE train: 0.440392	val: 0.572728	test: 0.581129

Epoch: 75
Loss: 0.39623041947682697
RMSE train: 0.529085	val: 0.759212	test: 0.738018
MAE train: 0.391734	val: 0.533622	test: 0.544079

Epoch: 76
Loss: 0.3733166952927907
RMSE train: 0.532437	val: 0.756745	test: 0.722009
MAE train: 0.395672	val: 0.534015	test: 0.531215

Epoch: 77
Loss: 0.35507579644521076
RMSE train: 0.669555	val: 0.844592	test: 0.812123
MAE train: 0.485072	val: 0.603866	test: 0.602210

Epoch: 78
Loss: 0.3990369339783986
RMSE train: 0.634041	val: 0.818834	test: 0.788191
MAE train: 0.457011	val: 0.580736	test: 0.577662

Epoch: 79
Loss: 0.3480823536713918
RMSE train: 0.545104	val: 0.772947	test: 0.720896
MAE train: 0.405110	val: 0.545952	test: 0.528569

Epoch: 80
Loss: 0.37676748633384705
RMSE train: 0.562200	val: 0.789861	test: 0.753913
MAE train: 0.412673	val: 0.553659	test: 0.553681

Epoch: 81
Loss: 0.4100116590658824
RMSE train: 0.580909	val: 0.802334	test: 0.772965
MAE train: 0.422530	val: 0.559664	test: 0.570522

Epoch: 82
Loss: 0.4008096158504486
RMSE train: 0.526267	val: 0.764940	test: 0.711244
MAE train: 0.392350	val: 0.539333	test: 0.522873

Epoch: 83
Loss: 0.38470720251401264
RMSE train: 0.575414	val: 0.797706	test: 0.730961
MAE train: 0.427882	val: 0.559456	test: 0.537049
RMSE train: 1.331305	val: 1.339108	test: 1.289132
MAE train: 1.067503	val: 1.047688	test: 1.034544

Epoch: 24
Loss: 1.6223506927490234
RMSE train: 1.199599	val: 1.250364	test: 1.153690
MAE train: 0.939483	val: 0.983102	test: 0.880876

Epoch: 25
Loss: 1.4573256572087605
RMSE train: 1.118028	val: 1.146409	test: 1.099160
MAE train: 0.883566	val: 0.901249	test: 0.856713

Epoch: 26
Loss: 1.236194332440694
RMSE train: 1.187349	val: 1.181703	test: 1.187338
MAE train: 0.944179	val: 0.919521	test: 0.937113

Epoch: 27
Loss: 1.0904817978541057
RMSE train: 1.047458	val: 1.093893	test: 1.056513
MAE train: 0.822765	val: 0.846601	test: 0.822579

Epoch: 28
Loss: 0.9824987649917603
RMSE train: 0.891778	val: 0.996860	test: 0.920705
MAE train: 0.669169	val: 0.743948	test: 0.696439

Epoch: 29
Loss: 0.8915390570958456
RMSE train: 0.848226	val: 0.926327	test: 0.913920
MAE train: 0.647587	val: 0.673674	test: 0.696920

Epoch: 30
Loss: 0.7671808203061422
RMSE train: 0.788155	val: 0.863638	test: 0.870091
MAE train: 0.593020	val: 0.628930	test: 0.652871

Epoch: 31
Loss: 0.730065643787384
RMSE train: 0.824193	val: 0.894854	test: 0.870680
MAE train: 0.635232	val: 0.668101	test: 0.673725

Epoch: 32
Loss: 0.7766008377075195
RMSE train: 0.824539	val: 0.905216	test: 0.859823
MAE train: 0.620185	val: 0.660191	test: 0.648365

Epoch: 33
Loss: 0.7367133100827535
RMSE train: 0.747120	val: 0.860827	test: 0.816915
MAE train: 0.555469	val: 0.625041	test: 0.612072

Epoch: 34
Loss: 0.633340855439504
RMSE train: 0.635721	val: 0.765011	test: 0.768722
MAE train: 0.477455	val: 0.558577	test: 0.578267

Epoch: 35
Loss: 0.65712837378184
RMSE train: 0.650642	val: 0.770972	test: 0.798794
MAE train: 0.494903	val: 0.572253	test: 0.606014

Epoch: 36
Loss: 0.6203356683254242
RMSE train: 0.604661	val: 0.798475	test: 0.769775
MAE train: 0.446694	val: 0.592357	test: 0.577022

Epoch: 37
Loss: 0.6248305042584738
RMSE train: 0.587508	val: 0.794817	test: 0.785217
MAE train: 0.450935	val: 0.604015	test: 0.596748

Epoch: 38
Loss: 0.6045875052611033
RMSE train: 0.542982	val: 0.725052	test: 0.770203
MAE train: 0.406249	val: 0.530908	test: 0.581719

Epoch: 39
Loss: 0.610625147819519
RMSE train: 0.583084	val: 0.733181	test: 0.783714
MAE train: 0.432858	val: 0.527280	test: 0.588597

Epoch: 40
Loss: 0.5660788218180338
RMSE train: 0.564714	val: 0.742776	test: 0.754457
MAE train: 0.413992	val: 0.543072	test: 0.552534

Epoch: 41
Loss: 0.5981754660606384
RMSE train: 0.715478	val: 0.824509	test: 0.841636
MAE train: 0.537624	val: 0.591667	test: 0.636384

Epoch: 42
Loss: 0.5705544551213583
RMSE train: 0.752782	val: 0.841453	test: 0.851393
MAE train: 0.561097	val: 0.598276	test: 0.649456

Epoch: 43
Loss: 0.5279997785886129
RMSE train: 0.636131	val: 0.775737	test: 0.744813
MAE train: 0.471765	val: 0.561753	test: 0.551376

Epoch: 44
Loss: 0.5473322868347168
RMSE train: 0.595151	val: 0.748268	test: 0.716023
MAE train: 0.439007	val: 0.535202	test: 0.525533

Epoch: 45
Loss: 0.5296598176161448
RMSE train: 0.701258	val: 0.813232	test: 0.816327
MAE train: 0.525089	val: 0.589603	test: 0.618274

Epoch: 46
Loss: 0.5147065122922262
RMSE train: 0.650468	val: 0.797080	test: 0.791392
MAE train: 0.483910	val: 0.576580	test: 0.584542

Epoch: 47
Loss: 0.5099402268727621
RMSE train: 0.558228	val: 0.766746	test: 0.738896
MAE train: 0.411645	val: 0.563113	test: 0.542521

Epoch: 48
Loss: 0.5136278967062632
RMSE train: 0.528598	val: 0.727241	test: 0.728949
MAE train: 0.388530	val: 0.537953	test: 0.530860

Epoch: 49
Loss: 0.5610773762067159
RMSE train: 0.568325	val: 0.739030	test: 0.736227
MAE train: 0.418996	val: 0.535644	test: 0.544243

Epoch: 50
Loss: 0.49874579906463623
RMSE train: 0.550922	val: 0.723970	test: 0.727716
MAE train: 0.410717	val: 0.517614	test: 0.531625

Epoch: 51
Loss: 0.4962799648443858
RMSE train: 0.539606	val: 0.713411	test: 0.732692
MAE train: 0.403913	val: 0.505870	test: 0.534226

Epoch: 52
Loss: 0.5019256969292959
RMSE train: 0.547758	val: 0.727279	test: 0.734535
MAE train: 0.410259	val: 0.514599	test: 0.539476

Epoch: 53
Loss: 0.5055208106835684
RMSE train: 0.554495	val: 0.739740	test: 0.735911
MAE train: 0.413331	val: 0.522644	test: 0.544308

Epoch: 54
Loss: 0.49787430961926776
RMSE train: 0.523433	val: 0.747328	test: 0.729587
MAE train: 0.393272	val: 0.541699	test: 0.546567

Epoch: 55
Loss: 0.46095622579256695
RMSE train: 0.501207	val: 0.725598	test: 0.734490
MAE train: 0.374765	val: 0.524379	test: 0.541399

Epoch: 56
Loss: 0.4869917531808217
RMSE train: 0.569740	val: 0.766335	test: 0.802463
MAE train: 0.432145	val: 0.558954	test: 0.605125

Epoch: 57
Loss: 0.4458425045013428
RMSE train: 0.543029	val: 0.760650	test: 0.753593
MAE train: 0.402523	val: 0.542719	test: 0.560752

Epoch: 58
Loss: 0.42975818117459613
RMSE train: 0.638871	val: 0.830053	test: 0.796888
MAE train: 0.473232	val: 0.590825	test: 0.592647

Epoch: 59
Loss: 0.4410417675971985
RMSE train: 0.645639	val: 0.822752	test: 0.797319
MAE train: 0.478470	val: 0.579854	test: 0.601690

Epoch: 60
Loss: 0.44498729705810547
RMSE train: 0.543654	val: 0.750208	test: 0.737333
MAE train: 0.400517	val: 0.526500	test: 0.538247

Epoch: 61
Loss: 0.44260520736376446
RMSE train: 0.511291	val: 0.730140	test: 0.731535
MAE train: 0.380430	val: 0.516664	test: 0.526094

Epoch: 62
Loss: 0.43160802125930786
RMSE train: 0.579364	val: 0.768321	test: 0.774856
MAE train: 0.437358	val: 0.543624	test: 0.574627

Epoch: 63
Loss: 0.4678972562154134
RMSE train: 0.556550	val: 0.751376	test: 0.752546
MAE train: 0.413627	val: 0.529868	test: 0.550502

Epoch: 64
Loss: 0.3913923700650533
RMSE train: 0.524553	val: 0.774689	test: 0.753012
MAE train: 0.391487	val: 0.561443	test: 0.561716

Epoch: 65
Loss: 0.46514304478963214
RMSE train: 0.454344	val: 0.689787	test: 0.709255
MAE train: 0.332964	val: 0.501237	test: 0.506693

Epoch: 66
Loss: 0.464992215236028
RMSE train: 0.489290	val: 0.705330	test: 0.738100
MAE train: 0.362387	val: 0.508650	test: 0.539951

Epoch: 67
Loss: 0.5160796642303467
RMSE train: 0.461012	val: 0.717978	test: 0.707624
MAE train: 0.340269	val: 0.513905	test: 0.522809

Epoch: 68
Loss: 0.41488897800445557
RMSE train: 0.507596	val: 0.771133	test: 0.723186
MAE train: 0.380069	val: 0.558483	test: 0.533673

Epoch: 69
Loss: 0.4716002643108368
RMSE train: 0.502108	val: 0.741912	test: 0.737086
MAE train: 0.378788	val: 0.536089	test: 0.540865

Epoch: 70
Loss: 0.4399730662504832
RMSE train: 0.449936	val: 0.721703	test: 0.713407
MAE train: 0.336680	val: 0.522515	test: 0.519076

Epoch: 71
Loss: 0.37075887123743695
RMSE train: 0.481662	val: 0.734202	test: 0.714001
MAE train: 0.355245	val: 0.528789	test: 0.516352

Epoch: 72
Loss: 0.42990225553512573
RMSE train: 0.545354	val: 0.749776	test: 0.737110
MAE train: 0.401842	val: 0.530159	test: 0.536169

Epoch: 73
Loss: 0.4152431885401408
RMSE train: 0.571594	val: 0.763919	test: 0.757268
MAE train: 0.420746	val: 0.547629	test: 0.558578

Epoch: 74
Loss: 0.3738557696342468
RMSE train: 0.473825	val: 0.736817	test: 0.707881
MAE train: 0.349805	val: 0.532577	test: 0.520324

Epoch: 75
Loss: 0.3984011809031169
RMSE train: 0.437361	val: 0.721556	test: 0.691181
MAE train: 0.322499	val: 0.512587	test: 0.499394

Epoch: 76
Loss: 0.3732295135656993
RMSE train: 0.457515	val: 0.725727	test: 0.702704
MAE train: 0.336847	val: 0.510097	test: 0.511889

Epoch: 77
Loss: 0.4564058283964793
RMSE train: 0.443940	val: 0.725789	test: 0.699792
MAE train: 0.335841	val: 0.523207	test: 0.511132

Epoch: 78
Loss: 0.38791022698084515
RMSE train: 0.484436	val: 0.756097	test: 0.735988
MAE train: 0.369018	val: 0.553721	test: 0.552069

Epoch: 79
Loss: 0.3852376143137614
RMSE train: 0.486128	val: 0.750394	test: 0.741657
MAE train: 0.366990	val: 0.541983	test: 0.541858

Epoch: 80
Loss: 0.39825281500816345
RMSE train: 0.432733	val: 0.708423	test: 0.702172
MAE train: 0.322952	val: 0.498793	test: 0.501583

Epoch: 81
Loss: 0.43389807144800824
RMSE train: 0.416285	val: 0.704828	test: 0.687282
MAE train: 0.300236	val: 0.494997	test: 0.489008

Epoch: 82
Loss: 0.3791228036085765
RMSE train: 0.473574	val: 0.732885	test: 0.690848
MAE train: 0.352380	val: 0.530119	test: 0.500485

Epoch: 83
Loss: 0.35094647606213886
RMSE train: 0.518306	val: 0.747326	test: 0.719738
MAE train: 0.385377	val: 0.538718	test: 0.530598
RMSE train: 1.592080	val: 1.603420	test: 1.543917
MAE train: 1.305515	val: 1.304680	test: 1.237904

Epoch: 24
Loss: 1.8158221244812012
RMSE train: 1.448980	val: 1.479507	test: 1.409376
MAE train: 1.165973	val: 1.182854	test: 1.104395

Epoch: 25
Loss: 1.580803632736206
RMSE train: 1.325771	val: 1.345373	test: 1.297710
MAE train: 1.081279	val: 1.088560	test: 1.042546

Epoch: 26
Loss: 1.4033677379290264
RMSE train: 1.232922	val: 1.252767	test: 1.215784
MAE train: 1.003449	val: 0.999170	test: 0.979574

Epoch: 27
Loss: 1.1353659431139629
RMSE train: 1.110229	val: 1.153894	test: 1.108334
MAE train: 0.884210	val: 0.894384	test: 0.865120

Epoch: 28
Loss: 1.077731768290202
RMSE train: 1.105048	val: 1.149704	test: 1.108458
MAE train: 0.876085	val: 0.873140	test: 0.874070

Epoch: 29
Loss: 0.9654249350229899
RMSE train: 1.019465	val: 1.083878	test: 1.037511
MAE train: 0.800956	val: 0.829649	test: 0.812199

Epoch: 30
Loss: 0.9178708791732788
RMSE train: 1.059500	val: 1.119913	test: 1.072343
MAE train: 0.840439	val: 0.864592	test: 0.846171

Epoch: 31
Loss: 0.7921204169591268
RMSE train: 1.032037	val: 1.104993	test: 1.044543
MAE train: 0.794704	val: 0.829373	test: 0.807615

Epoch: 32
Loss: 0.7081398765246073
RMSE train: 0.815132	val: 0.921732	test: 0.893824
MAE train: 0.605280	val: 0.665651	test: 0.670886

Epoch: 33
Loss: 0.7303640445073446
RMSE train: 0.646926	val: 0.777150	test: 0.783450
MAE train: 0.485132	val: 0.556961	test: 0.574664

Epoch: 34
Loss: 0.6635759671529134
RMSE train: 0.848555	val: 0.936270	test: 0.922164
MAE train: 0.644599	val: 0.694928	test: 0.711505

Epoch: 35
Loss: 0.6376021107037863
RMSE train: 0.677767	val: 0.819811	test: 0.778980
MAE train: 0.500751	val: 0.576093	test: 0.584225

Epoch: 36
Loss: 0.6292369167009989
RMSE train: 0.561146	val: 0.729205	test: 0.722432
MAE train: 0.415467	val: 0.501901	test: 0.536233

Epoch: 37
Loss: 0.6140134731928507
RMSE train: 0.589518	val: 0.749194	test: 0.765035
MAE train: 0.437715	val: 0.524914	test: 0.568359

Epoch: 38
Loss: 0.6192677815755209
RMSE train: 0.562193	val: 0.751475	test: 0.742430
MAE train: 0.419098	val: 0.537234	test: 0.534574

Epoch: 39
Loss: 0.5590631365776062
RMSE train: 0.558833	val: 0.739809	test: 0.752588
MAE train: 0.417349	val: 0.522346	test: 0.544138

Epoch: 40
Loss: 0.5809001922607422
RMSE train: 0.550424	val: 0.731127	test: 0.730231
MAE train: 0.408425	val: 0.498096	test: 0.528227

Epoch: 41
Loss: 0.6081718802452087
RMSE train: 0.599057	val: 0.777524	test: 0.739529
MAE train: 0.442194	val: 0.552817	test: 0.538485

Epoch: 42
Loss: 0.545637865861257
RMSE train: 0.567689	val: 0.770020	test: 0.743403
MAE train: 0.426916	val: 0.568878	test: 0.543206

Epoch: 43
Loss: 0.5441646774609884
RMSE train: 0.518693	val: 0.732262	test: 0.737510
MAE train: 0.386477	val: 0.523144	test: 0.533693

Epoch: 44
Loss: 0.5136941174666086
RMSE train: 0.538893	val: 0.739002	test: 0.754488
MAE train: 0.401249	val: 0.520853	test: 0.558208

Epoch: 45
Loss: 0.5587069392204285
RMSE train: 0.685772	val: 0.851334	test: 0.819150
MAE train: 0.511399	val: 0.604895	test: 0.610950

Epoch: 46
Loss: 0.5336489280064901
RMSE train: 0.771359	val: 0.918701	test: 0.864733
MAE train: 0.574311	val: 0.664135	test: 0.649411

Epoch: 47
Loss: 0.49684550364812213
RMSE train: 0.677162	val: 0.838962	test: 0.805176
MAE train: 0.501598	val: 0.595266	test: 0.589626

Epoch: 48
Loss: 0.5253202021121979
RMSE train: 0.642917	val: 0.811769	test: 0.789313
MAE train: 0.479655	val: 0.582039	test: 0.583745

Epoch: 49
Loss: 0.4495505193869273
RMSE train: 0.697816	val: 0.852191	test: 0.828436
MAE train: 0.526026	val: 0.616388	test: 0.613591

Epoch: 50
Loss: 0.4565865794817607
RMSE train: 0.696607	val: 0.850328	test: 0.812375
MAE train: 0.528220	val: 0.617178	test: 0.608703

Epoch: 51
Loss: 0.5253691176573435
RMSE train: 0.638179	val: 0.808166	test: 0.768411
MAE train: 0.478769	val: 0.575636	test: 0.574096

Epoch: 52
Loss: 0.48868778347969055
RMSE train: 0.666593	val: 0.841955	test: 0.791636
MAE train: 0.496705	val: 0.602551	test: 0.589372

Epoch: 53
Loss: 0.44564110040664673
RMSE train: 0.693692	val: 0.874890	test: 0.808542
MAE train: 0.510784	val: 0.620935	test: 0.589701

Epoch: 54
Loss: 0.4375901520252228
RMSE train: 0.659155	val: 0.840625	test: 0.792233
MAE train: 0.485498	val: 0.600787	test: 0.581327

Epoch: 55
Loss: 0.4789365331331889
RMSE train: 0.591725	val: 0.795314	test: 0.761637
MAE train: 0.435969	val: 0.562024	test: 0.565790

Epoch: 56
Loss: 0.47432557741800946
RMSE train: 0.513704	val: 0.752617	test: 0.709941
MAE train: 0.379722	val: 0.534090	test: 0.522625

Epoch: 57
Loss: 0.5003313223520914
RMSE train: 0.558052	val: 0.766467	test: 0.731533
MAE train: 0.404043	val: 0.539846	test: 0.531837

Epoch: 58
Loss: 0.4493405918280284
RMSE train: 0.513095	val: 0.743487	test: 0.729047
MAE train: 0.373634	val: 0.520556	test: 0.529081

Epoch: 59
Loss: 0.4986861050128937
RMSE train: 0.511651	val: 0.739674	test: 0.731170
MAE train: 0.379556	val: 0.512556	test: 0.544342

Epoch: 60
Loss: 0.46551064650217694
RMSE train: 0.553689	val: 0.767600	test: 0.742939
MAE train: 0.410291	val: 0.532029	test: 0.548522

Epoch: 61
Loss: 0.4571067988872528
RMSE train: 0.559292	val: 0.774742	test: 0.724464
MAE train: 0.414576	val: 0.556081	test: 0.526316

Epoch: 62
Loss: 0.4243765076001485
RMSE train: 0.482855	val: 0.726189	test: 0.695013
MAE train: 0.354813	val: 0.524259	test: 0.506848

Epoch: 63
Loss: 0.3926298916339874
RMSE train: 0.462756	val: 0.707782	test: 0.689376
MAE train: 0.341081	val: 0.506934	test: 0.500447

Epoch: 64
Loss: 0.497689425945282
RMSE train: 0.472410	val: 0.724223	test: 0.696119
MAE train: 0.346693	val: 0.512637	test: 0.501468

Epoch: 65
Loss: 0.4132870137691498
RMSE train: 0.526605	val: 0.783223	test: 0.714634
MAE train: 0.390407	val: 0.561315	test: 0.527992

Epoch: 66
Loss: 0.538231243689855
RMSE train: 0.635568	val: 0.833367	test: 0.785419
MAE train: 0.474095	val: 0.604924	test: 0.586049

Epoch: 67
Loss: 0.4473549723625183
RMSE train: 0.525249	val: 0.753378	test: 0.734491
MAE train: 0.385517	val: 0.540315	test: 0.541503

Epoch: 68
Loss: 0.4698425233364105
RMSE train: 0.456535	val: 0.720220	test: 0.691861
MAE train: 0.342070	val: 0.517551	test: 0.499367

Epoch: 69
Loss: 0.4106403589248657
RMSE train: 0.533695	val: 0.763403	test: 0.714816
MAE train: 0.392161	val: 0.543741	test: 0.513512

Epoch: 70
Loss: 0.3817179600397746
RMSE train: 0.547322	val: 0.769973	test: 0.733742
MAE train: 0.404236	val: 0.544474	test: 0.531547

Epoch: 71
Loss: 0.4151267409324646
RMSE train: 0.441005	val: 0.714998	test: 0.692297
MAE train: 0.323973	val: 0.495164	test: 0.506076

Epoch: 72
Loss: 0.4112055202325185
RMSE train: 0.490038	val: 0.729137	test: 0.710312
MAE train: 0.359295	val: 0.512517	test: 0.529493

Epoch: 73
Loss: 0.4162707527478536
RMSE train: 0.497032	val: 0.742224	test: 0.696711
MAE train: 0.366766	val: 0.522808	test: 0.506922

Epoch: 74
Loss: 0.37569526831309
RMSE train: 0.516072	val: 0.750845	test: 0.709391
MAE train: 0.379855	val: 0.530614	test: 0.518718

Epoch: 75
Loss: 0.4021523098150889
RMSE train: 0.470580	val: 0.723394	test: 0.699242
MAE train: 0.342065	val: 0.503457	test: 0.513059

Epoch: 76
Loss: 0.41718777020772296
RMSE train: 0.494299	val: 0.736580	test: 0.711515
MAE train: 0.362765	val: 0.510891	test: 0.517316

Epoch: 77
Loss: 0.4393167495727539
RMSE train: 0.619187	val: 0.829283	test: 0.807894
MAE train: 0.466540	val: 0.590280	test: 0.604168

Epoch: 78
Loss: 0.3371712664763133
RMSE train: 0.536738	val: 0.776197	test: 0.756246
MAE train: 0.400763	val: 0.543849	test: 0.558979

Epoch: 79
Loss: 0.3665110071500142
RMSE train: 0.524823	val: 0.763190	test: 0.749340
MAE train: 0.387796	val: 0.535192	test: 0.554901

Epoch: 80
Loss: 0.38986729582150775
RMSE train: 0.531925	val: 0.774397	test: 0.736782
MAE train: 0.385397	val: 0.539592	test: 0.537119

Epoch: 81
Loss: 0.4127742350101471
RMSE train: 0.472688	val: 0.748123	test: 0.706538
MAE train: 0.343982	val: 0.519791	test: 0.512612

Epoch: 82
Loss: 0.4332371453444163
RMSE train: 0.468370	val: 0.742002	test: 0.710986
MAE train: 0.348209	val: 0.522454	test: 0.522763

Epoch: 83
Loss: 0.40759141246477765
RMSE train: 0.537287	val: 0.780070	test: 0.748983
MAE train: 0.406083	val: 0.565024	test: 0.563108
RMSE train: 1.302450	val: 1.349644	test: 1.199664
MAE train: 1.015027	val: 1.081245	test: 0.926456

Epoch: 24
Loss: 1.0504379570484161
RMSE train: 0.906570	val: 0.933253	test: 0.877274
MAE train: 0.674755	val: 0.708453	test: 0.634746

Epoch: 25
Loss: 0.9539235085248947
RMSE train: 0.832251	val: 0.919133	test: 0.876022
MAE train: 0.622678	val: 0.689350	test: 0.642015

Epoch: 26
Loss: 1.011563092470169
RMSE train: 0.908005	val: 0.998256	test: 0.963024
MAE train: 0.696841	val: 0.784658	test: 0.744526

Epoch: 27
Loss: 0.8466533720493317
RMSE train: 0.742920	val: 0.806534	test: 0.824884
MAE train: 0.563170	val: 0.640595	test: 0.628602

Epoch: 28
Loss: 0.8296633213758469
RMSE train: 0.756380	val: 0.810866	test: 0.784875
MAE train: 0.565843	val: 0.601889	test: 0.579506

Epoch: 29
Loss: 0.847641184926033
RMSE train: 0.745677	val: 0.811580	test: 0.776222
MAE train: 0.573803	val: 0.608123	test: 0.586747

Epoch: 30
Loss: 0.9687495976686478
RMSE train: 0.692046	val: 0.755557	test: 0.753333
MAE train: 0.526059	val: 0.579207	test: 0.549326

Epoch: 31
Loss: 0.8852498382329941
RMSE train: 0.663117	val: 0.738295	test: 0.764818
MAE train: 0.511570	val: 0.560124	test: 0.578426

Epoch: 32
Loss: 0.7863747328519821
RMSE train: 0.801182	val: 0.882371	test: 0.864870
MAE train: 0.617724	val: 0.698406	test: 0.656577

Epoch: 33
Loss: 0.8356562107801437
RMSE train: 0.755630	val: 0.854132	test: 0.747130
MAE train: 0.569405	val: 0.648921	test: 0.559009

Epoch: 34
Loss: 0.7659956961870193
RMSE train: 0.757303	val: 0.806816	test: 0.833677
MAE train: 0.567931	val: 0.611788	test: 0.612547

Epoch: 35
Loss: 0.9893395155668259
RMSE train: 0.660697	val: 0.711871	test: 0.773479
MAE train: 0.496098	val: 0.535504	test: 0.567112

Epoch: 36
Loss: 0.7880559712648392
RMSE train: 0.658669	val: 0.772233	test: 0.744016
MAE train: 0.509087	val: 0.604728	test: 0.541192

Epoch: 37
Loss: 0.8691762983798981
RMSE train: 0.647676	val: 0.792139	test: 0.741550
MAE train: 0.504687	val: 0.603382	test: 0.541879

Epoch: 38
Loss: 0.6779396831989288
RMSE train: 0.637569	val: 0.808886	test: 0.731303
MAE train: 0.484047	val: 0.604445	test: 0.538728

Epoch: 39
Loss: 0.8553285449743271
RMSE train: 0.649139	val: 0.806201	test: 0.741883
MAE train: 0.485095	val: 0.601126	test: 0.533516

Epoch: 40
Loss: 0.7405941188335419
RMSE train: 0.606301	val: 0.767636	test: 0.717437
MAE train: 0.447386	val: 0.575765	test: 0.526779

Epoch: 41
Loss: 0.5961938500404358
RMSE train: 0.623723	val: 0.790759	test: 0.702895
MAE train: 0.469291	val: 0.594150	test: 0.528365

Epoch: 42
Loss: 0.6223410218954086
RMSE train: 0.628210	val: 0.803350	test: 0.706826
MAE train: 0.477921	val: 0.604157	test: 0.536236

Epoch: 43
Loss: 0.6631612330675125
RMSE train: 0.564343	val: 0.740700	test: 0.679067
MAE train: 0.424296	val: 0.559666	test: 0.497693

Epoch: 44
Loss: 1.148388683795929
RMSE train: 0.625884	val: 0.782547	test: 0.707850
MAE train: 0.469763	val: 0.600224	test: 0.525698

Epoch: 45
Loss: 0.654935359954834
RMSE train: 0.682213	val: 0.848646	test: 0.749249
MAE train: 0.515264	val: 0.648330	test: 0.548159

Epoch: 46
Loss: 0.573950856924057
RMSE train: 0.718577	val: 0.903611	test: 0.802840
MAE train: 0.551180	val: 0.688243	test: 0.605671

Epoch: 47
Loss: 0.6036971658468246
RMSE train: 0.583235	val: 0.785879	test: 0.726928
MAE train: 0.444947	val: 0.586899	test: 0.527504

Epoch: 48
Loss: 0.6896083354949951
RMSE train: 0.623787	val: 0.791146	test: 0.776395
MAE train: 0.479331	val: 0.591680	test: 0.585981

Epoch: 49
Loss: 0.9118632227182388
RMSE train: 0.764299	val: 0.897503	test: 0.868846
MAE train: 0.589281	val: 0.685511	test: 0.659478

Epoch: 50
Loss: 0.6442937105894089
RMSE train: 0.661166	val: 0.815133	test: 0.751203
MAE train: 0.497959	val: 0.621906	test: 0.556369

Epoch: 51
Loss: 0.7250215262174606
RMSE train: 0.589915	val: 0.762096	test: 0.703613
MAE train: 0.449091	val: 0.582724	test: 0.514701

Epoch: 52
Loss: 0.7075080871582031
RMSE train: 0.584274	val: 0.745485	test: 0.703425
MAE train: 0.443621	val: 0.567779	test: 0.505971

Epoch: 53
Loss: 0.6117048114538193
RMSE train: 0.586498	val: 0.743794	test: 0.729350
MAE train: 0.434764	val: 0.538711	test: 0.525632

Epoch: 54
Loss: 0.5904586017131805
RMSE train: 0.620997	val: 0.764226	test: 0.762629
MAE train: 0.465465	val: 0.557094	test: 0.567435

Epoch: 55
Loss: 0.5958760678768158
RMSE train: 0.553239	val: 0.714882	test: 0.720669
MAE train: 0.406186	val: 0.517247	test: 0.524062

Epoch: 56
Loss: 0.5384657233953476
RMSE train: 0.591007	val: 0.759028	test: 0.710456
MAE train: 0.444322	val: 0.561203	test: 0.517786

Epoch: 57
Loss: 0.64401476085186
RMSE train: 0.738455	val: 0.900628	test: 0.791380
MAE train: 0.562195	val: 0.684070	test: 0.596529

Epoch: 58
Loss: 0.6109010130167007
RMSE train: 0.733507	val: 0.901707	test: 0.793937
MAE train: 0.542789	val: 0.667754	test: 0.592506

Epoch: 59
Loss: 0.5543268918991089
RMSE train: 0.658162	val: 0.807580	test: 0.722419
MAE train: 0.482583	val: 0.597779	test: 0.526619

Epoch: 60
Loss: 0.5210757926106453
RMSE train: 0.609346	val: 0.763186	test: 0.692757
MAE train: 0.453293	val: 0.577059	test: 0.502933

Epoch: 61
Loss: 0.5058615729212761
RMSE train: 0.636623	val: 0.794337	test: 0.721138
MAE train: 0.472421	val: 0.591613	test: 0.518903

Epoch: 62
Loss: 0.6794396489858627
RMSE train: 0.582397	val: 0.775277	test: 0.702553
MAE train: 0.437501	val: 0.556081	test: 0.507589

Epoch: 63
Loss: 0.7545910850167274
RMSE train: 0.564950	val: 0.758191	test: 0.689496
MAE train: 0.425351	val: 0.553081	test: 0.502346

Epoch: 64
Loss: 0.794822171330452
RMSE train: 0.578376	val: 0.751787	test: 0.697972
MAE train: 0.431252	val: 0.575483	test: 0.509596

Epoch: 65
Loss: 0.8198832720518112
RMSE train: 0.539541	val: 0.729827	test: 0.738256
MAE train: 0.406006	val: 0.541658	test: 0.539843

Epoch: 66
Loss: 0.6353840231895447
RMSE train: 0.569660	val: 0.744787	test: 0.719988
MAE train: 0.421427	val: 0.566691	test: 0.536093

Epoch: 67
Loss: 0.653358593583107
RMSE train: 0.612161	val: 0.800933	test: 0.708948
MAE train: 0.454502	val: 0.599215	test: 0.515550

Epoch: 68
Loss: 0.6146476417779922
RMSE train: 0.653719	val: 0.831942	test: 0.726430
MAE train: 0.482469	val: 0.621455	test: 0.530627

Epoch: 69
Loss: 0.6257815659046173
RMSE train: 0.599997	val: 0.768705	test: 0.689165
MAE train: 0.443405	val: 0.571737	test: 0.503906

Epoch: 70
Loss: 0.6572365909814835
RMSE train: 0.663438	val: 0.786381	test: 0.744864
MAE train: 0.485743	val: 0.603030	test: 0.552860

Epoch: 71
Loss: 0.6613528579473495
RMSE train: 0.673061	val: 0.799716	test: 0.751792
MAE train: 0.506078	val: 0.611512	test: 0.575952

Epoch: 72
Loss: 0.6452925950288773
RMSE train: 0.614740	val: 0.754882	test: 0.690838
MAE train: 0.451700	val: 0.559617	test: 0.510206

Epoch: 73
Loss: 0.7363179177045822
RMSE train: 0.565007	val: 0.737313	test: 0.686354
MAE train: 0.416191	val: 0.535960	test: 0.496121

Epoch: 74
Loss: 0.6679717153310776
RMSE train: 0.531383	val: 0.738350	test: 0.715078
MAE train: 0.405058	val: 0.528628	test: 0.519050

Epoch: 75
Loss: 0.4703007638454437
RMSE train: 0.595461	val: 0.785296	test: 0.734791
MAE train: 0.452561	val: 0.567415	test: 0.523469

Epoch: 76
Loss: 0.6786556914448738
RMSE train: 0.552035	val: 0.750615	test: 0.692932
MAE train: 0.410916	val: 0.554919	test: 0.495975

Epoch: 77
Loss: 0.6420185565948486
RMSE train: 0.554973	val: 0.743605	test: 0.702339
MAE train: 0.410338	val: 0.554126	test: 0.514943

Epoch: 78
Loss: 0.7114062532782555
RMSE train: 0.515045	val: 0.740874	test: 0.697428
MAE train: 0.392271	val: 0.538808	test: 0.510790

Epoch: 79
Loss: 0.8610488474369049
RMSE train: 0.558144	val: 0.819017	test: 0.727722
MAE train: 0.421979	val: 0.573393	test: 0.523085

Epoch: 80
Loss: 0.6099721118807793
RMSE train: 0.601555	val: 0.851991	test: 0.723039
MAE train: 0.450443	val: 0.601249	test: 0.527093

Epoch: 81
Loss: 1.3143615424633026
RMSE train: 0.618538	val: 0.802558	test: 0.713424
MAE train: 0.468096	val: 0.583550	test: 0.522003

Epoch: 82
Loss: 0.7736495286226273
RMSE train: 0.608363	val: 0.758011	test: 0.731113
MAE train: 0.465511	val: 0.571453	test: 0.541772

Epoch: 83
Loss: 0.6369635462760925
RMSE train: 0.602424	val: 0.756460	test: 0.751215
MAE train: 0.456296	val: 0.552733	test: 0.546003

Epoch: 84
Loss: 0.6009570956230164
RMSE train: 1.064041	val: 1.072418	test: 1.051249
MAE train: 0.815800	val: 0.850100	test: 0.813306

Epoch: 24
Loss: 1.06676684319973
RMSE train: 1.285360	val: 1.298238	test: 1.225348
MAE train: 0.954262	val: 1.005491	test: 0.898180

Epoch: 25
Loss: 1.0934741646051407
RMSE train: 1.015380	val: 1.075478	test: 0.927147
MAE train: 0.769795	val: 0.831222	test: 0.667509

Epoch: 26
Loss: 1.0391889363527298
RMSE train: 0.847535	val: 0.935822	test: 0.806545
MAE train: 0.641921	val: 0.707689	test: 0.577806

Epoch: 27
Loss: 1.0853837430477142
RMSE train: 0.760585	val: 0.867133	test: 0.797835
MAE train: 0.593633	val: 0.642763	test: 0.589243

Epoch: 28
Loss: 1.0952279567718506
RMSE train: 0.700676	val: 0.786104	test: 0.814054
MAE train: 0.531523	val: 0.609581	test: 0.614293

Epoch: 29
Loss: 0.7489941865205765
RMSE train: 0.734706	val: 0.818297	test: 0.856351
MAE train: 0.553113	val: 0.622411	test: 0.643282

Epoch: 30
Loss: 0.9355921745300293
RMSE train: 0.735915	val: 0.832495	test: 0.814067
MAE train: 0.546644	val: 0.624073	test: 0.610745

Epoch: 31
Loss: 0.8761740177869797
RMSE train: 0.692231	val: 0.797726	test: 0.799891
MAE train: 0.520795	val: 0.611000	test: 0.608751

Epoch: 32
Loss: 0.9258330017328262
RMSE train: 0.674626	val: 0.783095	test: 0.830185
MAE train: 0.504178	val: 0.581191	test: 0.613530

Epoch: 33
Loss: 0.7553776651620865
RMSE train: 0.720508	val: 0.826065	test: 0.836877
MAE train: 0.541324	val: 0.627981	test: 0.619589

Epoch: 34
Loss: 0.7255124449729919
RMSE train: 0.759316	val: 0.852335	test: 0.832734
MAE train: 0.564396	val: 0.646567	test: 0.622289

Epoch: 35
Loss: 0.9715019464492798
RMSE train: 0.789898	val: 0.889715	test: 0.858480
MAE train: 0.590754	val: 0.674878	test: 0.653208

Epoch: 36
Loss: 0.6526682525873184
RMSE train: 0.698779	val: 0.821272	test: 0.762181
MAE train: 0.521191	val: 0.609624	test: 0.560058

Epoch: 37
Loss: 0.8864800930023193
RMSE train: 0.786790	val: 0.899245	test: 0.820275
MAE train: 0.582124	val: 0.685121	test: 0.597264

Epoch: 38
Loss: 0.8074504882097244
RMSE train: 0.982226	val: 1.058001	test: 0.990362
MAE train: 0.704511	val: 0.803357	test: 0.703719

Epoch: 39
Loss: 1.12214595079422
RMSE train: 0.778130	val: 0.858883	test: 0.820398
MAE train: 0.579703	val: 0.644505	test: 0.595224

Epoch: 40
Loss: 0.6520907431840897
RMSE train: 0.741208	val: 0.819505	test: 0.787143
MAE train: 0.551676	val: 0.607560	test: 0.583472

Epoch: 41
Loss: 0.8185065537691116
RMSE train: 0.768160	val: 0.896860	test: 0.829611
MAE train: 0.584445	val: 0.670976	test: 0.631292

Epoch: 42
Loss: 0.7128434926271439
RMSE train: 0.746079	val: 0.881580	test: 0.805804
MAE train: 0.561603	val: 0.668425	test: 0.600628

Epoch: 43
Loss: 0.6878439038991928
RMSE train: 0.716238	val: 0.855630	test: 0.763995
MAE train: 0.532041	val: 0.641283	test: 0.565170

Epoch: 44
Loss: 0.6735838502645493
RMSE train: 0.732775	val: 0.896977	test: 0.791069
MAE train: 0.554552	val: 0.672503	test: 0.593137

Epoch: 45
Loss: 0.7028812766075134
RMSE train: 0.614760	val: 0.761499	test: 0.702202
MAE train: 0.463663	val: 0.576972	test: 0.509889

Epoch: 46
Loss: 0.6735231131315231
RMSE train: 0.626832	val: 0.751995	test: 0.733381
MAE train: 0.477733	val: 0.584220	test: 0.546155

Epoch: 47
Loss: 0.5946576297283173
RMSE train: 0.639801	val: 0.757585	test: 0.763388
MAE train: 0.481823	val: 0.567688	test: 0.555321

Epoch: 48
Loss: 0.752466231584549
RMSE train: 0.636148	val: 0.758296	test: 0.766637
MAE train: 0.482472	val: 0.575536	test: 0.560061

Epoch: 49
Loss: 0.6380107551813126
RMSE train: 0.660977	val: 0.794488	test: 0.790991
MAE train: 0.511257	val: 0.609866	test: 0.606427

Epoch: 50
Loss: 0.6896107792854309
RMSE train: 0.619498	val: 0.793814	test: 0.770035
MAE train: 0.475131	val: 0.589733	test: 0.581360

Epoch: 51
Loss: 0.73410764336586
RMSE train: 0.746994	val: 0.872205	test: 0.856067
MAE train: 0.547948	val: 0.645657	test: 0.628718

Epoch: 52
Loss: 0.6258482784032822
RMSE train: 0.627411	val: 0.752402	test: 0.758353
MAE train: 0.458468	val: 0.548562	test: 0.537535

Epoch: 53
Loss: 0.7665221095085144
RMSE train: 0.653060	val: 0.777387	test: 0.775463
MAE train: 0.497995	val: 0.594466	test: 0.558315

Epoch: 54
Loss: 0.702401727437973
RMSE train: 0.647519	val: 0.791362	test: 0.728767
MAE train: 0.489139	val: 0.603289	test: 0.524452

Epoch: 55
Loss: 0.671092763543129
RMSE train: 0.670825	val: 0.871397	test: 0.734691
MAE train: 0.509650	val: 0.643817	test: 0.554145

Epoch: 56
Loss: 0.702870100736618
RMSE train: 0.601124	val: 0.795642	test: 0.690110
MAE train: 0.455351	val: 0.603556	test: 0.516054

Epoch: 57
Loss: 0.7458505779504776
RMSE train: 0.583494	val: 0.785758	test: 0.713604
MAE train: 0.446264	val: 0.602524	test: 0.533837

Epoch: 58
Loss: 1.0146779865026474
RMSE train: 0.556342	val: 0.790255	test: 0.693926
MAE train: 0.418532	val: 0.597714	test: 0.507497

Epoch: 59
Loss: 0.5840919315814972
RMSE train: 0.588659	val: 0.775765	test: 0.717274
MAE train: 0.447802	val: 0.585110	test: 0.520246

Epoch: 60
Loss: 0.6731276512145996
RMSE train: 0.622974	val: 0.752312	test: 0.732437
MAE train: 0.476406	val: 0.576896	test: 0.549062

Epoch: 61
Loss: 0.6019224971532822
RMSE train: 0.614623	val: 0.753274	test: 0.718264
MAE train: 0.457421	val: 0.562539	test: 0.521115

Epoch: 62
Loss: 0.6327241361141205
RMSE train: 0.613581	val: 0.761639	test: 0.724664
MAE train: 0.458065	val: 0.570267	test: 0.531699

Epoch: 63
Loss: 0.6813605576753616
RMSE train: 0.601298	val: 0.748357	test: 0.707389
MAE train: 0.437953	val: 0.553619	test: 0.509515

Epoch: 64
Loss: 0.5796662867069244
RMSE train: 0.570368	val: 0.725549	test: 0.690126
MAE train: 0.410946	val: 0.526846	test: 0.503867

Epoch: 65
Loss: 0.6812146604061127
RMSE train: 0.714483	val: 0.860370	test: 0.793402
MAE train: 0.527334	val: 0.631328	test: 0.598904

Epoch: 66
Loss: 0.6386509388685226
RMSE train: 0.700157	val: 0.821052	test: 0.751117
MAE train: 0.507173	val: 0.601792	test: 0.568447

Epoch: 67
Loss: 0.5828221440315247
RMSE train: 0.702236	val: 0.790304	test: 0.748979
MAE train: 0.510573	val: 0.592158	test: 0.545620

Epoch: 68
Loss: 0.7173006981611252
RMSE train: 0.661012	val: 0.759343	test: 0.742272
MAE train: 0.482251	val: 0.589145	test: 0.557639

Epoch: 69
Loss: 0.469759926199913
RMSE train: 0.850593	val: 0.955813	test: 0.891610
MAE train: 0.634021	val: 0.729838	test: 0.665520

Epoch: 70
Loss: 0.7142303436994553
RMSE train: 0.727778	val: 0.864808	test: 0.764814
MAE train: 0.529855	val: 0.649806	test: 0.568689

Epoch: 71
Loss: 0.4990757331252098
RMSE train: 0.673047	val: 0.807233	test: 0.712594
MAE train: 0.503222	val: 0.610637	test: 0.514933

Epoch: 72
Loss: 0.689798578619957
RMSE train: 0.647763	val: 0.794212	test: 0.708148
MAE train: 0.474443	val: 0.591778	test: 0.510672

Epoch: 73
Loss: 0.543401449918747
RMSE train: 0.634858	val: 0.773315	test: 0.735462
MAE train: 0.460428	val: 0.578726	test: 0.531063

Epoch: 74
Loss: 1.0117769539356232
RMSE train: 0.508914	val: 0.683694	test: 0.695429
MAE train: 0.381137	val: 0.499314	test: 0.505817

Epoch: 75
Loss: 0.4950648546218872
RMSE train: 0.523454	val: 0.705074	test: 0.699535
MAE train: 0.384636	val: 0.519397	test: 0.511832

Epoch: 76
Loss: 0.5834618434309959
RMSE train: 0.602683	val: 0.763133	test: 0.714786
MAE train: 0.442079	val: 0.568671	test: 0.520026

Epoch: 77
Loss: 0.5405751690268517
RMSE train: 0.758102	val: 0.879075	test: 0.836900
MAE train: 0.556040	val: 0.657232	test: 0.598069

Epoch: 78
Loss: 0.5001611560583115
RMSE train: 0.782296	val: 0.893019	test: 0.855789
MAE train: 0.582864	val: 0.674073	test: 0.612791

Epoch: 79
Loss: 0.6445212513208389
RMSE train: 0.782268	val: 0.902895	test: 0.836373
MAE train: 0.594609	val: 0.690359	test: 0.611424

Epoch: 80
Loss: 0.4485587105154991
RMSE train: 0.718023	val: 0.845550	test: 0.794597
MAE train: 0.535688	val: 0.640142	test: 0.584324

Epoch: 81
Loss: 0.6570877879858017
RMSE train: 0.624113	val: 0.759325	test: 0.706423
MAE train: 0.457580	val: 0.565440	test: 0.513869

Epoch: 82
Loss: 0.7461770176887512
RMSE train: 0.574407	val: 0.748929	test: 0.687797
MAE train: 0.416564	val: 0.541251	test: 0.500778

Epoch: 83
Loss: 0.6470979899168015
RMSE train: 0.604114	val: 0.831041	test: 0.783999
MAE train: 0.442975	val: 0.572627	test: 0.564245

Epoch: 84
Loss: 0.8480103313922882
RMSE train: 1.066176	val: 1.086211	test: 1.037586
MAE train: 0.826652	val: 0.868908	test: 0.797144

Epoch: 24
Loss: 1.268843173980713
RMSE train: 1.071224	val: 1.114484	test: 1.046452
MAE train: 0.819291	val: 0.881790	test: 0.795776

Epoch: 25
Loss: 0.8929826021194458
RMSE train: 0.794685	val: 0.801137	test: 0.855247
MAE train: 0.593542	val: 0.617402	test: 0.642781

Epoch: 26
Loss: 0.9414610117673874
RMSE train: 0.794618	val: 0.822167	test: 0.881116
MAE train: 0.594960	val: 0.640379	test: 0.660295

Epoch: 27
Loss: 0.8633556962013245
RMSE train: 0.849773	val: 0.922816	test: 0.895436
MAE train: 0.641577	val: 0.711980	test: 0.668671

Epoch: 28
Loss: 0.7452680766582489
RMSE train: 0.881802	val: 0.956239	test: 0.927074
MAE train: 0.673129	val: 0.728955	test: 0.705102

Epoch: 29
Loss: 0.7781849354505539
RMSE train: 0.752866	val: 0.808802	test: 0.834921
MAE train: 0.552577	val: 0.604970	test: 0.624175

Epoch: 30
Loss: 0.7922492474317551
RMSE train: 0.743695	val: 0.818868	test: 0.818357
MAE train: 0.557319	val: 0.618928	test: 0.613051

Epoch: 31
Loss: 0.8508924245834351
RMSE train: 0.940433	val: 1.004907	test: 0.964269
MAE train: 0.693608	val: 0.768582	test: 0.712535

Epoch: 32
Loss: 1.329623132944107
RMSE train: 0.804523	val: 0.826498	test: 0.830762
MAE train: 0.606656	val: 0.628636	test: 0.593680

Epoch: 33
Loss: 0.8986829370260239
RMSE train: 0.814973	val: 0.844589	test: 0.828921
MAE train: 0.603527	val: 0.615452	test: 0.594517

Epoch: 34
Loss: 0.8017427325248718
RMSE train: 0.857186	val: 0.914128	test: 0.889369
MAE train: 0.654222	val: 0.710555	test: 0.667489

Epoch: 35
Loss: 0.846181184053421
RMSE train: 0.669651	val: 0.695926	test: 0.778745
MAE train: 0.504215	val: 0.538112	test: 0.585423

Epoch: 36
Loss: 0.7896344661712646
RMSE train: 0.636898	val: 0.695259	test: 0.705769
MAE train: 0.475558	val: 0.528289	test: 0.510310

Epoch: 37
Loss: 0.8730791956186295
RMSE train: 0.772063	val: 0.846851	test: 0.795143
MAE train: 0.571958	val: 0.642745	test: 0.588991

Epoch: 38
Loss: 0.7064317017793655
RMSE train: 0.764779	val: 0.834044	test: 0.759856
MAE train: 0.570374	val: 0.635064	test: 0.556156

Epoch: 39
Loss: 0.6869512051343918
RMSE train: 0.717369	val: 0.776092	test: 0.750801
MAE train: 0.542973	val: 0.608226	test: 0.569668

Epoch: 40
Loss: 1.123753160238266
RMSE train: 0.822520	val: 0.882187	test: 0.812786
MAE train: 0.609790	val: 0.682345	test: 0.586177

Epoch: 41
Loss: 0.7643959522247314
RMSE train: 0.673047	val: 0.737738	test: 0.741789
MAE train: 0.501683	val: 0.547372	test: 0.531221

Epoch: 42
Loss: 0.7678896188735962
RMSE train: 0.642213	val: 0.712444	test: 0.751419
MAE train: 0.493030	val: 0.531287	test: 0.564340

Epoch: 43
Loss: 0.8826857656240463
RMSE train: 0.713954	val: 0.816948	test: 0.785685
MAE train: 0.540704	val: 0.598527	test: 0.593758

Epoch: 44
Loss: 0.7634211778640747
RMSE train: 0.723504	val: 0.888855	test: 0.790323
MAE train: 0.529126	val: 0.615842	test: 0.575635

Epoch: 45
Loss: 0.8925019651651382
RMSE train: 0.619645	val: 0.725510	test: 0.714356
MAE train: 0.462609	val: 0.530425	test: 0.522795

Epoch: 46
Loss: 0.6477182805538177
RMSE train: 0.630915	val: 0.733159	test: 0.735660
MAE train: 0.479188	val: 0.540988	test: 0.530338

Epoch: 47
Loss: 0.7327618151903152
RMSE train: 0.607496	val: 0.702760	test: 0.711549
MAE train: 0.455533	val: 0.520659	test: 0.522540

Epoch: 48
Loss: 0.6837223321199417
RMSE train: 0.667675	val: 0.755904	test: 0.753011
MAE train: 0.501553	val: 0.570424	test: 0.564064

Epoch: 49
Loss: 0.6517725735902786
RMSE train: 0.677951	val: 0.746171	test: 0.760560
MAE train: 0.503758	val: 0.572378	test: 0.561124

Epoch: 50
Loss: 0.6452678442001343
RMSE train: 0.757119	val: 0.823085	test: 0.783419
MAE train: 0.565249	val: 0.622485	test: 0.573833

Epoch: 51
Loss: 0.6378239095211029
RMSE train: 0.818019	val: 0.883072	test: 0.816879
MAE train: 0.605226	val: 0.663021	test: 0.593558

Epoch: 52
Loss: 0.919424831867218
RMSE train: 0.686553	val: 0.783874	test: 0.729995
MAE train: 0.503370	val: 0.577355	test: 0.526564

Epoch: 53
Loss: 0.770911693572998
RMSE train: 0.681543	val: 0.812997	test: 0.789503
MAE train: 0.511725	val: 0.584918	test: 0.588825

Epoch: 54
Loss: 0.7644993662834167
RMSE train: 0.627167	val: 0.713279	test: 0.796476
MAE train: 0.477857	val: 0.521842	test: 0.586539

Epoch: 55
Loss: 1.057061329483986
RMSE train: 0.598769	val: 0.682152	test: 0.748961
MAE train: 0.444523	val: 0.507623	test: 0.557858

Epoch: 56
Loss: 0.5916628241539001
RMSE train: 0.658633	val: 0.724861	test: 0.774100
MAE train: 0.493522	val: 0.536536	test: 0.578902

Epoch: 57
Loss: 0.981484517455101
RMSE train: 0.609554	val: 0.657885	test: 0.703783
MAE train: 0.448279	val: 0.500411	test: 0.510597

Epoch: 58
Loss: 0.688305139541626
RMSE train: 0.639907	val: 0.669040	test: 0.712419
MAE train: 0.476442	val: 0.519840	test: 0.513806

Epoch: 59
Loss: 0.6321041733026505
RMSE train: 0.683983	val: 0.728177	test: 0.743909
MAE train: 0.505401	val: 0.549214	test: 0.554964

Epoch: 60
Loss: 0.6102817356586456
RMSE train: 0.618633	val: 0.697003	test: 0.742076
MAE train: 0.473609	val: 0.513123	test: 0.566822

Epoch: 61
Loss: 0.4487501010298729
RMSE train: 0.555576	val: 0.656370	test: 0.725008
MAE train: 0.417478	val: 0.465486	test: 0.536545

Epoch: 62
Loss: 0.5966606587171555
RMSE train: 0.544421	val: 0.644871	test: 0.693194
MAE train: 0.399957	val: 0.471362	test: 0.513786

Epoch: 63
Loss: 0.583509236574173
RMSE train: 0.592625	val: 0.701612	test: 0.683913
MAE train: 0.427122	val: 0.514951	test: 0.501712

Epoch: 64
Loss: 0.8082609921693802
RMSE train: 0.643706	val: 0.737646	test: 0.712733
MAE train: 0.467283	val: 0.539444	test: 0.518308

Epoch: 65
Loss: 0.5660420879721642
RMSE train: 0.820638	val: 0.941819	test: 0.821816
MAE train: 0.601625	val: 0.704892	test: 0.605883

Epoch: 66
Loss: 0.6833269596099854
RMSE train: 0.721602	val: 0.837174	test: 0.759389
MAE train: 0.526701	val: 0.629041	test: 0.549618

Epoch: 67
Loss: 0.6812462359666824
RMSE train: 0.574259	val: 0.683850	test: 0.694758
MAE train: 0.415977	val: 0.509071	test: 0.508249

Epoch: 68
Loss: 0.7330892533063889
RMSE train: 0.566159	val: 0.680476	test: 0.709532
MAE train: 0.414960	val: 0.517692	test: 0.523569

Epoch: 69
Loss: 0.6420092582702637
RMSE train: 0.643408	val: 0.753315	test: 0.749750
MAE train: 0.472027	val: 0.564925	test: 0.559756

Epoch: 70
Loss: 0.5132389962673187
RMSE train: 0.683953	val: 0.824568	test: 0.769186
MAE train: 0.515204	val: 0.621171	test: 0.579379

Epoch: 71
Loss: 0.4525400251150131
RMSE train: 0.610521	val: 0.760389	test: 0.707457
MAE train: 0.455546	val: 0.576202	test: 0.527099

Epoch: 72
Loss: 0.5264560878276825
RMSE train: 0.563849	val: 0.712517	test: 0.700193
MAE train: 0.413178	val: 0.532488	test: 0.509621

Epoch: 73
Loss: 0.4700170233845711
RMSE train: 0.564451	val: 0.709118	test: 0.735684
MAE train: 0.407229	val: 0.511266	test: 0.532922

Epoch: 74
Loss: 0.5511124581098557
RMSE train: 0.624271	val: 0.770226	test: 0.760671
MAE train: 0.452483	val: 0.567516	test: 0.550047

Epoch: 75
Loss: 0.7178176492452621
RMSE train: 0.645566	val: 0.785196	test: 0.743630
MAE train: 0.469032	val: 0.574316	test: 0.538917

Epoch: 76
Loss: 0.5042573884129524
RMSE train: 0.552262	val: 0.710550	test: 0.687713
MAE train: 0.402736	val: 0.523893	test: 0.490873

Epoch: 77
Loss: 0.5528561621904373
RMSE train: 0.544602	val: 0.717442	test: 0.695346
MAE train: 0.395816	val: 0.518014	test: 0.498041

Epoch: 78
Loss: 0.5608661249279976
RMSE train: 0.661615	val: 0.802556	test: 0.775181
MAE train: 0.483534	val: 0.599466	test: 0.572005

Epoch: 79
Loss: 0.619009256362915
RMSE train: 0.618100	val: 0.761756	test: 0.725892
MAE train: 0.452813	val: 0.565770	test: 0.525971

Epoch: 80
Loss: 0.4943474382162094
RMSE train: 0.588478	val: 0.730776	test: 0.680630
MAE train: 0.450036	val: 0.550550	test: 0.496135

Epoch: 81
Loss: 0.5460782274603844
RMSE train: 0.539313	val: 0.686569	test: 0.662362
MAE train: 0.404643	val: 0.511334	test: 0.493388

Epoch: 82
Loss: 0.7654724419116974
RMSE train: 0.605315	val: 0.725745	test: 0.720063
MAE train: 0.440488	val: 0.536576	test: 0.533241

Epoch: 83
Loss: 0.6175335794687271
RMSE train: 0.609712	val: 0.728749	test: 0.716026
MAE train: 0.449996	val: 0.552305	test: 0.520968

Epoch: 84
Loss: 0.7521092146635056
RMSE train: 1.009930	val: 1.094993	test: 0.999449
MAE train: 0.793440	val: 0.848792	test: 0.794443

Epoch: 24
Loss: 0.7844740599393845
RMSE train: 0.906784	val: 0.975655	test: 0.944962
MAE train: 0.695852	val: 0.738982	test: 0.744033

Epoch: 25
Loss: 0.6388887614011765
RMSE train: 0.790021	val: 0.888158	test: 0.861851
MAE train: 0.595735	val: 0.670809	test: 0.668843

Epoch: 26
Loss: 0.7682143151760101
RMSE train: 0.682589	val: 0.795397	test: 0.760754
MAE train: 0.510167	val: 0.584067	test: 0.584384

Epoch: 27
Loss: 0.6984495967626572
RMSE train: 0.681433	val: 0.798321	test: 0.748755
MAE train: 0.506395	val: 0.576843	test: 0.581394

Epoch: 28
Loss: 0.603599801659584
RMSE train: 0.650857	val: 0.773499	test: 0.740562
MAE train: 0.482379	val: 0.583133	test: 0.556724

Epoch: 29
Loss: 0.6871080696582794
RMSE train: 0.592497	val: 0.709165	test: 0.732793
MAE train: 0.438819	val: 0.526813	test: 0.531335

Epoch: 30
Loss: 0.6332183629274368
RMSE train: 0.577574	val: 0.754903	test: 0.722832
MAE train: 0.432418	val: 0.562712	test: 0.547223

Epoch: 31
Loss: 0.602106124162674
RMSE train: 0.634464	val: 0.774763	test: 0.687988
MAE train: 0.459719	val: 0.536436	test: 0.524422

Epoch: 32
Loss: 0.6290934532880783
RMSE train: 0.717485	val: 0.882694	test: 0.784607
MAE train: 0.535756	val: 0.661491	test: 0.612420

Epoch: 33
Loss: 0.6471600383520126
RMSE train: 0.726289	val: 0.826688	test: 0.815287
MAE train: 0.545512	val: 0.625636	test: 0.627486

Epoch: 34
Loss: 0.6077690422534943
RMSE train: 0.857573	val: 0.960121	test: 0.883509
MAE train: 0.650877	val: 0.736933	test: 0.680243

Epoch: 35
Loss: 0.5453143864870071
RMSE train: 0.864765	val: 1.032409	test: 0.877123
MAE train: 0.636811	val: 0.784424	test: 0.670495

Epoch: 36
Loss: 0.5790287405252457
RMSE train: 0.614796	val: 0.761136	test: 0.735431
MAE train: 0.457533	val: 0.570753	test: 0.548308

Epoch: 37
Loss: 0.5567484349012375
RMSE train: 0.707959	val: 0.858008	test: 0.784179
MAE train: 0.533060	val: 0.659108	test: 0.594038

Epoch: 38
Loss: 0.5800110101699829
RMSE train: 0.555614	val: 0.730472	test: 0.710260
MAE train: 0.409663	val: 0.549352	test: 0.516456

Epoch: 39
Loss: 0.5280953347682953
RMSE train: 0.583364	val: 0.748298	test: 0.690251
MAE train: 0.432906	val: 0.538076	test: 0.527533

Epoch: 40
Loss: 0.5243277102708817
RMSE train: 0.808426	val: 0.947452	test: 0.831699
MAE train: 0.598566	val: 0.712807	test: 0.634446

Epoch: 41
Loss: 0.5324139147996902
RMSE train: 0.650845	val: 0.745734	test: 0.762076
MAE train: 0.475645	val: 0.551708	test: 0.561347

Epoch: 42
Loss: 0.5380037277936935
RMSE train: 0.601708	val: 0.733761	test: 0.747875
MAE train: 0.441473	val: 0.558322	test: 0.555763

Epoch: 43
Loss: 0.48492034524679184
RMSE train: 0.700020	val: 0.884545	test: 0.773650
MAE train: 0.521033	val: 0.680299	test: 0.591373

Epoch: 44
Loss: 0.49256037175655365
RMSE train: 0.584369	val: 0.763990	test: 0.715632
MAE train: 0.435642	val: 0.568575	test: 0.538766

Epoch: 45
Loss: 0.5603381618857384
RMSE train: 0.662620	val: 0.823717	test: 0.774020
MAE train: 0.498589	val: 0.620653	test: 0.595947

Epoch: 46
Loss: 0.49413716048002243
RMSE train: 0.597580	val: 0.720759	test: 0.729798
MAE train: 0.440007	val: 0.535271	test: 0.545029

Epoch: 47
Loss: 0.4527106508612633
RMSE train: 0.650107	val: 0.835571	test: 0.733024
MAE train: 0.476424	val: 0.618092	test: 0.554993

Epoch: 48
Loss: 0.4676150977611542
RMSE train: 0.644509	val: 0.821575	test: 0.728321
MAE train: 0.476329	val: 0.615986	test: 0.556005

Epoch: 49
Loss: 0.4770211800932884
RMSE train: 0.583387	val: 0.735027	test: 0.709706
MAE train: 0.434478	val: 0.558477	test: 0.529678

Epoch: 50
Loss: 0.48484594374895096
RMSE train: 0.578648	val: 0.727427	test: 0.683583
MAE train: 0.428472	val: 0.521356	test: 0.499818

Epoch: 51
Loss: 0.5067028775811195
RMSE train: 0.584521	val: 0.760911	test: 0.697195
MAE train: 0.428029	val: 0.572689	test: 0.508323

Epoch: 52
Loss: 0.44755958765745163
RMSE train: 0.497570	val: 0.653627	test: 0.679232
MAE train: 0.361866	val: 0.474980	test: 0.492604

Epoch: 53
Loss: 0.49962104111909866
RMSE train: 0.555465	val: 0.697474	test: 0.679000
MAE train: 0.402815	val: 0.502543	test: 0.496437

Epoch: 54
Loss: 0.47506459057331085
RMSE train: 0.550809	val: 0.694583	test: 0.663836
MAE train: 0.409531	val: 0.505853	test: 0.487769

Epoch: 55
Loss: 0.39984218776226044
RMSE train: 0.526596	val: 0.645925	test: 0.668377
MAE train: 0.388770	val: 0.464038	test: 0.498848

Epoch: 56
Loss: 0.47281329333782196
RMSE train: 0.591422	val: 0.723906	test: 0.712908
MAE train: 0.440152	val: 0.549311	test: 0.522968

Epoch: 57
Loss: 0.45205897092819214
RMSE train: 0.626833	val: 0.773010	test: 0.716309
MAE train: 0.465243	val: 0.577952	test: 0.526271

Epoch: 58
Loss: 0.4460456594824791
RMSE train: 0.649401	val: 0.792672	test: 0.748197
MAE train: 0.484777	val: 0.610159	test: 0.554684

Epoch: 59
Loss: 0.4685180261731148
RMSE train: 0.662625	val: 0.780990	test: 0.778729
MAE train: 0.482054	val: 0.605488	test: 0.570255

Epoch: 60
Loss: 0.46653731167316437
RMSE train: 0.561492	val: 0.753742	test: 0.698516
MAE train: 0.411379	val: 0.567119	test: 0.505022

Epoch: 61
Loss: 0.4302699938416481
RMSE train: 0.527637	val: 0.677139	test: 0.683318
MAE train: 0.390445	val: 0.517437	test: 0.510902

Epoch: 62
Loss: 0.43935976177453995
RMSE train: 0.675230	val: 0.761595	test: 0.769160
MAE train: 0.497321	val: 0.579051	test: 0.569924

Epoch: 63
Loss: 0.4107254594564438
RMSE train: 0.658747	val: 0.788435	test: 0.745550
MAE train: 0.471208	val: 0.596755	test: 0.548493

Epoch: 64
Loss: 0.4405031353235245
RMSE train: 0.495915	val: 0.624459	test: 0.652587
MAE train: 0.365957	val: 0.469207	test: 0.487240

Epoch: 65
Loss: 0.39674288034439087
RMSE train: 0.589889	val: 0.756298	test: 0.700338
MAE train: 0.441026	val: 0.576390	test: 0.537330

Epoch: 66
Loss: 0.43461713194847107
RMSE train: 0.494781	val: 0.656048	test: 0.650214
MAE train: 0.362835	val: 0.487025	test: 0.486101

Epoch: 67
Loss: 0.4385300278663635
RMSE train: 0.564849	val: 0.679865	test: 0.691116
MAE train: 0.418439	val: 0.509657	test: 0.504279

Epoch: 68
Loss: 0.37592314928770065
RMSE train: 0.556481	val: 0.708629	test: 0.690554
MAE train: 0.414288	val: 0.530256	test: 0.518278

Epoch: 69
Loss: 0.4254743680357933
RMSE train: 0.444252	val: 0.642903	test: 0.659777
MAE train: 0.324832	val: 0.475240	test: 0.486111

Epoch: 70
Loss: 0.4249550774693489
RMSE train: 0.546082	val: 0.678122	test: 0.689067
MAE train: 0.398317	val: 0.507654	test: 0.491447

Epoch: 71
Loss: 0.38640884310007095
RMSE train: 0.571904	val: 0.716798	test: 0.700123
MAE train: 0.420597	val: 0.543641	test: 0.510255

Epoch: 72
Loss: 0.36889877170324326
RMSE train: 0.518985	val: 0.663238	test: 0.696094
MAE train: 0.382667	val: 0.490759	test: 0.521629

Epoch: 73
Loss: 0.3878084793686867
RMSE train: 0.524223	val: 0.674755	test: 0.685174
MAE train: 0.379079	val: 0.496031	test: 0.503689

Epoch: 74
Loss: 0.3697897270321846
RMSE train: 0.469738	val: 0.649527	test: 0.654253
MAE train: 0.347708	val: 0.480747	test: 0.491179

Epoch: 75
Loss: 0.38997041434049606
RMSE train: 0.523287	val: 0.710478	test: 0.677109
MAE train: 0.381954	val: 0.545952	test: 0.502513

Epoch: 76
Loss: 0.36102111637592316
RMSE train: 0.525591	val: 0.651978	test: 0.654507
MAE train: 0.392018	val: 0.483691	test: 0.490064

Epoch: 77
Loss: 0.38101936131715775
RMSE train: 0.575441	val: 0.768851	test: 0.695684
MAE train: 0.429898	val: 0.586145	test: 0.523134

Epoch: 78
Loss: 0.3779119700193405
RMSE train: 0.490404	val: 0.632760	test: 0.689390
MAE train: 0.358480	val: 0.492541	test: 0.512679

Epoch: 79
Loss: 0.3867463767528534
RMSE train: 0.511270	val: 0.672721	test: 0.664148
MAE train: 0.373455	val: 0.512911	test: 0.484823

Epoch: 80
Loss: 0.40266990661621094
RMSE train: 0.526328	val: 0.737903	test: 0.672653
MAE train: 0.386531	val: 0.543554	test: 0.490696

Epoch: 81
Loss: 0.40550045669078827
RMSE train: 0.579898	val: 0.680799	test: 0.723934
MAE train: 0.428161	val: 0.517484	test: 0.526076

Epoch: 82
Loss: 0.37776201218366623
RMSE train: 0.670188	val: 0.790488	test: 0.770951
MAE train: 0.511000	val: 0.605218	test: 0.574310

Epoch: 83
Loss: 0.3555219992995262
RMSE train: 0.533328	val: 0.676626	test: 0.692524
MAE train: 0.394314	val: 0.510055	test: 0.511371
RMSE train: 0.950337	val: 1.023914	test: 0.895467
MAE train: 0.735094	val: 0.777851	test: 0.716064

Epoch: 24
Loss: 0.769165500998497
RMSE train: 0.817343	val: 0.846952	test: 0.817775
MAE train: 0.609285	val: 0.597141	test: 0.641885

Epoch: 25
Loss: 0.7144945710897446
RMSE train: 0.774843	val: 0.850021	test: 0.761751
MAE train: 0.567388	val: 0.620691	test: 0.587149

Epoch: 26
Loss: 0.6974012106657028
RMSE train: 0.691532	val: 0.876152	test: 0.708632
MAE train: 0.522964	val: 0.626572	test: 0.561692

Epoch: 27
Loss: 0.7537552118301392
RMSE train: 0.689618	val: 0.822461	test: 0.719197
MAE train: 0.520664	val: 0.591620	test: 0.559013

Epoch: 28
Loss: 0.661814957857132
RMSE train: 0.760277	val: 0.847033	test: 0.813890
MAE train: 0.560009	val: 0.601323	test: 0.590374

Epoch: 29
Loss: 0.6517210453748703
RMSE train: 0.612481	val: 0.788867	test: 0.739553
MAE train: 0.450286	val: 0.565080	test: 0.548501

Epoch: 30
Loss: 0.598787859082222
RMSE train: 0.588001	val: 0.771249	test: 0.716852
MAE train: 0.440052	val: 0.577827	test: 0.538763

Epoch: 31
Loss: 0.6296645402908325
RMSE train: 0.611799	val: 0.835383	test: 0.713991
MAE train: 0.457126	val: 0.612008	test: 0.539334

Epoch: 32
Loss: 0.616014838218689
RMSE train: 0.578990	val: 0.793870	test: 0.699068
MAE train: 0.428562	val: 0.587846	test: 0.535430

Epoch: 33
Loss: 0.5638605207204819
RMSE train: 0.643948	val: 0.831094	test: 0.759195
MAE train: 0.475845	val: 0.617961	test: 0.578708

Epoch: 34
Loss: 0.6492932438850403
RMSE train: 0.662747	val: 0.772824	test: 0.736575
MAE train: 0.490586	val: 0.544284	test: 0.548421

Epoch: 35
Loss: 0.6190808713436127
RMSE train: 0.706784	val: 0.831958	test: 0.740104
MAE train: 0.525804	val: 0.623936	test: 0.557671

Epoch: 36
Loss: 0.527427077293396
RMSE train: 0.571580	val: 0.751752	test: 0.689012
MAE train: 0.424816	val: 0.549290	test: 0.508155

Epoch: 37
Loss: 0.5430687367916107
RMSE train: 0.584440	val: 0.757832	test: 0.681477
MAE train: 0.430856	val: 0.561645	test: 0.514770

Epoch: 38
Loss: 0.581327922642231
RMSE train: 0.684866	val: 0.807040	test: 0.748382
MAE train: 0.492973	val: 0.620389	test: 0.544262

Epoch: 39
Loss: 0.5622916668653488
RMSE train: 0.588858	val: 0.753843	test: 0.661714
MAE train: 0.426073	val: 0.548787	test: 0.489223

Epoch: 40
Loss: 0.5533105731010437
RMSE train: 0.576075	val: 0.775308	test: 0.644415
MAE train: 0.423880	val: 0.558237	test: 0.485327

Epoch: 41
Loss: 0.48328061401844025
RMSE train: 0.638724	val: 0.759853	test: 0.703868
MAE train: 0.465447	val: 0.561913	test: 0.516849

Epoch: 42
Loss: 0.5153345763683319
RMSE train: 0.627244	val: 0.696292	test: 0.724781
MAE train: 0.458451	val: 0.502811	test: 0.525775

Epoch: 43
Loss: 0.5175097584724426
RMSE train: 0.561951	val: 0.716832	test: 0.659539
MAE train: 0.408562	val: 0.506224	test: 0.490625

Epoch: 44
Loss: 0.4444264695048332
RMSE train: 0.538559	val: 0.719499	test: 0.655509
MAE train: 0.387816	val: 0.518201	test: 0.477348

Epoch: 45
Loss: 0.52337297052145
RMSE train: 0.574780	val: 0.738815	test: 0.660622
MAE train: 0.415062	val: 0.535014	test: 0.492019

Epoch: 46
Loss: 0.4923015236854553
RMSE train: 0.608721	val: 0.758600	test: 0.690601
MAE train: 0.442051	val: 0.543181	test: 0.519075

Epoch: 47
Loss: 0.46059229224920273
RMSE train: 0.600360	val: 0.762709	test: 0.692135
MAE train: 0.440493	val: 0.548579	test: 0.526799

Epoch: 48
Loss: 0.4235927537083626
RMSE train: 0.622300	val: 0.787911	test: 0.694060
MAE train: 0.455128	val: 0.564113	test: 0.514983

Epoch: 49
Loss: 0.49024897813796997
RMSE train: 0.703351	val: 0.854349	test: 0.737182
MAE train: 0.518755	val: 0.640366	test: 0.543528

Epoch: 50
Loss: 0.4928400069475174
RMSE train: 0.679020	val: 0.776190	test: 0.740462
MAE train: 0.490826	val: 0.577163	test: 0.539150

Epoch: 51
Loss: 0.4767330139875412
RMSE train: 0.594172	val: 0.755895	test: 0.696448
MAE train: 0.435053	val: 0.541857	test: 0.525735

Epoch: 52
Loss: 0.46428847312927246
RMSE train: 0.618122	val: 0.816024	test: 0.712420
MAE train: 0.452594	val: 0.610898	test: 0.533683

Epoch: 53
Loss: 0.4245811700820923
RMSE train: 0.594369	val: 0.719765	test: 0.707721
MAE train: 0.433537	val: 0.540166	test: 0.512902

Epoch: 54
Loss: 0.4264521524310112
RMSE train: 0.641668	val: 0.761588	test: 0.746508
MAE train: 0.468907	val: 0.578610	test: 0.539667

Epoch: 55
Loss: 0.5124359056353569
RMSE train: 0.563200	val: 0.760591	test: 0.693534
MAE train: 0.409520	val: 0.548400	test: 0.518207

Epoch: 56
Loss: 0.5033544450998306
RMSE train: 0.574049	val: 0.693383	test: 0.704832
MAE train: 0.414619	val: 0.525661	test: 0.511130

Epoch: 57
Loss: 0.45212018489837646
RMSE train: 0.680803	val: 0.820423	test: 0.774216
MAE train: 0.497368	val: 0.630602	test: 0.579616

Epoch: 58
Loss: 0.43794671446084976
RMSE train: 0.596559	val: 0.710293	test: 0.720299
MAE train: 0.443123	val: 0.525303	test: 0.532740

Epoch: 59
Loss: 0.48079829663038254
RMSE train: 0.670334	val: 0.789647	test: 0.777176
MAE train: 0.502352	val: 0.600367	test: 0.599052

Epoch: 60
Loss: 0.4313134178519249
RMSE train: 0.643151	val: 0.757491	test: 0.756885
MAE train: 0.480559	val: 0.574051	test: 0.587977

Epoch: 61
Loss: 0.41616299748420715
RMSE train: 0.624838	val: 0.777994	test: 0.730507
MAE train: 0.459647	val: 0.588647	test: 0.562119

Epoch: 62
Loss: 0.4644809663295746
RMSE train: 0.516582	val: 0.692746	test: 0.685447
MAE train: 0.380244	val: 0.521212	test: 0.520330

Epoch: 63
Loss: 0.41547727584838867
RMSE train: 0.522556	val: 0.681610	test: 0.714755
MAE train: 0.385965	val: 0.528888	test: 0.526616

Epoch: 64
Loss: 0.40603965520858765
RMSE train: 0.539988	val: 0.702479	test: 0.674716
MAE train: 0.405427	val: 0.525077	test: 0.505349

Epoch: 65
Loss: 0.48802170902490616
RMSE train: 0.537212	val: 0.665566	test: 0.672014
MAE train: 0.402609	val: 0.482135	test: 0.499397

Epoch: 66
Loss: 0.43864914774894714
RMSE train: 0.621127	val: 0.752361	test: 0.720709
MAE train: 0.454606	val: 0.572531	test: 0.536145

Epoch: 67
Loss: 0.38610272854566574
RMSE train: 0.565752	val: 0.725866	test: 0.683201
MAE train: 0.414085	val: 0.538357	test: 0.503653

Epoch: 68
Loss: 0.41166096180677414
RMSE train: 0.672834	val: 0.838769	test: 0.742145
MAE train: 0.500942	val: 0.646238	test: 0.570387

Epoch: 69
Loss: 0.4080453962087631
RMSE train: 0.595510	val: 0.730503	test: 0.712698
MAE train: 0.437497	val: 0.560214	test: 0.532241

Epoch: 70
Loss: 0.36536208540201187
RMSE train: 0.576797	val: 0.735442	test: 0.699085
MAE train: 0.421139	val: 0.533999	test: 0.516972

Epoch: 71
Loss: 0.4108634367585182
RMSE train: 0.722817	val: 0.859385	test: 0.777921
MAE train: 0.537431	val: 0.661099	test: 0.586192

Epoch: 72
Loss: 0.41265156120061874
RMSE train: 0.490846	val: 0.698834	test: 0.657363
MAE train: 0.353501	val: 0.517344	test: 0.490757

Epoch: 73
Loss: 0.40725135803222656
RMSE train: 0.471388	val: 0.706373	test: 0.628593
MAE train: 0.348219	val: 0.523235	test: 0.483067

Epoch: 74
Loss: 0.39959636330604553
RMSE train: 0.472130	val: 0.703420	test: 0.657969
MAE train: 0.354996	val: 0.517966	test: 0.495079

Epoch: 75
Loss: 0.43907640874385834
RMSE train: 0.489557	val: 0.680676	test: 0.682698
MAE train: 0.366222	val: 0.523620	test: 0.517683

Epoch: 76
Loss: 0.3956083506345749
RMSE train: 0.505395	val: 0.710096	test: 0.658835
MAE train: 0.371274	val: 0.533988	test: 0.468752

Epoch: 77
Loss: 0.3649934306740761
RMSE train: 0.477491	val: 0.702175	test: 0.622445
MAE train: 0.364746	val: 0.512881	test: 0.473711

Epoch: 78
Loss: 0.39706799387931824
RMSE train: 0.540997	val: 0.747328	test: 0.666479
MAE train: 0.407074	val: 0.549254	test: 0.497547

Epoch: 79
Loss: 0.4130033999681473
RMSE train: 0.508258	val: 0.667943	test: 0.658517
MAE train: 0.379883	val: 0.513309	test: 0.499034

Epoch: 80
Loss: 0.4721979647874832
RMSE train: 0.511522	val: 0.729864	test: 0.632616
MAE train: 0.382216	val: 0.537058	test: 0.478840

Epoch: 81
Loss: 0.37328536063432693
RMSE train: 0.641779	val: 0.884343	test: 0.723546
MAE train: 0.476369	val: 0.661246	test: 0.545645

Epoch: 82
Loss: 0.3470505326986313
RMSE train: 0.493853	val: 0.661800	test: 0.678439
MAE train: 0.366254	val: 0.499952	test: 0.514013

Epoch: 83
Loss: 0.34856463223695755
RMSE train: 0.491237	val: 0.715792	test: 0.639441
MAE train: 0.361605	val: 0.532027	test: 0.484595
RMSE train: 0.924440	val: 1.100665	test: 0.885335
MAE train: 0.699074	val: 0.836593	test: 0.680423

Epoch: 24
Loss: 0.7506385743618011
RMSE train: 0.728804	val: 0.882879	test: 0.771476
MAE train: 0.534672	val: 0.657885	test: 0.575912

Epoch: 25
Loss: 0.7127251625061035
RMSE train: 0.761532	val: 0.905555	test: 0.794485
MAE train: 0.561437	val: 0.660314	test: 0.594889

Epoch: 26
Loss: 0.6814508140087128
RMSE train: 0.809377	val: 0.970946	test: 0.818538
MAE train: 0.600730	val: 0.707465	test: 0.619824

Epoch: 27
Loss: 0.604017972946167
RMSE train: 0.643601	val: 0.795867	test: 0.764856
MAE train: 0.484192	val: 0.584731	test: 0.592244

Epoch: 28
Loss: 0.6632215678691864
RMSE train: 0.828917	val: 1.005837	test: 0.826544
MAE train: 0.613069	val: 0.763711	test: 0.620710

Epoch: 29
Loss: 0.6429786682128906
RMSE train: 0.685208	val: 0.821048	test: 0.751764
MAE train: 0.512510	val: 0.599782	test: 0.577474

Epoch: 30
Loss: 0.5962908715009689
RMSE train: 0.750476	val: 0.971373	test: 0.822853
MAE train: 0.556149	val: 0.727938	test: 0.625664

Epoch: 31
Loss: 0.5679295063018799
RMSE train: 0.716204	val: 0.886768	test: 0.762481
MAE train: 0.539109	val: 0.669980	test: 0.587136

Epoch: 32
Loss: 0.6015844494104385
RMSE train: 0.647630	val: 0.793733	test: 0.724848
MAE train: 0.476892	val: 0.598698	test: 0.547715

Epoch: 33
Loss: 0.6062398254871368
RMSE train: 0.719537	val: 0.829963	test: 0.804095
MAE train: 0.541114	val: 0.643939	test: 0.619500

Epoch: 34
Loss: 0.5825300216674805
RMSE train: 0.815348	val: 0.889884	test: 0.848883
MAE train: 0.620429	val: 0.661502	test: 0.647126

Epoch: 35
Loss: 0.5638616234064102
RMSE train: 0.756855	val: 0.911508	test: 0.797686
MAE train: 0.574187	val: 0.693757	test: 0.625929

Epoch: 36
Loss: 0.5358550772070885
RMSE train: 0.704879	val: 0.857547	test: 0.772277
MAE train: 0.542033	val: 0.646455	test: 0.616818

Epoch: 37
Loss: 0.5314360707998276
RMSE train: 0.741030	val: 0.865598	test: 0.776590
MAE train: 0.561302	val: 0.638674	test: 0.599756

Epoch: 38
Loss: 0.5739543586969376
RMSE train: 0.713500	val: 0.866409	test: 0.768975
MAE train: 0.530569	val: 0.650553	test: 0.591696

Epoch: 39
Loss: 0.5199970081448555
RMSE train: 0.566427	val: 0.747060	test: 0.690263
MAE train: 0.423921	val: 0.543058	test: 0.539912

Epoch: 40
Loss: 0.5765728801488876
RMSE train: 0.690499	val: 0.857015	test: 0.712041
MAE train: 0.506494	val: 0.640370	test: 0.533709

Epoch: 41
Loss: 0.5287105664610863
RMSE train: 0.639306	val: 0.786170	test: 0.697175
MAE train: 0.470778	val: 0.593221	test: 0.524428

Epoch: 42
Loss: 0.5607140883803368
RMSE train: 0.637660	val: 0.791225	test: 0.697899
MAE train: 0.476907	val: 0.600788	test: 0.535193

Epoch: 43
Loss: 0.5386121347546577
RMSE train: 0.662526	val: 0.800021	test: 0.723777
MAE train: 0.494565	val: 0.590000	test: 0.541005

Epoch: 44
Loss: 0.573777474462986
RMSE train: 0.540456	val: 0.754434	test: 0.699497
MAE train: 0.399808	val: 0.582854	test: 0.539013

Epoch: 45
Loss: 0.48334912210702896
RMSE train: 0.665352	val: 0.857106	test: 0.726877
MAE train: 0.492326	val: 0.661968	test: 0.540058

Epoch: 46
Loss: 0.4857023134827614
RMSE train: 0.711945	val: 0.879875	test: 0.748919
MAE train: 0.525098	val: 0.671626	test: 0.561301

Epoch: 47
Loss: 0.5617599040269852
RMSE train: 0.605440	val: 0.805867	test: 0.700102
MAE train: 0.436607	val: 0.619559	test: 0.526087

Epoch: 48
Loss: 0.47277478873729706
RMSE train: 0.636862	val: 0.877097	test: 0.714491
MAE train: 0.475344	val: 0.668920	test: 0.540483

Epoch: 49
Loss: 0.5155346915125847
RMSE train: 0.528992	val: 0.711689	test: 0.646753
MAE train: 0.392437	val: 0.510252	test: 0.482909

Epoch: 50
Loss: 0.5108247920870781
RMSE train: 0.658928	val: 0.860092	test: 0.736679
MAE train: 0.491545	val: 0.667721	test: 0.563646

Epoch: 51
Loss: 0.4668149799108505
RMSE train: 0.668553	val: 0.845860	test: 0.731440
MAE train: 0.497308	val: 0.639517	test: 0.554117

Epoch: 52
Loss: 0.4525053948163986
RMSE train: 0.564993	val: 0.713841	test: 0.672387
MAE train: 0.417887	val: 0.522645	test: 0.498055

Epoch: 53
Loss: 0.4510822147130966
RMSE train: 0.585224	val: 0.803292	test: 0.672763
MAE train: 0.428867	val: 0.611620	test: 0.493614

Epoch: 54
Loss: 0.4728897958993912
RMSE train: 0.525488	val: 0.687063	test: 0.672292
MAE train: 0.385025	val: 0.509419	test: 0.491930

Epoch: 55
Loss: 0.42949479073286057
RMSE train: 0.640739	val: 0.773636	test: 0.733878
MAE train: 0.474155	val: 0.583548	test: 0.540173

Epoch: 56
Loss: 0.5313983708620071
RMSE train: 0.621364	val: 0.819023	test: 0.724793
MAE train: 0.451519	val: 0.628553	test: 0.539705

Epoch: 57
Loss: 0.4810214936733246
RMSE train: 0.521816	val: 0.657436	test: 0.696192
MAE train: 0.391322	val: 0.485297	test: 0.527306

Epoch: 58
Loss: 0.4359445944428444
RMSE train: 0.663269	val: 0.853327	test: 0.736321
MAE train: 0.505347	val: 0.639453	test: 0.548959

Epoch: 59
Loss: 0.4680781438946724
RMSE train: 0.477353	val: 0.727746	test: 0.639865
MAE train: 0.355852	val: 0.529420	test: 0.466321

Epoch: 60
Loss: 0.4594542235136032
RMSE train: 0.485059	val: 0.697477	test: 0.629631
MAE train: 0.361303	val: 0.518333	test: 0.467887

Epoch: 61
Loss: 0.43239450454711914
RMSE train: 0.553998	val: 0.806269	test: 0.656199
MAE train: 0.406974	val: 0.616111	test: 0.485668

Epoch: 62
Loss: 0.41724687069654465
RMSE train: 0.466878	val: 0.720741	test: 0.627950
MAE train: 0.349478	val: 0.546407	test: 0.467157

Epoch: 63
Loss: 0.4081217125058174
RMSE train: 0.491233	val: 0.767104	test: 0.625471
MAE train: 0.359642	val: 0.592519	test: 0.465008

Epoch: 64
Loss: 0.43546269088983536
RMSE train: 0.543013	val: 0.763187	test: 0.649682
MAE train: 0.394843	val: 0.588051	test: 0.477738

Epoch: 65
Loss: 0.39970438182353973
RMSE train: 0.501255	val: 0.680906	test: 0.642658
MAE train: 0.367556	val: 0.502677	test: 0.480146

Epoch: 66
Loss: 0.4258148968219757
RMSE train: 0.473160	val: 0.679926	test: 0.637111
MAE train: 0.340366	val: 0.505621	test: 0.471014

Epoch: 67
Loss: 0.3637614920735359
RMSE train: 0.527460	val: 0.736585	test: 0.657002
MAE train: 0.385956	val: 0.555839	test: 0.492657

Epoch: 68
Loss: 0.3970666155219078
RMSE train: 0.580022	val: 0.773418	test: 0.696015
MAE train: 0.432750	val: 0.589524	test: 0.533217

Epoch: 69
Loss: 0.43032195419073105
RMSE train: 0.547223	val: 0.711324	test: 0.680556
MAE train: 0.402798	val: 0.542070	test: 0.508295

Epoch: 70
Loss: 0.42332910746335983
RMSE train: 0.514678	val: 0.668046	test: 0.667673
MAE train: 0.379025	val: 0.501658	test: 0.492232

Epoch: 71
Loss: 0.38914240151643753
RMSE train: 0.624259	val: 0.805682	test: 0.723187
MAE train: 0.462419	val: 0.622249	test: 0.553687

Epoch: 72
Loss: 0.3899478018283844
RMSE train: 0.500176	val: 0.649704	test: 0.675810
MAE train: 0.369648	val: 0.480861	test: 0.501436

Epoch: 73
Loss: 0.3568436950445175
RMSE train: 0.471397	val: 0.638272	test: 0.667135
MAE train: 0.349609	val: 0.464977	test: 0.497035

Epoch: 74
Loss: 0.40203822404146194
RMSE train: 0.534908	val: 0.729106	test: 0.676543
MAE train: 0.391519	val: 0.550174	test: 0.504627

Epoch: 75
Loss: 0.39760372787714005
RMSE train: 0.515335	val: 0.733225	test: 0.655238
MAE train: 0.383837	val: 0.549871	test: 0.490509

Epoch: 76
Loss: 0.3678932338953018
RMSE train: 0.460987	val: 0.650239	test: 0.647942
MAE train: 0.348798	val: 0.485863	test: 0.489018

Epoch: 77
Loss: 0.39866098016500473
RMSE train: 0.570745	val: 0.748375	test: 0.696852
MAE train: 0.417023	val: 0.586813	test: 0.515227

Epoch: 78
Loss: 0.3714936897158623
RMSE train: 0.556041	val: 0.728456	test: 0.685103
MAE train: 0.409472	val: 0.565110	test: 0.511670

Epoch: 79
Loss: 0.3690011501312256
RMSE train: 0.499611	val: 0.687738	test: 0.642725
MAE train: 0.368431	val: 0.525635	test: 0.474020

Epoch: 80
Loss: 0.3671954721212387
RMSE train: 0.512861	val: 0.658272	test: 0.670465
MAE train: 0.375511	val: 0.497452	test: 0.482814

Epoch: 81
Loss: 0.37803152948617935
RMSE train: 0.563765	val: 0.734184	test: 0.692521
MAE train: 0.419582	val: 0.571089	test: 0.511828

Epoch: 82
Loss: 0.39394254237413406
RMSE train: 0.425759	val: 0.646357	test: 0.638690
MAE train: 0.314312	val: 0.477048	test: 0.472077

Epoch: 83
Loss: 0.3960076719522476
RMSE train: 0.464707	val: 0.698278	test: 0.654374
MAE train: 0.341067	val: 0.526109	test: 0.481288

Epoch: 84
Loss: 0.37162424127260846
RMSE train: 0.667672	val: 0.876896	test: 0.815447
MAE train: 0.496743	val: 0.620496	test: 0.606510

Epoch: 85
Loss: 0.37823357184727985
RMSE train: 0.514793	val: 0.783357	test: 0.737957
MAE train: 0.381363	val: 0.540727	test: 0.540757

Epoch: 86
Loss: 0.3727617661158244
RMSE train: 0.504317	val: 0.765290	test: 0.711092
MAE train: 0.381083	val: 0.548653	test: 0.518561

Epoch: 87
Loss: 0.4401888648668925
RMSE train: 0.641399	val: 0.844705	test: 0.807334
MAE train: 0.463219	val: 0.598264	test: 0.601018

Epoch: 88
Loss: 0.4205925365289052
RMSE train: 0.584915	val: 0.827725	test: 0.776875
MAE train: 0.427920	val: 0.591400	test: 0.579443

Epoch: 89
Loss: 0.36699087421099347
RMSE train: 0.441034	val: 0.750275	test: 0.694110
MAE train: 0.330559	val: 0.526764	test: 0.517220

Epoch: 90
Loss: 0.3514243463675181
RMSE train: 0.437291	val: 0.737819	test: 0.698627
MAE train: 0.325431	val: 0.518129	test: 0.520878

Epoch: 91
Loss: 0.3472677171230316
RMSE train: 0.482733	val: 0.749365	test: 0.735263
MAE train: 0.362265	val: 0.537469	test: 0.546974

Epoch: 92
Loss: 0.3806304434935252
RMSE train: 0.420892	val: 0.717823	test: 0.705029
MAE train: 0.310392	val: 0.506289	test: 0.518726

Epoch: 93
Loss: 0.37253716588020325
RMSE train: 0.448830	val: 0.731846	test: 0.707563
MAE train: 0.333685	val: 0.514346	test: 0.518326

Epoch: 94
Loss: 0.3543134033679962
RMSE train: 0.535845	val: 0.790626	test: 0.771860
MAE train: 0.394327	val: 0.553436	test: 0.565420

Epoch: 95
Loss: 0.398629625638326
RMSE train: 0.513507	val: 0.773818	test: 0.744133
MAE train: 0.379499	val: 0.534660	test: 0.544816

Epoch: 96
Loss: 0.38757920265197754
RMSE train: 0.473711	val: 0.747397	test: 0.714530
MAE train: 0.349469	val: 0.518930	test: 0.523995

Epoch: 97
Loss: 0.33482130368550617
RMSE train: 0.548838	val: 0.787269	test: 0.757990
MAE train: 0.395957	val: 0.552624	test: 0.557284

Epoch: 98
Loss: 0.29042301575342816
RMSE train: 0.630883	val: 0.841655	test: 0.803676
MAE train: 0.464473	val: 0.598546	test: 0.602687

Epoch: 99
Loss: 0.343095729748408
RMSE train: 0.470090	val: 0.742746	test: 0.704288
MAE train: 0.346913	val: 0.515888	test: 0.517770

Epoch: 100
Loss: 0.30797561009724933
RMSE train: 0.467304	val: 0.745246	test: 0.707132
MAE train: 0.344947	val: 0.517274	test: 0.523584

Epoch: 101
Loss: 0.3561405340830485
RMSE train: 0.574436	val: 0.805330	test: 0.764674
MAE train: 0.418768	val: 0.567497	test: 0.578598

Epoch: 102
Loss: 0.3246256212393443
RMSE train: 0.588800	val: 0.810746	test: 0.759455
MAE train: 0.436438	val: 0.573024	test: 0.575091

Epoch: 103
Loss: 0.2970428665479024
RMSE train: 0.543802	val: 0.783933	test: 0.736410
MAE train: 0.411444	val: 0.553208	test: 0.553992

Epoch: 104
Loss: 0.34751759966214496
RMSE train: 0.638290	val: 0.849897	test: 0.799274
MAE train: 0.476567	val: 0.598796	test: 0.609701

Epoch: 105
Loss: 0.34693174560864765
RMSE train: 0.599388	val: 0.831193	test: 0.785002
MAE train: 0.440768	val: 0.586499	test: 0.592068

Epoch: 106
Loss: 0.3195617000261943
RMSE train: 0.555464	val: 0.799240	test: 0.762879
MAE train: 0.406379	val: 0.564147	test: 0.574763

Epoch: 107
Loss: 0.2881852289040883
RMSE train: 0.482699	val: 0.758083	test: 0.729318
MAE train: 0.357703	val: 0.529103	test: 0.548336

Epoch: 108
Loss: 0.31106828649838764
RMSE train: 0.478392	val: 0.755981	test: 0.732086
MAE train: 0.351926	val: 0.530746	test: 0.549400

Epoch: 109
Loss: 0.33483676115671795
RMSE train: 0.503935	val: 0.778507	test: 0.744343
MAE train: 0.362920	val: 0.549652	test: 0.554090

Epoch: 110
Loss: 0.33225831389427185
RMSE train: 0.419379	val: 0.747952	test: 0.700561
MAE train: 0.307784	val: 0.518217	test: 0.514389

Epoch: 111
Loss: 0.31357784072558087
RMSE train: 0.459336	val: 0.753971	test: 0.727537
MAE train: 0.338663	val: 0.525854	test: 0.537504

Epoch: 112
Loss: 0.34642969568570453
RMSE train: 0.551090	val: 0.781629	test: 0.773797
MAE train: 0.411618	val: 0.553062	test: 0.574313

Epoch: 113
Loss: 0.3088941077391307
RMSE train: 0.460854	val: 0.730662	test: 0.727256
MAE train: 0.336477	val: 0.509056	test: 0.532749

Epoch: 114
Loss: 0.3055723011493683
RMSE train: 0.521082	val: 0.764482	test: 0.738300
MAE train: 0.372033	val: 0.535437	test: 0.544989

Epoch: 115
Loss: 0.31091225147247314
RMSE train: 0.574314	val: 0.803881	test: 0.777154
MAE train: 0.400581	val: 0.560212	test: 0.576879

Epoch: 116
Loss: 0.2965217133363088
RMSE train: 0.506233	val: 0.771279	test: 0.730065
MAE train: 0.363132	val: 0.540184	test: 0.543558

Epoch: 117
Loss: 0.277563417951266
RMSE train: 0.459673	val: 0.744540	test: 0.716375
MAE train: 0.340041	val: 0.529085	test: 0.536672

Epoch: 118
Loss: 0.3406064013640086
RMSE train: 0.421099	val: 0.733877	test: 0.726371
MAE train: 0.315163	val: 0.523623	test: 0.547540

Epoch: 119
Loss: 0.34003937244415283
RMSE train: 0.454189	val: 0.760851	test: 0.731839
MAE train: 0.337625	val: 0.532188	test: 0.548687

Epoch: 120
Loss: 0.3029762903849284
RMSE train: 0.443119	val: 0.770071	test: 0.726217
MAE train: 0.332777	val: 0.526559	test: 0.534977

Epoch: 121
Loss: 0.2693633735179901
RMSE train: 0.407496	val: 0.754934	test: 0.722850
MAE train: 0.307913	val: 0.513948	test: 0.526229

Early stopping
Best (RMSE):	 train: 0.442641	val: 0.705841	test: 0.702065
Best (MAE):	 train: 0.331325	val: 0.496954	test: 0.509557


Epoch: 84
Loss: 0.35648898283640545
RMSE train: 0.448441	val: 0.723545	test: 0.681748
MAE train: 0.326275	val: 0.516995	test: 0.493035

Epoch: 85
Loss: 0.3787144422531128
RMSE train: 0.422637	val: 0.719684	test: 0.667288
MAE train: 0.311926	val: 0.517050	test: 0.483964

Epoch: 86
Loss: 0.39550937215487164
RMSE train: 0.526996	val: 0.761722	test: 0.719211
MAE train: 0.389141	val: 0.544965	test: 0.524083

Epoch: 87
Loss: 0.3608246445655823
RMSE train: 0.504553	val: 0.754881	test: 0.699680
MAE train: 0.375751	val: 0.549000	test: 0.512118

Epoch: 88
Loss: 0.3261079589525859
RMSE train: 0.440692	val: 0.727228	test: 0.676560
MAE train: 0.329207	val: 0.522676	test: 0.498029

Epoch: 89
Loss: 0.3560613791147868
RMSE train: 0.522188	val: 0.756725	test: 0.731912
MAE train: 0.395932	val: 0.536784	test: 0.544031

Epoch: 90
Loss: 0.3650389015674591
RMSE train: 0.504062	val: 0.757081	test: 0.718676
MAE train: 0.377228	val: 0.540619	test: 0.526558

Epoch: 91
Loss: 0.33603451649347943
RMSE train: 0.448752	val: 0.744462	test: 0.709105
MAE train: 0.337677	val: 0.538847	test: 0.519818

Epoch: 92
Loss: 0.33535751700401306
RMSE train: 0.459840	val: 0.737247	test: 0.748811
MAE train: 0.350240	val: 0.535940	test: 0.539844

Epoch: 93
Loss: 0.3484342694282532
RMSE train: 0.454862	val: 0.738177	test: 0.724829
MAE train: 0.339707	val: 0.529656	test: 0.523653

Epoch: 94
Loss: 0.3291790386041005
RMSE train: 0.471129	val: 0.749617	test: 0.723350
MAE train: 0.349533	val: 0.538219	test: 0.527517

Epoch: 95
Loss: 0.3995869954427083
RMSE train: 0.508527	val: 0.756967	test: 0.755554
MAE train: 0.372508	val: 0.543177	test: 0.541983

Epoch: 96
Loss: 0.38666726152102154
RMSE train: 0.478450	val: 0.736133	test: 0.735679
MAE train: 0.352432	val: 0.523333	test: 0.528752

Epoch: 97
Loss: 0.3644519845644633
RMSE train: 0.494469	val: 0.738537	test: 0.744011
MAE train: 0.369479	val: 0.523647	test: 0.548980

Epoch: 98
Loss: 0.34814779957135517
RMSE train: 0.427067	val: 0.711579	test: 0.701805
MAE train: 0.316837	val: 0.500313	test: 0.512904

Epoch: 99
Loss: 0.3156270384788513
RMSE train: 0.393676	val: 0.715353	test: 0.691799
MAE train: 0.294224	val: 0.508276	test: 0.500332

Epoch: 100
Loss: 0.32793978850046795
RMSE train: 0.441389	val: 0.727702	test: 0.689084
MAE train: 0.323217	val: 0.514495	test: 0.499180

Epoch: 101
Loss: 0.36906638741493225
RMSE train: 0.461599	val: 0.733194	test: 0.695537
MAE train: 0.342370	val: 0.529308	test: 0.504949

Epoch: 102
Loss: 0.36221280694007874
RMSE train: 0.433643	val: 0.715590	test: 0.689835
MAE train: 0.321749	val: 0.512874	test: 0.497622

Epoch: 103
Loss: 0.356711486975352
RMSE train: 0.539588	val: 0.758808	test: 0.768535
MAE train: 0.403602	val: 0.546026	test: 0.570422

Epoch: 104
Loss: 0.3644961416721344
RMSE train: 0.447149	val: 0.714471	test: 0.708271
MAE train: 0.329434	val: 0.505031	test: 0.508999

Epoch: 105
Loss: 0.3722927471001943
RMSE train: 0.476205	val: 0.758491	test: 0.700483
MAE train: 0.362605	val: 0.553304	test: 0.520753

Epoch: 106
Loss: 0.36757312218348187
RMSE train: 0.488458	val: 0.759767	test: 0.737047
MAE train: 0.369564	val: 0.544894	test: 0.544449

Epoch: 107
Loss: 0.33046961824099225
RMSE train: 0.419776	val: 0.734680	test: 0.721261
MAE train: 0.311341	val: 0.518746	test: 0.521955

Epoch: 108
Loss: 0.332035889228185
RMSE train: 0.397019	val: 0.731301	test: 0.699616
MAE train: 0.301931	val: 0.533449	test: 0.516465

Epoch: 109
Loss: 0.38379226128260296
RMSE train: 0.471958	val: 0.748470	test: 0.714833
MAE train: 0.354815	val: 0.556924	test: 0.533158

Epoch: 110
Loss: 0.33363161484400433
RMSE train: 0.400586	val: 0.716399	test: 0.698871
MAE train: 0.298289	val: 0.526164	test: 0.511494

Epoch: 111
Loss: 0.2760112484296163
RMSE train: 0.381916	val: 0.722889	test: 0.715192
MAE train: 0.290701	val: 0.527840	test: 0.521255

Epoch: 112
Loss: 0.31772826115290326
RMSE train: 0.388794	val: 0.705354	test: 0.688022
MAE train: 0.292826	val: 0.503081	test: 0.495655

Epoch: 113
Loss: 0.34079041083653766
RMSE train: 0.426562	val: 0.716699	test: 0.680732
MAE train: 0.314244	val: 0.510798	test: 0.492859

Epoch: 114
Loss: 0.32377297679583233
RMSE train: 0.398447	val: 0.709269	test: 0.675375
MAE train: 0.296426	val: 0.502786	test: 0.486315

Epoch: 115
Loss: 0.2956888973712921
RMSE train: 0.384921	val: 0.711519	test: 0.684807
MAE train: 0.290423	val: 0.500363	test: 0.486900

Epoch: 116
Loss: 0.2795076270898183
RMSE train: 0.387457	val: 0.726087	test: 0.674805
MAE train: 0.289644	val: 0.517580	test: 0.483095

Epoch: 117
Loss: 0.27981407940387726
RMSE train: 0.418574	val: 0.741519	test: 0.677873
MAE train: 0.308308	val: 0.530751	test: 0.496219

Epoch: 118
Loss: 0.32923227548599243
RMSE train: 0.462577	val: 0.748308	test: 0.699713
MAE train: 0.340161	val: 0.540398	test: 0.516518

Epoch: 119
Loss: 0.3312291403611501
RMSE train: 0.454133	val: 0.729138	test: 0.723417
MAE train: 0.336216	val: 0.517535	test: 0.527299

Epoch: 120
Loss: 0.33664169907569885
RMSE train: 0.371907	val: 0.706046	test: 0.700581
MAE train: 0.278025	val: 0.495796	test: 0.506239

Epoch: 121
Loss: 0.3111180265744527
RMSE train: 0.467791	val: 0.744374	test: 0.712321
MAE train: 0.348336	val: 0.530727	test: 0.523876

Early stopping
Best (RMSE):	 train: 0.454344	val: 0.689787	test: 0.709255
Best (MAE):	 train: 0.332964	val: 0.501237	test: 0.506693


Epoch: 84
Loss: 0.38573439915974933
RMSE train: 0.475961	val: 0.759243	test: 0.694204
MAE train: 0.361831	val: 0.529292	test: 0.496425

Epoch: 85
Loss: 0.3791897694269816
RMSE train: 0.461966	val: 0.769943	test: 0.697856
MAE train: 0.348403	val: 0.516538	test: 0.503858

Epoch: 86
Loss: 0.36938581864039105
RMSE train: 0.497596	val: 0.780545	test: 0.716790
MAE train: 0.362525	val: 0.530516	test: 0.520131

Epoch: 87
Loss: 0.34753285845120746
RMSE train: 0.461535	val: 0.754424	test: 0.695456
MAE train: 0.340214	val: 0.536949	test: 0.506412

Epoch: 88
Loss: 0.3678140739599864
RMSE train: 0.431460	val: 0.726261	test: 0.706095
MAE train: 0.323289	val: 0.528319	test: 0.524819

Epoch: 89
Loss: 0.33217284083366394
RMSE train: 0.431050	val: 0.724397	test: 0.716588
MAE train: 0.319926	val: 0.513604	test: 0.534310

Epoch: 90
Loss: 0.366598645846049
RMSE train: 0.465915	val: 0.740106	test: 0.714860
MAE train: 0.339943	val: 0.511570	test: 0.529252

Epoch: 91
Loss: 0.3561036189397176
RMSE train: 0.496122	val: 0.765125	test: 0.719106
MAE train: 0.357144	val: 0.523030	test: 0.525944

Epoch: 92
Loss: 0.3747228781382243
RMSE train: 0.485531	val: 0.754885	test: 0.716644
MAE train: 0.350321	val: 0.516982	test: 0.511569

Epoch: 93
Loss: 0.4336385826269786
RMSE train: 0.501258	val: 0.768255	test: 0.750150
MAE train: 0.367405	val: 0.537450	test: 0.542811

Epoch: 94
Loss: 0.32576172550519306
RMSE train: 0.422858	val: 0.736457	test: 0.719173
MAE train: 0.308218	val: 0.514106	test: 0.525191

Epoch: 95
Loss: 0.33844104409217834
RMSE train: 0.425825	val: 0.749743	test: 0.714207
MAE train: 0.310531	val: 0.522782	test: 0.518091

Epoch: 96
Loss: 0.3715624511241913
RMSE train: 0.504141	val: 0.789074	test: 0.750949
MAE train: 0.376462	val: 0.552377	test: 0.548236

Epoch: 97
Loss: 0.3817310333251953
RMSE train: 0.453566	val: 0.758510	test: 0.731559
MAE train: 0.337415	val: 0.522528	test: 0.531707

Epoch: 98
Loss: 0.38274602095286053
RMSE train: 0.400671	val: 0.717290	test: 0.685131
MAE train: 0.297957	val: 0.491857	test: 0.503461

Epoch: 99
Loss: 0.3497339189052582
RMSE train: 0.445718	val: 0.732227	test: 0.694822
MAE train: 0.328096	val: 0.519675	test: 0.505339

Epoch: 100
Loss: 0.3604423900445302
RMSE train: 0.431147	val: 0.725106	test: 0.715536
MAE train: 0.321109	val: 0.510600	test: 0.521090

Epoch: 101
Loss: 0.3442934950192769
RMSE train: 0.370693	val: 0.701490	test: 0.709333
MAE train: 0.284143	val: 0.484922	test: 0.509366

Epoch: 102
Loss: 0.3096545338630676
RMSE train: 0.377274	val: 0.701342	test: 0.704135
MAE train: 0.288178	val: 0.486771	test: 0.511996

Epoch: 103
Loss: 0.34894631306330365
RMSE train: 0.421033	val: 0.722945	test: 0.696097
MAE train: 0.315819	val: 0.506402	test: 0.505642

Epoch: 104
Loss: 0.3278242846330007
RMSE train: 0.450171	val: 0.746779	test: 0.713074
MAE train: 0.335468	val: 0.526982	test: 0.527631

Epoch: 105
Loss: 0.405225137869517
RMSE train: 0.370868	val: 0.734716	test: 0.679974
MAE train: 0.283638	val: 0.508890	test: 0.500564

Epoch: 106
Loss: 0.30827949444452923
RMSE train: 0.409331	val: 0.752430	test: 0.670643
MAE train: 0.312118	val: 0.530432	test: 0.491512

Epoch: 107
Loss: 0.3656226595242818
RMSE train: 0.473649	val: 0.769683	test: 0.701959
MAE train: 0.349331	val: 0.542509	test: 0.518961

Epoch: 108
Loss: 0.339765598376592
RMSE train: 0.448626	val: 0.759262	test: 0.701690
MAE train: 0.328023	val: 0.526029	test: 0.518398

Epoch: 109
Loss: 0.342070996761322
RMSE train: 0.415376	val: 0.742995	test: 0.683069
MAE train: 0.305831	val: 0.507249	test: 0.501428

Epoch: 110
Loss: 0.33358845114707947
RMSE train: 0.423685	val: 0.744219	test: 0.678073
MAE train: 0.320759	val: 0.511678	test: 0.489659

Epoch: 111
Loss: 0.31942031780878705
RMSE train: 0.473544	val: 0.767222	test: 0.718814
MAE train: 0.359523	val: 0.529953	test: 0.516194

Epoch: 112
Loss: 0.31829412778218585
RMSE train: 0.429663	val: 0.743390	test: 0.728279
MAE train: 0.318204	val: 0.513912	test: 0.530100

Epoch: 113
Loss: 0.3418494164943695
RMSE train: 0.405254	val: 0.740025	test: 0.714513
MAE train: 0.299918	val: 0.510547	test: 0.524917

Epoch: 114
Loss: 0.31285186608632404
RMSE train: 0.423176	val: 0.742142	test: 0.705624
MAE train: 0.309299	val: 0.516576	test: 0.513235

Epoch: 115
Loss: 0.2814313471317291
RMSE train: 0.487099	val: 0.778203	test: 0.727706
MAE train: 0.361330	val: 0.542895	test: 0.530911

Epoch: 116
Loss: 0.3091116050879161
RMSE train: 0.416783	val: 0.747166	test: 0.705304
MAE train: 0.307678	val: 0.506714	test: 0.515837

Epoch: 117
Loss: 0.3224697709083557
RMSE train: 0.458565	val: 0.760067	test: 0.722356
MAE train: 0.339946	val: 0.522134	test: 0.535765

Epoch: 118
Loss: 0.3052864571412404
RMSE train: 0.576591	val: 0.831625	test: 0.761695
MAE train: 0.431591	val: 0.587335	test: 0.563060

Epoch: 119
Loss: 0.2966351906458537
RMSE train: 0.596058	val: 0.846083	test: 0.782941
MAE train: 0.446296	val: 0.605097	test: 0.582450

Epoch: 120
Loss: 0.29213937123616535
RMSE train: 0.466802	val: 0.773314	test: 0.723324
MAE train: 0.345236	val: 0.540855	test: 0.533214

Epoch: 121
Loss: 0.2966797749201457
RMSE train: 0.404149	val: 0.746484	test: 0.714413
MAE train: 0.301403	val: 0.517854	test: 0.509546

Epoch: 122
Loss: 0.3133574227492015
RMSE train: 0.455698	val: 0.770215	test: 0.717832
MAE train: 0.341477	val: 0.544312	test: 0.505535

Epoch: 123
Loss: 0.2955135901769002
RMSE train: 0.422556	val: 0.747021	test: 0.694459
MAE train: 0.314274	val: 0.522680	test: 0.499361

Epoch: 124
Loss: 0.30508819222450256
RMSE train: 0.475129	val: 0.766203	test: 0.729804
MAE train: 0.341290	val: 0.540489	test: 0.543220

Epoch: 125
Loss: 0.29772689938545227
RMSE train: 0.406654	val: 0.743729	test: 0.695572
MAE train: 0.296561	val: 0.519472	test: 0.513252

Epoch: 126
Loss: 0.27907779812812805
RMSE train: 0.392281	val: 0.755697	test: 0.687523
MAE train: 0.292467	val: 0.520233	test: 0.498398

Epoch: 127
Loss: 0.3150058885415395
RMSE train: 0.395348	val: 0.758211	test: 0.699760
MAE train: 0.292273	val: 0.514839	test: 0.509589

Epoch: 128
Loss: 0.31555067499478656
RMSE train: 0.399740	val: 0.754054	test: 0.704286
MAE train: 0.291399	val: 0.512892	test: 0.516050

Epoch: 129
Loss: 0.28943687677383423
RMSE train: 0.401999	val: 0.761571	test: 0.683885
MAE train: 0.299977	val: 0.533183	test: 0.504545

Epoch: 130
Loss: 0.28605236609776813
RMSE train: 0.455469	val: 0.765608	test: 0.718385
MAE train: 0.335536	val: 0.534687	test: 0.523069

Epoch: 131
Loss: 0.28404513994852704
RMSE train: 0.372831	val: 0.727760	test: 0.692674
MAE train: 0.273636	val: 0.506046	test: 0.499505

Epoch: 132
Loss: 0.24381876488526663
RMSE train: 0.364381	val: 0.727613	test: 0.704352
MAE train: 0.273825	val: 0.505698	test: 0.509036

Epoch: 133
Loss: 0.28630497058232623
RMSE train: 0.408520	val: 0.749174	test: 0.709538
MAE train: 0.299969	val: 0.519323	test: 0.515705

Epoch: 134
Loss: 0.2727053066094716
RMSE train: 0.401532	val: 0.746612	test: 0.702029
MAE train: 0.295919	val: 0.519063	test: 0.515330

Epoch: 135
Loss: 0.3079914152622223
RMSE train: 0.403139	val: 0.730961	test: 0.699476
MAE train: 0.298575	val: 0.512569	test: 0.522933

Epoch: 136
Loss: 0.27533607681592304
RMSE train: 0.417692	val: 0.734548	test: 0.686133
MAE train: 0.311597	val: 0.519761	test: 0.510907

Epoch: 137
Loss: 0.3075153628985087
RMSE train: 0.451485	val: 0.747378	test: 0.725071
MAE train: 0.331959	val: 0.518168	test: 0.530448

Early stopping
Best (RMSE):	 train: 0.377274	val: 0.701342	test: 0.704135
Best (MAE):	 train: 0.288178	val: 0.486771	test: 0.511996
All runs completed.

RMSE train: 0.603552	val: 0.704402	test: 0.703244
MAE train: 0.441887	val: 0.523598	test: 0.499939

Epoch: 85
Loss: 0.5838302969932556
RMSE train: 0.604714	val: 0.701292	test: 0.719604
MAE train: 0.443075	val: 0.526161	test: 0.541272

Epoch: 86
Loss: 0.5840521901845932
RMSE train: 0.588913	val: 0.697170	test: 0.690459
MAE train: 0.430718	val: 0.513158	test: 0.518425

Epoch: 87
Loss: 0.501028873026371
RMSE train: 0.673199	val: 0.769834	test: 0.731835
MAE train: 0.494274	val: 0.564898	test: 0.533978

Epoch: 88
Loss: 0.5773367285728455
RMSE train: 0.595639	val: 0.717950	test: 0.722677
MAE train: 0.441449	val: 0.537732	test: 0.529426

Epoch: 89
Loss: 0.5174410194158554
RMSE train: 0.536055	val: 0.688834	test: 0.727362
MAE train: 0.401862	val: 0.520485	test: 0.535747

Epoch: 90
Loss: 0.6686260774731636
RMSE train: 0.519849	val: 0.699168	test: 0.692947
MAE train: 0.389912	val: 0.523560	test: 0.509867

Epoch: 91
Loss: 0.6574114859104156
RMSE train: 0.572078	val: 0.757503	test: 0.712530
MAE train: 0.425223	val: 0.565857	test: 0.521451

Epoch: 92
Loss: 0.4727049693465233
RMSE train: 0.487520	val: 0.673631	test: 0.703409
MAE train: 0.358600	val: 0.486615	test: 0.505296

Epoch: 93
Loss: 0.459659144282341
RMSE train: 0.543727	val: 0.698434	test: 0.736043
MAE train: 0.402216	val: 0.516988	test: 0.538011

Epoch: 94
Loss: 0.7432744354009628
RMSE train: 0.582087	val: 0.740292	test: 0.694212
MAE train: 0.421313	val: 0.543326	test: 0.507419

Epoch: 95
Loss: 0.5818018764257431
RMSE train: 0.780049	val: 0.939619	test: 0.816618
MAE train: 0.588766	val: 0.721420	test: 0.609426

Epoch: 96
Loss: 0.512470044195652
RMSE train: 0.585703	val: 0.751154	test: 0.686629
MAE train: 0.429918	val: 0.551863	test: 0.498367

Epoch: 97
Loss: 0.9815122336149216
RMSE train: 0.570505	val: 0.698343	test: 0.705825
MAE train: 0.426192	val: 0.529192	test: 0.520050

Epoch: 98
Loss: 0.5510701537132263
RMSE train: 0.604217	val: 0.712022	test: 0.722214
MAE train: 0.444313	val: 0.541601	test: 0.510172

Epoch: 99
Loss: 0.5646810308098793
RMSE train: 0.693081	val: 0.829801	test: 0.788064
MAE train: 0.496366	val: 0.606094	test: 0.575139

Epoch: 100
Loss: 0.5717010498046875
RMSE train: 0.700169	val: 0.868990	test: 0.795992
MAE train: 0.511928	val: 0.650228	test: 0.583650

Epoch: 101
Loss: 0.45197344571352005
RMSE train: 0.543773	val: 0.737119	test: 0.676638
MAE train: 0.398326	val: 0.540636	test: 0.487995

Epoch: 102
Loss: 0.5095998868346214
RMSE train: 0.479019	val: 0.649400	test: 0.669934
MAE train: 0.358402	val: 0.476945	test: 0.491112

Epoch: 103
Loss: 0.5519551038742065
RMSE train: 0.490091	val: 0.655891	test: 0.711855
MAE train: 0.364090	val: 0.468688	test: 0.529065

Epoch: 104
Loss: 0.5292410925030708
RMSE train: 0.521148	val: 0.682837	test: 0.648956
MAE train: 0.380263	val: 0.512248	test: 0.474882

Epoch: 105
Loss: 0.46632757037878036
RMSE train: 0.514121	val: 0.706848	test: 0.670905
MAE train: 0.382333	val: 0.532148	test: 0.492410

Epoch: 106
Loss: 0.4518224075436592
RMSE train: 0.506285	val: 0.727324	test: 0.731888
MAE train: 0.380972	val: 0.531618	test: 0.539671

Epoch: 107
Loss: 0.5170386657118797
RMSE train: 0.499927	val: 0.697037	test: 0.751050
MAE train: 0.376644	val: 0.511286	test: 0.556548

Epoch: 108
Loss: 0.43965690582990646
RMSE train: 0.516190	val: 0.678989	test: 0.708886
MAE train: 0.380188	val: 0.507931	test: 0.524186

Epoch: 109
Loss: 0.4387700483202934
RMSE train: 0.701096	val: 0.840093	test: 0.778200
MAE train: 0.515718	val: 0.638485	test: 0.565902

Epoch: 110
Loss: 0.6965846717357635
RMSE train: 0.651274	val: 0.789986	test: 0.723419
MAE train: 0.480417	val: 0.588616	test: 0.518651

Epoch: 111
Loss: 0.44389254599809647
RMSE train: 0.549155	val: 0.667684	test: 0.671734
MAE train: 0.408462	val: 0.498452	test: 0.480627

Epoch: 112
Loss: 0.7472015544772148
RMSE train: 0.585133	val: 0.684927	test: 0.705450
MAE train: 0.431405	val: 0.523048	test: 0.505173

Epoch: 113
Loss: 0.5290985181927681
RMSE train: 0.730104	val: 0.817721	test: 0.826243
MAE train: 0.527297	val: 0.613884	test: 0.588446

Epoch: 114
Loss: 0.5039623230695724
RMSE train: 0.655399	val: 0.761340	test: 0.784408
MAE train: 0.474992	val: 0.571457	test: 0.553052

Epoch: 115
Loss: 0.39863772690296173
RMSE train: 0.548314	val: 0.694384	test: 0.726522
MAE train: 0.405538	val: 0.515678	test: 0.532516

Epoch: 116
Loss: 0.4413490891456604
RMSE train: 0.506767	val: 0.717468	test: 0.715629
MAE train: 0.376803	val: 0.514039	test: 0.518368

Epoch: 117
Loss: 0.4511928856372833
RMSE train: 0.538290	val: 0.755726	test: 0.717053
MAE train: 0.404921	val: 0.551434	test: 0.525193

Epoch: 118
Loss: 0.489729106426239
RMSE train: 0.571604	val: 0.763208	test: 0.704947
MAE train: 0.431908	val: 0.560702	test: 0.515618

Epoch: 119
Loss: 0.8662465512752533
RMSE train: 0.670755	val: 0.860968	test: 0.759021
MAE train: 0.485198	val: 0.635191	test: 0.544252

Epoch: 120
Loss: 0.48967245221138
RMSE train: 0.773675	val: 0.928937	test: 0.833620
MAE train: 0.570775	val: 0.701341	test: 0.627320

Epoch: 121
Loss: 0.5235925167798996
RMSE train: 0.602751	val: 0.758480	test: 0.697563
MAE train: 0.461010	val: 0.590876	test: 0.503688

Early stopping
Best (RMSE):	 train: 0.544421	val: 0.644871	test: 0.693194
Best (MAE):	 train: 0.399957	val: 0.471362	test: 0.513786

RMSE train: 0.533940	val: 0.743767	test: 0.776412
MAE train: 0.404890	val: 0.528403	test: 0.552899

Epoch: 85
Loss: 0.5292268693447113
RMSE train: 0.608350	val: 0.780044	test: 0.810440
MAE train: 0.462504	val: 0.577502	test: 0.613368

Epoch: 86
Loss: 0.6005741655826569
RMSE train: 0.585163	val: 0.765659	test: 0.774757
MAE train: 0.442265	val: 0.571827	test: 0.580006

Epoch: 87
Loss: 0.5330068022012711
RMSE train: 0.629083	val: 0.807336	test: 0.826181
MAE train: 0.482310	val: 0.603496	test: 0.623601

Epoch: 88
Loss: 0.58115354180336
RMSE train: 0.546831	val: 0.736358	test: 0.726332
MAE train: 0.419074	val: 0.553075	test: 0.545229

Epoch: 89
Loss: 0.49216514825820923
RMSE train: 0.612428	val: 0.799256	test: 0.701174
MAE train: 0.454691	val: 0.600151	test: 0.512168

Epoch: 90
Loss: 0.5828420668840408
RMSE train: 0.639177	val: 0.780823	test: 0.718224
MAE train: 0.469876	val: 0.594009	test: 0.507311

Epoch: 91
Loss: 0.4620329216122627
RMSE train: 0.528977	val: 0.661199	test: 0.714211
MAE train: 0.394307	val: 0.493588	test: 0.508466

Epoch: 92
Loss: 0.4685639515519142
RMSE train: 0.553488	val: 0.709273	test: 0.770968
MAE train: 0.408246	val: 0.524611	test: 0.555038

Epoch: 93
Loss: 0.6024310067296028
RMSE train: 0.582327	val: 0.736231	test: 0.720906
MAE train: 0.426744	val: 0.555332	test: 0.521899

Epoch: 94
Loss: 0.44359127432107925
RMSE train: 0.581755	val: 0.747351	test: 0.691860
MAE train: 0.426322	val: 0.566352	test: 0.503733

Epoch: 95
Loss: 0.5586479902267456
RMSE train: 0.524773	val: 0.708314	test: 0.690909
MAE train: 0.381427	val: 0.518676	test: 0.496127

Epoch: 96
Loss: 0.5098529011011124
RMSE train: 0.602023	val: 0.762114	test: 0.743923
MAE train: 0.432738	val: 0.564030	test: 0.548565

Epoch: 97
Loss: 0.9643713310360909
RMSE train: 0.717091	val: 0.859345	test: 0.792897
MAE train: 0.509159	val: 0.628486	test: 0.579853

Epoch: 98
Loss: 0.4896315783262253
RMSE train: 0.774156	val: 0.910767	test: 0.792481
MAE train: 0.561969	val: 0.654578	test: 0.579662

Epoch: 99
Loss: 0.5070893615484238
RMSE train: 0.655877	val: 0.823533	test: 0.736473
MAE train: 0.474752	val: 0.579431	test: 0.536835

Epoch: 100
Loss: 0.5225286185741425
RMSE train: 0.560325	val: 0.751099	test: 0.707729
MAE train: 0.411067	val: 0.545740	test: 0.517262

Epoch: 101
Loss: 0.5473770201206207
RMSE train: 0.599555	val: 0.752797	test: 0.714423
MAE train: 0.436966	val: 0.569524	test: 0.513309

Epoch: 102
Loss: 0.4414929300546646
RMSE train: 0.702006	val: 0.849860	test: 0.763890
MAE train: 0.505042	val: 0.628727	test: 0.554798

Epoch: 103
Loss: 0.482904277741909
RMSE train: 0.585306	val: 0.764721	test: 0.686683
MAE train: 0.422142	val: 0.565845	test: 0.496043

Epoch: 104
Loss: 0.5089171528816223
RMSE train: 0.509446	val: 0.710356	test: 0.645938
MAE train: 0.370382	val: 0.533125	test: 0.463137

Epoch: 105
Loss: 0.6077520772814751
RMSE train: 0.632511	val: 0.815137	test: 0.717681
MAE train: 0.468006	val: 0.618685	test: 0.531999

Epoch: 106
Loss: 0.47817978262901306
RMSE train: 0.555729	val: 0.724610	test: 0.674984
MAE train: 0.410368	val: 0.561269	test: 0.492004

Epoch: 107
Loss: 0.44539497047662735
RMSE train: 0.536222	val: 0.698716	test: 0.708192
MAE train: 0.404304	val: 0.549651	test: 0.529364

Epoch: 108
Loss: 0.765381470322609
RMSE train: 0.488788	val: 0.677168	test: 0.667885
MAE train: 0.358957	val: 0.511392	test: 0.492720

Epoch: 109
Loss: 0.47967613488435745
RMSE train: 0.675359	val: 0.835975	test: 0.744853
MAE train: 0.489326	val: 0.618944	test: 0.555462

Epoch: 110
Loss: 0.5446919947862625
RMSE train: 0.712437	val: 0.858219	test: 0.766019
MAE train: 0.515301	val: 0.637038	test: 0.569605

Epoch: 111
Loss: 0.4985898360610008
RMSE train: 0.539296	val: 0.697551	test: 0.686985
MAE train: 0.393131	val: 0.518680	test: 0.496405

Epoch: 112
Loss: 0.6178276091814041
RMSE train: 0.513423	val: 0.694202	test: 0.719667
MAE train: 0.379950	val: 0.509221	test: 0.523184

Epoch: 113
Loss: 0.6465509533882141
RMSE train: 0.510053	val: 0.690583	test: 0.697979
MAE train: 0.378693	val: 0.522127	test: 0.515577

Epoch: 114
Loss: 0.669846810400486
RMSE train: 0.552928	val: 0.723302	test: 0.715238
MAE train: 0.425271	val: 0.559617	test: 0.534507

Epoch: 115
Loss: 0.42607223242521286
RMSE train: 0.555033	val: 0.725220	test: 0.717257
MAE train: 0.422876	val: 0.555427	test: 0.524484

Epoch: 116
Loss: 0.49744468927383423
RMSE train: 0.519961	val: 0.713120	test: 0.692242
MAE train: 0.387849	val: 0.544174	test: 0.503451

Epoch: 117
Loss: 0.6980488672852516
RMSE train: 0.580640	val: 0.764345	test: 0.716403
MAE train: 0.435739	val: 0.594147	test: 0.518616

Epoch: 118
Loss: 0.4405136704444885
RMSE train: 0.622166	val: 0.797304	test: 0.711855
MAE train: 0.469793	val: 0.600375	test: 0.510445

Epoch: 119
Loss: 0.4586265981197357
RMSE train: 0.560373	val: 0.759790	test: 0.694204
MAE train: 0.416485	val: 0.579767	test: 0.497639

Epoch: 120
Loss: 0.5283956304192543
RMSE train: 0.540997	val: 0.749388	test: 0.732424
MAE train: 0.412998	val: 0.560509	test: 0.544084

Epoch: 121
Loss: 0.5658244639635086
RMSE train: 0.528729	val: 0.743635	test: 0.764832
MAE train: 0.398210	val: 0.540593	test: 0.566018

Epoch: 122
Loss: 0.5590949803590775
RMSE train: 0.588722	val: 0.784589	test: 0.749706
MAE train: 0.426825	val: 0.585225	test: 0.543529

Epoch: 123
Loss: 0.6241112798452377
RMSE train: 0.573685	val: 0.757305	test: 0.759975
MAE train: 0.415208	val: 0.571048	test: 0.559180

Epoch: 124
Loss: 0.468081071972847
RMSE train: 0.533249	val: 0.717508	test: 0.746471
MAE train: 0.384650	val: 0.531893	test: 0.544098

Epoch: 125
Loss: 0.46136054396629333
RMSE train: 0.521960	val: 0.729291	test: 0.712409
MAE train: 0.378280	val: 0.540723	test: 0.516504

Epoch: 126
Loss: 0.5594632104039192
RMSE train: 0.511686	val: 0.735523	test: 0.670271
MAE train: 0.370732	val: 0.546043	test: 0.487435

Early stopping
Best (RMSE):	 train: 0.528977	val: 0.661199	test: 0.714211
Best (MAE):	 train: 0.394307	val: 0.493588	test: 0.508466

RMSE train: 0.604688	val: 0.807298	test: 0.785615
MAE train: 0.462806	val: 0.572382	test: 0.575376

Epoch: 85
Loss: 0.5109403356909752
RMSE train: 0.541850	val: 0.756467	test: 0.727563
MAE train: 0.422659	val: 0.560349	test: 0.529699

Epoch: 86
Loss: 0.6003826335072517
RMSE train: 0.525257	val: 0.716058	test: 0.694949
MAE train: 0.400267	val: 0.541503	test: 0.504876

Epoch: 87
Loss: 0.571352943778038
RMSE train: 0.663916	val: 0.824600	test: 0.752131
MAE train: 0.496926	val: 0.627371	test: 0.565869

Epoch: 88
Loss: 0.5945092439651489
RMSE train: 0.641695	val: 0.798636	test: 0.720676
MAE train: 0.479236	val: 0.610253	test: 0.531275

Epoch: 89
Loss: 0.7228213995695114
RMSE train: 0.545760	val: 0.725610	test: 0.643394
MAE train: 0.411754	val: 0.556040	test: 0.474471

Epoch: 90
Loss: 0.530535414814949
RMSE train: 0.604771	val: 0.807210	test: 0.727426
MAE train: 0.464862	val: 0.602614	test: 0.551481

Epoch: 91
Loss: 0.5684735476970673
RMSE train: 0.529162	val: 0.760410	test: 0.735085
MAE train: 0.407596	val: 0.536905	test: 0.542090

Epoch: 92
Loss: 0.6214489638805389
RMSE train: 0.548793	val: 0.744794	test: 0.722559
MAE train: 0.421485	val: 0.553084	test: 0.548456

Epoch: 93
Loss: 0.5884715691208839
RMSE train: 0.631925	val: 0.815073	test: 0.682305
MAE train: 0.473516	val: 0.616893	test: 0.507105

Epoch: 94
Loss: 0.4675981178879738
RMSE train: 0.633220	val: 0.816100	test: 0.691564
MAE train: 0.485033	val: 0.623867	test: 0.512029

Epoch: 95
Loss: 0.5624368414282799
RMSE train: 0.529926	val: 0.739441	test: 0.661036
MAE train: 0.403537	val: 0.554661	test: 0.486851

Epoch: 96
Loss: 0.5334311276674271
RMSE train: 0.533049	val: 0.730910	test: 0.661552
MAE train: 0.395102	val: 0.552119	test: 0.479564

Epoch: 97
Loss: 0.7610707432031631
RMSE train: 0.606795	val: 0.767569	test: 0.678566
MAE train: 0.452809	val: 0.591577	test: 0.484091

Epoch: 98
Loss: 0.495180681347847
RMSE train: 0.566530	val: 0.706996	test: 0.683272
MAE train: 0.431918	val: 0.553631	test: 0.492084

Epoch: 99
Loss: 0.6220896989107132
RMSE train: 0.572725	val: 0.734334	test: 0.707377
MAE train: 0.422367	val: 0.559109	test: 0.509558

Epoch: 100
Loss: 0.5243303924798965
RMSE train: 0.643247	val: 0.870785	test: 0.761351
MAE train: 0.484390	val: 0.651708	test: 0.558837

Epoch: 101
Loss: 0.44220442324876785
RMSE train: 0.530586	val: 0.758599	test: 0.649381
MAE train: 0.393780	val: 0.572568	test: 0.465792

Epoch: 102
Loss: 0.5322577133774757
RMSE train: 0.566089	val: 0.764339	test: 0.645944
MAE train: 0.413468	val: 0.584917	test: 0.455972

Epoch: 103
Loss: 0.48474086076021194
RMSE train: 0.709137	val: 0.888455	test: 0.738264
MAE train: 0.515280	val: 0.684933	test: 0.532397

Epoch: 104
Loss: 0.46694672107696533
RMSE train: 0.569077	val: 0.752625	test: 0.642721
MAE train: 0.408626	val: 0.569741	test: 0.457301

Epoch: 105
Loss: 0.44822391122579575
RMSE train: 0.518935	val: 0.712003	test: 0.619845
MAE train: 0.380479	val: 0.545996	test: 0.448078

Epoch: 106
Loss: 0.4644367843866348
RMSE train: 0.641004	val: 0.831035	test: 0.726343
MAE train: 0.476614	val: 0.622280	test: 0.547327

Epoch: 107
Loss: 0.6090185344219208
RMSE train: 0.606761	val: 0.772915	test: 0.692486
MAE train: 0.451467	val: 0.587888	test: 0.516620

Epoch: 108
Loss: 0.5594494044780731
RMSE train: 0.548437	val: 0.726424	test: 0.694869
MAE train: 0.402152	val: 0.541525	test: 0.504235

Epoch: 109
Loss: 0.574150837957859
RMSE train: 0.534592	val: 0.720695	test: 0.666059
MAE train: 0.392791	val: 0.535240	test: 0.478856

Epoch: 110
Loss: 0.49990060925483704
RMSE train: 0.607341	val: 0.790495	test: 0.713025
MAE train: 0.446431	val: 0.598125	test: 0.528002

Epoch: 111
Loss: 0.5140478685498238
RMSE train: 0.497600	val: 0.709644	test: 0.644321
MAE train: 0.377765	val: 0.534551	test: 0.477461

Epoch: 112
Loss: 0.6318580582737923
RMSE train: 0.491376	val: 0.716962	test: 0.655900
MAE train: 0.375497	val: 0.527792	test: 0.481395

Epoch: 113
Loss: 0.5050970017910004
RMSE train: 0.568184	val: 0.751194	test: 0.703447
MAE train: 0.423431	val: 0.571828	test: 0.523611

Epoch: 114
Loss: 0.6095754206180573
RMSE train: 0.579819	val: 0.742955	test: 0.704571
MAE train: 0.429758	val: 0.575176	test: 0.529537

Epoch: 115
Loss: 0.5886737555265427
RMSE train: 0.508171	val: 0.690737	test: 0.662320
MAE train: 0.380631	val: 0.527827	test: 0.490339

Epoch: 116
Loss: 0.43374746292829514
RMSE train: 0.518760	val: 0.750833	test: 0.734169
MAE train: 0.396321	val: 0.560274	test: 0.523567

Epoch: 117
Loss: 0.46828731149435043
RMSE train: 0.472245	val: 0.727912	test: 0.704105
MAE train: 0.356640	val: 0.534875	test: 0.509513

Epoch: 118
Loss: 0.49501682817935944
RMSE train: 0.502443	val: 0.727717	test: 0.659832
MAE train: 0.379713	val: 0.551253	test: 0.484032

Epoch: 119
Loss: 0.5395907387137413
RMSE train: 0.559719	val: 0.752113	test: 0.676643
MAE train: 0.420411	val: 0.579918	test: 0.494781

Epoch: 120
Loss: 0.5583176538348198
RMSE train: 0.559838	val: 0.736893	test: 0.674316
MAE train: 0.418973	val: 0.568885	test: 0.480857

Epoch: 121
Loss: 0.47461485862731934
RMSE train: 0.541825	val: 0.719896	test: 0.661276
MAE train: 0.392661	val: 0.549104	test: 0.462278

Epoch: 122
Loss: 0.46264858543872833
RMSE train: 0.505729	val: 0.704094	test: 0.663309
MAE train: 0.371374	val: 0.525487	test: 0.472792

Epoch: 123
Loss: 0.5902237370610237
RMSE train: 0.504373	val: 0.721322	test: 0.646162
MAE train: 0.372523	val: 0.543248	test: 0.465361

Epoch: 124
Loss: 0.45028237253427505
RMSE train: 0.496955	val: 0.738727	test: 0.641507
MAE train: 0.364010	val: 0.544288	test: 0.459093

Epoch: 125
Loss: 0.46641379594802856
RMSE train: 0.496181	val: 0.714083	test: 0.652170
MAE train: 0.360596	val: 0.526890	test: 0.464390

Epoch: 126
Loss: 0.633034959435463
RMSE train: 0.489587	val: 0.667936	test: 0.640593
MAE train: 0.361727	val: 0.495168	test: 0.469853

Epoch: 127
Loss: 0.42184828221797943
RMSE train: 0.507848	val: 0.677456	test: 0.657668
MAE train: 0.375476	val: 0.513406	test: 0.484524

Epoch: 128
Loss: 0.4170454144477844
RMSE train: 0.661637	val: 0.828342	test: 0.754058
MAE train: 0.494305	val: 0.615126	test: 0.553733

Epoch: 129
Loss: 0.803604893386364
RMSE train: 0.490521	val: 0.712920	test: 0.670516
MAE train: 0.372314	val: 0.520814	test: 0.492945

Epoch: 130
Loss: 0.42324811965227127
RMSE train: 0.715967	val: 0.863158	test: 0.906484
MAE train: 0.541330	val: 0.633301	test: 0.684393

Epoch: 131
Loss: 0.6940879374742508
RMSE train: 0.507958	val: 0.705500	test: 0.687355
MAE train: 0.386152	val: 0.521793	test: 0.507036

Epoch: 132
Loss: 0.3947531431913376
RMSE train: 0.586190	val: 0.794014	test: 0.703807
MAE train: 0.454177	val: 0.591306	test: 0.535711

Epoch: 133
Loss: 0.6548259556293488
RMSE train: 0.529850	val: 0.770756	test: 0.730190
MAE train: 0.404155	val: 0.560738	test: 0.535464

Epoch: 134
Loss: 0.5622177198529243
RMSE train: 0.467164	val: 0.697666	test: 0.681013
MAE train: 0.350506	val: 0.515742	test: 0.486373

Epoch: 135
Loss: 0.41721948981285095
RMSE train: 0.659890	val: 0.813660	test: 0.738954
MAE train: 0.500207	val: 0.620832	test: 0.563382

Epoch: 136
Loss: 0.5493449345231056
RMSE train: 0.568005	val: 0.738214	test: 0.691649
MAE train: 0.420969	val: 0.566870	test: 0.516671

Epoch: 137
Loss: 0.39191921055316925
RMSE train: 0.463375	val: 0.701552	test: 0.660772
MAE train: 0.350791	val: 0.512311	test: 0.484922

Epoch: 138
Loss: 0.3673822060227394
RMSE train: 0.473422	val: 0.721742	test: 0.631570
MAE train: 0.354115	val: 0.528872	test: 0.459005

Epoch: 139
Loss: 0.49596210569143295
RMSE train: 0.553655	val: 0.780523	test: 0.655213
MAE train: 0.415879	val: 0.582447	test: 0.475026

Epoch: 140
Loss: 0.43492603302001953
RMSE train: 0.589805	val: 0.776646	test: 0.666113
MAE train: 0.434372	val: 0.597147	test: 0.486736

Epoch: 141
Loss: 0.39924701303243637
RMSE train: 0.525968	val: 0.701005	test: 0.670342
MAE train: 0.384445	val: 0.542492	test: 0.486681

Epoch: 142
Loss: 0.3511665165424347
RMSE train: 0.464586	val: 0.675186	test: 0.661267
MAE train: 0.336063	val: 0.505649	test: 0.481453

Epoch: 143
Loss: 0.41466227918863297
RMSE train: 0.438304	val: 0.690811	test: 0.649697
MAE train: 0.320528	val: 0.502403	test: 0.473807

Epoch: 144
Loss: 0.3634811192750931
RMSE train: 0.459687	val: 0.731122	test: 0.648069

Epoch: 84
Loss: 0.3635595515370369
RMSE train: 0.477696	val: 0.674838	test: 0.640056
MAE train: 0.351196	val: 0.509104	test: 0.482743

Epoch: 85
Loss: 0.33746688067913055
RMSE train: 0.476898	val: 0.653417	test: 0.658981
MAE train: 0.348716	val: 0.496216	test: 0.483693

Epoch: 86
Loss: 0.43465755879879
RMSE train: 0.488659	val: 0.649489	test: 0.635983
MAE train: 0.352846	val: 0.491883	test: 0.465677

Epoch: 87
Loss: 0.36027225106954575
RMSE train: 0.509714	val: 0.654578	test: 0.666222
MAE train: 0.375557	val: 0.494983	test: 0.487068

Epoch: 88
Loss: 0.3870073929429054
RMSE train: 0.549428	val: 0.688732	test: 0.709474
MAE train: 0.408459	val: 0.522624	test: 0.517871

Epoch: 89
Loss: 0.3348480090498924
RMSE train: 0.457485	val: 0.621049	test: 0.674936
MAE train: 0.340441	val: 0.472673	test: 0.499219

Epoch: 90
Loss: 0.3465772867202759
RMSE train: 0.541608	val: 0.771264	test: 0.664834
MAE train: 0.400287	val: 0.578431	test: 0.504684

Epoch: 91
Loss: 0.35769201070070267
RMSE train: 0.554949	val: 0.708421	test: 0.685122
MAE train: 0.405561	val: 0.551405	test: 0.507290

Epoch: 92
Loss: 0.326357938349247
RMSE train: 0.558840	val: 0.707373	test: 0.706928
MAE train: 0.415277	val: 0.544922	test: 0.526916

Epoch: 93
Loss: 0.3149954825639725
RMSE train: 0.643585	val: 0.776821	test: 0.725721
MAE train: 0.476208	val: 0.605481	test: 0.536739

Epoch: 94
Loss: 0.33219755440950394
RMSE train: 0.529325	val: 0.682247	test: 0.662910
MAE train: 0.392791	val: 0.516477	test: 0.486284

Epoch: 95
Loss: 0.34922026842832565
RMSE train: 0.530925	val: 0.734022	test: 0.679310
MAE train: 0.390217	val: 0.559020	test: 0.505853

Epoch: 96
Loss: 0.34510087221860886
RMSE train: 0.446833	val: 0.594787	test: 0.677203
MAE train: 0.327924	val: 0.449266	test: 0.498206

Epoch: 97
Loss: 0.31446611136198044
RMSE train: 0.473417	val: 0.675717	test: 0.658347
MAE train: 0.353748	val: 0.508433	test: 0.480866

Epoch: 98
Loss: 0.3362356796860695
RMSE train: 0.501703	val: 0.657335	test: 0.666771
MAE train: 0.366702	val: 0.514578	test: 0.485745

Epoch: 99
Loss: 0.31803515553474426
RMSE train: 0.554369	val: 0.647706	test: 0.697799
MAE train: 0.400036	val: 0.504193	test: 0.518291

Epoch: 100
Loss: 0.32015272229909897
RMSE train: 0.532312	val: 0.720601	test: 0.656561
MAE train: 0.389376	val: 0.542182	test: 0.489801

Epoch: 101
Loss: 0.33227426558732986
RMSE train: 0.518232	val: 0.682403	test: 0.669376
MAE train: 0.378970	val: 0.524877	test: 0.493786

Epoch: 102
Loss: 0.35078197717666626
RMSE train: 0.475334	val: 0.611192	test: 0.688865
MAE train: 0.346007	val: 0.473906	test: 0.500875

Epoch: 103
Loss: 0.34227147698402405
RMSE train: 0.488110	val: 0.730656	test: 0.656999
MAE train: 0.363301	val: 0.554256	test: 0.486446

Epoch: 104
Loss: 0.37627847492694855
RMSE train: 0.432659	val: 0.668454	test: 0.629908
MAE train: 0.324447	val: 0.512538	test: 0.469012

Epoch: 105
Loss: 0.3287394270300865
RMSE train: 0.448109	val: 0.628878	test: 0.649735
MAE train: 0.330283	val: 0.494528	test: 0.486525

Epoch: 106
Loss: 0.3325736075639725
RMSE train: 0.516124	val: 0.782662	test: 0.636428
MAE train: 0.384255	val: 0.578321	test: 0.465588

Epoch: 107
Loss: 0.3302910774946213
RMSE train: 0.433400	val: 0.689248	test: 0.612879
MAE train: 0.328773	val: 0.521852	test: 0.473657

Epoch: 108
Loss: 0.30465681850910187
RMSE train: 0.485351	val: 0.690751	test: 0.638541
MAE train: 0.354270	val: 0.533107	test: 0.462517

Epoch: 109
Loss: 0.3085314407944679
RMSE train: 0.456513	val: 0.719780	test: 0.629859
MAE train: 0.340159	val: 0.524261	test: 0.466864

Epoch: 110
Loss: 0.29962240904569626
RMSE train: 0.434664	val: 0.685527	test: 0.643245
MAE train: 0.322152	val: 0.492897	test: 0.479439

Epoch: 111
Loss: 0.33381909877061844
RMSE train: 0.521402	val: 0.673181	test: 0.683310
MAE train: 0.375304	val: 0.518329	test: 0.495034

Epoch: 112
Loss: 0.32066915929317474
RMSE train: 0.503506	val: 0.687475	test: 0.662072
MAE train: 0.371871	val: 0.530437	test: 0.490075

Epoch: 113
Loss: 0.32646700739860535
RMSE train: 0.440788	val: 0.616009	test: 0.654225
MAE train: 0.326346	val: 0.467806	test: 0.474275

Epoch: 114
Loss: 0.310808390378952
RMSE train: 0.515876	val: 0.727632	test: 0.686716
MAE train: 0.383788	val: 0.541590	test: 0.506359

Epoch: 115
Loss: 0.2866553068161011
RMSE train: 0.451868	val: 0.691264	test: 0.652272
MAE train: 0.336546	val: 0.516163	test: 0.479357

Epoch: 116
Loss: 0.3152030110359192
RMSE train: 0.457120	val: 0.651474	test: 0.675896
MAE train: 0.334387	val: 0.488112	test: 0.484142

Epoch: 117
Loss: 0.2901715561747551
RMSE train: 0.526045	val: 0.755729	test: 0.701438
MAE train: 0.388326	val: 0.554818	test: 0.512834

Epoch: 118
Loss: 0.32726259529590607
RMSE train: 0.470883	val: 0.700099	test: 0.651611
MAE train: 0.347852	val: 0.528567	test: 0.477574

Epoch: 119
Loss: 0.2957964763045311
RMSE train: 0.443855	val: 0.648373	test: 0.634958
MAE train: 0.330576	val: 0.496801	test: 0.462266

Epoch: 120
Loss: 0.28927645087242126
RMSE train: 0.475816	val: 0.668147	test: 0.645071
MAE train: 0.347572	val: 0.517952	test: 0.462461

Epoch: 121
Loss: 0.31157881021499634
RMSE train: 0.436484	val: 0.629058	test: 0.643086
MAE train: 0.320686	val: 0.486299	test: 0.478411

Epoch: 122
Loss: 0.309712965041399
RMSE train: 0.472720	val: 0.720348	test: 0.646473
MAE train: 0.342417	val: 0.533832	test: 0.481818

Epoch: 123
Loss: 0.3043777272105217
RMSE train: 0.461856	val: 0.606111	test: 0.663861
MAE train: 0.342324	val: 0.476200	test: 0.495177

Epoch: 124
Loss: 0.2903333492577076
RMSE train: 0.443306	val: 0.611600	test: 0.649895
MAE train: 0.322731	val: 0.477991	test: 0.475590

Epoch: 125
Loss: 0.28178034722805023
RMSE train: 0.475633	val: 0.706687	test: 0.656130
MAE train: 0.344445	val: 0.529713	test: 0.485555

Epoch: 126
Loss: 0.2869114354252815
RMSE train: 0.437280	val: 0.630682	test: 0.637544
MAE train: 0.322263	val: 0.484059	test: 0.470311

Epoch: 127
Loss: 0.30411242693662643
RMSE train: 0.466106	val: 0.635054	test: 0.665826
MAE train: 0.339569	val: 0.509164	test: 0.484215

Epoch: 128
Loss: 0.27296119555830956
RMSE train: 0.413843	val: 0.622062	test: 0.650285
MAE train: 0.306546	val: 0.484648	test: 0.477219

Epoch: 129
Loss: 0.2617243193089962
RMSE train: 0.420663	val: 0.621419	test: 0.644214
MAE train: 0.312655	val: 0.476306	test: 0.472100

Epoch: 130
Loss: 0.2966206707060337
RMSE train: 0.497709	val: 0.646417	test: 0.698390
MAE train: 0.363653	val: 0.511512	test: 0.503551

Epoch: 131
Loss: 0.2877306044101715
RMSE train: 0.449225	val: 0.643401	test: 0.648059
MAE train: 0.327140	val: 0.499596	test: 0.468571

Early stopping
Best (RMSE):	 train: 0.446833	val: 0.594787	test: 0.677203
Best (MAE):	 train: 0.327924	val: 0.449266	test: 0.498206


Epoch: 84
Loss: 0.3393106386065483
RMSE train: 0.513264	val: 0.684137	test: 0.681701
MAE train: 0.381916	val: 0.519585	test: 0.508377

Epoch: 85
Loss: 0.3451599180698395
RMSE train: 0.509304	val: 0.666938	test: 0.656766
MAE train: 0.371066	val: 0.509885	test: 0.486197

Epoch: 86
Loss: 0.3727213442325592
RMSE train: 0.475367	val: 0.635046	test: 0.640668
MAE train: 0.351695	val: 0.478865	test: 0.471896

Epoch: 87
Loss: 0.42002256214618683
RMSE train: 0.517014	val: 0.725402	test: 0.659362
MAE train: 0.385496	val: 0.563901	test: 0.481045

Epoch: 88
Loss: 0.3163379058241844
RMSE train: 0.475087	val: 0.652559	test: 0.655810
MAE train: 0.351653	val: 0.507392	test: 0.471957

Epoch: 89
Loss: 0.32800520956516266
RMSE train: 0.437006	val: 0.627784	test: 0.635452
MAE train: 0.322201	val: 0.467363	test: 0.473957

Epoch: 90
Loss: 0.3382497876882553
RMSE train: 0.463492	val: 0.645339	test: 0.655069
MAE train: 0.338969	val: 0.493578	test: 0.483355

Epoch: 91
Loss: 0.34776315838098526
RMSE train: 0.484621	val: 0.618723	test: 0.650796
MAE train: 0.358499	val: 0.468567	test: 0.482256

Epoch: 92
Loss: 0.3543471470475197
RMSE train: 0.524114	val: 0.706006	test: 0.694432
MAE train: 0.394294	val: 0.547653	test: 0.523502

Epoch: 93
Loss: 0.3652719110250473
RMSE train: 0.505541	val: 0.664680	test: 0.673311
MAE train: 0.380357	val: 0.504274	test: 0.500636

Epoch: 94
Loss: 0.35738757252693176
RMSE train: 0.497878	val: 0.649390	test: 0.668673
MAE train: 0.366189	val: 0.486804	test: 0.484771

Epoch: 95
Loss: 0.33344995230436325
RMSE train: 0.449017	val: 0.631919	test: 0.635412
MAE train: 0.329473	val: 0.478863	test: 0.471750

Epoch: 96
Loss: 0.3254076912999153
RMSE train: 0.463648	val: 0.632312	test: 0.630201
MAE train: 0.349470	val: 0.461606	test: 0.479809

Epoch: 97
Loss: 0.3689223751425743
RMSE train: 0.436482	val: 0.605874	test: 0.622721
MAE train: 0.327352	val: 0.463764	test: 0.478096

Epoch: 98
Loss: 0.34485258162021637
RMSE train: 0.496468	val: 0.650996	test: 0.658010
MAE train: 0.367232	val: 0.496508	test: 0.474710

Epoch: 99
Loss: 0.3243373781442642
RMSE train: 0.529047	val: 0.687845	test: 0.689520
MAE train: 0.393910	val: 0.535309	test: 0.489411

Epoch: 100
Loss: 0.3109031319618225
RMSE train: 0.447816	val: 0.597130	test: 0.660989
MAE train: 0.333386	val: 0.450596	test: 0.478172

Epoch: 101
Loss: 0.3597257286310196
RMSE train: 0.653573	val: 0.845182	test: 0.756790
MAE train: 0.493807	val: 0.669452	test: 0.564366

Epoch: 102
Loss: 0.3274291828274727
RMSE train: 0.542096	val: 0.654956	test: 0.705868
MAE train: 0.392407	val: 0.511099	test: 0.511649

Epoch: 103
Loss: 0.37143775820732117
RMSE train: 0.600784	val: 0.737588	test: 0.721007
MAE train: 0.441735	val: 0.576844	test: 0.530119

Epoch: 104
Loss: 0.36029066145420074
RMSE train: 0.604725	val: 0.768272	test: 0.676744
MAE train: 0.442608	val: 0.567286	test: 0.487047

Epoch: 105
Loss: 0.3867192938923836
RMSE train: 0.447468	val: 0.615907	test: 0.632226
MAE train: 0.326054	val: 0.456713	test: 0.459008

Epoch: 106
Loss: 0.3364591971039772
RMSE train: 0.454331	val: 0.623874	test: 0.646137
MAE train: 0.333110	val: 0.479970	test: 0.467657

Epoch: 107
Loss: 0.32379140704870224
RMSE train: 0.441526	val: 0.641787	test: 0.649566
MAE train: 0.331891	val: 0.506383	test: 0.491579

Epoch: 108
Loss: 0.32150012254714966
RMSE train: 0.424815	val: 0.664391	test: 0.664718
MAE train: 0.325350	val: 0.510021	test: 0.504167

Epoch: 109
Loss: 0.33879732340574265
RMSE train: 0.395963	val: 0.624997	test: 0.627470
MAE train: 0.297672	val: 0.457497	test: 0.478569

Epoch: 110
Loss: 0.3236103802919388
RMSE train: 0.436924	val: 0.576177	test: 0.642209
MAE train: 0.322904	val: 0.432162	test: 0.472758

Epoch: 111
Loss: 0.31377319246530533
RMSE train: 0.547682	val: 0.703031	test: 0.728079
MAE train: 0.401491	val: 0.551926	test: 0.522319

Epoch: 112
Loss: 0.3573978692293167
RMSE train: 0.564101	val: 0.724173	test: 0.711664
MAE train: 0.418910	val: 0.562732	test: 0.512161

Epoch: 113
Loss: 0.3357725515961647
RMSE train: 0.407750	val: 0.596589	test: 0.635527
MAE train: 0.310664	val: 0.454524	test: 0.466630

Epoch: 114
Loss: 0.3170711100101471
RMSE train: 0.433457	val: 0.679104	test: 0.624411
MAE train: 0.321417	val: 0.503518	test: 0.459257

Epoch: 115
Loss: 0.3134370222687721
RMSE train: 0.445114	val: 0.624601	test: 0.649210
MAE train: 0.320162	val: 0.487450	test: 0.467415

Epoch: 116
Loss: 0.3054346442222595
RMSE train: 0.512626	val: 0.659858	test: 0.685252
MAE train: 0.375249	val: 0.507332	test: 0.495744

Epoch: 117
Loss: 0.29522015154361725
RMSE train: 0.485622	val: 0.663809	test: 0.668008
MAE train: 0.356205	val: 0.509517	test: 0.486741

Epoch: 118
Loss: 0.2919275686144829
RMSE train: 0.542671	val: 0.703148	test: 0.719376
MAE train: 0.404594	val: 0.555329	test: 0.528218

Epoch: 119
Loss: 0.3047173544764519
RMSE train: 0.555617	val: 0.699375	test: 0.720519
MAE train: 0.412881	val: 0.540016	test: 0.526344

Epoch: 120
Loss: 0.2787811830639839
RMSE train: 0.458553	val: 0.653228	test: 0.664313
MAE train: 0.340969	val: 0.493390	test: 0.489068

Epoch: 121
Loss: 0.299263097345829
RMSE train: 0.470546	val: 0.694428	test: 0.672566
MAE train: 0.348837	val: 0.530753	test: 0.491511

Epoch: 122
Loss: 0.2762381285429001
RMSE train: 0.536750	val: 0.669368	test: 0.713557
MAE train: 0.391362	val: 0.512801	test: 0.507961

Epoch: 123
Loss: 0.31513186544179916
RMSE train: 0.461282	val: 0.644462	test: 0.696012
MAE train: 0.337032	val: 0.503046	test: 0.506722

Epoch: 124
Loss: 0.3091457709670067
RMSE train: 0.389422	val: 0.607640	test: 0.652518
MAE train: 0.287443	val: 0.464105	test: 0.475711

Epoch: 125
Loss: 0.2738988697528839
RMSE train: 0.370508	val: 0.603395	test: 0.633648
MAE train: 0.276654	val: 0.451702	test: 0.471371

Epoch: 126
Loss: 0.2576439082622528
RMSE train: 0.382116	val: 0.589807	test: 0.661911
MAE train: 0.278863	val: 0.451171	test: 0.483761

Epoch: 127
Loss: 0.28151752054691315
RMSE train: 0.435339	val: 0.582298	test: 0.664085
MAE train: 0.324566	val: 0.450217	test: 0.485345

Epoch: 128
Loss: 0.3111446909606457
RMSE train: 0.407327	val: 0.588950	test: 0.656868
MAE train: 0.304868	val: 0.436303	test: 0.484695

Epoch: 129
Loss: 0.28655053675174713
RMSE train: 0.391232	val: 0.617727	test: 0.635649
MAE train: 0.293274	val: 0.464367	test: 0.470175

Epoch: 130
Loss: 0.3378966450691223
RMSE train: 0.383600	val: 0.625248	test: 0.638939
MAE train: 0.287833	val: 0.479416	test: 0.474955

Epoch: 131
Loss: 0.2874528542160988
RMSE train: 0.370668	val: 0.606485	test: 0.648504
MAE train: 0.271512	val: 0.469511	test: 0.474956

Epoch: 132
Loss: 0.3104616329073906
RMSE train: 0.448328	val: 0.601493	test: 0.658203
MAE train: 0.327175	val: 0.459524	test: 0.491969

Epoch: 133
Loss: 0.2995768412947655
RMSE train: 0.485063	val: 0.713069	test: 0.680897
MAE train: 0.341690	val: 0.543678	test: 0.500357

Epoch: 134
Loss: 0.2872919887304306
RMSE train: 0.457666	val: 0.654670	test: 0.657547
MAE train: 0.338750	val: 0.503714	test: 0.483064

Epoch: 135
Loss: 0.31043215095996857
RMSE train: 0.452435	val: 0.657809	test: 0.654031
MAE train: 0.338704	val: 0.504535	test: 0.478516

Epoch: 136
Loss: 0.29062099009752274
RMSE train: 0.379344	val: 0.605046	test: 0.679073
MAE train: 0.282738	val: 0.440039	test: 0.497116

Epoch: 137
Loss: 0.2673003152012825
RMSE train: 0.439487	val: 0.658806	test: 0.666346
MAE train: 0.332848	val: 0.488240	test: 0.484235

Epoch: 138
Loss: 0.25834842398762703
RMSE train: 0.393932	val: 0.626209	test: 0.663497
MAE train: 0.294814	val: 0.473114	test: 0.487177

Epoch: 139
Loss: 0.26783301681280136
RMSE train: 0.400085	val: 0.639992	test: 0.642832
MAE train: 0.294529	val: 0.475262	test: 0.471625

Epoch: 140
Loss: 0.2818327844142914
RMSE train: 0.399461	val: 0.620545	test: 0.638964
MAE train: 0.295378	val: 0.461108	test: 0.470934

Epoch: 141
Loss: 0.25884853675961494
RMSE train: 0.421821	val: 0.669969	test: 0.656365
MAE train: 0.309217	val: 0.499347	test: 0.480572

Epoch: 142
Loss: 0.268176905810833
RMSE train: 0.435092	val: 0.618634	test: 0.687250
MAE train: 0.324941	val: 0.471828	test: 0.505153

Epoch: 143
Loss: 0.32081158459186554
RMSE train: 0.420483	val: 0.623642	test: 0.677671
MAE train: 0.311828	val: 0.473298	test: 0.489045

Epoch: 144
Loss: 0.27184145897626877
RMSE train: 0.374988	val: 0.662083	test: 0.629364
MAE train: 0.278361	val: 0.464277	test: 0.459772

Epoch: 145
Loss: 0.3329818919301033
RMSE train: 0.458079	val: 0.684039	test: 0.638075
MAE train: 0.338286	val: 0.501176	test: 0.462942

Early stopping
Best (RMSE):	 train: 0.436924	val: 0.576177	test: 0.642209
Best (MAE):	 train: 0.322904	val: 0.432162	test: 0.472758


Epoch: 84
Loss: 0.3524985983967781
RMSE train: 0.475657	val: 0.663305	test: 0.649493
MAE train: 0.351494	val: 0.513264	test: 0.477110

Epoch: 85
Loss: 0.34993627667427063
RMSE train: 0.477208	val: 0.690601	test: 0.681894
MAE train: 0.352935	val: 0.538091	test: 0.497985

Epoch: 86
Loss: 0.38152794539928436
RMSE train: 0.459867	val: 0.681209	test: 0.668528
MAE train: 0.343622	val: 0.517276	test: 0.488263

Epoch: 87
Loss: 0.34152523428201675
RMSE train: 0.441879	val: 0.689429	test: 0.632342
MAE train: 0.334650	val: 0.504286	test: 0.484125

Epoch: 88
Loss: 0.32589486986398697
RMSE train: 0.511340	val: 0.773515	test: 0.649567
MAE train: 0.373469	val: 0.584105	test: 0.484382

Epoch: 89
Loss: 0.38232477754354477
RMSE train: 0.544917	val: 0.767302	test: 0.651763
MAE train: 0.398291	val: 0.573799	test: 0.483187

Epoch: 90
Loss: 0.34133996069431305
RMSE train: 0.449023	val: 0.648563	test: 0.620784
MAE train: 0.336498	val: 0.481429	test: 0.458902

Epoch: 91
Loss: 0.36847732216119766
RMSE train: 0.501729	val: 0.736528	test: 0.660399
MAE train: 0.368449	val: 0.551293	test: 0.487641

Epoch: 92
Loss: 0.37887222319841385
RMSE train: 0.493098	val: 0.680588	test: 0.661324
MAE train: 0.368330	val: 0.502101	test: 0.490136

Epoch: 93
Loss: 0.3882271498441696
RMSE train: 0.522882	val: 0.735596	test: 0.677371
MAE train: 0.389543	val: 0.573405	test: 0.508287

Epoch: 94
Loss: 0.32857901602983475
RMSE train: 0.551665	val: 0.741325	test: 0.704784
MAE train: 0.413840	val: 0.579990	test: 0.522181

Epoch: 95
Loss: 0.3831884413957596
RMSE train: 0.547054	val: 0.731639	test: 0.683238
MAE train: 0.399904	val: 0.565976	test: 0.505455

Epoch: 96
Loss: 0.39989885687828064
RMSE train: 0.537513	val: 0.741054	test: 0.673725
MAE train: 0.388267	val: 0.575979	test: 0.493712

Epoch: 97
Loss: 0.42201466113328934
RMSE train: 0.544936	val: 0.750187	test: 0.694910
MAE train: 0.406630	val: 0.572827	test: 0.520509

Epoch: 98
Loss: 0.4197000339627266
RMSE train: 0.421214	val: 0.633502	test: 0.671198
MAE train: 0.310610	val: 0.472716	test: 0.489781

Epoch: 99
Loss: 0.3211319372057915
RMSE train: 0.444873	val: 0.695136	test: 0.650064
MAE train: 0.318895	val: 0.526111	test: 0.487423

Epoch: 100
Loss: 0.3207269161939621
RMSE train: 0.437312	val: 0.673269	test: 0.651864
MAE train: 0.321831	val: 0.499306	test: 0.487493

Epoch: 101
Loss: 0.3217986896634102
RMSE train: 0.463928	val: 0.704509	test: 0.677568
MAE train: 0.345424	val: 0.514477	test: 0.497647

Epoch: 102
Loss: 0.36379586160182953
RMSE train: 0.431713	val: 0.618534	test: 0.681118
MAE train: 0.317567	val: 0.464493	test: 0.512061

Epoch: 103
Loss: 0.3287561237812042
RMSE train: 0.486762	val: 0.693828	test: 0.647138
MAE train: 0.357187	val: 0.525235	test: 0.473800

Epoch: 104
Loss: 0.29783110320568085
RMSE train: 0.394382	val: 0.644416	test: 0.632329
MAE train: 0.298855	val: 0.463858	test: 0.485519

Epoch: 105
Loss: 0.3461049795150757
RMSE train: 0.476866	val: 0.697610	test: 0.654624
MAE train: 0.349730	val: 0.543031	test: 0.492447

Epoch: 106
Loss: 0.3458528369665146
RMSE train: 0.458927	val: 0.654060	test: 0.660593
MAE train: 0.336165	val: 0.505477	test: 0.492282

Epoch: 107
Loss: 0.3760387897491455
RMSE train: 0.457700	val: 0.621858	test: 0.676363
MAE train: 0.343192	val: 0.466626	test: 0.498812

Epoch: 108
Loss: 0.30924659222364426
RMSE train: 0.492090	val: 0.707857	test: 0.689735
MAE train: 0.351592	val: 0.542896	test: 0.500690

Epoch: 109
Loss: 0.31959249824285507
RMSE train: 0.496632	val: 0.679135	test: 0.669325
MAE train: 0.364009	val: 0.509348	test: 0.480169

Epoch: 110
Loss: 0.3108970671892166
RMSE train: 0.409743	val: 0.631317	test: 0.667120
MAE train: 0.309027	val: 0.463128	test: 0.489473

Epoch: 111
Loss: 0.35362598299980164
RMSE train: 0.436212	val: 0.711823	test: 0.642929
MAE train: 0.319899	val: 0.518459	test: 0.473112

Epoch: 112
Loss: 0.2858818247914314
RMSE train: 0.408674	val: 0.630297	test: 0.628467
MAE train: 0.300780	val: 0.471104	test: 0.472047

Epoch: 113
Loss: 0.31189294904470444
RMSE train: 0.436397	val: 0.641583	test: 0.649196
MAE train: 0.319827	val: 0.490439	test: 0.480949

Epoch: 114
Loss: 0.31288744509220123
RMSE train: 0.438972	val: 0.670380	test: 0.647312
MAE train: 0.323452	val: 0.510089	test: 0.477883

Epoch: 115
Loss: 0.3170502111315727
RMSE train: 0.418301	val: 0.621015	test: 0.648476
MAE train: 0.308579	val: 0.471776	test: 0.484739

Epoch: 116
Loss: 0.35174455493688583
RMSE train: 0.511597	val: 0.708443	test: 0.688141
MAE train: 0.368162	val: 0.548519	test: 0.512247

Epoch: 117
Loss: 0.29493172466754913
RMSE train: 0.450570	val: 0.669352	test: 0.647620
MAE train: 0.331483	val: 0.505150	test: 0.484521

Epoch: 118
Loss: 0.30137064680457115
RMSE train: 0.462057	val: 0.675053	test: 0.640083
MAE train: 0.338471	val: 0.505627	test: 0.479829

Epoch: 119
Loss: 0.32333918660879135
RMSE train: 0.501174	val: 0.687607	test: 0.672483
MAE train: 0.359793	val: 0.528793	test: 0.506141

Epoch: 120
Loss: 0.28446144238114357
RMSE train: 0.399016	val: 0.571191	test: 0.663397
MAE train: 0.298569	val: 0.433990	test: 0.510694

Epoch: 121
Loss: 0.3101765289902687
RMSE train: 0.455397	val: 0.683949	test: 0.642587
MAE train: 0.338834	val: 0.524543	test: 0.483154

Epoch: 122
Loss: 0.29571496695280075
RMSE train: 0.373347	val: 0.604250	test: 0.651292
MAE train: 0.277255	val: 0.459334	test: 0.492514

Epoch: 123
Loss: 0.2798544391989708
RMSE train: 0.403693	val: 0.613394	test: 0.639047
MAE train: 0.296062	val: 0.483960	test: 0.480670

Epoch: 124
Loss: 0.3017275258898735
RMSE train: 0.444290	val: 0.693458	test: 0.652116
MAE train: 0.324888	val: 0.526275	test: 0.490666

Epoch: 125
Loss: 0.29650213569402695
RMSE train: 0.381451	val: 0.614959	test: 0.644391
MAE train: 0.288157	val: 0.470045	test: 0.484699

Epoch: 126
Loss: 0.27395322173833847
RMSE train: 0.415749	val: 0.676115	test: 0.643552
MAE train: 0.310381	val: 0.516914	test: 0.483854

Epoch: 127
Loss: 0.26752032339572906
RMSE train: 0.456131	val: 0.647040	test: 0.659612
MAE train: 0.334555	val: 0.498603	test: 0.493528

Epoch: 128
Loss: 0.2898452877998352
RMSE train: 0.400194	val: 0.630015	test: 0.654469
MAE train: 0.292730	val: 0.482151	test: 0.491517

Epoch: 129
Loss: 0.28487062454223633
RMSE train: 0.470545	val: 0.703992	test: 0.669153
MAE train: 0.350252	val: 0.542356	test: 0.494550

Epoch: 130
Loss: 0.29796313494443893
RMSE train: 0.420552	val: 0.610186	test: 0.657307
MAE train: 0.310485	val: 0.471598	test: 0.486833

Epoch: 131
Loss: 0.29697247594594955
RMSE train: 0.428274	val: 0.645937	test: 0.656215
MAE train: 0.313217	val: 0.488245	test: 0.497788

Epoch: 132
Loss: 0.3000437542796135
RMSE train: 0.425565	val: 0.653301	test: 0.654851
MAE train: 0.306685	val: 0.495465	test: 0.493238

Epoch: 133
Loss: 0.2860409840941429
RMSE train: 0.426409	val: 0.655981	test: 0.659246
MAE train: 0.313070	val: 0.496828	test: 0.487012

Epoch: 134
Loss: 0.306216724216938
RMSE train: 0.392865	val: 0.628869	test: 0.694044
MAE train: 0.288746	val: 0.460654	test: 0.510124

Epoch: 135
Loss: 0.3260568231344223
RMSE train: 0.406418	val: 0.642139	test: 0.659606
MAE train: 0.294921	val: 0.479042	test: 0.490582

Epoch: 136
Loss: 0.29890522360801697
RMSE train: 0.404392	val: 0.643724	test: 0.634653
MAE train: 0.291317	val: 0.477864	test: 0.478761

Epoch: 137
Loss: 0.2967686541378498
RMSE train: 0.376650	val: 0.636911	test: 0.662594
MAE train: 0.275611	val: 0.474919	test: 0.487274

Epoch: 138
Loss: 0.29720304161310196
RMSE train: 0.393570	val: 0.649230	test: 0.671558
MAE train: 0.292109	val: 0.483625	test: 0.492762

Epoch: 139
Loss: 0.31219830363988876
RMSE train: 0.420643	val: 0.646420	test: 0.642322
MAE train: 0.299047	val: 0.487963	test: 0.470249

Epoch: 140
Loss: 0.30193305760622025
RMSE train: 0.437160	val: 0.624413	test: 0.651494
MAE train: 0.314837	val: 0.483355	test: 0.475114

Epoch: 141
Loss: 0.27530133724212646
RMSE train: 0.402287	val: 0.623522	test: 0.667484
MAE train: 0.299445	val: 0.462260	test: 0.493576

Epoch: 142
Loss: 0.31793002039194107
RMSE train: 0.451261	val: 0.661854	test: 0.651466
MAE train: 0.323644	val: 0.514847	test: 0.486632

Epoch: 143
Loss: 0.2508123852312565
RMSE train: 0.488630	val: 0.665890	test: 0.678819
MAE train: 0.347506	val: 0.519112	test: 0.494463

Epoch: 144
Loss: 0.2699337527155876
MAE train: 0.342880	val: 0.529657	test: 0.474765

Epoch: 145
Loss: 0.517132006585598
RMSE train: 0.491186	val: 0.731870	test: 0.653965
MAE train: 0.364898	val: 0.537461	test: 0.482924

Epoch: 146
Loss: 0.34624137729406357
RMSE train: 0.473592	val: 0.695829	test: 0.648099
MAE train: 0.346976	val: 0.508252	test: 0.472684

Epoch: 147
Loss: 0.4306943267583847
RMSE train: 0.490227	val: 0.695640	test: 0.657741
MAE train: 0.357720	val: 0.516518	test: 0.479855

Epoch: 148
Loss: 0.36049266904592514
RMSE train: 0.474551	val: 0.693649	test: 0.650044
MAE train: 0.346271	val: 0.516645	test: 0.473537

Epoch: 149
Loss: 0.353674940764904
RMSE train: 0.451502	val: 0.684296	test: 0.644198
MAE train: 0.329494	val: 0.502789	test: 0.470922

Epoch: 150
Loss: 0.5763291493058205
RMSE train: 0.462604	val: 0.712131	test: 0.626669
MAE train: 0.343157	val: 0.527983	test: 0.453754

Epoch: 151
Loss: 0.3395324796438217
RMSE train: 0.507977	val: 0.752127	test: 0.654155
MAE train: 0.375985	val: 0.555738	test: 0.468436

Epoch: 152
Loss: 0.39750878512859344
RMSE train: 0.473063	val: 0.722257	test: 0.656292
MAE train: 0.347122	val: 0.533069	test: 0.473074

Epoch: 153
Loss: 0.3723226487636566
RMSE train: 0.467551	val: 0.715977	test: 0.681259
MAE train: 0.344806	val: 0.531316	test: 0.497454

Epoch: 154
Loss: 0.3995544984936714
RMSE train: 0.453023	val: 0.700919	test: 0.681834
MAE train: 0.337582	val: 0.522331	test: 0.502549

Epoch: 155
Loss: 0.3442906439304352
RMSE train: 0.526657	val: 0.749659	test: 0.693957
MAE train: 0.386710	val: 0.565878	test: 0.505813

Epoch: 156
Loss: 0.5336874797940254
RMSE train: 0.543440	val: 0.741139	test: 0.703467
MAE train: 0.393060	val: 0.563050	test: 0.494996

Epoch: 157
Loss: 0.3200879581272602
RMSE train: 0.589673	val: 0.785781	test: 0.747426
MAE train: 0.410504	val: 0.572446	test: 0.514951

Epoch: 158
Loss: 0.435918465256691
RMSE train: 0.552857	val: 0.756134	test: 0.699760
MAE train: 0.398006	val: 0.566096	test: 0.490365

Epoch: 159
Loss: 0.3812129870057106
RMSE train: 0.498892	val: 0.715740	test: 0.658770
MAE train: 0.363369	val: 0.535161	test: 0.480320

Epoch: 160
Loss: 0.4161365181207657
RMSE train: 0.485275	val: 0.692666	test: 0.668741
MAE train: 0.354019	val: 0.513679	test: 0.495197

Epoch: 161
Loss: 0.5036438778042793
RMSE train: 0.543396	val: 0.758011	test: 0.686268
MAE train: 0.394830	val: 0.548443	test: 0.488613

Early stopping
Best (RMSE):	 train: 0.489587	val: 0.667936	test: 0.640593
Best (MAE):	 train: 0.361727	val: 0.495168	test: 0.469853
All runs completed.

RMSE train: 0.418755	val: 0.603120	test: 0.655950
MAE train: 0.306764	val: 0.468970	test: 0.489145

Epoch: 145
Loss: 0.2821817696094513
RMSE train: 0.411259	val: 0.605053	test: 0.651731
MAE train: 0.302155	val: 0.465484	test: 0.488495

Epoch: 146
Loss: 0.27351174876093864
RMSE train: 0.490067	val: 0.661899	test: 0.679092
MAE train: 0.352423	val: 0.522070	test: 0.490893

Epoch: 147
Loss: 0.2756565175950527
RMSE train: 0.422914	val: 0.619944	test: 0.640399
MAE train: 0.310728	val: 0.483713	test: 0.473102

Epoch: 148
Loss: 0.2580384500324726
RMSE train: 0.365323	val: 0.609955	test: 0.644285
MAE train: 0.269837	val: 0.456446	test: 0.487658

Epoch: 149
Loss: 0.22498835995793343
RMSE train: 0.431913	val: 0.689184	test: 0.654586
MAE train: 0.316343	val: 0.521199	test: 0.491070

Epoch: 150
Loss: 0.27312812954187393
RMSE train: 0.377647	val: 0.595952	test: 0.652677
MAE train: 0.277464	val: 0.451582	test: 0.497145

Epoch: 151
Loss: 0.26416756957769394
RMSE train: 0.435350	val: 0.651274	test: 0.675211
MAE train: 0.325061	val: 0.491271	test: 0.502716

Epoch: 152
Loss: 0.28543371334671974
RMSE train: 0.428813	val: 0.643883	test: 0.673797
MAE train: 0.318646	val: 0.487145	test: 0.497486

Epoch: 153
Loss: 0.272680327296257
RMSE train: 0.394312	val: 0.612993	test: 0.649019
MAE train: 0.283994	val: 0.479182	test: 0.483168

Epoch: 154
Loss: 0.27328646928071976
RMSE train: 0.446107	val: 0.647376	test: 0.645753
MAE train: 0.321765	val: 0.503260	test: 0.470025

Epoch: 155
Loss: 0.28969793766736984
RMSE train: 0.382482	val: 0.585499	test: 0.638429
MAE train: 0.284052	val: 0.451328	test: 0.489459

Early stopping
Best (RMSE):	 train: 0.399016	val: 0.571191	test: 0.663397
Best (MAE):	 train: 0.298569	val: 0.433990	test: 0.510694
All runs completed.
