>>> Starting run for dataset: bace
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running RANDOM configs_static_noise_experiments/GraphMVP/bace/noise=0.0.yml on cuda:0
Running RANDOM configs_static_noise_experiments/GraphMVP/bace/noise=0.05.yml on cuda:1
Running RANDOM configs_static_noise_experiments/GraphMVP/bace/noise=0.1.yml on cuda:2
Running RANDOM configs_static_noise_experiments/GraphMVP/bace/noise=0.2.yml on cuda:3
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphMVP/bace/noise=0.1.yml --runseed 4 --device cuda:2
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphMVP/bace/noise=0.1.yml --runseed 5 --device cuda:2
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphMVP/bace/noise=0.05.yml --runseed 4 --device cuda:1
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphMVP/bace/noise=0.1.yml --runseed 6 --device cuda:2
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphMVP/bace/noise=0.05.yml --runseed 5 --device cuda:1
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphMVP/bace/noise=0.05.yml --runseed 6 --device cuda:1
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphMVP/bace/noise=0.0.yml --runseed 4 --device cuda:0
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphMVP/bace/noise=0.2.yml --runseed 4 --device cuda:3
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphMVP/bace/noise=0.0.yml --runseed 5 --device cuda:0
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphMVP/bace/noise=0.2.yml --runseed 5 --device cuda:3
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphMVP/bace/noise=0.0.yml --runseed 6 --device cuda:0
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphMVP/bace/noise=0.2.yml --runseed 6 --device cuda:3
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/bace/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: bace
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/static-noise/GraphMVP/bace/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/bace/noise=0.0/bace_scaff_4_26-05_11-16-36  ]
[ Using Seed :  4  ]
[ Using device :  cuda:0  ]
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6783205851821158
ROC train: 0.702880	val: 0.508791	test: 0.635542
PRC train: 0.584330	val: 0.889263	test: 0.669977

Epoch: 2
Loss: 0.6227285326113595
ROC train: 0.744852	val: 0.542857	test: 0.683533
PRC train: 0.621199	val: 0.889254	test: 0.696461

Epoch: 3
Loss: 0.5743176019910091
ROC train: 0.800534	val: 0.570330	test: 0.725439
PRC train: 0.681230	val: 0.893962	test: 0.715496

Epoch: 4
Loss: 0.5257021757594259
ROC train: 0.846296	val: 0.630037	test: 0.776908
PRC train: 0.732021	val: 0.910986	test: 0.752386

Epoch: 5
Loss: 0.49396171079443885
ROC train: 0.852423	val: 0.649817	test: 0.771344
PRC train: 0.737844	val: 0.917737	test: 0.751207

Epoch: 6
Loss: 0.48863750006278484
ROC train: 0.863990	val: 0.671429	test: 0.743697
PRC train: 0.756636	val: 0.923409	test: 0.730572

Epoch: 7
Loss: 0.4738657915854267
ROC train: 0.878079	val: 0.684982	test: 0.754130
PRC train: 0.796419	val: 0.934103	test: 0.747644

Epoch: 8
Loss: 0.43879374073348976
ROC train: 0.878539	val: 0.702198	test: 0.758825
PRC train: 0.816813	val: 0.938332	test: 0.746016

Epoch: 9
Loss: 0.4376610312153987
ROC train: 0.897860	val: 0.701465	test: 0.788211
PRC train: 0.836167	val: 0.938532	test: 0.793563

Epoch: 10
Loss: 0.413922370723363
ROC train: 0.904309	val: 0.687546	test: 0.805425
PRC train: 0.847335	val: 0.935519	test: 0.810312

Epoch: 11
Loss: 0.4075913801933314
ROC train: 0.907671	val: 0.660073	test: 0.807512
PRC train: 0.852198	val: 0.928391	test: 0.799824

Epoch: 12
Loss: 0.42287009096405387
ROC train: 0.907155	val: 0.650549	test: 0.797774
PRC train: 0.852489	val: 0.925299	test: 0.809231

Epoch: 13
Loss: 0.4076796062610525
ROC train: 0.914232	val: 0.669231	test: 0.816554
PRC train: 0.865079	val: 0.929910	test: 0.812441

Epoch: 14
Loss: 0.40793683376079093
ROC train: 0.917383	val: 0.693407	test: 0.814119
PRC train: 0.869527	val: 0.936196	test: 0.810672

Epoch: 15
Loss: 0.3939384006448323
ROC train: 0.919512	val: 0.685348	test: 0.792558
PRC train: 0.873783	val: 0.931753	test: 0.798108

Epoch: 16
Loss: 0.4105035547172669
ROC train: 0.922366	val: 0.688278	test: 0.798991
PRC train: 0.874322	val: 0.933022	test: 0.793698

Epoch: 17
Loss: 0.40155623999890855
ROC train: 0.922551	val: 0.676190	test: 0.805599
PRC train: 0.875123	val: 0.930424	test: 0.790757

Epoch: 18
Loss: 0.3895701693889615
ROC train: 0.920011	val: 0.657509	test: 0.800556
PRC train: 0.872635	val: 0.926915	test: 0.800726

Epoch: 19
Loss: 0.39053875072799177
ROC train: 0.928005	val: 0.679853	test: 0.805947
PRC train: 0.885439	val: 0.931735	test: 0.802921

Epoch: 20
Loss: 0.37955403935778975
ROC train: 0.927380	val: 0.700000	test: 0.797253
PRC train: 0.881876	val: 0.936109	test: 0.796066

Epoch: 21
Loss: 0.37934160302235814
ROC train: 0.932129	val: 0.670330	test: 0.800383
PRC train: 0.889800	val: 0.929674	test: 0.807030

Epoch: 22
Loss: 0.3666586984771957
ROC train: 0.935959	val: 0.662271	test: 0.804903
PRC train: 0.897158	val: 0.928723	test: 0.813395

Epoch: 23
Loss: 0.3665249058966481
ROC train: 0.937700	val: 0.676190	test: 0.810294
PRC train: 0.900497	val: 0.931058	test: 0.817275

Epoch: 24
Loss: 0.3524984620589823
ROC train: 0.936955	val: 0.694139	test: 0.796731
PRC train: 0.898254	val: 0.934886	test: 0.805810

Epoch: 25
Loss: 0.3633217985269467
ROC train: 0.934138	val: 0.682051	test: 0.801426
PRC train: 0.893476	val: 0.932490	test: 0.812786

Epoch: 26
Loss: 0.34819103359658027
ROC train: 0.936849	val: 0.669597	test: 0.816206
PRC train: 0.898548	val: 0.931867	test: 0.818825

Epoch: 27
Loss: 0.34568490931852275
ROC train: 0.937828	val: 0.661172	test: 0.805425
PRC train: 0.898967	val: 0.924710	test: 0.816035

Epoch: 28
Loss: 0.34572056935814627
ROC train: 0.939466	val: 0.663736	test: 0.808555
PRC train: 0.903015	val: 0.925833	test: 0.814818

Epoch: 29
Loss: 0.32607335529482395
ROC train: 0.942523	val: 0.661905	test: 0.802295
PRC train: 0.910258	val: 0.925099	test: 0.804154

Epoch: 30
Loss: 0.34922540522777784
ROC train: 0.945063	val: 0.671795	test: 0.786124
PRC train: 0.913451	val: 0.929247	test: 0.803489

Epoch: 31
Loss: 0.34891674222102914
ROC train: 0.945882	val: 0.679121	test: 0.786820
PRC train: 0.913488	val: 0.930864	test: 0.800439

Epoch: 32
Loss: 0.33847337834179864
ROC train: 0.944381	val: 0.661538	test: 0.792558
PRC train: 0.913365	val: 0.928091	test: 0.802460

Epoch: 33
Loss: 0.3366115645179518
ROC train: 0.946684	val: 0.655678	test: 0.790819Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/bace/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: bace
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/static-noise/GraphMVP/bace/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/bace/noise=0.0/bace_scaff_6_26-05_11-16-36  ]
[ Using Seed :  6  ]
[ Using device :  cuda:0  ]
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6711908594910111
ROC train: 0.702982	val: 0.511355	test: 0.651017
PRC train: 0.593631	val: 0.884704	test: 0.712970

Epoch: 2
Loss: 0.6107520013054228
ROC train: 0.769663	val: 0.582418	test: 0.712224
PRC train: 0.650072	val: 0.893435	test: 0.729641

Epoch: 3
Loss: 0.5644811778270864
ROC train: 0.811396	val: 0.634799	test: 0.744218
PRC train: 0.684214	val: 0.895766	test: 0.733735

Epoch: 4
Loss: 0.5126631302052301
ROC train: 0.843191	val: 0.660806	test: 0.778995
PRC train: 0.714960	val: 0.900169	test: 0.752018

Epoch: 5
Loss: 0.49251403444987946
ROC train: 0.860522	val: 0.663370	test: 0.788559
PRC train: 0.739395	val: 0.904122	test: 0.760801

Epoch: 6
Loss: 0.4601621230493501
ROC train: 0.867092	val: 0.663004	test: 0.781603
PRC train: 0.753922	val: 0.907464	test: 0.768571

Epoch: 7
Loss: 0.4543346341075646
ROC train: 0.886273	val: 0.701832	test: 0.789602
PRC train: 0.804598	val: 0.933369	test: 0.765947

Epoch: 8
Loss: 0.45154451891958497
ROC train: 0.887968	val: 0.719780	test: 0.784559
PRC train: 0.820394	val: 0.939763	test: 0.776304

Epoch: 9
Loss: 0.42867167120729804
ROC train: 0.906789	val: 0.693773	test: 0.789776
PRC train: 0.845602	val: 0.931861	test: 0.797092

Epoch: 10
Loss: 0.42338771611187875
ROC train: 0.908596	val: 0.665934	test: 0.797948
PRC train: 0.848933	val: 0.924266	test: 0.794063

Epoch: 11
Loss: 0.4033493559153068
ROC train: 0.904501	val: 0.628571	test: 0.797600
PRC train: 0.843108	val: 0.915087	test: 0.802100

Epoch: 12
Loss: 0.39886859848107614
ROC train: 0.905217	val: 0.643223	test: 0.778647
PRC train: 0.843812	val: 0.921642	test: 0.786581

Epoch: 13
Loss: 0.40480219490377706
ROC train: 0.918065	val: 0.681319	test: 0.800556
PRC train: 0.868146	val: 0.929351	test: 0.794344

Epoch: 14
Loss: 0.38621256734192183
ROC train: 0.924221	val: 0.690476	test: 0.811511
PRC train: 0.877251	val: 0.932839	test: 0.810631

Epoch: 15
Loss: 0.3841436278275435
ROC train: 0.920571	val: 0.672894	test: 0.810120
PRC train: 0.869766	val: 0.930446	test: 0.816849

Epoch: 16
Loss: 0.3883454521121024
ROC train: 0.925271	val: 0.687912	test: 0.803165
PRC train: 0.879656	val: 0.932776	test: 0.807504

Epoch: 17
Loss: 0.37899226018317744
ROC train: 0.927306	val: 0.677656	test: 0.788211
PRC train: 0.882405	val: 0.927428	test: 0.811175

Epoch: 18
Loss: 0.37668464229787174
ROC train: 0.929275	val: 0.660440	test: 0.794471
PRC train: 0.884605	val: 0.923117	test: 0.821154

Epoch: 19
Loss: 0.36032208205609756
ROC train: 0.931244	val: 0.667766	test: 0.813076
PRC train: 0.886090	val: 0.924779	test: 0.824107

Epoch: 20
Loss: 0.3697851785837493
ROC train: 0.930337	val: 0.665934	test: 0.792906
PRC train: 0.882732	val: 0.926590	test: 0.813088

Epoch: 21
Loss: 0.3649360952262798
ROC train: 0.930634	val: 0.660073	test: 0.766302
PRC train: 0.884936	val: 0.925843	test: 0.795672

Epoch: 22
Loss: 0.36080406504726065
ROC train: 0.936259	val: 0.672894	test: 0.787515
PRC train: 0.896765	val: 0.924266	test: 0.802118

Epoch: 23
Loss: 0.36845454090278695
ROC train: 0.935608	val: 0.666667	test: 0.806121
PRC train: 0.894816	val: 0.926616	test: 0.826682

Epoch: 24
Loss: 0.3453693625702254
ROC train: 0.937471	val: 0.693407	test: 0.797948
PRC train: 0.894726	val: 0.932152	test: 0.816709

Epoch: 25
Loss: 0.3502820971524231
ROC train: 0.937546	val: 0.701099	test: 0.799513
PRC train: 0.898015	val: 0.931647	test: 0.813953

Epoch: 26
Loss: 0.34714781090654234
ROC train: 0.941270	val: 0.676923	test: 0.802295
PRC train: 0.904194	val: 0.926969	test: 0.823520

Epoch: 27
Loss: 0.34332245982195814
ROC train: 0.937055	val: 0.650916	test: 0.805599
PRC train: 0.899407	val: 0.920304	test: 0.819204

Epoch: 28
Loss: 0.3401103376790658
ROC train: 0.934760	val: 0.671062	test: 0.794123
PRC train: 0.898616	val: 0.926487	test: 0.803366

Epoch: 29
Loss: 0.3397101663416411
ROC train: 0.943299	val: 0.683883	test: 0.786646
PRC train: 0.907501	val: 0.930678	test: 0.799900

Epoch: 30
Loss: 0.3445624160802519
ROC train: 0.942785	val: 0.668864	test: 0.778995
PRC train: 0.908452	val: 0.926249	test: 0.795471

Epoch: 31
Loss: 0.349876978922579
ROC train: 0.944164	val: 0.655311	test: 0.789776
PRC train: 0.910650	val: 0.923185	test: 0.804323

Epoch: 32
Loss: 0.3281175622603307
ROC train: 0.944486	val: 0.654212	test: 0.794471
PRC train: 0.910256	val: 0.925201	test: 0.814866

Epoch: 33
Loss: 0.32871193439357793
ROC train: 0.947654	val: 0.678022	test: 0.777430Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/bace/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: bace
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/static-noise/GraphMVP/bace/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/bace/noise=0.0/bace_scaff_5_26-05_11-16-36  ]
[ Using Seed :  5  ]
[ Using device :  cuda:0  ]
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6605699371448022
ROC train: 0.712514	val: 0.567033	test: 0.654495
PRC train: 0.578302	val: 0.892205	test: 0.662610

Epoch: 2
Loss: 0.6035784010638063
ROC train: 0.752571	val: 0.553480	test: 0.699878
PRC train: 0.624872	val: 0.887818	test: 0.690918

Epoch: 3
Loss: 0.5536623219519609
ROC train: 0.809386	val: 0.594505	test: 0.768736
PRC train: 0.688125	val: 0.897286	test: 0.747679

Epoch: 4
Loss: 0.5125090918634628
ROC train: 0.850813	val: 0.624176	test: 0.789428
PRC train: 0.733740	val: 0.903914	test: 0.762795

Epoch: 5
Loss: 0.4738341546214754
ROC train: 0.866724	val: 0.627473	test: 0.768040
PRC train: 0.752050	val: 0.903478	test: 0.764822

Epoch: 6
Loss: 0.47057971027052165
ROC train: 0.871016	val: 0.621245	test: 0.748044
PRC train: 0.763979	val: 0.901791	test: 0.754214

Epoch: 7
Loss: 0.44195859542863214
ROC train: 0.888730	val: 0.637363	test: 0.796209
PRC train: 0.811716	val: 0.924417	test: 0.786954

Epoch: 8
Loss: 0.4284400916747396
ROC train: 0.895999	val: 0.660806	test: 0.792906
PRC train: 0.830436	val: 0.923514	test: 0.780973

Epoch: 9
Loss: 0.43231239446297887
ROC train: 0.902765	val: 0.693773	test: 0.783168
PRC train: 0.842743	val: 0.929917	test: 0.791917

Epoch: 10
Loss: 0.42016873551608763
ROC train: 0.899720	val: 0.689744	test: 0.797948
PRC train: 0.837831	val: 0.930860	test: 0.802735

Epoch: 11
Loss: 0.4223509623631282
ROC train: 0.913076	val: 0.660440	test: 0.813772
PRC train: 0.861620	val: 0.925945	test: 0.828841

Epoch: 12
Loss: 0.40199429203502657
ROC train: 0.918727	val: 0.659707	test: 0.820901
PRC train: 0.869699	val: 0.925787	test: 0.834691

Epoch: 13
Loss: 0.41176695681555364
ROC train: 0.916010	val: 0.657875	test: 0.827856
PRC train: 0.865914	val: 0.925069	test: 0.834148

Epoch: 14
Loss: 0.4014339530302025
ROC train: 0.915896	val: 0.650549	test: 0.819162
PRC train: 0.865391	val: 0.923999	test: 0.812953

Epoch: 15
Loss: 0.392344172986539
ROC train: 0.920360	val: 0.669231	test: 0.808903
PRC train: 0.872861	val: 0.927585	test: 0.820372

Epoch: 16
Loss: 0.37322253704743047
ROC train: 0.927994	val: 0.667399	test: 0.820031
PRC train: 0.884120	val: 0.924361	test: 0.828397

Epoch: 17
Loss: 0.37441122536360005
ROC train: 0.925171	val: 0.662637	test: 0.814989
PRC train: 0.878026	val: 0.923054	test: 0.824449

Epoch: 18
Loss: 0.38934263086201487
ROC train: 0.922694	val: 0.643956	test: 0.806295
PRC train: 0.876829	val: 0.920236	test: 0.811425

Epoch: 19
Loss: 0.36951286251744925
ROC train: 0.929164	val: 0.642857	test: 0.813424
PRC train: 0.889940	val: 0.920836	test: 0.822964

Epoch: 20
Loss: 0.37127761895898315
ROC train: 0.928057	val: 0.632967	test: 0.787167
PRC train: 0.886629	val: 0.919183	test: 0.824084

Epoch: 21
Loss: 0.3646499070804781
ROC train: 0.933142	val: 0.674725	test: 0.799339
PRC train: 0.891885	val: 0.927503	test: 0.810969

Epoch: 22
Loss: 0.36720213554506215
ROC train: 0.934906	val: 0.657509	test: 0.809598
PRC train: 0.894678	val: 0.926122	test: 0.824146

Epoch: 23
Loss: 0.351411916770611
ROC train: 0.933896	val: 0.665201	test: 0.805425
PRC train: 0.894358	val: 0.927508	test: 0.818032

Epoch: 24
Loss: 0.3633202062736084
ROC train: 0.936658	val: 0.668132	test: 0.786646
PRC train: 0.896567	val: 0.927567	test: 0.791959

Epoch: 25
Loss: 0.35632558926187874
ROC train: 0.936764	val: 0.675092	test: 0.758825
PRC train: 0.896973	val: 0.925077	test: 0.789699

Epoch: 26
Loss: 0.3641365639457349
ROC train: 0.940354	val: 0.663004	test: 0.795340
PRC train: 0.904209	val: 0.924933	test: 0.815312

Epoch: 27
Loss: 0.34577931978095877
ROC train: 0.931744	val: 0.651282	test: 0.793253
PRC train: 0.893290	val: 0.919951	test: 0.787257

Epoch: 28
Loss: 0.3612930860746405
ROC train: 0.932945	val: 0.662271	test: 0.800556
PRC train: 0.893827	val: 0.922097	test: 0.798930

Epoch: 29
Loss: 0.33865535839110616
ROC train: 0.939495	val: 0.662271	test: 0.808555
PRC train: 0.904182	val: 0.923426	test: 0.820065

Epoch: 30
Loss: 0.33016231878258
ROC train: 0.946099	val: 0.668498	test: 0.815163
PRC train: 0.913876	val: 0.925856	test: 0.827998

Epoch: 31
Loss: 0.3337918156840424
ROC train: 0.947711	val: 0.662637	test: 0.806990
PRC train: 0.917052	val: 0.921997	test: 0.823317

Epoch: 32
Loss: 0.3336783999350267
ROC train: 0.946632	val: 0.647619	test: 0.797253
PRC train: 0.915749	val: 0.913703	test: 0.815272

Epoch: 33
Loss: 0.3370023901567177
ROC train: 0.948938	val: 0.674359	test: 0.803512Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/bace/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: bace
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/static-noise/GraphMVP/bace/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/bace/noise=0.05/bace_scaff_4_26-05_11-16-36  ]
[ Using Seed :  4  ]
[ Using device :  cuda:1  ]
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6718750148610224
ROC train: 0.682800	val: 0.551648	test: 0.591202
PRC train: 0.570828	val: 0.892723	test: 0.610297

Epoch: 2
Loss: 0.6265814013064652
ROC train: 0.727845	val: 0.591209	test: 0.644410
PRC train: 0.609669	val: 0.903689	test: 0.659083

Epoch: 3
Loss: 0.5931810437321605
ROC train: 0.760108	val: 0.605128	test: 0.670492
PRC train: 0.641745	val: 0.906642	test: 0.686965

Epoch: 4
Loss: 0.5699060696639776
ROC train: 0.799951	val: 0.633333	test: 0.687880
PRC train: 0.683924	val: 0.903594	test: 0.697098

Epoch: 5
Loss: 0.5302401143497976
ROC train: 0.826116	val: 0.674725	test: 0.696227
PRC train: 0.714083	val: 0.911295	test: 0.688462

Epoch: 6
Loss: 0.5288449552491218
ROC train: 0.842971	val: 0.676557	test: 0.720396
PRC train: 0.740227	val: 0.908122	test: 0.688877

Epoch: 7
Loss: 0.4989975350300857
ROC train: 0.860933	val: 0.677289	test: 0.732568
PRC train: 0.767055	val: 0.913061	test: 0.717926

Epoch: 8
Loss: 0.4931533511683307
ROC train: 0.873813	val: 0.678388	test: 0.739176
PRC train: 0.786139	val: 0.913399	test: 0.728475

Epoch: 9
Loss: 0.4852085747978941
ROC train: 0.883818	val: 0.682051	test: 0.743349
PRC train: 0.806525	val: 0.918326	test: 0.743867

Epoch: 10
Loss: 0.4681012672899283
ROC train: 0.892266	val: 0.695971	test: 0.743523
PRC train: 0.819083	val: 0.927296	test: 0.731151

Epoch: 11
Loss: 0.4523467827266215
ROC train: 0.886952	val: 0.666667	test: 0.727700
PRC train: 0.813557	val: 0.921351	test: 0.735578

Epoch: 12
Loss: 0.435584780173219
ROC train: 0.904030	val: 0.701099	test: 0.750478
PRC train: 0.840305	val: 0.936662	test: 0.749669

Epoch: 13
Loss: 0.4224355620605252
ROC train: 0.916475	val: 0.723810	test: 0.750652
PRC train: 0.861262	val: 0.941331	test: 0.735817

Epoch: 14
Loss: 0.42663089382274294
ROC train: 0.916935	val: 0.734432	test: 0.748218
PRC train: 0.864652	val: 0.945441	test: 0.741351

Epoch: 15
Loss: 0.4283188409107342
ROC train: 0.920205	val: 0.708425	test: 0.758477
PRC train: 0.872926	val: 0.937982	test: 0.756415

Epoch: 16
Loss: 0.41601445401764936
ROC train: 0.927891	val: 0.693773	test: 0.773431
PRC train: 0.881009	val: 0.932711	test: 0.767280

Epoch: 17
Loss: 0.4118837245438078
ROC train: 0.926661	val: 0.687179	test: 0.764215
PRC train: 0.887290	val: 0.930222	test: 0.784627

Epoch: 18
Loss: 0.3947045559492639
ROC train: 0.932183	val: 0.687912	test: 0.750826
PRC train: 0.894873	val: 0.931264	test: 0.759607

Epoch: 19
Loss: 0.4079057528936431
ROC train: 0.933955	val: 0.708059	test: 0.753434
PRC train: 0.889603	val: 0.937472	test: 0.736958

Epoch: 20
Loss: 0.38200483947772085
ROC train: 0.939629	val: 0.713919	test: 0.772387
PRC train: 0.896347	val: 0.938420	test: 0.761430

Epoch: 21
Loss: 0.38219745971257274
ROC train: 0.945151	val: 0.712821	test: 0.769258
PRC train: 0.911457	val: 0.939450	test: 0.762660

Epoch: 22
Loss: 0.36868205461115133
ROC train: 0.948850	val: 0.723810	test: 0.756912
PRC train: 0.916047	val: 0.940741	test: 0.748756

Epoch: 23
Loss: 0.3603224096922265
ROC train: 0.950234	val: 0.715018	test: 0.755347
PRC train: 0.918533	val: 0.938193	test: 0.749902

Epoch: 24
Loss: 0.3789804526151265
ROC train: 0.946450	val: 0.699267	test: 0.759346
PRC train: 0.912634	val: 0.931351	test: 0.763637

Epoch: 25
Loss: 0.3483713589864047
ROC train: 0.950987	val: 0.702198	test: 0.768040
PRC train: 0.923744	val: 0.932796	test: 0.786888

Epoch: 26
Loss: 0.3457865956271745
ROC train: 0.956064	val: 0.710623	test: 0.773083
PRC train: 0.928487	val: 0.934881	test: 0.762477

Epoch: 27
Loss: 0.3393506001588448
ROC train: 0.959147	val: 0.709158	test: 0.777430
PRC train: 0.930628	val: 0.935756	test: 0.788668

Epoch: 28
Loss: 0.3325917374759227
ROC train: 0.961313	val: 0.727473	test: 0.770301
PRC train: 0.933888	val: 0.941524	test: 0.785432

Epoch: 29
Loss: 0.33640045347071357
ROC train: 0.960742	val: 0.734432	test: 0.750478
PRC train: 0.933716	val: 0.944970	test: 0.770520

Epoch: 30
Loss: 0.31683163029882355
ROC train: 0.965788	val: 0.703663	test: 0.768214
PRC train: 0.945954	val: 0.934983	test: 0.774526

Epoch: 31
Loss: 0.3152575922436204
ROC train: 0.967997	val: 0.706227	test: 0.771170
PRC train: 0.948195	val: 0.931367	test: 0.773356

Epoch: 32
Loss: 0.3119271296220655
ROC train: 0.972260	val: 0.712454	test: 0.769605Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/bace/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: bace
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/static-noise/GraphMVP/bace/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/bace/noise=0.1/bace_scaff_5_26-05_11-16-36  ]
[ Using Seed :  5  ]
[ Using device :  cuda:2  ]
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6754083504712087
ROC train: 0.636801	val: 0.578388	test: 0.663363
PRC train: 0.500373	val: 0.895239	test: 0.660333

Epoch: 2
Loss: 0.6399032677055048
ROC train: 0.682326	val: 0.580952	test: 0.704747
PRC train: 0.546788	val: 0.897933	test: 0.706857

Epoch: 3
Loss: 0.6081750407284612
ROC train: 0.726358	val: 0.590110	test: 0.746653
PRC train: 0.599819	val: 0.899708	test: 0.738883

Epoch: 4
Loss: 0.5862864115055478
ROC train: 0.769523	val: 0.584249	test: 0.738654
PRC train: 0.660330	val: 0.894092	test: 0.723657

Epoch: 5
Loss: 0.5636786099025869
ROC train: 0.799483	val: 0.571062	test: 0.712572
PRC train: 0.705558	val: 0.882112	test: 0.715247

Epoch: 6
Loss: 0.539594168404381
ROC train: 0.823456	val: 0.627106	test: 0.745783
PRC train: 0.729344	val: 0.899057	test: 0.732347

Epoch: 7
Loss: 0.5340311331164441
ROC train: 0.838379	val: 0.626740	test: 0.769258
PRC train: 0.748510	val: 0.900289	test: 0.755381

Epoch: 8
Loss: 0.5199348250551661
ROC train: 0.849980	val: 0.639560	test: 0.777952
PRC train: 0.765900	val: 0.903126	test: 0.757334

Epoch: 9
Loss: 0.518064626999571
ROC train: 0.862454	val: 0.663736	test: 0.776387
PRC train: 0.785689	val: 0.915663	test: 0.754181

Epoch: 10
Loss: 0.5051798764759315
ROC train: 0.874846	val: 0.672161	test: 0.784907
PRC train: 0.806521	val: 0.922785	test: 0.761970

Epoch: 11
Loss: 0.4854765623796326
ROC train: 0.877677	val: 0.656044	test: 0.780560
PRC train: 0.811906	val: 0.916400	test: 0.761982

Epoch: 12
Loss: 0.48183189599651144
ROC train: 0.888562	val: 0.690476	test: 0.780560
PRC train: 0.832325	val: 0.927243	test: 0.759849

Epoch: 13
Loss: 0.4808921373744832
ROC train: 0.900873	val: 0.704762	test: 0.789950
PRC train: 0.850584	val: 0.929631	test: 0.762027

Epoch: 14
Loss: 0.4494962871174639
ROC train: 0.905037	val: 0.697070	test: 0.789950
PRC train: 0.856131	val: 0.934775	test: 0.773193

Epoch: 15
Loss: 0.4493512087486905
ROC train: 0.910602	val: 0.717949	test: 0.793253
PRC train: 0.865487	val: 0.941903	test: 0.776223

Epoch: 16
Loss: 0.43540520102231656
ROC train: 0.923322	val: 0.730037	test: 0.788385
PRC train: 0.884653	val: 0.947111	test: 0.776246

Epoch: 17
Loss: 0.42405019781415093
ROC train: 0.928721	val: 0.741758	test: 0.786994
PRC train: 0.891407	val: 0.951360	test: 0.776503

Epoch: 18
Loss: 0.4174047245207331
ROC train: 0.930123	val: 0.716484	test: 0.790819
PRC train: 0.889780	val: 0.945500	test: 0.765226

Epoch: 19
Loss: 0.39940849472644946
ROC train: 0.941624	val: 0.715385	test: 0.795514
PRC train: 0.911396	val: 0.942516	test: 0.790921

Epoch: 20
Loss: 0.4158329838865624
ROC train: 0.940297	val: 0.715385	test: 0.794644
PRC train: 0.916393	val: 0.943424	test: 0.787695

Epoch: 21
Loss: 0.3870736537346673
ROC train: 0.946587	val: 0.749817	test: 0.783864
PRC train: 0.923692	val: 0.952469	test: 0.782952

Epoch: 22
Loss: 0.3866203243154598
ROC train: 0.955876	val: 0.740293	test: 0.781429
PRC train: 0.935414	val: 0.951402	test: 0.783181

Epoch: 23
Loss: 0.37223929328639965
ROC train: 0.963082	val: 0.733700	test: 0.786298
PRC train: 0.944116	val: 0.949846	test: 0.785599

Epoch: 24
Loss: 0.36010463030820394
ROC train: 0.964118	val: 0.719048	test: 0.789080
PRC train: 0.946324	val: 0.946389	test: 0.780082

Epoch: 25
Loss: 0.35390950657336273
ROC train: 0.964532	val: 0.703663	test: 0.803339
PRC train: 0.949020	val: 0.943683	test: 0.792660

Epoch: 26
Loss: 0.3429457128755319
ROC train: 0.972012	val: 0.705495	test: 0.793427
PRC train: 0.957786	val: 0.943248	test: 0.777118

Epoch: 27
Loss: 0.32703467559212396
ROC train: 0.977026	val: 0.723443	test: 0.786820
PRC train: 0.965734	val: 0.946872	test: 0.776616

Epoch: 28
Loss: 0.3142183918773617
ROC train: 0.975000	val: 0.740293	test: 0.789602
PRC train: 0.964906	val: 0.951204	test: 0.778130

Epoch: 29
Loss: 0.31689938664516265
ROC train: 0.972366	val: 0.723443	test: 0.785776
PRC train: 0.960199	val: 0.947730	test: 0.770545

Epoch: 30
Loss: 0.31287666794151003
ROC train: 0.984441	val: 0.739560	test: 0.769605
PRC train: 0.977477	val: 0.952296	test: 0.777299

Epoch: 31
Loss: 0.3045356917768761
ROC train: 0.983493	val: 0.738462	test: 0.783168
PRC train: 0.976835	val: 0.952912	test: 0.793763

Epoch: 32
Loss: 0.3050225502041813
ROC train: 0.981022	val: 0.716117	test: 0.794123Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/bace/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: bace
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/static-noise/GraphMVP/bace/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/bace/noise=0.05/bace_scaff_5_26-05_11-16-36  ]
[ Using Seed :  5  ]
[ Using device :  cuda:1  ]
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6753421108473223
ROC train: 0.666253	val: 0.558608	test: 0.687533
PRC train: 0.532371	val: 0.890781	test: 0.670817

Epoch: 2
Loss: 0.6314129006955593
ROC train: 0.720465	val: 0.553114	test: 0.736741
PRC train: 0.590743	val: 0.893613	test: 0.728138

Epoch: 3
Loss: 0.5972177321443686
ROC train: 0.761652	val: 0.549451	test: 0.776387
PRC train: 0.638070	val: 0.892390	test: 0.750215

Epoch: 4
Loss: 0.5693115636466872
ROC train: 0.790574	val: 0.582784	test: 0.790819
PRC train: 0.676880	val: 0.896522	test: 0.753737

Epoch: 5
Loss: 0.5428627489512716
ROC train: 0.819874	val: 0.611355	test: 0.796557
PRC train: 0.715450	val: 0.901364	test: 0.749084

Epoch: 6
Loss: 0.510613127704572
ROC train: 0.843231	val: 0.610256	test: 0.804382
PRC train: 0.752353	val: 0.898296	test: 0.766867

Epoch: 7
Loss: 0.5135067551918268
ROC train: 0.857777	val: 0.615751	test: 0.799687
PRC train: 0.772993	val: 0.902408	test: 0.781653

Epoch: 8
Loss: 0.5039386335743992
ROC train: 0.864124	val: 0.607692	test: 0.794471
PRC train: 0.784215	val: 0.909449	test: 0.783828

Epoch: 9
Loss: 0.48728024574228945
ROC train: 0.878810	val: 0.651648	test: 0.809772
PRC train: 0.810273	val: 0.925505	test: 0.784087

Epoch: 10
Loss: 0.4735354990966833
ROC train: 0.884035	val: 0.673260	test: 0.807860
PRC train: 0.824968	val: 0.930976	test: 0.788921

Epoch: 11
Loss: 0.44300390117324656
ROC train: 0.893370	val: 0.616484	test: 0.815858
PRC train: 0.832601	val: 0.916978	test: 0.788596

Epoch: 12
Loss: 0.4654585374331292
ROC train: 0.896715	val: 0.614652	test: 0.795340
PRC train: 0.837432	val: 0.919033	test: 0.783740

Epoch: 13
Loss: 0.4461580393374744
ROC train: 0.909033	val: 0.674359	test: 0.809946
PRC train: 0.856941	val: 0.934394	test: 0.801762

Epoch: 14
Loss: 0.4256259000537727
ROC train: 0.909115	val: 0.692674	test: 0.823161
PRC train: 0.862279	val: 0.938447	test: 0.814212

Epoch: 15
Loss: 0.41701597987583866
ROC train: 0.917491	val: 0.694139	test: 0.812554
PRC train: 0.871091	val: 0.938619	test: 0.792695

Epoch: 16
Loss: 0.4135054042242265
ROC train: 0.926227	val: 0.694505	test: 0.808381
PRC train: 0.883942	val: 0.939781	test: 0.790544

Epoch: 17
Loss: 0.3973845384226122
ROC train: 0.928025	val: 0.684615	test: 0.812207
PRC train: 0.883758	val: 0.938036	test: 0.804319

Epoch: 18
Loss: 0.4036406039032423
ROC train: 0.933114	val: 0.660806	test: 0.805947
PRC train: 0.890703	val: 0.926019	test: 0.787978

Epoch: 19
Loss: 0.38525704176601827
ROC train: 0.943388	val: 0.689744	test: 0.800730
PRC train: 0.909417	val: 0.937155	test: 0.791635

Epoch: 20
Loss: 0.4113040938165998
ROC train: 0.938299	val: 0.690842	test: 0.802643
PRC train: 0.904964	val: 0.936008	test: 0.796278

Epoch: 21
Loss: 0.37465788851076115
ROC train: 0.935491	val: 0.691941	test: 0.793949
PRC train: 0.901832	val: 0.934217	test: 0.786898

Epoch: 22
Loss: 0.37200916513696497
ROC train: 0.949027	val: 0.684615	test: 0.805773
PRC train: 0.918174	val: 0.932889	test: 0.811693

Epoch: 23
Loss: 0.3602874045594914
ROC train: 0.951692	val: 0.670696	test: 0.814467
PRC train: 0.921848	val: 0.929643	test: 0.808741

Epoch: 24
Loss: 0.35609878094018044
ROC train: 0.951815	val: 0.700733	test: 0.793253
PRC train: 0.917438	val: 0.939533	test: 0.793379

Epoch: 25
Loss: 0.34418732093405996
ROC train: 0.955588	val: 0.708059	test: 0.797774
PRC train: 0.927918	val: 0.942303	test: 0.807676

Epoch: 26
Loss: 0.3440295658290668
ROC train: 0.962120	val: 0.688278	test: 0.813424
PRC train: 0.942823	val: 0.936983	test: 0.801996

Epoch: 27
Loss: 0.3338095449349737
ROC train: 0.962868	val: 0.634799	test: 0.803165
PRC train: 0.940772	val: 0.923471	test: 0.785936

Epoch: 28
Loss: 0.3291917904236713
ROC train: 0.964389	val: 0.664469	test: 0.796557
PRC train: 0.942284	val: 0.930689	test: 0.794907

Epoch: 29
Loss: 0.31386973593282497
ROC train: 0.964172	val: 0.675092	test: 0.797948
PRC train: 0.944206	val: 0.934474	test: 0.803969

Epoch: 30
Loss: 0.3026664250104876
ROC train: 0.966798	val: 0.695238	test: 0.803339
PRC train: 0.950311	val: 0.935966	test: 0.795850

Epoch: 31
Loss: 0.3321676802384492
ROC train: 0.962386	val: 0.679121	test: 0.799861
PRC train: 0.943622	val: 0.931749	test: 0.787604

Epoch: 32
Loss: 0.28869627391964325
ROC train: 0.967334	val: 0.695238	test: 0.804556Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/bace/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:3
  input_data_dir: 
  dataset: bace
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/static-noise/GraphMVP/bace/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/bace/noise=0.2/bace_scaff_5_26-05_11-16-36  ]
[ Using Seed :  5  ]
[ Using device :  cuda:3  ]
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6831245990275349
ROC train: 0.596230	val: 0.560440	test: 0.657103
PRC train: 0.481210	val: 0.893291	test: 0.647160

Epoch: 2
Loss: 0.6602805859755942
ROC train: 0.647200	val: 0.565568	test: 0.679012
PRC train: 0.528227	val: 0.899615	test: 0.700247

Epoch: 3
Loss: 0.6425277914068932
ROC train: 0.686943	val: 0.573626	test: 0.691010
PRC train: 0.566763	val: 0.898828	test: 0.700798

Epoch: 4
Loss: 0.6271392697545343
ROC train: 0.723513	val: 0.594139	test: 0.692054
PRC train: 0.606516	val: 0.903391	test: 0.694457

Epoch: 5
Loss: 0.607903786745664
ROC train: 0.754737	val: 0.560073	test: 0.677969
PRC train: 0.643710	val: 0.890586	test: 0.680819

Epoch: 6
Loss: 0.5791169444757046
ROC train: 0.768057	val: 0.540293	test: 0.666145
PRC train: 0.669098	val: 0.879865	test: 0.679996

Epoch: 7
Loss: 0.5701443892778835
ROC train: 0.799652	val: 0.600733	test: 0.708746
PRC train: 0.695910	val: 0.897666	test: 0.703208

Epoch: 8
Loss: 0.5643285857546518
ROC train: 0.821721	val: 0.645421	test: 0.711876
PRC train: 0.721223	val: 0.905021	test: 0.702556

Epoch: 9
Loss: 0.5499855646346021
ROC train: 0.826632	val: 0.646154	test: 0.705964
PRC train: 0.734754	val: 0.901283	test: 0.698816

Epoch: 10
Loss: 0.5237933034200898
ROC train: 0.859618	val: 0.712088	test: 0.719527
PRC train: 0.779356	val: 0.924098	test: 0.698840

Epoch: 11
Loss: 0.5250500300013783
ROC train: 0.865999	val: 0.707692	test: 0.710833
PRC train: 0.790746	val: 0.923166	test: 0.696241

Epoch: 12
Loss: 0.5030375720218874
ROC train: 0.869561	val: 0.697802	test: 0.722309
PRC train: 0.797476	val: 0.918106	test: 0.709011

Epoch: 13
Loss: 0.5023736839692707
ROC train: 0.888510	val: 0.750549	test: 0.725613
PRC train: 0.828110	val: 0.941000	test: 0.696315

Epoch: 14
Loss: 0.48408232659245953
ROC train: 0.893322	val: 0.758608	test: 0.716049
PRC train: 0.833415	val: 0.946774	test: 0.686687

Epoch: 15
Loss: 0.46619395249333556
ROC train: 0.902123	val: 0.757509	test: 0.730134
PRC train: 0.855004	val: 0.946723	test: 0.715675

Epoch: 16
Loss: 0.4559434183516925
ROC train: 0.913753	val: 0.756410	test: 0.741610
PRC train: 0.868717	val: 0.946254	test: 0.711583

Epoch: 17
Loss: 0.4638067184414353
ROC train: 0.916983	val: 0.749451	test: 0.747870
PRC train: 0.872189	val: 0.945869	test: 0.722397

Epoch: 18
Loss: 0.43499594389214646
ROC train: 0.927551	val: 0.741392	test: 0.740741
PRC train: 0.890733	val: 0.945596	test: 0.723517

Epoch: 19
Loss: 0.4292896795698996
ROC train: 0.936416	val: 0.760073	test: 0.730308
PRC train: 0.905182	val: 0.949815	test: 0.707034

Epoch: 20
Loss: 0.4318442483706163
ROC train: 0.938584	val: 0.758608	test: 0.740567
PRC train: 0.909805	val: 0.948699	test: 0.711348

Epoch: 21
Loss: 0.39523483081103616
ROC train: 0.948396	val: 0.747253	test: 0.743001
PRC train: 0.926483	val: 0.946325	test: 0.720871

Epoch: 22
Loss: 0.39172490376581326
ROC train: 0.955325	val: 0.735897	test: 0.749783
PRC train: 0.935008	val: 0.942950	test: 0.723214

Epoch: 23
Loss: 0.38343525266332457
ROC train: 0.956784	val: 0.708059	test: 0.757607
PRC train: 0.937971	val: 0.937413	test: 0.727596

Epoch: 24
Loss: 0.39009161796242786
ROC train: 0.965836	val: 0.728938	test: 0.753782
PRC train: 0.951467	val: 0.943267	test: 0.732337

Epoch: 25
Loss: 0.3588789713720887
ROC train: 0.968516	val: 0.749817	test: 0.740741
PRC train: 0.953125	val: 0.950053	test: 0.728140

Epoch: 26
Loss: 0.33813447696436727
ROC train: 0.971689	val: 0.752015	test: 0.745262
PRC train: 0.957537	val: 0.951739	test: 0.721253

Epoch: 27
Loss: 0.35652051010225744
ROC train: 0.969840	val: 0.711355	test: 0.774126
PRC train: 0.954209	val: 0.941432	test: 0.741037

Epoch: 28
Loss: 0.34264447394861286
ROC train: 0.972052	val: 0.702198	test: 0.765780
PRC train: 0.958633	val: 0.936946	test: 0.732883

Epoch: 29
Loss: 0.32008107291575427
ROC train: 0.979121	val: 0.752015	test: 0.731873
PRC train: 0.970021	val: 0.950144	test: 0.716614

Epoch: 30
Loss: 0.29616848855735717
ROC train: 0.980254	val: 0.750916	test: 0.698835
PRC train: 0.971525	val: 0.951264	test: 0.700390

Epoch: 31
Loss: 0.3116399750514286
ROC train: 0.982041	val: 0.774725	test: 0.727352
PRC train: 0.975144	val: 0.957515	test: 0.705584

Epoch: 32
Loss: 0.3010883011299421
ROC train: 0.986110	val: 0.733700	test: 0.742653Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/bace/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: bace
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/static-noise/GraphMVP/bace/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/bace/noise=0.05/bace_scaff_6_26-05_11-16-36  ]
[ Using Seed :  6  ]
[ Using device :  cuda:1  ]
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6800054156252163
ROC train: 0.689338	val: 0.575092	test: 0.685968
PRC train: 0.569164	val: 0.899476	test: 0.666962

Epoch: 2
Loss: 0.6367889902559991
ROC train: 0.728253	val: 0.571062	test: 0.732916
PRC train: 0.602615	val: 0.905168	test: 0.713096

Epoch: 3
Loss: 0.6003033450425124
ROC train: 0.775990	val: 0.608059	test: 0.766997
PRC train: 0.651081	val: 0.906671	test: 0.741234

Epoch: 4
Loss: 0.5667614155027334
ROC train: 0.812337	val: 0.622711	test: 0.783168
PRC train: 0.697700	val: 0.907597	test: 0.750971

Epoch: 5
Loss: 0.5392183796247417
ROC train: 0.839424	val: 0.622344	test: 0.798818
PRC train: 0.728914	val: 0.909416	test: 0.751064

Epoch: 6
Loss: 0.5090610334954007
ROC train: 0.855905	val: 0.620513	test: 0.799513
PRC train: 0.759180	val: 0.908813	test: 0.764450

Epoch: 7
Loss: 0.49767588179685784
ROC train: 0.866475	val: 0.631868	test: 0.797600
PRC train: 0.789566	val: 0.914767	test: 0.754588

Epoch: 8
Loss: 0.4767018408186924
ROC train: 0.880130	val: 0.602564	test: 0.811511
PRC train: 0.808998	val: 0.909440	test: 0.781149

Epoch: 9
Loss: 0.4697379502000446
ROC train: 0.894284	val: 0.634432	test: 0.810120
PRC train: 0.830613	val: 0.922301	test: 0.776342

Epoch: 10
Loss: 0.445694690178407
ROC train: 0.900174	val: 0.652381	test: 0.798122
PRC train: 0.840760	val: 0.926661	test: 0.783674

Epoch: 11
Loss: 0.43996226368797187
ROC train: 0.906256	val: 0.625275	test: 0.797948
PRC train: 0.850193	val: 0.920056	test: 0.784962

Epoch: 12
Loss: 0.4295427952014886
ROC train: 0.915551	val: 0.626740	test: 0.801252
PRC train: 0.865354	val: 0.921177	test: 0.793726

Epoch: 13
Loss: 0.4230355647667194
ROC train: 0.921084	val: 0.633333	test: 0.827508
PRC train: 0.873143	val: 0.922938	test: 0.816414

Epoch: 14
Loss: 0.4060372469130952
ROC train: 0.927737	val: 0.643223	test: 0.823857
PRC train: 0.884140	val: 0.925615	test: 0.806937

Epoch: 15
Loss: 0.4052815427008622
ROC train: 0.928856	val: 0.673626	test: 0.808381
PRC train: 0.890016	val: 0.931345	test: 0.783634

Epoch: 16
Loss: 0.38641684174301255
ROC train: 0.933031	val: 0.669231	test: 0.820553
PRC train: 0.893998	val: 0.932044	test: 0.801541

Epoch: 17
Loss: 0.3750169147785994
ROC train: 0.940708	val: 0.655311	test: 0.828204
PRC train: 0.904615	val: 0.927282	test: 0.812075

Epoch: 18
Loss: 0.37261782668146404
ROC train: 0.944201	val: 0.655311	test: 0.816901
PRC train: 0.910423	val: 0.928246	test: 0.813591

Epoch: 19
Loss: 0.37199495280685957
ROC train: 0.946661	val: 0.662271	test: 0.813250
PRC train: 0.916099	val: 0.929203	test: 0.811091

Epoch: 20
Loss: 0.37059528675793746
ROC train: 0.950374	val: 0.660073	test: 0.813598
PRC train: 0.919376	val: 0.927972	test: 0.812686

Epoch: 21
Loss: 0.3529124270856896
ROC train: 0.953148	val: 0.638828	test: 0.815858
PRC train: 0.929804	val: 0.919548	test: 0.790914

Epoch: 22
Loss: 0.35676643249438705
ROC train: 0.953696	val: 0.658608	test: 0.832029
PRC train: 0.929754	val: 0.929134	test: 0.832178

Epoch: 23
Loss: 0.34410723207568794
ROC train: 0.959957	val: 0.647985	test: 0.827508
PRC train: 0.940484	val: 0.920688	test: 0.817060

Epoch: 24
Loss: 0.336610135071097
ROC train: 0.956290	val: 0.626007	test: 0.821422
PRC train: 0.931674	val: 0.919437	test: 0.817758

Epoch: 25
Loss: 0.3346730206972749
ROC train: 0.960525	val: 0.689011	test: 0.827682
PRC train: 0.938887	val: 0.932985	test: 0.822665

Epoch: 26
Loss: 0.32045777882696225
ROC train: 0.966764	val: 0.630769	test: 0.832899
PRC train: 0.949949	val: 0.919947	test: 0.826014

Epoch: 27
Loss: 0.3238890998097507
ROC train: 0.967420	val: 0.631136	test: 0.834637
PRC train: 0.949525	val: 0.919445	test: 0.827558

Epoch: 28
Loss: 0.3102418845379972
ROC train: 0.968796	val: 0.660440	test: 0.826117
PRC train: 0.952840	val: 0.924796	test: 0.822286

Epoch: 29
Loss: 0.3202492023553286
ROC train: 0.969298	val: 0.670696	test: 0.826639
PRC train: 0.953983	val: 0.927567	test: 0.825790

Epoch: 30
Loss: 0.299594203930514
ROC train: 0.974578	val: 0.656777	test: 0.819857
PRC train: 0.961866	val: 0.924906	test: 0.817028

Epoch: 31
Loss: 0.3057910011837879
ROC train: 0.970011	val: 0.680586	test: 0.817771
PRC train: 0.953784	val: 0.932730	test: 0.811539

Epoch: 32
Loss: 0.2941360175013596
ROC train: 0.976016	val: 0.670696	test: 0.824204Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/bace/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: bace
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/static-noise/GraphMVP/bace/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/bace/noise=0.1/bace_scaff_6_26-05_11-16-36  ]
[ Using Seed :  6  ]
[ Using device :  cuda:2  ]
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6830123472687941
ROC train: 0.642837	val: 0.584615	test: 0.669623
PRC train: 0.532808	val: 0.899759	test: 0.655480

Epoch: 2
Loss: 0.6470555524365427
ROC train: 0.708470	val: 0.579121	test: 0.698661
PRC train: 0.594826	val: 0.908493	test: 0.688909

Epoch: 3
Loss: 0.6102624661154196
ROC train: 0.749886	val: 0.583150	test: 0.706138
PRC train: 0.639981	val: 0.910536	test: 0.694164

Epoch: 4
Loss: 0.5874211493197464
ROC train: 0.778793	val: 0.589011	test: 0.708225
PRC train: 0.673860	val: 0.910175	test: 0.693581

Epoch: 5
Loss: 0.5570236293809008
ROC train: 0.805174	val: 0.638828	test: 0.735350
PRC train: 0.707427	val: 0.917141	test: 0.706441

Epoch: 6
Loss: 0.5312742218412378
ROC train: 0.827748	val: 0.652747	test: 0.742306
PRC train: 0.738279	val: 0.919765	test: 0.710309

Epoch: 7
Loss: 0.5227468627796411
ROC train: 0.850517	val: 0.675092	test: 0.747696
PRC train: 0.772644	val: 0.925093	test: 0.715750

Epoch: 8
Loss: 0.509426183794196
ROC train: 0.871458	val: 0.661538	test: 0.748913
PRC train: 0.803449	val: 0.921440	test: 0.723341

Epoch: 9
Loss: 0.4915698708848166
ROC train: 0.882260	val: 0.669963	test: 0.748739
PRC train: 0.819456	val: 0.923337	test: 0.721448

Epoch: 10
Loss: 0.4770267465891429
ROC train: 0.880671	val: 0.659341	test: 0.734655
PRC train: 0.816669	val: 0.921973	test: 0.704028

Epoch: 11
Loss: 0.4740012153530978
ROC train: 0.899372	val: 0.633700	test: 0.750478
PRC train: 0.843795	val: 0.919749	test: 0.733726

Epoch: 12
Loss: 0.45288983971298186
ROC train: 0.906915	val: 0.642125	test: 0.758651
PRC train: 0.856835	val: 0.920748	test: 0.740531

Epoch: 13
Loss: 0.43322795261682356
ROC train: 0.914332	val: 0.661538	test: 0.763867
PRC train: 0.872664	val: 0.923848	test: 0.735825

Epoch: 14
Loss: 0.41755031265060805
ROC train: 0.921159	val: 0.660073	test: 0.770822
PRC train: 0.883732	val: 0.924557	test: 0.738806

Epoch: 15
Loss: 0.4269828980116868
ROC train: 0.923830	val: 0.672894	test: 0.764737
PRC train: 0.886932	val: 0.931549	test: 0.725539

Epoch: 16
Loss: 0.41916559331012604
ROC train: 0.930611	val: 0.691575	test: 0.753608
PRC train: 0.890687	val: 0.936941	test: 0.726939

Epoch: 17
Loss: 0.40110059172384593
ROC train: 0.941507	val: 0.682051	test: 0.770127
PRC train: 0.907183	val: 0.936179	test: 0.753042

Epoch: 18
Loss: 0.395267345853488
ROC train: 0.946490	val: 0.674359	test: 0.778473
PRC train: 0.915364	val: 0.935128	test: 0.758215

Epoch: 19
Loss: 0.38980609205257205
ROC train: 0.945051	val: 0.687179	test: 0.776908
PRC train: 0.913804	val: 0.937302	test: 0.758359

Epoch: 20
Loss: 0.3671105278032073
ROC train: 0.950679	val: 0.671062	test: 0.775517
PRC train: 0.925605	val: 0.932670	test: 0.769847

Epoch: 21
Loss: 0.36661986753076625
ROC train: 0.961387	val: 0.706960	test: 0.764215
PRC train: 0.942475	val: 0.942232	test: 0.747645

Epoch: 22
Loss: 0.36189655252524655
ROC train: 0.966201	val: 0.688278	test: 0.788732
PRC train: 0.950231	val: 0.939079	test: 0.774963

Epoch: 23
Loss: 0.3482957008448788
ROC train: 0.963701	val: 0.672527	test: 0.797600
PRC train: 0.947399	val: 0.934554	test: 0.782324

Epoch: 24
Loss: 0.35228845851749063
ROC train: 0.968550	val: 0.706960	test: 0.801252
PRC train: 0.953334	val: 0.944154	test: 0.778411

Epoch: 25
Loss: 0.33759989372316007
ROC train: 0.974755	val: 0.698535	test: 0.792210
PRC train: 0.962266	val: 0.942076	test: 0.769164

Epoch: 26
Loss: 0.3371342216191053
ROC train: 0.972785	val: 0.671429	test: 0.797079
PRC train: 0.959708	val: 0.937173	test: 0.781665

Epoch: 27
Loss: 0.31834818246042246
ROC train: 0.978268	val: 0.685714	test: 0.792210
PRC train: 0.967312	val: 0.940193	test: 0.780472

Epoch: 28
Loss: 0.30590097536622507
ROC train: 0.979287	val: 0.658242	test: 0.796905
PRC train: 0.969039	val: 0.931978	test: 0.782714

Epoch: 29
Loss: 0.2996266806116748
ROC train: 0.977389	val: 0.681685	test: 0.764389
PRC train: 0.967394	val: 0.936817	test: 0.756039

Epoch: 30
Loss: 0.28038737360895843
ROC train: 0.980856	val: 0.660073	test: 0.781255
PRC train: 0.973030	val: 0.932883	test: 0.768989

Epoch: 31
Loss: 0.2923739090390004
ROC train: 0.986881	val: 0.675092	test: 0.797600
PRC train: 0.980557	val: 0.936302	test: 0.780815

Epoch: 32
Loss: 0.2646109913156892
ROC train: 0.981387	val: 0.693040	test: 0.796209Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/bace/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: bace
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/static-noise/GraphMVP/bace/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/bace/noise=0.1/bace_scaff_4_26-05_11-16-36  ]
[ Using Seed :  4  ]
[ Using device :  cuda:2  ]
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6762301784562458
ROC train: 0.665751	val: 0.523077	test: 0.617110
PRC train: 0.544730	val: 0.888491	test: 0.630128

Epoch: 2
Loss: 0.6406207124371924
ROC train: 0.707588	val: 0.551648	test: 0.675187
PRC train: 0.585481	val: 0.899329	test: 0.685308

Epoch: 3
Loss: 0.6109376973062701
ROC train: 0.738721	val: 0.547985	test: 0.690315
PRC train: 0.612178	val: 0.894849	test: 0.700670

Epoch: 4
Loss: 0.5952466852421987
ROC train: 0.773242	val: 0.543956	test: 0.711355
PRC train: 0.650444	val: 0.884235	test: 0.710762

Epoch: 5
Loss: 0.5584267142185343
ROC train: 0.797697	val: 0.553846	test: 0.707355
PRC train: 0.682689	val: 0.878693	test: 0.705994

Epoch: 6
Loss: 0.5487040078331484
ROC train: 0.826076	val: 0.580220	test: 0.728221
PRC train: 0.717347	val: 0.884882	test: 0.718296

Epoch: 7
Loss: 0.5275248032060758
ROC train: 0.848753	val: 0.606593	test: 0.759520
PRC train: 0.746622	val: 0.886753	test: 0.717623

Epoch: 8
Loss: 0.519015233614337
ROC train: 0.854538	val: 0.607326	test: 0.752217
PRC train: 0.761778	val: 0.886784	test: 0.730314

Epoch: 9
Loss: 0.5112198588297675
ROC train: 0.870380	val: 0.624908	test: 0.761781
PRC train: 0.789891	val: 0.892904	test: 0.729675

Epoch: 10
Loss: 0.48873857695114714
ROC train: 0.884775	val: 0.640659	test: 0.772561
PRC train: 0.809037	val: 0.905831	test: 0.708788

Epoch: 11
Loss: 0.4474941132032712
ROC train: 0.885328	val: 0.606593	test: 0.750826
PRC train: 0.815954	val: 0.890126	test: 0.734937

Epoch: 12
Loss: 0.4542740042398643
ROC train: 0.903122	val: 0.622711	test: 0.765258
PRC train: 0.841627	val: 0.895773	test: 0.745914

Epoch: 13
Loss: 0.4421014272414185
ROC train: 0.909144	val: 0.653480	test: 0.767345
PRC train: 0.850780	val: 0.915288	test: 0.703238

Epoch: 14
Loss: 0.42824602562266423
ROC train: 0.919506	val: 0.654579	test: 0.779517
PRC train: 0.867478	val: 0.914179	test: 0.737456

Epoch: 15
Loss: 0.43296910462237753
ROC train: 0.917140	val: 0.644689	test: 0.785950
PRC train: 0.870147	val: 0.914195	test: 0.753185

Epoch: 16
Loss: 0.4156973304721988
ROC train: 0.928818	val: 0.682051	test: 0.786472
PRC train: 0.886373	val: 0.920319	test: 0.725252

Epoch: 17
Loss: 0.4187008828761011
ROC train: 0.924061	val: 0.646154	test: 0.774474
PRC train: 0.881175	val: 0.896879	test: 0.731354

Epoch: 18
Loss: 0.3985425957537941
ROC train: 0.936615	val: 0.666300	test: 0.771344
PRC train: 0.900437	val: 0.917419	test: 0.717281

Epoch: 19
Loss: 0.41289396699008013
ROC train: 0.942494	val: 0.656410	test: 0.780386
PRC train: 0.905975	val: 0.912234	test: 0.736218

Epoch: 20
Loss: 0.3772275118819858
ROC train: 0.952240	val: 0.669963	test: 0.789428
PRC train: 0.924879	val: 0.917315	test: 0.754958

Epoch: 21
Loss: 0.36696891741899784
ROC train: 0.954735	val: 0.671429	test: 0.789428
PRC train: 0.932448	val: 0.920672	test: 0.753627

Epoch: 22
Loss: 0.3392996688309493
ROC train: 0.955023	val: 0.669597	test: 0.784559
PRC train: 0.931220	val: 0.921906	test: 0.751831

Epoch: 23
Loss: 0.3742045538023461
ROC train: 0.957691	val: 0.676923	test: 0.774648
PRC train: 0.934235	val: 0.924061	test: 0.735228

Epoch: 24
Loss: 0.3503635386285249
ROC train: 0.963893	val: 0.684982	test: 0.773952
PRC train: 0.941825	val: 0.919121	test: 0.735344

Epoch: 25
Loss: 0.35184824669316145
ROC train: 0.965428	val: 0.680220	test: 0.777778
PRC train: 0.941780	val: 0.901481	test: 0.729633

Epoch: 26
Loss: 0.3323735358204643
ROC train: 0.971812	val: 0.677289	test: 0.792384
PRC train: 0.951921	val: 0.915118	test: 0.761844

Epoch: 27
Loss: 0.33515296927401483
ROC train: 0.964329	val: 0.630037	test: 0.776039
PRC train: 0.939531	val: 0.902305	test: 0.783127

Epoch: 28
Loss: 0.32553557904993474
ROC train: 0.976835	val: 0.681319	test: 0.795166
PRC train: 0.962124	val: 0.923100	test: 0.756490

Epoch: 29
Loss: 0.3175596844076495
ROC train: 0.978867	val: 0.678388	test: 0.798644
PRC train: 0.966284	val: 0.923842	test: 0.761014

Epoch: 30
Loss: 0.29200042283085975
ROC train: 0.980645	val: 0.667766	test: 0.789254
PRC train: 0.968864	val: 0.917379	test: 0.751764

Epoch: 31
Loss: 0.30099429963035
ROC train: 0.980197	val: 0.689377	test: 0.770649
PRC train: 0.967183	val: 0.916915	test: 0.732470

Epoch: 32
Loss: 0.28650958174142266
ROC train: 0.982483	val: 0.700000	test: 0.757781Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/bace/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:3
  input_data_dir: 
  dataset: bace
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/static-noise/GraphMVP/bace/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/bace/noise=0.2/bace_scaff_4_26-05_11-16-36  ]
[ Using Seed :  4  ]
[ Using device :  cuda:3  ]
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.682153744803102
ROC train: 0.625400	val: 0.533333	test: 0.620240
PRC train: 0.502677	val: 0.890990	test: 0.608400

Epoch: 2
Loss: 0.6574144240588333
ROC train: 0.657486	val: 0.577656	test: 0.643888
PRC train: 0.531507	val: 0.907875	test: 0.657801

Epoch: 3
Loss: 0.641140057974565
ROC train: 0.679569	val: 0.586813	test: 0.638150
PRC train: 0.557851	val: 0.909953	test: 0.647490

Epoch: 4
Loss: 0.630749973026948
ROC train: 0.714578	val: 0.587179	test: 0.659885
PRC train: 0.594521	val: 0.907503	test: 0.678731

Epoch: 5
Loss: 0.6081135747694757
ROC train: 0.749278	val: 0.564835	test: 0.649278
PRC train: 0.638060	val: 0.891287	test: 0.665321

Epoch: 6
Loss: 0.5972774538450617
ROC train: 0.775397	val: 0.551282	test: 0.641454
PRC train: 0.675667	val: 0.885948	test: 0.657327

Epoch: 7
Loss: 0.582819067656847
ROC train: 0.801224	val: 0.554945	test: 0.682838
PRC train: 0.705314	val: 0.892432	test: 0.666815

Epoch: 8
Loss: 0.5609453519838509
ROC train: 0.821921	val: 0.585714	test: 0.696227
PRC train: 0.736658	val: 0.904123	test: 0.644789

Epoch: 9
Loss: 0.5506956675514627
ROC train: 0.843656	val: 0.568132	test: 0.694314
PRC train: 0.774157	val: 0.895986	test: 0.664819

Epoch: 10
Loss: 0.5310392610668024
ROC train: 0.864070	val: 0.614652	test: 0.698313
PRC train: 0.801063	val: 0.913781	test: 0.657736

Epoch: 11
Loss: 0.5107293360049392
ROC train: 0.882703	val: 0.617949	test: 0.715354
PRC train: 0.828311	val: 0.917135	test: 0.662968

Epoch: 12
Loss: 0.4896636478682607
ROC train: 0.888784	val: 0.611722	test: 0.715528
PRC train: 0.841164	val: 0.913777	test: 0.669833

Epoch: 13
Loss: 0.488720217671314
ROC train: 0.902677	val: 0.613553	test: 0.708920
PRC train: 0.860923	val: 0.913139	test: 0.662136

Epoch: 14
Loss: 0.47143846199320744
ROC train: 0.911084	val: 0.616850	test: 0.715875
PRC train: 0.871787	val: 0.912783	test: 0.667769

Epoch: 15
Loss: 0.44712604775551873
ROC train: 0.925071	val: 0.632234	test: 0.725439
PRC train: 0.890849	val: 0.917410	test: 0.671615

Epoch: 16
Loss: 0.4313083088719762
ROC train: 0.935634	val: 0.651648	test: 0.733090
PRC train: 0.906863	val: 0.926106	test: 0.668817

Epoch: 17
Loss: 0.43872952241190166
ROC train: 0.942925	val: 0.652747	test: 0.734481
PRC train: 0.919741	val: 0.927215	test: 0.699344

Epoch: 18
Loss: 0.4195201718156629
ROC train: 0.950885	val: 0.658974	test: 0.735698
PRC train: 0.931407	val: 0.928947	test: 0.688470

Epoch: 19
Loss: 0.4160479600574513
ROC train: 0.953388	val: 0.650183	test: 0.731351
PRC train: 0.932806	val: 0.928821	test: 0.660197

Epoch: 20
Loss: 0.39595756163661455
ROC train: 0.960967	val: 0.648718	test: 0.733612
PRC train: 0.943179	val: 0.926758	test: 0.665042

Epoch: 21
Loss: 0.3855430883293498
ROC train: 0.964469	val: 0.628571	test: 0.751174
PRC train: 0.948897	val: 0.915690	test: 0.749178

Epoch: 22
Loss: 0.338200851501142
ROC train: 0.968955	val: 0.646520	test: 0.739697
PRC train: 0.956402	val: 0.926048	test: 0.691313

Epoch: 23
Loss: 0.3584690865567103
ROC train: 0.972426	val: 0.664469	test: 0.746479
PRC train: 0.960223	val: 0.930503	test: 0.667022

Epoch: 24
Loss: 0.3493053375969311
ROC train: 0.967942	val: 0.646154	test: 0.748218
PRC train: 0.949687	val: 0.925400	test: 0.686636

Epoch: 25
Loss: 0.33866185090255063
ROC train: 0.981142	val: 0.648718	test: 0.757433
PRC train: 0.972138	val: 0.926735	test: 0.735092

Epoch: 26
Loss: 0.33438682470002756
ROC train: 0.984860	val: 0.636996	test: 0.764041
PRC train: 0.978613	val: 0.923766	test: 0.717655

Epoch: 27
Loss: 0.3202475763741071
ROC train: 0.988094	val: 0.630769	test: 0.767345
PRC train: 0.982724	val: 0.922478	test: 0.732155

Epoch: 28
Loss: 0.30114585000274874
ROC train: 0.989064	val: 0.676923	test: 0.748739
PRC train: 0.984079	val: 0.935565	test: 0.703383

Epoch: 29
Loss: 0.3139257121257138
ROC train: 0.989877	val: 0.686813	test: 0.754999
PRC train: 0.985232	val: 0.936496	test: 0.721305

Epoch: 30
Loss: 0.2852444702939708
ROC train: 0.992340	val: 0.673993	test: 0.762998
PRC train: 0.989305	val: 0.932841	test: 0.710444

Epoch: 31
Loss: 0.2770495540551844
ROC train: 0.992874	val: 0.667766	test: 0.753956
PRC train: 0.989593	val: 0.933378	test: 0.696166

Epoch: 32
Loss: 0.272997418552307
ROC train: 0.992286	val: 0.656044	test: 0.756912Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/bace/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:3
  input_data_dir: 
  dataset: bace
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/static-noise/GraphMVP/bace/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/bace/noise=0.2/bace_scaff_6_26-05_11-16-36  ]
[ Using Seed :  6  ]
[ Using device :  cuda:3  ]
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6849406100755957
ROC train: 0.613282	val: 0.576190	test: 0.682316
PRC train: 0.491300	val: 0.900936	test: 0.655946

Epoch: 2
Loss: 0.6595583256472466
ROC train: 0.661464	val: 0.567766	test: 0.674491
PRC train: 0.535094	val: 0.909500	test: 0.666448

Epoch: 3
Loss: 0.6321672541199397
ROC train: 0.703017	val: 0.568864	test: 0.676404
PRC train: 0.594081	val: 0.905555	test: 0.660266

Epoch: 4
Loss: 0.6177547197458079
ROC train: 0.732911	val: 0.539194	test: 0.663885
PRC train: 0.633267	val: 0.897585	test: 0.646032

Epoch: 5
Loss: 0.5979935866466422
ROC train: 0.764680	val: 0.501465	test: 0.663015
PRC train: 0.671194	val: 0.879810	test: 0.648273

Epoch: 6
Loss: 0.5756703041212337
ROC train: 0.781356	val: 0.488645	test: 0.644931
PRC train: 0.694460	val: 0.871451	test: 0.646798

Epoch: 7
Loss: 0.5607933298246246
ROC train: 0.802206	val: 0.514652	test: 0.665797
PRC train: 0.723252	val: 0.879214	test: 0.657856

Epoch: 8
Loss: 0.5582956419514777
ROC train: 0.818884	val: 0.513919	test: 0.667884
PRC train: 0.744671	val: 0.874346	test: 0.666040

Epoch: 9
Loss: 0.5243370777251548
ROC train: 0.841518	val: 0.527839	test: 0.697966
PRC train: 0.769492	val: 0.881246	test: 0.682260

Epoch: 10
Loss: 0.5247529513642317
ROC train: 0.853148	val: 0.545421	test: 0.699009
PRC train: 0.787161	val: 0.886666	test: 0.683311

Epoch: 11
Loss: 0.5103330578283858
ROC train: 0.870137	val: 0.582418	test: 0.714137
PRC train: 0.812218	val: 0.899132	test: 0.688522

Epoch: 12
Loss: 0.4980764185978838
ROC train: 0.876672	val: 0.595971	test: 0.714484
PRC train: 0.821121	val: 0.896209	test: 0.690243

Epoch: 13
Loss: 0.48141412768980707
ROC train: 0.898833	val: 0.655311	test: 0.724570
PRC train: 0.846902	val: 0.913506	test: 0.687153

Epoch: 14
Loss: 0.46590854020637557
ROC train: 0.907275	val: 0.651648	test: 0.749783
PRC train: 0.858861	val: 0.909924	test: 0.705564

Epoch: 15
Loss: 0.46977681024482454
ROC train: 0.913850	val: 0.678022	test: 0.741436
PRC train: 0.867975	val: 0.929764	test: 0.690096

Epoch: 16
Loss: 0.45491732900878096
ROC train: 0.922089	val: 0.661172	test: 0.738132
PRC train: 0.882024	val: 0.926304	test: 0.700699

Epoch: 17
Loss: 0.43420812499008843
ROC train: 0.932723	val: 0.642857	test: 0.743697
PRC train: 0.896721	val: 0.916167	test: 0.713781

Epoch: 18
Loss: 0.42601818371017686
ROC train: 0.940699	val: 0.684982	test: 0.749609
PRC train: 0.908165	val: 0.933395	test: 0.710670

Epoch: 19
Loss: 0.41786145654115875
ROC train: 0.942611	val: 0.684982	test: 0.746479
PRC train: 0.910031	val: 0.934489	test: 0.692837

Epoch: 20
Loss: 0.39865339316893583
ROC train: 0.951738	val: 0.679487	test: 0.755695
PRC train: 0.925322	val: 0.930010	test: 0.710907

Epoch: 21
Loss: 0.3962340884452707
ROC train: 0.954726	val: 0.641026	test: 0.748392
PRC train: 0.932377	val: 0.924287	test: 0.715859

Epoch: 22
Loss: 0.3737947383659628
ROC train: 0.962297	val: 0.647985	test: 0.747174
PRC train: 0.941445	val: 0.928236	test: 0.728695

Epoch: 23
Loss: 0.37278251732023426
ROC train: 0.968856	val: 0.685714	test: 0.751521
PRC train: 0.950112	val: 0.935187	test: 0.710968

Epoch: 24
Loss: 0.3478218224481132
ROC train: 0.966761	val: 0.670696	test: 0.762824
PRC train: 0.946481	val: 0.932923	test: 0.726635

Epoch: 25
Loss: 0.33148384193639535
ROC train: 0.973713	val: 0.645788	test: 0.725961
PRC train: 0.955950	val: 0.924714	test: 0.674606

Epoch: 26
Loss: 0.3585648850971347
ROC train: 0.982489	val: 0.679853	test: 0.720223
PRC train: 0.969585	val: 0.930161	test: 0.727071

Epoch: 27
Loss: 0.3246885940967341
ROC train: 0.983864	val: 0.667766	test: 0.738654
PRC train: 0.972228	val: 0.926914	test: 0.745368

Epoch: 28
Loss: 0.32404679987342044
ROC train: 0.981247	val: 0.642491	test: 0.730829
PRC train: 0.969753	val: 0.923354	test: 0.681454

Epoch: 29
Loss: 0.31320129837710975
ROC train: 0.981561	val: 0.717582	test: 0.755521
PRC train: 0.969365	val: 0.944015	test: 0.749109

Epoch: 30
Loss: 0.305244296729716
ROC train: 0.987263	val: 0.718315	test: 0.753782
PRC train: 0.979569	val: 0.941673	test: 0.736022

Epoch: 31
Loss: 0.297852354180156
ROC train: 0.993556	val: 0.719780	test: 0.770127
PRC train: 0.989750	val: 0.942727	test: 0.760064

Epoch: 32
Loss: 0.27887795743388144
ROC train: 0.994600	val: 0.708059	test: 0.760563
PRC train: 0.913721	val: 0.925208	test: 0.807783

Epoch: 34
Loss: 0.34322697444095807
ROC train: 0.946484	val: 0.662271	test: 0.793253
PRC train: 0.915000	val: 0.925807	test: 0.811669

Epoch: 35
Loss: 0.3350504123499238
ROC train: 0.946878	val: 0.656044	test: 0.797948
PRC train: 0.915749	val: 0.922910	test: 0.810510

Epoch: 36
Loss: 0.33493694267446944
ROC train: 0.947754	val: 0.640659	test: 0.793949
PRC train: 0.917627	val: 0.918733	test: 0.811475

Epoch: 37
Loss: 0.32665886185007814
ROC train: 0.949130	val: 0.653114	test: 0.794644
PRC train: 0.919851	val: 0.921076	test: 0.806364

Epoch: 38
Loss: 0.3240194720455962
ROC train: 0.948770	val: 0.679487	test: 0.797079
PRC train: 0.918362	val: 0.927097	test: 0.805292

Epoch: 39
Loss: 0.32109710097578686
ROC train: 0.945811	val: 0.682418	test: 0.796209
PRC train: 0.913136	val: 0.928875	test: 0.798192

Epoch: 40
Loss: 0.32511671017289406
ROC train: 0.952737	val: 0.674725	test: 0.798644
PRC train: 0.923901	val: 0.927732	test: 0.811485

Epoch: 41
Loss: 0.3217478482096506
ROC train: 0.955026	val: 0.669231	test: 0.782820
PRC train: 0.928489	val: 0.922601	test: 0.804663

Epoch: 42
Loss: 0.31243331683237174
ROC train: 0.957263	val: 0.654579	test: 0.766997
PRC train: 0.932806	val: 0.922090	test: 0.793014

Epoch: 43
Loss: 0.31297384618206275
ROC train: 0.957977	val: 0.658608	test: 0.796557
PRC train: 0.932703	val: 0.921937	test: 0.811433

Epoch: 44
Loss: 0.326194799365521
ROC train: 0.957329	val: 0.670696	test: 0.811337
PRC train: 0.931576	val: 0.929569	test: 0.826404

Epoch: 45
Loss: 0.3200897266115343
ROC train: 0.954923	val: 0.676190	test: 0.807860
PRC train: 0.926750	val: 0.929462	test: 0.834332

Epoch: 46
Loss: 0.3137046617581657
ROC train: 0.955819	val: 0.676190	test: 0.783690
PRC train: 0.929564	val: 0.926968	test: 0.814797

Epoch: 47
Loss: 0.3286950841324957
ROC train: 0.956102	val: 0.669597	test: 0.771692
PRC train: 0.928922	val: 0.921251	test: 0.792396

Epoch: 48
Loss: 0.3171884053378198
ROC train: 0.960400	val: 0.641758	test: 0.793601
PRC train: 0.936541	val: 0.913789	test: 0.815597

Epoch: 49
Loss: 0.3115709421950863
ROC train: 0.959244	val: 0.645421	test: 0.797427
PRC train: 0.936552	val: 0.919616	test: 0.805346

Epoch: 50
Loss: 0.30520252139375686
ROC train: 0.959384	val: 0.656410	test: 0.789950
PRC train: 0.935778	val: 0.919231	test: 0.794446

Epoch: 51
Loss: 0.30678035012240273
ROC train: 0.963213	val: 0.642857	test: 0.788732
PRC train: 0.941747	val: 0.916697	test: 0.792921

Epoch: 52
Loss: 0.29918941556702466
ROC train: 0.962617	val: 0.641392	test: 0.796557
PRC train: 0.941364	val: 0.916198	test: 0.813367

Epoch: 53
Loss: 0.30947159669028224
ROC train: 0.960462	val: 0.635897	test: 0.785255
PRC train: 0.939019	val: 0.917874	test: 0.807061

Epoch: 54
Loss: 0.31612403562003566
ROC train: 0.962343	val: 0.641392	test: 0.789602
PRC train: 0.940883	val: 0.923373	test: 0.802807

Epoch: 55
Loss: 0.2839214986933213
ROC train: 0.959563	val: 0.656777	test: 0.775691
PRC train: 0.937290	val: 0.922362	test: 0.782080

Epoch: 56
Loss: 0.2933278603374957
ROC train: 0.961824	val: 0.617582	test: 0.771344
PRC train: 0.940774	val: 0.913754	test: 0.778086

Epoch: 57
Loss: 0.2922010312870481
ROC train: 0.963873	val: 0.663736	test: 0.784211
PRC train: 0.943851	val: 0.924216	test: 0.787139

Epoch: 58
Loss: 0.27793541887244155
ROC train: 0.964284	val: 0.671429	test: 0.772387
PRC train: 0.944732	val: 0.922613	test: 0.778530

Epoch: 59
Loss: 0.29085020600227496
ROC train: 0.966182	val: 0.653114	test: 0.768910
PRC train: 0.945777	val: 0.920491	test: 0.776835

Epoch: 60
Loss: 0.2798214135124071
ROC train: 0.966022	val: 0.643223	test: 0.774126
PRC train: 0.945916	val: 0.919719	test: 0.782153

Epoch: 61
Loss: 0.2837634343413358
ROC train: 0.968436	val: 0.624908	test: 0.774126
PRC train: 0.951708	val: 0.910449	test: 0.776834

Epoch: 62
Loss: 0.2716737996338821
ROC train: 0.967600	val: 0.610989	test: 0.766997
PRC train: 0.951505	val: 0.903701	test: 0.768984

Epoch: 63
Loss: 0.2825415754562414
ROC train: 0.961144	val: 0.628205	test: 0.770301
PRC train: 0.942166	val: 0.910623	test: 0.762552

Epoch: 64
Loss: 0.2813818495711501
ROC train: 0.967235	val: 0.651648	test: 0.778299
PRC train: 0.948513	val: 0.923529	test: 0.784706

Epoch: 65
Loss: 0.27948333214771776
ROC train: 0.966941	val: 0.658974	test: 0.769431
PRC train: 0.948113	val: 0.925940	test: 0.780526

Epoch: 66
Loss: 0.27335955205008894
ROC train: 0.966210	val: 0.654212	test: 0.762128
PRC train: 0.946071	val: 0.922652	test: 0.777140

Epoch: 67
Loss: 0.2558643483264263
ROC train: 0.969600	val: 0.650916	test: 0.772387
PRC train: 0.951537	val: 0.919633	test: 0.782566

Epoch: 68
Loss: 0.2566270553315594
ROC train: 0.971450	val: 0.636630	test: 0.769084
PRC train: 0.954368	val: 0.916714	test: 0.787469

Epoch: 69
Loss: 0.2676911408873178
ROC train: 0.972286	val: 0.632601	test: 0.767345
PRC train: 0.958003	val: 0.923007	test: 0.773005

Epoch: 70
Loss: 0.2644586460493453
ROC train: 0.971310	val: 0.671795	test: 0.759346
PRC train: 0.955098	val: 0.927267	test: 0.764885

Epoch: 71
Loss: 0.27624819034404585
ROC train: 0.971918	val: 0.647253	test: 0.758129
PRC train: 0.956596	val: 0.920428	test: 0.768868

Epoch: 72
Loss: 0.2574594649980268
ROC train: 0.968984	val: 0.661172	test: 0.778126
PRC train: 0.951552	val: 0.921522	test: 0.791692

Epoch: 73
Loss: 0.26577288814062244
ROC train: 0.972155	val: 0.648718	test: 0.772387
PRC train: 0.955640	val: 0.924112	test: 0.782554

Epoch: 74
Loss: 0.2722518542645333
ROC train: 0.973562	val: 0.643223	test: 0.754825
PRC train: 0.958984	val: 0.920741	test: 0.767858

Epoch: 75
Loss: 0.270016001842921
ROC train: 0.974304	val: 0.646520	test: 0.758825
PRC train: 0.960458	val: 0.920617	test: 0.767485

Epoch: 76
Loss: 0.25598517937155785
ROC train: 0.972326	val: 0.655311	test: 0.748739
PRC train: 0.956146	val: 0.919565	test: 0.765613

Epoch: 77
Loss: 0.26662863285046406
ROC train: 0.971179	val: 0.666300	test: 0.749783
PRC train: 0.954390	val: 0.921105	test: 0.763375

Epoch: 78
Loss: 0.265330868086576
ROC train: 0.973376	val: 0.630403	test: 0.752565
PRC train: 0.958542	val: 0.912040	test: 0.757334

Epoch: 79
Loss: 0.23962477234616322
ROC train: 0.973165	val: 0.606960	test: 0.767345
PRC train: 0.959554	val: 0.906449	test: 0.770871

Epoch: 80
Loss: 0.25901492426792605
ROC train: 0.975171	val: 0.621978	test: 0.767171
PRC train: 0.963381	val: 0.913350	test: 0.772371

Epoch: 81
Loss: 0.24429265193622962
ROC train: 0.975508	val: 0.655678	test: 0.757086
PRC train: 0.964005	val: 0.925003	test: 0.755698

Epoch: 82
Loss: 0.24127042018480957
ROC train: 0.974064	val: 0.679853	test: 0.745436
PRC train: 0.960921	val: 0.927640	test: 0.748897

Epoch: 83
Loss: 0.26042906141317484
ROC train: 0.978271	val: 0.658608	test: 0.749783
PRC train: 0.966641	val: 0.925569	test: 0.755699

Epoch: 84
Loss: 0.2264673053118611
ROC train: 0.977320	val: 0.647619	test: 0.758651
PRC train: 0.964602	val: 0.924063	test: 0.764755

Epoch: 85
Loss: 0.2521888356718775
ROC train: 0.977922	val: 0.634432	test: 0.767693
PRC train: 0.966746	val: 0.922852	test: 0.784831

Epoch: 86
Loss: 0.24923128106570855
ROC train: 0.979235	val: 0.649451	test: 0.760216
PRC train: 0.969424	val: 0.925085	test: 0.773908

Epoch: 87
Loss: 0.2354802506289151
ROC train: 0.980380	val: 0.647619	test: 0.743523
PRC train: 0.970877	val: 0.921149	test: 0.755457

Epoch: 88
Loss: 0.2513074164032669
ROC train: 0.980936	val: 0.630403	test: 0.745957
PRC train: 0.971740	val: 0.916898	test: 0.754356

Epoch: 89
Loss: 0.24231569994568894
ROC train: 0.981918	val: 0.641758	test: 0.752043
PRC train: 0.972796	val: 0.920813	test: 0.766608

Epoch: 90
Loss: 0.24842911870981893
ROC train: 0.978930	val: 0.663370	test: 0.750130
PRC train: 0.967568	val: 0.927380	test: 0.774835

Epoch: 91
Loss: 0.24062965271418163
ROC train: 0.979272	val: 0.643590	test: 0.741436
PRC train: 0.968577	val: 0.921341	test: 0.766504

Epoch: 92
Loss: 0.2484212651965457
ROC train: 0.978410	val: 0.636264	test: 0.750826
PRC train: 0.967715	val: 0.919129	test: 0.768386

Epoch: 93
Loss: 0.230210222985496
ROC train: 0.981144	val: 0.653114	test: 0.748392
PRC train: 0.971528	val: 0.926553	test: 0.766302

Epoch: 94
Loss: 0.24513102965573275
ROC train: 0.979301	val: 0.653846	test: 0.732394
PRC train: 0.914209	val: 0.928820	test: 0.789178

Epoch: 34
Loss: 0.3472426899925482
ROC train: 0.948893	val: 0.676190	test: 0.789254
PRC train: 0.915655	val: 0.924919	test: 0.808944

Epoch: 35
Loss: 0.3366695193409945
ROC train: 0.946259	val: 0.667766	test: 0.799513
PRC train: 0.912993	val: 0.925089	test: 0.823695

Epoch: 36
Loss: 0.325614962428092
ROC train: 0.945756	val: 0.684615	test: 0.802817
PRC train: 0.913796	val: 0.928746	test: 0.830495

Epoch: 37
Loss: 0.33503896428947055
ROC train: 0.951396	val: 0.690842	test: 0.805947
PRC train: 0.920960	val: 0.928577	test: 0.826468

Epoch: 38
Loss: 0.3326740087758718
ROC train: 0.953176	val: 0.688645	test: 0.813772
PRC train: 0.923159	val: 0.929093	test: 0.833883

Epoch: 39
Loss: 0.31429816184909415
ROC train: 0.950910	val: 0.681685	test: 0.799339
PRC train: 0.919313	val: 0.930704	test: 0.818159

Epoch: 40
Loss: 0.33291161808962744
ROC train: 0.949298	val: 0.672894	test: 0.799165
PRC train: 0.916624	val: 0.930995	test: 0.815734

Epoch: 41
Loss: 0.3292822298652335
ROC train: 0.952620	val: 0.679853	test: 0.794471
PRC train: 0.923730	val: 0.928376	test: 0.808785

Epoch: 42
Loss: 0.32519428218709795
ROC train: 0.951764	val: 0.681319	test: 0.812380
PRC train: 0.923583	val: 0.927326	test: 0.832323

Epoch: 43
Loss: 0.3182703663837182
ROC train: 0.954315	val: 0.675824	test: 0.825248
PRC train: 0.926583	val: 0.925851	test: 0.845217

Epoch: 44
Loss: 0.3103466636601185
ROC train: 0.956667	val: 0.670696	test: 0.815336
PRC train: 0.930023	val: 0.926745	test: 0.839191

Epoch: 45
Loss: 0.2980545602171981
ROC train: 0.957671	val: 0.670696	test: 0.801947
PRC train: 0.932301	val: 0.926735	test: 0.825083

Epoch: 46
Loss: 0.30682110683980773
ROC train: 0.958333	val: 0.679487	test: 0.793253
PRC train: 0.933031	val: 0.928927	test: 0.817037

Epoch: 47
Loss: 0.3128079335433044
ROC train: 0.955913	val: 0.675092	test: 0.788906
PRC train: 0.929497	val: 0.927535	test: 0.809292

Epoch: 48
Loss: 0.30883348286631235
ROC train: 0.959669	val: 0.669963	test: 0.801252
PRC train: 0.936093	val: 0.926740	test: 0.808042

Epoch: 49
Loss: 0.31061883550774216
ROC train: 0.959575	val: 0.651282	test: 0.788559
PRC train: 0.937023	val: 0.923674	test: 0.794677

Epoch: 50
Loss: 0.3115491872167114
ROC train: 0.958273	val: 0.678022	test: 0.780560
PRC train: 0.935329	val: 0.926261	test: 0.787611

Epoch: 51
Loss: 0.30072465862507963
ROC train: 0.959874	val: 0.651282	test: 0.780038
PRC train: 0.937128	val: 0.924398	test: 0.780230

Epoch: 52
Loss: 0.30948217922985843
ROC train: 0.961572	val: 0.667766	test: 0.794818
PRC train: 0.937512	val: 0.928884	test: 0.799952

Epoch: 53
Loss: 0.3101012583306882
ROC train: 0.962072	val: 0.683883	test: 0.804556
PRC train: 0.938505	val: 0.930355	test: 0.814299

Epoch: 54
Loss: 0.29248812099431876
ROC train: 0.961553	val: 0.690842	test: 0.809251
PRC train: 0.938665	val: 0.929461	test: 0.817546

Epoch: 55
Loss: 0.2915872532515899
ROC train: 0.960277	val: 0.683516	test: 0.807338
PRC train: 0.937704	val: 0.928393	test: 0.819895

Epoch: 56
Loss: 0.3161518306265939
ROC train: 0.964121	val: 0.680952	test: 0.796731
PRC train: 0.942018	val: 0.928040	test: 0.814665

Epoch: 57
Loss: 0.2879570425772162
ROC train: 0.963102	val: 0.648352	test: 0.791688
PRC train: 0.939603	val: 0.924172	test: 0.798648

Epoch: 58
Loss: 0.2991584910687407
ROC train: 0.962083	val: 0.658608	test: 0.789080
PRC train: 0.940236	val: 0.928962	test: 0.790404

Epoch: 59
Loss: 0.28140138489503397
ROC train: 0.962848	val: 0.642491	test: 0.787863
PRC train: 0.940639	val: 0.922930	test: 0.798173

Epoch: 60
Loss: 0.29348130514421794
ROC train: 0.963211	val: 0.666667	test: 0.800730
PRC train: 0.943302	val: 0.924574	test: 0.810583

Epoch: 61
Loss: 0.2829278405734535
ROC train: 0.964740	val: 0.652015	test: 0.792384
PRC train: 0.945417	val: 0.921570	test: 0.793584

Epoch: 62
Loss: 0.28366402775043886
ROC train: 0.968679	val: 0.663736	test: 0.783342
PRC train: 0.951174	val: 0.927350	test: 0.782448

Epoch: 63
Loss: 0.28528623556302124
ROC train: 0.965562	val: 0.682418	test: 0.753608
PRC train: 0.946713	val: 0.928902	test: 0.763191

Epoch: 64
Loss: 0.29061925776341224
ROC train: 0.968216	val: 0.665934	test: 0.775170
PRC train: 0.950508	val: 0.923871	test: 0.785931

Epoch: 65
Loss: 0.2771697884736294
ROC train: 0.967757	val: 0.651282	test: 0.780386
PRC train: 0.949756	val: 0.922420	test: 0.791497

Epoch: 66
Loss: 0.2827451580130579
ROC train: 0.961906	val: 0.636264	test: 0.785429
PRC train: 0.942465	val: 0.918332	test: 0.789748

Epoch: 67
Loss: 0.2766595231796936
ROC train: 0.965896	val: 0.658608	test: 0.789776
PRC train: 0.947820	val: 0.923394	test: 0.785412

Epoch: 68
Loss: 0.2924419831820189
ROC train: 0.968998	val: 0.671062	test: 0.790993
PRC train: 0.950820	val: 0.927764	test: 0.795486

Epoch: 69
Loss: 0.2680544820037614
ROC train: 0.969181	val: 0.680952	test: 0.776213
PRC train: 0.952342	val: 0.929161	test: 0.781136

Epoch: 70
Loss: 0.2656975375599221
ROC train: 0.970865	val: 0.684249	test: 0.769258
PRC train: 0.954216	val: 0.930913	test: 0.773264

Epoch: 71
Loss: 0.27146056456357914
ROC train: 0.972089	val: 0.658974	test: 0.772561
PRC train: 0.956209	val: 0.923660	test: 0.774321

Epoch: 72
Loss: 0.29071487989327205
ROC train: 0.971424	val: 0.644322	test: 0.778995
PRC train: 0.954853	val: 0.919489	test: 0.771161

Epoch: 73
Loss: 0.28337081325482544
ROC train: 0.972109	val: 0.665201	test: 0.783168
PRC train: 0.955443	val: 0.926382	test: 0.774772

Epoch: 74
Loss: 0.268168673583132
ROC train: 0.972063	val: 0.661172	test: 0.773431
PRC train: 0.955637	val: 0.927717	test: 0.764170

Epoch: 75
Loss: 0.2726255718068188
ROC train: 0.972654	val: 0.657875	test: 0.792384
PRC train: 0.956465	val: 0.924678	test: 0.784096

Epoch: 76
Loss: 0.2690544641260177
ROC train: 0.973953	val: 0.657875	test: 0.799861
PRC train: 0.959257	val: 0.925391	test: 0.784967

Epoch: 77
Loss: 0.27479401190977276
ROC train: 0.973365	val: 0.657875	test: 0.789776
PRC train: 0.958610	val: 0.926849	test: 0.794333

Epoch: 78
Loss: 0.252525654810845
ROC train: 0.970205	val: 0.676190	test: 0.776734
PRC train: 0.953724	val: 0.930686	test: 0.773162

Epoch: 79
Loss: 0.2739519013811525
ROC train: 0.973179	val: 0.669231	test: 0.786646
PRC train: 0.958414	val: 0.926155	test: 0.781778

Epoch: 80
Loss: 0.2547680406899651
ROC train: 0.976698	val: 0.666667	test: 0.785776
PRC train: 0.964697	val: 0.926351	test: 0.780165

Epoch: 81
Loss: 0.26109841509787735
ROC train: 0.975614	val: 0.656044	test: 0.781255
PRC train: 0.963343	val: 0.927520	test: 0.772604

Epoch: 82
Loss: 0.25903528671296405
ROC train: 0.975939	val: 0.665934	test: 0.769953
PRC train: 0.963187	val: 0.927552	test: 0.762684

Epoch: 83
Loss: 0.2641861347990523
ROC train: 0.974158	val: 0.660073	test: 0.762998
PRC train: 0.959926	val: 0.924115	test: 0.761309

Epoch: 84
Loss: 0.2619961583116885
ROC train: 0.977041	val: 0.661905	test: 0.767866
PRC train: 0.964186	val: 0.927543	test: 0.766067

Epoch: 85
Loss: 0.25419756685446354
ROC train: 0.977055	val: 0.684982	test: 0.782820
PRC train: 0.964550	val: 0.929406	test: 0.781310

Epoch: 86
Loss: 0.25470559308551766
ROC train: 0.975043	val: 0.672527	test: 0.782125
PRC train: 0.960761	val: 0.926821	test: 0.772742

Epoch: 87
Loss: 0.22721754115125997
ROC train: 0.976547	val: 0.635165	test: 0.765258
PRC train: 0.963960	val: 0.920021	test: 0.767156

Epoch: 88
Loss: 0.2508526535872643
ROC train: 0.977200	val: 0.613919	test: 0.786646
PRC train: 0.966076	val: 0.911584	test: 0.783471

Epoch: 89
Loss: 0.24547598871738896
ROC train: 0.977563	val: 0.634066	test: 0.787863
PRC train: 0.965148	val: 0.916179	test: 0.776672

Epoch: 90
Loss: 0.24804104877384042
ROC train: 0.975916	val: 0.658608	test: 0.773083
PRC train: 0.964196	val: 0.921679	test: 0.748763

Epoch: 91
Loss: 0.24638442589179546
ROC train: 0.975591	val: 0.671795	test: 0.777778
PRC train: 0.963962	val: 0.927241	test: 0.771573

Epoch: 92
Loss: 0.2524851682146901
ROC train: 0.977591	val: 0.683516	test: 0.791862
PRC train: 0.966080	val: 0.932068	test: 0.790496

Epoch: 93
Loss: 0.2364279503720032
ROC train: 0.979492	val: 0.691941	test: 0.797600
PRC train: 0.968846	val: 0.931305	test: 0.799097

Epoch: 94
Loss: 0.22492926461884738
ROC train: 0.978978	val: 0.681319	test: 0.783516
PRC train: 0.915784	val: 0.924013	test: 0.820015

Epoch: 34
Loss: 0.3222581645210899
ROC train: 0.950953	val: 0.657509	test: 0.815163
PRC train: 0.921543	val: 0.922056	test: 0.829531

Epoch: 35
Loss: 0.33784734990092374
ROC train: 0.950850	val: 0.663370	test: 0.800209
PRC train: 0.922198	val: 0.923438	test: 0.820568

Epoch: 36
Loss: 0.34006207627353746
ROC train: 0.951727	val: 0.680220	test: 0.798991
PRC train: 0.923149	val: 0.926200	test: 0.825852

Epoch: 37
Loss: 0.32967578684212123
ROC train: 0.950967	val: 0.682418	test: 0.788037
PRC train: 0.923508	val: 0.927619	test: 0.817883

Epoch: 38
Loss: 0.31904282275265816
ROC train: 0.951513	val: 0.665934	test: 0.806468
PRC train: 0.924315	val: 0.927568	test: 0.821778

Epoch: 39
Loss: 0.3230519008899332
ROC train: 0.950477	val: 0.682418	test: 0.808381
PRC train: 0.922167	val: 0.927782	test: 0.810115

Epoch: 40
Loss: 0.32081810932543225
ROC train: 0.952340	val: 0.683883	test: 0.801947
PRC train: 0.924824	val: 0.927269	test: 0.805919

Epoch: 41
Loss: 0.3304760101287381
ROC train: 0.957295	val: 0.672894	test: 0.807686
PRC train: 0.933611	val: 0.925828	test: 0.820205

Epoch: 42
Loss: 0.30674177838158634
ROC train: 0.957586	val: 0.667033	test: 0.809424
PRC train: 0.932954	val: 0.923260	test: 0.827750

Epoch: 43
Loss: 0.31616360074426575
ROC train: 0.955796	val: 0.665568	test: 0.802643
PRC train: 0.931026	val: 0.923568	test: 0.814751

Epoch: 44
Loss: 0.3095308773234968
ROC train: 0.957480	val: 0.677289	test: 0.796383
PRC train: 0.932455	val: 0.926485	test: 0.813595

Epoch: 45
Loss: 0.2918826916109288
ROC train: 0.955325	val: 0.682418	test: 0.809772
PRC train: 0.929888	val: 0.927219	test: 0.828774

Epoch: 46
Loss: 0.30016135450075115
ROC train: 0.960691	val: 0.661538	test: 0.817423
PRC train: 0.937277	val: 0.926177	test: 0.835244

Epoch: 47
Loss: 0.32584504907745265
ROC train: 0.957951	val: 0.669231	test: 0.798991
PRC train: 0.933446	val: 0.929124	test: 0.817416

Epoch: 48
Loss: 0.315771924523862
ROC train: 0.960985	val: 0.655311	test: 0.798122
PRC train: 0.938327	val: 0.926075	test: 0.825064

Epoch: 49
Loss: 0.3024075145928424
ROC train: 0.963002	val: 0.647253	test: 0.811163
PRC train: 0.942182	val: 0.921930	test: 0.840446

Epoch: 50
Loss: 0.2861980280973716
ROC train: 0.961338	val: 0.660440	test: 0.814119
PRC train: 0.938134	val: 0.923195	test: 0.828490

Epoch: 51
Loss: 0.29562148401030514
ROC train: 0.962015	val: 0.638828	test: 0.826639
PRC train: 0.939848	val: 0.918474	test: 0.844200

Epoch: 52
Loss: 0.29684681707005706
ROC train: 0.961230	val: 0.656410	test: 0.835855
PRC train: 0.939180	val: 0.920280	test: 0.850572

Epoch: 53
Loss: 0.28784382872466474
ROC train: 0.961521	val: 0.661905	test: 0.808381
PRC train: 0.940193	val: 0.920114	test: 0.828747

Epoch: 54
Loss: 0.2931411019851338
ROC train: 0.962817	val: 0.648352	test: 0.791515
PRC train: 0.941561	val: 0.921601	test: 0.815739

Epoch: 55
Loss: 0.29762529322185716
ROC train: 0.964983	val: 0.654579	test: 0.797600
PRC train: 0.946695	val: 0.927061	test: 0.816504

Epoch: 56
Loss: 0.27900730216422975
ROC train: 0.958599	val: 0.669231	test: 0.769779
PRC train: 0.938033	val: 0.927942	test: 0.791874

Epoch: 57
Loss: 0.29527504915570246
ROC train: 0.964195	val: 0.656044	test: 0.760737
PRC train: 0.944834	val: 0.926032	test: 0.791242

Epoch: 58
Loss: 0.2894128327509982
ROC train: 0.962471	val: 0.640293	test: 0.781082
PRC train: 0.943098	val: 0.921786	test: 0.795139

Epoch: 59
Loss: 0.2882021362735854
ROC train: 0.961570	val: 0.659707	test: 0.799687
PRC train: 0.938941	val: 0.924658	test: 0.809583

Epoch: 60
Loss: 0.2787630009461985
ROC train: 0.965776	val: 0.639194	test: 0.810989
PRC train: 0.946242	val: 0.921349	test: 0.819255

Epoch: 61
Loss: 0.2771703017759468
ROC train: 0.968816	val: 0.669963	test: 0.781951
PRC train: 0.952035	val: 0.932071	test: 0.791360

Epoch: 62
Loss: 0.2723949958237655
ROC train: 0.966144	val: 0.691941	test: 0.740393
PRC train: 0.948903	val: 0.937160	test: 0.763073

Epoch: 63
Loss: 0.2798518018754536
ROC train: 0.965060	val: 0.691575	test: 0.761954
PRC train: 0.944377	val: 0.931113	test: 0.783982

Epoch: 64
Loss: 0.2859121201029697
ROC train: 0.967603	val: 0.670696	test: 0.800383
PRC train: 0.950653	val: 0.924566	test: 0.819763

Epoch: 65
Loss: 0.3061474929631524
ROC train: 0.969521	val: 0.671062	test: 0.812207
PRC train: 0.953684	val: 0.926504	test: 0.839660

Epoch: 66
Loss: 0.26715602999023896
ROC train: 0.970728	val: 0.642491	test: 0.802991
PRC train: 0.955119	val: 0.920775	test: 0.831331

Epoch: 67
Loss: 0.2529281182127773
ROC train: 0.970942	val: 0.669231	test: 0.793427
PRC train: 0.954637	val: 0.930042	test: 0.808690

Epoch: 68
Loss: 0.2755546168819897
ROC train: 0.973376	val: 0.671062	test: 0.793775
PRC train: 0.959146	val: 0.932599	test: 0.816326

Epoch: 69
Loss: 0.2570797820497137
ROC train: 0.970799	val: 0.653480	test: 0.795514
PRC train: 0.952574	val: 0.927675	test: 0.810259

Epoch: 70
Loss: 0.2899307724500429
ROC train: 0.973239	val: 0.658608	test: 0.802295
PRC train: 0.957903	val: 0.924553	test: 0.816948

Epoch: 71
Loss: 0.27308903701141807
ROC train: 0.973816	val: 0.653114	test: 0.800556
PRC train: 0.959531	val: 0.920600	test: 0.815035

Epoch: 72
Loss: 0.29955312592445005
ROC train: 0.973373	val: 0.651648	test: 0.805077
PRC train: 0.959292	val: 0.923636	test: 0.821838

Epoch: 73
Loss: 0.2627182036999186
ROC train: 0.973699	val: 0.647985	test: 0.784559
PRC train: 0.960904	val: 0.927642	test: 0.804257

Epoch: 74
Loss: 0.27457087109902945
ROC train: 0.973268	val: 0.625275	test: 0.766997
PRC train: 0.960334	val: 0.918773	test: 0.782709

Epoch: 75
Loss: 0.28072534992144016
ROC train: 0.975157	val: 0.659341	test: 0.798296
PRC train: 0.962880	val: 0.925725	test: 0.821997

Epoch: 76
Loss: 0.26920047301259786
ROC train: 0.967591	val: 0.698168	test: 0.783864
PRC train: 0.951673	val: 0.933587	test: 0.801510

Epoch: 77
Loss: 0.2605484204337949
ROC train: 0.974141	val: 0.694139	test: 0.778299
PRC train: 0.959564	val: 0.933989	test: 0.798668

Epoch: 78
Loss: 0.24762323701691485
ROC train: 0.975702	val: 0.678388	test: 0.772387
PRC train: 0.963308	val: 0.929411	test: 0.792012

Epoch: 79
Loss: 0.26845409627069095
ROC train: 0.977403	val: 0.650183	test: 0.762476
PRC train: 0.965228	val: 0.924382	test: 0.777057

Epoch: 80
Loss: 0.26339625003292894
ROC train: 0.979167	val: 0.647619	test: 0.764389
PRC train: 0.969215	val: 0.924506	test: 0.775540

Epoch: 81
Loss: 0.2638613725179459
ROC train: 0.976946	val: 0.669963	test: 0.771344
PRC train: 0.965667	val: 0.927797	test: 0.781674

Epoch: 82
Loss: 0.26097850163263864
ROC train: 0.978761	val: 0.660806	test: 0.776561
PRC train: 0.966804	val: 0.925693	test: 0.784372

Epoch: 83
Loss: 0.25348286865435293
ROC train: 0.976921	val: 0.669597	test: 0.761781
PRC train: 0.963438	val: 0.925584	test: 0.771706

Epoch: 84
Loss: 0.2562376877363505
ROC train: 0.979521	val: 0.675458	test: 0.770822
PRC train: 0.969516	val: 0.928015	test: 0.776503

Epoch: 85
Loss: 0.2661355888927999
ROC train: 0.978045	val: 0.674359	test: 0.769431
PRC train: 0.967174	val: 0.926995	test: 0.782308

Epoch: 86
Loss: 0.25766333194779945
ROC train: 0.976253	val: 0.666300	test: 0.770475
PRC train: 0.964378	val: 0.926662	test: 0.785870

Epoch: 87
Loss: 0.2474608192958661
ROC train: 0.978684	val: 0.657875	test: 0.768736
PRC train: 0.968403	val: 0.924545	test: 0.772950

Epoch: 88
Loss: 0.23888581481143203
ROC train: 0.979666	val: 0.663370	test: 0.754651
PRC train: 0.968468	val: 0.924716	test: 0.757430

Epoch: 89
Loss: 0.24940045295016344
ROC train: 0.981136	val: 0.665201	test: 0.761433
PRC train: 0.970995	val: 0.931194	test: 0.769754

Epoch: 90
Loss: 0.23359771961832537
ROC train: 0.979346	val: 0.669963	test: 0.750478
PRC train: 0.968791	val: 0.932250	test: 0.769720

Epoch: 91
Loss: 0.2470414040337737
ROC train: 0.979158	val: 0.663736	test: 0.753434
PRC train: 0.968688	val: 0.930702	test: 0.767463

Epoch: 92
Loss: 0.2265584342159567
ROC train: 0.982977	val: 0.660073	test: 0.782473
PRC train: 0.974460	val: 0.929358	test: 0.802495

Epoch: 93
Loss: 0.23973281084597642
ROC train: 0.975811	val: 0.670696	test: 0.770822
PRC train: 0.963111	val: 0.925744	test: 0.781150

Epoch: 94
Loss: 0.22657898526071812
ROC train: 0.980771	val: 0.671062	test: 0.778473
PRC train: 0.973584	val: 0.946544	test: 0.785344

Epoch: 33
Loss: 0.27796524486527513
ROC train: 0.979398	val: 0.720147	test: 0.797948
PRC train: 0.972634	val: 0.945619	test: 0.771656

Epoch: 34
Loss: 0.27631057701801826
ROC train: 0.986096	val: 0.750183	test: 0.785429
PRC train: 0.980351	val: 0.955231	test: 0.775665

Epoch: 35
Loss: 0.287019210451655
ROC train: 0.986946	val: 0.735531	test: 0.795688
PRC train: 0.980428	val: 0.951650	test: 0.787996

Epoch: 36
Loss: 0.26442497576201063
ROC train: 0.988151	val: 0.746154	test: 0.783864
PRC train: 0.982922	val: 0.952579	test: 0.780065

Epoch: 37
Loss: 0.274069520900451
ROC train: 0.990742	val: 0.764103	test: 0.781951
PRC train: 0.987287	val: 0.956998	test: 0.780224

Epoch: 38
Loss: 0.2614154610977708
ROC train: 0.992902	val: 0.751648	test: 0.766649
PRC train: 0.990469	val: 0.951267	test: 0.764580

Epoch: 39
Loss: 0.24630251838445597
ROC train: 0.993396	val: 0.722344	test: 0.762128
PRC train: 0.990674	val: 0.945686	test: 0.760621

Epoch: 40
Loss: 0.24847631335486337
ROC train: 0.991182	val: 0.695604	test: 0.774996
PRC train: 0.986944	val: 0.940220	test: 0.765121

Epoch: 41
Loss: 0.23257859101824319
ROC train: 0.993017	val: 0.718315	test: 0.787515
PRC train: 0.990388	val: 0.945200	test: 0.771910

Epoch: 42
Loss: 0.24887933061143652
ROC train: 0.994007	val: 0.724176	test: 0.792384
PRC train: 0.992186	val: 0.947082	test: 0.784110

Epoch: 43
Loss: 0.23762061198971907
ROC train: 0.996650	val: 0.730403	test: 0.776039
PRC train: 0.995369	val: 0.947501	test: 0.777501

Epoch: 44
Loss: 0.23952309490922935
ROC train: 0.996630	val: 0.732601	test: 0.756042
PRC train: 0.995656	val: 0.948897	test: 0.756710

Epoch: 45
Loss: 0.230487962263037
ROC train: 0.994104	val: 0.723810	test: 0.759172
PRC train: 0.991986	val: 0.947294	test: 0.742336

Epoch: 46
Loss: 0.21677942998169591
ROC train: 0.996849	val: 0.736630	test: 0.776908
PRC train: 0.995884	val: 0.950798	test: 0.759149

Epoch: 47
Loss: 0.206864440520045
ROC train: 0.996527	val: 0.712454	test: 0.795514
PRC train: 0.995259	val: 0.944479	test: 0.787566

Epoch: 48
Loss: 0.19047440998731308
ROC train: 0.997811	val: 0.719414	test: 0.777604
PRC train: 0.997052	val: 0.945105	test: 0.775666

Epoch: 49
Loss: 0.19795607793630476
ROC train: 0.998108	val: 0.743956	test: 0.765432
PRC train: 0.997396	val: 0.953625	test: 0.762837

Epoch: 50
Loss: 0.17957603759729665
ROC train: 0.997700	val: 0.741758	test: 0.758998
PRC train: 0.996736	val: 0.952790	test: 0.759996

Epoch: 51
Loss: 0.18245332569347933
ROC train: 0.997771	val: 0.733333	test: 0.774474
PRC train: 0.996927	val: 0.950536	test: 0.761661

Epoch: 52
Loss: 0.18410002059150804
ROC train: 0.998416	val: 0.727473	test: 0.765606
PRC train: 0.998188	val: 0.948865	test: 0.754575

Epoch: 53
Loss: 0.17543693728781085
ROC train: 0.999221	val: 0.742857	test: 0.773257
PRC train: 0.998904	val: 0.951748	test: 0.776880

Epoch: 54
Loss: 0.16589492656526383
ROC train: 0.997748	val: 0.739194	test: 0.747174
PRC train: 0.997064	val: 0.949979	test: 0.740953

Epoch: 55
Loss: 0.16871258261154623
ROC train: 0.998636	val: 0.751648	test: 0.739697
PRC train: 0.998026	val: 0.955218	test: 0.719151

Epoch: 56
Loss: 0.17904810342855845
ROC train: 0.998239	val: 0.743223	test: 0.752565
PRC train: 0.997519	val: 0.954125	test: 0.741616

Epoch: 57
Loss: 0.18283050326999026
ROC train: 0.997754	val: 0.711722	test: 0.776734
PRC train: 0.997135	val: 0.945275	test: 0.755582

Epoch: 58
Loss: 0.16411912744591267
ROC train: 0.996655	val: 0.700366	test: 0.773605
PRC train: 0.995057	val: 0.939757	test: 0.762383

Epoch: 59
Loss: 0.17191969177004485
ROC train: 0.999321	val: 0.716484	test: 0.764737
PRC train: 0.999024	val: 0.944368	test: 0.760627

Epoch: 60
Loss: 0.1506535271762596
ROC train: 0.999013	val: 0.774359	test: 0.733438
PRC train: 0.998580	val: 0.958875	test: 0.713218

Epoch: 61
Loss: 0.14900006363440732
ROC train: 0.998359	val: 0.766667	test: 0.737437
PRC train: 0.997601	val: 0.956797	test: 0.716443

Epoch: 62
Loss: 0.13311752091356496
ROC train: 0.999652	val: 0.739194	test: 0.770996
PRC train: 0.999486	val: 0.952181	test: 0.753430

Epoch: 63
Loss: 0.14631150907885476
ROC train: 0.999655	val: 0.709524	test: 0.790993
PRC train: 0.999488	val: 0.944311	test: 0.778686

Epoch: 64
Loss: 0.12832435098831224
ROC train: 0.998933	val: 0.721245	test: 0.785255
PRC train: 0.998501	val: 0.947412	test: 0.775122

Epoch: 65
Loss: 0.14648734393134913
ROC train: 0.999689	val: 0.717216	test: 0.769084
PRC train: 0.999572	val: 0.946379	test: 0.763416

Epoch: 66
Loss: 0.13551560614437008
ROC train: 0.999829	val: 0.719780	test: 0.784559
PRC train: 0.999742	val: 0.943893	test: 0.780298

Epoch: 67
Loss: 0.13731806202758556
ROC train: 0.999920	val: 0.724176	test: 0.776734
PRC train: 0.999880	val: 0.945881	test: 0.771880

Epoch: 68
Loss: 0.11276908621871184
ROC train: 0.999946	val: 0.726374	test: 0.782125
PRC train: 0.999918	val: 0.945698	test: 0.778152

Epoch: 69
Loss: 0.13543463794934357
ROC train: 0.999900	val: 0.701465	test: 0.780560
PRC train: 0.999848	val: 0.943544	test: 0.776750

Epoch: 70
Loss: 0.1315736665117644
ROC train: 0.999949	val: 0.700366	test: 0.764041
PRC train: 0.999922	val: 0.942892	test: 0.758462

Epoch: 71
Loss: 0.11886714513103533
ROC train: 0.999863	val: 0.723443	test: 0.766997
PRC train: 0.999801	val: 0.948158	test: 0.754515

Epoch: 72
Loss: 0.10202003277559442
ROC train: 0.999874	val: 0.744322	test: 0.783864
PRC train: 0.999820	val: 0.952023	test: 0.771120

Epoch: 73
Loss: 0.10400569393276346
ROC train: 0.999926	val: 0.698535	test: 0.780734
PRC train: 0.999886	val: 0.939513	test: 0.770870

Epoch: 74
Loss: 0.11198862573338302
ROC train: 0.999863	val: 0.710256	test: 0.771866
PRC train: 0.999794	val: 0.943321	test: 0.764088

Epoch: 75
Loss: 0.14024755689768478
ROC train: 0.999880	val: 0.719414	test: 0.766302
PRC train: 0.999822	val: 0.947125	test: 0.758485

Epoch: 76
Loss: 0.1052988700175114
ROC train: 0.999866	val: 0.756410	test: 0.766128
PRC train: 0.999793	val: 0.955847	test: 0.759428

Epoch: 77
Loss: 0.11664242005050011
ROC train: 0.999966	val: 0.757509	test: 0.770127
PRC train: 0.999948	val: 0.955913	test: 0.761855

Epoch: 78
Loss: 0.10930622883162164
ROC train: 0.999943	val: 0.739194	test: 0.751174
PRC train: 0.999912	val: 0.951770	test: 0.754686

Epoch: 79
Loss: 0.11056370942652953
ROC train: 0.999977	val: 0.739927	test: 0.773778
PRC train: 0.999965	val: 0.952234	test: 0.782355

Epoch: 80
Loss: 0.10314633991056679
ROC train: 0.999991	val: 0.726374	test: 0.775865
PRC train: 0.999987	val: 0.948900	test: 0.772343

Epoch: 81
Loss: 0.10429181586755416
ROC train: 0.999994	val: 0.730403	test: 0.757260
PRC train: 0.999991	val: 0.950623	test: 0.752796

Epoch: 82
Loss: 0.11299356164519103
ROC train: 0.999994	val: 0.746886	test: 0.763519
PRC train: 0.999991	val: 0.955060	test: 0.764860

Epoch: 83
Loss: 0.11421262422823326
ROC train: 0.999686	val: 0.754579	test: 0.762128
PRC train: 0.999518	val: 0.954247	test: 0.747051

Epoch: 84
Loss: 0.1187002149279716
ROC train: 0.999926	val: 0.759707	test: 0.770649
PRC train: 0.999885	val: 0.954598	test: 0.746022

Epoch: 85
Loss: 0.11190900555417786
ROC train: 0.999949	val: 0.746886	test: 0.796731
PRC train: 0.999922	val: 0.951771	test: 0.793450

Epoch: 86
Loss: 0.11523376077110511
ROC train: 0.999994	val: 0.725275	test: 0.796035
PRC train: 0.999991	val: 0.944744	test: 0.791059

Epoch: 87
Loss: 0.07345560962708896
ROC train: 0.999929	val: 0.702930	test: 0.768040
PRC train: 0.999892	val: 0.941629	test: 0.763312

Epoch: 88
Loss: 0.08807721847628038
ROC train: 0.999986	val: 0.676923	test: 0.778473
PRC train: 0.999979	val: 0.937598	test: 0.772117

Epoch: 89
Loss: 0.07944274519174159
ROC train: 1.000000	val: 0.689011	test: 0.786298
PRC train: 1.000000	val: 0.939610	test: 0.776716

Epoch: 90
Loss: 0.08773527770933924
ROC train: 0.999997	val: 0.722711	test: 0.785429
PRC train: 0.999996	val: 0.948512	test: 0.766391

Epoch: 91
Loss: 0.07403220543620045
ROC train: 1.000000	val: 0.705128	test: 0.774300
PRC train: 1.000000	val: 0.941259	test: 0.779514

Epoch: 92
Loss: 0.07393484299200793
ROC train: 1.000000	val: 0.701465	test: 0.761085
PRC train: 1.000000	val: 0.941687	test: 0.772979

Epoch: 93
Loss: 0.09456602107989105
ROC train: 0.999997	val: 0.719414	test: 0.770127
PRC train: 0.957843	val: 0.933066	test: 0.778188

Epoch: 33
Loss: 0.306900998635465
ROC train: 0.965719	val: 0.713553	test: 0.754130
PRC train: 0.948072	val: 0.937243	test: 0.759781

Epoch: 34
Loss: 0.30514676622449155
ROC train: 0.966207	val: 0.702930	test: 0.751695
PRC train: 0.951527	val: 0.936317	test: 0.762296

Epoch: 35
Loss: 0.2951154342016807
ROC train: 0.967808	val: 0.705495	test: 0.748218
PRC train: 0.952334	val: 0.934619	test: 0.756735

Epoch: 36
Loss: 0.31022609463505635
ROC train: 0.972080	val: 0.711355	test: 0.741262
PRC train: 0.954757	val: 0.934527	test: 0.752964

Epoch: 37
Loss: 0.28968076458967457
ROC train: 0.975491	val: 0.699267	test: 0.736046
PRC train: 0.961242	val: 0.928396	test: 0.757423

Epoch: 38
Loss: 0.28112608751671386
ROC train: 0.977494	val: 0.692308	test: 0.740741
PRC train: 0.966240	val: 0.926632	test: 0.765251

Epoch: 39
Loss: 0.2518217103132014
ROC train: 0.975645	val: 0.691209	test: 0.739176
PRC train: 0.964988	val: 0.928652	test: 0.747816

Epoch: 40
Loss: 0.26351701672861194
ROC train: 0.970662	val: 0.695238	test: 0.714311
PRC train: 0.958156	val: 0.932091	test: 0.739268

Epoch: 41
Loss: 0.28825912115081975
ROC train: 0.975063	val: 0.698535	test: 0.736220
PRC train: 0.963236	val: 0.931552	test: 0.753659

Epoch: 42
Loss: 0.26103297083703497
ROC train: 0.977740	val: 0.718681	test: 0.746131
PRC train: 0.967948	val: 0.941212	test: 0.760992

Epoch: 43
Loss: 0.26190625323154937
ROC train: 0.987197	val: 0.715751	test: 0.744045
PRC train: 0.980539	val: 0.936972	test: 0.763319

Epoch: 44
Loss: 0.25336012405681607
ROC train: 0.987409	val: 0.688645	test: 0.746131
PRC train: 0.980962	val: 0.924467	test: 0.770083

Epoch: 45
Loss: 0.23870003983686097
ROC train: 0.984538	val: 0.668132	test: 0.771518
PRC train: 0.975486	val: 0.917151	test: 0.785119

Epoch: 46
Loss: 0.24159245797324794
ROC train: 0.986433	val: 0.668864	test: 0.778647
PRC train: 0.980706	val: 0.923998	test: 0.782295

Epoch: 47
Loss: 0.23797219917618834
ROC train: 0.988964	val: 0.723077	test: 0.744914
PRC train: 0.982604	val: 0.937482	test: 0.756937

Epoch: 48
Loss: 0.2609002551842073
ROC train: 0.989575	val: 0.712454	test: 0.727178
PRC train: 0.984728	val: 0.934575	test: 0.741298

Epoch: 49
Loss: 0.23471094933203124
ROC train: 0.990942	val: 0.707326	test: 0.722831
PRC train: 0.987042	val: 0.931812	test: 0.753863

Epoch: 50
Loss: 0.21747676991255913
ROC train: 0.990850	val: 0.724908	test: 0.723526
PRC train: 0.986956	val: 0.935749	test: 0.756866

Epoch: 51
Loss: 0.21049129498295707
ROC train: 0.989506	val: 0.703297	test: 0.724222
PRC train: 0.985218	val: 0.930728	test: 0.764861

Epoch: 52
Loss: 0.22044727028336597
ROC train: 0.991687	val: 0.705495	test: 0.740567
PRC train: 0.988248	val: 0.933278	test: 0.755053

Epoch: 53
Loss: 0.21909487993021082
ROC train: 0.989526	val: 0.699267	test: 0.728047
PRC train: 0.985917	val: 0.934654	test: 0.761659

Epoch: 54
Loss: 0.21585111725907802
ROC train: 0.993687	val: 0.717216	test: 0.748218
PRC train: 0.991115	val: 0.934859	test: 0.766330

Epoch: 55
Loss: 0.18925870152482593
ROC train: 0.993365	val: 0.708791	test: 0.738306
PRC train: 0.990511	val: 0.931143	test: 0.762680

Epoch: 56
Loss: 0.1940023770263723
ROC train: 0.991658	val: 0.693773	test: 0.710659
PRC train: 0.987817	val: 0.929176	test: 0.733566

Epoch: 57
Loss: 0.21844230495564237
ROC train: 0.994612	val: 0.702198	test: 0.719701
PRC train: 0.992332	val: 0.928618	test: 0.729724

Epoch: 58
Loss: 0.18130023642972828
ROC train: 0.995003	val: 0.693040	test: 0.713441
PRC train: 0.993064	val: 0.931069	test: 0.740806

Epoch: 59
Loss: 0.2175897855744434
ROC train: 0.997058	val: 0.694872	test: 0.741089
PRC train: 0.996343	val: 0.930985	test: 0.762284

Epoch: 60
Loss: 0.20666931212436396
ROC train: 0.995885	val: 0.694872	test: 0.746827
PRC train: 0.994876	val: 0.926610	test: 0.773207

Epoch: 61
Loss: 0.20084428154695305
ROC train: 0.994455	val: 0.696337	test: 0.719527
PRC train: 0.993334	val: 0.930465	test: 0.739002

Epoch: 62
Loss: 0.18637370313829477
ROC train: 0.991590	val: 0.684615	test: 0.696401
PRC train: 0.988250	val: 0.929672	test: 0.694705

Epoch: 63
Loss: 0.17069210157484455
ROC train: 0.994001	val: 0.716850	test: 0.696748
PRC train: 0.991201	val: 0.937173	test: 0.737315

Epoch: 64
Loss: 0.18895968814583286
ROC train: 0.994486	val: 0.733700	test: 0.737437
PRC train: 0.991328	val: 0.938589	test: 0.744799

Epoch: 65
Loss: 0.181093737336194
ROC train: 0.996938	val: 0.723443	test: 0.735003
PRC train: 0.996166	val: 0.938895	test: 0.736456

Epoch: 66
Loss: 0.18794496993284127
ROC train: 0.996441	val: 0.727106	test: 0.685968
PRC train: 0.995549	val: 0.941547	test: 0.724681

Epoch: 67
Loss: 0.16539794648252998
ROC train: 0.996969	val: 0.730037	test: 0.680403
PRC train: 0.996032	val: 0.943464	test: 0.696392

Epoch: 68
Loss: 0.13821601790211302
ROC train: 0.997583	val: 0.720879	test: 0.700922
PRC train: 0.996724	val: 0.939314	test: 0.717557

Epoch: 69
Loss: 0.14736991175433362
ROC train: 0.997377	val: 0.697070	test: 0.722309
PRC train: 0.996663	val: 0.932223	test: 0.729484

Epoch: 70
Loss: 0.14692268406324915
ROC train: 0.997697	val: 0.699634	test: 0.724048
PRC train: 0.997309	val: 0.930800	test: 0.737936

Epoch: 71
Loss: 0.16264310949869093
ROC train: 0.997711	val: 0.725275	test: 0.720570
PRC train: 0.997323	val: 0.939452	test: 0.747667

Epoch: 72
Loss: 0.1526656157922904
ROC train: 0.996364	val: 0.711722	test: 0.727004
PRC train: 0.995346	val: 0.936512	test: 0.743744

Epoch: 73
Loss: 0.15073873290126505
ROC train: 0.998633	val: 0.710623	test: 0.752739
PRC train: 0.998437	val: 0.933580	test: 0.763896

Epoch: 74
Loss: 0.16621697999982016
ROC train: 0.996373	val: 0.692674	test: 0.715006
PRC train: 0.995117	val: 0.932202	test: 0.749410

Epoch: 75
Loss: 0.1666191063784576
ROC train: 0.996436	val: 0.685348	test: 0.706660
PRC train: 0.995278	val: 0.931374	test: 0.746372

Epoch: 76
Loss: 0.1532222967914004
ROC train: 0.998693	val: 0.691209	test: 0.739524
PRC train: 0.998259	val: 0.930517	test: 0.751983

Epoch: 77
Loss: 0.15530634431570242
ROC train: 0.996661	val: 0.663370	test: 0.744218
PRC train: 0.995870	val: 0.922363	test: 0.737699

Epoch: 78
Loss: 0.14588613564325925
ROC train: 0.995879	val: 0.687179	test: 0.711876
PRC train: 0.994608	val: 0.931410	test: 0.721840

Epoch: 79
Loss: 0.15078605444652976
ROC train: 0.998602	val: 0.715018	test: 0.713615
PRC train: 0.998231	val: 0.938708	test: 0.724537

Epoch: 80
Loss: 0.1256901317074776
ROC train: 0.998836	val: 0.706960	test: 0.719005
PRC train: 0.998591	val: 0.938192	test: 0.739200

Epoch: 81
Loss: 0.13811309999749194
ROC train: 0.998656	val: 0.709158	test: 0.720049
PRC train: 0.998387	val: 0.939079	test: 0.739177

Epoch: 82
Loss: 0.1410581078780485
ROC train: 0.998944	val: 0.717582	test: 0.735176
PRC train: 0.998676	val: 0.938638	test: 0.763494

Epoch: 83
Loss: 0.11456514391192352
ROC train: 0.998813	val: 0.709158	test: 0.712572
PRC train: 0.998462	val: 0.935074	test: 0.732078

Epoch: 84
Loss: 0.11557859022325774
ROC train: 0.998693	val: 0.703297	test: 0.693966
PRC train: 0.998329	val: 0.933924	test: 0.706370

Epoch: 85
Loss: 0.1168439782838691
ROC train: 0.998907	val: 0.727839	test: 0.689793
PRC train: 0.998566	val: 0.940437	test: 0.704071

Epoch: 86
Loss: 0.11848558433336276
ROC train: 0.999175	val: 0.708059	test: 0.689967
PRC train: 0.999094	val: 0.938210	test: 0.718389

Epoch: 87
Loss: 0.11411562796147796
ROC train: 0.999007	val: 0.681319	test: 0.705095
PRC train: 0.998923	val: 0.928999	test: 0.744755

Epoch: 88
Loss: 0.11190014020671823
ROC train: 0.999272	val: 0.671429	test: 0.713441
PRC train: 0.999145	val: 0.926123	test: 0.739903

Epoch: 89
Loss: 0.11391616915077338
ROC train: 0.999386	val: 0.695604	test: 0.713267
PRC train: 0.999296	val: 0.931768	test: 0.717531

Epoch: 90
Loss: 0.10572193460106889
ROC train: 0.999053	val: 0.703663	test: 0.705616
PRC train: 0.998936	val: 0.936120	test: 0.730601

Epoch: 91
Loss: 0.10598591466832748
ROC train: 0.999010	val: 0.705495	test: 0.699183
PRC train: 0.998821	val: 0.934195	test: 0.729573

Epoch: 92
Loss: 0.10041136169588986
ROC train: 0.999486	val: 0.723077	test: 0.714137
PRC train: 0.999345	val: 0.935376	test: 0.742557

Epoch: 93
Loss: 0.1171916290496929
ROC train: 0.999623	val: 0.684249	test: 0.742653
PRC train: 0.980526	val: 0.947015	test: 0.708883

Epoch: 33
Loss: 0.2957321726265992
ROC train: 0.989609	val: 0.710989	test: 0.742480
PRC train: 0.983704	val: 0.943436	test: 0.713904

Epoch: 34
Loss: 0.2994012158718228
ROC train: 0.991267	val: 0.709158	test: 0.756042
PRC train: 0.986854	val: 0.941112	test: 0.720766

Epoch: 35
Loss: 0.27926566005902437
ROC train: 0.992880	val: 0.724908	test: 0.737785
PRC train: 0.988798	val: 0.946714	test: 0.716335

Epoch: 36
Loss: 0.2654830266533731
ROC train: 0.988028	val: 0.726374	test: 0.729091
PRC train: 0.981281	val: 0.946883	test: 0.725865

Epoch: 37
Loss: 0.25700703554666826
ROC train: 0.990100	val: 0.691209	test: 0.779343
PRC train: 0.983556	val: 0.936690	test: 0.743497

Epoch: 38
Loss: 0.25154175583033134
ROC train: 0.994115	val: 0.738462	test: 0.757781
PRC train: 0.990963	val: 0.949415	test: 0.722990

Epoch: 39
Loss: 0.23783686710594024
ROC train: 0.996718	val: 0.746520	test: 0.745957
PRC train: 0.995291	val: 0.952447	test: 0.716261

Epoch: 40
Loss: 0.2398527134270394
ROC train: 0.996915	val: 0.717216	test: 0.762128
PRC train: 0.995528	val: 0.943906	test: 0.728112

Epoch: 41
Loss: 0.2549216959503533
ROC train: 0.997423	val: 0.704029	test: 0.753608
PRC train: 0.996147	val: 0.940091	test: 0.719752

Epoch: 42
Loss: 0.24702796094049756
ROC train: 0.997394	val: 0.719048	test: 0.751695
PRC train: 0.996056	val: 0.943472	test: 0.713746

Epoch: 43
Loss: 0.2233383083218894
ROC train: 0.998816	val: 0.707692	test: 0.768910
PRC train: 0.998219	val: 0.940394	test: 0.741449

Epoch: 44
Loss: 0.22759778021641322
ROC train: 0.996607	val: 0.716484	test: 0.733438
PRC train: 0.994675	val: 0.942164	test: 0.727855

Epoch: 45
Loss: 0.19556324315885368
ROC train: 0.999015	val: 0.733700	test: 0.758825
PRC train: 0.998577	val: 0.947485	test: 0.731709

Epoch: 46
Loss: 0.19232616321721333
ROC train: 0.999124	val: 0.737363	test: 0.760216
PRC train: 0.998661	val: 0.949441	test: 0.717830

Epoch: 47
Loss: 0.21396344738300663
ROC train: 0.999207	val: 0.715751	test: 0.765780
PRC train: 0.998795	val: 0.943186	test: 0.726471

Epoch: 48
Loss: 0.19539000623869743
ROC train: 0.999683	val: 0.722711	test: 0.749957
PRC train: 0.999523	val: 0.944391	test: 0.732919

Epoch: 49
Loss: 0.18287227622731583
ROC train: 0.999706	val: 0.738828	test: 0.760389
PRC train: 0.999551	val: 0.948939	test: 0.739051

Epoch: 50
Loss: 0.19231705714045993
ROC train: 0.999769	val: 0.726740	test: 0.778647
PRC train: 0.999649	val: 0.947153	test: 0.754242

Epoch: 51
Loss: 0.18378137524847898
ROC train: 0.999752	val: 0.742491	test: 0.769084
PRC train: 0.999627	val: 0.950591	test: 0.748372

Epoch: 52
Loss: 0.1833097237166647
ROC train: 0.999615	val: 0.723077	test: 0.760216
PRC train: 0.999422	val: 0.944333	test: 0.729927

Epoch: 53
Loss: 0.17930071653413043
ROC train: 0.997894	val: 0.700733	test: 0.756738
PRC train: 0.996777	val: 0.936998	test: 0.726224

Epoch: 54
Loss: 0.15870944007060245
ROC train: 0.997865	val: 0.698901	test: 0.767519
PRC train: 0.996593	val: 0.938868	test: 0.737175

Epoch: 55
Loss: 0.17889150237732163
ROC train: 0.999501	val: 0.740293	test: 0.771344
PRC train: 0.999277	val: 0.950625	test: 0.760005

Epoch: 56
Loss: 0.1719582253264107
ROC train: 0.999272	val: 0.734066	test: 0.772387
PRC train: 0.998927	val: 0.949058	test: 0.754948

Epoch: 57
Loss: 0.14424389299101859
ROC train: 0.999058	val: 0.733333	test: 0.755173
PRC train: 0.998598	val: 0.947811	test: 0.741215

Epoch: 58
Loss: 0.16555959453086402
ROC train: 0.999963	val: 0.743223	test: 0.756390
PRC train: 0.999945	val: 0.950538	test: 0.727027

Epoch: 59
Loss: 0.17131512498551554
ROC train: 0.999675	val: 0.711722	test: 0.785255
PRC train: 0.999524	val: 0.940844	test: 0.752547

Epoch: 60
Loss: 0.13578129674947084
ROC train: 0.999946	val: 0.707692	test: 0.782646
PRC train: 0.999918	val: 0.939646	test: 0.750780

Epoch: 61
Loss: 0.13464982760025673
ROC train: 0.999866	val: 0.717582	test: 0.786820
PRC train: 0.999799	val: 0.944622	test: 0.764141

Epoch: 62
Loss: 0.14508553159388113
ROC train: 0.999797	val: 0.751282	test: 0.771866
PRC train: 0.999689	val: 0.954085	test: 0.756580

Epoch: 63
Loss: 0.11276031049092428
ROC train: 0.999977	val: 0.756777	test: 0.770475
PRC train: 0.999966	val: 0.955626	test: 0.756258

Epoch: 64
Loss: 0.13230656382826952
ROC train: 0.999983	val: 0.727839	test: 0.760911
PRC train: 0.999974	val: 0.948554	test: 0.746161

Epoch: 65
Loss: 0.12358220935728056
ROC train: 1.000000	val: 0.721612	test: 0.761607
PRC train: 1.000000	val: 0.947158	test: 0.733422

Epoch: 66
Loss: 0.11231705909641432
ROC train: 1.000000	val: 0.727473	test: 0.772909
PRC train: 1.000000	val: 0.948722	test: 0.733277

Epoch: 67
Loss: 0.12872453178414883
ROC train: 0.999994	val: 0.726007	test: 0.781429
PRC train: 0.999991	val: 0.944754	test: 0.751029

Epoch: 68
Loss: 0.10547285440455963
ROC train: 1.000000	val: 0.726740	test: 0.780560
PRC train: 1.000000	val: 0.944822	test: 0.754199

Epoch: 69
Loss: 0.11866040119921029
ROC train: 0.999994	val: 0.730037	test: 0.778473
PRC train: 0.999991	val: 0.947960	test: 0.747089

Epoch: 70
Loss: 0.1055578935669819
ROC train: 0.999977	val: 0.718315	test: 0.777604
PRC train: 0.999966	val: 0.945241	test: 0.752196

Epoch: 71
Loss: 0.12163961615025683
ROC train: 0.999986	val: 0.719048	test: 0.772909
PRC train: 0.999978	val: 0.944599	test: 0.754630

Epoch: 72
Loss: 0.08999218146986329
ROC train: 0.999940	val: 0.731868	test: 0.769605
PRC train: 0.999913	val: 0.946847	test: 0.759284

Epoch: 73
Loss: 0.10759885649463789
ROC train: 1.000000	val: 0.715018	test: 0.773605
PRC train: 1.000000	val: 0.943798	test: 0.754396

Epoch: 74
Loss: 0.10931646156302832
ROC train: 1.000000	val: 0.721612	test: 0.778299
PRC train: 1.000000	val: 0.946267	test: 0.755884

Epoch: 75
Loss: 0.0906321766369105
ROC train: 1.000000	val: 0.764469	test: 0.783690
PRC train: 1.000000	val: 0.955217	test: 0.745597

Epoch: 76
Loss: 0.10313279642114241
ROC train: 1.000000	val: 0.761538	test: 0.765432
PRC train: 1.000000	val: 0.953765	test: 0.732938

Epoch: 77
Loss: 0.10081014446502803
ROC train: 1.000000	val: 0.727473	test: 0.751869
PRC train: 1.000000	val: 0.945938	test: 0.729617

Epoch: 78
Loss: 0.09369849676294731
ROC train: 1.000000	val: 0.690842	test: 0.768910
PRC train: 1.000000	val: 0.937581	test: 0.731610

Epoch: 79
Loss: 0.09555642182053878
ROC train: 0.999951	val: 0.714652	test: 0.782646
PRC train: 0.999928	val: 0.944086	test: 0.750789

Epoch: 80
Loss: 0.10307175096485169
ROC train: 0.999989	val: 0.727473	test: 0.770996
PRC train: 0.999983	val: 0.946896	test: 0.736467

Epoch: 81
Loss: 0.11521348729610473
ROC train: 0.999994	val: 0.732234	test: 0.772561
PRC train: 0.999991	val: 0.948698	test: 0.737267

Epoch: 82
Loss: 0.09256123478814678
ROC train: 0.999994	val: 0.740659	test: 0.768910
PRC train: 0.999991	val: 0.949817	test: 0.752964

Epoch: 83
Loss: 0.0900413028366732
ROC train: 0.999991	val: 0.738828	test: 0.762476
PRC train: 0.999987	val: 0.949962	test: 0.747934

Epoch: 84
Loss: 0.10517968253711225
ROC train: 1.000000	val: 0.724176	test: 0.779169
PRC train: 1.000000	val: 0.947212	test: 0.737031

Epoch: 85
Loss: 0.07412730198914638
ROC train: 0.999969	val: 0.686081	test: 0.771170
PRC train: 0.999952	val: 0.935209	test: 0.734470

Epoch: 86
Loss: 0.0898543598646454
ROC train: 0.999969	val: 0.711355	test: 0.747696
PRC train: 0.999953	val: 0.943621	test: 0.726328

Epoch: 87
Loss: 0.09244843932788091
ROC train: 0.999963	val: 0.716117	test: 0.764563
PRC train: 0.999943	val: 0.940216	test: 0.742444

Epoch: 88
Loss: 0.09133449635153455
ROC train: 0.999966	val: 0.735531	test: 0.776561
PRC train: 0.999947	val: 0.947302	test: 0.753637

Epoch: 89
Loss: 0.08296687442391941
ROC train: 1.000000	val: 0.727473	test: 0.780038
PRC train: 1.000000	val: 0.947385	test: 0.747696

Epoch: 90
Loss: 0.08200953627365086
ROC train: 1.000000	val: 0.710989	test: 0.785255
PRC train: 1.000000	val: 0.941663	test: 0.752954

Epoch: 91
Loss: 0.0703639457103816
ROC train: 1.000000	val: 0.717949	test: 0.775691
PRC train: 1.000000	val: 0.946371	test: 0.744520

Epoch: 92
Loss: 0.0717479716448555
ROC train: 1.000000	val: 0.705861	test: 0.749609
PRC train: 1.000000	val: 0.942582	test: 0.719786

Epoch: 93
Loss: 0.07669055969320179
ROC train: 1.000000	val: 0.693040	test: 0.748218
PRC train: 0.951926	val: 0.938013	test: 0.794803

Epoch: 33
Loss: 0.3087584421816672
ROC train: 0.975103	val: 0.706593	test: 0.812380
PRC train: 0.960378	val: 0.942138	test: 0.806601

Epoch: 34
Loss: 0.302760162390369
ROC train: 0.974158	val: 0.654945	test: 0.804034
PRC train: 0.958615	val: 0.926102	test: 0.804818

Epoch: 35
Loss: 0.30626978527384485
ROC train: 0.977075	val: 0.650183	test: 0.796557
PRC train: 0.964444	val: 0.921910	test: 0.804002

Epoch: 36
Loss: 0.2824531537686147
ROC train: 0.973045	val: 0.661538	test: 0.802295
PRC train: 0.959991	val: 0.924225	test: 0.793197

Epoch: 37
Loss: 0.2891731162078523
ROC train: 0.978088	val: 0.663004	test: 0.816032
PRC train: 0.966645	val: 0.926677	test: 0.808087

Epoch: 38
Loss: 0.26808151208355546
ROC train: 0.983222	val: 0.672894	test: 0.815510
PRC train: 0.975317	val: 0.930839	test: 0.802561

Epoch: 39
Loss: 0.25849230344168583
ROC train: 0.984261	val: 0.662637	test: 0.817945
PRC train: 0.975996	val: 0.929946	test: 0.798660

Epoch: 40
Loss: 0.2713818857966931
ROC train: 0.985325	val: 0.658974	test: 0.811337
PRC train: 0.978201	val: 0.928765	test: 0.791247

Epoch: 41
Loss: 0.23422989708991623
ROC train: 0.985685	val: 0.664469	test: 0.808555
PRC train: 0.980056	val: 0.929954	test: 0.792497

Epoch: 42
Loss: 0.2585504436643195
ROC train: 0.985117	val: 0.659707	test: 0.807860
PRC train: 0.976970	val: 0.929764	test: 0.802145

Epoch: 43
Loss: 0.2662632737400662
ROC train: 0.989130	val: 0.665201	test: 0.814293
PRC train: 0.983393	val: 0.930052	test: 0.799774

Epoch: 44
Loss: 0.2437981326666133
ROC train: 0.988864	val: 0.704762	test: 0.799513
PRC train: 0.982946	val: 0.941023	test: 0.785208

Epoch: 45
Loss: 0.24291960363956103
ROC train: 0.988673	val: 0.690110	test: 0.794644
PRC train: 0.983275	val: 0.940855	test: 0.774865

Epoch: 46
Loss: 0.22627751358228837
ROC train: 0.988256	val: 0.670330	test: 0.810468
PRC train: 0.982840	val: 0.932978	test: 0.790089

Epoch: 47
Loss: 0.2444371777034026
ROC train: 0.989951	val: 0.691941	test: 0.816901
PRC train: 0.985906	val: 0.932933	test: 0.794563

Epoch: 48
Loss: 0.24876953403664137
ROC train: 0.992668	val: 0.684249	test: 0.806816
PRC train: 0.989656	val: 0.931850	test: 0.794250

Epoch: 49
Loss: 0.23555202480334803
ROC train: 0.991284	val: 0.666300	test: 0.778647
PRC train: 0.987062	val: 0.932962	test: 0.768153

Epoch: 50
Loss: 0.20653507848671843
ROC train: 0.992842	val: 0.686081	test: 0.775517
PRC train: 0.988772	val: 0.935966	test: 0.755662

Epoch: 51
Loss: 0.2039992368246842
ROC train: 0.993747	val: 0.681685	test: 0.796035
PRC train: 0.991466	val: 0.932434	test: 0.778145

Epoch: 52
Loss: 0.1850011898674187
ROC train: 0.991447	val: 0.652381	test: 0.796035
PRC train: 0.988809	val: 0.925695	test: 0.798336

Epoch: 53
Loss: 0.21169214644690965
ROC train: 0.993787	val: 0.643956	test: 0.802121
PRC train: 0.991795	val: 0.924299	test: 0.791961

Epoch: 54
Loss: 0.22248371180482054
ROC train: 0.992888	val: 0.654945	test: 0.784733
PRC train: 0.990959	val: 0.921074	test: 0.772075

Epoch: 55
Loss: 0.241998346652115
ROC train: 0.992691	val: 0.656044	test: 0.769953
PRC train: 0.990239	val: 0.923089	test: 0.759829

Epoch: 56
Loss: 0.20571586146602455
ROC train: 0.993647	val: 0.668864	test: 0.778473
PRC train: 0.991671	val: 0.928613	test: 0.768540

Epoch: 57
Loss: 0.20065448012343726
ROC train: 0.994678	val: 0.658242	test: 0.793079
PRC train: 0.992568	val: 0.931578	test: 0.780796

Epoch: 58
Loss: 0.204931805296607
ROC train: 0.992643	val: 0.695971	test: 0.790471
PRC train: 0.989987	val: 0.939819	test: 0.782371

Epoch: 59
Loss: 0.20699792642617462
ROC train: 0.994278	val: 0.695971	test: 0.782299
PRC train: 0.992391	val: 0.938663	test: 0.760969

Epoch: 60
Loss: 0.20025530021355836
ROC train: 0.995953	val: 0.678755	test: 0.789428
PRC train: 0.994794	val: 0.932760	test: 0.773719

Epoch: 61
Loss: 0.1859745765127309
ROC train: 0.997120	val: 0.635897	test: 0.780560
PRC train: 0.996120	val: 0.922557	test: 0.753155

Epoch: 62
Loss: 0.17986382071360824
ROC train: 0.997334	val: 0.654945	test: 0.792384
PRC train: 0.996351	val: 0.927223	test: 0.767281

Epoch: 63
Loss: 0.16955609381627687
ROC train: 0.996841	val: 0.674359	test: 0.801600
PRC train: 0.995764	val: 0.931785	test: 0.779541

Epoch: 64
Loss: 0.17373314691084593
ROC train: 0.997671	val: 0.671795	test: 0.797600
PRC train: 0.997095	val: 0.931980	test: 0.773169

Epoch: 65
Loss: 0.1672481107393934
ROC train: 0.997606	val: 0.666300	test: 0.786298
PRC train: 0.996697	val: 0.931761	test: 0.770043

Epoch: 66
Loss: 0.17019194734706092
ROC train: 0.996866	val: 0.659341	test: 0.800209
PRC train: 0.995388	val: 0.931175	test: 0.782025

Epoch: 67
Loss: 0.17863806875779167
ROC train: 0.998385	val: 0.676190	test: 0.795166
PRC train: 0.997617	val: 0.934783	test: 0.772963

Epoch: 68
Loss: 0.16568721876046272
ROC train: 0.998673	val: 0.628205	test: 0.782299
PRC train: 0.998165	val: 0.922346	test: 0.764900

Epoch: 69
Loss: 0.15768202928604055
ROC train: 0.998545	val: 0.616117	test: 0.771518
PRC train: 0.997857	val: 0.917727	test: 0.756082

Epoch: 70
Loss: 0.1817219841306214
ROC train: 0.998596	val: 0.625641	test: 0.772735
PRC train: 0.997935	val: 0.918755	test: 0.765751

Epoch: 71
Loss: 0.1589494652819538
ROC train: 0.997971	val: 0.668864	test: 0.806121
PRC train: 0.997295	val: 0.927842	test: 0.792530

Epoch: 72
Loss: 0.14095831593066369
ROC train: 0.998761	val: 0.674725	test: 0.796731
PRC train: 0.998247	val: 0.932911	test: 0.759226

Epoch: 73
Loss: 0.13909991402765623
ROC train: 0.998039	val: 0.680952	test: 0.775517
PRC train: 0.997161	val: 0.936590	test: 0.753183

Epoch: 74
Loss: 0.14587709145663302
ROC train: 0.997882	val: 0.697802	test: 0.778821
PRC train: 0.996836	val: 0.938915	test: 0.752710

Epoch: 75
Loss: 0.1448693764606896
ROC train: 0.999121	val: 0.700733	test: 0.798470
PRC train: 0.998877	val: 0.937484	test: 0.778150

Epoch: 76
Loss: 0.12470300155496768
ROC train: 0.998690	val: 0.667399	test: 0.804556
PRC train: 0.998286	val: 0.929153	test: 0.792140

Epoch: 77
Loss: 0.11684348392617916
ROC train: 0.999449	val: 0.646520	test: 0.789254
PRC train: 0.999232	val: 0.924002	test: 0.776350

Epoch: 78
Loss: 0.14688763718020886
ROC train: 0.999686	val: 0.674359	test: 0.774300
PRC train: 0.999553	val: 0.931824	test: 0.747276

Epoch: 79
Loss: 0.11278846648332896
ROC train: 0.999446	val: 0.674359	test: 0.771692
PRC train: 0.999175	val: 0.930581	test: 0.749580

Epoch: 80
Loss: 0.12417036600384337
ROC train: 0.999692	val: 0.664469	test: 0.787167
PRC train: 0.999533	val: 0.930227	test: 0.760338

Epoch: 81
Loss: 0.12601329697973918
ROC train: 0.999852	val: 0.650916	test: 0.781255
PRC train: 0.999777	val: 0.927550	test: 0.765168

Epoch: 82
Loss: 0.128466972005601
ROC train: 0.999558	val: 0.659707	test: 0.792210
PRC train: 0.999332	val: 0.930537	test: 0.777126

Epoch: 83
Loss: 0.11553388424351803
ROC train: 0.999424	val: 0.673626	test: 0.795340
PRC train: 0.999118	val: 0.933343	test: 0.783800

Epoch: 84
Loss: 0.12342097740928384
ROC train: 0.999523	val: 0.693773	test: 0.782994
PRC train: 0.999313	val: 0.935976	test: 0.773320

Epoch: 85
Loss: 0.13424755042691344
ROC train: 0.999897	val: 0.656044	test: 0.798470
PRC train: 0.999847	val: 0.929330	test: 0.782567

Epoch: 86
Loss: 0.13673473748960768
ROC train: 0.999672	val: 0.675092	test: 0.790645
PRC train: 0.999549	val: 0.929233	test: 0.773417

Epoch: 87
Loss: 0.12820321001603235
ROC train: 0.999857	val: 0.670330	test: 0.772735
PRC train: 0.999784	val: 0.928922	test: 0.755410

Epoch: 88
Loss: 0.10778354397416594
ROC train: 0.999580	val: 0.651648	test: 0.764737
PRC train: 0.999408	val: 0.924533	test: 0.745674

Epoch: 89
Loss: 0.1113997675800061
ROC train: 0.999466	val: 0.634432	test: 0.795862
PRC train: 0.999174	val: 0.923445	test: 0.784777

Epoch: 90
Loss: 0.10995390371866365
ROC train: 0.999854	val: 0.649451	test: 0.798644
PRC train: 0.999778	val: 0.924459	test: 0.790072

Epoch: 91
Loss: 0.0946628876293385
ROC train: 0.999929	val: 0.653114	test: 0.782994
PRC train: 0.999892	val: 0.924607	test: 0.764994

Epoch: 92
Loss: 0.09605399109000903
ROC train: 0.999963	val: 0.626007	test: 0.768388
PRC train: 0.999944	val: 0.918284	test: 0.747405

Epoch: 93
Loss: 0.10862760742757746
ROC train: 0.999943	val: 0.652015	test: 0.762998
PRC train: 0.963264	val: 0.930855	test: 0.821038

Epoch: 33
Loss: 0.2937405595138574
ROC train: 0.977977	val: 0.653114	test: 0.829943
PRC train: 0.967632	val: 0.925165	test: 0.827453

Epoch: 34
Loss: 0.2958861701754698
ROC train: 0.979238	val: 0.671795	test: 0.828204
PRC train: 0.969233	val: 0.927634	test: 0.824793

Epoch: 35
Loss: 0.2768102131440498
ROC train: 0.982514	val: 0.658242	test: 0.840376
PRC train: 0.973749	val: 0.925432	test: 0.831884

Epoch: 36
Loss: 0.29307933394788055
ROC train: 0.981764	val: 0.629670	test: 0.844549
PRC train: 0.972153	val: 0.920691	test: 0.844169

Epoch: 37
Loss: 0.2480582921946619
ROC train: 0.986572	val: 0.641026	test: 0.831855
PRC train: 0.981089	val: 0.923243	test: 0.835856

Epoch: 38
Loss: 0.27348390069387046
ROC train: 0.983662	val: 0.675092	test: 0.819336
PRC train: 0.976860	val: 0.932378	test: 0.818514

Epoch: 39
Loss: 0.26453020136831223
ROC train: 0.987061	val: 0.654579	test: 0.791341
PRC train: 0.982059	val: 0.926281	test: 0.780752

Epoch: 40
Loss: 0.26136097837570393
ROC train: 0.986427	val: 0.627473	test: 0.819857
PRC train: 0.980152	val: 0.920313	test: 0.819763

Epoch: 41
Loss: 0.23846638253122202
ROC train: 0.990177	val: 0.642857	test: 0.825074
PRC train: 0.985134	val: 0.924941	test: 0.822466

Epoch: 42
Loss: 0.2824388607977457
ROC train: 0.991869	val: 0.643590	test: 0.819857
PRC train: 0.987916	val: 0.925752	test: 0.826762

Epoch: 43
Loss: 0.2547238454772153
ROC train: 0.987697	val: 0.660440	test: 0.811511
PRC train: 0.981025	val: 0.929319	test: 0.817627

Epoch: 44
Loss: 0.23239392499293615
ROC train: 0.989192	val: 0.673260	test: 0.822292
PRC train: 0.983386	val: 0.931524	test: 0.824913

Epoch: 45
Loss: 0.21816841285317237
ROC train: 0.992725	val: 0.641392	test: 0.836376
PRC train: 0.989748	val: 0.920994	test: 0.840173

Epoch: 46
Loss: 0.23394290633173132
ROC train: 0.991142	val: 0.635531	test: 0.829943
PRC train: 0.986894	val: 0.917169	test: 0.830792

Epoch: 47
Loss: 0.20506346774334175
ROC train: 0.993804	val: 0.634432	test: 0.818466
PRC train: 0.991085	val: 0.918745	test: 0.815988

Epoch: 48
Loss: 0.21538138069377308
ROC train: 0.993536	val: 0.643223	test: 0.827682
PRC train: 0.990348	val: 0.920080	test: 0.826389

Epoch: 49
Loss: 0.20318478443551108
ROC train: 0.995220	val: 0.642125	test: 0.810120
PRC train: 0.993216	val: 0.922376	test: 0.813462

Epoch: 50
Loss: 0.21221402923748536
ROC train: 0.997277	val: 0.636264	test: 0.821770
PRC train: 0.996133	val: 0.919528	test: 0.818493

Epoch: 51
Loss: 0.2071978157173465
ROC train: 0.997255	val: 0.649084	test: 0.809077
PRC train: 0.996015	val: 0.922174	test: 0.793709

Epoch: 52
Loss: 0.21539800081728772
ROC train: 0.998168	val: 0.646886	test: 0.806990
PRC train: 0.997258	val: 0.922545	test: 0.800243

Epoch: 53
Loss: 0.1908328062269823
ROC train: 0.995885	val: 0.615751	test: 0.828204
PRC train: 0.993762	val: 0.916943	test: 0.837802

Epoch: 54
Loss: 0.18421459058211478
ROC train: 0.996655	val: 0.658608	test: 0.822987
PRC train: 0.995107	val: 0.924040	test: 0.825068

Epoch: 55
Loss: 0.19039687747858008
ROC train: 0.996792	val: 0.616117	test: 0.824031
PRC train: 0.995264	val: 0.914672	test: 0.820168

Epoch: 56
Loss: 0.1814289270931289
ROC train: 0.996926	val: 0.593773	test: 0.817771
PRC train: 0.995553	val: 0.904626	test: 0.800903

Epoch: 57
Loss: 0.17861475591855086
ROC train: 0.997314	val: 0.679121	test: 0.799513
PRC train: 0.996016	val: 0.928302	test: 0.764646

Epoch: 58
Loss: 0.17229642141188176
ROC train: 0.995414	val: 0.652015	test: 0.828725
PRC train: 0.992962	val: 0.924665	test: 0.813167

Epoch: 59
Loss: 0.18997682529310417
ROC train: 0.998159	val: 0.617582	test: 0.828378
PRC train: 0.997313	val: 0.914523	test: 0.825886

Epoch: 60
Loss: 0.1801824583108642
ROC train: 0.998667	val: 0.642857	test: 0.810989
PRC train: 0.998054	val: 0.921608	test: 0.807940

Epoch: 61
Loss: 0.148075442872596
ROC train: 0.997377	val: 0.635165	test: 0.820727
PRC train: 0.996220	val: 0.920797	test: 0.817302

Epoch: 62
Loss: 0.14682563607645038
ROC train: 0.999132	val: 0.650916	test: 0.837420
PRC train: 0.998687	val: 0.923459	test: 0.825213

Epoch: 63
Loss: 0.17125873643341025
ROC train: 0.998836	val: 0.636630	test: 0.849244
PRC train: 0.998243	val: 0.918554	test: 0.849290

Epoch: 64
Loss: 0.15887344012245203
ROC train: 0.997757	val: 0.650549	test: 0.839854
PRC train: 0.996614	val: 0.923792	test: 0.837888

Epoch: 65
Loss: 0.15315183977013422
ROC train: 0.999678	val: 0.633333	test: 0.821944
PRC train: 0.999514	val: 0.919529	test: 0.819240

Epoch: 66
Loss: 0.14850504720850832
ROC train: 0.999404	val: 0.619414	test: 0.805773
PRC train: 0.999100	val: 0.915801	test: 0.795674

Epoch: 67
Loss: 0.16820070783430668
ROC train: 0.998761	val: 0.630037	test: 0.808555
PRC train: 0.998097	val: 0.918126	test: 0.795277

Epoch: 68
Loss: 0.1514898433706499
ROC train: 0.999081	val: 0.669963	test: 0.818640
PRC train: 0.998633	val: 0.930682	test: 0.812636

Epoch: 69
Loss: 0.13276538059295878
ROC train: 0.999181	val: 0.617216	test: 0.815858
PRC train: 0.998789	val: 0.917481	test: 0.809793

Epoch: 70
Loss: 0.1457952558923532
ROC train: 0.999315	val: 0.627839	test: 0.817771
PRC train: 0.998985	val: 0.918924	test: 0.810496

Epoch: 71
Loss: 0.14587034547020356
ROC train: 0.999295	val: 0.628571	test: 0.829943
PRC train: 0.998916	val: 0.916079	test: 0.819907

Epoch: 72
Loss: 0.13047668413415892
ROC train: 0.999241	val: 0.640293	test: 0.815684
PRC train: 0.998860	val: 0.920338	test: 0.803533

Epoch: 73
Loss: 0.13104621736096164
ROC train: 0.999469	val: 0.642125	test: 0.823335
PRC train: 0.999193	val: 0.921277	test: 0.812255

Epoch: 74
Loss: 0.13631122704006934
ROC train: 0.999852	val: 0.640293	test: 0.829421
PRC train: 0.999778	val: 0.922779	test: 0.821878

Epoch: 75
Loss: 0.1382408419324194
ROC train: 0.999912	val: 0.631136	test: 0.829247
PRC train: 0.999867	val: 0.922557	test: 0.829601

Epoch: 76
Loss: 0.1338618096888962
ROC train: 0.999954	val: 0.642857	test: 0.817423
PRC train: 0.999931	val: 0.924219	test: 0.816243

Epoch: 77
Loss: 0.1030673388037338
ROC train: 0.999795	val: 0.665568	test: 0.807512
PRC train: 0.999684	val: 0.930308	test: 0.795990

Epoch: 78
Loss: 0.10688499925817171
ROC train: 0.999983	val: 0.664103	test: 0.820205
PRC train: 0.999974	val: 0.930765	test: 0.806441

Epoch: 79
Loss: 0.116835428559854
ROC train: 0.999934	val: 0.632967	test: 0.825248
PRC train: 0.999901	val: 0.922809	test: 0.809699

Epoch: 80
Loss: 0.10661927919110295
ROC train: 0.999740	val: 0.609158	test: 0.815336
PRC train: 0.999608	val: 0.913454	test: 0.795494

Epoch: 81
Loss: 0.13202810436977053
ROC train: 0.999523	val: 0.665934	test: 0.813076
PRC train: 0.999291	val: 0.926283	test: 0.801522

Epoch: 82
Loss: 0.10647738439637429
ROC train: 0.999872	val: 0.630403	test: 0.831681
PRC train: 0.999802	val: 0.918560	test: 0.833231

Epoch: 83
Loss: 0.10096999812794209
ROC train: 0.999934	val: 0.631136	test: 0.813772
PRC train: 0.999899	val: 0.915432	test: 0.798298

Epoch: 84
Loss: 0.1095078683505597
ROC train: 0.999977	val: 0.608059	test: 0.813424
PRC train: 0.999966	val: 0.911816	test: 0.806372

Epoch: 85
Loss: 0.10299979650733726
ROC train: 1.000000	val: 0.594139	test: 0.822118
PRC train: 1.000000	val: 0.909724	test: 0.826478

Epoch: 86
Loss: 0.11227352042353667
ROC train: 0.999966	val: 0.636996	test: 0.819857
PRC train: 0.999949	val: 0.923526	test: 0.824989

Epoch: 87
Loss: 0.11064385740392395
ROC train: 0.999937	val: 0.620513	test: 0.813250
PRC train: 0.999905	val: 0.917799	test: 0.810386

Epoch: 88
Loss: 0.131498169948603
ROC train: 0.999997	val: 0.606960	test: 0.821770
PRC train: 0.999996	val: 0.914333	test: 0.819291

Epoch: 89
Loss: 0.11020574321548418
ROC train: 0.999735	val: 0.652381	test: 0.786994
PRC train: 0.999609	val: 0.925020	test: 0.781811

Epoch: 90
Loss: 0.08857797082508603
ROC train: 0.999966	val: 0.637729	test: 0.814815
PRC train: 0.999948	val: 0.918388	test: 0.808564

Epoch: 91
Loss: 0.08879262418111196
ROC train: 0.999994	val: 0.643590	test: 0.811337
PRC train: 0.999991	val: 0.921344	test: 0.802255

Epoch: 92
Loss: 0.09980334724266446
ROC train: 0.999980	val: 0.657875	test: 0.807164
PRC train: 0.999970	val: 0.926508	test: 0.793261

Epoch: 93
Loss: 0.10845511096820552
ROC train: 0.999954	val: 0.677289	test: 0.836898
PRC train: 0.972211	val: 0.941286	test: 0.768042

Epoch: 33
Loss: 0.28214829646047096
ROC train: 0.979723	val: 0.705495	test: 0.785081
PRC train: 0.970195	val: 0.943613	test: 0.775068

Epoch: 34
Loss: 0.2589543663252123
ROC train: 0.988045	val: 0.701465	test: 0.796209
PRC train: 0.981855	val: 0.940337	test: 0.779647

Epoch: 35
Loss: 0.2532761529414645
ROC train: 0.988450	val: 0.671429	test: 0.807860
PRC train: 0.984035	val: 0.936023	test: 0.792678

Epoch: 36
Loss: 0.2633990429058918
ROC train: 0.991441	val: 0.664469	test: 0.799513
PRC train: 0.987851	val: 0.934432	test: 0.787082

Epoch: 37
Loss: 0.24539760564190724
ROC train: 0.989443	val: 0.655311	test: 0.809772
PRC train: 0.985363	val: 0.933537	test: 0.790125

Epoch: 38
Loss: 0.2415152244503492
ROC train: 0.993596	val: 0.690110	test: 0.801600
PRC train: 0.991059	val: 0.940822	test: 0.782001

Epoch: 39
Loss: 0.22638753440519044
ROC train: 0.993853	val: 0.690476	test: 0.809772
PRC train: 0.991213	val: 0.941022	test: 0.788871

Epoch: 40
Loss: 0.24167885478213683
ROC train: 0.993856	val: 0.698901	test: 0.781082
PRC train: 0.991180	val: 0.942352	test: 0.750124

Epoch: 41
Loss: 0.20886055684641436
ROC train: 0.996201	val: 0.678022	test: 0.791341
PRC train: 0.994405	val: 0.937780	test: 0.750129

Epoch: 42
Loss: 0.20877738407974925
ROC train: 0.996164	val: 0.689377	test: 0.778821
PRC train: 0.994616	val: 0.938240	test: 0.727808

Epoch: 43
Loss: 0.23258987846784512
ROC train: 0.998279	val: 0.673993	test: 0.806295
PRC train: 0.997497	val: 0.934441	test: 0.789042

Epoch: 44
Loss: 0.20789613695089226
ROC train: 0.997954	val: 0.680952	test: 0.801774
PRC train: 0.996858	val: 0.935383	test: 0.787544

Epoch: 45
Loss: 0.18204410246743447
ROC train: 0.996892	val: 0.695971	test: 0.796557
PRC train: 0.995168	val: 0.939441	test: 0.782630

Epoch: 46
Loss: 0.21340908550534649
ROC train: 0.998279	val: 0.687179	test: 0.795340
PRC train: 0.997455	val: 0.938763	test: 0.774149

Epoch: 47
Loss: 0.1979007928394952
ROC train: 0.997686	val: 0.684982	test: 0.802643
PRC train: 0.996595	val: 0.938889	test: 0.785371

Epoch: 48
Loss: 0.1760071277054172
ROC train: 0.998088	val: 0.702198	test: 0.817945
PRC train: 0.997187	val: 0.939595	test: 0.791019

Epoch: 49
Loss: 0.18951353970168153
ROC train: 0.998821	val: 0.679487	test: 0.816380
PRC train: 0.998243	val: 0.935543	test: 0.801229

Epoch: 50
Loss: 0.17911660157784012
ROC train: 0.998328	val: 0.676923	test: 0.795862
PRC train: 0.997484	val: 0.934385	test: 0.791638

Epoch: 51
Loss: 0.1954099138513918
ROC train: 0.999372	val: 0.650549	test: 0.811685
PRC train: 0.999080	val: 0.929591	test: 0.802810

Epoch: 52
Loss: 0.17398800898306419
ROC train: 0.999552	val: 0.661172	test: 0.807338
PRC train: 0.999327	val: 0.932917	test: 0.788465

Epoch: 53
Loss: 0.1498754980008775
ROC train: 0.999215	val: 0.655678	test: 0.794471
PRC train: 0.998818	val: 0.932228	test: 0.778450

Epoch: 54
Loss: 0.158811257718194
ROC train: 0.999555	val: 0.641026	test: 0.785776
PRC train: 0.999324	val: 0.927418	test: 0.772282

Epoch: 55
Loss: 0.1646167473624681
ROC train: 0.999780	val: 0.655311	test: 0.798122
PRC train: 0.999675	val: 0.928914	test: 0.789095

Epoch: 56
Loss: 0.16215769014022222
ROC train: 0.999269	val: 0.667033	test: 0.805425
PRC train: 0.998934	val: 0.933456	test: 0.790711

Epoch: 57
Loss: 0.15357037078657607
ROC train: 0.999007	val: 0.700733	test: 0.797948
PRC train: 0.998503	val: 0.942567	test: 0.764723

Epoch: 58
Loss: 0.1481376626524274
ROC train: 0.999515	val: 0.690110	test: 0.796731
PRC train: 0.999263	val: 0.940670	test: 0.798106

Epoch: 59
Loss: 0.1464646046040973
ROC train: 0.999874	val: 0.667766	test: 0.813945
PRC train: 0.999809	val: 0.935090	test: 0.793866

Epoch: 60
Loss: 0.1344896801002463
ROC train: 0.999926	val: 0.672161	test: 0.819510
PRC train: 0.999889	val: 0.933976	test: 0.798317

Epoch: 61
Loss: 0.12167605175884322
ROC train: 0.999615	val: 0.635897	test: 0.824378
PRC train: 0.999407	val: 0.924521	test: 0.807042

Epoch: 62
Loss: 0.12751074788916078
ROC train: 0.999957	val: 0.655678	test: 0.816728
PRC train: 0.999935	val: 0.928304	test: 0.800518

Epoch: 63
Loss: 0.13199535213076266
ROC train: 0.999883	val: 0.675458	test: 0.810294
PRC train: 0.999825	val: 0.934103	test: 0.782746

Epoch: 64
Loss: 0.13693141099375522
ROC train: 0.999814	val: 0.681685	test: 0.812728
PRC train: 0.999720	val: 0.935912	test: 0.793977

Epoch: 65
Loss: 0.150465591033502
ROC train: 0.999977	val: 0.667399	test: 0.791341
PRC train: 0.999965	val: 0.931309	test: 0.774123

Epoch: 66
Loss: 0.1357883905679538
ROC train: 0.999589	val: 0.682784	test: 0.790993
PRC train: 0.999376	val: 0.932768	test: 0.774029

Epoch: 67
Loss: 0.14878437020950008
ROC train: 0.999872	val: 0.662637	test: 0.811163
PRC train: 0.999804	val: 0.928875	test: 0.792492

Epoch: 68
Loss: 0.11178851138936616
ROC train: 0.999897	val: 0.645421	test: 0.808555
PRC train: 0.999842	val: 0.928673	test: 0.799207

Epoch: 69
Loss: 0.10969256420010036
ROC train: 0.999971	val: 0.657143	test: 0.811859
PRC train: 0.999957	val: 0.930416	test: 0.796089

Epoch: 70
Loss: 0.11586726444561676
ROC train: 0.999900	val: 0.635531	test: 0.826639
PRC train: 0.999844	val: 0.922764	test: 0.810520

Epoch: 71
Loss: 0.11207364627756781
ROC train: 0.999960	val: 0.650183	test: 0.823857
PRC train: 0.999939	val: 0.929915	test: 0.794822

Epoch: 72
Loss: 0.10427043209935605
ROC train: 0.999960	val: 0.668864	test: 0.800556
PRC train: 0.999940	val: 0.934985	test: 0.767391

Epoch: 73
Loss: 0.10950673566869877
ROC train: 0.999672	val: 0.625275	test: 0.797079
PRC train: 0.999446	val: 0.921323	test: 0.765839

Epoch: 74
Loss: 0.09005702323460475
ROC train: 0.999991	val: 0.673993	test: 0.806121
PRC train: 0.999987	val: 0.930694	test: 0.782944

Epoch: 75
Loss: 0.1065743689814788
ROC train: 0.999969	val: 0.680952	test: 0.812902
PRC train: 0.999952	val: 0.935594	test: 0.791256

Epoch: 76
Loss: 0.11291198479100728
ROC train: 0.999986	val: 0.659341	test: 0.819336
PRC train: 0.999978	val: 0.932380	test: 0.785199

Epoch: 77
Loss: 0.09505182446552644
ROC train: 1.000000	val: 0.686813	test: 0.809598
PRC train: 1.000000	val: 0.938070	test: 0.775890

Epoch: 78
Loss: 0.10420348299083071
ROC train: 0.999929	val: 0.648718	test: 0.806295
PRC train: 0.999892	val: 0.932143	test: 0.794023

Epoch: 79
Loss: 0.10906197743394404
ROC train: 0.999932	val: 0.639194	test: 0.811337
PRC train: 0.999896	val: 0.928138	test: 0.798944

Epoch: 80
Loss: 0.11488996978617246
ROC train: 0.999963	val: 0.661538	test: 0.818814
PRC train: 0.999944	val: 0.932110	test: 0.785884

Epoch: 81
Loss: 0.0943618463321646
ROC train: 0.999980	val: 0.657143	test: 0.816901
PRC train: 0.999970	val: 0.930066	test: 0.792120

Epoch: 82
Loss: 0.10013711173663382
ROC train: 0.999957	val: 0.652381	test: 0.814989
PRC train: 0.999936	val: 0.926289	test: 0.791372

Epoch: 83
Loss: 0.0790606723211998
ROC train: 0.999986	val: 0.682784	test: 0.804034
PRC train: 0.999979	val: 0.932463	test: 0.782780

Epoch: 84
Loss: 0.07653904854709023
ROC train: 1.000000	val: 0.693040	test: 0.803686
PRC train: 1.000000	val: 0.937656	test: 0.780431

Epoch: 85
Loss: 0.09429034473880848
ROC train: 1.000000	val: 0.680586	test: 0.805077
PRC train: 1.000000	val: 0.936187	test: 0.781099

Epoch: 86
Loss: 0.07964110245870773
ROC train: 0.999954	val: 0.642491	test: 0.808207
PRC train: 0.999932	val: 0.926871	test: 0.793566

Epoch: 87
Loss: 0.08599173800489808
ROC train: 0.999997	val: 0.656044	test: 0.814293
PRC train: 0.999996	val: 0.930407	test: 0.799044

Epoch: 88
Loss: 0.08619505932194663
ROC train: 1.000000	val: 0.670330	test: 0.815163
PRC train: 1.000000	val: 0.932235	test: 0.801509

Epoch: 89
Loss: 0.08479913747100624
ROC train: 0.999994	val: 0.672894	test: 0.810294
PRC train: 0.999991	val: 0.935253	test: 0.796344

Epoch: 90
Loss: 0.09687583935430584
ROC train: 0.999957	val: 0.713553	test: 0.789950
PRC train: 0.999935	val: 0.939984	test: 0.767743

Epoch: 91
Loss: 0.08280934322361125
ROC train: 1.000000	val: 0.700733	test: 0.805077
PRC train: 1.000000	val: 0.936890	test: 0.786673

Epoch: 92
Loss: 0.08700216023813043
ROC train: 1.000000	val: 0.699634	test: 0.820205
PRC train: 1.000000	val: 0.939779	test: 0.806218

Epoch: 93
Loss: 0.057571187501825584
ROC train: 1.000000	val: 0.701099	test: 0.820031
PRC train: 0.988921	val: 0.927537	test: 0.725203

Epoch: 33
Loss: 0.27510498491708246
ROC train: 0.996147	val: 0.669231	test: 0.757781
PRC train: 0.994276	val: 0.933805	test: 0.720003

Epoch: 34
Loss: 0.2471752357803493
ROC train: 0.996433	val: 0.651648	test: 0.764215
PRC train: 0.994252	val: 0.929651	test: 0.714522

Epoch: 35
Loss: 0.25943804068118526
ROC train: 0.996787	val: 0.655678	test: 0.761781
PRC train: 0.994956	val: 0.930276	test: 0.701083

Epoch: 36
Loss: 0.22861539853461244
ROC train: 0.996975	val: 0.672527	test: 0.762302
PRC train: 0.995539	val: 0.933313	test: 0.714294

Epoch: 37
Loss: 0.23952613802098094
ROC train: 0.998088	val: 0.645788	test: 0.773778
PRC train: 0.997304	val: 0.923733	test: 0.749376

Epoch: 38
Loss: 0.23606127191014062
ROC train: 0.997406	val: 0.646886	test: 0.773778
PRC train: 0.996247	val: 0.920827	test: 0.752698

Epoch: 39
Loss: 0.2395923343546646
ROC train: 0.998136	val: 0.654212	test: 0.766649
PRC train: 0.997210	val: 0.920883	test: 0.720837

Epoch: 40
Loss: 0.23428230375422804
ROC train: 0.999007	val: 0.606593	test: 0.777082
PRC train: 0.998512	val: 0.910036	test: 0.735515

Epoch: 41
Loss: 0.21339577476755803
ROC train: 0.998958	val: 0.628938	test: 0.774300
PRC train: 0.998360	val: 0.921854	test: 0.722910

Epoch: 42
Loss: 0.21682966002615825
ROC train: 0.999609	val: 0.639927	test: 0.780908
PRC train: 0.999393	val: 0.923848	test: 0.750925

Epoch: 43
Loss: 0.21783377773885473
ROC train: 0.999247	val: 0.628571	test: 0.784038
PRC train: 0.998911	val: 0.917875	test: 0.761242

Epoch: 44
Loss: 0.17162250930194972
ROC train: 0.999503	val: 0.624176	test: 0.783168
PRC train: 0.999234	val: 0.917155	test: 0.751764

Epoch: 45
Loss: 0.17024126539338813
ROC train: 0.999823	val: 0.632234	test: 0.783168
PRC train: 0.999733	val: 0.919744	test: 0.749279

Epoch: 46
Loss: 0.176392461310236
ROC train: 0.999623	val: 0.635165	test: 0.781255
PRC train: 0.999419	val: 0.923716	test: 0.735228

Epoch: 47
Loss: 0.17291479745732408
ROC train: 0.999777	val: 0.632234	test: 0.778126
PRC train: 0.999664	val: 0.924586	test: 0.709276

Epoch: 48
Loss: 0.18128263982287945
ROC train: 0.999852	val: 0.611355	test: 0.784733
PRC train: 0.999768	val: 0.910539	test: 0.739312

Epoch: 49
Loss: 0.16648136819048354
ROC train: 0.999629	val: 0.602198	test: 0.780038
PRC train: 0.999444	val: 0.906637	test: 0.761672

Epoch: 50
Loss: 0.14687652331715498
ROC train: 0.999763	val: 0.607692	test: 0.781777
PRC train: 0.999643	val: 0.910405	test: 0.770358

Epoch: 51
Loss: 0.16084314231289798
ROC train: 0.999877	val: 0.613553	test: 0.788906
PRC train: 0.999811	val: 0.915782	test: 0.767964

Epoch: 52
Loss: 0.15392111427879912
ROC train: 0.999943	val: 0.612088	test: 0.778995
PRC train: 0.999913	val: 0.914478	test: 0.750269

Epoch: 53
Loss: 0.12918633753568826
ROC train: 0.999946	val: 0.617582	test: 0.769953
PRC train: 0.999917	val: 0.917051	test: 0.750669

Epoch: 54
Loss: 0.162866221056763
ROC train: 0.999903	val: 0.621978	test: 0.780386
PRC train: 0.999856	val: 0.917556	test: 0.773760

Epoch: 55
Loss: 0.13516912745733978
ROC train: 0.999917	val: 0.623810	test: 0.772909
PRC train: 0.999873	val: 0.917510	test: 0.734955

Epoch: 56
Loss: 0.10150268237402597
ROC train: 0.999977	val: 0.626007	test: 0.771692
PRC train: 0.999965	val: 0.919351	test: 0.737357

Epoch: 57
Loss: 0.11353984123642762
ROC train: 0.999977	val: 0.654945	test: 0.773778
PRC train: 0.999965	val: 0.924743	test: 0.749844

Epoch: 58
Loss: 0.11868645815599362
ROC train: 0.999997	val: 0.643956	test: 0.787689
PRC train: 0.999996	val: 0.918301	test: 0.761855

Epoch: 59
Loss: 0.1292792390691858
ROC train: 0.999971	val: 0.586447	test: 0.769431
PRC train: 0.999957	val: 0.898442	test: 0.765015

Epoch: 60
Loss: 0.13043198603857878
ROC train: 0.999997	val: 0.587179	test: 0.775691
PRC train: 0.999996	val: 0.906428	test: 0.739627

Epoch: 61
Loss: 0.10319796883989721
ROC train: 1.000000	val: 0.591209	test: 0.782820
PRC train: 1.000000	val: 0.902253	test: 0.743931

Epoch: 62
Loss: 0.12938424233014872
ROC train: 1.000000	val: 0.638462	test: 0.795514
PRC train: 1.000000	val: 0.917946	test: 0.774021

Epoch: 63
Loss: 0.11628656656437944
ROC train: 0.999969	val: 0.632601	test: 0.781429
PRC train: 0.999952	val: 0.917199	test: 0.772641

Epoch: 64
Loss: 0.10939446710122758
ROC train: 0.999866	val: 0.591209	test: 0.786472
PRC train: 0.999788	val: 0.906942	test: 0.771797

Epoch: 65
Loss: 0.14160513716199832
ROC train: 0.999986	val: 0.600000	test: 0.779169
PRC train: 0.999978	val: 0.912644	test: 0.749420

Epoch: 66
Loss: 0.09875895718075857
ROC train: 1.000000	val: 0.594505	test: 0.785776
PRC train: 1.000000	val: 0.907426	test: 0.782595

Epoch: 67
Loss: 0.10810932800159034
ROC train: 0.999997	val: 0.600000	test: 0.793949
PRC train: 0.999996	val: 0.900092	test: 0.779779

Epoch: 68
Loss: 0.11528679332171213
ROC train: 0.999994	val: 0.625275	test: 0.800209
PRC train: 0.999991	val: 0.916147	test: 0.760312

Epoch: 69
Loss: 0.09807269516380879
ROC train: 0.999974	val: 0.586447	test: 0.789602
PRC train: 0.999961	val: 0.906246	test: 0.747177

Epoch: 70
Loss: 0.11201330757545644
ROC train: 0.999954	val: 0.597070	test: 0.774996
PRC train: 0.999931	val: 0.908113	test: 0.734720

Epoch: 71
Loss: 0.115600137539655
ROC train: 1.000000	val: 0.612088	test: 0.791167
PRC train: 1.000000	val: 0.912672	test: 0.727070

Epoch: 72
Loss: 0.10503434244271088
ROC train: 0.999986	val: 0.648352	test: 0.774474
PRC train: 0.999978	val: 0.924607	test: 0.743532

Epoch: 73
Loss: 0.11311435293845007
ROC train: 1.000000	val: 0.640659	test: 0.765954
PRC train: 1.000000	val: 0.922213	test: 0.751094

Epoch: 74
Loss: 0.094628807741173
ROC train: 0.999997	val: 0.614652	test: 0.759520
PRC train: 0.999996	val: 0.906272	test: 0.750220

Epoch: 75
Loss: 0.10599773025362194
ROC train: 1.000000	val: 0.600366	test: 0.768736
PRC train: 1.000000	val: 0.907543	test: 0.753189

Epoch: 76
Loss: 0.08640711864559791
ROC train: 1.000000	val: 0.594872	test: 0.767345
PRC train: 1.000000	val: 0.907524	test: 0.725024

Epoch: 77
Loss: 0.08832507689668825
ROC train: 1.000000	val: 0.572161	test: 0.768214
PRC train: 1.000000	val: 0.897982	test: 0.732286

Epoch: 78
Loss: 0.08145806916528071
ROC train: 1.000000	val: 0.567766	test: 0.764041
PRC train: 1.000000	val: 0.884767	test: 0.736323

Epoch: 79
Loss: 0.09959620356894613
ROC train: 1.000000	val: 0.613553	test: 0.786646
PRC train: 1.000000	val: 0.910835	test: 0.750278

Epoch: 80
Loss: 0.08407273300550325
ROC train: 1.000000	val: 0.647985	test: 0.795340
PRC train: 1.000000	val: 0.921775	test: 0.743263

Epoch: 81
Loss: 0.08620945721440244
ROC train: 1.000000	val: 0.665934	test: 0.790123
PRC train: 1.000000	val: 0.930775	test: 0.801545

Epoch: 82
Loss: 0.0895033365045731
ROC train: 1.000000	val: 0.635165	test: 0.781777
PRC train: 1.000000	val: 0.919218	test: 0.764595

Epoch: 83
Loss: 0.08371970221100979
ROC train: 0.999997	val: 0.609158	test: 0.778995
PRC train: 0.999996	val: 0.906606	test: 0.763983

Epoch: 84
Loss: 0.06715328620897584
ROC train: 1.000000	val: 0.605495	test: 0.798122
PRC train: 1.000000	val: 0.913072	test: 0.768200

Epoch: 85
Loss: 0.0729184507250604
ROC train: 0.999997	val: 0.628571	test: 0.801947
PRC train: 0.999996	val: 0.923448	test: 0.750454

Epoch: 86
Loss: 0.07316110175569221
ROC train: 1.000000	val: 0.615385	test: 0.801252
PRC train: 1.000000	val: 0.914204	test: 0.746294

Epoch: 87
Loss: 0.06324280978895273
ROC train: 1.000000	val: 0.603663	test: 0.803165
PRC train: 1.000000	val: 0.904458	test: 0.747475

Epoch: 88
Loss: 0.06567357242198524
ROC train: 1.000000	val: 0.604762	test: 0.807860
PRC train: 1.000000	val: 0.907447	test: 0.763090

Epoch: 89
Loss: 0.05333394853757949
ROC train: 1.000000	val: 0.609890	test: 0.797079
PRC train: 1.000000	val: 0.914747	test: 0.739660

Epoch: 90
Loss: 0.0656163410858519
ROC train: 1.000000	val: 0.613187	test: 0.794471
PRC train: 1.000000	val: 0.916351	test: 0.796744

Epoch: 91
Loss: 0.05050224690945436
ROC train: 1.000000	val: 0.603663	test: 0.785081
PRC train: 1.000000	val: 0.901134	test: 0.781223

Epoch: 92
Loss: 0.06777795040449458
ROC train: 0.999994	val: 0.612454	test: 0.797253
PRC train: 0.999991	val: 0.909065	test: 0.740542

Epoch: 93
Loss: 0.05838954965944593
ROC train: 0.999997	val: 0.609158	test: 0.804730
PRC train: 0.973129	val: 0.916164	test: 0.716192

Epoch: 33
Loss: 0.29604040644846896
ROC train: 0.984372	val: 0.693773	test: 0.772387
PRC train: 0.975269	val: 0.917059	test: 0.735248

Epoch: 34
Loss: 0.2820949696959817
ROC train: 0.986541	val: 0.684615	test: 0.784907
PRC train: 0.979487	val: 0.914783	test: 0.751652

Epoch: 35
Loss: 0.2779750398229942
ROC train: 0.985380	val: 0.652015	test: 0.800730
PRC train: 0.977328	val: 0.910327	test: 0.780351

Epoch: 36
Loss: 0.2730854657351859
ROC train: 0.987626	val: 0.670330	test: 0.795688
PRC train: 0.980057	val: 0.911441	test: 0.766136

Epoch: 37
Loss: 0.25814667160856813
ROC train: 0.990000	val: 0.709890	test: 0.774996
PRC train: 0.983685	val: 0.920621	test: 0.749775

Epoch: 38
Loss: 0.25524272235348244
ROC train: 0.992303	val: 0.709524	test: 0.767171
PRC train: 0.987814	val: 0.912363	test: 0.720948

Epoch: 39
Loss: 0.24415129918857112
ROC train: 0.992443	val: 0.683150	test: 0.773605
PRC train: 0.987786	val: 0.902452	test: 0.727815

Epoch: 40
Loss: 0.24568358469536097
ROC train: 0.993707	val: 0.654579	test: 0.779343
PRC train: 0.990347	val: 0.906221	test: 0.753203

Epoch: 41
Loss: 0.24109589076282784
ROC train: 0.992306	val: 0.653846	test: 0.776561
PRC train: 0.987928	val: 0.902096	test: 0.756534

Epoch: 42
Loss: 0.2548844209442754
ROC train: 0.992295	val: 0.694505	test: 0.760389
PRC train: 0.987690	val: 0.917652	test: 0.735910

Epoch: 43
Loss: 0.24575154453084855
ROC train: 0.992982	val: 0.686081	test: 0.758651
PRC train: 0.988724	val: 0.918711	test: 0.734726

Epoch: 44
Loss: 0.21256948037566858
ROC train: 0.994175	val: 0.654945	test: 0.773605
PRC train: 0.990417	val: 0.909413	test: 0.747123

Epoch: 45
Loss: 0.22674166075351385
ROC train: 0.996256	val: 0.679487	test: 0.773431
PRC train: 0.993772	val: 0.908211	test: 0.742061

Epoch: 46
Loss: 0.23767490011904605
ROC train: 0.995965	val: 0.649451	test: 0.783516
PRC train: 0.993607	val: 0.904204	test: 0.765060

Epoch: 47
Loss: 0.21080931977616615
ROC train: 0.997517	val: 0.676557	test: 0.790645
PRC train: 0.996134	val: 0.904755	test: 0.746561

Epoch: 48
Loss: 0.2145518318727348
ROC train: 0.997449	val: 0.654212	test: 0.775865
PRC train: 0.996203	val: 0.897312	test: 0.747389

Epoch: 49
Loss: 0.19500111919163612
ROC train: 0.998408	val: 0.667399	test: 0.781429
PRC train: 0.997591	val: 0.902842	test: 0.732192

Epoch: 50
Loss: 0.18651447655146453
ROC train: 0.998656	val: 0.671795	test: 0.784907
PRC train: 0.997958	val: 0.909549	test: 0.740824

Epoch: 51
Loss: 0.1869174148026521
ROC train: 0.998019	val: 0.667399	test: 0.781255
PRC train: 0.996943	val: 0.919476	test: 0.786775

Epoch: 52
Loss: 0.1761053990385315
ROC train: 0.998730	val: 0.687546	test: 0.779343
PRC train: 0.998121	val: 0.916316	test: 0.752168

Epoch: 53
Loss: 0.1899984329570928
ROC train: 0.997988	val: 0.673626	test: 0.766997
PRC train: 0.996709	val: 0.912154	test: 0.743140

Epoch: 54
Loss: 0.16941804274121824
ROC train: 0.998861	val: 0.705495	test: 0.769953
PRC train: 0.998312	val: 0.914427	test: 0.719784

Epoch: 55
Loss: 0.1593729800952326
ROC train: 0.998616	val: 0.667399	test: 0.787515
PRC train: 0.997958	val: 0.909872	test: 0.784553

Epoch: 56
Loss: 0.164493344446424
ROC train: 0.994991	val: 0.693773	test: 0.775691
PRC train: 0.992026	val: 0.911984	test: 0.755031

Epoch: 57
Loss: 0.1634285667251309
ROC train: 0.997252	val: 0.702198	test: 0.773083
PRC train: 0.995744	val: 0.917609	test: 0.746729

Epoch: 58
Loss: 0.1586190746622338
ROC train: 0.998242	val: 0.653114	test: 0.770996
PRC train: 0.997008	val: 0.911831	test: 0.771620

Epoch: 59
Loss: 0.1472329526523741
ROC train: 0.999561	val: 0.672161	test: 0.780734
PRC train: 0.999323	val: 0.912784	test: 0.748004

Epoch: 60
Loss: 0.16334655252206026
ROC train: 0.999312	val: 0.667399	test: 0.798470
PRC train: 0.998933	val: 0.912126	test: 0.801738

Epoch: 61
Loss: 0.1514525752904952
ROC train: 0.999820	val: 0.688278	test: 0.792036
PRC train: 0.999726	val: 0.906253	test: 0.741591

Epoch: 62
Loss: 0.14538132929631115
ROC train: 0.999912	val: 0.677289	test: 0.782820
PRC train: 0.999867	val: 0.902445	test: 0.729628

Epoch: 63
Loss: 0.1494785159618445
ROC train: 0.999786	val: 0.672894	test: 0.766823
PRC train: 0.999685	val: 0.902751	test: 0.721121

Epoch: 64
Loss: 0.16233129495447762
ROC train: 0.999860	val: 0.675458	test: 0.763867
PRC train: 0.999788	val: 0.912269	test: 0.730833

Epoch: 65
Loss: 0.1422743637272963
ROC train: 0.999760	val: 0.675092	test: 0.762998
PRC train: 0.999625	val: 0.905929	test: 0.724085

Epoch: 66
Loss: 0.13578333075689758
ROC train: 0.999803	val: 0.669231	test: 0.773083
PRC train: 0.999702	val: 0.901100	test: 0.732277

Epoch: 67
Loss: 0.13643092772618454
ROC train: 0.999469	val: 0.664835	test: 0.771692
PRC train: 0.999180	val: 0.907542	test: 0.734114

Epoch: 68
Loss: 0.12348019165269375
ROC train: 0.999886	val: 0.653846	test: 0.786298
PRC train: 0.999825	val: 0.905703	test: 0.744098

Epoch: 69
Loss: 0.12089383294794151
ROC train: 0.999934	val: 0.642491	test: 0.787863
PRC train: 0.999900	val: 0.902718	test: 0.751433

Epoch: 70
Loss: 0.14444546086021523
ROC train: 0.999660	val: 0.648718	test: 0.783864
PRC train: 0.999493	val: 0.904881	test: 0.778787

Epoch: 71
Loss: 0.13286437715216717
ROC train: 0.999954	val: 0.635897	test: 0.789950
PRC train: 0.999931	val: 0.898759	test: 0.749560

Epoch: 72
Loss: 0.12993594271383407
ROC train: 0.999980	val: 0.652747	test: 0.780560
PRC train: 0.999970	val: 0.905668	test: 0.756637

Epoch: 73
Loss: 0.14345616487475776
ROC train: 0.999806	val: 0.689011	test: 0.770475
PRC train: 0.999706	val: 0.912160	test: 0.733238

Epoch: 74
Loss: 0.1240764929818577
ROC train: 0.998459	val: 0.676923	test: 0.756042
PRC train: 0.997598	val: 0.913522	test: 0.723488

Epoch: 75
Loss: 0.14870940545809094
ROC train: 0.999566	val: 0.689377	test: 0.770127
PRC train: 0.999349	val: 0.920684	test: 0.732005

Epoch: 76
Loss: 0.11701703805198763
ROC train: 0.999070	val: 0.656777	test: 0.750478
PRC train: 0.998652	val: 0.917276	test: 0.730835

Epoch: 77
Loss: 0.1303982866009916
ROC train: 0.999946	val: 0.661905	test: 0.776387
PRC train: 0.999920	val: 0.898845	test: 0.743126

Epoch: 78
Loss: 0.09800378141402141
ROC train: 1.000000	val: 0.655311	test: 0.779169
PRC train: 1.000000	val: 0.888217	test: 0.730765

Epoch: 79
Loss: 0.1093154584570158
ROC train: 1.000000	val: 0.653114	test: 0.776213
PRC train: 1.000000	val: 0.894240	test: 0.745245

Epoch: 80
Loss: 0.09030847787990318
ROC train: 0.999997	val: 0.683516	test: 0.773605
PRC train: 0.999996	val: 0.902506	test: 0.735008

Epoch: 81
Loss: 0.09201038623641629
ROC train: 1.000000	val: 0.684982	test: 0.773952
PRC train: 1.000000	val: 0.906761	test: 0.741515

Epoch: 82
Loss: 0.08586348173567804
ROC train: 1.000000	val: 0.650549	test: 0.783342
PRC train: 1.000000	val: 0.896708	test: 0.760767

Epoch: 83
Loss: 0.09809826604809777
ROC train: 0.999977	val: 0.654579	test: 0.781951
PRC train: 0.999965	val: 0.905950	test: 0.773153

Epoch: 84
Loss: 0.09839536941238687
ROC train: 1.000000	val: 0.667399	test: 0.777778
PRC train: 1.000000	val: 0.908898	test: 0.769055

Epoch: 85
Loss: 0.08606142687964254
ROC train: 1.000000	val: 0.693040	test: 0.774648
PRC train: 1.000000	val: 0.911472	test: 0.737474

Epoch: 86
Loss: 0.09180920971751086
ROC train: 0.999957	val: 0.680586	test: 0.768214
PRC train: 0.999935	val: 0.908104	test: 0.722053

Epoch: 87
Loss: 0.09902964575670349
ROC train: 0.999997	val: 0.660806	test: 0.767171
PRC train: 0.999996	val: 0.900502	test: 0.740019

Epoch: 88
Loss: 0.07969862184672163
ROC train: 1.000000	val: 0.650183	test: 0.775170
PRC train: 1.000000	val: 0.898795	test: 0.767779

Epoch: 89
Loss: 0.08001361548437257
ROC train: 1.000000	val: 0.673626	test: 0.778126
PRC train: 1.000000	val: 0.910070	test: 0.772070

Epoch: 90
Loss: 0.09974927027241273
ROC train: 1.000000	val: 0.664103	test: 0.779169
PRC train: 1.000000	val: 0.906071	test: 0.771946

Epoch: 91
Loss: 0.08160579272975567
ROC train: 0.999994	val: 0.653846	test: 0.781082
PRC train: 0.999991	val: 0.902130	test: 0.767263

Epoch: 92
Loss: 0.07010781927702733
ROC train: 0.999994	val: 0.665934	test: 0.779864
PRC train: 0.999991	val: 0.902580	test: 0.737703

Epoch: 93
Loss: 0.08650622549495204
ROC train: 0.999983	val: 0.660440	test: 0.779169
PRC train: 0.991845	val: 0.940038	test: 0.746476

Epoch: 33
Loss: 0.2658809716017144
ROC train: 0.992837	val: 0.705128	test: 0.762302
PRC train: 0.989123	val: 0.940189	test: 0.736116

Epoch: 34
Loss: 0.27891266223084055
ROC train: 0.992471	val: 0.702198	test: 0.751521
PRC train: 0.987879	val: 0.940419	test: 0.713157

Epoch: 35
Loss: 0.2690623708185121
ROC train: 0.992383	val: 0.697436	test: 0.768736
PRC train: 0.987027	val: 0.942597	test: 0.760750

Epoch: 36
Loss: 0.25197749823392057
ROC train: 0.995654	val: 0.675824	test: 0.771344
PRC train: 0.992544	val: 0.937226	test: 0.767652

Epoch: 37
Loss: 0.2673380552015292
ROC train: 0.997061	val: 0.661172	test: 0.747348
PRC train: 0.995487	val: 0.930764	test: 0.722980

Epoch: 38
Loss: 0.23773563509768728
ROC train: 0.995248	val: 0.697802	test: 0.785081
PRC train: 0.992202	val: 0.940040	test: 0.781006

Epoch: 39
Loss: 0.21767914759425144
ROC train: 0.995876	val: 0.711355	test: 0.775865
PRC train: 0.992857	val: 0.943799	test: 0.775900

Epoch: 40
Loss: 0.22941002272647207
ROC train: 0.998856	val: 0.720147	test: 0.771344
PRC train: 0.998299	val: 0.945915	test: 0.759495

Epoch: 41
Loss: 0.22317175889216453
ROC train: 0.998213	val: 0.649817	test: 0.741784
PRC train: 0.997295	val: 0.927777	test: 0.710826

Epoch: 42
Loss: 0.2360182189509718
ROC train: 0.999424	val: 0.684982	test: 0.757260
PRC train: 0.999147	val: 0.938096	test: 0.748931

Epoch: 43
Loss: 0.19903624021424857
ROC train: 0.998382	val: 0.683516	test: 0.759346
PRC train: 0.997756	val: 0.937641	test: 0.753322

Epoch: 44
Loss: 0.20238101295988317
ROC train: 0.999161	val: 0.700366	test: 0.761259
PRC train: 0.998815	val: 0.942487	test: 0.745547

Epoch: 45
Loss: 0.2020618137994045
ROC train: 0.999404	val: 0.715385	test: 0.776734
PRC train: 0.999066	val: 0.945924	test: 0.763389

Epoch: 46
Loss: 0.1837031165973788
ROC train: 0.999643	val: 0.730037	test: 0.771866
PRC train: 0.999454	val: 0.947100	test: 0.768188

Epoch: 47
Loss: 0.19798551618024582
ROC train: 0.999618	val: 0.735165	test: 0.760389
PRC train: 0.999441	val: 0.949365	test: 0.771192

Epoch: 48
Loss: 0.17459689003107826
ROC train: 0.999695	val: 0.707692	test: 0.746653
PRC train: 0.999530	val: 0.943428	test: 0.742765

Epoch: 49
Loss: 0.19246896469688934
ROC train: 0.999646	val: 0.704396	test: 0.742306
PRC train: 0.999461	val: 0.941721	test: 0.737763

Epoch: 50
Loss: 0.1775082153446306
ROC train: 0.999735	val: 0.686813	test: 0.740045
PRC train: 0.999601	val: 0.937809	test: 0.732266

Epoch: 51
Loss: 0.16494781231877326
ROC train: 0.999892	val: 0.655678	test: 0.728221
PRC train: 0.999839	val: 0.930028	test: 0.690873

Epoch: 52
Loss: 0.15876883369813996
ROC train: 0.999772	val: 0.687546	test: 0.745262
PRC train: 0.999663	val: 0.937354	test: 0.741542

Epoch: 53
Loss: 0.15409291212070012
ROC train: 0.999726	val: 0.692674	test: 0.758303
PRC train: 0.999582	val: 0.938184	test: 0.759820

Epoch: 54
Loss: 0.15981061335552138
ROC train: 0.999989	val: 0.701832	test: 0.747001
PRC train: 0.999983	val: 0.942907	test: 0.728063

Epoch: 55
Loss: 0.15052215428027976
ROC train: 0.999986	val: 0.710989	test: 0.757607
PRC train: 0.999978	val: 0.947509	test: 0.752717

Epoch: 56
Loss: 0.1546847110838732
ROC train: 0.999974	val: 0.734432	test: 0.764215
PRC train: 0.999961	val: 0.950964	test: 0.773624

Epoch: 57
Loss: 0.1491387471760744
ROC train: 1.000000	val: 0.691941	test: 0.736568
PRC train: 1.000000	val: 0.938517	test: 0.712879

Epoch: 58
Loss: 0.14240272866929732
ROC train: 0.999963	val: 0.699634	test: 0.753956
PRC train: 0.999944	val: 0.941473	test: 0.755544

Epoch: 59
Loss: 0.13907295788508178
ROC train: 1.000000	val: 0.697070	test: 0.755869
PRC train: 1.000000	val: 0.939824	test: 0.752204

Epoch: 60
Loss: 0.13128737668136461
ROC train: 1.000000	val: 0.702564	test: 0.738828
PRC train: 1.000000	val: 0.940880	test: 0.738020

Epoch: 61
Loss: 0.13824731490822917
ROC train: 0.999997	val: 0.708425	test: 0.757260
PRC train: 0.999996	val: 0.940764	test: 0.751412

Epoch: 62
Loss: 0.1300603594640904
ROC train: 0.999991	val: 0.716850	test: 0.760042
PRC train: 0.999987	val: 0.940713	test: 0.755686

Epoch: 63
Loss: 0.11617420088977752
ROC train: 1.000000	val: 0.715018	test: 0.744045
PRC train: 1.000000	val: 0.943081	test: 0.725036

Epoch: 64
Loss: 0.12949838803115915
ROC train: 0.999997	val: 0.721612	test: 0.731699
PRC train: 0.999996	val: 0.943707	test: 0.702801

Epoch: 65
Loss: 0.11517489913809682
ROC train: 0.999997	val: 0.723077	test: 0.756738
PRC train: 0.999996	val: 0.945400	test: 0.753305

Epoch: 66
Loss: 0.11601395880518417
ROC train: 1.000000	val: 0.701832	test: 0.745957
PRC train: 1.000000	val: 0.941361	test: 0.733310

Epoch: 67
Loss: 0.1252634318065736
ROC train: 0.999997	val: 0.689011	test: 0.736915
PRC train: 0.999996	val: 0.934693	test: 0.708682

Epoch: 68
Loss: 0.10313621100641635
ROC train: 1.000000	val: 0.691575	test: 0.724570
PRC train: 1.000000	val: 0.938554	test: 0.693516

Epoch: 69
Loss: 0.0938247236739739
ROC train: 0.999997	val: 0.695238	test: 0.730134
PRC train: 0.999996	val: 0.941086	test: 0.695698

Epoch: 70
Loss: 0.10810930586497561
ROC train: 0.999980	val: 0.704396	test: 0.747522
PRC train: 0.999970	val: 0.942728	test: 0.721902

Epoch: 71
Loss: 0.11800516860452814
ROC train: 1.000000	val: 0.711355	test: 0.734307
PRC train: 1.000000	val: 0.942658	test: 0.725175

Epoch: 72
Loss: 0.10209938548912068
ROC train: 0.999997	val: 0.689011	test: 0.727004
PRC train: 0.999996	val: 0.934648	test: 0.706631

Epoch: 73
Loss: 0.08993367126312032
ROC train: 0.999997	val: 0.689377	test: 0.749783
PRC train: 0.999996	val: 0.935353	test: 0.747881

Epoch: 74
Loss: 0.09014735161512902
ROC train: 0.999977	val: 0.709524	test: 0.759868
PRC train: 0.999966	val: 0.944040	test: 0.750538

Epoch: 75
Loss: 0.09853936779393227
ROC train: 1.000000	val: 0.726740	test: 0.769084
PRC train: 1.000000	val: 0.947394	test: 0.761508

Epoch: 76
Loss: 0.08478975714474632
ROC train: 1.000000	val: 0.719414	test: 0.772561
PRC train: 1.000000	val: 0.944237	test: 0.766282

Epoch: 77
Loss: 0.09590696307012682
ROC train: 1.000000	val: 0.704396	test: 0.746131
PRC train: 1.000000	val: 0.942349	test: 0.724970

Epoch: 78
Loss: 0.08879726964290194
ROC train: 1.000000	val: 0.714286	test: 0.767345
PRC train: 1.000000	val: 0.946154	test: 0.768809

Epoch: 79
Loss: 0.08951926089767946
ROC train: 1.000000	val: 0.720879	test: 0.781777
PRC train: 1.000000	val: 0.943818	test: 0.780818

Epoch: 80
Loss: 0.07793499610525553
ROC train: 0.999957	val: 0.685348	test: 0.775517
PRC train: 0.999935	val: 0.936255	test: 0.768737

Epoch: 81
Loss: 0.09237019470138774
ROC train: 1.000000	val: 0.716850	test: 0.746479
PRC train: 1.000000	val: 0.947806	test: 0.713585

Epoch: 82
Loss: 0.09182715703219302
ROC train: 1.000000	val: 0.726740	test: 0.760911
PRC train: 1.000000	val: 0.948534	test: 0.747552

Epoch: 83
Loss: 0.07558106368735817
ROC train: 1.000000	val: 0.729304	test: 0.783168
PRC train: 1.000000	val: 0.947078	test: 0.766656

Epoch: 84
Loss: 0.08030917917540917
ROC train: 1.000000	val: 0.726007	test: 0.775517
PRC train: 1.000000	val: 0.947306	test: 0.759042

Epoch: 85
Loss: 0.08459039835267197
ROC train: 1.000000	val: 0.705861	test: 0.732047
PRC train: 1.000000	val: 0.942482	test: 0.696414

Epoch: 86
Loss: 0.09122590800637895
ROC train: 1.000000	val: 0.728571	test: 0.762476
PRC train: 1.000000	val: 0.946439	test: 0.750613

Epoch: 87
Loss: 0.06054866989075856
ROC train: 1.000000	val: 0.703663	test: 0.776734
PRC train: 1.000000	val: 0.936701	test: 0.769150

Epoch: 88
Loss: 0.09834672447053991
ROC train: 1.000000	val: 0.710989	test: 0.790471
PRC train: 1.000000	val: 0.937917	test: 0.779859

Epoch: 89
Loss: 0.06609393602446514
ROC train: 1.000000	val: 0.718315	test: 0.784385
PRC train: 1.000000	val: 0.939190	test: 0.766012

Epoch: 90
Loss: 0.08554166291601936
ROC train: 1.000000	val: 0.722344	test: 0.800904
PRC train: 1.000000	val: 0.944279	test: 0.792486

Epoch: 91
Loss: 0.07634939336629867
ROC train: 1.000000	val: 0.724176	test: 0.793949
PRC train: 1.000000	val: 0.946761	test: 0.780055

Epoch: 92
Loss: 0.048088553423965355
ROC train: 0.999994	val: 0.728205	test: 0.775691
PRC train: 0.999991	val: 0.946608	test: 0.763198

Epoch: 93
Loss: 0.07767840840614963
ROC train: 1.000000	val: 0.723443	test: 0.781429
PRC train: 0.968016	val: 0.929185	test: 0.745054

Epoch: 95
Loss: 0.2371303690570262
ROC train: 0.978804	val: 0.641392	test: 0.746305
PRC train: 0.967185	val: 0.924675	test: 0.764895

Epoch: 96
Loss: 0.23060416469369488
ROC train: 0.982628	val: 0.653846	test: 0.741610
PRC train: 0.973897	val: 0.928647	test: 0.747958

Epoch: 97
Loss: 0.2580574211813135
ROC train: 0.981484	val: 0.663736	test: 0.729612
PRC train: 0.971482	val: 0.927082	test: 0.731691

Epoch: 98
Loss: 0.23551114432523312
ROC train: 0.980819	val: 0.653480	test: 0.740045
PRC train: 0.970922	val: 0.922501	test: 0.759225

Epoch: 99
Loss: 0.2412011533374369
ROC train: 0.981570	val: 0.661538	test: 0.736046
PRC train: 0.973219	val: 0.925430	test: 0.761527

Epoch: 100
Loss: 0.253651646478733
ROC train: 0.980888	val: 0.672894	test: 0.733612
PRC train: 0.970565	val: 0.926912	test: 0.762124

Epoch: 101
Loss: 0.22699942450880456
ROC train: 0.983964	val: 0.645055	test: 0.756738
PRC train: 0.975458	val: 0.923964	test: 0.780325

Epoch: 102
Loss: 0.22894590701104695
ROC train: 0.982631	val: 0.664835	test: 0.745957
PRC train: 0.974357	val: 0.929870	test: 0.764331

Epoch: 103
Loss: 0.2464914338228097
ROC train: 0.985431	val: 0.664103	test: 0.752391
PRC train: 0.978671	val: 0.931991	test: 0.767281

Epoch: 104
Loss: 0.2342970382623868
ROC train: 0.982646	val: 0.660073	test: 0.721788
PRC train: 0.974880	val: 0.931358	test: 0.740735

Epoch: 105
Loss: 0.23134969684571738
ROC train: 0.981099	val: 0.694872	test: 0.729960
PRC train: 0.970713	val: 0.938113	test: 0.739620

Epoch: 106
Loss: 0.21992583950877892
ROC train: 0.983844	val: 0.647619	test: 0.718310
PRC train: 0.976094	val: 0.926357	test: 0.738680

Epoch: 107
Loss: 0.23474910732431176
ROC train: 0.982777	val: 0.646154	test: 0.718484
PRC train: 0.974549	val: 0.920394	test: 0.750476

Epoch: 108
Loss: 0.21873341601912247
ROC train: 0.981866	val: 0.642857	test: 0.729264
PRC train: 0.973323	val: 0.921694	test: 0.751484

Epoch: 109
Loss: 0.2170768692272697
ROC train: 0.985257	val: 0.646520	test: 0.729438
PRC train: 0.977946	val: 0.920407	test: 0.756888

Epoch: 110
Loss: 0.21955273766587452
ROC train: 0.985845	val: 0.656044	test: 0.725265
PRC train: 0.978940	val: 0.921894	test: 0.750165

Epoch: 111
Loss: 0.2097970013348692
ROC train: 0.985865	val: 0.670696	test: 0.723526
PRC train: 0.979158	val: 0.927861	test: 0.750802

Epoch: 112
Loss: 0.21274238297932618
ROC train: 0.986421	val: 0.654945	test: 0.725265
PRC train: 0.980558	val: 0.925155	test: 0.754347

Epoch: 113
Loss: 0.21247054123596096
ROC train: 0.986515	val: 0.667033	test: 0.719005
PRC train: 0.980096	val: 0.928040	test: 0.748273

Epoch: 114
Loss: 0.20745112835767404
ROC train: 0.985731	val: 0.676190	test: 0.731699
PRC train: 0.978505	val: 0.930555	test: 0.755441

Epoch: 115
Loss: 0.2201108269588981
ROC train: 0.987314	val: 0.669231	test: 0.746131
PRC train: 0.981179	val: 0.928122	test: 0.768800

Epoch: 116
Loss: 0.21788900607663503
ROC train: 0.986983	val: 0.642491	test: 0.748044
PRC train: 0.980822	val: 0.920992	test: 0.767022

Epoch: 117
Loss: 0.2075594013177262
ROC train: 0.988753	val: 0.647253	test: 0.741262
PRC train: 0.983560	val: 0.921974	test: 0.755760

Epoch: 118
Loss: 0.21193598478586084
ROC train: 0.988619	val: 0.660073	test: 0.719179
PRC train: 0.983464	val: 0.929322	test: 0.739669

Epoch: 119
Loss: 0.2112025541252296
ROC train: 0.986079	val: 0.678755	test: 0.707007
PRC train: 0.977938	val: 0.930874	test: 0.727321

Epoch: 120
Loss: 0.21086937300083558
ROC train: 0.988305	val: 0.661172	test: 0.724744
PRC train: 0.982206	val: 0.927611	test: 0.746338

Early stopping
Best (ROC):	 train: 0.878539	val: 0.702198	test: 0.758825
Best (PRC):	 train: 0.816813	val: 0.938332	test: 0.746016

PRC train: 0.968708	val: 0.930557	test: 0.789732

Epoch: 95
Loss: 0.24517724855042808
ROC train: 0.978182	val: 0.657143	test: 0.793775
PRC train: 0.967420	val: 0.927778	test: 0.788557

Epoch: 96
Loss: 0.24803584028127013
ROC train: 0.979084	val: 0.672894	test: 0.786124
PRC train: 0.968952	val: 0.929604	test: 0.776778

Epoch: 97
Loss: 0.23947039415575727
ROC train: 0.980303	val: 0.669963	test: 0.778473
PRC train: 0.970095	val: 0.928494	test: 0.762043

Epoch: 98
Loss: 0.23925247859358958
ROC train: 0.980639	val: 0.657875	test: 0.782820
PRC train: 0.969463	val: 0.923119	test: 0.766993

Epoch: 99
Loss: 0.2228926507455764
ROC train: 0.981179	val: 0.651282	test: 0.781951
PRC train: 0.971235	val: 0.922093	test: 0.770555

Epoch: 100
Loss: 0.2511914191105394
ROC train: 0.977454	val: 0.660440	test: 0.768562
PRC train: 0.966264	val: 0.926702	test: 0.755774

Epoch: 101
Loss: 0.23972356641506476
ROC train: 0.978607	val: 0.678755	test: 0.785255
PRC train: 0.967565	val: 0.929889	test: 0.773059

Epoch: 102
Loss: 0.2417572537130015
ROC train: 0.982220	val: 0.651282	test: 0.786820
PRC train: 0.973569	val: 0.928233	test: 0.769062

Epoch: 103
Loss: 0.22454674124604304
ROC train: 0.979075	val: 0.662637	test: 0.770822
PRC train: 0.969497	val: 0.928295	test: 0.764732

Epoch: 104
Loss: 0.2379932917953938
ROC train: 0.983102	val: 0.665201	test: 0.766823
PRC train: 0.975444	val: 0.924992	test: 0.749798

Epoch: 105
Loss: 0.22537335801445452
ROC train: 0.980836	val: 0.658974	test: 0.765084
PRC train: 0.970915	val: 0.925038	test: 0.762162

Epoch: 106
Loss: 0.2318897756336598
ROC train: 0.975708	val: 0.655678	test: 0.763346
PRC train: 0.963036	val: 0.924475	test: 0.765377

Epoch: 107
Loss: 0.23566854170544263
ROC train: 0.978807	val: 0.655678	test: 0.768562
PRC train: 0.967238	val: 0.923811	test: 0.764779

Epoch: 108
Loss: 0.23400086013142601
ROC train: 0.984729	val: 0.658608	test: 0.774300
PRC train: 0.977018	val: 0.927289	test: 0.764566

Epoch: 109
Loss: 0.21371187721893836
ROC train: 0.982360	val: 0.670330	test: 0.769953
PRC train: 0.973814	val: 0.930096	test: 0.749866

Epoch: 110
Loss: 0.24497774809434203
ROC train: 0.984304	val: 0.669231	test: 0.777256
PRC train: 0.976167	val: 0.928720	test: 0.753973

Epoch: 111
Loss: 0.2329814993953297
ROC train: 0.984949	val: 0.684615	test: 0.784385
PRC train: 0.977298	val: 0.931840	test: 0.762439

Epoch: 112
Loss: 0.20792763405837006
ROC train: 0.982369	val: 0.685348	test: 0.787341
PRC train: 0.972853	val: 0.931317	test: 0.766333

Epoch: 113
Loss: 0.2361133750692226
ROC train: 0.984926	val: 0.667033	test: 0.781951
PRC train: 0.977459	val: 0.928532	test: 0.775908

Epoch: 114
Loss: 0.2200200006909653
ROC train: 0.984663	val: 0.642857	test: 0.778299
PRC train: 0.977335	val: 0.922409	test: 0.777908

Epoch: 115
Loss: 0.22346780941826455
ROC train: 0.984272	val: 0.640659	test: 0.775343
PRC train: 0.977145	val: 0.921976	test: 0.774347

Epoch: 116
Loss: 0.21219152499975094
ROC train: 0.980437	val: 0.665201	test: 0.765954
PRC train: 0.972283	val: 0.927483	test: 0.769498

Epoch: 117
Loss: 0.2194540469394556
ROC train: 0.984592	val: 0.668864	test: 0.772387
PRC train: 0.977429	val: 0.926978	test: 0.773208

Epoch: 118
Loss: 0.22745209590530013
ROC train: 0.986938	val: 0.642491	test: 0.776734
PRC train: 0.980463	val: 0.920906	test: 0.772945

Epoch: 119
Loss: 0.22965566935091486
ROC train: 0.987095	val: 0.655678	test: 0.782820
PRC train: 0.981090	val: 0.924923	test: 0.780223

Epoch: 120
Loss: 0.21009283669860562
ROC train: 0.986179	val: 0.656410	test: 0.784038
PRC train: 0.980264	val: 0.924099	test: 0.776158

Early stopping
Best (ROC):	 train: 0.887968	val: 0.719780	test: 0.784559
Best (PRC):	 train: 0.820394	val: 0.939763	test: 0.776304

PRC train: 0.999914	val: 0.929087	test: 0.735594

Epoch: 94
Loss: 0.13330650110870707
ROC train: 0.999954	val: 0.673626	test: 0.768388
PRC train: 0.999930	val: 0.930000	test: 0.746660

Epoch: 95
Loss: 0.11772666616737268
ROC train: 0.999809	val: 0.665568	test: 0.786646
PRC train: 0.999735	val: 0.925152	test: 0.765919

Epoch: 96
Loss: 0.10148227447480074
ROC train: 0.999926	val: 0.670330	test: 0.777778
PRC train: 0.999887	val: 0.927816	test: 0.757434

Epoch: 97
Loss: 0.08625931014449784
ROC train: 0.999971	val: 0.668864	test: 0.759520
PRC train: 0.999957	val: 0.932571	test: 0.739606

Epoch: 98
Loss: 0.09952167437767027
ROC train: 0.999863	val: 0.673626	test: 0.767171
PRC train: 0.999796	val: 0.931918	test: 0.745035

Epoch: 99
Loss: 0.08265745939567329
ROC train: 0.999837	val: 0.670330	test: 0.776908
PRC train: 0.999773	val: 0.931762	test: 0.746080

Epoch: 100
Loss: 0.10205431203216808
ROC train: 0.999900	val: 0.658608	test: 0.750826
PRC train: 0.999847	val: 0.926878	test: 0.714400

Epoch: 101
Loss: 0.09119364975806558
ROC train: 0.999409	val: 0.671795	test: 0.759694
PRC train: 0.999084	val: 0.931976	test: 0.736744

Epoch: 102
Loss: 0.09039650395970747
ROC train: 1.000000	val: 0.687912	test: 0.783516
PRC train: 1.000000	val: 0.936174	test: 0.768056

Epoch: 103
Loss: 0.10744882143743058
ROC train: 0.999960	val: 0.692674	test: 0.779690
PRC train: 0.999940	val: 0.936909	test: 0.757166

Epoch: 104
Loss: 0.10355506012062546
ROC train: 0.999509	val: 0.687179	test: 0.770127
PRC train: 0.999243	val: 0.935688	test: 0.763880

Epoch: 105
Loss: 0.09331159206570627
ROC train: 0.999897	val: 0.673993	test: 0.777430
PRC train: 0.999846	val: 0.931744	test: 0.745415

Epoch: 106
Loss: 0.09434807906963194
ROC train: 0.999980	val: 0.690842	test: 0.776039
PRC train: 0.999970	val: 0.935186	test: 0.755517

Epoch: 107
Loss: 0.07391067349980514
ROC train: 0.999954	val: 0.696703	test: 0.769258
PRC train: 0.999930	val: 0.935629	test: 0.747737

Epoch: 108
Loss: 0.07302740472342337
ROC train: 1.000000	val: 0.673993	test: 0.793949
PRC train: 1.000000	val: 0.931086	test: 0.772329

Epoch: 109
Loss: 0.09366189029122171
ROC train: 0.999989	val: 0.661905	test: 0.794992
PRC train: 0.999983	val: 0.926808	test: 0.780378

Epoch: 110
Loss: 0.07923479585682407
ROC train: 0.999949	val: 0.676190	test: 0.770996
PRC train: 0.999923	val: 0.934455	test: 0.755178

Epoch: 111
Loss: 0.09124457074417228
ROC train: 1.000000	val: 0.672527	test: 0.760911
PRC train: 1.000000	val: 0.934995	test: 0.726683

Epoch: 112
Loss: 0.07835480989629559
ROC train: 0.999980	val: 0.673993	test: 0.774300
PRC train: 0.999970	val: 0.932917	test: 0.741323

Epoch: 113
Loss: 0.07999175259445118
ROC train: 0.999509	val: 0.694872	test: 0.783342
PRC train: 0.999283	val: 0.930457	test: 0.762118

Epoch: 114
Loss: 0.06052761559836869
ROC train: 0.999920	val: 0.692308	test: 0.780734
PRC train: 0.999876	val: 0.935357	test: 0.762950

Epoch: 115
Loss: 0.0972532309024568
ROC train: 0.999983	val: 0.657875	test: 0.765780
PRC train: 0.999974	val: 0.928024	test: 0.737515

Epoch: 116
Loss: 0.07548522791484731
ROC train: 0.999986	val: 0.669597	test: 0.763867
PRC train: 0.999978	val: 0.929254	test: 0.733487

Epoch: 117
Loss: 0.08821714473439793
ROC train: 0.999926	val: 0.682418	test: 0.776561
PRC train: 0.999890	val: 0.932728	test: 0.745293

Epoch: 118
Loss: 0.07923805481481724
ROC train: 0.999989	val: 0.680586	test: 0.800556
PRC train: 0.999983	val: 0.931261	test: 0.775001

Epoch: 119
Loss: 0.09147960105688298
ROC train: 0.999991	val: 0.688645	test: 0.791862
PRC train: 0.999987	val: 0.934797	test: 0.775073

Epoch: 120
Loss: 0.08098408547311105
ROC train: 0.999780	val: 0.703663	test: 0.774474
PRC train: 0.999666	val: 0.935772	test: 0.764263

Early stopping
Best (ROC):	 train: 0.955588	val: 0.708059	test: 0.797774
Best (PRC):	 train: 0.927918	val: 0.942303	test: 0.807676

PRC train: 0.999930	val: 0.929466	test: 0.820000

Epoch: 94
Loss: 0.09862758374680336
ROC train: 0.999977	val: 0.673626	test: 0.844027
PRC train: 0.999965	val: 0.929266	test: 0.837919

Epoch: 95
Loss: 0.10344171346705469
ROC train: 1.000000	val: 0.647619	test: 0.831855
PRC train: 1.000000	val: 0.923971	test: 0.828164

Epoch: 96
Loss: 0.09928504694570905
ROC train: 0.999991	val: 0.635165	test: 0.809946
PRC train: 0.999987	val: 0.921254	test: 0.808909

Epoch: 97
Loss: 0.10631160923981002
ROC train: 0.999946	val: 0.630037	test: 0.803860
PRC train: 0.999918	val: 0.916655	test: 0.786807

Epoch: 98
Loss: 0.10752632194938878
ROC train: 0.999994	val: 0.632967	test: 0.810294
PRC train: 0.999991	val: 0.920748	test: 0.795208

Epoch: 99
Loss: 0.09485171328188118
ROC train: 1.000000	val: 0.643590	test: 0.815336
PRC train: 1.000000	val: 0.922077	test: 0.804643

Epoch: 100
Loss: 0.09356931427307544
ROC train: 0.999880	val: 0.628938	test: 0.839332
PRC train: 0.999820	val: 0.919634	test: 0.844458

Epoch: 101
Loss: 0.09526596932791806
ROC train: 0.999920	val: 0.640659	test: 0.818640
PRC train: 0.999878	val: 0.923896	test: 0.818373

Epoch: 102
Loss: 0.08590799818272378
ROC train: 0.999880	val: 0.663004	test: 0.812033
PRC train: 0.999823	val: 0.929250	test: 0.808910

Epoch: 103
Loss: 0.08075831790431451
ROC train: 0.999963	val: 0.660440	test: 0.819684
PRC train: 0.999943	val: 0.928182	test: 0.809825

Epoch: 104
Loss: 0.08976859248740701
ROC train: 1.000000	val: 0.647985	test: 0.819336
PRC train: 1.000000	val: 0.920271	test: 0.806151

Epoch: 105
Loss: 0.08776544407554279
ROC train: 0.999989	val: 0.645421	test: 0.829943
PRC train: 0.999983	val: 0.918521	test: 0.819911

Epoch: 106
Loss: 0.10653383165648903
ROC train: 1.000000	val: 0.633333	test: 0.833942
PRC train: 1.000000	val: 0.918228	test: 0.827952

Epoch: 107
Loss: 0.08225355491391956
ROC train: 0.999997	val: 0.623077	test: 0.817597
PRC train: 0.999996	val: 0.914936	test: 0.802993

Epoch: 108
Loss: 0.07697386971318454
ROC train: 1.000000	val: 0.641758	test: 0.814293
PRC train: 1.000000	val: 0.921457	test: 0.804680

Epoch: 109
Loss: 0.06404997439871263
ROC train: 0.999986	val: 0.638828	test: 0.817597
PRC train: 0.999978	val: 0.920934	test: 0.812125

Epoch: 110
Loss: 0.07116292862344309
ROC train: 0.999989	val: 0.653846	test: 0.819684
PRC train: 0.999983	val: 0.924823	test: 0.813881

Epoch: 111
Loss: 0.07365719088128322
ROC train: 0.999997	val: 0.663736	test: 0.831334
PRC train: 0.999996	val: 0.924921	test: 0.816151

Epoch: 112
Loss: 0.06095205705555601
ROC train: 1.000000	val: 0.665568	test: 0.828552
PRC train: 1.000000	val: 0.925326	test: 0.815550

Epoch: 113
Loss: 0.0748799942591096
ROC train: 1.000000	val: 0.659707	test: 0.819684
PRC train: 1.000000	val: 0.928144	test: 0.802599

Epoch: 114
Loss: 0.06422305312491848
ROC train: 1.000000	val: 0.643956	test: 0.820205
PRC train: 1.000000	val: 0.925324	test: 0.804852

Epoch: 115
Loss: 0.0813310737490945
ROC train: 1.000000	val: 0.623077	test: 0.828378
PRC train: 1.000000	val: 0.918941	test: 0.821533

Epoch: 116
Loss: 0.0610347992779019
ROC train: 0.999994	val: 0.640659	test: 0.820031
PRC train: 0.999991	val: 0.922446	test: 0.816189

Epoch: 117
Loss: 0.06486887021041594
ROC train: 1.000000	val: 0.634432	test: 0.822640
PRC train: 1.000000	val: 0.923457	test: 0.819356

Epoch: 118
Loss: 0.067196583992045
ROC train: 1.000000	val: 0.620147	test: 0.826465
PRC train: 1.000000	val: 0.916552	test: 0.816328

Epoch: 119
Loss: 0.06894102583313551
ROC train: 1.000000	val: 0.626007	test: 0.830986
PRC train: 1.000000	val: 0.915201	test: 0.819452

Epoch: 120
Loss: 0.06313492796767815
ROC train: 1.000000	val: 0.632967	test: 0.828378
PRC train: 1.000000	val: 0.912547	test: 0.810424

Early stopping
Best (ROC):	 train: 0.960525	val: 0.689011	test: 0.827682
Best (PRC):	 train: 0.938887	val: 0.932985	test: 0.822665

PRC train: 0.999974	val: 0.905750	test: 0.753426

Epoch: 94
Loss: 0.09199510911392131
ROC train: 1.000000	val: 0.643590	test: 0.780734
PRC train: 1.000000	val: 0.902651	test: 0.761659

Epoch: 95
Loss: 0.08366579112443454
ROC train: 0.999909	val: 0.694505	test: 0.766302
PRC train: 0.999861	val: 0.910832	test: 0.741901

Epoch: 96
Loss: 0.0971526058475817
ROC train: 0.999951	val: 0.643223	test: 0.746827
PRC train: 0.999928	val: 0.906108	test: 0.746783

Epoch: 97
Loss: 0.09790057983088317
ROC train: 0.999909	val: 0.613553	test: 0.764215
PRC train: 0.999859	val: 0.898264	test: 0.753203

Epoch: 98
Loss: 0.09566827303716596
ROC train: 1.000000	val: 0.646154	test: 0.777778
PRC train: 1.000000	val: 0.907205	test: 0.737215

Epoch: 99
Loss: 0.06905253307451746
ROC train: 0.999906	val: 0.650916	test: 0.762302
PRC train: 0.999862	val: 0.906594	test: 0.749553

Epoch: 100
Loss: 0.0943974749860951
ROC train: 0.999937	val: 0.663004	test: 0.752391
PRC train: 0.999904	val: 0.907252	test: 0.734945

Epoch: 101
Loss: 0.07742171355206144
ROC train: 1.000000	val: 0.700366	test: 0.766823
PRC train: 1.000000	val: 0.914489	test: 0.734247

Epoch: 102
Loss: 0.07924095152602137
ROC train: 1.000000	val: 0.688645	test: 0.769084
PRC train: 1.000000	val: 0.914713	test: 0.739916

Epoch: 103
Loss: 0.08088816657886959
ROC train: 1.000000	val: 0.669963	test: 0.772735
PRC train: 1.000000	val: 0.909450	test: 0.754112

Epoch: 104
Loss: 0.06765512272793134
ROC train: 0.999717	val: 0.689011	test: 0.745957
PRC train: 0.999606	val: 0.908318	test: 0.720817

Epoch: 105
Loss: 0.07245874505522713
ROC train: 1.000000	val: 0.676923	test: 0.765954
PRC train: 1.000000	val: 0.899113	test: 0.739354

Epoch: 106
Loss: 0.05762178777381173
ROC train: 1.000000	val: 0.672161	test: 0.786472
PRC train: 1.000000	val: 0.892295	test: 0.750001

Epoch: 107
Loss: 0.06268721899937288
ROC train: 1.000000	val: 0.658242	test: 0.790993
PRC train: 1.000000	val: 0.892020	test: 0.759106

Epoch: 108
Loss: 0.06426865259926468
ROC train: 1.000000	val: 0.660073	test: 0.785950
PRC train: 1.000000	val: 0.897031	test: 0.758468

Epoch: 109
Loss: 0.07602242079276081
ROC train: 1.000000	val: 0.647985	test: 0.779864
PRC train: 1.000000	val: 0.895283	test: 0.751730

Epoch: 110
Loss: 0.06487479956990282
ROC train: 1.000000	val: 0.639927	test: 0.779517
PRC train: 1.000000	val: 0.891420	test: 0.760865

Epoch: 111
Loss: 0.07000732385018521
ROC train: 1.000000	val: 0.687912	test: 0.788037
PRC train: 1.000000	val: 0.900996	test: 0.742846

Epoch: 112
Loss: 0.060850943938975464
ROC train: 0.999997	val: 0.681685	test: 0.787863
PRC train: 0.999996	val: 0.892610	test: 0.751547

Epoch: 113
Loss: 0.06112242454993679
ROC train: 1.000000	val: 0.668498	test: 0.773605
PRC train: 1.000000	val: 0.899837	test: 0.766393

Epoch: 114
Loss: 0.0683790680723323
ROC train: 1.000000	val: 0.691941	test: 0.771692
PRC train: 1.000000	val: 0.909858	test: 0.755137

Epoch: 115
Loss: 0.06906529027196243
ROC train: 1.000000	val: 0.660073	test: 0.778821
PRC train: 1.000000	val: 0.903425	test: 0.751933

Epoch: 116
Loss: 0.08187898207946476
ROC train: 0.999526	val: 0.629670	test: 0.766823
PRC train: 0.999343	val: 0.906047	test: 0.768854

Epoch: 117
Loss: 0.07861901412184634
ROC train: 1.000000	val: 0.681319	test: 0.792732
PRC train: 1.000000	val: 0.910588	test: 0.757609

Epoch: 118
Loss: 0.05500561163662341
ROC train: 1.000000	val: 0.688278	test: 0.789080
PRC train: 1.000000	val: 0.914079	test: 0.767771

Epoch: 119
Loss: 0.08353671669586407
ROC train: 0.999989	val: 0.669597	test: 0.781951
PRC train: 0.999983	val: 0.916758	test: 0.762620

Epoch: 120
Loss: 0.05337594541928552
ROC train: 1.000000	val: 0.667766	test: 0.772040
PRC train: 1.000000	val: 0.907327	test: 0.750623

Early stopping
Best (ROC):	 train: 0.990000	val: 0.709890	test: 0.774996
Best (PRC):	 train: 0.983685	val: 0.920621	test: 0.749775

PRC train: 1.000000	val: 0.935984	test: 0.695814

Epoch: 94
Loss: 0.07830988093850674
ROC train: 1.000000	val: 0.698168	test: 0.758651
PRC train: 1.000000	val: 0.938373	test: 0.712357

Epoch: 95
Loss: 0.07311811289937908
ROC train: 1.000000	val: 0.712088	test: 0.758998
PRC train: 1.000000	val: 0.943126	test: 0.730874

Epoch: 96
Loss: 0.0703346228818126
ROC train: 0.999997	val: 0.716850	test: 0.753608
PRC train: 0.999996	val: 0.944922	test: 0.718963

Epoch: 97
Loss: 0.05751901409516069
ROC train: 1.000000	val: 0.732234	test: 0.767693
PRC train: 1.000000	val: 0.946805	test: 0.726205

Epoch: 98
Loss: 0.08440226147962422
ROC train: 1.000000	val: 0.727839	test: 0.782646
PRC train: 1.000000	val: 0.945393	test: 0.744734

Epoch: 99
Loss: 0.07231373445508582
ROC train: 1.000000	val: 0.708059	test: 0.778126
PRC train: 1.000000	val: 0.942768	test: 0.755384

Epoch: 100
Loss: 0.0767548596659301
ROC train: 1.000000	val: 0.710989	test: 0.779343
PRC train: 1.000000	val: 0.944130	test: 0.746563

Epoch: 101
Loss: 0.07040827518698704
ROC train: 1.000000	val: 0.724542	test: 0.785081
PRC train: 1.000000	val: 0.945415	test: 0.752055

Epoch: 102
Loss: 0.07786455435440684
ROC train: 1.000000	val: 0.711355	test: 0.792210
PRC train: 1.000000	val: 0.942551	test: 0.756480

Epoch: 103
Loss: 0.07054065636293358
ROC train: 0.999997	val: 0.719414	test: 0.788385
PRC train: 0.999996	val: 0.945927	test: 0.752531

Epoch: 104
Loss: 0.06553842663032622
ROC train: 1.000000	val: 0.724908	test: 0.785255
PRC train: 1.000000	val: 0.947016	test: 0.744502

Epoch: 105
Loss: 0.06695132036708597
ROC train: 1.000000	val: 0.712821	test: 0.769431
PRC train: 1.000000	val: 0.943270	test: 0.738031

Epoch: 106
Loss: 0.06555961747236197
ROC train: 1.000000	val: 0.709890	test: 0.767866
PRC train: 1.000000	val: 0.943072	test: 0.742822

Epoch: 107
Loss: 0.06888800182439547
ROC train: 1.000000	val: 0.725275	test: 0.777778
PRC train: 1.000000	val: 0.947077	test: 0.745225

Epoch: 108
Loss: 0.06584962419710567
ROC train: 1.000000	val: 0.720513	test: 0.766823
PRC train: 1.000000	val: 0.946493	test: 0.745060

Epoch: 109
Loss: 0.055080675174052796
ROC train: 1.000000	val: 0.718681	test: 0.764910
PRC train: 1.000000	val: 0.944257	test: 0.744687

Epoch: 110
Loss: 0.0706877304377994
ROC train: 0.999983	val: 0.720879	test: 0.767866
PRC train: 0.999974	val: 0.945236	test: 0.731306

Epoch: 111
Loss: 0.07268831326472816
ROC train: 1.000000	val: 0.733700	test: 0.786994
PRC train: 1.000000	val: 0.948758	test: 0.747224

Epoch: 112
Loss: 0.06979593826201977
ROC train: 0.999943	val: 0.723443	test: 0.780386
PRC train: 0.999915	val: 0.947745	test: 0.739121

Epoch: 113
Loss: 0.07485729091417254
ROC train: 1.000000	val: 0.709158	test: 0.786124
PRC train: 1.000000	val: 0.942214	test: 0.756736

Epoch: 114
Loss: 0.0746877962644977
ROC train: 0.999983	val: 0.712821	test: 0.756216
PRC train: 0.999974	val: 0.942330	test: 0.751049

Epoch: 115
Loss: 0.07555797568139452
ROC train: 1.000000	val: 0.712088	test: 0.750304
PRC train: 1.000000	val: 0.944043	test: 0.742704

Epoch: 116
Loss: 0.05456675041605409
ROC train: 1.000000	val: 0.714286	test: 0.762128
PRC train: 1.000000	val: 0.944727	test: 0.739162

Epoch: 117
Loss: 0.05788589128106999
ROC train: 1.000000	val: 0.704396	test: 0.760042
PRC train: 1.000000	val: 0.941804	test: 0.732755

Epoch: 118
Loss: 0.0517948975122832
ROC train: 1.000000	val: 0.706227	test: 0.767171
PRC train: 1.000000	val: 0.941200	test: 0.726341

Epoch: 119
Loss: 0.04303859482334428
ROC train: 1.000000	val: 0.716117	test: 0.763346
PRC train: 1.000000	val: 0.945565	test: 0.731972

Epoch: 120
Loss: 0.05154261509329346
ROC train: 1.000000	val: 0.720513	test: 0.763867
PRC train: 1.000000	val: 0.946112	test: 0.743173

Early stopping
Best (ROC):	 train: 0.982041	val: 0.774725	test: 0.727352
Best (PRC):	 train: 0.975144	val: 0.957515	test: 0.705584

PRC train: 0.999996	val: 0.912776	test: 0.745198

Epoch: 94
Loss: 0.06653806931232928
ROC train: 1.000000	val: 0.568498	test: 0.805251
PRC train: 1.000000	val: 0.896636	test: 0.773702

Epoch: 95
Loss: 0.04886080331560449
ROC train: 0.999997	val: 0.544689	test: 0.766475
PRC train: 0.999996	val: 0.878436	test: 0.770045

Epoch: 96
Loss: 0.057886215364607196
ROC train: 1.000000	val: 0.593773	test: 0.784559
PRC train: 1.000000	val: 0.909101	test: 0.771723

Epoch: 97
Loss: 0.06433327808746303
ROC train: 1.000000	val: 0.620147	test: 0.804730
PRC train: 1.000000	val: 0.918512	test: 0.784860

Epoch: 98
Loss: 0.06799041250662644
ROC train: 1.000000	val: 0.588645	test: 0.811337
PRC train: 1.000000	val: 0.909332	test: 0.793514

Epoch: 99
Loss: 0.06684506206859184
ROC train: 1.000000	val: 0.613553	test: 0.816901
PRC train: 1.000000	val: 0.916369	test: 0.796258

Epoch: 100
Loss: 0.06307718210230429
ROC train: 1.000000	val: 0.600000	test: 0.810120
PRC train: 1.000000	val: 0.913770	test: 0.793802

Epoch: 101
Loss: 0.06939063468046197
ROC train: 1.000000	val: 0.562637	test: 0.770996
PRC train: 1.000000	val: 0.899302	test: 0.782846

Epoch: 102
Loss: 0.051552805222943164
ROC train: 1.000000	val: 0.573626	test: 0.764215
PRC train: 1.000000	val: 0.902837	test: 0.757257

Epoch: 103
Loss: 0.06906477786973554
ROC train: 1.000000	val: 0.599267	test: 0.788906
PRC train: 1.000000	val: 0.908863	test: 0.738926

Epoch: 104
Loss: 0.0543956446910019
ROC train: 1.000000	val: 0.609158	test: 0.796035
PRC train: 1.000000	val: 0.915299	test: 0.745044

Epoch: 105
Loss: 0.054691691935838706
ROC train: 1.000000	val: 0.594139	test: 0.784038
PRC train: 1.000000	val: 0.908116	test: 0.750676

Epoch: 106
Loss: 0.05550660421802546
ROC train: 1.000000	val: 0.599634	test: 0.786994
PRC train: 1.000000	val: 0.908994	test: 0.758213

Epoch: 107
Loss: 0.053462386422420516
ROC train: 1.000000	val: 0.604396	test: 0.788732
PRC train: 1.000000	val: 0.911454	test: 0.766357

Epoch: 108
Loss: 0.06256326755094488
ROC train: 1.000000	val: 0.610989	test: 0.796035
PRC train: 1.000000	val: 0.915467	test: 0.770006

Epoch: 109
Loss: 0.05775969868349977
ROC train: 1.000000	val: 0.600366	test: 0.800556
PRC train: 1.000000	val: 0.910699	test: 0.761804

Epoch: 110
Loss: 0.060127081525187184
ROC train: 1.000000	val: 0.595238	test: 0.784038
PRC train: 1.000000	val: 0.907912	test: 0.761028

Epoch: 111
Loss: 0.05068920429669192
ROC train: 1.000000	val: 0.612454	test: 0.795166
PRC train: 1.000000	val: 0.915625	test: 0.770098

Epoch: 112
Loss: 0.054476615712523824
ROC train: 1.000000	val: 0.620147	test: 0.789776
PRC train: 1.000000	val: 0.913113	test: 0.747793

Epoch: 113
Loss: 0.06364206830200111
ROC train: 1.000000	val: 0.591209	test: 0.765606
PRC train: 1.000000	val: 0.906718	test: 0.753809

Epoch: 114
Loss: 0.0396083638888713
ROC train: 0.999994	val: 0.578022	test: 0.737437
PRC train: 0.999991	val: 0.902006	test: 0.748445

Epoch: 115
Loss: 0.06260539585936051
ROC train: 1.000000	val: 0.582784	test: 0.739002
PRC train: 1.000000	val: 0.901248	test: 0.777284

Epoch: 116
Loss: 0.04765011729741218
ROC train: 1.000000	val: 0.601099	test: 0.782820
PRC train: 1.000000	val: 0.910270	test: 0.787450

Epoch: 117
Loss: 0.06649850229436463
ROC train: 1.000000	val: 0.641026	test: 0.791862
PRC train: 1.000000	val: 0.918833	test: 0.738748

Epoch: 118
Loss: 0.04315786841236681
ROC train: 1.000000	val: 0.623810	test: 0.786124
PRC train: 1.000000	val: 0.912534	test: 0.754417

Epoch: 119
Loss: 0.04661533742015396
ROC train: 1.000000	val: 0.623810	test: 0.791515
PRC train: 1.000000	val: 0.917119	test: 0.793933

Epoch: 120
Loss: 0.038702656464532664
ROC train: 1.000000	val: 0.617582	test: 0.784907
PRC train: 1.000000	val: 0.915873	test: 0.794841

Early stopping
Best (ROC):	 train: 0.989877	val: 0.686813	test: 0.754999
Best (PRC):	 train: 0.985232	val: 0.936496	test: 0.721305

PRC train: 1.000000	val: 0.939028	test: 0.794090

Epoch: 94
Loss: 0.08352446158065494
ROC train: 0.999997	val: 0.663370	test: 0.816206
PRC train: 0.999996	val: 0.932314	test: 0.795831

Epoch: 95
Loss: 0.07015070397178365
ROC train: 1.000000	val: 0.654579	test: 0.814119
PRC train: 1.000000	val: 0.928223	test: 0.786209

Epoch: 96
Loss: 0.07482370191164661
ROC train: 1.000000	val: 0.667766	test: 0.808381
PRC train: 1.000000	val: 0.931096	test: 0.782068

Epoch: 97
Loss: 0.05691786397463563
ROC train: 1.000000	val: 0.670330	test: 0.812380
PRC train: 1.000000	val: 0.932727	test: 0.776243

Epoch: 98
Loss: 0.06769765967210664
ROC train: 1.000000	val: 0.692674	test: 0.807686
PRC train: 1.000000	val: 0.937904	test: 0.770597

Epoch: 99
Loss: 0.0631312015609527
ROC train: 1.000000	val: 0.697802	test: 0.802817
PRC train: 1.000000	val: 0.937466	test: 0.775225

Epoch: 100
Loss: 0.06388467386987472
ROC train: 1.000000	val: 0.649084	test: 0.814641
PRC train: 1.000000	val: 0.929063	test: 0.795394

Epoch: 101
Loss: 0.08392074002578746
ROC train: 1.000000	val: 0.676190	test: 0.807338
PRC train: 1.000000	val: 0.933361	test: 0.781641

Epoch: 102
Loss: 0.061164938437976234
ROC train: 1.000000	val: 0.692674	test: 0.806642
PRC train: 1.000000	val: 0.935838	test: 0.770063

Epoch: 103
Loss: 0.06660942051124993
ROC train: 0.999980	val: 0.690842	test: 0.812728
PRC train: 0.999969	val: 0.938479	test: 0.791222

Epoch: 104
Loss: 0.06519427551626261
ROC train: 0.999503	val: 0.665201	test: 0.798818
PRC train: 0.999089	val: 0.934073	test: 0.787983

Epoch: 105
Loss: 0.06448475899220914
ROC train: 1.000000	val: 0.685348	test: 0.800383
PRC train: 1.000000	val: 0.937181	test: 0.774704

Epoch: 106
Loss: 0.06930024769581537
ROC train: 0.999997	val: 0.702198	test: 0.777604
PRC train: 0.999996	val: 0.937286	test: 0.741281

Epoch: 107
Loss: 0.056033572525482314
ROC train: 1.000000	val: 0.680220	test: 0.812902
PRC train: 1.000000	val: 0.933152	test: 0.772322

Epoch: 108
Loss: 0.04810561131307195
ROC train: 1.000000	val: 0.672161	test: 0.815510
PRC train: 1.000000	val: 0.933816	test: 0.781998

Epoch: 109
Loss: 0.058069945754168094
ROC train: 1.000000	val: 0.671795	test: 0.809946
PRC train: 1.000000	val: 0.934689	test: 0.781551

Epoch: 110
Loss: 0.05453486533290075
ROC train: 1.000000	val: 0.689377	test: 0.804034
PRC train: 1.000000	val: 0.940240	test: 0.774737

Epoch: 111
Loss: 0.06382361829686285
ROC train: 1.000000	val: 0.687179	test: 0.794644
PRC train: 1.000000	val: 0.940485	test: 0.762803

Epoch: 112
Loss: 0.07279318306328415
ROC train: 0.999991	val: 0.669597	test: 0.800904
PRC train: 0.999987	val: 0.934718	test: 0.762780

Epoch: 113
Loss: 0.051975139668042805
ROC train: 1.000000	val: 0.673260	test: 0.804903
PRC train: 1.000000	val: 0.934868	test: 0.776158

Epoch: 114
Loss: 0.04572552114259143
ROC train: 1.000000	val: 0.686813	test: 0.801426
PRC train: 1.000000	val: 0.935816	test: 0.771790

Epoch: 115
Loss: 0.04096604435242065
ROC train: 1.000000	val: 0.675824	test: 0.804208
PRC train: 1.000000	val: 0.933815	test: 0.779037

Epoch: 116
Loss: 0.05056138716330212
ROC train: 1.000000	val: 0.677656	test: 0.803686
PRC train: 1.000000	val: 0.934469	test: 0.793912

Epoch: 117
Loss: 0.0361565287168301
ROC train: 1.000000	val: 0.673626	test: 0.812554
PRC train: 1.000000	val: 0.931892	test: 0.792337

Epoch: 118
Loss: 0.045477276632700556
ROC train: 1.000000	val: 0.671795	test: 0.808381
PRC train: 1.000000	val: 0.931863	test: 0.779860

Epoch: 119
Loss: 0.05010792545751721
ROC train: 1.000000	val: 0.657143	test: 0.805947
PRC train: 1.000000	val: 0.930592	test: 0.777640

Epoch: 120
Loss: 0.05108422471012616
ROC train: 1.000000	val: 0.694505	test: 0.801252
PRC train: 1.000000	val: 0.936473	test: 0.757883

Epoch: 121
Loss: 0.04590090355423889
ROC train: 1.000000	val: 0.703297	test: 0.801426
PRC train: 1.000000	val: 0.937219	test: 0.760711

Epoch: 122
Loss: 0.0322636991828303
ROC train: 1.000000	val: 0.688278	test: 0.810468
PRC train: 1.000000	val: 0.938672	test: 0.781892

Epoch: 123
Loss: 0.03279201443787262
ROC train: 1.000000	val: 0.694139	test: 0.813598
PRC train: 1.000000	val: 0.940275	test: 0.779399

Epoch: 124
Loss: 0.05239074882585111
ROC train: 1.000000	val: 0.702198	test: 0.817423
PRC train: 1.000000	val: 0.940525	test: 0.779449

Epoch: 125
Loss: 0.038989525923529285
ROC train: 1.000000	val: 0.680220	test: 0.806990
PRC train: 1.000000	val: 0.935883	test: 0.778016

Early stopping
Best (ROC):	 train: 0.999957	val: 0.713553	test: 0.789950
Best (PRC):	 train: 0.999935	val: 0.939984	test: 0.767743

PRC train: 0.970621	val: 0.927131	test: 0.790111

Epoch: 95
Loss: 0.2235466283386836
ROC train: 0.983059	val: 0.656777	test: 0.763519
PRC train: 0.974067	val: 0.924128	test: 0.776996

Epoch: 96
Loss: 0.23646995368445145
ROC train: 0.984378	val: 0.649817	test: 0.770301
PRC train: 0.976382	val: 0.923528	test: 0.780619

Epoch: 97
Loss: 0.22140134814043227
ROC train: 0.981067	val: 0.652015	test: 0.785429
PRC train: 0.971794	val: 0.924997	test: 0.796272

Epoch: 98
Loss: 0.23106996097951696
ROC train: 0.983165	val: 0.662271	test: 0.772909
PRC train: 0.974924	val: 0.927907	test: 0.789882

Epoch: 99
Loss: 0.2227608688018868
ROC train: 0.984969	val: 0.665568	test: 0.768040
PRC train: 0.977495	val: 0.926879	test: 0.782350

Epoch: 100
Loss: 0.22715925832938838
ROC train: 0.979035	val: 0.661172	test: 0.758998
PRC train: 0.967853	val: 0.921604	test: 0.758282

Epoch: 101
Loss: 0.21957094894187784
ROC train: 0.978662	val: 0.665201	test: 0.778821
PRC train: 0.967385	val: 0.922611	test: 0.780876

Epoch: 102
Loss: 0.22752028493422638
ROC train: 0.981775	val: 0.666667	test: 0.768040
PRC train: 0.972842	val: 0.927414	test: 0.778322

Epoch: 103
Loss: 0.2339315545091465
ROC train: 0.977937	val: 0.677656	test: 0.730308
PRC train: 0.966424	val: 0.931865	test: 0.740226

Epoch: 104
Loss: 0.21773994903037147
ROC train: 0.983878	val: 0.664103	test: 0.732220
PRC train: 0.976259	val: 0.925177	test: 0.746080

Epoch: 105
Loss: 0.2185706512083986
ROC train: 0.984726	val: 0.667399	test: 0.736741
PRC train: 0.977628	val: 0.922854	test: 0.748392

Epoch: 106
Loss: 0.21684907298521
ROC train: 0.981358	val: 0.700366	test: 0.753956
PRC train: 0.971088	val: 0.930035	test: 0.767422

Epoch: 107
Loss: 0.23132136495232353
ROC train: 0.984620	val: 0.668132	test: 0.762998
PRC train: 0.976443	val: 0.929733	test: 0.777678

Epoch: 108
Loss: 0.2088417032414755
ROC train: 0.985893	val: 0.663004	test: 0.754130
PRC train: 0.978937	val: 0.929208	test: 0.765710

Epoch: 109
Loss: 0.23142004361213103
ROC train: 0.983559	val: 0.686447	test: 0.756390
PRC train: 0.975422	val: 0.935089	test: 0.760563

Epoch: 110
Loss: 0.21254698120391208
ROC train: 0.983844	val: 0.693040	test: 0.744740
PRC train: 0.975700	val: 0.934743	test: 0.755170

Epoch: 111
Loss: 0.2236241184940105
ROC train: 0.983499	val: 0.669231	test: 0.763172
PRC train: 0.974382	val: 0.927955	test: 0.773653

Epoch: 112
Loss: 0.20892910071249746
ROC train: 0.986578	val: 0.661905	test: 0.767693
PRC train: 0.980197	val: 0.925669	test: 0.777141

Epoch: 113
Loss: 0.2117947982856009
ROC train: 0.986781	val: 0.657875	test: 0.767519
PRC train: 0.980266	val: 0.922950	test: 0.774406

Epoch: 114
Loss: 0.20784130777988388
ROC train: 0.984757	val: 0.652381	test: 0.762824
PRC train: 0.976956	val: 0.922860	test: 0.770104

Epoch: 115
Loss: 0.21203873313832583
ROC train: 0.988051	val: 0.651282	test: 0.756738
PRC train: 0.982633	val: 0.923987	test: 0.765889

Epoch: 116
Loss: 0.20950987036862284
ROC train: 0.987169	val: 0.654945	test: 0.737089
PRC train: 0.980840	val: 0.926293	test: 0.757560

Epoch: 117
Loss: 0.21943772215119126
ROC train: 0.986002	val: 0.664469	test: 0.736568
PRC train: 0.978938	val: 0.928672	test: 0.758596

Epoch: 118
Loss: 0.2127357192599911
ROC train: 0.980342	val: 0.682784	test: 0.724744
PRC train: 0.969724	val: 0.932135	test: 0.717459

Epoch: 119
Loss: 0.22090729368326345
ROC train: 0.985930	val: 0.678022	test: 0.751869
PRC train: 0.978480	val: 0.931861	test: 0.757069

Epoch: 120
Loss: 0.20213033112242235
ROC train: 0.985976	val: 0.665201	test: 0.736741
PRC train: 0.978795	val: 0.926068	test: 0.742403

Epoch: 121
Loss: 0.20857024362688287
ROC train: 0.983833	val: 0.652381	test: 0.723352
PRC train: 0.976816	val: 0.921284	test: 0.737440

Epoch: 122
Loss: 0.20135576001137007
ROC train: 0.989095	val: 0.639927	test: 0.735872
PRC train: 0.984757	val: 0.921304	test: 0.743199

Epoch: 123
Loss: 0.19499395154830484
ROC train: 0.986807	val: 0.652015	test: 0.748044
PRC train: 0.980665	val: 0.920917	test: 0.756422

Epoch: 124
Loss: 0.1981080042304297
ROC train: 0.986276	val: 0.645421	test: 0.737437
PRC train: 0.980274	val: 0.921171	test: 0.739128

Epoch: 125
Loss: 0.20363997818396978
ROC train: 0.986367	val: 0.650183	test: 0.720744
PRC train: 0.979574	val: 0.923703	test: 0.741728

Epoch: 126
Loss: 0.19682161375474327
ROC train: 0.989857	val: 0.667033	test: 0.731873
PRC train: 0.985095	val: 0.929803	test: 0.752794

Epoch: 127
Loss: 0.20258367526097806
ROC train: 0.990060	val: 0.656777	test: 0.749957
PRC train: 0.985334	val: 0.927236	test: 0.765335

Epoch: 128
Loss: 0.17481699154109823
ROC train: 0.989092	val: 0.652747	test: 0.744740
PRC train: 0.983394	val: 0.925136	test: 0.760065

Epoch: 129
Loss: 0.17543894100930202
ROC train: 0.989104	val: 0.648718	test: 0.735524
PRC train: 0.983737	val: 0.927111	test: 0.749119

Epoch: 130
Loss: 0.2163146140281332
ROC train: 0.984786	val: 0.680586	test: 0.761607
PRC train: 0.978466	val: 0.935874	test: 0.773865

Epoch: 131
Loss: 0.21749656788890953
ROC train: 0.989780	val: 0.665568	test: 0.756216
PRC train: 0.985440	val: 0.929347	test: 0.766673

Epoch: 132
Loss: 0.19336309740787536
ROC train: 0.987560	val: 0.643223	test: 0.747696
PRC train: 0.981869	val: 0.921292	test: 0.757719

Epoch: 133
Loss: 0.2073537564032221
ROC train: 0.989763	val: 0.649817	test: 0.753260
PRC train: 0.985219	val: 0.924434	test: 0.773053

Epoch: 134
Loss: 0.18707450232869482
ROC train: 0.990993	val: 0.634799	test: 0.742306
PRC train: 0.987125	val: 0.922467	test: 0.767107

Epoch: 135
Loss: 0.1932552493718811
ROC train: 0.986963	val: 0.676190	test: 0.733785
PRC train: 0.981276	val: 0.928625	test: 0.767426

Epoch: 136
Loss: 0.1990003322004112
ROC train: 0.990271	val: 0.668132	test: 0.723526
PRC train: 0.986047	val: 0.929736	test: 0.749180

Epoch: 137
Loss: 0.1820535231553936
ROC train: 0.991704	val: 0.657875	test: 0.737089
PRC train: 0.987840	val: 0.927495	test: 0.751235

Epoch: 138
Loss: 0.19818901460584165
ROC train: 0.989894	val: 0.642857	test: 0.747696
PRC train: 0.985504	val: 0.920549	test: 0.767764

Epoch: 139
Loss: 0.19858090299103998
ROC train: 0.987862	val: 0.640659	test: 0.744218
PRC train: 0.981807	val: 0.915648	test: 0.763514

Epoch: 140
Loss: 0.20359798441590046
ROC train: 0.992794	val: 0.657875	test: 0.736394
PRC train: 0.989786	val: 0.923414	test: 0.756567

Epoch: 141
Loss: 0.18480051910411274
ROC train: 0.991341	val: 0.633700	test: 0.732568
PRC train: 0.987766	val: 0.912483	test: 0.736409

Early stopping
Best (ROC):	 train: 0.981358	val: 0.700366	test: 0.753956
Best (PRC):	 train: 0.971088	val: 0.930035	test: 0.767422
All runs completed.

PRC train: 0.999996	val: 0.946686	test: 0.764151

Epoch: 94
Loss: 0.09168215412768464
ROC train: 1.000000	val: 0.713919	test: 0.767866
PRC train: 1.000000	val: 0.945574	test: 0.757950

Epoch: 95
Loss: 0.08130144159645183
ROC train: 1.000000	val: 0.715751	test: 0.752739
PRC train: 1.000000	val: 0.947293	test: 0.744655

Epoch: 96
Loss: 0.079387948502572
ROC train: 1.000000	val: 0.735531	test: 0.749609
PRC train: 1.000000	val: 0.951832	test: 0.742810

Epoch: 97
Loss: 0.06714432353482247
ROC train: 1.000000	val: 0.736630	test: 0.755173
PRC train: 1.000000	val: 0.950305	test: 0.763990

Epoch: 98
Loss: 0.07027104505238162
ROC train: 1.000000	val: 0.723077	test: 0.772735
PRC train: 1.000000	val: 0.946770	test: 0.781056

Epoch: 99
Loss: 0.08466035198048978
ROC train: 1.000000	val: 0.738828	test: 0.784733
PRC train: 1.000000	val: 0.951150	test: 0.786009

Epoch: 100
Loss: 0.08181094926864102
ROC train: 1.000000	val: 0.744689	test: 0.776387
PRC train: 1.000000	val: 0.952296	test: 0.771546

Epoch: 101
Loss: 0.07964440700317861
ROC train: 1.000000	val: 0.747619	test: 0.769953
PRC train: 1.000000	val: 0.953570	test: 0.759201

Epoch: 102
Loss: 0.0822495018303798
ROC train: 0.999991	val: 0.757875	test: 0.757607
PRC train: 0.999987	val: 0.956182	test: 0.749669

Epoch: 103
Loss: 0.08408540584296934
ROC train: 0.999997	val: 0.750549	test: 0.770475
PRC train: 0.999996	val: 0.952812	test: 0.763454

Epoch: 104
Loss: 0.06664493074873964
ROC train: 1.000000	val: 0.732601	test: 0.774300
PRC train: 1.000000	val: 0.947554	test: 0.756885

Epoch: 105
Loss: 0.06915058813322741
ROC train: 0.999994	val: 0.726740	test: 0.764910
PRC train: 0.999991	val: 0.946117	test: 0.746858

Epoch: 106
Loss: 0.08102669473135812
ROC train: 0.999977	val: 0.756777	test: 0.762476
PRC train: 0.999966	val: 0.955889	test: 0.746173

Epoch: 107
Loss: 0.06120701831917823
ROC train: 0.999977	val: 0.779853	test: 0.771170
PRC train: 0.999965	val: 0.961352	test: 0.743458

Epoch: 108
Loss: 0.06613239950647046
ROC train: 1.000000	val: 0.769231	test: 0.758477
PRC train: 1.000000	val: 0.958756	test: 0.742188

Epoch: 109
Loss: 0.07369897670592196
ROC train: 1.000000	val: 0.775458	test: 0.748739
PRC train: 1.000000	val: 0.960653	test: 0.736729

Epoch: 110
Loss: 0.06930203462121476
ROC train: 0.999997	val: 0.767033	test: 0.757433
PRC train: 0.999996	val: 0.956630	test: 0.746705

Epoch: 111
Loss: 0.06536717496270564
ROC train: 0.999991	val: 0.752015	test: 0.758651
PRC train: 0.999987	val: 0.953815	test: 0.756477

Epoch: 112
Loss: 0.05950190189790736
ROC train: 0.999969	val: 0.741392	test: 0.723352
PRC train: 0.999953	val: 0.950806	test: 0.701786

Epoch: 113
Loss: 0.0787820523542284
ROC train: 0.999963	val: 0.722344	test: 0.742827
PRC train: 0.999943	val: 0.946778	test: 0.721895

Epoch: 114
Loss: 0.05431078338481132
ROC train: 1.000000	val: 0.727839	test: 0.768388
PRC train: 1.000000	val: 0.948812	test: 0.761678

Epoch: 115
Loss: 0.0778557539077485
ROC train: 0.999812	val: 0.703663	test: 0.780734
PRC train: 0.999748	val: 0.943211	test: 0.756506

Epoch: 116
Loss: 0.06819211927585167
ROC train: 0.999991	val: 0.690110	test: 0.775517
PRC train: 0.999987	val: 0.938633	test: 0.761741

Epoch: 117
Loss: 0.05969590785793304
ROC train: 1.000000	val: 0.702564	test: 0.749783
PRC train: 1.000000	val: 0.938836	test: 0.748561

Epoch: 118
Loss: 0.06002067011894859
ROC train: 1.000000	val: 0.712821	test: 0.727352
PRC train: 1.000000	val: 0.942132	test: 0.717944

Epoch: 119
Loss: 0.05462675850418569
ROC train: 1.000000	val: 0.714286	test: 0.739697
PRC train: 1.000000	val: 0.945905	test: 0.741033

Epoch: 120
Loss: 0.05563665272838675
ROC train: 1.000000	val: 0.681319	test: 0.752391
PRC train: 1.000000	val: 0.935806	test: 0.750592

Epoch: 121
Loss: 0.045337033038074785
ROC train: 1.000000	val: 0.704396	test: 0.769605
PRC train: 1.000000	val: 0.943353	test: 0.761597

Epoch: 122
Loss: 0.05852045289040797
ROC train: 1.000000	val: 0.715385	test: 0.769605
PRC train: 1.000000	val: 0.946076	test: 0.767242

Epoch: 123
Loss: 0.051582622668595736
ROC train: 1.000000	val: 0.712821	test: 0.756390
PRC train: 1.000000	val: 0.945707	test: 0.757650

Epoch: 124
Loss: 0.0513491500791739
ROC train: 1.000000	val: 0.712821	test: 0.751174
PRC train: 1.000000	val: 0.944849	test: 0.745539

Epoch: 125
Loss: 0.053420352085443955
ROC train: 1.000000	val: 0.706960	test: 0.766475
PRC train: 1.000000	val: 0.942271	test: 0.748921

Epoch: 126
Loss: 0.057209073885511955
ROC train: 1.000000	val: 0.709890	test: 0.782646
PRC train: 1.000000	val: 0.943326	test: 0.768711

Epoch: 127
Loss: 0.044875527386853374
ROC train: 1.000000	val: 0.712454	test: 0.788559
PRC train: 1.000000	val: 0.943453	test: 0.786537

Epoch: 128
Loss: 0.055216823933344684
ROC train: 1.000000	val: 0.725641	test: 0.772909
PRC train: 1.000000	val: 0.948254	test: 0.765284

Epoch: 129
Loss: 0.05997374816032246
ROC train: 1.000000	val: 0.738828	test: 0.778473
PRC train: 1.000000	val: 0.951173	test: 0.766572

Epoch: 130
Loss: 0.04964095980337933
ROC train: 1.000000	val: 0.720879	test: 0.778647
PRC train: 1.000000	val: 0.944971	test: 0.780846

Epoch: 131
Loss: 0.06666012113264438
ROC train: 1.000000	val: 0.735165	test: 0.778995
PRC train: 1.000000	val: 0.950210	test: 0.784479

Epoch: 132
Loss: 0.04033381690899003
ROC train: 1.000000	val: 0.728205	test: 0.754477
PRC train: 1.000000	val: 0.949813	test: 0.749170

Epoch: 133
Loss: 0.051267992049709686
ROC train: 1.000000	val: 0.682051	test: 0.744045
PRC train: 1.000000	val: 0.935519	test: 0.738903

Epoch: 134
Loss: 0.04770437578805225
ROC train: 1.000000	val: 0.724908	test: 0.746479
PRC train: 1.000000	val: 0.948662	test: 0.753233

Epoch: 135
Loss: 0.05480168364848027
ROC train: 1.000000	val: 0.741026	test: 0.771344
PRC train: 1.000000	val: 0.952989	test: 0.769249

Epoch: 136
Loss: 0.05382425191215674
ROC train: 1.000000	val: 0.735897	test: 0.774822
PRC train: 1.000000	val: 0.948596	test: 0.763398

Epoch: 137
Loss: 0.05487216659372827
ROC train: 1.000000	val: 0.745788	test: 0.765084
PRC train: 1.000000	val: 0.951801	test: 0.769949

Epoch: 138
Loss: 0.058279904918020466
ROC train: 1.000000	val: 0.716850	test: 0.763172
PRC train: 1.000000	val: 0.946848	test: 0.757659

Epoch: 139
Loss: 0.0691813619463028
ROC train: 1.000000	val: 0.706960	test: 0.748913
PRC train: 1.000000	val: 0.944500	test: 0.755937

Epoch: 140
Loss: 0.050739945701460884
ROC train: 1.000000	val: 0.687912	test: 0.762302
PRC train: 1.000000	val: 0.938080	test: 0.753351

Epoch: 141
Loss: 0.056024453092791617
ROC train: 1.000000	val: 0.696337	test: 0.765780
PRC train: 1.000000	val: 0.941396	test: 0.757940

Epoch: 142
Loss: 0.06993035282121143
ROC train: 1.000000	val: 0.706227	test: 0.768736
PRC train: 1.000000	val: 0.941225	test: 0.755055

Early stopping
Best (ROC):	 train: 0.999977	val: 0.779853	test: 0.771170
Best (PRC):	 train: 0.999965	val: 0.961352	test: 0.743458
All runs completed.

PRC train: 1.000000	val: 0.946736	test: 0.763635

Epoch: 94
Loss: 0.07319309973505324
ROC train: 1.000000	val: 0.710989	test: 0.766823
PRC train: 1.000000	val: 0.943103	test: 0.749144

Epoch: 95
Loss: 0.07813511265404793
ROC train: 1.000000	val: 0.707326	test: 0.755173
PRC train: 1.000000	val: 0.943394	test: 0.745686

Epoch: 96
Loss: 0.07039796230755001
ROC train: 1.000000	val: 0.700366	test: 0.771344
PRC train: 1.000000	val: 0.938992	test: 0.758224

Epoch: 97
Loss: 0.05967018484806745
ROC train: 1.000000	val: 0.689744	test: 0.774996
PRC train: 1.000000	val: 0.937497	test: 0.757822

Epoch: 98
Loss: 0.07857243101460418
ROC train: 1.000000	val: 0.697070	test: 0.761085
PRC train: 1.000000	val: 0.939857	test: 0.745375

Epoch: 99
Loss: 0.05950295200659321
ROC train: 1.000000	val: 0.701099	test: 0.752043
PRC train: 1.000000	val: 0.941394	test: 0.739545

Epoch: 100
Loss: 0.050274888340663924
ROC train: 1.000000	val: 0.715018	test: 0.769779
PRC train: 1.000000	val: 0.944790	test: 0.746761

Epoch: 101
Loss: 0.057126502997240355
ROC train: 1.000000	val: 0.722344	test: 0.764389
PRC train: 1.000000	val: 0.947166	test: 0.732248

Epoch: 102
Loss: 0.0713834376364941
ROC train: 1.000000	val: 0.707692	test: 0.759172
PRC train: 1.000000	val: 0.944680	test: 0.752562

Epoch: 103
Loss: 0.04581372522620211
ROC train: 1.000000	val: 0.695604	test: 0.753782
PRC train: 1.000000	val: 0.942303	test: 0.745235

Epoch: 104
Loss: 0.06504709913906168
ROC train: 1.000000	val: 0.709890	test: 0.764389
PRC train: 1.000000	val: 0.946123	test: 0.757723

Epoch: 105
Loss: 0.0602175343094513
ROC train: 1.000000	val: 0.719780	test: 0.744914
PRC train: 1.000000	val: 0.947882	test: 0.713319

Epoch: 106
Loss: 0.053956235117032625
ROC train: 1.000000	val: 0.736996	test: 0.742132
PRC train: 1.000000	val: 0.951563	test: 0.714940

Epoch: 107
Loss: 0.0559088134236974
ROC train: 1.000000	val: 0.731868	test: 0.740219
PRC train: 1.000000	val: 0.949870	test: 0.711749

Epoch: 108
Loss: 0.04547374363700484
ROC train: 1.000000	val: 0.735531	test: 0.757086
PRC train: 1.000000	val: 0.950724	test: 0.745139

Epoch: 109
Loss: 0.04858488251031605
ROC train: 1.000000	val: 0.743956	test: 0.777778
PRC train: 1.000000	val: 0.950965	test: 0.771338

Epoch: 110
Loss: 0.05756735369295714
ROC train: 1.000000	val: 0.726374	test: 0.781777
PRC train: 1.000000	val: 0.947089	test: 0.777667

Epoch: 111
Loss: 0.05633233046466929
ROC train: 1.000000	val: 0.691209	test: 0.766475
PRC train: 1.000000	val: 0.940347	test: 0.762193

Epoch: 112
Loss: 0.05307359266688365
ROC train: 0.999923	val: 0.669231	test: 0.778299
PRC train: 0.999880	val: 0.931888	test: 0.759891

Epoch: 113
Loss: 0.05107890093375084
ROC train: 0.999983	val: 0.685348	test: 0.781082
PRC train: 0.999974	val: 0.935401	test: 0.763422

Epoch: 114
Loss: 0.06284725235690139
ROC train: 1.000000	val: 0.728205	test: 0.762824
PRC train: 1.000000	val: 0.948705	test: 0.755129

Epoch: 115
Loss: 0.057256537465608034
ROC train: 1.000000	val: 0.739927	test: 0.775170
PRC train: 1.000000	val: 0.952797	test: 0.772008

Epoch: 116
Loss: 0.06214042740721313
ROC train: 1.000000	val: 0.723443	test: 0.789776
PRC train: 1.000000	val: 0.948553	test: 0.783909

Epoch: 117
Loss: 0.026627387150069348
ROC train: 1.000000	val: 0.727473	test: 0.788211
PRC train: 1.000000	val: 0.948401	test: 0.772715

Epoch: 118
Loss: 0.050415382420753385
ROC train: 1.000000	val: 0.726740	test: 0.772909
PRC train: 1.000000	val: 0.948011	test: 0.762385

Epoch: 119
Loss: 0.04165220184140378
ROC train: 1.000000	val: 0.713919	test: 0.770127
PRC train: 1.000000	val: 0.945105	test: 0.776391

Epoch: 120
Loss: 0.04945621889720171
ROC train: 1.000000	val: 0.700733	test: 0.766997
PRC train: 1.000000	val: 0.942044	test: 0.772102

Epoch: 121
Loss: 0.04227202237108224
ROC train: 1.000000	val: 0.706593	test: 0.763172
PRC train: 1.000000	val: 0.942892	test: 0.760513

Epoch: 122
Loss: 0.04153779467906247
ROC train: 1.000000	val: 0.693407	test: 0.785429
PRC train: 1.000000	val: 0.936417	test: 0.782771

Epoch: 123
Loss: 0.061155224387118304
ROC train: 1.000000	val: 0.686081	test: 0.793949
PRC train: 1.000000	val: 0.936178	test: 0.785575

Epoch: 124
Loss: 0.05516686657960627
ROC train: 1.000000	val: 0.710256	test: 0.765780
PRC train: 1.000000	val: 0.944845	test: 0.759749

Epoch: 125
Loss: 0.04340067855651363
ROC train: 1.000000	val: 0.721612	test: 0.775170
PRC train: 1.000000	val: 0.947117	test: 0.769946

Epoch: 126
Loss: 0.03846727007509286
ROC train: 1.000000	val: 0.714652	test: 0.787167
PRC train: 1.000000	val: 0.941237	test: 0.780158

Epoch: 127
Loss: 0.03969188568516994
ROC train: 1.000000	val: 0.701832	test: 0.776213
PRC train: 1.000000	val: 0.935607	test: 0.769411

Epoch: 128
Loss: 0.03452223283241605
ROC train: 1.000000	val: 0.702564	test: 0.778647
PRC train: 1.000000	val: 0.938246	test: 0.779820

Epoch: 129
Loss: 0.04693547433469698
ROC train: 1.000000	val: 0.683883	test: 0.786820
PRC train: 1.000000	val: 0.933043	test: 0.788226

Epoch: 130
Loss: 0.032740685023117236
ROC train: 1.000000	val: 0.701099	test: 0.775691
PRC train: 1.000000	val: 0.940662	test: 0.774895

Epoch: 131
Loss: 0.05083069495838945
ROC train: 1.000000	val: 0.689377	test: 0.765780
PRC train: 1.000000	val: 0.938234	test: 0.752217

Epoch: 132
Loss: 0.056421185017458894
ROC train: 1.000000	val: 0.660440	test: 0.764041
PRC train: 1.000000	val: 0.929677	test: 0.726383

Epoch: 133
Loss: 0.04775279251695613
ROC train: 1.000000	val: 0.667033	test: 0.772387
PRC train: 1.000000	val: 0.929598	test: 0.751168

Epoch: 134
Loss: 0.044296501518185545
ROC train: 1.000000	val: 0.702564	test: 0.769431
PRC train: 1.000000	val: 0.942162	test: 0.755657

Epoch: 135
Loss: 0.030043480479965284
ROC train: 1.000000	val: 0.708791	test: 0.763172
PRC train: 1.000000	val: 0.943148	test: 0.753993

Epoch: 136
Loss: 0.0439106885887769
ROC train: 1.000000	val: 0.702198	test: 0.758477
PRC train: 1.000000	val: 0.937920	test: 0.764898

Epoch: 137
Loss: 0.05376052940052038
ROC train: 1.000000	val: 0.712454	test: 0.768388
PRC train: 1.000000	val: 0.945134	test: 0.769872

Epoch: 138
Loss: 0.035305190677762874
ROC train: 1.000000	val: 0.706960	test: 0.775691
PRC train: 1.000000	val: 0.944899	test: 0.765964

Epoch: 139
Loss: 0.05020869091914941
ROC train: 1.000000	val: 0.708791	test: 0.767866
PRC train: 1.000000	val: 0.946384	test: 0.756170

Epoch: 140
Loss: 0.04189729665769063
ROC train: 1.000000	val: 0.694139	test: 0.782994
PRC train: 1.000000	val: 0.938997	test: 0.779822

Epoch: 141
Loss: 0.030602007859725982
ROC train: 1.000000	val: 0.689377	test: 0.782125
PRC train: 1.000000	val: 0.935619	test: 0.777932

Epoch: 142
Loss: 0.03692547590554013
ROC train: 1.000000	val: 0.720147	test: 0.771170
PRC train: 1.000000	val: 0.943874	test: 0.753199

Epoch: 143
Loss: 0.04608864151607607
ROC train: 1.000000	val: 0.724176	test: 0.730308
PRC train: 1.000000	val: 0.945087	test: 0.678054

Epoch: 144
Loss: 0.03959528415190927
ROC train: 1.000000	val: 0.729670	test: 0.741436
PRC train: 1.000000	val: 0.945676	test: 0.713041

Early stopping
Best (ROC):	 train: 1.000000	val: 0.743956	test: 0.777778
Best (PRC):	 train: 1.000000	val: 0.950965	test: 0.771338
All runs completed.

PRC train: 0.999541	val: 0.925176	test: 0.762185

Epoch: 94
Loss: 0.11258307294791696
ROC train: 0.999486	val: 0.684615	test: 0.756738
PRC train: 0.999346	val: 0.929442	test: 0.765430

Epoch: 95
Loss: 0.11168516038346663
ROC train: 0.999349	val: 0.698535	test: 0.753086
PRC train: 0.999258	val: 0.935939	test: 0.751528

Epoch: 96
Loss: 0.11905462526171628
ROC train: 0.999278	val: 0.668498	test: 0.718310
PRC train: 0.999146	val: 0.926933	test: 0.733150

Epoch: 97
Loss: 0.12818761009236448
ROC train: 0.999580	val: 0.687179	test: 0.727004
PRC train: 0.999472	val: 0.931000	test: 0.737818

Epoch: 98
Loss: 0.09171353031799828
ROC train: 0.997486	val: 0.680586	test: 0.721440
PRC train: 0.997072	val: 0.929989	test: 0.740538

Epoch: 99
Loss: 0.10991210335246415
ROC train: 0.996884	val: 0.682051	test: 0.685794
PRC train: 0.996147	val: 0.930340	test: 0.705114

Epoch: 100
Loss: 0.12298717148292963
ROC train: 0.999298	val: 0.684249	test: 0.724917
PRC train: 0.999132	val: 0.926921	test: 0.738151

Epoch: 101
Loss: 0.1103651820331207
ROC train: 0.999569	val: 0.705495	test: 0.722657
PRC train: 0.999487	val: 0.932119	test: 0.719418

Epoch: 102
Loss: 0.08092454084694235
ROC train: 0.999015	val: 0.735531	test: 0.683533
PRC train: 0.998758	val: 0.941483	test: 0.693359

Epoch: 103
Loss: 0.08979893837925106
ROC train: 0.999309	val: 0.730403	test: 0.689098
PRC train: 0.999168	val: 0.940039	test: 0.698051

Epoch: 104
Loss: 0.09681995071189767
ROC train: 0.999098	val: 0.727839	test: 0.725961
PRC train: 0.998755	val: 0.938312	test: 0.732074

Epoch: 105
Loss: 0.09608467697771718
ROC train: 0.999478	val: 0.703297	test: 0.697096
PRC train: 0.999397	val: 0.935982	test: 0.708082

Epoch: 106
Loss: 0.08517345031512367
ROC train: 0.998545	val: 0.685348	test: 0.693097
PRC train: 0.998056	val: 0.931587	test: 0.667546

Epoch: 107
Loss: 0.10069097815461321
ROC train: 0.999295	val: 0.689377	test: 0.712746
PRC train: 0.999146	val: 0.931013	test: 0.684998

Epoch: 108
Loss: 0.0911424915207214
ROC train: 0.999418	val: 0.684615	test: 0.706312
PRC train: 0.999232	val: 0.931871	test: 0.701169

Epoch: 109
Loss: 0.09723943295416823
ROC train: 0.999735	val: 0.672527	test: 0.723005
PRC train: 0.999616	val: 0.929672	test: 0.731387

Epoch: 110
Loss: 0.10134301474580831
ROC train: 0.999720	val: 0.708425	test: 0.739871
PRC train: 0.999583	val: 0.937932	test: 0.748237

Epoch: 111
Loss: 0.0738824614552901
ROC train: 0.999837	val: 0.710256	test: 0.728395
PRC train: 0.999771	val: 0.939366	test: 0.740068

Epoch: 112
Loss: 0.07845051785774666
ROC train: 0.999852	val: 0.713187	test: 0.713093
PRC train: 0.999796	val: 0.937987	test: 0.732824

Epoch: 113
Loss: 0.099475346345305
ROC train: 0.999840	val: 0.704029	test: 0.712224
PRC train: 0.999782	val: 0.934454	test: 0.735402

Epoch: 114
Loss: 0.09522010178130455
ROC train: 0.999666	val: 0.668864	test: 0.720744
PRC train: 0.999545	val: 0.924656	test: 0.742359

Epoch: 115
Loss: 0.08696646161934635
ROC train: 0.999729	val: 0.671062	test: 0.731699
PRC train: 0.999627	val: 0.924898	test: 0.736971

Epoch: 116
Loss: 0.08659906310888192
ROC train: 0.999795	val: 0.674359	test: 0.717093
PRC train: 0.999721	val: 0.927928	test: 0.730226

Epoch: 117
Loss: 0.08155942257558577
ROC train: 0.999626	val: 0.713553	test: 0.699009
PRC train: 0.999518	val: 0.936251	test: 0.717550

Epoch: 118
Loss: 0.06465239662495095
ROC train: 0.999672	val: 0.715018	test: 0.696227
PRC train: 0.999548	val: 0.936262	test: 0.703184

Epoch: 119
Loss: 0.07286812369048627
ROC train: 0.999912	val: 0.706593	test: 0.729612
PRC train: 0.999874	val: 0.931055	test: 0.738394

Epoch: 120
Loss: 0.06764017078918189
ROC train: 0.999834	val: 0.702930	test: 0.721614
PRC train: 0.999755	val: 0.929125	test: 0.730730

Epoch: 121
Loss: 0.07242286380277115
ROC train: 0.999780	val: 0.708059	test: 0.701269
PRC train: 0.999701	val: 0.934043	test: 0.711414

Epoch: 122
Loss: 0.06711359369108616
ROC train: 0.999720	val: 0.700366	test: 0.684403
PRC train: 0.999615	val: 0.935814	test: 0.712758

Epoch: 123
Loss: 0.06431005029496868
ROC train: 0.999906	val: 0.708425	test: 0.721266
PRC train: 0.999866	val: 0.936876	test: 0.736360

Epoch: 124
Loss: 0.07488367379093369
ROC train: 0.999937	val: 0.732967	test: 0.742306
PRC train: 0.999908	val: 0.942349	test: 0.748416

Epoch: 125
Loss: 0.08053020489525138
ROC train: 0.999932	val: 0.738462	test: 0.715354
PRC train: 0.999897	val: 0.942514	test: 0.722515

Epoch: 126
Loss: 0.07513333201980024
ROC train: 0.999991	val: 0.714286	test: 0.745436
PRC train: 0.999987	val: 0.933921	test: 0.755861

Epoch: 127
Loss: 0.0629756423041201
ROC train: 0.999923	val: 0.692674	test: 0.736394
PRC train: 0.999889	val: 0.930538	test: 0.744708

Epoch: 128
Loss: 0.06159616317890043
ROC train: 0.999820	val: 0.693407	test: 0.705616
PRC train: 0.999751	val: 0.935762	test: 0.716496

Epoch: 129
Loss: 0.056008025650664296
ROC train: 0.999923	val: 0.712088	test: 0.695531
PRC train: 0.999889	val: 0.940356	test: 0.708521

Epoch: 130
Loss: 0.06088919048719281
ROC train: 0.999809	val: 0.696703	test: 0.680056
PRC train: 0.999745	val: 0.933686	test: 0.689443

Epoch: 131
Loss: 0.06581715748652088
ROC train: 0.999869	val: 0.675458	test: 0.675882
PRC train: 0.999818	val: 0.927679	test: 0.685419

Epoch: 132
Loss: 0.054590814340287466
ROC train: 0.999909	val: 0.680952	test: 0.701791
PRC train: 0.999868	val: 0.930886	test: 0.704917

Epoch: 133
Loss: 0.07360638729767191
ROC train: 0.999966	val: 0.715751	test: 0.695705
PRC train: 0.999949	val: 0.939485	test: 0.694996

Epoch: 134
Loss: 0.06602785680359914
ROC train: 0.999994	val: 0.712821	test: 0.738480
PRC train: 0.999991	val: 0.938004	test: 0.748565

Epoch: 135
Loss: 0.05603356624473742
ROC train: 0.999966	val: 0.714652	test: 0.747696
PRC train: 0.999949	val: 0.935538	test: 0.766139

Epoch: 136
Loss: 0.0681567036652737
ROC train: 0.999986	val: 0.724542	test: 0.726135
PRC train: 0.999979	val: 0.936826	test: 0.738904

Epoch: 137
Loss: 0.06751447530785848
ROC train: 1.000000	val: 0.714652	test: 0.731699
PRC train: 1.000000	val: 0.934390	test: 0.738441

Epoch: 138
Loss: 0.05780839017129208
ROC train: 0.999983	val: 0.695604	test: 0.720570
PRC train: 0.999974	val: 0.929760	test: 0.716245

Epoch: 139
Loss: 0.05551092058965088
ROC train: 0.999960	val: 0.708425	test: 0.704051
PRC train: 0.999940	val: 0.936568	test: 0.702916

Epoch: 140
Loss: 0.06547764111479175
ROC train: 0.999658	val: 0.709158	test: 0.681099
PRC train: 0.999474	val: 0.937164	test: 0.683824

Epoch: 141
Loss: 0.07812469917615668
ROC train: 0.999969	val: 0.700000	test: 0.712572
PRC train: 0.999953	val: 0.932029	test: 0.720160

Epoch: 142
Loss: 0.05024941780097834
ROC train: 0.999991	val: 0.696703	test: 0.736046
PRC train: 0.999987	val: 0.931323	test: 0.745397

Epoch: 143
Loss: 0.06174515165031509
ROC train: 1.000000	val: 0.700000	test: 0.760911
PRC train: 1.000000	val: 0.932428	test: 0.765634

Epoch: 144
Loss: 0.05291464801920056
ROC train: 1.000000	val: 0.682051	test: 0.759346
PRC train: 1.000000	val: 0.927189	test: 0.760391

Epoch: 145
Loss: 0.05095127236916032
ROC train: 0.999994	val: 0.695971	test: 0.722831
PRC train: 0.999991	val: 0.934946	test: 0.729291

Epoch: 146
Loss: 0.04611444030924729
ROC train: 0.999997	val: 0.712088	test: 0.724917
PRC train: 0.999996	val: 0.939144	test: 0.718172

Epoch: 147
Loss: 0.05815462824800669
ROC train: 1.000000	val: 0.723443	test: 0.726308
PRC train: 1.000000	val: 0.942375	test: 0.739018

Epoch: 148
Loss: 0.0683220896684259
ROC train: 0.999986	val: 0.709158	test: 0.742132
PRC train: 0.999979	val: 0.938966	test: 0.751054

Epoch: 149
Loss: 0.0655519686811609
ROC train: 0.999977	val: 0.694872	test: 0.751174
PRC train: 0.999966	val: 0.934723	test: 0.751041

Epoch: 150
Loss: 0.07006498127528235
ROC train: 0.999989	val: 0.698168	test: 0.742827
PRC train: 0.999983	val: 0.933535	test: 0.755001

Epoch: 151
Loss: 0.06735505916021405
ROC train: 1.000000	val: 0.715018	test: 0.729091
PRC train: 1.000000	val: 0.936759	test: 0.736040

Epoch: 152
Loss: 0.055488253741106074
ROC train: 0.999991	val: 0.707692	test: 0.712746
PRC train: 0.999987	val: 0.936824	test: 0.713365

Epoch: 153
Loss: 0.0574638564121742
ROC train: 1.000000	val: 0.704396	test: 0.720570
PRC train: 1.000000	val: 0.937121	test: 0.716687

Epoch: 154
Loss: 0.06422127935410357
ROC train: 1.000000	val: 0.707326	test: 0.739697
PRC train: 1.000000	val: 0.936445	test: 0.733175

Epoch: 155
Loss: 0.05937659888927147
ROC train: 0.999997	val: 0.697802	test: 0.732220
PRC train: 0.999996	val: 0.934618	test: 0.733557

Epoch: 156
Loss: 0.053950365900587424
ROC train: 0.999986	val: 0.713553	test: 0.730829
PRC train: 0.999979	val: 0.936937	test: 0.737575

Epoch: 157
Loss: 0.07331416689017768
ROC train: 1.000000	val: 0.735531	test: 0.740915
PRC train: 1.000000	val: 0.936827	test: 0.738794

Epoch: 158
Loss: 0.05646301073714992
ROC train: 0.999977	val: 0.706227	test: 0.700226
PRC train: 0.999966	val: 0.929458	test: 0.703151

Epoch: 159
Loss: 0.05404488730642572
ROC train: 0.999364	val: 0.653114	test: 0.682316
PRC train: 0.999112	val: 0.920265	test: 0.701872

Epoch: 160
Loss: 0.0587014220635804
ROC train: 0.999994	val: 0.678022	test: 0.738306
PRC train: 0.999991	val: 0.928240	test: 0.747974

Early stopping
Best (ROC):	 train: 0.999932	val: 0.738462	test: 0.715354
Best (PRC):	 train: 0.999897	val: 0.942514	test: 0.722515
All runs completed.
