>>> Starting run for dataset: tox21
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running RANDOM configs_static_noise_experiments/GraphMVP/tox21/noise=0.0.yml on cuda:0
Running RANDOM configs_static_noise_experiments/GraphMVP/tox21/noise=0.05.yml on cuda:1
Running RANDOM configs_static_noise_experiments/GraphMVP/tox21/noise=0.1.yml on cuda:2
Running RANDOM configs_static_noise_experiments/GraphMVP/tox21/noise=0.2.yml on cuda:3
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphMVP/tox21/noise=0.2.yml --runseed 4 --device cuda:3
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphMVP/tox21/noise=0.2.yml --runseed 5 --device cuda:3
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphMVP/tox21/noise=0.1.yml --runseed 4 --device cuda:2
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphMVP/tox21/noise=0.2.yml --runseed 6 --device cuda:3
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphMVP/tox21/noise=0.1.yml --runseed 5 --device cuda:2
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphMVP/tox21/noise=0.1.yml --runseed 6 --device cuda:2
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphMVP/tox21/noise=0.05.yml --runseed 4 --device cuda:1
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphMVP/tox21/noise=0.05.yml --runseed 5 --device cuda:1
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphMVP/tox21/noise=0.05.yml --runseed 6 --device cuda:1
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphMVP/tox21/noise=0.0.yml --runseed 4 --device cuda:0
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphMVP/tox21/noise=0.0.yml --runseed 5 --device cuda:0
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphMVP/tox21/noise=0.0.yml --runseed 6 --device cuda:0
[11:20:03] WARNING: not removing hydrogen atom without neighbors
[11:20:03] WARNING: not removing hydrogen atom without neighbors
[11:20:03] WARNING: not removing hydrogen atom without neighbors
[11:20:04] WARNING: not removing hydrogen atom without neighbors
[11:20:04] WARNING: not removing hydrogen atom without neighbors
[11:20:04] WARNING: not removing hydrogen atom without neighbors
[11:20:04] WARNING: not removing hydrogen atom without neighbors
[11:20:05] WARNING: not removing hydrogen atom without neighbors
[11:20:05] WARNING: not removing hydrogen atom without neighbors
[11:20:05] WARNING: not removing hydrogen atom without neighbors
[11:20:06] WARNING: not removing hydrogen atom without neighbors
[11:20:06] WARNING: not removing hydrogen atom without neighbors
[11:20:06] WARNING: not removing hydrogen atom without neighbors
[11:20:06] WARNING: not removing hydrogen atom without neighbors
[11:20:06] WARNING: not removing hydrogen atom without neighbors
[11:20:06] WARNING: not removing hydrogen atom without neighbors
[11:20:07] WARNING: not removing hydrogen atom without neighbors
[11:20:07] WARNING: not removing hydrogen atom without neighbors
[11:20:07] WARNING: not removing hydrogen atom without neighbors
[11:20:08] WARNING: not removing hydrogen atom without neighbors
[11:20:08] WARNING: not removing hydrogen atom without neighbors
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/tox21/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/static-noise/GraphMVP/tox21/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/tox21/noise=0.0/tox21_scaff_6_26-05_11-20-03  ]
[ Using Seed :  6  ]
[ Using device :  cuda:0  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5489919536671742
ROC train: 0.703130	val: 0.619441	test: 0.578012
PRC train: 0.179271	val: 0.164254	test: 0.163167

Epoch: 2
Loss: 0.3538415905326897
ROC train: 0.750416	val: 0.674813	test: 0.631794
PRC train: 0.246376	val: 0.221227	test: 0.217572

Epoch: 3
Loss: 0.24964338478100412
ROC train: 0.783247	val: 0.727317	test: 0.673612
PRC train: 0.309001	val: 0.274597	test: 0.276471

Epoch: 4
Loss: 0.21318272163842394
ROC train: 0.805684	val: 0.735124	test: 0.699157
PRC train: 0.329551	val: 0.282350	test: 0.280067

Epoch: 5
Loss: 0.19873523602816234
ROC train: 0.823215	val: 0.741195	test: 0.704708
PRC train: 0.351507	val: 0.305142	test: 0.299362

Epoch: 6
Loss: 0.19465128918622135
ROC train: 0.823320	val: 0.750860	test: 0.723133
PRC train: 0.368661	val: 0.330526	test: 0.313828

Epoch: 7
Loss: 0.1914429882804942
ROC train: 0.835202	val: 0.755163	test: 0.717342
PRC train: 0.386523	val: 0.328505	test: 0.322605

Epoch: 8
Loss: 0.18949170937864057
ROC train: 0.839715	val: 0.755338	test: 0.729670
PRC train: 0.397222	val: 0.344744	test: 0.328248

Epoch: 9
Loss: 0.1848661804435969
ROC train: 0.855578	val: 0.753480	test: 0.732794
PRC train: 0.428521	val: 0.335139	test: 0.337448

Epoch: 10
Loss: 0.18294013883758
ROC train: 0.857288	val: 0.749774	test: 0.714553
PRC train: 0.444169	val: 0.330012	test: 0.322073

Epoch: 11
Loss: 0.18375179104196415
ROC train: 0.860123	val: 0.747030	test: 0.723807
PRC train: 0.451130	val: 0.325855	test: 0.334827

Epoch: 12
Loss: 0.1791120211554593
ROC train: 0.857675	val: 0.747455	test: 0.725172
PRC train: 0.440744	val: 0.315795	test: 0.316808

Epoch: 13
Loss: 0.17675058876342462
ROC train: 0.861022	val: 0.757224	test: 0.725320
PRC train: 0.473259	val: 0.338380	test: 0.341485

Epoch: 14
Loss: 0.17838465278980675
ROC train: 0.870797	val: 0.757050	test: 0.713628
PRC train: 0.488864	val: 0.322751	test: 0.330217

Epoch: 15
Loss: 0.17520965062765792
ROC train: 0.874247	val: 0.759880	test: 0.725117
PRC train: 0.498878	val: 0.333039	test: 0.323087

Epoch: 16
Loss: 0.17269336745959843
ROC train: 0.876851	val: 0.755995	test: 0.730301
PRC train: 0.506757	val: 0.328539	test: 0.344210

Epoch: 17
Loss: 0.17191008330174853
ROC train: 0.881298	val: 0.768245	test: 0.726968
PRC train: 0.523045	val: 0.350064	test: 0.347849

Epoch: 18
Loss: 0.17007125103139115
ROC train: 0.885973	val: 0.768126	test: 0.732305
PRC train: 0.530199	val: 0.342226	test: 0.345030

Epoch: 19
Loss: 0.16899237772804465
ROC train: 0.882576	val: 0.764578	test: 0.736130
PRC train: 0.523303	val: 0.339698	test: 0.353607

Epoch: 20
Loss: 0.16822308732871147
ROC train: 0.890864	val: 0.763085	test: 0.729950
PRC train: 0.546467	val: 0.349171	test: 0.340327

Epoch: 21
Loss: 0.16880986560298197
ROC train: 0.890543	val: 0.768845	test: 0.733438
PRC train: 0.542138	val: 0.346208	test: 0.353375

Epoch: 22
Loss: 0.1654868712510667
ROC train: 0.894207	val: 0.764324	test: 0.736794
PRC train: 0.554580	val: 0.347917	test: 0.346347

Epoch: 23
Loss: 0.1646457466609258
ROC train: 0.896166	val: 0.759910	test: 0.731479
PRC train: 0.560640	val: 0.346237	test: 0.346886

Epoch: 24
Loss: 0.16389209409946592
ROC train: 0.890959	val: 0.774285	test: 0.732286
PRC train: 0.546586	val: 0.356081	test: 0.342524

Epoch: 25
Loss: 0.16230317149333257
ROC train: 0.899855	val: 0.765802	test: 0.734253
PRC train: 0.566306	val: 0.341132	test: 0.354570

Epoch: 26
Loss: 0.16065167747524497
ROC train: 0.896347	val: 0.759714	test: 0.733551
PRC train: 0.563610	val: 0.359946	test: 0.354677

Epoch: 27
Loss: 0.16240610337073533
ROC train: 0.902411	val: 0.778329	test: 0.738397
PRC train: 0.581190	val: 0.366955	test: 0.360933

Epoch: 28
Loss: 0.16048421630489912
ROC train: 0.905967	val: 0.768787	test: 0.735868
PRC train: 0.588110	val: 0.345978	test: 0.350831

Epoch: 29
Loss: 0.15961586763368166
ROC train: 0.903638	val: 0.765862	test: 0.726388
PRC train: 0.576969	val: 0.339327	test: 0.343578

Epoch: 30
Loss: 0.1566472886488578
ROC train: 0.907776	val: 0.774920	test: 0.734869
PRC train: 0.596846	val: 0.362730	test: 0.354296

Epoch: 31
Loss: 0.15917717000597786
ROC train: 0.910418	val: 0.777044	test: 0.742701
PRC train: 0.599968	val: 0.365409	test: 0.362055

Epoch: 32
Loss: 0.15597645429082743
ROC train: 0.913531	val: 0.775888	test: 0.739229
PRC train: 0.612287	val: 0.364665	test: 0.358973

Epoch: 33
Loss: 0.15437397983433443Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/tox21/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/static-noise/GraphMVP/tox21/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/tox21/noise=0.0/tox21_scaff_4_26-05_11-20-03  ]
[ Using Seed :  4  ]
[ Using device :  cuda:0  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.558594192123071
ROC train: 0.695145	val: 0.610309	test: 0.568401
PRC train: 0.161519	val: 0.145548	test: 0.144582

Epoch: 2
Loss: 0.35547351281472217
ROC train: 0.758197	val: 0.690719	test: 0.657438
PRC train: 0.257574	val: 0.239664	test: 0.234942

Epoch: 3
Loss: 0.2519841414555622
ROC train: 0.795854	val: 0.734669	test: 0.692867
PRC train: 0.326015	val: 0.288380	test: 0.280739

Epoch: 4
Loss: 0.2111689288128795
ROC train: 0.802059	val: 0.744707	test: 0.704579
PRC train: 0.320644	val: 0.297146	test: 0.283014

Epoch: 5
Loss: 0.1989585564688522
ROC train: 0.817864	val: 0.735430	test: 0.704730
PRC train: 0.357996	val: 0.293540	test: 0.308946

Epoch: 6
Loss: 0.193202723993634
ROC train: 0.835260	val: 0.753121	test: 0.724137
PRC train: 0.387908	val: 0.314764	test: 0.316983

Epoch: 7
Loss: 0.1908798770178782
ROC train: 0.840913	val: 0.759359	test: 0.735276
PRC train: 0.403520	val: 0.343729	test: 0.332380

Epoch: 8
Loss: 0.1866529807692882
ROC train: 0.840089	val: 0.756046	test: 0.732245
PRC train: 0.392597	val: 0.347659	test: 0.331337

Epoch: 9
Loss: 0.18593347756035858
ROC train: 0.844423	val: 0.742175	test: 0.704583
PRC train: 0.413896	val: 0.311177	test: 0.320839

Epoch: 10
Loss: 0.18313089943864758
ROC train: 0.858868	val: 0.771491	test: 0.736109
PRC train: 0.437469	val: 0.349272	test: 0.344670

Epoch: 11
Loss: 0.17952960713014704
ROC train: 0.866133	val: 0.757587	test: 0.722311
PRC train: 0.460921	val: 0.335822	test: 0.341268

Epoch: 12
Loss: 0.17811539523911066
ROC train: 0.869058	val: 0.770383	test: 0.731744
PRC train: 0.477833	val: 0.342175	test: 0.342887

Epoch: 13
Loss: 0.1757173052673776
ROC train: 0.868676	val: 0.769625	test: 0.733228
PRC train: 0.483843	val: 0.350327	test: 0.346545

Epoch: 14
Loss: 0.17503980545572173
ROC train: 0.867200	val: 0.768412	test: 0.725094
PRC train: 0.472829	val: 0.349748	test: 0.341565

Epoch: 15
Loss: 0.17282958358843373
ROC train: 0.870302	val: 0.761456	test: 0.721954
PRC train: 0.494406	val: 0.315115	test: 0.325397

Epoch: 16
Loss: 0.17164359926663508
ROC train: 0.876444	val: 0.771052	test: 0.721416
PRC train: 0.506451	val: 0.345727	test: 0.351180

Epoch: 17
Loss: 0.17211935265698117
ROC train: 0.881791	val: 0.773002	test: 0.728033
PRC train: 0.524130	val: 0.349932	test: 0.351878

Epoch: 18
Loss: 0.16755639973730227
ROC train: 0.885113	val: 0.778392	test: 0.730905
PRC train: 0.534006	val: 0.355871	test: 0.360728

Epoch: 19
Loss: 0.16760738082148868
ROC train: 0.883840	val: 0.768612	test: 0.735957
PRC train: 0.529773	val: 0.338887	test: 0.348213

Epoch: 20
Loss: 0.16667573721059156
ROC train: 0.887184	val: 0.766819	test: 0.724475
PRC train: 0.529670	val: 0.339275	test: 0.341272

Epoch: 21
Loss: 0.16511252485300887
ROC train: 0.890396	val: 0.771395	test: 0.748695
PRC train: 0.552033	val: 0.352480	test: 0.357094

Epoch: 22
Loss: 0.1639884457242587
ROC train: 0.892868	val: 0.779053	test: 0.731834
PRC train: 0.554879	val: 0.351827	test: 0.346568

Epoch: 23
Loss: 0.1631915947425803
ROC train: 0.894026	val: 0.765898	test: 0.733500
PRC train: 0.551318	val: 0.353175	test: 0.330838

Epoch: 24
Loss: 0.16181010620444844
ROC train: 0.898267	val: 0.769238	test: 0.737282
PRC train: 0.565861	val: 0.351268	test: 0.356506

Epoch: 25
Loss: 0.16135076028967238
ROC train: 0.898366	val: 0.777796	test: 0.751617
PRC train: 0.579129	val: 0.362079	test: 0.364076

Epoch: 26
Loss: 0.16210378490300706
ROC train: 0.899982	val: 0.772061	test: 0.737230
PRC train: 0.565994	val: 0.355477	test: 0.343013

Epoch: 27
Loss: 0.1594093652538068
ROC train: 0.901361	val: 0.782754	test: 0.745256
PRC train: 0.575117	val: 0.376851	test: 0.355643

Epoch: 28
Loss: 0.15900842563854736
ROC train: 0.902402	val: 0.781068	test: 0.736142
PRC train: 0.584682	val: 0.360381	test: 0.350691

Epoch: 29
Loss: 0.15855091606102678
ROC train: 0.905981	val: 0.768370	test: 0.742844
PRC train: 0.586126	val: 0.355429	test: 0.346968

Epoch: 30
Loss: 0.15612325411066436
ROC train: 0.910597	val: 0.772701	test: 0.732931
PRC train: 0.602498	val: 0.357860	test: 0.344026

Epoch: 31
Loss: 0.15540508683475932
ROC train: 0.911367	val: 0.770661	test: 0.737880
PRC train: 0.604028	val: 0.358919	test: 0.341704

Epoch: 32
Loss: 0.15649494308775042
ROC train: 0.910102	val: 0.782579	test: 0.746136
PRC train: 0.607443	val: 0.371051	test: 0.358747

Epoch: 33
Loss: 0.15523952188013038Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/tox21/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/static-noise/GraphMVP/tox21/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/tox21/noise=0.0/tox21_scaff_5_26-05_11-20-03  ]
[ Using Seed :  5  ]
[ Using device :  cuda:0  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5431201632634127
ROC train: 0.725036	val: 0.662101	test: 0.616725
PRC train: 0.210578	val: 0.202658	test: 0.187550

Epoch: 2
Loss: 0.35122959759094663
ROC train: 0.740494	val: 0.663035	test: 0.632627
PRC train: 0.246300	val: 0.216712	test: 0.218638

Epoch: 3
Loss: 0.24747148667162083
ROC train: 0.786591	val: 0.725048	test: 0.670249
PRC train: 0.303114	val: 0.267835	test: 0.262094

Epoch: 4
Loss: 0.21064850653556777
ROC train: 0.806383	val: 0.742393	test: 0.692077
PRC train: 0.325927	val: 0.278157	test: 0.274749

Epoch: 5
Loss: 0.19856842754017456
ROC train: 0.821654	val: 0.751498	test: 0.710309
PRC train: 0.376327	val: 0.330436	test: 0.316250

Epoch: 6
Loss: 0.19353700018699538
ROC train: 0.824978	val: 0.752392	test: 0.708268
PRC train: 0.358268	val: 0.305762	test: 0.296285

Epoch: 7
Loss: 0.19158999810839103
ROC train: 0.834972	val: 0.755492	test: 0.718385
PRC train: 0.383148	val: 0.328241	test: 0.322953

Epoch: 8
Loss: 0.1869953389116263
ROC train: 0.842390	val: 0.753585	test: 0.726236
PRC train: 0.398720	val: 0.339647	test: 0.333552

Epoch: 9
Loss: 0.18686731794357075
ROC train: 0.842967	val: 0.743938	test: 0.710916
PRC train: 0.395186	val: 0.308335	test: 0.312356

Epoch: 10
Loss: 0.18388636309034717
ROC train: 0.856219	val: 0.755602	test: 0.718035
PRC train: 0.440089	val: 0.334484	test: 0.324623

Epoch: 11
Loss: 0.17977218422226574
ROC train: 0.859133	val: 0.759196	test: 0.718335
PRC train: 0.442378	val: 0.335507	test: 0.333677

Epoch: 12
Loss: 0.18019466691618483
ROC train: 0.861554	val: 0.764955	test: 0.723267
PRC train: 0.472703	val: 0.347521	test: 0.342452

Epoch: 13
Loss: 0.17888954885604705
ROC train: 0.864808	val: 0.761100	test: 0.724998
PRC train: 0.469711	val: 0.331230	test: 0.325915

Epoch: 14
Loss: 0.1765371346031056
ROC train: 0.865713	val: 0.742219	test: 0.707759
PRC train: 0.476795	val: 0.317254	test: 0.323461

Epoch: 15
Loss: 0.174893439622013
ROC train: 0.873554	val: 0.750036	test: 0.724209
PRC train: 0.505728	val: 0.340760	test: 0.330704

Epoch: 16
Loss: 0.17288314227228915
ROC train: 0.866100	val: 0.782531	test: 0.733100
PRC train: 0.480022	val: 0.359186	test: 0.345800

Epoch: 17
Loss: 0.17136847248411624
ROC train: 0.879086	val: 0.766478	test: 0.721107
PRC train: 0.508831	val: 0.346768	test: 0.327803

Epoch: 18
Loss: 0.17064726156652113
ROC train: 0.884714	val: 0.770082	test: 0.726293
PRC train: 0.537154	val: 0.349253	test: 0.334363

Epoch: 19
Loss: 0.16902128871522024
ROC train: 0.884732	val: 0.771826	test: 0.719422
PRC train: 0.531737	val: 0.352289	test: 0.325134

Epoch: 20
Loss: 0.16656375163555726
ROC train: 0.888425	val: 0.772456	test: 0.724375
PRC train: 0.544152	val: 0.346712	test: 0.342325

Epoch: 21
Loss: 0.1672403180483535
ROC train: 0.886669	val: 0.780928	test: 0.732092
PRC train: 0.537376	val: 0.366050	test: 0.340152

Epoch: 22
Loss: 0.16638638440128717
ROC train: 0.891733	val: 0.765710	test: 0.717119
PRC train: 0.550421	val: 0.327419	test: 0.310788

Epoch: 23
Loss: 0.16598945290646308
ROC train: 0.892694	val: 0.777068	test: 0.740311
PRC train: 0.554923	val: 0.354366	test: 0.348172

Epoch: 24
Loss: 0.16542464703249465
ROC train: 0.891214	val: 0.783523	test: 0.731524
PRC train: 0.549752	val: 0.374783	test: 0.333738

Epoch: 25
Loss: 0.16234523252991429
ROC train: 0.897637	val: 0.775908	test: 0.730108
PRC train: 0.562591	val: 0.363278	test: 0.337201

Epoch: 26
Loss: 0.1615672811186133
ROC train: 0.899209	val: 0.780471	test: 0.733972
PRC train: 0.576406	val: 0.373362	test: 0.351212

Epoch: 27
Loss: 0.15966255960205394
ROC train: 0.904369	val: 0.779803	test: 0.728217
PRC train: 0.588453	val: 0.373670	test: 0.342790

Epoch: 28
Loss: 0.15908481158196155
ROC train: 0.901722	val: 0.765452	test: 0.737960
PRC train: 0.575942	val: 0.357430	test: 0.349209

Epoch: 29
Loss: 0.1595272411849251
ROC train: 0.903464	val: 0.779691	test: 0.742711
PRC train: 0.594470	val: 0.368793	test: 0.351159

Epoch: 30
Loss: 0.15725764222785163
ROC train: 0.907901	val: 0.766946	test: 0.721703
PRC train: 0.589687	val: 0.360771	test: 0.339365

Epoch: 31
Loss: 0.1573163387321303
ROC train: 0.909967	val: 0.794238	test: 0.736889
PRC train: 0.601114	val: 0.372907	test: 0.351464

Epoch: 32
Loss: 0.1552001485757102
ROC train: 0.912754	val: 0.791231	test: 0.737050
PRC train: 0.608875	val: 0.384837	test: 0.352204

Epoch: 33
Loss: 0.15601115043905525Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/tox21/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:3
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/static-noise/GraphMVP/tox21/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/tox21/noise=0.2/tox21_scaff_6_26-05_11-20-03  ]
[ Using Seed :  6  ]
[ Using device :  cuda:3  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5690096325230679
ROC train: 0.690112	val: 0.566474	test: 0.564350
PRC train: 0.163159	val: 0.139425	test: 0.150048

Epoch: 2
Loss: 0.36463850946388876
ROC train: 0.734294	val: 0.696623	test: 0.661130
PRC train: 0.227652	val: 0.228351	test: 0.228624

Epoch: 3
Loss: 0.25783152241835
ROC train: 0.750508	val: 0.695379	test: 0.663646
PRC train: 0.257390	val: 0.251591	test: 0.237310

Epoch: 4
Loss: 0.22078869574117313
ROC train: 0.768652	val: 0.712787	test: 0.698303
PRC train: 0.285422	val: 0.280085	test: 0.267085

Epoch: 5
Loss: 0.21018769303828921
ROC train: 0.779247	val: 0.701928	test: 0.695843
PRC train: 0.299921	val: 0.269049	test: 0.261443

Epoch: 6
Loss: 0.20522083797785792
ROC train: 0.790167	val: 0.714662	test: 0.710529
PRC train: 0.307016	val: 0.287596	test: 0.283065

Epoch: 7
Loss: 0.20404412036785685
ROC train: 0.805663	val: 0.715732	test: 0.700893
PRC train: 0.332389	val: 0.290652	test: 0.274787

Epoch: 8
Loss: 0.19989197430225544
ROC train: 0.809112	val: 0.724108	test: 0.716076
PRC train: 0.339755	val: 0.307967	test: 0.295595

Epoch: 9
Loss: 0.19847877776293324
ROC train: 0.818974	val: 0.736434	test: 0.717346
PRC train: 0.354498	val: 0.316129	test: 0.301429

Epoch: 10
Loss: 0.1963629984900443
ROC train: 0.827946	val: 0.731118	test: 0.713957
PRC train: 0.377233	val: 0.317839	test: 0.296756

Epoch: 11
Loss: 0.1944618363052022
ROC train: 0.831343	val: 0.718329	test: 0.713788
PRC train: 0.377563	val: 0.288548	test: 0.293386

Epoch: 12
Loss: 0.19281561915966464
ROC train: 0.838145	val: 0.723328	test: 0.698680
PRC train: 0.385549	val: 0.296073	test: 0.287176

Epoch: 13
Loss: 0.19085307774323376
ROC train: 0.842948	val: 0.739492	test: 0.712320
PRC train: 0.396955	val: 0.305806	test: 0.303435

Epoch: 14
Loss: 0.18972354310293624
ROC train: 0.850498	val: 0.727980	test: 0.713459
PRC train: 0.419985	val: 0.302885	test: 0.305853

Epoch: 15
Loss: 0.18582489436491514
ROC train: 0.856795	val: 0.729496	test: 0.706775
PRC train: 0.430017	val: 0.306587	test: 0.294608

Epoch: 16
Loss: 0.18664420199411694
ROC train: 0.853836	val: 0.724003	test: 0.704596
PRC train: 0.428040	val: 0.305792	test: 0.301441

Epoch: 17
Loss: 0.18600629862131743
ROC train: 0.861412	val: 0.724151	test: 0.714632
PRC train: 0.445022	val: 0.314126	test: 0.308768

Epoch: 18
Loss: 0.18495908380737203
ROC train: 0.863767	val: 0.739245	test: 0.704481
PRC train: 0.444752	val: 0.310998	test: 0.295037

Epoch: 19
Loss: 0.1836922576906916
ROC train: 0.871400	val: 0.744133	test: 0.718504
PRC train: 0.473545	val: 0.315276	test: 0.313474

Epoch: 20
Loss: 0.18296245232277855
ROC train: 0.869926	val: 0.728338	test: 0.709492
PRC train: 0.459866	val: 0.308322	test: 0.289410

Epoch: 21
Loss: 0.17998659839212322
ROC train: 0.871173	val: 0.709085	test: 0.705261
PRC train: 0.476440	val: 0.306549	test: 0.298934

Epoch: 22
Loss: 0.1787970070021082
ROC train: 0.882245	val: 0.743660	test: 0.717250
PRC train: 0.506999	val: 0.327663	test: 0.317725

Epoch: 23
Loss: 0.17598850617953488
ROC train: 0.882815	val: 0.737395	test: 0.718362
PRC train: 0.507507	val: 0.325502	test: 0.315760

Epoch: 24
Loss: 0.1756554451055939
ROC train: 0.889479	val: 0.745025	test: 0.716354
PRC train: 0.523685	val: 0.316804	test: 0.320363

Epoch: 25
Loss: 0.17404570775653685
ROC train: 0.889050	val: 0.721643	test: 0.713600
PRC train: 0.528908	val: 0.302566	test: 0.321259

Epoch: 26
Loss: 0.17491483777769773
ROC train: 0.892578	val: 0.732854	test: 0.708908
PRC train: 0.527062	val: 0.310554	test: 0.315878

Epoch: 27
Loss: 0.17394826476197303
ROC train: 0.882276	val: 0.740021	test: 0.708326
PRC train: 0.504422	val: 0.299059	test: 0.302022

Epoch: 28
Loss: 0.16995990124662316
ROC train: 0.897253	val: 0.742461	test: 0.728786
PRC train: 0.543580	val: 0.332271	test: 0.320717

Epoch: 29
Loss: 0.1680343976294261
ROC train: 0.903109	val: 0.734205	test: 0.710387
PRC train: 0.558057	val: 0.295841	test: 0.311117

Epoch: 30
Loss: 0.167684067569257
ROC train: 0.904870	val: 0.755642	test: 0.717994
PRC train: 0.565156	val: 0.336451	test: 0.315245

Epoch: 31
Loss: 0.16438704074856486
ROC train: 0.900333	val: 0.723811	test: 0.702690
PRC train: 0.559385	val: 0.300592	test: 0.301874

Epoch: 32
Loss: 0.16382825437223317
ROC train: 0.906040	val: 0.729609	test: 0.717414Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/tox21/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/static-noise/GraphMVP/tox21/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/tox21/noise=0.1/tox21_scaff_4_26-05_11-20-03  ]
[ Using Seed :  4  ]
[ Using device :  cuda:2  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5463520288758817
ROC train: 0.707782	val: 0.636276	test: 0.597406
PRC train: 0.184365	val: 0.174925	test: 0.172641

Epoch: 2
Loss: 0.3544199483935928
ROC train: 0.738684	val: 0.688832	test: 0.646606
PRC train: 0.242096	val: 0.225449	test: 0.221704

Epoch: 3
Loss: 0.2532051641236637
ROC train: 0.776697	val: 0.720441	test: 0.676748
PRC train: 0.293875	val: 0.273503	test: 0.253653

Epoch: 4
Loss: 0.21551768663292412
ROC train: 0.786797	val: 0.717741	test: 0.665551
PRC train: 0.300584	val: 0.264027	test: 0.245104

Epoch: 5
Loss: 0.20633018018467592
ROC train: 0.807023	val: 0.738340	test: 0.702788
PRC train: 0.324429	val: 0.273023	test: 0.268441

Epoch: 6
Loss: 0.20025493330650768
ROC train: 0.811589	val: 0.726848	test: 0.699745
PRC train: 0.337705	val: 0.276369	test: 0.265617

Epoch: 7
Loss: 0.19628295270047671
ROC train: 0.821031	val: 0.735408	test: 0.698700
PRC train: 0.352543	val: 0.283611	test: 0.281960

Epoch: 8
Loss: 0.19345434802810096
ROC train: 0.830389	val: 0.739072	test: 0.704888
PRC train: 0.367006	val: 0.308264	test: 0.290791

Epoch: 9
Loss: 0.19134752457442775
ROC train: 0.839357	val: 0.744493	test: 0.713382
PRC train: 0.390886	val: 0.309053	test: 0.301040

Epoch: 10
Loss: 0.19046543222555748
ROC train: 0.840229	val: 0.739708	test: 0.710749
PRC train: 0.386218	val: 0.290187	test: 0.283405

Epoch: 11
Loss: 0.18825968540773083
ROC train: 0.843219	val: 0.756536	test: 0.727763
PRC train: 0.407379	val: 0.323430	test: 0.314313

Epoch: 12
Loss: 0.1870955930412501
ROC train: 0.852502	val: 0.748639	test: 0.713700
PRC train: 0.421176	val: 0.328005	test: 0.318011

Epoch: 13
Loss: 0.18556036287965585
ROC train: 0.860551	val: 0.755939	test: 0.720680
PRC train: 0.432641	val: 0.334051	test: 0.320291

Epoch: 14
Loss: 0.18496775573338697
ROC train: 0.860746	val: 0.749664	test: 0.716725
PRC train: 0.440225	val: 0.329807	test: 0.306942

Epoch: 15
Loss: 0.18387823632794162
ROC train: 0.864181	val: 0.751908	test: 0.717610
PRC train: 0.446740	val: 0.327194	test: 0.318103

Epoch: 16
Loss: 0.18269157126234753
ROC train: 0.866818	val: 0.749350	test: 0.713505
PRC train: 0.462767	val: 0.314154	test: 0.315197

Epoch: 17
Loss: 0.17784173396140154
ROC train: 0.870270	val: 0.759344	test: 0.720982
PRC train: 0.468937	val: 0.319527	test: 0.308891

Epoch: 18
Loss: 0.17746488042201095
ROC train: 0.873747	val: 0.766427	test: 0.730464
PRC train: 0.476942	val: 0.335803	test: 0.330789

Epoch: 19
Loss: 0.1763457444442266
ROC train: 0.879089	val: 0.768594	test: 0.732071
PRC train: 0.497978	val: 0.332968	test: 0.336757

Epoch: 20
Loss: 0.1760184905533338
ROC train: 0.878779	val: 0.759042	test: 0.717778
PRC train: 0.494285	val: 0.320612	test: 0.323437

Epoch: 21
Loss: 0.17390849805865494
ROC train: 0.882131	val: 0.767321	test: 0.725026
PRC train: 0.506386	val: 0.351248	test: 0.335975

Epoch: 22
Loss: 0.17253443231216928
ROC train: 0.888185	val: 0.764048	test: 0.727341
PRC train: 0.525307	val: 0.336912	test: 0.336597

Epoch: 23
Loss: 0.1725373377127089
ROC train: 0.889748	val: 0.773123	test: 0.726063
PRC train: 0.528756	val: 0.349810	test: 0.319311

Epoch: 24
Loss: 0.1702192284239704
ROC train: 0.892362	val: 0.773420	test: 0.734469
PRC train: 0.537447	val: 0.344519	test: 0.333973

Epoch: 25
Loss: 0.17093212052039264
ROC train: 0.893526	val: 0.767417	test: 0.718896
PRC train: 0.539451	val: 0.339247	test: 0.320431

Epoch: 26
Loss: 0.16891407452007867
ROC train: 0.895638	val: 0.767478	test: 0.726132
PRC train: 0.541471	val: 0.326432	test: 0.337890

Epoch: 27
Loss: 0.1682845828539297
ROC train: 0.893423	val: 0.780415	test: 0.727797
PRC train: 0.534953	val: 0.342160	test: 0.338756

Epoch: 28
Loss: 0.16520989952566903
ROC train: 0.896669	val: 0.775711	test: 0.733722
PRC train: 0.554773	val: 0.349141	test: 0.334463

Epoch: 29
Loss: 0.16482123064669904
ROC train: 0.905304	val: 0.773890	test: 0.724073
PRC train: 0.569657	val: 0.347586	test: 0.345292

Epoch: 30
Loss: 0.1630181167238514
ROC train: 0.902548	val: 0.764886	test: 0.720901
PRC train: 0.561712	val: 0.324028	test: 0.321007

Epoch: 31
Loss: 0.16532226122680022
ROC train: 0.906085	val: 0.774195	test: 0.728492
PRC train: 0.570786	val: 0.340322	test: 0.337013

Epoch: 32
Loss: 0.16083428786840517
ROC train: 0.908536	val: 0.768356	test: 0.713281Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/tox21/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:3
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/static-noise/GraphMVP/tox21/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/tox21/noise=0.2/tox21_scaff_5_26-05_11-20-03  ]
[ Using Seed :  5  ]
[ Using device :  cuda:3  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5651533614714421
ROC train: 0.682596	val: 0.606930	test: 0.553871
PRC train: 0.165900	val: 0.149408	test: 0.156198

Epoch: 2
Loss: 0.3654927884570782
ROC train: 0.721929	val: 0.660719	test: 0.607449
PRC train: 0.215160	val: 0.193155	test: 0.192505

Epoch: 3
Loss: 0.25970554927209416
ROC train: 0.739241	val: 0.684103	test: 0.640389
PRC train: 0.247970	val: 0.222523	test: 0.221451

Epoch: 4
Loss: 0.22269848720747343
ROC train: 0.758217	val: 0.689791	test: 0.637083
PRC train: 0.262837	val: 0.232111	test: 0.227503

Epoch: 5
Loss: 0.21036933647332112
ROC train: 0.779254	val: 0.719947	test: 0.669671
PRC train: 0.299705	val: 0.258527	test: 0.248015

Epoch: 6
Loss: 0.2068969236809943
ROC train: 0.795372	val: 0.714181	test: 0.694896
PRC train: 0.314874	val: 0.274888	test: 0.260905

Epoch: 7
Loss: 0.20380900528467014
ROC train: 0.800973	val: 0.740156	test: 0.702800
PRC train: 0.323131	val: 0.304543	test: 0.272226

Epoch: 8
Loss: 0.20062392747635038
ROC train: 0.808728	val: 0.728360	test: 0.695940
PRC train: 0.346176	val: 0.279414	test: 0.259165

Epoch: 9
Loss: 0.19897306705558784
ROC train: 0.815670	val: 0.749215	test: 0.705604
PRC train: 0.348935	val: 0.299828	test: 0.282972

Epoch: 10
Loss: 0.19618327251012818
ROC train: 0.821315	val: 0.746751	test: 0.712397
PRC train: 0.365445	val: 0.289936	test: 0.287172

Epoch: 11
Loss: 0.19509569253495695
ROC train: 0.827363	val: 0.733095	test: 0.696285
PRC train: 0.367572	val: 0.265630	test: 0.259067

Epoch: 12
Loss: 0.19263985491411006
ROC train: 0.834187	val: 0.741977	test: 0.709364
PRC train: 0.387114	val: 0.293818	test: 0.286506

Epoch: 13
Loss: 0.19033003526660533
ROC train: 0.839533	val: 0.734906	test: 0.704930
PRC train: 0.398657	val: 0.298973	test: 0.285434

Epoch: 14
Loss: 0.19101343000442683
ROC train: 0.845493	val: 0.750019	test: 0.716248
PRC train: 0.402429	val: 0.311393	test: 0.292700

Epoch: 15
Loss: 0.18888218728039785
ROC train: 0.851759	val: 0.749544	test: 0.714307
PRC train: 0.421504	val: 0.305154	test: 0.287122

Epoch: 16
Loss: 0.18753355513219247
ROC train: 0.852832	val: 0.753165	test: 0.711267
PRC train: 0.420900	val: 0.292759	test: 0.277437

Epoch: 17
Loss: 0.18462084712491336
ROC train: 0.859056	val: 0.749732	test: 0.706169
PRC train: 0.426962	val: 0.309488	test: 0.274051

Epoch: 18
Loss: 0.18546881994144868
ROC train: 0.862878	val: 0.742242	test: 0.720649
PRC train: 0.449901	val: 0.293305	test: 0.296711

Epoch: 19
Loss: 0.1833491125243718
ROC train: 0.869862	val: 0.759145	test: 0.712318
PRC train: 0.460354	val: 0.309080	test: 0.305535

Epoch: 20
Loss: 0.18109372662252024
ROC train: 0.868692	val: 0.753885	test: 0.706923
PRC train: 0.456020	val: 0.304198	test: 0.298447

Epoch: 21
Loss: 0.18081306085983748
ROC train: 0.875825	val: 0.745465	test: 0.700736
PRC train: 0.474524	val: 0.295565	test: 0.282111

Epoch: 22
Loss: 0.18063754892439668
ROC train: 0.876407	val: 0.755954	test: 0.714358
PRC train: 0.466732	val: 0.317642	test: 0.293680

Epoch: 23
Loss: 0.17863947549894657
ROC train: 0.879950	val: 0.730169	test: 0.692693
PRC train: 0.483453	val: 0.278861	test: 0.273353

Epoch: 24
Loss: 0.17599279140167848
ROC train: 0.883742	val: 0.727081	test: 0.703091
PRC train: 0.499642	val: 0.265960	test: 0.264762

Epoch: 25
Loss: 0.17602590659804093
ROC train: 0.888180	val: 0.727263	test: 0.698068
PRC train: 0.509337	val: 0.288385	test: 0.281503

Epoch: 26
Loss: 0.17419048844160517
ROC train: 0.890905	val: 0.749008	test: 0.704118
PRC train: 0.527819	val: 0.296211	test: 0.286405

Epoch: 27
Loss: 0.172491892229551
ROC train: 0.892621	val: 0.735870	test: 0.709163
PRC train: 0.522045	val: 0.287046	test: 0.281379

Epoch: 28
Loss: 0.17113624809272
ROC train: 0.897628	val: 0.747375	test: 0.708635
PRC train: 0.543254	val: 0.306787	test: 0.304763

Epoch: 29
Loss: 0.16887374528221066
ROC train: 0.900343	val: 0.740352	test: 0.693801
PRC train: 0.548167	val: 0.303861	test: 0.286693

Epoch: 30
Loss: 0.16835615279328195
ROC train: 0.903299	val: 0.730661	test: 0.693886
PRC train: 0.560086	val: 0.291581	test: 0.289061

Epoch: 31
Loss: 0.16769246761859172
ROC train: 0.900425	val: 0.749376	test: 0.700924
PRC train: 0.533827	val: 0.310041	test: 0.304468

Epoch: 32
Loss: 0.16502984619022265
ROC train: 0.908925	val: 0.735514	test: 0.703886Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/tox21/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:3
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/static-noise/GraphMVP/tox21/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/tox21/noise=0.2/tox21_scaff_4_26-05_11-20-03  ]
[ Using Seed :  4  ]
[ Using device :  cuda:3  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5479865178336428
ROC train: 0.691054	val: 0.606017	test: 0.581651
PRC train: 0.172175	val: 0.146679	test: 0.150397

Epoch: 2
Loss: 0.3572778159390856
ROC train: 0.725366	val: 0.671436	test: 0.639904
PRC train: 0.222540	val: 0.186282	test: 0.199401

Epoch: 3
Loss: 0.25660331221747
ROC train: 0.750296	val: 0.714314	test: 0.676548
PRC train: 0.258970	val: 0.252570	test: 0.241813

Epoch: 4
Loss: 0.22055142290611474
ROC train: 0.765057	val: 0.705458	test: 0.659046
PRC train: 0.267882	val: 0.240794	test: 0.235356

Epoch: 5
Loss: 0.2120661782106019
ROC train: 0.779713	val: 0.705126	test: 0.675570
PRC train: 0.286447	val: 0.233321	test: 0.241136

Epoch: 6
Loss: 0.20589330956091773
ROC train: 0.795400	val: 0.729950	test: 0.705265
PRC train: 0.317993	val: 0.291643	test: 0.279787

Epoch: 7
Loss: 0.20202866888635615
ROC train: 0.799460	val: 0.739202	test: 0.717078
PRC train: 0.330691	val: 0.304928	test: 0.298335

Epoch: 8
Loss: 0.19903913484703417
ROC train: 0.814401	val: 0.726833	test: 0.702132
PRC train: 0.343111	val: 0.284447	test: 0.285459

Epoch: 9
Loss: 0.19776805920633475
ROC train: 0.820476	val: 0.730003	test: 0.695547
PRC train: 0.349584	val: 0.273748	test: 0.271260

Epoch: 10
Loss: 0.19522314557693626
ROC train: 0.821943	val: 0.716186	test: 0.691466
PRC train: 0.355421	val: 0.245264	test: 0.250406

Epoch: 11
Loss: 0.19368754068997013
ROC train: 0.828246	val: 0.726777	test: 0.696569
PRC train: 0.378494	val: 0.279143	test: 0.276262

Epoch: 12
Loss: 0.1932671477042702
ROC train: 0.837132	val: 0.737978	test: 0.690488
PRC train: 0.388874	val: 0.282531	test: 0.266186

Epoch: 13
Loss: 0.19092559455973174
ROC train: 0.848715	val: 0.746115	test: 0.704797
PRC train: 0.398557	val: 0.299242	test: 0.294783

Epoch: 14
Loss: 0.19053370982797965
ROC train: 0.844886	val: 0.743851	test: 0.700007
PRC train: 0.409403	val: 0.298617	test: 0.288316

Epoch: 15
Loss: 0.1913484341172922
ROC train: 0.848988	val: 0.722907	test: 0.695742
PRC train: 0.405444	val: 0.252545	test: 0.259487

Epoch: 16
Loss: 0.1876225555416549
ROC train: 0.857113	val: 0.730838	test: 0.693861
PRC train: 0.429560	val: 0.269411	test: 0.272512

Epoch: 17
Loss: 0.18560155337160375
ROC train: 0.859919	val: 0.749267	test: 0.705481
PRC train: 0.438694	val: 0.285674	test: 0.278525

Epoch: 18
Loss: 0.18376257382166572
ROC train: 0.865674	val: 0.761419	test: 0.696332
PRC train: 0.435712	val: 0.325914	test: 0.296729

Epoch: 19
Loss: 0.1840201352488185
ROC train: 0.868225	val: 0.744155	test: 0.706212
PRC train: 0.450326	val: 0.279013	test: 0.283983

Epoch: 20
Loss: 0.18058162132378178
ROC train: 0.870945	val: 0.733960	test: 0.688808
PRC train: 0.457398	val: 0.270467	test: 0.281628

Epoch: 21
Loss: 0.17934350181159187
ROC train: 0.880003	val: 0.731930	test: 0.682873
PRC train: 0.484000	val: 0.275319	test: 0.256641

Epoch: 22
Loss: 0.17837244757678886
ROC train: 0.879817	val: 0.734184	test: 0.701043
PRC train: 0.475279	val: 0.292286	test: 0.307629

Epoch: 23
Loss: 0.17739069535567203
ROC train: 0.883299	val: 0.724041	test: 0.677321
PRC train: 0.493291	val: 0.270396	test: 0.252762

Epoch: 24
Loss: 0.17512971371276312
ROC train: 0.884011	val: 0.718603	test: 0.679575
PRC train: 0.504365	val: 0.251802	test: 0.250763

Epoch: 25
Loss: 0.1758932560884464
ROC train: 0.890152	val: 0.755266	test: 0.708268
PRC train: 0.516664	val: 0.326224	test: 0.312514

Epoch: 26
Loss: 0.17191481640486514
ROC train: 0.885118	val: 0.715254	test: 0.678140
PRC train: 0.494691	val: 0.261587	test: 0.266811

Epoch: 27
Loss: 0.1705775548836202
ROC train: 0.897164	val: 0.751798	test: 0.709563
PRC train: 0.527509	val: 0.333202	test: 0.319292

Epoch: 28
Loss: 0.16976200540264993
ROC train: 0.891377	val: 0.737807	test: 0.687214
PRC train: 0.511347	val: 0.295840	test: 0.285464

Epoch: 29
Loss: 0.1695122932455481
ROC train: 0.900693	val: 0.736842	test: 0.711958
PRC train: 0.547427	val: 0.282948	test: 0.296975

Epoch: 30
Loss: 0.1682268370205936
ROC train: 0.898817	val: 0.719845	test: 0.699839
PRC train: 0.542077	val: 0.275208	test: 0.281324

Epoch: 31
Loss: 0.1672580009390425
ROC train: 0.905055	val: 0.711795	test: 0.688405
PRC train: 0.559319	val: 0.264016	test: 0.275298

Epoch: 32
Loss: 0.16538244366860053
ROC train: 0.908788	val: 0.696505	test: 0.674726Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/tox21/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/static-noise/GraphMVP/tox21/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/tox21/noise=0.05/tox21_scaff_6_26-05_11-20-03  ]
[ Using Seed :  6  ]
[ Using device :  cuda:1  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5674323049553152
ROC train: 0.703628	val: 0.617591	test: 0.592930
PRC train: 0.164170	val: 0.148557	test: 0.158941

Epoch: 2
Loss: 0.3615737311683885
ROC train: 0.759634	val: 0.718056	test: 0.676811
PRC train: 0.259866	val: 0.257458	test: 0.233779

Epoch: 3
Loss: 0.2553280873361192
ROC train: 0.779429	val: 0.731069	test: 0.687106
PRC train: 0.288354	val: 0.271926	test: 0.256583

Epoch: 4
Loss: 0.2155274406805688
ROC train: 0.795494	val: 0.739249	test: 0.709664
PRC train: 0.320868	val: 0.294060	test: 0.283173

Epoch: 5
Loss: 0.20321084992425
ROC train: 0.811797	val: 0.737550	test: 0.712438
PRC train: 0.341216	val: 0.303561	test: 0.294327

Epoch: 6
Loss: 0.19719915048512282
ROC train: 0.817492	val: 0.744501	test: 0.726505
PRC train: 0.347407	val: 0.320551	test: 0.304300

Epoch: 7
Loss: 0.19531564404438892
ROC train: 0.830419	val: 0.749039	test: 0.722908
PRC train: 0.366608	val: 0.321543	test: 0.307106

Epoch: 8
Loss: 0.19069190299046504
ROC train: 0.840406	val: 0.747297	test: 0.720367
PRC train: 0.382705	val: 0.318991	test: 0.314536

Epoch: 9
Loss: 0.1893664701041349
ROC train: 0.844002	val: 0.757332	test: 0.727248
PRC train: 0.389226	val: 0.329374	test: 0.307871

Epoch: 10
Loss: 0.18682992131050077
ROC train: 0.852692	val: 0.740395	test: 0.721692
PRC train: 0.413324	val: 0.322384	test: 0.329934

Epoch: 11
Loss: 0.1855213337975422
ROC train: 0.853550	val: 0.752335	test: 0.723830
PRC train: 0.414878	val: 0.319075	test: 0.322705

Epoch: 12
Loss: 0.18328925497954632
ROC train: 0.861677	val: 0.759794	test: 0.733527
PRC train: 0.442607	val: 0.331406	test: 0.330493

Epoch: 13
Loss: 0.18097954352151152
ROC train: 0.862060	val: 0.767444	test: 0.733147
PRC train: 0.450453	val: 0.340787	test: 0.322539

Epoch: 14
Loss: 0.17898114957315406
ROC train: 0.869963	val: 0.768199	test: 0.741253
PRC train: 0.469389	val: 0.350891	test: 0.342192

Epoch: 15
Loss: 0.1777399016629392
ROC train: 0.873657	val: 0.766952	test: 0.740544
PRC train: 0.483239	val: 0.353554	test: 0.345893

Epoch: 16
Loss: 0.17636068974525837
ROC train: 0.876455	val: 0.760884	test: 0.739220
PRC train: 0.487231	val: 0.351011	test: 0.350038

Epoch: 17
Loss: 0.17515227398977573
ROC train: 0.876620	val: 0.773991	test: 0.738488
PRC train: 0.495439	val: 0.353268	test: 0.339376

Epoch: 18
Loss: 0.17332251920183758
ROC train: 0.880952	val: 0.774989	test: 0.733360
PRC train: 0.503261	val: 0.361488	test: 0.340005

Epoch: 19
Loss: 0.1730907712057154
ROC train: 0.885199	val: 0.769417	test: 0.737103
PRC train: 0.518223	val: 0.348533	test: 0.337974

Epoch: 20
Loss: 0.17463326686066527
ROC train: 0.884672	val: 0.766879	test: 0.742580
PRC train: 0.514594	val: 0.347013	test: 0.342892

Epoch: 21
Loss: 0.1699157829230124
ROC train: 0.889844	val: 0.776888	test: 0.744031
PRC train: 0.538716	val: 0.360549	test: 0.363577

Epoch: 22
Loss: 0.1686767446848506
ROC train: 0.893082	val: 0.779581	test: 0.741012
PRC train: 0.546235	val: 0.366454	test: 0.358126

Epoch: 23
Loss: 0.1680158878679042
ROC train: 0.891325	val: 0.779677	test: 0.748881
PRC train: 0.534192	val: 0.359914	test: 0.366656

Epoch: 24
Loss: 0.1664276792253357
ROC train: 0.898773	val: 0.774971	test: 0.738781
PRC train: 0.546851	val: 0.354443	test: 0.352825

Epoch: 25
Loss: 0.16672843215007016
ROC train: 0.898751	val: 0.779089	test: 0.749232
PRC train: 0.557515	val: 0.354196	test: 0.351482

Epoch: 26
Loss: 0.1648735111306794
ROC train: 0.904694	val: 0.780918	test: 0.747115
PRC train: 0.570719	val: 0.354437	test: 0.361534

Epoch: 27
Loss: 0.16434873118983442
ROC train: 0.901364	val: 0.781611	test: 0.733088
PRC train: 0.553364	val: 0.344392	test: 0.338361

Epoch: 28
Loss: 0.16250966844915532
ROC train: 0.902769	val: 0.771935	test: 0.748551
PRC train: 0.560735	val: 0.344259	test: 0.333219

Epoch: 29
Loss: 0.16111534086792265
ROC train: 0.907960	val: 0.782046	test: 0.748011
PRC train: 0.584682	val: 0.347926	test: 0.356061

Epoch: 30
Loss: 0.16093282819827678
ROC train: 0.909197	val: 0.786653	test: 0.748206
PRC train: 0.592341	val: 0.374355	test: 0.364812

Epoch: 31
Loss: 0.1585449011785
ROC train: 0.911882	val: 0.782043	test: 0.734696
PRC train: 0.593010	val: 0.367463	test: 0.361949

Epoch: 32
Loss: 0.15787659077345176
ROC train: 0.912027	val: 0.783598	test: 0.751467Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/tox21/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/static-noise/GraphMVP/tox21/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/tox21/noise=0.1/tox21_scaff_6_26-05_11-20-03  ]
[ Using Seed :  6  ]
[ Using device :  cuda:2  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5681571392886489
ROC train: 0.697532	val: 0.596561	test: 0.576891
PRC train: 0.165407	val: 0.140121	test: 0.150856

Epoch: 2
Loss: 0.3638403868116217
ROC train: 0.747952	val: 0.712695	test: 0.665664
PRC train: 0.240441	val: 0.239790	test: 0.227081

Epoch: 3
Loss: 0.25714001573496464
ROC train: 0.768370	val: 0.722214	test: 0.679937
PRC train: 0.272582	val: 0.273740	test: 0.249650

Epoch: 4
Loss: 0.21794334568784207
ROC train: 0.789166	val: 0.728596	test: 0.698686
PRC train: 0.309969	val: 0.290912	test: 0.281604

Epoch: 5
Loss: 0.2055055048578523
ROC train: 0.802924	val: 0.729095	test: 0.709806
PRC train: 0.326728	val: 0.298568	test: 0.286328

Epoch: 6
Loss: 0.2009986585840588
ROC train: 0.803856	val: 0.739375	test: 0.712341
PRC train: 0.327611	val: 0.300971	test: 0.289555

Epoch: 7
Loss: 0.19831861408503765
ROC train: 0.821730	val: 0.751814	test: 0.723630
PRC train: 0.356880	val: 0.325313	test: 0.302662

Epoch: 8
Loss: 0.19400254246698098
ROC train: 0.829130	val: 0.753396	test: 0.729811
PRC train: 0.368281	val: 0.325944	test: 0.315381

Epoch: 9
Loss: 0.19241658104047496
ROC train: 0.836520	val: 0.761009	test: 0.731750
PRC train: 0.386884	val: 0.323694	test: 0.309948

Epoch: 10
Loss: 0.19034097280634688
ROC train: 0.844032	val: 0.737568	test: 0.719852
PRC train: 0.400863	val: 0.319238	test: 0.315913

Epoch: 11
Loss: 0.18895403060168792
ROC train: 0.850152	val: 0.747225	test: 0.728880
PRC train: 0.404757	val: 0.310242	test: 0.321777

Epoch: 12
Loss: 0.18736946405730937
ROC train: 0.855731	val: 0.756980	test: 0.724744
PRC train: 0.419937	val: 0.321329	test: 0.321630

Epoch: 13
Loss: 0.18409765712485462
ROC train: 0.858839	val: 0.767456	test: 0.729672
PRC train: 0.433557	val: 0.332854	test: 0.327598

Epoch: 14
Loss: 0.18269081844586568
ROC train: 0.866768	val: 0.768842	test: 0.740665
PRC train: 0.456906	val: 0.339815	test: 0.329431

Epoch: 15
Loss: 0.18068883343564393
ROC train: 0.866806	val: 0.764046	test: 0.732941
PRC train: 0.455193	val: 0.344125	test: 0.328609

Epoch: 16
Loss: 0.18142627924234675
ROC train: 0.866882	val: 0.759704	test: 0.732917
PRC train: 0.456991	val: 0.339549	test: 0.324607

Epoch: 17
Loss: 0.179987178867589
ROC train: 0.874406	val: 0.764630	test: 0.739743
PRC train: 0.478023	val: 0.335878	test: 0.332183

Epoch: 18
Loss: 0.17788378582504305
ROC train: 0.880270	val: 0.771420	test: 0.733558
PRC train: 0.494342	val: 0.346073	test: 0.336318

Epoch: 19
Loss: 0.17654085497372282
ROC train: 0.879591	val: 0.765863	test: 0.733288
PRC train: 0.489167	val: 0.330854	test: 0.328569

Epoch: 20
Loss: 0.1768615843326959
ROC train: 0.879809	val: 0.770300	test: 0.744652
PRC train: 0.501340	val: 0.337096	test: 0.338966

Epoch: 21
Loss: 0.17268986560305294
ROC train: 0.884744	val: 0.770950	test: 0.734063
PRC train: 0.516344	val: 0.352085	test: 0.344165

Epoch: 22
Loss: 0.17217411174547567
ROC train: 0.891265	val: 0.778104	test: 0.739303
PRC train: 0.537236	val: 0.355118	test: 0.349057

Epoch: 23
Loss: 0.17148248561633103
ROC train: 0.884150	val: 0.781190	test: 0.743266
PRC train: 0.512874	val: 0.350130	test: 0.345741

Epoch: 24
Loss: 0.17028800961117174
ROC train: 0.894671	val: 0.771090	test: 0.725560
PRC train: 0.536471	val: 0.339229	test: 0.330950

Epoch: 25
Loss: 0.16914255861711097
ROC train: 0.896905	val: 0.773064	test: 0.744815
PRC train: 0.543140	val: 0.344553	test: 0.340954

Epoch: 26
Loss: 0.16756958248868734
ROC train: 0.900689	val: 0.775341	test: 0.736570
PRC train: 0.562473	val: 0.351127	test: 0.341228

Epoch: 27
Loss: 0.16612332771020738
ROC train: 0.903030	val: 0.784665	test: 0.736566
PRC train: 0.561432	val: 0.348636	test: 0.332754

Epoch: 28
Loss: 0.16437590077021333
ROC train: 0.902853	val: 0.785935	test: 0.750368
PRC train: 0.567712	val: 0.355627	test: 0.343240

Epoch: 29
Loss: 0.16373810626195265
ROC train: 0.905508	val: 0.779381	test: 0.739890
PRC train: 0.577825	val: 0.346327	test: 0.356069

Epoch: 30
Loss: 0.1628316803998596
ROC train: 0.906040	val: 0.790317	test: 0.753446
PRC train: 0.582489	val: 0.357629	test: 0.351034

Epoch: 31
Loss: 0.15896599607627776
ROC train: 0.912290	val: 0.783612	test: 0.734011
PRC train: 0.596872	val: 0.353151	test: 0.342063

Epoch: 32
Loss: 0.15894353921333323
ROC train: 0.913852	val: 0.781092	test: 0.743245Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/tox21/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/static-noise/GraphMVP/tox21/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/tox21/noise=0.1/tox21_scaff_5_26-05_11-20-03  ]
[ Using Seed :  5  ]
[ Using device :  cuda:2  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5632417959006435
ROC train: 0.692259	val: 0.613516	test: 0.572730
PRC train: 0.167896	val: 0.156899	test: 0.160077

Epoch: 2
Loss: 0.3640166025942995
ROC train: 0.739443	val: 0.696834	test: 0.645255
PRC train: 0.230077	val: 0.218425	test: 0.212616

Epoch: 3
Loss: 0.2569618489551212
ROC train: 0.757290	val: 0.707291	test: 0.667018
PRC train: 0.266271	val: 0.257867	test: 0.246512

Epoch: 4
Loss: 0.21893696283855515
ROC train: 0.783011	val: 0.724452	test: 0.685925
PRC train: 0.288176	val: 0.265608	test: 0.256030

Epoch: 5
Loss: 0.20591770162771345
ROC train: 0.799161	val: 0.745865	test: 0.707538
PRC train: 0.313560	val: 0.287573	test: 0.282214

Epoch: 6
Loss: 0.2024185954868109
ROC train: 0.811288	val: 0.752821	test: 0.729524
PRC train: 0.335115	val: 0.323987	test: 0.293504

Epoch: 7
Loss: 0.19827623244822018
ROC train: 0.818696	val: 0.759452	test: 0.723684
PRC train: 0.342006	val: 0.323959	test: 0.300996

Epoch: 8
Loss: 0.19591875319028074
ROC train: 0.824378	val: 0.761829	test: 0.726846
PRC train: 0.368727	val: 0.331872	test: 0.298767

Epoch: 9
Loss: 0.19418012242198718
ROC train: 0.831857	val: 0.757847	test: 0.730116
PRC train: 0.378108	val: 0.335261	test: 0.318724

Epoch: 10
Loss: 0.19206717984809
ROC train: 0.836848	val: 0.769503	test: 0.732968
PRC train: 0.392982	val: 0.341252	test: 0.322502

Epoch: 11
Loss: 0.19096561977817778
ROC train: 0.836981	val: 0.746985	test: 0.721536
PRC train: 0.376564	val: 0.284202	test: 0.292557

Epoch: 12
Loss: 0.18911004845531007
ROC train: 0.848521	val: 0.759854	test: 0.731420
PRC train: 0.410947	val: 0.328258	test: 0.318584

Epoch: 13
Loss: 0.1861124643239266
ROC train: 0.852029	val: 0.755614	test: 0.725734
PRC train: 0.418897	val: 0.343531	test: 0.319029

Epoch: 14
Loss: 0.18478061468526072
ROC train: 0.854291	val: 0.748547	test: 0.719823
PRC train: 0.436167	val: 0.319438	test: 0.311373

Epoch: 15
Loss: 0.18350076163092832
ROC train: 0.858373	val: 0.766249	test: 0.727696
PRC train: 0.432031	val: 0.345370	test: 0.327339

Epoch: 16
Loss: 0.18413210878397057
ROC train: 0.859565	val: 0.756444	test: 0.732376
PRC train: 0.436181	val: 0.334990	test: 0.325792

Epoch: 17
Loss: 0.18014529037677898
ROC train: 0.869821	val: 0.762780	test: 0.736133
PRC train: 0.470712	val: 0.352022	test: 0.337768

Epoch: 18
Loss: 0.1798197737279175
ROC train: 0.874114	val: 0.768313	test: 0.731654
PRC train: 0.482448	val: 0.351488	test: 0.338117

Epoch: 19
Loss: 0.17898127733432279
ROC train: 0.871119	val: 0.769932	test: 0.735485
PRC train: 0.474772	val: 0.336287	test: 0.327666

Epoch: 20
Loss: 0.1771203041294279
ROC train: 0.875339	val: 0.766560	test: 0.737788
PRC train: 0.482775	val: 0.356486	test: 0.337280

Epoch: 21
Loss: 0.17561364131985535
ROC train: 0.876826	val: 0.761501	test: 0.727932
PRC train: 0.475647	val: 0.331179	test: 0.326778

Epoch: 22
Loss: 0.1733711627134393
ROC train: 0.884803	val: 0.761631	test: 0.719782
PRC train: 0.505812	val: 0.350994	test: 0.336124

Epoch: 23
Loss: 0.17418500007597135
ROC train: 0.886461	val: 0.771385	test: 0.727778
PRC train: 0.521960	val: 0.343522	test: 0.334007

Epoch: 24
Loss: 0.1713884148435743
ROC train: 0.892554	val: 0.764531	test: 0.731065
PRC train: 0.542958	val: 0.351064	test: 0.338040

Epoch: 25
Loss: 0.17043722500674685
ROC train: 0.892220	val: 0.761091	test: 0.733461
PRC train: 0.539046	val: 0.342922	test: 0.321008

Epoch: 26
Loss: 0.17090913370178207
ROC train: 0.896076	val: 0.778568	test: 0.739038
PRC train: 0.556042	val: 0.351857	test: 0.335519

Epoch: 27
Loss: 0.1679954440607738
ROC train: 0.898561	val: 0.776835	test: 0.742608
PRC train: 0.561966	val: 0.352921	test: 0.342694

Epoch: 28
Loss: 0.16490955007931551
ROC train: 0.903473	val: 0.770111	test: 0.731802
PRC train: 0.574898	val: 0.362388	test: 0.338481

Epoch: 29
Loss: 0.16554091608171334
ROC train: 0.904709	val: 0.761170	test: 0.723298
PRC train: 0.580428	val: 0.357428	test: 0.344704

Epoch: 30
Loss: 0.16370399285450085
ROC train: 0.907371	val: 0.765225	test: 0.732640
PRC train: 0.581277	val: 0.352583	test: 0.343035

Epoch: 31
Loss: 0.16476345926663744
ROC train: 0.901082	val: 0.766594	test: 0.729877
PRC train: 0.551298	val: 0.350325	test: 0.336584

Epoch: 32
Loss: 0.1624171277013919
ROC train: 0.910658	val: 0.783679	test: 0.733172Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/tox21/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/static-noise/GraphMVP/tox21/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/tox21/noise=0.05/tox21_scaff_4_26-05_11-20-03  ]
[ Using Seed :  4  ]
[ Using device :  cuda:1  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5453581673138156
ROC train: 0.708915	val: 0.648691	test: 0.606834
PRC train: 0.188508	val: 0.184438	test: 0.175707

Epoch: 2
Loss: 0.3526526743531148
ROC train: 0.747397	val: 0.685501	test: 0.642854
PRC train: 0.246162	val: 0.224839	test: 0.218948

Epoch: 3
Loss: 0.2517413410339664
ROC train: 0.781929	val: 0.725263	test: 0.686109
PRC train: 0.297309	val: 0.283647	test: 0.268986

Epoch: 4
Loss: 0.21312633910577958
ROC train: 0.795547	val: 0.723873	test: 0.680643
PRC train: 0.309404	val: 0.277915	test: 0.269640

Epoch: 5
Loss: 0.20400808176543633
ROC train: 0.814668	val: 0.750263	test: 0.707274
PRC train: 0.341509	val: 0.297791	test: 0.291716

Epoch: 6
Loss: 0.1975844833562051
ROC train: 0.814818	val: 0.732060	test: 0.701102
PRC train: 0.340953	val: 0.268897	test: 0.268406

Epoch: 7
Loss: 0.19315391142980395
ROC train: 0.832731	val: 0.754744	test: 0.712078
PRC train: 0.377836	val: 0.306696	test: 0.307913

Epoch: 8
Loss: 0.1903887043794824
ROC train: 0.832950	val: 0.746868	test: 0.704989
PRC train: 0.376545	val: 0.309231	test: 0.307726

Epoch: 9
Loss: 0.18817954437381662
ROC train: 0.843502	val: 0.737727	test: 0.707455
PRC train: 0.390613	val: 0.275730	test: 0.285650

Epoch: 10
Loss: 0.18678071233592955
ROC train: 0.843923	val: 0.742695	test: 0.710883
PRC train: 0.400176	val: 0.287955	test: 0.288310

Epoch: 11
Loss: 0.18485208284310592
ROC train: 0.836248	val: 0.769821	test: 0.723265
PRC train: 0.397174	val: 0.352336	test: 0.330690

Epoch: 12
Loss: 0.1845789561662682
ROC train: 0.857533	val: 0.752264	test: 0.709682
PRC train: 0.436643	val: 0.326374	test: 0.317413

Epoch: 13
Loss: 0.18263836908694434
ROC train: 0.862521	val: 0.770072	test: 0.725814
PRC train: 0.454660	val: 0.332218	test: 0.324325

Epoch: 14
Loss: 0.18214654415597134
ROC train: 0.864243	val: 0.764128	test: 0.722875
PRC train: 0.465705	val: 0.341164	test: 0.324500

Epoch: 15
Loss: 0.18085280627925004
ROC train: 0.866381	val: 0.770341	test: 0.729011
PRC train: 0.462060	val: 0.328479	test: 0.327240

Epoch: 16
Loss: 0.17845813862882642
ROC train: 0.869641	val: 0.775404	test: 0.725238
PRC train: 0.485366	val: 0.349264	test: 0.338698

Epoch: 17
Loss: 0.1767066069801175
ROC train: 0.874411	val: 0.772515	test: 0.730850
PRC train: 0.501462	val: 0.331088	test: 0.329398

Epoch: 18
Loss: 0.17363319385991932
ROC train: 0.879095	val: 0.780294	test: 0.732179
PRC train: 0.510529	val: 0.335432	test: 0.323765

Epoch: 19
Loss: 0.17308056272059283
ROC train: 0.878837	val: 0.770504	test: 0.734178
PRC train: 0.502193	val: 0.317197	test: 0.322644

Epoch: 20
Loss: 0.1718589459571818
ROC train: 0.882260	val: 0.777358	test: 0.728563
PRC train: 0.521981	val: 0.343191	test: 0.337134

Epoch: 21
Loss: 0.1714794970887913
ROC train: 0.882878	val: 0.777075	test: 0.723978
PRC train: 0.518145	val: 0.359584	test: 0.342422

Epoch: 22
Loss: 0.16980703159231697
ROC train: 0.890820	val: 0.777317	test: 0.724561
PRC train: 0.540676	val: 0.342262	test: 0.333831

Epoch: 23
Loss: 0.16878773342573092
ROC train: 0.891193	val: 0.765914	test: 0.729846
PRC train: 0.538129	val: 0.314409	test: 0.317018

Epoch: 24
Loss: 0.16807697682751843
ROC train: 0.892349	val: 0.777568	test: 0.734579
PRC train: 0.539228	val: 0.341181	test: 0.336401

Epoch: 25
Loss: 0.16759553407087405
ROC train: 0.897319	val: 0.782113	test: 0.732357
PRC train: 0.566222	val: 0.343166	test: 0.339044

Epoch: 26
Loss: 0.16413099244327228
ROC train: 0.899651	val: 0.781207	test: 0.735872
PRC train: 0.558729	val: 0.335298	test: 0.339338

Epoch: 27
Loss: 0.1653749920549153
ROC train: 0.897712	val: 0.788194	test: 0.730811
PRC train: 0.558863	val: 0.358363	test: 0.346601

Epoch: 28
Loss: 0.16212349218105018
ROC train: 0.899467	val: 0.787166	test: 0.730526
PRC train: 0.568992	val: 0.365463	test: 0.347382

Epoch: 29
Loss: 0.1634781133197521
ROC train: 0.904148	val: 0.781984	test: 0.739101
PRC train: 0.583019	val: 0.341287	test: 0.351573

Epoch: 30
Loss: 0.16097477327970502
ROC train: 0.880615	val: 0.722261	test: 0.694853
PRC train: 0.493972	val: 0.252894	test: 0.266598

Epoch: 31
Loss: 0.16205848334759274
ROC train: 0.903506	val: 0.771820	test: 0.727657
PRC train: 0.575882	val: 0.339274	test: 0.345337

Epoch: 32
Loss: 0.15836328671916716
ROC train: 0.911315	val: 0.783060	test: 0.729555Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/tox21/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/static-noise/GraphMVP/tox21/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/tox21/noise=0.05/tox21_scaff_5_26-05_11-20-03  ]
[ Using Seed :  5  ]
[ Using device :  cuda:1  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5610721644455781
ROC train: 0.702961	val: 0.626926	test: 0.585526
PRC train: 0.169122	val: 0.152934	test: 0.161634

Epoch: 2
Loss: 0.36205247416680836
ROC train: 0.751915	val: 0.698531	test: 0.660260
PRC train: 0.239385	val: 0.218283	test: 0.217138

Epoch: 3
Loss: 0.2549429568097138
ROC train: 0.769997	val: 0.704970	test: 0.671584
PRC train: 0.279221	val: 0.256303	test: 0.248069

Epoch: 4
Loss: 0.21676034908068129
ROC train: 0.797183	val: 0.734444	test: 0.704530
PRC train: 0.308411	val: 0.280634	test: 0.268547

Epoch: 5
Loss: 0.2029296108258378
ROC train: 0.810433	val: 0.751454	test: 0.717429
PRC train: 0.335225	val: 0.303407	test: 0.293741

Epoch: 6
Loss: 0.19872823996692127
ROC train: 0.823837	val: 0.745726	test: 0.727345
PRC train: 0.357570	val: 0.312241	test: 0.297728

Epoch: 7
Loss: 0.19497736485112338
ROC train: 0.831080	val: 0.759181	test: 0.722857
PRC train: 0.370454	val: 0.326032	test: 0.305310

Epoch: 8
Loss: 0.19223715016868073
ROC train: 0.837842	val: 0.754919	test: 0.737659
PRC train: 0.386500	val: 0.327246	test: 0.307672

Epoch: 9
Loss: 0.19046373859663518
ROC train: 0.844323	val: 0.758954	test: 0.726837
PRC train: 0.395301	val: 0.327434	test: 0.320826

Epoch: 10
Loss: 0.18823928413178867
ROC train: 0.847678	val: 0.764791	test: 0.732556
PRC train: 0.402256	val: 0.332037	test: 0.317517

Epoch: 11
Loss: 0.18616531757357901
ROC train: 0.845394	val: 0.737775	test: 0.720213
PRC train: 0.393558	val: 0.287997	test: 0.290846

Epoch: 12
Loss: 0.18584395348731417
ROC train: 0.858367	val: 0.752212	test: 0.733301
PRC train: 0.429449	val: 0.323313	test: 0.326495

Epoch: 13
Loss: 0.18262831898308643
ROC train: 0.861299	val: 0.752137	test: 0.732180
PRC train: 0.435734	val: 0.339875	test: 0.320843

Epoch: 14
Loss: 0.181020665769709
ROC train: 0.869545	val: 0.757724	test: 0.734463
PRC train: 0.462010	val: 0.325728	test: 0.326599

Epoch: 15
Loss: 0.17850643440721242
ROC train: 0.870212	val: 0.761771	test: 0.735921
PRC train: 0.463747	val: 0.346114	test: 0.341866

Epoch: 16
Loss: 0.17959946903305726
ROC train: 0.869671	val: 0.766536	test: 0.743166
PRC train: 0.462740	val: 0.338288	test: 0.335643

Epoch: 17
Loss: 0.17650507337761925
ROC train: 0.876362	val: 0.765997	test: 0.742927
PRC train: 0.480490	val: 0.349443	test: 0.343370

Epoch: 18
Loss: 0.17532458633203762
ROC train: 0.878269	val: 0.769019	test: 0.735340
PRC train: 0.490074	val: 0.339045	test: 0.327432

Epoch: 19
Loss: 0.17531221571198063
ROC train: 0.874829	val: 0.770496	test: 0.742618
PRC train: 0.479265	val: 0.350042	test: 0.343397

Epoch: 20
Loss: 0.17310900910452884
ROC train: 0.879819	val: 0.775260	test: 0.733638
PRC train: 0.495754	val: 0.355752	test: 0.342601

Epoch: 21
Loss: 0.17234391937075844
ROC train: 0.886750	val: 0.773774	test: 0.738108
PRC train: 0.518887	val: 0.349776	test: 0.347290

Epoch: 22
Loss: 0.17054677630121745
ROC train: 0.888052	val: 0.774542	test: 0.740386
PRC train: 0.507498	val: 0.355185	test: 0.342437

Epoch: 23
Loss: 0.17099196731503366
ROC train: 0.878957	val: 0.778372	test: 0.734842
PRC train: 0.494958	val: 0.355940	test: 0.344839

Epoch: 24
Loss: 0.16949997537149886
ROC train: 0.895426	val: 0.768378	test: 0.729414
PRC train: 0.546535	val: 0.339237	test: 0.337992

Epoch: 25
Loss: 0.16673598408618023
ROC train: 0.896256	val: 0.764062	test: 0.729051
PRC train: 0.542255	val: 0.334010	test: 0.322578

Epoch: 26
Loss: 0.1657658764300314
ROC train: 0.898634	val: 0.773892	test: 0.737687
PRC train: 0.550821	val: 0.341092	test: 0.339099

Epoch: 27
Loss: 0.16493081570742216
ROC train: 0.899782	val: 0.777301	test: 0.733962
PRC train: 0.546045	val: 0.342905	test: 0.347008

Epoch: 28
Loss: 0.16383865929549293
ROC train: 0.903621	val: 0.776508	test: 0.726994
PRC train: 0.567262	val: 0.353214	test: 0.349262

Epoch: 29
Loss: 0.16280107172635627
ROC train: 0.907717	val: 0.765058	test: 0.732324
PRC train: 0.584727	val: 0.355422	test: 0.353862

Epoch: 30
Loss: 0.1620392133491498
ROC train: 0.909089	val: 0.774129	test: 0.743422
PRC train: 0.587532	val: 0.345285	test: 0.359112

Epoch: 31
Loss: 0.16072749265466732
ROC train: 0.906307	val: 0.771545	test: 0.732014
PRC train: 0.561106	val: 0.354286	test: 0.338737

Epoch: 32
Loss: 0.1596331223562635
ROC train: 0.913396	val: 0.788548	test: 0.732602
ROC train: 0.912729	val: 0.774620	test: 0.729657
PRC train: 0.608492	val: 0.365574	test: 0.351158

Epoch: 34
Loss: 0.15441713395943368
ROC train: 0.910887	val: 0.773953	test: 0.733954
PRC train: 0.598865	val: 0.359526	test: 0.358734

Epoch: 35
Loss: 0.15416306508028302
ROC train: 0.915528	val: 0.781627	test: 0.740216
PRC train: 0.618506	val: 0.384935	test: 0.364165

Epoch: 36
Loss: 0.1530092745428984
ROC train: 0.919157	val: 0.774471	test: 0.737416
PRC train: 0.632198	val: 0.377906	test: 0.351983

Epoch: 37
Loss: 0.15053432532126654
ROC train: 0.921200	val: 0.771090	test: 0.736362
PRC train: 0.631680	val: 0.358644	test: 0.352638

Epoch: 38
Loss: 0.15174431856768408
ROC train: 0.918309	val: 0.775127	test: 0.740152
PRC train: 0.631552	val: 0.374994	test: 0.363697

Epoch: 39
Loss: 0.1505262398451406
ROC train: 0.899290	val: 0.744303	test: 0.710721
PRC train: 0.548494	val: 0.300747	test: 0.288451

Epoch: 40
Loss: 0.1504811916030829
ROC train: 0.923032	val: 0.764767	test: 0.734730
PRC train: 0.642867	val: 0.356787	test: 0.356113

Epoch: 41
Loss: 0.14990268705840568
ROC train: 0.924424	val: 0.772909	test: 0.737631
PRC train: 0.639922	val: 0.369572	test: 0.358675

Epoch: 42
Loss: 0.14829346236083624
ROC train: 0.924739	val: 0.772788	test: 0.741891
PRC train: 0.646104	val: 0.379913	test: 0.369984

Epoch: 43
Loss: 0.14826916013560035
ROC train: 0.926887	val: 0.776469	test: 0.742059
PRC train: 0.652742	val: 0.372829	test: 0.367861

Epoch: 44
Loss: 0.1460036559915204
ROC train: 0.929924	val: 0.774246	test: 0.742781
PRC train: 0.663788	val: 0.371985	test: 0.364019

Epoch: 45
Loss: 0.14664576997916084
ROC train: 0.926976	val: 0.763070	test: 0.737736
PRC train: 0.648893	val: 0.346208	test: 0.360089

Epoch: 46
Loss: 0.14579537496610617
ROC train: 0.929315	val: 0.767879	test: 0.736176
PRC train: 0.660729	val: 0.356170	test: 0.348361

Epoch: 47
Loss: 0.1450216337535462
ROC train: 0.928890	val: 0.772233	test: 0.726665
PRC train: 0.659539	val: 0.358382	test: 0.348958

Epoch: 48
Loss: 0.1429818937593619
ROC train: 0.933105	val: 0.778474	test: 0.744201
PRC train: 0.672995	val: 0.377200	test: 0.376745

Epoch: 49
Loss: 0.14060493997921356
ROC train: 0.934703	val: 0.775122	test: 0.742134
PRC train: 0.677567	val: 0.374722	test: 0.368038

Epoch: 50
Loss: 0.14119441485970818
ROC train: 0.936330	val: 0.779375	test: 0.742237
PRC train: 0.693463	val: 0.377873	test: 0.365361

Epoch: 51
Loss: 0.14067969768340444
ROC train: 0.937289	val: 0.766982	test: 0.751004
PRC train: 0.685959	val: 0.369737	test: 0.366448

Epoch: 52
Loss: 0.14141814403764733
ROC train: 0.937518	val: 0.780139	test: 0.746011
PRC train: 0.690533	val: 0.396910	test: 0.374911

Epoch: 53
Loss: 0.14034996864924099
ROC train: 0.939546	val: 0.766483	test: 0.726096
PRC train: 0.691743	val: 0.369525	test: 0.345924

Epoch: 54
Loss: 0.14107972644801073
ROC train: 0.940506	val: 0.774998	test: 0.749241
PRC train: 0.704765	val: 0.387198	test: 0.365442

Epoch: 55
Loss: 0.13886433769938175
ROC train: 0.940495	val: 0.764361	test: 0.736697
PRC train: 0.690894	val: 0.359718	test: 0.355988

Epoch: 56
Loss: 0.13685615747654864
ROC train: 0.940775	val: 0.774686	test: 0.737566
PRC train: 0.699821	val: 0.384014	test: 0.354949

Epoch: 57
Loss: 0.13523337833125804
ROC train: 0.943581	val: 0.769500	test: 0.750399
PRC train: 0.715531	val: 0.384447	test: 0.374330

Epoch: 58
Loss: 0.13694588966862672
ROC train: 0.944372	val: 0.765275	test: 0.736125
PRC train: 0.718868	val: 0.364597	test: 0.362493

Epoch: 59
Loss: 0.13675690901886559
ROC train: 0.946910	val: 0.770647	test: 0.739062
PRC train: 0.721672	val: 0.369864	test: 0.365319

Epoch: 60
Loss: 0.13524762457826767
ROC train: 0.944891	val: 0.765895	test: 0.740149
PRC train: 0.707560	val: 0.360852	test: 0.367449

Epoch: 61
Loss: 0.13456240271254588
ROC train: 0.946425	val: 0.762508	test: 0.737801
PRC train: 0.716348	val: 0.356526	test: 0.360255

Epoch: 62
Loss: 0.13252681454249168
ROC train: 0.948920	val: 0.777071	test: 0.742500
PRC train: 0.730001	val: 0.388795	test: 0.366664

Epoch: 63
Loss: 0.1346195656542655
ROC train: 0.949648	val: 0.779573	test: 0.748229
PRC train: 0.738896	val: 0.373386	test: 0.370857

Epoch: 64
Loss: 0.1347946129410871
ROC train: 0.943278	val: 0.764317	test: 0.737209
PRC train: 0.701501	val: 0.358261	test: 0.357731

Epoch: 65
Loss: 0.13280955496737312
ROC train: 0.951649	val: 0.768807	test: 0.743693
PRC train: 0.736413	val: 0.366181	test: 0.381664

Epoch: 66
Loss: 0.13115653917753659
ROC train: 0.952676	val: 0.770375	test: 0.734178
PRC train: 0.743401	val: 0.368762	test: 0.361719

Epoch: 67
Loss: 0.12976533638390195
ROC train: 0.954222	val: 0.771335	test: 0.734099
PRC train: 0.753871	val: 0.367266	test: 0.365075

Epoch: 68
Loss: 0.12911398242218103
ROC train: 0.953944	val: 0.763625	test: 0.740305
PRC train: 0.747292	val: 0.353844	test: 0.356439

Epoch: 69
Loss: 0.13067522298771675
ROC train: 0.951476	val: 0.774467	test: 0.734328
PRC train: 0.744010	val: 0.382011	test: 0.353286

Epoch: 70
Loss: 0.12912570459949207
ROC train: 0.956050	val: 0.774321	test: 0.737208
PRC train: 0.759612	val: 0.380959	test: 0.365456

Epoch: 71
Loss: 0.12632366609352
ROC train: 0.956051	val: 0.775840	test: 0.749049
PRC train: 0.763833	val: 0.380597	test: 0.371067

Epoch: 72
Loss: 0.12726534504438208
ROC train: 0.957575	val: 0.780751	test: 0.740046
PRC train: 0.757608	val: 0.368627	test: 0.353519

Epoch: 73
Loss: 0.1269116681219584
ROC train: 0.955430	val: 0.766748	test: 0.742921
PRC train: 0.750587	val: 0.370031	test: 0.355278

Epoch: 74
Loss: 0.1262124023183552
ROC train: 0.959552	val: 0.768212	test: 0.737578
PRC train: 0.770230	val: 0.376102	test: 0.359297

Epoch: 75
Loss: 0.125307578989764
ROC train: 0.955603	val: 0.768386	test: 0.750736
PRC train: 0.752051	val: 0.361042	test: 0.365966

Epoch: 76
Loss: 0.1249060866491983
ROC train: 0.960330	val: 0.765519	test: 0.742514
PRC train: 0.779530	val: 0.359677	test: 0.361562

Epoch: 77
Loss: 0.12370146714113756
ROC train: 0.960401	val: 0.772207	test: 0.747558
PRC train: 0.776366	val: 0.364501	test: 0.364953

Epoch: 78
Loss: 0.12278506997633272
ROC train: 0.962494	val: 0.769658	test: 0.738944
PRC train: 0.787747	val: 0.367466	test: 0.367462

Epoch: 79
Loss: 0.12283173269675063
ROC train: 0.961549	val: 0.783174	test: 0.743618
PRC train: 0.783213	val: 0.402492	test: 0.365473

Epoch: 80
Loss: 0.12196143604731496
ROC train: 0.961358	val: 0.773520	test: 0.745867
PRC train: 0.779639	val: 0.361078	test: 0.356355

Epoch: 81
Loss: 0.12074480506068895
ROC train: 0.962807	val: 0.766293	test: 0.743313
PRC train: 0.787103	val: 0.364584	test: 0.359455

Epoch: 82
Loss: 0.12241314174732608
ROC train: 0.963636	val: 0.770532	test: 0.743580
PRC train: 0.784402	val: 0.357149	test: 0.361166

Epoch: 83
Loss: 0.12222592670395921
ROC train: 0.962489	val: 0.778685	test: 0.750188
PRC train: 0.786583	val: 0.369548	test: 0.373397

Epoch: 84
Loss: 0.12160460014663743
ROC train: 0.965016	val: 0.775356	test: 0.744637
PRC train: 0.795618	val: 0.377667	test: 0.364297

Epoch: 85
Loss: 0.12075954311456273
ROC train: 0.964194	val: 0.770057	test: 0.744561
PRC train: 0.790065	val: 0.369283	test: 0.373091

Epoch: 86
Loss: 0.11980489591913253
ROC train: 0.965238	val: 0.770333	test: 0.747373
PRC train: 0.798944	val: 0.372503	test: 0.375066

Epoch: 87
Loss: 0.11726943936352219
ROC train: 0.969005	val: 0.764471	test: 0.737397
PRC train: 0.804664	val: 0.363410	test: 0.354666

Epoch: 88
Loss: 0.11753699106968725
ROC train: 0.965696	val: 0.766372	test: 0.737458
PRC train: 0.801652	val: 0.355636	test: 0.353365

Epoch: 89
Loss: 0.11718059910818679
ROC train: 0.969966	val: 0.773378	test: 0.742281
PRC train: 0.816632	val: 0.368308	test: 0.362761

Epoch: 90
Loss: 0.11768672717275885
ROC train: 0.968606	val: 0.768286	test: 0.738254
PRC train: 0.814452	val: 0.369368	test: 0.361923

Epoch: 91
Loss: 0.11629629183258772
ROC train: 0.970515	val: 0.761328	test: 0.735335
PRC train: 0.821305	val: 0.386278	test: 0.362859

Epoch: 92
Loss: 0.11523000839818948
ROC train: 0.970560	val: 0.773081	test: 0.737337
PRC train: 0.818137	val: 0.366251	test: 0.356626

Epoch: 93
Loss: 0.11447444796404907
ROC train: 0.969470	val: 0.779293	test: 0.740550
PRC train: 0.819294	val: 0.360445	test: 0.353645

Epoch: 94
Loss: 0.1150649050180108
ROC train: 0.913890	val: 0.780741	test: 0.730013
PRC train: 0.607929	val: 0.370396	test: 0.346847

Epoch: 34
Loss: 0.1547442619030609
ROC train: 0.915877	val: 0.774886	test: 0.748434
PRC train: 0.619896	val: 0.363153	test: 0.346289

Epoch: 35
Loss: 0.15205862630397246
ROC train: 0.915597	val: 0.779155	test: 0.734240
PRC train: 0.612743	val: 0.367845	test: 0.341956

Epoch: 36
Loss: 0.15245241694658213
ROC train: 0.917104	val: 0.780168	test: 0.733601
PRC train: 0.621727	val: 0.366813	test: 0.343534

Epoch: 37
Loss: 0.15106806606662035
ROC train: 0.919115	val: 0.776213	test: 0.739016
PRC train: 0.628529	val: 0.367813	test: 0.353433

Epoch: 38
Loss: 0.14974964490332554
ROC train: 0.921491	val: 0.774001	test: 0.748618
PRC train: 0.637572	val: 0.371021	test: 0.355274

Epoch: 39
Loss: 0.15004375476188495
ROC train: 0.921841	val: 0.776286	test: 0.746217
PRC train: 0.637052	val: 0.364960	test: 0.350826

Epoch: 40
Loss: 0.1496873059516145
ROC train: 0.925146	val: 0.773160	test: 0.743906
PRC train: 0.651926	val: 0.374511	test: 0.361137

Epoch: 41
Loss: 0.14934590277106233
ROC train: 0.920863	val: 0.770881	test: 0.740990
PRC train: 0.635378	val: 0.366430	test: 0.357694

Epoch: 42
Loss: 0.14855598031570896
ROC train: 0.927063	val: 0.774687	test: 0.742631
PRC train: 0.653586	val: 0.372309	test: 0.354120

Epoch: 43
Loss: 0.1476933178574633
ROC train: 0.927427	val: 0.775058	test: 0.743421
PRC train: 0.652403	val: 0.367975	test: 0.359662

Epoch: 44
Loss: 0.1462424428075068
ROC train: 0.928937	val: 0.776450	test: 0.744807
PRC train: 0.657442	val: 0.370580	test: 0.356304

Epoch: 45
Loss: 0.14448649796561688
ROC train: 0.930382	val: 0.770511	test: 0.742017
PRC train: 0.665907	val: 0.368633	test: 0.346332

Epoch: 46
Loss: 0.14599255210107664
ROC train: 0.929928	val: 0.767815	test: 0.740391
PRC train: 0.672115	val: 0.383517	test: 0.360705

Epoch: 47
Loss: 0.1451000145019111
ROC train: 0.929379	val: 0.765874	test: 0.731020
PRC train: 0.655599	val: 0.363845	test: 0.341622

Epoch: 48
Loss: 0.1424926052455
ROC train: 0.934267	val: 0.763753	test: 0.731353
PRC train: 0.677550	val: 0.359551	test: 0.347750

Epoch: 49
Loss: 0.14293653678684637
ROC train: 0.935140	val: 0.771108	test: 0.739964
PRC train: 0.678097	val: 0.368541	test: 0.340972

Epoch: 50
Loss: 0.14195947135023246
ROC train: 0.935355	val: 0.781584	test: 0.736807
PRC train: 0.679903	val: 0.363450	test: 0.349776

Epoch: 51
Loss: 0.14159346226379316
ROC train: 0.937995	val: 0.775758	test: 0.747171
PRC train: 0.693181	val: 0.380398	test: 0.356720

Epoch: 52
Loss: 0.1403827398548407
ROC train: 0.934951	val: 0.758517	test: 0.739980
PRC train: 0.667255	val: 0.353108	test: 0.342906

Epoch: 53
Loss: 0.13888081420576298
ROC train: 0.939725	val: 0.773149	test: 0.745015
PRC train: 0.697060	val: 0.368926	test: 0.363486

Epoch: 54
Loss: 0.1397152423986769
ROC train: 0.940291	val: 0.769036	test: 0.739169
PRC train: 0.698835	val: 0.376412	test: 0.351306

Epoch: 55
Loss: 0.1374341240576439
ROC train: 0.941856	val: 0.769286	test: 0.739344
PRC train: 0.698116	val: 0.357988	test: 0.344951

Epoch: 56
Loss: 0.13806341863745308
ROC train: 0.943074	val: 0.776562	test: 0.741736
PRC train: 0.703356	val: 0.375356	test: 0.355607

Epoch: 57
Loss: 0.13628426952261527
ROC train: 0.940288	val: 0.763627	test: 0.731754
PRC train: 0.691115	val: 0.357606	test: 0.340872

Epoch: 58
Loss: 0.13645409967925942
ROC train: 0.944420	val: 0.773554	test: 0.737782
PRC train: 0.712218	val: 0.371348	test: 0.353666

Epoch: 59
Loss: 0.13733024940424465
ROC train: 0.945325	val: 0.775024	test: 0.737123
PRC train: 0.719499	val: 0.376398	test: 0.349720

Epoch: 60
Loss: 0.13492597800107808
ROC train: 0.947527	val: 0.769336	test: 0.742443
PRC train: 0.722311	val: 0.368820	test: 0.367049

Epoch: 61
Loss: 0.13556801005562932
ROC train: 0.947220	val: 0.768221	test: 0.736284
PRC train: 0.722905	val: 0.364889	test: 0.358860

Epoch: 62
Loss: 0.13401787988900138
ROC train: 0.946378	val: 0.776344	test: 0.742327
PRC train: 0.715966	val: 0.372662	test: 0.358171

Epoch: 63
Loss: 0.13327396108531522
ROC train: 0.948522	val: 0.779124	test: 0.744358
PRC train: 0.725219	val: 0.377680	test: 0.352123

Epoch: 64
Loss: 0.13342359562461825
ROC train: 0.950101	val: 0.775087	test: 0.739957
PRC train: 0.729694	val: 0.362530	test: 0.348835

Epoch: 65
Loss: 0.13285762693637693
ROC train: 0.951180	val: 0.775547	test: 0.752215
PRC train: 0.734979	val: 0.366428	test: 0.358860

Epoch: 66
Loss: 0.13253852746694902
ROC train: 0.951847	val: 0.773086	test: 0.747161
PRC train: 0.742481	val: 0.369958	test: 0.361332

Epoch: 67
Loss: 0.13093460839489562
ROC train: 0.950244	val: 0.776114	test: 0.745268
PRC train: 0.736861	val: 0.366351	test: 0.354656

Epoch: 68
Loss: 0.13215081791296993
ROC train: 0.954097	val: 0.772011	test: 0.743201
PRC train: 0.745523	val: 0.360349	test: 0.350122

Epoch: 69
Loss: 0.129943681162865
ROC train: 0.953001	val: 0.775915	test: 0.749857
PRC train: 0.746604	val: 0.372899	test: 0.362734

Epoch: 70
Loss: 0.12909991065979168
ROC train: 0.953994	val: 0.773563	test: 0.744019
PRC train: 0.745168	val: 0.361610	test: 0.344962

Epoch: 71
Loss: 0.12721694641703382
ROC train: 0.956320	val: 0.766657	test: 0.742800
PRC train: 0.759452	val: 0.362596	test: 0.341818

Epoch: 72
Loss: 0.1270377679511536
ROC train: 0.956498	val: 0.774977	test: 0.743834
PRC train: 0.762982	val: 0.373691	test: 0.345305

Epoch: 73
Loss: 0.12731035540367394
ROC train: 0.958303	val: 0.777604	test: 0.752203
PRC train: 0.765101	val: 0.367274	test: 0.366780

Epoch: 74
Loss: 0.12668756015173321
ROC train: 0.957489	val: 0.776219	test: 0.745219
PRC train: 0.763186	val: 0.384716	test: 0.355490

Epoch: 75
Loss: 0.12556002453477366
ROC train: 0.959894	val: 0.774280	test: 0.744923
PRC train: 0.779143	val: 0.364340	test: 0.345431

Epoch: 76
Loss: 0.12464365326588038
ROC train: 0.960397	val: 0.762758	test: 0.743187
PRC train: 0.777575	val: 0.370063	test: 0.360322

Epoch: 77
Loss: 0.12545218744927758
ROC train: 0.961317	val: 0.774372	test: 0.745951
PRC train: 0.779733	val: 0.380576	test: 0.356220

Epoch: 78
Loss: 0.12444557800037495
ROC train: 0.961522	val: 0.753069	test: 0.742074
PRC train: 0.777841	val: 0.350853	test: 0.346437

Epoch: 79
Loss: 0.1226258514531823
ROC train: 0.962301	val: 0.769617	test: 0.756374
PRC train: 0.783088	val: 0.372783	test: 0.362827

Epoch: 80
Loss: 0.1228594496933652
ROC train: 0.963220	val: 0.777350	test: 0.744091
PRC train: 0.788616	val: 0.375001	test: 0.359878

Epoch: 81
Loss: 0.12281863636806159
ROC train: 0.962985	val: 0.774254	test: 0.747021
PRC train: 0.786041	val: 0.366838	test: 0.348789

Epoch: 82
Loss: 0.12158784013715851
ROC train: 0.964257	val: 0.780395	test: 0.741384
PRC train: 0.795094	val: 0.381988	test: 0.355128

Epoch: 83
Loss: 0.12045224046521183
ROC train: 0.966383	val: 0.777704	test: 0.740577
PRC train: 0.803532	val: 0.383994	test: 0.359143

Epoch: 84
Loss: 0.1198767710661157
ROC train: 0.963391	val: 0.770290	test: 0.736536
PRC train: 0.781381	val: 0.360047	test: 0.344064

Epoch: 85
Loss: 0.12147258776695762
ROC train: 0.965922	val: 0.777415	test: 0.744431
PRC train: 0.796804	val: 0.370448	test: 0.345089

Epoch: 86
Loss: 0.1201114357743357
ROC train: 0.967510	val: 0.768033	test: 0.739121
PRC train: 0.806259	val: 0.364335	test: 0.339890

Epoch: 87
Loss: 0.11863292978746365
ROC train: 0.967248	val: 0.774969	test: 0.748260
PRC train: 0.804498	val: 0.383673	test: 0.361357

Epoch: 88
Loss: 0.11715259492606839
ROC train: 0.962289	val: 0.773613	test: 0.737519
PRC train: 0.784820	val: 0.367049	test: 0.331895

Epoch: 89
Loss: 0.11839536674605221
ROC train: 0.965116	val: 0.777422	test: 0.744303
PRC train: 0.793466	val: 0.371003	test: 0.343778

Epoch: 90
Loss: 0.11817015625161391
ROC train: 0.969640	val: 0.773666	test: 0.749555
PRC train: 0.819798	val: 0.356652	test: 0.348601

Epoch: 91
Loss: 0.11791889851437906
ROC train: 0.969038	val: 0.767398	test: 0.753734
PRC train: 0.816084	val: 0.358338	test: 0.350002

Epoch: 92
Loss: 0.11612969482941496
ROC train: 0.965879	val: 0.768070	test: 0.737134
PRC train: 0.801409	val: 0.339494	test: 0.334122

Epoch: 93
Loss: 0.1146934471589885
ROC train: 0.971498	val: 0.775987	test: 0.741553
PRC train: 0.828178	val: 0.374674	test: 0.344917

Epoch: 94
Loss: 0.11430295802341979
ROC train: 0.909831	val: 0.780638	test: 0.730282
PRC train: 0.604496	val: 0.385853	test: 0.342600

Epoch: 34
Loss: 0.154096610152166
ROC train: 0.914881	val: 0.788647	test: 0.738913
PRC train: 0.619240	val: 0.388126	test: 0.352591

Epoch: 35
Loss: 0.15478444444156422
ROC train: 0.912805	val: 0.786964	test: 0.736638
PRC train: 0.612503	val: 0.375774	test: 0.350169

Epoch: 36
Loss: 0.1535128673818624
ROC train: 0.917369	val: 0.785463	test: 0.741516
PRC train: 0.619835	val: 0.386282	test: 0.349485

Epoch: 37
Loss: 0.1503996936964096
ROC train: 0.917656	val: 0.786152	test: 0.737948
PRC train: 0.627097	val: 0.390495	test: 0.348570

Epoch: 38
Loss: 0.15211413055689488
ROC train: 0.921636	val: 0.781800	test: 0.747781
PRC train: 0.637722	val: 0.388483	test: 0.353852

Epoch: 39
Loss: 0.151028419961227
ROC train: 0.919772	val: 0.774703	test: 0.753562
PRC train: 0.635905	val: 0.393365	test: 0.363556

Epoch: 40
Loss: 0.14859354632532593
ROC train: 0.923410	val: 0.783311	test: 0.737991
PRC train: 0.641670	val: 0.372805	test: 0.351437

Epoch: 41
Loss: 0.14831070633470958
ROC train: 0.924627	val: 0.785442	test: 0.745778
PRC train: 0.643280	val: 0.383509	test: 0.352872

Epoch: 42
Loss: 0.14680541507417924
ROC train: 0.926576	val: 0.779984	test: 0.743298
PRC train: 0.652291	val: 0.388889	test: 0.355892

Epoch: 43
Loss: 0.1474596071493693
ROC train: 0.928561	val: 0.777478	test: 0.731748
PRC train: 0.652337	val: 0.383871	test: 0.356057

Epoch: 44
Loss: 0.14561196444157612
ROC train: 0.929376	val: 0.777351	test: 0.739961
PRC train: 0.663851	val: 0.378721	test: 0.353034

Epoch: 45
Loss: 0.1440575904498881
ROC train: 0.931209	val: 0.783266	test: 0.747047
PRC train: 0.663840	val: 0.379902	test: 0.351795

Epoch: 46
Loss: 0.14491976474852875
ROC train: 0.931677	val: 0.769199	test: 0.740636
PRC train: 0.667114	val: 0.375319	test: 0.346276

Epoch: 47
Loss: 0.1429433286480886
ROC train: 0.934405	val: 0.763133	test: 0.743371
PRC train: 0.679315	val: 0.383096	test: 0.367646

Epoch: 48
Loss: 0.1423381828933673
ROC train: 0.936650	val: 0.771416	test: 0.741979
PRC train: 0.679159	val: 0.380570	test: 0.364232

Epoch: 49
Loss: 0.1424352337417718
ROC train: 0.936324	val: 0.772208	test: 0.747988
PRC train: 0.680704	val: 0.377907	test: 0.359800

Epoch: 50
Loss: 0.14299084740373239
ROC train: 0.933960	val: 0.785734	test: 0.750425
PRC train: 0.678946	val: 0.402755	test: 0.374310

Epoch: 51
Loss: 0.14266393663907814
ROC train: 0.937539	val: 0.781708	test: 0.751459
PRC train: 0.684048	val: 0.388941	test: 0.370592

Epoch: 52
Loss: 0.14273267845621843
ROC train: 0.939235	val: 0.776686	test: 0.744715
PRC train: 0.690931	val: 0.391652	test: 0.360133

Epoch: 53
Loss: 0.13914045396452088
ROC train: 0.932350	val: 0.754143	test: 0.741012
PRC train: 0.659051	val: 0.334487	test: 0.326597

Epoch: 54
Loss: 0.13955085786997892
ROC train: 0.939343	val: 0.773335	test: 0.739115
PRC train: 0.690564	val: 0.381146	test: 0.355255

Epoch: 55
Loss: 0.13931829850009397
ROC train: 0.940338	val: 0.786523	test: 0.755001
PRC train: 0.694023	val: 0.409724	test: 0.367418

Epoch: 56
Loss: 0.1399208321445114
ROC train: 0.943958	val: 0.770952	test: 0.749599
PRC train: 0.716862	val: 0.391853	test: 0.371467

Epoch: 57
Loss: 0.136906206562627
ROC train: 0.944273	val: 0.784986	test: 0.738173
PRC train: 0.707022	val: 0.398775	test: 0.349242

Epoch: 58
Loss: 0.13864261781381987
ROC train: 0.945055	val: 0.779133	test: 0.748722
PRC train: 0.714193	val: 0.406969	test: 0.368556

Epoch: 59
Loss: 0.1353204519588687
ROC train: 0.947652	val: 0.757956	test: 0.741304
PRC train: 0.725760	val: 0.376862	test: 0.354750

Epoch: 60
Loss: 0.13632452495465702
ROC train: 0.947048	val: 0.778755	test: 0.747745
PRC train: 0.722788	val: 0.392830	test: 0.370705

Epoch: 61
Loss: 0.13354819147722843
ROC train: 0.945180	val: 0.778776	test: 0.746793
PRC train: 0.711852	val: 0.383304	test: 0.362753

Epoch: 62
Loss: 0.1332963502543651
ROC train: 0.947738	val: 0.771611	test: 0.753408
PRC train: 0.722583	val: 0.378654	test: 0.364114

Epoch: 63
Loss: 0.1330906074858441
ROC train: 0.949749	val: 0.771516	test: 0.746038
PRC train: 0.730247	val: 0.389584	test: 0.355680

Epoch: 64
Loss: 0.13051218492698477
ROC train: 0.949974	val: 0.778909	test: 0.752905
PRC train: 0.735837	val: 0.398187	test: 0.360564

Epoch: 65
Loss: 0.13195006322197414
ROC train: 0.950726	val: 0.770334	test: 0.748409
PRC train: 0.733853	val: 0.372031	test: 0.353796

Epoch: 66
Loss: 0.13051532831744161
ROC train: 0.950896	val: 0.769537	test: 0.754028
PRC train: 0.733800	val: 0.375731	test: 0.349285

Epoch: 67
Loss: 0.13117429906451325
ROC train: 0.954146	val: 0.773215	test: 0.745445
PRC train: 0.746538	val: 0.384794	test: 0.355344

Epoch: 68
Loss: 0.12968385330190577
ROC train: 0.954485	val: 0.777168	test: 0.750466
PRC train: 0.755487	val: 0.387633	test: 0.361692

Epoch: 69
Loss: 0.12955232989238127
ROC train: 0.956089	val: 0.768525	test: 0.745000
PRC train: 0.756461	val: 0.382262	test: 0.363176

Epoch: 70
Loss: 0.128796035204374
ROC train: 0.954223	val: 0.770085	test: 0.745578
PRC train: 0.754636	val: 0.372116	test: 0.362606

Epoch: 71
Loss: 0.1268350191262172
ROC train: 0.956648	val: 0.764551	test: 0.743192
PRC train: 0.760978	val: 0.372035	test: 0.350844

Epoch: 72
Loss: 0.1258748987467533
ROC train: 0.958885	val: 0.769890	test: 0.750091
PRC train: 0.775289	val: 0.387884	test: 0.360318

Epoch: 73
Loss: 0.12600403585029618
ROC train: 0.958109	val: 0.776086	test: 0.744909
PRC train: 0.767996	val: 0.381497	test: 0.356608

Epoch: 74
Loss: 0.125397068806393
ROC train: 0.957450	val: 0.771601	test: 0.743733
PRC train: 0.765457	val: 0.376855	test: 0.354497

Epoch: 75
Loss: 0.12616851390692882
ROC train: 0.958203	val: 0.776566	test: 0.740612
PRC train: 0.771837	val: 0.384322	test: 0.358792

Epoch: 76
Loss: 0.124785388256814
ROC train: 0.958934	val: 0.770913	test: 0.739384
PRC train: 0.772933	val: 0.383871	test: 0.360840

Epoch: 77
Loss: 0.1243862554285667
ROC train: 0.962155	val: 0.769165	test: 0.745783
PRC train: 0.784027	val: 0.376280	test: 0.357067

Epoch: 78
Loss: 0.12214113451451972
ROC train: 0.958493	val: 0.771918	test: 0.745744
PRC train: 0.763114	val: 0.378485	test: 0.360502

Epoch: 79
Loss: 0.1238316433650277
ROC train: 0.962408	val: 0.766623	test: 0.743914
PRC train: 0.787578	val: 0.374134	test: 0.360187

Epoch: 80
Loss: 0.12202760282878061
ROC train: 0.963009	val: 0.771222	test: 0.737843
PRC train: 0.785526	val: 0.378963	test: 0.355322

Epoch: 81
Loss: 0.12255342877873979
ROC train: 0.963149	val: 0.770665	test: 0.742720
PRC train: 0.792215	val: 0.387017	test: 0.366212

Epoch: 82
Loss: 0.12102064062316183
ROC train: 0.965475	val: 0.782539	test: 0.745489
PRC train: 0.797101	val: 0.402601	test: 0.369741

Epoch: 83
Loss: 0.11992185692167448
ROC train: 0.963431	val: 0.776880	test: 0.754033
PRC train: 0.788621	val: 0.381411	test: 0.368044

Epoch: 84
Loss: 0.11991388408599037
ROC train: 0.965559	val: 0.776950	test: 0.746446
PRC train: 0.799323	val: 0.393003	test: 0.354914

Epoch: 85
Loss: 0.11906783916005882
ROC train: 0.967133	val: 0.780064	test: 0.745702
PRC train: 0.802943	val: 0.406069	test: 0.363318

Epoch: 86
Loss: 0.12024208920881199
ROC train: 0.966418	val: 0.771800	test: 0.736863
PRC train: 0.798778	val: 0.382079	test: 0.354677

Epoch: 87
Loss: 0.11864849386137505
ROC train: 0.966453	val: 0.770311	test: 0.737246
PRC train: 0.803245	val: 0.389406	test: 0.362715

Epoch: 88
Loss: 0.11734019737311603
ROC train: 0.969173	val: 0.772593	test: 0.742164
PRC train: 0.817595	val: 0.388857	test: 0.349668

Epoch: 89
Loss: 0.11665758461128338
ROC train: 0.969487	val: 0.768461	test: 0.738406
PRC train: 0.820032	val: 0.374179	test: 0.342727

Epoch: 90
Loss: 0.11553185067047723
ROC train: 0.970273	val: 0.761685	test: 0.741564
PRC train: 0.818797	val: 0.376171	test: 0.352255

Epoch: 91
Loss: 0.1167222908671958
ROC train: 0.970554	val: 0.768919	test: 0.743637
PRC train: 0.821386	val: 0.379441	test: 0.358255

Epoch: 92
Loss: 0.11405922962481309
ROC train: 0.971132	val: 0.766553	test: 0.737421
PRC train: 0.823548	val: 0.367660	test: 0.345324

Epoch: 93
Loss: 0.11403129098950857
ROC train: 0.972023	val: 0.771516	test: 0.736435
PRC train: 0.830369	val: 0.388466	test: 0.356397

Epoch: 94
Loss: 0.1112032141970758
PRC train: 0.569742	val: 0.324117	test: 0.315739

Epoch: 33
Loss: 0.16299917571883035
ROC train: 0.913268	val: 0.714685	test: 0.700155
PRC train: 0.595700	val: 0.293905	test: 0.299066

Epoch: 34
Loss: 0.16234643201264987
ROC train: 0.915031	val: 0.731266	test: 0.720431
PRC train: 0.603088	val: 0.323549	test: 0.319476

Epoch: 35
Loss: 0.16171001186767178
ROC train: 0.918522	val: 0.744883	test: 0.719314
PRC train: 0.614164	val: 0.333918	test: 0.327493

Epoch: 36
Loss: 0.1595626678992873
ROC train: 0.908285	val: 0.725134	test: 0.704481
PRC train: 0.565518	val: 0.294317	test: 0.299153

Epoch: 37
Loss: 0.16178426949636868
ROC train: 0.917325	val: 0.739371	test: 0.712301
PRC train: 0.613544	val: 0.330612	test: 0.315733

Epoch: 38
Loss: 0.15879293358035884
ROC train: 0.919169	val: 0.748363	test: 0.721141
PRC train: 0.614114	val: 0.327346	test: 0.330190

Epoch: 39
Loss: 0.15688669205161757
ROC train: 0.927672	val: 0.733125	test: 0.708407
PRC train: 0.642162	val: 0.313570	test: 0.315520

Epoch: 40
Loss: 0.15552892085269598
ROC train: 0.919237	val: 0.731111	test: 0.699335
PRC train: 0.602529	val: 0.294769	test: 0.311618

Epoch: 41
Loss: 0.15373536128384763
ROC train: 0.931948	val: 0.733432	test: 0.695093
PRC train: 0.656225	val: 0.307712	test: 0.305164

Epoch: 42
Loss: 0.1519003437719696
ROC train: 0.932392	val: 0.741649	test: 0.721157
PRC train: 0.662567	val: 0.328350	test: 0.326318

Epoch: 43
Loss: 0.15422710656069244
ROC train: 0.933513	val: 0.735061	test: 0.706688
PRC train: 0.663458	val: 0.322088	test: 0.322479

Epoch: 44
Loss: 0.14913306856532174
ROC train: 0.934063	val: 0.746082	test: 0.717391
PRC train: 0.661536	val: 0.324719	test: 0.336910

Epoch: 45
Loss: 0.15040225883012748
ROC train: 0.936520	val: 0.738129	test: 0.703979
PRC train: 0.675474	val: 0.322039	test: 0.322198

Epoch: 46
Loss: 0.14921095148876598
ROC train: 0.938016	val: 0.734798	test: 0.708678
PRC train: 0.669598	val: 0.302811	test: 0.311457

Epoch: 47
Loss: 0.1490660586926025
ROC train: 0.940063	val: 0.730306	test: 0.710397
PRC train: 0.688252	val: 0.321999	test: 0.328098

Epoch: 48
Loss: 0.14476012284963072
ROC train: 0.944804	val: 0.732164	test: 0.719306
PRC train: 0.704464	val: 0.323269	test: 0.327193

Epoch: 49
Loss: 0.14464891807277602
ROC train: 0.944638	val: 0.739574	test: 0.711089
PRC train: 0.707487	val: 0.336702	test: 0.324133

Epoch: 50
Loss: 0.1434025147059957
ROC train: 0.947530	val: 0.721257	test: 0.706059
PRC train: 0.708891	val: 0.303965	test: 0.320236

Epoch: 51
Loss: 0.1435941517957741
ROC train: 0.950518	val: 0.732387	test: 0.708092
PRC train: 0.723359	val: 0.324709	test: 0.329210

Epoch: 52
Loss: 0.1421890014387926
ROC train: 0.951888	val: 0.724824	test: 0.713262
PRC train: 0.727083	val: 0.325705	test: 0.323512

Epoch: 53
Loss: 0.14018887590838844
ROC train: 0.951329	val: 0.724898	test: 0.719048
PRC train: 0.727954	val: 0.315038	test: 0.321182

Epoch: 54
Loss: 0.14046652151988906
ROC train: 0.952308	val: 0.740604	test: 0.711197
PRC train: 0.723840	val: 0.320186	test: 0.321472

Epoch: 55
Loss: 0.1379559594984256
ROC train: 0.955494	val: 0.727619	test: 0.715873
PRC train: 0.746321	val: 0.326147	test: 0.333257

Epoch: 56
Loss: 0.13782153580188625
ROC train: 0.956155	val: 0.725731	test: 0.704196
PRC train: 0.733513	val: 0.329882	test: 0.300249

Epoch: 57
Loss: 0.13484192188502497
ROC train: 0.956529	val: 0.729902	test: 0.704013
PRC train: 0.746464	val: 0.308109	test: 0.317837

Epoch: 58
Loss: 0.13351670543901098
ROC train: 0.958948	val: 0.740497	test: 0.709641
PRC train: 0.752312	val: 0.312981	test: 0.312862

Epoch: 59
Loss: 0.13523518350042316
ROC train: 0.961264	val: 0.734497	test: 0.707963
PRC train: 0.759153	val: 0.321226	test: 0.305883

Epoch: 60
Loss: 0.1321990634648204
ROC train: 0.960268	val: 0.726487	test: 0.713600
PRC train: 0.756157	val: 0.315795	test: 0.319247

Epoch: 61
Loss: 0.13285545588133452
ROC train: 0.962309	val: 0.727742	test: 0.705537
PRC train: 0.767468	val: 0.313471	test: 0.305389

Epoch: 62
Loss: 0.13034533475324162
ROC train: 0.963702	val: 0.737868	test: 0.719359
PRC train: 0.775156	val: 0.332467	test: 0.329413

Epoch: 63
Loss: 0.1271165215252894
ROC train: 0.966690	val: 0.725139	test: 0.703502
PRC train: 0.787509	val: 0.316871	test: 0.308124

Epoch: 64
Loss: 0.1270717376811917
ROC train: 0.967899	val: 0.728919	test: 0.710125
PRC train: 0.796206	val: 0.326249	test: 0.314105

Epoch: 65
Loss: 0.1259571291705784
ROC train: 0.969884	val: 0.744147	test: 0.715583
PRC train: 0.800161	val: 0.317849	test: 0.323417

Epoch: 66
Loss: 0.12378748233484621
ROC train: 0.971291	val: 0.723296	test: 0.706258
PRC train: 0.815224	val: 0.307121	test: 0.322783

Epoch: 67
Loss: 0.12366762525990942
ROC train: 0.969666	val: 0.738210	test: 0.718770
PRC train: 0.801189	val: 0.326198	test: 0.312761

Epoch: 68
Loss: 0.12293586192938405
ROC train: 0.971613	val: 0.738626	test: 0.693808
PRC train: 0.809517	val: 0.303698	test: 0.306660

Epoch: 69
Loss: 0.12361413448531718
ROC train: 0.974448	val: 0.726919	test: 0.704407
PRC train: 0.829927	val: 0.315701	test: 0.310900

Epoch: 70
Loss: 0.12018186266337613
ROC train: 0.974986	val: 0.723733	test: 0.703344
PRC train: 0.832141	val: 0.310720	test: 0.320771

Epoch: 71
Loss: 0.11975162408775562
ROC train: 0.974749	val: 0.718353	test: 0.699883
PRC train: 0.830177	val: 0.320695	test: 0.322500

Epoch: 72
Loss: 0.11856883558946021
ROC train: 0.975177	val: 0.713504	test: 0.708468
PRC train: 0.833339	val: 0.295890	test: 0.319747

Epoch: 73
Loss: 0.11718291251640231
ROC train: 0.977936	val: 0.719892	test: 0.702248
PRC train: 0.842643	val: 0.312198	test: 0.312033

Epoch: 74
Loss: 0.11575788155734179
ROC train: 0.977571	val: 0.708098	test: 0.700636
PRC train: 0.845049	val: 0.284731	test: 0.297243

Epoch: 75
Loss: 0.11526311957041693
ROC train: 0.981102	val: 0.694519	test: 0.683072
PRC train: 0.858614	val: 0.295546	test: 0.295365

Epoch: 76
Loss: 0.11526760380308812
ROC train: 0.979228	val: 0.733673	test: 0.713432
PRC train: 0.851520	val: 0.312152	test: 0.317549

Epoch: 77
Loss: 0.11307904915325745
ROC train: 0.978762	val: 0.728010	test: 0.705309
PRC train: 0.851711	val: 0.308895	test: 0.305462

Epoch: 78
Loss: 0.11257593326325205
ROC train: 0.982079	val: 0.727125	test: 0.695470
PRC train: 0.872163	val: 0.309085	test: 0.289108

Epoch: 79
Loss: 0.1119366320307501
ROC train: 0.982594	val: 0.693508	test: 0.687145
PRC train: 0.869628	val: 0.282196	test: 0.290617

Epoch: 80
Loss: 0.10982799408233428
ROC train: 0.984062	val: 0.739657	test: 0.706700
PRC train: 0.876917	val: 0.325419	test: 0.318131

Epoch: 81
Loss: 0.11025046646876507
ROC train: 0.983076	val: 0.727724	test: 0.710879
PRC train: 0.881106	val: 0.306512	test: 0.312629

Epoch: 82
Loss: 0.10883240207959027
ROC train: 0.982848	val: 0.719237	test: 0.701380
PRC train: 0.871513	val: 0.303439	test: 0.299301

Epoch: 83
Loss: 0.10789637923248291
ROC train: 0.985303	val: 0.724057	test: 0.700754
PRC train: 0.888221	val: 0.300125	test: 0.296436

Epoch: 84
Loss: 0.1049181560957928
ROC train: 0.985548	val: 0.708441	test: 0.690738
PRC train: 0.889189	val: 0.294367	test: 0.291520

Epoch: 85
Loss: 0.10490039113168174
ROC train: 0.987380	val: 0.725508	test: 0.698304
PRC train: 0.901236	val: 0.304062	test: 0.305083

Epoch: 86
Loss: 0.10293874185434002
ROC train: 0.986141	val: 0.712581	test: 0.691096
PRC train: 0.894606	val: 0.291950	test: 0.283524

Epoch: 87
Loss: 0.10259470537249048
ROC train: 0.987758	val: 0.719112	test: 0.701080
PRC train: 0.902583	val: 0.306908	test: 0.303503

Epoch: 88
Loss: 0.10141193562791141
ROC train: 0.989315	val: 0.724038	test: 0.694451
PRC train: 0.915098	val: 0.308346	test: 0.315162

Epoch: 89
Loss: 0.09968178685380943
ROC train: 0.989102	val: 0.712832	test: 0.701442
PRC train: 0.910761	val: 0.297032	test: 0.303164

Epoch: 90
Loss: 0.09760767253587568
ROC train: 0.987979	val: 0.690103	test: 0.663992
PRC train: 0.895690	val: 0.263711	test: 0.267749

Epoch: 91
Loss: 0.0994038208796926
ROC train: 0.989972	val: 0.710969	test: 0.677317
PRC train: 0.920292	val: 0.286301	test: 0.265277

Epoch: 92
Loss: 0.09505557131397925
ROC train: 0.990546	val: 0.713239	test: 0.693919
PRC train: 0.922627	val: 0.287893	test: 0.285255

Epoch: 93
Loss: 0.09489454680037855
ROC train: 0.991754	val: 0.703594	test: 0.692917
PRC train: 0.568647	val: 0.319861	test: 0.319848

Epoch: 33
Loss: 0.1602766489849612
ROC train: 0.912485	val: 0.759447	test: 0.725513
PRC train: 0.592530	val: 0.322700	test: 0.312192

Epoch: 34
Loss: 0.1602642988465407
ROC train: 0.914382	val: 0.772737	test: 0.730953
PRC train: 0.597974	val: 0.328292	test: 0.334533

Epoch: 35
Loss: 0.15802415862439656
ROC train: 0.918169	val: 0.765334	test: 0.715370
PRC train: 0.609285	val: 0.351829	test: 0.329022

Epoch: 36
Loss: 0.15875643658157404
ROC train: 0.917726	val: 0.768117	test: 0.726263
PRC train: 0.606905	val: 0.336395	test: 0.339217

Epoch: 37
Loss: 0.15546021753736683
ROC train: 0.926125	val: 0.770237	test: 0.722087
PRC train: 0.637616	val: 0.345491	test: 0.335411

Epoch: 38
Loss: 0.1544377275509115
ROC train: 0.921881	val: 0.767857	test: 0.730711
PRC train: 0.625812	val: 0.338998	test: 0.338404

Epoch: 39
Loss: 0.15267144415196796
ROC train: 0.925640	val: 0.771360	test: 0.729379
PRC train: 0.637288	val: 0.357608	test: 0.348224

Epoch: 40
Loss: 0.15321052045019637
ROC train: 0.927462	val: 0.774149	test: 0.728188
PRC train: 0.643793	val: 0.347386	test: 0.340154

Epoch: 41
Loss: 0.15082906477629096
ROC train: 0.926337	val: 0.759038	test: 0.713784
PRC train: 0.641349	val: 0.323765	test: 0.312572

Epoch: 42
Loss: 0.1502407860751649
ROC train: 0.930158	val: 0.775556	test: 0.723570
PRC train: 0.653358	val: 0.350077	test: 0.346488

Epoch: 43
Loss: 0.14926089649843796
ROC train: 0.933889	val: 0.772216	test: 0.735037
PRC train: 0.663920	val: 0.359936	test: 0.335601

Epoch: 44
Loss: 0.14726499816375074
ROC train: 0.934077	val: 0.764373	test: 0.716975
PRC train: 0.655766	val: 0.347953	test: 0.335811

Epoch: 45
Loss: 0.1485249358518715
ROC train: 0.936141	val: 0.773154	test: 0.731288
PRC train: 0.665401	val: 0.348475	test: 0.332567

Epoch: 46
Loss: 0.14519916633212165
ROC train: 0.940949	val: 0.769928	test: 0.721783
PRC train: 0.688887	val: 0.355438	test: 0.329683

Epoch: 47
Loss: 0.14726632924756328
ROC train: 0.939900	val: 0.766141	test: 0.728214
PRC train: 0.681191	val: 0.332612	test: 0.325316

Epoch: 48
Loss: 0.1436252153375949
ROC train: 0.943091	val: 0.759494	test: 0.717608
PRC train: 0.691358	val: 0.340552	test: 0.308072

Epoch: 49
Loss: 0.1433546663265413
ROC train: 0.935488	val: 0.778159	test: 0.721622
PRC train: 0.669464	val: 0.381366	test: 0.348323

Epoch: 50
Loss: 0.1427555179019236
ROC train: 0.943999	val: 0.770362	test: 0.736240
PRC train: 0.706886	val: 0.363312	test: 0.346639

Epoch: 51
Loss: 0.14021486487251705
ROC train: 0.946309	val: 0.768905	test: 0.740618
PRC train: 0.704516	val: 0.346202	test: 0.335574

Epoch: 52
Loss: 0.14059418132599893
ROC train: 0.944296	val: 0.759657	test: 0.733022
PRC train: 0.698978	val: 0.337149	test: 0.328677

Epoch: 53
Loss: 0.13876924853500758
ROC train: 0.950744	val: 0.772644	test: 0.737050
PRC train: 0.732412	val: 0.371975	test: 0.348918

Epoch: 54
Loss: 0.1371067489757254
ROC train: 0.950826	val: 0.769958	test: 0.725301
PRC train: 0.727598	val: 0.354702	test: 0.335818

Epoch: 55
Loss: 0.13558923874755974
ROC train: 0.951334	val: 0.776130	test: 0.724499
PRC train: 0.731629	val: 0.373144	test: 0.341763

Epoch: 56
Loss: 0.1371288053844945
ROC train: 0.953681	val: 0.752569	test: 0.719126
PRC train: 0.734966	val: 0.333634	test: 0.325537

Epoch: 57
Loss: 0.134046786734215
ROC train: 0.956086	val: 0.767998	test: 0.729494
PRC train: 0.748988	val: 0.364803	test: 0.336729

Epoch: 58
Loss: 0.1333802852693
ROC train: 0.955556	val: 0.761299	test: 0.719054
PRC train: 0.742916	val: 0.354192	test: 0.324330

Epoch: 59
Loss: 0.13363533585531898
ROC train: 0.956351	val: 0.750214	test: 0.708136
PRC train: 0.742517	val: 0.329236	test: 0.319050

Epoch: 60
Loss: 0.13197820618412884
ROC train: 0.955426	val: 0.772808	test: 0.720248
PRC train: 0.747244	val: 0.367055	test: 0.340386

Epoch: 61
Loss: 0.13098460712422372
ROC train: 0.959099	val: 0.771435	test: 0.719581
PRC train: 0.753634	val: 0.354963	test: 0.339514

Epoch: 62
Loss: 0.1301106689091157
ROC train: 0.960081	val: 0.769927	test: 0.720071
PRC train: 0.758648	val: 0.333151	test: 0.318268

Epoch: 63
Loss: 0.12810080101922933
ROC train: 0.964780	val: 0.767183	test: 0.722778
PRC train: 0.782630	val: 0.345704	test: 0.324034

Epoch: 64
Loss: 0.1262644864602356
ROC train: 0.964004	val: 0.762201	test: 0.716763
PRC train: 0.779503	val: 0.338731	test: 0.320401

Epoch: 65
Loss: 0.1276861711827886
ROC train: 0.966015	val: 0.764779	test: 0.725283
PRC train: 0.787628	val: 0.343030	test: 0.334905

Epoch: 66
Loss: 0.12777679566192393
ROC train: 0.964570	val: 0.761394	test: 0.718085
PRC train: 0.784413	val: 0.350039	test: 0.325451

Epoch: 67
Loss: 0.12489821545916226
ROC train: 0.967159	val: 0.772311	test: 0.731077
PRC train: 0.796196	val: 0.358817	test: 0.333343

Epoch: 68
Loss: 0.12379009579197865
ROC train: 0.970314	val: 0.756547	test: 0.718081
PRC train: 0.802676	val: 0.338896	test: 0.307236

Epoch: 69
Loss: 0.12179697980027777
ROC train: 0.967226	val: 0.764380	test: 0.722674
PRC train: 0.789591	val: 0.335303	test: 0.311650

Epoch: 70
Loss: 0.11988838944460489
ROC train: 0.971042	val: 0.764699	test: 0.724880
PRC train: 0.810266	val: 0.348007	test: 0.324810

Epoch: 71
Loss: 0.12052902077156091
ROC train: 0.971384	val: 0.766264	test: 0.720548
PRC train: 0.815320	val: 0.355985	test: 0.319454

Epoch: 72
Loss: 0.11861603740135435
ROC train: 0.971265	val: 0.767480	test: 0.734293
PRC train: 0.814718	val: 0.352947	test: 0.323968

Epoch: 73
Loss: 0.12051722106523419
ROC train: 0.974223	val: 0.768620	test: 0.733589
PRC train: 0.829037	val: 0.374926	test: 0.336497

Epoch: 74
Loss: 0.11802929805202497
ROC train: 0.969976	val: 0.760043	test: 0.726086
PRC train: 0.810583	val: 0.338380	test: 0.325432

Epoch: 75
Loss: 0.11704739227693832
ROC train: 0.975164	val: 0.774048	test: 0.727467
PRC train: 0.834106	val: 0.362267	test: 0.323327

Epoch: 76
Loss: 0.11478823286772366
ROC train: 0.976975	val: 0.771912	test: 0.721200
PRC train: 0.841288	val: 0.351794	test: 0.326530

Epoch: 77
Loss: 0.11551573782959969
ROC train: 0.978526	val: 0.761865	test: 0.721643
PRC train: 0.845667	val: 0.357964	test: 0.330903

Epoch: 78
Loss: 0.11367465263036541
ROC train: 0.979307	val: 0.767683	test: 0.727757
PRC train: 0.858697	val: 0.359452	test: 0.318784

Epoch: 79
Loss: 0.11106103889635793
ROC train: 0.977563	val: 0.745844	test: 0.725809
PRC train: 0.846645	val: 0.298622	test: 0.276672

Epoch: 80
Loss: 0.11126554014786523
ROC train: 0.979884	val: 0.761242	test: 0.737351
PRC train: 0.863189	val: 0.342518	test: 0.326110

Epoch: 81
Loss: 0.11146323080211877
ROC train: 0.981169	val: 0.757371	test: 0.717606
PRC train: 0.868400	val: 0.333550	test: 0.304990

Epoch: 82
Loss: 0.10978864009457905
ROC train: 0.981845	val: 0.751870	test: 0.720231
PRC train: 0.863655	val: 0.363439	test: 0.332662

Epoch: 83
Loss: 0.11025063279274626
ROC train: 0.979971	val: 0.752822	test: 0.722637
PRC train: 0.862025	val: 0.333444	test: 0.306292

Epoch: 84
Loss: 0.10755875058746382
ROC train: 0.982119	val: 0.756016	test: 0.723197
PRC train: 0.872783	val: 0.341974	test: 0.313274

Epoch: 85
Loss: 0.10465410930385378
ROC train: 0.983244	val: 0.752181	test: 0.715971
PRC train: 0.872527	val: 0.348790	test: 0.322498

Epoch: 86
Loss: 0.10440599314524616
ROC train: 0.984815	val: 0.761063	test: 0.707875
PRC train: 0.886471	val: 0.342460	test: 0.299194

Epoch: 87
Loss: 0.10386053234799672
ROC train: 0.982715	val: 0.757619	test: 0.712162
PRC train: 0.876250	val: 0.355002	test: 0.318611

Epoch: 88
Loss: 0.10412722795325136
ROC train: 0.984831	val: 0.749734	test: 0.719206
PRC train: 0.889246	val: 0.352192	test: 0.295311

Epoch: 89
Loss: 0.10309076125429367
ROC train: 0.986217	val: 0.754012	test: 0.718560
PRC train: 0.893125	val: 0.357374	test: 0.308267

Epoch: 90
Loss: 0.10372260617953068
ROC train: 0.987616	val: 0.746537	test: 0.710991
PRC train: 0.904991	val: 0.337828	test: 0.309456

Epoch: 91
Loss: 0.10134816294075262
ROC train: 0.987063	val: 0.747448	test: 0.712372
PRC train: 0.906322	val: 0.331394	test: 0.301468

Epoch: 92
Loss: 0.0996267003181473
ROC train: 0.988327	val: 0.747123	test: 0.717027
PRC train: 0.914502	val: 0.344287	test: 0.313604

Epoch: 93
Loss: 0.10034554375563438
ROC train: 0.987861	val: 0.762879	test: 0.731021
PRC train: 0.567628	val: 0.290266	test: 0.290331

Epoch: 33
Loss: 0.16519045659350926
ROC train: 0.910477	val: 0.741110	test: 0.690828
PRC train: 0.586130	val: 0.305258	test: 0.298240

Epoch: 34
Loss: 0.1623675428565398
ROC train: 0.912654	val: 0.743132	test: 0.707643
PRC train: 0.593075	val: 0.302112	test: 0.293353

Epoch: 35
Loss: 0.16298458282093878
ROC train: 0.914517	val: 0.732277	test: 0.707560
PRC train: 0.582675	val: 0.311128	test: 0.292767

Epoch: 36
Loss: 0.16251127254627667
ROC train: 0.920674	val: 0.736164	test: 0.699378
PRC train: 0.619920	val: 0.302121	test: 0.286114

Epoch: 37
Loss: 0.1617444448454835
ROC train: 0.920029	val: 0.740936	test: 0.711599
PRC train: 0.622350	val: 0.296068	test: 0.294072

Epoch: 38
Loss: 0.15945613545320297
ROC train: 0.920384	val: 0.748273	test: 0.702954
PRC train: 0.619336	val: 0.311931	test: 0.296542

Epoch: 39
Loss: 0.15947335040152474
ROC train: 0.923839	val: 0.742120	test: 0.695969
PRC train: 0.627394	val: 0.309453	test: 0.296593

Epoch: 40
Loss: 0.1569684067629125
ROC train: 0.925727	val: 0.749850	test: 0.691861
PRC train: 0.639267	val: 0.299641	test: 0.283190

Epoch: 41
Loss: 0.15505786106933203
ROC train: 0.926867	val: 0.740978	test: 0.697494
PRC train: 0.632750	val: 0.318585	test: 0.293442

Epoch: 42
Loss: 0.1557566729520058
ROC train: 0.927751	val: 0.748935	test: 0.702499
PRC train: 0.634105	val: 0.313180	test: 0.295408

Epoch: 43
Loss: 0.15435903374355175
ROC train: 0.934235	val: 0.729772	test: 0.692001
PRC train: 0.667529	val: 0.295210	test: 0.283151

Epoch: 44
Loss: 0.1512109422458794
ROC train: 0.929942	val: 0.739144	test: 0.702464
PRC train: 0.639223	val: 0.304050	test: 0.300230

Epoch: 45
Loss: 0.15111075278819372
ROC train: 0.933992	val: 0.746878	test: 0.688889
PRC train: 0.662079	val: 0.308635	test: 0.278049

Epoch: 46
Loss: 0.1511906919155448
ROC train: 0.935918	val: 0.737423	test: 0.709134
PRC train: 0.671524	val: 0.305870	test: 0.306493

Epoch: 47
Loss: 0.14787754060715885
ROC train: 0.937595	val: 0.735345	test: 0.702899
PRC train: 0.672981	val: 0.306205	test: 0.299807

Epoch: 48
Loss: 0.1460649210519266
ROC train: 0.938268	val: 0.734463	test: 0.715239
PRC train: 0.676534	val: 0.318229	test: 0.317721

Epoch: 49
Loss: 0.14424981944520548
ROC train: 0.943715	val: 0.731629	test: 0.701035
PRC train: 0.698725	val: 0.307067	test: 0.304579

Epoch: 50
Loss: 0.14357395194510636
ROC train: 0.943356	val: 0.739229	test: 0.700983
PRC train: 0.698407	val: 0.307430	test: 0.305419

Epoch: 51
Loss: 0.14404420551137487
ROC train: 0.946570	val: 0.732180	test: 0.691473
PRC train: 0.709184	val: 0.312069	test: 0.303017

Epoch: 52
Loss: 0.141949039150569
ROC train: 0.944749	val: 0.730027	test: 0.707934
PRC train: 0.697023	val: 0.289953	test: 0.295895

Epoch: 53
Loss: 0.1403726022794871
ROC train: 0.949988	val: 0.734513	test: 0.695538
PRC train: 0.725898	val: 0.316435	test: 0.304278

Epoch: 54
Loss: 0.1404800867316458
ROC train: 0.952584	val: 0.722490	test: 0.702475
PRC train: 0.727797	val: 0.301227	test: 0.298025

Epoch: 55
Loss: 0.14147291582922938
ROC train: 0.952592	val: 0.731288	test: 0.708148
PRC train: 0.737064	val: 0.301640	test: 0.301305

Epoch: 56
Loss: 0.1363882211491854
ROC train: 0.953789	val: 0.726438	test: 0.699714
PRC train: 0.742948	val: 0.300502	test: 0.304565

Epoch: 57
Loss: 0.13648822238514444
ROC train: 0.956058	val: 0.706034	test: 0.676535
PRC train: 0.747464	val: 0.291123	test: 0.278751

Epoch: 58
Loss: 0.13406373093099552
ROC train: 0.959407	val: 0.720564	test: 0.697340
PRC train: 0.756157	val: 0.302731	test: 0.300619

Epoch: 59
Loss: 0.13808768163924137
ROC train: 0.956240	val: 0.730710	test: 0.705030
PRC train: 0.746548	val: 0.299703	test: 0.290499

Epoch: 60
Loss: 0.13434989561140276
ROC train: 0.960117	val: 0.733753	test: 0.697484
PRC train: 0.769122	val: 0.307834	test: 0.288433

Epoch: 61
Loss: 0.13134422068340662
ROC train: 0.964542	val: 0.727481	test: 0.694392
PRC train: 0.781857	val: 0.301192	test: 0.294635

Epoch: 62
Loss: 0.1287743724547142
ROC train: 0.964583	val: 0.719059	test: 0.705740
PRC train: 0.778875	val: 0.282801	test: 0.273715

Epoch: 63
Loss: 0.13040380915930305
ROC train: 0.962819	val: 0.743242	test: 0.701372
PRC train: 0.774539	val: 0.321362	test: 0.277358

Epoch: 64
Loss: 0.1274147849911537
ROC train: 0.965692	val: 0.725271	test: 0.691039
PRC train: 0.793394	val: 0.313180	test: 0.279890

Epoch: 65
Loss: 0.1272933178358869
ROC train: 0.965579	val: 0.720974	test: 0.708148
PRC train: 0.785090	val: 0.281086	test: 0.269963

Epoch: 66
Loss: 0.12410609697935357
ROC train: 0.968515	val: 0.718942	test: 0.706144
PRC train: 0.800874	val: 0.316558	test: 0.302211

Epoch: 67
Loss: 0.12522693277325567
ROC train: 0.969035	val: 0.731394	test: 0.709482
PRC train: 0.802766	val: 0.319203	test: 0.305308

Epoch: 68
Loss: 0.12465660048329928
ROC train: 0.970727	val: 0.735339	test: 0.711963
PRC train: 0.806257	val: 0.319187	test: 0.304489

Epoch: 69
Loss: 0.122153406295231
ROC train: 0.971628	val: 0.733040	test: 0.696120
PRC train: 0.812697	val: 0.320928	test: 0.297913

Epoch: 70
Loss: 0.1221137966716054
ROC train: 0.970795	val: 0.733669	test: 0.711305
PRC train: 0.814373	val: 0.304294	test: 0.279264

Epoch: 71
Loss: 0.12234292040213308
ROC train: 0.973000	val: 0.723428	test: 0.686927
PRC train: 0.824319	val: 0.320776	test: 0.289629

Epoch: 72
Loss: 0.12119756986342273
ROC train: 0.975343	val: 0.724752	test: 0.691883
PRC train: 0.833018	val: 0.307096	test: 0.271049

Epoch: 73
Loss: 0.11962715907829491
ROC train: 0.976840	val: 0.723098	test: 0.687858
PRC train: 0.843410	val: 0.310047	test: 0.287714

Epoch: 74
Loss: 0.11629787819208821
ROC train: 0.977640	val: 0.719840	test: 0.697430
PRC train: 0.851409	val: 0.307806	test: 0.277017

Epoch: 75
Loss: 0.11642466498438912
ROC train: 0.977764	val: 0.717571	test: 0.698787
PRC train: 0.846911	val: 0.305261	test: 0.274510

Epoch: 76
Loss: 0.11454146384950854
ROC train: 0.977897	val: 0.722651	test: 0.678075
PRC train: 0.851296	val: 0.308151	test: 0.258127

Epoch: 77
Loss: 0.11506397678298183
ROC train: 0.978824	val: 0.730817	test: 0.689606
PRC train: 0.856109	val: 0.317239	test: 0.290931

Epoch: 78
Loss: 0.11219950390439867
ROC train: 0.980335	val: 0.728760	test: 0.694722
PRC train: 0.870041	val: 0.310831	test: 0.298301

Epoch: 79
Loss: 0.11380016536729136
ROC train: 0.979599	val: 0.740317	test: 0.697957
PRC train: 0.857288	val: 0.322960	test: 0.289878

Epoch: 80
Loss: 0.11210805381249415
ROC train: 0.982588	val: 0.703104	test: 0.690813
PRC train: 0.877454	val: 0.296301	test: 0.283226

Epoch: 81
Loss: 0.10825455029654141
ROC train: 0.983662	val: 0.721423	test: 0.694720
PRC train: 0.883817	val: 0.293722	test: 0.272226

Epoch: 82
Loss: 0.10702252953647509
ROC train: 0.983665	val: 0.722543	test: 0.690287
PRC train: 0.883266	val: 0.298627	test: 0.272654

Epoch: 83
Loss: 0.10700312513519161
ROC train: 0.986221	val: 0.724947	test: 0.692514
PRC train: 0.898216	val: 0.300848	test: 0.266138

Epoch: 84
Loss: 0.10589210458454318
ROC train: 0.984238	val: 0.724932	test: 0.685023
PRC train: 0.886116	val: 0.301251	test: 0.273529

Epoch: 85
Loss: 0.10394688494749273
ROC train: 0.983533	val: 0.705413	test: 0.691274
PRC train: 0.879552	val: 0.288344	test: 0.281733

Epoch: 86
Loss: 0.10243782363446456
ROC train: 0.985796	val: 0.713574	test: 0.702159
PRC train: 0.891238	val: 0.301807	test: 0.296703

Epoch: 87
Loss: 0.10173846106613801
ROC train: 0.987026	val: 0.717376	test: 0.680765
PRC train: 0.904198	val: 0.303722	test: 0.283803

Epoch: 88
Loss: 0.10147964939956688
ROC train: 0.988255	val: 0.732235	test: 0.685866
PRC train: 0.911912	val: 0.313755	test: 0.284652

Epoch: 89
Loss: 0.09979273491361303
ROC train: 0.988879	val: 0.717530	test: 0.697357
PRC train: 0.915966	val: 0.294386	test: 0.276035

Epoch: 90
Loss: 0.10293660821053355
ROC train: 0.988395	val: 0.725480	test: 0.693512
PRC train: 0.911197	val: 0.308860	test: 0.285459

Epoch: 91
Loss: 0.09775554052415204
ROC train: 0.990219	val: 0.724964	test: 0.696407
PRC train: 0.922240	val: 0.303524	test: 0.276427

Epoch: 92
Loss: 0.09618463164325262
ROC train: 0.989557	val: 0.718777	test: 0.688823
PRC train: 0.916077	val: 0.321632	test: 0.294287

Epoch: 93
Loss: 0.09753374638804312
ROC train: 0.990144	val: 0.702643	test: 0.688836
PRC train: 0.575155	val: 0.223673	test: 0.239088

Epoch: 33
Loss: 0.16236979640913918
ROC train: 0.913839	val: 0.712504	test: 0.679125
PRC train: 0.591921	val: 0.260667	test: 0.255618

Epoch: 34
Loss: 0.16274458384489549
ROC train: 0.912205	val: 0.738275	test: 0.697100
PRC train: 0.586489	val: 0.279290	test: 0.284844

Epoch: 35
Loss: 0.16221426843939707
ROC train: 0.910711	val: 0.745625	test: 0.698401
PRC train: 0.568926	val: 0.327796	test: 0.305743

Epoch: 36
Loss: 0.1608932088710876
ROC train: 0.919189	val: 0.734939	test: 0.707823
PRC train: 0.613760	val: 0.309548	test: 0.306686

Epoch: 37
Loss: 0.15835077284829108
ROC train: 0.924081	val: 0.731401	test: 0.693131
PRC train: 0.635353	val: 0.288212	test: 0.293516

Epoch: 38
Loss: 0.15681593416803305
ROC train: 0.921288	val: 0.728615	test: 0.685011
PRC train: 0.613094	val: 0.287410	test: 0.268857

Epoch: 39
Loss: 0.1557423041753711
ROC train: 0.920491	val: 0.723231	test: 0.693039
PRC train: 0.616649	val: 0.258381	test: 0.271169

Epoch: 40
Loss: 0.15564953954749272
ROC train: 0.924378	val: 0.739001	test: 0.692066
PRC train: 0.633264	val: 0.289124	test: 0.275216

Epoch: 41
Loss: 0.154360260553883
ROC train: 0.925392	val: 0.686544	test: 0.670482
PRC train: 0.624963	val: 0.226347	test: 0.238187

Epoch: 42
Loss: 0.15229810091974613
ROC train: 0.922894	val: 0.732644	test: 0.703157
PRC train: 0.624112	val: 0.292425	test: 0.295285

Epoch: 43
Loss: 0.15113008216813317
ROC train: 0.934162	val: 0.727562	test: 0.702142
PRC train: 0.654911	val: 0.288247	test: 0.309475

Epoch: 44
Loss: 0.14967621992216895
ROC train: 0.933692	val: 0.715715	test: 0.686026
PRC train: 0.653585	val: 0.300403	test: 0.294468

Epoch: 45
Loss: 0.14987945642717856
ROC train: 0.934019	val: 0.723266	test: 0.696374
PRC train: 0.660100	val: 0.258556	test: 0.267827

Epoch: 46
Loss: 0.14913810760753127
ROC train: 0.938872	val: 0.682313	test: 0.666290
PRC train: 0.680320	val: 0.221638	test: 0.232395

Epoch: 47
Loss: 0.14734357370917575
ROC train: 0.931335	val: 0.675773	test: 0.664760
PRC train: 0.632512	val: 0.240036	test: 0.259481

Epoch: 48
Loss: 0.1465458853924549
ROC train: 0.942835	val: 0.707594	test: 0.670242
PRC train: 0.684357	val: 0.267278	test: 0.247386

Epoch: 49
Loss: 0.14458849820881
ROC train: 0.942959	val: 0.708951	test: 0.673106
PRC train: 0.701134	val: 0.242512	test: 0.239248

Epoch: 50
Loss: 0.1432279954074527
ROC train: 0.942843	val: 0.727000	test: 0.697427
PRC train: 0.690594	val: 0.307879	test: 0.290544

Epoch: 51
Loss: 0.14403659797276971
ROC train: 0.943704	val: 0.662833	test: 0.642382
PRC train: 0.702475	val: 0.207169	test: 0.218201

Epoch: 52
Loss: 0.1422896906637099
ROC train: 0.948599	val: 0.692514	test: 0.670326
PRC train: 0.716174	val: 0.225665	test: 0.228171

Epoch: 53
Loss: 0.1393980718295212
ROC train: 0.950633	val: 0.710470	test: 0.685335
PRC train: 0.727668	val: 0.277780	test: 0.259572

Epoch: 54
Loss: 0.1409550529731013
ROC train: 0.949901	val: 0.711464	test: 0.685825
PRC train: 0.713153	val: 0.303379	test: 0.305321

Epoch: 55
Loss: 0.137542855989417
ROC train: 0.951988	val: 0.713731	test: 0.694659
PRC train: 0.731305	val: 0.299513	test: 0.285642

Epoch: 56
Loss: 0.1359516190331223
ROC train: 0.953577	val: 0.699194	test: 0.667375
PRC train: 0.729087	val: 0.261539	test: 0.233425

Epoch: 57
Loss: 0.13531314855362533
ROC train: 0.953499	val: 0.654006	test: 0.638318
PRC train: 0.732958	val: 0.215158	test: 0.206139

Epoch: 58
Loss: 0.13601669638268832
ROC train: 0.961047	val: 0.700943	test: 0.683452
PRC train: 0.770804	val: 0.253353	test: 0.248847

Epoch: 59
Loss: 0.13469350072710046
ROC train: 0.960068	val: 0.723419	test: 0.696485
PRC train: 0.756423	val: 0.302516	test: 0.292636

Epoch: 60
Loss: 0.13046574372601866
ROC train: 0.962770	val: 0.704598	test: 0.676029
PRC train: 0.777914	val: 0.276334	test: 0.262332

Epoch: 61
Loss: 0.128881324170893
ROC train: 0.962973	val: 0.700600	test: 0.673325
PRC train: 0.771795	val: 0.261928	test: 0.249385

Epoch: 62
Loss: 0.1303568369258877
ROC train: 0.964575	val: 0.712138	test: 0.674517
PRC train: 0.787893	val: 0.269239	test: 0.244884

Epoch: 63
Loss: 0.13047212468268066
ROC train: 0.964790	val: 0.703720	test: 0.687547
PRC train: 0.785671	val: 0.304376	test: 0.293625

Epoch: 64
Loss: 0.12591374183811674
ROC train: 0.966823	val: 0.702118	test: 0.675598
PRC train: 0.796113	val: 0.243856	test: 0.235532

Epoch: 65
Loss: 0.12730727085900984
ROC train: 0.968290	val: 0.699288	test: 0.682615
PRC train: 0.805642	val: 0.283930	test: 0.275089

Epoch: 66
Loss: 0.12511297210879127
ROC train: 0.969906	val: 0.713326	test: 0.698406
PRC train: 0.815118	val: 0.268703	test: 0.261506

Epoch: 67
Loss: 0.12418948254014296
ROC train: 0.970438	val: 0.707325	test: 0.692989
PRC train: 0.814318	val: 0.291120	test: 0.289140

Epoch: 68
Loss: 0.12229840311764806
ROC train: 0.972346	val: 0.707944	test: 0.685761
PRC train: 0.823021	val: 0.269919	test: 0.273947

Epoch: 69
Loss: 0.12186949562328156
ROC train: 0.973638	val: 0.681397	test: 0.662969
PRC train: 0.828648	val: 0.245926	test: 0.225325

Epoch: 70
Loss: 0.1178327511786931
ROC train: 0.974064	val: 0.705430	test: 0.683316
PRC train: 0.830871	val: 0.266378	test: 0.239206

Epoch: 71
Loss: 0.11784676655023553
ROC train: 0.974841	val: 0.699612	test: 0.675083
PRC train: 0.834317	val: 0.278727	test: 0.258634

Epoch: 72
Loss: 0.11561652355508586
ROC train: 0.976740	val: 0.682554	test: 0.665266
PRC train: 0.843683	val: 0.240034	test: 0.225379

Epoch: 73
Loss: 0.11651964140272776
ROC train: 0.976433	val: 0.699533	test: 0.687466
PRC train: 0.835977	val: 0.289996	test: 0.282303

Epoch: 74
Loss: 0.11419644238717488
ROC train: 0.976922	val: 0.681168	test: 0.681550
PRC train: 0.846798	val: 0.247458	test: 0.245526

Epoch: 75
Loss: 0.11453586583188397
ROC train: 0.977796	val: 0.688128	test: 0.671041
PRC train: 0.850446	val: 0.231814	test: 0.225710

Epoch: 76
Loss: 0.11319819492755481
ROC train: 0.979336	val: 0.690346	test: 0.675113
PRC train: 0.854326	val: 0.277882	test: 0.261379

Epoch: 77
Loss: 0.11174479354599325
ROC train: 0.981529	val: 0.717998	test: 0.703027
PRC train: 0.873929	val: 0.297322	test: 0.268287

Epoch: 78
Loss: 0.11059857440842831
ROC train: 0.982938	val: 0.701575	test: 0.685905
PRC train: 0.876346	val: 0.285576	test: 0.255254

Epoch: 79
Loss: 0.10999581378421891
ROC train: 0.981677	val: 0.694847	test: 0.673517
PRC train: 0.870470	val: 0.236465	test: 0.222396

Epoch: 80
Loss: 0.11009846318247864
ROC train: 0.982581	val: 0.716437	test: 0.682859
PRC train: 0.878939	val: 0.266206	test: 0.241846

Epoch: 81
Loss: 0.10861723722483708
ROC train: 0.984152	val: 0.668991	test: 0.638435
PRC train: 0.884795	val: 0.222737	test: 0.202479

Epoch: 82
Loss: 0.10559161818358781
ROC train: 0.984238	val: 0.680857	test: 0.659648
PRC train: 0.882571	val: 0.239994	test: 0.219843

Epoch: 83
Loss: 0.10714963345527327
ROC train: 0.984826	val: 0.657111	test: 0.634661
PRC train: 0.888893	val: 0.198560	test: 0.186727

Epoch: 84
Loss: 0.1064675223440463
ROC train: 0.984101	val: 0.711004	test: 0.680391
PRC train: 0.888366	val: 0.267517	test: 0.238133

Epoch: 85
Loss: 0.10479879560243342
ROC train: 0.987697	val: 0.701128	test: 0.667818
PRC train: 0.908306	val: 0.265122	test: 0.231820

Epoch: 86
Loss: 0.10124546307672672
ROC train: 0.986567	val: 0.700903	test: 0.678050
PRC train: 0.897806	val: 0.269987	test: 0.246419

Epoch: 87
Loss: 0.09937592896109872
ROC train: 0.987324	val: 0.691932	test: 0.665106
PRC train: 0.905662	val: 0.255479	test: 0.232775

Epoch: 88
Loss: 0.10119044789316314
ROC train: 0.987522	val: 0.700107	test: 0.673003
PRC train: 0.907375	val: 0.291235	test: 0.250016

Epoch: 89
Loss: 0.10019894000377727
ROC train: 0.989279	val: 0.694919	test: 0.683437
PRC train: 0.917027	val: 0.266704	test: 0.228943

Epoch: 90
Loss: 0.09879762124323016
ROC train: 0.990679	val: 0.686148	test: 0.663577
PRC train: 0.925801	val: 0.239351	test: 0.220569

Epoch: 91
Loss: 0.0977801738474329
ROC train: 0.990772	val: 0.680460	test: 0.656453
PRC train: 0.925215	val: 0.252155	test: 0.217172

Epoch: 92
Loss: 0.09501173487226898
ROC train: 0.990554	val: 0.704903	test: 0.679420
PRC train: 0.923346	val: 0.281287	test: 0.237776

Epoch: 93
Loss: 0.09400241172269641
ROC train: 0.990046	val: 0.679155	test: 0.670590
PRC train: 0.592219	val: 0.353429	test: 0.352649

Epoch: 33
Loss: 0.15628128493458904
ROC train: 0.917920	val: 0.783281	test: 0.747296
PRC train: 0.615148	val: 0.364415	test: 0.359830

Epoch: 34
Loss: 0.15674684320582627
ROC train: 0.915515	val: 0.767647	test: 0.726036
PRC train: 0.604345	val: 0.341593	test: 0.336014

Epoch: 35
Loss: 0.1595286285897932
ROC train: 0.920467	val: 0.776259	test: 0.742779
PRC train: 0.626213	val: 0.347881	test: 0.328938

Epoch: 36
Loss: 0.15534036097433984
ROC train: 0.920946	val: 0.773726	test: 0.735471
PRC train: 0.612244	val: 0.334959	test: 0.336592

Epoch: 37
Loss: 0.1558447791674576
ROC train: 0.924840	val: 0.777870	test: 0.742065
PRC train: 0.631595	val: 0.343055	test: 0.347123

Epoch: 38
Loss: 0.15319569215768752
ROC train: 0.927206	val: 0.779385	test: 0.742755
PRC train: 0.647434	val: 0.347249	test: 0.349738

Epoch: 39
Loss: 0.15231784883570396
ROC train: 0.927467	val: 0.777036	test: 0.729132
PRC train: 0.647375	val: 0.356106	test: 0.332806

Epoch: 40
Loss: 0.15053057019811467
ROC train: 0.925229	val: 0.772323	test: 0.735254
PRC train: 0.628912	val: 0.336988	test: 0.326637

Epoch: 41
Loss: 0.14964618685052453
ROC train: 0.933895	val: 0.764968	test: 0.725321
PRC train: 0.673905	val: 0.353694	test: 0.343681

Epoch: 42
Loss: 0.14580962890541957
ROC train: 0.933426	val: 0.778847	test: 0.749420
PRC train: 0.668744	val: 0.346575	test: 0.351806

Epoch: 43
Loss: 0.14881160926047837
ROC train: 0.934818	val: 0.778005	test: 0.738600
PRC train: 0.674583	val: 0.363539	test: 0.350602

Epoch: 44
Loss: 0.1453240764249361
ROC train: 0.936574	val: 0.781573	test: 0.733096
PRC train: 0.675195	val: 0.357232	test: 0.346125

Epoch: 45
Loss: 0.14730865459583214
ROC train: 0.934161	val: 0.773222	test: 0.740098
PRC train: 0.667037	val: 0.356662	test: 0.353205

Epoch: 46
Loss: 0.1451219548710515
ROC train: 0.936778	val: 0.771823	test: 0.729870
PRC train: 0.671286	val: 0.342564	test: 0.338309

Epoch: 47
Loss: 0.14476059574367686
ROC train: 0.938710	val: 0.776794	test: 0.748978
PRC train: 0.685739	val: 0.341322	test: 0.335173

Epoch: 48
Loss: 0.14266502030580247
ROC train: 0.944247	val: 0.775576	test: 0.741299
PRC train: 0.710187	val: 0.364844	test: 0.352254

Epoch: 49
Loss: 0.1421426432393189
ROC train: 0.947205	val: 0.779743	test: 0.739283
PRC train: 0.712449	val: 0.353627	test: 0.354473

Epoch: 50
Loss: 0.14100989050711665
ROC train: 0.942978	val: 0.769242	test: 0.728847
PRC train: 0.702710	val: 0.340600	test: 0.345092

Epoch: 51
Loss: 0.13872199942804211
ROC train: 0.942148	val: 0.774345	test: 0.738539
PRC train: 0.687033	val: 0.344354	test: 0.340557

Epoch: 52
Loss: 0.14026159856694453
ROC train: 0.947328	val: 0.774661	test: 0.734544
PRC train: 0.718815	val: 0.367133	test: 0.351774

Epoch: 53
Loss: 0.1392060228953496
ROC train: 0.948577	val: 0.765972	test: 0.732463
PRC train: 0.719627	val: 0.331319	test: 0.313558

Epoch: 54
Loss: 0.13903416480488848
ROC train: 0.951116	val: 0.780043	test: 0.733470
PRC train: 0.727926	val: 0.354950	test: 0.340170

Epoch: 55
Loss: 0.13541350327819035
ROC train: 0.953438	val: 0.780794	test: 0.742784
PRC train: 0.742396	val: 0.368006	test: 0.353082

Epoch: 56
Loss: 0.13673134757729108
ROC train: 0.951110	val: 0.763135	test: 0.724674
PRC train: 0.720774	val: 0.356958	test: 0.339031

Epoch: 57
Loss: 0.13719983901452582
ROC train: 0.954692	val: 0.776231	test: 0.739628
PRC train: 0.743006	val: 0.363162	test: 0.347077

Epoch: 58
Loss: 0.13381308078479434
ROC train: 0.958336	val: 0.767145	test: 0.730352
PRC train: 0.756286	val: 0.352735	test: 0.345667

Epoch: 59
Loss: 0.13305625764968818
ROC train: 0.958920	val: 0.778720	test: 0.740303
PRC train: 0.767408	val: 0.358615	test: 0.349908

Epoch: 60
Loss: 0.13106248569117038
ROC train: 0.958921	val: 0.772619	test: 0.741784
PRC train: 0.761148	val: 0.343942	test: 0.332864

Epoch: 61
Loss: 0.12941088029943773
ROC train: 0.960484	val: 0.771528	test: 0.727984
PRC train: 0.772636	val: 0.354449	test: 0.344818

Epoch: 62
Loss: 0.128348380283362
ROC train: 0.958193	val: 0.774361	test: 0.722409
PRC train: 0.759869	val: 0.360274	test: 0.338549

Epoch: 63
Loss: 0.12573660263505565
ROC train: 0.961768	val: 0.777521	test: 0.730571
PRC train: 0.775808	val: 0.365177	test: 0.343829

Epoch: 64
Loss: 0.12546177616656556
ROC train: 0.962852	val: 0.756051	test: 0.731688
PRC train: 0.771421	val: 0.321941	test: 0.303175

Epoch: 65
Loss: 0.12466841909044214
ROC train: 0.964136	val: 0.771150	test: 0.741184
PRC train: 0.787653	val: 0.351835	test: 0.350197

Epoch: 66
Loss: 0.12580481822498504
ROC train: 0.966377	val: 0.772673	test: 0.731979
PRC train: 0.792036	val: 0.353106	test: 0.348102

Epoch: 67
Loss: 0.12401481239126852
ROC train: 0.967497	val: 0.772112	test: 0.733947
PRC train: 0.800356	val: 0.358460	test: 0.344295

Epoch: 68
Loss: 0.12166912673381824
ROC train: 0.959170	val: 0.753737	test: 0.725709
PRC train: 0.754391	val: 0.325238	test: 0.310714

Epoch: 69
Loss: 0.12328278794561168
ROC train: 0.971349	val: 0.772521	test: 0.730488
PRC train: 0.822762	val: 0.363546	test: 0.335120

Epoch: 70
Loss: 0.12137077227320152
ROC train: 0.969990	val: 0.772243	test: 0.737039
PRC train: 0.812656	val: 0.346423	test: 0.334109

Epoch: 71
Loss: 0.12035857862839565
ROC train: 0.971474	val: 0.766548	test: 0.728022
PRC train: 0.826967	val: 0.350217	test: 0.337883

Epoch: 72
Loss: 0.11839124561400272
ROC train: 0.973088	val: 0.772237	test: 0.740337
PRC train: 0.830956	val: 0.352416	test: 0.352275

Epoch: 73
Loss: 0.11732259416960901
ROC train: 0.973592	val: 0.768523	test: 0.730864
PRC train: 0.836099	val: 0.348682	test: 0.326168

Epoch: 74
Loss: 0.11886808343132892
ROC train: 0.971999	val: 0.764052	test: 0.723873
PRC train: 0.827306	val: 0.338081	test: 0.288385

Epoch: 75
Loss: 0.11552952078603526
ROC train: 0.974935	val: 0.761328	test: 0.721778
PRC train: 0.846852	val: 0.358891	test: 0.332586

Epoch: 76
Loss: 0.11544156167790039
ROC train: 0.974528	val: 0.762503	test: 0.732507
PRC train: 0.835985	val: 0.357492	test: 0.353308

Epoch: 77
Loss: 0.11418628237042051
ROC train: 0.976493	val: 0.771402	test: 0.726600
PRC train: 0.851848	val: 0.354371	test: 0.351028

Epoch: 78
Loss: 0.112644819371134
ROC train: 0.976950	val: 0.758790	test: 0.715785
PRC train: 0.853614	val: 0.324996	test: 0.311980

Epoch: 79
Loss: 0.11327677779755183
ROC train: 0.979141	val: 0.764696	test: 0.727291
PRC train: 0.861521	val: 0.358896	test: 0.331449

Epoch: 80
Loss: 0.11118471006811742
ROC train: 0.978397	val: 0.771267	test: 0.731483
PRC train: 0.859893	val: 0.353033	test: 0.354019

Epoch: 81
Loss: 0.11115063814399545
ROC train: 0.978559	val: 0.763465	test: 0.719435
PRC train: 0.860962	val: 0.355772	test: 0.331420

Epoch: 82
Loss: 0.11096679776628077
ROC train: 0.980316	val: 0.757000	test: 0.727239
PRC train: 0.860821	val: 0.354495	test: 0.336356

Epoch: 83
Loss: 0.10809890566867159
ROC train: 0.981707	val: 0.763432	test: 0.730321
PRC train: 0.876062	val: 0.346204	test: 0.334542

Epoch: 84
Loss: 0.10646894591756544
ROC train: 0.981279	val: 0.752391	test: 0.720958
PRC train: 0.870247	val: 0.336235	test: 0.323610

Epoch: 85
Loss: 0.10885480766223019
ROC train: 0.981679	val: 0.758517	test: 0.727296
PRC train: 0.874260	val: 0.334720	test: 0.338082

Epoch: 86
Loss: 0.10780101786745754
ROC train: 0.982550	val: 0.756067	test: 0.713958
PRC train: 0.883562	val: 0.349611	test: 0.327681

Epoch: 87
Loss: 0.10580627989918426
ROC train: 0.983768	val: 0.765574	test: 0.736804
PRC train: 0.888853	val: 0.348704	test: 0.327681

Epoch: 88
Loss: 0.10402262235232522
ROC train: 0.983921	val: 0.758954	test: 0.729207
PRC train: 0.887994	val: 0.357180	test: 0.347952

Epoch: 89
Loss: 0.10130528246499967
ROC train: 0.985684	val: 0.762635	test: 0.725864
PRC train: 0.899983	val: 0.360460	test: 0.338190

Epoch: 90
Loss: 0.10199193906345672
ROC train: 0.986267	val: 0.757023	test: 0.718696
PRC train: 0.902522	val: 0.346583	test: 0.327884

Epoch: 91
Loss: 0.10138942239658764
ROC train: 0.987885	val: 0.770387	test: 0.735499
PRC train: 0.910847	val: 0.359964	test: 0.335722

Epoch: 92
Loss: 0.10030340002047856
ROC train: 0.984895	val: 0.773118	test: 0.740723
PRC train: 0.898774	val: 0.354654	test: 0.334455

Epoch: 93
Loss: 0.09847790524316613
ROC train: 0.989395	val: 0.754067	test: 0.729935
PRC train: 0.601029	val: 0.367723	test: 0.356005

Epoch: 33
Loss: 0.15637376446778695
ROC train: 0.918725	val: 0.776185	test: 0.747143
PRC train: 0.616317	val: 0.369603	test: 0.355101

Epoch: 34
Loss: 0.15622794656609668
ROC train: 0.915532	val: 0.778661	test: 0.744905
PRC train: 0.606280	val: 0.365814	test: 0.344283

Epoch: 35
Loss: 0.15783673207582677
ROC train: 0.917821	val: 0.784855	test: 0.748955
PRC train: 0.623528	val: 0.379736	test: 0.360096

Epoch: 36
Loss: 0.15413505767016733
ROC train: 0.917924	val: 0.775512	test: 0.739949
PRC train: 0.605535	val: 0.352525	test: 0.347584

Epoch: 37
Loss: 0.1553128902659259
ROC train: 0.924266	val: 0.776444	test: 0.747139
PRC train: 0.630805	val: 0.352721	test: 0.349541

Epoch: 38
Loss: 0.15126472586292028
ROC train: 0.923537	val: 0.779112	test: 0.735572
PRC train: 0.635048	val: 0.368849	test: 0.361882

Epoch: 39
Loss: 0.15075323632355
ROC train: 0.924818	val: 0.786955	test: 0.737366
PRC train: 0.626839	val: 0.375571	test: 0.348025

Epoch: 40
Loss: 0.15015320782307667
ROC train: 0.924028	val: 0.774832	test: 0.740994
PRC train: 0.629484	val: 0.361895	test: 0.340182

Epoch: 41
Loss: 0.14845395553388266
ROC train: 0.931507	val: 0.774472	test: 0.736157
PRC train: 0.659710	val: 0.359698	test: 0.345888

Epoch: 42
Loss: 0.14508005536003288
ROC train: 0.931172	val: 0.777089	test: 0.752828
PRC train: 0.657473	val: 0.358073	test: 0.362609

Epoch: 43
Loss: 0.1478590681460652
ROC train: 0.935599	val: 0.783263	test: 0.744030
PRC train: 0.677081	val: 0.379808	test: 0.349489

Epoch: 44
Loss: 0.1459073510404199
ROC train: 0.934183	val: 0.787645	test: 0.750245
PRC train: 0.666453	val: 0.374730	test: 0.367482

Epoch: 45
Loss: 0.14590182901951462
ROC train: 0.936270	val: 0.776716	test: 0.733499
PRC train: 0.675780	val: 0.376460	test: 0.356854

Epoch: 46
Loss: 0.14425421454051918
ROC train: 0.936685	val: 0.771071	test: 0.742172
PRC train: 0.675890	val: 0.347678	test: 0.346439

Epoch: 47
Loss: 0.14487564165204103
ROC train: 0.937055	val: 0.767624	test: 0.737434
PRC train: 0.677345	val: 0.353793	test: 0.344474

Epoch: 48
Loss: 0.14173530661740125
ROC train: 0.940555	val: 0.771048	test: 0.731587
PRC train: 0.684965	val: 0.378248	test: 0.339997

Epoch: 49
Loss: 0.14230391657095184
ROC train: 0.942055	val: 0.777026	test: 0.742465
PRC train: 0.699262	val: 0.359237	test: 0.362755

Epoch: 50
Loss: 0.13966739412398763
ROC train: 0.943933	val: 0.771427	test: 0.742939
PRC train: 0.703131	val: 0.348062	test: 0.348495

Epoch: 51
Loss: 0.13992587204477688
ROC train: 0.944941	val: 0.778485	test: 0.745673
PRC train: 0.705629	val: 0.373452	test: 0.358862

Epoch: 52
Loss: 0.1389543129969592
ROC train: 0.946739	val: 0.779446	test: 0.734250
PRC train: 0.720585	val: 0.364662	test: 0.358650

Epoch: 53
Loss: 0.13873902970359656
ROC train: 0.941765	val: 0.766768	test: 0.732579
PRC train: 0.690567	val: 0.347960	test: 0.328683

Epoch: 54
Loss: 0.13896838453394464
ROC train: 0.947142	val: 0.774083	test: 0.737095
PRC train: 0.713315	val: 0.366849	test: 0.335257

Epoch: 55
Loss: 0.136793629908141
ROC train: 0.943282	val: 0.778983	test: 0.732136
PRC train: 0.681914	val: 0.346293	test: 0.333616

Epoch: 56
Loss: 0.1349290387329826
ROC train: 0.948547	val: 0.781215	test: 0.738137
PRC train: 0.712471	val: 0.356481	test: 0.350766

Epoch: 57
Loss: 0.13537360982875898
ROC train: 0.952091	val: 0.771817	test: 0.730074
PRC train: 0.740111	val: 0.365677	test: 0.352070

Epoch: 58
Loss: 0.13358030506919605
ROC train: 0.956651	val: 0.772626	test: 0.737340
PRC train: 0.756566	val: 0.375076	test: 0.354722

Epoch: 59
Loss: 0.13232188791102778
ROC train: 0.955214	val: 0.770943	test: 0.730120
PRC train: 0.753748	val: 0.375755	test: 0.350113

Epoch: 60
Loss: 0.13079528596780984
ROC train: 0.955840	val: 0.773870	test: 0.742680
PRC train: 0.756077	val: 0.362407	test: 0.354430

Epoch: 61
Loss: 0.13056121216946046
ROC train: 0.958038	val: 0.769278	test: 0.733249
PRC train: 0.767971	val: 0.370169	test: 0.347687

Epoch: 62
Loss: 0.12994808988113152
ROC train: 0.959598	val: 0.780380	test: 0.730357
PRC train: 0.771414	val: 0.383315	test: 0.359958

Epoch: 63
Loss: 0.12703270700436844
ROC train: 0.957806	val: 0.777309	test: 0.736184
PRC train: 0.762926	val: 0.376739	test: 0.355294

Epoch: 64
Loss: 0.1263594172121001
ROC train: 0.959566	val: 0.760599	test: 0.729521
PRC train: 0.763672	val: 0.354654	test: 0.339497

Epoch: 65
Loss: 0.1248127170765707
ROC train: 0.961960	val: 0.776042	test: 0.740761
PRC train: 0.784734	val: 0.359841	test: 0.352973

Epoch: 66
Loss: 0.12590903868885703
ROC train: 0.963021	val: 0.768080	test: 0.721735
PRC train: 0.785038	val: 0.362183	test: 0.353107

Epoch: 67
Loss: 0.12414937654094559
ROC train: 0.963267	val: 0.777892	test: 0.728548
PRC train: 0.783738	val: 0.368965	test: 0.346713

Epoch: 68
Loss: 0.12273539903777005
ROC train: 0.963672	val: 0.771655	test: 0.741635
PRC train: 0.777115	val: 0.354298	test: 0.335234

Epoch: 69
Loss: 0.12468195441077748
ROC train: 0.966539	val: 0.767194	test: 0.729033
PRC train: 0.797977	val: 0.366387	test: 0.340125

Epoch: 70
Loss: 0.12251045342718747
ROC train: 0.967628	val: 0.773976	test: 0.735151
PRC train: 0.803685	val: 0.364842	test: 0.349152

Epoch: 71
Loss: 0.12138118757695031
ROC train: 0.967901	val: 0.765137	test: 0.727236
PRC train: 0.805912	val: 0.366099	test: 0.336076

Epoch: 72
Loss: 0.12128921021548625
ROC train: 0.968763	val: 0.767921	test: 0.736399
PRC train: 0.807528	val: 0.374891	test: 0.353353

Epoch: 73
Loss: 0.11974631451590755
ROC train: 0.968390	val: 0.773260	test: 0.737800
PRC train: 0.804456	val: 0.368792	test: 0.359912

Epoch: 74
Loss: 0.11901198561685726
ROC train: 0.969660	val: 0.768741	test: 0.739485
PRC train: 0.813390	val: 0.351493	test: 0.326155

Epoch: 75
Loss: 0.1183911626212462
ROC train: 0.972067	val: 0.760375	test: 0.723470
PRC train: 0.829847	val: 0.365936	test: 0.351956

Epoch: 76
Loss: 0.11636194559376266
ROC train: 0.973298	val: 0.765131	test: 0.725940
PRC train: 0.831220	val: 0.357743	test: 0.352690

Epoch: 77
Loss: 0.11614079782003149
ROC train: 0.971841	val: 0.767473	test: 0.733140
PRC train: 0.820737	val: 0.373226	test: 0.359274

Epoch: 78
Loss: 0.11634778819606824
ROC train: 0.973328	val: 0.766659	test: 0.732600
PRC train: 0.831616	val: 0.361359	test: 0.345151

Epoch: 79
Loss: 0.11428602051193769
ROC train: 0.975718	val: 0.763553	test: 0.719753
PRC train: 0.841304	val: 0.350858	test: 0.335714

Epoch: 80
Loss: 0.11410281880378216
ROC train: 0.972775	val: 0.771818	test: 0.741347
PRC train: 0.828728	val: 0.363375	test: 0.356899

Epoch: 81
Loss: 0.11547274756312448
ROC train: 0.976094	val: 0.760092	test: 0.726968
PRC train: 0.846021	val: 0.359382	test: 0.342794

Epoch: 82
Loss: 0.11353213750223422
ROC train: 0.976213	val: 0.764227	test: 0.726562
PRC train: 0.846107	val: 0.356739	test: 0.351441

Epoch: 83
Loss: 0.11286615245698142
ROC train: 0.976872	val: 0.765493	test: 0.727258
PRC train: 0.848285	val: 0.358172	test: 0.350801

Epoch: 84
Loss: 0.11073020302516513
ROC train: 0.978750	val: 0.764378	test: 0.722877
PRC train: 0.860649	val: 0.362562	test: 0.341337

Epoch: 85
Loss: 0.1108328869180892
ROC train: 0.976628	val: 0.768847	test: 0.722589
PRC train: 0.848085	val: 0.355262	test: 0.341076

Epoch: 86
Loss: 0.11095897122909049
ROC train: 0.979292	val: 0.759409	test: 0.723242
PRC train: 0.864679	val: 0.375042	test: 0.334946

Epoch: 87
Loss: 0.10589072683822467
ROC train: 0.979748	val: 0.763668	test: 0.727662
PRC train: 0.869352	val: 0.362993	test: 0.346684

Epoch: 88
Loss: 0.10605853841723466
ROC train: 0.981072	val: 0.758758	test: 0.723012
PRC train: 0.877497	val: 0.365805	test: 0.340139

Epoch: 89
Loss: 0.10368072502377965
ROC train: 0.981409	val: 0.755197	test: 0.735903
PRC train: 0.874746	val: 0.361629	test: 0.350783

Epoch: 90
Loss: 0.10592067423899314
ROC train: 0.982253	val: 0.769299	test: 0.723087
PRC train: 0.881739	val: 0.367601	test: 0.358294

Epoch: 91
Loss: 0.10304227718412536
ROC train: 0.982650	val: 0.770087	test: 0.728138
PRC train: 0.883802	val: 0.354550	test: 0.355503

Epoch: 92
Loss: 0.10342345256958095
ROC train: 0.983735	val: 0.771245	test: 0.731766
PRC train: 0.888651	val: 0.360292	test: 0.357866

Epoch: 93
Loss: 0.10270488542561292
ROC train: 0.984132	val: 0.764882	test: 0.731858
PRC train: 0.597304	val: 0.376326	test: 0.352236

Epoch: 33
Loss: 0.16113991807991085
ROC train: 0.911629	val: 0.778567	test: 0.726099
PRC train: 0.601895	val: 0.365291	test: 0.350686

Epoch: 34
Loss: 0.15870615928303317
ROC train: 0.913046	val: 0.769651	test: 0.740308
PRC train: 0.605269	val: 0.352507	test: 0.342515

Epoch: 35
Loss: 0.15886075993712956
ROC train: 0.917379	val: 0.772629	test: 0.736827
PRC train: 0.618710	val: 0.350350	test: 0.346811

Epoch: 36
Loss: 0.15772467995001593
ROC train: 0.918766	val: 0.765276	test: 0.729753
PRC train: 0.621882	val: 0.347163	test: 0.333245

Epoch: 37
Loss: 0.15751833753811006
ROC train: 0.920911	val: 0.768735	test: 0.736724
PRC train: 0.629577	val: 0.351039	test: 0.344809

Epoch: 38
Loss: 0.15472650725326015
ROC train: 0.924315	val: 0.779138	test: 0.743663
PRC train: 0.639090	val: 0.369491	test: 0.357932

Epoch: 39
Loss: 0.15502982231278323
ROC train: 0.924103	val: 0.765351	test: 0.730870
PRC train: 0.637133	val: 0.358340	test: 0.342974

Epoch: 40
Loss: 0.151640915217883
ROC train: 0.925712	val: 0.771121	test: 0.735179
PRC train: 0.643554	val: 0.350488	test: 0.346884

Epoch: 41
Loss: 0.15230982568227666
ROC train: 0.928305	val: 0.770422	test: 0.723012
PRC train: 0.651247	val: 0.369014	test: 0.337531

Epoch: 42
Loss: 0.15120063613806634
ROC train: 0.929634	val: 0.776050	test: 0.729479
PRC train: 0.656074	val: 0.370335	test: 0.344219

Epoch: 43
Loss: 0.14983943119602633
ROC train: 0.933161	val: 0.761453	test: 0.738947
PRC train: 0.664144	val: 0.363849	test: 0.344072

Epoch: 44
Loss: 0.1494670514356753
ROC train: 0.927738	val: 0.779934	test: 0.737354
PRC train: 0.640184	val: 0.362063	test: 0.352585

Epoch: 45
Loss: 0.14876996031963782
ROC train: 0.936628	val: 0.779835	test: 0.721079
PRC train: 0.675173	val: 0.365850	test: 0.330710

Epoch: 46
Loss: 0.14780274792064985
ROC train: 0.938669	val: 0.767714	test: 0.728558
PRC train: 0.684171	val: 0.354936	test: 0.344870

Epoch: 47
Loss: 0.14466301625591046
ROC train: 0.940209	val: 0.783321	test: 0.731147
PRC train: 0.694671	val: 0.374570	test: 0.352132

Epoch: 48
Loss: 0.14393619130826202
ROC train: 0.940360	val: 0.777814	test: 0.740109
PRC train: 0.687989	val: 0.357014	test: 0.350579

Epoch: 49
Loss: 0.14211973382844212
ROC train: 0.941722	val: 0.775576	test: 0.738400
PRC train: 0.694630	val: 0.367216	test: 0.354499

Epoch: 50
Loss: 0.1420479678533717
ROC train: 0.943062	val: 0.773952	test: 0.726797
PRC train: 0.704989	val: 0.365229	test: 0.347590

Epoch: 51
Loss: 0.14257751965845447
ROC train: 0.946844	val: 0.775385	test: 0.736226
PRC train: 0.720262	val: 0.371317	test: 0.350277

Epoch: 52
Loss: 0.1423566995300871
ROC train: 0.943163	val: 0.763202	test: 0.718829
PRC train: 0.709175	val: 0.363731	test: 0.341906

Epoch: 53
Loss: 0.13952499871806606
ROC train: 0.948340	val: 0.784083	test: 0.729589
PRC train: 0.720206	val: 0.374312	test: 0.353921

Epoch: 54
Loss: 0.14065175427614504
ROC train: 0.949418	val: 0.770824	test: 0.727215
PRC train: 0.721903	val: 0.352404	test: 0.348156

Epoch: 55
Loss: 0.13993266253535605
ROC train: 0.949820	val: 0.764418	test: 0.731042
PRC train: 0.728025	val: 0.359937	test: 0.357602

Epoch: 56
Loss: 0.13719960404274167
ROC train: 0.953480	val: 0.762555	test: 0.730390
PRC train: 0.742343	val: 0.357898	test: 0.354653

Epoch: 57
Loss: 0.13496245981270005
ROC train: 0.954867	val: 0.770423	test: 0.736525
PRC train: 0.749190	val: 0.366517	test: 0.358848

Epoch: 58
Loss: 0.13322183210069538
ROC train: 0.955488	val: 0.782986	test: 0.738973
PRC train: 0.753455	val: 0.369759	test: 0.355667

Epoch: 59
Loss: 0.1349853554174787
ROC train: 0.956759	val: 0.766097	test: 0.730255
PRC train: 0.754049	val: 0.368193	test: 0.352309

Epoch: 60
Loss: 0.1321277350448785
ROC train: 0.959919	val: 0.773786	test: 0.730378
PRC train: 0.772698	val: 0.360241	test: 0.343879

Epoch: 61
Loss: 0.13125984929062517
ROC train: 0.959636	val: 0.774604	test: 0.718534
PRC train: 0.774183	val: 0.372682	test: 0.346772

Epoch: 62
Loss: 0.12987147798477322
ROC train: 0.960296	val: 0.773099	test: 0.737910
PRC train: 0.770604	val: 0.351588	test: 0.351182

Epoch: 63
Loss: 0.12995247869233215
ROC train: 0.961343	val: 0.762253	test: 0.726107
PRC train: 0.773742	val: 0.351014	test: 0.340162

Epoch: 64
Loss: 0.1280232697167555
ROC train: 0.965178	val: 0.771617	test: 0.726728
PRC train: 0.789274	val: 0.369053	test: 0.352727

Epoch: 65
Loss: 0.12704804483274873
ROC train: 0.962878	val: 0.760380	test: 0.722054
PRC train: 0.782326	val: 0.333679	test: 0.323741

Epoch: 66
Loss: 0.1255088704659048
ROC train: 0.965106	val: 0.765305	test: 0.725369
PRC train: 0.787707	val: 0.360049	test: 0.344747

Epoch: 67
Loss: 0.1239799696313166
ROC train: 0.964713	val: 0.758067	test: 0.731474
PRC train: 0.790316	val: 0.350167	test: 0.339899

Epoch: 68
Loss: 0.12732145760869495
ROC train: 0.966124	val: 0.768908	test: 0.728100
PRC train: 0.795433	val: 0.370796	test: 0.355047

Epoch: 69
Loss: 0.1256126717251752
ROC train: 0.967163	val: 0.776716	test: 0.727190
PRC train: 0.805818	val: 0.371639	test: 0.359264

Epoch: 70
Loss: 0.12510458745014624
ROC train: 0.969564	val: 0.767541	test: 0.734576
PRC train: 0.810574	val: 0.346082	test: 0.330451

Epoch: 71
Loss: 0.12318380294321467
ROC train: 0.971270	val: 0.767152	test: 0.726763
PRC train: 0.824401	val: 0.361155	test: 0.336850

Epoch: 72
Loss: 0.12239493850412884
ROC train: 0.969290	val: 0.776385	test: 0.729630
PRC train: 0.807797	val: 0.346723	test: 0.349371

Epoch: 73
Loss: 0.121510242782269
ROC train: 0.969805	val: 0.767095	test: 0.717762
PRC train: 0.809243	val: 0.350294	test: 0.346907

Epoch: 74
Loss: 0.12047697258905998
ROC train: 0.973140	val: 0.754835	test: 0.712082
PRC train: 0.831565	val: 0.342527	test: 0.331330

Epoch: 75
Loss: 0.11851558986196561
ROC train: 0.972530	val: 0.772237	test: 0.724349
PRC train: 0.824393	val: 0.364027	test: 0.347716

Epoch: 76
Loss: 0.11824215205353983
ROC train: 0.972570	val: 0.766772	test: 0.731704
PRC train: 0.825729	val: 0.342703	test: 0.333276

Epoch: 77
Loss: 0.1164923610330237
ROC train: 0.976482	val: 0.768246	test: 0.714548
PRC train: 0.851880	val: 0.354440	test: 0.346404

Epoch: 78
Loss: 0.11709127217051186
ROC train: 0.976357	val: 0.769268	test: 0.725697
PRC train: 0.845305	val: 0.363492	test: 0.347652

Epoch: 79
Loss: 0.11581496569053126
ROC train: 0.975777	val: 0.765179	test: 0.711250
PRC train: 0.840999	val: 0.350993	test: 0.342911

Epoch: 80
Loss: 0.11591777217259903
ROC train: 0.978456	val: 0.771113	test: 0.731029
PRC train: 0.857970	val: 0.361229	test: 0.361126

Epoch: 81
Loss: 0.11246893790657791
ROC train: 0.977311	val: 0.763974	test: 0.722497
PRC train: 0.848693	val: 0.355988	test: 0.348792

Epoch: 82
Loss: 0.11091948995215739
ROC train: 0.978829	val: 0.775188	test: 0.721508
PRC train: 0.861719	val: 0.361350	test: 0.332278

Epoch: 83
Loss: 0.11006404505652313
ROC train: 0.980380	val: 0.760916	test: 0.713824
PRC train: 0.872977	val: 0.343549	test: 0.324517

Epoch: 84
Loss: 0.11100012009955179
ROC train: 0.979659	val: 0.763031	test: 0.727089
PRC train: 0.864472	val: 0.347871	test: 0.351819

Epoch: 85
Loss: 0.10831049328210468
ROC train: 0.979216	val: 0.767162	test: 0.702215
PRC train: 0.857482	val: 0.355682	test: 0.332980

Epoch: 86
Loss: 0.10749868735315954
ROC train: 0.981511	val: 0.764873	test: 0.721270
PRC train: 0.872923	val: 0.342733	test: 0.339121

Epoch: 87
Loss: 0.10697008349155389
ROC train: 0.983104	val: 0.765755	test: 0.732406
PRC train: 0.882299	val: 0.345609	test: 0.360651

Epoch: 88
Loss: 0.10633082760744134
ROC train: 0.982764	val: 0.768392	test: 0.721900
PRC train: 0.881491	val: 0.351884	test: 0.355624

Epoch: 89
Loss: 0.10807445898123028
ROC train: 0.984186	val: 0.771237	test: 0.719279
PRC train: 0.889653	val: 0.358916	test: 0.352227

Epoch: 90
Loss: 0.10718442496348933
ROC train: 0.983338	val: 0.778028	test: 0.718844
PRC train: 0.883497	val: 0.358463	test: 0.339163

Epoch: 91
Loss: 0.10430413006885429
ROC train: 0.983915	val: 0.770705	test: 0.722221
PRC train: 0.889477	val: 0.360766	test: 0.331540

Epoch: 92
Loss: 0.10387655321206064
ROC train: 0.986516	val: 0.765721	test: 0.714866
PRC train: 0.905161	val: 0.360451	test: 0.326752

Epoch: 93
Loss: 0.1020217974163813
ROC train: 0.985796	val: 0.768375	test: 0.723686
PRC train: 0.603967	val: 0.329055	test: 0.341607

Epoch: 33
Loss: 0.1571626850572522
ROC train: 0.914593	val: 0.773846	test: 0.730487
PRC train: 0.610556	val: 0.338088	test: 0.344492

Epoch: 34
Loss: 0.15632146617893872
ROC train: 0.913307	val: 0.794310	test: 0.733436
PRC train: 0.607300	val: 0.346126	test: 0.347334

Epoch: 35
Loss: 0.15472618794506057
ROC train: 0.917688	val: 0.782667	test: 0.734977
PRC train: 0.613959	val: 0.357773	test: 0.354548

Epoch: 36
Loss: 0.15649822070674996
ROC train: 0.915844	val: 0.785414	test: 0.732707
PRC train: 0.615122	val: 0.347194	test: 0.357843

Epoch: 37
Loss: 0.15313655669173204
ROC train: 0.922268	val: 0.791399	test: 0.738200
PRC train: 0.635203	val: 0.349953	test: 0.353702

Epoch: 38
Loss: 0.1534342531946831
ROC train: 0.917289	val: 0.783616	test: 0.731765
PRC train: 0.612513	val: 0.366354	test: 0.359572

Epoch: 39
Loss: 0.15292797175911957
ROC train: 0.924876	val: 0.780557	test: 0.731781
PRC train: 0.640680	val: 0.338638	test: 0.345059

Epoch: 40
Loss: 0.15076417413045468
ROC train: 0.925471	val: 0.780117	test: 0.733162
PRC train: 0.644659	val: 0.346005	test: 0.358450

Epoch: 41
Loss: 0.14960329667309039
ROC train: 0.924108	val: 0.772494	test: 0.730087
PRC train: 0.629285	val: 0.314825	test: 0.311084

Epoch: 42
Loss: 0.1479584881141583
ROC train: 0.926419	val: 0.791985	test: 0.736907
PRC train: 0.651315	val: 0.372749	test: 0.360060

Epoch: 43
Loss: 0.14852686534711831
ROC train: 0.929756	val: 0.778549	test: 0.743929
PRC train: 0.662151	val: 0.361058	test: 0.364469

Epoch: 44
Loss: 0.14525974352478063
ROC train: 0.932661	val: 0.781466	test: 0.728259
PRC train: 0.658152	val: 0.374305	test: 0.355574

Epoch: 45
Loss: 0.1469070555424919
ROC train: 0.934232	val: 0.796564	test: 0.733440
PRC train: 0.673846	val: 0.368039	test: 0.350785

Epoch: 46
Loss: 0.1459556256074729
ROC train: 0.936740	val: 0.776293	test: 0.735836
PRC train: 0.684109	val: 0.340025	test: 0.349866

Epoch: 47
Loss: 0.144801997379758
ROC train: 0.936819	val: 0.773410	test: 0.734227
PRC train: 0.677952	val: 0.343377	test: 0.336644

Epoch: 48
Loss: 0.14571434060049962
ROC train: 0.938428	val: 0.788531	test: 0.743330
PRC train: 0.692153	val: 0.366524	test: 0.362727

Epoch: 49
Loss: 0.14371243048935853
ROC train: 0.938079	val: 0.793434	test: 0.747683
PRC train: 0.694656	val: 0.380215	test: 0.374074

Epoch: 50
Loss: 0.14027784885234904
ROC train: 0.941300	val: 0.784090	test: 0.737752
PRC train: 0.700352	val: 0.373962	test: 0.372372

Epoch: 51
Loss: 0.1396292612571564
ROC train: 0.936651	val: 0.772412	test: 0.740216
PRC train: 0.672377	val: 0.323913	test: 0.321817

Epoch: 52
Loss: 0.14011770315480004
ROC train: 0.944629	val: 0.780793	test: 0.735252
PRC train: 0.706557	val: 0.360352	test: 0.346466

Epoch: 53
Loss: 0.1396847690356732
ROC train: 0.945191	val: 0.781775	test: 0.747305
PRC train: 0.715875	val: 0.363860	test: 0.360034

Epoch: 54
Loss: 0.13720516401271246
ROC train: 0.944143	val: 0.779905	test: 0.736888
PRC train: 0.706510	val: 0.358681	test: 0.362501

Epoch: 55
Loss: 0.1371126983576753
ROC train: 0.946340	val: 0.778299	test: 0.739587
PRC train: 0.723772	val: 0.369096	test: 0.363636

Epoch: 56
Loss: 0.13605330231801563
ROC train: 0.944558	val: 0.764751	test: 0.729989
PRC train: 0.708284	val: 0.341965	test: 0.343854

Epoch: 57
Loss: 0.13620583021294266
ROC train: 0.951345	val: 0.773091	test: 0.726866
PRC train: 0.739638	val: 0.347353	test: 0.340813

Epoch: 58
Loss: 0.13393588834380796
ROC train: 0.949628	val: 0.777336	test: 0.733750
PRC train: 0.730444	val: 0.359994	test: 0.350512

Epoch: 59
Loss: 0.13526305829444227
ROC train: 0.949383	val: 0.773461	test: 0.731985
PRC train: 0.717133	val: 0.352135	test: 0.351711

Epoch: 60
Loss: 0.1329403888324353
ROC train: 0.952472	val: 0.783023	test: 0.730061
PRC train: 0.736163	val: 0.384034	test: 0.363757

Epoch: 61
Loss: 0.13234206312179272
ROC train: 0.954524	val: 0.781309	test: 0.742568
PRC train: 0.747859	val: 0.367342	test: 0.367679

Epoch: 62
Loss: 0.13197745044594328
ROC train: 0.950083	val: 0.784568	test: 0.735122
PRC train: 0.728578	val: 0.385280	test: 0.383386

Epoch: 63
Loss: 0.13278147450046598
ROC train: 0.956234	val: 0.775666	test: 0.737587
PRC train: 0.756393	val: 0.365379	test: 0.358199

Epoch: 64
Loss: 0.12793209183513382
ROC train: 0.957542	val: 0.772104	test: 0.740757
PRC train: 0.764577	val: 0.349887	test: 0.365259

Epoch: 65
Loss: 0.12657209057423485
ROC train: 0.957523	val: 0.776011	test: 0.735896
PRC train: 0.757714	val: 0.359551	test: 0.361749

Epoch: 66
Loss: 0.12834302501108197
ROC train: 0.960695	val: 0.777479	test: 0.738883
PRC train: 0.778645	val: 0.370493	test: 0.365869

Epoch: 67
Loss: 0.12801855071385462
ROC train: 0.958432	val: 0.786032	test: 0.748869
PRC train: 0.763668	val: 0.389537	test: 0.373649

Epoch: 68
Loss: 0.12645974895190581
ROC train: 0.958778	val: 0.774275	test: 0.735605
PRC train: 0.763861	val: 0.355830	test: 0.355297

Epoch: 69
Loss: 0.12355304452461156
ROC train: 0.961200	val: 0.766806	test: 0.733464
PRC train: 0.777645	val: 0.362766	test: 0.358419

Epoch: 70
Loss: 0.12298146177681024
ROC train: 0.965220	val: 0.772882	test: 0.730814
PRC train: 0.789696	val: 0.378040	test: 0.354337

Epoch: 71
Loss: 0.12266593420392569
ROC train: 0.964312	val: 0.772986	test: 0.732584
PRC train: 0.796273	val: 0.358526	test: 0.349863

Epoch: 72
Loss: 0.12000619408590374
ROC train: 0.963977	val: 0.775518	test: 0.735569
PRC train: 0.779684	val: 0.376699	test: 0.356131

Epoch: 73
Loss: 0.12344533029259948
ROC train: 0.966670	val: 0.782475	test: 0.737309
PRC train: 0.797638	val: 0.366559	test: 0.355916

Epoch: 74
Loss: 0.12353663042528933
ROC train: 0.967550	val: 0.771629	test: 0.726945
PRC train: 0.810406	val: 0.370845	test: 0.351134

Epoch: 75
Loss: 0.121213110249279
ROC train: 0.965977	val: 0.770248	test: 0.731108
PRC train: 0.799205	val: 0.362790	test: 0.363234

Epoch: 76
Loss: 0.1178448876270371
ROC train: 0.968102	val: 0.769235	test: 0.735627
PRC train: 0.810622	val: 0.343270	test: 0.341979

Epoch: 77
Loss: 0.11764974047863336
ROC train: 0.971641	val: 0.769955	test: 0.725326
PRC train: 0.827974	val: 0.371833	test: 0.354996

Epoch: 78
Loss: 0.11663004692801637
ROC train: 0.970585	val: 0.778177	test: 0.725868
PRC train: 0.822293	val: 0.369297	test: 0.361017

Epoch: 79
Loss: 0.11433232179336685
ROC train: 0.972773	val: 0.773042	test: 0.738407
PRC train: 0.833906	val: 0.351703	test: 0.355579

Epoch: 80
Loss: 0.11515256244381998
ROC train: 0.973991	val: 0.770141	test: 0.740321
PRC train: 0.840384	val: 0.348919	test: 0.353090

Epoch: 81
Loss: 0.11454018984775084
ROC train: 0.973802	val: 0.766499	test: 0.721118
PRC train: 0.828385	val: 0.365621	test: 0.355839

Epoch: 82
Loss: 0.11523609483963412
ROC train: 0.972206	val: 0.776580	test: 0.719476
PRC train: 0.828799	val: 0.369295	test: 0.353327

Epoch: 83
Loss: 0.11449197244594743
ROC train: 0.975271	val: 0.757813	test: 0.725371
PRC train: 0.846299	val: 0.327900	test: 0.337222

Epoch: 84
Loss: 0.11290326126097529
ROC train: 0.974664	val: 0.775660	test: 0.740991
PRC train: 0.842555	val: 0.377859	test: 0.381009

Epoch: 85
Loss: 0.11287066026680374
ROC train: 0.979338	val: 0.764119	test: 0.722307
PRC train: 0.867724	val: 0.370592	test: 0.371845

Epoch: 86
Loss: 0.11013743198651932
ROC train: 0.978024	val: 0.765653	test: 0.727799
PRC train: 0.858501	val: 0.351184	test: 0.344815

Epoch: 87
Loss: 0.10951600083443365
ROC train: 0.978397	val: 0.764848	test: 0.727276
PRC train: 0.859377	val: 0.375755	test: 0.362079

Epoch: 88
Loss: 0.1107651121040511
ROC train: 0.978863	val: 0.764016	test: 0.729940
PRC train: 0.865095	val: 0.358632	test: 0.342159

Epoch: 89
Loss: 0.10866804721697268
ROC train: 0.980798	val: 0.751716	test: 0.721326
PRC train: 0.872217	val: 0.338849	test: 0.342761

Epoch: 90
Loss: 0.10765085670508133
ROC train: 0.982022	val: 0.766755	test: 0.718811
PRC train: 0.880707	val: 0.358896	test: 0.341968

Epoch: 91
Loss: 0.10606678812977434
ROC train: 0.982552	val: 0.765520	test: 0.719279
PRC train: 0.886383	val: 0.351804	test: 0.356945

Epoch: 92
Loss: 0.10510125828151896
ROC train: 0.983231	val: 0.762365	test: 0.732114
PRC train: 0.890676	val: 0.353105	test: 0.361818

Epoch: 93
Loss: 0.10461657992698654
ROC train: 0.982556	val: 0.758577	test: 0.728401
PRC train: 0.593692	val: 0.365092	test: 0.348985

Epoch: 33
Loss: 0.15828587565467583
ROC train: 0.916673	val: 0.786346	test: 0.739274
PRC train: 0.608728	val: 0.355112	test: 0.340786

Epoch: 34
Loss: 0.15676421135756
ROC train: 0.915785	val: 0.775939	test: 0.734134
PRC train: 0.609963	val: 0.355169	test: 0.343629

Epoch: 35
Loss: 0.15490679797992352
ROC train: 0.917414	val: 0.769831	test: 0.738788
PRC train: 0.613995	val: 0.350861	test: 0.338081

Epoch: 36
Loss: 0.1565710150367995
ROC train: 0.921697	val: 0.775304	test: 0.713592
PRC train: 0.622838	val: 0.350547	test: 0.340349

Epoch: 37
Loss: 0.15383517907643546
ROC train: 0.922852	val: 0.770086	test: 0.735596
PRC train: 0.635063	val: 0.342707	test: 0.332607

Epoch: 38
Loss: 0.15206343858178972
ROC train: 0.923421	val: 0.785231	test: 0.727261
PRC train: 0.627668	val: 0.356887	test: 0.335859

Epoch: 39
Loss: 0.15325144632886267
ROC train: 0.924338	val: 0.771372	test: 0.724730
PRC train: 0.623545	val: 0.354435	test: 0.341312

Epoch: 40
Loss: 0.1507533152940618
ROC train: 0.928397	val: 0.773087	test: 0.730075
PRC train: 0.646285	val: 0.350288	test: 0.324990

Epoch: 41
Loss: 0.14911043291587825
ROC train: 0.930956	val: 0.783650	test: 0.722690
PRC train: 0.656723	val: 0.357799	test: 0.347399

Epoch: 42
Loss: 0.14888109190062157
ROC train: 0.930255	val: 0.766208	test: 0.732197
PRC train: 0.659733	val: 0.349604	test: 0.335224

Epoch: 43
Loss: 0.14739793337588786
ROC train: 0.933645	val: 0.762320	test: 0.730680
PRC train: 0.673547	val: 0.364377	test: 0.350924

Epoch: 44
Loss: 0.146354336244657
ROC train: 0.929083	val: 0.778655	test: 0.731629
PRC train: 0.653952	val: 0.370091	test: 0.361083

Epoch: 45
Loss: 0.146735698659549
ROC train: 0.930029	val: 0.771312	test: 0.729501
PRC train: 0.643546	val: 0.353284	test: 0.343693

Epoch: 46
Loss: 0.14735745387924978
ROC train: 0.936086	val: 0.773643	test: 0.724774
PRC train: 0.677561	val: 0.358227	test: 0.348779

Epoch: 47
Loss: 0.1438335635920701
ROC train: 0.939526	val: 0.777745	test: 0.736348
PRC train: 0.696059	val: 0.355882	test: 0.355001

Epoch: 48
Loss: 0.14137189610166143
ROC train: 0.938926	val: 0.763318	test: 0.726935
PRC train: 0.693449	val: 0.363724	test: 0.350719

Epoch: 49
Loss: 0.14093319027430376
ROC train: 0.940300	val: 0.774574	test: 0.739407
PRC train: 0.695847	val: 0.372030	test: 0.360885

Epoch: 50
Loss: 0.1399929640038017
ROC train: 0.943926	val: 0.774451	test: 0.723268
PRC train: 0.706039	val: 0.365275	test: 0.346744

Epoch: 51
Loss: 0.14013100383385402
ROC train: 0.944190	val: 0.766292	test: 0.728748
PRC train: 0.712445	val: 0.356995	test: 0.351787

Epoch: 52
Loss: 0.14021639497739133
ROC train: 0.946560	val: 0.771971	test: 0.740547
PRC train: 0.718575	val: 0.374420	test: 0.354836

Epoch: 53
Loss: 0.13905970947761362
ROC train: 0.948814	val: 0.768956	test: 0.727381
PRC train: 0.727967	val: 0.360905	test: 0.346569

Epoch: 54
Loss: 0.1377428735565229
ROC train: 0.949648	val: 0.767312	test: 0.719879
PRC train: 0.735689	val: 0.365726	test: 0.352552

Epoch: 55
Loss: 0.1375763673503939
ROC train: 0.947598	val: 0.760789	test: 0.728768
PRC train: 0.720261	val: 0.349214	test: 0.353427

Epoch: 56
Loss: 0.1355842550951679
ROC train: 0.950106	val: 0.768710	test: 0.713343
PRC train: 0.733125	val: 0.364467	test: 0.342605

Epoch: 57
Loss: 0.13426397134429321
ROC train: 0.941780	val: 0.753634	test: 0.726381
PRC train: 0.688887	val: 0.338519	test: 0.327808

Epoch: 58
Loss: 0.1333837241990068
ROC train: 0.953720	val: 0.772493	test: 0.715682
PRC train: 0.745878	val: 0.364244	test: 0.353886

Epoch: 59
Loss: 0.13529711085146195
ROC train: 0.954359	val: 0.768538	test: 0.731304
PRC train: 0.746549	val: 0.354484	test: 0.359197

Epoch: 60
Loss: 0.13280083533181256
ROC train: 0.956124	val: 0.762822	test: 0.721274
PRC train: 0.750751	val: 0.355468	test: 0.345782

Epoch: 61
Loss: 0.12828044086294052
ROC train: 0.958629	val: 0.765848	test: 0.713631
PRC train: 0.766920	val: 0.360931	test: 0.348013

Epoch: 62
Loss: 0.130922002569799
ROC train: 0.956700	val: 0.765189	test: 0.722885
PRC train: 0.759663	val: 0.342622	test: 0.345178

Epoch: 63
Loss: 0.12921867463259823
ROC train: 0.959680	val: 0.765800	test: 0.729668
PRC train: 0.769073	val: 0.355589	test: 0.355595

Epoch: 64
Loss: 0.12809080537306344
ROC train: 0.961925	val: 0.764846	test: 0.717823
PRC train: 0.781069	val: 0.356635	test: 0.348474

Epoch: 65
Loss: 0.12783625617326685
ROC train: 0.962419	val: 0.766992	test: 0.722804
PRC train: 0.778877	val: 0.359850	test: 0.351185

Epoch: 66
Loss: 0.12729436428755955
ROC train: 0.963645	val: 0.759420	test: 0.717860
PRC train: 0.787801	val: 0.354989	test: 0.345196

Epoch: 67
Loss: 0.12593613566037498
ROC train: 0.965809	val: 0.761431	test: 0.733140
PRC train: 0.795820	val: 0.342580	test: 0.354019

Epoch: 68
Loss: 0.124626613707866
ROC train: 0.963547	val: 0.772766	test: 0.711627
PRC train: 0.785467	val: 0.353744	test: 0.332027

Epoch: 69
Loss: 0.12398990344849654
ROC train: 0.965995	val: 0.766566	test: 0.714760
PRC train: 0.800248	val: 0.369366	test: 0.353340

Epoch: 70
Loss: 0.12360300631725493
ROC train: 0.966900	val: 0.766476	test: 0.720111
PRC train: 0.797186	val: 0.346998	test: 0.342120

Epoch: 71
Loss: 0.12259706342516155
ROC train: 0.968960	val: 0.763611	test: 0.724551
PRC train: 0.819210	val: 0.339438	test: 0.350924

Epoch: 72
Loss: 0.12104331205921309
ROC train: 0.969829	val: 0.762096	test: 0.720692
PRC train: 0.817768	val: 0.353533	test: 0.348861

Epoch: 73
Loss: 0.12065859912238996
ROC train: 0.966767	val: 0.762931	test: 0.730102
PRC train: 0.800104	val: 0.344302	test: 0.346909

Epoch: 74
Loss: 0.11879726616718617
ROC train: 0.971050	val: 0.761065	test: 0.729845
PRC train: 0.824007	val: 0.338602	test: 0.344910

Epoch: 75
Loss: 0.12050261338716671
ROC train: 0.968985	val: 0.755049	test: 0.709231
PRC train: 0.812424	val: 0.334888	test: 0.320117

Epoch: 76
Loss: 0.11750217124351071
ROC train: 0.972622	val: 0.767415	test: 0.724491
PRC train: 0.829815	val: 0.349687	test: 0.331037

Epoch: 77
Loss: 0.11733638939740716
ROC train: 0.973274	val: 0.756893	test: 0.706504
PRC train: 0.822906	val: 0.344461	test: 0.345283

Epoch: 78
Loss: 0.11809851975207648
ROC train: 0.974706	val: 0.755817	test: 0.723016
PRC train: 0.842447	val: 0.347243	test: 0.335334

Epoch: 79
Loss: 0.11741410921275676
ROC train: 0.974605	val: 0.763577	test: 0.720156
PRC train: 0.837238	val: 0.359840	test: 0.351081

Epoch: 80
Loss: 0.11448736894197982
ROC train: 0.976463	val: 0.767853	test: 0.724216
PRC train: 0.848456	val: 0.346509	test: 0.348894

Epoch: 81
Loss: 0.11401296392703707
ROC train: 0.974403	val: 0.764248	test: 0.725268
PRC train: 0.842138	val: 0.344808	test: 0.348981

Epoch: 82
Loss: 0.11100766518880578
ROC train: 0.978027	val: 0.767046	test: 0.724773
PRC train: 0.860739	val: 0.358773	test: 0.341916

Epoch: 83
Loss: 0.11039360459542333
ROC train: 0.978742	val: 0.761663	test: 0.720855
PRC train: 0.863092	val: 0.352964	test: 0.338601

Epoch: 84
Loss: 0.10984042165537967
ROC train: 0.979824	val: 0.759356	test: 0.724443
PRC train: 0.864635	val: 0.368896	test: 0.344484

Epoch: 85
Loss: 0.10869363694240991
ROC train: 0.979483	val: 0.754254	test: 0.708285
PRC train: 0.867510	val: 0.354312	test: 0.338614

Epoch: 86
Loss: 0.10813113623017877
ROC train: 0.979824	val: 0.768374	test: 0.718867
PRC train: 0.866386	val: 0.358324	test: 0.344763

Epoch: 87
Loss: 0.1091338159604257
ROC train: 0.981485	val: 0.761160	test: 0.709990
PRC train: 0.875575	val: 0.337946	test: 0.340248

Epoch: 88
Loss: 0.10788338509013111
ROC train: 0.981123	val: 0.771014	test: 0.718406
PRC train: 0.871021	val: 0.340361	test: 0.346241

Epoch: 89
Loss: 0.10642229629204676
ROC train: 0.980614	val: 0.762247	test: 0.719566
PRC train: 0.872095	val: 0.356156	test: 0.352787

Epoch: 90
Loss: 0.10605292011840411
ROC train: 0.981961	val: 0.771684	test: 0.728108
PRC train: 0.876711	val: 0.342785	test: 0.340302

Epoch: 91
Loss: 0.10575636361646906
ROC train: 0.978666	val: 0.759207	test: 0.721435
PRC train: 0.862860	val: 0.336933	test: 0.343238

Epoch: 92
Loss: 0.10684242736086569
ROC train: 0.977756	val: 0.749753	test: 0.714618
PRC train: 0.857555	val: 0.348718	test: 0.349540

Epoch: 93
Loss: 0.10502507764433107
ROC train: 0.982079	val: 0.760822	test: 0.727846
ROC train: 0.972316	val: 0.774743	test: 0.746215
PRC train: 0.828336	val: 0.358719	test: 0.359275

Epoch: 95
Loss: 0.11400225292727661
ROC train: 0.973903	val: 0.766655	test: 0.735087
PRC train: 0.836949	val: 0.370575	test: 0.360645

Epoch: 96
Loss: 0.11394414896880463
ROC train: 0.973161	val: 0.775533	test: 0.735033
PRC train: 0.833032	val: 0.373598	test: 0.352461

Epoch: 97
Loss: 0.11221449945084855
ROC train: 0.971379	val: 0.770670	test: 0.742467
PRC train: 0.827681	val: 0.363811	test: 0.367931

Epoch: 98
Loss: 0.11226398791857405
ROC train: 0.972220	val: 0.779364	test: 0.729313
PRC train: 0.827888	val: 0.375493	test: 0.349151

Epoch: 99
Loss: 0.11243497388030674
ROC train: 0.973602	val: 0.778289	test: 0.750285
PRC train: 0.839231	val: 0.379584	test: 0.365663

Epoch: 100
Loss: 0.11061226514024533
ROC train: 0.974966	val: 0.767272	test: 0.728522
PRC train: 0.842398	val: 0.370300	test: 0.349950

Epoch: 101
Loss: 0.10924224383758718
ROC train: 0.975885	val: 0.762998	test: 0.732351
PRC train: 0.846992	val: 0.359986	test: 0.342163

Epoch: 102
Loss: 0.11014276564321662
ROC train: 0.975902	val: 0.778302	test: 0.744344
PRC train: 0.849018	val: 0.367308	test: 0.367757

Epoch: 103
Loss: 0.10930915676058685
ROC train: 0.975905	val: 0.767087	test: 0.740054
PRC train: 0.847711	val: 0.360113	test: 0.354143

Epoch: 104
Loss: 0.11048069815758678
ROC train: 0.972540	val: 0.760046	test: 0.731986
PRC train: 0.826195	val: 0.351404	test: 0.348340

Epoch: 105
Loss: 0.10930750186835617
ROC train: 0.976468	val: 0.767817	test: 0.732809
PRC train: 0.856127	val: 0.369029	test: 0.354732

Epoch: 106
Loss: 0.10761494618924772
ROC train: 0.976662	val: 0.772550	test: 0.742903
PRC train: 0.853429	val: 0.367367	test: 0.346593

Epoch: 107
Loss: 0.10560878094176139
ROC train: 0.978096	val: 0.771721	test: 0.741002
PRC train: 0.864775	val: 0.375250	test: 0.378872

Epoch: 108
Loss: 0.10523805445889506
ROC train: 0.978516	val: 0.767642	test: 0.737280
PRC train: 0.869524	val: 0.371685	test: 0.364278

Epoch: 109
Loss: 0.10537727031168967
ROC train: 0.979084	val: 0.775516	test: 0.739427
PRC train: 0.863452	val: 0.382790	test: 0.367931

Epoch: 110
Loss: 0.10723536448037221
ROC train: 0.975715	val: 0.762438	test: 0.730292
PRC train: 0.848069	val: 0.333349	test: 0.337484

Epoch: 111
Loss: 0.10621494114606611
ROC train: 0.978947	val: 0.775241	test: 0.732359
PRC train: 0.869645	val: 0.357094	test: 0.348791

Epoch: 112
Loss: 0.10469395946120716
ROC train: 0.981163	val: 0.768931	test: 0.743763
PRC train: 0.876668	val: 0.374976	test: 0.357753

Epoch: 113
Loss: 0.103869734338985
ROC train: 0.980547	val: 0.776716	test: 0.744822
PRC train: 0.871727	val: 0.377472	test: 0.369507

Epoch: 114
Loss: 0.10424434095715043
ROC train: 0.980231	val: 0.769061	test: 0.730358
PRC train: 0.871903	val: 0.370715	test: 0.334891

Epoch: 115
Loss: 0.10302663911523881
ROC train: 0.980878	val: 0.772670	test: 0.736248
PRC train: 0.879973	val: 0.368451	test: 0.354418

Epoch: 116
Loss: 0.10226387772881035
ROC train: 0.981773	val: 0.777604	test: 0.741743
PRC train: 0.881840	val: 0.369643	test: 0.356892

Epoch: 117
Loss: 0.10213876955425928
ROC train: 0.982073	val: 0.775712	test: 0.748271
PRC train: 0.883116	val: 0.364059	test: 0.359224

Epoch: 118
Loss: 0.10200074103440293
ROC train: 0.981985	val: 0.772751	test: 0.742708
PRC train: 0.881361	val: 0.361153	test: 0.349617

Epoch: 119
Loss: 0.10144394946890108
ROC train: 0.981613	val: 0.767031	test: 0.748587
PRC train: 0.880613	val: 0.367855	test: 0.357966

Epoch: 120
Loss: 0.10182293087210757
ROC train: 0.980457	val: 0.770596	test: 0.724202
PRC train: 0.870908	val: 0.364166	test: 0.334561

Early stopping
Best (ROC):	 train: 0.961549	val: 0.783174	test: 0.743618
Best (PRC):	 train: 0.783213	val: 0.402492	test: 0.365473

ROC train: 0.971348	val: 0.767319	test: 0.745552
PRC train: 0.826862	val: 0.357028	test: 0.338718

Epoch: 95
Loss: 0.11183514653645757
ROC train: 0.971338	val: 0.771840	test: 0.745562
PRC train: 0.826159	val: 0.362628	test: 0.335212

Epoch: 96
Loss: 0.11342471616841564
ROC train: 0.973032	val: 0.779474	test: 0.744616
PRC train: 0.832425	val: 0.374179	test: 0.339676

Epoch: 97
Loss: 0.1119065120353285
ROC train: 0.973169	val: 0.774738	test: 0.741295
PRC train: 0.836243	val: 0.372865	test: 0.336094

Epoch: 98
Loss: 0.11261528231350353
ROC train: 0.969696	val: 0.775694	test: 0.746822
PRC train: 0.813412	val: 0.367181	test: 0.339920

Epoch: 99
Loss: 0.11067976784204339
ROC train: 0.972371	val: 0.757412	test: 0.746397
PRC train: 0.826048	val: 0.371265	test: 0.352388

Epoch: 100
Loss: 0.10900336037909847
ROC train: 0.974554	val: 0.759245	test: 0.741341
PRC train: 0.841197	val: 0.357690	test: 0.343657

Epoch: 101
Loss: 0.10850489412620595
ROC train: 0.975267	val: 0.771257	test: 0.751771
PRC train: 0.846215	val: 0.383384	test: 0.357429

Epoch: 102
Loss: 0.10923403934235985
ROC train: 0.975497	val: 0.775815	test: 0.745080
PRC train: 0.842089	val: 0.381411	test: 0.351499

Epoch: 103
Loss: 0.10744669091343091
ROC train: 0.975752	val: 0.758528	test: 0.736569
PRC train: 0.845004	val: 0.362212	test: 0.330811

Epoch: 104
Loss: 0.10879964827501688
ROC train: 0.977196	val: 0.764612	test: 0.747120
PRC train: 0.853965	val: 0.366662	test: 0.339739

Epoch: 105
Loss: 0.10744621706174635
ROC train: 0.976440	val: 0.746369	test: 0.729234
PRC train: 0.842579	val: 0.332553	test: 0.307959

Epoch: 106
Loss: 0.10985917564048599
ROC train: 0.977541	val: 0.766881	test: 0.738615
PRC train: 0.853305	val: 0.353422	test: 0.325147

Epoch: 107
Loss: 0.10697422063914586
ROC train: 0.975908	val: 0.764540	test: 0.741294
PRC train: 0.848646	val: 0.361162	test: 0.336252

Epoch: 108
Loss: 0.1057875092558972
ROC train: 0.978324	val: 0.761974	test: 0.735198
PRC train: 0.859429	val: 0.354933	test: 0.331790

Epoch: 109
Loss: 0.10566670193767445
ROC train: 0.979049	val: 0.767904	test: 0.745540
PRC train: 0.868058	val: 0.353587	test: 0.336436

Epoch: 110
Loss: 0.1036886724432866
ROC train: 0.978246	val: 0.762455	test: 0.734992
PRC train: 0.856385	val: 0.361737	test: 0.323047

Epoch: 111
Loss: 0.10516233703468356
ROC train: 0.978443	val: 0.754885	test: 0.736504
PRC train: 0.855526	val: 0.361587	test: 0.346561

Epoch: 112
Loss: 0.10592153941506707
ROC train: 0.978578	val: 0.762187	test: 0.741267
PRC train: 0.856322	val: 0.360846	test: 0.343418

Epoch: 113
Loss: 0.10402646613103479
ROC train: 0.979786	val: 0.766794	test: 0.734244
PRC train: 0.869124	val: 0.351318	test: 0.334352

Epoch: 114
Loss: 0.10491972111446497
ROC train: 0.979344	val: 0.757608	test: 0.738198
PRC train: 0.870629	val: 0.363157	test: 0.339057

Epoch: 115
Loss: 0.10289905862846074
ROC train: 0.980287	val: 0.750588	test: 0.748102
PRC train: 0.869070	val: 0.351922	test: 0.341553

Epoch: 116
Loss: 0.1031303098658005
ROC train: 0.979977	val: 0.758274	test: 0.744189
PRC train: 0.869311	val: 0.351452	test: 0.338255

Epoch: 117
Loss: 0.10261632084371515
ROC train: 0.982141	val: 0.762058	test: 0.737486
PRC train: 0.878721	val: 0.363348	test: 0.342379

Epoch: 118
Loss: 0.10183209999707689
ROC train: 0.981382	val: 0.761043	test: 0.740397
PRC train: 0.874514	val: 0.355617	test: 0.343942

Epoch: 119
Loss: 0.10010236208780166
ROC train: 0.982272	val: 0.759220	test: 0.738124
PRC train: 0.880625	val: 0.362730	test: 0.342457

Epoch: 120
Loss: 0.10229595168001288
ROC train: 0.982224	val: 0.761181	test: 0.747743
PRC train: 0.877742	val: 0.366192	test: 0.358844

Early stopping
Best (ROC):	 train: 0.901361	val: 0.782754	test: 0.745256
Best (PRC):	 train: 0.575117	val: 0.376851	test: 0.355643

ROC train: 0.972898	val: 0.772149	test: 0.738166
PRC train: 0.838499	val: 0.384677	test: 0.347199

Epoch: 95
Loss: 0.11419231307676403
ROC train: 0.971111	val: 0.759857	test: 0.739966
PRC train: 0.825613	val: 0.361329	test: 0.341514

Epoch: 96
Loss: 0.1128738647247294
ROC train: 0.972941	val: 0.766872	test: 0.747025
PRC train: 0.832072	val: 0.379819	test: 0.367288

Epoch: 97
Loss: 0.11223114141084385
ROC train: 0.974476	val: 0.760701	test: 0.732307
PRC train: 0.836852	val: 0.365997	test: 0.340233

Epoch: 98
Loss: 0.11147910431184169
ROC train: 0.974767	val: 0.761851	test: 0.742011
PRC train: 0.842040	val: 0.378317	test: 0.365071

Epoch: 99
Loss: 0.11175445247712304
ROC train: 0.973338	val: 0.769054	test: 0.751261
PRC train: 0.835983	val: 0.377060	test: 0.350520

Epoch: 100
Loss: 0.10981098921709226
ROC train: 0.975035	val: 0.761812	test: 0.736418
PRC train: 0.845736	val: 0.369246	test: 0.342571

Epoch: 101
Loss: 0.1098819658114293
ROC train: 0.974436	val: 0.770999	test: 0.750169
PRC train: 0.842014	val: 0.376402	test: 0.357319

Epoch: 102
Loss: 0.10745826928307459
ROC train: 0.976288	val: 0.758100	test: 0.730467
PRC train: 0.849039	val: 0.373413	test: 0.349112

Epoch: 103
Loss: 0.10726675609700256
ROC train: 0.976711	val: 0.765147	test: 0.738778
PRC train: 0.856859	val: 0.380192	test: 0.351216

Epoch: 104
Loss: 0.10934663849753498
ROC train: 0.976579	val: 0.765567	test: 0.734551
PRC train: 0.851297	val: 0.388577	test: 0.353806

Epoch: 105
Loss: 0.10957683712553039
ROC train: 0.977457	val: 0.764654	test: 0.745296
PRC train: 0.858221	val: 0.388680	test: 0.347119

Epoch: 106
Loss: 0.10841684967278752
ROC train: 0.976002	val: 0.763202	test: 0.732954
PRC train: 0.847158	val: 0.372468	test: 0.342191

Epoch: 107
Loss: 0.10750902988122045
ROC train: 0.977329	val: 0.766275	test: 0.740343
PRC train: 0.851114	val: 0.382338	test: 0.341210

Epoch: 108
Loss: 0.1075626477070299
ROC train: 0.978102	val: 0.766440	test: 0.744874
PRC train: 0.857111	val: 0.380688	test: 0.348251

Epoch: 109
Loss: 0.10490776722758995
ROC train: 0.977781	val: 0.767651	test: 0.739902
PRC train: 0.859561	val: 0.395639	test: 0.348293

Epoch: 110
Loss: 0.10373013506200589
ROC train: 0.979445	val: 0.759238	test: 0.733301
PRC train: 0.866896	val: 0.385041	test: 0.347021

Epoch: 111
Loss: 0.10502851409627015
ROC train: 0.980110	val: 0.762957	test: 0.739530
PRC train: 0.871622	val: 0.393045	test: 0.344768

Epoch: 112
Loss: 0.1045141454390745
ROC train: 0.976971	val: 0.751092	test: 0.736683
PRC train: 0.853048	val: 0.367802	test: 0.345150

Epoch: 113
Loss: 0.10529696203744432
ROC train: 0.978210	val: 0.761518	test: 0.743337
PRC train: 0.856552	val: 0.387091	test: 0.354817

Epoch: 114
Loss: 0.10428867436160286
ROC train: 0.980315	val: 0.758583	test: 0.732161
PRC train: 0.873375	val: 0.373670	test: 0.339643

Epoch: 115
Loss: 0.10215332740992952
ROC train: 0.980776	val: 0.770756	test: 0.743375
PRC train: 0.873961	val: 0.403544	test: 0.356139

Epoch: 116
Loss: 0.10109771104494697
ROC train: 0.980245	val: 0.764356	test: 0.749529
PRC train: 0.870454	val: 0.382845	test: 0.352129

Epoch: 117
Loss: 0.10063270575571209
ROC train: 0.981883	val: 0.760327	test: 0.735212
PRC train: 0.875381	val: 0.378396	test: 0.341149

Epoch: 118
Loss: 0.1010590924227267
ROC train: 0.981867	val: 0.770466	test: 0.746172
PRC train: 0.879630	val: 0.378973	test: 0.336177

Epoch: 119
Loss: 0.09987271273744071
ROC train: 0.982181	val: 0.769226	test: 0.738324
PRC train: 0.882840	val: 0.384692	test: 0.343798

Epoch: 120
Loss: 0.10045113628639203
ROC train: 0.981383	val: 0.762226	test: 0.745373
PRC train: 0.876888	val: 0.358440	test: 0.332079

Early stopping
Best (ROC):	 train: 0.909967	val: 0.794238	test: 0.736889
Best (PRC):	 train: 0.601114	val: 0.372907	test: 0.351464
All runs completed.

PRC train: 0.927823	val: 0.287015	test: 0.282901

Epoch: 94
Loss: 0.09499367973482961
ROC train: 0.991332	val: 0.721546	test: 0.695989
PRC train: 0.927147	val: 0.305053	test: 0.288024

Epoch: 95
Loss: 0.09452674978044412
ROC train: 0.992753	val: 0.711113	test: 0.689419
PRC train: 0.934764	val: 0.280207	test: 0.280415

Epoch: 96
Loss: 0.09685855692737698
ROC train: 0.991830	val: 0.713771	test: 0.685614
PRC train: 0.931498	val: 0.304981	test: 0.292075

Epoch: 97
Loss: 0.09181919040328422
ROC train: 0.993097	val: 0.716675	test: 0.701378
PRC train: 0.942535	val: 0.294663	test: 0.298519

Epoch: 98
Loss: 0.09055124207161586
ROC train: 0.993008	val: 0.714249	test: 0.700597
PRC train: 0.939958	val: 0.301360	test: 0.306301

Epoch: 99
Loss: 0.0909666249834218
ROC train: 0.993443	val: 0.720011	test: 0.695396
PRC train: 0.940529	val: 0.300993	test: 0.302432

Epoch: 100
Loss: 0.08904740243216892
ROC train: 0.994525	val: 0.688726	test: 0.671548
PRC train: 0.950318	val: 0.256390	test: 0.263814

Epoch: 101
Loss: 0.08794538660666834
ROC train: 0.993292	val: 0.725053	test: 0.701916
PRC train: 0.941567	val: 0.309140	test: 0.310781

Epoch: 102
Loss: 0.0870690812636829
ROC train: 0.994352	val: 0.714424	test: 0.693661
PRC train: 0.949797	val: 0.281946	test: 0.287144

Epoch: 103
Loss: 0.0862445100361497
ROC train: 0.994156	val: 0.708752	test: 0.695812
PRC train: 0.948082	val: 0.299281	test: 0.303927

Epoch: 104
Loss: 0.08370299610251777
ROC train: 0.995207	val: 0.698071	test: 0.676827
PRC train: 0.956443	val: 0.279068	test: 0.279252

Epoch: 105
Loss: 0.08477477375807015
ROC train: 0.994302	val: 0.702432	test: 0.687011
PRC train: 0.952931	val: 0.292745	test: 0.290146

Epoch: 106
Loss: 0.0836988339808173
ROC train: 0.995525	val: 0.706414	test: 0.687534
PRC train: 0.957221	val: 0.297069	test: 0.291354

Epoch: 107
Loss: 0.08203142363724353
ROC train: 0.995829	val: 0.703685	test: 0.683965
PRC train: 0.962159	val: 0.277207	test: 0.286140

Epoch: 108
Loss: 0.08238766505306286
ROC train: 0.996485	val: 0.713165	test: 0.678363
PRC train: 0.964347	val: 0.281958	test: 0.282381

Epoch: 109
Loss: 0.07878152374013829
ROC train: 0.996224	val: 0.714622	test: 0.699789
PRC train: 0.965515	val: 0.276760	test: 0.274491

Epoch: 110
Loss: 0.07990873456705097
ROC train: 0.996908	val: 0.703412	test: 0.685004
PRC train: 0.969706	val: 0.283937	test: 0.284010

Epoch: 111
Loss: 0.0792138132786384
ROC train: 0.996325	val: 0.710743	test: 0.692333
PRC train: 0.964594	val: 0.281279	test: 0.293646

Epoch: 112
Loss: 0.07785495922116786
ROC train: 0.997046	val: 0.711671	test: 0.694967
PRC train: 0.971338	val: 0.296594	test: 0.299482

Epoch: 113
Loss: 0.07828280622664409
ROC train: 0.997168	val: 0.701628	test: 0.681313
PRC train: 0.970894	val: 0.273462	test: 0.280307

Epoch: 114
Loss: 0.0772943324865149
ROC train: 0.997460	val: 0.695524	test: 0.683213
PRC train: 0.975367	val: 0.280099	test: 0.284417

Epoch: 115
Loss: 0.07493284051410698
ROC train: 0.997904	val: 0.718663	test: 0.689637
PRC train: 0.980251	val: 0.298717	test: 0.291124

Epoch: 116
Loss: 0.07423992083422114
ROC train: 0.996579	val: 0.680936	test: 0.657201
PRC train: 0.968004	val: 0.272935	test: 0.253048

Epoch: 117
Loss: 0.07256475812193713
ROC train: 0.997992	val: 0.711491	test: 0.688008
PRC train: 0.981170	val: 0.300887	test: 0.297321

Epoch: 118
Loss: 0.07366437466336144
ROC train: 0.998178	val: 0.702122	test: 0.674107
PRC train: 0.982143	val: 0.289363	test: 0.267745

Epoch: 119
Loss: 0.07282677343588999
ROC train: 0.998057	val: 0.690670	test: 0.669445
PRC train: 0.980808	val: 0.272194	test: 0.263891

Epoch: 120
Loss: 0.07080189720948522
ROC train: 0.998616	val: 0.695514	test: 0.677842
PRC train: 0.986372	val: 0.274545	test: 0.277199

Early stopping
Best (ROC):	 train: 0.904870	val: 0.755642	test: 0.717994
Best (PRC):	 train: 0.565156	val: 0.336451	test: 0.315245

PRC train: 0.911434	val: 0.357911	test: 0.320863

Epoch: 94
Loss: 0.09638958260032526
ROC train: 0.988015	val: 0.756591	test: 0.710170
PRC train: 0.905804	val: 0.359815	test: 0.314512

Epoch: 95
Loss: 0.09724205795833855
ROC train: 0.989040	val: 0.753101	test: 0.715582
PRC train: 0.916419	val: 0.348264	test: 0.312122

Epoch: 96
Loss: 0.09598552295395787
ROC train: 0.989249	val: 0.753615	test: 0.720303
PRC train: 0.918296	val: 0.347131	test: 0.308795

Epoch: 97
Loss: 0.09614208738117438
ROC train: 0.990817	val: 0.747854	test: 0.714972
PRC train: 0.929586	val: 0.336742	test: 0.305321

Epoch: 98
Loss: 0.0926372003717838
ROC train: 0.988792	val: 0.752289	test: 0.723523
PRC train: 0.911213	val: 0.322080	test: 0.302484

Epoch: 99
Loss: 0.09158710303529805
ROC train: 0.989638	val: 0.749153	test: 0.709831
PRC train: 0.918876	val: 0.344774	test: 0.313016

Epoch: 100
Loss: 0.09462573599201997
ROC train: 0.989983	val: 0.757394	test: 0.718046
PRC train: 0.922670	val: 0.342499	test: 0.301850

Epoch: 101
Loss: 0.09188495770534494
ROC train: 0.989585	val: 0.746166	test: 0.708684
PRC train: 0.913931	val: 0.327301	test: 0.307567

Epoch: 102
Loss: 0.08813809633677476
ROC train: 0.992198	val: 0.762232	test: 0.718016
PRC train: 0.938403	val: 0.357995	test: 0.310843

Epoch: 103
Loss: 0.08907221741262443
ROC train: 0.993221	val: 0.745914	test: 0.717679
PRC train: 0.945343	val: 0.344711	test: 0.289870

Epoch: 104
Loss: 0.08784629632183559
ROC train: 0.993034	val: 0.752401	test: 0.711559
PRC train: 0.944812	val: 0.354017	test: 0.302814

Epoch: 105
Loss: 0.08838527165713352
ROC train: 0.992638	val: 0.745251	test: 0.704165
PRC train: 0.943043	val: 0.328301	test: 0.277830

Epoch: 106
Loss: 0.08691000824619731
ROC train: 0.992771	val: 0.756350	test: 0.711602
PRC train: 0.941592	val: 0.346513	test: 0.303284

Epoch: 107
Loss: 0.08555060903303671
ROC train: 0.993367	val: 0.749553	test: 0.717419
PRC train: 0.943893	val: 0.338057	test: 0.296646

Epoch: 108
Loss: 0.08631159224090328
ROC train: 0.994139	val: 0.748144	test: 0.711432
PRC train: 0.949989	val: 0.327375	test: 0.285247

Epoch: 109
Loss: 0.08433826940776155
ROC train: 0.993169	val: 0.745854	test: 0.715783
PRC train: 0.943254	val: 0.335699	test: 0.300225

Epoch: 110
Loss: 0.0837350261019075
ROC train: 0.992994	val: 0.749407	test: 0.722545
PRC train: 0.943781	val: 0.338135	test: 0.318807

Epoch: 111
Loss: 0.08395538407519838
ROC train: 0.994325	val: 0.744191	test: 0.708602
PRC train: 0.951459	val: 0.322336	test: 0.281048

Epoch: 112
Loss: 0.08115881679179754
ROC train: 0.994511	val: 0.751652	test: 0.699503
PRC train: 0.954966	val: 0.331298	test: 0.287464

Epoch: 113
Loss: 0.08088574405250187
ROC train: 0.994042	val: 0.745050	test: 0.717923
PRC train: 0.949576	val: 0.334362	test: 0.315570

Epoch: 114
Loss: 0.07820827478252292
ROC train: 0.995297	val: 0.737447	test: 0.708683
PRC train: 0.958119	val: 0.324727	test: 0.310068

Epoch: 115
Loss: 0.07914466795400715
ROC train: 0.995910	val: 0.754042	test: 0.727294
PRC train: 0.965028	val: 0.354713	test: 0.323702

Epoch: 116
Loss: 0.07709943233615109
ROC train: 0.996477	val: 0.741390	test: 0.702262
PRC train: 0.970387	val: 0.338914	test: 0.284799

Epoch: 117
Loss: 0.0778621826618109
ROC train: 0.994601	val: 0.748687	test: 0.720535
PRC train: 0.956767	val: 0.338993	test: 0.306575

Epoch: 118
Loss: 0.07541577672024471
ROC train: 0.995700	val: 0.750350	test: 0.722360
PRC train: 0.965741	val: 0.347863	test: 0.314837

Epoch: 119
Loss: 0.0766482048287826
ROC train: 0.996732	val: 0.741976	test: 0.706675
PRC train: 0.972290	val: 0.326691	test: 0.278617

Epoch: 120
Loss: 0.07637573154567381
ROC train: 0.996762	val: 0.747438	test: 0.717727
PRC train: 0.971234	val: 0.366056	test: 0.303646

Early stopping
Best (ROC):	 train: 0.893423	val: 0.780415	test: 0.727797
Best (PRC):	 train: 0.534953	val: 0.342160	test: 0.338756

PRC train: 0.920456	val: 0.346063	test: 0.320546

Epoch: 94
Loss: 0.09840863294071113
ROC train: 0.989031	val: 0.755268	test: 0.722314
PRC train: 0.919024	val: 0.342120	test: 0.320751

Epoch: 95
Loss: 0.09826891769322327
ROC train: 0.988169	val: 0.755956	test: 0.723717
PRC train: 0.914530	val: 0.350904	test: 0.322739

Epoch: 96
Loss: 0.0988261352209846
ROC train: 0.988806	val: 0.751418	test: 0.728474
PRC train: 0.919170	val: 0.344212	test: 0.329363

Epoch: 97
Loss: 0.09484233129656845
ROC train: 0.989952	val: 0.744796	test: 0.717908
PRC train: 0.926978	val: 0.338476	test: 0.309423

Epoch: 98
Loss: 0.09618738012451755
ROC train: 0.989828	val: 0.751143	test: 0.723968
PRC train: 0.922287	val: 0.342694	test: 0.325349

Epoch: 99
Loss: 0.09373358041753818
ROC train: 0.991570	val: 0.749043	test: 0.719475
PRC train: 0.936014	val: 0.349353	test: 0.319960

Epoch: 100
Loss: 0.09200294237854033
ROC train: 0.990350	val: 0.752721	test: 0.725901
PRC train: 0.924102	val: 0.347475	test: 0.332841

Epoch: 101
Loss: 0.09339741959088851
ROC train: 0.990373	val: 0.747191	test: 0.723234
PRC train: 0.928183	val: 0.366209	test: 0.335200

Epoch: 102
Loss: 0.09105105984880345
ROC train: 0.991711	val: 0.746540	test: 0.728369
PRC train: 0.938241	val: 0.346022	test: 0.322585

Epoch: 103
Loss: 0.08970503619766182
ROC train: 0.991415	val: 0.756737	test: 0.726369
PRC train: 0.935390	val: 0.350731	test: 0.322777

Epoch: 104
Loss: 0.08923346869932651
ROC train: 0.993342	val: 0.743924	test: 0.716446
PRC train: 0.947620	val: 0.331391	test: 0.303228

Epoch: 105
Loss: 0.08946078776483972
ROC train: 0.992728	val: 0.749441	test: 0.726893
PRC train: 0.948628	val: 0.342082	test: 0.330866

Epoch: 106
Loss: 0.08771265533435439
ROC train: 0.993258	val: 0.754051	test: 0.729086
PRC train: 0.951026	val: 0.347677	test: 0.332014

Epoch: 107
Loss: 0.0855345763633313
ROC train: 0.994121	val: 0.748173	test: 0.723729
PRC train: 0.955739	val: 0.336085	test: 0.315304

Epoch: 108
Loss: 0.08544316674991212
ROC train: 0.994347	val: 0.743093	test: 0.716505
PRC train: 0.954078	val: 0.336920	test: 0.298000

Epoch: 109
Loss: 0.08537338199692474
ROC train: 0.993497	val: 0.745665	test: 0.732348
PRC train: 0.948313	val: 0.340616	test: 0.340103

Epoch: 110
Loss: 0.08416447149717028
ROC train: 0.995093	val: 0.748915	test: 0.733927
PRC train: 0.960645	val: 0.356566	test: 0.330870

Epoch: 111
Loss: 0.08232139040593629
ROC train: 0.993936	val: 0.737167	test: 0.723363
PRC train: 0.953229	val: 0.338306	test: 0.324364

Epoch: 112
Loss: 0.08279984209863026
ROC train: 0.994659	val: 0.751967	test: 0.730767
PRC train: 0.958539	val: 0.355932	test: 0.335598

Epoch: 113
Loss: 0.08456050409573457
ROC train: 0.995039	val: 0.745159	test: 0.722808
PRC train: 0.961742	val: 0.355003	test: 0.324196

Epoch: 114
Loss: 0.08368884573020086
ROC train: 0.995390	val: 0.747605	test: 0.720806
PRC train: 0.963139	val: 0.334703	test: 0.310671

Epoch: 115
Loss: 0.08044237005584139
ROC train: 0.994619	val: 0.750246	test: 0.709392
PRC train: 0.957412	val: 0.343229	test: 0.325583

Epoch: 116
Loss: 0.08052185021554313
ROC train: 0.995888	val: 0.744070	test: 0.722573
PRC train: 0.967201	val: 0.348285	test: 0.331667

Epoch: 117
Loss: 0.07896958559743274
ROC train: 0.995209	val: 0.748800	test: 0.731365
PRC train: 0.961115	val: 0.332171	test: 0.314200

Epoch: 118
Loss: 0.07867670853060567
ROC train: 0.996340	val: 0.751138	test: 0.726261
PRC train: 0.968985	val: 0.361915	test: 0.336858

Epoch: 119
Loss: 0.07667153821958493
ROC train: 0.996421	val: 0.735731	test: 0.717737
PRC train: 0.967948	val: 0.349244	test: 0.331285

Epoch: 120
Loss: 0.0787318696208865
ROC train: 0.996750	val: 0.748101	test: 0.717705
PRC train: 0.974705	val: 0.344832	test: 0.317217

Early stopping
Best (ROC):	 train: 0.906040	val: 0.790317	test: 0.753446
Best (PRC):	 train: 0.582489	val: 0.357629	test: 0.351034

PRC train: 0.917844	val: 0.300468	test: 0.282143

Epoch: 94
Loss: 0.09578447813911588
ROC train: 0.990228	val: 0.718773	test: 0.694905
PRC train: 0.923782	val: 0.299742	test: 0.271172

Epoch: 95
Loss: 0.09484276471075667
ROC train: 0.991798	val: 0.717186	test: 0.681323
PRC train: 0.935149	val: 0.300019	test: 0.267005

Epoch: 96
Loss: 0.09666501337524495
ROC train: 0.992209	val: 0.726267	test: 0.691871
PRC train: 0.939366	val: 0.302845	test: 0.286392

Epoch: 97
Loss: 0.09162928181123174
ROC train: 0.992246	val: 0.720744	test: 0.684871
PRC train: 0.938145	val: 0.300214	test: 0.274147

Epoch: 98
Loss: 0.09092619207723023
ROC train: 0.992459	val: 0.716330	test: 0.695910
PRC train: 0.940900	val: 0.301290	test: 0.295561

Epoch: 99
Loss: 0.09123203570228675
ROC train: 0.993034	val: 0.719622	test: 0.687371
PRC train: 0.945418	val: 0.301416	test: 0.281215

Epoch: 100
Loss: 0.09028124531140333
ROC train: 0.993417	val: 0.721476	test: 0.686851
PRC train: 0.948747	val: 0.312173	test: 0.271851

Epoch: 101
Loss: 0.08979821600384938
ROC train: 0.993441	val: 0.707668	test: 0.687868
PRC train: 0.945590	val: 0.280045	test: 0.253902

Epoch: 102
Loss: 0.08864070690616915
ROC train: 0.993842	val: 0.707761	test: 0.678590
PRC train: 0.951894	val: 0.302947	test: 0.279451

Epoch: 103
Loss: 0.0868597391823806
ROC train: 0.993841	val: 0.714383	test: 0.688052
PRC train: 0.950544	val: 0.300688	test: 0.288000

Epoch: 104
Loss: 0.08529234066986825
ROC train: 0.994755	val: 0.724617	test: 0.688024
PRC train: 0.953823	val: 0.310068	test: 0.284870

Epoch: 105
Loss: 0.08250366895080917
ROC train: 0.995549	val: 0.699627	test: 0.672482
PRC train: 0.958838	val: 0.283661	test: 0.254659

Epoch: 106
Loss: 0.0819441495580105
ROC train: 0.995631	val: 0.723171	test: 0.695640
PRC train: 0.963341	val: 0.297345	test: 0.283774

Epoch: 107
Loss: 0.08116541238099587
ROC train: 0.995891	val: 0.713593	test: 0.683496
PRC train: 0.966046	val: 0.317514	test: 0.282183

Epoch: 108
Loss: 0.08195140700220388
ROC train: 0.995813	val: 0.722127	test: 0.687981
PRC train: 0.964779	val: 0.294590	test: 0.263951

Epoch: 109
Loss: 0.08054161119200591
ROC train: 0.996163	val: 0.714527	test: 0.685453
PRC train: 0.967919	val: 0.290433	test: 0.252487

Epoch: 110
Loss: 0.08050363296600942
ROC train: 0.995672	val: 0.699003	test: 0.663412
PRC train: 0.961997	val: 0.276218	test: 0.244755

Epoch: 111
Loss: 0.078891547768231
ROC train: 0.996790	val: 0.696331	test: 0.671866
PRC train: 0.972505	val: 0.293381	test: 0.257344

Epoch: 112
Loss: 0.07712522608836138
ROC train: 0.996226	val: 0.697186	test: 0.661377
PRC train: 0.966928	val: 0.268183	test: 0.239204

Epoch: 113
Loss: 0.07784420489610949
ROC train: 0.996889	val: 0.709837	test: 0.679724
PRC train: 0.971254	val: 0.309041	test: 0.271317

Epoch: 114
Loss: 0.07576759226527646
ROC train: 0.997098	val: 0.713515	test: 0.683575
PRC train: 0.974021	val: 0.290119	test: 0.267724

Epoch: 115
Loss: 0.07382650416746667
ROC train: 0.997088	val: 0.700518	test: 0.673343
PRC train: 0.975897	val: 0.295878	test: 0.270460

Epoch: 116
Loss: 0.07603997962534216
ROC train: 0.997541	val: 0.710550	test: 0.678252
PRC train: 0.978467	val: 0.303146	test: 0.271698

Epoch: 117
Loss: 0.07422573319258106
ROC train: 0.997530	val: 0.718710	test: 0.683177
PRC train: 0.977740	val: 0.306496	test: 0.271146

Epoch: 118
Loss: 0.07203111571287839
ROC train: 0.997755	val: 0.723444	test: 0.679384
PRC train: 0.979157	val: 0.304727	test: 0.264102

Epoch: 119
Loss: 0.0717488930916993
ROC train: 0.997568	val: 0.731850	test: 0.692456
PRC train: 0.977702	val: 0.319441	test: 0.289095

Epoch: 120
Loss: 0.07070210042813724
ROC train: 0.998109	val: 0.719071	test: 0.680479
PRC train: 0.981236	val: 0.298171	test: 0.275665

Early stopping
Best (ROC):	 train: 0.869862	val: 0.759145	test: 0.712318
Best (PRC):	 train: 0.460354	val: 0.309080	test: 0.305535

PRC train: 0.925312	val: 0.260045	test: 0.224090

Epoch: 94
Loss: 0.09499143334027556
ROC train: 0.991651	val: 0.699395	test: 0.680799
PRC train: 0.932551	val: 0.284973	test: 0.251528

Epoch: 95
Loss: 0.09144043628814387
ROC train: 0.992043	val: 0.673572	test: 0.656842
PRC train: 0.933818	val: 0.244026	test: 0.220649

Epoch: 96
Loss: 0.09366689200116073
ROC train: 0.991973	val: 0.677647	test: 0.647791
PRC train: 0.934065	val: 0.227287	test: 0.201440

Epoch: 97
Loss: 0.09096988819625279
ROC train: 0.991911	val: 0.678268	test: 0.669561
PRC train: 0.932776	val: 0.248415	test: 0.225274

Epoch: 98
Loss: 0.09072484130842758
ROC train: 0.993775	val: 0.678916	test: 0.656454
PRC train: 0.951053	val: 0.235224	test: 0.214931

Epoch: 99
Loss: 0.09090440092611861
ROC train: 0.992758	val: 0.675003	test: 0.666368
PRC train: 0.938529	val: 0.235802	test: 0.222500

Epoch: 100
Loss: 0.09304200647146788
ROC train: 0.993898	val: 0.678871	test: 0.662872
PRC train: 0.948184	val: 0.255924	test: 0.233216

Epoch: 101
Loss: 0.08618472519346809
ROC train: 0.994189	val: 0.688612	test: 0.665118
PRC train: 0.951109	val: 0.260474	test: 0.232534

Epoch: 102
Loss: 0.085229044719118
ROC train: 0.994296	val: 0.689878	test: 0.678111
PRC train: 0.950814	val: 0.301824	test: 0.277909

Epoch: 103
Loss: 0.08591591953276322
ROC train: 0.994036	val: 0.681217	test: 0.645627
PRC train: 0.953833	val: 0.221324	test: 0.195163

Epoch: 104
Loss: 0.08366502854037883
ROC train: 0.994601	val: 0.698504	test: 0.682351
PRC train: 0.952562	val: 0.275952	test: 0.256919

Epoch: 105
Loss: 0.08293052072595042
ROC train: 0.995386	val: 0.688558	test: 0.648790
PRC train: 0.959456	val: 0.244153	test: 0.217438

Epoch: 106
Loss: 0.08259028537110615
ROC train: 0.995742	val: 0.697849	test: 0.676654
PRC train: 0.960042	val: 0.267400	test: 0.242568

Epoch: 107
Loss: 0.08169381781298067
ROC train: 0.996003	val: 0.696699	test: 0.669562
PRC train: 0.963486	val: 0.294048	test: 0.268266

Epoch: 108
Loss: 0.08126900206809586
ROC train: 0.996186	val: 0.700244	test: 0.670687
PRC train: 0.965679	val: 0.278428	test: 0.235320

Epoch: 109
Loss: 0.07924939140974924
ROC train: 0.996469	val: 0.707640	test: 0.667635
PRC train: 0.967974	val: 0.285992	test: 0.236846

Epoch: 110
Loss: 0.07786200943802457
ROC train: 0.996480	val: 0.687999	test: 0.678760
PRC train: 0.967775	val: 0.262182	test: 0.237877

Epoch: 111
Loss: 0.0794686579788059
ROC train: 0.997020	val: 0.702765	test: 0.683133
PRC train: 0.970376	val: 0.280438	test: 0.252280

Epoch: 112
Loss: 0.07643700131350357
ROC train: 0.997241	val: 0.695723	test: 0.665248
PRC train: 0.975425	val: 0.256171	test: 0.231869

Epoch: 113
Loss: 0.07584673064407589
ROC train: 0.997311	val: 0.681162	test: 0.657383
PRC train: 0.975932	val: 0.220409	test: 0.210074

Epoch: 114
Loss: 0.07580487004787471
ROC train: 0.997661	val: 0.670773	test: 0.636193
PRC train: 0.978626	val: 0.223019	test: 0.191623

Epoch: 115
Loss: 0.07515346903628467
ROC train: 0.997574	val: 0.694966	test: 0.671480
PRC train: 0.977497	val: 0.279415	test: 0.254035

Epoch: 116
Loss: 0.07304582237218735
ROC train: 0.996991	val: 0.676398	test: 0.640439
PRC train: 0.972929	val: 0.214927	test: 0.193742

Epoch: 117
Loss: 0.07264560299334917
ROC train: 0.997346	val: 0.700892	test: 0.676652
PRC train: 0.976550	val: 0.276742	test: 0.260648

Epoch: 118
Loss: 0.0723805225240958
ROC train: 0.998049	val: 0.684922	test: 0.656502
PRC train: 0.982633	val: 0.238277	test: 0.215558

Epoch: 119
Loss: 0.06983630964226492
ROC train: 0.998347	val: 0.687680	test: 0.659833
PRC train: 0.982873	val: 0.231594	test: 0.210911

Epoch: 120
Loss: 0.06922691792301512
ROC train: 0.997408	val: 0.677013	test: 0.660816
PRC train: 0.974993	val: 0.251521	test: 0.218957

Early stopping
Best (ROC):	 train: 0.865674	val: 0.761419	test: 0.696332
Best (PRC):	 train: 0.435712	val: 0.325914	test: 0.296729

PRC train: 0.891355	val: 0.376953	test: 0.332226

Epoch: 94
Loss: 0.10334789792509018
ROC train: 0.982644	val: 0.779896	test: 0.738516
PRC train: 0.885791	val: 0.380646	test: 0.359290

Epoch: 95
Loss: 0.10230166815734144
ROC train: 0.985754	val: 0.772509	test: 0.722946
PRC train: 0.898385	val: 0.376402	test: 0.350908

Epoch: 96
Loss: 0.09949157718719637
ROC train: 0.985978	val: 0.775830	test: 0.730916
PRC train: 0.905919	val: 0.367262	test: 0.353894

Epoch: 97
Loss: 0.09972947749099034
ROC train: 0.983885	val: 0.774615	test: 0.730090
PRC train: 0.890022	val: 0.360575	test: 0.353409

Epoch: 98
Loss: 0.10038774486023128
ROC train: 0.985804	val: 0.769520	test: 0.735511
PRC train: 0.904714	val: 0.355709	test: 0.356959

Epoch: 99
Loss: 0.09784071661123754
ROC train: 0.985903	val: 0.763549	test: 0.730160
PRC train: 0.904794	val: 0.364809	test: 0.348382

Epoch: 100
Loss: 0.09471259908161751
ROC train: 0.988488	val: 0.769057	test: 0.722733
PRC train: 0.917232	val: 0.368331	test: 0.348575

Epoch: 101
Loss: 0.09687718414524893
ROC train: 0.987691	val: 0.766234	test: 0.729287
PRC train: 0.916315	val: 0.360554	test: 0.340316

Epoch: 102
Loss: 0.09469438811039498
ROC train: 0.988385	val: 0.768948	test: 0.732987
PRC train: 0.919650	val: 0.352094	test: 0.341853

Epoch: 103
Loss: 0.09520835778045868
ROC train: 0.988354	val: 0.775406	test: 0.721018
PRC train: 0.918146	val: 0.365200	test: 0.350630

Epoch: 104
Loss: 0.09308851106861281
ROC train: 0.989486	val: 0.754923	test: 0.714081
PRC train: 0.924989	val: 0.337025	test: 0.338858

Epoch: 105
Loss: 0.09335240877077579
ROC train: 0.988803	val: 0.764766	test: 0.726807
PRC train: 0.921437	val: 0.360030	test: 0.337728

Epoch: 106
Loss: 0.0924156897187471
ROC train: 0.989945	val: 0.768309	test: 0.726888
PRC train: 0.927537	val: 0.355522	test: 0.351405

Epoch: 107
Loss: 0.09052863986826296
ROC train: 0.990271	val: 0.763856	test: 0.721560
PRC train: 0.929633	val: 0.363567	test: 0.326686

Epoch: 108
Loss: 0.09260571174340931
ROC train: 0.989320	val: 0.759183	test: 0.716607
PRC train: 0.923549	val: 0.374182	test: 0.347690

Epoch: 109
Loss: 0.09096169807887156
ROC train: 0.990075	val: 0.773090	test: 0.727582
PRC train: 0.929660	val: 0.364374	test: 0.345756

Epoch: 110
Loss: 0.08768494433103578
ROC train: 0.991792	val: 0.760359	test: 0.728905
PRC train: 0.939761	val: 0.349216	test: 0.336055

Epoch: 111
Loss: 0.0878554761727131
ROC train: 0.991663	val: 0.765768	test: 0.723167
PRC train: 0.938504	val: 0.365347	test: 0.337501

Epoch: 112
Loss: 0.08676253363804057
ROC train: 0.991609	val: 0.760873	test: 0.725423
PRC train: 0.934982	val: 0.362743	test: 0.350169

Epoch: 113
Loss: 0.08956482267743823
ROC train: 0.990739	val: 0.768157	test: 0.727282
PRC train: 0.931015	val: 0.360970	test: 0.341456

Epoch: 114
Loss: 0.08580716809384438
ROC train: 0.992360	val: 0.767380	test: 0.735397
PRC train: 0.943202	val: 0.366807	test: 0.349797

Epoch: 115
Loss: 0.08443315540049
ROC train: 0.992896	val: 0.763128	test: 0.724592
PRC train: 0.948030	val: 0.352106	test: 0.341823

Epoch: 116
Loss: 0.08509090473979701
ROC train: 0.993346	val: 0.763360	test: 0.712723
PRC train: 0.951142	val: 0.355872	test: 0.331590

Epoch: 117
Loss: 0.08328146323824738
ROC train: 0.993756	val: 0.758465	test: 0.731835
PRC train: 0.955678	val: 0.362206	test: 0.337881

Epoch: 118
Loss: 0.08222860349871838
ROC train: 0.993046	val: 0.762915	test: 0.726310
PRC train: 0.949139	val: 0.349675	test: 0.337146

Epoch: 119
Loss: 0.08149628547755501
ROC train: 0.994395	val: 0.755879	test: 0.711324
PRC train: 0.957032	val: 0.345548	test: 0.332452

Epoch: 120
Loss: 0.08129655989459737
ROC train: 0.994418	val: 0.764382	test: 0.733233
PRC train: 0.957500	val: 0.353230	test: 0.327695

Early stopping
Best (ROC):	 train: 0.934183	val: 0.787645	test: 0.750245
Best (PRC):	 train: 0.666453	val: 0.374730	test: 0.367482

PRC train: 0.900182	val: 0.356073	test: 0.348632

Epoch: 94
Loss: 0.09986228535345816
ROC train: 0.985250	val: 0.768353	test: 0.720651
PRC train: 0.901093	val: 0.353140	test: 0.320672

Epoch: 95
Loss: 0.10078537400603553
ROC train: 0.986428	val: 0.768807	test: 0.714060
PRC train: 0.908041	val: 0.355684	test: 0.322576

Epoch: 96
Loss: 0.10208922633669916
ROC train: 0.983255	val: 0.753886	test: 0.714762
PRC train: 0.881986	val: 0.341510	test: 0.313130

Epoch: 97
Loss: 0.09989502751427529
ROC train: 0.988227	val: 0.757703	test: 0.711611
PRC train: 0.915399	val: 0.353855	test: 0.345758

Epoch: 98
Loss: 0.09996190822258365
ROC train: 0.987723	val: 0.757447	test: 0.711706
PRC train: 0.912991	val: 0.359124	test: 0.329826

Epoch: 99
Loss: 0.09645543340386097
ROC train: 0.988663	val: 0.765589	test: 0.714468
PRC train: 0.916307	val: 0.369308	test: 0.350525

Epoch: 100
Loss: 0.09556447558750404
ROC train: 0.988398	val: 0.755791	test: 0.708337
PRC train: 0.916290	val: 0.344864	test: 0.317821

Epoch: 101
Loss: 0.09511480697711176
ROC train: 0.988918	val: 0.762390	test: 0.710360
PRC train: 0.920198	val: 0.356273	test: 0.349719

Epoch: 102
Loss: 0.09316616418164222
ROC train: 0.990778	val: 0.758237	test: 0.712308
PRC train: 0.930614	val: 0.338088	test: 0.336398

Epoch: 103
Loss: 0.09194628004860181
ROC train: 0.990024	val: 0.765717	test: 0.717312
PRC train: 0.926626	val: 0.339412	test: 0.328364

Epoch: 104
Loss: 0.09155014399393448
ROC train: 0.988833	val: 0.764132	test: 0.713053
PRC train: 0.910692	val: 0.352092	test: 0.344285

Epoch: 105
Loss: 0.09215134860368308
ROC train: 0.990042	val: 0.751921	test: 0.706424
PRC train: 0.924799	val: 0.310790	test: 0.297041

Epoch: 106
Loss: 0.08921019353821145
ROC train: 0.991808	val: 0.763143	test: 0.702978
PRC train: 0.937242	val: 0.361758	test: 0.327193

Epoch: 107
Loss: 0.09147218358615804
ROC train: 0.991536	val: 0.770984	test: 0.706225
PRC train: 0.937694	val: 0.344901	test: 0.319087

Epoch: 108
Loss: 0.08955937932732981
ROC train: 0.991036	val: 0.748362	test: 0.693232
PRC train: 0.935104	val: 0.316767	test: 0.276720

Epoch: 109
Loss: 0.08755004228944442
ROC train: 0.992635	val: 0.748989	test: 0.716889
PRC train: 0.947736	val: 0.340596	test: 0.319529

Epoch: 110
Loss: 0.0895973096755858
ROC train: 0.993602	val: 0.766638	test: 0.711268
PRC train: 0.950713	val: 0.337767	test: 0.331590

Epoch: 111
Loss: 0.08512673750209121
ROC train: 0.992847	val: 0.769164	test: 0.712457
PRC train: 0.952426	val: 0.365220	test: 0.339564

Epoch: 112
Loss: 0.08868104468262594
ROC train: 0.993514	val: 0.759653	test: 0.715018
PRC train: 0.951189	val: 0.342770	test: 0.331174

Epoch: 113
Loss: 0.08511497469633039
ROC train: 0.993621	val: 0.767028	test: 0.725074
PRC train: 0.949734	val: 0.340234	test: 0.326201

Epoch: 114
Loss: 0.08398766472794124
ROC train: 0.994057	val: 0.763328	test: 0.712035
PRC train: 0.954847	val: 0.338877	test: 0.328101

Epoch: 115
Loss: 0.08188564021334896
ROC train: 0.994930	val: 0.763356	test: 0.701688
PRC train: 0.961484	val: 0.334963	test: 0.321480

Epoch: 116
Loss: 0.08179814973846915
ROC train: 0.994446	val: 0.761970	test: 0.710316
PRC train: 0.953046	val: 0.334725	test: 0.327821

Epoch: 117
Loss: 0.0831765967690305
ROC train: 0.994474	val: 0.765492	test: 0.714086
PRC train: 0.955533	val: 0.348148	test: 0.321080

Epoch: 118
Loss: 0.0813951352428099
ROC train: 0.993490	val: 0.763971	test: 0.707036
PRC train: 0.948643	val: 0.341858	test: 0.326604

Epoch: 119
Loss: 0.08103079880650933
ROC train: 0.995082	val: 0.767687	test: 0.710081
PRC train: 0.959155	val: 0.347206	test: 0.328736

Epoch: 120
Loss: 0.07999788885286352
ROC train: 0.995088	val: 0.762357	test: 0.708993
PRC train: 0.957832	val: 0.336961	test: 0.317053

Early stopping
Best (ROC):	 train: 0.948340	val: 0.784083	test: 0.729589
Best (PRC):	 train: 0.720206	val: 0.374312	test: 0.353921
All runs completed.

PRC train: 0.884608	val: 0.357168	test: 0.352575

Epoch: 94
Loss: 0.10529904967996387
ROC train: 0.984141	val: 0.770975	test: 0.721200
PRC train: 0.890488	val: 0.363451	test: 0.360182

Epoch: 95
Loss: 0.10288933696937226
ROC train: 0.983457	val: 0.771125	test: 0.727176
PRC train: 0.890919	val: 0.374339	test: 0.361455

Epoch: 96
Loss: 0.10247370030653485
ROC train: 0.984834	val: 0.769078	test: 0.731087
PRC train: 0.900296	val: 0.355917	test: 0.359930

Epoch: 97
Loss: 0.10107890080343747
ROC train: 0.986194	val: 0.757632	test: 0.722448
PRC train: 0.903631	val: 0.363740	test: 0.358604

Epoch: 98
Loss: 0.09988745414021334
ROC train: 0.986808	val: 0.758876	test: 0.727683
PRC train: 0.909658	val: 0.353952	test: 0.349505

Epoch: 99
Loss: 0.09961926482766228
ROC train: 0.986267	val: 0.745981	test: 0.727449
PRC train: 0.905051	val: 0.331995	test: 0.350880

Epoch: 100
Loss: 0.10177400132721502
ROC train: 0.986184	val: 0.765808	test: 0.722060
PRC train: 0.907514	val: 0.346045	test: 0.344515

Epoch: 101
Loss: 0.10054571868437195
ROC train: 0.985596	val: 0.750589	test: 0.724301
PRC train: 0.902428	val: 0.338003	test: 0.345559

Epoch: 102
Loss: 0.09897280043516146
ROC train: 0.985683	val: 0.762962	test: 0.716183
PRC train: 0.901645	val: 0.367868	test: 0.346370

Epoch: 103
Loss: 0.09763670940553074
ROC train: 0.985156	val: 0.763188	test: 0.708983
PRC train: 0.899754	val: 0.363168	test: 0.344649

Epoch: 104
Loss: 0.09786016360051902
ROC train: 0.986880	val: 0.761145	test: 0.730513
PRC train: 0.912966	val: 0.350129	test: 0.357864

Epoch: 105
Loss: 0.09345220253355732
ROC train: 0.987870	val: 0.763148	test: 0.722193
PRC train: 0.918101	val: 0.354159	test: 0.356869

Epoch: 106
Loss: 0.09285570619228917
ROC train: 0.988635	val: 0.769070	test: 0.723589
PRC train: 0.919805	val: 0.362104	test: 0.357629

Epoch: 107
Loss: 0.09301875732323887
ROC train: 0.989717	val: 0.757704	test: 0.720123
PRC train: 0.929823	val: 0.365680	test: 0.345900

Epoch: 108
Loss: 0.09232244748794269
ROC train: 0.987510	val: 0.762273	test: 0.729465
PRC train: 0.908288	val: 0.362359	test: 0.355993

Epoch: 109
Loss: 0.09186171317551248
ROC train: 0.990490	val: 0.760705	test: 0.723155
PRC train: 0.933978	val: 0.345237	test: 0.355463

Epoch: 110
Loss: 0.09181085795075593
ROC train: 0.986937	val: 0.753339	test: 0.723058
PRC train: 0.910428	val: 0.321125	test: 0.337014

Epoch: 111
Loss: 0.09027430492385202
ROC train: 0.990548	val: 0.758445	test: 0.716885
PRC train: 0.930658	val: 0.350411	test: 0.355565

Epoch: 112
Loss: 0.08911074816318747
ROC train: 0.990884	val: 0.755750	test: 0.710793
PRC train: 0.934546	val: 0.340796	test: 0.336861

Epoch: 113
Loss: 0.08767048076006592
ROC train: 0.990095	val: 0.755990	test: 0.726664
PRC train: 0.928737	val: 0.348036	test: 0.360690

Epoch: 114
Loss: 0.0878050447112234
ROC train: 0.992405	val: 0.761513	test: 0.711339
PRC train: 0.942789	val: 0.365450	test: 0.362370

Epoch: 115
Loss: 0.08903468841411565
ROC train: 0.992043	val: 0.755983	test: 0.730398
PRC train: 0.942763	val: 0.355358	test: 0.367549

Epoch: 116
Loss: 0.08544120712574552
ROC train: 0.991346	val: 0.750056	test: 0.718345
PRC train: 0.937240	val: 0.328758	test: 0.342195

Epoch: 117
Loss: 0.08704286907229869
ROC train: 0.993042	val: 0.759454	test: 0.716928
PRC train: 0.948548	val: 0.346968	test: 0.352442

Epoch: 118
Loss: 0.085297897071666
ROC train: 0.992697	val: 0.754477	test: 0.726562
PRC train: 0.948146	val: 0.341846	test: 0.359435

Epoch: 119
Loss: 0.0853170447648928
ROC train: 0.992764	val: 0.750091	test: 0.718466
PRC train: 0.951527	val: 0.337554	test: 0.341692

Epoch: 120
Loss: 0.08379163603529387
ROC train: 0.992348	val: 0.749664	test: 0.708392
PRC train: 0.944033	val: 0.352296	test: 0.349876

Early stopping
Best (ROC):	 train: 0.934232	val: 0.796564	test: 0.733440
Best (PRC):	 train: 0.673846	val: 0.368039	test: 0.350785
All runs completed.

PRC train: 0.881436	val: 0.342144	test: 0.356676

Epoch: 94
Loss: 0.1040157250762827
ROC train: 0.984024	val: 0.762837	test: 0.730405
PRC train: 0.889918	val: 0.356786	test: 0.359015

Epoch: 95
Loss: 0.10206507572227114
ROC train: 0.985267	val: 0.759173	test: 0.722316
PRC train: 0.898508	val: 0.352051	test: 0.341662

Epoch: 96
Loss: 0.10153480291484071
ROC train: 0.982650	val: 0.757358	test: 0.705131
PRC train: 0.879246	val: 0.341208	test: 0.335551

Epoch: 97
Loss: 0.1021513830258269
ROC train: 0.986230	val: 0.759876	test: 0.711869
PRC train: 0.899934	val: 0.340988	test: 0.341083

Epoch: 98
Loss: 0.0996658001081058
ROC train: 0.982796	val: 0.747313	test: 0.727756
PRC train: 0.881447	val: 0.328579	test: 0.324681

Epoch: 99
Loss: 0.0982249437917959
ROC train: 0.986706	val: 0.757946	test: 0.711334
PRC train: 0.907495	val: 0.358241	test: 0.350108

Epoch: 100
Loss: 0.10218956680680687
ROC train: 0.982217	val: 0.756803	test: 0.721931
PRC train: 0.878601	val: 0.328300	test: 0.313230

Epoch: 101
Loss: 0.10101160169748309
ROC train: 0.986382	val: 0.754758	test: 0.714246
PRC train: 0.907413	val: 0.347712	test: 0.339346

Epoch: 102
Loss: 0.0974585124142386
ROC train: 0.986038	val: 0.755356	test: 0.726442
PRC train: 0.902004	val: 0.342241	test: 0.342856

Epoch: 103
Loss: 0.09374412580193742
ROC train: 0.989456	val: 0.756358	test: 0.707976
PRC train: 0.925415	val: 0.347608	test: 0.340009

Epoch: 104
Loss: 0.09307376143433191
ROC train: 0.989272	val: 0.748192	test: 0.703892
PRC train: 0.922510	val: 0.349473	test: 0.348966

Epoch: 105
Loss: 0.09434942634515861
ROC train: 0.989631	val: 0.741591	test: 0.700310
PRC train: 0.922631	val: 0.324206	test: 0.307943

Epoch: 106
Loss: 0.09300458899866619
ROC train: 0.988962	val: 0.759840	test: 0.708080
PRC train: 0.919278	val: 0.346792	test: 0.330726

Epoch: 107
Loss: 0.09293476143617121
ROC train: 0.989842	val: 0.762999	test: 0.722090
PRC train: 0.928733	val: 0.347368	test: 0.341682

Epoch: 108
Loss: 0.0903277776593077
ROC train: 0.989948	val: 0.756241	test: 0.718560
PRC train: 0.927799	val: 0.342062	test: 0.339956

Epoch: 109
Loss: 0.09030763281214779
ROC train: 0.990056	val: 0.755598	test: 0.699042
PRC train: 0.923861	val: 0.327116	test: 0.320499

Epoch: 110
Loss: 0.08915538382062714
ROC train: 0.990057	val: 0.739962	test: 0.718793
PRC train: 0.925302	val: 0.336047	test: 0.332840

Epoch: 111
Loss: 0.08870760384994206
ROC train: 0.991783	val: 0.755169	test: 0.710691
PRC train: 0.937594	val: 0.333862	test: 0.322759

Epoch: 112
Loss: 0.08884977589752792
ROC train: 0.991802	val: 0.747814	test: 0.699913
PRC train: 0.937283	val: 0.337588	test: 0.339532

Epoch: 113
Loss: 0.08983063849067363
ROC train: 0.991605	val: 0.754048	test: 0.722383
PRC train: 0.934263	val: 0.332051	test: 0.323019

Epoch: 114
Loss: 0.08655715295911472
ROC train: 0.992896	val: 0.754857	test: 0.712060
PRC train: 0.945736	val: 0.339878	test: 0.333710

Epoch: 115
Loss: 0.08609938734902314
ROC train: 0.992803	val: 0.748259	test: 0.707713
PRC train: 0.945132	val: 0.315901	test: 0.321133

Epoch: 116
Loss: 0.0861237034333988
ROC train: 0.992706	val: 0.750103	test: 0.715742
PRC train: 0.940673	val: 0.345818	test: 0.339251

Epoch: 117
Loss: 0.08656163664940697
ROC train: 0.993107	val: 0.753767	test: 0.716931
PRC train: 0.948175	val: 0.348129	test: 0.334642

Epoch: 118
Loss: 0.08436824530118087
ROC train: 0.991923	val: 0.746034	test: 0.712798
PRC train: 0.935547	val: 0.341262	test: 0.330590

Epoch: 119
Loss: 0.08321319706909826
ROC train: 0.993962	val: 0.752136	test: 0.703009
PRC train: 0.953761	val: 0.323062	test: 0.321286

Epoch: 120
Loss: 0.08720406627761762
ROC train: 0.993581	val: 0.751186	test: 0.705816
PRC train: 0.950774	val: 0.344202	test: 0.325692

Early stopping
Best (ROC):	 train: 0.913396	val: 0.788548	test: 0.732602
Best (PRC):	 train: 0.593692	val: 0.365092	test: 0.348985
All runs completed.
