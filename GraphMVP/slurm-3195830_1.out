>>> Starting run for dataset: freesolv
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running RANDOM configs_static_noise_experiments/GraphCL/freesolv/noise=0.0.yml on cuda:0
Running RANDOM configs_static_noise_experiments/GraphCL/freesolv/noise=0.05.yml on cuda:1
Running RANDOM configs_static_noise_experiments/GraphCL/freesolv/noise=0.1.yml on cuda:2
Running RANDOM configs_static_noise_experiments/GraphCL/freesolv/noise=0.2.yml on cuda:3
Starting process for seed 1: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.1.yml --runseed 1 --device cuda:2
Starting process for seed 2: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.1.yml --runseed 2 --device cuda:2
Starting process for seed 3: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.1.yml --runseed 3 --device cuda:2
Starting process for seed 1: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.2.yml --runseed 1 --device cuda:3
Starting process for seed 2: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.2.yml --runseed 2 --device cuda:3
Starting process for seed 1: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.05.yml --runseed 1 --device cuda:1
Starting process for seed 3: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.2.yml --runseed 3 --device cuda:3
Starting process for seed 2: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.05.yml --runseed 2 --device cuda:1
Starting process for seed 1: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.0.yml --runseed 1 --device cuda:0
Starting process for seed 2: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.0.yml --runseed 2 --device cuda:0
Starting process for seed 3: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.05.yml --runseed 3 --device cuda:1
Starting process for seed 3: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.0.yml --runseed 3 --device cuda:0
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/freesolv/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/freesolv/noise=0.0/freesolv_scaff_3_20-05_14-43-31  ]
[ Using Seed :  3  ]
[ Using device :  cuda:0  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 21.00621223449707
RMSE train: 4.811733	val: 8.518590	test: 6.743691
MAE train: 3.933183	val: 6.499681	test: 5.837927

Epoch: 2
Loss: 19.751680374145508
RMSE train: 4.882005	val: 8.499481	test: 6.943514
MAE train: 4.043867	val: 6.508329	test: 6.035385

Epoch: 3
Loss: 18.751426696777344
RMSE train: 4.979021	val: 8.395327	test: 6.948446
MAE train: 4.192075	val: 6.426325	test: 6.043867

Epoch: 4
Loss: 17.782504081726074
RMSE train: 4.929856	val: 8.229859	test: 6.850927
MAE train: 4.186301	val: 6.332997	test: 5.957150

Epoch: 5
Loss: 16.61996555328369
RMSE train: 4.783375	val: 8.040753	test: 6.729215
MAE train: 4.108291	val: 6.239579	test: 5.852403

Epoch: 6
Loss: 15.205130100250244
RMSE train: 4.590211	val: 7.858705	test: 6.628786
MAE train: 3.979541	val: 6.179417	test: 5.771539

Epoch: 7
Loss: 14.171260356903076
RMSE train: 4.244192	val: 7.589745	test: 6.448304
MAE train: 3.706490	val: 6.022828	test: 5.606576

Epoch: 8
Loss: 13.024376392364502
RMSE train: 3.807721	val: 7.167265	test: 6.217473
MAE train: 3.361515	val: 5.751212	test: 5.382895

Epoch: 9
Loss: 12.304733753204346
RMSE train: 3.478572	val: 6.802557	test: 6.066092
MAE train: 3.088668	val: 5.523807	test: 5.242076

Epoch: 10
Loss: 11.490296840667725
RMSE train: 3.195804	val: 6.424676	test: 5.881318
MAE train: 2.805601	val: 5.205456	test: 5.026817

Epoch: 11
Loss: 10.726729393005371
RMSE train: 3.042891	val: 6.131728	test: 5.754357
MAE train: 2.628171	val: 4.928022	test: 4.887030

Epoch: 12
Loss: 10.357687950134277
RMSE train: 3.011002	val: 6.003931	test: 5.734557
MAE train: 2.592935	val: 4.853069	test: 4.883381

Epoch: 13
Loss: 9.82172966003418
RMSE train: 3.034643	val: 6.015927	test: 5.758247
MAE train: 2.625176	val: 4.944650	test: 4.924245

Epoch: 14
Loss: 9.25780439376831
RMSE train: 3.060311	val: 6.070669	test: 5.774618
MAE train: 2.666280	val: 5.086214	test: 4.969898

Epoch: 15
Loss: 8.881179571151733
RMSE train: 3.069936	val: 6.139569	test: 5.748308
MAE train: 2.693316	val: 5.202641	test: 4.974507

Epoch: 16
Loss: 8.284436225891113
RMSE train: 3.069510	val: 6.179704	test: 5.702337
MAE train: 2.714171	val: 5.252887	test: 4.948315

Epoch: 17
Loss: 7.984701633453369
RMSE train: 3.081033	val: 6.171547	test: 5.592500
MAE train: 2.750353	val: 5.232728	test: 4.864287

Epoch: 18
Loss: 7.543512344360352
RMSE train: 3.099129	val: 6.156958	test: 5.495398
MAE train: 2.791462	val: 5.198381	test: 4.776052

Epoch: 19
Loss: 7.21254563331604
RMSE train: 3.116825	val: 6.131381	test: 5.401810
MAE train: 2.820511	val: 5.124972	test: 4.669953

Epoch: 20
Loss: 6.95695948600769
RMSE train: 3.096533	val: 6.117902	test: 5.315701
MAE train: 2.799668	val: 5.070778	test: 4.571738

Epoch: 21
Loss: 6.598658800125122
RMSE train: 3.055009	val: 6.116563	test: 5.267383
MAE train: 2.754339	val: 5.046707	test: 4.519905

Epoch: 22
Loss: 6.1690919399261475
RMSE train: 2.993066	val: 6.094040	test: 5.205039
MAE train: 2.689512	val: 5.018535	test: 4.461811
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/freesolv/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/freesolv/noise=0.0/freesolv_scaff_1_20-05_14-43-31  ]
[ Using Seed :  1  ]
[ Using device :  cuda:0  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 21.71658420562744
RMSE train: 4.814452	val: 8.976474	test: 7.305106
MAE train: 3.924801	val: 6.949817	test: 6.444277

Epoch: 2
Loss: 20.577255249023438
RMSE train: 4.868626	val: 8.991254	test: 7.379811
MAE train: 3.993103	val: 7.009502	test: 6.520755

Epoch: 3
Loss: 19.673069953918457
RMSE train: 4.881849	val: 8.986431	test: 7.415624
MAE train: 4.036829	val: 7.051742	test: 6.551917

Epoch: 4
Loss: 18.613280296325684
RMSE train: 4.889605	val: 9.041821	test: 7.498262
MAE train: 4.086963	val: 7.189583	test: 6.628965

Epoch: 5
Loss: 17.217900276184082
RMSE train: 4.870380	val: 9.102656	test: 7.578267
MAE train: 4.105809	val: 7.332815	test: 6.686788

Epoch: 6
Loss: 16.30649709701538
RMSE train: 4.753078	val: 9.134539	test: 7.636586
MAE train: 4.048683	val: 7.435900	test: 6.710108

Epoch: 7
Loss: 15.010364532470703
RMSE train: 4.582478	val: 9.090809	test: 7.597886
MAE train: 3.952487	val: 7.454163	test: 6.668870

Epoch: 8
Loss: 13.990908145904541
RMSE train: 4.381064	val: 8.847589	test: 7.384811
MAE train: 3.827758	val: 7.286950	test: 6.486044

Epoch: 9
Loss: 13.027184963226318
RMSE train: 4.144466	val: 8.446184	test: 7.078085
MAE train: 3.666806	val: 6.998669	test: 6.217794

Epoch: 10
Loss: 12.14262342453003
RMSE train: 3.879945	val: 7.989970	test: 6.782218
MAE train: 3.449528	val: 6.659169	test: 5.937431

Epoch: 11
Loss: 11.172439575195312
RMSE train: 3.623145	val: 7.497307	test: 6.453001
MAE train: 3.215643	val: 6.281867	test: 5.620342

Epoch: 12
Loss: 10.459732055664062
RMSE train: 3.414854	val: 6.975357	test: 6.114774
MAE train: 3.009785	val: 5.904481	test: 5.304649

Epoch: 13
Loss: 9.963595867156982
RMSE train: 3.278021	val: 6.558381	test: 5.816700
MAE train: 2.876499	val: 5.599733	test: 5.039026

Epoch: 14
Loss: 9.648014068603516
RMSE train: 3.192124	val: 6.274864	test: 5.552239
MAE train: 2.795449	val: 5.369238	test: 4.776616

Epoch: 15
Loss: 9.222870349884033
RMSE train: 3.114272	val: 6.082990	test: 5.337688
MAE train: 2.725807	val: 5.189356	test: 4.539905

Epoch: 16
Loss: 8.6417396068573
RMSE train: 3.086648	val: 6.024839	test: 5.224680
MAE train: 2.711202	val: 5.100420	test: 4.373671

Epoch: 17
Loss: 8.477801322937012
RMSE train: 3.062856	val: 6.013297	test: 5.160195
MAE train: 2.702308	val: 5.078565	test: 4.278743

Epoch: 18
Loss: 8.393684387207031
RMSE train: 3.018295	val: 6.055835	test: 5.118967
MAE train: 2.665728	val: 5.103009	test: 4.225598

Epoch: 19
Loss: 7.886341333389282
RMSE train: 2.978649	val: 6.117334	test: 5.086117
MAE train: 2.630125	val: 5.134453	test: 4.193574

Epoch: 20
Loss: 7.662274599075317
RMSE train: 2.918319	val: 6.161589	test: 5.040877
MAE train: 2.570151	val: 5.117623	test: 4.150452

Epoch: 21
Loss: 6.869426727294922
RMSE train: 2.857181	val: 6.184280	test: 5.012812
MAE train: 2.514083	val: 5.113201	test: 4.127157

Epoch: 22
Loss: 6.644971609115601
RMSE train: 2.778762	val: 6.159965	test: 4.977253
MAE train: 2.437748	val: 5.088545	test: 4.117667
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/freesolv/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/freesolv/noise=0.0/freesolv_scaff_2_20-05_14-43-31  ]
[ Using Seed :  2  ]
[ Using device :  cuda:0  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 21.84232807159424
RMSE train: 4.679072	val: 8.736503	test: 6.491426
MAE train: 3.802652	val: 6.654065	test: 5.503326

Epoch: 2
Loss: 20.801426887512207
RMSE train: 4.812053	val: 8.714106	test: 6.692745
MAE train: 3.973987	val: 6.611338	test: 5.757459

Epoch: 3
Loss: 19.69269847869873
RMSE train: 4.838327	val: 8.601905	test: 6.676262
MAE train: 4.020397	val: 6.483537	test: 5.744492

Epoch: 4
Loss: 18.29893970489502
RMSE train: 4.753025	val: 8.452789	test: 6.556983
MAE train: 3.954482	val: 6.334925	test: 5.618261

Epoch: 5
Loss: 17.353221893310547
RMSE train: 4.621795	val: 8.271942	test: 6.409377
MAE train: 3.881623	val: 6.182395	test: 5.463713

Epoch: 6
Loss: 16.057064056396484
RMSE train: 4.427301	val: 8.030058	test: 6.296214
MAE train: 3.756666	val: 6.043236	test: 5.356704

Epoch: 7
Loss: 14.493267059326172
RMSE train: 4.230358	val: 7.775463	test: 6.264592
MAE train: 3.650169	val: 5.924606	test: 5.354916

Epoch: 8
Loss: 13.656960010528564
RMSE train: 4.045719	val: 7.499571	test: 6.258463
MAE train: 3.559222	val: 5.782347	test: 5.377830

Epoch: 9
Loss: 12.523808479309082
RMSE train: 3.883259	val: 7.251873	test: 6.266495
MAE train: 3.455032	val: 5.684810	test: 5.414214

Epoch: 10
Loss: 12.24644422531128
RMSE train: 3.816377	val: 7.110608	test: 6.289791
MAE train: 3.418638	val: 5.714169	test: 5.462050

Epoch: 11
Loss: 11.53548526763916
RMSE train: 3.764144	val: 7.030987	test: 6.279672
MAE train: 3.403913	val: 5.772292	test: 5.473159

Epoch: 12
Loss: 10.59054708480835
RMSE train: 3.758606	val: 7.084476	test: 6.331821
MAE train: 3.441403	val: 5.912744	test: 5.537698

Epoch: 13
Loss: 10.298802375793457
RMSE train: 3.778790	val: 7.190762	test: 6.364023
MAE train: 3.471531	val: 6.018488	test: 5.574254

Epoch: 14
Loss: 9.865754127502441
RMSE train: 3.757212	val: 7.206521	test: 6.363057
MAE train: 3.447724	val: 6.037216	test: 5.577882

Epoch: 15
Loss: 9.52345323562622
RMSE train: 3.676827	val: 7.071662	test: 6.266977
MAE train: 3.368788	val: 5.901110	test: 5.482967

Epoch: 16
Loss: 8.849666118621826
RMSE train: 3.612396	val: 6.928976	test: 6.156121
MAE train: 3.297607	val: 5.755476	test: 5.379133

Epoch: 17
Loss: 8.721490859985352
RMSE train: 3.592603	val: 6.807781	test: 6.083108
MAE train: 3.273677	val: 5.672152	test: 5.323439

Epoch: 18
Loss: 8.137481212615967
RMSE train: 3.584534	val: 6.741722	test: 6.021141
MAE train: 3.264384	val: 5.659578	test: 5.274295

Epoch: 19
Loss: 7.990774154663086
RMSE train: 3.555381	val: 6.677890	test: 5.908352
MAE train: 3.232856	val: 5.647047	test: 5.175025

Epoch: 20
Loss: 7.456716537475586
RMSE train: 3.480551	val: 6.624759	test: 5.798177
MAE train: 3.150974	val: 5.608375	test: 5.069387

Epoch: 21
Loss: 7.309140920639038
RMSE train: 3.341082	val: 6.567700	test: 5.676089
MAE train: 3.005546	val: 5.544033	test: 4.950755

Epoch: 22
Loss: 6.830262899398804
RMSE train: 3.188345	val: 6.476979	test: 5.531699
MAE train: 2.850566	val: 5.430214	test: 4.808133

Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/freesolv/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/freesolv/noise=0.05/freesolv_scaff_3_20-05_14-43-31  ]
[ Using Seed :  3  ]
[ Using device :  cuda:1  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 21.297893524169922
RMSE train: 4.452422	val: 8.596779	test: 6.962369
MAE train: 3.537826	val: 6.738653	test: 6.118930

Epoch: 2
Loss: 20.248400688171387
RMSE train: 4.479566	val: 8.600144	test: 7.048393
MAE train: 3.596148	val: 6.730073	test: 6.209306

Epoch: 3
Loss: 19.458524703979492
RMSE train: 4.460013	val: 8.594280	test: 7.124934
MAE train: 3.609905	val: 6.739489	test: 6.294375

Epoch: 4
Loss: 18.539693355560303
RMSE train: 4.421379	val: 8.553663	test: 7.163407
MAE train: 3.601609	val: 6.743260	test: 6.340516

Epoch: 5
Loss: 18.03322982788086
RMSE train: 4.337486	val: 8.448023	test: 7.133977
MAE train: 3.551714	val: 6.679062	test: 6.311197

Epoch: 6
Loss: 16.65170383453369
RMSE train: 4.248503	val: 8.332221	test: 7.121672
MAE train: 3.501445	val: 6.645094	test: 6.298920

Epoch: 7
Loss: 15.807372093200684
RMSE train: 4.130295	val: 8.188095	test: 7.069432
MAE train: 3.426739	val: 6.597518	test: 6.255114

Epoch: 8
Loss: 14.666613578796387
RMSE train: 4.003573	val: 8.033030	test: 7.006319
MAE train: 3.348407	val: 6.536226	test: 6.203087

Epoch: 9
Loss: 13.804132461547852
RMSE train: 3.833713	val: 7.830341	test: 6.881926
MAE train: 3.231676	val: 6.429802	test: 6.097057

Epoch: 10
Loss: 13.258354187011719
RMSE train: 3.688483	val: 7.645344	test: 6.772164
MAE train: 3.134533	val: 6.336212	test: 6.002476

Epoch: 11
Loss: 12.44785451889038
RMSE train: 3.547151	val: 7.453914	test: 6.631208
MAE train: 3.043786	val: 6.221581	test: 5.880601

Epoch: 12
Loss: 11.663392066955566
RMSE train: 3.410889	val: 7.252545	test: 6.475599
MAE train: 2.949384	val: 6.075209	test: 5.742138

Epoch: 13
Loss: 10.920070171356201
RMSE train: 3.352448	val: 7.176876	test: 6.402512
MAE train: 2.917751	val: 6.043164	test: 5.690812

Epoch: 14
Loss: 10.419398784637451
RMSE train: 3.347937	val: 7.188323	test: 6.404138
MAE train: 2.930744	val: 6.083477	test: 5.702332

Epoch: 15
Loss: 9.85533332824707
RMSE train: 3.386952	val: 7.248083	test: 6.445532
MAE train: 2.981034	val: 6.155735	test: 5.738930

Epoch: 16
Loss: 9.446541786193848
RMSE train: 3.433730	val: 7.330701	test: 6.511965
MAE train: 3.037931	val: 6.260131	test: 5.785402

Epoch: 17
Loss: 9.299322605133057
RMSE train: 3.465170	val: 7.392453	test: 6.546129
MAE train: 3.080550	val: 6.342151	test: 5.807699

Epoch: 18
Loss: 8.490811824798584
RMSE train: 3.467421	val: 7.417700	test: 6.529382
MAE train: 3.093795	val: 6.367793	test: 5.788580

Epoch: 19
Loss: 8.055531978607178
RMSE train: 3.451717	val: 7.416347	test: 6.474028
MAE train: 3.087712	val: 6.347825	test: 5.741243

Epoch: 20
Loss: 7.90014910697937
RMSE train: 3.409809	val: 7.394353	test: 6.410083
MAE train: 3.054014	val: 6.318196	test: 5.688173

Epoch: 21
Loss: 7.38614296913147
RMSE train: 3.323300	val: 7.270694	test: 6.322590
MAE train: 2.979103	val: 6.194219	test: 5.606743

Epoch: 22
Loss: 6.839505434036255Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/freesolv/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/freesolv/noise=0.05/freesolv_scaff_2_20-05_14-43-31  ]
[ Using Seed :  2  ]
[ Using device :  cuda:1  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 21.020023345947266
RMSE train: 4.925691	val: 8.783940	test: 6.914204
MAE train: 3.969106	val: 6.702086	test: 6.019183

Epoch: 2
Loss: 20.148673057556152
RMSE train: 4.839749	val: 8.728544	test: 7.034717
MAE train: 3.908056	val: 6.725830	test: 6.171210

Epoch: 3
Loss: 19.377647399902344
RMSE train: 4.746805	val: 8.623888	test: 7.012490
MAE train: 3.839825	val: 6.707981	test: 6.171066

Epoch: 4
Loss: 18.71810245513916
RMSE train: 4.680362	val: 8.492717	test: 6.899115
MAE train: 3.793288	val: 6.631770	test: 6.083703

Epoch: 5
Loss: 17.842711448669434
RMSE train: 4.654601	val: 8.378051	test: 6.792042
MAE train: 3.787063	val: 6.552293	test: 5.998062

Epoch: 6
Loss: 17.33248996734619
RMSE train: 4.578051	val: 8.270041	test: 6.684725
MAE train: 3.732673	val: 6.475082	test: 5.908715

Epoch: 7
Loss: 16.085999965667725
RMSE train: 4.469500	val: 8.163007	test: 6.595847
MAE train: 3.654443	val: 6.408707	test: 5.830589

Epoch: 8
Loss: 15.151781558990479
RMSE train: 4.292087	val: 8.009635	test: 6.529530
MAE train: 3.535881	val: 6.337359	test: 5.773953

Epoch: 9
Loss: 14.293578624725342
RMSE train: 4.056382	val: 7.725523	test: 6.419856
MAE train: 3.365461	val: 6.160796	test: 5.671705

Epoch: 10
Loss: 13.449064254760742
RMSE train: 3.824666	val: 7.420451	test: 6.348233
MAE train: 3.194878	val: 5.999704	test: 5.606705

Epoch: 11
Loss: 12.299307346343994
RMSE train: 3.646445	val: 7.080787	test: 6.266890
MAE train: 3.067439	val: 5.785385	test: 5.521488

Epoch: 12
Loss: 12.04924726486206
RMSE train: 3.477607	val: 6.728068	test: 6.147539
MAE train: 2.944639	val: 5.550564	test: 5.390256

Epoch: 13
Loss: 11.192753791809082
RMSE train: 3.279377	val: 6.394403	test: 6.009203
MAE train: 2.772135	val: 5.331950	test: 5.228431

Epoch: 14
Loss: 10.40308427810669
RMSE train: 3.104354	val: 6.138041	test: 5.882151
MAE train: 2.610013	val: 5.135071	test: 5.111359

Epoch: 15
Loss: 9.74616289138794
RMSE train: 2.969248	val: 6.009506	test: 5.793281
MAE train: 2.492146	val: 5.067221	test: 5.048812

Epoch: 16
Loss: 9.224449634552002
RMSE train: 2.904190	val: 6.059216	test: 5.762930
MAE train: 2.450137	val: 5.143247	test: 5.047385

Epoch: 17
Loss: 8.798232555389404
RMSE train: 2.879061	val: 6.147777	test: 5.750491
MAE train: 2.431543	val: 5.241735	test: 5.054451

Epoch: 18
Loss: 8.660455703735352
RMSE train: 2.888981	val: 6.278926	test: 5.765595
MAE train: 2.438154	val: 5.370352	test: 5.090503

Epoch: 19
Loss: 7.871805667877197
RMSE train: 2.898683	val: 6.343007	test: 5.780441
MAE train: 2.455702	val: 5.441274	test: 5.103248

Epoch: 20
Loss: 7.830084562301636
RMSE train: 2.854330	val: 6.296375	test: 5.717616
MAE train: 2.426513	val: 5.373304	test: 5.033579

Epoch: 21
Loss: 7.537759780883789
RMSE train: 2.790771	val: 6.199005	test: 5.606030
MAE train: 2.375497	val: 5.251230	test: 4.923018

Epoch: 22
Loss: 7.319202661514282Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:3
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/freesolv/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/freesolv/noise=0.2/freesolv_scaff_1_20-05_14-43-31  ]
[ Using Seed :  1  ]
[ Using device :  cuda:3  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 21.289719581604004
RMSE train: 5.049465	val: 8.628735	test: 7.002695
MAE train: 4.097619	val: 6.751172	test: 6.181839

Epoch: 2
Loss: 20.602131843566895
RMSE train: 4.783663	val: 8.514525	test: 6.836391
MAE train: 3.838919	val: 6.612809	test: 5.990925

Epoch: 3
Loss: 20.073737144470215
RMSE train: 4.620924	val: 8.435938	test: 6.737228
MAE train: 3.691107	val: 6.532164	test: 5.869092

Epoch: 4
Loss: 19.346750259399414
RMSE train: 4.514965	val: 8.410723	test: 6.698421
MAE train: 3.598693	val: 6.559675	test: 5.833468

Epoch: 5
Loss: 18.73130512237549
RMSE train: 4.439365	val: 8.400856	test: 6.683154
MAE train: 3.541082	val: 6.619144	test: 5.836013

Epoch: 6
Loss: 18.087414741516113
RMSE train: 4.387536	val: 8.417604	test: 6.699024
MAE train: 3.508084	val: 6.723097	test: 5.873146

Epoch: 7
Loss: 17.355748176574707
RMSE train: 4.347391	val: 8.474863	test: 6.766941
MAE train: 3.484659	val: 6.880500	test: 5.964694

Epoch: 8
Loss: 16.667036056518555
RMSE train: 4.276149	val: 8.511262	test: 6.784015
MAE train: 3.425323	val: 6.996057	test: 5.998777

Epoch: 9
Loss: 16.133413314819336
RMSE train: 4.189105	val: 8.467464	test: 6.705117
MAE train: 3.353281	val: 7.009861	test: 5.927424

Epoch: 10
Loss: 15.630513191223145
RMSE train: 4.065928	val: 8.377203	test: 6.550665
MAE train: 3.246287	val: 6.983611	test: 5.764681

Epoch: 11
Loss: 15.038083553314209
RMSE train: 3.911633	val: 8.230632	test: 6.331410
MAE train: 3.114303	val: 6.900328	test: 5.515626

Epoch: 12
Loss: 14.062932014465332
RMSE train: 3.758200	val: 8.048783	test: 6.137791
MAE train: 2.993396	val: 6.779145	test: 5.293861

Epoch: 13
Loss: 13.705884456634521
RMSE train: 3.626581	val: 7.861044	test: 5.985319
MAE train: 2.899906	val: 6.643008	test: 5.131559

Epoch: 14
Loss: 12.726912498474121
RMSE train: 3.510907	val: 7.678050	test: 5.839462
MAE train: 2.817349	val: 6.491881	test: 4.976615

Epoch: 15
Loss: 12.62905502319336
RMSE train: 3.438976	val: 7.466510	test: 5.678434
MAE train: 2.769783	val: 6.275082	test: 4.811022

Epoch: 16
Loss: 11.238497257232666
RMSE train: 3.390883	val: 7.263703	test: 5.535529
MAE train: 2.750892	val: 6.063817	test: 4.666598

Epoch: 17
Loss: 11.2308988571167
RMSE train: 3.336747	val: 7.087593	test: 5.398539
MAE train: 2.721724	val: 5.868282	test: 4.540014

Epoch: 18
Loss: 10.981114864349365
RMSE train: 3.309287	val: 6.987669	test: 5.339491
MAE train: 2.718162	val: 5.740027	test: 4.492992

Epoch: 19
Loss: 10.064945697784424
RMSE train: 3.263827	val: 6.913124	test: 5.299815
MAE train: 2.693295	val: 5.651846	test: 4.462341

Epoch: 20
Loss: 9.53042459487915
RMSE train: 3.238913	val: 6.896384	test: 5.296896
MAE train: 2.683779	val: 5.618964	test: 4.463467

Epoch: 21
Loss: 9.074073791503906
RMSE train: 3.212177	val: 6.881402	test: 5.301051
MAE train: 2.672960	val: 5.624584	test: 4.478939

Epoch: 22
Loss: 8.43446159362793Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/freesolv/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/freesolv/noise=0.1/freesolv_scaff_1_20-05_14-43-31  ]
[ Using Seed :  1  ]
[ Using device :  cuda:2  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 21.214405059814453
RMSE train: 4.664989	val: 8.607385	test: 7.041627
MAE train: 3.703754	val: 6.717694	test: 6.222011

Epoch: 2
Loss: 20.47886562347412
RMSE train: 4.620908	val: 8.615478	test: 7.174513
MAE train: 3.689027	val: 6.778286	test: 6.346519

Epoch: 3
Loss: 19.624948501586914
RMSE train: 4.577809	val: 8.612826	test: 7.230768
MAE train: 3.673622	val: 6.814368	test: 6.403624

Epoch: 4
Loss: 18.983494758605957
RMSE train: 4.543467	val: 8.633241	test: 7.330984
MAE train: 3.665963	val: 6.893438	test: 6.501494

Epoch: 5
Loss: 18.19486141204834
RMSE train: 4.546257	val: 8.706240	test: 7.528089
MAE train: 3.692383	val: 7.047771	test: 6.691157

Epoch: 6
Loss: 17.408555030822754
RMSE train: 4.505246	val: 8.779918	test: 7.724435
MAE train: 3.669976	val: 7.194870	test: 6.873152

Epoch: 7
Loss: 16.61521863937378
RMSE train: 4.455126	val: 8.824100	test: 7.887881
MAE train: 3.639473	val: 7.301600	test: 7.017480

Epoch: 8
Loss: 16.12376308441162
RMSE train: 4.362632	val: 8.776783	test: 7.901352
MAE train: 3.566968	val: 7.306320	test: 7.024682

Epoch: 9
Loss: 15.370500564575195
RMSE train: 4.218734	val: 8.630632	test: 7.770721
MAE train: 3.447428	val: 7.203198	test: 6.905280

Epoch: 10
Loss: 14.470657348632812
RMSE train: 4.022526	val: 8.377828	test: 7.495054
MAE train: 3.279779	val: 7.000181	test: 6.673818

Epoch: 11
Loss: 13.893277168273926
RMSE train: 3.813974	val: 8.063560	test: 7.146411
MAE train: 3.107473	val: 6.748261	test: 6.385215

Epoch: 12
Loss: 12.848775386810303
RMSE train: 3.611843	val: 7.746349	test: 6.820171
MAE train: 2.943820	val: 6.505893	test: 6.111730

Epoch: 13
Loss: 12.669551372528076
RMSE train: 3.443352	val: 7.483124	test: 6.575985
MAE train: 2.816005	val: 6.308054	test: 5.898952

Epoch: 14
Loss: 12.041625499725342
RMSE train: 3.290503	val: 7.278563	test: 6.332379
MAE train: 2.697988	val: 6.123125	test: 5.669258

Epoch: 15
Loss: 11.017484188079834
RMSE train: 3.167132	val: 7.086447	test: 6.130348
MAE train: 2.609068	val: 5.950242	test: 5.470486

Epoch: 16
Loss: 10.519696712493896
RMSE train: 3.095097	val: 6.952744	test: 6.018657
MAE train: 2.564487	val: 5.841180	test: 5.362482

Epoch: 17
Loss: 10.007068157196045
RMSE train: 3.068693	val: 6.929123	test: 6.023088
MAE train: 2.560997	val: 5.830258	test: 5.396292

Epoch: 18
Loss: 9.787378311157227
RMSE train: 3.042722	val: 6.950617	test: 6.091556
MAE train: 2.556459	val: 5.865849	test: 5.485823

Epoch: 19
Loss: 8.985485553741455
RMSE train: 3.031231	val: 6.995249	test: 6.177383
MAE train: 2.562177	val: 5.924688	test: 5.584896

Epoch: 20
Loss: 8.309634685516357
RMSE train: 3.016787	val: 7.038583	test: 6.215925
MAE train: 2.560564	val: 5.962174	test: 5.626161

Epoch: 21
Loss: 8.018928050994873
RMSE train: 2.992425	val: 7.033209	test: 6.236293
MAE train: 2.549073	val: 5.962320	test: 5.644603

Epoch: 22
Loss: 7.387101888656616Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/freesolv/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/freesolv/noise=0.05/freesolv_scaff_1_20-05_14-43-31  ]
[ Using Seed :  1  ]
[ Using device :  cuda:1  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 20.995012283325195
RMSE train: 4.523109	val: 8.675127	test: 7.162847
MAE train: 3.583800	val: 6.817422	test: 6.362243

Epoch: 2
Loss: 20.273797035217285
RMSE train: 4.694637	val: 8.838932	test: 7.528976
MAE train: 3.787863	val: 7.129157	test: 6.717236

Epoch: 3
Loss: 19.258443355560303
RMSE train: 4.860423	val: 8.991495	test: 7.816468
MAE train: 3.991801	val: 7.382342	test: 6.983506

Epoch: 4
Loss: 18.721341133117676
RMSE train: 4.889612	val: 9.074892	test: 7.943160
MAE train: 4.045216	val: 7.520506	test: 7.089450

Epoch: 5
Loss: 17.812329292297363
RMSE train: 4.823829	val: 9.049945	test: 7.898312
MAE train: 3.995839	val: 7.516588	test: 7.040119

Epoch: 6
Loss: 16.962928771972656
RMSE train: 4.710962	val: 9.024599	test: 7.825088
MAE train: 3.893969	val: 7.510636	test: 6.968790

Epoch: 7
Loss: 16.25430154800415
RMSE train: 4.535604	val: 8.894670	test: 7.633016
MAE train: 3.736120	val: 7.387775	test: 6.798703

Epoch: 8
Loss: 15.67322826385498
RMSE train: 4.320353	val: 8.705275	test: 7.333618
MAE train: 3.553795	val: 7.199879	test: 6.535717

Epoch: 9
Loss: 14.817085266113281
RMSE train: 4.089292	val: 8.488402	test: 7.001620
MAE train: 3.350987	val: 6.977331	test: 6.240490

Epoch: 10
Loss: 13.929692268371582
RMSE train: 3.862580	val: 8.235884	test: 6.671148
MAE train: 3.155684	val: 6.715806	test: 5.940527

Epoch: 11
Loss: 13.164309024810791
RMSE train: 3.662839	val: 7.979742	test: 6.395471
MAE train: 2.999521	val: 6.470734	test: 5.690004

Epoch: 12
Loss: 12.169740676879883
RMSE train: 3.485695	val: 7.717679	test: 6.162318
MAE train: 2.868141	val: 6.251981	test: 5.468276

Epoch: 13
Loss: 11.91501235961914
RMSE train: 3.323495	val: 7.410891	test: 5.961139
MAE train: 2.741745	val: 6.000745	test: 5.258385

Epoch: 14
Loss: 11.002359390258789
RMSE train: 3.208948	val: 7.196327	test: 5.856105
MAE train: 2.655134	val: 5.835535	test: 5.138335

Epoch: 15
Loss: 10.541114807128906
RMSE train: 3.143397	val: 7.022458	test: 5.781312
MAE train: 2.607823	val: 5.683086	test: 5.051749

Epoch: 16
Loss: 10.127213478088379
RMSE train: 3.113384	val: 6.925824	test: 5.785711
MAE train: 2.598685	val: 5.633638	test: 5.054796

Epoch: 17
Loss: 9.65981149673462
RMSE train: 3.084559	val: 6.891939	test: 5.847370
MAE train: 2.589870	val: 5.661892	test: 5.133358

Epoch: 18
Loss: 8.901440620422363
RMSE train: 3.040156	val: 6.857897	test: 5.907648
MAE train: 2.561370	val: 5.707371	test: 5.200995

Epoch: 19
Loss: 8.462462663650513
RMSE train: 2.980173	val: 6.811617	test: 5.918303
MAE train: 2.512560	val: 5.717769	test: 5.224780

Epoch: 20
Loss: 7.587985038757324
RMSE train: 2.919867	val: 6.782649	test: 5.920112
MAE train: 2.469223	val: 5.727785	test: 5.241000

Epoch: 21
Loss: 7.407899618148804
RMSE train: 2.830087	val: 6.702355	test: 5.890408
MAE train: 2.397857	val: 5.679004	test: 5.226237

Epoch: 22
Loss: 6.795156478881836Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:3
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/freesolv/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/freesolv/noise=0.2/freesolv_scaff_3_20-05_14-43-31  ]
[ Using Seed :  3  ]
[ Using device :  cuda:3  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 21.405524253845215
RMSE train: 4.484316	val: 8.567388	test: 6.854296
MAE train: 3.562704	val: 6.708813	test: 5.993990

Epoch: 2
Loss: 20.59996795654297
RMSE train: 4.420685	val: 8.551872	test: 6.899101
MAE train: 3.511614	val: 6.660603	test: 6.049754

Epoch: 3
Loss: 20.03480339050293
RMSE train: 4.374870	val: 8.498398	test: 6.894025
MAE train: 3.479719	val: 6.583517	test: 6.044581

Epoch: 4
Loss: 19.21858310699463
RMSE train: 4.329553	val: 8.420989	test: 6.867242
MAE train: 3.448324	val: 6.505756	test: 6.015969

Epoch: 5
Loss: 18.55847454071045
RMSE train: 4.293004	val: 8.365943	test: 6.872293
MAE train: 3.429279	val: 6.458347	test: 6.019059

Epoch: 6
Loss: 17.800771713256836
RMSE train: 4.226057	val: 8.309195	test: 6.871666
MAE train: 3.389742	val: 6.400082	test: 6.020095

Epoch: 7
Loss: 17.053353309631348
RMSE train: 4.150739	val: 8.258640	test: 6.855896
MAE train: 3.348717	val: 6.346360	test: 6.010204

Epoch: 8
Loss: 16.23777961730957
RMSE train: 4.076199	val: 8.223076	test: 6.864579
MAE train: 3.308349	val: 6.303162	test: 6.029944

Epoch: 9
Loss: 15.406495094299316
RMSE train: 3.977799	val: 8.171493	test: 6.867865
MAE train: 3.247989	val: 6.263960	test: 6.044192

Epoch: 10
Loss: 14.617371082305908
RMSE train: 3.858855	val: 8.098800	test: 6.837484
MAE train: 3.161574	val: 6.208285	test: 6.022936

Epoch: 11
Loss: 14.395014762878418
RMSE train: 3.770202	val: 8.042742	test: 6.820657
MAE train: 3.107060	val: 6.162033	test: 6.011707

Epoch: 12
Loss: 13.543846130371094
RMSE train: 3.686488	val: 7.963899	test: 6.779682
MAE train: 3.051457	val: 6.090643	test: 5.980851

Epoch: 13
Loss: 12.858925342559814
RMSE train: 3.649242	val: 7.892376	test: 6.743381
MAE train: 3.031849	val: 6.027076	test: 5.950409

Epoch: 14
Loss: 12.196661949157715
RMSE train: 3.640027	val: 7.839360	test: 6.729976
MAE train: 3.035430	val: 5.984042	test: 5.939668

Epoch: 15
Loss: 11.517499446868896
RMSE train: 3.654677	val: 7.778850	test: 6.686239
MAE train: 3.069042	val: 5.951581	test: 5.908364

Epoch: 16
Loss: 11.08988332748413
RMSE train: 3.670612	val: 7.777513	test: 6.655365
MAE train: 3.106216	val: 5.991303	test: 5.892523

Epoch: 17
Loss: 10.50419569015503
RMSE train: 3.710165	val: 7.815461	test: 6.638273
MAE train: 3.167038	val: 6.062492	test: 5.896735

Epoch: 18
Loss: 10.383537292480469
RMSE train: 3.741487	val: 7.840543	test: 6.606829
MAE train: 3.217829	val: 6.111452	test: 5.877878

Epoch: 19
Loss: 9.651639938354492
RMSE train: 3.738208	val: 7.831879	test: 6.548785
MAE train: 3.227744	val: 6.126612	test: 5.828775

Epoch: 20
Loss: 9.442208290100098
RMSE train: 3.688446	val: 7.786169	test: 6.462406
MAE train: 3.191514	val: 6.090940	test: 5.745420

Epoch: 21
Loss: 8.60230302810669
RMSE train: 3.598852	val: 7.678346	test: 6.333977
MAE train: 3.118115	val: 5.986151	test: 5.615273

Epoch: 22
Loss: 8.036169290542603Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/freesolv/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/freesolv/noise=0.1/freesolv_scaff_3_20-05_14-43-31  ]
[ Using Seed :  3  ]
[ Using device :  cuda:2  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 21.212594032287598
RMSE train: 4.443877	val: 8.595470	test: 6.979566
MAE train: 3.532824	val: 6.737885	test: 6.136210

Epoch: 2
Loss: 20.423669815063477
RMSE train: 4.418263	val: 8.585438	test: 7.046888
MAE train: 3.535327	val: 6.714132	test: 6.206905

Epoch: 3
Loss: 19.711679458618164
RMSE train: 4.347559	val: 8.529650	test: 7.069130
MAE train: 3.489359	val: 6.659221	test: 6.223624

Epoch: 4
Loss: 18.75099754333496
RMSE train: 4.249216	val: 8.427993	test: 7.025251
MAE train: 3.415140	val: 6.570135	test: 6.174529

Epoch: 5
Loss: 18.234627723693848
RMSE train: 4.134377	val: 8.311753	test: 6.961888
MAE train: 3.333092	val: 6.478534	test: 6.114995

Epoch: 6
Loss: 16.966858386993408
RMSE train: 4.011229	val: 8.193895	test: 6.903894
MAE train: 3.248919	val: 6.403844	test: 6.071947

Epoch: 7
Loss: 16.333415985107422
RMSE train: 3.877707	val: 8.076202	test: 6.847662
MAE train: 3.161754	val: 6.338534	test: 6.036639

Epoch: 8
Loss: 15.193880081176758
RMSE train: 3.754663	val: 7.963296	test: 6.805422
MAE train: 3.085774	val: 6.276222	test: 6.015901

Epoch: 9
Loss: 14.32397985458374
RMSE train: 3.624887	val: 7.853923	test: 6.773441
MAE train: 3.011371	val: 6.240240	test: 6.006278

Epoch: 10
Loss: 13.314292907714844
RMSE train: 3.506491	val: 7.744771	test: 6.743040
MAE train: 2.947183	val: 6.210288	test: 6.002291

Epoch: 11
Loss: 12.717309951782227
RMSE train: 3.386837	val: 7.615165	test: 6.671214
MAE train: 2.870973	val: 6.169380	test: 5.958549

Epoch: 12
Loss: 12.080852508544922
RMSE train: 3.266490	val: 7.452553	test: 6.538894
MAE train: 2.779301	val: 6.093063	test: 5.855510

Epoch: 13
Loss: 11.574265003204346
RMSE train: 3.220225	val: 7.358062	test: 6.427032
MAE train: 2.752597	val: 6.043924	test: 5.765699

Epoch: 14
Loss: 10.783586502075195
RMSE train: 3.207500	val: 7.295293	test: 6.324514
MAE train: 2.755778	val: 6.008749	test: 5.676890

Epoch: 15
Loss: 10.510091781616211
RMSE train: 3.211791	val: 7.261050	test: 6.256024
MAE train: 2.770295	val: 5.991749	test: 5.610585

Epoch: 16
Loss: 9.858538627624512
RMSE train: 3.230738	val: 7.238641	test: 6.227454
MAE train: 2.794851	val: 5.983200	test: 5.576661

Epoch: 17
Loss: 9.50112771987915
RMSE train: 3.290191	val: 7.263040	test: 6.251281
MAE train: 2.860871	val: 6.011636	test: 5.597139

Epoch: 18
Loss: 9.033262014389038
RMSE train: 3.321804	val: 7.242605	test: 6.250511
MAE train: 2.904020	val: 5.994544	test: 5.595510

Epoch: 19
Loss: 8.435835599899292
RMSE train: 3.354116	val: 7.262982	test: 6.279065
MAE train: 2.947682	val: 6.024233	test: 5.633143

Epoch: 20
Loss: 8.206389665603638
RMSE train: 3.354260	val: 7.263400	test: 6.310261
MAE train: 2.959900	val: 6.054699	test: 5.676258

Epoch: 21
Loss: 7.647081136703491
RMSE train: 3.272144	val: 7.136067	test: 6.248270
MAE train: 2.890056	val: 5.947584	test: 5.612529

Epoch: 22
Loss: 6.9535813331604Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/freesolv/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/freesolv/noise=0.1/freesolv_scaff_2_20-05_14-43-31  ]
[ Using Seed :  2  ]
[ Using device :  cuda:2  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 20.973377227783203
RMSE train: 4.920007	val: 8.768284	test: 6.908113
MAE train: 3.956583	val: 6.689071	test: 6.021240

Epoch: 2
Loss: 20.28514575958252
RMSE train: 4.795281	val: 8.778040	test: 7.152168
MAE train: 3.851003	val: 6.875054	test: 6.309912

Epoch: 3
Loss: 19.568778038024902
RMSE train: 4.692223	val: 8.787414	test: 7.292979
MAE train: 3.770790	val: 7.013066	test: 6.462653

Epoch: 4
Loss: 18.791537284851074
RMSE train: 4.621212	val: 8.762024	test: 7.314747
MAE train: 3.724175	val: 7.042490	test: 6.492234

Epoch: 5
Loss: 18.052863121032715
RMSE train: 4.590982	val: 8.754353	test: 7.338516
MAE train: 3.718778	val: 7.083044	test: 6.518952

Epoch: 6
Loss: 17.505298137664795
RMSE train: 4.504936	val: 8.706944	test: 7.314593
MAE train: 3.656116	val: 7.063532	test: 6.492657

Epoch: 7
Loss: 16.525032997131348
RMSE train: 4.373894	val: 8.623548	test: 7.256592
MAE train: 3.552990	val: 6.993912	test: 6.431950

Epoch: 8
Loss: 15.791333198547363
RMSE train: 4.195036	val: 8.479320	test: 7.139883
MAE train: 3.405659	val: 6.852592	test: 6.327259

Epoch: 9
Loss: 14.936830520629883
RMSE train: 3.997969	val: 8.255840	test: 6.968499
MAE train: 3.245402	val: 6.621103	test: 6.173770

Epoch: 10
Loss: 13.992568969726562
RMSE train: 3.793807	val: 8.018118	test: 6.803744
MAE train: 3.077362	val: 6.382673	test: 6.021850

Epoch: 11
Loss: 13.077118873596191
RMSE train: 3.617538	val: 7.781695	test: 6.685651
MAE train: 2.946704	val: 6.183622	test: 5.910532

Epoch: 12
Loss: 12.487022876739502
RMSE train: 3.477743	val: 7.528589	test: 6.561274
MAE train: 2.846836	val: 5.985741	test: 5.791682

Epoch: 13
Loss: 12.135951519012451
RMSE train: 3.360267	val: 7.290300	test: 6.421507
MAE train: 2.759437	val: 5.782556	test: 5.655553

Epoch: 14
Loss: 11.319926738739014
RMSE train: 3.291984	val: 7.106558	test: 6.301452
MAE train: 2.706320	val: 5.614626	test: 5.523969

Epoch: 15
Loss: 10.777326107025146
RMSE train: 3.211994	val: 6.895011	test: 6.169261
MAE train: 2.640880	val: 5.424818	test: 5.374617

Epoch: 16
Loss: 10.16153335571289
RMSE train: 3.163312	val: 6.798251	test: 6.109143
MAE train: 2.611956	val: 5.347147	test: 5.326656

Epoch: 17
Loss: 9.734407424926758
RMSE train: 3.125665	val: 6.755205	test: 6.107159
MAE train: 2.594739	val: 5.337308	test: 5.335659

Epoch: 18
Loss: 9.233857154846191
RMSE train: 3.106525	val: 6.751646	test: 6.128139
MAE train: 2.589265	val: 5.360486	test: 5.367497

Epoch: 19
Loss: 8.791850090026855
RMSE train: 3.073653	val: 6.697017	test: 6.105489
MAE train: 2.566749	val: 5.338942	test: 5.349949

Epoch: 20
Loss: 8.528978824615479
RMSE train: 2.990527	val: 6.580626	test: 6.045125
MAE train: 2.497553	val: 5.272067	test: 5.288929

Epoch: 21
Loss: 8.404254913330078
RMSE train: 2.891010	val: 6.464659	test: 5.938174
MAE train: 2.411096	val: 5.164598	test: 5.175889

Epoch: 22
Loss: 7.929499387741089Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:3
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/freesolv/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/freesolv/noise=0.2/freesolv_scaff_2_20-05_14-43-31  ]
[ Using Seed :  2  ]
[ Using device :  cuda:3  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 21.028413772583008
RMSE train: 4.877118	val: 8.762716	test: 6.867669
MAE train: 3.928755	val: 6.673820	test: 5.963250

Epoch: 2
Loss: 20.332855224609375
RMSE train: 4.696922	val: 8.734304	test: 7.057200
MAE train: 3.762007	val: 6.752606	test: 6.194058

Epoch: 3
Loss: 19.58442497253418
RMSE train: 4.560377	val: 8.697473	test: 7.155422
MAE train: 3.641792	val: 6.816461	test: 6.313194

Epoch: 4
Loss: 18.990970611572266
RMSE train: 4.499791	val: 8.670537	test: 7.191364
MAE train: 3.593921	val: 6.845795	test: 6.359650

Epoch: 5
Loss: 18.527939796447754
RMSE train: 4.474808	val: 8.623200	test: 7.174076
MAE train: 3.578049	val: 6.826277	test: 6.352349

Epoch: 6
Loss: 17.79666233062744
RMSE train: 4.448831	val: 8.573388	test: 7.127649
MAE train: 3.565697	val: 6.803924	test: 6.316007

Epoch: 7
Loss: 17.194272994995117
RMSE train: 4.415909	val: 8.542455	test: 7.076734
MAE train: 3.551660	val: 6.816627	test: 6.277565

Epoch: 8
Loss: 16.54643154144287
RMSE train: 4.360540	val: 8.457918	test: 6.963988
MAE train: 3.519173	val: 6.753064	test: 6.184019

Epoch: 9
Loss: 15.867204666137695
RMSE train: 4.259922	val: 8.330713	test: 6.799422
MAE train: 3.450407	val: 6.634771	test: 6.045815

Epoch: 10
Loss: 14.773501873016357
RMSE train: 4.132831	val: 8.186614	test: 6.615099
MAE train: 3.359151	val: 6.504041	test: 5.885889

Epoch: 11
Loss: 13.94037389755249
RMSE train: 4.015223	val: 8.045884	test: 6.471086
MAE train: 3.279369	val: 6.392442	test: 5.766669

Epoch: 12
Loss: 13.653027057647705
RMSE train: 3.909650	val: 7.893419	test: 6.342222
MAE train: 3.209660	val: 6.287602	test: 5.659626

Epoch: 13
Loss: 13.22947645187378
RMSE train: 3.810261	val: 7.728574	test: 6.229087
MAE train: 3.143207	val: 6.169577	test: 5.561446

Epoch: 14
Loss: 12.059003353118896
RMSE train: 3.711510	val: 7.572918	test: 6.151466
MAE train: 3.067056	val: 6.045609	test: 5.491496

Epoch: 15
Loss: 11.479348182678223
RMSE train: 3.643999	val: 7.448362	test: 6.104472
MAE train: 3.017625	val: 5.961563	test: 5.447435

Epoch: 16
Loss: 11.071909427642822
RMSE train: 3.569461	val: 7.309813	test: 6.042097
MAE train: 2.960224	val: 5.861119	test: 5.391238

Epoch: 17
Loss: 10.621975421905518
RMSE train: 3.531904	val: 7.226768	test: 6.020988
MAE train: 2.939630	val: 5.804497	test: 5.377650

Epoch: 18
Loss: 10.212133407592773
RMSE train: 3.473302	val: 7.140181	test: 5.973433
MAE train: 2.897161	val: 5.744688	test: 5.338220

Epoch: 19
Loss: 9.505758285522461
RMSE train: 3.426099	val: 7.069342	test: 5.930927
MAE train: 2.866199	val: 5.697544	test: 5.299218

Epoch: 20
Loss: 9.16970682144165
RMSE train: 3.362549	val: 6.982095	test: 5.870701
MAE train: 2.819413	val: 5.612809	test: 5.236942

Epoch: 21
Loss: 8.962615013122559
RMSE train: 3.286010	val: 6.864821	test: 5.758760
MAE train: 2.760927	val: 5.508082	test: 5.125416

Epoch: 22
Loss: 8.19321608543396
Epoch: 23
Loss: 6.116406202316284
RMSE train: 2.925880	val: 6.086979	test: 5.182615
MAE train: 2.617189	val: 5.016422	test: 4.453702

Epoch: 24
Loss: 5.600789308547974
RMSE train: 2.844973	val: 6.069465	test: 5.150248
MAE train: 2.528157	val: 4.995035	test: 4.433438

Epoch: 25
Loss: 5.354019641876221
RMSE train: 2.704685	val: 5.965410	test: 5.063816
MAE train: 2.386246	val: 4.886824	test: 4.352408

Epoch: 26
Loss: 4.997469663619995
RMSE train: 2.614789	val: 5.864705	test: 4.995526
MAE train: 2.295643	val: 4.790727	test: 4.284161

Epoch: 27
Loss: 4.7270424365997314
RMSE train: 2.548785	val: 5.775283	test: 4.955604
MAE train: 2.234335	val: 4.706886	test: 4.249933

Epoch: 28
Loss: 4.569291830062866
RMSE train: 2.458532	val: 5.677520	test: 4.888540
MAE train: 2.147332	val: 4.594403	test: 4.180847

Epoch: 29
Loss: 3.987222909927368
RMSE train: 2.370416	val: 5.580493	test: 4.773271
MAE train: 2.061862	val: 4.522402	test: 4.066039

Epoch: 30
Loss: 4.014953136444092
RMSE train: 2.310820	val: 5.472327	test: 4.651433
MAE train: 2.009898	val: 4.418373	test: 3.937005

Epoch: 31
Loss: 3.7040544748306274
RMSE train: 2.250342	val: 5.340708	test: 4.513745
MAE train: 1.959485	val: 4.287260	test: 3.796534

Epoch: 32
Loss: 3.5191404819488525
RMSE train: 2.196405	val: 5.247872	test: 4.420575
MAE train: 1.904124	val: 4.178346	test: 3.716309

Epoch: 33
Loss: 3.30332350730896
RMSE train: 2.126833	val: 5.167317	test: 4.333177
MAE train: 1.825616	val: 4.083552	test: 3.631184

Epoch: 34
Loss: 3.1600476503372192
RMSE train: 2.017276	val: 5.056877	test: 4.203179
MAE train: 1.709265	val: 3.967347	test: 3.504629

Epoch: 35
Loss: 3.010154962539673
RMSE train: 1.995379	val: 5.061593	test: 4.226264
MAE train: 1.685901	val: 3.965838	test: 3.543504

Epoch: 36
Loss: 2.8079781532287598
RMSE train: 1.927043	val: 5.001813	test: 4.227570
MAE train: 1.627011	val: 3.899735	test: 3.551953

Epoch: 37
Loss: 2.669791579246521
RMSE train: 1.834477	val: 4.879472	test: 4.179022
MAE train: 1.543568	val: 3.766298	test: 3.495286

Epoch: 38
Loss: 2.563733756542206
RMSE train: 1.674031	val: 4.631841	test: 3.980639
MAE train: 1.391384	val: 3.500733	test: 3.290769

Epoch: 39
Loss: 2.324060082435608
RMSE train: 1.492517	val: 4.425787	test: 3.791275
MAE train: 1.223279	val: 3.280929	test: 3.098421

Epoch: 40
Loss: 2.182354152202606
RMSE train: 1.324463	val: 4.286734	test: 3.629432
MAE train: 1.064693	val: 3.105221	test: 2.912439

Epoch: 41
Loss: 2.120360255241394
RMSE train: 1.257587	val: 4.271599	test: 3.581622
MAE train: 0.992462	val: 3.052684	test: 2.852551

Epoch: 42
Loss: 2.025696575641632
RMSE train: 1.269516	val: 4.342655	test: 3.624912
MAE train: 0.998620	val: 3.108160	test: 2.903213

Epoch: 43
Loss: 1.9278018474578857
RMSE train: 1.298481	val: 4.401307	test: 3.691975
MAE train: 1.038114	val: 3.209989	test: 2.972482

Epoch: 44
Loss: 1.766969084739685
RMSE train: 1.376784	val: 4.510305	test: 3.796734
MAE train: 1.107407	val: 3.354420	test: 3.073214

Epoch: 45
Loss: 1.7934784889221191
RMSE train: 1.418918	val: 4.571515	test: 3.817928
MAE train: 1.137053	val: 3.399668	test: 3.076717

Epoch: 46
Loss: 1.7046766877174377
RMSE train: 1.413035	val: 4.581523	test: 3.771333
MAE train: 1.121041	val: 3.360580	test: 3.004716

Epoch: 47
Loss: 1.77471262216568
RMSE train: 1.319770	val: 4.475666	test: 3.655249
MAE train: 1.029654	val: 3.209011	test: 2.858664

Epoch: 48
Loss: 1.554944097995758
RMSE train: 1.203570	val: 4.353217	test: 3.516289
MAE train: 0.921597	val: 3.071465	test: 2.721558

Epoch: 49
Loss: 1.4644599556922913
RMSE train: 1.096848	val: 4.257671	test: 3.421918
MAE train: 0.828626	val: 2.984012	test: 2.632220

Epoch: 50
Loss: 1.233338713645935
RMSE train: 1.028371	val: 4.163233	test: 3.341718
MAE train: 0.769415	val: 2.931940	test: 2.577080

Epoch: 51
Loss: 1.476953148841858
RMSE train: 0.995172	val: 4.127194	test: 3.284668
MAE train: 0.742006	val: 2.906314	test: 2.530224

Epoch: 52
Loss: 1.3326280117034912
RMSE train: 1.018211	val: 4.148042	test: 3.232411
MAE train: 0.759593	val: 2.938704	test: 2.473331

Epoch: 53
Loss: 1.3829621076583862
RMSE train: 0.976945	val: 4.103374	test: 3.206256
MAE train: 0.731256	val: 2.925091	test: 2.453452

Epoch: 54
Loss: 1.226849377155304
RMSE train: 0.932290	val: 4.055555	test: 3.222429
MAE train: 0.700176	val: 2.907754	test: 2.484550

Epoch: 55
Loss: 1.208075225353241
RMSE train: 0.915025	val: 4.022280	test: 3.272063
MAE train: 0.687024	val: 2.890606	test: 2.556442

Epoch: 56
Loss: 1.2654402256011963
RMSE train: 0.898180	val: 4.001341	test: 3.312101
MAE train: 0.674397	val: 2.860830	test: 2.608689

Epoch: 57
Loss: 1.1671530604362488
RMSE train: 0.898960	val: 4.034566	test: 3.354038
MAE train: 0.674746	val: 2.871729	test: 2.647134

Epoch: 58
Loss: 1.1618576645851135
RMSE train: 0.896993	val: 4.056420	test: 3.381463
MAE train: 0.669959	val: 2.874011	test: 2.669319

Epoch: 59
Loss: 1.2162264585494995
RMSE train: 0.866376	val: 4.044217	test: 3.358965
MAE train: 0.644446	val: 2.877503	test: 2.661202

Epoch: 60
Loss: 1.1016359329223633
RMSE train: 0.876654	val: 4.050955	test: 3.363232
MAE train: 0.651681	val: 2.898948	test: 2.665383

Epoch: 61
Loss: 1.130089819431305
RMSE train: 0.898916	val: 4.086967	test: 3.350427
MAE train: 0.670252	val: 2.940015	test: 2.640036

Epoch: 62
Loss: 1.1515817642211914
RMSE train: 0.915245	val: 4.158196	test: 3.333974
MAE train: 0.682246	val: 2.959987	test: 2.578756

Epoch: 63
Loss: 1.142309844493866
RMSE train: 0.913480	val: 4.206771	test: 3.296182
MAE train: 0.675931	val: 2.971912	test: 2.511975

Epoch: 64
Loss: 1.1012405753135681
RMSE train: 0.894421	val: 4.218451	test: 3.250704
MAE train: 0.652998	val: 2.971925	test: 2.456148

Epoch: 65
Loss: 1.0336881279945374
RMSE train: 0.843924	val: 4.170754	test: 3.199412
MAE train: 0.608528	val: 2.947901	test: 2.423330

Epoch: 66
Loss: 1.1718658804893494
RMSE train: 0.783971	val: 4.106707	test: 3.170478
MAE train: 0.563486	val: 2.887673	test: 2.407043

Epoch: 67
Loss: 1.080744057893753
RMSE train: 0.729600	val: 4.048614	test: 3.169181
MAE train: 0.526595	val: 2.843262	test: 2.409465

Epoch: 68
Loss: 1.0584707856178284
RMSE train: 0.760925	val: 4.089311	test: 3.204580
MAE train: 0.549254	val: 2.858047	test: 2.421289

Epoch: 69
Loss: 1.2589542865753174
RMSE train: 0.782916	val: 4.159222	test: 3.282386
MAE train: 0.567184	val: 2.951582	test: 2.530130

Epoch: 70
Loss: 0.9760420322418213
RMSE train: 0.819651	val: 4.233523	test: 3.408081
MAE train: 0.601291	val: 3.029294	test: 2.648215

Epoch: 71
Loss: 1.1211386620998383
RMSE train: 0.824650	val: 4.268644	test: 3.486161
MAE train: 0.610352	val: 3.083748	test: 2.741505

Epoch: 72
Loss: 0.9938364624977112
RMSE train: 0.822552	val: 4.244533	test: 3.477474
MAE train: 0.605383	val: 3.091415	test: 2.748497

Epoch: 73
Loss: 0.9939202964305878
RMSE train: 0.806818	val: 4.161530	test: 3.397641
MAE train: 0.590961	val: 3.009523	test: 2.674759

Epoch: 74
Loss: 0.9421397745609283
RMSE train: 0.830575	val: 4.122758	test: 3.326468
MAE train: 0.608659	val: 2.972930	test: 2.588618

Epoch: 75
Loss: 1.0470642745494843
RMSE train: 0.871615	val: 4.135173	test: 3.308452
MAE train: 0.632415	val: 2.976241	test: 2.560961

Epoch: 76
Loss: 0.9212786257266998
RMSE train: 0.849681	val: 4.156982	test: 3.327364
MAE train: 0.616845	val: 2.996214	test: 2.581310

Epoch: 77
Loss: 0.9575020372867584
RMSE train: 0.823533	val: 4.180059	test: 3.368667
MAE train: 0.603717	val: 3.008280	test: 2.622051

Epoch: 78
Loss: 1.0695018768310547
RMSE train: 0.810624	val: 4.201223	test: 3.394526
MAE train: 0.596367	val: 3.025881	test: 2.657191

Epoch: 79
Loss: 0.8760448396205902
RMSE train: 0.787097	val: 4.188993	test: 3.391408
MAE train: 0.575582	val: 2.999665	test: 2.660002

Epoch: 80
Loss: 0.9823268949985504
RMSE train: 0.791574	val: 4.208463	test: 3.396983
MAE train: 0.568293	val: 2.992559	test: 2.659208

Epoch: 81
Loss: 1.0457491278648376
RMSE train: 0.839853	val: 4.249329	test: 3.418919
MAE train: 0.604824	val: 2.995399	test: 2.661891

Epoch: 82
Loss: 0.8213102221488953
RMSE train: 0.844982	val: 4.240284	test: 3.414392
MAE train: 0.614541	val: 2.981239	test: 2.654330

Epoch: 83
Loss: 0.8715057969093323
RMSE train: 0.799428	val: 4.191227	test: 3.390504
MAE train: 0.590588	val: 2.974367	test: 2.642336Epoch: 23
Loss: 6.599839210510254
RMSE train: 2.993542	val: 6.286958	test: 5.330870
MAE train: 2.664051	val: 5.209575	test: 4.611859

Epoch: 24
Loss: 6.038482666015625
RMSE train: 2.849337	val: 6.135275	test: 5.156759
MAE train: 2.527853	val: 5.010925	test: 4.434442

Epoch: 25
Loss: 5.737437009811401
RMSE train: 2.715256	val: 5.946843	test: 4.994163
MAE train: 2.404782	val: 4.781373	test: 4.282697

Epoch: 26
Loss: 5.525042295455933
RMSE train: 2.672503	val: 5.846182	test: 4.902988
MAE train: 2.368022	val: 4.694819	test: 4.196604

Epoch: 27
Loss: 5.136077642440796
RMSE train: 2.693004	val: 5.816332	test: 4.873014
MAE train: 2.395133	val: 4.696207	test: 4.178733

Epoch: 28
Loss: 5.18585991859436
RMSE train: 2.669031	val: 5.782878	test: 4.851313
MAE train: 2.372328	val: 4.661202	test: 4.165546

Epoch: 29
Loss: 4.662733435630798
RMSE train: 2.635867	val: 5.780594	test: 4.832934
MAE train: 2.337079	val: 4.639661	test: 4.146863

Epoch: 30
Loss: 4.449538469314575
RMSE train: 2.588509	val: 5.729748	test: 4.794390
MAE train: 2.294386	val: 4.570159	test: 4.108658

Epoch: 31
Loss: 4.194646596908569
RMSE train: 2.488831	val: 5.636626	test: 4.729135
MAE train: 2.203878	val: 4.452912	test: 4.038160

Epoch: 32
Loss: 4.07209849357605
RMSE train: 2.396603	val: 5.472381	test: 4.597741
MAE train: 2.126529	val: 4.264554	test: 3.906963

Epoch: 33
Loss: 3.4990649223327637
RMSE train: 2.320239	val: 5.318388	test: 4.483112
MAE train: 2.059392	val: 4.102086	test: 3.795441

Epoch: 34
Loss: 3.492432713508606
RMSE train: 2.251670	val: 5.175883	test: 4.373176
MAE train: 1.997002	val: 3.953629	test: 3.691651

Epoch: 35
Loss: 3.2169528007507324
RMSE train: 2.229464	val: 5.073727	test: 4.281738
MAE train: 1.976881	val: 3.851306	test: 3.599364

Epoch: 36
Loss: 3.1539692878723145
RMSE train: 2.237783	val: 5.058487	test: 4.247792
MAE train: 1.980269	val: 3.825708	test: 3.552842

Epoch: 37
Loss: 2.7142304182052612
RMSE train: 2.227144	val: 5.094219	test: 4.249719
MAE train: 1.956442	val: 3.820812	test: 3.534862

Epoch: 38
Loss: 2.783314347267151
RMSE train: 2.177926	val: 5.070536	test: 4.216629
MAE train: 1.898165	val: 3.738920	test: 3.475997

Epoch: 39
Loss: 2.575636386871338
RMSE train: 2.065551	val: 4.959984	test: 4.111031
MAE train: 1.781597	val: 3.602156	test: 3.365044

Epoch: 40
Loss: 2.311255931854248
RMSE train: 1.936751	val: 4.829686	test: 3.981716
MAE train: 1.647246	val: 3.470701	test: 3.215917

Epoch: 41
Loss: 2.1369399428367615
RMSE train: 1.811791	val: 4.701889	test: 3.865603
MAE train: 1.522423	val: 3.334006	test: 3.090055

Epoch: 42
Loss: 1.9567269086837769
RMSE train: 1.669375	val: 4.544007	test: 3.752260
MAE train: 1.395018	val: 3.185850	test: 2.980869

Epoch: 43
Loss: 1.9796146154403687
RMSE train: 1.551132	val: 4.484332	test: 3.687472
MAE train: 1.290414	val: 3.160271	test: 2.917067

Epoch: 44
Loss: 2.1410590410232544
RMSE train: 1.448300	val: 4.456778	test: 3.643016
MAE train: 1.185439	val: 3.135787	test: 2.872639

Epoch: 45
Loss: 1.8336846828460693
RMSE train: 1.391304	val: 4.471563	test: 3.630820
MAE train: 1.110889	val: 3.132558	test: 2.861208

Epoch: 46
Loss: 1.570533812046051
RMSE train: 1.383859	val: 4.474373	test: 3.629680
MAE train: 1.100447	val: 3.140741	test: 2.848325

Epoch: 47
Loss: 1.637649655342102
RMSE train: 1.359251	val: 4.412629	test: 3.600972
MAE train: 1.073912	val: 3.095841	test: 2.817565

Epoch: 48
Loss: 1.6621119379997253
RMSE train: 1.331797	val: 4.349181	test: 3.566191
MAE train: 1.042325	val: 3.036884	test: 2.783890

Epoch: 49
Loss: 1.6894310116767883
RMSE train: 1.254274	val: 4.270975	test: 3.527423
MAE train: 0.977742	val: 2.961274	test: 2.754236

Epoch: 50
Loss: 1.3187525868415833
RMSE train: 1.213584	val: 4.250502	test: 3.505895
MAE train: 0.945719	val: 2.962329	test: 2.735084

Epoch: 51
Loss: 1.4911128282546997
RMSE train: 1.239646	val: 4.273298	test: 3.521775
MAE train: 0.973674	val: 3.026565	test: 2.749972

Epoch: 52
Loss: 1.2630965113639832
RMSE train: 1.229348	val: 4.260997	test: 3.511599
MAE train: 0.966974	val: 3.050137	test: 2.746745

Epoch: 53
Loss: 1.463122546672821
RMSE train: 1.240394	val: 4.270028	test: 3.511747
MAE train: 0.969734	val: 3.071809	test: 2.746902

Epoch: 54
Loss: 1.2186350226402283
RMSE train: 1.248244	val: 4.282465	test: 3.511526
MAE train: 0.968196	val: 3.054196	test: 2.750515

Epoch: 55
Loss: 1.3116746544837952
RMSE train: 1.202552	val: 4.213171	test: 3.492012
MAE train: 0.925753	val: 2.984822	test: 2.738071

Epoch: 56
Loss: 1.1145072281360626
RMSE train: 1.159232	val: 4.151282	test: 3.467732
MAE train: 0.890573	val: 2.948494	test: 2.726126

Epoch: 57
Loss: 1.1298053562641144
RMSE train: 1.072438	val: 4.079580	test: 3.451721
MAE train: 0.806805	val: 2.889288	test: 2.718716

Epoch: 58
Loss: 1.3696661591529846
RMSE train: 1.036229	val: 4.046508	test: 3.462842
MAE train: 0.771793	val: 2.841774	test: 2.748021

Epoch: 59
Loss: 1.1958186626434326
RMSE train: 0.998903	val: 4.022994	test: 3.469680
MAE train: 0.748428	val: 2.831283	test: 2.776057

Epoch: 60
Loss: 1.225267767906189
RMSE train: 0.945737	val: 4.073485	test: 3.471053
MAE train: 0.704834	val: 2.887313	test: 2.766706

Epoch: 61
Loss: 1.3307247757911682
RMSE train: 0.897664	val: 4.081762	test: 3.452300
MAE train: 0.659980	val: 2.890361	test: 2.738511

Epoch: 62
Loss: 1.0832431316375732
RMSE train: 0.843542	val: 4.038584	test: 3.400495
MAE train: 0.613244	val: 2.859938	test: 2.688732

Epoch: 63
Loss: 1.1375327110290527
RMSE train: 0.808211	val: 3.967898	test: 3.329991
MAE train: 0.583042	val: 2.803024	test: 2.622551

Epoch: 64
Loss: 1.0124020874500275
RMSE train: 0.803890	val: 3.900684	test: 3.297097
MAE train: 0.585764	val: 2.739773	test: 2.600780

Epoch: 65
Loss: 0.8507253229618073
RMSE train: 0.788689	val: 3.843443	test: 3.255129
MAE train: 0.580083	val: 2.689545	test: 2.560958

Epoch: 66
Loss: 1.0214969515800476
RMSE train: 0.777424	val: 3.867082	test: 3.271950
MAE train: 0.571664	val: 2.719604	test: 2.555840

Epoch: 67
Loss: 0.9383073449134827
RMSE train: 0.787915	val: 3.924484	test: 3.272728
MAE train: 0.577766	val: 2.768314	test: 2.532134

Epoch: 68
Loss: 1.0765393376350403
RMSE train: 0.795217	val: 3.981778	test: 3.299716
MAE train: 0.586550	val: 2.817149	test: 2.542016

Epoch: 69
Loss: 1.074275553226471
RMSE train: 0.805251	val: 4.010086	test: 3.281968
MAE train: 0.592761	val: 2.861506	test: 2.532309

Epoch: 70
Loss: 1.132411003112793
RMSE train: 0.778023	val: 3.982398	test: 3.267014
MAE train: 0.575635	val: 2.851290	test: 2.549307

Epoch: 71
Loss: 0.9351683259010315
RMSE train: 0.772996	val: 3.940618	test: 3.259767
MAE train: 0.579034	val: 2.811822	test: 2.581353

Epoch: 72
Loss: 1.0585293471813202
RMSE train: 0.812350	val: 3.991761	test: 3.304938
MAE train: 0.612271	val: 2.852290	test: 2.611057

Epoch: 73
Loss: 1.0075412392616272
RMSE train: 0.848199	val: 4.042487	test: 3.348036
MAE train: 0.643319	val: 2.904121	test: 2.608516

Epoch: 74
Loss: 0.9876755475997925
RMSE train: 0.870576	val: 4.062519	test: 3.380109
MAE train: 0.661758	val: 2.905428	test: 2.609542

Epoch: 75
Loss: 1.0987871289253235
RMSE train: 0.866749	val: 4.061970	test: 3.416928
MAE train: 0.654927	val: 2.882868	test: 2.627971

Epoch: 76
Loss: 0.9203918874263763
RMSE train: 0.857750	val: 4.044121	test: 3.414502
MAE train: 0.642752	val: 2.874844	test: 2.631814

Epoch: 77
Loss: 0.8718859851360321
RMSE train: 0.863690	val: 4.027151	test: 3.409284
MAE train: 0.645420	val: 2.889524	test: 2.649428

Epoch: 78
Loss: 0.9165677726268768
RMSE train: 0.858137	val: 4.015249	test: 3.391437
MAE train: 0.639012	val: 2.886141	test: 2.653245

Epoch: 79
Loss: 1.0134834349155426
RMSE train: 0.822067	val: 4.000678	test: 3.352573
MAE train: 0.613297	val: 2.851653	test: 2.626087

Epoch: 80
Loss: 0.9550696015357971
RMSE train: 0.865192	val: 4.022907	test: 3.333855
MAE train: 0.649725	val: 2.843303	test: 2.575816

Epoch: 81
Loss: 0.9235917925834656
RMSE train: 0.882345	val: 4.009468	test: 3.359278
MAE train: 0.670425	val: 2.817593	test: 2.581171

Epoch: 82
Loss: 0.9075188934803009
RMSE train: 0.838503	val: 3.976680	test: 3.363796
MAE train: 0.641742	val: 2.798842	test: 2.594205

Epoch: 83
Loss: 0.9385786950588226
RMSE train: 0.804951	val: 3.956850	test: 3.341419
MAE train: 0.616252	val: 2.838140	test: 2.613059
Epoch: 23
Loss: 6.253227472305298
RMSE train: 2.716467	val: 6.149797	test: 4.982982
MAE train: 2.378621	val: 5.117870	test: 4.151521

Epoch: 24
Loss: 5.922360897064209
RMSE train: 2.667223	val: 6.141088	test: 4.990019
MAE train: 2.327482	val: 5.150987	test: 4.179200

Epoch: 25
Loss: 5.620167255401611
RMSE train: 2.580482	val: 6.012509	test: 4.916154
MAE train: 2.238922	val: 5.040875	test: 4.129890

Epoch: 26
Loss: 5.439758062362671
RMSE train: 2.514146	val: 5.897698	test: 4.847230
MAE train: 2.166564	val: 4.936669	test: 4.089972

Epoch: 27
Loss: 5.150825262069702
RMSE train: 2.426164	val: 5.728485	test: 4.726493
MAE train: 2.078499	val: 4.748783	test: 3.995750

Epoch: 28
Loss: 4.821630954742432
RMSE train: 2.309273	val: 5.585288	test: 4.618294
MAE train: 1.956774	val: 4.591803	test: 3.899466

Epoch: 29
Loss: 4.620076656341553
RMSE train: 2.253005	val: 5.518933	test: 4.540609
MAE train: 1.894012	val: 4.482514	test: 3.821666

Epoch: 30
Loss: 4.278451204299927
RMSE train: 2.209538	val: 5.480163	test: 4.440103
MAE train: 1.846350	val: 4.391906	test: 3.713237

Epoch: 31
Loss: 4.272016763687134
RMSE train: 2.146941	val: 5.413326	test: 4.336684
MAE train: 1.777460	val: 4.227930	test: 3.580171

Epoch: 32
Loss: 3.7740923166275024
RMSE train: 2.095771	val: 5.344546	test: 4.289542
MAE train: 1.749935	val: 4.127384	test: 3.507874

Epoch: 33
Loss: 3.607754111289978
RMSE train: 2.066055	val: 5.317884	test: 4.278675
MAE train: 1.748739	val: 4.098178	test: 3.479671

Epoch: 34
Loss: 3.3764986991882324
RMSE train: 2.013187	val: 5.278458	test: 4.241677
MAE train: 1.714096	val: 4.042695	test: 3.432291

Epoch: 35
Loss: 3.2673741579055786
RMSE train: 1.894643	val: 5.136548	test: 4.123268
MAE train: 1.604028	val: 3.828293	test: 3.287698

Epoch: 36
Loss: 3.1420669555664062
RMSE train: 1.783934	val: 4.985795	test: 4.027086
MAE train: 1.498439	val: 3.648841	test: 3.183545

Epoch: 37
Loss: 2.916787624359131
RMSE train: 1.710460	val: 4.931481	test: 3.981576
MAE train: 1.425892	val: 3.596295	test: 3.139017

Epoch: 38
Loss: 2.752129077911377
RMSE train: 1.665396	val: 4.929401	test: 3.966248
MAE train: 1.379032	val: 3.575860	test: 3.111807

Epoch: 39
Loss: 2.5011179447174072
RMSE train: 1.633127	val: 4.930059	test: 3.969510
MAE train: 1.336664	val: 3.561131	test: 3.109669

Epoch: 40
Loss: 2.3431695699691772
RMSE train: 1.583973	val: 4.883201	test: 3.973321
MAE train: 1.282907	val: 3.508615	test: 3.120273

Epoch: 41
Loss: 2.0812215209007263
RMSE train: 1.548291	val: 4.823610	test: 3.966667
MAE train: 1.246517	val: 3.481867	test: 3.122684

Epoch: 42
Loss: 2.1557384729385376
RMSE train: 1.518360	val: 4.787580	test: 3.936692
MAE train: 1.219166	val: 3.466120	test: 3.094391

Epoch: 43
Loss: 2.093023359775543
RMSE train: 1.471069	val: 4.723690	test: 3.889868
MAE train: 1.182966	val: 3.418686	test: 3.059359

Epoch: 44
Loss: 1.7819523215293884
RMSE train: 1.412925	val: 4.666306	test: 3.827253
MAE train: 1.135726	val: 3.374369	test: 3.011779

Epoch: 45
Loss: 1.9709553122520447
RMSE train: 1.354838	val: 4.618104	test: 3.764929
MAE train: 1.062637	val: 3.284430	test: 2.938859

Epoch: 46
Loss: 1.7112464904785156
RMSE train: 1.316108	val: 4.598349	test: 3.728969
MAE train: 1.010043	val: 3.235707	test: 2.899433

Epoch: 47
Loss: 1.8937357664108276
RMSE train: 1.300488	val: 4.595586	test: 3.711829
MAE train: 0.982237	val: 3.239364	test: 2.884667

Epoch: 48
Loss: 1.5298523902893066
RMSE train: 1.269148	val: 4.600472	test: 3.671107
MAE train: 0.947287	val: 3.205011	test: 2.832861

Epoch: 49
Loss: 1.3729014992713928
RMSE train: 1.204200	val: 4.526378	test: 3.599460
MAE train: 0.892354	val: 3.122013	test: 2.748322

Epoch: 50
Loss: 1.4850071668624878
RMSE train: 1.165156	val: 4.467942	test: 3.546727
MAE train: 0.854841	val: 3.067449	test: 2.691747

Epoch: 51
Loss: 1.4632043838500977
RMSE train: 1.135921	val: 4.465597	test: 3.519864
MAE train: 0.835829	val: 3.076315	test: 2.668826

Epoch: 52
Loss: 1.3877358436584473
RMSE train: 1.089741	val: 4.446424	test: 3.490285
MAE train: 0.803715	val: 3.065515	test: 2.647879

Epoch: 53
Loss: 1.2626407742500305
RMSE train: 1.048866	val: 4.416565	test: 3.458894
MAE train: 0.771066	val: 3.087026	test: 2.621861

Epoch: 54
Loss: 1.3598659038543701
RMSE train: 0.998050	val: 4.372238	test: 3.408739
MAE train: 0.732599	val: 3.037905	test: 2.557155

Epoch: 55
Loss: 1.2259749174118042
RMSE train: 0.951473	val: 4.293950	test: 3.372180
MAE train: 0.699765	val: 2.967220	test: 2.513011

Epoch: 56
Loss: 1.4381109476089478
RMSE train: 0.926666	val: 4.237587	test: 3.360255
MAE train: 0.686062	val: 2.917698	test: 2.480167

Epoch: 57
Loss: 1.197451651096344
RMSE train: 0.937185	val: 4.198470	test: 3.353460
MAE train: 0.697883	val: 2.911030	test: 2.462840

Epoch: 58
Loss: 1.0999785661697388
RMSE train: 0.939654	val: 4.211020	test: 3.347072
MAE train: 0.698361	val: 2.960366	test: 2.457726

Epoch: 59
Loss: 1.1583243012428284
RMSE train: 0.940165	val: 4.270857	test: 3.338559
MAE train: 0.694655	val: 3.014786	test: 2.440090

Epoch: 60
Loss: 1.1671786904335022
RMSE train: 0.952737	val: 4.346080	test: 3.333132
MAE train: 0.704895	val: 3.042783	test: 2.414943

Epoch: 61
Loss: 1.1485560536384583
RMSE train: 0.943890	val: 4.363702	test: 3.320880
MAE train: 0.701580	val: 3.003948	test: 2.383615

Epoch: 62
Loss: 1.1169277429580688
RMSE train: 0.868648	val: 4.302073	test: 3.282776
MAE train: 0.644793	val: 2.916554	test: 2.342662

Epoch: 63
Loss: 1.2128241062164307
RMSE train: 0.816915	val: 4.271000	test: 3.240505
MAE train: 0.604027	val: 2.842167	test: 2.315505

Epoch: 64
Loss: 1.2205504775047302
RMSE train: 0.786885	val: 4.228299	test: 3.226003
MAE train: 0.580927	val: 2.801834	test: 2.310310

Epoch: 65
Loss: 1.1814064383506775
RMSE train: 0.802436	val: 4.248857	test: 3.272879
MAE train: 0.588774	val: 2.880991	test: 2.381821

Epoch: 66
Loss: 1.1331449151039124
RMSE train: 0.890524	val: 4.317327	test: 3.359275
MAE train: 0.651239	val: 3.006847	test: 2.485494

Epoch: 67
Loss: 1.0799919366836548
RMSE train: 0.922207	val: 4.300896	test: 3.376638
MAE train: 0.684673	val: 2.988870	test: 2.511351

Epoch: 68
Loss: 1.0050443410873413
RMSE train: 0.941856	val: 4.257886	test: 3.363583
MAE train: 0.711970	val: 2.910371	test: 2.490164

Epoch: 69
Loss: 1.1629678010940552
RMSE train: 0.920673	val: 4.201374	test: 3.332649
MAE train: 0.694136	val: 2.839918	test: 2.448667

Epoch: 70
Loss: 1.2079075574874878
RMSE train: 0.888981	val: 4.170483	test: 3.318127
MAE train: 0.661874	val: 2.823921	test: 2.423954

Epoch: 71
Loss: 0.9665798246860504
RMSE train: 0.853989	val: 4.130213	test: 3.312112
MAE train: 0.620557	val: 2.809199	test: 2.437498

Epoch: 72
Loss: 1.0350942015647888
RMSE train: 0.844591	val: 4.129246	test: 3.299710
MAE train: 0.608080	val: 2.774512	test: 2.440616

Epoch: 73
Loss: 1.1209772825241089
RMSE train: 0.864952	val: 4.165443	test: 3.276735
MAE train: 0.633195	val: 2.744730	test: 2.411436

Epoch: 74
Loss: 1.0878582000732422
RMSE train: 0.871857	val: 4.158031	test: 3.232831
MAE train: 0.641533	val: 2.702860	test: 2.363544

Epoch: 75
Loss: 1.070991724729538
RMSE train: 0.814282	val: 4.095311	test: 3.203295
MAE train: 0.598287	val: 2.652837	test: 2.349600

Epoch: 76
Loss: 1.232081800699234
RMSE train: 0.771220	val: 4.132980	test: 3.208191
MAE train: 0.571888	val: 2.763573	test: 2.375387

Epoch: 77
Loss: 0.9550274312496185
RMSE train: 0.773161	val: 4.174543	test: 3.219041
MAE train: 0.575789	val: 2.831740	test: 2.406610

Epoch: 78
Loss: 1.0382329225540161
RMSE train: 0.777402	val: 4.208868	test: 3.207697
MAE train: 0.571180	val: 2.855957	test: 2.392170

Epoch: 79
Loss: 1.0661911368370056
RMSE train: 0.795733	val: 4.246250	test: 3.205875
MAE train: 0.583318	val: 2.887077	test: 2.370296

Epoch: 80
Loss: 0.8873855471611023
RMSE train: 0.788313	val: 4.258064	test: 3.209866
MAE train: 0.584140	val: 2.902208	test: 2.350590

Epoch: 81
Loss: 0.9100309014320374
RMSE train: 0.721501	val: 4.192008	test: 3.188497
MAE train: 0.530646	val: 2.847824	test: 2.336989

Epoch: 82
Loss: 0.9158848226070404
RMSE train: 0.670641	val: 4.145598	test: 3.189255
MAE train: 0.492176	val: 2.796163	test: 2.345400

Epoch: 83
Loss: 1.0131755471229553
RMSE train: 0.650173	val: 4.094809	test: 3.162333
MAE train: 0.480007	val: 2.735403	test: 2.330691
RMSE train: 2.754449	val: 6.128703	test: 5.510652
MAE train: 2.344889	val: 5.166586	test: 4.830288

Epoch: 23
Loss: 6.891553163528442
RMSE train: 2.704562	val: 6.043718	test: 5.465177
MAE train: 2.306208	val: 5.071154	test: 4.798776

Epoch: 24
Loss: 6.404940605163574
RMSE train: 2.624778	val: 5.871016	test: 5.354351
MAE train: 2.232221	val: 4.860986	test: 4.698521

Epoch: 25
Loss: 6.115447759628296
RMSE train: 2.534815	val: 5.707154	test: 5.212795
MAE train: 2.149440	val: 4.657611	test: 4.560377

Epoch: 26
Loss: 5.526225566864014
RMSE train: 2.489987	val: 5.630784	test: 5.128308
MAE train: 2.116495	val: 4.551310	test: 4.477909

Epoch: 27
Loss: 5.359631061553955
RMSE train: 2.450251	val: 5.584457	test: 5.071552
MAE train: 2.085698	val: 4.495379	test: 4.422964

Epoch: 28
Loss: 5.110394477844238
RMSE train: 2.414561	val: 5.571490	test: 5.016796
MAE train: 2.057522	val: 4.491613	test: 4.363983

Epoch: 29
Loss: 4.713130354881287
RMSE train: 2.349012	val: 5.525371	test: 4.926983
MAE train: 1.998637	val: 4.450279	test: 4.263132

Epoch: 30
Loss: 4.5027995109558105
RMSE train: 2.291390	val: 5.452488	test: 4.825618
MAE train: 1.948019	val: 4.375736	test: 4.143308

Epoch: 31
Loss: 4.1041717529296875
RMSE train: 2.220808	val: 5.324224	test: 4.701765
MAE train: 1.878706	val: 4.236457	test: 4.012846

Epoch: 32
Loss: 3.9371787309646606
RMSE train: 2.161582	val: 5.199089	test: 4.578055
MAE train: 1.815416	val: 4.097603	test: 3.890657

Epoch: 33
Loss: 3.699674129486084
RMSE train: 2.109602	val: 5.102294	test: 4.478827
MAE train: 1.753908	val: 3.965149	test: 3.793624

Epoch: 34
Loss: 3.564807415008545
RMSE train: 2.052039	val: 5.012474	test: 4.394537
MAE train: 1.692348	val: 3.846891	test: 3.715034

Epoch: 35
Loss: 3.428954243659973
RMSE train: 1.967029	val: 4.902591	test: 4.304159
MAE train: 1.608509	val: 3.720415	test: 3.630242

Epoch: 36
Loss: 3.0072426795959473
RMSE train: 1.925377	val: 4.869700	test: 4.278944
MAE train: 1.572307	val: 3.708267	test: 3.603411

Epoch: 37
Loss: 3.068684458732605
RMSE train: 1.843523	val: 4.805485	test: 4.216360
MAE train: 1.493540	val: 3.659935	test: 3.539601

Epoch: 38
Loss: 2.9411330223083496
RMSE train: 1.757197	val: 4.766844	test: 4.140494
MAE train: 1.410498	val: 3.619228	test: 3.449226

Epoch: 39
Loss: 2.397176742553711
RMSE train: 1.659238	val: 4.701162	test: 4.051523
MAE train: 1.318485	val: 3.533741	test: 3.341597

Epoch: 40
Loss: 2.588446021080017
RMSE train: 1.575970	val: 4.565464	test: 3.916787
MAE train: 1.241468	val: 3.356190	test: 3.188836

Epoch: 41
Loss: 2.542060613632202
RMSE train: 1.492953	val: 4.438139	test: 3.771474
MAE train: 1.165487	val: 3.228975	test: 3.028323

Epoch: 42
Loss: 2.223871350288391
RMSE train: 1.373672	val: 4.319843	test: 3.618623
MAE train: 1.062692	val: 3.160077	test: 2.874002

Epoch: 43
Loss: 2.2887502908706665
RMSE train: 1.293134	val: 4.237333	test: 3.492104
MAE train: 0.995361	val: 3.128224	test: 2.745360

Epoch: 44
Loss: 2.2213738560676575
RMSE train: 1.242280	val: 4.166601	test: 3.397730
MAE train: 0.951385	val: 3.054637	test: 2.647167

Epoch: 45
Loss: 2.1162787079811096
RMSE train: 1.228653	val: 4.140905	test: 3.357529
MAE train: 0.937315	val: 2.965857	test: 2.593646

Epoch: 46
Loss: 1.8063467741012573
RMSE train: 1.200985	val: 4.124615	test: 3.330081
MAE train: 0.913143	val: 2.873810	test: 2.552518

Epoch: 47
Loss: 1.9143491983413696
RMSE train: 1.183417	val: 4.145707	test: 3.338331
MAE train: 0.903639	val: 2.868054	test: 2.552682

Epoch: 48
Loss: 1.8485188484191895
RMSE train: 1.127651	val: 4.106043	test: 3.308201
MAE train: 0.860038	val: 2.867312	test: 2.521543

Epoch: 49
Loss: 1.7307983040809631
RMSE train: 1.066422	val: 4.052471	test: 3.264475
MAE train: 0.814457	val: 2.883184	test: 2.474443

Epoch: 50
Loss: 1.5859051942825317
RMSE train: 1.027580	val: 4.047753	test: 3.242072
MAE train: 0.786189	val: 2.941421	test: 2.444504

Epoch: 51
Loss: 1.6756107807159424
RMSE train: 1.003630	val: 4.041467	test: 3.230334
MAE train: 0.769948	val: 2.979126	test: 2.428417

Epoch: 52
Loss: 1.7031976580619812
RMSE train: 1.004847	val: 4.058456	test: 3.206022
MAE train: 0.768216	val: 2.990010	test: 2.395636

Epoch: 53
Loss: 1.6362321972846985
RMSE train: 0.999751	val: 4.044066	test: 3.157461
MAE train: 0.751178	val: 2.939209	test: 2.343511

Epoch: 54
Loss: 1.5569899678230286
RMSE train: 0.998749	val: 4.010828	test: 3.119387
MAE train: 0.746890	val: 2.875357	test: 2.307021

Epoch: 55
Loss: 1.492927610874176
RMSE train: 0.952912	val: 3.901955	test: 3.026387
MAE train: 0.718087	val: 2.735541	test: 2.217715

Epoch: 56
Loss: 1.3322008848190308
RMSE train: 0.925558	val: 3.833234	test: 2.949411
MAE train: 0.705312	val: 2.655350	test: 2.151707

Epoch: 57
Loss: 1.477522373199463
RMSE train: 0.912558	val: 3.856895	test: 2.910152
MAE train: 0.702709	val: 2.686037	test: 2.127186

Epoch: 58
Loss: 1.3955417275428772
RMSE train: 0.925396	val: 3.914461	test: 2.910482
MAE train: 0.713434	val: 2.749598	test: 2.139223

Epoch: 59
Loss: 1.3744555711746216
RMSE train: 0.922077	val: 3.955411	test: 2.931322
MAE train: 0.704931	val: 2.767925	test: 2.144761

Epoch: 60
Loss: 1.521128535270691
RMSE train: 0.906071	val: 3.984159	test: 2.976054
MAE train: 0.685564	val: 2.783068	test: 2.173847

Epoch: 61
Loss: 1.4789666533470154
RMSE train: 0.852386	val: 3.967477	test: 2.993172
MAE train: 0.645861	val: 2.781427	test: 2.180763

Epoch: 62
Loss: 1.637725055217743
RMSE train: 0.843698	val: 3.991083	test: 3.026906
MAE train: 0.640314	val: 2.792389	test: 2.207843

Epoch: 63
Loss: 1.473160743713379
RMSE train: 0.871805	val: 4.065709	test: 3.111681
MAE train: 0.662309	val: 2.852002	test: 2.275060

Epoch: 64
Loss: 1.2830674648284912
RMSE train: 0.878019	val: 4.089619	test: 3.138727
MAE train: 0.667117	val: 2.886849	test: 2.299638

Epoch: 65
Loss: 1.2192115783691406
RMSE train: 0.860054	val: 4.103644	test: 3.146218
MAE train: 0.658282	val: 2.910863	test: 2.320412

Epoch: 66
Loss: 1.416496992111206
RMSE train: 0.845307	val: 4.131260	test: 3.157244
MAE train: 0.649941	val: 2.970998	test: 2.348162

Epoch: 67
Loss: 1.3334794640541077
RMSE train: 0.844049	val: 4.120482	test: 3.124961
MAE train: 0.643924	val: 2.969618	test: 2.324327

Epoch: 68
Loss: 1.2242110967636108
RMSE train: 0.852700	val: 4.065339	test: 3.098181
MAE train: 0.648141	val: 2.936616	test: 2.319690

Epoch: 69
Loss: 1.1882022619247437
RMSE train: 0.876697	val: 4.019409	test: 3.091080
MAE train: 0.668270	val: 2.903682	test: 2.331873

Epoch: 70
Loss: 1.2164164781570435
RMSE train: 0.865287	val: 3.943781	test: 3.079376
MAE train: 0.662999	val: 2.840936	test: 2.314375

Epoch: 71
Loss: 1.3443955183029175
RMSE train: 0.836560	val: 3.879013	test: 3.063995
MAE train: 0.641400	val: 2.800197	test: 2.301877

Epoch: 72
Loss: 1.1450579166412354
RMSE train: 0.830549	val: 3.877598	test: 3.072836
MAE train: 0.642129	val: 2.809650	test: 2.303950

Epoch: 73
Loss: 1.0412253141403198
RMSE train: 0.813827	val: 3.882905	test: 3.059198
MAE train: 0.633343	val: 2.820240	test: 2.299424

Epoch: 74
Loss: 1.2037367820739746
RMSE train: 0.811239	val: 3.882210	test: 3.049390
MAE train: 0.633431	val: 2.842887	test: 2.306123

Epoch: 75
Loss: 1.3248624205589294
RMSE train: 0.820604	val: 3.898354	test: 3.024881
MAE train: 0.644342	val: 2.847454	test: 2.279752

Epoch: 76
Loss: 1.110856294631958
RMSE train: 0.832785	val: 3.911116	test: 2.981610
MAE train: 0.655455	val: 2.811970	test: 2.227160

Epoch: 77
Loss: 1.1420036554336548
RMSE train: 0.823462	val: 3.866560	test: 2.924767
MAE train: 0.654090	val: 2.749074	test: 2.178158

Epoch: 78
Loss: 1.046843409538269
RMSE train: 0.730500	val: 3.717087	test: 2.795042
MAE train: 0.573277	val: 2.638239	test: 2.072846

Epoch: 79
Loss: 1.0163158178329468
RMSE train: 0.668769	val: 3.686831	test: 2.754298
MAE train: 0.512063	val: 2.664410	test: 2.034634

Epoch: 80
Loss: 1.2545848488807678
RMSE train: 0.652915	val: 3.699302	test: 2.760882
MAE train: 0.500143	val: 2.700333	test: 2.047052

Epoch: 81
Loss: 1.1400583386421204
RMSE train: 0.670936	val: 3.740144	test: 2.802571
MAE train: 0.521552	val: 2.724547	test: 2.091231

Epoch: 82
Loss: 0.9497395157814026
RMSE train: 0.698303	val: 3.826046	test: 2.873584
MAE train: 0.548592	val: 2.768633	test: 2.146771

Epoch: 83
Loss: 1.0920215845108032
RMSE train: 3.178716	val: 6.878061	test: 5.317195
MAE train: 2.657794	val: 5.642914	test: 4.490916

Epoch: 23
Loss: 8.261116743087769
RMSE train: 3.133979	val: 6.854904	test: 5.328066
MAE train: 2.632650	val: 5.649209	test: 4.488831

Epoch: 24
Loss: 7.745559215545654
RMSE train: 3.074003	val: 6.834648	test: 5.334928
MAE train: 2.591709	val: 5.652929	test: 4.495605

Epoch: 25
Loss: 7.398033380508423
RMSE train: 3.024901	val: 6.853384	test: 5.357820
MAE train: 2.560597	val: 5.692806	test: 4.512435

Epoch: 26
Loss: 6.667738437652588
RMSE train: 2.954193	val: 6.832852	test: 5.349024
MAE train: 2.504899	val: 5.695886	test: 4.497194

Epoch: 27
Loss: 6.8596577644348145
RMSE train: 2.877524	val: 6.769482	test: 5.303724
MAE train: 2.439892	val: 5.655835	test: 4.443802

Epoch: 28
Loss: 6.128988981246948
RMSE train: 2.798643	val: 6.738493	test: 5.285724
MAE train: 2.369346	val: 5.630850	test: 4.414960

Epoch: 29
Loss: 5.788780450820923
RMSE train: 2.717238	val: 6.684405	test: 5.276138
MAE train: 2.298422	val: 5.584052	test: 4.414245

Epoch: 30
Loss: 5.485759019851685
RMSE train: 2.617100	val: 6.611592	test: 5.239316
MAE train: 2.207648	val: 5.496593	test: 4.391894

Epoch: 31
Loss: 5.456171035766602
RMSE train: 2.510471	val: 6.545327	test: 5.207876
MAE train: 2.111546	val: 5.442079	test: 4.386792

Epoch: 32
Loss: 4.9867167472839355
RMSE train: 2.436056	val: 6.463674	test: 5.177845
MAE train: 2.047540	val: 5.343583	test: 4.376303

Epoch: 33
Loss: 5.128774404525757
RMSE train: 2.372724	val: 6.411412	test: 5.171414
MAE train: 1.996579	val: 5.293367	test: 4.382709

Epoch: 34
Loss: 4.906029939651489
RMSE train: 2.333644	val: 6.383308	test: 5.182004
MAE train: 1.970655	val: 5.246794	test: 4.402612

Epoch: 35
Loss: 4.168256163597107
RMSE train: 2.228434	val: 6.274733	test: 5.127614
MAE train: 1.874929	val: 5.099930	test: 4.347221

Epoch: 36
Loss: 3.9575663805007935
RMSE train: 2.142343	val: 6.195714	test: 5.080508
MAE train: 1.798127	val: 5.021390	test: 4.323433

Epoch: 37
Loss: 3.9318264722824097
RMSE train: 2.071161	val: 6.104176	test: 4.986656
MAE train: 1.733579	val: 4.905580	test: 4.220137

Epoch: 38
Loss: 3.7168294191360474
RMSE train: 1.947052	val: 5.983174	test: 4.874278
MAE train: 1.616353	val: 4.731965	test: 4.060661

Epoch: 39
Loss: 3.501516103744507
RMSE train: 1.869485	val: 5.929146	test: 4.801626
MAE train: 1.546416	val: 4.597061	test: 3.945296

Epoch: 40
Loss: 3.2702016830444336
RMSE train: 1.780528	val: 5.883648	test: 4.737629
MAE train: 1.461388	val: 4.477398	test: 3.853997

Epoch: 41
Loss: 3.039709448814392
RMSE train: 1.695219	val: 5.874406	test: 4.709646
MAE train: 1.380923	val: 4.409458	test: 3.809436

Epoch: 42
Loss: 2.8845564126968384
RMSE train: 1.613231	val: 5.875161	test: 4.696026
MAE train: 1.308061	val: 4.373089	test: 3.803929

Epoch: 43
Loss: 2.8453627824783325
RMSE train: 1.511631	val: 5.826844	test: 4.628967
MAE train: 1.213218	val: 4.276584	test: 3.746955

Epoch: 44
Loss: 2.776348829269409
RMSE train: 1.413815	val: 5.750833	test: 4.555359
MAE train: 1.119932	val: 4.180605	test: 3.686919

Epoch: 45
Loss: 2.464895248413086
RMSE train: 1.330100	val: 5.642836	test: 4.475418
MAE train: 1.043808	val: 4.054277	test: 3.612322

Epoch: 46
Loss: 2.7044124603271484
RMSE train: 1.276685	val: 5.573126	test: 4.392968
MAE train: 0.992412	val: 4.009551	test: 3.524453

Epoch: 47
Loss: 2.390182137489319
RMSE train: 1.250337	val: 5.550767	test: 4.336044
MAE train: 0.967766	val: 4.027699	test: 3.488955

Epoch: 48
Loss: 2.3195430040359497
RMSE train: 1.241798	val: 5.577893	test: 4.326630
MAE train: 0.960967	val: 4.036437	test: 3.483246

Epoch: 49
Loss: 2.217490792274475
RMSE train: 1.222437	val: 5.602969	test: 4.342309
MAE train: 0.951677	val: 4.029631	test: 3.470357

Epoch: 50
Loss: 2.1576071977615356
RMSE train: 1.172096	val: 5.595372	test: 4.357285
MAE train: 0.917107	val: 4.032027	test: 3.461766

Epoch: 51
Loss: 2.2588279247283936
RMSE train: 1.105759	val: 5.561601	test: 4.354298
MAE train: 0.866756	val: 4.009933	test: 3.433759

Epoch: 52
Loss: 2.1166306138038635
RMSE train: 1.090007	val: 5.561080	test: 4.358591
MAE train: 0.858892	val: 4.046103	test: 3.433757

Epoch: 53
Loss: 2.0607596039772034
RMSE train: 1.045799	val: 5.489819	test: 4.323977
MAE train: 0.822337	val: 4.020801	test: 3.372507

Epoch: 54
Loss: 2.023636221885681
RMSE train: 1.027091	val: 5.444755	test: 4.301168
MAE train: 0.805007	val: 3.974916	test: 3.334697

Epoch: 55
Loss: 1.8929055333137512
RMSE train: 1.037490	val: 5.470179	test: 4.329860
MAE train: 0.812264	val: 3.982370	test: 3.354809

Epoch: 56
Loss: 1.7975665926933289
RMSE train: 1.046582	val: 5.505406	test: 4.350612
MAE train: 0.819936	val: 4.001112	test: 3.389543

Epoch: 57
Loss: 2.061272621154785
RMSE train: 1.087926	val: 5.591960	test: 4.383420
MAE train: 0.853879	val: 4.041559	test: 3.443430

Epoch: 58
Loss: 1.7562592029571533
RMSE train: 1.129218	val: 5.668505	test: 4.388671
MAE train: 0.882054	val: 4.120393	test: 3.472834

Epoch: 59
Loss: 1.8573302030563354
RMSE train: 1.125303	val: 5.692782	test: 4.368671
MAE train: 0.868644	val: 4.169130	test: 3.480274

Epoch: 60
Loss: 1.7529470920562744
RMSE train: 1.094775	val: 5.656014	test: 4.340034
MAE train: 0.836693	val: 4.158884	test: 3.463315

Epoch: 61
Loss: 1.700161337852478
RMSE train: 1.009035	val: 5.518032	test: 4.279011
MAE train: 0.781590	val: 4.043861	test: 3.377768

Epoch: 62
Loss: 1.5365062952041626
RMSE train: 0.910981	val: 5.359834	test: 4.225496
MAE train: 0.721014	val: 3.885251	test: 3.283952

Epoch: 63
Loss: 1.7940157055854797
RMSE train: 0.862186	val: 5.229133	test: 4.171325
MAE train: 0.685720	val: 3.757189	test: 3.208559

Epoch: 64
Loss: 1.6004574298858643
RMSE train: 0.844177	val: 5.148249	test: 4.153127
MAE train: 0.670966	val: 3.672981	test: 3.188803

Epoch: 65
Loss: 1.6387104392051697
RMSE train: 0.853200	val: 5.139358	test: 4.160215
MAE train: 0.678849	val: 3.639258	test: 3.202206

Epoch: 66
Loss: 1.7296428084373474
RMSE train: 0.903240	val: 5.179990	test: 4.183341
MAE train: 0.718245	val: 3.646951	test: 3.239078

Epoch: 67
Loss: 1.5873122215270996
RMSE train: 0.933552	val: 5.220004	test: 4.210705
MAE train: 0.747067	val: 3.672310	test: 3.292079

Epoch: 68
Loss: 1.579125165939331
RMSE train: 0.945259	val: 5.252197	test: 4.244604
MAE train: 0.762980	val: 3.711878	test: 3.368785

Epoch: 69
Loss: 1.480357050895691
RMSE train: 0.987834	val: 5.341274	test: 4.308883
MAE train: 0.805127	val: 3.840201	test: 3.485110

Epoch: 70
Loss: 1.4872487783432007
RMSE train: 1.027447	val: 5.446873	test: 4.359399
MAE train: 0.838371	val: 4.025470	test: 3.570494

Epoch: 71
Loss: 1.3480150699615479
RMSE train: 1.033118	val: 5.500945	test: 4.350595
MAE train: 0.840136	val: 4.109328	test: 3.564204

Epoch: 72
Loss: 1.434745967388153
RMSE train: 1.036660	val: 5.519027	test: 4.324295
MAE train: 0.827961	val: 4.115246	test: 3.536782

Epoch: 73
Loss: 1.4394691586494446
RMSE train: 1.035984	val: 5.555271	test: 4.295700
MAE train: 0.808485	val: 4.120790	test: 3.488948

Epoch: 74
Loss: 1.3207345604896545
RMSE train: 0.994599	val: 5.540765	test: 4.242554
MAE train: 0.762174	val: 4.082101	test: 3.406047

Epoch: 75
Loss: 1.316501498222351
RMSE train: 0.942561	val: 5.534800	test: 4.215385
MAE train: 0.718671	val: 4.053189	test: 3.332434

Epoch: 76
Loss: 1.2779520153999329
RMSE train: 0.844302	val: 5.462367	test: 4.190810
MAE train: 0.652515	val: 3.953454	test: 3.266354

Epoch: 77
Loss: 1.295207142829895
RMSE train: 0.758928	val: 5.384462	test: 4.185922
MAE train: 0.593809	val: 3.839129	test: 3.232625

Epoch: 78
Loss: 1.2787618041038513
RMSE train: 0.737091	val: 5.366324	test: 4.211893
MAE train: 0.580493	val: 3.764578	test: 3.240764

Epoch: 79
Loss: 1.3425043225288391
RMSE train: 0.746198	val: 5.373968	test: 4.220656
MAE train: 0.596772	val: 3.786807	test: 3.262776

Epoch: 80
Loss: 1.2340189218521118
RMSE train: 0.807780	val: 5.416476	test: 4.235069
MAE train: 0.650953	val: 3.903019	test: 3.297454

Epoch: 81
Loss: 1.4057561159133911
RMSE train: 0.849217	val: 5.401371	test: 4.196182
MAE train: 0.678766	val: 3.931246	test: 3.271216

Epoch: 82
Loss: 1.3741145133972168
RMSE train: 0.854931	val: 5.360969	test: 4.138760
MAE train: 0.676121	val: 3.899116	test: 3.215707

Epoch: 83
Loss: 1.2004523873329163
RMSE train: 2.763836	val: 6.321683	test: 5.839949
MAE train: 2.295709	val: 5.041105	test: 5.067042

Epoch: 23
Loss: 7.324732065200806
RMSE train: 2.644522	val: 6.190033	test: 5.792510
MAE train: 2.189497	val: 4.934611	test: 5.005944

Epoch: 24
Loss: 7.1883544921875
RMSE train: 2.546930	val: 6.094838	test: 5.749707
MAE train: 2.105625	val: 4.852862	test: 4.957267

Epoch: 25
Loss: 6.578552722930908
RMSE train: 2.438115	val: 5.969606	test: 5.683746
MAE train: 2.011730	val: 4.744836	test: 4.886050

Epoch: 26
Loss: 6.180934190750122
RMSE train: 2.339696	val: 5.833497	test: 5.568160
MAE train: 1.929628	val: 4.609103	test: 4.764883

Epoch: 27
Loss: 5.885957479476929
RMSE train: 2.315998	val: 5.820295	test: 5.480354
MAE train: 1.913327	val: 4.555108	test: 4.671043

Epoch: 28
Loss: 5.722749471664429
RMSE train: 2.315863	val: 5.817386	test: 5.394413
MAE train: 1.914083	val: 4.475218	test: 4.575267

Epoch: 29
Loss: 5.287841796875
RMSE train: 2.298690	val: 5.744286	test: 5.282215
MAE train: 1.898398	val: 4.370487	test: 4.460171

Epoch: 30
Loss: 4.978191614151001
RMSE train: 2.271512	val: 5.637920	test: 5.176995
MAE train: 1.873409	val: 4.260120	test: 4.347578

Epoch: 31
Loss: 4.527750253677368
RMSE train: 2.220740	val: 5.512523	test: 5.089828
MAE train: 1.821310	val: 4.134168	test: 4.258862

Epoch: 32
Loss: 4.325619220733643
RMSE train: 2.144244	val: 5.366895	test: 4.984726
MAE train: 1.745042	val: 3.982275	test: 4.155328

Epoch: 33
Loss: 3.8088064193725586
RMSE train: 2.063160	val: 5.214572	test: 4.897894
MAE train: 1.668289	val: 3.845076	test: 4.070485

Epoch: 34
Loss: 4.114539742469788
RMSE train: 1.989172	val: 5.080369	test: 4.824916
MAE train: 1.603980	val: 3.733780	test: 3.999347

Epoch: 35
Loss: 3.631507635116577
RMSE train: 1.924551	val: 5.017345	test: 4.764105
MAE train: 1.545902	val: 3.649287	test: 3.934753

Epoch: 36
Loss: 3.381139039993286
RMSE train: 1.880870	val: 5.028697	test: 4.738308
MAE train: 1.505216	val: 3.639704	test: 3.913480

Epoch: 37
Loss: 3.423357605934143
RMSE train: 1.835547	val: 5.053556	test: 4.736206
MAE train: 1.466127	val: 3.644757	test: 3.908977

Epoch: 38
Loss: 3.0791211128234863
RMSE train: 1.816383	val: 5.140407	test: 4.746690
MAE train: 1.453234	val: 3.694336	test: 3.906036

Epoch: 39
Loss: 2.7164398431777954
RMSE train: 1.770765	val: 5.177835	test: 4.697759
MAE train: 1.414347	val: 3.669677	test: 3.837124

Epoch: 40
Loss: 2.7095530033111572
RMSE train: 1.716000	val: 5.128924	test: 4.575408
MAE train: 1.362976	val: 3.573734	test: 3.695118

Epoch: 41
Loss: 2.8502156734466553
RMSE train: 1.615989	val: 4.987793	test: 4.399421
MAE train: 1.272858	val: 3.411261	test: 3.500066

Epoch: 42
Loss: 2.7334868907928467
RMSE train: 1.488568	val: 4.790631	test: 4.223870
MAE train: 1.161301	val: 3.229990	test: 3.315030

Epoch: 43
Loss: 2.544433355331421
RMSE train: 1.367327	val: 4.556794	test: 4.045564
MAE train: 1.066300	val: 3.022506	test: 3.145181

Epoch: 44
Loss: 2.3184717893600464
RMSE train: 1.282946	val: 4.347538	test: 3.890484
MAE train: 0.997840	val: 2.853697	test: 3.010366

Epoch: 45
Loss: 2.4579092264175415
RMSE train: 1.254898	val: 4.244158	test: 3.803474
MAE train: 0.973154	val: 2.780928	test: 2.938851

Epoch: 46
Loss: 2.3353034257888794
RMSE train: 1.230346	val: 4.192724	test: 3.739161
MAE train: 0.951989	val: 2.747603	test: 2.883010

Epoch: 47
Loss: 2.293259620666504
RMSE train: 1.199364	val: 4.179020	test: 3.704534
MAE train: 0.927605	val: 2.738994	test: 2.852706

Epoch: 48
Loss: 2.078905463218689
RMSE train: 1.142683	val: 4.063170	test: 3.622763
MAE train: 0.880322	val: 2.644246	test: 2.786478

Epoch: 49
Loss: 2.270753264427185
RMSE train: 1.090967	val: 3.937676	test: 3.550096
MAE train: 0.835812	val: 2.561933	test: 2.735147

Epoch: 50
Loss: 1.9558050036430359
RMSE train: 1.077888	val: 3.983460	test: 3.560673
MAE train: 0.822956	val: 2.615335	test: 2.753071

Epoch: 51
Loss: 1.9036297798156738
RMSE train: 1.080268	val: 4.078791	test: 3.594187
MAE train: 0.827865	val: 2.676537	test: 2.781143

Epoch: 52
Loss: 1.8780686855316162
RMSE train: 1.072391	val: 4.088224	test: 3.572011
MAE train: 0.822876	val: 2.675150	test: 2.743801

Epoch: 53
Loss: 1.85757714509964
RMSE train: 1.074335	val: 4.103036	test: 3.548452
MAE train: 0.827287	val: 2.680007	test: 2.723003

Epoch: 54
Loss: 1.8140575289726257
RMSE train: 1.051008	val: 4.071730	test: 3.512638
MAE train: 0.806642	val: 2.672421	test: 2.704090

Epoch: 55
Loss: 1.6338008642196655
RMSE train: 1.012951	val: 4.019430	test: 3.488421
MAE train: 0.771854	val: 2.665711	test: 2.690652

Epoch: 56
Loss: 1.7049099206924438
RMSE train: 0.996280	val: 4.032554	test: 3.501834
MAE train: 0.758636	val: 2.696500	test: 2.697209

Epoch: 57
Loss: 1.6482583284378052
RMSE train: 0.992198	val: 4.116683	test: 3.543676
MAE train: 0.758521	val: 2.739613	test: 2.726625

Epoch: 58
Loss: 1.5057357549667358
RMSE train: 0.969692	val: 4.146252	test: 3.565105
MAE train: 0.746343	val: 2.737778	test: 2.740493

Epoch: 59
Loss: 1.45416259765625
RMSE train: 0.974059	val: 4.212650	test: 3.586697
MAE train: 0.754994	val: 2.770257	test: 2.754047

Epoch: 60
Loss: 1.6797231435775757
RMSE train: 0.966876	val: 4.292358	test: 3.624905
MAE train: 0.753106	val: 2.810492	test: 2.786919

Epoch: 61
Loss: 1.6684751510620117
RMSE train: 0.928417	val: 4.312100	test: 3.658645
MAE train: 0.726774	val: 2.816655	test: 2.811548

Epoch: 62
Loss: 1.8008161783218384
RMSE train: 0.915138	val: 4.308912	test: 3.678956
MAE train: 0.719140	val: 2.815279	test: 2.810443

Epoch: 63
Loss: 1.4848142266273499
RMSE train: 0.913915	val: 4.334574	test: 3.726150
MAE train: 0.716416	val: 2.826989	test: 2.839367

Epoch: 64
Loss: 1.3686053156852722
RMSE train: 0.905086	val: 4.363527	test: 3.756273
MAE train: 0.708564	val: 2.834500	test: 2.864222

Epoch: 65
Loss: 1.6057270765304565
RMSE train: 0.912354	val: 4.443124	test: 3.802273
MAE train: 0.716063	val: 2.889729	test: 2.915149

Epoch: 66
Loss: 1.5005807280540466
RMSE train: 0.910364	val: 4.493141	test: 3.835182
MAE train: 0.715077	val: 2.932677	test: 2.946749

Epoch: 67
Loss: 1.4711162447929382
RMSE train: 0.952657	val: 4.580442	test: 3.856082
MAE train: 0.745168	val: 2.996452	test: 2.957972

Epoch: 68
Loss: 1.3407250046730042
RMSE train: 0.991027	val: 4.639514	test: 3.872851
MAE train: 0.765338	val: 3.037405	test: 2.956391

Epoch: 69
Loss: 1.4492610692977905
RMSE train: 1.008576	val: 4.645333	test: 3.874682
MAE train: 0.777948	val: 3.059057	test: 2.944342

Epoch: 70
Loss: 1.291575014591217
RMSE train: 0.956794	val: 4.586794	test: 3.902148
MAE train: 0.745298	val: 3.032688	test: 2.962432

Epoch: 71
Loss: 1.4604631066322327
RMSE train: 0.859818	val: 4.438378	test: 3.912234
MAE train: 0.674840	val: 2.951757	test: 2.971931

Epoch: 72
Loss: 1.2051867842674255
RMSE train: 0.842798	val: 4.324703	test: 3.934356
MAE train: 0.662132	val: 2.905811	test: 2.982410

Epoch: 73
Loss: 1.2424758076667786
RMSE train: 0.861643	val: 4.274837	test: 3.932538
MAE train: 0.677819	val: 2.888306	test: 2.975915

Epoch: 74
Loss: 1.2046576738357544
RMSE train: 0.889309	val: 4.314632	test: 3.940397
MAE train: 0.700610	val: 2.928639	test: 3.002788

Epoch: 75
Loss: 1.4178600907325745
RMSE train: 0.945362	val: 4.439432	test: 3.930099
MAE train: 0.751949	val: 2.995078	test: 3.010986

Epoch: 76
Loss: 1.243061363697052
RMSE train: 0.978936	val: 4.535102	test: 3.896436
MAE train: 0.784725	val: 3.041494	test: 2.990851

Epoch: 77
Loss: 1.328811764717102
RMSE train: 0.963257	val: 4.529917	test: 3.803551
MAE train: 0.770662	val: 3.015947	test: 2.898565

Epoch: 78
Loss: 1.2626850605010986
RMSE train: 0.854844	val: 4.359572	test: 3.648274
MAE train: 0.679398	val: 2.864849	test: 2.741045

Epoch: 79
Loss: 1.2825432419776917
RMSE train: 0.742833	val: 4.217592	test: 3.544709
MAE train: 0.581203	val: 2.809064	test: 2.657122

Epoch: 80
Loss: 1.2354739308357239
RMSE train: 0.707986	val: 4.171363	test: 3.490437
MAE train: 0.553995	val: 2.777863	test: 2.615436

Epoch: 81
Loss: 1.2040555477142334
RMSE train: 0.741060	val: 4.155128	test: 3.469634
MAE train: 0.585932	val: 2.745397	test: 2.591602

Epoch: 82
Loss: 1.1540055871009827
RMSE train: 0.797360	val: 4.198932	test: 3.502831
MAE train: 0.631780	val: 2.749610	test: 2.614800

Epoch: 83
Loss: 1.0658739805221558
RMSE train: 3.225474	val: 7.124428	test: 6.217001
MAE train: 2.892532	val: 6.038437	test: 5.508984

Epoch: 23
Loss: 6.539291143417358
RMSE train: 3.118363	val: 6.952305	test: 6.108713
MAE train: 2.788927	val: 5.857375	test: 5.411437

Epoch: 24
Loss: 6.348684549331665
RMSE train: 3.036498	val: 6.852453	test: 6.060842
MAE train: 2.707604	val: 5.759904	test: 5.367754

Epoch: 25
Loss: 5.971437215805054
RMSE train: 2.970012	val: 6.784453	test: 6.031016
MAE train: 2.644992	val: 5.692135	test: 5.328495

Epoch: 26
Loss: 5.681701898574829
RMSE train: 2.889742	val: 6.705552	test: 5.969218
MAE train: 2.565504	val: 5.626076	test: 5.262064

Epoch: 27
Loss: 5.156111240386963
RMSE train: 2.789557	val: 6.619739	test: 5.834286
MAE train: 2.468243	val: 5.530358	test: 5.126646

Epoch: 28
Loss: 5.100564479827881
RMSE train: 2.699276	val: 6.549343	test: 5.698454
MAE train: 2.380611	val: 5.450406	test: 4.984755

Epoch: 29
Loss: 4.818772077560425
RMSE train: 2.608974	val: 6.473149	test: 5.569626
MAE train: 2.291420	val: 5.349259	test: 4.849643

Epoch: 30
Loss: 4.3801188468933105
RMSE train: 2.541478	val: 6.398845	test: 5.473711
MAE train: 2.235009	val: 5.246997	test: 4.751216

Epoch: 31
Loss: 4.047557830810547
RMSE train: 2.540934	val: 6.368082	test: 5.413330
MAE train: 2.247367	val: 5.175604	test: 4.689693

Epoch: 32
Loss: 4.156967639923096
RMSE train: 2.516956	val: 6.325495	test: 5.362248
MAE train: 2.228897	val: 5.100553	test: 4.634175

Epoch: 33
Loss: 3.7868359088897705
RMSE train: 2.477849	val: 6.288251	test: 5.277374
MAE train: 2.185729	val: 5.004269	test: 4.546894

Epoch: 34
Loss: 3.3583160638809204
RMSE train: 2.429673	val: 6.254086	test: 5.222681
MAE train: 2.133857	val: 4.932329	test: 4.491779

Epoch: 35
Loss: 3.487063407897949
RMSE train: 2.352489	val: 6.196046	test: 5.120182
MAE train: 2.047422	val: 4.810114	test: 4.389504

Epoch: 36
Loss: 3.1055139303207397
RMSE train: 2.240264	val: 6.104565	test: 5.021151
MAE train: 1.930448	val: 4.688390	test: 4.286134

Epoch: 37
Loss: 3.2066015005111694
RMSE train: 2.123509	val: 6.006188	test: 4.941211
MAE train: 1.807250	val: 4.567363	test: 4.194147

Epoch: 38
Loss: 2.818210244178772
RMSE train: 2.015533	val: 5.910056	test: 4.894052
MAE train: 1.704518	val: 4.453132	test: 4.137405

Epoch: 39
Loss: 2.7655794620513916
RMSE train: 1.927573	val: 5.836894	test: 4.836432
MAE train: 1.620559	val: 4.314717	test: 4.070386

Epoch: 40
Loss: 2.6569812297821045
RMSE train: 1.795894	val: 5.694818	test: 4.726869
MAE train: 1.496312	val: 4.142265	test: 3.950663

Epoch: 41
Loss: 2.3254355788230896
RMSE train: 1.718118	val: 5.623560	test: 4.643694
MAE train: 1.421688	val: 4.018285	test: 3.870995

Epoch: 42
Loss: 2.195868134498596
RMSE train: 1.627292	val: 5.535894	test: 4.540394
MAE train: 1.330157	val: 3.884206	test: 3.775015

Epoch: 43
Loss: 2.2507333755493164
RMSE train: 1.601480	val: 5.536377	test: 4.510087
MAE train: 1.302683	val: 3.864786	test: 3.736934

Epoch: 44
Loss: 1.9208043813705444
RMSE train: 1.566597	val: 5.539943	test: 4.458339
MAE train: 1.271075	val: 3.854611	test: 3.671533

Epoch: 45
Loss: 1.9741831421852112
RMSE train: 1.510854	val: 5.481036	test: 4.420943
MAE train: 1.223149	val: 3.822511	test: 3.636256

Epoch: 46
Loss: 1.9287607073783875
RMSE train: 1.444445	val: 5.421416	test: 4.397901
MAE train: 1.167102	val: 3.806846	test: 3.618479

Epoch: 47
Loss: 1.7459547519683838
RMSE train: 1.358249	val: 5.294624	test: 4.336618
MAE train: 1.096864	val: 3.723518	test: 3.561230

Epoch: 48
Loss: 1.8215550780296326
RMSE train: 1.304034	val: 5.248092	test: 4.307427
MAE train: 1.051546	val: 3.696500	test: 3.535276

Epoch: 49
Loss: 1.6747377514839172
RMSE train: 1.297386	val: 5.247495	test: 4.312625
MAE train: 1.048460	val: 3.724449	test: 3.537853

Epoch: 50
Loss: 1.6194571256637573
RMSE train: 1.249894	val: 5.193947	test: 4.224302
MAE train: 1.007544	val: 3.680457	test: 3.443435

Epoch: 51
Loss: 1.6274043321609497
RMSE train: 1.253015	val: 5.201567	test: 4.129472
MAE train: 1.006405	val: 3.660120	test: 3.343930

Epoch: 52
Loss: 1.495495080947876
RMSE train: 1.209582	val: 5.161854	test: 4.076863
MAE train: 0.959439	val: 3.634306	test: 3.296055

Epoch: 53
Loss: 1.6782439947128296
RMSE train: 1.174249	val: 5.098087	test: 4.070645
MAE train: 0.928590	val: 3.593018	test: 3.299231

Epoch: 54
Loss: 1.4998549222946167
RMSE train: 1.135470	val: 5.017130	test: 4.046379
MAE train: 0.895446	val: 3.538589	test: 3.281264

Epoch: 55
Loss: 1.6461780071258545
RMSE train: 1.076760	val: 4.935131	test: 3.982221
MAE train: 0.845891	val: 3.470537	test: 3.217644

Epoch: 56
Loss: 1.4525213241577148
RMSE train: 1.029061	val: 4.878364	test: 3.947622
MAE train: 0.809024	val: 3.429432	test: 3.183401

Epoch: 57
Loss: 1.3154236674308777
RMSE train: 1.017470	val: 4.882890	test: 3.935132
MAE train: 0.798944	val: 3.429197	test: 3.161460

Epoch: 58
Loss: 1.5545989871025085
RMSE train: 1.000699	val: 4.863411	test: 3.928357
MAE train: 0.785044	val: 3.421159	test: 3.151220

Epoch: 59
Loss: 1.3105026483535767
RMSE train: 1.017772	val: 4.893920	test: 3.947300
MAE train: 0.798561	val: 3.467671	test: 3.176118

Epoch: 60
Loss: 1.3135153651237488
RMSE train: 1.041398	val: 4.947536	test: 3.956433
MAE train: 0.816725	val: 3.529274	test: 3.190248

Epoch: 61
Loss: 1.2994747161865234
RMSE train: 1.040116	val: 4.977980	test: 3.935918
MAE train: 0.809569	val: 3.548781	test: 3.168243

Epoch: 62
Loss: 1.2720987200737
RMSE train: 1.002273	val: 4.952455	test: 3.900700
MAE train: 0.774632	val: 3.513043	test: 3.136925

Epoch: 63
Loss: 1.4133239388465881
RMSE train: 0.985171	val: 4.941728	test: 3.880807
MAE train: 0.761130	val: 3.480540	test: 3.127084

Epoch: 64
Loss: 1.335098147392273
RMSE train: 0.979658	val: 4.927590	test: 3.861430
MAE train: 0.755479	val: 3.432377	test: 3.103632

Epoch: 65
Loss: 1.2706488966941833
RMSE train: 0.977930	val: 4.909772	test: 3.873715
MAE train: 0.759228	val: 3.401101	test: 3.118089

Epoch: 66
Loss: 1.1825582385063171
RMSE train: 0.922753	val: 4.823733	test: 3.853293
MAE train: 0.723173	val: 3.329757	test: 3.104865

Epoch: 67
Loss: 1.2568886876106262
RMSE train: 0.867622	val: 4.742679	test: 3.793759
MAE train: 0.681387	val: 3.281441	test: 3.058134

Epoch: 68
Loss: 1.149506688117981
RMSE train: 0.875377	val: 4.766722	test: 3.774800
MAE train: 0.688681	val: 3.300049	test: 3.021657

Epoch: 69
Loss: 1.0279178619384766
RMSE train: 0.856977	val: 4.748931	test: 3.717433
MAE train: 0.674907	val: 3.288388	test: 2.958551

Epoch: 70
Loss: 1.3014245629310608
RMSE train: 0.845014	val: 4.705269	test: 3.647786
MAE train: 0.662150	val: 3.250709	test: 2.883610

Epoch: 71
Loss: 1.0665670037269592
RMSE train: 0.859225	val: 4.696840	test: 3.623768
MAE train: 0.669379	val: 3.241964	test: 2.859379

Epoch: 72
Loss: 1.1853289604187012
RMSE train: 0.859275	val: 4.687123	test: 3.609046
MAE train: 0.669009	val: 3.256176	test: 2.856029

Epoch: 73
Loss: 1.1375802159309387
RMSE train: 0.840598	val: 4.673785	test: 3.589657
MAE train: 0.652326	val: 3.277047	test: 2.853272

Epoch: 74
Loss: 1.0876616835594177
RMSE train: 0.868340	val: 4.725702	test: 3.591614
MAE train: 0.666423	val: 3.343907	test: 2.865921

Epoch: 75
Loss: 1.1163365244865417
RMSE train: 0.905864	val: 4.765080	test: 3.619084
MAE train: 0.694711	val: 3.364530	test: 2.883098

Epoch: 76
Loss: 1.0995176434516907
RMSE train: 0.937632	val: 4.771331	test: 3.652515
MAE train: 0.718315	val: 3.359933	test: 2.908371

Epoch: 77
Loss: 1.099629282951355
RMSE train: 0.883345	val: 4.672427	test: 3.620403
MAE train: 0.675497	val: 3.275187	test: 2.890005

Epoch: 78
Loss: 0.9101602435112
RMSE train: 0.807678	val: 4.561710	test: 3.576320
MAE train: 0.617454	val: 3.180985	test: 2.862192

Epoch: 79
Loss: 0.926388680934906
RMSE train: 0.755987	val: 4.500864	test: 3.549206
MAE train: 0.583701	val: 3.123094	test: 2.838141

Epoch: 80
Loss: 1.1984460353851318
RMSE train: 0.752574	val: 4.517040	test: 3.546860
MAE train: 0.581764	val: 3.122985	test: 2.832961

Epoch: 81
Loss: 1.2016689777374268
RMSE train: 0.802382	val: 4.607800	test: 3.581356
MAE train: 0.622311	val: 3.173120	test: 2.862514

Epoch: 82
Loss: 0.9291012287139893
RMSE train: 0.877109	val: 4.746645	test: 3.615675
MAE train: 0.684522	val: 3.267702	test: 2.880026

Epoch: 83
Loss: 0.9228588938713074
RMSE train: 3.173762	val: 6.971627	test: 6.142395
MAE train: 2.802313	val: 5.778128	test: 5.499933

Epoch: 23
Loss: 6.953169822692871
RMSE train: 3.074492	val: 6.809770	test: 6.093558
MAE train: 2.709159	val: 5.615308	test: 5.442585

Epoch: 24
Loss: 6.706954479217529
RMSE train: 2.985716	val: 6.684948	test: 6.036387
MAE train: 2.626422	val: 5.464463	test: 5.372791

Epoch: 25
Loss: 6.487184762954712
RMSE train: 2.918186	val: 6.646490	test: 5.983907
MAE train: 2.566507	val: 5.383664	test: 5.311389

Epoch: 26
Loss: 5.8560950756073
RMSE train: 2.845228	val: 6.625068	test: 5.930185
MAE train: 2.497195	val: 5.308928	test: 5.252601

Epoch: 27
Loss: 5.556220531463623
RMSE train: 2.795360	val: 6.642110	test: 5.895382
MAE train: 2.449468	val: 5.292752	test: 5.216263

Epoch: 28
Loss: 5.1264543533325195
RMSE train: 2.739421	val: 6.619177	test: 5.868783
MAE train: 2.395083	val: 5.272076	test: 5.190314

Epoch: 29
Loss: 5.311452150344849
RMSE train: 2.690840	val: 6.610172	test: 5.848079
MAE train: 2.348261	val: 5.275578	test: 5.166282

Epoch: 30
Loss: 4.755736589431763
RMSE train: 2.652530	val: 6.601887	test: 5.774635
MAE train: 2.314389	val: 5.217297	test: 5.084648

Epoch: 31
Loss: 4.363152980804443
RMSE train: 2.606672	val: 6.568510	test: 5.685052
MAE train: 2.274726	val: 5.104047	test: 4.977413

Epoch: 32
Loss: 4.4645349979400635
RMSE train: 2.537222	val: 6.514110	test: 5.570895
MAE train: 2.211177	val: 4.967767	test: 4.839122

Epoch: 33
Loss: 3.9774266481399536
RMSE train: 2.468129	val: 6.456850	test: 5.483890
MAE train: 2.145495	val: 4.859843	test: 4.736658

Epoch: 34
Loss: 3.7965372800827026
RMSE train: 2.396307	val: 6.402707	test: 5.402006
MAE train: 2.075444	val: 4.786403	test: 4.646571

Epoch: 35
Loss: 3.675912380218506
RMSE train: 2.326027	val: 6.354090	test: 5.295486
MAE train: 2.006112	val: 4.730262	test: 4.538534

Epoch: 36
Loss: 3.8331819772720337
RMSE train: 2.246760	val: 6.275167	test: 5.218386
MAE train: 1.924312	val: 4.684644	test: 4.466304

Epoch: 37
Loss: 3.231316566467285
RMSE train: 2.129264	val: 6.130711	test: 5.079958
MAE train: 1.804927	val: 4.597217	test: 4.327852

Epoch: 38
Loss: 2.993232846260071
RMSE train: 2.017916	val: 5.983088	test: 4.930566
MAE train: 1.692656	val: 4.487247	test: 4.177783

Epoch: 39
Loss: 2.8343364000320435
RMSE train: 1.921909	val: 5.837480	test: 4.798884
MAE train: 1.595569	val: 4.346901	test: 4.051125

Epoch: 40
Loss: 3.0097495317459106
RMSE train: 1.817192	val: 5.672462	test: 4.667747
MAE train: 1.495491	val: 4.176065	test: 3.928365

Epoch: 41
Loss: 2.690125286579132
RMSE train: 1.761248	val: 5.602925	test: 4.580986
MAE train: 1.443607	val: 4.084005	test: 3.856362

Epoch: 42
Loss: 2.2741492986679077
RMSE train: 1.708815	val: 5.542796	test: 4.476107
MAE train: 1.395080	val: 4.007770	test: 3.761478

Epoch: 43
Loss: 2.3326929807662964
RMSE train: 1.652847	val: 5.480620	test: 4.349227
MAE train: 1.343179	val: 3.940047	test: 3.633686

Epoch: 44
Loss: 2.316363573074341
RMSE train: 1.597730	val: 5.444478	test: 4.224492
MAE train: 1.287420	val: 3.883889	test: 3.498887

Epoch: 45
Loss: 2.1508184671401978
RMSE train: 1.534309	val: 5.382109	test: 4.125255
MAE train: 1.227935	val: 3.819019	test: 3.381273

Epoch: 46
Loss: 2.0970197916030884
RMSE train: 1.426595	val: 5.273759	test: 3.994347
MAE train: 1.134722	val: 3.738769	test: 3.235943

Epoch: 47
Loss: 1.8510692119598389
RMSE train: 1.297633	val: 5.083765	test: 3.859921
MAE train: 1.026242	val: 3.612477	test: 3.109407

Epoch: 48
Loss: 1.964322030544281
RMSE train: 1.212121	val: 4.952988	test: 3.788473
MAE train: 0.958641	val: 3.538637	test: 3.052664

Epoch: 49
Loss: 1.9253021478652954
RMSE train: 1.211086	val: 4.900590	test: 3.809675
MAE train: 0.963260	val: 3.519091	test: 3.086423

Epoch: 50
Loss: 1.9055254459381104
RMSE train: 1.235027	val: 4.887576	test: 3.830314
MAE train: 0.983130	val: 3.523416	test: 3.096373

Epoch: 51
Loss: 1.8274438381195068
RMSE train: 1.271573	val: 4.944091	test: 3.853684
MAE train: 1.014972	val: 3.581388	test: 3.106820

Epoch: 52
Loss: 1.6682433485984802
RMSE train: 1.259263	val: 4.942616	test: 3.864071
MAE train: 1.006237	val: 3.606102	test: 3.134479

Epoch: 53
Loss: 1.714384377002716
RMSE train: 1.228295	val: 4.893022	test: 3.861298
MAE train: 0.979396	val: 3.598366	test: 3.157975

Epoch: 54
Loss: 1.7371114492416382
RMSE train: 1.192796	val: 4.873744	test: 3.831625
MAE train: 0.949219	val: 3.595366	test: 3.144350

Epoch: 55
Loss: 1.671546995639801
RMSE train: 1.134939	val: 4.884875	test: 3.773995
MAE train: 0.903460	val: 3.600020	test: 3.090102

Epoch: 56
Loss: 1.5037859678268433
RMSE train: 1.084001	val: 4.895706	test: 3.724980
MAE train: 0.860188	val: 3.605449	test: 3.036322

Epoch: 57
Loss: 1.4921457171440125
RMSE train: 1.052814	val: 4.897603	test: 3.687727
MAE train: 0.830797	val: 3.608351	test: 2.987366

Epoch: 58
Loss: 1.5762279033660889
RMSE train: 1.028317	val: 4.870927	test: 3.661193
MAE train: 0.807506	val: 3.591444	test: 2.951154

Epoch: 59
Loss: 1.3901251554489136
RMSE train: 1.044669	val: 4.904324	test: 3.688607
MAE train: 0.818106	val: 3.605014	test: 2.965863

Epoch: 60
Loss: 1.6556742191314697
RMSE train: 1.023980	val: 4.871924	test: 3.687303
MAE train: 0.803785	val: 3.574798	test: 2.957104

Epoch: 61
Loss: 1.4410703778266907
RMSE train: 1.007589	val: 4.848101	test: 3.657227
MAE train: 0.789325	val: 3.546047	test: 2.922959

Epoch: 62
Loss: 1.3513309359550476
RMSE train: 0.967825	val: 4.797436	test: 3.591695
MAE train: 0.757698	val: 3.502759	test: 2.859781

Epoch: 63
Loss: 1.5287979245185852
RMSE train: 0.948993	val: 4.782423	test: 3.541536
MAE train: 0.743723	val: 3.488047	test: 2.813587

Epoch: 64
Loss: 1.4141221046447754
RMSE train: 0.951787	val: 4.817113	test: 3.521539
MAE train: 0.746669	val: 3.509634	test: 2.783384

Epoch: 65
Loss: 1.364965558052063
RMSE train: 0.986108	val: 4.874172	test: 3.543167
MAE train: 0.775274	val: 3.536655	test: 2.791502

Epoch: 66
Loss: 1.1833083629608154
RMSE train: 0.981120	val: 4.866443	test: 3.543444
MAE train: 0.772667	val: 3.507336	test: 2.790438

Epoch: 67
Loss: 1.3579686880111694
RMSE train: 0.934751	val: 4.808986	test: 3.512963
MAE train: 0.740130	val: 3.447178	test: 2.768359

Epoch: 68
Loss: 1.2836384773254395
RMSE train: 0.934520	val: 4.807462	test: 3.550609
MAE train: 0.743239	val: 3.435458	test: 2.815594

Epoch: 69
Loss: 1.2435839176177979
RMSE train: 0.931238	val: 4.799150	test: 3.580383
MAE train: 0.745064	val: 3.427860	test: 2.859012

Epoch: 70
Loss: 1.3556748032569885
RMSE train: 0.930347	val: 4.809524	test: 3.618478
MAE train: 0.742978	val: 3.446303	test: 2.910816

Epoch: 71
Loss: 1.2267274260520935
RMSE train: 0.949992	val: 4.815272	test: 3.643715
MAE train: 0.754164	val: 3.466759	test: 2.937861

Epoch: 72
Loss: 1.2166423201560974
RMSE train: 0.962476	val: 4.792336	test: 3.615049
MAE train: 0.763605	val: 3.468456	test: 2.905472

Epoch: 73
Loss: 1.116070032119751
RMSE train: 0.919196	val: 4.716342	test: 3.585185
MAE train: 0.730835	val: 3.429444	test: 2.885341

Epoch: 74
Loss: 1.0405826568603516
RMSE train: 0.880815	val: 4.656192	test: 3.512231
MAE train: 0.698496	val: 3.398773	test: 2.820021

Epoch: 75
Loss: 1.0905884504318237
RMSE train: 0.892694	val: 4.657986	test: 3.505713
MAE train: 0.710346	val: 3.400953	test: 2.816398

Epoch: 76
Loss: 1.1844195127487183
RMSE train: 0.918294	val: 4.666725	test: 3.534463
MAE train: 0.734661	val: 3.405767	test: 2.843801

Epoch: 77
Loss: 1.0591234862804413
RMSE train: 0.921617	val: 4.634945	test: 3.542148
MAE train: 0.744858	val: 3.383122	test: 2.850287

Epoch: 78
Loss: 1.1545965671539307
RMSE train: 0.929044	val: 4.649945	test: 3.521511
MAE train: 0.751253	val: 3.381057	test: 2.823583

Epoch: 79
Loss: 1.0288369059562683
RMSE train: 0.894037	val: 4.617759	test: 3.470425
MAE train: 0.720412	val: 3.337654	test: 2.762150

Epoch: 80
Loss: 1.163614273071289
RMSE train: 0.880924	val: 4.585697	test: 3.444745
MAE train: 0.710912	val: 3.287180	test: 2.725061

Epoch: 81
Loss: 1.0225336253643036
RMSE train: 0.893421	val: 4.586617	test: 3.436501
MAE train: 0.727302	val: 3.265399	test: 2.720187

Epoch: 82
Loss: 1.0665320754051208
RMSE train: 0.945705	val: 4.667509	test: 3.471405
MAE train: 0.777341	val: 3.305188	test: 2.769750

Epoch: 83
Loss: 1.055541306734085
RMSE train: 3.211805	val: 6.750929	test: 5.662008
MAE train: 2.705920	val: 5.404104	test: 5.028791

Epoch: 23
Loss: 8.04659628868103
RMSE train: 3.162024	val: 6.654919	test: 5.579474
MAE train: 2.675004	val: 5.306924	test: 4.935442

Epoch: 24
Loss: 7.6505889892578125
RMSE train: 3.111295	val: 6.600367	test: 5.499457
MAE train: 2.644311	val: 5.212747	test: 4.840031

Epoch: 25
Loss: 6.931183576583862
RMSE train: 3.085714	val: 6.570307	test: 5.446925
MAE train: 2.637485	val: 5.145309	test: 4.778357

Epoch: 26
Loss: 6.958927631378174
RMSE train: 3.036502	val: 6.524566	test: 5.371974
MAE train: 2.604547	val: 5.036545	test: 4.685515

Epoch: 27
Loss: 6.488098382949829
RMSE train: 2.996641	val: 6.481176	test: 5.305634
MAE train: 2.576009	val: 4.926115	test: 4.598655

Epoch: 28
Loss: 5.833815336227417
RMSE train: 2.898841	val: 6.368178	test: 5.161096
MAE train: 2.486624	val: 4.741529	test: 4.422845

Epoch: 29
Loss: 5.541997671127319
RMSE train: 2.793774	val: 6.263835	test: 5.021878
MAE train: 2.391534	val: 4.605635	test: 4.251157

Epoch: 30
Loss: 5.548261642456055
RMSE train: 2.674488	val: 6.143998	test: 4.870768
MAE train: 2.281242	val: 4.496325	test: 4.066927

Epoch: 31
Loss: 4.955705881118774
RMSE train: 2.517474	val: 6.024161	test: 4.721777
MAE train: 2.131361	val: 4.395345	test: 3.912427

Epoch: 32
Loss: 4.878645896911621
RMSE train: 2.404539	val: 5.929913	test: 4.604487
MAE train: 2.025116	val: 4.324719	test: 3.808952

Epoch: 33
Loss: 4.6985766887664795
RMSE train: 2.300505	val: 5.856958	test: 4.487408
MAE train: 1.922486	val: 4.264529	test: 3.714078

Epoch: 34
Loss: 4.562747478485107
RMSE train: 2.255259	val: 5.830130	test: 4.429311
MAE train: 1.878582	val: 4.235739	test: 3.674644

Epoch: 35
Loss: 4.063191771507263
RMSE train: 2.283406	val: 5.896169	test: 4.438533
MAE train: 1.904028	val: 4.265230	test: 3.689830

Epoch: 36
Loss: 3.846070647239685
RMSE train: 2.296050	val: 5.935653	test: 4.432865
MAE train: 1.919001	val: 4.264885	test: 3.674312

Epoch: 37
Loss: 3.8572555780410767
RMSE train: 2.220892	val: 5.902137	test: 4.341372
MAE train: 1.850931	val: 4.188476	test: 3.568082

Epoch: 38
Loss: 3.5163484811782837
RMSE train: 2.120867	val: 5.835951	test: 4.238408
MAE train: 1.763830	val: 4.106836	test: 3.442184

Epoch: 39
Loss: 3.348634123802185
RMSE train: 2.015080	val: 5.707873	test: 4.104735
MAE train: 1.674999	val: 3.984030	test: 3.280638

Epoch: 40
Loss: 2.9436413049697876
RMSE train: 1.929697	val: 5.580227	test: 3.991227
MAE train: 1.599872	val: 3.872247	test: 3.155201

Epoch: 41
Loss: 2.732645630836487
RMSE train: 1.849790	val: 5.449450	test: 3.876802
MAE train: 1.526533	val: 3.749083	test: 3.037034

Epoch: 42
Loss: 3.20108425617218
RMSE train: 1.803311	val: 5.361662	test: 3.779223
MAE train: 1.481824	val: 3.664444	test: 2.968859

Epoch: 43
Loss: 2.9798059463500977
RMSE train: 1.751087	val: 5.288484	test: 3.675160
MAE train: 1.428970	val: 3.593948	test: 2.891377

Epoch: 44
Loss: 2.6304819583892822
RMSE train: 1.657763	val: 5.213195	test: 3.560563
MAE train: 1.336072	val: 3.531719	test: 2.795474

Epoch: 45
Loss: 2.8109201192855835
RMSE train: 1.562103	val: 5.177859	test: 3.472777
MAE train: 1.247685	val: 3.512428	test: 2.705138

Epoch: 46
Loss: 2.2879698276519775
RMSE train: 1.443102	val: 5.128634	test: 3.384050
MAE train: 1.136809	val: 3.480139	test: 2.618913

Epoch: 47
Loss: 2.397971272468567
RMSE train: 1.371704	val: 5.128975	test: 3.355523
MAE train: 1.072053	val: 3.481859	test: 2.594035

Epoch: 48
Loss: 2.359447479248047
RMSE train: 1.315351	val: 5.119237	test: 3.346100
MAE train: 1.023575	val: 3.486917	test: 2.588176

Epoch: 49
Loss: 1.9772339463233948
RMSE train: 1.266256	val: 5.067942	test: 3.334977
MAE train: 0.983599	val: 3.473355	test: 2.581683

Epoch: 50
Loss: 2.1757659912109375
RMSE train: 1.211956	val: 5.010839	test: 3.332459
MAE train: 0.940134	val: 3.453588	test: 2.583482

Epoch: 51
Loss: 2.242549419403076
RMSE train: 1.190477	val: 4.986065	test: 3.339962
MAE train: 0.921729	val: 3.438998	test: 2.603157

Epoch: 52
Loss: 1.9795396327972412
RMSE train: 1.174021	val: 4.981200	test: 3.314275
MAE train: 0.901501	val: 3.418130	test: 2.571304

Epoch: 53
Loss: 1.9492425322532654
RMSE train: 1.154511	val: 4.974019	test: 3.276693
MAE train: 0.875889	val: 3.384869	test: 2.516183

Epoch: 54
Loss: 2.126333475112915
RMSE train: 1.167639	val: 5.052601	test: 3.289482
MAE train: 0.891325	val: 3.400575	test: 2.510283

Epoch: 55
Loss: 1.8731865286827087
RMSE train: 1.155233	val: 5.095222	test: 3.290121
MAE train: 0.890497	val: 3.410222	test: 2.508881

Epoch: 56
Loss: 1.694704830646515
RMSE train: 1.134853	val: 5.082932	test: 3.250482
MAE train: 0.876087	val: 3.382504	test: 2.475922

Epoch: 57
Loss: 1.7189244627952576
RMSE train: 1.108074	val: 5.062783	test: 3.202886
MAE train: 0.857057	val: 3.355642	test: 2.430587

Epoch: 58
Loss: 1.752677083015442
RMSE train: 1.057744	val: 5.041910	test: 3.138468
MAE train: 0.816455	val: 3.346305	test: 2.392612

Epoch: 59
Loss: 1.7734200954437256
RMSE train: 1.012576	val: 5.073631	test: 3.103380
MAE train: 0.787319	val: 3.369121	test: 2.365312

Epoch: 60
Loss: 1.6328274011611938
RMSE train: 0.980792	val: 5.070623	test: 3.078461
MAE train: 0.766956	val: 3.369512	test: 2.342480

Epoch: 61
Loss: 1.6316952109336853
RMSE train: 0.968266	val: 5.059861	test: 3.074784
MAE train: 0.754656	val: 3.367827	test: 2.337287

Epoch: 62
Loss: 1.5509597063064575
RMSE train: 0.981054	val: 5.065570	test: 3.091896
MAE train: 0.767754	val: 3.362844	test: 2.341627

Epoch: 63
Loss: 1.6054600477218628
RMSE train: 0.985638	val: 5.059185	test: 3.104399
MAE train: 0.773076	val: 3.353377	test: 2.344034

Epoch: 64
Loss: 1.4557003378868103
RMSE train: 0.994074	val: 5.070723	test: 3.125660
MAE train: 0.787794	val: 3.361288	test: 2.357279

Epoch: 65
Loss: 1.5522595047950745
RMSE train: 0.982064	val: 5.124553	test: 3.163436
MAE train: 0.789988	val: 3.393031	test: 2.403293

Epoch: 66
Loss: 1.5423992276191711
RMSE train: 1.005910	val: 5.194620	test: 3.221374
MAE train: 0.816986	val: 3.449970	test: 2.462155

Epoch: 67
Loss: 1.5994172096252441
RMSE train: 1.040890	val: 5.285559	test: 3.275749
MAE train: 0.845878	val: 3.514146	test: 2.502325

Epoch: 68
Loss: 1.3935860395431519
RMSE train: 1.035817	val: 5.310207	test: 3.299409
MAE train: 0.844320	val: 3.539527	test: 2.530230

Epoch: 69
Loss: 1.5267012119293213
RMSE train: 0.998522	val: 5.280566	test: 3.282398
MAE train: 0.811388	val: 3.518792	test: 2.520754

Epoch: 70
Loss: 1.5921190977096558
RMSE train: 0.963615	val: 5.250185	test: 3.277901
MAE train: 0.777451	val: 3.500766	test: 2.503960

Epoch: 71
Loss: 1.3523426055908203
RMSE train: 0.966374	val: 5.172365	test: 3.261926
MAE train: 0.767791	val: 3.452799	test: 2.486202

Epoch: 72
Loss: 1.3561946153640747
RMSE train: 0.984281	val: 5.119074	test: 3.264207
MAE train: 0.783506	val: 3.412517	test: 2.490518

Epoch: 73
Loss: 1.2973400950431824
RMSE train: 0.966322	val: 5.088826	test: 3.229627
MAE train: 0.775470	val: 3.383288	test: 2.468312

Epoch: 74
Loss: 1.4077128171920776
RMSE train: 0.925562	val: 5.075915	test: 3.190520
MAE train: 0.742671	val: 3.351234	test: 2.451737

Epoch: 75
Loss: 1.291850507259369
RMSE train: 0.913227	val: 5.092153	test: 3.163945
MAE train: 0.734082	val: 3.343481	test: 2.432867

Epoch: 76
Loss: 1.28350430727005
RMSE train: 0.928187	val: 5.137830	test: 3.145508
MAE train: 0.745053	val: 3.358723	test: 2.417384

Epoch: 77
Loss: 1.312196969985962
RMSE train: 0.970684	val: 5.186823	test: 3.158218
MAE train: 0.782954	val: 3.389998	test: 2.435157

Epoch: 78
Loss: 1.5048763155937195
RMSE train: 0.963226	val: 5.181386	test: 3.160799
MAE train: 0.782821	val: 3.404079	test: 2.449411

Epoch: 79
Loss: 1.34869784116745
RMSE train: 0.921510	val: 5.102928	test: 3.125584
MAE train: 0.749122	val: 3.339291	test: 2.473348

Epoch: 80
Loss: 1.4168596863746643
RMSE train: 0.893441	val: 5.029599	test: 3.105237
MAE train: 0.721001	val: 3.269038	test: 2.507527

Epoch: 81
Loss: 1.2296964526176453
RMSE train: 0.906533	val: 5.032783	test: 3.126500
MAE train: 0.723053	val: 3.263418	test: 2.548879

Epoch: 82
Loss: 1.2228561043739319
RMSE train: 0.901965	val: 5.067445	test: 3.142566
MAE train: 0.721057	val: 3.293362	test: 2.550015

Epoch: 83
Loss: 1.0656890273094177
RMSE train: 2.941132	val: 6.990809	test: 6.216887
MAE train: 2.511068	val: 5.912196	test: 5.625003

Epoch: 23
Loss: 7.20004940032959
RMSE train: 2.890213	val: 6.927455	test: 6.192025
MAE train: 2.472352	val: 5.850032	test: 5.602746

Epoch: 24
Loss: 6.658120393753052
RMSE train: 2.809658	val: 6.822981	test: 6.132125
MAE train: 2.405165	val: 5.743000	test: 5.542133

Epoch: 25
Loss: 6.223779916763306
RMSE train: 2.727549	val: 6.732283	test: 6.067805
MAE train: 2.335171	val: 5.654364	test: 5.475457

Epoch: 26
Loss: 5.744586944580078
RMSE train: 2.661219	val: 6.633693	test: 5.996635
MAE train: 2.278650	val: 5.559942	test: 5.405911

Epoch: 27
Loss: 5.676541566848755
RMSE train: 2.574108	val: 6.494895	test: 5.909636
MAE train: 2.199649	val: 5.448915	test: 5.325359

Epoch: 28
Loss: 5.283769369125366
RMSE train: 2.480033	val: 6.351049	test: 5.806867
MAE train: 2.108806	val: 5.324619	test: 5.222203

Epoch: 29
Loss: 4.903205871582031
RMSE train: 2.423969	val: 6.232920	test: 5.723185
MAE train: 2.056652	val: 5.213196	test: 5.140098

Epoch: 30
Loss: 4.871889591217041
RMSE train: 2.349287	val: 6.092663	test: 5.638232
MAE train: 1.989494	val: 5.078043	test: 5.048148

Epoch: 31
Loss: 4.705754518508911
RMSE train: 2.291991	val: 6.008764	test: 5.568183
MAE train: 1.939283	val: 4.984945	test: 4.968900

Epoch: 32
Loss: 4.423972129821777
RMSE train: 2.299966	val: 6.032272	test: 5.524653
MAE train: 1.946844	val: 4.972111	test: 4.918610

Epoch: 33
Loss: 4.439492702484131
RMSE train: 2.294437	val: 6.076972	test: 5.484641
MAE train: 1.937036	val: 4.979063	test: 4.863328

Epoch: 34
Loss: 4.204780578613281
RMSE train: 2.283781	val: 6.098638	test: 5.439174
MAE train: 1.915747	val: 4.961118	test: 4.798168

Epoch: 35
Loss: 3.3796157836914062
RMSE train: 2.221495	val: 6.071114	test: 5.389566
MAE train: 1.855649	val: 4.916878	test: 4.736867

Epoch: 36
Loss: 3.5579639673233032
RMSE train: 2.114402	val: 6.006254	test: 5.359007
MAE train: 1.762697	val: 4.874799	test: 4.699030

Epoch: 37
Loss: 3.296910047531128
RMSE train: 2.012650	val: 5.945866	test: 5.337817
MAE train: 1.670926	val: 4.832610	test: 4.670130

Epoch: 38
Loss: 3.077857255935669
RMSE train: 1.899694	val: 5.836289	test: 5.283749
MAE train: 1.566672	val: 4.720597	test: 4.612012

Epoch: 39
Loss: 2.942090392112732
RMSE train: 1.802294	val: 5.681518	test: 5.171445
MAE train: 1.475970	val: 4.527252	test: 4.488762

Epoch: 40
Loss: 2.9095369577407837
RMSE train: 1.749635	val: 5.550119	test: 5.091632
MAE train: 1.428071	val: 4.387926	test: 4.402723

Epoch: 41
Loss: 2.432416319847107
RMSE train: 1.735502	val: 5.484889	test: 5.028250
MAE train: 1.413781	val: 4.327361	test: 4.339604

Epoch: 42
Loss: 2.7327513694763184
RMSE train: 1.721118	val: 5.456874	test: 4.992786
MAE train: 1.402975	val: 4.303037	test: 4.308539

Epoch: 43
Loss: 2.423883080482483
RMSE train: 1.703316	val: 5.414307	test: 4.935476
MAE train: 1.387219	val: 4.258414	test: 4.257305

Epoch: 44
Loss: 2.4419147968292236
RMSE train: 1.659940	val: 5.342114	test: 4.838376
MAE train: 1.344686	val: 4.174167	test: 4.167093

Epoch: 45
Loss: 2.22072434425354
RMSE train: 1.569104	val: 5.205223	test: 4.706082
MAE train: 1.261510	val: 4.015659	test: 4.041210

Epoch: 46
Loss: 2.098092794418335
RMSE train: 1.431302	val: 5.020248	test: 4.566158
MAE train: 1.138412	val: 3.824222	test: 3.910166

Epoch: 47
Loss: 2.1292924284934998
RMSE train: 1.320124	val: 4.873881	test: 4.440689
MAE train: 1.040384	val: 3.692425	test: 3.783710

Epoch: 48
Loss: 1.899990737438202
RMSE train: 1.251592	val: 4.775421	test: 4.342499
MAE train: 0.977240	val: 3.574516	test: 3.676543

Epoch: 49
Loss: 1.8933043479919434
RMSE train: 1.207591	val: 4.735201	test: 4.284514
MAE train: 0.936753	val: 3.515597	test: 3.604308

Epoch: 50
Loss: 1.730310320854187
RMSE train: 1.166969	val: 4.716589	test: 4.251389
MAE train: 0.901240	val: 3.472266	test: 3.554234

Epoch: 51
Loss: 1.8346566557884216
RMSE train: 1.123475	val: 4.712514	test: 4.236951
MAE train: 0.867532	val: 3.454900	test: 3.524050

Epoch: 52
Loss: 1.793453574180603
RMSE train: 1.115673	val: 4.782509	test: 4.257777
MAE train: 0.868020	val: 3.508429	test: 3.547404

Epoch: 53
Loss: 1.853228211402893
RMSE train: 1.136860	val: 4.875765	test: 4.308815
MAE train: 0.891369	val: 3.612659	test: 3.611893

Epoch: 54
Loss: 1.8473994135856628
RMSE train: 1.191677	val: 4.995101	test: 4.392633
MAE train: 0.940983	val: 3.740203	test: 3.701971

Epoch: 55
Loss: 1.7739668488502502
RMSE train: 1.232432	val: 5.043913	test: 4.440035
MAE train: 0.968927	val: 3.793167	test: 3.750204

Epoch: 56
Loss: 1.7016489505767822
RMSE train: 1.199713	val: 4.971642	test: 4.411180
MAE train: 0.935924	val: 3.725636	test: 3.713277

Epoch: 57
Loss: 1.6702898144721985
RMSE train: 1.148031	val: 4.909013	test: 4.367724
MAE train: 0.895922	val: 3.634717	test: 3.646708

Epoch: 58
Loss: 1.5906934142112732
RMSE train: 1.061013	val: 4.800766	test: 4.277634
MAE train: 0.831679	val: 3.524910	test: 3.534929

Epoch: 59
Loss: 1.3925482630729675
RMSE train: 1.011313	val: 4.741848	test: 4.239809
MAE train: 0.796011	val: 3.474371	test: 3.491718

Epoch: 60
Loss: 1.3450028896331787
RMSE train: 0.989558	val: 4.695350	test: 4.207402
MAE train: 0.774914	val: 3.415778	test: 3.460969

Epoch: 61
Loss: 1.5873985886573792
RMSE train: 0.991558	val: 4.700099	test: 4.214910
MAE train: 0.772205	val: 3.410405	test: 3.475128

Epoch: 62
Loss: 1.299027144908905
RMSE train: 1.012522	val: 4.775614	test: 4.262189
MAE train: 0.792545	val: 3.474757	test: 3.529650

Epoch: 63
Loss: 1.3141366839408875
RMSE train: 1.025021	val: 4.812537	test: 4.257683
MAE train: 0.803442	val: 3.504729	test: 3.524045

Epoch: 64
Loss: 1.406046211719513
RMSE train: 1.012041	val: 4.804107	test: 4.221949
MAE train: 0.794304	val: 3.469281	test: 3.487469

Epoch: 65
Loss: 1.4025225639343262
RMSE train: 0.962252	val: 4.749733	test: 4.148614
MAE train: 0.752188	val: 3.416495	test: 3.409160

Epoch: 66
Loss: 1.303430438041687
RMSE train: 0.910466	val: 4.675250	test: 4.075335
MAE train: 0.714032	val: 3.389115	test: 3.332308

Epoch: 67
Loss: 1.5049435496330261
RMSE train: 0.867092	val: 4.597711	test: 4.012349
MAE train: 0.682025	val: 3.343006	test: 3.270165

Epoch: 68
Loss: 1.4640686511993408
RMSE train: 0.837912	val: 4.530133	test: 3.971036
MAE train: 0.659649	val: 3.299998	test: 3.245080

Epoch: 69
Loss: 1.2882145047187805
RMSE train: 0.799982	val: 4.433528	test: 3.904513
MAE train: 0.627957	val: 3.233603	test: 3.201854

Epoch: 70
Loss: 1.3614553809165955
RMSE train: 0.797353	val: 4.283865	test: 3.814409
MAE train: 0.616954	val: 3.162489	test: 3.137118

Epoch: 71
Loss: 1.2723053097724915
RMSE train: 0.796676	val: 4.211121	test: 3.757666
MAE train: 0.616475	val: 3.104386	test: 3.091263

Epoch: 72
Loss: 1.1943891048431396
RMSE train: 0.810201	val: 4.188673	test: 3.734402
MAE train: 0.626684	val: 3.071819	test: 3.068686

Epoch: 73
Loss: 1.321300745010376
RMSE train: 0.873679	val: 4.329311	test: 3.768162
MAE train: 0.681547	val: 3.180206	test: 3.097062

Epoch: 74
Loss: 1.1671132445335388
RMSE train: 0.925139	val: 4.466824	test: 3.799819
MAE train: 0.728112	val: 3.276586	test: 3.116360

Epoch: 75
Loss: 1.1182748675346375
RMSE train: 0.918582	val: 4.497957	test: 3.778342
MAE train: 0.724646	val: 3.271539	test: 3.089041

Epoch: 76
Loss: 1.1992383003234863
RMSE train: 0.866514	val: 4.455410	test: 3.743956
MAE train: 0.686225	val: 3.197684	test: 3.047028

Epoch: 77
Loss: 1.0698422193527222
RMSE train: 0.843667	val: 4.430835	test: 3.737039
MAE train: 0.668837	val: 3.140131	test: 3.033952

Epoch: 78
Loss: 1.2707293629646301
RMSE train: 0.905136	val: 4.523947	test: 3.786649
MAE train: 0.720204	val: 3.203996	test: 3.077651

Epoch: 79
Loss: 1.1156620979309082
RMSE train: 0.959130	val: 4.559246	test: 3.828930
MAE train: 0.766964	val: 3.244744	test: 3.113616

Epoch: 80
Loss: 1.1542862057685852
RMSE train: 1.026750	val: 4.615100	test: 3.869545
MAE train: 0.817790	val: 3.304624	test: 3.152705

Epoch: 81
Loss: 1.0582979619503021
RMSE train: 1.021762	val: 4.573525	test: 3.872522
MAE train: 0.810629	val: 3.273837	test: 3.163245

Epoch: 82
Loss: 1.234031617641449
RMSE train: 0.960690	val: 4.468597	test: 3.862507
MAE train: 0.763398	val: 3.183721	test: 3.154890

Epoch: 83
Loss: 1.2062774896621704
RMSE train: 3.481965	val: 7.545114	test: 6.179747
MAE train: 3.014992	val: 5.836901	test: 5.455772

Epoch: 23
Loss: 8.069559574127197
RMSE train: 3.376274	val: 7.436951	test: 6.090018
MAE train: 2.925641	val: 5.708078	test: 5.361659

Epoch: 24
Loss: 7.428218603134155
RMSE train: 3.282194	val: 7.352175	test: 6.012655
MAE train: 2.845804	val: 5.608375	test: 5.277997

Epoch: 25
Loss: 7.38973331451416
RMSE train: 3.200168	val: 7.283060	test: 5.931902
MAE train: 2.774736	val: 5.534263	test: 5.202884

Epoch: 26
Loss: 6.834036827087402
RMSE train: 3.118149	val: 7.215126	test: 5.862673
MAE train: 2.702498	val: 5.462309	test: 5.139736

Epoch: 27
Loss: 6.481208086013794
RMSE train: 3.082194	val: 7.177584	test: 5.807867
MAE train: 2.675410	val: 5.442504	test: 5.096681

Epoch: 28
Loss: 6.070449352264404
RMSE train: 3.040407	val: 7.127842	test: 5.726787
MAE train: 2.641005	val: 5.406777	test: 5.021236

Epoch: 29
Loss: 5.599108934402466
RMSE train: 2.951193	val: 7.014199	test: 5.595267
MAE train: 2.557802	val: 5.317570	test: 4.893618

Epoch: 30
Loss: 5.5611045360565186
RMSE train: 2.856116	val: 6.881658	test: 5.457610
MAE train: 2.468495	val: 5.182543	test: 4.751031

Epoch: 31
Loss: 5.055838346481323
RMSE train: 2.767940	val: 6.786744	test: 5.357967
MAE train: 2.387640	val: 5.077235	test: 4.645249

Epoch: 32
Loss: 4.872034072875977
RMSE train: 2.708354	val: 6.763217	test: 5.353102
MAE train: 2.336313	val: 5.048439	test: 4.647146

Epoch: 33
Loss: 4.670064926147461
RMSE train: 2.671396	val: 6.764963	test: 5.352575
MAE train: 2.306557	val: 5.031874	test: 4.654853

Epoch: 34
Loss: 4.462676286697388
RMSE train: 2.617514	val: 6.763679	test: 5.331191
MAE train: 2.259317	val: 5.011793	test: 4.643010

Epoch: 35
Loss: 4.203694224357605
RMSE train: 2.558207	val: 6.770551	test: 5.296696
MAE train: 2.203594	val: 5.019396	test: 4.614736

Epoch: 36
Loss: 4.100633144378662
RMSE train: 2.456334	val: 6.714632	test: 5.216072
MAE train: 2.109267	val: 4.977788	test: 4.543013

Epoch: 37
Loss: 3.7206428050994873
RMSE train: 2.345912	val: 6.633751	test: 5.156630
MAE train: 2.009260	val: 4.917668	test: 4.497832

Epoch: 38
Loss: 3.3958641290664673
RMSE train: 2.247537	val: 6.576088	test: 5.146474
MAE train: 1.919872	val: 4.894707	test: 4.506276

Epoch: 39
Loss: 3.0009835958480835
RMSE train: 2.139092	val: 6.497409	test: 5.124397
MAE train: 1.815951	val: 4.831022	test: 4.498078

Epoch: 40
Loss: 3.5347378253936768
RMSE train: 1.999978	val: 6.341598	test: 5.000259
MAE train: 1.679782	val: 4.662355	test: 4.368927

Epoch: 41
Loss: 3.2420650720596313
RMSE train: 1.902713	val: 6.203837	test: 4.854675
MAE train: 1.580884	val: 4.445142	test: 4.205639

Epoch: 42
Loss: 2.8807315826416016
RMSE train: 1.800054	val: 6.072013	test: 4.699385
MAE train: 1.477823	val: 4.235237	test: 4.031701

Epoch: 43
Loss: 2.7545673847198486
RMSE train: 1.681972	val: 5.915609	test: 4.498613
MAE train: 1.365168	val: 4.053438	test: 3.805329

Epoch: 44
Loss: 2.5986732244491577
RMSE train: 1.602811	val: 5.879993	test: 4.403478
MAE train: 1.292478	val: 3.996718	test: 3.681667

Epoch: 45
Loss: 2.6023155450820923
RMSE train: 1.516151	val: 5.818597	test: 4.298250
MAE train: 1.217051	val: 3.941684	test: 3.553113

Epoch: 46
Loss: 2.447324752807617
RMSE train: 1.430937	val: 5.799615	test: 4.264081
MAE train: 1.145998	val: 3.935578	test: 3.502306

Epoch: 47
Loss: 2.4076007604599
RMSE train: 1.357540	val: 5.772781	test: 4.242505
MAE train: 1.087550	val: 3.925207	test: 3.480587

Epoch: 48
Loss: 2.3713009357452393
RMSE train: 1.312413	val: 5.809784	test: 4.241261
MAE train: 1.050939	val: 3.964518	test: 3.490790

Epoch: 49
Loss: 2.2290316820144653
RMSE train: 1.270316	val: 5.759967	test: 4.189896
MAE train: 1.014359	val: 3.936712	test: 3.444626

Epoch: 50
Loss: 2.247765064239502
RMSE train: 1.257051	val: 5.742310	test: 4.158263
MAE train: 1.002529	val: 3.941178	test: 3.422781

Epoch: 51
Loss: 2.426517963409424
RMSE train: 1.275104	val: 5.781808	test: 4.179268
MAE train: 1.024673	val: 3.937563	test: 3.451130

Epoch: 52
Loss: 2.1358835697174072
RMSE train: 1.314676	val: 5.831117	test: 4.216262
MAE train: 1.062200	val: 3.924143	test: 3.494438

Epoch: 53
Loss: 2.0636147260665894
RMSE train: 1.330991	val: 5.854741	test: 4.242673
MAE train: 1.074828	val: 3.907272	test: 3.526023

Epoch: 54
Loss: 2.0520561933517456
RMSE train: 1.316377	val: 5.855214	test: 4.259565
MAE train: 1.062120	val: 3.908438	test: 3.551020

Epoch: 55
Loss: 2.2355921268463135
RMSE train: 1.273610	val: 5.826843	test: 4.216117
MAE train: 1.025696	val: 3.901324	test: 3.507206

Epoch: 56
Loss: 1.9846441745758057
RMSE train: 1.250209	val: 5.777549	test: 4.154137
MAE train: 1.005509	val: 3.874864	test: 3.433811

Epoch: 57
Loss: 1.8207005858421326
RMSE train: 1.229040	val: 5.729220	test: 4.101648
MAE train: 0.980311	val: 3.840306	test: 3.365291

Epoch: 58
Loss: 1.5827089548110962
RMSE train: 1.165178	val: 5.637532	test: 4.038827
MAE train: 0.927803	val: 3.794218	test: 3.286279

Epoch: 59
Loss: 1.8298152089118958
RMSE train: 1.122807	val: 5.551352	test: 3.976410
MAE train: 0.894230	val: 3.751988	test: 3.210489

Epoch: 60
Loss: 1.7101867198944092
RMSE train: 1.102037	val: 5.491067	test: 3.949387
MAE train: 0.877290	val: 3.734132	test: 3.181109

Epoch: 61
Loss: 1.7371465563774109
RMSE train: 1.133229	val: 5.510796	test: 3.986487
MAE train: 0.901781	val: 3.755930	test: 3.223157

Epoch: 62
Loss: 1.664985716342926
RMSE train: 1.156026	val: 5.505065	test: 4.006040
MAE train: 0.918171	val: 3.762342	test: 3.259257

Epoch: 63
Loss: 1.4977421760559082
RMSE train: 1.176018	val: 5.510646	test: 4.006704
MAE train: 0.932394	val: 3.781015	test: 3.273275

Epoch: 64
Loss: 1.9323179125785828
RMSE train: 1.217496	val: 5.586976	test: 4.015669
MAE train: 0.969307	val: 3.834849	test: 3.272324

Epoch: 65
Loss: 1.6655300855636597
RMSE train: 1.263740	val: 5.682247	test: 4.037237
MAE train: 1.012926	val: 3.914515	test: 3.274243

Epoch: 66
Loss: 1.5668671131134033
RMSE train: 1.237228	val: 5.684122	test: 4.002320
MAE train: 0.993924	val: 3.937985	test: 3.226651

Epoch: 67
Loss: 1.5367767810821533
RMSE train: 1.162428	val: 5.617220	test: 3.925859
MAE train: 0.933245	val: 3.912715	test: 3.140791

Epoch: 68
Loss: 1.5379385948181152
RMSE train: 1.124971	val: 5.573022	test: 3.867392
MAE train: 0.906372	val: 3.893969	test: 3.076270

Epoch: 69
Loss: 1.472282588481903
RMSE train: 1.136185	val: 5.613082	test: 3.933401
MAE train: 0.928072	val: 3.941325	test: 3.163749

Epoch: 70
Loss: 1.5952860713005066
RMSE train: 1.118275	val: 5.590427	test: 3.970797
MAE train: 0.919938	val: 3.922998	test: 3.220551

Epoch: 71
Loss: 1.3239673376083374
RMSE train: 1.077698	val: 5.533463	test: 3.977039
MAE train: 0.882703	val: 3.869173	test: 3.235326

Epoch: 72
Loss: 1.5783260464668274
RMSE train: 1.021416	val: 5.465431	test: 3.941088
MAE train: 0.829817	val: 3.813019	test: 3.205260

Epoch: 73
Loss: 1.2791646122932434
RMSE train: 0.950633	val: 5.378980	test: 3.893578
MAE train: 0.766368	val: 3.753174	test: 3.164342

Epoch: 74
Loss: 1.5182878375053406
RMSE train: 0.913155	val: 5.356168	test: 3.830136
MAE train: 0.731413	val: 3.732820	test: 3.108493

Epoch: 75
Loss: 1.3492141366004944
RMSE train: 0.908640	val: 5.365368	test: 3.780454
MAE train: 0.727353	val: 3.719355	test: 3.052862

Epoch: 76
Loss: 1.411703109741211
RMSE train: 0.917187	val: 5.360877	test: 3.744039
MAE train: 0.734820	val: 3.701749	test: 2.994334

Epoch: 77
Loss: 1.2214435338974
RMSE train: 0.905796	val: 5.324781	test: 3.727596
MAE train: 0.724253	val: 3.663178	test: 2.969424

Epoch: 78
Loss: 1.3059266805648804
RMSE train: 0.902177	val: 5.325327	test: 3.716752
MAE train: 0.724762	val: 3.638034	test: 2.947323

Epoch: 79
Loss: 1.2987784147262573
RMSE train: 0.921109	val: 5.359536	test: 3.752216
MAE train: 0.745129	val: 3.652664	test: 2.982558

Epoch: 80
Loss: 1.2282776236534119
RMSE train: 0.980553	val: 5.424516	test: 3.846779
MAE train: 0.794783	val: 3.693210	test: 3.077939

Epoch: 81
Loss: 1.191770076751709
RMSE train: 1.026383	val: 5.484693	test: 3.978556
MAE train: 0.836320	val: 3.728977	test: 3.211895

Epoch: 82
Loss: 1.217373788356781
RMSE train: 1.077511	val: 5.595019	test: 4.135273
MAE train: 0.883254	val: 3.807154	test: 3.374681

Epoch: 83
Loss: 1.2309186458587646
RMSE train: 2.738091	val: 6.615069	test: 5.834003
MAE train: 2.322542	val: 5.588665	test: 5.176892

Epoch: 23
Loss: 6.7403244972229
RMSE train: 2.669369	val: 6.550659	test: 5.808377
MAE train: 2.266838	val: 5.519176	test: 5.164387

Epoch: 24
Loss: 6.194632053375244
RMSE train: 2.582343	val: 6.452122	test: 5.743165
MAE train: 2.189556	val: 5.402678	test: 5.106262

Epoch: 25
Loss: 5.774789571762085
RMSE train: 2.497679	val: 6.335063	test: 5.664201
MAE train: 2.112873	val: 5.282245	test: 5.025382

Epoch: 26
Loss: 5.5323805809021
RMSE train: 2.438522	val: 6.275027	test: 5.601964
MAE train: 2.064003	val: 5.224508	test: 4.950080

Epoch: 27
Loss: 5.414935827255249
RMSE train: 2.383711	val: 6.196919	test: 5.515046
MAE train: 2.016103	val: 5.152239	test: 4.844776

Epoch: 28
Loss: 5.059725999832153
RMSE train: 2.332821	val: 6.125929	test: 5.405564
MAE train: 1.965862	val: 5.069292	test: 4.718638

Epoch: 29
Loss: 4.810105800628662
RMSE train: 2.290514	val: 6.059655	test: 5.305225
MAE train: 1.917345	val: 4.978626	test: 4.611367

Epoch: 30
Loss: 4.506789207458496
RMSE train: 2.224825	val: 5.951748	test: 5.204922
MAE train: 1.853458	val: 4.853274	test: 4.513062

Epoch: 31
Loss: 4.344138026237488
RMSE train: 2.163141	val: 5.823077	test: 5.120937
MAE train: 1.804079	val: 4.717787	test: 4.433192

Epoch: 32
Loss: 4.118669390678406
RMSE train: 2.116634	val: 5.784742	test: 5.079012
MAE train: 1.759417	val: 4.661137	test: 4.412926

Epoch: 33
Loss: 3.8846668004989624
RMSE train: 2.053935	val: 5.748013	test: 5.050688
MAE train: 1.700995	val: 4.638675	test: 4.389796

Epoch: 34
Loss: 3.8075191974639893
RMSE train: 1.995369	val: 5.700622	test: 5.008422
MAE train: 1.644648	val: 4.606716	test: 4.337504

Epoch: 35
Loss: 3.3823341131210327
RMSE train: 1.931036	val: 5.609565	test: 4.945902
MAE train: 1.584064	val: 4.530972	test: 4.264286

Epoch: 36
Loss: 3.407280683517456
RMSE train: 1.852798	val: 5.486047	test: 4.856796
MAE train: 1.509957	val: 4.384110	test: 4.156983

Epoch: 37
Loss: 3.146589756011963
RMSE train: 1.761530	val: 5.373292	test: 4.755318
MAE train: 1.419737	val: 4.216780	test: 4.025360

Epoch: 38
Loss: 2.604675054550171
RMSE train: 1.641437	val: 5.253716	test: 4.646369
MAE train: 1.304654	val: 4.015454	test: 3.901190

Epoch: 39
Loss: 2.91053307056427
RMSE train: 1.562591	val: 5.162642	test: 4.549682
MAE train: 1.226109	val: 3.832820	test: 3.785664

Epoch: 40
Loss: 2.8534176349639893
RMSE train: 1.498516	val: 5.101629	test: 4.476287
MAE train: 1.169116	val: 3.721009	test: 3.709085

Epoch: 41
Loss: 2.30594539642334
RMSE train: 1.413772	val: 4.998236	test: 4.384589
MAE train: 1.094043	val: 3.581195	test: 3.616233

Epoch: 42
Loss: 2.1884061098098755
RMSE train: 1.314884	val: 4.871256	test: 4.288459
MAE train: 1.010616	val: 3.441007	test: 3.522466

Epoch: 43
Loss: 2.1908291578292847
RMSE train: 1.235204	val: 4.763305	test: 4.220793
MAE train: 0.946416	val: 3.345603	test: 3.462923

Epoch: 44
Loss: 2.1215083599090576
RMSE train: 1.178656	val: 4.697337	test: 4.177362
MAE train: 0.896945	val: 3.287186	test: 3.425083

Epoch: 45
Loss: 2.1413943767547607
RMSE train: 1.161534	val: 4.687986	test: 4.175127
MAE train: 0.876427	val: 3.282745	test: 3.430722

Epoch: 46
Loss: 1.811737060546875
RMSE train: 1.149260	val: 4.704042	test: 4.196362
MAE train: 0.868947	val: 3.308648	test: 3.460431

Epoch: 47
Loss: 1.7526705861091614
RMSE train: 1.161517	val: 4.742041	test: 4.213077
MAE train: 0.883074	val: 3.355983	test: 3.476489

Epoch: 48
Loss: 1.737793505191803
RMSE train: 1.177547	val: 4.776768	test: 4.208912
MAE train: 0.896318	val: 3.369328	test: 3.466878

Epoch: 49
Loss: 1.7404754757881165
RMSE train: 1.168144	val: 4.791243	test: 4.172206
MAE train: 0.883100	val: 3.318443	test: 3.414061

Epoch: 50
Loss: 1.637589693069458
RMSE train: 1.139463	val: 4.769173	test: 4.118286
MAE train: 0.856840	val: 3.246201	test: 3.346266

Epoch: 51
Loss: 1.6868772506713867
RMSE train: 1.076938	val: 4.698980	test: 4.017363
MAE train: 0.808398	val: 3.124670	test: 3.228161

Epoch: 52
Loss: 1.504473865032196
RMSE train: 1.022843	val: 4.655306	test: 3.934635
MAE train: 0.765631	val: 3.078585	test: 3.141104

Epoch: 53
Loss: 1.5104912519454956
RMSE train: 0.952876	val: 4.580199	test: 3.868818
MAE train: 0.716400	val: 3.012926	test: 3.075869

Epoch: 54
Loss: 1.515015721321106
RMSE train: 0.893334	val: 4.520122	test: 3.846674
MAE train: 0.674012	val: 2.959061	test: 3.068109

Epoch: 55
Loss: 1.4294434785842896
RMSE train: 0.860378	val: 4.445782	test: 3.810500
MAE train: 0.645501	val: 2.867321	test: 3.042581

Epoch: 56
Loss: 1.5445354580879211
RMSE train: 0.841349	val: 4.354936	test: 3.727966
MAE train: 0.627626	val: 2.768530	test: 2.950613

Epoch: 57
Loss: 1.4289745688438416
RMSE train: 0.825366	val: 4.321722	test: 3.676363
MAE train: 0.613598	val: 2.735391	test: 2.882858

Epoch: 58
Loss: 1.3358210325241089
RMSE train: 0.777229	val: 4.252939	test: 3.616648
MAE train: 0.582530	val: 2.697933	test: 2.821979

Epoch: 59
Loss: 1.4148175716400146
RMSE train: 0.757853	val: 4.238264	test: 3.625992
MAE train: 0.570199	val: 2.716087	test: 2.846068

Epoch: 60
Loss: 1.4282336235046387
RMSE train: 0.783108	val: 4.275103	test: 3.642957
MAE train: 0.592392	val: 2.778329	test: 2.856940

Epoch: 61
Loss: 1.4484599828720093
RMSE train: 0.832700	val: 4.335725	test: 3.675514
MAE train: 0.628750	val: 2.847421	test: 2.889439

Epoch: 62
Loss: 1.2751469016075134
RMSE train: 0.860982	val: 4.380984	test: 3.726270
MAE train: 0.659022	val: 2.901179	test: 2.958163

Epoch: 63
Loss: 1.3262268900871277
RMSE train: 0.847756	val: 4.354035	test: 3.725410
MAE train: 0.652649	val: 2.898735	test: 2.958625

Epoch: 64
Loss: 1.236085832118988
RMSE train: 0.780833	val: 4.235819	test: 3.657894
MAE train: 0.602655	val: 2.807558	test: 2.872181

Epoch: 65
Loss: 1.4083239436149597
RMSE train: 0.758556	val: 4.149993	test: 3.598716
MAE train: 0.580875	val: 2.734236	test: 2.805385

Epoch: 66
Loss: 1.2770261764526367
RMSE train: 0.767974	val: 4.136117	test: 3.582819
MAE train: 0.587858	val: 2.679936	test: 2.789159

Epoch: 67
Loss: 1.1652072072029114
RMSE train: 0.800357	val: 4.198431	test: 3.619478
MAE train: 0.616505	val: 2.697192	test: 2.828231

Epoch: 68
Loss: 1.1809571981430054
RMSE train: 0.796062	val: 4.201577	test: 3.625642
MAE train: 0.611628	val: 2.716415	test: 2.845975

Epoch: 69
Loss: 1.020488977432251
RMSE train: 0.789731	val: 4.173644	test: 3.623168
MAE train: 0.604064	val: 2.739653	test: 2.844330

Epoch: 70
Loss: 1.0131213665008545
RMSE train: 0.775144	val: 4.101853	test: 3.593016
MAE train: 0.590291	val: 2.761424	test: 2.811246

Epoch: 71
Loss: 1.087128758430481
RMSE train: 0.778167	val: 4.045837	test: 3.558252
MAE train: 0.591948	val: 2.725135	test: 2.780518

Epoch: 72
Loss: 1.1251394748687744
RMSE train: 0.801323	val: 4.060230	test: 3.564701
MAE train: 0.607222	val: 2.726712	test: 2.790398

Epoch: 73
Loss: 1.117623746395111
RMSE train: 0.847324	val: 4.125163	test: 3.571635
MAE train: 0.640548	val: 2.747546	test: 2.803281

Epoch: 74
Loss: 1.052161991596222
RMSE train: 0.864404	val: 4.192620	test: 3.584543
MAE train: 0.655165	val: 2.784204	test: 2.828966

Epoch: 75
Loss: 1.0304669737815857
RMSE train: 0.829621	val: 4.205486	test: 3.567029
MAE train: 0.625690	val: 2.810084	test: 2.816653

Epoch: 76
Loss: 1.1139274835586548
RMSE train: 0.798075	val: 4.195062	test: 3.569722
MAE train: 0.609292	val: 2.815107	test: 2.818114

Epoch: 77
Loss: 1.1222359240055084
RMSE train: 0.781140	val: 4.174856	test: 3.547516
MAE train: 0.600218	val: 2.795076	test: 2.781000

Epoch: 78
Loss: 1.1344971656799316
RMSE train: 0.783556	val: 4.170989	test: 3.524498
MAE train: 0.606523	val: 2.732252	test: 2.736855

Epoch: 79
Loss: 1.06502965092659
RMSE train: 0.825672	val: 4.193009	test: 3.503878
MAE train: 0.639208	val: 2.713515	test: 2.706147

Epoch: 80
Loss: 1.0594757795333862
RMSE train: 0.863546	val: 4.220899	test: 3.501188
MAE train: 0.668129	val: 2.728087	test: 2.692466

Epoch: 81
Loss: 1.0962267518043518
RMSE train: 0.850597	val: 4.203841	test: 3.481999
MAE train: 0.659621	val: 2.720222	test: 2.663233

Epoch: 82
Loss: 1.0552069544792175
RMSE train: 0.818532	val: 4.172626	test: 3.462758
MAE train: 0.633658	val: 2.708897	test: 2.636388

Epoch: 83
Loss: 0.9718170464038849

Epoch: 84
Loss: 0.924826055765152
RMSE train: 0.728000	val: 4.163695	test: 3.400162
MAE train: 0.544922	val: 3.028035	test: 2.670969

Epoch: 85
Loss: 0.9182266592979431
RMSE train: 0.712489	val: 4.173375	test: 3.454181
MAE train: 0.537096	val: 3.089980	test: 2.729513

Epoch: 86
Loss: 0.8651100397109985
RMSE train: 0.717465	val: 4.206063	test: 3.528106
MAE train: 0.542219	val: 3.091820	test: 2.790364

Epoch: 87
Loss: 0.9265669584274292
RMSE train: 0.769276	val: 4.259609	test: 3.568964
MAE train: 0.580434	val: 3.075620	test: 2.829195

Epoch: 88
Loss: 0.8008772432804108
RMSE train: 0.800173	val: 4.278599	test: 3.564342
MAE train: 0.608404	val: 3.086635	test: 2.829564

Epoch: 89
Loss: 0.9836181402206421
RMSE train: 0.851172	val: 4.312621	test: 3.523972
MAE train: 0.642483	val: 3.159569	test: 2.790186

Epoch: 90
Loss: 0.8415245413780212
RMSE train: 0.889976	val: 4.328270	test: 3.458397
MAE train: 0.654774	val: 3.169846	test: 2.725868

Epoch: 91
Loss: 0.9406770169734955
RMSE train: 0.841566	val: 4.284508	test: 3.363394
MAE train: 0.613346	val: 3.083412	test: 2.615791

Epoch: 92
Loss: 0.9797041416168213
RMSE train: 0.776766	val: 4.204963	test: 3.281129
MAE train: 0.564074	val: 2.980094	test: 2.524303

Epoch: 93
Loss: 0.8826917707920074
RMSE train: 0.738595	val: 4.134104	test: 3.256751
MAE train: 0.540962	val: 2.907611	test: 2.486583

Epoch: 94
Loss: 0.8925231993198395
RMSE train: 0.734460	val: 4.117868	test: 3.264776
MAE train: 0.539951	val: 2.893669	test: 2.482960

Epoch: 95
Loss: 0.8390342593193054
RMSE train: 0.746206	val: 4.136320	test: 3.272343
MAE train: 0.550013	val: 2.916970	test: 2.483822

Epoch: 96
Loss: 0.7864393293857574
RMSE train: 0.783861	val: 4.188571	test: 3.296179
MAE train: 0.579094	val: 2.947736	test: 2.503317

Epoch: 97
Loss: 0.795320063829422
RMSE train: 0.815965	val: 4.218130	test: 3.291710
MAE train: 0.603331	val: 2.923244	test: 2.486723

Epoch: 98
Loss: 0.9610915184020996
RMSE train: 0.833848	val: 4.223760	test: 3.297374
MAE train: 0.620518	val: 2.926165	test: 2.507823

Epoch: 99
Loss: 0.9005110859870911
RMSE train: 0.830570	val: 4.204641	test: 3.318504
MAE train: 0.623683	val: 2.903851	test: 2.541080

Epoch: 100
Loss: 0.7663914263248444
RMSE train: 0.798918	val: 4.166730	test: 3.324096
MAE train: 0.599973	val: 2.895907	test: 2.562057

Epoch: 101
Loss: 0.7745550572872162
RMSE train: 0.785728	val: 4.147070	test: 3.319863
MAE train: 0.594156	val: 2.885985	test: 2.557647

Epoch: 102
Loss: 0.9021336436271667
RMSE train: 0.812765	val: 4.173007	test: 3.330098
MAE train: 0.615863	val: 2.943403	test: 2.571326

Epoch: 103
Loss: 0.8773634135723114
RMSE train: 0.816179	val: 4.186501	test: 3.353612
MAE train: 0.620508	val: 2.940987	test: 2.596128

Epoch: 104
Loss: 0.8038475513458252
RMSE train: 0.845728	val: 4.204372	test: 3.365579
MAE train: 0.633538	val: 2.897056	test: 2.579991

Epoch: 105
Loss: 0.8568945825099945
RMSE train: 0.808622	val: 4.182160	test: 3.379818
MAE train: 0.603665	val: 2.909549	test: 2.598012

Epoch: 106
Loss: 0.870164543390274
RMSE train: 0.796445	val: 4.177834	test: 3.386575
MAE train: 0.596350	val: 2.957393	test: 2.625167

Epoch: 107
Loss: 0.8148231506347656
RMSE train: 0.829323	val: 4.190309	test: 3.368365
MAE train: 0.620318	val: 3.012115	test: 2.627405

Epoch: 108
Loss: 0.7097682952880859
RMSE train: 0.834916	val: 4.192054	test: 3.353395
MAE train: 0.624574	val: 3.066475	test: 2.632057

Epoch: 109
Loss: 0.8318862617015839
RMSE train: 0.809498	val: 4.184121	test: 3.346350
MAE train: 0.605511	val: 3.044765	test: 2.617142

Epoch: 110
Loss: 0.7523517310619354
RMSE train: 0.817866	val: 4.187320	test: 3.338425
MAE train: 0.615399	val: 2.983945	test: 2.599701

Epoch: 111
Loss: 0.7592734694480896
RMSE train: 0.766489	val: 4.161404	test: 3.303587
MAE train: 0.579076	val: 2.946381	test: 2.562525

Epoch: 112
Loss: 0.7475286424160004
RMSE train: 0.682040	val: 4.129729	test: 3.271129
MAE train: 0.521104	val: 2.916021	test: 2.528128

Epoch: 113
Loss: 0.7959891557693481
RMSE train: 0.659725	val: 4.141182	test: 3.249137
MAE train: 0.504838	val: 2.915021	test: 2.509891

Epoch: 114
Loss: 0.7286428809165955
RMSE train: 0.653347	val: 4.168732	test: 3.224851
MAE train: 0.495619	val: 2.928617	test: 2.480207

Epoch: 115
Loss: 0.7602598369121552
RMSE train: 0.662698	val: 4.230580	test: 3.239570
MAE train: 0.497207	val: 3.009427	test: 2.489342

Epoch: 116
Loss: 0.7655153274536133
RMSE train: 0.693768	val: 4.289280	test: 3.256315
MAE train: 0.512735	val: 3.092923	test: 2.498322

Epoch: 117
Loss: 0.7539462149143219
RMSE train: 0.703210	val: 4.325566	test: 3.235475
MAE train: 0.510403	val: 3.085306	test: 2.449256

Epoch: 118
Loss: 0.804422527551651
RMSE train: 0.711460	val: 4.336992	test: 3.226123
MAE train: 0.517426	val: 3.043924	test: 2.421847

Epoch: 119
Loss: 0.7183474004268646
RMSE train: 0.665543	val: 4.272274	test: 3.215468
MAE train: 0.485199	val: 2.973343	test: 2.423890

Epoch: 120
Loss: 0.6593718230724335
RMSE train: 0.619347	val: 4.222198	test: 3.242810
MAE train: 0.457355	val: 2.959369	test: 2.474933

Epoch: 121
Loss: 0.7939407825469971
RMSE train: 0.642675	val: 4.204248	test: 3.286055
MAE train: 0.481031	val: 2.972699	test: 2.529301

Early stopping
Best (RMSE):	 train: 0.898180	val: 4.001341	test: 3.312101
Best (MAE):	 train: 0.674397	val: 2.860830	test: 2.608689


Epoch: 84
Loss: 0.9245115220546722
RMSE train: 0.663106	val: 4.105205	test: 3.153923
MAE train: 0.497687	val: 2.677092	test: 2.310330

Epoch: 85
Loss: 0.9286116659641266
RMSE train: 0.731385	val: 4.178730	test: 3.162332
MAE train: 0.554390	val: 2.726639	test: 2.296118

Epoch: 86
Loss: 1.0476705431938171
RMSE train: 0.831971	val: 4.252454	test: 3.154526
MAE train: 0.635548	val: 2.781330	test: 2.256810

Epoch: 87
Loss: 0.9620408713817596
RMSE train: 0.855195	val: 4.261714	test: 3.130653
MAE train: 0.647078	val: 2.783164	test: 2.217417

Epoch: 88
Loss: 0.9438861906528473
RMSE train: 0.782689	val: 4.166942	test: 3.092110
MAE train: 0.585138	val: 2.710350	test: 2.186050

Epoch: 89
Loss: 0.9498198926448822
RMSE train: 0.772982	val: 4.116897	test: 3.083689
MAE train: 0.570120	val: 2.693715	test: 2.188727

Epoch: 90
Loss: 0.9404047727584839
RMSE train: 0.825302	val: 4.125562	test: 3.110149
MAE train: 0.600561	val: 2.727730	test: 2.216359

Epoch: 91
Loss: 0.9003113806247711
RMSE train: 0.851237	val: 4.184462	test: 3.127450
MAE train: 0.618496	val: 2.760124	test: 2.218245

Epoch: 92
Loss: 0.9679122865200043
RMSE train: 0.872771	val: 4.241455	test: 3.160225
MAE train: 0.630179	val: 2.800973	test: 2.251857

Epoch: 93
Loss: 0.9411073327064514
RMSE train: 0.899584	val: 4.324316	test: 3.186856
MAE train: 0.654591	val: 2.855339	test: 2.278936

Epoch: 94
Loss: 0.9296978712081909
RMSE train: 0.917658	val: 4.394856	test: 3.192050
MAE train: 0.678383	val: 2.910472	test: 2.272685

Epoch: 95
Loss: 0.9082528650760651
RMSE train: 0.891905	val: 4.384849	test: 3.167096
MAE train: 0.647858	val: 2.896611	test: 2.265735

Epoch: 96
Loss: 0.9081366956233978
RMSE train: 0.848387	val: 4.331742	test: 3.148009
MAE train: 0.611886	val: 2.851443	test: 2.256601

Epoch: 97
Loss: 0.9077530801296234
RMSE train: 0.803422	val: 4.304869	test: 3.143804
MAE train: 0.585353	val: 2.825301	test: 2.242132

Epoch: 98
Loss: 0.9974959194660187
RMSE train: 0.807516	val: 4.312410	test: 3.160645
MAE train: 0.597869	val: 2.777441	test: 2.238233

Epoch: 99
Loss: 0.9002243280410767
RMSE train: 0.841298	val: 4.306853	test: 3.193212
MAE train: 0.616926	val: 2.750687	test: 2.273118

Epoch: 100
Loss: 0.8699511587619781
RMSE train: 0.863478	val: 4.284645	test: 3.195464
MAE train: 0.637431	val: 2.741962	test: 2.287362

Epoch: 101
Loss: 0.9319080114364624
RMSE train: 0.862817	val: 4.252755	test: 3.168411
MAE train: 0.639756	val: 2.791792	test: 2.267042

Epoch: 102
Loss: 0.7265596091747284
RMSE train: 0.880528	val: 4.226304	test: 3.133078
MAE train: 0.656903	val: 2.778371	test: 2.232390

Epoch: 103
Loss: 0.7542997002601624
RMSE train: 0.867054	val: 4.193466	test: 3.114662
MAE train: 0.651522	val: 2.709372	test: 2.207540

Epoch: 104
Loss: 0.8179948031902313
RMSE train: 0.868046	val: 4.170537	test: 3.129194
MAE train: 0.648616	val: 2.663197	test: 2.211892

Epoch: 105
Loss: 0.8502565622329712
RMSE train: 0.828065	val: 4.177082	test: 3.143876
MAE train: 0.615456	val: 2.664180	test: 2.234438

Epoch: 106
Loss: 0.7456740438938141
RMSE train: 0.814628	val: 4.239002	test: 3.164502
MAE train: 0.598726	val: 2.726025	test: 2.258208

Epoch: 107
Loss: 0.8109962642192841
RMSE train: 0.833514	val: 4.339764	test: 3.182522
MAE train: 0.600466	val: 2.850865	test: 2.280151

Epoch: 108
Loss: 0.8043045997619629
RMSE train: 0.814216	val: 4.372537	test: 3.188241
MAE train: 0.577420	val: 2.871614	test: 2.283979

Epoch: 109
Loss: 0.7160163521766663
RMSE train: 0.792976	val: 4.362808	test: 3.174683
MAE train: 0.564152	val: 2.827378	test: 2.269158

Epoch: 110
Loss: 0.8296406269073486
RMSE train: 0.746269	val: 4.269786	test: 3.140929
MAE train: 0.539758	val: 2.725408	test: 2.233141

Epoch: 111
Loss: 0.8348391652107239
RMSE train: 0.724003	val: 4.201943	test: 3.129412
MAE train: 0.532357	val: 2.682787	test: 2.220112

Epoch: 112
Loss: 0.8065289855003357
RMSE train: 0.730348	val: 4.154650	test: 3.133841
MAE train: 0.541656	val: 2.661244	test: 2.232148

Epoch: 113
Loss: 0.7913965284824371
RMSE train: 0.737750	val: 4.140064	test: 3.125095
MAE train: 0.546556	val: 2.661968	test: 2.223425

Epoch: 114
Loss: 0.7910954058170319
RMSE train: 0.732645	val: 4.182682	test: 3.111899
MAE train: 0.541905	val: 2.696413	test: 2.202201

Epoch: 115
Loss: 0.6979785263538361
RMSE train: 0.757418	val: 4.229881	test: 3.114452
MAE train: 0.559112	val: 2.758734	test: 2.186942

Epoch: 116
Loss: 0.7815691232681274
RMSE train: 0.745021	val: 4.232147	test: 3.127900
MAE train: 0.554526	val: 2.767936	test: 2.199525

Epoch: 117
Loss: 0.7230184376239777
RMSE train: 0.722083	val: 4.226405	test: 3.142701
MAE train: 0.538371	val: 2.729086	test: 2.212935

Epoch: 118
Loss: 0.7351285815238953
RMSE train: 0.717060	val: 4.203828	test: 3.131162
MAE train: 0.538638	val: 2.683116	test: 2.202908

Epoch: 119
Loss: 0.6989911496639252
RMSE train: 0.734806	val: 4.221636	test: 3.123356
MAE train: 0.551086	val: 2.691241	test: 2.190327

Epoch: 120
Loss: 0.747546523809433
RMSE train: 0.722333	val: 4.214929	test: 3.102063
MAE train: 0.537798	val: 2.685116	test: 2.168916

Epoch: 121
Loss: 0.6943813264369965
RMSE train: 0.716344	val: 4.228674	test: 3.099420
MAE train: 0.529992	val: 2.709844	test: 2.176280

Early stopping
Best (RMSE):	 train: 0.650173	val: 4.094809	test: 3.162333
Best (MAE):	 train: 0.480007	val: 2.735403	test: 2.330691

RMSE train: 0.716660	val: 3.883290	test: 2.915074
MAE train: 0.567046	val: 2.814475	test: 2.179052

Epoch: 84
Loss: 1.136677861213684
RMSE train: 0.738349	val: 3.933980	test: 2.943489
MAE train: 0.582714	val: 2.859346	test: 2.199626

Epoch: 85
Loss: 1.1011108756065369
RMSE train: 0.777326	val: 3.989682	test: 2.977831
MAE train: 0.607150	val: 2.906877	test: 2.223173

Epoch: 86
Loss: 1.0585085153579712
RMSE train: 0.803567	val: 4.046683	test: 3.012299
MAE train: 0.621705	val: 2.975071	test: 2.251434

Epoch: 87
Loss: 0.981098473072052
RMSE train: 0.780128	val: 4.015246	test: 2.989667
MAE train: 0.602174	val: 2.990290	test: 2.235698

Epoch: 88
Loss: 1.1417722702026367
RMSE train: 0.746757	val: 3.984096	test: 2.979654
MAE train: 0.578422	val: 2.956533	test: 2.215109

Epoch: 89
Loss: 1.121212124824524
RMSE train: 0.742688	val: 3.995713	test: 3.012953
MAE train: 0.574126	val: 2.940123	test: 2.223999

Epoch: 90
Loss: 0.95965176820755
RMSE train: 0.772534	val: 4.056979	test: 3.087412
MAE train: 0.593374	val: 2.972963	test: 2.282195

Epoch: 91
Loss: 0.9946486353874207
RMSE train: 0.774407	val: 4.090830	test: 3.129145
MAE train: 0.595542	val: 3.012289	test: 2.319701

Epoch: 92
Loss: 1.0083842277526855
RMSE train: 0.773770	val: 4.097246	test: 3.144350
MAE train: 0.599649	val: 2.978218	test: 2.336645

Epoch: 93
Loss: 0.8800454437732697
RMSE train: 0.754152	val: 4.047290	test: 3.105757
MAE train: 0.590545	val: 2.917029	test: 2.305698

Epoch: 94
Loss: 0.8760205507278442
RMSE train: 0.708461	val: 3.965238	test: 3.032842
MAE train: 0.557897	val: 2.824940	test: 2.245509

Epoch: 95
Loss: 0.9088485836982727
RMSE train: 0.677615	val: 3.880107	test: 2.972174
MAE train: 0.533799	val: 2.744558	test: 2.202914

Epoch: 96
Loss: 0.9381559491157532
RMSE train: 0.657677	val: 3.806205	test: 2.921870
MAE train: 0.517243	val: 2.710761	test: 2.173923

Epoch: 97
Loss: 0.8215611279010773
RMSE train: 0.665968	val: 3.790175	test: 2.916424
MAE train: 0.525514	val: 2.692391	test: 2.174486

Epoch: 98
Loss: 0.9810298085212708
RMSE train: 0.677041	val: 3.793739	test: 2.918504
MAE train: 0.536577	val: 2.707115	test: 2.183525

Epoch: 99
Loss: 0.9295316636562347
RMSE train: 0.682380	val: 3.810064	test: 2.911178
MAE train: 0.542853	val: 2.731150	test: 2.181702

Epoch: 100
Loss: 0.8461812436580658
RMSE train: 0.663682	val: 3.813785	test: 2.871195
MAE train: 0.527035	val: 2.719889	test: 2.149294

Epoch: 101
Loss: 0.9135175049304962
RMSE train: 0.652697	val: 3.838348	test: 2.852897
MAE train: 0.512639	val: 2.722101	test: 2.121328

Epoch: 102
Loss: 0.9629947543144226
RMSE train: 0.648833	val: 3.858827	test: 2.852310
MAE train: 0.505342	val: 2.728869	test: 2.112763

Epoch: 103
Loss: 0.8550912737846375
RMSE train: 0.625907	val: 3.820647	test: 2.819287
MAE train: 0.488712	val: 2.696536	test: 2.083884

Epoch: 104
Loss: 0.7881484627723694
RMSE train: 0.619235	val: 3.813701	test: 2.800344
MAE train: 0.484581	val: 2.699773	test: 2.069188

Epoch: 105
Loss: 0.8843382894992828
RMSE train: 0.635146	val: 3.824753	test: 2.831087
MAE train: 0.498218	val: 2.749118	test: 2.112068

Epoch: 106
Loss: 0.8629681766033173
RMSE train: 0.664617	val: 3.839490	test: 2.878479
MAE train: 0.529571	val: 2.799683	test: 2.155008

Epoch: 107
Loss: 0.9763401746749878
RMSE train: 0.696904	val: 3.859036	test: 2.929762
MAE train: 0.559075	val: 2.832237	test: 2.188269

Epoch: 108
Loss: 0.8328342437744141
RMSE train: 0.740282	val: 3.898263	test: 2.973554
MAE train: 0.594440	val: 2.858599	test: 2.221941

Epoch: 109
Loss: 0.8874517381191254
RMSE train: 0.756618	val: 3.893269	test: 2.979250
MAE train: 0.606692	val: 2.866922	test: 2.230301

Epoch: 110
Loss: 0.9084893763065338
RMSE train: 0.723783	val: 3.844625	test: 2.942550
MAE train: 0.572812	val: 2.844570	test: 2.215018

Epoch: 111
Loss: 0.7152339816093445
RMSE train: 0.684496	val: 3.813957	test: 2.899615
MAE train: 0.534492	val: 2.805018	test: 2.184902

Epoch: 112
Loss: 0.89106684923172
RMSE train: 0.710919	val: 3.920369	test: 2.940717
MAE train: 0.547222	val: 2.848039	test: 2.202228

Epoch: 113
Loss: 0.9079353511333466
RMSE train: 0.720791	val: 4.011641	test: 2.981400
MAE train: 0.554696	val: 2.866622	test: 2.210816

Epoch: 114
Loss: 0.8614984154701233
RMSE train: 0.730936	val: 4.094418	test: 3.018807
MAE train: 0.565347	val: 2.896781	test: 2.220578

Epoch: 115
Loss: 0.7699497044086456
RMSE train: 0.735683	val: 4.142027	test: 3.023676
MAE train: 0.573447	val: 2.922324	test: 2.220571

Epoch: 116
Loss: 0.7586421370506287
RMSE train: 0.704450	val: 4.116701	test: 2.987798
MAE train: 0.556250	val: 2.896214	test: 2.193843

Epoch: 117
Loss: 0.7461075782775879
RMSE train: 0.698598	val: 4.086933	test: 2.963749
MAE train: 0.550030	val: 2.855734	test: 2.171576

Epoch: 118
Loss: 0.8746227920055389
RMSE train: 0.694296	val: 4.031974	test: 2.927706
MAE train: 0.547525	val: 2.789962	test: 2.135927

Epoch: 119
Loss: 0.7594774961471558
RMSE train: 0.659897	val: 3.956596	test: 2.898596
MAE train: 0.523662	val: 2.754583	test: 2.121974

Epoch: 120
Loss: 0.7887651026248932
RMSE train: 0.685387	val: 3.972279	test: 2.933682
MAE train: 0.549416	val: 2.779324	test: 2.155780

Epoch: 121
Loss: 0.6975797116756439
RMSE train: 0.655694	val: 3.928680	test: 2.915226
MAE train: 0.526540	val: 2.762592	test: 2.157408

Early stopping
Best (RMSE):	 train: 0.668769	val: 3.686831	test: 2.754298
Best (MAE):	 train: 0.512063	val: 2.664410	test: 2.034634


Epoch: 84
Loss: 0.8865578174591064
RMSE train: 0.803003	val: 3.948958	test: 3.315121
MAE train: 0.609181	val: 2.854084	test: 2.618859

Epoch: 85
Loss: 0.9456052184104919
RMSE train: 0.777535	val: 3.975757	test: 3.261615
MAE train: 0.591098	val: 2.864962	test: 2.567037

Epoch: 86
Loss: 0.8536766767501831
RMSE train: 0.824842	val: 4.055981	test: 3.232776
MAE train: 0.616752	val: 2.903023	test: 2.518651

Epoch: 87
Loss: 0.7949278652667999
RMSE train: 0.821174	val: 4.080766	test: 3.201601
MAE train: 0.601083	val: 2.902283	test: 2.489371

Epoch: 88
Loss: 0.8582107722759247
RMSE train: 0.773310	val: 4.049063	test: 3.188455
MAE train: 0.560332	val: 2.889551	test: 2.494736

Epoch: 89
Loss: 0.9695958495140076
RMSE train: 0.769912	val: 4.011236	test: 3.195345
MAE train: 0.565186	val: 2.869605	test: 2.521634

Epoch: 90
Loss: 0.9648217856884003
RMSE train: 0.771295	val: 3.951305	test: 3.200691
MAE train: 0.576813	val: 2.826714	test: 2.513109

Epoch: 91
Loss: 0.8452075719833374
RMSE train: 0.797065	val: 3.934269	test: 3.207691
MAE train: 0.596276	val: 2.781909	test: 2.484170

Epoch: 92
Loss: 0.9467280805110931
RMSE train: 0.810367	val: 3.923891	test: 3.217371
MAE train: 0.603999	val: 2.744546	test: 2.485213

Epoch: 93
Loss: 0.8621717691421509
RMSE train: 0.827981	val: 3.937273	test: 3.206850
MAE train: 0.624391	val: 2.752884	test: 2.495953

Epoch: 94
Loss: 0.7735206782817841
RMSE train: 0.805605	val: 3.940950	test: 3.186589
MAE train: 0.614409	val: 2.746145	test: 2.493175

Epoch: 95
Loss: 0.82696533203125
RMSE train: 0.767361	val: 3.924075	test: 3.142868
MAE train: 0.588310	val: 2.709639	test: 2.452598

Epoch: 96
Loss: 0.807533323764801
RMSE train: 0.715404	val: 3.902693	test: 3.110189
MAE train: 0.544334	val: 2.679245	test: 2.405108

Epoch: 97
Loss: 0.8617716431617737
RMSE train: 0.657681	val: 3.862545	test: 3.074340
MAE train: 0.501677	val: 2.637166	test: 2.367884

Epoch: 98
Loss: 0.890077143907547
RMSE train: 0.611516	val: 3.852840	test: 3.051963
MAE train: 0.468848	val: 2.641631	test: 2.364567

Epoch: 99
Loss: 0.7377364039421082
RMSE train: 0.596906	val: 3.840424	test: 3.054236
MAE train: 0.458901	val: 2.644674	test: 2.359072

Epoch: 100
Loss: 0.8620996177196503
RMSE train: 0.622902	val: 3.868331	test: 3.057078
MAE train: 0.480492	val: 2.653105	test: 2.342241

Epoch: 101
Loss: 0.8789545595645905
RMSE train: 0.669493	val: 3.926159	test: 3.067515
MAE train: 0.523450	val: 2.681003	test: 2.313253

Epoch: 102
Loss: 0.8212871253490448
RMSE train: 0.748763	val: 3.998130	test: 3.103393
MAE train: 0.588785	val: 2.752156	test: 2.338620

Epoch: 103
Loss: 0.8787434697151184
RMSE train: 0.796297	val: 4.061462	test: 3.130476
MAE train: 0.620963	val: 2.821473	test: 2.378276

Epoch: 104
Loss: 0.8259198665618896
RMSE train: 0.771099	val: 4.076424	test: 3.154518
MAE train: 0.595705	val: 2.857896	test: 2.425755

Epoch: 105
Loss: 0.802087664604187
RMSE train: 0.730644	val: 4.051724	test: 3.189410
MAE train: 0.557482	val: 2.882499	test: 2.475327

Epoch: 106
Loss: 0.807039350271225
RMSE train: 0.695806	val: 4.010546	test: 3.189039
MAE train: 0.522493	val: 2.878143	test: 2.482337

Epoch: 107
Loss: 0.7286820709705353
RMSE train: 0.688723	val: 3.976346	test: 3.156963
MAE train: 0.514331	val: 2.856155	test: 2.448485

Epoch: 108
Loss: 0.8924408555030823
RMSE train: 0.722226	val: 3.966543	test: 3.152280
MAE train: 0.539986	val: 2.842058	test: 2.435167

Epoch: 109
Loss: 0.8258562088012695
RMSE train: 0.804240	val: 4.017397	test: 3.139471
MAE train: 0.599542	val: 2.860930	test: 2.409266

Epoch: 110
Loss: 0.847731202840805
RMSE train: 0.821824	val: 4.042208	test: 3.162067
MAE train: 0.621308	val: 2.887620	test: 2.430521

Epoch: 111
Loss: 0.768794447183609
RMSE train: 0.841964	val: 4.038189	test: 3.185424
MAE train: 0.639473	val: 2.885818	test: 2.462212

Epoch: 112
Loss: 0.8308620750904083
RMSE train: 0.800806	val: 4.004029	test: 3.161876
MAE train: 0.606090	val: 2.834138	test: 2.457451

Epoch: 113
Loss: 0.8259743750095367
RMSE train: 0.755379	val: 3.990884	test: 3.129562
MAE train: 0.571831	val: 2.787974	test: 2.419040

Epoch: 114
Loss: 0.8564706146717072
RMSE train: 0.765748	val: 3.997298	test: 3.111783
MAE train: 0.573233	val: 2.776498	test: 2.393370

Epoch: 115
Loss: 0.8090805411338806
RMSE train: 0.747386	val: 3.978326	test: 3.077952
MAE train: 0.559238	val: 2.740873	test: 2.368446

Epoch: 116
Loss: 0.8333576321601868
RMSE train: 0.727875	val: 3.986719	test: 3.057404
MAE train: 0.538458	val: 2.736326	test: 2.381270

Epoch: 117
Loss: 0.8081607520580292
RMSE train: 0.750906	val: 3.989781	test: 3.050362
MAE train: 0.549439	val: 2.749015	test: 2.386054

Epoch: 118
Loss: 0.8711769878864288
RMSE train: 0.747508	val: 3.934561	test: 3.026177
MAE train: 0.550340	val: 2.708330	test: 2.355648

Epoch: 119
Loss: 0.8024933040142059
RMSE train: 0.739150	val: 3.900806	test: 3.024067
MAE train: 0.561094	val: 2.670067	test: 2.322151

Epoch: 120
Loss: 0.7368670403957367
RMSE train: 0.726539	val: 3.884886	test: 3.061115
MAE train: 0.558266	val: 2.647586	test: 2.340173

Epoch: 121
Loss: 0.7364393174648285
RMSE train: 0.764266	val: 3.895132	test: 3.124983
MAE train: 0.593289	val: 2.641738	test: 2.377589

Epoch: 122
Loss: 0.9024994671344757
RMSE train: 0.782234	val: 3.908934	test: 3.188078
MAE train: 0.611444	val: 2.652702	test: 2.449856

Epoch: 123
Loss: 0.7733669877052307
RMSE train: 0.776804	val: 3.883643	test: 3.184443
MAE train: 0.599696	val: 2.644561	test: 2.483906

Epoch: 124
Loss: 0.8058575391769409
RMSE train: 0.751877	val: 3.839152	test: 3.133517
MAE train: 0.571910	val: 2.643475	test: 2.459635

Epoch: 125
Loss: 0.9558208286762238
RMSE train: 0.754712	val: 3.815451	test: 3.096954
MAE train: 0.567324	val: 2.654790	test: 2.414197

Epoch: 126
Loss: 0.7722149193286896
RMSE train: 0.770962	val: 3.832114	test: 3.077241
MAE train: 0.569209	val: 2.678477	test: 2.390338

Epoch: 127
Loss: 0.7741105854511261
RMSE train: 0.773761	val: 3.883262	test: 3.092958
MAE train: 0.570084	val: 2.712019	test: 2.413053

Epoch: 128
Loss: 0.8199194073677063
RMSE train: 0.804524	val: 3.946925	test: 3.130275
MAE train: 0.605228	val: 2.736215	test: 2.448516

Epoch: 129
Loss: 0.7636500000953674
RMSE train: 0.803113	val: 3.958214	test: 3.180184
MAE train: 0.617609	val: 2.717275	test: 2.497261

Epoch: 130
Loss: 0.7628967463970184
RMSE train: 0.775375	val: 3.924155	test: 3.194541
MAE train: 0.599618	val: 2.696492	test: 2.515966

Epoch: 131
Loss: 0.7184565663337708
RMSE train: 0.745633	val: 3.910605	test: 3.192507
MAE train: 0.576903	val: 2.686752	test: 2.522247

Epoch: 132
Loss: 0.8099854588508606
RMSE train: 0.743690	val: 3.899232	test: 3.174180
MAE train: 0.575877	val: 2.706639	test: 2.522067

Epoch: 133
Loss: 0.6946578919887543
RMSE train: 0.743327	val: 3.898084	test: 3.143472
MAE train: 0.575424	val: 2.707423	test: 2.499394

Epoch: 134
Loss: 0.6786000430583954
RMSE train: 0.733810	val: 3.887354	test: 3.118309
MAE train: 0.564781	val: 2.683880	test: 2.462800

Epoch: 135
Loss: 0.6973770260810852
RMSE train: 0.779376	val: 3.901921	test: 3.132606
MAE train: 0.597358	val: 2.673206	test: 2.450339

Epoch: 136
Loss: 0.638721227645874
RMSE train: 0.801347	val: 3.926857	test: 3.134450
MAE train: 0.619960	val: 2.692198	test: 2.447027

Epoch: 137
Loss: 0.6291468441486359
RMSE train: 0.789837	val: 3.921444	test: 3.135940
MAE train: 0.612749	val: 2.716082	test: 2.463393

Epoch: 138
Loss: 0.756839394569397
RMSE train: 0.725938	val: 3.872011	test: 3.119885
MAE train: 0.565077	val: 2.676764	test: 2.459750

Epoch: 139
Loss: 0.5736229121685028
RMSE train: 0.690096	val: 3.842165	test: 3.115169
MAE train: 0.539859	val: 2.634211	test: 2.449277

Epoch: 140
Loss: 0.6636172533035278
RMSE train: 0.716036	val: 3.853319	test: 3.089220
MAE train: 0.557386	val: 2.623338	test: 2.397193

Epoch: 141
Loss: 0.7249294221401215
RMSE train: 0.727430	val: 3.882028	test: 3.043562
MAE train: 0.553693	val: 2.636011	test: 2.336921

Epoch: 142
Loss: 0.5910955965518951
RMSE train: 0.738232	val: 3.934028	test: 2.997964
MAE train: 0.547855	val: 2.671338	test: 2.293901

Epoch: 143
Loss: 0.7211992144584656
RMSE train: 0.749669	val: 3.977330	test: 2.993509
MAE train: 0.555772	val: 2.698292	test: 2.279772

Epoch: 144
Loss: 0.680551677942276
RMSE train: 0.716164	val: 3.955395	test: 3.005882
RMSE train: 0.941012	val: 4.841381	test: 3.622834
MAE train: 0.728866	val: 3.317690	test: 2.859517

Epoch: 84
Loss: 1.1501872539520264
RMSE train: 0.957007	val: 4.877013	test: 3.612666
MAE train: 0.739105	val: 3.328310	test: 2.823424

Epoch: 85
Loss: 0.9516475200653076
RMSE train: 0.896790	val: 4.816764	test: 3.561663
MAE train: 0.688617	val: 3.262592	test: 2.757089

Epoch: 86
Loss: 0.9624786972999573
RMSE train: 0.828317	val: 4.735017	test: 3.508615
MAE train: 0.637407	val: 3.207870	test: 2.717415

Epoch: 87
Loss: 0.9390259087085724
RMSE train: 0.736801	val: 4.650495	test: 3.417084
MAE train: 0.563497	val: 3.145963	test: 2.628164

Epoch: 88
Loss: 0.9700928926467896
RMSE train: 0.685573	val: 4.609829	test: 3.339132
MAE train: 0.528184	val: 3.110965	test: 2.540906

Epoch: 89
Loss: 1.0731033086776733
RMSE train: 0.723121	val: 4.658224	test: 3.331943
MAE train: 0.567392	val: 3.135463	test: 2.511401

Epoch: 90
Loss: 1.0032514929771423
RMSE train: 0.792991	val: 4.721077	test: 3.378237
MAE train: 0.624027	val: 3.169124	test: 2.541214

Epoch: 91
Loss: 0.8865883350372314
RMSE train: 0.812925	val: 4.744704	test: 3.408872
MAE train: 0.636546	val: 3.176931	test: 2.568664

Epoch: 92
Loss: 0.907544881105423
RMSE train: 0.767210	val: 4.735222	test: 3.408299
MAE train: 0.601419	val: 3.177371	test: 2.577438

Epoch: 93
Loss: 0.893992006778717
RMSE train: 0.715445	val: 4.715638	test: 3.407468
MAE train: 0.561976	val: 3.176052	test: 2.595404

Epoch: 94
Loss: 1.176044523715973
RMSE train: 0.704057	val: 4.722020	test: 3.439251
MAE train: 0.556258	val: 3.189704	test: 2.636709

Epoch: 95
Loss: 0.8053069710731506
RMSE train: 0.747203	val: 4.770996	test: 3.496390
MAE train: 0.592173	val: 3.220683	test: 2.690831

Epoch: 96
Loss: 0.8607898652553558
RMSE train: 0.793528	val: 4.804333	test: 3.554897
MAE train: 0.625064	val: 3.232013	test: 2.742679

Epoch: 97
Loss: 0.8773261308670044
RMSE train: 0.803353	val: 4.806514	test: 3.590542
MAE train: 0.631416	val: 3.241496	test: 2.785036

Epoch: 98
Loss: 0.8847837150096893
RMSE train: 0.841388	val: 4.827926	test: 3.641167
MAE train: 0.659537	val: 3.271445	test: 2.844379

Epoch: 99
Loss: 0.7580349445343018
RMSE train: 0.870303	val: 4.834421	test: 3.654734
MAE train: 0.682837	val: 3.292222	test: 2.863489

Epoch: 100
Loss: 0.8978202939033508
RMSE train: 0.858585	val: 4.814573	test: 3.626661
MAE train: 0.678003	val: 3.290607	test: 2.839845

Epoch: 101
Loss: 0.917860746383667
RMSE train: 0.821310	val: 4.793604	test: 3.576188
MAE train: 0.649865	val: 3.281435	test: 2.794862

Epoch: 102
Loss: 0.8042972385883331
RMSE train: 0.816520	val: 4.823403	test: 3.566043
MAE train: 0.648873	val: 3.298105	test: 2.777955

Epoch: 103
Loss: 0.6996913254261017
RMSE train: 0.857664	val: 4.887091	test: 3.602317
MAE train: 0.682735	val: 3.331505	test: 2.802706

Epoch: 104
Loss: 0.7544859647750854
RMSE train: 0.874464	val: 4.920060	test: 3.617884
MAE train: 0.689471	val: 3.346477	test: 2.808918

Epoch: 105
Loss: 0.8021662533283234
RMSE train: 0.866423	val: 4.932233	test: 3.617292
MAE train: 0.679070	val: 3.356057	test: 2.808246

Epoch: 106
Loss: 0.7479191422462463
RMSE train: 0.877770	val: 4.946728	test: 3.618681
MAE train: 0.687624	val: 3.369343	test: 2.812270

Epoch: 107
Loss: 0.8026608228683472
RMSE train: 0.842690	val: 4.912544	test: 3.578085
MAE train: 0.661159	val: 3.337051	test: 2.765092

Epoch: 108
Loss: 0.873359888792038
RMSE train: 0.863208	val: 4.929537	test: 3.568753
MAE train: 0.680407	val: 3.346280	test: 2.750407

Epoch: 109
Loss: 0.7277496159076691
RMSE train: 0.888392	val: 4.934936	test: 3.543968
MAE train: 0.694950	val: 3.347500	test: 2.721652

Epoch: 110
Loss: 0.8162465691566467
RMSE train: 0.868643	val: 4.883001	test: 3.547335
MAE train: 0.681644	val: 3.306797	test: 2.726523

Epoch: 111
Loss: 0.7278624475002289
RMSE train: 0.824012	val: 4.831351	test: 3.539765
MAE train: 0.649930	val: 3.258559	test: 2.718143

Epoch: 112
Loss: 0.7334979176521301
RMSE train: 0.802331	val: 4.816977	test: 3.536506
MAE train: 0.634160	val: 3.237970	test: 2.719147

Epoch: 113
Loss: 0.8745253682136536
RMSE train: 0.796863	val: 4.836824	test: 3.529980
MAE train: 0.625066	val: 3.237234	test: 2.712663

Epoch: 114
Loss: 0.7517115771770477
RMSE train: 0.797545	val: 4.843594	test: 3.532576
MAE train: 0.622092	val: 3.226200	test: 2.703762

Epoch: 115
Loss: 0.7267529964447021
RMSE train: 0.807949	val: 4.853247	test: 3.519854
MAE train: 0.629908	val: 3.232471	test: 2.690036

Epoch: 116
Loss: 0.628799557685852
RMSE train: 0.770888	val: 4.814775	test: 3.480817
MAE train: 0.604298	val: 3.207465	test: 2.660715

Epoch: 117
Loss: 0.731596440076828
RMSE train: 0.724718	val: 4.759270	test: 3.457307
MAE train: 0.571494	val: 3.180897	test: 2.652433

Epoch: 118
Loss: 0.7576015591621399
RMSE train: 0.730322	val: 4.755601	test: 3.479046
MAE train: 0.578390	val: 3.187364	test: 2.678217

Epoch: 119
Loss: 0.7139689028263092
RMSE train: 0.744017	val: 4.772275	test: 3.495350
MAE train: 0.589085	val: 3.204646	test: 2.690625

Epoch: 120
Loss: 0.8347337543964386
RMSE train: 0.734930	val: 4.747063	test: 3.507252
MAE train: 0.583652	val: 3.190105	test: 2.700309

Epoch: 121
Loss: 0.7417098581790924
RMSE train: 0.736772	val: 4.736323	test: 3.514643
MAE train: 0.588955	val: 3.183687	test: 2.710821

Early stopping
Best (RMSE):	 train: 0.755987	val: 4.500864	test: 3.549206
Best (MAE):	 train: 0.583701	val: 3.123094	test: 2.838141

RMSE train: 0.809333	val: 4.181659	test: 3.558072
MAE train: 0.638894	val: 2.725351	test: 2.670056

Epoch: 84
Loss: 1.0566436052322388
RMSE train: 0.812304	val: 4.189675	test: 3.653411
MAE train: 0.641048	val: 2.724629	test: 2.773063

Epoch: 85
Loss: 1.126830816268921
RMSE train: 0.824763	val: 4.216398	test: 3.739933
MAE train: 0.652893	val: 2.766991	test: 2.862819

Epoch: 86
Loss: 1.17268705368042
RMSE train: 0.834630	val: 4.306284	test: 3.788324
MAE train: 0.656314	val: 2.852363	test: 2.916259

Epoch: 87
Loss: 1.1187001466751099
RMSE train: 0.800984	val: 4.291313	test: 3.795120
MAE train: 0.630327	val: 2.847698	test: 2.923075

Epoch: 88
Loss: 0.9771168529987335
RMSE train: 0.755867	val: 4.221860	test: 3.719262
MAE train: 0.598548	val: 2.784648	test: 2.848295

Epoch: 89
Loss: 1.2310205101966858
RMSE train: 0.742823	val: 4.183212	test: 3.635056
MAE train: 0.591498	val: 2.748959	test: 2.766758

Epoch: 90
Loss: 1.0777118504047394
RMSE train: 0.738997	val: 4.147805	test: 3.572488
MAE train: 0.586823	val: 2.744084	test: 2.707201

Epoch: 91
Loss: 1.053153395652771
RMSE train: 0.735128	val: 4.117431	test: 3.526579
MAE train: 0.582834	val: 2.739228	test: 2.654920

Epoch: 92
Loss: 1.09867662191391
RMSE train: 0.759278	val: 4.138845	test: 3.492196
MAE train: 0.603579	val: 2.746268	test: 2.602477

Epoch: 93
Loss: 0.9190061688423157
RMSE train: 0.779698	val: 4.156848	test: 3.448312
MAE train: 0.622732	val: 2.754291	test: 2.545525

Epoch: 94
Loss: 1.0168690085411072
RMSE train: 0.801960	val: 4.160206	test: 3.436876
MAE train: 0.643786	val: 2.751776	test: 2.526789

Epoch: 95
Loss: 0.957537978887558
RMSE train: 0.822363	val: 4.191987	test: 3.480590
MAE train: 0.659470	val: 2.769201	test: 2.588414

Epoch: 96
Loss: 1.0268184542655945
RMSE train: 0.814079	val: 4.154714	test: 3.515308
MAE train: 0.647218	val: 2.755270	test: 2.632303

Epoch: 97
Loss: 0.8827397525310516
RMSE train: 0.821729	val: 4.108463	test: 3.547974
MAE train: 0.648776	val: 2.739987	test: 2.665553

Epoch: 98
Loss: 1.0435075163841248
RMSE train: 0.822102	val: 4.057972	test: 3.569744
MAE train: 0.646302	val: 2.705828	test: 2.685351

Epoch: 99
Loss: 1.0117116868495941
RMSE train: 0.836987	val: 4.048614	test: 3.584958
MAE train: 0.656403	val: 2.670136	test: 2.699146

Epoch: 100
Loss: 0.9618719220161438
RMSE train: 0.860952	val: 4.131357	test: 3.624617
MAE train: 0.678657	val: 2.716616	test: 2.743535

Epoch: 101
Loss: 0.981685996055603
RMSE train: 0.861709	val: 4.266461	test: 3.663076
MAE train: 0.677215	val: 2.839031	test: 2.788021

Epoch: 102
Loss: 0.9668287038803101
RMSE train: 0.844286	val: 4.367319	test: 3.650572
MAE train: 0.655475	val: 2.905693	test: 2.764418

Epoch: 103
Loss: 0.841016948223114
RMSE train: 0.810842	val: 4.420927	test: 3.611486
MAE train: 0.625000	val: 2.912459	test: 2.707530

Epoch: 104
Loss: 0.8226671516895294
RMSE train: 0.765541	val: 4.391785	test: 3.547356
MAE train: 0.593819	val: 2.865958	test: 2.628868

Epoch: 105
Loss: 0.9741572439670563
RMSE train: 0.722111	val: 4.284809	test: 3.506709
MAE train: 0.568789	val: 2.790955	test: 2.571759

Epoch: 106
Loss: 0.9855866134166718
RMSE train: 0.755847	val: 4.232309	test: 3.510973
MAE train: 0.604805	val: 2.752338	test: 2.564701

Epoch: 107
Loss: 1.0813303589820862
RMSE train: 0.839419	val: 4.284960	test: 3.576051
MAE train: 0.673749	val: 2.792111	test: 2.623995

Epoch: 108
Loss: 0.8784371316432953
RMSE train: 0.933145	val: 4.439699	test: 3.679985
MAE train: 0.744593	val: 2.926541	test: 2.734987

Epoch: 109
Loss: 0.9762523770332336
RMSE train: 0.998776	val: 4.563983	test: 3.737091
MAE train: 0.790230	val: 3.045235	test: 2.800252

Epoch: 110
Loss: 0.9525194764137268
RMSE train: 0.959413	val: 4.533151	test: 3.710097
MAE train: 0.755447	val: 3.010428	test: 2.774124

Epoch: 111
Loss: 0.9348666369915009
RMSE train: 0.865046	val: 4.416415	test: 3.626571
MAE train: 0.678439	val: 2.916010	test: 2.687211

Epoch: 112
Loss: 0.9688757956027985
RMSE train: 0.794323	val: 4.310342	test: 3.558539
MAE train: 0.628368	val: 2.826186	test: 2.622512

Epoch: 113
Loss: 0.8303684294223785
RMSE train: 0.726714	val: 4.184567	test: 3.496673
MAE train: 0.584024	val: 2.738780	test: 2.564113

Epoch: 114
Loss: 0.8120202720165253
RMSE train: 0.695276	val: 4.113707	test: 3.480650
MAE train: 0.562698	val: 2.684899	test: 2.548965

Epoch: 115
Loss: 0.8244074881076813
RMSE train: 0.698016	val: 4.146623	test: 3.501731
MAE train: 0.567422	val: 2.700581	test: 2.573702

Epoch: 116
Loss: 0.7873719036579132
RMSE train: 0.738485	val: 4.294885	test: 3.567181
MAE train: 0.599211	val: 2.790710	test: 2.653731

Epoch: 117
Loss: 0.813922792673111
RMSE train: 0.777619	val: 4.398790	test: 3.604342
MAE train: 0.629120	val: 2.864701	test: 2.710745

Epoch: 118
Loss: 0.8724499046802521
RMSE train: 0.798969	val: 4.448257	test: 3.598480
MAE train: 0.643444	val: 2.894050	test: 2.712497

Epoch: 119
Loss: 0.7130440175533295
RMSE train: 0.773071	val: 4.417585	test: 3.593563
MAE train: 0.620800	val: 2.874633	test: 2.712006

Epoch: 120
Loss: 0.8479028940200806
RMSE train: 0.755645	val: 4.374477	test: 3.587502
MAE train: 0.609444	val: 2.840131	test: 2.700261

Epoch: 121
Loss: 0.7516793310642242
RMSE train: 0.711719	val: 4.299614	test: 3.590883
MAE train: 0.573746	val: 2.795937	test: 2.690828

Early stopping
Best (RMSE):	 train: 1.090967	val: 3.937676	test: 3.550096
Best (MAE):	 train: 0.835812	val: 2.561933	test: 2.735147

RMSE train: 0.931597	val: 4.442001	test: 3.853014
MAE train: 0.737834	val: 3.182445	test: 3.154386

Epoch: 84
Loss: 1.1945735812187195
RMSE train: 0.843462	val: 4.295304	test: 3.783564
MAE train: 0.665287	val: 3.095339	test: 3.107052

Epoch: 85
Loss: 1.0598433911800385
RMSE train: 0.781202	val: 4.191455	test: 3.728629
MAE train: 0.615171	val: 3.031852	test: 3.062407

Epoch: 86
Loss: 1.1547968089580536
RMSE train: 0.741890	val: 4.138925	test: 3.690687
MAE train: 0.590458	val: 2.989474	test: 3.033076

Epoch: 87
Loss: 0.9356130659580231
RMSE train: 0.751273	val: 4.156089	test: 3.664145
MAE train: 0.596776	val: 3.008288	test: 3.003452

Epoch: 88
Loss: 1.1112523972988129
RMSE train: 0.750778	val: 4.150130	test: 3.649437
MAE train: 0.593661	val: 3.020211	test: 2.990778

Epoch: 89
Loss: 1.0854771733283997
RMSE train: 0.756180	val: 4.159886	test: 3.654281
MAE train: 0.597877	val: 3.050127	test: 2.998229

Epoch: 90
Loss: 0.9999324977397919
RMSE train: 0.788728	val: 4.233949	test: 3.694722
MAE train: 0.627728	val: 3.113611	test: 3.044976

Epoch: 91
Loss: 1.128147840499878
RMSE train: 0.799953	val: 4.289005	test: 3.730900
MAE train: 0.640285	val: 3.152351	test: 3.093587

Epoch: 92
Loss: 0.9721879065036774
RMSE train: 0.848561	val: 4.375691	test: 3.769315
MAE train: 0.681528	val: 3.204044	test: 3.132030

Epoch: 93
Loss: 0.9150889217853546
RMSE train: 0.896723	val: 4.455466	test: 3.793069
MAE train: 0.715591	val: 3.266684	test: 3.152886

Epoch: 94
Loss: 1.0120121836662292
RMSE train: 0.910462	val: 4.494654	test: 3.797917
MAE train: 0.723213	val: 3.281946	test: 3.152504

Epoch: 95
Loss: 0.9982413947582245
RMSE train: 0.905157	val: 4.504682	test: 3.764732
MAE train: 0.712567	val: 3.271404	test: 3.110872

Epoch: 96
Loss: 0.9921241104602814
RMSE train: 0.831164	val: 4.404825	test: 3.654934
MAE train: 0.650482	val: 3.183653	test: 2.994726

Epoch: 97
Loss: 1.0302160382270813
RMSE train: 0.728529	val: 4.287777	test: 3.573417
MAE train: 0.575429	val: 3.084742	test: 2.905818

Epoch: 98
Loss: 0.9428322315216064
RMSE train: 0.697840	val: 4.230219	test: 3.547665
MAE train: 0.550498	val: 3.038650	test: 2.875092

Epoch: 99
Loss: 1.049402117729187
RMSE train: 0.735271	val: 4.285955	test: 3.584259
MAE train: 0.579774	val: 3.091377	test: 2.913861

Epoch: 100
Loss: 0.9094625115394592
RMSE train: 0.772789	val: 4.353877	test: 3.643610
MAE train: 0.607806	val: 3.138593	test: 2.978155

Epoch: 101
Loss: 0.8786669671535492
RMSE train: 0.791982	val: 4.383161	test: 3.696391
MAE train: 0.621126	val: 3.161052	test: 3.035437

Epoch: 102
Loss: 0.9181952178478241
RMSE train: 0.795371	val: 4.381464	test: 3.734911
MAE train: 0.620343	val: 3.137343	test: 3.085019

Epoch: 103
Loss: 0.946291446685791
RMSE train: 0.832783	val: 4.428858	test: 3.800430
MAE train: 0.650249	val: 3.142669	test: 3.153057

Epoch: 104
Loss: 0.9853275120258331
RMSE train: 0.844972	val: 4.474117	test: 3.841979
MAE train: 0.664012	val: 3.173316	test: 3.198348

Epoch: 105
Loss: 0.8440858721733093
RMSE train: 0.850460	val: 4.532504	test: 3.895200
MAE train: 0.670719	val: 3.225110	test: 3.251460

Epoch: 106
Loss: 0.8582780659198761
RMSE train: 0.835191	val: 4.568463	test: 3.915362
MAE train: 0.656684	val: 3.256309	test: 3.259723

Epoch: 107
Loss: 0.8311498463153839
RMSE train: 0.828263	val: 4.564938	test: 3.919185
MAE train: 0.648310	val: 3.276231	test: 3.256716

Epoch: 108
Loss: 0.8431665301322937
RMSE train: 0.835124	val: 4.562404	test: 3.901546
MAE train: 0.651858	val: 3.307142	test: 3.242135

Epoch: 109
Loss: 0.8109855651855469
RMSE train: 0.827231	val: 4.540273	test: 3.864509
MAE train: 0.633030	val: 3.305035	test: 3.217917

Epoch: 110
Loss: 0.831224799156189
RMSE train: 0.784084	val: 4.441255	test: 3.791682
MAE train: 0.581678	val: 3.236524	test: 3.155290

Epoch: 111
Loss: 0.8908239603042603
RMSE train: 0.731284	val: 4.325635	test: 3.706158
MAE train: 0.541061	val: 3.156272	test: 3.067792

Epoch: 112
Loss: 0.8451753258705139
RMSE train: 0.696692	val: 4.321402	test: 3.664127
MAE train: 0.530208	val: 3.148518	test: 3.010140

Epoch: 113
Loss: 0.8701387941837311
RMSE train: 0.696710	val: 4.357839	test: 3.653219
MAE train: 0.545635	val: 3.151291	test: 2.987934

Epoch: 114
Loss: 0.9154067039489746
RMSE train: 0.724753	val: 4.393562	test: 3.639737
MAE train: 0.566831	val: 3.163870	test: 2.971918

Epoch: 115
Loss: 0.7726596593856812
RMSE train: 0.728579	val: 4.396542	test: 3.627649
MAE train: 0.568470	val: 3.161212	test: 2.955055

Epoch: 116
Loss: 0.7753393352031708
RMSE train: 0.701347	val: 4.356493	test: 3.605395
MAE train: 0.547309	val: 3.128535	test: 2.930187

Epoch: 117
Loss: 0.845167726278305
RMSE train: 0.646542	val: 4.289021	test: 3.579523
MAE train: 0.502284	val: 3.095020	test: 2.912427

Epoch: 118
Loss: 0.8683717846870422
RMSE train: 0.623624	val: 4.248096	test: 3.580028
MAE train: 0.483357	val: 3.063857	test: 2.917278

Epoch: 119
Loss: 0.7223698496818542
RMSE train: 0.599478	val: 4.190864	test: 3.572175
MAE train: 0.465129	val: 3.040607	test: 2.915788

Epoch: 120
Loss: 0.7058187425136566
RMSE train: 0.617530	val: 4.171412	test: 3.564682
MAE train: 0.475518	val: 3.041553	test: 2.912640

Epoch: 121
Loss: 0.8424942791461945
RMSE train: 0.616608	val: 4.149306	test: 3.556928
MAE train: 0.475554	val: 3.015540	test: 2.908172

Early stopping
Best (RMSE):	 train: 0.741890	val: 4.138925	test: 3.690687
Best (MAE):	 train: 0.590458	val: 2.989474	test: 3.033076

RMSE train: 0.864853	val: 5.049194	test: 3.112087
MAE train: 0.698292	val: 3.302701	test: 2.492670

Epoch: 84
Loss: 1.1685449481010437
RMSE train: 0.857182	val: 5.069903	test: 3.119920
MAE train: 0.695083	val: 3.336978	test: 2.457402

Epoch: 85
Loss: 1.1591708362102509
RMSE train: 0.886227	val: 5.098227	test: 3.165870
MAE train: 0.717941	val: 3.370321	test: 2.477598

Epoch: 86
Loss: 1.023974359035492
RMSE train: 0.880767	val: 5.114006	test: 3.200545
MAE train: 0.714655	val: 3.379424	test: 2.513732

Epoch: 87
Loss: 1.0015342235565186
RMSE train: 0.876734	val: 5.146805	test: 3.231544
MAE train: 0.711940	val: 3.397222	test: 2.547550

Epoch: 88
Loss: 1.1629552841186523
RMSE train: 0.859044	val: 5.202581	test: 3.247186
MAE train: 0.695862	val: 3.426534	test: 2.542562

Epoch: 89
Loss: 1.1839538216590881
RMSE train: 0.837055	val: 5.245460	test: 3.238133
MAE train: 0.673172	val: 3.446867	test: 2.498252

Epoch: 90
Loss: 1.0035172700881958
RMSE train: 0.837769	val: 5.305785	test: 3.248292
MAE train: 0.665512	val: 3.478471	test: 2.484640

Epoch: 91
Loss: 1.1581286787986755
RMSE train: 0.828629	val: 5.328880	test: 3.234538
MAE train: 0.650797	val: 3.488894	test: 2.464775

Epoch: 92
Loss: 1.1259227991104126
RMSE train: 0.798118	val: 5.301488	test: 3.201669
MAE train: 0.623561	val: 3.472399	test: 2.447773

Epoch: 93
Loss: 1.0121227204799652
RMSE train: 0.752529	val: 5.219056	test: 3.155599
MAE train: 0.584168	val: 3.421988	test: 2.426826

Epoch: 94
Loss: 0.9683564901351929
RMSE train: 0.696539	val: 5.085606	test: 3.068895
MAE train: 0.536919	val: 3.350192	test: 2.369300

Epoch: 95
Loss: 0.9934244155883789
RMSE train: 0.681962	val: 5.002987	test: 3.019313
MAE train: 0.514265	val: 3.300846	test: 2.345121

Epoch: 96
Loss: 0.9956491589546204
RMSE train: 0.708215	val: 5.002425	test: 3.040260
MAE train: 0.531550	val: 3.289437	test: 2.370956

Epoch: 97
Loss: 1.0245416760444641
RMSE train: 0.764166	val: 5.085038	test: 3.129112
MAE train: 0.589763	val: 3.345151	test: 2.434274

Epoch: 98
Loss: 0.9097310304641724
RMSE train: 0.832604	val: 5.174278	test: 3.224940
MAE train: 0.662620	val: 3.420247	test: 2.491106

Epoch: 99
Loss: 0.8696450889110565
RMSE train: 0.890159	val: 5.262401	test: 3.304624
MAE train: 0.722397	val: 3.499667	test: 2.549809

Epoch: 100
Loss: 1.0226360857486725
RMSE train: 0.922234	val: 5.341746	test: 3.356591
MAE train: 0.765706	val: 3.547913	test: 2.592970

Epoch: 101
Loss: 1.2209811806678772
RMSE train: 0.952550	val: 5.391611	test: 3.374412
MAE train: 0.788493	val: 3.569666	test: 2.609190

Epoch: 102
Loss: 1.0084367394447327
RMSE train: 0.975768	val: 5.402740	test: 3.352416
MAE train: 0.796815	val: 3.558939	test: 2.599319

Epoch: 103
Loss: 1.0161585211753845
RMSE train: 0.925841	val: 5.327879	test: 3.292556
MAE train: 0.755607	val: 3.486404	test: 2.581335

Epoch: 104
Loss: 1.0601531863212585
RMSE train: 0.856111	val: 5.200781	test: 3.225551
MAE train: 0.704958	val: 3.391314	test: 2.560357

Epoch: 105
Loss: 0.9863156378269196
RMSE train: 0.791876	val: 5.054330	test: 3.157880
MAE train: 0.660559	val: 3.291974	test: 2.521379

Epoch: 106
Loss: 0.9183732867240906
RMSE train: 0.820605	val: 5.029678	test: 3.179779
MAE train: 0.689308	val: 3.294613	test: 2.524863

Epoch: 107
Loss: 1.0307610929012299
RMSE train: 0.876796	val: 5.028214	test: 3.201368
MAE train: 0.727226	val: 3.321639	test: 2.518512

Epoch: 108
Loss: 0.9412096440792084
RMSE train: 0.911323	val: 5.091805	test: 3.245400
MAE train: 0.757768	val: 3.375004	test: 2.542202

Epoch: 109
Loss: 1.0338012278079987
RMSE train: 0.921951	val: 5.201494	test: 3.285990
MAE train: 0.778950	val: 3.439020	test: 2.576688

Epoch: 110
Loss: 0.9386668503284454
RMSE train: 0.934679	val: 5.280898	test: 3.284879
MAE train: 0.776074	val: 3.482188	test: 2.572669

Epoch: 111
Loss: 0.9465033113956451
RMSE train: 0.916031	val: 5.306752	test: 3.243076
MAE train: 0.747853	val: 3.503135	test: 2.510415

Epoch: 112
Loss: 0.8999398648738861
RMSE train: 0.824564	val: 5.252709	test: 3.152108
MAE train: 0.671311	val: 3.470083	test: 2.419694

Epoch: 113
Loss: 0.9255511462688446
RMSE train: 0.738018	val: 5.156394	test: 3.059109
MAE train: 0.599411	val: 3.402066	test: 2.344837

Epoch: 114
Loss: 0.9846431016921997
RMSE train: 0.715325	val: 5.098383	test: 3.018911
MAE train: 0.581443	val: 3.350381	test: 2.318158

Epoch: 115
Loss: 0.9406393766403198
RMSE train: 0.729301	val: 5.111220	test: 3.030804
MAE train: 0.594513	val: 3.349500	test: 2.349334

Epoch: 116
Loss: 0.8332837522029877
RMSE train: 0.767082	val: 5.166394	test: 3.069123
MAE train: 0.622820	val: 3.380987	test: 2.399545

Epoch: 117
Loss: 0.7134346961975098
RMSE train: 0.767585	val: 5.172963	test: 3.077271
MAE train: 0.620055	val: 3.384662	test: 2.422665

Epoch: 118
Loss: 0.8785005807876587
RMSE train: 0.729454	val: 5.132787	test: 3.064697
MAE train: 0.586022	val: 3.364743	test: 2.434479

Epoch: 119
Loss: 0.745096892118454
RMSE train: 0.665691	val: 5.064645	test: 3.041919
MAE train: 0.539432	val: 3.326696	test: 2.430540

Epoch: 120
Loss: 0.824986457824707
RMSE train: 0.674856	val: 5.061341	test: 3.082941
MAE train: 0.551562	val: 3.318667	test: 2.466309

Epoch: 121
Loss: 0.8766105771064758
RMSE train: 0.726833	val: 5.090989	test: 3.142141
MAE train: 0.594738	val: 3.336114	test: 2.507182

Early stopping
Best (RMSE):	 train: 1.154511	val: 4.974019	test: 3.276693
Best (MAE):	 train: 0.875889	val: 3.384869	test: 2.516183

RMSE train: 1.026518	val: 4.784659	test: 3.531048
MAE train: 0.840821	val: 3.375236	test: 2.829014

Epoch: 84
Loss: 1.0779740810394287
RMSE train: 1.027398	val: 4.794961	test: 3.525128
MAE train: 0.831379	val: 3.393196	test: 2.819215

Epoch: 85
Loss: 1.0572896003723145
RMSE train: 0.969683	val: 4.710858	test: 3.476420
MAE train: 0.781834	val: 3.361097	test: 2.778134

Epoch: 86
Loss: 0.9399572610855103
RMSE train: 0.885263	val: 4.588572	test: 3.398094
MAE train: 0.709464	val: 3.311089	test: 2.708121

Epoch: 87
Loss: 0.9842761754989624
RMSE train: 0.796956	val: 4.442528	test: 3.301963
MAE train: 0.632529	val: 3.235580	test: 2.619143

Epoch: 88
Loss: 1.0002483129501343
RMSE train: 0.769204	val: 4.385632	test: 3.256152
MAE train: 0.610094	val: 3.199755	test: 2.568133

Epoch: 89
Loss: 1.085257351398468
RMSE train: 0.823350	val: 4.449259	test: 3.289084
MAE train: 0.658878	val: 3.225386	test: 2.585648

Epoch: 90
Loss: 0.9863766133785248
RMSE train: 0.903008	val: 4.551153	test: 3.368488
MAE train: 0.734532	val: 3.278366	test: 2.662352

Epoch: 91
Loss: 0.8971344828605652
RMSE train: 0.943632	val: 4.609588	test: 3.407051
MAE train: 0.769219	val: 3.292530	test: 2.692442

Epoch: 92
Loss: 0.9725887179374695
RMSE train: 0.915599	val: 4.560786	test: 3.400110
MAE train: 0.750227	val: 3.242206	test: 2.688477

Epoch: 93
Loss: 0.8496122658252716
RMSE train: 0.893517	val: 4.538695	test: 3.384018
MAE train: 0.729943	val: 3.218691	test: 2.668963

Epoch: 94
Loss: 1.184981107711792
RMSE train: 0.874187	val: 4.505750	test: 3.367500
MAE train: 0.706193	val: 3.200054	test: 2.654823

Epoch: 95
Loss: 0.8861021101474762
RMSE train: 0.855265	val: 4.489777	test: 3.345537
MAE train: 0.678865	val: 3.185540	test: 2.636211

Epoch: 96
Loss: 0.8483085930347443
RMSE train: 0.849986	val: 4.484077	test: 3.348303
MAE train: 0.670413	val: 3.185312	test: 2.644906

Epoch: 97
Loss: 0.9371467530727386
RMSE train: 0.853664	val: 4.503092	test: 3.379808
MAE train: 0.683269	val: 3.205227	test: 2.683787

Epoch: 98
Loss: 0.8439500629901886
RMSE train: 0.908074	val: 4.585403	test: 3.458759
MAE train: 0.738059	val: 3.263614	test: 2.774315

Epoch: 99
Loss: 0.8780516088008881
RMSE train: 0.971909	val: 4.681930	test: 3.501068
MAE train: 0.783441	val: 3.330163	test: 2.804480

Epoch: 100
Loss: 0.8656437397003174
RMSE train: 0.993207	val: 4.730529	test: 3.476168
MAE train: 0.785814	val: 3.354960	test: 2.756753

Epoch: 101
Loss: 0.9159591495990753
RMSE train: 0.933721	val: 4.676160	test: 3.400814
MAE train: 0.740413	val: 3.310730	test: 2.680355

Epoch: 102
Loss: 0.8756647706031799
RMSE train: 0.864617	val: 4.574913	test: 3.337424
MAE train: 0.694089	val: 3.246839	test: 2.624093

Epoch: 103
Loss: 0.9782576560974121
RMSE train: 0.863296	val: 4.540499	test: 3.330660
MAE train: 0.697679	val: 3.239504	test: 2.622782

Epoch: 104
Loss: 0.842162549495697
RMSE train: 0.891793	val: 4.536362	test: 3.364112
MAE train: 0.721544	val: 3.262911	test: 2.665696

Epoch: 105
Loss: 0.8391983807086945
RMSE train: 0.906100	val: 4.525605	test: 3.385094
MAE train: 0.733802	val: 3.270362	test: 2.687764

Epoch: 106
Loss: 0.8630271553993225
RMSE train: 0.904260	val: 4.523141	test: 3.398715
MAE train: 0.735892	val: 3.270794	test: 2.696802

Epoch: 107
Loss: 0.7742802798748016
RMSE train: 0.861830	val: 4.495308	test: 3.367497
MAE train: 0.706599	val: 3.241037	test: 2.656934

Epoch: 108
Loss: 0.9137879908084869
RMSE train: 0.834171	val: 4.529621	test: 3.341902
MAE train: 0.679069	val: 3.245688	test: 2.633481

Epoch: 109
Loss: 0.7175553143024445
RMSE train: 0.838409	val: 4.582619	test: 3.343073
MAE train: 0.679807	val: 3.268307	test: 2.643841

Epoch: 110
Loss: 0.8557513952255249
RMSE train: 0.873267	val: 4.650943	test: 3.378932
MAE train: 0.704865	val: 3.300239	test: 2.684506

Epoch: 111
Loss: 0.8019790351390839
RMSE train: 0.889716	val: 4.696128	test: 3.390804
MAE train: 0.715012	val: 3.309998	test: 2.695198

Epoch: 112
Loss: 0.8039401471614838
RMSE train: 0.933435	val: 4.767104	test: 3.445591
MAE train: 0.752404	val: 3.338972	test: 2.748785

Epoch: 113
Loss: 0.8470844030380249
RMSE train: 0.943674	val: 4.796751	test: 3.474219
MAE train: 0.765604	val: 3.339844	test: 2.774787

Epoch: 114
Loss: 0.8624067008495331
RMSE train: 0.966956	val: 4.839117	test: 3.493266
MAE train: 0.782890	val: 3.360581	test: 2.782507

Epoch: 115
Loss: 0.8130716979503632
RMSE train: 1.011984	val: 4.935732	test: 3.504990
MAE train: 0.813777	val: 3.409432	test: 2.780153

Epoch: 116
Loss: 0.7638784646987915
RMSE train: 0.986089	val: 4.930069	test: 3.449460
MAE train: 0.791100	val: 3.401923	test: 2.720918

Epoch: 117
Loss: 0.716347724199295
RMSE train: 0.895205	val: 4.830024	test: 3.361709
MAE train: 0.719115	val: 3.335925	test: 2.633632

Epoch: 118
Loss: 0.731622576713562
RMSE train: 0.827604	val: 4.768415	test: 3.313121
MAE train: 0.664573	val: 3.303885	test: 2.591583

Epoch: 119
Loss: 0.7443341612815857
RMSE train: 0.757981	val: 4.684718	test: 3.249180
MAE train: 0.608097	val: 3.260753	test: 2.530983

Epoch: 120
Loss: 0.8221292197704315
RMSE train: 0.714796	val: 4.588846	test: 3.203440
MAE train: 0.568425	val: 3.218424	test: 2.487576

Epoch: 121
Loss: 0.8161150217056274
RMSE train: 0.725102	val: 4.553133	test: 3.195559
MAE train: 0.575077	val: 3.221388	test: 2.473512

Epoch: 122
Loss: 0.7644946277141571
RMSE train: 0.771125	val: 4.579959	test: 3.220092
MAE train: 0.613343	val: 3.251324	test: 2.493540

Epoch: 123
Loss: 0.7322514653205872
RMSE train: 0.795081	val: 4.581985	test: 3.249065
MAE train: 0.631100	val: 3.258541	test: 2.526979

Early stopping
Best (RMSE):	 train: 0.769204	val: 4.385632	test: 3.256152
Best (MAE):	 train: 0.610094	val: 3.199755	test: 2.568133
All runs completed.

MAE train: 0.542050	val: 2.690969	test: 2.274228

Epoch: 145
Loss: 0.6964678466320038
RMSE train: 0.692593	val: 3.946104	test: 3.024971
MAE train: 0.533551	val: 2.683755	test: 2.276091

Epoch: 146
Loss: 0.7232735753059387
RMSE train: 0.680928	val: 3.952376	test: 3.010923
MAE train: 0.527998	val: 2.692774	test: 2.284521

Epoch: 147
Loss: 0.6665535271167755
RMSE train: 0.650934	val: 3.927695	test: 3.014288
MAE train: 0.506268	val: 2.669529	test: 2.295524

Epoch: 148
Loss: 0.5536555647850037
RMSE train: 0.664114	val: 3.916265	test: 3.007833
MAE train: 0.516540	val: 2.652751	test: 2.283268

Epoch: 149
Loss: 0.600425511598587
RMSE train: 0.681543	val: 3.916260	test: 3.011896
MAE train: 0.532894	val: 2.640854	test: 2.290908

Epoch: 150
Loss: 0.5852433741092682
RMSE train: 0.676704	val: 3.915136	test: 3.017037
MAE train: 0.530123	val: 2.641304	test: 2.306734

Epoch: 151
Loss: 0.7154114544391632
RMSE train: 0.689309	val: 3.936564	test: 3.019249
MAE train: 0.534882	val: 2.664041	test: 2.317111

Epoch: 152
Loss: 0.7122082412242889
RMSE train: 0.719847	val: 3.957580	test: 3.018840
MAE train: 0.551232	val: 2.688949	test: 2.314718

Epoch: 153
Loss: 0.6153375506401062
RMSE train: 0.752302	val: 3.972911	test: 2.989368
MAE train: 0.562125	val: 2.713420	test: 2.277243

Epoch: 154
Loss: 0.653788298368454
RMSE train: 0.725105	val: 3.933892	test: 2.965842
MAE train: 0.533317	val: 2.689904	test: 2.258820

Epoch: 155
Loss: 0.6516384482383728
RMSE train: 0.695102	val: 3.873736	test: 2.983229
MAE train: 0.523961	val: 2.643006	test: 2.286591

Epoch: 156
Loss: 0.5619577467441559
RMSE train: 0.707636	val: 3.848993	test: 3.026553
MAE train: 0.545728	val: 2.606940	test: 2.334653

Epoch: 157
Loss: 0.5849520862102509
RMSE train: 0.715401	val: 3.836176	test: 3.047080
MAE train: 0.556410	val: 2.592785	test: 2.359998

Epoch: 158
Loss: 0.6724362373352051
RMSE train: 0.737763	val: 3.857522	test: 3.038011
MAE train: 0.577443	val: 2.602921	test: 2.353095

Epoch: 159
Loss: 0.6126553118228912
RMSE train: 0.719826	val: 3.869397	test: 3.024465
MAE train: 0.563466	val: 2.594765	test: 2.336384

Epoch: 160
Loss: 0.6356395184993744
RMSE train: 0.701515	val: 3.890565	test: 3.006187
MAE train: 0.541716	val: 2.601170	test: 2.312060

Early stopping
Best (RMSE):	 train: 0.754712	val: 3.815451	test: 3.096954
Best (MAE):	 train: 0.567324	val: 2.654790	test: 2.414197
All runs completed.

RMSE train: 0.882962	val: 5.366696	test: 4.092772
MAE train: 0.683860	val: 3.904738	test: 3.175943

Epoch: 84
Loss: 1.3789855241775513
RMSE train: 0.836331	val: 5.315396	test: 4.035007
MAE train: 0.637831	val: 3.868145	test: 3.123562

Epoch: 85
Loss: 1.1529731750488281
RMSE train: 0.792159	val: 5.274622	test: 4.009500
MAE train: 0.602367	val: 3.830136	test: 3.088193

Epoch: 86
Loss: 1.3802465200424194
RMSE train: 0.760485	val: 5.214068	test: 3.982773
MAE train: 0.591592	val: 3.742133	test: 3.042510

Epoch: 87
Loss: 1.0785498917102814
RMSE train: 0.783447	val: 5.204669	test: 3.965351
MAE train: 0.614649	val: 3.712778	test: 3.012010

Epoch: 88
Loss: 1.2890695333480835
RMSE train: 0.802408	val: 5.221038	test: 3.961743
MAE train: 0.629095	val: 3.724476	test: 3.009157

Epoch: 89
Loss: 1.2327553033828735
RMSE train: 0.818514	val: 5.251902	test: 3.987178
MAE train: 0.642359	val: 3.752671	test: 3.050276

Epoch: 90
Loss: 1.0463471412658691
RMSE train: 0.850328	val: 5.319673	test: 4.031915
MAE train: 0.665105	val: 3.810130	test: 3.121555

Epoch: 91
Loss: 1.199653446674347
RMSE train: 0.851692	val: 5.385780	test: 4.069846
MAE train: 0.666024	val: 3.883141	test: 3.193575

Epoch: 92
Loss: 1.1167160272598267
RMSE train: 0.870180	val: 5.441401	test: 4.104758
MAE train: 0.674914	val: 3.948678	test: 3.254893

Epoch: 93
Loss: 1.068560630083084
RMSE train: 0.848932	val: 5.410117	test: 4.103261
MAE train: 0.661193	val: 3.937299	test: 3.254538

Epoch: 94
Loss: 1.1910048723220825
RMSE train: 0.829278	val: 5.361375	test: 4.093509
MAE train: 0.658535	val: 3.893079	test: 3.242767

Epoch: 95
Loss: 1.1751475930213928
RMSE train: 0.840754	val: 5.359134	test: 4.100627
MAE train: 0.668064	val: 3.892796	test: 3.243717

Epoch: 96
Loss: 1.2485466003417969
RMSE train: 0.849357	val: 5.330014	test: 4.085543
MAE train: 0.662259	val: 3.843826	test: 3.206009

Epoch: 97
Loss: 1.234261155128479
RMSE train: 0.840492	val: 5.311154	test: 4.067951
MAE train: 0.648442	val: 3.800761	test: 3.173771

Epoch: 98
Loss: 0.981210470199585
RMSE train: 0.856025	val: 5.322686	test: 4.066510
MAE train: 0.663701	val: 3.789500	test: 3.158771

Epoch: 99
Loss: 1.0586614608764648
RMSE train: 0.862018	val: 5.339963	test: 4.071691
MAE train: 0.680084	val: 3.797956	test: 3.191228

Epoch: 100
Loss: 0.9413233995437622
RMSE train: 0.883506	val: 5.346870	test: 4.093023
MAE train: 0.709598	val: 3.807061	test: 3.236429

Epoch: 101
Loss: 0.9721225500106812
RMSE train: 0.917124	val: 5.336948	test: 4.116890
MAE train: 0.745445	val: 3.795417	test: 3.267617

Epoch: 102
Loss: 1.2060814499855042
RMSE train: 0.929954	val: 5.321284	test: 4.130963
MAE train: 0.756419	val: 3.782360	test: 3.288547

Epoch: 103
Loss: 0.9470186531543732
RMSE train: 0.929081	val: 5.301109	test: 4.130734
MAE train: 0.751799	val: 3.765105	test: 3.283544

Epoch: 104
Loss: 0.9864771366119385
RMSE train: 0.889247	val: 5.247927	test: 4.108456
MAE train: 0.716136	val: 3.715304	test: 3.242387

Epoch: 105
Loss: 1.0788544416427612
RMSE train: 0.876564	val: 5.261344	test: 4.112104
MAE train: 0.700984	val: 3.766513	test: 3.256193

Epoch: 106
Loss: 0.9179122149944305
RMSE train: 0.844058	val: 5.277075	test: 4.114916
MAE train: 0.667899	val: 3.808681	test: 3.272601

Epoch: 107
Loss: 0.9435073733329773
RMSE train: 0.814185	val: 5.300237	test: 4.124551
MAE train: 0.631420	val: 3.876817	test: 3.294900

Epoch: 108
Loss: 1.009003758430481
RMSE train: 0.782961	val: 5.290761	test: 4.095662
MAE train: 0.607792	val: 3.896469	test: 3.276862

Epoch: 109
Loss: 0.9995800852775574
RMSE train: 0.728140	val: 5.213772	test: 4.028086
MAE train: 0.571604	val: 3.783752	test: 3.188458

Epoch: 110
Loss: 0.9188093543052673
RMSE train: 0.709686	val: 5.142401	test: 3.965403
MAE train: 0.562771	val: 3.706110	test: 3.100985

Epoch: 111
Loss: 0.8361614942550659
RMSE train: 0.736550	val: 5.135348	test: 3.929931
MAE train: 0.588242	val: 3.699367	test: 3.062902

Epoch: 112
Loss: 0.8277230858802795
RMSE train: 0.789135	val: 5.179051	test: 3.920661
MAE train: 0.627177	val: 3.742617	test: 3.060216

Epoch: 113
Loss: 0.9101751446723938
RMSE train: 0.783624	val: 5.178818	test: 3.905477
MAE train: 0.617148	val: 3.752367	test: 3.045853

Epoch: 114
Loss: 0.9752605557441711
RMSE train: 0.772973	val: 5.160458	test: 3.893471
MAE train: 0.605195	val: 3.744321	test: 3.049214

Epoch: 115
Loss: 0.8445543646812439
RMSE train: 0.785748	val: 5.178461	test: 3.903266
MAE train: 0.622144	val: 3.751421	test: 3.075422

Epoch: 116
Loss: 0.8441638648509979
RMSE train: 0.836907	val: 5.221021	test: 3.935134
MAE train: 0.668135	val: 3.769950	test: 3.103307

Epoch: 117
Loss: 0.8628482222557068
RMSE train: 0.867310	val: 5.225497	test: 3.935201
MAE train: 0.690661	val: 3.772260	test: 3.106648

Epoch: 118
Loss: 0.8973762691020966
RMSE train: 0.829820	val: 5.162569	test: 3.917968
MAE train: 0.668634	val: 3.679721	test: 3.078885

Epoch: 119
Loss: 0.8723751604557037
RMSE train: 0.789988	val: 5.108204	test: 3.898620
MAE train: 0.636796	val: 3.620091	test: 3.054144

Epoch: 120
Loss: 0.7696354687213898
RMSE train: 0.784647	val: 5.089229	test: 3.894182
MAE train: 0.632985	val: 3.587882	test: 3.036205

Epoch: 121
Loss: 0.9206838607788086
RMSE train: 0.780139	val: 5.111632	test: 3.904434
MAE train: 0.628693	val: 3.608359	test: 3.045423

Epoch: 122
Loss: 1.1129170656204224
RMSE train: 0.804577	val: 5.165493	test: 3.931584
MAE train: 0.653429	val: 3.646892	test: 3.079801

Epoch: 123
Loss: 0.8887186944484711
RMSE train: 0.871948	val: 5.259145	test: 3.969734
MAE train: 0.700950	val: 3.716925	test: 3.121100

Epoch: 124
Loss: 0.8940665125846863
RMSE train: 0.874341	val: 5.289730	test: 3.978065
MAE train: 0.691819	val: 3.761321	test: 3.144559

Epoch: 125
Loss: 0.8488322496414185
RMSE train: 0.814957	val: 5.242458	test: 3.957125
MAE train: 0.642689	val: 3.737095	test: 3.119659

Epoch: 126
Loss: 0.972487211227417
RMSE train: 0.700368	val: 5.121823	test: 3.915585
MAE train: 0.553605	val: 3.638099	test: 3.072452

Epoch: 127
Loss: 0.7828599810600281
RMSE train: 0.620773	val: 5.011459	test: 3.883407
MAE train: 0.489477	val: 3.578368	test: 3.024460

Epoch: 128
Loss: 0.8149011135101318
RMSE train: 0.621111	val: 4.945976	test: 3.887140
MAE train: 0.491742	val: 3.543313	test: 3.012357

Epoch: 129
Loss: 1.0215784907341003
RMSE train: 0.682139	val: 4.971341	test: 3.927696
MAE train: 0.534676	val: 3.572770	test: 3.052817

Epoch: 130
Loss: 0.8311408758163452
RMSE train: 0.767678	val: 5.059383	test: 3.940215
MAE train: 0.590717	val: 3.653241	test: 3.086271

Epoch: 131
Loss: 0.7479187846183777
RMSE train: 0.830591	val: 5.143485	test: 3.965862
MAE train: 0.637905	val: 3.698748	test: 3.106489

Epoch: 132
Loss: 0.7416960000991821
RMSE train: 0.872414	val: 5.217282	test: 3.973168
MAE train: 0.672186	val: 3.748550	test: 3.121973

Epoch: 133
Loss: 0.8876770436763763
RMSE train: 0.899206	val: 5.308195	test: 3.995696
MAE train: 0.697696	val: 3.810643	test: 3.157699

Epoch: 134
Loss: 0.8072870671749115
RMSE train: 0.869102	val: 5.339304	test: 4.002495
MAE train: 0.670559	val: 3.844409	test: 3.188081

Epoch: 135
Loss: 0.8896817862987518
RMSE train: 0.832868	val: 5.371877	test: 4.013888
MAE train: 0.623608	val: 3.895546	test: 3.223803

Epoch: 136
Loss: 0.8815823197364807
RMSE train: 0.807417	val: 5.382438	test: 4.031349
MAE train: 0.595021	val: 3.905649	test: 3.250691

Epoch: 137
Loss: 0.9279363751411438
RMSE train: 0.733574	val: 5.323634	test: 4.027219
MAE train: 0.548438	val: 3.865416	test: 3.246544

Epoch: 138
Loss: 0.7645010948181152
RMSE train: 0.719326	val: 5.313780	test: 4.052026
MAE train: 0.557194	val: 3.841288	test: 3.261363

Epoch: 139
Loss: 0.7322181463241577
RMSE train: 0.729361	val: 5.301512	test: 4.063018
MAE train: 0.577899	val: 3.789880	test: 3.252990

Epoch: 140
Loss: 0.8222291469573975
RMSE train: 0.740933	val: 5.299216	test: 4.065012
MAE train: 0.588574	val: 3.767378	test: 3.240103

Epoch: 141
Loss: 0.8514016568660736
RMSE train: 0.732661	val: 5.280873	test: 4.052886
MAE train: 0.582849	val: 3.740780	test: 3.210701

Epoch: 142
Loss: 0.7887655198574066
RMSE train: 0.742375	val: 5.261266	test: 4.040721
MAE train: 0.590197	val: 3.695265	test: 3.173985

Epoch: 143
Loss: 0.7474511563777924
RMSE train: 0.762029	val: 5.232579	test: 3.995306
MAE train: 0.598035	val: 3.667316	test: 3.098946
RMSE train: 1.084412	val: 5.684371	test: 4.206707
MAE train: 0.889723	val: 3.881446	test: 3.451091

Epoch: 84
Loss: 1.1444671750068665
RMSE train: 1.083342	val: 5.725871	test: 4.219269
MAE train: 0.888647	val: 3.917439	test: 3.460381

Epoch: 85
Loss: 1.22261244058609
RMSE train: 1.055506	val: 5.679440	test: 4.175472
MAE train: 0.867810	val: 3.920477	test: 3.404074

Epoch: 86
Loss: 1.0454337000846863
RMSE train: 1.011246	val: 5.603463	test: 4.124500
MAE train: 0.831781	val: 3.898275	test: 3.348399

Epoch: 87
Loss: 1.178917944431305
RMSE train: 0.946360	val: 5.516694	test: 4.050304
MAE train: 0.775847	val: 3.853692	test: 3.275389

Epoch: 88
Loss: 1.1030251383781433
RMSE train: 0.905443	val: 5.424477	test: 3.991694
MAE train: 0.739053	val: 3.802843	test: 3.218612

Epoch: 89
Loss: 1.1867582201957703
RMSE train: 0.905141	val: 5.357410	test: 3.961233
MAE train: 0.734302	val: 3.762793	test: 3.185441

Epoch: 90
Loss: 1.266658902168274
RMSE train: 0.952318	val: 5.392596	test: 3.980690
MAE train: 0.773366	val: 3.776080	test: 3.202211

Epoch: 91
Loss: 1.0033370554447174
RMSE train: 0.987335	val: 5.418159	test: 3.996809
MAE train: 0.803798	val: 3.788682	test: 3.209101

Epoch: 92
Loss: 1.2854794263839722
RMSE train: 0.965950	val: 5.409655	test: 3.972880
MAE train: 0.788849	val: 3.776363	test: 3.182888

Epoch: 93
Loss: 1.0387629866600037
RMSE train: 0.936673	val: 5.398393	test: 3.955182
MAE train: 0.763526	val: 3.763072	test: 3.164065

Epoch: 94
Loss: 1.264758288860321
RMSE train: 0.917766	val: 5.371168	test: 3.946379
MAE train: 0.748221	val: 3.749905	test: 3.151040

Epoch: 95
Loss: 1.1406120657920837
RMSE train: 0.948394	val: 5.452234	test: 3.990252
MAE train: 0.775037	val: 3.798142	test: 3.183367

Epoch: 96
Loss: 0.9938161969184875
RMSE train: 0.947483	val: 5.466377	test: 4.003156
MAE train: 0.773774	val: 3.818546	test: 3.184354

Epoch: 97
Loss: 1.001837432384491
RMSE train: 0.939703	val: 5.453880	test: 3.994086
MAE train: 0.765031	val: 3.820244	test: 3.174548

Epoch: 98
Loss: 0.9965415894985199
RMSE train: 0.951752	val: 5.453903	test: 3.985796
MAE train: 0.771123	val: 3.818319	test: 3.165472

Epoch: 99
Loss: 1.114340603351593
RMSE train: 0.964230	val: 5.458435	test: 3.994063
MAE train: 0.778551	val: 3.820213	test: 3.177340

Epoch: 100
Loss: 0.9734365046024323
RMSE train: 0.955318	val: 5.461503	test: 3.989633
MAE train: 0.769548	val: 3.830507	test: 3.181398

Epoch: 101
Loss: 0.980409562587738
RMSE train: 0.867127	val: 5.402627	test: 3.929869
MAE train: 0.697322	val: 3.813297	test: 3.127809

Epoch: 102
Loss: 1.0319372713565826
RMSE train: 0.782048	val: 5.314776	test: 3.846932
MAE train: 0.626411	val: 3.772343	test: 3.062484

Epoch: 103
Loss: 1.1167206168174744
RMSE train: 0.787713	val: 5.302220	test: 3.813657
MAE train: 0.637330	val: 3.762482	test: 3.026623

Epoch: 104
Loss: 0.9790849089622498
RMSE train: 0.806841	val: 5.306876	test: 3.811230
MAE train: 0.658804	val: 3.761992	test: 3.020347

Epoch: 105
Loss: 0.8533532023429871
RMSE train: 0.840054	val: 5.348140	test: 3.830022
MAE train: 0.692072	val: 3.779609	test: 3.032566

Epoch: 106
Loss: 1.0193658769130707
RMSE train: 0.824178	val: 5.343280	test: 3.806066
MAE train: 0.677332	val: 3.779241	test: 3.008537

Epoch: 107
Loss: 0.8857700526714325
RMSE train: 0.817765	val: 5.327677	test: 3.782125
MAE train: 0.660502	val: 3.771363	test: 2.983243

Epoch: 108
Loss: 1.036744862794876
RMSE train: 0.828967	val: 5.361126	test: 3.788132
MAE train: 0.662822	val: 3.778022	test: 2.985800

Epoch: 109
Loss: 0.8597268164157867
RMSE train: 0.863098	val: 5.433207	test: 3.805040
MAE train: 0.684057	val: 3.799844	test: 2.993992

Epoch: 110
Loss: 0.8626675009727478
RMSE train: 0.877168	val: 5.457553	test: 3.827289
MAE train: 0.689541	val: 3.808240	test: 3.016088

Epoch: 111
Loss: 0.8523059487342834
RMSE train: 0.856958	val: 5.426066	test: 3.820685
MAE train: 0.669684	val: 3.788861	test: 3.010574

Epoch: 112
Loss: 0.9829844832420349
RMSE train: 0.852789	val: 5.415331	test: 3.846463
MAE train: 0.668320	val: 3.783605	test: 3.037554

Epoch: 113
Loss: 0.8593569695949554
RMSE train: 0.810562	val: 5.359678	test: 3.861685
MAE train: 0.643689	val: 3.763953	test: 3.057041

Epoch: 114
Loss: 1.0535823702812195
RMSE train: 0.796441	val: 5.311543	test: 3.921164
MAE train: 0.646679	val: 3.747706	test: 3.129179

Epoch: 115
Loss: 0.9809025526046753
RMSE train: 0.850039	val: 5.373263	test: 3.999235
MAE train: 0.696001	val: 3.789086	test: 3.216077

Epoch: 116
Loss: 0.8854304254055023
RMSE train: 0.829001	val: 5.392017	test: 3.995101
MAE train: 0.674384	val: 3.795244	test: 3.223813

Epoch: 117
Loss: 0.8727886378765106
RMSE train: 0.765035	val: 5.372808	test: 3.913251
MAE train: 0.610299	val: 3.778259	test: 3.149704

Epoch: 118
Loss: 0.8044593632221222
RMSE train: 0.700031	val: 5.309941	test: 3.835312
MAE train: 0.550328	val: 3.738114	test: 3.074980

Epoch: 119
Loss: 0.7839928567409515
RMSE train: 0.665775	val: 5.274504	test: 3.802920
MAE train: 0.519652	val: 3.714073	test: 3.043552

Epoch: 120
Loss: 0.7974596321582794
RMSE train: 0.682132	val: 5.270206	test: 3.782500
MAE train: 0.534811	val: 3.714268	test: 3.016987

Epoch: 121
Loss: 0.8383305370807648
RMSE train: 0.721638	val: 5.270451	test: 3.791350
MAE train: 0.574246	val: 3.724085	test: 3.017153

Epoch: 122
Loss: 0.8120373785495758
RMSE train: 0.745043	val: 5.261137	test: 3.830852
MAE train: 0.594409	val: 3.737826	test: 3.055102

Epoch: 123
Loss: 0.7324398458003998
RMSE train: 0.769110	val: 5.249766	test: 3.895588
MAE train: 0.614106	val: 3.755405	test: 3.124963

Epoch: 124
Loss: 0.8190153539180756
RMSE train: 0.778475	val: 5.226233	test: 3.973847
MAE train: 0.626168	val: 3.763812	test: 3.206866

Epoch: 125
Loss: 0.7489365637302399
RMSE train: 0.809029	val: 5.230257	test: 4.026761
MAE train: 0.651782	val: 3.783270	test: 3.251676

Epoch: 126
Loss: 0.8447031080722809
RMSE train: 0.833986	val: 5.265753	test: 4.043404
MAE train: 0.671653	val: 3.802905	test: 3.260219

Epoch: 127
Loss: 0.8626269698143005
RMSE train: 0.829047	val: 5.288589	test: 3.991698
MAE train: 0.664011	val: 3.798720	test: 3.207326

Epoch: 128
Loss: 0.8370802700519562
RMSE train: 0.851983	val: 5.318092	test: 3.953233
MAE train: 0.686568	val: 3.791545	test: 3.165014

Epoch: 129
Loss: 0.6081989705562592
RMSE train: 0.874295	val: 5.355114	test: 3.929821
MAE train: 0.701525	val: 3.804440	test: 3.134900

Epoch: 130
Loss: 0.7493241131305695
RMSE train: 0.888727	val: 5.365611	test: 3.916326
MAE train: 0.713421	val: 3.810850	test: 3.109843

Epoch: 131
Loss: 0.7961225509643555
RMSE train: 0.856316	val: 5.339893	test: 3.874598
MAE train: 0.686856	val: 3.778413	test: 3.062285

Epoch: 132
Loss: 0.6867878437042236
RMSE train: 0.832941	val: 5.303159	test: 3.826144
MAE train: 0.663410	val: 3.749958	test: 3.004494

Epoch: 133
Loss: 0.7976844608783722
RMSE train: 0.834337	val: 5.284650	test: 3.834782
MAE train: 0.664388	val: 3.741888	test: 3.015802

Epoch: 134
Loss: 0.8185283243656158
RMSE train: 0.813345	val: 5.266722	test: 3.837173
MAE train: 0.652146	val: 3.734533	test: 3.029233

Epoch: 135
Loss: 0.8276660442352295
RMSE train: 0.756204	val: 5.189175	test: 3.821167
MAE train: 0.617060	val: 3.703669	test: 3.028083

Epoch: 136
Loss: 0.7923533022403717
RMSE train: 0.791504	val: 5.229812	test: 3.864536
MAE train: 0.644448	val: 3.742591	test: 3.072660

Epoch: 137
Loss: 0.7374324798583984
RMSE train: 0.756310	val: 5.244597	test: 3.860631
MAE train: 0.614787	val: 3.762887	test: 3.069918

Epoch: 138
Loss: 0.89598748087883
RMSE train: 0.732246	val: 5.236460	test: 3.822549
MAE train: 0.593933	val: 3.745487	test: 3.024363

Epoch: 139
Loss: 0.7317830324172974
RMSE train: 0.776669	val: 5.320108	test: 3.841849
MAE train: 0.621409	val: 3.782730	test: 3.038855

Epoch: 140
Loss: 0.6876358389854431
RMSE train: 0.792697	val: 5.369981	test: 3.865006
MAE train: 0.625162	val: 3.800489	test: 3.063928

Epoch: 141
Loss: 0.8924622237682343
RMSE train: 0.797972	val: 5.402563	test: 3.905877
MAE train: 0.626740	val: 3.817831	test: 3.115309

Epoch: 142
Loss: 0.8490587174892426
RMSE train: 0.791532	val: 5.410382	test: 3.919727
MAE train: 0.627535	val: 3.821351	test: 3.138368

Epoch: 143
Loss: 0.8811627328395844
RMSE train: 0.768079	val: 5.392997	test: 3.962361
MAE train: 0.622617	val: 3.825633	test: 3.189211
RMSE train: 0.805064	val: 4.173409	test: 3.473120
MAE train: 0.621943	val: 2.697426	test: 2.643076

Epoch: 84
Loss: 1.1518336534500122
RMSE train: 0.764122	val: 4.123499	test: 3.446584
MAE train: 0.589813	val: 2.681428	test: 2.621905

Epoch: 85
Loss: 0.9207298159599304
RMSE train: 0.742429	val: 4.093164	test: 3.430711
MAE train: 0.574286	val: 2.627014	test: 2.604266

Epoch: 86
Loss: 0.9693124294281006
RMSE train: 0.754432	val: 4.099161	test: 3.456187
MAE train: 0.583272	val: 2.573834	test: 2.620430

Epoch: 87
Loss: 0.9425396621227264
RMSE train: 0.752001	val: 4.108449	test: 3.477443
MAE train: 0.582379	val: 2.573251	test: 2.640678

Epoch: 88
Loss: 0.9545994400978088
RMSE train: 0.751476	val: 4.110889	test: 3.486354
MAE train: 0.580082	val: 2.631454	test: 2.661411

Epoch: 89
Loss: 1.1359238028526306
RMSE train: 0.746099	val: 4.087226	test: 3.474032
MAE train: 0.577228	val: 2.613823	test: 2.659247

Epoch: 90
Loss: 0.9998237192630768
RMSE train: 0.752946	val: 4.068142	test: 3.463207
MAE train: 0.583705	val: 2.586835	test: 2.656754

Epoch: 91
Loss: 1.0269739627838135
RMSE train: 0.723003	val: 4.032942	test: 3.436839
MAE train: 0.561846	val: 2.547900	test: 2.642884

Epoch: 92
Loss: 0.902806431055069
RMSE train: 0.712374	val: 4.028751	test: 3.398688
MAE train: 0.550501	val: 2.560783	test: 2.610897

Epoch: 93
Loss: 0.8795857727527618
RMSE train: 0.717153	val: 4.032332	test: 3.365124
MAE train: 0.547490	val: 2.599849	test: 2.578562

Epoch: 94
Loss: 0.9090622961521149
RMSE train: 0.716840	val: 4.023255	test: 3.365788
MAE train: 0.541383	val: 2.607508	test: 2.575546

Epoch: 95
Loss: 0.9785640835762024
RMSE train: 0.725213	val: 4.032497	test: 3.349794
MAE train: 0.540882	val: 2.615925	test: 2.547091

Epoch: 96
Loss: 0.986959844827652
RMSE train: 0.675929	val: 3.987605	test: 3.272603
MAE train: 0.502425	val: 2.589781	test: 2.456025

Epoch: 97
Loss: 0.8514916300773621
RMSE train: 0.604158	val: 3.918335	test: 3.209346
MAE train: 0.454116	val: 2.509523	test: 2.372189

Epoch: 98
Loss: 0.9270384907722473
RMSE train: 0.615550	val: 3.930165	test: 3.204127
MAE train: 0.463699	val: 2.503932	test: 2.355625

Epoch: 99
Loss: 0.9044618308544159
RMSE train: 0.634414	val: 3.967414	test: 3.220728
MAE train: 0.476291	val: 2.532601	test: 2.371720

Epoch: 100
Loss: 0.8656668663024902
RMSE train: 0.655992	val: 4.048961	test: 3.277584
MAE train: 0.496405	val: 2.624074	test: 2.432470

Epoch: 101
Loss: 0.8213578462600708
RMSE train: 0.650440	val: 4.079356	test: 3.320031
MAE train: 0.500781	val: 2.666102	test: 2.484425

Epoch: 102
Loss: 0.8618038296699524
RMSE train: 0.640344	val: 4.080649	test: 3.359080
MAE train: 0.501852	val: 2.652428	test: 2.538170

Epoch: 103
Loss: 0.8686495125293732
RMSE train: 0.654908	val: 4.089680	test: 3.391221
MAE train: 0.511451	val: 2.583858	test: 2.557297

Epoch: 104
Loss: 0.8408657014369965
RMSE train: 0.678717	val: 4.134071	test: 3.397554
MAE train: 0.524776	val: 2.561638	test: 2.547466

Epoch: 105
Loss: 0.7892360687255859
RMSE train: 0.703035	val: 4.205533	test: 3.423215
MAE train: 0.542599	val: 2.610510	test: 2.574807

Epoch: 106
Loss: 0.8933246433734894
RMSE train: 0.708505	val: 4.228822	test: 3.425553
MAE train: 0.543317	val: 2.626099	test: 2.585747

Epoch: 107
Loss: 0.7612419128417969
RMSE train: 0.717388	val: 4.247828	test: 3.414648
MAE train: 0.543506	val: 2.652865	test: 2.582379

Epoch: 108
Loss: 0.865345686674118
RMSE train: 0.733277	val: 4.271023	test: 3.398483
MAE train: 0.551413	val: 2.685625	test: 2.569582

Epoch: 109
Loss: 0.8957169055938721
RMSE train: 0.769270	val: 4.285975	test: 3.368786
MAE train: 0.571826	val: 2.713696	test: 2.548089

Epoch: 110
Loss: 0.8787879645824432
RMSE train: 0.764146	val: 4.236005	test: 3.334086
MAE train: 0.576569	val: 2.674859	test: 2.522304

Epoch: 111
Loss: 0.7264890968799591
RMSE train: 0.721840	val: 4.109617	test: 3.277946
MAE train: 0.559886	val: 2.560889	test: 2.468426

Epoch: 112
Loss: 0.7494808733463287
RMSE train: 0.711699	val: 4.041910	test: 3.239645
MAE train: 0.554592	val: 2.507835	test: 2.433066

Epoch: 113
Loss: 0.8529813885688782
RMSE train: 0.655281	val: 3.953731	test: 3.211409
MAE train: 0.511066	val: 2.473895	test: 2.412574

Epoch: 114
Loss: 0.8669052124023438
RMSE train: 0.657659	val: 3.947047	test: 3.221812
MAE train: 0.510091	val: 2.533046	test: 2.433608

Epoch: 115
Loss: 0.8301883935928345
RMSE train: 0.661082	val: 3.949635	test: 3.238143
MAE train: 0.515123	val: 2.540343	test: 2.451063

Epoch: 116
Loss: 0.7751483917236328
RMSE train: 0.675892	val: 3.963604	test: 3.244950
MAE train: 0.528755	val: 2.490274	test: 2.427567

Epoch: 117
Loss: 0.8473840355873108
RMSE train: 0.674136	val: 3.961354	test: 3.243842
MAE train: 0.525695	val: 2.448739	test: 2.417869

Epoch: 118
Loss: 0.8230419456958771
RMSE train: 0.667379	val: 3.954912	test: 3.245753
MAE train: 0.521749	val: 2.413811	test: 2.417614

Epoch: 119
Loss: 0.7689494490623474
RMSE train: 0.596079	val: 3.904449	test: 3.225413
MAE train: 0.462512	val: 2.441121	test: 2.437494

Epoch: 120
Loss: 0.6779878437519073
RMSE train: 0.625990	val: 3.951606	test: 3.255063
MAE train: 0.467171	val: 2.559376	test: 2.501697

Epoch: 121
Loss: 0.7510773241519928
RMSE train: 0.692972	val: 4.064790	test: 3.311207
MAE train: 0.496127	val: 2.657279	test: 2.556243

Epoch: 122
Loss: 0.9430842697620392
RMSE train: 0.740812	val: 4.157220	test: 3.353164
MAE train: 0.532102	val: 2.691606	test: 2.555785

Epoch: 123
Loss: 0.8217409253120422
RMSE train: 0.808958	val: 4.253852	test: 3.408646
MAE train: 0.597642	val: 2.755922	test: 2.582759

Epoch: 124
Loss: 0.714391827583313
RMSE train: 0.787312	val: 4.247988	test: 3.416257
MAE train: 0.601524	val: 2.733429	test: 2.579794

Epoch: 125
Loss: 0.7498064339160919
RMSE train: 0.726581	val: 4.201198	test: 3.405835
MAE train: 0.565977	val: 2.656786	test: 2.558533

Epoch: 126
Loss: 0.9090559780597687
RMSE train: 0.667319	val: 4.167517	test: 3.400060
MAE train: 0.515205	val: 2.645653	test: 2.550238

Epoch: 127
Loss: 0.6450387835502625
RMSE train: 0.607550	val: 4.114212	test: 3.396732
MAE train: 0.468796	val: 2.652456	test: 2.551100

Epoch: 128
Loss: 0.7123149633407593
RMSE train: 0.645414	val: 4.117558	test: 3.426468
MAE train: 0.492954	val: 2.658121	test: 2.573485

Epoch: 129
Loss: 0.7607470154762268
RMSE train: 0.739374	val: 4.161593	test: 3.461935
MAE train: 0.563022	val: 2.730329	test: 2.611802

Epoch: 130
Loss: 0.7477852404117584
RMSE train: 0.812821	val: 4.229891	test: 3.479523
MAE train: 0.611604	val: 2.809561	test: 2.652261

Epoch: 131
Loss: 0.6888225674629211
RMSE train: 0.820700	val: 4.238960	test: 3.460594
MAE train: 0.620525	val: 2.807077	test: 2.640370

Epoch: 132
Loss: 0.6820741593837738
RMSE train: 0.766931	val: 4.198835	test: 3.410348
MAE train: 0.582261	val: 2.772792	test: 2.587722

Epoch: 133
Loss: 0.7935550212860107
RMSE train: 0.684878	val: 4.118096	test: 3.351330
MAE train: 0.525519	val: 2.675426	test: 2.517704

Epoch: 134
Loss: 0.6896531879901886
RMSE train: 0.584754	val: 4.022196	test: 3.273552
MAE train: 0.447371	val: 2.592132	test: 2.453283

Epoch: 135
Loss: 0.7720702886581421
RMSE train: 0.517803	val: 3.998093	test: 3.234553
MAE train: 0.395121	val: 2.686846	test: 2.462129

Epoch: 136
Loss: 0.6917777955532074
RMSE train: 0.547642	val: 4.021668	test: 3.230970
MAE train: 0.411788	val: 2.685142	test: 2.444703

Epoch: 137
Loss: 0.8397224545478821
RMSE train: 0.570241	val: 4.026395	test: 3.229784
MAE train: 0.435875	val: 2.610412	test: 2.405860

Epoch: 138
Loss: 0.6490840315818787
RMSE train: 0.623523	val: 4.030836	test: 3.245263
MAE train: 0.484039	val: 2.538505	test: 2.384565

Epoch: 139
Loss: 0.6823739111423492
RMSE train: 0.642866	val: 4.036913	test: 3.274829
MAE train: 0.499579	val: 2.560303	test: 2.418646

Epoch: 140
Loss: 0.7734885811805725
RMSE train: 0.661872	val: 4.062378	test: 3.308435
MAE train: 0.509822	val: 2.636661	test: 2.463474

Epoch: 141
Loss: 0.7061877548694611
RMSE train: 0.659993	val: 4.066685	test: 3.318644
MAE train: 0.503826	val: 2.696804	test: 2.480896

Epoch: 142
Loss: 0.7625048458576202
RMSE train: 0.669017	val: 4.074374	test: 3.329277
MAE train: 0.507896	val: 2.704027	test: 2.491622

Epoch: 143
Loss: 0.6487078666687012
RMSE train: 0.716272	val: 4.114406	test: 3.343647
MAE train: 0.541065	val: 2.707614	test: 2.497885

Epoch: 144
Loss: 0.6311829686164856
RMSE train: 0.734798	val: 4.162743	test: 3.347065
MAE train: 0.557440	val: 2.735232	test: 2.497449

Epoch: 145
Loss: 0.6425184905529022
RMSE train: 0.727423	val: 4.188934	test: 3.349502
MAE train: 0.556554	val: 2.749204	test: 2.499550

Epoch: 146
Loss: 0.6553739905357361
RMSE train: 0.679337	val: 4.177872	test: 3.344667
MAE train: 0.525012	val: 2.726799	test: 2.499912

Epoch: 147
Loss: 0.5491152107715607
RMSE train: 0.599638	val: 4.137032	test: 3.329063
MAE train: 0.468066	val: 2.693859	test: 2.505570

Epoch: 148
Loss: 0.6379920244216919
RMSE train: 0.543266	val: 4.112661	test: 3.323204
MAE train: 0.427876	val: 2.730092	test: 2.522624

Epoch: 149
Loss: 0.6200754642486572
RMSE train: 0.570967	val: 4.135700	test: 3.317114
MAE train: 0.444295	val: 2.753071	test: 2.515932

Epoch: 150
Loss: 0.6381872296333313
RMSE train: 0.653985	val: 4.196213	test: 3.311561
MAE train: 0.501169	val: 2.754348	test: 2.488282

Epoch: 151
Loss: 0.7850944101810455
RMSE train: 0.687701	val: 4.194117	test: 3.289262
MAE train: 0.528189	val: 2.720908	test: 2.449802

Epoch: 152
Loss: 0.7219514548778534
RMSE train: 0.695220	val: 4.169219	test: 3.254646
MAE train: 0.531681	val: 2.684983	test: 2.403185

Epoch: 153
Loss: 0.6682115197181702
RMSE train: 0.660250	val: 4.114340	test: 3.201097
MAE train: 0.497225	val: 2.638099	test: 2.336658

Epoch: 154
Loss: 0.5849997401237488
RMSE train: 0.612552	val: 4.076705	test: 3.166707
MAE train: 0.454292	val: 2.648963	test: 2.302050

Early stopping
Best (RMSE):	 train: 0.596079	val: 3.904449	test: 3.225413
Best (MAE):	 train: 0.462512	val: 2.441121	test: 2.437494
All runs completed.


Epoch: 144
Loss: 0.8665981292724609
RMSE train: 0.789199	val: 5.276139	test: 3.961774
MAE train: 0.600323	val: 3.739408	test: 3.066960

Epoch: 145
Loss: 0.684494286775589
RMSE train: 0.789467	val: 5.284840	test: 3.937200
MAE train: 0.587177	val: 3.771572	test: 3.058680

Epoch: 146
Loss: 0.7646192014217377
RMSE train: 0.767297	val: 5.276840	test: 3.931488
MAE train: 0.565716	val: 3.789396	test: 3.082887

Epoch: 147
Loss: 0.7293265759944916
RMSE train: 0.736589	val: 5.284132	test: 3.950543
MAE train: 0.538406	val: 3.832789	test: 3.134140

Epoch: 148
Loss: 0.6369680464267731
RMSE train: 0.699688	val: 5.302650	test: 3.988749
MAE train: 0.509648	val: 3.930761	test: 3.207645

Epoch: 149
Loss: 0.7053637504577637
RMSE train: 0.674555	val: 5.318957	test: 4.036417
MAE train: 0.506072	val: 3.987507	test: 3.274311

Epoch: 150
Loss: 0.8270070850849152
RMSE train: 0.727096	val: 5.396851	test: 4.088188
MAE train: 0.549985	val: 4.035521	test: 3.326635

Epoch: 151
Loss: 0.7828471660614014
RMSE train: 0.770218	val: 5.424762	test: 4.109305
MAE train: 0.586119	val: 4.015559	test: 3.338492

Epoch: 152
Loss: 0.7111118137836456
RMSE train: 0.767302	val: 5.394723	test: 4.114590
MAE train: 0.597689	val: 3.953480	test: 3.343595

Epoch: 153
Loss: 0.732442706823349
RMSE train: 0.701820	val: 5.308210	test: 4.072192
MAE train: 0.556438	val: 3.860440	test: 3.304816

Epoch: 154
Loss: 0.8179368376731873
RMSE train: 0.634077	val: 5.193863	test: 4.002444
MAE train: 0.503456	val: 3.765797	test: 3.238959

Epoch: 155
Loss: 0.6808404326438904
RMSE train: 0.591497	val: 5.106737	test: 3.941256
MAE train: 0.465707	val: 3.688075	test: 3.169211

Epoch: 156
Loss: 0.7146817147731781
RMSE train: 0.646437	val: 5.144994	test: 3.936770
MAE train: 0.498590	val: 3.719845	test: 3.154246

Epoch: 157
Loss: 0.7688916027545929
RMSE train: 0.736708	val: 5.264039	test: 3.979233
MAE train: 0.572829	val: 3.792392	test: 3.187716

Epoch: 158
Loss: 0.6482768952846527
RMSE train: 0.807551	val: 5.361905	test: 4.015539
MAE train: 0.623839	val: 3.863352	test: 3.215971

Epoch: 159
Loss: 0.7119599878787994
RMSE train: 0.888934	val: 5.466493	test: 4.048629
MAE train: 0.673326	val: 3.932892	test: 3.235227

Epoch: 160
Loss: 0.7377361059188843
RMSE train: 0.867655	val: 5.455975	test: 4.041173
MAE train: 0.655164	val: 3.928441	test: 3.220456

Epoch: 161
Loss: 0.8116616606712341
RMSE train: 0.815156	val: 5.398749	test: 4.014634
MAE train: 0.611154	val: 3.874810	test: 3.178117

Epoch: 162
Loss: 0.6887839734554291
RMSE train: 0.736940	val: 5.324894	test: 3.985992
MAE train: 0.550960	val: 3.825682	test: 3.135311

Epoch: 163
Loss: 0.7558503746986389
RMSE train: 0.670989	val: 5.253874	test: 3.958433
MAE train: 0.495460	val: 3.803069	test: 3.114513

Early stopping
Best (RMSE):	 train: 0.621111	val: 4.945976	test: 3.887140
Best (MAE):	 train: 0.491742	val: 3.543313	test: 3.012357


Epoch: 144
Loss: 0.7146348357200623
RMSE train: 0.788643	val: 5.403889	test: 4.018651
MAE train: 0.644827	val: 3.845341	test: 3.241454

Epoch: 145
Loss: 0.7939557135105133
RMSE train: 0.814064	val: 5.448830	test: 4.062514
MAE train: 0.663668	val: 3.874006	test: 3.279760

Epoch: 146
Loss: 0.7501589953899384
RMSE train: 0.816024	val: 5.474824	test: 4.071048
MAE train: 0.660372	val: 3.873477	test: 3.284692

Epoch: 147
Loss: 0.7669467926025391
RMSE train: 0.820588	val: 5.443199	test: 4.043535
MAE train: 0.661852	val: 3.837482	test: 3.246793

Epoch: 148
Loss: 0.6780185401439667
RMSE train: 0.810820	val: 5.413201	test: 3.983892
MAE train: 0.649854	val: 3.797895	test: 3.179403

Epoch: 149
Loss: 0.7230943441390991
RMSE train: 0.796091	val: 5.382847	test: 3.945483
MAE train: 0.635836	val: 3.764174	test: 3.141337

Epoch: 150
Loss: 0.6999833285808563
RMSE train: 0.751423	val: 5.325036	test: 3.901529
MAE train: 0.601152	val: 3.722746	test: 3.101060

Epoch: 151
Loss: 0.7055298089981079
RMSE train: 0.694684	val: 5.238631	test: 3.861191
MAE train: 0.559515	val: 3.667211	test: 3.065979

Epoch: 152
Loss: 0.753214031457901
RMSE train: 0.701374	val: 5.246566	test: 3.869172
MAE train: 0.560813	val: 3.667881	test: 3.076647

Epoch: 153
Loss: 0.7163261771202087
RMSE train: 0.764005	val: 5.339121	test: 3.923289
MAE train: 0.598415	val: 3.715835	test: 3.133595

Epoch: 154
Loss: 0.8179458975791931
RMSE train: 0.824893	val: 5.442771	test: 3.985145
MAE train: 0.639083	val: 3.772818	test: 3.192706

Epoch: 155
Loss: 0.7835882008075714
RMSE train: 0.802840	val: 5.419406	test: 4.013039
MAE train: 0.630709	val: 3.774168	test: 3.219771

Epoch: 156
Loss: 0.681878924369812
RMSE train: 0.813795	val: 5.417323	test: 4.019739
MAE train: 0.642458	val: 3.784249	test: 3.214716

Epoch: 157
Loss: 0.6854714751243591
RMSE train: 0.782639	val: 5.377191	test: 3.980892
MAE train: 0.622063	val: 3.771686	test: 3.169340

Epoch: 158
Loss: 0.7174445688724518
RMSE train: 0.698955	val: 5.249116	test: 3.917130
MAE train: 0.562398	val: 3.709824	test: 3.103088

Epoch: 159
Loss: 0.7072349786758423
RMSE train: 0.637619	val: 5.130711	test: 3.866882
MAE train: 0.516530	val: 3.652973	test: 3.060056

Epoch: 160
Loss: 0.8197164535522461
RMSE train: 0.598878	val: 5.101021	test: 3.861896
MAE train: 0.482308	val: 3.646774	test: 3.063609

Epoch: 161
Loss: 0.6416122615337372
RMSE train: 0.660308	val: 5.183542	test: 3.939545
MAE train: 0.532287	val: 3.694005	test: 3.144206

Epoch: 162
Loss: 0.5849154591560364
RMSE train: 0.723701	val: 5.258494	test: 4.025139
MAE train: 0.584740	val: 3.744204	test: 3.234003

Epoch: 163
Loss: 0.6618481874465942
RMSE train: 0.795996	val: 5.327313	test: 4.077771
MAE train: 0.640172	val: 3.784488	test: 3.287023

Epoch: 164
Loss: 0.6753846406936646
RMSE train: 0.859679	val: 5.446746	test: 4.095466
MAE train: 0.680151	val: 3.838299	test: 3.299031

Epoch: 165
Loss: 0.6853204667568207
RMSE train: 0.854071	val: 5.465421	test: 4.039152
MAE train: 0.664728	val: 3.832204	test: 3.240984

Epoch: 166
Loss: 0.6760835349559784
RMSE train: 0.780514	val: 5.385141	test: 3.952005
MAE train: 0.604776	val: 3.782348	test: 3.151510

Epoch: 167
Loss: 0.648871898651123
RMSE train: 0.682940	val: 5.269277	test: 3.842025
MAE train: 0.538112	val: 3.718060	test: 3.041469

Epoch: 168
Loss: 0.6549000144004822
RMSE train: 0.621971	val: 5.142992	test: 3.772745
MAE train: 0.500431	val: 3.654882	test: 2.972769

Epoch: 169
Loss: 0.6945073306560516
RMSE train: 0.654070	val: 5.144861	test: 3.756855
MAE train: 0.531672	val: 3.656985	test: 2.951647

Epoch: 170
Loss: 0.742657870054245
RMSE train: 0.705418	val: 5.234442	test: 3.796487
MAE train: 0.573550	val: 3.714270	test: 2.985512

Epoch: 171
Loss: 0.6338780522346497
RMSE train: 0.759525	val: 5.317053	test: 3.844540
MAE train: 0.609029	val: 3.766628	test: 3.032890

Epoch: 172
Loss: 0.6484754383563995
RMSE train: 0.739230	val: 5.313634	test: 3.846233
MAE train: 0.587249	val: 3.763948	test: 3.042587

Epoch: 173
Loss: 0.7081642150878906
RMSE train: 0.676811	val: 5.264438	test: 3.799362
MAE train: 0.534922	val: 3.713977	test: 2.999476

Epoch: 174
Loss: 0.6621192395687103
RMSE train: 0.594420	val: 5.174335	test: 3.743882
MAE train: 0.466950	val: 3.648026	test: 2.951130

Epoch: 175
Loss: 0.7192597389221191
RMSE train: 0.614748	val: 5.156714	test: 3.748203
MAE train: 0.487398	val: 3.635749	test: 2.958289

Epoch: 176
Loss: 0.6034643948078156
RMSE train: 0.664466	val: 5.160362	test: 3.771887
MAE train: 0.534381	val: 3.647336	test: 2.979763

Epoch: 177
Loss: 0.5282361805438995
RMSE train: 0.728565	val: 5.196036	test: 3.799296
MAE train: 0.592360	val: 3.681190	test: 3.004966

Epoch: 178
Loss: 0.5939556658267975
RMSE train: 0.737072	val: 5.201275	test: 3.828192
MAE train: 0.596928	val: 3.706264	test: 3.038168

Epoch: 179
Loss: 0.6068792045116425
RMSE train: 0.730188	val: 5.201786	test: 3.868342
MAE train: 0.586279	val: 3.719791	test: 3.079666

Epoch: 180
Loss: 0.6794239580631256
RMSE train: 0.708016	val: 5.189055	test: 3.895676
MAE train: 0.551791	val: 3.726830	test: 3.105295

Epoch: 181
Loss: 0.5790339410305023
RMSE train: 0.679466	val: 5.181305	test: 3.912780
MAE train: 0.529436	val: 3.730229	test: 3.118704

Epoch: 182
Loss: 0.6403653919696808
RMSE train: 0.727001	val: 5.306688	test: 3.980584
MAE train: 0.578755	val: 3.802902	test: 3.179467

Epoch: 183
Loss: 0.5514820218086243
RMSE train: 0.748230	val: 5.423772	test: 3.996619
MAE train: 0.597157	val: 3.858301	test: 3.191565

Epoch: 184
Loss: 0.602856457233429
RMSE train: 0.771831	val: 5.476516	test: 3.985262
MAE train: 0.607861	val: 3.865134	test: 3.174430

Epoch: 185
Loss: 0.5412483811378479
RMSE train: 0.748333	val: 5.463327	test: 3.953963
MAE train: 0.575393	val: 3.846409	test: 3.144370

Epoch: 186
Loss: 0.524138554930687
RMSE train: 0.698748	val: 5.414175	test: 3.910501
MAE train: 0.530889	val: 3.818655	test: 3.101714

Epoch: 187
Loss: 0.551578938961029
RMSE train: 0.683656	val: 5.369687	test: 3.873118
MAE train: 0.520205	val: 3.788362	test: 3.058684

Epoch: 188
Loss: 0.6027535498142242
RMSE train: 0.702889	val: 5.371498	test: 3.874825
MAE train: 0.542065	val: 3.786501	test: 3.050567

Epoch: 189
Loss: 0.522308811545372
RMSE train: 0.705353	val: 5.381506	test: 3.914041
MAE train: 0.555055	val: 3.798857	test: 3.085633

Epoch: 190
Loss: 0.6225111186504364
RMSE train: 0.701657	val: 5.370594	test: 3.938070
MAE train: 0.554553	val: 3.804519	test: 3.110101

Epoch: 191
Loss: 0.6132723689079285
RMSE train: 0.700551	val: 5.374997	test: 3.973741
MAE train: 0.550345	val: 3.827159	test: 3.150323

Epoch: 192
Loss: 0.5853849649429321
RMSE train: 0.700307	val: 5.363197	test: 3.998420
MAE train: 0.545737	val: 3.833895	test: 3.183909

Epoch: 193
Loss: 0.5374544560909271
RMSE train: 0.701915	val: 5.326935	test: 4.007001
MAE train: 0.540269	val: 3.818732	test: 3.198978

Epoch: 194
Loss: 0.6439261138439178
RMSE train: 0.711580	val: 5.314082	test: 4.002360
MAE train: 0.543836	val: 3.804143	test: 3.195813

Epoch: 195
Loss: 0.5176074206829071
RMSE train: 0.708410	val: 5.332047	test: 3.979597
MAE train: 0.546498	val: 3.797662	test: 3.166189

Early stopping
Best (RMSE):	 train: 0.598878	val: 5.101021	test: 3.861896
Best (MAE):	 train: 0.482308	val: 3.646774	test: 3.063609
All runs completed.
