>>> Starting run for dataset: lipo
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running SCAFF configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.6.yml on cuda:0
Running SCAFF configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.7.yml on cuda:1
Running SCAFF configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.8.yml on cuda:2
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.8.yml --runseed 4 --device cuda:2
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.8.yml --runseed 5 --device cuda:2
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.6.yml --runseed 4 --device cuda:0
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.8.yml --runseed 6 --device cuda:2
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.6.yml --runseed 5 --device cuda:0
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.7.yml --runseed 4 --device cuda:1
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.6.yml --runseed 6 --device cuda:0
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.7.yml --runseed 5 --device cuda:1
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.7.yml --runseed 6 --device cuda:1
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/lipo/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/lipo/scaff/train_prop=0.6/lipophilicity_scaff_5_26-05_10-59-52  ]
[ Using Seed :  5  ]
[ Using device :  cuda:0  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.487126064300537
RMSE train: 2.425455	val: 2.484484	test: 2.472269
MAE train: 2.185977	val: 2.231509	test: 2.219111

Epoch: 2
Loss: 3.986818313598633
RMSE train: 2.192904	val: 2.245581	test: 2.214738
MAE train: 1.953963	val: 1.990209	test: 1.959978

Epoch: 3
Loss: 2.8221442222595217
RMSE train: 1.745443	val: 1.803854	test: 1.774937
MAE train: 1.531442	val: 1.572404	test: 1.543519

Epoch: 4
Loss: 1.9053125977516174
RMSE train: 1.355416	val: 1.435191	test: 1.422703
MAE train: 1.149308	val: 1.215498	test: 1.207362

Epoch: 5
Loss: 1.3134544610977172
RMSE train: 1.102694	val: 1.210008	test: 1.193267
MAE train: 0.914155	val: 1.009656	test: 1.001054

Epoch: 6
Loss: 0.9948203384876251
RMSE train: 0.950654	val: 1.078489	test: 1.056693
MAE train: 0.774720	val: 0.880753	test: 0.869068

Epoch: 7
Loss: 0.8371714234352112
RMSE train: 0.819050	val: 0.970961	test: 0.925322
MAE train: 0.639713	val: 0.759348	test: 0.738943

Epoch: 8
Loss: 0.7724273443222046
RMSE train: 0.798128	val: 0.952652	test: 0.916235
MAE train: 0.619639	val: 0.743623	test: 0.728073

Epoch: 9
Loss: 0.735610055923462
RMSE train: 0.769604	val: 0.962378	test: 0.904532
MAE train: 0.595910	val: 0.751551	test: 0.727350

Epoch: 10
Loss: 0.7040255129337311
RMSE train: 0.740351	val: 0.924373	test: 0.869235
MAE train: 0.570400	val: 0.719784	test: 0.693285

Epoch: 11
Loss: 0.6889775514602661
RMSE train: 0.729995	val: 0.925399	test: 0.893574
MAE train: 0.569654	val: 0.724207	test: 0.714038

Epoch: 12
Loss: 0.6678617298603058
RMSE train: 0.716635	val: 0.908655	test: 0.874064
MAE train: 0.563343	val: 0.717000	test: 0.697134

Epoch: 13
Loss: 0.6529355347156525
RMSE train: 0.702371	val: 0.908794	test: 0.867114
MAE train: 0.546188	val: 0.707079	test: 0.692733

Epoch: 14
Loss: 0.648029226064682
RMSE train: 0.711085	val: 0.910456	test: 0.864158
MAE train: 0.562896	val: 0.718335	test: 0.684521

Epoch: 15
Loss: 0.6201347589492798
RMSE train: 0.683936	val: 0.904922	test: 0.859756
MAE train: 0.534042	val: 0.712100	test: 0.681534

Epoch: 16
Loss: 0.602157500386238
RMSE train: 0.699793	val: 0.926319	test: 0.892227
MAE train: 0.538590	val: 0.717641	test: 0.708666

Epoch: 17
Loss: 0.5877271443605423
RMSE train: 0.713831	val: 0.913261	test: 0.884828
MAE train: 0.556275	val: 0.713927	test: 0.703195

Epoch: 18
Loss: 0.5629900693893433
RMSE train: 0.675151	val: 0.916176	test: 0.890915
MAE train: 0.526415	val: 0.715380	test: 0.708317

Epoch: 19
Loss: 0.5489644169807434
RMSE train: 0.652189	val: 0.875830	test: 0.840532
MAE train: 0.510910	val: 0.685847	test: 0.661790

Epoch: 20
Loss: 0.5376122534275055
RMSE train: 0.660507	val: 0.894224	test: 0.865659
MAE train: 0.514393	val: 0.691404	test: 0.686224

Epoch: 21
Loss: 0.5341871589422226
RMSE train: 0.661707	val: 0.902527	test: 0.868073
MAE train: 0.514416	val: 0.693171	test: 0.687923

Epoch: 22
Loss: 0.5221051663160324
RMSE train: 0.651418	val: 0.891928	test: 0.879407
MAE train: 0.507188	val: 0.689076	test: 0.695630Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/lipo/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/lipo/scaff/train_prop=0.6/lipophilicity_scaff_6_26-05_10-59-52  ]
[ Using Seed :  6  ]
[ Using device :  cuda:0  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.907399749755859
RMSE train: 2.528136	val: 2.576594	test: 2.577855
MAE train: 2.296977	val: 2.329943	test: 2.338205

Epoch: 2
Loss: 4.317352294921875
RMSE train: 2.369772	val: 2.407567	test: 2.395577
MAE train: 2.148374	val: 2.156718	test: 2.150305

Epoch: 3
Loss: 3.1070182085037232
RMSE train: 1.955930	val: 1.996677	test: 1.987905
MAE train: 1.750143	val: 1.764509	test: 1.753205

Epoch: 4
Loss: 2.1349345207214356
RMSE train: 1.591587	val: 1.661670	test: 1.643665
MAE train: 1.391814	val: 1.434672	test: 1.414367

Epoch: 5
Loss: 1.4792980432510376
RMSE train: 1.123571	val: 1.232718	test: 1.196782
MAE train: 0.941429	val: 1.025488	test: 1.001380

Epoch: 6
Loss: 1.068626093864441
RMSE train: 0.869683	val: 1.011854	test: 0.982241
MAE train: 0.700860	val: 0.816898	test: 0.798274

Epoch: 7
Loss: 0.8235501825809479
RMSE train: 0.788290	val: 0.941550	test: 0.905719
MAE train: 0.619089	val: 0.745517	test: 0.731061

Epoch: 8
Loss: 0.8023689270019532
RMSE train: 0.785911	val: 0.946729	test: 0.909657
MAE train: 0.611094	val: 0.733594	test: 0.730636

Epoch: 9
Loss: 0.7564146280288696
RMSE train: 0.755084	val: 0.923157	test: 0.882324
MAE train: 0.589883	val: 0.723969	test: 0.704183

Epoch: 10
Loss: 0.738389790058136
RMSE train: 0.757510	val: 0.928315	test: 0.887243
MAE train: 0.592778	val: 0.722083	test: 0.709940

Epoch: 11
Loss: 0.7040523290634155
RMSE train: 0.739788	val: 0.925195	test: 0.880500
MAE train: 0.574977	val: 0.718428	test: 0.704569

Epoch: 12
Loss: 0.6825113415718078
RMSE train: 0.739003	val: 0.924163	test: 0.904353
MAE train: 0.577040	val: 0.715324	test: 0.716825

Epoch: 13
Loss: 0.6403126358985901
RMSE train: 0.711262	val: 0.900919	test: 0.863265
MAE train: 0.554767	val: 0.693423	test: 0.683407

Epoch: 14
Loss: 0.6303763270378113
RMSE train: 0.704244	val: 0.887205	test: 0.856268
MAE train: 0.552407	val: 0.692767	test: 0.678134

Epoch: 15
Loss: 0.5963535487651825
RMSE train: 0.666220	val: 0.863033	test: 0.836072
MAE train: 0.516562	val: 0.670117	test: 0.659720

Epoch: 16
Loss: 0.6233967185020447
RMSE train: 0.709480	val: 0.906788	test: 0.873684
MAE train: 0.553027	val: 0.697789	test: 0.693417

Epoch: 17
Loss: 0.6195782780647278
RMSE train: 0.683940	val: 0.866782	test: 0.842856
MAE train: 0.530348	val: 0.672517	test: 0.666237

Epoch: 18
Loss: 0.5676278531551361
RMSE train: 0.658960	val: 0.874356	test: 0.843043
MAE train: 0.511389	val: 0.672478	test: 0.666583

Epoch: 19
Loss: 0.5700182318687439
RMSE train: 0.676388	val: 0.884130	test: 0.855514
MAE train: 0.523011	val: 0.681553	test: 0.676660

Epoch: 20
Loss: 0.5559059292078018
RMSE train: 0.660543	val: 0.885664	test: 0.858885
MAE train: 0.511320	val: 0.683034	test: 0.680494

Epoch: 21
Loss: 0.5391094446182251
RMSE train: 0.651047	val: 0.879923	test: 0.864982
MAE train: 0.503345	val: 0.674311	test: 0.686902

Epoch: 22
Loss: 0.5274361371994019
RMSE train: 0.656743	val: 0.868409	test: 0.856258
MAE train: 0.511796	val: 0.672418	test: 0.680486Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/lipo/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/lipo/scaff/train_prop=0.6/lipophilicity_scaff_4_26-05_10-59-52  ]
[ Using Seed :  4  ]
[ Using device :  cuda:0  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.313443660736084
RMSE train: 2.293980	val: 2.350842	test: 2.341664
MAE train: 2.054634	val: 2.093839	test: 2.087228

Epoch: 2
Loss: 3.8513420104980467
RMSE train: 2.023960	val: 2.077717	test: 2.044211
MAE train: 1.784725	val: 1.817513	test: 1.788027

Epoch: 3
Loss: 2.6924906969070435
RMSE train: 1.879171	val: 1.954528	test: 1.924945
MAE train: 1.662402	val: 1.709711	test: 1.687300

Epoch: 4
Loss: 1.891470980644226
RMSE train: 1.516252	val: 1.592803	test: 1.568945
MAE train: 1.315463	val: 1.365166	test: 1.346594

Epoch: 5
Loss: 1.2720169723033905
RMSE train: 1.118973	val: 1.220728	test: 1.190022
MAE train: 0.931348	val: 1.018009	test: 0.989278

Epoch: 6
Loss: 0.9363056182861328
RMSE train: 0.898560	val: 1.045891	test: 1.003910
MAE train: 0.718990	val: 0.842158	test: 0.814009

Epoch: 7
Loss: 0.8324585735797883
RMSE train: 0.791177	val: 0.963862	test: 0.911957
MAE train: 0.630584	val: 0.768558	test: 0.737099

Epoch: 8
Loss: 0.7993948042392731
RMSE train: 0.758113	val: 0.941937	test: 0.895411
MAE train: 0.599569	val: 0.744631	test: 0.715129

Epoch: 9
Loss: 0.7631294131278992
RMSE train: 0.736723	val: 0.951210	test: 0.908267
MAE train: 0.575517	val: 0.742292	test: 0.727893

Epoch: 10
Loss: 0.7124805748462677
RMSE train: 0.732741	val: 0.934060	test: 0.890964
MAE train: 0.572056	val: 0.723891	test: 0.711811

Epoch: 11
Loss: 0.6698125422000885
RMSE train: 0.704344	val: 0.909542	test: 0.855035
MAE train: 0.544479	val: 0.708917	test: 0.681295

Epoch: 12
Loss: 0.654513168334961
RMSE train: 0.713162	val: 0.923177	test: 0.885129
MAE train: 0.556248	val: 0.724392	test: 0.705874

Epoch: 13
Loss: 0.651141881942749
RMSE train: 0.697516	val: 0.915721	test: 0.874697
MAE train: 0.543537	val: 0.722946	test: 0.694699

Epoch: 14
Loss: 0.6334721922874451
RMSE train: 0.662693	val: 0.901194	test: 0.871361
MAE train: 0.514908	val: 0.707872	test: 0.692287

Epoch: 15
Loss: 0.6045300036668777
RMSE train: 0.663327	val: 0.886385	test: 0.848869
MAE train: 0.519407	val: 0.695911	test: 0.674072

Epoch: 16
Loss: 0.5919823586940766
RMSE train: 0.668452	val: 0.885583	test: 0.860626
MAE train: 0.523865	val: 0.698251	test: 0.683924

Epoch: 17
Loss: 0.5953307151794434
RMSE train: 0.679135	val: 0.893190	test: 0.862602
MAE train: 0.540236	val: 0.705268	test: 0.688004

Epoch: 18
Loss: 0.5900461077690125
RMSE train: 0.641228	val: 0.890927	test: 0.864087
MAE train: 0.501553	val: 0.703451	test: 0.690251

Epoch: 19
Loss: 0.5597215592861176
RMSE train: 0.657391	val: 0.889885	test: 0.875693
MAE train: 0.519198	val: 0.702540	test: 0.696844

Epoch: 20
Loss: 0.5233946114778518
RMSE train: 0.636738	val: 0.874502	test: 0.853987
MAE train: 0.501602	val: 0.684848	test: 0.675702

Epoch: 21
Loss: 0.5312287598848343
RMSE train: 0.644835	val: 0.876480	test: 0.852525
MAE train: 0.503961	val: 0.686073	test: 0.675342

Epoch: 22
Loss: 0.5484440922737122
RMSE train: 0.643697	val: 0.879269	test: 0.866631
MAE train: 0.503657	val: 0.684014	test: 0.687742Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/lipo/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/lipo/scaff/train_prop=0.7/lipophilicity_scaff_4_26-05_10-59-52  ]
[ Using Seed :  4  ]
[ Using device :  cuda:1  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.222122589747111
RMSE train: 2.359503	val: 2.378501	test: 2.403727
MAE train: 2.110987	val: 2.129348	test: 2.148089

Epoch: 2
Loss: 3.564956565697988
RMSE train: 2.114908	val: 2.127559	test: 2.136499
MAE train: 1.885264	val: 1.881896	test: 1.885926

Epoch: 3
Loss: 2.3289610743522644
RMSE train: 1.686148	val: 1.737763	test: 1.740704
MAE train: 1.477451	val: 1.520990	test: 1.513064

Epoch: 4
Loss: 1.5030419329802196
RMSE train: 1.288545	val: 1.340809	test: 1.330141
MAE train: 1.091328	val: 1.127801	test: 1.124448

Epoch: 5
Loss: 1.0427672664324443
RMSE train: 0.946905	val: 1.052469	test: 1.018166
MAE train: 0.773916	val: 0.855107	test: 0.828047

Epoch: 6
Loss: 0.8594392687082291
RMSE train: 0.825435	val: 0.974485	test: 0.921030
MAE train: 0.652311	val: 0.754437	test: 0.743611

Epoch: 7
Loss: 0.8046188354492188
RMSE train: 0.810984	val: 0.986249	test: 0.901277
MAE train: 0.632114	val: 0.764593	test: 0.726954

Epoch: 8
Loss: 0.7457603414853414
RMSE train: 0.786935	val: 0.975319	test: 0.899260
MAE train: 0.617072	val: 0.748266	test: 0.729475

Epoch: 9
Loss: 0.7308893948793411
RMSE train: 0.760659	val: 0.940700	test: 0.859464
MAE train: 0.598512	val: 0.720327	test: 0.696078

Epoch: 10
Loss: 0.7070549925168356
RMSE train: 0.758896	val: 0.954730	test: 0.872634
MAE train: 0.594461	val: 0.732802	test: 0.701585

Epoch: 11
Loss: 0.688881054520607
RMSE train: 0.763393	val: 0.943848	test: 0.867923
MAE train: 0.599453	val: 0.717875	test: 0.705953

Epoch: 12
Loss: 0.6738846898078918
RMSE train: 0.719256	val: 0.930831	test: 0.855355
MAE train: 0.564526	val: 0.711915	test: 0.690363

Epoch: 13
Loss: 0.679275393486023
RMSE train: 0.728533	val: 0.923119	test: 0.849950
MAE train: 0.576641	val: 0.715543	test: 0.686249

Epoch: 14
Loss: 0.6438085188468298
RMSE train: 0.690842	val: 0.896841	test: 0.827745
MAE train: 0.544884	val: 0.691838	test: 0.667217

Epoch: 15
Loss: 0.6270806938409805
RMSE train: 0.710351	val: 0.922478	test: 0.845474
MAE train: 0.553767	val: 0.710013	test: 0.683156

Epoch: 16
Loss: 0.5917369474967321
RMSE train: 0.677165	val: 0.892115	test: 0.820476
MAE train: 0.528355	val: 0.682222	test: 0.666061

Epoch: 17
Loss: 0.5825254867474238
RMSE train: 0.684823	val: 0.893874	test: 0.818588
MAE train: 0.543897	val: 0.692228	test: 0.656820

Epoch: 18
Loss: 0.5559582834442457
RMSE train: 0.660407	val: 0.875034	test: 0.806239
MAE train: 0.520368	val: 0.667173	test: 0.643847

Epoch: 19
Loss: 0.5812466616431872
RMSE train: 0.656390	val: 0.892181	test: 0.823775
MAE train: 0.514368	val: 0.681717	test: 0.663707

Epoch: 20
Loss: 0.5756953756014506
RMSE train: 0.686486	val: 0.880619	test: 0.812089
MAE train: 0.541079	val: 0.679231	test: 0.652347

Epoch: 21
Loss: 0.5389329517881075
RMSE train: 0.672566	val: 0.900598	test: 0.843920
MAE train: 0.523004	val: 0.687828	test: 0.686781

Epoch: 22
Loss: 0.5241843263308207
RMSE train: 0.642619	val: 0.872286	test: 0.805571
MAE train: 0.500863	val: 0.660329	test: 0.647442Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/lipo/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/lipo/scaff/train_prop=0.7/lipophilicity_scaff_6_26-05_10-59-52  ]
[ Using Seed :  6  ]
[ Using device :  cuda:1  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.825339317321777
RMSE train: 2.383498	val: 2.382782	test: 2.425711
MAE train: 2.145007	val: 2.135614	test: 2.178018

Epoch: 2
Loss: 4.049472014109294
RMSE train: 2.300318	val: 2.304715	test: 2.327743
MAE train: 2.082673	val: 2.071025	test: 2.089342

Epoch: 3
Loss: 2.7022927701473236
RMSE train: 1.832084	val: 1.849187	test: 1.866663
MAE train: 1.627782	val: 1.627748	test: 1.639141

Epoch: 4
Loss: 1.7077805002530415
RMSE train: 1.332105	val: 1.386251	test: 1.383630
MAE train: 1.136724	val: 1.175192	test: 1.175057

Epoch: 5
Loss: 1.157622218132019
RMSE train: 0.940334	val: 1.042313	test: 1.005997
MAE train: 0.767735	val: 0.839570	test: 0.829042

Epoch: 6
Loss: 0.8823022743066152
RMSE train: 0.816151	val: 0.963325	test: 0.869690
MAE train: 0.642312	val: 0.749739	test: 0.706967

Epoch: 7
Loss: 0.7907095750172933
RMSE train: 0.825463	val: 0.973891	test: 0.900170
MAE train: 0.643571	val: 0.745180	test: 0.727344

Epoch: 8
Loss: 0.7976217468579611
RMSE train: 0.771433	val: 0.940747	test: 0.853420
MAE train: 0.598503	val: 0.720726	test: 0.690466

Epoch: 9
Loss: 0.764459470907847
RMSE train: 0.789573	val: 0.936163	test: 0.869714
MAE train: 0.617147	val: 0.722296	test: 0.700403

Epoch: 10
Loss: 0.7100440561771393
RMSE train: 0.733891	val: 0.915653	test: 0.837521
MAE train: 0.574163	val: 0.704847	test: 0.672233

Epoch: 11
Loss: 0.6786666711171468
RMSE train: 0.742554	val: 0.933955	test: 0.857885
MAE train: 0.574906	val: 0.713814	test: 0.691904

Epoch: 12
Loss: 0.666991318265597
RMSE train: 0.731092	val: 0.906648	test: 0.842467
MAE train: 0.578885	val: 0.697634	test: 0.679709

Epoch: 13
Loss: 0.6374833658337593
RMSE train: 0.706196	val: 0.911483	test: 0.829954
MAE train: 0.555845	val: 0.697929	test: 0.671759

Epoch: 14
Loss: 0.6403711487849554
RMSE train: 0.692542	val: 0.896414	test: 0.815067
MAE train: 0.543054	val: 0.685496	test: 0.659680

Epoch: 15
Loss: 0.6174176782369614
RMSE train: 0.709854	val: 0.917431	test: 0.850635
MAE train: 0.554042	val: 0.699944	test: 0.690941

Epoch: 16
Loss: 0.5797510320941607
RMSE train: 0.675613	val: 0.872206	test: 0.815029
MAE train: 0.530279	val: 0.666985	test: 0.651540

Epoch: 17
Loss: 0.5747946401437124
RMSE train: 0.666288	val: 0.880734	test: 0.810616
MAE train: 0.519781	val: 0.668927	test: 0.646927

Epoch: 18
Loss: 0.5816837698221207
RMSE train: 0.679033	val: 0.890400	test: 0.823112
MAE train: 0.530331	val: 0.676374	test: 0.667261

Epoch: 19
Loss: 0.5391647815704346
RMSE train: 0.649856	val: 0.863048	test: 0.797826
MAE train: 0.508014	val: 0.658195	test: 0.642976

Epoch: 20
Loss: 0.5533799504240354
RMSE train: 0.711532	val: 0.912343	test: 0.863192
MAE train: 0.552345	val: 0.694094	test: 0.696041

Epoch: 21
Loss: 0.5783939758936564
RMSE train: 0.653549	val: 0.871109	test: 0.813055
MAE train: 0.508351	val: 0.660265	test: 0.653873

Epoch: 22
Loss: 0.54596908390522
RMSE train: 0.652465	val: 0.875460	test: 0.814446
MAE train: 0.509912	val: 0.663204	test: 0.651994Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/lipo/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/lipo/scaff/train_prop=0.7/lipophilicity_scaff_5_26-05_10-59-52  ]
[ Using Seed :  5  ]
[ Using device :  cuda:1  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.37801206111908
RMSE train: 2.335584	val: 2.356770	test: 2.397470
MAE train: 2.090976	val: 2.115055	test: 2.137638

Epoch: 2
Loss: 3.680341680844625
RMSE train: 2.030354	val: 2.045999	test: 2.070943
MAE train: 1.808003	val: 1.817992	test: 1.829355

Epoch: 3
Loss: 2.395001252492269
RMSE train: 1.597313	val: 1.626165	test: 1.641149
MAE train: 1.391184	val: 1.411054	test: 1.417805

Epoch: 4
Loss: 1.5192946096261342
RMSE train: 1.130360	val: 1.194443	test: 1.171850
MAE train: 0.938409	val: 0.983231	test: 0.975220

Epoch: 5
Loss: 1.0761901537577312
RMSE train: 0.876316	val: 0.985435	test: 0.930850
MAE train: 0.702035	val: 0.773849	test: 0.753384

Epoch: 6
Loss: 0.8545774668455124
RMSE train: 0.842353	val: 0.977012	test: 0.936049
MAE train: 0.660991	val: 0.758142	test: 0.746883

Epoch: 7
Loss: 0.8117205798625946
RMSE train: 0.791366	val: 0.966287	test: 0.872164
MAE train: 0.612830	val: 0.740193	test: 0.704187

Epoch: 8
Loss: 0.783396119872729
RMSE train: 0.811355	val: 0.982333	test: 0.886344
MAE train: 0.634336	val: 0.757534	test: 0.715986

Epoch: 9
Loss: 0.7504849433898926
RMSE train: 0.762532	val: 0.940986	test: 0.869208
MAE train: 0.592444	val: 0.722129	test: 0.700401

Epoch: 10
Loss: 0.6750396192073822
RMSE train: 0.745250	val: 0.935328	test: 0.845875
MAE train: 0.584589	val: 0.717159	test: 0.680957

Epoch: 11
Loss: 0.678027997414271
RMSE train: 0.749855	val: 0.958691	test: 0.868195
MAE train: 0.576154	val: 0.729280	test: 0.705079

Epoch: 12
Loss: 0.6573637276887894
RMSE train: 0.746012	val: 0.942961	test: 0.851491
MAE train: 0.578845	val: 0.722595	test: 0.692468

Epoch: 13
Loss: 0.6230236540238062
RMSE train: 0.720819	val: 0.930351	test: 0.844176
MAE train: 0.560564	val: 0.707919	test: 0.684477

Epoch: 14
Loss: 0.6145786692698797
RMSE train: 0.710890	val: 0.923286	test: 0.837969
MAE train: 0.555181	val: 0.702860	test: 0.677248

Epoch: 15
Loss: 0.6204613546530405
RMSE train: 0.720734	val: 0.932633	test: 0.855698
MAE train: 0.557059	val: 0.707987	test: 0.687943

Epoch: 16
Loss: 0.6104350611567497
RMSE train: 0.697606	val: 0.903497	test: 0.823193
MAE train: 0.546463	val: 0.686787	test: 0.661026

Epoch: 17
Loss: 0.58516725897789
RMSE train: 0.719945	val: 0.938994	test: 0.860822
MAE train: 0.554722	val: 0.711852	test: 0.693786

Epoch: 18
Loss: 0.5924228678146998
RMSE train: 0.690873	val: 0.897138	test: 0.827223
MAE train: 0.538525	val: 0.676957	test: 0.654298

Epoch: 19
Loss: 0.5840235476692518
RMSE train: 0.670563	val: 0.895467	test: 0.822858
MAE train: 0.522081	val: 0.678027	test: 0.660263

Epoch: 20
Loss: 0.5679708421230316
RMSE train: 0.663552	val: 0.901274	test: 0.818709
MAE train: 0.516219	val: 0.678329	test: 0.658500

Epoch: 21
Loss: 0.5619067450364431
RMSE train: 0.721035	val: 0.941428	test: 0.865169
MAE train: 0.553277	val: 0.713393	test: 0.695876

Epoch: 22
Loss: 0.5420087178548177
RMSE train: 0.683208	val: 0.910225	test: 0.828233
MAE train: 0.530431	val: 0.685461	test: 0.657349Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/lipo/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/lipo/scaff/train_prop=0.8/lipophilicity_scaff_6_26-05_10-59-52  ]
[ Using Seed :  6  ]
[ Using device :  cuda:2  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.673087460654123
RMSE train: 2.395213	val: 2.368857	test: 2.476915
MAE train: 2.155519	val: 2.104493	test: 2.247097

Epoch: 2
Loss: 3.7183964252471924
RMSE train: 1.996717	val: 1.973304	test: 2.061243
MAE train: 1.773409	val: 1.720517	test: 1.834796

Epoch: 3
Loss: 2.323075090135847
RMSE train: 1.399372	val: 1.415818	test: 1.479029
MAE train: 1.195351	val: 1.192836	test: 1.267687

Epoch: 4
Loss: 1.3973592732633864
RMSE train: 0.970762	val: 1.035975	test: 1.046344
MAE train: 0.792648	val: 0.839940	test: 0.864490

Epoch: 5
Loss: 0.9704262954848153
RMSE train: 0.887112	val: 0.982846	test: 0.951344
MAE train: 0.715794	val: 0.783570	test: 0.768158

Epoch: 6
Loss: 0.8557138357843671
RMSE train: 0.851200	val: 0.949873	test: 0.894716
MAE train: 0.664903	val: 0.740899	test: 0.717894

Epoch: 7
Loss: 0.844209611415863
RMSE train: 0.812864	val: 0.928028	test: 0.873748
MAE train: 0.636984	val: 0.715746	test: 0.705545

Epoch: 8
Loss: 0.8075282871723175
RMSE train: 0.806232	val: 0.917617	test: 0.848662
MAE train: 0.623604	val: 0.716487	test: 0.681399

Epoch: 9
Loss: 0.7747692380632673
RMSE train: 0.804668	val: 0.931838	test: 0.871085
MAE train: 0.628006	val: 0.720782	test: 0.710350

Epoch: 10
Loss: 0.7140688300132751
RMSE train: 0.774314	val: 0.910307	test: 0.859080
MAE train: 0.608659	val: 0.709619	test: 0.698704

Epoch: 11
Loss: 0.7162712727274213
RMSE train: 0.771692	val: 0.876838	test: 0.829576
MAE train: 0.597920	val: 0.685154	test: 0.675571

Epoch: 12
Loss: 0.7297150407518659
RMSE train: 0.760271	val: 0.900990	test: 0.841458
MAE train: 0.587908	val: 0.704026	test: 0.682451

Epoch: 13
Loss: 0.7119912888322558
RMSE train: 0.738460	val: 0.869987	test: 0.814749
MAE train: 0.571330	val: 0.680489	test: 0.658639

Epoch: 14
Loss: 0.6865294405392238
RMSE train: 0.739264	val: 0.871847	test: 0.817574
MAE train: 0.577494	val: 0.678415	test: 0.667437

Epoch: 15
Loss: 0.6474000300679889
RMSE train: 0.713391	val: 0.850771	test: 0.797586
MAE train: 0.553997	val: 0.663455	test: 0.638859

Epoch: 16
Loss: 0.665523179939815
RMSE train: 0.711624	val: 0.861283	test: 0.807341
MAE train: 0.552271	val: 0.674809	test: 0.649757

Epoch: 17
Loss: 0.6040047236851284
RMSE train: 0.698156	val: 0.848462	test: 0.794040
MAE train: 0.543498	val: 0.658302	test: 0.645292

Epoch: 18
Loss: 0.6443861126899719
RMSE train: 0.734679	val: 0.894935	test: 0.834114
MAE train: 0.570102	val: 0.702059	test: 0.673124

Epoch: 19
Loss: 0.6109411631311689
RMSE train: 0.702761	val: 0.852923	test: 0.797454
MAE train: 0.542122	val: 0.663450	test: 0.646403

Epoch: 20
Loss: 0.5887958620275769
RMSE train: 0.709743	val: 0.860636	test: 0.805207
MAE train: 0.552392	val: 0.664483	test: 0.652377

Epoch: 21
Loss: 0.5580922173602241
RMSE train: 0.678672	val: 0.831433	test: 0.791375
MAE train: 0.524638	val: 0.642222	test: 0.643317

Epoch: 22
Loss: 0.5667220311505454
RMSE train: 0.699647	val: 0.856204	test: 0.815646
MAE train: 0.538567	val: 0.660998	test: 0.663095Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/lipo/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/lipo/scaff/train_prop=0.8/lipophilicity_scaff_5_26-05_10-59-52  ]
[ Using Seed :  5  ]
[ Using device :  cuda:2  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.196827105113438
RMSE train: 2.172349	val: 2.152053	test: 2.268781
MAE train: 1.929786	val: 1.888300	test: 2.023869

Epoch: 2
Loss: 3.420146482331412
RMSE train: 1.909187	val: 1.905221	test: 2.003907
MAE train: 1.684312	val: 1.664233	test: 1.770869

Epoch: 3
Loss: 2.0926115001950945
RMSE train: 1.364650	val: 1.377787	test: 1.431696
MAE train: 1.147634	val: 1.148623	test: 1.219520

Epoch: 4
Loss: 1.2750649963106429
RMSE train: 1.006415	val: 1.064139	test: 1.059187
MAE train: 0.822567	val: 0.862868	test: 0.877360

Epoch: 5
Loss: 0.9709506801196507
RMSE train: 0.874387	val: 0.962840	test: 0.911748
MAE train: 0.688546	val: 0.767488	test: 0.743917

Epoch: 6
Loss: 0.8391655172620501
RMSE train: 0.824685	val: 0.925917	test: 0.873612
MAE train: 0.640178	val: 0.737031	test: 0.709803

Epoch: 7
Loss: 0.8527074711663383
RMSE train: 0.820713	val: 0.923653	test: 0.860037
MAE train: 0.648060	val: 0.739862	test: 0.702143

Epoch: 8
Loss: 0.7961969971656799
RMSE train: 0.791570	val: 0.926257	test: 0.874845
MAE train: 0.622973	val: 0.731421	test: 0.705029

Epoch: 9
Loss: 0.7632995247840881
RMSE train: 0.771238	val: 0.911844	test: 0.827979
MAE train: 0.606318	val: 0.718850	test: 0.671283

Epoch: 10
Loss: 0.716221456016813
RMSE train: 0.769492	val: 0.889387	test: 0.837012
MAE train: 0.606309	val: 0.696142	test: 0.676321

Epoch: 11
Loss: 0.7164526964936938
RMSE train: 0.760109	val: 0.899019	test: 0.828375
MAE train: 0.597872	val: 0.706762	test: 0.671303

Epoch: 12
Loss: 0.7097261505467551
RMSE train: 0.770350	val: 0.905412	test: 0.836684
MAE train: 0.600635	val: 0.707273	test: 0.674959

Epoch: 13
Loss: 0.7026695523943219
RMSE train: 0.748731	val: 0.907474	test: 0.830834
MAE train: 0.587465	val: 0.708097	test: 0.665335

Epoch: 14
Loss: 0.691292907510485
RMSE train: 0.733385	val: 0.874293	test: 0.816604
MAE train: 0.571006	val: 0.681861	test: 0.643319

Epoch: 15
Loss: 0.6344929443938392
RMSE train: 0.752437	val: 0.906180	test: 0.845926
MAE train: 0.590897	val: 0.707812	test: 0.686106

Epoch: 16
Loss: 0.624052780015128
RMSE train: 0.720233	val: 0.867867	test: 0.814923
MAE train: 0.564817	val: 0.678676	test: 0.647353

Epoch: 17
Loss: 0.65476308124406
RMSE train: 0.714427	val: 0.869051	test: 0.842436
MAE train: 0.555520	val: 0.674422	test: 0.681568

Epoch: 18
Loss: 0.6361267481531415
RMSE train: 0.734022	val: 0.880798	test: 0.829167
MAE train: 0.570772	val: 0.683653	test: 0.667769

Epoch: 19
Loss: 0.6266351427350726
RMSE train: 0.692145	val: 0.852881	test: 0.800213
MAE train: 0.542602	val: 0.665287	test: 0.638819

Epoch: 20
Loss: 0.5864948843206678
RMSE train: 0.705873	val: 0.881266	test: 0.828895
MAE train: 0.555899	val: 0.683864	test: 0.671614

Epoch: 21
Loss: 0.6039557882717678
RMSE train: 0.688318	val: 0.836089	test: 0.800339
MAE train: 0.538030	val: 0.645123	test: 0.647990

Epoch: 22
Loss: 0.5730257417474475
RMSE train: 0.674160	val: 0.836441	test: 0.790532
MAE train: 0.528853	val: 0.645254	test: 0.638934Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/lipo/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/lipo/scaff/train_prop=0.8/lipophilicity_scaff_4_26-05_10-59-52  ]
[ Using Seed :  4  ]
[ Using device :  cuda:2  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.108967883246286
RMSE train: 2.142599	val: 2.117981	test: 2.208207
MAE train: 1.900407	val: 1.846630	test: 1.973638

Epoch: 2
Loss: 3.2070435115269254
RMSE train: 1.899823	val: 1.862093	test: 1.954873
MAE train: 1.667747	val: 1.608153	test: 1.720197

Epoch: 3
Loss: 1.9715714795248849
RMSE train: 1.264391	val: 1.276361	test: 1.312087
MAE train: 1.062243	val: 1.067608	test: 1.103423

Epoch: 4
Loss: 1.24346695627485
RMSE train: 0.927458	val: 0.983498	test: 0.984832
MAE train: 0.749009	val: 0.790411	test: 0.803357

Epoch: 5
Loss: 0.8943800755909511
RMSE train: 0.853954	val: 0.940620	test: 0.918062
MAE train: 0.670632	val: 0.740152	test: 0.739741

Epoch: 6
Loss: 0.8280073957783836
RMSE train: 0.823659	val: 0.914831	test: 0.873451
MAE train: 0.644448	val: 0.716862	test: 0.704005

Epoch: 7
Loss: 0.8064799734524318
RMSE train: 0.808013	val: 0.912593	test: 0.863713
MAE train: 0.628915	val: 0.712416	test: 0.692263

Epoch: 8
Loss: 0.7574056684970856
RMSE train: 0.807564	val: 0.904240	test: 0.878007
MAE train: 0.623837	val: 0.714972	test: 0.705076

Epoch: 9
Loss: 0.7553746402263641
RMSE train: 0.782286	val: 0.914213	test: 0.884085
MAE train: 0.606135	val: 0.717784	test: 0.719175

Epoch: 10
Loss: 0.7569126614502498
RMSE train: 0.776486	val: 0.897732	test: 0.848022
MAE train: 0.604919	val: 0.709817	test: 0.691364

Epoch: 11
Loss: 0.6856947030339923
RMSE train: 0.783785	val: 0.913813	test: 0.868550
MAE train: 0.607790	val: 0.707993	test: 0.706825

Epoch: 12
Loss: 0.7260951995849609
RMSE train: 0.754082	val: 0.844670	test: 0.825705
MAE train: 0.583877	val: 0.663665	test: 0.668032

Epoch: 13
Loss: 0.6689811476639339
RMSE train: 0.758048	val: 0.890075	test: 0.839118
MAE train: 0.583707	val: 0.695705	test: 0.673468

Epoch: 14
Loss: 0.6405700147151947
RMSE train: 0.740366	val: 0.848834	test: 0.834246
MAE train: 0.570388	val: 0.664925	test: 0.676608

Epoch: 15
Loss: 0.6467886567115784
RMSE train: 0.716550	val: 0.842191	test: 0.825584
MAE train: 0.561685	val: 0.654851	test: 0.662946

Epoch: 16
Loss: 0.606132692524365
RMSE train: 0.700777	val: 0.834385	test: 0.826856
MAE train: 0.546543	val: 0.651309	test: 0.659421

Epoch: 17
Loss: 0.6493211090564728
RMSE train: 0.730214	val: 0.875182	test: 0.830250
MAE train: 0.567690	val: 0.686809	test: 0.671087

Epoch: 18
Loss: 0.601815619638988
RMSE train: 0.722039	val: 0.874725	test: 0.829352
MAE train: 0.561741	val: 0.687754	test: 0.668424

Epoch: 19
Loss: 0.5989197152001517
RMSE train: 0.675845	val: 0.835484	test: 0.811238
MAE train: 0.523019	val: 0.645496	test: 0.650563

Epoch: 20
Loss: 0.5509068327290672
RMSE train: 0.676069	val: 0.843156	test: 0.804042
MAE train: 0.522511	val: 0.657003	test: 0.648611

Epoch: 21
Loss: 0.546737579362733
RMSE train: 0.679122	val: 0.845825	test: 0.816032
MAE train: 0.526403	val: 0.661505	test: 0.658784

Epoch: 22
Loss: 0.5771254833255496
RMSE train: 0.665981	val: 0.825517	test: 0.790511
MAE train: 0.519089	val: 0.641509	test: 0.633510

Epoch: 23
Loss: 0.5511842787265777
RMSE train: 0.616876	val: 0.867463	test: 0.837707
MAE train: 0.481119	val: 0.668190	test: 0.658763

Epoch: 24
Loss: 0.5203197509050369
RMSE train: 0.641869	val: 0.870896	test: 0.843159
MAE train: 0.502973	val: 0.675832	test: 0.664521

Epoch: 25
Loss: 0.5165721356868744
RMSE train: 0.613445	val: 0.870722	test: 0.853654
MAE train: 0.476913	val: 0.671379	test: 0.674924

Epoch: 26
Loss: 0.4948602348566055
RMSE train: 0.598399	val: 0.840509	test: 0.829922
MAE train: 0.465783	val: 0.651004	test: 0.653104

Epoch: 27
Loss: 0.4948052704334259
RMSE train: 0.639184	val: 0.873404	test: 0.846824
MAE train: 0.497359	val: 0.674825	test: 0.664129

Epoch: 28
Loss: 0.4781904429197311
RMSE train: 0.598964	val: 0.853778	test: 0.831051
MAE train: 0.466457	val: 0.660377	test: 0.658891

Epoch: 29
Loss: 0.48096929490566254
RMSE train: 0.607978	val: 0.869745	test: 0.844516
MAE train: 0.471907	val: 0.671680	test: 0.664728

Epoch: 30
Loss: 0.48027643859386443
RMSE train: 0.600704	val: 0.871727	test: 0.850439
MAE train: 0.464434	val: 0.677899	test: 0.670160

Epoch: 31
Loss: 0.4741539597511292
RMSE train: 0.593668	val: 0.846986	test: 0.830356
MAE train: 0.464060	val: 0.659366	test: 0.654772

Epoch: 32
Loss: 0.4752377837896347
RMSE train: 0.581542	val: 0.840780	test: 0.824491
MAE train: 0.451305	val: 0.649884	test: 0.647278

Epoch: 33
Loss: 0.43441260457038877
RMSE train: 0.576065	val: 0.855816	test: 0.837828
MAE train: 0.443515	val: 0.662003	test: 0.658264

Epoch: 34
Loss: 0.45774819552898405
RMSE train: 0.591832	val: 0.845765	test: 0.829219
MAE train: 0.462445	val: 0.654821	test: 0.650346

Epoch: 35
Loss: 0.4572194904088974
RMSE train: 0.601232	val: 0.885209	test: 0.873564
MAE train: 0.468097	val: 0.685423	test: 0.688859

Epoch: 36
Loss: 0.4369475841522217
RMSE train: 0.591801	val: 0.870787	test: 0.854468
MAE train: 0.464461	val: 0.673074	test: 0.663816

Epoch: 37
Loss: 0.43514018356800077
RMSE train: 0.578869	val: 0.856625	test: 0.860134
MAE train: 0.445415	val: 0.662151	test: 0.677572

Epoch: 38
Loss: 0.4471198558807373
RMSE train: 0.569206	val: 0.845143	test: 0.832213
MAE train: 0.444659	val: 0.652523	test: 0.652309

Epoch: 39
Loss: 0.421568563580513
RMSE train: 0.580615	val: 0.847369	test: 0.842789
MAE train: 0.454077	val: 0.656685	test: 0.662957

Epoch: 40
Loss: 0.42058515548706055
RMSE train: 0.573077	val: 0.872808	test: 0.869615
MAE train: 0.440056	val: 0.669918	test: 0.679923

Epoch: 41
Loss: 0.40851359367370604
RMSE train: 0.551375	val: 0.839809	test: 0.836039
MAE train: 0.427606	val: 0.645470	test: 0.649397

Epoch: 42
Loss: 0.42734154462814333
RMSE train: 0.563396	val: 0.838638	test: 0.827910
MAE train: 0.435994	val: 0.648028	test: 0.651477

Epoch: 43
Loss: 0.42128309309482576
RMSE train: 0.560372	val: 0.833008	test: 0.816753
MAE train: 0.434278	val: 0.643844	test: 0.645345

Epoch: 44
Loss: 0.38846929669380187
RMSE train: 0.539990	val: 0.845067	test: 0.816057
MAE train: 0.417398	val: 0.648640	test: 0.637017

Epoch: 45
Loss: 0.3886741816997528
RMSE train: 0.531384	val: 0.849843	test: 0.820914
MAE train: 0.410709	val: 0.653252	test: 0.638626

Epoch: 46
Loss: 0.40080539882183075
RMSE train: 0.532181	val: 0.839371	test: 0.815172
MAE train: 0.413901	val: 0.643639	test: 0.638018

Epoch: 47
Loss: 0.38499010503292086
RMSE train: 0.565203	val: 0.860896	test: 0.852642
MAE train: 0.438814	val: 0.662558	test: 0.668907

Epoch: 48
Loss: 0.38096024096012115
RMSE train: 0.542537	val: 0.865981	test: 0.848055
MAE train: 0.423202	val: 0.665500	test: 0.666305

Epoch: 49
Loss: 0.3862981081008911
RMSE train: 0.546122	val: 0.853712	test: 0.845827
MAE train: 0.422581	val: 0.651534	test: 0.663098

Epoch: 50
Loss: 0.3778105705976486
RMSE train: 0.538113	val: 0.838852	test: 0.823895
MAE train: 0.418662	val: 0.641115	test: 0.641443

Epoch: 51
Loss: 0.37141440212726595
RMSE train: 0.538237	val: 0.863574	test: 0.838961
MAE train: 0.416772	val: 0.659689	test: 0.658457

Epoch: 52
Loss: 0.35893593430519105
RMSE train: 0.537588	val: 0.840289	test: 0.835795
MAE train: 0.416534	val: 0.644912	test: 0.655822

Epoch: 53
Loss: 0.36951793134212496
RMSE train: 0.548112	val: 0.841106	test: 0.830970
MAE train: 0.432635	val: 0.652589	test: 0.654991

Epoch: 54
Loss: 0.3662000834941864
RMSE train: 0.519202	val: 0.842711	test: 0.838331
MAE train: 0.399388	val: 0.647148	test: 0.656746

Epoch: 55
Loss: 0.37139703929424284
RMSE train: 0.536477	val: 0.839559	test: 0.824088
MAE train: 0.420527	val: 0.651646	test: 0.644393

Epoch: 56
Loss: 0.35438381135463715
RMSE train: 0.509837	val: 0.834170	test: 0.818274
MAE train: 0.395884	val: 0.642224	test: 0.639657

Epoch: 57
Loss: 0.36121049523353577
RMSE train: 0.531220	val: 0.833133	test: 0.815881
MAE train: 0.418205	val: 0.642589	test: 0.638092

Epoch: 58
Loss: 0.35793415307998655
RMSE train: 0.525073	val: 0.849453	test: 0.839924
MAE train: 0.407011	val: 0.653111	test: 0.657485

Epoch: 59
Loss: 0.35335586667060853
RMSE train: 0.504005	val: 0.822664	test: 0.812286
MAE train: 0.389278	val: 0.628496	test: 0.632393

Epoch: 60
Loss: 0.330840528011322
RMSE train: 0.525850	val: 0.864897	test: 0.855513
MAE train: 0.407751	val: 0.664042	test: 0.665958

Epoch: 61
Loss: 0.3142328917980194
RMSE train: 0.520812	val: 0.849353	test: 0.846263
MAE train: 0.402838	val: 0.650851	test: 0.660015

Epoch: 62
Loss: 0.3448279172182083
RMSE train: 0.512636	val: 0.837654	test: 0.834105
MAE train: 0.397040	val: 0.641932	test: 0.650316

Epoch: 63
Loss: 0.31806813180446625
RMSE train: 0.495178	val: 0.832035	test: 0.829641
MAE train: 0.382529	val: 0.637987	test: 0.641533

Epoch: 64
Loss: 0.32449236512184143
RMSE train: 0.494622	val: 0.816560	test: 0.808826
MAE train: 0.385490	val: 0.625395	test: 0.628840

Epoch: 65
Loss: 0.3201038330793381
RMSE train: 0.508140	val: 0.830626	test: 0.825977
MAE train: 0.395120	val: 0.635096	test: 0.641361

Epoch: 66
Loss: 0.31784954071044924
RMSE train: 0.501430	val: 0.829691	test: 0.829056
MAE train: 0.389875	val: 0.633777	test: 0.643511

Epoch: 67
Loss: 0.3172511368989944
RMSE train: 0.490562	val: 0.826273	test: 0.812808
MAE train: 0.382003	val: 0.627998	test: 0.630050

Epoch: 68
Loss: 0.3248221129179001
RMSE train: 0.480519	val: 0.829041	test: 0.819320
MAE train: 0.372680	val: 0.635988	test: 0.635913

Epoch: 69
Loss: 0.3121791422367096
RMSE train: 0.495018	val: 0.823925	test: 0.826180
MAE train: 0.384617	val: 0.635892	test: 0.640729

Epoch: 70
Loss: 0.3152096062898636
RMSE train: 0.509048	val: 0.863041	test: 0.861205
MAE train: 0.393592	val: 0.659237	test: 0.675964

Epoch: 71
Loss: 0.3225782960653305
RMSE train: 0.481585	val: 0.821982	test: 0.825034
MAE train: 0.371868	val: 0.627810	test: 0.641437

Epoch: 72
Loss: 0.30808786749839784
RMSE train: 0.485038	val: 0.815867	test: 0.816310
MAE train: 0.377600	val: 0.626473	test: 0.636439

Epoch: 73
Loss: 0.3144822001457214
RMSE train: 0.473637	val: 0.828416	test: 0.836048
MAE train: 0.365935	val: 0.633786	test: 0.644287

Epoch: 74
Loss: 0.312878355383873
RMSE train: 0.494811	val: 0.831790	test: 0.831712
MAE train: 0.385271	val: 0.642517	test: 0.643733

Epoch: 75
Loss: 0.3202521845698357
RMSE train: 0.477755	val: 0.825957	test: 0.826943
MAE train: 0.373027	val: 0.636342	test: 0.638437

Epoch: 76
Loss: 0.3154932767152786
RMSE train: 0.487362	val: 0.829487	test: 0.831121
MAE train: 0.378533	val: 0.635584	test: 0.647434

Epoch: 77
Loss: 0.29790267944335935
RMSE train: 0.487209	val: 0.834987	test: 0.843587
MAE train: 0.378055	val: 0.640073	test: 0.656675

Epoch: 78
Loss: 0.31533188819885255
RMSE train: 0.478821	val: 0.819636	test: 0.815569
MAE train: 0.374113	val: 0.631166	test: 0.637176

Epoch: 79
Loss: 0.30324000418186187
RMSE train: 0.483224	val: 0.814135	test: 0.819259
MAE train: 0.376357	val: 0.627216	test: 0.637852

Epoch: 80
Loss: 0.30228552520275115
RMSE train: 0.493693	val: 0.835277	test: 0.830783
MAE train: 0.383462	val: 0.637898	test: 0.645493

Epoch: 81
Loss: 0.30010374188423156
RMSE train: 0.486794	val: 0.824602	test: 0.832512
MAE train: 0.378710	val: 0.634287	test: 0.649334

Epoch: 82
Loss: 0.29373389631509783
RMSE train: 0.468003	val: 0.820743	test: 0.823734
MAE train: 0.359849	val: 0.626743	test: 0.638986

Epoch: 83
Loss: 0.2994943708181381
RMSE train: 0.473301	val: 0.822233	test: 0.820889

Epoch: 23
Loss: 0.528834593296051
RMSE train: 0.640806	val: 0.877447	test: 0.851117
MAE train: 0.498960	val: 0.679330	test: 0.670083

Epoch: 24
Loss: 0.5175156950950622
RMSE train: 0.625277	val: 0.863052	test: 0.843712
MAE train: 0.483252	val: 0.668127	test: 0.662049

Epoch: 25
Loss: 0.5038226485252381
RMSE train: 0.629411	val: 0.862572	test: 0.837501
MAE train: 0.490302	val: 0.662939	test: 0.659682

Epoch: 26
Loss: 0.5054423570632934
RMSE train: 0.621537	val: 0.861084	test: 0.840051
MAE train: 0.485863	val: 0.665329	test: 0.664117

Epoch: 27
Loss: 0.47254022061824796
RMSE train: 0.618217	val: 0.863637	test: 0.848989
MAE train: 0.484432	val: 0.668346	test: 0.673034

Epoch: 28
Loss: 0.4790600061416626
RMSE train: 0.606903	val: 0.858252	test: 0.845437
MAE train: 0.466707	val: 0.658294	test: 0.669041

Epoch: 29
Loss: 0.47962523698806764
RMSE train: 0.612502	val: 0.850948	test: 0.840541
MAE train: 0.478994	val: 0.665108	test: 0.660834

Epoch: 30
Loss: 0.4684439331293106
RMSE train: 0.593371	val: 0.842650	test: 0.836773
MAE train: 0.461195	val: 0.654905	test: 0.659580

Epoch: 31
Loss: 0.4549324423074722
RMSE train: 0.595663	val: 0.852815	test: 0.835261
MAE train: 0.464848	val: 0.660666	test: 0.659990

Epoch: 32
Loss: 0.488347527384758
RMSE train: 0.604788	val: 0.860032	test: 0.846502
MAE train: 0.473126	val: 0.668796	test: 0.667596

Epoch: 33
Loss: 0.4558172047138214
RMSE train: 0.590332	val: 0.857804	test: 0.847833
MAE train: 0.457584	val: 0.660850	test: 0.667920

Epoch: 34
Loss: 0.4607465922832489
RMSE train: 0.616508	val: 0.868778	test: 0.852428
MAE train: 0.479434	val: 0.670151	test: 0.673429

Epoch: 35
Loss: 0.42830434143543245
RMSE train: 0.609263	val: 0.858401	test: 0.849814
MAE train: 0.470265	val: 0.658473	test: 0.668514

Epoch: 36
Loss: 0.43546960949897767
RMSE train: 0.607703	val: 0.865380	test: 0.857022
MAE train: 0.465720	val: 0.669964	test: 0.674132

Epoch: 37
Loss: 0.42407518029212954
RMSE train: 0.618698	val: 0.879449	test: 0.861735
MAE train: 0.478440	val: 0.675204	test: 0.681063

Epoch: 38
Loss: 0.4081197828054428
RMSE train: 0.565400	val: 0.840619	test: 0.831305
MAE train: 0.437677	val: 0.649449	test: 0.651446

Epoch: 39
Loss: 0.4279825150966644
RMSE train: 0.585528	val: 0.849173	test: 0.831492
MAE train: 0.454442	val: 0.650530	test: 0.658119

Epoch: 40
Loss: 0.4293638616800308
RMSE train: 0.576108	val: 0.858442	test: 0.847394
MAE train: 0.448702	val: 0.663017	test: 0.664919

Epoch: 41
Loss: 0.41598916351795195
RMSE train: 0.561353	val: 0.836721	test: 0.838634
MAE train: 0.440877	val: 0.643559	test: 0.650062

Epoch: 42
Loss: 0.39688108265399935
RMSE train: 0.563484	val: 0.839548	test: 0.830335
MAE train: 0.443488	val: 0.648669	test: 0.650134

Epoch: 43
Loss: 0.3931469976902008
RMSE train: 0.556655	val: 0.859371	test: 0.840093
MAE train: 0.430094	val: 0.656807	test: 0.662548

Epoch: 44
Loss: 0.3928578317165375
RMSE train: 0.550202	val: 0.820617	test: 0.813348
MAE train: 0.429744	val: 0.631304	test: 0.637641

Epoch: 45
Loss: 0.38572767972946165
RMSE train: 0.584379	val: 0.881567	test: 0.866016
MAE train: 0.453815	val: 0.679264	test: 0.677899

Epoch: 46
Loss: 0.38360198140144347
RMSE train: 0.544834	val: 0.835010	test: 0.831150
MAE train: 0.421782	val: 0.643805	test: 0.645667

Epoch: 47
Loss: 0.3753726452589035
RMSE train: 0.546805	val: 0.834924	test: 0.819216
MAE train: 0.426917	val: 0.645136	test: 0.643617

Epoch: 48
Loss: 0.3891105860471725
RMSE train: 0.556515	val: 0.846181	test: 0.825483
MAE train: 0.434749	val: 0.650878	test: 0.644983

Epoch: 49
Loss: 0.3754504084587097
RMSE train: 0.555920	val: 0.856593	test: 0.847212
MAE train: 0.431221	val: 0.659803	test: 0.662737

Epoch: 50
Loss: 0.36498264968395233
RMSE train: 0.540494	val: 0.863928	test: 0.839102
MAE train: 0.415556	val: 0.659745	test: 0.657337

Epoch: 51
Loss: 0.3623339354991913
RMSE train: 0.552757	val: 0.851517	test: 0.838742
MAE train: 0.427024	val: 0.653248	test: 0.654851

Epoch: 52
Loss: 0.36476374268531797
RMSE train: 0.530601	val: 0.844272	test: 0.817604
MAE train: 0.411845	val: 0.646088	test: 0.639014

Epoch: 53
Loss: 0.365137392282486
RMSE train: 0.521806	val: 0.835824	test: 0.819309
MAE train: 0.404410	val: 0.638663	test: 0.641476

Epoch: 54
Loss: 0.36583156883716583
RMSE train: 0.532750	val: 0.852225	test: 0.839023
MAE train: 0.408262	val: 0.658320	test: 0.652442

Epoch: 55
Loss: 0.35962005853652956
RMSE train: 0.518762	val: 0.843982	test: 0.828285
MAE train: 0.402733	val: 0.645303	test: 0.649430

Epoch: 56
Loss: 0.3488741725683212
RMSE train: 0.536943	val: 0.840533	test: 0.834440
MAE train: 0.413973	val: 0.644783	test: 0.647780

Epoch: 57
Loss: 0.3528876662254333
RMSE train: 0.514281	val: 0.825853	test: 0.821150
MAE train: 0.394586	val: 0.632896	test: 0.640680

Epoch: 58
Loss: 0.33906244337558744
RMSE train: 0.527682	val: 0.835077	test: 0.828427
MAE train: 0.407861	val: 0.640037	test: 0.649136

Epoch: 59
Loss: 0.3434775769710541
RMSE train: 0.510899	val: 0.816377	test: 0.812165
MAE train: 0.398461	val: 0.627528	test: 0.634696

Epoch: 60
Loss: 0.34326851963996885
RMSE train: 0.520795	val: 0.830057	test: 0.826198
MAE train: 0.403074	val: 0.636873	test: 0.646175

Epoch: 61
Loss: 0.35030151903629303
RMSE train: 0.533292	val: 0.837568	test: 0.833857
MAE train: 0.411249	val: 0.644419	test: 0.650638

Epoch: 62
Loss: 0.3309679001569748
RMSE train: 0.511848	val: 0.821810	test: 0.816103
MAE train: 0.399242	val: 0.632800	test: 0.639420

Epoch: 63
Loss: 0.33555686175823213
RMSE train: 0.506029	val: 0.818380	test: 0.818974
MAE train: 0.395518	val: 0.632029	test: 0.635923

Epoch: 64
Loss: 0.3363711029291153
RMSE train: 0.520818	val: 0.838809	test: 0.838491
MAE train: 0.403327	val: 0.645184	test: 0.654117

Epoch: 65
Loss: 0.3213819533586502
RMSE train: 0.503426	val: 0.819203	test: 0.811640
MAE train: 0.394201	val: 0.630729	test: 0.634331

Epoch: 66
Loss: 0.32720710933208463
RMSE train: 0.505653	val: 0.824582	test: 0.812651
MAE train: 0.393449	val: 0.635924	test: 0.639005

Epoch: 67
Loss: 0.3343599021434784
RMSE train: 0.512567	val: 0.849828	test: 0.836868
MAE train: 0.397370	val: 0.651423	test: 0.656228

Epoch: 68
Loss: 0.322108393907547
RMSE train: 0.499665	val: 0.821250	test: 0.816106
MAE train: 0.391196	val: 0.635658	test: 0.635779

Epoch: 69
Loss: 0.32108456641435623
RMSE train: 0.511717	val: 0.848882	test: 0.831277
MAE train: 0.397175	val: 0.654489	test: 0.649316

Epoch: 70
Loss: 0.32411221861839296
RMSE train: 0.487581	val: 0.841278	test: 0.826847
MAE train: 0.378187	val: 0.640192	test: 0.647127

Epoch: 71
Loss: 0.3273758888244629
RMSE train: 0.515060	val: 0.839001	test: 0.826425
MAE train: 0.399772	val: 0.645965	test: 0.647019

Epoch: 72
Loss: 0.3177601873874664
RMSE train: 0.514922	val: 0.853714	test: 0.847413
MAE train: 0.397960	val: 0.654243	test: 0.667300

Epoch: 73
Loss: 0.3036829024553299
RMSE train: 0.483454	val: 0.813890	test: 0.806983
MAE train: 0.377292	val: 0.621739	test: 0.628877

Epoch: 74
Loss: 0.302018541097641
RMSE train: 0.484504	val: 0.836267	test: 0.825028
MAE train: 0.377187	val: 0.637994	test: 0.646444

Epoch: 75
Loss: 0.31415793895721433
RMSE train: 0.499572	val: 0.832465	test: 0.836140
MAE train: 0.389744	val: 0.640928	test: 0.655149

Epoch: 76
Loss: 0.3010686099529266
RMSE train: 0.485210	val: 0.819767	test: 0.816918
MAE train: 0.377056	val: 0.625708	test: 0.632201

Epoch: 77
Loss: 0.29650893807411194
RMSE train: 0.506577	val: 0.847910	test: 0.831258
MAE train: 0.394227	val: 0.644055	test: 0.646004

Epoch: 78
Loss: 0.3024521738290787
RMSE train: 0.495908	val: 0.822863	test: 0.817368
MAE train: 0.389869	val: 0.635871	test: 0.633976

Epoch: 79
Loss: 0.3118516176939011
RMSE train: 0.490029	val: 0.826281	test: 0.810186
MAE train: 0.383090	val: 0.632826	test: 0.634772

Epoch: 80
Loss: 0.29326410591602325
RMSE train: 0.483622	val: 0.830647	test: 0.821687
MAE train: 0.373663	val: 0.636597	test: 0.639726

Epoch: 81
Loss: 0.2838738977909088
RMSE train: 0.493254	val: 0.843121	test: 0.843888
MAE train: 0.383698	val: 0.647812	test: 0.658040

Epoch: 82
Loss: 0.29355978071689603
RMSE train: 0.522601	val: 0.861053	test: 0.855706
MAE train: 0.405206	val: 0.656567	test: 0.664317

Epoch: 83
Loss: 0.2900932118296623
RMSE train: 0.487031	val: 0.829991	test: 0.825281

Epoch: 23
Loss: 0.5448351711034775
RMSE train: 0.633372	val: 0.887991	test: 0.872317
MAE train: 0.500196	val: 0.699660	test: 0.690641

Epoch: 24
Loss: 0.5311782985925675
RMSE train: 0.621403	val: 0.871367	test: 0.863887
MAE train: 0.489463	val: 0.686828	test: 0.684828

Epoch: 25
Loss: 0.4985925853252411
RMSE train: 0.615438	val: 0.857632	test: 0.847643
MAE train: 0.481326	val: 0.674277	test: 0.668427

Epoch: 26
Loss: 0.4923088401556015
RMSE train: 0.619140	val: 0.874766	test: 0.868821
MAE train: 0.483647	val: 0.680990	test: 0.686201

Epoch: 27
Loss: 0.4902967572212219
RMSE train: 0.612926	val: 0.866105	test: 0.848960
MAE train: 0.482384	val: 0.678106	test: 0.669049

Epoch: 28
Loss: 0.46559399366378784
RMSE train: 0.590994	val: 0.861522	test: 0.852254
MAE train: 0.459706	val: 0.675627	test: 0.669891

Epoch: 29
Loss: 0.4880011022090912
RMSE train: 0.592606	val: 0.842877	test: 0.834865
MAE train: 0.463841	val: 0.656829	test: 0.659031

Epoch: 30
Loss: 0.4554015189409256
RMSE train: 0.605733	val: 0.853107	test: 0.841419
MAE train: 0.473449	val: 0.665273	test: 0.661331

Epoch: 31
Loss: 0.4466283738613129
RMSE train: 0.587963	val: 0.842721	test: 0.829013
MAE train: 0.459916	val: 0.658855	test: 0.652394

Epoch: 32
Loss: 0.44678781628608705
RMSE train: 0.579277	val: 0.856001	test: 0.841933
MAE train: 0.453201	val: 0.672419	test: 0.666680

Epoch: 33
Loss: 0.4413690000772476
RMSE train: 0.593333	val: 0.844737	test: 0.840165
MAE train: 0.469176	val: 0.663191	test: 0.660320

Epoch: 34
Loss: 0.42613676488399505
RMSE train: 0.575664	val: 0.844276	test: 0.832510
MAE train: 0.448523	val: 0.662297	test: 0.652167

Epoch: 35
Loss: 0.43674223124980927
RMSE train: 0.588598	val: 0.855016	test: 0.852367
MAE train: 0.455122	val: 0.668009	test: 0.666445

Epoch: 36
Loss: 0.4290365487337112
RMSE train: 0.565116	val: 0.850119	test: 0.838485
MAE train: 0.443876	val: 0.662592	test: 0.656055

Epoch: 37
Loss: 0.41650558412075045
RMSE train: 0.562976	val: 0.849326	test: 0.845638
MAE train: 0.439073	val: 0.660574	test: 0.660903

Epoch: 38
Loss: 0.43297248184680937
RMSE train: 0.572192	val: 0.857227	test: 0.855876
MAE train: 0.449556	val: 0.671834	test: 0.663290

Epoch: 39
Loss: 0.42345588803291323
RMSE train: 0.554109	val: 0.845008	test: 0.837092
MAE train: 0.431853	val: 0.657718	test: 0.653457

Epoch: 40
Loss: 0.4278086632490158
RMSE train: 0.574870	val: 0.856587	test: 0.843060
MAE train: 0.453837	val: 0.672640	test: 0.664844

Epoch: 41
Loss: 0.4106164276599884
RMSE train: 0.556970	val: 0.835772	test: 0.832953
MAE train: 0.438480	val: 0.657587	test: 0.652884

Epoch: 42
Loss: 0.4194674551486969
RMSE train: 0.557383	val: 0.852803	test: 0.843404
MAE train: 0.437584	val: 0.661886	test: 0.665402

Epoch: 43
Loss: 0.3934085428714752
RMSE train: 0.547759	val: 0.839090	test: 0.843977
MAE train: 0.428379	val: 0.655639	test: 0.660359

Epoch: 44
Loss: 0.3890872061252594
RMSE train: 0.550110	val: 0.849361	test: 0.847036
MAE train: 0.429797	val: 0.662371	test: 0.661215

Epoch: 45
Loss: 0.37745101749897003
RMSE train: 0.538630	val: 0.838789	test: 0.840123
MAE train: 0.422984	val: 0.659941	test: 0.660803

Epoch: 46
Loss: 0.3723187506198883
RMSE train: 0.570182	val: 0.857511	test: 0.850607
MAE train: 0.449720	val: 0.671986	test: 0.663986

Epoch: 47
Loss: 0.3836097687482834
RMSE train: 0.528057	val: 0.830940	test: 0.825228
MAE train: 0.414377	val: 0.649763	test: 0.645410

Epoch: 48
Loss: 0.37440285384655
RMSE train: 0.533470	val: 0.849378	test: 0.838398
MAE train: 0.414936	val: 0.654602	test: 0.662095

Epoch: 49
Loss: 0.37692175805568695
RMSE train: 0.513976	val: 0.831605	test: 0.819525
MAE train: 0.401473	val: 0.642424	test: 0.642410

Epoch: 50
Loss: 0.3661955177783966
RMSE train: 0.523887	val: 0.843138	test: 0.832247
MAE train: 0.411230	val: 0.658996	test: 0.650737

Epoch: 51
Loss: 0.36873758137226104
RMSE train: 0.546065	val: 0.868579	test: 0.853152
MAE train: 0.424498	val: 0.677444	test: 0.666422

Epoch: 52
Loss: 0.37046106457710265
RMSE train: 0.528398	val: 0.829201	test: 0.818718
MAE train: 0.414229	val: 0.646123	test: 0.639305

Epoch: 53
Loss: 0.35693579018115995
RMSE train: 0.517187	val: 0.832024	test: 0.820582
MAE train: 0.403320	val: 0.643676	test: 0.643087

Epoch: 54
Loss: 0.35127667486667635
RMSE train: 0.516542	val: 0.830354	test: 0.825945
MAE train: 0.403194	val: 0.647488	test: 0.644703

Epoch: 55
Loss: 0.36695595979690554
RMSE train: 0.510297	val: 0.840559	test: 0.829336
MAE train: 0.397918	val: 0.655261	test: 0.649865

Epoch: 56
Loss: 0.34886791110038756
RMSE train: 0.501421	val: 0.829417	test: 0.819136
MAE train: 0.389748	val: 0.638026	test: 0.637140

Epoch: 57
Loss: 0.35375516414642333
RMSE train: 0.501142	val: 0.841487	test: 0.838143
MAE train: 0.392309	val: 0.655155	test: 0.654783

Epoch: 58
Loss: 0.3452423125505447
RMSE train: 0.512276	val: 0.831246	test: 0.819859
MAE train: 0.401707	val: 0.645937	test: 0.644350

Epoch: 59
Loss: 0.34521719813346863
RMSE train: 0.507369	val: 0.832599	test: 0.825500
MAE train: 0.395659	val: 0.643803	test: 0.645865

Epoch: 60
Loss: 0.343249449133873
RMSE train: 0.492944	val: 0.829699	test: 0.820874
MAE train: 0.383044	val: 0.639434	test: 0.640212

Epoch: 61
Loss: 0.3285840779542923
RMSE train: 0.488636	val: 0.819344	test: 0.808912
MAE train: 0.382701	val: 0.634310	test: 0.629298

Epoch: 62
Loss: 0.33639736473560333
RMSE train: 0.485524	val: 0.832111	test: 0.822186
MAE train: 0.378140	val: 0.636934	test: 0.639670

Epoch: 63
Loss: 0.33196481466293337
RMSE train: 0.488702	val: 0.820866	test: 0.813889
MAE train: 0.380775	val: 0.631204	test: 0.634204

Epoch: 64
Loss: 0.3306801527738571
RMSE train: 0.507932	val: 0.816332	test: 0.812629
MAE train: 0.399834	val: 0.637153	test: 0.637339

Epoch: 65
Loss: 0.33479987680912016
RMSE train: 0.497892	val: 0.826583	test: 0.815126
MAE train: 0.390901	val: 0.642578	test: 0.632935

Epoch: 66
Loss: 0.3234567761421204
RMSE train: 0.501776	val: 0.835743	test: 0.822570
MAE train: 0.394549	val: 0.644685	test: 0.636838

Epoch: 67
Loss: 0.314400726556778
RMSE train: 0.475371	val: 0.817854	test: 0.804734
MAE train: 0.372077	val: 0.632203	test: 0.621180

Epoch: 68
Loss: 0.30870994329452517
RMSE train: 0.482114	val: 0.844670	test: 0.821126
MAE train: 0.376349	val: 0.649263	test: 0.639541

Epoch: 69
Loss: 0.3097241550683975
RMSE train: 0.477953	val: 0.834994	test: 0.824212
MAE train: 0.367175	val: 0.641830	test: 0.635174

Epoch: 70
Loss: 0.3305420845746994
RMSE train: 0.491832	val: 0.834225	test: 0.820164
MAE train: 0.386486	val: 0.643970	test: 0.639856

Epoch: 71
Loss: 0.3176774948835373
RMSE train: 0.466121	val: 0.828423	test: 0.808011
MAE train: 0.361948	val: 0.633199	test: 0.630149

Epoch: 72
Loss: 0.32068770527839663
RMSE train: 0.501395	val: 0.852405	test: 0.836095
MAE train: 0.392080	val: 0.659649	test: 0.651463

Epoch: 73
Loss: 0.31157168447971345
RMSE train: 0.492656	val: 0.830301	test: 0.823060
MAE train: 0.389922	val: 0.645631	test: 0.640251

Epoch: 74
Loss: 0.3126311138272285
RMSE train: 0.473227	val: 0.839795	test: 0.828533
MAE train: 0.368791	val: 0.648481	test: 0.644416

Epoch: 75
Loss: 0.3012867420911789
RMSE train: 0.469200	val: 0.846275	test: 0.817198
MAE train: 0.363551	val: 0.644465	test: 0.635668

Epoch: 76
Loss: 0.3204712927341461
RMSE train: 0.503644	val: 0.846590	test: 0.827427
MAE train: 0.390924	val: 0.645881	test: 0.647580

Epoch: 77
Loss: 0.2992826342582703
RMSE train: 0.479360	val: 0.857430	test: 0.839717
MAE train: 0.372294	val: 0.652588	test: 0.651258

Epoch: 78
Loss: 0.30224313139915465
RMSE train: 0.466773	val: 0.835849	test: 0.813739
MAE train: 0.365965	val: 0.642299	test: 0.626116

Epoch: 79
Loss: 0.29070783257484434
RMSE train: 0.483055	val: 0.844751	test: 0.811781
MAE train: 0.381572	val: 0.648025	test: 0.631979

Epoch: 80
Loss: 0.3152515828609467
RMSE train: 0.466878	val: 0.826008	test: 0.806710
MAE train: 0.364223	val: 0.627532	test: 0.624839

Epoch: 81
Loss: 0.29581703841686247
RMSE train: 0.459545	val: 0.831537	test: 0.820521
MAE train: 0.354888	val: 0.636077	test: 0.630010

Epoch: 82
Loss: 0.2957919627428055
RMSE train: 0.466450	val: 0.836003	test: 0.819177
MAE train: 0.365244	val: 0.638342	test: 0.633273

Epoch: 83
Loss: 0.2966930031776428
RMSE train: 0.480387	val: 0.829585	test: 0.814535

Epoch: 23
Loss: 0.5199600184957186
RMSE train: 0.631301	val: 0.852688	test: 0.792835
MAE train: 0.495218	val: 0.648341	test: 0.631962

Epoch: 24
Loss: 0.5184502129753431
RMSE train: 0.646124	val: 0.885527	test: 0.814481
MAE train: 0.502837	val: 0.674273	test: 0.651313

Epoch: 25
Loss: 0.5310187687476476
RMSE train: 0.633294	val: 0.873105	test: 0.805610
MAE train: 0.495499	val: 0.664062	test: 0.640614

Epoch: 26
Loss: 0.4852200870712598
RMSE train: 0.626150	val: 0.880093	test: 0.807540
MAE train: 0.489904	val: 0.669844	test: 0.643379

Epoch: 27
Loss: 0.5079237495859464
RMSE train: 0.627067	val: 0.872338	test: 0.782982
MAE train: 0.490001	val: 0.657049	test: 0.624915

Epoch: 28
Loss: 0.49392075339953107
RMSE train: 0.623150	val: 0.874385	test: 0.812204
MAE train: 0.482183	val: 0.660685	test: 0.649131

Epoch: 29
Loss: 0.5048888847231865
RMSE train: 0.625311	val: 0.862507	test: 0.809565
MAE train: 0.488855	val: 0.657483	test: 0.638089

Epoch: 30
Loss: 0.4715070500969887
RMSE train: 0.594445	val: 0.845613	test: 0.784127
MAE train: 0.465825	val: 0.639852	test: 0.623004

Epoch: 31
Loss: 0.4684717282652855
RMSE train: 0.617711	val: 0.872968	test: 0.809055
MAE train: 0.477978	val: 0.654777	test: 0.644098

Epoch: 32
Loss: 0.4722066968679428
RMSE train: 0.599833	val: 0.867900	test: 0.815544
MAE train: 0.465147	val: 0.656579	test: 0.643550

Epoch: 33
Loss: 0.45477553457021713
RMSE train: 0.600611	val: 0.847737	test: 0.804273
MAE train: 0.470101	val: 0.643638	test: 0.635799

Epoch: 34
Loss: 0.45710519204537076
RMSE train: 0.588186	val: 0.852722	test: 0.801710
MAE train: 0.460122	val: 0.645533	test: 0.636578

Epoch: 35
Loss: 0.43241266906261444
RMSE train: 0.572516	val: 0.832787	test: 0.794160
MAE train: 0.444955	val: 0.626079	test: 0.620795

Epoch: 36
Loss: 0.4391140465935071
RMSE train: 0.579774	val: 0.828299	test: 0.788144
MAE train: 0.453033	val: 0.627692	test: 0.624481

Epoch: 37
Loss: 0.4251326049367587
RMSE train: 0.563151	val: 0.825199	test: 0.777992
MAE train: 0.437802	val: 0.620322	test: 0.620292

Epoch: 38
Loss: 0.4301058327158292
RMSE train: 0.612382	val: 0.860608	test: 0.803097
MAE train: 0.479144	val: 0.650207	test: 0.636699

Epoch: 39
Loss: 0.4166969954967499
RMSE train: 0.573935	val: 0.838661	test: 0.784217
MAE train: 0.448701	val: 0.635395	test: 0.621279

Epoch: 40
Loss: 0.4096095412969589
RMSE train: 0.554696	val: 0.822459	test: 0.773760
MAE train: 0.432684	val: 0.617880	test: 0.611250

Epoch: 41
Loss: 0.39736446738243103
RMSE train: 0.599437	val: 0.864343	test: 0.833586
MAE train: 0.468813	val: 0.653870	test: 0.666281

Epoch: 42
Loss: 0.40611408402522403
RMSE train: 0.571706	val: 0.830135	test: 0.791383
MAE train: 0.448288	val: 0.624473	test: 0.620324

Epoch: 43
Loss: 0.40494882812102634
RMSE train: 0.595519	val: 0.872776	test: 0.830340
MAE train: 0.462100	val: 0.659821	test: 0.661814

Epoch: 44
Loss: 0.4165520096818606
RMSE train: 0.572202	val: 0.825313	test: 0.777939
MAE train: 0.446877	val: 0.621744	test: 0.611610

Epoch: 45
Loss: 0.3979557479421298
RMSE train: 0.552498	val: 0.813901	test: 0.766743
MAE train: 0.431573	val: 0.608829	test: 0.609172

Epoch: 46
Loss: 0.400627925992012
RMSE train: 0.552969	val: 0.816567	test: 0.779320
MAE train: 0.429841	val: 0.615972	test: 0.617876

Epoch: 47
Loss: 0.39454195151726407
RMSE train: 0.548066	val: 0.813256	test: 0.766703
MAE train: 0.424932	val: 0.612177	test: 0.602903

Epoch: 48
Loss: 0.3902190377314885
RMSE train: 0.535012	val: 0.817311	test: 0.768640
MAE train: 0.414639	val: 0.611731	test: 0.612525

Epoch: 49
Loss: 0.38830504566431046
RMSE train: 0.545803	val: 0.814956	test: 0.758092
MAE train: 0.424829	val: 0.613882	test: 0.597408

Epoch: 50
Loss: 0.38462528089682263
RMSE train: 0.558155	val: 0.826890	test: 0.786617
MAE train: 0.437636	val: 0.625650	test: 0.622815

Epoch: 51
Loss: 0.39364690085252124
RMSE train: 0.529701	val: 0.814917	test: 0.771271
MAE train: 0.408838	val: 0.607590	test: 0.607914

Epoch: 52
Loss: 0.38008715212345123
RMSE train: 0.534136	val: 0.823082	test: 0.770643
MAE train: 0.411605	val: 0.615528	test: 0.613532

Epoch: 53
Loss: 0.37660394112269086
RMSE train: 0.556901	val: 0.822113	test: 0.771622
MAE train: 0.439869	val: 0.627049	test: 0.606771

Epoch: 54
Loss: 0.364140585064888
RMSE train: 0.522970	val: 0.804631	test: 0.765568
MAE train: 0.405288	val: 0.607714	test: 0.603524

Epoch: 55
Loss: 0.3647284557422002
RMSE train: 0.528353	val: 0.807024	test: 0.778599
MAE train: 0.411764	val: 0.612981	test: 0.614036

Epoch: 56
Loss: 0.38087911655505496
RMSE train: 0.541489	val: 0.807734	test: 0.779272
MAE train: 0.424121	val: 0.614225	test: 0.613389

Epoch: 57
Loss: 0.3644285673896472
RMSE train: 0.532392	val: 0.811377	test: 0.763993
MAE train: 0.416298	val: 0.611500	test: 0.604049

Epoch: 58
Loss: 0.3511175662279129
RMSE train: 0.500857	val: 0.797976	test: 0.765430
MAE train: 0.387103	val: 0.594973	test: 0.603953

Epoch: 59
Loss: 0.35574544966220856
RMSE train: 0.518444	val: 0.804828	test: 0.751775
MAE train: 0.399741	val: 0.596913	test: 0.596868

Epoch: 60
Loss: 0.34065573414166767
RMSE train: 0.507344	val: 0.806528	test: 0.779747
MAE train: 0.392779	val: 0.608814	test: 0.618304

Epoch: 61
Loss: 0.3504246001442273
RMSE train: 0.510121	val: 0.801210	test: 0.761922
MAE train: 0.394793	val: 0.600031	test: 0.604304

Epoch: 62
Loss: 0.33160950740178424
RMSE train: 0.504355	val: 0.783642	test: 0.755544
MAE train: 0.389435	val: 0.591319	test: 0.597828

Epoch: 63
Loss: 0.3367455452680588
RMSE train: 0.514402	val: 0.787077	test: 0.763632
MAE train: 0.402779	val: 0.595383	test: 0.596154

Epoch: 64
Loss: 0.34317879875500995
RMSE train: 0.506678	val: 0.786706	test: 0.757652
MAE train: 0.393404	val: 0.590034	test: 0.596300

Epoch: 65
Loss: 0.3378564715385437
RMSE train: 0.512007	val: 0.792624	test: 0.757811
MAE train: 0.399380	val: 0.595763	test: 0.597378

Epoch: 66
Loss: 0.3239225596189499
RMSE train: 0.495923	val: 0.788896	test: 0.759132
MAE train: 0.384365	val: 0.588142	test: 0.596870

Epoch: 67
Loss: 0.33558129022518796
RMSE train: 0.508581	val: 0.805979	test: 0.776056
MAE train: 0.394519	val: 0.602554	test: 0.611604

Epoch: 68
Loss: 0.3286065186063449
RMSE train: 0.499660	val: 0.791659	test: 0.771020
MAE train: 0.386443	val: 0.589203	test: 0.605339

Epoch: 69
Loss: 0.3097998797893524
RMSE train: 0.513547	val: 0.793848	test: 0.775480
MAE train: 0.395717	val: 0.591818	test: 0.604344

Epoch: 70
Loss: 0.3190630575021108
RMSE train: 0.504763	val: 0.792835	test: 0.762585
MAE train: 0.390511	val: 0.595196	test: 0.603477

Epoch: 71
Loss: 0.31368665893872577
RMSE train: 0.492665	val: 0.785649	test: 0.757955
MAE train: 0.383601	val: 0.588246	test: 0.596654

Epoch: 72
Loss: 0.33481887976328534
RMSE train: 0.502365	val: 0.797882	test: 0.764057
MAE train: 0.390382	val: 0.597501	test: 0.607194

Epoch: 73
Loss: 0.31812189767758053
RMSE train: 0.515425	val: 0.811342	test: 0.781631
MAE train: 0.402864	val: 0.609187	test: 0.619282

Epoch: 74
Loss: 0.3059826170404752
RMSE train: 0.479905	val: 0.787989	test: 0.759208
MAE train: 0.370623	val: 0.584638	test: 0.598746

Epoch: 75
Loss: 0.31311725576718646
RMSE train: 0.491245	val: 0.795729	test: 0.759122
MAE train: 0.383838	val: 0.595597	test: 0.599117

Epoch: 76
Loss: 0.3008580853541692
RMSE train: 0.474101	val: 0.808043	test: 0.759641
MAE train: 0.365415	val: 0.593799	test: 0.596013

Epoch: 77
Loss: 0.30075809359550476
RMSE train: 0.483059	val: 0.800654	test: 0.755158
MAE train: 0.375409	val: 0.593430	test: 0.594541

Epoch: 78
Loss: 0.2977989415327708
RMSE train: 0.489810	val: 0.812782	test: 0.780041
MAE train: 0.381069	val: 0.607161	test: 0.614497

Epoch: 79
Loss: 0.30354652802149457
RMSE train: 0.487843	val: 0.796090	test: 0.754701
MAE train: 0.375234	val: 0.590553	test: 0.598666

Epoch: 80
Loss: 0.29331005613009137
RMSE train: 0.505627	val: 0.831598	test: 0.768794
MAE train: 0.393163	val: 0.616175	test: 0.607464

Epoch: 81
Loss: 0.2880658432841301
RMSE train: 0.465067	val: 0.793498	test: 0.749344
MAE train: 0.359538	val: 0.584659	test: 0.583733

Epoch: 82
Loss: 0.3056822021802266
RMSE train: 0.487941	val: 0.810516	test: 0.781381
MAE train: 0.377797	val: 0.603603	test: 0.611660

Epoch: 83
Loss: 0.2997134377559026
RMSE train: 0.477420	val: 0.799984	test: 0.759882

Epoch: 23
Loss: 0.533122663696607
RMSE train: 0.639042	val: 0.879219	test: 0.812837
MAE train: 0.494742	val: 0.654704	test: 0.644364

Epoch: 24
Loss: 0.5131322965025902
RMSE train: 0.655771	val: 0.886654	test: 0.840815
MAE train: 0.513073	val: 0.675338	test: 0.669670

Epoch: 25
Loss: 0.5444600358605385
RMSE train: 0.633704	val: 0.883994	test: 0.809414
MAE train: 0.493390	val: 0.669834	test: 0.648723

Epoch: 26
Loss: 0.5168757140636444
RMSE train: 0.652150	val: 0.901113	test: 0.818676
MAE train: 0.506020	val: 0.678781	test: 0.651901

Epoch: 27
Loss: 0.504964237411817
RMSE train: 0.643025	val: 0.873771	test: 0.821477
MAE train: 0.500025	val: 0.660206	test: 0.650310

Epoch: 28
Loss: 0.4896498570839564
RMSE train: 0.628747	val: 0.872763	test: 0.811846
MAE train: 0.493158	val: 0.658781	test: 0.649821

Epoch: 29
Loss: 0.4814470161994298
RMSE train: 0.613940	val: 0.868199	test: 0.800216
MAE train: 0.481067	val: 0.654834	test: 0.634483

Epoch: 30
Loss: 0.4802649145325025
RMSE train: 0.646369	val: 0.892713	test: 0.827006
MAE train: 0.503217	val: 0.671618	test: 0.662450

Epoch: 31
Loss: 0.4766601746280988
RMSE train: 0.619913	val: 0.883821	test: 0.812988
MAE train: 0.483631	val: 0.664482	test: 0.648850

Epoch: 32
Loss: 0.4552377114693324
RMSE train: 0.614839	val: 0.867162	test: 0.802038
MAE train: 0.484957	val: 0.659321	test: 0.639572

Epoch: 33
Loss: 0.4844910229245822
RMSE train: 0.615382	val: 0.871235	test: 0.811360
MAE train: 0.482230	val: 0.660054	test: 0.650095

Epoch: 34
Loss: 0.46993861595789593
RMSE train: 0.618004	val: 0.865836	test: 0.818589
MAE train: 0.484031	val: 0.656437	test: 0.651066

Epoch: 35
Loss: 0.4708341881632805
RMSE train: 0.625146	val: 0.874548	test: 0.825845
MAE train: 0.486787	val: 0.658613	test: 0.663533

Epoch: 36
Loss: 0.46227190643548965
RMSE train: 0.604335	val: 0.862430	test: 0.795325
MAE train: 0.472066	val: 0.650454	test: 0.631153

Epoch: 37
Loss: 0.4390614827473958
RMSE train: 0.602558	val: 0.853040	test: 0.793484
MAE train: 0.475330	val: 0.649058	test: 0.631406

Epoch: 38
Loss: 0.4459306498368581
RMSE train: 0.582388	val: 0.855581	test: 0.781512
MAE train: 0.453479	val: 0.642702	test: 0.620546

Epoch: 39
Loss: 0.4405457427104314
RMSE train: 0.622313	val: 0.873225	test: 0.807540
MAE train: 0.486881	val: 0.654501	test: 0.646935

Epoch: 40
Loss: 0.42884987344344455
RMSE train: 0.588304	val: 0.857197	test: 0.816553
MAE train: 0.457042	val: 0.639456	test: 0.644927

Epoch: 41
Loss: 0.4410201385617256
RMSE train: 0.593126	val: 0.874502	test: 0.807003
MAE train: 0.461422	val: 0.648236	test: 0.639399

Epoch: 42
Loss: 0.420761617521445
RMSE train: 0.587628	val: 0.863087	test: 0.790895
MAE train: 0.462209	val: 0.645526	test: 0.630449

Epoch: 43
Loss: 0.41955171277125675
RMSE train: 0.572146	val: 0.852267	test: 0.785299
MAE train: 0.445995	val: 0.635813	test: 0.626453

Epoch: 44
Loss: 0.4000181059042613
RMSE train: 0.570080	val: 0.857293	test: 0.786797
MAE train: 0.442809	val: 0.637438	test: 0.628014

Epoch: 45
Loss: 0.40433671573797864
RMSE train: 0.565270	val: 0.854076	test: 0.783388
MAE train: 0.438431	val: 0.637479	test: 0.623046

Epoch: 46
Loss: 0.4162554343541463
RMSE train: 0.561541	val: 0.850273	test: 0.791687
MAE train: 0.437219	val: 0.638827	test: 0.626429

Epoch: 47
Loss: 0.3896624967455864
RMSE train: 0.554378	val: 0.832699	test: 0.781149
MAE train: 0.430444	val: 0.624115	test: 0.618041

Epoch: 48
Loss: 0.3940234010418256
RMSE train: 0.567441	val: 0.851114	test: 0.793644
MAE train: 0.440534	val: 0.638594	test: 0.630322

Epoch: 49
Loss: 0.3917410671710968
RMSE train: 0.550712	val: 0.840352	test: 0.775853
MAE train: 0.429723	val: 0.632740	test: 0.613758

Epoch: 50
Loss: 0.3761862516403198
RMSE train: 0.554470	val: 0.837913	test: 0.777786
MAE train: 0.430845	val: 0.625020	test: 0.615325

Epoch: 51
Loss: 0.3835971380273501
RMSE train: 0.559540	val: 0.840069	test: 0.771149
MAE train: 0.435877	val: 0.624330	test: 0.612374

Epoch: 52
Loss: 0.3651345819234848
RMSE train: 0.551695	val: 0.841340	test: 0.775584
MAE train: 0.429800	val: 0.625041	test: 0.612358

Epoch: 53
Loss: 0.3801776071389516
RMSE train: 0.551220	val: 0.837660	test: 0.770852
MAE train: 0.432446	val: 0.630303	test: 0.610262

Epoch: 54
Loss: 0.36830442150433856
RMSE train: 0.557713	val: 0.835286	test: 0.775368
MAE train: 0.437403	val: 0.631489	test: 0.615170

Epoch: 55
Loss: 0.36387090384960175
RMSE train: 0.563571	val: 0.837991	test: 0.779529
MAE train: 0.441960	val: 0.638046	test: 0.622361

Epoch: 56
Loss: 0.35855358093976974
RMSE train: 0.547654	val: 0.841423	test: 0.784408
MAE train: 0.427005	val: 0.626986	test: 0.622264

Epoch: 57
Loss: 0.357209712266922
RMSE train: 0.528587	val: 0.830081	test: 0.772962
MAE train: 0.411690	val: 0.615893	test: 0.609957

Epoch: 58
Loss: 0.35260973622401554
RMSE train: 0.532562	val: 0.817471	test: 0.771052
MAE train: 0.417154	val: 0.614323	test: 0.606075

Epoch: 59
Loss: 0.3626820892095566
RMSE train: 0.546857	val: 0.841360	test: 0.780469
MAE train: 0.429174	val: 0.631579	test: 0.618973

Epoch: 60
Loss: 0.34379499902327854
RMSE train: 0.528607	val: 0.831010	test: 0.765219
MAE train: 0.413266	val: 0.617518	test: 0.600842

Epoch: 61
Loss: 0.3488433261712392
RMSE train: 0.539817	val: 0.817157	test: 0.761656
MAE train: 0.422396	val: 0.615094	test: 0.601384

Epoch: 62
Loss: 0.34538720548152924
RMSE train: 0.545183	val: 0.836418	test: 0.776850
MAE train: 0.425696	val: 0.628910	test: 0.614251

Epoch: 63
Loss: 0.3531294787923495
RMSE train: 0.541794	val: 0.827530	test: 0.767226
MAE train: 0.424687	val: 0.621398	test: 0.612395

Epoch: 64
Loss: 0.34934203575054806
RMSE train: 0.525357	val: 0.822290	test: 0.756347
MAE train: 0.413100	val: 0.621074	test: 0.599380

Epoch: 65
Loss: 0.3415161843101184
RMSE train: 0.509949	val: 0.817754	test: 0.755573
MAE train: 0.397421	val: 0.615041	test: 0.596747

Epoch: 66
Loss: 0.3376201465725899
RMSE train: 0.510716	val: 0.819109	test: 0.754581
MAE train: 0.396397	val: 0.615544	test: 0.595326

Epoch: 67
Loss: 0.331590602795283
RMSE train: 0.527528	val: 0.822509	test: 0.762106
MAE train: 0.412955	val: 0.620376	test: 0.604867

Epoch: 68
Loss: 0.3185652121901512
RMSE train: 0.532076	val: 0.808682	test: 0.771194
MAE train: 0.414751	val: 0.609594	test: 0.612109

Epoch: 69
Loss: 0.33843099574248
RMSE train: 0.515880	val: 0.808988	test: 0.759017
MAE train: 0.404446	val: 0.609615	test: 0.603947

Epoch: 70
Loss: 0.32639195024967194
RMSE train: 0.502284	val: 0.807969	test: 0.752782
MAE train: 0.389705	val: 0.600791	test: 0.593516

Epoch: 71
Loss: 0.32767344266176224
RMSE train: 0.530430	val: 0.819465	test: 0.755810
MAE train: 0.414869	val: 0.614182	test: 0.601725

Epoch: 72
Loss: 0.3048244280119737
RMSE train: 0.517601	val: 0.808550	test: 0.753087
MAE train: 0.401825	val: 0.615456	test: 0.601002

Epoch: 73
Loss: 0.31961362560590106
RMSE train: 0.537871	val: 0.824834	test: 0.767792
MAE train: 0.420047	val: 0.627127	test: 0.607242

Epoch: 74
Loss: 0.3317112773656845
RMSE train: 0.524761	val: 0.802356	test: 0.755490
MAE train: 0.411905	val: 0.613813	test: 0.604220

Epoch: 75
Loss: 0.3291034897168477
RMSE train: 0.513927	val: 0.802111	test: 0.765785
MAE train: 0.401396	val: 0.609106	test: 0.608177

Epoch: 76
Loss: 0.3144971194366614
RMSE train: 0.510682	val: 0.815408	test: 0.759147
MAE train: 0.400278	val: 0.610554	test: 0.600111

Epoch: 77
Loss: 0.3092558830976486
RMSE train: 0.523591	val: 0.819155	test: 0.763802
MAE train: 0.408562	val: 0.613231	test: 0.608862

Epoch: 78
Loss: 0.32474196950594586
RMSE train: 0.519694	val: 0.806991	test: 0.759173
MAE train: 0.404944	val: 0.610911	test: 0.600318

Epoch: 79
Loss: 0.3257952307661374
RMSE train: 0.512350	val: 0.808218	test: 0.767325
MAE train: 0.396202	val: 0.611837	test: 0.605338

Epoch: 80
Loss: 0.3179398278395335
RMSE train: 0.522313	val: 0.822501	test: 0.771826
MAE train: 0.406873	val: 0.618787	test: 0.615174

Epoch: 81
Loss: 0.30694163342316944
RMSE train: 0.506086	val: 0.795264	test: 0.746326
MAE train: 0.393398	val: 0.604320	test: 0.596266

Epoch: 82
Loss: 0.2965709740916888
RMSE train: 0.500869	val: 0.817232	test: 0.765172
MAE train: 0.388973	val: 0.614090	test: 0.605959

Epoch: 83
Loss: 0.29441673308610916
RMSE train: 0.478577	val: 0.804756	test: 0.754312

Epoch: 23
Loss: 0.4954021672407786
RMSE train: 0.647984	val: 0.881763	test: 0.810731
MAE train: 0.507669	val: 0.672816	test: 0.652682

Epoch: 24
Loss: 0.503893700738748
RMSE train: 0.627816	val: 0.855067	test: 0.786846
MAE train: 0.486878	val: 0.646700	test: 0.630578

Epoch: 25
Loss: 0.4970708067218463
RMSE train: 0.630599	val: 0.866974	test: 0.799933
MAE train: 0.492134	val: 0.663642	test: 0.638425

Epoch: 26
Loss: 0.5092489942908287
RMSE train: 0.644406	val: 0.875141	test: 0.824893
MAE train: 0.498850	val: 0.665142	test: 0.663697

Epoch: 27
Loss: 0.5165415480732918
RMSE train: 0.633722	val: 0.872150	test: 0.814670
MAE train: 0.493571	val: 0.657423	test: 0.654090

Epoch: 28
Loss: 0.49060505380233127
RMSE train: 0.632664	val: 0.858059	test: 0.816831
MAE train: 0.492635	val: 0.649621	test: 0.654092

Epoch: 29
Loss: 0.49657632162173587
RMSE train: 0.622917	val: 0.858041	test: 0.802494
MAE train: 0.485503	val: 0.649158	test: 0.637567

Epoch: 30
Loss: 0.4833398461341858
RMSE train: 0.608365	val: 0.843995	test: 0.789782
MAE train: 0.477174	val: 0.637611	test: 0.626774

Epoch: 31
Loss: 0.4733713020881017
RMSE train: 0.617001	val: 0.853833	test: 0.792867
MAE train: 0.484075	val: 0.640107	test: 0.633430

Epoch: 32
Loss: 0.46846261868874234
RMSE train: 0.596964	val: 0.848283	test: 0.792000
MAE train: 0.465850	val: 0.639684	test: 0.631125

Epoch: 33
Loss: 0.43412380168835324
RMSE train: 0.605160	val: 0.859985	test: 0.793362
MAE train: 0.471265	val: 0.642039	test: 0.636786

Epoch: 34
Loss: 0.4498695284128189
RMSE train: 0.598548	val: 0.838321	test: 0.787514
MAE train: 0.470324	val: 0.627293	test: 0.629106

Epoch: 35
Loss: 0.4331172928214073
RMSE train: 0.590811	val: 0.848224	test: 0.791911
MAE train: 0.463898	val: 0.630029	test: 0.625633

Epoch: 36
Loss: 0.44729115317265195
RMSE train: 0.613942	val: 0.852637	test: 0.797784
MAE train: 0.478781	val: 0.644197	test: 0.636899

Epoch: 37
Loss: 0.4481925119956334
RMSE train: 0.585053	val: 0.858048	test: 0.807215
MAE train: 0.454993	val: 0.640960	test: 0.639501

Epoch: 38
Loss: 0.4220130418737729
RMSE train: 0.583602	val: 0.852741	test: 0.802071
MAE train: 0.453773	val: 0.638914	test: 0.645515

Epoch: 39
Loss: 0.43131717542807263
RMSE train: 0.595039	val: 0.834815	test: 0.785499
MAE train: 0.466380	val: 0.631106	test: 0.626141

Epoch: 40
Loss: 0.4207633708914121
RMSE train: 0.598638	val: 0.868015	test: 0.810978
MAE train: 0.462494	val: 0.649830	test: 0.648706

Epoch: 41
Loss: 0.42304090162118274
RMSE train: 0.559898	val: 0.826889	test: 0.778698
MAE train: 0.435727	val: 0.621952	test: 0.618683

Epoch: 42
Loss: 0.4066495820879936
RMSE train: 0.575033	val: 0.832539	test: 0.777410
MAE train: 0.448801	val: 0.626829	test: 0.617859

Epoch: 43
Loss: 0.4180986185868581
RMSE train: 0.586123	val: 0.844744	test: 0.781206
MAE train: 0.454294	val: 0.638148	test: 0.623800

Epoch: 44
Loss: 0.4041583389043808
RMSE train: 0.583786	val: 0.834049	test: 0.794981
MAE train: 0.451352	val: 0.639851	test: 0.639695

Epoch: 45
Loss: 0.42241964489221573
RMSE train: 0.582371	val: 0.821304	test: 0.781447
MAE train: 0.449167	val: 0.624472	test: 0.624380

Epoch: 46
Loss: 0.40356771399577457
RMSE train: 0.551914	val: 0.817669	test: 0.782602
MAE train: 0.429557	val: 0.617976	test: 0.614991

Epoch: 47
Loss: 0.4034053732951482
RMSE train: 0.567700	val: 0.839389	test: 0.810582
MAE train: 0.438525	val: 0.636307	test: 0.645618

Epoch: 48
Loss: 0.38804012537002563
RMSE train: 0.569816	val: 0.842557	test: 0.793223
MAE train: 0.440939	val: 0.633777	test: 0.627760

Epoch: 49
Loss: 0.40264368057250977
RMSE train: 0.560628	val: 0.813022	test: 0.770562
MAE train: 0.438537	val: 0.612505	test: 0.612345

Epoch: 50
Loss: 0.3918588285644849
RMSE train: 0.583741	val: 0.843090	test: 0.787614
MAE train: 0.452500	val: 0.638948	test: 0.627125

Epoch: 51
Loss: 0.4055919423699379
RMSE train: 0.578586	val: 0.845735	test: 0.795269
MAE train: 0.448406	val: 0.640914	test: 0.631336

Epoch: 52
Loss: 0.38845231880744296
RMSE train: 0.541409	val: 0.815511	test: 0.767856
MAE train: 0.420068	val: 0.610748	test: 0.600513

Epoch: 53
Loss: 0.383087195456028
RMSE train: 0.551431	val: 0.831493	test: 0.784942
MAE train: 0.428581	val: 0.627907	test: 0.623895

Epoch: 54
Loss: 0.359654076397419
RMSE train: 0.552730	val: 0.841949	test: 0.775624
MAE train: 0.429555	val: 0.626949	test: 0.609115

Epoch: 55
Loss: 0.3755887597799301
RMSE train: 0.567327	val: 0.839790	test: 0.782252
MAE train: 0.441370	val: 0.633783	test: 0.619378

Epoch: 56
Loss: 0.3539293458064397
RMSE train: 0.552378	val: 0.824095	test: 0.767031
MAE train: 0.429806	val: 0.620960	test: 0.606105

Epoch: 57
Loss: 0.3567338561018308
RMSE train: 0.555513	val: 0.828904	test: 0.789831
MAE train: 0.432809	val: 0.628507	test: 0.627102

Epoch: 58
Loss: 0.3450842748085658
RMSE train: 0.522444	val: 0.804370	test: 0.761500
MAE train: 0.405890	val: 0.599178	test: 0.595876

Epoch: 59
Loss: 0.34172604978084564
RMSE train: 0.532264	val: 0.821971	test: 0.763523
MAE train: 0.413863	val: 0.619019	test: 0.609234

Epoch: 60
Loss: 0.3471323698759079
RMSE train: 0.529203	val: 0.815795	test: 0.765452
MAE train: 0.412390	val: 0.612206	test: 0.605519

Epoch: 61
Loss: 0.3548065821329753
RMSE train: 0.537526	val: 0.846814	test: 0.777885
MAE train: 0.417096	val: 0.633014	test: 0.618760

Epoch: 62
Loss: 0.3485969007015228
RMSE train: 0.516919	val: 0.823675	test: 0.753746
MAE train: 0.403102	val: 0.605639	test: 0.596391

Epoch: 63
Loss: 0.3465626041094462
RMSE train: 0.529041	val: 0.828754	test: 0.779559
MAE train: 0.410732	val: 0.623908	test: 0.621605

Epoch: 64
Loss: 0.3449469059705734
RMSE train: 0.554281	val: 0.827022	test: 0.796420
MAE train: 0.430011	val: 0.618528	test: 0.621579

Epoch: 65
Loss: 0.3701542417208354
RMSE train: 0.520709	val: 0.804675	test: 0.770869
MAE train: 0.405232	val: 0.599574	test: 0.609411

Epoch: 66
Loss: 0.3425770327448845
RMSE train: 0.551631	val: 0.846248	test: 0.792742
MAE train: 0.425988	val: 0.633793	test: 0.630123

Epoch: 67
Loss: 0.3231583734353383
RMSE train: 0.506862	val: 0.818201	test: 0.778455
MAE train: 0.393478	val: 0.613708	test: 0.615192

Epoch: 68
Loss: 0.33641908317804337
RMSE train: 0.513891	val: 0.803474	test: 0.767464
MAE train: 0.399297	val: 0.600263	test: 0.602626

Epoch: 69
Loss: 0.3251199002067248
RMSE train: 0.531954	val: 0.832912	test: 0.783631
MAE train: 0.412437	val: 0.627147	test: 0.622230

Epoch: 70
Loss: 0.3229913040995598
RMSE train: 0.537064	val: 0.813912	test: 0.782477
MAE train: 0.414310	val: 0.614270	test: 0.614256

Epoch: 71
Loss: 0.3172783429423968
RMSE train: 0.519300	val: 0.818454	test: 0.776875
MAE train: 0.402517	val: 0.612662	test: 0.616522

Epoch: 72
Loss: 0.3212803080677986
RMSE train: 0.497314	val: 0.808529	test: 0.757175
MAE train: 0.385830	val: 0.597899	test: 0.597158

Epoch: 73
Loss: 0.3102673639853795
RMSE train: 0.520189	val: 0.828241	test: 0.775491
MAE train: 0.405562	val: 0.618262	test: 0.613553

Epoch: 74
Loss: 0.3302484278877576
RMSE train: 0.509518	val: 0.791160	test: 0.761202
MAE train: 0.393893	val: 0.596094	test: 0.598943

Epoch: 75
Loss: 0.31164483229319256
RMSE train: 0.517259	val: 0.811953	test: 0.775496
MAE train: 0.394305	val: 0.611259	test: 0.604425

Epoch: 76
Loss: 0.3229975750048955
RMSE train: 0.514999	val: 0.818966	test: 0.787986
MAE train: 0.401677	val: 0.616639	test: 0.618718

Epoch: 77
Loss: 0.3129287275175254
RMSE train: 0.524048	val: 0.825682	test: 0.781807
MAE train: 0.405151	val: 0.621398	test: 0.615649

Epoch: 78
Loss: 0.30146684497594833
RMSE train: 0.495787	val: 0.812033	test: 0.760164
MAE train: 0.385023	val: 0.607057	test: 0.595883

Epoch: 79
Loss: 0.3075706859429677
RMSE train: 0.489419	val: 0.785207	test: 0.742605
MAE train: 0.381659	val: 0.587797	test: 0.587419

Epoch: 80
Loss: 0.29858652999003726
RMSE train: 0.476061	val: 0.786934	test: 0.744110
MAE train: 0.371146	val: 0.585886	test: 0.589126

Epoch: 81
Loss: 0.30033568541208905
RMSE train: 0.493754	val: 0.823810	test: 0.770839
MAE train: 0.383303	val: 0.618216	test: 0.601801

Epoch: 82
Loss: 0.3002052716910839
RMSE train: 0.507576	val: 0.822646	test: 0.777890
MAE train: 0.395127	val: 0.616673	test: 0.609555

Epoch: 83
Loss: 0.29737624277671176
RMSE train: 0.514040	val: 0.822399	test: 0.775999

Epoch: 23
Loss: 0.5621203290564674
RMSE train: 0.691965	val: 0.871272	test: 0.823165
MAE train: 0.539310	val: 0.677980	test: 0.669228

Epoch: 24
Loss: 0.5627998624529157
RMSE train: 0.691226	val: 0.860166	test: 0.804567
MAE train: 0.534287	val: 0.659936	test: 0.654409

Epoch: 25
Loss: 0.5718531438282558
RMSE train: 0.692801	val: 0.860143	test: 0.810004
MAE train: 0.538762	val: 0.661791	test: 0.660882

Epoch: 26
Loss: 0.5257241023438317
RMSE train: 0.676268	val: 0.858938	test: 0.817641
MAE train: 0.525305	val: 0.659956	test: 0.657807

Epoch: 27
Loss: 0.5334555442844119
RMSE train: 0.709123	val: 0.901865	test: 0.844356
MAE train: 0.550221	val: 0.696760	test: 0.690615

Epoch: 28
Loss: 0.5385136902332306
RMSE train: 0.663826	val: 0.853567	test: 0.793575
MAE train: 0.517171	val: 0.660715	test: 0.642466

Epoch: 29
Loss: 0.5163119052137647
RMSE train: 0.642652	val: 0.822737	test: 0.778873
MAE train: 0.500667	val: 0.632165	test: 0.629428

Epoch: 30
Loss: 0.5158394660268512
RMSE train: 0.653188	val: 0.834441	test: 0.784813
MAE train: 0.509687	val: 0.641826	test: 0.634065

Epoch: 31
Loss: 0.4797554931470326
RMSE train: 0.651969	val: 0.813062	test: 0.794470
MAE train: 0.511194	val: 0.627109	test: 0.632465

Epoch: 32
Loss: 0.4771297723054886
RMSE train: 0.632665	val: 0.821161	test: 0.785503
MAE train: 0.493316	val: 0.632947	test: 0.631588

Epoch: 33
Loss: 0.49283122590609957
RMSE train: 0.644720	val: 0.833793	test: 0.794100
MAE train: 0.501742	val: 0.637562	test: 0.640027

Epoch: 34
Loss: 0.4750993869134358
RMSE train: 0.642129	val: 0.842221	test: 0.805830
MAE train: 0.498092	val: 0.639173	test: 0.644239

Epoch: 35
Loss: 0.4723002761602402
RMSE train: 0.635288	val: 0.833978	test: 0.795380
MAE train: 0.493560	val: 0.644944	test: 0.634499

Epoch: 36
Loss: 0.46875292701380594
RMSE train: 0.615124	val: 0.810882	test: 0.786844
MAE train: 0.479786	val: 0.617297	test: 0.628804

Epoch: 37
Loss: 0.4705937100308282
RMSE train: 0.616609	val: 0.806471	test: 0.792746
MAE train: 0.482103	val: 0.614396	test: 0.637002

Epoch: 38
Loss: 0.4520594945975712
RMSE train: 0.621419	val: 0.810472	test: 0.767461
MAE train: 0.483100	val: 0.612884	test: 0.615353

Epoch: 39
Loss: 0.4476403615304402
RMSE train: 0.606225	val: 0.805447	test: 0.773521
MAE train: 0.471001	val: 0.608677	test: 0.620493

Epoch: 40
Loss: 0.4355858117341995
RMSE train: 0.620380	val: 0.820998	test: 0.777513
MAE train: 0.479770	val: 0.625950	test: 0.626704

Epoch: 41
Loss: 0.4638952165842056
RMSE train: 0.648506	val: 0.837151	test: 0.800060
MAE train: 0.502707	val: 0.636169	test: 0.643172

Epoch: 42
Loss: 0.4311887558017458
RMSE train: 0.599802	val: 0.803571	test: 0.764886
MAE train: 0.463909	val: 0.613839	test: 0.613156

Epoch: 43
Loss: 0.4558672968830381
RMSE train: 0.607575	val: 0.804968	test: 0.768205
MAE train: 0.471294	val: 0.614279	test: 0.617452

Epoch: 44
Loss: 0.44086258113384247
RMSE train: 0.599875	val: 0.802358	test: 0.767365
MAE train: 0.467284	val: 0.610801	test: 0.609825

Epoch: 45
Loss: 0.44164745083877016
RMSE train: 0.592172	val: 0.807797	test: 0.772926
MAE train: 0.460584	val: 0.617274	test: 0.612327

Epoch: 46
Loss: 0.4271324234349387
RMSE train: 0.587976	val: 0.805506	test: 0.774797
MAE train: 0.454298	val: 0.613896	test: 0.609595

Epoch: 47
Loss: 0.4207919878619058
RMSE train: 0.604822	val: 0.809881	test: 0.766899
MAE train: 0.470216	val: 0.613534	test: 0.615486

Epoch: 48
Loss: 0.4273579397371837
RMSE train: 0.580321	val: 0.804391	test: 0.771242
MAE train: 0.449985	val: 0.612302	test: 0.613230

Epoch: 49
Loss: 0.4023532803569521
RMSE train: 0.613456	val: 0.821139	test: 0.793531
MAE train: 0.479216	val: 0.621606	test: 0.634661

Epoch: 50
Loss: 0.4132280158145087
RMSE train: 0.581950	val: 0.801142	test: 0.764459
MAE train: 0.449909	val: 0.614740	test: 0.606538

Epoch: 51
Loss: 0.40223261288234163
RMSE train: 0.569320	val: 0.792275	test: 0.771871
MAE train: 0.441553	val: 0.601115	test: 0.608619

Epoch: 52
Loss: 0.40622475317546297
RMSE train: 0.567974	val: 0.793685	test: 0.756305
MAE train: 0.441288	val: 0.606295	test: 0.598243

Epoch: 53
Loss: 0.4005690621478217
RMSE train: 0.574019	val: 0.797590	test: 0.768194
MAE train: 0.447249	val: 0.608153	test: 0.609737

Epoch: 54
Loss: 0.40195033167089733
RMSE train: 0.567028	val: 0.807296	test: 0.779823
MAE train: 0.440621	val: 0.617499	test: 0.613035

Epoch: 55
Loss: 0.40608356467315126
RMSE train: 0.583194	val: 0.796843	test: 0.770537
MAE train: 0.456228	val: 0.606674	test: 0.612337

Epoch: 56
Loss: 0.4089987256697246
RMSE train: 0.584784	val: 0.809539	test: 0.772844
MAE train: 0.457172	val: 0.617834	test: 0.614632

Epoch: 57
Loss: 0.3871980756521225
RMSE train: 0.567907	val: 0.776031	test: 0.769022
MAE train: 0.442028	val: 0.593450	test: 0.610943

Epoch: 58
Loss: 0.3808489441871643
RMSE train: 0.555095	val: 0.787108	test: 0.774910
MAE train: 0.431974	val: 0.601202	test: 0.608711

Epoch: 59
Loss: 0.35472539493015837
RMSE train: 0.578614	val: 0.791357	test: 0.782048
MAE train: 0.454747	val: 0.602078	test: 0.621039

Epoch: 60
Loss: 0.37998007237911224
RMSE train: 0.564465	val: 0.800971	test: 0.770564
MAE train: 0.441858	val: 0.615156	test: 0.607267

Epoch: 61
Loss: 0.37274802156857084
RMSE train: 0.549958	val: 0.796306	test: 0.762058
MAE train: 0.425917	val: 0.607302	test: 0.604169

Epoch: 62
Loss: 0.37296570411750246
RMSE train: 0.548407	val: 0.783955	test: 0.761919
MAE train: 0.424910	val: 0.597381	test: 0.599016

Epoch: 63
Loss: 0.37375910367284504
RMSE train: 0.564097	val: 0.799541	test: 0.761739
MAE train: 0.440324	val: 0.611376	test: 0.601461

Epoch: 64
Loss: 0.3585628590413502
RMSE train: 0.539547	val: 0.788120	test: 0.757703
MAE train: 0.419128	val: 0.603060	test: 0.597261

Epoch: 65
Loss: 0.35850515961647034
RMSE train: 0.535328	val: 0.780318	test: 0.749804
MAE train: 0.415040	val: 0.591094	test: 0.592960

Epoch: 66
Loss: 0.35749787730830057
RMSE train: 0.537178	val: 0.779892	test: 0.750194
MAE train: 0.418937	val: 0.596062	test: 0.588544

Epoch: 67
Loss: 0.35893838107585907
RMSE train: 0.540202	val: 0.776701	test: 0.755639
MAE train: 0.419703	val: 0.590141	test: 0.598662

Epoch: 68
Loss: 0.36235048515456064
RMSE train: 0.528677	val: 0.779097	test: 0.760434
MAE train: 0.408505	val: 0.589006	test: 0.595170

Epoch: 69
Loss: 0.3496723004749843
RMSE train: 0.557724	val: 0.792013	test: 0.770035
MAE train: 0.435252	val: 0.604707	test: 0.612960

Epoch: 70
Loss: 0.36560840904712677
RMSE train: 0.542469	val: 0.787924	test: 0.755415
MAE train: 0.423624	val: 0.599905	test: 0.591196

Epoch: 71
Loss: 0.34487889281341005
RMSE train: 0.561024	val: 0.806038	test: 0.771805
MAE train: 0.433345	val: 0.616256	test: 0.606186

Epoch: 72
Loss: 0.3330778969185693
RMSE train: 0.526012	val: 0.780583	test: 0.757736
MAE train: 0.405963	val: 0.591844	test: 0.591123

Epoch: 73
Loss: 0.3580037547009332
RMSE train: 0.530405	val: 0.780615	test: 0.747198
MAE train: 0.411948	val: 0.597691	test: 0.587466

Epoch: 74
Loss: 0.34586203524044584
RMSE train: 0.549906	val: 0.802455	test: 0.761764
MAE train: 0.427533	val: 0.617111	test: 0.598577

Epoch: 75
Loss: 0.3438343427010945
RMSE train: 0.556574	val: 0.785258	test: 0.769779
MAE train: 0.435505	val: 0.603612	test: 0.612458

Epoch: 76
Loss: 0.32875631749629974
RMSE train: 0.533594	val: 0.774078	test: 0.764217
MAE train: 0.413274	val: 0.598441	test: 0.603225

Epoch: 77
Loss: 0.3284919879266194
RMSE train: 0.518130	val: 0.782237	test: 0.758307
MAE train: 0.402378	val: 0.598964	test: 0.590487

Epoch: 78
Loss: 0.32685857798372
RMSE train: 0.537053	val: 0.798319	test: 0.768249
MAE train: 0.417705	val: 0.610486	test: 0.607167

Epoch: 79
Loss: 0.3159272276929447
RMSE train: 0.517237	val: 0.768129	test: 0.758828
MAE train: 0.402942	val: 0.587275	test: 0.599550

Epoch: 80
Loss: 0.3239516573292868
RMSE train: 0.520863	val: 0.785089	test: 0.758665
MAE train: 0.402112	val: 0.594979	test: 0.589185

Epoch: 81
Loss: 0.33612956745283945
RMSE train: 0.522501	val: 0.771821	test: 0.764489
MAE train: 0.408033	val: 0.587582	test: 0.603399

Epoch: 82
Loss: 0.3428354944501604
RMSE train: 0.526098	val: 0.770131	test: 0.762378
MAE train: 0.410878	val: 0.592675	test: 0.595731

Epoch: 83
Loss: 0.330625308411462
RMSE train: 0.513543	val: 0.790251	test: 0.765321

Epoch: 23
Loss: 0.5666692533663341
RMSE train: 0.681272	val: 0.844714	test: 0.824917
MAE train: 0.526122	val: 0.651745	test: 0.663062

Epoch: 24
Loss: 0.5720184062208448
RMSE train: 0.666780	val: 0.822801	test: 0.795673
MAE train: 0.523851	val: 0.639924	test: 0.639192

Epoch: 25
Loss: 0.5293525457382202
RMSE train: 0.678930	val: 0.842994	test: 0.804252
MAE train: 0.528913	val: 0.652187	test: 0.647022

Epoch: 26
Loss: 0.5430837486471448
RMSE train: 0.669359	val: 0.842784	test: 0.798290
MAE train: 0.518630	val: 0.645066	test: 0.639364

Epoch: 27
Loss: 0.508000122649329
RMSE train: 0.649828	val: 0.818291	test: 0.784405
MAE train: 0.501762	val: 0.637284	test: 0.623836

Epoch: 28
Loss: 0.5557203271559307
RMSE train: 0.645179	val: 0.816967	test: 0.799519
MAE train: 0.499981	val: 0.625915	test: 0.643030

Epoch: 29
Loss: 0.5068382748535701
RMSE train: 0.646711	val: 0.804073	test: 0.785912
MAE train: 0.507386	val: 0.618497	test: 0.630779

Epoch: 30
Loss: 0.5254124658448356
RMSE train: 0.652219	val: 0.831673	test: 0.800039
MAE train: 0.507563	val: 0.642057	test: 0.645708

Epoch: 31
Loss: 0.49914347912584034
RMSE train: 0.666301	val: 0.845728	test: 0.810307
MAE train: 0.516770	val: 0.654713	test: 0.651790

Epoch: 32
Loss: 0.49426256120204926
RMSE train: 0.629761	val: 0.808301	test: 0.783733
MAE train: 0.487305	val: 0.625800	test: 0.620217

Epoch: 33
Loss: 0.4840272068977356
RMSE train: 0.612625	val: 0.799727	test: 0.782000
MAE train: 0.474400	val: 0.610891	test: 0.624922

Epoch: 34
Loss: 0.501836434006691
RMSE train: 0.617430	val: 0.806314	test: 0.775480
MAE train: 0.479988	val: 0.616876	test: 0.618822

Epoch: 35
Loss: 0.48189508063452585
RMSE train: 0.651932	val: 0.824773	test: 0.799014
MAE train: 0.504415	val: 0.631483	test: 0.640384

Epoch: 36
Loss: 0.4667591282299587
RMSE train: 0.612140	val: 0.799552	test: 0.770653
MAE train: 0.471694	val: 0.609095	test: 0.614267

Epoch: 37
Loss: 0.4720129200390407
RMSE train: 0.643049	val: 0.839459	test: 0.789523
MAE train: 0.496439	val: 0.638909	test: 0.637152

Epoch: 38
Loss: 0.4566593830074583
RMSE train: 0.614820	val: 0.807194	test: 0.772484
MAE train: 0.477711	val: 0.615286	test: 0.614705

Epoch: 39
Loss: 0.45555831704820904
RMSE train: 0.615162	val: 0.804362	test: 0.784190
MAE train: 0.475780	val: 0.616447	test: 0.623226

Epoch: 40
Loss: 0.42740110414368765
RMSE train: 0.608585	val: 0.793856	test: 0.774875
MAE train: 0.474434	val: 0.607040	test: 0.615588

Epoch: 41
Loss: 0.43009057215281893
RMSE train: 0.600271	val: 0.803382	test: 0.778020
MAE train: 0.468946	val: 0.611073	test: 0.618283

Epoch: 42
Loss: 0.4451201345239367
RMSE train: 0.601965	val: 0.795812	test: 0.782794
MAE train: 0.466554	val: 0.608316	test: 0.616056

Epoch: 43
Loss: 0.4215025284460613
RMSE train: 0.592612	val: 0.785194	test: 0.779321
MAE train: 0.460736	val: 0.598762	test: 0.620850

Epoch: 44
Loss: 0.43634821687425884
RMSE train: 0.596255	val: 0.794921	test: 0.774085
MAE train: 0.463127	val: 0.602242	test: 0.613033

Epoch: 45
Loss: 0.42248587948935373
RMSE train: 0.580318	val: 0.791243	test: 0.763814
MAE train: 0.450234	val: 0.608252	test: 0.606985

Epoch: 46
Loss: 0.41960381610052927
RMSE train: 0.594141	val: 0.798917	test: 0.775894
MAE train: 0.460276	val: 0.608982	test: 0.618403

Epoch: 47
Loss: 0.42357065635068075
RMSE train: 0.584398	val: 0.790074	test: 0.770870
MAE train: 0.450834	val: 0.606784	test: 0.613973

Epoch: 48
Loss: 0.4036304695265634
RMSE train: 0.589487	val: 0.791309	test: 0.774531
MAE train: 0.456084	val: 0.605119	test: 0.612094

Epoch: 49
Loss: 0.4361142324549811
RMSE train: 0.607381	val: 0.821052	test: 0.782063
MAE train: 0.472668	val: 0.620530	test: 0.617431

Epoch: 50
Loss: 0.4406994070325579
RMSE train: 0.594504	val: 0.792536	test: 0.768180
MAE train: 0.459152	val: 0.603646	test: 0.613741

Epoch: 51
Loss: 0.4233594856091908
RMSE train: 0.603821	val: 0.820849	test: 0.786431
MAE train: 0.466391	val: 0.624253	test: 0.621915

Epoch: 52
Loss: 0.41627360241753714
RMSE train: 0.580043	val: 0.798839	test: 0.773037
MAE train: 0.450667	val: 0.609193	test: 0.607421

Epoch: 53
Loss: 0.40108469128608704
RMSE train: 0.574710	val: 0.787159	test: 0.762390
MAE train: 0.448645	val: 0.595799	test: 0.607298

Epoch: 54
Loss: 0.3928269020148686
RMSE train: 0.590378	val: 0.796999	test: 0.767684
MAE train: 0.457180	val: 0.605009	test: 0.609796

Epoch: 55
Loss: 0.43619913501398905
RMSE train: 0.594940	val: 0.817458	test: 0.776798
MAE train: 0.460059	val: 0.621457	test: 0.609729

Epoch: 56
Loss: 0.38896156208855764
RMSE train: 0.576646	val: 0.796338	test: 0.762379
MAE train: 0.450542	val: 0.606156	test: 0.601784

Epoch: 57
Loss: 0.39128418266773224
RMSE train: 0.548434	val: 0.781139	test: 0.747178
MAE train: 0.424293	val: 0.595985	test: 0.584820

Epoch: 58
Loss: 0.38734995254448484
RMSE train: 0.596462	val: 0.811640	test: 0.773569
MAE train: 0.461323	val: 0.624071	test: 0.617701

Epoch: 59
Loss: 0.42986870237759184
RMSE train: 0.569217	val: 0.798345	test: 0.755591
MAE train: 0.443440	val: 0.615919	test: 0.596796

Epoch: 60
Loss: 0.37029874324798584
RMSE train: 0.569013	val: 0.780487	test: 0.750707
MAE train: 0.440685	val: 0.603395	test: 0.590685

Epoch: 61
Loss: 0.36908907762595583
RMSE train: 0.612792	val: 0.823398	test: 0.775787
MAE train: 0.477411	val: 0.628022	test: 0.619295

Epoch: 62
Loss: 0.37883493091378895
RMSE train: 0.541574	val: 0.782172	test: 0.741786
MAE train: 0.420782	val: 0.598503	test: 0.581465

Epoch: 63
Loss: 0.3957674035004207
RMSE train: 0.576792	val: 0.809225	test: 0.768924
MAE train: 0.447362	val: 0.620966	test: 0.606859

Epoch: 64
Loss: 0.3538056505577905
RMSE train: 0.554450	val: 0.784217	test: 0.751031
MAE train: 0.431310	val: 0.601247	test: 0.588422

Epoch: 65
Loss: 0.36018267273902893
RMSE train: 0.584574	val: 0.799575	test: 0.759850
MAE train: 0.453723	val: 0.613957	test: 0.604110

Epoch: 66
Loss: 0.37681139579841066
RMSE train: 0.597504	val: 0.822375	test: 0.780064
MAE train: 0.464551	val: 0.630473	test: 0.617139

Epoch: 67
Loss: 0.3554016074963978
RMSE train: 0.540550	val: 0.789054	test: 0.759336
MAE train: 0.418651	val: 0.605570	test: 0.595171

Epoch: 68
Loss: 0.3729112978492464
RMSE train: 0.563426	val: 0.791789	test: 0.753091
MAE train: 0.438553	val: 0.602851	test: 0.586050

Epoch: 69
Loss: 0.36206963871206554
RMSE train: 0.546882	val: 0.796308	test: 0.759729
MAE train: 0.423416	val: 0.605846	test: 0.593369

Epoch: 70
Loss: 0.3367847502231598
RMSE train: 0.532241	val: 0.789779	test: 0.744539
MAE train: 0.413364	val: 0.600989	test: 0.580435

Epoch: 71
Loss: 0.34410294038908823
RMSE train: 0.558801	val: 0.799088	test: 0.761889
MAE train: 0.432658	val: 0.605781	test: 0.598629

Epoch: 72
Loss: 0.36859537235328127
RMSE train: 0.562391	val: 0.804876	test: 0.765868
MAE train: 0.437814	val: 0.612148	test: 0.598976

Epoch: 73
Loss: 0.3614669825349535
RMSE train: 0.539590	val: 0.775840	test: 0.751881
MAE train: 0.418611	val: 0.594385	test: 0.594087

Epoch: 74
Loss: 0.3588788615805762
RMSE train: 0.538960	val: 0.780916	test: 0.766394
MAE train: 0.418114	val: 0.590087	test: 0.604597

Epoch: 75
Loss: 0.3474794157913753
RMSE train: 0.512983	val: 0.774330	test: 0.751158
MAE train: 0.394978	val: 0.586904	test: 0.584778

Epoch: 76
Loss: 0.3446344882249832
RMSE train: 0.583128	val: 0.826503	test: 0.793364
MAE train: 0.454390	val: 0.629412	test: 0.617641

Epoch: 77
Loss: 0.33372398572308676
RMSE train: 0.514206	val: 0.758280	test: 0.742824
MAE train: 0.397376	val: 0.575514	test: 0.582371

Epoch: 78
Loss: 0.33404675126075745
RMSE train: 0.507897	val: 0.757444	test: 0.742346
MAE train: 0.393936	val: 0.580687	test: 0.579341

Epoch: 79
Loss: 0.3214824029377529
RMSE train: 0.530654	val: 0.769996	test: 0.753240
MAE train: 0.409792	val: 0.589198	test: 0.596948

Epoch: 80
Loss: 0.3207697442599705
RMSE train: 0.526507	val: 0.770186	test: 0.750588
MAE train: 0.411754	val: 0.585263	test: 0.589108

Epoch: 81
Loss: 0.32433124099458965
RMSE train: 0.496924	val: 0.753636	test: 0.735485
MAE train: 0.384108	val: 0.571664	test: 0.576439

Epoch: 82
Loss: 0.3418431154319218
RMSE train: 0.534020	val: 0.787131	test: 0.763362
MAE train: 0.411965	val: 0.601557	test: 0.601502

Epoch: 83
Loss: 0.32510670380932943
RMSE train: 0.537148	val: 0.778296	test: 0.749805

Epoch: 23
Loss: 0.5663172815527234
RMSE train: 0.680770	val: 0.860732	test: 0.787997
MAE train: 0.534162	val: 0.663235	test: 0.636010

Epoch: 24
Loss: 0.5628450768334525
RMSE train: 0.684097	val: 0.853717	test: 0.790306
MAE train: 0.535492	val: 0.655484	test: 0.642976

Epoch: 25
Loss: 0.5589987401451383
RMSE train: 0.688110	val: 0.886630	test: 0.797702
MAE train: 0.536352	val: 0.677244	test: 0.644665

Epoch: 26
Loss: 0.5399758837052754
RMSE train: 0.661904	val: 0.851193	test: 0.799317
MAE train: 0.515292	val: 0.653728	test: 0.648644

Epoch: 27
Loss: 0.5326842367649078
RMSE train: 0.674029	val: 0.872084	test: 0.811076
MAE train: 0.526357	val: 0.678531	test: 0.643170

Epoch: 28
Loss: 0.5368426250559943
RMSE train: 0.650066	val: 0.835641	test: 0.793734
MAE train: 0.510981	val: 0.646174	test: 0.641594

Epoch: 29
Loss: 0.5011353194713593
RMSE train: 0.641219	val: 0.835553	test: 0.779995
MAE train: 0.503394	val: 0.645248	test: 0.631303

Epoch: 30
Loss: 0.5246198688234601
RMSE train: 0.637939	val: 0.842970	test: 0.787481
MAE train: 0.495796	val: 0.652217	test: 0.624498

Epoch: 31
Loss: 0.5185951505388532
RMSE train: 0.650333	val: 0.851326	test: 0.785875
MAE train: 0.505698	val: 0.651202	test: 0.634091

Epoch: 32
Loss: 0.49114836752414703
RMSE train: 0.652132	val: 0.831356	test: 0.774443
MAE train: 0.506136	val: 0.635801	test: 0.621523

Epoch: 33
Loss: 0.5044518113136292
RMSE train: 0.661064	val: 0.880009	test: 0.797886
MAE train: 0.515416	val: 0.666612	test: 0.641892

Epoch: 34
Loss: 0.5120849247489657
RMSE train: 0.630970	val: 0.838412	test: 0.779058
MAE train: 0.490578	val: 0.631613	test: 0.620327

Epoch: 35
Loss: 0.48815631440707613
RMSE train: 0.656553	val: 0.864124	test: 0.788950
MAE train: 0.510317	val: 0.666977	test: 0.629292

Epoch: 36
Loss: 0.5080726976905551
RMSE train: 0.633831	val: 0.822670	test: 0.776834
MAE train: 0.493610	val: 0.625968	test: 0.623686

Epoch: 37
Loss: 0.47520897005285534
RMSE train: 0.654124	val: 0.855419	test: 0.781787
MAE train: 0.511533	val: 0.654954	test: 0.621296

Epoch: 38
Loss: 0.48698278622967855
RMSE train: 0.614099	val: 0.814998	test: 0.767752
MAE train: 0.478849	val: 0.625938	test: 0.612326

Epoch: 39
Loss: 0.43991194665431976
RMSE train: 0.622883	val: 0.830318	test: 0.767109
MAE train: 0.486581	val: 0.634057	test: 0.616214

Epoch: 40
Loss: 0.49836774596146177
RMSE train: 0.621515	val: 0.842420	test: 0.782726
MAE train: 0.484579	val: 0.648174	test: 0.622129

Epoch: 41
Loss: 0.49572211291108814
RMSE train: 0.648429	val: 0.841682	test: 0.773029
MAE train: 0.508092	val: 0.650751	test: 0.620763

Epoch: 42
Loss: 0.4606571708406721
RMSE train: 0.600622	val: 0.818275	test: 0.770202
MAE train: 0.468811	val: 0.624866	test: 0.610622

Epoch: 43
Loss: 0.45236445537635256
RMSE train: 0.622599	val: 0.844174	test: 0.779214
MAE train: 0.485654	val: 0.638565	test: 0.634199

Epoch: 44
Loss: 0.4376427424805505
RMSE train: 0.602713	val: 0.817560	test: 0.768975
MAE train: 0.469284	val: 0.624878	test: 0.619858

Epoch: 45
Loss: 0.48396219313144684
RMSE train: 0.606503	val: 0.833172	test: 0.772655
MAE train: 0.472480	val: 0.639577	test: 0.622165

Epoch: 46
Loss: 0.4561377146414348
RMSE train: 0.595626	val: 0.809249	test: 0.770726
MAE train: 0.464159	val: 0.623745	test: 0.617056

Epoch: 47
Loss: 0.44364558160305023
RMSE train: 0.595448	val: 0.807685	test: 0.763995
MAE train: 0.464125	val: 0.616226	test: 0.616325

Epoch: 48
Loss: 0.4269813873938152
RMSE train: 0.604747	val: 0.815376	test: 0.766351
MAE train: 0.470814	val: 0.621536	test: 0.610514

Epoch: 49
Loss: 0.4194009836230959
RMSE train: 0.609732	val: 0.822178	test: 0.777350
MAE train: 0.476744	val: 0.635688	test: 0.623406

Epoch: 50
Loss: 0.46131934438432964
RMSE train: 0.603988	val: 0.818271	test: 0.772927
MAE train: 0.470747	val: 0.635088	test: 0.610152

Epoch: 51
Loss: 0.4463696926832199
RMSE train: 0.630445	val: 0.831074	test: 0.794190
MAE train: 0.491975	val: 0.637657	test: 0.632348

Epoch: 52
Loss: 0.41304069331714083
RMSE train: 0.594348	val: 0.798841	test: 0.778194
MAE train: 0.458573	val: 0.616505	test: 0.625506

Epoch: 53
Loss: 0.4157046356371471
RMSE train: 0.587889	val: 0.800240	test: 0.769877
MAE train: 0.458004	val: 0.613391	test: 0.611250

Epoch: 54
Loss: 0.42133474349975586
RMSE train: 0.623395	val: 0.851464	test: 0.794338
MAE train: 0.484625	val: 0.651485	test: 0.628746

Epoch: 55
Loss: 0.4240353448050363
RMSE train: 0.571427	val: 0.801336	test: 0.763708
MAE train: 0.445998	val: 0.614024	test: 0.609793

Epoch: 56
Loss: 0.39535788765975405
RMSE train: 0.607133	val: 0.832983	test: 0.790040
MAE train: 0.473840	val: 0.643685	test: 0.628915

Epoch: 57
Loss: 0.40221230685710907
RMSE train: 0.580515	val: 0.794071	test: 0.761995
MAE train: 0.449640	val: 0.609879	test: 0.607732

Epoch: 58
Loss: 0.40579495472567423
RMSE train: 0.586108	val: 0.805114	test: 0.773389
MAE train: 0.454518	val: 0.616677	test: 0.621018

Epoch: 59
Loss: 0.403260554586138
RMSE train: 0.606399	val: 0.823819	test: 0.772977
MAE train: 0.473078	val: 0.629199	test: 0.619159

Epoch: 60
Loss: 0.3803123129265649
RMSE train: 0.568719	val: 0.810016	test: 0.761859
MAE train: 0.441833	val: 0.621605	test: 0.598193

Epoch: 61
Loss: 0.38632605757032124
RMSE train: 0.548136	val: 0.775731	test: 0.746179
MAE train: 0.427193	val: 0.593086	test: 0.586498

Epoch: 62
Loss: 0.39698084337370737
RMSE train: 0.555454	val: 0.788432	test: 0.745115
MAE train: 0.431106	val: 0.600750	test: 0.590365

Epoch: 63
Loss: 0.38390229429517475
RMSE train: 0.552881	val: 0.781129	test: 0.751657
MAE train: 0.429025	val: 0.594056	test: 0.597592

Epoch: 64
Loss: 0.3845757075718471
RMSE train: 0.557158	val: 0.803460	test: 0.751160
MAE train: 0.433681	val: 0.618607	test: 0.589373

Epoch: 65
Loss: 0.3941285844360079
RMSE train: 0.567433	val: 0.808170	test: 0.767613
MAE train: 0.440913	val: 0.623235	test: 0.610068

Epoch: 66
Loss: 0.3828185370990208
RMSE train: 0.568248	val: 0.816518	test: 0.775241
MAE train: 0.442028	val: 0.625673	test: 0.618797

Epoch: 67
Loss: 0.3928775489330292
RMSE train: 0.601746	val: 0.848835	test: 0.800748
MAE train: 0.469403	val: 0.650235	test: 0.639305

Epoch: 68
Loss: 0.3764383750302451
RMSE train: 0.536651	val: 0.777037	test: 0.763403
MAE train: 0.416668	val: 0.597994	test: 0.587689

Epoch: 69
Loss: 0.36187276776347843
RMSE train: 0.544660	val: 0.799909	test: 0.760643
MAE train: 0.420310	val: 0.606011	test: 0.596972

Epoch: 70
Loss: 0.35368237112249645
RMSE train: 0.529477	val: 0.775935	test: 0.743052
MAE train: 0.410057	val: 0.598913	test: 0.585805

Epoch: 71
Loss: 0.3354887600455965
RMSE train: 0.537057	val: 0.793061	test: 0.758263
MAE train: 0.417907	val: 0.608123	test: 0.598132

Epoch: 72
Loss: 0.34579781762191225
RMSE train: 0.538469	val: 0.799789	test: 0.750391
MAE train: 0.419077	val: 0.608459	test: 0.583259

Epoch: 73
Loss: 0.3626311698130199
RMSE train: 0.524800	val: 0.782375	test: 0.742973
MAE train: 0.407712	val: 0.602382	test: 0.583458

Epoch: 74
Loss: 0.3570472151041031
RMSE train: 0.531052	val: 0.793032	test: 0.753261
MAE train: 0.415029	val: 0.608002	test: 0.591044

Epoch: 75
Loss: 0.3450210935303143
RMSE train: 0.556885	val: 0.773812	test: 0.760090
MAE train: 0.433774	val: 0.596449	test: 0.602598

Epoch: 76
Loss: 0.3466146354164396
RMSE train: 0.577001	val: 0.849411	test: 0.800681
MAE train: 0.450508	val: 0.649719	test: 0.635616

Epoch: 77
Loss: 0.3439599041427885
RMSE train: 0.525026	val: 0.792526	test: 0.763450
MAE train: 0.407515	val: 0.610309	test: 0.593300

Epoch: 78
Loss: 0.34607852143900736
RMSE train: 0.533588	val: 0.798023	test: 0.749402
MAE train: 0.414626	val: 0.607305	test: 0.587187

Epoch: 79
Loss: 0.331010388476508
RMSE train: 0.514480	val: 0.771408	test: 0.742067
MAE train: 0.399223	val: 0.589592	test: 0.580646

Epoch: 80
Loss: 0.343856292111533
RMSE train: 0.539515	val: 0.810913	test: 0.774275
MAE train: 0.418880	val: 0.618702	test: 0.611591

Epoch: 81
Loss: 0.326592817902565
RMSE train: 0.537138	val: 0.794713	test: 0.753969
MAE train: 0.417639	val: 0.606197	test: 0.598439

Epoch: 82
Loss: 0.3439014256000519
RMSE train: 0.542193	val: 0.812704	test: 0.779562
MAE train: 0.421007	val: 0.620011	test: 0.613348

Epoch: 83
Loss: 0.33209505677223206
RMSE train: 0.529462	val: 0.803906	test: 0.764253
MAE train: 0.366746	val: 0.629498	test: 0.643732

Epoch: 84
Loss: 0.28448245376348497
RMSE train: 0.474010	val: 0.822325	test: 0.818018
MAE train: 0.367985	val: 0.626142	test: 0.632360

Epoch: 85
Loss: 0.2791064590215683
RMSE train: 0.480762	val: 0.839767	test: 0.831502
MAE train: 0.373186	val: 0.638264	test: 0.645698

Epoch: 86
Loss: 0.2757175832986832
RMSE train: 0.478750	val: 0.814988	test: 0.829534
MAE train: 0.369038	val: 0.630333	test: 0.643371

Epoch: 87
Loss: 0.2792173609137535
RMSE train: 0.469322	val: 0.824313	test: 0.819614
MAE train: 0.366406	val: 0.631235	test: 0.638692

Epoch: 88
Loss: 0.28054540902376174
RMSE train: 0.464080	val: 0.816689	test: 0.814508
MAE train: 0.360830	val: 0.622903	test: 0.635332

Epoch: 89
Loss: 0.2761740326881409
RMSE train: 0.451595	val: 0.805108	test: 0.807636
MAE train: 0.351440	val: 0.616704	test: 0.625847

Epoch: 90
Loss: 0.28054951429367064
RMSE train: 0.454249	val: 0.819247	test: 0.819595
MAE train: 0.350644	val: 0.621890	test: 0.636256

Epoch: 91
Loss: 0.27035375535488126
RMSE train: 0.459106	val: 0.821241	test: 0.831811
MAE train: 0.356249	val: 0.624528	test: 0.643283

Epoch: 92
Loss: 0.27020645439624785
RMSE train: 0.491024	val: 0.846291	test: 0.857344
MAE train: 0.381643	val: 0.645364	test: 0.670559

Epoch: 93
Loss: 0.2642305225133896
RMSE train: 0.447965	val: 0.807396	test: 0.806875
MAE train: 0.348461	val: 0.617123	test: 0.624317

Epoch: 94
Loss: 0.2801391273736954
RMSE train: 0.477763	val: 0.831034	test: 0.847635
MAE train: 0.371605	val: 0.636177	test: 0.661253

Epoch: 95
Loss: 0.2657060846686363
RMSE train: 0.460502	val: 0.820236	test: 0.826907
MAE train: 0.356324	val: 0.628523	test: 0.639813

Epoch: 96
Loss: 0.2718865886330605
RMSE train: 0.472201	val: 0.816628	test: 0.836194
MAE train: 0.365578	val: 0.622941	test: 0.646224

Epoch: 97
Loss: 0.26709196269512175
RMSE train: 0.444796	val: 0.803055	test: 0.809070
MAE train: 0.345545	val: 0.616092	test: 0.623919

Epoch: 98
Loss: 0.261544394493103
RMSE train: 0.438264	val: 0.811539	test: 0.816627
MAE train: 0.339856	val: 0.620288	test: 0.630582

Epoch: 99
Loss: 0.2620965912938118
RMSE train: 0.443345	val: 0.810913	test: 0.818034
MAE train: 0.343506	val: 0.617966	test: 0.634698

Epoch: 100
Loss: 0.2475993812084198
RMSE train: 0.441219	val: 0.815652	test: 0.812634
MAE train: 0.341461	val: 0.621208	test: 0.631716

Epoch: 101
Loss: 0.26627384722232816
RMSE train: 0.455375	val: 0.805264	test: 0.811431
MAE train: 0.354693	val: 0.610519	test: 0.631733

Epoch: 102
Loss: 0.26746463477611543
RMSE train: 0.453792	val: 0.843971	test: 0.845476
MAE train: 0.351783	val: 0.638677	test: 0.653007

Epoch: 103
Loss: 0.2607962653040886
RMSE train: 0.455803	val: 0.832522	test: 0.837111
MAE train: 0.354154	val: 0.637992	test: 0.654344

Epoch: 104
Loss: 0.2536317840218544
RMSE train: 0.450225	val: 0.819267	test: 0.824987
MAE train: 0.349831	val: 0.626653	test: 0.639493

Epoch: 105
Loss: 0.26066563427448275
RMSE train: 0.463699	val: 0.829294	test: 0.835644
MAE train: 0.358214	val: 0.633968	test: 0.650367

Epoch: 106
Loss: 0.25675674974918367
RMSE train: 0.427989	val: 0.801341	test: 0.797823
MAE train: 0.329434	val: 0.610456	test: 0.622550

Epoch: 107
Loss: 0.25177313983440397
RMSE train: 0.437756	val: 0.806517	test: 0.812299
MAE train: 0.340853	val: 0.618520	test: 0.636343

Epoch: 108
Loss: 0.2583981275558472
RMSE train: 0.456582	val: 0.838805	test: 0.853556
MAE train: 0.354098	val: 0.641660	test: 0.663576

Epoch: 109
Loss: 0.2444308429956436
RMSE train: 0.438461	val: 0.817508	test: 0.823002
MAE train: 0.342269	val: 0.622653	test: 0.639566

Epoch: 110
Loss: 0.24736082851886748
RMSE train: 0.437227	val: 0.806394	test: 0.819729
MAE train: 0.339288	val: 0.620211	test: 0.634657

Epoch: 111
Loss: 0.24290980845689775
RMSE train: 0.457719	val: 0.840414	test: 0.842524
MAE train: 0.353046	val: 0.641127	test: 0.660618

Epoch: 112
Loss: 0.24118619859218599
RMSE train: 0.431756	val: 0.806634	test: 0.812166
MAE train: 0.333764	val: 0.614962	test: 0.631553

Epoch: 113
Loss: 0.23799657970666885
RMSE train: 0.467824	val: 0.865058	test: 0.866360
MAE train: 0.363013	val: 0.659848	test: 0.673153

Epoch: 114
Loss: 0.24231770783662795
RMSE train: 0.449449	val: 0.814432	test: 0.825572
MAE train: 0.350333	val: 0.621684	test: 0.643499

Epoch: 115
Loss: 0.23508802205324172
RMSE train: 0.432045	val: 0.818299	test: 0.833470
MAE train: 0.333115	val: 0.622423	test: 0.647405

Epoch: 116
Loss: 0.23281265944242477
RMSE train: 0.449576	val: 0.819225	test: 0.835974
MAE train: 0.347940	val: 0.627918	test: 0.651999

Epoch: 117
Loss: 0.24191561341285706
RMSE train: 0.420946	val: 0.796922	test: 0.806986
MAE train: 0.329429	val: 0.603798	test: 0.627288

Epoch: 118
Loss: 0.23851489275693893
RMSE train: 0.429404	val: 0.805522	test: 0.811498
MAE train: 0.334762	val: 0.614863	test: 0.633789

Epoch: 119
Loss: 0.22628590315580369
RMSE train: 0.415569	val: 0.790300	test: 0.794701
MAE train: 0.323374	val: 0.603422	test: 0.618659

Epoch: 120
Loss: 0.23130299896001816
RMSE train: 0.428215	val: 0.813476	test: 0.821175
MAE train: 0.332918	val: 0.621173	test: 0.639705

Epoch: 121
Loss: 0.23629579544067383
RMSE train: 0.418450	val: 0.795833	test: 0.804649
MAE train: 0.325730	val: 0.605353	test: 0.628039

Epoch: 122
Loss: 0.2309439405798912
RMSE train: 0.424513	val: 0.802038	test: 0.810338
MAE train: 0.329927	val: 0.616666	test: 0.634439

Epoch: 123
Loss: 0.23005017191171645
RMSE train: 0.418827	val: 0.798069	test: 0.806964
MAE train: 0.325274	val: 0.612213	test: 0.626023

Epoch: 124
Loss: 0.2303810179233551
RMSE train: 0.410522	val: 0.805530	test: 0.812268
MAE train: 0.319257	val: 0.611854	test: 0.625771

Epoch: 125
Loss: 0.236678646504879
RMSE train: 0.421600	val: 0.812794	test: 0.818786
MAE train: 0.328811	val: 0.620478	test: 0.639159

Epoch: 126
Loss: 0.22207731008529663
RMSE train: 0.412749	val: 0.811441	test: 0.812652
MAE train: 0.322246	val: 0.615919	test: 0.629289

Epoch: 127
Loss: 0.2315199688076973
RMSE train: 0.436961	val: 0.814265	test: 0.827665
MAE train: 0.342232	val: 0.623073	test: 0.636482

Epoch: 128
Loss: 0.2284007787704468
RMSE train: 0.412350	val: 0.805149	test: 0.825403
MAE train: 0.317410	val: 0.613814	test: 0.632521

Epoch: 129
Loss: 0.22939291447401047
RMSE train: 0.428104	val: 0.819004	test: 0.831732
MAE train: 0.335266	val: 0.623293	test: 0.638042

Epoch: 130
Loss: 0.22849039882421493
RMSE train: 0.412153	val: 0.798804	test: 0.815681
MAE train: 0.321739	val: 0.608244	test: 0.625591

Epoch: 131
Loss: 0.2249625325202942
RMSE train: 0.414269	val: 0.811852	test: 0.815334
MAE train: 0.320208	val: 0.614506	test: 0.634320

Epoch: 132
Loss: 0.2119359016418457
RMSE train: 0.403242	val: 0.784243	test: 0.789330
MAE train: 0.314304	val: 0.594484	test: 0.608548

Epoch: 133
Loss: 0.23110781461000443
RMSE train: 0.421350	val: 0.823210	test: 0.832587
MAE train: 0.327303	val: 0.629879	test: 0.641594

Epoch: 134
Loss: 0.21841682493686676
RMSE train: 0.429662	val: 0.794107	test: 0.803437
MAE train: 0.337953	val: 0.610146	test: 0.617703

Epoch: 135
Loss: 0.2147551730275154
RMSE train: 0.418942	val: 0.807133	test: 0.811002
MAE train: 0.326951	val: 0.611801	test: 0.628278

Epoch: 136
Loss: 0.2124679997563362
RMSE train: 0.403164	val: 0.815777	test: 0.831709
MAE train: 0.313286	val: 0.622086	test: 0.638682

Epoch: 137
Loss: 0.2029767632484436
RMSE train: 0.404709	val: 0.801121	test: 0.812536
MAE train: 0.316149	val: 0.610489	test: 0.630712

Epoch: 138
Loss: 0.20478166937828063
RMSE train: 0.410290	val: 0.806869	test: 0.824722
MAE train: 0.319264	val: 0.613423	test: 0.635021

Epoch: 139
Loss: 0.21591847836971284
RMSE train: 0.409553	val: 0.800279	test: 0.815395
MAE train: 0.318431	val: 0.613148	test: 0.633114

Epoch: 140
Loss: 0.21097214370965958
RMSE train: 0.394643	val: 0.808221	test: 0.823013
MAE train: 0.305142	val: 0.615777	test: 0.630219

Epoch: 141
Loss: 0.2022155210375786
RMSE train: 0.389826	val: 0.791555	test: 0.791325
MAE train: 0.304485	val: 0.600362	test: 0.611638

Epoch: 142
Loss: 0.21769458204507827
RMSE train: 0.397351	val: 0.811213	test: 0.825646
MAE train: 0.309897	val: 0.612846	test: 0.635118

Epoch: 143
Loss: 0.2056295320391655
RMSE train: 0.391709	val: 0.810107	test: 0.811771
MAE train: 0.306499	val: 0.613619	test: 0.626405
MAE train: 0.377888	val: 0.632223	test: 0.645416

Epoch: 84
Loss: 0.29311050176620485
RMSE train: 0.467839	val: 0.811872	test: 0.806641
MAE train: 0.361512	val: 0.621550	test: 0.626039

Epoch: 85
Loss: 0.2766352936625481
RMSE train: 0.453561	val: 0.806141	test: 0.797272
MAE train: 0.353270	val: 0.618487	test: 0.622889

Epoch: 86
Loss: 0.2846645087003708
RMSE train: 0.489156	val: 0.828100	test: 0.820448
MAE train: 0.383209	val: 0.638260	test: 0.643839

Epoch: 87
Loss: 0.28599835336208346
RMSE train: 0.473814	val: 0.836444	test: 0.833173
MAE train: 0.368654	val: 0.642049	test: 0.648553

Epoch: 88
Loss: 0.28647397458553314
RMSE train: 0.464774	val: 0.815069	test: 0.811733
MAE train: 0.363405	val: 0.627117	test: 0.631472

Epoch: 89
Loss: 0.2747947409749031
RMSE train: 0.481637	val: 0.821213	test: 0.814796
MAE train: 0.376621	val: 0.631698	test: 0.635194

Epoch: 90
Loss: 0.2741391688585281
RMSE train: 0.463692	val: 0.829953	test: 0.822623
MAE train: 0.360417	val: 0.636343	test: 0.643429

Epoch: 91
Loss: 0.2811821520328522
RMSE train: 0.465661	val: 0.802711	test: 0.799121
MAE train: 0.364465	val: 0.617299	test: 0.622397

Epoch: 92
Loss: 0.2757065951824188
RMSE train: 0.468862	val: 0.822128	test: 0.814958
MAE train: 0.365243	val: 0.629867	test: 0.637947

Epoch: 93
Loss: 0.27071339786052706
RMSE train: 0.449448	val: 0.797559	test: 0.793289
MAE train: 0.348615	val: 0.609677	test: 0.615225

Epoch: 94
Loss: 0.2568218469619751
RMSE train: 0.457309	val: 0.824115	test: 0.810445
MAE train: 0.355829	val: 0.623690	test: 0.633108

Epoch: 95
Loss: 0.27876378446817396
RMSE train: 0.447864	val: 0.810536	test: 0.798764
MAE train: 0.347266	val: 0.613772	test: 0.623074

Epoch: 96
Loss: 0.27426812052726746
RMSE train: 0.460499	val: 0.821593	test: 0.821822
MAE train: 0.357444	val: 0.629849	test: 0.641446

Epoch: 97
Loss: 0.2592031627893448
RMSE train: 0.456772	val: 0.838247	test: 0.823608
MAE train: 0.354309	val: 0.637094	test: 0.643880

Epoch: 98
Loss: 0.2638490781188011
RMSE train: 0.449988	val: 0.829922	test: 0.813336
MAE train: 0.347337	val: 0.627179	test: 0.632671

Epoch: 99
Loss: 0.25542505383491515
RMSE train: 0.470647	val: 0.815965	test: 0.819471
MAE train: 0.365211	val: 0.622227	test: 0.634139

Epoch: 100
Loss: 0.2715156048536301
RMSE train: 0.454864	val: 0.809637	test: 0.810001
MAE train: 0.354498	val: 0.618445	test: 0.629645

Epoch: 101
Loss: 0.2612264543771744
RMSE train: 0.456520	val: 0.829152	test: 0.812244
MAE train: 0.356462	val: 0.632571	test: 0.637217

Epoch: 102
Loss: 0.2699612841010094
RMSE train: 0.441766	val: 0.821795	test: 0.818961
MAE train: 0.341831	val: 0.626482	test: 0.635128

Epoch: 103
Loss: 0.25131575018167496
RMSE train: 0.453670	val: 0.807690	test: 0.803037
MAE train: 0.353654	val: 0.614861	test: 0.625314

Epoch: 104
Loss: 0.2608393728733063
RMSE train: 0.450040	val: 0.814243	test: 0.796026
MAE train: 0.352789	val: 0.618897	test: 0.622550

Epoch: 105
Loss: 0.24535682946443557
RMSE train: 0.442032	val: 0.806512	test: 0.798216
MAE train: 0.345062	val: 0.614692	test: 0.624104

Epoch: 106
Loss: 0.2593980386853218
RMSE train: 0.443986	val: 0.812275	test: 0.809765
MAE train: 0.345165	val: 0.621756	test: 0.628802

Epoch: 107
Loss: 0.2502267390489578
RMSE train: 0.477833	val: 0.856808	test: 0.842412
MAE train: 0.371318	val: 0.655327	test: 0.655458

Epoch: 108
Loss: 0.2534290313720703
RMSE train: 0.454181	val: 0.824446	test: 0.811561
MAE train: 0.354752	val: 0.627732	test: 0.633226

Epoch: 109
Loss: 0.2601849287748337
RMSE train: 0.438264	val: 0.808899	test: 0.805591
MAE train: 0.340384	val: 0.615873	test: 0.626790

Epoch: 110
Loss: 0.24777578115463256
RMSE train: 0.431133	val: 0.821131	test: 0.807604
MAE train: 0.334663	val: 0.625565	test: 0.628003

Epoch: 111
Loss: 0.2397959753870964
RMSE train: 0.430461	val: 0.812318	test: 0.802299
MAE train: 0.335647	val: 0.618658	test: 0.623227

Epoch: 112
Loss: 0.23767296373844146
RMSE train: 0.434085	val: 0.805150	test: 0.809831
MAE train: 0.339144	val: 0.611812	test: 0.629857

Epoch: 113
Loss: 0.24302363395690918
RMSE train: 0.430246	val: 0.804547	test: 0.805955
MAE train: 0.333545	val: 0.615487	test: 0.621657

Epoch: 114
Loss: 0.2407263547182083
RMSE train: 0.428226	val: 0.814450	test: 0.805940
MAE train: 0.333119	val: 0.620876	test: 0.623947

Epoch: 115
Loss: 0.2407946392893791
RMSE train: 0.434705	val: 0.802564	test: 0.797312
MAE train: 0.340066	val: 0.611884	test: 0.620075

Epoch: 116
Loss: 0.2461191788315773
RMSE train: 0.446849	val: 0.803272	test: 0.798588
MAE train: 0.353150	val: 0.617931	test: 0.621126

Epoch: 117
Loss: 0.23850256949663162
RMSE train: 0.425415	val: 0.801071	test: 0.802248
MAE train: 0.331346	val: 0.609563	test: 0.622274

Epoch: 118
Loss: 0.23501853048801422
RMSE train: 0.435372	val: 0.834041	test: 0.818883
MAE train: 0.338294	val: 0.636433	test: 0.633513

Epoch: 119
Loss: 0.23674650639295577
RMSE train: 0.406531	val: 0.803014	test: 0.791076
MAE train: 0.315363	val: 0.610139	test: 0.611538

Epoch: 120
Loss: 0.23661010414361955
RMSE train: 0.424191	val: 0.813819	test: 0.805422
MAE train: 0.330031	val: 0.621225	test: 0.626115

Epoch: 121
Loss: 0.23231289386749268
RMSE train: 0.432146	val: 0.805738	test: 0.796355
MAE train: 0.338411	val: 0.618616	test: 0.616109

Epoch: 122
Loss: 0.25074067413806916
RMSE train: 0.424302	val: 0.800119	test: 0.787933
MAE train: 0.330000	val: 0.608779	test: 0.609089

Epoch: 123
Loss: 0.22984957844018936
RMSE train: 0.444638	val: 0.824720	test: 0.828972
MAE train: 0.344574	val: 0.628309	test: 0.641048

Epoch: 124
Loss: 0.23776098638772963
RMSE train: 0.435261	val: 0.834792	test: 0.831722
MAE train: 0.338637	val: 0.637354	test: 0.640507

Epoch: 125
Loss: 0.23274187445640565
RMSE train: 0.414764	val: 0.797209	test: 0.785486
MAE train: 0.323819	val: 0.602478	test: 0.612773

Epoch: 126
Loss: 0.21747494637966155
RMSE train: 0.435115	val: 0.814771	test: 0.811964
MAE train: 0.340414	val: 0.617679	test: 0.633680

Epoch: 127
Loss: 0.22289701253175737
RMSE train: 0.434581	val: 0.835904	test: 0.823236
MAE train: 0.339093	val: 0.633838	test: 0.640226

Epoch: 128
Loss: 0.2182103544473648
RMSE train: 0.404952	val: 0.800316	test: 0.782124
MAE train: 0.315057	val: 0.606535	test: 0.603258

Epoch: 129
Loss: 0.22213843017816542
RMSE train: 0.416494	val: 0.806016	test: 0.794042
MAE train: 0.325144	val: 0.613481	test: 0.614008

Epoch: 130
Loss: 0.2251716285943985
RMSE train: 0.417112	val: 0.807882	test: 0.796530
MAE train: 0.326452	val: 0.616919	test: 0.618491

Epoch: 131
Loss: 0.21895281970500946
RMSE train: 0.408783	val: 0.797511	test: 0.793307
MAE train: 0.317798	val: 0.604580	test: 0.613288

Epoch: 132
Loss: 0.226822167634964
RMSE train: 0.415541	val: 0.813367	test: 0.801832
MAE train: 0.323300	val: 0.621561	test: 0.619412

Epoch: 133
Loss: 0.2139219343662262
RMSE train: 0.402786	val: 0.797140	test: 0.795103
MAE train: 0.312667	val: 0.609839	test: 0.612647

Epoch: 134
Loss: 0.21630184650421141
RMSE train: 0.427535	val: 0.823683	test: 0.826320
MAE train: 0.335649	val: 0.622896	test: 0.637375

Epoch: 135
Loss: 0.22686389833688736
RMSE train: 0.434480	val: 0.816374	test: 0.818793
MAE train: 0.337065	val: 0.623055	test: 0.627398

Epoch: 136
Loss: 0.22164650708436967
RMSE train: 0.404931	val: 0.794666	test: 0.784132
MAE train: 0.316657	val: 0.604346	test: 0.605453

Epoch: 137
Loss: 0.21424955427646636
RMSE train: 0.408967	val: 0.815047	test: 0.808806
MAE train: 0.317953	val: 0.619565	test: 0.630142

Epoch: 138
Loss: 0.22182756066322326
RMSE train: 0.431550	val: 0.824419	test: 0.818582
MAE train: 0.335022	val: 0.630342	test: 0.629535

Epoch: 139
Loss: 0.21620189547538757
RMSE train: 0.413236	val: 0.818301	test: 0.816572
MAE train: 0.319714	val: 0.623845	test: 0.629080

Epoch: 140
Loss: 0.2081940472126007
RMSE train: 0.409385	val: 0.799999	test: 0.792948
MAE train: 0.321996	val: 0.612270	test: 0.613477

Epoch: 141
Loss: 0.21791652590036392
RMSE train: 0.400752	val: 0.802632	test: 0.808698
MAE train: 0.309895	val: 0.610886	test: 0.622048

Epoch: 142
Loss: 0.20762260258197784
RMSE train: 0.403156	val: 0.790356	test: 0.791651
MAE train: 0.314877	val: 0.604688	test: 0.613208

Epoch: 143
Loss: 0.2076910763978958
RMSE train: 0.418951	val: 0.831689	test: 0.829162
MAE train: 0.326471	val: 0.633451	test: 0.639504
MAE train: 0.376534	val: 0.638296	test: 0.626225

Epoch: 84
Loss: 0.29348036348819734
RMSE train: 0.459434	val: 0.820319	test: 0.805492
MAE train: 0.359823	val: 0.625739	test: 0.621786

Epoch: 85
Loss: 0.29104721248149873
RMSE train: 0.453866	val: 0.824694	test: 0.813941
MAE train: 0.353875	val: 0.628426	test: 0.629970

Epoch: 86
Loss: 0.28534620106220243
RMSE train: 0.461327	val: 0.838685	test: 0.834303
MAE train: 0.357136	val: 0.644656	test: 0.644558

Epoch: 87
Loss: 0.2779792368412018
RMSE train: 0.454882	val: 0.807195	test: 0.803966
MAE train: 0.354006	val: 0.616997	test: 0.623674

Epoch: 88
Loss: 0.27739614248275757
RMSE train: 0.467783	val: 0.821697	test: 0.803058
MAE train: 0.367334	val: 0.623601	test: 0.626376

Epoch: 89
Loss: 0.27316137254238126
RMSE train: 0.443125	val: 0.818749	test: 0.810928
MAE train: 0.343310	val: 0.623452	test: 0.623976

Epoch: 90
Loss: 0.27180475294589995
RMSE train: 0.452368	val: 0.819429	test: 0.814268
MAE train: 0.351869	val: 0.631207	test: 0.623888

Epoch: 91
Loss: 0.29193519949913027
RMSE train: 0.451903	val: 0.829841	test: 0.826573
MAE train: 0.353118	val: 0.635460	test: 0.638725

Epoch: 92
Loss: 0.2831475302577019
RMSE train: 0.464624	val: 0.834337	test: 0.827471
MAE train: 0.364069	val: 0.639701	test: 0.643118

Epoch: 93
Loss: 0.2718548595905304
RMSE train: 0.440005	val: 0.822514	test: 0.809071
MAE train: 0.341443	val: 0.623488	test: 0.621086

Epoch: 94
Loss: 0.269146765768528
RMSE train: 0.441077	val: 0.815693	test: 0.809024
MAE train: 0.341178	val: 0.621515	test: 0.621362

Epoch: 95
Loss: 0.2654403880238533
RMSE train: 0.460822	val: 0.825775	test: 0.805147
MAE train: 0.359155	val: 0.626636	test: 0.622286

Epoch: 96
Loss: 0.2669020280241966
RMSE train: 0.433262	val: 0.813203	test: 0.804592
MAE train: 0.336575	val: 0.623674	test: 0.618555

Epoch: 97
Loss: 0.2748946636915207
RMSE train: 0.438709	val: 0.823345	test: 0.809100
MAE train: 0.341377	val: 0.631148	test: 0.621396

Epoch: 98
Loss: 0.2606759935617447
RMSE train: 0.430653	val: 0.821922	test: 0.799805
MAE train: 0.332446	val: 0.619648	test: 0.618513

Epoch: 99
Loss: 0.2577036052942276
RMSE train: 0.441864	val: 0.813810	test: 0.797643
MAE train: 0.344559	val: 0.616345	test: 0.609479

Epoch: 100
Loss: 0.2621618703007698
RMSE train: 0.437403	val: 0.823999	test: 0.807749
MAE train: 0.339710	val: 0.623445	test: 0.618457

Epoch: 101
Loss: 0.2558658719062805
RMSE train: 0.429815	val: 0.822606	test: 0.813508
MAE train: 0.333309	val: 0.632026	test: 0.621496

Epoch: 102
Loss: 0.2737319678068161
RMSE train: 0.427867	val: 0.820390	test: 0.804940
MAE train: 0.334114	val: 0.630068	test: 0.617607

Epoch: 103
Loss: 0.25049627274274827
RMSE train: 0.437880	val: 0.829106	test: 0.808005
MAE train: 0.341097	val: 0.632432	test: 0.622088

Epoch: 104
Loss: 0.2568992465734482
RMSE train: 0.423936	val: 0.821336	test: 0.815100
MAE train: 0.329805	val: 0.625830	test: 0.623684

Epoch: 105
Loss: 0.24579128324985505
RMSE train: 0.428896	val: 0.820551	test: 0.810412
MAE train: 0.336785	val: 0.630169	test: 0.625449

Epoch: 106
Loss: 0.24969000965356827
RMSE train: 0.469878	val: 0.843237	test: 0.824305
MAE train: 0.373490	val: 0.649953	test: 0.636489

Epoch: 107
Loss: 0.24178372621536254
RMSE train: 0.427547	val: 0.810035	test: 0.799559
MAE train: 0.331955	val: 0.612631	test: 0.612713

Epoch: 108
Loss: 0.24646225124597548
RMSE train: 0.422261	val: 0.814623	test: 0.806008
MAE train: 0.329012	val: 0.622372	test: 0.619871

Epoch: 109
Loss: 0.24559052437543868
RMSE train: 0.437439	val: 0.819023	test: 0.810675
MAE train: 0.343471	val: 0.623719	test: 0.621794

Epoch: 110
Loss: 0.24956915229558946
RMSE train: 0.434715	val: 0.808807	test: 0.808615
MAE train: 0.343311	val: 0.620667	test: 0.624996

Epoch: 111
Loss: 0.24234810918569566
RMSE train: 0.418225	val: 0.804065	test: 0.800166
MAE train: 0.325293	val: 0.611400	test: 0.616418

Epoch: 112
Loss: 0.2368357166647911
RMSE train: 0.423781	val: 0.824396	test: 0.815889
MAE train: 0.329499	val: 0.625123	test: 0.628127

Epoch: 113
Loss: 0.24121570438146592
RMSE train: 0.435773	val: 0.812619	test: 0.801609
MAE train: 0.339993	val: 0.616709	test: 0.618777

Epoch: 114
Loss: 0.23580169826745986
RMSE train: 0.423755	val: 0.834501	test: 0.819753
MAE train: 0.327354	val: 0.632914	test: 0.633610

Epoch: 115
Loss: 0.2456400290131569
RMSE train: 0.440701	val: 0.806300	test: 0.805741
MAE train: 0.345963	val: 0.625717	test: 0.624261

Epoch: 116
Loss: 0.23479666262865068
RMSE train: 0.433448	val: 0.828635	test: 0.833419
MAE train: 0.336529	val: 0.638056	test: 0.643101

Epoch: 117
Loss: 0.23584941774606705
RMSE train: 0.414947	val: 0.815591	test: 0.820554
MAE train: 0.322424	val: 0.623391	test: 0.629860

Epoch: 118
Loss: 0.24121886789798735
RMSE train: 0.416053	val: 0.813569	test: 0.810732
MAE train: 0.321475	val: 0.619985	test: 0.621010

Epoch: 119
Loss: 0.23069328367710112
RMSE train: 0.417118	val: 0.817965	test: 0.809964
MAE train: 0.324484	val: 0.624252	test: 0.627141

Epoch: 120
Loss: 0.23210053890943527
RMSE train: 0.416202	val: 0.817280	test: 0.808400
MAE train: 0.320417	val: 0.625304	test: 0.620179

Epoch: 121
Loss: 0.23726000487804413
RMSE train: 0.419184	val: 0.809846	test: 0.808659
MAE train: 0.326659	val: 0.624687	test: 0.622686

Epoch: 122
Loss: 0.2336486428976059
RMSE train: 0.425376	val: 0.813166	test: 0.809917
MAE train: 0.332299	val: 0.620915	test: 0.626679

Epoch: 123
Loss: 0.23240974843502044
RMSE train: 0.435442	val: 0.829509	test: 0.818808
MAE train: 0.339165	val: 0.632446	test: 0.634365

Epoch: 124
Loss: 0.22636595070362092
RMSE train: 0.422745	val: 0.817185	test: 0.811024
MAE train: 0.329218	val: 0.621689	test: 0.624068

Epoch: 125
Loss: 0.23550121188163758
RMSE train: 0.410307	val: 0.820218	test: 0.809015
MAE train: 0.319994	val: 0.622593	test: 0.627526

Epoch: 126
Loss: 0.22766309529542922
RMSE train: 0.402464	val: 0.801349	test: 0.797104
MAE train: 0.312923	val: 0.609981	test: 0.614643

Epoch: 127
Loss: 0.2379646494984627
RMSE train: 0.412104	val: 0.801563	test: 0.799939
MAE train: 0.323502	val: 0.615046	test: 0.621143

Epoch: 128
Loss: 0.2330896481871605
RMSE train: 0.417935	val: 0.834053	test: 0.825922
MAE train: 0.323466	val: 0.630801	test: 0.637750

Epoch: 129
Loss: 0.223090760409832
RMSE train: 0.411981	val: 0.823164	test: 0.820074
MAE train: 0.320029	val: 0.628585	test: 0.629153

Epoch: 130
Loss: 0.2184049516916275
RMSE train: 0.417663	val: 0.835393	test: 0.826772
MAE train: 0.324833	val: 0.635310	test: 0.640259

Epoch: 131
Loss: 0.22766617983579635
RMSE train: 0.397670	val: 0.808241	test: 0.798241
MAE train: 0.308935	val: 0.612465	test: 0.612997

Epoch: 132
Loss: 0.21870393455028533
RMSE train: 0.404598	val: 0.825563	test: 0.809202
MAE train: 0.314957	val: 0.626576	test: 0.622290

Epoch: 133
Loss: 0.222806878387928
RMSE train: 0.413519	val: 0.800058	test: 0.800024
MAE train: 0.325594	val: 0.610651	test: 0.618129

Epoch: 134
Loss: 0.22181593775749206
RMSE train: 0.417701	val: 0.803918	test: 0.790602
MAE train: 0.330152	val: 0.613330	test: 0.614633

Epoch: 135
Loss: 0.22537251561880112
RMSE train: 0.401701	val: 0.821483	test: 0.812946
MAE train: 0.312537	val: 0.623913	test: 0.621017

Epoch: 136
Loss: 0.21511989831924438
RMSE train: 0.412371	val: 0.835045	test: 0.820694
MAE train: 0.319275	val: 0.630148	test: 0.630122

Epoch: 137
Loss: 0.2091581329703331
RMSE train: 0.405459	val: 0.820741	test: 0.807454
MAE train: 0.313753	val: 0.619426	test: 0.616440

Epoch: 138
Loss: 0.20761553943157196
RMSE train: 0.399673	val: 0.808845	test: 0.804587
MAE train: 0.313601	val: 0.616413	test: 0.613134

Epoch: 139
Loss: 0.21483470052480697
RMSE train: 0.391564	val: 0.810378	test: 0.801389
MAE train: 0.305295	val: 0.621854	test: 0.613890

Epoch: 140
Loss: 0.2198801651597023
RMSE train: 0.399477	val: 0.813332	test: 0.803207
MAE train: 0.311676	val: 0.619917	test: 0.616572

Epoch: 141
Loss: 0.21253686845302583
RMSE train: 0.398751	val: 0.797709	test: 0.786003
MAE train: 0.311909	val: 0.611668	test: 0.606918

Epoch: 142
Loss: 0.20733963549137116
RMSE train: 0.399181	val: 0.805435	test: 0.802342
MAE train: 0.311025	val: 0.615541	test: 0.617719

Epoch: 143
Loss: 0.20214813351631164
RMSE train: 0.387119	val: 0.811805	test: 0.803913
MAE train: 0.299860	val: 0.617086	test: 0.616428
MAE train: 0.401149	val: 0.618683	test: 0.614780

Epoch: 84
Loss: 0.2918403781950474
RMSE train: 0.496154	val: 0.811272	test: 0.758307
MAE train: 0.388074	val: 0.614496	test: 0.596022

Epoch: 85
Loss: 0.2955842713514964
RMSE train: 0.515152	val: 0.821647	test: 0.773874
MAE train: 0.399719	val: 0.620203	test: 0.612123

Epoch: 86
Loss: 0.2866050253311793
RMSE train: 0.515685	val: 0.821308	test: 0.769933
MAE train: 0.403679	val: 0.621039	test: 0.613599

Epoch: 87
Loss: 0.3023084153731664
RMSE train: 0.550879	val: 0.859558	test: 0.833170
MAE train: 0.431430	val: 0.658493	test: 0.659443

Epoch: 88
Loss: 0.2731526183585326
RMSE train: 0.475148	val: 0.787150	test: 0.743814
MAE train: 0.369885	val: 0.588249	test: 0.584408

Epoch: 89
Loss: 0.28409377733866376
RMSE train: 0.479292	val: 0.793623	test: 0.754973
MAE train: 0.373824	val: 0.595239	test: 0.591894

Epoch: 90
Loss: 0.2978410944342613
RMSE train: 0.478823	val: 0.806958	test: 0.763681
MAE train: 0.372242	val: 0.604309	test: 0.605388

Epoch: 91
Loss: 0.2920408857365449
RMSE train: 0.483178	val: 0.815991	test: 0.782717
MAE train: 0.374249	val: 0.604861	test: 0.615892

Epoch: 92
Loss: 0.2774045566717784
RMSE train: 0.491151	val: 0.814873	test: 0.773138
MAE train: 0.383396	val: 0.608340	test: 0.609416

Epoch: 93
Loss: 0.2681536450982094
RMSE train: 0.485336	val: 0.825535	test: 0.778654
MAE train: 0.379792	val: 0.618113	test: 0.609198

Epoch: 94
Loss: 0.27406703184048337
RMSE train: 0.502376	val: 0.823794	test: 0.787674
MAE train: 0.390910	val: 0.621313	test: 0.617159

Epoch: 95
Loss: 0.26698843389749527
RMSE train: 0.504499	val: 0.827925	test: 0.792260
MAE train: 0.395902	val: 0.625891	test: 0.623604

Epoch: 96
Loss: 0.26944716523091
RMSE train: 0.471403	val: 0.797683	test: 0.752941
MAE train: 0.367517	val: 0.598132	test: 0.596167

Epoch: 97
Loss: 0.2675936110317707
RMSE train: 0.483812	val: 0.814493	test: 0.773999
MAE train: 0.377739	val: 0.607959	test: 0.613084

Epoch: 98
Loss: 0.258914561321338
RMSE train: 0.458954	val: 0.809082	test: 0.770025
MAE train: 0.355705	val: 0.604710	test: 0.603276

Epoch: 99
Loss: 0.27463534101843834
RMSE train: 0.497243	val: 0.843984	test: 0.795441
MAE train: 0.386013	val: 0.636729	test: 0.630136

Epoch: 100
Loss: 0.26700780540704727
RMSE train: 0.496947	val: 0.836120	test: 0.798812
MAE train: 0.386837	val: 0.629218	test: 0.624744

Epoch: 101
Loss: 0.25743868574500084
RMSE train: 0.447475	val: 0.783700	test: 0.750678
MAE train: 0.347687	val: 0.580021	test: 0.587728

Epoch: 102
Loss: 0.2663680960734685
RMSE train: 0.462152	val: 0.803675	test: 0.767987
MAE train: 0.357353	val: 0.598430	test: 0.604694

Epoch: 103
Loss: 0.2722904036442439
RMSE train: 0.474751	val: 0.808094	test: 0.769552
MAE train: 0.368232	val: 0.606554	test: 0.605513

Epoch: 104
Loss: 0.2619223805765311
RMSE train: 0.476897	val: 0.794635	test: 0.768418
MAE train: 0.369845	val: 0.597183	test: 0.598063

Epoch: 105
Loss: 0.26893331731359166
RMSE train: 0.452337	val: 0.788884	test: 0.751238
MAE train: 0.351448	val: 0.588437	test: 0.591927

Epoch: 106
Loss: 0.263418963799874
RMSE train: 0.496956	val: 0.827721	test: 0.786108
MAE train: 0.387717	val: 0.625482	test: 0.620056

Epoch: 107
Loss: 0.2629472663005193
RMSE train: 0.445398	val: 0.794019	test: 0.757597
MAE train: 0.344653	val: 0.592936	test: 0.599198

Epoch: 108
Loss: 0.2644432211915652
RMSE train: 0.511115	val: 0.838860	test: 0.799858
MAE train: 0.400985	val: 0.635439	test: 0.630739

Epoch: 109
Loss: 0.25514591485261917
RMSE train: 0.493246	val: 0.818766	test: 0.783725
MAE train: 0.384870	val: 0.621429	test: 0.618140

Epoch: 110
Loss: 0.24936881164709726
RMSE train: 0.445933	val: 0.796603	test: 0.753703
MAE train: 0.347114	val: 0.593932	test: 0.589192

Epoch: 111
Loss: 0.2470839867989222
RMSE train: 0.514449	val: 0.855249	test: 0.809947
MAE train: 0.404204	val: 0.649571	test: 0.642569

Epoch: 112
Loss: 0.23759870479504266
RMSE train: 0.468215	val: 0.817328	test: 0.785557
MAE train: 0.363643	val: 0.612057	test: 0.615076

Epoch: 113
Loss: 0.2397005669772625
RMSE train: 0.442021	val: 0.807418	test: 0.774608
MAE train: 0.341680	val: 0.598256	test: 0.605788

Epoch: 114
Loss: 0.25102899596095085
RMSE train: 0.473892	val: 0.817458	test: 0.779113
MAE train: 0.369075	val: 0.616283	test: 0.612558

Epoch: 115
Loss: 0.2406687152882417
RMSE train: 0.462242	val: 0.826981	test: 0.783269
MAE train: 0.360119	val: 0.619341	test: 0.617576

Epoch: 116
Loss: 0.24946392451723418
RMSE train: 0.451245	val: 0.788387	test: 0.743503
MAE train: 0.350894	val: 0.588173	test: 0.584297

Epoch: 117
Loss: 0.24273083979884783
RMSE train: 0.443460	val: 0.809305	test: 0.755902
MAE train: 0.345171	val: 0.603429	test: 0.596430

Epoch: 118
Loss: 0.23916088913877806
RMSE train: 0.456148	val: 0.804319	test: 0.767139
MAE train: 0.356485	val: 0.608648	test: 0.601242

Epoch: 119
Loss: 0.246654582520326
RMSE train: 0.431330	val: 0.797766	test: 0.755753
MAE train: 0.333421	val: 0.597139	test: 0.597557

Epoch: 120
Loss: 0.24549122775594392
RMSE train: 0.439770	val: 0.797788	test: 0.754838
MAE train: 0.341633	val: 0.590091	test: 0.590092

Epoch: 121
Loss: 0.24311585227648416
RMSE train: 0.445338	val: 0.801047	test: 0.759753
MAE train: 0.346108	val: 0.600965	test: 0.599634

Epoch: 122
Loss: 0.2350877821445465
RMSE train: 0.452367	val: 0.805542	test: 0.764884
MAE train: 0.353582	val: 0.600856	test: 0.601949

Epoch: 123
Loss: 0.24775955826044083
RMSE train: 0.442756	val: 0.800002	test: 0.760274
MAE train: 0.344614	val: 0.601665	test: 0.595756

Epoch: 124
Loss: 0.23425625264644623
RMSE train: 0.466469	val: 0.794950	test: 0.763237
MAE train: 0.358483	val: 0.601700	test: 0.596178

Epoch: 125
Loss: 0.23586000377933183
RMSE train: 0.444425	val: 0.807496	test: 0.770524
MAE train: 0.345350	val: 0.606581	test: 0.604338

Epoch: 126
Loss: 0.21866030991077423
RMSE train: 0.441909	val: 0.799780	test: 0.763943
MAE train: 0.342326	val: 0.597349	test: 0.600076

Epoch: 127
Loss: 0.22944002225995064
RMSE train: 0.442650	val: 0.796925	test: 0.762188
MAE train: 0.343694	val: 0.597197	test: 0.592030

Epoch: 128
Loss: 0.22616680959860483
RMSE train: 0.435484	val: 0.798939	test: 0.759487
MAE train: 0.337199	val: 0.598722	test: 0.593481

Epoch: 129
Loss: 0.2374260015785694
RMSE train: 0.465607	val: 0.839834	test: 0.798168
MAE train: 0.363534	val: 0.631140	test: 0.630414

Epoch: 130
Loss: 0.23519681145747504
RMSE train: 0.446226	val: 0.813367	test: 0.768441
MAE train: 0.348606	val: 0.608650	test: 0.606488

Epoch: 131
Loss: 0.23148527989784876
RMSE train: 0.429256	val: 0.785778	test: 0.753122
MAE train: 0.331963	val: 0.593272	test: 0.587881

Epoch: 132
Loss: 0.2308456115424633
RMSE train: 0.432696	val: 0.788225	test: 0.748272
MAE train: 0.334110	val: 0.594711	test: 0.589489

Epoch: 133
Loss: 0.23080036540826163
RMSE train: 0.443912	val: 0.816283	test: 0.771680
MAE train: 0.345578	val: 0.610441	test: 0.606982

Epoch: 134
Loss: 0.21267209326227507
RMSE train: 0.450244	val: 0.818397	test: 0.785926
MAE train: 0.351961	val: 0.615857	test: 0.615423

Epoch: 135
Loss: 0.21557383860150972
RMSE train: 0.447181	val: 0.821210	test: 0.777618
MAE train: 0.351043	val: 0.615924	test: 0.612134

Epoch: 136
Loss: 0.22173474604884783
RMSE train: 0.470463	val: 0.838485	test: 0.803110
MAE train: 0.368572	val: 0.635097	test: 0.631085

Early stopping
Best (RMSE):	 train: 0.447475	val: 0.783700	test: 0.750678
Best (MAE):	 train: 0.347687	val: 0.580021	test: 0.587728

MAE train: 0.370903	val: 0.589674	test: 0.598867

Epoch: 84
Loss: 0.29384760682781536
RMSE train: 0.473405	val: 0.778652	test: 0.750813
MAE train: 0.366470	val: 0.580194	test: 0.584858

Epoch: 85
Loss: 0.30110028634468716
RMSE train: 0.471240	val: 0.803949	test: 0.757613
MAE train: 0.364811	val: 0.594418	test: 0.596286

Epoch: 86
Loss: 0.30574260155359906
RMSE train: 0.473745	val: 0.821976	test: 0.775119
MAE train: 0.367062	val: 0.606301	test: 0.608676

Epoch: 87
Loss: 0.2841901530822118
RMSE train: 0.456148	val: 0.791854	test: 0.749365
MAE train: 0.354590	val: 0.588107	test: 0.589589

Epoch: 88
Loss: 0.28330836196740466
RMSE train: 0.476072	val: 0.797890	test: 0.754954
MAE train: 0.369436	val: 0.591849	test: 0.592680

Epoch: 89
Loss: 0.28683193897207576
RMSE train: 0.499788	val: 0.810619	test: 0.773404
MAE train: 0.388269	val: 0.603034	test: 0.606951

Epoch: 90
Loss: 0.2989364564418793
RMSE train: 0.453611	val: 0.805654	test: 0.767350
MAE train: 0.350537	val: 0.596468	test: 0.606077

Epoch: 91
Loss: 0.27199412137269974
RMSE train: 0.477213	val: 0.808229	test: 0.764940
MAE train: 0.367611	val: 0.594894	test: 0.601633

Epoch: 92
Loss: 0.2769699605802695
RMSE train: 0.462324	val: 0.794687	test: 0.743311
MAE train: 0.360169	val: 0.592374	test: 0.587512

Epoch: 93
Loss: 0.27101850261290866
RMSE train: 0.459837	val: 0.797486	test: 0.769197
MAE train: 0.356883	val: 0.591180	test: 0.605287

Epoch: 94
Loss: 0.2689632400870323
RMSE train: 0.454093	val: 0.789890	test: 0.757345
MAE train: 0.350397	val: 0.591426	test: 0.597522

Epoch: 95
Loss: 0.28685859094063443
RMSE train: 0.454102	val: 0.800615	test: 0.755654
MAE train: 0.350032	val: 0.593292	test: 0.591599

Epoch: 96
Loss: 0.26945022866129875
RMSE train: 0.470844	val: 0.801157	test: 0.755806
MAE train: 0.364978	val: 0.599177	test: 0.594116

Epoch: 97
Loss: 0.265677393724521
RMSE train: 0.449813	val: 0.778577	test: 0.737017
MAE train: 0.347413	val: 0.580325	test: 0.579730

Epoch: 98
Loss: 0.25722836578885716
RMSE train: 0.470023	val: 0.797911	test: 0.763673
MAE train: 0.364921	val: 0.593316	test: 0.595989

Epoch: 99
Loss: 0.2668613518277804
RMSE train: 0.453375	val: 0.799621	test: 0.763341
MAE train: 0.352265	val: 0.590360	test: 0.602088

Epoch: 100
Loss: 0.2608175426721573
RMSE train: 0.444903	val: 0.788831	test: 0.745198
MAE train: 0.346306	val: 0.583695	test: 0.579386

Epoch: 101
Loss: 0.26147743066151935
RMSE train: 0.447793	val: 0.794407	test: 0.750622
MAE train: 0.347086	val: 0.585879	test: 0.588062

Epoch: 102
Loss: 0.2625679572423299
RMSE train: 0.439800	val: 0.789196	test: 0.753442
MAE train: 0.339474	val: 0.582773	test: 0.590120

Epoch: 103
Loss: 0.2627224052945773
RMSE train: 0.449270	val: 0.810743	test: 0.779204
MAE train: 0.346513	val: 0.600390	test: 0.605785

Epoch: 104
Loss: 0.2557441865404447
RMSE train: 0.445741	val: 0.781557	test: 0.752321
MAE train: 0.345774	val: 0.582478	test: 0.593097

Epoch: 105
Loss: 0.25461237753431004
RMSE train: 0.458432	val: 0.777632	test: 0.759824
MAE train: 0.354143	val: 0.584204	test: 0.597996

Epoch: 106
Loss: 0.2603703799347083
RMSE train: 0.445947	val: 0.764534	test: 0.740905
MAE train: 0.349194	val: 0.573453	test: 0.582817

Epoch: 107
Loss: 0.2582230145732562
RMSE train: 0.432763	val: 0.779870	test: 0.761286
MAE train: 0.332449	val: 0.574795	test: 0.596425

Epoch: 108
Loss: 0.2532229336599509
RMSE train: 0.436156	val: 0.785264	test: 0.756192
MAE train: 0.336870	val: 0.581286	test: 0.590862

Epoch: 109
Loss: 0.24835235501329103
RMSE train: 0.446076	val: 0.800953	test: 0.763059
MAE train: 0.345431	val: 0.596036	test: 0.599727

Epoch: 110
Loss: 0.25406649832924205
RMSE train: 0.456323	val: 0.799104	test: 0.762467
MAE train: 0.354803	val: 0.594451	test: 0.603377

Epoch: 111
Loss: 0.2580452151596546
RMSE train: 0.442159	val: 0.796426	test: 0.756772
MAE train: 0.342336	val: 0.585355	test: 0.590897

Epoch: 112
Loss: 0.24943300212423006
RMSE train: 0.430642	val: 0.771990	test: 0.737663
MAE train: 0.334159	val: 0.572950	test: 0.580845

Epoch: 113
Loss: 0.24465815847118697
RMSE train: 0.442903	val: 0.793515	test: 0.769143
MAE train: 0.342847	val: 0.594051	test: 0.600901

Epoch: 114
Loss: 0.24899980798363686
RMSE train: 0.464598	val: 0.799563	test: 0.771024
MAE train: 0.361229	val: 0.598076	test: 0.610958

Epoch: 115
Loss: 0.23965499674280485
RMSE train: 0.423273	val: 0.787907	test: 0.744641
MAE train: 0.325494	val: 0.578759	test: 0.586798

Epoch: 116
Loss: 0.24527689069509506
RMSE train: 0.449759	val: 0.801296	test: 0.753522
MAE train: 0.347960	val: 0.592692	test: 0.595372

Epoch: 117
Loss: 0.25272221490740776
RMSE train: 0.455125	val: 0.802880	test: 0.767764
MAE train: 0.351868	val: 0.595466	test: 0.600702

Epoch: 118
Loss: 0.24200955778360367
RMSE train: 0.432359	val: 0.804565	test: 0.755995
MAE train: 0.335025	val: 0.590276	test: 0.593324

Epoch: 119
Loss: 0.23878256604075432
RMSE train: 0.448248	val: 0.805842	test: 0.763539
MAE train: 0.347257	val: 0.595911	test: 0.598810

Epoch: 120
Loss: 0.23772162944078445
RMSE train: 0.447694	val: 0.821761	test: 0.767817
MAE train: 0.346573	val: 0.599335	test: 0.602128

Epoch: 121
Loss: 0.24415748193860054
RMSE train: 0.421845	val: 0.781294	test: 0.731934
MAE train: 0.325283	val: 0.575669	test: 0.576434

Epoch: 122
Loss: 0.24400677780310312
RMSE train: 0.441522	val: 0.804609	test: 0.777390
MAE train: 0.339810	val: 0.596114	test: 0.606155

Epoch: 123
Loss: 0.24147083734472594
RMSE train: 0.422580	val: 0.787935	test: 0.757512
MAE train: 0.324887	val: 0.581572	test: 0.594218

Epoch: 124
Loss: 0.24243737508853277
RMSE train: 0.446457	val: 0.812210	test: 0.764368
MAE train: 0.347110	val: 0.596884	test: 0.595472

Epoch: 125
Loss: 0.2337522767484188
RMSE train: 0.438897	val: 0.809868	test: 0.773983
MAE train: 0.340374	val: 0.603156	test: 0.599904

Epoch: 126
Loss: 0.22327059879899025
RMSE train: 0.443347	val: 0.819740	test: 0.781135
MAE train: 0.345547	val: 0.607992	test: 0.609876

Epoch: 127
Loss: 0.22243350620071092
RMSE train: 0.419338	val: 0.797979	test: 0.744282
MAE train: 0.324075	val: 0.589443	test: 0.582260

Epoch: 128
Loss: 0.23024985194206238
RMSE train: 0.411064	val: 0.793333	test: 0.753047
MAE train: 0.316373	val: 0.585044	test: 0.587761

Epoch: 129
Loss: 0.22858505447705588
RMSE train: 0.431899	val: 0.796914	test: 0.758011
MAE train: 0.334164	val: 0.592591	test: 0.595873

Epoch: 130
Loss: 0.21854964271187782
RMSE train: 0.408037	val: 0.793425	test: 0.753082
MAE train: 0.313918	val: 0.585196	test: 0.587499

Epoch: 131
Loss: 0.2157708965241909
RMSE train: 0.421597	val: 0.792972	test: 0.759647
MAE train: 0.326539	val: 0.585894	test: 0.598232

Epoch: 132
Loss: 0.22744003931681314
RMSE train: 0.424581	val: 0.806849	test: 0.758327
MAE train: 0.330075	val: 0.590396	test: 0.593655

Epoch: 133
Loss: 0.22981687635183334
RMSE train: 0.436979	val: 0.801175	test: 0.748437
MAE train: 0.339015	val: 0.590317	test: 0.584407

Epoch: 134
Loss: 0.22210468476017317
RMSE train: 0.460036	val: 0.833212	test: 0.797941
MAE train: 0.358021	val: 0.621457	test: 0.625768

Epoch: 135
Loss: 0.21670089041193327
RMSE train: 0.404735	val: 0.798390	test: 0.758493
MAE train: 0.313665	val: 0.586489	test: 0.590263

Epoch: 136
Loss: 0.2162138596177101
RMSE train: 0.403941	val: 0.779225	test: 0.746999
MAE train: 0.313316	val: 0.573114	test: 0.584886

Epoch: 137
Loss: 0.2217746563255787
RMSE train: 0.420787	val: 0.786013	test: 0.736572
MAE train: 0.326648	val: 0.584031	test: 0.576125

Epoch: 138
Loss: 0.22308436905344328
RMSE train: 0.440538	val: 0.837876	test: 0.780413
MAE train: 0.340796	val: 0.618246	test: 0.614082

Epoch: 139
Loss: 0.2124732012550036
RMSE train: 0.400149	val: 0.804907	test: 0.745946
MAE train: 0.307167	val: 0.592463	test: 0.581499

Epoch: 140
Loss: 0.2224484123289585
RMSE train: 0.400689	val: 0.779277	test: 0.742703
MAE train: 0.309564	val: 0.580967	test: 0.583340

Epoch: 141
Loss: 0.21207934990525246
RMSE train: 0.406187	val: 0.786470	test: 0.743023
MAE train: 0.314556	val: 0.584890	test: 0.582335

Early stopping
Best (RMSE):	 train: 0.445947	val: 0.764534	test: 0.740905
Best (MAE):	 train: 0.349194	val: 0.573453	test: 0.582817

MAE train: 0.371527	val: 0.604738	test: 0.593392

Epoch: 84
Loss: 0.2885826975107193
RMSE train: 0.488689	val: 0.797846	test: 0.732724
MAE train: 0.381987	val: 0.597811	test: 0.579947

Epoch: 85
Loss: 0.29352402314543724
RMSE train: 0.466246	val: 0.801963	test: 0.747746
MAE train: 0.361073	val: 0.604855	test: 0.585012

Epoch: 86
Loss: 0.30553649614254635
RMSE train: 0.501089	val: 0.804012	test: 0.747619
MAE train: 0.391663	val: 0.610020	test: 0.592276

Epoch: 87
Loss: 0.2929508512218793
RMSE train: 0.493733	val: 0.816754	test: 0.764894
MAE train: 0.384861	val: 0.615427	test: 0.604837

Epoch: 88
Loss: 0.2810182335476081
RMSE train: 0.477707	val: 0.779601	test: 0.738896
MAE train: 0.372938	val: 0.592241	test: 0.582476

Epoch: 89
Loss: 0.2897854062418143
RMSE train: 0.488007	val: 0.797941	test: 0.753763
MAE train: 0.382745	val: 0.604699	test: 0.595348

Epoch: 90
Loss: 0.28859541937708855
RMSE train: 0.482772	val: 0.794053	test: 0.765128
MAE train: 0.374774	val: 0.598918	test: 0.603534

Epoch: 91
Loss: 0.2767794777949651
RMSE train: 0.486416	val: 0.798825	test: 0.753229
MAE train: 0.379867	val: 0.600850	test: 0.594008

Epoch: 92
Loss: 0.27706915388504666
RMSE train: 0.482644	val: 0.794470	test: 0.756637
MAE train: 0.379226	val: 0.598968	test: 0.589665

Epoch: 93
Loss: 0.27450549229979515
RMSE train: 0.473895	val: 0.787512	test: 0.739596
MAE train: 0.368728	val: 0.594854	test: 0.583528

Epoch: 94
Loss: 0.2742706624170144
RMSE train: 0.474044	val: 0.804394	test: 0.751753
MAE train: 0.371516	val: 0.606159	test: 0.589952

Epoch: 95
Loss: 0.2896587550640106
RMSE train: 0.477919	val: 0.807153	test: 0.759854
MAE train: 0.371989	val: 0.609257	test: 0.599830

Epoch: 96
Loss: 0.27411982665459317
RMSE train: 0.474335	val: 0.794076	test: 0.746024
MAE train: 0.373087	val: 0.602217	test: 0.585793

Epoch: 97
Loss: 0.2630137155453364
RMSE train: 0.474320	val: 0.800584	test: 0.758684
MAE train: 0.367545	val: 0.591453	test: 0.595121

Epoch: 98
Loss: 0.2668696815768878
RMSE train: 0.466806	val: 0.805704	test: 0.765481
MAE train: 0.364610	val: 0.605877	test: 0.596404

Epoch: 99
Loss: 0.2759979839126269
RMSE train: 0.460506	val: 0.788563	test: 0.743042
MAE train: 0.361104	val: 0.585428	test: 0.583118

Epoch: 100
Loss: 0.2712731199959914
RMSE train: 0.468241	val: 0.792471	test: 0.759732
MAE train: 0.365311	val: 0.595491	test: 0.596430

Epoch: 101
Loss: 0.24803482989470163
RMSE train: 0.477295	val: 0.793341	test: 0.745247
MAE train: 0.373797	val: 0.593922	test: 0.585620

Epoch: 102
Loss: 0.25819798558950424
RMSE train: 0.448160	val: 0.792496	test: 0.735002
MAE train: 0.348928	val: 0.593523	test: 0.577939

Epoch: 103
Loss: 0.28374914452433586
RMSE train: 0.475921	val: 0.801889	test: 0.744807
MAE train: 0.370297	val: 0.599521	test: 0.586853

Epoch: 104
Loss: 0.27260152995586395
RMSE train: 0.446246	val: 0.788819	test: 0.746065
MAE train: 0.346359	val: 0.594950	test: 0.583905

Epoch: 105
Loss: 0.2741660202542941
RMSE train: 0.454934	val: 0.773620	test: 0.743304
MAE train: 0.354764	val: 0.586867	test: 0.586782

Epoch: 106
Loss: 0.26173538466294605
RMSE train: 0.490639	val: 0.840039	test: 0.795478
MAE train: 0.383675	val: 0.638541	test: 0.623472

Epoch: 107
Loss: 0.2469879314303398
RMSE train: 0.449150	val: 0.799097	test: 0.751227
MAE train: 0.349334	val: 0.601709	test: 0.587261

Epoch: 108
Loss: 0.24552637214461961
RMSE train: 0.440368	val: 0.789846	test: 0.741372
MAE train: 0.343104	val: 0.591352	test: 0.579519

Epoch: 109
Loss: 0.25863317400217056
RMSE train: 0.455383	val: 0.789632	test: 0.734062
MAE train: 0.355652	val: 0.589375	test: 0.579039

Epoch: 110
Loss: 0.24378103390336037
RMSE train: 0.448492	val: 0.797538	test: 0.745506
MAE train: 0.347340	val: 0.596443	test: 0.582440

Epoch: 111
Loss: 0.24321872492631277
RMSE train: 0.445219	val: 0.784580	test: 0.739407
MAE train: 0.346423	val: 0.593890	test: 0.582099

Epoch: 112
Loss: 0.25129854182402295
RMSE train: 0.440298	val: 0.796281	test: 0.743402
MAE train: 0.342637	val: 0.598912	test: 0.581472

Epoch: 113
Loss: 0.24546398098270097
RMSE train: 0.435752	val: 0.780287	test: 0.736000
MAE train: 0.339725	val: 0.590525	test: 0.576147

Epoch: 114
Loss: 0.2493835004667441
RMSE train: 0.447198	val: 0.800482	test: 0.749764
MAE train: 0.347040	val: 0.598462	test: 0.585138

Epoch: 115
Loss: 0.24972465634346008
RMSE train: 0.456281	val: 0.814829	test: 0.764756
MAE train: 0.354858	val: 0.611451	test: 0.599690

Epoch: 116
Loss: 0.25759579613804817
RMSE train: 0.434310	val: 0.799718	test: 0.752606
MAE train: 0.337443	val: 0.603825	test: 0.589430

Epoch: 117
Loss: 0.2389075110356013
RMSE train: 0.463146	val: 0.789933	test: 0.736473
MAE train: 0.362415	val: 0.598682	test: 0.583779

Epoch: 118
Loss: 0.2412119284272194
RMSE train: 0.441148	val: 0.792390	test: 0.736321
MAE train: 0.343702	val: 0.596626	test: 0.581023

Epoch: 119
Loss: 0.24394108355045319
RMSE train: 0.474051	val: 0.805228	test: 0.746207
MAE train: 0.367072	val: 0.603615	test: 0.594605

Epoch: 120
Loss: 0.23635356749097505
RMSE train: 0.429508	val: 0.795342	test: 0.730558
MAE train: 0.333385	val: 0.592575	test: 0.574280

Epoch: 121
Loss: 0.24091052015622458
RMSE train: 0.440165	val: 0.796555	test: 0.742045
MAE train: 0.343506	val: 0.600364	test: 0.582051

Epoch: 122
Loss: 0.2381309283276399
RMSE train: 0.444379	val: 0.777786	test: 0.722517
MAE train: 0.344896	val: 0.582166	test: 0.574998

Epoch: 123
Loss: 0.23988372335831323
RMSE train: 0.444354	val: 0.800962	test: 0.754503
MAE train: 0.347067	val: 0.607227	test: 0.598044

Epoch: 124
Loss: 0.22965280339121819
RMSE train: 0.408975	val: 0.780873	test: 0.726465
MAE train: 0.316227	val: 0.585325	test: 0.569805

Epoch: 125
Loss: 0.23351181174317995
RMSE train: 0.437418	val: 0.781917	test: 0.732463
MAE train: 0.343754	val: 0.589409	test: 0.576006

Epoch: 126
Loss: 0.23439564431707063
RMSE train: 0.425019	val: 0.781846	test: 0.733281
MAE train: 0.329868	val: 0.590910	test: 0.574320

Epoch: 127
Loss: 0.23532423501213393
RMSE train: 0.428740	val: 0.774382	test: 0.731312
MAE train: 0.334323	val: 0.582703	test: 0.577546

Epoch: 128
Loss: 0.22983077789346376
RMSE train: 0.426442	val: 0.769842	test: 0.738309
MAE train: 0.331991	val: 0.580335	test: 0.575953

Epoch: 129
Loss: 0.24298320213953653
RMSE train: 0.432816	val: 0.792882	test: 0.748728
MAE train: 0.334302	val: 0.598630	test: 0.590276

Epoch: 130
Loss: 0.2242982511719068
RMSE train: 0.419048	val: 0.778917	test: 0.730967
MAE train: 0.325755	val: 0.585225	test: 0.575614

Epoch: 131
Loss: 0.22195209314425787
RMSE train: 0.437207	val: 0.817640	test: 0.758266
MAE train: 0.341280	val: 0.617746	test: 0.598999

Epoch: 132
Loss: 0.21793928494056067
RMSE train: 0.433436	val: 0.795274	test: 0.740795
MAE train: 0.338442	val: 0.600143	test: 0.586839

Epoch: 133
Loss: 0.22242451334993044
RMSE train: 0.408342	val: 0.773680	test: 0.725517
MAE train: 0.315901	val: 0.580377	test: 0.573508

Epoch: 134
Loss: 0.23199834302067757
RMSE train: 0.419676	val: 0.769786	test: 0.717864
MAE train: 0.324768	val: 0.576610	test: 0.566701

Epoch: 135
Loss: 0.22215084607402483
RMSE train: 0.417084	val: 0.766159	test: 0.736699
MAE train: 0.323317	val: 0.579446	test: 0.584182

Epoch: 136
Loss: 0.21226261431972185
RMSE train: 0.423896	val: 0.791345	test: 0.727975
MAE train: 0.330222	val: 0.591128	test: 0.576924

Epoch: 137
Loss: 0.22604838386178017
RMSE train: 0.391080	val: 0.786002	test: 0.735077
MAE train: 0.301288	val: 0.594123	test: 0.577843

Epoch: 138
Loss: 0.217227882395188
RMSE train: 0.415712	val: 0.766740	test: 0.722022
MAE train: 0.323407	val: 0.578504	test: 0.568988

Epoch: 139
Loss: 0.22185251116752625
RMSE train: 0.424969	val: 0.805254	test: 0.746987
MAE train: 0.328630	val: 0.600437	test: 0.590209

Epoch: 140
Loss: 0.21578830232222876
RMSE train: 0.429641	val: 0.806877	test: 0.752107
MAE train: 0.335860	val: 0.609651	test: 0.591366

Epoch: 141
Loss: 0.21996435274680456
RMSE train: 0.406550	val: 0.783418	test: 0.745641
MAE train: 0.314354	val: 0.587737	test: 0.582948

Epoch: 142
Loss: 0.2076249197125435
RMSE train: 0.405644	val: 0.778214	test: 0.731680
MAE train: 0.313440	val: 0.582676	test: 0.578515

Epoch: 143
Loss: 0.21184733882546425
RMSE train: 0.392649	val: 0.772086	test: 0.724959
MAE train: 0.301873	val: 0.575741	test: 0.570682

Epoch: 144
Loss: 0.20692474097013475
RMSE train: 0.405290	val: 0.812183	test: 0.820459
MAE train: 0.315063	val: 0.617655	test: 0.633936

Epoch: 145
Loss: 0.20908074527978898
RMSE train: 0.432823	val: 0.797791	test: 0.815331
MAE train: 0.338953	val: 0.607256	test: 0.631361

Epoch: 146
Loss: 0.21236602813005448
RMSE train: 0.412729	val: 0.818534	test: 0.820977
MAE train: 0.322386	val: 0.622179	test: 0.632905

Epoch: 147
Loss: 0.20509167462587358
RMSE train: 0.404140	val: 0.793684	test: 0.800423
MAE train: 0.316261	val: 0.599143	test: 0.623993

Epoch: 148
Loss: 0.20328267216682433
RMSE train: 0.403973	val: 0.813137	test: 0.832285
MAE train: 0.315413	val: 0.621907	test: 0.639920

Epoch: 149
Loss: 0.20731913894414902
RMSE train: 0.406177	val: 0.804247	test: 0.810099
MAE train: 0.318599	val: 0.612783	test: 0.626267

Epoch: 150
Loss: 0.19988676756620408
RMSE train: 0.401161	val: 0.797608	test: 0.801269
MAE train: 0.313651	val: 0.609614	test: 0.621746

Epoch: 151
Loss: 0.20273720175027848
RMSE train: 0.400390	val: 0.790325	test: 0.801857
MAE train: 0.316403	val: 0.602189	test: 0.619972

Epoch: 152
Loss: 0.2099071651697159
RMSE train: 0.409343	val: 0.814635	test: 0.833489
MAE train: 0.318377	val: 0.620994	test: 0.645041

Epoch: 153
Loss: 0.20431850850582123
RMSE train: 0.379739	val: 0.790075	test: 0.802456
MAE train: 0.295542	val: 0.596906	test: 0.618179

Epoch: 154
Loss: 0.19610717743635178
RMSE train: 0.392329	val: 0.789535	test: 0.800164
MAE train: 0.308192	val: 0.597738	test: 0.618751

Epoch: 155
Loss: 0.1955787420272827
RMSE train: 0.396062	val: 0.805185	test: 0.814066
MAE train: 0.311693	val: 0.611448	test: 0.625604

Epoch: 156
Loss: 0.19184771031141282
RMSE train: 0.385317	val: 0.799956	test: 0.812718
MAE train: 0.300145	val: 0.606825	test: 0.623878

Epoch: 157
Loss: 0.20513663142919542
RMSE train: 0.395198	val: 0.801032	test: 0.814111
MAE train: 0.309428	val: 0.603779	test: 0.628189

Epoch: 158
Loss: 0.2023131161928177
RMSE train: 0.397066	val: 0.807825	test: 0.818585
MAE train: 0.311405	val: 0.611184	test: 0.633439

Epoch: 159
Loss: 0.2015022322535515
RMSE train: 0.388002	val: 0.812653	test: 0.817231
MAE train: 0.302431	val: 0.616924	test: 0.632238

Epoch: 160
Loss: 0.19450830817222595
RMSE train: 0.400992	val: 0.804024	test: 0.806284
MAE train: 0.312621	val: 0.607292	test: 0.621890

Epoch: 161
Loss: 0.19878152012825012
RMSE train: 0.392057	val: 0.807698	test: 0.805218
MAE train: 0.308346	val: 0.614592	test: 0.619268

Epoch: 162
Loss: 0.18782414942979814
RMSE train: 0.372759	val: 0.794759	test: 0.802347
MAE train: 0.291275	val: 0.600942	test: 0.616568

Epoch: 163
Loss: 0.19075224697589874
RMSE train: 0.395550	val: 0.824026	test: 0.835882
MAE train: 0.308882	val: 0.625445	test: 0.641741

Epoch: 164
Loss: 0.19535528719425202
RMSE train: 0.392648	val: 0.822588	test: 0.827323
MAE train: 0.305351	val: 0.626903	test: 0.637240

Epoch: 165
Loss: 0.1868957057595253
RMSE train: 0.396422	val: 0.800459	test: 0.807861
MAE train: 0.312925	val: 0.608447	test: 0.623405

Epoch: 166
Loss: 0.19445144832134248
RMSE train: 0.387452	val: 0.807219	test: 0.811503
MAE train: 0.303581	val: 0.613907	test: 0.626011

Epoch: 167
Loss: 0.18855195641517639
RMSE train: 0.383200	val: 0.798011	test: 0.806669
MAE train: 0.301497	val: 0.608018	test: 0.620613

Early stopping
Best (RMSE):	 train: 0.403242	val: 0.784243	test: 0.789330
Best (MAE):	 train: 0.314304	val: 0.594484	test: 0.608548

MAE train: 0.396979	val: 0.604818	test: 0.589496

Epoch: 84
Loss: 0.3280970688377108
RMSE train: 0.512405	val: 0.783851	test: 0.760200
MAE train: 0.397807	val: 0.600263	test: 0.586964

Epoch: 85
Loss: 0.3295154848269054
RMSE train: 0.520738	val: 0.791107	test: 0.752145
MAE train: 0.401693	val: 0.602249	test: 0.584302

Epoch: 86
Loss: 0.30543267726898193
RMSE train: 0.508891	val: 0.772837	test: 0.744046
MAE train: 0.395362	val: 0.587913	test: 0.585539

Epoch: 87
Loss: 0.31155646060194286
RMSE train: 0.506574	val: 0.787358	test: 0.745337
MAE train: 0.392582	val: 0.601760	test: 0.579137

Epoch: 88
Loss: 0.31265193436826977
RMSE train: 0.522329	val: 0.782830	test: 0.759452
MAE train: 0.405348	val: 0.600586	test: 0.596431

Epoch: 89
Loss: 0.3069868790251868
RMSE train: 0.515086	val: 0.770082	test: 0.750405
MAE train: 0.399210	val: 0.589384	test: 0.586920

Epoch: 90
Loss: 0.3227561925138746
RMSE train: 0.523606	val: 0.788967	test: 0.762763
MAE train: 0.404614	val: 0.595998	test: 0.603225

Epoch: 91
Loss: 0.3216162153652736
RMSE train: 0.518590	val: 0.785511	test: 0.755453
MAE train: 0.405738	val: 0.597790	test: 0.594899

Epoch: 92
Loss: 0.319252572953701
RMSE train: 0.515935	val: 0.789125	test: 0.750720
MAE train: 0.398141	val: 0.600428	test: 0.590862

Epoch: 93
Loss: 0.3193680303437369
RMSE train: 0.522548	val: 0.795201	test: 0.752784
MAE train: 0.403994	val: 0.606553	test: 0.590483

Epoch: 94
Loss: 0.3052952481167657
RMSE train: 0.486163	val: 0.751575	test: 0.732417
MAE train: 0.375037	val: 0.575952	test: 0.572760

Epoch: 95
Loss: 0.3222229374306543
RMSE train: 0.514130	val: 0.799987	test: 0.745835
MAE train: 0.394926	val: 0.614073	test: 0.579787

Epoch: 96
Loss: 0.3170205588851656
RMSE train: 0.500415	val: 0.768950	test: 0.742256
MAE train: 0.390658	val: 0.596306	test: 0.575765

Epoch: 97
Loss: 0.3004386212144579
RMSE train: 0.502711	val: 0.764004	test: 0.732912
MAE train: 0.388378	val: 0.586124	test: 0.571252

Epoch: 98
Loss: 0.30464685069663183
RMSE train: 0.507348	val: 0.778423	test: 0.744008
MAE train: 0.392208	val: 0.597035	test: 0.582041

Epoch: 99
Loss: 0.30121093881981714
RMSE train: 0.508567	val: 0.793626	test: 0.762086
MAE train: 0.395294	val: 0.609473	test: 0.599783

Epoch: 100
Loss: 0.2908261577997889
RMSE train: 0.501131	val: 0.766069	test: 0.736963
MAE train: 0.389928	val: 0.587694	test: 0.578403

Epoch: 101
Loss: 0.3002753534487316
RMSE train: 0.482294	val: 0.766667	test: 0.746612
MAE train: 0.374931	val: 0.583857	test: 0.583596

Epoch: 102
Loss: 0.2916914416210992
RMSE train: 0.500117	val: 0.767710	test: 0.743872
MAE train: 0.388561	val: 0.591353	test: 0.583756

Epoch: 103
Loss: 0.30801554024219513
RMSE train: 0.471245	val: 0.773668	test: 0.743479
MAE train: 0.361353	val: 0.587039	test: 0.575160

Epoch: 104
Loss: 0.30332550193582264
RMSE train: 0.496218	val: 0.760385	test: 0.742888
MAE train: 0.383602	val: 0.583837	test: 0.586857

Epoch: 105
Loss: 0.288126072713307
RMSE train: 0.491486	val: 0.766975	test: 0.726024
MAE train: 0.381388	val: 0.586659	test: 0.570687

Epoch: 106
Loss: 0.283291115292481
RMSE train: 0.476430	val: 0.755510	test: 0.742244
MAE train: 0.368170	val: 0.579244	test: 0.580016

Epoch: 107
Loss: 0.2842425288898604
RMSE train: 0.485911	val: 0.757772	test: 0.741639
MAE train: 0.377686	val: 0.585189	test: 0.579252

Epoch: 108
Loss: 0.2810078114271164
RMSE train: 0.472996	val: 0.765373	test: 0.738534
MAE train: 0.363368	val: 0.591233	test: 0.580068

Epoch: 109
Loss: 0.26471176211323055
RMSE train: 0.486402	val: 0.776092	test: 0.734488
MAE train: 0.375195	val: 0.597939	test: 0.574371

Epoch: 110
Loss: 0.29190119888101307
RMSE train: 0.503050	val: 0.793841	test: 0.752501
MAE train: 0.390275	val: 0.610613	test: 0.588701

Epoch: 111
Loss: 0.2732342023934637
RMSE train: 0.478087	val: 0.774541	test: 0.740944
MAE train: 0.368815	val: 0.592382	test: 0.577800

Epoch: 112
Loss: 0.27558726391621996
RMSE train: 0.472633	val: 0.752965	test: 0.740977
MAE train: 0.367300	val: 0.579686	test: 0.578923

Epoch: 113
Loss: 0.26928615889378954
RMSE train: 0.467545	val: 0.745323	test: 0.745195
MAE train: 0.361937	val: 0.571161	test: 0.581965

Epoch: 114
Loss: 0.2774836612599237
RMSE train: 0.485239	val: 0.773593	test: 0.743427
MAE train: 0.376598	val: 0.595425	test: 0.582810

Epoch: 115
Loss: 0.2620887021933283
RMSE train: 0.466079	val: 0.750468	test: 0.749732
MAE train: 0.360027	val: 0.575689	test: 0.585482

Epoch: 116
Loss: 0.27648468954222544
RMSE train: 0.458326	val: 0.765482	test: 0.734797
MAE train: 0.353858	val: 0.588111	test: 0.574564

Epoch: 117
Loss: 0.2796021157077381
RMSE train: 0.511439	val: 0.800643	test: 0.756536
MAE train: 0.398543	val: 0.616786	test: 0.597170

Epoch: 118
Loss: 0.2721485101750919
RMSE train: 0.472828	val: 0.793597	test: 0.751980
MAE train: 0.363963	val: 0.601286	test: 0.584833

Epoch: 119
Loss: 0.25828216544219423
RMSE train: 0.482958	val: 0.779747	test: 0.740593
MAE train: 0.374285	val: 0.598943	test: 0.584640

Epoch: 120
Loss: 0.2680417373776436
RMSE train: 0.462103	val: 0.761188	test: 0.740267
MAE train: 0.358387	val: 0.582965	test: 0.585659

Epoch: 121
Loss: 0.2985648491552898
RMSE train: 0.470792	val: 0.761282	test: 0.740089
MAE train: 0.367504	val: 0.583410	test: 0.584631

Epoch: 122
Loss: 0.27709837896483286
RMSE train: 0.498427	val: 0.769900	test: 0.758496
MAE train: 0.392734	val: 0.596621	test: 0.598746

Epoch: 123
Loss: 0.27818995607750757
RMSE train: 0.476618	val: 0.772488	test: 0.747659
MAE train: 0.370805	val: 0.594500	test: 0.592501

Epoch: 124
Loss: 0.2599613421729633
RMSE train: 0.454848	val: 0.756731	test: 0.746036
MAE train: 0.353102	val: 0.586388	test: 0.588122

Epoch: 125
Loss: 0.26109361222812105
RMSE train: 0.474009	val: 0.786212	test: 0.735771
MAE train: 0.367349	val: 0.603441	test: 0.576136

Epoch: 126
Loss: 0.2622319310903549
RMSE train: 0.448525	val: 0.757140	test: 0.725851
MAE train: 0.346745	val: 0.581338	test: 0.573794

Epoch: 127
Loss: 0.26111595439059393
RMSE train: 0.465545	val: 0.789078	test: 0.743316
MAE train: 0.361140	val: 0.602046	test: 0.584048

Epoch: 128
Loss: 0.2496600438441549
RMSE train: 0.473451	val: 0.757302	test: 0.734679
MAE train: 0.366351	val: 0.586256	test: 0.582566

Epoch: 129
Loss: 0.24611107579299382
RMSE train: 0.467631	val: 0.753974	test: 0.740567
MAE train: 0.365082	val: 0.584768	test: 0.588033

Epoch: 130
Loss: 0.24784753152302333
RMSE train: 0.444550	val: 0.737651	test: 0.747036
MAE train: 0.342495	val: 0.568111	test: 0.587513

Epoch: 131
Loss: 0.25858314228909357
RMSE train: 0.480215	val: 0.777891	test: 0.748275
MAE train: 0.373059	val: 0.593594	test: 0.587143

Epoch: 132
Loss: 0.26168221661022734
RMSE train: 0.459814	val: 0.753428	test: 0.746117
MAE train: 0.358235	val: 0.584707	test: 0.582580

Epoch: 133
Loss: 0.2562324638877596
RMSE train: 0.459333	val: 0.753005	test: 0.731937
MAE train: 0.355292	val: 0.574954	test: 0.573200

Epoch: 134
Loss: 0.2688445917197636
RMSE train: 0.457608	val: 0.740939	test: 0.744855
MAE train: 0.356567	val: 0.570984	test: 0.586280

Epoch: 135
Loss: 0.26069322121994837
RMSE train: 0.449880	val: 0.750253	test: 0.739099
MAE train: 0.347756	val: 0.578715	test: 0.583151

Epoch: 136
Loss: 0.2502875764455114
RMSE train: 0.452642	val: 0.773414	test: 0.745925
MAE train: 0.350024	val: 0.596947	test: 0.580201

Epoch: 137
Loss: 0.2745240916098867
RMSE train: 0.451104	val: 0.759730	test: 0.744290
MAE train: 0.350300	val: 0.584331	test: 0.579623

Epoch: 138
Loss: 0.26292142804179874
RMSE train: 0.451685	val: 0.764875	test: 0.744117
MAE train: 0.348383	val: 0.583886	test: 0.585178

Epoch: 139
Loss: 0.25871180423668455
RMSE train: 0.445171	val: 0.765193	test: 0.740770
MAE train: 0.344781	val: 0.585466	test: 0.581019

Epoch: 140
Loss: 0.24270463309117726
RMSE train: 0.446852	val: 0.760599	test: 0.746132
MAE train: 0.345049	val: 0.579499	test: 0.582595

Epoch: 141
Loss: 0.26020739121096476
RMSE train: 0.466289	val: 0.765538	test: 0.733236
MAE train: 0.365898	val: 0.589572	test: 0.583083

Epoch: 142
Loss: 0.25242396550519125
RMSE train: 0.446106	val: 0.753394	test: 0.742496
MAE train: 0.343826	val: 0.576530	test: 0.586471

Epoch: 143
Loss: 0.2452713123389653
RMSE train: 0.495628	val: 0.803546	test: 0.769915
MAE train: 0.385385	val: 0.613320	test: 0.605793
MAE train: 0.415900	val: 0.592469	test: 0.588408

Epoch: 84
Loss: 0.3149742824690683
RMSE train: 0.524633	val: 0.773686	test: 0.750732
MAE train: 0.406742	val: 0.589543	test: 0.590648

Epoch: 85
Loss: 0.3064060317618506
RMSE train: 0.507697	val: 0.763667	test: 0.740493
MAE train: 0.395075	val: 0.579408	test: 0.576881

Epoch: 86
Loss: 0.33881909932409016
RMSE train: 0.515612	val: 0.769588	test: 0.754119
MAE train: 0.398007	val: 0.588203	test: 0.588525

Epoch: 87
Loss: 0.30443403124809265
RMSE train: 0.526095	val: 0.777176	test: 0.766706
MAE train: 0.405705	val: 0.591406	test: 0.595221

Epoch: 88
Loss: 0.31495445328099386
RMSE train: 0.496671	val: 0.769696	test: 0.748682
MAE train: 0.387049	val: 0.582901	test: 0.585207

Epoch: 89
Loss: 0.3088122734001705
RMSE train: 0.504939	val: 0.766354	test: 0.746759
MAE train: 0.391216	val: 0.585347	test: 0.585257

Epoch: 90
Loss: 0.30603991874626707
RMSE train: 0.504456	val: 0.773067	test: 0.740926
MAE train: 0.391351	val: 0.587532	test: 0.579685

Epoch: 91
Loss: 0.31299311561243875
RMSE train: 0.515723	val: 0.764393	test: 0.739941
MAE train: 0.401247	val: 0.581908	test: 0.584986

Epoch: 92
Loss: 0.3142517251627786
RMSE train: 0.491069	val: 0.756883	test: 0.732282
MAE train: 0.381840	val: 0.574219	test: 0.574491

Epoch: 93
Loss: 0.3159934399383409
RMSE train: 0.514062	val: 0.755032	test: 0.755562
MAE train: 0.400792	val: 0.579309	test: 0.589572

Epoch: 94
Loss: 0.29893012451274054
RMSE train: 0.549528	val: 0.796653	test: 0.774175
MAE train: 0.431109	val: 0.613045	test: 0.608859

Epoch: 95
Loss: 0.29730857802288874
RMSE train: 0.509595	val: 0.781743	test: 0.753402
MAE train: 0.395766	val: 0.593354	test: 0.581470

Epoch: 96
Loss: 0.2951898617403848
RMSE train: 0.488480	val: 0.760065	test: 0.741516
MAE train: 0.376573	val: 0.578911	test: 0.580374

Epoch: 97
Loss: 0.2843053234475
RMSE train: 0.495199	val: 0.757291	test: 0.756458
MAE train: 0.383347	val: 0.578571	test: 0.586868

Epoch: 98
Loss: 0.2977005999003138
RMSE train: 0.510482	val: 0.770270	test: 0.749219
MAE train: 0.396116	val: 0.590458	test: 0.583764

Epoch: 99
Loss: 0.2931716325027602
RMSE train: 0.505762	val: 0.760807	test: 0.746941
MAE train: 0.391052	val: 0.581364	test: 0.579848

Epoch: 100
Loss: 0.31398886122873854
RMSE train: 0.479356	val: 0.755111	test: 0.751720
MAE train: 0.369501	val: 0.576341	test: 0.582485

Epoch: 101
Loss: 0.3023261236292975
RMSE train: 0.501248	val: 0.791536	test: 0.746047
MAE train: 0.388285	val: 0.610213	test: 0.574093

Epoch: 102
Loss: 0.305098152586392
RMSE train: 0.508689	val: 0.784026	test: 0.744723
MAE train: 0.393824	val: 0.598819	test: 0.576263

Epoch: 103
Loss: 0.28894515335559845
RMSE train: 0.475789	val: 0.754531	test: 0.731179
MAE train: 0.367371	val: 0.576525	test: 0.562096

Epoch: 104
Loss: 0.2969966062477657
RMSE train: 0.521298	val: 0.809245	test: 0.751552
MAE train: 0.402162	val: 0.612937	test: 0.588280

Epoch: 105
Loss: 0.2897772512265614
RMSE train: 0.479273	val: 0.762688	test: 0.723995
MAE train: 0.368658	val: 0.580601	test: 0.562931

Epoch: 106
Loss: 0.284317315689155
RMSE train: 0.481575	val: 0.767487	test: 0.733944
MAE train: 0.373713	val: 0.580742	test: 0.569061

Epoch: 107
Loss: 0.28792982548475266
RMSE train: 0.500530	val: 0.770965	test: 0.742142
MAE train: 0.392292	val: 0.583123	test: 0.577512

Epoch: 108
Loss: 0.29684531475816456
RMSE train: 0.489670	val: 0.761790	test: 0.741550
MAE train: 0.378806	val: 0.578614	test: 0.578385

Epoch: 109
Loss: 0.288381678717477
RMSE train: 0.485966	val: 0.753193	test: 0.727837
MAE train: 0.378036	val: 0.569664	test: 0.561959

Epoch: 110
Loss: 0.28107334673404694
RMSE train: 0.505141	val: 0.762873	test: 0.729913
MAE train: 0.392613	val: 0.577602	test: 0.575943

Epoch: 111
Loss: 0.27268231553690775
RMSE train: 0.512836	val: 0.795394	test: 0.756737
MAE train: 0.398110	val: 0.607159	test: 0.594525

Epoch: 112
Loss: 0.2743974189673151
RMSE train: 0.483883	val: 0.775760	test: 0.738222
MAE train: 0.377380	val: 0.585705	test: 0.576337

Epoch: 113
Loss: 0.28251899565969196
RMSE train: 0.478031	val: 0.753496	test: 0.728973
MAE train: 0.371329	val: 0.574642	test: 0.571544

Epoch: 114
Loss: 0.27561421585934504
RMSE train: 0.487388	val: 0.779473	test: 0.749108
MAE train: 0.378349	val: 0.598816	test: 0.587336

Epoch: 115
Loss: 0.2891910065497671
RMSE train: 0.479766	val: 0.757435	test: 0.732729
MAE train: 0.373317	val: 0.577584	test: 0.573822

Epoch: 116
Loss: 0.2823985815048218
RMSE train: 0.457164	val: 0.761679	test: 0.738349
MAE train: 0.350552	val: 0.576421	test: 0.572081

Epoch: 117
Loss: 0.2751774000270026
RMSE train: 0.484640	val: 0.759378	test: 0.729964
MAE train: 0.374092	val: 0.577857	test: 0.573905

Epoch: 118
Loss: 0.27535837782280786
RMSE train: 0.471254	val: 0.762278	test: 0.736322
MAE train: 0.364345	val: 0.582563	test: 0.576975

Epoch: 119
Loss: 0.31604704367262976
RMSE train: 0.476689	val: 0.771280	test: 0.722812
MAE train: 0.369739	val: 0.586592	test: 0.565093

Epoch: 120
Loss: 0.26848392082112177
RMSE train: 0.465131	val: 0.756358	test: 0.733306
MAE train: 0.357929	val: 0.576070	test: 0.577863

Epoch: 121
Loss: 0.2723937864814486
RMSE train: 0.476843	val: 0.782433	test: 0.739759
MAE train: 0.366108	val: 0.598093	test: 0.574924

Epoch: 122
Loss: 0.2798806865300451
RMSE train: 0.490863	val: 0.786340	test: 0.750706
MAE train: 0.379591	val: 0.599190	test: 0.583191

Epoch: 123
Loss: 0.27245887368917465
RMSE train: 0.496038	val: 0.802681	test: 0.763559
MAE train: 0.384046	val: 0.607379	test: 0.595908

Epoch: 124
Loss: 0.27057030690567835
RMSE train: 0.467336	val: 0.760359	test: 0.739491
MAE train: 0.362393	val: 0.578901	test: 0.574774

Epoch: 125
Loss: 0.27212228200265337
RMSE train: 0.478875	val: 0.782989	test: 0.751067
MAE train: 0.370468	val: 0.596315	test: 0.587316

Epoch: 126
Loss: 0.2722243049315044
RMSE train: 0.472121	val: 0.764156	test: 0.743724
MAE train: 0.366764	val: 0.586703	test: 0.581418

Epoch: 127
Loss: 0.26064833253622055
RMSE train: 0.470315	val: 0.770639	test: 0.740050
MAE train: 0.363983	val: 0.586466	test: 0.579713

Epoch: 128
Loss: 0.2457869617002351
RMSE train: 0.456687	val: 0.764185	test: 0.740495
MAE train: 0.351878	val: 0.581487	test: 0.578336

Epoch: 129
Loss: 0.2559662493211882
RMSE train: 0.451243	val: 0.759330	test: 0.732588
MAE train: 0.348525	val: 0.583915	test: 0.568241

Epoch: 130
Loss: 0.2500762715935707
RMSE train: 0.448127	val: 0.764896	test: 0.735988
MAE train: 0.345545	val: 0.585325	test: 0.564542

Epoch: 131
Loss: 0.2647957482508251
RMSE train: 0.464079	val: 0.767629	test: 0.735340
MAE train: 0.359984	val: 0.586652	test: 0.570784

Epoch: 132
Loss: 0.27347867084400995
RMSE train: 0.450390	val: 0.761812	test: 0.734245
MAE train: 0.347862	val: 0.580724	test: 0.565036

Epoch: 133
Loss: 0.2475788476211684
RMSE train: 0.457762	val: 0.753483	test: 0.735023
MAE train: 0.356296	val: 0.571708	test: 0.572028

Epoch: 134
Loss: 0.25019320100545883
RMSE train: 0.451171	val: 0.760724	test: 0.734716
MAE train: 0.348445	val: 0.582906	test: 0.573098

Epoch: 135
Loss: 0.26146828063896727
RMSE train: 0.452533	val: 0.767782	test: 0.736776
MAE train: 0.349936	val: 0.583892	test: 0.569914

Epoch: 136
Loss: 0.2612216068165643
RMSE train: 0.447295	val: 0.770034	test: 0.727072
MAE train: 0.345351	val: 0.585570	test: 0.564775

Epoch: 137
Loss: 0.23908159136772156
RMSE train: 0.471055	val: 0.783134	test: 0.747829
MAE train: 0.367046	val: 0.597971	test: 0.578752

Epoch: 138
Loss: 0.2396392556173461
RMSE train: 0.451585	val: 0.750885	test: 0.745483
MAE train: 0.350347	val: 0.566121	test: 0.580745

Epoch: 139
Loss: 0.2448910272547177
RMSE train: 0.442227	val: 0.759094	test: 0.741914
MAE train: 0.341840	val: 0.576343	test: 0.572562

Epoch: 140
Loss: 0.253589177770274
RMSE train: 0.460848	val: 0.747185	test: 0.734053
MAE train: 0.359169	val: 0.572679	test: 0.577005

Epoch: 141
Loss: 0.2679626824600356
RMSE train: 0.451049	val: 0.753502	test: 0.713311
MAE train: 0.347731	val: 0.583196	test: 0.558134

Epoch: 142
Loss: 0.25055346318653654
RMSE train: 0.445108	val: 0.750404	test: 0.730899
MAE train: 0.345505	val: 0.574140	test: 0.573256

Epoch: 143
Loss: 0.23713756033352443
RMSE train: 0.445580	val: 0.757679	test: 0.727803
MAE train: 0.343842	val: 0.577805	test: 0.565029
MAE train: 0.410670	val: 0.613270	test: 0.605269

Epoch: 84
Loss: 0.33351592080933706
RMSE train: 0.524438	val: 0.782932	test: 0.762302
MAE train: 0.407160	val: 0.598741	test: 0.596074

Epoch: 85
Loss: 0.3192727044224739
RMSE train: 0.523646	val: 0.781055	test: 0.766340
MAE train: 0.406811	val: 0.594279	test: 0.609072

Epoch: 86
Loss: 0.33437348689351765
RMSE train: 0.531399	val: 0.791397	test: 0.756685
MAE train: 0.415832	val: 0.607632	test: 0.598725

Epoch: 87
Loss: 0.3358145271028791
RMSE train: 0.576144	val: 0.824208	test: 0.795019
MAE train: 0.450004	val: 0.635010	test: 0.627093

Epoch: 88
Loss: 0.3287806659936905
RMSE train: 0.512240	val: 0.775282	test: 0.759474
MAE train: 0.396901	val: 0.597711	test: 0.591740

Epoch: 89
Loss: 0.32874117685215815
RMSE train: 0.545848	val: 0.806850	test: 0.765852
MAE train: 0.425691	val: 0.626786	test: 0.605803

Epoch: 90
Loss: 0.3180003740957805
RMSE train: 0.513580	val: 0.803366	test: 0.758948
MAE train: 0.398760	val: 0.612901	test: 0.594555

Epoch: 91
Loss: 0.3157572192805154
RMSE train: 0.513875	val: 0.766680	test: 0.752547
MAE train: 0.400181	val: 0.587065	test: 0.588593

Epoch: 92
Loss: 0.3047084169728415
RMSE train: 0.508304	val: 0.778633	test: 0.755855
MAE train: 0.393885	val: 0.590758	test: 0.597576

Epoch: 93
Loss: 0.2940526625939778
RMSE train: 0.503021	val: 0.790836	test: 0.755653
MAE train: 0.390074	val: 0.605161	test: 0.595016

Epoch: 94
Loss: 0.30437015848500387
RMSE train: 0.502469	val: 0.781547	test: 0.750107
MAE train: 0.390267	val: 0.598897	test: 0.585642

Epoch: 95
Loss: 0.3087595073240144
RMSE train: 0.518621	val: 0.783785	test: 0.781177
MAE train: 0.403545	val: 0.602078	test: 0.609742

Epoch: 96
Loss: 0.3239802462714059
RMSE train: 0.515475	val: 0.811057	test: 0.757281
MAE train: 0.400856	val: 0.617646	test: 0.592166

Epoch: 97
Loss: 0.3246323657887323
RMSE train: 0.517993	val: 0.768338	test: 0.752934
MAE train: 0.404411	val: 0.596686	test: 0.598547

Epoch: 98
Loss: 0.3142183646559715
RMSE train: 0.529562	val: 0.810097	test: 0.767785
MAE train: 0.412022	val: 0.619552	test: 0.605552

Epoch: 99
Loss: 0.3135555216244289
RMSE train: 0.496571	val: 0.787830	test: 0.759677
MAE train: 0.385518	val: 0.599535	test: 0.595795

Epoch: 100
Loss: 0.30378088248627527
RMSE train: 0.519592	val: 0.798359	test: 0.754671
MAE train: 0.404953	val: 0.609667	test: 0.592062

Epoch: 101
Loss: 0.3005907301391874
RMSE train: 0.503691	val: 0.779993	test: 0.753082
MAE train: 0.392219	val: 0.595463	test: 0.593500

Epoch: 102
Loss: 0.2999745362571308
RMSE train: 0.486876	val: 0.774377	test: 0.742816
MAE train: 0.377606	val: 0.593939	test: 0.584671

Epoch: 103
Loss: 0.29000759550503324
RMSE train: 0.481520	val: 0.769943	test: 0.739051
MAE train: 0.373428	val: 0.588441	test: 0.577388

Epoch: 104
Loss: 0.28321515981640133
RMSE train: 0.482940	val: 0.769917	test: 0.748538
MAE train: 0.373570	val: 0.591221	test: 0.587052

Epoch: 105
Loss: 0.28761424869298935
RMSE train: 0.494267	val: 0.776498	test: 0.755500
MAE train: 0.383389	val: 0.594007	test: 0.595659

Epoch: 106
Loss: 0.29799709362643106
RMSE train: 0.490112	val: 0.793513	test: 0.755215
MAE train: 0.380441	val: 0.606794	test: 0.589725

Epoch: 107
Loss: 0.29104245560509817
RMSE train: 0.516123	val: 0.810205	test: 0.759186
MAE train: 0.401662	val: 0.614830	test: 0.598354

Epoch: 108
Loss: 0.28535835232053486
RMSE train: 0.469384	val: 0.775765	test: 0.747307
MAE train: 0.362213	val: 0.590266	test: 0.582892

Epoch: 109
Loss: 0.2686685472726822
RMSE train: 0.484908	val: 0.785673	test: 0.732483
MAE train: 0.377850	val: 0.601214	test: 0.572705

Epoch: 110
Loss: 0.29951491739068714
RMSE train: 0.481939	val: 0.781794	test: 0.738619
MAE train: 0.374933	val: 0.596290	test: 0.576457

Epoch: 111
Loss: 0.27729567885398865
RMSE train: 0.479816	val: 0.801686	test: 0.746678
MAE train: 0.372717	val: 0.610612	test: 0.583659

Epoch: 112
Loss: 0.28572986487831387
RMSE train: 0.476814	val: 0.771680	test: 0.731013
MAE train: 0.370829	val: 0.589564	test: 0.571374

Epoch: 113
Loss: 0.282708723630224
RMSE train: 0.460173	val: 0.768724	test: 0.753020
MAE train: 0.356384	val: 0.586975	test: 0.584423

Epoch: 114
Loss: 0.2936472999198096
RMSE train: 0.485410	val: 0.781670	test: 0.749976
MAE train: 0.380268	val: 0.602180	test: 0.588619

Epoch: 115
Loss: 0.2748079704386847
RMSE train: 0.490452	val: 0.771879	test: 0.757784
MAE train: 0.382459	val: 0.588731	test: 0.590680

Epoch: 116
Loss: 0.28458969188588007
RMSE train: 0.485501	val: 0.773363	test: 0.740930
MAE train: 0.379872	val: 0.592394	test: 0.578468

Epoch: 117
Loss: 0.2853818493230002
RMSE train: 0.474150	val: 0.778402	test: 0.738690
MAE train: 0.369249	val: 0.594019	test: 0.572520

Epoch: 118
Loss: 0.3141595125198364
RMSE train: 0.479846	val: 0.774588	test: 0.737547
MAE train: 0.376271	val: 0.590332	test: 0.578466

Epoch: 119
Loss: 0.28013873419591356
RMSE train: 0.467968	val: 0.781611	test: 0.742635
MAE train: 0.364386	val: 0.593820	test: 0.584105

Epoch: 120
Loss: 0.28041844602142063
RMSE train: 0.495385	val: 0.780374	test: 0.745461
MAE train: 0.390207	val: 0.598331	test: 0.587589

Epoch: 121
Loss: 0.29142571666410994
RMSE train: 0.471502	val: 0.781372	test: 0.740298
MAE train: 0.367069	val: 0.594514	test: 0.572980

Epoch: 122
Loss: 0.2741094423191888
RMSE train: 0.473795	val: 0.788663	test: 0.757129
MAE train: 0.368035	val: 0.596992	test: 0.589770

Epoch: 123
Loss: 0.2825691018785749
RMSE train: 0.453586	val: 0.764215	test: 0.737496
MAE train: 0.352177	val: 0.579000	test: 0.571085

Epoch: 124
Loss: 0.27637317031621933
RMSE train: 0.475814	val: 0.772300	test: 0.760592
MAE train: 0.371140	val: 0.591109	test: 0.596775

Epoch: 125
Loss: 0.26973605368818554
RMSE train: 0.461186	val: 0.772640	test: 0.740532
MAE train: 0.358377	val: 0.584297	test: 0.580217

Epoch: 126
Loss: 0.2597372382879257
RMSE train: 0.483947	val: 0.763300	test: 0.754583
MAE train: 0.377762	val: 0.581345	test: 0.591687

Epoch: 127
Loss: 0.257955150944846
RMSE train: 0.463674	val: 0.767313	test: 0.758500
MAE train: 0.362079	val: 0.585340	test: 0.590891

Epoch: 128
Loss: 0.26472033453839167
RMSE train: 0.447803	val: 0.764282	test: 0.748511
MAE train: 0.346429	val: 0.580273	test: 0.576887

Epoch: 129
Loss: 0.26037595740386416
RMSE train: 0.463581	val: 0.774611	test: 0.769334
MAE train: 0.360214	val: 0.591547	test: 0.595620

Epoch: 130
Loss: 0.26173040590115954
RMSE train: 0.463127	val: 0.765957	test: 0.764124
MAE train: 0.362110	val: 0.581985	test: 0.591642

Epoch: 131
Loss: 0.2684135671172823
RMSE train: 0.448544	val: 0.760519	test: 0.741715
MAE train: 0.351007	val: 0.576600	test: 0.574503

Epoch: 132
Loss: 0.25697510902370724
RMSE train: 0.468920	val: 0.764714	test: 0.758808
MAE train: 0.365797	val: 0.589211	test: 0.587341

Epoch: 133
Loss: 0.25262716731854845
RMSE train: 0.485564	val: 0.800818	test: 0.779471
MAE train: 0.378626	val: 0.612960	test: 0.608050

Epoch: 134
Loss: 0.2681177162698337
RMSE train: 0.454204	val: 0.769387	test: 0.749372
MAE train: 0.356887	val: 0.586946	test: 0.582065

Epoch: 135
Loss: 0.27587746190173285
RMSE train: 0.462311	val: 0.773799	test: 0.754690
MAE train: 0.361651	val: 0.591969	test: 0.583860

Epoch: 136
Loss: 0.2610234266945294
RMSE train: 0.449208	val: 0.764322	test: 0.751309
MAE train: 0.350201	val: 0.582085	test: 0.581971

Epoch: 137
Loss: 0.26776119747332167
RMSE train: 0.451021	val: 0.770440	test: 0.764069
MAE train: 0.351314	val: 0.595966	test: 0.589281

Epoch: 138
Loss: 0.2582156072769846
RMSE train: 0.458768	val: 0.772069	test: 0.761945
MAE train: 0.359019	val: 0.598460	test: 0.585896

Epoch: 139
Loss: 0.2710837200284004
RMSE train: 0.458684	val: 0.773179	test: 0.770006
MAE train: 0.359893	val: 0.594670	test: 0.592086

Epoch: 140
Loss: 0.25856450838702066
RMSE train: 0.455896	val: 0.786072	test: 0.764403
MAE train: 0.356339	val: 0.601496	test: 0.591094

Epoch: 141
Loss: 0.2521930922354971
RMSE train: 0.445299	val: 0.762608	test: 0.767628
MAE train: 0.345338	val: 0.584981	test: 0.583341

Epoch: 142
Loss: 0.23619683193308966
RMSE train: 0.441712	val: 0.762526	test: 0.753419
MAE train: 0.342804	val: 0.579877	test: 0.583013

Epoch: 143
Loss: 0.24750378834349768
RMSE train: 0.442483	val: 0.757356	test: 0.750162
MAE train: 0.345283	val: 0.577948	test: 0.586324

Epoch: 144
Loss: 0.21496817320585251
RMSE train: 0.406622	val: 0.810273	test: 0.807576
MAE train: 0.317134	val: 0.613427	test: 0.627044

Epoch: 145
Loss: 0.19802132695913316
RMSE train: 0.381998	val: 0.802496	test: 0.791785
MAE train: 0.297312	val: 0.606477	test: 0.614559

Epoch: 146
Loss: 0.19548759013414382
RMSE train: 0.409189	val: 0.802656	test: 0.799028
MAE train: 0.319913	val: 0.610174	test: 0.615971

Epoch: 147
Loss: 0.20642853975296022
RMSE train: 0.397451	val: 0.803164	test: 0.801524
MAE train: 0.308232	val: 0.608642	test: 0.620626

Epoch: 148
Loss: 0.2033047929406166
RMSE train: 0.390168	val: 0.799133	test: 0.783140
MAE train: 0.303950	val: 0.605831	test: 0.609380

Epoch: 149
Loss: 0.20179763585329055
RMSE train: 0.395143	val: 0.801342	test: 0.784290
MAE train: 0.308799	val: 0.610757	test: 0.607691

Epoch: 150
Loss: 0.21675419360399245
RMSE train: 0.445145	val: 0.838569	test: 0.832114
MAE train: 0.348907	val: 0.638009	test: 0.638590

Epoch: 151
Loss: 0.2063797876238823
RMSE train: 0.403697	val: 0.817480	test: 0.812839
MAE train: 0.314205	val: 0.621011	test: 0.627938

Epoch: 152
Loss: 0.2031451180577278
RMSE train: 0.402819	val: 0.789294	test: 0.798046
MAE train: 0.314573	val: 0.604185	test: 0.619342

Epoch: 153
Loss: 0.20497002452611923
RMSE train: 0.381275	val: 0.807633	test: 0.806453
MAE train: 0.296088	val: 0.611100	test: 0.620954

Epoch: 154
Loss: 0.20341988354921342
RMSE train: 0.400188	val: 0.797478	test: 0.792096
MAE train: 0.313885	val: 0.607743	test: 0.613814

Epoch: 155
Loss: 0.21086384057998658
RMSE train: 0.402062	val: 0.812522	test: 0.812727
MAE train: 0.313611	val: 0.618952	test: 0.625306

Epoch: 156
Loss: 0.19941850304603576
RMSE train: 0.387580	val: 0.795235	test: 0.794726
MAE train: 0.301739	val: 0.600688	test: 0.611664

Epoch: 157
Loss: 0.19450919330120087
RMSE train: 0.380898	val: 0.805178	test: 0.784889
MAE train: 0.294889	val: 0.607741	test: 0.606323

Epoch: 158
Loss: 0.2073078989982605
RMSE train: 0.396276	val: 0.789879	test: 0.786567
MAE train: 0.311026	val: 0.600893	test: 0.607232

Epoch: 159
Loss: 0.19894541501998902
RMSE train: 0.395526	val: 0.809798	test: 0.808909
MAE train: 0.308501	val: 0.613954	test: 0.621705

Epoch: 160
Loss: 0.18791576772928237
RMSE train: 0.387557	val: 0.809405	test: 0.803625
MAE train: 0.301448	val: 0.609868	test: 0.617863

Epoch: 161
Loss: 0.19401389211416245
RMSE train: 0.391893	val: 0.813873	test: 0.819164
MAE train: 0.304300	val: 0.620727	test: 0.629248

Epoch: 162
Loss: 0.19626755714416505
RMSE train: 0.376772	val: 0.792764	test: 0.794814
MAE train: 0.290424	val: 0.598436	test: 0.610521

Epoch: 163
Loss: 0.19237244576215745
RMSE train: 0.385780	val: 0.798710	test: 0.789981
MAE train: 0.299614	val: 0.605437	test: 0.608906

Epoch: 164
Loss: 0.18777144700288773
RMSE train: 0.396129	val: 0.803820	test: 0.797762
MAE train: 0.310964	val: 0.611697	test: 0.611551

Epoch: 165
Loss: 0.1942386269569397
RMSE train: 0.385388	val: 0.806851	test: 0.796851
MAE train: 0.300552	val: 0.613983	test: 0.613255

Epoch: 166
Loss: 0.20506596118211745
RMSE train: 0.394761	val: 0.813009	test: 0.802606
MAE train: 0.306186	val: 0.616504	test: 0.619527

Epoch: 167
Loss: 0.18343667834997177
RMSE train: 0.395008	val: 0.818246	test: 0.810882
MAE train: 0.309091	val: 0.619170	test: 0.624935

Epoch: 168
Loss: 0.17809538692235946
RMSE train: 0.382127	val: 0.796928	test: 0.792236
MAE train: 0.298072	val: 0.604008	test: 0.610020

Epoch: 169
Loss: 0.18826996833086013
RMSE train: 0.377952	val: 0.803242	test: 0.789929
MAE train: 0.295437	val: 0.608124	test: 0.608614

Epoch: 170
Loss: 0.1817821353673935
RMSE train: 0.367322	val: 0.798806	test: 0.785743
MAE train: 0.286169	val: 0.602360	test: 0.601064

Epoch: 171
Loss: 0.18992107808589936
RMSE train: 0.385947	val: 0.808314	test: 0.802743
MAE train: 0.299781	val: 0.611494	test: 0.615114

Epoch: 172
Loss: 0.18207863122224807
RMSE train: 0.385323	val: 0.806473	test: 0.793182
MAE train: 0.299339	val: 0.610256	test: 0.612987

Epoch: 173
Loss: 0.18002554327249526
RMSE train: 0.370061	val: 0.807680	test: 0.793371
MAE train: 0.287445	val: 0.614123	test: 0.609526

Epoch: 174
Loss: 0.18254566192626953
RMSE train: 0.391200	val: 0.814153	test: 0.803469
MAE train: 0.305929	val: 0.617278	test: 0.620740

Epoch: 175
Loss: 0.18715015053749084
RMSE train: 0.374671	val: 0.810376	test: 0.804991
MAE train: 0.292387	val: 0.615547	test: 0.615039

Epoch: 176
Loss: 0.19493286162614823
RMSE train: 0.367359	val: 0.802524	test: 0.789140
MAE train: 0.287215	val: 0.605119	test: 0.603945

Epoch: 177
Loss: 0.18179025799036025
RMSE train: 0.408629	val: 0.850266	test: 0.828602
MAE train: 0.319005	val: 0.646032	test: 0.640333

Epoch: 178
Loss: 0.19873782843351365
RMSE train: 0.381365	val: 0.814745	test: 0.805639
MAE train: 0.296794	val: 0.611399	test: 0.615590

Epoch: 179
Loss: 0.1767416074872017
RMSE train: 0.372781	val: 0.814377	test: 0.808170
MAE train: 0.288063	val: 0.613172	test: 0.618610

Epoch: 180
Loss: 0.18391957730054856
RMSE train: 0.373515	val: 0.793753	test: 0.792222
MAE train: 0.290721	val: 0.603823	test: 0.607111

Epoch: 181
Loss: 0.1798060044646263
RMSE train: 0.370677	val: 0.803155	test: 0.797872
MAE train: 0.286864	val: 0.609251	test: 0.609984

Epoch: 182
Loss: 0.17223767638206483
RMSE train: 0.372775	val: 0.801028	test: 0.796185
MAE train: 0.288667	val: 0.606572	test: 0.609409

Epoch: 183
Loss: 0.17483630329370498
RMSE train: 0.389036	val: 0.823038	test: 0.804195
MAE train: 0.302911	val: 0.621605	test: 0.621615

Epoch: 184
Loss: 0.18039492666721343
RMSE train: 0.371490	val: 0.809128	test: 0.792595
MAE train: 0.286947	val: 0.611766	test: 0.609865

Epoch: 185
Loss: 0.18161317557096482
RMSE train: 0.370741	val: 0.796774	test: 0.788349
MAE train: 0.287541	val: 0.602401	test: 0.607509

Epoch: 186
Loss: 0.18647468090057373
RMSE train: 0.372431	val: 0.803474	test: 0.799534
MAE train: 0.288177	val: 0.606488	test: 0.614246

Epoch: 187
Loss: 0.18175257593393326
RMSE train: 0.385800	val: 0.825566	test: 0.814887
MAE train: 0.300030	val: 0.624522	test: 0.624578

Early stopping
Best (RMSE):	 train: 0.402819	val: 0.789294	test: 0.798046
Best (MAE):	 train: 0.314573	val: 0.604185	test: 0.619342


Epoch: 144
Loss: 0.2054188847541809
RMSE train: 0.400880	val: 0.826457	test: 0.818949
MAE train: 0.308374	val: 0.624096	test: 0.623642

Epoch: 145
Loss: 0.2197543516755104
RMSE train: 0.393125	val: 0.807960	test: 0.788650
MAE train: 0.308048	val: 0.611316	test: 0.606836

Epoch: 146
Loss: 0.21398260891437532
RMSE train: 0.388639	val: 0.817335	test: 0.807466
MAE train: 0.301061	val: 0.620816	test: 0.617892

Epoch: 147
Loss: 0.2044585183262825
RMSE train: 0.399502	val: 0.813747	test: 0.816368
MAE train: 0.311259	val: 0.623578	test: 0.627906

Epoch: 148
Loss: 0.2097040072083473
RMSE train: 0.386752	val: 0.808811	test: 0.796313
MAE train: 0.301998	val: 0.613711	test: 0.610280

Epoch: 149
Loss: 0.19698793441057205
RMSE train: 0.383711	val: 0.821686	test: 0.809197
MAE train: 0.296704	val: 0.623299	test: 0.619385

Epoch: 150
Loss: 0.1923151731491089
RMSE train: 0.383401	val: 0.810418	test: 0.807607
MAE train: 0.298833	val: 0.616182	test: 0.619806

Epoch: 151
Loss: 0.2009023293852806
RMSE train: 0.392328	val: 0.833171	test: 0.827381
MAE train: 0.304395	val: 0.630801	test: 0.631955

Epoch: 152
Loss: 0.20570273250341414
RMSE train: 0.385255	val: 0.824243	test: 0.813222
MAE train: 0.299433	val: 0.621014	test: 0.622021

Epoch: 153
Loss: 0.19427284002304077
RMSE train: 0.389067	val: 0.828444	test: 0.825979
MAE train: 0.302152	val: 0.628155	test: 0.629586

Epoch: 154
Loss: 0.19862571656703948
RMSE train: 0.391706	val: 0.813822	test: 0.810158
MAE train: 0.306014	val: 0.618770	test: 0.620303

Epoch: 155
Loss: 0.20013923943042755
RMSE train: 0.374836	val: 0.813508	test: 0.810749
MAE train: 0.290797	val: 0.615334	test: 0.618062

Epoch: 156
Loss: 0.20643206238746642
RMSE train: 0.388378	val: 0.799483	test: 0.794444
MAE train: 0.303413	val: 0.608460	test: 0.608283

Epoch: 157
Loss: 0.1947152465581894
RMSE train: 0.393475	val: 0.812431	test: 0.808300
MAE train: 0.309540	val: 0.622881	test: 0.621419

Epoch: 158
Loss: 0.19634289294481277
RMSE train: 0.382083	val: 0.794520	test: 0.799293
MAE train: 0.298154	val: 0.604321	test: 0.614634

Epoch: 159
Loss: 0.20510012656450272
RMSE train: 0.398777	val: 0.823038	test: 0.832518
MAE train: 0.312253	val: 0.628136	test: 0.644553

Epoch: 160
Loss: 0.19590001851320266
RMSE train: 0.373669	val: 0.794105	test: 0.802956
MAE train: 0.290177	val: 0.607326	test: 0.614462

Epoch: 161
Loss: 0.19985174834728242
RMSE train: 0.387684	val: 0.821862	test: 0.820449
MAE train: 0.301200	val: 0.625944	test: 0.632418

Epoch: 162
Loss: 0.19901469796895982
RMSE train: 0.391286	val: 0.824619	test: 0.820490
MAE train: 0.302247	val: 0.630321	test: 0.627909

Epoch: 163
Loss: 0.19319234937429428
RMSE train: 0.385642	val: 0.808058	test: 0.807753
MAE train: 0.299079	val: 0.614701	test: 0.618091

Epoch: 164
Loss: 0.19133933037519454
RMSE train: 0.376067	val: 0.800119	test: 0.802999
MAE train: 0.290442	val: 0.605171	test: 0.613424

Epoch: 165
Loss: 0.19209743440151214
RMSE train: 0.386693	val: 0.803857	test: 0.807322
MAE train: 0.299489	val: 0.611026	test: 0.620662

Epoch: 166
Loss: 0.1950662225484848
RMSE train: 0.380084	val: 0.798775	test: 0.803433
MAE train: 0.293977	val: 0.605741	test: 0.616890

Epoch: 167
Loss: 0.19351068288087844
RMSE train: 0.373656	val: 0.801416	test: 0.804094
MAE train: 0.288685	val: 0.608546	test: 0.614969

Epoch: 168
Loss: 0.1868014246225357
RMSE train: 0.382691	val: 0.791956	test: 0.797447
MAE train: 0.299585	val: 0.605415	test: 0.613301

Epoch: 169
Loss: 0.18815560191869735
RMSE train: 0.372170	val: 0.796286	test: 0.808906
MAE train: 0.289510	val: 0.604643	test: 0.623956

Epoch: 170
Loss: 0.1853911206126213
RMSE train: 0.364052	val: 0.801430	test: 0.808004
MAE train: 0.282053	val: 0.609338	test: 0.617601

Epoch: 171
Loss: 0.18689542263746262
RMSE train: 0.377421	val: 0.813746	test: 0.816840
MAE train: 0.291481	val: 0.618841	test: 0.626805

Epoch: 172
Loss: 0.18988267779350282
RMSE train: 0.385583	val: 0.795390	test: 0.795744
MAE train: 0.302310	val: 0.609320	test: 0.612282

Epoch: 173
Loss: 0.18929391354322433
RMSE train: 0.366537	val: 0.798580	test: 0.807131
MAE train: 0.282638	val: 0.609975	test: 0.615848

Epoch: 174
Loss: 0.1837793618440628
RMSE train: 0.373102	val: 0.792961	test: 0.798145
MAE train: 0.292057	val: 0.606491	test: 0.612492

Epoch: 175
Loss: 0.1795707404613495
RMSE train: 0.377305	val: 0.789900	test: 0.799938
MAE train: 0.296560	val: 0.601809	test: 0.612986

Epoch: 176
Loss: 0.17483175694942474
RMSE train: 0.376208	val: 0.784661	test: 0.796213
MAE train: 0.297348	val: 0.599960	test: 0.609835

Epoch: 177
Loss: 0.1866297036409378
RMSE train: 0.388292	val: 0.798994	test: 0.808864
MAE train: 0.302977	val: 0.608207	test: 0.616852

Epoch: 178
Loss: 0.18485210388898848
RMSE train: 0.364191	val: 0.795639	test: 0.806570
MAE train: 0.283627	val: 0.604398	test: 0.614996

Epoch: 179
Loss: 0.19723433554172515
RMSE train: 0.384044	val: 0.813733	test: 0.818906
MAE train: 0.300962	val: 0.618904	test: 0.626597

Epoch: 180
Loss: 0.1786494344472885
RMSE train: 0.380674	val: 0.819482	test: 0.836635
MAE train: 0.296205	val: 0.622622	test: 0.639867

Epoch: 181
Loss: 0.18977484107017517
RMSE train: 0.371816	val: 0.800762	test: 0.815698
MAE train: 0.287438	val: 0.609291	test: 0.626104

Epoch: 182
Loss: 0.1780695676803589
RMSE train: 0.362766	val: 0.798948	test: 0.811720
MAE train: 0.280853	val: 0.607439	test: 0.618033

Epoch: 183
Loss: 0.18328404277563096
RMSE train: 0.377186	val: 0.790979	test: 0.799613
MAE train: 0.292616	val: 0.602889	test: 0.614750

Epoch: 184
Loss: 0.18209192603826524
RMSE train: 0.386060	val: 0.814884	test: 0.814798
MAE train: 0.299852	val: 0.615141	test: 0.618246

Epoch: 185
Loss: 0.18260613083839417
RMSE train: 0.368203	val: 0.803334	test: 0.803639
MAE train: 0.286419	val: 0.608954	test: 0.613236

Epoch: 186
Loss: 0.17639868557453156
RMSE train: 0.368601	val: 0.796984	test: 0.804731
MAE train: 0.287348	val: 0.607422	test: 0.617517

Epoch: 187
Loss: 0.17943834662437438
RMSE train: 0.366328	val: 0.800754	test: 0.814100
MAE train: 0.282449	val: 0.610787	test: 0.621509

Epoch: 188
Loss: 0.17900387793779374
RMSE train: 0.356528	val: 0.801761	test: 0.806136
MAE train: 0.276398	val: 0.607860	test: 0.614487

Epoch: 189
Loss: 0.176625494658947
RMSE train: 0.361124	val: 0.799743	test: 0.800766
MAE train: 0.280844	val: 0.606252	test: 0.616153

Epoch: 190
Loss: 0.17627663910388947
RMSE train: 0.364434	val: 0.795586	test: 0.803775
MAE train: 0.284470	val: 0.606308	test: 0.619567

Epoch: 191
Loss: 0.17708075195550918
RMSE train: 0.365364	val: 0.793915	test: 0.796519
MAE train: 0.285700	val: 0.598407	test: 0.617081

Epoch: 192
Loss: 0.17396606057882308
RMSE train: 0.351409	val: 0.794469	test: 0.800847
MAE train: 0.272137	val: 0.600715	test: 0.615206

Epoch: 193
Loss: 0.17327187955379486
RMSE train: 0.358705	val: 0.809851	test: 0.814970
MAE train: 0.279081	val: 0.612488	test: 0.623730

Epoch: 194
Loss: 0.16781833320856093
RMSE train: 0.355815	val: 0.797239	test: 0.803823
MAE train: 0.275204	val: 0.601504	test: 0.615155

Epoch: 195
Loss: 0.17423323094844817
RMSE train: 0.368433	val: 0.810384	test: 0.809890
MAE train: 0.287043	val: 0.617392	test: 0.619618

Epoch: 196
Loss: 0.17369966059923173
RMSE train: 0.359642	val: 0.793939	test: 0.797152
MAE train: 0.280050	val: 0.606230	test: 0.610103

Epoch: 197
Loss: 0.16909324526786804
RMSE train: 0.351013	val: 0.808221	test: 0.809022
MAE train: 0.271251	val: 0.611778	test: 0.618482

Epoch: 198
Loss: 0.17208904922008514
RMSE train: 0.348192	val: 0.787896	test: 0.796538
MAE train: 0.269659	val: 0.600696	test: 0.606863

Epoch: 199
Loss: 0.17342349290847778
RMSE train: 0.368081	val: 0.795624	test: 0.811811
MAE train: 0.285528	val: 0.603560	test: 0.619947

Epoch: 200
Loss: 0.1680120274424553
RMSE train: 0.367437	val: 0.799422	test: 0.811406
MAE train: 0.285332	val: 0.603190	test: 0.620465

Epoch: 201
Loss: 0.17312192916870117
RMSE train: 0.362234	val: 0.810552	test: 0.815741
MAE train: 0.280637	val: 0.614882	test: 0.623038

Epoch: 202
Loss: 0.17106638252735137
RMSE train: 0.360950	val: 0.793620	test: 0.796045
MAE train: 0.281203	val: 0.604579	test: 0.610396

Epoch: 203
Loss: 0.17190001159906387
RMSE train: 0.353711	val: 0.816467	test: 0.826314
MAE train: 0.273763	val: 0.619525	test: 0.630975

Epoch: 204
Loss: 0.16889151632785798
RMSE train: 0.375421	val: 0.821849	test: 0.829499
MAE train: 0.292669	val: 0.625805	test: 0.636778

Epoch: 205
Loss: 0.16289914697408675
RMSE train: 0.340132	val: 0.800488	test: 0.811601
MAE train: 0.261425	val: 0.604136	test: 0.614256

Epoch: 206
Loss: 0.15991309136152268
RMSE train: 0.364486	val: 0.801748	test: 0.799954
MAE train: 0.283291	val: 0.608366	test: 0.614059

Epoch: 207
Loss: 0.16604555547237396
RMSE train: 0.360481	val: 0.787439	test: 0.794276
MAE train: 0.282532	val: 0.604023	test: 0.603130

Epoch: 208
Loss: 0.15683968514204025
RMSE train: 0.364415	val: 0.807044	test: 0.809214
MAE train: 0.283283	val: 0.612484	test: 0.623024

Epoch: 209
Loss: 0.16637213081121444
RMSE train: 0.342957	val: 0.808473	test: 0.810132
MAE train: 0.264676	val: 0.613857	test: 0.622060

Epoch: 210
Loss: 0.16021584570407868
RMSE train: 0.349657	val: 0.793604	test: 0.791024
MAE train: 0.272319	val: 0.603245	test: 0.607462

Epoch: 211
Loss: 0.16198131293058396
RMSE train: 0.354628	val: 0.786494	test: 0.786744
MAE train: 0.275051	val: 0.602603	test: 0.604500

Early stopping
Best (RMSE):	 train: 0.376208	val: 0.784661	test: 0.796213
Best (MAE):	 train: 0.297348	val: 0.599960	test: 0.609835


Epoch: 144
Loss: 0.21152067929506302
RMSE train: 0.414896	val: 0.784868	test: 0.735547
MAE train: 0.320766	val: 0.591477	test: 0.582628

Epoch: 145
Loss: 0.20869558304548264
RMSE train: 0.400280	val: 0.780436	test: 0.729743
MAE train: 0.308438	val: 0.582669	test: 0.573240

Epoch: 146
Loss: 0.2084367237985134
RMSE train: 0.403034	val: 0.770367	test: 0.719098
MAE train: 0.310445	val: 0.568558	test: 0.567121

Epoch: 147
Loss: 0.19751579562822977
RMSE train: 0.418707	val: 0.777950	test: 0.735778
MAE train: 0.327426	val: 0.585989	test: 0.578816

Epoch: 148
Loss: 0.20731347799301147
RMSE train: 0.407571	val: 0.787228	test: 0.738509
MAE train: 0.315197	val: 0.592451	test: 0.582802

Epoch: 149
Loss: 0.20397466172774634
RMSE train: 0.402188	val: 0.774832	test: 0.724035
MAE train: 0.309339	val: 0.578177	test: 0.571515

Epoch: 150
Loss: 0.19984269017974535
RMSE train: 0.395157	val: 0.766943	test: 0.719132
MAE train: 0.302766	val: 0.573946	test: 0.568263

Epoch: 151
Loss: 0.2086684430638949
RMSE train: 0.392425	val: 0.769298	test: 0.722968
MAE train: 0.301274	val: 0.574093	test: 0.574175

Epoch: 152
Loss: 0.20660835256179175
RMSE train: 0.417689	val: 0.770651	test: 0.728113
MAE train: 0.324893	val: 0.580526	test: 0.574474

Epoch: 153
Loss: 0.20935582121213278
RMSE train: 0.397828	val: 0.777979	test: 0.727490
MAE train: 0.306527	val: 0.584028	test: 0.572537

Epoch: 154
Loss: 0.20525635530551276
RMSE train: 0.385946	val: 0.780605	test: 0.742225
MAE train: 0.296083	val: 0.581691	test: 0.580664

Epoch: 155
Loss: 0.19639690841237703
RMSE train: 0.384489	val: 0.759514	test: 0.723860
MAE train: 0.296894	val: 0.570692	test: 0.568255

Epoch: 156
Loss: 0.20476220175623894
RMSE train: 0.407038	val: 0.784852	test: 0.739203
MAE train: 0.316885	val: 0.588661	test: 0.585650

Epoch: 157
Loss: 0.20349831506609917
RMSE train: 0.387742	val: 0.774901	test: 0.733483
MAE train: 0.297393	val: 0.584479	test: 0.574628

Epoch: 158
Loss: 0.19507662827769914
RMSE train: 0.408996	val: 0.804540	test: 0.737359
MAE train: 0.316326	val: 0.603239	test: 0.584824

Epoch: 159
Loss: 0.21650960172216097
RMSE train: 0.375601	val: 0.763869	test: 0.729913
MAE train: 0.287743	val: 0.578900	test: 0.569039

Epoch: 160
Loss: 0.193793673068285
RMSE train: 0.411577	val: 0.770000	test: 0.731195
MAE train: 0.320692	val: 0.581805	test: 0.580173

Epoch: 161
Loss: 0.19218005239963531
RMSE train: 0.385037	val: 0.765730	test: 0.729551
MAE train: 0.298246	val: 0.578330	test: 0.572645

Epoch: 162
Loss: 0.20040998980402946
RMSE train: 0.390943	val: 0.775422	test: 0.739053
MAE train: 0.300871	val: 0.587050	test: 0.584667

Epoch: 163
Loss: 0.19049017379681268
RMSE train: 0.392993	val: 0.776331	test: 0.734436
MAE train: 0.304155	val: 0.587592	test: 0.579747

Epoch: 164
Loss: 0.18960249548157057
RMSE train: 0.380302	val: 0.772467	test: 0.733700
MAE train: 0.293101	val: 0.578151	test: 0.576385

Epoch: 165
Loss: 0.1967900755504767
RMSE train: 0.388054	val: 0.758968	test: 0.712663
MAE train: 0.300853	val: 0.571020	test: 0.563348

Epoch: 166
Loss: 0.18537520741422972
RMSE train: 0.392427	val: 0.775537	test: 0.735655
MAE train: 0.305889	val: 0.586591	test: 0.573359

Epoch: 167
Loss: 0.1962276796499888
RMSE train: 0.383741	val: 0.767961	test: 0.724266
MAE train: 0.297336	val: 0.573833	test: 0.565623

Epoch: 168
Loss: 0.19034203017751375
RMSE train: 0.416436	val: 0.802746	test: 0.745657
MAE train: 0.325441	val: 0.605837	test: 0.587339

Epoch: 169
Loss: 0.19408662244677544
RMSE train: 0.388510	val: 0.789208	test: 0.726378
MAE train: 0.299895	val: 0.594852	test: 0.575651

Epoch: 170
Loss: 0.19626758620142937
RMSE train: 0.383521	val: 0.779184	test: 0.731509
MAE train: 0.297317	val: 0.587695	test: 0.573187

Epoch: 171
Loss: 0.18822253122925758
RMSE train: 0.391472	val: 0.778030	test: 0.733896
MAE train: 0.301680	val: 0.589696	test: 0.582261

Epoch: 172
Loss: 0.1840631254017353
RMSE train: 0.392316	val: 0.781518	test: 0.746442
MAE train: 0.301792	val: 0.591615	test: 0.583542

Epoch: 173
Loss: 0.1993505246937275
RMSE train: 0.385014	val: 0.763011	test: 0.714075
MAE train: 0.298361	val: 0.576456	test: 0.564098

Epoch: 174
Loss: 0.18789046754439673
RMSE train: 0.366792	val: 0.775971	test: 0.727942
MAE train: 0.281465	val: 0.585708	test: 0.573486

Epoch: 175
Loss: 0.1960835556189219
RMSE train: 0.376944	val: 0.774603	test: 0.726100
MAE train: 0.289161	val: 0.587356	test: 0.572844

Epoch: 176
Loss: 0.18465316792329153
RMSE train: 0.378567	val: 0.773013	test: 0.732043
MAE train: 0.294192	val: 0.585485	test: 0.576446

Epoch: 177
Loss: 0.20383102695147196
RMSE train: 0.381948	val: 0.794485	test: 0.739386
MAE train: 0.295022	val: 0.598351	test: 0.578460

Epoch: 178
Loss: 0.18732320020596185
RMSE train: 0.394965	val: 0.784801	test: 0.728084
MAE train: 0.305119	val: 0.586543	test: 0.573223

Epoch: 179
Loss: 0.19427615528305373
RMSE train: 0.407525	val: 0.794035	test: 0.768640
MAE train: 0.318141	val: 0.609912	test: 0.602393

Epoch: 180
Loss: 0.19036765396595
RMSE train: 0.380674	val: 0.777651	test: 0.734855
MAE train: 0.293504	val: 0.589933	test: 0.579619

Epoch: 181
Loss: 0.1846004749337832
RMSE train: 0.379311	val: 0.767338	test: 0.719238
MAE train: 0.293838	val: 0.579978	test: 0.568496

Epoch: 182
Loss: 0.18402299409111342
RMSE train: 0.362371	val: 0.779422	test: 0.729118
MAE train: 0.279805	val: 0.586857	test: 0.572261

Epoch: 183
Loss: 0.1801164522767067
RMSE train: 0.381438	val: 0.768561	test: 0.725717
MAE train: 0.296568	val: 0.577587	test: 0.569889

Epoch: 184
Loss: 0.19094319269061089
RMSE train: 0.377771	val: 0.768226	test: 0.718304
MAE train: 0.293059	val: 0.575303	test: 0.565778

Epoch: 185
Loss: 0.18068776031335196
RMSE train: 0.384661	val: 0.784259	test: 0.751321
MAE train: 0.295396	val: 0.591130	test: 0.589763

Epoch: 186
Loss: 0.18204260617494583
RMSE train: 0.372398	val: 0.777959	test: 0.730335
MAE train: 0.286301	val: 0.586275	test: 0.568111

Epoch: 187
Loss: 0.18432583039005598
RMSE train: 0.386736	val: 0.776924	test: 0.726815
MAE train: 0.299283	val: 0.579712	test: 0.572151

Epoch: 188
Loss: 0.17970086261630058
RMSE train: 0.392341	val: 0.799970	test: 0.753819
MAE train: 0.304441	val: 0.606353	test: 0.589062

Epoch: 189
Loss: 0.17205237969756126
RMSE train: 0.371158	val: 0.787269	test: 0.743362
MAE train: 0.285589	val: 0.589505	test: 0.581148

Epoch: 190
Loss: 0.1791545177499453
RMSE train: 0.374113	val: 0.778870	test: 0.733981
MAE train: 0.290081	val: 0.584368	test: 0.578266

Epoch: 191
Loss: 0.17408309752742449
RMSE train: 0.376774	val: 0.763215	test: 0.730859
MAE train: 0.291676	val: 0.578394	test: 0.575033

Epoch: 192
Loss: 0.18008276696006456
RMSE train: 0.371654	val: 0.773130	test: 0.731532
MAE train: 0.286568	val: 0.579101	test: 0.573782

Epoch: 193
Loss: 0.17843558018406233
RMSE train: 0.370974	val: 0.799428	test: 0.750556
MAE train: 0.287188	val: 0.602112	test: 0.590421

Epoch: 194
Loss: 0.18096146484216055
RMSE train: 0.389982	val: 0.789251	test: 0.744389
MAE train: 0.301751	val: 0.595391	test: 0.584959

Epoch: 195
Loss: 0.19246324772636095
RMSE train: 0.353263	val: 0.785602	test: 0.736577
MAE train: 0.270515	val: 0.588963	test: 0.571609

Epoch: 196
Loss: 0.168977918724219
RMSE train: 0.359069	val: 0.775300	test: 0.729224
MAE train: 0.276123	val: 0.584922	test: 0.576225

Epoch: 197
Loss: 0.177182553956906
RMSE train: 0.362046	val: 0.766573	test: 0.727407
MAE train: 0.280329	val: 0.575461	test: 0.569815

Epoch: 198
Loss: 0.17492439597845078
RMSE train: 0.366450	val: 0.768958	test: 0.723609
MAE train: 0.284490	val: 0.578176	test: 0.572716

Epoch: 199
Loss: 0.17178411533435187
RMSE train: 0.378652	val: 0.769859	test: 0.734218
MAE train: 0.292850	val: 0.581764	test: 0.577743

Epoch: 200
Loss: 0.17255172257622084
RMSE train: 0.365472	val: 0.779860	test: 0.737669
MAE train: 0.282118	val: 0.590647	test: 0.576457

Early stopping
Best (RMSE):	 train: 0.388054	val: 0.758968	test: 0.712663
Best (MAE):	 train: 0.300853	val: 0.571020	test: 0.563348
All runs completed.
All runs completed.


Epoch: 144
Loss: 0.23678162374666759
RMSE train: 0.469569	val: 0.790711	test: 0.746238
MAE train: 0.365046	val: 0.607523	test: 0.587985

Epoch: 145
Loss: 0.24055523106030055
RMSE train: 0.456425	val: 0.788871	test: 0.750116
MAE train: 0.353434	val: 0.600695	test: 0.586539

Epoch: 146
Loss: 0.2434376563344683
RMSE train: 0.454778	val: 0.777702	test: 0.729068
MAE train: 0.353973	val: 0.599511	test: 0.577496

Epoch: 147
Loss: 0.25620599197489874
RMSE train: 0.446256	val: 0.761990	test: 0.736995
MAE train: 0.345489	val: 0.579967	test: 0.581606

Epoch: 148
Loss: 0.2262247002550534
RMSE train: 0.447568	val: 0.756869	test: 0.741269
MAE train: 0.348528	val: 0.581539	test: 0.579810

Epoch: 149
Loss: 0.22817055029528482
RMSE train: 0.439022	val: 0.748393	test: 0.722809
MAE train: 0.337963	val: 0.577376	test: 0.564734

Epoch: 150
Loss: 0.23270735144615173
RMSE train: 0.433122	val: 0.748063	test: 0.731058
MAE train: 0.335280	val: 0.573939	test: 0.580279

Epoch: 151
Loss: 0.2322326579264232
RMSE train: 0.439387	val: 0.750681	test: 0.734903
MAE train: 0.341397	val: 0.577372	test: 0.581080

Epoch: 152
Loss: 0.22736340867621557
RMSE train: 0.444580	val: 0.749260	test: 0.743911
MAE train: 0.347071	val: 0.577385	test: 0.592146

Epoch: 153
Loss: 0.22460900779281343
RMSE train: 0.475357	val: 0.798469	test: 0.760560
MAE train: 0.371858	val: 0.614773	test: 0.594709

Epoch: 154
Loss: 0.23794460935252054
RMSE train: 0.442665	val: 0.781526	test: 0.735594
MAE train: 0.341640	val: 0.600257	test: 0.580776

Epoch: 155
Loss: 0.22817787208727427
RMSE train: 0.419449	val: 0.754504	test: 0.741794
MAE train: 0.320543	val: 0.582227	test: 0.576130

Epoch: 156
Loss: 0.2323333801967757
RMSE train: 0.435214	val: 0.759211	test: 0.721439
MAE train: 0.336851	val: 0.583103	test: 0.567298

Epoch: 157
Loss: 0.23283874562808446
RMSE train: 0.423916	val: 0.749079	test: 0.729221
MAE train: 0.326314	val: 0.578082	test: 0.572629

Epoch: 158
Loss: 0.21498247129576548
RMSE train: 0.413592	val: 0.753503	test: 0.738166
MAE train: 0.317993	val: 0.582800	test: 0.575084

Epoch: 159
Loss: 0.23206293157168797
RMSE train: 0.439134	val: 0.746351	test: 0.725980
MAE train: 0.341140	val: 0.574970	test: 0.569429

Epoch: 160
Loss: 0.22741888569934027
RMSE train: 0.427225	val: 0.755797	test: 0.720777
MAE train: 0.328848	val: 0.583883	test: 0.563066

Epoch: 161
Loss: 0.2263660579919815
RMSE train: 0.440814	val: 0.768415	test: 0.733862
MAE train: 0.342577	val: 0.597223	test: 0.574268

Epoch: 162
Loss: 0.22107750390257155
RMSE train: 0.417251	val: 0.747682	test: 0.737753
MAE train: 0.321042	val: 0.575790	test: 0.578007

Epoch: 163
Loss: 0.22016005324465887
RMSE train: 0.425773	val: 0.754352	test: 0.742156
MAE train: 0.329885	val: 0.581731	test: 0.584306

Epoch: 164
Loss: 0.21890596726111003
RMSE train: 0.424713	val: 0.740973	test: 0.736809
MAE train: 0.329133	val: 0.568694	test: 0.576042

Epoch: 165
Loss: 0.25329102256468367
RMSE train: 0.419900	val: 0.747440	test: 0.735428
MAE train: 0.323840	val: 0.574663	test: 0.574836

Early stopping
Best (RMSE):	 train: 0.444550	val: 0.737651	test: 0.747036
Best (MAE):	 train: 0.342495	val: 0.568111	test: 0.587513


Epoch: 144
Loss: 0.25143040610211237
RMSE train: 0.454203	val: 0.766568	test: 0.727283
MAE train: 0.351445	val: 0.588335	test: 0.570795

Epoch: 145
Loss: 0.21849819059882844
RMSE train: 0.434194	val: 0.750380	test: 0.735637
MAE train: 0.334567	val: 0.577161	test: 0.571833

Epoch: 146
Loss: 0.23178764751979283
RMSE train: 0.433903	val: 0.751156	test: 0.737398
MAE train: 0.334505	val: 0.573714	test: 0.571975

Epoch: 147
Loss: 0.22760981853519166
RMSE train: 0.450633	val: 0.760001	test: 0.732417
MAE train: 0.350932	val: 0.578795	test: 0.573829

Epoch: 148
Loss: 0.22486147178070887
RMSE train: 0.439506	val: 0.759646	test: 0.739011
MAE train: 0.339022	val: 0.578951	test: 0.573500

Epoch: 149
Loss: 0.23588377663067409
RMSE train: 0.438905	val: 0.757268	test: 0.722626
MAE train: 0.340102	val: 0.577563	test: 0.558810

Epoch: 150
Loss: 0.2363360800913402
RMSE train: 0.435695	val: 0.758881	test: 0.736221
MAE train: 0.335131	val: 0.582906	test: 0.571254

Epoch: 151
Loss: 0.23803304455110005
RMSE train: 0.432384	val: 0.751089	test: 0.727646
MAE train: 0.334708	val: 0.575771	test: 0.564874

Epoch: 152
Loss: 0.23726339212485723
RMSE train: 0.445724	val: 0.768725	test: 0.732874
MAE train: 0.344163	val: 0.579888	test: 0.559292

Epoch: 153
Loss: 0.24198259945426667
RMSE train: 0.453437	val: 0.755866	test: 0.730966
MAE train: 0.352521	val: 0.579754	test: 0.560659

Epoch: 154
Loss: 0.23790704778262547
RMSE train: 0.444393	val: 0.757478	test: 0.732096
MAE train: 0.342143	val: 0.576594	test: 0.568310

Epoch: 155
Loss: 0.23297490711723054
RMSE train: 0.438441	val: 0.759443	test: 0.724455
MAE train: 0.337551	val: 0.579483	test: 0.557547

Epoch: 156
Loss: 0.2475019586937768
RMSE train: 0.444127	val: 0.769528	test: 0.730917
MAE train: 0.342870	val: 0.582558	test: 0.564542

Epoch: 157
Loss: 0.2375958412885666
RMSE train: 0.453286	val: 0.769947	test: 0.740360
MAE train: 0.352029	val: 0.580250	test: 0.567744

Epoch: 158
Loss: 0.24698532266276224
RMSE train: 0.430650	val: 0.776703	test: 0.739391
MAE train: 0.331767	val: 0.581853	test: 0.570888

Epoch: 159
Loss: 0.23071494698524475
RMSE train: 0.439322	val: 0.747105	test: 0.725739
MAE train: 0.340814	val: 0.562347	test: 0.553882

Epoch: 160
Loss: 0.2332319457616125
RMSE train: 0.442285	val: 0.776139	test: 0.736201
MAE train: 0.340591	val: 0.589242	test: 0.560971

Epoch: 161
Loss: 0.23465826043060847
RMSE train: 0.443395	val: 0.779562	test: 0.734116
MAE train: 0.342442	val: 0.593494	test: 0.566567

Epoch: 162
Loss: 0.226842500269413
RMSE train: 0.462156	val: 0.789275	test: 0.743828
MAE train: 0.356040	val: 0.600686	test: 0.577722

Epoch: 163
Loss: 0.23815918820244925
RMSE train: 0.438913	val: 0.751216	test: 0.729182
MAE train: 0.337880	val: 0.572131	test: 0.556943

Epoch: 164
Loss: 0.25025125380073276
RMSE train: 0.450136	val: 0.782575	test: 0.737320
MAE train: 0.349663	val: 0.597278	test: 0.568774

Epoch: 165
Loss: 0.24745891562529973
RMSE train: 0.452339	val: 0.774332	test: 0.735142
MAE train: 0.348970	val: 0.592786	test: 0.564636

Epoch: 166
Loss: 0.23155784287622996
RMSE train: 0.457512	val: 0.780823	test: 0.745828
MAE train: 0.353494	val: 0.590465	test: 0.579180

Epoch: 167
Loss: 0.2359192967414856
RMSE train: 0.421913	val: 0.747987	test: 0.732548
MAE train: 0.324003	val: 0.573989	test: 0.557868

Epoch: 168
Loss: 0.22638172656297684
RMSE train: 0.448487	val: 0.762495	test: 0.734399
MAE train: 0.349769	val: 0.581296	test: 0.564788

Epoch: 169
Loss: 0.2397108216370855
RMSE train: 0.436267	val: 0.756347	test: 0.734964
MAE train: 0.336615	val: 0.580199	test: 0.572770

Epoch: 170
Loss: 0.23614326012986048
RMSE train: 0.437980	val: 0.770263	test: 0.734832
MAE train: 0.338060	val: 0.590552	test: 0.561886

Epoch: 171
Loss: 0.2262656284230096
RMSE train: 0.427181	val: 0.764208	test: 0.741655
MAE train: 0.330356	val: 0.586085	test: 0.570060

Epoch: 172
Loss: 0.2296021016580718
RMSE train: 0.434141	val: 0.771649	test: 0.739590
MAE train: 0.337134	val: 0.591350	test: 0.568213

Epoch: 173
Loss: 0.23473597105060304
RMSE train: 0.443703	val: 0.790315	test: 0.744367
MAE train: 0.342547	val: 0.602064	test: 0.576824

Epoch: 174
Loss: 0.2402080340044839
RMSE train: 0.425826	val: 0.756070	test: 0.739821
MAE train: 0.330348	val: 0.582955	test: 0.576308

Epoch: 175
Loss: 0.22317546073879516
RMSE train: 0.430716	val: 0.752396	test: 0.728678
MAE train: 0.334602	val: 0.579632	test: 0.559302

Epoch: 176
Loss: 0.23237279057502747
RMSE train: 0.427631	val: 0.771555	test: 0.742855
MAE train: 0.331747	val: 0.582189	test: 0.573178

Epoch: 177
Loss: 0.22732911578246526
RMSE train: 0.441947	val: 0.761676	test: 0.729424
MAE train: 0.343783	val: 0.572448	test: 0.563702

Epoch: 178
Loss: 0.21187494482312882
RMSE train: 0.420316	val: 0.754001	test: 0.724683
MAE train: 0.324510	val: 0.571083	test: 0.562441

Epoch: 179
Loss: 0.20824218647820608
RMSE train: 0.431384	val: 0.773024	test: 0.742403
MAE train: 0.332957	val: 0.586382	test: 0.576484

Epoch: 180
Loss: 0.21269850113562175
RMSE train: 0.425074	val: 0.765596	test: 0.728297
MAE train: 0.329975	val: 0.577818	test: 0.562857

Epoch: 181
Loss: 0.22490840085915156
RMSE train: 0.410758	val: 0.747724	test: 0.726334
MAE train: 0.316915	val: 0.573823	test: 0.562385

Epoch: 182
Loss: 0.20371421639408385
RMSE train: 0.428720	val: 0.756952	test: 0.740705
MAE train: 0.334651	val: 0.579886	test: 0.579285

Epoch: 183
Loss: 0.20836215998445237
RMSE train: 0.417842	val: 0.771272	test: 0.730567
MAE train: 0.323991	val: 0.587048	test: 0.565148

Epoch: 184
Loss: 0.2027827032974788
RMSE train: 0.413354	val: 0.752600	test: 0.720321
MAE train: 0.320047	val: 0.575161	test: 0.558095

Epoch: 185
Loss: 0.21400816099984304
RMSE train: 0.396784	val: 0.751574	test: 0.719149
MAE train: 0.306158	val: 0.573460	test: 0.556740

Epoch: 186
Loss: 0.2099674226982253
RMSE train: 0.422374	val: 0.767080	test: 0.731709
MAE train: 0.327238	val: 0.582376	test: 0.564979

Epoch: 187
Loss: 0.21969176083803177
RMSE train: 0.418304	val: 0.763935	test: 0.740618
MAE train: 0.324000	val: 0.578793	test: 0.570677

Epoch: 188
Loss: 0.20231887378862926
RMSE train: 0.427832	val: 0.771666	test: 0.737314
MAE train: 0.332641	val: 0.589319	test: 0.566717

Epoch: 189
Loss: 0.20345524166311538
RMSE train: 0.426722	val: 0.770888	test: 0.731507
MAE train: 0.331223	val: 0.587672	test: 0.560455

Epoch: 190
Loss: 0.20496410131454468
RMSE train: 0.403268	val: 0.748156	test: 0.739193
MAE train: 0.309957	val: 0.574630	test: 0.575892

Epoch: 191
Loss: 0.21954447350331716
RMSE train: 0.413714	val: 0.750048	test: 0.729037
MAE train: 0.323193	val: 0.575552	test: 0.565748

Epoch: 192
Loss: 0.21380509649004256
RMSE train: 0.414261	val: 0.759061	test: 0.733057
MAE train: 0.321321	val: 0.578101	test: 0.566691

Epoch: 193
Loss: 0.21099983687911714
RMSE train: 0.405036	val: 0.757576	test: 0.728723
MAE train: 0.312455	val: 0.581573	test: 0.564259

Epoch: 194
Loss: 0.19926514689411437
RMSE train: 0.415991	val: 0.758514	test: 0.719578
MAE train: 0.324202	val: 0.577371	test: 0.557880

Early stopping
Best (RMSE):	 train: 0.439322	val: 0.747105	test: 0.725739
Best (MAE):	 train: 0.340814	val: 0.562347	test: 0.553882


Epoch: 144
Loss: 0.24691034534147807
RMSE train: 0.464110	val: 0.777493	test: 0.780708
MAE train: 0.361000	val: 0.598755	test: 0.603161

Epoch: 145
Loss: 0.24726092283214843
RMSE train: 0.452768	val: 0.782383	test: 0.787049
MAE train: 0.350860	val: 0.597659	test: 0.600448

Epoch: 146
Loss: 0.24680171694074357
RMSE train: 0.453765	val: 0.776037	test: 0.773083
MAE train: 0.354694	val: 0.595858	test: 0.590215

Epoch: 147
Loss: 0.2527130959289415
RMSE train: 0.443588	val: 0.779352	test: 0.755211
MAE train: 0.345143	val: 0.595866	test: 0.580426

Epoch: 148
Loss: 0.23737924013819015
RMSE train: 0.454134	val: 0.782919	test: 0.764620
MAE train: 0.353840	val: 0.602031	test: 0.592633

Epoch: 149
Loss: 0.25037979441029684
RMSE train: 0.449556	val: 0.764853	test: 0.758440
MAE train: 0.350139	val: 0.590082	test: 0.583338

Epoch: 150
Loss: 0.25577255870614735
RMSE train: 0.455812	val: 0.771502	test: 0.748943
MAE train: 0.357311	val: 0.593691	test: 0.579138

Epoch: 151
Loss: 0.24295196043593542
RMSE train: 0.443889	val: 0.774034	test: 0.765830
MAE train: 0.344799	val: 0.597999	test: 0.582845

Epoch: 152
Loss: 0.24914688084806716
RMSE train: 0.444097	val: 0.769251	test: 0.763550
MAE train: 0.345156	val: 0.593302	test: 0.585885

Epoch: 153
Loss: 0.238182860825743
RMSE train: 0.441668	val: 0.771632	test: 0.750137
MAE train: 0.346096	val: 0.590354	test: 0.573615

Epoch: 154
Loss: 0.24407910236290523
RMSE train: 0.458920	val: 0.773614	test: 0.751929
MAE train: 0.359495	val: 0.591836	test: 0.583105

Epoch: 155
Loss: 0.23079090671879904
RMSE train: 0.451332	val: 0.784007	test: 0.756250
MAE train: 0.353050	val: 0.603938	test: 0.584811

Epoch: 156
Loss: 0.23752012848854065
RMSE train: 0.439929	val: 0.767337	test: 0.772953
MAE train: 0.340425	val: 0.590986	test: 0.587265

Epoch: 157
Loss: 0.23418423533439636
RMSE train: 0.437448	val: 0.772825	test: 0.765297
MAE train: 0.340323	val: 0.589637	test: 0.585817

Epoch: 158
Loss: 0.25219590429748806
RMSE train: 0.448945	val: 0.793211	test: 0.776614
MAE train: 0.349245	val: 0.604262	test: 0.595103

Epoch: 159
Loss: 0.22665430711848394
RMSE train: 0.429869	val: 0.764188	test: 0.767482
MAE train: 0.334125	val: 0.584910	test: 0.586787

Epoch: 160
Loss: 0.23387215712240764
RMSE train: 0.454111	val: 0.761729	test: 0.764226
MAE train: 0.356981	val: 0.586591	test: 0.588386

Epoch: 161
Loss: 0.22719601222446986
RMSE train: 0.451151	val: 0.760960	test: 0.747792
MAE train: 0.353249	val: 0.590959	test: 0.573672

Epoch: 162
Loss: 0.21546529020581925
RMSE train: 0.453131	val: 0.791348	test: 0.768021
MAE train: 0.353471	val: 0.603357	test: 0.594756

Epoch: 163
Loss: 0.24777158775499888
RMSE train: 0.420035	val: 0.763116	test: 0.758195
MAE train: 0.325196	val: 0.589640	test: 0.577860

Epoch: 164
Loss: 0.24481183929102762
RMSE train: 0.456270	val: 0.774545	test: 0.764215
MAE train: 0.357073	val: 0.599472	test: 0.592168

Epoch: 165
Loss: 0.21166716516017914
RMSE train: 0.422490	val: 0.760617	test: 0.756841
MAE train: 0.327385	val: 0.587098	test: 0.575820

Epoch: 166
Loss: 0.23482719276632583
RMSE train: 0.421282	val: 0.757698	test: 0.764018
MAE train: 0.327038	val: 0.583816	test: 0.580811

Epoch: 167
Loss: 0.21186596900224686
RMSE train: 0.436638	val: 0.757745	test: 0.750677
MAE train: 0.340818	val: 0.586500	test: 0.576383

Epoch: 168
Loss: 0.22400927117892674
RMSE train: 0.458603	val: 0.784460	test: 0.775987
MAE train: 0.359379	val: 0.606577	test: 0.597719

Epoch: 169
Loss: 0.23778512754610606
RMSE train: 0.434353	val: 0.769736	test: 0.768662
MAE train: 0.336313	val: 0.587777	test: 0.581946

Epoch: 170
Loss: 0.22999338805675507
RMSE train: 0.427287	val: 0.777421	test: 0.761552
MAE train: 0.331856	val: 0.590066	test: 0.577169

Epoch: 171
Loss: 0.22322915920189448
RMSE train: 0.420699	val: 0.757070	test: 0.765255
MAE train: 0.324929	val: 0.580397	test: 0.584734

Epoch: 172
Loss: 0.22970709098236902
RMSE train: 0.443754	val: 0.775177	test: 0.779524
MAE train: 0.344707	val: 0.593696	test: 0.601098

Epoch: 173
Loss: 0.22616629941122873
RMSE train: 0.430815	val: 0.763737	test: 0.762902
MAE train: 0.334397	val: 0.588105	test: 0.584047

Epoch: 174
Loss: 0.23119201298270906
RMSE train: 0.413464	val: 0.755050	test: 0.768696
MAE train: 0.319445	val: 0.581737	test: 0.585170

Epoch: 175
Loss: 0.22805044587169374
RMSE train: 0.420476	val: 0.767359	test: 0.755539
MAE train: 0.325462	val: 0.588912	test: 0.578414

Epoch: 176
Loss: 0.2275718886937414
RMSE train: 0.414187	val: 0.764303	test: 0.758316
MAE train: 0.320680	val: 0.581927	test: 0.581827

Epoch: 177
Loss: 0.22175175270863942
RMSE train: 0.415734	val: 0.763084	test: 0.767780
MAE train: 0.322150	val: 0.583715	test: 0.587401

Epoch: 178
Loss: 0.22160714864730835
RMSE train: 0.411036	val: 0.738983	test: 0.760576
MAE train: 0.318377	val: 0.568393	test: 0.575494

Epoch: 179
Loss: 0.22010724885123117
RMSE train: 0.434621	val: 0.776214	test: 0.781396
MAE train: 0.337645	val: 0.595547	test: 0.596611

Epoch: 180
Loss: 0.22313358634710312
RMSE train: 0.430503	val: 0.786848	test: 0.761659
MAE train: 0.336535	val: 0.596790	test: 0.589678

Epoch: 181
Loss: 0.20939563427652633
RMSE train: 0.410318	val: 0.756049	test: 0.762737
MAE train: 0.318357	val: 0.576502	test: 0.580140

Epoch: 182
Loss: 0.2241047333393778
RMSE train: 0.418647	val: 0.751698	test: 0.759664
MAE train: 0.325612	val: 0.581285	test: 0.575236

Epoch: 183
Loss: 0.2027450187930039
RMSE train: 0.409339	val: 0.742549	test: 0.781196
MAE train: 0.315926	val: 0.573960	test: 0.590914

Epoch: 184
Loss: 0.20786824077367783
RMSE train: 0.411252	val: 0.768304	test: 0.758678
MAE train: 0.318612	val: 0.585748	test: 0.580491

Epoch: 185
Loss: 0.2165979359831129
RMSE train: 0.407917	val: 0.763854	test: 0.763459
MAE train: 0.314039	val: 0.585264	test: 0.584060

Epoch: 186
Loss: 0.22634098146642959
RMSE train: 0.425843	val: 0.758544	test: 0.758918
MAE train: 0.330084	val: 0.580773	test: 0.582019

Epoch: 187
Loss: 0.21859367511102132
RMSE train: 0.400875	val: 0.757371	test: 0.759770
MAE train: 0.308727	val: 0.581520	test: 0.577003

Epoch: 188
Loss: 0.20851426997355052
RMSE train: 0.460196	val: 0.809038	test: 0.793241
MAE train: 0.360371	val: 0.622173	test: 0.608656

Epoch: 189
Loss: 0.20854403930050985
RMSE train: 0.413462	val: 0.746755	test: 0.756796
MAE train: 0.321771	val: 0.577423	test: 0.576920

Epoch: 190
Loss: 0.20897096076181956
RMSE train: 0.418674	val: 0.755098	test: 0.768320
MAE train: 0.324248	val: 0.573663	test: 0.584541

Epoch: 191
Loss: 0.20760234977517808
RMSE train: 0.391906	val: 0.742460	test: 0.758078
MAE train: 0.300950	val: 0.571161	test: 0.569178

Epoch: 192
Loss: 0.19651465330805098
RMSE train: 0.414471	val: 0.768404	test: 0.778029
MAE train: 0.322673	val: 0.596618	test: 0.589745

Epoch: 193
Loss: 0.21217043910707747
RMSE train: 0.410803	val: 0.769250	test: 0.773528
MAE train: 0.317980	val: 0.591047	test: 0.591428

Epoch: 194
Loss: 0.20619941715683257
RMSE train: 0.405842	val: 0.767411	test: 0.771253
MAE train: 0.316152	val: 0.593894	test: 0.583457

Epoch: 195
Loss: 0.21179083841187613
RMSE train: 0.405868	val: 0.751323	test: 0.782974
MAE train: 0.314338	val: 0.579991	test: 0.590437

Epoch: 196
Loss: 0.20187204331159592
RMSE train: 0.405518	val: 0.746666	test: 0.771405
MAE train: 0.314909	val: 0.575454	test: 0.591496

Epoch: 197
Loss: 0.21405550837516785
RMSE train: 0.437821	val: 0.783247	test: 0.775121
MAE train: 0.341192	val: 0.601282	test: 0.594569

Epoch: 198
Loss: 0.21310170207704818
RMSE train: 0.397559	val: 0.755757	test: 0.777414
MAE train: 0.305990	val: 0.585042	test: 0.587012

Epoch: 199
Loss: 0.222829347210271
RMSE train: 0.411807	val: 0.753557	test: 0.782138
MAE train: 0.320119	val: 0.580492	test: 0.590206

Epoch: 200
Loss: 0.2131526225379535
RMSE train: 0.416509	val: 0.742945	test: 0.769086
MAE train: 0.327051	val: 0.571416	test: 0.588142

Epoch: 201
Loss: 0.19806239753961563
RMSE train: 0.424515	val: 0.774966	test: 0.766255
MAE train: 0.330288	val: 0.593061	test: 0.589239

Epoch: 202
Loss: 0.2145161202975682
RMSE train: 0.408257	val: 0.767287	test: 0.751197
MAE train: 0.315671	val: 0.583928	test: 0.580226

Epoch: 203
Loss: 0.20894291464771544
RMSE train: 0.389438	val: 0.751383	test: 0.752365
MAE train: 0.299890	val: 0.572526	test: 0.574119

Epoch: 204
Loss: 0.21102457280669892
RMSE train: 0.421099	val: 0.758042	test: 0.771899
MAE train: 0.330165	val: 0.585241	test: 0.593668

Epoch: 205
Loss: 0.2136109620332718
RMSE train: 0.413744	val: 0.761540	test: 0.780474
MAE train: 0.319486	val: 0.580252	test: 0.593438

Epoch: 206
Loss: 0.20638092500822885
RMSE train: 0.407482	val: 0.757095	test: 0.776474
MAE train: 0.315839	val: 0.580007	test: 0.586390

Epoch: 207
Loss: 0.2080472611955234
RMSE train: 0.398704	val: 0.763215	test: 0.767643
MAE train: 0.306555	val: 0.580927	test: 0.585245

Epoch: 208
Loss: 0.21516625157424382
RMSE train: 0.401364	val: 0.759841	test: 0.779364
MAE train: 0.310566	val: 0.581228	test: 0.597435

Epoch: 209
Loss: 0.19301749978746688
RMSE train: 0.398820	val: 0.752590	test: 0.762468
MAE train: 0.309071	val: 0.576875	test: 0.583241

Epoch: 210
Loss: 0.21151871979236603
RMSE train: 0.403082	val: 0.760245	test: 0.783089
MAE train: 0.313644	val: 0.583498	test: 0.601543

Epoch: 211
Loss: 0.22122182377747127
RMSE train: 0.403838	val: 0.756436	test: 0.767420
MAE train: 0.311725	val: 0.580176	test: 0.580182

Epoch: 212
Loss: 0.21643187957150595
RMSE train: 0.398491	val: 0.759142	test: 0.772067
MAE train: 0.308405	val: 0.580438	test: 0.587779

Epoch: 213
Loss: 0.2175127544573375
RMSE train: 0.404708	val: 0.759536	test: 0.769266
MAE train: 0.314459	val: 0.583390	test: 0.589327

Early stopping
Best (RMSE):	 train: 0.411036	val: 0.738983	test: 0.760576
Best (MAE):	 train: 0.318377	val: 0.568393	test: 0.575494
All runs completed.
