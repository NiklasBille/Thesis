>>> Starting run for dataset: esol
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running RANDOM configs_split_experiments/GraphCL/esol/random/train_prop=0.6.yml on cuda:0
Running RANDOM configs_split_experiments/GraphCL/esol/random/train_prop=0.7.yml on cuda:1
Running RANDOM configs_split_experiments/GraphCL/esol/random/train_prop=0.8.yml on cuda:2
Starting process for seed 1: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/esol/random/train_prop=0.6.yml --runseed 1 --device cuda:0
Starting process for seed 2: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/esol/random/train_prop=0.6.yml --runseed 2 --device cuda:0
Starting process for seed 1: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/esol/random/train_prop=0.7.yml --runseed 1 --device cuda:1
Starting process for seed 3: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/esol/random/train_prop=0.6.yml --runseed 3 --device cuda:0
Starting process for seed 2: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/esol/random/train_prop=0.7.yml --runseed 2 --device cuda:1
Starting process for seed 1: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/esol/random/train_prop=0.8.yml --runseed 1 --device cuda:2
Starting process for seed 3: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/esol/random/train_prop=0.7.yml --runseed 3 --device cuda:1
Starting process for seed 2: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/esol/random/train_prop=0.8.yml --runseed 2 --device cuda:2
Starting process for seed 3: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/esol/random/train_prop=0.8.yml --runseed 3 --device cuda:2
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/esol/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/esol/random/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/esol/random/train_prop=0.6/esol_random_1_20-05_14-43-15  ]
[ Using Seed :  1  ]
[ Using device :  cuda:0  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 14.05236021677653
RMSE train: 3.686846	val: 3.562250	test: 3.544528
MAE train: 3.102672	val: 3.028480	test: 3.004493

Epoch: 2
Loss: 13.299348513285318
RMSE train: 3.642499	val: 3.507247	test: 3.494880
MAE train: 3.068830	val: 2.991863	test: 2.947947

Epoch: 3
Loss: 12.235191345214844
RMSE train: 3.566422	val: 3.402910	test: 3.391003
MAE train: 3.008140	val: 2.909989	test: 2.856189

Epoch: 4
Loss: 11.241822560628256
RMSE train: 3.473727	val: 3.307800	test: 3.281510
MAE train: 2.927830	val: 2.832381	test: 2.758932

Epoch: 5
Loss: 10.762551307678223
RMSE train: 3.378801	val: 3.237394	test: 3.182327
MAE train: 2.846014	val: 2.769727	test: 2.673940

Epoch: 6
Loss: 9.626052379608154
RMSE train: 3.344253	val: 3.263552	test: 3.141388
MAE train: 2.844623	val: 2.824516	test: 2.658244

Epoch: 7
Loss: 9.227683385213217
RMSE train: 3.396488	val: 3.393105	test: 3.186218
MAE train: 2.916660	val: 2.958047	test: 2.729170

Epoch: 8
Loss: 8.656687100728353
RMSE train: 3.316884	val: 3.343272	test: 3.095701
MAE train: 2.862590	val: 2.922866	test: 2.669753

Epoch: 9
Loss: 8.123045444488525
RMSE train: 3.135027	val: 3.143553	test: 2.908853
MAE train: 2.691796	val: 2.733943	test: 2.494911

Epoch: 10
Loss: 7.587072054545085
RMSE train: 2.915660	val: 2.879064	test: 2.695813
MAE train: 2.483625	val: 2.483981	test: 2.285321

Epoch: 11
Loss: 6.998244762420654
RMSE train: 2.754916	val: 2.677841	test: 2.540753
MAE train: 2.322994	val: 2.288780	test: 2.122897

Epoch: 12
Loss: 6.500044822692871
RMSE train: 2.642049	val: 2.560655	test: 2.441331
MAE train: 2.213543	val: 2.171085	test: 2.017383

Epoch: 13
Loss: 6.08707841237386
RMSE train: 2.537784	val: 2.478847	test: 2.351639
MAE train: 2.124768	val: 2.091996	test: 1.935597

Epoch: 14
Loss: 5.645054976145427
RMSE train: 2.431451	val: 2.383246	test: 2.262663
MAE train: 2.028956	val: 1.999460	test: 1.863978

Epoch: 15
Loss: 5.020608584086101
RMSE train: 2.288320	val: 2.242811	test: 2.125645
MAE train: 1.893417	val: 1.877062	test: 1.746687

Epoch: 16
Loss: 4.766623338063558
RMSE train: 2.126060	val: 2.089921	test: 1.983673
MAE train: 1.725032	val: 1.715542	test: 1.601360

Epoch: 17
Loss: 4.4735439618428545
RMSE train: 1.974898	val: 1.932349	test: 1.846488
MAE train: 1.567588	val: 1.551185	test: 1.459495

Epoch: 18
Loss: 3.922019879023234
RMSE train: 1.765652	val: 1.732040	test: 1.651745
MAE train: 1.354725	val: 1.358726	test: 1.241985

Epoch: 19
Loss: 3.5524826049804688
RMSE train: 1.613265	val: 1.589593	test: 1.518847
MAE train: 1.227208	val: 1.236546	test: 1.112067

Epoch: 20
Loss: 3.275826613108317
RMSE train: 1.528503	val: 1.511931	test: 1.464225
MAE train: 1.147228	val: 1.146922	test: 1.089125

Epoch: 21
Loss: 2.936338424682617
RMSE train: 1.424782	val: 1.403218	test: 1.377051
MAE train: 1.058309	val: 1.045470	test: 1.017652

Epoch: 22
Loss: 2.6144086519877114
RMSE train: 1.311468	val: 1.301670	test: 1.277254
MAE train: 0.971708	val: 0.970318	test: 0.926421

Epoch: 23
Loss: 2.4783895015716553Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/esol/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/esol/random/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/esol/random/train_prop=0.6/esol_random_3_20-05_14-43-15  ]
[ Using Seed :  3  ]
[ Using device :  cuda:0  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 13.749338467915853
RMSE train: 3.377626	val: 3.187699	test: 3.240787
MAE train: 2.757179	val: 2.638348	test: 2.652403

Epoch: 2
Loss: 12.401033401489258
RMSE train: 3.337551	val: 3.166540	test: 3.205000
MAE train: 2.751888	val: 2.657320	test: 2.630152

Epoch: 3
Loss: 11.61673355102539
RMSE train: 3.283134	val: 3.118524	test: 3.124527
MAE train: 2.715439	val: 2.627901	test: 2.557248

Epoch: 4
Loss: 10.821674346923828
RMSE train: 3.190663	val: 3.025036	test: 2.987548
MAE train: 2.636615	val: 2.560412	test: 2.443057

Epoch: 5
Loss: 10.206729571024576
RMSE train: 3.188660	val: 3.046766	test: 2.941418
MAE train: 2.663648	val: 2.608572	test: 2.427668

Epoch: 6
Loss: 9.398180643717447
RMSE train: 3.129511	val: 3.012106	test: 2.861009
MAE train: 2.624962	val: 2.586933	test: 2.362716

Epoch: 7
Loss: 8.622406959533691
RMSE train: 3.076365	val: 2.990346	test: 2.811754
MAE train: 2.592528	val: 2.580457	test: 2.337971

Epoch: 8
Loss: 8.256358782450357
RMSE train: 3.044256	val: 2.975710	test: 2.795367
MAE train: 2.579029	val: 2.581298	test: 2.344866

Epoch: 9
Loss: 7.682971477508545
RMSE train: 2.996999	val: 2.914145	test: 2.752734
MAE train: 2.552267	val: 2.542096	test: 2.324063

Epoch: 10
Loss: 7.347001711527507
RMSE train: 2.904504	val: 2.812302	test: 2.671052
MAE train: 2.472323	val: 2.449480	test: 2.252139

Epoch: 11
Loss: 6.589818477630615
RMSE train: 2.776341	val: 2.691563	test: 2.549343
MAE train: 2.350942	val: 2.322664	test: 2.135521

Epoch: 12
Loss: 6.302051862080892
RMSE train: 2.639359	val: 2.567450	test: 2.418600
MAE train: 2.224839	val: 2.196310	test: 2.012851

Epoch: 13
Loss: 5.659550189971924
RMSE train: 2.540826	val: 2.498282	test: 2.333350
MAE train: 2.126266	val: 2.117678	test: 1.923601

Epoch: 14
Loss: 5.3711934089660645
RMSE train: 2.448689	val: 2.415522	test: 2.263409
MAE train: 2.049189	val: 2.040157	test: 1.869598

Epoch: 15
Loss: 4.627519766489665
RMSE train: 2.319936	val: 2.274549	test: 2.153706
MAE train: 1.935191	val: 1.916781	test: 1.772172

Epoch: 16
Loss: 4.541106383005778
RMSE train: 2.225744	val: 2.162525	test: 2.073147
MAE train: 1.836915	val: 1.796714	test: 1.678311

Epoch: 17
Loss: 4.050496816635132
RMSE train: 2.086737	val: 2.024337	test: 1.928112
MAE train: 1.707480	val: 1.658757	test: 1.536671

Epoch: 18
Loss: 3.750695069630941
RMSE train: 1.985441	val: 1.933151	test: 1.825383
MAE train: 1.605954	val: 1.567160	test: 1.431729

Epoch: 19
Loss: 3.312318960825602
RMSE train: 1.865489	val: 1.809989	test: 1.735604
MAE train: 1.490449	val: 1.443123	test: 1.352836

Epoch: 20
Loss: 3.0183637936909995
RMSE train: 1.759502	val: 1.705790	test: 1.637364
MAE train: 1.401182	val: 1.347739	test: 1.276068

Epoch: 21
Loss: 2.701481262842814
RMSE train: 1.662592	val: 1.643989	test: 1.545791
MAE train: 1.307281	val: 1.307175	test: 1.173407

Epoch: 22
Loss: 2.34590220451355
RMSE train: 1.600519	val: 1.606965	test: 1.507308
MAE train: 1.250464	val: 1.266923	test: 1.131770

Epoch: 23
Loss: 2.0450106064478555Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/esol/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/esol/random/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/esol/random/train_prop=0.6/esol_random_2_20-05_14-43-15  ]
[ Using Seed :  2  ]
[ Using device :  cuda:0  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 14.431023597717285
RMSE train: 3.397932	val: 3.212782	test: 3.278667
MAE train: 2.770961	val: 2.622598	test: 2.679398

Epoch: 2
Loss: 13.481210072835287
RMSE train: 3.372365	val: 3.188137	test: 3.243584
MAE train: 2.781906	val: 2.663246	test: 2.680108

Epoch: 3
Loss: 12.211654345194498
RMSE train: 3.336380	val: 3.170092	test: 3.200731
MAE train: 2.766937	val: 2.693793	test: 2.649291

Epoch: 4
Loss: 11.41042677561442
RMSE train: 3.284884	val: 3.146305	test: 3.136867
MAE train: 2.729427	val: 2.680660	test: 2.594003

Epoch: 5
Loss: 10.947055180867514
RMSE train: 3.297826	val: 3.216059	test: 3.116992
MAE train: 2.760589	val: 2.747983	test: 2.576295

Epoch: 6
Loss: 9.95097827911377
RMSE train: 3.472378	val: 3.432135	test: 3.261483
MAE train: 2.964074	val: 2.971723	test: 2.767473

Epoch: 7
Loss: 9.401832898457846
RMSE train: 3.330074	val: 3.313970	test: 3.106583
MAE train: 2.838692	val: 2.855354	test: 2.628666

Epoch: 8
Loss: 8.72325070699056
RMSE train: 3.021434	val: 3.000285	test: 2.804090
MAE train: 2.530119	val: 2.530770	test: 2.312667

Epoch: 9
Loss: 8.114360332489014
RMSE train: 2.861540	val: 2.827721	test: 2.660183
MAE train: 2.386229	val: 2.376626	test: 2.177610

Epoch: 10
Loss: 7.591394424438477
RMSE train: 2.742278	val: 2.683536	test: 2.551644
MAE train: 2.291073	val: 2.267177	test: 2.097502

Epoch: 11
Loss: 7.345585346221924
RMSE train: 2.738345	val: 2.672185	test: 2.555064
MAE train: 2.320748	val: 2.296183	test: 2.142653

Epoch: 12
Loss: 6.636055151621501
RMSE train: 2.753207	val: 2.693050	test: 2.577322
MAE train: 2.365251	val: 2.348204	test: 2.198807

Epoch: 13
Loss: 6.027131080627441
RMSE train: 2.703942	val: 2.650015	test: 2.538659
MAE train: 2.329853	val: 2.317203	test: 2.174834

Epoch: 14
Loss: 5.578782876332601
RMSE train: 2.570773	val: 2.529064	test: 2.416889
MAE train: 2.191058	val: 2.182267	test: 2.046625

Epoch: 15
Loss: 5.2970805168151855
RMSE train: 2.443389	val: 2.408635	test: 2.309682
MAE train: 2.056968	val: 2.050221	test: 1.931206

Epoch: 16
Loss: 4.624010403951009
RMSE train: 2.290010	val: 2.269992	test: 2.163277
MAE train: 1.902145	val: 1.905149	test: 1.776352

Epoch: 17
Loss: 4.4606553713480634
RMSE train: 2.183143	val: 2.172076	test: 2.069212
MAE train: 1.806879	val: 1.813759	test: 1.688583

Epoch: 18
Loss: 3.971902847290039
RMSE train: 2.064138	val: 2.056013	test: 1.974962
MAE train: 1.705536	val: 1.711962	test: 1.608422

Epoch: 19
Loss: 3.504902998606364
RMSE train: 1.960009	val: 1.965011	test: 1.896595
MAE train: 1.607925	val: 1.619048	test: 1.538863

Epoch: 20
Loss: 3.2361247539520264
RMSE train: 1.726015	val: 1.755524	test: 1.685357
MAE train: 1.386170	val: 1.406243	test: 1.340780

Epoch: 21
Loss: 2.963327725728353
RMSE train: 1.525244	val: 1.549466	test: 1.508548
MAE train: 1.195440	val: 1.208450	test: 1.172874

Epoch: 22
Loss: 2.5714343388875327
RMSE train: 1.428749	val: 1.453379	test: 1.429070
MAE train: 1.107826	val: 1.117998	test: 1.091398

Epoch: 23
Loss: 2.2842930952707925Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/esol/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/esol/random/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/esol/random/train_prop=0.7/esol_random_3_20-05_14-43-15  ]
[ Using Seed :  3  ]
[ Using device :  cuda:1  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 12.893611192703247
RMSE train: 3.313055	val: 3.387605	test: 3.142024
MAE train: 2.721179	val: 2.820797	test: 2.555032

Epoch: 2
Loss: 11.80645203590393
RMSE train: 3.315316	val: 3.409613	test: 3.143391
MAE train: 2.742090	val: 2.860904	test: 2.579590

Epoch: 3
Loss: 10.71201753616333
RMSE train: 3.178353	val: 3.257495	test: 2.989544
MAE train: 2.607766	val: 2.711700	test: 2.412863

Epoch: 4
Loss: 10.531068801879883
RMSE train: 3.064530	val: 3.103455	test: 2.852301
MAE train: 2.515452	val: 2.595417	test: 2.299409

Epoch: 5
Loss: 8.841380834579468
RMSE train: 2.825919	val: 2.828246	test: 2.587670
MAE train: 2.300626	val: 2.336593	test: 2.061471

Epoch: 6
Loss: 8.118075966835022
RMSE train: 2.767661	val: 2.753617	test: 2.520246
MAE train: 2.280557	val: 2.309522	test: 2.048482

Epoch: 7
Loss: 8.142894744873047
RMSE train: 2.791880	val: 2.775750	test: 2.549045
MAE train: 2.326241	val: 2.356116	test: 2.101306

Epoch: 8
Loss: 7.663323163986206
RMSE train: 2.897264	val: 2.936320	test: 2.675079
MAE train: 2.455401	val: 2.540882	test: 2.242640

Epoch: 9
Loss: 6.524581432342529
RMSE train: 2.970828	val: 3.071471	test: 2.772049
MAE train: 2.530692	val: 2.672755	test: 2.324443

Epoch: 10
Loss: 5.983546614646912
RMSE train: 2.756983	val: 2.878659	test: 2.572358
MAE train: 2.310722	val: 2.459390	test: 2.116001

Epoch: 11
Loss: 5.362275958061218
RMSE train: 2.460660	val: 2.530357	test: 2.268654
MAE train: 2.038967	val: 2.121978	test: 1.852631

Epoch: 12
Loss: 4.728809595108032
RMSE train: 2.205011	val: 2.216267	test: 2.018766
MAE train: 1.820281	val: 1.827598	test: 1.651661

Epoch: 13
Loss: 4.147730112075806
RMSE train: 1.984269	val: 2.013282	test: 1.813115
MAE train: 1.599523	val: 1.628386	test: 1.473772

Epoch: 14
Loss: 4.187878251075745
RMSE train: 1.948200	val: 1.998975	test: 1.805761
MAE train: 1.550512	val: 1.603722	test: 1.461269

Epoch: 15
Loss: 4.218301236629486
RMSE train: 1.920369	val: 2.003880	test: 1.804657
MAE train: 1.524388	val: 1.608081	test: 1.460540

Epoch: 16
Loss: 3.5162922739982605
RMSE train: 1.998123	val: 2.088112	test: 1.869390
MAE train: 1.615921	val: 1.701804	test: 1.524113

Epoch: 17
Loss: 3.820016324520111
RMSE train: 1.933795	val: 1.995716	test: 1.806632
MAE train: 1.583412	val: 1.652121	test: 1.472806

Epoch: 18
Loss: 2.4390390515327454
RMSE train: 1.667695	val: 1.686238	test: 1.570320
MAE train: 1.346652	val: 1.368795	test: 1.250933

Epoch: 19
Loss: 1.9770731031894684
RMSE train: 1.503188	val: 1.554077	test: 1.466540
MAE train: 1.175462	val: 1.220608	test: 1.129329

Epoch: 20
Loss: 1.90340256690979
RMSE train: 1.409744	val: 1.481857	test: 1.403072
MAE train: 1.087632	val: 1.149831	test: 1.102186

Epoch: 21
Loss: 1.8690294921398163
RMSE train: 1.360743	val: 1.402096	test: 1.359436
MAE train: 1.057102	val: 1.089376	test: 1.068931

Epoch: 22
Loss: 1.9478565156459808
RMSE train: 1.347698	val: 1.295068	test: 1.343248
MAE train: 1.062066	val: 1.025675	test: 1.031622

Epoch: 23
Loss: 1.6022769808769226Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/esol/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/esol/random/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/esol/random/train_prop=0.7/esol_random_1_20-05_14-43-15  ]
[ Using Seed :  1  ]
[ Using device :  cuda:1  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 12.969389200210571
RMSE train: 3.567938	val: 3.703423	test: 3.393533
MAE train: 2.986088	val: 3.144979	test: 2.860392

Epoch: 2
Loss: 11.876049757003784
RMSE train: 3.378492	val: 3.486464	test: 3.190430
MAE train: 2.815843	val: 2.950512	test: 2.653073

Epoch: 3
Loss: 11.514366388320923
RMSE train: 3.329342	val: 3.422082	test: 3.120324
MAE train: 2.784610	val: 2.905179	test: 2.602327

Epoch: 4
Loss: 10.474192142486572
RMSE train: 3.272400	val: 3.339256	test: 3.053824
MAE train: 2.744025	val: 2.834765	test: 2.556909

Epoch: 5
Loss: 9.23033893108368
RMSE train: 3.177210	val: 3.219366	test: 2.945010
MAE train: 2.667981	val: 2.737731	test: 2.465788

Epoch: 6
Loss: 10.533167362213135
RMSE train: 3.209180	val: 3.256390	test: 2.945562
MAE train: 2.727914	val: 2.801527	test: 2.495136

Epoch: 7
Loss: 8.869444251060486
RMSE train: 3.225805	val: 3.300476	test: 2.950570
MAE train: 2.750375	val: 2.860839	test: 2.506282

Epoch: 8
Loss: 7.788003087043762
RMSE train: 3.023264	val: 3.131908	test: 2.768415
MAE train: 2.526413	val: 2.678580	test: 2.294203

Epoch: 9
Loss: 8.034584522247314
RMSE train: 2.806125	val: 2.894355	test: 2.578917
MAE train: 2.326822	val: 2.479150	test: 2.130958

Epoch: 10
Loss: 6.134821891784668
RMSE train: 2.636209	val: 2.688143	test: 2.428493
MAE train: 2.183036	val: 2.308851	test: 2.020831

Epoch: 11
Loss: 5.633276700973511
RMSE train: 2.552864	val: 2.590382	test: 2.354081
MAE train: 2.101521	val: 2.212310	test: 1.952984

Epoch: 12
Loss: 6.653942227363586
RMSE train: 2.553456	val: 2.620423	test: 2.345429
MAE train: 2.112355	val: 2.246786	test: 1.945054

Epoch: 13
Loss: 5.1250996589660645
RMSE train: 2.483267	val: 2.577419	test: 2.272436
MAE train: 2.047390	val: 2.215916	test: 1.868620

Epoch: 14
Loss: 4.171048700809479
RMSE train: 2.231912	val: 2.315189	test: 2.049256
MAE train: 1.795352	val: 1.953527	test: 1.642077

Epoch: 15
Loss: 4.039222359657288
RMSE train: 2.016312	val: 2.057012	test: 1.872201
MAE train: 1.610154	val: 1.715557	test: 1.511123

Epoch: 16
Loss: 4.008752763271332
RMSE train: 1.868602	val: 1.881705	test: 1.767467
MAE train: 1.475803	val: 1.552570	test: 1.402437

Epoch: 17
Loss: 3.1246708631515503
RMSE train: 1.764180	val: 1.820179	test: 1.673254
MAE train: 1.377788	val: 1.471217	test: 1.309370

Epoch: 18
Loss: 2.850835621356964
RMSE train: 1.744734	val: 1.785903	test: 1.680532
MAE train: 1.379344	val: 1.449959	test: 1.346420

Epoch: 19
Loss: 2.4956319332122803
RMSE train: 1.515690	val: 1.547335	test: 1.467388
MAE train: 1.172181	val: 1.217044	test: 1.140842

Epoch: 20
Loss: 2.450015962123871
RMSE train: 1.464560	val: 1.472245	test: 1.419366
MAE train: 1.131454	val: 1.155696	test: 1.087563

Epoch: 21
Loss: 2.5461145639419556
RMSE train: 1.355827	val: 1.355722	test: 1.345619
MAE train: 1.039158	val: 1.043769	test: 1.030009

Epoch: 22
Loss: 2.044760435819626
RMSE train: 1.375269	val: 1.433373	test: 1.382854
MAE train: 1.044668	val: 1.109174	test: 1.054278

Epoch: 23
Loss: 1.8762113749980927Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/esol/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/esol/random/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/esol/random/train_prop=0.7/esol_random_2_20-05_14-43-15  ]
[ Using Seed :  2  ]
[ Using device :  cuda:1  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 13.44316053390503
RMSE train: 3.457272	val: 3.554035	test: 3.284592
MAE train: 2.864225	val: 2.992601	test: 2.714859

Epoch: 2
Loss: 12.271644115447998
RMSE train: 3.449852	val: 3.588063	test: 3.275625
MAE train: 2.891585	val: 3.071924	test: 2.737888

Epoch: 3
Loss: 10.676610112190247
RMSE train: 3.436984	val: 3.572480	test: 3.238728
MAE train: 2.909053	val: 3.082530	test: 2.730797

Epoch: 4
Loss: 10.08714246749878
RMSE train: 3.349820	val: 3.462276	test: 3.110003
MAE train: 2.828012	val: 2.980183	test: 2.601855

Epoch: 5
Loss: 10.580177783966064
RMSE train: 3.216599	val: 3.297979	test: 2.961852
MAE train: 2.697393	val: 2.825370	test: 2.453757

Epoch: 6
Loss: 9.389924764633179
RMSE train: 3.140504	val: 3.207513	test: 2.877759
MAE train: 2.644594	val: 2.758823	test: 2.388820

Epoch: 7
Loss: 8.382887601852417
RMSE train: 2.997606	val: 3.065380	test: 2.722371
MAE train: 2.507698	val: 2.611958	test: 2.245937

Epoch: 8
Loss: 7.311194539070129
RMSE train: 2.825134	val: 2.904026	test: 2.578913
MAE train: 2.343463	val: 2.451083	test: 2.108541

Epoch: 9
Loss: 6.725474834442139
RMSE train: 2.704879	val: 2.775306	test: 2.492469
MAE train: 2.241353	val: 2.339551	test: 2.037939

Epoch: 10
Loss: 6.468432545661926
RMSE train: 2.621899	val: 2.715701	test: 2.446736
MAE train: 2.159541	val: 2.277822	test: 1.989946

Epoch: 11
Loss: 5.989169597625732
RMSE train: 2.554477	val: 2.658798	test: 2.378803
MAE train: 2.090504	val: 2.217079	test: 1.920956

Epoch: 12
Loss: 4.620916903018951
RMSE train: 2.364600	val: 2.445507	test: 2.187872
MAE train: 1.910908	val: 2.012946	test: 1.763052

Epoch: 13
Loss: 4.526026725769043
RMSE train: 2.273542	val: 2.340694	test: 2.104521
MAE train: 1.843859	val: 1.926696	test: 1.707936

Epoch: 14
Loss: 4.370797753334045
RMSE train: 2.141950	val: 2.202362	test: 1.988747
MAE train: 1.726928	val: 1.803191	test: 1.621207

Epoch: 15
Loss: 3.773537337779999
RMSE train: 2.015573	val: 2.082420	test: 1.887037
MAE train: 1.621882	val: 1.699585	test: 1.536150

Epoch: 16
Loss: 3.2123669385910034
RMSE train: 1.941887	val: 2.003481	test: 1.813596
MAE train: 1.561869	val: 1.629934	test: 1.469244

Epoch: 17
Loss: 2.8742072582244873
RMSE train: 1.858716	val: 1.897100	test: 1.700379
MAE train: 1.470219	val: 1.512990	test: 1.333584

Epoch: 18
Loss: 2.9427806735038757
RMSE train: 1.709731	val: 1.734410	test: 1.548278
MAE train: 1.333463	val: 1.359851	test: 1.185975

Epoch: 19
Loss: 2.377475082874298
RMSE train: 1.556648	val: 1.588327	test: 1.418980
MAE train: 1.219946	val: 1.246265	test: 1.103099

Epoch: 20
Loss: 2.1457406282424927
RMSE train: 1.325774	val: 1.390573	test: 1.222765
MAE train: 1.005714	val: 1.079407	test: 0.928791

Epoch: 21
Loss: 2.0950689613819122
RMSE train: 1.278880	val: 1.361101	test: 1.207197
MAE train: 0.973729	val: 1.061601	test: 0.941892

Epoch: 22
Loss: 1.7289909720420837
RMSE train: 1.247345	val: 1.345132	test: 1.162202
MAE train: 0.951243	val: 1.034651	test: 0.880338

Epoch: 23
Loss: 1.6787475049495697Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/esol/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/esol/random/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/esol/random/train_prop=0.8/esol_random_2_20-05_14-43-15  ]
[ Using Seed :  2  ]
[ Using device :  cuda:2  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 13.887030363082886
RMSE train: 3.399014	val: 3.614280	test: 2.980136
MAE train: 2.799296	val: 3.039912	test: 2.392887

Epoch: 2
Loss: 12.596900224685669
RMSE train: 3.353682	val: 3.543526	test: 2.941028
MAE train: 2.785578	val: 3.017567	test: 2.369702

Epoch: 3
Loss: 11.24131965637207
RMSE train: 3.281237	val: 3.364802	test: 2.891616
MAE train: 2.741604	val: 2.816099	test: 2.381221

Epoch: 4
Loss: 10.234879493713379
RMSE train: 3.249598	val: 3.204408	test: 2.850145
MAE train: 2.735143	val: 2.656413	test: 2.374592

Epoch: 5
Loss: 9.223550081253052
RMSE train: 3.160597	val: 3.031785	test: 2.781432
MAE train: 2.669731	val: 2.528813	test: 2.338380

Epoch: 6
Loss: 8.706997871398926
RMSE train: 2.935961	val: 2.792131	test: 2.592722
MAE train: 2.461668	val: 2.302261	test: 2.165055

Epoch: 7
Loss: 7.79447066783905
RMSE train: 2.737025	val: 2.577568	test: 2.455122
MAE train: 2.286034	val: 2.103125	test: 2.036094

Epoch: 8
Loss: 7.289495825767517
RMSE train: 2.642413	val: 2.502171	test: 2.392320
MAE train: 2.206442	val: 2.048282	test: 1.993899

Epoch: 9
Loss: 6.613407015800476
RMSE train: 2.675127	val: 2.575223	test: 2.429351
MAE train: 2.250711	val: 2.142069	test: 2.044419

Epoch: 10
Loss: 5.889931321144104
RMSE train: 2.663769	val: 2.539808	test: 2.431366
MAE train: 2.262279	val: 2.120876	test: 2.061193

Epoch: 11
Loss: 5.263837099075317
RMSE train: 2.533343	val: 2.412652	test: 2.329498
MAE train: 2.146907	val: 2.007580	test: 1.955735

Epoch: 12
Loss: 4.70758056640625
RMSE train: 2.292604	val: 2.195048	test: 2.106189
MAE train: 1.899875	val: 1.789349	test: 1.728743

Epoch: 13
Loss: 4.1811991930007935
RMSE train: 2.076389	val: 2.025001	test: 1.895926
MAE train: 1.685992	val: 1.615895	test: 1.525774

Epoch: 14
Loss: 3.734688878059387
RMSE train: 1.869218	val: 1.850419	test: 1.704513
MAE train: 1.485889	val: 1.438888	test: 1.353145

Epoch: 15
Loss: 3.377750813961029
RMSE train: 1.679858	val: 1.650218	test: 1.549922
MAE train: 1.307962	val: 1.238165	test: 1.208275

Epoch: 16
Loss: 2.925515294075012
RMSE train: 1.524530	val: 1.543975	test: 1.386610
MAE train: 1.163973	val: 1.174139	test: 1.070849

Epoch: 17
Loss: 2.560720920562744
RMSE train: 1.483842	val: 1.507170	test: 1.353962
MAE train: 1.125813	val: 1.130024	test: 1.032447

Epoch: 18
Loss: 2.177119731903076
RMSE train: 1.323487	val: 1.364641	test: 1.232601
MAE train: 0.990710	val: 1.006728	test: 0.921335

Epoch: 19
Loss: 1.9066946804523468
RMSE train: 1.144020	val: 1.248283	test: 1.089766
MAE train: 0.852594	val: 0.958726	test: 0.820434

Epoch: 20
Loss: 1.6481510996818542
RMSE train: 1.121611	val: 1.222138	test: 1.106021
MAE train: 0.842818	val: 0.928490	test: 0.835899

Epoch: 21
Loss: 1.4324381649494171
RMSE train: 1.083787	val: 1.232899	test: 1.062244
MAE train: 0.816458	val: 0.943938	test: 0.808803

Epoch: 22
Loss: 1.1733854860067368
RMSE train: 0.840534	val: 1.008661	test: 0.878945
MAE train: 0.644827	val: 0.767759	test: 0.672528

Epoch: 23
Loss: 1.2273325026035309Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/esol/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/esol/random/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/esol/random/train_prop=0.8/esol_random_1_20-05_14-43-15  ]
[ Using Seed :  1  ]
[ Using device :  cuda:2  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 13.927077770233154
RMSE train: 3.615545	val: 3.721765	test: 3.273232
MAE train: 3.035563	val: 3.191810	test: 2.730482

Epoch: 2
Loss: 12.35076355934143
RMSE train: 3.444019	val: 3.559822	test: 3.060095
MAE train: 2.875192	val: 3.009279	test: 2.528822

Epoch: 3
Loss: 11.442183017730713
RMSE train: 3.296520	val: 3.389711	test: 2.893840
MAE train: 2.755366	val: 2.862217	test: 2.356468

Epoch: 4
Loss: 10.201808452606201
RMSE train: 3.224859	val: 3.246071	test: 2.807189
MAE train: 2.686087	val: 2.692957	test: 2.289025

Epoch: 5
Loss: 9.30989670753479
RMSE train: 3.215529	val: 3.178331	test: 2.757758
MAE train: 2.690100	val: 2.630476	test: 2.306534

Epoch: 6
Loss: 8.481448650360107
RMSE train: 3.082705	val: 3.030198	test: 2.649460
MAE train: 2.586491	val: 2.522858	test: 2.233072

Epoch: 7
Loss: 7.760323643684387
RMSE train: 2.925831	val: 2.883127	test: 2.536787
MAE train: 2.453309	val: 2.399432	test: 2.115454

Epoch: 8
Loss: 7.046813249588013
RMSE train: 2.762464	val: 2.696544	test: 2.431669
MAE train: 2.311260	val: 2.238582	test: 2.007093

Epoch: 9
Loss: 6.743660926818848
RMSE train: 2.651114	val: 2.593324	test: 2.345144
MAE train: 2.219807	val: 2.157025	test: 1.937528

Epoch: 10
Loss: 5.888152480125427
RMSE train: 2.368814	val: 2.278749	test: 2.105330
MAE train: 1.940653	val: 1.845694	test: 1.710343

Epoch: 11
Loss: 5.281541466712952
RMSE train: 2.255603	val: 2.196931	test: 1.996247
MAE train: 1.847609	val: 1.777902	test: 1.629143

Epoch: 12
Loss: 4.79488730430603
RMSE train: 2.101895	val: 2.041580	test: 1.868900
MAE train: 1.705563	val: 1.627923	test: 1.515196

Epoch: 13
Loss: 4.10506933927536
RMSE train: 1.954756	val: 1.878540	test: 1.777215
MAE train: 1.572734	val: 1.481234	test: 1.431842

Epoch: 14
Loss: 3.616380214691162
RMSE train: 1.835788	val: 1.777801	test: 1.687553
MAE train: 1.463022	val: 1.397388	test: 1.349814

Epoch: 15
Loss: 3.179598331451416
RMSE train: 1.676183	val: 1.615900	test: 1.551898
MAE train: 1.312272	val: 1.247562	test: 1.214648

Epoch: 16
Loss: 2.8110801577568054
RMSE train: 1.513538	val: 1.506198	test: 1.388443
MAE train: 1.168433	val: 1.177331	test: 1.069531

Epoch: 17
Loss: 2.538564443588257
RMSE train: 1.420066	val: 1.440534	test: 1.273788
MAE train: 1.083540	val: 1.097633	test: 0.969186

Epoch: 18
Loss: 2.1113322377204895
RMSE train: 1.258671	val: 1.276635	test: 1.144011
MAE train: 0.947581	val: 0.945265	test: 0.844759

Epoch: 19
Loss: 1.8825083673000336
RMSE train: 1.074230	val: 1.185567	test: 0.978659
MAE train: 0.790787	val: 0.883088	test: 0.731020

Epoch: 20
Loss: 1.573126807808876
RMSE train: 0.971711	val: 1.090259	test: 0.897736
MAE train: 0.714829	val: 0.801626	test: 0.675891

Epoch: 21
Loss: 1.4651735126972198
RMSE train: 0.870517	val: 0.981470	test: 0.875952
MAE train: 0.651195	val: 0.719156	test: 0.666107

Epoch: 22
Loss: 1.2680801153182983
RMSE train: 0.856796	val: 1.033399	test: 0.873351
MAE train: 0.643585	val: 0.765778	test: 0.660965

Epoch: 23
Loss: 1.1359773874282837Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/esol/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/esol/random/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/esol/random/train_prop=0.8/esol_random_3_20-05_14-43-15  ]
[ Using Seed :  3  ]
[ Using device :  cuda:2  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 13.101114511489868
RMSE train: 3.353372	val: 3.494072	test: 2.998883
MAE train: 2.762996	val: 2.928492	test: 2.429951

Epoch: 2
Loss: 11.989900350570679
RMSE train: 3.354518	val: 3.454213	test: 3.012657
MAE train: 2.779236	val: 2.891255	test: 2.448007

Epoch: 3
Loss: 10.949714660644531
RMSE train: 3.287359	val: 3.299772	test: 2.929938
MAE train: 2.743448	val: 2.743829	test: 2.386226

Epoch: 4
Loss: 9.779043436050415
RMSE train: 3.368573	val: 3.314618	test: 2.965231
MAE train: 2.882401	val: 2.813678	test: 2.507600

Epoch: 5
Loss: 9.022823333740234
RMSE train: 3.328535	val: 3.224465	test: 2.878449
MAE train: 2.865788	val: 2.742408	test: 2.456644

Epoch: 6
Loss: 8.306806564331055
RMSE train: 3.045368	val: 2.956129	test: 2.620567
MAE train: 2.570474	val: 2.443325	test: 2.186073

Epoch: 7
Loss: 7.396818995475769
RMSE train: 2.805127	val: 2.748561	test: 2.450505
MAE train: 2.344148	val: 2.263973	test: 2.024170

Epoch: 8
Loss: 6.835320472717285
RMSE train: 2.663282	val: 2.600004	test: 2.347089
MAE train: 2.229826	val: 2.149511	test: 1.949112

Epoch: 9
Loss: 6.155508518218994
RMSE train: 2.631626	val: 2.542523	test: 2.321551
MAE train: 2.232008	val: 2.103162	test: 1.954402

Epoch: 10
Loss: 5.474952340126038
RMSE train: 2.633643	val: 2.536924	test: 2.343376
MAE train: 2.245635	val: 2.106297	test: 1.983709

Epoch: 11
Loss: 4.888650894165039
RMSE train: 2.426349	val: 2.341033	test: 2.182127
MAE train: 2.039647	val: 1.919534	test: 1.819328

Epoch: 12
Loss: 4.332033812999725
RMSE train: 2.238310	val: 2.178088	test: 2.021068
MAE train: 1.862000	val: 1.778423	test: 1.667814

Epoch: 13
Loss: 3.912950575351715
RMSE train: 1.944666	val: 1.865295	test: 1.769403
MAE train: 1.563612	val: 1.450614	test: 1.422703

Epoch: 14
Loss: 3.605563700199127
RMSE train: 1.781883	val: 1.674651	test: 1.672592
MAE train: 1.405076	val: 1.218426	test: 1.313400

Epoch: 15
Loss: 3.0126208662986755
RMSE train: 1.623426	val: 1.543612	test: 1.535617
MAE train: 1.274902	val: 1.139676	test: 1.200439

Epoch: 16
Loss: 2.605870544910431
RMSE train: 1.560954	val: 1.524675	test: 1.453320
MAE train: 1.235084	val: 1.156030	test: 1.131032

Epoch: 17
Loss: 2.320878326892853
RMSE train: 1.507934	val: 1.472646	test: 1.403667
MAE train: 1.204863	val: 1.124760	test: 1.084625

Epoch: 18
Loss: 1.912620723247528
RMSE train: 1.517358	val: 1.464429	test: 1.429286
MAE train: 1.209173	val: 1.116345	test: 1.096993

Epoch: 19
Loss: 1.6750408411026
RMSE train: 1.331526	val: 1.390533	test: 1.218645
MAE train: 1.042652	val: 1.061120	test: 0.938324

Epoch: 20
Loss: 1.524111658334732
RMSE train: 1.123225	val: 1.146387	test: 1.085639
MAE train: 0.839732	val: 0.810495	test: 0.819412

Epoch: 21
Loss: 1.2840771973133087
RMSE train: 1.037038	val: 1.059765	test: 1.042808
MAE train: 0.788373	val: 0.760120	test: 0.795772

Epoch: 22
Loss: 1.2166854739189148
RMSE train: 0.995313	val: 1.060628	test: 0.985103
MAE train: 0.756951	val: 0.792518	test: 0.758536

Epoch: 23
Loss: 1.0582689046859741
RMSE train: 1.246047	val: 1.254229	test: 1.225465
MAE train: 0.919265	val: 0.935654	test: 0.899224

Epoch: 24
Loss: 2.107401410738627
RMSE train: 1.113421	val: 1.149600	test: 1.131789
MAE train: 0.817074	val: 0.859577	test: 0.840542

Epoch: 25
Loss: 1.9940730333328247
RMSE train: 1.005057	val: 1.074146	test: 1.047979
MAE train: 0.737953	val: 0.815941	test: 0.778090

Epoch: 26
Loss: 1.709755778312683
RMSE train: 1.007054	val: 1.093323	test: 1.083158
MAE train: 0.762070	val: 0.814978	test: 0.826817

Epoch: 27
Loss: 1.5119094053904216
RMSE train: 0.927455	val: 0.999249	test: 1.021619
MAE train: 0.702003	val: 0.755821	test: 0.777694

Epoch: 28
Loss: 1.3937647342681885
RMSE train: 0.913822	val: 0.965033	test: 1.009465
MAE train: 0.700770	val: 0.720986	test: 0.769383

Epoch: 29
Loss: 1.4226532379786174
RMSE train: 0.886152	val: 0.950129	test: 0.977904
MAE train: 0.671243	val: 0.696310	test: 0.735927

Epoch: 30
Loss: 1.2810285091400146
RMSE train: 0.826559	val: 0.902582	test: 0.944588
MAE train: 0.618567	val: 0.657821	test: 0.704755

Epoch: 31
Loss: 1.1397148768107097
RMSE train: 0.829207	val: 0.941142	test: 0.971510
MAE train: 0.628220	val: 0.708438	test: 0.746209

Epoch: 32
Loss: 1.2024357716242473
RMSE train: 0.827323	val: 0.964106	test: 0.998058
MAE train: 0.631088	val: 0.724676	test: 0.742267

Epoch: 33
Loss: 1.032670001188914
RMSE train: 0.749462	val: 0.905592	test: 0.972486
MAE train: 0.568706	val: 0.682292	test: 0.717242

Epoch: 34
Loss: 0.9429543813069662
RMSE train: 0.728548	val: 0.859701	test: 0.948276
MAE train: 0.552524	val: 0.639227	test: 0.706912

Epoch: 35
Loss: 0.8885045448939005
RMSE train: 0.728152	val: 0.851535	test: 0.920625
MAE train: 0.550099	val: 0.625650	test: 0.687307

Epoch: 36
Loss: 0.9228764772415161
RMSE train: 0.739154	val: 0.868568	test: 0.938878
MAE train: 0.565101	val: 0.644941	test: 0.702292

Epoch: 37
Loss: 0.9142373204231262
RMSE train: 0.745127	val: 0.893677	test: 0.950553
MAE train: 0.580765	val: 0.674963	test: 0.723028

Epoch: 38
Loss: 0.945650041103363
RMSE train: 0.651837	val: 0.804651	test: 0.900610
MAE train: 0.500290	val: 0.609226	test: 0.667618

Epoch: 39
Loss: 0.8241536021232605
RMSE train: 0.684845	val: 0.840074	test: 0.912057
MAE train: 0.530048	val: 0.632324	test: 0.685256

Epoch: 40
Loss: 0.7206035256385803
RMSE train: 0.674735	val: 0.835240	test: 0.913728
MAE train: 0.515805	val: 0.632540	test: 0.671353

Epoch: 41
Loss: 0.7559120456377665
RMSE train: 0.653711	val: 0.812416	test: 0.940601
MAE train: 0.507488	val: 0.617976	test: 0.688444

Epoch: 42
Loss: 0.7967746456464132
RMSE train: 0.659975	val: 0.808335	test: 0.955883
MAE train: 0.515468	val: 0.628252	test: 0.710016

Epoch: 43
Loss: 0.8096296191215515
RMSE train: 0.669912	val: 0.800781	test: 0.931241
MAE train: 0.516345	val: 0.615322	test: 0.691314

Epoch: 44
Loss: 0.6938032905260721
RMSE train: 0.682420	val: 0.812434	test: 0.917565
MAE train: 0.523519	val: 0.619901	test: 0.674394

Epoch: 45
Loss: 0.7758612632751465
RMSE train: 0.689838	val: 0.826404	test: 0.920764
MAE train: 0.526145	val: 0.622120	test: 0.664226

Epoch: 46
Loss: 0.7501631776491801
RMSE train: 0.687247	val: 0.834928	test: 0.926391
MAE train: 0.527166	val: 0.624100	test: 0.673218

Epoch: 47
Loss: 0.7718486984570821
RMSE train: 0.623658	val: 0.773638	test: 0.880584
MAE train: 0.471518	val: 0.567140	test: 0.634741

Epoch: 48
Loss: 0.700365682442983
RMSE train: 0.648627	val: 0.788145	test: 0.885390
MAE train: 0.493857	val: 0.593932	test: 0.658661

Epoch: 49
Loss: 0.7468222379684448
RMSE train: 0.640241	val: 0.792094	test: 0.907578
MAE train: 0.496396	val: 0.610771	test: 0.676071

Epoch: 50
Loss: 0.6593718330065409
RMSE train: 0.620359	val: 0.791218	test: 0.909262
MAE train: 0.490946	val: 0.622085	test: 0.683484

Epoch: 51
Loss: 0.70431520541509
RMSE train: 0.621285	val: 0.778406	test: 0.865976
MAE train: 0.479456	val: 0.598986	test: 0.647462

Epoch: 52
Loss: 0.7490675449371338
RMSE train: 0.642319	val: 0.799171	test: 0.851344
MAE train: 0.488440	val: 0.606275	test: 0.611171

Epoch: 53
Loss: 0.7259403268496195
RMSE train: 0.690759	val: 0.844320	test: 0.873767
MAE train: 0.524009	val: 0.633924	test: 0.639414

Epoch: 54
Loss: 0.6660883625348409
RMSE train: 0.623257	val: 0.793262	test: 0.837039
MAE train: 0.469118	val: 0.597636	test: 0.600162

Epoch: 55
Loss: 0.6763323148091634
RMSE train: 0.615077	val: 0.773023	test: 0.846190
MAE train: 0.467512	val: 0.582802	test: 0.601220

Epoch: 56
Loss: 0.6004646023114523
RMSE train: 0.675016	val: 0.814404	test: 0.889737
MAE train: 0.520512	val: 0.603450	test: 0.641641

Epoch: 57
Loss: 0.6393939852714539
RMSE train: 0.719491	val: 0.844382	test: 0.904175
MAE train: 0.560459	val: 0.626304	test: 0.669018

Epoch: 58
Loss: 0.7025940219561259
RMSE train: 0.707064	val: 0.853743	test: 0.888072
MAE train: 0.545589	val: 0.641178	test: 0.657488

Epoch: 59
Loss: 0.6602202653884888
RMSE train: 0.598465	val: 0.780881	test: 0.858577
MAE train: 0.457822	val: 0.591313	test: 0.631782

Epoch: 60
Loss: 0.684911330540975
RMSE train: 0.607436	val: 0.792001	test: 0.871608
MAE train: 0.462837	val: 0.587961	test: 0.640913

Epoch: 61
Loss: 0.5873936414718628
RMSE train: 0.671599	val: 0.838393	test: 0.873457
MAE train: 0.511243	val: 0.620912	test: 0.651743

Epoch: 62
Loss: 0.6433404485384623
RMSE train: 0.610751	val: 0.778671	test: 0.857400
MAE train: 0.464283	val: 0.572765	test: 0.630209

Epoch: 63
Loss: 0.5986103216807047
RMSE train: 0.577355	val: 0.748801	test: 0.870098
MAE train: 0.443236	val: 0.560055	test: 0.632866

Epoch: 64
Loss: 0.583443264166514
RMSE train: 0.546039	val: 0.734658	test: 0.862105
MAE train: 0.420518	val: 0.542573	test: 0.627082

Epoch: 65
Loss: 0.5423786242802938
RMSE train: 0.574957	val: 0.760824	test: 0.850568
MAE train: 0.440888	val: 0.563093	test: 0.626324

Epoch: 66
Loss: 0.5896804928779602
RMSE train: 0.573450	val: 0.758895	test: 0.836279
MAE train: 0.439638	val: 0.567736	test: 0.614422

Epoch: 67
Loss: 0.5265082518259684
RMSE train: 0.600297	val: 0.782843	test: 0.882859
MAE train: 0.461030	val: 0.588629	test: 0.629495

Epoch: 68
Loss: 0.5198911527792612
RMSE train: 0.583732	val: 0.756397	test: 0.850267
MAE train: 0.445475	val: 0.560990	test: 0.604267

Epoch: 69
Loss: 0.5291871428489685
RMSE train: 0.571098	val: 0.752564	test: 0.841792
MAE train: 0.436089	val: 0.562806	test: 0.605120

Epoch: 70
Loss: 0.5730615456899008
RMSE train: 0.612721	val: 0.793099	test: 0.885640
MAE train: 0.464320	val: 0.578296	test: 0.637895

Epoch: 71
Loss: 0.5593132376670837
RMSE train: 0.667810	val: 0.835339	test: 0.924174
MAE train: 0.515791	val: 0.621187	test: 0.672472

Epoch: 72
Loss: 0.4967607061068217
RMSE train: 0.568030	val: 0.743740	test: 0.842984
MAE train: 0.442087	val: 0.569253	test: 0.614065

Epoch: 73
Loss: 0.5937926371892294
RMSE train: 0.555234	val: 0.744070	test: 0.857066
MAE train: 0.434026	val: 0.579155	test: 0.624811

Epoch: 74
Loss: 0.4833763043085734
RMSE train: 0.604081	val: 0.797025	test: 0.907106
MAE train: 0.463116	val: 0.601289	test: 0.655329

Epoch: 75
Loss: 0.5763278305530548
RMSE train: 0.567417	val: 0.773100	test: 0.856606
MAE train: 0.429961	val: 0.564319	test: 0.624327

Epoch: 76
Loss: 0.572027325630188
RMSE train: 0.586842	val: 0.783948	test: 0.866251
MAE train: 0.441402	val: 0.568542	test: 0.638400

Epoch: 77
Loss: 0.5216692288716634
RMSE train: 0.578788	val: 0.765331	test: 0.884496
MAE train: 0.443978	val: 0.572022	test: 0.634078

Epoch: 78
Loss: 0.5537659923235575
RMSE train: 0.538664	val: 0.731110	test: 0.857777
MAE train: 0.412542	val: 0.553512	test: 0.612866

Epoch: 79
Loss: 0.5404892563819885
RMSE train: 0.531520	val: 0.726710	test: 0.833072
MAE train: 0.404814	val: 0.537289	test: 0.601207

Epoch: 80
Loss: 0.49861541390419006
RMSE train: 0.602667	val: 0.777685	test: 0.856693
MAE train: 0.452006	val: 0.562826	test: 0.630118

Epoch: 81
Loss: 0.5042420228322347
RMSE train: 0.605347	val: 0.788381	test: 0.860624
MAE train: 0.453499	val: 0.571896	test: 0.632380

Epoch: 82
Loss: 0.502149780591329
RMSE train: 0.572768	val: 0.771011	test: 0.857117
MAE train: 0.430346	val: 0.558875	test: 0.621326

Epoch: 83
Loss: 0.4852878550688426
RMSE train: 0.557065	val: 0.762526	test: 0.831559
MAE train: 0.419090	val: 0.555738	test: 0.606264

Epoch: 84
Loss: 0.4484482407569885
RMSE train: 1.507447	val: 1.530314	test: 1.442278
MAE train: 1.158846	val: 1.175214	test: 1.065878

Epoch: 24
Loss: 1.9744653701782227
RMSE train: 1.316626	val: 1.337327	test: 1.270926
MAE train: 0.996799	val: 1.008731	test: 0.932199

Epoch: 25
Loss: 1.7297250827153523
RMSE train: 1.263018	val: 1.316553	test: 1.225959
MAE train: 0.990700	val: 1.033089	test: 0.941258

Epoch: 26
Loss: 1.6078405777613323
RMSE train: 1.237904	val: 1.318695	test: 1.205896
MAE train: 0.969755	val: 1.043557	test: 0.919935

Epoch: 27
Loss: 1.3617956638336182
RMSE train: 1.095956	val: 1.189153	test: 1.112163
MAE train: 0.817516	val: 0.891550	test: 0.816385

Epoch: 28
Loss: 1.3201133807500203
RMSE train: 1.032747	val: 1.111783	test: 1.069678
MAE train: 0.781181	val: 0.831140	test: 0.795624

Epoch: 29
Loss: 1.2292197545369465
RMSE train: 1.075264	val: 1.179961	test: 1.091947
MAE train: 0.812686	val: 0.880712	test: 0.803102

Epoch: 30
Loss: 1.116170883178711
RMSE train: 1.006218	val: 1.105983	test: 1.043962
MAE train: 0.750288	val: 0.816460	test: 0.761891

Epoch: 31
Loss: 1.032493789990743
RMSE train: 1.025219	val: 1.094890	test: 1.089273
MAE train: 0.776557	val: 0.804169	test: 0.819126

Epoch: 32
Loss: 1.0308822790781658
RMSE train: 0.925984	val: 1.050810	test: 1.005092
MAE train: 0.696846	val: 0.770697	test: 0.741939

Epoch: 33
Loss: 0.9715622266133627
RMSE train: 0.843096	val: 0.956730	test: 0.948293
MAE train: 0.637253	val: 0.720640	test: 0.711783

Epoch: 34
Loss: 0.9575696786244711
RMSE train: 0.810612	val: 0.921760	test: 0.933270
MAE train: 0.617700	val: 0.691727	test: 0.713625

Epoch: 35
Loss: 0.8599196275075277
RMSE train: 0.838828	val: 0.969618	test: 0.955364
MAE train: 0.646360	val: 0.721488	test: 0.741261

Epoch: 36
Loss: 0.8809865514437357
RMSE train: 0.832587	val: 0.980950	test: 0.950471
MAE train: 0.638864	val: 0.725906	test: 0.738115

Epoch: 37
Loss: 0.8644104401270548
RMSE train: 0.754432	val: 0.900050	test: 0.897054
MAE train: 0.568644	val: 0.659642	test: 0.690085

Epoch: 38
Loss: 0.7959845066070557
RMSE train: 0.722676	val: 0.853469	test: 0.905526
MAE train: 0.549503	val: 0.631185	test: 0.683128

Epoch: 39
Loss: 0.7122026085853577
RMSE train: 0.761524	val: 0.891065	test: 0.924258
MAE train: 0.580278	val: 0.654916	test: 0.692110

Epoch: 40
Loss: 0.8084121147791544
RMSE train: 0.839104	val: 0.974842	test: 0.953027
MAE train: 0.639832	val: 0.728713	test: 0.716262

Epoch: 41
Loss: 0.8012242317199707
RMSE train: 0.756179	val: 0.879016	test: 0.893068
MAE train: 0.576656	val: 0.635936	test: 0.675724

Epoch: 42
Loss: 0.797264556090037
RMSE train: 0.718033	val: 0.834056	test: 0.904306
MAE train: 0.539079	val: 0.607510	test: 0.692512

Epoch: 43
Loss: 0.755144735177358
RMSE train: 0.710097	val: 0.835940	test: 0.889775
MAE train: 0.530272	val: 0.618599	test: 0.672885

Epoch: 44
Loss: 0.7236268520355225
RMSE train: 0.832849	val: 0.959358	test: 0.957751
MAE train: 0.629837	val: 0.726045	test: 0.710577

Epoch: 45
Loss: 0.7451427777608236
RMSE train: 0.764522	val: 0.868389	test: 0.905891
MAE train: 0.578311	val: 0.620805	test: 0.677455

Epoch: 46
Loss: 0.6755065520604452
RMSE train: 0.769416	val: 0.870582	test: 0.923707
MAE train: 0.581702	val: 0.632489	test: 0.703777

Epoch: 47
Loss: 0.6826122005780538
RMSE train: 0.681915	val: 0.814483	test: 0.848829
MAE train: 0.508351	val: 0.601002	test: 0.638776

Epoch: 48
Loss: 0.6843041777610779
RMSE train: 0.705617	val: 0.856987	test: 0.875960
MAE train: 0.523784	val: 0.643586	test: 0.660668

Epoch: 49
Loss: 0.6527764002482096
RMSE train: 0.720156	val: 0.858161	test: 0.897772
MAE train: 0.541167	val: 0.617608	test: 0.689745

Epoch: 50
Loss: 0.6683367093404134
RMSE train: 0.683897	val: 0.812490	test: 0.858388
MAE train: 0.515716	val: 0.594600	test: 0.651654

Epoch: 51
Loss: 0.6752151052157084
RMSE train: 0.681152	val: 0.816036	test: 0.855446
MAE train: 0.519626	val: 0.609454	test: 0.645500

Epoch: 52
Loss: 0.6440432469050089
RMSE train: 0.665079	val: 0.808656	test: 0.864338
MAE train: 0.501945	val: 0.587056	test: 0.637240

Epoch: 53
Loss: 0.6525505979855856
RMSE train: 0.675375	val: 0.822588	test: 0.890829
MAE train: 0.499906	val: 0.587445	test: 0.662939

Epoch: 54
Loss: 0.6263148188591003
RMSE train: 0.714451	val: 0.866979	test: 0.898998
MAE train: 0.525704	val: 0.612181	test: 0.663787

Epoch: 55
Loss: 0.659330427646637
RMSE train: 0.705285	val: 0.860013	test: 0.891853
MAE train: 0.521409	val: 0.620914	test: 0.658952

Epoch: 56
Loss: 0.6254016558329264
RMSE train: 0.664219	val: 0.822380	test: 0.858626
MAE train: 0.493621	val: 0.601484	test: 0.641102

Epoch: 57
Loss: 0.5789942741394043
RMSE train: 0.647654	val: 0.810698	test: 0.846503
MAE train: 0.482282	val: 0.594321	test: 0.637768

Epoch: 58
Loss: 0.605228583017985
RMSE train: 0.719592	val: 0.862093	test: 0.885979
MAE train: 0.528963	val: 0.609777	test: 0.666220

Epoch: 59
Loss: 0.6366833448410034
RMSE train: 0.761577	val: 0.900485	test: 0.914544
MAE train: 0.559165	val: 0.633347	test: 0.683889

Epoch: 60
Loss: 0.6049387256304423
RMSE train: 0.662483	val: 0.820034	test: 0.853407
MAE train: 0.496513	val: 0.596384	test: 0.649735

Epoch: 61
Loss: 0.5815421938896179
RMSE train: 0.656110	val: 0.819000	test: 0.839457
MAE train: 0.488497	val: 0.592394	test: 0.632011

Epoch: 62
Loss: 0.5705301562945048
RMSE train: 0.708662	val: 0.861014	test: 0.856097
MAE train: 0.522330	val: 0.613949	test: 0.628594

Epoch: 63
Loss: 0.5775477886199951
RMSE train: 0.752525	val: 0.880023	test: 0.869505
MAE train: 0.565273	val: 0.634769	test: 0.640780

Epoch: 64
Loss: 0.5847556591033936
RMSE train: 0.753333	val: 0.869334	test: 0.888058
MAE train: 0.560285	val: 0.624985	test: 0.664896

Epoch: 65
Loss: 0.612765888373057
RMSE train: 0.771985	val: 0.884998	test: 0.898940
MAE train: 0.569039	val: 0.630015	test: 0.679590

Epoch: 66
Loss: 0.6277084151903788
RMSE train: 0.725820	val: 0.847544	test: 0.856244
MAE train: 0.536560	val: 0.611397	test: 0.644256

Epoch: 67
Loss: 0.5628746151924133
RMSE train: 0.667978	val: 0.813501	test: 0.835094
MAE train: 0.497140	val: 0.602271	test: 0.636444

Epoch: 68
Loss: 0.5805688301722208
RMSE train: 0.669809	val: 0.825562	test: 0.853443
MAE train: 0.497123	val: 0.605446	test: 0.649947

Epoch: 69
Loss: 0.567562460899353
RMSE train: 0.664674	val: 0.831585	test: 0.838212
MAE train: 0.496571	val: 0.605967	test: 0.624132

Epoch: 70
Loss: 0.5869990587234497
RMSE train: 0.630797	val: 0.805308	test: 0.824370
MAE train: 0.480280	val: 0.601457	test: 0.617444

Epoch: 71
Loss: 0.5465604066848755
RMSE train: 0.605780	val: 0.795764	test: 0.825705
MAE train: 0.454597	val: 0.589036	test: 0.620528

Epoch: 72
Loss: 0.5707728465398153
RMSE train: 0.597654	val: 0.790234	test: 0.828101
MAE train: 0.446963	val: 0.588573	test: 0.630303

Epoch: 73
Loss: 0.5140876471996307
RMSE train: 0.575376	val: 0.774006	test: 0.813078
MAE train: 0.424773	val: 0.573367	test: 0.616199

Epoch: 74
Loss: 0.5497467517852783
RMSE train: 0.565781	val: 0.767666	test: 0.817207
MAE train: 0.422282	val: 0.579519	test: 0.622290

Epoch: 75
Loss: 0.4911396900812785
RMSE train: 0.567825	val: 0.770088	test: 0.830450
MAE train: 0.428510	val: 0.573564	test: 0.622858

Epoch: 76
Loss: 0.4928615887959798
RMSE train: 0.624155	val: 0.809771	test: 0.868710
MAE train: 0.478032	val: 0.602999	test: 0.654027

Epoch: 77
Loss: 0.5305376648902893
RMSE train: 0.614202	val: 0.797218	test: 0.827581
MAE train: 0.463525	val: 0.594363	test: 0.613827

Epoch: 78
Loss: 0.4940491219361623
RMSE train: 0.605593	val: 0.800635	test: 0.826048
MAE train: 0.466672	val: 0.602739	test: 0.626654

Epoch: 79
Loss: 0.5313124855359396
RMSE train: 0.552172	val: 0.764619	test: 0.800280
MAE train: 0.412702	val: 0.560199	test: 0.597782

Epoch: 80
Loss: 0.5631310343742371
RMSE train: 0.614177	val: 0.812910	test: 0.833044
MAE train: 0.446923	val: 0.582738	test: 0.621821

Epoch: 81
Loss: 0.500652958949407
RMSE train: 0.587721	val: 0.797945	test: 0.825040
MAE train: 0.427567	val: 0.590706	test: 0.616324

Epoch: 82
Loss: 0.48129687706629437
RMSE train: 0.533326	val: 0.742924	test: 0.793273
MAE train: 0.398846	val: 0.555506	test: 0.597294

Epoch: 83
Loss: 0.4837033152580261
RMSE train: 0.598785	val: 0.798690	test: 0.838515
MAE train: 0.448803	val: 0.589049	test: 0.622330


RMSE train: 1.448585	val: 1.482587	test: 1.465539
MAE train: 1.133114	val: 1.154538	test: 1.133445

Epoch: 24
Loss: 2.0267412265141806
RMSE train: 1.314242	val: 1.363592	test: 1.346655
MAE train: 1.014785	val: 1.040897	test: 1.035830

Epoch: 25
Loss: 1.8424046834309895
RMSE train: 1.164049	val: 1.225291	test: 1.200815
MAE train: 0.887833	val: 0.922077	test: 0.905020

Epoch: 26
Loss: 1.623902678489685
RMSE train: 1.086453	val: 1.146518	test: 1.119114
MAE train: 0.823714	val: 0.854462	test: 0.835973

Epoch: 27
Loss: 1.4400135676066081
RMSE train: 0.991072	val: 1.051907	test: 1.055564
MAE train: 0.751604	val: 0.770854	test: 0.798539

Epoch: 28
Loss: 1.3887802759806316
RMSE train: 0.870416	val: 0.956725	test: 0.970908
MAE train: 0.653143	val: 0.711845	test: 0.725735

Epoch: 29
Loss: 1.3061288992563884
RMSE train: 0.908154	val: 1.047266	test: 1.005767
MAE train: 0.686716	val: 0.788166	test: 0.756000

Epoch: 30
Loss: 1.127689520517985
RMSE train: 0.819605	val: 0.948610	test: 0.965796
MAE train: 0.609400	val: 0.710545	test: 0.725295

Epoch: 31
Loss: 1.1167682607968648
RMSE train: 0.863408	val: 0.937013	test: 0.991109
MAE train: 0.652214	val: 0.687166	test: 0.756257

Epoch: 32
Loss: 1.1010497013727825
RMSE train: 0.870203	val: 0.980705	test: 0.981839
MAE train: 0.658814	val: 0.730511	test: 0.738447

Epoch: 33
Loss: 0.9414358536402384
RMSE train: 0.780913	val: 0.855104	test: 0.938599
MAE train: 0.594072	val: 0.630368	test: 0.716768

Epoch: 34
Loss: 0.8815043965975443
RMSE train: 0.758248	val: 0.832992	test: 0.967850
MAE train: 0.579512	val: 0.621863	test: 0.732921

Epoch: 35
Loss: 0.850262979666392
RMSE train: 0.758858	val: 0.885379	test: 0.973665
MAE train: 0.577852	val: 0.662882	test: 0.747582

Epoch: 36
Loss: 0.8214578429857889
RMSE train: 0.766668	val: 0.904633	test: 0.950504
MAE train: 0.582480	val: 0.670023	test: 0.736234

Epoch: 37
Loss: 0.8093937238057455
RMSE train: 0.810171	val: 0.929410	test: 0.987903
MAE train: 0.615273	val: 0.694849	test: 0.768416

Epoch: 38
Loss: 0.848426342010498
RMSE train: 0.759691	val: 0.875769	test: 0.959549
MAE train: 0.573305	val: 0.658996	test: 0.736839

Epoch: 39
Loss: 0.754956066608429
RMSE train: 0.710011	val: 0.852283	test: 0.949645
MAE train: 0.537633	val: 0.645485	test: 0.715262

Epoch: 40
Loss: 0.772554079691569
RMSE train: 0.680519	val: 0.842709	test: 0.935776
MAE train: 0.520618	val: 0.647942	test: 0.699288

Epoch: 41
Loss: 0.7984048128128052
RMSE train: 0.639030	val: 0.814521	test: 0.915172
MAE train: 0.488223	val: 0.623009	test: 0.683370

Epoch: 42
Loss: 0.6996952891349792
RMSE train: 0.655590	val: 0.835441	test: 0.923337
MAE train: 0.500576	val: 0.619130	test: 0.693841

Epoch: 43
Loss: 0.7757434447606405
RMSE train: 0.707178	val: 0.870237	test: 0.940167
MAE train: 0.538111	val: 0.630583	test: 0.720523

Epoch: 44
Loss: 0.8048173983891805
RMSE train: 0.732281	val: 0.873570	test: 0.968374
MAE train: 0.554420	val: 0.639033	test: 0.737182

Epoch: 45
Loss: 0.6961448192596436
RMSE train: 0.762521	val: 0.899972	test: 0.993303
MAE train: 0.578578	val: 0.660970	test: 0.752484

Epoch: 46
Loss: 0.7021709283192953
RMSE train: 0.805539	val: 0.934489	test: 1.006668
MAE train: 0.615518	val: 0.685992	test: 0.780681

Epoch: 47
Loss: 0.7694030404090881
RMSE train: 0.709752	val: 0.863416	test: 0.919190
MAE train: 0.543374	val: 0.632047	test: 0.707097

Epoch: 48
Loss: 0.6602154970169067
RMSE train: 0.684186	val: 0.837983	test: 0.884538
MAE train: 0.518175	val: 0.621267	test: 0.663142

Epoch: 49
Loss: 0.6723944147427877
RMSE train: 0.709174	val: 0.858903	test: 0.930995
MAE train: 0.548527	val: 0.638933	test: 0.713062

Epoch: 50
Loss: 0.649262805779775
RMSE train: 0.690532	val: 0.848700	test: 0.911292
MAE train: 0.532455	val: 0.632072	test: 0.702193

Epoch: 51
Loss: 0.6991636355717977
RMSE train: 0.653623	val: 0.834489	test: 0.874259
MAE train: 0.508039	val: 0.628500	test: 0.674698

Epoch: 52
Loss: 0.6701204180717468
RMSE train: 0.625609	val: 0.812490	test: 0.850894
MAE train: 0.473521	val: 0.585351	test: 0.649903

Epoch: 53
Loss: 0.5970071057478586
RMSE train: 0.770241	val: 0.920152	test: 0.960688
MAE train: 0.573446	val: 0.661633	test: 0.736920

Epoch: 54
Loss: 0.6553118824958801
RMSE train: 0.663459	val: 0.828569	test: 0.885268
MAE train: 0.507243	val: 0.604498	test: 0.682381

Epoch: 55
Loss: 0.6247465014457703
RMSE train: 0.644120	val: 0.811448	test: 0.871545
MAE train: 0.492868	val: 0.613435	test: 0.661601

Epoch: 56
Loss: 0.6662485003471375
RMSE train: 0.636372	val: 0.803434	test: 0.883068
MAE train: 0.482255	val: 0.604753	test: 0.666574

Epoch: 57
Loss: 0.5587076544761658
RMSE train: 0.687089	val: 0.838403	test: 0.901436
MAE train: 0.524702	val: 0.609635	test: 0.692288

Epoch: 58
Loss: 0.6390249927838644
RMSE train: 0.730412	val: 0.863305	test: 0.927636
MAE train: 0.552225	val: 0.625945	test: 0.717165

Epoch: 59
Loss: 0.5756780902544657
RMSE train: 0.700321	val: 0.835884	test: 0.896409
MAE train: 0.527402	val: 0.603027	test: 0.684201

Epoch: 60
Loss: 0.5950001875559489
RMSE train: 0.751473	val: 0.861867	test: 0.951433
MAE train: 0.572162	val: 0.629786	test: 0.719710

Epoch: 61
Loss: 0.6006624301274618
RMSE train: 0.787123	val: 0.889461	test: 0.968444
MAE train: 0.598824	val: 0.652167	test: 0.736585

Epoch: 62
Loss: 0.5654044151306152
RMSE train: 0.833765	val: 0.937030	test: 0.997384
MAE train: 0.629777	val: 0.687560	test: 0.762798

Epoch: 63
Loss: 0.6443982919057211
RMSE train: 0.757591	val: 0.886936	test: 0.946004
MAE train: 0.571479	val: 0.643629	test: 0.728074

Epoch: 64
Loss: 0.577308714389801
RMSE train: 0.688034	val: 0.845923	test: 0.887350
MAE train: 0.520145	val: 0.605411	test: 0.674350

Epoch: 65
Loss: 0.524834950764974
RMSE train: 0.717709	val: 0.867691	test: 0.904564
MAE train: 0.545847	val: 0.621572	test: 0.688070

Epoch: 66
Loss: 0.6074117620786031
RMSE train: 0.835098	val: 0.946828	test: 1.011175
MAE train: 0.617206	val: 0.678327	test: 0.767962

Epoch: 67
Loss: 0.5857048233350118
RMSE train: 0.681194	val: 0.823837	test: 0.882393
MAE train: 0.519430	val: 0.591163	test: 0.676876

Epoch: 68
Loss: 0.5142554640769958
RMSE train: 0.640777	val: 0.805129	test: 0.838656
MAE train: 0.489971	val: 0.587710	test: 0.635649

Epoch: 69
Loss: 0.5155993700027466
RMSE train: 0.626127	val: 0.784191	test: 0.838088
MAE train: 0.477967	val: 0.568147	test: 0.635554

Epoch: 70
Loss: 0.48641107479731244
RMSE train: 0.656939	val: 0.788236	test: 0.870597
MAE train: 0.497011	val: 0.571086	test: 0.656740

Epoch: 71
Loss: 0.5550146102905273
RMSE train: 0.605447	val: 0.738447	test: 0.813276
MAE train: 0.466103	val: 0.530190	test: 0.608366

Epoch: 72
Loss: 0.4690135618050893
RMSE train: 0.606725	val: 0.739051	test: 0.811076
MAE train: 0.465680	val: 0.531221	test: 0.605757

Epoch: 73
Loss: 0.5371870696544647
RMSE train: 0.624125	val: 0.753044	test: 0.828186
MAE train: 0.476831	val: 0.545623	test: 0.631618

Epoch: 74
Loss: 0.5946479539076487
RMSE train: 0.681228	val: 0.804057	test: 0.885590
MAE train: 0.523234	val: 0.588770	test: 0.681075

Epoch: 75
Loss: 0.5567311445871989
RMSE train: 0.645095	val: 0.785817	test: 0.861822
MAE train: 0.493362	val: 0.563947	test: 0.660322

Epoch: 76
Loss: 0.549872636795044
RMSE train: 0.632866	val: 0.778601	test: 0.862778
MAE train: 0.484209	val: 0.551302	test: 0.653721

Epoch: 77
Loss: 0.48237208525339764
RMSE train: 0.698888	val: 0.838061	test: 0.904682
MAE train: 0.528394	val: 0.605981	test: 0.678250

Epoch: 78
Loss: 0.4865560233592987
RMSE train: 0.692369	val: 0.833231	test: 0.908628
MAE train: 0.520481	val: 0.606094	test: 0.682566

Epoch: 79
Loss: 0.453660507996877
RMSE train: 0.596499	val: 0.770122	test: 0.835682
MAE train: 0.448266	val: 0.551653	test: 0.631919

Epoch: 80
Loss: 0.45416690905888873
RMSE train: 0.646027	val: 0.805043	test: 0.849534
MAE train: 0.487965	val: 0.579695	test: 0.642867

Epoch: 81
Loss: 0.5005102455615997
RMSE train: 0.721131	val: 0.845103	test: 0.913400
MAE train: 0.553493	val: 0.617610	test: 0.700094

Epoch: 82
Loss: 0.4907674193382263
RMSE train: 0.680574	val: 0.817132	test: 0.902794
MAE train: 0.520620	val: 0.592223	test: 0.687465

Epoch: 83
Loss: 0.4549098213513692
RMSE train: 0.583513	val: 0.755569	test: 0.830599
MAE train: 0.444633	val: 0.540357	test: 0.622636


RMSE train: 1.249024	val: 1.266540	test: 1.257338
MAE train: 0.972227	val: 0.986269	test: 0.963269

Epoch: 24
Loss: 1.3981720507144928
RMSE train: 1.217663	val: 1.267033	test: 1.247894
MAE train: 0.950734	val: 1.005422	test: 0.993816

Epoch: 25
Loss: 1.2723695635795593
RMSE train: 0.915088	val: 0.927556	test: 1.009569
MAE train: 0.696903	val: 0.714339	test: 0.759081

Epoch: 26
Loss: 1.3634803295135498
RMSE train: 0.900842	val: 0.933636	test: 1.001598
MAE train: 0.700325	val: 0.704940	test: 0.759827

Epoch: 27
Loss: 1.2455489933490753
RMSE train: 0.871613	val: 0.942835	test: 0.943777
MAE train: 0.664565	val: 0.687872	test: 0.699297

Epoch: 28
Loss: 1.1606180369853973
RMSE train: 0.905669	val: 0.941891	test: 0.964363
MAE train: 0.698265	val: 0.725412	test: 0.735487

Epoch: 29
Loss: 1.2500365674495697
RMSE train: 0.903018	val: 0.951788	test: 0.954727
MAE train: 0.699550	val: 0.721348	test: 0.721489

Epoch: 30
Loss: 1.4838200807571411
RMSE train: 0.955592	val: 1.031389	test: 1.013528
MAE train: 0.731585	val: 0.770585	test: 0.775167

Epoch: 31
Loss: 1.124779298901558
RMSE train: 0.882693	val: 0.961526	test: 0.994594
MAE train: 0.680192	val: 0.731009	test: 0.751623

Epoch: 32
Loss: 1.1147848963737488
RMSE train: 0.914167	val: 0.972567	test: 1.023897
MAE train: 0.722095	val: 0.741362	test: 0.764664

Epoch: 33
Loss: 1.3249377310276031
RMSE train: 0.810260	val: 0.863968	test: 0.930490
MAE train: 0.627339	val: 0.661994	test: 0.688918

Epoch: 34
Loss: 1.0303152352571487
RMSE train: 0.914214	val: 1.010914	test: 0.970191
MAE train: 0.701582	val: 0.751352	test: 0.726538

Epoch: 35
Loss: 1.211747094988823
RMSE train: 0.852560	val: 0.891441	test: 0.915283
MAE train: 0.646613	val: 0.673997	test: 0.686066

Epoch: 36
Loss: 1.0035874247550964
RMSE train: 0.856649	val: 0.873235	test: 0.941661
MAE train: 0.660930	val: 0.664107	test: 0.712244

Epoch: 37
Loss: 0.9494966864585876
RMSE train: 0.849401	val: 0.861700	test: 0.941927
MAE train: 0.640974	val: 0.652339	test: 0.723057

Epoch: 38
Loss: 1.2495338767766953
RMSE train: 0.867129	val: 0.883660	test: 0.946755
MAE train: 0.651761	val: 0.679603	test: 0.719453

Epoch: 39
Loss: 1.0056487172842026
RMSE train: 0.888847	val: 0.909372	test: 0.968241
MAE train: 0.681326	val: 0.719968	test: 0.725147

Epoch: 40
Loss: 1.000415563583374
RMSE train: 0.837681	val: 0.894942	test: 0.944980
MAE train: 0.647077	val: 0.675483	test: 0.693536

Epoch: 41
Loss: 1.0086054056882858
RMSE train: 0.806350	val: 0.895787	test: 0.929784
MAE train: 0.622246	val: 0.672446	test: 0.683424

Epoch: 42
Loss: 0.958315446972847
RMSE train: 0.839885	val: 0.922418	test: 0.944273
MAE train: 0.638197	val: 0.711435	test: 0.711840

Epoch: 43
Loss: 1.0174249559640884
RMSE train: 0.909170	val: 0.972650	test: 0.976612
MAE train: 0.680498	val: 0.744035	test: 0.738208

Epoch: 44
Loss: 0.9565861821174622
RMSE train: 0.895071	val: 0.959971	test: 0.961901
MAE train: 0.664411	val: 0.710369	test: 0.716660

Epoch: 45
Loss: 0.9640129208564758
RMSE train: 0.887925	val: 0.997855	test: 0.957213
MAE train: 0.662050	val: 0.740198	test: 0.713323

Epoch: 46
Loss: 0.8603285104036331
RMSE train: 0.825632	val: 0.904601	test: 0.915255
MAE train: 0.625821	val: 0.708992	test: 0.678074

Epoch: 47
Loss: 0.7705608159303665
RMSE train: 0.797905	val: 0.844427	test: 0.950209
MAE train: 0.625870	val: 0.670323	test: 0.726043

Epoch: 48
Loss: 0.9741635024547577
RMSE train: 0.730934	val: 0.778185	test: 0.886843
MAE train: 0.577981	val: 0.598506	test: 0.657191

Epoch: 49
Loss: 1.2799866050481796
RMSE train: 0.873848	val: 0.984720	test: 0.968877
MAE train: 0.674899	val: 0.724720	test: 0.722829

Epoch: 50
Loss: 1.0570003688335419
RMSE train: 1.047370	val: 1.125288	test: 1.103723
MAE train: 0.795365	val: 0.827634	test: 0.844567

Epoch: 51
Loss: 0.7505886033177376
RMSE train: 0.780337	val: 0.797201	test: 0.884885
MAE train: 0.588160	val: 0.601573	test: 0.644937

Epoch: 52
Loss: 0.9940125048160553
RMSE train: 0.797661	val: 0.803114	test: 0.919907
MAE train: 0.620550	val: 0.628468	test: 0.684074

Epoch: 53
Loss: 1.038725197315216
RMSE train: 0.721310	val: 0.763387	test: 0.853023
MAE train: 0.530512	val: 0.578027	test: 0.615803

Epoch: 54
Loss: 0.8911625444889069
RMSE train: 0.731158	val: 0.790856	test: 0.875886
MAE train: 0.541744	val: 0.604517	test: 0.630805

Epoch: 55
Loss: 1.1176209300756454
RMSE train: 0.796514	val: 0.866960	test: 0.962839
MAE train: 0.607612	val: 0.664934	test: 0.705049

Epoch: 56
Loss: 0.8329760134220123
RMSE train: 0.728513	val: 0.801425	test: 0.905942
MAE train: 0.560594	val: 0.616978	test: 0.669061

Epoch: 57
Loss: 1.0026498138904572
RMSE train: 0.707657	val: 0.814461	test: 0.882829
MAE train: 0.536476	val: 0.616157	test: 0.651282

Epoch: 58
Loss: 0.9408729523420334
RMSE train: 0.859953	val: 0.963752	test: 1.004206
MAE train: 0.653670	val: 0.732746	test: 0.763418

Epoch: 59
Loss: 0.7584470063447952
RMSE train: 0.908017	val: 1.019998	test: 1.011974
MAE train: 0.692232	val: 0.753230	test: 0.753239

Epoch: 60
Loss: 1.0651127398014069
RMSE train: 0.784200	val: 0.849562	test: 0.895616
MAE train: 0.601498	val: 0.641166	test: 0.655876

Epoch: 61
Loss: 0.7366487234830856
RMSE train: 0.829635	val: 0.886706	test: 0.930792
MAE train: 0.622788	val: 0.674137	test: 0.685243

Epoch: 62
Loss: 0.8347914516925812
RMSE train: 0.864291	val: 0.942546	test: 0.952871
MAE train: 0.653559	val: 0.701031	test: 0.718906

Epoch: 63
Loss: 0.8014509826898575
RMSE train: 0.837231	val: 0.919169	test: 0.937386
MAE train: 0.634816	val: 0.688608	test: 0.711128

Epoch: 64
Loss: 0.8047230392694473
RMSE train: 0.799397	val: 0.849956	test: 0.902172
MAE train: 0.614137	val: 0.653501	test: 0.677351

Epoch: 65
Loss: 0.7636895775794983
RMSE train: 0.723064	val: 0.775063	test: 0.856032
MAE train: 0.545729	val: 0.586227	test: 0.637990

Epoch: 66
Loss: 0.9597118645906448
RMSE train: 0.772558	val: 0.838664	test: 0.886008
MAE train: 0.587002	val: 0.638497	test: 0.660178

Epoch: 67
Loss: 0.7500039935112
RMSE train: 0.841494	val: 0.937708	test: 0.963343
MAE train: 0.639404	val: 0.716715	test: 0.711410

Epoch: 68
Loss: 0.8699340373277664
RMSE train: 0.814501	val: 0.919672	test: 0.963953
MAE train: 0.620349	val: 0.706350	test: 0.713309

Epoch: 69
Loss: 0.6809360980987549
RMSE train: 0.697190	val: 0.780529	test: 0.881791
MAE train: 0.557323	val: 0.634770	test: 0.676782

Epoch: 70
Loss: 0.6368142813444138
RMSE train: 0.703859	val: 0.803143	test: 0.865931
MAE train: 0.545215	val: 0.613897	test: 0.650856

Epoch: 71
Loss: 0.9441035985946655
RMSE train: 0.664683	val: 0.747285	test: 0.826483
MAE train: 0.504188	val: 0.563348	test: 0.603925

Epoch: 72
Loss: 0.6845193803310394
RMSE train: 0.731102	val: 0.780047	test: 0.864894
MAE train: 0.544080	val: 0.597255	test: 0.625783

Epoch: 73
Loss: 0.8266742825508118
RMSE train: 0.702047	val: 0.747689	test: 0.866913
MAE train: 0.533098	val: 0.588253	test: 0.631600

Epoch: 74
Loss: 0.7427027821540833
RMSE train: 0.664337	val: 0.723619	test: 0.849464
MAE train: 0.506493	val: 0.561401	test: 0.617253

Epoch: 75
Loss: 0.681283488869667
RMSE train: 0.788199	val: 0.881494	test: 0.926395
MAE train: 0.587127	val: 0.644891	test: 0.689914

Epoch: 76
Loss: 0.8813315778970718
RMSE train: 0.783083	val: 0.860954	test: 0.909719
MAE train: 0.578271	val: 0.632571	test: 0.682863

Epoch: 77
Loss: 0.6512508392333984
RMSE train: 0.717306	val: 0.789461	test: 0.872151
MAE train: 0.538766	val: 0.600931	test: 0.653048

Epoch: 78
Loss: 0.7512859106063843
RMSE train: 0.700203	val: 0.792867	test: 0.867808
MAE train: 0.524002	val: 0.598439	test: 0.645650

Epoch: 79
Loss: 0.7581373900175095
RMSE train: 0.701543	val: 0.832063	test: 0.858170
MAE train: 0.531752	val: 0.598326	test: 0.652888

Epoch: 80
Loss: 0.6727375835180283
RMSE train: 0.722312	val: 0.842748	test: 0.872392
MAE train: 0.534772	val: 0.595040	test: 0.660588

Epoch: 81
Loss: 0.6121688634157181
RMSE train: 0.708368	val: 0.800422	test: 0.872587
MAE train: 0.523294	val: 0.580781	test: 0.655276

Epoch: 82
Loss: 0.7933533787727356
RMSE train: 0.682718	val: 0.765534	test: 0.845070
MAE train: 0.508872	val: 0.562405	test: 0.632534

Epoch: 83
Loss: 0.6442864015698433
RMSE train: 0.693747	val: 0.763964	test: 0.845564
MAE train: 0.519848	val: 0.561608	test: 0.633493

Epoch: 84
Loss: 0.6133434921503067
RMSE train: 1.249434	val: 1.319387	test: 1.288309
MAE train: 0.948611	val: 1.013029	test: 0.966081

Epoch: 24
Loss: 1.620067298412323
RMSE train: 1.145540	val: 1.200301	test: 1.225487
MAE train: 0.884074	val: 0.939774	test: 0.936192

Epoch: 25
Loss: 1.423991322517395
RMSE train: 1.051998	val: 1.083512	test: 1.149104
MAE train: 0.794290	val: 0.847073	test: 0.868264

Epoch: 26
Loss: 1.6934534013271332
RMSE train: 0.995476	val: 1.039963	test: 1.100078
MAE train: 0.743533	val: 0.804154	test: 0.833687

Epoch: 27
Loss: 1.475698083639145
RMSE train: 1.010915	val: 1.071903	test: 1.081534
MAE train: 0.766488	val: 0.823952	test: 0.813996

Epoch: 28
Loss: 1.2088784575462341
RMSE train: 0.904754	val: 0.945382	test: 1.001127
MAE train: 0.698179	val: 0.727385	test: 0.751182

Epoch: 29
Loss: 1.3288877457380295
RMSE train: 0.990291	val: 1.002920	test: 1.117412
MAE train: 0.787725	val: 0.813229	test: 0.853265

Epoch: 30
Loss: 1.1017400324344635
RMSE train: 0.856925	val: 0.900984	test: 0.993907
MAE train: 0.663465	val: 0.713324	test: 0.748893

Epoch: 31
Loss: 1.2161341607570648
RMSE train: 0.835271	val: 0.885326	test: 0.970749
MAE train: 0.651682	val: 0.691638	test: 0.760016

Epoch: 32
Loss: 1.041673183441162
RMSE train: 0.826018	val: 0.901675	test: 0.962859
MAE train: 0.646388	val: 0.700415	test: 0.730403

Epoch: 33
Loss: 1.6150061786174774
RMSE train: 1.130380	val: 1.216141	test: 1.266087
MAE train: 0.895643	val: 0.970942	test: 0.986465

Epoch: 34
Loss: 1.2483431100845337
RMSE train: 1.170297	val: 1.246406	test: 1.323037
MAE train: 0.911609	val: 0.976359	test: 1.027964

Epoch: 35
Loss: 1.09832301735878
RMSE train: 0.908924	val: 0.951570	test: 1.093338
MAE train: 0.730453	val: 0.742221	test: 0.862624

Epoch: 36
Loss: 1.5005365163087845
RMSE train: 0.767131	val: 0.851626	test: 0.956552
MAE train: 0.608320	val: 0.676147	test: 0.749901

Epoch: 37
Loss: 1.1315658688545227
RMSE train: 1.011647	val: 1.118856	test: 1.139900
MAE train: 0.812831	val: 0.915234	test: 0.894245

Epoch: 38
Loss: 1.079056590795517
RMSE train: 0.866512	val: 0.974123	test: 1.030516
MAE train: 0.675675	val: 0.777734	test: 0.800603

Epoch: 39
Loss: 1.0789411962032318
RMSE train: 0.760664	val: 0.820772	test: 0.961803
MAE train: 0.591267	val: 0.631265	test: 0.738781

Epoch: 40
Loss: 1.3232925683259964
RMSE train: 0.730151	val: 0.797876	test: 0.918773
MAE train: 0.561879	val: 0.613282	test: 0.701468

Epoch: 41
Loss: 1.0295554399490356
RMSE train: 0.856754	val: 0.939244	test: 1.000695
MAE train: 0.659160	val: 0.731698	test: 0.756803

Epoch: 42
Loss: 1.067328780889511
RMSE train: 0.832041	val: 0.908023	test: 0.951446
MAE train: 0.641876	val: 0.710426	test: 0.697493

Epoch: 43
Loss: 0.9087184369564056
RMSE train: 0.946568	val: 1.043216	test: 1.036146
MAE train: 0.706341	val: 0.798232	test: 0.778582

Epoch: 44
Loss: 0.9887420982122421
RMSE train: 0.974400	val: 1.057141	test: 1.052734
MAE train: 0.734821	val: 0.803877	test: 0.786136

Epoch: 45
Loss: 1.088537871837616
RMSE train: 0.907517	val: 0.944988	test: 0.997842
MAE train: 0.689760	val: 0.709941	test: 0.737712

Epoch: 46
Loss: 1.2409512102603912
RMSE train: 0.808668	val: 0.847864	test: 0.933684
MAE train: 0.612762	val: 0.634560	test: 0.699989

Epoch: 47
Loss: 0.984525516629219
RMSE train: 0.876650	val: 0.950492	test: 1.000433
MAE train: 0.653575	val: 0.724347	test: 0.773645

Epoch: 48
Loss: 1.1731709092855453
RMSE train: 0.830396	val: 0.888894	test: 0.976511
MAE train: 0.624375	val: 0.686179	test: 0.734132

Epoch: 49
Loss: 1.496385857462883
RMSE train: 0.819892	val: 0.877578	test: 0.958692
MAE train: 0.611919	val: 0.670554	test: 0.704987

Epoch: 50
Loss: 0.9490203559398651
RMSE train: 0.791031	val: 0.841888	test: 0.932662
MAE train: 0.615752	val: 0.662314	test: 0.695676

Epoch: 51
Loss: 0.7991725951433182
RMSE train: 0.677213	val: 0.741150	test: 0.849313
MAE train: 0.516146	val: 0.561269	test: 0.652415

Epoch: 52
Loss: 1.0448430180549622
RMSE train: 0.690318	val: 0.737881	test: 0.860303
MAE train: 0.523437	val: 0.572890	test: 0.669622

Epoch: 53
Loss: 0.7851046323776245
RMSE train: 0.730070	val: 0.788385	test: 0.889609
MAE train: 0.556466	val: 0.615521	test: 0.684216

Epoch: 54
Loss: 0.7247674092650414
RMSE train: 0.742157	val: 0.820674	test: 0.902183
MAE train: 0.564538	val: 0.637153	test: 0.681730

Epoch: 55
Loss: 0.6831491515040398
RMSE train: 0.674716	val: 0.755319	test: 0.860650
MAE train: 0.512041	val: 0.583655	test: 0.646767

Epoch: 56
Loss: 0.7530654668807983
RMSE train: 0.695573	val: 0.774592	test: 0.888866
MAE train: 0.526366	val: 0.590603	test: 0.670681

Epoch: 57
Loss: 0.7773287296295166
RMSE train: 0.707763	val: 0.790218	test: 0.902172
MAE train: 0.537943	val: 0.606338	test: 0.684731

Epoch: 58
Loss: 0.7408854961395264
RMSE train: 0.728617	val: 0.790731	test: 0.923775
MAE train: 0.551986	val: 0.601852	test: 0.688223

Epoch: 59
Loss: 0.7340149134397507
RMSE train: 0.757881	val: 0.825123	test: 0.949668
MAE train: 0.574160	val: 0.636923	test: 0.709908

Epoch: 60
Loss: 0.7024269998073578
RMSE train: 0.863247	val: 0.932962	test: 1.013076
MAE train: 0.652500	val: 0.723527	test: 0.779914

Epoch: 61
Loss: 0.8913427740335464
RMSE train: 0.845446	val: 0.930378	test: 0.967440
MAE train: 0.635889	val: 0.725235	test: 0.741567

Epoch: 62
Loss: 0.9669120162725449
RMSE train: 0.666963	val: 0.756253	test: 0.827175
MAE train: 0.495449	val: 0.574620	test: 0.629587

Epoch: 63
Loss: 0.8152927756309509
RMSE train: 0.693389	val: 0.840920	test: 0.842094
MAE train: 0.517191	val: 0.613035	test: 0.630440

Epoch: 64
Loss: 0.8293248564004898
RMSE train: 0.660834	val: 0.769441	test: 0.809853
MAE train: 0.487434	val: 0.563894	test: 0.604409

Epoch: 65
Loss: 0.9182642847299576
RMSE train: 0.705754	val: 0.741757	test: 0.861313
MAE train: 0.530524	val: 0.568672	test: 0.647536

Epoch: 66
Loss: 0.887174442410469
RMSE train: 0.763875	val: 0.795104	test: 0.901962
MAE train: 0.570096	val: 0.622976	test: 0.697416

Epoch: 67
Loss: 1.1259277164936066
RMSE train: 0.757511	val: 0.782922	test: 0.885278
MAE train: 0.585181	val: 0.630241	test: 0.698315

Epoch: 68
Loss: 0.7845996022224426
RMSE train: 0.789663	val: 0.855160	test: 0.908216
MAE train: 0.627634	val: 0.681220	test: 0.719771

Epoch: 69
Loss: 0.874000757932663
RMSE train: 0.714974	val: 0.841421	test: 0.873085
MAE train: 0.555944	val: 0.671412	test: 0.675423

Epoch: 70
Loss: 0.7641676366329193
RMSE train: 0.798006	val: 0.892480	test: 0.946554
MAE train: 0.598052	val: 0.673017	test: 0.697491

Epoch: 71
Loss: 0.8467951565980911
RMSE train: 0.698350	val: 0.799183	test: 0.907871
MAE train: 0.532104	val: 0.604370	test: 0.668012

Epoch: 72
Loss: 0.8262483328580856
RMSE train: 0.605142	val: 0.750799	test: 0.870112
MAE train: 0.469806	val: 0.560651	test: 0.640416

Epoch: 73
Loss: 0.6897999793291092
RMSE train: 0.632380	val: 0.780966	test: 0.888211
MAE train: 0.493015	val: 0.573611	test: 0.646731

Epoch: 74
Loss: 0.7011135667562485
RMSE train: 0.640136	val: 0.774666	test: 0.838044
MAE train: 0.492562	val: 0.567269	test: 0.612536

Epoch: 75
Loss: 0.6760650426149368
RMSE train: 0.695090	val: 0.798866	test: 0.833321
MAE train: 0.530995	val: 0.591736	test: 0.618951

Epoch: 76
Loss: 0.8020166307687759
RMSE train: 0.671180	val: 0.768852	test: 0.831311
MAE train: 0.520156	val: 0.585493	test: 0.626260

Epoch: 77
Loss: 0.6912913918495178
RMSE train: 0.641393	val: 0.748696	test: 0.842566
MAE train: 0.500691	val: 0.568843	test: 0.646207

Epoch: 78
Loss: 0.8741771876811981
RMSE train: 0.616132	val: 0.751330	test: 0.846798
MAE train: 0.484207	val: 0.564005	test: 0.639560

Epoch: 79
Loss: 0.9608347862958908
RMSE train: 0.619649	val: 0.733583	test: 0.837759
MAE train: 0.482246	val: 0.560419	test: 0.626030

Epoch: 80
Loss: 0.8318651020526886
RMSE train: 0.745743	val: 0.848507	test: 0.921287
MAE train: 0.572210	val: 0.640031	test: 0.677479

Epoch: 81
Loss: 0.7375238686800003
RMSE train: 0.756498	val: 0.879858	test: 0.924299
MAE train: 0.582340	val: 0.658307	test: 0.694312

Epoch: 82
Loss: 0.7063139230012894
RMSE train: 0.667920	val: 0.790757	test: 0.849233
MAE train: 0.514031	val: 0.593464	test: 0.643693

Epoch: 83
Loss: 0.7385278046131134
RMSE train: 0.686675	val: 0.784821	test: 0.860679
MAE train: 0.528829	val: 0.616482	test: 0.654944

Epoch: 84
Loss: 0.900412529706955
RMSE train: 1.146927	val: 1.230210	test: 1.054039
MAE train: 0.889816	val: 0.950458	test: 0.797863

Epoch: 24
Loss: 1.7985344529151917
RMSE train: 1.016418	val: 1.085958	test: 0.974518
MAE train: 0.781947	val: 0.822531	test: 0.738374

Epoch: 25
Loss: 1.4819324612617493
RMSE train: 1.002145	val: 1.103604	test: 1.013792
MAE train: 0.764434	val: 0.837025	test: 0.765107

Epoch: 26
Loss: 1.3917332589626312
RMSE train: 0.937785	val: 1.051164	test: 1.011031
MAE train: 0.718809	val: 0.812629	test: 0.751084

Epoch: 27
Loss: 1.2344155758619308
RMSE train: 0.855720	val: 0.979252	test: 0.921917
MAE train: 0.641248	val: 0.730298	test: 0.677188

Epoch: 28
Loss: 1.3446701765060425
RMSE train: 0.960487	val: 1.100312	test: 0.989649
MAE train: 0.754542	val: 0.857973	test: 0.735584

Epoch: 29
Loss: 1.2635307908058167
RMSE train: 0.958457	val: 1.087859	test: 0.985347
MAE train: 0.755653	val: 0.841370	test: 0.745777

Epoch: 30
Loss: 1.36735400557518
RMSE train: 0.901044	val: 1.022602	test: 0.987330
MAE train: 0.711896	val: 0.802459	test: 0.737824

Epoch: 31
Loss: 1.2734571695327759
RMSE train: 0.879419	val: 1.005599	test: 0.948905
MAE train: 0.699187	val: 0.805372	test: 0.729373

Epoch: 32
Loss: 1.3595397174358368
RMSE train: 0.810982	val: 0.972469	test: 0.918436
MAE train: 0.637338	val: 0.757933	test: 0.696383

Epoch: 33
Loss: 1.5999981760978699
RMSE train: 0.877398	val: 1.033414	test: 0.966079
MAE train: 0.663254	val: 0.762582	test: 0.691755

Epoch: 34
Loss: 1.0880180299282074
RMSE train: 0.916613	val: 1.017954	test: 0.975983
MAE train: 0.733677	val: 0.808882	test: 0.738942

Epoch: 35
Loss: 0.9668289721012115
RMSE train: 0.823210	val: 0.911811	test: 0.931305
MAE train: 0.647037	val: 0.723138	test: 0.708963

Epoch: 36
Loss: 1.0590361505746841
RMSE train: 0.779534	val: 0.885778	test: 0.899521
MAE train: 0.611273	val: 0.696843	test: 0.669148

Epoch: 37
Loss: 1.126094788312912
RMSE train: 0.795686	val: 0.892114	test: 0.888180
MAE train: 0.609604	val: 0.674566	test: 0.643186

Epoch: 38
Loss: 1.188936784863472
RMSE train: 1.013515	val: 1.064494	test: 1.040811
MAE train: 0.777939	val: 0.798414	test: 0.753335

Epoch: 39
Loss: 1.0458389818668365
RMSE train: 0.968414	val: 1.064010	test: 1.046873
MAE train: 0.741219	val: 0.804160	test: 0.783917

Epoch: 40
Loss: 1.2563602924346924
RMSE train: 0.889762	val: 1.019964	test: 0.984681
MAE train: 0.675083	val: 0.780704	test: 0.757079

Epoch: 41
Loss: 0.9226370006799698
RMSE train: 0.877023	val: 0.998023	test: 0.955347
MAE train: 0.654540	val: 0.755494	test: 0.698692

Epoch: 42
Loss: 0.9474566131830215
RMSE train: 0.880411	val: 0.973059	test: 0.928564
MAE train: 0.658693	val: 0.754445	test: 0.686170

Epoch: 43
Loss: 0.9700545966625214
RMSE train: 0.816462	val: 0.900491	test: 0.867863
MAE train: 0.617014	val: 0.709631	test: 0.630675

Epoch: 44
Loss: 0.9514989703893661
RMSE train: 0.752865	val: 0.837999	test: 0.834348
MAE train: 0.570752	val: 0.656936	test: 0.614932

Epoch: 45
Loss: 1.171054169535637
RMSE train: 0.776494	val: 0.879101	test: 0.864871
MAE train: 0.582998	val: 0.675340	test: 0.653285

Epoch: 46
Loss: 0.8509115129709244
RMSE train: 0.812562	val: 0.906465	test: 0.883820
MAE train: 0.613461	val: 0.688056	test: 0.663536

Epoch: 47
Loss: 0.8391714841127396
RMSE train: 0.803365	val: 0.910362	test: 0.874438
MAE train: 0.610808	val: 0.689739	test: 0.660068

Epoch: 48
Loss: 0.9024437069892883
RMSE train: 0.717817	val: 0.836724	test: 0.824507
MAE train: 0.553054	val: 0.642103	test: 0.618640

Epoch: 49
Loss: 1.0562236309051514
RMSE train: 0.740122	val: 0.885381	test: 0.878585
MAE train: 0.567681	val: 0.681244	test: 0.674978

Epoch: 50
Loss: 1.1399154216051102
RMSE train: 0.830757	val: 0.971841	test: 0.949290
MAE train: 0.622242	val: 0.715954	test: 0.710196

Epoch: 51
Loss: 0.9981321543455124
RMSE train: 0.788330	val: 0.904176	test: 0.848391
MAE train: 0.609429	val: 0.683813	test: 0.647766

Epoch: 52
Loss: 1.1539645493030548
RMSE train: 0.806705	val: 0.886662	test: 0.846117
MAE train: 0.629677	val: 0.690434	test: 0.642953

Epoch: 53
Loss: 1.0061644613742828
RMSE train: 0.795852	val: 0.886665	test: 0.875788
MAE train: 0.608041	val: 0.683537	test: 0.654440

Epoch: 54
Loss: 0.9872421026229858
RMSE train: 0.775113	val: 0.881223	test: 0.865401
MAE train: 0.592323	val: 0.673621	test: 0.623932

Epoch: 55
Loss: 1.000281646847725
RMSE train: 0.759355	val: 0.855188	test: 0.868588
MAE train: 0.588243	val: 0.676020	test: 0.643856

Epoch: 56
Loss: 0.9132180213928223
RMSE train: 0.827071	val: 0.885766	test: 0.926949
MAE train: 0.653571	val: 0.717509	test: 0.706295

Epoch: 57
Loss: 0.8683586418628693
RMSE train: 0.740762	val: 0.883936	test: 0.869390
MAE train: 0.572949	val: 0.667293	test: 0.657053

Epoch: 58
Loss: 0.8698253929615021
RMSE train: 0.870929	val: 1.037222	test: 1.008322
MAE train: 0.673835	val: 0.773541	test: 0.780843

Epoch: 59
Loss: 0.7737707868218422
RMSE train: 0.734810	val: 0.865380	test: 0.855844
MAE train: 0.562972	val: 0.670861	test: 0.659254

Epoch: 60
Loss: 0.8645036667585373
RMSE train: 0.720263	val: 0.815030	test: 0.850534
MAE train: 0.557312	val: 0.662735	test: 0.649204

Epoch: 61
Loss: 0.7289228439331055
RMSE train: 0.715829	val: 0.797672	test: 0.849451
MAE train: 0.554182	val: 0.643418	test: 0.647545

Epoch: 62
Loss: 0.7330743670463562
RMSE train: 0.730306	val: 0.833920	test: 0.865274
MAE train: 0.562834	val: 0.641851	test: 0.648897

Epoch: 63
Loss: 0.8094458132982254
RMSE train: 0.775466	val: 0.911142	test: 0.902448
MAE train: 0.595806	val: 0.660637	test: 0.668622

Epoch: 64
Loss: 0.8249877244234085
RMSE train: 0.719511	val: 0.852021	test: 0.853421
MAE train: 0.547828	val: 0.619371	test: 0.632363

Epoch: 65
Loss: 0.7974435091018677
RMSE train: 0.703708	val: 0.813767	test: 0.835598
MAE train: 0.541389	val: 0.617643	test: 0.628284

Epoch: 66
Loss: 0.885376363992691
RMSE train: 0.704832	val: 0.841368	test: 0.841086
MAE train: 0.549411	val: 0.640664	test: 0.633683

Epoch: 67
Loss: 0.8726741671562195
RMSE train: 0.695518	val: 0.864861	test: 0.848580
MAE train: 0.537101	val: 0.640681	test: 0.638068

Epoch: 68
Loss: 0.7889875769615173
RMSE train: 0.669907	val: 0.840510	test: 0.809935
MAE train: 0.505241	val: 0.616381	test: 0.593916

Epoch: 69
Loss: 0.9231502860784531
RMSE train: 0.716228	val: 0.848273	test: 0.817644
MAE train: 0.547059	val: 0.638025	test: 0.602430

Epoch: 70
Loss: 0.92490653693676
RMSE train: 0.720526	val: 0.821023	test: 0.812837
MAE train: 0.558171	val: 0.640661	test: 0.601007

Epoch: 71
Loss: 0.854621484875679
RMSE train: 0.722055	val: 0.803056	test: 0.837040
MAE train: 0.558347	val: 0.612141	test: 0.625332

Epoch: 72
Loss: 0.7995095402002335
RMSE train: 0.709635	val: 0.787685	test: 0.811344
MAE train: 0.544002	val: 0.605386	test: 0.606761

Epoch: 73
Loss: 0.7800245881080627
RMSE train: 0.761507	val: 0.855316	test: 0.859421
MAE train: 0.573952	val: 0.650943	test: 0.648294

Epoch: 74
Loss: 0.6849624067544937
RMSE train: 0.803149	val: 0.907071	test: 0.911847
MAE train: 0.594594	val: 0.682740	test: 0.681948

Epoch: 75
Loss: 0.682252898812294
RMSE train: 0.875745	val: 0.945711	test: 0.982729
MAE train: 0.658921	val: 0.708248	test: 0.723913

Epoch: 76
Loss: 0.7850831001996994
RMSE train: 0.752820	val: 0.864836	test: 0.859203
MAE train: 0.561988	val: 0.643405	test: 0.639655

Epoch: 77
Loss: 0.5793076530098915
RMSE train: 0.713776	val: 0.847945	test: 0.815241
MAE train: 0.531443	val: 0.636794	test: 0.605105

Epoch: 78
Loss: 0.8414386212825775
RMSE train: 0.731389	val: 0.885107	test: 0.831553
MAE train: 0.556876	val: 0.671740	test: 0.628611

Epoch: 79
Loss: 0.8079528659582138
RMSE train: 0.696889	val: 0.861502	test: 0.820522
MAE train: 0.532435	val: 0.655359	test: 0.610661

Epoch: 80
Loss: 0.7286837100982666
RMSE train: 0.644684	val: 0.796838	test: 0.788345
MAE train: 0.484924	val: 0.599838	test: 0.579091

Epoch: 81
Loss: 0.5588305816054344
RMSE train: 0.724322	val: 0.885804	test: 0.851413
MAE train: 0.546003	val: 0.647760	test: 0.638992

Epoch: 82
Loss: 0.8209382891654968
RMSE train: 0.758050	val: 0.919404	test: 0.859691
MAE train: 0.568564	val: 0.679174	test: 0.640970

Epoch: 83
Loss: 0.823198989033699
RMSE train: 0.829586	val: 0.973096	test: 0.904110
MAE train: 0.611806	val: 0.707907	test: 0.658808

Epoch: 84
Loss: 0.8325515240430832
RMSE train: 0.790437	val: 1.002595	test: 0.872404
MAE train: 0.601226	val: 0.762350	test: 0.668222

Epoch: 24
Loss: 1.0578567385673523
RMSE train: 0.875622	val: 1.083573	test: 0.941736
MAE train: 0.661140	val: 0.831623	test: 0.708664

Epoch: 25
Loss: 1.082806259393692
RMSE train: 0.804147	val: 1.051133	test: 0.900931
MAE train: 0.635048	val: 0.807458	test: 0.691305

Epoch: 26
Loss: 0.8858613818883896
RMSE train: 0.732932	val: 1.088738	test: 0.860028
MAE train: 0.561998	val: 0.751577	test: 0.645098

Epoch: 27
Loss: 0.9305564165115356
RMSE train: 0.728629	val: 0.968310	test: 0.897360
MAE train: 0.547222	val: 0.697292	test: 0.685694

Epoch: 28
Loss: 0.9530810564756393
RMSE train: 0.760481	val: 0.949538	test: 0.899951
MAE train: 0.575197	val: 0.702600	test: 0.672025

Epoch: 29
Loss: 0.9545294940471649
RMSE train: 0.779559	val: 1.015573	test: 0.904131
MAE train: 0.602965	val: 0.763794	test: 0.689269

Epoch: 30
Loss: 0.9400503039360046
RMSE train: 0.802909	val: 1.080181	test: 0.930803
MAE train: 0.613346	val: 0.799157	test: 0.690642

Epoch: 31
Loss: 0.8150286674499512
RMSE train: 0.725608	val: 0.948126	test: 0.940400
MAE train: 0.552440	val: 0.693244	test: 0.685637

Epoch: 32
Loss: 0.8600083142518997
RMSE train: 0.689931	val: 1.002745	test: 0.885797
MAE train: 0.525809	val: 0.723290	test: 0.661025

Epoch: 33
Loss: 0.7802329212427139
RMSE train: 0.674696	val: 0.941197	test: 0.879846
MAE train: 0.508905	val: 0.687607	test: 0.639424

Epoch: 34
Loss: 0.7785866260528564
RMSE train: 0.789348	val: 1.038636	test: 0.908413
MAE train: 0.598686	val: 0.778744	test: 0.673123

Epoch: 35
Loss: 0.7774428427219391
RMSE train: 0.705601	val: 0.949908	test: 0.856573
MAE train: 0.535653	val: 0.709705	test: 0.647954

Epoch: 36
Loss: 0.7538065165281296
RMSE train: 0.664581	val: 0.966800	test: 0.834273
MAE train: 0.516024	val: 0.712228	test: 0.624997

Epoch: 37
Loss: 0.7129144817590714
RMSE train: 0.660477	val: 0.891538	test: 0.813132
MAE train: 0.498257	val: 0.673507	test: 0.585271

Epoch: 38
Loss: 0.6786258667707443
RMSE train: 0.670675	val: 0.879276	test: 0.803197
MAE train: 0.507968	val: 0.652551	test: 0.584776

Epoch: 39
Loss: 0.6940663456916809
RMSE train: 0.641864	val: 0.938909	test: 0.754832
MAE train: 0.486971	val: 0.685607	test: 0.561124

Epoch: 40
Loss: 0.7023823261260986
RMSE train: 0.642355	val: 0.920991	test: 0.785236
MAE train: 0.484672	val: 0.698997	test: 0.573215

Epoch: 41
Loss: 0.6474469900131226
RMSE train: 0.654516	val: 0.887178	test: 0.811887
MAE train: 0.498898	val: 0.694851	test: 0.581932

Epoch: 42
Loss: 0.6565483510494232
RMSE train: 0.634200	val: 0.894331	test: 0.805350
MAE train: 0.485330	val: 0.678043	test: 0.581291

Epoch: 43
Loss: 0.7152164280414581
RMSE train: 0.642685	val: 0.923691	test: 0.814645
MAE train: 0.497979	val: 0.688167	test: 0.603116

Epoch: 44
Loss: 0.6629726886749268
RMSE train: 0.697859	val: 0.999747	test: 0.830565
MAE train: 0.542244	val: 0.760572	test: 0.622499

Epoch: 45
Loss: 0.64246666431427
RMSE train: 0.633959	val: 0.898566	test: 0.825730
MAE train: 0.494418	val: 0.722490	test: 0.622382

Epoch: 46
Loss: 0.6428721696138382
RMSE train: 0.604745	val: 0.927111	test: 0.790275
MAE train: 0.466666	val: 0.672278	test: 0.573797

Epoch: 47
Loss: 0.575544960796833
RMSE train: 0.590358	val: 0.859753	test: 0.806101
MAE train: 0.448854	val: 0.649008	test: 0.587910

Epoch: 48
Loss: 0.6315372288227081
RMSE train: 0.607357	val: 0.866486	test: 0.818608
MAE train: 0.465702	val: 0.653882	test: 0.597208

Epoch: 49
Loss: 0.6079738289117813
RMSE train: 0.603390	val: 0.880946	test: 0.803102
MAE train: 0.461250	val: 0.665922	test: 0.590136

Epoch: 50
Loss: 0.6180174946784973
RMSE train: 0.630954	val: 0.879188	test: 0.823346
MAE train: 0.482213	val: 0.672047	test: 0.600156

Epoch: 51
Loss: 0.5929810106754303
RMSE train: 0.642335	val: 0.884090	test: 0.821337
MAE train: 0.481107	val: 0.679864	test: 0.600121

Epoch: 52
Loss: 0.6078872233629227
RMSE train: 0.712198	val: 0.977619	test: 0.804532
MAE train: 0.531590	val: 0.742479	test: 0.596186

Epoch: 53
Loss: 0.5550339818000793
RMSE train: 0.698833	val: 0.898406	test: 0.796362
MAE train: 0.524249	val: 0.696046	test: 0.577375

Epoch: 54
Loss: 0.5930521190166473
RMSE train: 0.698969	val: 0.923189	test: 0.787694
MAE train: 0.525051	val: 0.716865	test: 0.582144

Epoch: 55
Loss: 0.5598849803209305
RMSE train: 0.657319	val: 0.876545	test: 0.765420
MAE train: 0.494623	val: 0.667839	test: 0.564169

Epoch: 56
Loss: 0.5631376355886459
RMSE train: 0.725180	val: 0.940895	test: 0.813393
MAE train: 0.540186	val: 0.720393	test: 0.579095

Epoch: 57
Loss: 0.5722626000642776
RMSE train: 0.628451	val: 0.874578	test: 0.774294
MAE train: 0.470421	val: 0.663112	test: 0.561329

Epoch: 58
Loss: 0.6193418651819229
RMSE train: 0.625357	val: 0.888718	test: 0.767248
MAE train: 0.472692	val: 0.663680	test: 0.558888

Epoch: 59
Loss: 0.5461700558662415
RMSE train: 0.692837	val: 0.976478	test: 0.815305
MAE train: 0.541372	val: 0.743454	test: 0.598822

Epoch: 60
Loss: 0.5880058854818344
RMSE train: 0.579609	val: 0.902149	test: 0.773393
MAE train: 0.448647	val: 0.653736	test: 0.566600

Epoch: 61
Loss: 0.47412412613630295
RMSE train: 0.572983	val: 0.869455	test: 0.805104
MAE train: 0.440035	val: 0.653595	test: 0.584833

Epoch: 62
Loss: 0.49084143340587616
RMSE train: 0.601083	val: 0.908954	test: 0.809755
MAE train: 0.463670	val: 0.690225	test: 0.590946

Epoch: 63
Loss: 0.5193252563476562
RMSE train: 0.534436	val: 0.911252	test: 0.782529
MAE train: 0.416578	val: 0.663427	test: 0.566425

Epoch: 64
Loss: 0.5043164640665054
RMSE train: 0.545008	val: 0.901739	test: 0.785649
MAE train: 0.422360	val: 0.663082	test: 0.565365

Epoch: 65
Loss: 0.5125354677438736
RMSE train: 0.603004	val: 0.921946	test: 0.790268
MAE train: 0.463962	val: 0.693447	test: 0.573210

Epoch: 66
Loss: 0.505343109369278
RMSE train: 0.562327	val: 0.886972	test: 0.767521
MAE train: 0.428282	val: 0.641578	test: 0.554679

Epoch: 67
Loss: 0.5295668691396713
RMSE train: 0.595948	val: 0.895619	test: 0.782401
MAE train: 0.450091	val: 0.671338	test: 0.549634

Epoch: 68
Loss: 0.5359480679035187
RMSE train: 0.617504	val: 0.937565	test: 0.784495
MAE train: 0.463416	val: 0.714690	test: 0.553651

Epoch: 69
Loss: 0.4800303727388382
RMSE train: 0.594271	val: 0.925411	test: 0.774242
MAE train: 0.454743	val: 0.700978	test: 0.573623

Epoch: 70
Loss: 0.47116899490356445
RMSE train: 0.585310	val: 0.906550	test: 0.784814
MAE train: 0.447323	val: 0.676285	test: 0.580356

Epoch: 71
Loss: 0.5099750384688377
RMSE train: 0.584140	val: 0.924460	test: 0.794294
MAE train: 0.453320	val: 0.672227	test: 0.577360

Epoch: 72
Loss: 0.5242031961679459
RMSE train: 0.574970	val: 0.924365	test: 0.774390
MAE train: 0.447428	val: 0.683014	test: 0.574432

Epoch: 73
Loss: 0.49427518248558044
RMSE train: 0.598917	val: 0.899156	test: 0.780481
MAE train: 0.459504	val: 0.687411	test: 0.580584

Epoch: 74
Loss: 0.4582628905773163
RMSE train: 0.558990	val: 0.873609	test: 0.767795
MAE train: 0.430263	val: 0.645031	test: 0.559034

Epoch: 75
Loss: 0.49654819071292877
RMSE train: 0.567323	val: 0.957599	test: 0.759720
MAE train: 0.437146	val: 0.690298	test: 0.564331

Epoch: 76
Loss: 0.4901903122663498
RMSE train: 0.551264	val: 0.876104	test: 0.759891
MAE train: 0.428732	val: 0.628104	test: 0.550829

Epoch: 77
Loss: 0.4637987017631531
RMSE train: 0.582787	val: 0.888387	test: 0.756612
MAE train: 0.444141	val: 0.677638	test: 0.552375

Epoch: 78
Loss: 0.49127230793237686
RMSE train: 0.614563	val: 1.002125	test: 0.758099
MAE train: 0.476103	val: 0.734053	test: 0.571998

Epoch: 79
Loss: 0.4632501006126404
RMSE train: 0.545643	val: 0.909608	test: 0.747092
MAE train: 0.428112	val: 0.660760	test: 0.560650

Epoch: 80
Loss: 0.4779777452349663
RMSE train: 0.539858	val: 0.944780	test: 0.761401
MAE train: 0.426918	val: 0.682148	test: 0.565350

Epoch: 81
Loss: 0.42478786408901215
RMSE train: 0.557040	val: 0.966200	test: 0.780098
MAE train: 0.436987	val: 0.703589	test: 0.584716

Epoch: 82
Loss: 0.4664995074272156
RMSE train: 0.537326	val: 0.870866	test: 0.753975
MAE train: 0.418733	val: 0.652461	test: 0.562729

Epoch: 83
Loss: 0.4749661013484001
RMSE train: 0.589223	val: 0.929496	test: 0.782485
MAE train: 0.460067	val: 0.707782	test: 0.587162
RMSE train: 0.998648	val: 1.045129	test: 1.020807
MAE train: 0.761193	val: 0.772511	test: 0.787103

Epoch: 24
Loss: 0.9954179674386978
RMSE train: 0.896933	val: 0.968295	test: 0.972715
MAE train: 0.683606	val: 0.708554	test: 0.758173

Epoch: 25
Loss: 0.9586091488599777
RMSE train: 0.931973	val: 0.995292	test: 1.014441
MAE train: 0.724756	val: 0.770895	test: 0.800535

Epoch: 26
Loss: 0.9165487438440323
RMSE train: 0.927047	val: 1.065454	test: 0.986621
MAE train: 0.718656	val: 0.820792	test: 0.780221

Epoch: 27
Loss: 0.853271946310997
RMSE train: 0.819724	val: 0.979855	test: 0.928074
MAE train: 0.623555	val: 0.729994	test: 0.723018

Epoch: 28
Loss: 0.8079847097396851
RMSE train: 0.753047	val: 0.959557	test: 0.854449
MAE train: 0.564097	val: 0.680305	test: 0.657645

Epoch: 29
Loss: 0.8320713937282562
RMSE train: 0.738564	val: 0.916430	test: 0.864538
MAE train: 0.562334	val: 0.670102	test: 0.680928

Epoch: 30
Loss: 0.8080408722162247
RMSE train: 0.807760	val: 1.014502	test: 0.882125
MAE train: 0.616510	val: 0.768408	test: 0.702044

Epoch: 31
Loss: 0.7396693676710129
RMSE train: 0.845848	val: 1.048597	test: 0.896158
MAE train: 0.634575	val: 0.795595	test: 0.702428

Epoch: 32
Loss: 0.7330883592367172
RMSE train: 0.744930	val: 0.852832	test: 0.889250
MAE train: 0.559681	val: 0.624556	test: 0.678255

Epoch: 33
Loss: 0.8508307784795761
RMSE train: 0.773520	val: 0.945334	test: 0.853297
MAE train: 0.574879	val: 0.696981	test: 0.648850

Epoch: 34
Loss: 0.7619414776563644
RMSE train: 0.891824	val: 1.085718	test: 0.926003
MAE train: 0.677807	val: 0.836920	test: 0.715004

Epoch: 35
Loss: 0.7688993811607361
RMSE train: 0.752117	val: 0.860083	test: 0.914222
MAE train: 0.570386	val: 0.642253	test: 0.703455

Epoch: 36
Loss: 0.8360744118690491
RMSE train: 0.759164	val: 0.876585	test: 0.865750
MAE train: 0.564895	val: 0.643005	test: 0.634451

Epoch: 37
Loss: 0.7910333275794983
RMSE train: 0.789027	val: 0.953342	test: 0.863075
MAE train: 0.584440	val: 0.723477	test: 0.635477

Epoch: 38
Loss: 0.6763458400964737
RMSE train: 0.683202	val: 0.811828	test: 0.850943
MAE train: 0.517692	val: 0.608676	test: 0.654057

Epoch: 39
Loss: 0.7016168832778931
RMSE train: 0.650399	val: 0.845065	test: 0.836987
MAE train: 0.496543	val: 0.630995	test: 0.645359

Epoch: 40
Loss: 0.7386347502470016
RMSE train: 0.666474	val: 0.846389	test: 0.811319
MAE train: 0.500236	val: 0.625298	test: 0.624631

Epoch: 41
Loss: 0.7352573871612549
RMSE train: 0.734483	val: 0.875280	test: 0.851975
MAE train: 0.538224	val: 0.646263	test: 0.633051

Epoch: 42
Loss: 0.6582261919975281
RMSE train: 0.726371	val: 0.842924	test: 0.879877
MAE train: 0.540637	val: 0.618799	test: 0.652973

Epoch: 43
Loss: 0.6256462186574936
RMSE train: 0.711795	val: 0.868670	test: 0.869909
MAE train: 0.538742	val: 0.644271	test: 0.666547

Epoch: 44
Loss: 0.6396229267120361
RMSE train: 0.815321	val: 0.983375	test: 0.913842
MAE train: 0.620118	val: 0.750021	test: 0.686018

Epoch: 45
Loss: 0.6487778574228287
RMSE train: 0.798889	val: 0.937620	test: 0.905106
MAE train: 0.615533	val: 0.697066	test: 0.682079

Epoch: 46
Loss: 0.6916620135307312
RMSE train: 0.779825	val: 0.929082	test: 0.885066
MAE train: 0.599325	val: 0.695928	test: 0.661834

Epoch: 47
Loss: 0.6240931004285812
RMSE train: 0.828323	val: 0.993018	test: 0.918219
MAE train: 0.632797	val: 0.769227	test: 0.687761

Epoch: 48
Loss: 0.6363863199949265
RMSE train: 0.760339	val: 0.920703	test: 0.875659
MAE train: 0.568669	val: 0.705751	test: 0.632527

Epoch: 49
Loss: 0.6210814118385315
RMSE train: 0.671484	val: 0.884335	test: 0.809482
MAE train: 0.501967	val: 0.657412	test: 0.598833

Epoch: 50
Loss: 0.651047945022583
RMSE train: 0.658254	val: 0.844591	test: 0.846259
MAE train: 0.488783	val: 0.616163	test: 0.620956

Epoch: 51
Loss: 0.6329259425401688
RMSE train: 0.709273	val: 0.920844	test: 0.859050
MAE train: 0.540860	val: 0.697714	test: 0.634287

Epoch: 52
Loss: 0.5958208739757538
RMSE train: 0.672363	val: 0.873409	test: 0.832629
MAE train: 0.511071	val: 0.652168	test: 0.617672

Epoch: 53
Loss: 0.5679211318492889
RMSE train: 0.658729	val: 0.868843	test: 0.825506
MAE train: 0.498346	val: 0.631120	test: 0.608596

Epoch: 54
Loss: 0.5878374725580215
RMSE train: 0.697566	val: 0.985045	test: 0.825981
MAE train: 0.515026	val: 0.716079	test: 0.622859

Epoch: 55
Loss: 0.5383892357349396
RMSE train: 0.656974	val: 0.848366	test: 0.833376
MAE train: 0.499258	val: 0.634705	test: 0.607670

Epoch: 56
Loss: 0.5748767703771591
RMSE train: 0.616689	val: 0.820436	test: 0.812942
MAE train: 0.472231	val: 0.605670	test: 0.601221

Epoch: 57
Loss: 0.5515638440847397
RMSE train: 0.570765	val: 0.830337	test: 0.784508
MAE train: 0.425765	val: 0.588773	test: 0.574227

Epoch: 58
Loss: 0.5158163383603096
RMSE train: 0.571216	val: 0.784215	test: 0.794834
MAE train: 0.432734	val: 0.555810	test: 0.573356

Epoch: 59
Loss: 0.5763053596019745
RMSE train: 0.623484	val: 0.828665	test: 0.822694
MAE train: 0.466326	val: 0.611955	test: 0.595940

Epoch: 60
Loss: 0.5770991891622543
RMSE train: 0.689982	val: 0.871187	test: 0.851886
MAE train: 0.515647	val: 0.677535	test: 0.616586

Epoch: 61
Loss: 0.5362160056829453
RMSE train: 0.586328	val: 0.757462	test: 0.798831
MAE train: 0.445695	val: 0.561537	test: 0.595476

Epoch: 62
Loss: 0.5636618062853813
RMSE train: 0.674670	val: 0.842055	test: 0.820025
MAE train: 0.501077	val: 0.634525	test: 0.594555

Epoch: 63
Loss: 0.5425293147563934
RMSE train: 0.684095	val: 0.889412	test: 0.804820
MAE train: 0.508692	val: 0.661083	test: 0.578600

Epoch: 64
Loss: 0.5755562335252762
RMSE train: 0.622132	val: 0.867147	test: 0.755729
MAE train: 0.467602	val: 0.635106	test: 0.542129

Epoch: 65
Loss: 0.48450660705566406
RMSE train: 0.626851	val: 0.880164	test: 0.749289
MAE train: 0.462369	val: 0.639915	test: 0.536999

Epoch: 66
Loss: 0.5575117692351341
RMSE train: 0.724800	val: 0.911190	test: 0.822216
MAE train: 0.530240	val: 0.686396	test: 0.588833

Epoch: 67
Loss: 0.5161423459649086
RMSE train: 0.625034	val: 0.873156	test: 0.763307
MAE train: 0.462706	val: 0.637354	test: 0.561741

Epoch: 68
Loss: 0.5242709964513779
RMSE train: 0.611001	val: 0.840298	test: 0.763355
MAE train: 0.458411	val: 0.624905	test: 0.559193

Epoch: 69
Loss: 0.4442710801959038
RMSE train: 0.596540	val: 0.839824	test: 0.770143
MAE train: 0.447121	val: 0.630471	test: 0.561577

Epoch: 70
Loss: 0.472915418446064
RMSE train: 0.503500	val: 0.774731	test: 0.754627
MAE train: 0.382289	val: 0.557788	test: 0.564098

Epoch: 71
Loss: 0.5132799223065376
RMSE train: 0.546506	val: 0.789434	test: 0.749568
MAE train: 0.406705	val: 0.580931	test: 0.549247

Epoch: 72
Loss: 0.47173523157835007
RMSE train: 0.535351	val: 0.819748	test: 0.730995
MAE train: 0.401935	val: 0.596002	test: 0.544029

Epoch: 73
Loss: 0.4768723249435425
RMSE train: 0.570625	val: 0.778239	test: 0.760905
MAE train: 0.432789	val: 0.571440	test: 0.556726

Epoch: 74
Loss: 0.507686123251915
RMSE train: 0.675721	val: 0.852268	test: 0.800527
MAE train: 0.506127	val: 0.651137	test: 0.568565

Epoch: 75
Loss: 0.6003143265843391
RMSE train: 0.670825	val: 0.843151	test: 0.803412
MAE train: 0.500195	val: 0.643993	test: 0.574910

Epoch: 76
Loss: 0.45980703085660934
RMSE train: 0.712383	val: 0.874558	test: 0.839221
MAE train: 0.536988	val: 0.677609	test: 0.596921

Epoch: 77
Loss: 0.4918549582362175
RMSE train: 0.714294	val: 0.924097	test: 0.832781
MAE train: 0.533754	val: 0.713861	test: 0.590593

Epoch: 78
Loss: 0.4871205911040306
RMSE train: 0.546567	val: 0.816045	test: 0.745866
MAE train: 0.421985	val: 0.609569	test: 0.563464

Epoch: 79
Loss: 0.5050401091575623
RMSE train: 0.566307	val: 0.862265	test: 0.724209
MAE train: 0.424800	val: 0.636302	test: 0.537851

Epoch: 80
Loss: 0.4531899094581604
RMSE train: 0.590135	val: 0.800338	test: 0.764383
MAE train: 0.438538	val: 0.608703	test: 0.549922

Epoch: 81
Loss: 0.4314318299293518
RMSE train: 0.565459	val: 0.774798	test: 0.758184
MAE train: 0.426331	val: 0.585199	test: 0.545003

Epoch: 82
Loss: 0.4501895233988762
RMSE train: 0.581295	val: 0.827779	test: 0.732776
MAE train: 0.431723	val: 0.624292	test: 0.524111

Epoch: 83
Loss: 0.4639310836791992
RMSE train: 0.550130	val: 0.858849	test: 0.706993
MAE train: 0.410220	val: 0.632020	test: 0.518036
RMSE train: 0.826674	val: 1.003372	test: 0.855906
MAE train: 0.631496	val: 0.722066	test: 0.642381

Epoch: 24
Loss: 1.0607717633247375
RMSE train: 0.804671	val: 0.990089	test: 0.806324
MAE train: 0.601197	val: 0.706988	test: 0.596264

Epoch: 25
Loss: 0.9498105049133301
RMSE train: 0.811387	val: 0.942918	test: 0.837530
MAE train: 0.620680	val: 0.705095	test: 0.646638

Epoch: 26
Loss: 1.0406370759010315
RMSE train: 0.827156	val: 0.962434	test: 0.899211
MAE train: 0.640708	val: 0.735966	test: 0.684089

Epoch: 27
Loss: 0.9121072590351105
RMSE train: 0.748562	val: 0.926479	test: 0.872856
MAE train: 0.584446	val: 0.701190	test: 0.682628

Epoch: 28
Loss: 0.8646582663059235
RMSE train: 0.750835	val: 0.896969	test: 0.865589
MAE train: 0.575515	val: 0.670197	test: 0.654493

Epoch: 29
Loss: 0.8936695754528046
RMSE train: 0.710413	val: 0.935610	test: 0.809879
MAE train: 0.556736	val: 0.704021	test: 0.633493

Epoch: 30
Loss: 0.8993409872055054
RMSE train: 0.755291	val: 0.968969	test: 0.864334
MAE train: 0.603579	val: 0.756619	test: 0.697585

Epoch: 31
Loss: 0.8357465416193008
RMSE train: 0.705156	val: 0.854382	test: 0.844960
MAE train: 0.550689	val: 0.661486	test: 0.643390

Epoch: 32
Loss: 0.868167906999588
RMSE train: 0.655379	val: 0.864721	test: 0.778205
MAE train: 0.496125	val: 0.640868	test: 0.594499

Epoch: 33
Loss: 0.8101456463336945
RMSE train: 0.701831	val: 0.838640	test: 0.831399
MAE train: 0.532086	val: 0.644245	test: 0.644267

Epoch: 34
Loss: 0.7878645956516266
RMSE train: 0.765502	val: 0.973905	test: 0.812502
MAE train: 0.597773	val: 0.742943	test: 0.649559

Epoch: 35
Loss: 0.8131527751684189
RMSE train: 0.678293	val: 0.846834	test: 0.809638
MAE train: 0.522616	val: 0.662427	test: 0.631468

Epoch: 36
Loss: 0.7873057723045349
RMSE train: 0.706791	val: 0.892927	test: 0.820286
MAE train: 0.535461	val: 0.674830	test: 0.621596

Epoch: 37
Loss: 0.7609752863645554
RMSE train: 0.694108	val: 0.898926	test: 0.779885
MAE train: 0.519069	val: 0.663227	test: 0.582991

Epoch: 38
Loss: 0.7546830028295517
RMSE train: 0.685805	val: 0.825445	test: 0.796125
MAE train: 0.508923	val: 0.621504	test: 0.601003

Epoch: 39
Loss: 0.7393451929092407
RMSE train: 0.668007	val: 0.908101	test: 0.755891
MAE train: 0.500243	val: 0.674808	test: 0.579910

Epoch: 40
Loss: 0.621571958065033
RMSE train: 0.637260	val: 0.872896	test: 0.759921
MAE train: 0.475476	val: 0.645225	test: 0.563946

Epoch: 41
Loss: 0.6674708724021912
RMSE train: 0.637344	val: 0.861201	test: 0.798442
MAE train: 0.488804	val: 0.644247	test: 0.598128

Epoch: 42
Loss: 0.762483537197113
RMSE train: 0.627389	val: 0.879921	test: 0.812647
MAE train: 0.483267	val: 0.641985	test: 0.620375

Epoch: 43
Loss: 0.6758630275726318
RMSE train: 0.632932	val: 0.855057	test: 0.824116
MAE train: 0.489604	val: 0.648254	test: 0.632912

Epoch: 44
Loss: 0.6134367138147354
RMSE train: 0.649423	val: 0.820909	test: 0.800910
MAE train: 0.482396	val: 0.639790	test: 0.594327

Epoch: 45
Loss: 0.6709427833557129
RMSE train: 0.697982	val: 0.869993	test: 0.805664
MAE train: 0.520479	val: 0.668920	test: 0.588588

Epoch: 46
Loss: 0.6824448257684708
RMSE train: 0.652341	val: 0.855265	test: 0.777754
MAE train: 0.485907	val: 0.631349	test: 0.555246

Epoch: 47
Loss: 0.6354803144931793
RMSE train: 0.629179	val: 0.834167	test: 0.760474
MAE train: 0.468873	val: 0.639192	test: 0.552898

Epoch: 48
Loss: 0.5858568549156189
RMSE train: 0.638537	val: 0.823817	test: 0.770263
MAE train: 0.470143	val: 0.637597	test: 0.560082

Epoch: 49
Loss: 0.637172058224678
RMSE train: 0.663322	val: 0.837372	test: 0.779611
MAE train: 0.491958	val: 0.641547	test: 0.565838

Epoch: 50
Loss: 0.500360757112503
RMSE train: 0.677290	val: 0.865284	test: 0.772698
MAE train: 0.503868	val: 0.660504	test: 0.559848

Epoch: 51
Loss: 0.5700914561748505
RMSE train: 0.628102	val: 0.847037	test: 0.745900
MAE train: 0.469885	val: 0.635312	test: 0.559798

Epoch: 52
Loss: 0.595748782157898
RMSE train: 0.630272	val: 0.869765	test: 0.733922
MAE train: 0.473338	val: 0.664601	test: 0.552838

Epoch: 53
Loss: 0.5872790962457657
RMSE train: 0.666168	val: 0.864289	test: 0.750295
MAE train: 0.491005	val: 0.663534	test: 0.557238

Epoch: 54
Loss: 0.6041037365794182
RMSE train: 0.591166	val: 0.817939	test: 0.738239
MAE train: 0.443953	val: 0.590048	test: 0.553591

Epoch: 55
Loss: 0.5484028533101082
RMSE train: 0.642826	val: 0.857557	test: 0.762260
MAE train: 0.479910	val: 0.637131	test: 0.562693

Epoch: 56
Loss: 0.5608704537153244
RMSE train: 0.619160	val: 0.835442	test: 0.754266
MAE train: 0.461046	val: 0.633972	test: 0.554232

Epoch: 57
Loss: 0.5608636289834976
RMSE train: 0.651085	val: 0.846912	test: 0.764774
MAE train: 0.483595	val: 0.652461	test: 0.557069

Epoch: 58
Loss: 0.5607926696538925
RMSE train: 0.724781	val: 0.969963	test: 0.771269
MAE train: 0.551328	val: 0.739956	test: 0.579586

Epoch: 59
Loss: 0.5197878926992416
RMSE train: 0.611150	val: 0.842965	test: 0.729273
MAE train: 0.449366	val: 0.635983	test: 0.542003

Epoch: 60
Loss: 0.553791269659996
RMSE train: 0.671463	val: 0.906221	test: 0.760747
MAE train: 0.494440	val: 0.696180	test: 0.554047

Epoch: 61
Loss: 0.5415880605578423
RMSE train: 0.664311	val: 0.869336	test: 0.772699
MAE train: 0.490900	val: 0.668758	test: 0.558863

Epoch: 62
Loss: 0.5186342895030975
RMSE train: 0.650669	val: 0.860502	test: 0.770863
MAE train: 0.482625	val: 0.661456	test: 0.557072

Epoch: 63
Loss: 0.49694540351629257
RMSE train: 0.589350	val: 0.800346	test: 0.758344
MAE train: 0.439956	val: 0.607647	test: 0.553990

Epoch: 64
Loss: 0.5889284238219261
RMSE train: 0.605922	val: 0.852532	test: 0.743441
MAE train: 0.454000	val: 0.657212	test: 0.553698

Epoch: 65
Loss: 0.6045594960451126
RMSE train: 0.598045	val: 0.817638	test: 0.738770
MAE train: 0.444340	val: 0.623984	test: 0.535609

Epoch: 66
Loss: 0.6017083823680878
RMSE train: 0.624389	val: 0.825136	test: 0.736827
MAE train: 0.467597	val: 0.596753	test: 0.536770

Epoch: 67
Loss: 0.5087752342224121
RMSE train: 0.740396	val: 0.929931	test: 0.790328
MAE train: 0.554875	val: 0.718667	test: 0.578788

Epoch: 68
Loss: 0.5261844918131828
RMSE train: 0.686506	val: 0.940713	test: 0.756642
MAE train: 0.511912	val: 0.725875	test: 0.577050

Epoch: 69
Loss: 0.5417902916669846
RMSE train: 0.590138	val: 0.823170	test: 0.737669
MAE train: 0.442020	val: 0.631720	test: 0.549517

Epoch: 70
Loss: 0.5252947434782982
RMSE train: 0.646408	val: 0.819736	test: 0.772282
MAE train: 0.476936	val: 0.637259	test: 0.571571

Epoch: 71
Loss: 0.4606441855430603
RMSE train: 0.681416	val: 0.870099	test: 0.760874
MAE train: 0.507778	val: 0.674667	test: 0.572199

Epoch: 72
Loss: 0.5833014994859695
RMSE train: 0.619675	val: 0.790341	test: 0.737117
MAE train: 0.453101	val: 0.606185	test: 0.538208

Epoch: 73
Loss: 0.4907902851700783
RMSE train: 0.623543	val: 0.779327	test: 0.753782
MAE train: 0.458986	val: 0.598824	test: 0.543356

Epoch: 74
Loss: 0.5520980209112167
RMSE train: 0.642475	val: 0.913602	test: 0.756783
MAE train: 0.487758	val: 0.688365	test: 0.568008

Epoch: 75
Loss: 0.5052682086825371
RMSE train: 0.517332	val: 0.769506	test: 0.747637
MAE train: 0.396368	val: 0.570985	test: 0.555379

Epoch: 76
Loss: 0.44524771720170975
RMSE train: 0.537549	val: 0.838780	test: 0.742244
MAE train: 0.415539	val: 0.618227	test: 0.560271

Epoch: 77
Loss: 0.4910692349076271
RMSE train: 0.548628	val: 0.826736	test: 0.740934
MAE train: 0.417217	val: 0.620576	test: 0.551762

Epoch: 78
Loss: 0.442936971783638
RMSE train: 0.535184	val: 0.802670	test: 0.743195
MAE train: 0.406998	val: 0.608712	test: 0.552608

Epoch: 79
Loss: 0.4834112524986267
RMSE train: 0.537993	val: 0.804786	test: 0.736305
MAE train: 0.411146	val: 0.602806	test: 0.561220

Epoch: 80
Loss: 0.45852623879909515
RMSE train: 0.561927	val: 0.825536	test: 0.762470
MAE train: 0.438402	val: 0.611921	test: 0.576023

Epoch: 81
Loss: 0.4912392497062683
RMSE train: 0.541507	val: 0.851925	test: 0.767343
MAE train: 0.436626	val: 0.628981	test: 0.585604

Epoch: 82
Loss: 0.45481210947036743
RMSE train: 0.529603	val: 0.804497	test: 0.744772
MAE train: 0.417143	val: 0.608886	test: 0.568604

Epoch: 83
Loss: 0.47840531915426254
RMSE train: 0.621596	val: 0.865688	test: 0.775318
MAE train: 0.481890	val: 0.666229	test: 0.586105
RMSE train: 0.557847	val: 0.763940	test: 0.834148
MAE train: 0.421900	val: 0.576458	test: 0.613389

Epoch: 85
Loss: 0.545913835366567
RMSE train: 0.578873	val: 0.778962	test: 0.859509
MAE train: 0.451215	val: 0.596261	test: 0.630747

Epoch: 86
Loss: 0.45480286081631977
RMSE train: 0.539299	val: 0.749044	test: 0.845908
MAE train: 0.420183	val: 0.562776	test: 0.609520

Epoch: 87
Loss: 0.47637619574864704
RMSE train: 0.543634	val: 0.749522	test: 0.850089
MAE train: 0.415271	val: 0.553801	test: 0.616734

Epoch: 88
Loss: 0.4920792281627655
RMSE train: 0.570319	val: 0.770614	test: 0.855014
MAE train: 0.429597	val: 0.568668	test: 0.628427

Epoch: 89
Loss: 0.4582911630471547
RMSE train: 0.547405	val: 0.758228	test: 0.839836
MAE train: 0.413651	val: 0.569997	test: 0.617046

Epoch: 90
Loss: 0.47765182455380756
RMSE train: 0.503933	val: 0.725318	test: 0.823857
MAE train: 0.385572	val: 0.544072	test: 0.602587

Epoch: 91
Loss: 0.4659812053044637
RMSE train: 0.512326	val: 0.726715	test: 0.847415
MAE train: 0.393809	val: 0.535634	test: 0.615269

Epoch: 92
Loss: 0.46084922552108765
RMSE train: 0.538876	val: 0.743525	test: 0.859858
MAE train: 0.414293	val: 0.547424	test: 0.632372

Epoch: 93
Loss: 0.4805605312188466
RMSE train: 0.513708	val: 0.736324	test: 0.853299
MAE train: 0.396131	val: 0.549333	test: 0.633800

Epoch: 94
Loss: 0.4595215717951457
RMSE train: 0.492806	val: 0.731251	test: 0.866845
MAE train: 0.385728	val: 0.555224	test: 0.634109

Epoch: 95
Loss: 0.47362690170605976
RMSE train: 0.530825	val: 0.749419	test: 0.895550
MAE train: 0.414324	val: 0.563018	test: 0.636809

Epoch: 96
Loss: 0.4430928627649943
RMSE train: 0.548573	val: 0.754430	test: 0.867353
MAE train: 0.426939	val: 0.561686	test: 0.631088

Epoch: 97
Loss: 0.4587257305781047
RMSE train: 0.532705	val: 0.753610	test: 0.846395
MAE train: 0.411446	val: 0.562899	test: 0.624205

Epoch: 98
Loss: 0.41882437467575073
RMSE train: 0.523984	val: 0.750772	test: 0.833799
MAE train: 0.400023	val: 0.557466	test: 0.612067

Epoch: 99
Loss: 0.4991205632686615
RMSE train: 0.546816	val: 0.765764	test: 0.859819
MAE train: 0.416085	val: 0.571030	test: 0.624603

Epoch: 100
Loss: 0.44305118918418884
RMSE train: 0.532293	val: 0.756418	test: 0.842265
MAE train: 0.405321	val: 0.562291	test: 0.608656

Epoch: 101
Loss: 0.47608160972595215
RMSE train: 0.518491	val: 0.754705	test: 0.858761
MAE train: 0.400101	val: 0.569065	test: 0.633739

Epoch: 102
Loss: 0.44555996855099994
RMSE train: 0.492636	val: 0.734162	test: 0.855057
MAE train: 0.381751	val: 0.554205	test: 0.614707

Epoch: 103
Loss: 0.43463419874509174
RMSE train: 0.531084	val: 0.752309	test: 0.858560
MAE train: 0.408403	val: 0.563664	test: 0.626564

Epoch: 104
Loss: 0.4353264570236206
RMSE train: 0.522671	val: 0.745068	test: 0.830065
MAE train: 0.393182	val: 0.539574	test: 0.601348

Epoch: 105
Loss: 0.4207421938578288
RMSE train: 0.508697	val: 0.739641	test: 0.829335
MAE train: 0.375448	val: 0.528277	test: 0.597814

Epoch: 106
Loss: 0.41012202699979144
RMSE train: 0.527263	val: 0.742619	test: 0.835827
MAE train: 0.393081	val: 0.536191	test: 0.606329

Epoch: 107
Loss: 0.39414456486701965
RMSE train: 0.514586	val: 0.738114	test: 0.827907
MAE train: 0.389093	val: 0.539553	test: 0.604059

Epoch: 108
Loss: 0.4099464813868205
RMSE train: 0.498445	val: 0.739205	test: 0.830245
MAE train: 0.382237	val: 0.547131	test: 0.603151

Epoch: 109
Loss: 0.39138202865918476
RMSE train: 0.496235	val: 0.740823	test: 0.833236
MAE train: 0.381034	val: 0.543049	test: 0.594534

Epoch: 110
Loss: 0.4145525594552358
RMSE train: 0.489276	val: 0.734663	test: 0.827895
MAE train: 0.370855	val: 0.525023	test: 0.595209

Epoch: 111
Loss: 0.41827474037806195
RMSE train: 0.561762	val: 0.791456	test: 0.879081
MAE train: 0.417804	val: 0.573666	test: 0.642763

Epoch: 112
Loss: 0.44071343541145325
RMSE train: 0.518182	val: 0.758308	test: 0.884965
MAE train: 0.391712	val: 0.558395	test: 0.639195

Epoch: 113
Loss: 0.3799903591473897
RMSE train: 0.510703	val: 0.737326	test: 0.852867
MAE train: 0.389882	val: 0.544336	test: 0.618745

Epoch: 114
Loss: 0.40834524234135944
RMSE train: 0.520594	val: 0.754860	test: 0.863614
MAE train: 0.397660	val: 0.563807	test: 0.629212

Epoch: 115
Loss: 0.3768582244714101
RMSE train: 0.501070	val: 0.750238	test: 0.885898
MAE train: 0.381798	val: 0.553424	test: 0.637191

Epoch: 116
Loss: 0.396078219016393
RMSE train: 0.483580	val: 0.726406	test: 0.846838
MAE train: 0.371740	val: 0.530288	test: 0.604703

Epoch: 117
Loss: 0.37337998549143475
RMSE train: 0.519370	val: 0.740757	test: 0.842381
MAE train: 0.398813	val: 0.542030	test: 0.611808

Epoch: 118
Loss: 0.3541980783144633
RMSE train: 0.518179	val: 0.748504	test: 0.860320
MAE train: 0.394240	val: 0.541140	test: 0.621322

Epoch: 119
Loss: 0.41415854295094806
RMSE train: 0.503584	val: 0.742320	test: 0.850230
MAE train: 0.383283	val: 0.535129	test: 0.611557

Epoch: 120
Loss: 0.3230653405189514
RMSE train: 0.483905	val: 0.728522	test: 0.832408
MAE train: 0.371093	val: 0.532284	test: 0.603865

Epoch: 121
Loss: 0.3875681757926941
RMSE train: 0.472670	val: 0.725743	test: 0.828483
MAE train: 0.361964	val: 0.529556	test: 0.600180

Epoch: 122
Loss: 0.37622588872909546
RMSE train: 0.503512	val: 0.742519	test: 0.833144
MAE train: 0.381873	val: 0.537459	test: 0.604860

Epoch: 123
Loss: 0.36125196019808453
RMSE train: 0.495020	val: 0.736241	test: 0.818123
MAE train: 0.378885	val: 0.532253	test: 0.595644

Epoch: 124
Loss: 0.37490647037823993
RMSE train: 0.498019	val: 0.748971	test: 0.856507
MAE train: 0.381341	val: 0.547657	test: 0.618113

Epoch: 125
Loss: 0.3837786714235942
RMSE train: 0.507445	val: 0.750762	test: 0.852497
MAE train: 0.387895	val: 0.556481	test: 0.623578

Early stopping
Best (RMSE):	 train: 0.503933	val: 0.725318	test: 0.823857
Best (MAE):	 train: 0.385572	val: 0.544072	test: 0.602587
Epoch: 84
Loss: 0.49472910165786743
RMSE train: 0.639967	val: 0.836123	test: 0.861070
MAE train: 0.478110	val: 0.605752	test: 0.640772

Epoch: 85
Loss: 0.44146570563316345
RMSE train: 0.589035	val: 0.797303	test: 0.830775
MAE train: 0.439967	val: 0.577775	test: 0.623298

Epoch: 86
Loss: 0.4394245743751526
RMSE train: 0.601681	val: 0.800280	test: 0.825655
MAE train: 0.449293	val: 0.574876	test: 0.625794

Epoch: 87
Loss: 0.4714845021565755
RMSE train: 0.648911	val: 0.828923	test: 0.852895
MAE train: 0.480043	val: 0.590729	test: 0.642653

Epoch: 88
Loss: 0.4194454451402028
RMSE train: 0.667494	val: 0.835762	test: 0.861162
MAE train: 0.492395	val: 0.600433	test: 0.645327

Epoch: 89
Loss: 0.4488780200481415
RMSE train: 0.622054	val: 0.811912	test: 0.841644
MAE train: 0.457568	val: 0.579606	test: 0.626943

Epoch: 90
Loss: 0.43162958820660907
RMSE train: 0.580028	val: 0.788329	test: 0.822503
MAE train: 0.428740	val: 0.568176	test: 0.613605

Epoch: 91
Loss: 0.4458811382452647
RMSE train: 0.572268	val: 0.780367	test: 0.806272
MAE train: 0.428452	val: 0.565359	test: 0.600145

Epoch: 92
Loss: 0.5923839211463928
RMSE train: 0.628532	val: 0.828289	test: 0.844065
MAE train: 0.462370	val: 0.592017	test: 0.620573

Epoch: 93
Loss: 0.45199355483055115
RMSE train: 0.630750	val: 0.825379	test: 0.853056
MAE train: 0.474479	val: 0.594677	test: 0.638166

Epoch: 94
Loss: 0.49486419558525085
RMSE train: 0.597265	val: 0.804728	test: 0.849459
MAE train: 0.452497	val: 0.575773	test: 0.642303

Epoch: 95
Loss: 0.4537900586922963
RMSE train: 0.608618	val: 0.820015	test: 0.857449
MAE train: 0.458912	val: 0.587202	test: 0.634804

Epoch: 96
Loss: 0.48821479082107544
RMSE train: 0.615599	val: 0.818055	test: 0.853383
MAE train: 0.465015	val: 0.583902	test: 0.641451

Epoch: 97
Loss: 0.40471189220746356
RMSE train: 0.637768	val: 0.837926	test: 0.880150
MAE train: 0.469347	val: 0.598079	test: 0.663016

Epoch: 98
Loss: 0.4245331486066182
RMSE train: 0.599652	val: 0.800195	test: 0.830499
MAE train: 0.445193	val: 0.569214	test: 0.627808

Epoch: 99
Loss: 0.4724750717480977
RMSE train: 0.623106	val: 0.817792	test: 0.831775
MAE train: 0.461443	val: 0.589775	test: 0.623084

Epoch: 100
Loss: 0.4562590221563975
RMSE train: 0.564730	val: 0.781801	test: 0.813916
MAE train: 0.425208	val: 0.576607	test: 0.614499

Epoch: 101
Loss: 0.38907740513483685
RMSE train: 0.525262	val: 0.771881	test: 0.835126
MAE train: 0.401533	val: 0.575649	test: 0.624274

Epoch: 102
Loss: 0.4989033341407776
RMSE train: 0.512599	val: 0.750133	test: 0.809194
MAE train: 0.382813	val: 0.554636	test: 0.597952

Epoch: 103
Loss: 0.4813210566838582
RMSE train: 0.573972	val: 0.790122	test: 0.830237
MAE train: 0.421658	val: 0.583015	test: 0.605663

Epoch: 104
Loss: 0.4360747238000234
RMSE train: 0.551073	val: 0.775523	test: 0.820087
MAE train: 0.418328	val: 0.582253	test: 0.610849

Epoch: 105
Loss: 0.4086417257785797
RMSE train: 0.563563	val: 0.793751	test: 0.834263
MAE train: 0.424069	val: 0.589688	test: 0.623251

Epoch: 106
Loss: 0.4583637515703837
RMSE train: 0.589221	val: 0.805211	test: 0.840910
MAE train: 0.438595	val: 0.585340	test: 0.620849

Epoch: 107
Loss: 0.4240916570027669
RMSE train: 0.588841	val: 0.807792	test: 0.845701
MAE train: 0.437586	val: 0.580460	test: 0.625268

Epoch: 108
Loss: 0.4152294596036275
RMSE train: 0.571004	val: 0.802830	test: 0.854199
MAE train: 0.425834	val: 0.582072	test: 0.644652

Epoch: 109
Loss: 0.40648401776949566
RMSE train: 0.581174	val: 0.805858	test: 0.872109
MAE train: 0.438241	val: 0.602791	test: 0.661589

Epoch: 110
Loss: 0.39467166860898334
RMSE train: 0.559009	val: 0.767741	test: 0.853317
MAE train: 0.436302	val: 0.589259	test: 0.646459

Epoch: 111
Loss: 0.3666953245798747
RMSE train: 0.494858	val: 0.731750	test: 0.839371
MAE train: 0.392997	val: 0.581320	test: 0.628528

Epoch: 112
Loss: 0.4249456524848938
RMSE train: 0.474200	val: 0.729432	test: 0.851645
MAE train: 0.376148	val: 0.571034	test: 0.631099

Epoch: 113
Loss: 0.39535102248191833
RMSE train: 0.511195	val: 0.757006	test: 0.855436
MAE train: 0.392975	val: 0.562684	test: 0.630804

Epoch: 114
Loss: 0.38725664218266803
RMSE train: 0.577799	val: 0.794581	test: 0.874325
MAE train: 0.437286	val: 0.589998	test: 0.650457

Epoch: 115
Loss: 0.3940982222557068
RMSE train: 0.546442	val: 0.777370	test: 0.846013
MAE train: 0.416942	val: 0.581461	test: 0.638950

Epoch: 116
Loss: 0.4365767439206441
RMSE train: 0.495470	val: 0.757759	test: 0.839643
MAE train: 0.382651	val: 0.560918	test: 0.623162

Epoch: 117
Loss: 0.41171502073605853
RMSE train: 0.499543	val: 0.760348	test: 0.831560
MAE train: 0.383847	val: 0.561153	test: 0.618285

Epoch: 118
Loss: 0.3674141267935435
RMSE train: 0.544325	val: 0.786431	test: 0.838584
MAE train: 0.413876	val: 0.576836	test: 0.626131

Epoch: 119
Loss: 0.3480177919069926
RMSE train: 0.569731	val: 0.799325	test: 0.851852
MAE train: 0.432818	val: 0.572140	test: 0.634645

Epoch: 120
Loss: 0.3676876127719879
RMSE train: 0.575447	val: 0.811756	test: 0.867377
MAE train: 0.432455	val: 0.578577	test: 0.650220

Epoch: 121
Loss: 0.4212908943494161
RMSE train: 0.536235	val: 0.783071	test: 0.846993
MAE train: 0.403690	val: 0.570226	test: 0.629497

Epoch: 122
Loss: 0.34346871574719745
RMSE train: 0.499188	val: 0.749036	test: 0.816577
MAE train: 0.381887	val: 0.561103	test: 0.602725

Epoch: 123
Loss: 0.39018551508585614
RMSE train: 0.480325	val: 0.737413	test: 0.802171
MAE train: 0.366324	val: 0.551836	test: 0.597972

Epoch: 124
Loss: 0.40082529187202454
RMSE train: 0.535269	val: 0.771310	test: 0.814514
MAE train: 0.400795	val: 0.560185	test: 0.606859

Epoch: 125
Loss: 0.3654870887597402
RMSE train: 0.562305	val: 0.793139	test: 0.839190
MAE train: 0.417899	val: 0.567441	test: 0.619921

Epoch: 126
Loss: 0.3307849168777466
RMSE train: 0.572236	val: 0.797983	test: 0.857299
MAE train: 0.423661	val: 0.568362	test: 0.628104

Epoch: 127
Loss: 0.32772086064020794
RMSE train: 0.586053	val: 0.803484	test: 0.861545
MAE train: 0.435596	val: 0.575856	test: 0.634708

Epoch: 128
Loss: 0.37968583901723224
RMSE train: 0.562209	val: 0.798379	test: 0.855310
MAE train: 0.422319	val: 0.580523	test: 0.632203

Epoch: 129
Loss: 0.35917578140894574
RMSE train: 0.496462	val: 0.761650	test: 0.843319
MAE train: 0.377406	val: 0.558192	test: 0.620110

Epoch: 130
Loss: 0.3730774521827698
RMSE train: 0.510309	val: 0.755448	test: 0.866896
MAE train: 0.393752	val: 0.559398	test: 0.638046

Epoch: 131
Loss: 0.37103378772735596
RMSE train: 0.538450	val: 0.766826	test: 0.864441
MAE train: 0.405102	val: 0.565779	test: 0.646376

Epoch: 132
Loss: 0.3130495051542918
RMSE train: 0.527052	val: 0.761875	test: 0.866902
MAE train: 0.396807	val: 0.564695	test: 0.650169

Epoch: 133
Loss: 0.3759732246398926
RMSE train: 0.505795	val: 0.751328	test: 0.869095
MAE train: 0.382102	val: 0.560316	test: 0.647103

Epoch: 134
Loss: 0.37593866388003033
RMSE train: 0.525511	val: 0.774518	test: 0.879177
MAE train: 0.399806	val: 0.579620	test: 0.651904

Epoch: 135
Loss: 0.36583693822224933
RMSE train: 0.520856	val: 0.766425	test: 0.868837
MAE train: 0.392078	val: 0.571798	test: 0.647612

Epoch: 136
Loss: 0.3251517117023468
RMSE train: 0.478753	val: 0.734741	test: 0.864223
MAE train: 0.368351	val: 0.547658	test: 0.643880

Epoch: 137
Loss: 0.35584856073061627
RMSE train: 0.517267	val: 0.753058	test: 0.853766
MAE train: 0.391332	val: 0.557982	test: 0.638422

Epoch: 138
Loss: 0.35348063707351685
RMSE train: 0.519589	val: 0.769647	test: 0.860610
MAE train: 0.391568	val: 0.569607	test: 0.631813

Epoch: 139
Loss: 0.34656769037246704
RMSE train: 0.506814	val: 0.767577	test: 0.862801
MAE train: 0.384010	val: 0.564891	test: 0.631863

Epoch: 140
Loss: 0.3117794096469879
RMSE train: 0.514415	val: 0.771043	test: 0.842694
MAE train: 0.388803	val: 0.548407	test: 0.615424

Epoch: 141
Loss: 0.31616955002148944
RMSE train: 0.526877	val: 0.775380	test: 0.844381
MAE train: 0.398577	val: 0.551106	test: 0.620080

Epoch: 142
Loss: 0.3126433491706848
RMSE train: 0.540850	val: 0.786504	test: 0.871948
MAE train: 0.408318	val: 0.568486	test: 0.648142

Epoch: 143
Loss: 0.362445463736852
RMSE train: 0.556127	val: 0.792007	test: 0.871301
MAE train: 0.422284	val: 0.575558	test: 0.650673

Epoch: 144
Loss: 0.33242709438006085Epoch: 84
Loss: 0.506300151348114
RMSE train: 0.606343	val: 0.777418	test: 0.845542
MAE train: 0.458594	val: 0.559409	test: 0.632699

Epoch: 85
Loss: 0.5110522011915842
RMSE train: 0.627331	val: 0.790859	test: 0.903693
MAE train: 0.480185	val: 0.581116	test: 0.677636

Epoch: 86
Loss: 0.48047401507695514
RMSE train: 0.569072	val: 0.748305	test: 0.851147
MAE train: 0.437745	val: 0.539177	test: 0.629642

Epoch: 87
Loss: 0.4692059854666392
RMSE train: 0.582138	val: 0.761263	test: 0.871300
MAE train: 0.448742	val: 0.549920	test: 0.646721

Epoch: 88
Loss: 0.49581876397132874
RMSE train: 0.591547	val: 0.778595	test: 0.865927
MAE train: 0.456248	val: 0.567867	test: 0.647748

Epoch: 89
Loss: 0.47602911790211994
RMSE train: 0.542561	val: 0.752347	test: 0.830789
MAE train: 0.417847	val: 0.544834	test: 0.618994

Epoch: 90
Loss: 0.4243345359961192
RMSE train: 0.531447	val: 0.740927	test: 0.850739
MAE train: 0.413769	val: 0.540110	test: 0.625125

Epoch: 91
Loss: 0.47092575828234357
RMSE train: 0.528728	val: 0.732187	test: 0.848013
MAE train: 0.412465	val: 0.538379	test: 0.619839

Epoch: 92
Loss: 0.48945969343185425
RMSE train: 0.540160	val: 0.741909	test: 0.855347
MAE train: 0.423783	val: 0.550807	test: 0.625893

Epoch: 93
Loss: 0.43292630712191266
RMSE train: 0.547182	val: 0.750109	test: 0.843849
MAE train: 0.423322	val: 0.553952	test: 0.632043

Epoch: 94
Loss: 0.48722007870674133
RMSE train: 0.545192	val: 0.744313	test: 0.840136
MAE train: 0.417112	val: 0.543696	test: 0.629541

Epoch: 95
Loss: 0.3955128689606984
RMSE train: 0.611753	val: 0.781965	test: 0.876418
MAE train: 0.466328	val: 0.582310	test: 0.653313

Epoch: 96
Loss: 0.3850378195444743
RMSE train: 0.560726	val: 0.745761	test: 0.830179
MAE train: 0.421385	val: 0.540002	test: 0.622228

Epoch: 97
Loss: 0.43979355692863464
RMSE train: 0.558617	val: 0.732319	test: 0.842789
MAE train: 0.422799	val: 0.529603	test: 0.627957

Epoch: 98
Loss: 0.4578884045283
RMSE train: 0.565050	val: 0.735439	test: 0.848772
MAE train: 0.428186	val: 0.530019	test: 0.627975

Epoch: 99
Loss: 0.4169473350048065
RMSE train: 0.596717	val: 0.756315	test: 0.868684
MAE train: 0.451560	val: 0.542354	test: 0.640172

Epoch: 100
Loss: 0.4547323981920878
RMSE train: 0.629270	val: 0.797335	test: 0.866562
MAE train: 0.474569	val: 0.574604	test: 0.644061

Epoch: 101
Loss: 0.41990046699841815
RMSE train: 0.543254	val: 0.752681	test: 0.833823
MAE train: 0.418077	val: 0.545887	test: 0.614377

Epoch: 102
Loss: 0.41815564036369324
RMSE train: 0.537149	val: 0.748696	test: 0.851636
MAE train: 0.416374	val: 0.538739	test: 0.619075

Epoch: 103
Loss: 0.40789805849393207
RMSE train: 0.566733	val: 0.748639	test: 0.837926
MAE train: 0.433083	val: 0.530538	test: 0.617854

Epoch: 104
Loss: 0.36878188451131183
RMSE train: 0.664643	val: 0.819056	test: 0.896661
MAE train: 0.489013	val: 0.581594	test: 0.667562

Epoch: 105
Loss: 0.44222864508628845
RMSE train: 0.579289	val: 0.757633	test: 0.845984
MAE train: 0.434335	val: 0.536893	test: 0.623373

Epoch: 106
Loss: 0.40600207448005676
RMSE train: 0.545164	val: 0.735471	test: 0.835660
MAE train: 0.418364	val: 0.526571	test: 0.614813

Epoch: 107
Loss: 0.3624359766642253
RMSE train: 0.603514	val: 0.780066	test: 0.865690
MAE train: 0.465045	val: 0.567469	test: 0.646472

Epoch: 108
Loss: 0.3894044856230418
RMSE train: 0.666155	val: 0.828520	test: 0.901439
MAE train: 0.510043	val: 0.607666	test: 0.678597

Epoch: 109
Loss: 0.40460216999053955
RMSE train: 0.574434	val: 0.766406	test: 0.847639
MAE train: 0.443757	val: 0.554490	test: 0.626746

Epoch: 110
Loss: 0.3644711375236511
RMSE train: 0.549696	val: 0.750344	test: 0.820688
MAE train: 0.421472	val: 0.545791	test: 0.600224

Epoch: 111
Loss: 0.44255897402763367
RMSE train: 0.609436	val: 0.781766	test: 0.862634
MAE train: 0.459153	val: 0.564850	test: 0.637074

Epoch: 112
Loss: 0.3530488610267639
RMSE train: 0.626290	val: 0.805036	test: 0.850384
MAE train: 0.465568	val: 0.581350	test: 0.629503

Epoch: 113
Loss: 0.41055893898010254
RMSE train: 0.504349	val: 0.732340	test: 0.801424
MAE train: 0.387662	val: 0.539891	test: 0.596879

Epoch: 114
Loss: 0.36369362473487854
RMSE train: 0.516308	val: 0.726527	test: 0.842832
MAE train: 0.397817	val: 0.530766	test: 0.617962

Epoch: 115
Loss: 0.3879384795824687
RMSE train: 0.551290	val: 0.741942	test: 0.845862
MAE train: 0.418853	val: 0.548508	test: 0.622318

Epoch: 116
Loss: 0.37933462858200073
RMSE train: 0.499046	val: 0.719534	test: 0.812188
MAE train: 0.382399	val: 0.531903	test: 0.591076

Epoch: 117
Loss: 0.37176550428072613
RMSE train: 0.478754	val: 0.714534	test: 0.805872
MAE train: 0.366978	val: 0.524463	test: 0.586286

Epoch: 118
Loss: 0.38291802008946735
RMSE train: 0.519736	val: 0.721690	test: 0.826654
MAE train: 0.397426	val: 0.514628	test: 0.608488

Epoch: 119
Loss: 0.4318794012069702
RMSE train: 0.569544	val: 0.739493	test: 0.831201
MAE train: 0.435822	val: 0.528741	test: 0.611933

Epoch: 120
Loss: 0.40655654668807983
RMSE train: 0.572877	val: 0.757150	test: 0.819117
MAE train: 0.436828	val: 0.551737	test: 0.604784

Epoch: 121
Loss: 0.3977111379305522
RMSE train: 0.531728	val: 0.725342	test: 0.817924
MAE train: 0.407538	val: 0.525692	test: 0.593328

Epoch: 122
Loss: 0.33885355790456134
RMSE train: 0.581912	val: 0.768845	test: 0.881555
MAE train: 0.444373	val: 0.567119	test: 0.645895

Epoch: 123
Loss: 0.41098036368687946
RMSE train: 0.509325	val: 0.717660	test: 0.827818
MAE train: 0.393550	val: 0.527259	test: 0.608315

Epoch: 124
Loss: 0.3422575394312541
RMSE train: 0.484105	val: 0.715862	test: 0.801647
MAE train: 0.368087	val: 0.524957	test: 0.587547

Epoch: 125
Loss: 0.37769046425819397
RMSE train: 0.524016	val: 0.733981	test: 0.818961
MAE train: 0.402622	val: 0.540775	test: 0.602403

Epoch: 126
Loss: 0.3480383058389028
RMSE train: 0.537238	val: 0.742185	test: 0.847169
MAE train: 0.414154	val: 0.544551	test: 0.616401

Epoch: 127
Loss: 0.3665884534517924
RMSE train: 0.495104	val: 0.725178	test: 0.816527
MAE train: 0.378860	val: 0.521845	test: 0.592807

Epoch: 128
Loss: 0.4266480008761088
RMSE train: 0.506655	val: 0.727553	test: 0.832741
MAE train: 0.386366	val: 0.524452	test: 0.604340

Epoch: 129
Loss: 0.3943336109320323
RMSE train: 0.551359	val: 0.742511	test: 0.883540
MAE train: 0.423879	val: 0.546003	test: 0.632389

Epoch: 130
Loss: 0.42316537102063495
RMSE train: 0.527685	val: 0.731523	test: 0.824541
MAE train: 0.408421	val: 0.540945	test: 0.608055

Epoch: 131
Loss: 0.35208844145139057
RMSE train: 0.536816	val: 0.751210	test: 0.815846
MAE train: 0.408787	val: 0.544793	test: 0.604338

Epoch: 132
Loss: 0.37076138456662494
RMSE train: 0.542926	val: 0.758925	test: 0.871762
MAE train: 0.404934	val: 0.534103	test: 0.617657

Epoch: 133
Loss: 0.3576105038324992
RMSE train: 0.607732	val: 0.800054	test: 0.890593
MAE train: 0.443330	val: 0.573538	test: 0.647739

Epoch: 134
Loss: 0.3541884620984395
RMSE train: 0.560131	val: 0.773781	test: 0.833347
MAE train: 0.421986	val: 0.553671	test: 0.614934

Epoch: 135
Loss: 0.3711077074209849
RMSE train: 0.521893	val: 0.762544	test: 0.879887
MAE train: 0.400175	val: 0.553071	test: 0.624679

Epoch: 136
Loss: 0.37770597139994305
RMSE train: 0.494141	val: 0.735440	test: 0.841024
MAE train: 0.386774	val: 0.545746	test: 0.612906

Epoch: 137
Loss: 0.3421623210112254
RMSE train: 0.502343	val: 0.740982	test: 0.837241
MAE train: 0.387762	val: 0.549176	test: 0.614091

Epoch: 138
Loss: 0.3708101511001587
RMSE train: 0.524478	val: 0.754259	test: 0.864711
MAE train: 0.404794	val: 0.562991	test: 0.631597

Epoch: 139
Loss: 0.433780699968338
RMSE train: 0.522411	val: 0.743179	test: 0.845972
MAE train: 0.401791	val: 0.540954	test: 0.616010

Epoch: 140
Loss: 0.3450342019399007
RMSE train: 0.559507	val: 0.760478	test: 0.847944
MAE train: 0.425321	val: 0.552068	test: 0.620570

Epoch: 141
Loss: 0.3213535447915395
RMSE train: 0.519582	val: 0.744932	test: 0.839520
MAE train: 0.402292	val: 0.545582	test: 0.605718

Epoch: 142
Loss: 0.3069767653942108
RMSE train: 0.480114	val: 0.724997	test: 0.814790
MAE train: 0.372688	val: 0.525567	test: 0.587270

Epoch: 143
Loss: 0.3873988489309947
RMSE train: 0.476018	val: 0.717408	test: 0.812437
MAE train: 0.372823	val: 0.525176	test: 0.591621

Epoch: 144
Loss: 0.3244873483975728
RMSE train: 0.569062	val: 0.800581	test: 0.866498
MAE train: 0.427668	val: 0.577702	test: 0.640630

Epoch: 145
Loss: 0.30689849456151325
RMSE train: 0.514342	val: 0.770458	test: 0.855180
MAE train: 0.391477	val: 0.553307	test: 0.631148

Epoch: 146
Loss: 0.32983551422754925
RMSE train: 0.499132	val: 0.764303	test: 0.856905
MAE train: 0.380583	val: 0.551776	test: 0.628749

Epoch: 147
Loss: 0.3574235538641612
RMSE train: 0.482776	val: 0.756818	test: 0.847608
MAE train: 0.366387	val: 0.552301	test: 0.617452

Early stopping
Best (RMSE):	 train: 0.474200	val: 0.729432	test: 0.851645
Best (MAE):	 train: 0.376148	val: 0.571034	test: 0.631099

RMSE train: 0.487123	val: 0.723609	test: 0.832314
MAE train: 0.386380	val: 0.531071	test: 0.599508

Epoch: 145
Loss: 0.3544343908627828
RMSE train: 0.477910	val: 0.726337	test: 0.824067
MAE train: 0.377416	val: 0.534620	test: 0.595075

Epoch: 146
Loss: 0.3280065854390462
RMSE train: 0.494665	val: 0.742645	test: 0.823413
MAE train: 0.377797	val: 0.541582	test: 0.599095

Epoch: 147
Loss: 0.3892076512177785
RMSE train: 0.531350	val: 0.758161	test: 0.867698
MAE train: 0.397725	val: 0.544390	test: 0.624787

Epoch: 148
Loss: 0.3597583770751953
RMSE train: 0.498276	val: 0.737722	test: 0.822346
MAE train: 0.387154	val: 0.536860	test: 0.606959

Epoch: 149
Loss: 0.3412285844484965
RMSE train: 0.470799	val: 0.735215	test: 0.835037
MAE train: 0.368129	val: 0.545100	test: 0.608144

Epoch: 150
Loss: 0.2959512968858083
RMSE train: 0.444725	val: 0.733376	test: 0.817359
MAE train: 0.346585	val: 0.538181	test: 0.592723

Epoch: 151
Loss: 0.32986681660016376
RMSE train: 0.521386	val: 0.766239	test: 0.842166
MAE train: 0.394274	val: 0.558081	test: 0.617792

Epoch: 152
Loss: 0.3437163134415944
RMSE train: 0.499198	val: 0.752697	test: 0.833658
MAE train: 0.383225	val: 0.556430	test: 0.616575

Early stopping
Best (RMSE):	 train: 0.478754	val: 0.714534	test: 0.805872
Best (MAE):	 train: 0.366978	val: 0.524463	test: 0.586286

RMSE train: 0.745827	val: 0.815641	test: 0.880923
MAE train: 0.552097	val: 0.587766	test: 0.659702

Epoch: 85
Loss: 0.6653833836317062
RMSE train: 0.826326	val: 0.924456	test: 0.967064
MAE train: 0.620322	val: 0.670228	test: 0.738833

Epoch: 86
Loss: 0.8014670759439468
RMSE train: 0.954296	val: 1.047125	test: 1.071038
MAE train: 0.718973	val: 0.763633	test: 0.822046

Epoch: 87
Loss: 0.7435283958911896
RMSE train: 0.724230	val: 0.794574	test: 0.870242
MAE train: 0.546205	val: 0.598225	test: 0.655455

Epoch: 88
Loss: 0.8231408447027206
RMSE train: 0.704437	val: 0.787364	test: 0.825041
MAE train: 0.528352	val: 0.585813	test: 0.621359

Epoch: 89
Loss: 0.6344677656888962
RMSE train: 0.863966	val: 0.978486	test: 0.967662
MAE train: 0.644412	val: 0.700474	test: 0.735096

Epoch: 90
Loss: 0.6705442070960999
RMSE train: 0.755940	val: 0.869102	test: 0.898185
MAE train: 0.560326	val: 0.629642	test: 0.680775

Epoch: 91
Loss: 0.6410120353102684
RMSE train: 0.639584	val: 0.746243	test: 0.845367
MAE train: 0.485681	val: 0.554079	test: 0.634188

Epoch: 92
Loss: 0.6745352894067764
RMSE train: 0.650473	val: 0.830596	test: 0.862002
MAE train: 0.489836	val: 0.590214	test: 0.636031

Epoch: 93
Loss: 0.5630796849727631
RMSE train: 0.663197	val: 0.837705	test: 0.843179
MAE train: 0.504791	val: 0.604039	test: 0.621673

Epoch: 94
Loss: 0.7482541054487228
RMSE train: 0.687976	val: 0.812491	test: 0.829379
MAE train: 0.520957	val: 0.597304	test: 0.620212

Epoch: 95
Loss: 0.6652103960514069
RMSE train: 0.672557	val: 0.787145	test: 0.828423
MAE train: 0.511348	val: 0.587889	test: 0.620561

Epoch: 96
Loss: 0.7230116873979568
RMSE train: 0.682041	val: 0.808192	test: 0.852804
MAE train: 0.525141	val: 0.596220	test: 0.637752

Epoch: 97
Loss: 0.711819976568222
RMSE train: 0.690093	val: 0.832625	test: 0.859300
MAE train: 0.527475	val: 0.602654	test: 0.637873

Epoch: 98
Loss: 0.7626462429761887
RMSE train: 0.642194	val: 0.776065	test: 0.814833
MAE train: 0.486123	val: 0.572887	test: 0.600638

Epoch: 99
Loss: 0.5725626647472382
RMSE train: 0.650864	val: 0.791011	test: 0.816738
MAE train: 0.484757	val: 0.581763	test: 0.609631

Epoch: 100
Loss: 0.6226755082607269
RMSE train: 0.666315	val: 0.795841	test: 0.833568
MAE train: 0.498798	val: 0.596122	test: 0.623746

Epoch: 101
Loss: 0.6017581522464752
RMSE train: 0.644954	val: 0.768253	test: 0.820631
MAE train: 0.487529	val: 0.571479	test: 0.613863

Epoch: 102
Loss: 0.6756676733493805
RMSE train: 0.708624	val: 0.838293	test: 0.855024
MAE train: 0.533944	val: 0.614982	test: 0.637515

Epoch: 103
Loss: 0.5509299114346504
RMSE train: 0.781103	val: 0.900754	test: 0.919811
MAE train: 0.588604	val: 0.675640	test: 0.677655

Epoch: 104
Loss: 0.5756579637527466
RMSE train: 0.649917	val: 0.773657	test: 0.817176
MAE train: 0.486865	val: 0.587832	test: 0.598002

Epoch: 105
Loss: 0.744937539100647
RMSE train: 0.629639	val: 0.736662	test: 0.821102
MAE train: 0.478392	val: 0.577356	test: 0.625743

Epoch: 106
Loss: 0.5839991569519043
RMSE train: 0.618506	val: 0.755741	test: 0.800333
MAE train: 0.464537	val: 0.573885	test: 0.603584

Epoch: 107
Loss: 0.5557335391640663
RMSE train: 0.657441	val: 0.798382	test: 0.822935
MAE train: 0.486009	val: 0.595840	test: 0.611462

Epoch: 108
Loss: 0.6159102618694305
RMSE train: 0.658147	val: 0.795988	test: 0.822916
MAE train: 0.490514	val: 0.588651	test: 0.601844

Epoch: 109
Loss: 0.8128943890333176
RMSE train: 0.645351	val: 0.801810	test: 0.826039
MAE train: 0.485014	val: 0.589410	test: 0.602797

Epoch: 110
Loss: 0.5474451556801796
RMSE train: 0.616092	val: 0.736150	test: 0.819563
MAE train: 0.467718	val: 0.565829	test: 0.607302

Epoch: 111
Loss: 0.7119103521108627
RMSE train: 0.609554	val: 0.769425	test: 0.819477
MAE train: 0.465985	val: 0.580945	test: 0.601857

Epoch: 112
Loss: 0.9259637892246246
RMSE train: 0.660360	val: 0.843712	test: 0.861522
MAE train: 0.494377	val: 0.617479	test: 0.636184

Epoch: 113
Loss: 0.59886484593153
RMSE train: 0.619670	val: 0.767367	test: 0.811724
MAE train: 0.456471	val: 0.553281	test: 0.599415

Epoch: 114
Loss: 0.5711635500192642
RMSE train: 0.701284	val: 0.814811	test: 0.844403
MAE train: 0.510138	val: 0.585483	test: 0.610561

Epoch: 115
Loss: 0.8878363221883774
RMSE train: 0.620495	val: 0.744443	test: 0.802785
MAE train: 0.454644	val: 0.544922	test: 0.579924

Epoch: 116
Loss: 0.5405577421188354
RMSE train: 0.590450	val: 0.725114	test: 0.792539
MAE train: 0.440144	val: 0.532267	test: 0.578003

Epoch: 117
Loss: 0.5356849431991577
RMSE train: 0.587830	val: 0.738349	test: 0.791290
MAE train: 0.440864	val: 0.545853	test: 0.576238

Epoch: 118
Loss: 0.7558752149343491
RMSE train: 0.639226	val: 0.799236	test: 0.807668
MAE train: 0.479947	val: 0.597107	test: 0.587813

Epoch: 119
Loss: 0.5995817631483078
RMSE train: 0.662202	val: 0.787765	test: 0.810418
MAE train: 0.497513	val: 0.597668	test: 0.590290

Epoch: 120
Loss: 0.4982522577047348
RMSE train: 0.659569	val: 0.759233	test: 0.809315
MAE train: 0.492598	val: 0.585026	test: 0.583397

Epoch: 121
Loss: 0.5843366533517838
RMSE train: 0.625924	val: 0.745293	test: 0.793668
MAE train: 0.473700	val: 0.570829	test: 0.589199

Early stopping
Best (RMSE):	 train: 0.664337	val: 0.723619	test: 0.849464
Best (MAE):	 train: 0.506493	val: 0.561401	test: 0.617253
All runs completed.

RMSE train: 0.710854	val: 0.797355	test: 0.866338
MAE train: 0.536750	val: 0.622106	test: 0.665577

Epoch: 85
Loss: 0.7678488343954086
RMSE train: 0.848937	val: 0.910714	test: 0.950687
MAE train: 0.651700	val: 0.712834	test: 0.732265

Epoch: 86
Loss: 0.7120859622955322
RMSE train: 0.732379	val: 0.784633	test: 0.860777
MAE train: 0.569001	val: 0.621370	test: 0.664558

Epoch: 87
Loss: 0.678362175822258
RMSE train: 0.714055	val: 0.791516	test: 0.857495
MAE train: 0.571026	val: 0.627366	test: 0.669959

Epoch: 88
Loss: 0.6747211366891861
RMSE train: 0.660631	val: 0.752340	test: 0.802587
MAE train: 0.526358	val: 0.584057	test: 0.622238

Epoch: 89
Loss: 0.5907039195299149
RMSE train: 0.588945	val: 0.690615	test: 0.759154
MAE train: 0.452176	val: 0.528749	test: 0.569986

Epoch: 90
Loss: 0.5346583127975464
RMSE train: 0.626549	val: 0.714305	test: 0.813707
MAE train: 0.464631	val: 0.546226	test: 0.600176

Epoch: 91
Loss: 1.2237670421600342
RMSE train: 0.698638	val: 0.789813	test: 0.853547
MAE train: 0.513931	val: 0.593035	test: 0.630797

Epoch: 92
Loss: 0.6072170585393906
RMSE train: 0.607760	val: 0.731024	test: 0.809867
MAE train: 0.467071	val: 0.554884	test: 0.598019

Epoch: 93
Loss: 0.834601029753685
RMSE train: 0.628107	val: 0.793532	test: 0.845402
MAE train: 0.499118	val: 0.598192	test: 0.628530

Epoch: 94
Loss: 0.6244519278407097
RMSE train: 0.627447	val: 0.794979	test: 0.840508
MAE train: 0.486527	val: 0.601076	test: 0.622570

Epoch: 95
Loss: 0.6801397502422333
RMSE train: 0.655310	val: 0.780290	test: 0.854161
MAE train: 0.509246	val: 0.610509	test: 0.638169

Epoch: 96
Loss: 0.5667291954159737
RMSE train: 0.651434	val: 0.767975	test: 0.845012
MAE train: 0.501755	val: 0.604275	test: 0.631103

Epoch: 97
Loss: 0.6646824032068253
RMSE train: 0.638213	val: 0.751478	test: 0.825811
MAE train: 0.488615	val: 0.585434	test: 0.610909

Epoch: 98
Loss: 0.5376065298914909
RMSE train: 0.628793	val: 0.744913	test: 0.823518
MAE train: 0.468853	val: 0.568406	test: 0.604567

Epoch: 99
Loss: 0.7271833121776581
RMSE train: 0.600683	val: 0.728577	test: 0.805147
MAE train: 0.446171	val: 0.550187	test: 0.590564

Epoch: 100
Loss: 0.7353136092424393
RMSE train: 0.614133	val: 0.751773	test: 0.788929
MAE train: 0.456323	val: 0.566751	test: 0.579211

Epoch: 101
Loss: 0.5601412504911423
RMSE train: 0.622370	val: 0.765286	test: 0.780087
MAE train: 0.458404	val: 0.574372	test: 0.563800

Epoch: 102
Loss: 0.6315289735794067
RMSE train: 0.617247	val: 0.756192	test: 0.804463
MAE train: 0.472461	val: 0.578369	test: 0.584440

Epoch: 103
Loss: 0.7049723416566849
RMSE train: 0.659301	val: 0.788288	test: 0.843123
MAE train: 0.522407	val: 0.612198	test: 0.634658

Epoch: 104
Loss: 0.6707390397787094
RMSE train: 0.661119	val: 0.795827	test: 0.848892
MAE train: 0.524949	val: 0.611850	test: 0.634328

Epoch: 105
Loss: 0.7105524837970734
RMSE train: 0.642023	val: 0.792946	test: 0.869071
MAE train: 0.517064	val: 0.613048	test: 0.655420

Epoch: 106
Loss: 0.9682292193174362
RMSE train: 0.673537	val: 0.826185	test: 0.878843
MAE train: 0.526225	val: 0.648352	test: 0.660099

Epoch: 107
Loss: 0.6188475042581558
RMSE train: 0.646922	val: 0.787737	test: 0.857174
MAE train: 0.489417	val: 0.607041	test: 0.621722

Epoch: 108
Loss: 0.6220948994159698
RMSE train: 0.728562	val: 0.834829	test: 0.900862
MAE train: 0.543668	val: 0.636270	test: 0.673670

Epoch: 109
Loss: 0.8291193544864655
RMSE train: 0.744347	val: 0.844428	test: 0.901718
MAE train: 0.549043	val: 0.637905	test: 0.660442

Epoch: 110
Loss: 0.604537159204483
RMSE train: 0.645057	val: 0.782153	test: 0.808769
MAE train: 0.480152	val: 0.579935	test: 0.589088

Epoch: 111
Loss: 0.6546693742275238
RMSE train: 0.592598	val: 0.752840	test: 0.757720
MAE train: 0.451236	val: 0.562020	test: 0.561425

Epoch: 112
Loss: 0.6866589933633804
RMSE train: 0.642899	val: 0.765961	test: 0.801921
MAE train: 0.497351	val: 0.599311	test: 0.614839

Epoch: 113
Loss: 0.49994176626205444
RMSE train: 0.662118	val: 0.768276	test: 0.822268
MAE train: 0.495427	val: 0.596179	test: 0.623465

Epoch: 114
Loss: 0.57355135679245
RMSE train: 0.620040	val: 0.738700	test: 0.795355
MAE train: 0.472350	val: 0.583857	test: 0.606948

Epoch: 115
Loss: 0.6206484287977219
RMSE train: 0.618677	val: 0.755101	test: 0.812127
MAE train: 0.469714	val: 0.593198	test: 0.618028

Epoch: 116
Loss: 0.5533839762210846
RMSE train: 0.569010	val: 0.700699	test: 0.823495
MAE train: 0.441045	val: 0.539114	test: 0.619209

Epoch: 117
Loss: 0.5742505565285683
RMSE train: 0.582157	val: 0.702750	test: 0.835493
MAE train: 0.455978	val: 0.542559	test: 0.624211

Epoch: 118
Loss: 0.5620627701282501
RMSE train: 0.619579	val: 0.754106	test: 0.853468
MAE train: 0.496792	val: 0.580883	test: 0.642007

Epoch: 119
Loss: 0.5715828835964203
RMSE train: 0.608750	val: 0.752823	test: 0.848095
MAE train: 0.488172	val: 0.580752	test: 0.641374

Epoch: 120
Loss: 0.7514826059341431
RMSE train: 0.698637	val: 0.803554	test: 0.933927
MAE train: 0.556952	val: 0.630574	test: 0.723392

Epoch: 121
Loss: 0.6249134093523026
RMSE train: 0.633169	val: 0.738425	test: 0.888638
MAE train: 0.488144	val: 0.585304	test: 0.665138

Epoch: 122
Loss: 0.711297981441021
RMSE train: 0.649990	val: 0.731505	test: 0.863107
MAE train: 0.508075	val: 0.573930	test: 0.663746

Epoch: 123
Loss: 0.5776280462741852
RMSE train: 0.665746	val: 0.751750	test: 0.832747
MAE train: 0.517020	val: 0.593674	test: 0.641560

Epoch: 124
Loss: 0.8189341723918915
RMSE train: 0.608616	val: 0.705260	test: 0.799295
MAE train: 0.480301	val: 0.564554	test: 0.606648

Early stopping
Best (RMSE):	 train: 0.588945	val: 0.690615	test: 0.759154
Best (MAE):	 train: 0.452176	val: 0.528749	test: 0.569986


Epoch: 84
Loss: 0.4215052127838135
RMSE train: 0.565633	val: 0.851892	test: 0.710106
MAE train: 0.425800	val: 0.628352	test: 0.537387

Epoch: 85
Loss: 0.44913534820079803
RMSE train: 0.611184	val: 0.877783	test: 0.729914
MAE train: 0.457298	val: 0.649820	test: 0.545490

Epoch: 86
Loss: 0.4279039427638054
RMSE train: 0.564994	val: 0.845504	test: 0.725469
MAE train: 0.427667	val: 0.635241	test: 0.551110

Epoch: 87
Loss: 0.47203340381383896
RMSE train: 0.534685	val: 0.839626	test: 0.722063
MAE train: 0.407610	val: 0.624097	test: 0.546547

Epoch: 88
Loss: 0.39571287482976913
RMSE train: 0.567739	val: 0.781318	test: 0.746369
MAE train: 0.422911	val: 0.587147	test: 0.549505

Epoch: 89
Loss: 0.4397280067205429
RMSE train: 0.567711	val: 0.761499	test: 0.760986
MAE train: 0.424992	val: 0.573940	test: 0.561144

Epoch: 90
Loss: 0.41132332384586334
RMSE train: 0.574266	val: 0.831954	test: 0.735101
MAE train: 0.438213	val: 0.618706	test: 0.563057

Epoch: 91
Loss: 0.4506474882364273
RMSE train: 0.536254	val: 0.839445	test: 0.732072
MAE train: 0.424366	val: 0.618912	test: 0.569430

Epoch: 92
Loss: 0.4379819706082344
RMSE train: 0.518312	val: 0.830095	test: 0.714555
MAE train: 0.399684	val: 0.610866	test: 0.551183

Epoch: 93
Loss: 0.45632394403219223
RMSE train: 0.599501	val: 0.835797	test: 0.752671
MAE train: 0.458704	val: 0.643787	test: 0.558407

Epoch: 94
Loss: 0.4866282716393471
RMSE train: 0.577428	val: 0.848367	test: 0.722796
MAE train: 0.440740	val: 0.643611	test: 0.542325

Epoch: 95
Loss: 0.4363156482577324
RMSE train: 0.558720	val: 0.771982	test: 0.735968
MAE train: 0.415603	val: 0.575282	test: 0.534329

Epoch: 96
Loss: 0.41063807904720306
RMSE train: 0.645703	val: 0.841749	test: 0.780804
MAE train: 0.481672	val: 0.647962	test: 0.570379

Epoch: 97
Loss: 0.43645400553941727
RMSE train: 0.573998	val: 0.876096	test: 0.723656
MAE train: 0.438516	val: 0.665987	test: 0.557042

Epoch: 98
Loss: 0.3929392099380493
RMSE train: 0.539259	val: 0.772762	test: 0.737924
MAE train: 0.407376	val: 0.580249	test: 0.535474

Epoch: 99
Loss: 0.40906064957380295
RMSE train: 0.514618	val: 0.820059	test: 0.719122
MAE train: 0.390212	val: 0.613671	test: 0.535679

Epoch: 100
Loss: 0.36832692474126816
RMSE train: 0.521202	val: 0.875113	test: 0.746145
MAE train: 0.405127	val: 0.633459	test: 0.560715

Epoch: 101
Loss: 0.39153902977705
RMSE train: 0.503388	val: 0.828950	test: 0.736965
MAE train: 0.383510	val: 0.600178	test: 0.551730

Epoch: 102
Loss: 0.3708338364958763
RMSE train: 0.508788	val: 0.801570	test: 0.752795
MAE train: 0.385131	val: 0.590675	test: 0.562319

Epoch: 103
Loss: 0.38353583216667175
RMSE train: 0.521318	val: 0.823836	test: 0.752703
MAE train: 0.401644	val: 0.612572	test: 0.566140

Epoch: 104
Loss: 0.36205562949180603
RMSE train: 0.480590	val: 0.796168	test: 0.734574
MAE train: 0.376531	val: 0.580492	test: 0.561245

Epoch: 105
Loss: 0.3555079996585846
RMSE train: 0.534829	val: 0.796086	test: 0.737242
MAE train: 0.410338	val: 0.611337	test: 0.553064

Epoch: 106
Loss: 0.36347392201423645
RMSE train: 0.570748	val: 0.849719	test: 0.721303
MAE train: 0.424848	val: 0.649302	test: 0.541712

Epoch: 107
Loss: 0.3895455226302147
RMSE train: 0.527032	val: 0.776634	test: 0.722483
MAE train: 0.393100	val: 0.594256	test: 0.537964

Epoch: 108
Loss: 0.404385469853878
RMSE train: 0.561187	val: 0.827102	test: 0.732673
MAE train: 0.426251	val: 0.636379	test: 0.549945

Epoch: 109
Loss: 0.33603986352682114
RMSE train: 0.531541	val: 0.802626	test: 0.758610
MAE train: 0.406090	val: 0.603642	test: 0.570965

Epoch: 110
Loss: 0.3929107114672661
RMSE train: 0.489232	val: 0.844426	test: 0.723287
MAE train: 0.378489	val: 0.623133	test: 0.553888

Epoch: 111
Loss: 0.32145656645298004
RMSE train: 0.522838	val: 0.801395	test: 0.732372
MAE train: 0.393903	val: 0.601581	test: 0.552674

Epoch: 112
Loss: 0.3544817641377449
RMSE train: 0.551289	val: 0.861598	test: 0.742554
MAE train: 0.414082	val: 0.648363	test: 0.565789

Epoch: 113
Loss: 0.4010217785835266
RMSE train: 0.503853	val: 0.791813	test: 0.761925
MAE train: 0.379519	val: 0.595571	test: 0.567639

Epoch: 114
Loss: 0.37617064267396927
RMSE train: 0.482689	val: 0.825734	test: 0.760671
MAE train: 0.373227	val: 0.614771	test: 0.576121

Epoch: 115
Loss: 0.39122486859560013
RMSE train: 0.502178	val: 0.847399	test: 0.741739
MAE train: 0.394552	val: 0.633127	test: 0.577584

Epoch: 116
Loss: 0.3502170816063881
RMSE train: 0.495005	val: 0.865968	test: 0.715249
MAE train: 0.381201	val: 0.642246	test: 0.547697

Epoch: 117
Loss: 0.3622444197535515
RMSE train: 0.547855	val: 0.822307	test: 0.762710
MAE train: 0.414322	val: 0.623985	test: 0.554811

Epoch: 118
Loss: 0.35336901247501373
RMSE train: 0.504389	val: 0.766053	test: 0.792429
MAE train: 0.389134	val: 0.575380	test: 0.574011

Epoch: 119
Loss: 0.32551998645067215
RMSE train: 0.483514	val: 0.802514	test: 0.754150
MAE train: 0.372242	val: 0.603542	test: 0.550297

Epoch: 120
Loss: 0.35797350108623505
RMSE train: 0.473807	val: 0.836083	test: 0.731819
MAE train: 0.365836	val: 0.627737	test: 0.554502

Epoch: 121
Loss: 0.33782029151916504
RMSE train: 0.460031	val: 0.839833	test: 0.694912
MAE train: 0.349534	val: 0.619368	test: 0.537623

Epoch: 122
Loss: 0.32155686616897583
RMSE train: 0.513886	val: 0.804261	test: 0.739501
MAE train: 0.379860	val: 0.607248	test: 0.550401

Epoch: 123
Loss: 0.34141524881124496
RMSE train: 0.508886	val: 0.782984	test: 0.737970
MAE train: 0.383862	val: 0.597906	test: 0.552152

Epoch: 124
Loss: 0.3561861589550972
RMSE train: 0.491548	val: 0.768351	test: 0.734638
MAE train: 0.372633	val: 0.583789	test: 0.538863

Early stopping
Best (RMSE):	 train: 0.567711	val: 0.761499	test: 0.760986
Best (MAE):	 train: 0.424992	val: 0.573940	test: 0.561144

RMSE train: 0.699501	val: 0.814449	test: 0.785795
MAE train: 0.521355	val: 0.610266	test: 0.571178

Epoch: 85
Loss: 0.7934641540050507
RMSE train: 0.691175	val: 0.789286	test: 0.763763
MAE train: 0.521322	val: 0.601442	test: 0.570458

Epoch: 86
Loss: 0.6499045342206955
RMSE train: 0.786808	val: 0.921761	test: 0.905583
MAE train: 0.589648	val: 0.673541	test: 0.677454

Epoch: 87
Loss: 0.634632334113121
RMSE train: 0.794080	val: 0.924138	test: 0.925194
MAE train: 0.592973	val: 0.670544	test: 0.680208

Epoch: 88
Loss: 0.7302549928426743
RMSE train: 0.679985	val: 0.796901	test: 0.833742
MAE train: 0.517036	val: 0.598888	test: 0.610106

Epoch: 89
Loss: 0.8347046673297882
RMSE train: 0.706478	val: 0.824283	test: 0.861609
MAE train: 0.549627	val: 0.635866	test: 0.651368

Epoch: 90
Loss: 0.6569908559322357
RMSE train: 0.691506	val: 0.813208	test: 0.856994
MAE train: 0.542486	val: 0.631245	test: 0.649598

Epoch: 91
Loss: 0.7835632711648941
RMSE train: 0.669069	val: 0.808257	test: 0.830114
MAE train: 0.520586	val: 0.611900	test: 0.626042

Epoch: 92
Loss: 0.8019960969686508
RMSE train: 0.615027	val: 0.769693	test: 0.764376
MAE train: 0.471662	val: 0.570096	test: 0.578500

Epoch: 93
Loss: 0.5795777961611748
RMSE train: 0.669994	val: 0.803466	test: 0.797080
MAE train: 0.514923	val: 0.609364	test: 0.611356

Epoch: 94
Loss: 0.8153979331254959
RMSE train: 0.663511	val: 0.757065	test: 0.793828
MAE train: 0.506501	val: 0.581172	test: 0.600816

Epoch: 95
Loss: 0.5893822386860847
RMSE train: 0.696598	val: 0.752747	test: 0.833333
MAE train: 0.537695	val: 0.595203	test: 0.633557

Epoch: 96
Loss: 0.8571151047945023
RMSE train: 0.665302	val: 0.728662	test: 0.831201
MAE train: 0.516164	val: 0.556674	test: 0.629075

Epoch: 97
Loss: 0.6982138305902481
RMSE train: 0.651008	val: 0.730552	test: 0.804011
MAE train: 0.502393	val: 0.564088	test: 0.599637

Epoch: 98
Loss: 0.6060789078474045
RMSE train: 0.730407	val: 0.824449	test: 0.840315
MAE train: 0.556415	val: 0.646580	test: 0.616536

Epoch: 99
Loss: 0.607799306511879
RMSE train: 0.763299	val: 0.866181	test: 0.849216
MAE train: 0.567594	val: 0.665455	test: 0.607504

Epoch: 100
Loss: 0.8665109127759933
RMSE train: 0.715623	val: 0.804907	test: 0.803413
MAE train: 0.529849	val: 0.620446	test: 0.574790

Epoch: 101
Loss: 0.6275774836540222
RMSE train: 0.706825	val: 0.807014	test: 0.796377
MAE train: 0.524367	val: 0.596309	test: 0.585993

Epoch: 102
Loss: 0.6879159659147263
RMSE train: 0.709214	val: 0.809960	test: 0.819538
MAE train: 0.531372	val: 0.607985	test: 0.613111

Epoch: 103
Loss: 0.6170517057180405
RMSE train: 0.693733	val: 0.794217	test: 0.820655
MAE train: 0.523141	val: 0.610360	test: 0.608295

Epoch: 104
Loss: 0.7418346703052521
RMSE train: 0.675963	val: 0.772001	test: 0.810498
MAE train: 0.512025	val: 0.599855	test: 0.593416

Epoch: 105
Loss: 0.6465113610029221
RMSE train: 0.618976	val: 0.704354	test: 0.794586
MAE train: 0.465867	val: 0.551776	test: 0.589043

Epoch: 106
Loss: 0.6448208540678024
RMSE train: 0.603168	val: 0.715341	test: 0.834556
MAE train: 0.470187	val: 0.536885	test: 0.632625

Epoch: 107
Loss: 0.6318142861127853
RMSE train: 0.621369	val: 0.754500	test: 0.876899
MAE train: 0.494512	val: 0.553866	test: 0.660233

Epoch: 108
Loss: 0.6650405377149582
RMSE train: 0.634028	val: 0.795118	test: 0.892387
MAE train: 0.500061	val: 0.580478	test: 0.666035

Epoch: 109
Loss: 0.6296885460615158
RMSE train: 0.640484	val: 0.801165	test: 0.870370
MAE train: 0.502405	val: 0.592992	test: 0.663078

Epoch: 110
Loss: 0.8647668808698654
RMSE train: 0.626296	val: 0.746352	test: 0.794812
MAE train: 0.486434	val: 0.582685	test: 0.601936

Epoch: 111
Loss: 0.7619389444589615
RMSE train: 0.616158	val: 0.744553	test: 0.763022
MAE train: 0.478294	val: 0.567392	test: 0.567175

Epoch: 112
Loss: 0.566349670290947
RMSE train: 0.659554	val: 0.856694	test: 0.822432
MAE train: 0.507795	val: 0.618210	test: 0.592842

Epoch: 113
Loss: 0.7912565469741821
RMSE train: 0.580306	val: 0.769040	test: 0.763745
MAE train: 0.447512	val: 0.569815	test: 0.561720

Epoch: 114
Loss: 0.5525046586990356
RMSE train: 0.651462	val: 0.761295	test: 0.782810
MAE train: 0.495251	val: 0.580201	test: 0.579992

Epoch: 115
Loss: 0.6783540695905685
RMSE train: 0.727698	val: 0.848539	test: 0.855058
MAE train: 0.538013	val: 0.630386	test: 0.619734

Epoch: 116
Loss: 0.7137941122055054
RMSE train: 0.730188	val: 0.853923	test: 0.860335
MAE train: 0.543774	val: 0.636302	test: 0.631945

Epoch: 117
Loss: 0.5233075246214867
RMSE train: 0.605290	val: 0.745934	test: 0.759097
MAE train: 0.461763	val: 0.560607	test: 0.559990

Epoch: 118
Loss: 0.8421377241611481
RMSE train: 0.577734	val: 0.726173	test: 0.748160
MAE train: 0.441544	val: 0.548811	test: 0.562513

Epoch: 119
Loss: 0.7418075650930405
RMSE train: 0.679945	val: 0.811893	test: 0.797578
MAE train: 0.511911	val: 0.607116	test: 0.587795

Epoch: 120
Loss: 0.7506310939788818
RMSE train: 0.750697	val: 0.841849	test: 0.846980
MAE train: 0.561745	val: 0.629088	test: 0.622700

Epoch: 121
Loss: 0.6245189607143402
RMSE train: 0.650620	val: 0.743152	test: 0.780118
MAE train: 0.496727	val: 0.561698	test: 0.581134

Epoch: 122
Loss: 0.5833699256181717
RMSE train: 0.610186	val: 0.728122	test: 0.773798
MAE train: 0.473042	val: 0.568311	test: 0.584370

Epoch: 123
Loss: 0.6622357368469238
RMSE train: 0.620354	val: 0.773326	test: 0.767619
MAE train: 0.471178	val: 0.576452	test: 0.567590

Epoch: 124
Loss: 0.8526143133640289
RMSE train: 0.648628	val: 0.801051	test: 0.785658
MAE train: 0.491758	val: 0.596141	test: 0.579836

Epoch: 125
Loss: 0.6413681656122208
RMSE train: 0.640315	val: 0.747325	test: 0.775349
MAE train: 0.493840	val: 0.575158	test: 0.567661

Epoch: 126
Loss: 0.5426908284425735
RMSE train: 0.670614	val: 0.773367	test: 0.792401
MAE train: 0.516293	val: 0.592661	test: 0.579380

Epoch: 127
Loss: 0.5382281988859177
RMSE train: 0.621422	val: 0.742394	test: 0.772852
MAE train: 0.475375	val: 0.571030	test: 0.566696

Epoch: 128
Loss: 0.5388812124729156
RMSE train: 0.596660	val: 0.718794	test: 0.774943
MAE train: 0.457780	val: 0.550551	test: 0.571079

Epoch: 129
Loss: 0.6356080621480942
RMSE train: 0.576272	val: 0.697808	test: 0.763430
MAE train: 0.438387	val: 0.534927	test: 0.556988

Epoch: 130
Loss: 0.5073391273617744
RMSE train: 0.633230	val: 0.733379	test: 0.783441
MAE train: 0.476420	val: 0.555026	test: 0.570884

Epoch: 131
Loss: 0.5964538753032684
RMSE train: 0.728476	val: 0.824614	test: 0.834079
MAE train: 0.541696	val: 0.628976	test: 0.596022

Epoch: 132
Loss: 0.5962765067815781
RMSE train: 0.663229	val: 0.788863	test: 0.788592
MAE train: 0.494215	val: 0.582318	test: 0.566457

Epoch: 133
Loss: 0.48125845193862915
RMSE train: 0.639739	val: 0.795186	test: 0.778128
MAE train: 0.476452	val: 0.568281	test: 0.549313

Epoch: 134
Loss: 0.45226994529366493
RMSE train: 0.652229	val: 0.787294	test: 0.784545
MAE train: 0.484189	val: 0.580536	test: 0.569664

Epoch: 135
Loss: 0.48481982946395874
RMSE train: 0.674113	val: 0.776500	test: 0.804590
MAE train: 0.503010	val: 0.586427	test: 0.596460

Epoch: 136
Loss: 0.5186708122491837
RMSE train: 0.642360	val: 0.734869	test: 0.803930
MAE train: 0.491195	val: 0.567182	test: 0.595763

Epoch: 137
Loss: 0.5764898210763931
RMSE train: 0.601467	val: 0.706452	test: 0.797819
MAE train: 0.467292	val: 0.564140	test: 0.601071

Epoch: 138
Loss: 0.6044798716902733
RMSE train: 0.577745	val: 0.710192	test: 0.776798
MAE train: 0.450249	val: 0.562925	test: 0.592206

Epoch: 139
Loss: 0.5931500047445297
RMSE train: 0.585380	val: 0.713103	test: 0.762448
MAE train: 0.449384	val: 0.553502	test: 0.575166

Epoch: 140
Loss: 0.5454044714570045
RMSE train: 0.684924	val: 0.822487	test: 0.814786
MAE train: 0.525434	val: 0.602577	test: 0.595697

Epoch: 141
Loss: 0.5966397449374199
RMSE train: 0.607490	val: 0.772345	test: 0.753294
MAE train: 0.461221	val: 0.564944	test: 0.547181

Epoch: 142
Loss: 0.5745633095502853
RMSE train: 0.608193	val: 0.756050	test: 0.757906
MAE train: 0.458181	val: 0.577319	test: 0.548126

Epoch: 143
Loss: 0.5639592409133911
RMSE train: 0.685841	val: 0.834524	test: 0.850707
MAE train: 0.510414	val: 0.628749	test: 0.621074

Epoch: 144
Loss: 0.6377339512109756
RMSE train: 0.651606	val: 0.803694	test: 0.821950

Epoch: 84
Loss: 0.45900609344244003
RMSE train: 0.490781	val: 0.852310	test: 0.736615
MAE train: 0.380091	val: 0.612823	test: 0.543745

Epoch: 85
Loss: 0.4482565373182297
RMSE train: 0.549117	val: 0.836486	test: 0.760570
MAE train: 0.413900	val: 0.627426	test: 0.550301

Epoch: 86
Loss: 0.47160014510154724
RMSE train: 0.593142	val: 0.935710	test: 0.751333
MAE train: 0.457599	val: 0.720548	test: 0.551813

Epoch: 87
Loss: 0.4725729301571846
RMSE train: 0.527807	val: 0.841131	test: 0.741946
MAE train: 0.401991	val: 0.645276	test: 0.541341

Epoch: 88
Loss: 0.4051526114344597
RMSE train: 0.639448	val: 0.885274	test: 0.798706
MAE train: 0.483768	val: 0.691715	test: 0.590005

Epoch: 89
Loss: 0.43247321248054504
RMSE train: 0.625857	val: 0.980414	test: 0.769955
MAE train: 0.486488	val: 0.721756	test: 0.594272

Epoch: 90
Loss: 0.4617050811648369
RMSE train: 0.542645	val: 0.884916	test: 0.745972
MAE train: 0.422101	val: 0.652605	test: 0.558792

Epoch: 91
Loss: 0.40215037018060684
RMSE train: 0.583194	val: 0.964142	test: 0.758626
MAE train: 0.461116	val: 0.736124	test: 0.571894

Epoch: 92
Loss: 0.4556625485420227
RMSE train: 0.530076	val: 0.920679	test: 0.752170
MAE train: 0.418224	val: 0.692067	test: 0.573538

Epoch: 93
Loss: 0.42180369794368744
RMSE train: 0.537714	val: 0.877973	test: 0.769787
MAE train: 0.426424	val: 0.664694	test: 0.583261

Epoch: 94
Loss: 0.4068823903799057
RMSE train: 0.573495	val: 0.940337	test: 0.787925
MAE train: 0.453942	val: 0.698592	test: 0.594776

Epoch: 95
Loss: 0.4090913161635399
RMSE train: 0.546299	val: 0.904719	test: 0.764364
MAE train: 0.428701	val: 0.681879	test: 0.577024

Epoch: 96
Loss: 0.42167751491069794
RMSE train: 0.510325	val: 0.869530	test: 0.757388
MAE train: 0.398836	val: 0.650810	test: 0.569142

Epoch: 97
Loss: 0.3775600343942642
RMSE train: 0.498753	val: 0.854655	test: 0.747752
MAE train: 0.386026	val: 0.627107	test: 0.553960

Epoch: 98
Loss: 0.405039481818676
RMSE train: 0.588179	val: 0.917538	test: 0.765230
MAE train: 0.453681	val: 0.696774	test: 0.566803

Epoch: 99
Loss: 0.3997992053627968
RMSE train: 0.531671	val: 0.812400	test: 0.765769
MAE train: 0.411487	val: 0.616962	test: 0.571881

Epoch: 100
Loss: 0.4019147455692291
RMSE train: 0.539960	val: 0.830717	test: 0.764493
MAE train: 0.415070	val: 0.632554	test: 0.568789

Epoch: 101
Loss: 0.39830368012189865
RMSE train: 0.626427	val: 0.968734	test: 0.775644
MAE train: 0.479163	val: 0.732400	test: 0.581453

Epoch: 102
Loss: 0.4007003530859947
RMSE train: 0.526891	val: 0.886243	test: 0.729244
MAE train: 0.403293	val: 0.665310	test: 0.548481

Epoch: 103
Loss: 0.3951195701956749
RMSE train: 0.544141	val: 0.902899	test: 0.729806
MAE train: 0.424340	val: 0.677669	test: 0.547901

Epoch: 104
Loss: 0.4207255318760872
RMSE train: 0.587188	val: 0.907731	test: 0.755519
MAE train: 0.456823	val: 0.681870	test: 0.558755

Epoch: 105
Loss: 0.4487094059586525
RMSE train: 0.533855	val: 0.856402	test: 0.728966
MAE train: 0.411091	val: 0.644894	test: 0.532967

Epoch: 106
Loss: 0.37745655328035355
RMSE train: 0.522599	val: 0.870246	test: 0.731263
MAE train: 0.402328	val: 0.652963	test: 0.548296

Epoch: 107
Loss: 0.45386940240859985
RMSE train: 0.573672	val: 0.971866	test: 0.743628
MAE train: 0.441258	val: 0.716414	test: 0.570269

Epoch: 108
Loss: 0.3786232993006706
RMSE train: 0.491052	val: 0.841353	test: 0.722251
MAE train: 0.378790	val: 0.617250	test: 0.543070

Epoch: 109
Loss: 0.3716423958539963
RMSE train: 0.514672	val: 0.878014	test: 0.748599
MAE train: 0.397692	val: 0.651640	test: 0.546164

Epoch: 110
Loss: 0.38784781098365784
RMSE train: 0.527036	val: 0.862014	test: 0.764057
MAE train: 0.408595	val: 0.655448	test: 0.559940

Epoch: 111
Loss: 0.39271219074726105
RMSE train: 0.473240	val: 0.822981	test: 0.749156
MAE train: 0.367563	val: 0.618093	test: 0.547243

Epoch: 112
Loss: 0.38698970526456833
RMSE train: 0.513610	val: 0.929120	test: 0.772735
MAE train: 0.401131	val: 0.690626	test: 0.555079

Epoch: 113
Loss: 0.38880014419555664
RMSE train: 0.501499	val: 0.834433	test: 0.777222
MAE train: 0.392493	val: 0.627105	test: 0.575524

Epoch: 114
Loss: 0.39269937574863434
RMSE train: 0.534197	val: 0.878125	test: 0.806909
MAE train: 0.424260	val: 0.654837	test: 0.601146

Epoch: 115
Loss: 0.3565201535820961
RMSE train: 0.547981	val: 0.914824	test: 0.793825
MAE train: 0.423823	val: 0.688716	test: 0.583741

Epoch: 116
Loss: 0.39971407502889633
RMSE train: 0.545667	val: 0.808919	test: 0.768273
MAE train: 0.415690	val: 0.626559	test: 0.568897

Epoch: 117
Loss: 0.3956899642944336
RMSE train: 0.535337	val: 0.806304	test: 0.772087
MAE train: 0.408899	val: 0.628000	test: 0.568394

Epoch: 118
Loss: 0.3622026666998863
RMSE train: 0.558307	val: 0.908573	test: 0.794760
MAE train: 0.428247	val: 0.694562	test: 0.565459

Epoch: 119
Loss: 0.3550472781062126
RMSE train: 0.526445	val: 0.823944	test: 0.773713
MAE train: 0.398806	val: 0.633374	test: 0.534093

Epoch: 120
Loss: 0.32271233946084976
RMSE train: 0.538680	val: 0.908684	test: 0.760570
MAE train: 0.418026	val: 0.687761	test: 0.551646

Epoch: 121
Loss: 0.36464013159275055
RMSE train: 0.512000	val: 0.905739	test: 0.752446
MAE train: 0.395760	val: 0.669915	test: 0.560983

Epoch: 122
Loss: 0.38009896874427795
RMSE train: 0.480756	val: 0.884080	test: 0.749215
MAE train: 0.375400	val: 0.633026	test: 0.559490

Epoch: 123
Loss: 0.3952212706208229
RMSE train: 0.506752	val: 0.934445	test: 0.782391
MAE train: 0.405346	val: 0.657509	test: 0.584915

Epoch: 124
Loss: 0.4121410623192787
RMSE train: 0.493016	val: 0.898150	test: 0.791448
MAE train: 0.387871	val: 0.640112	test: 0.573432

Epoch: 125
Loss: 0.3495852202177048
RMSE train: 0.501898	val: 0.819687	test: 0.798577
MAE train: 0.391858	val: 0.617474	test: 0.576007

Epoch: 126
Loss: 0.3220234140753746
RMSE train: 0.491994	val: 0.887554	test: 0.776246
MAE train: 0.391775	val: 0.649716	test: 0.582172

Epoch: 127
Loss: 0.36794087290763855
RMSE train: 0.522745	val: 0.932764	test: 0.776234
MAE train: 0.418928	val: 0.694844	test: 0.595034

Epoch: 128
Loss: 0.3814532533288002
RMSE train: 0.467294	val: 0.863829	test: 0.738742
MAE train: 0.370850	val: 0.634226	test: 0.565984

Epoch: 129
Loss: 0.34755007177591324
RMSE train: 0.515143	val: 0.906752	test: 0.775532
MAE train: 0.406443	val: 0.668450	test: 0.594484

Epoch: 130
Loss: 0.3341919332742691
RMSE train: 0.493669	val: 0.867006	test: 0.793902
MAE train: 0.388455	val: 0.643263	test: 0.591909

Epoch: 131
Loss: 0.4181584045290947
RMSE train: 0.460287	val: 0.841901	test: 0.768353
MAE train: 0.366848	val: 0.625731	test: 0.570869

Epoch: 132
Loss: 0.34487275779247284
RMSE train: 0.557262	val: 0.981028	test: 0.753346
MAE train: 0.436748	val: 0.720308	test: 0.566605

Epoch: 133
Loss: 0.34192565828561783
RMSE train: 0.485515	val: 0.854377	test: 0.741513
MAE train: 0.385141	val: 0.637569	test: 0.554091

Epoch: 134
Loss: 0.3279600068926811
RMSE train: 0.444147	val: 0.804599	test: 0.738863
MAE train: 0.346355	val: 0.596710	test: 0.543001

Epoch: 135
Loss: 0.349941685795784
RMSE train: 0.499988	val: 0.850376	test: 0.755680
MAE train: 0.388256	val: 0.640421	test: 0.562901

Epoch: 136
Loss: 0.3531173840165138
RMSE train: 0.509443	val: 0.866608	test: 0.746170
MAE train: 0.398696	val: 0.648039	test: 0.567779

Epoch: 137
Loss: 0.34042755514383316
RMSE train: 0.469911	val: 0.795231	test: 0.742816
MAE train: 0.368220	val: 0.608263	test: 0.561337

Epoch: 138
Loss: 0.36936158686876297
RMSE train: 0.474341	val: 0.803165	test: 0.761872
MAE train: 0.369465	val: 0.613178	test: 0.563017

Epoch: 139
Loss: 0.37871383130550385
RMSE train: 0.516015	val: 0.860716	test: 0.760242
MAE train: 0.405859	val: 0.656839	test: 0.569096

Epoch: 140
Loss: 0.3317897394299507
RMSE train: 0.478211	val: 0.797817	test: 0.729024
MAE train: 0.363322	val: 0.599142	test: 0.533137

Epoch: 141
Loss: 0.3409530520439148
RMSE train: 0.538964	val: 0.870223	test: 0.740495
MAE train: 0.408611	val: 0.663712	test: 0.530392

Epoch: 142
Loss: 0.3516779765486717
RMSE train: 0.509027	val: 0.864473	test: 0.737229
MAE train: 0.394500	val: 0.653748	test: 0.554803

Epoch: 143
Loss: 0.31458840519189835
RMSE train: 0.477332	val: 0.823845	test: 0.731109
MAE train: 0.369327	val: 0.618456	test: 0.552059

Epoch: 144
Loss: 0.3342571333050728

Epoch: 84
Loss: 0.43381378799676895
RMSE train: 0.560423	val: 0.767500	test: 0.758410
MAE train: 0.421959	val: 0.578982	test: 0.549576

Epoch: 85
Loss: 0.461949422955513
RMSE train: 0.577231	val: 0.805051	test: 0.782986
MAE train: 0.436372	val: 0.621106	test: 0.568310

Epoch: 86
Loss: 0.4017045348882675
RMSE train: 0.494471	val: 0.824270	test: 0.725392
MAE train: 0.374343	val: 0.592839	test: 0.543175

Epoch: 87
Loss: 0.41969021409749985
RMSE train: 0.495272	val: 0.775608	test: 0.742013
MAE train: 0.374742	val: 0.575973	test: 0.544188

Epoch: 88
Loss: 0.4245842769742012
RMSE train: 0.532685	val: 0.814407	test: 0.736044
MAE train: 0.398811	val: 0.618139	test: 0.536747

Epoch: 89
Loss: 0.41832301765680313
RMSE train: 0.541498	val: 0.800376	test: 0.748890
MAE train: 0.405539	val: 0.614300	test: 0.544477

Epoch: 90
Loss: 0.41354136914014816
RMSE train: 0.517175	val: 0.772999	test: 0.750067
MAE train: 0.387256	val: 0.585525	test: 0.549035

Epoch: 91
Loss: 0.39633920043706894
RMSE train: 0.576455	val: 0.789474	test: 0.783237
MAE train: 0.431373	val: 0.612366	test: 0.562970

Epoch: 92
Loss: 0.4097346216440201
RMSE train: 0.595497	val: 0.854150	test: 0.764372
MAE train: 0.437218	val: 0.651515	test: 0.552438

Epoch: 93
Loss: 0.3875293508172035
RMSE train: 0.563154	val: 0.817780	test: 0.750933
MAE train: 0.412274	val: 0.620897	test: 0.527228

Epoch: 94
Loss: 0.42639148235321045
RMSE train: 0.564905	val: 0.781922	test: 0.776343
MAE train: 0.420450	val: 0.610083	test: 0.554146

Epoch: 95
Loss: 0.3820563405752182
RMSE train: 0.587909	val: 0.865866	test: 0.755798
MAE train: 0.437197	val: 0.666975	test: 0.559370

Epoch: 96
Loss: 0.4280777722597122
RMSE train: 0.603141	val: 0.850334	test: 0.742592
MAE train: 0.442613	val: 0.664740	test: 0.518219

Epoch: 97
Loss: 0.406337171792984
RMSE train: 0.539729	val: 0.786324	test: 0.730750
MAE train: 0.404113	val: 0.610266	test: 0.527810

Epoch: 98
Loss: 0.42594632506370544
RMSE train: 0.529137	val: 0.822831	test: 0.738706
MAE train: 0.401424	val: 0.617843	test: 0.544442

Epoch: 99
Loss: 0.42371775209903717
RMSE train: 0.587787	val: 0.780073	test: 0.772360
MAE train: 0.432678	val: 0.597403	test: 0.540946

Epoch: 100
Loss: 0.42401905357837677
RMSE train: 0.535455	val: 0.841310	test: 0.727560
MAE train: 0.397330	val: 0.621852	test: 0.522821

Epoch: 101
Loss: 0.3622705489397049
RMSE train: 0.538908	val: 0.785639	test: 0.748596
MAE train: 0.402097	val: 0.611431	test: 0.537483

Epoch: 102
Loss: 0.3821336030960083
RMSE train: 0.589231	val: 0.835249	test: 0.775077
MAE train: 0.437825	val: 0.654786	test: 0.559623

Epoch: 103
Loss: 0.3121982365846634
RMSE train: 0.535126	val: 0.814096	test: 0.730827
MAE train: 0.398022	val: 0.622316	test: 0.532471

Epoch: 104
Loss: 0.43354640156030655
RMSE train: 0.530357	val: 0.841377	test: 0.729797
MAE train: 0.395384	val: 0.623415	test: 0.538885

Epoch: 105
Loss: 0.4046976566314697
RMSE train: 0.608069	val: 0.857234	test: 0.774557
MAE train: 0.456904	val: 0.671220	test: 0.569799

Epoch: 106
Loss: 0.3557674288749695
RMSE train: 0.619410	val: 0.822824	test: 0.798998
MAE train: 0.463498	val: 0.630031	test: 0.579345

Epoch: 107
Loss: 0.3845948576927185
RMSE train: 0.603252	val: 0.924180	test: 0.779069
MAE train: 0.448651	val: 0.699260	test: 0.579090

Epoch: 108
Loss: 0.429205022752285
RMSE train: 0.582537	val: 0.841988	test: 0.777271
MAE train: 0.434282	val: 0.651197	test: 0.553331

Epoch: 109
Loss: 0.3810728192329407
RMSE train: 0.535879	val: 0.754778	test: 0.779435
MAE train: 0.395286	val: 0.569585	test: 0.548794

Epoch: 110
Loss: 0.3919447883963585
RMSE train: 0.494598	val: 0.823571	test: 0.749499
MAE train: 0.374991	val: 0.607272	test: 0.548662

Epoch: 111
Loss: 0.3746347725391388
RMSE train: 0.527586	val: 0.845226	test: 0.765305
MAE train: 0.397313	val: 0.635834	test: 0.557542

Epoch: 112
Loss: 0.4138585552573204
RMSE train: 0.499829	val: 0.775245	test: 0.750889
MAE train: 0.373129	val: 0.583522	test: 0.544828

Epoch: 113
Loss: 0.3552582412958145
RMSE train: 0.487898	val: 0.781452	test: 0.737309
MAE train: 0.367095	val: 0.583982	test: 0.537462

Epoch: 114
Loss: 0.37937770783901215
RMSE train: 0.553359	val: 0.885629	test: 0.784080
MAE train: 0.426942	val: 0.660047	test: 0.565250

Epoch: 115
Loss: 0.3455791771411896
RMSE train: 0.489009	val: 0.759703	test: 0.760841
MAE train: 0.377618	val: 0.573111	test: 0.571638

Epoch: 116
Loss: 0.41032256931066513
RMSE train: 0.513857	val: 0.800775	test: 0.721353
MAE train: 0.386578	val: 0.599022	test: 0.528900

Epoch: 117
Loss: 0.36497899144887924
RMSE train: 0.514281	val: 0.803275	test: 0.722792
MAE train: 0.384800	val: 0.601542	test: 0.523328

Epoch: 118
Loss: 0.37417321652173996
RMSE train: 0.507350	val: 0.744224	test: 0.755580
MAE train: 0.379801	val: 0.564894	test: 0.545573

Epoch: 119
Loss: 0.3852013647556305
RMSE train: 0.506104	val: 0.791793	test: 0.755246
MAE train: 0.380856	val: 0.597996	test: 0.549796

Epoch: 120
Loss: 0.3699961379170418
RMSE train: 0.567983	val: 0.803346	test: 0.759618
MAE train: 0.428783	val: 0.632470	test: 0.550205

Epoch: 121
Loss: 0.36173033714294434
RMSE train: 0.523721	val: 0.795229	test: 0.717345
MAE train: 0.390064	val: 0.615818	test: 0.525545

Epoch: 122
Loss: 0.3668270781636238
RMSE train: 0.550039	val: 0.811919	test: 0.737582
MAE train: 0.406267	val: 0.635778	test: 0.533871

Epoch: 123
Loss: 0.36215903609991074
RMSE train: 0.548380	val: 0.792584	test: 0.749590
MAE train: 0.407442	val: 0.617918	test: 0.531245

Epoch: 124
Loss: 0.34055686742067337
RMSE train: 0.484352	val: 0.816406	test: 0.722772
MAE train: 0.367465	val: 0.603491	test: 0.525908

Epoch: 125
Loss: 0.33003000915050507
RMSE train: 0.446057	val: 0.806294	test: 0.717221
MAE train: 0.342423	val: 0.592157	test: 0.531187

Epoch: 126
Loss: 0.3327958285808563
RMSE train: 0.522625	val: 0.789522	test: 0.755694
MAE train: 0.393590	val: 0.611145	test: 0.547544

Epoch: 127
Loss: 0.36230725049972534
RMSE train: 0.523360	val: 0.771420	test: 0.769844
MAE train: 0.392668	val: 0.600978	test: 0.558968

Epoch: 128
Loss: 0.32672953605651855
RMSE train: 0.461889	val: 0.805774	test: 0.731870
MAE train: 0.348166	val: 0.593907	test: 0.546405

Epoch: 129
Loss: 0.34729088097810745
RMSE train: 0.470025	val: 0.770470	test: 0.769813
MAE train: 0.347992	val: 0.573872	test: 0.559857

Epoch: 130
Loss: 0.3240693360567093
RMSE train: 0.490268	val: 0.827143	test: 0.784728
MAE train: 0.367072	val: 0.627935	test: 0.571927

Epoch: 131
Loss: 0.3738449737429619
RMSE train: 0.455600	val: 0.846556	test: 0.759159
MAE train: 0.339365	val: 0.625033	test: 0.555980

Epoch: 132
Loss: 0.33718281984329224
RMSE train: 0.470961	val: 0.806139	test: 0.785220
MAE train: 0.356975	val: 0.614410	test: 0.588064

Epoch: 133
Loss: 0.3462292030453682
RMSE train: 0.492104	val: 0.793289	test: 0.772862
MAE train: 0.370855	val: 0.606017	test: 0.565425

Epoch: 134
Loss: 0.442926324903965
RMSE train: 0.470294	val: 0.759332	test: 0.750302
MAE train: 0.349886	val: 0.574402	test: 0.554885

Epoch: 135
Loss: 0.36446696519851685
RMSE train: 0.496296	val: 0.738950	test: 0.761449
MAE train: 0.367910	val: 0.564808	test: 0.561146

Epoch: 136
Loss: 0.29597432166337967
RMSE train: 0.499175	val: 0.836578	test: 0.729522
MAE train: 0.370309	val: 0.626828	test: 0.536894

Epoch: 137
Loss: 0.35690271854400635
RMSE train: 0.536871	val: 0.809587	test: 0.756509
MAE train: 0.395683	val: 0.637069	test: 0.548209

Epoch: 138
Loss: 0.37845566123723984
RMSE train: 0.553172	val: 0.799222	test: 0.764235
MAE train: 0.410855	val: 0.628325	test: 0.546353

Epoch: 139
Loss: 0.3242352455854416
RMSE train: 0.508024	val: 0.842439	test: 0.729443
MAE train: 0.372458	val: 0.639147	test: 0.532009

Epoch: 140
Loss: 0.33120907843112946
RMSE train: 0.494218	val: 0.819030	test: 0.739990
MAE train: 0.365609	val: 0.618869	test: 0.537785

Epoch: 141
Loss: 0.32237957417964935
RMSE train: 0.483765	val: 0.831953	test: 0.746673
MAE train: 0.363001	val: 0.615942	test: 0.547513

Epoch: 142
Loss: 0.3379412218928337
RMSE train: 0.465523	val: 0.798684	test: 0.730167
MAE train: 0.345647	val: 0.602930	test: 0.529145

Epoch: 143
Loss: 0.3582318499684334
RMSE train: 0.470137	val: 0.761926	test: 0.751305
MAE train: 0.351747	val: 0.587667	test: 0.544731

Epoch: 144
Loss: 0.31638602912425995
MAE train: 0.484600	val: 0.595444	test: 0.603174

Epoch: 145
Loss: 0.568486861884594
RMSE train: 0.585412	val: 0.724132	test: 0.774079
MAE train: 0.443879	val: 0.544640	test: 0.576947

Epoch: 146
Loss: 0.5466193184256554
RMSE train: 0.660444	val: 0.771680	test: 0.847128
MAE train: 0.518855	val: 0.592865	test: 0.660769

Epoch: 147
Loss: 0.6213455498218536
RMSE train: 0.609707	val: 0.753604	test: 0.783508
MAE train: 0.476414	val: 0.575921	test: 0.589294

Epoch: 148
Loss: 0.5086852833628654
RMSE train: 0.605079	val: 0.784455	test: 0.780047
MAE train: 0.476283	val: 0.595281	test: 0.577468

Epoch: 149
Loss: 0.9237476512789726
RMSE train: 0.652136	val: 0.811042	test: 0.798460
MAE train: 0.495961	val: 0.628582	test: 0.599922

Epoch: 150
Loss: 0.5665287151932716
RMSE train: 0.626153	val: 0.755562	test: 0.796120
MAE train: 0.480671	val: 0.579409	test: 0.592285

Epoch: 151
Loss: 0.5544933378696442
RMSE train: 0.617907	val: 0.758408	test: 0.804319
MAE train: 0.489769	val: 0.597662	test: 0.608743

Epoch: 152
Loss: 0.6670156568288803
RMSE train: 0.567860	val: 0.719477	test: 0.754271
MAE train: 0.448857	val: 0.560236	test: 0.577396

Epoch: 153
Loss: 0.5775759220123291
RMSE train: 0.608237	val: 0.762773	test: 0.756003
MAE train: 0.462952	val: 0.563626	test: 0.566411

Epoch: 154
Loss: 0.4711442142724991
RMSE train: 0.643636	val: 0.786185	test: 0.784079
MAE train: 0.487038	val: 0.585235	test: 0.576899

Epoch: 155
Loss: 0.5032006204128265
RMSE train: 0.625019	val: 0.759285	test: 0.775213
MAE train: 0.477871	val: 0.582957	test: 0.570966

Epoch: 156
Loss: 0.5615067481994629
RMSE train: 0.634959	val: 0.770601	test: 0.782192
MAE train: 0.485387	val: 0.592523	test: 0.586296

Epoch: 157
Loss: 0.5659698024392128
RMSE train: 0.576788	val: 0.730566	test: 0.760916
MAE train: 0.448934	val: 0.558000	test: 0.570523

Epoch: 158
Loss: 0.6255214959383011
RMSE train: 0.598321	val: 0.788243	test: 0.756987
MAE train: 0.456159	val: 0.583579	test: 0.564226

Epoch: 159
Loss: 0.54621621966362
RMSE train: 0.560924	val: 0.734001	test: 0.729645
MAE train: 0.433193	val: 0.546310	test: 0.546399

Epoch: 160
Loss: 0.4471452310681343
RMSE train: 0.560687	val: 0.715028	test: 0.732296
MAE train: 0.434802	val: 0.532066	test: 0.553291

Epoch: 161
Loss: 0.5874450877308846
RMSE train: 0.588364	val: 0.721491	test: 0.744788
MAE train: 0.452345	val: 0.535171	test: 0.554856

Epoch: 162
Loss: 0.5488977581262589
RMSE train: 0.663755	val: 0.781088	test: 0.812851
MAE train: 0.514470	val: 0.594239	test: 0.609063

Epoch: 163
Loss: 0.4618327468633652
RMSE train: 0.574433	val: 0.709888	test: 0.774737
MAE train: 0.448960	val: 0.539503	test: 0.571979

Epoch: 164
Loss: 0.4469458609819412
RMSE train: 0.550641	val: 0.706405	test: 0.760490
MAE train: 0.426896	val: 0.541026	test: 0.562537

Early stopping
Best (RMSE):	 train: 0.576272	val: 0.697808	test: 0.763430
Best (MAE):	 train: 0.438387	val: 0.534927	test: 0.556988
All runs completed.

RMSE train: 0.480168	val: 0.876603	test: 0.723264
MAE train: 0.374304	val: 0.657573	test: 0.543660

Epoch: 145
Loss: 0.30334842950105667
RMSE train: 0.484007	val: 0.884705	test: 0.732279
MAE train: 0.373945	val: 0.659854	test: 0.548629

Epoch: 146
Loss: 0.32680005580186844
RMSE train: 0.495031	val: 0.898298	test: 0.724746
MAE train: 0.379455	val: 0.666673	test: 0.535430

Epoch: 147
Loss: 0.3371461108326912
RMSE train: 0.501274	val: 0.862670	test: 0.732562
MAE train: 0.378824	val: 0.665947	test: 0.527809

Epoch: 148
Loss: 0.3390222042798996
RMSE train: 0.504462	val: 0.846405	test: 0.745609
MAE train: 0.379415	val: 0.637034	test: 0.546192

Epoch: 149
Loss: 0.39758266508579254
RMSE train: 0.545744	val: 1.004218	test: 0.775994
MAE train: 0.427875	val: 0.693854	test: 0.600620

Epoch: 150
Loss: 0.40729599446058273
RMSE train: 0.463609	val: 0.860342	test: 0.757885
MAE train: 0.363878	val: 0.625182	test: 0.568454

Epoch: 151
Loss: 0.3540651202201843
RMSE train: 0.476176	val: 0.758701	test: 0.783545
MAE train: 0.361130	val: 0.588410	test: 0.570219

Epoch: 152
Loss: 0.340113028883934
RMSE train: 0.513602	val: 0.838483	test: 0.798799
MAE train: 0.408147	val: 0.639866	test: 0.600443

Epoch: 153
Loss: 0.3499988690018654
RMSE train: 0.467027	val: 0.800455	test: 0.760745
MAE train: 0.367586	val: 0.599503	test: 0.576036

Epoch: 154
Loss: 0.32866962254047394
RMSE train: 0.444049	val: 0.796214	test: 0.748275
MAE train: 0.348277	val: 0.602495	test: 0.557435

Epoch: 155
Loss: 0.3260217607021332
RMSE train: 0.474414	val: 0.878358	test: 0.788706
MAE train: 0.375948	val: 0.644863	test: 0.582034

Epoch: 156
Loss: 0.292325172573328
RMSE train: 0.469883	val: 0.906347	test: 0.755314
MAE train: 0.369025	val: 0.642074	test: 0.563208

Epoch: 157
Loss: 0.33807628601789474
RMSE train: 0.476550	val: 0.871691	test: 0.737844
MAE train: 0.369546	val: 0.630637	test: 0.547927

Epoch: 158
Loss: 0.2797984927892685
RMSE train: 0.537551	val: 0.854601	test: 0.752809
MAE train: 0.411884	val: 0.665559	test: 0.551779

Epoch: 159
Loss: 0.3247736990451813
RMSE train: 0.520723	val: 0.826230	test: 0.757429
MAE train: 0.398206	val: 0.659028	test: 0.548785

Epoch: 160
Loss: 0.35354091972112656
RMSE train: 0.451219	val: 0.792466	test: 0.755339
MAE train: 0.352108	val: 0.620567	test: 0.551361

Epoch: 161
Loss: 0.30485915392637253
RMSE train: 0.478561	val: 0.867927	test: 0.742392
MAE train: 0.368604	val: 0.659991	test: 0.536366

Epoch: 162
Loss: 0.2962484136223793
RMSE train: 0.492131	val: 0.859748	test: 0.728639
MAE train: 0.375116	val: 0.657365	test: 0.524272

Epoch: 163
Loss: 0.30814605951309204
RMSE train: 0.487099	val: 0.925804	test: 0.732465
MAE train: 0.376323	val: 0.694156	test: 0.531987

Epoch: 164
Loss: 0.2941408082842827
RMSE train: 0.454572	val: 0.923055	test: 0.747747
MAE train: 0.353762	val: 0.666554	test: 0.537944

Epoch: 165
Loss: 0.3336104303598404
RMSE train: 0.512730	val: 0.896944	test: 0.768211
MAE train: 0.398151	val: 0.684904	test: 0.548844

Epoch: 166
Loss: 0.3828648030757904
RMSE train: 0.500903	val: 0.847132	test: 0.760622
MAE train: 0.389118	val: 0.649271	test: 0.549554

Epoch: 167
Loss: 0.2699882462620735
RMSE train: 0.466353	val: 0.816545	test: 0.743216
MAE train: 0.361608	val: 0.624109	test: 0.548607

Epoch: 168
Loss: 0.2790573388338089
RMSE train: 0.480898	val: 0.836173	test: 0.750859
MAE train: 0.372968	val: 0.642984	test: 0.562221

Epoch: 169
Loss: 0.292803019285202
RMSE train: 0.474889	val: 0.819808	test: 0.769393
MAE train: 0.369704	val: 0.629003	test: 0.580771

Epoch: 170
Loss: 0.30214377492666245
RMSE train: 0.474823	val: 0.931828	test: 0.775423
MAE train: 0.375914	val: 0.666273	test: 0.583682

Epoch: 171
Loss: 0.3163137882947922
RMSE train: 0.433304	val: 0.874020	test: 0.752794
MAE train: 0.335222	val: 0.637767	test: 0.546776

Epoch: 172
Loss: 0.3100230023264885
RMSE train: 0.431018	val: 0.841299	test: 0.764605
MAE train: 0.336048	val: 0.623157	test: 0.552781

Epoch: 173
Loss: 0.29760414361953735
RMSE train: 0.503815	val: 0.955467	test: 0.800580
MAE train: 0.399736	val: 0.691444	test: 0.592829

Epoch: 174
Loss: 0.33903732895851135
RMSE train: 0.449350	val: 0.848905	test: 0.778594
MAE train: 0.354994	val: 0.642728	test: 0.568698

Epoch: 175
Loss: 0.2841491512954235
RMSE train: 0.430284	val: 0.838045	test: 0.762883
MAE train: 0.333777	val: 0.627472	test: 0.548860

Epoch: 176
Loss: 0.3065830245614052
RMSE train: 0.482864	val: 0.860701	test: 0.777824
MAE train: 0.376322	val: 0.658329	test: 0.568219

Epoch: 177
Loss: 0.2661082334816456
RMSE train: 0.514263	val: 0.879977	test: 0.799043
MAE train: 0.400537	val: 0.673234	test: 0.589819

Epoch: 178
Loss: 0.276507243514061
RMSE train: 0.470400	val: 0.845042	test: 0.775226
MAE train: 0.368953	val: 0.644395	test: 0.574684

Epoch: 179
Loss: 0.29795414954423904
RMSE train: 0.449692	val: 0.822279	test: 0.777798
MAE train: 0.353070	val: 0.623175	test: 0.558536

Epoch: 180
Loss: 0.3363463133573532
RMSE train: 0.492626	val: 0.908493	test: 0.777499
MAE train: 0.386192	val: 0.666200	test: 0.561201

Epoch: 181
Loss: 0.3080364540219307
RMSE train: 0.495060	val: 0.851576	test: 0.750108
MAE train: 0.385249	val: 0.645994	test: 0.542072

Epoch: 182
Loss: 0.30970799550414085
RMSE train: 0.489122	val: 0.805504	test: 0.756739
MAE train: 0.374454	val: 0.628664	test: 0.528756

Epoch: 183
Loss: 0.31658191978931427
RMSE train: 0.457923	val: 0.840101	test: 0.742162
MAE train: 0.355887	val: 0.639877	test: 0.526901

Epoch: 184
Loss: 0.3238091245293617
RMSE train: 0.488911	val: 0.899500	test: 0.745159
MAE train: 0.378934	val: 0.664510	test: 0.549109

Epoch: 185
Loss: 0.30436085164546967
RMSE train: 0.474862	val: 0.835403	test: 0.756033
MAE train: 0.366810	val: 0.636399	test: 0.550820

Epoch: 186
Loss: 0.3147009089589119
RMSE train: 0.459547	val: 0.822614	test: 0.752140
MAE train: 0.348106	val: 0.629784	test: 0.533967

Early stopping
Best (RMSE):	 train: 0.476176	val: 0.758701	test: 0.783545
Best (MAE):	 train: 0.361130	val: 0.588410	test: 0.570219

RMSE train: 0.468291	val: 0.746560	test: 0.767031
MAE train: 0.357822	val: 0.572340	test: 0.559367

Epoch: 145
Loss: 0.37162307649850845
RMSE train: 0.460672	val: 0.740745	test: 0.755188
MAE train: 0.348219	val: 0.565885	test: 0.549694

Epoch: 146
Loss: 0.3704773783683777
RMSE train: 0.487180	val: 0.753684	test: 0.723574
MAE train: 0.360588	val: 0.574966	test: 0.516044

Epoch: 147
Loss: 0.3418123498558998
RMSE train: 0.528160	val: 0.875432	test: 0.713121
MAE train: 0.384889	val: 0.649450	test: 0.512405

Epoch: 148
Loss: 0.31658564507961273
RMSE train: 0.498307	val: 0.728027	test: 0.744512
MAE train: 0.373489	val: 0.543900	test: 0.540615

Epoch: 149
Loss: 0.26743221655488014
RMSE train: 0.492321	val: 0.771794	test: 0.721182
MAE train: 0.359678	val: 0.581211	test: 0.515801

Epoch: 150
Loss: 0.3094181790947914
RMSE train: 0.459468	val: 0.827292	test: 0.712776
MAE train: 0.343824	val: 0.618061	test: 0.511277

Epoch: 151
Loss: 0.3479952737689018
RMSE train: 0.482817	val: 0.785203	test: 0.708604
MAE train: 0.361026	val: 0.599988	test: 0.512973

Epoch: 152
Loss: 0.3154228925704956
RMSE train: 0.463535	val: 0.757973	test: 0.697424
MAE train: 0.344921	val: 0.572232	test: 0.511391

Epoch: 153
Loss: 0.3004157915711403
RMSE train: 0.464342	val: 0.842691	test: 0.694899
MAE train: 0.345857	val: 0.628544	test: 0.510539

Epoch: 154
Loss: 0.32322267442941666
RMSE train: 0.483266	val: 0.802733	test: 0.713403
MAE train: 0.352586	val: 0.620139	test: 0.508079

Epoch: 155
Loss: 0.3246736451983452
RMSE train: 0.545337	val: 0.765321	test: 0.745102
MAE train: 0.384602	val: 0.584451	test: 0.527509

Epoch: 156
Loss: 0.36263295263051987
RMSE train: 0.426784	val: 0.833235	test: 0.731592
MAE train: 0.326838	val: 0.588854	test: 0.522139

Epoch: 157
Loss: 0.3142065703868866
RMSE train: 0.431696	val: 0.769626	test: 0.711457
MAE train: 0.320662	val: 0.554453	test: 0.515272

Epoch: 158
Loss: 0.3158419355750084
RMSE train: 0.525289	val: 0.788933	test: 0.743785
MAE train: 0.394672	val: 0.603515	test: 0.539356

Epoch: 159
Loss: 0.34456874430179596
RMSE train: 0.467970	val: 0.839341	test: 0.715856
MAE train: 0.353819	val: 0.624295	test: 0.527676

Epoch: 160
Loss: 0.31707972288131714
RMSE train: 0.440364	val: 0.771794	test: 0.707047
MAE train: 0.330839	val: 0.564227	test: 0.521266

Epoch: 161
Loss: 0.32161765545606613
RMSE train: 0.505814	val: 0.797681	test: 0.723237
MAE train: 0.373074	val: 0.602834	test: 0.527600

Epoch: 162
Loss: 0.2919863611459732
RMSE train: 0.456275	val: 0.775133	test: 0.708748
MAE train: 0.337884	val: 0.588617	test: 0.517570

Epoch: 163
Loss: 0.28618865087628365
RMSE train: 0.452915	val: 0.742220	test: 0.702019
MAE train: 0.334720	val: 0.570425	test: 0.509756

Epoch: 164
Loss: 0.3044424168765545
RMSE train: 0.485420	val: 0.778690	test: 0.714755
MAE train: 0.359228	val: 0.602481	test: 0.514478

Epoch: 165
Loss: 0.32568325102329254
RMSE train: 0.506717	val: 0.788437	test: 0.722580
MAE train: 0.376239	val: 0.614763	test: 0.521958

Epoch: 166
Loss: 0.29439131915569305
RMSE train: 0.501692	val: 0.751732	test: 0.743944
MAE train: 0.376443	val: 0.596380	test: 0.531341

Epoch: 167
Loss: 0.28185541182756424
RMSE train: 0.484512	val: 0.803689	test: 0.736157
MAE train: 0.350791	val: 0.622360	test: 0.527838

Epoch: 168
Loss: 0.2909472957253456
RMSE train: 0.429475	val: 0.719833	test: 0.709492
MAE train: 0.319433	val: 0.560336	test: 0.521941

Epoch: 169
Loss: 0.2972516641020775
RMSE train: 0.479338	val: 0.764001	test: 0.688460
MAE train: 0.351523	val: 0.584900	test: 0.505854

Epoch: 170
Loss: 0.2933298572897911
RMSE train: 0.461301	val: 0.787894	test: 0.696105
MAE train: 0.340267	val: 0.595067	test: 0.504490

Epoch: 171
Loss: 0.2823454700410366
RMSE train: 0.420387	val: 0.706110	test: 0.743133
MAE train: 0.312212	val: 0.534952	test: 0.525930

Epoch: 172
Loss: 0.26514462009072304
RMSE train: 0.413058	val: 0.747169	test: 0.730549
MAE train: 0.312640	val: 0.551185	test: 0.524660

Epoch: 173
Loss: 0.35743655264377594
RMSE train: 0.415408	val: 0.743639	test: 0.729124
MAE train: 0.315321	val: 0.545554	test: 0.532992

Epoch: 174
Loss: 0.2671571299433708
RMSE train: 0.420295	val: 0.762702	test: 0.725259
MAE train: 0.317362	val: 0.562787	test: 0.526173

Epoch: 175
Loss: 0.28864140063524246
RMSE train: 0.446046	val: 0.796435	test: 0.735220
MAE train: 0.331546	val: 0.583747	test: 0.529955

Epoch: 176
Loss: 0.30004994571208954
RMSE train: 0.441762	val: 0.804812	test: 0.732671
MAE train: 0.327600	val: 0.593184	test: 0.527111

Epoch: 177
Loss: 0.2816793918609619
RMSE train: 0.468776	val: 0.800125	test: 0.744769
MAE train: 0.347691	val: 0.610312	test: 0.531281

Epoch: 178
Loss: 0.2981095463037491
RMSE train: 0.556799	val: 0.894523	test: 0.756698
MAE train: 0.403988	val: 0.678125	test: 0.542656

Epoch: 179
Loss: 0.2858722433447838
RMSE train: 0.465510	val: 0.753590	test: 0.746148
MAE train: 0.351636	val: 0.575920	test: 0.539405

Epoch: 180
Loss: 0.2920026406645775
RMSE train: 0.495534	val: 0.827090	test: 0.730209
MAE train: 0.372044	val: 0.631992	test: 0.522350

Epoch: 181
Loss: 0.2587651386857033
RMSE train: 0.523266	val: 0.797415	test: 0.744995
MAE train: 0.390569	val: 0.619859	test: 0.526161

Epoch: 182
Loss: 0.2556295432150364
RMSE train: 0.500124	val: 0.777977	test: 0.735425
MAE train: 0.370376	val: 0.597950	test: 0.524526

Epoch: 183
Loss: 0.28505317866802216
RMSE train: 0.431269	val: 0.792701	test: 0.706415
MAE train: 0.320675	val: 0.583050	test: 0.511260

Epoch: 184
Loss: 0.3059083968400955
RMSE train: 0.455702	val: 0.778988	test: 0.720529
MAE train: 0.341240	val: 0.585377	test: 0.523747

Epoch: 185
Loss: 0.2824431546032429
RMSE train: 0.430810	val: 0.770409	test: 0.723570
MAE train: 0.324258	val: 0.570974	test: 0.520833

Epoch: 186
Loss: 0.2673022896051407
RMSE train: 0.414596	val: 0.764820	test: 0.718474
MAE train: 0.308830	val: 0.571381	test: 0.513734

Epoch: 187
Loss: 0.30256855487823486
RMSE train: 0.478005	val: 0.810598	test: 0.744070
MAE train: 0.351744	val: 0.621783	test: 0.527313

Epoch: 188
Loss: 0.25398924574255943
RMSE train: 0.464026	val: 0.743278	test: 0.758864
MAE train: 0.335972	val: 0.570016	test: 0.536556

Epoch: 189
Loss: 0.2747681736946106
RMSE train: 0.405812	val: 0.758011	test: 0.741030
MAE train: 0.303862	val: 0.573938	test: 0.539155

Epoch: 190
Loss: 0.3087274841964245
RMSE train: 0.427327	val: 0.755417	test: 0.725751
MAE train: 0.323700	val: 0.576957	test: 0.532235

Epoch: 191
Loss: 0.27247392386198044
RMSE train: 0.458554	val: 0.721350	test: 0.727504
MAE train: 0.344557	val: 0.557348	test: 0.518606

Epoch: 192
Loss: 0.3408950939774513
RMSE train: 0.476896	val: 0.741805	test: 0.729040
MAE train: 0.351636	val: 0.573336	test: 0.515035

Epoch: 193
Loss: 0.2590581327676773
RMSE train: 0.454602	val: 0.700595	test: 0.726655
MAE train: 0.331634	val: 0.536219	test: 0.515628

Epoch: 194
Loss: 0.268003910779953
RMSE train: 0.429481	val: 0.793346	test: 0.692072
MAE train: 0.318047	val: 0.576276	test: 0.499477

Epoch: 195
Loss: 0.2689167410135269
RMSE train: 0.412107	val: 0.800172	test: 0.703261
MAE train: 0.314527	val: 0.573310	test: 0.520696

Epoch: 196
Loss: 0.30766919255256653
RMSE train: 0.407659	val: 0.724044	test: 0.772589
MAE train: 0.305018	val: 0.535986	test: 0.552908

Epoch: 197
Loss: 0.28133442252874374
RMSE train: 0.445478	val: 0.806686	test: 0.794563
MAE train: 0.344768	val: 0.611986	test: 0.562290

Epoch: 198
Loss: 0.297195628285408
RMSE train: 0.414499	val: 0.810073	test: 0.772939
MAE train: 0.318320	val: 0.588765	test: 0.572303

Epoch: 199
Loss: 0.27246226370334625
RMSE train: 0.427617	val: 0.800846	test: 0.732938
MAE train: 0.324260	val: 0.574319	test: 0.536742

Epoch: 200
Loss: 0.2630344033241272
RMSE train: 0.417080	val: 0.796693	test: 0.723938
MAE train: 0.313960	val: 0.576716	test: 0.518748

Epoch: 201
Loss: 0.2725718058645725
RMSE train: 0.412522	val: 0.730174	test: 0.741843
MAE train: 0.311853	val: 0.543547	test: 0.536234

Epoch: 202
Loss: 0.2723183259367943
RMSE train: 0.461043	val: 0.770646	test: 0.746504
MAE train: 0.354384	val: 0.599952	test: 0.529268

Epoch: 203
Loss: 0.2791942283511162
RMSE train: 0.426128	val: 0.787140	test: 0.723225
MAE train: 0.326343	val: 0.587581	test: 0.518408

Epoch: 204
Loss: 0.28836241364479065
RMSE train: 0.438288	val: 0.772373	test: 0.717244
MAE train: 0.329822	val: 0.597517	test: 0.506844

Epoch: 205
Loss: 0.2691972367465496
RMSE train: 0.496345	val: 0.826203	test: 0.726107
MAE train: 0.371418	val: 0.642782	test: 0.516081

Epoch: 206
Loss: 0.2720835916697979
RMSE train: 0.467829	val: 0.792014	test: 0.725809
MAE train: 0.353146	val: 0.609716	test: 0.522146

Epoch: 207
Loss: 0.30655665323138237
RMSE train: 0.484975	val: 0.813203	test: 0.722241
MAE train: 0.364109	val: 0.626664	test: 0.517141

Epoch: 208
Loss: 0.233248770236969
RMSE train: 0.485304	val: 0.831216	test: 0.713739
MAE train: 0.363581	val: 0.630604	test: 0.511913

Epoch: 209
Loss: 0.23976104706525803
RMSE train: 0.457085	val: 0.773239	test: 0.724684
MAE train: 0.344787	val: 0.593625	test: 0.521589

Epoch: 210
Loss: 0.2637622095644474
RMSE train: 0.442394	val: 0.739064	test: 0.742184
MAE train: 0.329278	val: 0.556088	test: 0.534166

Epoch: 211
Loss: 0.2937462478876114
RMSE train: 0.412094	val: 0.739283	test: 0.748847
MAE train: 0.307656	val: 0.549081	test: 0.535645

Epoch: 212
Loss: 0.2044486477971077
RMSE train: 0.406612	val: 0.760711	test: 0.729928
MAE train: 0.302269	val: 0.576036	test: 0.523234

Epoch: 213
Loss: 0.25000230967998505
RMSE train: 0.401325	val: 0.732493	test: 0.738143
MAE train: 0.300795	val: 0.547320	test: 0.525575

Epoch: 214
Loss: 0.22828591242432594
RMSE train: 0.386418	val: 0.737168	test: 0.754920
MAE train: 0.292567	val: 0.554291	test: 0.525999

Epoch: 215
Loss: 0.25899870321154594
RMSE train: 0.375114	val: 0.774930	test: 0.745198
MAE train: 0.285495	val: 0.564532	test: 0.544863

Epoch: 216
Loss: 0.24431275948882103
RMSE train: 0.429646	val: 0.775461	test: 0.715565
MAE train: 0.322277	val: 0.573344	test: 0.534771

Epoch: 217
Loss: 0.29478656873106956
RMSE train: 0.483743	val: 0.759175	test: 0.715726
MAE train: 0.343533	val: 0.584931	test: 0.521633

Epoch: 218
Loss: 0.2493237406015396
RMSE train: 0.507297	val: 0.824799	test: 0.737065
MAE train: 0.365942	val: 0.618806	test: 0.525678

Epoch: 219
Loss: 0.26296159997582436
RMSE train: 0.463965	val: 0.838680	test: 0.751186
MAE train: 0.343890	val: 0.612229	test: 0.541738

Epoch: 220
Loss: 0.2810937687754631
RMSE train: 0.415266	val: 0.743316	test: 0.760723
MAE train: 0.313106	val: 0.534799	test: 0.540446

Epoch: 221
Loss: 0.2855883277952671
RMSE train: 0.451163	val: 0.824814	test: 0.755867
MAE train: 0.344411	val: 0.617900	test: 0.533907

Epoch: 222
Loss: 0.24681001156568527
RMSE train: 0.408021	val: 0.749950	test: 0.778649
MAE train: 0.309899	val: 0.560985	test: 0.561540

Epoch: 223
Loss: 0.22177769988775253
RMSE train: 0.439903	val: 0.782472	test: 0.772010
MAE train: 0.334284	val: 0.594510	test: 0.562400

Epoch: 224
Loss: 0.2382657453417778
RMSE train: 0.462112	val: 0.810710	test: 0.769778
MAE train: 0.345850	val: 0.622623	test: 0.539737

Epoch: 225
Loss: 0.22790593653917313
RMSE train: 0.387813	val: 0.794365	test: 0.736896
MAE train: 0.287095	val: 0.572285	test: 0.523358

Epoch: 226
Loss: 0.3074924871325493
RMSE train: 0.428704	val: 0.749115	test: 0.753681
MAE train: 0.323046	val: 0.573620	test: 0.538090

Epoch: 227
Loss: 0.2277335189282894
RMSE train: 0.478689	val: 0.788607	test: 0.774659
MAE train: 0.354744	val: 0.619317	test: 0.544287

Epoch: 228
Loss: 0.253075510263443
RMSE train: 0.385922	val: 0.809771	test: 0.756461
MAE train: 0.288057	val: 0.587330	test: 0.535769

Early stopping
Best (RMSE):	 train: 0.454602	val: 0.700595	test: 0.726655
Best (MAE):	 train: 0.331634	val: 0.536219	test: 0.515628
All runs completed.
