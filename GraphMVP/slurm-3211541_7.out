>>> Starting run for dataset: lipo
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running SCAFF configs_split_experiments/GraphMVP/lipo/scaff/train_prop=0.6.yml on cuda:0
Running SCAFF configs_split_experiments/GraphMVP/lipo/scaff/train_prop=0.7.yml on cuda:1
Running SCAFF configs_split_experiments/GraphMVP/lipo/scaff/train_prop=0.8.yml on cuda:2
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/lipo/scaff/train_prop=0.6.yml --runseed 4 --device cuda:0
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/lipo/scaff/train_prop=0.6.yml --runseed 5 --device cuda:0
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/lipo/scaff/train_prop=0.6.yml --runseed 6 --device cuda:0
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/lipo/scaff/train_prop=0.8.yml --runseed 4 --device cuda:2
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/lipo/scaff/train_prop=0.8.yml --runseed 5 --device cuda:2
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/lipo/scaff/train_prop=0.8.yml --runseed 6 --device cuda:2
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/lipo/scaff/train_prop=0.7.yml --runseed 4 --device cuda:1
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/lipo/scaff/train_prop=0.7.yml --runseed 5 --device cuda:1
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/lipo/scaff/train_prop=0.7.yml --runseed 6 --device cuda:1
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/lipo/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/lipo/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/lipo/scaff/train_prop=0.6/lipophilicity_scaff_6_26-05_11-12-21  ]
[ Using Seed :  6  ]
[ Using device :  cuda:0  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.204765653610229
RMSE train: 2.181048	val: 2.255082	test: 2.262679
MAE train: 1.930271	val: 2.002941	test: 2.006098

Epoch: 2
Loss: 3.74315767288208
RMSE train: 1.940131	val: 2.003241	test: 1.988565
MAE train: 1.710307	val: 1.770049	test: 1.744340

Epoch: 3
Loss: 2.622487926483154
RMSE train: 1.533224	val: 1.594223	test: 1.578668
MAE train: 1.317780	val: 1.366817	test: 1.345633

Epoch: 4
Loss: 1.744124460220337
RMSE train: 1.201833	val: 1.278718	test: 1.271760
MAE train: 1.009052	val: 1.069234	test: 1.062250

Epoch: 5
Loss: 1.1634833216667175
RMSE train: 0.872326	val: 1.009459	test: 0.990281
MAE train: 0.705730	val: 0.823878	test: 0.803719

Epoch: 6
Loss: 0.8321503102779388
RMSE train: 0.763576	val: 0.927896	test: 0.926471
MAE train: 0.598910	val: 0.742658	test: 0.738872

Epoch: 7
Loss: 0.7263468623161315
RMSE train: 0.732239	val: 0.913906	test: 0.896412
MAE train: 0.567065	val: 0.721410	test: 0.712208

Epoch: 8
Loss: 0.6824321746826172
RMSE train: 0.694044	val: 0.884852	test: 0.872358
MAE train: 0.538360	val: 0.702800	test: 0.692749

Epoch: 9
Loss: 0.644568532705307
RMSE train: 0.686879	val: 0.877747	test: 0.860937
MAE train: 0.536605	val: 0.696318	test: 0.683914

Epoch: 10
Loss: 0.6114972293376922
RMSE train: 0.668392	val: 0.881789	test: 0.877855
MAE train: 0.516036	val: 0.693869	test: 0.691171

Epoch: 11
Loss: 0.601864579319954
RMSE train: 0.668226	val: 0.862197	test: 0.846826
MAE train: 0.519971	val: 0.683500	test: 0.676541

Epoch: 12
Loss: 0.559411808848381
RMSE train: 0.648575	val: 0.845402	test: 0.840977
MAE train: 0.503613	val: 0.670741	test: 0.668362

Epoch: 13
Loss: 0.5695210576057435
RMSE train: 0.636258	val: 0.857529	test: 0.851890
MAE train: 0.488800	val: 0.678256	test: 0.674967

Epoch: 14
Loss: 0.5189112156629563
RMSE train: 0.636943	val: 0.846319	test: 0.837543
MAE train: 0.492844	val: 0.668918	test: 0.661968

Epoch: 15
Loss: 0.51126149892807
RMSE train: 0.636289	val: 0.848860	test: 0.846831
MAE train: 0.485900	val: 0.664683	test: 0.666214

Epoch: 16
Loss: 0.5104893773794175
RMSE train: 0.615749	val: 0.851292	test: 0.844399
MAE train: 0.471849	val: 0.672352	test: 0.668900

Epoch: 17
Loss: 0.5207131713628769
RMSE train: 0.607542	val: 0.826605	test: 0.830464
MAE train: 0.466380	val: 0.646633	test: 0.654894

Epoch: 18
Loss: 0.4904328018426895
RMSE train: 0.601819	val: 0.826634	test: 0.825819
MAE train: 0.463075	val: 0.650088	test: 0.650876

Epoch: 19
Loss: 0.49269787669181825
RMSE train: 0.588413	val: 0.834714	test: 0.828806
MAE train: 0.451109	val: 0.654651	test: 0.653251

Epoch: 20
Loss: 0.46674391329288484
RMSE train: 0.575037	val: 0.809276	test: 0.803093
MAE train: 0.445904	val: 0.635306	test: 0.631198

Epoch: 21
Loss: 0.44719131886959074
RMSE train: 0.589246	val: 0.814688	test: 0.819691
MAE train: 0.454300	val: 0.643360	test: 0.640984

Epoch: 22
Loss: 0.4742834478616714
RMSE train: 0.581890	val: 0.823277	test: 0.823885
MAE train: 0.450797	val: 0.649587	test: 0.648033Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/lipo/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/lipo/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/lipo/scaff/train_prop=0.6/lipophilicity_scaff_4_26-05_11-12-21  ]
[ Using Seed :  4  ]
[ Using device :  cuda:0  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.400736665725708
RMSE train: 2.107973	val: 2.154255	test: 2.153585
MAE train: 1.854524	val: 1.894870	test: 1.887084

Epoch: 2
Loss: 3.9572036981582643
RMSE train: 1.767212	val: 1.826480	test: 1.812537
MAE train: 1.531841	val: 1.582347	test: 1.564307

Epoch: 3
Loss: 2.798868680000305
RMSE train: 1.342626	val: 1.407931	test: 1.412951
MAE train: 1.132119	val: 1.195010	test: 1.194736

Epoch: 4
Loss: 1.9393096208572387
RMSE train: 1.107794	val: 1.189972	test: 1.182763
MAE train: 0.923752	val: 0.992423	test: 0.985195

Epoch: 5
Loss: 1.3157850742340087
RMSE train: 0.980665	val: 1.081326	test: 1.066042
MAE train: 0.811065	val: 0.897144	test: 0.880178

Epoch: 6
Loss: 0.8957314252853393
RMSE train: 0.791481	val: 0.952218	test: 0.937211
MAE train: 0.636788	val: 0.769359	test: 0.757921

Epoch: 7
Loss: 0.7373300790786743
RMSE train: 0.748306	val: 0.941313	test: 0.930670
MAE train: 0.585890	val: 0.743026	test: 0.735881

Epoch: 8
Loss: 0.713962972164154
RMSE train: 0.734055	val: 0.927020	test: 0.925917
MAE train: 0.569862	val: 0.725473	test: 0.730630

Epoch: 9
Loss: 0.6668757677078248
RMSE train: 0.685702	val: 0.900010	test: 0.902662
MAE train: 0.531624	val: 0.706251	test: 0.708630

Epoch: 10
Loss: 0.6263871788978577
RMSE train: 0.656863	val: 0.877172	test: 0.866915
MAE train: 0.509733	val: 0.692322	test: 0.684103

Epoch: 11
Loss: 0.603612819314003
RMSE train: 0.660825	val: 0.866909	test: 0.861253
MAE train: 0.513502	val: 0.682313	test: 0.679048

Epoch: 12
Loss: 0.5987068027257919
RMSE train: 0.672007	val: 0.906920	test: 0.913570
MAE train: 0.520176	val: 0.714063	test: 0.718534

Epoch: 13
Loss: 0.5463191896677018
RMSE train: 0.643771	val: 0.870271	test: 0.859262
MAE train: 0.503824	val: 0.681878	test: 0.678836

Epoch: 14
Loss: 0.5541695415973663
RMSE train: 0.670274	val: 0.877855	test: 0.887813
MAE train: 0.518697	val: 0.692494	test: 0.699494

Epoch: 15
Loss: 0.5174947321414948
RMSE train: 0.663737	val: 0.912133	test: 0.923848
MAE train: 0.513473	val: 0.716168	test: 0.725031

Epoch: 16
Loss: 0.5150660067796707
RMSE train: 0.608282	val: 0.845489	test: 0.844105
MAE train: 0.469835	val: 0.657687	test: 0.660393

Epoch: 17
Loss: 0.5198824524879455
RMSE train: 0.634242	val: 0.886786	test: 0.881088
MAE train: 0.485717	val: 0.694695	test: 0.688084

Epoch: 18
Loss: 0.5044995844364166
RMSE train: 0.607618	val: 0.851171	test: 0.857474
MAE train: 0.468103	val: 0.667483	test: 0.669220

Epoch: 19
Loss: 0.471952822804451
RMSE train: 0.620879	val: 0.871893	test: 0.884825
MAE train: 0.475086	val: 0.683698	test: 0.686748

Epoch: 20
Loss: 0.4627545207738876
RMSE train: 0.631497	val: 0.886825	test: 0.894270
MAE train: 0.489583	val: 0.696532	test: 0.699593

Epoch: 21
Loss: 0.46820562779903413
RMSE train: 0.622009	val: 0.861586	test: 0.865789
MAE train: 0.477696	val: 0.675658	test: 0.678535

Epoch: 22
Loss: 0.47339590489864347
RMSE train: 0.605049	val: 0.852540	test: 0.851802
MAE train: 0.463091	val: 0.668290	test: 0.672190Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/lipo/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/lipo/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/lipo/scaff/train_prop=0.6/lipophilicity_scaff_5_26-05_11-12-21  ]
[ Using Seed :  5  ]
[ Using device :  cuda:0  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 4.916498327255249
RMSE train: 1.953747	val: 2.033355	test: 2.025670
MAE train: 1.704023	val: 1.774299	test: 1.757959

Epoch: 2
Loss: 3.520723271369934
RMSE train: 1.762234	val: 1.826520	test: 1.807519
MAE train: 1.532747	val: 1.585737	test: 1.557419

Epoch: 3
Loss: 2.435979497432709
RMSE train: 1.407465	val: 1.480997	test: 1.465693
MAE train: 1.194851	val: 1.258675	test: 1.238131

Epoch: 4
Loss: 1.5929501533508301
RMSE train: 1.093172	val: 1.187565	test: 1.177158
MAE train: 0.913904	val: 0.986264	test: 0.975321

Epoch: 5
Loss: 1.1126267850399016
RMSE train: 0.840484	val: 0.983342	test: 0.963372
MAE train: 0.665508	val: 0.793058	test: 0.770428

Epoch: 6
Loss: 0.8185315668582916
RMSE train: 0.816561	val: 0.955604	test: 0.938144
MAE train: 0.654096	val: 0.770079	test: 0.750626

Epoch: 7
Loss: 0.6828527987003327
RMSE train: 0.721350	val: 0.926564	test: 0.906410
MAE train: 0.564223	val: 0.732900	test: 0.718709

Epoch: 8
Loss: 0.6915731370449066
RMSE train: 0.717529	val: 0.897456	test: 0.874950
MAE train: 0.554736	val: 0.704605	test: 0.694153

Epoch: 9
Loss: 0.6601224482059479
RMSE train: 0.677528	val: 0.871001	test: 0.846625
MAE train: 0.528796	val: 0.686592	test: 0.670431

Epoch: 10
Loss: 0.6123720467090606
RMSE train: 0.673968	val: 0.874545	test: 0.855634
MAE train: 0.521942	val: 0.688750	test: 0.677676

Epoch: 11
Loss: 0.5950247526168824
RMSE train: 0.667226	val: 0.872607	test: 0.852310
MAE train: 0.517576	val: 0.687171	test: 0.678439

Epoch: 12
Loss: 0.5675159096717834
RMSE train: 0.694652	val: 0.896932	test: 0.888909
MAE train: 0.541918	val: 0.706096	test: 0.703717

Epoch: 13
Loss: 0.5620223462581635
RMSE train: 0.639307	val: 0.853471	test: 0.841741
MAE train: 0.491815	val: 0.666864	test: 0.665552

Epoch: 14
Loss: 0.5220108419656754
RMSE train: 0.641097	val: 0.849696	test: 0.830258
MAE train: 0.492323	val: 0.664692	test: 0.657937

Epoch: 15
Loss: 0.5072578310966491
RMSE train: 0.628587	val: 0.831895	test: 0.816782
MAE train: 0.489370	val: 0.655270	test: 0.645637

Epoch: 16
Loss: 0.5142915070056915
RMSE train: 0.617511	val: 0.848010	test: 0.829955
MAE train: 0.477699	val: 0.666711	test: 0.657761

Epoch: 17
Loss: 0.4978748202323914
RMSE train: 0.604009	val: 0.825701	test: 0.815426
MAE train: 0.461967	val: 0.647622	test: 0.641394

Epoch: 18
Loss: 0.4867103070020676
RMSE train: 0.597234	val: 0.830148	test: 0.806432
MAE train: 0.455987	val: 0.652412	test: 0.636970

Epoch: 19
Loss: 0.48548664450645446
RMSE train: 0.608529	val: 0.834811	test: 0.816487
MAE train: 0.459764	val: 0.656767	test: 0.648351

Epoch: 20
Loss: 0.4586977154016495
RMSE train: 0.588711	val: 0.828068	test: 0.803651
MAE train: 0.452862	val: 0.648206	test: 0.635181

Epoch: 21
Loss: 0.47774350345134736
RMSE train: 0.593753	val: 0.833421	test: 0.830012
MAE train: 0.454752	val: 0.658406	test: 0.655471

Epoch: 22
Loss: 0.47684640884399415
RMSE train: 0.583759	val: 0.811863	test: 0.797804
MAE train: 0.455375	val: 0.633707	test: 0.624104Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/lipo/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/lipo/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/lipo/scaff/train_prop=0.7/lipophilicity_scaff_4_26-05_11-12-21  ]
[ Using Seed :  4  ]
[ Using device :  cuda:1  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.365746696790059
RMSE train: 2.222560	val: 2.235989	test: 2.283461
MAE train: 1.972209	val: 1.985802	test: 2.022466

Epoch: 2
Loss: 3.689067284266154
RMSE train: 1.716838	val: 1.740808	test: 1.771273
MAE train: 1.486017	val: 1.504591	test: 1.522347

Epoch: 3
Loss: 2.444475849469503
RMSE train: 1.364558	val: 1.421308	test: 1.430479
MAE train: 1.160215	val: 1.206109	test: 1.208537

Epoch: 4
Loss: 1.61502871910731
RMSE train: 0.997052	val: 1.085166	test: 1.066549
MAE train: 0.819769	val: 0.879416	test: 0.877692

Epoch: 5
Loss: 1.0447842677434285
RMSE train: 0.840511	val: 0.982783	test: 0.925930
MAE train: 0.673147	val: 0.774120	test: 0.757431

Epoch: 6
Loss: 0.8016301244497299
RMSE train: 0.787076	val: 0.955242	test: 0.897722
MAE train: 0.606881	val: 0.740966	test: 0.717997

Epoch: 7
Loss: 0.7349768330653509
RMSE train: 0.752272	val: 0.932620	test: 0.868670
MAE train: 0.580802	val: 0.717322	test: 0.697014

Epoch: 8
Loss: 0.6852850019931793
RMSE train: 0.756680	val: 0.959505	test: 0.895332
MAE train: 0.579616	val: 0.737349	test: 0.717792

Epoch: 9
Loss: 0.6556358337402344
RMSE train: 0.695559	val: 0.899203	test: 0.841454
MAE train: 0.541547	val: 0.690249	test: 0.679494

Epoch: 10
Loss: 0.628164753317833
RMSE train: 0.692443	val: 0.892224	test: 0.837389
MAE train: 0.538730	val: 0.690328	test: 0.679886

Epoch: 11
Loss: 0.6128722627957662
RMSE train: 0.674979	val: 0.868104	test: 0.816516
MAE train: 0.527152	val: 0.670750	test: 0.660558

Epoch: 12
Loss: 0.5769258489211401
RMSE train: 0.663473	val: 0.869589	test: 0.807145
MAE train: 0.522562	val: 0.672772	test: 0.646724

Epoch: 13
Loss: 0.5825790365537008
RMSE train: 0.668046	val: 0.883679	test: 0.830469
MAE train: 0.512663	val: 0.677915	test: 0.663842

Epoch: 14
Loss: 0.5421793957551321
RMSE train: 0.666594	val: 0.872707	test: 0.826043
MAE train: 0.517813	val: 0.664181	test: 0.669284

Epoch: 15
Loss: 0.5329454566041628
RMSE train: 0.643712	val: 0.859145	test: 0.793193
MAE train: 0.506876	val: 0.658294	test: 0.637882

Epoch: 16
Loss: 0.526736614604791
RMSE train: 0.653727	val: 0.875380	test: 0.831199
MAE train: 0.508322	val: 0.664697	test: 0.666200

Epoch: 17
Loss: 0.522698886692524
RMSE train: 0.668088	val: 0.869381	test: 0.824115
MAE train: 0.511129	val: 0.671914	test: 0.658864

Epoch: 18
Loss: 0.5152282317479452
RMSE train: 0.627658	val: 0.840728	test: 0.778636
MAE train: 0.486480	val: 0.639884	test: 0.626341

Epoch: 19
Loss: 0.4942773679892222
RMSE train: 0.611442	val: 0.833574	test: 0.786447
MAE train: 0.470080	val: 0.639944	test: 0.624121

Epoch: 20
Loss: 0.4773508384823799
RMSE train: 0.614724	val: 0.831841	test: 0.775033
MAE train: 0.473310	val: 0.643841	test: 0.621739

Epoch: 21
Loss: 0.4927327757080396
RMSE train: 0.600348	val: 0.832156	test: 0.782404
MAE train: 0.460689	val: 0.637039	test: 0.621562

Epoch: 22
Loss: 0.4671425024668376
RMSE train: 0.587890	val: 0.823090	test: 0.777070
MAE train: 0.452438	val: 0.629553	test: 0.618549Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/lipo/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/lipo/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/lipo/scaff/train_prop=0.7/lipophilicity_scaff_6_26-05_11-12-21  ]
[ Using Seed :  6  ]
[ Using device :  cuda:1  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.132364312807719
RMSE train: 2.068511	val: 2.098474	test: 2.140667
MAE train: 1.806685	val: 1.846310	test: 1.870002

Epoch: 2
Loss: 3.4782143036524453
RMSE train: 1.601671	val: 1.643294	test: 1.658075
MAE train: 1.371719	val: 1.410519	test: 1.412465

Epoch: 3
Loss: 2.2565036614735923
RMSE train: 1.193799	val: 1.258651	test: 1.242534
MAE train: 0.995504	val: 1.048892	test: 1.035161

Epoch: 4
Loss: 1.399800290664037
RMSE train: 0.984131	val: 1.079202	test: 1.043779
MAE train: 0.807193	val: 0.881823	test: 0.852880

Epoch: 5
Loss: 0.9595697621504465
RMSE train: 0.801815	val: 0.947769	test: 0.871211
MAE train: 0.627494	val: 0.741807	test: 0.712245

Epoch: 6
Loss: 0.7933758546908697
RMSE train: 0.770750	val: 0.954860	test: 0.883893
MAE train: 0.592958	val: 0.734269	test: 0.713650

Epoch: 7
Loss: 0.7331704050302505
RMSE train: 0.744984	val: 0.927568	test: 0.855842
MAE train: 0.573711	val: 0.708409	test: 0.686915

Epoch: 8
Loss: 0.6765590906143188
RMSE train: 0.723773	val: 0.900405	test: 0.849199
MAE train: 0.560848	val: 0.698882	test: 0.680566

Epoch: 9
Loss: 0.6609107404947281
RMSE train: 0.728753	val: 0.916175	test: 0.834337
MAE train: 0.557933	val: 0.702411	test: 0.671088

Epoch: 10
Loss: 0.6342827926079432
RMSE train: 0.700275	val: 0.873393	test: 0.824182
MAE train: 0.545052	val: 0.673539	test: 0.655902

Epoch: 11
Loss: 0.6224167843659719
RMSE train: 0.675645	val: 0.876540	test: 0.812405
MAE train: 0.522562	val: 0.677101	test: 0.652387

Epoch: 12
Loss: 0.5782406181097031
RMSE train: 0.673918	val: 0.864454	test: 0.804563
MAE train: 0.522876	val: 0.664131	test: 0.644627

Epoch: 13
Loss: 0.5593408097823461
RMSE train: 0.686182	val: 0.900778	test: 0.847468
MAE train: 0.524976	val: 0.696615	test: 0.671985

Epoch: 14
Loss: 0.5730080952246984
RMSE train: 0.670931	val: 0.875348	test: 0.809964
MAE train: 0.511320	val: 0.672994	test: 0.642914

Epoch: 15
Loss: 0.5587389593323072
RMSE train: 0.682492	val: 0.887041	test: 0.836177
MAE train: 0.523954	val: 0.682018	test: 0.661144

Epoch: 16
Loss: 0.5438175549109777
RMSE train: 0.636287	val: 0.839838	test: 0.776646
MAE train: 0.496530	val: 0.647177	test: 0.617556

Epoch: 17
Loss: 0.5303531885147095
RMSE train: 0.628545	val: 0.844322	test: 0.791971
MAE train: 0.484262	val: 0.649609	test: 0.629668

Epoch: 18
Loss: 0.4952285016576449
RMSE train: 0.637484	val: 0.848256	test: 0.788504
MAE train: 0.491603	val: 0.649572	test: 0.629802

Epoch: 19
Loss: 0.48408269385496777
RMSE train: 0.619513	val: 0.830055	test: 0.785530
MAE train: 0.475778	val: 0.637525	test: 0.618541

Epoch: 20
Loss: 0.5013214498758316
RMSE train: 0.612035	val: 0.844852	test: 0.772576
MAE train: 0.469769	val: 0.642796	test: 0.604767

Epoch: 21
Loss: 0.48570315291484195
RMSE train: 0.608489	val: 0.829187	test: 0.780797
MAE train: 0.469809	val: 0.635283	test: 0.617274

Epoch: 22
Loss: 0.4846099689602852
RMSE train: 0.600157	val: 0.818294	test: 0.772863
MAE train: 0.463363	val: 0.620771	test: 0.603909Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/lipo/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/lipo/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/lipo/scaff/train_prop=0.7/lipophilicity_scaff_5_26-05_11-12-21  ]
[ Using Seed :  5  ]
[ Using device :  cuda:1  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 4.8306935628255205
RMSE train: 1.897827	val: 1.915220	test: 1.958920
MAE train: 1.652529	val: 1.670789	test: 1.693594

Epoch: 2
Loss: 3.2749904791514077
RMSE train: 1.595537	val: 1.616864	test: 1.641871
MAE train: 1.371468	val: 1.387880	test: 1.399312

Epoch: 3
Loss: 2.096952219804128
RMSE train: 1.185062	val: 1.227881	test: 1.228522
MAE train: 0.989129	val: 1.022833	test: 1.020583

Epoch: 4
Loss: 1.2776189545790355
RMSE train: 0.883118	val: 0.994905	test: 0.960750
MAE train: 0.711509	val: 0.794367	test: 0.787404

Epoch: 5
Loss: 0.9493754059076309
RMSE train: 0.782969	val: 0.934868	test: 0.880955
MAE train: 0.617165	val: 0.732413	test: 0.721714

Epoch: 6
Loss: 0.7700694302717844
RMSE train: 0.894664	val: 1.063546	test: 1.014718
MAE train: 0.693628	val: 0.814601	test: 0.821567

Epoch: 7
Loss: 0.7316430856784185
RMSE train: 0.753438	val: 0.930466	test: 0.861050
MAE train: 0.583537	val: 0.714797	test: 0.691500

Epoch: 8
Loss: 0.6679127067327499
RMSE train: 0.716550	val: 0.908781	test: 0.833861
MAE train: 0.559100	val: 0.703637	test: 0.672147

Epoch: 9
Loss: 0.6472504188617071
RMSE train: 0.695551	val: 0.884749	test: 0.812624
MAE train: 0.539152	val: 0.680526	test: 0.655748

Epoch: 10
Loss: 0.6356953581174215
RMSE train: 0.685605	val: 0.869082	test: 0.811705
MAE train: 0.531679	val: 0.667183	test: 0.649949

Epoch: 11
Loss: 0.6143452326456705
RMSE train: 0.657293	val: 0.872187	test: 0.801191
MAE train: 0.511904	val: 0.669200	test: 0.643497

Epoch: 12
Loss: 0.5797010610500971
RMSE train: 0.685030	val: 0.884811	test: 0.831507
MAE train: 0.527018	val: 0.676659	test: 0.666820

Epoch: 13
Loss: 0.5567482560873032
RMSE train: 0.685041	val: 0.888730	test: 0.835233
MAE train: 0.528595	val: 0.677267	test: 0.667896

Epoch: 14
Loss: 0.5606291244427363
RMSE train: 0.659596	val: 0.883939	test: 0.827211
MAE train: 0.505638	val: 0.673263	test: 0.659114

Epoch: 15
Loss: 0.5329244782527288
RMSE train: 0.640735	val: 0.844921	test: 0.795489
MAE train: 0.500973	val: 0.638906	test: 0.635695

Epoch: 16
Loss: 0.5256549219290415
RMSE train: 0.632127	val: 0.839879	test: 0.781009
MAE train: 0.496111	val: 0.641587	test: 0.624248

Epoch: 17
Loss: 0.5214908296863238
RMSE train: 0.633384	val: 0.865334	test: 0.792904
MAE train: 0.486412	val: 0.660444	test: 0.626434

Epoch: 18
Loss: 0.5164844766259193
RMSE train: 0.654113	val: 0.863899	test: 0.820253
MAE train: 0.505577	val: 0.656877	test: 0.660003

Epoch: 19
Loss: 0.491161751250426
RMSE train: 0.639363	val: 0.855827	test: 0.797293
MAE train: 0.490267	val: 0.651738	test: 0.636809

Epoch: 20
Loss: 0.4757863407333692
RMSE train: 0.626135	val: 0.856931	test: 0.799220
MAE train: 0.479378	val: 0.649032	test: 0.639970

Epoch: 21
Loss: 0.504749725262324
RMSE train: 0.612246	val: 0.821168	test: 0.772224
MAE train: 0.468059	val: 0.623694	test: 0.614752

Epoch: 22
Loss: 0.4545942892630895
RMSE train: 0.599080	val: 0.821989	test: 0.763266
MAE train: 0.458484	val: 0.622121	test: 0.606210Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/lipo/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/lipo/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/lipo/scaff/train_prop=0.8/lipophilicity_scaff_5_26-05_11-12-21  ]
[ Using Seed :  5  ]
[ Using device :  cuda:2  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 4.66970808165414
RMSE train: 1.850549	val: 1.836161	test: 1.935868
MAE train: 1.605466	val: 1.568019	test: 1.680256

Epoch: 2
Loss: 2.968407358442034
RMSE train: 1.423301	val: 1.443328	test: 1.478368
MAE train: 1.208041	val: 1.216602	test: 1.253210

Epoch: 3
Loss: 1.824176481791905
RMSE train: 1.101031	val: 1.155211	test: 1.148133
MAE train: 0.910038	val: 0.960587	test: 0.939280

Epoch: 4
Loss: 1.0877515120165688
RMSE train: 0.887975	val: 0.989371	test: 0.951090
MAE train: 0.716405	val: 0.787836	test: 0.771311

Epoch: 5
Loss: 0.8657935985497066
RMSE train: 0.830789	val: 0.947567	test: 0.904548
MAE train: 0.665229	val: 0.743860	test: 0.731737

Epoch: 6
Loss: 0.7945436196667808
RMSE train: 0.795989	val: 0.948189	test: 0.871938
MAE train: 0.622599	val: 0.754768	test: 0.697296

Epoch: 7
Loss: 0.7675873168877193
RMSE train: 0.762496	val: 0.888714	test: 0.839099
MAE train: 0.605679	val: 0.692874	test: 0.678801

Epoch: 8
Loss: 0.6929941986288343
RMSE train: 0.724476	val: 0.883013	test: 0.819810
MAE train: 0.563376	val: 0.696735	test: 0.647269

Epoch: 9
Loss: 0.678245987210955
RMSE train: 0.728923	val: 0.869040	test: 0.818874
MAE train: 0.577516	val: 0.677140	test: 0.660941

Epoch: 10
Loss: 0.6490653114659446
RMSE train: 0.689413	val: 0.844382	test: 0.793289
MAE train: 0.536220	val: 0.663131	test: 0.628984

Epoch: 11
Loss: 0.6101319789886475
RMSE train: 0.688364	val: 0.854904	test: 0.782813
MAE train: 0.534587	val: 0.670429	test: 0.628538

Epoch: 12
Loss: 0.6326779467718942
RMSE train: 0.679894	val: 0.825110	test: 0.773336
MAE train: 0.527550	val: 0.641396	test: 0.615366

Epoch: 13
Loss: 0.6009953490325383
RMSE train: 0.685449	val: 0.863610	test: 0.798188
MAE train: 0.526396	val: 0.676849	test: 0.629312

Epoch: 14
Loss: 0.6026756848607745
RMSE train: 0.653697	val: 0.816428	test: 0.787263
MAE train: 0.503026	val: 0.635886	test: 0.618703

Epoch: 15
Loss: 0.5898015988724572
RMSE train: 0.675502	val: 0.843060	test: 0.779243
MAE train: 0.527664	val: 0.657929	test: 0.626247

Epoch: 16
Loss: 0.5847221996103015
RMSE train: 0.658781	val: 0.815387	test: 0.778803
MAE train: 0.516442	val: 0.639703	test: 0.613509

Epoch: 17
Loss: 0.5656653642654419
RMSE train: 0.660381	val: 0.836065	test: 0.790402
MAE train: 0.513987	val: 0.644974	test: 0.628323

Epoch: 18
Loss: 0.5526575531278338
RMSE train: 0.665199	val: 0.833807	test: 0.792375
MAE train: 0.511117	val: 0.649711	test: 0.628128

Epoch: 19
Loss: 0.555504132594381
RMSE train: 0.623148	val: 0.805168	test: 0.771337
MAE train: 0.483275	val: 0.629673	test: 0.609256

Epoch: 20
Loss: 0.5141860502106803
RMSE train: 0.643824	val: 0.809714	test: 0.770301
MAE train: 0.508551	val: 0.633775	test: 0.608943

Epoch: 21
Loss: 0.5178556548697608
RMSE train: 0.634997	val: 0.814605	test: 0.762524
MAE train: 0.494216	val: 0.619499	test: 0.602403

Epoch: 22
Loss: 0.4733815980809076
RMSE train: 0.632521	val: 0.822458	test: 0.772002
MAE train: 0.491920	val: 0.629941	test: 0.618627Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/lipo/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/lipo/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/lipo/scaff/train_prop=0.8/lipophilicity_scaff_6_26-05_11-12-21  ]
[ Using Seed :  6  ]
[ Using device :  cuda:2  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 4.961528062820435
RMSE train: 2.058660	val: 2.045200	test: 2.151824
MAE train: 1.807095	val: 1.772127	test: 1.901341

Epoch: 2
Loss: 3.2046102796282088
RMSE train: 1.611642	val: 1.623468	test: 1.684853
MAE train: 1.395506	val: 1.386619	test: 1.449696

Epoch: 3
Loss: 1.914272972515651
RMSE train: 1.204505	val: 1.239641	test: 1.242352
MAE train: 1.006796	val: 1.030737	test: 1.032833

Epoch: 4
Loss: 1.1408376736300332
RMSE train: 0.864876	val: 0.977248	test: 0.940809
MAE train: 0.693965	val: 0.785737	test: 0.770738

Epoch: 5
Loss: 0.8352770039013454
RMSE train: 0.823770	val: 0.936243	test: 0.877061
MAE train: 0.656938	val: 0.737573	test: 0.711420

Epoch: 6
Loss: 0.7245765881878989
RMSE train: 0.770337	val: 0.914339	test: 0.843574
MAE train: 0.603478	val: 0.723535	test: 0.677448

Epoch: 7
Loss: 0.7183523965733392
RMSE train: 0.826281	val: 0.914595	test: 0.912518
MAE train: 0.662598	val: 0.713405	test: 0.730054

Epoch: 8
Loss: 0.7234281046049935
RMSE train: 0.782258	val: 0.927132	test: 0.852006
MAE train: 0.598987	val: 0.735043	test: 0.670820

Epoch: 9
Loss: 0.6720432043075562
RMSE train: 0.722551	val: 0.877926	test: 0.832619
MAE train: 0.568314	val: 0.684609	test: 0.663176

Epoch: 10
Loss: 0.6305561363697052
RMSE train: 0.725382	val: 0.882855	test: 0.834954
MAE train: 0.568524	val: 0.676538	test: 0.661110

Epoch: 11
Loss: 0.603194785969598
RMSE train: 0.723699	val: 0.889480	test: 0.834139
MAE train: 0.567558	val: 0.684354	test: 0.656203

Epoch: 12
Loss: 0.6100926739828927
RMSE train: 0.690928	val: 0.854825	test: 0.803182
MAE train: 0.538088	val: 0.671104	test: 0.638047

Epoch: 13
Loss: 0.5931815547602517
RMSE train: 0.663214	val: 0.826367	test: 0.788306
MAE train: 0.513503	val: 0.633426	test: 0.626827

Epoch: 14
Loss: 0.5634170983518872
RMSE train: 0.707114	val: 0.857200	test: 0.823987
MAE train: 0.559056	val: 0.652161	test: 0.657306

Epoch: 15
Loss: 0.5760704576969147
RMSE train: 0.676328	val: 0.837008	test: 0.806307
MAE train: 0.531764	val: 0.644286	test: 0.647783

Epoch: 16
Loss: 0.6024275975567954
RMSE train: 0.646586	val: 0.834526	test: 0.789264
MAE train: 0.497943	val: 0.642302	test: 0.624123

Epoch: 17
Loss: 0.5500933123486382
RMSE train: 0.655454	val: 0.824989	test: 0.775949
MAE train: 0.505789	val: 0.641416	test: 0.614990

Epoch: 18
Loss: 0.5372917758566993
RMSE train: 0.634102	val: 0.818715	test: 0.774049
MAE train: 0.489869	val: 0.627192	test: 0.610656

Epoch: 19
Loss: 0.5273264838116509
RMSE train: 0.636352	val: 0.820715	test: 0.775741
MAE train: 0.495401	val: 0.628786	test: 0.619648

Epoch: 20
Loss: 0.5260188409260341
RMSE train: 0.631612	val: 0.807762	test: 0.759077
MAE train: 0.490546	val: 0.615589	test: 0.605393

Epoch: 21
Loss: 0.5248655676841736
RMSE train: 0.616441	val: 0.809917	test: 0.780456
MAE train: 0.471779	val: 0.622217	test: 0.610950

Epoch: 22
Loss: 0.4989724372114454
RMSE train: 0.685494	val: 0.844412	test: 0.817616
MAE train: 0.547066	val: 0.652681	test: 0.658546Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/lipo/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/lipo/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/lipo/scaff/train_prop=0.8/lipophilicity_scaff_4_26-05_11-12-21  ]
[ Using Seed :  4  ]
[ Using device :  cuda:2  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.213439328329904
RMSE train: 2.123617	val: 2.124033	test: 2.202239
MAE train: 1.872545	val: 1.854246	test: 1.951032

Epoch: 2
Loss: 3.2852766513824463
RMSE train: 1.460741	val: 1.490892	test: 1.524584
MAE train: 1.247347	val: 1.260461	test: 1.302170

Epoch: 3
Loss: 2.061816232545035
RMSE train: 1.096894	val: 1.150032	test: 1.139855
MAE train: 0.914314	val: 0.950336	test: 0.944320

Epoch: 4
Loss: 1.3156479554516929
RMSE train: 0.877326	val: 0.996344	test: 0.939329
MAE train: 0.706798	val: 0.797311	test: 0.775210

Epoch: 5
Loss: 0.8637099606650216
RMSE train: 0.795022	val: 0.933731	test: 0.868055
MAE train: 0.623275	val: 0.733485	test: 0.698386

Epoch: 6
Loss: 0.7634828984737396
RMSE train: 0.790102	val: 0.939579	test: 0.851221
MAE train: 0.622639	val: 0.738963	test: 0.684521

Epoch: 7
Loss: 0.753322171313422
RMSE train: 0.754013	val: 0.930844	test: 0.849634
MAE train: 0.583587	val: 0.726941	test: 0.678638

Epoch: 8
Loss: 0.7082788348197937
RMSE train: 0.772347	val: 0.934662	test: 0.856035
MAE train: 0.598331	val: 0.733484	test: 0.667439

Epoch: 9
Loss: 0.7003020090716225
RMSE train: 0.745854	val: 0.908734	test: 0.849007
MAE train: 0.576692	val: 0.703956	test: 0.679264

Epoch: 10
Loss: 0.6738611161708832
RMSE train: 0.736888	val: 0.917014	test: 0.836094
MAE train: 0.568188	val: 0.715928	test: 0.665748

Epoch: 11
Loss: 0.6424143654959542
RMSE train: 0.706130	val: 0.869893	test: 0.804216
MAE train: 0.547530	val: 0.670333	test: 0.631864

Epoch: 12
Loss: 0.6176447996071407
RMSE train: 0.724997	val: 0.914721	test: 0.842476
MAE train: 0.555124	val: 0.711563	test: 0.670453

Epoch: 13
Loss: 0.5930103680917195
RMSE train: 0.669166	val: 0.860960	test: 0.795269
MAE train: 0.515421	val: 0.665692	test: 0.628225

Epoch: 14
Loss: 0.5911886457886014
RMSE train: 0.660460	val: 0.845889	test: 0.778684
MAE train: 0.510915	val: 0.653876	test: 0.613803

Epoch: 15
Loss: 0.573140697819846
RMSE train: 0.664768	val: 0.860454	test: 0.799384
MAE train: 0.511536	val: 0.654569	test: 0.642132

Epoch: 16
Loss: 0.5504105282681329
RMSE train: 0.678483	val: 0.864789	test: 0.819397
MAE train: 0.522628	val: 0.666712	test: 0.645720

Epoch: 17
Loss: 0.5460434662444251
RMSE train: 0.632700	val: 0.824545	test: 0.774550
MAE train: 0.488758	val: 0.633149	test: 0.610856

Epoch: 18
Loss: 0.5392445517437798
RMSE train: 0.641286	val: 0.844253	test: 0.792765
MAE train: 0.494930	val: 0.647886	test: 0.626116

Epoch: 19
Loss: 0.5326538511684963
RMSE train: 0.662252	val: 0.844874	test: 0.801829
MAE train: 0.520132	val: 0.654979	test: 0.637197

Epoch: 20
Loss: 0.5371909609862736
RMSE train: 0.638816	val: 0.836459	test: 0.778557
MAE train: 0.488879	val: 0.636724	test: 0.618719

Epoch: 21
Loss: 0.5369581537587302
RMSE train: 0.620949	val: 0.810831	test: 0.750676
MAE train: 0.480145	val: 0.628576	test: 0.591487

Epoch: 22
Loss: 0.5316251665353775
RMSE train: 0.665407	val: 0.867357	test: 0.799117
MAE train: 0.511766	val: 0.665628	test: 0.634480

Epoch: 23
Loss: 0.46123210787773133
RMSE train: 0.583297	val: 0.819095	test: 0.812461
MAE train: 0.454687	val: 0.643812	test: 0.636307

Epoch: 24
Loss: 0.4422066777944565
RMSE train: 0.565710	val: 0.812182	test: 0.808109
MAE train: 0.433980	val: 0.636476	test: 0.633353

Epoch: 25
Loss: 0.46013742983341216
RMSE train: 0.579581	val: 0.813911	test: 0.814453
MAE train: 0.453338	val: 0.641374	test: 0.638766

Epoch: 26
Loss: 0.4469259113073349
RMSE train: 0.590680	val: 0.837186	test: 0.846538
MAE train: 0.448860	val: 0.654829	test: 0.657069

Epoch: 27
Loss: 0.4421588987112045
RMSE train: 0.568283	val: 0.805525	test: 0.805870
MAE train: 0.446806	val: 0.636733	test: 0.632183

Epoch: 28
Loss: 0.4198551535606384
RMSE train: 0.542798	val: 0.806098	test: 0.813168
MAE train: 0.414768	val: 0.634610	test: 0.628225

Epoch: 29
Loss: 0.4051786482334137
RMSE train: 0.548670	val: 0.798210	test: 0.801814
MAE train: 0.422711	val: 0.627666	test: 0.619269

Epoch: 30
Loss: 0.40633671581745145
RMSE train: 0.559354	val: 0.814732	test: 0.820137
MAE train: 0.429003	val: 0.639644	test: 0.636509

Epoch: 31
Loss: 0.40690855383872987
RMSE train: 0.545597	val: 0.799336	test: 0.789683
MAE train: 0.427512	val: 0.632086	test: 0.619583

Epoch: 32
Loss: 0.39444500207901
RMSE train: 0.521490	val: 0.796250	test: 0.802787
MAE train: 0.398865	val: 0.624270	test: 0.624817

Epoch: 33
Loss: 0.40014682710170746
RMSE train: 0.523079	val: 0.802785	test: 0.794315
MAE train: 0.402342	val: 0.629243	test: 0.622836

Epoch: 34
Loss: 0.39257138073444364
RMSE train: 0.540152	val: 0.817652	test: 0.827727
MAE train: 0.413141	val: 0.640782	test: 0.642444

Epoch: 35
Loss: 0.3794461011886597
RMSE train: 0.524005	val: 0.801285	test: 0.795366
MAE train: 0.401446	val: 0.625468	test: 0.621882

Epoch: 36
Loss: 0.3734738200902939
RMSE train: 0.509726	val: 0.781177	test: 0.786234
MAE train: 0.390816	val: 0.608541	test: 0.606831

Epoch: 37
Loss: 0.37232877016067506
RMSE train: 0.520161	val: 0.787333	test: 0.791131
MAE train: 0.403020	val: 0.619891	test: 0.616683

Epoch: 38
Loss: 0.36986786425113677
RMSE train: 0.515278	val: 0.782106	test: 0.800505
MAE train: 0.395356	val: 0.611541	test: 0.622034

Epoch: 39
Loss: 0.35885947942733765
RMSE train: 0.514717	val: 0.773734	test: 0.775731
MAE train: 0.394580	val: 0.603603	test: 0.597446

Epoch: 40
Loss: 0.3876541078090668
RMSE train: 0.536747	val: 0.810980	test: 0.814756
MAE train: 0.409747	val: 0.634050	test: 0.633366

Epoch: 41
Loss: 0.3631947189569473
RMSE train: 0.496732	val: 0.772465	test: 0.788046
MAE train: 0.383989	val: 0.600584	test: 0.612663

Epoch: 42
Loss: 0.3507390469312668
RMSE train: 0.513920	val: 0.782137	test: 0.784552
MAE train: 0.397839	val: 0.610612	test: 0.614389

Epoch: 43
Loss: 0.36071676909923556
RMSE train: 0.498992	val: 0.773925	test: 0.790922
MAE train: 0.383590	val: 0.601915	test: 0.609686

Epoch: 44
Loss: 0.3464279443025589
RMSE train: 0.517156	val: 0.796733	test: 0.808922
MAE train: 0.396426	val: 0.621923	test: 0.628195

Epoch: 45
Loss: 0.3405494153499603
RMSE train: 0.490307	val: 0.796871	test: 0.803129
MAE train: 0.375006	val: 0.620889	test: 0.623049

Epoch: 46
Loss: 0.3325064152479172
RMSE train: 0.514411	val: 0.788623	test: 0.795154
MAE train: 0.395740	val: 0.614912	test: 0.620147

Epoch: 47
Loss: 0.34729219377040865
RMSE train: 0.479259	val: 0.782257	test: 0.781672
MAE train: 0.368220	val: 0.611107	test: 0.604675

Epoch: 48
Loss: 0.3293466567993164
RMSE train: 0.475997	val: 0.778293	test: 0.781832
MAE train: 0.366041	val: 0.602974	test: 0.606598

Epoch: 49
Loss: 0.31131787300109864
RMSE train: 0.471165	val: 0.765635	test: 0.772648
MAE train: 0.361223	val: 0.592487	test: 0.597594

Epoch: 50
Loss: 0.3245502144098282
RMSE train: 0.509720	val: 0.789189	test: 0.800009
MAE train: 0.390343	val: 0.616630	test: 0.622336

Epoch: 51
Loss: 0.32056869864463805
RMSE train: 0.531056	val: 0.819508	test: 0.829819
MAE train: 0.413610	val: 0.638097	test: 0.643460

Epoch: 52
Loss: 0.32027179300785064
RMSE train: 0.485901	val: 0.777204	test: 0.786513
MAE train: 0.373453	val: 0.602555	test: 0.609833

Epoch: 53
Loss: 0.31271528005599974
RMSE train: 0.486525	val: 0.777797	test: 0.794918
MAE train: 0.378536	val: 0.601578	test: 0.613906

Epoch: 54
Loss: 0.30994808971881865
RMSE train: 0.493662	val: 0.793873	test: 0.808338
MAE train: 0.377496	val: 0.615439	test: 0.623845

Epoch: 55
Loss: 0.3129975229501724
RMSE train: 0.460952	val: 0.758630	test: 0.770940
MAE train: 0.356228	val: 0.587914	test: 0.596644

Epoch: 56
Loss: 0.307103131711483
RMSE train: 0.488757	val: 0.777585	test: 0.800669
MAE train: 0.380146	val: 0.606751	test: 0.613066

Epoch: 57
Loss: 0.3075531244277954
RMSE train: 0.456205	val: 0.767767	test: 0.779673
MAE train: 0.356051	val: 0.594040	test: 0.603562

Epoch: 58
Loss: 0.295923912525177
RMSE train: 0.444898	val: 0.759360	test: 0.769632
MAE train: 0.341930	val: 0.588489	test: 0.593774

Epoch: 59
Loss: 0.2979632705450058
RMSE train: 0.440064	val: 0.760292	test: 0.772634
MAE train: 0.339768	val: 0.585759	test: 0.596734

Epoch: 60
Loss: 0.2911490470170975
RMSE train: 0.460283	val: 0.770746	test: 0.786126
MAE train: 0.354871	val: 0.594820	test: 0.610892

Epoch: 61
Loss: 0.2858947813510895
RMSE train: 0.459113	val: 0.757486	test: 0.776048
MAE train: 0.354384	val: 0.587701	test: 0.601672

Epoch: 62
Loss: 0.2871544107794762
RMSE train: 0.445677	val: 0.758277	test: 0.771447
MAE train: 0.345098	val: 0.587104	test: 0.599872

Epoch: 63
Loss: 0.29552198350429537
RMSE train: 0.464502	val: 0.786515	test: 0.798469
MAE train: 0.356060	val: 0.608460	test: 0.617499

Epoch: 64
Loss: 0.29986160695552827
RMSE train: 0.471945	val: 0.786186	test: 0.798642
MAE train: 0.365644	val: 0.603376	test: 0.619116

Epoch: 65
Loss: 0.2763613790273666
RMSE train: 0.458525	val: 0.771397	test: 0.793766
MAE train: 0.352754	val: 0.592486	test: 0.613557

Epoch: 66
Loss: 0.2872461035847664
RMSE train: 0.442992	val: 0.755024	test: 0.771486
MAE train: 0.346940	val: 0.582516	test: 0.595201

Epoch: 67
Loss: 0.2756007075309753
RMSE train: 0.442483	val: 0.772455	test: 0.811404
MAE train: 0.342911	val: 0.595472	test: 0.621074

Epoch: 68
Loss: 0.27331614047288894
RMSE train: 0.442765	val: 0.757992	test: 0.784103
MAE train: 0.342709	val: 0.584065	test: 0.603226

Epoch: 69
Loss: 0.27259976863861085
RMSE train: 0.466561	val: 0.800009	test: 0.801584
MAE train: 0.360176	val: 0.615988	test: 0.619740

Epoch: 70
Loss: 0.2882222175598145
RMSE train: 0.530936	val: 0.847541	test: 0.851014
MAE train: 0.420611	val: 0.658198	test: 0.656959

Epoch: 71
Loss: 0.2942883789539337
RMSE train: 0.457067	val: 0.772857	test: 0.797568
MAE train: 0.350947	val: 0.599102	test: 0.615205

Epoch: 72
Loss: 0.288909849524498
RMSE train: 0.444309	val: 0.776666	test: 0.799431
MAE train: 0.344641	val: 0.598455	test: 0.616751

Epoch: 73
Loss: 0.26535533368587494
RMSE train: 0.438478	val: 0.749876	test: 0.766037
MAE train: 0.343696	val: 0.583789	test: 0.589112

Epoch: 74
Loss: 0.2537445887923241
RMSE train: 0.433266	val: 0.762763	test: 0.786758
MAE train: 0.334429	val: 0.587777	test: 0.605796

Epoch: 75
Loss: 0.2656569018959999
RMSE train: 0.429575	val: 0.781351	test: 0.794328
MAE train: 0.331689	val: 0.600419	test: 0.612570

Epoch: 76
Loss: 0.26656496375799177
RMSE train: 0.427365	val: 0.780581	test: 0.808352
MAE train: 0.330427	val: 0.597243	test: 0.621324

Epoch: 77
Loss: 0.25292662978172303
RMSE train: 0.433386	val: 0.764539	test: 0.772702
MAE train: 0.335980	val: 0.587196	test: 0.594064

Epoch: 78
Loss: 0.2463630110025406
RMSE train: 0.477632	val: 0.811707	test: 0.835890
MAE train: 0.372290	val: 0.628488	test: 0.644029

Epoch: 79
Loss: 0.2519776657223701
RMSE train: 0.415561	val: 0.751748	test: 0.772658
MAE train: 0.322765	val: 0.579574	test: 0.594208

Epoch: 80
Loss: 0.25709793120622637
RMSE train: 0.419223	val: 0.749289	test: 0.772929
MAE train: 0.325548	val: 0.583339	test: 0.591133

Epoch: 81
Loss: 0.25311117619276047
RMSE train: 0.429317	val: 0.767589	test: 0.792332
MAE train: 0.330332	val: 0.590574	test: 0.604318

Epoch: 82
Loss: 0.25983188301324844
RMSE train: 0.450030	val: 0.786871	test: 0.811295
MAE train: 0.347970	val: 0.603153	test: 0.625662

Epoch: 83
Loss: 0.2677832692861557
RMSE train: 0.443817	val: 0.774228	test: 0.795410

Epoch: 23
Loss: 0.45406111478805544
RMSE train: 0.570701	val: 0.831316	test: 0.826334
MAE train: 0.439197	val: 0.646492	test: 0.648391

Epoch: 24
Loss: 0.44392813742160797
RMSE train: 0.552233	val: 0.822500	test: 0.822446
MAE train: 0.429785	val: 0.640659	test: 0.641533

Epoch: 25
Loss: 0.43243431448936465
RMSE train: 0.573289	val: 0.821736	test: 0.819991
MAE train: 0.442591	val: 0.640313	test: 0.645722

Epoch: 26
Loss: 0.43008328080177305
RMSE train: 0.581998	val: 0.845708	test: 0.852694
MAE train: 0.446048	val: 0.659954	test: 0.667076

Epoch: 27
Loss: 0.4065162390470505
RMSE train: 0.573762	val: 0.843330	test: 0.848493
MAE train: 0.444659	val: 0.659631	test: 0.665112

Epoch: 28
Loss: 0.4126752853393555
RMSE train: 0.546685	val: 0.812829	test: 0.814033
MAE train: 0.422011	val: 0.630566	test: 0.635659

Epoch: 29
Loss: 0.38871960937976835
RMSE train: 0.565922	val: 0.833747	test: 0.843887
MAE train: 0.435172	val: 0.647670	test: 0.659245

Epoch: 30
Loss: 0.3966869503259659
RMSE train: 0.549447	val: 0.820808	test: 0.835749
MAE train: 0.423353	val: 0.639744	test: 0.651497

Epoch: 31
Loss: 0.3870103269815445
RMSE train: 0.536637	val: 0.816226	test: 0.820629
MAE train: 0.412235	val: 0.636946	test: 0.641451

Epoch: 32
Loss: 0.38830864131450654
RMSE train: 0.541794	val: 0.815460	test: 0.829330
MAE train: 0.418147	val: 0.631879	test: 0.648159

Epoch: 33
Loss: 0.3755639523267746
RMSE train: 0.526451	val: 0.794625	test: 0.794466
MAE train: 0.408055	val: 0.616030	test: 0.619401

Epoch: 34
Loss: 0.3797171711921692
RMSE train: 0.543123	val: 0.822690	test: 0.836549
MAE train: 0.419869	val: 0.640656	test: 0.653787

Epoch: 35
Loss: 0.3880550265312195
RMSE train: 0.524175	val: 0.811269	test: 0.814952
MAE train: 0.400461	val: 0.626871	test: 0.632971

Epoch: 36
Loss: 0.3705388009548187
RMSE train: 0.532290	val: 0.818849	test: 0.827101
MAE train: 0.409960	val: 0.635395	test: 0.642831

Epoch: 37
Loss: 0.38538967072963715
RMSE train: 0.527283	val: 0.818199	test: 0.816234
MAE train: 0.405818	val: 0.634229	test: 0.632822

Epoch: 38
Loss: 0.37407334744930265
RMSE train: 0.517033	val: 0.807607	test: 0.815193
MAE train: 0.398329	val: 0.627460	test: 0.633471

Epoch: 39
Loss: 0.37080115973949435
RMSE train: 0.527435	val: 0.789478	test: 0.791800
MAE train: 0.410092	val: 0.609736	test: 0.618252

Epoch: 40
Loss: 0.36585718095302583
RMSE train: 0.495150	val: 0.800602	test: 0.804696
MAE train: 0.381793	val: 0.622425	test: 0.615750

Epoch: 41
Loss: 0.36003845036029813
RMSE train: 0.506745	val: 0.801311	test: 0.807581
MAE train: 0.391404	val: 0.624816	test: 0.621718

Epoch: 42
Loss: 0.361048749089241
RMSE train: 0.557305	val: 0.811367	test: 0.823331
MAE train: 0.435582	val: 0.625248	test: 0.638301

Epoch: 43
Loss: 0.3305919229984283
RMSE train: 0.515315	val: 0.793160	test: 0.810074
MAE train: 0.397106	val: 0.618682	test: 0.623792

Epoch: 44
Loss: 0.34582155346870425
RMSE train: 0.509448	val: 0.794053	test: 0.808383
MAE train: 0.390905	val: 0.614850	test: 0.626497

Epoch: 45
Loss: 0.34296861588954924
RMSE train: 0.507564	val: 0.780944	test: 0.789840
MAE train: 0.391268	val: 0.601938	test: 0.617752

Epoch: 46
Loss: 0.32146632969379424
RMSE train: 0.513296	val: 0.800220	test: 0.808067
MAE train: 0.396092	val: 0.618163	test: 0.623766

Epoch: 47
Loss: 0.3267853230237961
RMSE train: 0.499542	val: 0.800789	test: 0.814904
MAE train: 0.384987	val: 0.622252	test: 0.623153

Epoch: 48
Loss: 0.3222327560186386
RMSE train: 0.508337	val: 0.807429	test: 0.820793
MAE train: 0.392544	val: 0.624744	test: 0.636811

Epoch: 49
Loss: 0.3459725856781006
RMSE train: 0.538054	val: 0.813395	test: 0.822657
MAE train: 0.415396	val: 0.631206	test: 0.642281

Epoch: 50
Loss: 0.34708699882030486
RMSE train: 0.522827	val: 0.811928	test: 0.828194
MAE train: 0.409420	val: 0.634677	test: 0.639019

Epoch: 51
Loss: 0.3350958704948425
RMSE train: 0.482219	val: 0.780765	test: 0.788404
MAE train: 0.371347	val: 0.599442	test: 0.610223

Epoch: 52
Loss: 0.3113515853881836
RMSE train: 0.478585	val: 0.778007	test: 0.793896
MAE train: 0.366599	val: 0.600881	test: 0.610823

Epoch: 53
Loss: 0.3092816710472107
RMSE train: 0.531794	val: 0.818233	test: 0.823838
MAE train: 0.410236	val: 0.631291	test: 0.639853

Epoch: 54
Loss: 0.3000090628862381
RMSE train: 0.498440	val: 0.794690	test: 0.800827
MAE train: 0.383308	val: 0.609182	test: 0.620031

Epoch: 55
Loss: 0.30118032097816466
RMSE train: 0.470720	val: 0.776220	test: 0.781225
MAE train: 0.363923	val: 0.594682	test: 0.599207

Epoch: 56
Loss: 0.31582584977149963
RMSE train: 0.500475	val: 0.792419	test: 0.803439
MAE train: 0.387036	val: 0.610813	test: 0.620085

Epoch: 57
Loss: 0.29953897893428805
RMSE train: 0.466724	val: 0.779384	test: 0.788657
MAE train: 0.363502	val: 0.595242	test: 0.608872

Epoch: 58
Loss: 0.2934259444475174
RMSE train: 0.456000	val: 0.782139	test: 0.794078
MAE train: 0.351842	val: 0.600556	test: 0.609530

Epoch: 59
Loss: 0.2963113456964493
RMSE train: 0.478831	val: 0.784536	test: 0.797614
MAE train: 0.371767	val: 0.602471	test: 0.612873

Epoch: 60
Loss: 0.2931006610393524
RMSE train: 0.485399	val: 0.803651	test: 0.816532
MAE train: 0.374316	val: 0.614116	test: 0.628679

Epoch: 61
Loss: 0.291341258585453
RMSE train: 0.460932	val: 0.782921	test: 0.790062
MAE train: 0.357178	val: 0.599262	test: 0.611993

Epoch: 62
Loss: 0.2898167252540588
RMSE train: 0.472935	val: 0.777196	test: 0.790480
MAE train: 0.367712	val: 0.597758	test: 0.612013

Epoch: 63
Loss: 0.28674942851066587
RMSE train: 0.481641	val: 0.787262	test: 0.812042
MAE train: 0.372741	val: 0.607791	test: 0.626832

Epoch: 64
Loss: 0.27604722827672956
RMSE train: 0.461978	val: 0.780342	test: 0.798039
MAE train: 0.358374	val: 0.592706	test: 0.610164

Epoch: 65
Loss: 0.28917077779769895
RMSE train: 0.446378	val: 0.767639	test: 0.791200
MAE train: 0.344619	val: 0.587446	test: 0.604622

Epoch: 66
Loss: 0.2885532110929489
RMSE train: 0.470904	val: 0.804402	test: 0.819210
MAE train: 0.363727	val: 0.616974	test: 0.633164

Epoch: 67
Loss: 0.2880010724067688
RMSE train: 0.451474	val: 0.783722	test: 0.804586
MAE train: 0.350000	val: 0.597227	test: 0.620027

Epoch: 68
Loss: 0.27086516171693803
RMSE train: 0.445304	val: 0.777925	test: 0.797668
MAE train: 0.344165	val: 0.594798	test: 0.614406

Epoch: 69
Loss: 0.2781321182847023
RMSE train: 0.451196	val: 0.776931	test: 0.795697
MAE train: 0.350305	val: 0.592508	test: 0.611947

Epoch: 70
Loss: 0.26161777675151826
RMSE train: 0.452423	val: 0.783804	test: 0.795028
MAE train: 0.350551	val: 0.597078	test: 0.610994

Epoch: 71
Loss: 0.27320370972156527
RMSE train: 0.454014	val: 0.786023	test: 0.804573
MAE train: 0.351156	val: 0.596801	test: 0.620137

Epoch: 72
Loss: 0.26924079954624175
RMSE train: 0.460101	val: 0.784967	test: 0.793772
MAE train: 0.354508	val: 0.600861	test: 0.612513

Epoch: 73
Loss: 0.27681634575128555
RMSE train: 0.433534	val: 0.769239	test: 0.796820
MAE train: 0.336161	val: 0.586154	test: 0.605181

Epoch: 74
Loss: 0.2807556360960007
RMSE train: 0.439859	val: 0.775481	test: 0.800593
MAE train: 0.337736	val: 0.589937	test: 0.612424

Epoch: 75
Loss: 0.26991300135850904
RMSE train: 0.456060	val: 0.777876	test: 0.788222
MAE train: 0.352399	val: 0.595960	test: 0.609648

Epoch: 76
Loss: 0.2745367556810379
RMSE train: 0.489533	val: 0.805205	test: 0.805493
MAE train: 0.381402	val: 0.620842	test: 0.624291

Epoch: 77
Loss: 0.2691815748810768
RMSE train: 0.442334	val: 0.786923	test: 0.793541
MAE train: 0.343795	val: 0.601589	test: 0.609826

Epoch: 78
Loss: 0.2641872316598892
RMSE train: 0.439601	val: 0.760343	test: 0.774935
MAE train: 0.340950	val: 0.577288	test: 0.600536

Epoch: 79
Loss: 0.2536940947175026
RMSE train: 0.430464	val: 0.765199	test: 0.792066
MAE train: 0.332799	val: 0.584978	test: 0.611881

Epoch: 80
Loss: 0.2681887000799179
RMSE train: 0.453858	val: 0.786246	test: 0.817083
MAE train: 0.350091	val: 0.597896	test: 0.628501

Epoch: 81
Loss: 0.25215292125940325
RMSE train: 0.482436	val: 0.800687	test: 0.847517
MAE train: 0.379704	val: 0.614304	test: 0.644949

Epoch: 82
Loss: 0.2597583144903183
RMSE train: 0.439402	val: 0.774638	test: 0.805824
MAE train: 0.339831	val: 0.591206	test: 0.623196

Epoch: 83
Loss: 0.25015884041786196
RMSE train: 0.430668	val: 0.764530	test: 0.800397

Epoch: 23
Loss: 0.4415198028087616
RMSE train: 0.594397	val: 0.837226	test: 0.828504
MAE train: 0.456081	val: 0.658125	test: 0.647511

Epoch: 24
Loss: 0.44861386716365814
RMSE train: 0.590191	val: 0.839846	test: 0.829440
MAE train: 0.453275	val: 0.663748	test: 0.655738

Epoch: 25
Loss: 0.44064399003982546
RMSE train: 0.563012	val: 0.809948	test: 0.801566
MAE train: 0.434602	val: 0.632775	test: 0.627214

Epoch: 26
Loss: 0.44688202142715455
RMSE train: 0.570707	val: 0.814885	test: 0.802242
MAE train: 0.436338	val: 0.637368	test: 0.628065

Epoch: 27
Loss: 0.4222083270549774
RMSE train: 0.601584	val: 0.817097	test: 0.803964
MAE train: 0.466393	val: 0.636550	test: 0.630072

Epoch: 28
Loss: 0.4035903215408325
RMSE train: 0.573624	val: 0.826899	test: 0.824301
MAE train: 0.442568	val: 0.654896	test: 0.648151

Epoch: 29
Loss: 0.4140878677368164
RMSE train: 0.564107	val: 0.799888	test: 0.786171
MAE train: 0.440211	val: 0.624642	test: 0.612314

Epoch: 30
Loss: 0.41355507671833036
RMSE train: 0.550477	val: 0.812560	test: 0.800858
MAE train: 0.419308	val: 0.634669	test: 0.625597

Epoch: 31
Loss: 0.3922267407178879
RMSE train: 0.543428	val: 0.798493	test: 0.786897
MAE train: 0.416116	val: 0.621967	test: 0.615981

Epoch: 32
Loss: 0.3759757488965988
RMSE train: 0.536594	val: 0.797162	test: 0.787623
MAE train: 0.411489	val: 0.622804	test: 0.616271

Epoch: 33
Loss: 0.38573464155197146
RMSE train: 0.535179	val: 0.794260	test: 0.796008
MAE train: 0.410398	val: 0.623435	test: 0.618942

Epoch: 34
Loss: 0.40914264917373655
RMSE train: 0.527157	val: 0.783849	test: 0.794767
MAE train: 0.402211	val: 0.609635	test: 0.616339

Epoch: 35
Loss: 0.3757743418216705
RMSE train: 0.536244	val: 0.804759	test: 0.798641
MAE train: 0.412557	val: 0.626151	test: 0.627076

Epoch: 36
Loss: 0.375073105096817
RMSE train: 0.526516	val: 0.790283	test: 0.794280
MAE train: 0.406476	val: 0.618468	test: 0.615378

Epoch: 37
Loss: 0.372900253534317
RMSE train: 0.528566	val: 0.814786	test: 0.809879
MAE train: 0.406550	val: 0.634206	test: 0.632762

Epoch: 38
Loss: 0.3740288823843002
RMSE train: 0.506900	val: 0.780579	test: 0.774358
MAE train: 0.390798	val: 0.604033	test: 0.601651

Epoch: 39
Loss: 0.3640732914209366
RMSE train: 0.513959	val: 0.779500	test: 0.773135
MAE train: 0.397700	val: 0.605704	test: 0.597244

Epoch: 40
Loss: 0.3726985096931458
RMSE train: 0.529658	val: 0.805697	test: 0.800137
MAE train: 0.404510	val: 0.626703	test: 0.623600

Epoch: 41
Loss: 0.35846564173698425
RMSE train: 0.507063	val: 0.790106	test: 0.787726
MAE train: 0.390043	val: 0.617409	test: 0.614047

Epoch: 42
Loss: 0.34730406403541564
RMSE train: 0.517936	val: 0.796447	test: 0.787496
MAE train: 0.397469	val: 0.622178	test: 0.613654

Epoch: 43
Loss: 0.36198737621307375
RMSE train: 0.521543	val: 0.793285	test: 0.779301
MAE train: 0.403471	val: 0.613910	test: 0.613148

Epoch: 44
Loss: 0.3488939493894577
RMSE train: 0.517922	val: 0.788262	test: 0.792914
MAE train: 0.397485	val: 0.615944	test: 0.617520

Epoch: 45
Loss: 0.3308299481868744
RMSE train: 0.512742	val: 0.800373	test: 0.795368
MAE train: 0.391006	val: 0.621179	test: 0.621954

Epoch: 46
Loss: 0.3334756135940552
RMSE train: 0.491607	val: 0.780681	test: 0.769009
MAE train: 0.380033	val: 0.603917	test: 0.595824

Epoch: 47
Loss: 0.32375945150852203
RMSE train: 0.484697	val: 0.767037	test: 0.758954
MAE train: 0.374533	val: 0.593670	test: 0.586381

Epoch: 48
Loss: 0.33990040719509124
RMSE train: 0.498770	val: 0.793011	test: 0.781019
MAE train: 0.381625	val: 0.611374	test: 0.609629

Epoch: 49
Loss: 0.3322079211473465
RMSE train: 0.534027	val: 0.822471	test: 0.816831
MAE train: 0.414018	val: 0.639224	test: 0.641341

Epoch: 50
Loss: 0.3222871035337448
RMSE train: 0.483872	val: 0.780183	test: 0.766304
MAE train: 0.370287	val: 0.601203	test: 0.591752

Epoch: 51
Loss: 0.327117383480072
RMSE train: 0.481185	val: 0.787895	test: 0.783333
MAE train: 0.368352	val: 0.609701	test: 0.604549

Epoch: 52
Loss: 0.31751068234443663
RMSE train: 0.473554	val: 0.768929	test: 0.758446
MAE train: 0.363887	val: 0.593578	test: 0.586080

Epoch: 53
Loss: 0.3302150160074234
RMSE train: 0.496021	val: 0.777211	test: 0.767871
MAE train: 0.384788	val: 0.601941	test: 0.596352

Epoch: 54
Loss: 0.32398752868175507
RMSE train: 0.489128	val: 0.788268	test: 0.787849
MAE train: 0.377068	val: 0.608020	test: 0.610134

Epoch: 55
Loss: 0.3210839360952377
RMSE train: 0.509946	val: 0.795835	test: 0.799559
MAE train: 0.395981	val: 0.615973	test: 0.621536

Epoch: 56
Loss: 0.32587758004665374
RMSE train: 0.485932	val: 0.782127	test: 0.771517
MAE train: 0.380797	val: 0.603039	test: 0.599111

Epoch: 57
Loss: 0.32277967035770416
RMSE train: 0.479898	val: 0.766292	test: 0.768622
MAE train: 0.370100	val: 0.590681	test: 0.594246

Epoch: 58
Loss: 0.30615500509738924
RMSE train: 0.494105	val: 0.785734	test: 0.769615
MAE train: 0.381603	val: 0.604344	test: 0.608504

Epoch: 59
Loss: 0.3076344966888428
RMSE train: 0.481591	val: 0.791564	test: 0.796960
MAE train: 0.369527	val: 0.614542	test: 0.618613

Epoch: 60
Loss: 0.2919746577739716
RMSE train: 0.468822	val: 0.768777	test: 0.776424
MAE train: 0.358205	val: 0.589874	test: 0.600631

Epoch: 61
Loss: 0.2967248857021332
RMSE train: 0.478673	val: 0.772740	test: 0.785196
MAE train: 0.370117	val: 0.596449	test: 0.609458

Epoch: 62
Loss: 0.31579182744026185
RMSE train: 0.457761	val: 0.776092	test: 0.779555
MAE train: 0.349502	val: 0.596535	test: 0.603902

Epoch: 63
Loss: 0.30390607863664626
RMSE train: 0.459392	val: 0.766338	test: 0.759386
MAE train: 0.355010	val: 0.590772	test: 0.586111

Epoch: 64
Loss: 0.27987148612737656
RMSE train: 0.472315	val: 0.789768	test: 0.789468
MAE train: 0.366347	val: 0.606306	test: 0.613454

Epoch: 65
Loss: 0.28303690254688263
RMSE train: 0.465863	val: 0.758595	test: 0.764227
MAE train: 0.359122	val: 0.582729	test: 0.596645

Epoch: 66
Loss: 0.2850467681884766
RMSE train: 0.448471	val: 0.752172	test: 0.762894
MAE train: 0.343473	val: 0.576867	test: 0.585791

Epoch: 67
Loss: 0.2861206248402596
RMSE train: 0.450964	val: 0.768920	test: 0.774719
MAE train: 0.346241	val: 0.590690	test: 0.597724

Epoch: 68
Loss: 0.2731113716959953
RMSE train: 0.458723	val: 0.790918	test: 0.802982
MAE train: 0.352566	val: 0.613080	test: 0.613194

Epoch: 69
Loss: 0.2721890062093735
RMSE train: 0.451103	val: 0.758913	test: 0.766369
MAE train: 0.346677	val: 0.582661	test: 0.589762

Epoch: 70
Loss: 0.27375776022672654
RMSE train: 0.447582	val: 0.769862	test: 0.775474
MAE train: 0.344458	val: 0.591875	test: 0.601237

Epoch: 71
Loss: 0.2757116496562958
RMSE train: 0.471037	val: 0.789960	test: 0.791204
MAE train: 0.363850	val: 0.612604	test: 0.610408

Epoch: 72
Loss: 0.2717192888259888
RMSE train: 0.472834	val: 0.779685	test: 0.787489
MAE train: 0.369922	val: 0.599416	test: 0.608072

Epoch: 73
Loss: 0.2648400396108627
RMSE train: 0.441516	val: 0.758170	test: 0.773773
MAE train: 0.338195	val: 0.581425	test: 0.596577

Epoch: 74
Loss: 0.2696864947676659
RMSE train: 0.442054	val: 0.760422	test: 0.762133
MAE train: 0.340921	val: 0.581893	test: 0.587593

Epoch: 75
Loss: 0.26248677223920824
RMSE train: 0.465525	val: 0.799268	test: 0.799572
MAE train: 0.359473	val: 0.613645	test: 0.615858

Epoch: 76
Loss: 0.2670455202460289
RMSE train: 0.435918	val: 0.753356	test: 0.764468
MAE train: 0.336723	val: 0.573841	test: 0.588994

Epoch: 77
Loss: 0.26769410222768786
RMSE train: 0.435974	val: 0.762273	test: 0.770599
MAE train: 0.335713	val: 0.584489	test: 0.592535

Epoch: 78
Loss: 0.2635540559887886
RMSE train: 0.446786	val: 0.758317	test: 0.758893
MAE train: 0.344710	val: 0.582617	test: 0.585879

Epoch: 79
Loss: 0.247018663585186
RMSE train: 0.431393	val: 0.754719	test: 0.764720
MAE train: 0.331191	val: 0.574767	test: 0.593335

Epoch: 80
Loss: 0.24749577641487122
RMSE train: 0.433740	val: 0.762782	test: 0.761165
MAE train: 0.333829	val: 0.582521	test: 0.582784

Epoch: 81
Loss: 0.2623501941561699
RMSE train: 0.477847	val: 0.807299	test: 0.811913
MAE train: 0.371624	val: 0.623016	test: 0.626663

Epoch: 82
Loss: 0.2645856812596321
RMSE train: 0.433137	val: 0.756340	test: 0.769035
MAE train: 0.333017	val: 0.579199	test: 0.589206

Epoch: 83
Loss: 0.273114812374115
RMSE train: 0.438235	val: 0.756443	test: 0.763108

Epoch: 23
Loss: 0.45842286696036655
RMSE train: 0.604217	val: 0.831692	test: 0.793056
MAE train: 0.464295	val: 0.634702	test: 0.629331

Epoch: 24
Loss: 0.4385247478882472
RMSE train: 0.608598	val: 0.816630	test: 0.769197
MAE train: 0.477440	val: 0.625984	test: 0.613019

Epoch: 25
Loss: 0.46120910346508026
RMSE train: 0.591631	val: 0.822928	test: 0.781775
MAE train: 0.453794	val: 0.626649	test: 0.620767

Epoch: 26
Loss: 0.44463353604078293
RMSE train: 0.567149	val: 0.810286	test: 0.765788
MAE train: 0.433497	val: 0.615323	test: 0.609179

Epoch: 27
Loss: 0.4418762947122256
RMSE train: 0.578162	val: 0.812319	test: 0.778709
MAE train: 0.444073	val: 0.623756	test: 0.617457

Epoch: 28
Loss: 0.42688294500112534
RMSE train: 0.558569	val: 0.801405	test: 0.766116
MAE train: 0.428354	val: 0.611147	test: 0.605247

Epoch: 29
Loss: 0.44716708610455197
RMSE train: 0.565147	val: 0.810596	test: 0.765486
MAE train: 0.435671	val: 0.614371	test: 0.609841

Epoch: 30
Loss: 0.4101618031660716
RMSE train: 0.561727	val: 0.795118	test: 0.751965
MAE train: 0.435409	val: 0.605066	test: 0.594062

Epoch: 31
Loss: 0.4197922373811404
RMSE train: 0.547729	val: 0.786457	test: 0.761917
MAE train: 0.422982	val: 0.598418	test: 0.601312

Epoch: 32
Loss: 0.40964794903993607
RMSE train: 0.534931	val: 0.782486	test: 0.749180
MAE train: 0.411273	val: 0.601568	test: 0.592683

Epoch: 33
Loss: 0.3904424508412679
RMSE train: 0.565006	val: 0.815529	test: 0.783332
MAE train: 0.435455	val: 0.621911	test: 0.615040

Epoch: 34
Loss: 0.40308210998773575
RMSE train: 0.587461	val: 0.833550	test: 0.802841
MAE train: 0.454298	val: 0.632145	test: 0.638333

Epoch: 35
Loss: 0.399110771715641
RMSE train: 0.526392	val: 0.790626	test: 0.745336
MAE train: 0.405546	val: 0.596335	test: 0.582619

Epoch: 36
Loss: 0.4035155971844991
RMSE train: 0.564890	val: 0.794938	test: 0.767436
MAE train: 0.435577	val: 0.607555	test: 0.609977

Epoch: 37
Loss: 0.39897557348012924
RMSE train: 0.540218	val: 0.781637	test: 0.743954
MAE train: 0.415603	val: 0.590353	test: 0.587646

Epoch: 38
Loss: 0.3728700553377469
RMSE train: 0.549449	val: 0.809484	test: 0.785381
MAE train: 0.422693	val: 0.610700	test: 0.621555

Epoch: 39
Loss: 0.3752124384045601
RMSE train: 0.523362	val: 0.773270	test: 0.741918
MAE train: 0.403819	val: 0.581650	test: 0.584402

Epoch: 40
Loss: 0.37823618451754254
RMSE train: 0.547892	val: 0.808829	test: 0.779279
MAE train: 0.423013	val: 0.614526	test: 0.612393

Epoch: 41
Loss: 0.3551618879040082
RMSE train: 0.529989	val: 0.779915	test: 0.737765
MAE train: 0.407799	val: 0.590328	test: 0.586939

Epoch: 42
Loss: 0.35083094735940296
RMSE train: 0.521727	val: 0.774903	test: 0.742871
MAE train: 0.406119	val: 0.588841	test: 0.579958

Epoch: 43
Loss: 0.3615297848979632
RMSE train: 0.526663	val: 0.781514	test: 0.761288
MAE train: 0.404703	val: 0.591305	test: 0.594924

Epoch: 44
Loss: 0.35873966415723163
RMSE train: 0.566905	val: 0.810104	test: 0.798623
MAE train: 0.443787	val: 0.614799	test: 0.634988

Epoch: 45
Loss: 0.3620410238703092
RMSE train: 0.505740	val: 0.766769	test: 0.736889
MAE train: 0.388723	val: 0.579661	test: 0.583332

Epoch: 46
Loss: 0.3167949716250102
RMSE train: 0.524675	val: 0.777184	test: 0.753811
MAE train: 0.405699	val: 0.584926	test: 0.596795

Epoch: 47
Loss: 0.3474648768703143
RMSE train: 0.512403	val: 0.783426	test: 0.759168
MAE train: 0.394659	val: 0.589252	test: 0.599844

Epoch: 48
Loss: 0.34900111705064774
RMSE train: 0.507856	val: 0.773399	test: 0.745290
MAE train: 0.393038	val: 0.581730	test: 0.588670

Epoch: 49
Loss: 0.3525751282771428
RMSE train: 0.540818	val: 0.803570	test: 0.795533
MAE train: 0.415925	val: 0.611140	test: 0.626296

Epoch: 50
Loss: 0.3508578489224116
RMSE train: 0.510644	val: 0.778846	test: 0.751396
MAE train: 0.392617	val: 0.594378	test: 0.596014

Epoch: 51
Loss: 0.34978557378053665
RMSE train: 0.490586	val: 0.757681	test: 0.733152
MAE train: 0.379654	val: 0.569712	test: 0.576226

Epoch: 52
Loss: 0.33269810924927395
RMSE train: 0.497735	val: 0.771559	test: 0.749494
MAE train: 0.385071	val: 0.582961	test: 0.588128

Epoch: 53
Loss: 0.33023855090141296
RMSE train: 0.521104	val: 0.803323	test: 0.786204
MAE train: 0.402922	val: 0.610752	test: 0.624438

Epoch: 54
Loss: 0.3279755314191182
RMSE train: 0.496504	val: 0.779871	test: 0.748604
MAE train: 0.382700	val: 0.587817	test: 0.585627

Epoch: 55
Loss: 0.3338044161597888
RMSE train: 0.486018	val: 0.779139	test: 0.768664
MAE train: 0.373705	val: 0.583485	test: 0.606470

Epoch: 56
Loss: 0.3129553844531377
RMSE train: 0.500964	val: 0.778763	test: 0.741668
MAE train: 0.386576	val: 0.584658	test: 0.584656

Epoch: 57
Loss: 0.32007253418366116
RMSE train: 0.531353	val: 0.809145	test: 0.801351
MAE train: 0.413468	val: 0.616795	test: 0.634341

Epoch: 58
Loss: 0.3208683244884014
RMSE train: 0.494900	val: 0.785046	test: 0.754286
MAE train: 0.381410	val: 0.592733	test: 0.597770

Epoch: 59
Loss: 0.33356283853451413
RMSE train: 0.507659	val: 0.790478	test: 0.771162
MAE train: 0.393006	val: 0.602176	test: 0.606517

Epoch: 60
Loss: 0.3116937403877576
RMSE train: 0.482716	val: 0.761728	test: 0.723246
MAE train: 0.373551	val: 0.575354	test: 0.564007

Epoch: 61
Loss: 0.315743384261926
RMSE train: 0.497312	val: 0.784334	test: 0.779271
MAE train: 0.385638	val: 0.592632	test: 0.606304

Epoch: 62
Loss: 0.2884012038509051
RMSE train: 0.487742	val: 0.767515	test: 0.741729
MAE train: 0.376557	val: 0.576433	test: 0.579657

Epoch: 63
Loss: 0.3075319627920787
RMSE train: 0.466777	val: 0.776602	test: 0.759369
MAE train: 0.358583	val: 0.582660	test: 0.594624

Epoch: 64
Loss: 0.30945127333203953
RMSE train: 0.512278	val: 0.793516	test: 0.767547
MAE train: 0.398499	val: 0.599836	test: 0.604664

Epoch: 65
Loss: 0.31238608062267303
RMSE train: 0.496161	val: 0.799609	test: 0.786197
MAE train: 0.387601	val: 0.602149	test: 0.617552

Epoch: 66
Loss: 0.296360045671463
RMSE train: 0.490891	val: 0.806407	test: 0.786560
MAE train: 0.377056	val: 0.608520	test: 0.609492

Epoch: 67
Loss: 0.29273315891623497
RMSE train: 0.463874	val: 0.772700	test: 0.748417
MAE train: 0.357751	val: 0.585933	test: 0.586179

Epoch: 68
Loss: 0.304043710231781
RMSE train: 0.479020	val: 0.782335	test: 0.747280
MAE train: 0.368314	val: 0.591783	test: 0.588781

Epoch: 69
Loss: 0.2916353816787402
RMSE train: 0.490958	val: 0.785788	test: 0.766552
MAE train: 0.380398	val: 0.590245	test: 0.606149

Epoch: 70
Loss: 0.2862275342146556
RMSE train: 0.474639	val: 0.776932	test: 0.760308
MAE train: 0.367990	val: 0.582553	test: 0.597440

Epoch: 71
Loss: 0.2867959936459859
RMSE train: 0.479849	val: 0.791528	test: 0.766951
MAE train: 0.373256	val: 0.593508	test: 0.605928

Epoch: 72
Loss: 0.27101163441936177
RMSE train: 0.463948	val: 0.773192	test: 0.739048
MAE train: 0.357532	val: 0.573653	test: 0.579004

Epoch: 73
Loss: 0.28592559322714806
RMSE train: 0.464429	val: 0.775074	test: 0.756836
MAE train: 0.356065	val: 0.581963	test: 0.594469

Epoch: 74
Loss: 0.28284304464856785
RMSE train: 0.460820	val: 0.758804	test: 0.730661
MAE train: 0.354142	val: 0.571164	test: 0.574297

Epoch: 75
Loss: 0.2800121617813905
RMSE train: 0.467101	val: 0.777639	test: 0.743103
MAE train: 0.361364	val: 0.585351	test: 0.582887

Epoch: 76
Loss: 0.2858235115806262
RMSE train: 0.457403	val: 0.761184	test: 0.726316
MAE train: 0.351635	val: 0.574711	test: 0.572670

Epoch: 77
Loss: 0.2844851687550545
RMSE train: 0.452118	val: 0.763878	test: 0.719024
MAE train: 0.349795	val: 0.570381	test: 0.563424

Epoch: 78
Loss: 0.27295641601085663
RMSE train: 0.454708	val: 0.757678	test: 0.732965
MAE train: 0.351349	val: 0.573200	test: 0.575102

Epoch: 79
Loss: 0.26868120456735295
RMSE train: 0.468048	val: 0.771972	test: 0.748387
MAE train: 0.359293	val: 0.584954	test: 0.589956

Epoch: 80
Loss: 0.26167791709303856
RMSE train: 0.453664	val: 0.784992	test: 0.759438
MAE train: 0.348944	val: 0.588320	test: 0.589217

Epoch: 81
Loss: 0.2700783237814903
RMSE train: 0.478285	val: 0.799411	test: 0.776892
MAE train: 0.373519	val: 0.601399	test: 0.610360

Epoch: 82
Loss: 0.2670620170732339
RMSE train: 0.437675	val: 0.753405	test: 0.720087
MAE train: 0.336869	val: 0.559684	test: 0.559829

Epoch: 83
Loss: 0.2695978631575902
RMSE train: 0.455973	val: 0.780713	test: 0.754705

Epoch: 23
Loss: 0.46548551321029663
RMSE train: 0.585280	val: 0.819758	test: 0.778752
MAE train: 0.455471	val: 0.621850	test: 0.606770

Epoch: 24
Loss: 0.44399918367465335
RMSE train: 0.611699	val: 0.842853	test: 0.802019
MAE train: 0.467415	val: 0.644789	test: 0.631893

Epoch: 25
Loss: 0.4585177078843117
RMSE train: 0.583802	val: 0.805553	test: 0.769836
MAE train: 0.452095	val: 0.615036	test: 0.605891

Epoch: 26
Loss: 0.4399735952417056
RMSE train: 0.559215	val: 0.802783	test: 0.753342
MAE train: 0.431332	val: 0.612948	test: 0.587075

Epoch: 27
Loss: 0.4304307500521342
RMSE train: 0.576357	val: 0.813047	test: 0.767394
MAE train: 0.442619	val: 0.621430	test: 0.599138

Epoch: 28
Loss: 0.4395804877082507
RMSE train: 0.566673	val: 0.809951	test: 0.766036
MAE train: 0.434954	val: 0.618176	test: 0.605892

Epoch: 29
Loss: 0.42705265680948895
RMSE train: 0.584212	val: 0.828881	test: 0.788129
MAE train: 0.447841	val: 0.624884	test: 0.615505

Epoch: 30
Loss: 0.4476172998547554
RMSE train: 0.571116	val: 0.820012	test: 0.780481
MAE train: 0.439101	val: 0.632765	test: 0.608884

Epoch: 31
Loss: 0.425494484603405
RMSE train: 0.572553	val: 0.822793	test: 0.786943
MAE train: 0.438437	val: 0.627073	test: 0.618582

Epoch: 32
Loss: 0.41395244002342224
RMSE train: 0.554211	val: 0.800876	test: 0.755183
MAE train: 0.428177	val: 0.607986	test: 0.591472

Epoch: 33
Loss: 0.39167173206806183
RMSE train: 0.563655	val: 0.807702	test: 0.778448
MAE train: 0.432459	val: 0.615637	test: 0.609251

Epoch: 34
Loss: 0.40634934852520627
RMSE train: 0.561986	val: 0.791168	test: 0.749592
MAE train: 0.433473	val: 0.608720	test: 0.587955

Epoch: 35
Loss: 0.40096253405014676
RMSE train: 0.588799	val: 0.820532	test: 0.789615
MAE train: 0.451381	val: 0.622842	test: 0.623127

Epoch: 36
Loss: 0.4012139042218526
RMSE train: 0.534370	val: 0.773369	test: 0.738512
MAE train: 0.409532	val: 0.593947	test: 0.576384

Epoch: 37
Loss: 0.3977593431870143
RMSE train: 0.553216	val: 0.793437	test: 0.774327
MAE train: 0.427239	val: 0.602610	test: 0.607346

Epoch: 38
Loss: 0.37578536321719486
RMSE train: 0.529274	val: 0.784368	test: 0.759268
MAE train: 0.406970	val: 0.593917	test: 0.592116

Epoch: 39
Loss: 0.3738471269607544
RMSE train: 0.516715	val: 0.787404	test: 0.759500
MAE train: 0.398199	val: 0.596610	test: 0.586353

Epoch: 40
Loss: 0.35888353486855823
RMSE train: 0.505659	val: 0.776121	test: 0.738665
MAE train: 0.388319	val: 0.584608	test: 0.577664

Epoch: 41
Loss: 0.35525477429231006
RMSE train: 0.549932	val: 0.819616	test: 0.803415
MAE train: 0.426581	val: 0.626468	test: 0.634979

Epoch: 42
Loss: 0.36455291757980984
RMSE train: 0.513994	val: 0.777051	test: 0.749677
MAE train: 0.395926	val: 0.589979	test: 0.586191

Epoch: 43
Loss: 0.38159676889578503
RMSE train: 0.555045	val: 0.812700	test: 0.785053
MAE train: 0.426653	val: 0.616469	test: 0.612566

Epoch: 44
Loss: 0.36393405745426816
RMSE train: 0.505517	val: 0.783324	test: 0.748760
MAE train: 0.387720	val: 0.590007	test: 0.582515

Epoch: 45
Loss: 0.3549371014038722
RMSE train: 0.527088	val: 0.795651	test: 0.772380
MAE train: 0.407194	val: 0.604552	test: 0.608131

Epoch: 46
Loss: 0.34403858582178753
RMSE train: 0.522600	val: 0.791646	test: 0.770378
MAE train: 0.403651	val: 0.596821	test: 0.594517

Epoch: 47
Loss: 0.3562934423486392
RMSE train: 0.504094	val: 0.774810	test: 0.751951
MAE train: 0.387825	val: 0.582621	test: 0.587612

Epoch: 48
Loss: 0.3426079824566841
RMSE train: 0.503185	val: 0.787126	test: 0.767223
MAE train: 0.385698	val: 0.592956	test: 0.594137

Epoch: 49
Loss: 0.3394947871565819
RMSE train: 0.524283	val: 0.803016	test: 0.786682
MAE train: 0.401952	val: 0.598490	test: 0.614489

Epoch: 50
Loss: 0.3192122181256612
RMSE train: 0.518984	val: 0.785587	test: 0.774281
MAE train: 0.403100	val: 0.599158	test: 0.604130

Epoch: 51
Loss: 0.35589423030614853
RMSE train: 0.507261	val: 0.786727	test: 0.758403
MAE train: 0.391519	val: 0.593603	test: 0.594076

Epoch: 52
Loss: 0.3312842349211375
RMSE train: 0.500009	val: 0.783651	test: 0.755046
MAE train: 0.384723	val: 0.590029	test: 0.591195

Epoch: 53
Loss: 0.3271188611785571
RMSE train: 0.530264	val: 0.811779	test: 0.798955
MAE train: 0.412152	val: 0.612983	test: 0.622734

Epoch: 54
Loss: 0.32237670322259265
RMSE train: 0.537859	val: 0.793814	test: 0.766481
MAE train: 0.418558	val: 0.602504	test: 0.591801

Epoch: 55
Loss: 0.32956182956695557
RMSE train: 0.501640	val: 0.793140	test: 0.760128
MAE train: 0.387726	val: 0.606225	test: 0.593714

Epoch: 56
Loss: 0.33578840146462124
RMSE train: 0.496453	val: 0.784601	test: 0.765353
MAE train: 0.382756	val: 0.593014	test: 0.598693

Epoch: 57
Loss: 0.3283613199989001
RMSE train: 0.485058	val: 0.778248	test: 0.753278
MAE train: 0.374430	val: 0.583480	test: 0.583565

Epoch: 58
Loss: 0.31705234199762344
RMSE train: 0.486283	val: 0.768386	test: 0.744561
MAE train: 0.376460	val: 0.576722	test: 0.581091

Epoch: 59
Loss: 0.31788574407498044
RMSE train: 0.519382	val: 0.804499	test: 0.790931
MAE train: 0.401669	val: 0.608616	test: 0.620273

Epoch: 60
Loss: 0.3097681316236655
RMSE train: 0.496913	val: 0.786319	test: 0.769310
MAE train: 0.384516	val: 0.592920	test: 0.594346

Epoch: 61
Loss: 0.3119790554046631
RMSE train: 0.474826	val: 0.777614	test: 0.748417
MAE train: 0.364815	val: 0.582885	test: 0.584646

Epoch: 62
Loss: 0.2978596513470014
RMSE train: 0.479469	val: 0.779365	test: 0.761010
MAE train: 0.370536	val: 0.583112	test: 0.589826

Epoch: 63
Loss: 0.3107437963287036
RMSE train: 0.477736	val: 0.797275	test: 0.759520
MAE train: 0.367514	val: 0.597416	test: 0.595963

Epoch: 64
Loss: 0.30491721257567406
RMSE train: 0.494262	val: 0.803308	test: 0.787528
MAE train: 0.381422	val: 0.606346	test: 0.619951

Epoch: 65
Loss: 0.28090546280145645
RMSE train: 0.472094	val: 0.781363	test: 0.767161
MAE train: 0.365788	val: 0.582813	test: 0.594946

Epoch: 66
Loss: 0.30686847617228824
RMSE train: 0.466213	val: 0.785502	test: 0.762688
MAE train: 0.360656	val: 0.583774	test: 0.589733

Epoch: 67
Loss: 0.29063254098097485
RMSE train: 0.447912	val: 0.774648	test: 0.737645
MAE train: 0.343201	val: 0.582803	test: 0.573190

Epoch: 68
Loss: 0.2975460762778918
RMSE train: 0.476405	val: 0.787614	test: 0.758744
MAE train: 0.366129	val: 0.592275	test: 0.587937

Epoch: 69
Loss: 0.29328223317861557
RMSE train: 0.462085	val: 0.778051	test: 0.754272
MAE train: 0.354294	val: 0.586985	test: 0.590117

Epoch: 70
Loss: 0.2895439962546031
RMSE train: 0.450845	val: 0.769305	test: 0.744218
MAE train: 0.348986	val: 0.574004	test: 0.573777

Epoch: 71
Loss: 0.28015700976053876
RMSE train: 0.484058	val: 0.798085	test: 0.775033
MAE train: 0.374058	val: 0.603080	test: 0.608761

Epoch: 72
Loss: 0.2989647102852662
RMSE train: 0.467687	val: 0.773642	test: 0.732895
MAE train: 0.361179	val: 0.580282	test: 0.569759

Epoch: 73
Loss: 0.2796877697110176
RMSE train: 0.447774	val: 0.757483	test: 0.736601
MAE train: 0.347401	val: 0.571332	test: 0.569905

Epoch: 74
Loss: 0.2660830182333787
RMSE train: 0.476102	val: 0.796273	test: 0.771912
MAE train: 0.369883	val: 0.597583	test: 0.601004

Epoch: 75
Loss: 0.2860448422531287
RMSE train: 0.483407	val: 0.802375	test: 0.783456
MAE train: 0.377766	val: 0.604082	test: 0.612304

Epoch: 76
Loss: 0.2859105132520199
RMSE train: 0.451257	val: 0.776843	test: 0.751584
MAE train: 0.347177	val: 0.584115	test: 0.588355

Epoch: 77
Loss: 0.2673618917663892
RMSE train: 0.432340	val: 0.759612	test: 0.749121
MAE train: 0.334211	val: 0.567867	test: 0.574894

Epoch: 78
Loss: 0.27109278614322346
RMSE train: 0.440410	val: 0.768757	test: 0.744617
MAE train: 0.338232	val: 0.581543	test: 0.582615

Epoch: 79
Loss: 0.26828116675217945
RMSE train: 0.478677	val: 0.814757	test: 0.781616
MAE train: 0.371518	val: 0.610515	test: 0.614996

Epoch: 80
Loss: 0.27182311937212944
RMSE train: 0.463579	val: 0.786886	test: 0.759834
MAE train: 0.356672	val: 0.591703	test: 0.594460

Epoch: 81
Loss: 0.26879486193259555
RMSE train: 0.431168	val: 0.762842	test: 0.743879
MAE train: 0.335093	val: 0.568909	test: 0.578572

Epoch: 82
Loss: 0.25625161454081535
RMSE train: 0.440215	val: 0.789014	test: 0.748889
MAE train: 0.339924	val: 0.588160	test: 0.586220

Epoch: 83
Loss: 0.2555490645269553
RMSE train: 0.428476	val: 0.775923	test: 0.745979

Epoch: 23
Loss: 0.4693194851279259
RMSE train: 0.583474	val: 0.815747	test: 0.765653
MAE train: 0.444159	val: 0.616015	test: 0.605100

Epoch: 24
Loss: 0.4541105503837268
RMSE train: 0.594258	val: 0.824070	test: 0.772657
MAE train: 0.454304	val: 0.619920	test: 0.613351

Epoch: 25
Loss: 0.4731922224164009
RMSE train: 0.632637	val: 0.853092	test: 0.803932
MAE train: 0.491467	val: 0.649887	test: 0.635708

Epoch: 26
Loss: 0.4589136044184367
RMSE train: 0.600400	val: 0.807965	test: 0.761955
MAE train: 0.465193	val: 0.614241	test: 0.606139

Epoch: 27
Loss: 0.44912561029195786
RMSE train: 0.581415	val: 0.805702	test: 0.764360
MAE train: 0.445502	val: 0.610404	test: 0.603473

Epoch: 28
Loss: 0.43590909242630005
RMSE train: 0.572073	val: 0.813267	test: 0.757407
MAE train: 0.440937	val: 0.616077	test: 0.600478

Epoch: 29
Loss: 0.41796113302310306
RMSE train: 0.578787	val: 0.805524	test: 0.756641
MAE train: 0.446191	val: 0.604173	test: 0.601335

Epoch: 30
Loss: 0.4184153949220975
RMSE train: 0.564722	val: 0.816275	test: 0.771246
MAE train: 0.433213	val: 0.615000	test: 0.608267

Epoch: 31
Loss: 0.41474657754103345
RMSE train: 0.576034	val: 0.818506	test: 0.775161
MAE train: 0.444546	val: 0.618413	test: 0.614329

Epoch: 32
Loss: 0.40582478046417236
RMSE train: 0.585977	val: 0.831874	test: 0.797739
MAE train: 0.450896	val: 0.628634	test: 0.635696

Epoch: 33
Loss: 0.40546849370002747
RMSE train: 0.588937	val: 0.821128	test: 0.780445
MAE train: 0.457622	val: 0.623597	test: 0.613989

Epoch: 34
Loss: 0.40235166748364765
RMSE train: 0.550326	val: 0.802792	test: 0.761054
MAE train: 0.420513	val: 0.609869	test: 0.601556

Epoch: 35
Loss: 0.39910682042439777
RMSE train: 0.555522	val: 0.787910	test: 0.752103
MAE train: 0.429806	val: 0.597319	test: 0.594700

Epoch: 36
Loss: 0.38541774948438007
RMSE train: 0.553578	val: 0.804801	test: 0.753700
MAE train: 0.427089	val: 0.603952	test: 0.597731

Epoch: 37
Loss: 0.42049213250478107
RMSE train: 0.565022	val: 0.804435	test: 0.784937
MAE train: 0.433643	val: 0.615194	test: 0.616791

Epoch: 38
Loss: 0.38706082850694656
RMSE train: 0.537336	val: 0.783164	test: 0.745500
MAE train: 0.410973	val: 0.596658	test: 0.589163

Epoch: 39
Loss: 0.37897146741549176
RMSE train: 0.541581	val: 0.781117	test: 0.742862
MAE train: 0.415538	val: 0.590121	test: 0.585657

Epoch: 40
Loss: 0.3713127126296361
RMSE train: 0.528312	val: 0.780635	test: 0.749733
MAE train: 0.408804	val: 0.594258	test: 0.586856

Epoch: 41
Loss: 0.3820151090621948
RMSE train: 0.551019	val: 0.811157	test: 0.779729
MAE train: 0.427347	val: 0.613836	test: 0.613322

Epoch: 42
Loss: 0.36777998010317486
RMSE train: 0.537373	val: 0.788775	test: 0.746927
MAE train: 0.415534	val: 0.589997	test: 0.583351

Epoch: 43
Loss: 0.38203144321839017
RMSE train: 0.529146	val: 0.789739	test: 0.745103
MAE train: 0.406844	val: 0.596305	test: 0.586554

Epoch: 44
Loss: 0.3656899506847064
RMSE train: 0.529838	val: 0.813608	test: 0.756261
MAE train: 0.405731	val: 0.620828	test: 0.596060

Epoch: 45
Loss: 0.37038710961739224
RMSE train: 0.531078	val: 0.786098	test: 0.742805
MAE train: 0.410408	val: 0.601082	test: 0.587121

Epoch: 46
Loss: 0.35533247391382855
RMSE train: 0.554386	val: 0.815720	test: 0.784712
MAE train: 0.427146	val: 0.622757	test: 0.621098

Epoch: 47
Loss: 0.36605160186688107
RMSE train: 0.557983	val: 0.827738	test: 0.776812
MAE train: 0.429009	val: 0.631466	test: 0.604634

Epoch: 48
Loss: 0.356293223798275
RMSE train: 0.529965	val: 0.788563	test: 0.742320
MAE train: 0.410898	val: 0.593269	test: 0.585694

Epoch: 49
Loss: 0.35824064662059146
RMSE train: 0.507019	val: 0.771817	test: 0.732082
MAE train: 0.392774	val: 0.582833	test: 0.571913

Epoch: 50
Loss: 0.34713178128004074
RMSE train: 0.537426	val: 0.805059	test: 0.768273
MAE train: 0.412387	val: 0.606677	test: 0.602532

Epoch: 51
Loss: 0.35045453409353894
RMSE train: 0.515964	val: 0.801964	test: 0.754301
MAE train: 0.397145	val: 0.601388	test: 0.601619

Epoch: 52
Loss: 0.3439353406429291
RMSE train: 0.524671	val: 0.790524	test: 0.757583
MAE train: 0.407186	val: 0.600185	test: 0.594186

Epoch: 53
Loss: 0.33942000319560367
RMSE train: 0.493803	val: 0.774444	test: 0.733859
MAE train: 0.380057	val: 0.589668	test: 0.579103

Epoch: 54
Loss: 0.33042293538649875
RMSE train: 0.536970	val: 0.799519	test: 0.769314
MAE train: 0.415506	val: 0.612603	test: 0.614348

Epoch: 55
Loss: 0.32214144617319107
RMSE train: 0.517839	val: 0.798524	test: 0.753541
MAE train: 0.400486	val: 0.601250	test: 0.590081

Epoch: 56
Loss: 0.31192823871970177
RMSE train: 0.498668	val: 0.780731	test: 0.739387
MAE train: 0.384439	val: 0.587928	test: 0.582811

Epoch: 57
Loss: 0.32026103387276333
RMSE train: 0.488638	val: 0.766571	test: 0.725705
MAE train: 0.375713	val: 0.579164	test: 0.575725

Epoch: 58
Loss: 0.3199368566274643
RMSE train: 0.517927	val: 0.808059	test: 0.774534
MAE train: 0.402481	val: 0.614243	test: 0.607649

Epoch: 59
Loss: 0.30823981016874313
RMSE train: 0.501541	val: 0.789148	test: 0.739338
MAE train: 0.387139	val: 0.594577	test: 0.578337

Epoch: 60
Loss: 0.31468988458315533
RMSE train: 0.480017	val: 0.762416	test: 0.727370
MAE train: 0.371694	val: 0.570574	test: 0.570546

Epoch: 61
Loss: 0.3157054955760638
RMSE train: 0.483374	val: 0.777975	test: 0.735242
MAE train: 0.371751	val: 0.580911	test: 0.575434

Epoch: 62
Loss: 0.3111637656887372
RMSE train: 0.482771	val: 0.767030	test: 0.725416
MAE train: 0.372407	val: 0.571451	test: 0.572597

Epoch: 63
Loss: 0.3109132970372836
RMSE train: 0.479970	val: 0.771218	test: 0.732526
MAE train: 0.371565	val: 0.576580	test: 0.575020

Epoch: 64
Loss: 0.29677928735812503
RMSE train: 0.496512	val: 0.785868	test: 0.742722
MAE train: 0.385511	val: 0.593398	test: 0.581492

Epoch: 65
Loss: 0.3071557978789012
RMSE train: 0.503530	val: 0.800543	test: 0.752632
MAE train: 0.387652	val: 0.608246	test: 0.587637

Epoch: 66
Loss: 0.303923017034928
RMSE train: 0.502776	val: 0.790334	test: 0.754530
MAE train: 0.389611	val: 0.595185	test: 0.594444

Epoch: 67
Loss: 0.29422733436028164
RMSE train: 0.469452	val: 0.762225	test: 0.740390
MAE train: 0.360652	val: 0.576258	test: 0.584417

Epoch: 68
Loss: 0.28367019320527714
RMSE train: 0.483027	val: 0.762946	test: 0.723193
MAE train: 0.375602	val: 0.577047	test: 0.573063

Epoch: 69
Loss: 0.3058907836675644
RMSE train: 0.466114	val: 0.781692	test: 0.752140
MAE train: 0.359600	val: 0.589329	test: 0.586372

Epoch: 70
Loss: 0.2768598509331544
RMSE train: 0.510257	val: 0.815219	test: 0.779604
MAE train: 0.399238	val: 0.614899	test: 0.611812

Epoch: 71
Loss: 0.2904696650803089
RMSE train: 0.480095	val: 0.769330	test: 0.741219
MAE train: 0.373914	val: 0.576855	test: 0.580861

Epoch: 72
Loss: 0.2881690214077632
RMSE train: 0.475365	val: 0.785555	test: 0.754830
MAE train: 0.367212	val: 0.588959	test: 0.594042

Epoch: 73
Loss: 0.29086266209681827
RMSE train: 0.494313	val: 0.793314	test: 0.749104
MAE train: 0.384400	val: 0.597461	test: 0.588450

Epoch: 74
Loss: 0.27959611391027767
RMSE train: 0.477576	val: 0.802702	test: 0.754105
MAE train: 0.367841	val: 0.602826	test: 0.594583

Epoch: 75
Loss: 0.2820020044843356
RMSE train: 0.484573	val: 0.798179	test: 0.760615
MAE train: 0.377569	val: 0.602445	test: 0.594552

Epoch: 76
Loss: 0.2752618466814359
RMSE train: 0.486332	val: 0.812205	test: 0.751169
MAE train: 0.374148	val: 0.607148	test: 0.593531

Epoch: 77
Loss: 0.28026875977714855
RMSE train: 0.530461	val: 0.781549	test: 0.756116
MAE train: 0.413698	val: 0.593551	test: 0.594342

Epoch: 78
Loss: 0.281679713477691
RMSE train: 0.462971	val: 0.770862	test: 0.735376
MAE train: 0.357342	val: 0.585509	test: 0.583988

Epoch: 79
Loss: 0.29603000978628796
RMSE train: 0.564238	val: 0.849694	test: 0.821509
MAE train: 0.448615	val: 0.652713	test: 0.647666

Epoch: 80
Loss: 0.27765899896621704
RMSE train: 0.474590	val: 0.793437	test: 0.753341
MAE train: 0.366790	val: 0.602071	test: 0.595915

Epoch: 81
Loss: 0.2868468090891838
RMSE train: 0.458943	val: 0.775763	test: 0.728273
MAE train: 0.355729	val: 0.583040	test: 0.567630

Epoch: 82
Loss: 0.2653755421439807
RMSE train: 0.468476	val: 0.793018	test: 0.754557
MAE train: 0.362278	val: 0.595952	test: 0.592613

Epoch: 83
Loss: 0.25269590442379314
RMSE train: 0.447621	val: 0.783963	test: 0.742268

Epoch: 23
Loss: 0.5012414412839072
RMSE train: 0.629824	val: 0.822282	test: 0.773716
MAE train: 0.487429	val: 0.637050	test: 0.620788

Epoch: 24
Loss: 0.47909759623663767
RMSE train: 0.602166	val: 0.784086	test: 0.756754
MAE train: 0.464613	val: 0.606238	test: 0.603991

Epoch: 25
Loss: 0.4753617154700415
RMSE train: 0.601221	val: 0.785266	test: 0.742967
MAE train: 0.468197	val: 0.606320	test: 0.583624

Epoch: 26
Loss: 0.4625320179121835
RMSE train: 0.596511	val: 0.784179	test: 0.756789
MAE train: 0.461749	val: 0.596724	test: 0.595596

Epoch: 27
Loss: 0.47495959060532705
RMSE train: 0.608504	val: 0.790224	test: 0.759163
MAE train: 0.472820	val: 0.602331	test: 0.603842

Epoch: 28
Loss: 0.43733621707984377
RMSE train: 0.600094	val: 0.785818	test: 0.757350
MAE train: 0.464929	val: 0.593126	test: 0.598784

Epoch: 29
Loss: 0.4384928324392864
RMSE train: 0.590662	val: 0.795667	test: 0.768072
MAE train: 0.459427	val: 0.610429	test: 0.598089

Epoch: 30
Loss: 0.45104878715106417
RMSE train: 0.652869	val: 0.881873	test: 0.816438
MAE train: 0.509764	val: 0.670303	test: 0.651718

Epoch: 31
Loss: 0.4551822968891689
RMSE train: 0.591305	val: 0.790472	test: 0.744859
MAE train: 0.459219	val: 0.597843	test: 0.594151

Epoch: 32
Loss: 0.46663843946797506
RMSE train: 0.619816	val: 0.827464	test: 0.787676
MAE train: 0.481910	val: 0.616690	test: 0.627238

Epoch: 33
Loss: 0.4528984682900565
RMSE train: 0.596305	val: 0.792495	test: 0.744794
MAE train: 0.457590	val: 0.614837	test: 0.578032

Epoch: 34
Loss: 0.43905474671295713
RMSE train: 0.596396	val: 0.780879	test: 0.765925
MAE train: 0.457674	val: 0.598859	test: 0.606949

Epoch: 35
Loss: 0.4336190926177161
RMSE train: 0.595467	val: 0.793096	test: 0.760471
MAE train: 0.458261	val: 0.611821	test: 0.595530

Epoch: 36
Loss: 0.47270073209490093
RMSE train: 0.592599	val: 0.781226	test: 0.772057
MAE train: 0.455289	val: 0.593675	test: 0.609142

Epoch: 37
Loss: 0.40576871378081186
RMSE train: 0.574265	val: 0.774838	test: 0.741248
MAE train: 0.442859	val: 0.588614	test: 0.581809

Epoch: 38
Loss: 0.3996683124985014
RMSE train: 0.553289	val: 0.762452	test: 0.743690
MAE train: 0.425532	val: 0.573045	test: 0.583836

Epoch: 39
Loss: 0.4107151691402708
RMSE train: 0.571480	val: 0.758363	test: 0.752985
MAE train: 0.445106	val: 0.581291	test: 0.586696

Epoch: 40
Loss: 0.4152001866272518
RMSE train: 0.562588	val: 0.771088	test: 0.740238
MAE train: 0.438832	val: 0.591313	test: 0.581194

Epoch: 41
Loss: 0.3997775380100523
RMSE train: 0.555438	val: 0.770610	test: 0.750251
MAE train: 0.432654	val: 0.592469	test: 0.585614

Epoch: 42
Loss: 0.3910719262702124
RMSE train: 0.564043	val: 0.767644	test: 0.736507
MAE train: 0.435928	val: 0.587568	test: 0.575425

Epoch: 43
Loss: 0.37382520203079495
RMSE train: 0.576399	val: 0.797299	test: 0.753855
MAE train: 0.450174	val: 0.602763	test: 0.604658

Epoch: 44
Loss: 0.3668869818959917
RMSE train: 0.548008	val: 0.759979	test: 0.734541
MAE train: 0.425135	val: 0.569193	test: 0.584769

Epoch: 45
Loss: 0.3956191135304315
RMSE train: 0.545041	val: 0.762767	test: 0.742446
MAE train: 0.421962	val: 0.582457	test: 0.578138

Epoch: 46
Loss: 0.42746512591838837
RMSE train: 0.555429	val: 0.782707	test: 0.759600
MAE train: 0.430167	val: 0.595766	test: 0.595701

Epoch: 47
Loss: 0.3885183015039989
RMSE train: 0.540726	val: 0.770766	test: 0.755853
MAE train: 0.416871	val: 0.572874	test: 0.592676

Epoch: 48
Loss: 0.38555048406124115
RMSE train: 0.539404	val: 0.756630	test: 0.739399
MAE train: 0.421084	val: 0.573014	test: 0.574176

Epoch: 49
Loss: 0.375563668353217
RMSE train: 0.537521	val: 0.749590	test: 0.743922
MAE train: 0.415903	val: 0.574160	test: 0.582132

Epoch: 50
Loss: 0.4237555308001382
RMSE train: 0.538902	val: 0.759929	test: 0.737401
MAE train: 0.417902	val: 0.578926	test: 0.582635

Epoch: 51
Loss: 0.3895223098141806
RMSE train: 0.529377	val: 0.756136	test: 0.746521
MAE train: 0.410708	val: 0.571958	test: 0.584802

Epoch: 52
Loss: 0.36449170325483593
RMSE train: 0.520208	val: 0.732787	test: 0.745284
MAE train: 0.399713	val: 0.555155	test: 0.576498

Epoch: 53
Loss: 0.37918866532189505
RMSE train: 0.570792	val: 0.798798	test: 0.775224
MAE train: 0.441823	val: 0.590769	test: 0.613592

Epoch: 54
Loss: 0.36153787800243925
RMSE train: 0.513651	val: 0.742372	test: 0.737646
MAE train: 0.399244	val: 0.555129	test: 0.572982

Epoch: 55
Loss: 0.36916212311812807
RMSE train: 0.515256	val: 0.761211	test: 0.746210
MAE train: 0.396857	val: 0.565594	test: 0.580643

Epoch: 56
Loss: 0.3503068430083139
RMSE train: 0.537852	val: 0.769581	test: 0.760076
MAE train: 0.415533	val: 0.580667	test: 0.599731

Epoch: 57
Loss: 0.35801892621176584
RMSE train: 0.505216	val: 0.747080	test: 0.733043
MAE train: 0.389943	val: 0.565807	test: 0.556788

Epoch: 58
Loss: 0.36418402407850536
RMSE train: 0.519645	val: 0.752010	test: 0.739900
MAE train: 0.405574	val: 0.571057	test: 0.577655

Epoch: 59
Loss: 0.3461630344390869
RMSE train: 0.521386	val: 0.762203	test: 0.727953
MAE train: 0.403321	val: 0.572669	test: 0.576389

Epoch: 60
Loss: 0.3305672151701791
RMSE train: 0.544340	val: 0.775400	test: 0.765976
MAE train: 0.418598	val: 0.577553	test: 0.593506

Epoch: 61
Loss: 0.3491160124540329
RMSE train: 0.500327	val: 0.747042	test: 0.745852
MAE train: 0.386110	val: 0.555175	test: 0.572467

Epoch: 62
Loss: 0.3587690825973238
RMSE train: 0.537116	val: 0.760150	test: 0.749426
MAE train: 0.421646	val: 0.578918	test: 0.587178

Epoch: 63
Loss: 0.3643632318292345
RMSE train: 0.526547	val: 0.748590	test: 0.758530
MAE train: 0.414399	val: 0.563797	test: 0.591883

Epoch: 64
Loss: 0.355722399694579
RMSE train: 0.514511	val: 0.751323	test: 0.737463
MAE train: 0.400846	val: 0.568216	test: 0.574681

Epoch: 65
Loss: 0.34234171892915455
RMSE train: 0.502893	val: 0.754198	test: 0.731827
MAE train: 0.386656	val: 0.569997	test: 0.567934

Epoch: 66
Loss: 0.3258502462080547
RMSE train: 0.512699	val: 0.766491	test: 0.745958
MAE train: 0.395791	val: 0.572016	test: 0.580817

Epoch: 67
Loss: 0.33049882096903666
RMSE train: 0.501326	val: 0.752182	test: 0.737085
MAE train: 0.387831	val: 0.563469	test: 0.569145

Epoch: 68
Loss: 0.3379670466695513
RMSE train: 0.478938	val: 0.744761	test: 0.733210
MAE train: 0.371238	val: 0.556471	test: 0.557906

Epoch: 69
Loss: 0.3184841445514134
RMSE train: 0.505016	val: 0.764365	test: 0.755947
MAE train: 0.391726	val: 0.572491	test: 0.587934

Epoch: 70
Loss: 0.3047786610467093
RMSE train: 0.482283	val: 0.751892	test: 0.738144
MAE train: 0.369932	val: 0.557273	test: 0.571236

Epoch: 71
Loss: 0.31255937048367094
RMSE train: 0.505903	val: 0.751352	test: 0.729324
MAE train: 0.393062	val: 0.567993	test: 0.565842

Epoch: 72
Loss: 0.3064653405121395
RMSE train: 0.492687	val: 0.732912	test: 0.719211
MAE train: 0.379260	val: 0.552777	test: 0.559193

Epoch: 73
Loss: 0.3397347352334431
RMSE train: 0.525872	val: 0.748421	test: 0.759926
MAE train: 0.408476	val: 0.577838	test: 0.595920

Epoch: 74
Loss: 0.32403850129672457
RMSE train: 0.513579	val: 0.790121	test: 0.756314
MAE train: 0.399960	val: 0.594442	test: 0.587688

Epoch: 75
Loss: 0.3018540484564645
RMSE train: 0.477695	val: 0.736612	test: 0.736896
MAE train: 0.366327	val: 0.550927	test: 0.564767

Epoch: 76
Loss: 0.29849827183144434
RMSE train: 0.498262	val: 0.770331	test: 0.746483
MAE train: 0.383834	val: 0.578783	test: 0.585689

Epoch: 77
Loss: 0.29871502944401335
RMSE train: 0.487161	val: 0.740074	test: 0.735565
MAE train: 0.379450	val: 0.561949	test: 0.564914

Epoch: 78
Loss: 0.29547793524605886
RMSE train: 0.505810	val: 0.777636	test: 0.754430
MAE train: 0.392846	val: 0.577688	test: 0.590803

Epoch: 79
Loss: 0.3010426142386028
RMSE train: 0.461559	val: 0.740611	test: 0.723018
MAE train: 0.354791	val: 0.560643	test: 0.552249

Epoch: 80
Loss: 0.31209810078144073
RMSE train: 0.470101	val: 0.760347	test: 0.726966
MAE train: 0.362263	val: 0.572963	test: 0.564067

Epoch: 81
Loss: 0.30598905256816317
RMSE train: 0.488992	val: 0.761299	test: 0.738630
MAE train: 0.377388	val: 0.572360	test: 0.583571

Epoch: 82
Loss: 0.3030490790094648
RMSE train: 0.484529	val: 0.750344	test: 0.737690
MAE train: 0.375480	val: 0.560009	test: 0.568006

Epoch: 83
Loss: 0.29497080402714865
RMSE train: 0.487113	val: 0.762307	test: 0.737814

Epoch: 23
Loss: 0.5000659716980798
RMSE train: 0.628126	val: 0.813031	test: 0.767254
MAE train: 0.487396	val: 0.631936	test: 0.604368

Epoch: 24
Loss: 0.4930146485567093
RMSE train: 0.621701	val: 0.818879	test: 0.767648
MAE train: 0.480972	val: 0.623697	test: 0.607195

Epoch: 25
Loss: 0.47316821771008627
RMSE train: 0.610801	val: 0.795386	test: 0.779858
MAE train: 0.479628	val: 0.604265	test: 0.615660

Epoch: 26
Loss: 0.47917384122099194
RMSE train: 0.605886	val: 0.811346	test: 0.761622
MAE train: 0.470338	val: 0.627334	test: 0.602664

Epoch: 27
Loss: 0.47135572135448456
RMSE train: 0.596201	val: 0.794811	test: 0.761461
MAE train: 0.462656	val: 0.602696	test: 0.594121

Epoch: 28
Loss: 0.4600983900683267
RMSE train: 0.583444	val: 0.793332	test: 0.758686
MAE train: 0.450642	val: 0.613309	test: 0.584782

Epoch: 29
Loss: 0.4368892993245806
RMSE train: 0.614423	val: 0.797387	test: 0.778523
MAE train: 0.478155	val: 0.613685	test: 0.609811

Epoch: 30
Loss: 0.4468834378889629
RMSE train: 0.580101	val: 0.805224	test: 0.768216
MAE train: 0.444099	val: 0.612367	test: 0.599560

Epoch: 31
Loss: 0.4545455149241856
RMSE train: 0.602484	val: 0.808322	test: 0.778195
MAE train: 0.468312	val: 0.627469	test: 0.612421

Epoch: 32
Loss: 0.44439908649240223
RMSE train: 0.590881	val: 0.780443	test: 0.765563
MAE train: 0.459339	val: 0.603653	test: 0.596265

Epoch: 33
Loss: 0.44853257707187105
RMSE train: 0.575533	val: 0.781753	test: 0.752089
MAE train: 0.444896	val: 0.593586	test: 0.593952

Epoch: 34
Loss: 0.4214301492486681
RMSE train: 0.545326	val: 0.758371	test: 0.753384
MAE train: 0.424032	val: 0.576364	test: 0.586089

Epoch: 35
Loss: 0.42344156333378385
RMSE train: 0.559083	val: 0.776414	test: 0.757812
MAE train: 0.434503	val: 0.590266	test: 0.596911

Epoch: 36
Loss: 0.41371270375592367
RMSE train: 0.604917	val: 0.826323	test: 0.784535
MAE train: 0.469187	val: 0.626453	test: 0.616007

Epoch: 37
Loss: 0.417782261967659
RMSE train: 0.565613	val: 0.786605	test: 0.763872
MAE train: 0.436012	val: 0.593128	test: 0.594008

Epoch: 38
Loss: 0.4004678896495274
RMSE train: 0.568353	val: 0.781390	test: 0.770499
MAE train: 0.445591	val: 0.590992	test: 0.606932

Epoch: 39
Loss: 0.3745099093232836
RMSE train: 0.546666	val: 0.778963	test: 0.747656
MAE train: 0.422195	val: 0.591009	test: 0.577760

Epoch: 40
Loss: 0.39145191439560484
RMSE train: 0.540329	val: 0.767141	test: 0.743568
MAE train: 0.416907	val: 0.577827	test: 0.576400

Epoch: 41
Loss: 0.39792261591979433
RMSE train: 0.538295	val: 0.775652	test: 0.753021
MAE train: 0.414183	val: 0.579461	test: 0.585536

Epoch: 42
Loss: 0.384166276880673
RMSE train: 0.552561	val: 0.791885	test: 0.763932
MAE train: 0.425573	val: 0.595085	test: 0.592366

Epoch: 43
Loss: 0.4007779615265982
RMSE train: 0.541969	val: 0.775498	test: 0.757192
MAE train: 0.421768	val: 0.588588	test: 0.593233

Epoch: 44
Loss: 0.3788044367517744
RMSE train: 0.566378	val: 0.772659	test: 0.768591
MAE train: 0.448023	val: 0.581619	test: 0.601867

Epoch: 45
Loss: 0.3751278689929417
RMSE train: 0.519003	val: 0.768789	test: 0.741326
MAE train: 0.402075	val: 0.575379	test: 0.572546

Epoch: 46
Loss: 0.364339394228799
RMSE train: 0.534573	val: 0.782600	test: 0.780683
MAE train: 0.411203	val: 0.587441	test: 0.604725

Epoch: 47
Loss: 0.37416900907244
RMSE train: 0.522915	val: 0.753797	test: 0.760954
MAE train: 0.404483	val: 0.573958	test: 0.594050

Epoch: 48
Loss: 0.36003394212041584
RMSE train: 0.514439	val: 0.753032	test: 0.743348
MAE train: 0.400342	val: 0.570422	test: 0.581252

Epoch: 49
Loss: 0.3650457412004471
RMSE train: 0.506903	val: 0.750264	test: 0.747897
MAE train: 0.396137	val: 0.564668	test: 0.579527

Epoch: 50
Loss: 0.3745003661939076
RMSE train: 0.520236	val: 0.753324	test: 0.754248
MAE train: 0.407338	val: 0.560410	test: 0.594388

Epoch: 51
Loss: 0.3606526936803545
RMSE train: 0.522826	val: 0.768321	test: 0.752657
MAE train: 0.404316	val: 0.570545	test: 0.588911

Epoch: 52
Loss: 0.34916561203343527
RMSE train: 0.521113	val: 0.788075	test: 0.769877
MAE train: 0.402610	val: 0.591146	test: 0.596200

Epoch: 53
Loss: 0.3656755472932543
RMSE train: 0.512555	val: 0.776497	test: 0.757282
MAE train: 0.400588	val: 0.577491	test: 0.586347

Epoch: 54
Loss: 0.3804672956466675
RMSE train: 0.521749	val: 0.785680	test: 0.782294
MAE train: 0.404317	val: 0.589233	test: 0.592306

Epoch: 55
Loss: 0.3728362130267279
RMSE train: 0.542225	val: 0.771985	test: 0.772532
MAE train: 0.421002	val: 0.579046	test: 0.594175

Epoch: 56
Loss: 0.36378830458436695
RMSE train: 0.526256	val: 0.789545	test: 0.766528
MAE train: 0.409416	val: 0.586779	test: 0.603390

Epoch: 57
Loss: 0.3605622478893825
RMSE train: 0.579727	val: 0.802202	test: 0.791054
MAE train: 0.457590	val: 0.603705	test: 0.611976

Epoch: 58
Loss: 0.3490377111094339
RMSE train: 0.507173	val: 0.768500	test: 0.768807
MAE train: 0.394057	val: 0.579356	test: 0.580835

Epoch: 59
Loss: 0.33101451077631544
RMSE train: 0.508545	val: 0.756758	test: 0.760076
MAE train: 0.396529	val: 0.568619	test: 0.587359

Epoch: 60
Loss: 0.3421058974095753
RMSE train: 0.512566	val: 0.777004	test: 0.744161
MAE train: 0.395058	val: 0.574341	test: 0.573648

Epoch: 61
Loss: 0.3349398991891316
RMSE train: 0.497213	val: 0.758922	test: 0.745383
MAE train: 0.384600	val: 0.571895	test: 0.571810

Epoch: 62
Loss: 0.3307998095239912
RMSE train: 0.486549	val: 0.749735	test: 0.735381
MAE train: 0.374255	val: 0.557620	test: 0.561106

Epoch: 63
Loss: 0.32845004967280794
RMSE train: 0.496439	val: 0.769559	test: 0.737952
MAE train: 0.380660	val: 0.574941	test: 0.561761

Epoch: 64
Loss: 0.3223761873585837
RMSE train: 0.520239	val: 0.773312	test: 0.744033
MAE train: 0.400821	val: 0.575302	test: 0.566495

Epoch: 65
Loss: 0.32124178750174387
RMSE train: 0.489571	val: 0.765941	test: 0.753622
MAE train: 0.379522	val: 0.571200	test: 0.575643

Epoch: 66
Loss: 0.31733979284763336
RMSE train: 0.488421	val: 0.759205	test: 0.753371
MAE train: 0.377745	val: 0.559876	test: 0.574752

Epoch: 67
Loss: 0.35001123803002493
RMSE train: 0.526924	val: 0.813254	test: 0.768128
MAE train: 0.409906	val: 0.604621	test: 0.593629

Epoch: 68
Loss: 0.3239367050783975
RMSE train: 0.490127	val: 0.779030	test: 0.760878
MAE train: 0.376021	val: 0.584402	test: 0.570922

Epoch: 69
Loss: 0.32212853005954195
RMSE train: 0.481659	val: 0.766522	test: 0.739835
MAE train: 0.372416	val: 0.569417	test: 0.577032

Epoch: 70
Loss: 0.3120222921882357
RMSE train: 0.478852	val: 0.740204	test: 0.752834
MAE train: 0.374443	val: 0.566831	test: 0.578188

Epoch: 71
Loss: 0.3119637093373707
RMSE train: 0.483870	val: 0.756857	test: 0.728205
MAE train: 0.371893	val: 0.571729	test: 0.562075

Epoch: 72
Loss: 0.29665123245545794
RMSE train: 0.458556	val: 0.745233	test: 0.737248
MAE train: 0.354264	val: 0.550114	test: 0.552348

Epoch: 73
Loss: 0.308725899883679
RMSE train: 0.467855	val: 0.766337	test: 0.741407
MAE train: 0.362818	val: 0.567908	test: 0.571966

Epoch: 74
Loss: 0.32556563190051485
RMSE train: 0.460776	val: 0.771241	test: 0.752267
MAE train: 0.355114	val: 0.561333	test: 0.568282

Epoch: 75
Loss: 0.30200135707855225
RMSE train: 0.492426	val: 0.780315	test: 0.758974
MAE train: 0.379047	val: 0.580774	test: 0.577171

Epoch: 76
Loss: 0.3122009241155216
RMSE train: 0.476455	val: 0.764853	test: 0.742544
MAE train: 0.367546	val: 0.577804	test: 0.571087

Epoch: 77
Loss: 0.3028877079486847
RMSE train: 0.457565	val: 0.750972	test: 0.744949
MAE train: 0.355331	val: 0.562156	test: 0.560369

Epoch: 78
Loss: 0.2923358402081898
RMSE train: 0.455172	val: 0.754822	test: 0.735197
MAE train: 0.352147	val: 0.560429	test: 0.563430

Epoch: 79
Loss: 0.28962734660931994
RMSE train: 0.475341	val: 0.753239	test: 0.744630
MAE train: 0.372918	val: 0.567443	test: 0.566120

Epoch: 80
Loss: 0.2966532739145415
RMSE train: 0.455703	val: 0.775802	test: 0.739473
MAE train: 0.351607	val: 0.576181	test: 0.563420

Epoch: 81
Loss: 0.29673628509044647
RMSE train: 0.447187	val: 0.736552	test: 0.740922
MAE train: 0.345630	val: 0.548115	test: 0.560843

Epoch: 82
Loss: 0.3094212519271033
RMSE train: 0.461996	val: 0.781363	test: 0.743562
MAE train: 0.357107	val: 0.580913	test: 0.563869

Epoch: 83
Loss: 0.31303519010543823
RMSE train: 0.484647	val: 0.773500	test: 0.764344

Epoch: 23
Loss: 0.5457676180771419
RMSE train: 0.640015	val: 0.835384	test: 0.765854
MAE train: 0.494621	val: 0.645916	test: 0.615499

Epoch: 24
Loss: 0.4937417209148407
RMSE train: 0.622891	val: 0.840637	test: 0.780601
MAE train: 0.475486	val: 0.644682	test: 0.618170

Epoch: 25
Loss: 0.4862308757645743
RMSE train: 0.627465	val: 0.834833	test: 0.783383
MAE train: 0.485687	val: 0.640463	test: 0.623025

Epoch: 26
Loss: 0.4993331176894052
RMSE train: 0.610641	val: 0.809631	test: 0.764055
MAE train: 0.470401	val: 0.623276	test: 0.599673

Epoch: 27
Loss: 0.45498486501829966
RMSE train: 0.592965	val: 0.797595	test: 0.741769
MAE train: 0.456799	val: 0.613005	test: 0.575582

Epoch: 28
Loss: 0.4784291109868458
RMSE train: 0.595770	val: 0.804497	test: 0.758980
MAE train: 0.456212	val: 0.615650	test: 0.595114

Epoch: 29
Loss: 0.45980661468846457
RMSE train: 0.602005	val: 0.817838	test: 0.766647
MAE train: 0.465316	val: 0.621459	test: 0.607104

Epoch: 30
Loss: 0.46545631119183134
RMSE train: 0.597473	val: 0.796850	test: 0.749826
MAE train: 0.460605	val: 0.611803	test: 0.591839

Epoch: 31
Loss: 0.44266675625528606
RMSE train: 0.589998	val: 0.814315	test: 0.775176
MAE train: 0.455304	val: 0.621871	test: 0.616880

Epoch: 32
Loss: 0.41987953441483633
RMSE train: 0.577617	val: 0.787275	test: 0.756847
MAE train: 0.445384	val: 0.611061	test: 0.587790

Epoch: 33
Loss: 0.43362391207899365
RMSE train: 0.577423	val: 0.790358	test: 0.773040
MAE train: 0.445088	val: 0.605623	test: 0.603017

Epoch: 34
Loss: 0.413907636489187
RMSE train: 0.584697	val: 0.814769	test: 0.774124
MAE train: 0.450016	val: 0.613502	test: 0.603205

Epoch: 35
Loss: 0.4275844075850078
RMSE train: 0.571198	val: 0.793231	test: 0.758482
MAE train: 0.438983	val: 0.602746	test: 0.598718

Epoch: 36
Loss: 0.4276686097894396
RMSE train: 0.561228	val: 0.772708	test: 0.741939
MAE train: 0.432434	val: 0.588814	test: 0.575688

Epoch: 37
Loss: 0.4354631177016667
RMSE train: 0.570337	val: 0.776289	test: 0.726351
MAE train: 0.440234	val: 0.594493	test: 0.566853

Epoch: 38
Loss: 0.4238876146929605
RMSE train: 0.542071	val: 0.770724	test: 0.739093
MAE train: 0.418211	val: 0.586149	test: 0.569717

Epoch: 39
Loss: 0.4023359588214329
RMSE train: 0.595584	val: 0.810253	test: 0.775417
MAE train: 0.457583	val: 0.611695	test: 0.615523

Epoch: 40
Loss: 0.3971794864961079
RMSE train: 0.560486	val: 0.766462	test: 0.746581
MAE train: 0.435535	val: 0.591915	test: 0.583633

Epoch: 41
Loss: 0.38402154190199717
RMSE train: 0.546242	val: 0.773191	test: 0.744390
MAE train: 0.422683	val: 0.592896	test: 0.579083

Epoch: 42
Loss: 0.3846924283674785
RMSE train: 0.543227	val: 0.770104	test: 0.736989
MAE train: 0.419758	val: 0.587093	test: 0.577222

Epoch: 43
Loss: 0.3863241991826466
RMSE train: 0.563902	val: 0.808762	test: 0.762353
MAE train: 0.435845	val: 0.619768	test: 0.596185

Epoch: 44
Loss: 0.4123801567724773
RMSE train: 0.558428	val: 0.796281	test: 0.738420
MAE train: 0.431026	val: 0.603854	test: 0.573280

Epoch: 45
Loss: 0.3945332659142358
RMSE train: 0.549163	val: 0.792015	test: 0.753132
MAE train: 0.427656	val: 0.598171	test: 0.587565

Epoch: 46
Loss: 0.3656103270394461
RMSE train: 0.551441	val: 0.785509	test: 0.748413
MAE train: 0.426878	val: 0.591357	test: 0.586863

Epoch: 47
Loss: 0.3895749811615263
RMSE train: 0.581202	val: 0.816071	test: 0.770071
MAE train: 0.452116	val: 0.614790	test: 0.614375

Epoch: 48
Loss: 0.3812026008963585
RMSE train: 0.558101	val: 0.801349	test: 0.768400
MAE train: 0.434995	val: 0.606368	test: 0.607682

Epoch: 49
Loss: 0.3960162507636206
RMSE train: 0.571357	val: 0.801647	test: 0.771911
MAE train: 0.445935	val: 0.606480	test: 0.600909

Epoch: 50
Loss: 0.38801442086696625
RMSE train: 0.534885	val: 0.754883	test: 0.739757
MAE train: 0.413193	val: 0.582447	test: 0.580344

Epoch: 51
Loss: 0.40058412296431406
RMSE train: 0.573218	val: 0.793532	test: 0.761226
MAE train: 0.444694	val: 0.606193	test: 0.601791

Epoch: 52
Loss: 0.3739603417260306
RMSE train: 0.582241	val: 0.843960	test: 0.788049
MAE train: 0.454910	val: 0.631492	test: 0.621616

Epoch: 53
Loss: 0.3562798968383244
RMSE train: 0.513555	val: 0.767132	test: 0.751358
MAE train: 0.394526	val: 0.576766	test: 0.584420

Epoch: 54
Loss: 0.3809007725545338
RMSE train: 0.530052	val: 0.776025	test: 0.750553
MAE train: 0.412565	val: 0.587550	test: 0.591785

Epoch: 55
Loss: 0.3753933736256191
RMSE train: 0.531587	val: 0.785397	test: 0.756026
MAE train: 0.409276	val: 0.599248	test: 0.592267

Epoch: 56
Loss: 0.34995868163449423
RMSE train: 0.516787	val: 0.778410	test: 0.727071
MAE train: 0.400670	val: 0.592009	test: 0.566389

Epoch: 57
Loss: 0.3521300660712378
RMSE train: 0.518724	val: 0.786759	test: 0.751139
MAE train: 0.402357	val: 0.600787	test: 0.580767

Epoch: 58
Loss: 0.37441172344344004
RMSE train: 0.522938	val: 0.759953	test: 0.723029
MAE train: 0.405586	val: 0.587453	test: 0.569283

Epoch: 59
Loss: 0.42929653184754507
RMSE train: 0.610213	val: 0.807005	test: 0.763578
MAE train: 0.476180	val: 0.617603	test: 0.603685

Epoch: 60
Loss: 0.38243986453328815
RMSE train: 0.589594	val: 0.816972	test: 0.780817
MAE train: 0.461097	val: 0.624912	test: 0.616765

Epoch: 61
Loss: 0.3549325295857021
RMSE train: 0.517345	val: 0.775462	test: 0.723746
MAE train: 0.400041	val: 0.585559	test: 0.560094

Epoch: 62
Loss: 0.345709604876382
RMSE train: 0.514885	val: 0.772928	test: 0.733215
MAE train: 0.399400	val: 0.582951	test: 0.572596

Epoch: 63
Loss: 0.3557394679103579
RMSE train: 0.546238	val: 0.783356	test: 0.752267
MAE train: 0.426174	val: 0.592458	test: 0.594761

Epoch: 64
Loss: 0.3272627421787807
RMSE train: 0.540810	val: 0.810259	test: 0.771599
MAE train: 0.422198	val: 0.608892	test: 0.605327

Epoch: 65
Loss: 0.32942022596086773
RMSE train: 0.507149	val: 0.759605	test: 0.742282
MAE train: 0.392626	val: 0.574563	test: 0.585312

Epoch: 66
Loss: 0.33674946214471546
RMSE train: 0.507485	val: 0.763423	test: 0.735836
MAE train: 0.392468	val: 0.577352	test: 0.578429

Epoch: 67
Loss: 0.3358319635902132
RMSE train: 0.510930	val: 0.777762	test: 0.749216
MAE train: 0.397275	val: 0.591228	test: 0.588047

Epoch: 68
Loss: 0.3303222826548985
RMSE train: 0.487760	val: 0.758136	test: 0.722934
MAE train: 0.376556	val: 0.573572	test: 0.560540

Epoch: 69
Loss: 0.3327520340681076
RMSE train: 0.514253	val: 0.784786	test: 0.746269
MAE train: 0.401264	val: 0.591527	test: 0.589074

Epoch: 70
Loss: 0.3082348331809044
RMSE train: 0.491613	val: 0.773844	test: 0.741801
MAE train: 0.379603	val: 0.589095	test: 0.579256

Epoch: 71
Loss: 0.309155598282814
RMSE train: 0.582714	val: 0.857305	test: 0.807642
MAE train: 0.460458	val: 0.652966	test: 0.642332

Epoch: 72
Loss: 0.3306884127003806
RMSE train: 0.516719	val: 0.791403	test: 0.743081
MAE train: 0.400943	val: 0.599982	test: 0.581218

Epoch: 73
Loss: 0.3210828261716025
RMSE train: 0.557405	val: 0.837288	test: 0.789485
MAE train: 0.439258	val: 0.629611	test: 0.621453

Epoch: 74
Loss: 0.310834418450083
RMSE train: 0.481245	val: 0.763206	test: 0.746905
MAE train: 0.373460	val: 0.576480	test: 0.577874

Epoch: 75
Loss: 0.3112610174076898
RMSE train: 0.478514	val: 0.763106	test: 0.732517
MAE train: 0.368252	val: 0.576298	test: 0.572220

Epoch: 76
Loss: 0.3022078147956303
RMSE train: 0.481474	val: 0.754298	test: 0.720692
MAE train: 0.372320	val: 0.574800	test: 0.566431

Epoch: 77
Loss: 0.29471659873213085
RMSE train: 0.487107	val: 0.759024	test: 0.743960
MAE train: 0.376799	val: 0.587691	test: 0.581375

Epoch: 78
Loss: 0.31033473781176973
RMSE train: 0.489672	val: 0.777762	test: 0.756068
MAE train: 0.377074	val: 0.585958	test: 0.584617

Epoch: 79
Loss: 0.2972006318824632
RMSE train: 0.491173	val: 0.772153	test: 0.758464
MAE train: 0.380949	val: 0.580693	test: 0.593792

Epoch: 80
Loss: 0.2952889512692179
RMSE train: 0.475526	val: 0.744944	test: 0.728029
MAE train: 0.367488	val: 0.562055	test: 0.568959

Epoch: 81
Loss: 0.291123024054936
RMSE train: 0.511560	val: 0.787960	test: 0.772552
MAE train: 0.398406	val: 0.591586	test: 0.606483

Epoch: 82
Loss: 0.3153955638408661
RMSE train: 0.496002	val: 0.777259	test: 0.755075
MAE train: 0.385376	val: 0.587184	test: 0.596103

Epoch: 83
Loss: 0.2927507609128952
RMSE train: 0.501291	val: 0.778205	test: 0.745165
MAE train: 0.342859	val: 0.598407	test: 0.612190

Epoch: 84
Loss: 0.25134635120630267
RMSE train: 0.422507	val: 0.762429	test: 0.797826
MAE train: 0.328365	val: 0.584059	test: 0.610271

Epoch: 85
Loss: 0.25118666887283325
RMSE train: 0.440879	val: 0.772116	test: 0.800110
MAE train: 0.340293	val: 0.594789	test: 0.617414

Epoch: 86
Loss: 0.2581789791584015
RMSE train: 0.455384	val: 0.776322	test: 0.793049
MAE train: 0.352186	val: 0.597525	test: 0.614092

Epoch: 87
Loss: 0.26451468765735625
RMSE train: 0.421050	val: 0.766612	test: 0.806120
MAE train: 0.323853	val: 0.586288	test: 0.617920

Epoch: 88
Loss: 0.25050974637269974
RMSE train: 0.460380	val: 0.793896	test: 0.831566
MAE train: 0.359974	val: 0.613670	test: 0.640402

Epoch: 89
Loss: 0.24267832189798355
RMSE train: 0.429800	val: 0.778670	test: 0.812474
MAE train: 0.334311	val: 0.599042	test: 0.622843

Epoch: 90
Loss: 0.25127706676721573
RMSE train: 0.418162	val: 0.749628	test: 0.781794
MAE train: 0.324755	val: 0.577232	test: 0.601460

Epoch: 91
Loss: 0.24181465953588485
RMSE train: 0.406241	val: 0.753892	test: 0.783541
MAE train: 0.315315	val: 0.577728	test: 0.599545

Epoch: 92
Loss: 0.24450942724943162
RMSE train: 0.413063	val: 0.772139	test: 0.799494
MAE train: 0.320374	val: 0.588762	test: 0.613431

Epoch: 93
Loss: 0.2503026008605957
RMSE train: 0.432242	val: 0.784009	test: 0.802166
MAE train: 0.331068	val: 0.599937	test: 0.620601

Epoch: 94
Loss: 0.23794944435358048
RMSE train: 0.402546	val: 0.758818	test: 0.789412
MAE train: 0.310298	val: 0.579109	test: 0.603382

Epoch: 95
Loss: 0.23294492959976196
RMSE train: 0.411063	val: 0.763496	test: 0.790327
MAE train: 0.319716	val: 0.587512	test: 0.609594

Epoch: 96
Loss: 0.24138289988040923
RMSE train: 0.428313	val: 0.768294	test: 0.796600
MAE train: 0.333640	val: 0.592999	test: 0.614766

Epoch: 97
Loss: 0.23099148869514466
RMSE train: 0.428801	val: 0.781274	test: 0.797857
MAE train: 0.334582	val: 0.595986	test: 0.614585

Epoch: 98
Loss: 0.22557127028703688
RMSE train: 0.385277	val: 0.756935	test: 0.773108
MAE train: 0.297286	val: 0.580663	test: 0.592682

Epoch: 99
Loss: 0.22048499435186386
RMSE train: 0.389718	val: 0.762544	test: 0.784563
MAE train: 0.303091	val: 0.585320	test: 0.600755

Epoch: 100
Loss: 0.21986272633075715
RMSE train: 0.409449	val: 0.769137	test: 0.796972
MAE train: 0.316232	val: 0.585576	test: 0.610572

Epoch: 101
Loss: 0.24091415107250214
RMSE train: 0.412354	val: 0.760619	test: 0.782603
MAE train: 0.324173	val: 0.586985	test: 0.599617

Epoch: 102
Loss: 0.23370446115732194
RMSE train: 0.407719	val: 0.765823	test: 0.779560
MAE train: 0.316261	val: 0.588692	test: 0.601136

Epoch: 103
Loss: 0.2345499038696289
RMSE train: 0.444000	val: 0.793389	test: 0.827886
MAE train: 0.348137	val: 0.607871	test: 0.635300

Epoch: 104
Loss: 0.22972506284713745
RMSE train: 0.391494	val: 0.766314	test: 0.784740
MAE train: 0.303338	val: 0.588370	test: 0.603629

Epoch: 105
Loss: 0.21386264115571976
RMSE train: 0.386231	val: 0.758494	test: 0.779740
MAE train: 0.299550	val: 0.577542	test: 0.601004

Epoch: 106
Loss: 0.2265186384320259
RMSE train: 0.407654	val: 0.763918	test: 0.795381
MAE train: 0.316891	val: 0.588188	test: 0.609456

Epoch: 107
Loss: 0.21818163394927978
RMSE train: 0.401548	val: 0.772631	test: 0.788179
MAE train: 0.311506	val: 0.592371	test: 0.603871

Epoch: 108
Loss: 0.2126358062028885
RMSE train: 0.401853	val: 0.778210	test: 0.799788
MAE train: 0.311943	val: 0.593221	test: 0.614428

Epoch: 109
Loss: 0.21698477417230605
RMSE train: 0.397641	val: 0.769888	test: 0.813425
MAE train: 0.309052	val: 0.588151	test: 0.626754

Epoch: 110
Loss: 0.20827149152755736
RMSE train: 0.390350	val: 0.758384	test: 0.789674
MAE train: 0.302182	val: 0.581480	test: 0.607605

Epoch: 111
Loss: 0.21284628808498382
RMSE train: 0.423610	val: 0.788624	test: 0.827762
MAE train: 0.332548	val: 0.607138	test: 0.636631

Epoch: 112
Loss: 0.21818216443061828
RMSE train: 0.408112	val: 0.769835	test: 0.788342
MAE train: 0.314977	val: 0.591712	test: 0.608204

Epoch: 113
Loss: 0.21260478347539902
RMSE train: 0.385689	val: 0.758372	test: 0.772439
MAE train: 0.300566	val: 0.578093	test: 0.592583

Epoch: 114
Loss: 0.21465664356946945
RMSE train: 0.380136	val: 0.768504	test: 0.785562
MAE train: 0.295079	val: 0.585365	test: 0.603377

Epoch: 115
Loss: 0.2004447266459465
RMSE train: 0.420342	val: 0.791088	test: 0.811789
MAE train: 0.324964	val: 0.604510	test: 0.625254

Epoch: 116
Loss: 0.21274001598358155
RMSE train: 0.406011	val: 0.769041	test: 0.801969
MAE train: 0.314892	val: 0.588748	test: 0.618835

Epoch: 117
Loss: 0.21105348616838454
RMSE train: 0.383681	val: 0.765993	test: 0.791983
MAE train: 0.298622	val: 0.584139	test: 0.604141

Epoch: 118
Loss: 0.2119299054145813
RMSE train: 0.389677	val: 0.765351	test: 0.784872
MAE train: 0.303022	val: 0.583374	test: 0.605472

Epoch: 119
Loss: 0.2142566904425621
RMSE train: 0.382378	val: 0.760982	test: 0.780141
MAE train: 0.296081	val: 0.581262	test: 0.599913

Epoch: 120
Loss: 0.20809012949466704
RMSE train: 0.368164	val: 0.753716	test: 0.774258
MAE train: 0.283202	val: 0.571949	test: 0.595115

Epoch: 121
Loss: 0.20140597224235535
RMSE train: 0.370687	val: 0.758468	test: 0.779589
MAE train: 0.288290	val: 0.583276	test: 0.599307

Early stopping
Best (RMSE):	 train: 0.419223	val: 0.749289	test: 0.772929
Best (MAE):	 train: 0.325548	val: 0.583339	test: 0.591133

MAE train: 0.332991	val: 0.583923	test: 0.615457

Epoch: 84
Loss: 0.260160306096077
RMSE train: 0.429332	val: 0.761884	test: 0.784087
MAE train: 0.332781	val: 0.583782	test: 0.604398

Epoch: 85
Loss: 0.24750042110681533
RMSE train: 0.454736	val: 0.798915	test: 0.815029
MAE train: 0.354958	val: 0.609030	test: 0.627639

Epoch: 86
Loss: 0.25867965668439863
RMSE train: 0.453940	val: 0.795165	test: 0.824078
MAE train: 0.355727	val: 0.612840	test: 0.641524

Epoch: 87
Loss: 0.2551120638847351
RMSE train: 0.458208	val: 0.776174	test: 0.803166
MAE train: 0.355206	val: 0.594531	test: 0.620398

Epoch: 88
Loss: 0.2437156468629837
RMSE train: 0.431044	val: 0.767275	test: 0.784415
MAE train: 0.334009	val: 0.581258	test: 0.607838

Epoch: 89
Loss: 0.24198104441165924
RMSE train: 0.496635	val: 0.823384	test: 0.858395
MAE train: 0.390978	val: 0.634090	test: 0.659309

Epoch: 90
Loss: 0.24080355316400529
RMSE train: 0.415071	val: 0.762867	test: 0.798946
MAE train: 0.320389	val: 0.579448	test: 0.612587

Epoch: 91
Loss: 0.23830910176038742
RMSE train: 0.430723	val: 0.777586	test: 0.810943
MAE train: 0.337454	val: 0.592820	test: 0.621555

Epoch: 92
Loss: 0.2371574491262436
RMSE train: 0.446780	val: 0.787015	test: 0.805342
MAE train: 0.347017	val: 0.597444	test: 0.626131

Epoch: 93
Loss: 0.25047165900468826
RMSE train: 0.416841	val: 0.761601	test: 0.791793
MAE train: 0.321484	val: 0.581332	test: 0.611751

Epoch: 94
Loss: 0.23446772247552872
RMSE train: 0.423027	val: 0.773982	test: 0.802903
MAE train: 0.325179	val: 0.592352	test: 0.615788

Epoch: 95
Loss: 0.23580970317125322
RMSE train: 0.448926	val: 0.794911	test: 0.811916
MAE train: 0.347672	val: 0.602979	test: 0.627819

Epoch: 96
Loss: 0.23193178474903106
RMSE train: 0.431505	val: 0.787644	test: 0.813049
MAE train: 0.337610	val: 0.597458	test: 0.624211

Epoch: 97
Loss: 0.23192203938961028
RMSE train: 0.442549	val: 0.781432	test: 0.822200
MAE train: 0.348638	val: 0.596415	test: 0.624190

Epoch: 98
Loss: 0.22873458564281463
RMSE train: 0.441760	val: 0.787249	test: 0.812461
MAE train: 0.341027	val: 0.600028	test: 0.629433

Epoch: 99
Loss: 0.23701438754796983
RMSE train: 0.418134	val: 0.760425	test: 0.797278
MAE train: 0.325170	val: 0.578499	test: 0.608630

Epoch: 100
Loss: 0.23100676387548447
RMSE train: 0.409372	val: 0.751961	test: 0.778464
MAE train: 0.317001	val: 0.571530	test: 0.596707

Epoch: 101
Loss: 0.22036634236574174
RMSE train: 0.407851	val: 0.780053	test: 0.788744
MAE train: 0.316354	val: 0.592421	test: 0.607668

Epoch: 102
Loss: 0.23528726547956466
RMSE train: 0.429886	val: 0.769466	test: 0.794904
MAE train: 0.333319	val: 0.585267	test: 0.614610

Epoch: 103
Loss: 0.22672205567359924
RMSE train: 0.427600	val: 0.802046	test: 0.828972
MAE train: 0.333698	val: 0.611337	test: 0.635352

Epoch: 104
Loss: 0.22400922030210496
RMSE train: 0.399008	val: 0.769388	test: 0.790377
MAE train: 0.309313	val: 0.584200	test: 0.601544

Epoch: 105
Loss: 0.2186232939362526
RMSE train: 0.421971	val: 0.774859	test: 0.799112
MAE train: 0.327233	val: 0.590335	test: 0.606504

Epoch: 106
Loss: 0.22704167664051056
RMSE train: 0.401131	val: 0.763569	test: 0.789205
MAE train: 0.311451	val: 0.576035	test: 0.601817

Epoch: 107
Loss: 0.22512532621622086
RMSE train: 0.399828	val: 0.762297	test: 0.772878
MAE train: 0.310309	val: 0.581666	test: 0.593029

Epoch: 108
Loss: 0.22412666529417039
RMSE train: 0.430236	val: 0.799218	test: 0.818473
MAE train: 0.334325	val: 0.610661	test: 0.630531

Epoch: 109
Loss: 0.21341277211904525
RMSE train: 0.427803	val: 0.781332	test: 0.802749
MAE train: 0.334726	val: 0.595353	test: 0.617698

Epoch: 110
Loss: 0.2102248415350914
RMSE train: 0.395975	val: 0.774617	test: 0.798896
MAE train: 0.308000	val: 0.581739	test: 0.613062

Epoch: 111
Loss: 0.21022800654172896
RMSE train: 0.401833	val: 0.775834	test: 0.787717
MAE train: 0.310028	val: 0.586069	test: 0.606196

Epoch: 112
Loss: 0.20902740806341172
RMSE train: 0.399799	val: 0.768932	test: 0.786221
MAE train: 0.309323	val: 0.582861	test: 0.606245

Epoch: 113
Loss: 0.19122541099786758
RMSE train: 0.405738	val: 0.778063	test: 0.801825
MAE train: 0.315835	val: 0.590016	test: 0.619721

Epoch: 114
Loss: 0.21896712630987167
RMSE train: 0.392662	val: 0.767355	test: 0.782724
MAE train: 0.302643	val: 0.578006	test: 0.600800

Epoch: 115
Loss: 0.21869546473026275
RMSE train: 0.389053	val: 0.749470	test: 0.769962
MAE train: 0.299655	val: 0.568567	test: 0.587137

Epoch: 116
Loss: 0.2048610717058182
RMSE train: 0.383476	val: 0.774610	test: 0.810502
MAE train: 0.296261	val: 0.580362	test: 0.616455

Epoch: 117
Loss: 0.20300753712654113
RMSE train: 0.399752	val: 0.748039	test: 0.778935
MAE train: 0.311983	val: 0.568667	test: 0.593266

Epoch: 118
Loss: 0.22827940583229064
RMSE train: 0.395817	val: 0.779028	test: 0.818103
MAE train: 0.305471	val: 0.592801	test: 0.623121

Epoch: 119
Loss: 0.21051705181598662
RMSE train: 0.397523	val: 0.765309	test: 0.790981
MAE train: 0.306214	val: 0.579644	test: 0.606871

Epoch: 120
Loss: 0.2029694750905037
RMSE train: 0.443236	val: 0.793843	test: 0.824130
MAE train: 0.346722	val: 0.596282	test: 0.630428

Epoch: 121
Loss: 0.20251161605119705
RMSE train: 0.423722	val: 0.783343	test: 0.811518
MAE train: 0.332230	val: 0.592002	test: 0.625943

Epoch: 122
Loss: 0.20788997411727905
RMSE train: 0.412176	val: 0.781548	test: 0.796837
MAE train: 0.321505	val: 0.592557	test: 0.618191

Epoch: 123
Loss: 0.20620617717504502
RMSE train: 0.426791	val: 0.787909	test: 0.810532
MAE train: 0.332383	val: 0.597492	test: 0.630891

Epoch: 124
Loss: 0.21245910972356796
RMSE train: 0.407536	val: 0.762216	test: 0.786209
MAE train: 0.315407	val: 0.581427	test: 0.607273

Epoch: 125
Loss: 0.1992095783352852
RMSE train: 0.401753	val: 0.781301	test: 0.806249
MAE train: 0.313369	val: 0.591204	test: 0.618275

Epoch: 126
Loss: 0.1955236539244652
RMSE train: 0.396531	val: 0.778636	test: 0.797480
MAE train: 0.306947	val: 0.588942	test: 0.616985

Epoch: 127
Loss: 0.20187672972679138
RMSE train: 0.390083	val: 0.780359	test: 0.804527
MAE train: 0.302752	val: 0.591175	test: 0.617093

Epoch: 128
Loss: 0.20673577189445497
RMSE train: 0.393599	val: 0.779495	test: 0.780438
MAE train: 0.302617	val: 0.589157	test: 0.604704

Epoch: 129
Loss: 0.19433440864086152
RMSE train: 0.399817	val: 0.784069	test: 0.806381
MAE train: 0.310814	val: 0.593537	test: 0.619321

Epoch: 130
Loss: 0.19164759516716004
RMSE train: 0.387112	val: 0.769240	test: 0.789139
MAE train: 0.300878	val: 0.578310	test: 0.605988

Epoch: 131
Loss: 0.19933101683855056
RMSE train: 0.387396	val: 0.766646	test: 0.771603
MAE train: 0.298361	val: 0.575893	test: 0.596201

Epoch: 132
Loss: 0.18951033502817155
RMSE train: 0.383267	val: 0.770134	test: 0.780072
MAE train: 0.295360	val: 0.582591	test: 0.598149

Epoch: 133
Loss: 0.190840445458889
RMSE train: 0.386406	val: 0.761089	test: 0.780583
MAE train: 0.299686	val: 0.579183	test: 0.597458

Epoch: 134
Loss: 0.19540231078863143
RMSE train: 0.388059	val: 0.776108	test: 0.802135
MAE train: 0.302673	val: 0.589195	test: 0.613469

Epoch: 135
Loss: 0.19655346721410752
RMSE train: 0.405206	val: 0.783734	test: 0.818574
MAE train: 0.315914	val: 0.593460	test: 0.627342

Epoch: 136
Loss: 0.1890629306435585
RMSE train: 0.397667	val: 0.791647	test: 0.814574
MAE train: 0.309187	val: 0.597024	test: 0.621658

Epoch: 137
Loss: 0.19295916855335235
RMSE train: 0.393099	val: 0.780410	test: 0.796180
MAE train: 0.307221	val: 0.587768	test: 0.610951

Epoch: 138
Loss: 0.19209182858467103
RMSE train: 0.417232	val: 0.791141	test: 0.807053
MAE train: 0.327877	val: 0.597962	test: 0.624675

Epoch: 139
Loss: 0.19142418801784516
RMSE train: 0.393331	val: 0.776726	test: 0.806109
MAE train: 0.305990	val: 0.582715	test: 0.617265

Epoch: 140
Loss: 0.18779982924461364
RMSE train: 0.402081	val: 0.781174	test: 0.803928
MAE train: 0.313907	val: 0.593738	test: 0.623165

Epoch: 141
Loss: 0.19595654904842377
RMSE train: 0.370453	val: 0.747306	test: 0.773726
MAE train: 0.288132	val: 0.567415	test: 0.591961

Epoch: 142
Loss: 0.18523121476173401
RMSE train: 0.379497	val: 0.784441	test: 0.807958
MAE train: 0.295097	val: 0.594017	test: 0.621889

Epoch: 143
Loss: 0.186360165476799
RMSE train: 0.386008	val: 0.777307	test: 0.802059
MAE train: 0.301074	val: 0.588546	test: 0.619855
MAE train: 0.335929	val: 0.582634	test: 0.593262

Epoch: 84
Loss: 0.2547954559326172
RMSE train: 0.419621	val: 0.751612	test: 0.744079
MAE train: 0.320844	val: 0.572310	test: 0.574175

Epoch: 85
Loss: 0.25581920742988584
RMSE train: 0.459597	val: 0.795202	test: 0.789923
MAE train: 0.357921	val: 0.612892	test: 0.612748

Epoch: 86
Loss: 0.24451445937156677
RMSE train: 0.474531	val: 0.793696	test: 0.819275
MAE train: 0.367329	val: 0.613379	test: 0.628952

Epoch: 87
Loss: 0.26121933907270434
RMSE train: 0.412232	val: 0.758280	test: 0.768340
MAE train: 0.314803	val: 0.576992	test: 0.592498

Epoch: 88
Loss: 0.24640877395868302
RMSE train: 0.434064	val: 0.754207	test: 0.766603
MAE train: 0.341018	val: 0.583363	test: 0.593276

Epoch: 89
Loss: 0.2508371531963348
RMSE train: 0.416354	val: 0.755050	test: 0.767455
MAE train: 0.319376	val: 0.580786	test: 0.591729

Epoch: 90
Loss: 0.25593548864126203
RMSE train: 0.410563	val: 0.752586	test: 0.766754
MAE train: 0.318485	val: 0.573828	test: 0.589137

Epoch: 91
Loss: 0.2340836852788925
RMSE train: 0.430898	val: 0.766941	test: 0.774173
MAE train: 0.332216	val: 0.584682	test: 0.595578

Epoch: 92
Loss: 0.24983646273612975
RMSE train: 0.449545	val: 0.783762	test: 0.796767
MAE train: 0.350959	val: 0.598999	test: 0.612518

Epoch: 93
Loss: 0.23366895616054534
RMSE train: 0.408484	val: 0.755827	test: 0.763744
MAE train: 0.314017	val: 0.575354	test: 0.586159

Epoch: 94
Loss: 0.2331780031323433
RMSE train: 0.496656	val: 0.819109	test: 0.862199
MAE train: 0.391170	val: 0.630154	test: 0.659382

Epoch: 95
Loss: 0.2455831542611122
RMSE train: 0.424714	val: 0.759762	test: 0.771612
MAE train: 0.327952	val: 0.583445	test: 0.597063

Epoch: 96
Loss: 0.2371597930788994
RMSE train: 0.415933	val: 0.751774	test: 0.772525
MAE train: 0.321996	val: 0.576490	test: 0.587668

Epoch: 97
Loss: 0.23563650101423264
RMSE train: 0.405534	val: 0.747205	test: 0.764165
MAE train: 0.311537	val: 0.574878	test: 0.587161

Epoch: 98
Loss: 0.22780525237321853
RMSE train: 0.410649	val: 0.757417	test: 0.785053
MAE train: 0.317097	val: 0.582858	test: 0.600780

Epoch: 99
Loss: 0.22499935626983641
RMSE train: 0.423440	val: 0.764462	test: 0.786376
MAE train: 0.327963	val: 0.581384	test: 0.604522

Epoch: 100
Loss: 0.21997988522052764
RMSE train: 0.400407	val: 0.754154	test: 0.773675
MAE train: 0.308571	val: 0.573985	test: 0.590578

Epoch: 101
Loss: 0.23261941373348236
RMSE train: 0.425663	val: 0.762851	test: 0.781268
MAE train: 0.325211	val: 0.581087	test: 0.601088

Epoch: 102
Loss: 0.23241562098264695
RMSE train: 0.438450	val: 0.780081	test: 0.806503
MAE train: 0.343115	val: 0.591015	test: 0.614789

Epoch: 103
Loss: 0.219947013258934
RMSE train: 0.397781	val: 0.749776	test: 0.766344
MAE train: 0.305016	val: 0.569559	test: 0.585925

Epoch: 104
Loss: 0.22308235317468644
RMSE train: 0.433524	val: 0.782552	test: 0.798103
MAE train: 0.336264	val: 0.596752	test: 0.614041

Epoch: 105
Loss: 0.22295835614204407
RMSE train: 0.400683	val: 0.756892	test: 0.774996
MAE train: 0.309698	val: 0.574332	test: 0.590314

Epoch: 106
Loss: 0.21495441496372222
RMSE train: 0.395138	val: 0.753101	test: 0.781140
MAE train: 0.302082	val: 0.570929	test: 0.593769

Epoch: 107
Loss: 0.21719795614480972
RMSE train: 0.423513	val: 0.767070	test: 0.792815
MAE train: 0.329454	val: 0.582265	test: 0.603065

Epoch: 108
Loss: 0.21711576282978057
RMSE train: 0.416933	val: 0.769169	test: 0.791490
MAE train: 0.323957	val: 0.590095	test: 0.606067

Epoch: 109
Loss: 0.22003201246261597
RMSE train: 0.400599	val: 0.750736	test: 0.776642
MAE train: 0.310832	val: 0.569320	test: 0.600937

Epoch: 110
Loss: 0.21882542818784714
RMSE train: 0.403522	val: 0.763885	test: 0.774311
MAE train: 0.309560	val: 0.576941	test: 0.596570

Epoch: 111
Loss: 0.20584812611341477
RMSE train: 0.390276	val: 0.752233	test: 0.766390
MAE train: 0.301753	val: 0.571950	test: 0.587065

Epoch: 112
Loss: 0.21459026932716369
RMSE train: 0.391879	val: 0.755356	test: 0.770318
MAE train: 0.301459	val: 0.570505	test: 0.588571

Epoch: 113
Loss: 0.20532633364200592
RMSE train: 0.421220	val: 0.779275	test: 0.797562
MAE train: 0.328157	val: 0.595540	test: 0.612436

Epoch: 114
Loss: 0.20890268236398696
RMSE train: 0.401451	val: 0.752512	test: 0.776299
MAE train: 0.308494	val: 0.571430	test: 0.597108

Epoch: 115
Loss: 0.2193448066711426
RMSE train: 0.388021	val: 0.749424	test: 0.775912
MAE train: 0.297707	val: 0.571044	test: 0.595135

Epoch: 116
Loss: 0.20975219905376435
RMSE train: 0.378617	val: 0.755200	test: 0.776740
MAE train: 0.292163	val: 0.573801	test: 0.588140

Epoch: 117
Loss: 0.21626852154731752
RMSE train: 0.397419	val: 0.753753	test: 0.768223
MAE train: 0.307426	val: 0.573826	test: 0.590002

Epoch: 118
Loss: 0.20776012688875198
RMSE train: 0.386653	val: 0.747728	test: 0.771711
MAE train: 0.299717	val: 0.568607	test: 0.589465

Epoch: 119
Loss: 0.20601711720228194
RMSE train: 0.437998	val: 0.789976	test: 0.811776
MAE train: 0.345182	val: 0.603011	test: 0.619733

Epoch: 120
Loss: 0.2213246777653694
RMSE train: 0.412318	val: 0.766978	test: 0.799477
MAE train: 0.318999	val: 0.583581	test: 0.607354

Epoch: 121
Loss: 0.20745748281478882
RMSE train: 0.424869	val: 0.779629	test: 0.803026
MAE train: 0.331132	val: 0.590517	test: 0.616834

Epoch: 122
Loss: 0.20735324323177337
RMSE train: 0.383159	val: 0.744465	test: 0.760635
MAE train: 0.295066	val: 0.565277	test: 0.579908

Epoch: 123
Loss: 0.21063473969697952
RMSE train: 0.381630	val: 0.748137	test: 0.771600
MAE train: 0.296319	val: 0.567837	test: 0.589123

Epoch: 124
Loss: 0.20338243395090103
RMSE train: 0.436615	val: 0.783530	test: 0.811828
MAE train: 0.342298	val: 0.598100	test: 0.619567

Epoch: 125
Loss: 0.1984958052635193
RMSE train: 0.417829	val: 0.784180	test: 0.813505
MAE train: 0.327310	val: 0.596050	test: 0.624649

Epoch: 126
Loss: 0.2003609538078308
RMSE train: 0.415241	val: 0.772012	test: 0.792318
MAE train: 0.325188	val: 0.585797	test: 0.612567

Epoch: 127
Loss: 0.2072516933083534
RMSE train: 0.424862	val: 0.776850	test: 0.809810
MAE train: 0.331971	val: 0.590592	test: 0.618330

Epoch: 128
Loss: 0.19858088344335556
RMSE train: 0.389164	val: 0.760559	test: 0.784261
MAE train: 0.301139	val: 0.576324	test: 0.597284

Epoch: 129
Loss: 0.19718197882175445
RMSE train: 0.396136	val: 0.770959	test: 0.778245
MAE train: 0.309123	val: 0.583618	test: 0.601619

Epoch: 130
Loss: 0.19786013066768646
RMSE train: 0.385542	val: 0.740013	test: 0.767020
MAE train: 0.301212	val: 0.567784	test: 0.587303

Epoch: 131
Loss: 0.20237494707107545
RMSE train: 0.403178	val: 0.782480	test: 0.801764
MAE train: 0.312419	val: 0.592422	test: 0.615828

Epoch: 132
Loss: 0.19654761254787445
RMSE train: 0.435799	val: 0.796317	test: 0.810531
MAE train: 0.344304	val: 0.604724	test: 0.620098

Epoch: 133
Loss: 0.20093293339014054
RMSE train: 0.390057	val: 0.757199	test: 0.778572
MAE train: 0.303163	val: 0.573776	test: 0.596652

Epoch: 134
Loss: 0.18591613620519637
RMSE train: 0.385349	val: 0.759604	test: 0.784842
MAE train: 0.299360	val: 0.577125	test: 0.594308

Epoch: 135
Loss: 0.1975921779870987
RMSE train: 0.406791	val: 0.784889	test: 0.807160
MAE train: 0.315900	val: 0.595545	test: 0.620831

Epoch: 136
Loss: 0.1964458167552948
RMSE train: 0.393134	val: 0.772208	test: 0.802588
MAE train: 0.307607	val: 0.588246	test: 0.608903

Epoch: 137
Loss: 0.18763053119182588
RMSE train: 0.370482	val: 0.756023	test: 0.772358
MAE train: 0.287146	val: 0.575136	test: 0.588221

Epoch: 138
Loss: 0.18589147329330444
RMSE train: 0.363924	val: 0.748461	test: 0.774174
MAE train: 0.281274	val: 0.568414	test: 0.585313

Epoch: 139
Loss: 0.18601029813289643
RMSE train: 0.376872	val: 0.752006	test: 0.771006
MAE train: 0.295735	val: 0.570453	test: 0.587991

Epoch: 140
Loss: 0.1926806315779686
RMSE train: 0.374682	val: 0.751466	test: 0.769385
MAE train: 0.290251	val: 0.572868	test: 0.591825

Epoch: 141
Loss: 0.1856983333826065
RMSE train: 0.401711	val: 0.777248	test: 0.794799
MAE train: 0.315319	val: 0.591167	test: 0.611802

Epoch: 142
Loss: 0.19825272709131242
RMSE train: 0.397990	val: 0.760996	test: 0.780622
MAE train: 0.311068	val: 0.581890	test: 0.597481

Epoch: 143
Loss: 0.18835238814353944
RMSE train: 0.365797	val: 0.751505	test: 0.775917
MAE train: 0.281864	val: 0.570765	test: 0.591311
MAE train: 0.389785	val: 0.577887	test: 0.585876

Epoch: 84
Loss: 0.2961720313344683
RMSE train: 0.473483	val: 0.756042	test: 0.756162
MAE train: 0.364229	val: 0.570573	test: 0.588546

Epoch: 85
Loss: 0.27981326409748625
RMSE train: 0.490698	val: 0.789546	test: 0.772987
MAE train: 0.380321	val: 0.592096	test: 0.591680

Epoch: 86
Loss: 0.3024436278002603
RMSE train: 0.486194	val: 0.748107	test: 0.737316
MAE train: 0.376236	val: 0.569358	test: 0.571089

Epoch: 87
Loss: 0.2755992369992392
RMSE train: 0.634765	val: 0.876635	test: 0.851825
MAE train: 0.512628	val: 0.676495	test: 0.676010

Epoch: 88
Loss: 0.3036186099052429
RMSE train: 0.466686	val: 0.731703	test: 0.738984
MAE train: 0.363710	val: 0.556507	test: 0.574191

Epoch: 89
Loss: 0.2890919234071459
RMSE train: 0.503099	val: 0.784656	test: 0.764891
MAE train: 0.392827	val: 0.587553	test: 0.600806

Epoch: 90
Loss: 0.29163546966654913
RMSE train: 0.496418	val: 0.772863	test: 0.748104
MAE train: 0.385311	val: 0.576410	test: 0.585770

Epoch: 91
Loss: 0.29217310888426645
RMSE train: 0.465372	val: 0.753122	test: 0.736920
MAE train: 0.361479	val: 0.572109	test: 0.578976

Epoch: 92
Loss: 0.2782752641609737
RMSE train: 0.471559	val: 0.770468	test: 0.760186
MAE train: 0.365932	val: 0.579866	test: 0.587972

Epoch: 93
Loss: 0.2973401950938361
RMSE train: 0.512202	val: 0.780300	test: 0.777485
MAE train: 0.395778	val: 0.590923	test: 0.610343

Epoch: 94
Loss: 0.27141507289239336
RMSE train: 0.464985	val: 0.757911	test: 0.732210
MAE train: 0.359246	val: 0.576426	test: 0.575428

Epoch: 95
Loss: 0.27048604296786444
RMSE train: 0.461182	val: 0.749803	test: 0.741061
MAE train: 0.356137	val: 0.565116	test: 0.574803

Epoch: 96
Loss: 0.27251764812639784
RMSE train: 0.458391	val: 0.744734	test: 0.736621
MAE train: 0.354190	val: 0.558370	test: 0.573260

Epoch: 97
Loss: 0.2640116901269981
RMSE train: 0.496268	val: 0.789731	test: 0.792661
MAE train: 0.385241	val: 0.599854	test: 0.624464

Epoch: 98
Loss: 0.2759214405502592
RMSE train: 0.439684	val: 0.734569	test: 0.726052
MAE train: 0.337507	val: 0.561787	test: 0.562063

Epoch: 99
Loss: 0.26571898055928095
RMSE train: 0.460731	val: 0.743597	test: 0.728444
MAE train: 0.357548	val: 0.564013	test: 0.570636

Epoch: 100
Loss: 0.26796902822596685
RMSE train: 0.453676	val: 0.741469	test: 0.734339
MAE train: 0.351010	val: 0.556126	test: 0.572313

Epoch: 101
Loss: 0.2701759050999369
RMSE train: 0.498401	val: 0.772594	test: 0.751964
MAE train: 0.387803	val: 0.582905	test: 0.588782

Epoch: 102
Loss: 0.2771422469190189
RMSE train: 0.464653	val: 0.771735	test: 0.749130
MAE train: 0.359419	val: 0.580005	test: 0.589890

Epoch: 103
Loss: 0.2810517368572099
RMSE train: 0.445832	val: 0.756440	test: 0.730431
MAE train: 0.345509	val: 0.563875	test: 0.564505

Epoch: 104
Loss: 0.28039420928273884
RMSE train: 0.486846	val: 0.780940	test: 0.760943
MAE train: 0.377841	val: 0.592745	test: 0.598202

Epoch: 105
Loss: 0.27677392427410397
RMSE train: 0.454429	val: 0.761871	test: 0.748352
MAE train: 0.350862	val: 0.573355	test: 0.581850

Epoch: 106
Loss: 0.2743372395634651
RMSE train: 0.494196	val: 0.792583	test: 0.765353
MAE train: 0.387662	val: 0.598067	test: 0.606270

Epoch: 107
Loss: 0.26046413076775415
RMSE train: 0.465403	val: 0.764981	test: 0.739265
MAE train: 0.359733	val: 0.580421	test: 0.571746

Epoch: 108
Loss: 0.27891259640455246
RMSE train: 0.465860	val: 0.773792	test: 0.749108
MAE train: 0.364318	val: 0.583872	test: 0.582879

Epoch: 109
Loss: 0.27794339614255087
RMSE train: 0.461084	val: 0.769035	test: 0.751296
MAE train: 0.360161	val: 0.580287	test: 0.581285

Epoch: 110
Loss: 0.2565669896347182
RMSE train: 0.476864	val: 0.784393	test: 0.754272
MAE train: 0.368512	val: 0.586857	test: 0.593226

Epoch: 111
Loss: 0.2772923782467842
RMSE train: 0.447498	val: 0.737633	test: 0.744620
MAE train: 0.344748	val: 0.557609	test: 0.581505

Epoch: 112
Loss: 0.27535139130694525
RMSE train: 0.485147	val: 0.796872	test: 0.798208
MAE train: 0.373198	val: 0.591406	test: 0.614154

Epoch: 113
Loss: 0.2750801805938993
RMSE train: 0.520973	val: 0.812507	test: 0.799227
MAE train: 0.407500	val: 0.615827	test: 0.627971

Epoch: 114
Loss: 0.27543156700474875
RMSE train: 0.497591	val: 0.797447	test: 0.770036
MAE train: 0.388676	val: 0.608384	test: 0.611355

Epoch: 115
Loss: 0.26249407125370844
RMSE train: 0.481258	val: 0.784556	test: 0.762712
MAE train: 0.375953	val: 0.590117	test: 0.599154

Epoch: 116
Loss: 0.2590684731091772
RMSE train: 0.484252	val: 0.783598	test: 0.762190
MAE train: 0.376150	val: 0.594408	test: 0.594121

Epoch: 117
Loss: 0.26093497020857676
RMSE train: 0.462820	val: 0.751967	test: 0.751141
MAE train: 0.358361	val: 0.570197	test: 0.593075

Epoch: 118
Loss: 0.25304886379412245
RMSE train: 0.433080	val: 0.761125	test: 0.763254
MAE train: 0.335230	val: 0.575122	test: 0.586926

Epoch: 119
Loss: 0.28129720794303076
RMSE train: 0.435633	val: 0.744697	test: 0.740802
MAE train: 0.336640	val: 0.562000	test: 0.576583

Epoch: 120
Loss: 0.25210232606955935
RMSE train: 0.443316	val: 0.766395	test: 0.753463
MAE train: 0.340108	val: 0.571601	test: 0.584664

Epoch: 121
Loss: 0.25314917841127943
RMSE train: 0.444650	val: 0.757373	test: 0.743828
MAE train: 0.348266	val: 0.564619	test: 0.575288

Epoch: 122
Loss: 0.27600627818277906
RMSE train: 0.567637	val: 0.851093	test: 0.820888
MAE train: 0.445216	val: 0.646061	test: 0.651055

Epoch: 123
Loss: 0.24828367573874338
RMSE train: 0.479494	val: 0.782654	test: 0.779534
MAE train: 0.373075	val: 0.584744	test: 0.605945

Early stopping
Best (RMSE):	 train: 0.466686	val: 0.731703	test: 0.738984
Best (MAE):	 train: 0.363710	val: 0.556507	test: 0.574191

MAE train: 0.347991	val: 0.590492	test: 0.578332

Epoch: 84
Loss: 0.2640695944428444
RMSE train: 0.498710	val: 0.818035	test: 0.767636
MAE train: 0.387573	val: 0.615243	test: 0.602192

Epoch: 85
Loss: 0.2598135521014531
RMSE train: 0.447920	val: 0.787510	test: 0.752317
MAE train: 0.347223	val: 0.590017	test: 0.589384

Epoch: 86
Loss: 0.2634931690990925
RMSE train: 0.434406	val: 0.767638	test: 0.726181
MAE train: 0.336031	val: 0.575661	test: 0.568147

Epoch: 87
Loss: 0.26145628094673157
RMSE train: 0.486891	val: 0.796938	test: 0.752150
MAE train: 0.376267	val: 0.599055	test: 0.593311

Epoch: 88
Loss: 0.2588844361404578
RMSE train: 0.434815	val: 0.764582	test: 0.723366
MAE train: 0.336695	val: 0.574337	test: 0.562438

Epoch: 89
Loss: 0.25432557240128517
RMSE train: 0.457994	val: 0.778073	test: 0.751446
MAE train: 0.355289	val: 0.588194	test: 0.590100

Epoch: 90
Loss: 0.2571477418144544
RMSE train: 0.441391	val: 0.769218	test: 0.729605
MAE train: 0.339331	val: 0.578888	test: 0.568214

Epoch: 91
Loss: 0.24210361888011298
RMSE train: 0.494627	val: 0.816599	test: 0.777939
MAE train: 0.380696	val: 0.616104	test: 0.613031

Epoch: 92
Loss: 0.25074752047657967
RMSE train: 0.456633	val: 0.791181	test: 0.760501
MAE train: 0.356892	val: 0.590592	test: 0.595532

Epoch: 93
Loss: 0.24809671565890312
RMSE train: 0.424258	val: 0.763594	test: 0.734546
MAE train: 0.328245	val: 0.573061	test: 0.572314

Epoch: 94
Loss: 0.2501534012456735
RMSE train: 0.452490	val: 0.797139	test: 0.768488
MAE train: 0.352489	val: 0.594204	test: 0.598892

Epoch: 95
Loss: 0.25550995394587517
RMSE train: 0.480535	val: 0.803096	test: 0.772658
MAE train: 0.376766	val: 0.600840	test: 0.608254

Epoch: 96
Loss: 0.23469176764289537
RMSE train: 0.460885	val: 0.785072	test: 0.746131
MAE train: 0.357102	val: 0.588778	test: 0.591504

Epoch: 97
Loss: 0.23815797020991644
RMSE train: 0.435779	val: 0.776755	test: 0.735404
MAE train: 0.338565	val: 0.580876	test: 0.574710

Epoch: 98
Loss: 0.24090446159243584
RMSE train: 0.410729	val: 0.755400	test: 0.713991
MAE train: 0.318383	val: 0.560458	test: 0.554998

Epoch: 99
Loss: 0.24775625889499983
RMSE train: 0.443353	val: 0.791704	test: 0.759999
MAE train: 0.346485	val: 0.591121	test: 0.592216

Epoch: 100
Loss: 0.2366566484173139
RMSE train: 0.469491	val: 0.803549	test: 0.773461
MAE train: 0.369659	val: 0.602957	test: 0.606594

Epoch: 101
Loss: 0.23829733456174532
RMSE train: 0.446658	val: 0.783958	test: 0.757256
MAE train: 0.345572	val: 0.591388	test: 0.596361

Epoch: 102
Loss: 0.22682911654313406
RMSE train: 0.454684	val: 0.806203	test: 0.756914
MAE train: 0.355712	val: 0.600980	test: 0.590904

Epoch: 103
Loss: 0.24621635923782983
RMSE train: 0.443942	val: 0.778098	test: 0.740286
MAE train: 0.346488	val: 0.581532	test: 0.578507

Epoch: 104
Loss: 0.24231824154655138
RMSE train: 0.427170	val: 0.769101	test: 0.745625
MAE train: 0.331458	val: 0.573545	test: 0.589605

Epoch: 105
Loss: 0.2440946102142334
RMSE train: 0.423759	val: 0.768024	test: 0.742292
MAE train: 0.328209	val: 0.573941	test: 0.578114

Epoch: 106
Loss: 0.23019721607367197
RMSE train: 0.499950	val: 0.832364	test: 0.792268
MAE train: 0.396225	val: 0.631327	test: 0.625888

Epoch: 107
Loss: 0.2339976541697979
RMSE train: 0.441877	val: 0.775083	test: 0.738231
MAE train: 0.343762	val: 0.579445	test: 0.581527

Epoch: 108
Loss: 0.22282843415935835
RMSE train: 0.437937	val: 0.792348	test: 0.763408
MAE train: 0.342171	val: 0.597649	test: 0.597746

Epoch: 109
Loss: 0.23476387684543928
RMSE train: 0.432598	val: 0.781487	test: 0.743752
MAE train: 0.335412	val: 0.582602	test: 0.579722

Epoch: 110
Loss: 0.2255852371454239
RMSE train: 0.433467	val: 0.785700	test: 0.755756
MAE train: 0.335929	val: 0.588741	test: 0.597850

Epoch: 111
Loss: 0.22167833397785822
RMSE train: 0.423883	val: 0.776381	test: 0.752111
MAE train: 0.329071	val: 0.585096	test: 0.588006

Epoch: 112
Loss: 0.22773480663696924
RMSE train: 0.423370	val: 0.778836	test: 0.747708
MAE train: 0.330333	val: 0.583601	test: 0.590313

Epoch: 113
Loss: 0.2129377263287703
RMSE train: 0.461259	val: 0.832455	test: 0.781564
MAE train: 0.362929	val: 0.627518	test: 0.611922

Epoch: 114
Loss: 0.22491559758782387
RMSE train: 0.436534	val: 0.783143	test: 0.738622
MAE train: 0.338985	val: 0.584184	test: 0.585995

Epoch: 115
Loss: 0.22936793292562166
RMSE train: 0.416440	val: 0.784968	test: 0.739296
MAE train: 0.324540	val: 0.583566	test: 0.572465

Epoch: 116
Loss: 0.23028291761875153
RMSE train: 0.429342	val: 0.794539	test: 0.760958
MAE train: 0.333321	val: 0.592144	test: 0.589957

Epoch: 117
Loss: 0.22234958906968436
RMSE train: 0.447960	val: 0.815688	test: 0.764382
MAE train: 0.346017	val: 0.606203	test: 0.597898

Epoch: 118
Loss: 0.21875922878583273
RMSE train: 0.414384	val: 0.774752	test: 0.742879
MAE train: 0.321108	val: 0.580413	test: 0.583978

Epoch: 119
Loss: 0.23191381866733232
RMSE train: 0.459531	val: 0.824990	test: 0.771822
MAE train: 0.361134	val: 0.615566	test: 0.601353

Epoch: 120
Loss: 0.21903674801190695
RMSE train: 0.441723	val: 0.822520	test: 0.765550
MAE train: 0.341875	val: 0.613735	test: 0.595818

Epoch: 121
Loss: 0.2158802847067515
RMSE train: 0.422424	val: 0.791017	test: 0.740855
MAE train: 0.329086	val: 0.594698	test: 0.576790

Epoch: 122
Loss: 0.20765299101670584
RMSE train: 0.460146	val: 0.798033	test: 0.752737
MAE train: 0.355561	val: 0.600438	test: 0.589301

Epoch: 123
Loss: 0.2258515196541945
RMSE train: 0.444443	val: 0.791275	test: 0.755999
MAE train: 0.342397	val: 0.597264	test: 0.590240

Epoch: 124
Loss: 0.20490103835860887
RMSE train: 0.414915	val: 0.781333	test: 0.743751
MAE train: 0.324581	val: 0.586245	test: 0.577713

Epoch: 125
Loss: 0.21268683423598608
RMSE train: 0.402795	val: 0.786357	test: 0.745329
MAE train: 0.310594	val: 0.582160	test: 0.580490

Epoch: 126
Loss: 0.21468116715550423
RMSE train: 0.400485	val: 0.775689	test: 0.731857
MAE train: 0.310099	val: 0.578401	test: 0.571450

Epoch: 127
Loss: 0.21211866786082587
RMSE train: 0.424622	val: 0.795361	test: 0.744468
MAE train: 0.330666	val: 0.592227	test: 0.579832

Epoch: 128
Loss: 0.2142406441271305
RMSE train: 0.443030	val: 0.799230	test: 0.766577
MAE train: 0.346153	val: 0.598915	test: 0.600633

Epoch: 129
Loss: 0.21635475133856139
RMSE train: 0.492281	val: 0.873011	test: 0.817565
MAE train: 0.387968	val: 0.659271	test: 0.636675

Epoch: 130
Loss: 0.2079244131843249
RMSE train: 0.397218	val: 0.764769	test: 0.739784
MAE train: 0.307507	val: 0.571712	test: 0.579288

Epoch: 131
Loss: 0.20328346764047941
RMSE train: 0.399681	val: 0.774944	test: 0.743022
MAE train: 0.310534	val: 0.577308	test: 0.584141

Epoch: 132
Loss: 0.2061559297144413
RMSE train: 0.412089	val: 0.801352	test: 0.761246
MAE train: 0.322873	val: 0.596892	test: 0.593765

Epoch: 133
Loss: 0.20361780499418577
RMSE train: 0.420761	val: 0.793251	test: 0.747124
MAE train: 0.331108	val: 0.589448	test: 0.588527

Early stopping
Best (RMSE):	 train: 0.410729	val: 0.755400	test: 0.713991
Best (MAE):	 train: 0.318383	val: 0.560458	test: 0.554998

MAE train: 0.350407	val: 0.590335	test: 0.592870

Epoch: 84
Loss: 0.266842994838953
RMSE train: 0.434102	val: 0.757054	test: 0.725361
MAE train: 0.333863	val: 0.567293	test: 0.570097

Epoch: 85
Loss: 0.25026576841870946
RMSE train: 0.425905	val: 0.753043	test: 0.727884
MAE train: 0.327327	val: 0.561482	test: 0.565346

Epoch: 86
Loss: 0.2565435568491618
RMSE train: 0.475256	val: 0.792473	test: 0.765028
MAE train: 0.366247	val: 0.598138	test: 0.596893

Epoch: 87
Loss: 0.26730596646666527
RMSE train: 0.470881	val: 0.803978	test: 0.774481
MAE train: 0.364467	val: 0.606731	test: 0.607972

Epoch: 88
Loss: 0.24483444914221764
RMSE train: 0.432484	val: 0.774750	test: 0.748548
MAE train: 0.332323	val: 0.578537	test: 0.582081

Epoch: 89
Loss: 0.2612416011591752
RMSE train: 0.423236	val: 0.757003	test: 0.726080
MAE train: 0.328157	val: 0.559561	test: 0.564650

Epoch: 90
Loss: 0.2587952179213365
RMSE train: 0.449109	val: 0.779526	test: 0.753145
MAE train: 0.346167	val: 0.586326	test: 0.592627

Epoch: 91
Loss: 0.2619651108980179
RMSE train: 0.439479	val: 0.780279	test: 0.750610
MAE train: 0.336778	val: 0.587597	test: 0.591757

Epoch: 92
Loss: 0.25261955584088963
RMSE train: 0.421709	val: 0.766118	test: 0.736040
MAE train: 0.323894	val: 0.576888	test: 0.578827

Epoch: 93
Loss: 0.2502908992270629
RMSE train: 0.456146	val: 0.799712	test: 0.759304
MAE train: 0.354526	val: 0.598990	test: 0.594705

Epoch: 94
Loss: 0.23876329759756723
RMSE train: 0.447503	val: 0.786462	test: 0.756723
MAE train: 0.346082	val: 0.586843	test: 0.588537

Epoch: 95
Loss: 0.24188470343748728
RMSE train: 0.444834	val: 0.795531	test: 0.765938
MAE train: 0.347071	val: 0.591965	test: 0.597232

Epoch: 96
Loss: 0.24650811279813448
RMSE train: 0.508332	val: 0.834994	test: 0.806164
MAE train: 0.402878	val: 0.637384	test: 0.633003

Epoch: 97
Loss: 0.2491821882625421
RMSE train: 0.425361	val: 0.771454	test: 0.729813
MAE train: 0.329186	val: 0.569948	test: 0.567028

Epoch: 98
Loss: 0.24187187353769937
RMSE train: 0.424832	val: 0.771999	test: 0.742684
MAE train: 0.327658	val: 0.575606	test: 0.582171

Epoch: 99
Loss: 0.24845710396766663
RMSE train: 0.433087	val: 0.781174	test: 0.763277
MAE train: 0.333609	val: 0.585427	test: 0.595115

Epoch: 100
Loss: 0.2425058906277021
RMSE train: 0.446300	val: 0.796794	test: 0.763520
MAE train: 0.347422	val: 0.590784	test: 0.593926

Epoch: 101
Loss: 0.23531444494922957
RMSE train: 0.417758	val: 0.775043	test: 0.735768
MAE train: 0.323973	val: 0.570161	test: 0.573211

Epoch: 102
Loss: 0.23733087008198103
RMSE train: 0.421546	val: 0.774382	test: 0.751552
MAE train: 0.323981	val: 0.575938	test: 0.584660

Epoch: 103
Loss: 0.2365699658791224
RMSE train: 0.422992	val: 0.783085	test: 0.753039
MAE train: 0.327742	val: 0.585552	test: 0.583766

Epoch: 104
Loss: 0.2302958940466245
RMSE train: 0.425396	val: 0.757569	test: 0.729257
MAE train: 0.328992	val: 0.566547	test: 0.569395

Epoch: 105
Loss: 0.23230048517386118
RMSE train: 0.526399	val: 0.852399	test: 0.838967
MAE train: 0.422584	val: 0.653712	test: 0.657999

Epoch: 106
Loss: 0.22850947578748068
RMSE train: 0.424143	val: 0.765017	test: 0.728242
MAE train: 0.326062	val: 0.571688	test: 0.567631

Epoch: 107
Loss: 0.2399146817624569
RMSE train: 0.424849	val: 0.771054	test: 0.747913
MAE train: 0.330520	val: 0.580081	test: 0.583981

Epoch: 108
Loss: 0.2310954617957274
RMSE train: 0.419348	val: 0.752574	test: 0.722685
MAE train: 0.322430	val: 0.565892	test: 0.561349

Epoch: 109
Loss: 0.2195766530930996
RMSE train: 0.427725	val: 0.769791	test: 0.738164
MAE train: 0.330886	val: 0.575882	test: 0.576966

Epoch: 110
Loss: 0.23029285048445067
RMSE train: 0.412018	val: 0.761917	test: 0.724709
MAE train: 0.315715	val: 0.568867	test: 0.562853

Epoch: 111
Loss: 0.229399257649978
RMSE train: 0.471941	val: 0.820185	test: 0.793117
MAE train: 0.374702	val: 0.619383	test: 0.625610

Epoch: 112
Loss: 0.2134710984925429
RMSE train: 0.426566	val: 0.794215	test: 0.754932
MAE train: 0.331147	val: 0.591594	test: 0.588061

Epoch: 113
Loss: 0.22381570314367613
RMSE train: 0.428876	val: 0.784951	test: 0.754838
MAE train: 0.331990	val: 0.586163	test: 0.588087

Epoch: 114
Loss: 0.2194033165772756
RMSE train: 0.417030	val: 0.770938	test: 0.736262
MAE train: 0.322751	val: 0.572104	test: 0.578544

Epoch: 115
Loss: 0.2317380172510942
RMSE train: 0.404308	val: 0.766136	test: 0.741606
MAE train: 0.311942	val: 0.573978	test: 0.579619

Epoch: 116
Loss: 0.22532256444295248
RMSE train: 0.396354	val: 0.775682	test: 0.730954
MAE train: 0.305355	val: 0.578460	test: 0.570667

Epoch: 117
Loss: 0.22856027260422707
RMSE train: 0.433083	val: 0.796495	test: 0.767886
MAE train: 0.334717	val: 0.596409	test: 0.597527

Epoch: 118
Loss: 0.22641105701526007
RMSE train: 0.408398	val: 0.766434	test: 0.729048
MAE train: 0.316836	val: 0.566274	test: 0.573324

Epoch: 119
Loss: 0.22339913994073868
RMSE train: 0.410837	val: 0.779589	test: 0.750805
MAE train: 0.319116	val: 0.578922	test: 0.588125

Epoch: 120
Loss: 0.22075829903284708
RMSE train: 0.412927	val: 0.778085	test: 0.741566
MAE train: 0.322303	val: 0.582431	test: 0.582826

Epoch: 121
Loss: 0.2290745091934999
RMSE train: 0.417033	val: 0.773385	test: 0.742075
MAE train: 0.322543	val: 0.583706	test: 0.577558

Epoch: 122
Loss: 0.2333232176800569
RMSE train: 0.425309	val: 0.762437	test: 0.738985
MAE train: 0.331275	val: 0.571773	test: 0.579664

Epoch: 123
Loss: 0.22956006228923798
RMSE train: 0.430611	val: 0.773374	test: 0.752460
MAE train: 0.334276	val: 0.577150	test: 0.589375

Epoch: 124
Loss: 0.21066761141022047
RMSE train: 0.394901	val: 0.760037	test: 0.727977
MAE train: 0.300892	val: 0.563516	test: 0.560862

Epoch: 125
Loss: 0.2161386969188849
RMSE train: 0.425849	val: 0.792042	test: 0.758674
MAE train: 0.333433	val: 0.591048	test: 0.594332

Epoch: 126
Loss: 0.20081301406025887
RMSE train: 0.452874	val: 0.807937	test: 0.770904
MAE train: 0.355463	val: 0.609085	test: 0.601903

Epoch: 127
Loss: 0.20722118020057678
RMSE train: 0.400954	val: 0.765292	test: 0.733339
MAE train: 0.307956	val: 0.571392	test: 0.572418

Epoch: 128
Loss: 0.2101178914308548
RMSE train: 0.389571	val: 0.755979	test: 0.723889
MAE train: 0.300876	val: 0.562191	test: 0.564107

Epoch: 129
Loss: 0.21140280986825624
RMSE train: 0.406829	val: 0.773152	test: 0.747659
MAE train: 0.313532	val: 0.578315	test: 0.585664

Epoch: 130
Loss: 0.20851371934016547
RMSE train: 0.413406	val: 0.778975	test: 0.752675
MAE train: 0.320147	val: 0.582616	test: 0.594541

Epoch: 131
Loss: 0.21979385366042456
RMSE train: 0.453339	val: 0.828783	test: 0.784871
MAE train: 0.358957	val: 0.618986	test: 0.611762

Epoch: 132
Loss: 0.21592727303504944
RMSE train: 0.390658	val: 0.771716	test: 0.739409
MAE train: 0.303389	val: 0.568861	test: 0.575825

Epoch: 133
Loss: 0.2114190012216568
RMSE train: 0.412304	val: 0.792696	test: 0.754176
MAE train: 0.321247	val: 0.589019	test: 0.591711

Epoch: 134
Loss: 0.2079253929356734
RMSE train: 0.378292	val: 0.762781	test: 0.722005
MAE train: 0.292771	val: 0.564230	test: 0.563604

Epoch: 135
Loss: 0.2069904295106729
RMSE train: 0.472895	val: 0.833695	test: 0.794392
MAE train: 0.376225	val: 0.620693	test: 0.619998

Epoch: 136
Loss: 0.20207771907250086
RMSE train: 0.401084	val: 0.796936	test: 0.760870
MAE train: 0.313064	val: 0.585129	test: 0.588852

Epoch: 137
Loss: 0.18676585207382837
RMSE train: 0.400852	val: 0.755804	test: 0.715127
MAE train: 0.309720	val: 0.559214	test: 0.557360

Epoch: 138
Loss: 0.19681432098150253
RMSE train: 0.392051	val: 0.773014	test: 0.729949
MAE train: 0.303461	val: 0.573925	test: 0.567940

Epoch: 139
Loss: 0.205829290052255
RMSE train: 0.379208	val: 0.764476	test: 0.731038
MAE train: 0.292443	val: 0.560318	test: 0.569469

Epoch: 140
Loss: 0.20889337112506232
RMSE train: 0.405603	val: 0.807002	test: 0.760880
MAE train: 0.316554	val: 0.592864	test: 0.595540

Epoch: 141
Loss: 0.2064996249973774
RMSE train: 0.412751	val: 0.799459	test: 0.757993
MAE train: 0.323763	val: 0.596500	test: 0.588785

Epoch: 142
Loss: 0.1993161030113697
RMSE train: 0.375877	val: 0.758237	test: 0.723508
MAE train: 0.291871	val: 0.560012	test: 0.560744

Epoch: 143
Loss: 0.19564793507258096
RMSE train: 0.395941	val: 0.769285	test: 0.725610
MAE train: 0.306841	val: 0.569597	test: 0.570521

Early stopping
Best (RMSE):	 train: 0.419348	val: 0.752574	test: 0.722685
Best (MAE):	 train: 0.322430	val: 0.565892	test: 0.561349

MAE train: 0.329924	val: 0.583895	test: 0.578084

Epoch: 84
Loss: 0.2632380649447441
RMSE train: 0.425958	val: 0.782241	test: 0.741351
MAE train: 0.330025	val: 0.582114	test: 0.575031

Epoch: 85
Loss: 0.26910514757037163
RMSE train: 0.437254	val: 0.761954	test: 0.736153
MAE train: 0.337509	val: 0.573448	test: 0.571122

Epoch: 86
Loss: 0.2727540284395218
RMSE train: 0.467971	val: 0.805586	test: 0.772353
MAE train: 0.364913	val: 0.613872	test: 0.607279

Epoch: 87
Loss: 0.2545258179306984
RMSE train: 0.463447	val: 0.796577	test: 0.767092
MAE train: 0.360027	val: 0.600480	test: 0.598363

Epoch: 88
Loss: 0.2585337708393733
RMSE train: 0.453520	val: 0.786994	test: 0.762068
MAE train: 0.348603	val: 0.592798	test: 0.595977

Epoch: 89
Loss: 0.26279810070991516
RMSE train: 0.448924	val: 0.781424	test: 0.756758
MAE train: 0.348315	val: 0.588532	test: 0.592870

Epoch: 90
Loss: 0.2693696084121863
RMSE train: 0.430541	val: 0.775514	test: 0.748695
MAE train: 0.332865	val: 0.579840	test: 0.587501

Epoch: 91
Loss: 0.25543759390711784
RMSE train: 0.414481	val: 0.765672	test: 0.727103
MAE train: 0.319088	val: 0.569527	test: 0.559740

Epoch: 92
Loss: 0.2506913368900617
RMSE train: 0.439994	val: 0.780731	test: 0.752427
MAE train: 0.341232	val: 0.585568	test: 0.587970

Epoch: 93
Loss: 0.24252077812949815
RMSE train: 0.432844	val: 0.787776	test: 0.758988
MAE train: 0.334324	val: 0.592633	test: 0.593386

Epoch: 94
Loss: 0.24224272246162096
RMSE train: 0.411873	val: 0.754144	test: 0.719816
MAE train: 0.319739	val: 0.569130	test: 0.561665

Epoch: 95
Loss: 0.2506353283921878
RMSE train: 0.445617	val: 0.782746	test: 0.756455
MAE train: 0.342671	val: 0.590954	test: 0.589353

Epoch: 96
Loss: 0.24174305920799574
RMSE train: 0.453747	val: 0.800362	test: 0.777695
MAE train: 0.352364	val: 0.604089	test: 0.610036

Epoch: 97
Loss: 0.23883614937464395
RMSE train: 0.402084	val: 0.767463	test: 0.727679
MAE train: 0.310370	val: 0.577837	test: 0.564584

Epoch: 98
Loss: 0.2301971564690272
RMSE train: 0.432063	val: 0.789487	test: 0.760754
MAE train: 0.335118	val: 0.594438	test: 0.596951

Epoch: 99
Loss: 0.2430358131726583
RMSE train: 0.438541	val: 0.789078	test: 0.751329
MAE train: 0.341149	val: 0.593900	test: 0.585870

Epoch: 100
Loss: 0.2355175664027532
RMSE train: 0.418245	val: 0.767509	test: 0.748662
MAE train: 0.322430	val: 0.574597	test: 0.584312

Epoch: 101
Loss: 0.2359388346473376
RMSE train: 0.413707	val: 0.774300	test: 0.752878
MAE train: 0.322229	val: 0.574765	test: 0.588079

Epoch: 102
Loss: 0.23245473951101303
RMSE train: 0.421506	val: 0.772658	test: 0.751759
MAE train: 0.324788	val: 0.577800	test: 0.592700

Epoch: 103
Loss: 0.23349452267090479
RMSE train: 0.406917	val: 0.775495	test: 0.745918
MAE train: 0.315478	val: 0.577191	test: 0.581059

Epoch: 104
Loss: 0.23348240926861763
RMSE train: 0.414749	val: 0.753080	test: 0.732404
MAE train: 0.320323	val: 0.564201	test: 0.569005

Epoch: 105
Loss: 0.22415394832690558
RMSE train: 0.431269	val: 0.780096	test: 0.747805
MAE train: 0.335212	val: 0.580876	test: 0.579834

Epoch: 106
Loss: 0.24804299448927244
RMSE train: 0.438628	val: 0.800277	test: 0.768760
MAE train: 0.339451	val: 0.599793	test: 0.598553

Epoch: 107
Loss: 0.22770928964018822
RMSE train: 0.437684	val: 0.788693	test: 0.763002
MAE train: 0.340163	val: 0.590133	test: 0.595087

Epoch: 108
Loss: 0.23503809794783592
RMSE train: 0.422578	val: 0.786166	test: 0.757788
MAE train: 0.325763	val: 0.585793	test: 0.586941

Epoch: 109
Loss: 0.23003343492746353
RMSE train: 0.405268	val: 0.765852	test: 0.732248
MAE train: 0.315168	val: 0.572355	test: 0.568243

Epoch: 110
Loss: 0.2224332628150781
RMSE train: 0.402938	val: 0.776163	test: 0.748794
MAE train: 0.311953	val: 0.582022	test: 0.581804

Epoch: 111
Loss: 0.23246563598513603
RMSE train: 0.411864	val: 0.773348	test: 0.759622
MAE train: 0.317613	val: 0.574620	test: 0.587815

Epoch: 112
Loss: 0.22362320125102997
RMSE train: 0.422695	val: 0.788035	test: 0.765874
MAE train: 0.329788	val: 0.588669	test: 0.593514

Epoch: 113
Loss: 0.21639286230007806
RMSE train: 0.405914	val: 0.742050	test: 0.743421
MAE train: 0.314443	val: 0.556549	test: 0.570459

Epoch: 114
Loss: 0.21827835341294607
RMSE train: 0.425968	val: 0.777773	test: 0.766816
MAE train: 0.326768	val: 0.583880	test: 0.597386

Epoch: 115
Loss: 0.23620126023888588
RMSE train: 0.449024	val: 0.815853	test: 0.797007
MAE train: 0.352288	val: 0.608357	test: 0.622298

Epoch: 116
Loss: 0.22501699378093085
RMSE train: 0.439217	val: 0.792885	test: 0.768101
MAE train: 0.339944	val: 0.595043	test: 0.601189

Epoch: 117
Loss: 0.22165429964661598
RMSE train: 0.434507	val: 0.804531	test: 0.770345
MAE train: 0.338828	val: 0.605687	test: 0.600847

Epoch: 118
Loss: 0.2082997461160024
RMSE train: 0.387375	val: 0.760967	test: 0.736600
MAE train: 0.299532	val: 0.568173	test: 0.568456

Epoch: 119
Loss: 0.21690939863522848
RMSE train: 0.398293	val: 0.754942	test: 0.726315
MAE train: 0.307033	val: 0.564735	test: 0.559048

Epoch: 120
Loss: 0.21742919584115347
RMSE train: 0.454800	val: 0.814702	test: 0.791484
MAE train: 0.355740	val: 0.618201	test: 0.622073

Epoch: 121
Loss: 0.2187885952492555
RMSE train: 0.415779	val: 0.789447	test: 0.752191
MAE train: 0.324758	val: 0.586760	test: 0.581659

Epoch: 122
Loss: 0.2103195476035277
RMSE train: 0.471888	val: 0.821784	test: 0.797243
MAE train: 0.372521	val: 0.620986	test: 0.621987

Epoch: 123
Loss: 0.2083046001692613
RMSE train: 0.409175	val: 0.760290	test: 0.740947
MAE train: 0.319496	val: 0.567116	test: 0.574198

Epoch: 124
Loss: 0.2076720930635929
RMSE train: 0.378435	val: 0.754757	test: 0.734097
MAE train: 0.293584	val: 0.562002	test: 0.570256

Epoch: 125
Loss: 0.21180813883741698
RMSE train: 0.424309	val: 0.804999	test: 0.770567
MAE train: 0.330454	val: 0.598784	test: 0.596475

Epoch: 126
Loss: 0.2054098571340243
RMSE train: 0.400188	val: 0.772452	test: 0.741553
MAE train: 0.311472	val: 0.575192	test: 0.573025

Epoch: 127
Loss: 0.21576405937472978
RMSE train: 0.380022	val: 0.764271	test: 0.725959
MAE train: 0.293835	val: 0.570086	test: 0.564161

Epoch: 128
Loss: 0.205854844301939
RMSE train: 0.409898	val: 0.793707	test: 0.766005
MAE train: 0.318690	val: 0.590968	test: 0.591289

Epoch: 129
Loss: 0.20419069876273474
RMSE train: 0.413482	val: 0.793467	test: 0.753037
MAE train: 0.319457	val: 0.593173	test: 0.587682

Epoch: 130
Loss: 0.21060669794678688
RMSE train: 0.406414	val: 0.790730	test: 0.763857
MAE train: 0.318530	val: 0.588841	test: 0.583227

Epoch: 131
Loss: 0.2098129689693451
RMSE train: 0.378329	val: 0.760456	test: 0.732957
MAE train: 0.295334	val: 0.569256	test: 0.572361

Epoch: 132
Loss: 0.20487661908070245
RMSE train: 0.415121	val: 0.794263	test: 0.769719
MAE train: 0.325799	val: 0.590765	test: 0.600032

Epoch: 133
Loss: 0.21072737500071526
RMSE train: 0.417446	val: 0.804949	test: 0.768553
MAE train: 0.324760	val: 0.596634	test: 0.595452

Epoch: 134
Loss: 0.19663043320178986
RMSE train: 0.435055	val: 0.821451	test: 0.776161
MAE train: 0.340298	val: 0.609660	test: 0.605041

Epoch: 135
Loss: 0.19883348047733307
RMSE train: 0.375203	val: 0.768871	test: 0.740607
MAE train: 0.289320	val: 0.567990	test: 0.566953

Epoch: 136
Loss: 0.19903035337726274
RMSE train: 0.389167	val: 0.762604	test: 0.742493
MAE train: 0.302333	val: 0.572847	test: 0.580971

Epoch: 137
Loss: 0.1912901351849238
RMSE train: 0.375323	val: 0.775336	test: 0.748347
MAE train: 0.291644	val: 0.576775	test: 0.578342

Epoch: 138
Loss: 0.1989780031144619
RMSE train: 0.399605	val: 0.788960	test: 0.771713
MAE train: 0.308188	val: 0.583500	test: 0.593706

Epoch: 139
Loss: 0.20595324163635573
RMSE train: 0.386069	val: 0.784410	test: 0.761086
MAE train: 0.297463	val: 0.576086	test: 0.588341

Epoch: 140
Loss: 0.19161143650611243
RMSE train: 0.410975	val: 0.786738	test: 0.765531
MAE train: 0.318376	val: 0.587154	test: 0.598049

Epoch: 141
Loss: 0.1927413928012053
RMSE train: 0.402871	val: 0.777504	test: 0.750867
MAE train: 0.313145	val: 0.579919	test: 0.593043

Epoch: 142
Loss: 0.20353787764906883
RMSE train: 0.397534	val: 0.788130	test: 0.761025
MAE train: 0.311784	val: 0.585923	test: 0.589999

Epoch: 143
Loss: 0.19709955528378487
RMSE train: 0.365222	val: 0.777118	test: 0.728014
MAE train: 0.285291	val: 0.570729	test: 0.559168

Epoch: 144
Loss: 0.1866898864507675
RMSE train: 0.393494	val: 0.774794	test: 0.801665
MAE train: 0.308596	val: 0.590143	test: 0.615520

Epoch: 145
Loss: 0.1827733561396599
RMSE train: 0.394928	val: 0.775568	test: 0.799680
MAE train: 0.307933	val: 0.586607	test: 0.608116

Epoch: 146
Loss: 0.187446428835392
RMSE train: 0.358872	val: 0.753920	test: 0.771416
MAE train: 0.277143	val: 0.569502	test: 0.587045

Epoch: 147
Loss: 0.17921715676784516
RMSE train: 0.377167	val: 0.741295	test: 0.765137
MAE train: 0.295030	val: 0.567678	test: 0.584421

Epoch: 148
Loss: 0.18136211186647416
RMSE train: 0.449579	val: 0.817136	test: 0.842007
MAE train: 0.358822	val: 0.626902	test: 0.648601

Epoch: 149
Loss: 0.18205703496932985
RMSE train: 0.366675	val: 0.752614	test: 0.776364
MAE train: 0.283363	val: 0.572807	test: 0.588442

Epoch: 150
Loss: 0.18359047472476958
RMSE train: 0.362217	val: 0.753676	test: 0.783331
MAE train: 0.280308	val: 0.569856	test: 0.596231

Epoch: 151
Loss: 0.18814308643341066
RMSE train: 0.379759	val: 0.746360	test: 0.760841
MAE train: 0.295713	val: 0.572443	test: 0.581801

Epoch: 152
Loss: 0.18505835980176927
RMSE train: 0.390708	val: 0.783905	test: 0.820785
MAE train: 0.306536	val: 0.595866	test: 0.627517

Epoch: 153
Loss: 0.18313905149698256
RMSE train: 0.353515	val: 0.740334	test: 0.765203
MAE train: 0.274000	val: 0.564517	test: 0.585676

Epoch: 154
Loss: 0.17368754744529724
RMSE train: 0.365315	val: 0.754872	test: 0.786439
MAE train: 0.282709	val: 0.569421	test: 0.597438

Epoch: 155
Loss: 0.18070243000984193
RMSE train: 0.384365	val: 0.761670	test: 0.790932
MAE train: 0.297457	val: 0.580319	test: 0.602101

Epoch: 156
Loss: 0.17485546171665192
RMSE train: 0.390186	val: 0.789208	test: 0.810312
MAE train: 0.307188	val: 0.600135	test: 0.617367

Epoch: 157
Loss: 0.1798950433731079
RMSE train: 0.372380	val: 0.777035	test: 0.792522
MAE train: 0.290879	val: 0.586960	test: 0.602069

Epoch: 158
Loss: 0.17295312732458115
RMSE train: 0.362489	val: 0.744346	test: 0.756307
MAE train: 0.280501	val: 0.569993	test: 0.575179

Epoch: 159
Loss: 0.17757190763950348
RMSE train: 0.375105	val: 0.769671	test: 0.787122
MAE train: 0.295279	val: 0.581757	test: 0.601525

Epoch: 160
Loss: 0.17324847877025604
RMSE train: 0.419730	val: 0.800927	test: 0.823639
MAE train: 0.331435	val: 0.611034	test: 0.626989

Epoch: 161
Loss: 0.19017620533704757
RMSE train: 0.424148	val: 0.794880	test: 0.820157
MAE train: 0.335416	val: 0.603796	test: 0.630102

Epoch: 162
Loss: 0.1872722774744034
RMSE train: 0.372286	val: 0.742943	test: 0.770242
MAE train: 0.289399	val: 0.567917	test: 0.589851

Epoch: 163
Loss: 0.174058398604393
RMSE train: 0.382807	val: 0.769583	test: 0.801067
MAE train: 0.296150	val: 0.585354	test: 0.608177

Epoch: 164
Loss: 0.17256789058446884
RMSE train: 0.366793	val: 0.753798	test: 0.770193
MAE train: 0.284644	val: 0.568596	test: 0.591895

Epoch: 165
Loss: 0.16721171736717225
RMSE train: 0.398312	val: 0.783131	test: 0.815547
MAE train: 0.312627	val: 0.596516	test: 0.622473

Early stopping
Best (RMSE):	 train: 0.385542	val: 0.740013	test: 0.767020
Best (MAE):	 train: 0.301212	val: 0.567784	test: 0.587303


Epoch: 144
Loss: 0.1871540608505408
RMSE train: 0.404326	val: 0.772010	test: 0.738794
MAE train: 0.310660	val: 0.577249	test: 0.578594

Epoch: 145
Loss: 0.19853390256563822
RMSE train: 0.397793	val: 0.785891	test: 0.743869
MAE train: 0.307240	val: 0.580888	test: 0.577923

Epoch: 146
Loss: 0.20183715969324112
RMSE train: 0.401517	val: 0.794443	test: 0.767802
MAE train: 0.314538	val: 0.587921	test: 0.596685

Epoch: 147
Loss: 0.19764490549763045
RMSE train: 0.381919	val: 0.788263	test: 0.759018
MAE train: 0.295502	val: 0.583102	test: 0.588081

Epoch: 148
Loss: 0.18526519586642584
RMSE train: 0.386854	val: 0.761266	test: 0.727438
MAE train: 0.299753	val: 0.565600	test: 0.567885

Early stopping
Best (RMSE):	 train: 0.405914	val: 0.742050	test: 0.743421
Best (MAE):	 train: 0.314443	val: 0.556549	test: 0.570459
All runs completed.

MAE train: 0.374570	val: 0.573392	test: 0.577999

Epoch: 84
Loss: 0.28690190613269806
RMSE train: 0.475184	val: 0.775222	test: 0.746273
MAE train: 0.370808	val: 0.575718	test: 0.571766

Epoch: 85
Loss: 0.3008498528173992
RMSE train: 0.443252	val: 0.758645	test: 0.736817
MAE train: 0.339881	val: 0.558317	test: 0.555182

Epoch: 86
Loss: 0.2823034014020647
RMSE train: 0.451809	val: 0.770868	test: 0.734135
MAE train: 0.347456	val: 0.573282	test: 0.547620

Epoch: 87
Loss: 0.2717337821211134
RMSE train: 0.449008	val: 0.759455	test: 0.734105
MAE train: 0.348508	val: 0.557521	test: 0.562602

Epoch: 88
Loss: 0.27574074055467335
RMSE train: 0.449520	val: 0.759003	test: 0.743188
MAE train: 0.347940	val: 0.555891	test: 0.569092

Epoch: 89
Loss: 0.28161073263202396
RMSE train: 0.462100	val: 0.767525	test: 0.745743
MAE train: 0.358751	val: 0.571741	test: 0.572676

Epoch: 90
Loss: 0.2805215801511492
RMSE train: 0.456672	val: 0.750127	test: 0.734186
MAE train: 0.354496	val: 0.553721	test: 0.566316

Epoch: 91
Loss: 0.2891697170478957
RMSE train: 0.457598	val: 0.770693	test: 0.739720
MAE train: 0.352446	val: 0.572517	test: 0.553005

Epoch: 92
Loss: 0.28496064777885166
RMSE train: 0.450985	val: 0.770784	test: 0.732197
MAE train: 0.347640	val: 0.562506	test: 0.562320

Epoch: 93
Loss: 0.27660293877124786
RMSE train: 0.449759	val: 0.746900	test: 0.737643
MAE train: 0.346957	val: 0.551135	test: 0.560150

Epoch: 94
Loss: 0.2716637562428202
RMSE train: 0.445093	val: 0.765406	test: 0.752443
MAE train: 0.342365	val: 0.562797	test: 0.565168

Epoch: 95
Loss: 0.2829556827034269
RMSE train: 0.437034	val: 0.744298	test: 0.718560
MAE train: 0.337020	val: 0.557099	test: 0.544601

Epoch: 96
Loss: 0.2821023485490254
RMSE train: 0.445248	val: 0.764854	test: 0.745118
MAE train: 0.344506	val: 0.566643	test: 0.558311

Epoch: 97
Loss: 0.2786250614694187
RMSE train: 0.452154	val: 0.762388	test: 0.732504
MAE train: 0.352161	val: 0.574334	test: 0.558466

Epoch: 98
Loss: 0.28282170423439573
RMSE train: 0.459234	val: 0.753487	test: 0.738139
MAE train: 0.357474	val: 0.563081	test: 0.566160

Epoch: 99
Loss: 0.2861412231411253
RMSE train: 0.445293	val: 0.769474	test: 0.740330
MAE train: 0.345202	val: 0.569517	test: 0.554833

Epoch: 100
Loss: 0.271948509982654
RMSE train: 0.450488	val: 0.753010	test: 0.755075
MAE train: 0.348232	val: 0.556607	test: 0.574196

Epoch: 101
Loss: 0.26226236458335606
RMSE train: 0.428253	val: 0.753151	test: 0.734017
MAE train: 0.331357	val: 0.555304	test: 0.554753

Epoch: 102
Loss: 0.27210954150983263
RMSE train: 0.450288	val: 0.746226	test: 0.740584
MAE train: 0.353410	val: 0.555290	test: 0.571050

Epoch: 103
Loss: 0.2862923006926264
RMSE train: 0.439453	val: 0.771469	test: 0.733538
MAE train: 0.340725	val: 0.577825	test: 0.545144

Epoch: 104
Loss: 0.27702619241816656
RMSE train: 0.461567	val: 0.782166	test: 0.769365
MAE train: 0.359281	val: 0.579503	test: 0.582296

Epoch: 105
Loss: 0.2683882915547916
RMSE train: 0.443584	val: 0.752469	test: 0.725057
MAE train: 0.345454	val: 0.561644	test: 0.551624

Epoch: 106
Loss: 0.2571330964565277
RMSE train: 0.460780	val: 0.761294	test: 0.748423
MAE train: 0.351743	val: 0.564424	test: 0.557278

Epoch: 107
Loss: 0.2595611757465771
RMSE train: 0.447589	val: 0.761596	test: 0.721014
MAE train: 0.346673	val: 0.570338	test: 0.553356

Epoch: 108
Loss: 0.24926065440688813
RMSE train: 0.436693	val: 0.758109	test: 0.733927
MAE train: 0.336560	val: 0.549996	test: 0.559517

Epoch: 109
Loss: 0.23787167029721396
RMSE train: 0.473391	val: 0.791636	test: 0.752291
MAE train: 0.371991	val: 0.589019	test: 0.569310

Epoch: 110
Loss: 0.2586097653423037
RMSE train: 0.426857	val: 0.745105	test: 0.729922
MAE train: 0.332576	val: 0.558530	test: 0.555462

Epoch: 111
Loss: 0.24624282121658325
RMSE train: 0.433857	val: 0.763998	test: 0.727913
MAE train: 0.335168	val: 0.566264	test: 0.547748

Epoch: 112
Loss: 0.24128020235470363
RMSE train: 0.415868	val: 0.737701	test: 0.735537
MAE train: 0.322279	val: 0.547456	test: 0.555024

Epoch: 113
Loss: 0.25645232519933153
RMSE train: 0.430695	val: 0.737642	test: 0.737842
MAE train: 0.333368	val: 0.551125	test: 0.557936

Epoch: 114
Loss: 0.2558378853968212
RMSE train: 0.426913	val: 0.741009	test: 0.722245
MAE train: 0.332057	val: 0.550122	test: 0.554351

Epoch: 115
Loss: 0.24131451972893306
RMSE train: 0.410839	val: 0.730208	test: 0.731911
MAE train: 0.318083	val: 0.540296	test: 0.548061

Epoch: 116
Loss: 0.2588987265314375
RMSE train: 0.433920	val: 0.752147	test: 0.735979
MAE train: 0.334003	val: 0.569054	test: 0.552420

Epoch: 117
Loss: 0.2550548996244158
RMSE train: 0.451990	val: 0.749165	test: 0.782288
MAE train: 0.350782	val: 0.561361	test: 0.588174

Epoch: 118
Loss: 0.25728348216840197
RMSE train: 0.454702	val: 0.780802	test: 0.753998
MAE train: 0.351770	val: 0.580067	test: 0.578377

Epoch: 119
Loss: 0.23352344653436116
RMSE train: 0.452794	val: 0.775338	test: 0.755303
MAE train: 0.355190	val: 0.566078	test: 0.573342

Epoch: 120
Loss: 0.24128101021051407
RMSE train: 0.403299	val: 0.738207	test: 0.729645
MAE train: 0.312712	val: 0.547587	test: 0.555265

Epoch: 121
Loss: 0.25996129640511106
RMSE train: 0.425780	val: 0.747291	test: 0.733168
MAE train: 0.330576	val: 0.557925	test: 0.551980

Epoch: 122
Loss: 0.24134051480463573
RMSE train: 0.439414	val: 0.763798	test: 0.741619
MAE train: 0.341354	val: 0.569624	test: 0.576441

Epoch: 123
Loss: 0.2507753020950726
RMSE train: 0.418274	val: 0.766266	test: 0.754979
MAE train: 0.323542	val: 0.564876	test: 0.563787

Epoch: 124
Loss: 0.23665009971175874
RMSE train: 0.417031	val: 0.759742	test: 0.739259
MAE train: 0.321176	val: 0.561223	test: 0.557166

Epoch: 125
Loss: 0.23590758655752456
RMSE train: 0.413319	val: 0.748910	test: 0.737485
MAE train: 0.321092	val: 0.555704	test: 0.557127

Epoch: 126
Loss: 0.23462756723165512
RMSE train: 0.404887	val: 0.740967	test: 0.725663
MAE train: 0.313193	val: 0.547145	test: 0.551896

Epoch: 127
Loss: 0.239072679408959
RMSE train: 0.423753	val: 0.750915	test: 0.738420
MAE train: 0.331116	val: 0.548455	test: 0.566152

Epoch: 128
Loss: 0.23025419669491903
RMSE train: 0.434360	val: 0.776288	test: 0.723612
MAE train: 0.337914	val: 0.575402	test: 0.553234

Epoch: 129
Loss: 0.23201993639980042
RMSE train: 0.416624	val: 0.760470	test: 0.747762
MAE train: 0.322128	val: 0.563052	test: 0.566084

Epoch: 130
Loss: 0.23302345935787475
RMSE train: 0.431920	val: 0.780224	test: 0.753241
MAE train: 0.336194	val: 0.582360	test: 0.574373

Epoch: 131
Loss: 0.23643007342304503
RMSE train: 0.423191	val: 0.755443	test: 0.727542
MAE train: 0.330552	val: 0.559868	test: 0.548877

Epoch: 132
Loss: 0.2368338874408177
RMSE train: 0.401970	val: 0.742838	test: 0.726550
MAE train: 0.311600	val: 0.557755	test: 0.556557

Epoch: 133
Loss: 0.2378757500222751
RMSE train: 0.404956	val: 0.757658	test: 0.728034
MAE train: 0.314891	val: 0.557739	test: 0.552341

Epoch: 134
Loss: 0.24560644158295222
RMSE train: 0.416276	val: 0.759741	test: 0.745580
MAE train: 0.325541	val: 0.557740	test: 0.547791

Epoch: 135
Loss: 0.24792596804244177
RMSE train: 0.467903	val: 0.780730	test: 0.761475
MAE train: 0.365469	val: 0.581827	test: 0.588692

Epoch: 136
Loss: 0.2286987943308694
RMSE train: 0.405109	val: 0.751794	test: 0.735459
MAE train: 0.311638	val: 0.556180	test: 0.547474

Epoch: 137
Loss: 0.24172486577715194
RMSE train: 0.396025	val: 0.745203	test: 0.733016
MAE train: 0.307174	val: 0.548917	test: 0.548369

Epoch: 138
Loss: 0.23533597801412856
RMSE train: 0.406615	val: 0.767319	test: 0.739513
MAE train: 0.314459	val: 0.564944	test: 0.560460

Epoch: 139
Loss: 0.2454451939889363
RMSE train: 0.411624	val: 0.766634	test: 0.732451
MAE train: 0.318597	val: 0.561786	test: 0.552876

Epoch: 140
Loss: 0.21587223133870534
RMSE train: 0.404756	val: 0.731934	test: 0.731378
MAE train: 0.316376	val: 0.543868	test: 0.559099

Epoch: 141
Loss: 0.22465564416987555
RMSE train: 0.394974	val: 0.749321	test: 0.728792
MAE train: 0.305003	val: 0.553973	test: 0.556312

Epoch: 142
Loss: 0.22436455211469106
RMSE train: 0.397089	val: 0.743755	test: 0.729417
MAE train: 0.306581	val: 0.552357	test: 0.555337

Epoch: 143
Loss: 0.21677671585764205
RMSE train: 0.416701	val: 0.765519	test: 0.742190
MAE train: 0.323137	val: 0.557593	test: 0.553951
MAE train: 0.375957	val: 0.572433	test: 0.574188

Epoch: 84
Loss: 0.289755506174905
RMSE train: 0.474052	val: 0.759558	test: 0.726688
MAE train: 0.362649	val: 0.569680	test: 0.557406

Epoch: 85
Loss: 0.3085210898092815
RMSE train: 0.487403	val: 0.768094	test: 0.739778
MAE train: 0.374878	val: 0.571372	test: 0.576392

Epoch: 86
Loss: 0.29713078694684164
RMSE train: 0.519177	val: 0.816768	test: 0.776218
MAE train: 0.407721	val: 0.613462	test: 0.605099

Epoch: 87
Loss: 0.28489202154534204
RMSE train: 0.476108	val: 0.768644	test: 0.761308
MAE train: 0.367528	val: 0.576393	test: 0.588238

Epoch: 88
Loss: 0.29222834748881205
RMSE train: 0.480243	val: 0.761964	test: 0.743292
MAE train: 0.371376	val: 0.574716	test: 0.571588

Epoch: 89
Loss: 0.27885792191539494
RMSE train: 0.457799	val: 0.744347	test: 0.728312
MAE train: 0.351916	val: 0.563286	test: 0.566225

Epoch: 90
Loss: 0.294940356697355
RMSE train: 0.518050	val: 0.817433	test: 0.788795
MAE train: 0.405270	val: 0.614914	test: 0.617461

Epoch: 91
Loss: 0.26803930529526304
RMSE train: 0.450384	val: 0.745799	test: 0.722668
MAE train: 0.346612	val: 0.559436	test: 0.547280

Epoch: 92
Loss: 0.27962739020586014
RMSE train: 0.506361	val: 0.793790	test: 0.768287
MAE train: 0.396179	val: 0.592199	test: 0.602775

Epoch: 93
Loss: 0.28667537442275454
RMSE train: 0.458793	val: 0.757467	test: 0.724431
MAE train: 0.353654	val: 0.565875	test: 0.563000

Epoch: 94
Loss: 0.27194753502096447
RMSE train: 0.458214	val: 0.758499	test: 0.725645
MAE train: 0.354012	val: 0.567943	test: 0.560386

Epoch: 95
Loss: 0.2739566053662981
RMSE train: 0.507118	val: 0.783307	test: 0.755658
MAE train: 0.397829	val: 0.589080	test: 0.602482

Epoch: 96
Loss: 0.28717347660235
RMSE train: 0.457420	val: 0.763735	test: 0.744377
MAE train: 0.354320	val: 0.561109	test: 0.568484

Epoch: 97
Loss: 0.26629426223891123
RMSE train: 0.491366	val: 0.789584	test: 0.759709
MAE train: 0.380507	val: 0.581291	test: 0.585421

Epoch: 98
Loss: 0.2933924389736993
RMSE train: 0.496726	val: 0.795322	test: 0.763714
MAE train: 0.383794	val: 0.599569	test: 0.602544

Epoch: 99
Loss: 0.28103807249239515
RMSE train: 0.477166	val: 0.765582	test: 0.758455
MAE train: 0.369131	val: 0.574406	test: 0.589311

Epoch: 100
Loss: 0.28677764322076527
RMSE train: 0.458484	val: 0.753480	test: 0.712704
MAE train: 0.353723	val: 0.568490	test: 0.543901

Epoch: 101
Loss: 0.2751381312097822
RMSE train: 0.455737	val: 0.754487	test: 0.728446
MAE train: 0.351577	val: 0.560493	test: 0.563384

Epoch: 102
Loss: 0.26985890737601687
RMSE train: 0.465198	val: 0.767925	test: 0.733118
MAE train: 0.361640	val: 0.575496	test: 0.568771

Epoch: 103
Loss: 0.27413334058863775
RMSE train: 0.453906	val: 0.751822	test: 0.718936
MAE train: 0.347736	val: 0.564336	test: 0.561520

Epoch: 104
Loss: 0.2730530542986734
RMSE train: 0.510845	val: 0.808960	test: 0.778648
MAE train: 0.403733	val: 0.604342	test: 0.608375

Epoch: 105
Loss: 0.2643695015992437
RMSE train: 0.443700	val: 0.753973	test: 0.749355
MAE train: 0.343962	val: 0.560068	test: 0.570651

Epoch: 106
Loss: 0.2729304720248495
RMSE train: 0.440844	val: 0.751417	test: 0.724029
MAE train: 0.339557	val: 0.559166	test: 0.556519

Epoch: 107
Loss: 0.2551970247711454
RMSE train: 0.514506	val: 0.802548	test: 0.770526
MAE train: 0.402150	val: 0.596228	test: 0.607828

Epoch: 108
Loss: 0.25132754019328524
RMSE train: 0.432565	val: 0.717653	test: 0.707578
MAE train: 0.332862	val: 0.542967	test: 0.543712

Epoch: 109
Loss: 0.24874170763151987
RMSE train: 0.439818	val: 0.747891	test: 0.724843
MAE train: 0.341167	val: 0.554908	test: 0.557673

Epoch: 110
Loss: 0.24549662321805954
RMSE train: 0.457736	val: 0.764245	test: 0.732253
MAE train: 0.351827	val: 0.571311	test: 0.570058

Epoch: 111
Loss: 0.24468923253672464
RMSE train: 0.438934	val: 0.767180	test: 0.737164
MAE train: 0.337430	val: 0.567487	test: 0.569733

Epoch: 112
Loss: 0.2662929104907172
RMSE train: 0.450239	val: 0.782345	test: 0.747809
MAE train: 0.350020	val: 0.582176	test: 0.580810

Epoch: 113
Loss: 0.2475620305963925
RMSE train: 0.466297	val: 0.772595	test: 0.739506
MAE train: 0.365147	val: 0.584585	test: 0.579999

Epoch: 114
Loss: 0.2620079911180905
RMSE train: 0.441585	val: 0.767176	test: 0.720256
MAE train: 0.341703	val: 0.578264	test: 0.554271

Epoch: 115
Loss: 0.2509370648435184
RMSE train: 0.450523	val: 0.761067	test: 0.719995
MAE train: 0.350381	val: 0.572600	test: 0.559205

Epoch: 116
Loss: 0.2533380261489323
RMSE train: 0.447152	val: 0.770080	test: 0.741146
MAE train: 0.347856	val: 0.573606	test: 0.572534

Epoch: 117
Loss: 0.2555647130523409
RMSE train: 0.519841	val: 0.837490	test: 0.785564
MAE train: 0.413549	val: 0.634448	test: 0.617128

Epoch: 118
Loss: 0.2828610784241131
RMSE train: 0.443613	val: 0.745146	test: 0.726014
MAE train: 0.343092	val: 0.557593	test: 0.559410

Epoch: 119
Loss: 0.24934794860226767
RMSE train: 0.435365	val: 0.743954	test: 0.717395
MAE train: 0.335263	val: 0.550137	test: 0.560554

Epoch: 120
Loss: 0.24224559856312616
RMSE train: 0.429668	val: 0.736173	test: 0.724968
MAE train: 0.333808	val: 0.552205	test: 0.559989

Epoch: 121
Loss: 0.25586412101984024
RMSE train: 0.435397	val: 0.741034	test: 0.721528
MAE train: 0.334789	val: 0.554573	test: 0.560896

Epoch: 122
Loss: 0.24539232679775783
RMSE train: 0.429848	val: 0.743106	test: 0.723944
MAE train: 0.331583	val: 0.561475	test: 0.565172

Epoch: 123
Loss: 0.2467150805251939
RMSE train: 0.432703	val: 0.753186	test: 0.720919
MAE train: 0.335042	val: 0.564886	test: 0.558416

Epoch: 124
Loss: 0.25480434724262785
RMSE train: 0.458946	val: 0.776526	test: 0.740947
MAE train: 0.358889	val: 0.576661	test: 0.570625

Epoch: 125
Loss: 0.24082744121551514
RMSE train: 0.441569	val: 0.765395	test: 0.725921
MAE train: 0.343191	val: 0.573603	test: 0.573404

Epoch: 126
Loss: 0.2428423443010875
RMSE train: 0.443503	val: 0.754956	test: 0.717961
MAE train: 0.342796	val: 0.567081	test: 0.561070

Epoch: 127
Loss: 0.24647290046725953
RMSE train: 0.439612	val: 0.772039	test: 0.728483
MAE train: 0.341757	val: 0.574219	test: 0.566515

Epoch: 128
Loss: 0.2424528385911669
RMSE train: 0.454221	val: 0.766191	test: 0.743702
MAE train: 0.354262	val: 0.569063	test: 0.582689

Epoch: 129
Loss: 0.2639605573245457
RMSE train: 0.448228	val: 0.745693	test: 0.729385
MAE train: 0.344615	val: 0.567916	test: 0.561402

Epoch: 130
Loss: 0.2504723221063614
RMSE train: 0.518278	val: 0.831869	test: 0.797576
MAE train: 0.410627	val: 0.628087	test: 0.630711

Epoch: 131
Loss: 0.23541462314980371
RMSE train: 0.409645	val: 0.745251	test: 0.720712
MAE train: 0.314416	val: 0.557979	test: 0.545137

Epoch: 132
Loss: 0.22444711838449752
RMSE train: 0.416193	val: 0.758664	test: 0.722893
MAE train: 0.322052	val: 0.563016	test: 0.552413

Epoch: 133
Loss: 0.22546785856996263
RMSE train: 0.436547	val: 0.752233	test: 0.726857
MAE train: 0.338414	val: 0.563378	test: 0.562461

Epoch: 134
Loss: 0.2395458838769368
RMSE train: 0.445678	val: 0.754979	test: 0.721191
MAE train: 0.344956	val: 0.564863	test: 0.561265

Epoch: 135
Loss: 0.23709942826202937
RMSE train: 0.451710	val: 0.769930	test: 0.740905
MAE train: 0.353249	val: 0.572272	test: 0.575156

Epoch: 136
Loss: 0.24408024762357985
RMSE train: 0.440484	val: 0.746777	test: 0.717738
MAE train: 0.344363	val: 0.560646	test: 0.555144

Epoch: 137
Loss: 0.2323045517717089
RMSE train: 0.450803	val: 0.785003	test: 0.736889
MAE train: 0.352908	val: 0.591115	test: 0.575704

Epoch: 138
Loss: 0.23474628158978053
RMSE train: 0.414396	val: 0.757238	test: 0.718065
MAE train: 0.321316	val: 0.566155	test: 0.550300

Epoch: 139
Loss: 0.25742609586034504
RMSE train: 0.401918	val: 0.738096	test: 0.701200
MAE train: 0.310720	val: 0.557051	test: 0.537214

Epoch: 140
Loss: 0.2369662576488086
RMSE train: 0.476058	val: 0.782890	test: 0.736777
MAE train: 0.375512	val: 0.585160	test: 0.578586

Epoch: 141
Loss: 0.2200878517968314
RMSE train: 0.425604	val: 0.759917	test: 0.718117
MAE train: 0.331130	val: 0.564604	test: 0.558424

Epoch: 142
Loss: 0.2183141229408128
RMSE train: 0.415337	val: 0.738564	test: 0.719891
MAE train: 0.320086	val: 0.560453	test: 0.556839

Epoch: 143
Loss: 0.23110259643622807
RMSE train: 0.421251	val: 0.740842	test: 0.709927
MAE train: 0.323702	val: 0.552050	test: 0.548542

Early stopping
Best (RMSE):	 train: 0.432565	val: 0.717653	test: 0.707578
Best (MAE):	 train: 0.332862	val: 0.542967	test: 0.543712


Epoch: 144
Loss: 0.19705421924591066
RMSE train: 0.396287	val: 0.775598	test: 0.804807
MAE train: 0.311236	val: 0.581218	test: 0.614582

Epoch: 145
Loss: 0.1912288948893547
RMSE train: 0.434897	val: 0.809253	test: 0.837488
MAE train: 0.346315	val: 0.615819	test: 0.642485

Epoch: 146
Loss: 0.18525270819664003
RMSE train: 0.401192	val: 0.759187	test: 0.792942
MAE train: 0.311406	val: 0.573690	test: 0.606165

Epoch: 147
Loss: 0.1854923978447914
RMSE train: 0.368024	val: 0.753202	test: 0.784274
MAE train: 0.285605	val: 0.571059	test: 0.606248

Epoch: 148
Loss: 0.18917683511972427
RMSE train: 0.362345	val: 0.767104	test: 0.789974
MAE train: 0.283117	val: 0.576869	test: 0.605935

Epoch: 149
Loss: 0.18361149430274964
RMSE train: 0.374057	val: 0.766749	test: 0.779980
MAE train: 0.290165	val: 0.583156	test: 0.604362

Epoch: 150
Loss: 0.18169027268886567
RMSE train: 0.403086	val: 0.780638	test: 0.812626
MAE train: 0.313700	val: 0.593496	test: 0.622201

Epoch: 151
Loss: 0.19888959676027299
RMSE train: 0.401573	val: 0.780136	test: 0.804655
MAE train: 0.316879	val: 0.590558	test: 0.620280

Epoch: 152
Loss: 0.1802578017115593
RMSE train: 0.387604	val: 0.773044	test: 0.788185
MAE train: 0.301983	val: 0.583630	test: 0.607469

Epoch: 153
Loss: 0.1756987139582634
RMSE train: 0.426751	val: 0.791799	test: 0.827268
MAE train: 0.338009	val: 0.599236	test: 0.636756

Epoch: 154
Loss: 0.17943273037672042
RMSE train: 0.373230	val: 0.770616	test: 0.786632
MAE train: 0.292444	val: 0.579475	test: 0.608701

Epoch: 155
Loss: 0.18906266689300538
RMSE train: 0.384489	val: 0.780319	test: 0.804250
MAE train: 0.299876	val: 0.592898	test: 0.619921

Epoch: 156
Loss: 0.1893855631351471
RMSE train: 0.342921	val: 0.739703	test: 0.761392
MAE train: 0.264496	val: 0.565160	test: 0.582156

Epoch: 157
Loss: 0.1832812249660492
RMSE train: 0.393648	val: 0.788300	test: 0.822290
MAE train: 0.310633	val: 0.594695	test: 0.632515

Epoch: 158
Loss: 0.17945458889007568
RMSE train: 0.390012	val: 0.772961	test: 0.804125
MAE train: 0.302374	val: 0.582799	test: 0.613761

Epoch: 159
Loss: 0.17851552367210388
RMSE train: 0.365084	val: 0.776039	test: 0.804712
MAE train: 0.282418	val: 0.588482	test: 0.620778

Epoch: 160
Loss: 0.17983824610710145
RMSE train: 0.345835	val: 0.750649	test: 0.786918
MAE train: 0.267080	val: 0.567263	test: 0.598762

Epoch: 161
Loss: 0.18410324007272721
RMSE train: 0.402599	val: 0.789032	test: 0.817963
MAE train: 0.316876	val: 0.599650	test: 0.629472

Epoch: 162
Loss: 0.17775526493787766
RMSE train: 0.383463	val: 0.761283	test: 0.775671
MAE train: 0.299176	val: 0.580307	test: 0.601648

Epoch: 163
Loss: 0.17884918600320815
RMSE train: 0.395794	val: 0.765734	test: 0.794117
MAE train: 0.305268	val: 0.584839	test: 0.614092

Epoch: 164
Loss: 0.17059451341629028
RMSE train: 0.341978	val: 0.744885	test: 0.763059
MAE train: 0.263436	val: 0.565127	test: 0.586573

Epoch: 165
Loss: 0.16454583555459976
RMSE train: 0.363870	val: 0.765522	test: 0.787189
MAE train: 0.281917	val: 0.578711	test: 0.607680

Epoch: 166
Loss: 0.17297435998916627
RMSE train: 0.394180	val: 0.790904	test: 0.813265
MAE train: 0.307905	val: 0.600176	test: 0.625729

Epoch: 167
Loss: 0.17750077694654465
RMSE train: 0.339828	val: 0.752870	test: 0.762455
MAE train: 0.263154	val: 0.565357	test: 0.581048

Epoch: 168
Loss: 0.16719541549682618
RMSE train: 0.370613	val: 0.774933	test: 0.806110
MAE train: 0.289552	val: 0.586207	test: 0.614805

Epoch: 169
Loss: 0.1745069921016693
RMSE train: 0.379151	val: 0.779201	test: 0.796606
MAE train: 0.294342	val: 0.587648	test: 0.612595

Epoch: 170
Loss: 0.16422693580389022
RMSE train: 0.359684	val: 0.755770	test: 0.771617
MAE train: 0.278869	val: 0.566709	test: 0.594692

Epoch: 171
Loss: 0.17619054913520812
RMSE train: 0.390937	val: 0.783730	test: 0.812666
MAE train: 0.307983	val: 0.590887	test: 0.625351

Epoch: 172
Loss: 0.16328527703881263
RMSE train: 0.376767	val: 0.776789	test: 0.801324
MAE train: 0.295056	val: 0.585122	test: 0.619207

Epoch: 173
Loss: 0.16272593289613724
RMSE train: 0.350460	val: 0.764269	test: 0.794489
MAE train: 0.274454	val: 0.575663	test: 0.605710

Epoch: 174
Loss: 0.1694638103246689
RMSE train: 0.347143	val: 0.759089	test: 0.774585
MAE train: 0.268721	val: 0.570113	test: 0.595218

Epoch: 175
Loss: 0.17093290835618974
RMSE train: 0.394077	val: 0.781928	test: 0.815329
MAE train: 0.312070	val: 0.583881	test: 0.622626

Epoch: 176
Loss: 0.16135152280330659
RMSE train: 0.390394	val: 0.785629	test: 0.818093
MAE train: 0.306177	val: 0.594545	test: 0.626201

Epoch: 177
Loss: 0.16154445260763167
RMSE train: 0.362587	val: 0.769059	test: 0.790590
MAE train: 0.280574	val: 0.580309	test: 0.604123

Epoch: 178
Loss: 0.15826244205236434
RMSE train: 0.342794	val: 0.747254	test: 0.776387
MAE train: 0.266288	val: 0.567364	test: 0.595773

Epoch: 179
Loss: 0.16874610930681228
RMSE train: 0.349077	val: 0.756261	test: 0.769544
MAE train: 0.270543	val: 0.571025	test: 0.593980

Epoch: 180
Loss: 0.15760811865329744
RMSE train: 0.375111	val: 0.789075	test: 0.803182
MAE train: 0.291337	val: 0.596580	test: 0.623116

Epoch: 181
Loss: 0.17931608706712723
RMSE train: 0.346926	val: 0.766497	test: 0.785456
MAE train: 0.269082	val: 0.575270	test: 0.602615

Epoch: 182
Loss: 0.1760519415140152
RMSE train: 0.351868	val: 0.755876	test: 0.785157
MAE train: 0.270935	val: 0.573200	test: 0.601788

Epoch: 183
Loss: 0.16809157878160477
RMSE train: 0.360923	val: 0.779962	test: 0.802857
MAE train: 0.279117	val: 0.589973	test: 0.617398

Epoch: 184
Loss: 0.16087584495544432
RMSE train: 0.343000	val: 0.753057	test: 0.781575
MAE train: 0.266483	val: 0.572676	test: 0.604754

Epoch: 185
Loss: 0.1557336762547493
RMSE train: 0.366343	val: 0.770172	test: 0.799941
MAE train: 0.285494	val: 0.580155	test: 0.615929

Epoch: 186
Loss: 0.16744440644979477
RMSE train: 0.367955	val: 0.766840	test: 0.790323
MAE train: 0.288321	val: 0.576358	test: 0.604676

Epoch: 187
Loss: 0.17358088344335557
RMSE train: 0.358330	val: 0.752680	test: 0.769537
MAE train: 0.280735	val: 0.569381	test: 0.590603

Epoch: 188
Loss: 0.16058763414621352
RMSE train: 0.361380	val: 0.784196	test: 0.802976
MAE train: 0.282737	val: 0.585976	test: 0.616680

Epoch: 189
Loss: 0.1558981642127037
RMSE train: 0.335412	val: 0.755330	test: 0.770674
MAE train: 0.259588	val: 0.564620	test: 0.591650

Epoch: 190
Loss: 0.16578477025032043
RMSE train: 0.336578	val: 0.761871	test: 0.786364
MAE train: 0.260497	val: 0.568951	test: 0.606707

Epoch: 191
Loss: 0.16255047619342805
RMSE train: 0.358437	val: 0.756825	test: 0.773470
MAE train: 0.276802	val: 0.568521	test: 0.598324

Early stopping
Best (RMSE):	 train: 0.342921	val: 0.739703	test: 0.761392
Best (MAE):	 train: 0.264496	val: 0.565160	test: 0.582156
All runs completed.


Epoch: 144
Loss: 0.22324167298419134
RMSE train: 0.404906	val: 0.746714	test: 0.729596
MAE train: 0.315858	val: 0.553335	test: 0.550925

Epoch: 145
Loss: 0.21536054355757578
RMSE train: 0.419099	val: 0.745468	test: 0.732873
MAE train: 0.323490	val: 0.553095	test: 0.559212

Epoch: 146
Loss: 0.23344522501741136
RMSE train: 0.394384	val: 0.756431	test: 0.719241
MAE train: 0.305941	val: 0.549532	test: 0.537517

Epoch: 147
Loss: 0.23841982973473413
RMSE train: 0.469082	val: 0.800999	test: 0.775254
MAE train: 0.370378	val: 0.591683	test: 0.593820

Epoch: 148
Loss: 0.2173184316073145
RMSE train: 0.452952	val: 0.798706	test: 0.758213
MAE train: 0.356262	val: 0.587786	test: 0.576207

Epoch: 149
Loss: 0.2123479140656335
RMSE train: 0.397472	val: 0.745365	test: 0.721427
MAE train: 0.307244	val: 0.540411	test: 0.549803

Epoch: 150
Loss: 0.21435866504907608
RMSE train: 0.384687	val: 0.728618	test: 0.711077
MAE train: 0.298726	val: 0.544228	test: 0.542618

Epoch: 151
Loss: 0.2177169163312231
RMSE train: 0.391292	val: 0.745290	test: 0.722454
MAE train: 0.303718	val: 0.553617	test: 0.548907

Epoch: 152
Loss: 0.2087052081312452
RMSE train: 0.388166	val: 0.755973	test: 0.723629
MAE train: 0.299659	val: 0.550672	test: 0.541719

Epoch: 153
Loss: 0.20562906989029475
RMSE train: 0.461625	val: 0.795124	test: 0.779221
MAE train: 0.363101	val: 0.587918	test: 0.597347

Epoch: 154
Loss: 0.20787459292582103
RMSE train: 0.394528	val: 0.738550	test: 0.728153
MAE train: 0.304350	val: 0.548428	test: 0.546682

Epoch: 155
Loss: 0.2067685372063092
RMSE train: 0.397998	val: 0.757093	test: 0.731668
MAE train: 0.306079	val: 0.561492	test: 0.549441

Epoch: 156
Loss: 0.2022858283349446
RMSE train: 0.419852	val: 0.782148	test: 0.760201
MAE train: 0.329145	val: 0.573174	test: 0.572482

Epoch: 157
Loss: 0.209554451916899
RMSE train: 0.399159	val: 0.766993	test: 0.746242
MAE train: 0.309337	val: 0.558763	test: 0.560203

Epoch: 158
Loss: 0.20449734798499516
RMSE train: 0.378173	val: 0.756651	test: 0.728054
MAE train: 0.292224	val: 0.554576	test: 0.546201

Epoch: 159
Loss: 0.22045337089470454
RMSE train: 0.419726	val: 0.780367	test: 0.747061
MAE train: 0.326573	val: 0.580286	test: 0.562427

Epoch: 160
Loss: 0.2125277551157134
RMSE train: 0.367888	val: 0.737776	test: 0.731906
MAE train: 0.282952	val: 0.546348	test: 0.538650

Epoch: 161
Loss: 0.2115304810660226
RMSE train: 0.417639	val: 0.749544	test: 0.747461
MAE train: 0.323606	val: 0.551278	test: 0.564358

Epoch: 162
Loss: 0.20769021021468298
RMSE train: 0.377241	val: 0.734344	test: 0.726931
MAE train: 0.292398	val: 0.541144	test: 0.535828

Epoch: 163
Loss: 0.20192233153751918
RMSE train: 0.378544	val: 0.739470	test: 0.730599
MAE train: 0.292797	val: 0.544586	test: 0.549334

Epoch: 164
Loss: 0.19528721592256001
RMSE train: 0.386667	val: 0.756850	test: 0.738257
MAE train: 0.298363	val: 0.552534	test: 0.548728

Epoch: 165
Loss: 0.2423322701028415
RMSE train: 0.380672	val: 0.736434	test: 0.728610
MAE train: 0.295096	val: 0.544334	test: 0.555998

Epoch: 166
Loss: 0.19637987869126455
RMSE train: 0.372658	val: 0.746596	test: 0.746185
MAE train: 0.286772	val: 0.540645	test: 0.557958

Epoch: 167
Loss: 0.19898432280336106
RMSE train: 0.378111	val: 0.744357	test: 0.734061
MAE train: 0.294192	val: 0.556170	test: 0.549934

Epoch: 168
Loss: 0.19519682441438949
RMSE train: 0.374175	val: 0.736953	test: 0.730801
MAE train: 0.289209	val: 0.545799	test: 0.551018

Epoch: 169
Loss: 0.2154114991426468
RMSE train: 0.385456	val: 0.751131	test: 0.732860
MAE train: 0.299725	val: 0.553507	test: 0.559554

Epoch: 170
Loss: 0.19800133683851787
RMSE train: 0.390953	val: 0.739694	test: 0.713998
MAE train: 0.305777	val: 0.552938	test: 0.545998

Epoch: 171
Loss: 0.20645924444709504
RMSE train: 0.397573	val: 0.774188	test: 0.754426
MAE train: 0.308658	val: 0.563303	test: 0.574550

Epoch: 172
Loss: 0.2112359862242426
RMSE train: 0.398651	val: 0.757638	test: 0.710922
MAE train: 0.310680	val: 0.564849	test: 0.539450

Epoch: 173
Loss: 0.20380860247782298
RMSE train: 0.423279	val: 0.804250	test: 0.782753
MAE train: 0.334997	val: 0.578422	test: 0.590579

Epoch: 174
Loss: 0.22134470513888768
RMSE train: 0.424268	val: 0.743453	test: 0.740828
MAE train: 0.334624	val: 0.552937	test: 0.572754

Epoch: 175
Loss: 0.21416256257465907
RMSE train: 0.415637	val: 0.769589	test: 0.734920
MAE train: 0.323410	val: 0.560289	test: 0.554005

Epoch: 176
Loss: 0.20012566447257996
RMSE train: 0.439122	val: 0.818305	test: 0.753225
MAE train: 0.342182	val: 0.606623	test: 0.576251

Epoch: 177
Loss: 0.20387004102979386
RMSE train: 0.372866	val: 0.745886	test: 0.726307
MAE train: 0.286960	val: 0.552317	test: 0.540866

Epoch: 178
Loss: 0.2094638454062598
RMSE train: 0.388330	val: 0.758547	test: 0.732420
MAE train: 0.301155	val: 0.563427	test: 0.555772

Epoch: 179
Loss: 0.1985932897244181
RMSE train: 0.380197	val: 0.737900	test: 0.731652
MAE train: 0.294518	val: 0.543432	test: 0.555023

Epoch: 180
Loss: 0.20156367548874446
RMSE train: 0.387854	val: 0.757174	test: 0.741141
MAE train: 0.303458	val: 0.558535	test: 0.552045

Epoch: 181
Loss: 0.20168317428656987
RMSE train: 0.365536	val: 0.732972	test: 0.725992
MAE train: 0.284377	val: 0.542605	test: 0.544105

Epoch: 182
Loss: 0.1918681753533227
RMSE train: 0.378573	val: 0.724260	test: 0.708180
MAE train: 0.295094	val: 0.535298	test: 0.530469

Epoch: 183
Loss: 0.1912220854844366
RMSE train: 0.402626	val: 0.761614	test: 0.744609
MAE train: 0.312803	val: 0.565437	test: 0.557001

Epoch: 184
Loss: 0.18599980750254222
RMSE train: 0.413453	val: 0.770562	test: 0.747366
MAE train: 0.322456	val: 0.572365	test: 0.566184

Epoch: 185
Loss: 0.19398725352116994
RMSE train: 0.381055	val: 0.729409	test: 0.732936
MAE train: 0.294687	val: 0.538090	test: 0.552536

Epoch: 186
Loss: 0.18599202803203038
RMSE train: 0.371148	val: 0.729344	test: 0.722478
MAE train: 0.288205	val: 0.542968	test: 0.551045

Epoch: 187
Loss: 0.1952232431088175
RMSE train: 0.457154	val: 0.823300	test: 0.796433
MAE train: 0.362172	val: 0.602637	test: 0.602453

Epoch: 188
Loss: 0.19520258264882223
RMSE train: 0.383016	val: 0.742607	test: 0.715357
MAE train: 0.295972	val: 0.544137	test: 0.534969

Epoch: 189
Loss: 0.20559405109712056
RMSE train: 0.387355	val: 0.738849	test: 0.728063
MAE train: 0.299751	val: 0.550339	test: 0.555796

Epoch: 190
Loss: 0.19765829188483103
RMSE train: 0.358527	val: 0.736380	test: 0.732736
MAE train: 0.277347	val: 0.540298	test: 0.541927

Epoch: 191
Loss: 0.19550737738609314
RMSE train: 0.363392	val: 0.736626	test: 0.718353
MAE train: 0.280543	val: 0.539266	test: 0.542547

Epoch: 192
Loss: 0.21368676317589624
RMSE train: 0.377638	val: 0.747124	test: 0.719504
MAE train: 0.294709	val: 0.555556	test: 0.542621

Epoch: 193
Loss: 0.1902543923684529
RMSE train: 0.377529	val: 0.767997	test: 0.738529
MAE train: 0.293225	val: 0.562719	test: 0.553379

Epoch: 194
Loss: 0.18830766954592296
RMSE train: 0.399636	val: 0.776171	test: 0.740858
MAE train: 0.309531	val: 0.570960	test: 0.558376

Epoch: 195
Loss: 0.18782667815685272
RMSE train: 0.371025	val: 0.753803	test: 0.730363
MAE train: 0.287661	val: 0.557194	test: 0.546125

Epoch: 196
Loss: 0.17942037007638387
RMSE train: 0.363030	val: 0.733697	test: 0.731566
MAE train: 0.279993	val: 0.537712	test: 0.546359

Epoch: 197
Loss: 0.19135175538914545
RMSE train: 0.385324	val: 0.754191	test: 0.741083
MAE train: 0.300636	val: 0.552585	test: 0.546626

Epoch: 198
Loss: 0.19121856348855154
RMSE train: 0.398219	val: 0.756907	test: 0.726808
MAE train: 0.310439	val: 0.568767	test: 0.553205

Epoch: 199
Loss: 0.19938214336122786
RMSE train: 0.386355	val: 0.732208	test: 0.715218
MAE train: 0.303979	val: 0.550778	test: 0.533491

Epoch: 200
Loss: 0.18248227557965688
RMSE train: 0.378878	val: 0.765666	test: 0.736455
MAE train: 0.292653	val: 0.564116	test: 0.549348

Epoch: 201
Loss: 0.19017312143530166
RMSE train: 0.362160	val: 0.743406	test: 0.728970
MAE train: 0.279466	val: 0.552758	test: 0.544790

Epoch: 202
Loss: 0.18022966065577098
RMSE train: 0.370522	val: 0.755948	test: 0.747849
MAE train: 0.286368	val: 0.553732	test: 0.558152

Epoch: 203
Loss: 0.19093521045786993
RMSE train: 0.377603	val: 0.770323	test: 0.734150
MAE train: 0.292189	val: 0.567575	test: 0.552661

Epoch: 204
Loss: 0.1876489296555519
RMSE train: 0.391752	val: 0.779792	test: 0.744183
MAE train: 0.305689	val: 0.575182	test: 0.560375

Epoch: 205
Loss: 0.17333763731377466
RMSE train: 0.356845	val: 0.712294	test: 0.723797
MAE train: 0.276273	val: 0.531847	test: 0.535048

Epoch: 206
Loss: 0.18166159625564302
RMSE train: 0.377852	val: 0.764014	test: 0.735679
MAE train: 0.291788	val: 0.560300	test: 0.560121

Epoch: 207
Loss: 0.17407684879643576
RMSE train: 0.366829	val: 0.756647	test: 0.727323
MAE train: 0.282578	val: 0.557876	test: 0.548233

Epoch: 208
Loss: 0.17652516386338643
RMSE train: 0.346607	val: 0.729633	test: 0.713282
MAE train: 0.267504	val: 0.543432	test: 0.534047

Epoch: 209
Loss: 0.1824056570019041
RMSE train: 0.365712	val: 0.762631	test: 0.745615
MAE train: 0.283488	val: 0.551948	test: 0.559567

Epoch: 210
Loss: 0.19484387976782663
RMSE train: 0.354937	val: 0.753630	test: 0.731382
MAE train: 0.272455	val: 0.552919	test: 0.538523

Epoch: 211
Loss: 0.1880020477942058
RMSE train: 0.411263	val: 0.786151	test: 0.774418
MAE train: 0.324172	val: 0.569967	test: 0.593111

Epoch: 212
Loss: 0.1822890309350831
RMSE train: 0.349663	val: 0.743619	test: 0.707623
MAE train: 0.268857	val: 0.546133	test: 0.532273

Epoch: 213
Loss: 0.18595086250986373
RMSE train: 0.367505	val: 0.776509	test: 0.751494
MAE train: 0.284128	val: 0.560211	test: 0.562129

Epoch: 214
Loss: 0.1875073409506253
RMSE train: 0.358576	val: 0.750751	test: 0.724343
MAE train: 0.276617	val: 0.540131	test: 0.544601

Epoch: 215
Loss: 0.17935369376625335
RMSE train: 0.360280	val: 0.761919	test: 0.725019
MAE train: 0.278910	val: 0.559386	test: 0.546017

Epoch: 216
Loss: 0.1686201872570174
RMSE train: 0.340747	val: 0.740327	test: 0.719573
MAE train: 0.264835	val: 0.547037	test: 0.544294

Epoch: 217
Loss: 0.19244904283966338
RMSE train: 0.345701	val: 0.746430	test: 0.716255
MAE train: 0.268220	val: 0.551394	test: 0.540701

Epoch: 218
Loss: 0.17938065315995896
RMSE train: 0.383899	val: 0.790091	test: 0.762195
MAE train: 0.300217	val: 0.572848	test: 0.575895

Epoch: 219
Loss: 0.17615961070571626
RMSE train: 0.356748	val: 0.749952	test: 0.726087
MAE train: 0.276841	val: 0.557349	test: 0.554303

Epoch: 220
Loss: 0.16853866087538855
RMSE train: 0.353836	val: 0.746008	test: 0.709773
MAE train: 0.275043	val: 0.552545	test: 0.536664

Epoch: 221
Loss: 0.16688266609396255
RMSE train: 0.354555	val: 0.736926	test: 0.719647
MAE train: 0.274897	val: 0.542482	test: 0.544848

Epoch: 222
Loss: 0.17199706924813135
RMSE train: 0.334809	val: 0.729532	test: 0.710746
MAE train: 0.257799	val: 0.538449	test: 0.533962

Epoch: 223
Loss: 0.1622175829751151
RMSE train: 0.354150	val: 0.737772	test: 0.715598
MAE train: 0.273231	val: 0.546541	test: 0.545682

Epoch: 224
Loss: 0.1819194651075772
RMSE train: 0.349797	val: 0.742713	test: 0.709297
MAE train: 0.270040	val: 0.541848	test: 0.530672

Epoch: 225
Loss: 0.18378861035619462
RMSE train: 0.381013	val: 0.783501	test: 0.762277
MAE train: 0.297497	val: 0.572628	test: 0.571163

Epoch: 226
Loss: 0.18586920201778412
RMSE train: 0.342293	val: 0.742619	test: 0.703200
MAE train: 0.265775	val: 0.556793	test: 0.530439

Epoch: 227
Loss: 0.1693989070398467
RMSE train: 0.381259	val: 0.773947	test: 0.743445
MAE train: 0.297147	val: 0.560841	test: 0.561157

Epoch: 228
Loss: 0.16592844894954137
RMSE train: 0.361912	val: 0.746323	test: 0.714848
MAE train: 0.282173	val: 0.557077	test: 0.540008

Epoch: 229
Loss: 0.16487074962684087
RMSE train: 0.334919	val: 0.730162	test: 0.704471
MAE train: 0.257943	val: 0.543458	test: 0.522472

Epoch: 230
Loss: 0.16752895287105016
RMSE train: 0.403857	val: 0.800253	test: 0.755904
MAE train: 0.316754	val: 0.585609	test: 0.579273

Epoch: 231
Loss: 0.1743128044264657
RMSE train: 0.368415	val: 0.770092	test: 0.707984
MAE train: 0.285314	val: 0.565649	test: 0.527591

Epoch: 232
Loss: 0.17997418769768306
RMSE train: 0.369101	val: 0.735033	test: 0.710891
MAE train: 0.288915	val: 0.546298	test: 0.545424

Epoch: 233
Loss: 0.19216095336845943
RMSE train: 0.395406	val: 0.770866	test: 0.721436
MAE train: 0.307088	val: 0.567395	test: 0.555506

Epoch: 234
Loss: 0.1819580282483782
RMSE train: 0.392337	val: 0.793992	test: 0.750674
MAE train: 0.308870	val: 0.579102	test: 0.569246

Epoch: 235
Loss: 0.17720978600638254
RMSE train: 0.334320	val: 0.741589	test: 0.714965
MAE train: 0.257057	val: 0.542713	test: 0.536784

Epoch: 236
Loss: 0.16514971745865686
RMSE train: 0.350722	val: 0.734525	test: 0.716629
MAE train: 0.270022	val: 0.549813	test: 0.539692

Epoch: 237
Loss: 0.1764996967145375
RMSE train: 0.362081	val: 0.762136	test: 0.722157
MAE train: 0.282220	val: 0.565075	test: 0.554281

Epoch: 238
Loss: 0.1690276222569602
RMSE train: 0.361985	val: 0.764251	test: 0.722098
MAE train: 0.281515	val: 0.562917	test: 0.542089

Epoch: 239
Loss: 0.15394139289855957
RMSE train: 0.335022	val: 0.763434	test: 0.714417
MAE train: 0.259287	val: 0.562642	test: 0.535971

Epoch: 240
Loss: 0.16926459223031998
RMSE train: 0.334361	val: 0.754968	test: 0.715706
MAE train: 0.258233	val: 0.555974	test: 0.541608

Early stopping
Best (RMSE):	 train: 0.356845	val: 0.712294	test: 0.723797
Best (MAE):	 train: 0.276273	val: 0.531847	test: 0.535048
All runs completed.
