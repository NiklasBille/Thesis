>>> Starting run for dataset: bace
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running RANDOM configs_static_noise_experiments/GraphCL/bace/noise=0.0.yml on cuda:0
Running RANDOM configs_static_noise_experiments/GraphCL/bace/noise=0.05.yml on cuda:1
Running RANDOM configs_static_noise_experiments/GraphCL/bace/noise=0.1.yml on cuda:2
Running RANDOM configs_static_noise_experiments/GraphCL/bace/noise=0.2.yml on cuda:3
Starting process for seed 1: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.0.yml --runseed 1 --device cuda:0
Starting process for seed 2: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.0.yml --runseed 2 --device cuda:0
Starting process for seed 3: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.0.yml --runseed 3 --device cuda:0
Starting process for seed 1: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.1.yml --runseed 1 --device cuda:2
Starting process for seed 1: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.2.yml --runseed 1 --device cuda:3
Starting process for seed 2: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.1.yml --runseed 2 --device cuda:2Starting process for seed 1: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.05.yml --runseed 1 --device cuda:1

Starting process for seed 2: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.2.yml --runseed 2 --device cuda:3
Starting process for seed 2: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.05.yml --runseed 2 --device cuda:1
Starting process for seed 3: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.1.yml --runseed 3 --device cuda:2
Starting process for seed 3: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.2.yml --runseed 3 --device cuda:3
Starting process for seed 3: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.05.yml --runseed 3 --device cuda:1
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: bace
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/bace/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/bace/noise=0.0/bace_scaff_3_20-05_14-43-32  ]
[ Using Seed :  3  ]
[ Using device :  cuda:0  ]
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6785763897004318
ROC train: 0.704038	val: 0.667033	test: 0.582681
PRC train: 0.606260	val: 0.914276	test: 0.553952

Epoch: 2
Loss: 0.6325084829307982
ROC train: 0.770497	val: 0.657875	test: 0.585637
PRC train: 0.665492	val: 0.913605	test: 0.582690

Epoch: 3
Loss: 0.5713022235649127
ROC train: 0.803570	val: 0.623810	test: 0.614850
PRC train: 0.702814	val: 0.904787	test: 0.613365

Epoch: 4
Loss: 0.5579115751067422
ROC train: 0.830779	val: 0.628205	test: 0.649278
PRC train: 0.734007	val: 0.910953	test: 0.655771

Epoch: 5
Loss: 0.5163962961355789
ROC train: 0.850979	val: 0.634432	test: 0.689967
PRC train: 0.762471	val: 0.911740	test: 0.696256

Epoch: 6
Loss: 0.47976144175492036
ROC train: 0.867785	val: 0.647985	test: 0.738306
PRC train: 0.796034	val: 0.912518	test: 0.759454

Epoch: 7
Loss: 0.4726497476236042
ROC train: 0.879344	val: 0.646154	test: 0.769605
PRC train: 0.812056	val: 0.922298	test: 0.778812

Epoch: 8
Loss: 0.4356091249074706
ROC train: 0.890696	val: 0.632234	test: 0.785776
PRC train: 0.824816	val: 0.924784	test: 0.783345

Epoch: 9
Loss: 0.44487248230529186
ROC train: 0.897360	val: 0.646886	test: 0.792036
PRC train: 0.835420	val: 0.929135	test: 0.794708

Epoch: 10
Loss: 0.41778383591006935
ROC train: 0.902026	val: 0.655678	test: 0.782125
PRC train: 0.841549	val: 0.931756	test: 0.791323

Epoch: 11
Loss: 0.4093592733379895
ROC train: 0.908861	val: 0.671429	test: 0.789776
PRC train: 0.854034	val: 0.937116	test: 0.799099

Epoch: 12
Loss: 0.4229325314252875
ROC train: 0.911992	val: 0.673626	test: 0.780212
PRC train: 0.860782	val: 0.936004	test: 0.790157

Epoch: 13
Loss: 0.397747045007346
ROC train: 0.915733	val: 0.665568	test: 0.770996
PRC train: 0.866887	val: 0.934568	test: 0.777987

Epoch: 14
Loss: 0.3932828702769274
ROC train: 0.917997	val: 0.662637	test: 0.763346
PRC train: 0.869223	val: 0.931863	test: 0.769419

Epoch: 15
Loss: 0.38987651971649856
ROC train: 0.919543	val: 0.670330	test: 0.761607
PRC train: 0.871736	val: 0.933728	test: 0.768120

Epoch: 16
Loss: 0.38621586644030215
ROC train: 0.920565	val: 0.647253	test: 0.760389
PRC train: 0.875375	val: 0.928994	test: 0.766537

Epoch: 17
Loss: 0.3857561502755265
ROC train: 0.926336	val: 0.651282	test: 0.768214
PRC train: 0.882937	val: 0.928891	test: 0.772786

Epoch: 18
Loss: 0.3825573833781534
ROC train: 0.927922	val: 0.636264	test: 0.768910
PRC train: 0.883774	val: 0.922255	test: 0.767768

Epoch: 19
Loss: 0.35694079801145373
ROC train: 0.929541	val: 0.619780	test: 0.764041
PRC train: 0.886837	val: 0.917776	test: 0.762678

Epoch: 20
Loss: 0.373124325445504
ROC train: 0.934703	val: 0.627473	test: 0.767171
PRC train: 0.894857	val: 0.917427	test: 0.759106

Epoch: 21
Loss: 0.3572469171674033
ROC train: 0.936647	val: 0.664103	test: 0.773778
PRC train: 0.898264	val: 0.925824	test: 0.767726

Epoch: 22
Loss: 0.35991488171147745
ROC train: 0.938059	val: 0.661905	test: 0.774648
PRC train: 0.901785	val: 0.924580	test: 0.766689

Epoch: 23
Loss: 0.3394925640965047
ROC train: 0.938519	val: 0.642857	test: 0.770649
PRC train: 0.901581	val: 0.919620	test: 0.763787

Epoch: 24
Loss: 0.3481425230174504
ROC train: 0.940537	val: 0.651282	test: 0.771692
PRC train: 0.904793	val: 0.922252	test: 0.760809

Epoch: 25
Loss: 0.35521409933577963
ROC train: 0.940434	val: 0.659707	test: 0.756042
PRC train: 0.905096	val: 0.926661	test: 0.753355

Epoch: 26
Loss: 0.3329044282293679
ROC train: 0.945111	val: 0.663004	test: 0.755347
PRC train: 0.913407	val: 0.927237	test: 0.749298

Epoch: 27
Loss: 0.3443327024874668
ROC train: 0.943550	val: 0.653114	test: 0.759868
PRC train: 0.909066	val: 0.925017	test: 0.755003

Epoch: 28
Loss: 0.3388214613798046
ROC train: 0.942526	val: 0.650549	test: 0.762998
PRC train: 0.906919	val: 0.923363	test: 0.760638

Epoch: 29
Loss: 0.33659774148683613
ROC train: 0.945251	val: 0.644322	test: 0.762302
PRC train: 0.912466	val: 0.923234	test: 0.759422

Epoch: 30
Loss: 0.352271399160322
ROC train: 0.948447	val: 0.652015	test: 0.766649
PRC train: 0.917353	val: 0.924053	test: 0.770055

Epoch: 31
Loss: 0.33129163216253965
ROC train: 0.949729	val: 0.662271	test: 0.757955
PRC train: 0.920443	val: 0.925772	test: 0.749998

Epoch: 32
Loss: 0.33838434442852894
ROC train: 0.950320	val: 0.645788	test: 0.746653
PRC train: 0.921628	val: 0.921865	test: 0.738992

Epoch: 33
Loss: 0.33059653533870936
ROC train: 0.951187	val: 0.651648	test: 0.741089Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: bace
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/bace/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/bace/noise=0.0/bace_scaff_1_20-05_14-43-32  ]
[ Using Seed :  1  ]
[ Using device :  cuda:0  ]
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6830273724849805
ROC train: 0.707212	val: 0.605128	test: 0.752913
PRC train: 0.602098	val: 0.916114	test: 0.775947

Epoch: 2
Loss: 0.629711837063791
ROC train: 0.781227	val: 0.617216	test: 0.677795
PRC train: 0.677597	val: 0.909472	test: 0.656783

Epoch: 3
Loss: 0.5809227811149485
ROC train: 0.821621	val: 0.632234	test: 0.709963
PRC train: 0.712213	val: 0.907777	test: 0.699684

Epoch: 4
Loss: 0.5249507151672883
ROC train: 0.854207	val: 0.641392	test: 0.758651
PRC train: 0.744527	val: 0.908067	test: 0.755023

Epoch: 5
Loss: 0.508619116612703
ROC train: 0.867066	val: 0.623077	test: 0.783864
PRC train: 0.758584	val: 0.895649	test: 0.771155

Epoch: 6
Loss: 0.47310673381490176
ROC train: 0.871373	val: 0.637363	test: 0.763172
PRC train: 0.765760	val: 0.898043	test: 0.746751

Epoch: 7
Loss: 0.46716672105197865
ROC train: 0.889418	val: 0.657509	test: 0.761954
PRC train: 0.801493	val: 0.924879	test: 0.749158

Epoch: 8
Loss: 0.45131082945533035
ROC train: 0.896436	val: 0.690110	test: 0.747001
PRC train: 0.818541	val: 0.934292	test: 0.746688

Epoch: 9
Loss: 0.426611440842137
ROC train: 0.896113	val: 0.709158	test: 0.755869
PRC train: 0.828501	val: 0.940480	test: 0.760289

Epoch: 10
Loss: 0.4220957018050733
ROC train: 0.898439	val: 0.713187	test: 0.751348
PRC train: 0.835760	val: 0.941879	test: 0.758040

Epoch: 11
Loss: 0.42460700573390164
ROC train: 0.906849	val: 0.718681	test: 0.740741
PRC train: 0.842825	val: 0.944295	test: 0.755805

Epoch: 12
Loss: 0.42443762182416594
ROC train: 0.913302	val: 0.712821	test: 0.739697
PRC train: 0.852327	val: 0.944293	test: 0.748763

Epoch: 13
Loss: 0.3941952346514744
ROC train: 0.914669	val: 0.700366	test: 0.738654
PRC train: 0.858576	val: 0.942290	test: 0.744014

Epoch: 14
Loss: 0.40598450528729935
ROC train: 0.917663	val: 0.693773	test: 0.728917
PRC train: 0.863123	val: 0.940027	test: 0.732740

Epoch: 15
Loss: 0.39421437797023284
ROC train: 0.917631	val: 0.716484	test: 0.701443
PRC train: 0.861883	val: 0.943393	test: 0.682575

Epoch: 16
Loss: 0.40071853681594105
ROC train: 0.923810	val: 0.715018	test: 0.719875
PRC train: 0.869042	val: 0.945688	test: 0.702852

Epoch: 17
Loss: 0.3830030474489912
ROC train: 0.919472	val: 0.689377	test: 0.745609
PRC train: 0.866027	val: 0.938997	test: 0.731537

Epoch: 18
Loss: 0.387925697401256
ROC train: 0.926475	val: 0.686813	test: 0.727873
PRC train: 0.874940	val: 0.936175	test: 0.707541

Epoch: 19
Loss: 0.38495182569536246
ROC train: 0.927880	val: 0.670330	test: 0.722135
PRC train: 0.880300	val: 0.929490	test: 0.693615

Epoch: 20
Loss: 0.37221177794610794
ROC train: 0.932312	val: 0.676190	test: 0.735698
PRC train: 0.884577	val: 0.932933	test: 0.714372

Epoch: 21
Loss: 0.3613425299740169
ROC train: 0.932360	val: 0.685348	test: 0.723700
PRC train: 0.888041	val: 0.934431	test: 0.700849

Epoch: 22
Loss: 0.35865258326284993
ROC train: 0.931838	val: 0.690110	test: 0.725787
PRC train: 0.890635	val: 0.932640	test: 0.697752

Epoch: 23
Loss: 0.36890967862723184
ROC train: 0.933139	val: 0.697802	test: 0.729786
PRC train: 0.890980	val: 0.932605	test: 0.701755

Epoch: 24
Loss: 0.3578238999123955
ROC train: 0.936592	val: 0.678388	test: 0.734307
PRC train: 0.894523	val: 0.932393	test: 0.724537

Epoch: 25
Loss: 0.3575420348457013
ROC train: 0.937180	val: 0.676557	test: 0.728221
PRC train: 0.895627	val: 0.932901	test: 0.728296

Epoch: 26
Loss: 0.34665066398372196
ROC train: 0.939555	val: 0.670696	test: 0.731177
PRC train: 0.901381	val: 0.928136	test: 0.722412

Epoch: 27
Loss: 0.35561435644378236
ROC train: 0.940017	val: 0.698535	test: 0.723005
PRC train: 0.904246	val: 0.935752	test: 0.699132

Epoch: 28
Loss: 0.3444726213402462
ROC train: 0.942979	val: 0.709524	test: 0.728047
PRC train: 0.906520	val: 0.937128	test: 0.699311

Epoch: 29
Loss: 0.33602046782416234
ROC train: 0.942517	val: 0.723810	test: 0.726135
PRC train: 0.906626	val: 0.939296	test: 0.693646

Epoch: 30
Loss: 0.3331537369008715
ROC train: 0.942777	val: 0.721245	test: 0.721266
PRC train: 0.907990	val: 0.938840	test: 0.689355

Epoch: 31
Loss: 0.3401140809324178
ROC train: 0.943867	val: 0.691941	test: 0.745088
PRC train: 0.906986	val: 0.936242	test: 0.726368

Epoch: 32
Loss: 0.3433010644965433
ROC train: 0.946798	val: 0.705495	test: 0.748565
PRC train: 0.915076	val: 0.938828	test: 0.729444

Epoch: 33
Loss: 0.3445830226207668
ROC train: 0.948793	val: 0.695604	test: 0.733612Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: bace
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/bace/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/bace/noise=0.0/bace_scaff_2_20-05_14-43-32  ]
[ Using Seed :  2  ]
[ Using device :  cuda:0  ]
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6745622958997878
ROC train: 0.675628	val: 0.628571	test: 0.642497
PRC train: 0.577769	val: 0.911174	test: 0.602742

Epoch: 2
Loss: 0.6251324408524126
ROC train: 0.751909	val: 0.590110	test: 0.645801
PRC train: 0.649686	val: 0.901082	test: 0.608565

Epoch: 3
Loss: 0.5697383677598469
ROC train: 0.805031	val: 0.617216	test: 0.688228
PRC train: 0.705130	val: 0.897268	test: 0.644613

Epoch: 4
Loss: 0.5466784093342577
ROC train: 0.833328	val: 0.624176	test: 0.711181
PRC train: 0.731200	val: 0.897138	test: 0.693521

Epoch: 5
Loss: 0.5088044850837654
ROC train: 0.857337	val: 0.611355	test: 0.751348
PRC train: 0.757046	val: 0.895660	test: 0.742403

Epoch: 6
Loss: 0.4824619342699781
ROC train: 0.873445	val: 0.617949	test: 0.762824
PRC train: 0.786540	val: 0.896726	test: 0.751600

Epoch: 7
Loss: 0.4636326029645074
ROC train: 0.889469	val: 0.641758	test: 0.786298
PRC train: 0.818418	val: 0.922445	test: 0.768488

Epoch: 8
Loss: 0.4456833205962658
ROC train: 0.893114	val: 0.650916	test: 0.810294
PRC train: 0.823539	val: 0.929360	test: 0.799961

Epoch: 9
Loss: 0.4353781436383345
ROC train: 0.898088	val: 0.647253	test: 0.802121
PRC train: 0.829214	val: 0.929869	test: 0.796113

Epoch: 10
Loss: 0.42311567516000476
ROC train: 0.899070	val: 0.694872	test: 0.756912
PRC train: 0.832991	val: 0.942447	test: 0.745255

Epoch: 11
Loss: 0.4229307845726205
ROC train: 0.904341	val: 0.700000	test: 0.767693
PRC train: 0.840218	val: 0.943957	test: 0.748472

Epoch: 12
Loss: 0.4198241939859203
ROC train: 0.910676	val: 0.671429	test: 0.790645
PRC train: 0.851939	val: 0.935114	test: 0.779707

Epoch: 13
Loss: 0.39368731370278665
ROC train: 0.911938	val: 0.654579	test: 0.774648
PRC train: 0.853721	val: 0.927903	test: 0.768867

Epoch: 14
Loss: 0.4020695005006334
ROC train: 0.916998	val: 0.658608	test: 0.781603
PRC train: 0.862312	val: 0.930005	test: 0.773059

Epoch: 15
Loss: 0.4037267506936771
ROC train: 0.919261	val: 0.675458	test: 0.782646
PRC train: 0.865590	val: 0.934323	test: 0.779312

Epoch: 16
Loss: 0.3782127152264846
ROC train: 0.920468	val: 0.682784	test: 0.769431
PRC train: 0.869070	val: 0.936539	test: 0.769824

Epoch: 17
Loss: 0.3935429541186092
ROC train: 0.923430	val: 0.699634	test: 0.758825
PRC train: 0.872528	val: 0.940255	test: 0.759584

Epoch: 18
Loss: 0.3647353421111947
ROC train: 0.924695	val: 0.703663	test: 0.762128
PRC train: 0.876692	val: 0.941247	test: 0.761927

Epoch: 19
Loss: 0.37220473327915826
ROC train: 0.926361	val: 0.709158	test: 0.765084
PRC train: 0.880634	val: 0.942589	test: 0.767629

Epoch: 20
Loss: 0.372354192772765
ROC train: 0.928856	val: 0.697070	test: 0.761433
PRC train: 0.882771	val: 0.938280	test: 0.763782

Epoch: 21
Loss: 0.36122796771823984
ROC train: 0.931698	val: 0.699267	test: 0.753086
PRC train: 0.886920	val: 0.937346	test: 0.756391

Epoch: 22
Loss: 0.37699110732889896
ROC train: 0.931070	val: 0.712821	test: 0.758477
PRC train: 0.887726	val: 0.939092	test: 0.753360

Epoch: 23
Loss: 0.36941898762499437
ROC train: 0.933139	val: 0.708059	test: 0.769431
PRC train: 0.891482	val: 0.939427	test: 0.769839

Epoch: 24
Loss: 0.34420113926173423
ROC train: 0.935308	val: 0.704762	test: 0.762998
PRC train: 0.894915	val: 0.937842	test: 0.767209

Epoch: 25
Loss: 0.3687694890260263
ROC train: 0.934715	val: 0.722344	test: 0.757433
PRC train: 0.894342	val: 0.941789	test: 0.764273

Epoch: 26
Loss: 0.3513425701254339
ROC train: 0.938853	val: 0.715018	test: 0.769431
PRC train: 0.897998	val: 0.941075	test: 0.765569

Epoch: 27
Loss: 0.3431488559503882
ROC train: 0.940830	val: 0.715385	test: 0.763867
PRC train: 0.903452	val: 0.940704	test: 0.764325

Epoch: 28
Loss: 0.34500721583971844
ROC train: 0.941190	val: 0.697070	test: 0.753086
PRC train: 0.906347	val: 0.935350	test: 0.745530

Epoch: 29
Loss: 0.34550568508011276
ROC train: 0.940054	val: 0.697802	test: 0.754477
PRC train: 0.905850	val: 0.936033	test: 0.762260

Epoch: 30
Loss: 0.3369062708151881
ROC train: 0.941789	val: 0.692674	test: 0.755521
PRC train: 0.908340	val: 0.929341	test: 0.767445

Epoch: 31
Loss: 0.33580376725704986
ROC train: 0.943670	val: 0.686081	test: 0.747001
PRC train: 0.910779	val: 0.926384	test: 0.753512

Epoch: 32
Loss: 0.34745324907028363
ROC train: 0.944164	val: 0.697436	test: 0.728221
PRC train: 0.911377	val: 0.933222	test: 0.733900

Epoch: 33
Loss: 0.32867799024758737
ROC train: 0.944535	val: 0.680586	test: 0.758651Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: bace
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/bace/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/bace/noise=0.1/bace_scaff_1_20-05_14-43-32  ]
[ Using Seed :  1  ]
[ Using device :  cuda:2  ]
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.676982183427234
ROC train: 0.651838	val: 0.567399	test: 0.563380
PRC train: 0.544859	val: 0.901387	test: 0.598351

Epoch: 2
Loss: 0.6430146161590827
ROC train: 0.698938	val: 0.601465	test: 0.604417
PRC train: 0.590046	val: 0.902060	test: 0.603509

Epoch: 3
Loss: 0.6316287744947171
ROC train: 0.735220	val: 0.568498	test: 0.561120
PRC train: 0.636192	val: 0.890483	test: 0.589224

Epoch: 4
Loss: 0.6056755942891427
ROC train: 0.767266	val: 0.619048	test: 0.576943
PRC train: 0.673557	val: 0.902036	test: 0.592979

Epoch: 5
Loss: 0.5829835135869896
ROC train: 0.791581	val: 0.635165	test: 0.614676
PRC train: 0.707019	val: 0.907618	test: 0.621944

Epoch: 6
Loss: 0.5744183580106901
ROC train: 0.820788	val: 0.646154	test: 0.681447
PRC train: 0.739578	val: 0.905269	test: 0.668631

Epoch: 7
Loss: 0.5546264037648935
ROC train: 0.844535	val: 0.661538	test: 0.695879
PRC train: 0.767462	val: 0.911640	test: 0.672526

Epoch: 8
Loss: 0.5341594635293014
ROC train: 0.859797	val: 0.671795	test: 0.735003
PRC train: 0.785465	val: 0.918502	test: 0.703853

Epoch: 9
Loss: 0.5151280854948933
ROC train: 0.873156	val: 0.682418	test: 0.738654
PRC train: 0.806403	val: 0.919249	test: 0.715230

Epoch: 10
Loss: 0.5174391810302817
ROC train: 0.892997	val: 0.688645	test: 0.743523
PRC train: 0.832363	val: 0.925855	test: 0.725943

Epoch: 11
Loss: 0.4970088210957765
ROC train: 0.908556	val: 0.665201	test: 0.778473
PRC train: 0.856273	val: 0.922998	test: 0.766106

Epoch: 12
Loss: 0.48089282115051607
ROC train: 0.922349	val: 0.659707	test: 0.800035
PRC train: 0.878278	val: 0.924796	test: 0.788267

Epoch: 13
Loss: 0.4595519463652442
ROC train: 0.924546	val: 0.630769	test: 0.771344
PRC train: 0.882220	val: 0.919471	test: 0.749047

Epoch: 14
Loss: 0.4412944397354194
ROC train: 0.936872	val: 0.684982	test: 0.773257
PRC train: 0.900756	val: 0.932896	test: 0.755391

Epoch: 15
Loss: 0.41749982017402865
ROC train: 0.941615	val: 0.678755	test: 0.757086
PRC train: 0.907242	val: 0.928060	test: 0.750338

Epoch: 16
Loss: 0.4182676416565833
ROC train: 0.953941	val: 0.674359	test: 0.774126
PRC train: 0.926141	val: 0.927988	test: 0.770526

Epoch: 17
Loss: 0.406877815188423
ROC train: 0.960850	val: 0.661905	test: 0.773778
PRC train: 0.938718	val: 0.926434	test: 0.768416

Epoch: 18
Loss: 0.3658218244257635
ROC train: 0.960679	val: 0.659341	test: 0.777604
PRC train: 0.939498	val: 0.925870	test: 0.769754

Epoch: 19
Loss: 0.3933051771675027
ROC train: 0.966787	val: 0.680220	test: 0.782125
PRC train: 0.948149	val: 0.932319	test: 0.783597

Epoch: 20
Loss: 0.3609628560764153
ROC train: 0.968359	val: 0.669597	test: 0.766649
PRC train: 0.951043	val: 0.930114	test: 0.760709

Epoch: 21
Loss: 0.35422630509737163
ROC train: 0.975645	val: 0.676923	test: 0.780038
PRC train: 0.960988	val: 0.930630	test: 0.777418

Epoch: 22
Loss: 0.3236260219224471
ROC train: 0.978673	val: 0.685714	test: 0.784038
PRC train: 0.965060	val: 0.933178	test: 0.789831

Epoch: 23
Loss: 0.34490284028818985
ROC train: 0.980322	val: 0.672527	test: 0.801078
PRC train: 0.968713	val: 0.929065	test: 0.794370

Epoch: 24
Loss: 0.3389378400379476
ROC train: 0.984989	val: 0.686447	test: 0.805599
PRC train: 0.976341	val: 0.934815	test: 0.805992

Epoch: 25
Loss: 0.29271713429471025
ROC train: 0.983696	val: 0.648718	test: 0.778647
PRC train: 0.975849	val: 0.924759	test: 0.790307

Epoch: 26
Loss: 0.29922474773774044
ROC train: 0.987178	val: 0.661172	test: 0.789776
PRC train: 0.980741	val: 0.928306	test: 0.802047

Epoch: 27
Loss: 0.27871673061288804
ROC train: 0.987894	val: 0.672527	test: 0.790993
PRC train: 0.981448	val: 0.931135	test: 0.789452

Epoch: 28
Loss: 0.29049762184816835
ROC train: 0.992143	val: 0.677656	test: 0.774126
PRC train: 0.987723	val: 0.930417	test: 0.784347

Epoch: 29
Loss: 0.2599038968053487
ROC train: 0.989481	val: 0.695971	test: 0.791341
PRC train: 0.984580	val: 0.934590	test: 0.780837

Epoch: 30
Loss: 0.25757588629107586
ROC train: 0.992820	val: 0.702930	test: 0.793949
PRC train: 0.989019	val: 0.936049	test: 0.796466

Epoch: 31
Loss: 0.29278960514825225
ROC train: 0.993796	val: 0.676557	test: 0.776561
PRC train: 0.990722	val: 0.930669	test: 0.797491

Epoch: 32
Loss: 0.28314038679752773
ROC train: 0.994118	val: 0.690842	test: 0.787167Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: bace
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/bace/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/bace/noise=0.05/bace_scaff_1_20-05_14-43-32  ]
[ Using Seed :  1  ]
[ Using device :  cuda:1  ]
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6716307334060676
ROC train: 0.680497	val: 0.606960	test: 0.611372
PRC train: 0.591200	val: 0.901593	test: 0.647917

Epoch: 2
Loss: 0.628888869419472
ROC train: 0.748602	val: 0.582418	test: 0.652061
PRC train: 0.647034	val: 0.891114	test: 0.674168

Epoch: 3
Loss: 0.6060627950369286
ROC train: 0.784920	val: 0.593040	test: 0.667362
PRC train: 0.687086	val: 0.888799	test: 0.682587

Epoch: 4
Loss: 0.5825753145578918
ROC train: 0.819863	val: 0.641026	test: 0.719527
PRC train: 0.722835	val: 0.912324	test: 0.722038

Epoch: 5
Loss: 0.5495918524617582
ROC train: 0.847623	val: 0.636996	test: 0.774300
PRC train: 0.758981	val: 0.904168	test: 0.749082

Epoch: 6
Loss: 0.5291991869575071
ROC train: 0.861455	val: 0.650916	test: 0.785776
PRC train: 0.784788	val: 0.904008	test: 0.769425

Epoch: 7
Loss: 0.5075755776411441
ROC train: 0.872954	val: 0.661172	test: 0.774648
PRC train: 0.807112	val: 0.916729	test: 0.764977

Epoch: 8
Loss: 0.48654642002786447
ROC train: 0.886233	val: 0.659341	test: 0.800904
PRC train: 0.825497	val: 0.926326	test: 0.787883

Epoch: 9
Loss: 0.471158844686669
ROC train: 0.894889	val: 0.655311	test: 0.789428
PRC train: 0.837145	val: 0.921500	test: 0.774169

Epoch: 10
Loss: 0.45682833685505014
ROC train: 0.908679	val: 0.661905	test: 0.807686
PRC train: 0.858509	val: 0.932116	test: 0.789264

Epoch: 11
Loss: 0.45749881558884253
ROC train: 0.915685	val: 0.662637	test: 0.828552
PRC train: 0.870520	val: 0.934562	test: 0.817538

Epoch: 12
Loss: 0.43012832027610937
ROC train: 0.924569	val: 0.660440	test: 0.820031
PRC train: 0.883153	val: 0.934303	test: 0.812273

Epoch: 13
Loss: 0.4339631922089048
ROC train: 0.929150	val: 0.679487	test: 0.814815
PRC train: 0.890274	val: 0.939013	test: 0.812701

Epoch: 14
Loss: 0.4049117875649464
ROC train: 0.933527	val: 0.687546	test: 0.811685
PRC train: 0.897908	val: 0.940475	test: 0.809953

Epoch: 15
Loss: 0.39195921949473655
ROC train: 0.940514	val: 0.687179	test: 0.808381
PRC train: 0.907417	val: 0.939977	test: 0.807887

Epoch: 16
Loss: 0.39241015452516004
ROC train: 0.947043	val: 0.690476	test: 0.794123
PRC train: 0.917231	val: 0.938988	test: 0.792998

Epoch: 17
Loss: 0.38097623698004857
ROC train: 0.952018	val: 0.673626	test: 0.809077
PRC train: 0.923912	val: 0.936478	test: 0.797086

Epoch: 18
Loss: 0.37348680809445894
ROC train: 0.957029	val: 0.671062	test: 0.807860
PRC train: 0.930678	val: 0.935806	test: 0.795559

Epoch: 19
Loss: 0.36304685405071774
ROC train: 0.959646	val: 0.680952	test: 0.801426
PRC train: 0.938025	val: 0.936884	test: 0.797874

Epoch: 20
Loss: 0.36956610306509413
ROC train: 0.957055	val: 0.698901	test: 0.809772
PRC train: 0.934782	val: 0.940987	test: 0.801570

Epoch: 21
Loss: 0.3478297123425086
ROC train: 0.967172	val: 0.684249	test: 0.813250
PRC train: 0.951624	val: 0.934193	test: 0.788697

Epoch: 22
Loss: 0.3216550078032034
ROC train: 0.969258	val: 0.681685	test: 0.804034
PRC train: 0.954832	val: 0.933700	test: 0.789760

Epoch: 23
Loss: 0.3368262920451407
ROC train: 0.971338	val: 0.688645	test: 0.811511
PRC train: 0.956680	val: 0.937334	test: 0.799726

Epoch: 24
Loss: 0.3180549338235623
ROC train: 0.971924	val: 0.691209	test: 0.810816
PRC train: 0.957074	val: 0.938538	test: 0.809435

Epoch: 25
Loss: 0.2947185575914263
ROC train: 0.974158	val: 0.683150	test: 0.802643
PRC train: 0.961899	val: 0.937566	test: 0.806061

Epoch: 26
Loss: 0.30488541991378293
ROC train: 0.977103	val: 0.687546	test: 0.808555
PRC train: 0.966235	val: 0.937618	test: 0.807567

Epoch: 27
Loss: 0.28739650723981724
ROC train: 0.977483	val: 0.684982	test: 0.811337
PRC train: 0.967577	val: 0.938368	test: 0.800112

Epoch: 28
Loss: 0.30696962417415863
ROC train: 0.982334	val: 0.663370	test: 0.789080
PRC train: 0.975118	val: 0.932086	test: 0.788823

Epoch: 29
Loss: 0.29375761607301343
ROC train: 0.985545	val: 0.686813	test: 0.803512
PRC train: 0.978869	val: 0.938242	test: 0.810656

Epoch: 30
Loss: 0.2796241801991971
ROC train: 0.987871	val: 0.699634	test: 0.802991
PRC train: 0.982471	val: 0.941692	test: 0.809214

Epoch: 31
Loss: 0.29103225369197455
ROC train: 0.986881	val: 0.700733	test: 0.800904
PRC train: 0.981434	val: 0.941587	test: 0.800809

Epoch: 32
Loss: 0.2601224792908381
ROC train: 0.986159	val: 0.670330	test: 0.815684Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: bace
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/bace/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/bace/noise=0.05/bace_scaff_2_20-05_14-43-32  ]
[ Using Seed :  2  ]
[ Using device :  cuda:1  ]
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6860231814642794
ROC train: 0.667394	val: 0.582051	test: 0.573465
PRC train: 0.567083	val: 0.905953	test: 0.573195

Epoch: 2
Loss: 0.637703214805042
ROC train: 0.720465	val: 0.657875	test: 0.649974
PRC train: 0.610807	val: 0.917690	test: 0.625647

Epoch: 3
Loss: 0.6195797923827764
ROC train: 0.764272	val: 0.651648	test: 0.677274
PRC train: 0.660523	val: 0.904009	test: 0.638739

Epoch: 4
Loss: 0.5943573619773856
ROC train: 0.811047	val: 0.636996	test: 0.716397
PRC train: 0.723523	val: 0.907575	test: 0.685259

Epoch: 5
Loss: 0.5592988639100567
ROC train: 0.836909	val: 0.623077	test: 0.726830
PRC train: 0.752602	val: 0.900746	test: 0.710057

Epoch: 6
Loss: 0.5234798956095059
ROC train: 0.855936	val: 0.649817	test: 0.726830
PRC train: 0.770371	val: 0.907740	test: 0.713606

Epoch: 7
Loss: 0.5092966573725505
ROC train: 0.865208	val: 0.650549	test: 0.718484
PRC train: 0.776964	val: 0.915962	test: 0.710511

Epoch: 8
Loss: 0.5073051039172638
ROC train: 0.876353	val: 0.641758	test: 0.724048
PRC train: 0.785001	val: 0.915438	test: 0.707349

Epoch: 9
Loss: 0.4603886592759242
ROC train: 0.893339	val: 0.667399	test: 0.712224
PRC train: 0.805158	val: 0.918560	test: 0.697293

Epoch: 10
Loss: 0.4722732617471558
ROC train: 0.900916	val: 0.676923	test: 0.731351
PRC train: 0.818722	val: 0.925543	test: 0.714556

Epoch: 11
Loss: 0.45154343818457426
ROC train: 0.910656	val: 0.674359	test: 0.752739
PRC train: 0.841531	val: 0.917489	test: 0.738108

Epoch: 12
Loss: 0.4311234524690618
ROC train: 0.912440	val: 0.664835	test: 0.716571
PRC train: 0.843093	val: 0.915030	test: 0.696319

Epoch: 13
Loss: 0.4210873339846735
ROC train: 0.923487	val: 0.664469	test: 0.729091
PRC train: 0.860509	val: 0.915720	test: 0.724104

Epoch: 14
Loss: 0.3901192050255988
ROC train: 0.932089	val: 0.685348	test: 0.736046
PRC train: 0.873231	val: 0.931283	test: 0.739380

Epoch: 15
Loss: 0.4045186324950044
ROC train: 0.932001	val: 0.688278	test: 0.735872
PRC train: 0.876646	val: 0.935189	test: 0.740102

Epoch: 16
Loss: 0.3988029238895834
ROC train: 0.936036	val: 0.683516	test: 0.747348
PRC train: 0.886004	val: 0.932764	test: 0.743222

Epoch: 17
Loss: 0.38977049248787393
ROC train: 0.939883	val: 0.696703	test: 0.747696
PRC train: 0.894147	val: 0.940428	test: 0.737711

Epoch: 18
Loss: 0.3640880232398933
ROC train: 0.949757	val: 0.693040	test: 0.751521
PRC train: 0.908144	val: 0.937121	test: 0.751232

Epoch: 19
Loss: 0.37891974305236664
ROC train: 0.953208	val: 0.688645	test: 0.735524
PRC train: 0.917739	val: 0.932203	test: 0.730706

Epoch: 20
Loss: 0.38527527877058476
ROC train: 0.955377	val: 0.705861	test: 0.725613
PRC train: 0.920802	val: 0.941790	test: 0.720535

Epoch: 21
Loss: 0.35742850193123094
ROC train: 0.956744	val: 0.696703	test: 0.724744
PRC train: 0.925895	val: 0.939961	test: 0.712753

Epoch: 22
Loss: 0.359497442429631
ROC train: 0.959409	val: 0.689011	test: 0.722657
PRC train: 0.929072	val: 0.935235	test: 0.717773

Epoch: 23
Loss: 0.33978094213581694
ROC train: 0.964033	val: 0.693407	test: 0.706834
PRC train: 0.936866	val: 0.933407	test: 0.716744

Epoch: 24
Loss: 0.33867815945730545
ROC train: 0.968082	val: 0.679487	test: 0.706312
PRC train: 0.945461	val: 0.928768	test: 0.714450

Epoch: 25
Loss: 0.3270756977963719
ROC train: 0.972132	val: 0.676923	test: 0.733959
PRC train: 0.954627	val: 0.929689	test: 0.729498

Epoch: 26
Loss: 0.3217609497709864
ROC train: 0.975502	val: 0.701099	test: 0.735176
PRC train: 0.959795	val: 0.938343	test: 0.737907

Epoch: 27
Loss: 0.3000004985360051
ROC train: 0.974563	val: 0.695604	test: 0.713789
PRC train: 0.957811	val: 0.933163	test: 0.719920

Epoch: 28
Loss: 0.2901861607582613
ROC train: 0.975639	val: 0.678388	test: 0.705790
PRC train: 0.960203	val: 0.931144	test: 0.706008

Epoch: 29
Loss: 0.2836140076129861
ROC train: 0.977303	val: 0.642491	test: 0.724570
PRC train: 0.963026	val: 0.923993	test: 0.722996

Epoch: 30
Loss: 0.28668463132088373
ROC train: 0.982146	val: 0.662271	test: 0.745783
PRC train: 0.971181	val: 0.929128	test: 0.748221

Epoch: 31
Loss: 0.2744837644318988
ROC train: 0.983704	val: 0.698901	test: 0.748565
PRC train: 0.974453	val: 0.937714	test: 0.751724

Epoch: 32
Loss: 0.2601790700140054
ROC train: 0.988134	val: 0.712454	test: 0.757433Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:3
  input_data_dir: 
  dataset: bace
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/bace/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/bace/noise=0.2/bace_scaff_2_20-05_14-43-32  ]
[ Using Seed :  2  ]
[ Using device :  cuda:3  ]
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6922206416837343
ROC train: 0.577483	val: 0.575458	test: 0.553469
PRC train: 0.453547	val: 0.898738	test: 0.575712

Epoch: 2
Loss: 0.6637272357617464
ROC train: 0.646501	val: 0.635165	test: 0.628412
PRC train: 0.522468	val: 0.912957	test: 0.616379

Epoch: 3
Loss: 0.6582901893935456
ROC train: 0.686681	val: 0.636264	test: 0.617458
PRC train: 0.564929	val: 0.910990	test: 0.601344

Epoch: 4
Loss: 0.6414959862556517
ROC train: 0.721016	val: 0.607326	test: 0.578508
PRC train: 0.606536	val: 0.903912	test: 0.585582

Epoch: 5
Loss: 0.6271793137286046
ROC train: 0.741407	val: 0.589011	test: 0.580247
PRC train: 0.630050	val: 0.897033	test: 0.591658

Epoch: 6
Loss: 0.6187977191810441
ROC train: 0.761290	val: 0.617582	test: 0.604069
PRC train: 0.653750	val: 0.906624	test: 0.608559

Epoch: 7
Loss: 0.6020521430680005
ROC train: 0.787751	val: 0.625275	test: 0.593984
PRC train: 0.683424	val: 0.914197	test: 0.601945

Epoch: 8
Loss: 0.5910709922651377
ROC train: 0.795411	val: 0.649451	test: 0.566684
PRC train: 0.688931	val: 0.923137	test: 0.579294

Epoch: 9
Loss: 0.5688929130487573
ROC train: 0.827480	val: 0.696337	test: 0.618153
PRC train: 0.733492	val: 0.940597	test: 0.604109

Epoch: 10
Loss: 0.5646483047158106
ROC train: 0.854780	val: 0.716484	test: 0.635889
PRC train: 0.773699	val: 0.946489	test: 0.613618

Epoch: 11
Loss: 0.537543450546747
ROC train: 0.872446	val: 0.709890	test: 0.673622
PRC train: 0.801247	val: 0.945373	test: 0.638932

Epoch: 12
Loss: 0.5230415575086963
ROC train: 0.898302	val: 0.683150	test: 0.688750
PRC train: 0.841326	val: 0.936695	test: 0.637111

Epoch: 13
Loss: 0.48780119619916035
ROC train: 0.913527	val: 0.661172	test: 0.696227
PRC train: 0.862001	val: 0.925169	test: 0.651316

Epoch: 14
Loss: 0.48232277090202713
ROC train: 0.927971	val: 0.650183	test: 0.695183
PRC train: 0.886836	val: 0.923446	test: 0.647154

Epoch: 15
Loss: 0.46319505373505476
ROC train: 0.940976	val: 0.663736	test: 0.707181
PRC train: 0.907400	val: 0.930702	test: 0.675673

Epoch: 16
Loss: 0.4444381665777749
ROC train: 0.947977	val: 0.707326	test: 0.727873
PRC train: 0.917550	val: 0.945519	test: 0.695951

Epoch: 17
Loss: 0.4556336720451767
ROC train: 0.952109	val: 0.702564	test: 0.719353
PRC train: 0.922068	val: 0.943710	test: 0.684082

Epoch: 18
Loss: 0.4264155088614741
ROC train: 0.950522	val: 0.692674	test: 0.717440
PRC train: 0.922075	val: 0.938710	test: 0.699216

Epoch: 19
Loss: 0.4321965675832569
ROC train: 0.967717	val: 0.679487	test: 0.721266
PRC train: 0.952050	val: 0.936447	test: 0.704492

Epoch: 20
Loss: 0.39354716312575067
ROC train: 0.975682	val: 0.699267	test: 0.724396
PRC train: 0.964372	val: 0.941228	test: 0.692084

Epoch: 21
Loss: 0.39540541119111533
ROC train: 0.977763	val: 0.680586	test: 0.739176
PRC train: 0.967341	val: 0.933867	test: 0.694586

Epoch: 22
Loss: 0.36757357477149116
ROC train: 0.982560	val: 0.625275	test: 0.737089
PRC train: 0.973755	val: 0.916404	test: 0.684956

Epoch: 23
Loss: 0.36743787689470564
ROC train: 0.981804	val: 0.669597	test: 0.728395
PRC train: 0.973326	val: 0.930513	test: 0.702752

Epoch: 24
Loss: 0.3665489242255869
ROC train: 0.983091	val: 0.677289	test: 0.720918
PRC train: 0.975883	val: 0.933109	test: 0.705922

Epoch: 25
Loss: 0.3173542594314293
ROC train: 0.987340	val: 0.658974	test: 0.733612
PRC train: 0.981679	val: 0.929347	test: 0.705319

Epoch: 26
Loss: 0.3416618360615279
ROC train: 0.988239	val: 0.632967	test: 0.753086
PRC train: 0.981957	val: 0.916182	test: 0.713158

Epoch: 27
Loss: 0.3209075117892149
ROC train: 0.990993	val: 0.608059	test: 0.748218
PRC train: 0.986374	val: 0.907229	test: 0.708710

Epoch: 28
Loss: 0.31797931346246167
ROC train: 0.992968	val: 0.618681	test: 0.733090
PRC train: 0.990597	val: 0.915116	test: 0.705462

Epoch: 29
Loss: 0.296787285074857
ROC train: 0.997115	val: 0.687179	test: 0.739697
PRC train: 0.996084	val: 0.939759	test: 0.698203

Epoch: 30
Loss: 0.284473274397396
ROC train: 0.997463	val: 0.713187	test: 0.727700
PRC train: 0.996308	val: 0.944642	test: 0.707591

Epoch: 31
Loss: 0.2528415826621063
ROC train: 0.997714	val: 0.710989	test: 0.736220
PRC train: 0.996748	val: 0.941380	test: 0.719837

Epoch: 32
Loss: 0.26467852511339707
ROC train: 0.997991	val: 0.641026	test: 0.744045Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:3
  input_data_dir: 
  dataset: bace
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/bace/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/bace/noise=0.2/bace_scaff_3_20-05_14-43-32  ]
[ Using Seed :  3  ]
[ Using device :  cuda:3  ]
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6839282997355867
ROC train: 0.596473	val: 0.563370	test: 0.595722
PRC train: 0.490273	val: 0.908522	test: 0.564610

Epoch: 2
Loss: 0.6638076047259259
ROC train: 0.640117	val: 0.587179	test: 0.642149
PRC train: 0.535135	val: 0.909055	test: 0.616623

Epoch: 3
Loss: 0.6454121843473357
ROC train: 0.674212	val: 0.603663	test: 0.576595
PRC train: 0.566716	val: 0.904635	test: 0.569060

Epoch: 4
Loss: 0.6329109371202348
ROC train: 0.716142	val: 0.596703	test: 0.550339
PRC train: 0.621055	val: 0.900470	test: 0.555109

Epoch: 5
Loss: 0.6217167522189546
ROC train: 0.739395	val: 0.573260	test: 0.563554
PRC train: 0.646228	val: 0.889522	test: 0.570090

Epoch: 6
Loss: 0.6080559107925467
ROC train: 0.764047	val: 0.566300	test: 0.579725
PRC train: 0.672503	val: 0.884205	test: 0.586129

Epoch: 7
Loss: 0.5878861207738464
ROC train: 0.785913	val: 0.576923	test: 0.612937
PRC train: 0.701311	val: 0.890538	test: 0.617991

Epoch: 8
Loss: 0.5851750903181138
ROC train: 0.797257	val: 0.582418	test: 0.609459
PRC train: 0.715378	val: 0.897147	test: 0.610143

Epoch: 9
Loss: 0.5743232857123665
ROC train: 0.815320	val: 0.580220	test: 0.626152
PRC train: 0.734176	val: 0.897558	test: 0.627984

Epoch: 10
Loss: 0.5496764424391168
ROC train: 0.841210	val: 0.573626	test: 0.607199
PRC train: 0.769861	val: 0.889766	test: 0.611722

Epoch: 11
Loss: 0.543998465525809
ROC train: 0.865420	val: 0.584249	test: 0.648235
PRC train: 0.801397	val: 0.887254	test: 0.634636

Epoch: 12
Loss: 0.5252827287338364
ROC train: 0.875628	val: 0.589377	test: 0.639367
PRC train: 0.809687	val: 0.886407	test: 0.627825

Epoch: 13
Loss: 0.5195306322856205
ROC train: 0.894546	val: 0.619048	test: 0.666145
PRC train: 0.831232	val: 0.895698	test: 0.644463

Epoch: 14
Loss: 0.5073028885850377
ROC train: 0.907871	val: 0.639194	test: 0.678491
PRC train: 0.849077	val: 0.898169	test: 0.652430

Epoch: 15
Loss: 0.4894864501180014
ROC train: 0.910742	val: 0.601099	test: 0.664580
PRC train: 0.860279	val: 0.873552	test: 0.643258

Epoch: 16
Loss: 0.4800739644444073
ROC train: 0.925074	val: 0.589744	test: 0.666319
PRC train: 0.882282	val: 0.870565	test: 0.639724

Epoch: 17
Loss: 0.45239426270519945
ROC train: 0.937751	val: 0.618315	test: 0.680230
PRC train: 0.900371	val: 0.900127	test: 0.669369

Epoch: 18
Loss: 0.43755886190501947
ROC train: 0.943773	val: 0.609158	test: 0.656755
PRC train: 0.909119	val: 0.896682	test: 0.648241

Epoch: 19
Loss: 0.432092301650724
ROC train: 0.946598	val: 0.606960	test: 0.640063
PRC train: 0.914857	val: 0.899553	test: 0.630451

Epoch: 20
Loss: 0.42801997261162195
ROC train: 0.960588	val: 0.652015	test: 0.653973
PRC train: 0.933371	val: 0.922887	test: 0.651324

Epoch: 21
Loss: 0.38950824619614366
ROC train: 0.969309	val: 0.626740	test: 0.680751
PRC train: 0.945947	val: 0.906731	test: 0.670241

Epoch: 22
Loss: 0.3813852791148703
ROC train: 0.966772	val: 0.576190	test: 0.679360
PRC train: 0.945331	val: 0.871528	test: 0.662625

Epoch: 23
Loss: 0.38410999727785133
ROC train: 0.979752	val: 0.600366	test: 0.708746
PRC train: 0.964799	val: 0.884469	test: 0.672048

Epoch: 24
Loss: 0.3657578460944795
ROC train: 0.982537	val: 0.598535	test: 0.705790
PRC train: 0.970527	val: 0.892031	test: 0.670412

Epoch: 25
Loss: 0.3324942483477974
ROC train: 0.987451	val: 0.606227	test: 0.694314
PRC train: 0.978651	val: 0.892237	test: 0.676279

Epoch: 26
Loss: 0.3461453267329727
ROC train: 0.987197	val: 0.617582	test: 0.690315
PRC train: 0.978357	val: 0.898774	test: 0.676745

Epoch: 27
Loss: 0.3385668758775636
ROC train: 0.988773	val: 0.604396	test: 0.712050
PRC train: 0.981620	val: 0.878167	test: 0.684354

Epoch: 28
Loss: 0.33389994587775823
ROC train: 0.990696	val: 0.611355	test: 0.716049
PRC train: 0.985771	val: 0.887960	test: 0.690472

Epoch: 29
Loss: 0.2996084273931446
ROC train: 0.992557	val: 0.665201	test: 0.704399
PRC train: 0.988293	val: 0.923571	test: 0.686617

Epoch: 30
Loss: 0.303755973713055
ROC train: 0.993059	val: 0.696703	test: 0.699531
PRC train: 0.988115	val: 0.932444	test: 0.672215

Epoch: 31
Loss: 0.30052164410934357
ROC train: 0.995228	val: 0.683150	test: 0.691010
PRC train: 0.992127	val: 0.925206	test: 0.686289

Epoch: 32
Loss: 0.2879552371038136
ROC train: 0.995691	val: 0.608425	test: 0.668753Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: bace
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/bace/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/bace/noise=0.05/bace_scaff_3_20-05_14-43-32  ]
[ Using Seed :  3  ]
[ Using device :  cuda:1  ]
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6771656579401093
ROC train: 0.677489	val: 0.589377	test: 0.647713
PRC train: 0.562761	val: 0.903295	test: 0.597274

Epoch: 2
Loss: 0.6404897967737967
ROC train: 0.722765	val: 0.615751	test: 0.676926
PRC train: 0.617133	val: 0.903198	test: 0.647159

Epoch: 3
Loss: 0.6137997640666015
ROC train: 0.760237	val: 0.637363	test: 0.662841
PRC train: 0.650856	val: 0.908273	test: 0.641127

Epoch: 4
Loss: 0.5812483676320189
ROC train: 0.796447	val: 0.611355	test: 0.683707
PRC train: 0.687336	val: 0.897026	test: 0.671962

Epoch: 5
Loss: 0.5550823961718379
ROC train: 0.822757	val: 0.635165	test: 0.727700
PRC train: 0.719271	val: 0.905319	test: 0.723049

Epoch: 6
Loss: 0.5340697846751127
ROC train: 0.845768	val: 0.669231	test: 0.745436
PRC train: 0.754083	val: 0.919019	test: 0.744774

Epoch: 7
Loss: 0.5298566658548665
ROC train: 0.859803	val: 0.682051	test: 0.732568
PRC train: 0.780504	val: 0.917614	test: 0.735982

Epoch: 8
Loss: 0.5040927643852186
ROC train: 0.880782	val: 0.674725	test: 0.760216
PRC train: 0.807989	val: 0.919739	test: 0.757362

Epoch: 9
Loss: 0.4729393189948864
ROC train: 0.895151	val: 0.676923	test: 0.767866
PRC train: 0.832933	val: 0.919126	test: 0.764911

Epoch: 10
Loss: 0.45036218191334143
ROC train: 0.908236	val: 0.688278	test: 0.771866
PRC train: 0.855526	val: 0.935148	test: 0.775011

Epoch: 11
Loss: 0.42691080192346587
ROC train: 0.914072	val: 0.680586	test: 0.761781
PRC train: 0.864693	val: 0.932219	test: 0.760979

Epoch: 12
Loss: 0.42005137934804393
ROC train: 0.924906	val: 0.679121	test: 0.791688
PRC train: 0.881504	val: 0.935327	test: 0.800173

Epoch: 13
Loss: 0.42625524456766256
ROC train: 0.933522	val: 0.700366	test: 0.802991
PRC train: 0.894399	val: 0.943465	test: 0.816311

Epoch: 14
Loss: 0.39559819687479025
ROC train: 0.933156	val: 0.683150	test: 0.804208
PRC train: 0.894714	val: 0.931365	test: 0.814061

Epoch: 15
Loss: 0.3920019383086369
ROC train: 0.942477	val: 0.701099	test: 0.814989
PRC train: 0.910382	val: 0.942496	test: 0.818779

Epoch: 16
Loss: 0.3835504375053628
ROC train: 0.947566	val: 0.713553	test: 0.820553
PRC train: 0.919118	val: 0.946316	test: 0.827228

Epoch: 17
Loss: 0.37549727744830225
ROC train: 0.952454	val: 0.690110	test: 0.807338
PRC train: 0.926782	val: 0.940683	test: 0.809971

Epoch: 18
Loss: 0.36067692256244877
ROC train: 0.955160	val: 0.676923	test: 0.810642
PRC train: 0.930493	val: 0.935853	test: 0.808142

Epoch: 19
Loss: 0.35444876887848736
ROC train: 0.958453	val: 0.675824	test: 0.810816
PRC train: 0.935686	val: 0.936847	test: 0.807740

Epoch: 20
Loss: 0.35327749129515845
ROC train: 0.959652	val: 0.695238	test: 0.793775
PRC train: 0.937660	val: 0.942352	test: 0.786499

Epoch: 21
Loss: 0.31816750074835637
ROC train: 0.962686	val: 0.719780	test: 0.795862
PRC train: 0.940819	val: 0.948817	test: 0.789942

Epoch: 22
Loss: 0.3336844199925001
ROC train: 0.965454	val: 0.722344	test: 0.792210
PRC train: 0.945990	val: 0.950262	test: 0.797150

Epoch: 23
Loss: 0.3095301592184938
ROC train: 0.966869	val: 0.708425	test: 0.805947
PRC train: 0.949598	val: 0.946500	test: 0.807120

Epoch: 24
Loss: 0.31676897284800526
ROC train: 0.969795	val: 0.687179	test: 0.809077
PRC train: 0.952922	val: 0.939890	test: 0.806671

Epoch: 25
Loss: 0.29776313074364535
ROC train: 0.975796	val: 0.664469	test: 0.821248
PRC train: 0.960676	val: 0.932734	test: 0.811657

Epoch: 26
Loss: 0.30751647932051046
ROC train: 0.976550	val: 0.674725	test: 0.823857
PRC train: 0.963069	val: 0.934022	test: 0.802248

Epoch: 27
Loss: 0.2859223805101591
ROC train: 0.974620	val: 0.691209	test: 0.809598
PRC train: 0.961607	val: 0.936507	test: 0.787699

Epoch: 28
Loss: 0.28182878341575524
ROC train: 0.976538	val: 0.701099	test: 0.800556
PRC train: 0.965358	val: 0.939541	test: 0.792118

Epoch: 29
Loss: 0.29774857386312803
ROC train: 0.979900	val: 0.693407	test: 0.805251
PRC train: 0.970046	val: 0.937889	test: 0.802452

Epoch: 30
Loss: 0.26661711922669684
ROC train: 0.984832	val: 0.678388	test: 0.801774
PRC train: 0.975866	val: 0.936351	test: 0.811768

Epoch: 31
Loss: 0.2645630038739172
ROC train: 0.985733	val: 0.688278	test: 0.813772
PRC train: 0.975510	val: 0.940305	test: 0.820199

Epoch: 32
Loss: 0.2734104607618174
ROC train: 0.985203	val: 0.689377	test: 0.806990Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: bace
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/bace/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/bace/noise=0.1/bace_scaff_2_20-05_14-43-32  ]
[ Using Seed :  2  ]
[ Using device :  cuda:2  ]
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6873913851595482
ROC train: 0.627671	val: 0.537363	test: 0.605982
PRC train: 0.506359	val: 0.891583	test: 0.596148

Epoch: 2
Loss: 0.653703603242192
ROC train: 0.674472	val: 0.590842	test: 0.601808
PRC train: 0.556266	val: 0.897790	test: 0.559855

Epoch: 3
Loss: 0.6397701319487284
ROC train: 0.706767	val: 0.593040	test: 0.604069
PRC train: 0.588435	val: 0.891124	test: 0.570151

Epoch: 4
Loss: 0.6215932606710521
ROC train: 0.742155	val: 0.609524	test: 0.622327
PRC train: 0.624543	val: 0.896173	test: 0.588013

Epoch: 5
Loss: 0.589284733572086
ROC train: 0.784384	val: 0.610256	test: 0.632412
PRC train: 0.677804	val: 0.894036	test: 0.611062

Epoch: 6
Loss: 0.5721239808985029
ROC train: 0.807058	val: 0.629304	test: 0.658668
PRC train: 0.710395	val: 0.907597	test: 0.628219

Epoch: 7
Loss: 0.5449460519030989
ROC train: 0.829135	val: 0.653114	test: 0.653625
PRC train: 0.741308	val: 0.919884	test: 0.625313

Epoch: 8
Loss: 0.5518924814189987
ROC train: 0.852166	val: 0.653114	test: 0.685794
PRC train: 0.767274	val: 0.924495	test: 0.652429

Epoch: 9
Loss: 0.5175562550852358
ROC train: 0.870434	val: 0.694139	test: 0.686315
PRC train: 0.791850	val: 0.936247	test: 0.655893

Epoch: 10
Loss: 0.5258523170672789
ROC train: 0.887437	val: 0.680586	test: 0.689271
PRC train: 0.811838	val: 0.932274	test: 0.666420

Epoch: 11
Loss: 0.49246836607661126
ROC train: 0.891744	val: 0.647619	test: 0.710137
PRC train: 0.821735	val: 0.915857	test: 0.682469

Epoch: 12
Loss: 0.4737051862365842
ROC train: 0.906367	val: 0.693407	test: 0.711876
PRC train: 0.839440	val: 0.934781	test: 0.679976

Epoch: 13
Loss: 0.466154248814466
ROC train: 0.918713	val: 0.703297	test: 0.725961
PRC train: 0.862372	val: 0.943141	test: 0.702454

Epoch: 14
Loss: 0.43764221562452504
ROC train: 0.929235	val: 0.684982	test: 0.744392
PRC train: 0.879060	val: 0.937839	test: 0.719151

Epoch: 15
Loss: 0.4257615033983937
ROC train: 0.930830	val: 0.696337	test: 0.760563
PRC train: 0.886824	val: 0.941160	test: 0.726422

Epoch: 16
Loss: 0.428937656509385
ROC train: 0.943730	val: 0.721612	test: 0.761085
PRC train: 0.905665	val: 0.949312	test: 0.724493

Epoch: 17
Loss: 0.40888790172151823
ROC train: 0.951344	val: 0.720879	test: 0.760563
PRC train: 0.918993	val: 0.949779	test: 0.732575

Epoch: 18
Loss: 0.3791303437670185
ROC train: 0.960491	val: 0.707692	test: 0.769779
PRC train: 0.933929	val: 0.947504	test: 0.729258

Epoch: 19
Loss: 0.387228592431916
ROC train: 0.966835	val: 0.702564	test: 0.779169
PRC train: 0.942147	val: 0.946103	test: 0.731532

Epoch: 20
Loss: 0.3737087559588037
ROC train: 0.970400	val: 0.696703	test: 0.764215
PRC train: 0.947666	val: 0.942292	test: 0.725950

Epoch: 21
Loss: 0.36719189694786253
ROC train: 0.971881	val: 0.712454	test: 0.746653
PRC train: 0.950598	val: 0.945724	test: 0.703858

Epoch: 22
Loss: 0.3583808453010543
ROC train: 0.978231	val: 0.734066	test: 0.771518
PRC train: 0.963808	val: 0.953517	test: 0.726533

Epoch: 23
Loss: 0.327108327581514
ROC train: 0.983176	val: 0.733700	test: 0.773083
PRC train: 0.971251	val: 0.952328	test: 0.743804

Epoch: 24
Loss: 0.30810588092391417
ROC train: 0.984966	val: 0.743590	test: 0.777778
PRC train: 0.974395	val: 0.955013	test: 0.735655

Epoch: 25
Loss: 0.3169010705306172
ROC train: 0.985006	val: 0.745421	test: 0.776213
PRC train: 0.974901	val: 0.954896	test: 0.740587

Epoch: 26
Loss: 0.3181768304966842
ROC train: 0.986424	val: 0.734432	test: 0.782994
PRC train: 0.975997	val: 0.953357	test: 0.747817

Epoch: 27
Loss: 0.2974704068858748
ROC train: 0.987780	val: 0.723810	test: 0.776387
PRC train: 0.979873	val: 0.950670	test: 0.746733

Epoch: 28
Loss: 0.28918027612418984
ROC train: 0.987780	val: 0.733333	test: 0.759172
PRC train: 0.980642	val: 0.952419	test: 0.723220

Epoch: 29
Loss: 0.2528352618243912
ROC train: 0.990771	val: 0.736630	test: 0.758825
PRC train: 0.984336	val: 0.952229	test: 0.705591

Epoch: 30
Loss: 0.25845904246678353
ROC train: 0.992840	val: 0.742491	test: 0.772040
PRC train: 0.988865	val: 0.953335	test: 0.723497

Epoch: 31
Loss: 0.2735303750682928
ROC train: 0.993248	val: 0.711722	test: 0.779690
PRC train: 0.989291	val: 0.947968	test: 0.724551

Epoch: 32
Loss: 0.2641412199632023
ROC train: 0.994695	val: 0.669231	test: 0.783516
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: bace
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/bace/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/bace/noise=0.1/bace_scaff_3_20-05_14-43-32  ]
[ Using Seed :  3  ]
[ Using device :  cuda:2  ]
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6793190581066082
ROC train: 0.640685	val: 0.599267	test: 0.638498
PRC train: 0.527308	val: 0.908115	test: 0.579067

Epoch: 2
Loss: 0.6533291375143765
ROC train: 0.696558	val: 0.637729	test: 0.638845
PRC train: 0.582823	val: 0.910144	test: 0.589233

Epoch: 3
Loss: 0.6324387232014466
ROC train: 0.737997	val: 0.624176	test: 0.605634
PRC train: 0.621875	val: 0.899185	test: 0.590632

Epoch: 4
Loss: 0.6043450353878904
ROC train: 0.765517	val: 0.606227	test: 0.641280
PRC train: 0.657578	val: 0.894368	test: 0.633215

Epoch: 5
Loss: 0.5828939992039902
ROC train: 0.793319	val: 0.625275	test: 0.680577
PRC train: 0.695634	val: 0.900807	test: 0.658606

Epoch: 6
Loss: 0.5644954666739299
ROC train: 0.822745	val: 0.656044	test: 0.715354
PRC train: 0.728081	val: 0.907438	test: 0.684000

Epoch: 7
Loss: 0.5478151700780309
ROC train: 0.845953	val: 0.661172	test: 0.728917
PRC train: 0.760109	val: 0.909228	test: 0.687190

Epoch: 8
Loss: 0.5312175163874645
ROC train: 0.862275	val: 0.658608	test: 0.742132
PRC train: 0.782398	val: 0.909648	test: 0.690228

Epoch: 9
Loss: 0.5113709521553776
ROC train: 0.876276	val: 0.642491	test: 0.727004
PRC train: 0.804338	val: 0.904562	test: 0.688713

Epoch: 10
Loss: 0.4999610334851571
ROC train: 0.885785	val: 0.664103	test: 0.727178
PRC train: 0.823346	val: 0.911291	test: 0.686118

Epoch: 11
Loss: 0.47931475776362353
ROC train: 0.900171	val: 0.679487	test: 0.733264
PRC train: 0.844180	val: 0.924483	test: 0.687949

Epoch: 12
Loss: 0.46937784733171206
ROC train: 0.905009	val: 0.649084	test: 0.734133
PRC train: 0.852839	val: 0.899463	test: 0.706924

Epoch: 13
Loss: 0.461449998361571
ROC train: 0.912471	val: 0.639927	test: 0.733264
PRC train: 0.863801	val: 0.916405	test: 0.711922

Epoch: 14
Loss: 0.45009868957197796
ROC train: 0.922660	val: 0.628938	test: 0.727526
PRC train: 0.881167	val: 0.900003	test: 0.715027

Epoch: 15
Loss: 0.4370820397675038
ROC train: 0.933699	val: 0.681685	test: 0.737089
PRC train: 0.901032	val: 0.911181	test: 0.713602

Epoch: 16
Loss: 0.41365191456874023
ROC train: 0.945360	val: 0.694872	test: 0.764041
PRC train: 0.920984	val: 0.929701	test: 0.731654

Epoch: 17
Loss: 0.40289756086823153
ROC train: 0.952437	val: 0.694505	test: 0.768736
PRC train: 0.929345	val: 0.933379	test: 0.745891

Epoch: 18
Loss: 0.3856743496309809
ROC train: 0.959361	val: 0.686813	test: 0.764563
PRC train: 0.938370	val: 0.925798	test: 0.744144

Epoch: 19
Loss: 0.3742744073892782
ROC train: 0.961433	val: 0.697802	test: 0.761259
PRC train: 0.942582	val: 0.935426	test: 0.739778

Epoch: 20
Loss: 0.39969807036836685
ROC train: 0.964155	val: 0.693773	test: 0.775343
PRC train: 0.947950	val: 0.934017	test: 0.754035

Epoch: 21
Loss: 0.353129133568448
ROC train: 0.968122	val: 0.680586	test: 0.773083
PRC train: 0.953088	val: 0.934087	test: 0.756185

Epoch: 22
Loss: 0.35560821067397147
ROC train: 0.972888	val: 0.640293	test: 0.777082
PRC train: 0.961057	val: 0.912588	test: 0.752594

Epoch: 23
Loss: 0.3355462339829257
ROC train: 0.977466	val: 0.674725	test: 0.760563
PRC train: 0.967387	val: 0.933696	test: 0.744200

Epoch: 24
Loss: 0.30460462977477815
ROC train: 0.978416	val: 0.643590	test: 0.752217
PRC train: 0.968168	val: 0.924127	test: 0.735764

Epoch: 25
Loss: 0.30482919961007215
ROC train: 0.982663	val: 0.640293	test: 0.781082
PRC train: 0.974568	val: 0.920892	test: 0.764660

Epoch: 26
Loss: 0.2996390714482592
ROC train: 0.983987	val: 0.689377	test: 0.782299
PRC train: 0.976440	val: 0.939822	test: 0.780565

Epoch: 27
Loss: 0.28716631252727404
ROC train: 0.985565	val: 0.678388	test: 0.788037
PRC train: 0.978339	val: 0.933846	test: 0.784669

Epoch: 28
Loss: 0.27927833006092284
ROC train: 0.986684	val: 0.675092	test: 0.778647
PRC train: 0.980753	val: 0.924590	test: 0.766915

Epoch: 29
Loss: 0.2862145703411424
ROC train: 0.988876	val: 0.689744	test: 0.783516
PRC train: 0.984396	val: 0.939000	test: 0.780335

Epoch: 30
Loss: 0.2842226724392419
ROC train: 0.993924	val: 0.660806	test: 0.795166
PRC train: 0.991647	val: 0.919672	test: 0.793178

Epoch: 31
Loss: 0.2594438673491342
ROC train: 0.994538	val: 0.668498	test: 0.778299
PRC train: 0.992500	val: 0.925349	test: 0.781053

Epoch: 32
Loss: 0.25287384165981464
ROC train: 0.994912	val: 0.686081	test: 0.776213Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/bace/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:3
  input_data_dir: 
  dataset: bace
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/bace/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/bace/noise=0.2/bace_scaff_1_20-05_14-43-32  ]
[ Using Seed :  1  ]
[ Using device :  cuda:3  ]
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6780614540946444
ROC train: 0.613816	val: 0.638095	test: 0.620066
PRC train: 0.500994	val: 0.910927	test: 0.641138

Epoch: 2
Loss: 0.6562845659991661
ROC train: 0.650377	val: 0.624176	test: 0.676056
PRC train: 0.524669	val: 0.907348	test: 0.668309

Epoch: 3
Loss: 0.6484236865232242
ROC train: 0.682814	val: 0.621978	test: 0.656755
PRC train: 0.562475	val: 0.902002	test: 0.653387

Epoch: 4
Loss: 0.6338111903249958
ROC train: 0.719501	val: 0.591941	test: 0.539558
PRC train: 0.609285	val: 0.896060	test: 0.574987

Epoch: 5
Loss: 0.6300449843202225
ROC train: 0.732897	val: 0.566667	test: 0.477308
PRC train: 0.623720	val: 0.884765	test: 0.538348

Epoch: 6
Loss: 0.6166255288803411
ROC train: 0.762483	val: 0.548352	test: 0.505303
PRC train: 0.652921	val: 0.873852	test: 0.562336

Epoch: 7
Loss: 0.6009695539203117
ROC train: 0.793291	val: 0.557143	test: 0.560772
PRC train: 0.684928	val: 0.875598	test: 0.584672

Epoch: 8
Loss: 0.5868527659942564
ROC train: 0.820017	val: 0.577289	test: 0.596592
PRC train: 0.720477	val: 0.885555	test: 0.611957

Epoch: 9
Loss: 0.5734242326562533
ROC train: 0.842977	val: 0.574359	test: 0.613459
PRC train: 0.756930	val: 0.878008	test: 0.628345

Epoch: 10
Loss: 0.5596362938768105
ROC train: 0.863465	val: 0.561905	test: 0.630847
PRC train: 0.792212	val: 0.862567	test: 0.646235

Epoch: 11
Loss: 0.5368741248421735
ROC train: 0.880174	val: 0.549084	test: 0.638845
PRC train: 0.821184	val: 0.851808	test: 0.658131

Epoch: 12
Loss: 0.5387777730323801
ROC train: 0.895439	val: 0.550549	test: 0.640410
PRC train: 0.843491	val: 0.846923	test: 0.659908

Epoch: 13
Loss: 0.5148865016789441
ROC train: 0.905959	val: 0.544689	test: 0.674491
PRC train: 0.857628	val: 0.854401	test: 0.676160

Epoch: 14
Loss: 0.49182938133707876
ROC train: 0.919820	val: 0.554945	test: 0.663885
PRC train: 0.881974	val: 0.855131	test: 0.672311

Epoch: 15
Loss: 0.47665342680510553
ROC train: 0.929983	val: 0.575824	test: 0.692923
PRC train: 0.900336	val: 0.862054	test: 0.696016

Epoch: 16
Loss: 0.48892573302633774
ROC train: 0.940177	val: 0.580952	test: 0.709790
PRC train: 0.913997	val: 0.864302	test: 0.715210

Epoch: 17
Loss: 0.44071729334693976
ROC train: 0.948527	val: 0.564103	test: 0.659537
PRC train: 0.925119	val: 0.851903	test: 0.670986

Epoch: 18
Loss: 0.435459450923881
ROC train: 0.955505	val: 0.544322	test: 0.629456
PRC train: 0.934854	val: 0.847804	test: 0.654410

Epoch: 19
Loss: 0.421880640554646
ROC train: 0.962959	val: 0.551648	test: 0.631716
PRC train: 0.946373	val: 0.849394	test: 0.659887

Epoch: 20
Loss: 0.3986539252992901
ROC train: 0.968901	val: 0.559707	test: 0.652234
PRC train: 0.954844	val: 0.855850	test: 0.675510

Epoch: 21
Loss: 0.3827830704909006
ROC train: 0.975996	val: 0.547253	test: 0.686142
PRC train: 0.964620	val: 0.848288	test: 0.698841

Epoch: 22
Loss: 0.3863055849520405
ROC train: 0.980676	val: 0.563736	test: 0.691010
PRC train: 0.971060	val: 0.863252	test: 0.698379

Epoch: 23
Loss: 0.3714670011968507
ROC train: 0.981555	val: 0.573626	test: 0.680925
PRC train: 0.973325	val: 0.868932	test: 0.693669

Epoch: 24
Loss: 0.36242670816359635
ROC train: 0.985922	val: 0.570696	test: 0.718658
PRC train: 0.979405	val: 0.869644	test: 0.729401

Epoch: 25
Loss: 0.3434165014013687
ROC train: 0.991256	val: 0.568132	test: 0.733438
PRC train: 0.987309	val: 0.860721	test: 0.743903

Epoch: 26
Loss: 0.31850119756443307
ROC train: 0.991561	val: 0.557875	test: 0.720049
PRC train: 0.988082	val: 0.851926	test: 0.739769

Epoch: 27
Loss: 0.33773807297882386
ROC train: 0.993382	val: 0.571429	test: 0.768214
PRC train: 0.990093	val: 0.862772	test: 0.775902

Epoch: 28
Loss: 0.30152664756389275
ROC train: 0.993776	val: 0.587179	test: 0.755173
PRC train: 0.990629	val: 0.877732	test: 0.765555

Epoch: 29
Loss: 0.28961161831223114
ROC train: 0.995890	val: 0.603297	test: 0.736741
PRC train: 0.994236	val: 0.881121	test: 0.738109

Epoch: 30
Loss: 0.30402286976796955
ROC train: 0.997691	val: 0.585348	test: 0.689445
PRC train: 0.996620	val: 0.873723	test: 0.711571

Epoch: 31
Loss: 0.2753953040893194
ROC train: 0.996450	val: 0.572527	test: 0.667362
PRC train: 0.994534	val: 0.862170	test: 0.701083

Epoch: 32
Loss: 0.29931999422074307
ROC train: 0.997725	val: 0.587546	test: 0.746131
PRC train: 0.912279	val: 0.928385	test: 0.755795

Epoch: 34
Loss: 0.33518644259151714
ROC train: 0.947905	val: 0.676557	test: 0.747870
PRC train: 0.918658	val: 0.929025	test: 0.737036

Epoch: 35
Loss: 0.3269091129265858
ROC train: 0.945057	val: 0.677656	test: 0.727700
PRC train: 0.913304	val: 0.931448	test: 0.724248

Epoch: 36
Loss: 0.33429020581654134
ROC train: 0.945197	val: 0.681319	test: 0.730134
PRC train: 0.913637	val: 0.933254	test: 0.728009

Epoch: 37
Loss: 0.3278485523380949
ROC train: 0.948042	val: 0.683150	test: 0.756564
PRC train: 0.918636	val: 0.932798	test: 0.755131

Epoch: 38
Loss: 0.30497380366574445
ROC train: 0.951361	val: 0.675824	test: 0.748565
PRC train: 0.923279	val: 0.930157	test: 0.749484

Epoch: 39
Loss: 0.3101900910059313
ROC train: 0.954184	val: 0.686813	test: 0.735698
PRC train: 0.927150	val: 0.930814	test: 0.742245

Epoch: 40
Loss: 0.31651517377552996
ROC train: 0.952851	val: 0.675824	test: 0.727700
PRC train: 0.925218	val: 0.928333	test: 0.738631

Epoch: 41
Loss: 0.31698101097006914
ROC train: 0.957058	val: 0.684615	test: 0.730134
PRC train: 0.933294	val: 0.929997	test: 0.748433

Epoch: 42
Loss: 0.3280549137841942
ROC train: 0.957931	val: 0.689011	test: 0.739350
PRC train: 0.934748	val: 0.928075	test: 0.761966

Epoch: 43
Loss: 0.3125832946670374
ROC train: 0.957725	val: 0.684982	test: 0.745436
PRC train: 0.935150	val: 0.925789	test: 0.762179

Epoch: 44
Loss: 0.30993113663078087
ROC train: 0.955537	val: 0.668498	test: 0.738480
PRC train: 0.930571	val: 0.922298	test: 0.745995

Epoch: 45
Loss: 0.3059344265052251
ROC train: 0.956998	val: 0.690842	test: 0.749435
PRC train: 0.932374	val: 0.930243	test: 0.757905

Epoch: 46
Loss: 0.3124070054634376
ROC train: 0.958950	val: 0.692308	test: 0.747001
PRC train: 0.935907	val: 0.932891	test: 0.758961

Epoch: 47
Loss: 0.3007082966316485
ROC train: 0.960905	val: 0.679487	test: 0.745609
PRC train: 0.938946	val: 0.928950	test: 0.756719

Epoch: 48
Loss: 0.3041500561752189
ROC train: 0.963005	val: 0.655311	test: 0.756042
PRC train: 0.942332	val: 0.923264	test: 0.761381

Epoch: 49
Loss: 0.30521499088545967
ROC train: 0.964749	val: 0.668498	test: 0.747696
PRC train: 0.943854	val: 0.928503	test: 0.756058

Epoch: 50
Loss: 0.29861854153526196
ROC train: 0.961906	val: 0.678388	test: 0.733438
PRC train: 0.939599	val: 0.929502	test: 0.734590

Epoch: 51
Loss: 0.28161318779082173
ROC train: 0.960011	val: 0.665934	test: 0.719353
PRC train: 0.937530	val: 0.925788	test: 0.719501

Epoch: 52
Loss: 0.3100609102567556
ROC train: 0.965850	val: 0.677656	test: 0.735524
PRC train: 0.946618	val: 0.924601	test: 0.740235

Epoch: 53
Loss: 0.293008524918241
ROC train: 0.965514	val: 0.683883	test: 0.736915
PRC train: 0.947086	val: 0.926897	test: 0.745824

Epoch: 54
Loss: 0.2841954482171437
ROC train: 0.966127	val: 0.684615	test: 0.739002
PRC train: 0.947655	val: 0.924988	test: 0.736502

Epoch: 55
Loss: 0.2887756455953211
ROC train: 0.964678	val: 0.675458	test: 0.732394
PRC train: 0.945274	val: 0.922988	test: 0.723678

Epoch: 56
Loss: 0.29653716205481584
ROC train: 0.967871	val: 0.660440	test: 0.733090
PRC train: 0.950087	val: 0.919951	test: 0.726073

Epoch: 57
Loss: 0.28926655315249195
ROC train: 0.966336	val: 0.655678	test: 0.720223
PRC train: 0.948169	val: 0.917400	test: 0.713321

Epoch: 58
Loss: 0.29093931794709726
ROC train: 0.967061	val: 0.663370	test: 0.723352
PRC train: 0.950035	val: 0.915510	test: 0.725823

Epoch: 59
Loss: 0.28120392578883024
ROC train: 0.969421	val: 0.668132	test: 0.725787
PRC train: 0.954598	val: 0.916960	test: 0.736279

Epoch: 60
Loss: 0.2815136095828759
ROC train: 0.969087	val: 0.670330	test: 0.719875
PRC train: 0.953129	val: 0.924339	test: 0.718068

Epoch: 61
Loss: 0.27929877521836133
ROC train: 0.965619	val: 0.669597	test: 0.711876
PRC train: 0.947979	val: 0.923664	test: 0.703049

Epoch: 62
Loss: 0.2820086911932516
ROC train: 0.968231	val: 0.662637	test: 0.719179
PRC train: 0.951511	val: 0.916687	test: 0.732173

Epoch: 63
Loss: 0.2760197298360644
ROC train: 0.970111	val: 0.673260	test: 0.733959
PRC train: 0.952359	val: 0.919356	test: 0.747343

Epoch: 64
Loss: 0.27267958083844623
ROC train: 0.972914	val: 0.674359	test: 0.731177
PRC train: 0.957574	val: 0.919750	test: 0.740418

Epoch: 65
Loss: 0.25754027348503455
ROC train: 0.970711	val: 0.686447	test: 0.713615
PRC train: 0.954681	val: 0.926450	test: 0.721005

Epoch: 66
Loss: 0.27506655270182356
ROC train: 0.968168	val: 0.720513	test: 0.705790
PRC train: 0.951483	val: 0.938988	test: 0.721149

Epoch: 67
Loss: 0.2693335012613035
ROC train: 0.974053	val: 0.709524	test: 0.744566
PRC train: 0.960930	val: 0.935368	test: 0.757775

Epoch: 68
Loss: 0.27361443431590654
ROC train: 0.975274	val: 0.691941	test: 0.738306
PRC train: 0.962537	val: 0.926752	test: 0.741492

Epoch: 69
Loss: 0.2634532846072174
ROC train: 0.974207	val: 0.681685	test: 0.717093
PRC train: 0.960553	val: 0.925507	test: 0.715290

Epoch: 70
Loss: 0.25755538709094933
ROC train: 0.973687	val: 0.663370	test: 0.706660
PRC train: 0.960513	val: 0.921735	test: 0.695694

Epoch: 71
Loss: 0.28536598615601116
ROC train: 0.973604	val: 0.674725	test: 0.699009
PRC train: 0.959433	val: 0.927423	test: 0.701213

Epoch: 72
Loss: 0.23936340597736513
ROC train: 0.973482	val: 0.694139	test: 0.685446
PRC train: 0.959766	val: 0.934101	test: 0.686083

Epoch: 73
Loss: 0.25960575749261305
ROC train: 0.975505	val: 0.702564	test: 0.694140
PRC train: 0.962334	val: 0.935951	test: 0.688035

Epoch: 74
Loss: 0.26594454755760094
ROC train: 0.976687	val: 0.678388	test: 0.712572
PRC train: 0.965198	val: 0.929508	test: 0.711194

Epoch: 75
Loss: 0.2557844614576722
ROC train: 0.975177	val: 0.682051	test: 0.702139
PRC train: 0.962756	val: 0.927762	test: 0.697948

Epoch: 76
Loss: 0.2522157791367649
ROC train: 0.978333	val: 0.683150	test: 0.718832
PRC train: 0.966560	val: 0.929183	test: 0.716189

Epoch: 77
Loss: 0.2647012326535586
ROC train: 0.978396	val: 0.693407	test: 0.715006
PRC train: 0.966136	val: 0.934052	test: 0.712544

Epoch: 78
Loss: 0.23097646119707696
ROC train: 0.977514	val: 0.676557	test: 0.709094
PRC train: 0.965231	val: 0.930810	test: 0.705201

Epoch: 79
Loss: 0.24611843924313437
ROC train: 0.979067	val: 0.669963	test: 0.711876
PRC train: 0.968698	val: 0.925981	test: 0.715364

Epoch: 80
Loss: 0.24885709529082414
ROC train: 0.975188	val: 0.659341	test: 0.707355
PRC train: 0.962199	val: 0.922962	test: 0.710321

Epoch: 81
Loss: 0.226603993398723
ROC train: 0.979666	val: 0.668132	test: 0.707007
PRC train: 0.968752	val: 0.924727	test: 0.703449

Epoch: 82
Loss: 0.2593680675421921
ROC train: 0.979269	val: 0.676190	test: 0.702660
PRC train: 0.969134	val: 0.925467	test: 0.692595

Epoch: 83
Loss: 0.2344025721509114
ROC train: 0.978984	val: 0.672161	test: 0.712572
PRC train: 0.968959	val: 0.924362	test: 0.708720

Epoch: 84
Loss: 0.2594412395062475
ROC train: 0.979561	val: 0.651648	test: 0.704051
PRC train: 0.969906	val: 0.918909	test: 0.694725

Epoch: 85
Loss: 0.24341953743660244
ROC train: 0.979044	val: 0.652381	test: 0.698313
PRC train: 0.968778	val: 0.917903	test: 0.698404

Epoch: 86
Loss: 0.23859455040699268
ROC train: 0.980240	val: 0.678388	test: 0.701965
PRC train: 0.970118	val: 0.926936	test: 0.701517

Epoch: 87
Loss: 0.2420659431363199
ROC train: 0.979703	val: 0.674359	test: 0.697618
PRC train: 0.969064	val: 0.927578	test: 0.696497

Epoch: 88
Loss: 0.248865516932374
ROC train: 0.981153	val: 0.661905	test: 0.685620
PRC train: 0.971647	val: 0.922747	test: 0.680275

Epoch: 89
Loss: 0.23221442432797818
ROC train: 0.980268	val: 0.651648	test: 0.671362
PRC train: 0.970594	val: 0.916926	test: 0.676700

Epoch: 90
Loss: 0.2491333863477881
ROC train: 0.981159	val: 0.654212	test: 0.684924
PRC train: 0.971864	val: 0.914704	test: 0.684942

Epoch: 91
Loss: 0.22884800078202067
ROC train: 0.980776	val: 0.662271	test: 0.705095
PRC train: 0.971463	val: 0.916801	test: 0.692744

Epoch: 92
Loss: 0.24156983032274545
ROC train: 0.979047	val: 0.657143	test: 0.695705
PRC train: 0.969173	val: 0.917453	test: 0.691829

Epoch: 93
Loss: 0.22752669723722602
ROC train: 0.982837	val: 0.646154	test: 0.709442
PRC train: 0.974878	val: 0.914798	test: 0.703650

Epoch: 94
Loss: 0.22176080326996866
ROC train: 0.980531	val: 0.675092	test: 0.701791
PRC train: 0.923586	val: 0.921640	test: 0.737698

Epoch: 34
Loss: 0.3310442175772068
ROC train: 0.948582	val: 0.661172	test: 0.720570
PRC train: 0.919738	val: 0.926074	test: 0.717525

Epoch: 35
Loss: 0.32726554047072487
ROC train: 0.953993	val: 0.656777	test: 0.736915
PRC train: 0.929173	val: 0.925601	test: 0.737589

Epoch: 36
Loss: 0.3173934684690583
ROC train: 0.955437	val: 0.663370	test: 0.728395
PRC train: 0.930967	val: 0.926025	test: 0.728211

Epoch: 37
Loss: 0.3045555213467369
ROC train: 0.956067	val: 0.669597	test: 0.725091
PRC train: 0.931508	val: 0.925928	test: 0.732047

Epoch: 38
Loss: 0.32367275490646014
ROC train: 0.958074	val: 0.661538	test: 0.722135
PRC train: 0.934095	val: 0.923536	test: 0.731628

Epoch: 39
Loss: 0.29545691862884343
ROC train: 0.957517	val: 0.677656	test: 0.725439
PRC train: 0.934766	val: 0.928455	test: 0.729090

Epoch: 40
Loss: 0.30294147656527365
ROC train: 0.957760	val: 0.682784	test: 0.739350
PRC train: 0.935561	val: 0.929311	test: 0.731575

Epoch: 41
Loss: 0.2998152018303852
ROC train: 0.959312	val: 0.669231	test: 0.750652
PRC train: 0.936922	val: 0.924472	test: 0.744665

Epoch: 42
Loss: 0.30657610873323116
ROC train: 0.959663	val: 0.666667	test: 0.748565
PRC train: 0.935981	val: 0.923722	test: 0.748652

Epoch: 43
Loss: 0.29964007926399633
ROC train: 0.959983	val: 0.677289	test: 0.741784
PRC train: 0.936396	val: 0.925369	test: 0.744061

Epoch: 44
Loss: 0.3078795559957836
ROC train: 0.961259	val: 0.674725	test: 0.740219
PRC train: 0.939648	val: 0.926679	test: 0.749242

Epoch: 45
Loss: 0.29597746937659575
ROC train: 0.961196	val: 0.679121	test: 0.745783
PRC train: 0.938939	val: 0.927077	test: 0.751624

Epoch: 46
Loss: 0.30874241140878084
ROC train: 0.964409	val: 0.666300	test: 0.754477
PRC train: 0.945244	val: 0.924132	test: 0.762297

Epoch: 47
Loss: 0.29627954871581086
ROC train: 0.965471	val: 0.663370	test: 0.739350
PRC train: 0.947518	val: 0.925486	test: 0.748211

Epoch: 48
Loss: 0.29961810648156206
ROC train: 0.961241	val: 0.671429	test: 0.725265
PRC train: 0.940836	val: 0.929051	test: 0.735037

Epoch: 49
Loss: 0.29479998531156976
ROC train: 0.966053	val: 0.650183	test: 0.739871
PRC train: 0.946683	val: 0.922620	test: 0.749921

Epoch: 50
Loss: 0.28719815546850586
ROC train: 0.965762	val: 0.642491	test: 0.736394
PRC train: 0.947023	val: 0.917936	test: 0.745385

Epoch: 51
Loss: 0.28976771760599435
ROC train: 0.966844	val: 0.643956	test: 0.723700
PRC train: 0.949160	val: 0.919503	test: 0.731532

Epoch: 52
Loss: 0.28285162193604985
ROC train: 0.966116	val: 0.671062	test: 0.711528
PRC train: 0.949044	val: 0.924988	test: 0.719299

Epoch: 53
Loss: 0.28800931796720064
ROC train: 0.967103	val: 0.665934	test: 0.704747
PRC train: 0.950205	val: 0.928515	test: 0.712299

Epoch: 54
Loss: 0.2775648179341269
ROC train: 0.968196	val: 0.672894	test: 0.727526
PRC train: 0.950747	val: 0.931545	test: 0.737978

Epoch: 55
Loss: 0.2843738268836343
ROC train: 0.968385	val: 0.657509	test: 0.747348
PRC train: 0.951248	val: 0.925468	test: 0.755691

Epoch: 56
Loss: 0.27258568445125136
ROC train: 0.969050	val: 0.661172	test: 0.746827
PRC train: 0.952110	val: 0.926617	test: 0.755771

Epoch: 57
Loss: 0.277521544258722
ROC train: 0.969797	val: 0.660073	test: 0.718832
PRC train: 0.953166	val: 0.923711	test: 0.724509

Epoch: 58
Loss: 0.2688590766797253
ROC train: 0.969883	val: 0.649817	test: 0.710659
PRC train: 0.952710	val: 0.920213	test: 0.717102

Epoch: 59
Loss: 0.2656037238366763
ROC train: 0.968345	val: 0.650183	test: 0.720223
PRC train: 0.951648	val: 0.919957	test: 0.730289

Epoch: 60
Loss: 0.266443519074149
ROC train: 0.972982	val: 0.642125	test: 0.746827
PRC train: 0.958710	val: 0.917001	test: 0.759605

Epoch: 61
Loss: 0.2752246297163586
ROC train: 0.971247	val: 0.633700	test: 0.744045
PRC train: 0.955317	val: 0.905179	test: 0.757083

Epoch: 62
Loss: 0.27725597820769565
ROC train: 0.971133	val: 0.652381	test: 0.742306
PRC train: 0.954534	val: 0.915128	test: 0.753622

Epoch: 63
Loss: 0.2708930196488469
ROC train: 0.972195	val: 0.653480	test: 0.734307
PRC train: 0.957849	val: 0.917717	test: 0.740145

Epoch: 64
Loss: 0.2661369987251062
ROC train: 0.973907	val: 0.623810	test: 0.735524
PRC train: 0.960116	val: 0.911781	test: 0.746062

Epoch: 65
Loss: 0.253293004745759
ROC train: 0.975534	val: 0.661172	test: 0.731351
PRC train: 0.962003	val: 0.919506	test: 0.733406

Epoch: 66
Loss: 0.26727646511368996
ROC train: 0.973116	val: 0.662637	test: 0.716745
PRC train: 0.958545	val: 0.921935	test: 0.717058

Epoch: 67
Loss: 0.2525592347292972
ROC train: 0.974164	val: 0.652015	test: 0.725613
PRC train: 0.961612	val: 0.921050	test: 0.727652

Epoch: 68
Loss: 0.2605226408622554
ROC train: 0.972700	val: 0.665568	test: 0.725961
PRC train: 0.960836	val: 0.923541	test: 0.734705

Epoch: 69
Loss: 0.2580449213948571
ROC train: 0.975194	val: 0.665934	test: 0.740741
PRC train: 0.963800	val: 0.920038	test: 0.751121

Epoch: 70
Loss: 0.2552084257679851
ROC train: 0.978131	val: 0.642857	test: 0.740219
PRC train: 0.967541	val: 0.914411	test: 0.746244

Epoch: 71
Loss: 0.25398925543888823
ROC train: 0.978727	val: 0.630403	test: 0.741958
PRC train: 0.967854	val: 0.910437	test: 0.757757

Epoch: 72
Loss: 0.2532239480753112
ROC train: 0.978542	val: 0.635165	test: 0.727352
PRC train: 0.967098	val: 0.912115	test: 0.742171

Epoch: 73
Loss: 0.24204363467311135
ROC train: 0.977825	val: 0.653114	test: 0.731699
PRC train: 0.966431	val: 0.918127	test: 0.745946

Epoch: 74
Loss: 0.27391056163406247
ROC train: 0.977469	val: 0.647253	test: 0.729612
PRC train: 0.966293	val: 0.919384	test: 0.734382

Epoch: 75
Loss: 0.2493079703874213
ROC train: 0.977877	val: 0.666300	test: 0.728395
PRC train: 0.966066	val: 0.924554	test: 0.725434

Epoch: 76
Loss: 0.24698041012394806
ROC train: 0.974620	val: 0.669597	test: 0.707007
PRC train: 0.961458	val: 0.922566	test: 0.706469

Epoch: 77
Loss: 0.24271803426340816
ROC train: 0.976122	val: 0.668132	test: 0.703530
PRC train: 0.963798	val: 0.922853	test: 0.714341

Epoch: 78
Loss: 0.23796726014490122
ROC train: 0.976524	val: 0.657143	test: 0.716571
PRC train: 0.964842	val: 0.918125	test: 0.725399

Epoch: 79
Loss: 0.24100717601283658
ROC train: 0.976875	val: 0.662271	test: 0.716397
PRC train: 0.965870	val: 0.916679	test: 0.723773

Epoch: 80
Loss: 0.24376864233832535
ROC train: 0.980522	val: 0.654212	test: 0.712919
PRC train: 0.971206	val: 0.914328	test: 0.718320

Epoch: 81
Loss: 0.24359356264470175
ROC train: 0.980679	val: 0.630037	test: 0.713441
PRC train: 0.971328	val: 0.913428	test: 0.716939

Epoch: 82
Loss: 0.2318067079880591
ROC train: 0.978008	val: 0.670330	test: 0.723700
PRC train: 0.966980	val: 0.924052	test: 0.730943

Epoch: 83
Loss: 0.23331125360026062
ROC train: 0.983659	val: 0.676923	test: 0.723179
PRC train: 0.976299	val: 0.927142	test: 0.727254

Epoch: 84
Loss: 0.23976915759118106
ROC train: 0.978916	val: 0.667766	test: 0.696227
PRC train: 0.969678	val: 0.923313	test: 0.695677

Epoch: 85
Loss: 0.26457372578824473
ROC train: 0.979341	val: 0.678388	test: 0.700400
PRC train: 0.969951	val: 0.922650	test: 0.707506

Epoch: 86
Loss: 0.22660436641192439
ROC train: 0.980599	val: 0.672161	test: 0.709442
PRC train: 0.971781	val: 0.910916	test: 0.720084

Epoch: 87
Loss: 0.24305818259212036
ROC train: 0.979013	val: 0.644689	test: 0.683359
PRC train: 0.969345	val: 0.902166	test: 0.696100

Epoch: 88
Loss: 0.22717719825020977
ROC train: 0.976436	val: 0.649451	test: 0.673448
PRC train: 0.965228	val: 0.911507	test: 0.676491

Epoch: 89
Loss: 0.22481331434549673
ROC train: 0.977566	val: 0.647619	test: 0.687880
PRC train: 0.967647	val: 0.913586	test: 0.696278

Epoch: 90
Loss: 0.23564715713419687
ROC train: 0.979823	val: 0.663004	test: 0.701617
PRC train: 0.970646	val: 0.920656	test: 0.708700

Epoch: 91
Loss: 0.22141914601505647
ROC train: 0.981256	val: 0.655311	test: 0.695183
PRC train: 0.971972	val: 0.916560	test: 0.708234

Epoch: 92
Loss: 0.21212973479268102
ROC train: 0.980063	val: 0.636996	test: 0.700226
PRC train: 0.970318	val: 0.910089	test: 0.707431

Epoch: 93
Loss: 0.2277182842087858
ROC train: 0.982962	val: 0.652381	test: 0.702313
PRC train: 0.975679	val: 0.914597	test: 0.709856

Epoch: 94
Loss: 0.22528845913372259
ROC train: 0.984737	val: 0.656044	test: 0.713267
PRC train: 0.920900	val: 0.933640	test: 0.714632

Epoch: 34
Loss: 0.3287882537991903
ROC train: 0.947506	val: 0.676190	test: 0.715354
PRC train: 0.920378	val: 0.929025	test: 0.697130

Epoch: 35
Loss: 0.3168050360735603
ROC train: 0.949989	val: 0.687912	test: 0.732394
PRC train: 0.920508	val: 0.929727	test: 0.713684

Epoch: 36
Loss: 0.3258092300837937
ROC train: 0.950805	val: 0.676190	test: 0.733959
PRC train: 0.920600	val: 0.928635	test: 0.716115

Epoch: 37
Loss: 0.33309825557720835
ROC train: 0.951104	val: 0.710256	test: 0.735176
PRC train: 0.921027	val: 0.936483	test: 0.717156

Epoch: 38
Loss: 0.33104687041150554
ROC train: 0.951772	val: 0.694139	test: 0.733612
PRC train: 0.921996	val: 0.932996	test: 0.717407

Epoch: 39
Loss: 0.3108846751747323
ROC train: 0.952900	val: 0.678388	test: 0.733612
PRC train: 0.924304	val: 0.930668	test: 0.719287

Epoch: 40
Loss: 0.3257763163601158
ROC train: 0.953830	val: 0.682418	test: 0.722831
PRC train: 0.928532	val: 0.928807	test: 0.703726

Epoch: 41
Loss: 0.3169635308845878
ROC train: 0.956855	val: 0.688645	test: 0.723874
PRC train: 0.932576	val: 0.929528	test: 0.707240

Epoch: 42
Loss: 0.3077930814102064
ROC train: 0.952386	val: 0.686081	test: 0.705443
PRC train: 0.926822	val: 0.930015	test: 0.704594

Epoch: 43
Loss: 0.3200451807590457
ROC train: 0.956761	val: 0.681685	test: 0.717093
PRC train: 0.934808	val: 0.927607	test: 0.713796

Epoch: 44
Loss: 0.31266198365115594
ROC train: 0.959267	val: 0.697436	test: 0.728047
PRC train: 0.936006	val: 0.933442	test: 0.702370

Epoch: 45
Loss: 0.30843938581818586
ROC train: 0.958328	val: 0.695604	test: 0.724222
PRC train: 0.933771	val: 0.934646	test: 0.705333

Epoch: 46
Loss: 0.31073847604135146
ROC train: 0.957871	val: 0.691209	test: 0.707529
PRC train: 0.933839	val: 0.932477	test: 0.693382

Epoch: 47
Loss: 0.2975010632083857
ROC train: 0.959292	val: 0.683150	test: 0.714137
PRC train: 0.936422	val: 0.929909	test: 0.701303

Epoch: 48
Loss: 0.2939654481052639
ROC train: 0.963225	val: 0.671062	test: 0.731351
PRC train: 0.941719	val: 0.928882	test: 0.710319

Epoch: 49
Loss: 0.30635257757270734
ROC train: 0.962945	val: 0.676190	test: 0.721266
PRC train: 0.942929	val: 0.929986	test: 0.714356

Epoch: 50
Loss: 0.28905908106321465
ROC train: 0.961749	val: 0.691941	test: 0.728743
PRC train: 0.942403	val: 0.934047	test: 0.729986

Epoch: 51
Loss: 0.284733400817603
ROC train: 0.963793	val: 0.687546	test: 0.737263
PRC train: 0.943974	val: 0.932977	test: 0.729476

Epoch: 52
Loss: 0.289530653468056
ROC train: 0.964675	val: 0.678022	test: 0.744045
PRC train: 0.944641	val: 0.927584	test: 0.735176

Epoch: 53
Loss: 0.2906912928440867
ROC train: 0.964095	val: 0.687179	test: 0.749609
PRC train: 0.945199	val: 0.928371	test: 0.744380

Epoch: 54
Loss: 0.2847737339070293
ROC train: 0.965910	val: 0.700733	test: 0.745262
PRC train: 0.948902	val: 0.933180	test: 0.740515

Epoch: 55
Loss: 0.3000170727925437
ROC train: 0.970722	val: 0.691209	test: 0.741436
PRC train: 0.954981	val: 0.930312	test: 0.737059

Epoch: 56
Loss: 0.2895675838576732
ROC train: 0.969304	val: 0.702198	test: 0.724048
PRC train: 0.951136	val: 0.932678	test: 0.725813

Epoch: 57
Loss: 0.2673034118244769
ROC train: 0.965985	val: 0.692308	test: 0.714311
PRC train: 0.946579	val: 0.932156	test: 0.709704

Epoch: 58
Loss: 0.2820948578363117
ROC train: 0.966558	val: 0.695238	test: 0.717093
PRC train: 0.946912	val: 0.931744	test: 0.713307

Epoch: 59
Loss: 0.28088427950484707
ROC train: 0.968142	val: 0.694139	test: 0.710833
PRC train: 0.950099	val: 0.932461	test: 0.714713

Epoch: 60
Loss: 0.29587556747545246
ROC train: 0.963944	val: 0.674359	test: 0.715006
PRC train: 0.945334	val: 0.932516	test: 0.710457

Epoch: 61
Loss: 0.2736063512710153
ROC train: 0.970360	val: 0.663370	test: 0.733264
PRC train: 0.954796	val: 0.923483	test: 0.710036

Epoch: 62
Loss: 0.27660136488925385
ROC train: 0.968348	val: 0.679121	test: 0.723526
PRC train: 0.952066	val: 0.925554	test: 0.705303

Epoch: 63
Loss: 0.2696243774611
ROC train: 0.969418	val: 0.681319	test: 0.719179
PRC train: 0.954009	val: 0.925024	test: 0.715184

Epoch: 64
Loss: 0.2813973243034772
ROC train: 0.972891	val: 0.678022	test: 0.738654
PRC train: 0.958517	val: 0.928413	test: 0.736431

Epoch: 65
Loss: 0.26030309714516614
ROC train: 0.973570	val: 0.682418	test: 0.752565
PRC train: 0.959199	val: 0.931642	test: 0.745186

Epoch: 66
Loss: 0.2671636174994077
ROC train: 0.972489	val: 0.693773	test: 0.740219
PRC train: 0.957664	val: 0.933551	test: 0.746322

Epoch: 67
Loss: 0.25436437646973353
ROC train: 0.972035	val: 0.687546	test: 0.729438
PRC train: 0.956364	val: 0.930865	test: 0.734196

Epoch: 68
Loss: 0.2648059497939047
ROC train: 0.973767	val: 0.664103	test: 0.735872
PRC train: 0.959170	val: 0.922713	test: 0.742008

Epoch: 69
Loss: 0.25059071898654717
ROC train: 0.973162	val: 0.662271	test: 0.727526
PRC train: 0.958658	val: 0.917400	test: 0.732528

Epoch: 70
Loss: 0.2679391204688026
ROC train: 0.972018	val: 0.665934	test: 0.725613
PRC train: 0.956905	val: 0.919058	test: 0.712928

Epoch: 71
Loss: 0.26676197485920794
ROC train: 0.971441	val: 0.666300	test: 0.723874
PRC train: 0.956212	val: 0.920278	test: 0.708354

Epoch: 72
Loss: 0.2503201210310181
ROC train: 0.973336	val: 0.663370	test: 0.734133
PRC train: 0.959455	val: 0.919752	test: 0.722916

Epoch: 73
Loss: 0.25348299457817447
ROC train: 0.975031	val: 0.659707	test: 0.731003
PRC train: 0.961725	val: 0.923089	test: 0.717398

Epoch: 74
Loss: 0.2633092877325175
ROC train: 0.973233	val: 0.655311	test: 0.722135
PRC train: 0.958473	val: 0.922638	test: 0.700355

Epoch: 75
Loss: 0.2475591650782421
ROC train: 0.975559	val: 0.685714	test: 0.738480
PRC train: 0.962382	val: 0.928518	test: 0.735965

Epoch: 76
Loss: 0.24040622866572323
ROC train: 0.977369	val: 0.698168	test: 0.745609
PRC train: 0.964922	val: 0.935589	test: 0.753423

Epoch: 77
Loss: 0.2442862078548062
ROC train: 0.976612	val: 0.712821	test: 0.739697
PRC train: 0.964750	val: 0.939924	test: 0.742492

Epoch: 78
Loss: 0.251576196505593
ROC train: 0.975134	val: 0.703663	test: 0.734307
PRC train: 0.963840	val: 0.937731	test: 0.738425

Epoch: 79
Loss: 0.24618200146743657
ROC train: 0.974429	val: 0.683150	test: 0.743697
PRC train: 0.960391	val: 0.925512	test: 0.764448

Epoch: 80
Loss: 0.23722341343878267
ROC train: 0.976150	val: 0.668132	test: 0.742306
PRC train: 0.963175	val: 0.925569	test: 0.753049

Epoch: 81
Loss: 0.2407891231869616
ROC train: 0.975097	val: 0.681319	test: 0.718136
PRC train: 0.961936	val: 0.932476	test: 0.729449

Epoch: 82
Loss: 0.24452047709459285
ROC train: 0.977414	val: 0.678755	test: 0.708746
PRC train: 0.965671	val: 0.931319	test: 0.720248

Epoch: 83
Loss: 0.2511343531558962
ROC train: 0.978447	val: 0.679121	test: 0.727873
PRC train: 0.966550	val: 0.930351	test: 0.735381

Epoch: 84
Loss: 0.23903488741008472
ROC train: 0.976892	val: 0.672527	test: 0.736046
PRC train: 0.963076	val: 0.925480	test: 0.740924

Epoch: 85
Loss: 0.25060748934049143
ROC train: 0.978122	val: 0.675092	test: 0.745957
PRC train: 0.965434	val: 0.924760	test: 0.741977

Epoch: 86
Loss: 0.23009326482848236
ROC train: 0.976661	val: 0.694139	test: 0.736915
PRC train: 0.964223	val: 0.933045	test: 0.735650

Epoch: 87
Loss: 0.23565581217807757
ROC train: 0.977714	val: 0.693773	test: 0.737089
PRC train: 0.966099	val: 0.936148	test: 0.739789

Epoch: 88
Loss: 0.2322457913684628
ROC train: 0.980594	val: 0.687179	test: 0.739002
PRC train: 0.970563	val: 0.934510	test: 0.747395

Epoch: 89
Loss: 0.24329496206207152
ROC train: 0.981244	val: 0.692308	test: 0.732568
PRC train: 0.971425	val: 0.933015	test: 0.744288

Epoch: 90
Loss: 0.2407844793056643
ROC train: 0.981333	val: 0.695971	test: 0.731525
PRC train: 0.971762	val: 0.930225	test: 0.746501

Epoch: 91
Loss: 0.24920428689801555
ROC train: 0.982306	val: 0.696337	test: 0.748044
PRC train: 0.972338	val: 0.928038	test: 0.758105

Epoch: 92
Loss: 0.238157513039034
ROC train: 0.982240	val: 0.697802	test: 0.731525
PRC train: 0.973039	val: 0.931671	test: 0.720484

Epoch: 93
Loss: 0.23134309588458546
ROC train: 0.981809	val: 0.685714	test: 0.733264
PRC train: 0.972858	val: 0.931218	test: 0.727332

Epoch: 94
Loss: 0.25324275657610795
ROC train: 0.982754	val: 0.667033	test: 0.735350
PRC train: 0.981728	val: 0.936861	test: 0.753189

Epoch: 33
Loss: 0.2904892028927983
ROC train: 0.990160	val: 0.684615	test: 0.748565
PRC train: 0.984895	val: 0.928226	test: 0.750928

Epoch: 34
Loss: 0.2709643997049456
ROC train: 0.986307	val: 0.687546	test: 0.711355
PRC train: 0.978455	val: 0.930156	test: 0.727354

Epoch: 35
Loss: 0.25282812956169415
ROC train: 0.981493	val: 0.672894	test: 0.737611
PRC train: 0.968045	val: 0.927743	test: 0.748278

Epoch: 36
Loss: 0.2521223841529937
ROC train: 0.989763	val: 0.666300	test: 0.722135
PRC train: 0.984533	val: 0.921499	test: 0.727718

Epoch: 37
Loss: 0.24735842178913248
ROC train: 0.991864	val: 0.686081	test: 0.741436
PRC train: 0.987611	val: 0.930302	test: 0.743762

Epoch: 38
Loss: 0.2522511584704119
ROC train: 0.992423	val: 0.695971	test: 0.742306
PRC train: 0.988366	val: 0.934939	test: 0.750648

Epoch: 39
Loss: 0.23433860295577427
ROC train: 0.994934	val: 0.665934	test: 0.753260
PRC train: 0.992697	val: 0.922568	test: 0.756395

Epoch: 40
Loss: 0.23004388081124882
ROC train: 0.993704	val: 0.646886	test: 0.758998
PRC train: 0.990797	val: 0.921083	test: 0.756122

Epoch: 41
Loss: 0.22531182984388748
ROC train: 0.993002	val: 0.648352	test: 0.741610
PRC train: 0.989722	val: 0.920999	test: 0.731814

Epoch: 42
Loss: 0.20769535376988948
ROC train: 0.996179	val: 0.662271	test: 0.756564
PRC train: 0.994125	val: 0.922675	test: 0.754062

Epoch: 43
Loss: 0.22970669857009413
ROC train: 0.993482	val: 0.653846	test: 0.767866
PRC train: 0.990118	val: 0.919321	test: 0.773305

Epoch: 44
Loss: 0.2028731539812123
ROC train: 0.996801	val: 0.662637	test: 0.721961
PRC train: 0.995251	val: 0.918326	test: 0.731284

Epoch: 45
Loss: 0.20676023359353995
ROC train: 0.996304	val: 0.697802	test: 0.727178
PRC train: 0.994595	val: 0.930579	test: 0.739501

Epoch: 46
Loss: 0.16961841919669823
ROC train: 0.996216	val: 0.704762	test: 0.740567
PRC train: 0.994356	val: 0.930903	test: 0.746494

Epoch: 47
Loss: 0.18015416751749852
ROC train: 0.996501	val: 0.690476	test: 0.733264
PRC train: 0.994811	val: 0.926839	test: 0.735505

Epoch: 48
Loss: 0.19078586158891347
ROC train: 0.997343	val: 0.675824	test: 0.734481
PRC train: 0.995932	val: 0.921736	test: 0.724976

Epoch: 49
Loss: 0.18282147154927025
ROC train: 0.998790	val: 0.653114	test: 0.744740
PRC train: 0.998234	val: 0.913662	test: 0.732120

Epoch: 50
Loss: 0.1600519227630251
ROC train: 0.997854	val: 0.658974	test: 0.763519
PRC train: 0.996903	val: 0.914916	test: 0.749173

Epoch: 51
Loss: 0.17099889177142413
ROC train: 0.997483	val: 0.663736	test: 0.757607
PRC train: 0.995981	val: 0.919515	test: 0.740200

Epoch: 52
Loss: 0.16571291956942152
ROC train: 0.998656	val: 0.646886	test: 0.737437
PRC train: 0.998006	val: 0.920002	test: 0.711341

Epoch: 53
Loss: 0.17436170572066736
ROC train: 0.998602	val: 0.669597	test: 0.710659
PRC train: 0.997921	val: 0.922692	test: 0.686062

Epoch: 54
Loss: 0.16380282262404605
ROC train: 0.998579	val: 0.685348	test: 0.739176
PRC train: 0.998007	val: 0.928427	test: 0.726760

Epoch: 55
Loss: 0.16280869242431412
ROC train: 0.999386	val: 0.680952	test: 0.721440
PRC train: 0.999098	val: 0.926845	test: 0.711034

Epoch: 56
Loss: 0.1490286325971759
ROC train: 0.999526	val: 0.683516	test: 0.736568
PRC train: 0.999305	val: 0.925413	test: 0.710688

Epoch: 57
Loss: 0.14244828660370903
ROC train: 0.999700	val: 0.702564	test: 0.741784
PRC train: 0.999548	val: 0.933486	test: 0.714835

Epoch: 58
Loss: 0.12338767610317473
ROC train: 0.999415	val: 0.697436	test: 0.727526
PRC train: 0.999106	val: 0.933538	test: 0.701686

Epoch: 59
Loss: 0.14942511682765983
ROC train: 0.999455	val: 0.676557	test: 0.729612
PRC train: 0.999195	val: 0.926558	test: 0.715697

Epoch: 60
Loss: 0.15887842374522335
ROC train: 0.999586	val: 0.706227	test: 0.723179
PRC train: 0.999379	val: 0.933983	test: 0.704286

Epoch: 61
Loss: 0.14510105443109156
ROC train: 0.999284	val: 0.704029	test: 0.732220
PRC train: 0.998883	val: 0.935760	test: 0.708618

Epoch: 62
Loss: 0.14081343489023745
ROC train: 0.999255	val: 0.673626	test: 0.709094
PRC train: 0.998845	val: 0.927190	test: 0.693584

Epoch: 63
Loss: 0.11983305342635113
ROC train: 0.999004	val: 0.636630	test: 0.714311
PRC train: 0.998515	val: 0.917886	test: 0.692309

Epoch: 64
Loss: 0.1367376653162376
ROC train: 0.999161	val: 0.642491	test: 0.748913
PRC train: 0.998732	val: 0.918321	test: 0.729093

Epoch: 65
Loss: 0.12782497128288933
ROC train: 0.999301	val: 0.663004	test: 0.751348
PRC train: 0.998931	val: 0.921753	test: 0.741782

Epoch: 66
Loss: 0.12062298974225294
ROC train: 0.999900	val: 0.689011	test: 0.723874
PRC train: 0.999851	val: 0.926952	test: 0.703086

Epoch: 67
Loss: 0.12438877646418418
ROC train: 0.999909	val: 0.708791	test: 0.722309
PRC train: 0.999862	val: 0.934223	test: 0.701967

Epoch: 68
Loss: 0.12284303639276761
ROC train: 0.999949	val: 0.685714	test: 0.718484
PRC train: 0.999922	val: 0.926367	test: 0.698552

Epoch: 69
Loss: 0.12171229380510482
ROC train: 0.999683	val: 0.685714	test: 0.752391
PRC train: 0.999532	val: 0.929948	test: 0.742604

Epoch: 70
Loss: 0.09251218570461979
ROC train: 0.999675	val: 0.673993	test: 0.748392
PRC train: 0.999500	val: 0.929445	test: 0.728734

Epoch: 71
Loss: 0.10309337422336178
ROC train: 0.999934	val: 0.652381	test: 0.731351
PRC train: 0.999900	val: 0.923141	test: 0.710856

Epoch: 72
Loss: 0.1144609497808867
ROC train: 0.999989	val: 0.652015	test: 0.720223
PRC train: 0.999983	val: 0.920884	test: 0.705077

Epoch: 73
Loss: 0.11281107489300275
ROC train: 0.999951	val: 0.642857	test: 0.705443
PRC train: 0.999927	val: 0.914592	test: 0.688388

Epoch: 74
Loss: 0.12049902073579699
ROC train: 0.999986	val: 0.673626	test: 0.743523
PRC train: 0.999978	val: 0.921952	test: 0.724895

Epoch: 75
Loss: 0.10221357525797967
ROC train: 0.999863	val: 0.704762	test: 0.764389
PRC train: 0.999794	val: 0.929116	test: 0.745773

Epoch: 76
Loss: 0.10559452424495727
ROC train: 0.999997	val: 0.673626	test: 0.772561
PRC train: 0.999996	val: 0.923010	test: 0.752197

Epoch: 77
Loss: 0.11531251448591143
ROC train: 0.999957	val: 0.652015	test: 0.726656
PRC train: 0.999934	val: 0.912900	test: 0.702777

Epoch: 78
Loss: 0.09930287357626426
ROC train: 0.999991	val: 0.678022	test: 0.756042
PRC train: 0.999987	val: 0.927689	test: 0.729189

Epoch: 79
Loss: 0.09264978952269913
ROC train: 0.999994	val: 0.669963	test: 0.765258
PRC train: 0.999991	val: 0.926016	test: 0.743150

Epoch: 80
Loss: 0.09121473604078603
ROC train: 0.999974	val: 0.647253	test: 0.757607
PRC train: 0.999962	val: 0.920275	test: 0.733979

Epoch: 81
Loss: 0.07900701458143486
ROC train: 0.999963	val: 0.676923	test: 0.759868
PRC train: 0.999944	val: 0.927001	test: 0.733562

Epoch: 82
Loss: 0.08673240322939363
ROC train: 1.000000	val: 0.702930	test: 0.753956
PRC train: 1.000000	val: 0.931917	test: 0.727112

Epoch: 83
Loss: 0.08701908175845856
ROC train: 0.999994	val: 0.689377	test: 0.748565
PRC train: 0.999991	val: 0.929599	test: 0.724543

Epoch: 84
Loss: 0.07251595781077791
ROC train: 0.999971	val: 0.676923	test: 0.750826
PRC train: 0.999957	val: 0.926811	test: 0.724983

Epoch: 85
Loss: 0.08141508435699092
ROC train: 0.999997	val: 0.672894	test: 0.726656
PRC train: 0.999996	val: 0.923721	test: 0.708561

Epoch: 86
Loss: 0.07831501422826279
ROC train: 1.000000	val: 0.689011	test: 0.724917
PRC train: 1.000000	val: 0.929375	test: 0.709216

Epoch: 87
Loss: 0.06478973119567089
ROC train: 0.999969	val: 0.678388	test: 0.707529
PRC train: 0.999952	val: 0.929889	test: 0.703547

Epoch: 88
Loss: 0.0789378046087209
ROC train: 0.999989	val: 0.674359	test: 0.708051
PRC train: 0.999983	val: 0.922825	test: 0.696401

Epoch: 89
Loss: 0.08549635980159213
ROC train: 1.000000	val: 0.691941	test: 0.754999
PRC train: 1.000000	val: 0.923682	test: 0.733521

Epoch: 90
Loss: 0.08754788227727181
ROC train: 1.000000	val: 0.705128	test: 0.750826
PRC train: 1.000000	val: 0.930412	test: 0.723345

Epoch: 91
Loss: 0.062212175169088745
ROC train: 1.000000	val: 0.676190	test: 0.710311
PRC train: 1.000000	val: 0.920113	test: 0.692007

Epoch: 92
Loss: 0.08116861028223679
ROC train: 0.999997	val: 0.690476	test: 0.738480
PRC train: 0.999996	val: 0.926203	test: 0.723081

Epoch: 93
Loss: 0.09692631476948134
ROC train: 0.999986	val: 0.672161	test: 0.747174
PRC train: 0.990945	val: 0.935369	test: 0.791633

Epoch: 33
Loss: 0.24579255380951442
ROC train: 0.996527	val: 0.682418	test: 0.761259
PRC train: 0.994433	val: 0.934051	test: 0.783729

Epoch: 34
Loss: 0.23589832914295622
ROC train: 0.996872	val: 0.637363	test: 0.781777
PRC train: 0.995192	val: 0.918020	test: 0.798139

Epoch: 35
Loss: 0.25250606139652304
ROC train: 0.998114	val: 0.640293	test: 0.772909
PRC train: 0.997045	val: 0.916115	test: 0.780773

Epoch: 36
Loss: 0.22705482401723134
ROC train: 0.998271	val: 0.672161	test: 0.762302
PRC train: 0.997316	val: 0.927080	test: 0.782695

Epoch: 37
Loss: 0.2200919495009722
ROC train: 0.998465	val: 0.671429	test: 0.762824
PRC train: 0.997626	val: 0.924953	test: 0.788049

Epoch: 38
Loss: 0.21405382862182237
ROC train: 0.998841	val: 0.664469	test: 0.781603
PRC train: 0.998201	val: 0.922908	test: 0.805332

Epoch: 39
Loss: 0.18602315988423432
ROC train: 0.998533	val: 0.682418	test: 0.754651
PRC train: 0.997850	val: 0.933372	test: 0.790934

Epoch: 40
Loss: 0.20422682467206035
ROC train: 0.998430	val: 0.667399	test: 0.788211
PRC train: 0.997636	val: 0.929636	test: 0.814193

Epoch: 41
Loss: 0.19228808225323069
ROC train: 0.999586	val: 0.648352	test: 0.792384
PRC train: 0.999387	val: 0.924956	test: 0.821988

Epoch: 42
Loss: 0.1732747184953006
ROC train: 0.999138	val: 0.653846	test: 0.763172
PRC train: 0.998751	val: 0.912851	test: 0.794836

Epoch: 43
Loss: 0.1846445085482035
ROC train: 0.999238	val: 0.680220	test: 0.760563
PRC train: 0.998832	val: 0.927989	test: 0.773672

Epoch: 44
Loss: 0.1754069394462099
ROC train: 0.999635	val: 0.687546	test: 0.773952
PRC train: 0.999436	val: 0.934178	test: 0.794649

Epoch: 45
Loss: 0.1764656203425754
ROC train: 0.999521	val: 0.690476	test: 0.769431
PRC train: 0.999272	val: 0.937185	test: 0.788898

Epoch: 46
Loss: 0.15923668029839239
ROC train: 0.999706	val: 0.673993	test: 0.778299
PRC train: 0.999552	val: 0.930715	test: 0.807732

Epoch: 47
Loss: 0.1685575114056836
ROC train: 0.999786	val: 0.646154	test: 0.769431
PRC train: 0.999670	val: 0.920406	test: 0.799983

Epoch: 48
Loss: 0.15429210011045705
ROC train: 0.999857	val: 0.634799	test: 0.751869
PRC train: 0.999782	val: 0.902217	test: 0.786502

Epoch: 49
Loss: 0.14843980081885694
ROC train: 0.999954	val: 0.650183	test: 0.757260
PRC train: 0.999931	val: 0.912379	test: 0.794200

Epoch: 50
Loss: 0.17745235877638083
ROC train: 0.999275	val: 0.677656	test: 0.748739
PRC train: 0.998760	val: 0.927553	test: 0.777297

Epoch: 51
Loss: 0.16249557457553437
ROC train: 0.999937	val: 0.664835	test: 0.793253
PRC train: 0.999905	val: 0.922767	test: 0.807495

Epoch: 52
Loss: 0.12783138386883292
ROC train: 0.999994	val: 0.642491	test: 0.786298
PRC train: 0.999991	val: 0.910646	test: 0.785753

Epoch: 53
Loss: 0.14803277248498398
ROC train: 0.999946	val: 0.663004	test: 0.749957
PRC train: 0.999920	val: 0.921350	test: 0.776364

Epoch: 54
Loss: 0.14083094866839058
ROC train: 0.999991	val: 0.656044	test: 0.784559
PRC train: 0.999987	val: 0.917033	test: 0.813263

Epoch: 55
Loss: 0.15285168451218062
ROC train: 0.999971	val: 0.676190	test: 0.785255
PRC train: 0.999957	val: 0.922537	test: 0.809364

Epoch: 56
Loss: 0.1271302362118519
ROC train: 0.999980	val: 0.683883	test: 0.770996
PRC train: 0.999970	val: 0.924596	test: 0.798267

Epoch: 57
Loss: 0.1344078206845431
ROC train: 0.999989	val: 0.686081	test: 0.785603
PRC train: 0.999983	val: 0.933684	test: 0.816478

Epoch: 58
Loss: 0.12674351341123152
ROC train: 0.999940	val: 0.661172	test: 0.797427
PRC train: 0.999908	val: 0.925756	test: 0.820789

Epoch: 59
Loss: 0.12500295185991334
ROC train: 0.999980	val: 0.654579	test: 0.758303
PRC train: 0.999970	val: 0.918734	test: 0.789215

Epoch: 60
Loss: 0.09552948800100736
ROC train: 0.999969	val: 0.650549	test: 0.751521
PRC train: 0.999953	val: 0.917856	test: 0.784332

Epoch: 61
Loss: 0.09424560616772316
ROC train: 0.999934	val: 0.678022	test: 0.777778
PRC train: 0.999903	val: 0.930970	test: 0.804733

Epoch: 62
Loss: 0.11404834496392595
ROC train: 0.999977	val: 0.659707	test: 0.762128
PRC train: 0.999966	val: 0.921767	test: 0.797758

Epoch: 63
Loss: 0.09861642776972852
ROC train: 0.999997	val: 0.656044	test: 0.747522
PRC train: 0.999996	val: 0.913669	test: 0.784239

Epoch: 64
Loss: 0.09847931158602227
ROC train: 1.000000	val: 0.671429	test: 0.774300
PRC train: 1.000000	val: 0.921963	test: 0.803189

Epoch: 65
Loss: 0.10269530648904232
ROC train: 0.999997	val: 0.679121	test: 0.772214
PRC train: 0.999996	val: 0.927784	test: 0.799031

Epoch: 66
Loss: 0.09620045217626108
ROC train: 0.999997	val: 0.660806	test: 0.735698
PRC train: 0.999996	val: 0.920077	test: 0.763315

Epoch: 67
Loss: 0.09691641790577164
ROC train: 1.000000	val: 0.653114	test: 0.751000
PRC train: 1.000000	val: 0.919723	test: 0.778049

Epoch: 68
Loss: 0.09182607166998279
ROC train: 0.999989	val: 0.654579	test: 0.745609
PRC train: 0.999983	val: 0.919202	test: 0.766636

Epoch: 69
Loss: 0.11361520115006474
ROC train: 0.999966	val: 0.654945	test: 0.742306
PRC train: 0.999949	val: 0.912350	test: 0.755998

Epoch: 70
Loss: 0.11289546058081419
ROC train: 1.000000	val: 0.666300	test: 0.754651
PRC train: 1.000000	val: 0.919353	test: 0.774793

Epoch: 71
Loss: 0.09674510099886587
ROC train: 1.000000	val: 0.669231	test: 0.770475
PRC train: 1.000000	val: 0.924166	test: 0.792676

Epoch: 72
Loss: 0.10319491005041292
ROC train: 1.000000	val: 0.667033	test: 0.756216
PRC train: 1.000000	val: 0.922281	test: 0.778669

Epoch: 73
Loss: 0.07597747387525153
ROC train: 0.999986	val: 0.670696	test: 0.756042
PRC train: 0.999978	val: 0.922346	test: 0.786151

Epoch: 74
Loss: 0.11294472410637399
ROC train: 1.000000	val: 0.678388	test: 0.769953
PRC train: 1.000000	val: 0.923929	test: 0.793982

Epoch: 75
Loss: 0.10165839016995588
ROC train: 1.000000	val: 0.673260	test: 0.768736
PRC train: 1.000000	val: 0.921636	test: 0.788883

Epoch: 76
Loss: 0.08303010114385408
ROC train: 0.999997	val: 0.667399	test: 0.778299
PRC train: 0.999996	val: 0.924796	test: 0.803483

Epoch: 77
Loss: 0.09591121050269269
ROC train: 1.000000	val: 0.653114	test: 0.774822
PRC train: 1.000000	val: 0.921271	test: 0.801491

Epoch: 78
Loss: 0.08223513947732525
ROC train: 1.000000	val: 0.652381	test: 0.758477
PRC train: 1.000000	val: 0.922617	test: 0.789398

Epoch: 79
Loss: 0.0691592160251451
ROC train: 1.000000	val: 0.673993	test: 0.773778
PRC train: 1.000000	val: 0.932039	test: 0.800324

Epoch: 80
Loss: 0.06509229789848156
ROC train: 1.000000	val: 0.693040	test: 0.775517
PRC train: 1.000000	val: 0.936796	test: 0.795958

Epoch: 81
Loss: 0.07493661826466595
ROC train: 1.000000	val: 0.682051	test: 0.773952
PRC train: 1.000000	val: 0.932720	test: 0.799236

Epoch: 82
Loss: 0.08942857995172748
ROC train: 1.000000	val: 0.665568	test: 0.753956
PRC train: 1.000000	val: 0.925994	test: 0.782343

Epoch: 83
Loss: 0.0710864951060319
ROC train: 0.999991	val: 0.691941	test: 0.754304
PRC train: 0.999987	val: 0.935925	test: 0.786122

Epoch: 84
Loss: 0.07370285854277675
ROC train: 1.000000	val: 0.699634	test: 0.755347
PRC train: 1.000000	val: 0.937876	test: 0.791135

Epoch: 85
Loss: 0.06063307270117627
ROC train: 1.000000	val: 0.687912	test: 0.772735
PRC train: 1.000000	val: 0.933159	test: 0.808452

Epoch: 86
Loss: 0.06378671872262905
ROC train: 0.999983	val: 0.689011	test: 0.800556
PRC train: 0.999974	val: 0.933615	test: 0.820317

Epoch: 87
Loss: 0.07490861782406444
ROC train: 1.000000	val: 0.678022	test: 0.781082
PRC train: 1.000000	val: 0.929830	test: 0.809015

Epoch: 88
Loss: 0.05757087007741233
ROC train: 0.999989	val: 0.654945	test: 0.754999
PRC train: 0.999983	val: 0.922669	test: 0.780857

Epoch: 89
Loss: 0.07059163666971675
ROC train: 1.000000	val: 0.675092	test: 0.769605
PRC train: 1.000000	val: 0.928934	test: 0.787267

Epoch: 90
Loss: 0.058200160330180384
ROC train: 1.000000	val: 0.689011	test: 0.772909
PRC train: 1.000000	val: 0.934406	test: 0.796563

Epoch: 91
Loss: 0.0790925296490729
ROC train: 0.999994	val: 0.676923	test: 0.731873
PRC train: 0.999991	val: 0.929214	test: 0.764302

Epoch: 92
Loss: 0.0571099675485117
ROC train: 0.999997	val: 0.673626	test: 0.716571
PRC train: 0.999996	val: 0.926443	test: 0.757777

Epoch: 93
Loss: 0.048948273794046745
ROC train: 1.000000	val: 0.679121	test: 0.750826
PRC train: 0.979591	val: 0.935655	test: 0.808695

Epoch: 33
Loss: 0.269054254291056
ROC train: 0.986256	val: 0.666667	test: 0.811859
PRC train: 0.979532	val: 0.935726	test: 0.801743

Epoch: 34
Loss: 0.26441442163550744
ROC train: 0.991775	val: 0.676923	test: 0.814815
PRC train: 0.987778	val: 0.935335	test: 0.808835

Epoch: 35
Loss: 0.28414467451802217
ROC train: 0.992366	val: 0.671062	test: 0.791167
PRC train: 0.988894	val: 0.933445	test: 0.784235

Epoch: 36
Loss: 0.2573746552694585
ROC train: 0.993379	val: 0.675092	test: 0.797253
PRC train: 0.990322	val: 0.934761	test: 0.796609

Epoch: 37
Loss: 0.24835443156772374
ROC train: 0.987349	val: 0.687912	test: 0.789602
PRC train: 0.981576	val: 0.938253	test: 0.797150

Epoch: 38
Loss: 0.24755239441159568
ROC train: 0.986330	val: 0.669231	test: 0.801078
PRC train: 0.979736	val: 0.934804	test: 0.799833

Epoch: 39
Loss: 0.23502677787677956
ROC train: 0.992800	val: 0.689011	test: 0.789602
PRC train: 0.989424	val: 0.939783	test: 0.787569

Epoch: 40
Loss: 0.2254311579687643
ROC train: 0.993048	val: 0.701465	test: 0.789080
PRC train: 0.990035	val: 0.941313	test: 0.787248

Epoch: 41
Loss: 0.22080803845268884
ROC train: 0.993211	val: 0.695971	test: 0.774300
PRC train: 0.990425	val: 0.940918	test: 0.772058

Epoch: 42
Loss: 0.21367315095850725
ROC train: 0.995114	val: 0.673993	test: 0.796557
PRC train: 0.992894	val: 0.935510	test: 0.792823

Epoch: 43
Loss: 0.2143673472295659
ROC train: 0.994740	val: 0.676190	test: 0.824378
PRC train: 0.991840	val: 0.934937	test: 0.815725

Epoch: 44
Loss: 0.21153297248752095
ROC train: 0.996136	val: 0.693407	test: 0.817945
PRC train: 0.994377	val: 0.938970	test: 0.819934

Epoch: 45
Loss: 0.19831319211243176
ROC train: 0.994949	val: 0.716850	test: 0.805425
PRC train: 0.993010	val: 0.941102	test: 0.816578

Epoch: 46
Loss: 0.19981261763822208
ROC train: 0.997252	val: 0.695971	test: 0.804382
PRC train: 0.996237	val: 0.936417	test: 0.817849

Epoch: 47
Loss: 0.1944084073570767
ROC train: 0.996869	val: 0.651282	test: 0.810294
PRC train: 0.995229	val: 0.924583	test: 0.813059

Epoch: 48
Loss: 0.1851275039017683
ROC train: 0.995913	val: 0.671795	test: 0.780908
PRC train: 0.993740	val: 0.931906	test: 0.784520

Epoch: 49
Loss: 0.19051983350431298
ROC train: 0.995699	val: 0.708059	test: 0.770475
PRC train: 0.993723	val: 0.941349	test: 0.771529

Epoch: 50
Loss: 0.18024788563779223
ROC train: 0.997086	val: 0.700733	test: 0.807164
PRC train: 0.995765	val: 0.940394	test: 0.793785

Epoch: 51
Loss: 0.19388469778845124
ROC train: 0.998604	val: 0.678388	test: 0.807686
PRC train: 0.998043	val: 0.935366	test: 0.785638

Epoch: 52
Loss: 0.1802331721837221
ROC train: 0.997925	val: 0.688645	test: 0.801426
PRC train: 0.997044	val: 0.938281	test: 0.784263

Epoch: 53
Loss: 0.18441388627667052
ROC train: 0.998856	val: 0.701832	test: 0.797600
PRC train: 0.998373	val: 0.941494	test: 0.787720

Epoch: 54
Loss: 0.17264963799672084
ROC train: 0.999515	val: 0.693040	test: 0.790297
PRC train: 0.999270	val: 0.939570	test: 0.774768

Epoch: 55
Loss: 0.1669979946264937
ROC train: 0.999412	val: 0.687179	test: 0.790471
PRC train: 0.999111	val: 0.938435	test: 0.774856

Epoch: 56
Loss: 0.17652439306146556
ROC train: 0.999438	val: 0.704762	test: 0.802121
PRC train: 0.999175	val: 0.941580	test: 0.790022

Epoch: 57
Loss: 0.17532220331649997
ROC train: 0.999255	val: 0.698168	test: 0.804034
PRC train: 0.998923	val: 0.939381	test: 0.803255

Epoch: 58
Loss: 0.15770137221907238
ROC train: 0.999589	val: 0.669963	test: 0.793427
PRC train: 0.999405	val: 0.932412	test: 0.795636

Epoch: 59
Loss: 0.14575770609819003
ROC train: 0.999535	val: 0.691575	test: 0.800556
PRC train: 0.999291	val: 0.934976	test: 0.794991

Epoch: 60
Loss: 0.1509359914832949
ROC train: 0.999566	val: 0.693773	test: 0.798644
PRC train: 0.999384	val: 0.937114	test: 0.798549

Epoch: 61
Loss: 0.13988632877554702
ROC train: 0.999643	val: 0.686447	test: 0.791515
PRC train: 0.999468	val: 0.935781	test: 0.790950

Epoch: 62
Loss: 0.1486764110948478
ROC train: 0.999678	val: 0.700000	test: 0.799165
PRC train: 0.999517	val: 0.937732	test: 0.795236

Epoch: 63
Loss: 0.1355132048571746
ROC train: 0.999697	val: 0.694872	test: 0.789950
PRC train: 0.999556	val: 0.935821	test: 0.785891

Epoch: 64
Loss: 0.14814751116875163
ROC train: 0.999849	val: 0.684982	test: 0.797427
PRC train: 0.999773	val: 0.935591	test: 0.794783

Epoch: 65
Loss: 0.15840013088633448
ROC train: 0.998953	val: 0.696337	test: 0.813598
PRC train: 0.998453	val: 0.939137	test: 0.807944

Epoch: 66
Loss: 0.15374185164023263
ROC train: 0.999709	val: 0.681319	test: 0.791341
PRC train: 0.999569	val: 0.935870	test: 0.789315

Epoch: 67
Loss: 0.13219126843197126
ROC train: 0.999894	val: 0.680586	test: 0.790819
PRC train: 0.999844	val: 0.933434	test: 0.785780

Epoch: 68
Loss: 0.11066620479026383
ROC train: 0.999646	val: 0.673260	test: 0.780038
PRC train: 0.999508	val: 0.928388	test: 0.775643

Epoch: 69
Loss: 0.1342873170769831
ROC train: 0.999669	val: 0.696337	test: 0.789776
PRC train: 0.999517	val: 0.934778	test: 0.787630

Epoch: 70
Loss: 0.1036937616726394
ROC train: 0.999874	val: 0.685714	test: 0.780212
PRC train: 0.999812	val: 0.935864	test: 0.783356

Epoch: 71
Loss: 0.10565949044318142
ROC train: 0.999963	val: 0.677656	test: 0.781082
PRC train: 0.999944	val: 0.932068	test: 0.772182

Epoch: 72
Loss: 0.09367543846011603
ROC train: 0.999949	val: 0.672894	test: 0.784907
PRC train: 0.999921	val: 0.927763	test: 0.767609

Epoch: 73
Loss: 0.10850561891098578
ROC train: 0.999912	val: 0.668864	test: 0.780212
PRC train: 0.999865	val: 0.925074	test: 0.773455

Epoch: 74
Loss: 0.12009289027437933
ROC train: 0.999897	val: 0.669597	test: 0.789254
PRC train: 0.999845	val: 0.927927	test: 0.785197

Epoch: 75
Loss: 0.12234269234057829
ROC train: 0.999957	val: 0.671062	test: 0.802643
PRC train: 0.999935	val: 0.931181	test: 0.797961

Epoch: 76
Loss: 0.11318611536688657
ROC train: 0.999886	val: 0.677656	test: 0.785776
PRC train: 0.999829	val: 0.932124	test: 0.780137

Epoch: 77
Loss: 0.08876861479677667
ROC train: 0.999272	val: 0.694505	test: 0.765954
PRC train: 0.998928	val: 0.935291	test: 0.774103

Epoch: 78
Loss: 0.1271568261642935
ROC train: 0.999755	val: 0.683883	test: 0.767866
PRC train: 0.999635	val: 0.935619	test: 0.767936

Epoch: 79
Loss: 0.10980018018437976
ROC train: 0.999989	val: 0.679487	test: 0.776908
PRC train: 0.999983	val: 0.934225	test: 0.771783

Epoch: 80
Loss: 0.08989030819126581
ROC train: 1.000000	val: 0.666300	test: 0.791688
PRC train: 1.000000	val: 0.928410	test: 0.784528

Epoch: 81
Loss: 0.10433554031073904
ROC train: 0.999971	val: 0.660073	test: 0.789602
PRC train: 0.999957	val: 0.925575	test: 0.784828

Epoch: 82
Loss: 0.0961785749991618
ROC train: 0.999977	val: 0.662637	test: 0.800209
PRC train: 0.999965	val: 0.926570	test: 0.794463

Epoch: 83
Loss: 0.07606431768605719
ROC train: 0.999997	val: 0.669963	test: 0.806295
PRC train: 0.999996	val: 0.930295	test: 0.798327

Epoch: 84
Loss: 0.097616664176893
ROC train: 0.999994	val: 0.661538	test: 0.779864
PRC train: 0.999991	val: 0.928395	test: 0.771023

Epoch: 85
Loss: 0.09687480984054476
ROC train: 0.999980	val: 0.669231	test: 0.786646
PRC train: 0.999970	val: 0.930115	test: 0.784967

Epoch: 86
Loss: 0.10672132853679224
ROC train: 0.999994	val: 0.659707	test: 0.805947
PRC train: 0.999991	val: 0.926861	test: 0.799019

Epoch: 87
Loss: 0.09627504705908821
ROC train: 1.000000	val: 0.648718	test: 0.796557
PRC train: 1.000000	val: 0.926824	test: 0.789057

Epoch: 88
Loss: 0.09013731251982851
ROC train: 1.000000	val: 0.657143	test: 0.775691
PRC train: 1.000000	val: 0.928061	test: 0.775277

Epoch: 89
Loss: 0.08652003019355883
ROC train: 1.000000	val: 0.661538	test: 0.755347
PRC train: 1.000000	val: 0.929644	test: 0.759031

Epoch: 90
Loss: 0.10401955796377027
ROC train: 0.999994	val: 0.661172	test: 0.749609
PRC train: 0.999991	val: 0.929633	test: 0.756079

Epoch: 91
Loss: 0.07463624812274867
ROC train: 1.000000	val: 0.656044	test: 0.772735
PRC train: 1.000000	val: 0.925831	test: 0.772046

Epoch: 92
Loss: 0.09625869872598777
ROC train: 1.000000	val: 0.679487	test: 0.779169
PRC train: 1.000000	val: 0.932123	test: 0.784867

Epoch: 93
Loss: 0.10420646475857127
ROC train: 0.999900	val: 0.692308	test: 0.765954
PRC train: 0.974847	val: 0.939155	test: 0.804723

Epoch: 33
Loss: 0.26144711131980125
ROC train: 0.984361	val: 0.685714	test: 0.797600
PRC train: 0.973914	val: 0.936927	test: 0.792785

Epoch: 34
Loss: 0.263603586293844
ROC train: 0.985882	val: 0.696337	test: 0.746305
PRC train: 0.976075	val: 0.940094	test: 0.750833

Epoch: 35
Loss: 0.2296788935912349
ROC train: 0.987309	val: 0.700366	test: 0.759346
PRC train: 0.980463	val: 0.938884	test: 0.757259

Epoch: 36
Loss: 0.2435991411896171
ROC train: 0.989755	val: 0.696337	test: 0.805251
PRC train: 0.984401	val: 0.939438	test: 0.799996

Epoch: 37
Loss: 0.24620772587325188
ROC train: 0.990491	val: 0.697802	test: 0.820901
PRC train: 0.985535	val: 0.940031	test: 0.810084

Epoch: 38
Loss: 0.23813095460990202
ROC train: 0.991575	val: 0.695238	test: 0.831160
PRC train: 0.986216	val: 0.939615	test: 0.821475

Epoch: 39
Loss: 0.21022068369096955
ROC train: 0.993719	val: 0.686447	test: 0.823161
PRC train: 0.989674	val: 0.939445	test: 0.809782

Epoch: 40
Loss: 0.20525952524271793
ROC train: 0.993490	val: 0.693407	test: 0.783516
PRC train: 0.989540	val: 0.940319	test: 0.782975

Epoch: 41
Loss: 0.21341872252998847
ROC train: 0.994038	val: 0.697070	test: 0.788732
PRC train: 0.990951	val: 0.938195	test: 0.788051

Epoch: 42
Loss: 0.18448792335149813
ROC train: 0.995614	val: 0.673626	test: 0.826813
PRC train: 0.992949	val: 0.932605	test: 0.828847

Epoch: 43
Loss: 0.1871784648225415
ROC train: 0.995211	val: 0.699267	test: 0.822640
PRC train: 0.992613	val: 0.940274	test: 0.827292

Epoch: 44
Loss: 0.19354192364776568
ROC train: 0.995791	val: 0.684615	test: 0.830117
PRC train: 0.993602	val: 0.935374	test: 0.835056

Epoch: 45
Loss: 0.1740356101391878
ROC train: 0.994703	val: 0.667766	test: 0.815858
PRC train: 0.991758	val: 0.928906	test: 0.813723

Epoch: 46
Loss: 0.20113246533498086
ROC train: 0.995365	val: 0.691209	test: 0.820727
PRC train: 0.993180	val: 0.934846	test: 0.818251

Epoch: 47
Loss: 0.17856636232006387
ROC train: 0.996450	val: 0.699267	test: 0.832725
PRC train: 0.994819	val: 0.938758	test: 0.832454

Epoch: 48
Loss: 0.17381714486830055
ROC train: 0.997503	val: 0.678755	test: 0.831508
PRC train: 0.996255	val: 0.933942	test: 0.824279

Epoch: 49
Loss: 0.16219619199916205
ROC train: 0.997594	val: 0.673993	test: 0.810120
PRC train: 0.996409	val: 0.934013	test: 0.801864

Epoch: 50
Loss: 0.19526848670547797
ROC train: 0.997757	val: 0.680952	test: 0.810816
PRC train: 0.996497	val: 0.935402	test: 0.803081

Epoch: 51
Loss: 0.17254155659272413
ROC train: 0.996869	val: 0.685348	test: 0.815684
PRC train: 0.994979	val: 0.934238	test: 0.804319

Epoch: 52
Loss: 0.16843172442586993
ROC train: 0.997549	val: 0.685714	test: 0.818814
PRC train: 0.996079	val: 0.936580	test: 0.813723

Epoch: 53
Loss: 0.14814041285121082
ROC train: 0.999015	val: 0.675092	test: 0.813945
PRC train: 0.998515	val: 0.935462	test: 0.814873

Epoch: 54
Loss: 0.18281718361441346
ROC train: 0.997757	val: 0.692308	test: 0.790123
PRC train: 0.996674	val: 0.939265	test: 0.786710

Epoch: 55
Loss: 0.15999136488922203
ROC train: 0.997865	val: 0.683883	test: 0.780734
PRC train: 0.996923	val: 0.938036	test: 0.775426

Epoch: 56
Loss: 0.154615212123893
ROC train: 0.998256	val: 0.684982	test: 0.787689
PRC train: 0.997397	val: 0.937426	test: 0.775577

Epoch: 57
Loss: 0.15317077965505926
ROC train: 0.998530	val: 0.693040	test: 0.802643
PRC train: 0.997797	val: 0.937699	test: 0.789137

Epoch: 58
Loss: 0.16536045653165424
ROC train: 0.997845	val: 0.704029	test: 0.804208
PRC train: 0.996727	val: 0.942251	test: 0.789096

Epoch: 59
Loss: 0.14759465786206133
ROC train: 0.998470	val: 0.689011	test: 0.818119
PRC train: 0.997491	val: 0.939836	test: 0.804359

Epoch: 60
Loss: 0.15309634827446852
ROC train: 0.998630	val: 0.670696	test: 0.825596
PRC train: 0.997870	val: 0.933845	test: 0.808813

Epoch: 61
Loss: 0.13940316565143443
ROC train: 0.999198	val: 0.680220	test: 0.807338
PRC train: 0.998780	val: 0.936011	test: 0.794886

Epoch: 62
Loss: 0.1286479124018764
ROC train: 0.999132	val: 0.684982	test: 0.787689
PRC train: 0.998681	val: 0.937778	test: 0.781123

Epoch: 63
Loss: 0.14189284322168277
ROC train: 0.999663	val: 0.674359	test: 0.796557
PRC train: 0.999489	val: 0.935693	test: 0.788645

Epoch: 64
Loss: 0.13810376079563028
ROC train: 0.999863	val: 0.677656	test: 0.795688
PRC train: 0.999792	val: 0.935829	test: 0.788007

Epoch: 65
Loss: 0.11931774399605836
ROC train: 0.999726	val: 0.676557	test: 0.806816
PRC train: 0.999582	val: 0.934312	test: 0.794483

Epoch: 66
Loss: 0.1129831155628512
ROC train: 0.999415	val: 0.678388	test: 0.809251
PRC train: 0.999114	val: 0.933605	test: 0.786566

Epoch: 67
Loss: 0.11307401160919775
ROC train: 0.998824	val: 0.704029	test: 0.792384
PRC train: 0.998299	val: 0.941053	test: 0.774756

Epoch: 68
Loss: 0.12390372137005132
ROC train: 0.999475	val: 0.698168	test: 0.797427
PRC train: 0.999251	val: 0.941413	test: 0.778176

Epoch: 69
Loss: 0.11920890067985242
ROC train: 0.999903	val: 0.682051	test: 0.818292
PRC train: 0.999858	val: 0.935857	test: 0.803366

Epoch: 70
Loss: 0.10127092591524578
ROC train: 0.999960	val: 0.666300	test: 0.817771
PRC train: 0.999941	val: 0.932524	test: 0.808255

Epoch: 71
Loss: 0.10991351040777717
ROC train: 0.999820	val: 0.672894	test: 0.806816
PRC train: 0.999739	val: 0.932671	test: 0.802727

Epoch: 72
Loss: 0.09733400607791029
ROC train: 0.999635	val: 0.690842	test: 0.802121
PRC train: 0.999467	val: 0.937921	test: 0.803232

Epoch: 73
Loss: 0.09887676732136966
ROC train: 0.999749	val: 0.687179	test: 0.825943
PRC train: 0.999633	val: 0.938852	test: 0.830622

Epoch: 74
Loss: 0.10513587515014729
ROC train: 0.999675	val: 0.694505	test: 0.827334
PRC train: 0.999491	val: 0.939781	test: 0.827314

Epoch: 75
Loss: 0.12351452174105053
ROC train: 0.999894	val: 0.678755	test: 0.823683
PRC train: 0.999841	val: 0.936523	test: 0.818519

Epoch: 76
Loss: 0.10576083206274745
ROC train: 0.999812	val: 0.697802	test: 0.807512
PRC train: 0.999722	val: 0.940356	test: 0.790696

Epoch: 77
Loss: 0.10993317605128025
ROC train: 0.999752	val: 0.712088	test: 0.797079
PRC train: 0.999631	val: 0.944178	test: 0.781475

Epoch: 78
Loss: 0.09819101884415869
ROC train: 0.999886	val: 0.700733	test: 0.812380
PRC train: 0.999826	val: 0.940993	test: 0.799605

Epoch: 79
Loss: 0.10384023388647319
ROC train: 0.999920	val: 0.694872	test: 0.830117
PRC train: 0.999878	val: 0.940695	test: 0.831379

Epoch: 80
Loss: 0.10200475895148395
ROC train: 0.999974	val: 0.690476	test: 0.828725
PRC train: 0.999961	val: 0.939598	test: 0.829557

Epoch: 81
Loss: 0.07678339231851025
ROC train: 0.999658	val: 0.690476	test: 0.812554
PRC train: 0.999475	val: 0.939546	test: 0.808716

Epoch: 82
Loss: 0.0994572603383385
ROC train: 0.999737	val: 0.682051	test: 0.818988
PRC train: 0.999588	val: 0.936966	test: 0.808639

Epoch: 83
Loss: 0.09771859139116358
ROC train: 0.999603	val: 0.687179	test: 0.800209
PRC train: 0.999331	val: 0.940421	test: 0.786247

Epoch: 84
Loss: 0.08883539722401393
ROC train: 0.999957	val: 0.673993	test: 0.824726
PRC train: 0.999936	val: 0.937251	test: 0.813369

Epoch: 85
Loss: 0.07933086654377723
ROC train: 0.999980	val: 0.687179	test: 0.826465
PRC train: 0.999970	val: 0.939179	test: 0.818307

Epoch: 86
Loss: 0.076327132387671
ROC train: 0.999951	val: 0.701099	test: 0.815163
PRC train: 0.999926	val: 0.942527	test: 0.811040

Epoch: 87
Loss: 0.08519072222079607
ROC train: 0.999940	val: 0.696337	test: 0.797774
PRC train: 0.999909	val: 0.940019	test: 0.790615

Epoch: 88
Loss: 0.0669374520577678
ROC train: 0.999949	val: 0.677289	test: 0.801774
PRC train: 0.999923	val: 0.935145	test: 0.792653

Epoch: 89
Loss: 0.1016588331457006
ROC train: 0.999980	val: 0.674359	test: 0.809424
PRC train: 0.999970	val: 0.934036	test: 0.798915

Epoch: 90
Loss: 0.09499339663890635
ROC train: 0.999983	val: 0.682784	test: 0.809424
PRC train: 0.999974	val: 0.935498	test: 0.801767

Epoch: 91
Loss: 0.06765221683279135
ROC train: 0.999954	val: 0.677656	test: 0.803339
PRC train: 0.999930	val: 0.934367	test: 0.796904

Epoch: 92
Loss: 0.08908014028126106
ROC train: 0.999971	val: 0.679853	test: 0.802991
PRC train: 0.999956	val: 0.934677	test: 0.798050

Epoch: 93
Loss: 0.0641485074607123
ROC train: 0.999963	val: 0.675824	test: 0.813598PRC train: 0.991391	val: 0.934268	test: 0.738645

Epoch: 33
Loss: 0.2522774216493803
ROC train: 0.996201	val: 0.686081	test: 0.786298
PRC train: 0.994109	val: 0.939423	test: 0.742085

Epoch: 34
Loss: 0.25166640652256
ROC train: 0.997403	val: 0.708425	test: 0.776213
PRC train: 0.996056	val: 0.945899	test: 0.746324

Epoch: 35
Loss: 0.21486183436478107
ROC train: 0.993944	val: 0.704029	test: 0.780908
PRC train: 0.990942	val: 0.942893	test: 0.760166

Epoch: 36
Loss: 0.22037151667699
ROC train: 0.994743	val: 0.706593	test: 0.774300
PRC train: 0.992368	val: 0.943198	test: 0.737587

Epoch: 37
Loss: 0.21906936592754317
ROC train: 0.998462	val: 0.699634	test: 0.776734
PRC train: 0.997730	val: 0.942152	test: 0.724055

Epoch: 38
Loss: 0.20061820303381897
ROC train: 0.999035	val: 0.676923	test: 0.772387
PRC train: 0.998590	val: 0.937734	test: 0.731980

Epoch: 39
Loss: 0.19274275893259163
ROC train: 0.999438	val: 0.673260	test: 0.767519
PRC train: 0.999176	val: 0.937178	test: 0.734739

Epoch: 40
Loss: 0.19475436838776244
ROC train: 0.998233	val: 0.659341	test: 0.758129
PRC train: 0.997072	val: 0.932388	test: 0.730722

Epoch: 41
Loss: 0.19369719899752152
ROC train: 0.998970	val: 0.683516	test: 0.768736
PRC train: 0.998354	val: 0.938124	test: 0.733736

Epoch: 42
Loss: 0.19316460285462317
ROC train: 0.999703	val: 0.692674	test: 0.780212
PRC train: 0.999544	val: 0.940672	test: 0.743980

Epoch: 43
Loss: 0.17588854781679056
ROC train: 0.999797	val: 0.687912	test: 0.783342
PRC train: 0.999697	val: 0.940865	test: 0.755274

Epoch: 44
Loss: 0.18877087953456115
ROC train: 0.999649	val: 0.722711	test: 0.766823
PRC train: 0.999474	val: 0.948465	test: 0.738304

Epoch: 45
Loss: 0.166685133344103
ROC train: 0.999292	val: 0.709524	test: 0.768040
PRC train: 0.998926	val: 0.946462	test: 0.736345

Epoch: 46
Loss: 0.14291761947631296
ROC train: 0.999024	val: 0.688278	test: 0.783516
PRC train: 0.998521	val: 0.938551	test: 0.758784

Epoch: 47
Loss: 0.18335963091263965
ROC train: 0.999638	val: 0.695971	test: 0.781777
PRC train: 0.999476	val: 0.941863	test: 0.740565

Epoch: 48
Loss: 0.17308888849575457
ROC train: 0.999535	val: 0.697436	test: 0.765954
PRC train: 0.999329	val: 0.942128	test: 0.727830

Epoch: 49
Loss: 0.1472273728693984
ROC train: 0.999854	val: 0.708791	test: 0.775517
PRC train: 0.999783	val: 0.946523	test: 0.746682

Epoch: 50
Loss: 0.1560863853020081
ROC train: 0.999803	val: 0.684615	test: 0.787689
PRC train: 0.999699	val: 0.941024	test: 0.771400

Epoch: 51
Loss: 0.1469738885323009
ROC train: 0.999726	val: 0.654212	test: 0.778126
PRC train: 0.999570	val: 0.931292	test: 0.759513

Epoch: 52
Loss: 0.13734797140615135
ROC train: 0.999980	val: 0.682051	test: 0.765258
PRC train: 0.999970	val: 0.938982	test: 0.741495

Epoch: 53
Loss: 0.13292019765341065
ROC train: 1.000000	val: 0.678388	test: 0.760737
PRC train: 1.000000	val: 0.938104	test: 0.744086

Epoch: 54
Loss: 0.12702935473113647
ROC train: 1.000000	val: 0.669231	test: 0.766475
PRC train: 1.000000	val: 0.935806	test: 0.753881

Epoch: 55
Loss: 0.11859213461014714
ROC train: 0.999937	val: 0.683150	test: 0.762476
PRC train: 0.999903	val: 0.941806	test: 0.749009

Epoch: 56
Loss: 0.12933856483116635
ROC train: 0.999806	val: 0.722344	test: 0.760042
PRC train: 0.999690	val: 0.950226	test: 0.732554

Epoch: 57
Loss: 0.13863888387954243
ROC train: 0.999997	val: 0.715385	test: 0.765780
PRC train: 0.999996	val: 0.946935	test: 0.740651

Epoch: 58
Loss: 0.10590853195648786
ROC train: 0.999997	val: 0.682784	test: 0.768910
PRC train: 0.999996	val: 0.939104	test: 0.756432

Epoch: 59
Loss: 0.12042536143006674
ROC train: 0.999989	val: 0.669963	test: 0.769605
PRC train: 0.999983	val: 0.935689	test: 0.752489

Epoch: 60
Loss: 0.0997093670287726
ROC train: 0.999792	val: 0.687179	test: 0.758998
PRC train: 0.999666	val: 0.937840	test: 0.729420

Epoch: 61
Loss: 0.12802806272402806
ROC train: 0.999971	val: 0.681685	test: 0.760911
PRC train: 0.999957	val: 0.937220	test: 0.718605

Epoch: 62
Loss: 0.12838526835685057
ROC train: 0.999997	val: 0.695604	test: 0.756912
PRC train: 0.999996	val: 0.943773	test: 0.743521

Epoch: 63
Loss: 0.10948930585445096
ROC train: 1.000000	val: 0.697436	test: 0.781603
PRC train: 1.000000	val: 0.943378	test: 0.767612

Epoch: 64
Loss: 0.10147672442909283
ROC train: 0.999963	val: 0.694872	test: 0.778473
PRC train: 0.999943	val: 0.942245	test: 0.758168

Epoch: 65
Loss: 0.09186991808183154
ROC train: 0.999683	val: 0.701465	test: 0.779169
PRC train: 0.999501	val: 0.943874	test: 0.758795

Epoch: 66
Loss: 0.10671452246008215
ROC train: 0.999951	val: 0.705128	test: 0.781082
PRC train: 0.999926	val: 0.947245	test: 0.764365

Epoch: 67
Loss: 0.11153064715372293
ROC train: 0.999966	val: 0.693407	test: 0.768562
PRC train: 0.999948	val: 0.943729	test: 0.763503

Epoch: 68
Loss: 0.1090462090052792
ROC train: 1.000000	val: 0.708425	test: 0.781429
PRC train: 1.000000	val: 0.946751	test: 0.761747

Epoch: 69
Loss: 0.09493086267743153
ROC train: 0.999960	val: 0.698168	test: 0.782299
PRC train: 0.999941	val: 0.943810	test: 0.757058

Epoch: 70
Loss: 0.07615074139555064
ROC train: 0.999980	val: 0.705495	test: 0.782820
PRC train: 0.999970	val: 0.944473	test: 0.770407

Epoch: 71
Loss: 0.0993387225035067
ROC train: 1.000000	val: 0.696337	test: 0.779690
PRC train: 1.000000	val: 0.938576	test: 0.761050

Epoch: 72
Loss: 0.10461922388577798
ROC train: 1.000000	val: 0.676923	test: 0.772387
PRC train: 1.000000	val: 0.929792	test: 0.747851

Epoch: 73
Loss: 0.08204629201089024
ROC train: 1.000000	val: 0.658242	test: 0.757260
PRC train: 1.000000	val: 0.922449	test: 0.728336

Epoch: 74
Loss: 0.09436873034272986
ROC train: 1.000000	val: 0.676190	test: 0.764910
PRC train: 1.000000	val: 0.931894	test: 0.732813

Epoch: 75
Loss: 0.08411730255738463
ROC train: 1.000000	val: 0.686447	test: 0.765606
PRC train: 1.000000	val: 0.936667	test: 0.753518

Epoch: 76
Loss: 0.06802948625842539
ROC train: 0.999997	val: 0.703663	test: 0.754651
PRC train: 0.999996	val: 0.942392	test: 0.756543

Epoch: 77
Loss: 0.08784125334456702
ROC train: 0.999983	val: 0.738095	test: 0.759346
PRC train: 0.999974	val: 0.951584	test: 0.733700

Epoch: 78
Loss: 0.0686240424062432
ROC train: 0.999994	val: 0.738828	test: 0.753434
PRC train: 0.999991	val: 0.951029	test: 0.728282

Epoch: 79
Loss: 0.08245289517222518
ROC train: 1.000000	val: 0.726007	test: 0.746653
PRC train: 1.000000	val: 0.948562	test: 0.715876

Epoch: 80
Loss: 0.0638840186579629
ROC train: 1.000000	val: 0.680586	test: 0.741089
PRC train: 1.000000	val: 0.935486	test: 0.706578

Epoch: 81
Loss: 0.06938515705522347
ROC train: 1.000000	val: 0.677289	test: 0.749087
PRC train: 1.000000	val: 0.935078	test: 0.721515

Epoch: 82
Loss: 0.06164019887905371
ROC train: 1.000000	val: 0.691575	test: 0.754825
PRC train: 1.000000	val: 0.937903	test: 0.732999

Epoch: 83
Loss: 0.05690864408641333
ROC train: 1.000000	val: 0.681685	test: 0.759694
PRC train: 1.000000	val: 0.936140	test: 0.740929

Epoch: 84
Loss: 0.07422107600463626
ROC train: 1.000000	val: 0.669597	test: 0.762302
PRC train: 1.000000	val: 0.933033	test: 0.748148

Epoch: 85
Loss: 0.04167396615489341
ROC train: 1.000000	val: 0.671795	test: 0.760389
PRC train: 1.000000	val: 0.933828	test: 0.735484

Epoch: 86
Loss: 0.0673887897050793
ROC train: 1.000000	val: 0.683150	test: 0.758998
PRC train: 1.000000	val: 0.937265	test: 0.715070

Epoch: 87
Loss: 0.06639225874946535
ROC train: 0.999974	val: 0.686447	test: 0.749087
PRC train: 0.999961	val: 0.937081	test: 0.707533

Epoch: 88
Loss: 0.06544388525891509
ROC train: 1.000000	val: 0.674725	test: 0.748392
PRC train: 1.000000	val: 0.933769	test: 0.730277

Epoch: 89
Loss: 0.07455594428798909
ROC train: 1.000000	val: 0.682418	test: 0.762476
PRC train: 1.000000	val: 0.935030	test: 0.755680

Epoch: 90
Loss: 0.0644917797335201
ROC train: 0.999994	val: 0.674359	test: 0.771692
PRC train: 0.999991	val: 0.934436	test: 0.764274

Epoch: 91
Loss: 0.07486728678362006
ROC train: 1.000000	val: 0.673260	test: 0.766128
PRC train: 1.000000	val: 0.935892	test: 0.758001

Epoch: 92
Loss: 0.07493434875723703
ROC train: 1.000000	val: 0.689011	test: 0.764563
PRC train: 1.000000	val: 0.940854	test: 0.758603

Epoch: 93
Loss: 0.06374206420319498
ROC train: 1.000000	val: 0.696703	test: 0.767866
PRC train: 0.993552	val: 0.897057	test: 0.667342

Epoch: 33
Loss: 0.2843937337494567
ROC train: 0.996627	val: 0.623443	test: 0.698835
PRC train: 0.994449	val: 0.910798	test: 0.676911

Epoch: 34
Loss: 0.2619995725080286
ROC train: 0.996855	val: 0.639560	test: 0.713963
PRC train: 0.994871	val: 0.913698	test: 0.686838

Epoch: 35
Loss: 0.24686073403117553
ROC train: 0.997757	val: 0.622344	test: 0.714832
PRC train: 0.996550	val: 0.897796	test: 0.715391

Epoch: 36
Loss: 0.23719175619939073
ROC train: 0.999221	val: 0.638828	test: 0.728743
PRC train: 0.998836	val: 0.913400	test: 0.727303

Epoch: 37
Loss: 0.2318826745930603
ROC train: 0.999121	val: 0.652015	test: 0.723700
PRC train: 0.998627	val: 0.921856	test: 0.709717

Epoch: 38
Loss: 0.23221193306362484
ROC train: 0.999375	val: 0.664103	test: 0.724048
PRC train: 0.999023	val: 0.925730	test: 0.722638

Epoch: 39
Loss: 0.2092535688314606
ROC train: 0.999643	val: 0.645421	test: 0.737611
PRC train: 0.999470	val: 0.915981	test: 0.724117

Epoch: 40
Loss: 0.18474834812052213
ROC train: 0.999854	val: 0.649451	test: 0.732568
PRC train: 0.999781	val: 0.924489	test: 0.705701

Epoch: 41
Loss: 0.20327356591149082
ROC train: 0.999521	val: 0.663004	test: 0.729960
PRC train: 0.999286	val: 0.927623	test: 0.696640

Epoch: 42
Loss: 0.18387755505744113
ROC train: 0.999783	val: 0.638095	test: 0.726308
PRC train: 0.999677	val: 0.912014	test: 0.732704

Epoch: 43
Loss: 0.18058953153088247
ROC train: 0.999932	val: 0.638828	test: 0.719353
PRC train: 0.999896	val: 0.914972	test: 0.729959

Epoch: 44
Loss: 0.18467691330503416
ROC train: 0.999957	val: 0.648352	test: 0.697966
PRC train: 0.999934	val: 0.920272	test: 0.714112

Epoch: 45
Loss: 0.17820141353184546
ROC train: 0.999971	val: 0.670330	test: 0.694662
PRC train: 0.999957	val: 0.927179	test: 0.687527

Epoch: 46
Loss: 0.18906758879312607
ROC train: 0.999994	val: 0.652747	test: 0.715180
PRC train: 0.999991	val: 0.917254	test: 0.711576

Epoch: 47
Loss: 0.13740376164281015
ROC train: 0.999797	val: 0.634432	test: 0.722483
PRC train: 0.999668	val: 0.907460	test: 0.712308

Epoch: 48
Loss: 0.1614387169167639
ROC train: 0.999906	val: 0.638462	test: 0.721440
PRC train: 0.999856	val: 0.917216	test: 0.686257

Epoch: 49
Loss: 0.14636421893741117
ROC train: 0.999803	val: 0.618681	test: 0.726308
PRC train: 0.999688	val: 0.912119	test: 0.689242

Epoch: 50
Loss: 0.15855204542445314
ROC train: 0.999997	val: 0.625275	test: 0.736046
PRC train: 0.999996	val: 0.903861	test: 0.752498

Epoch: 51
Loss: 0.13798151732815428
ROC train: 1.000000	val: 0.620513	test: 0.735524
PRC train: 1.000000	val: 0.903552	test: 0.724549

Epoch: 52
Loss: 0.12532058750543884
ROC train: 0.999957	val: 0.630403	test: 0.718832
PRC train: 0.999935	val: 0.912641	test: 0.707081

Epoch: 53
Loss: 0.14640706667729375
ROC train: 0.999994	val: 0.625275	test: 0.719353
PRC train: 0.999991	val: 0.905739	test: 0.707219

Epoch: 54
Loss: 0.1308492575481829
ROC train: 0.999989	val: 0.616850	test: 0.727526
PRC train: 0.999983	val: 0.906869	test: 0.709267

Epoch: 55
Loss: 0.12662793173259868
ROC train: 0.999980	val: 0.625641	test: 0.729786
PRC train: 0.999970	val: 0.915917	test: 0.707467

Epoch: 56
Loss: 0.11354487617375822
ROC train: 0.999989	val: 0.624176	test: 0.736568
PRC train: 0.999983	val: 0.911058	test: 0.711723

Epoch: 57
Loss: 0.12331166512467848
ROC train: 1.000000	val: 0.639194	test: 0.731525
PRC train: 1.000000	val: 0.903115	test: 0.708856

Epoch: 58
Loss: 0.13189152070695226
ROC train: 1.000000	val: 0.639560	test: 0.717267
PRC train: 1.000000	val: 0.907813	test: 0.698432

Epoch: 59
Loss: 0.1238095059834565
ROC train: 1.000000	val: 0.671795	test: 0.718310
PRC train: 1.000000	val: 0.923814	test: 0.705981

Epoch: 60
Loss: 0.12381476530564824
ROC train: 1.000000	val: 0.685714	test: 0.715702
PRC train: 1.000000	val: 0.929523	test: 0.713152

Epoch: 61
Loss: 0.12513958459314684
ROC train: 1.000000	val: 0.658242	test: 0.714484
PRC train: 1.000000	val: 0.917214	test: 0.711012

Epoch: 62
Loss: 0.1296249016008269
ROC train: 0.999997	val: 0.669231	test: 0.715180
PRC train: 0.999996	val: 0.923827	test: 0.709610

Epoch: 63
Loss: 0.1131779727451742
ROC train: 1.000000	val: 0.680220	test: 0.729091
PRC train: 1.000000	val: 0.929797	test: 0.706847

Epoch: 64
Loss: 0.1194096835530444
ROC train: 1.000000	val: 0.679487	test: 0.711181
PRC train: 1.000000	val: 0.933361	test: 0.681694

Epoch: 65
Loss: 0.08727495792049286
ROC train: 1.000000	val: 0.673260	test: 0.699183
PRC train: 1.000000	val: 0.931360	test: 0.671168

Epoch: 66
Loss: 0.10168362460377947
ROC train: 1.000000	val: 0.656410	test: 0.708920
PRC train: 1.000000	val: 0.922771	test: 0.683300

Epoch: 67
Loss: 0.07805304865532918
ROC train: 1.000000	val: 0.665201	test: 0.717962
PRC train: 1.000000	val: 0.923578	test: 0.684988

Epoch: 68
Loss: 0.11250459812435543
ROC train: 1.000000	val: 0.669963	test: 0.724570
PRC train: 1.000000	val: 0.922740	test: 0.694951

Epoch: 69
Loss: 0.08623555858566521
ROC train: 1.000000	val: 0.615751	test: 0.725613
PRC train: 1.000000	val: 0.896929	test: 0.703122

Epoch: 70
Loss: 0.07726630695817091
ROC train: 1.000000	val: 0.613553	test: 0.734133
PRC train: 1.000000	val: 0.896983	test: 0.707420

Epoch: 71
Loss: 0.10366002372702805
ROC train: 1.000000	val: 0.632601	test: 0.726830
PRC train: 1.000000	val: 0.908052	test: 0.711545

Epoch: 72
Loss: 0.10295129393165392
ROC train: 1.000000	val: 0.650183	test: 0.706138
PRC train: 1.000000	val: 0.905750	test: 0.707572

Epoch: 73
Loss: 0.09161109591470437
ROC train: 1.000000	val: 0.647985	test: 0.699531
PRC train: 1.000000	val: 0.909942	test: 0.709820

Epoch: 74
Loss: 0.07862744439562194
ROC train: 1.000000	val: 0.654945	test: 0.702487
PRC train: 1.000000	val: 0.915127	test: 0.704140

Epoch: 75
Loss: 0.07113714288364917
ROC train: 1.000000	val: 0.661905	test: 0.717440
PRC train: 1.000000	val: 0.916157	test: 0.721516

Epoch: 76
Loss: 0.09267379844383287
ROC train: 1.000000	val: 0.653114	test: 0.712919
PRC train: 1.000000	val: 0.916551	test: 0.720220

Epoch: 77
Loss: 0.08378780622151781
ROC train: 1.000000	val: 0.632967	test: 0.701269
PRC train: 1.000000	val: 0.913121	test: 0.704553

Epoch: 78
Loss: 0.07261979597687336
ROC train: 1.000000	val: 0.651648	test: 0.708920
PRC train: 1.000000	val: 0.920168	test: 0.696592

Epoch: 79
Loss: 0.0959409233080413
ROC train: 1.000000	val: 0.650549	test: 0.712746
PRC train: 1.000000	val: 0.914597	test: 0.708898

Epoch: 80
Loss: 0.06415056666348862
ROC train: 1.000000	val: 0.661172	test: 0.723179
PRC train: 1.000000	val: 0.916801	test: 0.714903

Epoch: 81
Loss: 0.06303341040125307
ROC train: 1.000000	val: 0.671429	test: 0.731351
PRC train: 1.000000	val: 0.921187	test: 0.714100

Epoch: 82
Loss: 0.07891787080089138
ROC train: 1.000000	val: 0.669597	test: 0.736046
PRC train: 1.000000	val: 0.921685	test: 0.718626

Epoch: 83
Loss: 0.060940530148414586
ROC train: 1.000000	val: 0.671062	test: 0.729612
PRC train: 1.000000	val: 0.924542	test: 0.709069

Epoch: 84
Loss: 0.07896471616157057
ROC train: 1.000000	val: 0.663736	test: 0.724048
PRC train: 1.000000	val: 0.920967	test: 0.700954

Epoch: 85
Loss: 0.06114832679633582
ROC train: 1.000000	val: 0.664835	test: 0.722135
PRC train: 1.000000	val: 0.921758	test: 0.690803

Epoch: 86
Loss: 0.0819578365345319
ROC train: 1.000000	val: 0.663004	test: 0.723700
PRC train: 1.000000	val: 0.920187	test: 0.700267

Epoch: 87
Loss: 0.06329588980272549
ROC train: 1.000000	val: 0.640659	test: 0.730308
PRC train: 1.000000	val: 0.899852	test: 0.715978

Epoch: 88
Loss: 0.04367637269589379
ROC train: 1.000000	val: 0.631868	test: 0.737611
PRC train: 1.000000	val: 0.897131	test: 0.728109

Epoch: 89
Loss: 0.05982824501004704
ROC train: 1.000000	val: 0.637363	test: 0.743001
PRC train: 1.000000	val: 0.906160	test: 0.727751

Epoch: 90
Loss: 0.056576761466044324
ROC train: 1.000000	val: 0.633333	test: 0.740393
PRC train: 1.000000	val: 0.911644	test: 0.722468

Epoch: 91
Loss: 0.07932643891234556
ROC train: 1.000000	val: 0.641758	test: 0.736568
PRC train: 1.000000	val: 0.916802	test: 0.732272

Epoch: 92
Loss: 0.06178419522501634
ROC train: 1.000000	val: 0.660806	test: 0.728743
PRC train: 1.000000	val: 0.918894	test: 0.739498

Epoch: 93
Loss: 0.05597839169525556
ROC train: 1.000000	val: 0.655678	test: 0.726308
PRC train: 0.997177	val: 0.918629	test: 0.730179

Epoch: 33
Loss: 0.26195892503111484
ROC train: 0.998593	val: 0.667766	test: 0.748565
PRC train: 0.997972	val: 0.922743	test: 0.743803

Epoch: 34
Loss: 0.25272040656746164
ROC train: 0.999075	val: 0.695971	test: 0.751174
PRC train: 0.998662	val: 0.937753	test: 0.732030

Epoch: 35
Loss: 0.21584639136082853
ROC train: 0.999415	val: 0.697802	test: 0.741089
PRC train: 0.999169	val: 0.939612	test: 0.725563

Epoch: 36
Loss: 0.23347377630796706
ROC train: 0.999757	val: 0.670330	test: 0.747522
PRC train: 0.999640	val: 0.931904	test: 0.727874

Epoch: 37
Loss: 0.2156534020216047
ROC train: 0.999846	val: 0.627839	test: 0.742827
PRC train: 0.999768	val: 0.916520	test: 0.694509

Epoch: 38
Loss: 0.2066402308182355
ROC train: 0.999732	val: 0.654945	test: 0.728395
PRC train: 0.999606	val: 0.928814	test: 0.689006

Epoch: 39
Loss: 0.196778006522572
ROC train: 0.999272	val: 0.667399	test: 0.734133
PRC train: 0.998904	val: 0.931237	test: 0.707040

Epoch: 40
Loss: 0.19091524096514725
ROC train: 0.999795	val: 0.624908	test: 0.742827
PRC train: 0.999690	val: 0.915471	test: 0.680303

Epoch: 41
Loss: 0.18445674861413097
ROC train: 0.999846	val: 0.583516	test: 0.742653
PRC train: 0.999770	val: 0.903722	test: 0.674509

Epoch: 42
Loss: 0.18993113922068552
ROC train: 0.999929	val: 0.563736	test: 0.731003
PRC train: 0.999892	val: 0.896127	test: 0.671336

Epoch: 43
Loss: 0.1719221992722196
ROC train: 0.999997	val: 0.583883	test: 0.728569
PRC train: 0.999996	val: 0.900314	test: 0.692292

Epoch: 44
Loss: 0.1564586658433092
ROC train: 0.999929	val: 0.646886	test: 0.710659
PRC train: 0.999892	val: 0.923230	test: 0.688988

Epoch: 45
Loss: 0.18418100505708807
ROC train: 0.999946	val: 0.650916	test: 0.738654
PRC train: 0.999918	val: 0.923994	test: 0.703235

Epoch: 46
Loss: 0.18108549222497344
ROC train: 0.999997	val: 0.622344	test: 0.733090
PRC train: 0.999996	val: 0.915996	test: 0.714553

Epoch: 47
Loss: 0.15785496587894113
ROC train: 0.999977	val: 0.625275	test: 0.717267
PRC train: 0.999966	val: 0.914714	test: 0.692802

Epoch: 48
Loss: 0.17888029741735784
ROC train: 0.999986	val: 0.642857	test: 0.730482
PRC train: 0.999978	val: 0.916984	test: 0.677132

Epoch: 49
Loss: 0.15723097220109739
ROC train: 0.999991	val: 0.677289	test: 0.725613
PRC train: 0.999987	val: 0.933736	test: 0.679766

Epoch: 50
Loss: 0.14352912587905875
ROC train: 1.000000	val: 0.678388	test: 0.729960
PRC train: 1.000000	val: 0.936274	test: 0.689007

Epoch: 51
Loss: 0.1450053553278244
ROC train: 0.999989	val: 0.625275	test: 0.723700
PRC train: 0.999983	val: 0.915062	test: 0.672807

Epoch: 52
Loss: 0.15501658332478618
ROC train: 0.999971	val: 0.587179	test: 0.732220
PRC train: 0.999957	val: 0.896285	test: 0.677744

Epoch: 53
Loss: 0.13106385223704375
ROC train: 0.999997	val: 0.631868	test: 0.743001
PRC train: 0.999996	val: 0.915038	test: 0.699347

Epoch: 54
Loss: 0.12090670089932018
ROC train: 1.000000	val: 0.639560	test: 0.747174
PRC train: 1.000000	val: 0.911862	test: 0.693304

Epoch: 55
Loss: 0.13274273471854314
ROC train: 1.000000	val: 0.642491	test: 0.738480
PRC train: 1.000000	val: 0.915201	test: 0.678460

Epoch: 56
Loss: 0.10668747556276548
ROC train: 1.000000	val: 0.646886	test: 0.734133
PRC train: 1.000000	val: 0.923910	test: 0.672976

Epoch: 57
Loss: 0.12557090452257288
ROC train: 1.000000	val: 0.608425	test: 0.734307
PRC train: 1.000000	val: 0.915517	test: 0.677976

Epoch: 58
Loss: 0.11904161568220906
ROC train: 1.000000	val: 0.623077	test: 0.733785
PRC train: 1.000000	val: 0.909717	test: 0.669698

Epoch: 59
Loss: 0.11222614413334313
ROC train: 0.999997	val: 0.624176	test: 0.722657
PRC train: 0.999996	val: 0.917565	test: 0.668525

Epoch: 60
Loss: 0.11853846969309881
ROC train: 1.000000	val: 0.628938	test: 0.726830
PRC train: 1.000000	val: 0.919901	test: 0.684113

Epoch: 61
Loss: 0.09755140928978709
ROC train: 1.000000	val: 0.623443	test: 0.738828
PRC train: 1.000000	val: 0.913982	test: 0.690378

Epoch: 62
Loss: 0.10416881551381037
ROC train: 1.000000	val: 0.599267	test: 0.743697
PRC train: 1.000000	val: 0.900733	test: 0.671855

Epoch: 63
Loss: 0.10927678011582664
ROC train: 1.000000	val: 0.599267	test: 0.742132
PRC train: 1.000000	val: 0.902506	test: 0.667067

Epoch: 64
Loss: 0.09381269525308486
ROC train: 1.000000	val: 0.581319	test: 0.746653
PRC train: 1.000000	val: 0.896437	test: 0.668722

Epoch: 65
Loss: 0.07844796286906874
ROC train: 1.000000	val: 0.615751	test: 0.744566
PRC train: 1.000000	val: 0.901312	test: 0.666386

Epoch: 66
Loss: 0.09949333754853812
ROC train: 1.000000	val: 0.631136	test: 0.732742
PRC train: 1.000000	val: 0.909447	test: 0.663201

Epoch: 67
Loss: 0.1153100353962357
ROC train: 1.000000	val: 0.610623	test: 0.724396
PRC train: 1.000000	val: 0.904051	test: 0.667672

Epoch: 68
Loss: 0.09036310703252035
ROC train: 0.999997	val: 0.605861	test: 0.717093
PRC train: 0.999996	val: 0.897030	test: 0.657133

Epoch: 69
Loss: 0.06787597813026634
ROC train: 0.999994	val: 0.591941	test: 0.709442
PRC train: 0.999991	val: 0.888811	test: 0.647633

Epoch: 70
Loss: 0.08094879186965187
ROC train: 0.999997	val: 0.590842	test: 0.716397
PRC train: 0.999996	val: 0.891182	test: 0.653429

Epoch: 71
Loss: 0.08576531965108172
ROC train: 0.999974	val: 0.627106	test: 0.730829
PRC train: 0.999961	val: 0.915692	test: 0.708437

Epoch: 72
Loss: 0.11192770709895154
ROC train: 1.000000	val: 0.635165	test: 0.736568
PRC train: 1.000000	val: 0.919334	test: 0.707779

Epoch: 73
Loss: 0.0790879855685173
ROC train: 0.999994	val: 0.610623	test: 0.741089
PRC train: 0.999991	val: 0.899040	test: 0.680589

Epoch: 74
Loss: 0.08398439270448418
ROC train: 1.000000	val: 0.608425	test: 0.735003
PRC train: 1.000000	val: 0.907087	test: 0.677487

Epoch: 75
Loss: 0.08635450533755462
ROC train: 1.000000	val: 0.594505	test: 0.726830
PRC train: 1.000000	val: 0.905927	test: 0.695335

Epoch: 76
Loss: 0.079327626609224
ROC train: 1.000000	val: 0.628938	test: 0.737437
PRC train: 1.000000	val: 0.911955	test: 0.703862

Epoch: 77
Loss: 0.08123649296058798
ROC train: 1.000000	val: 0.646520	test: 0.742132
PRC train: 1.000000	val: 0.915167	test: 0.707198

Epoch: 78
Loss: 0.07749458439372456
ROC train: 1.000000	val: 0.639927	test: 0.742827
PRC train: 1.000000	val: 0.911316	test: 0.715640

Epoch: 79
Loss: 0.06177564183167261
ROC train: 1.000000	val: 0.620513	test: 0.752565
PRC train: 1.000000	val: 0.905916	test: 0.707787

Epoch: 80
Loss: 0.05431464601768138
ROC train: 1.000000	val: 0.594505	test: 0.750652
PRC train: 1.000000	val: 0.894028	test: 0.696389

Epoch: 81
Loss: 0.05803404516674223
ROC train: 1.000000	val: 0.580220	test: 0.747174
PRC train: 1.000000	val: 0.894388	test: 0.704710

Epoch: 82
Loss: 0.07328148722799563
ROC train: 1.000000	val: 0.597436	test: 0.742827
PRC train: 1.000000	val: 0.899019	test: 0.706516

Epoch: 83
Loss: 0.06318347759587342
ROC train: 1.000000	val: 0.620513	test: 0.743175
PRC train: 1.000000	val: 0.900430	test: 0.689723

Epoch: 84
Loss: 0.05369383091490719
ROC train: 1.000000	val: 0.626740	test: 0.744045
PRC train: 1.000000	val: 0.901494	test: 0.683051

Epoch: 85
Loss: 0.07311788087334158
ROC train: 1.000000	val: 0.623443	test: 0.751695
PRC train: 1.000000	val: 0.907940	test: 0.692572

Epoch: 86
Loss: 0.0753923733421032
ROC train: 1.000000	val: 0.609524	test: 0.747348
PRC train: 1.000000	val: 0.907161	test: 0.675561

Epoch: 87
Loss: 0.05623611388097507
ROC train: 1.000000	val: 0.594139	test: 0.740741
PRC train: 1.000000	val: 0.901978	test: 0.659092

Epoch: 88
Loss: 0.05746732544837124
ROC train: 1.000000	val: 0.576923	test: 0.740393
PRC train: 1.000000	val: 0.890345	test: 0.657880

Epoch: 89
Loss: 0.07153290340604365
ROC train: 1.000000	val: 0.605495	test: 0.740393
PRC train: 1.000000	val: 0.901270	test: 0.664580

Epoch: 90
Loss: 0.06915497781922446
ROC train: 1.000000	val: 0.610256	test: 0.739176
PRC train: 1.000000	val: 0.908139	test: 0.688800

Epoch: 91
Loss: 0.061205278325813905
ROC train: 1.000000	val: 0.569963	test: 0.734655
PRC train: 1.000000	val: 0.898323	test: 0.695746

Epoch: 92
Loss: 0.0613387196927329
ROC train: 1.000000	val: 0.572527	test: 0.743523
PRC train: 1.000000	val: 0.889231	test: 0.700186

Epoch: 93
Loss: 0.0509590801524212
ROC train: 1.000000	val: 0.586447	test: 0.745262
PRC train: 0.992952	val: 0.936484	test: 0.776720

Epoch: 33
Loss: 0.2563525646089304
ROC train: 0.994575	val: 0.683883	test: 0.765432
PRC train: 0.992567	val: 0.932637	test: 0.761368

Epoch: 34
Loss: 0.23480758510274344
ROC train: 0.995174	val: 0.698168	test: 0.783516
PRC train: 0.993240	val: 0.939871	test: 0.782785

Epoch: 35
Loss: 0.23268446481994803
ROC train: 0.995360	val: 0.694872	test: 0.805773
PRC train: 0.993076	val: 0.942447	test: 0.804497

Epoch: 36
Loss: 0.2285158442129805
ROC train: 0.995799	val: 0.691941	test: 0.803165
PRC train: 0.993396	val: 0.938804	test: 0.798098

Epoch: 37
Loss: 0.22907139141541358
ROC train: 0.995748	val: 0.691209	test: 0.798644
PRC train: 0.993193	val: 0.935194	test: 0.788595

Epoch: 38
Loss: 0.2091636518981183
ROC train: 0.995205	val: 0.698168	test: 0.798991
PRC train: 0.992321	val: 0.943205	test: 0.796196

Epoch: 39
Loss: 0.22297709927140338
ROC train: 0.997683	val: 0.680952	test: 0.789950
PRC train: 0.996616	val: 0.936820	test: 0.784875

Epoch: 40
Loss: 0.2085347677748482
ROC train: 0.997945	val: 0.688278	test: 0.803165
PRC train: 0.997003	val: 0.937652	test: 0.794676

Epoch: 41
Loss: 0.2085609242172491
ROC train: 0.997366	val: 0.671062	test: 0.815163
PRC train: 0.996150	val: 0.930941	test: 0.812965

Epoch: 42
Loss: 0.17927704841446562
ROC train: 0.998995	val: 0.672527	test: 0.802121
PRC train: 0.998553	val: 0.930327	test: 0.803134

Epoch: 43
Loss: 0.1850456753310505
ROC train: 0.999247	val: 0.693407	test: 0.794644
PRC train: 0.998870	val: 0.939208	test: 0.798244

Epoch: 44
Loss: 0.18057925649705647
ROC train: 0.999329	val: 0.673626	test: 0.808729
PRC train: 0.998963	val: 0.934422	test: 0.813855

Epoch: 45
Loss: 0.15800500088387318
ROC train: 0.999298	val: 0.687179	test: 0.808903
PRC train: 0.998935	val: 0.936275	test: 0.815313

Epoch: 46
Loss: 0.17256637830770646
ROC train: 0.999392	val: 0.691209	test: 0.807338
PRC train: 0.999111	val: 0.935262	test: 0.816087

Epoch: 47
Loss: 0.16454467685267102
ROC train: 0.999355	val: 0.697436	test: 0.804382
PRC train: 0.999035	val: 0.938641	test: 0.812182

Epoch: 48
Loss: 0.16691204756639308
ROC train: 0.999466	val: 0.684615	test: 0.800904
PRC train: 0.999212	val: 0.935920	test: 0.796984

Epoch: 49
Loss: 0.17076003376922774
ROC train: 0.998907	val: 0.704029	test: 0.791167
PRC train: 0.998391	val: 0.941733	test: 0.782968

Epoch: 50
Loss: 0.1483039273665377
ROC train: 0.999204	val: 0.693407	test: 0.811859
PRC train: 0.998809	val: 0.939938	test: 0.799070

Epoch: 51
Loss: 0.14110238355087795
ROC train: 0.999832	val: 0.669597	test: 0.828030
PRC train: 0.999748	val: 0.933135	test: 0.813829

Epoch: 52
Loss: 0.138227036487735
ROC train: 0.999840	val: 0.669963	test: 0.816032
PRC train: 0.999763	val: 0.932137	test: 0.812764

Epoch: 53
Loss: 0.1374639957168988
ROC train: 0.999726	val: 0.696703	test: 0.801947
PRC train: 0.999602	val: 0.939378	test: 0.803691

Epoch: 54
Loss: 0.14323336294975073
ROC train: 0.999829	val: 0.690110	test: 0.815684
PRC train: 0.999745	val: 0.938767	test: 0.818177

Epoch: 55
Loss: 0.14692534221744313
ROC train: 0.999683	val: 0.655678	test: 0.829943
PRC train: 0.999541	val: 0.929014	test: 0.830585

Epoch: 56
Loss: 0.14896665593584368
ROC train: 0.999723	val: 0.661905	test: 0.828552
PRC train: 0.999603	val: 0.930593	test: 0.829050

Epoch: 57
Loss: 0.15198209907442561
ROC train: 0.999763	val: 0.686813	test: 0.810816
PRC train: 0.999647	val: 0.937728	test: 0.810825

Epoch: 58
Loss: 0.14641616646311054
ROC train: 0.999897	val: 0.697802	test: 0.819857
PRC train: 0.999844	val: 0.941324	test: 0.810012

Epoch: 59
Loss: 0.12043655594997289
ROC train: 0.999951	val: 0.693407	test: 0.826639
PRC train: 0.999926	val: 0.939660	test: 0.822320

Epoch: 60
Loss: 0.11520369518734655
ROC train: 0.999914	val: 0.692308	test: 0.822118
PRC train: 0.999871	val: 0.939169	test: 0.821655

Epoch: 61
Loss: 0.12655310901969918
ROC train: 0.999832	val: 0.699267	test: 0.802643
PRC train: 0.999749	val: 0.942475	test: 0.801344

Epoch: 62
Loss: 0.10815249793465916
ROC train: 0.999757	val: 0.701099	test: 0.795862
PRC train: 0.999636	val: 0.942113	test: 0.783192

Epoch: 63
Loss: 0.13051853012363512
ROC train: 0.999923	val: 0.683883	test: 0.806642
PRC train: 0.999885	val: 0.938510	test: 0.790424

Epoch: 64
Loss: 0.11256833579067589
ROC train: 1.000000	val: 0.690110	test: 0.803165
PRC train: 1.000000	val: 0.940570	test: 0.789011

Epoch: 65
Loss: 0.10025116477637752
ROC train: 1.000000	val: 0.684249	test: 0.802817
PRC train: 1.000000	val: 0.939210	test: 0.800980

Epoch: 66
Loss: 0.11062657229957584
ROC train: 1.000000	val: 0.701099	test: 0.809946
PRC train: 1.000000	val: 0.942266	test: 0.809581

Epoch: 67
Loss: 0.12060887653037589
ROC train: 0.999997	val: 0.704396	test: 0.815858
PRC train: 0.999996	val: 0.942820	test: 0.822265

Epoch: 68
Loss: 0.10825504889615958
ROC train: 1.000000	val: 0.692308	test: 0.814467
PRC train: 1.000000	val: 0.939795	test: 0.818133

Epoch: 69
Loss: 0.09297268243679153
ROC train: 0.999994	val: 0.685348	test: 0.825248
PRC train: 0.999991	val: 0.937923	test: 0.824773

Epoch: 70
Loss: 0.0897386161666429
ROC train: 1.000000	val: 0.687912	test: 0.832899
PRC train: 1.000000	val: 0.938787	test: 0.828529

Epoch: 71
Loss: 0.09844977222739529
ROC train: 1.000000	val: 0.683516	test: 0.833942
PRC train: 1.000000	val: 0.937758	test: 0.830840

Epoch: 72
Loss: 0.09721735144683427
ROC train: 1.000000	val: 0.687912	test: 0.817249
PRC train: 1.000000	val: 0.939387	test: 0.815443

Epoch: 73
Loss: 0.0908812983816709
ROC train: 0.999974	val: 0.700000	test: 0.789950
PRC train: 0.999961	val: 0.942609	test: 0.786778

Epoch: 74
Loss: 0.07633355575065723
ROC train: 0.999986	val: 0.694872	test: 0.788559
PRC train: 0.999978	val: 0.941811	test: 0.782538

Epoch: 75
Loss: 0.0740518834708834
ROC train: 1.000000	val: 0.700000	test: 0.813250
PRC train: 1.000000	val: 0.943237	test: 0.803718

Epoch: 76
Loss: 0.08302290302228389
ROC train: 1.000000	val: 0.706227	test: 0.830638
PRC train: 1.000000	val: 0.944812	test: 0.816659

Epoch: 77
Loss: 0.08637914768888506
ROC train: 1.000000	val: 0.709524	test: 0.832029
PRC train: 1.000000	val: 0.944906	test: 0.817514

Epoch: 78
Loss: 0.07860653625342866
ROC train: 1.000000	val: 0.710623	test: 0.822118
PRC train: 1.000000	val: 0.944795	test: 0.806423

Epoch: 79
Loss: 0.08388118173128159
ROC train: 1.000000	val: 0.698168	test: 0.796905
PRC train: 1.000000	val: 0.940272	test: 0.788890

Epoch: 80
Loss: 0.09107207404145075
ROC train: 0.999977	val: 0.698901	test: 0.809424
PRC train: 0.999965	val: 0.940126	test: 0.797324

Epoch: 81
Loss: 0.06711772715759426
ROC train: 0.999997	val: 0.686447	test: 0.822813
PRC train: 0.999996	val: 0.937290	test: 0.809613

Epoch: 82
Loss: 0.08511535102658162
ROC train: 1.000000	val: 0.701099	test: 0.826291
PRC train: 1.000000	val: 0.940266	test: 0.819768

Epoch: 83
Loss: 0.06324963255678631
ROC train: 0.999986	val: 0.717949	test: 0.816554
PRC train: 0.999978	val: 0.943843	test: 0.817387

Epoch: 84
Loss: 0.0675056406247059
ROC train: 1.000000	val: 0.708059	test: 0.819336
PRC train: 1.000000	val: 0.941813	test: 0.804990

Epoch: 85
Loss: 0.07843973674358215
ROC train: 1.000000	val: 0.708059	test: 0.812554
PRC train: 1.000000	val: 0.943080	test: 0.787049

Epoch: 86
Loss: 0.06801593050029404
ROC train: 0.999994	val: 0.702564	test: 0.796905
PRC train: 0.999991	val: 0.942485	test: 0.765906

Epoch: 87
Loss: 0.07944758023830642
ROC train: 1.000000	val: 0.698168	test: 0.784559
PRC train: 1.000000	val: 0.940884	test: 0.760641

Epoch: 88
Loss: 0.06699936648142538
ROC train: 1.000000	val: 0.708425	test: 0.784559
PRC train: 1.000000	val: 0.942308	test: 0.759380

Epoch: 89
Loss: 0.06715427610181236
ROC train: 1.000000	val: 0.725641	test: 0.801252
PRC train: 1.000000	val: 0.947793	test: 0.785840

Epoch: 90
Loss: 0.07679060812269764
ROC train: 1.000000	val: 0.721245	test: 0.798644
PRC train: 1.000000	val: 0.946043	test: 0.788369

Epoch: 91
Loss: 0.05875802967581782
ROC train: 1.000000	val: 0.695604	test: 0.804556
PRC train: 1.000000	val: 0.935563	test: 0.792294

Epoch: 92
Loss: 0.06840590865750698
ROC train: 1.000000	val: 0.690110	test: 0.817075
PRC train: 1.000000	val: 0.935224	test: 0.806113

Epoch: 93
Loss: 0.07035358983371105
ROC train: 1.000000	val: 0.690476	test: 0.811685
PRC train: 0.996474	val: 0.876268	test: 0.742810

Epoch: 33
Loss: 0.27820183115925473
ROC train: 0.998088	val: 0.591941	test: 0.701617
PRC train: 0.997238	val: 0.875618	test: 0.725290

Epoch: 34
Loss: 0.25737994098192624
ROC train: 0.998830	val: 0.581319	test: 0.729091
PRC train: 0.998243	val: 0.865520	test: 0.748695

Epoch: 35
Loss: 0.25896127486607934
ROC train: 0.998590	val: 0.579121	test: 0.745783
PRC train: 0.997863	val: 0.867795	test: 0.759773

Epoch: 36
Loss: 0.22405671739419647
ROC train: 0.999384	val: 0.595238	test: 0.736915
PRC train: 0.999070	val: 0.877231	test: 0.748988

Epoch: 37
Loss: 0.2289229334563872
ROC train: 0.999449	val: 0.590476	test: 0.726308
PRC train: 0.999163	val: 0.876273	test: 0.744711

Epoch: 38
Loss: 0.2526992094075635
ROC train: 0.999680	val: 0.591941	test: 0.768736
PRC train: 0.999509	val: 0.879787	test: 0.765847

Epoch: 39
Loss: 0.20207882452924858
ROC train: 0.999252	val: 0.568864	test: 0.774648
PRC train: 0.998915	val: 0.868717	test: 0.773588

Epoch: 40
Loss: 0.18812433269462875
ROC train: 0.999740	val: 0.591941	test: 0.742306
PRC train: 0.999615	val: 0.881462	test: 0.743714

Epoch: 41
Loss: 0.18797619603055485
ROC train: 0.999820	val: 0.602930	test: 0.749261
PRC train: 0.999741	val: 0.892264	test: 0.758858

Epoch: 42
Loss: 0.1880602409111435
ROC train: 0.999883	val: 0.586447	test: 0.745609
PRC train: 0.999827	val: 0.880342	test: 0.759574

Epoch: 43
Loss: 0.17955864970169413
ROC train: 0.999897	val: 0.592308	test: 0.710833
PRC train: 0.999845	val: 0.876378	test: 0.730144

Epoch: 44
Loss: 0.16932013554138886
ROC train: 0.999997	val: 0.602564	test: 0.745609
PRC train: 0.999996	val: 0.891644	test: 0.741772

Epoch: 45
Loss: 0.1447157045589677
ROC train: 0.999991	val: 0.594872	test: 0.738828
PRC train: 0.999987	val: 0.888646	test: 0.738214

Epoch: 46
Loss: 0.16761263585712124
ROC train: 1.000000	val: 0.575824	test: 0.736741
PRC train: 1.000000	val: 0.876165	test: 0.739988

Epoch: 47
Loss: 0.14570756070964794
ROC train: 1.000000	val: 0.564835	test: 0.717962
PRC train: 1.000000	val: 0.869901	test: 0.729155

Epoch: 48
Loss: 0.16137320644846437
ROC train: 0.999991	val: 0.547253	test: 0.715702
PRC train: 0.999987	val: 0.859813	test: 0.730470

Epoch: 49
Loss: 0.15073821055496034
ROC train: 0.999937	val: 0.556777	test: 0.695357
PRC train: 0.999904	val: 0.862510	test: 0.712156

Epoch: 50
Loss: 0.17159628506005564
ROC train: 0.999986	val: 0.564103	test: 0.729960
PRC train: 0.999978	val: 0.870297	test: 0.735124

Epoch: 51
Loss: 0.1546627972897109
ROC train: 0.999840	val: 0.567766	test: 0.712572
PRC train: 0.999753	val: 0.871974	test: 0.720084

Epoch: 52
Loss: 0.15355088586895574
ROC train: 0.999852	val: 0.561538	test: 0.717788
PRC train: 0.999777	val: 0.869870	test: 0.734737

Epoch: 53
Loss: 0.1562138346201876
ROC train: 0.999991	val: 0.587912	test: 0.735524
PRC train: 0.999987	val: 0.881264	test: 0.742526

Epoch: 54
Loss: 0.13862721405703762
ROC train: 1.000000	val: 0.608425	test: 0.732047
PRC train: 1.000000	val: 0.886413	test: 0.738546

Epoch: 55
Loss: 0.1264451538039194
ROC train: 1.000000	val: 0.587912	test: 0.737611
PRC train: 1.000000	val: 0.878526	test: 0.748433

Epoch: 56
Loss: 0.14841025655632972
ROC train: 0.999957	val: 0.577289	test: 0.746305
PRC train: 0.999935	val: 0.878651	test: 0.764465

Epoch: 57
Loss: 0.1370723001195014
ROC train: 0.999951	val: 0.576557	test: 0.739871
PRC train: 0.999926	val: 0.885663	test: 0.755094

Epoch: 58
Loss: 0.12136321136589208
ROC train: 0.999989	val: 0.594872	test: 0.725439
PRC train: 0.999983	val: 0.890302	test: 0.742020

Epoch: 59
Loss: 0.11293107801754229
ROC train: 1.000000	val: 0.603663	test: 0.723352
PRC train: 1.000000	val: 0.888752	test: 0.738899

Epoch: 60
Loss: 0.11606495082304998
ROC train: 1.000000	val: 0.597070	test: 0.732047
PRC train: 1.000000	val: 0.888897	test: 0.739582

Epoch: 61
Loss: 0.12185357657477458
ROC train: 0.999997	val: 0.610623	test: 0.731351
PRC train: 0.999996	val: 0.894746	test: 0.737365

Epoch: 62
Loss: 0.11551248026536001
ROC train: 0.999997	val: 0.601465	test: 0.730134
PRC train: 0.999996	val: 0.892921	test: 0.745183

Epoch: 63
Loss: 0.09229238288014983
ROC train: 1.000000	val: 0.594505	test: 0.728917
PRC train: 1.000000	val: 0.886595	test: 0.747237

Epoch: 64
Loss: 0.11217554674994552
ROC train: 0.999997	val: 0.606227	test: 0.741089
PRC train: 0.999996	val: 0.886865	test: 0.754999

Epoch: 65
Loss: 0.09007223598277873
ROC train: 1.000000	val: 0.610989	test: 0.739524
PRC train: 1.000000	val: 0.889656	test: 0.751341

Epoch: 66
Loss: 0.0872117687654
ROC train: 1.000000	val: 0.599267	test: 0.731699
PRC train: 1.000000	val: 0.886482	test: 0.748739

Epoch: 67
Loss: 0.11385539367760178
ROC train: 1.000000	val: 0.588645	test: 0.694836
PRC train: 1.000000	val: 0.877711	test: 0.728608

Epoch: 68
Loss: 0.0866762998319573
ROC train: 1.000000	val: 0.599267	test: 0.699531
PRC train: 1.000000	val: 0.887282	test: 0.736575

Epoch: 69
Loss: 0.08835391669068135
ROC train: 1.000000	val: 0.604029	test: 0.694836
PRC train: 1.000000	val: 0.892445	test: 0.728529

Epoch: 70
Loss: 0.09374008347011505
ROC train: 1.000000	val: 0.604396	test: 0.713093
PRC train: 1.000000	val: 0.893560	test: 0.733524

Epoch: 71
Loss: 0.09173484766368996
ROC train: 1.000000	val: 0.594139	test: 0.729786
PRC train: 1.000000	val: 0.889840	test: 0.747739

Epoch: 72
Loss: 0.0776028358253259
ROC train: 1.000000	val: 0.612088	test: 0.733438
PRC train: 1.000000	val: 0.892415	test: 0.753613

Epoch: 73
Loss: 0.08537763813764387
ROC train: 1.000000	val: 0.606593	test: 0.724570
PRC train: 1.000000	val: 0.886456	test: 0.751462

Epoch: 74
Loss: 0.10143531553649912
ROC train: 1.000000	val: 0.600366	test: 0.753782
PRC train: 1.000000	val: 0.888081	test: 0.768659

Epoch: 75
Loss: 0.07603258607974564
ROC train: 1.000000	val: 0.597070	test: 0.748739
PRC train: 1.000000	val: 0.891039	test: 0.758098

Epoch: 76
Loss: 0.07212092262479618
ROC train: 0.999997	val: 0.587179	test: 0.729960
PRC train: 0.999996	val: 0.888157	test: 0.750159

Epoch: 77
Loss: 0.08350921140179572
ROC train: 0.999994	val: 0.573626	test: 0.726482
PRC train: 0.999991	val: 0.880686	test: 0.749774

Epoch: 78
Loss: 0.062388727835850186
ROC train: 1.000000	val: 0.594872	test: 0.724744
PRC train: 1.000000	val: 0.888555	test: 0.750189

Epoch: 79
Loss: 0.08215342859000765
ROC train: 1.000000	val: 0.608059	test: 0.710485
PRC train: 1.000000	val: 0.893314	test: 0.742963

Epoch: 80
Loss: 0.06560465498134176
ROC train: 1.000000	val: 0.617582	test: 0.739350
PRC train: 1.000000	val: 0.902798	test: 0.745959

Epoch: 81
Loss: 0.06162125577355676
ROC train: 1.000000	val: 0.623077	test: 0.764737
PRC train: 1.000000	val: 0.906003	test: 0.751980

Epoch: 82
Loss: 0.07128154143476836
ROC train: 1.000000	val: 0.617582	test: 0.744218
PRC train: 1.000000	val: 0.901007	test: 0.741472

Epoch: 83
Loss: 0.049770684058947426
ROC train: 1.000000	val: 0.621978	test: 0.724917
PRC train: 1.000000	val: 0.899540	test: 0.735145

Epoch: 84
Loss: 0.07195564161074589
ROC train: 1.000000	val: 0.625641	test: 0.728743
PRC train: 1.000000	val: 0.899982	test: 0.747154

Epoch: 85
Loss: 0.061913072735936145
ROC train: 1.000000	val: 0.606593	test: 0.737437
PRC train: 1.000000	val: 0.893692	test: 0.751922

Epoch: 86
Loss: 0.06147723581290384
ROC train: 0.999997	val: 0.608791	test: 0.740741
PRC train: 0.999996	val: 0.890982	test: 0.748809

Epoch: 87
Loss: 0.05627785243082266
ROC train: 1.000000	val: 0.605128	test: 0.752739
PRC train: 1.000000	val: 0.888580	test: 0.758131

Epoch: 88
Loss: 0.06589834025778937
ROC train: 1.000000	val: 0.593773	test: 0.756738
PRC train: 1.000000	val: 0.887424	test: 0.762138

Epoch: 89
Loss: 0.07301864804808778
ROC train: 1.000000	val: 0.595604	test: 0.766128
PRC train: 1.000000	val: 0.891210	test: 0.773834

Epoch: 90
Loss: 0.06572313688923294
ROC train: 1.000000	val: 0.610623	test: 0.740915
PRC train: 1.000000	val: 0.895817	test: 0.753163

Epoch: 91
Loss: 0.053249017466220225
ROC train: 1.000000	val: 0.615751	test: 0.741610
PRC train: 1.000000	val: 0.899259	test: 0.759329

Epoch: 92
Loss: 0.045082311895944985
ROC train: 1.000000	val: 0.620513	test: 0.749261
PRC train: 1.000000	val: 0.905202	test: 0.765765

Epoch: 93
Loss: 0.06083062066734313
ROC train: 1.000000	val: 0.636630	test: 0.725961
PRC train: 0.970741	val: 0.922459	test: 0.697202

Epoch: 95
Loss: 0.22258999127810025
ROC train: 0.981307	val: 0.671062	test: 0.692054
PRC train: 0.972791	val: 0.922894	test: 0.690607

Epoch: 96
Loss: 0.22443110431517255
ROC train: 0.984386	val: 0.647619	test: 0.696401
PRC train: 0.977160	val: 0.916134	test: 0.691958

Epoch: 97
Loss: 0.22618979490437569
ROC train: 0.984224	val: 0.678755	test: 0.687880
PRC train: 0.977135	val: 0.924157	test: 0.683752

Epoch: 98
Loss: 0.23742869641033942
ROC train: 0.983034	val: 0.669597	test: 0.681099
PRC train: 0.975408	val: 0.921208	test: 0.679601

Epoch: 99
Loss: 0.22853041208806096
ROC train: 0.983878	val: 0.651648	test: 0.697966
PRC train: 0.976438	val: 0.916424	test: 0.696469

Epoch: 100
Loss: 0.22495912925419
ROC train: 0.985148	val: 0.656777	test: 0.703356
PRC train: 0.978693	val: 0.920536	test: 0.708649

Epoch: 101
Loss: 0.22138060556231526
ROC train: 0.985825	val: 0.661172	test: 0.697444
PRC train: 0.980041	val: 0.921343	test: 0.703873

Epoch: 102
Loss: 0.22572143712596254
ROC train: 0.984155	val: 0.666667	test: 0.691010
PRC train: 0.976715	val: 0.922210	test: 0.690823

Epoch: 103
Loss: 0.21774308341251386
ROC train: 0.986367	val: 0.663736	test: 0.708746
PRC train: 0.980582	val: 0.925378	test: 0.707076

Epoch: 104
Loss: 0.21260354441564502
ROC train: 0.986572	val: 0.672161	test: 0.714137
PRC train: 0.981260	val: 0.929310	test: 0.708222

Epoch: 105
Loss: 0.23066580181061375
ROC train: 0.983893	val: 0.679853	test: 0.678491
PRC train: 0.976437	val: 0.926533	test: 0.670347

Epoch: 106
Loss: 0.24487323316778023
ROC train: 0.986513	val: 0.670696	test: 0.681447
PRC train: 0.980528	val: 0.924843	test: 0.671952

Epoch: 107
Loss: 0.22556937984017766
ROC train: 0.987023	val: 0.657509	test: 0.699878
PRC train: 0.981937	val: 0.921894	test: 0.697323

Epoch: 108
Loss: 0.22080689353638044
ROC train: 0.983759	val: 0.645788	test: 0.709790
PRC train: 0.976731	val: 0.914813	test: 0.701779

Epoch: 109
Loss: 0.21079215276666682
ROC train: 0.982534	val: 0.652015	test: 0.711528
PRC train: 0.974636	val: 0.915383	test: 0.706501

Epoch: 110
Loss: 0.22534447912152858
ROC train: 0.984680	val: 0.657875	test: 0.711355
PRC train: 0.978252	val: 0.921092	test: 0.710225

Epoch: 111
Loss: 0.20922444851277203
ROC train: 0.981838	val: 0.629304	test: 0.712572
PRC train: 0.973440	val: 0.912324	test: 0.712906

Epoch: 112
Loss: 0.2024493774711579
ROC train: 0.984284	val: 0.644689	test: 0.712050
PRC train: 0.977735	val: 0.921470	test: 0.703125

Epoch: 113
Loss: 0.20666880302956328
ROC train: 0.986130	val: 0.632601	test: 0.713441
PRC train: 0.980663	val: 0.919232	test: 0.695779

Epoch: 114
Loss: 0.22322469412621054
ROC train: 0.984612	val: 0.662637	test: 0.693792
PRC train: 0.977890	val: 0.921007	test: 0.674534

Epoch: 115
Loss: 0.19810916081845803
ROC train: 0.986424	val: 0.669231	test: 0.671188
PRC train: 0.980941	val: 0.925998	test: 0.659019

Epoch: 116
Loss: 0.2164269677225467
ROC train: 0.987389	val: 0.667399	test: 0.677100
PRC train: 0.981555	val: 0.926930	test: 0.671541

Epoch: 117
Loss: 0.20696509305908334
ROC train: 0.986470	val: 0.647985	test: 0.701965
PRC train: 0.980090	val: 0.915926	test: 0.688409

Epoch: 118
Loss: 0.22494666918256584
ROC train: 0.988830	val: 0.677289	test: 0.725787
PRC train: 0.983758	val: 0.926952	test: 0.715038

Epoch: 119
Loss: 0.2038722542747932
ROC train: 0.988316	val: 0.699267	test: 0.707007
PRC train: 0.983589	val: 0.925630	test: 0.697596

Epoch: 120
Loss: 0.180888625979129
ROC train: 0.988419	val: 0.699634	test: 0.691532
PRC train: 0.983532	val: 0.932399	test: 0.683785

Early stopping
Best (ROC):	 train: 0.934715	val: 0.722344	test: 0.757433
Best (PRC):	 train: 0.894342	val: 0.941789	test: 0.764273

PRC train: 0.974153	val: 0.925989	test: 0.724391

Epoch: 95
Loss: 0.22101277079095957
ROC train: 0.979044	val: 0.664835	test: 0.721614
PRC train: 0.968873	val: 0.927348	test: 0.712223

Epoch: 96
Loss: 0.22923489206682338
ROC train: 0.978211	val: 0.672161	test: 0.722483
PRC train: 0.966709	val: 0.928682	test: 0.714156

Epoch: 97
Loss: 0.22992193036047787
ROC train: 0.981064	val: 0.684249	test: 0.725091
PRC train: 0.971302	val: 0.934847	test: 0.714103

Epoch: 98
Loss: 0.23808945228745557
ROC train: 0.981513	val: 0.706593	test: 0.726830
PRC train: 0.972066	val: 0.940609	test: 0.720512

Epoch: 99
Loss: 0.23123448925752782
ROC train: 0.982854	val: 0.705128	test: 0.739524
PRC train: 0.973912	val: 0.940536	test: 0.742930

Epoch: 100
Loss: 0.2394056377563773
ROC train: 0.983847	val: 0.707692	test: 0.744740
PRC train: 0.975671	val: 0.941589	test: 0.753060

Epoch: 101
Loss: 0.22303244667792949
ROC train: 0.985200	val: 0.704762	test: 0.728743
PRC train: 0.978121	val: 0.939398	test: 0.731583

Epoch: 102
Loss: 0.22064117003264277
ROC train: 0.984583	val: 0.695238	test: 0.730829
PRC train: 0.977505	val: 0.934791	test: 0.733727

Epoch: 103
Loss: 0.21394389602675545
ROC train: 0.985257	val: 0.690476	test: 0.743349
PRC train: 0.977854	val: 0.933082	test: 0.744024

Epoch: 104
Loss: 0.2227770001916896
ROC train: 0.985334	val: 0.686813	test: 0.734655
PRC train: 0.977980	val: 0.931223	test: 0.734037

Epoch: 105
Loss: 0.20160599917168978
ROC train: 0.984332	val: 0.668864	test: 0.733438
PRC train: 0.976405	val: 0.927473	test: 0.741736

Epoch: 106
Loss: 0.22459506677681834
ROC train: 0.985100	val: 0.679487	test: 0.737263
PRC train: 0.977699	val: 0.930320	test: 0.745238

Epoch: 107
Loss: 0.2229416264475888
ROC train: 0.979820	val: 0.691941	test: 0.724744
PRC train: 0.970326	val: 0.928287	test: 0.741471

Epoch: 108
Loss: 0.2230751990573602
ROC train: 0.984007	val: 0.687546	test: 0.710659
PRC train: 0.976402	val: 0.932001	test: 0.715405

Epoch: 109
Loss: 0.21159267233893414
ROC train: 0.982203	val: 0.686081	test: 0.724048
PRC train: 0.973615	val: 0.932784	test: 0.724591

Epoch: 110
Loss: 0.21798768848944955
ROC train: 0.983116	val: 0.688278	test: 0.733612
PRC train: 0.975000	val: 0.930650	test: 0.744239

Epoch: 111
Loss: 0.2131258741385403
ROC train: 0.984820	val: 0.671795	test: 0.729786
PRC train: 0.977754	val: 0.922703	test: 0.738898

Epoch: 112
Loss: 0.21362533040548595
ROC train: 0.983921	val: 0.668864	test: 0.720570
PRC train: 0.976614	val: 0.917895	test: 0.724452

Epoch: 113
Loss: 0.20970874992135619
ROC train: 0.984147	val: 0.665201	test: 0.731699
PRC train: 0.976889	val: 0.920104	test: 0.734804

Epoch: 114
Loss: 0.20411504970778965
ROC train: 0.985080	val: 0.658608	test: 0.734829
PRC train: 0.978218	val: 0.920323	test: 0.740264

Epoch: 115
Loss: 0.20513358809922616
ROC train: 0.987820	val: 0.664835	test: 0.734655
PRC train: 0.982398	val: 0.925312	test: 0.740077

Epoch: 116
Loss: 0.20938337110957073
ROC train: 0.985696	val: 0.695971	test: 0.716919
PRC train: 0.979441	val: 0.936027	test: 0.727590

Epoch: 117
Loss: 0.21507580199334536
ROC train: 0.985919	val: 0.694505	test: 0.716397
PRC train: 0.979720	val: 0.934928	test: 0.720678

Epoch: 118
Loss: 0.20726157742834964
ROC train: 0.986761	val: 0.659707	test: 0.733612
PRC train: 0.980387	val: 0.923898	test: 0.729718

Epoch: 119
Loss: 0.19083907557194196
ROC train: 0.985693	val: 0.657509	test: 0.735003
PRC train: 0.978253	val: 0.922663	test: 0.737782

Epoch: 120
Loss: 0.18474186630209122
ROC train: 0.985040	val: 0.679487	test: 0.732394
PRC train: 0.977792	val: 0.932126	test: 0.733648

Early stopping
Best (ROC):	 train: 0.942517	val: 0.723810	test: 0.726135
Best (PRC):	 train: 0.906626	val: 0.939296	test: 0.693646

PRC train: 0.999978	val: 0.923020	test: 0.732814

Epoch: 94
Loss: 0.0852994352132856
ROC train: 0.999735	val: 0.608425	test: 0.722657
PRC train: 0.999601	val: 0.888231	test: 0.717948

Epoch: 95
Loss: 0.09453255244347439
ROC train: 1.000000	val: 0.687546	test: 0.756216
PRC train: 1.000000	val: 0.920935	test: 0.740873

Epoch: 96
Loss: 0.08764564753774895
ROC train: 0.999892	val: 0.694872	test: 0.735872
PRC train: 0.999837	val: 0.925558	test: 0.716125

Epoch: 97
Loss: 0.0935756844184646
ROC train: 0.999857	val: 0.670696	test: 0.735350
PRC train: 0.999781	val: 0.918266	test: 0.709285

Epoch: 98
Loss: 0.09748731238328397
ROC train: 0.999997	val: 0.662271	test: 0.723526
PRC train: 0.999996	val: 0.909502	test: 0.706117

Epoch: 99
Loss: 0.06437815476595773
ROC train: 1.000000	val: 0.679487	test: 0.712919
PRC train: 1.000000	val: 0.917129	test: 0.701684

Epoch: 100
Loss: 0.0628312277681944
ROC train: 1.000000	val: 0.678022	test: 0.699878
PRC train: 1.000000	val: 0.920245	test: 0.690345

Epoch: 101
Loss: 0.06561226457881444
ROC train: 1.000000	val: 0.695971	test: 0.715528
PRC train: 1.000000	val: 0.930549	test: 0.700876

Epoch: 102
Loss: 0.07184212265318025
ROC train: 0.999934	val: 0.683516	test: 0.719701
PRC train: 0.999898	val: 0.926853	test: 0.696791

Epoch: 103
Loss: 0.06643292934661366
ROC train: 1.000000	val: 0.673626	test: 0.711355
PRC train: 1.000000	val: 0.919447	test: 0.694475

Epoch: 104
Loss: 0.05989313964195893
ROC train: 0.999977	val: 0.680220	test: 0.699878
PRC train: 0.999965	val: 0.919923	test: 0.690545

Epoch: 105
Loss: 0.08331448499002854
ROC train: 0.999946	val: 0.665934	test: 0.724396
PRC train: 0.999916	val: 0.914435	test: 0.701093

Epoch: 106
Loss: 0.0629290706872124
ROC train: 0.999957	val: 0.664835	test: 0.711702
PRC train: 0.999935	val: 0.919083	test: 0.697647

Epoch: 107
Loss: 0.06340251777086492
ROC train: 1.000000	val: 0.676923	test: 0.703008
PRC train: 1.000000	val: 0.923256	test: 0.689917

Epoch: 108
Loss: 0.045075879583947445
ROC train: 1.000000	val: 0.701832	test: 0.713615
PRC train: 1.000000	val: 0.927265	test: 0.695485

Epoch: 109
Loss: 0.06129588375844054
ROC train: 1.000000	val: 0.687546	test: 0.735698
PRC train: 1.000000	val: 0.926836	test: 0.708443

Epoch: 110
Loss: 0.08320686916246792
ROC train: 0.999974	val: 0.682418	test: 0.732742
PRC train: 0.999961	val: 0.923724	test: 0.701925

Epoch: 111
Loss: 0.0578895107714825
ROC train: 0.999966	val: 0.671429	test: 0.724570
PRC train: 0.999949	val: 0.919146	test: 0.700041

Epoch: 112
Loss: 0.06066831670054416
ROC train: 0.999966	val: 0.690842	test: 0.748218
PRC train: 0.999947	val: 0.930000	test: 0.723234

Epoch: 113
Loss: 0.06467839056485229
ROC train: 1.000000	val: 0.672527	test: 0.742306
PRC train: 1.000000	val: 0.919555	test: 0.714122

Epoch: 114
Loss: 0.05120338364291099
ROC train: 1.000000	val: 0.674359	test: 0.736741
PRC train: 1.000000	val: 0.913896	test: 0.706203

Epoch: 115
Loss: 0.049784782157958646
ROC train: 1.000000	val: 0.683516	test: 0.734481
PRC train: 1.000000	val: 0.922994	test: 0.708618

Epoch: 116
Loss: 0.062154348964150084
ROC train: 1.000000	val: 0.685348	test: 0.708920
PRC train: 1.000000	val: 0.926823	test: 0.692026

Epoch: 117
Loss: 0.0471499030949277
ROC train: 1.000000	val: 0.668132	test: 0.698313
PRC train: 1.000000	val: 0.921385	test: 0.686549

Epoch: 118
Loss: 0.04020388413344792
ROC train: 0.999994	val: 0.639927	test: 0.722483
PRC train: 0.999991	val: 0.906942	test: 0.716471

Epoch: 119
Loss: 0.04688894779676987
ROC train: 1.000000	val: 0.676923	test: 0.758651
PRC train: 1.000000	val: 0.919467	test: 0.751008

Epoch: 120
Loss: 0.06198806110747408
ROC train: 0.999983	val: 0.710256	test: 0.767693
PRC train: 0.999974	val: 0.933352	test: 0.746432

Early stopping
Best (ROC):	 train: 0.988134	val: 0.712454	test: 0.757433
Best (PRC):	 train: 0.981728	val: 0.936861	test: 0.753189

PRC train: 1.000000	val: 0.927077	test: 0.790060

Epoch: 94
Loss: 0.062457643149014384
ROC train: 1.000000	val: 0.669597	test: 0.749783
PRC train: 1.000000	val: 0.925545	test: 0.786801

Epoch: 95
Loss: 0.058673310272407034
ROC train: 0.999989	val: 0.654945	test: 0.739871
PRC train: 0.999983	val: 0.921252	test: 0.774453

Epoch: 96
Loss: 0.0656667927597494
ROC train: 0.999966	val: 0.653480	test: 0.730134
PRC train: 0.999949	val: 0.918974	test: 0.766218

Epoch: 97
Loss: 0.0807124391616922
ROC train: 1.000000	val: 0.659707	test: 0.762128
PRC train: 1.000000	val: 0.927019	test: 0.791388

Epoch: 98
Loss: 0.06572657091798577
ROC train: 1.000000	val: 0.661538	test: 0.743349
PRC train: 1.000000	val: 0.927072	test: 0.778209

Epoch: 99
Loss: 0.06303630125992092
ROC train: 1.000000	val: 0.632967	test: 0.737089
PRC train: 1.000000	val: 0.915842	test: 0.770014

Epoch: 100
Loss: 0.06656711589833995
ROC train: 1.000000	val: 0.644689	test: 0.759346
PRC train: 1.000000	val: 0.919383	test: 0.780586

Epoch: 101
Loss: 0.051835172210989745
ROC train: 1.000000	val: 0.657143	test: 0.763867
PRC train: 1.000000	val: 0.921419	test: 0.780442

Epoch: 102
Loss: 0.04651900530222909
ROC train: 1.000000	val: 0.671795	test: 0.729786
PRC train: 1.000000	val: 0.927343	test: 0.759167

Epoch: 103
Loss: 0.048929387759564805
ROC train: 1.000000	val: 0.680586	test: 0.710137
PRC train: 1.000000	val: 0.928202	test: 0.745439

Epoch: 104
Loss: 0.04967013466010337
ROC train: 1.000000	val: 0.670696	test: 0.723352
PRC train: 1.000000	val: 0.920587	test: 0.761906

Epoch: 105
Loss: 0.03707250022011939
ROC train: 1.000000	val: 0.678022	test: 0.753434
PRC train: 1.000000	val: 0.925364	test: 0.784571

Epoch: 106
Loss: 0.0518160535257227
ROC train: 1.000000	val: 0.671062	test: 0.760563
PRC train: 1.000000	val: 0.923535	test: 0.790517

Epoch: 107
Loss: 0.04144603892516635
ROC train: 1.000000	val: 0.666667	test: 0.745436
PRC train: 1.000000	val: 0.921075	test: 0.774358

Epoch: 108
Loss: 0.05110817716427507
ROC train: 1.000000	val: 0.680586	test: 0.747870
PRC train: 1.000000	val: 0.927744	test: 0.770945

Epoch: 109
Loss: 0.055633310708805984
ROC train: 1.000000	val: 0.693040	test: 0.751869
PRC train: 1.000000	val: 0.932852	test: 0.777830

Epoch: 110
Loss: 0.0541967071936955
ROC train: 1.000000	val: 0.676190	test: 0.762476
PRC train: 1.000000	val: 0.922546	test: 0.792342

Epoch: 111
Loss: 0.05972049413705116
ROC train: 1.000000	val: 0.661172	test: 0.772561
PRC train: 1.000000	val: 0.915399	test: 0.795791

Epoch: 112
Loss: 0.05360904806105045
ROC train: 1.000000	val: 0.651282	test: 0.756912
PRC train: 1.000000	val: 0.912066	test: 0.783383

Epoch: 113
Loss: 0.05095917825495512
ROC train: 1.000000	val: 0.654579	test: 0.728743
PRC train: 1.000000	val: 0.915805	test: 0.751285

Epoch: 114
Loss: 0.04197754427810526
ROC train: 1.000000	val: 0.656044	test: 0.717093
PRC train: 1.000000	val: 0.917076	test: 0.747933

Epoch: 115
Loss: 0.045181954362386575
ROC train: 1.000000	val: 0.658608	test: 0.747696
PRC train: 1.000000	val: 0.918095	test: 0.776072

Epoch: 116
Loss: 0.04996950306069322
ROC train: 1.000000	val: 0.645421	test: 0.743175
PRC train: 1.000000	val: 0.914012	test: 0.775907

Epoch: 117
Loss: 0.0347451469974623
ROC train: 1.000000	val: 0.650916	test: 0.727352
PRC train: 1.000000	val: 0.918109	test: 0.765730

Epoch: 118
Loss: 0.04656427864055872
ROC train: 1.000000	val: 0.655311	test: 0.735872
PRC train: 1.000000	val: 0.921584	test: 0.774276

Epoch: 119
Loss: 0.05004451909213744
ROC train: 1.000000	val: 0.668132	test: 0.765606
PRC train: 1.000000	val: 0.927467	test: 0.796462

Epoch: 120
Loss: 0.040630563690186194
ROC train: 1.000000	val: 0.655678	test: 0.778821
PRC train: 1.000000	val: 0.922478	test: 0.806491

Early stopping
Best (ROC):	 train: 0.992820	val: 0.702930	test: 0.793949
Best (PRC):	 train: 0.989019	val: 0.936049	test: 0.796466

PRC train: 0.999851	val: 0.936430	test: 0.782168

Epoch: 94
Loss: 0.10339413472787676
ROC train: 0.999974	val: 0.683150	test: 0.768562
PRC train: 0.999961	val: 0.933857	test: 0.783339

Epoch: 95
Loss: 0.08651748633010828
ROC train: 0.999969	val: 0.655311	test: 0.775343
PRC train: 0.999952	val: 0.926055	test: 0.776623

Epoch: 96
Loss: 0.08582631946197594
ROC train: 0.999994	val: 0.673626	test: 0.780212
PRC train: 0.999991	val: 0.931753	test: 0.775549

Epoch: 97
Loss: 0.07941161001890018
ROC train: 0.999983	val: 0.693407	test: 0.781603
PRC train: 0.999974	val: 0.937569	test: 0.776377

Epoch: 98
Loss: 0.08488399715706332
ROC train: 0.999951	val: 0.710989	test: 0.774300
PRC train: 0.999926	val: 0.941039	test: 0.773757

Epoch: 99
Loss: 0.08180456603492639
ROC train: 1.000000	val: 0.673260	test: 0.773431
PRC train: 1.000000	val: 0.932549	test: 0.779088

Epoch: 100
Loss: 0.08392370824846564
ROC train: 1.000000	val: 0.652747	test: 0.792732
PRC train: 1.000000	val: 0.928278	test: 0.798284

Epoch: 101
Loss: 0.08597171681488955
ROC train: 1.000000	val: 0.657143	test: 0.790297
PRC train: 1.000000	val: 0.929361	test: 0.795179

Epoch: 102
Loss: 0.07255985066051222
ROC train: 1.000000	val: 0.663370	test: 0.772561
PRC train: 1.000000	val: 0.930621	test: 0.778567

Epoch: 103
Loss: 0.0637267313914029
ROC train: 1.000000	val: 0.655311	test: 0.759520
PRC train: 1.000000	val: 0.928287	test: 0.769194

Epoch: 104
Loss: 0.08370517012861436
ROC train: 1.000000	val: 0.656410	test: 0.757781
PRC train: 1.000000	val: 0.927306	test: 0.760832

Epoch: 105
Loss: 0.0771678767610221
ROC train: 1.000000	val: 0.664103	test: 0.771692
PRC train: 1.000000	val: 0.929447	test: 0.773976

Epoch: 106
Loss: 0.06624359926987894
ROC train: 1.000000	val: 0.660073	test: 0.773431
PRC train: 1.000000	val: 0.927546	test: 0.776257

Epoch: 107
Loss: 0.07627928837890866
ROC train: 1.000000	val: 0.651648	test: 0.780734
PRC train: 1.000000	val: 0.923577	test: 0.782014

Epoch: 108
Loss: 0.09219955141877169
ROC train: 1.000000	val: 0.631136	test: 0.784385
PRC train: 1.000000	val: 0.918598	test: 0.789066

Epoch: 109
Loss: 0.05400946740591296
ROC train: 1.000000	val: 0.630037	test: 0.766475
PRC train: 1.000000	val: 0.918074	test: 0.767531

Epoch: 110
Loss: 0.08417466092089118
ROC train: 1.000000	val: 0.649084	test: 0.773605
PRC train: 1.000000	val: 0.924029	test: 0.771892

Epoch: 111
Loss: 0.06824306611881419
ROC train: 1.000000	val: 0.665568	test: 0.784559
PRC train: 1.000000	val: 0.931036	test: 0.783901

Epoch: 112
Loss: 0.060345256505172876
ROC train: 1.000000	val: 0.666667	test: 0.791167
PRC train: 1.000000	val: 0.930026	test: 0.788837

Epoch: 113
Loss: 0.052742425147875104
ROC train: 1.000000	val: 0.663736	test: 0.771344
PRC train: 1.000000	val: 0.928690	test: 0.771520

Epoch: 114
Loss: 0.08095620099843122
ROC train: 1.000000	val: 0.649451	test: 0.748392
PRC train: 1.000000	val: 0.923532	test: 0.754242

Epoch: 115
Loss: 0.0512530130067536
ROC train: 1.000000	val: 0.639560	test: 0.740567
PRC train: 1.000000	val: 0.920170	test: 0.744174

Epoch: 116
Loss: 0.052547062630495425
ROC train: 1.000000	val: 0.666300	test: 0.757260
PRC train: 1.000000	val: 0.928834	test: 0.762046

Epoch: 117
Loss: 0.06235275711239174
ROC train: 1.000000	val: 0.676190	test: 0.760911
PRC train: 1.000000	val: 0.929271	test: 0.773715

Epoch: 118
Loss: 0.06340596396135717
ROC train: 1.000000	val: 0.671429	test: 0.769779
PRC train: 1.000000	val: 0.929178	test: 0.777516

Epoch: 119
Loss: 0.06602635189066638
ROC train: 1.000000	val: 0.671062	test: 0.778299
PRC train: 1.000000	val: 0.931009	test: 0.779989

Epoch: 120
Loss: 0.05113270070280115
ROC train: 1.000000	val: 0.649817	test: 0.782473
PRC train: 1.000000	val: 0.923065	test: 0.779552

Early stopping
Best (ROC):	 train: 0.994949	val: 0.716850	test: 0.805425
Best (PRC):	 train: 0.993010	val: 0.941102	test: 0.816578

PRC train: 1.000000	val: 0.942756	test: 0.770952

Epoch: 94
Loss: 0.060121859408898935
ROC train: 1.000000	val: 0.687912	test: 0.746827
PRC train: 1.000000	val: 0.940547	test: 0.742068

Epoch: 95
Loss: 0.062443738690105756
ROC train: 1.000000	val: 0.668498	test: 0.748565
PRC train: 1.000000	val: 0.935630	test: 0.739361

Epoch: 96
Loss: 0.05649758301547758
ROC train: 1.000000	val: 0.690110	test: 0.750826
PRC train: 1.000000	val: 0.939785	test: 0.737711

Epoch: 97
Loss: 0.07639742409058041
ROC train: 1.000000	val: 0.691941	test: 0.755347
PRC train: 1.000000	val: 0.941499	test: 0.733837

Epoch: 98
Loss: 0.05435034414832003
ROC train: 1.000000	val: 0.692674	test: 0.751695
PRC train: 1.000000	val: 0.943479	test: 0.751754

Epoch: 99
Loss: 0.057006431901813724
ROC train: 1.000000	val: 0.720513	test: 0.752913
PRC train: 1.000000	val: 0.948739	test: 0.738565

Epoch: 100
Loss: 0.06424714352936008
ROC train: 1.000000	val: 0.716117	test: 0.757781
PRC train: 1.000000	val: 0.945269	test: 0.742801

Epoch: 101
Loss: 0.0528591575913116
ROC train: 1.000000	val: 0.702564	test: 0.759694
PRC train: 1.000000	val: 0.940972	test: 0.733434

Epoch: 102
Loss: 0.05722862405110039
ROC train: 1.000000	val: 0.692308	test: 0.767345
PRC train: 1.000000	val: 0.940729	test: 0.737541

Epoch: 103
Loss: 0.05614163766738506
ROC train: 1.000000	val: 0.711355	test: 0.768736
PRC train: 1.000000	val: 0.946062	test: 0.748240

Epoch: 104
Loss: 0.04595582790171291
ROC train: 1.000000	val: 0.720513	test: 0.768562
PRC train: 1.000000	val: 0.949190	test: 0.760007

Epoch: 105
Loss: 0.06577010671321062
ROC train: 1.000000	val: 0.715751	test: 0.761259
PRC train: 1.000000	val: 0.947830	test: 0.744299

Epoch: 106
Loss: 0.06238876486578436
ROC train: 1.000000	val: 0.708425	test: 0.773083
PRC train: 1.000000	val: 0.944742	test: 0.750230

Epoch: 107
Loss: 0.04517066601526911
ROC train: 1.000000	val: 0.700733	test: 0.787515
PRC train: 1.000000	val: 0.942455	test: 0.769415

Epoch: 108
Loss: 0.04700501146609319
ROC train: 1.000000	val: 0.711722	test: 0.782820
PRC train: 1.000000	val: 0.944826	test: 0.762997

Epoch: 109
Loss: 0.0481073165827797
ROC train: 1.000000	val: 0.723443	test: 0.768388
PRC train: 1.000000	val: 0.948439	test: 0.751941

Epoch: 110
Loss: 0.05269620288327119
ROC train: 1.000000	val: 0.719414	test: 0.772040
PRC train: 1.000000	val: 0.948846	test: 0.765019

Epoch: 111
Loss: 0.043564726218039695
ROC train: 1.000000	val: 0.712088	test: 0.767693
PRC train: 1.000000	val: 0.948668	test: 0.747468

Epoch: 112
Loss: 0.03648269648473716
ROC train: 1.000000	val: 0.691575	test: 0.772040
PRC train: 1.000000	val: 0.943297	test: 0.751180

Epoch: 113
Loss: 0.042749617625196905
ROC train: 0.999997	val: 0.687179	test: 0.761607
PRC train: 0.999996	val: 0.941502	test: 0.744520

Epoch: 114
Loss: 0.049350997653166695
ROC train: 1.000000	val: 0.693773	test: 0.750130
PRC train: 1.000000	val: 0.942297	test: 0.735690

Epoch: 115
Loss: 0.04316975318647916
ROC train: 1.000000	val: 0.690476	test: 0.748218
PRC train: 1.000000	val: 0.940971	test: 0.738891

Epoch: 116
Loss: 0.028054381216705865
ROC train: 1.000000	val: 0.691575	test: 0.752913
PRC train: 1.000000	val: 0.940690	test: 0.745915

Epoch: 117
Loss: 0.03241764440658054
ROC train: 1.000000	val: 0.663736	test: 0.755869
PRC train: 1.000000	val: 0.932764	test: 0.746283

Epoch: 118
Loss: 0.039523681330171596
ROC train: 1.000000	val: 0.665568	test: 0.760389
PRC train: 1.000000	val: 0.934107	test: 0.747972

Epoch: 119
Loss: 0.0385548108352841
ROC train: 1.000000	val: 0.677656	test: 0.761259
PRC train: 1.000000	val: 0.937498	test: 0.751477

Epoch: 120
Loss: 0.0289704913828348
ROC train: 1.000000	val: 0.675824	test: 0.753434
PRC train: 1.000000	val: 0.937161	test: 0.743239

Early stopping
Best (ROC):	 train: 0.985006	val: 0.745421	test: 0.776213
Best (PRC):	 train: 0.974901	val: 0.954896	test: 0.740587

PRC train: 1.000000	val: 0.908454	test: 0.727277

Epoch: 94
Loss: 0.0537794580497437
ROC train: 1.000000	val: 0.674725	test: 0.731177
PRC train: 1.000000	val: 0.921404	test: 0.717115

Epoch: 95
Loss: 0.05289508421864986
ROC train: 1.000000	val: 0.667766	test: 0.725439
PRC train: 1.000000	val: 0.925764	test: 0.697126

Epoch: 96
Loss: 0.06228783621725155
ROC train: 1.000000	val: 0.640659	test: 0.726656
PRC train: 1.000000	val: 0.916680	test: 0.694837

Epoch: 97
Loss: 0.060898126100458264
ROC train: 1.000000	val: 0.628205	test: 0.725091
PRC train: 1.000000	val: 0.894339	test: 0.720658

Epoch: 98
Loss: 0.049939681262829806
ROC train: 1.000000	val: 0.658608	test: 0.717267
PRC train: 1.000000	val: 0.911184	test: 0.708474

Epoch: 99
Loss: 0.06316489862482418
ROC train: 1.000000	val: 0.677289	test: 0.724048
PRC train: 1.000000	val: 0.925375	test: 0.699202

Epoch: 100
Loss: 0.05677586297681449
ROC train: 1.000000	val: 0.662271	test: 0.720223
PRC train: 1.000000	val: 0.921865	test: 0.681070

Epoch: 101
Loss: 0.04983977708791073
ROC train: 1.000000	val: 0.659341	test: 0.725961
PRC train: 1.000000	val: 0.915748	test: 0.712989

Epoch: 102
Loss: 0.047018638647813756
ROC train: 1.000000	val: 0.644689	test: 0.724917
PRC train: 1.000000	val: 0.909541	test: 0.728480

Epoch: 103
Loss: 0.06624867254530695
ROC train: 1.000000	val: 0.670330	test: 0.734481
PRC train: 1.000000	val: 0.921311	test: 0.736149

Epoch: 104
Loss: 0.03958718925024603
ROC train: 1.000000	val: 0.683516	test: 0.733785
PRC train: 1.000000	val: 0.930652	test: 0.718497

Epoch: 105
Loss: 0.046984242395603545
ROC train: 1.000000	val: 0.686081	test: 0.739176
PRC train: 1.000000	val: 0.930263	test: 0.736937

Epoch: 106
Loss: 0.05914902716930056
ROC train: 1.000000	val: 0.673626	test: 0.737089
PRC train: 1.000000	val: 0.925025	test: 0.730039

Epoch: 107
Loss: 0.039068565626813205
ROC train: 1.000000	val: 0.670696	test: 0.728917
PRC train: 1.000000	val: 0.927961	test: 0.712113

Epoch: 108
Loss: 0.03852297138932796
ROC train: 1.000000	val: 0.678755	test: 0.708920
PRC train: 1.000000	val: 0.932557	test: 0.695153

Epoch: 109
Loss: 0.04652656722712354
ROC train: 1.000000	val: 0.676557	test: 0.711355
PRC train: 1.000000	val: 0.934034	test: 0.681044

Epoch: 110
Loss: 0.03761434381856615
ROC train: 1.000000	val: 0.673626	test: 0.714484
PRC train: 1.000000	val: 0.931781	test: 0.685947

Epoch: 111
Loss: 0.04481557564222638
ROC train: 1.000000	val: 0.672527	test: 0.726308
PRC train: 1.000000	val: 0.929674	test: 0.691423

Epoch: 112
Loss: 0.0390275077923036
ROC train: 1.000000	val: 0.664103	test: 0.732394
PRC train: 1.000000	val: 0.919590	test: 0.713755

Epoch: 113
Loss: 0.06084704587528524
ROC train: 1.000000	val: 0.673626	test: 0.725961
PRC train: 1.000000	val: 0.925094	test: 0.708834

Epoch: 114
Loss: 0.05203742844073922
ROC train: 1.000000	val: 0.682051	test: 0.722135
PRC train: 1.000000	val: 0.931473	test: 0.702241

Epoch: 115
Loss: 0.0406751680146268
ROC train: 1.000000	val: 0.679121	test: 0.723179
PRC train: 1.000000	val: 0.929508	test: 0.706515

Epoch: 116
Loss: 0.04128231348336249
ROC train: 1.000000	val: 0.659341	test: 0.732220
PRC train: 1.000000	val: 0.912122	test: 0.724831

Epoch: 117
Loss: 0.03740029692073273
ROC train: 1.000000	val: 0.669231	test: 0.749609
PRC train: 1.000000	val: 0.916178	test: 0.739863

Epoch: 118
Loss: 0.035222496676269664
ROC train: 1.000000	val: 0.692674	test: 0.750304
PRC train: 1.000000	val: 0.929105	test: 0.732163

Epoch: 119
Loss: 0.024770598421407376
ROC train: 1.000000	val: 0.696337	test: 0.740741
PRC train: 1.000000	val: 0.933263	test: 0.733263

Epoch: 120
Loss: 0.046099060222067646
ROC train: 1.000000	val: 0.681319	test: 0.739524
PRC train: 1.000000	val: 0.928530	test: 0.728017

Early stopping
Best (ROC):	 train: 0.993059	val: 0.696703	test: 0.699531
Best (PRC):	 train: 0.988115	val: 0.932444	test: 0.672215

PRC train: 1.000000	val: 0.886446	test: 0.699547

Epoch: 94
Loss: 0.04170651064141743
ROC train: 1.000000	val: 0.578022	test: 0.741784
PRC train: 1.000000	val: 0.883361	test: 0.711130

Epoch: 95
Loss: 0.06975236456696218
ROC train: 1.000000	val: 0.587179	test: 0.743175
PRC train: 1.000000	val: 0.891605	test: 0.693877

Epoch: 96
Loss: 0.051849851407581006
ROC train: 1.000000	val: 0.595604	test: 0.745436
PRC train: 1.000000	val: 0.897735	test: 0.697780

Epoch: 97
Loss: 0.06691934840662063
ROC train: 1.000000	val: 0.592308	test: 0.738306
PRC train: 1.000000	val: 0.900288	test: 0.699846

Epoch: 98
Loss: 0.04860345893294095
ROC train: 1.000000	val: 0.587912	test: 0.712224
PRC train: 1.000000	val: 0.900077	test: 0.674980

Epoch: 99
Loss: 0.05277573167599252
ROC train: 1.000000	val: 0.608059	test: 0.738828
PRC train: 1.000000	val: 0.897462	test: 0.681409

Epoch: 100
Loss: 0.04466740727074806
ROC train: 1.000000	val: 0.599634	test: 0.745436
PRC train: 1.000000	val: 0.892493	test: 0.707015

Epoch: 101
Loss: 0.04965039015765641
ROC train: 1.000000	val: 0.621245	test: 0.736394
PRC train: 1.000000	val: 0.903995	test: 0.717181

Epoch: 102
Loss: 0.05967813826010461
ROC train: 1.000000	val: 0.620513	test: 0.739871
PRC train: 1.000000	val: 0.900627	test: 0.704755

Epoch: 103
Loss: 0.040666675626299595
ROC train: 1.000000	val: 0.617949	test: 0.744566
PRC train: 1.000000	val: 0.904640	test: 0.680248

Epoch: 104
Loss: 0.03865978304521024
ROC train: 1.000000	val: 0.638828	test: 0.741610
PRC train: 1.000000	val: 0.915660	test: 0.690184

Epoch: 105
Loss: 0.03423026876733991
ROC train: 1.000000	val: 0.640293	test: 0.744566
PRC train: 1.000000	val: 0.917135	test: 0.681741

Epoch: 106
Loss: 0.05095915350372018
ROC train: 1.000000	val: 0.631136	test: 0.746305
PRC train: 1.000000	val: 0.915030	test: 0.685254

Epoch: 107
Loss: 0.03604610470062329
ROC train: 1.000000	val: 0.616117	test: 0.747522
PRC train: 1.000000	val: 0.906195	test: 0.682824

Epoch: 108
Loss: 0.0529143863524981
ROC train: 1.000000	val: 0.617949	test: 0.750826
PRC train: 1.000000	val: 0.907419	test: 0.698637

Epoch: 109
Loss: 0.04208043607651729
ROC train: 1.000000	val: 0.621612	test: 0.748218
PRC train: 1.000000	val: 0.911669	test: 0.710252

Epoch: 110
Loss: 0.038334025688675485
ROC train: 1.000000	val: 0.601465	test: 0.744045
PRC train: 1.000000	val: 0.908522	test: 0.709756

Epoch: 111
Loss: 0.04508326147623567
ROC train: 1.000000	val: 0.567399	test: 0.747522
PRC train: 1.000000	val: 0.902319	test: 0.697420

Epoch: 112
Loss: 0.04822254399571289
ROC train: 1.000000	val: 0.544689	test: 0.750130
PRC train: 1.000000	val: 0.892351	test: 0.687697

Epoch: 113
Loss: 0.06166802182303064
ROC train: 1.000000	val: 0.544322	test: 0.754477
PRC train: 1.000000	val: 0.888920	test: 0.686861

Epoch: 114
Loss: 0.05385549220779431
ROC train: 1.000000	val: 0.575824	test: 0.743523
PRC train: 1.000000	val: 0.900807	test: 0.688908

Epoch: 115
Loss: 0.05173742218154269
ROC train: 1.000000	val: 0.596703	test: 0.730482
PRC train: 1.000000	val: 0.903289	test: 0.696627

Epoch: 116
Loss: 0.045174103323094926
ROC train: 1.000000	val: 0.605128	test: 0.741784
PRC train: 1.000000	val: 0.908193	test: 0.701983

Epoch: 117
Loss: 0.04433115125520852
ROC train: 1.000000	val: 0.590110	test: 0.741784
PRC train: 1.000000	val: 0.902762	test: 0.695886

Epoch: 118
Loss: 0.04376032923741042
ROC train: 1.000000	val: 0.594872	test: 0.743523
PRC train: 1.000000	val: 0.903277	test: 0.713596

Epoch: 119
Loss: 0.05296417990557868
ROC train: 1.000000	val: 0.595604	test: 0.748392
PRC train: 1.000000	val: 0.900513	test: 0.706844

Epoch: 120
Loss: 0.0464155484934897
ROC train: 1.000000	val: 0.622711	test: 0.758303
PRC train: 1.000000	val: 0.906535	test: 0.708297

Early stopping
Best (ROC):	 train: 0.854780	val: 0.716484	test: 0.635889
Best (PRC):	 train: 0.773699	val: 0.946489	test: 0.613618

PRC train: 0.977973	val: 0.913797	test: 0.711084

Epoch: 95
Loss: 0.2107021940619572
ROC train: 0.984215	val: 0.647985	test: 0.709442
PRC train: 0.976565	val: 0.910680	test: 0.700319

Epoch: 96
Loss: 0.21901022298902695
ROC train: 0.985197	val: 0.665568	test: 0.713789
PRC train: 0.978199	val: 0.915085	test: 0.703517

Epoch: 97
Loss: 0.19126765785193175
ROC train: 0.984766	val: 0.663736	test: 0.709268
PRC train: 0.977597	val: 0.916407	test: 0.704567

Epoch: 98
Loss: 0.2200853658792868
ROC train: 0.985656	val: 0.661172	test: 0.717788
PRC train: 0.979519	val: 0.914826	test: 0.719637

Epoch: 99
Loss: 0.20491301301147416
ROC train: 0.983122	val: 0.655678	test: 0.723005
PRC train: 0.975742	val: 0.914027	test: 0.717633

Epoch: 100
Loss: 0.2078772769574182
ROC train: 0.984118	val: 0.656410	test: 0.704225
PRC train: 0.977580	val: 0.916147	test: 0.705784

Epoch: 101
Loss: 0.2210130722301027
ROC train: 0.985400	val: 0.658974	test: 0.701269
PRC train: 0.978884	val: 0.916055	test: 0.702834

Epoch: 102
Loss: 0.19799363284846797
ROC train: 0.983051	val: 0.677656	test: 0.683881
PRC train: 0.975129	val: 0.919015	test: 0.689132

Epoch: 103
Loss: 0.2279813156753196
ROC train: 0.982914	val: 0.665934	test: 0.685620
PRC train: 0.974925	val: 0.915547	test: 0.693223

Epoch: 104
Loss: 0.22876708698443665
ROC train: 0.980265	val: 0.638462	test: 0.703008
PRC train: 0.970445	val: 0.912691	test: 0.700983

Epoch: 105
Loss: 0.21265397806998387
ROC train: 0.985976	val: 0.658242	test: 0.706834
PRC train: 0.979227	val: 0.919886	test: 0.707361

Epoch: 106
Loss: 0.21970536843450889
ROC train: 0.987160	val: 0.672894	test: 0.721266
PRC train: 0.981073	val: 0.916748	test: 0.724413

Epoch: 107
Loss: 0.210207216308454
ROC train: 0.987554	val: 0.675824	test: 0.718136
PRC train: 0.981699	val: 0.917854	test: 0.723010

Epoch: 108
Loss: 0.20751797568252445
ROC train: 0.988142	val: 0.683883	test: 0.705790
PRC train: 0.982817	val: 0.928102	test: 0.704580

Epoch: 109
Loss: 0.20422666685071422
ROC train: 0.987714	val: 0.683516	test: 0.697966
PRC train: 0.982014	val: 0.924839	test: 0.691167

Epoch: 110
Loss: 0.19858326961139963
ROC train: 0.987203	val: 0.676557	test: 0.690141
PRC train: 0.981394	val: 0.926506	test: 0.681823

Epoch: 111
Loss: 0.19786946968805963
ROC train: 0.986989	val: 0.669597	test: 0.688750
PRC train: 0.981259	val: 0.923331	test: 0.679011

Epoch: 112
Loss: 0.19696207216186754
ROC train: 0.986835	val: 0.660073	test: 0.703878
PRC train: 0.980854	val: 0.919558	test: 0.698413

Epoch: 113
Loss: 0.18899337833569035
ROC train: 0.989267	val: 0.661538	test: 0.720396
PRC train: 0.984694	val: 0.918997	test: 0.718612

Epoch: 114
Loss: 0.19758608212585674
ROC train: 0.988456	val: 0.655678	test: 0.699531
PRC train: 0.983497	val: 0.916293	test: 0.707259

Epoch: 115
Loss: 0.19576845434316137
ROC train: 0.986972	val: 0.651282	test: 0.684229
PRC train: 0.981337	val: 0.914046	test: 0.686882

Epoch: 116
Loss: 0.20683097436524403
ROC train: 0.985987	val: 0.665201	test: 0.693445
PRC train: 0.979882	val: 0.919273	test: 0.696228

Epoch: 117
Loss: 0.20314790394464716
ROC train: 0.987914	val: 0.679853	test: 0.714311
PRC train: 0.982641	val: 0.924290	test: 0.704568

Epoch: 118
Loss: 0.20305423442915563
ROC train: 0.989235	val: 0.679853	test: 0.733090
PRC train: 0.984202	val: 0.920648	test: 0.720197

Epoch: 119
Loss: 0.1971075684933222
ROC train: 0.989164	val: 0.662637	test: 0.715528
PRC train: 0.984373	val: 0.913357	test: 0.703350

Epoch: 120
Loss: 0.19079502979407273
ROC train: 0.988696	val: 0.675092	test: 0.702834
PRC train: 0.983670	val: 0.923414	test: 0.694130

Epoch: 121
Loss: 0.20273745317183606
ROC train: 0.989218	val: 0.662271	test: 0.699357
PRC train: 0.984777	val: 0.918643	test: 0.690173

Epoch: 122
Loss: 0.20792163387661305
ROC train: 0.987175	val: 0.665201	test: 0.700226
PRC train: 0.980949	val: 0.912887	test: 0.690240

Epoch: 123
Loss: 0.1932516774438307
ROC train: 0.988245	val: 0.631136	test: 0.714484
PRC train: 0.983278	val: 0.908158	test: 0.704087

Epoch: 124
Loss: 0.1785245120687711
ROC train: 0.986998	val: 0.642857	test: 0.699878
PRC train: 0.981484	val: 0.912152	test: 0.682445

Epoch: 125
Loss: 0.17587336726762964
ROC train: 0.988096	val: 0.670696	test: 0.684750
PRC train: 0.982871	val: 0.916979	test: 0.658775

Epoch: 126
Loss: 0.1992340428893866
ROC train: 0.991650	val: 0.657509	test: 0.698139
PRC train: 0.988401	val: 0.918100	test: 0.682446

Epoch: 127
Loss: 0.18662986404719237
ROC train: 0.990634	val: 0.661172	test: 0.706486
PRC train: 0.986473	val: 0.916440	test: 0.705976

Epoch: 128
Loss: 0.18994029265026444
ROC train: 0.989438	val: 0.660440	test: 0.699878
PRC train: 0.984721	val: 0.916844	test: 0.697066

Epoch: 129
Loss: 0.18517887243961959
ROC train: 0.990049	val: 0.670696	test: 0.692923
PRC train: 0.986037	val: 0.921695	test: 0.685566

Epoch: 130
Loss: 0.1887361478440171
ROC train: 0.990628	val: 0.682051	test: 0.689793
PRC train: 0.986926	val: 0.926952	test: 0.675955

Epoch: 131
Loss: 0.16938289548038862
ROC train: 0.990619	val: 0.669963	test: 0.707355
PRC train: 0.986922	val: 0.925211	test: 0.690978

Epoch: 132
Loss: 0.1729966854645618
ROC train: 0.990108	val: 0.664835	test: 0.715875
PRC train: 0.986216	val: 0.923828	test: 0.698818

Epoch: 133
Loss: 0.17785455893533847
ROC train: 0.990154	val: 0.672161	test: 0.702139
PRC train: 0.986194	val: 0.920677	test: 0.690068

Epoch: 134
Loss: 0.1740585518843478
ROC train: 0.991142	val: 0.673260	test: 0.699357
PRC train: 0.987762	val: 0.920031	test: 0.684406

Epoch: 135
Loss: 0.1745764227167087
ROC train: 0.990111	val: 0.679853	test: 0.693966
PRC train: 0.986067	val: 0.922335	test: 0.678801

Epoch: 136
Loss: 0.1668014926552595
ROC train: 0.989184	val: 0.664103	test: 0.698835
PRC train: 0.983859	val: 0.916823	test: 0.689578

Epoch: 137
Loss: 0.1768594845137734
ROC train: 0.990648	val: 0.661905	test: 0.713093
PRC train: 0.986002	val: 0.923176	test: 0.701587

Epoch: 138
Loss: 0.19087221559710613
ROC train: 0.991267	val: 0.678755	test: 0.711528
PRC train: 0.987357	val: 0.925382	test: 0.692658

Epoch: 139
Loss: 0.17501173701077866
ROC train: 0.988182	val: 0.675458	test: 0.701269
PRC train: 0.983195	val: 0.922828	test: 0.685794

Epoch: 140
Loss: 0.1815876959720068
ROC train: 0.989475	val: 0.668864	test: 0.703878
PRC train: 0.984924	val: 0.920246	test: 0.684978

Epoch: 141
Loss: 0.17342961503132145
ROC train: 0.991824	val: 0.671429	test: 0.693792
PRC train: 0.988431	val: 0.921385	test: 0.675952

Epoch: 142
Loss: 0.17131974232469716
ROC train: 0.990006	val: 0.681685	test: 0.685968
PRC train: 0.985672	val: 0.921666	test: 0.672562

Epoch: 143
Loss: 0.181268407292838
ROC train: 0.992129	val: 0.692674	test: 0.709442
PRC train: 0.988729	val: 0.932437	test: 0.691007

Epoch: 144
Loss: 0.16482727946005762
ROC train: 0.992243	val: 0.698168	test: 0.711702
PRC train: 0.988947	val: 0.934824	test: 0.696932

Epoch: 145
Loss: 0.18187151899633602
ROC train: 0.993071	val: 0.699634	test: 0.709094
PRC train: 0.990455	val: 0.931231	test: 0.693289

Epoch: 146
Loss: 0.1769030324824419
ROC train: 0.991136	val: 0.686813	test: 0.709094
PRC train: 0.987541	val: 0.921679	test: 0.696223

Epoch: 147
Loss: 0.16518461602988035
ROC train: 0.988005	val: 0.683150	test: 0.714311
PRC train: 0.981674	val: 0.915323	test: 0.711526

Epoch: 148
Loss: 0.1594554702528623
ROC train: 0.991741	val: 0.666667	test: 0.712398
PRC train: 0.988052	val: 0.919315	test: 0.715518

Epoch: 149
Loss: 0.18168259104577228
ROC train: 0.990830	val: 0.676557	test: 0.706486
PRC train: 0.986988	val: 0.923816	test: 0.710035

Epoch: 150
Loss: 0.1680121457162205
ROC train: 0.990928	val: 0.667399	test: 0.695705
PRC train: 0.987241	val: 0.921328	test: 0.698424

Epoch: 151
Loss: 0.17325603260753322
ROC train: 0.991575	val: 0.658608	test: 0.706834
PRC train: 0.987891	val: 0.918154	test: 0.708891

Epoch: 152
Loss: 0.1789379879010123
ROC train: 0.992997	val: 0.663004	test: 0.713963
PRC train: 0.989963	val: 0.921690	test: 0.708232

Epoch: 153
Loss: 0.16856915880341466
ROC train: 0.993933	val: 0.674725	test: 0.713093
PRC train: 0.991356	val: 0.923915	test: 0.713244

Epoch: 154
Loss: 0.17135653353975727
ROC train: 0.991946	val: 0.684249	test: 0.707529
PRC train: 0.988335	val: 0.927140	test: 0.708623

Epoch: 155
Loss: 0.16421601185417828
PRC train: 1.000000	val: 0.935704	test: 0.793954

Epoch: 94
Loss: 0.0628849224071551
ROC train: 0.999943	val: 0.701099	test: 0.806121
PRC train: 0.999912	val: 0.937515	test: 0.790408

Epoch: 95
Loss: 0.07113541718640706
ROC train: 1.000000	val: 0.721245	test: 0.809251
PRC train: 1.000000	val: 0.943756	test: 0.792143

Epoch: 96
Loss: 0.07664654334581278
ROC train: 1.000000	val: 0.718681	test: 0.822118
PRC train: 1.000000	val: 0.943694	test: 0.804125

Epoch: 97
Loss: 0.06790799528813565
ROC train: 1.000000	val: 0.712454	test: 0.824031
PRC train: 1.000000	val: 0.942796	test: 0.814406

Epoch: 98
Loss: 0.0615247636856597
ROC train: 1.000000	val: 0.709158	test: 0.792384
PRC train: 1.000000	val: 0.943321	test: 0.787801

Epoch: 99
Loss: 0.06745822562270622
ROC train: 1.000000	val: 0.711722	test: 0.794297
PRC train: 1.000000	val: 0.943742	test: 0.790431

Epoch: 100
Loss: 0.05850962006296617
ROC train: 1.000000	val: 0.719048	test: 0.818466
PRC train: 1.000000	val: 0.945586	test: 0.803670

Epoch: 101
Loss: 0.05981654248683974
ROC train: 1.000000	val: 0.708425	test: 0.829421
PRC train: 1.000000	val: 0.943175	test: 0.812284

Epoch: 102
Loss: 0.06425924696379393
ROC train: 1.000000	val: 0.698168	test: 0.811337
PRC train: 1.000000	val: 0.937878	test: 0.799054

Epoch: 103
Loss: 0.06399637242034942
ROC train: 1.000000	val: 0.717216	test: 0.791515
PRC train: 1.000000	val: 0.941326	test: 0.783998

Epoch: 104
Loss: 0.04221923735455381
ROC train: 1.000000	val: 0.721245	test: 0.818466
PRC train: 1.000000	val: 0.943811	test: 0.818082

Epoch: 105
Loss: 0.05643913688299388
ROC train: 1.000000	val: 0.717582	test: 0.821075
PRC train: 1.000000	val: 0.943712	test: 0.816124

Epoch: 106
Loss: 0.04720840736428776
ROC train: 1.000000	val: 0.716117	test: 0.804034
PRC train: 1.000000	val: 0.943317	test: 0.790345

Epoch: 107
Loss: 0.06628872494652958
ROC train: 0.999997	val: 0.715751	test: 0.800383
PRC train: 0.999996	val: 0.943645	test: 0.788276

Epoch: 108
Loss: 0.04502824115230688
ROC train: 0.999977	val: 0.698901	test: 0.812033
PRC train: 0.999966	val: 0.938574	test: 0.802575

Epoch: 109
Loss: 0.05173461666509392
ROC train: 1.000000	val: 0.709890	test: 0.811163
PRC train: 1.000000	val: 0.940739	test: 0.795935

Epoch: 110
Loss: 0.0390242012073874
ROC train: 1.000000	val: 0.727106	test: 0.802295
PRC train: 1.000000	val: 0.945268	test: 0.788504

Epoch: 111
Loss: 0.056932538483471073
ROC train: 1.000000	val: 0.726740	test: 0.807512
PRC train: 1.000000	val: 0.945497	test: 0.791367

Epoch: 112
Loss: 0.03780649649494958
ROC train: 1.000000	val: 0.718681	test: 0.802295
PRC train: 1.000000	val: 0.944188	test: 0.787723

Epoch: 113
Loss: 0.050141259975337735
ROC train: 1.000000	val: 0.707692	test: 0.797253
PRC train: 1.000000	val: 0.940358	test: 0.780621

Epoch: 114
Loss: 0.04330236488236855
ROC train: 1.000000	val: 0.699267	test: 0.804556
PRC train: 1.000000	val: 0.938064	test: 0.783516

Epoch: 115
Loss: 0.03693091313674428
ROC train: 1.000000	val: 0.701832	test: 0.813772
PRC train: 1.000000	val: 0.938618	test: 0.793178

Epoch: 116
Loss: 0.05250827050291855
ROC train: 1.000000	val: 0.724176	test: 0.819684
PRC train: 1.000000	val: 0.945079	test: 0.801979

Epoch: 117
Loss: 0.055302719732871056
ROC train: 1.000000	val: 0.719048	test: 0.817597
PRC train: 1.000000	val: 0.943709	test: 0.804488

Epoch: 118
Loss: 0.041130091340643854
ROC train: 1.000000	val: 0.715385	test: 0.805599
PRC train: 1.000000	val: 0.940434	test: 0.795242

Epoch: 119
Loss: 0.05715955227126525
ROC train: 1.000000	val: 0.709158	test: 0.817249
PRC train: 1.000000	val: 0.936601	test: 0.807191

Epoch: 120
Loss: 0.04185149057958767
ROC train: 1.000000	val: 0.716484	test: 0.833073
PRC train: 1.000000	val: 0.941597	test: 0.816844

Epoch: 121
Loss: 0.04929295447217971
ROC train: 1.000000	val: 0.705128	test: 0.830638
PRC train: 1.000000	val: 0.939373	test: 0.813904

Epoch: 122
Loss: 0.05950116887589226
ROC train: 1.000000	val: 0.704762	test: 0.820901
PRC train: 1.000000	val: 0.938876	test: 0.804127

Epoch: 123
Loss: 0.0404490461821536
ROC train: 1.000000	val: 0.717582	test: 0.815684
PRC train: 1.000000	val: 0.943409	test: 0.800676

Epoch: 124
Loss: 0.03973351301404108
ROC train: 1.000000	val: 0.713187	test: 0.808729
PRC train: 1.000000	val: 0.942192	test: 0.795749

Epoch: 125
Loss: 0.04455235448951448
ROC train: 1.000000	val: 0.694505	test: 0.814641
PRC train: 1.000000	val: 0.937640	test: 0.797852

Epoch: 126
Loss: 0.036713734711171876
ROC train: 1.000000	val: 0.691941	test: 0.815684
PRC train: 1.000000	val: 0.938611	test: 0.802434

Epoch: 127
Loss: 0.052711913087775675
ROC train: 1.000000	val: 0.707326	test: 0.820379
PRC train: 1.000000	val: 0.942795	test: 0.807128

Epoch: 128
Loss: 0.03349257414798968
ROC train: 1.000000	val: 0.719048	test: 0.812207
PRC train: 1.000000	val: 0.944379	test: 0.801446

Epoch: 129
Loss: 0.032673627157383964
ROC train: 1.000000	val: 0.716850	test: 0.804208
PRC train: 1.000000	val: 0.943133	test: 0.792840

Epoch: 130
Loss: 0.04085519485463398
ROC train: 1.000000	val: 0.702930	test: 0.803339
PRC train: 1.000000	val: 0.936326	test: 0.795022

Epoch: 131
Loss: 0.04602285697327061
ROC train: 1.000000	val: 0.701465	test: 0.802121
PRC train: 1.000000	val: 0.937563	test: 0.791342

Epoch: 132
Loss: 0.029735863330942247
ROC train: 1.000000	val: 0.702930	test: 0.798296
PRC train: 1.000000	val: 0.939329	test: 0.779300

Epoch: 133
Loss: 0.04259101179069895
ROC train: 1.000000	val: 0.708791	test: 0.797253
PRC train: 1.000000	val: 0.939585	test: 0.777926

Epoch: 134
Loss: 0.028281337097594318
ROC train: 1.000000	val: 0.722344	test: 0.803339
PRC train: 1.000000	val: 0.943536	test: 0.788462

Epoch: 135
Loss: 0.02922627650722165
ROC train: 1.000000	val: 0.713553	test: 0.810120
PRC train: 1.000000	val: 0.942125	test: 0.793620

Epoch: 136
Loss: 0.029459027294863986
ROC train: 1.000000	val: 0.722711	test: 0.816728
PRC train: 1.000000	val: 0.944287	test: 0.804108

Epoch: 137
Loss: 0.03008502436322184
ROC train: 1.000000	val: 0.717216	test: 0.816380
PRC train: 1.000000	val: 0.944360	test: 0.804845

Epoch: 138
Loss: 0.028075011716501563
ROC train: 1.000000	val: 0.708059	test: 0.813945
PRC train: 1.000000	val: 0.942299	test: 0.803559

Epoch: 139
Loss: 0.02701450049331392
ROC train: 1.000000	val: 0.704029	test: 0.813598
PRC train: 1.000000	val: 0.942216	test: 0.798109

Epoch: 140
Loss: 0.03166197399674288
ROC train: 1.000000	val: 0.694872	test: 0.816901
PRC train: 1.000000	val: 0.939614	test: 0.796979

Epoch: 141
Loss: 0.02303102462533707
ROC train: 1.000000	val: 0.698535	test: 0.800209
PRC train: 1.000000	val: 0.940682	test: 0.779887

Epoch: 142
Loss: 0.03238266217433074
ROC train: 1.000000	val: 0.708425	test: 0.801600
PRC train: 1.000000	val: 0.942245	test: 0.780965

Epoch: 143
Loss: 0.02854489636437038
ROC train: 1.000000	val: 0.712821	test: 0.813250
PRC train: 1.000000	val: 0.942450	test: 0.791716

Epoch: 144
Loss: 0.03435459087247876
ROC train: 1.000000	val: 0.712454	test: 0.817771
PRC train: 1.000000	val: 0.941133	test: 0.795690

Epoch: 145
Loss: 0.027650829795220135
ROC train: 1.000000	val: 0.713553	test: 0.803686
PRC train: 1.000000	val: 0.938159	test: 0.785301

Early stopping
Best (ROC):	 train: 1.000000	val: 0.727106	test: 0.802295
Best (PRC):	 train: 1.000000	val: 0.945268	test: 0.788504

PRC train: 1.000000	val: 0.910463	test: 0.750510

Epoch: 94
Loss: 0.05902299392585371
ROC train: 1.000000	val: 0.627473	test: 0.714311
PRC train: 1.000000	val: 0.908542	test: 0.739024

Epoch: 95
Loss: 0.0595379594372338
ROC train: 1.000000	val: 0.613919	test: 0.727178
PRC train: 1.000000	val: 0.906757	test: 0.736082

Epoch: 96
Loss: 0.06987750911297319
ROC train: 1.000000	val: 0.616117	test: 0.715875
PRC train: 1.000000	val: 0.909862	test: 0.720123

Epoch: 97
Loss: 0.061917152533226195
ROC train: 1.000000	val: 0.623077	test: 0.723005
PRC train: 1.000000	val: 0.903313	test: 0.725490

Epoch: 98
Loss: 0.06644795561214727
ROC train: 1.000000	val: 0.614286	test: 0.729786
PRC train: 1.000000	val: 0.895413	test: 0.741729

Epoch: 99
Loss: 0.05472550691101299
ROC train: 1.000000	val: 0.611355	test: 0.723352
PRC train: 1.000000	val: 0.894029	test: 0.741590

Epoch: 100
Loss: 0.06135952101589075
ROC train: 1.000000	val: 0.601465	test: 0.714658
PRC train: 1.000000	val: 0.890537	test: 0.734192

Epoch: 101
Loss: 0.05851126188119754
ROC train: 1.000000	val: 0.586813	test: 0.738828
PRC train: 1.000000	val: 0.889214	test: 0.740716

Epoch: 102
Loss: 0.05001338496769745
ROC train: 0.999997	val: 0.584982	test: 0.744914
PRC train: 0.999996	val: 0.893693	test: 0.738569

Epoch: 103
Loss: 0.06464824068801643
ROC train: 1.000000	val: 0.594139	test: 0.740393
PRC train: 1.000000	val: 0.894273	test: 0.752091

Epoch: 104
Loss: 0.0526590103635311
ROC train: 1.000000	val: 0.605495	test: 0.728047
PRC train: 1.000000	val: 0.893944	test: 0.744485

Epoch: 105
Loss: 0.03914355528832559
ROC train: 1.000000	val: 0.591941	test: 0.727352
PRC train: 1.000000	val: 0.893599	test: 0.743479

Epoch: 106
Loss: 0.04634856959816377
ROC train: 1.000000	val: 0.598535	test: 0.747696
PRC train: 1.000000	val: 0.896250	test: 0.759890

Epoch: 107
Loss: 0.04547732475667205
ROC train: 1.000000	val: 0.608791	test: 0.753434
PRC train: 1.000000	val: 0.897925	test: 0.763380

Epoch: 108
Loss: 0.04444156507690958
ROC train: 1.000000	val: 0.617216	test: 0.752391
PRC train: 1.000000	val: 0.898283	test: 0.770790

Epoch: 109
Loss: 0.04421074368993539
ROC train: 1.000000	val: 0.618681	test: 0.737089
PRC train: 1.000000	val: 0.899473	test: 0.757298

Epoch: 110
Loss: 0.04260064766948304
ROC train: 1.000000	val: 0.632967	test: 0.730308
PRC train: 1.000000	val: 0.908163	test: 0.746009

Epoch: 111
Loss: 0.04716659500918631
ROC train: 1.000000	val: 0.646886	test: 0.740393
PRC train: 1.000000	val: 0.915205	test: 0.752074

Epoch: 112
Loss: 0.045583696182925806
ROC train: 1.000000	val: 0.624542	test: 0.743523
PRC train: 1.000000	val: 0.907820	test: 0.765085

Epoch: 113
Loss: 0.04340626089932679
ROC train: 1.000000	val: 0.615751	test: 0.735350
PRC train: 1.000000	val: 0.902798	test: 0.761692

Epoch: 114
Loss: 0.050172850478609185
ROC train: 1.000000	val: 0.576923	test: 0.731351
PRC train: 1.000000	val: 0.889154	test: 0.742033

Epoch: 115
Loss: 0.04835047946490785
ROC train: 1.000000	val: 0.557509	test: 0.729786
PRC train: 1.000000	val: 0.875271	test: 0.739826

Epoch: 116
Loss: 0.053003390607911885
ROC train: 1.000000	val: 0.588278	test: 0.720049
PRC train: 1.000000	val: 0.890416	test: 0.747747

Epoch: 117
Loss: 0.029994264833260342
ROC train: 1.000000	val: 0.613919	test: 0.714137
PRC train: 1.000000	val: 0.898669	test: 0.738498

Epoch: 118
Loss: 0.039235343873487184
ROC train: 1.000000	val: 0.611722	test: 0.710833
PRC train: 1.000000	val: 0.898751	test: 0.732357

Epoch: 119
Loss: 0.05074321507169247
ROC train: 1.000000	val: 0.599267	test: 0.711876
PRC train: 1.000000	val: 0.896822	test: 0.727523

Epoch: 120
Loss: 0.04145502748738565
ROC train: 1.000000	val: 0.614286	test: 0.730656
PRC train: 1.000000	val: 0.904077	test: 0.738712

Epoch: 121
Loss: 0.0560867456290741
ROC train: 1.000000	val: 0.606960	test: 0.745088
PRC train: 1.000000	val: 0.901130	test: 0.756521

Epoch: 122
Loss: 0.06741063221962675
ROC train: 1.000000	val: 0.598535	test: 0.727873
PRC train: 1.000000	val: 0.893372	test: 0.741391

Epoch: 123
Loss: 0.043275975148540965
ROC train: 1.000000	val: 0.595238	test: 0.719875
PRC train: 1.000000	val: 0.894440	test: 0.732129

Epoch: 124
Loss: 0.04676647510732854
ROC train: 1.000000	val: 0.578388	test: 0.738132
PRC train: 1.000000	val: 0.894494	test: 0.743731

Epoch: 125
Loss: 0.037519164718485945
ROC train: 1.000000	val: 0.592674	test: 0.745957
PRC train: 1.000000	val: 0.901673	test: 0.751501

Epoch: 126
Loss: 0.0474698463900279
ROC train: 1.000000	val: 0.606227	test: 0.747001
PRC train: 1.000000	val: 0.902795	test: 0.756129

Epoch: 127
Loss: 0.040885089727044
ROC train: 1.000000	val: 0.620147	test: 0.756390
PRC train: 1.000000	val: 0.904373	test: 0.767173

Epoch: 128
Loss: 0.03502002198715139
ROC train: 1.000000	val: 0.617582	test: 0.758998
PRC train: 1.000000	val: 0.902133	test: 0.764438

Epoch: 129
Loss: 0.03588910536746377
ROC train: 1.000000	val: 0.615751	test: 0.755869
PRC train: 1.000000	val: 0.901208	test: 0.763550

Epoch: 130
Loss: 0.023801002622773106
ROC train: 1.000000	val: 0.604396	test: 0.742827
PRC train: 1.000000	val: 0.898024	test: 0.756322

Epoch: 131
Loss: 0.044178336126753745
ROC train: 1.000000	val: 0.614286	test: 0.725787
PRC train: 1.000000	val: 0.900816	test: 0.743656

Epoch: 132
Loss: 0.05405290927244599
ROC train: 1.000000	val: 0.635165	test: 0.736046
PRC train: 1.000000	val: 0.909852	test: 0.743880

Epoch: 133
Loss: 0.025529281116595127
ROC train: 1.000000	val: 0.644322	test: 0.756564
PRC train: 1.000000	val: 0.913231	test: 0.753262

Epoch: 134
Loss: 0.04470658192388688
ROC train: 1.000000	val: 0.634066	test: 0.759346
PRC train: 1.000000	val: 0.908647	test: 0.759211

Epoch: 135
Loss: 0.02421074582861611
ROC train: 1.000000	val: 0.605128	test: 0.751000
PRC train: 1.000000	val: 0.897773	test: 0.757245

Epoch: 136
Loss: 0.045283114032491446
ROC train: 1.000000	val: 0.596337	test: 0.743349
PRC train: 1.000000	val: 0.895093	test: 0.753147

Epoch: 137
Loss: 0.02861543683992914
ROC train: 1.000000	val: 0.595971	test: 0.749261
PRC train: 1.000000	val: 0.895370	test: 0.756965

Epoch: 138
Loss: 0.021077197908728747
ROC train: 1.000000	val: 0.593407	test: 0.741784
PRC train: 1.000000	val: 0.894899	test: 0.751882

Epoch: 139
Loss: 0.04353177934308945
ROC train: 1.000000	val: 0.587546	test: 0.738132
PRC train: 1.000000	val: 0.892360	test: 0.753138

Epoch: 140
Loss: 0.03301432374024983
ROC train: 1.000000	val: 0.583883	test: 0.736915
PRC train: 1.000000	val: 0.886856	test: 0.757278

Epoch: 141
Loss: 0.040660959583503596
ROC train: 1.000000	val: 0.582051	test: 0.742480
PRC train: 1.000000	val: 0.884797	test: 0.763348

Epoch: 142
Loss: 0.03344178050613523
ROC train: 1.000000	val: 0.568864	test: 0.736394
PRC train: 1.000000	val: 0.881842	test: 0.755427

Epoch: 143
Loss: 0.051215885862487195
ROC train: 1.000000	val: 0.572161	test: 0.717267
PRC train: 1.000000	val: 0.882304	test: 0.740191

Epoch: 144
Loss: 0.03069106030276414
ROC train: 1.000000	val: 0.572527	test: 0.720918
PRC train: 1.000000	val: 0.879864	test: 0.740596

Epoch: 145
Loss: 0.03463606055169984
ROC train: 1.000000	val: 0.572527	test: 0.746305
PRC train: 1.000000	val: 0.879716	test: 0.759413

Epoch: 146
Loss: 0.022442890938782715
ROC train: 1.000000	val: 0.584982	test: 0.753782
PRC train: 1.000000	val: 0.885809	test: 0.758884

Early stopping
Best (ROC):	 train: 1.000000	val: 0.646886	test: 0.740393
Best (PRC):	 train: 1.000000	val: 0.915205	test: 0.752074
All runs completed.
All runs completed.

PRC train: 0.999943	val: 0.934512	test: 0.811322

Epoch: 94
Loss: 0.07952397864480028
ROC train: 0.999917	val: 0.687179	test: 0.800209
PRC train: 0.999871	val: 0.939008	test: 0.792556

Epoch: 95
Loss: 0.06645070029835874
ROC train: 0.999800	val: 0.678022	test: 0.792384
PRC train: 0.999689	val: 0.936364	test: 0.793286

Epoch: 96
Loss: 0.0676045765737887
ROC train: 0.999877	val: 0.675824	test: 0.811163
PRC train: 0.999812	val: 0.935525	test: 0.801118

Epoch: 97
Loss: 0.08178585282151843
ROC train: 0.999983	val: 0.659707	test: 0.819162
PRC train: 0.999974	val: 0.934112	test: 0.819589

Epoch: 98
Loss: 0.07262978695457158
ROC train: 0.999777	val: 0.680586	test: 0.798991
PRC train: 0.999661	val: 0.936193	test: 0.799350

Epoch: 99
Loss: 0.07745502038075397
ROC train: 1.000000	val: 0.678755	test: 0.802121
PRC train: 1.000000	val: 0.934707	test: 0.799551

Epoch: 100
Loss: 0.06982861001640839
ROC train: 0.999940	val: 0.664835	test: 0.807338
PRC train: 0.999913	val: 0.931605	test: 0.795498

Epoch: 101
Loss: 0.07485985947757015
ROC train: 0.999852	val: 0.690842	test: 0.781082
PRC train: 0.999777	val: 0.937302	test: 0.766689

Epoch: 102
Loss: 0.06583343284357157
ROC train: 0.999920	val: 0.692674	test: 0.786994
PRC train: 0.999883	val: 0.938473	test: 0.775462

Epoch: 103
Loss: 0.07372315726964832
ROC train: 0.999974	val: 0.676923	test: 0.806121
PRC train: 0.999961	val: 0.936446	test: 0.792806

Epoch: 104
Loss: 0.07404879399460865
ROC train: 0.999994	val: 0.682051	test: 0.798470
PRC train: 0.999991	val: 0.938035	test: 0.794800

Epoch: 105
Loss: 0.05562224493945258
ROC train: 0.999914	val: 0.688278	test: 0.774474
PRC train: 0.999868	val: 0.938809	test: 0.773871

Epoch: 106
Loss: 0.0545435858233841
ROC train: 0.999997	val: 0.674359	test: 0.805251
PRC train: 0.999996	val: 0.933224	test: 0.803708

Epoch: 107
Loss: 0.05967263107204982
ROC train: 1.000000	val: 0.662271	test: 0.823683
PRC train: 1.000000	val: 0.928738	test: 0.813248

Epoch: 108
Loss: 0.0422947861234505
ROC train: 1.000000	val: 0.691575	test: 0.817075
PRC train: 1.000000	val: 0.937000	test: 0.809146

Epoch: 109
Loss: 0.0696990358928068
ROC train: 0.999989	val: 0.701099	test: 0.811685
PRC train: 0.999983	val: 0.938579	test: 0.797246

Epoch: 110
Loss: 0.06448826860723528
ROC train: 0.999986	val: 0.691941	test: 0.820901
PRC train: 0.999978	val: 0.938463	test: 0.802604

Epoch: 111
Loss: 0.05998110131843826
ROC train: 1.000000	val: 0.683883	test: 0.823857
PRC train: 1.000000	val: 0.937825	test: 0.826559

Epoch: 112
Loss: 0.06182209597042472
ROC train: 1.000000	val: 0.670696	test: 0.821248
PRC train: 1.000000	val: 0.934334	test: 0.830964

Epoch: 113
Loss: 0.06597713621776999
ROC train: 1.000000	val: 0.671062	test: 0.808381
PRC train: 1.000000	val: 0.934532	test: 0.807784

Epoch: 114
Loss: 0.06735001065828602
ROC train: 1.000000	val: 0.683150	test: 0.810642
PRC train: 1.000000	val: 0.936232	test: 0.810738

Epoch: 115
Loss: 0.06358939945942738
ROC train: 1.000000	val: 0.686081	test: 0.802991
PRC train: 1.000000	val: 0.937025	test: 0.796880

Epoch: 116
Loss: 0.053746943770830016
ROC train: 1.000000	val: 0.684982	test: 0.790819
PRC train: 1.000000	val: 0.937669	test: 0.776736

Epoch: 117
Loss: 0.06440974964079806
ROC train: 1.000000	val: 0.698901	test: 0.803686
PRC train: 1.000000	val: 0.941048	test: 0.793296

Epoch: 118
Loss: 0.06589630199811757
ROC train: 1.000000	val: 0.701465	test: 0.813250
PRC train: 1.000000	val: 0.941166	test: 0.796464

Epoch: 119
Loss: 0.07325054681624271
ROC train: 0.999994	val: 0.713919	test: 0.820727
PRC train: 0.999991	val: 0.944877	test: 0.810297

Epoch: 120
Loss: 0.04476799590151537
ROC train: 0.999989	val: 0.723077	test: 0.805425
PRC train: 0.999983	val: 0.948097	test: 0.806777

Epoch: 121
Loss: 0.044885435969813973
ROC train: 1.000000	val: 0.700733	test: 0.794297
PRC train: 1.000000	val: 0.944384	test: 0.797048

Epoch: 122
Loss: 0.0681949104885908
ROC train: 1.000000	val: 0.683150	test: 0.817423
PRC train: 1.000000	val: 0.937839	test: 0.816658

Epoch: 123
Loss: 0.055395135603449185
ROC train: 0.999757	val: 0.704029	test: 0.798644
PRC train: 0.999631	val: 0.940361	test: 0.797994

Epoch: 124
Loss: 0.05681322458465613
ROC train: 0.999983	val: 0.682418	test: 0.802469
PRC train: 0.999974	val: 0.936559	test: 0.797654

Epoch: 125
Loss: 0.06151894875809342
ROC train: 0.999994	val: 0.687179	test: 0.814815
PRC train: 0.999991	val: 0.935931	test: 0.795513

Epoch: 126
Loss: 0.04352807075284497
ROC train: 0.999997	val: 0.698168	test: 0.821075
PRC train: 0.999996	val: 0.937615	test: 0.805135

Epoch: 127
Loss: 0.03913016669759088
ROC train: 0.999991	val: 0.693040	test: 0.812554
PRC train: 0.999987	val: 0.937367	test: 0.796292

Epoch: 128
Loss: 0.0546298686888659
ROC train: 0.999994	val: 0.686081	test: 0.796209
PRC train: 0.999991	val: 0.939096	test: 0.783340

Epoch: 129
Loss: 0.04459426133681606
ROC train: 1.000000	val: 0.668498	test: 0.792210
PRC train: 1.000000	val: 0.935298	test: 0.774649

Epoch: 130
Loss: 0.046416514792153654
ROC train: 1.000000	val: 0.674359	test: 0.797427
PRC train: 1.000000	val: 0.935969	test: 0.778249

Epoch: 131
Loss: 0.061317135334252294
ROC train: 0.999997	val: 0.684615	test: 0.799861
PRC train: 0.999996	val: 0.938577	test: 0.780985

Epoch: 132
Loss: 0.03954980190151016
ROC train: 0.999991	val: 0.698535	test: 0.798122
PRC train: 0.999987	val: 0.942403	test: 0.780781

Epoch: 133
Loss: 0.04459172082366985
ROC train: 0.999994	val: 0.704029	test: 0.797079
PRC train: 0.999991	val: 0.942060	test: 0.779866

Epoch: 134
Loss: 0.031179011733659934
ROC train: 0.999991	val: 0.700366	test: 0.801600
PRC train: 0.999987	val: 0.940671	test: 0.781086

Epoch: 135
Loss: 0.042633501857975084
ROC train: 1.000000	val: 0.676923	test: 0.801426
PRC train: 1.000000	val: 0.935414	test: 0.777222

Epoch: 136
Loss: 0.03873877013938811
ROC train: 1.000000	val: 0.663370	test: 0.817771
PRC train: 1.000000	val: 0.932270	test: 0.812900

Epoch: 137
Loss: 0.05450684241105262
ROC train: 1.000000	val: 0.672527	test: 0.828204
PRC train: 1.000000	val: 0.935127	test: 0.826237

Epoch: 138
Loss: 0.041090460006653944
ROC train: 0.999980	val: 0.695971	test: 0.810816
PRC train: 0.999970	val: 0.938607	test: 0.804075

Epoch: 139
Loss: 0.049725976607888786
ROC train: 1.000000	val: 0.686813	test: 0.811163
PRC train: 1.000000	val: 0.937827	test: 0.805378

Epoch: 140
Loss: 0.039449907649094514
ROC train: 1.000000	val: 0.688645	test: 0.816728
PRC train: 1.000000	val: 0.939897	test: 0.805363

Epoch: 141
Loss: 0.032180958334114454
ROC train: 1.000000	val: 0.701465	test: 0.797253
PRC train: 1.000000	val: 0.943239	test: 0.779933

Epoch: 142
Loss: 0.05858230810267297
ROC train: 1.000000	val: 0.701832	test: 0.778473
PRC train: 1.000000	val: 0.941745	test: 0.771027

Epoch: 143
Loss: 0.033085207561222706
ROC train: 1.000000	val: 0.690476	test: 0.791862
PRC train: 1.000000	val: 0.939067	test: 0.789619

Epoch: 144
Loss: 0.031190171039969024
ROC train: 1.000000	val: 0.678388	test: 0.808207
PRC train: 1.000000	val: 0.937918	test: 0.805845

Epoch: 145
Loss: 0.04319201159047921
ROC train: 1.000000	val: 0.669963	test: 0.801426
PRC train: 1.000000	val: 0.936155	test: 0.799179

Epoch: 146
Loss: 0.04876109988855817
ROC train: 1.000000	val: 0.675092	test: 0.816380
PRC train: 1.000000	val: 0.936540	test: 0.804869

Epoch: 147
Loss: 0.04660688443128981
ROC train: 0.999991	val: 0.697436	test: 0.812033
PRC train: 0.999987	val: 0.941562	test: 0.806414

Epoch: 148
Loss: 0.0498562306858351
ROC train: 0.999989	val: 0.692308	test: 0.817249
PRC train: 0.999983	val: 0.940471	test: 0.809006

Epoch: 149
Loss: 0.043075742214843914
ROC train: 0.999991	val: 0.681319	test: 0.820727
PRC train: 0.999987	val: 0.935221	test: 0.821900

Epoch: 150
Loss: 0.03252443616217192
ROC train: 0.999957	val: 0.700366	test: 0.813945
PRC train: 0.999935	val: 0.938344	test: 0.812968

Epoch: 151
Loss: 0.050144208597509544
ROC train: 0.999991	val: 0.698168	test: 0.816380
PRC train: 0.999987	val: 0.938097	test: 0.812483

Epoch: 152
Loss: 0.024551141095269898
ROC train: 1.000000	val: 0.691941	test: 0.824552
PRC train: 1.000000	val: 0.939069	test: 0.818893

Epoch: 153
Loss: 0.039087741453493856
ROC train: 1.000000	val: 0.698901	test: 0.817075
PRC train: 1.000000	val: 0.941768	test: 0.810380

Epoch: 154
Loss: 0.034353716757621236
ROC train: 1.000000	val: 0.715751	test: 0.808207
PRC train: 1.000000	val: 0.947114	test: 0.801235

Epoch: 155
Loss: 0.032546504069510214
ROC train: 1.000000	val: 0.722711	test: 0.806642
PRC train: 1.000000	val: 0.948789	test: 0.800930

Early stopping
Best (ROC):	 train: 0.999989	val: 0.723077	test: 0.805425
Best (PRC):	 train: 0.999983	val: 0.948097	test: 0.806777
All runs completed.

ROC train: 0.991578	val: 0.652015	test: 0.701965
PRC train: 0.988109	val: 0.921449	test: 0.701900

Epoch: 156
Loss: 0.16908296610424714
ROC train: 0.992126	val: 0.667399	test: 0.704573
PRC train: 0.988849	val: 0.924201	test: 0.698771

Epoch: 157
Loss: 0.1585639840878914
ROC train: 0.992885	val: 0.675092	test: 0.692054
PRC train: 0.989881	val: 0.923544	test: 0.684239

Epoch: 158
Loss: 0.15680170290833312
ROC train: 0.993650	val: 0.666667	test: 0.689098
PRC train: 0.990900	val: 0.919115	test: 0.681803

Epoch: 159
Loss: 0.155040381518959
ROC train: 0.993884	val: 0.648352	test: 0.696401
PRC train: 0.991359	val: 0.916717	test: 0.696276

Epoch: 160
Loss: 0.15977223589863826
ROC train: 0.994021	val: 0.652015	test: 0.710485
PRC train: 0.991627	val: 0.919371	test: 0.711401

Epoch: 161
Loss: 0.15588555059682613
ROC train: 0.994010	val: 0.652747	test: 0.711528
PRC train: 0.991520	val: 0.920214	test: 0.707879

Epoch: 162
Loss: 0.17230875399435633
ROC train: 0.993710	val: 0.646154	test: 0.707703
PRC train: 0.991175	val: 0.917440	test: 0.702515

Epoch: 163
Loss: 0.15388836720101598
ROC train: 0.994284	val: 0.664103	test: 0.705269
PRC train: 0.991831	val: 0.920660	test: 0.699896

Epoch: 164
Loss: 0.1414597375552328
ROC train: 0.994050	val: 0.661172	test: 0.711181
PRC train: 0.991475	val: 0.919682	test: 0.705268

Epoch: 165
Loss: 0.17685489466253068
ROC train: 0.992851	val: 0.642857	test: 0.691706
PRC train: 0.989727	val: 0.912959	test: 0.674397

Epoch: 166
Loss: 0.15849709399676498
ROC train: 0.993582	val: 0.647619	test: 0.692749
PRC train: 0.990652	val: 0.915794	test: 0.677799

Epoch: 167
Loss: 0.1619232605493903
ROC train: 0.992680	val: 0.664469	test: 0.701791
PRC train: 0.989481	val: 0.918624	test: 0.692963

Epoch: 168
Loss: 0.1620302046815535
ROC train: 0.994312	val: 0.665568	test: 0.693445
PRC train: 0.991949	val: 0.922315	test: 0.684918

Epoch: 169
Loss: 0.15093045002694047
ROC train: 0.994720	val: 0.672527	test: 0.701269
PRC train: 0.992491	val: 0.930560	test: 0.688337

Epoch: 170
Loss: 0.15990155071368198
ROC train: 0.994780	val: 0.682051	test: 0.697270
PRC train: 0.992642	val: 0.931567	test: 0.678158

Epoch: 171
Loss: 0.1499698936252351
ROC train: 0.995171	val: 0.676557	test: 0.705095
PRC train: 0.993300	val: 0.923522	test: 0.693104

Epoch: 172
Loss: 0.15492459112261964
ROC train: 0.994892	val: 0.667766	test: 0.715006
PRC train: 0.992798	val: 0.920341	test: 0.717449

Epoch: 173
Loss: 0.17674899168856473
ROC train: 0.993219	val: 0.664103	test: 0.710659
PRC train: 0.989953	val: 0.918338	test: 0.706282

Epoch: 174
Loss: 0.1603544614948313
ROC train: 0.994033	val: 0.658974	test: 0.704747
PRC train: 0.991423	val: 0.911837	test: 0.700300

Epoch: 175
Loss: 0.1519717517166408
ROC train: 0.993348	val: 0.641026	test: 0.700574
PRC train: 0.990312	val: 0.910639	test: 0.697709

Epoch: 176
Loss: 0.16664821702975266
ROC train: 0.994401	val: 0.671795	test: 0.694836
PRC train: 0.991957	val: 0.923535	test: 0.687038

Epoch: 177
Loss: 0.14693557880136848
ROC train: 0.994275	val: 0.675458	test: 0.682316
PRC train: 0.991737	val: 0.926631	test: 0.677297

Epoch: 178
Loss: 0.13019815190332656
ROC train: 0.995245	val: 0.649817	test: 0.689445
PRC train: 0.993317	val: 0.921693	test: 0.679915

Epoch: 179
Loss: 0.1460723278595562
ROC train: 0.994501	val: 0.661905	test: 0.695183
PRC train: 0.992222	val: 0.922825	test: 0.682154

Epoch: 180
Loss: 0.1581771722859468
ROC train: 0.993162	val: 0.662271	test: 0.690315
PRC train: 0.990334	val: 0.920483	test: 0.692205

Early stopping
Best (ROC):	 train: 0.993071	val: 0.699634	test: 0.709094
Best (PRC):	 train: 0.990455	val: 0.931231	test: 0.693289
All runs completed.
