>>> Starting run for dataset: freesolv
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running RANDOM configs_split_experiments/GraphCL/freesolv/random/train_prop=0.6.yml on cuda:0
Running RANDOM configs_split_experiments/GraphCL/freesolv/random/train_prop=0.7.yml on cuda:1
Running RANDOM configs_split_experiments/GraphCL/freesolv/random/train_prop=0.8.yml on cuda:2
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/freesolv/random/train_prop=0.8.yml --runseed 4 --device cuda:2
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/freesolv/random/train_prop=0.8.yml --runseed 5 --device cuda:2
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/freesolv/random/train_prop=0.8.yml --runseed 6 --device cuda:2
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/freesolv/random/train_prop=0.6.yml --runseed 4 --device cuda:0
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/freesolv/random/train_prop=0.6.yml --runseed 5 --device cuda:0
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/freesolv/random/train_prop=0.6.yml --runseed 6 --device cuda:0
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/freesolv/random/train_prop=0.7.yml --runseed 4 --device cuda:1
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/freesolv/random/train_prop=0.7.yml --runseed 5 --device cuda:1
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/freesolv/random/train_prop=0.7.yml --runseed 6 --device cuda:1
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/freesolv/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/freesolv/random/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/freesolv/random/train_prop=0.6/freesolv_random_4_26-05_09-43-12  ]
[ Using Seed :  4  ]
[ Using device :  cuda:0  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 31.241816520690918
RMSE train: 5.474012	val: 5.505515	test: 5.390942
MAE train: 4.255285	val: 4.177493	test: 4.281353

Epoch: 2
Loss: 29.737693786621094
RMSE train: 5.593492	val: 5.632466	test: 5.540655
MAE train: 4.443981	val: 4.413752	test: 4.529120

Epoch: 3
Loss: 25.549044609069824
RMSE train: 5.645242	val: 5.696293	test: 5.623857
MAE train: 4.562892	val: 4.533026	test: 4.633718

Epoch: 4
Loss: 25.452829360961914
RMSE train: 5.619441	val: 5.674426	test: 5.635456
MAE train: 4.579677	val: 4.543822	test: 4.653183

Epoch: 5
Loss: 24.032797813415527
RMSE train: 5.509267	val: 5.570910	test: 5.570123
MAE train: 4.520678	val: 4.487430	test: 4.600164

Epoch: 6
Loss: 22.807466506958008
RMSE train: 5.379398	val: 5.429749	test: 5.483579
MAE train: 4.450894	val: 4.407345	test: 4.540797

Epoch: 7
Loss: 21.557490348815918
RMSE train: 5.240317	val: 5.277961	test: 5.404768
MAE train: 4.392548	val: 4.338526	test: 4.502078

Epoch: 8
Loss: 20.434019088745117
RMSE train: 5.094150	val: 5.126527	test: 5.310045
MAE train: 4.342538	val: 4.280978	test: 4.473920

Epoch: 9
Loss: 18.74183177947998
RMSE train: 4.847765	val: 4.858581	test: 5.098918
MAE train: 4.186151	val: 4.133132	test: 4.358437

Epoch: 10
Loss: 17.037984371185303
RMSE train: 4.607375	val: 4.541012	test: 4.845093
MAE train: 4.011597	val: 3.951120	test: 4.227359

Epoch: 11
Loss: 16.26249122619629
RMSE train: 4.308519	val: 4.082192	test: 4.503174
MAE train: 3.782340	val: 3.630890	test: 3.999089

Epoch: 12
Loss: 15.725767135620117
RMSE train: 4.030962	val: 3.671475	test: 4.223278
MAE train: 3.543106	val: 3.301586	test: 3.790182

Epoch: 13
Loss: 14.956570148468018
RMSE train: 3.804106	val: 3.409916	test: 3.993898
MAE train: 3.325474	val: 3.046683	test: 3.575852

Epoch: 14
Loss: 14.884331703186035
RMSE train: 3.649414	val: 3.298396	test: 3.851709
MAE train: 3.185384	val: 2.936893	test: 3.424405

Epoch: 15
Loss: 14.183330535888672
RMSE train: 3.607528	val: 3.295354	test: 3.813789
MAE train: 3.151153	val: 2.920287	test: 3.384473

Epoch: 16
Loss: 12.819410800933838
RMSE train: 3.655113	val: 3.358721	test: 3.858662
MAE train: 3.202024	val: 2.977510	test: 3.425289

Epoch: 17
Loss: 12.806809902191162
RMSE train: 3.758095	val: 3.467247	test: 3.944109
MAE train: 3.309732	val: 3.066403	test: 3.509514

Epoch: 18
Loss: 11.46568489074707
RMSE train: 3.842818	val: 3.559373	test: 4.008324
MAE train: 3.392671	val: 3.143342	test: 3.568667

Epoch: 19
Loss: 11.155098915100098
RMSE train: 3.896349	val: 3.634455	test: 4.055756
MAE train: 3.459713	val: 3.224462	test: 3.620756

Epoch: 20
Loss: 11.107869148254395
RMSE train: 3.875108	val: 3.663199	test: 4.047633
MAE train: 3.441087	val: 3.257170	test: 3.601117

Epoch: 21
Loss: 10.761116027832031
RMSE train: 3.844122	val: 3.665589	test: 4.031016
MAE train: 3.412679	val: 3.257923	test: 3.572824

Epoch: 22
Loss: 10.377957344055176
RMSE train: 3.808369	val: 3.655806	test: 3.992883
MAE train: 3.379918	val: 3.245722	test: 3.531513Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/freesolv/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/freesolv/random/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/freesolv/random/train_prop=0.6/freesolv_random_6_26-05_09-43-12  ]
[ Using Seed :  6  ]
[ Using device :  cuda:0  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 29.64200782775879
RMSE train: 5.251237	val: 5.348451	test: 5.243818
MAE train: 4.117345	val: 4.117653	test: 4.218517

Epoch: 2
Loss: 26.84261417388916
RMSE train: 5.198498	val: 5.275755	test: 5.231902
MAE train: 4.113953	val: 4.102221	test: 4.220203

Epoch: 3
Loss: 25.891615867614746
RMSE train: 5.065452	val: 5.126777	test: 5.157055
MAE train: 4.039738	val: 4.032617	test: 4.170714

Epoch: 4
Loss: 23.038822174072266
RMSE train: 4.879750	val: 4.915988	test: 5.048748
MAE train: 3.920096	val: 3.907123	test: 4.086997

Epoch: 5
Loss: 21.72641658782959
RMSE train: 4.654490	val: 4.657574	test: 4.909060
MAE train: 3.768457	val: 3.756455	test: 3.974327

Epoch: 6
Loss: 20.922300338745117
RMSE train: 4.432810	val: 4.384140	test: 4.751035
MAE train: 3.622251	val: 3.596525	test: 3.880351

Epoch: 7
Loss: 18.108028888702393
RMSE train: 4.236375	val: 4.102768	test: 4.588475
MAE train: 3.512857	val: 3.434741	test: 3.819896

Epoch: 8
Loss: 18.852510452270508
RMSE train: 4.049464	val: 3.862229	test: 4.362205
MAE train: 3.380978	val: 3.291003	test: 3.712632

Epoch: 9
Loss: 17.51891803741455
RMSE train: 3.881980	val: 3.666783	test: 4.120648
MAE train: 3.274175	val: 3.170201	test: 3.572185

Epoch: 10
Loss: 16.018847465515137
RMSE train: 3.794861	val: 3.577358	test: 3.964444
MAE train: 3.250744	val: 3.111766	test: 3.483908

Epoch: 11
Loss: 15.268315315246582
RMSE train: 3.776371	val: 3.597659	test: 3.919696
MAE train: 3.269232	val: 3.133221	test: 3.448180

Epoch: 12
Loss: 13.959866523742676
RMSE train: 3.838653	val: 3.674386	test: 4.021033
MAE train: 3.341357	val: 3.211447	test: 3.519392

Epoch: 13
Loss: 14.861008644104004
RMSE train: 3.952427	val: 3.763018	test: 4.158760
MAE train: 3.459116	val: 3.299992	test: 3.646183

Epoch: 14
Loss: 13.706671237945557
RMSE train: 4.032786	val: 3.833629	test: 4.255596
MAE train: 3.564537	val: 3.389782	test: 3.757579

Epoch: 15
Loss: 11.710031032562256
RMSE train: 4.088429	val: 3.878716	test: 4.317479
MAE train: 3.642066	val: 3.453794	test: 3.833559

Epoch: 16
Loss: 11.998997688293457
RMSE train: 4.128469	val: 3.940912	test: 4.363742
MAE train: 3.706889	val: 3.528728	test: 3.887918

Epoch: 17
Loss: 12.576215744018555
RMSE train: 4.154641	val: 4.008426	test: 4.404866
MAE train: 3.745013	val: 3.605133	test: 3.930877

Epoch: 18
Loss: 11.046138286590576
RMSE train: 4.121141	val: 4.017001	test: 4.389762
MAE train: 3.707509	val: 3.613184	test: 3.906844

Epoch: 19
Loss: 10.837808609008789
RMSE train: 4.073408	val: 3.965874	test: 4.350956
MAE train: 3.651977	val: 3.566516	test: 3.874684

Epoch: 20
Loss: 10.161823272705078
RMSE train: 4.001189	val: 3.857326	test: 4.271313
MAE train: 3.584550	val: 3.475755	test: 3.811167

Epoch: 21
Loss: 9.897650241851807
RMSE train: 3.902031	val: 3.713641	test: 4.148290
MAE train: 3.495922	val: 3.350138	test: 3.709010

Epoch: 22
Loss: 9.676966190338135
RMSE train: 3.802401	val: 3.590710	test: 4.041139
MAE train: 3.414973	val: 3.233212	test: 3.612850Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/freesolv/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/freesolv/random/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/freesolv/random/train_prop=0.6/freesolv_random_5_26-05_09-43-12  ]
[ Using Seed :  5  ]
[ Using device :  cuda:0  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 27.56006908416748
RMSE train: 5.971487	val: 5.907730	test: 5.948646
MAE train: 4.963220	val: 4.827245	test: 5.010740

Epoch: 2
Loss: 26.926664352416992
RMSE train: 6.322075	val: 6.196632	test: 6.280984
MAE train: 5.349587	val: 5.124537	test: 5.328259

Epoch: 3
Loss: 26.721471786499023
RMSE train: 6.255866	val: 6.149799	test: 6.246798
MAE train: 5.304827	val: 5.089651	test: 5.280103

Epoch: 4
Loss: 24.401755332946777
RMSE train: 6.128364	val: 5.996020	test: 6.171197
MAE train: 5.196836	val: 4.987852	test: 5.190340

Epoch: 5
Loss: 23.795065879821777
RMSE train: 5.819954	val: 5.659583	test: 5.944177
MAE train: 4.926671	val: 4.735717	test: 4.955193

Epoch: 6
Loss: 21.846911430358887
RMSE train: 5.400642	val: 5.228388	test: 5.640616
MAE train: 4.568504	val: 4.404198	test: 4.676891

Epoch: 7
Loss: 21.560935974121094
RMSE train: 4.958175	val: 4.779919	test: 5.316563
MAE train: 4.187419	val: 4.053188	test: 4.378827

Epoch: 8
Loss: 18.35883331298828
RMSE train: 4.582985	val: 4.394608	test: 5.039434
MAE train: 3.881763	val: 3.754380	test: 4.133830

Epoch: 9
Loss: 18.61562728881836
RMSE train: 4.257549	val: 4.085967	test: 4.735606
MAE train: 3.613343	val: 3.519169	test: 3.902522

Epoch: 10
Loss: 16.594364166259766
RMSE train: 3.960071	val: 3.844025	test: 4.348995
MAE train: 3.352270	val: 3.343235	test: 3.649981

Epoch: 11
Loss: 16.342310428619385
RMSE train: 3.742925	val: 3.629865	test: 4.012865
MAE train: 3.168017	val: 3.181219	test: 3.423437

Epoch: 12
Loss: 14.985969066619873
RMSE train: 3.628119	val: 3.508701	test: 3.838147
MAE train: 3.090516	val: 3.065451	test: 3.277614

Epoch: 13
Loss: 13.948704242706299
RMSE train: 3.599422	val: 3.509182	test: 3.814070
MAE train: 3.100669	val: 3.073880	test: 3.281442

Epoch: 14
Loss: 14.63729476928711
RMSE train: 3.585516	val: 3.516491	test: 3.834330
MAE train: 3.124677	val: 3.111237	test: 3.335189

Epoch: 15
Loss: 13.696885108947754
RMSE train: 3.626979	val: 3.551507	test: 3.912196
MAE train: 3.186337	val: 3.176485	test: 3.437357

Epoch: 16
Loss: 12.148574829101562
RMSE train: 3.652626	val: 3.543147	test: 3.937372
MAE train: 3.220528	val: 3.183574	test: 3.491750

Epoch: 17
Loss: 11.088430404663086
RMSE train: 3.648917	val: 3.518039	test: 3.935952
MAE train: 3.226911	val: 3.160143	test: 3.506730

Epoch: 18
Loss: 11.012275695800781
RMSE train: 3.602136	val: 3.473370	test: 3.894087
MAE train: 3.180360	val: 3.101878	test: 3.453006

Epoch: 19
Loss: 10.663302898406982
RMSE train: 3.567151	val: 3.433736	test: 3.855258
MAE train: 3.153298	val: 3.061750	test: 3.410603

Epoch: 20
Loss: 10.156388282775879
RMSE train: 3.551865	val: 3.411548	test: 3.834558
MAE train: 3.149076	val: 3.044013	test: 3.394243

Epoch: 21
Loss: 11.091481685638428
RMSE train: 3.540382	val: 3.426717	test: 3.816836
MAE train: 3.138032	val: 3.048634	test: 3.373994

Epoch: 22
Loss: 10.095961570739746
RMSE train: 3.500722	val: 3.398493	test: 3.777641
MAE train: 3.092397	val: 3.009916	test: 3.327196Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/freesolv/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/freesolv/random/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/freesolv/random/train_prop=0.8/freesolv_random_4_26-05_09-43-12  ]
[ Using Seed :  4  ]
[ Using device :  cuda:2  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 29.59812068939209
RMSE train: 5.505303	val: 5.550834	test: 5.319309
MAE train: 4.254560	val: 4.653498	test: 3.992847

Epoch: 2
Loss: 28.21895122528076
RMSE train: 5.603524	val: 5.660190	test: 5.489470
MAE train: 4.425883	val: 4.846161	test: 4.228341

Epoch: 3
Loss: 27.015515327453613
RMSE train: 5.661480	val: 5.687431	test: 5.596480
MAE train: 4.580549	val: 4.897376	test: 4.425224

Epoch: 4
Loss: 25.42810344696045
RMSE train: 5.638083	val: 5.641917	test: 5.622588
MAE train: 4.622551	val: 4.861421	test: 4.511592

Epoch: 5
Loss: 23.971158027648926
RMSE train: 5.625784	val: 5.598929	test: 5.662104
MAE train: 4.665862	val: 4.842731	test: 4.605318

Epoch: 6
Loss: 22.916545867919922
RMSE train: 5.481887	val: 5.444133	test: 5.595160
MAE train: 4.585332	val: 4.706283	test: 4.578884

Epoch: 7
Loss: 21.57002067565918
RMSE train: 5.233468	val: 5.236732	test: 5.395749
MAE train: 4.422718	val: 4.516070	test: 4.470327

Epoch: 8
Loss: 19.471997261047363
RMSE train: 4.900394	val: 4.994740	test: 5.034198
MAE train: 4.209635	val: 4.329173	test: 4.287783

Epoch: 9
Loss: 17.964872360229492
RMSE train: 4.549471	val: 4.765823	test: 4.647487
MAE train: 3.972911	val: 4.150119	test: 4.080303

Epoch: 10
Loss: 16.29051923751831
RMSE train: 4.209680	val: 4.562364	test: 4.285239
MAE train: 3.718111	val: 4.006716	test: 3.872097

Epoch: 11
Loss: 15.774978160858154
RMSE train: 3.890636	val: 4.347697	test: 3.941772
MAE train: 3.436412	val: 3.829950	test: 3.625533

Epoch: 12
Loss: 14.938375473022461
RMSE train: 3.660527	val: 4.182386	test: 3.672749
MAE train: 3.214537	val: 3.676075	test: 3.388235

Epoch: 13
Loss: 14.059366226196289
RMSE train: 3.576656	val: 4.145573	test: 3.548675
MAE train: 3.131120	val: 3.609287	test: 3.263875

Epoch: 14
Loss: 13.54270601272583
RMSE train: 3.657948	val: 4.276119	test: 3.633825
MAE train: 3.229211	val: 3.749988	test: 3.363681

Epoch: 15
Loss: 12.865749835968018
RMSE train: 3.788364	val: 4.443080	test: 3.776103
MAE train: 3.388939	val: 3.937263	test: 3.504636

Epoch: 16
Loss: 12.415466785430908
RMSE train: 3.934325	val: 4.571730	test: 3.930898
MAE train: 3.551611	val: 4.078664	test: 3.635384

Epoch: 17
Loss: 11.763235569000244
RMSE train: 4.036539	val: 4.618320	test: 4.040879
MAE train: 3.654476	val: 4.131606	test: 3.705408

Epoch: 18
Loss: 11.441176414489746
RMSE train: 4.051676	val: 4.605728	test: 4.065730
MAE train: 3.663123	val: 4.117322	test: 3.698442

Epoch: 19
Loss: 10.867537021636963
RMSE train: 4.001833	val: 4.551799	test: 4.031380
MAE train: 3.602514	val: 4.047699	test: 3.632087

Epoch: 20
Loss: 10.397241592407227
RMSE train: 3.886420	val: 4.476639	test: 3.931459
MAE train: 3.481101	val: 3.958723	test: 3.508361

Epoch: 21
Loss: 10.018142700195312
RMSE train: 3.748892	val: 4.395737	test: 3.831252
MAE train: 3.335427	val: 3.862102	test: 3.375103

Epoch: 22
Loss: 9.551314353942871
RMSE train: 3.631613	val: 4.317339	test: 3.746735
MAE train: 3.215529	val: 3.776820	test: 3.276283Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/freesolv/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/freesolv/random/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/freesolv/random/train_prop=0.7/freesolv_random_5_26-05_09-43-12  ]
[ Using Seed :  5  ]
[ Using device :  cuda:1  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 27.80198097229004
RMSE train: 5.782049	val: 6.350204	test: 5.839760
MAE train: 4.749856	val: 5.306081	test: 4.872178

Epoch: 2
Loss: 25.82275104522705
RMSE train: 6.102536	val: 6.603453	test: 6.115331
MAE train: 5.100980	val: 5.613841	test: 5.147142

Epoch: 3
Loss: 24.87453269958496
RMSE train: 6.149708	val: 6.580448	test: 6.170715
MAE train: 5.170268	val: 5.611329	test: 5.193734

Epoch: 4
Loss: 23.530945777893066
RMSE train: 5.986371	val: 6.376324	test: 6.076545
MAE train: 5.054176	val: 5.453523	test: 5.105977

Epoch: 5
Loss: 21.245305061340332
RMSE train: 5.681135	val: 6.039720	test: 5.875895
MAE train: 4.816738	val: 5.188896	test: 4.923325

Epoch: 6
Loss: 20.504395484924316
RMSE train: 5.353665	val: 5.687990	test: 5.648938
MAE train: 4.562154	val: 4.905460	test: 4.721383

Epoch: 7
Loss: 18.685633659362793
RMSE train: 4.955681	val: 5.339634	test: 5.358507
MAE train: 4.273887	val: 4.639804	test: 4.493655

Epoch: 8
Loss: 17.22930335998535
RMSE train: 4.571119	val: 5.036296	test: 4.989516
MAE train: 3.993398	val: 4.400483	test: 4.243242

Epoch: 9
Loss: 16.84606647491455
RMSE train: 4.234187	val: 4.738390	test: 4.512956
MAE train: 3.699708	val: 4.160055	test: 3.915703

Epoch: 10
Loss: 15.400187492370605
RMSE train: 3.963195	val: 4.423774	test: 4.074561
MAE train: 3.487372	val: 3.906780	test: 3.613782

Epoch: 11
Loss: 14.91943645477295
RMSE train: 3.698727	val: 4.117496	test: 3.775005
MAE train: 3.264482	val: 3.634321	test: 3.352117

Epoch: 12
Loss: 13.83766508102417
RMSE train: 3.538628	val: 3.960367	test: 3.651264
MAE train: 3.124704	val: 3.498894	test: 3.245688

Epoch: 13
Loss: 12.818385124206543
RMSE train: 3.491673	val: 3.915438	test: 3.642065
MAE train: 3.097421	val: 3.467082	test: 3.243262

Epoch: 14
Loss: 12.618276596069336
RMSE train: 3.490559	val: 3.897853	test: 3.647658
MAE train: 3.111443	val: 3.458025	test: 3.250305

Epoch: 15
Loss: 12.36369800567627
RMSE train: 3.575254	val: 4.002415	test: 3.725549
MAE train: 3.196037	val: 3.554766	test: 3.317574

Epoch: 16
Loss: 11.487473487854004
RMSE train: 3.655400	val: 4.096928	test: 3.801583
MAE train: 3.263010	val: 3.627535	test: 3.369841

Epoch: 17
Loss: 10.901104927062988
RMSE train: 3.709507	val: 4.138268	test: 3.859443
MAE train: 3.305340	val: 3.639135	test: 3.416680

Epoch: 18
Loss: 10.613287925720215
RMSE train: 3.696691	val: 4.113225	test: 3.870834
MAE train: 3.286689	val: 3.595467	test: 3.428698

Epoch: 19
Loss: 10.326913356781006
RMSE train: 3.658737	val: 4.079515	test: 3.850759
MAE train: 3.254545	val: 3.568104	test: 3.422668

Epoch: 20
Loss: 9.543294906616211
RMSE train: 3.594375	val: 3.992174	test: 3.791191
MAE train: 3.208560	val: 3.496121	test: 3.373611

Epoch: 21
Loss: 9.203965663909912
RMSE train: 3.512869	val: 3.876739	test: 3.707378
MAE train: 3.140942	val: 3.398272	test: 3.314997

Epoch: 22
Loss: 8.899767875671387
RMSE train: 3.424164	val: 3.764126	test: 3.620355
MAE train: 3.052023	val: 3.292374	test: 3.243262Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/freesolv/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/freesolv/random/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/freesolv/random/train_prop=0.8/freesolv_random_5_26-05_09-43-12  ]
[ Using Seed :  5  ]
[ Using device :  cuda:2  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 28.525150299072266
RMSE train: 5.902841	val: 6.051369	test: 5.722089
MAE train: 4.867521	val: 5.293990	test: 4.589013

Epoch: 2
Loss: 26.849793434143066
RMSE train: 6.284464	val: 6.398084	test: 6.129971
MAE train: 5.283227	val: 5.620930	test: 5.016378

Epoch: 3
Loss: 25.192338943481445
RMSE train: 6.161356	val: 6.281769	test: 6.076170
MAE train: 5.200872	val: 5.487193	test: 4.989169

Epoch: 4
Loss: 24.064230918884277
RMSE train: 5.891507	val: 6.029528	test: 5.936082
MAE train: 4.991372	val: 5.244196	test: 4.869197

Epoch: 5
Loss: 22.492812156677246
RMSE train: 5.544012	val: 5.749425	test: 5.747346
MAE train: 4.706187	val: 4.975630	test: 4.681730

Epoch: 6
Loss: 20.74709129333496
RMSE train: 5.167666	val: 5.458463	test: 5.459407
MAE train: 4.401705	val: 4.736605	test: 4.432175

Epoch: 7
Loss: 19.18941593170166
RMSE train: 4.820472	val: 5.217048	test: 5.086325
MAE train: 4.147650	val: 4.575446	test: 4.192805

Epoch: 8
Loss: 17.611827850341797
RMSE train: 4.431190	val: 4.931068	test: 4.539314
MAE train: 3.841612	val: 4.351338	test: 3.906832

Epoch: 9
Loss: 16.35528564453125
RMSE train: 4.109484	val: 4.641613	test: 3.951480
MAE train: 3.552175	val: 4.050319	test: 3.543927

Epoch: 10
Loss: 16.166097164154053
RMSE train: 3.884031	val: 4.439495	test: 3.624035
MAE train: 3.344634	val: 3.865623	test: 3.238642

Epoch: 11
Loss: 15.07603931427002
RMSE train: 3.641318	val: 4.327294	test: 3.348429
MAE train: 3.150626	val: 3.749862	test: 3.022793

Epoch: 12
Loss: 14.25107192993164
RMSE train: 3.499432	val: 4.261097	test: 3.231656
MAE train: 3.045721	val: 3.659254	test: 2.936548

Epoch: 13
Loss: 13.759827136993408
RMSE train: 3.489076	val: 4.270639	test: 3.287711
MAE train: 3.061383	val: 3.669207	test: 2.998298

Epoch: 14
Loss: 13.225592613220215
RMSE train: 3.541589	val: 4.303138	test: 3.413872
MAE train: 3.131200	val: 3.714835	test: 3.127996

Epoch: 15
Loss: 12.10277271270752
RMSE train: 3.613105	val: 4.356036	test: 3.545643
MAE train: 3.212699	val: 3.777611	test: 3.260383

Epoch: 16
Loss: 12.07999324798584
RMSE train: 3.641812	val: 4.367973	test: 3.614605
MAE train: 3.251003	val: 3.801275	test: 3.322413

Epoch: 17
Loss: 11.399040222167969
RMSE train: 3.663765	val: 4.342602	test: 3.659144
MAE train: 3.284492	val: 3.805619	test: 3.354148

Epoch: 18
Loss: 10.91215467453003
RMSE train: 3.694347	val: 4.336595	test: 3.696264
MAE train: 3.326051	val: 3.840706	test: 3.385422

Epoch: 19
Loss: 10.163104057312012
RMSE train: 3.730070	val: 4.337305	test: 3.739161
MAE train: 3.367214	val: 3.875910	test: 3.427246

Epoch: 20
Loss: 10.019808292388916
RMSE train: 3.739846	val: 4.335575	test: 3.763058
MAE train: 3.385678	val: 3.893549	test: 3.450227

Epoch: 21
Loss: 9.683321952819824
RMSE train: 3.701473	val: 4.307639	test: 3.710742
MAE train: 3.357526	val: 3.878486	test: 3.397886

Epoch: 22
Loss: 9.108034133911133
RMSE train: 3.653948	val: 4.274804	test: 3.628564
MAE train: 3.312775	val: 3.841770	test: 3.309068Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/freesolv/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/freesolv/random/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/freesolv/random/train_prop=0.7/freesolv_random_4_26-05_09-43-12  ]
[ Using Seed :  4  ]
[ Using device :  cuda:1  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 28.621395111083984
RMSE train: 5.315499	val: 5.950811	test: 5.330840
MAE train: 4.117809	val: 4.591124	test: 4.226828

Epoch: 2
Loss: 27.210111618041992
RMSE train: 5.529009	val: 6.102867	test: 5.531312
MAE train: 4.418341	val: 4.909897	test: 4.552313

Epoch: 3
Loss: 25.603981971740723
RMSE train: 5.600848	val: 6.155131	test: 5.576285
MAE train: 4.547431	val: 5.050498	test: 4.641758

Epoch: 4
Loss: 24.64614200592041
RMSE train: 5.541653	val: 6.078966	test: 5.530095
MAE train: 4.528422	val: 5.036267	test: 4.615679

Epoch: 5
Loss: 23.41328716278076
RMSE train: 5.457768	val: 5.970329	test: 5.447446
MAE train: 4.481800	val: 4.977352	test: 4.553968

Epoch: 6
Loss: 22.363554000854492
RMSE train: 5.353231	val: 5.793574	test: 5.355083
MAE train: 4.414306	val: 4.867843	test: 4.479662

Epoch: 7
Loss: 21.386357307434082
RMSE train: 5.173715	val: 5.540711	test: 5.223053
MAE train: 4.295309	val: 4.688999	test: 4.392811

Epoch: 8
Loss: 19.39389705657959
RMSE train: 4.911767	val: 5.267879	test: 4.997018
MAE train: 4.127950	val: 4.499777	test: 4.252872

Epoch: 9
Loss: 17.75498867034912
RMSE train: 4.550407	val: 4.902407	test: 4.645290
MAE train: 3.891831	val: 4.237339	test: 4.016447

Epoch: 10
Loss: 16.33634376525879
RMSE train: 4.172956	val: 4.509271	test: 4.258138
MAE train: 3.626772	val: 3.944841	test: 3.759623

Epoch: 11
Loss: 15.128922462463379
RMSE train: 3.796172	val: 4.100849	test: 3.845306
MAE train: 3.335114	val: 3.594611	test: 3.462203

Epoch: 12
Loss: 13.873491764068604
RMSE train: 3.548017	val: 3.776098	test: 3.552395
MAE train: 3.087434	val: 3.264357	test: 3.179071

Epoch: 13
Loss: 13.502458095550537
RMSE train: 3.486289	val: 3.648300	test: 3.487998
MAE train: 3.006128	val: 3.167462	test: 3.129100

Epoch: 14
Loss: 13.436124801635742
RMSE train: 3.419117	val: 3.615213	test: 3.448396
MAE train: 2.962887	val: 3.140503	test: 3.073697

Epoch: 15
Loss: 12.395880699157715
RMSE train: 3.472245	val: 3.708978	test: 3.535958
MAE train: 3.053085	val: 3.201725	test: 3.183261

Epoch: 16
Loss: 11.74207878112793
RMSE train: 3.597477	val: 3.869346	test: 3.679513
MAE train: 3.201105	val: 3.358797	test: 3.342245

Epoch: 17
Loss: 11.198852062225342
RMSE train: 3.751843	val: 4.052959	test: 3.828812
MAE train: 3.372293	val: 3.532314	test: 3.474132

Epoch: 18
Loss: 11.061160564422607
RMSE train: 3.873965	val: 4.166538	test: 3.950464
MAE train: 3.487419	val: 3.650164	test: 3.575812

Epoch: 19
Loss: 11.478102684020996
RMSE train: 3.901516	val: 4.165459	test: 3.998411
MAE train: 3.500284	val: 3.645212	test: 3.606706

Epoch: 20
Loss: 9.98787546157837
RMSE train: 3.805630	val: 4.064582	test: 3.939388
MAE train: 3.396965	val: 3.521146	test: 3.533311

Epoch: 21
Loss: 9.713505744934082
RMSE train: 3.665066	val: 3.941027	test: 3.828665
MAE train: 3.258028	val: 3.381398	test: 3.420849

Epoch: 22
Loss: 9.680721282958984
RMSE train: 3.537435	val: 3.868824	test: 3.720761
MAE train: 3.131025	val: 3.290612	test: 3.320972Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/freesolv/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/freesolv/random/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/freesolv/random/train_prop=0.7/freesolv_random_6_26-05_09-43-12  ]
[ Using Seed :  6  ]
[ Using device :  cuda:1  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 26.847928047180176
RMSE train: 5.131960	val: 5.829693	test: 5.195331
MAE train: 4.013947	val: 4.594547	test: 4.172563

Epoch: 2
Loss: 25.535009384155273
RMSE train: 5.092288	val: 5.757698	test: 5.191030
MAE train: 4.022620	val: 4.571966	test: 4.179292

Epoch: 3
Loss: 23.95644760131836
RMSE train: 4.954057	val: 5.589071	test: 5.092552
MAE train: 3.939967	val: 4.446446	test: 4.110660

Epoch: 4
Loss: 22.177239418029785
RMSE train: 4.760903	val: 5.356797	test: 4.969654
MAE train: 3.817443	val: 4.290856	test: 4.034949

Epoch: 5
Loss: 21.00473117828369
RMSE train: 4.552290	val: 5.105201	test: 4.822698
MAE train: 3.681809	val: 4.138812	test: 3.959477

Epoch: 6
Loss: 19.327638626098633
RMSE train: 4.312941	val: 4.806503	test: 4.624056
MAE train: 3.540700	val: 3.969492	test: 3.865018

Epoch: 7
Loss: 17.333898544311523
RMSE train: 4.099846	val: 4.510388	test: 4.396610
MAE train: 3.414950	val: 3.802015	test: 3.743662

Epoch: 8
Loss: 16.432722091674805
RMSE train: 3.931133	val: 4.268279	test: 4.144175
MAE train: 3.304912	val: 3.629153	test: 3.579070

Epoch: 9
Loss: 15.59763765335083
RMSE train: 3.832906	val: 4.096590	test: 3.937121
MAE train: 3.259147	val: 3.489265	test: 3.441276

Epoch: 10
Loss: 14.435265064239502
RMSE train: 3.808032	val: 3.982021	test: 3.794056
MAE train: 3.250472	val: 3.378513	test: 3.334819

Epoch: 11
Loss: 13.865488529205322
RMSE train: 3.835995	val: 3.948903	test: 3.778130
MAE train: 3.290366	val: 3.336568	test: 3.346191

Epoch: 12
Loss: 13.132818698883057
RMSE train: 3.833483	val: 3.924020	test: 3.824028
MAE train: 3.320338	val: 3.310033	test: 3.407158

Epoch: 13
Loss: 12.400609016418457
RMSE train: 3.804145	val: 3.928725	test: 3.855888
MAE train: 3.332310	val: 3.342259	test: 3.430437

Epoch: 14
Loss: 12.02100419998169
RMSE train: 3.783720	val: 3.955805	test: 3.895827
MAE train: 3.353293	val: 3.416398	test: 3.448959

Epoch: 15
Loss: 11.377947330474854
RMSE train: 3.803924	val: 4.010367	test: 3.947549
MAE train: 3.392278	val: 3.470344	test: 3.495495

Epoch: 16
Loss: 10.864635467529297
RMSE train: 3.890166	val: 4.141091	test: 4.033923
MAE train: 3.494350	val: 3.578776	test: 3.598608

Epoch: 17
Loss: 10.941271305084229
RMSE train: 3.965953	val: 4.256594	test: 4.099214
MAE train: 3.580967	val: 3.676603	test: 3.689728

Epoch: 18
Loss: 10.171246528625488
RMSE train: 3.991759	val: 4.272179	test: 4.095956
MAE train: 3.606118	val: 3.692025	test: 3.704826

Epoch: 19
Loss: 9.864416599273682
RMSE train: 3.956759	val: 4.244955	test: 4.035564
MAE train: 3.569409	val: 3.668067	test: 3.661119

Epoch: 20
Loss: 9.320531845092773
RMSE train: 3.915837	val: 4.185455	test: 3.985664
MAE train: 3.539499	val: 3.628249	test: 3.623802

Epoch: 21
Loss: 9.081853151321411
RMSE train: 3.848309	val: 4.106011	test: 3.933192
MAE train: 3.490720	val: 3.572607	test: 3.577351

Epoch: 22
Loss: 8.844743728637695
RMSE train: 3.757656	val: 4.019119	test: 3.888664
MAE train: 3.416170	val: 3.506391	test: 3.519820Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/freesolv/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/freesolv/random/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/freesolv/random/train_prop=0.8/freesolv_random_6_26-05_09-43-12  ]
[ Using Seed :  6  ]
[ Using device :  cuda:2  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 27.977821350097656
RMSE train: 5.313756	val: 5.449549	test: 5.157009
MAE train: 4.190220	val: 4.672953	test: 3.967954

Epoch: 2
Loss: 26.687328338623047
RMSE train: 5.243188	val: 5.426946	test: 5.131916
MAE train: 4.169842	val: 4.663149	test: 3.972415

Epoch: 3
Loss: 24.940731048583984
RMSE train: 5.025624	val: 5.255590	test: 4.981338
MAE train: 4.006188	val: 4.492687	test: 3.838972

Epoch: 4
Loss: 23.132823944091797
RMSE train: 4.789389	val: 5.074700	test: 4.828606
MAE train: 3.850247	val: 4.317343	test: 3.744884

Epoch: 5
Loss: 21.360970497131348
RMSE train: 4.526573	val: 4.906243	test: 4.654647
MAE train: 3.685526	val: 4.150224	test: 3.674406

Epoch: 6
Loss: 19.808484077453613
RMSE train: 4.253698	val: 4.775742	test: 4.412994
MAE train: 3.506274	val: 4.071061	test: 3.568391

Epoch: 7
Loss: 18.217190742492676
RMSE train: 3.975101	val: 4.638070	test: 4.080576
MAE train: 3.323841	val: 3.979049	test: 3.406098

Epoch: 8
Loss: 16.7819766998291
RMSE train: 3.736163	val: 4.455964	test: 3.739736
MAE train: 3.152370	val: 3.817228	test: 3.225793

Epoch: 9
Loss: 16.01272201538086
RMSE train: 3.530553	val: 4.207864	test: 3.436541
MAE train: 2.992288	val: 3.561104	test: 3.054749

Epoch: 10
Loss: 15.035562992095947
RMSE train: 3.423953	val: 4.045068	test: 3.307707
MAE train: 2.903520	val: 3.441332	test: 2.950815

Epoch: 11
Loss: 14.334277629852295
RMSE train: 3.325813	val: 3.940927	test: 3.269529
MAE train: 2.817531	val: 3.358463	test: 2.926470

Epoch: 12
Loss: 13.50551700592041
RMSE train: 3.313981	val: 3.962522	test: 3.311143
MAE train: 2.822552	val: 3.338165	test: 2.981665

Epoch: 13
Loss: 13.487115383148193
RMSE train: 3.330150	val: 4.003490	test: 3.351868
MAE train: 2.858741	val: 3.354605	test: 3.024141

Epoch: 14
Loss: 12.889153957366943
RMSE train: 3.424081	val: 4.108867	test: 3.451253
MAE train: 2.988234	val: 3.483811	test: 3.123229

Epoch: 15
Loss: 12.023077487945557
RMSE train: 3.552867	val: 4.221338	test: 3.624032
MAE train: 3.138347	val: 3.617457	test: 3.286970

Epoch: 16
Loss: 11.784014225006104
RMSE train: 3.665713	val: 4.304399	test: 3.787568
MAE train: 3.256312	val: 3.712724	test: 3.432710

Epoch: 17
Loss: 11.272767543792725
RMSE train: 3.731106	val: 4.324396	test: 3.897636
MAE train: 3.331187	val: 3.752481	test: 3.533663

Epoch: 18
Loss: 11.03095817565918
RMSE train: 3.767593	val: 4.320182	test: 3.959216
MAE train: 3.376375	val: 3.751014	test: 3.600596

Epoch: 19
Loss: 10.489260196685791
RMSE train: 3.780266	val: 4.315150	test: 3.966756
MAE train: 3.397564	val: 3.742892	test: 3.617178

Epoch: 20
Loss: 9.801572799682617
RMSE train: 3.770582	val: 4.295264	test: 3.965305
MAE train: 3.399442	val: 3.726005	test: 3.618252

Epoch: 21
Loss: 9.599488258361816
RMSE train: 3.734051	val: 4.249149	test: 3.956355
MAE train: 3.365787	val: 3.671874	test: 3.601502

Epoch: 22
Loss: 9.001914024353027
RMSE train: 3.671084	val: 4.189830	test: 3.908535
MAE train: 3.304608	val: 3.602332	test: 3.544357

Epoch: 23
Loss: 9.611665725708008
RMSE train: 3.720319	val: 3.590355	test: 3.899510
MAE train: 3.297410	val: 3.178901	test: 3.445703

Epoch: 24
Loss: 9.519237995147705
RMSE train: 3.567133	val: 3.437510	test: 3.762334
MAE train: 3.151333	val: 3.034369	test: 3.318576

Epoch: 25
Loss: 10.30218505859375
RMSE train: 3.450633	val: 3.320544	test: 3.652598
MAE train: 3.048996	val: 2.933008	test: 3.227503

Epoch: 26
Loss: 9.0255446434021
RMSE train: 3.336851	val: 3.193529	test: 3.545719
MAE train: 2.949753	val: 2.824859	test: 3.136768

Epoch: 27
Loss: 8.340451717376709
RMSE train: 3.247817	val: 3.090404	test: 3.459597
MAE train: 2.875300	val: 2.743079	test: 3.057303

Epoch: 28
Loss: 7.6353676319122314
RMSE train: 3.175678	val: 3.011279	test: 3.376334
MAE train: 2.816084	val: 2.672165	test: 2.976364

Epoch: 29
Loss: 7.359829425811768
RMSE train: 3.158409	val: 2.986994	test: 3.345767
MAE train: 2.810257	val: 2.640831	test: 2.962715

Epoch: 30
Loss: 7.230712413787842
RMSE train: 3.130681	val: 2.957215	test: 3.312682
MAE train: 2.786741	val: 2.605401	test: 2.934013

Epoch: 31
Loss: 6.906914472579956
RMSE train: 3.067128	val: 2.920322	test: 3.258345
MAE train: 2.719974	val: 2.563666	test: 2.864027

Epoch: 32
Loss: 6.288337469100952
RMSE train: 2.992321	val: 2.900084	test: 3.201441
MAE train: 2.634528	val: 2.527935	test: 2.778995

Epoch: 33
Loss: 7.892639875411987
RMSE train: 2.953966	val: 2.887523	test: 3.157341
MAE train: 2.583449	val: 2.501949	test: 2.729912

Epoch: 34
Loss: 6.489530563354492
RMSE train: 2.846704	val: 2.787336	test: 3.036929
MAE train: 2.472027	val: 2.392521	test: 2.615115

Epoch: 35
Loss: 5.857145309448242
RMSE train: 2.697984	val: 2.634363	test: 2.880139
MAE train: 2.325252	val: 2.243698	test: 2.469432

Epoch: 36
Loss: 5.222570180892944
RMSE train: 2.552077	val: 2.466872	test: 2.727357
MAE train: 2.183516	val: 2.085697	test: 2.316114

Epoch: 37
Loss: 5.103731632232666
RMSE train: 2.460443	val: 2.358058	test: 2.634852
MAE train: 2.094290	val: 1.983464	test: 2.224448

Epoch: 38
Loss: 4.9909138679504395
RMSE train: 2.407928	val: 2.298083	test: 2.603319
MAE train: 2.044878	val: 1.928738	test: 2.192034

Epoch: 39
Loss: 4.5018980503082275
RMSE train: 2.404085	val: 2.277090	test: 2.612316
MAE train: 2.036543	val: 1.905492	test: 2.190917

Epoch: 40
Loss: 4.053839087486267
RMSE train: 2.377407	val: 2.232672	test: 2.601013
MAE train: 2.001540	val: 1.863195	test: 2.164776

Epoch: 41
Loss: 3.913506507873535
RMSE train: 2.323351	val: 2.172604	test: 2.564372
MAE train: 1.933598	val: 1.800179	test: 2.104663

Epoch: 42
Loss: 3.6569035053253174
RMSE train: 2.199313	val: 2.063926	test: 2.456451
MAE train: 1.804473	val: 1.691126	test: 1.985815

Epoch: 43
Loss: 3.7025612592697144
RMSE train: 2.054936	val: 1.938878	test: 2.326618
MAE train: 1.663851	val: 1.574435	test: 1.859622

Epoch: 44
Loss: 3.4217026233673096
RMSE train: 1.973448	val: 1.877669	test: 2.248142
MAE train: 1.587520	val: 1.520340	test: 1.790533

Epoch: 45
Loss: 3.00853431224823
RMSE train: 1.913749	val: 1.831777	test: 2.167260
MAE train: 1.538984	val: 1.482963	test: 1.728163

Epoch: 46
Loss: 3.106867790222168
RMSE train: 1.883892	val: 1.802801	test: 2.109172
MAE train: 1.515249	val: 1.457153	test: 1.681401

Epoch: 47
Loss: 3.0341405868530273
RMSE train: 1.814277	val: 1.727868	test: 2.022990
MAE train: 1.449377	val: 1.390578	test: 1.604169

Epoch: 48
Loss: 2.902103900909424
RMSE train: 1.745175	val: 1.698865	test: 1.950142
MAE train: 1.383992	val: 1.347243	test: 1.544181

Epoch: 49
Loss: 2.4677436351776123
RMSE train: 1.700014	val: 1.703639	test: 1.921259
MAE train: 1.336863	val: 1.323763	test: 1.511692

Epoch: 50
Loss: 2.8347874879837036
RMSE train: 1.688345	val: 1.747752	test: 1.919408
MAE train: 1.324497	val: 1.333922	test: 1.499469

Epoch: 51
Loss: 2.282442092895508
RMSE train: 1.662567	val: 1.775725	test: 1.902207
MAE train: 1.299884	val: 1.331659	test: 1.470945

Epoch: 52
Loss: 2.192497968673706
RMSE train: 1.603583	val: 1.773559	test: 1.866583
MAE train: 1.246390	val: 1.313968	test: 1.419206

Epoch: 53
Loss: 2.3553472757339478
RMSE train: 1.527012	val: 1.707067	test: 1.819263
MAE train: 1.187895	val: 1.287019	test: 1.374338

Epoch: 54
Loss: 2.5544393062591553
RMSE train: 1.471761	val: 1.628225	test: 1.763189
MAE train: 1.148301	val: 1.250638	test: 1.341448

Epoch: 55
Loss: 2.1240921020507812
RMSE train: 1.405100	val: 1.548578	test: 1.694530
MAE train: 1.099016	val: 1.190649	test: 1.290464

Epoch: 56
Loss: 1.8505481481552124
RMSE train: 1.324620	val: 1.450561	test: 1.624422
MAE train: 1.033773	val: 1.114316	test: 1.226070

Epoch: 57
Loss: 2.035058856010437
RMSE train: 1.235581	val: 1.349612	test: 1.545778
MAE train: 0.946726	val: 1.029841	test: 1.144791

Epoch: 58
Loss: 1.6651533246040344
RMSE train: 1.105388	val: 1.235671	test: 1.440476
MAE train: 0.828563	val: 0.921319	test: 1.025159

Epoch: 59
Loss: 1.7200106382369995
RMSE train: 1.080207	val: 1.248471	test: 1.415516
MAE train: 0.809617	val: 0.903354	test: 0.990159

Epoch: 60
Loss: 1.691041111946106
RMSE train: 1.081363	val: 1.271212	test: 1.409670
MAE train: 0.810726	val: 0.914112	test: 0.987593

Epoch: 61
Loss: 1.7290112972259521
RMSE train: 1.098812	val: 1.304216	test: 1.415943
MAE train: 0.821747	val: 0.942542	test: 0.996067

Epoch: 62
Loss: 1.6044937372207642
RMSE train: 1.078891	val: 1.295048	test: 1.406133
MAE train: 0.807116	val: 0.943816	test: 0.986203

Epoch: 63
Loss: 1.6957212090492249
RMSE train: 1.049009	val: 1.266810	test: 1.380624
MAE train: 0.783039	val: 0.920616	test: 0.957118

Epoch: 64
Loss: 1.5819151997566223
RMSE train: 1.079188	val: 1.281408	test: 1.391857
MAE train: 0.806193	val: 0.933198	test: 0.974463

Epoch: 65
Loss: 1.458021879196167
RMSE train: 1.055308	val: 1.256099	test: 1.392107
MAE train: 0.787993	val: 0.907335	test: 0.958324

Epoch: 66
Loss: 1.4555588364601135
RMSE train: 1.019320	val: 1.243188	test: 1.381928
MAE train: 0.761946	val: 0.891532	test: 0.949128

Epoch: 67
Loss: 1.876988410949707
RMSE train: 0.990053	val: 1.243430	test: 1.368933
MAE train: 0.740361	val: 0.885714	test: 0.941569

Epoch: 68
Loss: 1.3123866319656372
RMSE train: 0.951742	val: 1.218696	test: 1.339354
MAE train: 0.714253	val: 0.869964	test: 0.921981

Epoch: 69
Loss: 1.2683553099632263
RMSE train: 0.918856	val: 1.223340	test: 1.312710
MAE train: 0.687345	val: 0.854829	test: 0.897065

Epoch: 70
Loss: 1.336141288280487
RMSE train: 0.898582	val: 1.232557	test: 1.304524
MAE train: 0.670407	val: 0.856978	test: 0.882366

Epoch: 71
Loss: 1.3137600421905518
RMSE train: 0.916543	val: 1.259477	test: 1.325930
MAE train: 0.690986	val: 0.892250	test: 0.897102

Epoch: 72
Loss: 1.3358770608901978
RMSE train: 0.928357	val: 1.266913	test: 1.341602
MAE train: 0.701334	val: 0.902965	test: 0.908626

Epoch: 73
Loss: 1.3781961798667908
RMSE train: 0.939419	val: 1.251969	test: 1.348402
MAE train: 0.699628	val: 0.890060	test: 0.916009

Epoch: 74
Loss: 1.6172932386398315
RMSE train: 0.882675	val: 1.168413	test: 1.293025
MAE train: 0.649461	val: 0.835499	test: 0.865789

Epoch: 75
Loss: 1.4951717853546143
RMSE train: 0.880938	val: 1.129236	test: 1.275157
MAE train: 0.640234	val: 0.802428	test: 0.853702

Epoch: 76
Loss: 1.602173089981079
RMSE train: 0.874130	val: 1.108412	test: 1.262882
MAE train: 0.635882	val: 0.789502	test: 0.849967

Epoch: 77
Loss: 1.5226263403892517
RMSE train: 0.923890	val: 1.178501	test: 1.288796
MAE train: 0.687432	val: 0.847249	test: 0.880460

Epoch: 78
Loss: 1.4224629402160645
RMSE train: 0.995554	val: 1.279434	test: 1.340664
MAE train: 0.753671	val: 0.905472	test: 0.922953

Epoch: 79
Loss: 1.4077728390693665
RMSE train: 1.030108	val: 1.334520	test: 1.372355
MAE train: 0.784451	val: 0.943747	test: 0.943754

Epoch: 80
Loss: 1.2539296746253967
RMSE train: 0.965059	val: 1.287157	test: 1.338488
MAE train: 0.736244	val: 0.922922	test: 0.905234

Epoch: 81
Loss: 1.3197020888328552
RMSE train: 0.933457	val: 1.280331	test: 1.353535
MAE train: 0.706211	val: 0.916847	test: 0.909211

Epoch: 82
Loss: 1.22395658493042
RMSE train: 0.938429	val: 1.265984	test: 1.409481
MAE train: 0.684267	val: 0.910468	test: 0.927670

Epoch: 83
Loss: 1.165061354637146
RMSE train: 0.968449	val: 1.228116	test: 1.438598
MAE train: 0.676950	val: 0.887524	test: 0.923828

Epoch: 23
Loss: 9.387754440307617
RMSE train: 3.738466	val: 3.500064	test: 3.965613
MAE train: 3.366159	val: 3.139652	test: 3.535721

Epoch: 24
Loss: 9.624178409576416
RMSE train: 3.676799	val: 3.435060	test: 3.885358
MAE train: 3.312103	val: 3.061290	test: 3.442117

Epoch: 25
Loss: 9.321812152862549
RMSE train: 3.611729	val: 3.412354	test: 3.812640
MAE train: 3.238658	val: 3.012678	test: 3.354068

Epoch: 26
Loss: 7.824996471405029
RMSE train: 3.517949	val: 3.375953	test: 3.718788
MAE train: 3.125869	val: 2.941284	test: 3.243674

Epoch: 27
Loss: 7.822470188140869
RMSE train: 3.435083	val: 3.333213	test: 3.642594
MAE train: 3.032554	val: 2.880800	test: 3.161857

Epoch: 28
Loss: 7.01377272605896
RMSE train: 3.331115	val: 3.240939	test: 3.533007
MAE train: 2.947205	val: 2.790963	test: 3.077629

Epoch: 29
Loss: 7.326521396636963
RMSE train: 3.273079	val: 3.139198	test: 3.483538
MAE train: 2.920537	val: 2.729491	test: 3.063460

Epoch: 30
Loss: 7.070359230041504
RMSE train: 3.241641	val: 3.067898	test: 3.451863
MAE train: 2.910868	val: 2.687445	test: 3.059791

Epoch: 31
Loss: 6.535149335861206
RMSE train: 3.226157	val: 3.004069	test: 3.435405
MAE train: 2.900538	val: 2.643871	test: 3.057342

Epoch: 32
Loss: 6.104588270187378
RMSE train: 3.200413	val: 2.948055	test: 3.414387
MAE train: 2.875703	val: 2.601176	test: 3.040834

Epoch: 33
Loss: 5.976037502288818
RMSE train: 3.206765	val: 2.941103	test: 3.420296
MAE train: 2.880556	val: 2.586031	test: 3.026641

Epoch: 34
Loss: 5.690260171890259
RMSE train: 3.166439	val: 2.946322	test: 3.383134
MAE train: 2.833273	val: 2.576341	test: 2.966400

Epoch: 35
Loss: 5.890160083770752
RMSE train: 3.073693	val: 2.915044	test: 3.293121
MAE train: 2.730563	val: 2.522945	test: 2.860557

Epoch: 36
Loss: 5.018048286437988
RMSE train: 2.927607	val: 2.830914	test: 3.155755
MAE train: 2.572749	val: 2.417864	test: 2.711896

Epoch: 37
Loss: 4.686410427093506
RMSE train: 2.781710	val: 2.696771	test: 3.010507
MAE train: 2.425398	val: 2.286165	test: 2.577817

Epoch: 38
Loss: 4.49762225151062
RMSE train: 2.627292	val: 2.540713	test: 2.852127
MAE train: 2.276587	val: 2.133043	test: 2.435501

Epoch: 39
Loss: 4.190410733222961
RMSE train: 2.518581	val: 2.425298	test: 2.740770
MAE train: 2.174312	val: 2.031788	test: 2.337003

Epoch: 40
Loss: 3.57291841506958
RMSE train: 2.387297	val: 2.298943	test: 2.605565
MAE train: 2.049850	val: 1.930642	test: 2.217206

Epoch: 41
Loss: 3.927857995033264
RMSE train: 2.264427	val: 2.196888	test: 2.482097
MAE train: 1.932196	val: 1.834032	test: 2.100601

Epoch: 42
Loss: 3.2368475198745728
RMSE train: 2.178739	val: 2.141337	test: 2.418476
MAE train: 1.845293	val: 1.776532	test: 2.030219

Epoch: 43
Loss: 3.3787693977355957
RMSE train: 2.146476	val: 2.132172	test: 2.403600
MAE train: 1.800444	val: 1.750089	test: 1.991779

Epoch: 44
Loss: 3.3799017667770386
RMSE train: 2.143822	val: 2.138091	test: 2.409356
MAE train: 1.782791	val: 1.740649	test: 1.969090

Epoch: 45
Loss: 2.822987914085388
RMSE train: 2.120373	val: 2.121245	test: 2.376905
MAE train: 1.761413	val: 1.716302	test: 1.942223

Epoch: 46
Loss: 3.8094165325164795
RMSE train: 2.052578	val: 2.085831	test: 2.302514
MAE train: 1.702978	val: 1.672341	test: 1.882984

Epoch: 47
Loss: 3.0296924114227295
RMSE train: 1.901412	val: 1.969850	test: 2.150217
MAE train: 1.563831	val: 1.566221	test: 1.750976

Epoch: 48
Loss: 2.587893009185791
RMSE train: 1.799558	val: 1.868264	test: 2.031790
MAE train: 1.467865	val: 1.473924	test: 1.642290

Epoch: 49
Loss: 2.427392840385437
RMSE train: 1.702250	val: 1.761404	test: 1.910640
MAE train: 1.373652	val: 1.390107	test: 1.532476

Epoch: 50
Loss: 3.2215728759765625
RMSE train: 1.679922	val: 1.697259	test: 1.850344
MAE train: 1.345207	val: 1.338660	test: 1.483063

Epoch: 51
Loss: 2.4966061115264893
RMSE train: 1.631967	val: 1.614988	test: 1.804553
MAE train: 1.305494	val: 1.274135	test: 1.431593

Epoch: 52
Loss: 2.495972692966461
RMSE train: 1.610803	val: 1.550896	test: 1.789605
MAE train: 1.283241	val: 1.220932	test: 1.405804

Epoch: 53
Loss: 2.2786431312561035
RMSE train: 1.561034	val: 1.517236	test: 1.788557
MAE train: 1.223971	val: 1.180255	test: 1.375177

Epoch: 54
Loss: 1.9477875232696533
RMSE train: 1.546662	val: 1.528189	test: 1.801445
MAE train: 1.192769	val: 1.168438	test: 1.356044

Epoch: 55
Loss: 1.812580645084381
RMSE train: 1.520858	val: 1.532572	test: 1.808356
MAE train: 1.148796	val: 1.159361	test: 1.338890

Epoch: 56
Loss: 2.0636789798736572
RMSE train: 1.458304	val: 1.478543	test: 1.760981
MAE train: 1.095220	val: 1.132240	test: 1.292794

Epoch: 57
Loss: 2.543919801712036
RMSE train: 1.402757	val: 1.446189	test: 1.706372
MAE train: 1.059821	val: 1.130827	test: 1.256245

Epoch: 58
Loss: 1.760093629360199
RMSE train: 1.334933	val: 1.421131	test: 1.620039
MAE train: 1.029273	val: 1.126609	test: 1.212131

Epoch: 59
Loss: 1.8772692680358887
RMSE train: 1.305040	val: 1.431106	test: 1.573892
MAE train: 1.013179	val: 1.130057	test: 1.178521

Epoch: 60
Loss: 1.9514455795288086
RMSE train: 1.295988	val: 1.470744	test: 1.559213
MAE train: 0.996720	val: 1.141277	test: 1.164617

Epoch: 61
Loss: 2.10117506980896
RMSE train: 1.311102	val: 1.490423	test: 1.553142
MAE train: 0.993864	val: 1.134084	test: 1.148826

Epoch: 62
Loss: 1.726504147052765
RMSE train: 1.327889	val: 1.491649	test: 1.562418
MAE train: 1.015898	val: 1.122025	test: 1.154658

Epoch: 63
Loss: 2.3204488158226013
RMSE train: 1.344576	val: 1.460081	test: 1.611448
MAE train: 1.024730	val: 1.099399	test: 1.158440

Epoch: 64
Loss: 1.6413729190826416
RMSE train: 1.309061	val: 1.398763	test: 1.641492
MAE train: 0.976973	val: 1.055488	test: 1.130170

Epoch: 65
Loss: 1.6255894303321838
RMSE train: 1.271454	val: 1.359959	test: 1.647288
MAE train: 0.938054	val: 1.028396	test: 1.115484

Epoch: 66
Loss: 1.7004700303077698
RMSE train: 1.226920	val: 1.359211	test: 1.641789
MAE train: 0.906619	val: 1.032452	test: 1.134528

Epoch: 67
Loss: 1.5317350029945374
RMSE train: 1.239811	val: 1.435339	test: 1.660628
MAE train: 0.946241	val: 1.094752	test: 1.192668

Epoch: 68
Loss: 1.5184457898139954
RMSE train: 1.259403	val: 1.491833	test: 1.674580
MAE train: 0.972378	val: 1.135145	test: 1.216029

Epoch: 69
Loss: 1.5839287638664246
RMSE train: 1.268367	val: 1.511135	test: 1.644185
MAE train: 0.969291	val: 1.145965	test: 1.183888

Epoch: 70
Loss: 1.4315457940101624
RMSE train: 1.265042	val: 1.517472	test: 1.604303
MAE train: 0.944519	val: 1.132564	test: 1.124518

Epoch: 71
Loss: 1.4287018775939941
RMSE train: 1.204423	val: 1.460395	test: 1.556803
MAE train: 0.886687	val: 1.079232	test: 1.076478

Epoch: 72
Loss: 1.4616038799285889
RMSE train: 1.109660	val: 1.378716	test: 1.515620
MAE train: 0.810726	val: 1.009049	test: 1.042056

Epoch: 73
Loss: 1.5512725710868835
RMSE train: 1.023763	val: 1.307268	test: 1.480190
MAE train: 0.742393	val: 0.936636	test: 1.003589

Epoch: 74
Loss: 1.4846262335777283
RMSE train: 0.966811	val: 1.259774	test: 1.464553
MAE train: 0.705744	val: 0.891143	test: 0.987114

Epoch: 75
Loss: 1.2670135498046875
RMSE train: 0.921029	val: 1.244130	test: 1.463596
MAE train: 0.677232	val: 0.871780	test: 0.977738

Epoch: 76
Loss: 1.5709826946258545
RMSE train: 0.938701	val: 1.281038	test: 1.509435
MAE train: 0.698678	val: 0.912770	test: 1.010381

Epoch: 77
Loss: 1.3734604120254517
RMSE train: 0.996520	val: 1.307317	test: 1.564470
MAE train: 0.748665	val: 0.955757	test: 1.061716

Epoch: 78
Loss: 1.2931429743766785
RMSE train: 1.028446	val: 1.303777	test: 1.573934
MAE train: 0.779345	val: 0.975242	test: 1.089142

Epoch: 79
Loss: 1.2795476913452148
RMSE train: 1.080616	val: 1.340954	test: 1.616804
MAE train: 0.811973	val: 1.013824	test: 1.123814

Epoch: 80
Loss: 1.20284104347229
RMSE train: 1.097985	val: 1.356295	test: 1.627434
MAE train: 0.816785	val: 1.025154	test: 1.127150

Epoch: 81
Loss: 1.7572463750839233
RMSE train: 1.054193	val: 1.316450	test: 1.584076
MAE train: 0.782648	val: 0.990805	test: 1.092172

Epoch: 82
Loss: 1.2215261459350586
RMSE train: 0.988676	val: 1.287057	test: 1.543133
MAE train: 0.736032	val: 0.969685	test: 1.067062

Epoch: 83
Loss: 1.5801406502723694
RMSE train: 0.950661	val: 1.265487	test: 1.511821
MAE train: 0.695048	val: 0.939599	test: 1.031049

Epoch: 23
Loss: 8.981053829193115
RMSE train: 3.484614	val: 3.383616	test: 3.780824
MAE train: 3.071066	val: 2.990350	test: 3.320743

Epoch: 24
Loss: 9.283095836639404
RMSE train: 3.440009	val: 3.332910	test: 3.737525
MAE train: 3.032261	val: 2.942377	test: 3.276600

Epoch: 25
Loss: 8.527523040771484
RMSE train: 3.379375	val: 3.246948	test: 3.671722
MAE train: 2.974486	val: 2.865395	test: 3.217257

Epoch: 26
Loss: 8.13976788520813
RMSE train: 3.345021	val: 3.198857	test: 3.631422
MAE train: 2.942517	val: 2.811276	test: 3.176849

Epoch: 27
Loss: 7.975027322769165
RMSE train: 3.303297	val: 3.161323	test: 3.584977
MAE train: 2.902170	val: 2.756128	test: 3.130130

Epoch: 28
Loss: 7.776993274688721
RMSE train: 3.206189	val: 3.090052	test: 3.493749
MAE train: 2.799547	val: 2.656099	test: 3.020577

Epoch: 29
Loss: 7.006568908691406
RMSE train: 3.051782	val: 2.958334	test: 3.353877
MAE train: 2.638241	val: 2.508195	test: 2.866563

Epoch: 30
Loss: 6.8434107303619385
RMSE train: 2.885423	val: 2.806623	test: 3.201283
MAE train: 2.456102	val: 2.344191	test: 2.697910

Epoch: 31
Loss: 6.602659463882446
RMSE train: 2.786917	val: 2.682867	test: 3.112646
MAE train: 2.363909	val: 2.241698	test: 2.619078

Epoch: 32
Loss: 6.001191854476929
RMSE train: 2.715077	val: 2.576567	test: 3.039652
MAE train: 2.312039	val: 2.162506	test: 2.563445

Epoch: 33
Loss: 6.015210151672363
RMSE train: 2.671602	val: 2.515276	test: 2.983544
MAE train: 2.292207	val: 2.122525	test: 2.530116

Epoch: 34
Loss: 5.471147060394287
RMSE train: 2.628461	val: 2.502222	test: 2.939327
MAE train: 2.266899	val: 2.122734	test: 2.491356

Epoch: 35
Loss: 4.9489641189575195
RMSE train: 2.586437	val: 2.491062	test: 2.893549
MAE train: 2.230526	val: 2.118797	test: 2.441398

Epoch: 36
Loss: 4.429755926132202
RMSE train: 2.510975	val: 2.432487	test: 2.816436
MAE train: 2.152891	val: 2.069650	test: 2.367334

Epoch: 37
Loss: 4.30895209312439
RMSE train: 2.454698	val: 2.364716	test: 2.741730
MAE train: 2.106607	val: 2.013461	test: 2.312621

Epoch: 38
Loss: 4.680595397949219
RMSE train: 2.416210	val: 2.307425	test: 2.690482
MAE train: 2.076030	val: 1.963145	test: 2.267909

Epoch: 39
Loss: 4.611776828765869
RMSE train: 2.355545	val: 2.297557	test: 2.647926
MAE train: 2.018504	val: 1.935709	test: 2.216258

Epoch: 40
Loss: 4.959907531738281
RMSE train: 2.250797	val: 2.250092	test: 2.577229
MAE train: 1.898217	val: 1.867433	test: 2.120458

Epoch: 41
Loss: 3.675017476081848
RMSE train: 2.133982	val: 2.194940	test: 2.481456
MAE train: 1.763364	val: 1.784802	test: 1.999299

Epoch: 42
Loss: 3.578208565711975
RMSE train: 2.056822	val: 2.124580	test: 2.405993
MAE train: 1.666019	val: 1.693268	test: 1.898245

Epoch: 43
Loss: 3.650525689125061
RMSE train: 1.998292	val: 2.040810	test: 2.342458
MAE train: 1.615143	val: 1.620148	test: 1.841478

Epoch: 44
Loss: 3.0476813316345215
RMSE train: 1.993824	val: 1.978145	test: 2.312491
MAE train: 1.626816	val: 1.586377	test: 1.839707

Epoch: 45
Loss: 3.0738312005996704
RMSE train: 2.003687	val: 1.957218	test: 2.286599
MAE train: 1.633777	val: 1.568839	test: 1.821332

Epoch: 46
Loss: 3.064476490020752
RMSE train: 1.989604	val: 1.963106	test: 2.255930
MAE train: 1.622689	val: 1.563807	test: 1.789769

Epoch: 47
Loss: 2.8415141105651855
RMSE train: 1.968777	val: 2.004677	test: 2.231222
MAE train: 1.600616	val: 1.574459	test: 1.767002

Epoch: 48
Loss: 2.68230402469635
RMSE train: 1.922004	val: 2.028982	test: 2.203955
MAE train: 1.540386	val: 1.572495	test: 1.724707

Epoch: 49
Loss: 2.688542366027832
RMSE train: 1.857698	val: 2.011117	test: 2.169694
MAE train: 1.478676	val: 1.552938	test: 1.681472

Epoch: 50
Loss: 2.538387417793274
RMSE train: 1.781993	val: 1.954263	test: 2.115621
MAE train: 1.428129	val: 1.507386	test: 1.626838

Epoch: 51
Loss: 2.3739824295043945
RMSE train: 1.660674	val: 1.856226	test: 2.031612
MAE train: 1.318771	val: 1.415245	test: 1.525350

Epoch: 52
Loss: 2.120507001876831
RMSE train: 1.506861	val: 1.735492	test: 1.932830
MAE train: 1.173007	val: 1.303447	test: 1.405713

Epoch: 53
Loss: 2.2026973962783813
RMSE train: 1.356618	val: 1.591640	test: 1.841000
MAE train: 1.025681	val: 1.179231	test: 1.293630

Epoch: 54
Loss: 1.9100801348686218
RMSE train: 1.314048	val: 1.546254	test: 1.814947
MAE train: 0.981643	val: 1.138025	test: 1.229831

Epoch: 55
Loss: 2.068984031677246
RMSE train: 1.305931	val: 1.521708	test: 1.802054
MAE train: 0.984284	val: 1.128025	test: 1.216828

Epoch: 56
Loss: 2.111972689628601
RMSE train: 1.320937	val: 1.502039	test: 1.796460
MAE train: 0.998781	val: 1.129972	test: 1.222543

Epoch: 57
Loss: 1.8230774402618408
RMSE train: 1.308802	val: 1.494246	test: 1.773182
MAE train: 1.000904	val: 1.133440	test: 1.231647

Epoch: 58
Loss: 1.954167366027832
RMSE train: 1.313518	val: 1.514774	test: 1.759724
MAE train: 1.017802	val: 1.153464	test: 1.252745

Epoch: 59
Loss: 2.114770293235779
RMSE train: 1.344529	val: 1.547155	test: 1.767670
MAE train: 1.051221	val: 1.191404	test: 1.275901

Epoch: 60
Loss: 1.869292974472046
RMSE train: 1.340149	val: 1.568477	test: 1.765568
MAE train: 1.049971	val: 1.208602	test: 1.275788

Epoch: 61
Loss: 1.7078356742858887
RMSE train: 1.297763	val: 1.542576	test: 1.743153
MAE train: 1.001556	val: 1.177567	test: 1.232396

Epoch: 62
Loss: 1.532427966594696
RMSE train: 1.245786	val: 1.474888	test: 1.713581
MAE train: 0.936211	val: 1.111743	test: 1.183321

Epoch: 63
Loss: 1.7987263202667236
RMSE train: 1.209767	val: 1.459590	test: 1.696447
MAE train: 0.893375	val: 1.079759	test: 1.139535

Epoch: 64
Loss: 1.627204179763794
RMSE train: 1.181213	val: 1.409947	test: 1.646637
MAE train: 0.861139	val: 1.035246	test: 1.072828

Epoch: 65
Loss: 1.4635713696479797
RMSE train: 1.177469	val: 1.393906	test: 1.590581
MAE train: 0.851744	val: 1.017055	test: 1.031316

Epoch: 66
Loss: 1.5317830443382263
RMSE train: 1.139474	val: 1.388519	test: 1.530319
MAE train: 0.811871	val: 0.992126	test: 0.976758

Epoch: 67
Loss: 1.5332777500152588
RMSE train: 1.031426	val: 1.344338	test: 1.455341
MAE train: 0.739662	val: 0.949716	test: 0.922349

Epoch: 68
Loss: 1.529845118522644
RMSE train: 0.893993	val: 1.274008	test: 1.401274
MAE train: 0.657859	val: 0.891088	test: 0.889960

Epoch: 69
Loss: 1.4549546241760254
RMSE train: 0.909528	val: 1.313067	test: 1.446230
MAE train: 0.690270	val: 0.912340	test: 0.944429

Epoch: 70
Loss: 1.3659307956695557
RMSE train: 0.928042	val: 1.334165	test: 1.486242
MAE train: 0.713239	val: 0.931436	test: 0.981065

Epoch: 71
Loss: 1.3145837187767029
RMSE train: 0.907678	val: 1.317203	test: 1.465873
MAE train: 0.695853	val: 0.922353	test: 0.955049

Epoch: 72
Loss: 1.1963326036930084
RMSE train: 0.900708	val: 1.295596	test: 1.442604
MAE train: 0.687838	val: 0.920844	test: 0.929510

Epoch: 73
Loss: 1.2390657663345337
RMSE train: 0.914384	val: 1.300859	test: 1.449825
MAE train: 0.695740	val: 0.932718	test: 0.943255

Epoch: 74
Loss: 1.2804032564163208
RMSE train: 0.946005	val: 1.318009	test: 1.466741
MAE train: 0.711817	val: 0.951017	test: 0.969614

Epoch: 75
Loss: 1.1631860136985779
RMSE train: 0.980540	val: 1.330035	test: 1.489415
MAE train: 0.731469	val: 0.966524	test: 0.989284

Epoch: 76
Loss: 1.3367648720741272
RMSE train: 1.016750	val: 1.332095	test: 1.515990
MAE train: 0.756486	val: 0.972024	test: 0.997515

Epoch: 77
Loss: 1.1748251914978027
RMSE train: 1.037036	val: 1.346406	test: 1.528083
MAE train: 0.772964	val: 0.969434	test: 0.998296

Epoch: 78
Loss: 1.1312123537063599
RMSE train: 1.024839	val: 1.325608	test: 1.527966
MAE train: 0.759342	val: 0.955166	test: 0.996494

Epoch: 79
Loss: 1.352635383605957
RMSE train: 1.020386	val: 1.317271	test: 1.538918
MAE train: 0.754911	val: 0.955206	test: 1.010497

Epoch: 80
Loss: 1.3281180262565613
RMSE train: 0.979198	val: 1.288136	test: 1.535096
MAE train: 0.730169	val: 0.930704	test: 1.012658

Epoch: 81
Loss: 1.2576467394828796
RMSE train: 0.973007	val: 1.271680	test: 1.524271
MAE train: 0.727408	val: 0.923931	test: 1.003590

Epoch: 82
Loss: 1.4503149390220642
RMSE train: 0.956512	val: 1.279554	test: 1.518311
MAE train: 0.726665	val: 0.939865	test: 1.004130

Epoch: 83
Loss: 1.3827447891235352
RMSE train: 0.965021	val: 1.319099	test: 1.526949
MAE train: 0.728834	val: 0.963316	test: 0.993278

Epoch: 23
Loss: 8.602337837219238
RMSE train: 3.324761	val: 3.653242	test: 3.533983
MAE train: 2.953810	val: 3.184916	test: 3.155401

Epoch: 24
Loss: 8.099900722503662
RMSE train: 3.204948	val: 3.546355	test: 3.421654
MAE train: 2.828927	val: 3.065991	test: 3.038269

Epoch: 25
Loss: 7.84000825881958
RMSE train: 3.140488	val: 3.492403	test: 3.362766
MAE train: 2.758062	val: 3.010082	test: 2.980391

Epoch: 26
Loss: 7.429800510406494
RMSE train: 3.058134	val: 3.425567	test: 3.276518
MAE train: 2.667441	val: 2.933802	test: 2.888692

Epoch: 27
Loss: 7.0228869915008545
RMSE train: 3.004298	val: 3.386507	test: 3.228583
MAE train: 2.610937	val: 2.884677	test: 2.832641

Epoch: 28
Loss: 6.892310619354248
RMSE train: 2.965250	val: 3.336981	test: 3.203125
MAE train: 2.584312	val: 2.841980	test: 2.810330

Epoch: 29
Loss: 6.361412763595581
RMSE train: 2.879507	val: 3.240242	test: 3.149353
MAE train: 2.502061	val: 2.735745	test: 2.731641

Epoch: 30
Loss: 6.32819938659668
RMSE train: 2.819466	val: 3.154902	test: 3.100186
MAE train: 2.452836	val: 2.660079	test: 2.678604

Epoch: 31
Loss: 5.8889055252075195
RMSE train: 2.711949	val: 3.045368	test: 3.015735
MAE train: 2.346797	val: 2.552540	test: 2.579097

Epoch: 32
Loss: 5.718536376953125
RMSE train: 2.565225	val: 2.918141	test: 2.896937
MAE train: 2.193330	val: 2.420568	test: 2.425782

Epoch: 33
Loss: 5.619394063949585
RMSE train: 2.453555	val: 2.831960	test: 2.802082
MAE train: 2.075268	val: 2.331546	test: 2.306684

Epoch: 34
Loss: 4.9255828857421875
RMSE train: 2.358685	val: 2.768399	test: 2.714168
MAE train: 1.971056	val: 2.256298	test: 2.205203

Epoch: 35
Loss: 4.775745868682861
RMSE train: 2.265266	val: 2.700052	test: 2.602331
MAE train: 1.880553	val: 2.188534	test: 2.105567

Epoch: 36
Loss: 4.18012797832489
RMSE train: 2.172165	val: 2.591112	test: 2.478190
MAE train: 1.800053	val: 2.086976	test: 2.031514

Epoch: 37
Loss: 4.468917608261108
RMSE train: 2.123234	val: 2.523682	test: 2.398436
MAE train: 1.760687	val: 2.029312	test: 1.992004

Epoch: 38
Loss: 4.493849039077759
RMSE train: 2.076836	val: 2.460068	test: 2.332203
MAE train: 1.724487	val: 1.974038	test: 1.953926

Epoch: 39
Loss: 3.8224074840545654
RMSE train: 2.073238	val: 2.436225	test: 2.311269
MAE train: 1.717357	val: 1.953297	test: 1.931548

Epoch: 40
Loss: 3.7525051832199097
RMSE train: 2.003090	val: 2.379022	test: 2.236580
MAE train: 1.636436	val: 1.880493	test: 1.850557

Epoch: 41
Loss: 3.4967540502548218
RMSE train: 1.912237	val: 2.328195	test: 2.142863
MAE train: 1.528406	val: 1.812319	test: 1.748948

Epoch: 42
Loss: 3.2681024074554443
RMSE train: 1.835708	val: 2.289647	test: 2.054260
MAE train: 1.447086	val: 1.764069	test: 1.670762

Epoch: 43
Loss: 2.9763070344924927
RMSE train: 1.762725	val: 2.244082	test: 1.989779
MAE train: 1.375481	val: 1.711234	test: 1.614131

Epoch: 44
Loss: 2.590416431427002
RMSE train: 1.697778	val: 2.179144	test: 1.951814
MAE train: 1.331899	val: 1.658828	test: 1.590113

Epoch: 45
Loss: 2.8184731006622314
RMSE train: 1.627261	val: 2.096909	test: 1.920765
MAE train: 1.286900	val: 1.587063	test: 1.544578

Epoch: 46
Loss: 2.6709113121032715
RMSE train: 1.565879	val: 2.041056	test: 1.924584
MAE train: 1.244146	val: 1.532416	test: 1.504047

Epoch: 47
Loss: 2.3249884843826294
RMSE train: 1.513250	val: 1.986505	test: 1.902914
MAE train: 1.180127	val: 1.472097	test: 1.434805

Epoch: 48
Loss: 2.7037346363067627
RMSE train: 1.490794	val: 1.943072	test: 1.893735
MAE train: 1.150934	val: 1.429300	test: 1.385795

Epoch: 49
Loss: 2.240160822868347
RMSE train: 1.462292	val: 1.893200	test: 1.877891
MAE train: 1.118465	val: 1.384896	test: 1.349939

Epoch: 50
Loss: 2.1446237564086914
RMSE train: 1.426131	val: 1.836222	test: 1.831000
MAE train: 1.084063	val: 1.335222	test: 1.306833

Epoch: 51
Loss: 2.2260000705718994
RMSE train: 1.376538	val: 1.774700	test: 1.774426
MAE train: 1.044913	val: 1.288670	test: 1.278803

Epoch: 52
Loss: 2.0887750387191772
RMSE train: 1.345469	val: 1.715334	test: 1.730062
MAE train: 1.019408	val: 1.240241	test: 1.252913

Epoch: 53
Loss: 1.890824556350708
RMSE train: 1.311448	val: 1.676084	test: 1.704774
MAE train: 0.996179	val: 1.216473	test: 1.229323

Epoch: 54
Loss: 1.572928786277771
RMSE train: 1.275687	val: 1.637719	test: 1.690102
MAE train: 0.972664	val: 1.189496	test: 1.199975

Epoch: 55
Loss: 1.7767944931983948
RMSE train: 1.240710	val: 1.617265	test: 1.667476
MAE train: 0.946684	val: 1.139181	test: 1.155865

Epoch: 56
Loss: 1.5594192147254944
RMSE train: 1.196565	val: 1.629079	test: 1.642902
MAE train: 0.909538	val: 1.111242	test: 1.109820

Epoch: 57
Loss: 1.5441340208053589
RMSE train: 1.168814	val: 1.640816	test: 1.633790
MAE train: 0.889252	val: 1.097101	test: 1.083953

Epoch: 58
Loss: 1.7438194155693054
RMSE train: 1.132375	val: 1.617107	test: 1.620985
MAE train: 0.861581	val: 1.072166	test: 1.061288

Epoch: 59
Loss: 1.6994959115982056
RMSE train: 1.078069	val: 1.583430	test: 1.624078
MAE train: 0.819063	val: 1.043784	test: 1.044042

Epoch: 60
Loss: 1.471635341644287
RMSE train: 1.014674	val: 1.554951	test: 1.621130
MAE train: 0.767976	val: 1.026236	test: 1.028218

Epoch: 61
Loss: 1.411291778087616
RMSE train: 0.974124	val: 1.540610	test: 1.620411
MAE train: 0.733368	val: 1.018441	test: 1.010812

Epoch: 62
Loss: 1.3738844394683838
RMSE train: 0.967093	val: 1.502393	test: 1.616964
MAE train: 0.728364	val: 1.016911	test: 1.009288

Epoch: 63
Loss: 1.6671922206878662
RMSE train: 0.972623	val: 1.485121	test: 1.593252
MAE train: 0.729779	val: 1.027333	test: 1.017192

Epoch: 64
Loss: 1.4404828548431396
RMSE train: 0.973731	val: 1.484940	test: 1.531542
MAE train: 0.731377	val: 1.028754	test: 1.010271

Epoch: 65
Loss: 1.4843162894248962
RMSE train: 0.953447	val: 1.494297	test: 1.474423
MAE train: 0.715658	val: 1.013088	test: 0.999898

Epoch: 66
Loss: 1.5289424657821655
RMSE train: 0.967850	val: 1.512449	test: 1.439680
MAE train: 0.723850	val: 1.014330	test: 0.996734

Epoch: 67
Loss: 1.5652030110359192
RMSE train: 0.993241	val: 1.549457	test: 1.450500
MAE train: 0.746289	val: 1.042170	test: 1.007579

Epoch: 68
Loss: 1.426527976989746
RMSE train: 0.988350	val: 1.558999	test: 1.462896
MAE train: 0.742501	val: 1.044881	test: 1.008229

Epoch: 69
Loss: 1.4033976793289185
RMSE train: 0.919458	val: 1.530425	test: 1.443546
MAE train: 0.688630	val: 1.013851	test: 0.970861

Epoch: 70
Loss: 1.393369972705841
RMSE train: 0.841679	val: 1.474336	test: 1.417840
MAE train: 0.634802	val: 0.957636	test: 0.950893

Epoch: 71
Loss: 1.273263931274414
RMSE train: 0.826786	val: 1.484740	test: 1.434625
MAE train: 0.625010	val: 0.961466	test: 0.958349

Epoch: 72
Loss: 1.4750803112983704
RMSE train: 0.825525	val: 1.470495	test: 1.433290
MAE train: 0.621395	val: 0.943654	test: 0.944044

Epoch: 73
Loss: 1.349729835987091
RMSE train: 0.851501	val: 1.465158	test: 1.423502
MAE train: 0.643746	val: 0.955782	test: 0.955929

Epoch: 74
Loss: 1.2325957417488098
RMSE train: 0.912591	val: 1.481961	test: 1.411011
MAE train: 0.698684	val: 1.017124	test: 0.977938

Epoch: 75
Loss: 1.3399614095687866
RMSE train: 0.932037	val: 1.508683	test: 1.413631
MAE train: 0.717733	val: 1.043643	test: 0.977132

Epoch: 76
Loss: 1.2004382014274597
RMSE train: 0.895997	val: 1.527961	test: 1.417568
MAE train: 0.684438	val: 1.034812	test: 0.948893

Epoch: 77
Loss: 1.325539767742157
RMSE train: 0.835149	val: 1.484305	test: 1.409798
MAE train: 0.637003	val: 0.981463	test: 0.910370

Epoch: 78
Loss: 1.1094697713851929
RMSE train: 0.789137	val: 1.425263	test: 1.422058
MAE train: 0.602896	val: 0.928449	test: 0.886391

Epoch: 79
Loss: 1.192996084690094
RMSE train: 0.795842	val: 1.381963	test: 1.440066
MAE train: 0.604726	val: 0.905212	test: 0.880792

Epoch: 80
Loss: 1.0895899832248688
RMSE train: 0.846286	val: 1.381068	test: 1.456075
MAE train: 0.635435	val: 0.918523	test: 0.891958

Epoch: 81
Loss: 1.0331825017929077
RMSE train: 0.895200	val: 1.411683	test: 1.476435
MAE train: 0.666845	val: 0.948990	test: 0.924699

Epoch: 82
Loss: 1.2576141953468323
RMSE train: 0.886214	val: 1.449105	test: 1.485088
MAE train: 0.668964	val: 0.977133	test: 0.940778

Epoch: 83
Loss: 1.3909936547279358
RMSE train: 0.860962	val: 1.454403	test: 1.484195
MAE train: 0.652707	val: 0.978010	test: 0.928892

Epoch: 23
Loss: 9.014223575592041
RMSE train: 3.388106	val: 3.727731	test: 3.579906
MAE train: 2.991351	val: 3.152130	test: 3.204006

Epoch: 24
Loss: 8.357057571411133
RMSE train: 3.260902	val: 3.600807	test: 3.453665
MAE train: 2.871035	val: 3.029620	test: 3.094484

Epoch: 25
Loss: 8.540530443191528
RMSE train: 3.155990	val: 3.483871	test: 3.332749
MAE train: 2.780637	val: 2.925413	test: 2.994821

Epoch: 26
Loss: 7.942316055297852
RMSE train: 3.051939	val: 3.358230	test: 3.212040
MAE train: 2.685756	val: 2.817641	test: 2.894432

Epoch: 27
Loss: 7.7485222816467285
RMSE train: 2.935484	val: 3.212071	test: 3.091385
MAE train: 2.570180	val: 2.691094	test: 2.780046

Epoch: 28
Loss: 6.974252700805664
RMSE train: 2.862818	val: 3.156281	test: 3.027906
MAE train: 2.491310	val: 2.629184	test: 2.706300

Epoch: 29
Loss: 6.605424165725708
RMSE train: 2.812493	val: 3.126652	test: 2.979601
MAE train: 2.437498	val: 2.594464	test: 2.648856

Epoch: 30
Loss: 6.323687791824341
RMSE train: 2.783100	val: 3.122588	test: 2.942862
MAE train: 2.400068	val: 2.578731	test: 2.605514

Epoch: 31
Loss: 5.876974105834961
RMSE train: 2.731692	val: 3.056321	test: 2.890655
MAE train: 2.348438	val: 2.517092	test: 2.552280

Epoch: 32
Loss: 5.638197183609009
RMSE train: 2.671903	val: 2.992479	test: 2.836828
MAE train: 2.291671	val: 2.458698	test: 2.492791

Epoch: 33
Loss: 5.606074810028076
RMSE train: 2.592962	val: 2.914600	test: 2.756699
MAE train: 2.215767	val: 2.388640	test: 2.409699

Epoch: 34
Loss: 5.167442321777344
RMSE train: 2.506449	val: 2.815208	test: 2.664798
MAE train: 2.137142	val: 2.294594	test: 2.319109

Epoch: 35
Loss: 4.7608888149261475
RMSE train: 2.441170	val: 2.750186	test: 2.604859
MAE train: 2.076772	val: 2.228589	test: 2.243918

Epoch: 36
Loss: 4.673213720321655
RMSE train: 2.425378	val: 2.741095	test: 2.598188
MAE train: 2.065307	val: 2.220588	test: 2.219193

Epoch: 37
Loss: 4.510521650314331
RMSE train: 2.369799	val: 2.695672	test: 2.542080
MAE train: 2.009549	val: 2.179592	test: 2.166843

Epoch: 38
Loss: 4.09869384765625
RMSE train: 2.275116	val: 2.604892	test: 2.453723
MAE train: 1.912684	val: 2.094643	test: 2.087526

Epoch: 39
Loss: 3.796509861946106
RMSE train: 2.168987	val: 2.487780	test: 2.353418
MAE train: 1.812222	val: 1.981309	test: 1.998664

Epoch: 40
Loss: 3.7056387662887573
RMSE train: 2.052602	val: 2.355825	test: 2.242881
MAE train: 1.704463	val: 1.852958	test: 1.897772

Epoch: 41
Loss: 3.891292095184326
RMSE train: 1.934180	val: 2.239209	test: 2.140160
MAE train: 1.593284	val: 1.737747	test: 1.795986

Epoch: 42
Loss: 3.762823462486267
RMSE train: 1.821149	val: 2.150955	test: 2.019831
MAE train: 1.473887	val: 1.633163	test: 1.681134

Epoch: 43
Loss: 3.2851496934890747
RMSE train: 1.759365	val: 2.121627	test: 1.935661
MAE train: 1.410025	val: 1.583344	test: 1.605393

Epoch: 44
Loss: 2.788979172706604
RMSE train: 1.717842	val: 2.101686	test: 1.892240
MAE train: 1.371525	val: 1.553029	test: 1.561964

Epoch: 45
Loss: 2.8245773315429688
RMSE train: 1.669883	val: 2.027387	test: 1.879860
MAE train: 1.331116	val: 1.497061	test: 1.541827

Epoch: 46
Loss: 2.676142692565918
RMSE train: 1.624332	val: 1.965683	test: 1.867214
MAE train: 1.289531	val: 1.452843	test: 1.515156

Epoch: 47
Loss: 2.428033709526062
RMSE train: 1.583321	val: 1.922647	test: 1.842002
MAE train: 1.251778	val: 1.422655	test: 1.482431

Epoch: 48
Loss: 2.1784125566482544
RMSE train: 1.573413	val: 1.927480	test: 1.836989
MAE train: 1.256716	val: 1.437634	test: 1.483322

Epoch: 49
Loss: 2.182896375656128
RMSE train: 1.556131	val: 1.924778	test: 1.816168
MAE train: 1.251738	val: 1.434366	test: 1.471264

Epoch: 50
Loss: 2.1689140796661377
RMSE train: 1.540657	val: 1.917433	test: 1.804514
MAE train: 1.239131	val: 1.420403	test: 1.454521

Epoch: 51
Loss: 1.998438835144043
RMSE train: 1.475077	val: 1.895559	test: 1.741151
MAE train: 1.154922	val: 1.368861	test: 1.379120

Epoch: 52
Loss: 2.041214942932129
RMSE train: 1.395534	val: 1.823981	test: 1.663981
MAE train: 1.070092	val: 1.297176	test: 1.303408

Epoch: 53
Loss: 1.7261891961097717
RMSE train: 1.364479	val: 1.773594	test: 1.629086
MAE train: 1.034224	val: 1.256513	test: 1.261965

Epoch: 54
Loss: 1.8053076267242432
RMSE train: 1.303257	val: 1.692522	test: 1.554259
MAE train: 0.971324	val: 1.187688	test: 1.201417

Epoch: 55
Loss: 1.776274561882019
RMSE train: 1.272012	val: 1.645426	test: 1.503721
MAE train: 0.934141	val: 1.141194	test: 1.156851

Epoch: 56
Loss: 1.998138964176178
RMSE train: 1.162684	val: 1.555665	test: 1.406435
MAE train: 0.841090	val: 1.042275	test: 1.048954

Epoch: 57
Loss: 1.832602083683014
RMSE train: 1.099415	val: 1.537567	test: 1.339327
MAE train: 0.797547	val: 1.006021	test: 0.993871

Epoch: 58
Loss: 1.4801537990570068
RMSE train: 1.057914	val: 1.518806	test: 1.302646
MAE train: 0.764253	val: 0.995089	test: 0.958486

Epoch: 59
Loss: 1.632068157196045
RMSE train: 1.024792	val: 1.461080	test: 1.270831
MAE train: 0.738373	val: 0.965351	test: 0.937573

Epoch: 60
Loss: 1.8736799955368042
RMSE train: 0.987081	val: 1.385748	test: 1.235703
MAE train: 0.711610	val: 0.915704	test: 0.890125

Epoch: 61
Loss: 1.5695223212242126
RMSE train: 0.970352	val: 1.320619	test: 1.209177
MAE train: 0.698297	val: 0.868349	test: 0.853014

Epoch: 62
Loss: 1.4207938313484192
RMSE train: 0.962558	val: 1.298074	test: 1.196939
MAE train: 0.695922	val: 0.858064	test: 0.847514

Epoch: 63
Loss: 1.311295211315155
RMSE train: 0.987493	val: 1.348307	test: 1.219434
MAE train: 0.715486	val: 0.895527	test: 0.870038

Epoch: 64
Loss: 1.2484588027000427
RMSE train: 1.034784	val: 1.447906	test: 1.269202
MAE train: 0.760645	val: 0.966042	test: 0.918505

Epoch: 65
Loss: 1.6088880896568298
RMSE train: 1.041633	val: 1.489152	test: 1.268920
MAE train: 0.766029	val: 0.996112	test: 0.927868

Epoch: 66
Loss: 1.3330800533294678
RMSE train: 1.025741	val: 1.467888	test: 1.253232
MAE train: 0.748756	val: 0.961497	test: 0.910426

Epoch: 67
Loss: 1.4921584129333496
RMSE train: 0.984002	val: 1.418558	test: 1.207487
MAE train: 0.710566	val: 0.914679	test: 0.868732

Epoch: 68
Loss: 1.4686845541000366
RMSE train: 0.935424	val: 1.362762	test: 1.146297
MAE train: 0.674297	val: 0.879010	test: 0.830005

Epoch: 69
Loss: 1.7270195484161377
RMSE train: 0.914204	val: 1.337058	test: 1.149685
MAE train: 0.666029	val: 0.876106	test: 0.850805

Epoch: 70
Loss: 1.312129259109497
RMSE train: 0.920618	val: 1.337241	test: 1.198622
MAE train: 0.672421	val: 0.887565	test: 0.863495

Epoch: 71
Loss: 1.2443680167198181
RMSE train: 0.991962	val: 1.401520	test: 1.271058
MAE train: 0.725940	val: 0.944417	test: 0.903707

Epoch: 72
Loss: 1.5426663160324097
RMSE train: 1.053203	val: 1.508607	test: 1.335756
MAE train: 0.790005	val: 1.041362	test: 0.991560

Epoch: 73
Loss: 1.4208664298057556
RMSE train: 1.044220	val: 1.524725	test: 1.347194
MAE train: 0.797305	val: 1.065859	test: 1.006186

Epoch: 74
Loss: 1.5569489002227783
RMSE train: 1.027689	val: 1.477537	test: 1.324198
MAE train: 0.789081	val: 1.023266	test: 0.975172

Epoch: 75
Loss: 1.2420952320098877
RMSE train: 1.030967	val: 1.451384	test: 1.314406
MAE train: 0.787792	val: 0.988156	test: 0.944306

Epoch: 76
Loss: 1.1554614901542664
RMSE train: 1.009239	val: 1.435246	test: 1.297335
MAE train: 0.765358	val: 0.962878	test: 0.920705

Epoch: 77
Loss: 1.320724070072174
RMSE train: 0.964271	val: 1.421658	test: 1.296524
MAE train: 0.724839	val: 0.933040	test: 0.900248

Epoch: 78
Loss: 1.464357078075409
RMSE train: 0.912041	val: 1.364547	test: 1.298884
MAE train: 0.676970	val: 0.882861	test: 0.870442

Epoch: 79
Loss: 1.135338008403778
RMSE train: 0.900579	val: 1.343018	test: 1.334037
MAE train: 0.658228	val: 0.868911	test: 0.880953

Epoch: 80
Loss: 1.110951840877533
RMSE train: 0.886119	val: 1.324074	test: 1.342465
MAE train: 0.647039	val: 0.856297	test: 0.885596

Epoch: 81
Loss: 1.3410491347312927
RMSE train: 0.871731	val: 1.289006	test: 1.308898
MAE train: 0.637238	val: 0.834652	test: 0.861065

Epoch: 82
Loss: 1.25436669588089
RMSE train: 0.891052	val: 1.304272	test: 1.286859
MAE train: 0.654058	val: 0.848833	test: 0.871227

Epoch: 83
Loss: 1.1596161425113678
RMSE train: 0.881876	val: 1.298414	test: 1.242602
MAE train: 0.653374	val: 0.848023	test: 0.860764

Epoch: 23
Loss: 8.371681451797485
RMSE train: 3.691455	val: 3.963586	test: 3.840524
MAE train: 3.352073	val: 3.462904	test: 3.463187

Epoch: 24
Loss: 7.775979042053223
RMSE train: 3.593940	val: 3.862330	test: 3.745446
MAE train: 3.252547	val: 3.369297	test: 3.365936

Epoch: 25
Loss: 7.995019912719727
RMSE train: 3.472134	val: 3.761143	test: 3.621037
MAE train: 3.133854	val: 3.253723	test: 3.248998

Epoch: 26
Loss: 7.027145862579346
RMSE train: 3.350623	val: 3.645262	test: 3.502799
MAE train: 3.018795	val: 3.138065	test: 3.136348

Epoch: 27
Loss: 7.212563991546631
RMSE train: 3.270003	val: 3.571257	test: 3.407343
MAE train: 2.947696	val: 3.063818	test: 3.052775

Epoch: 28
Loss: 6.483173131942749
RMSE train: 3.205504	val: 3.509323	test: 3.327340
MAE train: 2.885257	val: 2.997655	test: 2.972529

Epoch: 29
Loss: 6.287680625915527
RMSE train: 3.160444	val: 3.436804	test: 3.281949
MAE train: 2.844953	val: 2.943820	test: 2.930169

Epoch: 30
Loss: 5.964304685592651
RMSE train: 3.137499	val: 3.387581	test: 3.258787
MAE train: 2.819803	val: 2.903101	test: 2.899064

Epoch: 31
Loss: 5.7943456172943115
RMSE train: 3.147297	val: 3.373583	test: 3.269863
MAE train: 2.815626	val: 2.894805	test: 2.881175

Epoch: 32
Loss: 5.2082202434539795
RMSE train: 3.082076	val: 3.316147	test: 3.217650
MAE train: 2.745988	val: 2.837866	test: 2.810379

Epoch: 33
Loss: 5.2823169231414795
RMSE train: 2.966008	val: 3.235862	test: 3.119902
MAE train: 2.626480	val: 2.749661	test: 2.706490

Epoch: 34
Loss: 4.5977113246917725
RMSE train: 2.833711	val: 3.117707	test: 2.998653
MAE train: 2.503677	val: 2.618826	test: 2.617264

Epoch: 35
Loss: 4.386608242988586
RMSE train: 2.766498	val: 3.034777	test: 2.934995
MAE train: 2.442068	val: 2.533530	test: 2.567465

Epoch: 36
Loss: 4.180305480957031
RMSE train: 2.695007	val: 2.941048	test: 2.865987
MAE train: 2.374044	val: 2.441629	test: 2.502925

Epoch: 37
Loss: 4.046755075454712
RMSE train: 2.617227	val: 2.873756	test: 2.781176
MAE train: 2.295693	val: 2.366182	test: 2.414048

Epoch: 38
Loss: 3.695858597755432
RMSE train: 2.482771	val: 2.793616	test: 2.666472
MAE train: 2.145426	val: 2.265728	test: 2.274535

Epoch: 39
Loss: 3.610877752304077
RMSE train: 2.270674	val: 2.624985	test: 2.481046
MAE train: 1.925665	val: 2.094095	test: 2.085732

Epoch: 40
Loss: 3.1822999715805054
RMSE train: 2.113276	val: 2.476424	test: 2.343021
MAE train: 1.761711	val: 1.946494	test: 1.930484

Epoch: 41
Loss: 3.0377864837646484
RMSE train: 2.005280	val: 2.357433	test: 2.246780
MAE train: 1.656792	val: 1.833768	test: 1.837549

Epoch: 42
Loss: 3.2058303356170654
RMSE train: 1.924136	val: 2.251918	test: 2.185274
MAE train: 1.588477	val: 1.756179	test: 1.792475

Epoch: 43
Loss: 3.0098516941070557
RMSE train: 1.844512	val: 2.170069	test: 2.099291
MAE train: 1.522733	val: 1.690478	test: 1.741810

Epoch: 44
Loss: 2.5129971504211426
RMSE train: 1.795355	val: 2.130487	test: 2.049799
MAE train: 1.478114	val: 1.649620	test: 1.698163

Epoch: 45
Loss: 2.565272569656372
RMSE train: 1.754850	val: 2.110588	test: 2.014652
MAE train: 1.440216	val: 1.628232	test: 1.661642

Epoch: 46
Loss: 2.5522865056991577
RMSE train: 1.721368	val: 2.106543	test: 1.987373
MAE train: 1.397978	val: 1.625022	test: 1.612144

Epoch: 47
Loss: 2.335423469543457
RMSE train: 1.687941	val: 2.084404	test: 1.971733
MAE train: 1.344021	val: 1.592660	test: 1.555272

Epoch: 48
Loss: 2.1127880811691284
RMSE train: 1.678005	val: 2.060468	test: 1.986922
MAE train: 1.311927	val: 1.563463	test: 1.522120

Epoch: 49
Loss: 1.9806312322616577
RMSE train: 1.655069	val: 2.027213	test: 1.980232
MAE train: 1.283922	val: 1.519474	test: 1.483182

Epoch: 50
Loss: 2.117335319519043
RMSE train: 1.662893	val: 2.005295	test: 1.992770
MAE train: 1.286848	val: 1.487552	test: 1.471490

Epoch: 51
Loss: 1.8105303645133972
RMSE train: 1.581034	val: 1.938676	test: 1.906255
MAE train: 1.225109	val: 1.415602	test: 1.410856

Epoch: 52
Loss: 1.7051736116409302
RMSE train: 1.480824	val: 1.872328	test: 1.796577
MAE train: 1.139229	val: 1.340347	test: 1.330970

Epoch: 53
Loss: 1.9293890595436096
RMSE train: 1.415884	val: 1.842774	test: 1.751763
MAE train: 1.073249	val: 1.297612	test: 1.284241

Epoch: 54
Loss: 1.8594709038734436
RMSE train: 1.382984	val: 1.816975	test: 1.732194
MAE train: 1.031735	val: 1.256507	test: 1.261951

Epoch: 55
Loss: 1.7049723863601685
RMSE train: 1.397497	val: 1.802522	test: 1.765831
MAE train: 1.021895	val: 1.241437	test: 1.254897

Epoch: 56
Loss: 1.7611021995544434
RMSE train: 1.358355	val: 1.710866	test: 1.745185
MAE train: 0.977775	val: 1.173576	test: 1.202790

Epoch: 57
Loss: 1.6558542251586914
RMSE train: 1.309199	val: 1.627828	test: 1.684655
MAE train: 0.950875	val: 1.119915	test: 1.150985

Epoch: 58
Loss: 1.7375941276550293
RMSE train: 1.254183	val: 1.623880	test: 1.609330
MAE train: 0.918506	val: 1.114078	test: 1.129663

Epoch: 59
Loss: 1.6139366030693054
RMSE train: 1.202728	val: 1.642998	test: 1.543810
MAE train: 0.883526	val: 1.115412	test: 1.083976

Epoch: 60
Loss: 1.400081217288971
RMSE train: 1.146418	val: 1.652687	test: 1.465696
MAE train: 0.845972	val: 1.118827	test: 1.033870

Epoch: 61
Loss: 1.6017600893974304
RMSE train: 1.085689	val: 1.659236	test: 1.356031
MAE train: 0.805251	val: 1.112242	test: 0.978872

Epoch: 62
Loss: 1.5834810733795166
RMSE train: 1.050228	val: 1.605385	test: 1.273474
MAE train: 0.773638	val: 1.053856	test: 0.932223

Epoch: 63
Loss: 1.7439033389091492
RMSE train: 1.031543	val: 1.547414	test: 1.248280
MAE train: 0.761584	val: 1.008291	test: 0.917044

Epoch: 64
Loss: 1.474925458431244
RMSE train: 1.035523	val: 1.505618	test: 1.267752
MAE train: 0.761794	val: 0.976575	test: 0.907655

Epoch: 65
Loss: 1.4150248765945435
RMSE train: 1.084778	val: 1.505392	test: 1.344720
MAE train: 0.792041	val: 0.997843	test: 0.948126

Epoch: 66
Loss: 1.321774959564209
RMSE train: 1.119375	val: 1.524881	test: 1.407200
MAE train: 0.822638	val: 1.030942	test: 0.992595

Epoch: 67
Loss: 1.400867998600006
RMSE train: 1.131632	val: 1.564478	test: 1.432225
MAE train: 0.834707	val: 1.060600	test: 1.002754

Epoch: 68
Loss: 1.3563374876976013
RMSE train: 1.097103	val: 1.576324	test: 1.407174
MAE train: 0.804087	val: 1.065277	test: 0.958417

Epoch: 69
Loss: 1.163010597229004
RMSE train: 1.033037	val: 1.540729	test: 1.376268
MAE train: 0.752041	val: 1.033806	test: 0.905960

Epoch: 70
Loss: 1.5229249596595764
RMSE train: 0.966912	val: 1.477835	test: 1.326309
MAE train: 0.702683	val: 0.974766	test: 0.881582

Epoch: 71
Loss: 1.2710950374603271
RMSE train: 0.941514	val: 1.421882	test: 1.285708
MAE train: 0.693778	val: 0.920458	test: 0.886174

Epoch: 72
Loss: 1.2285386323928833
RMSE train: 0.937903	val: 1.382646	test: 1.259977
MAE train: 0.696766	val: 0.893782	test: 0.888252

Epoch: 73
Loss: 1.290164828300476
RMSE train: 0.993786	val: 1.421326	test: 1.324489
MAE train: 0.737262	val: 0.932252	test: 0.938970

Epoch: 74
Loss: 1.3185769319534302
RMSE train: 1.035005	val: 1.460443	test: 1.383256
MAE train: 0.762451	val: 0.962509	test: 0.977664

Epoch: 75
Loss: 1.2151365876197815
RMSE train: 1.040244	val: 1.455474	test: 1.391331
MAE train: 0.763003	val: 0.955408	test: 0.962391

Epoch: 76
Loss: 1.200975775718689
RMSE train: 1.036318	val: 1.454611	test: 1.365917
MAE train: 0.763632	val: 0.950299	test: 0.935520

Epoch: 77
Loss: 1.1916886568069458
RMSE train: 1.007024	val: 1.438678	test: 1.319911
MAE train: 0.747691	val: 0.942237	test: 0.905531

Epoch: 78
Loss: 1.2353823781013489
RMSE train: 0.972199	val: 1.445820	test: 1.293344
MAE train: 0.710973	val: 0.948974	test: 0.882899

Epoch: 79
Loss: 1.2555553317070007
RMSE train: 0.957526	val: 1.426319	test: 1.309799
MAE train: 0.701270	val: 0.943460	test: 0.901442

Epoch: 80
Loss: 1.132230818271637
RMSE train: 0.934884	val: 1.409621	test: 1.311106
MAE train: 0.688972	val: 0.939107	test: 0.907880

Epoch: 81
Loss: 1.0452565848827362
RMSE train: 0.900452	val: 1.345307	test: 1.300727
MAE train: 0.670602	val: 0.908409	test: 0.897669

Epoch: 82
Loss: 1.2082377672195435
RMSE train: 0.881788	val: 1.333117	test: 1.290499
MAE train: 0.654944	val: 0.894623	test: 0.886686

Epoch: 83
Loss: 1.1822926998138428
RMSE train: 0.816330	val: 1.277793	test: 1.238779
MAE train: 0.600735	val: 0.841287	test: 0.846009

Epoch: 23
Loss: 9.308008193969727
RMSE train: 3.512614	val: 4.235068	test: 3.637526
MAE train: 3.100992	val: 3.691071	test: 3.179430

Epoch: 24
Loss: 8.90658950805664
RMSE train: 3.395901	val: 4.150961	test: 3.495175
MAE train: 2.991665	val: 3.606342	test: 3.079008

Epoch: 25
Loss: 8.391633033752441
RMSE train: 3.339287	val: 4.059408	test: 3.433419
MAE train: 2.956109	val: 3.533872	test: 3.045581

Epoch: 26
Loss: 8.262158870697021
RMSE train: 3.286630	val: 3.969967	test: 3.385561
MAE train: 2.921012	val: 3.462937	test: 3.020626

Epoch: 27
Loss: 7.769900321960449
RMSE train: 3.284311	val: 3.916491	test: 3.393718
MAE train: 2.930289	val: 3.426379	test: 3.028936

Epoch: 28
Loss: 7.116193056106567
RMSE train: 3.239060	val: 3.850882	test: 3.348238
MAE train: 2.887192	val: 3.358930	test: 2.983117

Epoch: 29
Loss: 6.932559013366699
RMSE train: 3.165902	val: 3.785106	test: 3.262507
MAE train: 2.808013	val: 3.276651	test: 2.896398

Epoch: 30
Loss: 6.8075079917907715
RMSE train: 3.114811	val: 3.737581	test: 3.202533
MAE train: 2.746986	val: 3.216303	test: 2.823161

Epoch: 31
Loss: 6.334651231765747
RMSE train: 3.047861	val: 3.679118	test: 3.123285
MAE train: 2.670142	val: 3.155221	test: 2.738626

Epoch: 32
Loss: 6.162925720214844
RMSE train: 2.978344	val: 3.618824	test: 3.066783
MAE train: 2.593070	val: 3.097110	test: 2.661409

Epoch: 33
Loss: 5.512230157852173
RMSE train: 2.929228	val: 3.555200	test: 3.052458
MAE train: 2.552362	val: 3.047973	test: 2.629545

Epoch: 34
Loss: 5.324591875076294
RMSE train: 2.899399	val: 3.506367	test: 3.027622
MAE train: 2.534978	val: 3.009841	test: 2.610809

Epoch: 35
Loss: 5.2204132080078125
RMSE train: 2.848210	val: 3.445933	test: 2.968187
MAE train: 2.496889	val: 2.957836	test: 2.576307

Epoch: 36
Loss: 4.843939781188965
RMSE train: 2.768435	val: 3.379743	test: 2.852751
MAE train: 2.421553	val: 2.896006	test: 2.487596

Epoch: 37
Loss: 4.718466639518738
RMSE train: 2.649949	val: 3.289594	test: 2.716699
MAE train: 2.301362	val: 2.799617	test: 2.354507

Epoch: 38
Loss: 4.234037399291992
RMSE train: 2.556055	val: 3.190735	test: 2.612673
MAE train: 2.206098	val: 2.695719	test: 2.245504

Epoch: 39
Loss: 3.95195472240448
RMSE train: 2.480397	val: 3.107444	test: 2.506055
MAE train: 2.118071	val: 2.589083	test: 2.139193

Epoch: 40
Loss: 3.762188196182251
RMSE train: 2.437586	val: 3.027830	test: 2.493797
MAE train: 2.080178	val: 2.513120	test: 2.109487

Epoch: 41
Loss: 3.5272778272628784
RMSE train: 2.373415	val: 2.922804	test: 2.443591
MAE train: 2.026241	val: 2.411641	test: 2.068614

Epoch: 42
Loss: 3.4247254133224487
RMSE train: 2.258333	val: 2.812214	test: 2.349423
MAE train: 1.926024	val: 2.299125	test: 1.996444

Epoch: 43
Loss: 3.093580484390259
RMSE train: 2.111690	val: 2.704975	test: 2.208067
MAE train: 1.785548	val: 2.179356	test: 1.876875

Epoch: 44
Loss: 3.300735354423523
RMSE train: 1.971952	val: 2.631948	test: 2.028260
MAE train: 1.643956	val: 2.065956	test: 1.741080

Epoch: 45
Loss: 2.883700370788574
RMSE train: 1.872848	val: 2.582607	test: 1.893224
MAE train: 1.526545	val: 1.981303	test: 1.609800

Epoch: 46
Loss: 2.5679099559783936
RMSE train: 1.821393	val: 2.523091	test: 1.819639
MAE train: 1.481094	val: 1.923432	test: 1.556326

Epoch: 47
Loss: 2.285989761352539
RMSE train: 1.784436	val: 2.441203	test: 1.801871
MAE train: 1.470387	val: 1.896523	test: 1.556780

Epoch: 48
Loss: 2.361961245536804
RMSE train: 1.762117	val: 2.364629	test: 1.807148
MAE train: 1.460815	val: 1.870931	test: 1.551024

Epoch: 49
Loss: 2.3724849224090576
RMSE train: 1.738852	val: 2.283434	test: 1.793829
MAE train: 1.418440	val: 1.799605	test: 1.493498

Epoch: 50
Loss: 2.35091769695282
RMSE train: 1.692763	val: 2.223870	test: 1.713342
MAE train: 1.338428	val: 1.710660	test: 1.381627

Epoch: 51
Loss: 2.1474387645721436
RMSE train: 1.632302	val: 2.172781	test: 1.641736
MAE train: 1.269142	val: 1.637781	test: 1.299920

Epoch: 52
Loss: 1.9353798627853394
RMSE train: 1.536911	val: 2.086525	test: 1.563338
MAE train: 1.201425	val: 1.571629	test: 1.249629

Epoch: 53
Loss: 2.0377718806266785
RMSE train: 1.429773	val: 2.001263	test: 1.467844
MAE train: 1.118752	val: 1.500480	test: 1.185908

Epoch: 54
Loss: 1.9835218787193298
RMSE train: 1.354114	val: 1.919842	test: 1.389681
MAE train: 1.052027	val: 1.422699	test: 1.128067

Epoch: 55
Loss: 2.166211247444153
RMSE train: 1.314800	val: 1.874687	test: 1.351757
MAE train: 1.012297	val: 1.369134	test: 1.097520

Epoch: 56
Loss: 1.9577407240867615
RMSE train: 1.301282	val: 1.864291	test: 1.332425
MAE train: 0.999118	val: 1.348066	test: 1.073632

Epoch: 57
Loss: 1.713314950466156
RMSE train: 1.301642	val: 1.852470	test: 1.332596
MAE train: 0.999014	val: 1.332263	test: 1.059256

Epoch: 58
Loss: 1.5049118995666504
RMSE train: 1.327430	val: 1.846570	test: 1.373022
MAE train: 1.016338	val: 1.330095	test: 1.064909

Epoch: 59
Loss: 1.4541704058647156
RMSE train: 1.314312	val: 1.800545	test: 1.391533
MAE train: 0.986705	val: 1.298644	test: 1.039619

Epoch: 60
Loss: 1.5254520177841187
RMSE train: 1.240422	val: 1.757224	test: 1.320949
MAE train: 0.916291	val: 1.233689	test: 0.967917

Epoch: 61
Loss: 1.4852441549301147
RMSE train: 1.136068	val: 1.681118	test: 1.223604
MAE train: 0.835164	val: 1.149818	test: 0.900400

Epoch: 62
Loss: 1.612415075302124
RMSE train: 1.062650	val: 1.622549	test: 1.171275
MAE train: 0.788356	val: 1.110627	test: 0.880664

Epoch: 63
Loss: 1.5480340123176575
RMSE train: 1.026825	val: 1.584657	test: 1.196554
MAE train: 0.768543	val: 1.085896	test: 0.894798

Epoch: 64
Loss: 1.3584111332893372
RMSE train: 1.002526	val: 1.540870	test: 1.233874
MAE train: 0.756571	val: 1.060400	test: 0.917410

Epoch: 65
Loss: 1.7067919373512268
RMSE train: 0.998267	val: 1.526065	test: 1.329317
MAE train: 0.754317	val: 1.059127	test: 0.926601

Epoch: 66
Loss: 1.17860546708107
RMSE train: 1.048316	val: 1.515459	test: 1.512781
MAE train: 0.782277	val: 1.071823	test: 0.955859

Epoch: 67
Loss: 1.286571979522705
RMSE train: 1.096982	val: 1.535375	test: 1.577893
MAE train: 0.806713	val: 1.099497	test: 0.973615

Epoch: 68
Loss: 1.2758397459983826
RMSE train: 1.141815	val: 1.611922	test: 1.489569
MAE train: 0.857258	val: 1.146210	test: 0.971670

Epoch: 69
Loss: 1.3406152129173279
RMSE train: 1.113843	val: 1.636962	test: 1.355161
MAE train: 0.833153	val: 1.156535	test: 0.935162

Epoch: 70
Loss: 1.2494285106658936
RMSE train: 1.049348	val: 1.633277	test: 1.187173
MAE train: 0.783964	val: 1.134988	test: 0.886269

Epoch: 71
Loss: 1.1203301548957825
RMSE train: 0.991862	val: 1.591607	test: 1.064827
MAE train: 0.736347	val: 1.088003	test: 0.830227

Epoch: 72
Loss: 1.257352352142334
RMSE train: 0.940672	val: 1.522864	test: 1.021140
MAE train: 0.698774	val: 1.038498	test: 0.797438

Epoch: 73
Loss: 1.238634467124939
RMSE train: 0.922044	val: 1.497117	test: 1.045574
MAE train: 0.689205	val: 1.016033	test: 0.788874

Epoch: 74
Loss: 1.2646748423576355
RMSE train: 0.938079	val: 1.528055	test: 1.076397
MAE train: 0.691300	val: 1.040823	test: 0.784657

Epoch: 75
Loss: 1.3486799001693726
RMSE train: 0.968612	val: 1.566015	test: 1.105999
MAE train: 0.716996	val: 1.074925	test: 0.800749

Epoch: 76
Loss: 1.391791045665741
RMSE train: 0.953393	val: 1.532235	test: 1.112458
MAE train: 0.711797	val: 1.053828	test: 0.815465

Epoch: 77
Loss: 1.5397230386734009
RMSE train: 1.044463	val: 1.557152	test: 1.217136
MAE train: 0.790468	val: 1.085137	test: 0.926985

Epoch: 78
Loss: 1.2738524079322815
RMSE train: 1.095582	val: 1.569026	test: 1.270486
MAE train: 0.828854	val: 1.097346	test: 0.965348

Epoch: 79
Loss: 1.389134705066681
RMSE train: 1.034010	val: 1.527007	test: 1.198251
MAE train: 0.774888	val: 1.055663	test: 0.916237

Epoch: 80
Loss: 1.2231834530830383
RMSE train: 0.975252	val: 1.479806	test: 1.152124
MAE train: 0.723004	val: 1.014086	test: 0.866120

Epoch: 81
Loss: 1.1116496920585632
RMSE train: 0.904643	val: 1.442178	test: 1.095966
MAE train: 0.666309	val: 0.977332	test: 0.830616

Epoch: 82
Loss: 1.3005974292755127
RMSE train: 0.867449	val: 1.467914	test: 1.047220
MAE train: 0.641760	val: 0.996520	test: 0.798380

Epoch: 83
Loss: 1.2738946080207825
RMSE train: 0.883507	val: 1.478766	test: 1.065978
MAE train: 0.652566	val: 0.988186	test: 0.814961

Epoch: 23
Loss: 8.582472801208496
RMSE train: 3.586130	val: 4.133444	test: 3.826969
MAE train: 3.220610	val: 3.551787	test: 3.460412

Epoch: 24
Loss: 8.309384107589722
RMSE train: 3.508826	val: 4.085071	test: 3.741667
MAE train: 3.149484	val: 3.509065	test: 3.379398

Epoch: 25
Loss: 7.718398571014404
RMSE train: 3.406185	val: 4.000798	test: 3.641558
MAE train: 3.060391	val: 3.430905	test: 3.287303

Epoch: 26
Loss: 7.435137510299683
RMSE train: 3.324450	val: 3.929946	test: 3.544084
MAE train: 2.989674	val: 3.373410	test: 3.210216

Epoch: 27
Loss: 7.2760889530181885
RMSE train: 3.266279	val: 3.879193	test: 3.443272
MAE train: 2.940169	val: 3.337429	test: 3.142512

Epoch: 28
Loss: 6.9737770557403564
RMSE train: 3.230860	val: 3.834283	test: 3.406127
MAE train: 2.908728	val: 3.303702	test: 3.106488

Epoch: 29
Loss: 6.601920127868652
RMSE train: 3.220257	val: 3.821476	test: 3.414946
MAE train: 2.889969	val: 3.281621	test: 3.081268

Epoch: 30
Loss: 6.168774604797363
RMSE train: 3.178974	val: 3.781716	test: 3.397264
MAE train: 2.836658	val: 3.220716	test: 3.016966

Epoch: 31
Loss: 5.855100870132446
RMSE train: 3.108166	val: 3.712392	test: 3.358219
MAE train: 2.763386	val: 3.164339	test: 2.927891

Epoch: 32
Loss: 5.6529221534729
RMSE train: 3.024249	val: 3.633305	test: 3.279065
MAE train: 2.682133	val: 3.099985	test: 2.826072

Epoch: 33
Loss: 5.352323055267334
RMSE train: 2.949457	val: 3.538363	test: 3.228946
MAE train: 2.607926	val: 3.033814	test: 2.757763

Epoch: 34
Loss: 5.1295225620269775
RMSE train: 2.874138	val: 3.446514	test: 3.139691
MAE train: 2.532252	val: 2.954121	test: 2.673720

Epoch: 35
Loss: 4.730676174163818
RMSE train: 2.784212	val: 3.359034	test: 3.006794
MAE train: 2.444715	val: 2.851597	test: 2.564626

Epoch: 36
Loss: 4.490127444267273
RMSE train: 2.666387	val: 3.249248	test: 2.853209
MAE train: 2.324938	val: 2.717001	test: 2.438096

Epoch: 37
Loss: 4.232489705085754
RMSE train: 2.574610	val: 3.151700	test: 2.764296
MAE train: 2.223735	val: 2.588969	test: 2.339087

Epoch: 38
Loss: 3.8038346767425537
RMSE train: 2.478660	val: 3.024718	test: 2.670024
MAE train: 2.126452	val: 2.463952	test: 2.246228

Epoch: 39
Loss: 4.057435750961304
RMSE train: 2.402747	val: 2.914519	test: 2.591428
MAE train: 2.060297	val: 2.377253	test: 2.172522

Epoch: 40
Loss: 3.9160090684890747
RMSE train: 2.319380	val: 2.816242	test: 2.515937
MAE train: 1.985094	val: 2.298335	test: 2.093905

Epoch: 41
Loss: 3.2813085317611694
RMSE train: 2.227617	val: 2.707538	test: 2.457318
MAE train: 1.896992	val: 2.185931	test: 2.010253

Epoch: 42
Loss: 3.2144904136657715
RMSE train: 2.163043	val: 2.638828	test: 2.432618
MAE train: 1.823543	val: 2.088608	test: 1.927975

Epoch: 43
Loss: 3.0066123008728027
RMSE train: 2.059143	val: 2.569634	test: 2.315167
MAE train: 1.719749	val: 1.993214	test: 1.814670

Epoch: 44
Loss: 2.8845549821853638
RMSE train: 1.922453	val: 2.483112	test: 2.131024
MAE train: 1.580131	val: 1.878621	test: 1.671687

Epoch: 45
Loss: 2.7429623007774353
RMSE train: 1.775308	val: 2.348857	test: 1.976219
MAE train: 1.440339	val: 1.743815	test: 1.561497

Epoch: 46
Loss: 2.569313406944275
RMSE train: 1.680751	val: 2.246255	test: 1.872981
MAE train: 1.364230	val: 1.660403	test: 1.503519

Epoch: 47
Loss: 2.472479820251465
RMSE train: 1.625174	val: 2.167548	test: 1.879086
MAE train: 1.318440	val: 1.597653	test: 1.474032

Epoch: 48
Loss: 2.1862491369247437
RMSE train: 1.560642	val: 2.084085	test: 1.895932
MAE train: 1.253908	val: 1.536947	test: 1.427044

Epoch: 49
Loss: 2.3229479789733887
RMSE train: 1.493306	val: 2.029626	test: 1.866732
MAE train: 1.183092	val: 1.493859	test: 1.357137

Epoch: 50
Loss: 2.0166571140289307
RMSE train: 1.452104	val: 1.994554	test: 1.882326
MAE train: 1.129695	val: 1.460756	test: 1.311258

Epoch: 51
Loss: 2.057410717010498
RMSE train: 1.427774	val: 1.974135	test: 1.903510
MAE train: 1.097390	val: 1.424496	test: 1.277979

Epoch: 52
Loss: 1.9493760466575623
RMSE train: 1.377553	val: 1.934938	test: 1.855745
MAE train: 1.063510	val: 1.385872	test: 1.257803

Epoch: 53
Loss: 2.051919460296631
RMSE train: 1.331903	val: 1.908969	test: 1.771163
MAE train: 1.045618	val: 1.354187	test: 1.259626

Epoch: 54
Loss: 1.733916997909546
RMSE train: 1.337144	val: 1.904469	test: 1.749684
MAE train: 1.060144	val: 1.340211	test: 1.257511

Epoch: 55
Loss: 1.82044517993927
RMSE train: 1.307604	val: 1.869763	test: 1.718625
MAE train: 1.036516	val: 1.299144	test: 1.240191

Epoch: 56
Loss: 1.692651629447937
RMSE train: 1.269710	val: 1.836753	test: 1.655953
MAE train: 0.998307	val: 1.254091	test: 1.180766

Epoch: 57
Loss: 1.711179494857788
RMSE train: 1.233448	val: 1.794382	test: 1.578728
MAE train: 0.965531	val: 1.206555	test: 1.116829

Epoch: 58
Loss: 1.4840741157531738
RMSE train: 1.227125	val: 1.780418	test: 1.609596
MAE train: 0.953274	val: 1.206193	test: 1.093746

Epoch: 59
Loss: 1.6603578925132751
RMSE train: 1.277770	val: 1.802000	test: 1.704547
MAE train: 0.980207	val: 1.236058	test: 1.115581

Epoch: 60
Loss: 1.3329529166221619
RMSE train: 1.239622	val: 1.796693	test: 1.659547
MAE train: 0.935434	val: 1.230822	test: 1.062019

Epoch: 61
Loss: 1.5969018936157227
RMSE train: 1.212721	val: 1.797206	test: 1.612257
MAE train: 0.906221	val: 1.237845	test: 1.021532

Epoch: 62
Loss: 1.2936161756515503
RMSE train: 1.099561	val: 1.719312	test: 1.497152
MAE train: 0.819943	val: 1.183526	test: 0.965952

Epoch: 63
Loss: 1.4682847261428833
RMSE train: 1.060050	val: 1.656289	test: 1.470491
MAE train: 0.792093	val: 1.135750	test: 0.976905

Epoch: 64
Loss: 1.3543921113014221
RMSE train: 1.055560	val: 1.629114	test: 1.413132
MAE train: 0.805515	val: 1.119079	test: 1.009163

Epoch: 65
Loss: 1.434440553188324
RMSE train: 1.062484	val: 1.637511	test: 1.380105
MAE train: 0.821526	val: 1.116190	test: 1.015048

Epoch: 66
Loss: 1.3822767734527588
RMSE train: 1.087197	val: 1.663776	test: 1.408196
MAE train: 0.848419	val: 1.125074	test: 1.028747

Epoch: 67
Loss: 1.1181178092956543
RMSE train: 1.083373	val: 1.661582	test: 1.417245
MAE train: 0.837780	val: 1.108968	test: 1.008684

Epoch: 68
Loss: 1.5631300806999207
RMSE train: 1.005880	val: 1.581544	test: 1.329402
MAE train: 0.761150	val: 1.043574	test: 0.939378

Epoch: 69
Loss: 1.2900617718696594
RMSE train: 0.937880	val: 1.536223	test: 1.232411
MAE train: 0.698172	val: 1.010030	test: 0.876319

Epoch: 70
Loss: 1.3428379893302917
RMSE train: 0.913803	val: 1.561579	test: 1.255031
MAE train: 0.680758	val: 1.032744	test: 0.866382

Epoch: 71
Loss: 1.2770060300827026
RMSE train: 0.896453	val: 1.602937	test: 1.290056
MAE train: 0.666720	val: 1.071342	test: 0.863456

Epoch: 72
Loss: 1.3077731132507324
RMSE train: 0.885770	val: 1.623806	test: 1.315997
MAE train: 0.665319	val: 1.094230	test: 0.866806

Epoch: 73
Loss: 1.4131128787994385
RMSE train: 0.885589	val: 1.610077	test: 1.325571
MAE train: 0.669163	val: 1.091969	test: 0.879107

Epoch: 74
Loss: 1.1006460785865784
RMSE train: 0.872085	val: 1.571369	test: 1.283191
MAE train: 0.654629	val: 1.058692	test: 0.858318

Epoch: 75
Loss: 1.3084756731987
RMSE train: 0.890162	val: 1.566879	test: 1.297263
MAE train: 0.663668	val: 1.020658	test: 0.872063

Epoch: 76
Loss: 1.3724830746650696
RMSE train: 0.925826	val: 1.591395	test: 1.304544
MAE train: 0.688872	val: 1.000070	test: 0.894978

Epoch: 77
Loss: 1.2267646789550781
RMSE train: 0.989757	val: 1.608482	test: 1.356557
MAE train: 0.729853	val: 1.016725	test: 0.919923

Epoch: 78
Loss: 1.2761560082435608
RMSE train: 1.001723	val: 1.610209	test: 1.355649
MAE train: 0.734664	val: 1.031979	test: 0.904956

Epoch: 79
Loss: 1.2383620738983154
RMSE train: 1.006009	val: 1.621242	test: 1.340124
MAE train: 0.747535	val: 1.033369	test: 0.895854

Epoch: 80
Loss: 1.0877512097358704
RMSE train: 0.975979	val: 1.630531	test: 1.296402
MAE train: 0.738345	val: 1.059889	test: 0.889015

Epoch: 81
Loss: 1.5125622153282166
RMSE train: 0.916617	val: 1.629420	test: 1.182462
MAE train: 0.698317	val: 1.097785	test: 0.837280

Epoch: 82
Loss: 1.0283267796039581
RMSE train: 0.884598	val: 1.622729	test: 1.114504
MAE train: 0.669122	val: 1.109110	test: 0.808292

Epoch: 83
Loss: 1.2092391848564148
RMSE train: 0.890877	val: 1.600309	test: 1.117449
MAE train: 0.670687	val: 1.082393	test: 0.824053

Epoch: 23
Loss: 8.835174322128296
RMSE train: 3.595800	val: 4.227882	test: 3.536543
MAE train: 3.252903	val: 3.799041	test: 3.218165

Epoch: 24
Loss: 8.529489278793335
RMSE train: 3.537295	val: 4.170025	test: 3.457773
MAE train: 3.188883	val: 3.723964	test: 3.132910

Epoch: 25
Loss: 8.019776821136475
RMSE train: 3.472681	val: 4.089332	test: 3.385453
MAE train: 3.121627	val: 3.641990	test: 3.066071

Epoch: 26
Loss: 7.852350950241089
RMSE train: 3.440791	val: 4.029310	test: 3.375731
MAE train: 3.098839	val: 3.586294	test: 3.061903

Epoch: 27
Loss: 7.1956703662872314
RMSE train: 3.415323	val: 3.971799	test: 3.369624
MAE train: 3.081019	val: 3.538060	test: 3.066042

Epoch: 28
Loss: 6.90739369392395
RMSE train: 3.328073	val: 3.868064	test: 3.297086
MAE train: 2.998144	val: 3.437418	test: 3.004818

Epoch: 29
Loss: 6.658227443695068
RMSE train: 3.188472	val: 3.729187	test: 3.174944
MAE train: 2.861290	val: 3.292820	test: 2.887039

Epoch: 30
Loss: 6.339182376861572
RMSE train: 3.046145	val: 3.600214	test: 3.056077
MAE train: 2.719404	val: 3.151005	test: 2.759126

Epoch: 31
Loss: 6.256191730499268
RMSE train: 2.920161	val: 3.496339	test: 2.942545
MAE train: 2.591273	val: 3.033099	test: 2.633256

Epoch: 32
Loss: 5.7525718212127686
RMSE train: 2.783665	val: 3.386082	test: 2.809770
MAE train: 2.448058	val: 2.893364	test: 2.483769

Epoch: 33
Loss: 5.3833842277526855
RMSE train: 2.645445	val: 3.275445	test: 2.650922
MAE train: 2.303760	val: 2.760500	test: 2.342191

Epoch: 34
Loss: 5.192776918411255
RMSE train: 2.552762	val: 3.176697	test: 2.561788
MAE train: 2.210108	val: 2.658113	test: 2.256185

Epoch: 35
Loss: 4.794578790664673
RMSE train: 2.513191	val: 3.110313	test: 2.556328
MAE train: 2.169438	val: 2.588926	test: 2.235187

Epoch: 36
Loss: 4.842339277267456
RMSE train: 2.494218	val: 3.025278	test: 2.574722
MAE train: 2.155752	val: 2.509837	test: 2.251375

Epoch: 37
Loss: 4.496145486831665
RMSE train: 2.486284	val: 2.954797	test: 2.598210
MAE train: 2.138349	val: 2.454152	test: 2.250292

Epoch: 38
Loss: 4.162538170814514
RMSE train: 2.373226	val: 2.865904	test: 2.497891
MAE train: 2.027572	val: 2.372275	test: 2.152088

Epoch: 39
Loss: 3.8760440349578857
RMSE train: 2.217096	val: 2.778519	test: 2.311087
MAE train: 1.882390	val: 2.256988	test: 1.996581

Epoch: 40
Loss: 3.6882296800613403
RMSE train: 2.090709	val: 2.728084	test: 2.129660
MAE train: 1.749165	val: 2.153977	test: 1.836027

Epoch: 41
Loss: 3.358731269836426
RMSE train: 2.027730	val: 2.678192	test: 2.036504
MAE train: 1.689827	val: 2.082271	test: 1.749827

Epoch: 42
Loss: 3.246179699897766
RMSE train: 2.005668	val: 2.638536	test: 2.040915
MAE train: 1.682377	val: 2.042635	test: 1.748824

Epoch: 43
Loss: 3.175368905067444
RMSE train: 1.973160	val: 2.576600	test: 2.031964
MAE train: 1.655229	val: 1.995562	test: 1.724884

Epoch: 44
Loss: 2.9300129413604736
RMSE train: 1.962541	val: 2.514909	test: 2.034186
MAE train: 1.627933	val: 1.949740	test: 1.697335

Epoch: 45
Loss: 2.642093777656555
RMSE train: 1.879922	val: 2.447714	test: 1.927557
MAE train: 1.532306	val: 1.859039	test: 1.583445

Epoch: 46
Loss: 2.5176894664764404
RMSE train: 1.754807	val: 2.341518	test: 1.787882
MAE train: 1.410094	val: 1.736722	test: 1.465927

Epoch: 47
Loss: 2.5764734745025635
RMSE train: 1.648229	val: 2.262111	test: 1.661166
MAE train: 1.312301	val: 1.641656	test: 1.375443

Epoch: 48
Loss: 2.380109429359436
RMSE train: 1.632327	val: 2.213753	test: 1.666491
MAE train: 1.299115	val: 1.609873	test: 1.384324

Epoch: 49
Loss: 2.136566460132599
RMSE train: 1.615308	val: 2.161906	test: 1.677532
MAE train: 1.283912	val: 1.572400	test: 1.396899

Epoch: 50
Loss: 2.075930655002594
RMSE train: 1.586518	val: 2.126744	test: 1.635961
MAE train: 1.258916	val: 1.551644	test: 1.358093

Epoch: 51
Loss: 2.02939772605896
RMSE train: 1.553111	val: 2.108102	test: 1.567812
MAE train: 1.231595	val: 1.542296	test: 1.303784

Epoch: 52
Loss: 1.9985437989234924
RMSE train: 1.527021	val: 2.093312	test: 1.514233
MAE train: 1.202469	val: 1.535935	test: 1.245344

Epoch: 53
Loss: 1.8676338195800781
RMSE train: 1.507942	val: 2.066165	test: 1.493989
MAE train: 1.187234	val: 1.525390	test: 1.227602

Epoch: 54
Loss: 1.7110081315040588
RMSE train: 1.474156	val: 2.031441	test: 1.463643
MAE train: 1.156148	val: 1.510472	test: 1.195131

Epoch: 55
Loss: 1.7384571433067322
RMSE train: 1.408692	val: 1.978022	test: 1.420552
MAE train: 1.094763	val: 1.466917	test: 1.145191

Epoch: 56
Loss: 1.8109054565429688
RMSE train: 1.355915	val: 1.939804	test: 1.387775
MAE train: 1.045138	val: 1.422379	test: 1.105881

Epoch: 57
Loss: 1.7023340463638306
RMSE train: 1.297301	val: 1.861200	test: 1.358945
MAE train: 0.995990	val: 1.354726	test: 1.073212

Epoch: 58
Loss: 1.5719350576400757
RMSE train: 1.261595	val: 1.825177	test: 1.315545
MAE train: 0.965520	val: 1.313367	test: 1.035240

Epoch: 59
Loss: 1.5448532104492188
RMSE train: 1.227123	val: 1.789736	test: 1.297328
MAE train: 0.934325	val: 1.265705	test: 1.005934

Epoch: 60
Loss: 1.5272548198699951
RMSE train: 1.198179	val: 1.730131	test: 1.312576
MAE train: 0.911125	val: 1.214194	test: 0.992337

Epoch: 61
Loss: 1.4040205478668213
RMSE train: 1.148766	val: 1.697060	test: 1.287580
MAE train: 0.867285	val: 1.177450	test: 0.963528

Epoch: 62
Loss: 1.467869222164154
RMSE train: 1.158276	val: 1.693509	test: 1.318006
MAE train: 0.875136	val: 1.155963	test: 0.988688

Epoch: 63
Loss: 1.310360848903656
RMSE train: 1.168642	val: 1.701119	test: 1.352474
MAE train: 0.881850	val: 1.152123	test: 1.001919

Epoch: 64
Loss: 1.3768560290336609
RMSE train: 1.155215	val: 1.697465	test: 1.340860
MAE train: 0.872860	val: 1.165682	test: 0.985761

Epoch: 65
Loss: 1.4673367738723755
RMSE train: 1.124455	val: 1.696954	test: 1.313622
MAE train: 0.847752	val: 1.180607	test: 0.943473

Epoch: 66
Loss: 1.3574045300483704
RMSE train: 1.120649	val: 1.712301	test: 1.306653
MAE train: 0.841859	val: 1.195296	test: 0.932079

Epoch: 67
Loss: 1.4277729392051697
RMSE train: 1.113735	val: 1.703909	test: 1.296085
MAE train: 0.839978	val: 1.187978	test: 0.928462

Epoch: 68
Loss: 1.4619222283363342
RMSE train: 1.120389	val: 1.667677	test: 1.291147
MAE train: 0.851986	val: 1.173150	test: 0.938730

Epoch: 69
Loss: 1.2743380069732666
RMSE train: 1.113330	val: 1.650957	test: 1.266946
MAE train: 0.848102	val: 1.157021	test: 0.943668

Epoch: 70
Loss: 1.3058146834373474
RMSE train: 1.082548	val: 1.624624	test: 1.210393
MAE train: 0.822433	val: 1.129332	test: 0.917155

Epoch: 71
Loss: 1.4428077936172485
RMSE train: 1.003804	val: 1.581662	test: 1.103930
MAE train: 0.763526	val: 1.098906	test: 0.834826

Epoch: 72
Loss: 1.471744179725647
RMSE train: 0.930617	val: 1.516072	test: 1.034140
MAE train: 0.699224	val: 1.049394	test: 0.776155

Epoch: 73
Loss: 1.5856097340583801
RMSE train: 0.899714	val: 1.466402	test: 1.044196
MAE train: 0.672581	val: 1.003795	test: 0.753000

Epoch: 74
Loss: 1.2863733172416687
RMSE train: 0.903927	val: 1.417447	test: 1.102914
MAE train: 0.674689	val: 0.953143	test: 0.771362

Epoch: 75
Loss: 1.2815272212028503
RMSE train: 0.868813	val: 1.399153	test: 1.093051
MAE train: 0.641857	val: 0.932372	test: 0.756927

Epoch: 76
Loss: 1.259664237499237
RMSE train: 0.875997	val: 1.455169	test: 1.074191
MAE train: 0.649951	val: 0.958571	test: 0.748383

Epoch: 77
Loss: 1.1432968378067017
RMSE train: 0.894304	val: 1.511614	test: 1.047510
MAE train: 0.660510	val: 0.998315	test: 0.745331

Epoch: 78
Loss: 1.4601659178733826
RMSE train: 0.844067	val: 1.472105	test: 1.005724
MAE train: 0.624221	val: 0.978878	test: 0.731092

Epoch: 79
Loss: 1.1299001574516296
RMSE train: 0.804781	val: 1.433115	test: 0.977607
MAE train: 0.601532	val: 0.961291	test: 0.722878

Epoch: 80
Loss: 1.1537511944770813
RMSE train: 0.802397	val: 1.414177	test: 0.980976
MAE train: 0.605524	val: 0.956228	test: 0.736956

Epoch: 81
Loss: 1.227830946445465
RMSE train: 0.875913	val: 1.452751	test: 1.071496
MAE train: 0.654423	val: 1.004701	test: 0.776342

Epoch: 82
Loss: 1.270340085029602
RMSE train: 0.938664	val: 1.506313	test: 1.144852
MAE train: 0.689635	val: 1.037097	test: 0.796486

Epoch: 83
Loss: 1.2588769793510437
RMSE train: 0.959070	val: 1.515924	test: 1.180544
MAE train: 0.699410	val: 1.030925	test: 0.804748

Epoch: 84
Loss: 1.4030117988586426
RMSE train: 0.985097	val: 1.213190	test: 1.435824
MAE train: 0.689507	val: 0.873756	test: 0.921358

Epoch: 85
Loss: 1.2749686241149902
RMSE train: 0.994271	val: 1.225461	test: 1.426431
MAE train: 0.703816	val: 0.866178	test: 0.927009

Epoch: 86
Loss: 1.3098562359809875
RMSE train: 1.012550	val: 1.268149	test: 1.426503
MAE train: 0.739906	val: 0.894492	test: 0.956705

Epoch: 87
Loss: 1.3714979887008667
RMSE train: 1.002502	val: 1.280798	test: 1.401586
MAE train: 0.741923	val: 0.894947	test: 0.956127

Epoch: 88
Loss: 1.4771912097930908
RMSE train: 0.918613	val: 1.224418	test: 1.327336
MAE train: 0.686520	val: 0.862225	test: 0.910238

Epoch: 89
Loss: 1.2704960107803345
RMSE train: 0.857717	val: 1.175866	test: 1.275303
MAE train: 0.642329	val: 0.828498	test: 0.872851

Epoch: 90
Loss: 1.3474318981170654
RMSE train: 0.819456	val: 1.130537	test: 1.247247
MAE train: 0.611743	val: 0.793489	test: 0.838562

Epoch: 91
Loss: 1.167091429233551
RMSE train: 0.844182	val: 1.119940	test: 1.266167
MAE train: 0.632318	val: 0.781571	test: 0.848052

Epoch: 92
Loss: 1.117372453212738
RMSE train: 0.876976	val: 1.136038	test: 1.296883
MAE train: 0.656650	val: 0.796952	test: 0.858915

Epoch: 93
Loss: 1.3093611001968384
RMSE train: 0.917123	val: 1.191877	test: 1.329764
MAE train: 0.686598	val: 0.840548	test: 0.885352

Epoch: 94
Loss: 1.2216184735298157
RMSE train: 0.966912	val: 1.238604	test: 1.370586
MAE train: 0.724567	val: 0.881818	test: 0.912632

Epoch: 95
Loss: 1.2367777824401855
RMSE train: 0.968929	val: 1.229519	test: 1.362811
MAE train: 0.715610	val: 0.885024	test: 0.902254

Epoch: 96
Loss: 1.383742094039917
RMSE train: 0.956110	val: 1.176979	test: 1.336875
MAE train: 0.685888	val: 0.864966	test: 0.885820

Epoch: 97
Loss: 1.062920093536377
RMSE train: 0.883570	val: 1.095387	test: 1.292258
MAE train: 0.636620	val: 0.821172	test: 0.864876

Epoch: 98
Loss: 1.1416834592819214
RMSE train: 0.807772	val: 1.025560	test: 1.253925
MAE train: 0.598293	val: 0.778279	test: 0.841115

Epoch: 99
Loss: 1.1854925751686096
RMSE train: 0.798911	val: 1.039358	test: 1.265504
MAE train: 0.601467	val: 0.777184	test: 0.854663

Epoch: 100
Loss: 0.9698369801044464
RMSE train: 0.839907	val: 1.101225	test: 1.295078
MAE train: 0.633997	val: 0.804164	test: 0.881769

Epoch: 101
Loss: 1.5220218896865845
RMSE train: 0.887004	val: 1.164030	test: 1.289711
MAE train: 0.676156	val: 0.834795	test: 0.887114

Epoch: 102
Loss: 1.0270647406578064
RMSE train: 0.895419	val: 1.173368	test: 1.277157
MAE train: 0.683580	val: 0.840432	test: 0.883659

Epoch: 103
Loss: 1.266245424747467
RMSE train: 0.859663	val: 1.146572	test: 1.239092
MAE train: 0.656088	val: 0.814341	test: 0.856057

Epoch: 104
Loss: 1.1700804829597473
RMSE train: 0.795991	val: 1.076380	test: 1.214486
MAE train: 0.607805	val: 0.763957	test: 0.827570

Epoch: 105
Loss: 0.9538022875785828
RMSE train: 0.792825	val: 1.061154	test: 1.226434
MAE train: 0.606069	val: 0.761001	test: 0.841581

Epoch: 106
Loss: 1.125836730003357
RMSE train: 0.791345	val: 1.061736	test: 1.239734
MAE train: 0.604875	val: 0.767447	test: 0.846233

Epoch: 107
Loss: 0.9671835899353027
RMSE train: 0.789629	val: 1.085609	test: 1.241600
MAE train: 0.595403	val: 0.795885	test: 0.844447

Epoch: 108
Loss: 1.2710026502609253
RMSE train: 0.827386	val: 1.136206	test: 1.258649
MAE train: 0.617809	val: 0.836051	test: 0.868528

Epoch: 109
Loss: 1.1945512890815735
RMSE train: 0.830637	val: 1.144109	test: 1.270542
MAE train: 0.629331	val: 0.839287	test: 0.870882

Epoch: 110
Loss: 1.1103968620300293
RMSE train: 0.799406	val: 1.124268	test: 1.274128
MAE train: 0.614382	val: 0.827516	test: 0.869592

Epoch: 111
Loss: 0.9023988246917725
RMSE train: 0.813629	val: 1.106725	test: 1.290576
MAE train: 0.622098	val: 0.811132	test: 0.884465

Epoch: 112
Loss: 1.0316041707992554
RMSE train: 0.813883	val: 1.090738	test: 1.289521
MAE train: 0.621153	val: 0.800812	test: 0.894756

Epoch: 113
Loss: 1.017818033695221
RMSE train: 0.791341	val: 1.091079	test: 1.272628
MAE train: 0.601986	val: 0.795105	test: 0.877657

Epoch: 114
Loss: 1.2927334904670715
RMSE train: 0.760152	val: 1.065260	test: 1.261306
MAE train: 0.571407	val: 0.780382	test: 0.875081

Epoch: 115
Loss: 1.0700297951698303
RMSE train: 0.800905	val: 1.093464	test: 1.280084
MAE train: 0.597326	val: 0.802457	test: 0.888942

Epoch: 116
Loss: 1.1243257522583008
RMSE train: 0.784264	val: 1.092991	test: 1.291884
MAE train: 0.595941	val: 0.804390	test: 0.882078

Epoch: 117
Loss: 1.022163450717926
RMSE train: 0.827220	val: 1.151766	test: 1.353097
MAE train: 0.638597	val: 0.844183	test: 0.923624

Epoch: 118
Loss: 0.9650465548038483
RMSE train: 0.914982	val: 1.247448	test: 1.430767
MAE train: 0.716373	val: 0.907594	test: 0.999240

Epoch: 119
Loss: 1.2250897288322449
RMSE train: 0.853453	val: 1.203176	test: 1.373038
MAE train: 0.668897	val: 0.867721	test: 0.943189

Epoch: 120
Loss: 0.8753578066825867
RMSE train: 0.733919	val: 1.112867	test: 1.288684
MAE train: 0.567401	val: 0.791755	test: 0.867412

Epoch: 121
Loss: 1.2261852025985718
RMSE train: 0.726303	val: 1.098015	test: 1.274060
MAE train: 0.559511	val: 0.802039	test: 0.855151

Epoch: 122
Loss: 0.9854798913002014
RMSE train: 0.719735	val: 1.096397	test: 1.281148
MAE train: 0.550649	val: 0.802987	test: 0.849631

Epoch: 123
Loss: 0.9547427296638489
RMSE train: 0.711959	val: 1.109579	test: 1.279560
MAE train: 0.541948	val: 0.811344	test: 0.853238

Epoch: 124
Loss: 1.0173830389976501
RMSE train: 0.739651	val: 1.142991	test: 1.307249
MAE train: 0.571310	val: 0.856229	test: 0.886620

Epoch: 125
Loss: 1.071133702993393
RMSE train: 0.802063	val: 1.193443	test: 1.360175
MAE train: 0.626695	val: 0.898087	test: 0.939057

Epoch: 126
Loss: 1.0064278542995453
RMSE train: 0.796787	val: 1.192890	test: 1.340148
MAE train: 0.620043	val: 0.885715	test: 0.925864

Epoch: 127
Loss: 1.010195642709732
RMSE train: 0.728153	val: 1.124245	test: 1.255959
MAE train: 0.558661	val: 0.825898	test: 0.853221

Epoch: 128
Loss: 0.9481076002120972
RMSE train: 0.717981	val: 1.086497	test: 1.197461
MAE train: 0.546010	val: 0.795387	test: 0.821965

Epoch: 129
Loss: 0.8152039647102356
RMSE train: 0.741126	val: 1.092667	test: 1.194261
MAE train: 0.565503	val: 0.785554	test: 0.826244

Epoch: 130
Loss: 1.0409210324287415
RMSE train: 0.749881	val: 1.091006	test: 1.214964
MAE train: 0.572749	val: 0.777861	test: 0.835416

Epoch: 131
Loss: 0.9609736204147339
RMSE train: 0.817480	val: 1.118955	test: 1.261811
MAE train: 0.631900	val: 0.797602	test: 0.867741

Epoch: 132
Loss: 1.363653302192688
RMSE train: 0.799054	val: 1.080402	test: 1.260677
MAE train: 0.624176	val: 0.787619	test: 0.869933

Epoch: 133
Loss: 0.7947936058044434
RMSE train: 0.733600	val: 0.972219	test: 1.223631
MAE train: 0.572702	val: 0.730446	test: 0.832805

Epoch: 134
Loss: 0.9399102926254272
RMSE train: 0.726177	val: 0.947121	test: 1.216182
MAE train: 0.548989	val: 0.713163	test: 0.812502

Epoch: 135
Loss: 0.8142524659633636
RMSE train: 0.728343	val: 0.966959	test: 1.228589
MAE train: 0.545902	val: 0.717202	test: 0.828825

Epoch: 136
Loss: 0.9040109217166901
RMSE train: 0.721238	val: 1.006988	test: 1.240214
MAE train: 0.543705	val: 0.739039	test: 0.844100

Epoch: 137
Loss: 1.0592219829559326
RMSE train: 0.776611	val: 1.101328	test: 1.276112
MAE train: 0.593735	val: 0.797299	test: 0.880818

Epoch: 138
Loss: 0.9551048874855042
RMSE train: 0.856177	val: 1.191784	test: 1.314843
MAE train: 0.657921	val: 0.847152	test: 0.913047

Epoch: 139
Loss: 0.8868064880371094
RMSE train: 0.909525	val: 1.235859	test: 1.344550
MAE train: 0.698632	val: 0.878190	test: 0.937780

Epoch: 140
Loss: 0.9850108921527863
RMSE train: 0.848067	val: 1.178736	test: 1.310277
MAE train: 0.648709	val: 0.842036	test: 0.908475

Epoch: 141
Loss: 1.0050939321517944
RMSE train: 0.799110	val: 1.117653	test: 1.285419
MAE train: 0.608776	val: 0.808334	test: 0.889872

Epoch: 142
Loss: 0.8742228448390961
RMSE train: 0.720858	val: 1.031514	test: 1.264525
MAE train: 0.550209	val: 0.764722	test: 0.867478

Epoch: 143
Loss: 1.0154914259910583
RMSE train: 0.697110	val: 0.998930	test: 1.262400
MAE train: 0.541072	val: 0.739123	test: 0.869138

Epoch: 144
Loss: 0.7811135947704315
RMSE train: 0.712068	val: 1.008547	test: 1.267690

Epoch: 84
Loss: 1.3955087661743164
RMSE train: 0.943891	val: 1.245432	test: 1.485791
MAE train: 0.675920	val: 0.909640	test: 0.994212

Epoch: 85
Loss: 1.3707370162010193
RMSE train: 0.953538	val: 1.269488	test: 1.495620
MAE train: 0.697804	val: 0.905378	test: 0.995167

Epoch: 86
Loss: 1.4899039268493652
RMSE train: 0.911477	val: 1.235720	test: 1.486726
MAE train: 0.675084	val: 0.870081	test: 0.985688

Epoch: 87
Loss: 1.3666267395019531
RMSE train: 0.878899	val: 1.178359	test: 1.449432
MAE train: 0.640869	val: 0.839747	test: 0.950426

Epoch: 88
Loss: 1.4235833883285522
RMSE train: 0.886521	val: 1.179442	test: 1.449912
MAE train: 0.642309	val: 0.849015	test: 0.938630

Epoch: 89
Loss: 1.0159970223903656
RMSE train: 0.837575	val: 1.174175	test: 1.436985
MAE train: 0.606140	val: 0.835264	test: 0.925696

Epoch: 90
Loss: 1.0723493993282318
RMSE train: 0.803003	val: 1.163332	test: 1.419464
MAE train: 0.585305	val: 0.831038	test: 0.934498

Epoch: 91
Loss: 1.0041989386081696
RMSE train: 0.817473	val: 1.159610	test: 1.416988
MAE train: 0.604620	val: 0.841487	test: 0.959841

Epoch: 92
Loss: 1.4710186123847961
RMSE train: 0.848224	val: 1.200230	test: 1.449817
MAE train: 0.631241	val: 0.865083	test: 0.988066

Epoch: 93
Loss: 1.2858198881149292
RMSE train: 0.902542	val: 1.253347	test: 1.499783
MAE train: 0.667835	val: 0.907713	test: 1.023982

Epoch: 94
Loss: 1.2659303545951843
RMSE train: 0.934711	val: 1.262855	test: 1.528616
MAE train: 0.690781	val: 0.926608	test: 1.039797

Epoch: 95
Loss: 1.3056503534317017
RMSE train: 0.955612	val: 1.254743	test: 1.525376
MAE train: 0.704688	val: 0.938861	test: 1.026716

Epoch: 96
Loss: 1.1290048360824585
RMSE train: 0.979680	val: 1.249209	test: 1.501735
MAE train: 0.731331	val: 0.944390	test: 1.013072

Epoch: 97
Loss: 1.1629449725151062
RMSE train: 1.006127	val: 1.237927	test: 1.496232
MAE train: 0.764547	val: 0.937008	test: 1.030182

Epoch: 98
Loss: 1.064266860485077
RMSE train: 1.005837	val: 1.213050	test: 1.494770
MAE train: 0.761087	val: 0.932162	test: 1.028086

Epoch: 99
Loss: 0.9559805393218994
RMSE train: 0.997571	val: 1.217388	test: 1.512539
MAE train: 0.724035	val: 0.913686	test: 1.018461

Epoch: 100
Loss: 1.238549530506134
RMSE train: 1.038511	val: 1.276297	test: 1.555689
MAE train: 0.740271	val: 0.932831	test: 1.047063

Epoch: 101
Loss: 1.0639278292655945
RMSE train: 1.000981	val: 1.260372	test: 1.537300
MAE train: 0.721223	val: 0.924409	test: 1.035414

Epoch: 102
Loss: 1.3006688952445984
RMSE train: 0.923725	val: 1.221453	test: 1.490366
MAE train: 0.682541	val: 0.915782	test: 1.006789

Epoch: 103
Loss: 1.1401391625404358
RMSE train: 0.852340	val: 1.174404	test: 1.437319
MAE train: 0.630584	val: 0.892477	test: 0.966814

Epoch: 104
Loss: 1.2701979875564575
RMSE train: 0.852685	val: 1.182083	test: 1.415418
MAE train: 0.625924	val: 0.899458	test: 0.957380

Epoch: 105
Loss: 1.1411779522895813
RMSE train: 0.842569	val: 1.198281	test: 1.418235
MAE train: 0.614715	val: 0.902701	test: 0.957430

Epoch: 106
Loss: 1.4495556354522705
RMSE train: 0.859750	val: 1.198229	test: 1.427166
MAE train: 0.623012	val: 0.884358	test: 0.972008

Epoch: 107
Loss: 0.99249067902565
RMSE train: 0.876048	val: 1.177953	test: 1.452968
MAE train: 0.637694	val: 0.855263	test: 0.992364

Epoch: 108
Loss: 1.1165591478347778
RMSE train: 0.867095	val: 1.137341	test: 1.450557
MAE train: 0.642436	val: 0.819753	test: 0.989891

Epoch: 109
Loss: 1.1470443606376648
RMSE train: 0.868587	val: 1.091908	test: 1.427711
MAE train: 0.627555	val: 0.799594	test: 0.975413

Epoch: 110
Loss: 1.0426989793777466
RMSE train: 0.882357	val: 1.076036	test: 1.408393
MAE train: 0.640851	val: 0.813036	test: 0.981478

Epoch: 111
Loss: 1.260696142911911
RMSE train: 0.910579	val: 1.083109	test: 1.404293
MAE train: 0.672828	val: 0.830036	test: 0.993809

Epoch: 112
Loss: 1.1080791354179382
RMSE train: 0.875456	val: 1.078199	test: 1.403049
MAE train: 0.650994	val: 0.824957	test: 0.988374

Epoch: 113
Loss: 1.1405571103096008
RMSE train: 0.853957	val: 1.085947	test: 1.404775
MAE train: 0.627854	val: 0.812850	test: 0.966513

Epoch: 114
Loss: 1.1377252042293549
RMSE train: 0.852946	val: 1.103606	test: 1.407652
MAE train: 0.624583	val: 0.809201	test: 0.963586

Epoch: 115
Loss: 1.3869855999946594
RMSE train: 0.877426	val: 1.129436	test: 1.416619
MAE train: 0.644101	val: 0.823222	test: 0.974797

Epoch: 116
Loss: 0.9373087882995605
RMSE train: 0.881853	val: 1.119934	test: 1.447003
MAE train: 0.649975	val: 0.827264	test: 0.982650

Epoch: 117
Loss: 0.901787668466568
RMSE train: 0.868640	val: 1.109242	test: 1.480931
MAE train: 0.631968	val: 0.819533	test: 0.985108

Epoch: 118
Loss: 1.2525521516799927
RMSE train: 0.813713	val: 1.095745	test: 1.470670
MAE train: 0.584805	val: 0.795746	test: 0.966220

Epoch: 119
Loss: 1.0611343383789062
RMSE train: 0.771326	val: 1.085590	test: 1.438121
MAE train: 0.555718	val: 0.791420	test: 0.953927

Epoch: 120
Loss: 0.9133438169956207
RMSE train: 0.783847	val: 1.104350	test: 1.424183
MAE train: 0.569011	val: 0.811409	test: 0.958939

Epoch: 121
Loss: 0.9353915154933929
RMSE train: 0.800610	val: 1.105747	test: 1.422797
MAE train: 0.581856	val: 0.818668	test: 0.950688

Epoch: 122
Loss: 0.8599963188171387
RMSE train: 0.826190	val: 1.111407	test: 1.413172
MAE train: 0.596488	val: 0.831241	test: 0.947201

Epoch: 123
Loss: 1.1402568817138672
RMSE train: 0.846260	val: 1.157761	test: 1.408177
MAE train: 0.609345	val: 0.854788	test: 0.943244

Epoch: 124
Loss: 1.063037931919098
RMSE train: 0.890302	val: 1.232536	test: 1.422195
MAE train: 0.654440	val: 0.906273	test: 0.966501

Epoch: 125
Loss: 0.9064738154411316
RMSE train: 0.932965	val: 1.279036	test: 1.467522
MAE train: 0.695486	val: 0.935814	test: 1.006994

Epoch: 126
Loss: 0.9890235364437103
RMSE train: 0.941393	val: 1.276690	test: 1.496540
MAE train: 0.709516	val: 0.938892	test: 1.028124

Epoch: 127
Loss: 1.2761774063110352
RMSE train: 0.857915	val: 1.207324	test: 1.468373
MAE train: 0.645752	val: 0.889999	test: 0.998401

Epoch: 128
Loss: 1.1891862750053406
RMSE train: 0.848155	val: 1.148200	test: 1.454156
MAE train: 0.630736	val: 0.863674	test: 0.990957

Epoch: 129
Loss: 1.0556634664535522
RMSE train: 0.851225	val: 1.111364	test: 1.449034
MAE train: 0.626455	val: 0.832823	test: 0.986763

Epoch: 130
Loss: 0.902687281370163
RMSE train: 0.852753	val: 1.102623	test: 1.441436
MAE train: 0.620667	val: 0.812259	test: 0.976638

Epoch: 131
Loss: 0.874715119600296
RMSE train: 0.849529	val: 1.092937	test: 1.422392
MAE train: 0.616584	val: 0.797169	test: 0.957952

Epoch: 132
Loss: 1.045945942401886
RMSE train: 0.823284	val: 1.074761	test: 1.411193
MAE train: 0.606975	val: 0.787044	test: 0.945582

Epoch: 133
Loss: 1.1954373717308044
RMSE train: 0.833758	val: 1.108941	test: 1.435628
MAE train: 0.616431	val: 0.803035	test: 0.959340

Epoch: 134
Loss: 1.2629896402359009
RMSE train: 0.844468	val: 1.146772	test: 1.456984
MAE train: 0.625027	val: 0.825722	test: 0.959192

Epoch: 135
Loss: 0.903038740158081
RMSE train: 0.826651	val: 1.155295	test: 1.468306
MAE train: 0.610184	val: 0.829585	test: 0.960423

Epoch: 136
Loss: 0.8289906680583954
RMSE train: 0.818169	val: 1.159423	test: 1.467891
MAE train: 0.602442	val: 0.834999	test: 0.960577

Epoch: 137
Loss: 0.8715799152851105
RMSE train: 0.799413	val: 1.149493	test: 1.471656
MAE train: 0.587653	val: 0.826088	test: 0.964189

Epoch: 138
Loss: 1.202382355928421
RMSE train: 0.788854	val: 1.140695	test: 1.448693
MAE train: 0.578865	val: 0.816634	test: 0.937071

Epoch: 139
Loss: 0.9438391327857971
RMSE train: 0.716855	val: 1.072892	test: 1.393565
MAE train: 0.533810	val: 0.779045	test: 0.886120

Epoch: 140
Loss: 0.9992276728153229
RMSE train: 0.697348	val: 1.046833	test: 1.364444
MAE train: 0.518178	val: 0.758414	test: 0.869462

Epoch: 141
Loss: 0.8124471306800842
RMSE train: 0.720841	val: 1.046579	test: 1.355691
MAE train: 0.539933	val: 0.757493	test: 0.874030

Epoch: 142
Loss: 0.7485064268112183
RMSE train: 0.735182	val: 1.065485	test: 1.359788
MAE train: 0.558534	val: 0.763831	test: 0.883565

Epoch: 143
Loss: 0.8698721826076508
RMSE train: 0.764376	val: 1.100279	test: 1.365602
MAE train: 0.582425	val: 0.795016	test: 0.899424

Epoch: 144
Loss: 0.7841138243675232
RMSE train: 0.832711	val: 1.153187	test: 1.395217

Epoch: 84
Loss: 1.6744424700737
RMSE train: 0.937206	val: 1.315255	test: 1.510277
MAE train: 0.698547	val: 0.946533	test: 0.953462

Epoch: 85
Loss: 1.3137308955192566
RMSE train: 0.916142	val: 1.295585	test: 1.465462
MAE train: 0.686388	val: 0.924883	test: 0.928101

Epoch: 86
Loss: 1.3105138540267944
RMSE train: 0.888371	val: 1.277135	test: 1.422936
MAE train: 0.669109	val: 0.900100	test: 0.911149

Epoch: 87
Loss: 1.388819396495819
RMSE train: 0.818351	val: 1.225891	test: 1.388564
MAE train: 0.619998	val: 0.868996	test: 0.898072

Epoch: 88
Loss: 1.2546348571777344
RMSE train: 0.807279	val: 1.214140	test: 1.377256
MAE train: 0.607124	val: 0.856328	test: 0.896014

Epoch: 89
Loss: 1.4046747088432312
RMSE train: 0.828797	val: 1.263714	test: 1.391909
MAE train: 0.624531	val: 0.877677	test: 0.909101

Epoch: 90
Loss: 1.0040901899337769
RMSE train: 0.863922	val: 1.335724	test: 1.428720
MAE train: 0.658751	val: 0.924568	test: 0.938045

Epoch: 91
Loss: 1.1642311811447144
RMSE train: 0.885323	val: 1.358005	test: 1.472794
MAE train: 0.674760	val: 0.954751	test: 0.970413

Epoch: 92
Loss: 1.1968640089035034
RMSE train: 0.897507	val: 1.342798	test: 1.472517
MAE train: 0.684948	val: 0.943784	test: 0.965399

Epoch: 93
Loss: 1.0217922925949097
RMSE train: 0.944288	val: 1.318241	test: 1.475546
MAE train: 0.716273	val: 0.949037	test: 0.982599

Epoch: 94
Loss: 1.2817267179489136
RMSE train: 0.948658	val: 1.302743	test: 1.479053
MAE train: 0.714911	val: 0.941358	test: 0.986756

Epoch: 95
Loss: 1.0658801198005676
RMSE train: 0.907793	val: 1.283267	test: 1.479549
MAE train: 0.681104	val: 0.911462	test: 0.966184

Epoch: 96
Loss: 0.903897613286972
RMSE train: 0.870788	val: 1.298981	test: 1.496347
MAE train: 0.650075	val: 0.896015	test: 0.948545

Epoch: 97
Loss: 1.1261483430862427
RMSE train: 0.866921	val: 1.295930	test: 1.513015
MAE train: 0.645614	val: 0.887597	test: 0.950496

Epoch: 98
Loss: 1.1441504657268524
RMSE train: 0.849189	val: 1.277041	test: 1.510134
MAE train: 0.639284	val: 0.877642	test: 0.945784

Epoch: 99
Loss: 1.1032307744026184
RMSE train: 0.818023	val: 1.237938	test: 1.488595
MAE train: 0.612018	val: 0.864012	test: 0.919100

Epoch: 100
Loss: 1.2798289060592651
RMSE train: 0.815610	val: 1.225855	test: 1.451321
MAE train: 0.606872	val: 0.880918	test: 0.905096

Epoch: 101
Loss: 1.013512372970581
RMSE train: 0.806415	val: 1.208120	test: 1.412443
MAE train: 0.609440	val: 0.882054	test: 0.895851

Epoch: 102
Loss: 0.9859353303909302
RMSE train: 0.808204	val: 1.226593	test: 1.401488
MAE train: 0.624721	val: 0.891502	test: 0.901225

Epoch: 103
Loss: 1.1805047988891602
RMSE train: 0.792098	val: 1.256021	test: 1.414500
MAE train: 0.615552	val: 0.894872	test: 0.904164

Epoch: 104
Loss: 1.273980438709259
RMSE train: 0.797646	val: 1.276023	test: 1.425851
MAE train: 0.617077	val: 0.901100	test: 0.910565

Epoch: 105
Loss: 1.0986354053020477
RMSE train: 0.789592	val: 1.274750	test: 1.433584
MAE train: 0.593824	val: 0.890317	test: 0.906376

Epoch: 106
Loss: 0.993137538433075
RMSE train: 0.779122	val: 1.250063	test: 1.405063
MAE train: 0.576802	val: 0.873642	test: 0.894365

Epoch: 107
Loss: 1.1467904448509216
RMSE train: 0.783985	val: 1.228723	test: 1.378407
MAE train: 0.584899	val: 0.863989	test: 0.888758

Epoch: 108
Loss: 1.0395088195800781
RMSE train: 0.753132	val: 1.188072	test: 1.350929
MAE train: 0.568639	val: 0.840234	test: 0.883588

Epoch: 109
Loss: 1.115641176700592
RMSE train: 0.741725	val: 1.171200	test: 1.353601
MAE train: 0.565001	val: 0.834105	test: 0.885061

Epoch: 110
Loss: 1.150074064731598
RMSE train: 0.787387	val: 1.164961	test: 1.378494
MAE train: 0.595186	val: 0.850018	test: 0.901283

Epoch: 111
Loss: 1.1966588497161865
RMSE train: 0.825109	val: 1.180345	test: 1.406850
MAE train: 0.624717	val: 0.871982	test: 0.919162

Epoch: 112
Loss: 0.9437546133995056
RMSE train: 0.892686	val: 1.208147	test: 1.444497
MAE train: 0.674538	val: 0.903128	test: 0.951827

Epoch: 113
Loss: 1.162876546382904
RMSE train: 0.888597	val: 1.210021	test: 1.456504
MAE train: 0.669366	val: 0.891528	test: 0.951129

Epoch: 114
Loss: 1.0139105916023254
RMSE train: 0.885174	val: 1.203154	test: 1.465851
MAE train: 0.667972	val: 0.874025	test: 0.951044

Epoch: 115
Loss: 1.3612350821495056
RMSE train: 0.897913	val: 1.201090	test: 1.478861
MAE train: 0.674272	val: 0.863458	test: 0.944366

Epoch: 116
Loss: 1.1092289090156555
RMSE train: 0.853093	val: 1.150988	test: 1.441851
MAE train: 0.640592	val: 0.824604	test: 0.916159

Epoch: 117
Loss: 0.97412970662117
RMSE train: 0.820078	val: 1.119252	test: 1.395718
MAE train: 0.618729	val: 0.808535	test: 0.886787

Epoch: 118
Loss: 1.0340102314949036
RMSE train: 0.811725	val: 1.118315	test: 1.364697
MAE train: 0.624231	val: 0.814381	test: 0.880607

Epoch: 119
Loss: 0.8996520340442657
RMSE train: 0.858833	val: 1.150752	test: 1.371502
MAE train: 0.670803	val: 0.852109	test: 0.906430

Epoch: 120
Loss: 0.9869688451290131
RMSE train: 0.864729	val: 1.209993	test: 1.387506
MAE train: 0.679910	val: 0.883336	test: 0.916123

Epoch: 121
Loss: 0.9857586026191711
RMSE train: 0.861075	val: 1.276588	test: 1.397462
MAE train: 0.675641	val: 0.912751	test: 0.915410

Epoch: 122
Loss: 1.218867540359497
RMSE train: 0.850634	val: 1.299864	test: 1.395990
MAE train: 0.654742	val: 0.921368	test: 0.909231

Epoch: 123
Loss: 1.0055516958236694
RMSE train: 0.814171	val: 1.309656	test: 1.405531
MAE train: 0.621372	val: 0.913970	test: 0.909580

Epoch: 124
Loss: 0.9721513390541077
RMSE train: 0.826123	val: 1.320782	test: 1.420411
MAE train: 0.623517	val: 0.913819	test: 0.919342

Epoch: 125
Loss: 0.9180514216423035
RMSE train: 0.826653	val: 1.298351	test: 1.423002
MAE train: 0.627019	val: 0.904523	test: 0.919743

Epoch: 126
Loss: 1.0005567073822021
RMSE train: 0.819697	val: 1.241103	test: 1.404393
MAE train: 0.618314	val: 0.875013	test: 0.907122

Epoch: 127
Loss: 0.8024201095104218
RMSE train: 0.820794	val: 1.230118	test: 1.407399
MAE train: 0.619478	val: 0.866330	test: 0.902533

Epoch: 128
Loss: 0.8400283753871918
RMSE train: 0.832156	val: 1.218661	test: 1.401203
MAE train: 0.639184	val: 0.865709	test: 0.902020

Epoch: 129
Loss: 0.8488623797893524
RMSE train: 0.844900	val: 1.219062	test: 1.403049
MAE train: 0.651594	val: 0.874985	test: 0.901864

Epoch: 130
Loss: 1.3454456329345703
RMSE train: 0.826553	val: 1.184636	test: 1.381413
MAE train: 0.639632	val: 0.863736	test: 0.891824

Epoch: 131
Loss: 0.9279260039329529
RMSE train: 0.779674	val: 1.140503	test: 1.359172
MAE train: 0.599130	val: 0.834378	test: 0.869857

Epoch: 132
Loss: 1.0681698322296143
RMSE train: 0.741352	val: 1.107021	test: 1.360851
MAE train: 0.567947	val: 0.809214	test: 0.868774

Epoch: 133
Loss: 0.9497556090354919
RMSE train: 0.717038	val: 1.099635	test: 1.368072
MAE train: 0.546906	val: 0.795510	test: 0.867017

Epoch: 134
Loss: 1.1033321917057037
RMSE train: 0.718633	val: 1.130746	test: 1.378625
MAE train: 0.547597	val: 0.814420	test: 0.870773

Epoch: 135
Loss: 0.9139025807380676
RMSE train: 0.795751	val: 1.209056	test: 1.384747
MAE train: 0.612751	val: 0.873734	test: 0.900478

Epoch: 136
Loss: 0.7703932821750641
RMSE train: 0.906830	val: 1.266817	test: 1.425517
MAE train: 0.681370	val: 0.915320	test: 0.933862

Epoch: 137
Loss: 0.9342123568058014
RMSE train: 0.933326	val: 1.293555	test: 1.460789
MAE train: 0.690596	val: 0.922863	test: 0.947776

Epoch: 138
Loss: 0.9140131175518036
RMSE train: 0.914969	val: 1.306420	test: 1.461147
MAE train: 0.673043	val: 0.924333	test: 0.950238

Epoch: 139
Loss: 0.9416847825050354
RMSE train: 0.852230	val: 1.312148	test: 1.431309
MAE train: 0.635661	val: 0.919438	test: 0.925614

Epoch: 140
Loss: 0.9012384712696075
RMSE train: 0.727132	val: 1.281314	test: 1.400441
MAE train: 0.550001	val: 0.877796	test: 0.892506

Epoch: 141
Loss: 0.9732300043106079
RMSE train: 0.663106	val: 1.256420	test: 1.411706
MAE train: 0.508901	val: 0.835717	test: 0.880653

Epoch: 142
Loss: 1.0610199570655823
RMSE train: 0.641024	val: 1.223455	test: 1.408410
MAE train: 0.479742	val: 0.792858	test: 0.854219

Epoch: 143
Loss: 1.0899528861045837
RMSE train: 0.649880	val: 1.173113	test: 1.374429
MAE train: 0.469897	val: 0.771276	test: 0.849993

Epoch: 144
Loss: 1.0035459995269775
RMSE train: 0.656935	val: 1.154720	test: 1.348549

Epoch: 84
Loss: 1.168338418006897
RMSE train: 0.760056	val: 1.218977	test: 1.166195
MAE train: 0.550035	val: 0.798085	test: 0.783978

Epoch: 85
Loss: 1.1808331608772278
RMSE train: 0.765134	val: 1.252278	test: 1.126028
MAE train: 0.555941	val: 0.823006	test: 0.772780

Epoch: 86
Loss: 1.2139702439308167
RMSE train: 0.815230	val: 1.348821	test: 1.137441
MAE train: 0.591103	val: 0.905015	test: 0.826507

Epoch: 87
Loss: 1.0603446960449219
RMSE train: 0.880476	val: 1.438690	test: 1.204125
MAE train: 0.643986	val: 0.982159	test: 0.901706

Epoch: 88
Loss: 1.1153365075588226
RMSE train: 0.892118	val: 1.443189	test: 1.242360
MAE train: 0.662039	val: 0.982511	test: 0.913228

Epoch: 89
Loss: 1.1486672163009644
RMSE train: 0.887691	val: 1.360594	test: 1.257001
MAE train: 0.657167	val: 0.916517	test: 0.868621

Epoch: 90
Loss: 1.125020682811737
RMSE train: 0.911883	val: 1.332705	test: 1.306756
MAE train: 0.665751	val: 0.883890	test: 0.873333

Epoch: 91
Loss: 1.2320660948753357
RMSE train: 0.905767	val: 1.345322	test: 1.329235
MAE train: 0.667742	val: 0.896468	test: 0.899333

Epoch: 92
Loss: 1.0879371762275696
RMSE train: 0.895795	val: 1.369769	test: 1.324004
MAE train: 0.666129	val: 0.910540	test: 0.912204

Epoch: 93
Loss: 1.2232375144958496
RMSE train: 0.876723	val: 1.375185	test: 1.303428
MAE train: 0.654322	val: 0.914773	test: 0.901149

Epoch: 94
Loss: 1.1744226217269897
RMSE train: 0.845280	val: 1.355992	test: 1.254977
MAE train: 0.623389	val: 0.896390	test: 0.868106

Epoch: 95
Loss: 1.0062663555145264
RMSE train: 0.835960	val: 1.330794	test: 1.227615
MAE train: 0.611208	val: 0.874429	test: 0.831908

Epoch: 96
Loss: 1.0704302787780762
RMSE train: 0.868070	val: 1.373631	test: 1.246166
MAE train: 0.632945	val: 0.902357	test: 0.844664

Epoch: 97
Loss: 1.2624823451042175
RMSE train: 0.859155	val: 1.347580	test: 1.245082
MAE train: 0.632055	val: 0.877790	test: 0.838606

Epoch: 98
Loss: 0.9741730988025665
RMSE train: 0.890485	val: 1.372938	test: 1.276587
MAE train: 0.660000	val: 0.886315	test: 0.846597

Epoch: 99
Loss: 0.9927597939968109
RMSE train: 0.919046	val: 1.392569	test: 1.305851
MAE train: 0.673986	val: 0.898592	test: 0.852924

Epoch: 100
Loss: 0.9688821732997894
RMSE train: 0.925405	val: 1.421299	test: 1.308346
MAE train: 0.678849	val: 0.913707	test: 0.877742

Epoch: 101
Loss: 1.142898976802826
RMSE train: 0.928407	val: 1.451413	test: 1.277936
MAE train: 0.691546	val: 0.938930	test: 0.907898

Epoch: 102
Loss: 0.9178369343280792
RMSE train: 0.908382	val: 1.431110	test: 1.235394
MAE train: 0.676203	val: 0.931280	test: 0.900512

Epoch: 103
Loss: 0.9868275225162506
RMSE train: 0.855798	val: 1.355089	test: 1.179216
MAE train: 0.630716	val: 0.885050	test: 0.851744

Epoch: 104
Loss: 1.0291063785552979
RMSE train: 0.865181	val: 1.363011	test: 1.186523
MAE train: 0.640778	val: 0.897397	test: 0.841503

Epoch: 105
Loss: 0.9838693737983704
RMSE train: 0.927804	val: 1.392371	test: 1.249079
MAE train: 0.695267	val: 0.906771	test: 0.854956

Epoch: 106
Loss: 0.8686542212963104
RMSE train: 1.022749	val: 1.481541	test: 1.339218
MAE train: 0.757288	val: 0.962518	test: 0.897042

Epoch: 107
Loss: 1.0144325196743011
RMSE train: 1.077374	val: 1.556802	test: 1.420129
MAE train: 0.782403	val: 1.005998	test: 0.939256

Epoch: 108
Loss: 0.9964310228824615
RMSE train: 1.023194	val: 1.515587	test: 1.408267
MAE train: 0.737687	val: 0.973590	test: 0.929294

Epoch: 109
Loss: 0.9521839022636414
RMSE train: 0.901985	val: 1.421994	test: 1.317401
MAE train: 0.654670	val: 0.915579	test: 0.895939

Epoch: 110
Loss: 0.9409627020359039
RMSE train: 0.834778	val: 1.346559	test: 1.254831
MAE train: 0.603427	val: 0.862164	test: 0.868582

Epoch: 111
Loss: 1.1230208277702332
RMSE train: 0.816898	val: 1.314341	test: 1.221726
MAE train: 0.594691	val: 0.833386	test: 0.853449

Epoch: 112
Loss: 0.9270728826522827
RMSE train: 0.851191	val: 1.331433	test: 1.221010
MAE train: 0.622236	val: 0.832995	test: 0.845692

Epoch: 113
Loss: 1.1203904151916504
RMSE train: 0.906796	val: 1.413816	test: 1.256402
MAE train: 0.661228	val: 0.896021	test: 0.859125

Epoch: 114
Loss: 1.1021125316619873
RMSE train: 0.976377	val: 1.498799	test: 1.341708
MAE train: 0.716963	val: 0.964949	test: 0.898049

Epoch: 115
Loss: 0.9121943712234497
RMSE train: 0.968598	val: 1.525368	test: 1.347404
MAE train: 0.711795	val: 0.986350	test: 0.908320

Epoch: 116
Loss: 0.9909120202064514
RMSE train: 0.904594	val: 1.468655	test: 1.290232
MAE train: 0.660704	val: 0.942693	test: 0.881852

Epoch: 117
Loss: 0.895503968000412
RMSE train: 0.865659	val: 1.425459	test: 1.246674
MAE train: 0.626636	val: 0.897795	test: 0.855274

Epoch: 118
Loss: 0.9757833182811737
RMSE train: 0.867931	val: 1.448896	test: 1.231982
MAE train: 0.627481	val: 0.903631	test: 0.853796

Epoch: 119
Loss: 1.1930245757102966
RMSE train: 0.893656	val: 1.458883	test: 1.264632
MAE train: 0.649598	val: 0.909583	test: 0.848673

Epoch: 120
Loss: 0.8249040246009827
RMSE train: 0.907582	val: 1.449433	test: 1.278740
MAE train: 0.658904	val: 0.896892	test: 0.832129

Epoch: 121
Loss: 0.9232961237430573
RMSE train: 0.908361	val: 1.430713	test: 1.283595
MAE train: 0.664622	val: 0.887136	test: 0.835866

Early stopping
Best (RMSE):	 train: 0.760056	val: 1.218977	test: 1.166195
Best (MAE):	 train: 0.550035	val: 0.798085	test: 0.783978


Epoch: 84
Loss: 1.1785323023796082
RMSE train: 0.915605	val: 1.549898	test: 1.107608
MAE train: 0.679649	val: 1.034197	test: 0.859302

Epoch: 85
Loss: 1.2373672723770142
RMSE train: 0.869199	val: 1.513694	test: 1.044043
MAE train: 0.644363	val: 1.008126	test: 0.816864

Epoch: 86
Loss: 1.115139663219452
RMSE train: 0.822243	val: 1.484862	test: 1.076155
MAE train: 0.621410	val: 1.000462	test: 0.796790

Epoch: 87
Loss: 1.0988313555717468
RMSE train: 0.830866	val: 1.465820	test: 1.153932
MAE train: 0.624277	val: 0.995071	test: 0.794461

Epoch: 88
Loss: 1.1143646836280823
RMSE train: 0.840042	val: 1.435433	test: 1.181792
MAE train: 0.622873	val: 0.986978	test: 0.788476

Epoch: 89
Loss: 1.2912357449531555
RMSE train: 0.850451	val: 1.467013	test: 1.160434
MAE train: 0.639571	val: 1.014448	test: 0.786825

Epoch: 90
Loss: 1.0632688403129578
RMSE train: 0.833837	val: 1.494235	test: 1.085884
MAE train: 0.637717	val: 1.026913	test: 0.765113

Epoch: 91
Loss: 1.291606366634369
RMSE train: 0.834276	val: 1.525224	test: 1.057008
MAE train: 0.634452	val: 1.037196	test: 0.754243

Epoch: 92
Loss: 1.196058452129364
RMSE train: 0.851045	val: 1.553983	test: 1.149392
MAE train: 0.643566	val: 1.043292	test: 0.777708

Epoch: 93
Loss: 1.226980209350586
RMSE train: 0.890449	val: 1.592194	test: 1.259386
MAE train: 0.668772	val: 1.069226	test: 0.812186

Epoch: 94
Loss: 1.2137566804885864
RMSE train: 0.909409	val: 1.566767	test: 1.349300
MAE train: 0.672837	val: 1.046234	test: 0.828359

Epoch: 95
Loss: 1.0339264273643494
RMSE train: 0.946896	val: 1.548137	test: 1.440806
MAE train: 0.679604	val: 1.031620	test: 0.853679

Epoch: 96
Loss: 0.9733669757843018
RMSE train: 0.925066	val: 1.528477	test: 1.405293
MAE train: 0.662339	val: 1.023648	test: 0.854821

Epoch: 97
Loss: 1.1009507775306702
RMSE train: 0.898731	val: 1.549462	test: 1.303611
MAE train: 0.659297	val: 1.044909	test: 0.851372

Epoch: 98
Loss: 1.0695300102233887
RMSE train: 0.855412	val: 1.534683	test: 1.208513
MAE train: 0.635091	val: 1.015790	test: 0.824118

Epoch: 99
Loss: 0.9248457849025726
RMSE train: 0.843464	val: 1.560191	test: 1.049763
MAE train: 0.631387	val: 1.014753	test: 0.776578

Epoch: 100
Loss: 1.1122466325759888
RMSE train: 0.869863	val: 1.588393	test: 0.993600
MAE train: 0.645569	val: 1.033853	test: 0.746716

Epoch: 101
Loss: 1.0188352465629578
RMSE train: 0.866560	val: 1.600069	test: 0.990731
MAE train: 0.645288	val: 1.036868	test: 0.750449

Epoch: 102
Loss: 1.0093366205692291
RMSE train: 0.862364	val: 1.579121	test: 1.024654
MAE train: 0.648573	val: 1.007279	test: 0.766630

Epoch: 103
Loss: 1.2443106174468994
RMSE train: 0.900455	val: 1.570899	test: 1.112690
MAE train: 0.675400	val: 0.983142	test: 0.798460

Epoch: 104
Loss: 0.9331896901130676
RMSE train: 0.927460	val: 1.584723	test: 1.160440
MAE train: 0.697340	val: 1.004323	test: 0.827130

Epoch: 105
Loss: 1.2547029852867126
RMSE train: 0.872696	val: 1.585345	test: 1.092633
MAE train: 0.660198	val: 1.022133	test: 0.800096

Epoch: 106
Loss: 1.1569980382919312
RMSE train: 0.826500	val: 1.550893	test: 1.002990
MAE train: 0.624888	val: 1.025369	test: 0.741100

Epoch: 107
Loss: 0.8563783764839172
RMSE train: 0.793034	val: 1.519212	test: 0.940084
MAE train: 0.596042	val: 1.005105	test: 0.691875

Epoch: 108
Loss: 0.8705452978610992
RMSE train: 0.788546	val: 1.514589	test: 0.922562
MAE train: 0.587186	val: 1.000660	test: 0.696314

Epoch: 109
Loss: 0.9354281723499298
RMSE train: 0.815967	val: 1.566186	test: 0.930884
MAE train: 0.600665	val: 1.036216	test: 0.696685

Epoch: 110
Loss: 1.1810374855995178
RMSE train: 0.859629	val: 1.581165	test: 1.007684
MAE train: 0.624787	val: 1.038790	test: 0.721485

Epoch: 111
Loss: 0.8613826036453247
RMSE train: 0.869377	val: 1.542254	test: 1.117342
MAE train: 0.630337	val: 1.001802	test: 0.754337

Epoch: 112
Loss: 1.034838318824768
RMSE train: 0.851290	val: 1.476092	test: 1.258119
MAE train: 0.615436	val: 0.924862	test: 0.794099

Epoch: 113
Loss: 0.9688495099544525
RMSE train: 0.839027	val: 1.451516	test: 1.326196
MAE train: 0.609371	val: 0.897251	test: 0.814398

Epoch: 114
Loss: 1.0439441502094269
RMSE train: 0.834145	val: 1.495195	test: 1.301454
MAE train: 0.620093	val: 0.939417	test: 0.823289

Epoch: 115
Loss: 0.8959026634693146
RMSE train: 0.828213	val: 1.556510	test: 1.212614
MAE train: 0.624370	val: 1.000054	test: 0.818367

Epoch: 116
Loss: 1.0051433444023132
RMSE train: 0.853438	val: 1.622813	test: 1.151515
MAE train: 0.640839	val: 1.067009	test: 0.823711

Epoch: 117
Loss: 1.1195114850997925
RMSE train: 0.889198	val: 1.643579	test: 1.147001
MAE train: 0.665053	val: 1.080270	test: 0.822946

Epoch: 118
Loss: 0.9761607050895691
RMSE train: 0.909514	val: 1.604832	test: 1.175294
MAE train: 0.680642	val: 1.055224	test: 0.828176

Epoch: 119
Loss: 0.9275363683700562
RMSE train: 0.893803	val: 1.541861	test: 1.201957
MAE train: 0.669513	val: 1.014816	test: 0.821993

Epoch: 120
Loss: 0.952935129404068
RMSE train: 0.845862	val: 1.489546	test: 1.210781
MAE train: 0.633560	val: 0.974712	test: 0.809108

Epoch: 121
Loss: 0.9579590559005737
RMSE train: 0.801681	val: 1.469486	test: 1.172586
MAE train: 0.605589	val: 0.944210	test: 0.782714

Epoch: 122
Loss: 0.9492739140987396
RMSE train: 0.805090	val: 1.496276	test: 1.172196
MAE train: 0.613717	val: 0.963797	test: 0.787107

Epoch: 123
Loss: 1.146201729774475
RMSE train: 0.786967	val: 1.494020	test: 1.128837
MAE train: 0.599506	val: 0.953498	test: 0.776006

Early stopping
Best (RMSE):	 train: 0.840042	val: 1.435433	test: 1.181792
Best (MAE):	 train: 0.622873	val: 0.986978	test: 0.788476


Epoch: 84
Loss: 1.2480642199516296
RMSE train: 0.971956	val: 1.532507	test: 1.206764
MAE train: 0.702512	val: 1.025060	test: 0.813031

Epoch: 85
Loss: 1.2797743082046509
RMSE train: 0.938046	val: 1.545914	test: 1.168955
MAE train: 0.678614	val: 1.011454	test: 0.811249

Epoch: 86
Loss: 1.1866605877876282
RMSE train: 0.886712	val: 1.521935	test: 1.108304
MAE train: 0.642607	val: 0.979088	test: 0.799368

Epoch: 87
Loss: 1.2098267674446106
RMSE train: 0.850448	val: 1.456588	test: 1.088807
MAE train: 0.621748	val: 0.918763	test: 0.801603

Epoch: 88
Loss: 1.3079253435134888
RMSE train: 0.864745	val: 1.427031	test: 1.130454
MAE train: 0.634335	val: 0.906751	test: 0.832483

Epoch: 89
Loss: 1.1982364058494568
RMSE train: 0.825601	val: 1.396764	test: 1.108623
MAE train: 0.604271	val: 0.881733	test: 0.822896

Epoch: 90
Loss: 1.0633981227874756
RMSE train: 0.795483	val: 1.407857	test: 1.088963
MAE train: 0.582543	val: 0.917847	test: 0.806142

Epoch: 91
Loss: 1.109079658985138
RMSE train: 0.771796	val: 1.409446	test: 1.079320
MAE train: 0.568941	val: 0.937593	test: 0.796701

Epoch: 92
Loss: 1.1365901231765747
RMSE train: 0.776182	val: 1.413386	test: 1.084069
MAE train: 0.574414	val: 0.954759	test: 0.785385

Epoch: 93
Loss: 1.060520052909851
RMSE train: 0.816293	val: 1.413162	test: 1.119021
MAE train: 0.612984	val: 0.968254	test: 0.802516

Epoch: 94
Loss: 1.2377004027366638
RMSE train: 0.911315	val: 1.424539	test: 1.208038
MAE train: 0.688966	val: 0.982052	test: 0.860078

Epoch: 95
Loss: 0.8938269019126892
RMSE train: 0.925902	val: 1.440343	test: 1.189172
MAE train: 0.706910	val: 0.984470	test: 0.848487

Epoch: 96
Loss: 1.1221763491630554
RMSE train: 0.966620	val: 1.476402	test: 1.245525
MAE train: 0.734563	val: 1.013317	test: 0.862030

Epoch: 97
Loss: 1.2405244708061218
RMSE train: 0.940490	val: 1.493033	test: 1.198139
MAE train: 0.721548	val: 1.017361	test: 0.844484

Epoch: 98
Loss: 0.9620512425899506
RMSE train: 0.914664	val: 1.491644	test: 1.160431
MAE train: 0.697043	val: 1.016023	test: 0.817500

Epoch: 99
Loss: 1.0048796236515045
RMSE train: 0.925436	val: 1.515794	test: 1.134360
MAE train: 0.697615	val: 1.034109	test: 0.802995

Epoch: 100
Loss: 1.0510360598564148
RMSE train: 0.966430	val: 1.563350	test: 1.148986
MAE train: 0.725235	val: 1.067256	test: 0.800683

Epoch: 101
Loss: 1.166079819202423
RMSE train: 0.994867	val: 1.591269	test: 1.156978
MAE train: 0.741640	val: 1.084950	test: 0.799501

Epoch: 102
Loss: 1.1716388463974
RMSE train: 0.965860	val: 1.559334	test: 1.144540
MAE train: 0.722725	val: 1.059223	test: 0.789429

Epoch: 103
Loss: 0.8981722593307495
RMSE train: 0.909551	val: 1.517305	test: 1.104620
MAE train: 0.683813	val: 1.036941	test: 0.766365

Epoch: 104
Loss: 0.8762637674808502
RMSE train: 0.840581	val: 1.467440	test: 1.051032
MAE train: 0.637472	val: 1.016828	test: 0.736403

Epoch: 105
Loss: 1.1636600494384766
RMSE train: 0.806040	val: 1.439837	test: 1.018134
MAE train: 0.613617	val: 1.006662	test: 0.728692

Epoch: 106
Loss: 1.0625129342079163
RMSE train: 0.806723	val: 1.406567	test: 1.052410
MAE train: 0.619105	val: 0.998686	test: 0.746036

Epoch: 107
Loss: 1.0636422634124756
RMSE train: 0.843968	val: 1.424046	test: 1.123531
MAE train: 0.645855	val: 1.010041	test: 0.778840

Epoch: 108
Loss: 0.9624689519405365
RMSE train: 0.879020	val: 1.474731	test: 1.184896
MAE train: 0.670008	val: 1.037810	test: 0.809757

Epoch: 109
Loss: 1.041387140750885
RMSE train: 0.888346	val: 1.509411	test: 1.191819
MAE train: 0.673567	val: 1.046564	test: 0.817641

Epoch: 110
Loss: 0.9770313203334808
RMSE train: 0.905366	val: 1.549958	test: 1.196038
MAE train: 0.695054	val: 1.083205	test: 0.832416

Epoch: 111
Loss: 0.9813252389431
RMSE train: 0.859700	val: 1.517823	test: 1.164133
MAE train: 0.656572	val: 1.054079	test: 0.812477

Epoch: 112
Loss: 1.1066482365131378
RMSE train: 0.840705	val: 1.490176	test: 1.156432
MAE train: 0.632195	val: 1.018248	test: 0.801340

Epoch: 113
Loss: 1.070193886756897
RMSE train: 0.826565	val: 1.485641	test: 1.118374
MAE train: 0.621047	val: 1.010365	test: 0.797338

Epoch: 114
Loss: 1.047090232372284
RMSE train: 0.847500	val: 1.524135	test: 1.141413
MAE train: 0.635823	val: 1.034260	test: 0.800084

Epoch: 115
Loss: 0.9845435619354248
RMSE train: 0.888554	val: 1.547147	test: 1.192984
MAE train: 0.660145	val: 1.045130	test: 0.812148

Epoch: 116
Loss: 0.9710153043270111
RMSE train: 0.859831	val: 1.507035	test: 1.162967
MAE train: 0.641860	val: 1.013625	test: 0.794605

Epoch: 117
Loss: 0.8168195486068726
RMSE train: 0.840043	val: 1.482483	test: 1.116101
MAE train: 0.633148	val: 0.996834	test: 0.785343

Epoch: 118
Loss: 0.9899746775627136
RMSE train: 0.856274	val: 1.458584	test: 1.135387
MAE train: 0.644374	val: 0.975046	test: 0.797860

Epoch: 119
Loss: 0.9039421677589417
RMSE train: 0.874366	val: 1.464180	test: 1.168089
MAE train: 0.658698	val: 0.978964	test: 0.810771

Epoch: 120
Loss: 1.1451581716537476
RMSE train: 0.925829	val: 1.491526	test: 1.251437
MAE train: 0.686916	val: 0.998649	test: 0.830941

Epoch: 121
Loss: 0.947018563747406
RMSE train: 0.985431	val: 1.537927	test: 1.306024
MAE train: 0.712836	val: 1.028010	test: 0.838933

Epoch: 122
Loss: 0.9454587399959564
RMSE train: 0.984494	val: 1.545420	test: 1.285048
MAE train: 0.711073	val: 1.025076	test: 0.842379

Epoch: 123
Loss: 0.8902280330657959
RMSE train: 0.937378	val: 1.530027	test: 1.226823
MAE train: 0.683775	val: 1.000956	test: 0.838252

Epoch: 124
Loss: 0.988133430480957
RMSE train: 0.857155	val: 1.497082	test: 1.130132
MAE train: 0.627576	val: 0.976247	test: 0.794046

Early stopping
Best (RMSE):	 train: 0.825601	val: 1.396764	test: 1.108623
Best (MAE):	 train: 0.604271	val: 0.881733	test: 0.822896

MAE train: 0.554889	val: 0.737685	test: 0.880105

Epoch: 145
Loss: 1.114650547504425
RMSE train: 0.721937	val: 1.049883	test: 1.272766
MAE train: 0.561790	val: 0.769267	test: 0.888725

Epoch: 146
Loss: 0.9198439121246338
RMSE train: 0.766960	val: 1.108850	test: 1.287833
MAE train: 0.594424	val: 0.813848	test: 0.899430

Epoch: 147
Loss: 0.8971320390701294
RMSE train: 0.802260	val: 1.142995	test: 1.295644
MAE train: 0.610273	val: 0.839316	test: 0.895364

Epoch: 148
Loss: 0.9389239549636841
RMSE train: 0.776790	val: 1.144458	test: 1.287287
MAE train: 0.580435	val: 0.841873	test: 0.883931

Epoch: 149
Loss: 0.8255668580532074
RMSE train: 0.730294	val: 1.098285	test: 1.258815
MAE train: 0.538089	val: 0.804238	test: 0.859170

Epoch: 150
Loss: 0.8560698628425598
RMSE train: 0.689191	val: 1.023801	test: 1.238646
MAE train: 0.509054	val: 0.762103	test: 0.834360

Epoch: 151
Loss: 0.9468449354171753
RMSE train: 0.677609	val: 0.983087	test: 1.232566
MAE train: 0.504750	val: 0.733575	test: 0.839802

Epoch: 152
Loss: 0.8629125654697418
RMSE train: 0.714443	val: 0.978080	test: 1.256806
MAE train: 0.537621	val: 0.722342	test: 0.862056

Epoch: 153
Loss: 0.8134497106075287
RMSE train: 0.755893	val: 1.000515	test: 1.291092
MAE train: 0.576203	val: 0.733036	test: 0.886986

Epoch: 154
Loss: 1.0111547112464905
RMSE train: 0.785036	val: 1.040530	test: 1.316454
MAE train: 0.606972	val: 0.767881	test: 0.904478

Epoch: 155
Loss: 0.6811119318008423
RMSE train: 0.769106	val: 1.053582	test: 1.304782
MAE train: 0.599995	val: 0.780469	test: 0.884274

Epoch: 156
Loss: 0.7813416719436646
RMSE train: 0.752919	val: 1.048096	test: 1.281072
MAE train: 0.578579	val: 0.776122	test: 0.858712

Epoch: 157
Loss: 0.8857872784137726
RMSE train: 0.739459	val: 1.018109	test: 1.260672
MAE train: 0.558132	val: 0.751970	test: 0.846224

Epoch: 158
Loss: 0.8719130456447601
RMSE train: 0.753551	val: 1.017940	test: 1.257941
MAE train: 0.564691	val: 0.747213	test: 0.854367

Epoch: 159
Loss: 0.9302653074264526
RMSE train: 0.765609	val: 1.040061	test: 1.265500
MAE train: 0.571537	val: 0.756080	test: 0.863722

Epoch: 160
Loss: 1.060936689376831
RMSE train: 0.778364	val: 1.070107	test: 1.291403
MAE train: 0.591824	val: 0.777688	test: 0.891320

Epoch: 161
Loss: 0.9574498534202576
RMSE train: 0.726982	val: 1.051469	test: 1.273491
MAE train: 0.562436	val: 0.765193	test: 0.876413

Epoch: 162
Loss: 0.9074475765228271
RMSE train: 0.661743	val: 1.028436	test: 1.231163
MAE train: 0.511728	val: 0.746892	test: 0.828607

Epoch: 163
Loss: 0.8813809454441071
RMSE train: 0.671524	val: 1.071044	test: 1.236183
MAE train: 0.514572	val: 0.779882	test: 0.839164

Epoch: 164
Loss: 1.0513208210468292
RMSE train: 0.675058	val: 1.093042	test: 1.245798
MAE train: 0.509354	val: 0.793699	test: 0.846730

Epoch: 165
Loss: 1.0545121729373932
RMSE train: 0.640716	val: 1.076091	test: 1.258136
MAE train: 0.482248	val: 0.771666	test: 0.848225

Epoch: 166
Loss: 0.856797456741333
RMSE train: 0.688744	val: 1.089706	test: 1.319371
MAE train: 0.525280	val: 0.793389	test: 0.911353

Epoch: 167
Loss: 0.9916673004627228
RMSE train: 0.776520	val: 1.119048	test: 1.377538
MAE train: 0.597535	val: 0.836311	test: 0.973873

Epoch: 168
Loss: 0.7897298634052277
RMSE train: 0.742461	val: 1.081686	test: 1.344934
MAE train: 0.558439	val: 0.798938	test: 0.922574

Epoch: 169
Loss: 1.0100316107273102
RMSE train: 0.711430	val: 1.032275	test: 1.296783
MAE train: 0.527157	val: 0.758122	test: 0.858437

Early stopping
Best (RMSE):	 train: 0.726177	val: 0.947121	test: 1.216182
Best (MAE):	 train: 0.548989	val: 0.713163	test: 0.812502


Epoch: 84
Loss: 1.0915334224700928
RMSE train: 0.869497	val: 1.281406	test: 1.202047
MAE train: 0.640412	val: 0.835296	test: 0.846009

Epoch: 85
Loss: 1.1054648756980896
RMSE train: 0.861191	val: 1.282830	test: 1.210951
MAE train: 0.638006	val: 0.832864	test: 0.842476

Epoch: 86
Loss: 1.1221932768821716
RMSE train: 0.888437	val: 1.341525	test: 1.273621
MAE train: 0.664158	val: 0.875650	test: 0.868560

Epoch: 87
Loss: 1.447638213634491
RMSE train: 0.917198	val: 1.402867	test: 1.299793
MAE train: 0.686716	val: 0.922697	test: 0.887485

Epoch: 88
Loss: 1.1673076152801514
RMSE train: 0.935474	val: 1.428312	test: 1.291539
MAE train: 0.708529	val: 0.936938	test: 0.894916

Epoch: 89
Loss: 1.1887968182563782
RMSE train: 0.887270	val: 1.354367	test: 1.245449
MAE train: 0.673061	val: 0.879814	test: 0.864472

Epoch: 90
Loss: 1.1328769326210022
RMSE train: 0.867639	val: 1.274831	test: 1.222521
MAE train: 0.659726	val: 0.824935	test: 0.850912

Epoch: 91
Loss: 1.0824523866176605
RMSE train: 0.888090	val: 1.243738	test: 1.198384
MAE train: 0.674011	val: 0.815348	test: 0.858512

Epoch: 92
Loss: 1.012658715248108
RMSE train: 0.935154	val: 1.296959	test: 1.208805
MAE train: 0.703120	val: 0.851395	test: 0.887669

Epoch: 93
Loss: 0.9683656692504883
RMSE train: 0.950175	val: 1.358825	test: 1.222710
MAE train: 0.705268	val: 0.890990	test: 0.903495

Epoch: 94
Loss: 1.258739709854126
RMSE train: 0.928181	val: 1.381335	test: 1.231617
MAE train: 0.690570	val: 0.919441	test: 0.883516

Epoch: 95
Loss: 1.0668915510177612
RMSE train: 0.903396	val: 1.385577	test: 1.228497
MAE train: 0.674550	val: 0.935821	test: 0.849571

Epoch: 96
Loss: 1.103657603263855
RMSE train: 0.842754	val: 1.356105	test: 1.189947
MAE train: 0.633002	val: 0.914983	test: 0.825185

Epoch: 97
Loss: 1.1509395241737366
RMSE train: 0.790183	val: 1.292224	test: 1.158762
MAE train: 0.598129	val: 0.862956	test: 0.811366

Epoch: 98
Loss: 1.1535342931747437
RMSE train: 0.802492	val: 1.271061	test: 1.138892
MAE train: 0.612306	val: 0.854966	test: 0.806533

Epoch: 99
Loss: 1.1272355616092682
RMSE train: 0.785779	val: 1.239547	test: 1.101643
MAE train: 0.598214	val: 0.825329	test: 0.795129

Epoch: 100
Loss: 1.122624695301056
RMSE train: 0.794950	val: 1.219401	test: 1.071938
MAE train: 0.601079	val: 0.816576	test: 0.790620

Epoch: 101
Loss: 1.0197433829307556
RMSE train: 0.771924	val: 1.163153	test: 1.030882
MAE train: 0.582520	val: 0.778096	test: 0.763397

Epoch: 102
Loss: 0.9759305417537689
RMSE train: 0.776702	val: 1.163857	test: 1.012601
MAE train: 0.582667	val: 0.777638	test: 0.759251

Epoch: 103
Loss: 0.9566680192947388
RMSE train: 0.796678	val: 1.242802	test: 1.020829
MAE train: 0.591713	val: 0.808285	test: 0.751903

Epoch: 104
Loss: 0.9776136577129364
RMSE train: 0.818670	val: 1.332776	test: 1.062077
MAE train: 0.606088	val: 0.866475	test: 0.769954

Epoch: 105
Loss: 0.965512216091156
RMSE train: 0.827334	val: 1.367086	test: 1.105088
MAE train: 0.614248	val: 0.889692	test: 0.785186

Epoch: 106
Loss: 1.3377801179885864
RMSE train: 0.802142	val: 1.317656	test: 1.122823
MAE train: 0.600619	val: 0.856706	test: 0.779424

Epoch: 107
Loss: 0.896956205368042
RMSE train: 0.781418	val: 1.253544	test: 1.125136
MAE train: 0.593550	val: 0.814039	test: 0.768098

Epoch: 108
Loss: 1.2246696054935455
RMSE train: 0.790085	val: 1.241226	test: 1.144402
MAE train: 0.602656	val: 0.809721	test: 0.785183

Epoch: 109
Loss: 0.9239464998245239
RMSE train: 0.821761	val: 1.283023	test: 1.172679
MAE train: 0.625984	val: 0.845917	test: 0.811279

Epoch: 110
Loss: 1.0425300002098083
RMSE train: 0.821243	val: 1.306237	test: 1.160180
MAE train: 0.621840	val: 0.864506	test: 0.818722

Epoch: 111
Loss: 0.9040082693099976
RMSE train: 0.784110	val: 1.278413	test: 1.117685
MAE train: 0.584504	val: 0.835667	test: 0.787771

Epoch: 112
Loss: 0.9583562314510345
RMSE train: 0.790202	val: 1.288544	test: 1.104990
MAE train: 0.586173	val: 0.834551	test: 0.776902

Epoch: 113
Loss: 0.8313762545585632
RMSE train: 0.772607	val: 1.297421	test: 1.090895
MAE train: 0.571394	val: 0.838979	test: 0.762044

Epoch: 114
Loss: 0.9503648281097412
RMSE train: 0.714263	val: 1.300509	test: 1.098441
MAE train: 0.532890	val: 0.838613	test: 0.748635

Epoch: 115
Loss: 1.0656581223011017
RMSE train: 0.688489	val: 1.297113	test: 1.137189
MAE train: 0.517359	val: 0.830645	test: 0.767655

Epoch: 116
Loss: 0.9505598247051239
RMSE train: 0.714469	val: 1.280884	test: 1.160998
MAE train: 0.541366	val: 0.841104	test: 0.798997

Epoch: 117
Loss: 0.8372435569763184
RMSE train: 0.702389	val: 1.264131	test: 1.150173
MAE train: 0.537183	val: 0.841632	test: 0.799972

Epoch: 118
Loss: 0.8713337182998657
RMSE train: 0.696750	val: 1.260388	test: 1.136670
MAE train: 0.532276	val: 0.836621	test: 0.799601

Epoch: 119
Loss: 0.826549232006073
RMSE train: 0.706405	val: 1.247855	test: 1.101351
MAE train: 0.534654	val: 0.821734	test: 0.786297

Epoch: 120
Loss: 0.8360461592674255
RMSE train: 0.760513	val: 1.281536	test: 1.085134
MAE train: 0.563566	val: 0.828924	test: 0.780774

Epoch: 121
Loss: 1.033728301525116
RMSE train: 0.860932	val: 1.353707	test: 1.093623
MAE train: 0.626777	val: 0.878277	test: 0.788996

Epoch: 122
Loss: 1.1906179785728455
RMSE train: 0.909063	val: 1.427263	test: 1.136982
MAE train: 0.679391	val: 0.917857	test: 0.820598

Epoch: 123
Loss: 0.8549753725528717
RMSE train: 0.917254	val: 1.432823	test: 1.160289
MAE train: 0.681775	val: 0.909037	test: 0.823824

Epoch: 124
Loss: 0.8032378852367401
RMSE train: 0.879728	val: 1.390732	test: 1.161993
MAE train: 0.650942	val: 0.880489	test: 0.811514

Epoch: 125
Loss: 1.018544226884842
RMSE train: 0.847177	val: 1.346704	test: 1.171981
MAE train: 0.631854	val: 0.877117	test: 0.811452

Epoch: 126
Loss: 0.7161732017993927
RMSE train: 0.771634	val: 1.279963	test: 1.148316
MAE train: 0.582120	val: 0.851293	test: 0.793842

Epoch: 127
Loss: 1.001271665096283
RMSE train: 0.775623	val: 1.285384	test: 1.179973
MAE train: 0.581012	val: 0.858369	test: 0.808561

Epoch: 128
Loss: 0.9337859451770782
RMSE train: 0.779087	val: 1.272525	test: 1.184925
MAE train: 0.584526	val: 0.845176	test: 0.805602

Epoch: 129
Loss: 1.0445255041122437
RMSE train: 0.791820	val: 1.291589	test: 1.175390
MAE train: 0.594803	val: 0.855218	test: 0.808740

Epoch: 130
Loss: 0.9554613530635834
RMSE train: 0.783510	val: 1.285390	test: 1.131270
MAE train: 0.590660	val: 0.843175	test: 0.781202

Epoch: 131
Loss: 1.0972239077091217
RMSE train: 0.814672	val: 1.308486	test: 1.105012
MAE train: 0.609366	val: 0.840925	test: 0.785919

Epoch: 132
Loss: 0.810479462146759
RMSE train: 0.830514	val: 1.305209	test: 1.088439
MAE train: 0.619560	val: 0.828665	test: 0.785507

Epoch: 133
Loss: 0.9065304696559906
RMSE train: 0.827577	val: 1.303373	test: 1.079372
MAE train: 0.615073	val: 0.826682	test: 0.778652

Epoch: 134
Loss: 0.9977862536907196
RMSE train: 0.853301	val: 1.328762	test: 1.095753
MAE train: 0.633151	val: 0.845328	test: 0.794847

Epoch: 135
Loss: 0.8044990599155426
RMSE train: 0.853017	val: 1.327098	test: 1.114428
MAE train: 0.640845	val: 0.852786	test: 0.802621

Epoch: 136
Loss: 0.8554833829402924
RMSE train: 0.818458	val: 1.279003	test: 1.122612
MAE train: 0.615996	val: 0.827677	test: 0.787037

Early stopping
Best (RMSE):	 train: 0.771924	val: 1.163153	test: 1.030882
Best (MAE):	 train: 0.582520	val: 0.778096	test: 0.763397


Epoch: 84
Loss: 1.4643099904060364
RMSE train: 0.877561	val: 1.455488	test: 1.492033
MAE train: 0.660473	val: 0.982389	test: 0.915149

Epoch: 85
Loss: 1.1113153100013733
RMSE train: 0.842969	val: 1.452733	test: 1.501515
MAE train: 0.636430	val: 0.970973	test: 0.907356

Epoch: 86
Loss: 1.0636526942253113
RMSE train: 0.800616	val: 1.453279	test: 1.484359
MAE train: 0.610250	val: 0.953271	test: 0.883019

Epoch: 87
Loss: 1.1091395616531372
RMSE train: 0.790449	val: 1.444589	test: 1.485719
MAE train: 0.603133	val: 0.937862	test: 0.882366

Epoch: 88
Loss: 1.0394787192344666
RMSE train: 0.794846	val: 1.423843	test: 1.473924
MAE train: 0.605533	val: 0.933375	test: 0.890027

Epoch: 89
Loss: 1.1899786591529846
RMSE train: 0.821194	val: 1.407160	test: 1.446642
MAE train: 0.633122	val: 0.938958	test: 0.902292

Epoch: 90
Loss: 1.0039189755916595
RMSE train: 0.860044	val: 1.404681	test: 1.451748
MAE train: 0.674520	val: 0.957145	test: 0.936729

Epoch: 91
Loss: 1.1300936341285706
RMSE train: 0.854013	val: 1.404043	test: 1.433376
MAE train: 0.671493	val: 0.959851	test: 0.946037

Epoch: 92
Loss: 1.0698709785938263
RMSE train: 0.790746	val: 1.381009	test: 1.387728
MAE train: 0.616826	val: 0.924920	test: 0.898448

Epoch: 93
Loss: 1.1435412764549255
RMSE train: 0.753404	val: 1.355646	test: 1.372716
MAE train: 0.575821	val: 0.883013	test: 0.847574

Epoch: 94
Loss: 1.0048123896121979
RMSE train: 0.719757	val: 1.378213	test: 1.383441
MAE train: 0.546973	val: 0.863762	test: 0.835768

Epoch: 95
Loss: 0.9984943568706512
RMSE train: 0.688778	val: 1.397298	test: 1.373143
MAE train: 0.520537	val: 0.852023	test: 0.823794

Epoch: 96
Loss: 1.0892850160598755
RMSE train: 0.683043	val: 1.383839	test: 1.341197
MAE train: 0.521221	val: 0.838086	test: 0.818104

Epoch: 97
Loss: 1.1233620047569275
RMSE train: 0.716732	val: 1.373238	test: 1.312184
MAE train: 0.552813	val: 0.850089	test: 0.844834

Epoch: 98
Loss: 0.9776617288589478
RMSE train: 0.771549	val: 1.350329	test: 1.315430
MAE train: 0.596201	val: 0.876716	test: 0.884601

Epoch: 99
Loss: 0.9470760226249695
RMSE train: 0.805083	val: 1.348241	test: 1.312431
MAE train: 0.626422	val: 0.904136	test: 0.911948

Epoch: 100
Loss: 1.092972457408905
RMSE train: 0.806879	val: 1.376359	test: 1.306655
MAE train: 0.626762	val: 0.931383	test: 0.916924

Epoch: 101
Loss: 1.2638147473335266
RMSE train: 0.774732	val: 1.373768	test: 1.297413
MAE train: 0.600180	val: 0.918906	test: 0.909197

Epoch: 102
Loss: 0.9307456016540527
RMSE train: 0.745223	val: 1.336328	test: 1.285073
MAE train: 0.575688	val: 0.884431	test: 0.887078

Epoch: 103
Loss: 1.0772618055343628
RMSE train: 0.750203	val: 1.284910	test: 1.271232
MAE train: 0.584200	val: 0.857003	test: 0.878444

Epoch: 104
Loss: 0.897333025932312
RMSE train: 0.769591	val: 1.264160	test: 1.286573
MAE train: 0.595543	val: 0.840105	test: 0.884160

Epoch: 105
Loss: 0.9230705201625824
RMSE train: 0.753884	val: 1.293661	test: 1.312769
MAE train: 0.577694	val: 0.846431	test: 0.884863

Epoch: 106
Loss: 1.0551663637161255
RMSE train: 0.767144	val: 1.320004	test: 1.330648
MAE train: 0.586628	val: 0.862004	test: 0.895307

Epoch: 107
Loss: 0.9164113998413086
RMSE train: 0.788883	val: 1.348580	test: 1.350886
MAE train: 0.602999	val: 0.872604	test: 0.897230

Epoch: 108
Loss: 1.074793517589569
RMSE train: 0.802703	val: 1.377843	test: 1.371490
MAE train: 0.615240	val: 0.893277	test: 0.906573

Epoch: 109
Loss: 1.1142516732215881
RMSE train: 0.764720	val: 1.361892	test: 1.395548
MAE train: 0.580706	val: 0.872095	test: 0.907463

Epoch: 110
Loss: 0.8975622355937958
RMSE train: 0.742660	val: 1.324508	test: 1.412096
MAE train: 0.555738	val: 0.838976	test: 0.903420

Epoch: 111
Loss: 0.8491838574409485
RMSE train: 0.740188	val: 1.276489	test: 1.395943
MAE train: 0.553262	val: 0.805992	test: 0.891257

Epoch: 112
Loss: 0.9543323814868927
RMSE train: 0.773669	val: 1.265232	test: 1.371787
MAE train: 0.580261	val: 0.817266	test: 0.885357

Epoch: 113
Loss: 1.0097038745880127
RMSE train: 0.819072	val: 1.280148	test: 1.349816
MAE train: 0.619032	val: 0.840825	test: 0.885746

Epoch: 114
Loss: 0.8323718905448914
RMSE train: 0.829574	val: 1.324069	test: 1.352771
MAE train: 0.626881	val: 0.855562	test: 0.896069

Epoch: 115
Loss: 0.9203155338764191
RMSE train: 0.853532	val: 1.385090	test: 1.381316
MAE train: 0.644569	val: 0.884007	test: 0.914014

Epoch: 116
Loss: 0.9262611865997314
RMSE train: 0.814792	val: 1.379853	test: 1.366476
MAE train: 0.617106	val: 0.878328	test: 0.887543

Epoch: 117
Loss: 1.0636690855026245
RMSE train: 0.841680	val: 1.372976	test: 1.339655
MAE train: 0.644467	val: 0.892376	test: 0.883373

Epoch: 118
Loss: 0.999786376953125
RMSE train: 0.851468	val: 1.360329	test: 1.313727
MAE train: 0.661910	val: 0.900560	test: 0.893511

Epoch: 119
Loss: 0.9037682116031647
RMSE train: 0.791603	val: 1.324321	test: 1.279098
MAE train: 0.619405	val: 0.873577	test: 0.870369

Epoch: 120
Loss: 0.8547472655773163
RMSE train: 0.751983	val: 1.312739	test: 1.281263
MAE train: 0.585674	val: 0.856641	test: 0.863817

Epoch: 121
Loss: 0.8386017382144928
RMSE train: 0.697585	val: 1.274672	test: 1.306505
MAE train: 0.538210	val: 0.822440	test: 0.851419

Epoch: 122
Loss: 0.9176954329013824
RMSE train: 0.666158	val: 1.269526	test: 1.320673
MAE train: 0.510141	val: 0.818807	test: 0.848348

Epoch: 123
Loss: 0.7986100912094116
RMSE train: 0.663412	val: 1.251449	test: 1.308249
MAE train: 0.508611	val: 0.819603	test: 0.848165

Epoch: 124
Loss: 0.8233533799648285
RMSE train: 0.690835	val: 1.267152	test: 1.308866
MAE train: 0.526429	val: 0.829578	test: 0.860834

Epoch: 125
Loss: 0.9798078536987305
RMSE train: 0.748340	val: 1.288467	test: 1.314584
MAE train: 0.570759	val: 0.859475	test: 0.868336

Epoch: 126
Loss: 1.0204333364963531
RMSE train: 0.717871	val: 1.259240	test: 1.301147
MAE train: 0.554196	val: 0.845599	test: 0.867014

Epoch: 127
Loss: 0.9873387217521667
RMSE train: 0.677965	val: 1.214866	test: 1.295041
MAE train: 0.529075	val: 0.829764	test: 0.877435

Epoch: 128
Loss: 0.9195751547813416
RMSE train: 0.685959	val: 1.199435	test: 1.310052
MAE train: 0.527585	val: 0.821842	test: 0.878498

Epoch: 129
Loss: 0.8158362507820129
RMSE train: 0.676244	val: 1.163264	test: 1.303357
MAE train: 0.508289	val: 0.789623	test: 0.850708

Epoch: 130
Loss: 0.9166840612888336
RMSE train: 0.668991	val: 1.190557	test: 1.309227
MAE train: 0.501888	val: 0.801071	test: 0.836351

Epoch: 131
Loss: 0.9017125368118286
RMSE train: 0.682003	val: 1.227694	test: 1.307291
MAE train: 0.517722	val: 0.823893	test: 0.833321

Epoch: 132
Loss: 0.7894898951053619
RMSE train: 0.689444	val: 1.253290	test: 1.316923
MAE train: 0.528706	val: 0.838388	test: 0.841955

Epoch: 133
Loss: 0.7882817089557648
RMSE train: 0.701640	val: 1.283789	test: 1.330030
MAE train: 0.541206	val: 0.863083	test: 0.863760

Epoch: 134
Loss: 0.8813363909721375
RMSE train: 0.712030	val: 1.307526	test: 1.316356
MAE train: 0.552660	val: 0.889592	test: 0.877457

Epoch: 135
Loss: 0.7917819023132324
RMSE train: 0.716164	val: 1.313328	test: 1.287673
MAE train: 0.556179	val: 0.897652	test: 0.877382

Epoch: 136
Loss: 0.8177196681499481
RMSE train: 0.707670	val: 1.270232	test: 1.248221
MAE train: 0.546473	val: 0.865055	test: 0.852176

Epoch: 137
Loss: 0.8973608016967773
RMSE train: 0.725471	val: 1.235151	test: 1.218578
MAE train: 0.555031	val: 0.844094	test: 0.836739

Epoch: 138
Loss: 0.9052930772304535
RMSE train: 0.768636	val: 1.225978	test: 1.220700
MAE train: 0.583390	val: 0.841589	test: 0.849145

Epoch: 139
Loss: 0.7480284571647644
RMSE train: 0.792846	val: 1.211476	test: 1.221608
MAE train: 0.604460	val: 0.833468	test: 0.857760

Epoch: 140
Loss: 0.9604596197605133
RMSE train: 0.805640	val: 1.236624	test: 1.227627
MAE train: 0.619606	val: 0.843870	test: 0.868306

Epoch: 141
Loss: 0.8010590970516205
RMSE train: 0.805671	val: 1.256954	test: 1.242121
MAE train: 0.618429	val: 0.859063	test: 0.883340

Epoch: 142
Loss: 0.7339612245559692
RMSE train: 0.796937	val: 1.260476	test: 1.239368
MAE train: 0.612337	val: 0.856955	test: 0.879007

Epoch: 143
Loss: 0.874399334192276
RMSE train: 0.803649	val: 1.268614	test: 1.229344
MAE train: 0.611400	val: 0.855396	test: 0.863304

Epoch: 144
Loss: 0.8144331872463226
RMSE train: 0.752823	val: 1.250058	test: 1.203638

Epoch: 84
Loss: 1.2400313019752502
RMSE train: 0.968300	val: 1.515948	test: 1.150226
MAE train: 0.721941	val: 1.008764	test: 0.875284

Epoch: 85
Loss: 1.2733147144317627
RMSE train: 1.039490	val: 1.576696	test: 1.190033
MAE train: 0.779470	val: 1.061177	test: 0.911697

Epoch: 86
Loss: 1.215937316417694
RMSE train: 1.113813	val: 1.637692	test: 1.264722
MAE train: 0.837886	val: 1.112223	test: 0.952000

Epoch: 87
Loss: 1.05755153298378
RMSE train: 1.137685	val: 1.656713	test: 1.305082
MAE train: 0.865088	val: 1.130612	test: 0.970391

Epoch: 88
Loss: 1.238001823425293
RMSE train: 1.059757	val: 1.592037	test: 1.274789
MAE train: 0.808776	val: 1.090171	test: 0.942257

Epoch: 89
Loss: 1.093421459197998
RMSE train: 0.949872	val: 1.518807	test: 1.216899
MAE train: 0.718133	val: 1.055667	test: 0.878636

Epoch: 90
Loss: 1.0895102322101593
RMSE train: 0.874552	val: 1.488059	test: 1.152171
MAE train: 0.643403	val: 1.034532	test: 0.808337

Epoch: 91
Loss: 1.1469768285751343
RMSE train: 0.858283	val: 1.488670	test: 1.111297
MAE train: 0.625781	val: 1.040321	test: 0.798127

Epoch: 92
Loss: 1.1315396428108215
RMSE train: 0.870530	val: 1.508879	test: 1.086620
MAE train: 0.642935	val: 1.052345	test: 0.808566

Epoch: 93
Loss: 0.9302370250225067
RMSE train: 0.884728	val: 1.508396	test: 1.090986
MAE train: 0.667060	val: 1.051351	test: 0.821221

Epoch: 94
Loss: 1.0905082821846008
RMSE train: 0.881184	val: 1.468750	test: 1.112465
MAE train: 0.665256	val: 1.014107	test: 0.821695

Epoch: 95
Loss: 1.1565942764282227
RMSE train: 0.914966	val: 1.467101	test: 1.163874
MAE train: 0.686225	val: 0.994664	test: 0.836335

Epoch: 96
Loss: 1.105301022529602
RMSE train: 0.961840	val: 1.490554	test: 1.238295
MAE train: 0.716485	val: 1.004559	test: 0.866558

Epoch: 97
Loss: 1.1842020153999329
RMSE train: 0.950936	val: 1.494753	test: 1.231271
MAE train: 0.708535	val: 1.001694	test: 0.869344

Epoch: 98
Loss: 1.0212394297122955
RMSE train: 0.904863	val: 1.479059	test: 1.198093
MAE train: 0.670591	val: 0.995174	test: 0.849025

Epoch: 99
Loss: 1.1598803400993347
RMSE train: 0.864135	val: 1.470196	test: 1.135460
MAE train: 0.641492	val: 1.000689	test: 0.827000

Epoch: 100
Loss: 1.001013308763504
RMSE train: 0.860529	val: 1.487769	test: 1.073729
MAE train: 0.644595	val: 1.020455	test: 0.806391

Epoch: 101
Loss: 0.9676910638809204
RMSE train: 0.890687	val: 1.520932	test: 1.056095
MAE train: 0.672745	val: 1.048966	test: 0.797590

Epoch: 102
Loss: 1.3179135918617249
RMSE train: 0.962474	val: 1.526061	test: 1.112569
MAE train: 0.727084	val: 1.043222	test: 0.823709

Epoch: 103
Loss: 1.046468198299408
RMSE train: 0.956731	val: 1.492310	test: 1.155077
MAE train: 0.724509	val: 1.015675	test: 0.839999

Epoch: 104
Loss: 0.962031364440918
RMSE train: 0.873946	val: 1.433204	test: 1.119433
MAE train: 0.667883	val: 0.960145	test: 0.816901

Epoch: 105
Loss: 1.4693795442581177
RMSE train: 0.767507	val: 1.376958	test: 1.027147
MAE train: 0.595622	val: 0.932165	test: 0.766517

Epoch: 106
Loss: 1.0681061148643494
RMSE train: 0.737087	val: 1.313698	test: 1.049717
MAE train: 0.565982	val: 0.896222	test: 0.754920

Epoch: 107
Loss: 0.9575579762458801
RMSE train: 0.773450	val: 1.305870	test: 1.151300
MAE train: 0.570682	val: 0.902509	test: 0.782842

Epoch: 108
Loss: 1.0991065204143524
RMSE train: 0.850776	val: 1.370621	test: 1.221072
MAE train: 0.609236	val: 0.925092	test: 0.803075

Epoch: 109
Loss: 0.9738971889019012
RMSE train: 0.952009	val: 1.472421	test: 1.267136
MAE train: 0.685776	val: 0.993912	test: 0.836033

Epoch: 110
Loss: 1.1036016941070557
RMSE train: 0.992810	val: 1.567597	test: 1.268948
MAE train: 0.730808	val: 1.077571	test: 0.858996

Epoch: 111
Loss: 1.0568683743476868
RMSE train: 0.959288	val: 1.543775	test: 1.268325
MAE train: 0.711955	val: 1.072000	test: 0.871486

Epoch: 112
Loss: 0.8905599415302277
RMSE train: 0.895530	val: 1.454898	test: 1.240260
MAE train: 0.674977	val: 1.020891	test: 0.906541

Epoch: 113
Loss: 1.050579994916916
RMSE train: 0.863085	val: 1.449080	test: 1.208643
MAE train: 0.657868	val: 1.007881	test: 0.882313

Epoch: 114
Loss: 1.0028434991836548
RMSE train: 0.882469	val: 1.519218	test: 1.207541
MAE train: 0.675530	val: 1.062853	test: 0.867611

Epoch: 115
Loss: 0.8306741118431091
RMSE train: 0.936808	val: 1.599523	test: 1.216376
MAE train: 0.708094	val: 1.123181	test: 0.887388

Epoch: 116
Loss: 0.9564226865768433
RMSE train: 0.950550	val: 1.587128	test: 1.216512
MAE train: 0.717393	val: 1.099916	test: 0.891087

Epoch: 117
Loss: 0.8386609554290771
RMSE train: 0.918869	val: 1.544780	test: 1.181074
MAE train: 0.691844	val: 1.059969	test: 0.867621

Epoch: 118
Loss: 0.9446803629398346
RMSE train: 0.852433	val: 1.448227	test: 1.128253
MAE train: 0.642071	val: 0.980069	test: 0.828318

Epoch: 119
Loss: 0.863426685333252
RMSE train: 0.786344	val: 1.379700	test: 1.064233
MAE train: 0.600996	val: 0.934341	test: 0.787202

Epoch: 120
Loss: 0.8164495527744293
RMSE train: 0.767817	val: 1.380246	test: 1.021013
MAE train: 0.597134	val: 0.961132	test: 0.775310

Epoch: 121
Loss: 0.8862679302692413
RMSE train: 0.781480	val: 1.404598	test: 1.011502
MAE train: 0.613348	val: 0.997070	test: 0.780584

Epoch: 122
Loss: 0.9159098565578461
RMSE train: 0.788029	val: 1.381567	test: 1.024699
MAE train: 0.618291	val: 0.974449	test: 0.782588

Epoch: 123
Loss: 0.9462298452854156
RMSE train: 0.822338	val: 1.393250	test: 1.043279
MAE train: 0.636596	val: 0.973774	test: 0.789096

Epoch: 124
Loss: 0.794249564409256
RMSE train: 0.866836	val: 1.467154	test: 1.061374
MAE train: 0.660075	val: 1.009127	test: 0.798647

Epoch: 125
Loss: 1.0880475044250488
RMSE train: 0.891034	val: 1.507423	test: 1.075271
MAE train: 0.677709	val: 1.034660	test: 0.804202

Epoch: 126
Loss: 1.0155589878559113
RMSE train: 0.933227	val: 1.509726	test: 1.117192
MAE train: 0.718985	val: 1.036349	test: 0.840095

Epoch: 127
Loss: 0.790744423866272
RMSE train: 0.897415	val: 1.488229	test: 1.077092
MAE train: 0.691382	val: 1.025583	test: 0.815533

Epoch: 128
Loss: 0.9031343460083008
RMSE train: 0.870323	val: 1.470726	test: 1.052014
MAE train: 0.671199	val: 1.018105	test: 0.789555

Epoch: 129
Loss: 0.7979672849178314
RMSE train: 0.853269	val: 1.468900	test: 1.028744
MAE train: 0.658359	val: 1.025477	test: 0.768111

Epoch: 130
Loss: 0.8558182716369629
RMSE train: 0.852357	val: 1.481902	test: 1.013420
MAE train: 0.657829	val: 1.032122	test: 0.756338

Epoch: 131
Loss: 0.8091190457344055
RMSE train: 0.854853	val: 1.469145	test: 1.049876
MAE train: 0.653171	val: 1.010315	test: 0.770044

Epoch: 132
Loss: 0.8467688858509064
RMSE train: 0.868889	val: 1.488783	test: 1.085855
MAE train: 0.662001	val: 1.011737	test: 0.802624

Epoch: 133
Loss: 0.816581666469574
RMSE train: 0.912518	val: 1.530527	test: 1.150546
MAE train: 0.688079	val: 1.025651	test: 0.832173

Epoch: 134
Loss: 1.0042240619659424
RMSE train: 0.939079	val: 1.536293	test: 1.223532
MAE train: 0.703166	val: 1.034785	test: 0.843187

Epoch: 135
Loss: 0.8340602517127991
RMSE train: 0.901483	val: 1.532308	test: 1.208350
MAE train: 0.673899	val: 1.037179	test: 0.831608

Epoch: 136
Loss: 0.8875771462917328
RMSE train: 0.865735	val: 1.525202	test: 1.159432
MAE train: 0.650639	val: 1.036341	test: 0.824416

Epoch: 137
Loss: 0.8258486688137054
RMSE train: 0.832375	val: 1.540580	test: 1.114451
MAE train: 0.630795	val: 1.039193	test: 0.804476

Epoch: 138
Loss: 0.9437560439109802
RMSE train: 0.814291	val: 1.514004	test: 1.110450
MAE train: 0.620810	val: 1.017601	test: 0.783883

Epoch: 139
Loss: 0.6892710030078888
RMSE train: 0.815460	val: 1.504571	test: 1.122409
MAE train: 0.618210	val: 1.009900	test: 0.775304

Epoch: 140
Loss: 0.7451863884925842
RMSE train: 0.806600	val: 1.497664	test: 1.112163
MAE train: 0.610243	val: 0.995742	test: 0.774022

Epoch: 141
Loss: 0.9090421199798584
RMSE train: 0.763042	val: 1.478085	test: 1.079762
MAE train: 0.578626	val: 0.972013	test: 0.778518

Epoch: 142
Loss: 0.8206100761890411
RMSE train: 0.716574	val: 1.493153	test: 1.045104
MAE train: 0.544169	val: 0.997521	test: 0.780777

Early stopping
Best (RMSE):	 train: 0.773450	val: 1.305870	test: 1.151300
Best (MAE):	 train: 0.570682	val: 0.902509	test: 0.782842
All runs completed.

MAE train: 0.479045	val: 0.785629	test: 0.859233

Epoch: 145
Loss: 0.8618590235710144
RMSE train: 0.717982	val: 1.182632	test: 1.361521
MAE train: 0.516360	val: 0.817985	test: 0.846570

Epoch: 146
Loss: 1.0542289912700653
RMSE train: 0.790614	val: 1.298422	test: 1.426321
MAE train: 0.568320	val: 0.880153	test: 0.870694

Epoch: 147
Loss: 1.12383371591568
RMSE train: 0.835590	val: 1.380268	test: 1.494578
MAE train: 0.616576	val: 0.942142	test: 0.924940

Epoch: 148
Loss: 1.1644158959388733
RMSE train: 0.769819	val: 1.320036	test: 1.464587
MAE train: 0.581210	val: 0.915133	test: 0.895399

Epoch: 149
Loss: 1.0722672939300537
RMSE train: 0.694967	val: 1.188748	test: 1.358202
MAE train: 0.527086	val: 0.836824	test: 0.849899

Epoch: 150
Loss: 0.8607980608940125
RMSE train: 0.738548	val: 1.157831	test: 1.333445
MAE train: 0.546109	val: 0.831674	test: 0.856369

Epoch: 151
Loss: 0.9382800757884979
RMSE train: 0.749825	val: 1.121823	test: 1.321964
MAE train: 0.558265	val: 0.812183	test: 0.847432

Epoch: 152
Loss: 0.8263260722160339
RMSE train: 0.744596	val: 1.104359	test: 1.319987
MAE train: 0.564984	val: 0.793198	test: 0.838715

Epoch: 153
Loss: 1.128398597240448
RMSE train: 0.716369	val: 1.096532	test: 1.318205
MAE train: 0.560091	val: 0.787558	test: 0.838610

Epoch: 154
Loss: 0.8237966001033783
RMSE train: 0.683624	val: 1.066496	test: 1.282938
MAE train: 0.530635	val: 0.771155	test: 0.820211

Epoch: 155
Loss: 0.991559624671936
RMSE train: 0.679721	val: 1.054163	test: 1.257202
MAE train: 0.517363	val: 0.768166	test: 0.814591

Epoch: 156
Loss: 0.8107399344444275
RMSE train: 0.706313	val: 1.065782	test: 1.259167
MAE train: 0.532274	val: 0.783767	test: 0.825079

Epoch: 157
Loss: 0.9401448369026184
RMSE train: 0.793082	val: 1.109409	test: 1.297125
MAE train: 0.586348	val: 0.814348	test: 0.854647

Epoch: 158
Loss: 1.0203728675842285
RMSE train: 0.864248	val: 1.149182	test: 1.346109
MAE train: 0.625487	val: 0.832224	test: 0.873898

Epoch: 159
Loss: 0.9736191034317017
RMSE train: 0.877582	val: 1.167842	test: 1.369805
MAE train: 0.636223	val: 0.841503	test: 0.891571

Epoch: 160
Loss: 0.8389531970024109
RMSE train: 0.837978	val: 1.160828	test: 1.385413
MAE train: 0.618900	val: 0.845866	test: 0.903790

Epoch: 161
Loss: 0.8890013694763184
RMSE train: 0.808529	val: 1.176412	test: 1.408787
MAE train: 0.606271	val: 0.858766	test: 0.919518

Epoch: 162
Loss: 0.8338519632816315
RMSE train: 0.780296	val: 1.186111	test: 1.430095
MAE train: 0.591748	val: 0.862015	test: 0.933989

Epoch: 163
Loss: 0.8295089900493622
RMSE train: 0.759606	val: 1.159100	test: 1.424181
MAE train: 0.586302	val: 0.840993	test: 0.933956

Epoch: 164
Loss: 0.7812269926071167
RMSE train: 0.746380	val: 1.127199	test: 1.384049
MAE train: 0.584308	val: 0.820360	test: 0.913189

Epoch: 165
Loss: 0.82057785987854
RMSE train: 0.761299	val: 1.113987	test: 1.342758
MAE train: 0.591955	val: 0.824385	test: 0.896204

Epoch: 166
Loss: 0.8735097050666809
RMSE train: 0.796350	val: 1.133199	test: 1.348385
MAE train: 0.622273	val: 0.839854	test: 0.900203

Epoch: 167
Loss: 0.7120690643787384
RMSE train: 0.813586	val: 1.167286	test: 1.375697
MAE train: 0.636027	val: 0.857872	test: 0.911316

Epoch: 168
Loss: 0.9654809832572937
RMSE train: 0.819774	val: 1.210694	test: 1.405806
MAE train: 0.638920	val: 0.887595	test: 0.933927

Epoch: 169
Loss: 0.9117386341094971
RMSE train: 0.757034	val: 1.226665	test: 1.413094
MAE train: 0.592053	val: 0.888041	test: 0.935205

Epoch: 170
Loss: 0.9109958112239838
RMSE train: 0.737603	val: 1.246863	test: 1.428705
MAE train: 0.575086	val: 0.898078	test: 0.944948

Epoch: 171
Loss: 0.8740218579769135
RMSE train: 0.677998	val: 1.228685	test: 1.413140
MAE train: 0.526918	val: 0.858805	test: 0.914089

Epoch: 172
Loss: 0.7254287898540497
RMSE train: 0.662264	val: 1.212435	test: 1.404599
MAE train: 0.506508	val: 0.827407	test: 0.884757

Epoch: 173
Loss: 0.8817438781261444
RMSE train: 0.686201	val: 1.224950	test: 1.428126
MAE train: 0.519054	val: 0.835955	test: 0.885271

Epoch: 174
Loss: 0.829063206911087
RMSE train: 0.717430	val: 1.268411	test: 1.442152
MAE train: 0.542924	val: 0.865697	test: 0.891413

Epoch: 175
Loss: 0.7560867667198181
RMSE train: 0.769103	val: 1.295618	test: 1.430746
MAE train: 0.581999	val: 0.882755	test: 0.910222

Epoch: 176
Loss: 1.4346491992473602
RMSE train: 0.755521	val: 1.261619	test: 1.394627
MAE train: 0.573091	val: 0.859761	test: 0.904190

Epoch: 177
Loss: 0.8077841699123383
RMSE train: 0.666235	val: 1.203672	test: 1.374953
MAE train: 0.520824	val: 0.825202	test: 0.899525

Epoch: 178
Loss: 0.8307075202465057
RMSE train: 0.592796	val: 1.172523	test: 1.351645
MAE train: 0.463861	val: 0.789417	test: 0.872626

Epoch: 179
Loss: 0.7881241142749786
RMSE train: 0.579127	val: 1.142677	test: 1.329740
MAE train: 0.449284	val: 0.772229	test: 0.859473

Epoch: 180
Loss: 0.7596727907657623
RMSE train: 0.604435	val: 1.116792	test: 1.320590
MAE train: 0.454281	val: 0.765208	test: 0.857504

Epoch: 181
Loss: 0.7936997711658478
RMSE train: 0.646998	val: 1.118269	test: 1.319751
MAE train: 0.479499	val: 0.779883	test: 0.863635

Epoch: 182
Loss: 1.0059017539024353
RMSE train: 0.720836	val: 1.153758	test: 1.341134
MAE train: 0.544275	val: 0.837766	test: 0.900900

Epoch: 183
Loss: 0.9652794301509857
RMSE train: 0.783998	val: 1.210265	test: 1.365306
MAE train: 0.595247	val: 0.878725	test: 0.919301

Epoch: 184
Loss: 0.7886191606521606
RMSE train: 0.737058	val: 1.186919	test: 1.339737
MAE train: 0.555472	val: 0.857363	test: 0.884336

Epoch: 185
Loss: 0.8247006833553314
RMSE train: 0.670911	val: 1.154856	test: 1.309150
MAE train: 0.501356	val: 0.825607	test: 0.848646

Epoch: 186
Loss: 0.7313646376132965
RMSE train: 0.597900	val: 1.113272	test: 1.288262
MAE train: 0.446007	val: 0.785482	test: 0.821922

Epoch: 187
Loss: 0.7325229644775391
RMSE train: 0.570806	val: 1.071416	test: 1.273020
MAE train: 0.422654	val: 0.754193	test: 0.808949

Epoch: 188
Loss: 0.8552636206150055
RMSE train: 0.594644	val: 1.053362	test: 1.280807
MAE train: 0.440391	val: 0.749450	test: 0.818125

Epoch: 189
Loss: 0.9116571545600891
RMSE train: 0.638374	val: 1.066124	test: 1.316427
MAE train: 0.474200	val: 0.756962	test: 0.846323

Epoch: 190
Loss: 0.7378028631210327
RMSE train: 0.666043	val: 1.086342	test: 1.350058
MAE train: 0.502732	val: 0.772548	test: 0.874942

Epoch: 191
Loss: 0.7532857656478882
RMSE train: 0.685823	val: 1.102595	test: 1.360952
MAE train: 0.530012	val: 0.793435	test: 0.886577

Epoch: 192
Loss: 0.7299690842628479
RMSE train: 0.721257	val: 1.140809	test: 1.366697
MAE train: 0.562223	val: 0.830913	test: 0.895464

Epoch: 193
Loss: 0.6985122561454773
RMSE train: 0.760073	val: 1.188125	test: 1.393687
MAE train: 0.595176	val: 0.864643	test: 0.917374

Epoch: 194
Loss: 0.8918056786060333
RMSE train: 0.790144	val: 1.232062	test: 1.428295
MAE train: 0.625437	val: 0.899265	test: 0.950253

Epoch: 195
Loss: 0.6619276106357574
RMSE train: 0.791176	val: 1.239747	test: 1.439425
MAE train: 0.622293	val: 0.900639	test: 0.962835

Epoch: 196
Loss: 0.7179640829563141
RMSE train: 0.791523	val: 1.226132	test: 1.429197
MAE train: 0.617983	val: 0.885786	test: 0.956078

Epoch: 197
Loss: 0.655662477016449
RMSE train: 0.736231	val: 1.160634	test: 1.382856
MAE train: 0.575549	val: 0.834881	test: 0.915422

Epoch: 198
Loss: 0.645567774772644
RMSE train: 0.725841	val: 1.133677	test: 1.371053
MAE train: 0.555841	val: 0.822470	test: 0.894104

Epoch: 199
Loss: 0.8405781686306
RMSE train: 0.715254	val: 1.137410	test: 1.384720
MAE train: 0.540351	val: 0.818959	test: 0.878692

Epoch: 200
Loss: 0.6754693388938904
RMSE train: 0.712586	val: 1.158430	test: 1.406777
MAE train: 0.533322	val: 0.824147	test: 0.887818

Epoch: 201
Loss: 0.7280019819736481
RMSE train: 0.656711	val: 1.139435	test: 1.395687
MAE train: 0.490083	val: 0.804304	test: 0.881073

Epoch: 202
Loss: 0.9811891913414001
RMSE train: 0.605438	val: 1.091662	test: 1.360365
MAE train: 0.452939	val: 0.770832	test: 0.864441

Epoch: 203
Loss: 0.744813859462738
RMSE train: 0.611405	val: 1.069728	test: 1.350491
MAE train: 0.466023	val: 0.782010	test: 0.867273

Epoch: 204
Loss: 0.7366628050804138
RMSE train: 0.668759	val: 1.085664	test: 1.388516
MAE train: 0.514603	val: 0.816562	test: 0.891485
MAE train: 0.633164	val: 0.845525	test: 0.939607

Epoch: 145
Loss: 0.9095355868339539
RMSE train: 0.853169	val: 1.176469	test: 1.397550
MAE train: 0.639832	val: 0.863011	test: 0.939908

Epoch: 146
Loss: 0.882600724697113
RMSE train: 0.853465	val: 1.188106	test: 1.416198
MAE train: 0.633300	val: 0.867728	test: 0.946075

Epoch: 147
Loss: 0.9638463854789734
RMSE train: 0.854374	val: 1.199880	test: 1.442567
MAE train: 0.628124	val: 0.874961	test: 0.962905

Epoch: 148
Loss: 1.0300424098968506
RMSE train: 0.812954	val: 1.173637	test: 1.446142
MAE train: 0.597500	val: 0.857341	test: 0.959082

Epoch: 149
Loss: 0.9703332781791687
RMSE train: 0.760060	val: 1.144278	test: 1.436165
MAE train: 0.561014	val: 0.833844	test: 0.942199

Epoch: 150
Loss: 0.9391781687736511
RMSE train: 0.701387	val: 1.148611	test: 1.440764
MAE train: 0.517423	val: 0.818056	test: 0.932387

Epoch: 151
Loss: 0.9989109933376312
RMSE train: 0.703378	val: 1.200270	test: 1.473517
MAE train: 0.519208	val: 0.846174	test: 0.952846

Epoch: 152
Loss: 0.8411113619804382
RMSE train: 0.784548	val: 1.261337	test: 1.494366
MAE train: 0.585613	val: 0.899105	test: 0.989367

Epoch: 153
Loss: 0.8192920386791229
RMSE train: 0.860260	val: 1.315646	test: 1.520509
MAE train: 0.647154	val: 0.939310	test: 1.015650

Epoch: 154
Loss: 1.0460348725318909
RMSE train: 0.845390	val: 1.307096	test: 1.522998
MAE train: 0.638713	val: 0.929348	test: 1.008856

Epoch: 155
Loss: 0.7640494406223297
RMSE train: 0.810750	val: 1.266548	test: 1.484044
MAE train: 0.606761	val: 0.898730	test: 0.974428

Epoch: 156
Loss: 0.8316682875156403
RMSE train: 0.733088	val: 1.200719	test: 1.427024
MAE train: 0.536816	val: 0.846009	test: 0.930970

Epoch: 157
Loss: 0.8297621011734009
RMSE train: 0.655096	val: 1.123521	test: 1.373129
MAE train: 0.475930	val: 0.788138	test: 0.881663

Epoch: 158
Loss: 0.867473691701889
RMSE train: 0.617554	val: 1.039468	test: 1.316259
MAE train: 0.449532	val: 0.737641	test: 0.832671

Epoch: 159
Loss: 0.6970524489879608
RMSE train: 0.638183	val: 1.013755	test: 1.291840
MAE train: 0.464553	val: 0.725780	test: 0.816116

Epoch: 160
Loss: 0.7730164229869843
RMSE train: 0.683324	val: 1.029361	test: 1.297886
MAE train: 0.500777	val: 0.734644	test: 0.829050

Epoch: 161
Loss: 0.8056569397449493
RMSE train: 0.729581	val: 1.060289	test: 1.310294
MAE train: 0.544440	val: 0.757176	test: 0.847810

Epoch: 162
Loss: 0.9829127490520477
RMSE train: 0.812718	val: 1.101645	test: 1.326593
MAE train: 0.617400	val: 0.796201	test: 0.887448

Epoch: 163
Loss: 0.8720115423202515
RMSE train: 0.878245	val: 1.139402	test: 1.381737
MAE train: 0.670725	val: 0.834070	test: 0.936509

Epoch: 164
Loss: 0.9599122405052185
RMSE train: 0.894247	val: 1.151536	test: 1.411376
MAE train: 0.681425	val: 0.845696	test: 0.958377

Epoch: 165
Loss: 0.7573440372943878
RMSE train: 0.831340	val: 1.145921	test: 1.405152
MAE train: 0.626843	val: 0.834054	test: 0.941647

Epoch: 166
Loss: 0.832758367061615
RMSE train: 0.789090	val: 1.152250	test: 1.414594
MAE train: 0.587546	val: 0.833158	test: 0.934058

Epoch: 167
Loss: 0.7614499032497406
RMSE train: 0.765395	val: 1.173651	test: 1.428294
MAE train: 0.562444	val: 0.844566	test: 0.944830

Epoch: 168
Loss: 0.7506342530250549
RMSE train: 0.753238	val: 1.191218	test: 1.435567
MAE train: 0.542537	val: 0.847144	test: 0.955162

Epoch: 169
Loss: 1.0790141820907593
RMSE train: 0.735838	val: 1.169943	test: 1.415298
MAE train: 0.531500	val: 0.826693	test: 0.947518

Epoch: 170
Loss: 0.9511081576347351
RMSE train: 0.691257	val: 1.082557	test: 1.344561
MAE train: 0.501885	val: 0.788817	test: 0.900717

Epoch: 171
Loss: 0.8949137926101685
RMSE train: 0.689427	val: 1.053543	test: 1.303303
MAE train: 0.518113	val: 0.786823	test: 0.875778

Epoch: 172
Loss: 1.1550268530845642
RMSE train: 0.720172	val: 1.058955	test: 1.281675
MAE train: 0.550125	val: 0.800598	test: 0.867290

Epoch: 173
Loss: 1.117856651544571
RMSE train: 0.729294	val: 1.073032	test: 1.280776
MAE train: 0.564514	val: 0.806616	test: 0.869008

Epoch: 174
Loss: 0.855708509683609
RMSE train: 0.758129	val: 1.083676	test: 1.294241
MAE train: 0.588774	val: 0.812334	test: 0.883772

Epoch: 175
Loss: 0.8685867190361023
RMSE train: 0.779352	val: 1.093908	test: 1.317293
MAE train: 0.598402	val: 0.809846	test: 0.899943

Epoch: 176
Loss: 0.943899005651474
RMSE train: 0.757113	val: 1.086053	test: 1.346612
MAE train: 0.573075	val: 0.809498	test: 0.919689

Epoch: 177
Loss: 0.8077301681041718
RMSE train: 0.717483	val: 1.061077	test: 1.355410
MAE train: 0.535228	val: 0.793881	test: 0.914760

Epoch: 178
Loss: 0.7150313258171082
RMSE train: 0.694726	val: 1.038707	test: 1.359620
MAE train: 0.509918	val: 0.776978	test: 0.906661

Epoch: 179
Loss: 0.8368029892444611
RMSE train: 0.662880	val: 1.012829	test: 1.329668
MAE train: 0.485935	val: 0.766413	test: 0.888856

Epoch: 180
Loss: 0.7379441261291504
RMSE train: 0.666211	val: 0.995371	test: 1.306623
MAE train: 0.490512	val: 0.763127	test: 0.880699

Epoch: 181
Loss: 0.6715396046638489
RMSE train: 0.687387	val: 1.000487	test: 1.305489
MAE train: 0.512800	val: 0.768870	test: 0.885217

Epoch: 182
Loss: 0.7606552243232727
RMSE train: 0.720892	val: 1.016464	test: 1.312139
MAE train: 0.545170	val: 0.778027	test: 0.889336

Epoch: 183
Loss: 0.6751045286655426
RMSE train: 0.710697	val: 1.019076	test: 1.316551
MAE train: 0.537310	val: 0.775360	test: 0.884859

Epoch: 184
Loss: 0.719820111989975
RMSE train: 0.704598	val: 1.020018	test: 1.320034
MAE train: 0.535727	val: 0.768031	test: 0.876953

Epoch: 185
Loss: 0.7044647336006165
RMSE train: 0.693018	val: 1.017778	test: 1.316661
MAE train: 0.530389	val: 0.757322	test: 0.870270

Epoch: 186
Loss: 0.7000206410884857
RMSE train: 0.684305	val: 1.027702	test: 1.314458
MAE train: 0.522359	val: 0.756129	test: 0.866830

Epoch: 187
Loss: 0.6713801026344299
RMSE train: 0.677783	val: 1.039710	test: 1.316371
MAE train: 0.513948	val: 0.757872	test: 0.869362

Epoch: 188
Loss: 0.6937719583511353
RMSE train: 0.660735	val: 1.025616	test: 1.298534
MAE train: 0.498973	val: 0.743955	test: 0.861299

Epoch: 189
Loss: 0.7049035131931305
RMSE train: 0.640914	val: 1.021084	test: 1.289871
MAE train: 0.486504	val: 0.741425	test: 0.856696

Epoch: 190
Loss: 0.7690668702125549
RMSE train: 0.634175	val: 1.053234	test: 1.317360
MAE train: 0.477651	val: 0.756306	test: 0.874456

Epoch: 191
Loss: 0.7130932807922363
RMSE train: 0.656896	val: 1.121859	test: 1.376269
MAE train: 0.494435	val: 0.792867	test: 0.912189

Epoch: 192
Loss: 0.7706654965877533
RMSE train: 0.716396	val: 1.211936	test: 1.446124
MAE train: 0.543063	val: 0.851619	test: 0.961611

Epoch: 193
Loss: 0.6972441971302032
RMSE train: 0.757801	val: 1.291793	test: 1.514749
MAE train: 0.571803	val: 0.899721	test: 1.014701

Epoch: 194
Loss: 1.1233470439910889
RMSE train: 0.723721	val: 1.273087	test: 1.496586
MAE train: 0.537769	val: 0.893019	test: 0.989273

Epoch: 195
Loss: 0.7902181446552277
RMSE train: 0.718324	val: 1.214860	test: 1.437696
MAE train: 0.534361	val: 0.879812	test: 0.944524

Epoch: 196
Loss: 0.62661412358284
RMSE train: 0.735089	val: 1.185638	test: 1.397910
MAE train: 0.546312	val: 0.869344	test: 0.926212

Epoch: 197
Loss: 0.9266136884689331
RMSE train: 0.768010	val: 1.191764	test: 1.388061
MAE train: 0.565360	val: 0.862547	test: 0.924656

Epoch: 198
Loss: 0.6569949686527252
RMSE train: 0.725670	val: 1.130665	test: 1.358540
MAE train: 0.532103	val: 0.820822	test: 0.907543

Epoch: 199
Loss: 0.6731761991977692
RMSE train: 0.694343	val: 1.095785	test: 1.336644
MAE train: 0.501722	val: 0.796381	test: 0.888323

Epoch: 200
Loss: 0.6882871985435486
RMSE train: 0.664860	val: 1.073461	test: 1.327655
MAE train: 0.480533	val: 0.783887	test: 0.877269

Epoch: 201
Loss: 0.7161658108234406
RMSE train: 0.664810	val: 1.086573	test: 1.345944
MAE train: 0.489759	val: 0.789533	test: 0.884527

Epoch: 202
Loss: 0.7379964888095856
RMSE train: 0.657168	val: 1.076025	test: 1.358881
MAE train: 0.493415	val: 0.785365	test: 0.891886

Epoch: 203
Loss: 0.7788817882537842
RMSE train: 0.666537	val: 1.067807	test: 1.378308
MAE train: 0.508651	val: 0.775002	test: 0.909389

Epoch: 204
Loss: 0.8245652616024017
RMSE train: 0.709303	val: 1.066339	test: 1.403001
MAE train: 0.547040	val: 0.774553	test: 0.942865

Epoch: 205
Loss: 0.6595469415187836
RMSE train: 0.721697	val: 1.069326	test: 1.407538
MAE train: 0.560307	val: 0.779459	test: 0.955527

Epoch: 206
Loss: 0.7034555673599243
RMSE train: 0.722371	val: 1.061511	test: 1.383485
MAE train: 0.559023	val: 0.780758	test: 0.932546

Epoch: 207
Loss: 0.6671810746192932
RMSE train: 0.711883	val: 1.068406	test: 1.362836
MAE train: 0.550287	val: 0.784438	test: 0.908921

Epoch: 208
Loss: 0.8289333581924438
RMSE train: 0.730592	val: 1.109837	test: 1.370522
MAE train: 0.564084	val: 0.809467	test: 0.904778

Epoch: 209
Loss: 0.7504879832267761
RMSE train: 0.762791	val: 1.156258	test: 1.406367
MAE train: 0.586795	val: 0.839398	test: 0.916796

Epoch: 210
Loss: 0.6748835742473602
RMSE train: 0.764523	val: 1.167034	test: 1.427170
MAE train: 0.584218	val: 0.847186	test: 0.925472

Epoch: 211
Loss: 0.8353002071380615
RMSE train: 0.689515	val: 1.122679	test: 1.406125
MAE train: 0.524510	val: 0.814706	test: 0.914841

Epoch: 212
Loss: 0.6443198919296265
RMSE train: 0.623194	val: 1.060899	test: 1.376749
MAE train: 0.472079	val: 0.763462	test: 0.914684

Epoch: 213
Loss: 0.8348802328109741
RMSE train: 0.637518	val: 1.059622	test: 1.387189
MAE train: 0.479344	val: 0.767037	test: 0.929996

Epoch: 214
Loss: 1.102971225976944
RMSE train: 0.624974	val: 1.069975	test: 1.393781
MAE train: 0.457345	val: 0.784749	test: 0.918735

Epoch: 215
Loss: 0.7596791982650757
RMSE train: 0.739874	val: 1.139640	test: 1.475080
MAE train: 0.547717	val: 0.850440	test: 0.951214

Early stopping
Best (RMSE):	 train: 0.666211	val: 0.995371	test: 1.306623
Best (MAE):	 train: 0.490512	val: 0.763127	test: 0.880699


Epoch: 205
Loss: 0.7862517535686493
RMSE train: 0.676628	val: 1.097892	test: 1.413426
MAE train: 0.517032	val: 0.822316	test: 0.900753

Epoch: 206
Loss: 0.6880940496921539
RMSE train: 0.685937	val: 1.114369	test: 1.434891
MAE train: 0.523772	val: 0.821878	test: 0.908963

Epoch: 207
Loss: 0.6650793552398682
RMSE train: 0.679943	val: 1.124616	test: 1.426312
MAE train: 0.513229	val: 0.803760	test: 0.900895

Epoch: 208
Loss: 0.7112230062484741
RMSE train: 0.661355	val: 1.131403	test: 1.420619
MAE train: 0.496399	val: 0.784222	test: 0.882766

Epoch: 209
Loss: 0.9663380086421967
RMSE train: 0.676719	val: 1.142860	test: 1.414660
MAE train: 0.508149	val: 0.783153	test: 0.886026

Epoch: 210
Loss: 0.6486357152462006
RMSE train: 0.667205	val: 1.139252	test: 1.414964
MAE train: 0.500892	val: 0.777055	test: 0.885999

Epoch: 211
Loss: 0.8140805065631866
RMSE train: 0.680380	val: 1.137804	test: 1.414555
MAE train: 0.507675	val: 0.784084	test: 0.892304

Epoch: 212
Loss: 0.7564914226531982
RMSE train: 0.665116	val: 1.125515	test: 1.406847
MAE train: 0.495119	val: 0.783211	test: 0.885208

Epoch: 213
Loss: 0.8048901557922363
RMSE train: 0.640658	val: 1.117874	test: 1.395446
MAE train: 0.475806	val: 0.784214	test: 0.866491

Epoch: 214
Loss: 0.6916079521179199
RMSE train: 0.640998	val: 1.116363	test: 1.394520
MAE train: 0.477976	val: 0.791588	test: 0.857748

Epoch: 215
Loss: 0.6339975297451019
RMSE train: 0.645358	val: 1.106747	test: 1.396283
MAE train: 0.483429	val: 0.791528	test: 0.856540

Epoch: 216
Loss: 0.6371266841888428
RMSE train: 0.643974	val: 1.105687	test: 1.396774
MAE train: 0.482845	val: 0.789944	test: 0.868961

Epoch: 217
Loss: 0.7003356516361237
RMSE train: 0.640565	val: 1.088903	test: 1.386308
MAE train: 0.489164	val: 0.786018	test: 0.873142

Epoch: 218
Loss: 0.9318472743034363
RMSE train: 0.635565	val: 1.065006	test: 1.360421
MAE train: 0.486316	val: 0.779676	test: 0.864540

Epoch: 219
Loss: 0.700347363948822
RMSE train: 0.641260	val: 1.067074	test: 1.346518
MAE train: 0.493435	val: 0.785727	test: 0.866945

Epoch: 220
Loss: 0.5675829350948334
RMSE train: 0.654729	val: 1.088191	test: 1.355043
MAE train: 0.504164	val: 0.792672	test: 0.876475

Epoch: 221
Loss: 0.6608072221279144
RMSE train: 0.665748	val: 1.114271	test: 1.372014
MAE train: 0.509112	val: 0.798311	test: 0.883507

Epoch: 222
Loss: 0.8192562162876129
RMSE train: 0.692669	val: 1.128879	test: 1.373608
MAE train: 0.520965	val: 0.810164	test: 0.879320

Epoch: 223
Loss: 0.7144498229026794
RMSE train: 0.709377	val: 1.126571	test: 1.368908
MAE train: 0.534350	val: 0.818340	test: 0.872371

Early stopping
Best (RMSE):	 train: 0.594644	val: 1.053362	test: 1.280807
Best (MAE):	 train: 0.440391	val: 0.749450	test: 0.818125
All runs completed.

MAE train: 0.577848	val: 0.843865	test: 0.837594

Epoch: 145
Loss: 0.7425098121166229
RMSE train: 0.691062	val: 1.241465	test: 1.194486
MAE train: 0.527399	val: 0.834421	test: 0.825055

Epoch: 146
Loss: 0.7757971882820129
RMSE train: 0.650338	val: 1.223943	test: 1.183014
MAE train: 0.488935	val: 0.818119	test: 0.806232

Epoch: 147
Loss: 0.8184885084629059
RMSE train: 0.619420	val: 1.191679	test: 1.171845
MAE train: 0.462309	val: 0.799554	test: 0.787504

Epoch: 148
Loss: 0.7926264703273773
RMSE train: 0.625686	val: 1.169223	test: 1.165045
MAE train: 0.471855	val: 0.794306	test: 0.794372

Epoch: 149
Loss: 0.8297622501850128
RMSE train: 0.673405	val: 1.208549	test: 1.164558
MAE train: 0.515587	val: 0.828838	test: 0.816059

Epoch: 150
Loss: 0.8901128768920898
RMSE train: 0.727163	val: 1.256001	test: 1.194920
MAE train: 0.566732	val: 0.875006	test: 0.860855

Epoch: 151
Loss: 0.7767615616321564
RMSE train: 0.758004	val: 1.326118	test: 1.245071
MAE train: 0.588947	val: 0.919920	test: 0.903575

Epoch: 152
Loss: 0.9023372232913971
RMSE train: 0.750406	val: 1.359384	test: 1.276416
MAE train: 0.579644	val: 0.931165	test: 0.909995

Epoch: 153
Loss: 0.7044083178043365
RMSE train: 0.723689	val: 1.313373	test: 1.268845
MAE train: 0.559689	val: 0.894495	test: 0.889907

Epoch: 154
Loss: 0.8199003040790558
RMSE train: 0.724720	val: 1.267270	test: 1.248707
MAE train: 0.552597	val: 0.856871	test: 0.870589

Epoch: 155
Loss: 0.7709318995475769
RMSE train: 0.713757	val: 1.228295	test: 1.228027
MAE train: 0.550468	val: 0.833578	test: 0.851057

Epoch: 156
Loss: 0.7619814872741699
RMSE train: 0.676078	val: 1.200784	test: 1.205775
MAE train: 0.527723	val: 0.819866	test: 0.827597

Epoch: 157
Loss: 0.8460419476032257
RMSE train: 0.690999	val: 1.230095	test: 1.204894
MAE train: 0.548112	val: 0.842209	test: 0.846092

Epoch: 158
Loss: 0.68138387799263
RMSE train: 0.694626	val: 1.232264	test: 1.220964
MAE train: 0.547592	val: 0.848186	test: 0.864100

Epoch: 159
Loss: 0.6323563754558563
RMSE train: 0.654603	val: 1.209732	test: 1.222068
MAE train: 0.510163	val: 0.827053	test: 0.845081

Epoch: 160
Loss: 0.7614670097827911
RMSE train: 0.625431	val: 1.190688	test: 1.232595
MAE train: 0.482036	val: 0.812285	test: 0.827711

Epoch: 161
Loss: 0.6335723400115967
RMSE train: 0.621617	val: 1.154702	test: 1.238216
MAE train: 0.473981	val: 0.792358	test: 0.824368

Epoch: 162
Loss: 0.7687813937664032
RMSE train: 0.634625	val: 1.154539	test: 1.250863
MAE train: 0.487927	val: 0.789276	test: 0.825141

Epoch: 163
Loss: 0.7808008790016174
RMSE train: 0.660386	val: 1.169127	test: 1.255544
MAE train: 0.516547	val: 0.796018	test: 0.829289

Epoch: 164
Loss: 0.7266009747982025
RMSE train: 0.705679	val: 1.226745	test: 1.260134
MAE train: 0.557193	val: 0.833625	test: 0.854260

Epoch: 165
Loss: 0.80110302567482
RMSE train: 0.725879	val: 1.197909	test: 1.251197
MAE train: 0.574188	val: 0.819642	test: 0.850910

Epoch: 166
Loss: 0.7063588500022888
RMSE train: 0.767189	val: 1.148099	test: 1.258470
MAE train: 0.575030	val: 0.791935	test: 0.846677

Epoch: 167
Loss: 0.78431037068367
RMSE train: 0.797652	val: 1.148637	test: 1.244957
MAE train: 0.585617	val: 0.788248	test: 0.853482

Epoch: 168
Loss: 0.8027577102184296
RMSE train: 0.751736	val: 1.183239	test: 1.215375
MAE train: 0.564370	val: 0.810124	test: 0.829832

Epoch: 169
Loss: 0.6921772062778473
RMSE train: 0.713778	val: 1.243364	test: 1.205620
MAE train: 0.551580	val: 0.849210	test: 0.840793

Epoch: 170
Loss: 0.8095226287841797
RMSE train: 0.703418	val: 1.312384	test: 1.209581
MAE train: 0.552513	val: 0.902034	test: 0.850267

Epoch: 171
Loss: 0.7072640955448151
RMSE train: 0.688099	val: 1.321443	test: 1.226499
MAE train: 0.545711	val: 0.912741	test: 0.857816

Epoch: 172
Loss: 0.8055382966995239
RMSE train: 0.626071	val: 1.259345	test: 1.212534
MAE train: 0.499222	val: 0.865177	test: 0.819083

Epoch: 173
Loss: 0.6801834404468536
RMSE train: 0.612392	val: 1.233650	test: 1.213631
MAE train: 0.488520	val: 0.851377	test: 0.803833

Epoch: 174
Loss: 0.8407839834690094
RMSE train: 0.595012	val: 1.224499	test: 1.220521
MAE train: 0.475457	val: 0.842798	test: 0.786507

Epoch: 175
Loss: 0.7710136473178864
RMSE train: 0.602691	val: 1.212109	test: 1.214268
MAE train: 0.478481	val: 0.833946	test: 0.797447

Epoch: 176
Loss: 0.8587093949317932
RMSE train: 0.655013	val: 1.200082	test: 1.211871
MAE train: 0.512171	val: 0.834557	test: 0.828901

Epoch: 177
Loss: 0.7224088311195374
RMSE train: 0.698410	val: 1.212503	test: 1.224900
MAE train: 0.529140	val: 0.835310	test: 0.847534

Epoch: 178
Loss: 0.6619041264057159
RMSE train: 0.723129	val: 1.242112	test: 1.211025
MAE train: 0.550452	val: 0.844700	test: 0.852286

Epoch: 179
Loss: 0.6873114705085754
RMSE train: 0.745948	val: 1.305054	test: 1.208822
MAE train: 0.578696	val: 0.883700	test: 0.864918

Epoch: 180
Loss: 0.651501476764679
RMSE train: 0.748832	val: 1.352141	test: 1.195285
MAE train: 0.583571	val: 0.911425	test: 0.848300

Epoch: 181
Loss: 0.7772106826305389
RMSE train: 0.692652	val: 1.326620	test: 1.197239
MAE train: 0.533683	val: 0.887795	test: 0.833242

Epoch: 182
Loss: 0.7639001905918121
RMSE train: 0.639685	val: 1.286208	test: 1.200359
MAE train: 0.492122	val: 0.856093	test: 0.818247

Epoch: 183
Loss: 0.7043553292751312
RMSE train: 0.595058	val: 1.245009	test: 1.183257
MAE train: 0.457487	val: 0.830799	test: 0.798075

Epoch: 184
Loss: 0.6818157136440277
RMSE train: 0.570950	val: 1.238632	test: 1.171032
MAE train: 0.444013	val: 0.829311	test: 0.778440

Epoch: 185
Loss: 0.584266871213913
RMSE train: 0.541966	val: 1.237267	test: 1.190291
MAE train: 0.417205	val: 0.823919	test: 0.764990

Epoch: 186
Loss: 0.7273895144462585
RMSE train: 0.589379	val: 1.268633	test: 1.220206
MAE train: 0.453049	val: 0.860822	test: 0.811736

Epoch: 187
Loss: 0.7115837931632996
RMSE train: 0.651142	val: 1.281577	test: 1.223215
MAE train: 0.509215	val: 0.895803	test: 0.839641

Epoch: 188
Loss: 0.8819222450256348
RMSE train: 0.707131	val: 1.269399	test: 1.238185
MAE train: 0.554903	val: 0.898780	test: 0.882976

Epoch: 189
Loss: 0.7067278027534485
RMSE train: 0.723708	val: 1.257032	test: 1.248313
MAE train: 0.564735	val: 0.889842	test: 0.891319

Epoch: 190
Loss: 0.8010228276252747
RMSE train: 0.698317	val: 1.237272	test: 1.232070
MAE train: 0.542517	val: 0.866454	test: 0.857803

Epoch: 191
Loss: 0.6096099317073822
RMSE train: 0.679865	val: 1.226680	test: 1.224084
MAE train: 0.525170	val: 0.855935	test: 0.822239

Epoch: 192
Loss: 0.7481768131256104
RMSE train: 0.676112	val: 1.233514	test: 1.221595
MAE train: 0.520205	val: 0.857987	test: 0.800035

Epoch: 193
Loss: 0.7394558787345886
RMSE train: 0.654233	val: 1.260675	test: 1.236946
MAE train: 0.501878	val: 0.863435	test: 0.816868

Epoch: 194
Loss: 0.58366858959198
RMSE train: 0.665277	val: 1.307355	test: 1.269900
MAE train: 0.514072	val: 0.886652	test: 0.858211

Epoch: 195
Loss: 0.9677799046039581
RMSE train: 0.680440	val: 1.316580	test: 1.269594
MAE train: 0.528483	val: 0.897353	test: 0.878917

Epoch: 196
Loss: 0.9987935423851013
RMSE train: 0.681297	val: 1.312741	test: 1.249946
MAE train: 0.530818	val: 0.899881	test: 0.873225

Epoch: 197
Loss: 0.6587056219577789
RMSE train: 0.678174	val: 1.285743	test: 1.208945
MAE train: 0.527457	val: 0.888148	test: 0.849687

Epoch: 198
Loss: 0.7514699697494507
RMSE train: 0.662353	val: 1.260078	test: 1.180039
MAE train: 0.517297	val: 0.866269	test: 0.827890

Epoch: 199
Loss: 0.5590631365776062
RMSE train: 0.629041	val: 1.216330	test: 1.167426
MAE train: 0.488205	val: 0.826656	test: 0.807704

Epoch: 200
Loss: 1.428142011165619
RMSE train: 0.634968	val: 1.173444	test: 1.164010
MAE train: 0.491862	val: 0.804741	test: 0.801099

Epoch: 201
Loss: 0.9381047487258911
RMSE train: 0.632533	val: 1.174558	test: 1.184045
MAE train: 0.490208	val: 0.801237	test: 0.807795

Early stopping
Best (RMSE):	 train: 0.767189	val: 1.148099	test: 1.258470
Best (MAE):	 train: 0.575030	val: 0.791935	test: 0.846677
All runs completed.
