>>> Starting run for dataset: clintox
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running RANDOM configs_static_noise_experiments/GraphCL/clintox/noise=0.0.yml on cuda:0
Running RANDOM configs_static_noise_experiments/GraphCL/clintox/noise=0.05.yml on cuda:1
Running RANDOM configs_static_noise_experiments/GraphCL/clintox/noise=0.1.yml on cuda:2
Running RANDOM configs_static_noise_experiments/GraphCL/clintox/noise=0.2.yml on cuda:3
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.1.yml --runseed 4 --device cuda:2
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.0.yml --runseed 4 --device cuda:0
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.1.yml --runseed 5 --device cuda:2
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.0.yml --runseed 5 --device cuda:0
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.1.yml --runseed 6 --device cuda:2
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.0.yml --runseed 6 --device cuda:0
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.05.yml --runseed 4 --device cuda:1
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.05.yml --runseed 5 --device cuda:1
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.05.yml --runseed 6 --device cuda:1
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.2.yml --runseed 4 --device cuda:3
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.2.yml --runseed 5 --device cuda:3
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.2.yml --runseed 6 --device cuda:3
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: clintox
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/clintox/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/clintox/noise=0.0/clintox_scaff_5_26-05_11-28-51  ]
[ Using Seed :  5  ]
[ Using device :  cuda:0  ]
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.6589845154199825
ROC train: 0.549779	val: 0.623086	test: 0.461409
PRC train: 0.517783	val: 0.598643	test: 0.494545

Epoch: 2
Loss: 0.5703694804235699
ROC train: 0.612465	val: 0.697630	test: 0.487201
PRC train: 0.532535	val: 0.542822	test: 0.495396

Epoch: 3
Loss: 0.5005605114643373
ROC train: 0.687142	val: 0.761587	test: 0.500069
PRC train: 0.552390	val: 0.560335	test: 0.501353

Epoch: 4
Loss: 0.44514048191339184
ROC train: 0.734022	val: 0.789970	test: 0.513636
PRC train: 0.568464	val: 0.574625	test: 0.504741

Epoch: 5
Loss: 0.3977435225580869
ROC train: 0.759267	val: 0.807815	test: 0.549562
PRC train: 0.578109	val: 0.563862	test: 0.514059

Epoch: 6
Loss: 0.37363717274383396
ROC train: 0.773805	val: 0.826745	test: 0.561466
PRC train: 0.582263	val: 0.561619	test: 0.517048

Epoch: 7
Loss: 0.33131151778933904
ROC train: 0.821135	val: 0.861833	test: 0.582421
PRC train: 0.614106	val: 0.584007	test: 0.519808

Epoch: 8
Loss: 0.3051872307810206
ROC train: 0.831383	val: 0.844639	test: 0.568022
PRC train: 0.620567	val: 0.590857	test: 0.518154

Epoch: 9
Loss: 0.2869089054452471
ROC train: 0.843611	val: 0.839020	test: 0.576247
PRC train: 0.630424	val: 0.588118	test: 0.520179

Epoch: 10
Loss: 0.26558028487324936
ROC train: 0.874511	val: 0.843465	test: 0.616065
PRC train: 0.660146	val: 0.589179	test: 0.531104

Epoch: 11
Loss: 0.2458919953400014
ROC train: 0.879648	val: 0.831366	test: 0.627656
PRC train: 0.674311	val: 0.628842	test: 0.533243

Epoch: 12
Loss: 0.23618874537780848
ROC train: 0.898083	val: 0.874920	test: 0.639074
PRC train: 0.705912	val: 0.587525	test: 0.539180

Epoch: 13
Loss: 0.22704766527635462
ROC train: 0.901231	val: 0.846462	test: 0.642148
PRC train: 0.716870	val: 0.587832	test: 0.543075

Epoch: 14
Loss: 0.21818829559193337
ROC train: 0.886476	val: 0.794043	test: 0.635190
PRC train: 0.714446	val: 0.555080	test: 0.536742

Epoch: 15
Loss: 0.21478280939118566
ROC train: 0.902544	val: 0.818592	test: 0.660317
PRC train: 0.739564	val: 0.588747	test: 0.546981

Epoch: 16
Loss: 0.2092642064701932
ROC train: 0.904482	val: 0.806405	test: 0.678549
PRC train: 0.740759	val: 0.659794	test: 0.554980

Epoch: 17
Loss: 0.20109014468961
ROC train: 0.919143	val: 0.788237	test: 0.696613
PRC train: 0.764311	val: 0.618968	test: 0.554637

Epoch: 18
Loss: 0.19817317962648603
ROC train: 0.928561	val: 0.851333	test: 0.716776
PRC train: 0.770498	val: 0.626190	test: 0.559170

Epoch: 19
Loss: 0.1894568076145005
ROC train: 0.933876	val: 0.908922	test: 0.695895
PRC train: 0.774508	val: 0.625666	test: 0.551594

Epoch: 20
Loss: 0.19213246973551082
ROC train: 0.929030	val: 0.901092	test: 0.690117
PRC train: 0.766790	val: 0.623774	test: 0.545689

Epoch: 21
Loss: 0.18273938133958317
ROC train: 0.940711	val: 0.874083	test: 0.745855
PRC train: 0.801224	val: 0.616440	test: 0.563459

Epoch: 22
Loss: 0.17577975636080628
ROC train: 0.944688	val: 0.866254	test: 0.757810
PRC train: 0.801914	val: 0.607269	test: 0.566847

Epoch: 23
Loss: 0.1743958359114311
ROC train: 0.943379	val: 0.862596	test: 0.740233
PRC train: 0.789667	val: 0.599048	test: 0.554923

Epoch: 24
Loss: 0.18388758469625488
ROC train: 0.946814	val: 0.849260	test: 0.743308
PRC train: 0.795295	val: 0.578783	test: 0.563622

Epoch: 25
Loss: 0.17021401496257432
ROC train: 0.949994	val: 0.824847	test: 0.739079
PRC train: 0.814134	val: 0.576520	test: 0.567343

Epoch: 26
Loss: 0.17286319552654755
ROC train: 0.950197	val: 0.824598	test: 0.729155
PRC train: 0.818698	val: 0.581799	test: 0.559644

Epoch: 27
Loss: 0.1658030987210016
ROC train: 0.953181	val: 0.830442	test: 0.755413
PRC train: 0.820256	val: 0.586990	test: 0.566687

Epoch: 28
Loss: 0.1703785933041322
ROC train: 0.958923	val: 0.854179	test: 0.766418
PRC train: 0.826755	val: 0.599975	test: 0.567384

Epoch: 29
Loss: 0.16359512676563825
ROC train: 0.962737	val: 0.839782	test: 0.766791
PRC train: 0.835589	val: 0.589934	test: 0.570087

Epoch: 30
Loss: 0.16140132524852702
ROC train: 0.963953	val: 0.809589	test: 0.778408
PRC train: 0.841837	val: 0.600145	test: 0.578839

Epoch: 31
Loss: 0.15742180367079425
ROC train: 0.960642	val: 0.822451	test: 0.778046
PRC train: 0.828921	val: 0.596469	test: 0.571242

Epoch: 32
Loss: 0.16100324169417676
ROC train: 0.966178	val: 0.833601	test: 0.796435
PRC train: 0.845443	val: 0.633747	test: 0.579326

Epoch: 33
Loss: 0.1517858237426975Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: clintox
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/clintox/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/clintox/noise=0.0/clintox_scaff_4_26-05_11-28-51  ]
[ Using Seed :  4  ]
[ Using device :  cuda:0  ]
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.6693470395350811
ROC train: 0.575448	val: 0.623357	test: 0.460027
PRC train: 0.522908	val: 0.528226	test: 0.504764

Epoch: 2
Loss: 0.584805783452677
ROC train: 0.650606	val: 0.720342	test: 0.519145
PRC train: 0.541485	val: 0.544552	test: 0.511559

Epoch: 3
Loss: 0.516180738722365
ROC train: 0.683777	val: 0.744255	test: 0.531210
PRC train: 0.550201	val: 0.554194	test: 0.508166

Epoch: 4
Loss: 0.4536719715106211
ROC train: 0.745868	val: 0.830878	test: 0.574532
PRC train: 0.578931	val: 0.567191	test: 0.520529

Epoch: 5
Loss: 0.4058318856053956
ROC train: 0.765801	val: 0.808201	test: 0.583377
PRC train: 0.587499	val: 0.565165	test: 0.525762

Epoch: 6
Loss: 0.36229314834026
ROC train: 0.793587	val: 0.812285	test: 0.596656
PRC train: 0.602797	val: 0.563205	test: 0.529417

Epoch: 7
Loss: 0.33397242684625134
ROC train: 0.817293	val: 0.838932	test: 0.604601
PRC train: 0.614085	val: 0.569894	test: 0.531555

Epoch: 8
Loss: 0.31254395438705374
ROC train: 0.848324	val: 0.821013	test: 0.628223
PRC train: 0.647538	val: 0.553342	test: 0.540901

Epoch: 9
Loss: 0.28428987500059033
ROC train: 0.855027	val: 0.811761	test: 0.624525
PRC train: 0.651587	val: 0.551211	test: 0.531449

Epoch: 10
Loss: 0.2666601914375207
ROC train: 0.871134	val: 0.832726	test: 0.649884
PRC train: 0.666116	val: 0.555273	test: 0.537183

Epoch: 11
Loss: 0.24548881722761234
ROC train: 0.883580	val: 0.853080	test: 0.653986
PRC train: 0.681376	val: 0.563449	test: 0.548994

Epoch: 12
Loss: 0.23579901847066642
ROC train: 0.898745	val: 0.857750	test: 0.687032
PRC train: 0.706238	val: 0.566898	test: 0.569769

Epoch: 13
Loss: 0.23396369140711415
ROC train: 0.911426	val: 0.840418	test: 0.694652
PRC train: 0.721142	val: 0.568560	test: 0.565056

Epoch: 14
Loss: 0.20743802039627238
ROC train: 0.914796	val: 0.822099	test: 0.681511
PRC train: 0.717933	val: 0.567069	test: 0.559593

Epoch: 15
Loss: 0.2148966943719603
ROC train: 0.916119	val: 0.812734	test: 0.663447
PRC train: 0.711552	val: 0.573173	test: 0.545163

Epoch: 16
Loss: 0.20013460692867024
ROC train: 0.920321	val: 0.810499	test: 0.673603
PRC train: 0.742169	val: 0.610170	test: 0.546382

Epoch: 17
Loss: 0.2116668959957036
ROC train: 0.930737	val: 0.865442	test: 0.694278
PRC train: 0.754511	val: 0.597418	test: 0.556334

Epoch: 18
Loss: 0.19152117043642755
ROC train: 0.929793	val: 0.858136	test: 0.724033
PRC train: 0.748921	val: 0.601675	test: 0.566963

Epoch: 19
Loss: 0.1953255919053735
ROC train: 0.936040	val: 0.847074	test: 0.694166
PRC train: 0.755140	val: 0.566028	test: 0.552137

Epoch: 20
Loss: 0.1942935391671139
ROC train: 0.942183	val: 0.890466	test: 0.675303
PRC train: 0.773232	val: 0.601711	test: 0.553862

Epoch: 21
Loss: 0.17982295775739943
ROC train: 0.940873	val: 0.912830	test: 0.647098
PRC train: 0.778871	val: 0.620338	test: 0.546149

Epoch: 22
Loss: 0.18071556420526483
ROC train: 0.943571	val: 0.909108	test: 0.660175
PRC train: 0.796855	val: 0.613411	test: 0.546471

Epoch: 23
Loss: 0.18069765440515026
ROC train: 0.943943	val: 0.835611	test: 0.706993
PRC train: 0.797382	val: 0.598658	test: 0.550383

Epoch: 24
Loss: 0.17790443842984174
ROC train: 0.954045	val: 0.832076	test: 0.710904
PRC train: 0.814587	val: 0.615770	test: 0.553976

Epoch: 25
Loss: 0.17196400750915206
ROC train: 0.956617	val: 0.845862	test: 0.681873
PRC train: 0.815257	val: 0.613903	test: 0.543970

Epoch: 26
Loss: 0.1762507597199499
ROC train: 0.954187	val: 0.880676	test: 0.659301
PRC train: 0.815168	val: 0.624091	test: 0.539361

Epoch: 27
Loss: 0.16047090384031987
ROC train: 0.955107	val: 0.884871	test: 0.673891
PRC train: 0.820355	val: 0.668504	test: 0.546905

Epoch: 28
Loss: 0.16148594793849474
ROC train: 0.959814	val: 0.819527	test: 0.719166
PRC train: 0.834148	val: 0.651857	test: 0.561032

Epoch: 29
Loss: 0.15981982005702353
ROC train: 0.954355	val: 0.808426	test: 0.770964
PRC train: 0.815556	val: 0.663893	test: 0.565289

Epoch: 30
Loss: 0.16249464632873273
ROC train: 0.944308	val: 0.796900	test: 0.796998
PRC train: 0.796580	val: 0.676687	test: 0.574661

Epoch: 31
Loss: 0.15594299942393125
ROC train: 0.965903	val: 0.821625	test: 0.771864
PRC train: 0.837584	val: 0.622367	test: 0.579139

Epoch: 32
Loss: 0.16095575546766544
ROC train: 0.965032	val: 0.858474	test: 0.740827
PRC train: 0.837639	val: 0.619194	test: 0.562060

Epoch: 33
Loss: 0.15644723588718862Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: clintox
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/clintox/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/clintox/noise=0.0/clintox_scaff_6_26-05_11-28-51  ]
[ Using Seed :  6  ]
[ Using device :  cuda:0  ]
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.6424762594594099
ROC train: 0.552961	val: 0.550731	test: 0.520176
PRC train: 0.517718	val: 0.524486	test: 0.510174

Epoch: 2
Loss: 0.5620431524632188
ROC train: 0.605344	val: 0.582787	test: 0.503950
PRC train: 0.528223	val: 0.519938	test: 0.510544

Epoch: 3
Loss: 0.49337991501101375
ROC train: 0.683977	val: 0.720019	test: 0.537467
PRC train: 0.547246	val: 0.541976	test: 0.511846

Epoch: 4
Loss: 0.4374483770715676
ROC train: 0.750940	val: 0.834423	test: 0.565582
PRC train: 0.574416	val: 0.570278	test: 0.517561

Epoch: 5
Loss: 0.39385273512427005
ROC train: 0.789051	val: 0.855789	test: 0.574483
PRC train: 0.591160	val: 0.570395	test: 0.520640

Epoch: 6
Loss: 0.35178081490279656
ROC train: 0.805810	val: 0.873820	test: 0.631222
PRC train: 0.602774	val: 0.580186	test: 0.537388

Epoch: 7
Loss: 0.3200809787880892
ROC train: 0.828051	val: 0.878715	test: 0.619143
PRC train: 0.614986	val: 0.578846	test: 0.535089

Epoch: 8
Loss: 0.2920363989989072
ROC train: 0.842299	val: 0.844052	test: 0.584502
PRC train: 0.627305	val: 0.592054	test: 0.523740

Epoch: 9
Loss: 0.2835796327142031
ROC train: 0.856788	val: 0.867340	test: 0.619479
PRC train: 0.639072	val: 0.568837	test: 0.531262

Epoch: 10
Loss: 0.2581355947424873
ROC train: 0.870276	val: 0.851119	test: 0.650564
PRC train: 0.663062	val: 0.567737	test: 0.537981

Epoch: 11
Loss: 0.24350314624994268
ROC train: 0.881742	val: 0.846375	test: 0.617865
PRC train: 0.676492	val: 0.564659	test: 0.529118

Epoch: 12
Loss: 0.23057173741403386
ROC train: 0.888307	val: 0.864630	test: 0.622213
PRC train: 0.683837	val: 0.592672	test: 0.530381

Epoch: 13
Loss: 0.22140121751642386
ROC train: 0.893685	val: 0.862420	test: 0.650134
PRC train: 0.688757	val: 0.580033	test: 0.538430

Epoch: 14
Loss: 0.22108914192461576
ROC train: 0.900525	val: 0.848996	test: 0.680315
PRC train: 0.704238	val: 0.576130	test: 0.545942

Epoch: 15
Loss: 0.21448636905724
ROC train: 0.918459	val: 0.822686	test: 0.707209
PRC train: 0.749855	val: 0.573433	test: 0.550651

Epoch: 16
Loss: 0.20407969016776323
ROC train: 0.920387	val: 0.815356	test: 0.691193
PRC train: 0.751375	val: 0.579330	test: 0.543800

Epoch: 17
Loss: 0.20863033440663953
ROC train: 0.920576	val: 0.846835	test: 0.700736
PRC train: 0.756675	val: 0.594565	test: 0.546916

Epoch: 18
Loss: 0.18938969419913165
ROC train: 0.918532	val: 0.852893	test: 0.663447
PRC train: 0.757138	val: 0.581193	test: 0.538200

Epoch: 19
Loss: 0.19506886835845794
ROC train: 0.926729	val: 0.825659	test: 0.694584
PRC train: 0.779308	val: 0.587532	test: 0.549628

Epoch: 20
Loss: 0.19377452332950673
ROC train: 0.935059	val: 0.821737	test: 0.745188
PRC train: 0.784870	val: 0.649301	test: 0.567934

Epoch: 21
Loss: 0.18739189404947715
ROC train: 0.942128	val: 0.833787	test: 0.747543
PRC train: 0.804769	val: 0.620842	test: 0.566494

Epoch: 22
Loss: 0.1768910067319503
ROC train: 0.943240	val: 0.847573	test: 0.756101
PRC train: 0.801397	val: 0.618160	test: 0.562901

Epoch: 23
Loss: 0.17452604383735476
ROC train: 0.949241	val: 0.821576	test: 0.741690
PRC train: 0.813213	val: 0.652050	test: 0.560020

Epoch: 24
Loss: 0.17604890041295185
ROC train: 0.949956	val: 0.826471	test: 0.733263
PRC train: 0.806637	val: 0.609378	test: 0.554056

Epoch: 25
Loss: 0.1685167995720042
ROC train: 0.952882	val: 0.830354	test: 0.778722
PRC train: 0.818112	val: 0.615544	test: 0.571819

Epoch: 26
Loss: 0.18016169008789668
ROC train: 0.954381	val: 0.847998	test: 0.767191
PRC train: 0.818798	val: 0.663481	test: 0.567090

Epoch: 27
Loss: 0.16495780778001629
ROC train: 0.948762	val: 0.861447	test: 0.744619
PRC train: 0.817901	val: 0.639152	test: 0.565068

Epoch: 28
Loss: 0.17460874670673215
ROC train: 0.957342	val: 0.846287	test: 0.777609
PRC train: 0.829673	val: 0.710326	test: 0.580249

Epoch: 29
Loss: 0.16861234508089082
ROC train: 0.953958	val: 0.819703	test: 0.744992
PRC train: 0.820202	val: 0.638600	test: 0.561196

Epoch: 30
Loss: 0.16325917277134952
ROC train: 0.957477	val: 0.867789	test: 0.754562
PRC train: 0.836747	val: 0.698997	test: 0.569755

Epoch: 31
Loss: 0.17123734910026095
ROC train: 0.953942	val: 0.836173	test: 0.738134
PRC train: 0.831883	val: 0.641474	test: 0.560430

Epoch: 32
Loss: 0.15855308251783282
ROC train: 0.954215	val: 0.801584	test: 0.770538
PRC train: 0.803522	val: 0.628639	test: 0.568569

Epoch: 33
Loss: 0.16118513350997044Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: clintox
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/clintox/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/clintox/noise=0.05/clintox_scaff_6_26-05_11-28-51  ]
[ Using Seed :  6  ]
[ Using device :  cuda:1  ]
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.707970910272873
ROC train: 0.585226	val: 0.602768	test: 0.573887
PRC train: 0.529263	val: 0.518020	test: 0.514673

Epoch: 2
Loss: 0.619999292729333
ROC train: 0.634730	val: 0.673742	test: 0.535566
PRC train: 0.536400	val: 0.530442	test: 0.510320

Epoch: 3
Loss: 0.5456858241692057
ROC train: 0.669876	val: 0.769729	test: 0.524812
PRC train: 0.545026	val: 0.550247	test: 0.508529

Epoch: 4
Loss: 0.4846821655968139
ROC train: 0.693159	val: 0.760701	test: 0.531345
PRC train: 0.552328	val: 0.617466	test: 0.509446

Epoch: 5
Loss: 0.43746328351932584
ROC train: 0.762786	val: 0.818016	test: 0.550496
PRC train: 0.583452	val: 0.578287	test: 0.515001

Epoch: 6
Loss: 0.3867938169344467
ROC train: 0.780154	val: 0.845025	test: 0.568597
PRC train: 0.596034	val: 0.571394	test: 0.517252

Epoch: 7
Loss: 0.35467285863539144
ROC train: 0.808574	val: 0.845226	test: 0.576773
PRC train: 0.627417	val: 0.577457	test: 0.518300

Epoch: 8
Loss: 0.32569664213546246
ROC train: 0.835013	val: 0.858337	test: 0.587303
PRC train: 0.664946	val: 0.576561	test: 0.527452

Epoch: 9
Loss: 0.2901185017753687
ROC train: 0.854963	val: 0.817742	test: 0.568321
PRC train: 0.681053	val: 0.624010	test: 0.522770

Epoch: 10
Loss: 0.27284904069864957
ROC train: 0.877458	val: 0.819952	test: 0.562523
PRC train: 0.713252	val: 0.624339	test: 0.519674

Epoch: 11
Loss: 0.24827548859264806
ROC train: 0.905296	val: 0.839132	test: 0.558052
PRC train: 0.745174	val: 0.628390	test: 0.520361

Epoch: 12
Loss: 0.23835915011631298
ROC train: 0.921443	val: 0.886744	test: 0.583929
PRC train: 0.776365	val: 0.652967	test: 0.531691

Epoch: 13
Loss: 0.23069719685217244
ROC train: 0.924586	val: 0.829430	test: 0.534292
PRC train: 0.792057	val: 0.635086	test: 0.519372

Epoch: 14
Loss: 0.2045805414333807
ROC train: 0.930500	val: 0.841841	test: 0.526784
PRC train: 0.799514	val: 0.631891	test: 0.518493

Epoch: 15
Loss: 0.20301220145808388
ROC train: 0.945947	val: 0.804993	test: 0.549913
PRC train: 0.851172	val: 0.670564	test: 0.525384

Epoch: 16
Loss: 0.18726430326033713
ROC train: 0.957918	val: 0.782267	test: 0.559864
PRC train: 0.864771	val: 0.636576	test: 0.523653

Epoch: 17
Loss: 0.18948360589825544
ROC train: 0.963245	val: 0.861584	test: 0.575354
PRC train: 0.880728	val: 0.665554	test: 0.526982

Epoch: 18
Loss: 0.1730901777813673
ROC train: 0.970206	val: 0.855153	test: 0.604071
PRC train: 0.898959	val: 0.640205	test: 0.529373

Epoch: 19
Loss: 0.17279235508710533
ROC train: 0.978458	val: 0.798850	test: 0.645693
PRC train: 0.925597	val: 0.586488	test: 0.536684

Epoch: 20
Loss: 0.1553934109968747
ROC train: 0.980014	val: 0.848610	test: 0.684443
PRC train: 0.930463	val: 0.571013	test: 0.548722

Epoch: 21
Loss: 0.158143711120163
ROC train: 0.987792	val: 0.807179	test: 0.681896
PRC train: 0.957369	val: 0.631019	test: 0.547436

Epoch: 22
Loss: 0.16104238040115287
ROC train: 0.987693	val: 0.781343	test: 0.698835
PRC train: 0.953149	val: 0.645723	test: 0.551401

Epoch: 23
Loss: 0.14414329727331637
ROC train: 0.986944	val: 0.722854	test: 0.707068
PRC train: 0.954219	val: 0.626354	test: 0.551337

Epoch: 24
Loss: 0.14142952213001744
ROC train: 0.987667	val: 0.700442	test: 0.687043
PRC train: 0.956749	val: 0.612885	test: 0.548228

Epoch: 25
Loss: 0.14592150346152238
ROC train: 0.985761	val: 0.734830	test: 0.679935
PRC train: 0.959354	val: 0.590898	test: 0.556000

Epoch: 26
Loss: 0.12755063023294982
ROC train: 0.987611	val: 0.819467	test: 0.644976
PRC train: 0.961301	val: 0.634626	test: 0.549542

Epoch: 27
Loss: 0.11169562513222286
ROC train: 0.992328	val: 0.841669	test: 0.649092
PRC train: 0.971118	val: 0.610981	test: 0.546028

Epoch: 28
Loss: 0.11814593026911084
ROC train: 0.994894	val: 0.789049	test: 0.657911
PRC train: 0.977358	val: 0.601949	test: 0.548841

Epoch: 29
Loss: 0.11480414288118246
ROC train: 0.995250	val: 0.769968	test: 0.618994
PRC train: 0.979384	val: 0.604325	test: 0.536568

Epoch: 30
Loss: 0.09956227744767243
ROC train: 0.995070	val: 0.801359	test: 0.603690
PRC train: 0.977288	val: 0.613631	test: 0.540435

Epoch: 31
Loss: 0.10282175239476446
ROC train: 0.994340	val: 0.844438	test: 0.631510
PRC train: 0.981981	val: 0.648717	test: 0.548227

Epoch: 32
Loss: 0.08972309332324115
ROC train: 0.994769	val: 0.858361	test: 0.638974Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:3
  input_data_dir: 
  dataset: clintox
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/clintox/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/clintox/noise=0.2/clintox_scaff_5_26-05_11-28-51  ]
[ Using Seed :  5  ]
[ Using device :  cuda:3  ]
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.6807164871253283
ROC train: 0.528933	val: 0.684270	test: 0.460606
PRC train: 0.511211	val: 0.529023	test: 0.509891

Epoch: 2
Loss: 0.5975727530222215
ROC train: 0.630911	val: 0.710594	test: 0.473030
PRC train: 0.529800	val: 0.565188	test: 0.504435

Epoch: 3
Loss: 0.535273991604799
ROC train: 0.663663	val: 0.673696	test: 0.457704
PRC train: 0.541093	val: 0.558519	test: 0.499686

Epoch: 4
Loss: 0.48226887299001764
ROC train: 0.693012	val: 0.688455	test: 0.446027
PRC train: 0.550823	val: 0.560682	test: 0.502645

Epoch: 5
Loss: 0.43031100332739325
ROC train: 0.730485	val: 0.745007	test: 0.492072
PRC train: 0.575783	val: 0.570727	test: 0.515487

Epoch: 6
Loss: 0.3865108296563344
ROC train: 0.735830	val: 0.751575	test: 0.538782
PRC train: 0.591515	val: 0.572261	test: 0.535829

Epoch: 7
Loss: 0.36232294994120195
ROC train: 0.798526	val: 0.733506	test: 0.460359
PRC train: 0.623978	val: 0.571828	test: 0.508439

Epoch: 8
Loss: 0.32362340053087457
ROC train: 0.824363	val: 0.746343	test: 0.488153
PRC train: 0.653542	val: 0.571541	test: 0.512930

Epoch: 9
Loss: 0.2949490113520843
ROC train: 0.853194	val: 0.748103	test: 0.519971
PRC train: 0.688651	val: 0.573683	test: 0.517249

Epoch: 10
Loss: 0.27253275737643623
ROC train: 0.878646	val: 0.771440	test: 0.541318
PRC train: 0.728718	val: 0.621992	test: 0.518087

Epoch: 11
Loss: 0.2500344069770929
ROC train: 0.898366	val: 0.797726	test: 0.563016
PRC train: 0.753422	val: 0.624755	test: 0.522391

Epoch: 12
Loss: 0.23862392636269797
ROC train: 0.908296	val: 0.785700	test: 0.560069
PRC train: 0.767403	val: 0.627619	test: 0.523732

Epoch: 13
Loss: 0.22303956919690343
ROC train: 0.929242	val: 0.701464	test: 0.540190
PRC train: 0.820350	val: 0.608772	test: 0.525247

Epoch: 14
Loss: 0.2076838173298164
ROC train: 0.936723	val: 0.684632	test: 0.550114
PRC train: 0.820558	val: 0.613368	test: 0.535934

Epoch: 15
Loss: 0.20280755738819053
ROC train: 0.955987	val: 0.699953	test: 0.585316
PRC train: 0.874885	val: 0.609333	test: 0.530086

Epoch: 16
Loss: 0.18383410996317234
ROC train: 0.969874	val: 0.643338	test: 0.585065
PRC train: 0.909035	val: 0.600038	test: 0.526913

Epoch: 17
Loss: 0.17119628307250903
ROC train: 0.970050	val: 0.670572	test: 0.584117
PRC train: 0.915111	val: 0.602598	test: 0.528785

Epoch: 18
Loss: 0.16411798267220506
ROC train: 0.974020	val: 0.709406	test: 0.583754
PRC train: 0.915922	val: 0.608557	test: 0.526984

Epoch: 19
Loss: 0.15749384159258797
ROC train: 0.981432	val: 0.734093	test: 0.598483
PRC train: 0.944440	val: 0.612688	test: 0.538615

Epoch: 20
Loss: 0.15062188083222094
ROC train: 0.985071	val: 0.675917	test: 0.593466
PRC train: 0.958724	val: 0.603741	test: 0.531706

Epoch: 21
Loss: 0.140854060522321
ROC train: 0.985667	val: 0.705748	test: 0.594983
PRC train: 0.953139	val: 0.608277	test: 0.531197

Epoch: 22
Loss: 0.13707812791334867
ROC train: 0.985822	val: 0.769381	test: 0.577307
PRC train: 0.954051	val: 0.605715	test: 0.533736

Epoch: 23
Loss: 0.12757901793971452
ROC train: 0.990385	val: 0.704887	test: 0.520706
PRC train: 0.969010	val: 0.614505	test: 0.534217

Epoch: 24
Loss: 0.1306474379847205
ROC train: 0.993507	val: 0.695908	test: 0.520837
PRC train: 0.978113	val: 0.614050	test: 0.535342

Epoch: 25
Loss: 0.1051593974272739
ROC train: 0.995548	val: 0.729310	test: 0.519690
PRC train: 0.982458	val: 0.612083	test: 0.516405

Epoch: 26
Loss: 0.09951536724557378
ROC train: 0.996376	val: 0.777509	test: 0.550895
PRC train: 0.986625	val: 0.564830	test: 0.521044

Epoch: 27
Loss: 0.10471123273265195
ROC train: 0.996481	val: 0.816431	test: 0.543929
PRC train: 0.986061	val: 0.649986	test: 0.519739

Epoch: 28
Loss: 0.10530845715061059
ROC train: 0.997005	val: 0.788473	test: 0.520221
PRC train: 0.988354	val: 0.592853	test: 0.515451

Epoch: 29
Loss: 0.0781693482202978
ROC train: 0.998099	val: 0.642477	test: 0.504775
PRC train: 0.991188	val: 0.603680	test: 0.515068

Epoch: 30
Loss: 0.08440933976119261
ROC train: 0.998726	val: 0.636383	test: 0.538610
PRC train: 0.993611	val: 0.603393	test: 0.526475

Epoch: 31
Loss: 0.0737455322992003
ROC train: 0.998843	val: 0.643875	test: 0.527254
PRC train: 0.994507	val: 0.603050	test: 0.523974

Epoch: 32
Loss: 0.07428979429709685
ROC train: 0.997854	val: 0.698168	test: 0.536910Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:3
  input_data_dir: 
  dataset: clintox
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/clintox/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/clintox/noise=0.2/clintox_scaff_6_26-05_11-28-51  ]
[ Using Seed :  6  ]
[ Using device :  cuda:3  ]
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.7102504095105892
ROC train: 0.503312	val: 0.556178	test: 0.588465
PRC train: 0.513297	val: 0.514255	test: 0.518265

Epoch: 2
Loss: 0.632336387232644
ROC train: 0.569366	val: 0.671609	test: 0.564018
PRC train: 0.521181	val: 0.528425	test: 0.511977

Epoch: 3
Loss: 0.5571684070403118
ROC train: 0.620660	val: 0.729784	test: 0.541449
PRC train: 0.535878	val: 0.541348	test: 0.511804

Epoch: 4
Loss: 0.49523769183730504
ROC train: 0.683452	val: 0.771015	test: 0.545098
PRC train: 0.555070	val: 0.544892	test: 0.511821

Epoch: 5
Loss: 0.4435247624256565
ROC train: 0.719616	val: 0.787124	test: 0.539921
PRC train: 0.576524	val: 0.552637	test: 0.507126

Epoch: 6
Loss: 0.3932495461176829
ROC train: 0.726747	val: 0.764897	test: 0.532402
PRC train: 0.589531	val: 0.565218	test: 0.506029

Epoch: 7
Loss: 0.3629084297949924
ROC train: 0.788659	val: 0.799848	test: 0.545554
PRC train: 0.627644	val: 0.576552	test: 0.507944

Epoch: 8
Loss: 0.3300096888492562
ROC train: 0.840535	val: 0.822187	test: 0.556103
PRC train: 0.681563	val: 0.639745	test: 0.516028

Epoch: 9
Loss: 0.30112350674068794
ROC train: 0.832572	val: 0.799349	test: 0.539339
PRC train: 0.678904	val: 0.575987	test: 0.512794

Epoch: 10
Loss: 0.2823288969936727
ROC train: 0.846243	val: 0.811511	test: 0.522948
PRC train: 0.710586	val: 0.591347	test: 0.511167

Epoch: 11
Loss: 0.25833543475948445
ROC train: 0.887188	val: 0.856801	test: 0.519601
PRC train: 0.773247	val: 0.691798	test: 0.517644

Epoch: 12
Loss: 0.24237028429209673
ROC train: 0.905529	val: 0.836472	test: 0.514641
PRC train: 0.803224	val: 0.640668	test: 0.524133

Epoch: 13
Loss: 0.23391185065617437
ROC train: 0.915182	val: 0.782742	test: 0.500118
PRC train: 0.818210	val: 0.553169	test: 0.530424

Epoch: 14
Loss: 0.21500078336559572
ROC train: 0.933509	val: 0.789896	test: 0.519874
PRC train: 0.850873	val: 0.553129	test: 0.525098

Epoch: 15
Loss: 0.19913292977615282
ROC train: 0.945270	val: 0.760041	test: 0.537490
PRC train: 0.870725	val: 0.560666	test: 0.518665

Epoch: 16
Loss: 0.18271326216879608
ROC train: 0.950966	val: 0.777397	test: 0.585596
PRC train: 0.894800	val: 0.617503	test: 0.537399

Epoch: 17
Loss: 0.18441525380443952
ROC train: 0.949265	val: 0.785476	test: 0.581050
PRC train: 0.892923	val: 0.597728	test: 0.543421

Epoch: 18
Loss: 0.16878420298586336
ROC train: 0.966595	val: 0.792968	test: 0.588133
PRC train: 0.929423	val: 0.587715	test: 0.537605

Epoch: 19
Loss: 0.165260277392869
ROC train: 0.974181	val: 0.769205	test: 0.550593
PRC train: 0.944566	val: 0.554079	test: 0.530728

Epoch: 20
Loss: 0.15338951018476726
ROC train: 0.978134	val: 0.718834	test: 0.552349
PRC train: 0.949902	val: 0.542978	test: 0.542327

Epoch: 21
Loss: 0.15236164133763355
ROC train: 0.979329	val: 0.654976	test: 0.534942
PRC train: 0.951273	val: 0.544338	test: 0.540639

Epoch: 22
Loss: 0.1338172893989767
ROC train: 0.984167	val: 0.636158	test: 0.516366
PRC train: 0.965038	val: 0.527629	test: 0.563958

Epoch: 23
Loss: 0.13182268120432544
ROC train: 0.985513	val: 0.672508	test: 0.565822
PRC train: 0.968474	val: 0.528860	test: 0.542828

Epoch: 24
Loss: 0.128055013627631
ROC train: 0.990100	val: 0.752598	test: 0.531401
PRC train: 0.971842	val: 0.554056	test: 0.523602

Epoch: 25
Loss: 0.12138988131195076
ROC train: 0.993531	val: 0.713613	test: 0.564148
PRC train: 0.978728	val: 0.541996	test: 0.532558

Epoch: 26
Loss: 0.11003832786903116
ROC train: 0.993791	val: 0.735677	test: 0.560764
PRC train: 0.981289	val: 0.539298	test: 0.531954

Epoch: 27
Loss: 0.10418911242063666
ROC train: 0.992444	val: 0.621723	test: 0.532619
PRC train: 0.980906	val: 0.535933	test: 0.567141

Epoch: 28
Loss: 0.11302441298865382
ROC train: 0.992899	val: 0.626955	test: 0.542637
PRC train: 0.980002	val: 0.546664	test: 0.572810

Epoch: 29
Loss: 0.10433819496536254
ROC train: 0.994177	val: 0.704399	test: 0.556215
PRC train: 0.985345	val: 0.616370	test: 0.553587

Epoch: 30
Loss: 0.07916273358245723
ROC train: 0.995713	val: 0.738338	test: 0.518077
PRC train: 0.989690	val: 0.633969	test: 0.570912

Epoch: 31
Loss: 0.09073712574017435
ROC train: 0.996858	val: 0.793280	test: 0.512769
PRC train: 0.990976	val: 0.581491	test: 0.564702

Epoch: 32
Loss: 0.07808783280118846
ROC train: 0.997912	val: 0.779333	test: 0.536055Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: clintox
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/clintox/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/clintox/noise=0.05/clintox_scaff_5_26-05_11-28-51  ]
[ Using Seed :  5  ]
[ Using device :  cuda:1  ]
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.6773481738217771
ROC train: 0.584430	val: 0.591941	test: 0.444260
PRC train: 0.526225	val: 0.519868	test: 0.504976

Epoch: 2
Loss: 0.5923856413426897
ROC train: 0.635724	val: 0.623834	test: 0.461360
PRC train: 0.536629	val: 0.528588	test: 0.501016

Epoch: 3
Loss: 0.52306156972762
ROC train: 0.658568	val: 0.704574	test: 0.482491
PRC train: 0.543080	val: 0.541839	test: 0.500944

Epoch: 4
Loss: 0.47285191391755477
ROC train: 0.691686	val: 0.772589	test: 0.507050
PRC train: 0.553952	val: 0.551310	test: 0.506459

Epoch: 5
Loss: 0.419265658054341
ROC train: 0.741476	val: 0.831826	test: 0.525413
PRC train: 0.577605	val: 0.563076	test: 0.511180

Epoch: 6
Loss: 0.3788167653065477
ROC train: 0.761583	val: 0.828256	test: 0.509075
PRC train: 0.589578	val: 0.558499	test: 0.509481

Epoch: 7
Loss: 0.34493322220607253
ROC train: 0.796338	val: 0.816705	test: 0.504611
PRC train: 0.617806	val: 0.552484	test: 0.514846

Epoch: 8
Loss: 0.3150318690131481
ROC train: 0.837536	val: 0.846111	test: 0.554653
PRC train: 0.650777	val: 0.566361	test: 0.524396

Epoch: 9
Loss: 0.2904111596809479
ROC train: 0.850473	val: 0.857275	test: 0.569000
PRC train: 0.662212	val: 0.568544	test: 0.520568

Epoch: 10
Loss: 0.26547710754005455
ROC train: 0.872674	val: 0.827082	test: 0.572735
PRC train: 0.674621	val: 0.553374	test: 0.521964

Epoch: 11
Loss: 0.25432411182984654
ROC train: 0.896464	val: 0.829567	test: 0.601105
PRC train: 0.741272	val: 0.554094	test: 0.534917

Epoch: 12
Loss: 0.2368015988577567
ROC train: 0.900613	val: 0.849309	test: 0.598531
PRC train: 0.746149	val: 0.558269	test: 0.532325

Epoch: 13
Loss: 0.21853556837604587
ROC train: 0.910494	val: 0.821362	test: 0.609812
PRC train: 0.759502	val: 0.567191	test: 0.545214

Epoch: 14
Loss: 0.20364236268122243
ROC train: 0.929660	val: 0.833137	test: 0.646619
PRC train: 0.803314	val: 0.565837	test: 0.555959

Epoch: 15
Loss: 0.2014990493059247
ROC train: 0.942973	val: 0.778746	test: 0.667261
PRC train: 0.845818	val: 0.540079	test: 0.551656

Epoch: 16
Loss: 0.18603891863651184
ROC train: 0.957148	val: 0.792507	test: 0.666799
PRC train: 0.867278	val: 0.541746	test: 0.546220

Epoch: 17
Loss: 0.19028196279612133
ROC train: 0.957064	val: 0.830017	test: 0.677241
PRC train: 0.868751	val: 0.554230	test: 0.546979

Epoch: 18
Loss: 0.17546119190528428
ROC train: 0.961651	val: 0.845127	test: 0.672169
PRC train: 0.883471	val: 0.554939	test: 0.546103

Epoch: 19
Loss: 0.16391512053863333
ROC train: 0.973652	val: 0.831689	test: 0.718016
PRC train: 0.913170	val: 0.632732	test: 0.572775

Epoch: 20
Loss: 0.16137334323247743
ROC train: 0.973077	val: 0.790121	test: 0.700064
PRC train: 0.922919	val: 0.547306	test: 0.557844

Epoch: 21
Loss: 0.15409897052916036
ROC train: 0.971690	val: 0.772340	test: 0.667261
PRC train: 0.919774	val: 0.543152	test: 0.546127

Epoch: 22
Loss: 0.1569707192782972
ROC train: 0.972338	val: 0.782815	test: 0.684237
PRC train: 0.908892	val: 0.548249	test: 0.549710

Epoch: 23
Loss: 0.1535936069913208
ROC train: 0.984686	val: 0.755694	test: 0.699952
PRC train: 0.949433	val: 0.543385	test: 0.559854

Epoch: 24
Loss: 0.12812061440766978
ROC train: 0.987157	val: 0.781680	test: 0.697240
PRC train: 0.954840	val: 0.544786	test: 0.546465

Epoch: 25
Loss: 0.12971902610291103
ROC train: 0.989494	val: 0.749551	test: 0.709257
PRC train: 0.959666	val: 0.628238	test: 0.548869

Epoch: 26
Loss: 0.12238102764716279
ROC train: 0.990583	val: 0.742010	test: 0.696627
PRC train: 0.966429	val: 0.612129	test: 0.544511

Epoch: 27
Loss: 0.13485810290496705
ROC train: 0.991325	val: 0.765347	test: 0.681951
PRC train: 0.971625	val: 0.553703	test: 0.551006

Epoch: 28
Loss: 0.12785390588160844
ROC train: 0.989770	val: 0.723090	test: 0.646862
PRC train: 0.963206	val: 0.534608	test: 0.551359

Epoch: 29
Loss: 0.09853199988731498
ROC train: 0.986155	val: 0.751238	test: 0.604575
PRC train: 0.940139	val: 0.538421	test: 0.527303

Epoch: 30
Loss: 0.10157608694353368
ROC train: 0.992504	val: 0.678964	test: 0.641125
PRC train: 0.965485	val: 0.526166	test: 0.531832

Epoch: 31
Loss: 0.09562557542215078
ROC train: 0.995231	val: 0.687292	test: 0.673917
PRC train: 0.977788	val: 0.527489	test: 0.545428

Epoch: 32
Loss: 0.09249039626155739
ROC train: 0.996088	val: 0.715050	test: 0.693531Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: clintox
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/clintox/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/clintox/noise=0.1/clintox_scaff_4_26-05_11-28-51  ]
[ Using Seed :  4  ]
[ Using device :  cuda:2  ]
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.6882654230408465
ROC train: 0.528473	val: 0.570304	test: 0.442728
PRC train: 0.513004	val: 0.514338	test: 0.496798

Epoch: 2
Loss: 0.6064239709446817
ROC train: 0.571474	val: 0.640055	test: 0.433629
PRC train: 0.521744	val: 0.556363	test: 0.494739

Epoch: 3
Loss: 0.5390741051370485
ROC train: 0.619499	val: 0.739111	test: 0.451776
PRC train: 0.534991	val: 0.542035	test: 0.495667

Epoch: 4
Loss: 0.4838317149701988
ROC train: 0.653906	val: 0.693059	test: 0.476608
PRC train: 0.548088	val: 0.535941	test: 0.503895

Epoch: 5
Loss: 0.4287214607997159
ROC train: 0.707350	val: 0.773250	test: 0.485490
PRC train: 0.574156	val: 0.549833	test: 0.502953

Epoch: 6
Loss: 0.3847703918304208
ROC train: 0.756043	val: 0.791569	test: 0.503842
PRC train: 0.601093	val: 0.551632	test: 0.509879

Epoch: 7
Loss: 0.3448284210997774
ROC train: 0.794479	val: 0.816168	test: 0.552460
PRC train: 0.642164	val: 0.562993	test: 0.515795

Epoch: 8
Loss: 0.3262263387494435
ROC train: 0.819209	val: 0.831377	test: 0.534535
PRC train: 0.652106	val: 0.567771	test: 0.509404

Epoch: 9
Loss: 0.2919583686352819
ROC train: 0.858655	val: 0.813434	test: 0.541542
PRC train: 0.679311	val: 0.557249	test: 0.509612

Epoch: 10
Loss: 0.2737831767842436
ROC train: 0.891700	val: 0.818779	test: 0.550772
PRC train: 0.709208	val: 0.553711	test: 0.512950

Epoch: 11
Loss: 0.25278147602246676
ROC train: 0.910659	val: 0.816519	test: 0.543451
PRC train: 0.731727	val: 0.559825	test: 0.512812

Epoch: 12
Loss: 0.23597150426398353
ROC train: 0.929300	val: 0.836222	test: 0.581427
PRC train: 0.768484	val: 0.561646	test: 0.516565

Epoch: 13
Loss: 0.22324628455994366
ROC train: 0.939181	val: 0.837333	test: 0.629934
PRC train: 0.796531	val: 0.563455	test: 0.525186

Epoch: 14
Loss: 0.21580852601449543
ROC train: 0.946124	val: 0.813458	test: 0.609935
PRC train: 0.834213	val: 0.554605	test: 0.522880

Epoch: 15
Loss: 0.19763150262766244
ROC train: 0.958316	val: 0.843602	test: 0.620977
PRC train: 0.874460	val: 0.559137	test: 0.526278

Epoch: 16
Loss: 0.18335522859617392
ROC train: 0.969518	val: 0.842203	test: 0.629833
PRC train: 0.893475	val: 0.558558	test: 0.529727

Epoch: 17
Loss: 0.17569244087130992
ROC train: 0.975676	val: 0.851207	test: 0.604123
PRC train: 0.913154	val: 0.562001	test: 0.526961

Epoch: 18
Loss: 0.1596216633278646
ROC train: 0.976142	val: 0.829043	test: 0.597191
PRC train: 0.911928	val: 0.552021	test: 0.524629

Epoch: 19
Loss: 0.1550776617376846
ROC train: 0.986928	val: 0.863481	test: 0.620824
PRC train: 0.941382	val: 0.566270	test: 0.534721

Epoch: 20
Loss: 0.15932144452475858
ROC train: 0.986895	val: 0.874245	test: 0.640225
PRC train: 0.945618	val: 0.572835	test: 0.537990

Epoch: 21
Loss: 0.1425515656934821
ROC train: 0.989055	val: 0.884060	test: 0.614754
PRC train: 0.952931	val: 0.577729	test: 0.530162

Epoch: 22
Loss: 0.13488498526061252
ROC train: 0.994210	val: 0.718258	test: 0.619587
PRC train: 0.969337	val: 0.535938	test: 0.532901

Epoch: 23
Loss: 0.1251047417612133
ROC train: 0.994108	val: 0.793692	test: 0.644984
PRC train: 0.972167	val: 0.547733	test: 0.533180

Epoch: 24
Loss: 0.1312893463450597
ROC train: 0.992011	val: 0.897557	test: 0.634857
PRC train: 0.966120	val: 0.592079	test: 0.534517

Epoch: 25
Loss: 0.12419813517457437
ROC train: 0.992102	val: 0.885732	test: 0.641226
PRC train: 0.959560	val: 0.581763	test: 0.533747

Epoch: 26
Loss: 0.10083885764903851
ROC train: 0.995982	val: 0.855515	test: 0.624798
PRC train: 0.980833	val: 0.563892	test: 0.530159

Epoch: 27
Loss: 0.09327806155971034
ROC train: 0.996837	val: 0.820950	test: 0.646197
PRC train: 0.986369	val: 0.556917	test: 0.535080

Epoch: 28
Loss: 0.09521412440727045
ROC train: 0.997619	val: 0.815556	test: 0.640251
PRC train: 0.989471	val: 0.551710	test: 0.534098

Epoch: 29
Loss: 0.0909825107102357
ROC train: 0.997948	val: 0.822236	test: 0.653892
PRC train: 0.991510	val: 0.554426	test: 0.536286

Epoch: 30
Loss: 0.10408955650367438
ROC train: 0.995912	val: 0.776434	test: 0.656891
PRC train: 0.981948	val: 0.565239	test: 0.536200

Epoch: 31
Loss: 0.10364473357178594
ROC train: 0.998226	val: 0.858249	test: 0.632848
PRC train: 0.991351	val: 0.570918	test: 0.533465

Epoch: 32
Loss: 0.0907696651591913
ROC train: 0.998892	val: 0.867052	test: 0.635279Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: clintox
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/clintox/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/clintox/noise=0.1/clintox_scaff_6_26-05_11-28-51  ]
[ Using Seed :  6  ]
[ Using device :  cuda:2  ]
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.7091764359091502
ROC train: 0.584502	val: 0.642189	test: 0.568265
PRC train: 0.528448	val: 0.525963	test: 0.513613

Epoch: 2
Loss: 0.6217255682326499
ROC train: 0.638588	val: 0.711602	test: 0.540119
PRC train: 0.537797	val: 0.539165	test: 0.510380

Epoch: 3
Loss: 0.5483572257110068
ROC train: 0.668829	val: 0.781417	test: 0.528947
PRC train: 0.548848	val: 0.556118	test: 0.509621

Epoch: 4
Loss: 0.48890975766715555
ROC train: 0.703367	val: 0.784889	test: 0.522552
PRC train: 0.570193	val: 0.578272	test: 0.508824

Epoch: 5
Loss: 0.43321202563321737
ROC train: 0.751782	val: 0.791495	test: 0.523000
PRC train: 0.601456	val: 0.567168	test: 0.510618

Epoch: 6
Loss: 0.3873061574466801
ROC train: 0.772963	val: 0.793143	test: 0.517606
PRC train: 0.617388	val: 0.563906	test: 0.509768

Epoch: 7
Loss: 0.3540269265965422
ROC train: 0.816889	val: 0.827869	test: 0.519631
PRC train: 0.653762	val: 0.583492	test: 0.508151

Epoch: 8
Loss: 0.3207197053646481
ROC train: 0.852817	val: 0.872386	test: 0.559916
PRC train: 0.690215	val: 0.605723	test: 0.516595

Epoch: 9
Loss: 0.2877137281852024
ROC train: 0.858137	val: 0.770119	test: 0.511238
PRC train: 0.697045	val: 0.613561	test: 0.510422

Epoch: 10
Loss: 0.2717414867991076
ROC train: 0.887932	val: 0.792683	test: 0.526358
PRC train: 0.759996	val: 0.615182	test: 0.513125

Epoch: 11
Loss: 0.2489069688359781
ROC train: 0.910038	val: 0.853881	test: 0.554366
PRC train: 0.799175	val: 0.633870	test: 0.516536

Epoch: 12
Loss: 0.23786229831862177
ROC train: 0.929127	val: 0.849186	test: 0.504813
PRC train: 0.829061	val: 0.636496	test: 0.512916

Epoch: 13
Loss: 0.2280086609049184
ROC train: 0.931742	val: 0.794131	test: 0.496020
PRC train: 0.838234	val: 0.626201	test: 0.510716

Epoch: 14
Loss: 0.19648271395964917
ROC train: 0.945618	val: 0.803921	test: 0.465051
PRC train: 0.866174	val: 0.632787	test: 0.504363

Epoch: 15
Loss: 0.1972064487972478
ROC train: 0.954660	val: 0.761590	test: 0.506625
PRC train: 0.878734	val: 0.631950	test: 0.513924

Epoch: 16
Loss: 0.1770861605521916
ROC train: 0.961330	val: 0.794855	test: 0.507308
PRC train: 0.886916	val: 0.648144	test: 0.516755

Epoch: 17
Loss: 0.17710767628267451
ROC train: 0.967939	val: 0.835200	test: 0.486689
PRC train: 0.908519	val: 0.707954	test: 0.514603

Epoch: 18
Loss: 0.16694522718415336
ROC train: 0.970246	val: 0.837835	test: 0.517944
PRC train: 0.910895	val: 0.636825	test: 0.526337

Epoch: 19
Loss: 0.16757224507653895
ROC train: 0.976611	val: 0.768446	test: 0.502557
PRC train: 0.918084	val: 0.617279	test: 0.524009

Epoch: 20
Loss: 0.14907965064804266
ROC train: 0.981479	val: 0.772305	test: 0.548049
PRC train: 0.933376	val: 0.631974	test: 0.525134

Epoch: 21
Loss: 0.14983594289057872
ROC train: 0.984748	val: 0.707596	test: 0.560689
PRC train: 0.949817	val: 0.680996	test: 0.522498

Epoch: 22
Loss: 0.14651528472362385
ROC train: 0.987074	val: 0.704912	test: 0.555792
PRC train: 0.962696	val: 0.639017	test: 0.519643

Epoch: 23
Loss: 0.14031057822837636
ROC train: 0.989999	val: 0.760329	test: 0.572657
PRC train: 0.970357	val: 0.629554	test: 0.522615

Epoch: 24
Loss: 0.12299448034752727
ROC train: 0.991363	val: 0.788037	test: 0.565037
PRC train: 0.969969	val: 0.591979	test: 0.520935

Epoch: 25
Loss: 0.12949573153820854
ROC train: 0.994653	val: 0.857989	test: 0.563341
PRC train: 0.976649	val: 0.683503	test: 0.524941

Epoch: 26
Loss: 0.10964990657016763
ROC train: 0.995790	val: 0.856302	test: 0.537396
PRC train: 0.978406	val: 0.659665	test: 0.524232

Epoch: 27
Loss: 0.11638063642555734
ROC train: 0.996027	val: 0.770442	test: 0.531812
PRC train: 0.981307	val: 0.650027	test: 0.521209

Epoch: 28
Loss: 0.11530666257447442
ROC train: 0.995651	val: 0.813785	test: 0.566725
PRC train: 0.980179	val: 0.595703	test: 0.524503

Epoch: 29
Loss: 0.10240022378884713
ROC train: 0.992467	val: 0.838194	test: 0.585051
PRC train: 0.963064	val: 0.578199	test: 0.527483

Epoch: 30
Loss: 0.10642793762835989
ROC train: 0.996244	val: 0.845524	test: 0.571335
PRC train: 0.981243	val: 0.589977	test: 0.524893

Epoch: 31
Loss: 0.10613910184114039
ROC train: 0.993177	val: 0.775812	test: 0.557055
PRC train: 0.980542	val: 0.642374	test: 0.526895

Epoch: 32
Loss: 0.09937017118465272
ROC train: 0.994893	val: 0.850121	test: 0.560641Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: clintox
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/clintox/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/clintox/noise=0.1/clintox_scaff_5_26-05_11-28-51  ]
[ Using Seed :  5  ]
[ Using device :  cuda:2  ]
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.6788320418910716
ROC train: 0.564322	val: 0.663231	test: 0.482659
PRC train: 0.519800	val: 0.524042	test: 0.509625

Epoch: 2
Loss: 0.5944403891919736
ROC train: 0.626074	val: 0.635610	test: 0.454260
PRC train: 0.538149	val: 0.531209	test: 0.501309

Epoch: 3
Loss: 0.5304334281804668
ROC train: 0.639237	val: 0.650394	test: 0.458716
PRC train: 0.543818	val: 0.558578	test: 0.500610

Epoch: 4
Loss: 0.47926966665868226
ROC train: 0.675595	val: 0.726551	test: 0.497970
PRC train: 0.550331	val: 0.558116	test: 0.505992

Epoch: 5
Loss: 0.4280196301278358
ROC train: 0.717872	val: 0.809976	test: 0.528248
PRC train: 0.569842	val: 0.554582	test: 0.512650

Epoch: 6
Loss: 0.3861155664450546
ROC train: 0.751837	val: 0.792595	test: 0.520841
PRC train: 0.597022	val: 0.574845	test: 0.512329

Epoch: 7
Loss: 0.35364236235503455
ROC train: 0.788991	val: 0.806904	test: 0.543029
PRC train: 0.641026	val: 0.563204	test: 0.518749

Epoch: 8
Loss: 0.3215503784538229
ROC train: 0.817384	val: 0.836173	test: 0.555516
PRC train: 0.659926	val: 0.570651	test: 0.524594

Epoch: 9
Loss: 0.29428462474739103
ROC train: 0.849034	val: 0.843553	test: 0.551325
PRC train: 0.691128	val: 0.592198	test: 0.541936

Epoch: 10
Loss: 0.27095692991017145
ROC train: 0.866026	val: 0.843328	test: 0.525454
PRC train: 0.721217	val: 0.589154	test: 0.514716

Epoch: 11
Loss: 0.2537990553355131
ROC train: 0.888760	val: 0.833738	test: 0.539644
PRC train: 0.749749	val: 0.585541	test: 0.514305

Epoch: 12
Loss: 0.23491351670541877
ROC train: 0.912604	val: 0.766447	test: 0.537758
PRC train: 0.787069	val: 0.571152	test: 0.512580

Epoch: 13
Loss: 0.22467307988956842
ROC train: 0.935076	val: 0.804244	test: 0.559501
PRC train: 0.822295	val: 0.580179	test: 0.522920

Epoch: 14
Loss: 0.20483476220385297
ROC train: 0.930910	val: 0.713802	test: 0.557783
PRC train: 0.838346	val: 0.607417	test: 0.523331

Epoch: 15
Loss: 0.2080179291685039
ROC train: 0.942375	val: 0.762114	test: 0.588939
PRC train: 0.865667	val: 0.560843	test: 0.534579

Epoch: 16
Loss: 0.19299902661470855
ROC train: 0.955447	val: 0.759155	test: 0.574861
PRC train: 0.892805	val: 0.622225	test: 0.536841

Epoch: 17
Loss: 0.17958775429083093
ROC train: 0.956008	val: 0.761840	test: 0.623909
PRC train: 0.896415	val: 0.630840	test: 0.534095

Epoch: 18
Loss: 0.1758627232848242
ROC train: 0.962752	val: 0.768608	test: 0.625989
PRC train: 0.911163	val: 0.640588	test: 0.539859

Epoch: 19
Loss: 0.17227303126077895
ROC train: 0.970069	val: 0.835449	test: 0.646063
PRC train: 0.919551	val: 0.637267	test: 0.548669

Epoch: 20
Loss: 0.15432981287029188
ROC train: 0.966569	val: 0.791920	test: 0.662165
PRC train: 0.920462	val: 0.597716	test: 0.545442

Epoch: 21
Loss: 0.14523388413581428
ROC train: 0.966088	val: 0.712990	test: 0.669685
PRC train: 0.929303	val: 0.584791	test: 0.542175

Epoch: 22
Loss: 0.14842310397341438
ROC train: 0.976328	val: 0.735442	test: 0.673670
PRC train: 0.943777	val: 0.642601	test: 0.547947

Epoch: 23
Loss: 0.13902788528769439
ROC train: 0.979227	val: 0.706995	test: 0.710470
PRC train: 0.928857	val: 0.560900	test: 0.549792

Epoch: 24
Loss: 0.13167991368564885
ROC train: 0.984444	val: 0.700403	test: 0.700340
PRC train: 0.948133	val: 0.570120	test: 0.554446

Epoch: 25
Loss: 0.11891697451507444
ROC train: 0.985871	val: 0.748890	test: 0.674245
PRC train: 0.960262	val: 0.580326	test: 0.550700

Epoch: 26
Loss: 0.11595709987131012
ROC train: 0.989484	val: 0.718198	test: 0.700665
PRC train: 0.967566	val: 0.562340	test: 0.551163

Epoch: 27
Loss: 0.11369996486602237
ROC train: 0.993299	val: 0.696197	test: 0.727966
PRC train: 0.976357	val: 0.611042	test: 0.561974

Epoch: 28
Loss: 0.11013900112710535
ROC train: 0.992374	val: 0.695135	test: 0.731145
PRC train: 0.978485	val: 0.596491	test: 0.560069

Epoch: 29
Loss: 0.10067917251600826
ROC train: 0.991233	val: 0.682186	test: 0.749444
PRC train: 0.968213	val: 0.581735	test: 0.562399

Epoch: 30
Loss: 0.11313532761813613
ROC train: 0.994367	val: 0.659734	test: 0.770250
PRC train: 0.977618	val: 0.576090	test: 0.572104

Epoch: 31
Loss: 0.09871340889774737
ROC train: 0.996888	val: 0.661583	test: 0.727809
PRC train: 0.987799	val: 0.559163	test: 0.570028

Epoch: 32
Loss: 0.08746559721804718
ROC train: 0.996990	val: 0.679614	test: 0.730857Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: clintox
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/clintox/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/clintox/noise=0.05/clintox_scaff_4_26-05_11-28-51  ]
[ Using Seed :  4  ]
[ Using device :  cuda:1  ]
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.6903179727518195
ROC train: 0.559858	val: 0.599475	test: 0.433435
PRC train: 0.519329	val: 0.516412	test: 0.492327

Epoch: 2
Loss: 0.6029072816987833
ROC train: 0.611293	val: 0.666952	test: 0.460101
PRC train: 0.531346	val: 0.558766	test: 0.496756

Epoch: 3
Loss: 0.5335496580866651
ROC train: 0.640429	val: 0.672245	test: 0.477011
PRC train: 0.537684	val: 0.604129	test: 0.497189

Epoch: 4
Loss: 0.4739704356726843
ROC train: 0.696721	val: 0.764310	test: 0.496158
PRC train: 0.554205	val: 0.575571	test: 0.499964

Epoch: 5
Loss: 0.42277565198665534
ROC train: 0.754127	val: 0.802470	test: 0.507220
PRC train: 0.581787	val: 0.562051	test: 0.502336

Epoch: 6
Loss: 0.3776932985602749
ROC train: 0.805587	val: 0.805629	test: 0.526097
PRC train: 0.621114	val: 0.552339	test: 0.509042

Epoch: 7
Loss: 0.3446385074791012
ROC train: 0.847377	val: 0.830066	test: 0.525902
PRC train: 0.643624	val: 0.562452	test: 0.510617

Epoch: 8
Loss: 0.31846732263984745
ROC train: 0.850762	val: 0.834399	test: 0.530437
PRC train: 0.644508	val: 0.562281	test: 0.513207

Epoch: 9
Loss: 0.2902315855486509
ROC train: 0.869435	val: 0.833425	test: 0.541942
PRC train: 0.671275	val: 0.558006	test: 0.517202

Epoch: 10
Loss: 0.2642891338957526
ROC train: 0.889429	val: 0.841954	test: 0.559767
PRC train: 0.693811	val: 0.557888	test: 0.521021

Epoch: 11
Loss: 0.2465746083225569
ROC train: 0.912616	val: 0.840218	test: 0.582514
PRC train: 0.750420	val: 0.560575	test: 0.525400

Epoch: 12
Loss: 0.2317156683429163
ROC train: 0.925132	val: 0.825958	test: 0.603070
PRC train: 0.778199	val: 0.561735	test: 0.525987

Epoch: 13
Loss: 0.21511238892592227
ROC train: 0.930402	val: 0.831102	test: 0.609834
PRC train: 0.788025	val: 0.566903	test: 0.527778

Epoch: 14
Loss: 0.2037203890787178
ROC train: 0.934974	val: 0.809189	test: 0.611171
PRC train: 0.822914	val: 0.557769	test: 0.526027

Epoch: 15
Loss: 0.19998318569931442
ROC train: 0.941177	val: 0.729872	test: 0.603518
PRC train: 0.849834	val: 0.540129	test: 0.527610

Epoch: 16
Loss: 0.18634444206522344
ROC train: 0.953859	val: 0.737702	test: 0.646758
PRC train: 0.884355	val: 0.543983	test: 0.535142

Epoch: 17
Loss: 0.17398346087086586
ROC train: 0.963438	val: 0.816930	test: 0.675889
PRC train: 0.884498	val: 0.554401	test: 0.541704

Epoch: 18
Loss: 0.16251218257146888
ROC train: 0.966827	val: 0.796239	test: 0.679330
PRC train: 0.877508	val: 0.547034	test: 0.541831

Epoch: 19
Loss: 0.149923022129194
ROC train: 0.980508	val: 0.820051	test: 0.684096
PRC train: 0.920085	val: 0.561163	test: 0.547243

Epoch: 20
Loss: 0.1616371274633968
ROC train: 0.978881	val: 0.878602	test: 0.691141
PRC train: 0.908926	val: 0.579112	test: 0.543351

Epoch: 21
Loss: 0.14924705686304138
ROC train: 0.983291	val: 0.866616	test: 0.679729
PRC train: 0.923689	val: 0.568034	test: 0.541822

Epoch: 22
Loss: 0.1435547012898429
ROC train: 0.989479	val: 0.845812	test: 0.710979
PRC train: 0.949246	val: 0.568749	test: 0.551182

Epoch: 23
Loss: 0.1389634939532507
ROC train: 0.988131	val: 0.857999	test: 0.714159
PRC train: 0.946173	val: 0.574534	test: 0.550501

Epoch: 24
Loss: 0.14041157145452993
ROC train: 0.983076	val: 0.841480	test: 0.707501
PRC train: 0.924239	val: 0.570346	test: 0.545750

Epoch: 25
Loss: 0.135360571487621
ROC train: 0.990534	val: 0.865242	test: 0.755776
PRC train: 0.963504	val: 0.585419	test: 0.560520

Epoch: 26
Loss: 0.12124434456123287
ROC train: 0.995977	val: 0.804156	test: 0.721023
PRC train: 0.981618	val: 0.561911	test: 0.550124

Epoch: 27
Loss: 0.11450758082123538
ROC train: 0.996912	val: 0.809438	test: 0.732352
PRC train: 0.985088	val: 0.577199	test: 0.552041

Epoch: 28
Loss: 0.11415450187612516
ROC train: 0.992303	val: 0.823161	test: 0.750965
PRC train: 0.958719	val: 0.566630	test: 0.558104

Epoch: 29
Loss: 0.10713622385293271
ROC train: 0.994115	val: 0.771802	test: 0.725206
PRC train: 0.974131	val: 0.550929	test: 0.551404

Epoch: 30
Loss: 0.10083743280065076
ROC train: 0.996031	val: 0.741135	test: 0.758835
PRC train: 0.985406	val: 0.597442	test: 0.559893

Epoch: 31
Loss: 0.09555271716782929
ROC train: 0.997766	val: 0.829029	test: 0.739909
PRC train: 0.990224	val: 0.574953	test: 0.552982

Epoch: 32
Loss: 0.0981508948847474
ROC train: 0.997204	val: 0.855476	test: 0.720421Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:3
  input_data_dir: 
  dataset: clintox
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/clintox/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/clintox/noise=0.2/clintox_scaff_4_26-05_11-28-51  ]
[ Using Seed :  4  ]
[ Using device :  cuda:3  ]
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.691535233271639
ROC train: 0.510172	val: 0.581219	test: 0.458237
PRC train: 0.509649	val: 0.514438	test: 0.500223

Epoch: 2
Loss: 0.6139747952501985
ROC train: 0.562779	val: 0.584203	test: 0.415886
PRC train: 0.525630	val: 0.517065	test: 0.496083

Epoch: 3
Loss: 0.5474004014161722
ROC train: 0.582080	val: 0.594952	test: 0.434601
PRC train: 0.532750	val: 0.519572	test: 0.498936

Epoch: 4
Loss: 0.4913712398258997
ROC train: 0.621240	val: 0.627830	test: 0.466392
PRC train: 0.541003	val: 0.529840	test: 0.501804

Epoch: 5
Loss: 0.4402579518846347
ROC train: 0.681148	val: 0.694809	test: 0.490862
PRC train: 0.558139	val: 0.531611	test: 0.505325

Epoch: 6
Loss: 0.39209664463893157
ROC train: 0.716267	val: 0.706272	test: 0.500618
PRC train: 0.579188	val: 0.545235	test: 0.506283

Epoch: 7
Loss: 0.35562992652691894
ROC train: 0.736100	val: 0.693336	test: 0.494997
PRC train: 0.587928	val: 0.604449	test: 0.505695

Epoch: 8
Loss: 0.33471248499272827
ROC train: 0.783870	val: 0.702613	test: 0.502004
PRC train: 0.630615	val: 0.606599	test: 0.508668

Epoch: 9
Loss: 0.30051619819426956
ROC train: 0.822759	val: 0.706858	test: 0.488688
PRC train: 0.659255	val: 0.608668	test: 0.506829

Epoch: 10
Loss: 0.27497638833639865
ROC train: 0.853168	val: 0.749776	test: 0.507977
PRC train: 0.688632	val: 0.540715	test: 0.509419

Epoch: 11
Loss: 0.2623105163171957
ROC train: 0.881777	val: 0.735604	test: 0.509687
PRC train: 0.725516	val: 0.610725	test: 0.510736

Epoch: 12
Loss: 0.24753466810550476
ROC train: 0.908564	val: 0.705361	test: 0.500543
PRC train: 0.771884	val: 0.607050	test: 0.511385

Epoch: 13
Loss: 0.22779017906428142
ROC train: 0.921109	val: 0.759816	test: 0.503293
PRC train: 0.798037	val: 0.615913	test: 0.511421

Epoch: 14
Loss: 0.2113400237492486
ROC train: 0.924762	val: 0.706134	test: 0.481770
PRC train: 0.815779	val: 0.608207	test: 0.508687

Epoch: 15
Loss: 0.20459403798102885
ROC train: 0.945468	val: 0.776149	test: 0.518282
PRC train: 0.855529	val: 0.615038	test: 0.513708

Epoch: 16
Loss: 0.19471404723321495
ROC train: 0.964200	val: 0.769943	test: 0.510371
PRC train: 0.890775	val: 0.575690	test: 0.511929

Epoch: 17
Loss: 0.17887385875120154
ROC train: 0.961252	val: 0.797065	test: 0.481292
PRC train: 0.880313	val: 0.616372	test: 0.509917

Epoch: 18
Loss: 0.17002908732865685
ROC train: 0.977574	val: 0.869163	test: 0.457894
PRC train: 0.922630	val: 0.636382	test: 0.506517

Epoch: 19
Loss: 0.16617035707902655
ROC train: 0.988248	val: 0.857388	test: 0.501040
PRC train: 0.950346	val: 0.638023	test: 0.512140

Epoch: 20
Loss: 0.1565470548091013
ROC train: 0.990262	val: 0.869800	test: 0.504850
PRC train: 0.957449	val: 0.640878	test: 0.512087

Epoch: 21
Loss: 0.1459891134697674
ROC train: 0.984922	val: 0.816793	test: 0.519003
PRC train: 0.924309	val: 0.624469	test: 0.514547

Epoch: 22
Loss: 0.13997666484770002
ROC train: 0.992779	val: 0.840843	test: 0.581088
PRC train: 0.974259	val: 0.628297	test: 0.524206

Epoch: 23
Loss: 0.1357058858042006
ROC train: 0.991710	val: 0.842291	test: 0.602663
PRC train: 0.965463	val: 0.582287	test: 0.528715

Epoch: 24
Loss: 0.1378107521656407
ROC train: 0.991450	val: 0.802558	test: 0.551997
PRC train: 0.959632	val: 0.638913	test: 0.518032

Epoch: 25
Loss: 0.1284529476163901
ROC train: 0.995754	val: 0.829454	test: 0.542734
PRC train: 0.983482	val: 0.650879	test: 0.517270

Epoch: 26
Loss: 0.10637516099248145
ROC train: 0.995638	val: 0.775973	test: 0.528499
PRC train: 0.982363	val: 0.624430	test: 0.514667

Epoch: 27
Loss: 0.1022934502327757
ROC train: 0.997075	val: 0.746392	test: 0.558377
PRC train: 0.988438	val: 0.615355	test: 0.518289

Epoch: 28
Loss: 0.09330556884441474
ROC train: 0.996758	val: 0.779069	test: 0.593328
PRC train: 0.987111	val: 0.622281	test: 0.524091

Epoch: 29
Loss: 0.109307656538147
ROC train: 0.997310	val: 0.747952	test: 0.615053
PRC train: 0.992759	val: 0.617125	test: 0.529037

Epoch: 30
Loss: 0.09264022652459686
ROC train: 0.997423	val: 0.840056	test: 0.580038
PRC train: 0.992681	val: 0.628790	test: 0.523986

Epoch: 31
Loss: 0.08859593536327928
ROC train: 0.998240	val: 0.861309	test: 0.583960
PRC train: 0.994422	val: 0.632970	test: 0.523741

Epoch: 32
Loss: 0.0790002597643521
ROC train: 0.998540	val: 0.858175	test: 0.592342
ROC train: 0.965812	val: 0.858899	test: 0.794511
PRC train: 0.847392	val: 0.627991	test: 0.573311

Epoch: 34
Loss: 0.15422286299210722
ROC train: 0.961889	val: 0.823849	test: 0.774012
PRC train: 0.836348	val: 0.583524	test: 0.564558

Epoch: 35
Loss: 0.15443046637023428
ROC train: 0.969039	val: 0.860160	test: 0.777172
PRC train: 0.852277	val: 0.625570	test: 0.569766

Epoch: 36
Loss: 0.16023863384021647
ROC train: 0.969268	val: 0.862870	test: 0.766627
PRC train: 0.857223	val: 0.647887	test: 0.568749

Epoch: 37
Loss: 0.14833708696512313
ROC train: 0.965292	val: 0.834163	test: 0.764341
PRC train: 0.855049	val: 0.628081	test: 0.564945

Epoch: 38
Loss: 0.15343814779689952
ROC train: 0.970033	val: 0.794791	test: 0.810162
PRC train: 0.856245	val: 0.587626	test: 0.593634

Epoch: 39
Loss: 0.143384787162641
ROC train: 0.971555	val: 0.809638	test: 0.823329
PRC train: 0.858165	val: 0.585473	test: 0.599683

Epoch: 40
Loss: 0.1454012683115121
ROC train: 0.972178	val: 0.807316	test: 0.796509
PRC train: 0.862167	val: 0.589554	test: 0.577597

Epoch: 41
Loss: 0.14842960362564878
ROC train: 0.973317	val: 0.783690	test: 0.799076
PRC train: 0.866847	val: 0.595528	test: 0.580554

Epoch: 42
Loss: 0.1368318486566254
ROC train: 0.974509	val: 0.822637	test: 0.818768
PRC train: 0.867891	val: 0.605190	test: 0.593970

Epoch: 43
Loss: 0.15237619399578967
ROC train: 0.973228	val: 0.776785	test: 0.833728
PRC train: 0.867186	val: 0.628838	test: 0.595518

Epoch: 44
Loss: 0.13991278003189142
ROC train: 0.972153	val: 0.774164	test: 0.808526
PRC train: 0.863894	val: 0.627725	test: 0.585246

Epoch: 45
Loss: 0.1389343245950115
ROC train: 0.970308	val: 0.784340	test: 0.816295
PRC train: 0.858987	val: 0.633763	test: 0.598939

Epoch: 46
Loss: 0.1466095071955979
ROC train: 0.971480	val: 0.771029	test: 0.819455
PRC train: 0.868105	val: 0.644432	test: 0.599534

Epoch: 47
Loss: 0.13368667975297535
ROC train: 0.974854	val: 0.776149	test: 0.811249
PRC train: 0.873872	val: 0.603639	test: 0.596135

Epoch: 48
Loss: 0.13781637221453522
ROC train: 0.976373	val: 0.822500	test: 0.817857
PRC train: 0.880712	val: 0.603007	test: 0.601854

Epoch: 49
Loss: 0.14209868251743357
ROC train: 0.976538	val: 0.783803	test: 0.813759
PRC train: 0.887763	val: 0.593561	test: 0.591502

Epoch: 50
Loss: 0.12908182658400716
ROC train: 0.969587	val: 0.790283	test: 0.818581
PRC train: 0.864883	val: 0.610846	test: 0.588154

Epoch: 51
Loss: 0.13777657610900662
ROC train: 0.977007	val: 0.815170	test: 0.816631
PRC train: 0.888021	val: 0.584334	test: 0.585794

Epoch: 52
Loss: 0.1411536703087459
ROC train: 0.976640	val: 0.840780	test: 0.817857
PRC train: 0.885100	val: 0.604378	test: 0.588330

Epoch: 53
Loss: 0.13971083213330768
ROC train: 0.973749	val: 0.792244	test: 0.826052
PRC train: 0.874377	val: 0.602856	test: 0.585763

Epoch: 54
Loss: 0.14190690560822117
ROC train: 0.976877	val: 0.769630	test: 0.820143
PRC train: 0.880837	val: 0.587187	test: 0.584421

Epoch: 55
Loss: 0.1304022022667543
ROC train: 0.977400	val: 0.788923	test: 0.824352
PRC train: 0.885037	val: 0.576720	test: 0.592596

Epoch: 56
Loss: 0.13742019300789615
ROC train: 0.978113	val: 0.791246	test: 0.817169
PRC train: 0.885705	val: 0.565701	test: 0.591062

Epoch: 57
Loss: 0.12935208879885846
ROC train: 0.979976	val: 0.797339	test: 0.840717
PRC train: 0.891084	val: 0.592242	test: 0.604540

Epoch: 58
Loss: 0.13521886422078
ROC train: 0.980774	val: 0.738650	test: 0.842453
PRC train: 0.892582	val: 0.593952	test: 0.617138

Epoch: 59
Loss: 0.133719523976332
ROC train: 0.980018	val: 0.744768	test: 0.844265
PRC train: 0.886732	val: 0.594812	test: 0.616879

Epoch: 60
Loss: 0.12313275941692212
ROC train: 0.980695	val: 0.763811	test: 0.840705
PRC train: 0.893713	val: 0.577795	test: 0.646611

Epoch: 61
Loss: 0.12338089911816012
ROC train: 0.979823	val: 0.761464	test: 0.827400
PRC train: 0.889298	val: 0.579009	test: 0.638090

Epoch: 62
Loss: 0.12640305897724685
ROC train: 0.980763	val: 0.792107	test: 0.839955
PRC train: 0.895233	val: 0.597499	test: 0.625874

Epoch: 63
Loss: 0.1290869052386509
ROC train: 0.981016	val: 0.808602	test: 0.851385
PRC train: 0.895921	val: 0.648270	test: 0.627582

Epoch: 64
Loss: 0.12276582059557309
ROC train: 0.981477	val: 0.810812	test: 0.852871
PRC train: 0.905789	val: 0.588228	test: 0.634646

Epoch: 65
Loss: 0.1371286853124393
ROC train: 0.980933	val: 0.773763	test: 0.857981
PRC train: 0.906290	val: 0.574766	test: 0.628700

Epoch: 66
Loss: 0.13067932799976387
ROC train: 0.980825	val: 0.764897	test: 0.862990
PRC train: 0.904738	val: 0.568517	test: 0.623830

Epoch: 67
Loss: 0.12746814436424675
ROC train: 0.982109	val: 0.778458	test: 0.875331
PRC train: 0.903331	val: 0.603020	test: 0.621996

Epoch: 68
Loss: 0.11663567769158245
ROC train: 0.981333	val: 0.750363	test: 0.882552
PRC train: 0.899158	val: 0.583676	test: 0.631638

Epoch: 69
Loss: 0.12092041295142968
ROC train: 0.982612	val: 0.780918	test: 0.872821
PRC train: 0.905487	val: 0.599156	test: 0.620811

Epoch: 70
Loss: 0.1200759235464078
ROC train: 0.980499	val: 0.859124	test: 0.826885
PRC train: 0.900730	val: 0.576636	test: 0.589971

Epoch: 71
Loss: 0.11406903897899359
ROC train: 0.981358	val: 0.826808	test: 0.829783
PRC train: 0.901482	val: 0.564038	test: 0.594825

Epoch: 72
Loss: 0.1293242608339687
ROC train: 0.982414	val: 0.767420	test: 0.866800
PRC train: 0.902475	val: 0.564601	test: 0.642135

Epoch: 73
Loss: 0.10581416727727551
ROC train: 0.983325	val: 0.756119	test: 0.878492
PRC train: 0.907413	val: 0.582006	test: 0.711603

Epoch: 74
Loss: 0.10816652209041382
ROC train: 0.983887	val: 0.776360	test: 0.877405
PRC train: 0.911990	val: 0.581228	test: 0.716682

Epoch: 75
Loss: 0.11701282536738067
ROC train: 0.984062	val: 0.794166	test: 0.861365
PRC train: 0.912762	val: 0.585560	test: 0.662702

Epoch: 76
Loss: 0.12085549988062878
ROC train: 0.982895	val: 0.791319	test: 0.854907
PRC train: 0.910981	val: 0.572825	test: 0.646350

Epoch: 77
Loss: 0.11817036429940571
ROC train: 0.981402	val: 0.748651	test: 0.875574
PRC train: 0.906190	val: 0.569305	test: 0.646488

Epoch: 78
Loss: 0.11278141617463713
ROC train: 0.985004	val: 0.738587	test: 0.856244
PRC train: 0.918944	val: 0.552716	test: 0.632617

Epoch: 79
Loss: 0.12262026814175538
ROC train: 0.983931	val: 0.750387	test: 0.832148
PRC train: 0.912005	val: 0.563664	test: 0.621150

Epoch: 80
Loss: 0.12003733878334236
ROC train: 0.982289	val: 0.740910	test: 0.850835
PRC train: 0.906696	val: 0.561416	test: 0.638798

Epoch: 81
Loss: 0.11033113564802617
ROC train: 0.984747	val: 0.759390	test: 0.874301
PRC train: 0.912394	val: 0.610162	test: 0.641126

Epoch: 82
Loss: 0.11700961166714308
ROC train: 0.984282	val: 0.730122	test: 0.885506
PRC train: 0.913256	val: 0.551865	test: 0.662003

Epoch: 83
Loss: 0.11537140263003713
ROC train: 0.985291	val: 0.753322	test: 0.885925
PRC train: 0.919512	val: 0.572149	test: 0.661430

Epoch: 84
Loss: 0.1112780484683106
ROC train: 0.984169	val: 0.756231	test: 0.865276
PRC train: 0.916289	val: 0.574157	test: 0.644569

Epoch: 85
Loss: 0.1107717114698071
ROC train: 0.984288	val: 0.730508	test: 0.846626
PRC train: 0.918814	val: 0.567646	test: 0.650234

Epoch: 86
Loss: 0.11441192839466412
ROC train: 0.983945	val: 0.742583	test: 0.862915
PRC train: 0.913280	val: 0.594340	test: 0.640652

Epoch: 87
Loss: 0.11349713902378726
ROC train: 0.984739	val: 0.755982	test: 0.864290
PRC train: 0.919328	val: 0.597867	test: 0.637336

Epoch: 88
Loss: 0.10879410527425926
ROC train: 0.985158	val: 0.769430	test: 0.859793
PRC train: 0.922227	val: 0.604240	test: 0.637523

Epoch: 89
Loss: 0.10330130334459309
ROC train: 0.985390	val: 0.754720	test: 0.850835
PRC train: 0.920718	val: 0.641850	test: 0.631694

Epoch: 90
Loss: 0.11744789148552773
ROC train: 0.984041	val: 0.735790	test: 0.855407
PRC train: 0.913901	val: 0.582826	test: 0.630894

Epoch: 91
Loss: 0.10958622075645238
ROC train: 0.983994	val: 0.706746	test: 0.843652
PRC train: 0.912073	val: 0.563544	test: 0.641351

Epoch: 92
Loss: 0.11463178882236016
ROC train: 0.985255	val: 0.764261	test: 0.869161
PRC train: 0.915802	val: 0.590196	test: 0.661817

Epoch: 93
Loss: 0.10908940544252535
ROC train: 0.983832	val: 0.811511	test: 0.871772
PRC train: 0.904831	val: 0.665428	test: 0.649857

Epoch: 94
Loss: 0.10763675481468524
ROC train: 0.962263	val: 0.868176	test: 0.739796
PRC train: 0.825512	val: 0.607146	test: 0.560896

Epoch: 34
Loss: 0.1488745928018717
ROC train: 0.966524	val: 0.798724	test: 0.744899
PRC train: 0.844805	val: 0.617859	test: 0.571084

Epoch: 35
Loss: 0.16000138485689996
ROC train: 0.964818	val: 0.735878	test: 0.740708
PRC train: 0.848439	val: 0.567068	test: 0.567394

Epoch: 36
Loss: 0.1511699629445425
ROC train: 0.968081	val: 0.775935	test: 0.720085
PRC train: 0.851891	val: 0.574673	test: 0.559666

Epoch: 37
Loss: 0.1634758328823729
ROC train: 0.968331	val: 0.812959	test: 0.724000
PRC train: 0.852486	val: 0.577552	test: 0.561813

Epoch: 38
Loss: 0.14876409821476563
ROC train: 0.969482	val: 0.783529	test: 0.761555
PRC train: 0.856690	val: 0.571937	test: 0.569921

Epoch: 39
Loss: 0.14809228172280536
ROC train: 0.968561	val: 0.742959	test: 0.806288
PRC train: 0.846042	val: 0.569220	test: 0.585037

Epoch: 40
Loss: 0.14926222932168098
ROC train: 0.970780	val: 0.810387	test: 0.808925
PRC train: 0.859412	val: 0.616931	test: 0.597205

Epoch: 41
Loss: 0.1394233099589896
ROC train: 0.971566	val: 0.804206	test: 0.774311
PRC train: 0.871353	val: 0.641237	test: 0.584593

Epoch: 42
Loss: 0.14184522409654027
ROC train: 0.969546	val: 0.807477	test: 0.750364
PRC train: 0.861976	val: 0.636447	test: 0.567495

Epoch: 43
Loss: 0.142838054722256
ROC train: 0.971255	val: 0.812896	test: 0.772200
PRC train: 0.862020	val: 0.643913	test: 0.575069

Epoch: 44
Loss: 0.13756948573665087
ROC train: 0.972825	val: 0.806353	test: 0.785554
PRC train: 0.870256	val: 0.631793	test: 0.576602

Epoch: 45
Loss: 0.13666608274481212
ROC train: 0.972500	val: 0.782092	test: 0.795534
PRC train: 0.868024	val: 0.592285	test: 0.577753

Epoch: 46
Loss: 0.1402107203616402
ROC train: 0.975139	val: 0.793692	test: 0.804391
PRC train: 0.877510	val: 0.598467	test: 0.584399

Epoch: 47
Loss: 0.13521919972762247
ROC train: 0.975247	val: 0.805854	test: 0.814047
PRC train: 0.875160	val: 0.656929	test: 0.600789

Epoch: 48
Loss: 0.13411901188496306
ROC train: 0.975723	val: 0.834125	test: 0.813210
PRC train: 0.880845	val: 0.690997	test: 0.607855

Epoch: 49
Loss: 0.13866175866482755
ROC train: 0.975613	val: 0.817043	test: 0.809400
PRC train: 0.883771	val: 0.631610	test: 0.607416

Epoch: 50
Loss: 0.1330994287882205
ROC train: 0.973890	val: 0.808064	test: 0.799419
PRC train: 0.874508	val: 0.680708	test: 0.598388

Epoch: 51
Loss: 0.13636642974385008
ROC train: 0.975381	val: 0.783378	test: 0.781893
PRC train: 0.883072	val: 0.608333	test: 0.587421

Epoch: 52
Loss: 0.134166735313355
ROC train: 0.976982	val: 0.802445	test: 0.781467
PRC train: 0.891371	val: 0.605515	test: 0.580227

Epoch: 53
Loss: 0.12397688455588962
ROC train: 0.976615	val: 0.822075	test: 0.762305
PRC train: 0.890326	val: 0.596067	test: 0.568168

Epoch: 54
Loss: 0.12530027099796576
ROC train: 0.975465	val: 0.811561	test: 0.756759
PRC train: 0.884139	val: 0.595580	test: 0.566860

Epoch: 55
Loss: 0.12592091390020504
ROC train: 0.978859	val: 0.799423	test: 0.768569
PRC train: 0.895355	val: 0.623634	test: 0.580420

Epoch: 56
Loss: 0.14360044612745146
ROC train: 0.978086	val: 0.806690	test: 0.749064
PRC train: 0.892503	val: 0.631301	test: 0.568584

Epoch: 57
Loss: 0.13223356355792865
ROC train: 0.979474	val: 0.799173	test: 0.774890
PRC train: 0.893814	val: 0.593087	test: 0.575630

Epoch: 58
Loss: 0.1292174759528842
ROC train: 0.980238	val: 0.782952	test: 0.812235
PRC train: 0.889769	val: 0.598613	test: 0.593334

Epoch: 59
Loss: 0.12982078941503605
ROC train: 0.980104	val: 0.763685	test: 0.824403
PRC train: 0.894277	val: 0.594110	test: 0.607336

Epoch: 60
Loss: 0.13022612311921214
ROC train: 0.979035	val: 0.776184	test: 0.810711
PRC train: 0.893348	val: 0.591281	test: 0.596907

Epoch: 61
Loss: 0.13871118236858704
ROC train: 0.980279	val: 0.789833	test: 0.786252
PRC train: 0.897315	val: 0.602077	test: 0.589821

Epoch: 62
Loss: 0.12564497310220366
ROC train: 0.978490	val: 0.778757	test: 0.788027
PRC train: 0.892649	val: 0.606921	test: 0.597208

Epoch: 63
Loss: 0.12210626814928374
ROC train: 0.980298	val: 0.788909	test: 0.801698
PRC train: 0.893627	val: 0.607382	test: 0.607599

Epoch: 64
Loss: 0.11599337615072206
ROC train: 0.980626	val: 0.789608	test: 0.811873
PRC train: 0.892239	val: 0.603631	test: 0.611612

Epoch: 65
Loss: 0.12568926830032917
ROC train: 0.980275	val: 0.826633	test: 0.788912
PRC train: 0.892474	val: 0.602994	test: 0.591561

Epoch: 66
Loss: 0.11973510365822108
ROC train: 0.979558	val: 0.804930	test: 0.771898
PRC train: 0.891173	val: 0.610745	test: 0.586978

Epoch: 67
Loss: 0.1183996260653272
ROC train: 0.980928	val: 0.753758	test: 0.766164
PRC train: 0.895013	val: 0.636801	test: 0.598279

Epoch: 68
Loss: 0.11079994495318975
ROC train: 0.981945	val: 0.763998	test: 0.758820
PRC train: 0.901548	val: 0.587052	test: 0.595598

Epoch: 69
Loss: 0.11235035613229287
ROC train: 0.983475	val: 0.776722	test: 0.770288
PRC train: 0.907332	val: 0.592225	test: 0.599845

Epoch: 70
Loss: 0.12406266993926089
ROC train: 0.982760	val: 0.806328	test: 0.793585
PRC train: 0.902249	val: 0.613525	test: 0.601730

Epoch: 71
Loss: 0.12290564325582472
ROC train: 0.981442	val: 0.835885	test: 0.811185
PRC train: 0.901017	val: 0.611326	test: 0.607856

Epoch: 72
Loss: 0.1165604470392287
ROC train: 0.983714	val: 0.811448	test: 0.829074
PRC train: 0.908866	val: 0.599832	test: 0.614663

Epoch: 73
Loss: 0.1087615443263112
ROC train: 0.983440	val: 0.780605	test: 0.842009
PRC train: 0.906564	val: 0.601687	test: 0.619401

Epoch: 74
Loss: 0.10897616493222817
ROC train: 0.983551	val: 0.806553	test: 0.832047
PRC train: 0.907848	val: 0.588250	test: 0.614022

Epoch: 75
Loss: 0.11764420038554127
ROC train: 0.983033	val: 0.841666	test: 0.794459
PRC train: 0.909558	val: 0.631585	test: 0.587941

Epoch: 76
Loss: 0.12859896493050244
ROC train: 0.983412	val: 0.798049	test: 0.802803
PRC train: 0.910423	val: 0.643748	test: 0.612026

Epoch: 77
Loss: 0.1180750803652473
ROC train: 0.978689	val: 0.764272	test: 0.806501
PRC train: 0.897990	val: 0.619273	test: 0.604559

Epoch: 78
Loss: 0.11649366204025871
ROC train: 0.983282	val: 0.766732	test: 0.807263
PRC train: 0.913070	val: 0.619945	test: 0.599519

Epoch: 79
Loss: 0.1229273051382146
ROC train: 0.984986	val: 0.785300	test: 0.824027
PRC train: 0.915393	val: 0.613277	test: 0.605061

Epoch: 80
Loss: 0.11952618879787344
ROC train: 0.985332	val: 0.788185	test: 0.842078
PRC train: 0.916102	val: 0.622295	test: 0.622460

Epoch: 81
Loss: 0.1078542440314256
ROC train: 0.984414	val: 0.789833	test: 0.834695
PRC train: 0.915109	val: 0.621982	test: 0.618678

Epoch: 82
Loss: 0.11510060856651635
ROC train: 0.982878	val: 0.783564	test: 0.833859
PRC train: 0.908646	val: 0.626869	test: 0.629010

Epoch: 83
Loss: 0.11063161437425344
ROC train: 0.984332	val: 0.773138	test: 0.854307
PRC train: 0.908184	val: 0.612751	test: 0.642798

Epoch: 84
Loss: 0.10548632228592283
ROC train: 0.985521	val: 0.784151	test: 0.855325
PRC train: 0.916299	val: 0.620792	test: 0.628283

Epoch: 85
Loss: 0.10370840366565481
ROC train: 0.984649	val: 0.797350	test: 0.844945
PRC train: 0.915423	val: 0.619832	test: 0.611101

Epoch: 86
Loss: 0.10291488373432858
ROC train: 0.983396	val: 0.797824	test: 0.853271
PRC train: 0.912213	val: 0.640223	test: 0.617749

Epoch: 87
Loss: 0.10831423310310231
ROC train: 0.985538	val: 0.788821	test: 0.845976
PRC train: 0.917807	val: 0.629136	test: 0.621398

Epoch: 88
Loss: 0.13149462744943233
ROC train: 0.983749	val: 0.786449	test: 0.790918
PRC train: 0.907456	val: 0.601457	test: 0.591378

Epoch: 89
Loss: 0.10697595345052643
ROC train: 0.979428	val: 0.822461	test: 0.767703
PRC train: 0.891030	val: 0.609352	test: 0.575692

Epoch: 90
Loss: 0.11859510810338791
ROC train: 0.982568	val: 0.853828	test: 0.784209
PRC train: 0.902983	val: 0.610591	test: 0.585915

Epoch: 91
Loss: 0.11024725769903006
ROC train: 0.984876	val: 0.831714	test: 0.810173
PRC train: 0.914945	val: 0.592589	test: 0.596851

Epoch: 92
Loss: 0.11676620554481976
ROC train: 0.984997	val: 0.836384	test: 0.829029
PRC train: 0.915810	val: 0.592684	test: 0.599676

Epoch: 93
Loss: 0.11329292795809001
ROC train: 0.984220	val: 0.849945	test: 0.837131
PRC train: 0.912641	val: 0.613080	test: 0.610518

Epoch: 94
Loss: 0.10132598866765552
ROC train: 0.961167	val: 0.791794	test: 0.776623
PRC train: 0.824127	val: 0.630701	test: 0.570389

Epoch: 34
Loss: 0.15287375283774893
ROC train: 0.964812	val: 0.828056	test: 0.749971
PRC train: 0.852629	val: 0.634561	test: 0.563094

Epoch: 35
Loss: 0.16174184129985425
ROC train: 0.961022	val: 0.887493	test: 0.741459
PRC train: 0.839066	val: 0.628295	test: 0.559684

Epoch: 36
Loss: 0.1553811163241224
ROC train: 0.963207	val: 0.873957	test: 0.768278
PRC train: 0.850554	val: 0.653097	test: 0.571942

Epoch: 37
Loss: 0.15281432166095626
ROC train: 0.962452	val: 0.843739	test: 0.781131
PRC train: 0.844564	val: 0.611931	test: 0.567190

Epoch: 38
Loss: 0.15173490194179148
ROC train: 0.966685	val: 0.785300	test: 0.798396
PRC train: 0.853136	val: 0.605630	test: 0.579807

Epoch: 39
Loss: 0.14337607321554982
ROC train: 0.969368	val: 0.737550	test: 0.819526
PRC train: 0.866175	val: 0.596320	test: 0.604444

Epoch: 40
Loss: 0.13336145035376637
ROC train: 0.971395	val: 0.762687	test: 0.812511
PRC train: 0.870317	val: 0.613823	test: 0.601649

Epoch: 41
Loss: 0.14156457906852132
ROC train: 0.969788	val: 0.796151	test: 0.795796
PRC train: 0.856628	val: 0.609776	test: 0.591590

Epoch: 42
Loss: 0.14596741041274633
ROC train: 0.971668	val: 0.738724	test: 0.798145
PRC train: 0.867583	val: 0.620905	test: 0.601410

Epoch: 43
Loss: 0.1446462216534365
ROC train: 0.970518	val: 0.715935	test: 0.787190
PRC train: 0.864160	val: 0.616752	test: 0.611964

Epoch: 44
Loss: 0.13343422956613343
ROC train: 0.972043	val: 0.775524	test: 0.780287
PRC train: 0.864141	val: 0.599389	test: 0.581178

Epoch: 45
Loss: 0.1383606468243936
ROC train: 0.972074	val: 0.797750	test: 0.808925
PRC train: 0.867222	val: 0.619060	test: 0.631079

Epoch: 46
Loss: 0.1444239050271655
ROC train: 0.973845	val: 0.814495	test: 0.815134
PRC train: 0.873102	val: 0.606808	test: 0.631473

Epoch: 47
Loss: 0.13438383259991674
ROC train: 0.975601	val: 0.814382	test: 0.827781
PRC train: 0.874019	val: 0.599726	test: 0.602677

Epoch: 48
Loss: 0.13985827796834677
ROC train: 0.976221	val: 0.787647	test: 0.828876
PRC train: 0.881666	val: 0.616417	test: 0.612477

Epoch: 49
Loss: 0.1335662020865718
ROC train: 0.975902	val: 0.751997	test: 0.828151
PRC train: 0.884773	val: 0.624817	test: 0.610201

Epoch: 50
Loss: 0.1509741657696841
ROC train: 0.977418	val: 0.724127	test: 0.819362
PRC train: 0.886242	val: 0.608769	test: 0.602515

Epoch: 51
Loss: 0.14209094747796816
ROC train: 0.976174	val: 0.726949	test: 0.834572
PRC train: 0.882281	val: 0.598640	test: 0.611805

Epoch: 52
Loss: 0.1321754323297206
ROC train: 0.978061	val: 0.738411	test: 0.843066
PRC train: 0.882986	val: 0.606173	test: 0.610922

Epoch: 53
Loss: 0.1297805123650492
ROC train: 0.977639	val: 0.735927	test: 0.849674
PRC train: 0.885565	val: 0.619464	test: 0.609099

Epoch: 54
Loss: 0.13220489230840754
ROC train: 0.978003	val: 0.778121	test: 0.849618
PRC train: 0.888934	val: 0.628610	test: 0.614816

Epoch: 55
Loss: 0.12201516921998598
ROC train: 0.976291	val: 0.803619	test: 0.828214
PRC train: 0.878203	val: 0.611562	test: 0.602040

Epoch: 56
Loss: 0.11924615144840485
ROC train: 0.976997	val: 0.777333	test: 0.843025
PRC train: 0.881872	val: 0.625751	test: 0.644327

Epoch: 57
Loss: 0.1438267216078713
ROC train: 0.978305	val: 0.749238	test: 0.845908
PRC train: 0.886577	val: 0.649938	test: 0.649538

Epoch: 58
Loss: 0.13199136873026918
ROC train: 0.973701	val: 0.746891	test: 0.839555
PRC train: 0.856045	val: 0.605887	test: 0.630142

Epoch: 59
Loss: 0.13416729248543588
ROC train: 0.979021	val: 0.781666	test: 0.871846
PRC train: 0.886839	val: 0.616129	test: 0.653997

Epoch: 60
Loss: 0.1315020210139956
ROC train: 0.979536	val: 0.813683	test: 0.858056
PRC train: 0.891787	val: 0.627616	test: 0.641139

Epoch: 61
Loss: 0.12645288571507401
ROC train: 0.976603	val: 0.802582	test: 0.842203
PRC train: 0.885506	val: 0.650421	test: 0.614352

Epoch: 62
Loss: 0.12598013354301005
ROC train: 0.979758	val: 0.778258	test: 0.840717
PRC train: 0.893872	val: 0.634120	test: 0.617291

Epoch: 63
Loss: 0.10630219936711013
ROC train: 0.979419	val: 0.761088	test: 0.841804
PRC train: 0.890829	val: 0.627378	test: 0.621941

Epoch: 64
Loss: 0.12833891002689796
ROC train: 0.981129	val: 0.760501	test: 0.806994
PRC train: 0.899651	val: 0.635474	test: 0.599266

Epoch: 65
Loss: 0.12132306830949144
ROC train: 0.982612	val: 0.758878	test: 0.828125
PRC train: 0.902771	val: 0.635970	test: 0.601988

Epoch: 66
Loss: 0.12518063641073723
ROC train: 0.982858	val: 0.762462	test: 0.848486
PRC train: 0.908756	val: 0.653880	test: 0.626926

Epoch: 67
Loss: 0.1233235032654731
ROC train: 0.981817	val: 0.765596	test: 0.876456
PRC train: 0.902225	val: 0.670949	test: 0.639829

Epoch: 68
Loss: 0.1127869910297659
ROC train: 0.980477	val: 0.783040	test: 0.852322
PRC train: 0.901805	val: 0.643939	test: 0.614511

Epoch: 69
Loss: 0.11985945658942207
ROC train: 0.977714	val: 0.842927	test: 0.831860
PRC train: 0.893542	val: 0.645151	test: 0.592199

Epoch: 70
Loss: 0.1310157640254676
ROC train: 0.977431	val: 0.868538	test: 0.834621
PRC train: 0.890879	val: 0.669730	test: 0.608177

Epoch: 71
Loss: 0.11607705057590816
ROC train: 0.980084	val: 0.803594	test: 0.850260
PRC train: 0.897265	val: 0.618826	test: 0.639637

Epoch: 72
Loss: 0.1192280365371112
ROC train: 0.981521	val: 0.791432	test: 0.865750
PRC train: 0.896760	val: 0.612735	test: 0.648074

Epoch: 73
Loss: 0.11588071318158506
ROC train: 0.980533	val: 0.792992	test: 0.878704
PRC train: 0.893544	val: 0.597178	test: 0.663854

Epoch: 74
Loss: 0.13181171556768884
ROC train: 0.983111	val: 0.761450	test: 0.888984
PRC train: 0.905501	val: 0.599334	test: 0.677727

Epoch: 75
Loss: 0.12297290052424067
ROC train: 0.982489	val: 0.764198	test: 0.856524
PRC train: 0.904118	val: 0.601612	test: 0.648545

Epoch: 76
Loss: 0.10624065335056047
ROC train: 0.981905	val: 0.801184	test: 0.833003
PRC train: 0.904021	val: 0.664565	test: 0.617388

Epoch: 77
Loss: 0.11220137641721843
ROC train: 0.982673	val: 0.820114	test: 0.871197
PRC train: 0.905631	val: 0.668180	test: 0.633586

Epoch: 78
Loss: 0.11047372151022686
ROC train: 0.983083	val: 0.791819	test: 0.875257
PRC train: 0.904925	val: 0.614944	test: 0.639132

Epoch: 79
Loss: 0.11591015260408008
ROC train: 0.983263	val: 0.795027	test: 0.887961
PRC train: 0.908717	val: 0.628927	test: 0.660382

Epoch: 80
Loss: 0.10739269485106744
ROC train: 0.984653	val: 0.786224	test: 0.876131
PRC train: 0.912521	val: 0.628685	test: 0.648466

Epoch: 81
Loss: 0.11042524821599513
ROC train: 0.983720	val: 0.763386	test: 0.866049
PRC train: 0.912791	val: 0.598641	test: 0.631145

Epoch: 82
Loss: 0.11386198258843912
ROC train: 0.985408	val: 0.757679	test: 0.858605
PRC train: 0.921592	val: 0.602370	test: 0.627984

Epoch: 83
Loss: 0.11246237934750683
ROC train: 0.985394	val: 0.765396	test: 0.847063
PRC train: 0.919812	val: 0.616624	test: 0.620586

Epoch: 84
Loss: 0.11959120492450506
ROC train: 0.985419	val: 0.747664	test: 0.840048
PRC train: 0.920803	val: 0.608854	test: 0.623015

Epoch: 85
Loss: 0.10625425319082149
ROC train: 0.985779	val: 0.733179	test: 0.839305
PRC train: 0.921925	val: 0.586784	test: 0.628007

Epoch: 86
Loss: 0.09766072278728481
ROC train: 0.985081	val: 0.733854	test: 0.838756
PRC train: 0.917956	val: 0.584202	test: 0.624621

Epoch: 87
Loss: 0.10809077699403233
ROC train: 0.984160	val: 0.745180	test: 0.857581
PRC train: 0.913135	val: 0.589175	test: 0.638034

Epoch: 88
Loss: 0.1064375589919265
ROC train: 0.985088	val: 0.719432	test: 0.887811
PRC train: 0.914919	val: 0.585950	test: 0.681787

Epoch: 89
Loss: 0.10874027320734
ROC train: 0.985889	val: 0.737213	test: 0.897011
PRC train: 0.918185	val: 0.601834	test: 0.682848

Epoch: 90
Loss: 0.1108341683964242
ROC train: 0.985445	val: 0.782141	test: 0.881640
PRC train: 0.916440	val: 0.637771	test: 0.639620

Epoch: 91
Loss: 0.09776531743314415
ROC train: 0.984445	val: 0.805179	test: 0.862553
PRC train: 0.916400	val: 0.638412	test: 0.635611

Epoch: 92
Loss: 0.11421575509168777
ROC train: 0.985100	val: 0.779431	test: 0.863815
PRC train: 0.921868	val: 0.671914	test: 0.644112

Epoch: 93
Loss: 0.10771065289133874
ROC train: 0.983672	val: 0.726587	test: 0.874596
PRC train: 0.915875	val: 0.631630	test: 0.657140

Epoch: 94
Loss: 0.11005057685053155
PRC train: 0.980223	val: 0.661965	test: 0.540097

Epoch: 33
Loss: 0.10152043750492772
ROC train: 0.996583	val: 0.849446	test: 0.628007
PRC train: 0.981881	val: 0.615304	test: 0.535614

Epoch: 34
Loss: 0.10406141006818688
ROC train: 0.997101	val: 0.880089	test: 0.651016
PRC train: 0.983131	val: 0.656917	test: 0.542473

Epoch: 35
Loss: 0.08990641826768384
ROC train: 0.996460	val: 0.753698	test: 0.630636
PRC train: 0.983121	val: 0.625626	test: 0.541664

Epoch: 36
Loss: 0.0931198517082062
ROC train: 0.995486	val: 0.762026	test: 0.609831
PRC train: 0.977311	val: 0.583866	test: 0.537337

Epoch: 37
Loss: 0.08509070274161307
ROC train: 0.992619	val: 0.885283	test: 0.569594
PRC train: 0.967303	val: 0.628333	test: 0.527459

Epoch: 38
Loss: 0.0860280549254554
ROC train: 0.995044	val: 0.887043	test: 0.635052
PRC train: 0.976092	val: 0.733939	test: 0.547530

Epoch: 39
Loss: 0.0807246652651974
ROC train: 0.996854	val: 0.821737	test: 0.625769
PRC train: 0.983622	val: 0.712165	test: 0.541413

Epoch: 40
Loss: 0.07352543406900423
ROC train: 0.998366	val: 0.817654	test: 0.634476
PRC train: 0.990348	val: 0.699919	test: 0.540517

Epoch: 41
Loss: 0.07121203353432383
ROC train: 0.998056	val: 0.853579	test: 0.636725
PRC train: 0.988762	val: 0.660531	test: 0.538896

Epoch: 42
Loss: 0.06464663275258987
ROC train: 0.998063	val: 0.805555	test: 0.627181
PRC train: 0.990824	val: 0.611453	test: 0.539074

Epoch: 43
Loss: 0.08563001847279966
ROC train: 0.998131	val: 0.792831	test: 0.646082
PRC train: 0.991571	val: 0.679175	test: 0.547191

Epoch: 44
Loss: 0.06500664039350931
ROC train: 0.997430	val: 0.860435	test: 0.627969
PRC train: 0.988668	val: 0.701017	test: 0.536609

Epoch: 45
Loss: 0.06554583935752972
ROC train: 0.998615	val: 0.833313	test: 0.674365
PRC train: 0.991231	val: 0.698675	test: 0.547059

Epoch: 46
Loss: 0.057321171088719716
ROC train: 0.998852	val: 0.805492	test: 0.656727
PRC train: 0.993359	val: 0.696497	test: 0.554499

Epoch: 47
Loss: 0.05689880575467787
ROC train: 0.998616	val: 0.797251	test: 0.652581
PRC train: 0.992263	val: 0.674057	test: 0.553019

Epoch: 48
Loss: 0.06209470933594875
ROC train: 0.998979	val: 0.857075	test: 0.675714
PRC train: 0.993243	val: 0.693538	test: 0.554068

Epoch: 49
Loss: 0.050468311092190486
ROC train: 0.999739	val: 0.871897	test: 0.644621
PRC train: 0.998490	val: 0.620792	test: 0.540511

Epoch: 50
Loss: 0.05986246127468585
ROC train: 0.999646	val: 0.843353	test: 0.625284
PRC train: 0.998253	val: 0.639560	test: 0.538169

Epoch: 51
Loss: 0.05826072037349104
ROC train: 0.999715	val: 0.849558	test: 0.620024
PRC train: 0.998719	val: 0.646962	test: 0.551685

Epoch: 52
Loss: 0.05828179049613844
ROC train: 0.999246	val: 0.839006	test: 0.606364
PRC train: 0.995905	val: 0.675725	test: 0.550618

Epoch: 53
Loss: 0.055739852532330825
ROC train: 0.998957	val: 0.853666	test: 0.621955
PRC train: 0.993121	val: 0.612208	test: 0.541335

Epoch: 54
Loss: 0.05738636915703762
ROC train: 0.999505	val: 0.879551	test: 0.633516
PRC train: 0.997733	val: 0.707685	test: 0.555457

Epoch: 55
Loss: 0.054221935408890996
ROC train: 0.999436	val: 0.880388	test: 0.618463
PRC train: 0.996487	val: 0.707219	test: 0.553504

Epoch: 56
Loss: 0.05511948756160793
ROC train: 0.999540	val: 0.900130	test: 0.597896
PRC train: 0.996443	val: 0.726359	test: 0.544746

Epoch: 57
Loss: 0.0558645501667719
ROC train: 0.999935	val: 0.890628	test: 0.585111
PRC train: 0.999475	val: 0.648578	test: 0.530474

Epoch: 58
Loss: 0.05284825036120757
ROC train: 0.999965	val: 0.866103	test: 0.605285
PRC train: 0.999741	val: 0.629527	test: 0.535566

Epoch: 59
Loss: 0.056334455133313185
ROC train: 0.999971	val: 0.784963	test: 0.641637
PRC train: 0.999694	val: 0.621720	test: 0.545220

Epoch: 60
Loss: 0.054180829944639186
ROC train: 0.999723	val: 0.807326	test: 0.627133
PRC train: 0.998112	val: 0.698658	test: 0.543025

Epoch: 61
Loss: 0.037686571155120585
ROC train: 0.999956	val: 0.825220	test: 0.641472
PRC train: 0.999584	val: 0.663802	test: 0.547690

Epoch: 62
Loss: 0.03556848510336859
ROC train: 0.999887	val: 0.852704	test: 0.653989
PRC train: 0.999299	val: 0.660454	test: 0.545724

Epoch: 63
Loss: 0.03414418529326339
ROC train: 0.999952	val: 0.800558	test: 0.652279
PRC train: 0.999509	val: 0.674124	test: 0.549855

Epoch: 64
Loss: 0.058870365901734466
ROC train: 0.999799	val: 0.787422	test: 0.649780
PRC train: 0.998049	val: 0.698937	test: 0.550053

Epoch: 65
Loss: 0.039510179385894885
ROC train: 0.999938	val: 0.849495	test: 0.624096
PRC train: 0.999299	val: 0.650097	test: 0.556930

Epoch: 66
Loss: 0.04727344505376339
ROC train: 0.999977	val: 0.915926	test: 0.628392
PRC train: 0.999998	val: 0.667638	test: 0.549000

Epoch: 67
Loss: 0.0476507884165
ROC train: 0.999981	val: 0.910893	test: 0.656287
PRC train: 0.999792	val: 0.688521	test: 0.548043

Epoch: 68
Loss: 0.03976133670705197
ROC train: 0.999276	val: 0.861046	test: 0.640610
PRC train: 0.995667	val: 0.607898	test: 0.537772

Epoch: 69
Loss: 0.04333354822007695
ROC train: 0.999866	val: 0.869649	test: 0.668460
PRC train: 0.998608	val: 0.697100	test: 0.549356

Epoch: 70
Loss: 0.03962517706170654
ROC train: 0.999916	val: 0.893386	test: 0.661796
PRC train: 0.999272	val: 0.711216	test: 0.554821

Epoch: 71
Loss: 0.034058018361307375
ROC train: 0.999971	val: 0.891190	test: 0.626214
PRC train: 0.999673	val: 0.636564	test: 0.553407

Epoch: 72
Loss: 0.046460337826806905
ROC train: 1.000000	val: 0.902365	test: 0.627607
PRC train: 1.000000	val: 0.640550	test: 0.543837

Epoch: 73
Loss: 0.03592722835456174
ROC train: 0.999295	val: 0.901440	test: 0.634939
PRC train: 0.993655	val: 0.614350	test: 0.539702

Epoch: 74
Loss: 0.033051058369849416
ROC train: 1.000000	val: 0.838331	test: 0.617002
PRC train: 1.000000	val: 0.656453	test: 0.541373

Epoch: 75
Loss: 0.03687520211931969
ROC train: 0.999990	val: 0.845212	test: 0.599114
PRC train: 0.999894	val: 0.678470	test: 0.535019

Epoch: 76
Loss: 0.031200157222404233
ROC train: 0.999995	val: 0.893137	test: 0.602674
PRC train: 0.999946	val: 0.717349	test: 0.536283

Epoch: 77
Loss: 0.023833502188359296
ROC train: 1.000000	val: 0.919109	test: 0.618762
PRC train: 1.000000	val: 0.694033	test: 0.541768

Epoch: 78
Loss: 0.024277529895438105
ROC train: 1.000000	val: 0.933707	test: 0.599413
PRC train: 1.000000	val: 0.696311	test: 0.536286

Epoch: 79
Loss: 0.023634359690514244
ROC train: 1.000000	val: 0.928176	test: 0.594979
PRC train: 1.000000	val: 0.688555	test: 0.530586

Epoch: 80
Loss: 0.026609691018170206
ROC train: 0.999990	val: 0.908433	test: 0.600707
PRC train: 0.999892	val: 0.679851	test: 0.530229

Epoch: 81
Loss: 0.024880662060490007
ROC train: 0.999981	val: 0.909632	test: 0.617578
PRC train: 0.999786	val: 0.703552	test: 0.533642

Epoch: 82
Loss: 0.02063856071282793
ROC train: 0.999995	val: 0.887156	test: 0.633468
PRC train: 0.999946	val: 0.710709	test: 0.534099

Epoch: 83
Loss: 0.02179514424224669
ROC train: 1.000000	val: 0.867277	test: 0.646029
PRC train: 1.000000	val: 0.664402	test: 0.535719

Epoch: 84
Loss: 0.0318891258065667
ROC train: 0.999941	val: 0.904463	test: 0.660926
PRC train: 0.999465	val: 0.622183	test: 0.546984

Epoch: 85
Loss: 0.021172101923733807
ROC train: 0.999946	val: 0.911143	test: 0.679613
PRC train: 0.999464	val: 0.647476	test: 0.547899

Epoch: 86
Loss: 0.03262858622493453
ROC train: 0.999990	val: 0.885082	test: 0.659738
PRC train: 0.999894	val: 0.682058	test: 0.552311

Epoch: 87
Loss: 0.021946248432098348
ROC train: 1.000000	val: 0.876954	test: 0.638002
PRC train: 1.000000	val: 0.637213	test: 0.556582

Epoch: 88
Loss: 0.02750471155572016
ROC train: 1.000000	val: 0.915788	test: 0.668669
PRC train: 1.000000	val: 0.630128	test: 0.549091

Epoch: 89
Loss: 0.020719457079529637
ROC train: 0.999966	val: 0.922606	test: 0.672486
PRC train: 0.999646	val: 0.655205	test: 0.543774

Epoch: 90
Loss: 0.04210660022814924
ROC train: 1.000000	val: 0.906360	test: 0.622800
PRC train: 1.000000	val: 0.719159	test: 0.534619

Epoch: 91
Loss: 0.04124375445026759
ROC train: 0.999927	val: 0.891489	test: 0.602300
PRC train: 0.999192	val: 0.688158	test: 0.526251

Epoch: 92
Loss: 0.024083244397096328
ROC train: 1.000000	val: 0.842302	test: 0.600788
PRC train: 1.000000	val: 0.673399	test: 0.533712

Epoch: 93
Loss: 0.021281899817871718
PRC train: 0.991156	val: 0.612881	test: 0.522026

Epoch: 33
Loss: 0.06661288713472802
ROC train: 0.996824	val: 0.745893	test: 0.528340
PRC train: 0.984205	val: 0.635389	test: 0.518601

Epoch: 34
Loss: 0.07176216773685082
ROC train: 0.998569	val: 0.709381	test: 0.506830
PRC train: 0.993775	val: 0.632580	test: 0.525358

Epoch: 35
Loss: 0.06600647073938447
ROC train: 0.998957	val: 0.748490	test: 0.527180
PRC train: 0.994891	val: 0.599285	test: 0.527313

Epoch: 36
Loss: 0.05835632803078209
ROC train: 0.998850	val: 0.762799	test: 0.527430
PRC train: 0.992808	val: 0.578945	test: 0.523958

Epoch: 37
Loss: 0.05940332989848855
ROC train: 0.998808	val: 0.779294	test: 0.528991
PRC train: 0.992325	val: 0.598367	test: 0.522467

Epoch: 38
Loss: 0.060348055080076034
ROC train: 0.998774	val: 0.749938	test: 0.539584
PRC train: 0.993823	val: 0.553899	test: 0.525938

Epoch: 39
Loss: 0.061070008519717334
ROC train: 0.999438	val: 0.703499	test: 0.526474
PRC train: 0.997138	val: 0.535542	test: 0.520069

Epoch: 40
Loss: 0.05832778527581715
ROC train: 0.999667	val: 0.725251	test: 0.550040
PRC train: 0.998057	val: 0.547721	test: 0.523152

Epoch: 41
Loss: 0.060660055510898016
ROC train: 0.999338	val: 0.693037	test: 0.573460
PRC train: 0.996745	val: 0.558826	test: 0.530975

Epoch: 42
Loss: 0.0668524482099447
ROC train: 0.999638	val: 0.724155	test: 0.553061
PRC train: 0.998145	val: 0.621707	test: 0.526520

Epoch: 43
Loss: 0.05291132776690279
ROC train: 0.999681	val: 0.740885	test: 0.528166
PRC train: 0.997123	val: 0.575827	test: 0.517255

Epoch: 44
Loss: 0.0440116230425467
ROC train: 0.999578	val: 0.733369	test: 0.522682
PRC train: 0.995959	val: 0.612830	test: 0.516677

Epoch: 45
Loss: 0.04335227092906343
ROC train: 0.999795	val: 0.742284	test: 0.533163
PRC train: 0.998353	val: 0.576598	test: 0.529481

Epoch: 46
Loss: 0.04247944870929191
ROC train: 0.999894	val: 0.747928	test: 0.540272
PRC train: 0.998993	val: 0.555753	test: 0.536284

Epoch: 47
Loss: 0.04172497520209113
ROC train: 0.999873	val: 0.723466	test: 0.549815
PRC train: 0.998784	val: 0.538481	test: 0.532600

Epoch: 48
Loss: 0.04446263861712884
ROC train: 0.999794	val: 0.726587	test: 0.570176
PRC train: 0.998060	val: 0.558972	test: 0.536376

Epoch: 49
Loss: 0.045072645583185224
ROC train: 0.999926	val: 0.768281	test: 0.569138
PRC train: 0.999378	val: 0.584738	test: 0.534398

Epoch: 50
Loss: 0.03747699041864041
ROC train: 0.999966	val: 0.767933	test: 0.539428
PRC train: 0.999632	val: 0.644531	test: 0.523392

Epoch: 51
Loss: 0.03299437224766189
ROC train: 0.999947	val: 0.814533	test: 0.529253
PRC train: 0.999426	val: 0.666935	test: 0.521167

Epoch: 52
Loss: 0.04325072397244932
ROC train: 0.999981	val: 0.792693	test: 0.535961
PRC train: 0.999792	val: 0.610037	test: 0.536258

Epoch: 53
Loss: 0.033596060418928694
ROC train: 0.999990	val: 0.769019	test: 0.518947
PRC train: 0.999892	val: 0.660202	test: 0.522484

Epoch: 54
Loss: 0.03194332201200605
ROC train: 0.999957	val: 0.752587	test: 0.507017
PRC train: 0.999518	val: 0.645393	test: 0.523547

Epoch: 55
Loss: 0.027093165623559158
ROC train: 0.999966	val: 0.709220	test: 0.513762
PRC train: 0.999673	val: 0.610736	test: 0.513783

Epoch: 56
Loss: 0.02544630009652026
ROC train: 0.999986	val: 0.721793	test: 0.534706
PRC train: 0.999839	val: 0.614662	test: 0.520334

Epoch: 57
Loss: 0.02525276659492572
ROC train: 0.999990	val: 0.728062	test: 0.534187
PRC train: 0.999894	val: 0.617959	test: 0.519469

Epoch: 58
Loss: 0.02771146790453608
ROC train: 0.999986	val: 0.723793	test: 0.536260
PRC train: 0.999839	val: 0.621827	test: 0.518599

Epoch: 59
Loss: 0.02979388583206254
ROC train: 0.999971	val: 0.765111	test: 0.542207
PRC train: 0.999678	val: 0.693861	test: 0.521101

Epoch: 60
Loss: 0.026036495570336944
ROC train: 0.999976	val: 0.763013	test: 0.534262
PRC train: 0.999742	val: 0.664568	test: 0.520826

Epoch: 61
Loss: 0.025861278854601422
ROC train: 0.999986	val: 0.759042	test: 0.534262
PRC train: 0.999842	val: 0.662003	test: 0.530619

Epoch: 62
Loss: 0.025086799166053393
ROC train: 0.999990	val: 0.777260	test: 0.542801
PRC train: 0.999894	val: 0.594217	test: 0.532788

Epoch: 63
Loss: 0.030412722721881325
ROC train: 0.999981	val: 0.780032	test: 0.547772
PRC train: 0.999786	val: 0.612995	test: 0.530642

Epoch: 64
Loss: 0.027026963784391688
ROC train: 0.999981	val: 0.793231	test: 0.548340
PRC train: 0.999786	val: 0.603586	test: 0.531732

Epoch: 65
Loss: 0.030840172597510884
ROC train: 0.999995	val: 0.829992	test: 0.562818
PRC train: 0.999946	val: 0.608233	test: 0.534028

Epoch: 66
Loss: 0.022161935059504725
ROC train: 1.000000	val: 0.799064	test: 0.573460
PRC train: 1.000000	val: 0.702584	test: 0.529644

Epoch: 67
Loss: 0.02196456258010327
ROC train: 1.000000	val: 0.766548	test: 0.549139
PRC train: 1.000000	val: 0.709196	test: 0.519480

Epoch: 68
Loss: 0.022608091317423456
ROC train: 1.000000	val: 0.744708	test: 0.507065
PRC train: 1.000000	val: 0.667574	test: 0.511856

Epoch: 69
Loss: 0.02185071731147037
ROC train: 1.000000	val: 0.775176	test: 0.516934
PRC train: 1.000000	val: 0.637031	test: 0.523375

Epoch: 70
Loss: 0.020319536931044376
ROC train: 1.000000	val: 0.714227	test: 0.503898
PRC train: 1.000000	val: 0.567904	test: 0.520866

Epoch: 71
Loss: 0.025587695126238374
ROC train: 1.000000	val: 0.640491	test: 0.495161
PRC train: 1.000000	val: 0.531683	test: 0.511012

Epoch: 72
Loss: 0.021374963347019705
ROC train: 1.000000	val: 0.596449	test: 0.509979
PRC train: 1.000000	val: 0.520002	test: 0.518138

Epoch: 73
Loss: 0.01870005496136483
ROC train: 0.999995	val: 0.571088	test: 0.509467
PRC train: 0.999946	val: 0.515999	test: 0.516997

Epoch: 74
Loss: 0.02100212061011878
ROC train: 0.999918	val: 0.565958	test: 0.502108
PRC train: 0.999466	val: 0.518892	test: 0.511264

Epoch: 75
Loss: 0.02743453864089635
ROC train: 0.999966	val: 0.640604	test: 0.531602
PRC train: 0.999622	val: 0.528118	test: 0.517132

Epoch: 76
Loss: 0.021874894197906943
ROC train: 0.999990	val: 0.717959	test: 0.568578
PRC train: 0.999894	val: 0.556219	test: 0.529749

Epoch: 77
Loss: 0.02264240188971789
ROC train: 0.999995	val: 0.763811	test: 0.592424
PRC train: 0.999946	val: 0.644799	test: 0.528425

Epoch: 78
Loss: 0.02453354488907883
ROC train: 1.000000	val: 0.745320	test: 0.589798
PRC train: 1.000000	val: 0.629986	test: 0.526216

Epoch: 79
Loss: 0.017704342218961437
ROC train: 0.999995	val: 0.779009	test: 0.598569
PRC train: 0.999946	val: 0.702571	test: 0.527752

Epoch: 80
Loss: 0.018235697895159445
ROC train: 0.999986	val: 0.769269	test: 0.576997
PRC train: 0.999837	val: 0.625122	test: 0.525481

Epoch: 81
Loss: 0.017011795842394407
ROC train: 1.000000	val: 0.741985	test: 0.547541
PRC train: 1.000000	val: 0.603319	test: 0.520184

Epoch: 82
Loss: 0.012036923569247484
ROC train: 0.999990	val: 0.724292	test: 0.532282
PRC train: 0.999946	val: 0.635726	test: 0.518023

Epoch: 83
Loss: 0.01532433928807857
ROC train: 1.000000	val: 0.726214	test: 0.534426
PRC train: 1.000000	val: 0.573353	test: 0.518941

Epoch: 84
Loss: 0.016866299550992003
ROC train: 1.000000	val: 0.711490	test: 0.551414
PRC train: 1.000000	val: 0.557458	test: 0.526354

Epoch: 85
Loss: 0.014784571862817921
ROC train: 1.000000	val: 0.721892	test: 0.532532
PRC train: 1.000000	val: 0.552564	test: 0.522280

Epoch: 86
Loss: 0.013247651361065529
ROC train: 0.999995	val: 0.688005	test: 0.499445
PRC train: 0.999946	val: 0.590903	test: 0.515736

Epoch: 87
Loss: 0.01194221605843817
ROC train: 1.000000	val: 0.671173	test: 0.489214
PRC train: 1.000000	val: 0.564372	test: 0.513021

Epoch: 88
Loss: 0.012362640698090826
ROC train: 1.000000	val: 0.677653	test: 0.496610
PRC train: 1.000000	val: 0.561992	test: 0.514233

Epoch: 89
Loss: 0.013348699932526626
ROC train: 1.000000	val: 0.663118	test: 0.496573
PRC train: 1.000000	val: 0.623216	test: 0.512274

Epoch: 90
Loss: 0.010831546532051765
ROC train: 1.000000	val: 0.696720	test: 0.516486
PRC train: 1.000000	val: 0.589412	test: 0.515730

Epoch: 91
Loss: 0.011317705498476625
ROC train: 1.000000	val: 0.694770	test: 0.523407
PRC train: 1.000000	val: 0.541620	test: 0.516815

Epoch: 92
Loss: 0.014430972176127757
ROC train: 1.000000	val: 0.678549	test: 0.513150
PRC train: 1.000000	val: 0.531880	test: 0.516125

Epoch: 93
Loss: 0.016468066232200183
PRC train: 0.993104	val: 0.601439	test: 0.571620

Epoch: 33
Loss: 0.07642550932410944
ROC train: 0.998260	val: 0.742372	test: 0.571794
PRC train: 0.992185	val: 0.557747	test: 0.578451

Epoch: 34
Loss: 0.08436924734043434
ROC train: 0.997288	val: 0.776574	test: 0.578100
PRC train: 0.988761	val: 0.567611	test: 0.573516

Epoch: 35
Loss: 0.06928704553096157
ROC train: 0.997015	val: 0.769567	test: 0.611381
PRC train: 0.989192	val: 0.570636	test: 0.582044

Epoch: 36
Loss: 0.07405047171232157
ROC train: 0.998415	val: 0.715176	test: 0.578443
PRC train: 0.991975	val: 0.561452	test: 0.574926

Epoch: 37
Loss: 0.06241651615879271
ROC train: 0.997817	val: 0.716325	test: 0.551773
PRC train: 0.992146	val: 0.562695	test: 0.568364

Epoch: 38
Loss: 0.05800152955896114
ROC train: 0.999105	val: 0.731359	test: 0.558075
PRC train: 0.993816	val: 0.578417	test: 0.573564

Epoch: 39
Loss: 0.06235472192579981
ROC train: 0.996591	val: 0.705449	test: 0.597751
PRC train: 0.989283	val: 0.537634	test: 0.576910

Epoch: 40
Loss: 0.057360747328450576
ROC train: 0.998627	val: 0.546855	test: 0.584558
PRC train: 0.995059	val: 0.516301	test: 0.569368

Epoch: 41
Loss: 0.0582346939552703
ROC train: 0.999607	val: 0.629766	test: 0.591128
PRC train: 0.996892	val: 0.523341	test: 0.571388

Epoch: 42
Loss: 0.05985016688628668
ROC train: 0.998384	val: 0.736542	test: 0.610216
PRC train: 0.994453	val: 0.556331	test: 0.577887

Epoch: 43
Loss: 0.04681047774498057
ROC train: 0.997809	val: 0.754186	test: 0.635537
PRC train: 0.994269	val: 0.580229	test: 0.583179

Epoch: 44
Loss: 0.0526798477614208
ROC train: 0.999243	val: 0.768520	test: 0.648577
PRC train: 0.996249	val: 0.585849	test: 0.567069

Epoch: 45
Loss: 0.0442988853016434
ROC train: 0.999735	val: 0.742024	test: 0.618359
PRC train: 0.998129	val: 0.566129	test: 0.559330

Epoch: 46
Loss: 0.043656382415911646
ROC train: 0.999692	val: 0.744122	test: 0.588861
PRC train: 0.997653	val: 0.587934	test: 0.555487

Epoch: 47
Loss: 0.042522604602478455
ROC train: 0.999772	val: 0.710857	test: 0.567973
PRC train: 0.998499	val: 0.586635	test: 0.570559

Epoch: 48
Loss: 0.04336892306846321
ROC train: 0.999994	val: 0.711170	test: 0.575175
PRC train: 1.000000	val: 0.565732	test: 0.570087

Epoch: 49
Loss: 0.03822235242515898
ROC train: 0.999974	val: 0.677755	test: 0.573251
PRC train: 0.999839	val: 0.534166	test: 0.569455

Epoch: 50
Loss: 0.040621340865197204
ROC train: 0.999902	val: 0.652256	test: 0.569236
PRC train: 0.999461	val: 0.530572	test: 0.568910

Epoch: 51
Loss: 0.027584637709990107
ROC train: 0.999885	val: 0.589010	test: 0.559449
PRC train: 0.999440	val: 0.523509	test: 0.566109

Epoch: 52
Loss: 0.04126187038636846
ROC train: 0.999941	val: 0.564935	test: 0.582347
PRC train: 0.999789	val: 0.515873	test: 0.569574

Epoch: 53
Loss: 0.03603586735639416
ROC train: 0.999955	val: 0.758294	test: 0.658509
PRC train: 0.999997	val: 0.549703	test: 0.582737

Epoch: 54
Loss: 0.031077944921582577
ROC train: 0.999876	val: 0.781195	test: 0.682587
PRC train: 0.999938	val: 0.625946	test: 0.553224

Epoch: 55
Loss: 0.03568140503867962
ROC train: 0.999901	val: 0.796404	test: 0.683218
PRC train: 0.999781	val: 0.631713	test: 0.554163

Epoch: 56
Loss: 0.02526610005407227
ROC train: 0.999995	val: 0.774726	test: 0.657385
PRC train: 0.999946	val: 0.553908	test: 0.584419

Epoch: 57
Loss: 0.03681687360872781
ROC train: 0.999832	val: 0.706612	test: 0.610914
PRC train: 0.998608	val: 0.543217	test: 0.575113

Epoch: 58
Loss: 0.0393735131900346
ROC train: 0.999732	val: 0.676869	test: 0.589653
PRC train: 0.998544	val: 0.546328	test: 0.571293

Epoch: 59
Loss: 0.024548488833877547
ROC train: 0.999950	val: 0.709283	test: 0.597475
PRC train: 0.999943	val: 0.545658	test: 0.585227

Epoch: 60
Loss: 0.031575286974353314
ROC train: 0.999967	val: 0.685647	test: 0.599223
PRC train: 0.999944	val: 0.539033	test: 0.577224

Epoch: 61
Loss: 0.028738403206760438
ROC train: 0.999986	val: 0.669651	test: 0.632938
PRC train: 0.999837	val: 0.544678	test: 0.537954

Epoch: 62
Loss: 0.023000866869687546
ROC train: 1.000000	val: 0.700044	test: 0.667414
PRC train: 1.000000	val: 0.530801	test: 0.549386

Epoch: 63
Loss: 0.024878993120282543
ROC train: 0.999989	val: 0.711982	test: 0.662592
PRC train: 0.999999	val: 0.532133	test: 0.582946

Epoch: 64
Loss: 0.02412112266148041
ROC train: 1.000000	val: 0.708910	test: 0.654072
PRC train: 1.000000	val: 0.542108	test: 0.580166

Epoch: 65
Loss: 0.03136929508155908
ROC train: 1.000000	val: 0.641619	test: 0.616790
PRC train: 1.000000	val: 0.531852	test: 0.573414

Epoch: 66
Loss: 0.026663317430373396
ROC train: 1.000000	val: 0.614160	test: 0.612976
PRC train: 1.000000	val: 0.524082	test: 0.573213

Epoch: 67
Loss: 0.025726989993291927
ROC train: 1.000000	val: 0.642754	test: 0.626744
PRC train: 1.000000	val: 0.528352	test: 0.575758

Epoch: 68
Loss: 0.024768411978039664
ROC train: 1.000000	val: 0.734570	test: 0.648954
PRC train: 1.000000	val: 0.552242	test: 0.581809

Epoch: 69
Loss: 0.026137502003085546
ROC train: 1.000000	val: 0.777288	test: 0.674683
PRC train: 1.000000	val: 0.586667	test: 0.558112

Epoch: 70
Loss: 0.01856040310660068
ROC train: 1.000000	val: 0.753125	test: 0.655966
PRC train: 1.000000	val: 0.564333	test: 0.583194

Epoch: 71
Loss: 0.022727690869403842
ROC train: 1.000000	val: 0.712579	test: 0.634424
PRC train: 1.000000	val: 0.547934	test: 0.584104

Epoch: 72
Loss: 0.0203767712002833
ROC train: 0.999989	val: 0.628642	test: 0.628567
PRC train: 0.999999	val: 0.531227	test: 0.585794

Epoch: 73
Loss: 0.027623676569605765
ROC train: 1.000000	val: 0.607290	test: 0.594740
PRC train: 1.000000	val: 0.525695	test: 0.573938

Epoch: 74
Loss: 0.02713260441796076
ROC train: 1.000000	val: 0.619041	test: 0.584984
PRC train: 1.000000	val: 0.524221	test: 0.571652

Epoch: 75
Loss: 0.022399309194762875
ROC train: 1.000000	val: 0.693213	test: 0.594490
PRC train: 1.000000	val: 0.547377	test: 0.577679

Epoch: 76
Loss: 0.01748770081993629
ROC train: 1.000000	val: 0.769307	test: 0.641981
PRC train: 1.000000	val: 0.571295	test: 0.586007

Epoch: 77
Loss: 0.02051585361319961
ROC train: 0.999978	val: 0.791122	test: 0.654221
PRC train: 0.999945	val: 0.643413	test: 0.585226

Epoch: 78
Loss: 0.02226155369479237
ROC train: 0.999990	val: 0.758083	test: 0.635410
PRC train: 0.999946	val: 0.625369	test: 0.577706

Epoch: 79
Loss: 0.019212554891937513
ROC train: 0.999989	val: 0.698196	test: 0.618149
PRC train: 0.999999	val: 0.550622	test: 0.574870

Epoch: 80
Loss: 0.01746543630575296
ROC train: 0.999994	val: 0.640421	test: 0.618187
PRC train: 1.000000	val: 0.522398	test: 0.574714

Epoch: 81
Loss: 0.02013223740905312
ROC train: 1.000000	val: 0.697085	test: 0.605752
PRC train: 1.000000	val: 0.538439	test: 0.574473

Epoch: 82
Loss: 0.015810339949196665
ROC train: 1.000000	val: 0.760817	test: 0.607746
PRC train: 1.000000	val: 0.576547	test: 0.586080

Epoch: 83
Loss: 0.015328551348497903
ROC train: 1.000000	val: 0.772603	test: 0.597572
PRC train: 1.000000	val: 0.589694	test: 0.596991

Epoch: 84
Loss: 0.021282363703652885
ROC train: 1.000000	val: 0.766461	test: 0.597277
PRC train: 1.000000	val: 0.606239	test: 0.589054

Epoch: 85
Loss: 0.00835207736089403
ROC train: 1.000000	val: 0.765150	test: 0.627405
PRC train: 1.000000	val: 0.603541	test: 0.594001

Epoch: 86
Loss: 0.016526015653682873
ROC train: 1.000000	val: 0.750591	test: 0.642956
PRC train: 1.000000	val: 0.605750	test: 0.585714

Epoch: 87
Loss: 0.016214885490544752
ROC train: 1.000000	val: 0.716726	test: 0.614388
PRC train: 1.000000	val: 0.552278	test: 0.576638

Epoch: 88
Loss: 0.0132604120004037
ROC train: 0.999995	val: 0.708858	test: 0.624193
PRC train: 0.999946	val: 0.546483	test: 0.577780

Epoch: 89
Loss: 0.014926994030776384
ROC train: 0.999990	val: 0.765898	test: 0.648577
PRC train: 0.999894	val: 0.569990	test: 0.591733

Epoch: 90
Loss: 0.01391531637602352
ROC train: 1.000000	val: 0.767272	test: 0.643755
PRC train: 1.000000	val: 0.571963	test: 0.590718

Epoch: 91
Loss: 0.00866734965921918
ROC train: 1.000000	val: 0.720047	test: 0.645204
PRC train: 1.000000	val: 0.556440	test: 0.585486

Epoch: 92
Loss: 0.009391998237067763
ROC train: 1.000000	val: 0.721245	test: 0.657759
PRC train: 1.000000	val: 0.566644	test: 0.587437

Epoch: 93
Loss: 0.017537931132356292
PRC train: 0.981228	val: 0.659537	test: 0.528115

Epoch: 33
Loss: 0.08863683068084219
ROC train: 0.997640	val: 0.861060	test: 0.553009
PRC train: 0.987036	val: 0.597650	test: 0.521139

Epoch: 34
Loss: 0.09649828339786938
ROC train: 0.996765	val: 0.844090	test: 0.553435
PRC train: 0.985478	val: 0.602482	test: 0.520210

Epoch: 35
Loss: 0.07790271748077518
ROC train: 0.994985	val: 0.859587	test: 0.556494
PRC train: 0.979921	val: 0.649156	test: 0.518362

Epoch: 36
Loss: 0.09713519172598742
ROC train: 0.994878	val: 0.819492	test: 0.560099
PRC train: 0.980778	val: 0.646487	test: 0.520969

Epoch: 37
Loss: 0.08522456205123816
ROC train: 0.996235	val: 0.910531	test: 0.547014
PRC train: 0.984777	val: 0.700824	test: 0.519328

Epoch: 38
Loss: 0.07149286311706218
ROC train: 0.997507	val: 0.884647	test: 0.552337
PRC train: 0.988831	val: 0.711226	test: 0.520215

Epoch: 39
Loss: 0.07069197526600182
ROC train: 0.997067	val: 0.815883	test: 0.578193
PRC train: 0.986561	val: 0.629835	test: 0.522166

Epoch: 40
Loss: 0.07000850523761205
ROC train: 0.997287	val: 0.881301	test: 0.564876
PRC train: 0.986431	val: 0.667120	test: 0.522645

Epoch: 41
Loss: 0.06677985955858734
ROC train: 0.998191	val: 0.865843	test: 0.522018
PRC train: 0.989230	val: 0.643963	test: 0.514060

Epoch: 42
Loss: 0.0642246694138659
ROC train: 0.999175	val: 0.846350	test: 0.513161
PRC train: 0.993331	val: 0.640053	test: 0.512421

Epoch: 43
Loss: 0.06352329710913152
ROC train: 0.998869	val: 0.905837	test: 0.545329
PRC train: 0.992516	val: 0.677304	test: 0.519223

Epoch: 44
Loss: 0.05126792126143752
ROC train: 0.998304	val: 0.870460	test: 0.544567
PRC train: 0.993076	val: 0.670579	test: 0.520937

Epoch: 45
Loss: 0.04754941247834181
ROC train: 0.999285	val: 0.861696	test: 0.531012
PRC train: 0.995448	val: 0.627664	test: 0.516044

Epoch: 46
Loss: 0.05151399024332034
ROC train: 0.999568	val: 0.816793	test: 0.520882
PRC train: 0.996921	val: 0.609740	test: 0.514247

Epoch: 47
Loss: 0.05658112972584302
ROC train: 0.999249	val: 0.805492	test: 0.520083
PRC train: 0.994524	val: 0.614216	test: 0.515410

Epoch: 48
Loss: 0.05519042995135248
ROC train: 0.996723	val: 0.779544	test: 0.520531
PRC train: 0.988615	val: 0.590979	test: 0.514189

Epoch: 49
Loss: 0.048022122782842944
ROC train: 0.998729	val: 0.874108	test: 0.553435
PRC train: 0.992955	val: 0.709266	test: 0.519309

Epoch: 50
Loss: 0.059620625271503545
ROC train: 0.999595	val: 0.856889	test: 0.562941
PRC train: 0.996854	val: 0.673763	test: 0.521369

Epoch: 51
Loss: 0.049149329730910546
ROC train: 0.999477	val: 0.797676	test: 0.537512
PRC train: 0.997077	val: 0.630319	test: 0.517638

Epoch: 52
Loss: 0.051538347580337526
ROC train: 0.998869	val: 0.874856	test: 0.554959
PRC train: 0.993824	val: 0.766006	test: 0.522679

Epoch: 53
Loss: 0.053748068239595034
ROC train: 0.999403	val: 0.833826	test: 0.559594
PRC train: 0.995379	val: 0.658298	test: 0.521503

Epoch: 54
Loss: 0.05220576558435126
ROC train: 0.999606	val: 0.837646	test: 0.546192
PRC train: 0.997523	val: 0.687892	test: 0.519163

Epoch: 55
Loss: 0.037722276474046454
ROC train: 0.999717	val: 0.852605	test: 0.558971
PRC train: 0.997832	val: 0.730117	test: 0.521723

Epoch: 56
Loss: 0.0464689969570168
ROC train: 0.999640	val: 0.834687	test: 0.570636
PRC train: 0.997151	val: 0.738965	test: 0.523088

Epoch: 57
Loss: 0.048482833816367886
ROC train: 0.999496	val: 0.799212	test: 0.585574
PRC train: 0.996067	val: 0.689655	test: 0.524908

Epoch: 58
Loss: 0.04763757597317816
ROC train: 0.998995	val: 0.732371	test: 0.586523
PRC train: 0.994685	val: 0.612034	test: 0.524249

Epoch: 59
Loss: 0.05129605707531697
ROC train: 0.999308	val: 0.752099	test: 0.571010
PRC train: 0.996622	val: 0.577562	test: 0.523590

Epoch: 60
Loss: 0.043197354943932995
ROC train: 0.999212	val: 0.730821	test: 0.554496
PRC train: 0.994279	val: 0.618963	test: 0.522143

Epoch: 61
Loss: 0.039090769127434595
ROC train: 0.999553	val: 0.733955	test: 0.551123
PRC train: 0.996283	val: 0.583408	test: 0.519874

Epoch: 62
Loss: 0.030089370042421217
ROC train: 0.999955	val: 0.744431	test: 0.562897
PRC train: 0.999634	val: 0.614946	test: 0.520206

Epoch: 63
Loss: 0.033409160449165864
ROC train: 0.999981	val: 0.738313	test: 0.586519
PRC train: 0.999786	val: 0.611545	test: 0.523598

Epoch: 64
Loss: 0.03658339039776952
ROC train: 0.999947	val: 0.730659	test: 0.588099
PRC train: 0.999440	val: 0.608585	test: 0.522751

Epoch: 65
Loss: 0.026253183725337054
ROC train: 1.000000	val: 0.772990	test: 0.570378
PRC train: 1.000000	val: 0.617888	test: 0.519874

Epoch: 66
Loss: 0.03496008575365188
ROC train: 1.000000	val: 0.787661	test: 0.556494
PRC train: 1.000000	val: 0.604328	test: 0.518361

Epoch: 67
Loss: 0.038051915709373364
ROC train: 0.999986	val: 0.851182	test: 0.577389
PRC train: 0.999842	val: 0.609883	test: 0.522167

Epoch: 68
Loss: 0.025627907604562194
ROC train: 0.999832	val: 0.755121	test: 0.568195
PRC train: 0.998390	val: 0.571304	test: 0.519708

Epoch: 69
Loss: 0.028996438297378507
ROC train: 0.999654	val: 0.798014	test: 0.558044
PRC train: 0.997660	val: 0.593709	test: 0.518510

Epoch: 70
Loss: 0.033666199779952985
ROC train: 0.999909	val: 0.803383	test: 0.550888
PRC train: 0.999044	val: 0.590387	test: 0.517405

Epoch: 71
Loss: 0.023535089966817597
ROC train: 0.999995	val: 0.806542	test: 0.547403
PRC train: 0.999946	val: 0.621455	test: 0.516541

Epoch: 72
Loss: 0.024968907730185196
ROC train: 1.000000	val: 0.819815	test: 0.526354
PRC train: 1.000000	val: 0.628453	test: 0.514135

Epoch: 73
Loss: 0.028606024317513266
ROC train: 0.999989	val: 0.845900	test: 0.517248
PRC train: 0.999999	val: 0.591912	test: 0.512767

Epoch: 74
Loss: 0.02825840074336642
ROC train: 0.999697	val: 0.815830	test: 0.528479
PRC train: 0.998079	val: 0.561334	test: 0.516613

Epoch: 75
Loss: 0.027878606618803627
ROC train: 0.999800	val: 0.738787	test: 0.525368
PRC train: 0.998524	val: 0.575324	test: 0.515384

Epoch: 76
Loss: 0.026771471343633325
ROC train: 0.999976	val: 0.746817	test: 0.525562
PRC train: 0.999733	val: 0.576620	test: 0.516426

Epoch: 77
Loss: 0.028424524425465215
ROC train: 1.000000	val: 0.784565	test: 0.515985
PRC train: 1.000000	val: 0.581455	test: 0.513906

Epoch: 78
Loss: 0.03158663963742244
ROC train: 0.999977	val: 0.826471	test: 0.528289
PRC train: 0.999998	val: 0.637332	test: 0.516823

Epoch: 79
Loss: 0.023846534866603555
ROC train: 0.999989	val: 0.842379	test: 0.556408
PRC train: 0.999999	val: 0.642980	test: 0.519455

Epoch: 80
Loss: 0.02424812958034154
ROC train: 1.000000	val: 0.844951	test: 0.569549
PRC train: 1.000000	val: 0.624031	test: 0.520883

Epoch: 81
Loss: 0.016975563390956426
ROC train: 1.000000	val: 0.863794	test: 0.581465
PRC train: 1.000000	val: 0.632949	test: 0.522897

Epoch: 82
Loss: 0.026027370575258606
ROC train: 1.000000	val: 0.885409	test: 0.582563
PRC train: 1.000000	val: 0.638313	test: 0.523706

Epoch: 83
Loss: 0.022296171777005817
ROC train: 0.999962	val: 0.868914	test: 0.581887
PRC train: 0.999581	val: 0.601055	test: 0.523330

Epoch: 84
Loss: 0.017977753668286595
ROC train: 0.999909	val: 0.847362	test: 0.549065
PRC train: 0.999019	val: 0.651134	test: 0.518989

Epoch: 85
Loss: 0.029665781351334138
ROC train: 0.999990	val: 0.844115	test: 0.538192
PRC train: 0.999892	val: 0.651516	test: 0.515571

Epoch: 86
Loss: 0.03024248775435323
ROC train: 0.999981	val: 0.828569	test: 0.515306
PRC train: 0.999786	val: 0.643398	test: 0.512594

Epoch: 87
Loss: 0.028157623952877363
ROC train: 0.999440	val: 0.844653	test: 0.535681
PRC train: 0.996687	val: 0.645859	test: 0.516161

Epoch: 88
Loss: 0.02266000933756489
ROC train: 0.999574	val: 0.851832	test: 0.556943
PRC train: 0.997808	val: 0.642586	test: 0.519420

Epoch: 89
Loss: 0.030302041380894013
ROC train: 0.999989	val: 0.836922	test: 0.568175
PRC train: 0.999999	val: 0.666312	test: 0.520986

Epoch: 90
Loss: 0.0240144511156811
ROC train: 0.999989	val: 0.775675	test: 0.567125
PRC train: 0.999999	val: 0.614828	test: 0.520709

Epoch: 91
Loss: 0.02149936857687011
ROC train: 0.999960	val: 0.831229	test: 0.577218
PRC train: 0.999682	val: 0.588836	test: 0.521596

Epoch: 92
Loss: 0.028675884711230597
ROC train: 0.999978	val: 0.872959	test: 0.585588
PRC train: 0.999945	val: 0.621455	test: 0.524982

Epoch: 93
Loss: 0.023675862333918836
PRC train: 0.979267	val: 0.531419	test: 0.551219

Epoch: 33
Loss: 0.089992722409346
ROC train: 0.997167	val: 0.782380	test: 0.722453
PRC train: 0.984970	val: 0.551192	test: 0.550670

Epoch: 34
Loss: 0.09357335205727385
ROC train: 0.995478	val: 0.771078	test: 0.698824
PRC train: 0.979612	val: 0.552424	test: 0.544824

Epoch: 35
Loss: 0.09357834514747512
ROC train: 0.998661	val: 0.774375	test: 0.737163
PRC train: 0.992086	val: 0.560288	test: 0.558462

Epoch: 36
Loss: 0.08350273325144034
ROC train: 0.998089	val: 0.749551	test: 0.744185
PRC train: 0.989235	val: 0.537422	test: 0.563886

Epoch: 37
Loss: 0.07845252057100621
ROC train: 0.997839	val: 0.743545	test: 0.729868
PRC train: 0.988241	val: 0.533848	test: 0.552200

Epoch: 38
Loss: 0.08279325594829692
ROC train: 0.997697	val: 0.732044	test: 0.754853
PRC train: 0.986767	val: 0.536926	test: 0.577954

Epoch: 39
Loss: 0.08354271898389212
ROC train: 0.997845	val: 0.737227	test: 0.706492
PRC train: 0.987710	val: 0.534074	test: 0.558529

Epoch: 40
Loss: 0.07376248188208791
ROC train: 0.999261	val: 0.817243	test: 0.730794
PRC train: 0.994582	val: 0.578342	test: 0.560594

Epoch: 41
Loss: 0.07936510491949021
ROC train: 0.999550	val: 0.810563	test: 0.732329
PRC train: 0.997067	val: 0.578857	test: 0.556273

Epoch: 42
Loss: 0.0742518275254708
ROC train: 0.999282	val: 0.791970	test: 0.734272
PRC train: 0.996025	val: 0.572041	test: 0.554819

Epoch: 43
Loss: 0.07034772761747927
ROC train: 0.998503	val: 0.769680	test: 0.727458
PRC train: 0.990269	val: 0.625395	test: 0.559101

Epoch: 44
Loss: 0.07252896425176936
ROC train: 0.999194	val: 0.768281	test: 0.692306
PRC train: 0.994948	val: 0.544472	test: 0.550956

Epoch: 45
Loss: 0.06900925558268126
ROC train: 0.999579	val: 0.754035	test: 0.681686
PRC train: 0.996875	val: 0.536216	test: 0.547830

Epoch: 46
Loss: 0.06634208226023329
ROC train: 0.999761	val: 0.791882	test: 0.699212
PRC train: 0.998072	val: 0.560703	test: 0.545378

Epoch: 47
Loss: 0.06338960753788389
ROC train: 0.998798	val: 0.798151	test: 0.660638
PRC train: 0.993359	val: 0.544553	test: 0.535593

Epoch: 48
Loss: 0.05919002727543706
ROC train: 0.999313	val: 0.778483	test: 0.714198
PRC train: 0.994940	val: 0.550612	test: 0.549619

Epoch: 49
Loss: 0.05649112056062901
ROC train: 0.999055	val: 0.770453	test: 0.719764
PRC train: 0.994041	val: 0.553092	test: 0.551141

Epoch: 50
Loss: 0.054785049053267934
ROC train: 0.999757	val: 0.759278	test: 0.712394
PRC train: 0.999249	val: 0.544852	test: 0.553819

Epoch: 51
Loss: 0.06042197334338364
ROC train: 0.999642	val: 0.820128	test: 0.698958
PRC train: 0.998050	val: 0.551033	test: 0.549217

Epoch: 52
Loss: 0.05836232567352995
ROC train: 0.999790	val: 0.719583	test: 0.714254
PRC train: 0.998275	val: 0.614719	test: 0.551126

Epoch: 53
Loss: 0.05366485257662988
ROC train: 0.999958	val: 0.709244	test: 0.686284
PRC train: 0.999780	val: 0.606189	test: 0.540562

Epoch: 54
Loss: 0.045438999466436994
ROC train: 0.999893	val: 0.711704	test: 0.684667
PRC train: 0.999243	val: 0.606744	test: 0.540926

Epoch: 55
Loss: 0.050919837134169234
ROC train: 0.999835	val: 0.742259	test: 0.710807
PRC train: 0.998410	val: 0.614442	test: 0.550326

Epoch: 56
Loss: 0.04555405387929987
ROC train: 0.999875	val: 0.733056	test: 0.729420
PRC train: 0.998694	val: 0.611391	test: 0.554027

Epoch: 57
Loss: 0.03686846642110862
ROC train: 0.999844	val: 0.746754	test: 0.733861
PRC train: 0.998741	val: 0.614421	test: 0.554400

Epoch: 58
Loss: 0.04460517092851306
ROC train: 0.999990	val: 0.735540	test: 0.760781
PRC train: 0.999894	val: 0.615013	test: 0.566108

Epoch: 59
Loss: 0.036754270859342755
ROC train: 0.999990	val: 0.747091	test: 0.726278
PRC train: 0.999892	val: 0.617456	test: 0.555769

Epoch: 60
Loss: 0.03615143604207377
ROC train: 0.999712	val: 0.736503	test: 0.700475
PRC train: 0.997642	val: 0.554481	test: 0.544569

Epoch: 61
Loss: 0.0382320634405688
ROC train: 0.999990	val: 0.754559	test: 0.725759
PRC train: 0.999946	val: 0.613473	test: 0.552763

Epoch: 62
Loss: 0.04362102668603325
ROC train: 0.999971	val: 0.758368	test: 0.734216
PRC train: 0.999678	val: 0.612750	test: 0.555319

Epoch: 63
Loss: 0.031040856718133114
ROC train: 0.999784	val: 0.767009	test: 0.727358
PRC train: 0.998012	val: 0.613845	test: 0.553115

Epoch: 64
Loss: 0.03148286599466231
ROC train: 0.999990	val: 0.766872	test: 0.722898
PRC train: 0.999892	val: 0.614900	test: 0.553231

Epoch: 65
Loss: 0.04542663481244956
ROC train: 1.000000	val: 0.776736	test: 0.732856
PRC train: 1.000000	val: 0.623848	test: 0.560929

Epoch: 66
Loss: 0.035815883064349366
ROC train: 0.999968	val: 0.771366	test: 0.739001
PRC train: 0.999890	val: 0.620990	test: 0.560038

Epoch: 67
Loss: 0.039514216516622694
ROC train: 0.999950	val: 0.809252	test: 0.723260
PRC train: 0.999563	val: 0.626027	test: 0.554427

Epoch: 68
Loss: 0.03145569030126226
ROC train: 0.999974	val: 0.820528	test: 0.702449
PRC train: 0.999839	val: 0.621424	test: 0.549233

Epoch: 69
Loss: 0.029362640343293788
ROC train: 1.000000	val: 0.803882	test: 0.695398
PRC train: 1.000000	val: 0.621555	test: 0.548226

Epoch: 70
Loss: 0.02897511394344157
ROC train: 0.999995	val: 0.782629	test: 0.718064
PRC train: 0.999946	val: 0.624161	test: 0.552702

Epoch: 71
Loss: 0.024514259858325546
ROC train: 0.999966	val: 0.820602	test: 0.697251
PRC train: 0.999673	val: 0.629046	test: 0.544084

Epoch: 72
Loss: 0.022253593902118386
ROC train: 1.000000	val: 0.840980	test: 0.696127
PRC train: 1.000000	val: 0.643077	test: 0.544348

Epoch: 73
Loss: 0.027957249394650514
ROC train: 0.999995	val: 0.815057	test: 0.728183
PRC train: 0.999946	val: 0.650455	test: 0.558060

Epoch: 74
Loss: 0.017075974658644584
ROC train: 0.999955	val: 0.783416	test: 0.709357
PRC train: 0.999616	val: 0.627386	test: 0.550215

Epoch: 75
Loss: 0.022698376492563434
ROC train: 0.999980	val: 0.796527	test: 0.700781
PRC train: 0.999837	val: 0.628071	test: 0.548041

Epoch: 76
Loss: 0.03517109206619391
ROC train: 1.000000	val: 0.755145	test: 0.732355
PRC train: 1.000000	val: 0.620190	test: 0.556905

Epoch: 77
Loss: 0.042747448378224584
ROC train: 0.999983	val: 0.720170	test: 0.752429
PRC train: 0.999999	val: 0.611217	test: 0.560975

Epoch: 78
Loss: 0.03457800160363844
ROC train: 0.999983	val: 0.737452	test: 0.734466
PRC train: 0.999999	val: 0.611466	test: 0.553338

Epoch: 79
Loss: 0.033311877644527144
ROC train: 0.999972	val: 0.746680	test: 0.714504
PRC train: 0.999998	val: 0.567073	test: 0.552056

Epoch: 80
Loss: 0.03612682155940718
ROC train: 0.999886	val: 0.747341	test: 0.732225
PRC train: 0.999079	val: 0.555933	test: 0.561095

Epoch: 81
Loss: 0.03147019352508146
ROC train: 1.000000	val: 0.737452	test: 0.724874
PRC train: 1.000000	val: 0.571723	test: 0.559019

Epoch: 82
Loss: 0.02832760845538245
ROC train: 1.000000	val: 0.772990	test: 0.706485
PRC train: 1.000000	val: 0.620112	test: 0.551180

Epoch: 83
Loss: 0.02226736843493015
ROC train: 1.000000	val: 0.784165	test: 0.702600
PRC train: 1.000000	val: 0.600776	test: 0.553241

Epoch: 84
Loss: 0.020750709284645133
ROC train: 0.999986	val: 0.773426	test: 0.696104
PRC train: 0.999839	val: 0.606395	test: 0.551664

Epoch: 85
Loss: 0.0205123986027084
ROC train: 0.999774	val: 0.780756	test: 0.735028
PRC train: 0.998274	val: 0.624291	test: 0.556077

Epoch: 86
Loss: 0.021101980264145392
ROC train: 0.999981	val: 0.785725	test: 0.777474
PRC train: 0.999786	val: 0.641827	test: 0.564971

Epoch: 87
Loss: 0.023046104867160057
ROC train: 1.000000	val: 0.757792	test: 0.779174
PRC train: 1.000000	val: 0.638044	test: 0.565839

Epoch: 88
Loss: 0.02138887196251691
ROC train: 1.000000	val: 0.777583	test: 0.757782
PRC train: 1.000000	val: 0.645088	test: 0.559887

Epoch: 89
Loss: 0.016369888712728403
ROC train: 1.000000	val: 0.796426	test: 0.746632
PRC train: 1.000000	val: 0.645701	test: 0.555879

Epoch: 90
Loss: 0.017750516315419556
ROC train: 1.000000	val: 0.814657	test: 0.763041
PRC train: 1.000000	val: 0.655431	test: 0.568470

Epoch: 91
Loss: 0.025991295779840495
ROC train: 1.000000	val: 0.813395	test: 0.761823
PRC train: 1.000000	val: 0.658105	test: 0.570367

Epoch: 92
Loss: 0.02244936411535062
ROC train: 1.000000	val: 0.824696	test: 0.771415
PRC train: 1.000000	val: 0.659128	test: 0.564836

Epoch: 93
Loss: 0.03664135216827674
PRC train: 0.993711	val: 0.579787	test: 0.532931

Epoch: 33
Loss: 0.08048890843653271
ROC train: 0.997826	val: 0.772126	test: 0.676083
PRC train: 0.986373	val: 0.565125	test: 0.544795

Epoch: 34
Loss: 0.0851092491864043
ROC train: 0.999306	val: 0.832726	test: 0.638888
PRC train: 0.996589	val: 0.555187	test: 0.533273

Epoch: 35
Loss: 0.07151182172474128
ROC train: 0.999465	val: 0.785563	test: 0.628645
PRC train: 0.997358	val: 0.544353	test: 0.531645

Epoch: 36
Loss: 0.07048849116658489
ROC train: 0.999239	val: 0.722953	test: 0.655166
PRC train: 0.996793	val: 0.542054	test: 0.536674

Epoch: 37
Loss: 0.0657901072734014
ROC train: 0.999510	val: 0.813258	test: 0.656865
PRC train: 0.997476	val: 0.559767	test: 0.536472

Epoch: 38
Loss: 0.05957485992746522
ROC train: 0.999705	val: 0.845612	test: 0.660388
PRC train: 0.998127	val: 0.562646	test: 0.537135

Epoch: 39
Loss: 0.0610348397477131
ROC train: 0.999548	val: 0.800597	test: 0.675751
PRC train: 0.997502	val: 0.550909	test: 0.541333

Epoch: 40
Loss: 0.05060700499630895
ROC train: 0.999264	val: 0.828330	test: 0.666181
PRC train: 0.996825	val: 0.557981	test: 0.537902

Epoch: 41
Loss: 0.05336259238698038
ROC train: 0.999346	val: 0.858498	test: 0.662760
PRC train: 0.996971	val: 0.569286	test: 0.536993

Epoch: 42
Loss: 0.05435842195147313
ROC train: 0.999272	val: 0.876617	test: 0.649081
PRC train: 0.995119	val: 0.579110	test: 0.535588

Epoch: 43
Loss: 0.05845958841357385
ROC train: 0.999598	val: 0.871560	test: 0.645940
PRC train: 0.998300	val: 0.571285	test: 0.535402

Epoch: 44
Loss: 0.04857822970109091
ROC train: 0.999934	val: 0.820799	test: 0.683461
PRC train: 0.999552	val: 0.565069	test: 0.543709

Epoch: 45
Loss: 0.055839523783417944
ROC train: 0.999799	val: 0.786410	test: 0.677391
PRC train: 0.998435	val: 0.568984	test: 0.541750

Epoch: 46
Loss: 0.04419859890166481
ROC train: 0.999865	val: 0.809111	test: 0.673516
PRC train: 0.998914	val: 0.571609	test: 0.542003

Epoch: 47
Loss: 0.05268465397134745
ROC train: 0.999920	val: 0.841803	test: 0.662752
PRC train: 0.999357	val: 0.567062	test: 0.539013

Epoch: 48
Loss: 0.055972700565332566
ROC train: 0.999947	val: 0.846536	test: 0.651296
PRC train: 0.999420	val: 0.560582	test: 0.535634

Epoch: 49
Loss: 0.044939967963737885
ROC train: 0.999684	val: 0.817977	test: 0.697337
PRC train: 0.998155	val: 0.559392	test: 0.546183

Epoch: 50
Loss: 0.05308338982575889
ROC train: 0.999888	val: 0.811571	test: 0.694002
PRC train: 0.999586	val: 0.560594	test: 0.544617

Epoch: 51
Loss: 0.04084649730618909
ROC train: 0.999930	val: 0.825332	test: 0.681884
PRC train: 0.999778	val: 0.560746	test: 0.541731

Epoch: 52
Loss: 0.040277730635091005
ROC train: 0.999963	val: 0.858611	test: 0.678650
PRC train: 0.999836	val: 0.569191	test: 0.540764

Epoch: 53
Loss: 0.03513418097407558
ROC train: 0.999986	val: 0.855452	test: 0.681485
PRC train: 0.999837	val: 0.570728	test: 0.540758

Epoch: 54
Loss: 0.03439028170554152
ROC train: 1.000000	val: 0.856038	test: 0.669573
PRC train: 1.000000	val: 0.568364	test: 0.538809

Epoch: 55
Loss: 0.02892247255247587
ROC train: 0.999995	val: 0.860733	test: 0.676838
PRC train: 0.999946	val: 0.573002	test: 0.542671

Epoch: 56
Loss: 0.037113818075863045
ROC train: 0.999977	val: 0.845823	test: 0.671317
PRC train: 0.999998	val: 0.575230	test: 0.540657

Epoch: 57
Loss: 0.02832272392259197
ROC train: 0.999995	val: 0.806276	test: 0.682661
PRC train: 0.999946	val: 0.576336	test: 0.543494

Epoch: 58
Loss: 0.033752189272950005
ROC train: 1.000000	val: 0.831588	test: 0.680323
PRC train: 1.000000	val: 0.576872	test: 0.541296

Epoch: 59
Loss: 0.023392308906953218
ROC train: 0.999994	val: 0.870685	test: 0.671030
PRC train: 1.000000	val: 0.580131	test: 0.539184

Epoch: 60
Loss: 0.03329045864621592
ROC train: 1.000000	val: 0.859922	test: 0.680499
PRC train: 1.000000	val: 0.574240	test: 0.541067

Epoch: 61
Loss: 0.03528024172004314
ROC train: 0.999867	val: 0.824995	test: 0.700561
PRC train: 0.999503	val: 0.562985	test: 0.545613

Epoch: 62
Loss: 0.03969078364455812
ROC train: 0.999955	val: 0.828991	test: 0.694383
PRC train: 0.999997	val: 0.567122	test: 0.543593

Epoch: 63
Loss: 0.027323472092120087
ROC train: 0.999983	val: 0.809523	test: 0.688361
PRC train: 0.999999	val: 0.567577	test: 0.541857

Epoch: 64
Loss: 0.025674868712943673
ROC train: 0.999952	val: 0.800182	test: 0.678986
PRC train: 0.999490	val: 0.568557	test: 0.539631

Epoch: 65
Loss: 0.028877183537663453
ROC train: 1.000000	val: 0.833485	test: 0.692228
PRC train: 1.000000	val: 0.577124	test: 0.543258

Epoch: 66
Loss: 0.02923585094002818
ROC train: 0.999989	val: 0.842102	test: 0.690304
PRC train: 0.999999	val: 0.573152	test: 0.543477

Epoch: 67
Loss: 0.022882693350614214
ROC train: 1.000000	val: 0.834747	test: 0.683334
PRC train: 1.000000	val: 0.576348	test: 0.542530

Epoch: 68
Loss: 0.019852990812439176
ROC train: 1.000000	val: 0.843525	test: 0.690647
PRC train: 1.000000	val: 0.577113	test: 0.542918

Epoch: 69
Loss: 0.025404025244014583
ROC train: 1.000000	val: 0.880588	test: 0.679486
PRC train: 1.000000	val: 0.579393	test: 0.539801

Epoch: 70
Loss: 0.024813370066040754
ROC train: 0.999984	val: 0.828878	test: 0.682934
PRC train: 0.999945	val: 0.568023	test: 0.541918

Epoch: 71
Loss: 0.01800490486057544
ROC train: 0.999989	val: 0.819675	test: 0.684346
PRC train: 0.999999	val: 0.570549	test: 0.544781

Epoch: 72
Loss: 0.023083446384986148
ROC train: 1.000000	val: 0.827778	test: 0.664497
PRC train: 1.000000	val: 0.578008	test: 0.538618

Epoch: 73
Loss: 0.018008203700732604
ROC train: 1.000000	val: 0.813518	test: 0.661288
PRC train: 1.000000	val: 0.582657	test: 0.536457

Epoch: 74
Loss: 0.02336258879325122
ROC train: 1.000000	val: 0.826766	test: 0.664885
PRC train: 1.000000	val: 0.580926	test: 0.536830

Epoch: 75
Loss: 0.02030218038056402
ROC train: 1.000000	val: 0.814130	test: 0.684223
PRC train: 1.000000	val: 0.572339	test: 0.541071

Epoch: 76
Loss: 0.024793398832299713
ROC train: 0.999983	val: 0.803478	test: 0.704584
PRC train: 0.999999	val: 0.580157	test: 0.544275

Epoch: 77
Loss: 0.021045340180007865
ROC train: 0.999989	val: 0.832013	test: 0.714101
PRC train: 0.999999	val: 0.567016	test: 0.545916

Epoch: 78
Loss: 0.027149640488077907
ROC train: 1.000000	val: 0.875830	test: 0.703534
PRC train: 1.000000	val: 0.582286	test: 0.544494

Epoch: 79
Loss: 0.0172479680476104
ROC train: 1.000000	val: 0.880075	test: 0.701450
PRC train: 1.000000	val: 0.590625	test: 0.544884

Epoch: 80
Loss: 0.01923072912970559
ROC train: 1.000000	val: 0.898868	test: 0.697233
PRC train: 1.000000	val: 0.597901	test: 0.543567

Epoch: 81
Loss: 0.023835806439456486
ROC train: 0.999840	val: 0.877966	test: 0.688231
PRC train: 0.999716	val: 0.576898	test: 0.540535

Epoch: 82
Loss: 0.02291001148436212
ROC train: 1.000000	val: 0.864504	test: 0.688500
PRC train: 1.000000	val: 0.580242	test: 0.541484

Epoch: 83
Loss: 0.023571749304642262
ROC train: 0.999922	val: 0.829015	test: 0.676334
PRC train: 0.999596	val: 0.569187	test: 0.540342

Epoch: 84
Loss: 0.027590313477572893
ROC train: 0.999955	val: 0.823122	test: 0.678889
PRC train: 0.999943	val: 0.564886	test: 0.540833

Epoch: 85
Loss: 0.021963800294158065
ROC train: 0.999398	val: 0.844937	test: 0.655278
PRC train: 0.997290	val: 0.568172	test: 0.536455

Epoch: 86
Loss: 0.025745278071759626
ROC train: 0.999971	val: 0.841989	test: 0.685272
PRC train: 0.999669	val: 0.574046	test: 0.544063

Epoch: 87
Loss: 0.018026411868869625
ROC train: 1.000000	val: 0.839642	test: 0.680405
PRC train: 1.000000	val: 0.572190	test: 0.537819

Epoch: 88
Loss: 0.019922510070704978
ROC train: 1.000000	val: 0.851980	test: 0.682060
PRC train: 1.000000	val: 0.573041	test: 0.538933

Epoch: 89
Loss: 0.01525434870047282
ROC train: 1.000000	val: 0.845148	test: 0.694233
PRC train: 1.000000	val: 0.577073	test: 0.541020

Epoch: 90
Loss: 0.01585480334856098
ROC train: 1.000000	val: 0.825979	test: 0.696669
PRC train: 1.000000	val: 0.580302	test: 0.541086

Epoch: 91
Loss: 0.014899817721182443
ROC train: 1.000000	val: 0.830537	test: 0.684152
PRC train: 1.000000	val: 0.588166	test: 0.538297

Epoch: 92
Loss: 0.011988498525336712
ROC train: 1.000000	val: 0.842938	test: 0.685370
PRC train: 1.000000	val: 0.577913	test: 0.538541

Epoch: 93
Loss: 0.022776549533084195
PRC train: 0.988267	val: 0.546127	test: 0.567614

Epoch: 33
Loss: 0.0815749287580089
ROC train: 0.995764	val: 0.714951	test: 0.709484
PRC train: 0.984109	val: 0.612734	test: 0.559356

Epoch: 34
Loss: 0.0893785798864261
ROC train: 0.994864	val: 0.734131	test: 0.697729
PRC train: 0.983136	val: 0.563609	test: 0.558959

Epoch: 35
Loss: 0.08784715332709485
ROC train: 0.997628	val: 0.702065	test: 0.697367
PRC train: 0.990479	val: 0.536749	test: 0.552362

Epoch: 36
Loss: 0.0741937339311702
ROC train: 0.997497	val: 0.669012	test: 0.734630
PRC train: 0.988238	val: 0.529844	test: 0.565904

Epoch: 37
Loss: 0.06861869462717876
ROC train: 0.997929	val: 0.608113	test: 0.743374
PRC train: 0.991036	val: 0.524310	test: 0.566190

Epoch: 38
Loss: 0.06783790322215474
ROC train: 0.998387	val: 0.617541	test: 0.720077
PRC train: 0.992679	val: 0.528795	test: 0.566995

Epoch: 39
Loss: 0.06849955992843407
ROC train: 0.998596	val: 0.633948	test: 0.730383
PRC train: 0.992968	val: 0.552290	test: 0.571488

Epoch: 40
Loss: 0.06891957638570928
ROC train: 0.998505	val: 0.663593	test: 0.717579
PRC train: 0.993187	val: 0.561851	test: 0.562253

Epoch: 41
Loss: 0.05919400570705373
ROC train: 0.998499	val: 0.682049	test: 0.711240
PRC train: 0.993649	val: 0.554674	test: 0.567601

Epoch: 42
Loss: 0.07097430385455758
ROC train: 0.999035	val: 0.702178	test: 0.715756
PRC train: 0.995725	val: 0.546631	test: 0.563959

Epoch: 43
Loss: 0.047938753634551474
ROC train: 0.998862	val: 0.677765	test: 0.710396
PRC train: 0.994573	val: 0.536281	test: 0.555825

Epoch: 44
Loss: 0.05673585022605772
ROC train: 0.999125	val: 0.711567	test: 0.714856
PRC train: 0.993583	val: 0.572480	test: 0.561038

Epoch: 45
Loss: 0.06054823749792926
ROC train: 0.998375	val: 0.635821	test: 0.708334
PRC train: 0.990986	val: 0.574409	test: 0.555521

Epoch: 46
Loss: 0.0648224186937547
ROC train: 0.999128	val: 0.663168	test: 0.729520
PRC train: 0.993726	val: 0.607681	test: 0.561792

Epoch: 47
Loss: 0.05597000787660913
ROC train: 0.999528	val: 0.719446	test: 0.723275
PRC train: 0.996030	val: 0.614353	test: 0.561063

Epoch: 48
Loss: 0.04135594949328551
ROC train: 0.999637	val: 0.680837	test: 0.696280
PRC train: 0.997039	val: 0.557065	test: 0.557433

Epoch: 49
Loss: 0.04938322710876682
ROC train: 0.999517	val: 0.647172	test: 0.741981
PRC train: 0.996001	val: 0.583155	test: 0.565358

Epoch: 50
Loss: 0.04505149112969018
ROC train: 0.999652	val: 0.648321	test: 0.746247
PRC train: 0.997031	val: 0.577519	test: 0.567293

Epoch: 51
Loss: 0.04722538731013387
ROC train: 0.999652	val: 0.642565	test: 0.718714
PRC train: 0.997254	val: 0.545603	test: 0.558965

Epoch: 52
Loss: 0.04518302083019309
ROC train: 0.999413	val: 0.671559	test: 0.711176
PRC train: 0.996402	val: 0.568143	test: 0.562767

Epoch: 53
Loss: 0.04940271337008057
ROC train: 0.999242	val: 0.641728	test: 0.712532
PRC train: 0.995463	val: 0.560982	test: 0.562470

Epoch: 54
Loss: 0.04515214102469732
ROC train: 0.999868	val: 0.608376	test: 0.714037
PRC train: 0.998761	val: 0.562946	test: 0.569360

Epoch: 55
Loss: 0.03649510962108994
ROC train: 0.999956	val: 0.645812	test: 0.669823
PRC train: 0.999891	val: 0.577352	test: 0.564918

Epoch: 56
Loss: 0.04446876406232414
ROC train: 0.999952	val: 0.606514	test: 0.677955
PRC train: 0.999790	val: 0.597271	test: 0.564121

Epoch: 57
Loss: 0.04717813530110455
ROC train: 0.999730	val: 0.641191	test: 0.756228
PRC train: 0.997524	val: 0.560843	test: 0.574242

Epoch: 58
Loss: 0.04056720256732295
ROC train: 0.999811	val: 0.655451	test: 0.744798
PRC train: 0.998175	val: 0.561401	test: 0.562564

Epoch: 59
Loss: 0.041481790889199135
ROC train: 0.999971	val: 0.696495	test: 0.732094
PRC train: 0.999688	val: 0.571333	test: 0.561806

Epoch: 60
Loss: 0.03249327795600897
ROC train: 0.999913	val: 0.709357	test: 0.698667
PRC train: 0.999116	val: 0.576370	test: 0.554748

Epoch: 61
Loss: 0.03324653356455461
ROC train: 0.999986	val: 0.692588	test: 0.710422
PRC train: 0.999842	val: 0.577719	test: 0.557732

Epoch: 62
Loss: 0.037205252875135
ROC train: 0.999994	val: 0.701478	test: 0.724900
PRC train: 1.000000	val: 0.612383	test: 0.562127

Epoch: 63
Loss: 0.028682240505804913
ROC train: 1.000000	val: 0.689316	test: 0.730783
PRC train: 1.000000	val: 0.609516	test: 0.567027

Epoch: 64
Loss: 0.031013267107395583
ROC train: 0.999938	val: 0.690939	test: 0.722513
PRC train: 0.999367	val: 0.595678	test: 0.564748

Epoch: 65
Loss: 0.036813346607866546
ROC train: 0.999707	val: 0.691438	test: 0.714930
PRC train: 0.997724	val: 0.578402	test: 0.560171

Epoch: 66
Loss: 0.03251861784615163
ROC train: 0.999947	val: 0.700666	test: 0.718703
PRC train: 0.999454	val: 0.611151	test: 0.564382

Epoch: 67
Loss: 0.0367202937882971
ROC train: 0.999920	val: 0.754534	test: 0.693718
PRC train: 0.999409	val: 0.596942	test: 0.555252

Epoch: 68
Loss: 0.03625066616252477
ROC train: 0.999862	val: 0.751761	test: 0.690920
PRC train: 0.999434	val: 0.573605	test: 0.557956

Epoch: 69
Loss: 0.03336232591044909
ROC train: 0.999922	val: 0.705548	test: 0.739912
PRC train: 0.999889	val: 0.568071	test: 0.564624

Epoch: 70
Loss: 0.03152147944500925
ROC train: 0.999968	val: 0.696007	test: 0.757001
PRC train: 0.999892	val: 0.568066	test: 0.568988

Epoch: 71
Loss: 0.0329678148186053
ROC train: 0.999983	val: 0.713215	test: 0.732243
PRC train: 0.999999	val: 0.549176	test: 0.564956

Epoch: 72
Loss: 0.03552675332370133
ROC train: 1.000000	val: 0.737740	test: 0.684637
PRC train: 1.000000	val: 0.571118	test: 0.555458

Epoch: 73
Loss: 0.03672493421805318
ROC train: 0.999995	val: 0.746181	test: 0.675586
PRC train: 0.999946	val: 0.588942	test: 0.549869

Epoch: 74
Loss: 0.02662754557835802
ROC train: 0.999854	val: 0.718311	test: 0.698940
PRC train: 0.999242	val: 0.583000	test: 0.553794

Epoch: 75
Loss: 0.027894201901616112
ROC train: 0.999955	val: 0.706960	test: 0.701039
PRC train: 0.999622	val: 0.640577	test: 0.554679

Epoch: 76
Loss: 0.03659409216546365
ROC train: 0.999947	val: 0.706510	test: 0.691650
PRC train: 0.999446	val: 0.597605	test: 0.553434

Epoch: 77
Loss: 0.021564716092352665
ROC train: 0.999916	val: 0.721906	test: 0.691282
PRC train: 0.999323	val: 0.589480	test: 0.552359

Epoch: 78
Loss: 0.02305725567056555
ROC train: 1.000000	val: 0.720644	test: 0.705536
PRC train: 1.000000	val: 0.574583	test: 0.557529

Epoch: 79
Loss: 0.023621760236964835
ROC train: 0.999957	val: 0.712815	test: 0.723312
PRC train: 0.999837	val: 0.566660	test: 0.559521

Epoch: 80
Loss: 0.02065702845373245
ROC train: 1.000000	val: 0.727887	test: 0.707934
PRC train: 1.000000	val: 0.570292	test: 0.554877

Epoch: 81
Loss: 0.023746309317494788
ROC train: 1.000000	val: 0.724728	test: 0.694143
PRC train: 1.000000	val: 0.559432	test: 0.553905

Epoch: 82
Loss: 0.01806740393378496
ROC train: 1.000000	val: 0.721319	test: 0.695417
PRC train: 1.000000	val: 0.570486	test: 0.552200

Epoch: 83
Loss: 0.023748914929949315
ROC train: 1.000000	val: 0.739388	test: 0.686811
PRC train: 1.000000	val: 0.614871	test: 0.551189

Epoch: 84
Loss: 0.019978751994321375
ROC train: 1.000000	val: 0.741961	test: 0.679310
PRC train: 1.000000	val: 0.615741	test: 0.546927

Epoch: 85
Loss: 0.018164430433369218
ROC train: 1.000000	val: 0.710618	test: 0.690147
PRC train: 1.000000	val: 0.589324	test: 0.547483

Epoch: 86
Loss: 0.026534780165732148
ROC train: 0.999952	val: 0.691751	test: 0.705880
PRC train: 0.999509	val: 0.614175	test: 0.551825

Epoch: 87
Loss: 0.023303771279960226
ROC train: 0.999995	val: 0.674831	test: 0.711094
PRC train: 0.999946	val: 0.622141	test: 0.550669

Epoch: 88
Loss: 0.02641194725867178
ROC train: 1.000000	val: 0.653016	test: 0.730850
PRC train: 1.000000	val: 0.621113	test: 0.558755

Epoch: 89
Loss: 0.023366451969595862
ROC train: 1.000000	val: 0.622422	test: 0.711270
PRC train: 1.000000	val: 0.599885	test: 0.556441

Epoch: 90
Loss: 0.015079886546131844
ROC train: 1.000000	val: 0.626080	test: 0.701745
PRC train: 1.000000	val: 0.599203	test: 0.555129

Epoch: 91
Loss: 0.02052225470412146
ROC train: 1.000000	val: 0.628403	test: 0.686673
PRC train: 1.000000	val: 0.565591	test: 0.552093

Epoch: 92
Loss: 0.018664963714008772
ROC train: 1.000000	val: 0.688979	test: 0.651878
PRC train: 1.000000	val: 0.596018	test: 0.544118

Epoch: 93
Loss: 0.023766484886036165
PRC train: 0.992986	val: 0.637167	test: 0.524899

Epoch: 33
Loss: 0.06878181656301978
ROC train: 0.998589	val: 0.867428	test: 0.615378
PRC train: 0.992945	val: 0.639693	test: 0.528742

Epoch: 34
Loss: 0.07007666872416975
ROC train: 0.998954	val: 0.835161	test: 0.610317
PRC train: 0.994619	val: 0.632320	test: 0.529703

Epoch: 35
Loss: 0.05828211009314625
ROC train: 0.999237	val: 0.815257	test: 0.611403
PRC train: 0.997161	val: 0.628700	test: 0.529748

Epoch: 36
Loss: 0.07231834162252267
ROC train: 0.999458	val: 0.832564	test: 0.639186
PRC train: 0.997152	val: 0.634326	test: 0.534418

Epoch: 37
Loss: 0.06369778210061827
ROC train: 0.998981	val: 0.876431	test: 0.630255
PRC train: 0.994737	val: 0.644133	test: 0.532211

Epoch: 38
Loss: 0.06757750303943623
ROC train: 0.999350	val: 0.881600	test: 0.640617
PRC train: 0.996083	val: 0.648838	test: 0.535579

Epoch: 39
Loss: 0.060494630864267374
ROC train: 0.998690	val: 0.856889	test: 0.675901
PRC train: 0.991704	val: 0.653472	test: 0.550585

Epoch: 40
Loss: 0.05474432300148239
ROC train: 0.999353	val: 0.788923	test: 0.653426
PRC train: 0.996117	val: 0.628126	test: 0.537823

Epoch: 41
Loss: 0.05308302547006866
ROC train: 0.999423	val: 0.828843	test: 0.628205
PRC train: 0.997770	val: 0.642024	test: 0.534152

Epoch: 42
Loss: 0.062360818842424726
ROC train: 0.999441	val: 0.849196	test: 0.590467
PRC train: 0.998552	val: 0.647647	test: 0.527115

Epoch: 43
Loss: 0.04627602585268718
ROC train: 0.999621	val: 0.877516	test: 0.592540
PRC train: 0.998747	val: 0.660544	test: 0.526442

Epoch: 44
Loss: 0.04438000577048325
ROC train: 0.999665	val: 0.873858	test: 0.619748
PRC train: 0.998449	val: 0.647262	test: 0.530786

Epoch: 45
Loss: 0.04783371671689096
ROC train: 0.999673	val: 0.791270	test: 0.624025
PRC train: 0.998361	val: 0.633009	test: 0.532943

Epoch: 46
Loss: 0.04027737643261786
ROC train: 0.999820	val: 0.750250	test: 0.585032
PRC train: 0.999589	val: 0.620218	test: 0.524052

Epoch: 47
Loss: 0.045018643352675795
ROC train: 0.999562	val: 0.724615	test: 0.618676
PRC train: 0.998515	val: 0.615837	test: 0.527209

Epoch: 48
Loss: 0.04968508773605394
ROC train: 0.999764	val: 0.746392	test: 0.602962
PRC train: 0.998785	val: 0.623403	test: 0.524804

Epoch: 49
Loss: 0.04374475269811791
ROC train: 0.999792	val: 0.779245	test: 0.582537
PRC train: 0.998797	val: 0.630756	test: 0.521855

Epoch: 50
Loss: 0.03820331938163199
ROC train: 0.999712	val: 0.850658	test: 0.570285
PRC train: 0.998435	val: 0.633210	test: 0.520987

Epoch: 51
Loss: 0.04296405869941418
ROC train: 0.999894	val: 0.821801	test: 0.567002
PRC train: 0.999047	val: 0.630930	test: 0.520009

Epoch: 52
Loss: 0.038909174545460945
ROC train: 0.999980	val: 0.895723	test: 0.569494
PRC train: 0.999842	val: 0.650678	test: 0.523339

Epoch: 53
Loss: 0.04539796519967271
ROC train: 0.999977	val: 0.845563	test: 0.600224
PRC train: 0.999998	val: 0.659913	test: 0.529058

Epoch: 54
Loss: 0.03459821945233353
ROC train: 0.999983	val: 0.788473	test: 0.600022
PRC train: 0.999999	val: 0.635664	test: 0.525018

Epoch: 55
Loss: 0.03206401929565167
ROC train: 0.999881	val: 0.787911	test: 0.580972
PRC train: 0.999992	val: 0.633346	test: 0.522094

Epoch: 56
Loss: 0.04103602433223872
ROC train: 0.999892	val: 0.818592	test: 0.575407
PRC train: 0.999992	val: 0.634812	test: 0.521123

Epoch: 57
Loss: 0.03958272373773951
ROC train: 1.000000	val: 0.815683	test: 0.602383
PRC train: 1.000000	val: 0.643537	test: 0.526801

Epoch: 58
Loss: 0.03539709649922411
ROC train: 1.000000	val: 0.803770	test: 0.603070
PRC train: 1.000000	val: 0.640352	test: 0.527624

Epoch: 59
Loss: 0.03500605466580919
ROC train: 0.999966	val: 0.832065	test: 0.609028
PRC train: 0.999998	val: 0.648468	test: 0.527584

Epoch: 60
Loss: 0.028824814081344702
ROC train: 0.999989	val: 0.826696	test: 0.614874
PRC train: 0.999999	val: 0.643583	test: 0.528314

Epoch: 61
Loss: 0.030370615273398155
ROC train: 0.999994	val: 0.797251	test: 0.619345
PRC train: 1.000000	val: 0.635306	test: 0.528609

Epoch: 62
Loss: 0.026924338993928015
ROC train: 0.999994	val: 0.788473	test: 0.622449
PRC train: 1.000000	val: 0.638273	test: 0.528677

Epoch: 63
Loss: 0.02429984417027966
ROC train: 1.000000	val: 0.808215	test: 0.627002
PRC train: 1.000000	val: 0.640876	test: 0.528771

Epoch: 64
Loss: 0.02099727231987477
ROC train: 1.000000	val: 0.828344	test: 0.617896
PRC train: 1.000000	val: 0.682919	test: 0.526930

Epoch: 65
Loss: 0.023611544269980057
ROC train: 1.000000	val: 0.830579	test: 0.633311
PRC train: 1.000000	val: 0.662149	test: 0.531798

Epoch: 66
Loss: 0.028060257752630484
ROC train: 1.000000	val: 0.822637	test: 0.634947
PRC train: 1.000000	val: 0.658427	test: 0.533189

Epoch: 67
Loss: 0.023062466865956994
ROC train: 1.000000	val: 0.816568	test: 0.636210
PRC train: 1.000000	val: 0.660043	test: 0.535725

Epoch: 68
Loss: 0.021164909589842053
ROC train: 1.000000	val: 0.803932	test: 0.621706
PRC train: 1.000000	val: 0.644050	test: 0.531427

Epoch: 69
Loss: 0.026127540539889027
ROC train: 1.000000	val: 0.805418	test: 0.615348
PRC train: 1.000000	val: 0.638740	test: 0.529869

Epoch: 70
Loss: 0.023400491304225297
ROC train: 0.999972	val: 0.815345	test: 0.604792
PRC train: 0.999998	val: 0.641953	test: 0.527828

Epoch: 71
Loss: 0.022449296211474214
ROC train: 0.999972	val: 0.826334	test: 0.601606
PRC train: 0.999998	val: 0.643262	test: 0.527179

Epoch: 72
Loss: 0.025473339211049523
ROC train: 1.000000	val: 0.809027	test: 0.590438
PRC train: 1.000000	val: 0.638353	test: 0.523421

Epoch: 73
Loss: 0.02175742882991189
ROC train: 1.000000	val: 0.805281	test: 0.586852
PRC train: 1.000000	val: 0.633963	test: 0.522909

Epoch: 74
Loss: 0.01705503378901866
ROC train: 1.000000	val: 0.829180	test: 0.581888
PRC train: 1.000000	val: 0.610547	test: 0.524313

Epoch: 75
Loss: 0.019861723477975664
ROC train: 1.000000	val: 0.804631	test: 0.582138
PRC train: 1.000000	val: 0.584207	test: 0.523601

Epoch: 76
Loss: 0.019619397811767407
ROC train: 1.000000	val: 0.775836	test: 0.568022
PRC train: 1.000000	val: 0.634898	test: 0.518633

Epoch: 77
Loss: 0.016568588419320705
ROC train: 1.000000	val: 0.772877	test: 0.577741
PRC train: 1.000000	val: 0.628311	test: 0.519318

Epoch: 78
Loss: 0.021710383004669237
ROC train: 1.000000	val: 0.782917	test: 0.597434
PRC train: 1.000000	val: 0.632239	test: 0.522401

Epoch: 79
Loss: 0.022188581181108457
ROC train: 1.000000	val: 0.789260	test: 0.617227
PRC train: 1.000000	val: 0.640549	test: 0.527957

Epoch: 80
Loss: 0.015466493406968512
ROC train: 1.000000	val: 0.787162	test: 0.610152
PRC train: 1.000000	val: 0.640453	test: 0.525277

Epoch: 81
Loss: 0.02000194157241786
ROC train: 1.000000	val: 0.787949	test: 0.604232
PRC train: 1.000000	val: 0.636490	test: 0.523663

Epoch: 82
Loss: 0.015072183223084581
ROC train: 1.000000	val: 0.791583	test: 0.583919
PRC train: 1.000000	val: 0.635320	test: 0.521267

Epoch: 83
Loss: 0.015280090728474157
ROC train: 1.000000	val: 0.796053	test: 0.575937
PRC train: 1.000000	val: 0.636312	test: 0.521111

Epoch: 84
Loss: 0.018869189151990436
ROC train: 1.000000	val: 0.783191	test: 0.593138
PRC train: 1.000000	val: 0.635766	test: 0.523526

Epoch: 85
Loss: 0.01280965265029653
ROC train: 1.000000	val: 0.769381	test: 0.608191
PRC train: 1.000000	val: 0.630331	test: 0.523829

Epoch: 86
Loss: 0.014871861452756954
ROC train: 1.000000	val: 0.758754	test: 0.621115
PRC train: 1.000000	val: 0.627189	test: 0.527058

Epoch: 87
Loss: 0.01447470984272744
ROC train: 1.000000	val: 0.760041	test: 0.624365
PRC train: 1.000000	val: 0.586986	test: 0.529152

Epoch: 88
Loss: 0.018643035487595765
ROC train: 1.000000	val: 0.776761	test: 0.627992
PRC train: 1.000000	val: 0.590552	test: 0.531348

Epoch: 89
Loss: 0.0121876604326695
ROC train: 1.000000	val: 0.785989	test: 0.613387
PRC train: 1.000000	val: 0.633829	test: 0.526327

Epoch: 90
Loss: 0.013654923045577787
ROC train: 1.000000	val: 0.777685	test: 0.584095
PRC train: 1.000000	val: 0.627176	test: 0.521492

Epoch: 91
Loss: 0.010398095216567461
ROC train: 1.000000	val: 0.773127	test: 0.573016
PRC train: 1.000000	val: 0.584419	test: 0.519359

Epoch: 92
Loss: 0.009449337322515588
ROC train: 1.000000	val: 0.766721	test: 0.574488
PRC train: 1.000000	val: 0.559190	test: 0.519504

Epoch: 93
Loss: 0.012131608624303987
PRC train: 0.985136	val: 0.576110	test: 0.546569

Epoch: 33
Loss: 0.07824224152076537
ROC train: 0.998243	val: 0.860435	test: 0.729804
PRC train: 0.991277	val: 0.572255	test: 0.550343

Epoch: 34
Loss: 0.0759137247104634
ROC train: 0.997998	val: 0.823087	test: 0.714714
PRC train: 0.990679	val: 0.554939	test: 0.545253

Epoch: 35
Loss: 0.08091653877200992
ROC train: 0.997458	val: 0.835773	test: 0.757487
PRC train: 0.989598	val: 0.566599	test: 0.557372

Epoch: 36
Loss: 0.08093851325694183
ROC train: 0.999016	val: 0.750226	test: 0.766429
PRC train: 0.995073	val: 0.545395	test: 0.560714

Epoch: 37
Loss: 0.07369004129235472
ROC train: 0.999095	val: 0.820226	test: 0.770114
PRC train: 0.995173	val: 0.567491	test: 0.561905

Epoch: 38
Loss: 0.08126592634271358
ROC train: 0.997802	val: 0.869937	test: 0.751981
PRC train: 0.986646	val: 0.583729	test: 0.554725

Epoch: 39
Loss: 0.07617844888422767
ROC train: 0.997422	val: 0.833699	test: 0.756161
PRC train: 0.987035	val: 0.650812	test: 0.563127

Epoch: 40
Loss: 0.08006211959619847
ROC train: 0.998853	val: 0.851319	test: 0.748421
PRC train: 0.993130	val: 0.642598	test: 0.556137

Epoch: 41
Loss: 0.07967934028102894
ROC train: 0.998502	val: 0.870861	test: 0.676961
PRC train: 0.991925	val: 0.575011	test: 0.538053

Epoch: 42
Loss: 0.06973914265708428
ROC train: 0.997574	val: 0.854303	test: 0.666495
PRC train: 0.987481	val: 0.569934	test: 0.536321

Epoch: 43
Loss: 0.06747738510086707
ROC train: 0.999460	val: 0.810000	test: 0.665912
PRC train: 0.996694	val: 0.546868	test: 0.535617

Epoch: 44
Loss: 0.06384087562594574
ROC train: 0.999488	val: 0.824085	test: 0.692795
PRC train: 0.996595	val: 0.555569	test: 0.542549

Epoch: 45
Loss: 0.06122377360819079
ROC train: 0.999069	val: 0.860396	test: 0.706668
PRC train: 0.994599	val: 0.580977	test: 0.547159

Epoch: 46
Loss: 0.05384398454878159
ROC train: 0.999401	val: 0.834799	test: 0.684757
PRC train: 0.997151	val: 0.563240	test: 0.541979

Epoch: 47
Loss: 0.06160187413820489
ROC train: 0.999501	val: 0.809751	test: 0.705506
PRC train: 0.998207	val: 0.555025	test: 0.545815

Epoch: 48
Loss: 0.063620129613156
ROC train: 0.999391	val: 0.811824	test: 0.699986
PRC train: 0.997267	val: 0.548878	test: 0.545557

Epoch: 49
Loss: 0.057785885660658495
ROC train: 0.999193	val: 0.839357	test: 0.706694
PRC train: 0.995728	val: 0.582542	test: 0.545324

Epoch: 50
Loss: 0.06276378189156137
ROC train: 0.999640	val: 0.859461	test: 0.691529
PRC train: 0.997171	val: 0.565429	test: 0.544372

Epoch: 51
Loss: 0.050494985192233645
ROC train: 0.999633	val: 0.811076	test: 0.704558
PRC train: 0.997418	val: 0.547562	test: 0.546960

Epoch: 52
Loss: 0.057449345852181186
ROC train: 0.999570	val: 0.790733	test: 0.729379
PRC train: 0.997514	val: 0.553372	test: 0.554593

Epoch: 53
Loss: 0.04985621003813588
ROC train: 0.999442	val: 0.777984	test: 0.712402
PRC train: 0.996722	val: 0.548693	test: 0.546184

Epoch: 54
Loss: 0.054520031062699405
ROC train: 0.999682	val: 0.802220	test: 0.661535
PRC train: 0.997998	val: 0.565363	test: 0.536426

Epoch: 55
Loss: 0.04956148650602094
ROC train: 0.998982	val: 0.865403	test: 0.667982
PRC train: 0.993544	val: 0.585272	test: 0.540188

Epoch: 56
Loss: 0.05792476882150708
ROC train: 0.999788	val: 0.865467	test: 0.682101
PRC train: 0.998474	val: 0.581502	test: 0.543778

Epoch: 57
Loss: 0.04771421008032889
ROC train: 0.999884	val: 0.797701	test: 0.666630
PRC train: 0.998881	val: 0.554874	test: 0.540860

Epoch: 58
Loss: 0.05502569209779691
ROC train: 0.999523	val: 0.770917	test: 0.634450
PRC train: 0.996569	val: 0.541828	test: 0.535419

Epoch: 59
Loss: 0.03403278293167139
ROC train: 0.999553	val: 0.835597	test: 0.677417
PRC train: 0.996236	val: 0.565689	test: 0.547079

Epoch: 60
Loss: 0.04154766089406019
ROC train: 0.999547	val: 0.831601	test: 0.686255
PRC train: 0.996006	val: 0.564375	test: 0.547725

Epoch: 61
Loss: 0.03598831223446131
ROC train: 0.999735	val: 0.793730	test: 0.678060
PRC train: 0.997698	val: 0.544014	test: 0.545179

Epoch: 62
Loss: 0.0320861977964549
ROC train: 0.999783	val: 0.770105	test: 0.692937
PRC train: 0.998571	val: 0.540284	test: 0.549971

Epoch: 63
Loss: 0.0443938675005965
ROC train: 0.999610	val: 0.801535	test: 0.722805
PRC train: 0.997681	val: 0.545385	test: 0.554561

Epoch: 64
Loss: 0.026519608009037205
ROC train: 0.999820	val: 0.817306	test: 0.715260
PRC train: 0.998637	val: 0.546007	test: 0.549536

Epoch: 65
Loss: 0.03153018328714381
ROC train: 0.999931	val: 0.817580	test: 0.720743
PRC train: 0.999420	val: 0.548759	test: 0.550108

Epoch: 66
Loss: 0.04685932915207297
ROC train: 0.999813	val: 0.812211	test: 0.730500
PRC train: 0.999622	val: 0.552179	test: 0.552047

Epoch: 67
Loss: 0.029241111182883285
ROC train: 0.999902	val: 0.801897	test: 0.728770
PRC train: 0.999072	val: 0.549096	test: 0.551652

Epoch: 68
Loss: 0.03761019148112934
ROC train: 0.999986	val: 0.830530	test: 0.692612
PRC train: 0.999842	val: 0.552444	test: 0.543662

Epoch: 69
Loss: 0.039274824188391094
ROC train: 0.999947	val: 0.871472	test: 0.700158
PRC train: 0.999465	val: 0.575043	test: 0.546584

Epoch: 70
Loss: 0.04381774370160664
ROC train: 0.999613	val: 0.854615	test: 0.704592
PRC train: 0.997447	val: 0.564423	test: 0.545834

Epoch: 71
Loss: 0.04235958548053794
ROC train: 0.999956	val: 0.847960	test: 0.709952
PRC train: 0.999891	val: 0.565329	test: 0.549672

Epoch: 72
Loss: 0.039684882008639
ROC train: 0.999864	val: 0.843764	test: 0.711950
PRC train: 0.999615	val: 0.575357	test: 0.554690

Epoch: 73
Loss: 0.03524190654814054
ROC train: 0.999943	val: 0.847984	test: 0.701544
PRC train: 0.999996	val: 0.577412	test: 0.547040

Epoch: 74
Loss: 0.027554356046913735
ROC train: 0.999989	val: 0.862582	test: 0.694936
PRC train: 0.999999	val: 0.576123	test: 0.544425

Epoch: 75
Loss: 0.032717537972081656
ROC train: 0.999842	val: 0.871722	test: 0.692213
PRC train: 0.999267	val: 0.582163	test: 0.543816

Epoch: 76
Loss: 0.03596704276887593
ROC train: 0.999945	val: 0.872896	test: 0.714710
PRC train: 0.999581	val: 0.584197	test: 0.548630

Epoch: 77
Loss: 0.025104276920096424
ROC train: 0.999904	val: 0.853642	test: 0.708614
PRC train: 0.999303	val: 0.576104	test: 0.547608

Epoch: 78
Loss: 0.029577233675811516
ROC train: 0.999953	val: 0.865105	test: 0.693076
PRC train: 0.999741	val: 0.577600	test: 0.546309

Epoch: 79
Loss: 0.026739211752879695
ROC train: 0.999947	val: 0.867403	test: 0.672158
PRC train: 0.999465	val: 0.568870	test: 0.542292

Epoch: 80
Loss: 0.022547013098162982
ROC train: 0.999945	val: 0.869163	test: 0.673794
PRC train: 0.999890	val: 0.568873	test: 0.541900

Epoch: 81
Loss: 0.020805528746164842
ROC train: 0.999983	val: 0.884759	test: 0.694192
PRC train: 0.999999	val: 0.582374	test: 0.546535

Epoch: 82
Loss: 0.026012446199488438
ROC train: 0.999989	val: 0.878803	test: 0.695847
PRC train: 0.999999	val: 0.584216	test: 0.545220

Epoch: 83
Loss: 0.02200191587512143
ROC train: 0.999994	val: 0.870499	test: 0.693449
PRC train: 1.000000	val: 0.574064	test: 0.543821

Epoch: 84
Loss: 0.01907460881441295
ROC train: 0.999994	val: 0.874856	test: 0.718382
PRC train: 1.000000	val: 0.579118	test: 0.549426

Epoch: 85
Loss: 0.02119426048970698
ROC train: 0.999969	val: 0.856014	test: 0.726764
PRC train: 0.999791	val: 0.573634	test: 0.550267

Epoch: 86
Loss: 0.030017783082848894
ROC train: 0.999995	val: 0.844751	test: 0.743752
PRC train: 0.999946	val: 0.567896	test: 0.555977

Epoch: 87
Loss: 0.019879341023382832
ROC train: 1.000000	val: 0.865604	test: 0.728863
PRC train: 1.000000	val: 0.581236	test: 0.556816

Epoch: 88
Loss: 0.021060555333917645
ROC train: 1.000000	val: 0.878578	test: 0.731336
PRC train: 1.000000	val: 0.589941	test: 0.554610

Epoch: 89
Loss: 0.01658086091947461
ROC train: 1.000000	val: 0.883585	test: 0.728150
PRC train: 1.000000	val: 0.580077	test: 0.552491

Epoch: 90
Loss: 0.018707139501327408
ROC train: 1.000000	val: 0.890266	test: 0.724654
PRC train: 1.000000	val: 0.589337	test: 0.552301

Epoch: 91
Loss: 0.018628728174306115
ROC train: 0.999994	val: 0.882211	test: 0.699033
PRC train: 1.000000	val: 0.586456	test: 0.548395

Epoch: 92
Loss: 0.020849316307194575
ROC train: 0.999994	val: 0.874245	test: 0.707490
PRC train: 1.000000	val: 0.581614	test: 0.551453

Epoch: 93
Loss: 0.021918277528765746
ROC train: 0.986764	val: 0.766496	test: 0.872145
PRC train: 0.920969	val: 0.599012	test: 0.678436

Epoch: 95
Loss: 0.10994311661307729
ROC train: 0.986811	val: 0.770604	test: 0.864051
PRC train: 0.923532	val: 0.606943	test: 0.665166

Epoch: 96
Loss: 0.10890288756549527
ROC train: 0.986158	val: 0.765846	test: 0.863539
PRC train: 0.920330	val: 0.606334	test: 0.666281

Epoch: 97
Loss: 0.1083320268023609
ROC train: 0.984259	val: 0.769005	test: 0.860267
PRC train: 0.914038	val: 0.613214	test: 0.679021

Epoch: 98
Loss: 0.10333501755511845
ROC train: 0.985190	val: 0.759278	test: 0.873158
PRC train: 0.920104	val: 0.579984	test: 0.689579

Epoch: 99
Loss: 0.10857022944594737
ROC train: 0.986358	val: 0.787911	test: 0.894456
PRC train: 0.923175	val: 0.584044	test: 0.741347

Epoch: 100
Loss: 0.11158936877598999
ROC train: 0.986537	val: 0.806005	test: 0.886231
PRC train: 0.921956	val: 0.580681	test: 0.718333

Epoch: 101
Loss: 0.09242522449294013
ROC train: 0.986907	val: 0.763224	test: 0.865713
PRC train: 0.925961	val: 0.591659	test: 0.702180

Epoch: 102
Loss: 0.10654230692366469
ROC train: 0.986869	val: 0.743345	test: 0.881427
PRC train: 0.925771	val: 0.581844	test: 0.698304

Epoch: 103
Loss: 0.10222371607527021
ROC train: 0.986976	val: 0.764535	test: 0.890459
PRC train: 0.925047	val: 0.628446	test: 0.656614

Epoch: 104
Loss: 0.10917074320828904
ROC train: 0.987223	val: 0.741384	test: 0.881353
PRC train: 0.929795	val: 0.625399	test: 0.651590

Epoch: 105
Loss: 0.0977978572594238
ROC train: 0.986460	val: 0.759503	test: 0.869960
PRC train: 0.929728	val: 0.594048	test: 0.677405

Epoch: 106
Loss: 0.10960406112587184
ROC train: 0.985820	val: 0.772364	test: 0.888323
PRC train: 0.917601	val: 0.607050	test: 0.682348

Epoch: 107
Loss: 0.10841150285347816
ROC train: 0.986699	val: 0.762975	test: 0.884513
PRC train: 0.924577	val: 0.601236	test: 0.698136

Epoch: 108
Loss: 0.0901687918620365
ROC train: 0.987433	val: 0.733369	test: 0.877180
PRC train: 0.929079	val: 0.582831	test: 0.688412

Epoch: 109
Loss: 0.09919009873410818
ROC train: 0.987253	val: 0.756157	test: 0.883751
PRC train: 0.928684	val: 0.589508	test: 0.700241

Epoch: 110
Loss: 0.10707570190485356
ROC train: 0.986462	val: 0.766496	test: 0.887699
PRC train: 0.919653	val: 0.641507	test: 0.700793

Epoch: 111
Loss: 0.11082620511145189
ROC train: 0.987329	val: 0.758842	test: 0.848912
PRC train: 0.930554	val: 0.627231	test: 0.684986

Epoch: 112
Loss: 0.1012647686652097
ROC train: 0.987117	val: 0.727862	test: 0.845614
PRC train: 0.926585	val: 0.615659	test: 0.685464

Epoch: 113
Loss: 0.09581946985996177
ROC train: 0.986175	val: 0.729060	test: 0.860129
PRC train: 0.919422	val: 0.618309	test: 0.693683

Epoch: 114
Loss: 0.09393973232410971
ROC train: 0.987574	val: 0.730459	test: 0.861403
PRC train: 0.931178	val: 0.622266	test: 0.690409

Epoch: 115
Loss: 0.0992288219105324
ROC train: 0.988109	val: 0.736577	test: 0.862295
PRC train: 0.933737	val: 0.629433	test: 0.680082

Epoch: 116
Loss: 0.10461544686055935
ROC train: 0.988439	val: 0.723828	test: 0.844295
PRC train: 0.938029	val: 0.620405	test: 0.673874

Epoch: 117
Loss: 0.09225592804857996
ROC train: 0.988406	val: 0.712727	test: 0.851097
PRC train: 0.934153	val: 0.618636	test: 0.674834

Epoch: 118
Loss: 0.09985408130587717
ROC train: 0.988519	val: 0.725789	test: 0.857118
PRC train: 0.934043	val: 0.618941	test: 0.681377

Epoch: 119
Loss: 0.09911124400317123
ROC train: 0.988451	val: 0.722992	test: 0.834833
PRC train: 0.939380	val: 0.617190	test: 0.682331

Epoch: 120
Loss: 0.107193871019805
ROC train: 0.987607	val: 0.745106	test: 0.828775
PRC train: 0.930288	val: 0.635641	test: 0.681449

Early stopping
Best (ROC):	 train: 0.933876	val: 0.908922	test: 0.695895
Best (PRC):	 train: 0.774508	val: 0.625666	test: 0.551594

ROC train: 0.982906	val: 0.834125	test: 0.827737
PRC train: 0.907660	val: 0.600420	test: 0.604847

Epoch: 95
Loss: 0.10782400221472485
ROC train: 0.985268	val: 0.809962	test: 0.828936
PRC train: 0.917959	val: 0.625994	test: 0.613926

Epoch: 96
Loss: 0.11611477196278641
ROC train: 0.985609	val: 0.760389	test: 0.824526
PRC train: 0.918883	val: 0.626014	test: 0.614318

Epoch: 97
Loss: 0.10009911796641804
ROC train: 0.985401	val: 0.729384	test: 0.792834
PRC train: 0.920171	val: 0.603678	test: 0.616118

Epoch: 98
Loss: 0.11226655608313736
ROC train: 0.984268	val: 0.742583	test: 0.790548
PRC train: 0.917978	val: 0.644184	test: 0.614042

Epoch: 99
Loss: 0.09793952073872728
ROC train: 0.984100	val: 0.782615	test: 0.836156
PRC train: 0.915743	val: 0.687491	test: 0.626929

Epoch: 100
Loss: 0.11999139681026534
ROC train: 0.985999	val: 0.807751	test: 0.855344
PRC train: 0.925013	val: 0.657004	test: 0.627058

Epoch: 101
Loss: 0.10089838082511386
ROC train: 0.986404	val: 0.795677	test: 0.857955
PRC train: 0.926638	val: 0.612672	test: 0.625240

Epoch: 102
Loss: 0.10884623631136225
ROC train: 0.986943	val: 0.780742	test: 0.870435
PRC train: 0.928686	val: 0.621363	test: 0.647255

Epoch: 103
Loss: 0.10392145937091538
ROC train: 0.986587	val: 0.786224	test: 0.861515
PRC train: 0.927914	val: 0.612881	test: 0.636465

Epoch: 104
Loss: 0.10620096683819565
ROC train: 0.983655	val: 0.785525	test: 0.836518
PRC train: 0.919516	val: 0.610310	test: 0.621873

Epoch: 105
Loss: 0.10908787825502493
ROC train: 0.984890	val: 0.781466	test: 0.850503
PRC train: 0.925610	val: 0.624559	test: 0.629830

Epoch: 106
Loss: 0.10718286546533753
ROC train: 0.987100	val: 0.774512	test: 0.828797
PRC train: 0.931795	val: 0.625667	test: 0.615806

Epoch: 107
Loss: 0.10429293198921681
ROC train: 0.987210	val: 0.766482	test: 0.824132
PRC train: 0.929725	val: 0.617998	test: 0.616776

Epoch: 108
Loss: 0.10211529278167442
ROC train: 0.985667	val: 0.761362	test: 0.822634
PRC train: 0.920542	val: 0.645620	test: 0.612476

Epoch: 109
Loss: 0.11233162214900586
ROC train: 0.984409	val: 0.757117	test: 0.833265
PRC train: 0.917042	val: 0.609040	test: 0.619126

Epoch: 110
Loss: 0.10749145623943375
ROC train: 0.985650	val: 0.742857	test: 0.810211
PRC train: 0.917402	val: 0.582354	test: 0.615737

Epoch: 111
Loss: 0.10621219516385287
ROC train: 0.984284	val: 0.742969	test: 0.801847
PRC train: 0.918178	val: 0.594582	test: 0.622706

Epoch: 112
Loss: 0.1004842472409762
ROC train: 0.986858	val: 0.752559	test: 0.831446
PRC train: 0.925507	val: 0.597912	test: 0.622870

Epoch: 113
Loss: 0.09646931010280893
ROC train: 0.987762	val: 0.763773	test: 0.824987
PRC train: 0.931630	val: 0.608090	test: 0.620328

Epoch: 114
Loss: 0.08679462145471142
ROC train: 0.986451	val: 0.784913	test: 0.808474
PRC train: 0.928957	val: 0.609919	test: 0.609421

Epoch: 115
Loss: 0.10247743894429817
ROC train: 0.987962	val: 0.791618	test: 0.843813
PRC train: 0.931288	val: 0.631092	test: 0.631617

Epoch: 116
Loss: 0.09493548042758773
ROC train: 0.987758	val: 0.787060	test: 0.848198
PRC train: 0.928842	val: 0.615676	test: 0.634926

Epoch: 117
Loss: 0.08980983518199956
ROC train: 0.987962	val: 0.767269	test: 0.837318
PRC train: 0.932998	val: 0.602605	test: 0.617898

Epoch: 118
Loss: 0.09288509700966577
ROC train: 0.987643	val: 0.756730	test: 0.861003
PRC train: 0.931852	val: 0.606500	test: 0.641686

Epoch: 119
Loss: 0.09784582447920165
ROC train: 0.987283	val: 0.747165	test: 0.854182
PRC train: 0.931760	val: 0.612760	test: 0.631934

Epoch: 120
Loss: 0.09432922386744558
ROC train: 0.986808	val: 0.746241	test: 0.840616
PRC train: 0.931113	val: 0.625085	test: 0.634681

Early stopping
Best (ROC):	 train: 0.940873	val: 0.912830	test: 0.647098
Best (PRC):	 train: 0.778871	val: 0.620338	test: 0.546149

ROC train: 0.985288	val: 0.701651	test: 0.871873
PRC train: 0.918632	val: 0.584353	test: 0.661485

Epoch: 95
Loss: 0.11102380532909262
ROC train: 0.983913	val: 0.719344	test: 0.868100
PRC train: 0.908792	val: 0.626767	test: 0.644490

Epoch: 96
Loss: 0.11161404964959529
ROC train: 0.985605	val: 0.742270	test: 0.841792
PRC train: 0.920579	val: 0.643053	test: 0.614899

Epoch: 97
Loss: 0.10951301200867251
ROC train: 0.986311	val: 0.769866	test: 0.844822
PRC train: 0.924379	val: 0.611667	test: 0.625431

Epoch: 98
Loss: 0.110224140401772
ROC train: 0.986556	val: 0.784263	test: 0.865582
PRC train: 0.925522	val: 0.619591	test: 0.624946

Epoch: 99
Loss: 0.10663878619963134
ROC train: 0.987180	val: 0.785325	test: 0.866725
PRC train: 0.929196	val: 0.654368	test: 0.640787

Epoch: 100
Loss: 0.0964686485765056
ROC train: 0.986135	val: 0.799810	test: 0.864376
PRC train: 0.923797	val: 0.672232	test: 0.631115

Epoch: 101
Loss: 0.10948836088731115
ROC train: 0.986369	val: 0.795951	test: 0.860054
PRC train: 0.923158	val: 0.674471	test: 0.632715

Epoch: 102
Loss: 0.12182695703864439
ROC train: 0.986369	val: 0.779344	test: 0.860529
PRC train: 0.924151	val: 0.658371	test: 0.660963

Epoch: 103
Loss: 0.09314790567969837
ROC train: 0.986466	val: 0.750574	test: 0.832809
PRC train: 0.926168	val: 0.630907	test: 0.655949

Epoch: 104
Loss: 0.11110632102150193
ROC train: 0.985363	val: 0.784376	test: 0.832697
PRC train: 0.917578	val: 0.611784	test: 0.629192

Epoch: 105
Loss: 0.09571272738847295
ROC train: 0.986029	val: 0.811634	test: 0.877681
PRC train: 0.920722	val: 0.676250	test: 0.635619

Epoch: 106
Loss: 0.11299159329603055
ROC train: 0.987012	val: 0.800983	test: 0.880210
PRC train: 0.925274	val: 0.684627	test: 0.633570

Epoch: 107
Loss: 0.10284806414010342
ROC train: 0.987017	val: 0.798137	test: 0.873296
PRC train: 0.927840	val: 0.678936	test: 0.629531

Epoch: 108
Loss: 0.1063488248086896
ROC train: 0.987614	val: 0.758740	test: 0.863166
PRC train: 0.928967	val: 0.622292	test: 0.638694

Epoch: 109
Loss: 0.09983380507051676
ROC train: 0.987033	val: 0.762037	test: 0.853547
PRC train: 0.927296	val: 0.662877	test: 0.640325

Epoch: 110
Loss: 0.09892136267951263
ROC train: 0.986968	val: 0.738798	test: 0.855370
PRC train: 0.927607	val: 0.641996	test: 0.634013

Epoch: 111
Loss: 0.1001884112668077
ROC train: 0.986707	val: 0.750711	test: 0.863577
PRC train: 0.925915	val: 0.658938	test: 0.638133

Epoch: 112
Loss: 0.08749648613088896
ROC train: 0.986900	val: 0.771402	test: 0.876019
PRC train: 0.929340	val: 0.632972	test: 0.644432

Epoch: 113
Loss: 0.09281433056211616
ROC train: 0.986301	val: 0.781104	test: 0.865425
PRC train: 0.926255	val: 0.642156	test: 0.653196

Epoch: 114
Loss: 0.09141946968539251
ROC train: 0.985605	val: 0.771939	test: 0.864152
PRC train: 0.922770	val: 0.620685	test: 0.660572

Epoch: 115
Loss: 0.09178440639075573
ROC train: 0.987133	val: 0.757117	test: 0.872534
PRC train: 0.927936	val: 0.618198	test: 0.671895

Epoch: 116
Loss: 0.09102752996296837
ROC train: 0.986692	val: 0.760051	test: 0.851060
PRC train: 0.927910	val: 0.621360	test: 0.656503

Epoch: 117
Loss: 0.132816940227805
ROC train: 0.986138	val: 0.783090	test: 0.822216
PRC train: 0.925348	val: 0.650498	test: 0.627583

Epoch: 118
Loss: 0.09456413443915952
ROC train: 0.977662	val: 0.854615	test: 0.719645
PRC train: 0.895422	val: 0.641687	test: 0.553297

Epoch: 119
Loss: 0.0944994297886318
ROC train: 0.979647	val: 0.825621	test: 0.727403
PRC train: 0.900748	val: 0.627663	test: 0.557987

Epoch: 120
Loss: 0.09188859625642636
ROC train: 0.985854	val: 0.793804	test: 0.780007
PRC train: 0.918894	val: 0.636244	test: 0.591991

Early stopping
Best (ROC):	 train: 0.961022	val: 0.887493	test: 0.741459
Best (PRC):	 train: 0.839066	val: 0.628295	test: 0.559684
All runs completed.

ROC train: 0.999933	val: 0.824770	test: 0.586922
PRC train: 0.999312	val: 0.687131	test: 0.532343

Epoch: 94
Loss: 0.02564478569483699
ROC train: 0.999976	val: 0.828629	test: 0.584285
PRC train: 0.999742	val: 0.671639	test: 0.530996

Epoch: 95
Loss: 0.02819433043016408
ROC train: 1.000000	val: 0.866264	test: 0.577371
PRC train: 1.000000	val: 0.640516	test: 0.525540

Epoch: 96
Loss: 0.02626793385853417
ROC train: 1.000000	val: 0.876329	test: 0.604646
PRC train: 1.000000	val: 0.646171	test: 0.530827

Epoch: 97
Loss: 0.030269580713708798
ROC train: 1.000000	val: 0.871659	test: 0.618807
PRC train: 1.000000	val: 0.630049	test: 0.538537

Epoch: 98
Loss: 0.03199272141838013
ROC train: 1.000000	val: 0.887880	test: 0.590657
PRC train: 1.000000	val: 0.657649	test: 0.531973

Epoch: 99
Loss: 0.02408252619023718
ROC train: 1.000000	val: 0.925041	test: 0.610171
PRC train: 1.000000	val: 0.735361	test: 0.539484

Epoch: 100
Loss: 0.020468242528826248
ROC train: 0.999990	val: 0.922606	test: 0.616080
PRC train: 0.999894	val: 0.758527	test: 0.542830

Epoch: 101
Loss: 0.02033953112431606
ROC train: 1.000000	val: 0.896271	test: 0.618695
PRC train: 1.000000	val: 0.737795	test: 0.539012

Epoch: 102
Loss: 0.01469287789656778
ROC train: 1.000000	val: 0.885894	test: 0.605296
PRC train: 1.000000	val: 0.691587	test: 0.541134

Epoch: 103
Loss: 0.020341568926325666
ROC train: 0.999995	val: 0.890452	test: 0.592992
PRC train: 0.999946	val: 0.660157	test: 0.538630

Epoch: 104
Loss: 0.021772934279206436
ROC train: 0.999995	val: 0.870573	test: 0.605009
PRC train: 0.999946	val: 0.680909	test: 0.538364

Epoch: 105
Loss: 0.019552829173324838
ROC train: 1.000000	val: 0.836433	test: 0.632291
PRC train: 1.000000	val: 0.662871	test: 0.546777

Epoch: 106
Loss: 0.01733209120049228
ROC train: 1.000000	val: 0.847896	test: 0.639299
PRC train: 1.000000	val: 0.696384	test: 0.540646

Epoch: 107
Loss: 0.01955832382190207
ROC train: 1.000000	val: 0.856200	test: 0.640348
PRC train: 1.000000	val: 0.750841	test: 0.543548

Epoch: 108
Loss: 0.028204712324489966
ROC train: 1.000000	val: 0.850356	test: 0.644670
PRC train: 1.000000	val: 0.757193	test: 0.547852

Epoch: 109
Loss: 0.028740846090129345
ROC train: 1.000000	val: 0.856875	test: 0.654494
PRC train: 1.000000	val: 0.685556	test: 0.548119

Epoch: 110
Loss: 0.018545221946683677
ROC train: 1.000000	val: 0.880138	test: 0.664306
PRC train: 1.000000	val: 0.669535	test: 0.548685

Epoch: 111
Loss: 0.015323822169685235
ROC train: 1.000000	val: 0.880750	test: 0.665046
PRC train: 1.000000	val: 0.726929	test: 0.556899

Epoch: 112
Loss: 0.0226381604607915
ROC train: 1.000000	val: 0.868450	test: 0.649417
PRC train: 1.000000	val: 0.643443	test: 0.560998

Epoch: 113
Loss: 0.022047392720043555
ROC train: 1.000000	val: 0.864817	test: 0.657273
PRC train: 1.000000	val: 0.629574	test: 0.588639

Epoch: 114
Loss: 0.01620133819959964
ROC train: 1.000000	val: 0.873145	test: 0.680047
PRC train: 1.000000	val: 0.702788	test: 0.577999

Epoch: 115
Loss: 0.014908621549959011
ROC train: 1.000000	val: 0.854689	test: 0.692400
PRC train: 1.000000	val: 0.652219	test: 0.598014

Epoch: 116
Loss: 0.013740453752538576
ROC train: 1.000000	val: 0.848258	test: 0.665681
PRC train: 1.000000	val: 0.652835	test: 0.600398

Epoch: 117
Loss: 0.018792705847948905
ROC train: 1.000000	val: 0.842938	test: 0.639235
PRC train: 1.000000	val: 0.697820	test: 0.593717

Epoch: 118
Loss: 0.016600126910410257
ROC train: 1.000000	val: 0.862793	test: 0.668979
PRC train: 1.000000	val: 0.683282	test: 0.591027

Epoch: 119
Loss: 0.014911832302820366
ROC train: 1.000000	val: 0.859633	test: 0.671952
PRC train: 1.000000	val: 0.649553	test: 0.592624

Epoch: 120
Loss: 0.014633328247318129
ROC train: 1.000000	val: 0.853428	test: 0.665819
PRC train: 1.000000	val: 0.673187	test: 0.594077

Early stopping
Best (ROC):	 train: 1.000000	val: 0.933707	test: 0.599413
Best (PRC):	 train: 1.000000	val: 0.696311	test: 0.536286

ROC train: 0.999990	val: 0.731844	test: 0.513706
PRC train: 0.999892	val: 0.542244	test: 0.517198

Epoch: 94
Loss: 0.009666482060390225
ROC train: 0.999985	val: 0.711167	test: 0.502482
PRC train: 0.999892	val: 0.554986	test: 0.517883

Epoch: 95
Loss: 0.01300559094414199
ROC train: 1.000000	val: 0.742171	test: 0.482819
PRC train: 1.000000	val: 0.603374	test: 0.531515

Epoch: 96
Loss: 0.01446317762782708
ROC train: 1.000000	val: 0.759865	test: 0.479465
PRC train: 1.000000	val: 0.584749	test: 0.530648

Epoch: 97
Loss: 0.00947473595125622
ROC train: 1.000000	val: 0.759777	test: 0.477048
PRC train: 1.000000	val: 0.557780	test: 0.522013

Epoch: 98
Loss: 0.012813408509155782
ROC train: 1.000000	val: 0.775724	test: 0.465618
PRC train: 1.000000	val: 0.611057	test: 0.513466

Epoch: 99
Loss: 0.009816224939005339
ROC train: 1.000000	val: 0.757531	test: 0.469316
PRC train: 1.000000	val: 0.646788	test: 0.513491

Epoch: 100
Loss: 0.010359941891193748
ROC train: 1.000000	val: 0.752749	test: 0.476637
PRC train: 1.000000	val: 0.646500	test: 0.513958

Epoch: 101
Loss: 0.01035000583465015
ROC train: 1.000000	val: 0.762637	test: 0.483096
PRC train: 1.000000	val: 0.590227	test: 0.520742

Epoch: 102
Loss: 0.016495337763525596
ROC train: 1.000000	val: 0.760627	test: 0.478699
PRC train: 1.000000	val: 0.596809	test: 0.514654

Epoch: 103
Loss: 0.013548509957644575
ROC train: 0.999995	val: 0.782281	test: 0.483731
PRC train: 0.999946	val: 0.640804	test: 0.511442

Epoch: 104
Loss: 0.009775542589702063
ROC train: 0.999984	val: 0.768921	test: 0.484878
PRC train: 0.999945	val: 0.685995	test: 0.510619

Epoch: 105
Loss: 0.011093316263070314
ROC train: 1.000000	val: 0.773366	test: 0.492942
PRC train: 1.000000	val: 0.664295	test: 0.511422

Epoch: 106
Loss: 0.011397876886488623
ROC train: 1.000000	val: 0.775214	test: 0.494660
PRC train: 1.000000	val: 0.656049	test: 0.511745

Epoch: 107
Loss: 0.009945222105498395
ROC train: 1.000000	val: 0.758519	test: 0.483693
PRC train: 1.000000	val: 0.639792	test: 0.509104

Epoch: 108
Loss: 0.006192820159614158
ROC train: 1.000000	val: 0.760778	test: 0.487903
PRC train: 1.000000	val: 0.603602	test: 0.509939

Epoch: 109
Loss: 0.010612752863732432
ROC train: 0.999989	val: 0.739501	test: 0.500106
PRC train: 0.999999	val: 0.596526	test: 0.510698

Epoch: 110
Loss: 0.009193168596198056
ROC train: 1.000000	val: 0.748254	test: 0.504055
PRC train: 1.000000	val: 0.577394	test: 0.512266

Epoch: 111
Loss: 0.008884895819973497
ROC train: 1.000000	val: 0.750015	test: 0.491751
PRC train: 1.000000	val: 0.613123	test: 0.510253

Epoch: 112
Loss: 0.00725511369414554
ROC train: 1.000000	val: 0.710569	test: 0.472170
PRC train: 1.000000	val: 0.664840	test: 0.506131

Epoch: 113
Loss: 0.008539544398323383
ROC train: 1.000000	val: 0.705063	test: 0.481908
PRC train: 1.000000	val: 0.663097	test: 0.507810

Epoch: 114
Loss: 0.005505625117436948
ROC train: 1.000000	val: 0.717836	test: 0.482595
PRC train: 1.000000	val: 0.640876	test: 0.509321

Epoch: 115
Loss: 0.006222618608468341
ROC train: 0.999994	val: 0.716775	test: 0.478221
PRC train: 1.000000	val: 0.632893	test: 0.507710

Epoch: 116
Loss: 0.0095984718133406
ROC train: 0.999994	val: 0.692201	test: 0.474699
PRC train: 1.000000	val: 0.617808	test: 0.507535

Epoch: 117
Loss: 0.0063667123144913376
ROC train: 1.000000	val: 0.695135	test: 0.477784
PRC train: 1.000000	val: 0.604177	test: 0.508369

Epoch: 118
Loss: 0.009142018813391663
ROC train: 1.000000	val: 0.697957	test: 0.490040
PRC train: 1.000000	val: 0.620046	test: 0.510487

Epoch: 119
Loss: 0.006750338861871178
ROC train: 1.000000	val: 0.687668	test: 0.471251
PRC train: 1.000000	val: 0.632271	test: 0.506945

Epoch: 120
Loss: 0.008322878553280122
ROC train: 1.000000	val: 0.694485	test: 0.477022
PRC train: 1.000000	val: 0.622786	test: 0.507270

Early stopping
Best (ROC):	 train: 0.999995	val: 0.829992	test: 0.562818
Best (PRC):	 train: 0.999946	val: 0.608233	test: 0.534028

ROC train: 1.000000	val: 0.847612	test: 0.582914
PRC train: 1.000000	val: 0.604371	test: 0.524479

Epoch: 94
Loss: 0.024963122733141295
ROC train: 1.000000	val: 0.807966	test: 0.552049
PRC train: 1.000000	val: 0.579700	test: 0.519723

Epoch: 95
Loss: 0.025427809100675193
ROC train: 1.000000	val: 0.804044	test: 0.545001
PRC train: 1.000000	val: 0.621265	test: 0.518731

Epoch: 96
Loss: 0.025950692796894537
ROC train: 1.000000	val: 0.830241	test: 0.549741
PRC train: 1.000000	val: 0.637696	test: 0.519308

Epoch: 97
Loss: 0.023732283023400896
ROC train: 1.000000	val: 0.856913	test: 0.584289
PRC train: 1.000000	val: 0.677119	test: 0.524277

Epoch: 98
Loss: 0.02390352831753411
ROC train: 0.999861	val: 0.837147	test: 0.548101
PRC train: 0.998578	val: 0.743511	test: 0.519437

Epoch: 99
Loss: 0.02315979293303059
ROC train: 0.999927	val: 0.849172	test: 0.485131
PRC train: 0.999337	val: 0.653943	test: 0.509347

Epoch: 100
Loss: 0.023967690642012592
ROC train: 0.999990	val: 0.854678	test: 0.477523
PRC train: 0.999892	val: 0.644085	test: 0.507129

Epoch: 101
Loss: 0.020404100509022582
ROC train: 1.000000	val: 0.828007	test: 0.500106
PRC train: 1.000000	val: 0.632966	test: 0.509782

Epoch: 102
Loss: 0.020936204979418817
ROC train: 1.000000	val: 0.782330	test: 0.518488
PRC train: 1.000000	val: 0.617360	test: 0.513055

Epoch: 103
Loss: 0.02057050115740204
ROC train: 0.999922	val: 0.772804	test: 0.528312
PRC train: 0.999571	val: 0.614930	test: 0.515501

Epoch: 104
Loss: 0.023690879510693968
ROC train: 1.000000	val: 0.840394	test: 0.547190
PRC train: 1.000000	val: 0.608727	test: 0.519661

Epoch: 105
Loss: 0.024676001894939733
ROC train: 1.000000	val: 0.837234	test: 0.555684
PRC train: 1.000000	val: 0.588570	test: 0.520456

Epoch: 106
Loss: 0.012602784552055397
ROC train: 1.000000	val: 0.838746	test: 0.537934
PRC train: 1.000000	val: 0.589237	test: 0.517048

Epoch: 107
Loss: 0.025023702326549
ROC train: 1.000000	val: 0.843191	test: 0.521894
PRC train: 1.000000	val: 0.592006	test: 0.514063

Epoch: 108
Loss: 0.01571753802183063
ROC train: 1.000000	val: 0.859461	test: 0.524569
PRC train: 1.000000	val: 0.641349	test: 0.516100

Epoch: 109
Loss: 0.018970614700273523
ROC train: 1.000000	val: 0.853505	test: 0.531501
PRC train: 1.000000	val: 0.668087	test: 0.516859

Epoch: 110
Loss: 0.01593798945012135
ROC train: 1.000000	val: 0.842492	test: 0.547754
PRC train: 1.000000	val: 0.637367	test: 0.517912

Epoch: 111
Loss: 0.011663449163857233
ROC train: 1.000000	val: 0.830217	test: 0.550114
PRC train: 1.000000	val: 0.617064	test: 0.518065

Epoch: 112
Loss: 0.022344921172868674
ROC train: 1.000000	val: 0.857339	test: 0.541908
PRC train: 1.000000	val: 0.617024	test: 0.517263

Epoch: 113
Loss: 0.011794306858507533
ROC train: 1.000000	val: 0.814084	test: 0.503397
PRC train: 1.000000	val: 0.588518	test: 0.510690

Epoch: 114
Loss: 0.015895080961111658
ROC train: 1.000000	val: 0.758080	test: 0.476970
PRC train: 1.000000	val: 0.574691	test: 0.505570

Epoch: 115
Loss: 0.014641095487575657
ROC train: 1.000000	val: 0.741247	test: 0.477750
PRC train: 1.000000	val: 0.572218	test: 0.507158

Epoch: 116
Loss: 0.012612946285677046
ROC train: 1.000000	val: 0.727500	test: 0.487918
PRC train: 1.000000	val: 0.573837	test: 0.509262

Epoch: 117
Loss: 0.02014565177183412
ROC train: 1.000000	val: 0.687717	test: 0.494938
PRC train: 1.000000	val: 0.562680	test: 0.510071

Epoch: 118
Loss: 0.014671976795918595
ROC train: 1.000000	val: 0.672832	test: 0.481658
PRC train: 1.000000	val: 0.602478	test: 0.506980

Epoch: 119
Loss: 0.010641528503782922
ROC train: 1.000000	val: 0.709118	test: 0.482308
PRC train: 1.000000	val: 0.609909	test: 0.507715

Epoch: 120
Loss: 0.011282192458417791
ROC train: 1.000000	val: 0.744656	test: 0.492613
PRC train: 1.000000	val: 0.617523	test: 0.509372

Early stopping
Best (ROC):	 train: 0.996235	val: 0.910531	test: 0.547014
Best (PRC):	 train: 0.984777	val: 0.700824	test: 0.519328

ROC train: 0.999830	val: 0.795427	test: 0.742508
PRC train: 0.999304	val: 0.633061	test: 0.554174

Epoch: 94
Loss: 0.019518197822627686
ROC train: 0.999973	val: 0.765871	test: 0.760983
PRC train: 0.999891	val: 0.622730	test: 0.561522

Epoch: 95
Loss: 0.02303153329302105
ROC train: 1.000000	val: 0.775510	test: 0.775338
PRC train: 1.000000	val: 0.634696	test: 0.576602

Epoch: 96
Loss: 0.029769291388766946
ROC train: 0.999994	val: 0.811873	test: 0.722898
PRC train: 1.000000	val: 0.636279	test: 0.550843

Epoch: 97
Loss: 0.024272849880012475
ROC train: 0.999994	val: 0.812773	test: 0.702798
PRC train: 1.000000	val: 0.627048	test: 0.544149

Epoch: 98
Loss: 0.016812440242045146
ROC train: 0.999990	val: 0.791632	test: 0.738519
PRC train: 0.999946	val: 0.635371	test: 0.554727

Epoch: 99
Loss: 0.025248297167402657
ROC train: 1.000000	val: 0.784165	test: 0.740962
PRC train: 1.000000	val: 0.646852	test: 0.557727

Epoch: 100
Loss: 0.014202570814945522
ROC train: 0.999995	val: 0.760515	test: 0.725484
PRC train: 0.999946	val: 0.635632	test: 0.552648

Epoch: 101
Loss: 0.01691050447394811
ROC train: 1.000000	val: 0.726899	test: 0.748167
PRC train: 1.000000	val: 0.627671	test: 0.556021

Epoch: 102
Loss: 0.013838540345027933
ROC train: 1.000000	val: 0.753933	test: 0.744869
PRC train: 1.000000	val: 0.643639	test: 0.552966

Epoch: 103
Loss: 0.016596294756242636
ROC train: 0.999995	val: 0.789721	test: 0.711828
PRC train: 0.999946	val: 0.660597	test: 0.545617

Epoch: 104
Loss: 0.017037889246949784
ROC train: 1.000000	val: 0.827244	test: 0.708805
PRC train: 1.000000	val: 0.592388	test: 0.544596

Epoch: 105
Loss: 0.012980267997231293
ROC train: 0.999995	val: 0.837410	test: 0.729580
PRC train: 0.999946	val: 0.578636	test: 0.550801

Epoch: 106
Loss: 0.014921762420790703
ROC train: 1.000000	val: 0.829468	test: 0.757136
PRC train: 1.000000	val: 0.641804	test: 0.557267

Epoch: 107
Loss: 0.02381051793106962
ROC train: 0.999990	val: 0.752872	test: 0.782282
PRC train: 0.999892	val: 0.631115	test: 0.568369

Epoch: 108
Loss: 0.02111414123305102
ROC train: 1.000000	val: 0.754070	test: 0.760460
PRC train: 1.000000	val: 0.636127	test: 0.562180

Epoch: 109
Loss: 0.013512206769922448
ROC train: 1.000000	val: 0.743556	test: 0.701775
PRC train: 1.000000	val: 0.553577	test: 0.545168

Epoch: 110
Loss: 0.020505042811606516
ROC train: 1.000000	val: 0.732156	test: 0.714254
PRC train: 1.000000	val: 0.544487	test: 0.548586

Epoch: 111
Loss: 0.014482968524975331
ROC train: 0.999975	val: 0.730733	test: 0.782244
PRC train: 0.999781	val: 0.617793	test: 0.571978

Epoch: 112
Loss: 0.025028541894073164
ROC train: 1.000000	val: 0.742396	test: 0.790977
PRC train: 1.000000	val: 0.619617	test: 0.577168

Epoch: 113
Loss: 0.02594864806046764
ROC train: 0.999995	val: 0.728263	test: 0.693654
PRC train: 0.999946	val: 0.545794	test: 0.545306

Epoch: 114
Loss: 0.023604142366721363
ROC train: 0.999990	val: 0.737652	test: 0.679075
PRC train: 0.999894	val: 0.547068	test: 0.540079

Epoch: 115
Loss: 0.031831503642761064
ROC train: 1.000000	val: 0.781930	test: 0.714803
PRC train: 1.000000	val: 0.551949	test: 0.558588

Epoch: 116
Loss: 0.024944378480810842
ROC train: 0.999907	val: 0.757180	test: 0.736378
PRC train: 0.999025	val: 0.553849	test: 0.575564

Epoch: 117
Loss: 0.028493716903452572
ROC train: 0.999986	val: 0.780394	test: 0.755865
PRC train: 0.999842	val: 0.562540	test: 0.574369

Epoch: 118
Loss: 0.01725545162574062
ROC train: 0.999962	val: 0.812074	test: 0.727783
PRC train: 0.999571	val: 0.559821	test: 0.556959

Epoch: 119
Loss: 0.017373905115959827
ROC train: 1.000000	val: 0.827469	test: 0.728433
PRC train: 1.000000	val: 0.571024	test: 0.558966

Epoch: 120
Loss: 0.010186262394454096
ROC train: 1.000000	val: 0.839094	test: 0.734929
PRC train: 1.000000	val: 0.621953	test: 0.565407

Early stopping
Best (ROC):	 train: 0.850473	val: 0.857275	test: 0.569000
Best (PRC):	 train: 0.662212	val: 0.568544	test: 0.520568

ROC train: 1.000000	val: 0.770818	test: 0.670249
PRC train: 1.000000	val: 0.573232	test: 0.591423

Epoch: 94
Loss: 0.012365405239570372
ROC train: 1.000000	val: 0.754137	test: 0.676177
PRC train: 1.000000	val: 0.550372	test: 0.585425

Epoch: 95
Loss: 0.017310781536484743
ROC train: 1.000000	val: 0.731024	test: 0.660982
PRC train: 1.000000	val: 0.537556	test: 0.580066

Epoch: 96
Loss: 0.02019044950789409
ROC train: 1.000000	val: 0.645903	test: 0.611116
PRC train: 1.000000	val: 0.519476	test: 0.573257

Epoch: 97
Loss: 0.02228518070673325
ROC train: 0.999989	val: 0.640059	test: 0.621097
PRC train: 0.999999	val: 0.522609	test: 0.574640

Epoch: 98
Loss: 0.019937026583787848
ROC train: 1.000000	val: 0.675421	test: 0.671437
PRC train: 1.000000	val: 0.534637	test: 0.583636

Epoch: 99
Loss: 0.011840100626053512
ROC train: 1.000000	val: 0.689094	test: 0.665633
PRC train: 1.000000	val: 0.540180	test: 0.589427

Epoch: 100
Loss: 0.011725879716219731
ROC train: 1.000000	val: 0.704665	test: 0.645552
PRC train: 1.000000	val: 0.542269	test: 0.580787

Epoch: 101
Loss: 0.013533315821457448
ROC train: 1.000000	val: 0.734233	test: 0.648394
PRC train: 1.000000	val: 0.551658	test: 0.586751

Epoch: 102
Loss: 0.013735412621865182
ROC train: 1.000000	val: 0.793368	test: 0.666768
PRC train: 1.000000	val: 0.576945	test: 0.592138

Epoch: 103
Loss: 0.013229852753751974
ROC train: 1.000000	val: 0.798875	test: 0.679117
PRC train: 1.000000	val: 0.608854	test: 0.592485

Epoch: 104
Loss: 0.014716186214490275
ROC train: 1.000000	val: 0.776623	test: 0.666854
PRC train: 1.000000	val: 0.572626	test: 0.586805

Epoch: 105
Loss: 0.024975005084839193
ROC train: 1.000000	val: 0.779783	test: 0.654737
PRC train: 1.000000	val: 0.574500	test: 0.589572

Epoch: 106
Loss: 0.012137748856456035
ROC train: 1.000000	val: 0.767533	test: 0.633128
PRC train: 1.000000	val: 0.585136	test: 0.594793

Epoch: 107
Loss: 0.02256886660621273
ROC train: 0.999976	val: 0.750401	test: 0.610604
PRC train: 0.999742	val: 0.560234	test: 0.591492

Epoch: 108
Loss: 0.01919548301498212
ROC train: 0.999986	val: 0.752650	test: 0.617018
PRC train: 0.999842	val: 0.565312	test: 0.575630

Epoch: 109
Loss: 0.012637195375522608
ROC train: 1.000000	val: 0.766735	test: 0.630513
PRC train: 1.000000	val: 0.564837	test: 0.579361

Epoch: 110
Loss: 0.014925847963912822
ROC train: 1.000000	val: 0.740713	test: 0.619722
PRC train: 1.000000	val: 0.559999	test: 0.580028

Epoch: 111
Loss: 0.009549714541741392
ROC train: 1.000000	val: 0.726941	test: 0.598072
PRC train: 1.000000	val: 0.537732	test: 0.573305

Epoch: 112
Loss: 0.012481207964901328
ROC train: 1.000000	val: 0.718476	test: 0.608602
PRC train: 1.000000	val: 0.533221	test: 0.574309

Epoch: 113
Loss: 0.009269927125657903
ROC train: 1.000000	val: 0.708998	test: 0.615636
PRC train: 1.000000	val: 0.543653	test: 0.576800

Epoch: 114
Loss: 0.006523932364214576
ROC train: 1.000000	val: 0.659977	test: 0.608016
PRC train: 1.000000	val: 0.527714	test: 0.575875

Epoch: 115
Loss: 0.007199748713792216
ROC train: 1.000000	val: 0.623890	test: 0.618314
PRC train: 1.000000	val: 0.517747	test: 0.576122

Epoch: 116
Loss: 0.009190921213235009
ROC train: 1.000000	val: 0.611640	test: 0.613324
PRC train: 1.000000	val: 0.516459	test: 0.575696

Epoch: 117
Loss: 0.007834336864660457
ROC train: 1.000000	val: 0.653170	test: 0.611613
PRC train: 1.000000	val: 0.521656	test: 0.575804

Epoch: 118
Loss: 0.008188793073495043
ROC train: 1.000000	val: 0.724081	test: 0.637995
PRC train: 1.000000	val: 0.544605	test: 0.581153

Epoch: 119
Loss: 0.008018614106564025
ROC train: 1.000000	val: 0.764725	test: 0.667795
PRC train: 1.000000	val: 0.573232	test: 0.588685

Epoch: 120
Loss: 0.008457550581932118
ROC train: 1.000000	val: 0.776687	test: 0.695369
PRC train: 1.000000	val: 0.578755	test: 0.595173

Early stopping
Best (ROC):	 train: 0.887188	val: 0.856801	test: 0.519601
Best (PRC):	 train: 0.773247	val: 0.691798	test: 0.517644

ROC train: 1.000000	val: 0.820785	test: 0.683334
PRC train: 1.000000	val: 0.573789	test: 0.538646

Epoch: 94
Loss: 0.01553162303630784
ROC train: 0.999983	val: 0.817264	test: 0.655907
PRC train: 0.999999	val: 0.569087	test: 0.535186

Epoch: 95
Loss: 0.023698024942301932
ROC train: 0.999956	val: 0.848346	test: 0.624783
PRC train: 0.999891	val: 0.574695	test: 0.529757

Epoch: 96
Loss: 0.020030662352557165
ROC train: 1.000000	val: 0.827480	test: 0.622116
PRC train: 1.000000	val: 0.571458	test: 0.529717

Epoch: 97
Loss: 0.02252306353108192
ROC train: 0.999759	val: 0.784299	test: 0.635832
PRC train: 0.999244	val: 0.559626	test: 0.531148

Epoch: 98
Loss: 0.01680991810186705
ROC train: 0.999989	val: 0.823769	test: 0.678463
PRC train: 0.999999	val: 0.586277	test: 0.539191

Epoch: 99
Loss: 0.013507290084877143
ROC train: 1.000000	val: 0.819636	test: 0.663697
PRC train: 1.000000	val: 0.597707	test: 0.535813

Epoch: 100
Loss: 0.018438230649912558
ROC train: 1.000000	val: 0.838753	test: 0.675408
PRC train: 1.000000	val: 0.599408	test: 0.538256

Epoch: 101
Loss: 0.012848181813025685
ROC train: 0.999979	val: 0.827367	test: 0.681403
PRC train: 0.999893	val: 0.575859	test: 0.539134

Epoch: 102
Loss: 0.01455768432651738
ROC train: 1.000000	val: 0.825231	test: 0.688081
PRC train: 1.000000	val: 0.574876	test: 0.540722

Epoch: 103
Loss: 0.01746376855022174
ROC train: 1.000000	val: 0.813880	test: 0.687868
PRC train: 1.000000	val: 0.581852	test: 0.540728

Epoch: 104
Loss: 0.011081802131518225
ROC train: 1.000000	val: 0.825730	test: 0.689142
PRC train: 1.000000	val: 0.586174	test: 0.541087

Epoch: 105
Loss: 0.014224875482758647
ROC train: 1.000000	val: 0.832909	test: 0.687805
PRC train: 1.000000	val: 0.591957	test: 0.539736

Epoch: 106
Loss: 0.009595182548683311
ROC train: 1.000000	val: 0.824356	test: 0.690379
PRC train: 1.000000	val: 0.581066	test: 0.540527

Epoch: 107
Loss: 0.012632403427683455
ROC train: 1.000000	val: 0.820248	test: 0.701760
PRC train: 1.000000	val: 0.576642	test: 0.543866

Epoch: 108
Loss: 0.012418799353883556
ROC train: 1.000000	val: 0.813318	test: 0.705245
PRC train: 1.000000	val: 0.571268	test: 0.544205

Epoch: 109
Loss: 0.01057254685803934
ROC train: 1.000000	val: 0.803141	test: 0.705331
PRC train: 1.000000	val: 0.566351	test: 0.543333

Epoch: 110
Loss: 0.008471878142486365
ROC train: 1.000000	val: 0.810883	test: 0.699848
PRC train: 1.000000	val: 0.567677	test: 0.542470

Epoch: 111
Loss: 0.01517279350490489
ROC train: 1.000000	val: 0.807274	test: 0.682871
PRC train: 1.000000	val: 0.570051	test: 0.540897

Epoch: 112
Loss: 0.01694169866133155
ROC train: 1.000000	val: 0.814154	test: 0.670966
PRC train: 1.000000	val: 0.572014	test: 0.538257

Epoch: 113
Loss: 0.012610743015194575
ROC train: 1.000000	val: 0.815440	test: 0.685295
PRC train: 1.000000	val: 0.573759	test: 0.540845

Epoch: 114
Loss: 0.01067756523323644
ROC train: 1.000000	val: 0.799957	test: 0.694103
PRC train: 1.000000	val: 0.576291	test: 0.544552

Epoch: 115
Loss: 0.011321633701408814
ROC train: 1.000000	val: 0.801243	test: 0.716126
PRC train: 1.000000	val: 0.579652	test: 0.548192

Epoch: 116
Loss: 0.008227861857376222
ROC train: 1.000000	val: 0.804427	test: 0.718113
PRC train: 1.000000	val: 0.575600	test: 0.547684

Epoch: 117
Loss: 0.010552757096688754
ROC train: 1.000000	val: 0.818375	test: 0.693658
PRC train: 1.000000	val: 0.577453	test: 0.541439

Epoch: 118
Loss: 0.010826630825314154
ROC train: 1.000000	val: 0.830200	test: 0.675464
PRC train: 1.000000	val: 0.587655	test: 0.540125

Epoch: 119
Loss: 0.009870996541411051
ROC train: 1.000000	val: 0.830337	test: 0.653089
PRC train: 1.000000	val: 0.590820	test: 0.534946

Epoch: 120
Loss: 0.009069807845859718
ROC train: 1.000000	val: 0.816702	test: 0.645368
PRC train: 1.000000	val: 0.583746	test: 0.533391

Early stopping
Best (ROC):	 train: 1.000000	val: 0.898868	test: 0.697233
Best (PRC):	 train: 1.000000	val: 0.597901	test: 0.543567

ROC train: 1.000000	val: 0.720321	test: 0.599308
PRC train: 1.000000	val: 0.586683	test: 0.535452

Epoch: 94
Loss: 0.015441057302263094
ROC train: 0.999957	val: 0.708896	test: 0.595573
PRC train: 0.999840	val: 0.568763	test: 0.536251

Epoch: 95
Loss: 0.029589291936075224
ROC train: 0.999978	val: 0.698969	test: 0.617420
PRC train: 0.999945	val: 0.553568	test: 0.540363

Epoch: 96
Loss: 0.01610807959065737
ROC train: 1.000000	val: 0.678689	test: 0.648823
PRC train: 1.000000	val: 0.556354	test: 0.548822

Epoch: 97
Loss: 0.015742579443926517
ROC train: 1.000000	val: 0.608812	test: 0.665113
PRC train: 1.000000	val: 0.555350	test: 0.555722

Epoch: 98
Loss: 0.02250660913514375
ROC train: 1.000000	val: 0.632349	test: 0.673794
PRC train: 1.000000	val: 0.558237	test: 0.555154

Epoch: 99
Loss: 0.0209280378245369
ROC train: 1.000000	val: 0.681536	test: 0.685661
PRC train: 1.000000	val: 0.568229	test: 0.554236

Epoch: 100
Loss: 0.017005459471577604
ROC train: 1.000000	val: 0.714501	test: 0.661213
PRC train: 1.000000	val: 0.590441	test: 0.543186

Epoch: 101
Loss: 0.022121168238317522
ROC train: 1.000000	val: 0.693673	test: 0.660821
PRC train: 1.000000	val: 0.582915	test: 0.546632

Epoch: 102
Loss: 0.01620935300950395
ROC train: 1.000000	val: 0.666977	test: 0.620680
PRC train: 1.000000	val: 0.563195	test: 0.542160

Epoch: 103
Loss: 0.01809367663801776
ROC train: 1.000000	val: 0.674768	test: 0.642066
PRC train: 1.000000	val: 0.563656	test: 0.550158

Epoch: 104
Loss: 0.012539017444672926
ROC train: 1.000000	val: 0.620436	test: 0.653808
PRC train: 1.000000	val: 0.529116	test: 0.554280

Epoch: 105
Loss: 0.015489489704811007
ROC train: 1.000000	val: 0.625170	test: 0.677054
PRC train: 1.000000	val: 0.523800	test: 0.551945

Epoch: 106
Loss: 0.008558784566827718
ROC train: 1.000000	val: 0.617903	test: 0.690283
PRC train: 1.000000	val: 0.528844	test: 0.554902

Epoch: 107
Loss: 0.015302991763329147
ROC train: 1.000000	val: 0.666626	test: 0.700437
PRC train: 1.000000	val: 0.562344	test: 0.553965

Epoch: 108
Loss: 0.007561564530658949
ROC train: 1.000000	val: 0.704374	test: 0.714703
PRC train: 1.000000	val: 0.572683	test: 0.556710

Epoch: 109
Loss: 0.010536289509482853
ROC train: 1.000000	val: 0.724703	test: 0.725733
PRC train: 1.000000	val: 0.591633	test: 0.558782

Epoch: 110
Loss: 0.01732988905460913
ROC train: 1.000000	val: 0.757781	test: 0.718019
PRC train: 1.000000	val: 0.606462	test: 0.556502

Epoch: 111
Loss: 0.010518221967432856
ROC train: 1.000000	val: 0.756157	test: 0.715704
PRC train: 1.000000	val: 0.610047	test: 0.558272

Epoch: 112
Loss: 0.007049062877207374
ROC train: 1.000000	val: 0.731921	test: 0.714568
PRC train: 1.000000	val: 0.592889	test: 0.563560

Epoch: 113
Loss: 0.009555538420739165
ROC train: 1.000000	val: 0.699405	test: 0.717653
PRC train: 1.000000	val: 0.576498	test: 0.566277

Epoch: 114
Loss: 0.011064979737592982
ROC train: 1.000000	val: 0.692774	test: 0.705824
PRC train: 1.000000	val: 0.564861	test: 0.561025

Epoch: 115
Loss: 0.012343831551688377
ROC train: 1.000000	val: 0.693136	test: 0.686449
PRC train: 1.000000	val: 0.572274	test: 0.558255

Epoch: 116
Loss: 0.016779291436736732
ROC train: 1.000000	val: 0.691825	test: 0.683513
PRC train: 1.000000	val: 0.587990	test: 0.559705

Epoch: 117
Loss: 0.009531184677958635
ROC train: 1.000000	val: 0.708520	test: 0.678941
PRC train: 1.000000	val: 0.597280	test: 0.561418

Epoch: 118
Loss: 0.009111707356794636
ROC train: 1.000000	val: 0.710443	test: 0.661676
PRC train: 1.000000	val: 0.589992	test: 0.558801

Epoch: 119
Loss: 0.0115741240212592
ROC train: 1.000000	val: 0.719021	test: 0.629049
PRC train: 1.000000	val: 0.580436	test: 0.555427

Epoch: 120
Loss: 0.007851524437787668
ROC train: 1.000000	val: 0.741697	test: 0.624551
PRC train: 1.000000	val: 0.582869	test: 0.553137

Early stopping
Best (ROC):	 train: 0.849034	val: 0.843553	test: 0.551325
Best (PRC):	 train: 0.691128	val: 0.592198	test: 0.541936
All runs completed.

ROC train: 1.000000	val: 0.881213	test: 0.712249
PRC train: 1.000000	val: 0.581396	test: 0.548883

Epoch: 94
Loss: 0.015429142350612365
ROC train: 1.000000	val: 0.887268	test: 0.727227
PRC train: 1.000000	val: 0.589182	test: 0.549781

Epoch: 95
Loss: 0.017001408956090765
ROC train: 0.999989	val: 0.876891	test: 0.731650
PRC train: 0.999999	val: 0.592241	test: 0.553122

Epoch: 96
Loss: 0.02606970473653115
ROC train: 1.000000	val: 0.868675	test: 0.722700
PRC train: 1.000000	val: 0.586137	test: 0.556135

Epoch: 97
Loss: 0.01988388602666306
ROC train: 1.000000	val: 0.831352	test: 0.712039
PRC train: 1.000000	val: 0.599502	test: 0.552645

Epoch: 98
Loss: 0.02510537015951953
ROC train: 1.000000	val: 0.852380	test: 0.695238
PRC train: 1.000000	val: 0.580978	test: 0.546772

Epoch: 99
Loss: 0.013527104781257815
ROC train: 1.000000	val: 0.856601	test: 0.696519
PRC train: 1.000000	val: 0.584331	test: 0.546824

Epoch: 100
Loss: 0.012952550348415559
ROC train: 1.000000	val: 0.863281	test: 0.685470
PRC train: 1.000000	val: 0.580488	test: 0.543865

Epoch: 101
Loss: 0.010059336079112257
ROC train: 1.000000	val: 0.870098	test: 0.693602
PRC train: 1.000000	val: 0.591960	test: 0.548481

Epoch: 102
Loss: 0.019171619088392603
ROC train: 1.000000	val: 0.869399	test: 0.706829
PRC train: 1.000000	val: 0.592826	test: 0.548662

Epoch: 103
Loss: 0.014774297971804736
ROC train: 1.000000	val: 0.861545	test: 0.684230
PRC train: 1.000000	val: 0.578501	test: 0.543889

Epoch: 104
Loss: 0.010936241469764633
ROC train: 0.999995	val: 0.859510	test: 0.675411
PRC train: 0.999946	val: 0.576304	test: 0.542614

Epoch: 105
Loss: 0.013438973326169923
ROC train: 0.999962	val: 0.860273	test: 0.675699
PRC train: 0.999599	val: 0.572258	test: 0.543271

Epoch: 106
Loss: 0.011533919859972198
ROC train: 0.999885	val: 0.845401	test: 0.679572
PRC train: 0.998958	val: 0.560007	test: 0.543964

Epoch: 107
Loss: 0.014884715807773952
ROC train: 0.999938	val: 0.865217	test: 0.707255
PRC train: 0.999379	val: 0.575977	test: 0.549920

Epoch: 108
Loss: 0.014722686597604432
ROC train: 0.999990	val: 0.887943	test: 0.702982
PRC train: 0.999894	val: 0.587863	test: 0.545882

Epoch: 109
Loss: 0.014567950952305692
ROC train: 0.999837	val: 0.877067	test: 0.675912
PRC train: 0.998956	val: 0.580807	test: 0.541245

Epoch: 110
Loss: 0.014290032415619066
ROC train: 1.000000	val: 0.867863	test: 0.712738
PRC train: 1.000000	val: 0.584064	test: 0.550137

Epoch: 111
Loss: 0.011493863144036867
ROC train: 0.999977	val: 0.838781	test: 0.724243
PRC train: 0.999998	val: 0.587412	test: 0.553126

Epoch: 112
Loss: 0.016523414073487213
ROC train: 1.000000	val: 0.834624	test: 0.710744
PRC train: 1.000000	val: 0.568276	test: 0.549106

Epoch: 113
Loss: 0.012911840598702661
ROC train: 1.000000	val: 0.844526	test: 0.694536
PRC train: 1.000000	val: 0.573099	test: 0.546088

Epoch: 114
Loss: 0.012040505671071615
ROC train: 1.000000	val: 0.848297	test: 0.691283
PRC train: 1.000000	val: 0.575772	test: 0.544000

Epoch: 115
Loss: 0.01111962310762496
ROC train: 1.000000	val: 0.822873	test: 0.696811
PRC train: 1.000000	val: 0.573861	test: 0.545038

Epoch: 116
Loss: 0.010759275416703604
ROC train: 1.000000	val: 0.807976	test: 0.691514
PRC train: 1.000000	val: 0.565272	test: 0.542022

Epoch: 117
Loss: 0.010773375930043579
ROC train: 1.000000	val: 0.825396	test: 0.692138
PRC train: 1.000000	val: 0.590977	test: 0.543917

Epoch: 118
Loss: 0.010767395112835806
ROC train: 1.000000	val: 0.849495	test: 0.681257
PRC train: 1.000000	val: 0.577231	test: 0.545225

Epoch: 119
Loss: 0.016778214572730207
ROC train: 1.000000	val: 0.856112	test: 0.696859
PRC train: 1.000000	val: 0.581901	test: 0.548937

Epoch: 120
Loss: 0.011230845261620173
ROC train: 1.000000	val: 0.843613	test: 0.688235
PRC train: 1.000000	val: 0.572181	test: 0.547333

Epoch: 121
Loss: 0.01291424367213174
ROC train: 1.000000	val: 0.819327	test: 0.680447
PRC train: 1.000000	val: 0.556444	test: 0.545150

Epoch: 122
Loss: 0.013259926070694868
ROC train: 1.000000	val: 0.837646	test: 0.679628
PRC train: 1.000000	val: 0.563532	test: 0.546863

Epoch: 123
Loss: 0.016371821307440297
ROC train: 0.999994	val: 0.850483	test: 0.671202
PRC train: 1.000000	val: 0.573606	test: 0.543264

Epoch: 124
Loss: 0.018122858255656484
ROC train: 1.000000	val: 0.856439	test: 0.658786
PRC train: 1.000000	val: 0.570307	test: 0.540443

Epoch: 125
Loss: 0.012831728613844873
ROC train: 0.999995	val: 0.852781	test: 0.650254
PRC train: 0.999946	val: 0.561371	test: 0.537973

Early stopping
Best (ROC):	 train: 1.000000	val: 0.890266	test: 0.724654
Best (PRC):	 train: 1.000000	val: 0.589337	test: 0.552301
All runs completed.

ROC train: 1.000000	val: 0.787275	test: 0.586179
PRC train: 1.000000	val: 0.635426	test: 0.523519

Epoch: 94
Loss: 0.010714628481517236
ROC train: 1.000000	val: 0.786101	test: 0.583968
PRC train: 1.000000	val: 0.634554	test: 0.522707

Epoch: 95
Loss: 0.016561454167969693
ROC train: 1.000000	val: 0.793481	test: 0.590576
PRC train: 1.000000	val: 0.638093	test: 0.525695

Epoch: 96
Loss: 0.0085662576374728
ROC train: 1.000000	val: 0.810064	test: 0.594087
PRC train: 1.000000	val: 0.644948	test: 0.526037

Epoch: 97
Loss: 0.01928711691745553
ROC train: 1.000000	val: 0.787724	test: 0.597684
PRC train: 1.000000	val: 0.633893	test: 0.524467

Epoch: 98
Loss: 0.021436504180532924
ROC train: 1.000000	val: 0.780232	test: 0.598020
PRC train: 1.000000	val: 0.631125	test: 0.523914

Epoch: 99
Loss: 0.006065587319222902
ROC train: 1.000000	val: 0.782218	test: 0.619069
PRC train: 1.000000	val: 0.632462	test: 0.529974

Epoch: 100
Loss: 0.013951762965716333
ROC train: 1.000000	val: 0.810401	test: 0.631111
PRC train: 1.000000	val: 0.639007	test: 0.534419

Epoch: 101
Loss: 0.010814883678013693
ROC train: 1.000000	val: 0.883012	test: 0.611400
PRC train: 1.000000	val: 0.646800	test: 0.531864

Epoch: 102
Loss: 0.010728278870282845
ROC train: 1.000000	val: 0.900593	test: 0.588066
PRC train: 1.000000	val: 0.648728	test: 0.526876

Epoch: 103
Loss: 0.01204840920237096
ROC train: 1.000000	val: 0.849411	test: 0.592499
PRC train: 1.000000	val: 0.635248	test: 0.525844

Epoch: 104
Loss: 0.008707849749282928
ROC train: 1.000000	val: 0.854580	test: 0.613073
PRC train: 1.000000	val: 0.639560	test: 0.529121

Epoch: 105
Loss: 0.012614284453895263
ROC train: 1.000000	val: 0.867241	test: 0.624081
PRC train: 1.000000	val: 0.645647	test: 0.530197

Epoch: 106
Loss: 0.010231374881563532
ROC train: 1.000000	val: 0.860199	test: 0.608412
PRC train: 1.000000	val: 0.640850	test: 0.527989

Epoch: 107
Loss: 0.011107158075857212
ROC train: 1.000000	val: 0.847562	test: 0.597273
PRC train: 1.000000	val: 0.640727	test: 0.526217

Epoch: 108
Loss: 0.01029329087061415
ROC train: 1.000000	val: 0.812499	test: 0.594692
PRC train: 1.000000	val: 0.640515	test: 0.525873

Epoch: 109
Loss: 0.00964286089199667
ROC train: 1.000000	val: 0.789735	test: 0.606626
PRC train: 1.000000	val: 0.645525	test: 0.527004

Epoch: 110
Loss: 0.008996387113633656
ROC train: 1.000000	val: 0.803682	test: 0.600007
PRC train: 1.000000	val: 0.595394	test: 0.528064

Epoch: 111
Loss: 0.007690360004062166
ROC train: 1.000000	val: 0.839318	test: 0.579878
PRC train: 1.000000	val: 0.601331	test: 0.528203

Epoch: 112
Loss: 0.00926190229741312
ROC train: 1.000000	val: 0.846673	test: 0.576161
PRC train: 1.000000	val: 0.597099	test: 0.526658

Epoch: 113
Loss: 0.012226721065822892
ROC train: 1.000000	val: 0.845001	test: 0.582119
PRC train: 1.000000	val: 0.584414	test: 0.526441

Epoch: 114
Loss: 0.00462219876855423
ROC train: 1.000000	val: 0.814758	test: 0.581981
PRC train: 1.000000	val: 0.566076	test: 0.524362

Epoch: 115
Loss: 0.009862287906159596
ROC train: 1.000000	val: 0.789260	test: 0.597583
PRC train: 1.000000	val: 0.563757	test: 0.526823

Epoch: 116
Loss: 0.0050545808478161865
ROC train: 1.000000	val: 0.782604	test: 0.604098
PRC train: 1.000000	val: 0.565561	test: 0.527878

Epoch: 117
Loss: 0.011453491781508365
ROC train: 1.000000	val: 0.782243	test: 0.601580
PRC train: 1.000000	val: 0.570063	test: 0.526506

Epoch: 118
Loss: 0.004371317900918297
ROC train: 1.000000	val: 0.791720	test: 0.596496
PRC train: 1.000000	val: 0.595010	test: 0.526181

Epoch: 119
Loss: 0.01547581430575352
ROC train: 1.000000	val: 0.851382	test: 0.567634
PRC train: 1.000000	val: 0.570503	test: 0.522094

Epoch: 120
Loss: 0.0071208493915251745
ROC train: 1.000000	val: 0.864019	test: 0.544318
PRC train: 1.000000	val: 0.567220	test: 0.518669

Epoch: 121
Loss: 0.007267680935263946
ROC train: 1.000000	val: 0.852869	test: 0.541808
PRC train: 1.000000	val: 0.591355	test: 0.518653

Epoch: 122
Loss: 0.01287797812037853
ROC train: 1.000000	val: 0.838784	test: 0.551863
PRC train: 1.000000	val: 0.635014	test: 0.519940

Epoch: 123
Loss: 0.0055118359268683855
ROC train: 1.000000	val: 0.804307	test: 0.588802
PRC train: 1.000000	val: 0.635042	test: 0.524115

Epoch: 124
Loss: 0.015188886928375413
ROC train: 1.000000	val: 0.803471	test: 0.602454
PRC train: 1.000000	val: 0.632732	test: 0.526128

Epoch: 125
Loss: 0.008475999384121139
ROC train: 1.000000	val: 0.823800	test: 0.601810
PRC train: 1.000000	val: 0.636722	test: 0.525990

Epoch: 126
Loss: 0.013420362287982737
ROC train: 1.000000	val: 0.851533	test: 0.600557
PRC train: 1.000000	val: 0.585573	test: 0.526779

Epoch: 127
Loss: 0.006964457655253583
ROC train: 1.000000	val: 0.854355	test: 0.600631
PRC train: 1.000000	val: 0.570595	test: 0.526183

Epoch: 128
Loss: 0.026738213624117414
ROC train: 1.000000	val: 0.862796	test: 0.603642
PRC train: 1.000000	val: 0.572497	test: 0.526931

Epoch: 129
Loss: 0.007146867820163631
ROC train: 1.000000	val: 0.882724	test: 0.598446
PRC train: 1.000000	val: 0.609821	test: 0.527057

Epoch: 130
Loss: 0.007580741247187625
ROC train: 1.000000	val: 0.885409	test: 0.609450
PRC train: 1.000000	val: 0.650084	test: 0.530827

Epoch: 131
Loss: 0.007687920338842835
ROC train: 1.000000	val: 0.886695	test: 0.621698
PRC train: 1.000000	val: 0.646170	test: 0.532809

Epoch: 132
Loss: 0.013013683452201958
ROC train: 1.000000	val: 0.877379	test: 0.627301
PRC train: 1.000000	val: 0.578536	test: 0.532687

Epoch: 133
Loss: 0.0094132533090592
ROC train: 1.000000	val: 0.861197	test: 0.630125
PRC train: 1.000000	val: 0.597473	test: 0.531183

Epoch: 134
Loss: 0.006702126024042912
ROC train: 1.000000	val: 0.835924	test: 0.611463
PRC train: 1.000000	val: 0.636574	test: 0.526090

Epoch: 135
Loss: 0.012146226852323156
ROC train: 1.000000	val: 0.822700	test: 0.610328
PRC train: 1.000000	val: 0.637228	test: 0.525055

Epoch: 136
Loss: 0.00818239448771658
ROC train: 1.000000	val: 0.797090	test: 0.619995
PRC train: 1.000000	val: 0.636304	test: 0.526500

Epoch: 137
Loss: 0.0062944846504830055
ROC train: 1.000000	val: 0.802709	test: 0.614948
PRC train: 1.000000	val: 0.638696	test: 0.525958

Early stopping
Best (ROC):	 train: 1.000000	val: 0.900593	test: 0.588066
Best (PRC):	 train: 1.000000	val: 0.648728	test: 0.526876
All runs completed.
