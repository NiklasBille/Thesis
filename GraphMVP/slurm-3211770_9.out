>>> Starting run for dataset: tox21
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running RANDOM configs_static_noise_experiments/GraphCL/tox21/noise=0.0.yml on cuda:0
Running RANDOM configs_static_noise_experiments/GraphCL/tox21/noise=0.05.yml on cuda:1
Running RANDOM configs_static_noise_experiments/GraphCL/tox21/noise=0.1.yml on cuda:2
Running RANDOM configs_static_noise_experiments/GraphCL/tox21/noise=0.2.yml on cuda:3
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.05.yml --runseed 4 --device cuda:1
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.05.yml --runseed 5 --device cuda:1
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.05.yml --runseed 6 --device cuda:1
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.1.yml --runseed 4 --device cuda:2
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.2.yml --runseed 4 --device cuda:3
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.1.yml --runseed 5 --device cuda:2
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.2.yml --runseed 5 --device cuda:3
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.2.yml --runseed 6 --device cuda:3
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.0.yml --runseed 4 --device cuda:0
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.1.yml --runseed 6 --device cuda:2
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.0.yml --runseed 5 --device cuda:0
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.0.yml --runseed 6 --device cuda:0
[11:31:04] WARNING: not removing hydrogen atom without neighbors
[11:31:04] [11:31:04] WARNING: not removing hydrogen atom without neighborsWARNING: not removing hydrogen atom without neighbors

[11:31:05] WARNING: not removing hydrogen atom without neighbors
[11:31:05] WARNING: not removing hydrogen atom without neighbors
[11:31:06] WARNING: not removing hydrogen atom without neighbors
[11:31:06] WARNING: not removing hydrogen atom without neighbors
[11:31:06] WARNING: not removing hydrogen atom without neighbors
[11:31:06] WARNING: not removing hydrogen atom without neighbors
[11:31:07] WARNING: not removing hydrogen atom without neighbors
[11:31:07] WARNING: not removing hydrogen atom without neighbors
[11:31:07] WARNING: not removing hydrogen atom without neighbors
[11:31:07] WARNING: not removing hydrogen atom without neighbors
[11:31:07] WARNING: not removing hydrogen atom without neighbors
[11:31:08] WARNING: not removing hydrogen atom without neighbors
[11:31:08] WARNING: not removing hydrogen atom without neighbors
[11:31:08] WARNING: not removing hydrogen atom without neighbors
[11:31:08] WARNING: not removing hydrogen atom without neighbors
[11:31:09] WARNING: not removing hydrogen atom without neighbors
[11:31:09] WARNING: not removing hydrogen atom without neighbors
[11:31:09] WARNING: not removing hydrogen atom without neighbors
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/tox21/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/tox21/noise=0.0/tox21_scaff_6_26-05_11-31-04  ]
[ Using Seed :  6  ]
[ Using device :  cuda:0  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.537722846035848
ROC train: 0.697663	val: 0.615591	test: 0.588340
PRC train: 0.206438	val: 0.191369	test: 0.170880

Epoch: 2
Loss: 0.32321041919507515
ROC train: 0.754855	val: 0.691075	test: 0.627213
PRC train: 0.263736	val: 0.238682	test: 0.225126

Epoch: 3
Loss: 0.23568761855457476
ROC train: 0.796945	val: 0.741981	test: 0.680312
PRC train: 0.323330	val: 0.275014	test: 0.271346

Epoch: 4
Loss: 0.20999451621365584
ROC train: 0.815451	val: 0.754888	test: 0.699136
PRC train: 0.344004	val: 0.290321	test: 0.294532

Epoch: 5
Loss: 0.19978133067942905
ROC train: 0.824099	val: 0.752924	test: 0.709888
PRC train: 0.375038	val: 0.312630	test: 0.307198

Epoch: 6
Loss: 0.19437251638696304
ROC train: 0.832366	val: 0.756217	test: 0.723105
PRC train: 0.386008	val: 0.328004	test: 0.312319

Epoch: 7
Loss: 0.18975832627308
ROC train: 0.841344	val: 0.772427	test: 0.733877
PRC train: 0.407599	val: 0.346116	test: 0.341211

Epoch: 8
Loss: 0.1880367149184186
ROC train: 0.849471	val: 0.759283	test: 0.742237
PRC train: 0.431934	val: 0.351847	test: 0.337299

Epoch: 9
Loss: 0.18309187342338215
ROC train: 0.856483	val: 0.764476	test: 0.734136
PRC train: 0.449753	val: 0.329551	test: 0.348788

Epoch: 10
Loss: 0.18227102427414082
ROC train: 0.860441	val: 0.766853	test: 0.738744
PRC train: 0.462582	val: 0.331564	test: 0.340027

Epoch: 11
Loss: 0.1815022253660455
ROC train: 0.857367	val: 0.758679	test: 0.749484
PRC train: 0.464972	val: 0.332264	test: 0.342667

Epoch: 12
Loss: 0.18002842788804785
ROC train: 0.866226	val: 0.755492	test: 0.739648
PRC train: 0.478220	val: 0.325678	test: 0.338942

Epoch: 13
Loss: 0.17559546478732302
ROC train: 0.868416	val: 0.760592	test: 0.740724
PRC train: 0.479998	val: 0.314204	test: 0.331546

Epoch: 14
Loss: 0.17376104474361964
ROC train: 0.878056	val: 0.765747	test: 0.741819
PRC train: 0.514695	val: 0.359508	test: 0.361099

Epoch: 15
Loss: 0.17347821875763278
ROC train: 0.880819	val: 0.777757	test: 0.739148
PRC train: 0.529324	val: 0.343803	test: 0.356525

Epoch: 16
Loss: 0.16917632260620205
ROC train: 0.884949	val: 0.780828	test: 0.753292
PRC train: 0.537429	val: 0.353970	test: 0.374510

Epoch: 17
Loss: 0.16799658383480462
ROC train: 0.885151	val: 0.781342	test: 0.746764
PRC train: 0.541951	val: 0.342379	test: 0.373470

Epoch: 18
Loss: 0.16817819437369255
ROC train: 0.890606	val: 0.778746	test: 0.753618
PRC train: 0.547181	val: 0.354918	test: 0.374119

Epoch: 19
Loss: 0.16694716175808438
ROC train: 0.890761	val: 0.780970	test: 0.742723
PRC train: 0.553694	val: 0.344137	test: 0.370082

Epoch: 20
Loss: 0.16588341074325427
ROC train: 0.894199	val: 0.779871	test: 0.746252
PRC train: 0.564282	val: 0.345429	test: 0.361440

Epoch: 21
Loss: 0.16455606701709097
ROC train: 0.895823	val: 0.791372	test: 0.755382
PRC train: 0.563266	val: 0.367482	test: 0.373503

Epoch: 22
Loss: 0.16259543047640349
ROC train: 0.897289	val: 0.782770	test: 0.748525
PRC train: 0.583650	val: 0.357298	test: 0.380957

Epoch: 23
Loss: 0.16080687350459652
ROC train: 0.893475	val: 0.783837	test: 0.738534
PRC train: 0.570447	val: 0.350371	test: 0.363950

Epoch: 24
Loss: 0.16045444855216537
ROC train: 0.900528	val: 0.784095	test: 0.737244
PRC train: 0.577252	val: 0.363912	test: 0.351620

Epoch: 25
Loss: 0.16091345227705273
ROC train: 0.901807	val: 0.788480	test: 0.737224
PRC train: 0.583215	val: 0.340212	test: 0.357550

Epoch: 26
Loss: 0.1584844390445646
ROC train: 0.899972	val: 0.790770	test: 0.750467
PRC train: 0.588205	val: 0.359758	test: 0.383090

Epoch: 27
Loss: 0.15932945575768623
ROC train: 0.908045	val: 0.788511	test: 0.744481
PRC train: 0.603356	val: 0.365535	test: 0.375354

Epoch: 28
Loss: 0.1577528607000496
ROC train: 0.909270	val: 0.784187	test: 0.745148
PRC train: 0.605615	val: 0.358437	test: 0.372850

Epoch: 29
Loss: 0.15587114954387382
ROC train: 0.907056	val: 0.770284	test: 0.735302
PRC train: 0.594759	val: 0.333212	test: 0.358173

Epoch: 30
Loss: 0.15250164149268663
ROC train: 0.913354	val: 0.794824	test: 0.743865
PRC train: 0.626229	val: 0.370947	test: 0.366424

Epoch: 31
Loss: 0.15408875728150653
ROC train: 0.913817	val: 0.781486	test: 0.742873
PRC train: 0.625442	val: 0.348993	test: 0.343337

Epoch: 32
Loss: 0.15385918351273056
ROC train: 0.918999	val: 0.775548	test: 0.740818
PRC train: 0.632658	val: 0.338196	test: 0.346271

Epoch: 33
Loss: 0.15178093708629145Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/tox21/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/tox21/noise=0.0/tox21_scaff_5_26-05_11-31-04  ]
[ Using Seed :  5  ]
[ Using device :  cuda:0  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5218651164846883
ROC train: 0.708452	val: 0.640289	test: 0.606483
PRC train: 0.227631	val: 0.207244	test: 0.199862

Epoch: 2
Loss: 0.31702070504776086
ROC train: 0.758190	val: 0.718740	test: 0.670032
PRC train: 0.282874	val: 0.298853	test: 0.253572

Epoch: 3
Loss: 0.23281399643941658
ROC train: 0.791684	val: 0.729803	test: 0.669317
PRC train: 0.304726	val: 0.280236	test: 0.269886

Epoch: 4
Loss: 0.2073363946576328
ROC train: 0.817409	val: 0.751292	test: 0.706790
PRC train: 0.350307	val: 0.306407	test: 0.305552

Epoch: 5
Loss: 0.1973538216688795
ROC train: 0.826201	val: 0.748603	test: 0.725252
PRC train: 0.385122	val: 0.330773	test: 0.324212

Epoch: 6
Loss: 0.19049437454941423
ROC train: 0.837377	val: 0.756882	test: 0.730714
PRC train: 0.402632	val: 0.342129	test: 0.330877

Epoch: 7
Loss: 0.18831033500528302
ROC train: 0.842262	val: 0.759384	test: 0.728945
PRC train: 0.405378	val: 0.351709	test: 0.342081

Epoch: 8
Loss: 0.18489322719586113
ROC train: 0.854554	val: 0.750933	test: 0.728365
PRC train: 0.449772	val: 0.345482	test: 0.347189

Epoch: 9
Loss: 0.18298271028798335
ROC train: 0.856748	val: 0.759430	test: 0.736867
PRC train: 0.448481	val: 0.355550	test: 0.346558

Epoch: 10
Loss: 0.17895580980290737
ROC train: 0.859525	val: 0.758629	test: 0.738155
PRC train: 0.468870	val: 0.340637	test: 0.347833

Epoch: 11
Loss: 0.178561444330801
ROC train: 0.869591	val: 0.759090	test: 0.744853
PRC train: 0.489378	val: 0.330631	test: 0.366912

Epoch: 12
Loss: 0.17620981526200602
ROC train: 0.870555	val: 0.760831	test: 0.745655
PRC train: 0.496815	val: 0.354542	test: 0.360716

Epoch: 13
Loss: 0.1733757844704565
ROC train: 0.874643	val: 0.758001	test: 0.751066
PRC train: 0.502012	val: 0.352613	test: 0.357095

Epoch: 14
Loss: 0.17248086298753149
ROC train: 0.872648	val: 0.747227	test: 0.729126
PRC train: 0.496680	val: 0.315581	test: 0.344117

Epoch: 15
Loss: 0.17062629899638573
ROC train: 0.879236	val: 0.758627	test: 0.745952
PRC train: 0.520148	val: 0.368740	test: 0.360532

Epoch: 16
Loss: 0.16821479480675447
ROC train: 0.885476	val: 0.773136	test: 0.745318
PRC train: 0.531027	val: 0.366518	test: 0.362892

Epoch: 17
Loss: 0.16742066356615884
ROC train: 0.888818	val: 0.763168	test: 0.746827
PRC train: 0.540484	val: 0.365883	test: 0.352314

Epoch: 18
Loss: 0.16727372370987642
ROC train: 0.891301	val: 0.775530	test: 0.750610
PRC train: 0.547624	val: 0.363283	test: 0.369498

Epoch: 19
Loss: 0.16737638266714988
ROC train: 0.893172	val: 0.766555	test: 0.752101
PRC train: 0.553408	val: 0.357017	test: 0.377189

Epoch: 20
Loss: 0.16391792945949263
ROC train: 0.895494	val: 0.767737	test: 0.748082
PRC train: 0.558851	val: 0.356202	test: 0.373094

Epoch: 21
Loss: 0.16216051708266532
ROC train: 0.898574	val: 0.772235	test: 0.750040
PRC train: 0.568253	val: 0.345944	test: 0.364988

Epoch: 22
Loss: 0.16174427849089706
ROC train: 0.900620	val: 0.771556	test: 0.758660
PRC train: 0.580780	val: 0.363347	test: 0.375780

Epoch: 23
Loss: 0.1609740825263448
ROC train: 0.903805	val: 0.776576	test: 0.754685
PRC train: 0.585705	val: 0.376387	test: 0.368194

Epoch: 24
Loss: 0.16091944138851208
ROC train: 0.905615	val: 0.773930	test: 0.749609
PRC train: 0.595579	val: 0.367719	test: 0.377037

Epoch: 25
Loss: 0.15881165392120278
ROC train: 0.905803	val: 0.778654	test: 0.753823
PRC train: 0.589419	val: 0.371545	test: 0.366999

Epoch: 26
Loss: 0.15719794110145152
ROC train: 0.908922	val: 0.766609	test: 0.752699
PRC train: 0.600941	val: 0.361231	test: 0.372398

Epoch: 27
Loss: 0.15740139606919995
ROC train: 0.908872	val: 0.780947	test: 0.751328
PRC train: 0.599969	val: 0.367998	test: 0.366319

Epoch: 28
Loss: 0.1544081139541976
ROC train: 0.908560	val: 0.763951	test: 0.743293
PRC train: 0.607276	val: 0.360596	test: 0.363894

Epoch: 29
Loss: 0.1554769322524721
ROC train: 0.914034	val: 0.779865	test: 0.743293
PRC train: 0.621791	val: 0.371024	test: 0.368263

Epoch: 30
Loss: 0.15212334169840505
ROC train: 0.915024	val: 0.771363	test: 0.748581
PRC train: 0.626609	val: 0.375307	test: 0.356048

Epoch: 31
Loss: 0.1525569974582598
ROC train: 0.917088	val: 0.775149	test: 0.750093
PRC train: 0.631855	val: 0.366375	test: 0.369598

Epoch: 32
Loss: 0.15117389818800106
ROC train: 0.921772	val: 0.777047	test: 0.749331
PRC train: 0.636926	val: 0.378482	test: 0.371620

Epoch: 33
Loss: 0.15074315244281047Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/tox21/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/tox21/noise=0.0/tox21_scaff_4_26-05_11-31-04  ]
[ Using Seed :  4  ]
[ Using device :  cuda:0  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.526239559307266
ROC train: 0.661514	val: 0.576312	test: 0.549644
PRC train: 0.179324	val: 0.168893	test: 0.153176

Epoch: 2
Loss: 0.3182079085310942
ROC train: 0.753030	val: 0.720313	test: 0.661429
PRC train: 0.269079	val: 0.291936	test: 0.254892

Epoch: 3
Loss: 0.2352918741234479
ROC train: 0.796063	val: 0.727423	test: 0.679507
PRC train: 0.324697	val: 0.301630	test: 0.290405

Epoch: 4
Loss: 0.20850187185729724
ROC train: 0.807720	val: 0.745325	test: 0.685055
PRC train: 0.323156	val: 0.295221	test: 0.287531

Epoch: 5
Loss: 0.1975528908044539
ROC train: 0.822897	val: 0.747160	test: 0.711702
PRC train: 0.367745	val: 0.316610	test: 0.318561

Epoch: 6
Loss: 0.19233385405160633
ROC train: 0.835036	val: 0.759770	test: 0.716792
PRC train: 0.399479	val: 0.323998	test: 0.326071

Epoch: 7
Loss: 0.1875741600318485
ROC train: 0.842221	val: 0.756171	test: 0.726409
PRC train: 0.424921	val: 0.323034	test: 0.322688

Epoch: 8
Loss: 0.1877138269779769
ROC train: 0.851702	val: 0.757919	test: 0.717741
PRC train: 0.435983	val: 0.346213	test: 0.331700

Epoch: 9
Loss: 0.18240323187748583
ROC train: 0.857440	val: 0.759019	test: 0.739084
PRC train: 0.444851	val: 0.305290	test: 0.312531

Epoch: 10
Loss: 0.18086811305059983
ROC train: 0.861149	val: 0.773515	test: 0.733449
PRC train: 0.468609	val: 0.337512	test: 0.360522

Epoch: 11
Loss: 0.17736086865024656
ROC train: 0.868667	val: 0.761556	test: 0.741577
PRC train: 0.498496	val: 0.332112	test: 0.352134

Epoch: 12
Loss: 0.17653649877037164
ROC train: 0.870866	val: 0.775101	test: 0.749984
PRC train: 0.507716	val: 0.361804	test: 0.368255

Epoch: 13
Loss: 0.17399342404660495
ROC train: 0.873731	val: 0.773104	test: 0.749825
PRC train: 0.510117	val: 0.348742	test: 0.361453

Epoch: 14
Loss: 0.17396667463621437
ROC train: 0.875095	val: 0.775946	test: 0.748326
PRC train: 0.515619	val: 0.349773	test: 0.358535

Epoch: 15
Loss: 0.17037314809953166
ROC train: 0.879079	val: 0.773679	test: 0.747389
PRC train: 0.522513	val: 0.355342	test: 0.357381

Epoch: 16
Loss: 0.16990890495301578
ROC train: 0.882413	val: 0.773848	test: 0.749349
PRC train: 0.536634	val: 0.351687	test: 0.362504

Epoch: 17
Loss: 0.1680283745125714
ROC train: 0.888201	val: 0.773025	test: 0.755525
PRC train: 0.549974	val: 0.333396	test: 0.367273

Epoch: 18
Loss: 0.16524452787664123
ROC train: 0.888266	val: 0.770043	test: 0.756574
PRC train: 0.561940	val: 0.352523	test: 0.351360

Epoch: 19
Loss: 0.16590793541231913
ROC train: 0.893118	val: 0.772872	test: 0.757521
PRC train: 0.562992	val: 0.356814	test: 0.375024

Epoch: 20
Loss: 0.16542030660712748
ROC train: 0.887699	val: 0.762442	test: 0.757413
PRC train: 0.547286	val: 0.342682	test: 0.364244

Epoch: 21
Loss: 0.16346729541809307
ROC train: 0.892791	val: 0.768796	test: 0.762621
PRC train: 0.569338	val: 0.354002	test: 0.367455

Epoch: 22
Loss: 0.1618522709454104
ROC train: 0.898506	val: 0.777036	test: 0.747903
PRC train: 0.580952	val: 0.363317	test: 0.361958

Epoch: 23
Loss: 0.1606703303155398
ROC train: 0.900200	val: 0.784580	test: 0.767964
PRC train: 0.590229	val: 0.371216	test: 0.375244

Epoch: 24
Loss: 0.15939064081277624
ROC train: 0.901637	val: 0.774398	test: 0.761553
PRC train: 0.601383	val: 0.388534	test: 0.366615

Epoch: 25
Loss: 0.15829446127529964
ROC train: 0.903697	val: 0.774913	test: 0.757430
PRC train: 0.604860	val: 0.362932	test: 0.355619

Epoch: 26
Loss: 0.15806638461690692
ROC train: 0.905578	val: 0.782972	test: 0.757835
PRC train: 0.604598	val: 0.384822	test: 0.370276

Epoch: 27
Loss: 0.1573704874943525
ROC train: 0.907915	val: 0.781219	test: 0.756259
PRC train: 0.606146	val: 0.378640	test: 0.374624

Epoch: 28
Loss: 0.1563083430673848
ROC train: 0.908529	val: 0.771325	test: 0.754143
PRC train: 0.613882	val: 0.347763	test: 0.353256

Epoch: 29
Loss: 0.15578574509600251
ROC train: 0.909779	val: 0.768806	test: 0.766210
PRC train: 0.615815	val: 0.371635	test: 0.359532

Epoch: 30
Loss: 0.1545309974766376
ROC train: 0.914535	val: 0.776648	test: 0.768914
PRC train: 0.631608	val: 0.372987	test: 0.363916

Epoch: 31
Loss: 0.15106326525810967
ROC train: 0.915600	val: 0.773451	test: 0.758416
PRC train: 0.630116	val: 0.367581	test: 0.360813

Epoch: 32
Loss: 0.15096431009048744
ROC train: 0.916908	val: 0.777920	test: 0.757556
PRC train: 0.643110	val: 0.384884	test: 0.369308

Epoch: 33
Loss: 0.15231610598309847Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/tox21/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/tox21/noise=0.1/tox21_scaff_5_26-05_11-31-04  ]
[ Using Seed :  5  ]
[ Using device :  cuda:2  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5609058331962334
ROC train: 0.680465	val: 0.657357	test: 0.605483
PRC train: 0.178559	val: 0.195218	test: 0.160570

Epoch: 2
Loss: 0.3358937066105005
ROC train: 0.726721	val: 0.699261	test: 0.626503
PRC train: 0.236195	val: 0.248205	test: 0.222452

Epoch: 3
Loss: 0.2452678394419977
ROC train: 0.754126	val: 0.734780	test: 0.660547
PRC train: 0.276822	val: 0.302699	test: 0.258691

Epoch: 4
Loss: 0.21788692441004906
ROC train: 0.775222	val: 0.733733	test: 0.667552
PRC train: 0.287513	val: 0.270738	test: 0.252652

Epoch: 5
Loss: 0.20829508494626037
ROC train: 0.792520	val: 0.755759	test: 0.683612
PRC train: 0.325014	val: 0.303772	test: 0.279994

Epoch: 6
Loss: 0.20295295959360782
ROC train: 0.811133	val: 0.746289	test: 0.703474
PRC train: 0.347517	val: 0.312445	test: 0.318209

Epoch: 7
Loss: 0.19888083772756932
ROC train: 0.816862	val: 0.764142	test: 0.717306
PRC train: 0.344112	val: 0.319704	test: 0.312449

Epoch: 8
Loss: 0.19797348163275688
ROC train: 0.822762	val: 0.756622	test: 0.721386
PRC train: 0.367036	val: 0.324331	test: 0.314056

Epoch: 9
Loss: 0.1950714192360017
ROC train: 0.843007	val: 0.762083	test: 0.717227
PRC train: 0.396142	val: 0.327927	test: 0.322670

Epoch: 10
Loss: 0.1917647892190264
ROC train: 0.843290	val: 0.766805	test: 0.714333
PRC train: 0.399437	val: 0.344038	test: 0.324517

Epoch: 11
Loss: 0.18982045452302473
ROC train: 0.850179	val: 0.750006	test: 0.709934
PRC train: 0.404875	val: 0.306816	test: 0.311165

Epoch: 12
Loss: 0.18818206553739625
ROC train: 0.854236	val: 0.756052	test: 0.715879
PRC train: 0.433174	val: 0.325466	test: 0.323087

Epoch: 13
Loss: 0.1840832988381886
ROC train: 0.857681	val: 0.761158	test: 0.720484
PRC train: 0.434559	val: 0.346350	test: 0.335744

Epoch: 14
Loss: 0.18442568952461286
ROC train: 0.861510	val: 0.764034	test: 0.722825
PRC train: 0.446299	val: 0.353351	test: 0.330612

Epoch: 15
Loss: 0.1836493800528449
ROC train: 0.864247	val: 0.752704	test: 0.719616
PRC train: 0.456944	val: 0.326992	test: 0.332206

Epoch: 16
Loss: 0.18033724708617074
ROC train: 0.874673	val: 0.761663	test: 0.736012
PRC train: 0.483430	val: 0.339631	test: 0.336960

Epoch: 17
Loss: 0.17933037934535062
ROC train: 0.874874	val: 0.753486	test: 0.717689
PRC train: 0.475896	val: 0.340450	test: 0.328536

Epoch: 18
Loss: 0.17691395734363077
ROC train: 0.884551	val: 0.761719	test: 0.725649
PRC train: 0.502565	val: 0.345571	test: 0.337770

Epoch: 19
Loss: 0.17682961758967594
ROC train: 0.884507	val: 0.766751	test: 0.727742
PRC train: 0.502063	val: 0.355451	test: 0.339937

Epoch: 20
Loss: 0.17312422433346902
ROC train: 0.888882	val: 0.755466	test: 0.713058
PRC train: 0.526636	val: 0.347189	test: 0.325261

Epoch: 21
Loss: 0.17214113794323505
ROC train: 0.891826	val: 0.754518	test: 0.723483
PRC train: 0.537685	val: 0.341710	test: 0.337797

Epoch: 22
Loss: 0.17176788231950563
ROC train: 0.894293	val: 0.741839	test: 0.709633
PRC train: 0.549502	val: 0.342322	test: 0.328027

Epoch: 23
Loss: 0.16993993832856025
ROC train: 0.890274	val: 0.758592	test: 0.722718
PRC train: 0.533989	val: 0.354778	test: 0.336340

Epoch: 24
Loss: 0.16915714015074232
ROC train: 0.901093	val: 0.748302	test: 0.715315
PRC train: 0.561093	val: 0.340174	test: 0.336097

Epoch: 25
Loss: 0.16779181093440992
ROC train: 0.901088	val: 0.763015	test: 0.719605
PRC train: 0.564058	val: 0.358093	test: 0.346560

Epoch: 26
Loss: 0.16434966515194815
ROC train: 0.906087	val: 0.751147	test: 0.724292
PRC train: 0.579445	val: 0.332228	test: 0.323250

Epoch: 27
Loss: 0.16352103321324024
ROC train: 0.910240	val: 0.755322	test: 0.718668
PRC train: 0.602226	val: 0.348360	test: 0.342880

Epoch: 28
Loss: 0.1640876020470713
ROC train: 0.908835	val: 0.772192	test: 0.728549
PRC train: 0.586702	val: 0.353175	test: 0.351471

Epoch: 29
Loss: 0.16033956394340032
ROC train: 0.915791	val: 0.761554	test: 0.730258
PRC train: 0.611574	val: 0.356095	test: 0.357661

Epoch: 30
Loss: 0.16081472033549007
ROC train: 0.919433	val: 0.755979	test: 0.723014
PRC train: 0.626800	val: 0.334088	test: 0.351543

Epoch: 31
Loss: 0.15985471094871212
ROC train: 0.921082	val: 0.749164	test: 0.723357
PRC train: 0.625355	val: 0.337836	test: 0.342346

Epoch: 32
Loss: 0.15753341370550888
ROC train: 0.923885	val: 0.746090	test: 0.718820Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/tox21/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/tox21/noise=0.1/tox21_scaff_6_26-05_11-31-04  ]
[ Using Seed :  6  ]
[ Using device :  cuda:2  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5442863810232247
ROC train: 0.685867	val: 0.679906	test: 0.646787
PRC train: 0.210349	val: 0.234938	test: 0.222470

Epoch: 2
Loss: 0.3305564896302182
ROC train: 0.728782	val: 0.696909	test: 0.647275
PRC train: 0.247717	val: 0.262340	test: 0.234684

Epoch: 3
Loss: 0.2453162327202707
ROC train: 0.745328	val: 0.669656	test: 0.617627
PRC train: 0.246972	val: 0.237153	test: 0.236006

Epoch: 4
Loss: 0.21781669794292463
ROC train: 0.774150	val: 0.739595	test: 0.666184
PRC train: 0.305344	val: 0.282639	test: 0.282559

Epoch: 5
Loss: 0.20816915467721597
ROC train: 0.789755	val: 0.756201	test: 0.696457
PRC train: 0.316992	val: 0.307703	test: 0.289688

Epoch: 6
Loss: 0.20262802111327013
ROC train: 0.810245	val: 0.759983	test: 0.704174
PRC train: 0.349531	val: 0.315157	test: 0.300633

Epoch: 7
Loss: 0.19928432487144732
ROC train: 0.819340	val: 0.760647	test: 0.724080
PRC train: 0.366608	val: 0.334675	test: 0.322231

Epoch: 8
Loss: 0.19583363364103165
ROC train: 0.827498	val: 0.756684	test: 0.707658
PRC train: 0.381706	val: 0.321611	test: 0.308146

Epoch: 9
Loss: 0.19315770543408062
ROC train: 0.843715	val: 0.769413	test: 0.716850
PRC train: 0.394948	val: 0.331468	test: 0.312995

Epoch: 10
Loss: 0.19307379336893005
ROC train: 0.844607	val: 0.758660	test: 0.718734
PRC train: 0.400697	val: 0.329420	test: 0.314912

Epoch: 11
Loss: 0.19088944204416866
ROC train: 0.852488	val: 0.752468	test: 0.710514
PRC train: 0.431550	val: 0.314423	test: 0.314949

Epoch: 12
Loss: 0.18922718095033908
ROC train: 0.856093	val: 0.767438	test: 0.720245
PRC train: 0.428062	val: 0.325470	test: 0.326706

Epoch: 13
Loss: 0.18484616229857653
ROC train: 0.861672	val: 0.771682	test: 0.721656
PRC train: 0.451965	val: 0.336840	test: 0.328024

Epoch: 14
Loss: 0.18291422291703585
ROC train: 0.866860	val: 0.764981	test: 0.719109
PRC train: 0.455299	val: 0.341222	test: 0.327518

Epoch: 15
Loss: 0.18116480895170098
ROC train: 0.866619	val: 0.777806	test: 0.735033
PRC train: 0.459469	val: 0.348190	test: 0.343413

Epoch: 16
Loss: 0.18052418943187848
ROC train: 0.876157	val: 0.774886	test: 0.741396
PRC train: 0.484354	val: 0.350616	test: 0.344521

Epoch: 17
Loss: 0.17909233823477372
ROC train: 0.879463	val: 0.772447	test: 0.731234
PRC train: 0.486693	val: 0.342028	test: 0.329860

Epoch: 18
Loss: 0.1762471854985347
ROC train: 0.884464	val: 0.770484	test: 0.723883
PRC train: 0.511820	val: 0.337828	test: 0.336119

Epoch: 19
Loss: 0.17381169835731342
ROC train: 0.888258	val: 0.776308	test: 0.737190
PRC train: 0.519341	val: 0.358847	test: 0.339369

Epoch: 20
Loss: 0.17435057691821892
ROC train: 0.889217	val: 0.770604	test: 0.735682
PRC train: 0.524894	val: 0.352589	test: 0.345217

Epoch: 21
Loss: 0.17026277890136693
ROC train: 0.896040	val: 0.772232	test: 0.733974
PRC train: 0.541827	val: 0.346546	test: 0.335716

Epoch: 22
Loss: 0.17052841354497084
ROC train: 0.891338	val: 0.760716	test: 0.726503
PRC train: 0.531908	val: 0.322258	test: 0.320218

Epoch: 23
Loss: 0.1688607806501761
ROC train: 0.899889	val: 0.769619	test: 0.736556
PRC train: 0.555667	val: 0.341353	test: 0.343873

Epoch: 24
Loss: 0.16799719875565763
ROC train: 0.902020	val: 0.763137	test: 0.731630
PRC train: 0.567792	val: 0.341653	test: 0.338677

Epoch: 25
Loss: 0.16498602921263564
ROC train: 0.904053	val: 0.761861	test: 0.733966
PRC train: 0.564829	val: 0.308228	test: 0.324853

Epoch: 26
Loss: 0.16496811651495175
ROC train: 0.909911	val: 0.759222	test: 0.726418
PRC train: 0.592129	val: 0.315440	test: 0.322763

Epoch: 27
Loss: 0.16310807104233066
ROC train: 0.909983	val: 0.769875	test: 0.726391
PRC train: 0.588350	val: 0.329748	test: 0.331923

Epoch: 28
Loss: 0.16127137491453214
ROC train: 0.912622	val: 0.774378	test: 0.730409
PRC train: 0.603290	val: 0.337150	test: 0.324420

Epoch: 29
Loss: 0.16007972715520755
ROC train: 0.917856	val: 0.773204	test: 0.737458
PRC train: 0.623639	val: 0.340226	test: 0.344829

Epoch: 30
Loss: 0.15831211420473387
ROC train: 0.918222	val: 0.769515	test: 0.737308
PRC train: 0.624211	val: 0.341832	test: 0.340644

Epoch: 31
Loss: 0.15632154906403176
ROC train: 0.921536	val: 0.773186	test: 0.733752
PRC train: 0.631832	val: 0.348818	test: 0.337612

Epoch: 32
Loss: 0.15672672314139086
ROC train: 0.923312	val: 0.780481	test: 0.744081Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/tox21/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/tox21/noise=0.1/tox21_scaff_4_26-05_11-31-04  ]
[ Using Seed :  4  ]
[ Using device :  cuda:2  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.538084191513692
ROC train: 0.678655	val: 0.646005	test: 0.618543
PRC train: 0.188476	val: 0.185723	test: 0.175603

Epoch: 2
Loss: 0.3271499155413609
ROC train: 0.727489	val: 0.701309	test: 0.637317
PRC train: 0.246711	val: 0.250350	test: 0.231764

Epoch: 3
Loss: 0.24392425441306828
ROC train: 0.757863	val: 0.704981	test: 0.654359
PRC train: 0.276331	val: 0.246107	test: 0.250175

Epoch: 4
Loss: 0.21751807403876508
ROC train: 0.780129	val: 0.746110	test: 0.684342
PRC train: 0.308467	val: 0.282884	test: 0.272730

Epoch: 5
Loss: 0.2083478281209331
ROC train: 0.797975	val: 0.743941	test: 0.687132
PRC train: 0.323610	val: 0.285887	test: 0.280207

Epoch: 6
Loss: 0.20348735096614273
ROC train: 0.809845	val: 0.753180	test: 0.694382
PRC train: 0.354201	val: 0.298475	test: 0.283352

Epoch: 7
Loss: 0.20034427530428933
ROC train: 0.826799	val: 0.748690	test: 0.707757
PRC train: 0.366754	val: 0.307843	test: 0.301533

Epoch: 8
Loss: 0.19683389762996142
ROC train: 0.832377	val: 0.748401	test: 0.706312
PRC train: 0.376606	val: 0.307731	test: 0.302334

Epoch: 9
Loss: 0.19293701378361028
ROC train: 0.836569	val: 0.746910	test: 0.711067
PRC train: 0.388560	val: 0.299676	test: 0.300238

Epoch: 10
Loss: 0.19217958208063227
ROC train: 0.846213	val: 0.753477	test: 0.724075
PRC train: 0.420764	val: 0.332562	test: 0.317054

Epoch: 11
Loss: 0.19050103385256623
ROC train: 0.847561	val: 0.751699	test: 0.723210
PRC train: 0.419469	val: 0.335924	test: 0.320989

Epoch: 12
Loss: 0.1877837224551694
ROC train: 0.851974	val: 0.730016	test: 0.704380
PRC train: 0.427504	val: 0.290969	test: 0.291803

Epoch: 13
Loss: 0.18579626539738336
ROC train: 0.863433	val: 0.746855	test: 0.718359
PRC train: 0.457571	val: 0.322275	test: 0.309828

Epoch: 14
Loss: 0.1830769400145643
ROC train: 0.865714	val: 0.741226	test: 0.729260
PRC train: 0.465913	val: 0.321806	test: 0.317077

Epoch: 15
Loss: 0.18273505879770166
ROC train: 0.869876	val: 0.740194	test: 0.723071
PRC train: 0.478536	val: 0.310333	test: 0.317021

Epoch: 16
Loss: 0.18096294072886931
ROC train: 0.875013	val: 0.743031	test: 0.724622
PRC train: 0.479359	val: 0.289178	test: 0.282474

Epoch: 17
Loss: 0.1785798317675377
ROC train: 0.878892	val: 0.757411	test: 0.737403
PRC train: 0.503401	val: 0.342623	test: 0.319859

Epoch: 18
Loss: 0.17728296690179637
ROC train: 0.884306	val: 0.741275	test: 0.722131
PRC train: 0.523640	val: 0.335684	test: 0.322159

Epoch: 19
Loss: 0.1749047939195854
ROC train: 0.887968	val: 0.755595	test: 0.731866
PRC train: 0.522411	val: 0.333970	test: 0.334214

Epoch: 20
Loss: 0.17439111587850217
ROC train: 0.887941	val: 0.746608	test: 0.732711
PRC train: 0.532583	val: 0.330563	test: 0.335725

Epoch: 21
Loss: 0.16971423035631933
ROC train: 0.896064	val: 0.744351	test: 0.733444
PRC train: 0.557762	val: 0.332414	test: 0.333174

Epoch: 22
Loss: 0.1686835516707007
ROC train: 0.899048	val: 0.748986	test: 0.732513
PRC train: 0.571706	val: 0.332865	test: 0.327365

Epoch: 23
Loss: 0.1672248973441666
ROC train: 0.901408	val: 0.760469	test: 0.739313
PRC train: 0.574245	val: 0.331263	test: 0.335852

Epoch: 24
Loss: 0.16685933679612674
ROC train: 0.896408	val: 0.731632	test: 0.718348
PRC train: 0.546966	val: 0.276432	test: 0.286006

Epoch: 25
Loss: 0.16705053023852348
ROC train: 0.905649	val: 0.754781	test: 0.729078
PRC train: 0.581329	val: 0.329114	test: 0.334123

Epoch: 26
Loss: 0.1637568981725401
ROC train: 0.906406	val: 0.750701	test: 0.732759
PRC train: 0.595526	val: 0.316363	test: 0.333667

Epoch: 27
Loss: 0.1634234946006421
ROC train: 0.903952	val: 0.734868	test: 0.727301
PRC train: 0.572586	val: 0.281835	test: 0.296928

Epoch: 28
Loss: 0.16153020447430452
ROC train: 0.911627	val: 0.738987	test: 0.722432
PRC train: 0.600671	val: 0.305419	test: 0.307512

Epoch: 29
Loss: 0.1585117436068925
ROC train: 0.912547	val: 0.752256	test: 0.730484
PRC train: 0.614082	val: 0.322950	test: 0.314290

Epoch: 30
Loss: 0.15957363221996976
ROC train: 0.911929	val: 0.736554	test: 0.736359
PRC train: 0.601563	val: 0.287623	test: 0.288785

Epoch: 31
Loss: 0.15719719525744694
ROC train: 0.915677	val: 0.747430	test: 0.724310
PRC train: 0.611390	val: 0.306504	test: 0.312137

Epoch: 32
Loss: 0.1563399904780577
ROC train: 0.920144	val: 0.733101	test: 0.728966Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/tox21/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/tox21/noise=0.05/tox21_scaff_5_26-05_11-31-04  ]
[ Using Seed :  5  ]
[ Using device :  cuda:1  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5589288187040726
ROC train: 0.681408	val: 0.629556	test: 0.586001
PRC train: 0.198382	val: 0.198313	test: 0.163434

Epoch: 2
Loss: 0.3358573164390797
ROC train: 0.737531	val: 0.697064	test: 0.624731
PRC train: 0.244344	val: 0.265877	test: 0.225668

Epoch: 3
Loss: 0.24397314903957804
ROC train: 0.766956	val: 0.745750	test: 0.674158
PRC train: 0.292627	val: 0.321357	test: 0.278219

Epoch: 4
Loss: 0.21589651824798264
ROC train: 0.787725	val: 0.743045	test: 0.685526
PRC train: 0.309549	val: 0.293947	test: 0.275294

Epoch: 5
Loss: 0.20473451138889348
ROC train: 0.805100	val: 0.752969	test: 0.702672
PRC train: 0.339580	val: 0.315687	test: 0.304781

Epoch: 6
Loss: 0.19983828125250963
ROC train: 0.816933	val: 0.751193	test: 0.720722
PRC train: 0.349577	val: 0.322840	test: 0.322099

Epoch: 7
Loss: 0.19636787773443348
ROC train: 0.834497	val: 0.764075	test: 0.708920
PRC train: 0.372141	val: 0.322136	test: 0.325018

Epoch: 8
Loss: 0.19287978510972528
ROC train: 0.837172	val: 0.758471	test: 0.726446
PRC train: 0.390129	val: 0.365354	test: 0.328529

Epoch: 9
Loss: 0.19043066086947139
ROC train: 0.848418	val: 0.765116	test: 0.726899
PRC train: 0.416062	val: 0.349764	test: 0.332075

Epoch: 10
Loss: 0.1883080508216045
ROC train: 0.853470	val: 0.775705	test: 0.733158
PRC train: 0.418593	val: 0.361539	test: 0.341504

Epoch: 11
Loss: 0.1859989126351386
ROC train: 0.858150	val: 0.764576	test: 0.733425
PRC train: 0.433131	val: 0.338851	test: 0.333295

Epoch: 12
Loss: 0.1840964788360889
ROC train: 0.866028	val: 0.770554	test: 0.743946
PRC train: 0.456978	val: 0.358080	test: 0.345960

Epoch: 13
Loss: 0.179605462724714
ROC train: 0.866020	val: 0.767196	test: 0.723088
PRC train: 0.449432	val: 0.353655	test: 0.328086

Epoch: 14
Loss: 0.17894011702757548
ROC train: 0.868820	val: 0.773020	test: 0.739000
PRC train: 0.472121	val: 0.361991	test: 0.341143

Epoch: 15
Loss: 0.17888682794563407
ROC train: 0.874306	val: 0.774066	test: 0.746206
PRC train: 0.483671	val: 0.351334	test: 0.334265

Epoch: 16
Loss: 0.17506088335752917
ROC train: 0.881854	val: 0.770458	test: 0.751526
PRC train: 0.503686	val: 0.352243	test: 0.351717

Epoch: 17
Loss: 0.17423397379988026
ROC train: 0.881859	val: 0.776069	test: 0.745380
PRC train: 0.512097	val: 0.378750	test: 0.349489

Epoch: 18
Loss: 0.1705529141930788
ROC train: 0.888474	val: 0.778135	test: 0.755240
PRC train: 0.534786	val: 0.366507	test: 0.356797

Epoch: 19
Loss: 0.17172225997220514
ROC train: 0.892159	val: 0.779019	test: 0.758505
PRC train: 0.543584	val: 0.358355	test: 0.361973

Epoch: 20
Loss: 0.1687765261361198
ROC train: 0.893992	val: 0.769813	test: 0.748933
PRC train: 0.542727	val: 0.364949	test: 0.350046

Epoch: 21
Loss: 0.16870574215529635
ROC train: 0.895298	val: 0.771147	test: 0.756829
PRC train: 0.555496	val: 0.360776	test: 0.373598

Epoch: 22
Loss: 0.16690406961993312
ROC train: 0.899224	val: 0.767818	test: 0.753115
PRC train: 0.567056	val: 0.353325	test: 0.351963

Epoch: 23
Loss: 0.16503636253176623
ROC train: 0.890093	val: 0.778645	test: 0.747885
PRC train: 0.536949	val: 0.375790	test: 0.346142

Epoch: 24
Loss: 0.16433800920308284
ROC train: 0.904544	val: 0.771197	test: 0.745095
PRC train: 0.587352	val: 0.361065	test: 0.353542

Epoch: 25
Loss: 0.16224979413894045
ROC train: 0.907982	val: 0.782326	test: 0.745762
PRC train: 0.593028	val: 0.363255	test: 0.367485

Epoch: 26
Loss: 0.1615972763923257
ROC train: 0.908284	val: 0.774485	test: 0.754697
PRC train: 0.598442	val: 0.375486	test: 0.372350

Epoch: 27
Loss: 0.15877344240858698
ROC train: 0.913435	val: 0.776667	test: 0.743768
PRC train: 0.608712	val: 0.359853	test: 0.357884

Epoch: 28
Loss: 0.1583324951379111
ROC train: 0.913931	val: 0.776347	test: 0.737491
PRC train: 0.614007	val: 0.354864	test: 0.354775

Epoch: 29
Loss: 0.15512482483390053
ROC train: 0.916696	val: 0.769932	test: 0.744122
PRC train: 0.623826	val: 0.356459	test: 0.369214

Epoch: 30
Loss: 0.1570901389423452
ROC train: 0.922420	val: 0.784988	test: 0.744070
PRC train: 0.634230	val: 0.361536	test: 0.368183

Epoch: 31
Loss: 0.15531827216953886
ROC train: 0.924185	val: 0.776281	test: 0.742700
PRC train: 0.646312	val: 0.358029	test: 0.356201

Epoch: 32
Loss: 0.15431559091832944
ROC train: 0.921849	val: 0.775154	test: 0.742473Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:3
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/tox21/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/tox21/noise=0.2/tox21_scaff_6_26-05_11-31-04  ]
[ Using Seed :  6  ]
[ Using device :  cuda:3  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5491006628580832
ROC train: 0.668288	val: 0.662005	test: 0.632957
PRC train: 0.164933	val: 0.208683	test: 0.192332

Epoch: 2
Loss: 0.3317976949557652
ROC train: 0.710534	val: 0.682897	test: 0.633211
PRC train: 0.222033	val: 0.232952	test: 0.218725

Epoch: 3
Loss: 0.2463896911699779
ROC train: 0.732806	val: 0.692595	test: 0.652335
PRC train: 0.243994	val: 0.252982	test: 0.236167

Epoch: 4
Loss: 0.22155309502549275
ROC train: 0.749086	val: 0.716071	test: 0.657463
PRC train: 0.272418	val: 0.264449	test: 0.257744

Epoch: 5
Loss: 0.21464253557825494
ROC train: 0.757728	val: 0.727174	test: 0.678612
PRC train: 0.289590	val: 0.281881	test: 0.267663

Epoch: 6
Loss: 0.2105360998544653
ROC train: 0.778322	val: 0.734551	test: 0.682027
PRC train: 0.311520	val: 0.279316	test: 0.273471

Epoch: 7
Loss: 0.20718981367053002
ROC train: 0.791782	val: 0.733366	test: 0.695760
PRC train: 0.330350	val: 0.290829	test: 0.284889

Epoch: 8
Loss: 0.20423322571505612
ROC train: 0.799368	val: 0.737749	test: 0.696782
PRC train: 0.348214	val: 0.298768	test: 0.283699

Epoch: 9
Loss: 0.2021157957151404
ROC train: 0.811394	val: 0.727138	test: 0.700716
PRC train: 0.362697	val: 0.292182	test: 0.281185

Epoch: 10
Loss: 0.20025964980611868
ROC train: 0.811840	val: 0.742447	test: 0.708487
PRC train: 0.363067	val: 0.305122	test: 0.295846

Epoch: 11
Loss: 0.19813606507360706
ROC train: 0.830168	val: 0.734449	test: 0.702613
PRC train: 0.386622	val: 0.286423	test: 0.284839

Epoch: 12
Loss: 0.19772079491806005
ROC train: 0.832886	val: 0.732895	test: 0.703100
PRC train: 0.376419	val: 0.271481	test: 0.273909

Epoch: 13
Loss: 0.19264146881896926
ROC train: 0.841875	val: 0.750711	test: 0.715983
PRC train: 0.409378	val: 0.298909	test: 0.301012

Epoch: 14
Loss: 0.1914005133020234
ROC train: 0.854117	val: 0.733583	test: 0.717655
PRC train: 0.434346	val: 0.304085	test: 0.311677

Epoch: 15
Loss: 0.18825805991990585
ROC train: 0.858394	val: 0.754631	test: 0.724749
PRC train: 0.441677	val: 0.306518	test: 0.308807

Epoch: 16
Loss: 0.18817807157434804
ROC train: 0.853985	val: 0.736871	test: 0.721460
PRC train: 0.438407	val: 0.296232	test: 0.307020

Epoch: 17
Loss: 0.18709726889998934
ROC train: 0.861508	val: 0.734405	test: 0.718273
PRC train: 0.447367	val: 0.298590	test: 0.307403

Epoch: 18
Loss: 0.18457512614807084
ROC train: 0.870857	val: 0.742657	test: 0.712742
PRC train: 0.466695	val: 0.301821	test: 0.307952

Epoch: 19
Loss: 0.1824570952146779
ROC train: 0.876909	val: 0.746756	test: 0.731030
PRC train: 0.492381	val: 0.311427	test: 0.320286

Epoch: 20
Loss: 0.18051528116084295
ROC train: 0.880680	val: 0.753772	test: 0.728878
PRC train: 0.501325	val: 0.313222	test: 0.324143

Epoch: 21
Loss: 0.17658768307558131
ROC train: 0.882964	val: 0.748663	test: 0.732382
PRC train: 0.505343	val: 0.310492	test: 0.307463

Epoch: 22
Loss: 0.17817572266498055
ROC train: 0.885442	val: 0.759186	test: 0.740439
PRC train: 0.510378	val: 0.318790	test: 0.329947

Epoch: 23
Loss: 0.17454661304759753
ROC train: 0.888926	val: 0.750996	test: 0.734712
PRC train: 0.519454	val: 0.303456	test: 0.329774

Epoch: 24
Loss: 0.17441222368299272
ROC train: 0.891948	val: 0.733338	test: 0.724744
PRC train: 0.522969	val: 0.299802	test: 0.312421

Epoch: 25
Loss: 0.1728301407310677
ROC train: 0.892939	val: 0.749181	test: 0.721860
PRC train: 0.539724	val: 0.280181	test: 0.298178

Epoch: 26
Loss: 0.17039365246159285
ROC train: 0.899633	val: 0.741839	test: 0.729507
PRC train: 0.555024	val: 0.293049	test: 0.320593

Epoch: 27
Loss: 0.17005727068729037
ROC train: 0.903225	val: 0.743267	test: 0.715649
PRC train: 0.575314	val: 0.290168	test: 0.307274

Epoch: 28
Loss: 0.1687633472826685
ROC train: 0.904915	val: 0.744816	test: 0.733867
PRC train: 0.562734	val: 0.309186	test: 0.320225

Epoch: 29
Loss: 0.16853140721865625
ROC train: 0.906865	val: 0.738861	test: 0.721793
PRC train: 0.581636	val: 0.296508	test: 0.311340

Epoch: 30
Loss: 0.16502355061274354
ROC train: 0.912923	val: 0.742531	test: 0.718149
PRC train: 0.598213	val: 0.301332	test: 0.309813

Epoch: 31
Loss: 0.16097695372055845
ROC train: 0.915324	val: 0.731331	test: 0.711557
PRC train: 0.593257	val: 0.296217	test: 0.318773

Epoch: 32
Loss: 0.16101247822564083
ROC train: 0.917485	val: 0.733885	test: 0.710668Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:3
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/tox21/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/tox21/noise=0.2/tox21_scaff_5_26-05_11-31-04  ]
[ Using Seed :  5  ]
[ Using device :  cuda:3  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5656397190375037
ROC train: 0.645026	val: 0.642546	test: 0.598209
PRC train: 0.142737	val: 0.175774	test: 0.151146

Epoch: 2
Loss: 0.34144637258887955
ROC train: 0.702939	val: 0.661924	test: 0.593063
PRC train: 0.217108	val: 0.219467	test: 0.193148

Epoch: 3
Loss: 0.24926068678267552
ROC train: 0.727499	val: 0.699384	test: 0.625002
PRC train: 0.243424	val: 0.253443	test: 0.210259

Epoch: 4
Loss: 0.22415776188319758
ROC train: 0.747471	val: 0.694915	test: 0.644738
PRC train: 0.256307	val: 0.249831	test: 0.228816

Epoch: 5
Loss: 0.2173591130638868
ROC train: 0.767778	val: 0.713968	test: 0.652334
PRC train: 0.282358	val: 0.260396	test: 0.237099

Epoch: 6
Loss: 0.21103034063575105
ROC train: 0.779938	val: 0.723499	test: 0.672756
PRC train: 0.311377	val: 0.292843	test: 0.262667

Epoch: 7
Loss: 0.20888557137682198
ROC train: 0.793303	val: 0.720493	test: 0.685620
PRC train: 0.312884	val: 0.280030	test: 0.254075

Epoch: 8
Loss: 0.20556430566415643
ROC train: 0.797841	val: 0.732677	test: 0.701081
PRC train: 0.336631	val: 0.297255	test: 0.276910

Epoch: 9
Loss: 0.20275482311619367
ROC train: 0.814127	val: 0.739972	test: 0.701225
PRC train: 0.354852	val: 0.307746	test: 0.283324

Epoch: 10
Loss: 0.1999167813517083
ROC train: 0.820688	val: 0.740041	test: 0.700661
PRC train: 0.367029	val: 0.304769	test: 0.291197

Epoch: 11
Loss: 0.1989356503230022
ROC train: 0.829032	val: 0.742471	test: 0.702658
PRC train: 0.373178	val: 0.310605	test: 0.288904

Epoch: 12
Loss: 0.1955687078674495
ROC train: 0.838274	val: 0.738983	test: 0.712191
PRC train: 0.397844	val: 0.298788	test: 0.289202

Epoch: 13
Loss: 0.19190907931298948
ROC train: 0.846217	val: 0.753037	test: 0.712214
PRC train: 0.407064	val: 0.310977	test: 0.296146

Epoch: 14
Loss: 0.1901574219518141
ROC train: 0.847216	val: 0.754687	test: 0.709423
PRC train: 0.412782	val: 0.316203	test: 0.298034

Epoch: 15
Loss: 0.1909321887055223
ROC train: 0.856029	val: 0.745544	test: 0.713422
PRC train: 0.431145	val: 0.302308	test: 0.300482

Epoch: 16
Loss: 0.187614363737374
ROC train: 0.862707	val: 0.756894	test: 0.717104
PRC train: 0.450172	val: 0.296868	test: 0.298226

Epoch: 17
Loss: 0.1851761181033912
ROC train: 0.860423	val: 0.759790	test: 0.713213
PRC train: 0.441825	val: 0.314701	test: 0.304779

Epoch: 18
Loss: 0.1839460039370472
ROC train: 0.868736	val: 0.749292	test: 0.717112
PRC train: 0.458983	val: 0.305525	test: 0.307646

Epoch: 19
Loss: 0.18271748062147958
ROC train: 0.867665	val: 0.750671	test: 0.713507
PRC train: 0.457397	val: 0.311364	test: 0.300051

Epoch: 20
Loss: 0.18172827417467252
ROC train: 0.872614	val: 0.745931	test: 0.715618
PRC train: 0.480677	val: 0.303972	test: 0.286650

Epoch: 21
Loss: 0.1781384265863469
ROC train: 0.881986	val: 0.729541	test: 0.694980
PRC train: 0.497882	val: 0.268223	test: 0.271148

Epoch: 22
Loss: 0.17850922208185388
ROC train: 0.885136	val: 0.742022	test: 0.702579
PRC train: 0.510163	val: 0.269079	test: 0.261987

Epoch: 23
Loss: 0.17647570193223083
ROC train: 0.886985	val: 0.745646	test: 0.712929
PRC train: 0.512194	val: 0.283686	test: 0.288025

Epoch: 24
Loss: 0.17380133168621292
ROC train: 0.893248	val: 0.751167	test: 0.713033
PRC train: 0.530420	val: 0.301973	test: 0.306665

Epoch: 25
Loss: 0.1746981394730995
ROC train: 0.895016	val: 0.720002	test: 0.698851
PRC train: 0.544459	val: 0.252226	test: 0.260496

Epoch: 26
Loss: 0.17121551035908564
ROC train: 0.897712	val: 0.732280	test: 0.698867
PRC train: 0.547429	val: 0.258577	test: 0.255714

Epoch: 27
Loss: 0.16830690515885702
ROC train: 0.905155	val: 0.737621	test: 0.706166
PRC train: 0.566159	val: 0.281764	test: 0.289172

Epoch: 28
Loss: 0.16706096467244752
ROC train: 0.907584	val: 0.745083	test: 0.704500
PRC train: 0.586216	val: 0.271443	test: 0.270053

Epoch: 29
Loss: 0.16528109463755286
ROC train: 0.909409	val: 0.751670	test: 0.728157
PRC train: 0.580151	val: 0.298650	test: 0.296547

Epoch: 30
Loss: 0.1659389938084214
ROC train: 0.912014	val: 0.736033	test: 0.706150
PRC train: 0.588888	val: 0.280993	test: 0.269764

Epoch: 31
Loss: 0.16271110542155157
ROC train: 0.917887	val: 0.729103	test: 0.703147
PRC train: 0.608195	val: 0.266365	test: 0.272106

Epoch: 32
Loss: 0.16174051698391684
ROC train: 0.917633	val: 0.745242	test: 0.699491Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:3
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/tox21/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/tox21/noise=0.2/tox21_scaff_4_26-05_11-31-04  ]
[ Using Seed :  4  ]
[ Using device :  cuda:3  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.542711110891217
ROC train: 0.660519	val: 0.645990	test: 0.599671
PRC train: 0.159569	val: 0.186116	test: 0.167150

Epoch: 2
Loss: 0.3290039056643455
ROC train: 0.715706	val: 0.701517	test: 0.640472
PRC train: 0.219001	val: 0.243125	test: 0.204323

Epoch: 3
Loss: 0.24726682445395487
ROC train: 0.741168	val: 0.693484	test: 0.643130
PRC train: 0.242614	val: 0.241836	test: 0.219859

Epoch: 4
Loss: 0.22326774791180629
ROC train: 0.759004	val: 0.717294	test: 0.670030
PRC train: 0.268457	val: 0.262370	test: 0.246908

Epoch: 5
Loss: 0.2147900297068017
ROC train: 0.773018	val: 0.702693	test: 0.662732
PRC train: 0.279159	val: 0.245412	test: 0.242691

Epoch: 6
Loss: 0.21035619047702409
ROC train: 0.784654	val: 0.736800	test: 0.697906
PRC train: 0.311094	val: 0.293091	test: 0.261865

Epoch: 7
Loss: 0.20754697607222475
ROC train: 0.801508	val: 0.747640	test: 0.708976
PRC train: 0.327108	val: 0.295092	test: 0.283972

Epoch: 8
Loss: 0.20407119197461815
ROC train: 0.809711	val: 0.732632	test: 0.697520
PRC train: 0.335647	val: 0.274407	test: 0.276360

Epoch: 9
Loss: 0.20170676137790647
ROC train: 0.821470	val: 0.735164	test: 0.709128
PRC train: 0.366679	val: 0.296319	test: 0.285961

Epoch: 10
Loss: 0.2001632624690856
ROC train: 0.827781	val: 0.744178	test: 0.718114
PRC train: 0.384310	val: 0.312766	test: 0.292518

Epoch: 11
Loss: 0.1986313643329256
ROC train: 0.828523	val: 0.744659	test: 0.710802
PRC train: 0.379248	val: 0.322770	test: 0.282833

Epoch: 12
Loss: 0.19607505241478473
ROC train: 0.839253	val: 0.742397	test: 0.713435
PRC train: 0.406722	val: 0.310833	test: 0.302224

Epoch: 13
Loss: 0.1922316796983447
ROC train: 0.849463	val: 0.743911	test: 0.719612
PRC train: 0.416261	val: 0.299388	test: 0.281145

Epoch: 14
Loss: 0.19198308024555188
ROC train: 0.851559	val: 0.726291	test: 0.708894
PRC train: 0.426454	val: 0.285081	test: 0.265372

Epoch: 15
Loss: 0.1903043500951913
ROC train: 0.855749	val: 0.736197	test: 0.720054
PRC train: 0.438970	val: 0.294942	test: 0.282257

Epoch: 16
Loss: 0.18771464055181766
ROC train: 0.865007	val: 0.735784	test: 0.718462
PRC train: 0.457305	val: 0.292987	test: 0.269672

Epoch: 17
Loss: 0.18663929702979723
ROC train: 0.865560	val: 0.742671	test: 0.736882
PRC train: 0.462495	val: 0.328000	test: 0.315052

Epoch: 18
Loss: 0.18468723660111372
ROC train: 0.871299	val: 0.734597	test: 0.707432
PRC train: 0.478191	val: 0.337408	test: 0.296152

Epoch: 19
Loss: 0.18252879803898123
ROC train: 0.876620	val: 0.735161	test: 0.720294
PRC train: 0.492453	val: 0.314640	test: 0.297382

Epoch: 20
Loss: 0.18185289038200383
ROC train: 0.877248	val: 0.733031	test: 0.729844
PRC train: 0.491428	val: 0.332818	test: 0.329236

Epoch: 21
Loss: 0.17849547185500267
ROC train: 0.884100	val: 0.720939	test: 0.708709
PRC train: 0.506313	val: 0.297733	test: 0.284933

Epoch: 22
Loss: 0.17698376299492466
ROC train: 0.887401	val: 0.720558	test: 0.698969
PRC train: 0.520891	val: 0.317720	test: 0.289188

Epoch: 23
Loss: 0.17716993215136764
ROC train: 0.893219	val: 0.720454	test: 0.707443
PRC train: 0.530117	val: 0.299404	test: 0.271550

Epoch: 24
Loss: 0.17583306629694234
ROC train: 0.888134	val: 0.723063	test: 0.711707
PRC train: 0.507917	val: 0.283236	test: 0.265276

Epoch: 25
Loss: 0.17455242262338125
ROC train: 0.896218	val: 0.723193	test: 0.694756
PRC train: 0.541515	val: 0.308844	test: 0.288239

Epoch: 26
Loss: 0.17100576547428514
ROC train: 0.900200	val: 0.727304	test: 0.714355
PRC train: 0.550606	val: 0.332713	test: 0.309072

Epoch: 27
Loss: 0.17055535750528403
ROC train: 0.902291	val: 0.710208	test: 0.691063
PRC train: 0.566795	val: 0.319069	test: 0.280258

Epoch: 28
Loss: 0.16860327180663107
ROC train: 0.908012	val: 0.710070	test: 0.687739
PRC train: 0.583742	val: 0.314461	test: 0.288631

Epoch: 29
Loss: 0.1672807879500714
ROC train: 0.908316	val: 0.724665	test: 0.702337
PRC train: 0.580660	val: 0.339122	test: 0.294372

Epoch: 30
Loss: 0.16546993842588897
ROC train: 0.913477	val: 0.707953	test: 0.696129
PRC train: 0.603811	val: 0.283173	test: 0.260090

Epoch: 31
Loss: 0.16346982844635696
ROC train: 0.913154	val: 0.688894	test: 0.676682
PRC train: 0.588783	val: 0.285098	test: 0.270775

Epoch: 32
Loss: 0.16347869370922413
ROC train: 0.915685	val: 0.683685	test: 0.668170Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/tox21/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/tox21/noise=0.05/tox21_scaff_6_26-05_11-31-04  ]
[ Using Seed :  6  ]
[ Using device :  cuda:1  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5424563140912523
ROC train: 0.685715	val: 0.667833	test: 0.621389
PRC train: 0.218125	val: 0.224588	test: 0.210317

Epoch: 2
Loss: 0.3282410556697036
ROC train: 0.730506	val: 0.697185	test: 0.652698
PRC train: 0.259424	val: 0.264974	test: 0.250040

Epoch: 3
Loss: 0.24337664802114026
ROC train: 0.760750	val: 0.714474	test: 0.649832
PRC train: 0.268323	val: 0.276465	test: 0.260954

Epoch: 4
Loss: 0.2153327717867477
ROC train: 0.793252	val: 0.755302	test: 0.688763
PRC train: 0.324179	val: 0.313008	test: 0.295324

Epoch: 5
Loss: 0.20452709276940706
ROC train: 0.806354	val: 0.756269	test: 0.702363
PRC train: 0.347324	val: 0.322186	test: 0.312721

Epoch: 6
Loss: 0.19902716414294727
ROC train: 0.816686	val: 0.774477	test: 0.732079
PRC train: 0.355777	val: 0.359787	test: 0.327393

Epoch: 7
Loss: 0.19482750250965533
ROC train: 0.827129	val: 0.771986	test: 0.731341
PRC train: 0.380003	val: 0.351409	test: 0.335019

Epoch: 8
Loss: 0.1908248695666472
ROC train: 0.842371	val: 0.768534	test: 0.727311
PRC train: 0.405278	val: 0.350207	test: 0.323248

Epoch: 9
Loss: 0.1896329957420003
ROC train: 0.849094	val: 0.777536	test: 0.718942
PRC train: 0.412225	val: 0.344283	test: 0.332972

Epoch: 10
Loss: 0.18791944022985752
ROC train: 0.850625	val: 0.768392	test: 0.729700
PRC train: 0.420958	val: 0.347326	test: 0.318963

Epoch: 11
Loss: 0.18579573882873512
ROC train: 0.863051	val: 0.770577	test: 0.724934
PRC train: 0.456737	val: 0.348771	test: 0.338837

Epoch: 12
Loss: 0.1820941740092078
ROC train: 0.861828	val: 0.763697	test: 0.713411
PRC train: 0.450649	val: 0.342762	test: 0.332722

Epoch: 13
Loss: 0.1799598075822579
ROC train: 0.867524	val: 0.775992	test: 0.730500
PRC train: 0.479038	val: 0.349990	test: 0.335733

Epoch: 14
Loss: 0.17813956155689326
ROC train: 0.870986	val: 0.773731	test: 0.733755
PRC train: 0.483560	val: 0.360588	test: 0.358860

Epoch: 15
Loss: 0.17707008402715985
ROC train: 0.877959	val: 0.776185	test: 0.726220
PRC train: 0.505111	val: 0.368378	test: 0.346283

Epoch: 16
Loss: 0.17502788228888547
ROC train: 0.878723	val: 0.770800	test: 0.739561
PRC train: 0.510042	val: 0.367274	test: 0.351009

Epoch: 17
Loss: 0.1735023548060948
ROC train: 0.884118	val: 0.770462	test: 0.716985
PRC train: 0.519087	val: 0.360319	test: 0.348151

Epoch: 18
Loss: 0.1712579271820305
ROC train: 0.888321	val: 0.763046	test: 0.725562
PRC train: 0.543885	val: 0.349367	test: 0.350427

Epoch: 19
Loss: 0.16860110301829653
ROC train: 0.890052	val: 0.768559	test: 0.723956
PRC train: 0.551458	val: 0.348470	test: 0.343284

Epoch: 20
Loss: 0.16979082745401816
ROC train: 0.890731	val: 0.764268	test: 0.720608
PRC train: 0.553289	val: 0.347676	test: 0.333219

Epoch: 21
Loss: 0.1661488782364774
ROC train: 0.894528	val: 0.768866	test: 0.724607
PRC train: 0.569676	val: 0.359874	test: 0.352936

Epoch: 22
Loss: 0.16547255055809704
ROC train: 0.897951	val: 0.771194	test: 0.734091
PRC train: 0.588475	val: 0.380613	test: 0.370480

Epoch: 23
Loss: 0.16274501234669647
ROC train: 0.900892	val: 0.772481	test: 0.737625
PRC train: 0.586463	val: 0.367738	test: 0.362328

Epoch: 24
Loss: 0.16314287688637488
ROC train: 0.903337	val: 0.772635	test: 0.735913
PRC train: 0.586881	val: 0.361518	test: 0.373259

Epoch: 25
Loss: 0.16192396387343483
ROC train: 0.902733	val: 0.774853	test: 0.744060
PRC train: 0.587017	val: 0.343471	test: 0.367093

Epoch: 26
Loss: 0.15980871124134233
ROC train: 0.908603	val: 0.771562	test: 0.734624
PRC train: 0.606995	val: 0.343110	test: 0.353245

Epoch: 27
Loss: 0.15986332593855654
ROC train: 0.909776	val: 0.775166	test: 0.738312
PRC train: 0.601967	val: 0.337691	test: 0.358072

Epoch: 28
Loss: 0.15948858983242423
ROC train: 0.913642	val: 0.771603	test: 0.736627
PRC train: 0.625642	val: 0.342985	test: 0.362627

Epoch: 29
Loss: 0.15711677074820565
ROC train: 0.916344	val: 0.775589	test: 0.736476
PRC train: 0.635233	val: 0.348102	test: 0.359306

Epoch: 30
Loss: 0.15611501679665707
ROC train: 0.915252	val: 0.767301	test: 0.740724
PRC train: 0.634093	val: 0.348706	test: 0.364980

Epoch: 31
Loss: 0.15399966088811531
ROC train: 0.915324	val: 0.770039	test: 0.735167
PRC train: 0.623897	val: 0.342439	test: 0.362922

Epoch: 32
Loss: 0.1527260751231094
ROC train: 0.917920	val: 0.778114	test: 0.748379Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/tox21/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/tox21/noise=0.05/tox21_scaff_4_26-05_11-31-04  ]
[ Using Seed :  4  ]
[ Using device :  cuda:1  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5336691355686523
ROC train: 0.683873	val: 0.618698	test: 0.607775
PRC train: 0.191064	val: 0.178360	test: 0.173931

Epoch: 2
Loss: 0.3263662075771876
ROC train: 0.741241	val: 0.697837	test: 0.639182
PRC train: 0.257643	val: 0.264765	test: 0.233738

Epoch: 3
Loss: 0.24119454202328058
ROC train: 0.773902	val: 0.716211	test: 0.671152
PRC train: 0.285272	val: 0.247222	test: 0.257908

Epoch: 4
Loss: 0.21401595638365756
ROC train: 0.795750	val: 0.749762	test: 0.692312
PRC train: 0.328430	val: 0.281375	test: 0.289690

Epoch: 5
Loss: 0.2054581646556235
ROC train: 0.814103	val: 0.747595	test: 0.702057
PRC train: 0.345206	val: 0.297347	test: 0.297011

Epoch: 6
Loss: 0.20006365355243513
ROC train: 0.817816	val: 0.757520	test: 0.704520
PRC train: 0.365914	val: 0.315587	test: 0.311268

Epoch: 7
Loss: 0.19631682521300894
ROC train: 0.833368	val: 0.762787	test: 0.707968
PRC train: 0.372627	val: 0.310883	test: 0.310988

Epoch: 8
Loss: 0.1912951165301138
ROC train: 0.836846	val: 0.756167	test: 0.701770
PRC train: 0.381536	val: 0.298507	test: 0.306497

Epoch: 9
Loss: 0.189063030216501
ROC train: 0.845762	val: 0.754559	test: 0.720310
PRC train: 0.424618	val: 0.323088	test: 0.324791

Epoch: 10
Loss: 0.18678725332869311
ROC train: 0.852762	val: 0.765182	test: 0.715162
PRC train: 0.443228	val: 0.326586	test: 0.323652

Epoch: 11
Loss: 0.18436696891373872
ROC train: 0.854548	val: 0.764847	test: 0.713855
PRC train: 0.455555	val: 0.332281	test: 0.332184

Epoch: 12
Loss: 0.18260310376368982
ROC train: 0.866029	val: 0.748699	test: 0.713535
PRC train: 0.484563	val: 0.311226	test: 0.321238

Epoch: 13
Loss: 0.18131734775082148
ROC train: 0.865340	val: 0.761910	test: 0.732644
PRC train: 0.480520	val: 0.321753	test: 0.332159

Epoch: 14
Loss: 0.17859634490348988
ROC train: 0.867773	val: 0.755960	test: 0.718794
PRC train: 0.490573	val: 0.322484	test: 0.327294

Epoch: 15
Loss: 0.17867269551023973
ROC train: 0.877406	val: 0.769438	test: 0.740810
PRC train: 0.507132	val: 0.327674	test: 0.350056

Epoch: 16
Loss: 0.17654446713391145
ROC train: 0.877950	val: 0.775690	test: 0.734478
PRC train: 0.514883	val: 0.328655	test: 0.344169

Epoch: 17
Loss: 0.17332362442757776
ROC train: 0.879232	val: 0.766463	test: 0.730776
PRC train: 0.515610	val: 0.320074	test: 0.338398

Epoch: 18
Loss: 0.1720107077412885
ROC train: 0.884784	val: 0.768671	test: 0.733996
PRC train: 0.540843	val: 0.338685	test: 0.358106

Epoch: 19
Loss: 0.17021377643913208
ROC train: 0.886325	val: 0.776602	test: 0.728759
PRC train: 0.542875	val: 0.337161	test: 0.337213

Epoch: 20
Loss: 0.1694338154759599
ROC train: 0.889080	val: 0.769064	test: 0.739161
PRC train: 0.554802	val: 0.344609	test: 0.352186

Epoch: 21
Loss: 0.16769811282995165
ROC train: 0.894712	val: 0.770293	test: 0.739739
PRC train: 0.566475	val: 0.344791	test: 0.345296

Epoch: 22
Loss: 0.1656658653576592
ROC train: 0.897810	val: 0.766749	test: 0.733668
PRC train: 0.576669	val: 0.336012	test: 0.346740

Epoch: 23
Loss: 0.16464347208081392
ROC train: 0.902110	val: 0.771725	test: 0.746466
PRC train: 0.582379	val: 0.336180	test: 0.356178

Epoch: 24
Loss: 0.16276594039547432
ROC train: 0.899852	val: 0.760235	test: 0.729385
PRC train: 0.589112	val: 0.326790	test: 0.337793

Epoch: 25
Loss: 0.1624992186775035
ROC train: 0.906810	val: 0.760333	test: 0.720789
PRC train: 0.601792	val: 0.330101	test: 0.335023

Epoch: 26
Loss: 0.1585609014518354
ROC train: 0.906674	val: 0.771944	test: 0.737617
PRC train: 0.605394	val: 0.339366	test: 0.356628

Epoch: 27
Loss: 0.15938323836306315
ROC train: 0.911925	val: 0.767248	test: 0.728220
PRC train: 0.615992	val: 0.332426	test: 0.351157

Epoch: 28
Loss: 0.1581784826020376
ROC train: 0.906981	val: 0.764217	test: 0.726432
PRC train: 0.616092	val: 0.339451	test: 0.357443

Epoch: 29
Loss: 0.15746238381523206
ROC train: 0.914492	val: 0.780370	test: 0.738754
PRC train: 0.625991	val: 0.341065	test: 0.352699

Epoch: 30
Loss: 0.15612177532007412
ROC train: 0.914927	val: 0.762915	test: 0.735037
PRC train: 0.612222	val: 0.309794	test: 0.329453

Epoch: 31
Loss: 0.15283155208001437
ROC train: 0.919881	val: 0.775843	test: 0.730435
PRC train: 0.637860	val: 0.337531	test: 0.340726

Epoch: 32
Loss: 0.15130752087442453
ROC train: 0.921935	val: 0.772055	test: 0.726492
ROC train: 0.920762	val: 0.769507	test: 0.752624
PRC train: 0.639844	val: 0.366043	test: 0.353513

Epoch: 34
Loss: 0.15189437353659468
ROC train: 0.920175	val: 0.776641	test: 0.739652
PRC train: 0.629966	val: 0.351207	test: 0.351884

Epoch: 35
Loss: 0.15052326879344313
ROC train: 0.921828	val: 0.767029	test: 0.748121
PRC train: 0.642135	val: 0.354008	test: 0.364648

Epoch: 36
Loss: 0.14995821544908403
ROC train: 0.927040	val: 0.766129	test: 0.745602
PRC train: 0.653958	val: 0.368963	test: 0.356052

Epoch: 37
Loss: 0.1479313366511842
ROC train: 0.926391	val: 0.775898	test: 0.750655
PRC train: 0.652407	val: 0.362511	test: 0.355800

Epoch: 38
Loss: 0.147494431020008
ROC train: 0.927140	val: 0.774660	test: 0.756784
PRC train: 0.660959	val: 0.364268	test: 0.364344

Epoch: 39
Loss: 0.14653921668693393
ROC train: 0.929556	val: 0.767028	test: 0.749758
PRC train: 0.665875	val: 0.372597	test: 0.363482

Epoch: 40
Loss: 0.14698779076835528
ROC train: 0.932301	val: 0.782554	test: 0.758034
PRC train: 0.674476	val: 0.380140	test: 0.368689

Epoch: 41
Loss: 0.14350638852149006
ROC train: 0.933610	val: 0.769794	test: 0.738584
PRC train: 0.676191	val: 0.345755	test: 0.359277

Epoch: 42
Loss: 0.14407763136596297
ROC train: 0.934830	val: 0.769078	test: 0.753341
PRC train: 0.680476	val: 0.381318	test: 0.371850

Epoch: 43
Loss: 0.14309766034431867
ROC train: 0.933960	val: 0.772247	test: 0.745472
PRC train: 0.683346	val: 0.365560	test: 0.356671

Epoch: 44
Loss: 0.14156749263556342
ROC train: 0.938795	val: 0.772059	test: 0.741181
PRC train: 0.697658	val: 0.371272	test: 0.357692

Epoch: 45
Loss: 0.1410444145208366
ROC train: 0.939823	val: 0.772277	test: 0.740212
PRC train: 0.701216	val: 0.369387	test: 0.347825

Epoch: 46
Loss: 0.14040218894099232
ROC train: 0.939885	val: 0.765401	test: 0.735243
PRC train: 0.706053	val: 0.363013	test: 0.348167

Epoch: 47
Loss: 0.13913265861763813
ROC train: 0.942156	val: 0.766559	test: 0.735209
PRC train: 0.706083	val: 0.375358	test: 0.356769

Epoch: 48
Loss: 0.13695534517861552
ROC train: 0.942062	val: 0.763784	test: 0.737718
PRC train: 0.707270	val: 0.357463	test: 0.352476

Epoch: 49
Loss: 0.13685057444259752
ROC train: 0.945817	val: 0.765032	test: 0.730306
PRC train: 0.716371	val: 0.374744	test: 0.346275

Epoch: 50
Loss: 0.13826285714928338
ROC train: 0.943327	val: 0.777168	test: 0.741308
PRC train: 0.718402	val: 0.374377	test: 0.345011

Epoch: 51
Loss: 0.13649577959839992
ROC train: 0.944180	val: 0.763655	test: 0.728048
PRC train: 0.709868	val: 0.347743	test: 0.349842

Epoch: 52
Loss: 0.13658425649297629
ROC train: 0.947549	val: 0.766779	test: 0.737249
PRC train: 0.732477	val: 0.368958	test: 0.343135

Epoch: 53
Loss: 0.13457053244026235
ROC train: 0.948543	val: 0.761227	test: 0.736211
PRC train: 0.734715	val: 0.357073	test: 0.340074

Epoch: 54
Loss: 0.13283620864774442
ROC train: 0.949143	val: 0.775111	test: 0.737241
PRC train: 0.737688	val: 0.378523	test: 0.343275

Epoch: 55
Loss: 0.13461040814061723
ROC train: 0.950474	val: 0.778303	test: 0.740604
PRC train: 0.744006	val: 0.389865	test: 0.352529

Epoch: 56
Loss: 0.13518349106935018
ROC train: 0.953278	val: 0.775143	test: 0.737555
PRC train: 0.750629	val: 0.385356	test: 0.365608

Epoch: 57
Loss: 0.1316841815019399
ROC train: 0.952417	val: 0.788758	test: 0.744291
PRC train: 0.745466	val: 0.395405	test: 0.357433

Epoch: 58
Loss: 0.13448727574050529
ROC train: 0.951883	val: 0.773536	test: 0.737817
PRC train: 0.750636	val: 0.377660	test: 0.356159

Epoch: 59
Loss: 0.13063995029878617
ROC train: 0.956014	val: 0.772462	test: 0.738909
PRC train: 0.763704	val: 0.381766	test: 0.349662

Epoch: 60
Loss: 0.13170089922134423
ROC train: 0.953360	val: 0.764436	test: 0.719459
PRC train: 0.750529	val: 0.360462	test: 0.348062

Epoch: 61
Loss: 0.1305762390488337
ROC train: 0.955827	val: 0.774837	test: 0.735980
PRC train: 0.758411	val: 0.382276	test: 0.359495

Epoch: 62
Loss: 0.12929580381061842
ROC train: 0.958348	val: 0.770871	test: 0.740661
PRC train: 0.771170	val: 0.376461	test: 0.350333

Epoch: 63
Loss: 0.1270996233214513
ROC train: 0.959847	val: 0.774175	test: 0.733777
PRC train: 0.778257	val: 0.376661	test: 0.359515

Epoch: 64
Loss: 0.12611636905008308
ROC train: 0.958300	val: 0.780964	test: 0.729977
PRC train: 0.777899	val: 0.386170	test: 0.358475

Epoch: 65
Loss: 0.12493485731593904
ROC train: 0.959876	val: 0.766466	test: 0.726461
PRC train: 0.773226	val: 0.363585	test: 0.344650

Epoch: 66
Loss: 0.12525158251159577
ROC train: 0.961100	val: 0.774346	test: 0.738956
PRC train: 0.787704	val: 0.370955	test: 0.344253

Epoch: 67
Loss: 0.12640521577728842
ROC train: 0.960810	val: 0.762338	test: 0.726243
PRC train: 0.778457	val: 0.354703	test: 0.335037

Epoch: 68
Loss: 0.12295034691870725
ROC train: 0.962410	val: 0.780072	test: 0.733358
PRC train: 0.794841	val: 0.386854	test: 0.355204

Epoch: 69
Loss: 0.12493051596772065
ROC train: 0.963949	val: 0.764506	test: 0.728589
PRC train: 0.792545	val: 0.360456	test: 0.341623

Epoch: 70
Loss: 0.123549325285287
ROC train: 0.963879	val: 0.776111	test: 0.736347
PRC train: 0.801690	val: 0.378758	test: 0.357558

Epoch: 71
Loss: 0.12050911684662229
ROC train: 0.965707	val: 0.775899	test: 0.734032
PRC train: 0.802633	val: 0.382957	test: 0.347984

Epoch: 72
Loss: 0.1215022497758313
ROC train: 0.966768	val: 0.771442	test: 0.728253
PRC train: 0.807786	val: 0.366288	test: 0.344695

Epoch: 73
Loss: 0.12040445437244049
ROC train: 0.966080	val: 0.771917	test: 0.730786
PRC train: 0.804645	val: 0.368670	test: 0.355385

Epoch: 74
Loss: 0.12091223295726138
ROC train: 0.966440	val: 0.775821	test: 0.729515
PRC train: 0.812718	val: 0.369641	test: 0.349628

Epoch: 75
Loss: 0.11893090208708992
ROC train: 0.965094	val: 0.773510	test: 0.736224
PRC train: 0.810064	val: 0.377772	test: 0.329255

Epoch: 76
Loss: 0.1190939364949396
ROC train: 0.964936	val: 0.770066	test: 0.723220
PRC train: 0.793749	val: 0.357944	test: 0.342164

Epoch: 77
Loss: 0.11929053586210064
ROC train: 0.966370	val: 0.769833	test: 0.726916
PRC train: 0.812497	val: 0.375750	test: 0.331262

Epoch: 78
Loss: 0.11693326410054855
ROC train: 0.970022	val: 0.770519	test: 0.732070
PRC train: 0.823558	val: 0.359800	test: 0.339132

Epoch: 79
Loss: 0.11660811517982245
ROC train: 0.969044	val: 0.768281	test: 0.730037
PRC train: 0.825597	val: 0.364022	test: 0.325406

Epoch: 80
Loss: 0.1175646828749185
ROC train: 0.970649	val: 0.773744	test: 0.738648
PRC train: 0.829491	val: 0.378511	test: 0.350564

Epoch: 81
Loss: 0.11768085634570971
ROC train: 0.970490	val: 0.767144	test: 0.729926
PRC train: 0.824659	val: 0.364446	test: 0.344363

Epoch: 82
Loss: 0.11652654286833018
ROC train: 0.971073	val: 0.775603	test: 0.737858
PRC train: 0.832238	val: 0.395078	test: 0.374313

Epoch: 83
Loss: 0.11471557797342097
ROC train: 0.970671	val: 0.773332	test: 0.731911
PRC train: 0.833200	val: 0.372848	test: 0.335344

Epoch: 84
Loss: 0.11307411003774416
ROC train: 0.972210	val: 0.777576	test: 0.730368
PRC train: 0.838521	val: 0.374518	test: 0.347386

Epoch: 85
Loss: 0.11421919117289134
ROC train: 0.973155	val: 0.776435	test: 0.733783
PRC train: 0.844946	val: 0.382278	test: 0.356455

Epoch: 86
Loss: 0.11177150018233518
ROC train: 0.974055	val: 0.765747	test: 0.727026
PRC train: 0.843912	val: 0.378333	test: 0.349606

Epoch: 87
Loss: 0.11233543425090114
ROC train: 0.973555	val: 0.779637	test: 0.725630
PRC train: 0.843809	val: 0.378381	test: 0.353624

Epoch: 88
Loss: 0.11230722126125846
ROC train: 0.975244	val: 0.765718	test: 0.732581
PRC train: 0.850238	val: 0.380614	test: 0.351589

Epoch: 89
Loss: 0.11086074031660856
ROC train: 0.975220	val: 0.772205	test: 0.731073
PRC train: 0.851322	val: 0.384315	test: 0.363143

Epoch: 90
Loss: 0.10779209538161584
ROC train: 0.976671	val: 0.768381	test: 0.718688
PRC train: 0.856195	val: 0.369206	test: 0.327619

Epoch: 91
Loss: 0.10962584969925473
ROC train: 0.976778	val: 0.760594	test: 0.716977
PRC train: 0.855143	val: 0.370191	test: 0.343230

Epoch: 92
Loss: 0.10971859487267789
ROC train: 0.977533	val: 0.770260	test: 0.715678
PRC train: 0.861840	val: 0.387737	test: 0.356027

Epoch: 93
Loss: 0.10727667789792106
ROC train: 0.978022	val: 0.769942	test: 0.724271
PRC train: 0.864057	val: 0.372241	test: 0.341723

Epoch: 94
Loss: 0.10648977740360664
ROC train: 0.917625	val: 0.776230	test: 0.741363
PRC train: 0.634633	val: 0.348774	test: 0.380471

Epoch: 34
Loss: 0.14951499466116222
ROC train: 0.920411	val: 0.780403	test: 0.738833
PRC train: 0.639198	val: 0.337306	test: 0.359090

Epoch: 35
Loss: 0.14962597863432953
ROC train: 0.917813	val: 0.788417	test: 0.751216
PRC train: 0.634901	val: 0.353921	test: 0.374794

Epoch: 36
Loss: 0.1491982105572648
ROC train: 0.922994	val: 0.776889	test: 0.742139
PRC train: 0.653111	val: 0.366237	test: 0.357355

Epoch: 37
Loss: 0.1466225470351923
ROC train: 0.921220	val: 0.786016	test: 0.749018
PRC train: 0.637711	val: 0.340443	test: 0.371412

Epoch: 38
Loss: 0.14887885320623404
ROC train: 0.923482	val: 0.778343	test: 0.746417
PRC train: 0.653622	val: 0.336321	test: 0.362635

Epoch: 39
Loss: 0.14739921464050412
ROC train: 0.926254	val: 0.781543	test: 0.744538
PRC train: 0.658619	val: 0.348663	test: 0.353106

Epoch: 40
Loss: 0.14652643804374588
ROC train: 0.930432	val: 0.781100	test: 0.751655
PRC train: 0.673679	val: 0.352887	test: 0.365442

Epoch: 41
Loss: 0.1464897430681011
ROC train: 0.933136	val: 0.776399	test: 0.741560
PRC train: 0.683572	val: 0.368872	test: 0.358442

Epoch: 42
Loss: 0.14509512134096159
ROC train: 0.929493	val: 0.788948	test: 0.755047
PRC train: 0.674761	val: 0.382341	test: 0.385479

Epoch: 43
Loss: 0.14437387724434367
ROC train: 0.932192	val: 0.791770	test: 0.757544
PRC train: 0.678557	val: 0.368106	test: 0.375784

Epoch: 44
Loss: 0.1427682985305291
ROC train: 0.934198	val: 0.782140	test: 0.751349
PRC train: 0.688551	val: 0.374980	test: 0.382764

Epoch: 45
Loss: 0.14226783390097175
ROC train: 0.933740	val: 0.761860	test: 0.743624
PRC train: 0.678885	val: 0.345614	test: 0.348267

Epoch: 46
Loss: 0.14332940995647037
ROC train: 0.935311	val: 0.778689	test: 0.751372
PRC train: 0.689674	val: 0.364259	test: 0.358696

Epoch: 47
Loss: 0.14334599162243916
ROC train: 0.938721	val: 0.786457	test: 0.747058
PRC train: 0.702051	val: 0.371609	test: 0.367268

Epoch: 48
Loss: 0.1394013462643337
ROC train: 0.939182	val: 0.783276	test: 0.748114
PRC train: 0.697553	val: 0.382765	test: 0.366337

Epoch: 49
Loss: 0.13984333636007634
ROC train: 0.941689	val: 0.781794	test: 0.754783
PRC train: 0.709789	val: 0.380122	test: 0.373560

Epoch: 50
Loss: 0.13885709904107002
ROC train: 0.942142	val: 0.783477	test: 0.747002
PRC train: 0.713926	val: 0.361633	test: 0.358310

Epoch: 51
Loss: 0.13643704691855163
ROC train: 0.943747	val: 0.778063	test: 0.747138
PRC train: 0.717214	val: 0.359074	test: 0.348634

Epoch: 52
Loss: 0.13699296243742276
ROC train: 0.942924	val: 0.785271	test: 0.749663
PRC train: 0.710549	val: 0.373268	test: 0.344232

Epoch: 53
Loss: 0.13660905244057203
ROC train: 0.946643	val: 0.772453	test: 0.735087
PRC train: 0.722036	val: 0.350467	test: 0.344906

Epoch: 54
Loss: 0.13663560256334478
ROC train: 0.946277	val: 0.788013	test: 0.753605
PRC train: 0.728914	val: 0.377368	test: 0.363357

Epoch: 55
Loss: 0.13533365824821245
ROC train: 0.948253	val: 0.785374	test: 0.744666
PRC train: 0.728797	val: 0.354685	test: 0.362830

Epoch: 56
Loss: 0.13358221194280878
ROC train: 0.949815	val: 0.786400	test: 0.737332
PRC train: 0.737851	val: 0.378037	test: 0.361091

Epoch: 57
Loss: 0.13326041351074272
ROC train: 0.946723	val: 0.769442	test: 0.739840
PRC train: 0.726031	val: 0.356676	test: 0.370195

Epoch: 58
Loss: 0.13411798410166825
ROC train: 0.951806	val: 0.761896	test: 0.743570
PRC train: 0.745052	val: 0.353850	test: 0.351912

Epoch: 59
Loss: 0.1318559776552764
ROC train: 0.951788	val: 0.775356	test: 0.740493
PRC train: 0.739679	val: 0.336189	test: 0.333505

Epoch: 60
Loss: 0.1316581396170228
ROC train: 0.954331	val: 0.768173	test: 0.739770
PRC train: 0.747371	val: 0.348462	test: 0.369390

Epoch: 61
Loss: 0.13016474383267368
ROC train: 0.954255	val: 0.756006	test: 0.742970
PRC train: 0.750555	val: 0.337160	test: 0.331401

Epoch: 62
Loss: 0.1292519954721052
ROC train: 0.956234	val: 0.775227	test: 0.737793
PRC train: 0.755917	val: 0.356256	test: 0.358135

Epoch: 63
Loss: 0.13043945116647207
ROC train: 0.956555	val: 0.769873	test: 0.740264
PRC train: 0.760240	val: 0.352428	test: 0.350846

Epoch: 64
Loss: 0.1309640325197168
ROC train: 0.955880	val: 0.773192	test: 0.732681
PRC train: 0.757155	val: 0.346200	test: 0.338461

Epoch: 65
Loss: 0.12851314191063243
ROC train: 0.957874	val: 0.780252	test: 0.745027
PRC train: 0.764951	val: 0.354472	test: 0.354575

Epoch: 66
Loss: 0.12716295314269319
ROC train: 0.957382	val: 0.764929	test: 0.735408
PRC train: 0.763853	val: 0.330400	test: 0.337087

Epoch: 67
Loss: 0.127747757090084
ROC train: 0.960855	val: 0.779188	test: 0.744783
PRC train: 0.778101	val: 0.362191	test: 0.354285

Epoch: 68
Loss: 0.12709145288784554
ROC train: 0.958467	val: 0.771736	test: 0.743793
PRC train: 0.767209	val: 0.352074	test: 0.361306

Epoch: 69
Loss: 0.1262503041458656
ROC train: 0.960762	val: 0.768961	test: 0.737156
PRC train: 0.779862	val: 0.345930	test: 0.348027

Epoch: 70
Loss: 0.12446808558804014
ROC train: 0.962011	val: 0.770064	test: 0.736086
PRC train: 0.783052	val: 0.344387	test: 0.330889

Epoch: 71
Loss: 0.12230254291636079
ROC train: 0.961093	val: 0.779256	test: 0.743509
PRC train: 0.780669	val: 0.380246	test: 0.379842

Epoch: 72
Loss: 0.1239945811002244
ROC train: 0.963008	val: 0.768174	test: 0.739852
PRC train: 0.775891	val: 0.361456	test: 0.350657

Epoch: 73
Loss: 0.12134322674303918
ROC train: 0.960287	val: 0.758809	test: 0.728513
PRC train: 0.775558	val: 0.342055	test: 0.352183

Epoch: 74
Loss: 0.12245150634039481
ROC train: 0.965776	val: 0.778397	test: 0.736179
PRC train: 0.800339	val: 0.379367	test: 0.352420

Epoch: 75
Loss: 0.12135839460691765
ROC train: 0.962269	val: 0.772966	test: 0.733485
PRC train: 0.783704	val: 0.362284	test: 0.341215

Epoch: 76
Loss: 0.12157007971188703
ROC train: 0.966389	val: 0.760354	test: 0.729006
PRC train: 0.803612	val: 0.360006	test: 0.347423

Epoch: 77
Loss: 0.12081108635093145
ROC train: 0.964971	val: 0.769124	test: 0.730096
PRC train: 0.799955	val: 0.354500	test: 0.335318

Epoch: 78
Loss: 0.1192799597032859
ROC train: 0.962193	val: 0.757893	test: 0.728804
PRC train: 0.783676	val: 0.354461	test: 0.346616

Epoch: 79
Loss: 0.11960750947143296
ROC train: 0.964000	val: 0.771318	test: 0.735142
PRC train: 0.795413	val: 0.352029	test: 0.352103

Epoch: 80
Loss: 0.11677459466732892
ROC train: 0.968803	val: 0.763757	test: 0.745844
PRC train: 0.816984	val: 0.345921	test: 0.344246

Epoch: 81
Loss: 0.11767110323651052
ROC train: 0.968714	val: 0.761029	test: 0.734404
PRC train: 0.817819	val: 0.352390	test: 0.333554

Epoch: 82
Loss: 0.11839613997151165
ROC train: 0.970545	val: 0.763781	test: 0.734192
PRC train: 0.824121	val: 0.356784	test: 0.354929

Epoch: 83
Loss: 0.11632416832462837
ROC train: 0.970068	val: 0.766142	test: 0.739172
PRC train: 0.821825	val: 0.362044	test: 0.353686

Epoch: 84
Loss: 0.11445646940230221
ROC train: 0.969015	val: 0.765143	test: 0.733356
PRC train: 0.812162	val: 0.354852	test: 0.340948

Epoch: 85
Loss: 0.11609048255937715
ROC train: 0.970719	val: 0.772111	test: 0.742179
PRC train: 0.823514	val: 0.374948	test: 0.350771

Epoch: 86
Loss: 0.1143063103093527
ROC train: 0.972002	val: 0.756266	test: 0.736375
PRC train: 0.830787	val: 0.347755	test: 0.346238

Epoch: 87
Loss: 0.11092963252042443
ROC train: 0.972696	val: 0.750864	test: 0.724047
PRC train: 0.831251	val: 0.338648	test: 0.346699

Epoch: 88
Loss: 0.11520436889594242
ROC train: 0.972765	val: 0.757998	test: 0.732389
PRC train: 0.836811	val: 0.356022	test: 0.350349

Epoch: 89
Loss: 0.1121080515244887
ROC train: 0.974618	val: 0.759020	test: 0.731512
PRC train: 0.842617	val: 0.349458	test: 0.340204

Epoch: 90
Loss: 0.11124439425527162
ROC train: 0.972764	val: 0.770593	test: 0.734577
PRC train: 0.839887	val: 0.374904	test: 0.353701

Epoch: 91
Loss: 0.10973385336566274
ROC train: 0.974839	val: 0.756977	test: 0.735641
PRC train: 0.847831	val: 0.360453	test: 0.346984

Epoch: 92
Loss: 0.11173437777292124
ROC train: 0.975582	val: 0.761689	test: 0.733637
PRC train: 0.847507	val: 0.366193	test: 0.348649

Epoch: 93
Loss: 0.11168840762684523
ROC train: 0.975048	val: 0.761073	test: 0.734542
PRC train: 0.847551	val: 0.343749	test: 0.347240

Epoch: 94
Loss: 0.11111345208811599
ROC train: 0.920060	val: 0.777297	test: 0.755057
PRC train: 0.645262	val: 0.382118	test: 0.362695

Epoch: 34
Loss: 0.15192097944049449
ROC train: 0.919158	val: 0.780732	test: 0.755710
PRC train: 0.645376	val: 0.392986	test: 0.364482

Epoch: 35
Loss: 0.15012029269291627
ROC train: 0.921994	val: 0.782889	test: 0.763611
PRC train: 0.645743	val: 0.379031	test: 0.360129

Epoch: 36
Loss: 0.1476032166708284
ROC train: 0.918149	val: 0.767549	test: 0.762404
PRC train: 0.637570	val: 0.358908	test: 0.361694

Epoch: 37
Loss: 0.1473935621326548
ROC train: 0.926489	val: 0.772394	test: 0.745715
PRC train: 0.660396	val: 0.380000	test: 0.371902

Epoch: 38
Loss: 0.14644033248779237
ROC train: 0.926126	val: 0.780677	test: 0.768379
PRC train: 0.662373	val: 0.383638	test: 0.380443

Epoch: 39
Loss: 0.14792581197877186
ROC train: 0.929359	val: 0.782516	test: 0.757447
PRC train: 0.669800	val: 0.386025	test: 0.363951

Epoch: 40
Loss: 0.1462309378683042
ROC train: 0.929886	val: 0.771183	test: 0.755274
PRC train: 0.675799	val: 0.376654	test: 0.376371

Epoch: 41
Loss: 0.14490488462324483
ROC train: 0.932317	val: 0.773724	test: 0.747648
PRC train: 0.683269	val: 0.378328	test: 0.359648

Epoch: 42
Loss: 0.1449956871567326
ROC train: 0.933326	val: 0.767464	test: 0.753996
PRC train: 0.685767	val: 0.369585	test: 0.343669

Epoch: 43
Loss: 0.14114654125606474
ROC train: 0.934960	val: 0.777281	test: 0.764661
PRC train: 0.690894	val: 0.377954	test: 0.377523

Epoch: 44
Loss: 0.14180473017486034
ROC train: 0.936627	val: 0.775636	test: 0.753319
PRC train: 0.695433	val: 0.378642	test: 0.361933

Epoch: 45
Loss: 0.14083187085722168
ROC train: 0.937622	val: 0.768239	test: 0.758967
PRC train: 0.705707	val: 0.376193	test: 0.349684

Epoch: 46
Loss: 0.14233384655726988
ROC train: 0.934895	val: 0.779509	test: 0.749179
PRC train: 0.688197	val: 0.391696	test: 0.351429

Epoch: 47
Loss: 0.14055482000686456
ROC train: 0.939911	val: 0.774077	test: 0.750943
PRC train: 0.705092	val: 0.372410	test: 0.348518

Epoch: 48
Loss: 0.13931338502196935
ROC train: 0.941403	val: 0.763484	test: 0.750787
PRC train: 0.711822	val: 0.373097	test: 0.357347

Epoch: 49
Loss: 0.13858027386791533
ROC train: 0.942619	val: 0.774333	test: 0.750219
PRC train: 0.720067	val: 0.367327	test: 0.352003

Epoch: 50
Loss: 0.13676813541070412
ROC train: 0.943914	val: 0.771924	test: 0.750645
PRC train: 0.723531	val: 0.374136	test: 0.352540

Epoch: 51
Loss: 0.13571953401244216
ROC train: 0.944743	val: 0.781604	test: 0.758604
PRC train: 0.727142	val: 0.378484	test: 0.354705

Epoch: 52
Loss: 0.13642344304386875
ROC train: 0.945048	val: 0.775954	test: 0.753734
PRC train: 0.728340	val: 0.390432	test: 0.368549

Epoch: 53
Loss: 0.1357713757272129
ROC train: 0.946980	val: 0.766944	test: 0.750396
PRC train: 0.732256	val: 0.375609	test: 0.349003

Epoch: 54
Loss: 0.13366600787002
ROC train: 0.948450	val: 0.770752	test: 0.749751
PRC train: 0.740607	val: 0.381580	test: 0.370323

Epoch: 55
Loss: 0.13242051733597524
ROC train: 0.947010	val: 0.768065	test: 0.757887
PRC train: 0.735006	val: 0.377053	test: 0.361909

Epoch: 56
Loss: 0.13175775013681484
ROC train: 0.950894	val: 0.772637	test: 0.754045
PRC train: 0.746557	val: 0.390928	test: 0.355328

Epoch: 57
Loss: 0.13291398806654006
ROC train: 0.951156	val: 0.772524	test: 0.746727
PRC train: 0.751131	val: 0.364289	test: 0.339135

Epoch: 58
Loss: 0.13098420116756063
ROC train: 0.952042	val: 0.771685	test: 0.753777
PRC train: 0.749599	val: 0.382671	test: 0.365601

Epoch: 59
Loss: 0.1296608117400856
ROC train: 0.954477	val: 0.763830	test: 0.738069
PRC train: 0.762461	val: 0.371091	test: 0.359817

Epoch: 60
Loss: 0.13016685098058756
ROC train: 0.954961	val: 0.765704	test: 0.748522
PRC train: 0.760134	val: 0.381532	test: 0.370031

Epoch: 61
Loss: 0.1303732614856571
ROC train: 0.955185	val: 0.766416	test: 0.747463
PRC train: 0.762312	val: 0.369459	test: 0.353479

Epoch: 62
Loss: 0.12749331499264124
ROC train: 0.956020	val: 0.771920	test: 0.749247
PRC train: 0.767300	val: 0.383028	test: 0.362390

Epoch: 63
Loss: 0.12674657741314596
ROC train: 0.958973	val: 0.762774	test: 0.739160
PRC train: 0.778193	val: 0.366083	test: 0.340526

Epoch: 64
Loss: 0.1260870564206751
ROC train: 0.959156	val: 0.768252	test: 0.743551
PRC train: 0.778066	val: 0.380781	test: 0.365523

Epoch: 65
Loss: 0.12593126653002892
ROC train: 0.958748	val: 0.775914	test: 0.755879
PRC train: 0.779807	val: 0.376060	test: 0.371948

Epoch: 66
Loss: 0.12579268665734358
ROC train: 0.959619	val: 0.767632	test: 0.745425
PRC train: 0.787075	val: 0.375333	test: 0.369785

Epoch: 67
Loss: 0.12396977980615026
ROC train: 0.958804	val: 0.779342	test: 0.747837
PRC train: 0.778546	val: 0.384658	test: 0.362660

Epoch: 68
Loss: 0.12493899727125168
ROC train: 0.960737	val: 0.764640	test: 0.746060
PRC train: 0.780613	val: 0.365596	test: 0.354522

Epoch: 69
Loss: 0.1237278506538046
ROC train: 0.962154	val: 0.770665	test: 0.746543
PRC train: 0.798930	val: 0.377640	test: 0.353588

Epoch: 70
Loss: 0.12107551391582907
ROC train: 0.963449	val: 0.769528	test: 0.746013
PRC train: 0.797372	val: 0.380407	test: 0.350309

Epoch: 71
Loss: 0.12272336285613723
ROC train: 0.962774	val: 0.763362	test: 0.745704
PRC train: 0.794300	val: 0.363601	test: 0.356175

Epoch: 72
Loss: 0.12139357700018172
ROC train: 0.964580	val: 0.767861	test: 0.749192
PRC train: 0.797560	val: 0.378202	test: 0.358461

Epoch: 73
Loss: 0.1208692751548674
ROC train: 0.965306	val: 0.758646	test: 0.751945
PRC train: 0.809034	val: 0.361364	test: 0.343163

Epoch: 74
Loss: 0.11923494361906428
ROC train: 0.966336	val: 0.773775	test: 0.748416
PRC train: 0.811704	val: 0.385219	test: 0.362236

Epoch: 75
Loss: 0.11999902901939968
ROC train: 0.966433	val: 0.771210	test: 0.737968
PRC train: 0.805817	val: 0.371338	test: 0.343843

Epoch: 76
Loss: 0.11888870992090343
ROC train: 0.966140	val: 0.771362	test: 0.744505
PRC train: 0.804651	val: 0.383323	test: 0.353129

Epoch: 77
Loss: 0.11748824984509121
ROC train: 0.969272	val: 0.763484	test: 0.735042
PRC train: 0.821892	val: 0.371588	test: 0.349343

Epoch: 78
Loss: 0.11654641828812641
ROC train: 0.968425	val: 0.764599	test: 0.735364
PRC train: 0.821648	val: 0.378915	test: 0.355910

Epoch: 79
Loss: 0.11611169838709377
ROC train: 0.970186	val: 0.768168	test: 0.751667
PRC train: 0.825975	val: 0.391782	test: 0.378515

Epoch: 80
Loss: 0.11636730421080119
ROC train: 0.968239	val: 0.768053	test: 0.742727
PRC train: 0.815951	val: 0.381642	test: 0.347863

Epoch: 81
Loss: 0.11591850144966247
ROC train: 0.970769	val: 0.769983	test: 0.739276
PRC train: 0.828621	val: 0.371494	test: 0.350641

Epoch: 82
Loss: 0.11370658942946581
ROC train: 0.971112	val: 0.772962	test: 0.757624
PRC train: 0.831703	val: 0.385508	test: 0.370816

Epoch: 83
Loss: 0.11239272690558284
ROC train: 0.972056	val: 0.769845	test: 0.745296
PRC train: 0.833032	val: 0.380178	test: 0.345797

Epoch: 84
Loss: 0.11259093170870939
ROC train: 0.973267	val: 0.768417	test: 0.743339
PRC train: 0.837797	val: 0.389493	test: 0.371864

Epoch: 85
Loss: 0.1143368637300954
ROC train: 0.973024	val: 0.772107	test: 0.740813
PRC train: 0.838761	val: 0.387018	test: 0.360467

Epoch: 86
Loss: 0.11213584909156228
ROC train: 0.973342	val: 0.767894	test: 0.732918
PRC train: 0.840754	val: 0.377557	test: 0.346841

Epoch: 87
Loss: 0.11321988600869883
ROC train: 0.973209	val: 0.774665	test: 0.753488
PRC train: 0.838296	val: 0.391687	test: 0.374663

Epoch: 88
Loss: 0.10920397802502169
ROC train: 0.974176	val: 0.771048	test: 0.743960
PRC train: 0.841006	val: 0.395303	test: 0.362245

Epoch: 89
Loss: 0.11090188621119865
ROC train: 0.974921	val: 0.776615	test: 0.745750
PRC train: 0.846620	val: 0.395595	test: 0.367204

Epoch: 90
Loss: 0.11224909028780412
ROC train: 0.974901	val: 0.770953	test: 0.739761
PRC train: 0.848514	val: 0.383927	test: 0.340064

Epoch: 91
Loss: 0.11056796318294403
ROC train: 0.976072	val: 0.765747	test: 0.746181
PRC train: 0.853082	val: 0.392044	test: 0.353088

Epoch: 92
Loss: 0.107617576333811
ROC train: 0.976181	val: 0.776092	test: 0.746835
PRC train: 0.852852	val: 0.401042	test: 0.360873

Epoch: 93
Loss: 0.10853116366483553
ROC train: 0.975946	val: 0.775163	test: 0.754492
PRC train: 0.856088	val: 0.407742	test: 0.357259

Epoch: 94
Loss: 0.10767876021113969
PRC train: 0.644845	val: 0.335313	test: 0.347081

Epoch: 33
Loss: 0.15664970632665523
ROC train: 0.925971	val: 0.755036	test: 0.724438
PRC train: 0.644676	val: 0.346070	test: 0.346819

Epoch: 34
Loss: 0.15435331113435063
ROC train: 0.924307	val: 0.761559	test: 0.730625
PRC train: 0.648954	val: 0.360698	test: 0.356741

Epoch: 35
Loss: 0.15303201788969403
ROC train: 0.931703	val: 0.748862	test: 0.721749
PRC train: 0.671191	val: 0.341725	test: 0.346261

Epoch: 36
Loss: 0.14937603584989018
ROC train: 0.935510	val: 0.754057	test: 0.723105
PRC train: 0.684358	val: 0.354838	test: 0.360564

Epoch: 37
Loss: 0.15075304707981335
ROC train: 0.936957	val: 0.762033	test: 0.724090
PRC train: 0.690300	val: 0.346820	test: 0.357314

Epoch: 38
Loss: 0.14880817700529764
ROC train: 0.934254	val: 0.763144	test: 0.723628
PRC train: 0.683730	val: 0.346126	test: 0.342360

Epoch: 39
Loss: 0.14831104879884693
ROC train: 0.937537	val: 0.763427	test: 0.727340
PRC train: 0.681598	val: 0.359515	test: 0.345444

Epoch: 40
Loss: 0.14670133759358694
ROC train: 0.942449	val: 0.761186	test: 0.717962
PRC train: 0.698372	val: 0.349475	test: 0.333363

Epoch: 41
Loss: 0.14605064778602764
ROC train: 0.939329	val: 0.771097	test: 0.724489
PRC train: 0.701071	val: 0.364087	test: 0.354347

Epoch: 42
Loss: 0.1442220874398189
ROC train: 0.942455	val: 0.758967	test: 0.713194
PRC train: 0.716238	val: 0.358975	test: 0.352473

Epoch: 43
Loss: 0.14390529173902272
ROC train: 0.945329	val: 0.746867	test: 0.708395
PRC train: 0.720327	val: 0.333461	test: 0.337496

Epoch: 44
Loss: 0.1421811474039194
ROC train: 0.946432	val: 0.748227	test: 0.713850
PRC train: 0.718332	val: 0.333555	test: 0.345340

Epoch: 45
Loss: 0.14195518462860493
ROC train: 0.949287	val: 0.746435	test: 0.710845
PRC train: 0.726993	val: 0.343344	test: 0.346817

Epoch: 46
Loss: 0.13805047774424545
ROC train: 0.949869	val: 0.752995	test: 0.720860
PRC train: 0.741836	val: 0.359838	test: 0.348820

Epoch: 47
Loss: 0.1364630148788422
ROC train: 0.952830	val: 0.756781	test: 0.719664
PRC train: 0.746611	val: 0.355869	test: 0.353485

Epoch: 48
Loss: 0.13783038230369152
ROC train: 0.955473	val: 0.747075	test: 0.702323
PRC train: 0.759065	val: 0.356804	test: 0.342212

Epoch: 49
Loss: 0.13631262558181828
ROC train: 0.953684	val: 0.751177	test: 0.717539
PRC train: 0.754068	val: 0.337825	test: 0.350866

Epoch: 50
Loss: 0.13510154544317599
ROC train: 0.955738	val: 0.749083	test: 0.708165
PRC train: 0.759915	val: 0.354908	test: 0.337479

Epoch: 51
Loss: 0.13380728342131312
ROC train: 0.958639	val: 0.743238	test: 0.717069
PRC train: 0.765815	val: 0.329734	test: 0.351662

Epoch: 52
Loss: 0.13341956425720097
ROC train: 0.959436	val: 0.752003	test: 0.713322
PRC train: 0.774770	val: 0.342806	test: 0.348770

Epoch: 53
Loss: 0.1302574118238004
ROC train: 0.962520	val: 0.749587	test: 0.712074
PRC train: 0.785868	val: 0.346776	test: 0.360833

Epoch: 54
Loss: 0.1297900166099425
ROC train: 0.963315	val: 0.750176	test: 0.712436
PRC train: 0.791904	val: 0.356709	test: 0.349819

Epoch: 55
Loss: 0.1299322451467577
ROC train: 0.964258	val: 0.736309	test: 0.705547
PRC train: 0.790803	val: 0.316285	test: 0.324676

Epoch: 56
Loss: 0.12690813773199902
ROC train: 0.965474	val: 0.743943	test: 0.716185
PRC train: 0.802364	val: 0.339954	test: 0.347776

Epoch: 57
Loss: 0.12441423304251377
ROC train: 0.967854	val: 0.752021	test: 0.715393
PRC train: 0.806180	val: 0.353903	test: 0.344760

Epoch: 58
Loss: 0.12367218719559674
ROC train: 0.969188	val: 0.748709	test: 0.707519
PRC train: 0.814121	val: 0.356049	test: 0.341581

Epoch: 59
Loss: 0.12457604204899014
ROC train: 0.967191	val: 0.751440	test: 0.711924
PRC train: 0.810553	val: 0.361776	test: 0.342670

Epoch: 60
Loss: 0.12136252832959243
ROC train: 0.972556	val: 0.745261	test: 0.714595
PRC train: 0.828601	val: 0.350846	test: 0.348975

Epoch: 61
Loss: 0.12217001970220323
ROC train: 0.971896	val: 0.742527	test: 0.708406
PRC train: 0.821688	val: 0.341724	test: 0.329158

Epoch: 62
Loss: 0.12074663865289544
ROC train: 0.971974	val: 0.755889	test: 0.724388
PRC train: 0.829677	val: 0.328960	test: 0.343995

Epoch: 63
Loss: 0.12088959618016949
ROC train: 0.974741	val: 0.746317	test: 0.713758
PRC train: 0.840622	val: 0.339190	test: 0.348160

Epoch: 64
Loss: 0.11517615038120815
ROC train: 0.974193	val: 0.734847	test: 0.710013
PRC train: 0.834002	val: 0.325973	test: 0.316261

Epoch: 65
Loss: 0.11576661117446903
ROC train: 0.975871	val: 0.741219	test: 0.701654
PRC train: 0.846126	val: 0.333428	test: 0.326513

Epoch: 66
Loss: 0.11828564853219767
ROC train: 0.976296	val: 0.740369	test: 0.706505
PRC train: 0.850713	val: 0.343128	test: 0.334712

Epoch: 67
Loss: 0.1147057612630233
ROC train: 0.977881	val: 0.733722	test: 0.701502
PRC train: 0.850752	val: 0.307553	test: 0.304787

Epoch: 68
Loss: 0.11512869277187264
ROC train: 0.978042	val: 0.738492	test: 0.716500
PRC train: 0.855080	val: 0.323217	test: 0.319903

Epoch: 69
Loss: 0.11446964695949319
ROC train: 0.977700	val: 0.748929	test: 0.724661
PRC train: 0.854300	val: 0.356990	test: 0.355637

Epoch: 70
Loss: 0.11203779107467847
ROC train: 0.980144	val: 0.747696	test: 0.721265
PRC train: 0.863332	val: 0.349609	test: 0.347962

Epoch: 71
Loss: 0.11102507287273757
ROC train: 0.981519	val: 0.739187	test: 0.715249
PRC train: 0.871855	val: 0.352770	test: 0.321130

Epoch: 72
Loss: 0.11060046982557932
ROC train: 0.983336	val: 0.753293	test: 0.716229
PRC train: 0.884557	val: 0.352454	test: 0.342192

Epoch: 73
Loss: 0.10909458987474954
ROC train: 0.982877	val: 0.749172	test: 0.721268
PRC train: 0.880273	val: 0.362434	test: 0.344467

Epoch: 74
Loss: 0.11000281488672331
ROC train: 0.981998	val: 0.740405	test: 0.708448
PRC train: 0.879484	val: 0.330248	test: 0.323897

Epoch: 75
Loss: 0.10837619385104215
ROC train: 0.984583	val: 0.738852	test: 0.707511
PRC train: 0.888196	val: 0.338950	test: 0.332606

Epoch: 76
Loss: 0.1055830617741793
ROC train: 0.984515	val: 0.751701	test: 0.713316
PRC train: 0.891189	val: 0.351267	test: 0.339798

Epoch: 77
Loss: 0.10596444348758753
ROC train: 0.985401	val: 0.739427	test: 0.724090
PRC train: 0.889356	val: 0.328151	test: 0.348670

Epoch: 78
Loss: 0.10459949262827664
ROC train: 0.984201	val: 0.746075	test: 0.708164
PRC train: 0.890520	val: 0.348056	test: 0.336288

Epoch: 79
Loss: 0.10456611380279181
ROC train: 0.984769	val: 0.744109	test: 0.707439
PRC train: 0.887199	val: 0.321739	test: 0.331779

Epoch: 80
Loss: 0.10220392053820315
ROC train: 0.987832	val: 0.743094	test: 0.705278
PRC train: 0.910009	val: 0.335258	test: 0.319274

Epoch: 81
Loss: 0.10122375365882759
ROC train: 0.986827	val: 0.742578	test: 0.715424
PRC train: 0.899823	val: 0.316183	test: 0.328646

Epoch: 82
Loss: 0.10175198276142755
ROC train: 0.988558	val: 0.747887	test: 0.710680
PRC train: 0.915815	val: 0.334302	test: 0.342064

Epoch: 83
Loss: 0.09765758154765064
ROC train: 0.988861	val: 0.741890	test: 0.714781
PRC train: 0.914382	val: 0.322047	test: 0.336072

Epoch: 84
Loss: 0.09513193203340191
ROC train: 0.989296	val: 0.736551	test: 0.696250
PRC train: 0.920674	val: 0.317830	test: 0.315605

Epoch: 85
Loss: 0.0968412931020144
ROC train: 0.989314	val: 0.744086	test: 0.707164
PRC train: 0.919911	val: 0.339168	test: 0.328295

Epoch: 86
Loss: 0.09596700796051852
ROC train: 0.989921	val: 0.746801	test: 0.704760
PRC train: 0.922256	val: 0.332818	test: 0.312460

Epoch: 87
Loss: 0.09722817456659145
ROC train: 0.990449	val: 0.742266	test: 0.701155
PRC train: 0.926301	val: 0.333507	test: 0.324458

Epoch: 88
Loss: 0.09414573451679496
ROC train: 0.989133	val: 0.737514	test: 0.699839
PRC train: 0.916653	val: 0.329212	test: 0.330388

Epoch: 89
Loss: 0.09366227561566177
ROC train: 0.990928	val: 0.739514	test: 0.701165
PRC train: 0.930219	val: 0.335211	test: 0.329463

Epoch: 90
Loss: 0.09330830791376854
ROC train: 0.991637	val: 0.743217	test: 0.700565
PRC train: 0.933138	val: 0.344527	test: 0.328034

Epoch: 91
Loss: 0.09208201692641174
ROC train: 0.992206	val: 0.745574	test: 0.709686
PRC train: 0.936317	val: 0.344944	test: 0.349672

Epoch: 92
Loss: 0.09031844090678529
ROC train: 0.991833	val: 0.735798	test: 0.693181
PRC train: 0.932596	val: 0.316793	test: 0.318906

Epoch: 93
Loss: 0.09161288708491164
ROC train: 0.992430	val: 0.749260	test: 0.709673
PRC train: 0.634257	val: 0.284477	test: 0.295211

Epoch: 33
Loss: 0.15467812708903506
ROC train: 0.923379	val: 0.746742	test: 0.729701
PRC train: 0.649273	val: 0.319090	test: 0.315794

Epoch: 34
Loss: 0.15435619620147317
ROC train: 0.923553	val: 0.731491	test: 0.711023
PRC train: 0.630624	val: 0.292661	test: 0.290712

Epoch: 35
Loss: 0.1517838480889452
ROC train: 0.931671	val: 0.750397	test: 0.728836
PRC train: 0.670213	val: 0.333682	test: 0.318106

Epoch: 36
Loss: 0.1513091749087274
ROC train: 0.929350	val: 0.745414	test: 0.732015
PRC train: 0.665874	val: 0.307364	test: 0.315727

Epoch: 37
Loss: 0.14980192886039392
ROC train: 0.931390	val: 0.751139	test: 0.722000
PRC train: 0.667220	val: 0.317132	test: 0.316781

Epoch: 38
Loss: 0.14860672463104194
ROC train: 0.935483	val: 0.756511	test: 0.722005
PRC train: 0.679183	val: 0.337844	test: 0.321070

Epoch: 39
Loss: 0.14768314842410116
ROC train: 0.937938	val: 0.748410	test: 0.717279
PRC train: 0.690665	val: 0.350873	test: 0.326418

Epoch: 40
Loss: 0.1469354210118184
ROC train: 0.941883	val: 0.757368	test: 0.719318
PRC train: 0.706921	val: 0.329576	test: 0.325772

Epoch: 41
Loss: 0.14383918252263128
ROC train: 0.941425	val: 0.743834	test: 0.726819
PRC train: 0.693620	val: 0.319932	test: 0.310841

Epoch: 42
Loss: 0.1443289000314933
ROC train: 0.943663	val: 0.745430	test: 0.709023
PRC train: 0.711755	val: 0.327137	test: 0.312844

Epoch: 43
Loss: 0.14042321173523864
ROC train: 0.946033	val: 0.749395	test: 0.721889
PRC train: 0.719007	val: 0.339674	test: 0.314829

Epoch: 44
Loss: 0.1415803546864865
ROC train: 0.943155	val: 0.742189	test: 0.705719
PRC train: 0.698612	val: 0.324671	test: 0.313246

Epoch: 45
Loss: 0.14028326689358736
ROC train: 0.949348	val: 0.754114	test: 0.728228
PRC train: 0.731368	val: 0.322175	test: 0.330577

Epoch: 46
Loss: 0.1379454185993242
ROC train: 0.950516	val: 0.743423	test: 0.724749
PRC train: 0.734167	val: 0.317275	test: 0.314655

Epoch: 47
Loss: 0.13803642784863857
ROC train: 0.954473	val: 0.737366	test: 0.726995
PRC train: 0.749768	val: 0.324116	test: 0.325365

Epoch: 48
Loss: 0.1374858517117554
ROC train: 0.954148	val: 0.747072	test: 0.731954
PRC train: 0.746031	val: 0.323756	test: 0.327218

Epoch: 49
Loss: 0.13554051565421069
ROC train: 0.955140	val: 0.750591	test: 0.723294
PRC train: 0.750615	val: 0.339582	test: 0.328573

Epoch: 50
Loss: 0.13390283109558893
ROC train: 0.956034	val: 0.746014	test: 0.725711
PRC train: 0.757514	val: 0.314536	test: 0.328923

Epoch: 51
Loss: 0.13376153924812015
ROC train: 0.957844	val: 0.741504	test: 0.720441
PRC train: 0.760551	val: 0.312427	test: 0.313864

Epoch: 52
Loss: 0.13187891631308665
ROC train: 0.962146	val: 0.737609	test: 0.723082
PRC train: 0.777831	val: 0.299642	test: 0.317848

Epoch: 53
Loss: 0.1297052450977109
ROC train: 0.960972	val: 0.748829	test: 0.730706
PRC train: 0.772790	val: 0.327486	test: 0.326406

Epoch: 54
Loss: 0.1291740139181454
ROC train: 0.961591	val: 0.748398	test: 0.726041
PRC train: 0.780739	val: 0.326472	test: 0.340804

Epoch: 55
Loss: 0.1288598073641921
ROC train: 0.963333	val: 0.746663	test: 0.728220
PRC train: 0.784295	val: 0.330185	test: 0.331826

Epoch: 56
Loss: 0.12693092090652386
ROC train: 0.966014	val: 0.741409	test: 0.708495
PRC train: 0.797451	val: 0.312143	test: 0.317009

Epoch: 57
Loss: 0.1251967941972829
ROC train: 0.968456	val: 0.747434	test: 0.721841
PRC train: 0.804987	val: 0.316291	test: 0.311247

Epoch: 58
Loss: 0.12723684306067684
ROC train: 0.967979	val: 0.751692	test: 0.724744
PRC train: 0.798553	val: 0.334100	test: 0.320739

Epoch: 59
Loss: 0.12262510123477285
ROC train: 0.969385	val: 0.750232	test: 0.719356
PRC train: 0.807461	val: 0.328199	test: 0.326875

Epoch: 60
Loss: 0.12052504680602055
ROC train: 0.969955	val: 0.744333	test: 0.718777
PRC train: 0.814308	val: 0.301770	test: 0.324817

Epoch: 61
Loss: 0.11987814310046137
ROC train: 0.973289	val: 0.736924	test: 0.708528
PRC train: 0.824860	val: 0.312307	test: 0.312665

Epoch: 62
Loss: 0.12033359625129712
ROC train: 0.973561	val: 0.743401	test: 0.712788
PRC train: 0.822291	val: 0.334934	test: 0.320420

Epoch: 63
Loss: 0.11856317801516202
ROC train: 0.975139	val: 0.743943	test: 0.715436
PRC train: 0.832468	val: 0.334196	test: 0.327123

Epoch: 64
Loss: 0.11955788482556202
ROC train: 0.975460	val: 0.738160	test: 0.706528
PRC train: 0.831917	val: 0.305921	test: 0.297818

Epoch: 65
Loss: 0.11926834409459441
ROC train: 0.975580	val: 0.739515	test: 0.720742
PRC train: 0.839673	val: 0.316290	test: 0.329158

Epoch: 66
Loss: 0.11825863619795936
ROC train: 0.976096	val: 0.737145	test: 0.714355
PRC train: 0.842685	val: 0.313088	test: 0.307293

Epoch: 67
Loss: 0.11550446838017503
ROC train: 0.976654	val: 0.739431	test: 0.719493
PRC train: 0.841545	val: 0.324840	test: 0.322099

Epoch: 68
Loss: 0.11205949552671968
ROC train: 0.979628	val: 0.739484	test: 0.707441
PRC train: 0.858140	val: 0.313560	test: 0.313283

Epoch: 69
Loss: 0.1125774429169504
ROC train: 0.979930	val: 0.729878	test: 0.704776
PRC train: 0.862056	val: 0.299506	test: 0.303390

Epoch: 70
Loss: 0.11050637496454724
ROC train: 0.981372	val: 0.746813	test: 0.717549
PRC train: 0.868058	val: 0.318726	test: 0.314037

Epoch: 71
Loss: 0.10969407602824102
ROC train: 0.980884	val: 0.751615	test: 0.722719
PRC train: 0.867991	val: 0.313277	test: 0.324054

Epoch: 72
Loss: 0.10949601603098628
ROC train: 0.981632	val: 0.733968	test: 0.713462
PRC train: 0.869381	val: 0.291850	test: 0.303706

Epoch: 73
Loss: 0.10804893182200162
ROC train: 0.982580	val: 0.737086	test: 0.721634
PRC train: 0.869549	val: 0.290588	test: 0.309254

Epoch: 74
Loss: 0.10744041320531503
ROC train: 0.983608	val: 0.739252	test: 0.724259
PRC train: 0.880206	val: 0.311834	test: 0.317292

Epoch: 75
Loss: 0.10530923539652726
ROC train: 0.984777	val: 0.736797	test: 0.720686
PRC train: 0.886600	val: 0.300223	test: 0.303036

Epoch: 76
Loss: 0.10324626447365222
ROC train: 0.984639	val: 0.731921	test: 0.711829
PRC train: 0.883579	val: 0.294823	test: 0.287494

Epoch: 77
Loss: 0.10348684916683915
ROC train: 0.986286	val: 0.749426	test: 0.726883
PRC train: 0.896394	val: 0.316301	test: 0.315595

Epoch: 78
Loss: 0.10412800867832281
ROC train: 0.986838	val: 0.726410	test: 0.709066
PRC train: 0.894235	val: 0.290725	test: 0.291845

Epoch: 79
Loss: 0.0997168099813038
ROC train: 0.986938	val: 0.743344	test: 0.721076
PRC train: 0.900950	val: 0.307029	test: 0.317340

Epoch: 80
Loss: 0.09971906148262176
ROC train: 0.989062	val: 0.736771	test: 0.711251
PRC train: 0.911556	val: 0.309377	test: 0.303791

Epoch: 81
Loss: 0.10079002577462264
ROC train: 0.989027	val: 0.739337	test: 0.703317
PRC train: 0.912040	val: 0.309854	test: 0.311402

Epoch: 82
Loss: 0.09576205873277287
ROC train: 0.989129	val: 0.736849	test: 0.709473
PRC train: 0.910416	val: 0.309207	test: 0.311794

Epoch: 83
Loss: 0.09662311382085693
ROC train: 0.988586	val: 0.722013	test: 0.701750
PRC train: 0.910802	val: 0.293035	test: 0.282406

Epoch: 84
Loss: 0.09465985543148164
ROC train: 0.989491	val: 0.736271	test: 0.706938
PRC train: 0.912256	val: 0.312594	test: 0.307983

Epoch: 85
Loss: 0.09654601020916438
ROC train: 0.990293	val: 0.731436	test: 0.706244
PRC train: 0.922944	val: 0.312252	test: 0.297026

Epoch: 86
Loss: 0.09575446582458345
ROC train: 0.991082	val: 0.734930	test: 0.703695
PRC train: 0.926341	val: 0.311705	test: 0.313401

Epoch: 87
Loss: 0.09470259084867623
ROC train: 0.990493	val: 0.737896	test: 0.717585
PRC train: 0.925553	val: 0.303933	test: 0.324183

Epoch: 88
Loss: 0.0934670337331343
ROC train: 0.991160	val: 0.736140	test: 0.704002
PRC train: 0.929795	val: 0.321704	test: 0.324544

Epoch: 89
Loss: 0.09240473720940047
ROC train: 0.991725	val: 0.724830	test: 0.695758
PRC train: 0.929809	val: 0.291904	test: 0.291397

Epoch: 90
Loss: 0.09185525507430116
ROC train: 0.992259	val: 0.738981	test: 0.705587
PRC train: 0.933744	val: 0.299257	test: 0.305515

Epoch: 91
Loss: 0.0870549116948454
ROC train: 0.991607	val: 0.730771	test: 0.698276
PRC train: 0.927315	val: 0.293315	test: 0.292769

Epoch: 92
Loss: 0.08804233259686679
ROC train: 0.993702	val: 0.747059	test: 0.706855
PRC train: 0.944909	val: 0.328028	test: 0.321073

Epoch: 93
Loss: 0.08907669595076205
ROC train: 0.993164	val: 0.728321	test: 0.696300
PRC train: 0.641512	val: 0.350618	test: 0.344700

Epoch: 33
Loss: 0.15554780280731506
ROC train: 0.924310	val: 0.773966	test: 0.733175
PRC train: 0.642620	val: 0.346167	test: 0.338895

Epoch: 34
Loss: 0.15457423658136218
ROC train: 0.930477	val: 0.774415	test: 0.720834
PRC train: 0.659351	val: 0.349301	test: 0.335669

Epoch: 35
Loss: 0.15194548313567346
ROC train: 0.929280	val: 0.768829	test: 0.737650
PRC train: 0.666152	val: 0.354395	test: 0.349392

Epoch: 36
Loss: 0.14926764730169637
ROC train: 0.933597	val: 0.764059	test: 0.723053
PRC train: 0.664673	val: 0.343641	test: 0.336249

Epoch: 37
Loss: 0.149706911192622
ROC train: 0.937957	val: 0.772206	test: 0.733623
PRC train: 0.690238	val: 0.341451	test: 0.345542

Epoch: 38
Loss: 0.14916609977766968
ROC train: 0.939360	val: 0.772600	test: 0.728354
PRC train: 0.696274	val: 0.358155	test: 0.347914

Epoch: 39
Loss: 0.14640549672290218
ROC train: 0.937312	val: 0.769102	test: 0.725412
PRC train: 0.689796	val: 0.354645	test: 0.347748

Epoch: 40
Loss: 0.14566828051018105
ROC train: 0.942716	val: 0.757389	test: 0.733544
PRC train: 0.707712	val: 0.343905	test: 0.342987

Epoch: 41
Loss: 0.14360773168975885
ROC train: 0.940879	val: 0.750682	test: 0.721132
PRC train: 0.694908	val: 0.320949	test: 0.327221

Epoch: 42
Loss: 0.14072740030713304
ROC train: 0.946807	val: 0.755824	test: 0.735999
PRC train: 0.726792	val: 0.340423	test: 0.338960

Epoch: 43
Loss: 0.14400944740648663
ROC train: 0.948235	val: 0.767557	test: 0.723774
PRC train: 0.724447	val: 0.351515	test: 0.333542

Epoch: 44
Loss: 0.14088419901890828
ROC train: 0.946600	val: 0.774849	test: 0.733232
PRC train: 0.722965	val: 0.376704	test: 0.341570

Epoch: 45
Loss: 0.14059067157983446
ROC train: 0.948117	val: 0.755483	test: 0.717805
PRC train: 0.715372	val: 0.325289	test: 0.325404

Epoch: 46
Loss: 0.13708165054081292
ROC train: 0.950488	val: 0.760451	test: 0.719119
PRC train: 0.741231	val: 0.334415	test: 0.325992

Epoch: 47
Loss: 0.1369257706607242
ROC train: 0.951127	val: 0.772597	test: 0.725139
PRC train: 0.734139	val: 0.348746	test: 0.344673

Epoch: 48
Loss: 0.13538981983415005
ROC train: 0.955243	val: 0.768528	test: 0.732405
PRC train: 0.755941	val: 0.345404	test: 0.342202

Epoch: 49
Loss: 0.13291236345422655
ROC train: 0.958465	val: 0.777434	test: 0.736437
PRC train: 0.765512	val: 0.361364	test: 0.353082

Epoch: 50
Loss: 0.1316028105450739
ROC train: 0.959113	val: 0.770194	test: 0.732265
PRC train: 0.768395	val: 0.362969	test: 0.350186

Epoch: 51
Loss: 0.1334563931232572
ROC train: 0.961794	val: 0.776427	test: 0.734351
PRC train: 0.774296	val: 0.361309	test: 0.345926

Epoch: 52
Loss: 0.13119361056473366
ROC train: 0.963046	val: 0.768210	test: 0.736605
PRC train: 0.785831	val: 0.371295	test: 0.347005

Epoch: 53
Loss: 0.1311253966049634
ROC train: 0.963752	val: 0.762493	test: 0.728171
PRC train: 0.786580	val: 0.357800	test: 0.346222

Epoch: 54
Loss: 0.1300751920814786
ROC train: 0.963259	val: 0.762229	test: 0.729522
PRC train: 0.785086	val: 0.344739	test: 0.333938

Epoch: 55
Loss: 0.12798643122695988
ROC train: 0.965992	val: 0.768473	test: 0.732343
PRC train: 0.794610	val: 0.365218	test: 0.354658

Epoch: 56
Loss: 0.12777883766041268
ROC train: 0.966595	val: 0.771982	test: 0.724958
PRC train: 0.795703	val: 0.349026	test: 0.343567

Epoch: 57
Loss: 0.12407804054173278
ROC train: 0.969259	val: 0.761242	test: 0.722151
PRC train: 0.809525	val: 0.338304	test: 0.328546

Epoch: 58
Loss: 0.12182197712758275
ROC train: 0.971836	val: 0.770725	test: 0.732488
PRC train: 0.820494	val: 0.344840	test: 0.350704

Epoch: 59
Loss: 0.12212543357075348
ROC train: 0.968905	val: 0.768191	test: 0.716631
PRC train: 0.804991	val: 0.363143	test: 0.332335

Epoch: 60
Loss: 0.12329794558184823
ROC train: 0.971618	val: 0.768520	test: 0.723963
PRC train: 0.821630	val: 0.349761	test: 0.340253

Epoch: 61
Loss: 0.12146509806372079
ROC train: 0.973393	val: 0.775839	test: 0.732693
PRC train: 0.825368	val: 0.352309	test: 0.352801

Epoch: 62
Loss: 0.11918521127142093
ROC train: 0.973670	val: 0.764555	test: 0.720907
PRC train: 0.834683	val: 0.346386	test: 0.345394

Epoch: 63
Loss: 0.1165236741463022
ROC train: 0.974380	val: 0.748448	test: 0.720499
PRC train: 0.830595	val: 0.332100	test: 0.325819

Epoch: 64
Loss: 0.11820145228218981
ROC train: 0.975627	val: 0.751404	test: 0.723355
PRC train: 0.834576	val: 0.327918	test: 0.337243

Epoch: 65
Loss: 0.1169907402360157
ROC train: 0.975741	val: 0.770541	test: 0.728713
PRC train: 0.839563	val: 0.353707	test: 0.353144

Epoch: 66
Loss: 0.11706293956901838
ROC train: 0.977904	val: 0.754503	test: 0.716783
PRC train: 0.848224	val: 0.346642	test: 0.351718

Epoch: 67
Loss: 0.11471716078480984
ROC train: 0.979842	val: 0.767782	test: 0.729346
PRC train: 0.856736	val: 0.343613	test: 0.352031

Epoch: 68
Loss: 0.11238156177381325
ROC train: 0.979846	val: 0.754441	test: 0.719732
PRC train: 0.851683	val: 0.335376	test: 0.335571

Epoch: 69
Loss: 0.1123115354322295
ROC train: 0.980624	val: 0.762738	test: 0.719291
PRC train: 0.862624	val: 0.360084	test: 0.346634

Epoch: 70
Loss: 0.11053788176685005
ROC train: 0.980506	val: 0.762368	test: 0.724208
PRC train: 0.863630	val: 0.340698	test: 0.334676

Epoch: 71
Loss: 0.11417500914057988
ROC train: 0.981867	val: 0.751643	test: 0.714272
PRC train: 0.866489	val: 0.336275	test: 0.340045

Epoch: 72
Loss: 0.10846328980260839
ROC train: 0.982830	val: 0.767744	test: 0.725847
PRC train: 0.876733	val: 0.362101	test: 0.356370

Epoch: 73
Loss: 0.10678568329381197
ROC train: 0.983595	val: 0.762061	test: 0.715961
PRC train: 0.880347	val: 0.352508	test: 0.345239

Epoch: 74
Loss: 0.10574391698941676
ROC train: 0.984395	val: 0.761450	test: 0.719462
PRC train: 0.884583	val: 0.361062	test: 0.345589

Epoch: 75
Loss: 0.10488495218785593
ROC train: 0.984626	val: 0.765895	test: 0.717582
PRC train: 0.886410	val: 0.368442	test: 0.359858

Epoch: 76
Loss: 0.10897214427771676
ROC train: 0.984044	val: 0.756434	test: 0.718827
PRC train: 0.885728	val: 0.361287	test: 0.334081

Epoch: 77
Loss: 0.10380992416343501
ROC train: 0.985722	val: 0.763821	test: 0.717895
PRC train: 0.894451	val: 0.354182	test: 0.346738

Epoch: 78
Loss: 0.10171636087983733
ROC train: 0.986120	val: 0.758525	test: 0.719900
PRC train: 0.899957	val: 0.360447	test: 0.352563

Epoch: 79
Loss: 0.10232096158239226
ROC train: 0.981998	val: 0.714253	test: 0.696500
PRC train: 0.872239	val: 0.306196	test: 0.308153

Epoch: 80
Loss: 0.10135540766732255
ROC train: 0.988615	val: 0.760754	test: 0.716531
PRC train: 0.909572	val: 0.361074	test: 0.340658

Epoch: 81
Loss: 0.10241748950585765
ROC train: 0.987018	val: 0.756695	test: 0.714908
PRC train: 0.902638	val: 0.351132	test: 0.335561

Epoch: 82
Loss: 0.10098543857879685
ROC train: 0.988779	val: 0.751259	test: 0.718960
PRC train: 0.912075	val: 0.340986	test: 0.334571

Epoch: 83
Loss: 0.09977634864577238
ROC train: 0.989647	val: 0.754396	test: 0.714160
PRC train: 0.918044	val: 0.348663	test: 0.335578

Epoch: 84
Loss: 0.09597432261062983
ROC train: 0.989967	val: 0.759009	test: 0.725210
PRC train: 0.922353	val: 0.360230	test: 0.331426

Epoch: 85
Loss: 0.0958178050423687
ROC train: 0.990478	val: 0.760921	test: 0.723582
PRC train: 0.920079	val: 0.353593	test: 0.349346

Epoch: 86
Loss: 0.09411664996717878
ROC train: 0.990760	val: 0.757466	test: 0.715433
PRC train: 0.926246	val: 0.359831	test: 0.333030

Epoch: 87
Loss: 0.09318784012256977
ROC train: 0.991400	val: 0.753393	test: 0.714213
PRC train: 0.929896	val: 0.348639	test: 0.337411

Epoch: 88
Loss: 0.09163042515570724
ROC train: 0.992355	val: 0.762461	test: 0.718467
PRC train: 0.936181	val: 0.359645	test: 0.345895

Epoch: 89
Loss: 0.09037723177111925
ROC train: 0.992402	val: 0.754094	test: 0.720317
PRC train: 0.937400	val: 0.350267	test: 0.337286

Epoch: 90
Loss: 0.09038335608335528
ROC train: 0.992670	val: 0.757788	test: 0.714925
PRC train: 0.940832	val: 0.356847	test: 0.335593

Epoch: 91
Loss: 0.09044147385050864
ROC train: 0.993024	val: 0.756507	test: 0.711421
PRC train: 0.941722	val: 0.345127	test: 0.336374

Epoch: 92
Loss: 0.08739167971049869
ROC train: 0.994039	val: 0.749745	test: 0.717531
PRC train: 0.950271	val: 0.351067	test: 0.341515

Epoch: 93
Loss: 0.08771014359761367
ROC train: 0.993176	val: 0.751193	test: 0.706594
PRC train: 0.615409	val: 0.290024	test: 0.267117

Epoch: 33
Loss: 0.16075360601753402
ROC train: 0.918085	val: 0.731376	test: 0.689271
PRC train: 0.612204	val: 0.254254	test: 0.248197

Epoch: 34
Loss: 0.15781638242852597
ROC train: 0.922314	val: 0.727981	test: 0.703543
PRC train: 0.629710	val: 0.244091	test: 0.243555

Epoch: 35
Loss: 0.1552249044217184
ROC train: 0.929108	val: 0.739154	test: 0.697504
PRC train: 0.641816	val: 0.254237	test: 0.248681

Epoch: 36
Loss: 0.15679137352563757
ROC train: 0.930398	val: 0.726587	test: 0.690303
PRC train: 0.650468	val: 0.259441	test: 0.245232

Epoch: 37
Loss: 0.15617228926226015
ROC train: 0.932147	val: 0.739960	test: 0.700668
PRC train: 0.664732	val: 0.265154	test: 0.264869

Epoch: 38
Loss: 0.153835514442519
ROC train: 0.934147	val: 0.748327	test: 0.715293
PRC train: 0.673469	val: 0.282851	test: 0.273969

Epoch: 39
Loss: 0.1529995342044404
ROC train: 0.936561	val: 0.731804	test: 0.704114
PRC train: 0.668959	val: 0.265844	test: 0.266989

Epoch: 40
Loss: 0.1502082322283419
ROC train: 0.939194	val: 0.739798	test: 0.699813
PRC train: 0.682097	val: 0.256918	test: 0.242912

Epoch: 41
Loss: 0.14856789225621175
ROC train: 0.938337	val: 0.746433	test: 0.709589
PRC train: 0.680448	val: 0.295537	test: 0.293936

Epoch: 42
Loss: 0.14811825729524952
ROC train: 0.942153	val: 0.739696	test: 0.698376
PRC train: 0.689746	val: 0.272471	test: 0.257654

Epoch: 43
Loss: 0.14564291025837026
ROC train: 0.946282	val: 0.726906	test: 0.686680
PRC train: 0.696811	val: 0.252006	test: 0.245067

Epoch: 44
Loss: 0.1451771399216299
ROC train: 0.942784	val: 0.742390	test: 0.707406
PRC train: 0.701363	val: 0.290524	test: 0.269625

Epoch: 45
Loss: 0.14422483969252678
ROC train: 0.949907	val: 0.734532	test: 0.703282
PRC train: 0.720779	val: 0.255151	test: 0.248877

Epoch: 46
Loss: 0.14281092311131552
ROC train: 0.952752	val: 0.735144	test: 0.699774
PRC train: 0.735170	val: 0.264029	test: 0.261926

Epoch: 47
Loss: 0.14122835735456976
ROC train: 0.949416	val: 0.731929	test: 0.690758
PRC train: 0.717940	val: 0.263494	test: 0.245221

Epoch: 48
Loss: 0.13874581956355933
ROC train: 0.951796	val: 0.729782	test: 0.693786
PRC train: 0.735505	val: 0.262593	test: 0.256139

Epoch: 49
Loss: 0.13738543592203187
ROC train: 0.953714	val: 0.736150	test: 0.700302
PRC train: 0.737327	val: 0.270733	test: 0.266570

Epoch: 50
Loss: 0.13603504595421292
ROC train: 0.957266	val: 0.742220	test: 0.704625
PRC train: 0.751499	val: 0.278762	test: 0.283256

Epoch: 51
Loss: 0.13422250921069734
ROC train: 0.960377	val: 0.711923	test: 0.688379
PRC train: 0.768800	val: 0.221996	test: 0.218609

Epoch: 52
Loss: 0.13396439619743059
ROC train: 0.960129	val: 0.737392	test: 0.697448
PRC train: 0.761429	val: 0.253356	test: 0.246952

Epoch: 53
Loss: 0.13152944857011875
ROC train: 0.961913	val: 0.699181	test: 0.674096
PRC train: 0.762279	val: 0.194538	test: 0.199758

Epoch: 54
Loss: 0.13193575045106504
ROC train: 0.964357	val: 0.734395	test: 0.702397
PRC train: 0.781869	val: 0.267926	test: 0.278236

Epoch: 55
Loss: 0.1328655231860203
ROC train: 0.961813	val: 0.733930	test: 0.697930
PRC train: 0.764854	val: 0.263101	test: 0.250598

Epoch: 56
Loss: 0.12935072957375104
ROC train: 0.966345	val: 0.726023	test: 0.690166
PRC train: 0.792355	val: 0.268135	test: 0.248073

Epoch: 57
Loss: 0.12669716022843072
ROC train: 0.968915	val: 0.729373	test: 0.683894
PRC train: 0.795609	val: 0.267063	test: 0.250377

Epoch: 58
Loss: 0.12765336299815266
ROC train: 0.969154	val: 0.727249	test: 0.692173
PRC train: 0.796921	val: 0.262086	test: 0.258579

Epoch: 59
Loss: 0.12767244516575843
ROC train: 0.969865	val: 0.733304	test: 0.691840
PRC train: 0.803349	val: 0.281955	test: 0.265792

Epoch: 60
Loss: 0.12644893008514174
ROC train: 0.970725	val: 0.728713	test: 0.689023
PRC train: 0.808133	val: 0.250303	test: 0.241733

Epoch: 61
Loss: 0.12272097623494452
ROC train: 0.973449	val: 0.737856	test: 0.696531
PRC train: 0.817939	val: 0.267805	test: 0.251012

Epoch: 62
Loss: 0.12184367972103737
ROC train: 0.973624	val: 0.730898	test: 0.685060
PRC train: 0.814325	val: 0.276593	test: 0.256572

Epoch: 63
Loss: 0.11972665898289415
ROC train: 0.974616	val: 0.729110	test: 0.697540
PRC train: 0.827758	val: 0.250859	test: 0.251826

Epoch: 64
Loss: 0.11846071446136158
ROC train: 0.976497	val: 0.725701	test: 0.695688
PRC train: 0.835717	val: 0.232843	test: 0.222724

Epoch: 65
Loss: 0.11785677940924581
ROC train: 0.977737	val: 0.730391	test: 0.693312
PRC train: 0.839456	val: 0.245630	test: 0.230080

Epoch: 66
Loss: 0.11672072956153778
ROC train: 0.979619	val: 0.735697	test: 0.701366
PRC train: 0.851847	val: 0.255037	test: 0.249343

Epoch: 67
Loss: 0.11439397836483145
ROC train: 0.980745	val: 0.708253	test: 0.679309
PRC train: 0.856830	val: 0.212744	test: 0.208007

Epoch: 68
Loss: 0.11424137628091277
ROC train: 0.980873	val: 0.717752	test: 0.692171
PRC train: 0.852355	val: 0.235726	test: 0.232679

Epoch: 69
Loss: 0.1120949551178763
ROC train: 0.979866	val: 0.733208	test: 0.701307
PRC train: 0.847828	val: 0.262890	test: 0.251450

Epoch: 70
Loss: 0.11186909782250762
ROC train: 0.982189	val: 0.731548	test: 0.701156
PRC train: 0.865103	val: 0.270153	test: 0.257718

Epoch: 71
Loss: 0.11029353291305738
ROC train: 0.984013	val: 0.687487	test: 0.672836
PRC train: 0.872834	val: 0.222031	test: 0.211167

Epoch: 72
Loss: 0.10781538469330015
ROC train: 0.984110	val: 0.734227	test: 0.700189
PRC train: 0.874829	val: 0.277117	test: 0.260372

Epoch: 73
Loss: 0.10770032511823427
ROC train: 0.984616	val: 0.728681	test: 0.695366
PRC train: 0.874628	val: 0.280338	test: 0.264410

Epoch: 74
Loss: 0.1074488989891043
ROC train: 0.985286	val: 0.686627	test: 0.677082
PRC train: 0.875348	val: 0.209096	test: 0.209941

Epoch: 75
Loss: 0.10718366258649131
ROC train: 0.985966	val: 0.711010	test: 0.686868
PRC train: 0.877654	val: 0.250377	test: 0.235680

Epoch: 76
Loss: 0.10741193700027786
ROC train: 0.984131	val: 0.726378	test: 0.698544
PRC train: 0.875180	val: 0.264054	test: 0.260368

Epoch: 77
Loss: 0.10491021076411744
ROC train: 0.986815	val: 0.703752	test: 0.681019
PRC train: 0.885239	val: 0.243017	test: 0.229352

Epoch: 78
Loss: 0.10361724892744856
ROC train: 0.988146	val: 0.732231	test: 0.698784
PRC train: 0.899994	val: 0.258538	test: 0.242588

Epoch: 79
Loss: 0.10315809863206071
ROC train: 0.987727	val: 0.719045	test: 0.680519
PRC train: 0.893615	val: 0.255350	test: 0.232100

Epoch: 80
Loss: 0.10142801550974535
ROC train: 0.989408	val: 0.725397	test: 0.700672
PRC train: 0.907686	val: 0.246582	test: 0.248656

Epoch: 81
Loss: 0.1000175611110225
ROC train: 0.989561	val: 0.727753	test: 0.694903
PRC train: 0.902019	val: 0.257557	test: 0.238026

Epoch: 82
Loss: 0.09848382325160188
ROC train: 0.988973	val: 0.707867	test: 0.688779
PRC train: 0.902828	val: 0.256917	test: 0.243388

Epoch: 83
Loss: 0.09452050289231155
ROC train: 0.991805	val: 0.708062	test: 0.683815
PRC train: 0.923203	val: 0.221856	test: 0.215380

Epoch: 84
Loss: 0.09647829106871525
ROC train: 0.991944	val: 0.718039	test: 0.690283
PRC train: 0.925444	val: 0.258651	test: 0.244097

Epoch: 85
Loss: 0.09553662175605844
ROC train: 0.991838	val: 0.704567	test: 0.684112
PRC train: 0.923621	val: 0.228145	test: 0.217701

Epoch: 86
Loss: 0.09313136787640933
ROC train: 0.992879	val: 0.723700	test: 0.697599
PRC train: 0.933506	val: 0.266542	test: 0.245715

Epoch: 87
Loss: 0.09335242354967428
ROC train: 0.992148	val: 0.720124	test: 0.687783
PRC train: 0.926212	val: 0.247560	test: 0.226328

Epoch: 88
Loss: 0.09244454861764248
ROC train: 0.992522	val: 0.728330	test: 0.691967
PRC train: 0.924356	val: 0.263552	test: 0.238104

Epoch: 89
Loss: 0.09337949140955835
ROC train: 0.993695	val: 0.710710	test: 0.678945
PRC train: 0.937022	val: 0.241277	test: 0.221295

Epoch: 90
Loss: 0.08972206197719564
ROC train: 0.993745	val: 0.726429	test: 0.681181
PRC train: 0.936780	val: 0.269826	test: 0.246714

Epoch: 91
Loss: 0.08963126310486262
ROC train: 0.993930	val: 0.725441	test: 0.698233
PRC train: 0.940869	val: 0.258550	test: 0.243652

Epoch: 92
Loss: 0.09042268444142558
ROC train: 0.993849	val: 0.726356	test: 0.693745
PRC train: 0.936976	val: 0.276637	test: 0.251875

Epoch: 93
Loss: 0.08739974864258439
ROC train: 0.994562	val: 0.719218	test: 0.699859
PRC train: 0.609233	val: 0.288992	test: 0.304202

Epoch: 33
Loss: 0.16022650283391993
ROC train: 0.920737	val: 0.726938	test: 0.704805
PRC train: 0.618713	val: 0.295093	test: 0.300699

Epoch: 34
Loss: 0.15839866165124622
ROC train: 0.924813	val: 0.734066	test: 0.711533
PRC train: 0.631110	val: 0.287464	test: 0.304302

Epoch: 35
Loss: 0.1563653629765179
ROC train: 0.923815	val: 0.717637	test: 0.688304
PRC train: 0.634810	val: 0.280564	test: 0.289529

Epoch: 36
Loss: 0.15482427440458543
ROC train: 0.923427	val: 0.718495	test: 0.690958
PRC train: 0.625871	val: 0.269105	test: 0.292070

Epoch: 37
Loss: 0.1575659363738
ROC train: 0.930745	val: 0.720680	test: 0.709785
PRC train: 0.660334	val: 0.291809	test: 0.310417

Epoch: 38
Loss: 0.15343959445972025
ROC train: 0.930390	val: 0.744396	test: 0.720211
PRC train: 0.659344	val: 0.318239	test: 0.316179

Epoch: 39
Loss: 0.15391965108509736
ROC train: 0.934288	val: 0.735333	test: 0.713618
PRC train: 0.672060	val: 0.290970	test: 0.311006

Epoch: 40
Loss: 0.15063054228615777
ROC train: 0.934239	val: 0.733262	test: 0.709252
PRC train: 0.665156	val: 0.300772	test: 0.308824

Epoch: 41
Loss: 0.14972738871059132
ROC train: 0.936085	val: 0.736892	test: 0.705651
PRC train: 0.680551	val: 0.303059	test: 0.307727

Epoch: 42
Loss: 0.14768610656027636
ROC train: 0.939761	val: 0.722822	test: 0.698528
PRC train: 0.682051	val: 0.290834	test: 0.303087

Epoch: 43
Loss: 0.14693899816310022
ROC train: 0.941189	val: 0.719385	test: 0.703698
PRC train: 0.691190	val: 0.273683	test: 0.307063

Epoch: 44
Loss: 0.1449912033508677
ROC train: 0.944382	val: 0.736315	test: 0.712111
PRC train: 0.696138	val: 0.305763	test: 0.322248

Epoch: 45
Loss: 0.14476839280903073
ROC train: 0.944487	val: 0.723838	test: 0.713797
PRC train: 0.699694	val: 0.301485	test: 0.328495

Epoch: 46
Loss: 0.14382283103157867
ROC train: 0.947169	val: 0.722463	test: 0.708299
PRC train: 0.718358	val: 0.287349	test: 0.298904

Epoch: 47
Loss: 0.14190561769837973
ROC train: 0.950742	val: 0.730181	test: 0.710507
PRC train: 0.730592	val: 0.289724	test: 0.314276

Epoch: 48
Loss: 0.13890207491780016
ROC train: 0.951190	val: 0.726858	test: 0.719492
PRC train: 0.727549	val: 0.305497	test: 0.321673

Epoch: 49
Loss: 0.1379931798658781
ROC train: 0.955149	val: 0.717673	test: 0.695801
PRC train: 0.746090	val: 0.295559	test: 0.305494

Epoch: 50
Loss: 0.1358899443856646
ROC train: 0.956960	val: 0.731802	test: 0.695755
PRC train: 0.751374	val: 0.286213	test: 0.301800

Epoch: 51
Loss: 0.1370410148913966
ROC train: 0.959114	val: 0.726052	test: 0.696379
PRC train: 0.758746	val: 0.313677	test: 0.302261

Epoch: 52
Loss: 0.13589294671906882
ROC train: 0.961055	val: 0.715277	test: 0.688853
PRC train: 0.764216	val: 0.283244	test: 0.294244

Epoch: 53
Loss: 0.1335218575406287
ROC train: 0.961842	val: 0.725597	test: 0.698249
PRC train: 0.771711	val: 0.295908	test: 0.294312

Epoch: 54
Loss: 0.1300935342690258
ROC train: 0.964585	val: 0.695845	test: 0.680752
PRC train: 0.782049	val: 0.265718	test: 0.287042

Epoch: 55
Loss: 0.13228542460290765
ROC train: 0.962547	val: 0.721667	test: 0.696996
PRC train: 0.778614	val: 0.284924	test: 0.292704

Epoch: 56
Loss: 0.12772860299952185
ROC train: 0.964251	val: 0.722570	test: 0.693748
PRC train: 0.776221	val: 0.280520	test: 0.296536

Epoch: 57
Loss: 0.12846389463675126
ROC train: 0.964543	val: 0.696255	test: 0.663367
PRC train: 0.781623	val: 0.269179	test: 0.282793

Epoch: 58
Loss: 0.127094402565066
ROC train: 0.968233	val: 0.700071	test: 0.670161
PRC train: 0.798125	val: 0.254611	test: 0.278065

Epoch: 59
Loss: 0.12703282655782605
ROC train: 0.968002	val: 0.718672	test: 0.699914
PRC train: 0.793883	val: 0.279856	test: 0.305291

Epoch: 60
Loss: 0.12609897064529935
ROC train: 0.971047	val: 0.725193	test: 0.693999
PRC train: 0.812571	val: 0.290035	test: 0.303987

Epoch: 61
Loss: 0.12401092666802661
ROC train: 0.972085	val: 0.712417	test: 0.680260
PRC train: 0.815340	val: 0.276161	test: 0.286783

Epoch: 62
Loss: 0.12315944617833574
ROC train: 0.973466	val: 0.713998	test: 0.676843
PRC train: 0.821450	val: 0.266726	test: 0.280127

Epoch: 63
Loss: 0.11958458044969475
ROC train: 0.970609	val: 0.732561	test: 0.700254
PRC train: 0.809201	val: 0.291797	test: 0.302617

Epoch: 64
Loss: 0.12092703604606463
ROC train: 0.976116	val: 0.708005	test: 0.676194
PRC train: 0.833156	val: 0.275925	test: 0.287188

Epoch: 65
Loss: 0.11685955175504467
ROC train: 0.975889	val: 0.718414	test: 0.685173
PRC train: 0.833457	val: 0.271820	test: 0.291241

Epoch: 66
Loss: 0.11843212829002409
ROC train: 0.975836	val: 0.700438	test: 0.679815
PRC train: 0.830142	val: 0.269084	test: 0.272327

Epoch: 67
Loss: 0.11742360245518457
ROC train: 0.978504	val: 0.726131	test: 0.688683
PRC train: 0.844370	val: 0.277940	test: 0.275630

Epoch: 68
Loss: 0.11711512660706953
ROC train: 0.979434	val: 0.704816	test: 0.683152
PRC train: 0.849774	val: 0.272317	test: 0.282703

Epoch: 69
Loss: 0.11559297786677243
ROC train: 0.980684	val: 0.718126	test: 0.688209
PRC train: 0.860267	val: 0.282843	test: 0.289104

Epoch: 70
Loss: 0.11304926188843678
ROC train: 0.980143	val: 0.718060	test: 0.695691
PRC train: 0.856023	val: 0.287456	test: 0.292965

Epoch: 71
Loss: 0.11187960357315305
ROC train: 0.983641	val: 0.712566	test: 0.679267
PRC train: 0.876444	val: 0.278379	test: 0.277724

Epoch: 72
Loss: 0.11192580705543316
ROC train: 0.981765	val: 0.709265	test: 0.691684
PRC train: 0.863799	val: 0.270326	test: 0.289550

Epoch: 73
Loss: 0.1103609633313844
ROC train: 0.983839	val: 0.720496	test: 0.693191
PRC train: 0.879889	val: 0.293631	test: 0.285460

Epoch: 74
Loss: 0.10721828938803356
ROC train: 0.984101	val: 0.716982	test: 0.699745
PRC train: 0.878637	val: 0.278193	test: 0.300093

Epoch: 75
Loss: 0.10708489673449409
ROC train: 0.985190	val: 0.720236	test: 0.688620
PRC train: 0.882965	val: 0.289554	test: 0.292302

Epoch: 76
Loss: 0.10707484144877526
ROC train: 0.985742	val: 0.701743	test: 0.690295
PRC train: 0.887447	val: 0.273820	test: 0.282131

Epoch: 77
Loss: 0.10238192116415772
ROC train: 0.986758	val: 0.702393	test: 0.672667
PRC train: 0.893026	val: 0.277217	test: 0.282871

Epoch: 78
Loss: 0.10273413808475303
ROC train: 0.987158	val: 0.712254	test: 0.686069
PRC train: 0.901038	val: 0.277859	test: 0.291486

Epoch: 79
Loss: 0.10057191275321975
ROC train: 0.986620	val: 0.707195	test: 0.692196
PRC train: 0.896315	val: 0.264636	test: 0.267265

Epoch: 80
Loss: 0.10047275445364892
ROC train: 0.988112	val: 0.714113	test: 0.694819
PRC train: 0.906511	val: 0.281910	test: 0.290460

Epoch: 81
Loss: 0.09999105922202761
ROC train: 0.988664	val: 0.707927	test: 0.688990
PRC train: 0.908025	val: 0.294877	test: 0.289261

Epoch: 82
Loss: 0.09832345595117617
ROC train: 0.989296	val: 0.706915	test: 0.682430
PRC train: 0.915941	val: 0.276358	test: 0.263766

Epoch: 83
Loss: 0.09918844580615545
ROC train: 0.990156	val: 0.692194	test: 0.681396
PRC train: 0.915529	val: 0.263563	test: 0.277127

Epoch: 84
Loss: 0.09567809148433737
ROC train: 0.990105	val: 0.696143	test: 0.682128
PRC train: 0.921019	val: 0.275362	test: 0.287432

Epoch: 85
Loss: 0.09567048905077924
ROC train: 0.989526	val: 0.683927	test: 0.671079
PRC train: 0.916925	val: 0.254914	test: 0.262728

Epoch: 86
Loss: 0.09306317853433727
ROC train: 0.990764	val: 0.698218	test: 0.693944
PRC train: 0.923938	val: 0.269745	test: 0.272711

Epoch: 87
Loss: 0.09154134292853187
ROC train: 0.990719	val: 0.690378	test: 0.686677
PRC train: 0.922595	val: 0.271704	test: 0.277688

Epoch: 88
Loss: 0.09240840772430704
ROC train: 0.992436	val: 0.711388	test: 0.689760
PRC train: 0.934316	val: 0.296242	test: 0.291405

Epoch: 89
Loss: 0.09181293533096907
ROC train: 0.993561	val: 0.697578	test: 0.691783
PRC train: 0.941638	val: 0.275070	test: 0.278121

Epoch: 90
Loss: 0.09014869691846433
ROC train: 0.993300	val: 0.703210	test: 0.683772
PRC train: 0.939674	val: 0.274626	test: 0.286650

Epoch: 91
Loss: 0.0892743741554331
ROC train: 0.993212	val: 0.703418	test: 0.692951
PRC train: 0.939846	val: 0.279803	test: 0.294835

Epoch: 92
Loss: 0.08702468155489866
ROC train: 0.994319	val: 0.701428	test: 0.681584
PRC train: 0.946702	val: 0.279999	test: 0.285532

Epoch: 93
Loss: 0.08640703810962833
ROC train: 0.994543	val: 0.705293	test: 0.691214
PRC train: 0.603639	val: 0.280721	test: 0.264670

Epoch: 33
Loss: 0.1615158170594073
ROC train: 0.913489	val: 0.676261	test: 0.660154
PRC train: 0.593249	val: 0.240661	test: 0.223507

Epoch: 34
Loss: 0.15980117636867486
ROC train: 0.924549	val: 0.688430	test: 0.659918
PRC train: 0.637546	val: 0.300477	test: 0.266981

Epoch: 35
Loss: 0.15822205932334657
ROC train: 0.927269	val: 0.707758	test: 0.686102
PRC train: 0.641968	val: 0.330756	test: 0.300187

Epoch: 36
Loss: 0.15477599958335034
ROC train: 0.928264	val: 0.706071	test: 0.682798
PRC train: 0.646495	val: 0.308522	test: 0.280958

Epoch: 37
Loss: 0.15486873815531552
ROC train: 0.932452	val: 0.699455	test: 0.671271
PRC train: 0.663202	val: 0.303411	test: 0.262592

Epoch: 38
Loss: 0.15257874105176594
ROC train: 0.929890	val: 0.697287	test: 0.673068
PRC train: 0.644680	val: 0.294266	test: 0.259346

Epoch: 39
Loss: 0.15270228395833502
ROC train: 0.936864	val: 0.677261	test: 0.657667
PRC train: 0.681576	val: 0.256145	test: 0.235028

Epoch: 40
Loss: 0.1524221847806484
ROC train: 0.937630	val: 0.701445	test: 0.680699
PRC train: 0.680548	val: 0.318264	test: 0.266887

Epoch: 41
Loss: 0.14910504031538838
ROC train: 0.941089	val: 0.657400	test: 0.638428
PRC train: 0.694369	val: 0.261831	test: 0.234740

Epoch: 42
Loss: 0.14903389161521094
ROC train: 0.941183	val: 0.680420	test: 0.665610
PRC train: 0.680410	val: 0.275171	test: 0.265324

Epoch: 43
Loss: 0.14458851498949704
ROC train: 0.944698	val: 0.685079	test: 0.671500
PRC train: 0.704394	val: 0.285455	test: 0.248031

Epoch: 44
Loss: 0.14601855034966912
ROC train: 0.944613	val: 0.669233	test: 0.651792
PRC train: 0.704636	val: 0.264275	test: 0.247402

Epoch: 45
Loss: 0.1456442113033561
ROC train: 0.947743	val: 0.666827	test: 0.669860
PRC train: 0.706845	val: 0.240727	test: 0.230087

Epoch: 46
Loss: 0.14230842770242563
ROC train: 0.952088	val: 0.664850	test: 0.661237
PRC train: 0.730808	val: 0.252672	test: 0.237005

Epoch: 47
Loss: 0.1417501326085009
ROC train: 0.952861	val: 0.683144	test: 0.681899
PRC train: 0.731635	val: 0.263204	test: 0.247028

Epoch: 48
Loss: 0.14125462417545173
ROC train: 0.954446	val: 0.680929	test: 0.668517
PRC train: 0.741619	val: 0.284755	test: 0.248009

Epoch: 49
Loss: 0.13931181758283903
ROC train: 0.952981	val: 0.683886	test: 0.677976
PRC train: 0.735880	val: 0.287611	test: 0.256553

Epoch: 50
Loss: 0.13789422743488222
ROC train: 0.958288	val: 0.692714	test: 0.680840
PRC train: 0.755086	val: 0.317470	test: 0.269685

Epoch: 51
Loss: 0.13682546777036575
ROC train: 0.955803	val: 0.664644	test: 0.654263
PRC train: 0.744876	val: 0.243471	test: 0.228419

Epoch: 52
Loss: 0.13573081704028434
ROC train: 0.958609	val: 0.695999	test: 0.687554
PRC train: 0.758776	val: 0.295498	test: 0.258733

Epoch: 53
Loss: 0.13428607429635592
ROC train: 0.959070	val: 0.688383	test: 0.671425
PRC train: 0.756688	val: 0.279189	test: 0.246891

Epoch: 54
Loss: 0.1324712826986935
ROC train: 0.965560	val: 0.683588	test: 0.673546
PRC train: 0.780669	val: 0.279603	test: 0.267431

Epoch: 55
Loss: 0.13092825500584243
ROC train: 0.963755	val: 0.683142	test: 0.671186
PRC train: 0.782174	val: 0.284189	test: 0.252437

Epoch: 56
Loss: 0.12951848287102194
ROC train: 0.968100	val: 0.660407	test: 0.655570
PRC train: 0.801428	val: 0.244526	test: 0.228801

Epoch: 57
Loss: 0.12757902841566648
ROC train: 0.965832	val: 0.658465	test: 0.653619
PRC train: 0.788320	val: 0.236305	test: 0.205068

Epoch: 58
Loss: 0.12792098944096614
ROC train: 0.970187	val: 0.679319	test: 0.664659
PRC train: 0.812814	val: 0.287543	test: 0.245905

Epoch: 59
Loss: 0.12672959572491346
ROC train: 0.970804	val: 0.672022	test: 0.671362
PRC train: 0.811827	val: 0.286530	test: 0.258595

Epoch: 60
Loss: 0.12264243978489348
ROC train: 0.971630	val: 0.671476	test: 0.664589
PRC train: 0.820329	val: 0.271342	test: 0.232793

Epoch: 61
Loss: 0.12263493606295978
ROC train: 0.973240	val: 0.668326	test: 0.666696
PRC train: 0.821801	val: 0.265104	test: 0.233261

Epoch: 62
Loss: 0.12508265441943414
ROC train: 0.971795	val: 0.680734	test: 0.669113
PRC train: 0.815956	val: 0.265101	test: 0.247555

Epoch: 63
Loss: 0.12155993050548684
ROC train: 0.974457	val: 0.659181	test: 0.656785
PRC train: 0.827360	val: 0.246544	test: 0.231524

Epoch: 64
Loss: 0.11993197383331511
ROC train: 0.976067	val: 0.663734	test: 0.669062
PRC train: 0.836870	val: 0.251246	test: 0.247674

Epoch: 65
Loss: 0.11934986397142583
ROC train: 0.977080	val: 0.680284	test: 0.688493
PRC train: 0.838845	val: 0.253686	test: 0.244924

Epoch: 66
Loss: 0.11948974465714166
ROC train: 0.977212	val: 0.656168	test: 0.661541
PRC train: 0.842319	val: 0.219685	test: 0.207445

Epoch: 67
Loss: 0.1170518467077211
ROC train: 0.976365	val: 0.674589	test: 0.678820
PRC train: 0.841837	val: 0.253975	test: 0.241516

Epoch: 68
Loss: 0.11608795014075764
ROC train: 0.980312	val: 0.651525	test: 0.653741
PRC train: 0.852163	val: 0.234401	test: 0.238830

Epoch: 69
Loss: 0.11369864781229339
ROC train: 0.981293	val: 0.663230	test: 0.671578
PRC train: 0.868277	val: 0.243963	test: 0.223962

Epoch: 70
Loss: 0.11168946867666606
ROC train: 0.980467	val: 0.657213	test: 0.653227
PRC train: 0.854843	val: 0.223645	test: 0.217522

Epoch: 71
Loss: 0.11189521793950126
ROC train: 0.982597	val: 0.675473	test: 0.669189
PRC train: 0.870686	val: 0.248099	test: 0.236582

Epoch: 72
Loss: 0.11225746725711996
ROC train: 0.981550	val: 0.638253	test: 0.643062
PRC train: 0.863675	val: 0.186957	test: 0.189578

Epoch: 73
Loss: 0.11113831727929671
ROC train: 0.982450	val: 0.687536	test: 0.673060
PRC train: 0.862084	val: 0.285676	test: 0.275172

Epoch: 74
Loss: 0.10837253094704291
ROC train: 0.985634	val: 0.666166	test: 0.677283
PRC train: 0.886151	val: 0.276935	test: 0.257223

Epoch: 75
Loss: 0.10906238883851443
ROC train: 0.984899	val: 0.662720	test: 0.659741
PRC train: 0.883864	val: 0.237342	test: 0.225721

Epoch: 76
Loss: 0.10538354994126414
ROC train: 0.985155	val: 0.632582	test: 0.635292
PRC train: 0.881492	val: 0.197449	test: 0.182243

Epoch: 77
Loss: 0.10405956811310782
ROC train: 0.986913	val: 0.657838	test: 0.653597
PRC train: 0.893385	val: 0.254286	test: 0.234439

Epoch: 78
Loss: 0.10551072805226712
ROC train: 0.987535	val: 0.648336	test: 0.654041
PRC train: 0.899330	val: 0.215028	test: 0.220738

Epoch: 79
Loss: 0.10136840430289118
ROC train: 0.987468	val: 0.633156	test: 0.644521
PRC train: 0.899120	val: 0.196563	test: 0.198539

Epoch: 80
Loss: 0.10217980932888107
ROC train: 0.987708	val: 0.657874	test: 0.660795
PRC train: 0.901254	val: 0.219623	test: 0.219599

Epoch: 81
Loss: 0.10154191886126122
ROC train: 0.989536	val: 0.640711	test: 0.632966
PRC train: 0.913060	val: 0.218165	test: 0.208453

Epoch: 82
Loss: 0.09986717084880661
ROC train: 0.989587	val: 0.676106	test: 0.675106
PRC train: 0.913850	val: 0.259298	test: 0.246331

Epoch: 83
Loss: 0.10130344163794851
ROC train: 0.989826	val: 0.658017	test: 0.666285
PRC train: 0.918952	val: 0.230563	test: 0.228434

Epoch: 84
Loss: 0.09786029318203662
ROC train: 0.990560	val: 0.661128	test: 0.671356
PRC train: 0.921036	val: 0.226833	test: 0.229907

Epoch: 85
Loss: 0.09763579439382125
ROC train: 0.991043	val: 0.632644	test: 0.643404
PRC train: 0.927259	val: 0.205704	test: 0.198069

Epoch: 86
Loss: 0.09553258818971724
ROC train: 0.991475	val: 0.678567	test: 0.680854
PRC train: 0.929914	val: 0.268809	test: 0.269812

Epoch: 87
Loss: 0.09380985610226693
ROC train: 0.992355	val: 0.675190	test: 0.679657
PRC train: 0.935390	val: 0.277210	test: 0.271655

Epoch: 88
Loss: 0.09188303969699074
ROC train: 0.992879	val: 0.672110	test: 0.671974
PRC train: 0.941608	val: 0.277247	test: 0.272662

Epoch: 89
Loss: 0.09031890201016916
ROC train: 0.992930	val: 0.648593	test: 0.654726
PRC train: 0.941666	val: 0.222010	test: 0.218252

Epoch: 90
Loss: 0.09117810858343602
ROC train: 0.993508	val: 0.676284	test: 0.683097
PRC train: 0.941278	val: 0.258069	test: 0.261882

Epoch: 91
Loss: 0.08916539544677801
ROC train: 0.993853	val: 0.665931	test: 0.667072
PRC train: 0.944925	val: 0.245342	test: 0.238571

Epoch: 92
Loss: 0.09050696302081074
ROC train: 0.994207	val: 0.673408	test: 0.665955
PRC train: 0.948054	val: 0.271469	test: 0.243727

Epoch: 93
Loss: 0.08994783374285018
ROC train: 0.994297	val: 0.656073	test: 0.652757
PRC train: 0.639672	val: 0.367406	test: 0.353175

Epoch: 33
Loss: 0.1537111509351244
ROC train: 0.927707	val: 0.782019	test: 0.747448
PRC train: 0.654791	val: 0.381147	test: 0.363464

Epoch: 34
Loss: 0.15192524101378316
ROC train: 0.924491	val: 0.774363	test: 0.738856
PRC train: 0.643494	val: 0.353739	test: 0.365441

Epoch: 35
Loss: 0.15165272661173465
ROC train: 0.930826	val: 0.784520	test: 0.744400
PRC train: 0.662659	val: 0.366356	test: 0.355150

Epoch: 36
Loss: 0.14726766885758436
ROC train: 0.929490	val: 0.771729	test: 0.745565
PRC train: 0.660781	val: 0.366820	test: 0.357553

Epoch: 37
Loss: 0.14684909906597432
ROC train: 0.936272	val: 0.772686	test: 0.743130
PRC train: 0.685956	val: 0.355854	test: 0.372946

Epoch: 38
Loss: 0.14438370266649925
ROC train: 0.936362	val: 0.787113	test: 0.753224
PRC train: 0.684316	val: 0.381105	test: 0.374519

Epoch: 39
Loss: 0.14717133597889392
ROC train: 0.938577	val: 0.785340	test: 0.736515
PRC train: 0.688314	val: 0.374634	test: 0.357908

Epoch: 40
Loss: 0.1456366934164606
ROC train: 0.937772	val: 0.780126	test: 0.741965
PRC train: 0.685877	val: 0.359996	test: 0.358194

Epoch: 41
Loss: 0.14300424265703474
ROC train: 0.941984	val: 0.780301	test: 0.738722
PRC train: 0.702453	val: 0.370247	test: 0.368484

Epoch: 42
Loss: 0.14384571898004594
ROC train: 0.941595	val: 0.785883	test: 0.733645
PRC train: 0.704551	val: 0.381048	test: 0.364857

Epoch: 43
Loss: 0.14112233876283592
ROC train: 0.946159	val: 0.772932	test: 0.742786
PRC train: 0.721603	val: 0.356021	test: 0.373460

Epoch: 44
Loss: 0.13818272536002324
ROC train: 0.947048	val: 0.779966	test: 0.738516
PRC train: 0.721946	val: 0.359606	test: 0.369504

Epoch: 45
Loss: 0.1382731791791075
ROC train: 0.948969	val: 0.775846	test: 0.730942
PRC train: 0.727355	val: 0.364974	test: 0.357524

Epoch: 46
Loss: 0.13801048472224683
ROC train: 0.948890	val: 0.775342	test: 0.738190
PRC train: 0.733643	val: 0.362017	test: 0.374323

Epoch: 47
Loss: 0.13738002371548652
ROC train: 0.952185	val: 0.780244	test: 0.740187
PRC train: 0.738372	val: 0.373272	test: 0.368129

Epoch: 48
Loss: 0.13270506833996634
ROC train: 0.954484	val: 0.776073	test: 0.732058
PRC train: 0.753539	val: 0.356387	test: 0.363589

Epoch: 49
Loss: 0.13347721736450208
ROC train: 0.952884	val: 0.772012	test: 0.735882
PRC train: 0.749133	val: 0.360818	test: 0.366075

Epoch: 50
Loss: 0.13192462080257264
ROC train: 0.955234	val: 0.771428	test: 0.729373
PRC train: 0.752447	val: 0.367798	test: 0.351728

Epoch: 51
Loss: 0.13205889156980746
ROC train: 0.956978	val: 0.772050	test: 0.724974
PRC train: 0.754959	val: 0.364053	test: 0.351783

Epoch: 52
Loss: 0.13141632181921234
ROC train: 0.956109	val: 0.784340	test: 0.736524
PRC train: 0.761063	val: 0.384061	test: 0.371937

Epoch: 53
Loss: 0.12940451544359077
ROC train: 0.959545	val: 0.767048	test: 0.728925
PRC train: 0.767339	val: 0.362040	test: 0.340779

Epoch: 54
Loss: 0.12926714096599545
ROC train: 0.962947	val: 0.779366	test: 0.727551
PRC train: 0.788218	val: 0.364743	test: 0.371308

Epoch: 55
Loss: 0.12970784523810322
ROC train: 0.958518	val: 0.776778	test: 0.736840
PRC train: 0.770408	val: 0.368806	test: 0.368425

Epoch: 56
Loss: 0.12847184720271534
ROC train: 0.964296	val: 0.776601	test: 0.735751
PRC train: 0.792631	val: 0.381101	test: 0.372003

Epoch: 57
Loss: 0.12450441537407211
ROC train: 0.965722	val: 0.784705	test: 0.732965
PRC train: 0.800306	val: 0.379147	test: 0.366375

Epoch: 58
Loss: 0.1235319467588079
ROC train: 0.964951	val: 0.781070	test: 0.727674
PRC train: 0.785776	val: 0.375477	test: 0.352888

Epoch: 59
Loss: 0.1242104852660088
ROC train: 0.966539	val: 0.780735	test: 0.729113
PRC train: 0.808000	val: 0.383003	test: 0.351465

Epoch: 60
Loss: 0.12266582874175404
ROC train: 0.967703	val: 0.774586	test: 0.727611
PRC train: 0.808782	val: 0.352335	test: 0.351867

Epoch: 61
Loss: 0.12225367751920709
ROC train: 0.969099	val: 0.772939	test: 0.729661
PRC train: 0.813025	val: 0.364772	test: 0.355678

Epoch: 62
Loss: 0.12047156822616507
ROC train: 0.968663	val: 0.770485	test: 0.732601
PRC train: 0.806536	val: 0.361342	test: 0.353192

Epoch: 63
Loss: 0.11856889809651046
ROC train: 0.971291	val: 0.775814	test: 0.725815
PRC train: 0.827705	val: 0.376735	test: 0.363618

Epoch: 64
Loss: 0.11891146510519685
ROC train: 0.972766	val: 0.775114	test: 0.721300
PRC train: 0.831349	val: 0.383366	test: 0.355180

Epoch: 65
Loss: 0.11946955722452732
ROC train: 0.973118	val: 0.767651	test: 0.721346
PRC train: 0.834136	val: 0.358351	test: 0.354981

Epoch: 66
Loss: 0.11917335638127588
ROC train: 0.974692	val: 0.771123	test: 0.726343
PRC train: 0.839697	val: 0.359194	test: 0.357591

Epoch: 67
Loss: 0.1172387098997148
ROC train: 0.971764	val: 0.766618	test: 0.725821
PRC train: 0.823434	val: 0.361465	test: 0.344689

Epoch: 68
Loss: 0.11692959753723114
ROC train: 0.975245	val: 0.773525	test: 0.724968
PRC train: 0.848449	val: 0.350583	test: 0.351833

Epoch: 69
Loss: 0.11437611040759896
ROC train: 0.972770	val: 0.773218	test: 0.719404
PRC train: 0.825829	val: 0.377228	test: 0.358192

Epoch: 70
Loss: 0.11407346042364555
ROC train: 0.978352	val: 0.769803	test: 0.718998
PRC train: 0.860644	val: 0.356369	test: 0.343577

Epoch: 71
Loss: 0.11120768343379499
ROC train: 0.978983	val: 0.766216	test: 0.727241
PRC train: 0.867086	val: 0.367565	test: 0.348044

Epoch: 72
Loss: 0.11136176888020294
ROC train: 0.978369	val: 0.773409	test: 0.725776
PRC train: 0.861858	val: 0.358276	test: 0.350398

Epoch: 73
Loss: 0.11070176379382034
ROC train: 0.979724	val: 0.778655	test: 0.713930
PRC train: 0.869000	val: 0.369269	test: 0.332839

Epoch: 74
Loss: 0.10998007078594636
ROC train: 0.980820	val: 0.770779	test: 0.719073
PRC train: 0.872472	val: 0.355788	test: 0.341255

Epoch: 75
Loss: 0.10923787388801634
ROC train: 0.981782	val: 0.779848	test: 0.720903
PRC train: 0.878276	val: 0.346679	test: 0.353624

Epoch: 76
Loss: 0.10973492596152892
ROC train: 0.981294	val: 0.774814	test: 0.724773
PRC train: 0.878701	val: 0.362277	test: 0.338245

Epoch: 77
Loss: 0.10850769947474494
ROC train: 0.981775	val: 0.779141	test: 0.724654
PRC train: 0.875489	val: 0.354923	test: 0.355382

Epoch: 78
Loss: 0.1059338212347663
ROC train: 0.980232	val: 0.774033	test: 0.727138
PRC train: 0.869935	val: 0.366821	test: 0.332136

Epoch: 79
Loss: 0.10708014169388941
ROC train: 0.983857	val: 0.782476	test: 0.722283
PRC train: 0.887378	val: 0.370273	test: 0.348623

Epoch: 80
Loss: 0.10422130021623018
ROC train: 0.983720	val: 0.778143	test: 0.722698
PRC train: 0.887318	val: 0.358965	test: 0.320638

Epoch: 81
Loss: 0.10284605092112514
ROC train: 0.985952	val: 0.774955	test: 0.726801
PRC train: 0.901324	val: 0.348503	test: 0.330985

Epoch: 82
Loss: 0.10209500754715287
ROC train: 0.984794	val: 0.781985	test: 0.724286
PRC train: 0.898156	val: 0.373874	test: 0.348107

Epoch: 83
Loss: 0.10298121543874073
ROC train: 0.985267	val: 0.776639	test: 0.719534
PRC train: 0.897749	val: 0.366511	test: 0.344939

Epoch: 84
Loss: 0.09933534968004817
ROC train: 0.986694	val: 0.774457	test: 0.714437
PRC train: 0.904308	val: 0.348344	test: 0.332629

Epoch: 85
Loss: 0.09478312920039618
ROC train: 0.987274	val: 0.774525	test: 0.719009
PRC train: 0.907913	val: 0.363974	test: 0.332490

Epoch: 86
Loss: 0.09840470459940297
ROC train: 0.988294	val: 0.778936	test: 0.723801
PRC train: 0.915946	val: 0.372194	test: 0.362067

Epoch: 87
Loss: 0.09796555864946241
ROC train: 0.989466	val: 0.767883	test: 0.716412
PRC train: 0.922350	val: 0.357146	test: 0.331557

Epoch: 88
Loss: 0.0967272690984931
ROC train: 0.988730	val: 0.777552	test: 0.723231
PRC train: 0.915317	val: 0.362620	test: 0.348667

Epoch: 89
Loss: 0.09717241582777042
ROC train: 0.989355	val: 0.773652	test: 0.718995
PRC train: 0.920312	val: 0.366034	test: 0.352655

Epoch: 90
Loss: 0.09414316264382475
ROC train: 0.989582	val: 0.770400	test: 0.708544
PRC train: 0.919370	val: 0.368656	test: 0.342147

Epoch: 91
Loss: 0.09455354318902616
ROC train: 0.989931	val: 0.771339	test: 0.717796
PRC train: 0.927828	val: 0.365405	test: 0.338992

Epoch: 92
Loss: 0.09273593344880267
ROC train: 0.989929	val: 0.773382	test: 0.722313
PRC train: 0.927366	val: 0.358734	test: 0.336707

Epoch: 93
Loss: 0.0941767685382108
ROC train: 0.990156	val: 0.768348	test: 0.712722
PRC train: 0.639963	val: 0.342400	test: 0.367126

Epoch: 33
Loss: 0.15347877745638727
ROC train: 0.922934	val: 0.769090	test: 0.739587
PRC train: 0.654458	val: 0.361500	test: 0.374213

Epoch: 34
Loss: 0.1514511753194897
ROC train: 0.926184	val: 0.776646	test: 0.733685
PRC train: 0.663114	val: 0.346891	test: 0.363011

Epoch: 35
Loss: 0.15129898433710393
ROC train: 0.925661	val: 0.762508	test: 0.725676
PRC train: 0.648488	val: 0.337784	test: 0.351446

Epoch: 36
Loss: 0.14923715016009803
ROC train: 0.929581	val: 0.776106	test: 0.738870
PRC train: 0.658516	val: 0.354812	test: 0.361992

Epoch: 37
Loss: 0.14907423188539037
ROC train: 0.931546	val: 0.753383	test: 0.733050
PRC train: 0.674475	val: 0.330705	test: 0.351682

Epoch: 38
Loss: 0.1491383953357669
ROC train: 0.929973	val: 0.760025	test: 0.741360
PRC train: 0.680176	val: 0.321027	test: 0.348376

Epoch: 39
Loss: 0.1471045852041684
ROC train: 0.934478	val: 0.770526	test: 0.742984
PRC train: 0.690797	val: 0.360235	test: 0.366470

Epoch: 40
Loss: 0.14377045613244488
ROC train: 0.937229	val: 0.765791	test: 0.735235
PRC train: 0.697147	val: 0.351920	test: 0.367897

Epoch: 41
Loss: 0.14246038519666654
ROC train: 0.939323	val: 0.762181	test: 0.729375
PRC train: 0.706516	val: 0.353998	test: 0.356847

Epoch: 42
Loss: 0.14293724109669806
ROC train: 0.940756	val: 0.758515	test: 0.735273
PRC train: 0.710985	val: 0.342216	test: 0.370524

Epoch: 43
Loss: 0.14188225808744637
ROC train: 0.942543	val: 0.769283	test: 0.737660
PRC train: 0.712937	val: 0.351367	test: 0.363776

Epoch: 44
Loss: 0.14324137302383635
ROC train: 0.941577	val: 0.777722	test: 0.745321
PRC train: 0.714614	val: 0.366526	test: 0.372878

Epoch: 45
Loss: 0.14118712441805925
ROC train: 0.940359	val: 0.767661	test: 0.727444
PRC train: 0.704571	val: 0.348202	test: 0.357450

Epoch: 46
Loss: 0.1399846864117532
ROC train: 0.945991	val: 0.766098	test: 0.741299
PRC train: 0.720242	val: 0.342748	test: 0.360654

Epoch: 47
Loss: 0.13758027580375912
ROC train: 0.944321	val: 0.762084	test: 0.738379
PRC train: 0.724786	val: 0.329167	test: 0.357535

Epoch: 48
Loss: 0.13556516100028002
ROC train: 0.950804	val: 0.767346	test: 0.736287
PRC train: 0.748245	val: 0.359023	test: 0.365295

Epoch: 49
Loss: 0.1344692408885229
ROC train: 0.950128	val: 0.763719	test: 0.736690
PRC train: 0.752579	val: 0.350050	test: 0.363941

Epoch: 50
Loss: 0.1323207381634951
ROC train: 0.952648	val: 0.769359	test: 0.735251
PRC train: 0.755538	val: 0.357888	test: 0.357475

Epoch: 51
Loss: 0.13350613959895724
ROC train: 0.955704	val: 0.763795	test: 0.730482
PRC train: 0.763105	val: 0.355674	test: 0.355962

Epoch: 52
Loss: 0.13199782656637138
ROC train: 0.955733	val: 0.766010	test: 0.735250
PRC train: 0.766189	val: 0.354068	test: 0.363138

Epoch: 53
Loss: 0.1306722933257255
ROC train: 0.956399	val: 0.768850	test: 0.730117
PRC train: 0.767974	val: 0.351002	test: 0.363440

Epoch: 54
Loss: 0.1311966755064751
ROC train: 0.959229	val: 0.765983	test: 0.739478
PRC train: 0.780774	val: 0.355200	test: 0.361033

Epoch: 55
Loss: 0.12958253907883396
ROC train: 0.956971	val: 0.762313	test: 0.737796
PRC train: 0.770627	val: 0.343770	test: 0.354627

Epoch: 56
Loss: 0.1290573880610141
ROC train: 0.959812	val: 0.765159	test: 0.740839
PRC train: 0.778912	val: 0.356979	test: 0.361629

Epoch: 57
Loss: 0.12479429207944964
ROC train: 0.964467	val: 0.753043	test: 0.727711
PRC train: 0.799420	val: 0.340907	test: 0.368319

Epoch: 58
Loss: 0.12571257630535188
ROC train: 0.964560	val: 0.747547	test: 0.723958
PRC train: 0.797050	val: 0.331206	test: 0.345107

Epoch: 59
Loss: 0.12521133563865663
ROC train: 0.965084	val: 0.765803	test: 0.734692
PRC train: 0.797813	val: 0.359354	test: 0.363638

Epoch: 60
Loss: 0.1254608503845792
ROC train: 0.963863	val: 0.760028	test: 0.734881
PRC train: 0.793740	val: 0.348634	test: 0.356882

Epoch: 61
Loss: 0.12398514216469927
ROC train: 0.965464	val: 0.760083	test: 0.735515
PRC train: 0.807426	val: 0.357171	test: 0.365372

Epoch: 62
Loss: 0.12310491711784109
ROC train: 0.967243	val: 0.759868	test: 0.739752
PRC train: 0.817896	val: 0.341667	test: 0.368559

Epoch: 63
Loss: 0.12107150468542063
ROC train: 0.968257	val: 0.767999	test: 0.733507
PRC train: 0.815558	val: 0.348092	test: 0.360281

Epoch: 64
Loss: 0.12082759335462245
ROC train: 0.970277	val: 0.760026	test: 0.736068
PRC train: 0.819319	val: 0.360470	test: 0.367046

Epoch: 65
Loss: 0.1184491110927597
ROC train: 0.971172	val: 0.753574	test: 0.733581
PRC train: 0.828928	val: 0.355536	test: 0.374122

Epoch: 66
Loss: 0.11768889551692728
ROC train: 0.969660	val: 0.756740	test: 0.745585
PRC train: 0.817228	val: 0.336434	test: 0.361725

Epoch: 67
Loss: 0.11704279993268019
ROC train: 0.971130	val: 0.759287	test: 0.742100
PRC train: 0.829853	val: 0.345895	test: 0.362808

Epoch: 68
Loss: 0.1142279397256075
ROC train: 0.973181	val: 0.758135	test: 0.737314
PRC train: 0.829118	val: 0.338490	test: 0.354288

Epoch: 69
Loss: 0.11741917078666686
ROC train: 0.972414	val: 0.751672	test: 0.733484
PRC train: 0.833582	val: 0.351218	test: 0.359999

Epoch: 70
Loss: 0.11475320268372421
ROC train: 0.975631	val: 0.750050	test: 0.732091
PRC train: 0.848928	val: 0.338150	test: 0.367071

Epoch: 71
Loss: 0.11397293327365345
ROC train: 0.976297	val: 0.748018	test: 0.731986
PRC train: 0.847464	val: 0.354963	test: 0.359807

Epoch: 72
Loss: 0.11309207574573028
ROC train: 0.975332	val: 0.755929	test: 0.739484
PRC train: 0.850283	val: 0.336443	test: 0.370284

Epoch: 73
Loss: 0.11067397831349485
ROC train: 0.978714	val: 0.755351	test: 0.738028
PRC train: 0.863564	val: 0.338790	test: 0.368275

Epoch: 74
Loss: 0.10958736122485294
ROC train: 0.979329	val: 0.750675	test: 0.741170
PRC train: 0.866698	val: 0.351847	test: 0.360556

Epoch: 75
Loss: 0.10829801325116704
ROC train: 0.976575	val: 0.757900	test: 0.743850
PRC train: 0.850177	val: 0.368398	test: 0.376736

Epoch: 76
Loss: 0.11014950838704789
ROC train: 0.979630	val: 0.757322	test: 0.743181
PRC train: 0.870727	val: 0.363003	test: 0.362956

Epoch: 77
Loss: 0.1078444499163405
ROC train: 0.978947	val: 0.767214	test: 0.738809
PRC train: 0.867499	val: 0.363031	test: 0.387749

Epoch: 78
Loss: 0.10470492288540138
ROC train: 0.982420	val: 0.748572	test: 0.721472
PRC train: 0.882737	val: 0.350829	test: 0.356726

Epoch: 79
Loss: 0.10611996890156121
ROC train: 0.982714	val: 0.758034	test: 0.735958
PRC train: 0.882821	val: 0.351887	test: 0.360250

Epoch: 80
Loss: 0.1053314312919135
ROC train: 0.981089	val: 0.759513	test: 0.740843
PRC train: 0.882976	val: 0.355166	test: 0.375532

Epoch: 81
Loss: 0.10559308752881004
ROC train: 0.982125	val: 0.761771	test: 0.735871
PRC train: 0.879427	val: 0.348881	test: 0.373103

Epoch: 82
Loss: 0.10431151602997883
ROC train: 0.983768	val: 0.749115	test: 0.741957
PRC train: 0.889885	val: 0.343039	test: 0.385715

Epoch: 83
Loss: 0.1016054160961715
ROC train: 0.984199	val: 0.750413	test: 0.727432
PRC train: 0.894661	val: 0.362200	test: 0.360753

Epoch: 84
Loss: 0.09986491780512835
ROC train: 0.984817	val: 0.746555	test: 0.720375
PRC train: 0.895623	val: 0.341554	test: 0.347263

Epoch: 85
Loss: 0.10277451081870856
ROC train: 0.984610	val: 0.746391	test: 0.719345
PRC train: 0.894288	val: 0.342511	test: 0.355302

Epoch: 86
Loss: 0.09956587408950249
ROC train: 0.986376	val: 0.755971	test: 0.729707
PRC train: 0.902166	val: 0.353629	test: 0.352287

Epoch: 87
Loss: 0.09907041801938657
ROC train: 0.986361	val: 0.758733	test: 0.731428
PRC train: 0.906250	val: 0.359863	test: 0.372054

Epoch: 88
Loss: 0.10016498438516779
ROC train: 0.987721	val: 0.754054	test: 0.735144
PRC train: 0.911174	val: 0.345206	test: 0.354001

Epoch: 89
Loss: 0.09783097912085516
ROC train: 0.987135	val: 0.742792	test: 0.727497
PRC train: 0.907826	val: 0.350108	test: 0.349425

Epoch: 90
Loss: 0.09659593253002846
ROC train: 0.988445	val: 0.753183	test: 0.729210
PRC train: 0.917508	val: 0.340619	test: 0.356244

Epoch: 91
Loss: 0.09493380514189148
ROC train: 0.987725	val: 0.750932	test: 0.742020
PRC train: 0.914355	val: 0.338866	test: 0.351772

Epoch: 92
Loss: 0.09267811878993788
ROC train: 0.989033	val: 0.752761	test: 0.731177
PRC train: 0.919064	val: 0.344106	test: 0.350080

Epoch: 93
Loss: 0.09193269654318843
ROC train: 0.989473	val: 0.756909	test: 0.729615
PRC train: 0.643247	val: 0.323968	test: 0.333963

Epoch: 33
Loss: 0.15272078700459027
ROC train: 0.924078	val: 0.763775	test: 0.727817
PRC train: 0.659352	val: 0.327880	test: 0.344757

Epoch: 34
Loss: 0.15264694144547852
ROC train: 0.928200	val: 0.766310	test: 0.735811
PRC train: 0.663116	val: 0.337846	test: 0.357091

Epoch: 35
Loss: 0.1490193953055148
ROC train: 0.929811	val: 0.770083	test: 0.731294
PRC train: 0.670595	val: 0.342286	test: 0.352685

Epoch: 36
Loss: 0.14769029068296702
ROC train: 0.931547	val: 0.754727	test: 0.729174
PRC train: 0.678107	val: 0.318773	test: 0.344784

Epoch: 37
Loss: 0.14750197084514322
ROC train: 0.935729	val: 0.762299	test: 0.725588
PRC train: 0.690753	val: 0.332722	test: 0.352065

Epoch: 38
Loss: 0.1463523807222238
ROC train: 0.934301	val: 0.766433	test: 0.737053
PRC train: 0.687446	val: 0.338338	test: 0.357341

Epoch: 39
Loss: 0.144143463327381
ROC train: 0.933350	val: 0.766425	test: 0.741245
PRC train: 0.681882	val: 0.351504	test: 0.363796

Epoch: 40
Loss: 0.14565993410617317
ROC train: 0.936743	val: 0.752063	test: 0.734653
PRC train: 0.692892	val: 0.317740	test: 0.351121

Epoch: 41
Loss: 0.14264809617139154
ROC train: 0.943086	val: 0.768554	test: 0.734307
PRC train: 0.709736	val: 0.323921	test: 0.365758

Epoch: 42
Loss: 0.1410124407519158
ROC train: 0.942271	val: 0.771111	test: 0.733373
PRC train: 0.713861	val: 0.332305	test: 0.362561

Epoch: 43
Loss: 0.1388521448846788
ROC train: 0.944601	val: 0.768468	test: 0.736878
PRC train: 0.723857	val: 0.342954	test: 0.358846

Epoch: 44
Loss: 0.13780082874754032
ROC train: 0.946798	val: 0.780080	test: 0.739888
PRC train: 0.731649	val: 0.349895	test: 0.369231

Epoch: 45
Loss: 0.13762755964993512
ROC train: 0.946021	val: 0.769697	test: 0.741087
PRC train: 0.715794	val: 0.326756	test: 0.360868

Epoch: 46
Loss: 0.13894562584835912
ROC train: 0.947250	val: 0.763314	test: 0.737227
PRC train: 0.733217	val: 0.324005	test: 0.351412

Epoch: 47
Loss: 0.13818951627894432
ROC train: 0.949134	val: 0.759336	test: 0.738914
PRC train: 0.730243	val: 0.313431	test: 0.370267

Epoch: 48
Loss: 0.13528439572855444
ROC train: 0.952021	val: 0.770989	test: 0.732813
PRC train: 0.745648	val: 0.332404	test: 0.353779

Epoch: 49
Loss: 0.134674308482544
ROC train: 0.951702	val: 0.777268	test: 0.738562
PRC train: 0.744870	val: 0.353822	test: 0.376993

Epoch: 50
Loss: 0.13442918487241334
ROC train: 0.954309	val: 0.764095	test: 0.729199
PRC train: 0.757592	val: 0.320549	test: 0.350360

Epoch: 51
Loss: 0.13025690202905027
ROC train: 0.955088	val: 0.769246	test: 0.735834
PRC train: 0.756866	val: 0.339628	test: 0.355448

Epoch: 52
Loss: 0.13101077872946143
ROC train: 0.956810	val: 0.746046	test: 0.723194
PRC train: 0.757791	val: 0.305486	test: 0.340603

Epoch: 53
Loss: 0.13356564749136818
ROC train: 0.952321	val: 0.756543	test: 0.730902
PRC train: 0.747840	val: 0.338580	test: 0.350416

Epoch: 54
Loss: 0.12977986790437496
ROC train: 0.955627	val: 0.772385	test: 0.727402
PRC train: 0.756517	val: 0.337515	test: 0.349644

Epoch: 55
Loss: 0.12892971233808712
ROC train: 0.959808	val: 0.770715	test: 0.736026
PRC train: 0.778514	val: 0.347306	test: 0.356102

Epoch: 56
Loss: 0.1266137714396517
ROC train: 0.962760	val: 0.766261	test: 0.721700
PRC train: 0.786334	val: 0.326188	test: 0.352071

Epoch: 57
Loss: 0.12574269907953334
ROC train: 0.963783	val: 0.766139	test: 0.732573
PRC train: 0.790921	val: 0.336626	test: 0.357681

Epoch: 58
Loss: 0.12415780350169733
ROC train: 0.962086	val: 0.763796	test: 0.728262
PRC train: 0.777092	val: 0.324907	test: 0.341584

Epoch: 59
Loss: 0.12318400648256812
ROC train: 0.965122	val: 0.763774	test: 0.732041
PRC train: 0.786184	val: 0.341225	test: 0.361450

Epoch: 60
Loss: 0.1224928730179856
ROC train: 0.967739	val: 0.772050	test: 0.730335
PRC train: 0.810336	val: 0.348428	test: 0.365651

Epoch: 61
Loss: 0.1221538958528277
ROC train: 0.969209	val: 0.749670	test: 0.726538
PRC train: 0.812012	val: 0.314822	test: 0.348547

Epoch: 62
Loss: 0.12202363784733115
ROC train: 0.967685	val: 0.770472	test: 0.735248
PRC train: 0.808314	val: 0.339917	test: 0.356902

Epoch: 63
Loss: 0.12257323432824896
ROC train: 0.968292	val: 0.753619	test: 0.728299
PRC train: 0.808794	val: 0.316386	test: 0.350959

Epoch: 64
Loss: 0.11953389727592967
ROC train: 0.970549	val: 0.762270	test: 0.735424
PRC train: 0.821475	val: 0.320078	test: 0.357348

Epoch: 65
Loss: 0.11806056905063145
ROC train: 0.971625	val: 0.757146	test: 0.721482
PRC train: 0.828908	val: 0.324785	test: 0.351062

Epoch: 66
Loss: 0.11834985326325677
ROC train: 0.972809	val: 0.753755	test: 0.717486
PRC train: 0.835662	val: 0.327002	test: 0.353083

Epoch: 67
Loss: 0.11681426898909107
ROC train: 0.973813	val: 0.773338	test: 0.733056
PRC train: 0.842694	val: 0.345386	test: 0.368285

Epoch: 68
Loss: 0.11566153041143604
ROC train: 0.975701	val: 0.761391	test: 0.722073
PRC train: 0.847161	val: 0.330325	test: 0.360450

Epoch: 69
Loss: 0.11391160981969554
ROC train: 0.975061	val: 0.758195	test: 0.724665
PRC train: 0.842204	val: 0.326165	test: 0.358550

Epoch: 70
Loss: 0.11189449868230578
ROC train: 0.977481	val: 0.774126	test: 0.731183
PRC train: 0.858137	val: 0.349790	test: 0.373752

Epoch: 71
Loss: 0.11062610787678119
ROC train: 0.976622	val: 0.771902	test: 0.741847
PRC train: 0.850734	val: 0.343753	test: 0.369717

Epoch: 72
Loss: 0.11086553352075688
ROC train: 0.977265	val: 0.763416	test: 0.731989
PRC train: 0.858928	val: 0.349426	test: 0.374337

Epoch: 73
Loss: 0.11050961166738528
ROC train: 0.979134	val: 0.755257	test: 0.723464
PRC train: 0.860169	val: 0.323141	test: 0.353924

Epoch: 74
Loss: 0.10897776704027835
ROC train: 0.978160	val: 0.764417	test: 0.735816
PRC train: 0.863292	val: 0.352148	test: 0.375265

Epoch: 75
Loss: 0.11176966751802317
ROC train: 0.980340	val: 0.757325	test: 0.726698
PRC train: 0.871249	val: 0.320154	test: 0.354781

Epoch: 76
Loss: 0.10565159206713934
ROC train: 0.978691	val: 0.762082	test: 0.732120
PRC train: 0.865823	val: 0.324722	test: 0.360379

Epoch: 77
Loss: 0.10585674590090009
ROC train: 0.980198	val: 0.766429	test: 0.741143
PRC train: 0.870976	val: 0.340887	test: 0.376584

Epoch: 78
Loss: 0.1054324321352103
ROC train: 0.981938	val: 0.767471	test: 0.727084
PRC train: 0.877885	val: 0.338027	test: 0.355412

Epoch: 79
Loss: 0.10466041537250913
ROC train: 0.982349	val: 0.760830	test: 0.739632
PRC train: 0.886862	val: 0.331995	test: 0.368843

Epoch: 80
Loss: 0.10573263583423506
ROC train: 0.982971	val: 0.773780	test: 0.742306
PRC train: 0.888661	val: 0.349355	test: 0.376134

Epoch: 81
Loss: 0.10452976286313115
ROC train: 0.984293	val: 0.760630	test: 0.738278
PRC train: 0.893587	val: 0.332047	test: 0.361803

Epoch: 82
Loss: 0.1014035682410654
ROC train: 0.984505	val: 0.758780	test: 0.733622
PRC train: 0.894887	val: 0.331081	test: 0.363420

Epoch: 83
Loss: 0.10127450367326984
ROC train: 0.982840	val: 0.759854	test: 0.730679
PRC train: 0.887028	val: 0.330921	test: 0.356791

Epoch: 84
Loss: 0.09891381073527614
ROC train: 0.986788	val: 0.757704	test: 0.721634
PRC train: 0.908849	val: 0.339906	test: 0.357256

Epoch: 85
Loss: 0.09945090458287284
ROC train: 0.987499	val: 0.758780	test: 0.734722
PRC train: 0.910471	val: 0.334730	test: 0.379870

Epoch: 86
Loss: 0.09847167292868514
ROC train: 0.986482	val: 0.765026	test: 0.730207
PRC train: 0.908812	val: 0.335003	test: 0.358544

Epoch: 87
Loss: 0.09755811414609633
ROC train: 0.986880	val: 0.754910	test: 0.731760
PRC train: 0.911841	val: 0.336627	test: 0.347204

Epoch: 88
Loss: 0.09766125949789521
ROC train: 0.986976	val: 0.762442	test: 0.727904
PRC train: 0.912410	val: 0.340323	test: 0.376147

Epoch: 89
Loss: 0.09629083677080802
ROC train: 0.987305	val: 0.753297	test: 0.729289
PRC train: 0.913477	val: 0.341525	test: 0.350851

Epoch: 90
Loss: 0.09376762640671563
ROC train: 0.989058	val: 0.753856	test: 0.730886
PRC train: 0.922109	val: 0.330273	test: 0.363609

Epoch: 91
Loss: 0.09346550665705873
ROC train: 0.988484	val: 0.763202	test: 0.726261
PRC train: 0.922283	val: 0.340416	test: 0.368520

Epoch: 92
Loss: 0.09602186603922813
ROC train: 0.989623	val: 0.758694	test: 0.730321
PRC train: 0.925933	val: 0.343087	test: 0.369370

Epoch: 93
Loss: 0.09514000078851215
ROC train: 0.989152	val: 0.765504	test: 0.728145
ROC train: 0.974427	val: 0.764888	test: 0.735609
PRC train: 0.841547	val: 0.354443	test: 0.345697

Epoch: 95
Loss: 0.10828485012413738
ROC train: 0.976674	val: 0.759417	test: 0.728746
PRC train: 0.852893	val: 0.350277	test: 0.333842

Epoch: 96
Loss: 0.10613695914310521
ROC train: 0.976860	val: 0.768523	test: 0.736190
PRC train: 0.854417	val: 0.359750	test: 0.349664

Epoch: 97
Loss: 0.10738672840531654
ROC train: 0.977753	val: 0.767518	test: 0.734289
PRC train: 0.859780	val: 0.363767	test: 0.343473

Epoch: 98
Loss: 0.10693584133127315
ROC train: 0.977872	val: 0.767751	test: 0.728036
PRC train: 0.864268	val: 0.366416	test: 0.342977

Epoch: 99
Loss: 0.1064066345546983
ROC train: 0.978789	val: 0.765958	test: 0.733402
PRC train: 0.866458	val: 0.361054	test: 0.354527

Epoch: 100
Loss: 0.1052876991173361
ROC train: 0.978585	val: 0.752757	test: 0.727309
PRC train: 0.862267	val: 0.363239	test: 0.341836

Epoch: 101
Loss: 0.10450030022689116
ROC train: 0.978609	val: 0.748297	test: 0.733663
PRC train: 0.862570	val: 0.345451	test: 0.338811

Epoch: 102
Loss: 0.10514565282877958
ROC train: 0.979009	val: 0.760108	test: 0.731397
PRC train: 0.866977	val: 0.359036	test: 0.338224

Epoch: 103
Loss: 0.10522534459775759
ROC train: 0.980177	val: 0.758185	test: 0.728175
PRC train: 0.875481	val: 0.352265	test: 0.355619

Epoch: 104
Loss: 0.103001066551128
ROC train: 0.980607	val: 0.761052	test: 0.724261
PRC train: 0.875150	val: 0.352401	test: 0.338504

Epoch: 105
Loss: 0.10268477777469859
ROC train: 0.980682	val: 0.751815	test: 0.735740
PRC train: 0.873444	val: 0.341264	test: 0.342272

Epoch: 106
Loss: 0.10234606252239885
ROC train: 0.981507	val: 0.751872	test: 0.728004
PRC train: 0.878114	val: 0.340348	test: 0.324212

Epoch: 107
Loss: 0.10190233917868838
ROC train: 0.981618	val: 0.760755	test: 0.734026
PRC train: 0.880923	val: 0.359460	test: 0.355704

Epoch: 108
Loss: 0.10125488678707155
ROC train: 0.981558	val: 0.760009	test: 0.734203
PRC train: 0.883229	val: 0.356087	test: 0.356266

Epoch: 109
Loss: 0.09926844353770631
ROC train: 0.980776	val: 0.757223	test: 0.735480
PRC train: 0.882476	val: 0.362726	test: 0.358065

Epoch: 110
Loss: 0.1003801763493285
ROC train: 0.981280	val: 0.749344	test: 0.721702
PRC train: 0.877333	val: 0.340988	test: 0.321888

Epoch: 111
Loss: 0.10299798462675218
ROC train: 0.981815	val: 0.753574	test: 0.719656
PRC train: 0.882987	val: 0.340034	test: 0.338131

Epoch: 112
Loss: 0.10029724039785654
ROC train: 0.982956	val: 0.759416	test: 0.728898
PRC train: 0.888793	val: 0.369567	test: 0.338839

Epoch: 113
Loss: 0.09869112491974179
ROC train: 0.982811	val: 0.752286	test: 0.729198
PRC train: 0.888146	val: 0.346781	test: 0.339696

Epoch: 114
Loss: 0.10033628262193821
ROC train: 0.982276	val: 0.743797	test: 0.722392
PRC train: 0.882327	val: 0.336281	test: 0.320037

Epoch: 115
Loss: 0.10197934575425005
ROC train: 0.983521	val: 0.756644	test: 0.727791
PRC train: 0.890631	val: 0.352322	test: 0.334608

Epoch: 116
Loss: 0.09791780948630711
ROC train: 0.983516	val: 0.748508	test: 0.716178
PRC train: 0.890601	val: 0.350749	test: 0.327115

Epoch: 117
Loss: 0.09748425470111466
ROC train: 0.983727	val: 0.752778	test: 0.728541
PRC train: 0.890854	val: 0.352823	test: 0.349885

Epoch: 118
Loss: 0.09859202809443411
ROC train: 0.984520	val: 0.764629	test: 0.727505
PRC train: 0.895804	val: 0.355310	test: 0.343705

Epoch: 119
Loss: 0.09417507058557487
ROC train: 0.984292	val: 0.761016	test: 0.728124
PRC train: 0.894306	val: 0.355230	test: 0.336852

Epoch: 120
Loss: 0.09715875809144241
ROC train: 0.985687	val: 0.748017	test: 0.728679
PRC train: 0.897279	val: 0.346302	test: 0.335993

Early stopping
Best (ROC):	 train: 0.913354	val: 0.794824	test: 0.743865
Best (PRC):	 train: 0.626229	val: 0.370947	test: 0.366424

ROC train: 0.977056	val: 0.771198	test: 0.727899
PRC train: 0.860770	val: 0.375356	test: 0.345518

Epoch: 95
Loss: 0.10852210498790534
ROC train: 0.978992	val: 0.764987	test: 0.724500
PRC train: 0.869119	val: 0.372699	test: 0.337000

Epoch: 96
Loss: 0.10620566439045798
ROC train: 0.977820	val: 0.763134	test: 0.718565
PRC train: 0.864996	val: 0.368906	test: 0.343962

Epoch: 97
Loss: 0.10690106579986491
ROC train: 0.979418	val: 0.758347	test: 0.716972
PRC train: 0.867332	val: 0.360404	test: 0.343044

Epoch: 98
Loss: 0.10699464034975005
ROC train: 0.978988	val: 0.767322	test: 0.733235
PRC train: 0.868317	val: 0.373384	test: 0.336132

Epoch: 99
Loss: 0.10436004242505255
ROC train: 0.979598	val: 0.761312	test: 0.731748
PRC train: 0.873537	val: 0.370072	test: 0.334430

Epoch: 100
Loss: 0.1063282015304822
ROC train: 0.980005	val: 0.762691	test: 0.728404
PRC train: 0.874429	val: 0.369002	test: 0.336919

Epoch: 101
Loss: 0.10519086085584597
ROC train: 0.979932	val: 0.767265	test: 0.732382
PRC train: 0.875535	val: 0.362313	test: 0.334684

Epoch: 102
Loss: 0.10353156668183372
ROC train: 0.981196	val: 0.765462	test: 0.718437
PRC train: 0.877030	val: 0.355602	test: 0.326383

Epoch: 103
Loss: 0.10346329019100448
ROC train: 0.981088	val: 0.768976	test: 0.725392
PRC train: 0.883257	val: 0.376174	test: 0.333161

Epoch: 104
Loss: 0.10250100546056305
ROC train: 0.981148	val: 0.761520	test: 0.723067
PRC train: 0.881645	val: 0.375076	test: 0.343822

Epoch: 105
Loss: 0.10099649106868418
ROC train: 0.981841	val: 0.770099	test: 0.730459
PRC train: 0.884004	val: 0.379497	test: 0.340480

Epoch: 106
Loss: 0.09981202633048601
ROC train: 0.982518	val: 0.763610	test: 0.724896
PRC train: 0.886520	val: 0.380360	test: 0.345887

Epoch: 107
Loss: 0.09861922424716756
ROC train: 0.981520	val: 0.776733	test: 0.721739
PRC train: 0.886521	val: 0.379388	test: 0.347801

Epoch: 108
Loss: 0.1002270446395347
ROC train: 0.982457	val: 0.767568	test: 0.735716
PRC train: 0.886021	val: 0.397239	test: 0.351914

Epoch: 109
Loss: 0.09878297304303739
ROC train: 0.982053	val: 0.766665	test: 0.726794
PRC train: 0.884188	val: 0.376799	test: 0.338406

Epoch: 110
Loss: 0.09892962148643548
ROC train: 0.983461	val: 0.761805	test: 0.733364
PRC train: 0.891783	val: 0.382004	test: 0.345602

Epoch: 111
Loss: 0.09872547212148533
ROC train: 0.984281	val: 0.761056	test: 0.728202
PRC train: 0.895903	val: 0.377065	test: 0.351144

Epoch: 112
Loss: 0.09911310961095504
ROC train: 0.983738	val: 0.771312	test: 0.733329
PRC train: 0.891559	val: 0.370306	test: 0.345477

Epoch: 113
Loss: 0.09950468562650293
ROC train: 0.984985	val: 0.770700	test: 0.734086
PRC train: 0.901174	val: 0.384390	test: 0.344547

Epoch: 114
Loss: 0.0978729073560929
ROC train: 0.984168	val: 0.760672	test: 0.724392
PRC train: 0.898443	val: 0.366688	test: 0.330732

Epoch: 115
Loss: 0.09627337779453166
ROC train: 0.985232	val: 0.772436	test: 0.734217
PRC train: 0.902998	val: 0.388264	test: 0.348584

Epoch: 116
Loss: 0.09573347713828342
ROC train: 0.986065	val: 0.772310	test: 0.729995
PRC train: 0.907767	val: 0.387269	test: 0.340362

Epoch: 117
Loss: 0.0971736743535868
ROC train: 0.985022	val: 0.764889	test: 0.727839
PRC train: 0.897872	val: 0.367620	test: 0.326133

Epoch: 118
Loss: 0.09507638667022618
ROC train: 0.985533	val: 0.772105	test: 0.728462
PRC train: 0.905740	val: 0.377765	test: 0.332778

Epoch: 119
Loss: 0.09516535534085976
ROC train: 0.986668	val: 0.764969	test: 0.734007
PRC train: 0.910088	val: 0.381263	test: 0.340087

Epoch: 120
Loss: 0.09454144306590546
ROC train: 0.983579	val: 0.762204	test: 0.724414
PRC train: 0.889771	val: 0.345006	test: 0.315158

Early stopping
Best (ROC):	 train: 0.952417	val: 0.788758	test: 0.744291
Best (PRC):	 train: 0.745466	val: 0.395405	test: 0.357433

ROC train: 0.978008	val: 0.772709	test: 0.750492
PRC train: 0.863526	val: 0.404639	test: 0.361116

Epoch: 95
Loss: 0.107333226541787
ROC train: 0.977877	val: 0.768556	test: 0.752498
PRC train: 0.864043	val: 0.396077	test: 0.369068

Epoch: 96
Loss: 0.1077938435121873
ROC train: 0.976989	val: 0.762331	test: 0.750830
PRC train: 0.858455	val: 0.387327	test: 0.347883

Epoch: 97
Loss: 0.10715974502806254
ROC train: 0.978293	val: 0.773302	test: 0.741459
PRC train: 0.864981	val: 0.398353	test: 0.354845

Epoch: 98
Loss: 0.10480311541481616
ROC train: 0.978815	val: 0.770817	test: 0.745180
PRC train: 0.868197	val: 0.385538	test: 0.355181

Epoch: 99
Loss: 0.10337062068262376
ROC train: 0.979686	val: 0.772797	test: 0.736130
PRC train: 0.870920	val: 0.386199	test: 0.348060

Epoch: 100
Loss: 0.10306794333346515
ROC train: 0.979649	val: 0.766228	test: 0.730605
PRC train: 0.869668	val: 0.376641	test: 0.346292

Epoch: 101
Loss: 0.10293323856060338
ROC train: 0.980773	val: 0.769394	test: 0.748121
PRC train: 0.877722	val: 0.398232	test: 0.364392

Epoch: 102
Loss: 0.10431118562291189
ROC train: 0.980244	val: 0.777865	test: 0.749752
PRC train: 0.874146	val: 0.389923	test: 0.361246

Epoch: 103
Loss: 0.10243940304340979
ROC train: 0.981211	val: 0.766657	test: 0.740686
PRC train: 0.878790	val: 0.390310	test: 0.366278

Epoch: 104
Loss: 0.10278341905875817
ROC train: 0.981924	val: 0.771536	test: 0.746871
PRC train: 0.882891	val: 0.399222	test: 0.371464

Epoch: 105
Loss: 0.10289808719516928
ROC train: 0.981551	val: 0.764653	test: 0.731591
PRC train: 0.880480	val: 0.380695	test: 0.356371

Epoch: 106
Loss: 0.10171003524493055
ROC train: 0.982864	val: 0.762368	test: 0.743898
PRC train: 0.887159	val: 0.392239	test: 0.359039

Epoch: 107
Loss: 0.10005554110994909
ROC train: 0.981478	val: 0.766358	test: 0.750989
PRC train: 0.882669	val: 0.400766	test: 0.358332

Epoch: 108
Loss: 0.09955948364401865
ROC train: 0.983778	val: 0.764802	test: 0.748330
PRC train: 0.894157	val: 0.388274	test: 0.365617

Epoch: 109
Loss: 0.09687332648495467
ROC train: 0.983160	val: 0.762685	test: 0.744531
PRC train: 0.892341	val: 0.391096	test: 0.362715

Epoch: 110
Loss: 0.09738790368885385
ROC train: 0.983134	val: 0.771969	test: 0.748921
PRC train: 0.888630	val: 0.385629	test: 0.362088

Epoch: 111
Loss: 0.098124076332941
ROC train: 0.984130	val: 0.763669	test: 0.742713
PRC train: 0.894799	val: 0.384729	test: 0.369396

Epoch: 112
Loss: 0.10045931163975923
ROC train: 0.982712	val: 0.764316	test: 0.749698
PRC train: 0.888087	val: 0.384115	test: 0.363558

Epoch: 113
Loss: 0.09789303559964874
ROC train: 0.983611	val: 0.765392	test: 0.744434
PRC train: 0.893035	val: 0.379342	test: 0.344079

Epoch: 114
Loss: 0.09715897650687814
ROC train: 0.984833	val: 0.762196	test: 0.742635
PRC train: 0.900255	val: 0.382732	test: 0.363712

Epoch: 115
Loss: 0.097148516874744
ROC train: 0.984448	val: 0.757271	test: 0.740906
PRC train: 0.895783	val: 0.375984	test: 0.350884

Epoch: 116
Loss: 0.09674067617893262
ROC train: 0.985408	val: 0.760851	test: 0.746443
PRC train: 0.902509	val: 0.374776	test: 0.355910

Epoch: 117
Loss: 0.09574172584042874
ROC train: 0.984829	val: 0.762818	test: 0.741435
PRC train: 0.897790	val: 0.395172	test: 0.345654

Epoch: 118
Loss: 0.09445519119702377
ROC train: 0.986331	val: 0.754395	test: 0.738660
PRC train: 0.908252	val: 0.382170	test: 0.348795

Epoch: 119
Loss: 0.09349211116003307
ROC train: 0.985870	val: 0.761089	test: 0.743667
PRC train: 0.905472	val: 0.380866	test: 0.344710

Epoch: 120
Loss: 0.09443802744506154
ROC train: 0.985235	val: 0.759570	test: 0.733694
PRC train: 0.898634	val: 0.394639	test: 0.357720

Early stopping
Best (ROC):	 train: 0.900200	val: 0.784580	test: 0.767964
Best (PRC):	 train: 0.590229	val: 0.371216	test: 0.375244
All runs completed.

PRC train: 0.940554	val: 0.336419	test: 0.344501

Epoch: 94
Loss: 0.08996798297529604
ROC train: 0.993738	val: 0.744828	test: 0.702414
PRC train: 0.948937	val: 0.345949	test: 0.336732

Epoch: 95
Loss: 0.08660701405865812
ROC train: 0.993768	val: 0.738528	test: 0.698448
PRC train: 0.947708	val: 0.308256	test: 0.308635

Epoch: 96
Loss: 0.08886202836663007
ROC train: 0.992769	val: 0.739903	test: 0.706846
PRC train: 0.943476	val: 0.325065	test: 0.329552

Epoch: 97
Loss: 0.0862346572550173
ROC train: 0.994023	val: 0.737147	test: 0.701584
PRC train: 0.953246	val: 0.325937	test: 0.320516

Epoch: 98
Loss: 0.08529056780898385
ROC train: 0.994540	val: 0.741355	test: 0.700301
PRC train: 0.955724	val: 0.315152	test: 0.329782

Epoch: 99
Loss: 0.08412137543764335
ROC train: 0.994351	val: 0.740374	test: 0.705175
PRC train: 0.952305	val: 0.313829	test: 0.305676

Epoch: 100
Loss: 0.08285220099135189
ROC train: 0.994968	val: 0.738339	test: 0.696334
PRC train: 0.959375	val: 0.311898	test: 0.318561

Epoch: 101
Loss: 0.08346846160240533
ROC train: 0.995065	val: 0.738611	test: 0.696384
PRC train: 0.956613	val: 0.317921	test: 0.322268

Epoch: 102
Loss: 0.08163125102993471
ROC train: 0.995462	val: 0.738930	test: 0.700534
PRC train: 0.960304	val: 0.332670	test: 0.328428

Epoch: 103
Loss: 0.08071372352596275
ROC train: 0.994694	val: 0.734554	test: 0.706999
PRC train: 0.955742	val: 0.318243	test: 0.337543

Epoch: 104
Loss: 0.07942723184056905
ROC train: 0.995289	val: 0.736181	test: 0.702114
PRC train: 0.960987	val: 0.316304	test: 0.325001

Epoch: 105
Loss: 0.08233427320303845
ROC train: 0.996427	val: 0.739831	test: 0.695804
PRC train: 0.967130	val: 0.311958	test: 0.316703

Epoch: 106
Loss: 0.07951451662987417
ROC train: 0.996116	val: 0.734117	test: 0.686106
PRC train: 0.964798	val: 0.322701	test: 0.309127

Epoch: 107
Loss: 0.08012956965247019
ROC train: 0.996116	val: 0.736902	test: 0.689544
PRC train: 0.966127	val: 0.321770	test: 0.310757

Epoch: 108
Loss: 0.0768639770843114
ROC train: 0.996518	val: 0.729536	test: 0.699716
PRC train: 0.969263	val: 0.330369	test: 0.337228

Epoch: 109
Loss: 0.07512115440027764
ROC train: 0.996540	val: 0.733841	test: 0.697655
PRC train: 0.970072	val: 0.331701	test: 0.322259

Epoch: 110
Loss: 0.0759553534804507
ROC train: 0.997038	val: 0.733796	test: 0.702449
PRC train: 0.971276	val: 0.319066	test: 0.330189

Epoch: 111
Loss: 0.07510019846125038
ROC train: 0.997059	val: 0.735463	test: 0.695378
PRC train: 0.972322	val: 0.312700	test: 0.317914

Epoch: 112
Loss: 0.07452977708034425
ROC train: 0.996538	val: 0.727235	test: 0.689270
PRC train: 0.968969	val: 0.311332	test: 0.309713

Epoch: 113
Loss: 0.0749814844320192
ROC train: 0.997393	val: 0.740685	test: 0.704936
PRC train: 0.976128	val: 0.329268	test: 0.330146

Epoch: 114
Loss: 0.06975916803132227
ROC train: 0.997274	val: 0.731506	test: 0.697714
PRC train: 0.973274	val: 0.323720	test: 0.316519

Epoch: 115
Loss: 0.07337433146358084
ROC train: 0.997723	val: 0.741024	test: 0.702843
PRC train: 0.978782	val: 0.329735	test: 0.321680

Epoch: 116
Loss: 0.07060351897407997
ROC train: 0.997302	val: 0.732101	test: 0.693770
PRC train: 0.975481	val: 0.316297	test: 0.323559

Epoch: 117
Loss: 0.0709303344965277
ROC train: 0.997584	val: 0.724346	test: 0.692499
PRC train: 0.974538	val: 0.301888	test: 0.323318

Epoch: 118
Loss: 0.06970340398327213
ROC train: 0.997768	val: 0.747182	test: 0.705395
PRC train: 0.980261	val: 0.335367	test: 0.324206

Epoch: 119
Loss: 0.06755782354685151
ROC train: 0.998060	val: 0.734577	test: 0.694939
PRC train: 0.982739	val: 0.317955	test: 0.313926

Epoch: 120
Loss: 0.06801016232134904
ROC train: 0.998062	val: 0.732366	test: 0.703796
PRC train: 0.982148	val: 0.310665	test: 0.300965

Early stopping
Best (ROC):	 train: 0.908835	val: 0.772192	test: 0.728549
Best (PRC):	 train: 0.586702	val: 0.353175	test: 0.351471

PRC train: 0.942580	val: 0.300385	test: 0.296525

Epoch: 94
Loss: 0.08613263080280528
ROC train: 0.993782	val: 0.743279	test: 0.701910
PRC train: 0.944031	val: 0.300290	test: 0.303424

Epoch: 95
Loss: 0.08748470538961887
ROC train: 0.994496	val: 0.743099	test: 0.704873
PRC train: 0.949364	val: 0.311666	test: 0.314506

Epoch: 96
Loss: 0.08647532557039542
ROC train: 0.993885	val: 0.747245	test: 0.707377
PRC train: 0.948540	val: 0.321019	test: 0.325888

Epoch: 97
Loss: 0.08593880205780648
ROC train: 0.995027	val: 0.745720	test: 0.704028
PRC train: 0.953826	val: 0.325772	test: 0.324779

Epoch: 98
Loss: 0.082362292156115
ROC train: 0.995008	val: 0.730569	test: 0.703216
PRC train: 0.953962	val: 0.306134	test: 0.306024

Epoch: 99
Loss: 0.08109588097825847
ROC train: 0.993522	val: 0.726780	test: 0.701416
PRC train: 0.938904	val: 0.286372	test: 0.296065

Epoch: 100
Loss: 0.08179029064542657
ROC train: 0.995023	val: 0.713485	test: 0.685342
PRC train: 0.952605	val: 0.282619	test: 0.284035

Epoch: 101
Loss: 0.0819847315350986
ROC train: 0.995430	val: 0.733264	test: 0.703781
PRC train: 0.957115	val: 0.303794	test: 0.305801

Epoch: 102
Loss: 0.08147203229437038
ROC train: 0.995806	val: 0.729140	test: 0.696980
PRC train: 0.960810	val: 0.296172	test: 0.311865

Epoch: 103
Loss: 0.07928648414694957
ROC train: 0.996924	val: 0.732726	test: 0.698460
PRC train: 0.970453	val: 0.305774	test: 0.303059

Epoch: 104
Loss: 0.0800223585509281
ROC train: 0.996485	val: 0.738698	test: 0.701594
PRC train: 0.966694	val: 0.322425	test: 0.315723

Epoch: 105
Loss: 0.07619847554498234
ROC train: 0.996827	val: 0.728903	test: 0.702109
PRC train: 0.969011	val: 0.286408	test: 0.293795

Epoch: 106
Loss: 0.07458534861687656
ROC train: 0.996822	val: 0.734591	test: 0.699985
PRC train: 0.967778	val: 0.304815	test: 0.306035

Epoch: 107
Loss: 0.07435991432431363
ROC train: 0.994266	val: 0.718314	test: 0.693167
PRC train: 0.943490	val: 0.290666	test: 0.288868

Epoch: 108
Loss: 0.07603980445165243
ROC train: 0.997428	val: 0.728796	test: 0.697109
PRC train: 0.975487	val: 0.297192	test: 0.299399

Epoch: 109
Loss: 0.07271796745510288
ROC train: 0.996961	val: 0.736365	test: 0.703354
PRC train: 0.972000	val: 0.315479	test: 0.305396

Epoch: 110
Loss: 0.07171862655032368
ROC train: 0.997008	val: 0.730397	test: 0.704588
PRC train: 0.970954	val: 0.294864	test: 0.290624

Epoch: 111
Loss: 0.07084350806756826
ROC train: 0.997376	val: 0.727102	test: 0.697947
PRC train: 0.974395	val: 0.291553	test: 0.300947

Epoch: 112
Loss: 0.0710217072075758
ROC train: 0.997769	val: 0.730055	test: 0.704069
PRC train: 0.978588	val: 0.296728	test: 0.293015

Epoch: 113
Loss: 0.06963696851851071
ROC train: 0.997820	val: 0.739139	test: 0.711153
PRC train: 0.978994	val: 0.295210	test: 0.305941

Epoch: 114
Loss: 0.06910684134097139
ROC train: 0.997667	val: 0.730196	test: 0.708974
PRC train: 0.977285	val: 0.295827	test: 0.312886

Epoch: 115
Loss: 0.07217455879221418
ROC train: 0.997477	val: 0.732049	test: 0.701567
PRC train: 0.974812	val: 0.305061	test: 0.313547

Epoch: 116
Loss: 0.07234788698922986
ROC train: 0.997806	val: 0.731998	test: 0.705993
PRC train: 0.978325	val: 0.302103	test: 0.309341

Epoch: 117
Loss: 0.06673829928017565
ROC train: 0.998026	val: 0.749544	test: 0.713428
PRC train: 0.980351	val: 0.320403	test: 0.322370

Epoch: 118
Loss: 0.06565004247689772
ROC train: 0.998335	val: 0.740090	test: 0.711186
PRC train: 0.983551	val: 0.312490	test: 0.313866

Epoch: 119
Loss: 0.06436826322208161
ROC train: 0.998604	val: 0.740311	test: 0.709232
PRC train: 0.985530	val: 0.305135	test: 0.300422

Epoch: 120
Loss: 0.06626596942729752
ROC train: 0.998263	val: 0.752326	test: 0.712134
PRC train: 0.983376	val: 0.324982	test: 0.306215

Early stopping
Best (ROC):	 train: 0.901408	val: 0.760469	test: 0.739313
Best (PRC):	 train: 0.574245	val: 0.331263	test: 0.335852

PRC train: 0.943243	val: 0.363212	test: 0.334760

Epoch: 94
Loss: 0.08900124500946799
ROC train: 0.994140	val: 0.756892	test: 0.708597
PRC train: 0.948633	val: 0.351501	test: 0.336984

Epoch: 95
Loss: 0.08734468193918689
ROC train: 0.994322	val: 0.753506	test: 0.711573
PRC train: 0.953632	val: 0.354940	test: 0.335650

Epoch: 96
Loss: 0.08594639477794079
ROC train: 0.994023	val: 0.750750	test: 0.699888
PRC train: 0.949072	val: 0.338512	test: 0.342443

Epoch: 97
Loss: 0.08612077011273539
ROC train: 0.994037	val: 0.756478	test: 0.716823
PRC train: 0.953668	val: 0.360387	test: 0.352302

Epoch: 98
Loss: 0.08453646599654896
ROC train: 0.995126	val: 0.755984	test: 0.719118
PRC train: 0.957496	val: 0.356265	test: 0.338111

Epoch: 99
Loss: 0.0826078856435228
ROC train: 0.995195	val: 0.746328	test: 0.709915
PRC train: 0.957476	val: 0.337523	test: 0.329467

Epoch: 100
Loss: 0.08178978951096987
ROC train: 0.995224	val: 0.761805	test: 0.723031
PRC train: 0.956501	val: 0.340402	test: 0.337715

Epoch: 101
Loss: 0.0817364777039562
ROC train: 0.995272	val: 0.756306	test: 0.728848
PRC train: 0.958549	val: 0.351013	test: 0.345643

Epoch: 102
Loss: 0.07999341096501471
ROC train: 0.995777	val: 0.755839	test: 0.716070
PRC train: 0.964563	val: 0.350119	test: 0.334678

Epoch: 103
Loss: 0.0801405585333374
ROC train: 0.995585	val: 0.756437	test: 0.714171
PRC train: 0.962101	val: 0.342868	test: 0.336219

Epoch: 104
Loss: 0.07727516646972275
ROC train: 0.996646	val: 0.758520	test: 0.713148
PRC train: 0.971014	val: 0.357739	test: 0.338571

Epoch: 105
Loss: 0.07928399566491114
ROC train: 0.996258	val: 0.766308	test: 0.715276
PRC train: 0.969529	val: 0.352713	test: 0.343474

Epoch: 106
Loss: 0.07757009536187055
ROC train: 0.996803	val: 0.748829	test: 0.707035
PRC train: 0.971883	val: 0.331933	test: 0.312611

Epoch: 107
Loss: 0.07360624305091369
ROC train: 0.997103	val: 0.756193	test: 0.706091
PRC train: 0.974111	val: 0.340329	test: 0.330252

Epoch: 108
Loss: 0.07664250608495758
ROC train: 0.997135	val: 0.757772	test: 0.699500
PRC train: 0.975107	val: 0.344695	test: 0.330726

Epoch: 109
Loss: 0.0751981711401842
ROC train: 0.996926	val: 0.746955	test: 0.698437
PRC train: 0.970791	val: 0.333106	test: 0.320288

Epoch: 110
Loss: 0.07535775176229129
ROC train: 0.996791	val: 0.742954	test: 0.705150
PRC train: 0.973637	val: 0.312611	test: 0.308140

Epoch: 111
Loss: 0.0763035829053303
ROC train: 0.997381	val: 0.749955	test: 0.709147
PRC train: 0.977053	val: 0.345394	test: 0.316586

Epoch: 112
Loss: 0.07530931492150965
ROC train: 0.997660	val: 0.756795	test: 0.714808
PRC train: 0.978848	val: 0.355921	test: 0.343033

Epoch: 113
Loss: 0.07327568243915565
ROC train: 0.997623	val: 0.750350	test: 0.709317
PRC train: 0.978256	val: 0.344985	test: 0.341659

Epoch: 114
Loss: 0.07182146248357692
ROC train: 0.997652	val: 0.749515	test: 0.711945
PRC train: 0.978021	val: 0.351962	test: 0.328219

Epoch: 115
Loss: 0.07028935719672376
ROC train: 0.997999	val: 0.743515	test: 0.706529
PRC train: 0.982442	val: 0.335840	test: 0.335622

Epoch: 116
Loss: 0.06965878008092431
ROC train: 0.998259	val: 0.757270	test: 0.717409
PRC train: 0.983518	val: 0.352067	test: 0.339488

Epoch: 117
Loss: 0.06782880604160252
ROC train: 0.998153	val: 0.755324	test: 0.718316
PRC train: 0.981957	val: 0.356548	test: 0.346329

Epoch: 118
Loss: 0.0721801736069797
ROC train: 0.997455	val: 0.746046	test: 0.714554
PRC train: 0.977730	val: 0.342719	test: 0.337084

Epoch: 119
Loss: 0.06895738939631907
ROC train: 0.998113	val: 0.743464	test: 0.706694
PRC train: 0.982933	val: 0.334103	test: 0.331527

Epoch: 120
Loss: 0.06787514389746317
ROC train: 0.998621	val: 0.753660	test: 0.711022
PRC train: 0.987657	val: 0.342171	test: 0.321774

Early stopping
Best (ROC):	 train: 0.923312	val: 0.780481	test: 0.744081
Best (PRC):	 train: 0.641512	val: 0.350618	test: 0.344700
All runs completed.

PRC train: 0.945244	val: 0.238675	test: 0.236488

Epoch: 94
Loss: 0.08910186069058228
ROC train: 0.994866	val: 0.713531	test: 0.690359
PRC train: 0.948869	val: 0.241113	test: 0.236460

Epoch: 95
Loss: 0.08540162892256048
ROC train: 0.995318	val: 0.725160	test: 0.693687
PRC train: 0.949916	val: 0.248753	test: 0.248516

Epoch: 96
Loss: 0.08625353840312186
ROC train: 0.994563	val: 0.723075	test: 0.690568
PRC train: 0.948512	val: 0.252149	test: 0.236329

Epoch: 97
Loss: 0.08475724507741908
ROC train: 0.995688	val: 0.721852	test: 0.689476
PRC train: 0.954471	val: 0.261044	test: 0.245975

Epoch: 98
Loss: 0.0823125595044514
ROC train: 0.995891	val: 0.717224	test: 0.687611
PRC train: 0.958511	val: 0.257818	test: 0.239424

Epoch: 99
Loss: 0.08171148958597456
ROC train: 0.995602	val: 0.719331	test: 0.692667
PRC train: 0.956179	val: 0.241058	test: 0.217769

Epoch: 100
Loss: 0.08013274702474449
ROC train: 0.996527	val: 0.727194	test: 0.695637
PRC train: 0.961633	val: 0.273643	test: 0.251868

Epoch: 101
Loss: 0.07942087608062813
ROC train: 0.996368	val: 0.717474	test: 0.682639
PRC train: 0.960600	val: 0.255577	test: 0.229195

Epoch: 102
Loss: 0.07940974781470342
ROC train: 0.995780	val: 0.711976	test: 0.688944
PRC train: 0.957426	val: 0.236971	test: 0.225203

Epoch: 103
Loss: 0.07840011464898035
ROC train: 0.996770	val: 0.716383	test: 0.690336
PRC train: 0.965296	val: 0.248627	test: 0.232260

Epoch: 104
Loss: 0.07755897010949812
ROC train: 0.997326	val: 0.727559	test: 0.687882
PRC train: 0.970207	val: 0.263256	test: 0.242251

Epoch: 105
Loss: 0.07702159962044872
ROC train: 0.997815	val: 0.728613	test: 0.689180
PRC train: 0.975942	val: 0.264195	test: 0.236990

Epoch: 106
Loss: 0.07337468133603822
ROC train: 0.997976	val: 0.710731	test: 0.693930
PRC train: 0.977482	val: 0.243132	test: 0.230816

Epoch: 107
Loss: 0.07418352541083516
ROC train: 0.997335	val: 0.724745	test: 0.688780
PRC train: 0.971404	val: 0.263840	test: 0.234306

Epoch: 108
Loss: 0.07379695343040757
ROC train: 0.997484	val: 0.709585	test: 0.691147
PRC train: 0.973146	val: 0.240505	test: 0.228112

Epoch: 109
Loss: 0.07442024271851479
ROC train: 0.998097	val: 0.710951	test: 0.684511
PRC train: 0.979748	val: 0.237807	test: 0.223451

Epoch: 110
Loss: 0.07375645809030265
ROC train: 0.997505	val: 0.719430	test: 0.673926
PRC train: 0.965952	val: 0.265503	test: 0.247552

Epoch: 111
Loss: 0.07212324040087431
ROC train: 0.998120	val: 0.721534	test: 0.687495
PRC train: 0.978454	val: 0.253954	test: 0.240934

Epoch: 112
Loss: 0.07045798855534331
ROC train: 0.998231	val: 0.721163	test: 0.674589
PRC train: 0.977087	val: 0.265178	test: 0.259179

Epoch: 113
Loss: 0.07271897285049504
ROC train: 0.998365	val: 0.727856	test: 0.690801
PRC train: 0.979007	val: 0.267969	test: 0.261273

Epoch: 114
Loss: 0.06838580054729183
ROC train: 0.998731	val: 0.713858	test: 0.684475
PRC train: 0.984448	val: 0.233933	test: 0.221056

Epoch: 115
Loss: 0.06768407890157405
ROC train: 0.998615	val: 0.701523	test: 0.678000
PRC train: 0.982125	val: 0.236321	test: 0.227714

Epoch: 116
Loss: 0.07008377877345792
ROC train: 0.998556	val: 0.719222	test: 0.679421
PRC train: 0.981734	val: 0.251087	test: 0.231804

Epoch: 117
Loss: 0.0669179018039178
ROC train: 0.998962	val: 0.709863	test: 0.683197
PRC train: 0.986392	val: 0.236869	test: 0.229307

Epoch: 118
Loss: 0.06772107574580175
ROC train: 0.998838	val: 0.715838	test: 0.675079
PRC train: 0.986957	val: 0.244377	test: 0.217574

Epoch: 119
Loss: 0.06627579964368921
ROC train: 0.998890	val: 0.696795	test: 0.668927
PRC train: 0.985788	val: 0.223674	test: 0.206443

Epoch: 120
Loss: 0.06469735543589993
ROC train: 0.999154	val: 0.697294	test: 0.656389
PRC train: 0.988436	val: 0.219216	test: 0.207926

Early stopping
Best (ROC):	 train: 0.860423	val: 0.759790	test: 0.713213
Best (PRC):	 train: 0.441825	val: 0.314701	test: 0.304779

PRC train: 0.951350	val: 0.276675	test: 0.290586

Epoch: 94
Loss: 0.08641009270915102
ROC train: 0.994826	val: 0.699241	test: 0.683056
PRC train: 0.951024	val: 0.274611	test: 0.278575

Epoch: 95
Loss: 0.08618392677099423
ROC train: 0.995097	val: 0.695954	test: 0.675074
PRC train: 0.949735	val: 0.272280	test: 0.279520

Epoch: 96
Loss: 0.08559389540895035
ROC train: 0.995139	val: 0.703662	test: 0.682368
PRC train: 0.952870	val: 0.257143	test: 0.267549

Epoch: 97
Loss: 0.08576116155836383
ROC train: 0.995778	val: 0.697243	test: 0.671955
PRC train: 0.957970	val: 0.264190	test: 0.267962

Epoch: 98
Loss: 0.0828061208577675
ROC train: 0.995581	val: 0.699543	test: 0.676871
PRC train: 0.955930	val: 0.259405	test: 0.263893

Epoch: 99
Loss: 0.08116410632892926
ROC train: 0.994972	val: 0.706713	test: 0.686065
PRC train: 0.950787	val: 0.273249	test: 0.276159

Epoch: 100
Loss: 0.07993705734028135
ROC train: 0.995676	val: 0.711755	test: 0.676468
PRC train: 0.959886	val: 0.275602	test: 0.271980

Epoch: 101
Loss: 0.08159474247403187
ROC train: 0.996387	val: 0.716787	test: 0.682249
PRC train: 0.963756	val: 0.290312	test: 0.290430

Epoch: 102
Loss: 0.08126733262992687
ROC train: 0.996651	val: 0.698233	test: 0.673063
PRC train: 0.966550	val: 0.265522	test: 0.273234

Epoch: 103
Loss: 0.08073235373688871
ROC train: 0.996251	val: 0.710568	test: 0.684903
PRC train: 0.964952	val: 0.267043	test: 0.276092

Epoch: 104
Loss: 0.07614295237421366
ROC train: 0.996543	val: 0.698917	test: 0.669870
PRC train: 0.964682	val: 0.271268	test: 0.275760

Epoch: 105
Loss: 0.07743453972919072
ROC train: 0.996585	val: 0.709277	test: 0.692886
PRC train: 0.966974	val: 0.278974	test: 0.285406

Epoch: 106
Loss: 0.07706792284761295
ROC train: 0.996921	val: 0.709950	test: 0.692755
PRC train: 0.966860	val: 0.277434	test: 0.283281

Epoch: 107
Loss: 0.07555131014810684
ROC train: 0.996814	val: 0.704627	test: 0.680484
PRC train: 0.967388	val: 0.278501	test: 0.283105

Epoch: 108
Loss: 0.07435842060336277
ROC train: 0.997619	val: 0.698648	test: 0.682736
PRC train: 0.975328	val: 0.272764	test: 0.272108

Epoch: 109
Loss: 0.07099342980200539
ROC train: 0.998331	val: 0.695068	test: 0.679078
PRC train: 0.981794	val: 0.260823	test: 0.263002

Epoch: 110
Loss: 0.0728386864500698
ROC train: 0.998097	val: 0.693292	test: 0.685530
PRC train: 0.978592	val: 0.263799	test: 0.278266

Epoch: 111
Loss: 0.06935246469185469
ROC train: 0.998208	val: 0.697995	test: 0.678012
PRC train: 0.979586	val: 0.263705	test: 0.268994

Epoch: 112
Loss: 0.06977180397381802
ROC train: 0.997807	val: 0.703212	test: 0.678945
PRC train: 0.975950	val: 0.257540	test: 0.275208

Epoch: 113
Loss: 0.0708377713676977
ROC train: 0.998442	val: 0.713580	test: 0.691174
PRC train: 0.983665	val: 0.276307	test: 0.278617

Epoch: 114
Loss: 0.06810389092278156
ROC train: 0.998515	val: 0.704068	test: 0.692754
PRC train: 0.984998	val: 0.259835	test: 0.278103

Epoch: 115
Loss: 0.06708788315607003
ROC train: 0.998622	val: 0.701051	test: 0.675652
PRC train: 0.984088	val: 0.261347	test: 0.268197

Epoch: 116
Loss: 0.06970717011644832
ROC train: 0.998570	val: 0.699826	test: 0.681366
PRC train: 0.985125	val: 0.259193	test: 0.269574

Epoch: 117
Loss: 0.06428775168169318
ROC train: 0.998504	val: 0.697949	test: 0.681418
PRC train: 0.984680	val: 0.259802	test: 0.270064

Epoch: 118
Loss: 0.06574955346063324
ROC train: 0.998814	val: 0.708830	test: 0.688253
PRC train: 0.987048	val: 0.273582	test: 0.282447

Epoch: 119
Loss: 0.06572544834828578
ROC train: 0.998965	val: 0.685567	test: 0.670407
PRC train: 0.988443	val: 0.260541	test: 0.261746

Epoch: 120
Loss: 0.06614363130431847
ROC train: 0.998802	val: 0.693805	test: 0.681215
PRC train: 0.987418	val: 0.265213	test: 0.267102

Early stopping
Best (ROC):	 train: 0.885442	val: 0.759186	test: 0.740439
Best (PRC):	 train: 0.510378	val: 0.318790	test: 0.329947

PRC train: 0.950141	val: 0.245510	test: 0.237140

Epoch: 94
Loss: 0.08820637849667287
ROC train: 0.994865	val: 0.663981	test: 0.661857
PRC train: 0.951110	val: 0.248319	test: 0.253614

Epoch: 95
Loss: 0.08740044436582271
ROC train: 0.993988	val: 0.641601	test: 0.649827
PRC train: 0.946763	val: 0.208974	test: 0.210465

Epoch: 96
Loss: 0.08509957119345371
ROC train: 0.994671	val: 0.642239	test: 0.665477
PRC train: 0.952268	val: 0.203268	test: 0.222749

Epoch: 97
Loss: 0.08479539258968771
ROC train: 0.995384	val: 0.671734	test: 0.676411
PRC train: 0.958364	val: 0.252504	test: 0.251050

Epoch: 98
Loss: 0.08215657231977173
ROC train: 0.995015	val: 0.653521	test: 0.658985
PRC train: 0.956457	val: 0.221000	test: 0.223548

Epoch: 99
Loss: 0.08163777793351513
ROC train: 0.995690	val: 0.669735	test: 0.673098
PRC train: 0.959295	val: 0.250838	test: 0.261964

Epoch: 100
Loss: 0.0812028830985941
ROC train: 0.995343	val: 0.635870	test: 0.650657
PRC train: 0.955021	val: 0.214865	test: 0.213593

Epoch: 101
Loss: 0.08114359401966766
ROC train: 0.996184	val: 0.658144	test: 0.669062
PRC train: 0.963458	val: 0.227289	test: 0.234893

Epoch: 102
Loss: 0.08082420437465983
ROC train: 0.996690	val: 0.677683	test: 0.688174
PRC train: 0.966164	val: 0.270719	test: 0.268492

Epoch: 103
Loss: 0.07943340989134033
ROC train: 0.996839	val: 0.667863	test: 0.682385
PRC train: 0.970192	val: 0.248801	test: 0.242979

Epoch: 104
Loss: 0.07853540223089767
ROC train: 0.997245	val: 0.666249	test: 0.676808
PRC train: 0.973616	val: 0.235951	test: 0.235071

Epoch: 105
Loss: 0.07698715162279285
ROC train: 0.997050	val: 0.639476	test: 0.661661
PRC train: 0.969517	val: 0.207995	test: 0.215490

Epoch: 106
Loss: 0.07566723729213833
ROC train: 0.997493	val: 0.646303	test: 0.669374
PRC train: 0.975174	val: 0.222642	test: 0.226984

Epoch: 107
Loss: 0.07547824880218715
ROC train: 0.997748	val: 0.662913	test: 0.676670
PRC train: 0.976961	val: 0.256423	test: 0.256664

Epoch: 108
Loss: 0.0748209665902905
ROC train: 0.996951	val: 0.659924	test: 0.673281
PRC train: 0.971466	val: 0.248775	test: 0.248662

Epoch: 109
Loss: 0.07290713893573797
ROC train: 0.997738	val: 0.664577	test: 0.679026
PRC train: 0.978368	val: 0.262243	test: 0.243008

Epoch: 110
Loss: 0.0732625020141505
ROC train: 0.997680	val: 0.645251	test: 0.663807
PRC train: 0.978537	val: 0.230284	test: 0.226961

Epoch: 111
Loss: 0.07120746305294791
ROC train: 0.998012	val: 0.660889	test: 0.671820
PRC train: 0.979260	val: 0.242754	test: 0.237030

Epoch: 112
Loss: 0.06888240793485605
ROC train: 0.998373	val: 0.648839	test: 0.664378
PRC train: 0.984499	val: 0.228289	test: 0.219412

Epoch: 113
Loss: 0.06994994836760086
ROC train: 0.998382	val: 0.648330	test: 0.676633
PRC train: 0.984550	val: 0.228541	test: 0.229588

Epoch: 114
Loss: 0.07007565565222847
ROC train: 0.998400	val: 0.671648	test: 0.670456
PRC train: 0.984124	val: 0.259110	test: 0.237203

Epoch: 115
Loss: 0.07043915196642221
ROC train: 0.998054	val: 0.672523	test: 0.674298
PRC train: 0.978793	val: 0.254664	test: 0.243638

Epoch: 116
Loss: 0.06768915245162757
ROC train: 0.998746	val: 0.643594	test: 0.650061
PRC train: 0.987464	val: 0.228586	test: 0.221841

Epoch: 117
Loss: 0.06758208440086586
ROC train: 0.998447	val: 0.667211	test: 0.670846
PRC train: 0.984646	val: 0.248686	test: 0.253096

Epoch: 118
Loss: 0.0662744440003806
ROC train: 0.998829	val: 0.669329	test: 0.664160
PRC train: 0.988099	val: 0.268456	test: 0.249471

Epoch: 119
Loss: 0.06604974388979039
ROC train: 0.998750	val: 0.630586	test: 0.652193
PRC train: 0.987851	val: 0.208480	test: 0.210220

Epoch: 120
Loss: 0.06460528937724129
ROC train: 0.998803	val: 0.664001	test: 0.665805
PRC train: 0.986316	val: 0.233878	test: 0.222965

Early stopping
Best (ROC):	 train: 0.801508	val: 0.747640	test: 0.708976
Best (PRC):	 train: 0.327108	val: 0.295092	test: 0.283972

PRC train: 0.922297	val: 0.353343	test: 0.353227

Epoch: 94
Loss: 0.09104406576751695
ROC train: 0.989071	val: 0.762119	test: 0.738051
PRC train: 0.921085	val: 0.355529	test: 0.365469

Epoch: 95
Loss: 0.0941456872886075
ROC train: 0.990359	val: 0.753414	test: 0.729619
PRC train: 0.927932	val: 0.341489	test: 0.354671

Epoch: 96
Loss: 0.09264087823611283
ROC train: 0.989683	val: 0.759324	test: 0.737798
PRC train: 0.924117	val: 0.344428	test: 0.358783

Epoch: 97
Loss: 0.09067357617257198
ROC train: 0.989479	val: 0.757001	test: 0.736742
PRC train: 0.923507	val: 0.354202	test: 0.353036

Epoch: 98
Loss: 0.09009781255251122
ROC train: 0.990858	val: 0.744453	test: 0.736915
PRC train: 0.930471	val: 0.347241	test: 0.360389

Epoch: 99
Loss: 0.08927561121599284
ROC train: 0.991291	val: 0.756599	test: 0.737076
PRC train: 0.935791	val: 0.350739	test: 0.362392

Epoch: 100
Loss: 0.0862059241758973
ROC train: 0.991752	val: 0.751528	test: 0.733566
PRC train: 0.936116	val: 0.342053	test: 0.351597

Epoch: 101
Loss: 0.08747081895061762
ROC train: 0.992093	val: 0.742594	test: 0.735482
PRC train: 0.937604	val: 0.350514	test: 0.355306

Epoch: 102
Loss: 0.08712320973563425
ROC train: 0.990471	val: 0.749264	test: 0.736146
PRC train: 0.931490	val: 0.348794	test: 0.362906

Epoch: 103
Loss: 0.0845189718563308
ROC train: 0.991478	val: 0.727355	test: 0.718718
PRC train: 0.933228	val: 0.329836	test: 0.348808

Epoch: 104
Loss: 0.08518697693287125
ROC train: 0.993212	val: 0.735444	test: 0.719759
PRC train: 0.944269	val: 0.332000	test: 0.340699

Epoch: 105
Loss: 0.08349488674519553
ROC train: 0.992494	val: 0.746474	test: 0.735564
PRC train: 0.942980	val: 0.353303	test: 0.361003

Epoch: 106
Loss: 0.08431075079963772
ROC train: 0.993456	val: 0.749372	test: 0.731260
PRC train: 0.947815	val: 0.344867	test: 0.361141

Epoch: 107
Loss: 0.08393827644796331
ROC train: 0.993345	val: 0.747971	test: 0.736987
PRC train: 0.946578	val: 0.346402	test: 0.354427

Epoch: 108
Loss: 0.08240569548349691
ROC train: 0.993872	val: 0.757348	test: 0.736443
PRC train: 0.952346	val: 0.345397	test: 0.353724

Epoch: 109
Loss: 0.07986715933014693
ROC train: 0.994703	val: 0.745977	test: 0.744756
PRC train: 0.957918	val: 0.341367	test: 0.351332

Epoch: 110
Loss: 0.08001936339462327
ROC train: 0.994536	val: 0.742425	test: 0.731498
PRC train: 0.957712	val: 0.337261	test: 0.365006

Epoch: 111
Loss: 0.07906270834163456
ROC train: 0.994532	val: 0.740445	test: 0.733133
PRC train: 0.957079	val: 0.336643	test: 0.358750

Epoch: 112
Loss: 0.07927725201226266
ROC train: 0.993722	val: 0.746058	test: 0.736374
PRC train: 0.950155	val: 0.343365	test: 0.361969

Epoch: 113
Loss: 0.0783827203577031
ROC train: 0.995138	val: 0.739966	test: 0.726913
PRC train: 0.961795	val: 0.331487	test: 0.359129

Epoch: 114
Loss: 0.07820251322022662
ROC train: 0.994989	val: 0.747088	test: 0.738043
PRC train: 0.960948	val: 0.328346	test: 0.361331

Epoch: 115
Loss: 0.0784543326172387
ROC train: 0.995072	val: 0.739601	test: 0.726128
PRC train: 0.961491	val: 0.332332	test: 0.341013

Epoch: 116
Loss: 0.0793138638227998
ROC train: 0.995351	val: 0.751395	test: 0.734250
PRC train: 0.962555	val: 0.336538	test: 0.342050

Epoch: 117
Loss: 0.0772264778100555
ROC train: 0.996069	val: 0.738858	test: 0.733839
PRC train: 0.965654	val: 0.337334	test: 0.355676

Epoch: 118
Loss: 0.0768474535491019
ROC train: 0.996053	val: 0.739545	test: 0.734713
PRC train: 0.966096	val: 0.341050	test: 0.361145

Epoch: 119
Loss: 0.07461936613896106
ROC train: 0.995951	val: 0.733829	test: 0.730852
PRC train: 0.966264	val: 0.326112	test: 0.352922

Epoch: 120
Loss: 0.07282149734263901
ROC train: 0.996630	val: 0.731905	test: 0.725416
PRC train: 0.971193	val: 0.323053	test: 0.339179

Early stopping
Best (ROC):	 train: 0.917920	val: 0.778114	test: 0.748379
Best (PRC):	 train: 0.639963	val: 0.342400	test: 0.367126

PRC train: 0.928430	val: 0.365640	test: 0.333344

Epoch: 94
Loss: 0.09164904657495337
ROC train: 0.990352	val: 0.775149	test: 0.706907
PRC train: 0.929152	val: 0.370146	test: 0.327298

Epoch: 95
Loss: 0.09228593771091101
ROC train: 0.990893	val: 0.772644	test: 0.708349
PRC train: 0.932753	val: 0.377896	test: 0.333992

Epoch: 96
Loss: 0.09065929988990322
ROC train: 0.991568	val: 0.773233	test: 0.707020
PRC train: 0.937060	val: 0.366476	test: 0.341486

Epoch: 97
Loss: 0.08901773426501192
ROC train: 0.992154	val: 0.767176	test: 0.706645
PRC train: 0.939907	val: 0.352518	test: 0.322314

Epoch: 98
Loss: 0.08572943718938789
ROC train: 0.992564	val: 0.770685	test: 0.715530
PRC train: 0.943385	val: 0.352406	test: 0.346651

Epoch: 99
Loss: 0.08706135178161943
ROC train: 0.991874	val: 0.759352	test: 0.716608
PRC train: 0.941994	val: 0.356871	test: 0.346771

Epoch: 100
Loss: 0.08557261538150289
ROC train: 0.992735	val: 0.768187	test: 0.717797
PRC train: 0.945154	val: 0.352120	test: 0.347852

Epoch: 101
Loss: 0.08525667455915853
ROC train: 0.992330	val: 0.753567	test: 0.711669
PRC train: 0.942516	val: 0.330972	test: 0.320722

Epoch: 102
Loss: 0.08731663610770295
ROC train: 0.992475	val: 0.754665	test: 0.714667
PRC train: 0.940785	val: 0.335813	test: 0.318907

Epoch: 103
Loss: 0.08501509792341562
ROC train: 0.993222	val: 0.769006	test: 0.723399
PRC train: 0.945810	val: 0.357287	test: 0.330264

Epoch: 104
Loss: 0.0833467781106101
ROC train: 0.994156	val: 0.763858	test: 0.719054
PRC train: 0.954637	val: 0.354107	test: 0.330880

Epoch: 105
Loss: 0.08538206872241069
ROC train: 0.993110	val: 0.763754	test: 0.715466
PRC train: 0.944952	val: 0.333221	test: 0.319382

Epoch: 106
Loss: 0.08476937655510625
ROC train: 0.993336	val: 0.758896	test: 0.705662
PRC train: 0.945686	val: 0.348115	test: 0.300039

Epoch: 107
Loss: 0.08328719658174523
ROC train: 0.993478	val: 0.777569	test: 0.721584
PRC train: 0.948194	val: 0.346082	test: 0.346344

Epoch: 108
Loss: 0.08324541938721591
ROC train: 0.994615	val: 0.770628	test: 0.718294
PRC train: 0.957456	val: 0.356410	test: 0.333612

Epoch: 109
Loss: 0.0802359231231444
ROC train: 0.994855	val: 0.775598	test: 0.715779
PRC train: 0.960604	val: 0.351861	test: 0.341382

Epoch: 110
Loss: 0.07960390681705914
ROC train: 0.995160	val: 0.770764	test: 0.705651
PRC train: 0.958978	val: 0.344624	test: 0.325588

Epoch: 111
Loss: 0.07879995905427128
ROC train: 0.994824	val: 0.767549	test: 0.717811
PRC train: 0.956722	val: 0.338073	test: 0.346497

Epoch: 112
Loss: 0.07980313471370566
ROC train: 0.994568	val: 0.769302	test: 0.715364
PRC train: 0.953593	val: 0.341536	test: 0.336704

Epoch: 113
Loss: 0.07872772419836305
ROC train: 0.995870	val: 0.760147	test: 0.722516
PRC train: 0.966211	val: 0.337263	test: 0.354672

Epoch: 114
Loss: 0.0754584930542547
ROC train: 0.995706	val: 0.769010	test: 0.709606
PRC train: 0.965517	val: 0.351270	test: 0.325744

Epoch: 115
Loss: 0.07737834500228664
ROC train: 0.996002	val: 0.762975	test: 0.717602
PRC train: 0.966055	val: 0.340784	test: 0.335764

Epoch: 116
Loss: 0.07607329678148499
ROC train: 0.995885	val: 0.763247	test: 0.714239
PRC train: 0.965458	val: 0.348049	test: 0.354010

Epoch: 117
Loss: 0.07488495392123592
ROC train: 0.996695	val: 0.770606	test: 0.710634
PRC train: 0.971449	val: 0.351423	test: 0.339513

Epoch: 118
Loss: 0.07550406762056329
ROC train: 0.996931	val: 0.773671	test: 0.708973
PRC train: 0.972942	val: 0.365085	test: 0.349698

Epoch: 119
Loss: 0.07441274841319831
ROC train: 0.997187	val: 0.762448	test: 0.706043
PRC train: 0.974504	val: 0.339225	test: 0.325876

Epoch: 120
Loss: 0.07102094432947574
ROC train: 0.997115	val: 0.762687	test: 0.704210
PRC train: 0.973516	val: 0.346246	test: 0.317307

Early stopping
Best (ROC):	 train: 0.936362	val: 0.787113	test: 0.753224
Best (PRC):	 train: 0.684316	val: 0.381105	test: 0.374519
All runs completed.

PRC train: 0.927852	val: 0.358445	test: 0.355501

Epoch: 94
Loss: 0.09285002703482792
ROC train: 0.989961	val: 0.760107	test: 0.717463
PRC train: 0.927573	val: 0.345853	test: 0.345113

Epoch: 95
Loss: 0.09261833384784297
ROC train: 0.989589	val: 0.768573	test: 0.733913
PRC train: 0.928178	val: 0.352823	test: 0.366592

Epoch: 96
Loss: 0.09058747480610832
ROC train: 0.990513	val: 0.760427	test: 0.720126
PRC train: 0.934098	val: 0.336784	test: 0.351448

Epoch: 97
Loss: 0.08959345995863811
ROC train: 0.990487	val: 0.763986	test: 0.727688
PRC train: 0.932501	val: 0.336032	test: 0.360457

Epoch: 98
Loss: 0.08976223520111151
ROC train: 0.991664	val: 0.753691	test: 0.722759
PRC train: 0.938628	val: 0.329324	test: 0.336437

Epoch: 99
Loss: 0.0888715048766674
ROC train: 0.988744	val: 0.743953	test: 0.721371
PRC train: 0.920731	val: 0.311149	test: 0.356116

Epoch: 100
Loss: 0.0879441300967258
ROC train: 0.992196	val: 0.757307	test: 0.719493
PRC train: 0.944834	val: 0.341462	test: 0.355826

Epoch: 101
Loss: 0.08743313656892687
ROC train: 0.991832	val: 0.747826	test: 0.721273
PRC train: 0.943532	val: 0.329264	test: 0.353164

Epoch: 102
Loss: 0.08543455314170217
ROC train: 0.992778	val: 0.758103	test: 0.712813
PRC train: 0.947234	val: 0.335456	test: 0.341889

Epoch: 103
Loss: 0.08686560679615526
ROC train: 0.992858	val: 0.760131	test: 0.730643
PRC train: 0.947673	val: 0.334881	test: 0.348123

Epoch: 104
Loss: 0.08456443932108244
ROC train: 0.992589	val: 0.768135	test: 0.732916
PRC train: 0.947041	val: 0.347688	test: 0.361290

Epoch: 105
Loss: 0.08445036527225538
ROC train: 0.992584	val: 0.751639	test: 0.727809
PRC train: 0.946381	val: 0.325223	test: 0.351911

Epoch: 106
Loss: 0.08201075834467282
ROC train: 0.994025	val: 0.756330	test: 0.724644
PRC train: 0.953195	val: 0.341006	test: 0.359164

Epoch: 107
Loss: 0.08112666418481135
ROC train: 0.994125	val: 0.752305	test: 0.711516
PRC train: 0.956234	val: 0.336899	test: 0.338679

Epoch: 108
Loss: 0.08164684273438363
ROC train: 0.994562	val: 0.753864	test: 0.725034
PRC train: 0.959908	val: 0.325183	test: 0.345840

Epoch: 109
Loss: 0.0789979330199146
ROC train: 0.994637	val: 0.761178	test: 0.727201
PRC train: 0.960099	val: 0.340446	test: 0.347993

Epoch: 110
Loss: 0.07985860885282345
ROC train: 0.994773	val: 0.758277	test: 0.727073
PRC train: 0.961325	val: 0.344150	test: 0.355333

Epoch: 111
Loss: 0.07775396727977388
ROC train: 0.994924	val: 0.761561	test: 0.728751
PRC train: 0.961093	val: 0.345119	test: 0.347214

Epoch: 112
Loss: 0.07583708213774236
ROC train: 0.994701	val: 0.745693	test: 0.715446
PRC train: 0.956277	val: 0.318789	test: 0.333329

Epoch: 113
Loss: 0.07816583083642081
ROC train: 0.995493	val: 0.757060	test: 0.723704
PRC train: 0.966220	val: 0.331976	test: 0.345003

Epoch: 114
Loss: 0.07560948626522339
ROC train: 0.995621	val: 0.748063	test: 0.710807
PRC train: 0.966284	val: 0.333094	test: 0.350901

Epoch: 115
Loss: 0.07556721756133068
ROC train: 0.995314	val: 0.760304	test: 0.719239
PRC train: 0.964678	val: 0.351672	test: 0.351463

Epoch: 116
Loss: 0.07789116571656346
ROC train: 0.995920	val: 0.755465	test: 0.715797
PRC train: 0.968266	val: 0.341444	test: 0.359012

Epoch: 117
Loss: 0.07421848997326189
ROC train: 0.995783	val: 0.757635	test: 0.727130
PRC train: 0.966914	val: 0.345876	test: 0.356847

Epoch: 118
Loss: 0.07394846589212746
ROC train: 0.996178	val: 0.756734	test: 0.719872
PRC train: 0.970205	val: 0.348926	test: 0.344700

Epoch: 119
Loss: 0.07335668697267819
ROC train: 0.996650	val: 0.743883	test: 0.702184
PRC train: 0.973844	val: 0.323978	test: 0.321776

Epoch: 120
Loss: 0.07383247051979663
ROC train: 0.996169	val: 0.760781	test: 0.715015
PRC train: 0.969248	val: 0.348249	test: 0.342385

Early stopping
Best (ROC):	 train: 0.914492	val: 0.780370	test: 0.738754
Best (PRC):	 train: 0.625991	val: 0.341065	test: 0.352699
All runs completed.
