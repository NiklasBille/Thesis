>>> Starting run for dataset: lipo
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running RANDOM configs_static_noise_experiments/GraphCL/lipo/noise=0.0.yml on cuda:0
Running RANDOM configs_static_noise_experiments/GraphCL/lipo/noise=0.05.yml on cuda:1
Running RANDOM configs_static_noise_experiments/GraphCL/lipo/noise=0.1.yml on cuda:2
Running RANDOM configs_static_noise_experiments/GraphCL/lipo/noise=0.2.yml on cuda:3
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.05.yml --runseed 4 --device cuda:1
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.05.yml --runseed 5 --device cuda:1
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.05.yml --runseed 6 --device cuda:1
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.2.yml --runseed 4 --device cuda:3
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.2.yml --runseed 5 --device cuda:3
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.2.yml --runseed 6 --device cuda:3
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.1.yml --runseed 4 --device cuda:2
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.1.yml --runseed 5 --device cuda:2
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.1.yml --runseed 6 --device cuda:2
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.0.yml --runseed 4 --device cuda:0
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.0.yml --runseed 5 --device cuda:0
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.0.yml --runseed 6 --device cuda:0
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/lipophilicity/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/lipophilicity/noise=0.0/lipophilicity_scaff_5_26-05_11-30-36  ]
[ Using Seed :  5  ]
[ Using device :  cuda:0  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.196234890392849
RMSE train: 2.175326	val: 2.154486	test: 2.272556
MAE train: 1.932386	val: 1.890509	test: 2.027224

Epoch: 2
Loss: 3.4160288402012418
RMSE train: 1.905569	val: 1.902544	test: 2.000249
MAE train: 1.680774	val: 1.662830	test: 1.767457

Epoch: 3
Loss: 2.091801106929779
RMSE train: 1.340797	val: 1.358496	test: 1.405566
MAE train: 1.127397	val: 1.131780	test: 1.195486

Epoch: 4
Loss: 1.2722460201808385
RMSE train: 0.991862	val: 1.052585	test: 1.036964
MAE train: 0.807364	val: 0.850727	test: 0.857566

Epoch: 5
Loss: 0.96739604643413
RMSE train: 0.876847	val: 0.965690	test: 0.911748
MAE train: 0.692463	val: 0.770828	test: 0.744789

Epoch: 6
Loss: 0.8458568709237235
RMSE train: 0.834446	val: 0.925105	test: 0.870509
MAE train: 0.651498	val: 0.738971	test: 0.705273

Epoch: 7
Loss: 0.8711828461715153
RMSE train: 0.832180	val: 0.929094	test: 0.866196
MAE train: 0.654300	val: 0.742099	test: 0.705285

Epoch: 8
Loss: 0.7885242487703051
RMSE train: 0.802397	val: 0.934488	test: 0.889508
MAE train: 0.632680	val: 0.737148	test: 0.710299

Epoch: 9
Loss: 0.7659192723887307
RMSE train: 0.770178	val: 0.905235	test: 0.824917
MAE train: 0.604728	val: 0.712714	test: 0.667083

Epoch: 10
Loss: 0.7235937757151467
RMSE train: 0.763080	val: 0.891574	test: 0.832828
MAE train: 0.600166	val: 0.697197	test: 0.667194

Epoch: 11
Loss: 0.7150579137461526
RMSE train: 0.775286	val: 0.920354	test: 0.829480
MAE train: 0.608279	val: 0.725957	test: 0.673488

Epoch: 12
Loss: 0.6923225479466575
RMSE train: 0.771275	val: 0.909628	test: 0.832104
MAE train: 0.600031	val: 0.716384	test: 0.671747

Epoch: 13
Loss: 0.7117461008684975
RMSE train: 0.751381	val: 0.900714	test: 0.813743
MAE train: 0.588531	val: 0.704109	test: 0.647970

Epoch: 14
Loss: 0.6877589012895312
RMSE train: 0.744410	val: 0.883611	test: 0.817192
MAE train: 0.579940	val: 0.691377	test: 0.649775

Epoch: 15
Loss: 0.6319448522159031
RMSE train: 0.748770	val: 0.899454	test: 0.832401
MAE train: 0.587996	val: 0.706051	test: 0.671634

Epoch: 16
Loss: 0.6138899666922433
RMSE train: 0.728878	val: 0.876953	test: 0.816605
MAE train: 0.569628	val: 0.687295	test: 0.648278

Epoch: 17
Loss: 0.658901218857084
RMSE train: 0.722957	val: 0.879625	test: 0.825417
MAE train: 0.561655	val: 0.684498	test: 0.669247

Epoch: 18
Loss: 0.6357975431850978
RMSE train: 0.735917	val: 0.873180	test: 0.813824
MAE train: 0.572723	val: 0.678067	test: 0.654722

Epoch: 19
Loss: 0.6221921401364463
RMSE train: 0.695045	val: 0.853201	test: 0.795179
MAE train: 0.540897	val: 0.666724	test: 0.640208

Epoch: 20
Loss: 0.592777709875788
RMSE train: 0.703619	val: 0.872912	test: 0.816684
MAE train: 0.552875	val: 0.678962	test: 0.662518

Epoch: 21
Loss: 0.6064085534640721
RMSE train: 0.709356	val: 0.842754	test: 0.803209
MAE train: 0.554925	val: 0.650521	test: 0.655301

Epoch: 22
Loss: 0.5664378936801638
RMSE train: 0.671929	val: 0.827259	test: 0.785652
MAE train: 0.521606	val: 0.641987	test: 0.640549Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/lipophilicity/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/lipophilicity/noise=0.0/lipophilicity_scaff_4_26-05_11-30-36  ]
[ Using Seed :  4  ]
[ Using device :  cuda:0  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.108751160757882
RMSE train: 2.141731	val: 2.116846	test: 2.204930
MAE train: 1.899809	val: 1.845811	test: 1.970065

Epoch: 2
Loss: 3.20464083978108
RMSE train: 1.947387	val: 1.903476	test: 1.999296
MAE train: 1.718149	val: 1.652983	test: 1.765123

Epoch: 3
Loss: 1.9706034660339355
RMSE train: 1.273770	val: 1.291216	test: 1.322950
MAE train: 1.070337	val: 1.081778	test: 1.112361

Epoch: 4
Loss: 1.2465599136693137
RMSE train: 0.936426	val: 0.992121	test: 0.995757
MAE train: 0.755991	val: 0.795528	test: 0.810258

Epoch: 5
Loss: 0.8999721152441842
RMSE train: 0.853859	val: 0.949074	test: 0.917738
MAE train: 0.672532	val: 0.741996	test: 0.737218

Epoch: 6
Loss: 0.8277595915964672
RMSE train: 0.815822	val: 0.901714	test: 0.866332
MAE train: 0.641805	val: 0.712063	test: 0.697975

Epoch: 7
Loss: 0.8109848328999111
RMSE train: 0.817746	val: 0.922089	test: 0.866155
MAE train: 0.634955	val: 0.719448	test: 0.699006

Epoch: 8
Loss: 0.7642469108104706
RMSE train: 0.824263	val: 0.910996	test: 0.879288
MAE train: 0.639003	val: 0.724359	test: 0.700419

Epoch: 9
Loss: 0.7580887079238892
RMSE train: 0.787503	val: 0.904213	test: 0.875112
MAE train: 0.614143	val: 0.709376	test: 0.710780

Epoch: 10
Loss: 0.7498230040073395
RMSE train: 0.801189	val: 0.922772	test: 0.862518
MAE train: 0.623779	val: 0.728916	test: 0.705937

Epoch: 11
Loss: 0.6909394306795937
RMSE train: 0.774425	val: 0.907603	test: 0.865914
MAE train: 0.598855	val: 0.696256	test: 0.707841

Epoch: 12
Loss: 0.7191619191850934
RMSE train: 0.792892	val: 0.893743	test: 0.856591
MAE train: 0.615486	val: 0.698866	test: 0.696285

Epoch: 13
Loss: 0.6686947814055851
RMSE train: 0.762660	val: 0.899690	test: 0.833780
MAE train: 0.589279	val: 0.704033	test: 0.671432

Epoch: 14
Loss: 0.6468340967382703
RMSE train: 0.739823	val: 0.853454	test: 0.833767
MAE train: 0.570771	val: 0.665011	test: 0.675943

Epoch: 15
Loss: 0.6429383030959538
RMSE train: 0.722095	val: 0.856585	test: 0.823816
MAE train: 0.566562	val: 0.662572	test: 0.668902

Epoch: 16
Loss: 0.6013311296701431
RMSE train: 0.708814	val: 0.843220	test: 0.824505
MAE train: 0.551580	val: 0.652335	test: 0.664635

Epoch: 17
Loss: 0.6211603369031634
RMSE train: 0.732609	val: 0.886786	test: 0.837275
MAE train: 0.566432	val: 0.687731	test: 0.682352

Epoch: 18
Loss: 0.6001047747475761
RMSE train: 0.773327	val: 0.908664	test: 0.859219
MAE train: 0.599991	val: 0.714072	test: 0.692902

Epoch: 19
Loss: 0.601162714617593
RMSE train: 0.680029	val: 0.832588	test: 0.818332
MAE train: 0.527557	val: 0.640027	test: 0.659783

Epoch: 20
Loss: 0.5684181834970202
RMSE train: 0.695537	val: 0.864611	test: 0.826411
MAE train: 0.538347	val: 0.675818	test: 0.669588

Epoch: 21
Loss: 0.5621058429990496
RMSE train: 0.704419	val: 0.869859	test: 0.827010
MAE train: 0.545884	val: 0.673365	test: 0.671087

Epoch: 22
Loss: 0.5719951965979168
RMSE train: 0.680173	val: 0.852664	test: 0.814005
MAE train: 0.523655	val: 0.656420	test: 0.658416Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/lipophilicity/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/lipophilicity/noise=0.0/lipophilicity_scaff_6_26-05_11-30-36  ]
[ Using Seed :  6  ]
[ Using device :  cuda:0  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.671705484390259
RMSE train: 2.386766	val: 2.360769	test: 2.468415
MAE train: 2.147404	val: 2.096639	test: 2.238901

Epoch: 2
Loss: 3.7174969230379378
RMSE train: 1.999217	val: 1.974305	test: 2.065134
MAE train: 1.776747	val: 1.722118	test: 1.839883

Epoch: 3
Loss: 2.3204527497291565
RMSE train: 1.388466	val: 1.402801	test: 1.468449
MAE train: 1.185840	val: 1.176866	test: 1.259440

Epoch: 4
Loss: 1.385607123374939
RMSE train: 0.989137	val: 1.053149	test: 1.067788
MAE train: 0.810245	val: 0.857909	test: 0.888264

Epoch: 5
Loss: 0.9674314558506012
RMSE train: 0.880407	val: 0.977947	test: 0.944853
MAE train: 0.709421	val: 0.774552	test: 0.762743

Epoch: 6
Loss: 0.8564100095203945
RMSE train: 0.841348	val: 0.942852	test: 0.893373
MAE train: 0.658765	val: 0.739992	test: 0.715968

Epoch: 7
Loss: 0.8407278231212071
RMSE train: 0.808902	val: 0.931235	test: 0.877426
MAE train: 0.633612	val: 0.721467	test: 0.705360

Epoch: 8
Loss: 0.7968555774007525
RMSE train: 0.804457	val: 0.918760	test: 0.851800
MAE train: 0.618818	val: 0.720094	test: 0.685098

Epoch: 9
Loss: 0.7690326763050896
RMSE train: 0.809565	val: 0.931505	test: 0.866669
MAE train: 0.631376	val: 0.724458	test: 0.702162

Epoch: 10
Loss: 0.7133581680910928
RMSE train: 0.767170	val: 0.904512	test: 0.853308
MAE train: 0.602710	val: 0.708460	test: 0.691222

Epoch: 11
Loss: 0.7108442102159772
RMSE train: 0.765042	val: 0.882636	test: 0.834404
MAE train: 0.592871	val: 0.682676	test: 0.677122

Epoch: 12
Loss: 0.7202582359313965
RMSE train: 0.779576	val: 0.922990	test: 0.869354
MAE train: 0.599449	val: 0.722501	test: 0.710220

Epoch: 13
Loss: 0.7133002323763711
RMSE train: 0.736077	val: 0.861158	test: 0.805316
MAE train: 0.572083	val: 0.670549	test: 0.648988

Epoch: 14
Loss: 0.6936083563736507
RMSE train: 0.754972	val: 0.895512	test: 0.828154
MAE train: 0.585716	val: 0.699658	test: 0.678505

Epoch: 15
Loss: 0.6521226763725281
RMSE train: 0.721042	val: 0.849762	test: 0.796155
MAE train: 0.560007	val: 0.665726	test: 0.634064

Epoch: 16
Loss: 0.6659455171653202
RMSE train: 0.728629	val: 0.873836	test: 0.818957
MAE train: 0.563713	val: 0.685271	test: 0.655473

Epoch: 17
Loss: 0.614868049110685
RMSE train: 0.704618	val: 0.852544	test: 0.811022
MAE train: 0.544080	val: 0.658544	test: 0.653211

Epoch: 18
Loss: 0.6479844961847577
RMSE train: 0.749590	val: 0.913026	test: 0.850113
MAE train: 0.581403	val: 0.713692	test: 0.685107

Epoch: 19
Loss: 0.6174715714795249
RMSE train: 0.715597	val: 0.860959	test: 0.809101
MAE train: 0.552411	val: 0.669621	test: 0.653827

Epoch: 20
Loss: 0.5985406126294818
RMSE train: 0.715100	val: 0.855932	test: 0.805113
MAE train: 0.556116	val: 0.666220	test: 0.650509

Epoch: 21
Loss: 0.559346643941743
RMSE train: 0.679769	val: 0.829290	test: 0.792959
MAE train: 0.525378	val: 0.648685	test: 0.641844

Epoch: 22
Loss: 0.5621247483151299
RMSE train: 0.720344	val: 0.868271	test: 0.832588
MAE train: 0.561021	val: 0.675315	test: 0.677477Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/lipophilicity/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/lipophilicity/noise=0.05/lipophilicity_scaff_5_26-05_11-30-36  ]
[ Using Seed :  5  ]
[ Using device :  cuda:1  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.159291080066136
RMSE train: 2.278131	val: 2.269461	test: 2.372390
MAE train: 2.034262	val: 2.001766	test: 2.140147

Epoch: 2
Loss: 3.4820359093802318
RMSE train: 1.931772	val: 1.874215	test: 1.967249
MAE train: 1.694081	val: 1.615218	test: 1.736348

Epoch: 3
Loss: 2.252937282834734
RMSE train: 1.412086	val: 1.386226	test: 1.437786
MAE train: 1.196482	val: 1.166507	test: 1.223720

Epoch: 4
Loss: 1.4715625473431178
RMSE train: 1.075361	val: 1.091805	test: 1.058615
MAE train: 0.885721	val: 0.896592	test: 0.872984

Epoch: 5
Loss: 1.1332661594663347
RMSE train: 0.979403	val: 1.013073	test: 0.968165
MAE train: 0.791765	val: 0.815323	test: 0.785542

Epoch: 6
Loss: 0.9879497630255563
RMSE train: 0.966460	val: 1.022725	test: 0.931112
MAE train: 0.764671	val: 0.800206	test: 0.743296

Epoch: 7
Loss: 0.9817120007106236
RMSE train: 0.935238	val: 1.014884	test: 0.912914
MAE train: 0.737272	val: 0.792566	test: 0.725025

Epoch: 8
Loss: 0.9164108123098101
RMSE train: 0.885844	val: 0.986835	test: 0.894056
MAE train: 0.706958	val: 0.774655	test: 0.710328

Epoch: 9
Loss: 0.9465622987065997
RMSE train: 0.879529	val: 0.991523	test: 0.907828
MAE train: 0.701032	val: 0.783322	test: 0.726838

Epoch: 10
Loss: 0.9455978359494891
RMSE train: 0.880063	val: 1.000069	test: 0.904607
MAE train: 0.699085	val: 0.787381	test: 0.720262

Epoch: 11
Loss: 0.8791314831801823
RMSE train: 0.847722	val: 0.966326	test: 0.884032
MAE train: 0.670496	val: 0.766571	test: 0.705575

Epoch: 12
Loss: 0.8233267537185124
RMSE train: 0.838273	val: 0.949193	test: 0.863392
MAE train: 0.664170	val: 0.752275	test: 0.689625

Epoch: 13
Loss: 0.8254002204963139
RMSE train: 0.822175	val: 0.963858	test: 0.866566
MAE train: 0.650534	val: 0.764155	test: 0.690721

Epoch: 14
Loss: 0.8147689316953931
RMSE train: 0.810576	val: 0.930736	test: 0.852509
MAE train: 0.646955	val: 0.742294	test: 0.682247

Epoch: 15
Loss: 0.7965386552470071
RMSE train: 0.813105	val: 0.960620	test: 0.871966
MAE train: 0.639385	val: 0.762439	test: 0.700635

Epoch: 16
Loss: 0.7495014880384717
RMSE train: 0.784893	val: 0.931720	test: 0.853281
MAE train: 0.616934	val: 0.736412	test: 0.679976

Epoch: 17
Loss: 0.7290875102792468
RMSE train: 0.774233	val: 0.914233	test: 0.842615
MAE train: 0.612884	val: 0.724608	test: 0.675264

Epoch: 18
Loss: 0.7336612939834595
RMSE train: 0.760676	val: 0.906065	test: 0.847648
MAE train: 0.604994	val: 0.716523	test: 0.680364

Epoch: 19
Loss: 0.7322385949747903
RMSE train: 0.793188	val: 0.955391	test: 0.867346
MAE train: 0.618447	val: 0.753950	test: 0.685787

Epoch: 20
Loss: 0.7367365956306458
RMSE train: 0.753712	val: 0.882388	test: 0.838877
MAE train: 0.597119	val: 0.697456	test: 0.671947

Epoch: 21
Loss: 0.7113377068723951
RMSE train: 0.754840	val: 0.923105	test: 0.872894
MAE train: 0.600873	val: 0.736437	test: 0.699477Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/lipophilicity/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/lipophilicity/noise=0.1/lipophilicity_scaff_5_26-05_11-30-36  ]
[ Using Seed :  5  ]
[ Using device :  cuda:2  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.188540628978184
RMSE train: 2.267357	val: 2.265979	test: 2.368322
MAE train: 2.022984	val: 1.992636	test: 2.133080

Epoch: 2
Loss: 3.527523466518947
RMSE train: 1.954871	val: 1.889257	test: 1.975916
MAE train: 1.716589	val: 1.623186	test: 1.742530

Epoch: 3
Loss: 2.3096554705074857
RMSE train: 1.416622	val: 1.446388	test: 1.507241
MAE train: 1.203127	val: 1.224991	test: 1.279172

Epoch: 4
Loss: 1.5411586846624101
RMSE train: 1.110766	val: 1.150334	test: 1.117962
MAE train: 0.917347	val: 0.945030	test: 0.917510

Epoch: 5
Loss: 1.234695621899196
RMSE train: 1.013536	val: 1.111377	test: 1.000362
MAE train: 0.818386	val: 0.871424	test: 0.802256

Epoch: 6
Loss: 1.1147273353167944
RMSE train: 1.024670	val: 1.165936	test: 1.037899
MAE train: 0.808089	val: 0.903416	test: 0.826835

Epoch: 7
Loss: 1.0682410768100195
RMSE train: 0.963414	val: 1.092001	test: 0.980295
MAE train: 0.766912	val: 0.848493	test: 0.786642

Epoch: 8
Loss: 1.0141789530004774
RMSE train: 0.953060	val: 1.068745	test: 0.962037
MAE train: 0.754930	val: 0.830052	test: 0.766891

Epoch: 9
Loss: 1.0155295814786638
RMSE train: 0.916560	val: 1.065943	test: 0.972604
MAE train: 0.731981	val: 0.827667	test: 0.782032

Epoch: 10
Loss: 1.0015395453998022
RMSE train: 0.917839	val: 1.030189	test: 0.950586
MAE train: 0.733918	val: 0.816967	test: 0.756951

Epoch: 11
Loss: 0.9520222204072135
RMSE train: 0.884521	val: 1.002713	test: 0.914973
MAE train: 0.706228	val: 0.792295	test: 0.730117

Epoch: 12
Loss: 0.9044726320675441
RMSE train: 0.910674	val: 1.084794	test: 0.975153
MAE train: 0.719815	val: 0.854681	test: 0.771383

Epoch: 13
Loss: 0.9089092654841286
RMSE train: 0.856386	val: 1.065262	test: 0.970732
MAE train: 0.680822	val: 0.842125	test: 0.772981

Epoch: 14
Loss: 0.8610461609704154
RMSE train: 0.841977	val: 1.001364	test: 0.922094
MAE train: 0.672639	val: 0.797938	test: 0.732368

Epoch: 15
Loss: 0.8285700678825378
RMSE train: 0.824604	val: 1.031019	test: 0.957989
MAE train: 0.657032	val: 0.812208	test: 0.771174

Epoch: 16
Loss: 0.8162298032215664
RMSE train: 0.808698	val: 1.013328	test: 0.939089
MAE train: 0.644935	val: 0.804552	test: 0.755912

Epoch: 17
Loss: 0.8094305183206286
RMSE train: 0.797535	val: 1.000999	test: 0.935598
MAE train: 0.633968	val: 0.796271	test: 0.757158

Epoch: 18
Loss: 0.787633900131498
RMSE train: 0.781041	val: 0.959700	test: 0.906351
MAE train: 0.623706	val: 0.762099	test: 0.730810

Epoch: 19
Loss: 0.7826760538986751
RMSE train: 0.817701	val: 1.021429	test: 0.948217
MAE train: 0.643567	val: 0.808124	test: 0.754254

Epoch: 20
Loss: 0.758869856595993
RMSE train: 0.792045	val: 1.081234	test: 1.009174
MAE train: 0.627193	val: 0.852084	test: 0.805458

Epoch: 21
Loss: 0.7657696902751923
RMSE train: 0.790158	val: 1.034163	test: 0.959742
MAE train: 0.628142	val: 0.827744	test: 0.767268Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/lipophilicity/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/lipophilicity/noise=0.1/lipophilicity_scaff_4_26-05_11-30-36  ]
[ Using Seed :  4  ]
[ Using device :  cuda:2  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.31096397127424
RMSE train: 2.140373	val: 2.111875	test: 2.221727
MAE train: 1.893205	val: 1.844306	test: 1.980143

Epoch: 2
Loss: 3.552055767604283
RMSE train: 1.857509	val: 1.783054	test: 1.855979
MAE train: 1.616219	val: 1.527887	test: 1.622878

Epoch: 3
Loss: 2.2462398069245473
RMSE train: 1.487376	val: 1.421577	test: 1.451983
MAE train: 1.260755	val: 1.196816	test: 1.221668

Epoch: 4
Loss: 1.5345080580030168
RMSE train: 1.125694	val: 1.140356	test: 1.103127
MAE train: 0.936578	val: 0.937683	test: 0.914027

Epoch: 5
Loss: 1.2644775424684798
RMSE train: 1.029448	val: 1.073875	test: 0.991228
MAE train: 0.834801	val: 0.850093	test: 0.798075

Epoch: 6
Loss: 1.1742390309061324
RMSE train: 0.989747	val: 1.057375	test: 0.959773
MAE train: 0.797096	val: 0.825614	test: 0.770865

Epoch: 7
Loss: 1.1371426241738456
RMSE train: 1.001283	val: 1.100122	test: 0.981882
MAE train: 0.791525	val: 0.849072	test: 0.781898

Epoch: 8
Loss: 1.0905721996511732
RMSE train: 0.962462	val: 1.041695	test: 0.939200
MAE train: 0.759803	val: 0.807629	test: 0.751929

Epoch: 9
Loss: 1.0479337275028229
RMSE train: 0.932050	val: 1.028979	test: 0.916133
MAE train: 0.746923	val: 0.802066	test: 0.737621

Epoch: 10
Loss: 0.9958227872848511
RMSE train: 0.923132	val: 1.033341	test: 0.919317
MAE train: 0.732620	val: 0.806821	test: 0.734375

Epoch: 11
Loss: 0.9354953254972186
RMSE train: 0.907995	val: 1.053577	test: 0.928012
MAE train: 0.722440	val: 0.814417	test: 0.741748

Epoch: 12
Loss: 0.9580411229814801
RMSE train: 0.895278	val: 1.036960	test: 0.926620
MAE train: 0.709419	val: 0.805633	test: 0.741625

Epoch: 13
Loss: 0.9480325196470533
RMSE train: 0.879599	val: 1.022932	test: 0.912427
MAE train: 0.698474	val: 0.795966	test: 0.733690

Epoch: 14
Loss: 0.896371168749673
RMSE train: 0.908589	val: 1.078557	test: 0.967285
MAE train: 0.717758	val: 0.832534	test: 0.776646

Epoch: 15
Loss: 0.8539264883313861
RMSE train: 0.825937	val: 0.993499	test: 0.910266
MAE train: 0.658623	val: 0.775121	test: 0.724848

Epoch: 16
Loss: 0.8417873084545135
RMSE train: 0.825944	val: 1.012198	test: 0.920186
MAE train: 0.658804	val: 0.799342	test: 0.743201

Epoch: 17
Loss: 0.8298422140734536
RMSE train: 0.813101	val: 0.999392	test: 0.924337
MAE train: 0.654252	val: 0.795760	test: 0.741816

Epoch: 18
Loss: 0.8196051503930774
RMSE train: 0.793380	val: 0.978958	test: 0.915568
MAE train: 0.633031	val: 0.774830	test: 0.725804

Epoch: 19
Loss: 0.7860461728913444
RMSE train: 0.798731	val: 0.959924	test: 0.889201
MAE train: 0.637318	val: 0.761849	test: 0.707569

Epoch: 20
Loss: 0.7709738527025495
RMSE train: 0.776452	val: 0.965877	test: 0.912632
MAE train: 0.618979	val: 0.764287	test: 0.721560

Epoch: 21
Loss: 0.7710076783384595
RMSE train: 0.767082	val: 0.972340	test: 0.900751
MAE train: 0.611907	val: 0.776418	test: 0.724012Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/lipophilicity/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/lipophilicity/noise=0.1/lipophilicity_scaff_6_26-05_11-30-36  ]
[ Using Seed :  6  ]
[ Using device :  cuda:2  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 4.97773015499115
RMSE train: 2.199431	val: 2.248918	test: 2.337659
MAE train: 1.952980	val: 1.979946	test: 2.096803

Epoch: 2
Loss: 3.239424909864153
RMSE train: 1.906320	val: 1.812912	test: 1.871409
MAE train: 1.663316	val: 1.550824	test: 1.633948

Epoch: 3
Loss: 2.1117198382105147
RMSE train: 1.549633	val: 1.416079	test: 1.460144
MAE train: 1.320441	val: 1.189298	test: 1.226562

Epoch: 4
Loss: 1.4695355807031905
RMSE train: 1.287924	val: 1.200087	test: 1.178724
MAE train: 1.078935	val: 0.987708	test: 0.972193

Epoch: 5
Loss: 1.25065450157438
RMSE train: 1.032984	val: 1.109762	test: 1.009687
MAE train: 0.842506	val: 0.881855	test: 0.821814

Epoch: 6
Loss: 1.1391352415084839
RMSE train: 0.994580	val: 1.067893	test: 0.956597
MAE train: 0.805794	val: 0.837134	test: 0.769377

Epoch: 7
Loss: 1.061211211340768
RMSE train: 0.970570	val: 1.104055	test: 0.977394
MAE train: 0.779932	val: 0.856908	test: 0.783688

Epoch: 8
Loss: 1.0594206367220198
RMSE train: 0.944025	val: 1.031754	test: 0.927231
MAE train: 0.757691	val: 0.806319	test: 0.738170

Epoch: 9
Loss: 1.0541686458247048
RMSE train: 0.929186	val: 1.033968	test: 0.927108
MAE train: 0.746735	val: 0.804287	test: 0.744224

Epoch: 10
Loss: 1.0049479135445185
RMSE train: 0.913736	val: 1.018351	test: 0.919879
MAE train: 0.728534	val: 0.790921	test: 0.732096

Epoch: 11
Loss: 0.9563668242522648
RMSE train: 0.923317	val: 1.069025	test: 0.938230
MAE train: 0.732992	val: 0.832652	test: 0.749701

Epoch: 12
Loss: 0.9243795360837664
RMSE train: 0.881342	val: 1.024997	test: 0.922590
MAE train: 0.703463	val: 0.797296	test: 0.740747

Epoch: 13
Loss: 0.903363789830889
RMSE train: 0.867018	val: 1.058917	test: 0.946152
MAE train: 0.689095	val: 0.824267	test: 0.753537

Epoch: 14
Loss: 0.897710519177573
RMSE train: 0.860622	val: 1.098403	test: 0.989627
MAE train: 0.683242	val: 0.853643	test: 0.792192

Epoch: 15
Loss: 0.8552306933062417
RMSE train: 0.849827	val: 1.056348	test: 0.945854
MAE train: 0.673403	val: 0.824056	test: 0.747916

Epoch: 16
Loss: 0.8701104990073613
RMSE train: 0.829463	val: 0.974669	test: 0.880755
MAE train: 0.663620	val: 0.768144	test: 0.707533

Epoch: 17
Loss: 0.8349422940186092
RMSE train: 0.830037	val: 0.994355	test: 0.889402
MAE train: 0.663732	val: 0.778777	test: 0.717776

Epoch: 18
Loss: 0.7887146089758191
RMSE train: 0.828408	val: 1.003740	test: 0.914778
MAE train: 0.660265	val: 0.794075	test: 0.729354

Epoch: 19
Loss: 0.7977624663284847
RMSE train: 0.798041	val: 1.015667	test: 0.912751
MAE train: 0.637198	val: 0.798022	test: 0.733612

Epoch: 20
Loss: 0.8042828951563153
RMSE train: 0.802804	val: 0.983454	test: 0.888186
MAE train: 0.640766	val: 0.773343	test: 0.712231

Epoch: 21
Loss: 0.7676507490021842
RMSE train: 0.775822	val: 0.991875	test: 0.905701
MAE train: 0.617462	val: 0.774900	test: 0.727936Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/lipophilicity/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/lipophilicity/noise=0.05/lipophilicity_scaff_4_26-05_11-30-36  ]
[ Using Seed :  4  ]
[ Using device :  cuda:1  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.302015168326242
RMSE train: 2.063240	val: 2.038087	test: 2.130487
MAE train: 1.813679	val: 1.767088	test: 1.887043

Epoch: 2
Loss: 3.5204897948673795
RMSE train: 1.882210	val: 1.650399	test: 1.695125
MAE train: 1.641222	val: 1.393541	test: 1.465116

Epoch: 3
Loss: 2.1734338658196584
RMSE train: 1.599219	val: 1.451913	test: 1.522706
MAE train: 1.373144	val: 1.221166	test: 1.300576

Epoch: 4
Loss: 1.429316827229091
RMSE train: 1.141638	val: 1.130554	test: 1.119741
MAE train: 0.948181	val: 0.930323	test: 0.934888

Epoch: 5
Loss: 1.1721930163247245
RMSE train: 0.983420	val: 1.080785	test: 0.985733
MAE train: 0.788732	val: 0.838491	test: 0.786045

Epoch: 6
Loss: 1.0634964321340834
RMSE train: 0.937744	val: 0.992714	test: 0.924536
MAE train: 0.754752	val: 0.778005	test: 0.750212

Epoch: 7
Loss: 1.0174022827829634
RMSE train: 0.918529	val: 1.051484	test: 0.947472
MAE train: 0.732020	val: 0.803474	test: 0.755165

Epoch: 8
Loss: 0.9590624357972827
RMSE train: 0.888093	val: 1.031142	test: 0.930618
MAE train: 0.702586	val: 0.796509	test: 0.737846

Epoch: 9
Loss: 0.9096373958247048
RMSE train: 0.890514	val: 0.958130	test: 0.896587
MAE train: 0.716544	val: 0.738993	test: 0.717458

Epoch: 10
Loss: 0.9052505024841854
RMSE train: 0.855995	val: 0.985294	test: 0.901331
MAE train: 0.677555	val: 0.763342	test: 0.722396

Epoch: 11
Loss: 0.8447682772363935
RMSE train: 0.847593	val: 0.983041	test: 0.893728
MAE train: 0.674517	val: 0.767665	test: 0.715266

Epoch: 12
Loss: 0.8617411639009204
RMSE train: 0.826560	val: 0.956719	test: 0.876527
MAE train: 0.657048	val: 0.747507	test: 0.705777

Epoch: 13
Loss: 0.8538527488708496
RMSE train: 0.814878	val: 0.972942	test: 0.901517
MAE train: 0.644916	val: 0.755007	test: 0.731602

Epoch: 14
Loss: 0.8059534813676562
RMSE train: 0.810328	val: 0.933510	test: 0.866035
MAE train: 0.644177	val: 0.732258	test: 0.700090

Epoch: 15
Loss: 0.7804081184523446
RMSE train: 0.792672	val: 0.958551	test: 0.885883
MAE train: 0.621342	val: 0.754188	test: 0.711352

Epoch: 16
Loss: 0.7365766380514417
RMSE train: 0.779584	val: 0.930565	test: 0.857204
MAE train: 0.614741	val: 0.727315	test: 0.687173

Epoch: 17
Loss: 0.745007553270885
RMSE train: 0.776947	val: 0.914982	test: 0.848874
MAE train: 0.615459	val: 0.718223	test: 0.679801

Epoch: 18
Loss: 0.7244349930967603
RMSE train: 0.757723	val: 0.892647	test: 0.844680
MAE train: 0.600667	val: 0.710028	test: 0.677629

Epoch: 19
Loss: 0.7288371281964439
RMSE train: 0.768891	val: 0.908757	test: 0.851968
MAE train: 0.614506	val: 0.720544	test: 0.683962

Epoch: 20
Loss: 0.7016229501792363
RMSE train: 0.749753	val: 0.889122	test: 0.843651
MAE train: 0.594721	val: 0.691541	test: 0.670321

Epoch: 21
Loss: 0.7145448284489768
RMSE train: 0.736521	val: 0.922747	test: 0.862593
MAE train: 0.580601	val: 0.720862	test: 0.694662Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/lipophilicity/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/lipophilicity/noise=0.05/lipophilicity_scaff_6_26-05_11-30-36  ]
[ Using Seed :  6  ]
[ Using device :  cuda:1  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 4.9870608534131735
RMSE train: 2.167038	val: 2.196122	test: 2.284042
MAE train: 1.922052	val: 1.928866	test: 2.042926

Epoch: 2
Loss: 3.2090831654412404
RMSE train: 1.983124	val: 1.872452	test: 1.944550
MAE train: 1.737675	val: 1.611856	test: 1.701888

Epoch: 3
Loss: 2.0218711410249983
RMSE train: 1.556374	val: 1.405204	test: 1.463510
MAE train: 1.333428	val: 1.179295	test: 1.228510

Epoch: 4
Loss: 1.4027195743152074
RMSE train: 1.186024	val: 1.114283	test: 1.111297
MAE train: 0.988888	val: 0.910723	test: 0.916497

Epoch: 5
Loss: 1.1281894871166773
RMSE train: 0.977480	val: 1.042969	test: 0.962620
MAE train: 0.794699	val: 0.821673	test: 0.771149

Epoch: 6
Loss: 1.0548682255404336
RMSE train: 0.930981	val: 1.050289	test: 0.950974
MAE train: 0.747099	val: 0.818239	test: 0.759948

Epoch: 7
Loss: 1.025331152336938
RMSE train: 0.920217	val: 1.119937	test: 1.010935
MAE train: 0.732994	val: 0.867797	test: 0.814242

Epoch: 8
Loss: 0.9649847362722669
RMSE train: 0.899483	val: 1.002460	test: 0.919704
MAE train: 0.720562	val: 0.779859	test: 0.732803

Epoch: 9
Loss: 0.9496908187866211
RMSE train: 0.885877	val: 1.032305	test: 0.931270
MAE train: 0.707370	val: 0.802357	test: 0.743902

Epoch: 10
Loss: 0.9053261790956769
RMSE train: 0.863564	val: 0.976454	test: 0.899415
MAE train: 0.692420	val: 0.756798	test: 0.712193

Epoch: 11
Loss: 0.8987923434802464
RMSE train: 0.850044	val: 1.013846	test: 0.912211
MAE train: 0.675098	val: 0.792931	test: 0.722747

Epoch: 12
Loss: 0.8552836733204978
RMSE train: 0.823493	val: 0.982529	test: 0.915335
MAE train: 0.655065	val: 0.765891	test: 0.735729

Epoch: 13
Loss: 0.8308503542627607
RMSE train: 0.825521	val: 0.953363	test: 0.893593
MAE train: 0.658239	val: 0.754424	test: 0.702574

Epoch: 14
Loss: 0.804052323102951
RMSE train: 0.794315	val: 0.983605	test: 0.916460
MAE train: 0.632825	val: 0.766466	test: 0.735958

Epoch: 15
Loss: 0.7656007536820003
RMSE train: 0.790486	val: 0.974073	test: 0.904624
MAE train: 0.631019	val: 0.768514	test: 0.724848

Epoch: 16
Loss: 0.7767890862056187
RMSE train: 0.788392	val: 0.919602	test: 0.866752
MAE train: 0.632268	val: 0.727403	test: 0.685128

Epoch: 17
Loss: 0.7860870829650334
RMSE train: 0.776984	val: 0.988986	test: 0.910842
MAE train: 0.619750	val: 0.773772	test: 0.736777

Epoch: 18
Loss: 0.7236333744866508
RMSE train: 0.775634	val: 0.935445	test: 0.880660
MAE train: 0.614637	val: 0.738915	test: 0.685662

Epoch: 19
Loss: 0.6969824135303497
RMSE train: 0.750222	val: 1.003958	test: 0.949966
MAE train: 0.596667	val: 0.795695	test: 0.774345

Epoch: 20
Loss: 0.7198314837047032
RMSE train: 0.746524	val: 0.912423	test: 0.865274
MAE train: 0.600128	val: 0.721085	test: 0.693896

Epoch: 21
Loss: 0.7169614221368518
RMSE train: 0.726250	val: 0.964864	test: 0.907209
MAE train: 0.578490	val: 0.759587	test: 0.732374Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:3
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/lipophilicity/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/lipophilicity/noise=0.2/lipophilicity_scaff_4_26-05_11-30-36  ]
[ Using Seed :  4  ]
[ Using device :  cuda:3  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.331981931413923
RMSE train: 2.200743	val: 2.185080	test: 2.302711
MAE train: 1.955537	val: 1.918893	test: 2.060746

Epoch: 2
Loss: 3.6225116763796126
RMSE train: 1.810037	val: 1.742435	test: 1.807063
MAE train: 1.571486	val: 1.496667	test: 1.569176

Epoch: 3
Loss: 2.314084904534476
RMSE train: 1.566961	val: 1.583968	test: 1.644107
MAE train: 1.341279	val: 1.344078	test: 1.412013

Epoch: 4
Loss: 1.6058365958077567
RMSE train: 1.214942	val: 1.268141	test: 1.231310
MAE train: 1.011917	val: 1.044499	test: 1.014534

Epoch: 5
Loss: 1.3778993487358093
RMSE train: 1.087330	val: 1.193769	test: 1.106820
MAE train: 0.886145	val: 0.950342	test: 0.880896

Epoch: 6
Loss: 1.2807637878826685
RMSE train: 1.062399	val: 1.164469	test: 1.093584
MAE train: 0.862911	val: 0.940731	test: 0.879173

Epoch: 7
Loss: 1.2435364893504552
RMSE train: 1.046186	val: 1.162283	test: 1.066350
MAE train: 0.833668	val: 0.912388	test: 0.844279

Epoch: 8
Loss: 1.194926095860345
RMSE train: 1.030345	val: 1.174042	test: 1.067605
MAE train: 0.822942	val: 0.928168	test: 0.853003

Epoch: 9
Loss: 1.1758348601205009
RMSE train: 0.995826	val: 1.173488	test: 1.038226
MAE train: 0.801574	val: 0.919354	test: 0.828093

Epoch: 10
Loss: 1.1606104629380363
RMSE train: 0.994098	val: 1.213327	test: 1.066117
MAE train: 0.799182	val: 0.948902	test: 0.855527

Epoch: 11
Loss: 1.0798467099666595
RMSE train: 0.972771	val: 1.138934	test: 1.042216
MAE train: 0.788796	val: 0.879454	test: 0.841826

Epoch: 12
Loss: 1.062080762216023
RMSE train: 0.966122	val: 1.106786	test: 1.090030
MAE train: 0.786918	val: 0.893284	test: 0.869546

Epoch: 13
Loss: 1.0830401011875697
RMSE train: 0.955937	val: 1.170533	test: 1.100646
MAE train: 0.765208	val: 0.906431	test: 0.889851

Epoch: 14
Loss: 1.0239018159253257
RMSE train: 0.919242	val: 1.133691	test: 1.112728
MAE train: 0.738642	val: 0.893010	test: 0.880782

Epoch: 15
Loss: 1.002465844154358
RMSE train: 0.921842	val: 1.089495	test: 1.065255
MAE train: 0.747309	val: 0.855519	test: 0.846763

Epoch: 16
Loss: 0.9448453954287938
RMSE train: 0.888592	val: 1.104101	test: 1.090867
MAE train: 0.717878	val: 0.865414	test: 0.865212

Epoch: 17
Loss: 0.9401172527245113
RMSE train: 0.874430	val: 1.107045	test: 1.110992
MAE train: 0.702552	val: 0.864898	test: 0.884081

Epoch: 18
Loss: 0.9429880806377956
RMSE train: 0.863323	val: 1.065695	test: 1.041782
MAE train: 0.696128	val: 0.837939	test: 0.832493

Epoch: 19
Loss: 0.9241753092833928
RMSE train: 0.856863	val: 1.103427	test: 1.066629
MAE train: 0.692075	val: 0.867367	test: 0.849171

Epoch: 20
Loss: 0.9181760081223079
RMSE train: 0.862472	val: 1.129112	test: 1.133625
MAE train: 0.696432	val: 0.893532	test: 0.892396

Epoch: 21
Loss: 0.9042784571647644
RMSE train: 0.835884	val: 1.060411	test: 1.025614
MAE train: 0.675871	val: 0.828662	test: 0.822630Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:3
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/lipophilicity/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/lipophilicity/noise=0.2/lipophilicity_scaff_6_26-05_11-30-36  ]
[ Using Seed :  6  ]
[ Using device :  cuda:3  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.02743319102696
RMSE train: 2.141593	val: 2.197308	test: 2.279592
MAE train: 1.895878	val: 1.923929	test: 2.037540

Epoch: 2
Loss: 3.3134334087371826
RMSE train: 1.903197	val: 1.852505	test: 1.893731
MAE train: 1.660909	val: 1.582849	test: 1.642686

Epoch: 3
Loss: 2.157350948878697
RMSE train: 1.622103	val: 1.535006	test: 1.565434
MAE train: 1.390459	val: 1.291095	test: 1.314507

Epoch: 4
Loss: 1.5451898319380624
RMSE train: 1.324568	val: 1.289923	test: 1.271005
MAE train: 1.114594	val: 1.063209	test: 1.047114

Epoch: 5
Loss: 1.3488951240267073
RMSE train: 1.104312	val: 1.185589	test: 1.087702
MAE train: 0.912641	val: 0.939159	test: 0.873152

Epoch: 6
Loss: 1.2523798431668962
RMSE train: 1.056964	val: 1.288432	test: 1.145056
MAE train: 0.855553	val: 1.009532	test: 0.920133

Epoch: 7
Loss: 1.1950752011367254
RMSE train: 1.021595	val: 1.356964	test: 1.269682
MAE train: 0.827085	val: 1.065758	test: 1.041288

Epoch: 8
Loss: 1.1435017415455409
RMSE train: 1.025753	val: 1.093759	test: 1.010058
MAE train: 0.842755	val: 0.864342	test: 0.807908

Epoch: 9
Loss: 1.1383264745984758
RMSE train: 0.986581	val: 1.230374	test: 1.140693
MAE train: 0.800284	val: 0.963131	test: 0.928477

Epoch: 10
Loss: 1.082626027720315
RMSE train: 0.976198	val: 1.181544	test: 1.076460
MAE train: 0.795198	val: 0.941029	test: 0.874179

Epoch: 11
Loss: 1.0817891614777702
RMSE train: 0.957370	val: 1.222522	test: 1.111685
MAE train: 0.775798	val: 0.962016	test: 0.897645

Epoch: 12
Loss: 1.0362638958862849
RMSE train: 0.943467	val: 1.172279	test: 1.065126
MAE train: 0.769285	val: 0.921550	test: 0.862320

Epoch: 13
Loss: 0.9883782821042197
RMSE train: 0.912324	val: 1.240377	test: 1.135009
MAE train: 0.733173	val: 0.970266	test: 0.913723

Epoch: 14
Loss: 0.9989383561270577
RMSE train: 0.894082	val: 1.220670	test: 1.116542
MAE train: 0.712251	val: 0.953923	test: 0.899553

Epoch: 15
Loss: 0.9321477242878505
RMSE train: 0.874709	val: 1.151793	test: 1.082703
MAE train: 0.702424	val: 0.900849	test: 0.879530

Epoch: 16
Loss: 0.9238870101315635
RMSE train: 0.862076	val: 1.083661	test: 1.007576
MAE train: 0.692597	val: 0.857125	test: 0.802643

Epoch: 17
Loss: 0.924942889383861
RMSE train: 0.864697	val: 1.110057	test: 1.036010
MAE train: 0.696594	val: 0.879906	test: 0.848657

Epoch: 18
Loss: 0.9076461451394218
RMSE train: 0.864427	val: 1.063920	test: 1.018405
MAE train: 0.700320	val: 0.854851	test: 0.815366

Epoch: 19
Loss: 0.8381192599024091
RMSE train: 0.840035	val: 1.145845	test: 1.059761
MAE train: 0.676232	val: 0.908917	test: 0.850411

Epoch: 20
Loss: 0.877032390662602
RMSE train: 0.830501	val: 1.033843	test: 0.981124
MAE train: 0.675168	val: 0.836938	test: 0.784808

Epoch: 21
Loss: 0.8471199444362095
RMSE train: 0.793556	val: 1.068697	test: 1.013954
MAE train: 0.635665	val: 0.855023	test: 0.807313Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:3
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/lipophilicity/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/lipophilicity/noise=0.2/lipophilicity_scaff_5_26-05_11-30-36  ]
[ Using Seed :  5  ]
[ Using device :  cuda:3  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.239903943879264
RMSE train: 2.215176	val: 2.227984	test: 2.337980
MAE train: 1.968928	val: 1.953522	test: 2.098732

Epoch: 2
Loss: 3.58344909123012
RMSE train: 1.981673	val: 1.914327	test: 2.003176
MAE train: 1.735593	val: 1.650770	test: 1.758903

Epoch: 3
Loss: 2.368640516485487
RMSE train: 1.466949	val: 1.415884	test: 1.447220
MAE train: 1.248280	val: 1.198890	test: 1.213510

Epoch: 4
Loss: 1.6240859201976232
RMSE train: 1.226627	val: 1.283705	test: 1.300402
MAE train: 1.022534	val: 1.073347	test: 1.084154

Epoch: 5
Loss: 1.3039718185152327
RMSE train: 1.083648	val: 1.156793	test: 1.086642
MAE train: 0.885959	val: 0.932065	test: 0.890070

Epoch: 6
Loss: 1.209577637059348
RMSE train: 1.049087	val: 1.170312	test: 1.083768
MAE train: 0.840687	val: 0.924450	test: 0.862044

Epoch: 7
Loss: 1.2320109946387154
RMSE train: 1.028906	val: 1.164473	test: 1.069804
MAE train: 0.828119	val: 0.918895	test: 0.857247

Epoch: 8
Loss: 1.1380014249256678
RMSE train: 1.012294	val: 1.125772	test: 1.042555
MAE train: 0.818848	val: 0.891810	test: 0.841616

Epoch: 9
Loss: 1.1649216668946403
RMSE train: 1.002997	val: 1.190665	test: 1.095145
MAE train: 0.807053	val: 0.937334	test: 0.888552

Epoch: 10
Loss: 1.144317205463137
RMSE train: 0.984958	val: 1.114514	test: 1.069939
MAE train: 0.796364	val: 0.901043	test: 0.866002

Epoch: 11
Loss: 1.0635598131588526
RMSE train: 0.969558	val: 1.092298	test: 1.072370
MAE train: 0.778044	val: 0.883147	test: 0.861648

Epoch: 12
Loss: 1.0158966183662415
RMSE train: 0.970348	val: 1.116851	test: 1.033989
MAE train: 0.772371	val: 0.871478	test: 0.833504

Epoch: 13
Loss: 1.0059667783124107
RMSE train: 0.924712	val: 1.070948	test: 1.012943
MAE train: 0.739751	val: 0.848608	test: 0.819429

Epoch: 14
Loss: 0.9923506932599204
RMSE train: 0.929336	val: 1.089326	test: 1.013413
MAE train: 0.739163	val: 0.850699	test: 0.822967

Epoch: 15
Loss: 0.9686943505491529
RMSE train: 0.899522	val: 1.065975	test: 1.032961
MAE train: 0.727262	val: 0.845582	test: 0.831197

Epoch: 16
Loss: 0.9254250781876701
RMSE train: 0.904811	val: 1.065927	test: 1.031532
MAE train: 0.716904	val: 0.850112	test: 0.824104

Epoch: 17
Loss: 0.9285822681018284
RMSE train: 0.860925	val: 1.068729	test: 1.020326
MAE train: 0.691807	val: 0.840419	test: 0.828973

Epoch: 18
Loss: 0.8617317165647235
RMSE train: 0.841454	val: 1.047925	test: 1.035398
MAE train: 0.675744	val: 0.840456	test: 0.829293

Epoch: 19
Loss: 0.868237384728023
RMSE train: 0.882518	val: 1.036007	test: 0.991419
MAE train: 0.696875	val: 0.837456	test: 0.809522

Epoch: 20
Loss: 0.8453812428883144
RMSE train: 0.833896	val: 1.064977	test: 1.030838
MAE train: 0.662196	val: 0.847095	test: 0.833449

Epoch: 21
Loss: 0.7750332994120461
RMSE train: 0.797336	val: 1.032069	test: 1.032610
MAE train: 0.638645	val: 0.829330	test: 0.826215

Epoch: 22
Loss: 0.6859606845038277
RMSE train: 0.749748	val: 0.901778	test: 0.862995
MAE train: 0.599449	val: 0.722221	test: 0.690177

Epoch: 23
Loss: 0.6454152507441384
RMSE train: 0.737746	val: 0.893418	test: 0.864747
MAE train: 0.587659	val: 0.707021	test: 0.700734

Epoch: 24
Loss: 0.656414121389389
RMSE train: 0.731814	val: 0.888332	test: 0.835997
MAE train: 0.577572	val: 0.703796	test: 0.661717

Epoch: 25
Loss: 0.637205353804997
RMSE train: 0.692165	val: 0.894289	test: 0.838367
MAE train: 0.545938	val: 0.698999	test: 0.660113

Epoch: 26
Loss: 0.6294225454330444
RMSE train: 0.711473	val: 0.925960	test: 0.860112
MAE train: 0.556889	val: 0.725919	test: 0.688550

Epoch: 27
Loss: 0.5890403177056994
RMSE train: 0.683883	val: 0.893228	test: 0.821918
MAE train: 0.537978	val: 0.699106	test: 0.662787

Epoch: 28
Loss: 0.6114679064069476
RMSE train: 0.706655	val: 0.924287	test: 0.856271
MAE train: 0.554079	val: 0.726507	test: 0.688683

Epoch: 29
Loss: 0.5608814571584974
RMSE train: 0.645069	val: 0.881835	test: 0.838663
MAE train: 0.507777	val: 0.695532	test: 0.664119

Epoch: 30
Loss: 0.5631685342107501
RMSE train: 0.692298	val: 0.903461	test: 0.836580
MAE train: 0.546426	val: 0.710441	test: 0.668064

Epoch: 31
Loss: 0.5669486118214471
RMSE train: 0.645584	val: 0.891830	test: 0.835552
MAE train: 0.510920	val: 0.698255	test: 0.656084

Epoch: 32
Loss: 0.5323889532259533
RMSE train: 0.658524	val: 0.894659	test: 0.841399
MAE train: 0.517177	val: 0.706349	test: 0.677811

Epoch: 33
Loss: 0.5328303320067269
RMSE train: 0.642868	val: 0.890708	test: 0.842119
MAE train: 0.506912	val: 0.702434	test: 0.673971

Epoch: 34
Loss: 0.5341386049985886
RMSE train: 0.626323	val: 0.894816	test: 0.856491
MAE train: 0.495053	val: 0.712101	test: 0.680420

Epoch: 35
Loss: 0.5117651181561607
RMSE train: 0.612412	val: 0.871466	test: 0.825961
MAE train: 0.482888	val: 0.686722	test: 0.657291

Epoch: 36
Loss: 0.4982020684650966
RMSE train: 0.611951	val: 0.894881	test: 0.836783
MAE train: 0.481006	val: 0.702556	test: 0.660062

Epoch: 37
Loss: 0.5063614398241043
RMSE train: 0.592381	val: 0.873974	test: 0.832537
MAE train: 0.469190	val: 0.689812	test: 0.658385

Epoch: 38
Loss: 0.47998746590954916
RMSE train: 0.613637	val: 0.906445	test: 0.845542
MAE train: 0.483022	val: 0.716028	test: 0.667444

Epoch: 39
Loss: 0.48943953428949627
RMSE train: 0.642850	val: 0.933379	test: 0.881427
MAE train: 0.504779	val: 0.727759	test: 0.701399

Epoch: 40
Loss: 0.4630857471908842
RMSE train: 0.589812	val: 0.886953	test: 0.846684
MAE train: 0.464285	val: 0.693673	test: 0.672194

Epoch: 41
Loss: 0.5051746836730412
RMSE train: 0.594521	val: 0.875555	test: 0.858931
MAE train: 0.473278	val: 0.685664	test: 0.680696

Epoch: 42
Loss: 0.46344949305057526
RMSE train: 0.599038	val: 0.910258	test: 0.864506
MAE train: 0.472850	val: 0.711752	test: 0.683037

Epoch: 43
Loss: 0.45814903931958334
RMSE train: 0.557801	val: 0.879992	test: 0.845467
MAE train: 0.440347	val: 0.689669	test: 0.667964

Epoch: 44
Loss: 0.44566153415611814
RMSE train: 0.574311	val: 0.877897	test: 0.848957
MAE train: 0.452300	val: 0.681236	test: 0.677809

Epoch: 45
Loss: 0.42537050374916624
RMSE train: 0.563142	val: 0.884935	test: 0.847474
MAE train: 0.442306	val: 0.697057	test: 0.672738

Epoch: 46
Loss: 0.4251940143959863
RMSE train: 0.559573	val: 0.879622	test: 0.843821
MAE train: 0.441629	val: 0.687218	test: 0.670396

Epoch: 47
Loss: 0.4290124220507486
RMSE train: 0.554388	val: 0.891012	test: 0.870092
MAE train: 0.442510	val: 0.696851	test: 0.684929

Epoch: 48
Loss: 0.3995771280356816
RMSE train: 0.558225	val: 0.901755	test: 0.897924
MAE train: 0.444812	val: 0.706767	test: 0.709963

Epoch: 49
Loss: 0.39953622009072987
RMSE train: 0.543566	val: 0.885194	test: 0.843538
MAE train: 0.430132	val: 0.700953	test: 0.670441

Epoch: 50
Loss: 0.3917275773627417
RMSE train: 0.523432	val: 0.899116	test: 0.873271
MAE train: 0.414615	val: 0.706392	test: 0.687581

Epoch: 51
Loss: 0.3813953101634979
RMSE train: 0.506425	val: 0.894039	test: 0.862486
MAE train: 0.400835	val: 0.704375	test: 0.680302

Epoch: 52
Loss: 0.36942695719855173
RMSE train: 0.520539	val: 0.889843	test: 0.843795
MAE train: 0.411950	val: 0.702560	test: 0.668027

Epoch: 53
Loss: 0.3645173077072416
RMSE train: 0.493412	val: 0.892241	test: 0.853838
MAE train: 0.388307	val: 0.692056	test: 0.679188

Epoch: 54
Loss: 0.3668419952903475
RMSE train: 0.579200	val: 0.932947	test: 0.930468
MAE train: 0.465294	val: 0.733899	test: 0.736607

Epoch: 55
Loss: 0.3756904197590692
RMSE train: 0.487272	val: 0.878204	test: 0.863563
MAE train: 0.383761	val: 0.683911	test: 0.684732

Epoch: 56
Loss: 0.3560915972505297
RMSE train: 0.510674	val: 0.873909	test: 0.848964
MAE train: 0.405556	val: 0.687920	test: 0.677007

Epoch: 57
Loss: 0.36061896809509825
RMSE train: 0.472497	val: 0.899650	test: 0.889871
MAE train: 0.372784	val: 0.707134	test: 0.701592

Epoch: 58
Loss: 0.33887994289398193
RMSE train: 0.480104	val: 0.865901	test: 0.845487
MAE train: 0.379073	val: 0.673947	test: 0.667230

Epoch: 59
Loss: 0.3339856820447104
RMSE train: 0.473727	val: 0.879116	test: 0.858350
MAE train: 0.376177	val: 0.688546	test: 0.678983

Epoch: 60
Loss: 0.3239658496209553
RMSE train: 0.458713	val: 0.877991	test: 0.847071
MAE train: 0.362934	val: 0.681631	test: 0.671076

Epoch: 61
Loss: 0.3398274672882898
RMSE train: 0.468866	val: 0.892019	test: 0.854074
MAE train: 0.369589	val: 0.703627	test: 0.679315

Epoch: 62
Loss: 0.31831786249365124
RMSE train: 0.452844	val: 0.875147	test: 0.844727
MAE train: 0.357061	val: 0.685775	test: 0.673832

Epoch: 63
Loss: 0.3190160649163382
RMSE train: 0.438012	val: 0.869752	test: 0.848679
MAE train: 0.345243	val: 0.685426	test: 0.672589

Epoch: 64
Loss: 0.30670888083321707
RMSE train: 0.452910	val: 0.875500	test: 0.853843
MAE train: 0.356951	val: 0.680825	test: 0.672530

Epoch: 65
Loss: 0.30017538368701935
RMSE train: 0.436618	val: 0.884021	test: 0.842412
MAE train: 0.344705	val: 0.691789	test: 0.669885

Epoch: 66
Loss: 0.2888315234865461
RMSE train: 0.483917	val: 0.918902	test: 0.888196
MAE train: 0.385864	val: 0.720804	test: 0.710960

Epoch: 67
Loss: 0.3049737236329487
RMSE train: 0.436761	val: 0.897002	test: 0.851962
MAE train: 0.345230	val: 0.705523	test: 0.679955

Epoch: 68
Loss: 0.287441543170384
RMSE train: 0.435332	val: 0.877103	test: 0.831615
MAE train: 0.343292	val: 0.682991	test: 0.662599

Epoch: 69
Loss: 0.27962621620723177
RMSE train: 0.443672	val: 0.894501	test: 0.875882
MAE train: 0.350868	val: 0.707173	test: 0.700563

Epoch: 70
Loss: 0.28736765682697296
RMSE train: 0.415852	val: 0.874179	test: 0.831544
MAE train: 0.327453	val: 0.680585	test: 0.667848

Epoch: 71
Loss: 0.27141690786395756
RMSE train: 0.419293	val: 0.885139	test: 0.853094
MAE train: 0.330978	val: 0.696430	test: 0.678518

Epoch: 72
Loss: 0.27549186029604505
RMSE train: 0.434202	val: 0.880128	test: 0.841531
MAE train: 0.344010	val: 0.691455	test: 0.670222

Epoch: 73
Loss: 0.27517640058483395
RMSE train: 0.441308	val: 0.897551	test: 0.866720
MAE train: 0.347772	val: 0.707998	test: 0.687877

Epoch: 74
Loss: 0.2944605435643877
RMSE train: 0.442584	val: 0.891727	test: 0.856608
MAE train: 0.350675	val: 0.699788	test: 0.677457

Epoch: 75
Loss: 0.2743014938064984
RMSE train: 0.480922	val: 0.890739	test: 0.884681
MAE train: 0.384215	val: 0.699785	test: 0.705710

Epoch: 76
Loss: 0.2608517738325255
RMSE train: 0.447233	val: 0.890602	test: 0.874657
MAE train: 0.352285	val: 0.701734	test: 0.696619

Epoch: 77
Loss: 0.2570429550749915
RMSE train: 0.449429	val: 0.891829	test: 0.846355
MAE train: 0.357453	val: 0.701503	test: 0.678857

Epoch: 78
Loss: 0.256365210882255
RMSE train: 0.440455	val: 0.896634	test: 0.847175
MAE train: 0.348022	val: 0.697801	test: 0.680671

Epoch: 79
Loss: 0.2516865762216704
RMSE train: 0.386713	val: 0.880117	test: 0.849754
MAE train: 0.303782	val: 0.691815	test: 0.670370

Epoch: 80
Loss: 0.2721632442304066
RMSE train: 0.395543	val: 0.870585	test: 0.846514
MAE train: 0.311235	val: 0.682570	test: 0.672849

Epoch: 81
Loss: 0.25002849102020264
RMSE train: 0.442400	val: 0.897237	test: 0.853943
MAE train: 0.350459	val: 0.703710	test: 0.680814

Epoch: 82
Loss: 0.23295326850243978
RMSE train: 0.415188	val: 0.888495	test: 0.861372

Epoch: 23
Loss: 0.5622921607324055
RMSE train: 0.703980	val: 0.873820	test: 0.798275
MAE train: 0.547727	val: 0.680000	test: 0.646872

Epoch: 24
Loss: 0.5627656536442893
RMSE train: 0.686741	val: 0.844646	test: 0.794135
MAE train: 0.535726	val: 0.651235	test: 0.646406

Epoch: 25
Loss: 0.557562706725938
RMSE train: 0.684524	val: 0.875983	test: 0.805566
MAE train: 0.532278	val: 0.675769	test: 0.651661

Epoch: 26
Loss: 0.5282589750630515
RMSE train: 0.660967	val: 0.856886	test: 0.806459
MAE train: 0.513729	val: 0.655136	test: 0.651847

Epoch: 27
Loss: 0.5341733545064926
RMSE train: 0.688304	val: 0.887518	test: 0.830947
MAE train: 0.537241	val: 0.692072	test: 0.669599

Epoch: 28
Loss: 0.5474386300359454
RMSE train: 0.652455	val: 0.836103	test: 0.793484
MAE train: 0.511914	val: 0.644310	test: 0.640952

Epoch: 29
Loss: 0.5049862052713122
RMSE train: 0.658787	val: 0.852947	test: 0.804946
MAE train: 0.512835	val: 0.660077	test: 0.649350

Epoch: 30
Loss: 0.5252213988985334
RMSE train: 0.644407	val: 0.841915	test: 0.790521
MAE train: 0.500466	val: 0.650670	test: 0.636306

Epoch: 31
Loss: 0.5087837214980807
RMSE train: 0.669566	val: 0.859476	test: 0.801191
MAE train: 0.523490	val: 0.664983	test: 0.644344

Epoch: 32
Loss: 0.4880362025329045
RMSE train: 0.646172	val: 0.811420	test: 0.771960
MAE train: 0.501602	val: 0.623222	test: 0.624493

Epoch: 33
Loss: 0.5148911348411015
RMSE train: 0.692164	val: 0.904133	test: 0.827948
MAE train: 0.538668	val: 0.692011	test: 0.671156

Epoch: 34
Loss: 0.5047357380390167
RMSE train: 0.645512	val: 0.858963	test: 0.800414
MAE train: 0.504040	val: 0.651330	test: 0.645837

Epoch: 35
Loss: 0.48206374049186707
RMSE train: 0.677135	val: 0.889873	test: 0.824069
MAE train: 0.528458	val: 0.686598	test: 0.657954

Epoch: 36
Loss: 0.5014895647764206
RMSE train: 0.637704	val: 0.835187	test: 0.792672
MAE train: 0.498570	val: 0.637937	test: 0.642356

Epoch: 37
Loss: 0.47653513082436155
RMSE train: 0.676457	val: 0.871108	test: 0.800757
MAE train: 0.528583	val: 0.668570	test: 0.637382

Epoch: 38
Loss: 0.4899500480719975
RMSE train: 0.616901	val: 0.819640	test: 0.777189
MAE train: 0.478661	val: 0.628608	test: 0.618294

Epoch: 39
Loss: 0.4490032323769161
RMSE train: 0.631017	val: 0.827325	test: 0.778730
MAE train: 0.490852	val: 0.628862	test: 0.627383

Epoch: 40
Loss: 0.49413007284913746
RMSE train: 0.635328	val: 0.856596	test: 0.802341
MAE train: 0.494171	val: 0.654636	test: 0.634724

Epoch: 41
Loss: 0.4868492845978056
RMSE train: 0.641318	val: 0.827952	test: 0.783725
MAE train: 0.502678	val: 0.636581	test: 0.622464

Epoch: 42
Loss: 0.4673888513020107
RMSE train: 0.619846	val: 0.843944	test: 0.799546
MAE train: 0.482547	val: 0.642153	test: 0.632674

Epoch: 43
Loss: 0.4554308844464166
RMSE train: 0.630195	val: 0.844467	test: 0.789764
MAE train: 0.490606	val: 0.641647	test: 0.642392

Epoch: 44
Loss: 0.43538350079740795
RMSE train: 0.611641	val: 0.827190	test: 0.779324
MAE train: 0.475812	val: 0.628272	test: 0.627874

Epoch: 45
Loss: 0.46776420303753447
RMSE train: 0.602063	val: 0.829844	test: 0.780160
MAE train: 0.466770	val: 0.625534	test: 0.624412

Epoch: 46
Loss: 0.4485442468098232
RMSE train: 0.603043	val: 0.823672	test: 0.790493
MAE train: 0.468317	val: 0.627713	test: 0.633849

Epoch: 47
Loss: 0.44432403998715536
RMSE train: 0.618922	val: 0.821657	test: 0.789081
MAE train: 0.482660	val: 0.623132	test: 0.635501

Epoch: 48
Loss: 0.4255141147545406
RMSE train: 0.629733	val: 0.841357	test: 0.797596
MAE train: 0.491078	val: 0.639274	test: 0.638351

Epoch: 49
Loss: 0.4252113721200398
RMSE train: 0.610235	val: 0.820998	test: 0.788503
MAE train: 0.475953	val: 0.627130	test: 0.639248

Epoch: 50
Loss: 0.45405248020376476
RMSE train: 0.601173	val: 0.825815	test: 0.788249
MAE train: 0.469229	val: 0.628035	test: 0.623261

Epoch: 51
Loss: 0.4358048864773342
RMSE train: 0.617155	val: 0.822283	test: 0.787798
MAE train: 0.480756	val: 0.621549	test: 0.628238

Epoch: 52
Loss: 0.41782107736383167
RMSE train: 0.629068	val: 0.835865	test: 0.815473
MAE train: 0.488513	val: 0.639727	test: 0.657988

Epoch: 53
Loss: 0.4242233612707683
RMSE train: 0.582701	val: 0.799846	test: 0.781960
MAE train: 0.454049	val: 0.600226	test: 0.623136

Epoch: 54
Loss: 0.417560481599399
RMSE train: 0.620271	val: 0.860480	test: 0.801103
MAE train: 0.482498	val: 0.655082	test: 0.634609

Epoch: 55
Loss: 0.4259800123316901
RMSE train: 0.601971	val: 0.843519	test: 0.793475
MAE train: 0.472477	val: 0.641101	test: 0.635743

Epoch: 56
Loss: 0.40372127294540405
RMSE train: 0.607026	val: 0.851502	test: 0.801385
MAE train: 0.473249	val: 0.647978	test: 0.635743

Epoch: 57
Loss: 0.3952484556606838
RMSE train: 0.591661	val: 0.803596	test: 0.774730
MAE train: 0.460608	val: 0.605436	test: 0.619925

Epoch: 58
Loss: 0.409710630774498
RMSE train: 0.588570	val: 0.817323	test: 0.778309
MAE train: 0.457817	val: 0.616379	test: 0.626286

Epoch: 59
Loss: 0.4038710870913097
RMSE train: 0.635021	val: 0.855311	test: 0.805408
MAE train: 0.493925	val: 0.647598	test: 0.647218

Epoch: 60
Loss: 0.38276805196489605
RMSE train: 0.589379	val: 0.849106	test: 0.789329
MAE train: 0.458173	val: 0.645555	test: 0.623299

Epoch: 61
Loss: 0.3861113488674164
RMSE train: 0.554602	val: 0.782865	test: 0.754156
MAE train: 0.431136	val: 0.595725	test: 0.596090

Epoch: 62
Loss: 0.39859179300921305
RMSE train: 0.583484	val: 0.813903	test: 0.775055
MAE train: 0.453969	val: 0.618542	test: 0.620707

Epoch: 63
Loss: 0.38995975468839916
RMSE train: 0.582946	val: 0.826525	test: 0.782867
MAE train: 0.454474	val: 0.623415	test: 0.625835

Epoch: 64
Loss: 0.37830168860299246
RMSE train: 0.570938	val: 0.824681	test: 0.773450
MAE train: 0.445869	val: 0.630972	test: 0.612038

Epoch: 65
Loss: 0.38522520661354065
RMSE train: 0.589172	val: 0.833802	test: 0.786354
MAE train: 0.459701	val: 0.630197	test: 0.629707

Epoch: 66
Loss: 0.37774626910686493
RMSE train: 0.552930	val: 0.803826	test: 0.757825
MAE train: 0.430692	val: 0.605558	test: 0.600468

Epoch: 67
Loss: 0.3809826650789806
RMSE train: 0.563616	val: 0.818063	test: 0.771615
MAE train: 0.437931	val: 0.617590	test: 0.617734

Epoch: 68
Loss: 0.3684125265904835
RMSE train: 0.551223	val: 0.802101	test: 0.769167
MAE train: 0.428788	val: 0.607202	test: 0.600769

Epoch: 69
Loss: 0.36058556182043894
RMSE train: 0.552957	val: 0.813561	test: 0.767758
MAE train: 0.429345	val: 0.612657	test: 0.606068

Epoch: 70
Loss: 0.34624512067862917
RMSE train: 0.548292	val: 0.803003	test: 0.760197
MAE train: 0.425472	val: 0.610823	test: 0.605669

Epoch: 71
Loss: 0.34191739346299854
RMSE train: 0.541860	val: 0.804663	test: 0.757784
MAE train: 0.422395	val: 0.610600	test: 0.601572

Epoch: 72
Loss: 0.34961160378796713
RMSE train: 0.573106	val: 0.827558	test: 0.763153
MAE train: 0.446129	val: 0.623827	test: 0.602522

Epoch: 73
Loss: 0.36158211529254913
RMSE train: 0.544407	val: 0.806210	test: 0.761951
MAE train: 0.421339	val: 0.609163	test: 0.604433

Epoch: 74
Loss: 0.36096616089344025
RMSE train: 0.585688	val: 0.836533	test: 0.783630
MAE train: 0.458145	val: 0.631054	test: 0.624312

Epoch: 75
Loss: 0.3477903051035745
RMSE train: 0.557393	val: 0.780492	test: 0.769452
MAE train: 0.433914	val: 0.596092	test: 0.610979

Epoch: 76
Loss: 0.34353975100176676
RMSE train: 0.581192	val: 0.832922	test: 0.801448
MAE train: 0.453116	val: 0.635373	test: 0.636151

Epoch: 77
Loss: 0.3400261721440724
RMSE train: 0.542761	val: 0.815616	test: 0.773978
MAE train: 0.422760	val: 0.617456	test: 0.608262

Epoch: 78
Loss: 0.34510509456907
RMSE train: 0.558326	val: 0.830239	test: 0.771782
MAE train: 0.435671	val: 0.626577	test: 0.609518

Epoch: 79
Loss: 0.3237663762910025
RMSE train: 0.533336	val: 0.793092	test: 0.750435
MAE train: 0.416802	val: 0.599521	test: 0.590090

Epoch: 80
Loss: 0.33219051361083984
RMSE train: 0.518636	val: 0.799522	test: 0.759591
MAE train: 0.401739	val: 0.603147	test: 0.601779

Epoch: 81
Loss: 0.3311969212123326
RMSE train: 0.552611	val: 0.827685	test: 0.770443
MAE train: 0.431230	val: 0.623587	test: 0.611358

Epoch: 82
Loss: 0.3503682017326355
RMSE train: 0.535966	val: 0.804232	test: 0.765142
MAE train: 0.414947	val: 0.609318	test: 0.598552

Epoch: 83
Loss: 0.33796414945806774
RMSE train: 0.542104	val: 0.801096	test: 0.755918

Epoch: 22
Loss: 0.7367907464504242
RMSE train: 0.757183	val: 0.974690	test: 0.911505
MAE train: 0.602075	val: 0.775098	test: 0.722579

Epoch: 23
Loss: 0.7402470111846924
RMSE train: 0.754486	val: 0.998620	test: 0.913792
MAE train: 0.598948	val: 0.795751	test: 0.729359

Epoch: 24
Loss: 0.6957406784806933
RMSE train: 0.744555	val: 0.976551	test: 0.894597
MAE train: 0.591981	val: 0.768198	test: 0.709181

Epoch: 25
Loss: 0.674551248550415
RMSE train: 0.716582	val: 0.953676	test: 0.896537
MAE train: 0.571704	val: 0.764320	test: 0.707068

Epoch: 26
Loss: 0.6669177072388786
RMSE train: 0.733415	val: 0.990500	test: 0.902933
MAE train: 0.584004	val: 0.782895	test: 0.715974

Epoch: 27
Loss: 0.6279541552066803
RMSE train: 0.697156	val: 0.955379	test: 0.881647
MAE train: 0.554490	val: 0.762598	test: 0.698048

Epoch: 28
Loss: 0.6304171085357666
RMSE train: 0.698314	val: 0.959382	test: 0.894863
MAE train: 0.556889	val: 0.757818	test: 0.714168

Epoch: 29
Loss: 0.6230935496943337
RMSE train: 0.702979	val: 0.954243	test: 0.914185
MAE train: 0.561477	val: 0.757478	test: 0.720543

Epoch: 30
Loss: 0.6396059202296394
RMSE train: 0.680834	val: 0.967344	test: 0.909954
MAE train: 0.542897	val: 0.768125	test: 0.724308

Epoch: 31
Loss: 0.5979332072394234
RMSE train: 0.662411	val: 0.960213	test: 0.884352
MAE train: 0.525995	val: 0.759027	test: 0.699570

Epoch: 32
Loss: 0.5830902542386737
RMSE train: 0.656445	val: 0.964890	test: 0.889703
MAE train: 0.521127	val: 0.767283	test: 0.708000

Epoch: 33
Loss: 0.5625878998211452
RMSE train: 0.680736	val: 0.957103	test: 0.916546
MAE train: 0.547867	val: 0.763031	test: 0.723346

Epoch: 34
Loss: 0.5426077502114433
RMSE train: 0.649875	val: 0.944781	test: 0.872110
MAE train: 0.518985	val: 0.752945	test: 0.694783

Epoch: 35
Loss: 0.5454159038407462
RMSE train: 0.637797	val: 0.952106	test: 0.899778
MAE train: 0.508342	val: 0.751424	test: 0.713391

Epoch: 36
Loss: 0.5267126858234406
RMSE train: 0.621462	val: 0.971075	test: 0.938368
MAE train: 0.494788	val: 0.775534	test: 0.741687

Epoch: 37
Loss: 0.5286493045943124
RMSE train: 0.634905	val: 0.944362	test: 0.865348
MAE train: 0.505179	val: 0.750119	test: 0.687271

Epoch: 38
Loss: 0.5203506542103631
RMSE train: 0.586271	val: 0.928087	test: 0.867810
MAE train: 0.467966	val: 0.736579	test: 0.682514

Epoch: 39
Loss: 0.49493327736854553
RMSE train: 0.612107	val: 0.954049	test: 0.881767
MAE train: 0.487197	val: 0.754686	test: 0.702190

Epoch: 40
Loss: 0.4888225942850113
RMSE train: 0.586026	val: 0.950520	test: 0.885189
MAE train: 0.464892	val: 0.754258	test: 0.706553

Epoch: 41
Loss: 0.5010532907077244
RMSE train: 0.579496	val: 0.920971	test: 0.866183
MAE train: 0.460134	val: 0.734701	test: 0.689213

Epoch: 42
Loss: 0.47158279376370565
RMSE train: 0.575073	val: 0.965979	test: 0.918955
MAE train: 0.458723	val: 0.779408	test: 0.722626

Epoch: 43
Loss: 0.45909132063388824
RMSE train: 0.580327	val: 0.930290	test: 0.867296
MAE train: 0.462314	val: 0.735561	test: 0.689665

Epoch: 44
Loss: 0.4407814932721002
RMSE train: 0.548963	val: 0.950679	test: 0.902182
MAE train: 0.436680	val: 0.754782	test: 0.716810

Epoch: 45
Loss: 0.43508969247341156
RMSE train: 0.576435	val: 0.927928	test: 0.858224
MAE train: 0.460468	val: 0.739022	test: 0.680021

Epoch: 46
Loss: 0.4316736842904772
RMSE train: 0.515924	val: 0.926918	test: 0.870631
MAE train: 0.410705	val: 0.731315	test: 0.686216

Epoch: 47
Loss: 0.40651234132902964
RMSE train: 0.550714	val: 0.965591	test: 0.902669
MAE train: 0.439925	val: 0.768718	test: 0.715597

Epoch: 48
Loss: 0.41355098358222414
RMSE train: 0.577573	val: 0.948823	test: 0.871534
MAE train: 0.456559	val: 0.738903	test: 0.695598

Epoch: 49
Loss: 0.3827183651072638
RMSE train: 0.496443	val: 0.948744	test: 0.893177
MAE train: 0.393384	val: 0.757427	test: 0.707002

Epoch: 50
Loss: 0.38360832205840517
RMSE train: 0.521345	val: 0.957404	test: 0.900506
MAE train: 0.413655	val: 0.750694	test: 0.709005

Epoch: 51
Loss: 0.3892554257597242
RMSE train: 0.492628	val: 0.940796	test: 0.884029
MAE train: 0.390100	val: 0.745491	test: 0.696246

Epoch: 52
Loss: 0.3668004870414734
RMSE train: 0.490942	val: 0.925886	test: 0.876015
MAE train: 0.390136	val: 0.732844	test: 0.689286

Epoch: 53
Loss: 0.3687436580657959
RMSE train: 0.503605	val: 0.946000	test: 0.883333
MAE train: 0.402755	val: 0.741505	test: 0.702701

Epoch: 54
Loss: 0.3565260853086199
RMSE train: 0.490312	val: 0.938456	test: 0.871638
MAE train: 0.390345	val: 0.744071	test: 0.692813

Epoch: 55
Loss: 0.3669984425817217
RMSE train: 0.471746	val: 0.940764	test: 0.894021
MAE train: 0.374625	val: 0.753965	test: 0.704746

Epoch: 56
Loss: 0.3749768691403525
RMSE train: 0.477656	val: 0.946757	test: 0.882639
MAE train: 0.380925	val: 0.747532	test: 0.693549

Epoch: 57
Loss: 0.348141359431403
RMSE train: 0.452667	val: 0.920002	test: 0.860260
MAE train: 0.358701	val: 0.723096	test: 0.674940

Epoch: 58
Loss: 0.32930032908916473
RMSE train: 0.516576	val: 0.952836	test: 0.875008
MAE train: 0.411740	val: 0.752276	test: 0.691882

Epoch: 59
Loss: 0.3349094625030245
RMSE train: 0.440673	val: 0.943539	test: 0.894251
MAE train: 0.350222	val: 0.753257	test: 0.706907

Epoch: 60
Loss: 0.31337148802621023
RMSE train: 0.436789	val: 0.927638	test: 0.881492
MAE train: 0.349899	val: 0.737698	test: 0.690156

Epoch: 61
Loss: 0.31637413586888996
RMSE train: 0.434333	val: 0.933190	test: 0.899822
MAE train: 0.345070	val: 0.741372	test: 0.701062

Epoch: 62
Loss: 0.3373207811798368
RMSE train: 0.513669	val: 0.952145	test: 0.872069
MAE train: 0.409881	val: 0.750074	test: 0.684585

Epoch: 63
Loss: 0.30480630908693584
RMSE train: 0.461123	val: 0.938675	test: 0.869718
MAE train: 0.368293	val: 0.742396	test: 0.685885

Epoch: 64
Loss: 0.2993791678122112
RMSE train: 0.417112	val: 0.934529	test: 0.870192
MAE train: 0.330719	val: 0.735017	test: 0.686183

Epoch: 65
Loss: 0.291860642177718
RMSE train: 0.450653	val: 0.930186	test: 0.858637
MAE train: 0.360710	val: 0.737926	test: 0.668690

Epoch: 66
Loss: 0.2969263643026352
RMSE train: 0.482668	val: 0.940418	test: 0.860748
MAE train: 0.386357	val: 0.742047	test: 0.672045

Epoch: 67
Loss: 0.28867749869823456
RMSE train: 0.411148	val: 0.948652	test: 0.876915
MAE train: 0.327497	val: 0.746899	test: 0.690127

Epoch: 68
Loss: 0.2735817517553057
RMSE train: 0.423622	val: 0.938415	test: 0.875274
MAE train: 0.336748	val: 0.743312	test: 0.691156

Epoch: 69
Loss: 0.26520920757736477
RMSE train: 0.420886	val: 0.945642	test: 0.897414
MAE train: 0.336287	val: 0.752226	test: 0.707216

Epoch: 70
Loss: 0.27622624167374205
RMSE train: 0.395194	val: 0.936423	test: 0.882428
MAE train: 0.313037	val: 0.737712	test: 0.693263

Epoch: 71
Loss: 0.27422534780842917
RMSE train: 0.390751	val: 0.920477	test: 0.862556
MAE train: 0.309253	val: 0.724236	test: 0.668346

Epoch: 72
Loss: 0.283561504312924
RMSE train: 0.411389	val: 0.940141	test: 0.878795
MAE train: 0.325073	val: 0.739163	test: 0.692533

Epoch: 73
Loss: 0.2596700979130609
RMSE train: 0.420172	val: 0.944642	test: 0.864812
MAE train: 0.333362	val: 0.741526	test: 0.679536

Epoch: 74
Loss: 0.2644661143422127
RMSE train: 0.446590	val: 0.963361	test: 0.879338
MAE train: 0.356214	val: 0.763335	test: 0.697203

Epoch: 75
Loss: 0.26716568427426474
RMSE train: 0.388395	val: 0.927770	test: 0.856874
MAE train: 0.309599	val: 0.734711	test: 0.672441

Epoch: 76
Loss: 0.23347098380327225
RMSE train: 0.382908	val: 0.934142	test: 0.865624
MAE train: 0.304983	val: 0.734478	test: 0.673767

Epoch: 77
Loss: 0.2551145468439375
RMSE train: 0.443228	val: 0.954078	test: 0.874873
MAE train: 0.353992	val: 0.754299	test: 0.696755

Epoch: 78
Loss: 0.24314823001623154
RMSE train: 0.379131	val: 0.925122	test: 0.870530
MAE train: 0.302560	val: 0.732974	test: 0.690298

Epoch: 79
Loss: 0.22865157467978342
RMSE train: 0.364375	val: 0.920515	test: 0.858005
MAE train: 0.287826	val: 0.725742	test: 0.675723

Epoch: 80
Loss: 0.21831307347331727
RMSE train: 0.397605	val: 0.934578	test: 0.867861
MAE train: 0.318154	val: 0.734604	test: 0.682071

Epoch: 81
Loss: 0.21413093911749975
RMSE train: 0.412277	val: 0.927004	test: 0.857741
MAE train: 0.327587	val: 0.734356	test: 0.674549

Epoch: 82
Loss: 0.22408869117498398
RMSE train: 0.361389	val: 0.926837	test: 0.871889

Epoch: 23
Loss: 0.5783247564520154
RMSE train: 0.719403	val: 0.872127	test: 0.843266
MAE train: 0.555154	val: 0.670692	test: 0.679144

Epoch: 24
Loss: 0.5741108059883118
RMSE train: 0.672728	val: 0.841398	test: 0.790460
MAE train: 0.523219	val: 0.651800	test: 0.644354

Epoch: 25
Loss: 0.5237315659012113
RMSE train: 0.680169	val: 0.857738	test: 0.807024
MAE train: 0.528399	val: 0.662180	test: 0.656447

Epoch: 26
Loss: 0.5505270234176091
RMSE train: 0.668306	val: 0.844477	test: 0.795768
MAE train: 0.514229	val: 0.637459	test: 0.645904

Epoch: 27
Loss: 0.5040584653615952
RMSE train: 0.638226	val: 0.810199	test: 0.780787
MAE train: 0.493100	val: 0.628015	test: 0.618669

Epoch: 28
Loss: 0.5538897067308426
RMSE train: 0.647274	val: 0.824509	test: 0.798350
MAE train: 0.497482	val: 0.631290	test: 0.642905

Epoch: 29
Loss: 0.5016585226569857
RMSE train: 0.658655	val: 0.831451	test: 0.786781
MAE train: 0.513105	val: 0.636278	test: 0.635534

Epoch: 30
Loss: 0.532662781221526
RMSE train: 0.667031	val: 0.853434	test: 0.805767
MAE train: 0.519962	val: 0.661447	test: 0.656067

Epoch: 31
Loss: 0.4992324028696333
RMSE train: 0.665935	val: 0.855766	test: 0.806958
MAE train: 0.515507	val: 0.660573	test: 0.658549

Epoch: 32
Loss: 0.48063957691192627
RMSE train: 0.640820	val: 0.821665	test: 0.780170
MAE train: 0.495042	val: 0.636853	test: 0.622531

Epoch: 33
Loss: 0.4763209415333612
RMSE train: 0.625329	val: 0.814262	test: 0.781776
MAE train: 0.482200	val: 0.616873	test: 0.626480

Epoch: 34
Loss: 0.4948180168867111
RMSE train: 0.623871	val: 0.818346	test: 0.770730
MAE train: 0.483647	val: 0.625969	test: 0.621042

Epoch: 35
Loss: 0.4690854698419571
RMSE train: 0.662538	val: 0.846014	test: 0.800239
MAE train: 0.509378	val: 0.642094	test: 0.643979

Epoch: 36
Loss: 0.46663158919130054
RMSE train: 0.632611	val: 0.829091	test: 0.787204
MAE train: 0.488039	val: 0.632998	test: 0.629058

Epoch: 37
Loss: 0.4767242648771831
RMSE train: 0.643821	val: 0.848847	test: 0.794420
MAE train: 0.496928	val: 0.645031	test: 0.645877

Epoch: 38
Loss: 0.45787922186510904
RMSE train: 0.626974	val: 0.827586	test: 0.770099
MAE train: 0.485055	val: 0.630486	test: 0.618100

Epoch: 39
Loss: 0.459157475403377
RMSE train: 0.635826	val: 0.831655	test: 0.798967
MAE train: 0.489846	val: 0.637530	test: 0.639192

Epoch: 40
Loss: 0.4401437448603766
RMSE train: 0.605013	val: 0.799498	test: 0.763675
MAE train: 0.467831	val: 0.611344	test: 0.611926

Epoch: 41
Loss: 0.42581782170704435
RMSE train: 0.618603	val: 0.820404	test: 0.774712
MAE train: 0.480009	val: 0.623576	test: 0.622735

Epoch: 42
Loss: 0.44420563110283445
RMSE train: 0.624165	val: 0.818087	test: 0.779228
MAE train: 0.480865	val: 0.625588	test: 0.621404

Epoch: 43
Loss: 0.4206870070525578
RMSE train: 0.602473	val: 0.801064	test: 0.767120
MAE train: 0.465868	val: 0.615127	test: 0.615840

Epoch: 44
Loss: 0.4407994108540671
RMSE train: 0.589828	val: 0.793244	test: 0.754359
MAE train: 0.455544	val: 0.606125	test: 0.599359

Epoch: 45
Loss: 0.41855994292667936
RMSE train: 0.580353	val: 0.792499	test: 0.756813
MAE train: 0.446247	val: 0.608193	test: 0.602887

Epoch: 46
Loss: 0.40792778985840933
RMSE train: 0.623342	val: 0.844192	test: 0.796481
MAE train: 0.483293	val: 0.642105	test: 0.634601

Epoch: 47
Loss: 0.4362338866506304
RMSE train: 0.609522	val: 0.832259	test: 0.788032
MAE train: 0.471080	val: 0.633238	test: 0.626620

Epoch: 48
Loss: 0.4085061720439366
RMSE train: 0.603534	val: 0.825214	test: 0.778925
MAE train: 0.468784	val: 0.629314	test: 0.619156

Epoch: 49
Loss: 0.44621798608984264
RMSE train: 0.630194	val: 0.851733	test: 0.795578
MAE train: 0.489122	val: 0.650328	test: 0.631060

Epoch: 50
Loss: 0.45584796369075775
RMSE train: 0.606989	val: 0.820234	test: 0.778242
MAE train: 0.467611	val: 0.624114	test: 0.620778

Epoch: 51
Loss: 0.4291322422879083
RMSE train: 0.604468	val: 0.830139	test: 0.788267
MAE train: 0.466022	val: 0.632854	test: 0.623384

Epoch: 52
Loss: 0.4238872762237276
RMSE train: 0.600147	val: 0.814694	test: 0.775686
MAE train: 0.464369	val: 0.617805	test: 0.619493

Epoch: 53
Loss: 0.41099525349480764
RMSE train: 0.587887	val: 0.801236	test: 0.759581
MAE train: 0.455723	val: 0.614061	test: 0.601963

Epoch: 54
Loss: 0.3908370999353273
RMSE train: 0.595343	val: 0.809162	test: 0.770042
MAE train: 0.459768	val: 0.614167	test: 0.613559

Epoch: 55
Loss: 0.42561509353773935
RMSE train: 0.623612	val: 0.840676	test: 0.793767
MAE train: 0.484499	val: 0.644410	test: 0.627910

Epoch: 56
Loss: 0.3779147097042629
RMSE train: 0.582198	val: 0.815119	test: 0.769714
MAE train: 0.452276	val: 0.615824	test: 0.604753

Epoch: 57
Loss: 0.3969337833779199
RMSE train: 0.554361	val: 0.792172	test: 0.750142
MAE train: 0.427125	val: 0.604440	test: 0.586217

Epoch: 58
Loss: 0.38017791722502026
RMSE train: 0.580609	val: 0.802226	test: 0.761351
MAE train: 0.447752	val: 0.612065	test: 0.605273

Epoch: 59
Loss: 0.41943612481866566
RMSE train: 0.583230	val: 0.812045	test: 0.767958
MAE train: 0.453289	val: 0.621841	test: 0.604721

Epoch: 60
Loss: 0.36565173098019194
RMSE train: 0.569716	val: 0.786881	test: 0.752505
MAE train: 0.440791	val: 0.606378	test: 0.594988

Epoch: 61
Loss: 0.3715348392724991
RMSE train: 0.590887	val: 0.809593	test: 0.760040
MAE train: 0.458186	val: 0.614462	test: 0.604079

Epoch: 62
Loss: 0.3769884577819279
RMSE train: 0.565342	val: 0.799629	test: 0.748721
MAE train: 0.438487	val: 0.608063	test: 0.590141

Epoch: 63
Loss: 0.40164038964680265
RMSE train: 0.599723	val: 0.835689	test: 0.780859
MAE train: 0.466677	val: 0.638713	test: 0.627041

Epoch: 64
Loss: 0.3580894981111799
RMSE train: 0.570285	val: 0.792560	test: 0.756491
MAE train: 0.443466	val: 0.609072	test: 0.592469

Epoch: 65
Loss: 0.37769995842661175
RMSE train: 0.584963	val: 0.803512	test: 0.754023
MAE train: 0.453830	val: 0.613788	test: 0.601633

Epoch: 66
Loss: 0.38184112523283276
RMSE train: 0.597723	val: 0.825580	test: 0.773801
MAE train: 0.464469	val: 0.628211	test: 0.614869

Epoch: 67
Loss: 0.36360316830021994
RMSE train: 0.550144	val: 0.789721	test: 0.753801
MAE train: 0.427199	val: 0.604434	test: 0.592368

Epoch: 68
Loss: 0.3641348992075239
RMSE train: 0.595658	val: 0.828587	test: 0.773114
MAE train: 0.463883	val: 0.634654	test: 0.602696

Epoch: 69
Loss: 0.35893817245960236
RMSE train: 0.566976	val: 0.814728	test: 0.769111
MAE train: 0.438968	val: 0.612489	test: 0.608105

Epoch: 70
Loss: 0.3430223890713283
RMSE train: 0.549086	val: 0.811472	test: 0.754540
MAE train: 0.425532	val: 0.614296	test: 0.589370

Epoch: 71
Loss: 0.3468754121235439
RMSE train: 0.567398	val: 0.810914	test: 0.756605
MAE train: 0.439590	val: 0.613388	test: 0.599091

Epoch: 72
Loss: 0.36418583137648447
RMSE train: 0.581743	val: 0.832722	test: 0.772885
MAE train: 0.451184	val: 0.634645	test: 0.603971

Epoch: 73
Loss: 0.3620466504778181
RMSE train: 0.527231	val: 0.775149	test: 0.745131
MAE train: 0.409371	val: 0.591849	test: 0.583010

Epoch: 74
Loss: 0.3547773403780801
RMSE train: 0.536943	val: 0.781451	test: 0.757313
MAE train: 0.415669	val: 0.589130	test: 0.595887

Epoch: 75
Loss: 0.35866389317171915
RMSE train: 0.525968	val: 0.781784	test: 0.755813
MAE train: 0.405664	val: 0.595520	test: 0.590182

Epoch: 76
Loss: 0.35747818435941425
RMSE train: 0.547102	val: 0.817652	test: 0.773856
MAE train: 0.424050	val: 0.619985	test: 0.600852

Epoch: 77
Loss: 0.33170186621802195
RMSE train: 0.528989	val: 0.773390	test: 0.739836
MAE train: 0.406600	val: 0.585390	test: 0.578609

Epoch: 78
Loss: 0.3279374177966799
RMSE train: 0.514695	val: 0.773300	test: 0.740115
MAE train: 0.398427	val: 0.592803	test: 0.577839

Epoch: 79
Loss: 0.31749119077410015
RMSE train: 0.535016	val: 0.784281	test: 0.755681
MAE train: 0.412577	val: 0.598798	test: 0.594465

Epoch: 80
Loss: 0.3232026462044035
RMSE train: 0.519027	val: 0.786398	test: 0.741840
MAE train: 0.404474	val: 0.597212	test: 0.574768

Epoch: 81
Loss: 0.3225613683462143
RMSE train: 0.500619	val: 0.769369	test: 0.742807
MAE train: 0.388032	val: 0.581988	test: 0.580435

Epoch: 82
Loss: 0.35883912444114685
RMSE train: 0.556330	val: 0.807176	test: 0.777172
MAE train: 0.430494	val: 0.611265	test: 0.611429

Epoch: 83
Loss: 0.3324590197631291
RMSE train: 0.553433	val: 0.791552	test: 0.761348

Epoch: 23
Loss: 0.5643686141286578
RMSE train: 0.679158	val: 0.848365	test: 0.810413
MAE train: 0.527989	val: 0.657428	test: 0.655597

Epoch: 24
Loss: 0.5536717048713139
RMSE train: 0.703758	val: 0.861902	test: 0.810729
MAE train: 0.548092	val: 0.662927	test: 0.654148

Epoch: 25
Loss: 0.5704404626573835
RMSE train: 0.697494	val: 0.852929	test: 0.801366
MAE train: 0.541643	val: 0.660056	test: 0.652370

Epoch: 26
Loss: 0.5182991847395897
RMSE train: 0.712151	val: 0.877380	test: 0.840518
MAE train: 0.552374	val: 0.681860	test: 0.675569

Epoch: 27
Loss: 0.5401157098157066
RMSE train: 0.691660	val: 0.869946	test: 0.813915
MAE train: 0.536921	val: 0.674840	test: 0.664392

Epoch: 28
Loss: 0.5296670517751149
RMSE train: 0.686888	val: 0.870451	test: 0.811325
MAE train: 0.532279	val: 0.678908	test: 0.653134

Epoch: 29
Loss: 0.5229836979082653
RMSE train: 0.646688	val: 0.818470	test: 0.778694
MAE train: 0.502215	val: 0.631030	test: 0.624818

Epoch: 30
Loss: 0.5181573012045452
RMSE train: 0.656361	val: 0.830790	test: 0.777004
MAE train: 0.509672	val: 0.643383	test: 0.625689

Epoch: 31
Loss: 0.47377272163118633
RMSE train: 0.636989	val: 0.807566	test: 0.784728
MAE train: 0.495446	val: 0.619115	test: 0.620168

Epoch: 32
Loss: 0.48114125217710224
RMSE train: 0.640487	val: 0.826004	test: 0.778357
MAE train: 0.499564	val: 0.634997	test: 0.629932

Epoch: 33
Loss: 0.4983427290405546
RMSE train: 0.658658	val: 0.836032	test: 0.790125
MAE train: 0.511436	val: 0.640886	test: 0.635095

Epoch: 34
Loss: 0.4642124261174883
RMSE train: 0.645083	val: 0.829173	test: 0.798786
MAE train: 0.499535	val: 0.632518	test: 0.637518

Epoch: 35
Loss: 0.47351172672850744
RMSE train: 0.672076	val: 0.859997	test: 0.820507
MAE train: 0.522178	val: 0.667210	test: 0.653145

Epoch: 36
Loss: 0.47009411454200745
RMSE train: 0.631696	val: 0.823460	test: 0.800447
MAE train: 0.491142	val: 0.632651	test: 0.641473

Epoch: 37
Loss: 0.4683475153786795
RMSE train: 0.596662	val: 0.779025	test: 0.772789
MAE train: 0.465420	val: 0.596256	test: 0.619508

Epoch: 38
Loss: 0.45132123146738323
RMSE train: 0.622483	val: 0.794012	test: 0.775908
MAE train: 0.485648	val: 0.603319	test: 0.622836

Epoch: 39
Loss: 0.4567235857248306
RMSE train: 0.629881	val: 0.831483	test: 0.794318
MAE train: 0.488728	val: 0.630175	test: 0.639402

Epoch: 40
Loss: 0.4359038131577628
RMSE train: 0.629834	val: 0.828004	test: 0.779754
MAE train: 0.485779	val: 0.627199	test: 0.627018

Epoch: 41
Loss: 0.4616349424634661
RMSE train: 0.646881	val: 0.832488	test: 0.807190
MAE train: 0.501542	val: 0.631004	test: 0.646472

Epoch: 42
Loss: 0.4307157737868173
RMSE train: 0.612945	val: 0.808485	test: 0.768169
MAE train: 0.473841	val: 0.617550	test: 0.616285

Epoch: 43
Loss: 0.4558636801583426
RMSE train: 0.599914	val: 0.801809	test: 0.771072
MAE train: 0.465267	val: 0.610806	test: 0.619978

Epoch: 44
Loss: 0.44184863780226025
RMSE train: 0.615086	val: 0.805732	test: 0.776033
MAE train: 0.478540	val: 0.612210	test: 0.616585

Epoch: 45
Loss: 0.44580332721982685
RMSE train: 0.614275	val: 0.818077	test: 0.776168
MAE train: 0.474788	val: 0.626372	test: 0.613466

Epoch: 46
Loss: 0.42597623808043344
RMSE train: 0.583601	val: 0.793441	test: 0.770734
MAE train: 0.451022	val: 0.603907	test: 0.600449

Epoch: 47
Loss: 0.4233076253107616
RMSE train: 0.615241	val: 0.823708	test: 0.773865
MAE train: 0.480348	val: 0.622833	test: 0.618059

Epoch: 48
Loss: 0.423722614135061
RMSE train: 0.586124	val: 0.805367	test: 0.773329
MAE train: 0.453811	val: 0.610426	test: 0.614983

Epoch: 49
Loss: 0.39725723224026815
RMSE train: 0.614360	val: 0.815022	test: 0.795804
MAE train: 0.481522	val: 0.613816	test: 0.635721

Epoch: 50
Loss: 0.4091525993176869
RMSE train: 0.583989	val: 0.795871	test: 0.776252
MAE train: 0.452153	val: 0.600731	test: 0.611427

Epoch: 51
Loss: 0.40561236441135406
RMSE train: 0.569159	val: 0.802377	test: 0.781386
MAE train: 0.440996	val: 0.606338	test: 0.612987

Epoch: 52
Loss: 0.40455437558037893
RMSE train: 0.578825	val: 0.803459	test: 0.773558
MAE train: 0.449941	val: 0.614867	test: 0.604728

Epoch: 53
Loss: 0.40859568757670267
RMSE train: 0.567255	val: 0.805454	test: 0.771499
MAE train: 0.439950	val: 0.610970	test: 0.610828

Epoch: 54
Loss: 0.4050378373691014
RMSE train: 0.574426	val: 0.830956	test: 0.798763
MAE train: 0.444153	val: 0.630932	test: 0.627805

Epoch: 55
Loss: 0.40316542770181385
RMSE train: 0.575007	val: 0.793139	test: 0.781537
MAE train: 0.446889	val: 0.605560	test: 0.623274

Epoch: 56
Loss: 0.4047615570681436
RMSE train: 0.562865	val: 0.805844	test: 0.772204
MAE train: 0.438757	val: 0.611963	test: 0.610576

Epoch: 57
Loss: 0.39033836339201244
RMSE train: 0.564653	val: 0.779971	test: 0.770920
MAE train: 0.439931	val: 0.587812	test: 0.609353

Epoch: 58
Loss: 0.3766566983291081
RMSE train: 0.551773	val: 0.800912	test: 0.777204
MAE train: 0.426333	val: 0.599944	test: 0.607866

Epoch: 59
Loss: 0.3644656166434288
RMSE train: 0.556967	val: 0.795592	test: 0.762261
MAE train: 0.432963	val: 0.597158	test: 0.600624

Epoch: 60
Loss: 0.3815890827349254
RMSE train: 0.564013	val: 0.804351	test: 0.769657
MAE train: 0.444897	val: 0.613291	test: 0.609110

Epoch: 61
Loss: 0.38142378415380207
RMSE train: 0.541213	val: 0.777218	test: 0.752529
MAE train: 0.417520	val: 0.588459	test: 0.594601

Epoch: 62
Loss: 0.3826736807823181
RMSE train: 0.561603	val: 0.787120	test: 0.771303
MAE train: 0.433696	val: 0.594625	test: 0.612212

Epoch: 63
Loss: 0.3796297162771225
RMSE train: 0.567885	val: 0.792572	test: 0.773730
MAE train: 0.443551	val: 0.601293	test: 0.615459

Epoch: 64
Loss: 0.35965634882450104
RMSE train: 0.546469	val: 0.800838	test: 0.765622
MAE train: 0.423402	val: 0.605088	test: 0.604494

Epoch: 65
Loss: 0.36741090033735546
RMSE train: 0.550108	val: 0.804781	test: 0.777695
MAE train: 0.422339	val: 0.603864	test: 0.610270

Epoch: 66
Loss: 0.35311682309423176
RMSE train: 0.536003	val: 0.787717	test: 0.766253
MAE train: 0.416221	val: 0.595193	test: 0.596313

Epoch: 67
Loss: 0.3684632522719247
RMSE train: 0.533595	val: 0.768182	test: 0.763417
MAE train: 0.415556	val: 0.582644	test: 0.605135

Epoch: 68
Loss: 0.3621617321457182
RMSE train: 0.536246	val: 0.781282	test: 0.774024
MAE train: 0.416379	val: 0.584656	test: 0.605280

Epoch: 69
Loss: 0.34625101728098734
RMSE train: 0.539578	val: 0.783657	test: 0.769414
MAE train: 0.419165	val: 0.593277	test: 0.610083

Epoch: 70
Loss: 0.35709493287972044
RMSE train: 0.554201	val: 0.792602	test: 0.767939
MAE train: 0.434653	val: 0.601827	test: 0.604620

Epoch: 71
Loss: 0.34553065470286776
RMSE train: 0.544681	val: 0.788959	test: 0.766788
MAE train: 0.425031	val: 0.598400	test: 0.605742

Epoch: 72
Loss: 0.34033486034188953
RMSE train: 0.533388	val: 0.782521	test: 0.765445
MAE train: 0.413679	val: 0.593692	test: 0.597546

Epoch: 73
Loss: 0.3526140089545931
RMSE train: 0.531345	val: 0.791011	test: 0.749938
MAE train: 0.414214	val: 0.600174	test: 0.590798

Epoch: 74
Loss: 0.34614197271210806
RMSE train: 0.552402	val: 0.810301	test: 0.779206
MAE train: 0.427329	val: 0.619436	test: 0.608804

Epoch: 75
Loss: 0.3321631295340402
RMSE train: 0.525569	val: 0.789332	test: 0.760797
MAE train: 0.408012	val: 0.599563	test: 0.601831

Epoch: 76
Loss: 0.32455802815301077
RMSE train: 0.511567	val: 0.772855	test: 0.750385
MAE train: 0.395676	val: 0.587888	test: 0.590775

Epoch: 77
Loss: 0.33916100221020834
RMSE train: 0.537372	val: 0.799463	test: 0.778072
MAE train: 0.415184	val: 0.606998	test: 0.598674

Epoch: 78
Loss: 0.33333089309079306
RMSE train: 0.530486	val: 0.794593	test: 0.761131
MAE train: 0.411977	val: 0.600257	test: 0.596237

Epoch: 79
Loss: 0.3176565191575459
RMSE train: 0.508404	val: 0.778317	test: 0.759430
MAE train: 0.394010	val: 0.585872	test: 0.595716

Epoch: 80
Loss: 0.31827786564826965
RMSE train: 0.524798	val: 0.804312	test: 0.762641
MAE train: 0.403499	val: 0.606038	test: 0.591676

Epoch: 81
Loss: 0.3279450110026768
RMSE train: 0.525425	val: 0.787366	test: 0.771149
MAE train: 0.411251	val: 0.598332	test: 0.605458

Epoch: 82
Loss: 0.3391325835670744
RMSE train: 0.519308	val: 0.789107	test: 0.757213
MAE train: 0.401874	val: 0.600370	test: 0.591318

Epoch: 83
Loss: 0.32754326718194143
RMSE train: 0.522938	val: 0.799902	test: 0.759677

Epoch: 22
Loss: 0.8731912587370191
RMSE train: 0.830196	val: 1.120964	test: 1.135589
MAE train: 0.668924	val: 0.881542	test: 0.888193

Epoch: 23
Loss: 0.8238245929990496
RMSE train: 0.797363	val: 1.155252	test: 1.178371
MAE train: 0.643542	val: 0.906662	test: 0.943992

Epoch: 24
Loss: 0.7714519543307168
RMSE train: 0.780455	val: 1.186666	test: 1.240354
MAE train: 0.627533	val: 0.929838	test: 0.986911

Epoch: 25
Loss: 0.7634073453290122
RMSE train: 0.791769	val: 1.118109	test: 1.179485
MAE train: 0.641255	val: 0.886337	test: 0.947839

Epoch: 26
Loss: 0.7579724703516278
RMSE train: 0.796832	val: 1.190503	test: 1.225615
MAE train: 0.645720	val: 0.920893	test: 1.003288

Epoch: 27
Loss: 0.7188741649900164
RMSE train: 0.750331	val: 1.238803	test: 1.310138
MAE train: 0.601508	val: 0.999142	test: 1.029185

Epoch: 28
Loss: 0.6824365215642112
RMSE train: 0.731812	val: 1.147548	test: 1.190896
MAE train: 0.588387	val: 0.899906	test: 0.971047

Epoch: 29
Loss: 0.6432704435927528
RMSE train: 0.726830	val: 1.171766	test: 1.253185
MAE train: 0.582330	val: 0.916186	test: 1.007830

Epoch: 30
Loss: 0.681743038552148
RMSE train: 0.723576	val: 1.115192	test: 1.155784
MAE train: 0.583498	val: 0.877731	test: 0.940029

Epoch: 31
Loss: 0.6469958211694445
RMSE train: 0.710075	val: 1.148200	test: 1.223300
MAE train: 0.571608	val: 0.897794	test: 0.993947

Epoch: 32
Loss: 0.6135018255029406
RMSE train: 0.754159	val: 1.116150	test: 1.167899
MAE train: 0.611987	val: 0.885563	test: 0.932199

Epoch: 33
Loss: 0.5836684703826904
RMSE train: 0.698281	val: 1.143721	test: 1.185659
MAE train: 0.562454	val: 0.910919	test: 0.961906

Epoch: 34
Loss: 0.5689498037099838
RMSE train: 0.663125	val: 1.208429	test: 1.277918
MAE train: 0.532772	val: 0.963582	test: 1.040645

Epoch: 35
Loss: 0.5708563221352441
RMSE train: 0.666655	val: 1.144004	test: 1.191828
MAE train: 0.536727	val: 0.902690	test: 0.970456

Epoch: 36
Loss: 0.5506200981991631
RMSE train: 0.650005	val: 1.157783	test: 1.201549
MAE train: 0.519625	val: 0.907464	test: 0.948573

Epoch: 37
Loss: 0.5315213884626117
RMSE train: 0.642487	val: 1.115557	test: 1.163035
MAE train: 0.515485	val: 0.879592	test: 0.952811

Epoch: 38
Loss: 0.5292060034615653
RMSE train: 0.655863	val: 1.116596	test: 1.168432
MAE train: 0.527041	val: 0.879077	test: 0.943304

Epoch: 39
Loss: 0.5149713839803424
RMSE train: 0.637135	val: 1.174214	test: 1.239993
MAE train: 0.513524	val: 0.928438	test: 1.016288

Epoch: 40
Loss: 0.4666165305035455
RMSE train: 0.577700	val: 1.170535	test: 1.218068
MAE train: 0.461889	val: 0.917174	test: 0.998748

Epoch: 41
Loss: 0.4837173670530319
RMSE train: 0.690576	val: 1.106040	test: 1.179180
MAE train: 0.562975	val: 0.880773	test: 0.939534

Epoch: 42
Loss: 0.4927934323038374
RMSE train: 0.659255	val: 1.119490	test: 1.169367
MAE train: 0.534096	val: 0.879578	test: 0.924769

Epoch: 43
Loss: 0.47494183693613323
RMSE train: 0.566799	val: 1.159384	test: 1.209106
MAE train: 0.452059	val: 0.921572	test: 0.985067

Epoch: 44
Loss: 0.47312188574246
RMSE train: 0.551724	val: 1.104580	test: 1.141495
MAE train: 0.439839	val: 0.873153	test: 0.925189

Epoch: 45
Loss: 0.42154467957360403
RMSE train: 0.554317	val: 1.101834	test: 1.084219
MAE train: 0.442299	val: 0.865086	test: 0.874358

Epoch: 46
Loss: 0.42689926496573855
RMSE train: 0.532546	val: 1.095768	test: 1.096900
MAE train: 0.423244	val: 0.864543	test: 0.888348

Epoch: 47
Loss: 0.399504223040172
RMSE train: 0.542979	val: 1.117513	test: 1.143121
MAE train: 0.431857	val: 0.873768	test: 0.929270

Epoch: 48
Loss: 0.3911068482058389
RMSE train: 0.514738	val: 1.094512	test: 1.099057
MAE train: 0.409683	val: 0.861616	test: 0.896204

Epoch: 49
Loss: 0.3720014861651829
RMSE train: 0.515265	val: 1.070068	test: 1.099772
MAE train: 0.409201	val: 0.837477	test: 0.886445

Epoch: 50
Loss: 0.36320762762001585
RMSE train: 0.522482	val: 1.119543	test: 1.142505
MAE train: 0.416398	val: 0.892917	test: 0.929463

Epoch: 51
Loss: 0.38218100368976593
RMSE train: 0.490766	val: 1.119930	test: 1.144101
MAE train: 0.389337	val: 0.876926	test: 0.939292

Epoch: 52
Loss: 0.33048673080546515
RMSE train: 0.480838	val: 1.110365	test: 1.132356
MAE train: 0.381170	val: 0.884744	test: 0.910887

Epoch: 53
Loss: 0.33446345158985685
RMSE train: 0.485350	val: 1.058367	test: 1.073637
MAE train: 0.386361	val: 0.832432	test: 0.851673

Epoch: 54
Loss: 0.3181513122149876
RMSE train: 0.609810	val: 1.083632	test: 1.098547
MAE train: 0.498359	val: 0.867822	test: 0.864823

Epoch: 55
Loss: 0.3569501148802893
RMSE train: 0.483142	val: 1.057875	test: 1.101721
MAE train: 0.386000	val: 0.840501	test: 0.890601

Epoch: 56
Loss: 0.35082593134471346
RMSE train: 0.490232	val: 1.087576	test: 1.108123
MAE train: 0.391575	val: 0.855159	test: 0.908308

Epoch: 57
Loss: 0.3242600964648383
RMSE train: 0.491931	val: 1.041649	test: 1.062472
MAE train: 0.392380	val: 0.817448	test: 0.853107

Epoch: 58
Loss: 0.2981287900890623
RMSE train: 0.535896	val: 1.101659	test: 1.130754
MAE train: 0.434052	val: 0.896793	test: 0.903931

Epoch: 59
Loss: 0.31509177812508177
RMSE train: 0.543646	val: 1.056759	test: 1.085934
MAE train: 0.447670	val: 0.842431	test: 0.860854

Epoch: 60
Loss: 0.30743403307029177
RMSE train: 0.432103	val: 1.072509	test: 1.073068
MAE train: 0.342503	val: 0.841219	test: 0.867898

Epoch: 61
Loss: 0.2956654897757939
RMSE train: 0.527626	val: 1.133432	test: 1.183455
MAE train: 0.433172	val: 0.894815	test: 0.936347

Epoch: 62
Loss: 0.2796190561992781
RMSE train: 0.532976	val: 1.042819	test: 1.051140
MAE train: 0.433647	val: 0.833395	test: 0.847167

Epoch: 63
Loss: 0.26295935788324903
RMSE train: 0.412831	val: 1.096985	test: 1.113340
MAE train: 0.327746	val: 0.866822	test: 0.890737

Epoch: 64
Loss: 0.26577180943318773
RMSE train: 0.409716	val: 1.127356	test: 1.119697
MAE train: 0.325102	val: 0.877410	test: 0.899436

Epoch: 65
Loss: 0.26539823200021473
RMSE train: 0.496002	val: 1.114364	test: 1.135982
MAE train: 0.404083	val: 0.889712	test: 0.888225

Epoch: 66
Loss: 0.26714841595717836
RMSE train: 0.435633	val: 1.075809	test: 1.060148
MAE train: 0.346128	val: 0.848619	test: 0.842023

Epoch: 67
Loss: 0.2589007073215076
RMSE train: 0.390635	val: 1.075654	test: 1.056287
MAE train: 0.309059	val: 0.851794	test: 0.835838

Epoch: 68
Loss: 0.25060431552784784
RMSE train: 0.404609	val: 1.065034	test: 1.060516
MAE train: 0.318850	val: 0.856788	test: 0.834560

Epoch: 69
Loss: 0.25894440497670856
RMSE train: 0.442611	val: 1.051399	test: 1.064613
MAE train: 0.352274	val: 0.826586	test: 0.849959

Epoch: 70
Loss: 0.24291812841381347
RMSE train: 0.418694	val: 1.138335	test: 1.147695
MAE train: 0.334926	val: 0.889884	test: 0.925926

Epoch: 71
Loss: 0.24005328118801117
RMSE train: 0.458355	val: 1.072967	test: 1.070386
MAE train: 0.372807	val: 0.858023	test: 0.849736

Epoch: 72
Loss: 0.22115263449294226
RMSE train: 0.455325	val: 1.088268	test: 1.083490
MAE train: 0.370612	val: 0.865643	test: 0.875379

Epoch: 73
Loss: 0.22199815192392894
RMSE train: 0.440195	val: 1.067379	test: 1.057829
MAE train: 0.361123	val: 0.844743	test: 0.853535

Epoch: 74
Loss: 0.23533250497920172
RMSE train: 0.527333	val: 1.127479	test: 1.133026
MAE train: 0.441372	val: 0.922416	test: 0.910409

Epoch: 75
Loss: 0.2246174014040402
RMSE train: 0.405423	val: 1.120399	test: 1.105677
MAE train: 0.324698	val: 0.884676	test: 0.905028

Epoch: 76
Loss: 0.20354547670909337
RMSE train: 0.530188	val: 1.175318	test: 1.198952
MAE train: 0.448863	val: 0.972147	test: 0.969658

Epoch: 77
Loss: 0.22936623011316573
RMSE train: 0.393668	val: 1.131493	test: 1.100218
MAE train: 0.312760	val: 0.889700	test: 0.893607

Epoch: 78
Loss: 0.20919312217405864
RMSE train: 0.393373	val: 1.089335	test: 1.100032
MAE train: 0.315080	val: 0.862051	test: 0.890538

Epoch: 79
Loss: 0.20511803988899505
RMSE train: 0.513841	val: 1.036494	test: 1.037504
MAE train: 0.435640	val: 0.822359	test: 0.835375

Epoch: 80
Loss: 0.21205710832561767
RMSE train: 0.380687	val: 1.169758	test: 1.165483
MAE train: 0.304959	val: 0.913976	test: 0.946569

Epoch: 81
Loss: 0.21026475408247539
RMSE train: 0.413712	val: 1.054423	test: 1.045484
MAE train: 0.333322	val: 0.850902	test: 0.846838

Epoch: 82
Loss: 0.19430735920156753
RMSE train: 0.424094	val: 1.155997	test: 1.158745

Epoch: 22
Loss: 0.7920418126242501
RMSE train: 0.830884	val: 1.053264	test: 1.021395
MAE train: 0.676720	val: 0.861051	test: 0.824008

Epoch: 23
Loss: 0.8096367461340768
RMSE train: 0.787929	val: 1.032456	test: 0.966280
MAE train: 0.637518	val: 0.831672	test: 0.774894

Epoch: 24
Loss: 0.7673873858792442
RMSE train: 0.767021	val: 1.045256	test: 0.999040
MAE train: 0.619177	val: 0.843397	test: 0.801816

Epoch: 25
Loss: 0.7270244445119586
RMSE train: 0.780346	val: 1.063137	test: 1.001205
MAE train: 0.631070	val: 0.846660	test: 0.811888

Epoch: 26
Loss: 0.7063548862934113
RMSE train: 0.729519	val: 1.078977	test: 0.992701
MAE train: 0.588793	val: 0.858741	test: 0.803714

Epoch: 27
Loss: 0.7071855919701713
RMSE train: 0.713440	val: 1.088619	test: 1.031338
MAE train: 0.571077	val: 0.866635	test: 0.820807

Epoch: 28
Loss: 0.6639853204999652
RMSE train: 0.686991	val: 1.131089	test: 1.041653
MAE train: 0.549934	val: 0.893483	test: 0.821106

Epoch: 29
Loss: 0.6552943331854684
RMSE train: 0.719744	val: 1.059808	test: 0.989147
MAE train: 0.582786	val: 0.843798	test: 0.799230

Epoch: 30
Loss: 0.6454004730497088
RMSE train: 0.678830	val: 1.069428	test: 0.995686
MAE train: 0.548384	val: 0.856712	test: 0.802861

Epoch: 31
Loss: 0.5972794379506793
RMSE train: 0.895752	val: 1.123813	test: 1.114816
MAE train: 0.742820	val: 0.908386	test: 0.892907

Epoch: 32
Loss: 0.6082848310470581
RMSE train: 0.671785	val: 1.060969	test: 0.990168
MAE train: 0.544708	val: 0.855834	test: 0.797882

Epoch: 33
Loss: 0.6138051237378802
RMSE train: 0.630494	val: 1.062623	test: 1.010294
MAE train: 0.505965	val: 0.855792	test: 0.816257

Epoch: 34
Loss: 0.557950530733381
RMSE train: 0.641066	val: 1.095699	test: 1.027580
MAE train: 0.515002	val: 0.877861	test: 0.827487

Epoch: 35
Loss: 0.557853462440627
RMSE train: 0.660259	val: 1.064690	test: 1.007966
MAE train: 0.531634	val: 0.853765	test: 0.815454

Epoch: 36
Loss: 0.5414544343948364
RMSE train: 0.648912	val: 1.040812	test: 1.005916
MAE train: 0.524596	val: 0.844873	test: 0.812931

Epoch: 37
Loss: 0.5315726441996438
RMSE train: 0.586083	val: 1.113540	test: 1.040980
MAE train: 0.466820	val: 0.886516	test: 0.830388

Epoch: 38
Loss: 0.5077712216547557
RMSE train: 0.590826	val: 1.040445	test: 1.017614
MAE train: 0.473875	val: 0.844309	test: 0.812703

Epoch: 39
Loss: 0.48998609823840006
RMSE train: 0.618306	val: 1.050088	test: 1.012208
MAE train: 0.497025	val: 0.849975	test: 0.820930

Epoch: 40
Loss: 0.45844657932009014
RMSE train: 0.544253	val: 1.047671	test: 1.006784
MAE train: 0.430460	val: 0.840062	test: 0.799809

Epoch: 41
Loss: 0.48134281379835947
RMSE train: 0.545304	val: 1.095776	test: 1.016252
MAE train: 0.434504	val: 0.867817	test: 0.802572

Epoch: 42
Loss: 0.4278787523508072
RMSE train: 0.594653	val: 1.073385	test: 1.017520
MAE train: 0.476277	val: 0.873669	test: 0.812810

Epoch: 43
Loss: 0.42095841680254253
RMSE train: 0.525163	val: 1.052669	test: 0.976553
MAE train: 0.416583	val: 0.843976	test: 0.769216

Epoch: 44
Loss: 0.4178915683712278
RMSE train: 0.539877	val: 1.042983	test: 0.956601
MAE train: 0.429405	val: 0.833316	test: 0.770546

Epoch: 45
Loss: 0.4161876014300755
RMSE train: 0.565477	val: 0.996545	test: 0.944893
MAE train: 0.453968	val: 0.799823	test: 0.756201

Epoch: 46
Loss: 0.4157405176333019
RMSE train: 0.510603	val: 1.147324	test: 1.030940
MAE train: 0.405081	val: 0.901063	test: 0.817548

Epoch: 47
Loss: 0.3997686633041927
RMSE train: 0.493090	val: 1.057795	test: 0.981276
MAE train: 0.391677	val: 0.851552	test: 0.774472

Epoch: 48
Loss: 0.39270496581281933
RMSE train: 0.491652	val: 1.051052	test: 0.969273
MAE train: 0.391484	val: 0.844861	test: 0.775236

Epoch: 49
Loss: 0.3745196099792208
RMSE train: 0.525116	val: 1.027235	test: 0.960619
MAE train: 0.421405	val: 0.829408	test: 0.775646

Epoch: 50
Loss: 0.3611485894237246
RMSE train: 0.481397	val: 1.145649	test: 1.042824
MAE train: 0.383202	val: 0.909665	test: 0.815841

Epoch: 51
Loss: 0.37316214612552095
RMSE train: 0.488332	val: 1.143958	test: 1.041409
MAE train: 0.388338	val: 0.905128	test: 0.824234

Epoch: 52
Loss: 0.36120680613177164
RMSE train: 0.482283	val: 1.066915	test: 0.999716
MAE train: 0.382514	val: 0.859593	test: 0.794005

Epoch: 53
Loss: 0.3600807126079287
RMSE train: 0.569094	val: 1.055902	test: 1.008388
MAE train: 0.466629	val: 0.849981	test: 0.808388

Epoch: 54
Loss: 0.33321484284741537
RMSE train: 0.441146	val: 1.042263	test: 0.951351
MAE train: 0.348570	val: 0.831685	test: 0.763524

Epoch: 55
Loss: 0.32410602484430584
RMSE train: 0.463258	val: 1.054338	test: 0.968062
MAE train: 0.369053	val: 0.842927	test: 0.776622

Epoch: 56
Loss: 0.3213089917387281
RMSE train: 0.437231	val: 1.051463	test: 0.964522
MAE train: 0.346670	val: 0.835447	test: 0.768586

Epoch: 57
Loss: 0.30781158272709164
RMSE train: 0.415568	val: 1.071562	test: 0.976676
MAE train: 0.327487	val: 0.845257	test: 0.778536

Epoch: 58
Loss: 0.29551494121551514
RMSE train: 0.409202	val: 1.086493	test: 0.997612
MAE train: 0.322167	val: 0.858792	test: 0.788551

Epoch: 59
Loss: 0.29241174672331127
RMSE train: 0.447906	val: 1.043917	test: 0.953578
MAE train: 0.356417	val: 0.821576	test: 0.766462

Epoch: 60
Loss: 0.2842767302479063
RMSE train: 0.484879	val: 1.054618	test: 0.971474
MAE train: 0.393914	val: 0.837568	test: 0.773941

Epoch: 61
Loss: 0.2765409531337874
RMSE train: 0.412304	val: 1.096510	test: 1.003267
MAE train: 0.324667	val: 0.864732	test: 0.797259

Epoch: 62
Loss: 0.28340053877660204
RMSE train: 0.417112	val: 1.018320	test: 0.942327
MAE train: 0.333026	val: 0.808746	test: 0.753596

Epoch: 63
Loss: 0.2694499705518995
RMSE train: 0.393593	val: 1.118086	test: 1.014832
MAE train: 0.310910	val: 0.874148	test: 0.807089

Epoch: 64
Loss: 0.2742379211953708
RMSE train: 0.366791	val: 1.070480	test: 0.970716
MAE train: 0.288734	val: 0.845809	test: 0.773690

Epoch: 65
Loss: 0.271749136703355
RMSE train: 0.370968	val: 1.076176	test: 0.993403
MAE train: 0.293134	val: 0.850263	test: 0.786630

Epoch: 66
Loss: 0.2729729861021042
RMSE train: 0.389785	val: 1.067569	test: 0.965148
MAE train: 0.307255	val: 0.838427	test: 0.771246

Epoch: 67
Loss: 0.2735987401434353
RMSE train: 0.396906	val: 1.144860	test: 1.050682
MAE train: 0.316971	val: 0.897123	test: 0.829508

Epoch: 68
Loss: 0.25778056574719294
RMSE train: 0.386852	val: 1.060007	test: 0.981627
MAE train: 0.308041	val: 0.828046	test: 0.784108

Epoch: 69
Loss: 0.23811190681798117
RMSE train: 0.432508	val: 1.078102	test: 0.966269
MAE train: 0.347467	val: 0.842083	test: 0.773524

Epoch: 70
Loss: 0.26307076109307154
RMSE train: 0.395159	val: 1.065745	test: 0.968527
MAE train: 0.310394	val: 0.843209	test: 0.764725

Epoch: 71
Loss: 0.28344912081956863
RMSE train: 0.355413	val: 1.045659	test: 0.964833
MAE train: 0.276655	val: 0.828082	test: 0.764689

Epoch: 72
Loss: 0.23615561425685883
RMSE train: 0.388933	val: 1.016569	test: 0.945256
MAE train: 0.308757	val: 0.804461	test: 0.757652

Epoch: 73
Loss: 0.23690295858042582
RMSE train: 0.356204	val: 1.117935	test: 1.006902
MAE train: 0.282475	val: 0.877424	test: 0.798399

Epoch: 74
Loss: 0.22380123181002481
RMSE train: 0.426848	val: 1.040335	test: 0.962312
MAE train: 0.347327	val: 0.824449	test: 0.767956

Epoch: 75
Loss: 0.2134193629026413
RMSE train: 0.343168	val: 1.100323	test: 0.997485
MAE train: 0.269774	val: 0.869601	test: 0.790980

Epoch: 76
Loss: 0.2065261464033808
RMSE train: 0.383118	val: 1.067944	test: 0.965083
MAE train: 0.307624	val: 0.842734	test: 0.767640

Epoch: 77
Loss: 0.20585068741015025
RMSE train: 0.340584	val: 1.114744	test: 0.996554
MAE train: 0.269002	val: 0.873798	test: 0.788535

Epoch: 78
Loss: 0.20403013804129191
RMSE train: 0.357527	val: 1.092921	test: 0.991877
MAE train: 0.286190	val: 0.862924	test: 0.782170

Epoch: 79
Loss: 0.20814066380262375
RMSE train: 0.317672	val: 1.091144	test: 0.976864
MAE train: 0.251929	val: 0.856057	test: 0.774712

Epoch: 80
Loss: 0.2104638389178685
RMSE train: 0.331286	val: 1.044140	test: 0.944704
MAE train: 0.264122	val: 0.827280	test: 0.752137

Epoch: 81
Loss: 0.19892981116260802
RMSE train: 0.336967	val: 1.075497	test: 0.971757
MAE train: 0.269204	val: 0.847463	test: 0.770948

Epoch: 82
Loss: 0.1986799995814051
RMSE train: 0.346913	val: 1.083671	test: 0.985288

Epoch: 22
Loss: 0.7343322719846453
RMSE train: 0.749101	val: 0.999358	test: 0.939751
MAE train: 0.598566	val: 0.788132	test: 0.758505

Epoch: 23
Loss: 0.6971932436738696
RMSE train: 0.733929	val: 0.983792	test: 0.927253
MAE train: 0.586018	val: 0.777866	test: 0.740651

Epoch: 24
Loss: 0.6785975311483655
RMSE train: 0.737858	val: 0.971628	test: 0.911092
MAE train: 0.585097	val: 0.763511	test: 0.716146

Epoch: 25
Loss: 0.672896500144686
RMSE train: 0.769388	val: 1.054832	test: 0.983427
MAE train: 0.610290	val: 0.837178	test: 0.779727

Epoch: 26
Loss: 0.6665385280336652
RMSE train: 0.717848	val: 1.067478	test: 0.990234
MAE train: 0.568722	val: 0.841262	test: 0.786761

Epoch: 27
Loss: 0.6395366702760968
RMSE train: 0.683697	val: 0.982432	test: 0.925234
MAE train: 0.547846	val: 0.771598	test: 0.731064

Epoch: 28
Loss: 0.663363094840731
RMSE train: 0.718937	val: 1.025172	test: 0.970654
MAE train: 0.571108	val: 0.807407	test: 0.764196

Epoch: 29
Loss: 0.6258609976087298
RMSE train: 0.674367	val: 0.947527	test: 0.909959
MAE train: 0.544574	val: 0.748029	test: 0.719615

Epoch: 30
Loss: 0.5935027854783195
RMSE train: 0.698362	val: 0.993902	test: 0.919430
MAE train: 0.553295	val: 0.780498	test: 0.732866

Epoch: 31
Loss: 0.5872389887060437
RMSE train: 0.654632	val: 0.952237	test: 0.909858
MAE train: 0.522043	val: 0.757039	test: 0.717516

Epoch: 32
Loss: 0.5514734515122005
RMSE train: 0.677856	val: 0.983546	test: 0.906330
MAE train: 0.538274	val: 0.778706	test: 0.720573

Epoch: 33
Loss: 0.5505491239683968
RMSE train: 0.621625	val: 0.981237	test: 0.912155
MAE train: 0.495588	val: 0.768007	test: 0.721449

Epoch: 34
Loss: 0.5444272820438657
RMSE train: 0.661318	val: 0.979815	test: 0.917528
MAE train: 0.526993	val: 0.769845	test: 0.725045

Epoch: 35
Loss: 0.5317295087235314
RMSE train: 0.619605	val: 0.984723	test: 0.920235
MAE train: 0.492745	val: 0.771771	test: 0.721440

Epoch: 36
Loss: 0.5310208116258893
RMSE train: 0.685464	val: 1.025444	test: 0.958881
MAE train: 0.540757	val: 0.809421	test: 0.749066

Epoch: 37
Loss: 0.5427771572555814
RMSE train: 0.622718	val: 1.011900	test: 0.963561
MAE train: 0.492952	val: 0.795221	test: 0.751447

Epoch: 38
Loss: 0.5109150792871203
RMSE train: 0.629540	val: 0.955886	test: 0.894652
MAE train: 0.500091	val: 0.755905	test: 0.706931

Epoch: 39
Loss: 0.5147598917995181
RMSE train: 0.573514	val: 0.967581	test: 0.913027
MAE train: 0.455644	val: 0.757697	test: 0.717467

Epoch: 40
Loss: 0.47349581122398376
RMSE train: 0.580245	val: 1.005960	test: 0.951966
MAE train: 0.460745	val: 0.794792	test: 0.746904

Epoch: 41
Loss: 0.48174415954521727
RMSE train: 0.559365	val: 0.955404	test: 0.924145
MAE train: 0.445805	val: 0.747684	test: 0.720837

Epoch: 42
Loss: 0.46214356805597034
RMSE train: 0.605560	val: 1.026041	test: 0.972793
MAE train: 0.481235	val: 0.810926	test: 0.757097

Epoch: 43
Loss: 0.4567690683262689
RMSE train: 0.592114	val: 0.976129	test: 0.931666
MAE train: 0.469115	val: 0.770762	test: 0.727172

Epoch: 44
Loss: 0.45386821244444164
RMSE train: 0.578524	val: 0.972463	test: 0.906029
MAE train: 0.458917	val: 0.759659	test: 0.721975

Epoch: 45
Loss: 0.43645169905253817
RMSE train: 0.567720	val: 0.965797	test: 0.914628
MAE train: 0.450390	val: 0.754911	test: 0.725099

Epoch: 46
Loss: 0.4178119408232825
RMSE train: 0.510808	val: 0.944583	test: 0.909988
MAE train: 0.407320	val: 0.744055	test: 0.719728

Epoch: 47
Loss: 0.4036560867513929
RMSE train: 0.516101	val: 0.961149	test: 0.908484
MAE train: 0.408883	val: 0.755053	test: 0.711710

Epoch: 48
Loss: 0.39248836253370556
RMSE train: 0.498197	val: 0.932929	test: 0.894616
MAE train: 0.396312	val: 0.735561	test: 0.704689

Epoch: 49
Loss: 0.3996627437216895
RMSE train: 0.551531	val: 0.944641	test: 0.894204
MAE train: 0.436736	val: 0.744146	test: 0.699240

Epoch: 50
Loss: 0.3799812900168555
RMSE train: 0.539279	val: 0.972672	test: 0.911336
MAE train: 0.426617	val: 0.763550	test: 0.714004

Epoch: 51
Loss: 0.3867442288569042
RMSE train: 0.530892	val: 0.973610	test: 0.899477
MAE train: 0.422042	val: 0.762233	test: 0.711263

Epoch: 52
Loss: 0.36388249908174786
RMSE train: 0.485621	val: 0.940067	test: 0.880938
MAE train: 0.386675	val: 0.737694	test: 0.705304

Epoch: 53
Loss: 0.35781159571238924
RMSE train: 0.479625	val: 0.947259	test: 0.893127
MAE train: 0.382201	val: 0.740312	test: 0.713843

Epoch: 54
Loss: 0.34597622070993694
RMSE train: 0.476144	val: 0.944011	test: 0.881448
MAE train: 0.380030	val: 0.741463	test: 0.694686

Epoch: 55
Loss: 0.348358011671475
RMSE train: 0.456010	val: 0.915567	test: 0.875573
MAE train: 0.362929	val: 0.723035	test: 0.701177

Epoch: 56
Loss: 0.34637873087610516
RMSE train: 0.493845	val: 0.936616	test: 0.883876
MAE train: 0.394018	val: 0.737536	test: 0.702789

Epoch: 57
Loss: 0.3606673798390797
RMSE train: 0.454035	val: 0.930089	test: 0.878197
MAE train: 0.361639	val: 0.737445	test: 0.696940

Epoch: 58
Loss: 0.33151456500802723
RMSE train: 0.477749	val: 0.958792	test: 0.886888
MAE train: 0.378806	val: 0.757326	test: 0.700851

Epoch: 59
Loss: 0.3178078361919948
RMSE train: 0.456155	val: 0.946268	test: 0.887820
MAE train: 0.361600	val: 0.747583	test: 0.700407

Epoch: 60
Loss: 0.31037142659936634
RMSE train: 0.441312	val: 0.953109	test: 0.897612
MAE train: 0.348677	val: 0.752807	test: 0.708676

Epoch: 61
Loss: 0.3019481471606663
RMSE train: 0.447153	val: 0.938970	test: 0.888733
MAE train: 0.352232	val: 0.742177	test: 0.705705

Epoch: 62
Loss: 0.2891309154885156
RMSE train: 0.425781	val: 0.928388	test: 0.877075
MAE train: 0.338870	val: 0.734761	test: 0.704724

Epoch: 63
Loss: 0.28171738237142563
RMSE train: 0.455451	val: 0.943761	test: 0.889229
MAE train: 0.359847	val: 0.743917	test: 0.709528

Epoch: 64
Loss: 0.2758426400167601
RMSE train: 0.431462	val: 0.945872	test: 0.886046
MAE train: 0.342385	val: 0.745034	test: 0.705534

Epoch: 65
Loss: 0.27400076921497074
RMSE train: 0.522398	val: 0.981138	test: 0.907010
MAE train: 0.411627	val: 0.770625	test: 0.708821

Epoch: 66
Loss: 0.2727403108562742
RMSE train: 0.419454	val: 0.982121	test: 0.915093
MAE train: 0.332520	val: 0.776517	test: 0.727859

Epoch: 67
Loss: 0.27947159111499786
RMSE train: 0.425673	val: 0.962725	test: 0.882153
MAE train: 0.339947	val: 0.755234	test: 0.694234

Epoch: 68
Loss: 0.2712512654917581
RMSE train: 0.424826	val: 0.978586	test: 0.902445
MAE train: 0.335182	val: 0.767517	test: 0.708161

Epoch: 69
Loss: 0.25546498064483913
RMSE train: 0.419195	val: 0.968198	test: 0.892175
MAE train: 0.330007	val: 0.762298	test: 0.705984

Epoch: 70
Loss: 0.25447327217885424
RMSE train: 0.452242	val: 0.950810	test: 0.880908
MAE train: 0.358600	val: 0.746895	test: 0.702247

Epoch: 71
Loss: 0.2675088441797665
RMSE train: 0.392569	val: 0.965429	test: 0.903266
MAE train: 0.308599	val: 0.757133	test: 0.713563

Epoch: 72
Loss: 0.24406297185591289
RMSE train: 0.456529	val: 0.960699	test: 0.879604
MAE train: 0.360605	val: 0.751329	test: 0.699845

Epoch: 73
Loss: 0.24292233373437608
RMSE train: 0.394799	val: 0.928821	test: 0.868387
MAE train: 0.314092	val: 0.727406	test: 0.692527

Epoch: 74
Loss: 0.24111489419426238
RMSE train: 0.391807	val: 0.951327	test: 0.886243
MAE train: 0.309723	val: 0.747150	test: 0.701357

Epoch: 75
Loss: 0.23911863565444946
RMSE train: 0.417533	val: 0.944874	test: 0.882788
MAE train: 0.329902	val: 0.739182	test: 0.695587

Epoch: 76
Loss: 0.2502579391002655
RMSE train: 0.459237	val: 0.982664	test: 0.907000
MAE train: 0.366284	val: 0.770770	test: 0.714470

Epoch: 77
Loss: 0.24128937082631247
RMSE train: 0.465307	val: 1.012099	test: 0.928071
MAE train: 0.369298	val: 0.803134	test: 0.738769

Epoch: 78
Loss: 0.23436009671006883
RMSE train: 0.407438	val: 0.948541	test: 0.875467
MAE train: 0.325915	val: 0.741771	test: 0.692129

Epoch: 79
Loss: 0.21840171622378485
RMSE train: 0.407736	val: 0.985469	test: 0.909676
MAE train: 0.324727	val: 0.776016	test: 0.715077

Epoch: 80
Loss: 0.20921091096741812
RMSE train: 0.375764	val: 0.974205	test: 0.892808
MAE train: 0.296482	val: 0.767487	test: 0.705458

Epoch: 81
Loss: 0.22032020560332707
RMSE train: 0.377944	val: 0.942622	test: 0.881461
MAE train: 0.301504	val: 0.739986	test: 0.701407

Epoch: 82
Loss: 0.212874670113836
RMSE train: 0.367456	val: 0.950313	test: 0.894020

Epoch: 22
Loss: 0.7074355866227832
RMSE train: 0.770711	val: 0.997747	test: 0.913098
MAE train: 0.615408	val: 0.782890	test: 0.738574

Epoch: 23
Loss: 0.73824571285929
RMSE train: 0.748121	val: 0.997155	test: 0.911114
MAE train: 0.595323	val: 0.774232	test: 0.735478

Epoch: 24
Loss: 0.7122484786169869
RMSE train: 0.770740	val: 0.987863	test: 0.902600
MAE train: 0.611719	val: 0.771472	test: 0.725969

Epoch: 25
Loss: 0.6663741852555957
RMSE train: 0.728278	val: 0.983128	test: 0.907195
MAE train: 0.580333	val: 0.768355	test: 0.727822

Epoch: 26
Loss: 0.662164432661874
RMSE train: 0.721943	val: 1.013930	test: 0.947193
MAE train: 0.574203	val: 0.788076	test: 0.767539

Epoch: 27
Loss: 0.6730258294514248
RMSE train: 0.727196	val: 0.951866	test: 0.873524
MAE train: 0.583259	val: 0.755190	test: 0.696926

Epoch: 28
Loss: 0.6139936404568809
RMSE train: 0.695201	val: 0.978144	test: 0.894975
MAE train: 0.548771	val: 0.768955	test: 0.713029

Epoch: 29
Loss: 0.6010256090334484
RMSE train: 0.688482	val: 0.952691	test: 0.871505
MAE train: 0.551420	val: 0.747298	test: 0.701829

Epoch: 30
Loss: 0.5983449965715408
RMSE train: 0.681397	val: 0.945207	test: 0.870787
MAE train: 0.547635	val: 0.744890	test: 0.701401

Epoch: 31
Loss: 0.543082628931318
RMSE train: 0.677781	val: 0.985451	test: 0.919819
MAE train: 0.542648	val: 0.775631	test: 0.731071

Epoch: 32
Loss: 0.5906976205962045
RMSE train: 0.691075	val: 0.943176	test: 0.863265
MAE train: 0.556994	val: 0.744885	test: 0.691753

Epoch: 33
Loss: 0.5803512781858444
RMSE train: 0.644383	val: 0.981445	test: 0.904442
MAE train: 0.515827	val: 0.764820	test: 0.724591

Epoch: 34
Loss: 0.5406893491744995
RMSE train: 0.668635	val: 0.973114	test: 0.900154
MAE train: 0.538708	val: 0.769765	test: 0.719259

Epoch: 35
Loss: 0.5519460822854724
RMSE train: 0.627917	val: 0.947381	test: 0.876738
MAE train: 0.501012	val: 0.744349	test: 0.705257

Epoch: 36
Loss: 0.5359638290745872
RMSE train: 0.617073	val: 0.979506	test: 0.888151
MAE train: 0.491436	val: 0.773189	test: 0.705354

Epoch: 37
Loss: 0.5119082842554364
RMSE train: 0.613823	val: 0.980870	test: 0.894938
MAE train: 0.490159	val: 0.766065	test: 0.721553

Epoch: 38
Loss: 0.4952187091112137
RMSE train: 0.590847	val: 0.986120	test: 0.909130
MAE train: 0.467059	val: 0.766974	test: 0.727964

Epoch: 39
Loss: 0.46127777865954805
RMSE train: 0.622179	val: 1.009713	test: 0.920699
MAE train: 0.492592	val: 0.783715	test: 0.736502

Epoch: 40
Loss: 0.464361880506788
RMSE train: 0.557390	val: 0.951695	test: 0.879689
MAE train: 0.443967	val: 0.742108	test: 0.710437

Epoch: 41
Loss: 0.46897745558193754
RMSE train: 0.580202	val: 0.997182	test: 0.903114
MAE train: 0.464028	val: 0.774212	test: 0.728561

Epoch: 42
Loss: 0.4459001464503152
RMSE train: 0.568723	val: 1.000918	test: 0.915311
MAE train: 0.450069	val: 0.784317	test: 0.726734

Epoch: 43
Loss: 0.4430002783026014
RMSE train: 0.593496	val: 0.966456	test: 0.886645
MAE train: 0.471336	val: 0.761405	test: 0.695205

Epoch: 44
Loss: 0.42381844776017324
RMSE train: 0.568899	val: 0.937126	test: 0.871245
MAE train: 0.455100	val: 0.742778	test: 0.706199

Epoch: 45
Loss: 0.416913132582392
RMSE train: 0.529582	val: 0.996685	test: 0.910824
MAE train: 0.420905	val: 0.775269	test: 0.737149

Epoch: 46
Loss: 0.4119926967791149
RMSE train: 0.565132	val: 0.950907	test: 0.878687
MAE train: 0.454748	val: 0.746289	test: 0.714092

Epoch: 47
Loss: 0.4111275907073702
RMSE train: 0.525227	val: 0.972344	test: 0.897395
MAE train: 0.416037	val: 0.758381	test: 0.716972

Epoch: 48
Loss: 0.4068629486220224
RMSE train: 0.570408	val: 1.003918	test: 0.910200
MAE train: 0.451579	val: 0.780900	test: 0.731993

Epoch: 49
Loss: 0.36827907604830606
RMSE train: 0.505923	val: 0.951963	test: 0.877483
MAE train: 0.399619	val: 0.744153	test: 0.709101

Epoch: 50
Loss: 0.38236865401268005
RMSE train: 0.526335	val: 0.931645	test: 0.879283
MAE train: 0.418502	val: 0.742684	test: 0.707034

Epoch: 51
Loss: 0.3840147022690092
RMSE train: 0.482735	val: 0.914797	test: 0.859067
MAE train: 0.382994	val: 0.722908	test: 0.692308

Epoch: 52
Loss: 0.333606410239424
RMSE train: 0.502769	val: 0.959321	test: 0.874430
MAE train: 0.402557	val: 0.753301	test: 0.703286

Epoch: 53
Loss: 0.3338570701224463
RMSE train: 0.473694	val: 0.936515	test: 0.869897
MAE train: 0.376492	val: 0.739247	test: 0.702165

Epoch: 54
Loss: 0.34577556805951254
RMSE train: 0.528005	val: 0.967400	test: 0.884595
MAE train: 0.421173	val: 0.753542	test: 0.711523

Epoch: 55
Loss: 0.3381933548620769
RMSE train: 0.482166	val: 0.936092	test: 0.875004
MAE train: 0.384091	val: 0.741244	test: 0.706813

Epoch: 56
Loss: 0.3309954489980425
RMSE train: 0.494069	val: 1.017756	test: 0.914418
MAE train: 0.393206	val: 0.791613	test: 0.725003

Epoch: 57
Loss: 0.31852681934833527
RMSE train: 0.468421	val: 0.927578	test: 0.867964
MAE train: 0.372432	val: 0.735134	test: 0.701649

Epoch: 58
Loss: 0.31567859649658203
RMSE train: 0.462303	val: 0.957728	test: 0.866301
MAE train: 0.370009	val: 0.753266	test: 0.703896

Epoch: 59
Loss: 0.30044945648738314
RMSE train: 0.435298	val: 0.968943	test: 0.878180
MAE train: 0.347190	val: 0.757627	test: 0.704497

Epoch: 60
Loss: 0.30299636508737293
RMSE train: 0.436529	val: 0.973990	test: 0.887340
MAE train: 0.348152	val: 0.764115	test: 0.715924

Epoch: 61
Loss: 0.3157385630267007
RMSE train: 0.441578	val: 0.911942	test: 0.852273
MAE train: 0.351381	val: 0.727906	test: 0.688862

Epoch: 62
Loss: 0.28439605236053467
RMSE train: 0.427153	val: 0.948565	test: 0.867408
MAE train: 0.340086	val: 0.747110	test: 0.696774

Epoch: 63
Loss: 0.285948959844453
RMSE train: 0.423859	val: 0.920407	test: 0.847936
MAE train: 0.337465	val: 0.733052	test: 0.687412

Epoch: 64
Loss: 0.2770280124885695
RMSE train: 0.394145	val: 0.955202	test: 0.872138
MAE train: 0.312291	val: 0.745774	test: 0.696733

Epoch: 65
Loss: 0.2792114040681294
RMSE train: 0.441339	val: 0.952226	test: 0.861102
MAE train: 0.352138	val: 0.750196	test: 0.685646

Epoch: 66
Loss: 0.2653397002390453
RMSE train: 0.453498	val: 0.936934	test: 0.862883
MAE train: 0.361989	val: 0.748708	test: 0.702454

Epoch: 67
Loss: 0.2675365262797901
RMSE train: 0.393995	val: 0.955728	test: 0.868266
MAE train: 0.311930	val: 0.757321	test: 0.697227

Epoch: 68
Loss: 0.24908043763467244
RMSE train: 0.426437	val: 0.914173	test: 0.858237
MAE train: 0.339475	val: 0.731058	test: 0.688996

Epoch: 69
Loss: 0.24285476548331125
RMSE train: 0.436672	val: 0.994641	test: 0.894985
MAE train: 0.348202	val: 0.781674	test: 0.721601

Epoch: 70
Loss: 0.26756331005266737
RMSE train: 0.377514	val: 0.931321	test: 0.857846
MAE train: 0.299217	val: 0.736851	test: 0.692143

Epoch: 71
Loss: 0.2994310642991747
RMSE train: 0.385203	val: 0.925329	test: 0.854877
MAE train: 0.304069	val: 0.736708	test: 0.684278

Epoch: 72
Loss: 0.2588085872786386
RMSE train: 0.420625	val: 0.957139	test: 0.874567
MAE train: 0.331986	val: 0.748883	test: 0.707826

Epoch: 73
Loss: 0.24334824830293655
RMSE train: 0.410512	val: 0.998893	test: 0.901302
MAE train: 0.324354	val: 0.780121	test: 0.725695

Epoch: 74
Loss: 0.2264352770788329
RMSE train: 0.406444	val: 0.952373	test: 0.873207
MAE train: 0.317331	val: 0.750765	test: 0.693427

Epoch: 75
Loss: 0.22783090599945613
RMSE train: 0.378318	val: 0.934933	test: 0.863411
MAE train: 0.299779	val: 0.737232	test: 0.695499

Epoch: 76
Loss: 0.22885909037930624
RMSE train: 0.375857	val: 0.986082	test: 0.896839
MAE train: 0.297319	val: 0.773081	test: 0.715907

Epoch: 77
Loss: 0.2253223466021674
RMSE train: 0.390235	val: 0.915401	test: 0.854240
MAE train: 0.311814	val: 0.728117	test: 0.686878

Epoch: 78
Loss: 0.21571368404797145
RMSE train: 0.406656	val: 0.993269	test: 0.886865
MAE train: 0.320468	val: 0.779270	test: 0.708874

Epoch: 79
Loss: 0.2162692951304572
RMSE train: 0.413600	val: 0.980100	test: 0.889051
MAE train: 0.330413	val: 0.765109	test: 0.705714

Epoch: 80
Loss: 0.22142798134258815
RMSE train: 0.383560	val: 0.931114	test: 0.854565
MAE train: 0.305795	val: 0.737601	test: 0.688232

Epoch: 81
Loss: 0.2205143336738859
RMSE train: 0.371987	val: 0.928192	test: 0.853387
MAE train: 0.296932	val: 0.732724	test: 0.692132

Epoch: 82
Loss: 0.21202806809118815
RMSE train: 0.413734	val: 0.935996	test: 0.858301

Epoch: 22
Loss: 0.7958157956600189
RMSE train: 0.772567	val: 1.051746	test: 1.046764
MAE train: 0.622859	val: 0.858958	test: 0.841581

Epoch: 23
Loss: 0.7810074687004089
RMSE train: 0.768149	val: 1.069224	test: 1.043048
MAE train: 0.620100	val: 0.864453	test: 0.841253

Epoch: 24
Loss: 0.7228333055973053
RMSE train: 0.759387	val: 1.098096	test: 1.105664
MAE train: 0.607750	val: 0.886907	test: 0.861224

Epoch: 25
Loss: 0.7182830316679818
RMSE train: 0.745728	val: 1.078774	test: 1.044500
MAE train: 0.597756	val: 0.865046	test: 0.841735

Epoch: 26
Loss: 0.7275741355759757
RMSE train: 0.731041	val: 1.171515	test: 1.163340
MAE train: 0.593160	val: 0.962085	test: 0.940793

Epoch: 27
Loss: 0.7026119615350451
RMSE train: 0.713862	val: 1.071792	test: 1.063580
MAE train: 0.571418	val: 0.859257	test: 0.838802

Epoch: 28
Loss: 0.6656859389373234
RMSE train: 0.718878	val: 1.047486	test: 1.013165
MAE train: 0.575473	val: 0.838979	test: 0.821569

Epoch: 29
Loss: 0.6480988902705056
RMSE train: 0.679274	val: 1.085984	test: 1.087662
MAE train: 0.545987	val: 0.888733	test: 0.874950

Epoch: 30
Loss: 0.6395740509033203
RMSE train: 0.694450	val: 1.035060	test: 1.006461
MAE train: 0.556178	val: 0.839447	test: 0.824248

Epoch: 31
Loss: 0.5930714436939785
RMSE train: 0.657007	val: 1.031302	test: 1.015259
MAE train: 0.523277	val: 0.828929	test: 0.819987

Epoch: 32
Loss: 0.579448755298342
RMSE train: 0.649978	val: 1.042485	test: 1.005527
MAE train: 0.519467	val: 0.826318	test: 0.822276

Epoch: 33
Loss: 0.5651313705103738
RMSE train: 0.614429	val: 1.086448	test: 1.050005
MAE train: 0.492085	val: 0.866851	test: 0.864343

Epoch: 34
Loss: 0.5724564152104514
RMSE train: 0.631371	val: 1.070261	test: 1.066896
MAE train: 0.508493	val: 0.889051	test: 0.866031

Epoch: 35
Loss: 0.5515576303005219
RMSE train: 0.598583	val: 1.021123	test: 0.998249
MAE train: 0.477586	val: 0.826939	test: 0.811793

Epoch: 36
Loss: 0.5416390980992999
RMSE train: 0.652435	val: 1.063488	test: 1.053168
MAE train: 0.518661	val: 0.853982	test: 0.849994

Epoch: 37
Loss: 0.5088538250752858
RMSE train: 0.572499	val: 1.031827	test: 1.004152
MAE train: 0.457687	val: 0.835107	test: 0.822692

Epoch: 38
Loss: 0.4864345065185002
RMSE train: 0.600640	val: 1.118410	test: 1.149467
MAE train: 0.479518	val: 0.922624	test: 0.926388

Epoch: 39
Loss: 0.49116495890276773
RMSE train: 0.577340	val: 1.050607	test: 1.021929
MAE train: 0.457890	val: 0.848902	test: 0.838597

Epoch: 40
Loss: 0.46308474242687225
RMSE train: 0.574740	val: 1.095748	test: 1.091326
MAE train: 0.460357	val: 0.904884	test: 0.900180

Epoch: 41
Loss: 0.4931341367108481
RMSE train: 0.535704	val: 1.064139	test: 1.098932
MAE train: 0.427138	val: 0.863999	test: 0.886686

Epoch: 42
Loss: 0.4611936105149133
RMSE train: 0.553000	val: 1.127034	test: 1.172539
MAE train: 0.439377	val: 0.937076	test: 0.956156

Epoch: 43
Loss: 0.43379215470382143
RMSE train: 0.510621	val: 1.039185	test: 1.040455
MAE train: 0.405628	val: 0.837633	test: 0.845771

Epoch: 44
Loss: 0.4340377300977707
RMSE train: 0.530934	val: 1.047335	test: 1.062711
MAE train: 0.421623	val: 0.856850	test: 0.871747

Epoch: 45
Loss: 0.3875270996774946
RMSE train: 0.543376	val: 1.043509	test: 1.048290
MAE train: 0.432028	val: 0.848561	test: 0.848438

Epoch: 46
Loss: 0.40353648364543915
RMSE train: 0.494832	val: 1.039744	test: 1.036276
MAE train: 0.393143	val: 0.840532	test: 0.838653

Epoch: 47
Loss: 0.40958742584500996
RMSE train: 0.505505	val: 1.031263	test: 1.030433
MAE train: 0.404832	val: 0.836163	test: 0.836527

Epoch: 48
Loss: 0.38638743119580404
RMSE train: 0.499060	val: 1.042634	test: 1.031083
MAE train: 0.398856	val: 0.849253	test: 0.838987

Epoch: 49
Loss: 0.37410004436969757
RMSE train: 0.489010	val: 1.040332	test: 1.047493
MAE train: 0.389858	val: 0.847666	test: 0.850266

Epoch: 50
Loss: 0.3608042023011616
RMSE train: 0.480648	val: 1.075371	test: 1.077458
MAE train: 0.381622	val: 0.883146	test: 0.880419

Epoch: 51
Loss: 0.3793118787663324
RMSE train: 0.471745	val: 1.068812	test: 1.071496
MAE train: 0.372603	val: 0.884645	test: 0.875641

Epoch: 52
Loss: 0.35408499198300497
RMSE train: 0.483478	val: 1.030007	test: 1.018678
MAE train: 0.386317	val: 0.840628	test: 0.830383

Epoch: 53
Loss: 0.3459783302886145
RMSE train: 0.482231	val: 1.063624	test: 1.077566
MAE train: 0.386209	val: 0.871319	test: 0.871720

Epoch: 54
Loss: 0.3325103925807135
RMSE train: 0.532321	val: 1.039304	test: 1.049637
MAE train: 0.430474	val: 0.850134	test: 0.856026

Epoch: 55
Loss: 0.3358179543699537
RMSE train: 0.437789	val: 1.054880	test: 1.067065
MAE train: 0.350449	val: 0.860143	test: 0.859738

Epoch: 56
Loss: 0.32234208072934833
RMSE train: 0.474672	val: 1.033579	test: 1.021346
MAE train: 0.380893	val: 0.842375	test: 0.836412

Epoch: 57
Loss: 0.31964272260665894
RMSE train: 0.424130	val: 1.026068	test: 1.038491
MAE train: 0.338094	val: 0.830730	test: 0.834991

Epoch: 58
Loss: 0.3087875949484961
RMSE train: 0.457049	val: 1.062398	test: 1.095524
MAE train: 0.364879	val: 0.866218	test: 0.875715

Epoch: 59
Loss: 0.31298409402370453
RMSE train: 0.445539	val: 1.041855	test: 1.076172
MAE train: 0.351719	val: 0.855170	test: 0.867279

Epoch: 60
Loss: 0.29760512709617615
RMSE train: 0.437959	val: 1.026024	test: 1.042550
MAE train: 0.348525	val: 0.840506	test: 0.848514

Epoch: 61
Loss: 0.2811885007790157
RMSE train: 0.418263	val: 1.014247	test: 1.015185
MAE train: 0.332139	val: 0.829378	test: 0.831018

Epoch: 62
Loss: 0.26425545662641525
RMSE train: 0.430497	val: 1.017795	test: 1.009608
MAE train: 0.343059	val: 0.823469	test: 0.821245

Epoch: 63
Loss: 0.2791192339999335
RMSE train: 0.389791	val: 1.004264	test: 0.988356
MAE train: 0.308034	val: 0.807622	test: 0.801731

Epoch: 64
Loss: 0.25835011367286953
RMSE train: 0.418225	val: 1.043566	test: 1.042450
MAE train: 0.333768	val: 0.858520	test: 0.842077

Epoch: 65
Loss: 0.2639917550342424
RMSE train: 0.440596	val: 1.073997	test: 1.108084
MAE train: 0.348518	val: 0.885825	test: 0.897702

Epoch: 66
Loss: 0.2661395211304937
RMSE train: 0.382780	val: 1.069525	test: 1.099800
MAE train: 0.304154	val: 0.877444	test: 0.883378

Epoch: 67
Loss: 0.2756391923342432
RMSE train: 0.459349	val: 1.139404	test: 1.192311
MAE train: 0.370477	val: 0.944605	test: 0.964445

Epoch: 68
Loss: 0.2579203737633569
RMSE train: 0.406584	val: 1.038035	test: 1.047347
MAE train: 0.321560	val: 0.844180	test: 0.851093

Epoch: 69
Loss: 0.2460607247693198
RMSE train: 0.400708	val: 1.134483	test: 1.182491
MAE train: 0.322184	val: 0.939939	test: 0.949223

Epoch: 70
Loss: 0.2507919041173799
RMSE train: 0.427959	val: 1.019736	test: 1.005861
MAE train: 0.341229	val: 0.820512	test: 0.812132

Epoch: 71
Loss: 0.2359611423952239
RMSE train: 0.373765	val: 1.017408	test: 1.020590
MAE train: 0.296046	val: 0.818892	test: 0.827249

Epoch: 72
Loss: 0.22866882916007722
RMSE train: 0.376307	val: 1.023359	test: 1.012528
MAE train: 0.300630	val: 0.830409	test: 0.821368

Epoch: 73
Loss: 0.2251064298408372
RMSE train: 0.354315	val: 1.026587	test: 1.033179
MAE train: 0.282108	val: 0.834008	test: 0.837885

Epoch: 74
Loss: 0.23697121973548615
RMSE train: 0.389225	val: 1.075579	test: 1.100928
MAE train: 0.312949	val: 0.885731	test: 0.892014

Epoch: 75
Loss: 0.2384869722383363
RMSE train: 0.371897	val: 1.071839	test: 1.097838
MAE train: 0.298080	val: 0.883780	test: 0.893271

Epoch: 76
Loss: 0.22964448588235037
RMSE train: 0.375197	val: 1.027321	test: 1.040569
MAE train: 0.296793	val: 0.839404	test: 0.848126

Epoch: 77
Loss: 0.20839134603738785
RMSE train: 0.390172	val: 1.034827	test: 1.051470
MAE train: 0.310885	val: 0.850256	test: 0.849100

Epoch: 78
Loss: 0.20572094938584737
RMSE train: 0.332819	val: 1.020484	test: 1.028710
MAE train: 0.263046	val: 0.830566	test: 0.829170

Epoch: 79
Loss: 0.20991001278162003
RMSE train: 0.325686	val: 1.028487	test: 1.036074
MAE train: 0.257876	val: 0.841233	test: 0.841305

Epoch: 80
Loss: 0.20692302925246103
RMSE train: 0.340642	val: 0.975874	test: 0.965445
MAE train: 0.270715	val: 0.784192	test: 0.789019

Epoch: 81
Loss: 0.19728908794266836
RMSE train: 0.377018	val: 1.030015	test: 1.077321
MAE train: 0.293706	val: 0.830569	test: 0.859726

Epoch: 82
Loss: 0.1850324049592018
RMSE train: 0.349673	val: 1.027802	test: 1.061032

Epoch: 22
Loss: 0.6882844652448382
RMSE train: 0.744705	val: 0.946574	test: 0.873018
MAE train: 0.592732	val: 0.754755	test: 0.691751

Epoch: 23
Loss: 0.6577944202082497
RMSE train: 0.724774	val: 0.922594	test: 0.861185
MAE train: 0.579056	val: 0.723336	test: 0.685029

Epoch: 24
Loss: 0.6557024078709739
RMSE train: 0.726730	val: 0.945493	test: 0.877893
MAE train: 0.580098	val: 0.741738	test: 0.697198

Epoch: 25
Loss: 0.6331050225666591
RMSE train: 0.683749	val: 0.925370	test: 0.864707
MAE train: 0.542540	val: 0.729750	test: 0.691486

Epoch: 26
Loss: 0.6232276814324516
RMSE train: 0.688087	val: 0.937999	test: 0.876962
MAE train: 0.547178	val: 0.735565	test: 0.712053

Epoch: 27
Loss: 0.6045511322362083
RMSE train: 0.675023	val: 0.902969	test: 0.844715
MAE train: 0.540689	val: 0.704763	test: 0.674902

Epoch: 28
Loss: 0.5880227195365089
RMSE train: 0.687329	val: 0.911070	test: 0.848420
MAE train: 0.551434	val: 0.726847	test: 0.672216

Epoch: 29
Loss: 0.5708093898636954
RMSE train: 0.658617	val: 0.921238	test: 0.869272
MAE train: 0.528867	val: 0.727076	test: 0.697748

Epoch: 30
Loss: 0.5521395312888282
RMSE train: 0.714977	val: 0.896441	test: 0.853745
MAE train: 0.575312	val: 0.704648	test: 0.689287

Epoch: 31
Loss: 0.5306248026234763
RMSE train: 0.716703	val: 0.896686	test: 0.869399
MAE train: 0.577081	val: 0.697581	test: 0.684683

Epoch: 32
Loss: 0.5619774652378899
RMSE train: 0.666730	val: 0.882881	test: 0.828367
MAE train: 0.536337	val: 0.698319	test: 0.661941

Epoch: 33
Loss: 0.5597989942346301
RMSE train: 0.650564	val: 0.902467	test: 0.842848
MAE train: 0.522647	val: 0.705395	test: 0.673959

Epoch: 34
Loss: 0.5152947774955204
RMSE train: 0.648752	val: 0.906578	test: 0.862516
MAE train: 0.519749	val: 0.709260	test: 0.686243

Epoch: 35
Loss: 0.5601298149142947
RMSE train: 0.663791	val: 0.906241	test: 0.838723
MAE train: 0.533477	val: 0.709418	test: 0.671095

Epoch: 36
Loss: 0.5405202337673732
RMSE train: 0.632502	val: 0.896296	test: 0.838781
MAE train: 0.509457	val: 0.705733	test: 0.663484

Epoch: 37
Loss: 0.5274315476417542
RMSE train: 0.625691	val: 0.921392	test: 0.849655
MAE train: 0.501603	val: 0.724648	test: 0.675727

Epoch: 38
Loss: 0.4853948801755905
RMSE train: 0.606565	val: 0.886859	test: 0.839463
MAE train: 0.486456	val: 0.689606	test: 0.671658

Epoch: 39
Loss: 0.488268962928227
RMSE train: 0.592780	val: 0.907120	test: 0.855417
MAE train: 0.475288	val: 0.708255	test: 0.693929

Epoch: 40
Loss: 0.4986452545438494
RMSE train: 0.589562	val: 0.886396	test: 0.845850
MAE train: 0.471518	val: 0.694994	test: 0.670358

Epoch: 41
Loss: 0.469309715288026
RMSE train: 0.611049	val: 0.895409	test: 0.853804
MAE train: 0.489143	val: 0.707595	test: 0.682399

Epoch: 42
Loss: 0.4569926304476602
RMSE train: 0.630341	val: 0.893403	test: 0.867081
MAE train: 0.509878	val: 0.706666	test: 0.685182

Epoch: 43
Loss: 0.4332036908183779
RMSE train: 0.595800	val: 0.888599	test: 0.834374
MAE train: 0.474915	val: 0.706855	test: 0.653435

Epoch: 44
Loss: 0.4475254224879401
RMSE train: 0.572843	val: 0.915039	test: 0.857412
MAE train: 0.457442	val: 0.718629	test: 0.686665

Epoch: 45
Loss: 0.426212483218738
RMSE train: 0.551516	val: 0.870792	test: 0.827486
MAE train: 0.441107	val: 0.682483	test: 0.662654

Epoch: 46
Loss: 0.4377807761941637
RMSE train: 0.576837	val: 0.883437	test: 0.838727
MAE train: 0.462675	val: 0.701412	test: 0.673497

Epoch: 47
Loss: 0.40739016022000996
RMSE train: 0.540463	val: 0.862262	test: 0.836468
MAE train: 0.431748	val: 0.672976	test: 0.666152

Epoch: 48
Loss: 0.403888681105205
RMSE train: 0.546255	val: 0.915480	test: 0.846472
MAE train: 0.437636	val: 0.722462	test: 0.676051

Epoch: 49
Loss: 0.41259089963776724
RMSE train: 0.531787	val: 0.852380	test: 0.828225
MAE train: 0.425862	val: 0.665501	test: 0.660236

Epoch: 50
Loss: 0.38415724677698954
RMSE train: 0.606708	val: 0.883788	test: 0.873688
MAE train: 0.492138	val: 0.702662	test: 0.702608

Epoch: 51
Loss: 0.3924188699041094
RMSE train: 0.536690	val: 0.865336	test: 0.830458
MAE train: 0.430928	val: 0.685656	test: 0.661368

Epoch: 52
Loss: 0.36829558440617155
RMSE train: 0.513537	val: 0.878194	test: 0.835480
MAE train: 0.410673	val: 0.687018	test: 0.668693

Epoch: 53
Loss: 0.3885201577629362
RMSE train: 0.504325	val: 0.875156	test: 0.854766
MAE train: 0.401746	val: 0.676195	test: 0.691061

Epoch: 54
Loss: 0.378260606101581
RMSE train: 0.573017	val: 0.899970	test: 0.866326
MAE train: 0.459767	val: 0.707567	test: 0.698441

Epoch: 55
Loss: 0.3808060203279768
RMSE train: 0.533371	val: 0.880243	test: 0.840312
MAE train: 0.427984	val: 0.694745	test: 0.670241

Epoch: 56
Loss: 0.37289399760110037
RMSE train: 0.523720	val: 0.883640	test: 0.855270
MAE train: 0.417774	val: 0.692992	test: 0.677365

Epoch: 57
Loss: 0.34563041371958597
RMSE train: 0.528753	val: 0.881485	test: 0.849217
MAE train: 0.425166	val: 0.689273	test: 0.677591

Epoch: 58
Loss: 0.3344035255057471
RMSE train: 0.482604	val: 0.875409	test: 0.823729
MAE train: 0.384060	val: 0.681635	test: 0.660008

Epoch: 59
Loss: 0.33137508375304087
RMSE train: 0.477374	val: 0.855515	test: 0.822256
MAE train: 0.381023	val: 0.663221	test: 0.651251

Epoch: 60
Loss: 0.3346348830631801
RMSE train: 0.491886	val: 0.859596	test: 0.829962
MAE train: 0.391574	val: 0.676513	test: 0.662188

Epoch: 61
Loss: 0.3378600946494511
RMSE train: 0.493049	val: 0.886576	test: 0.838404
MAE train: 0.392436	val: 0.696138	test: 0.680062

Epoch: 62
Loss: 0.3260721926178251
RMSE train: 0.503862	val: 0.862369	test: 0.831724
MAE train: 0.404962	val: 0.678516	test: 0.662818

Epoch: 63
Loss: 0.31238675117492676
RMSE train: 0.479837	val: 0.873152	test: 0.823900
MAE train: 0.383962	val: 0.689009	test: 0.664794

Epoch: 64
Loss: 0.32518582897526876
RMSE train: 0.481184	val: 0.853746	test: 0.817647
MAE train: 0.386991	val: 0.660802	test: 0.655691

Epoch: 65
Loss: 0.3043057194777897
RMSE train: 0.504229	val: 0.900219	test: 0.840033
MAE train: 0.400378	val: 0.714568	test: 0.671531

Epoch: 66
Loss: 0.30083837253706797
RMSE train: 0.496296	val: 0.859895	test: 0.831366
MAE train: 0.398599	val: 0.671517	test: 0.667192

Epoch: 67
Loss: 0.2943672112056187
RMSE train: 0.512195	val: 0.868625	test: 0.828629
MAE train: 0.413660	val: 0.683993	test: 0.656861

Epoch: 68
Loss: 0.2775465856705393
RMSE train: 0.459895	val: 0.867257	test: 0.827098
MAE train: 0.365603	val: 0.679643	test: 0.666417

Epoch: 69
Loss: 0.2711236391748701
RMSE train: 0.421072	val: 0.866839	test: 0.830726
MAE train: 0.332678	val: 0.674540	test: 0.667896

Epoch: 70
Loss: 0.2892904739294733
RMSE train: 0.439452	val: 0.863983	test: 0.807280
MAE train: 0.350581	val: 0.679142	test: 0.646948

Epoch: 71
Loss: 0.297853667821203
RMSE train: 0.472486	val: 0.871677	test: 0.835487
MAE train: 0.379229	val: 0.682504	test: 0.662798

Epoch: 72
Loss: 0.2892282839332308
RMSE train: 0.476736	val: 0.877888	test: 0.855411
MAE train: 0.379899	val: 0.688130	test: 0.686684

Epoch: 73
Loss: 0.3021079365696226
RMSE train: 0.468697	val: 0.864726	test: 0.808140
MAE train: 0.371301	val: 0.678009	test: 0.650603

Epoch: 74
Loss: 0.28486387112310957
RMSE train: 0.449466	val: 0.882195	test: 0.837493
MAE train: 0.354487	val: 0.695508	test: 0.675839

Epoch: 75
Loss: 0.2728562610489981
RMSE train: 0.463378	val: 0.875963	test: 0.830053
MAE train: 0.371041	val: 0.687852	test: 0.663446

Epoch: 76
Loss: 0.2571467086672783
RMSE train: 0.537140	val: 0.874297	test: 0.851928
MAE train: 0.443580	val: 0.680478	test: 0.676709

Epoch: 77
Loss: 0.2582218955670084
RMSE train: 0.430818	val: 0.912025	test: 0.854722
MAE train: 0.341763	val: 0.721972	test: 0.690915

Epoch: 78
Loss: 0.2587279751896858
RMSE train: 0.424354	val: 0.851906	test: 0.809202
MAE train: 0.337574	val: 0.667788	test: 0.642449

Epoch: 79
Loss: 0.2350482397845813
RMSE train: 0.434941	val: 0.860203	test: 0.825412
MAE train: 0.348474	val: 0.675925	test: 0.657581

Epoch: 80
Loss: 0.2519716907824789
RMSE train: 0.413941	val: 0.861754	test: 0.819182
MAE train: 0.329157	val: 0.673317	test: 0.658458

Epoch: 81
Loss: 0.25073755638939993
RMSE train: 0.412819	val: 0.881885	test: 0.831792
MAE train: 0.329366	val: 0.682574	test: 0.668531

Epoch: 82
Loss: 0.24226955217974527
RMSE train: 0.467192	val: 0.861844	test: 0.823478

Epoch: 22
Loss: 0.6899190417357853
RMSE train: 0.729844	val: 0.893326	test: 0.827494
MAE train: 0.573274	val: 0.704966	test: 0.655383

Epoch: 23
Loss: 0.6793935384069171
RMSE train: 0.743358	val: 0.941552	test: 0.863782
MAE train: 0.584578	val: 0.741412	test: 0.693338

Epoch: 24
Loss: 0.6241724193096161
RMSE train: 0.709851	val: 0.898791	test: 0.847791
MAE train: 0.559240	val: 0.700211	test: 0.666549

Epoch: 25
Loss: 0.6295754398618426
RMSE train: 0.710348	val: 0.889263	test: 0.821788
MAE train: 0.561594	val: 0.703894	test: 0.655369

Epoch: 26
Loss: 0.6332949272223881
RMSE train: 0.698849	val: 0.878932	test: 0.832054
MAE train: 0.548740	val: 0.693229	test: 0.665059

Epoch: 27
Loss: 0.6118396520614624
RMSE train: 0.706900	val: 0.906033	test: 0.851019
MAE train: 0.554586	val: 0.712531	test: 0.685566

Epoch: 28
Loss: 0.5782887552465711
RMSE train: 0.687485	val: 0.882764	test: 0.837708
MAE train: 0.538147	val: 0.692251	test: 0.669557

Epoch: 29
Loss: 0.5672211136136737
RMSE train: 0.648640	val: 0.858806	test: 0.818128
MAE train: 0.515181	val: 0.678829	test: 0.655379

Epoch: 30
Loss: 0.5733854430062431
RMSE train: 0.659619	val: 0.888268	test: 0.844216
MAE train: 0.521278	val: 0.697040	test: 0.676737

Epoch: 31
Loss: 0.590823403426579
RMSE train: 0.667221	val: 0.890366	test: 0.825604
MAE train: 0.524516	val: 0.696452	test: 0.652388

Epoch: 32
Loss: 0.5451844717775073
RMSE train: 0.652596	val: 0.945942	test: 0.881665
MAE train: 0.511343	val: 0.733673	test: 0.703755

Epoch: 33
Loss: 0.5192330309322902
RMSE train: 0.647542	val: 0.888035	test: 0.825372
MAE train: 0.512395	val: 0.690911	test: 0.652972

Epoch: 34
Loss: 0.50888805942876
RMSE train: 0.650749	val: 0.877374	test: 0.816928
MAE train: 0.512706	val: 0.692371	test: 0.656033

Epoch: 35
Loss: 0.4999322401625769
RMSE train: 0.630693	val: 0.873118	test: 0.827417
MAE train: 0.499164	val: 0.688257	test: 0.663241

Epoch: 36
Loss: 0.5114800206252507
RMSE train: 0.631559	val: 0.874651	test: 0.836027
MAE train: 0.504138	val: 0.684453	test: 0.666555

Epoch: 37
Loss: 0.5129629394837788
RMSE train: 0.631636	val: 0.857469	test: 0.820237
MAE train: 0.500566	val: 0.670863	test: 0.649218

Epoch: 38
Loss: 0.5079983238662992
RMSE train: 0.691718	val: 0.930411	test: 0.871844
MAE train: 0.549051	val: 0.729790	test: 0.695785

Epoch: 39
Loss: 0.4640862835305078
RMSE train: 0.655591	val: 0.941942	test: 0.868965
MAE train: 0.513937	val: 0.729779	test: 0.701374

Epoch: 40
Loss: 0.47070901095867157
RMSE train: 0.583079	val: 0.869948	test: 0.832016
MAE train: 0.459482	val: 0.672026	test: 0.662889

Epoch: 41
Loss: 0.47097095421382357
RMSE train: 0.580054	val: 0.846333	test: 0.820768
MAE train: 0.459227	val: 0.667525	test: 0.651012

Epoch: 42
Loss: 0.4744977674313954
RMSE train: 0.564357	val: 0.860056	test: 0.814026
MAE train: 0.446097	val: 0.676303	test: 0.645516

Epoch: 43
Loss: 0.4654221215418407
RMSE train: 0.592632	val: 0.863320	test: 0.816887
MAE train: 0.465266	val: 0.679364	test: 0.656196

Epoch: 44
Loss: 0.45049731007644106
RMSE train: 0.546984	val: 0.868449	test: 0.828079
MAE train: 0.427446	val: 0.678755	test: 0.663062

Epoch: 45
Loss: 0.4193892117057528
RMSE train: 0.617325	val: 0.895224	test: 0.823227
MAE train: 0.486437	val: 0.702664	test: 0.657002

Epoch: 46
Loss: 0.42432970234325956
RMSE train: 0.566494	val: 0.885209	test: 0.836277
MAE train: 0.444837	val: 0.698591	test: 0.674026

Epoch: 47
Loss: 0.4114631137677601
RMSE train: 0.534333	val: 0.856997	test: 0.831592
MAE train: 0.422146	val: 0.673000	test: 0.662304

Epoch: 48
Loss: 0.40923063669885906
RMSE train: 0.557337	val: 0.881114	test: 0.824340
MAE train: 0.440018	val: 0.692040	test: 0.664930

Epoch: 49
Loss: 0.40341588854789734
RMSE train: 0.525826	val: 0.856685	test: 0.823459
MAE train: 0.413908	val: 0.671036	test: 0.658799

Epoch: 50
Loss: 0.3735996208020619
RMSE train: 0.530444	val: 0.862078	test: 0.828657
MAE train: 0.416507	val: 0.667752	test: 0.670127

Epoch: 51
Loss: 0.39897713916642324
RMSE train: 0.514905	val: 0.866674	test: 0.821328
MAE train: 0.404083	val: 0.675212	test: 0.663261

Epoch: 52
Loss: 0.3591073283127376
RMSE train: 0.541314	val: 0.858329	test: 0.828902
MAE train: 0.429142	val: 0.671988	test: 0.660193

Epoch: 53
Loss: 0.37939334767205374
RMSE train: 0.500354	val: 0.855520	test: 0.826477
MAE train: 0.394485	val: 0.669240	test: 0.662136

Epoch: 54
Loss: 0.3601946532726288
RMSE train: 0.513704	val: 0.853268	test: 0.817779
MAE train: 0.402321	val: 0.668610	test: 0.656113

Epoch: 55
Loss: 0.3637414595910481
RMSE train: 0.518107	val: 0.854966	test: 0.812336
MAE train: 0.409565	val: 0.667067	test: 0.650561

Epoch: 56
Loss: 0.37967277211802347
RMSE train: 0.527557	val: 0.883974	test: 0.819989
MAE train: 0.418246	val: 0.687558	test: 0.651018

Epoch: 57
Loss: 0.369706038917814
RMSE train: 0.571740	val: 0.882133	test: 0.816113
MAE train: 0.454002	val: 0.686484	test: 0.657383

Epoch: 58
Loss: 0.3531546933310373
RMSE train: 0.514066	val: 0.862501	test: 0.808108
MAE train: 0.407099	val: 0.673517	test: 0.650559

Epoch: 59
Loss: 0.345531838280814
RMSE train: 0.487139	val: 0.853170	test: 0.822922
MAE train: 0.383651	val: 0.668316	test: 0.653546

Epoch: 60
Loss: 0.33255497898374287
RMSE train: 0.489232	val: 0.865983	test: 0.826136
MAE train: 0.389133	val: 0.679164	test: 0.662404

Epoch: 61
Loss: 0.3216710835695267
RMSE train: 0.460214	val: 0.843067	test: 0.823458
MAE train: 0.365077	val: 0.657226	test: 0.659817

Epoch: 62
Loss: 0.3142560635294233
RMSE train: 0.478728	val: 0.834289	test: 0.801586
MAE train: 0.379182	val: 0.662400	test: 0.638394

Epoch: 63
Loss: 0.3027951419353485
RMSE train: 0.501112	val: 0.876592	test: 0.827661
MAE train: 0.398506	val: 0.682631	test: 0.665385

Epoch: 64
Loss: 0.2928862507854189
RMSE train: 0.440701	val: 0.836727	test: 0.813481
MAE train: 0.347940	val: 0.655680	test: 0.646659

Epoch: 65
Loss: 0.30542931812150137
RMSE train: 0.463420	val: 0.855777	test: 0.805123
MAE train: 0.368111	val: 0.669428	test: 0.639219

Epoch: 66
Loss: 0.2925048066037042
RMSE train: 0.484086	val: 0.873834	test: 0.821083
MAE train: 0.382075	val: 0.685335	test: 0.661718

Epoch: 67
Loss: 0.29141709208488464
RMSE train: 0.439635	val: 0.869478	test: 0.825096
MAE train: 0.347252	val: 0.671220	test: 0.664983

Epoch: 68
Loss: 0.2862958993230547
RMSE train: 0.534717	val: 0.899258	test: 0.838104
MAE train: 0.427970	val: 0.707711	test: 0.669349

Epoch: 69
Loss: 0.28164530226162504
RMSE train: 0.409994	val: 0.834082	test: 0.800639
MAE train: 0.321536	val: 0.655104	test: 0.637728

Epoch: 70
Loss: 0.31202433151858194
RMSE train: 0.541403	val: 0.900386	test: 0.817752
MAE train: 0.434377	val: 0.697179	test: 0.647753

Epoch: 71
Loss: 0.3048802326832499
RMSE train: 0.504937	val: 0.887206	test: 0.832142
MAE train: 0.406553	val: 0.691659	test: 0.666063

Epoch: 72
Loss: 0.2816107656274523
RMSE train: 0.446119	val: 0.854304	test: 0.835939
MAE train: 0.351043	val: 0.675042	test: 0.665821

Epoch: 73
Loss: 0.2764871450407164
RMSE train: 0.479515	val: 0.884707	test: 0.819851
MAE train: 0.380785	val: 0.689026	test: 0.653307

Epoch: 74
Loss: 0.2686422552381243
RMSE train: 0.517742	val: 0.883838	test: 0.812039
MAE train: 0.419543	val: 0.687226	test: 0.649873

Epoch: 75
Loss: 0.275846571794578
RMSE train: 0.429016	val: 0.858116	test: 0.810215
MAE train: 0.338686	val: 0.674822	test: 0.638941

Epoch: 76
Loss: 0.2577263531940324
RMSE train: 0.431131	val: 0.853037	test: 0.803992
MAE train: 0.341013	val: 0.665832	test: 0.641063

Epoch: 77
Loss: 0.264180135514055
RMSE train: 0.605289	val: 0.954221	test: 0.898801
MAE train: 0.501594	val: 0.739321	test: 0.720372

Epoch: 78
Loss: 0.25639621168375015
RMSE train: 0.403496	val: 0.842037	test: 0.825329
MAE train: 0.318036	val: 0.663109	test: 0.665671

Epoch: 79
Loss: 0.2404500309910093
RMSE train: 0.409866	val: 0.845952	test: 0.817401
MAE train: 0.322687	val: 0.668287	test: 0.653496

Epoch: 80
Loss: 0.2505894952586719
RMSE train: 0.420772	val: 0.850278	test: 0.816490
MAE train: 0.333303	val: 0.663808	test: 0.661046

Epoch: 81
Loss: 0.2443993581192834
RMSE train: 0.395587	val: 0.855209	test: 0.808367
MAE train: 0.311693	val: 0.670252	test: 0.651665

Epoch: 82
Loss: 0.22796341457537242
RMSE train: 0.526248	val: 0.900347	test: 0.832017
MAE train: 0.287355	val: 0.732146	test: 0.688359

Epoch: 83
Loss: 0.20875599341733114
RMSE train: 0.346635	val: 0.948386	test: 0.874208
MAE train: 0.274948	val: 0.751189	test: 0.699571

Epoch: 84
Loss: 0.20687506454331533
RMSE train: 0.353784	val: 0.930662	test: 0.869637
MAE train: 0.280126	val: 0.735225	test: 0.687637

Epoch: 85
Loss: 0.21657032200268336
RMSE train: 0.439162	val: 0.943511	test: 0.869457
MAE train: 0.347972	val: 0.746669	test: 0.687758

Epoch: 86
Loss: 0.19229997800929205
RMSE train: 0.360223	val: 0.952888	test: 0.873187
MAE train: 0.287114	val: 0.757291	test: 0.689246

Epoch: 87
Loss: 0.20230013770716532
RMSE train: 0.355879	val: 0.950877	test: 0.868208
MAE train: 0.282105	val: 0.754447	test: 0.685842

Epoch: 88
Loss: 0.20325671242816107
RMSE train: 0.343096	val: 0.933409	test: 0.867985
MAE train: 0.272230	val: 0.740018	test: 0.683789

Epoch: 89
Loss: 0.18246951273509435
RMSE train: 0.330450	val: 0.956190	test: 0.881446
MAE train: 0.260966	val: 0.755638	test: 0.698880

Epoch: 90
Loss: 0.2054018708212035
RMSE train: 0.390039	val: 0.940378	test: 0.866606
MAE train: 0.309675	val: 0.749523	test: 0.683621

Epoch: 91
Loss: 0.18628279119729996
RMSE train: 0.371654	val: 0.933342	test: 0.860036
MAE train: 0.295651	val: 0.739342	test: 0.676268

Epoch: 92
Loss: 0.1864845326968602
RMSE train: 0.347197	val: 0.924916	test: 0.860796
MAE train: 0.275829	val: 0.737430	test: 0.678432

Epoch: 93
Loss: 0.18073056638240814
RMSE train: 0.357810	val: 0.935948	test: 0.859772
MAE train: 0.284767	val: 0.742445	test: 0.682280

Epoch: 94
Loss: 0.1809986350791795
RMSE train: 0.317760	val: 0.930726	test: 0.867874
MAE train: 0.251208	val: 0.734211	test: 0.682249

Epoch: 95
Loss: 0.17744892622743333
RMSE train: 0.324685	val: 0.933846	test: 0.870466
MAE train: 0.258723	val: 0.739666	test: 0.689057

Epoch: 96
Loss: 0.18916301535708563
RMSE train: 0.326098	val: 0.931069	test: 0.871267
MAE train: 0.259638	val: 0.739768	test: 0.687992

Epoch: 97
Loss: 0.1852697655558586
RMSE train: 0.292218	val: 0.921771	test: 0.866373
MAE train: 0.229752	val: 0.725505	test: 0.678447

Epoch: 98
Loss: 0.18083454029900686
RMSE train: 0.365479	val: 0.947587	test: 0.870355
MAE train: 0.291360	val: 0.750105	test: 0.690727

Epoch: 99
Loss: 0.19096980350358145
RMSE train: 0.351504	val: 0.929037	test: 0.855582
MAE train: 0.279687	val: 0.730531	test: 0.675995

Epoch: 100
Loss: 0.20868601756436483
RMSE train: 0.473558	val: 0.963807	test: 0.885901
MAE train: 0.385528	val: 0.754612	test: 0.701390

Epoch: 101
Loss: 0.17353970344577516
RMSE train: 0.350283	val: 0.936083	test: 0.871335
MAE train: 0.277573	val: 0.733801	test: 0.686428

Epoch: 102
Loss: 0.16433729178139142
RMSE train: 0.306038	val: 0.932504	test: 0.877578
MAE train: 0.244911	val: 0.733503	test: 0.690752

Epoch: 103
Loss: 0.15786608627864293
RMSE train: 0.330993	val: 0.930905	test: 0.864256
MAE train: 0.262476	val: 0.735908	test: 0.682790

Epoch: 104
Loss: 0.15797006764582225
RMSE train: 0.307865	val: 0.957092	test: 0.872785
MAE train: 0.243633	val: 0.753074	test: 0.693707

Epoch: 105
Loss: 0.18103654416544096
RMSE train: 0.327260	val: 0.953718	test: 0.880660
MAE train: 0.258010	val: 0.751652	test: 0.700624

Epoch: 106
Loss: 0.16523262113332748
RMSE train: 0.419666	val: 0.966497	test: 0.923787
MAE train: 0.348046	val: 0.778502	test: 0.733186

Epoch: 107
Loss: 0.171259520309312
RMSE train: 0.299895	val: 0.923438	test: 0.863766
MAE train: 0.235235	val: 0.723549	test: 0.678979

Epoch: 108
Loss: 0.16532388116632188
RMSE train: 0.348512	val: 0.940442	test: 0.863495
MAE train: 0.279355	val: 0.745784	test: 0.682511

Epoch: 109
Loss: 0.14871346205472946
RMSE train: 0.321794	val: 0.948442	test: 0.866644
MAE train: 0.254730	val: 0.750001	test: 0.690963

Epoch: 110
Loss: 0.15789783213819777
RMSE train: 0.319316	val: 0.940585	test: 0.873498
MAE train: 0.255351	val: 0.743811	test: 0.691493

Epoch: 111
Loss: 0.14288125400032317
RMSE train: 0.324319	val: 0.931767	test: 0.869202
MAE train: 0.256157	val: 0.735492	test: 0.688373

Epoch: 112
Loss: 0.15631103834935597
RMSE train: 0.290791	val: 0.933567	test: 0.875976
MAE train: 0.231107	val: 0.733811	test: 0.690721

Epoch: 113
Loss: 0.15466143297297613
RMSE train: 0.300429	val: 0.943430	test: 0.874376
MAE train: 0.238916	val: 0.750020	test: 0.688830

Epoch: 114
Loss: 0.15761200764349528
RMSE train: 0.306579	val: 0.929931	test: 0.871105
MAE train: 0.244361	val: 0.733507	test: 0.688934

Epoch: 115
Loss: 0.1470794379711151
RMSE train: 0.329454	val: 0.954597	test: 0.878925
MAE train: 0.260114	val: 0.760203	test: 0.700121

Epoch: 116
Loss: 0.1550561562180519
RMSE train: 0.319264	val: 0.936315	test: 0.868730
MAE train: 0.250856	val: 0.734627	test: 0.690430

Epoch: 117
Loss: 0.14892746987087385
RMSE train: 0.339581	val: 0.933185	test: 0.862842
MAE train: 0.271608	val: 0.735199	test: 0.685413

Epoch: 118
Loss: 0.15799348801374435
RMSE train: 0.328571	val: 0.930950	test: 0.866950
MAE train: 0.262972	val: 0.729059	test: 0.679642

Epoch: 119
Loss: 0.1367236679153783
RMSE train: 0.302443	val: 0.933536	test: 0.870260
MAE train: 0.241769	val: 0.737012	test: 0.690085

Epoch: 120
Loss: 0.15265265532902308
RMSE train: 0.312092	val: 0.947982	test: 0.873523
MAE train: 0.248288	val: 0.746100	test: 0.688353

Epoch: 121
Loss: 0.14493734602417266
RMSE train: 0.311400	val: 0.931728	test: 0.867839
MAE train: 0.245475	val: 0.724262	test: 0.680910

Early stopping
Best (RMSE):	 train: 0.452667	val: 0.920002	test: 0.860260
Best (MAE):	 train: 0.358701	val: 0.723096	test: 0.674940

MAE train: 0.290809	val: 0.744916	test: 0.706933

Epoch: 83
Loss: 0.21615288725921086
RMSE train: 0.421082	val: 0.987792	test: 0.910253
MAE train: 0.329890	val: 0.781198	test: 0.722453

Epoch: 84
Loss: 0.21530927611248835
RMSE train: 0.352014	val: 0.948326	test: 0.872244
MAE train: 0.278743	val: 0.745358	test: 0.693285

Epoch: 85
Loss: 0.2119460010102817
RMSE train: 0.372525	val: 0.968060	test: 0.899249
MAE train: 0.293685	val: 0.761251	test: 0.713732

Epoch: 86
Loss: 0.20576622549976623
RMSE train: 0.420727	val: 0.947762	test: 0.875893
MAE train: 0.331749	val: 0.751031	test: 0.698243

Epoch: 87
Loss: 0.19664438920361654
RMSE train: 0.373380	val: 0.935406	test: 0.882116
MAE train: 0.290948	val: 0.736000	test: 0.691245

Epoch: 88
Loss: 0.18837481204952514
RMSE train: 0.347741	val: 0.944895	test: 0.881044
MAE train: 0.275806	val: 0.741447	test: 0.700192

Epoch: 89
Loss: 0.18752422290188925
RMSE train: 0.395836	val: 0.923263	test: 0.860073
MAE train: 0.315872	val: 0.727989	test: 0.689120

Epoch: 90
Loss: 0.18559438309499196
RMSE train: 0.403293	val: 0.992582	test: 0.914910
MAE train: 0.318932	val: 0.781870	test: 0.727749

Epoch: 91
Loss: 0.18613657248871668
RMSE train: 0.359683	val: 0.948687	test: 0.894868
MAE train: 0.290759	val: 0.747980	test: 0.721308

Epoch: 92
Loss: 0.19022625152553832
RMSE train: 0.342890	val: 0.933273	test: 0.868052
MAE train: 0.272821	val: 0.727858	test: 0.693112

Epoch: 93
Loss: 0.19497826163257873
RMSE train: 0.330345	val: 0.934144	test: 0.870815
MAE train: 0.262932	val: 0.734755	test: 0.693352

Epoch: 94
Loss: 0.17980125759329116
RMSE train: 0.375912	val: 0.964889	test: 0.881599
MAE train: 0.295880	val: 0.760644	test: 0.694880

Epoch: 95
Loss: 0.1702815762587956
RMSE train: 0.372674	val: 0.930589	test: 0.857384
MAE train: 0.294433	val: 0.733893	test: 0.682878

Epoch: 96
Loss: 0.1813089389886175
RMSE train: 0.302594	val: 0.960298	test: 0.879382
MAE train: 0.237968	val: 0.746871	test: 0.692408

Epoch: 97
Loss: 0.17390496177332743
RMSE train: 0.419402	val: 0.967338	test: 0.881390
MAE train: 0.328487	val: 0.762630	test: 0.692919

Epoch: 98
Loss: 0.17744853453976767
RMSE train: 0.314521	val: 0.953448	test: 0.877596
MAE train: 0.247813	val: 0.754264	test: 0.690484

Epoch: 99
Loss: 0.1688939253134387
RMSE train: 0.329423	val: 0.939454	test: 0.870180
MAE train: 0.256989	val: 0.738642	test: 0.688791

Epoch: 100
Loss: 0.1543534185205187
RMSE train: 0.313238	val: 0.943304	test: 0.874921
MAE train: 0.248558	val: 0.740796	test: 0.693130

Epoch: 101
Loss: 0.1609857524079936
RMSE train: 0.405167	val: 0.974463	test: 0.890200
MAE train: 0.318874	val: 0.771636	test: 0.703985

Epoch: 102
Loss: 0.17580545055014746
RMSE train: 0.359866	val: 0.948791	test: 0.875420
MAE train: 0.280717	val: 0.742748	test: 0.687824

Epoch: 103
Loss: 0.1617921878184591
RMSE train: 0.332084	val: 0.947751	test: 0.881004
MAE train: 0.260601	val: 0.744271	test: 0.693424

Epoch: 104
Loss: 0.16781882090227945
RMSE train: 0.342415	val: 0.947926	test: 0.883600
MAE train: 0.271138	val: 0.748212	test: 0.701724

Epoch: 105
Loss: 0.16761640246425355
RMSE train: 0.316537	val: 0.924413	test: 0.870203
MAE train: 0.255269	val: 0.727457	test: 0.704992

Epoch: 106
Loss: 0.1704484298825264
RMSE train: 0.331688	val: 0.949526	test: 0.875630
MAE train: 0.258724	val: 0.738601	test: 0.695240

Epoch: 107
Loss: 0.1561336543943201
RMSE train: 0.356922	val: 0.966146	test: 0.882153
MAE train: 0.279832	val: 0.753976	test: 0.702149

Epoch: 108
Loss: 0.15266919136047363
RMSE train: 0.337957	val: 0.953287	test: 0.862344
MAE train: 0.263711	val: 0.745891	test: 0.680258

Epoch: 109
Loss: 0.14664246354784286
RMSE train: 0.366109	val: 0.979521	test: 0.912033
MAE train: 0.291340	val: 0.768772	test: 0.719457

Epoch: 110
Loss: 0.15080973505973816
RMSE train: 0.319751	val: 0.928056	test: 0.868278
MAE train: 0.257776	val: 0.731175	test: 0.701241

Epoch: 111
Loss: 0.14503984366144454
RMSE train: 0.319470	val: 0.938504	test: 0.873822
MAE train: 0.251849	val: 0.744512	test: 0.695363

Epoch: 112
Loss: 0.14995190926960536
RMSE train: 0.313157	val: 0.947656	test: 0.879477
MAE train: 0.248457	val: 0.738193	test: 0.697319

Epoch: 113
Loss: 0.14799014372485025
RMSE train: 0.421499	val: 0.961018	test: 0.879951
MAE train: 0.325693	val: 0.753119	test: 0.699511

Epoch: 114
Loss: 0.15541507144059455
RMSE train: 0.327245	val: 0.915899	test: 0.855046
MAE train: 0.257527	val: 0.725462	test: 0.685331

Epoch: 115
Loss: 0.16448725014925003
RMSE train: 0.309082	val: 0.965272	test: 0.894431
MAE train: 0.247971	val: 0.753132	test: 0.709037

Epoch: 116
Loss: 0.14309457849178994
RMSE train: 0.283309	val: 0.919288	test: 0.848260
MAE train: 0.225542	val: 0.720637	test: 0.674520

Epoch: 117
Loss: 0.14270699609603202
RMSE train: 0.332123	val: 0.954029	test: 0.870496
MAE train: 0.258828	val: 0.747655	test: 0.690868

Epoch: 118
Loss: 0.12730016559362411
RMSE train: 0.354544	val: 0.956114	test: 0.874628
MAE train: 0.275161	val: 0.754320	test: 0.689196

Epoch: 119
Loss: 0.14015059119888715
RMSE train: 0.309263	val: 0.948499	test: 0.873049
MAE train: 0.241216	val: 0.742540	test: 0.691975

Epoch: 120
Loss: 0.13493752319897925
RMSE train: 0.316057	val: 0.932869	test: 0.850051
MAE train: 0.247994	val: 0.729989	test: 0.676536

Epoch: 121
Loss: 0.1398257575929165
RMSE train: 0.325046	val: 0.947756	test: 0.872458
MAE train: 0.255662	val: 0.742525	test: 0.690996

Early stopping
Best (RMSE):	 train: 0.456010	val: 0.915567	test: 0.875573
Best (MAE):	 train: 0.362929	val: 0.723035	test: 0.701177

MAE train: 0.327476	val: 0.741539	test: 0.689582

Epoch: 83
Loss: 0.23238357050078257
RMSE train: 0.336171	val: 0.938908	test: 0.854351
MAE train: 0.266174	val: 0.736610	test: 0.683518

Epoch: 84
Loss: 0.2203355229326657
RMSE train: 0.402231	val: 0.947436	test: 0.871692
MAE train: 0.322110	val: 0.739277	test: 0.687308

Epoch: 85
Loss: 0.20286728654588973
RMSE train: 0.383903	val: 0.915115	test: 0.844145
MAE train: 0.307951	val: 0.729844	test: 0.680943

Epoch: 86
Loss: 0.1981021644813674
RMSE train: 0.385277	val: 0.918209	test: 0.849285
MAE train: 0.307274	val: 0.730852	test: 0.675189

Epoch: 87
Loss: 0.19121448802096502
RMSE train: 0.372384	val: 0.920843	test: 0.843818
MAE train: 0.296831	val: 0.733618	test: 0.671967

Epoch: 88
Loss: 0.2018128942166056
RMSE train: 0.351551	val: 0.985703	test: 0.891625
MAE train: 0.278410	val: 0.775333	test: 0.705949

Epoch: 89
Loss: 0.1897201868040221
RMSE train: 0.388881	val: 0.940858	test: 0.857043
MAE train: 0.313565	val: 0.751356	test: 0.691300

Epoch: 90
Loss: 0.19262475733246123
RMSE train: 0.365163	val: 0.969254	test: 0.866563
MAE train: 0.288285	val: 0.766637	test: 0.686585

Epoch: 91
Loss: 0.1864652048264231
RMSE train: 0.410644	val: 1.013692	test: 0.908207
MAE train: 0.329420	val: 0.793835	test: 0.717430

Epoch: 92
Loss: 0.18010104979787553
RMSE train: 0.355181	val: 0.926927	test: 0.854284
MAE train: 0.284934	val: 0.736910	test: 0.682012

Epoch: 93
Loss: 0.1897147542663983
RMSE train: 0.385780	val: 0.930292	test: 0.854082
MAE train: 0.312859	val: 0.744434	test: 0.688213

Epoch: 94
Loss: 0.1883505338004657
RMSE train: 0.413545	val: 0.945775	test: 0.889363
MAE train: 0.336329	val: 0.756403	test: 0.721587

Epoch: 95
Loss: 0.18531302894864762
RMSE train: 0.387695	val: 0.943242	test: 0.865352
MAE train: 0.305291	val: 0.737921	test: 0.684867

Epoch: 96
Loss: 0.18654884078672954
RMSE train: 0.351475	val: 0.928181	test: 0.852888
MAE train: 0.282066	val: 0.738587	test: 0.685637

Epoch: 97
Loss: 0.17142425796815328
RMSE train: 0.384212	val: 0.920847	test: 0.865686
MAE train: 0.312474	val: 0.740637	test: 0.701269

Epoch: 98
Loss: 0.16724831291607448
RMSE train: 0.330554	val: 0.972499	test: 0.874071
MAE train: 0.259565	val: 0.768296	test: 0.695942

Epoch: 99
Loss: 0.16828707392726625
RMSE train: 0.339089	val: 0.938141	test: 0.865184
MAE train: 0.271320	val: 0.750804	test: 0.697498

Epoch: 100
Loss: 0.16141096928289958
RMSE train: 0.346201	val: 0.949599	test: 0.871829
MAE train: 0.277267	val: 0.745097	test: 0.699813

Epoch: 101
Loss: 0.15842855721712112
RMSE train: 0.306160	val: 0.934043	test: 0.860349
MAE train: 0.242087	val: 0.744745	test: 0.687409

Epoch: 102
Loss: 0.1574176475405693
RMSE train: 0.319358	val: 0.966852	test: 0.869202
MAE train: 0.254129	val: 0.760321	test: 0.697206

Epoch: 103
Loss: 0.1509884191410882
RMSE train: 0.305008	val: 0.953282	test: 0.858136
MAE train: 0.243642	val: 0.750291	test: 0.688207

Epoch: 104
Loss: 0.17378866033894674
RMSE train: 0.308221	val: 0.931206	test: 0.857985
MAE train: 0.243799	val: 0.735442	test: 0.688442

Epoch: 105
Loss: 0.15654747933149338
RMSE train: 0.322924	val: 0.948265	test: 0.859293
MAE train: 0.256656	val: 0.749153	test: 0.684199

Epoch: 106
Loss: 0.13679001107811928
RMSE train: 0.325394	val: 0.946856	test: 0.859814
MAE train: 0.260219	val: 0.752870	test: 0.687966

Epoch: 107
Loss: 0.16001170128583908
RMSE train: 0.291168	val: 0.920688	test: 0.843079
MAE train: 0.231783	val: 0.729045	test: 0.677180

Epoch: 108
Loss: 0.15956045793635504
RMSE train: 0.321015	val: 0.961538	test: 0.878774
MAE train: 0.256397	val: 0.749810	test: 0.701675

Epoch: 109
Loss: 0.14733720038618361
RMSE train: 0.425135	val: 0.924601	test: 0.858894
MAE train: 0.362934	val: 0.739915	test: 0.689592

Epoch: 110
Loss: 0.14462727734020778
RMSE train: 0.375080	val: 0.933277	test: 0.868830
MAE train: 0.307969	val: 0.745212	test: 0.701215

Epoch: 111
Loss: 0.13529075682163239
RMSE train: 0.392419	val: 0.981580	test: 0.892229
MAE train: 0.311335	val: 0.767255	test: 0.708622

Epoch: 112
Loss: 0.14326283176030433
RMSE train: 0.299202	val: 0.929991	test: 0.845005
MAE train: 0.241510	val: 0.741436	test: 0.678519

Epoch: 113
Loss: 0.12974388205579349
RMSE train: 0.304119	val: 0.975492	test: 0.877599
MAE train: 0.242677	val: 0.768186	test: 0.702727

Epoch: 114
Loss: 0.13787540474108287
RMSE train: 0.361542	val: 0.928661	test: 0.866196
MAE train: 0.297331	val: 0.743057	test: 0.697525

Epoch: 115
Loss: 0.14062703613724029
RMSE train: 0.286220	val: 0.967473	test: 0.869798
MAE train: 0.225758	val: 0.762161	test: 0.693248

Epoch: 116
Loss: 0.1360902264714241
RMSE train: 0.293064	val: 0.925086	test: 0.846983
MAE train: 0.232923	val: 0.735629	test: 0.683937

Epoch: 117
Loss: 0.13911324154053414
RMSE train: 0.310958	val: 0.966828	test: 0.868036
MAE train: 0.248883	val: 0.757172	test: 0.697437

Epoch: 118
Loss: 0.13021061037267959
RMSE train: 0.311507	val: 0.954623	test: 0.864318
MAE train: 0.246307	val: 0.756235	test: 0.689079

Epoch: 119
Loss: 0.1321883691208703
RMSE train: 0.398554	val: 0.919773	test: 0.855067
MAE train: 0.333978	val: 0.736578	test: 0.691079

Epoch: 120
Loss: 0.13106576779059
RMSE train: 0.314635	val: 0.958379	test: 0.869502
MAE train: 0.248267	val: 0.754556	test: 0.694588

Epoch: 121
Loss: 0.12711332738399506
RMSE train: 0.283423	val: 0.936597	test: 0.859462
MAE train: 0.228681	val: 0.743050	test: 0.691646

Early stopping
Best (RMSE):	 train: 0.441578	val: 0.911942	test: 0.852273
Best (MAE):	 train: 0.351381	val: 0.727906	test: 0.688862

MAE train: 0.275055	val: 0.855920	test: 0.784010

Epoch: 83
Loss: 0.20957103158746446
RMSE train: 0.322690	val: 1.096134	test: 0.985092
MAE train: 0.258251	val: 0.859247	test: 0.786029

Epoch: 84
Loss: 0.19802280834742955
RMSE train: 0.353102	val: 1.080556	test: 0.983216
MAE train: 0.284986	val: 0.855528	test: 0.779548

Epoch: 85
Loss: 0.19313316260065352
RMSE train: 0.331543	val: 1.062272	test: 0.962856
MAE train: 0.266185	val: 0.839351	test: 0.762882

Epoch: 86
Loss: 0.19759656488895416
RMSE train: 0.386256	val: 1.138512	test: 1.016001
MAE train: 0.303408	val: 0.893896	test: 0.800359

Epoch: 87
Loss: 0.1862062875713621
RMSE train: 0.329638	val: 1.112610	test: 0.998705
MAE train: 0.261152	val: 0.875429	test: 0.790374

Epoch: 88
Loss: 0.18030476570129395
RMSE train: 0.307706	val: 1.051100	test: 0.943994
MAE train: 0.245088	val: 0.828256	test: 0.751932

Epoch: 89
Loss: 0.18412313078130996
RMSE train: 0.300013	val: 1.079315	test: 0.973167
MAE train: 0.235735	val: 0.845122	test: 0.774747

Epoch: 90
Loss: 0.18802872938769205
RMSE train: 0.339107	val: 1.216767	test: 1.081215
MAE train: 0.262206	val: 0.952223	test: 0.843598

Epoch: 91
Loss: 0.17946717675243104
RMSE train: 0.340012	val: 1.058198	test: 0.964957
MAE train: 0.269359	val: 0.834472	test: 0.767767

Epoch: 92
Loss: 0.17545710716928756
RMSE train: 0.323302	val: 1.155110	test: 1.033536
MAE train: 0.253879	val: 0.905092	test: 0.820049

Epoch: 93
Loss: 0.17116200923919678
RMSE train: 0.333127	val: 1.049993	test: 0.942374
MAE train: 0.267082	val: 0.826109	test: 0.751293

Epoch: 94
Loss: 0.1803630760737828
RMSE train: 0.303400	val: 1.083990	test: 0.977366
MAE train: 0.241203	val: 0.850960	test: 0.778867

Epoch: 95
Loss: 0.1726165892822402
RMSE train: 0.306450	val: 1.085264	test: 0.972297
MAE train: 0.238322	val: 0.851087	test: 0.771206

Epoch: 96
Loss: 0.1806126215628215
RMSE train: 0.337546	val: 1.100723	test: 0.984214
MAE train: 0.271957	val: 0.862138	test: 0.784163

Epoch: 97
Loss: 0.1651123943073409
RMSE train: 0.347262	val: 1.135744	test: 1.027549
MAE train: 0.283801	val: 0.892672	test: 0.810838

Epoch: 98
Loss: 0.15992528732333863
RMSE train: 0.310317	val: 1.122171	test: 1.008020
MAE train: 0.242088	val: 0.887505	test: 0.797150

Epoch: 99
Loss: 0.1662160408283983
RMSE train: 0.370886	val: 1.141741	test: 1.024580
MAE train: 0.291824	val: 0.897788	test: 0.811605

Epoch: 100
Loss: 0.17403288504907063
RMSE train: 0.295098	val: 1.101927	test: 0.996923
MAE train: 0.233852	val: 0.870467	test: 0.794257

Epoch: 101
Loss: 0.15488522499799728
RMSE train: 0.311617	val: 1.116497	test: 1.005015
MAE train: 0.243731	val: 0.876765	test: 0.801102

Epoch: 102
Loss: 0.1593093371817044
RMSE train: 0.285842	val: 1.083441	test: 0.978374
MAE train: 0.222897	val: 0.853534	test: 0.779567

Epoch: 103
Loss: 0.14043417679412024
RMSE train: 0.311379	val: 1.085762	test: 0.979136
MAE train: 0.247954	val: 0.856715	test: 0.778480

Epoch: 104
Loss: 0.15326168122036116
RMSE train: 0.313546	val: 1.103595	test: 1.001337
MAE train: 0.246335	val: 0.869225	test: 0.797569

Epoch: 105
Loss: 0.14907859797988618
RMSE train: 0.299579	val: 1.071307	test: 0.976561
MAE train: 0.237314	val: 0.848781	test: 0.778672

Epoch: 106
Loss: 0.14255442417093686
RMSE train: 0.321980	val: 1.120567	test: 0.996497
MAE train: 0.249062	val: 0.877295	test: 0.792286

Epoch: 107
Loss: 0.14778297767043114
RMSE train: 0.294711	val: 1.063735	test: 0.955563
MAE train: 0.239177	val: 0.835768	test: 0.762034

Epoch: 108
Loss: 0.14745772310665675
RMSE train: 0.318066	val: 1.112233	test: 0.989587
MAE train: 0.248865	val: 0.872308	test: 0.789676

Epoch: 109
Loss: 0.14448710903525352
RMSE train: 0.272828	val: 1.082488	test: 0.984420
MAE train: 0.213767	val: 0.854662	test: 0.779115

Epoch: 110
Loss: 0.14400861731597356
RMSE train: 0.304626	val: 1.129688	test: 1.017243
MAE train: 0.239662	val: 0.890467	test: 0.805382

Epoch: 111
Loss: 0.14474738921437943
RMSE train: 0.309675	val: 1.069194	test: 0.967630
MAE train: 0.241927	val: 0.841716	test: 0.770618

Epoch: 112
Loss: 0.14161207101174764
RMSE train: 0.290546	val: 1.142649	test: 1.024176
MAE train: 0.226625	val: 0.896908	test: 0.810819

Epoch: 113
Loss: 0.12737311529261724
RMSE train: 0.299918	val: 1.085245	test: 0.983790
MAE train: 0.240135	val: 0.858912	test: 0.783353

Epoch: 114
Loss: 0.13063635943191393
RMSE train: 0.318764	val: 1.099550	test: 1.000759
MAE train: 0.248369	val: 0.870866	test: 0.792054

Epoch: 115
Loss: 0.13617217860051564
RMSE train: 0.263500	val: 1.065320	test: 0.976522
MAE train: 0.206959	val: 0.845470	test: 0.782476

Epoch: 116
Loss: 0.12551412837845938
RMSE train: 0.312550	val: 1.082137	test: 0.972614
MAE train: 0.251243	val: 0.850703	test: 0.776651

Epoch: 117
Loss: 0.12505886969821794
RMSE train: 0.294633	val: 1.047381	test: 0.952407
MAE train: 0.238496	val: 0.826772	test: 0.764558

Epoch: 118
Loss: 0.12716741487383842
RMSE train: 0.281697	val: 1.068772	test: 0.968259
MAE train: 0.221396	val: 0.842349	test: 0.771256

Epoch: 119
Loss: 0.12955111850585257
RMSE train: 0.272738	val: 1.098545	test: 0.987549
MAE train: 0.212901	val: 0.868591	test: 0.785310

Epoch: 120
Loss: 0.12375791424087115
RMSE train: 0.280350	val: 1.083786	test: 0.971299
MAE train: 0.223160	val: 0.856737	test: 0.773082

Epoch: 121
Loss: 0.12096257348145757
RMSE train: 0.264841	val: 1.071170	test: 0.972676
MAE train: 0.210090	val: 0.843086	test: 0.778083

Early stopping
Best (RMSE):	 train: 0.565477	val: 0.996545	test: 0.944893
Best (MAE):	 train: 0.453968	val: 0.799823	test: 0.756201

MAE train: 0.277583	val: 0.846924	test: 0.857676

Epoch: 83
Loss: 0.19109517548765456
RMSE train: 0.372825	val: 1.004272	test: 1.010063
MAE train: 0.296514	val: 0.821078	test: 0.826182

Epoch: 84
Loss: 0.20175439332212722
RMSE train: 0.309918	val: 1.009523	test: 1.027172
MAE train: 0.245422	val: 0.823535	test: 0.829716

Epoch: 85
Loss: 0.19826367391007288
RMSE train: 0.312923	val: 0.997483	test: 0.998557
MAE train: 0.248338	val: 0.810028	test: 0.812642

Epoch: 86
Loss: 0.19270284048148564
RMSE train: 0.336264	val: 1.029988	test: 1.052963
MAE train: 0.267977	val: 0.844880	test: 0.846648

Epoch: 87
Loss: 0.180386164358684
RMSE train: 0.367337	val: 0.997926	test: 0.994350
MAE train: 0.287809	val: 0.802551	test: 0.804251

Epoch: 88
Loss: 0.18494744385991777
RMSE train: 0.334659	val: 1.021537	test: 1.028404
MAE train: 0.266918	val: 0.832720	test: 0.833846

Epoch: 89
Loss: 0.1735475297485079
RMSE train: 0.304520	val: 1.027162	test: 1.045062
MAE train: 0.241921	val: 0.835943	test: 0.844808

Epoch: 90
Loss: 0.17491155862808228
RMSE train: 0.318551	val: 1.049779	test: 1.081105
MAE train: 0.251782	val: 0.855843	test: 0.866274

Epoch: 91
Loss: 0.17643740666764124
RMSE train: 0.389900	val: 0.986865	test: 0.957685
MAE train: 0.305034	val: 0.786261	test: 0.775944

Epoch: 92
Loss: 0.16641146476779664
RMSE train: 0.322214	val: 1.009162	test: 1.033100
MAE train: 0.256757	val: 0.817828	test: 0.832706

Epoch: 93
Loss: 0.1721529715827533
RMSE train: 0.315221	val: 1.099824	test: 1.147173
MAE train: 0.252846	val: 0.900364	test: 0.916624

Epoch: 94
Loss: 0.1619613000324794
RMSE train: 0.330227	val: 1.001414	test: 1.005400
MAE train: 0.260015	val: 0.804394	test: 0.810834

Epoch: 95
Loss: 0.15916835623128073
RMSE train: 0.311642	val: 1.037618	test: 1.059387
MAE train: 0.245892	val: 0.846595	test: 0.859381

Epoch: 96
Loss: 0.1693932967526572
RMSE train: 0.310847	val: 1.014057	test: 1.007236
MAE train: 0.245471	val: 0.823457	test: 0.822294

Epoch: 97
Loss: 0.16859369086367743
RMSE train: 0.390555	val: 1.013714	test: 1.017556
MAE train: 0.305218	val: 0.823129	test: 0.827350

Epoch: 98
Loss: 0.17082553676196507
RMSE train: 0.303583	val: 1.044618	test: 1.054724
MAE train: 0.239961	val: 0.850194	test: 0.851353

Epoch: 99
Loss: 0.15647263079881668
RMSE train: 0.313616	val: 1.023413	test: 1.023455
MAE train: 0.251908	val: 0.832862	test: 0.832475

Epoch: 100
Loss: 0.14307495951652527
RMSE train: 0.317109	val: 0.995851	test: 0.984621
MAE train: 0.247716	val: 0.799928	test: 0.803482

Epoch: 101
Loss: 0.1377766483596393
RMSE train: 0.317211	val: 1.055365	test: 1.089103
MAE train: 0.254301	val: 0.858965	test: 0.870765

Epoch: 102
Loss: 0.15102404249565943
RMSE train: 0.341066	val: 1.019553	test: 1.027251
MAE train: 0.266215	val: 0.827922	test: 0.824676

Epoch: 103
Loss: 0.15233552030154637
RMSE train: 0.292110	val: 1.062040	test: 1.085772
MAE train: 0.229992	val: 0.869074	test: 0.875241

Epoch: 104
Loss: 0.15161458083561488
RMSE train: 0.346298	val: 1.008979	test: 1.015611
MAE train: 0.267922	val: 0.822097	test: 0.823102

Epoch: 105
Loss: 0.14609521520989283
RMSE train: 0.314790	val: 1.084610	test: 1.118140
MAE train: 0.253353	val: 0.893788	test: 0.900351

Epoch: 106
Loss: 0.16080869947160994
RMSE train: 0.364743	val: 0.999250	test: 0.992965
MAE train: 0.285605	val: 0.797598	test: 0.803576

Epoch: 107
Loss: 0.14423696271010808
RMSE train: 0.320326	val: 1.008177	test: 1.011180
MAE train: 0.248252	val: 0.817597	test: 0.821619

Epoch: 108
Loss: 0.14176330396107265
RMSE train: 0.295949	val: 1.049936	test: 1.062766
MAE train: 0.233050	val: 0.855807	test: 0.854309

Epoch: 109
Loss: 0.14020538649388722
RMSE train: 0.291722	val: 1.021247	test: 1.028293
MAE train: 0.228134	val: 0.830553	test: 0.830294

Epoch: 110
Loss: 0.1330820077231952
RMSE train: 0.296537	val: 1.092245	test: 1.121826
MAE train: 0.234590	val: 0.898830	test: 0.903219

Epoch: 111
Loss: 0.12700522212045534
RMSE train: 0.285735	val: 1.072366	test: 1.102181
MAE train: 0.223878	val: 0.875738	test: 0.882678

Epoch: 112
Loss: 0.13187303340860776
RMSE train: 0.296238	val: 1.063306	test: 1.081255
MAE train: 0.236452	val: 0.875105	test: 0.871699

Epoch: 113
Loss: 0.13353672410760606
RMSE train: 0.330098	val: 1.022867	test: 1.029106
MAE train: 0.261542	val: 0.841439	test: 0.837644

Epoch: 114
Loss: 0.1457744613289833
RMSE train: 0.300349	val: 0.997796	test: 0.992443
MAE train: 0.236472	val: 0.809202	test: 0.801814

Epoch: 115
Loss: 0.14709380960890225
RMSE train: 0.286984	val: 1.032197	test: 1.046901
MAE train: 0.224602	val: 0.842064	test: 0.840149

Epoch: 116
Loss: 0.13613307156733104
RMSE train: 0.270189	val: 1.068368	test: 1.103682
MAE train: 0.213347	val: 0.876806	test: 0.881148

Epoch: 117
Loss: 0.13840664497443608
RMSE train: 0.336652	val: 1.052567	test: 1.081632
MAE train: 0.276671	val: 0.863307	test: 0.867543

Epoch: 118
Loss: 0.13142001735312597
RMSE train: 0.441917	val: 1.013428	test: 0.998211
MAE train: 0.340915	val: 0.819056	test: 0.806792

Epoch: 119
Loss: 0.13025921689612524
RMSE train: 0.353868	val: 1.014156	test: 1.000477
MAE train: 0.273026	val: 0.829172	test: 0.810870

Epoch: 120
Loss: 0.12315301011715617
RMSE train: 0.295061	val: 1.008007	test: 1.006303
MAE train: 0.233233	val: 0.815547	test: 0.815869

Epoch: 121
Loss: 0.13151443590010917
RMSE train: 0.283508	val: 1.007015	test: 1.008781
MAE train: 0.221051	val: 0.809991	test: 0.811343

Early stopping
Best (RMSE):	 train: 0.340642	val: 0.975874	test: 0.965445
Best (MAE):	 train: 0.270715	val: 0.784192	test: 0.789019
All runs completed.

MAE train: 0.426764	val: 0.703514	test: 0.665140

Epoch: 83
Loss: 0.24553216780935014
RMSE train: 0.384884	val: 0.851932	test: 0.814632
MAE train: 0.304581	val: 0.662361	test: 0.648187

Epoch: 84
Loss: 0.23736464551516942
RMSE train: 0.418424	val: 0.867184	test: 0.813660
MAE train: 0.331739	val: 0.668907	test: 0.655408

Epoch: 85
Loss: 0.22350409414087022
RMSE train: 0.482211	val: 0.902943	test: 0.831164
MAE train: 0.386998	val: 0.706596	test: 0.660603

Epoch: 86
Loss: 0.21176339047295706
RMSE train: 0.375722	val: 0.854217	test: 0.806224
MAE train: 0.296692	val: 0.669897	test: 0.651484

Epoch: 87
Loss: 0.21792222346578324
RMSE train: 0.396251	val: 0.849682	test: 0.803167
MAE train: 0.316477	val: 0.670986	test: 0.646279

Epoch: 88
Loss: 0.20329332670995168
RMSE train: 0.444638	val: 0.863003	test: 0.810872
MAE train: 0.358316	val: 0.675282	test: 0.654318

Epoch: 89
Loss: 0.19871672562190465
RMSE train: 0.365010	val: 0.857144	test: 0.815661
MAE train: 0.286498	val: 0.674981	test: 0.660235

Epoch: 90
Loss: 0.22021014988422394
RMSE train: 0.420723	val: 0.865921	test: 0.816798
MAE train: 0.337182	val: 0.678102	test: 0.660603

Epoch: 91
Loss: 0.20588778917278563
RMSE train: 0.406208	val: 0.854765	test: 0.809830
MAE train: 0.324531	val: 0.669597	test: 0.653100

Epoch: 92
Loss: 0.2003978661128453
RMSE train: 0.421186	val: 0.874740	test: 0.815230
MAE train: 0.338104	val: 0.677820	test: 0.660678

Epoch: 93
Loss: 0.19316694566181727
RMSE train: 0.448229	val: 0.885736	test: 0.825456
MAE train: 0.362753	val: 0.694969	test: 0.661167

Epoch: 94
Loss: 0.20469852749790465
RMSE train: 0.381201	val: 0.866538	test: 0.814096
MAE train: 0.303815	val: 0.673631	test: 0.652294

Epoch: 95
Loss: 0.1940076702407428
RMSE train: 0.399136	val: 0.863165	test: 0.814390
MAE train: 0.319695	val: 0.674288	test: 0.660135

Epoch: 96
Loss: 0.20127301876034057
RMSE train: 0.357447	val: 0.854377	test: 0.812465
MAE train: 0.280658	val: 0.676319	test: 0.654950

Epoch: 97
Loss: 0.1975102179816791
RMSE train: 0.343021	val: 0.849493	test: 0.819257
MAE train: 0.268940	val: 0.670984	test: 0.655533

Epoch: 98
Loss: 0.19631960030112947
RMSE train: 0.349943	val: 0.854652	test: 0.809072
MAE train: 0.279276	val: 0.671129	test: 0.654125

Epoch: 99
Loss: 0.20237447321414948
RMSE train: 0.336103	val: 0.850987	test: 0.816563
MAE train: 0.264917	val: 0.668446	test: 0.657919

Epoch: 100
Loss: 0.22029045862810953
RMSE train: 0.500858	val: 0.902817	test: 0.847938
MAE train: 0.412984	val: 0.699627	test: 0.678429

Epoch: 101
Loss: 0.1928136401942798
RMSE train: 0.505158	val: 0.902774	test: 0.829039
MAE train: 0.408615	val: 0.699356	test: 0.667246

Epoch: 102
Loss: 0.20196078611271723
RMSE train: 0.446025	val: 0.870261	test: 0.818706
MAE train: 0.359778	val: 0.672677	test: 0.650334

Epoch: 103
Loss: 0.18857425983463014
RMSE train: 0.481975	val: 0.893072	test: 0.831096
MAE train: 0.390550	val: 0.693934	test: 0.663146

Epoch: 104
Loss: 0.19073270367724554
RMSE train: 0.370825	val: 0.866308	test: 0.807037
MAE train: 0.293578	val: 0.676754	test: 0.649958

Epoch: 105
Loss: 0.2046049258538655
RMSE train: 0.391795	val: 0.884787	test: 0.830457
MAE train: 0.316577	val: 0.682408	test: 0.672399

Epoch: 106
Loss: 0.17577002410377776
RMSE train: 0.379583	val: 0.860313	test: 0.850276
MAE train: 0.303473	val: 0.680116	test: 0.683585

Epoch: 107
Loss: 0.1814973790730749
RMSE train: 0.332381	val: 0.853646	test: 0.819671
MAE train: 0.262074	val: 0.664929	test: 0.660703

Epoch: 108
Loss: 0.1762584841677121
RMSE train: 0.530573	val: 0.942075	test: 0.856793
MAE train: 0.435733	val: 0.730348	test: 0.687403

Epoch: 109
Loss: 0.16343592958790915
RMSE train: 0.361175	val: 0.856474	test: 0.805172
MAE train: 0.288496	val: 0.665251	test: 0.644027

Epoch: 110
Loss: 0.1668017181966986
RMSE train: 0.367797	val: 0.877182	test: 0.823649
MAE train: 0.291657	val: 0.683409	test: 0.667558

Epoch: 111
Loss: 0.1664380505681038
RMSE train: 0.337930	val: 0.856510	test: 0.816152
MAE train: 0.268284	val: 0.668892	test: 0.657322

Epoch: 112
Loss: 0.1623676014798028
RMSE train: 0.391616	val: 0.885490	test: 0.829199
MAE train: 0.317898	val: 0.685628	test: 0.666414

Epoch: 113
Loss: 0.15708497485944203
RMSE train: 0.306449	val: 0.856152	test: 0.811771
MAE train: 0.240866	val: 0.667699	test: 0.657312

Epoch: 114
Loss: 0.17068616513695037
RMSE train: 0.307498	val: 0.846940	test: 0.817952
MAE train: 0.243362	val: 0.663284	test: 0.660570

Epoch: 115
Loss: 0.1628995261022023
RMSE train: 0.425417	val: 0.887018	test: 0.819412
MAE train: 0.342299	val: 0.695827	test: 0.657773

Epoch: 116
Loss: 0.17019987744944437
RMSE train: 0.330499	val: 0.868356	test: 0.825796
MAE train: 0.259599	val: 0.680843	test: 0.667616

Epoch: 117
Loss: 0.16720555882368768
RMSE train: 0.350735	val: 0.864892	test: 0.820609
MAE train: 0.278939	val: 0.674694	test: 0.664642

Epoch: 118
Loss: 0.16767232013600214
RMSE train: 0.311443	val: 0.847929	test: 0.823968
MAE train: 0.245530	val: 0.668416	test: 0.663268

Epoch: 119
Loss: 0.15233302009957178
RMSE train: 0.376469	val: 0.877724	test: 0.821426
MAE train: 0.300791	val: 0.687238	test: 0.671270

Epoch: 120
Loss: 0.15216292228017533
RMSE train: 0.350356	val: 0.870770	test: 0.829293
MAE train: 0.282680	val: 0.677990	test: 0.673064

Epoch: 121
Loss: 0.15559273532458714
RMSE train: 0.368503	val: 0.867528	test: 0.829287
MAE train: 0.293517	val: 0.680631	test: 0.670149

Early stopping
Best (RMSE):	 train: 0.409994	val: 0.834082	test: 0.800639
Best (MAE):	 train: 0.321536	val: 0.655104	test: 0.637728

MAE train: 0.327535	val: 0.706134	test: 0.686162

Epoch: 83
Loss: 0.25173192258392063
RMSE train: 0.398812	val: 0.879872	test: 0.858088
MAE train: 0.315064	val: 0.692582	test: 0.686851

Epoch: 84
Loss: 0.2384085761649268
RMSE train: 0.404594	val: 0.885698	test: 0.868636
MAE train: 0.320668	val: 0.694957	test: 0.695383

Epoch: 85
Loss: 0.23088043821709497
RMSE train: 0.408677	val: 0.883885	test: 0.853111
MAE train: 0.322967	val: 0.687135	test: 0.684712

Epoch: 86
Loss: 0.22454901039600372
RMSE train: 0.394897	val: 0.900004	test: 0.850281
MAE train: 0.312018	val: 0.707062	test: 0.673824

Epoch: 87
Loss: 0.23026574296610697
RMSE train: 0.398639	val: 0.892524	test: 0.842667
MAE train: 0.314275	val: 0.693393	test: 0.670657

Epoch: 88
Loss: 0.21529111159699305
RMSE train: 0.444073	val: 0.927693	test: 0.932007
MAE train: 0.356201	val: 0.727727	test: 0.746348

Epoch: 89
Loss: 0.21511159517935344
RMSE train: 0.398536	val: 0.889653	test: 0.842812
MAE train: 0.315967	val: 0.696638	test: 0.669534

Epoch: 90
Loss: 0.21482258715799876
RMSE train: 0.359322	val: 0.873555	test: 0.863741
MAE train: 0.283860	val: 0.678740	test: 0.690127

Epoch: 91
Loss: 0.20965158407177245
RMSE train: 0.391033	val: 0.890801	test: 0.855919
MAE train: 0.312451	val: 0.699647	test: 0.685462

Epoch: 92
Loss: 0.21131412791354315
RMSE train: 0.349212	val: 0.874166	test: 0.840803
MAE train: 0.276816	val: 0.676766	test: 0.672447

Epoch: 93
Loss: 0.2044829928449222
RMSE train: 0.365054	val: 0.870047	test: 0.834307
MAE train: 0.288225	val: 0.677362	test: 0.665287

Epoch: 94
Loss: 0.203247036252703
RMSE train: 0.362715	val: 0.871720	test: 0.833607
MAE train: 0.287680	val: 0.680002	test: 0.668651

Epoch: 95
Loss: 0.19167741175208772
RMSE train: 0.355383	val: 0.873200	test: 0.842506
MAE train: 0.282276	val: 0.681172	test: 0.674556

Epoch: 96
Loss: 0.2022424585052899
RMSE train: 0.349569	val: 0.888829	test: 0.852671
MAE train: 0.277946	val: 0.692473	test: 0.674803

Epoch: 97
Loss: 0.2051540772829737
RMSE train: 0.377201	val: 0.887249	test: 0.868606
MAE train: 0.300553	val: 0.696108	test: 0.694570

Epoch: 98
Loss: 0.20298931428364345
RMSE train: 0.346477	val: 0.882792	test: 0.856861
MAE train: 0.272369	val: 0.691113	test: 0.677628

Epoch: 99
Loss: 0.18899954110383987
RMSE train: 0.346723	val: 0.884858	test: 0.847490
MAE train: 0.274672	val: 0.693291	test: 0.670488

Epoch: 100
Loss: 0.1789049506187439
RMSE train: 0.342347	val: 0.882862	test: 0.846124
MAE train: 0.272294	val: 0.684862	test: 0.668451

Epoch: 101
Loss: 0.18540116718837193
RMSE train: 0.350458	val: 0.890022	test: 0.838167
MAE train: 0.278862	val: 0.691888	test: 0.666509

Epoch: 102
Loss: 0.19098150091511862
RMSE train: 0.345495	val: 0.874992	test: 0.857866
MAE train: 0.273828	val: 0.686101	test: 0.683762

Epoch: 103
Loss: 0.17905152163335256
RMSE train: 0.334003	val: 0.864974	test: 0.848676
MAE train: 0.265941	val: 0.681736	test: 0.680166

Epoch: 104
Loss: 0.18729749854121888
RMSE train: 0.376801	val: 0.894166	test: 0.839051
MAE train: 0.298357	val: 0.695854	test: 0.664910

Epoch: 105
Loss: 0.190637401172093
RMSE train: 0.345174	val: 0.888628	test: 0.862167
MAE train: 0.273352	val: 0.699503	test: 0.681221

Epoch: 106
Loss: 0.1917440571955272
RMSE train: 0.330712	val: 0.868876	test: 0.842632
MAE train: 0.263634	val: 0.681768	test: 0.670784

Epoch: 107
Loss: 0.17427025522504533
RMSE train: 0.352890	val: 0.876009	test: 0.856925
MAE train: 0.280668	val: 0.682045	test: 0.684050

Epoch: 108
Loss: 0.16637546994856425
RMSE train: 0.339361	val: 0.882863	test: 0.846435
MAE train: 0.267576	val: 0.685729	test: 0.672530

Epoch: 109
Loss: 0.16570109022515161
RMSE train: 0.356996	val: 0.883281	test: 0.881673
MAE train: 0.285509	val: 0.689198	test: 0.702374

Epoch: 110
Loss: 0.16553318074771337
RMSE train: 0.353815	val: 0.880313	test: 0.845029
MAE train: 0.282520	val: 0.698583	test: 0.670145

Epoch: 111
Loss: 0.16304420360497066
RMSE train: 0.295933	val: 0.871780	test: 0.847590
MAE train: 0.233366	val: 0.680120	test: 0.671954

Epoch: 112
Loss: 0.17659234894173487
RMSE train: 0.327046	val: 0.872855	test: 0.857684
MAE train: 0.257337	val: 0.677372	test: 0.688657

Epoch: 113
Loss: 0.1796047634312085
RMSE train: 0.372522	val: 0.871705	test: 0.866283
MAE train: 0.297915	val: 0.686248	test: 0.690857

Epoch: 114
Loss: 0.16568939813545772
RMSE train: 0.313928	val: 0.880102	test: 0.843414
MAE train: 0.245895	val: 0.686258	test: 0.667988

Epoch: 115
Loss: 0.17388273562703813
RMSE train: 0.343098	val: 0.893728	test: 0.848554
MAE train: 0.272330	val: 0.697312	test: 0.676511

Epoch: 116
Loss: 0.16887680441141129
RMSE train: 0.305824	val: 0.881606	test: 0.844838
MAE train: 0.239406	val: 0.685297	test: 0.669921

Epoch: 117
Loss: 0.1655580018247877
RMSE train: 0.329103	val: 0.877382	test: 0.840014
MAE train: 0.261238	val: 0.687220	test: 0.672838

Epoch: 118
Loss: 0.15132704377174377
RMSE train: 0.324779	val: 0.877574	test: 0.847066
MAE train: 0.258708	val: 0.686442	test: 0.678100

Epoch: 119
Loss: 0.16012932040861674
RMSE train: 0.317394	val: 0.888999	test: 0.860093
MAE train: 0.249598	val: 0.696329	test: 0.688933

Epoch: 120
Loss: 0.15299001868282044
RMSE train: 0.303053	val: 0.865341	test: 0.836450
MAE train: 0.239328	val: 0.674020	test: 0.668659

Epoch: 121
Loss: 0.14861481849636352
RMSE train: 0.316471	val: 0.877027	test: 0.848338
MAE train: 0.251979	val: 0.690068	test: 0.678621

Epoch: 122
Loss: 0.17332196395312036
RMSE train: 0.295479	val: 0.870818	test: 0.840023
MAE train: 0.232331	val: 0.680440	test: 0.667524

Epoch: 123
Loss: 0.15627260399716242
RMSE train: 0.317611	val: 0.880008	test: 0.852089
MAE train: 0.252267	val: 0.694360	test: 0.680170

Epoch: 124
Loss: 0.16837955000145094
RMSE train: 0.311280	val: 0.877500	test: 0.845665
MAE train: 0.248001	val: 0.687975	test: 0.672282

Epoch: 125
Loss: 0.14017244109085628
RMSE train: 0.332296	val: 0.887596	test: 0.841388
MAE train: 0.259513	val: 0.692892	test: 0.664025

Epoch: 126
Loss: 0.14191203777279174
RMSE train: 0.303627	val: 0.876230	test: 0.854827
MAE train: 0.241725	val: 0.687968	test: 0.680169

Epoch: 127
Loss: 0.14047890635473387
RMSE train: 0.320441	val: 0.876183	test: 0.841357
MAE train: 0.254868	val: 0.683301	test: 0.674110

Epoch: 128
Loss: 0.14453273000461714
RMSE train: 0.306717	val: 0.867061	test: 0.847463
MAE train: 0.244672	val: 0.678867	test: 0.674370

Epoch: 129
Loss: 0.1430645686175142
RMSE train: 0.301103	val: 0.868058	test: 0.847531
MAE train: 0.239403	val: 0.677015	test: 0.670870

Epoch: 130
Loss: 0.14688859401004656
RMSE train: 0.314695	val: 0.871490	test: 0.832822
MAE train: 0.248809	val: 0.673373	test: 0.664343

Epoch: 131
Loss: 0.15067746649895394
RMSE train: 0.332698	val: 0.873858	test: 0.836824
MAE train: 0.261997	val: 0.678823	test: 0.667654

Epoch: 132
Loss: 0.14254109242132731
RMSE train: 0.323074	val: 0.871459	test: 0.863550
MAE train: 0.260494	val: 0.685112	test: 0.682043

Epoch: 133
Loss: 0.1477878519466945
RMSE train: 0.292929	val: 0.865246	test: 0.859062
MAE train: 0.234856	val: 0.679693	test: 0.690072

Epoch: 134
Loss: 0.14149940652506693
RMSE train: 0.339277	val: 0.880411	test: 0.859100
MAE train: 0.271219	val: 0.689430	test: 0.692765

Epoch: 135
Loss: 0.13847525790333748
RMSE train: 0.338879	val: 0.881878	test: 0.880788
MAE train: 0.274622	val: 0.692853	test: 0.704489

Epoch: 136
Loss: 0.1429541174854551
RMSE train: 0.313695	val: 0.870186	test: 0.863960
MAE train: 0.250627	val: 0.682539	test: 0.692516

Epoch: 137
Loss: 0.1388418030525957
RMSE train: 0.290596	val: 0.877810	test: 0.849721
MAE train: 0.228400	val: 0.680343	test: 0.680449

Epoch: 138
Loss: 0.13289308175444603
RMSE train: 0.307970	val: 0.871900	test: 0.858448
MAE train: 0.246695	val: 0.683551	test: 0.691635

Early stopping
Best (RMSE):	 train: 0.334003	val: 0.864974	test: 0.848676
Best (MAE):	 train: 0.265941	val: 0.681736	test: 0.680166

MAE train: 0.343648	val: 0.960050	test: 0.942017

Epoch: 83
Loss: 0.2033950931259564
RMSE train: 0.485467	val: 1.061076	test: 1.089657
MAE train: 0.408349	val: 0.839405	test: 0.862817

Epoch: 84
Loss: 0.1949663289955684
RMSE train: 0.417948	val: 1.074833	test: 1.082408
MAE train: 0.340695	val: 0.859384	test: 0.870299

Epoch: 85
Loss: 0.19078426914555685
RMSE train: 0.418886	val: 1.079409	test: 1.083298
MAE train: 0.339710	val: 0.852492	test: 0.880203

Epoch: 86
Loss: 0.17844443874699728
RMSE train: 0.391621	val: 1.142269	test: 1.135734
MAE train: 0.316445	val: 0.940650	test: 0.918823

Epoch: 87
Loss: 0.1862244957259723
RMSE train: 0.416744	val: 1.082705	test: 1.072202
MAE train: 0.343156	val: 0.851309	test: 0.858631

Epoch: 88
Loss: 0.17828127635376795
RMSE train: 0.487249	val: 1.035671	test: 1.053608
MAE train: 0.410516	val: 0.824999	test: 0.841598

Epoch: 89
Loss: 0.17252077800886972
RMSE train: 0.358502	val: 1.094062	test: 1.062995
MAE train: 0.286842	val: 0.863536	test: 0.860393

Epoch: 90
Loss: 0.17543736845254898
RMSE train: 0.352884	val: 1.119310	test: 1.119264
MAE train: 0.281767	val: 0.883212	test: 0.916745

Epoch: 91
Loss: 0.16916810614722116
RMSE train: 0.368980	val: 1.093185	test: 1.101681
MAE train: 0.298580	val: 0.874897	test: 0.883554

Epoch: 92
Loss: 0.15697235826935088
RMSE train: 0.374082	val: 1.101462	test: 1.082571
MAE train: 0.301526	val: 0.893524	test: 0.875340

Epoch: 93
Loss: 0.16063131179128373
RMSE train: 0.403303	val: 1.059939	test: 1.057134
MAE train: 0.332691	val: 0.845733	test: 0.845212

Epoch: 94
Loss: 0.17273112067154475
RMSE train: 0.478046	val: 1.102319	test: 1.095433
MAE train: 0.408269	val: 0.876539	test: 0.886794

Epoch: 95
Loss: 0.16881261233772551
RMSE train: 0.346893	val: 1.092832	test: 1.095247
MAE train: 0.276702	val: 0.858003	test: 0.879984

Epoch: 96
Loss: 0.17528817696230753
RMSE train: 0.426045	val: 1.062703	test: 1.051640
MAE train: 0.352445	val: 0.856468	test: 0.847179

Epoch: 97
Loss: 0.18897133959191187
RMSE train: 0.468867	val: 1.132641	test: 1.143227
MAE train: 0.396451	val: 0.924433	test: 0.917190

Epoch: 98
Loss: 0.16268514628921235
RMSE train: 0.415083	val: 1.074429	test: 1.052662
MAE train: 0.341670	val: 0.861731	test: 0.842818

Epoch: 99
Loss: 0.18295894456761225
RMSE train: 0.341775	val: 1.085274	test: 1.039356
MAE train: 0.271466	val: 0.859555	test: 0.832842

Epoch: 100
Loss: 0.18662973386900766
RMSE train: 0.333010	val: 1.154619	test: 1.091817
MAE train: 0.261370	val: 0.906894	test: 0.866137

Epoch: 101
Loss: 0.15474134577172144
RMSE train: 0.375891	val: 1.054052	test: 1.042662
MAE train: 0.301769	val: 0.829972	test: 0.843853

Epoch: 102
Loss: 0.14639908181769506
RMSE train: 0.316557	val: 1.146410	test: 1.111351
MAE train: 0.252904	val: 0.902400	test: 0.893825

Epoch: 103
Loss: 0.15045274794101715
RMSE train: 0.397767	val: 1.093952	test: 1.099815
MAE train: 0.327399	val: 0.872366	test: 0.893465

Epoch: 104
Loss: 0.14641331881284714
RMSE train: 0.338331	val: 1.122488	test: 1.102569
MAE train: 0.270875	val: 0.889091	test: 0.898036

Epoch: 105
Loss: 0.1609526893922261
RMSE train: 0.324825	val: 1.083239	test: 1.064944
MAE train: 0.260170	val: 0.861972	test: 0.854103

Epoch: 106
Loss: 0.14425354663814818
RMSE train: 0.422851	val: 1.109325	test: 1.107791
MAE train: 0.354753	val: 0.874725	test: 0.898409

Epoch: 107
Loss: 0.15521433683378355
RMSE train: 0.458222	val: 1.103763	test: 1.083756
MAE train: 0.389057	val: 0.901971	test: 0.880966

Epoch: 108
Loss: 0.1398613793509347
RMSE train: 0.375297	val: 1.071971	test: 1.045078
MAE train: 0.305828	val: 0.851649	test: 0.849911

Epoch: 109
Loss: 0.13691031719957078
RMSE train: 0.332443	val: 1.083896	test: 1.048505
MAE train: 0.267006	val: 0.856476	test: 0.847963

Epoch: 110
Loss: 0.13689271360635757
RMSE train: 0.325555	val: 1.092166	test: 1.048811
MAE train: 0.262217	val: 0.868914	test: 0.847126

Epoch: 111
Loss: 0.13084884892616952
RMSE train: 0.315985	val: 1.141513	test: 1.093621
MAE train: 0.250575	val: 0.894510	test: 0.884353

Epoch: 112
Loss: 0.1347242561834199
RMSE train: 0.364511	val: 1.098645	test: 1.078593
MAE train: 0.299792	val: 0.861957	test: 0.869351

Epoch: 113
Loss: 0.13728186967117445
RMSE train: 0.409085	val: 1.041543	test: 1.015576
MAE train: 0.341608	val: 0.831084	test: 0.816774

Epoch: 114
Loss: 0.1413306168147496
RMSE train: 0.322449	val: 1.109647	test: 1.085928
MAE train: 0.260272	val: 0.880826	test: 0.883615

Epoch: 115
Loss: 0.12652067095041275
RMSE train: 0.320294	val: 1.080786	test: 1.048327
MAE train: 0.256912	val: 0.852934	test: 0.843635

Epoch: 116
Loss: 0.1297685599752835
RMSE train: 0.382217	val: 1.040688	test: 1.012903
MAE train: 0.319137	val: 0.828647	test: 0.815312

Epoch: 117
Loss: 0.1311527006328106
RMSE train: 0.389003	val: 1.038671	test: 0.996780
MAE train: 0.321538	val: 0.821852	test: 0.794179

Epoch: 118
Loss: 0.1480226687022618
RMSE train: 0.391644	val: 1.034925	test: 1.021363
MAE train: 0.320806	val: 0.836953	test: 0.820342

Epoch: 119
Loss: 0.12875801750591823
RMSE train: 0.345926	val: 1.044536	test: 1.029231
MAE train: 0.284376	val: 0.833915	test: 0.828085

Epoch: 120
Loss: 0.1360023772077901
RMSE train: 0.394847	val: 1.083005	test: 1.080646
MAE train: 0.328762	val: 0.876255	test: 0.875071

Epoch: 121
Loss: 0.12873489196811402
RMSE train: 0.325193	val: 1.112139	test: 1.082217
MAE train: 0.263895	val: 0.869415	test: 0.873265

Epoch: 122
Loss: 0.12154063742075648
RMSE train: 0.285604	val: 1.095935	test: 1.082999
MAE train: 0.229680	val: 0.867340	test: 0.883294

Epoch: 123
Loss: 0.1263858936727047
RMSE train: 0.357196	val: 1.070108	test: 1.049132
MAE train: 0.295970	val: 0.854733	test: 0.844928

Epoch: 124
Loss: 0.11720633187464305
RMSE train: 0.348688	val: 1.053316	test: 1.009747
MAE train: 0.287421	val: 0.840760	test: 0.812958

Epoch: 125
Loss: 0.12807294407061168
RMSE train: 0.446548	val: 1.058003	test: 1.027935
MAE train: 0.383840	val: 0.846807	test: 0.821277

Epoch: 126
Loss: 0.13178199636084692
RMSE train: 0.355110	val: 1.090895	test: 1.057432
MAE train: 0.294809	val: 0.865482	test: 0.859381

Epoch: 127
Loss: 0.11628964117595128
RMSE train: 0.300124	val: 1.090669	test: 1.038123
MAE train: 0.238535	val: 0.863497	test: 0.836193

Epoch: 128
Loss: 0.13272353155272348
RMSE train: 0.382970	val: 1.061632	test: 1.014567
MAE train: 0.320104	val: 0.846472	test: 0.825801

Epoch: 129
Loss: 0.1318179422191211
RMSE train: 0.312366	val: 1.089318	test: 1.051541
MAE train: 0.258117	val: 0.869106	test: 0.844803

Epoch: 130
Loss: 0.11261225704635892
RMSE train: 0.312257	val: 1.067550	test: 1.015171
MAE train: 0.253598	val: 0.843383	test: 0.818220

Epoch: 131
Loss: 0.12102046981453896
RMSE train: 0.347141	val: 1.043565	test: 1.019617
MAE train: 0.283081	val: 0.840772	test: 0.822657

Epoch: 132
Loss: 0.1074478876377855
RMSE train: 0.355782	val: 1.051459	test: 1.024956
MAE train: 0.292093	val: 0.841781	test: 0.822049

Epoch: 133
Loss: 0.11102027179939407
RMSE train: 0.362514	val: 1.037445	test: 1.016375
MAE train: 0.302594	val: 0.834174	test: 0.817491

Epoch: 134
Loss: 0.10548100939818791
RMSE train: 0.360943	val: 1.076448	test: 1.055354
MAE train: 0.301319	val: 0.867619	test: 0.856741

Epoch: 135
Loss: 0.1103746092745236
RMSE train: 0.356218	val: 1.049342	test: 1.008723
MAE train: 0.295022	val: 0.835248	test: 0.808964

Epoch: 136
Loss: 0.10679793570722852
RMSE train: 0.333125	val: 1.072817	test: 1.028626
MAE train: 0.273563	val: 0.854768	test: 0.825729

Epoch: 137
Loss: 0.13397720456123352
RMSE train: 0.319376	val: 1.049226	test: 1.013060
MAE train: 0.261211	val: 0.829214	test: 0.814797

Epoch: 138
Loss: 0.12024695106915065
RMSE train: 0.390440	val: 1.070925	test: 1.046249
MAE train: 0.329535	val: 0.857011	test: 0.840227

Epoch: 139
Loss: 0.1090290578348296
RMSE train: 0.275148	val: 1.089684	test: 1.047625
MAE train: 0.221359	val: 0.865862	test: 0.833469

Epoch: 140
Loss: 0.1067001883472715
RMSE train: 0.441237	val: 1.056651	test: 1.030091
MAE train: 0.373027	val: 0.850832	test: 0.835638

Epoch: 141
Loss: 0.12751815521291324
RMSE train: 0.353993	val: 1.080870	test: 1.064196
MAE train: 0.294594	val: 0.867326	test: 0.857287

Epoch: 142
Loss: 0.10481704665081841
RMSE train: 0.327894	val: 1.041993	test: 1.011311
MAE train: 0.270686	val: 0.822296	test: 0.815082
MAE train: 0.422204	val: 0.606490	test: 0.596275

Epoch: 84
Loss: 0.3291160890034267
RMSE train: 0.534410	val: 0.791722	test: 0.757936
MAE train: 0.415730	val: 0.598287	test: 0.592286

Epoch: 85
Loss: 0.3164538677249636
RMSE train: 0.526758	val: 0.791866	test: 0.754987
MAE train: 0.410311	val: 0.592251	test: 0.600666

Epoch: 86
Loss: 0.33003683388233185
RMSE train: 0.520860	val: 0.781409	test: 0.751087
MAE train: 0.406427	val: 0.594981	test: 0.592099

Epoch: 87
Loss: 0.33182435589177267
RMSE train: 0.561870	val: 0.815496	test: 0.772689
MAE train: 0.438895	val: 0.613987	test: 0.609368

Epoch: 88
Loss: 0.32595753243991304
RMSE train: 0.533342	val: 0.787551	test: 0.764400
MAE train: 0.414901	val: 0.599643	test: 0.604184

Epoch: 89
Loss: 0.32356210478714537
RMSE train: 0.533316	val: 0.794864	test: 0.758414
MAE train: 0.416359	val: 0.610140	test: 0.606371

Epoch: 90
Loss: 0.3166063300200871
RMSE train: 0.507597	val: 0.796026	test: 0.745796
MAE train: 0.391775	val: 0.599225	test: 0.588375

Epoch: 91
Loss: 0.3164165573460715
RMSE train: 0.519782	val: 0.775343	test: 0.750209
MAE train: 0.405701	val: 0.591206	test: 0.584997

Epoch: 92
Loss: 0.3052323652165277
RMSE train: 0.500279	val: 0.776180	test: 0.746126
MAE train: 0.388325	val: 0.587121	test: 0.586450

Epoch: 93
Loss: 0.30176638811826706
RMSE train: 0.514720	val: 0.800547	test: 0.761764
MAE train: 0.398862	val: 0.605671	test: 0.600013

Epoch: 94
Loss: 0.29518579478774754
RMSE train: 0.513875	val: 0.809175	test: 0.758006
MAE train: 0.399256	val: 0.615489	test: 0.595747

Epoch: 95
Loss: 0.30302357247897554
RMSE train: 0.530907	val: 0.803057	test: 0.774053
MAE train: 0.413661	val: 0.610583	test: 0.612362

Epoch: 96
Loss: 0.32015468393053326
RMSE train: 0.524131	val: 0.809875	test: 0.756751
MAE train: 0.408292	val: 0.620259	test: 0.596142

Epoch: 97
Loss: 0.32227509149483274
RMSE train: 0.511671	val: 0.779206	test: 0.743267
MAE train: 0.398105	val: 0.594628	test: 0.586937

Epoch: 98
Loss: 0.3172632370676313
RMSE train: 0.528072	val: 0.829618	test: 0.770146
MAE train: 0.410811	val: 0.626820	test: 0.609020

Epoch: 99
Loss: 0.31844723011766163
RMSE train: 0.525542	val: 0.825544	test: 0.775132
MAE train: 0.406875	val: 0.622783	test: 0.609364

Epoch: 100
Loss: 0.3092645640884127
RMSE train: 0.524010	val: 0.799033	test: 0.751359
MAE train: 0.407765	val: 0.604631	test: 0.588609

Epoch: 101
Loss: 0.3005162115607943
RMSE train: 0.503399	val: 0.771995	test: 0.753095
MAE train: 0.392312	val: 0.584635	test: 0.589415

Epoch: 102
Loss: 0.2994451959218298
RMSE train: 0.489148	val: 0.777617	test: 0.756248
MAE train: 0.379157	val: 0.587298	test: 0.591053

Epoch: 103
Loss: 0.29190976704869953
RMSE train: 0.484467	val: 0.769023	test: 0.749893
MAE train: 0.376332	val: 0.586067	test: 0.581924

Epoch: 104
Loss: 0.28545968128102167
RMSE train: 0.479071	val: 0.770631	test: 0.735447
MAE train: 0.370907	val: 0.581647	test: 0.574120

Epoch: 105
Loss: 0.2855912766286305
RMSE train: 0.508277	val: 0.776208	test: 0.754501
MAE train: 0.395077	val: 0.592711	test: 0.595941

Epoch: 106
Loss: 0.3108734680073602
RMSE train: 0.516424	val: 0.805824	test: 0.758077
MAE train: 0.402599	val: 0.614255	test: 0.595009

Epoch: 107
Loss: 0.2947509342006275
RMSE train: 0.508140	val: 0.810054	test: 0.755163
MAE train: 0.395065	val: 0.612057	test: 0.591097

Epoch: 108
Loss: 0.2782462164759636
RMSE train: 0.505392	val: 0.802665	test: 0.764649
MAE train: 0.390638	val: 0.603983	test: 0.601282

Epoch: 109
Loss: 0.2738478886229651
RMSE train: 0.492994	val: 0.798035	test: 0.739571
MAE train: 0.382941	val: 0.604155	test: 0.578534

Epoch: 110
Loss: 0.2974233776330948
RMSE train: 0.486091	val: 0.786470	test: 0.739845
MAE train: 0.377281	val: 0.594186	test: 0.578793

Epoch: 111
Loss: 0.28096667357853483
RMSE train: 0.515012	val: 0.829221	test: 0.769195
MAE train: 0.399441	val: 0.622799	test: 0.604105

Epoch: 112
Loss: 0.2806199554886137
RMSE train: 0.503696	val: 0.805253	test: 0.758743
MAE train: 0.390753	val: 0.606077	test: 0.593758

Epoch: 113
Loss: 0.2862776964902878
RMSE train: 0.469166	val: 0.783652	test: 0.742782
MAE train: 0.363026	val: 0.594488	test: 0.580068

Epoch: 114
Loss: 0.29598752834967207
RMSE train: 0.496028	val: 0.796072	test: 0.743521
MAE train: 0.386670	val: 0.608378	test: 0.579963

Epoch: 115
Loss: 0.2743253069264548
RMSE train: 0.488170	val: 0.770576	test: 0.742293
MAE train: 0.382358	val: 0.581613	test: 0.580342

Epoch: 116
Loss: 0.2898231883134161
RMSE train: 0.484562	val: 0.782609	test: 0.732165
MAE train: 0.377501	val: 0.591346	test: 0.572487

Epoch: 117
Loss: 0.2881658598780632
RMSE train: 0.474013	val: 0.790946	test: 0.728666
MAE train: 0.367418	val: 0.598940	test: 0.565380

Epoch: 118
Loss: 0.3192723648888724
RMSE train: 0.484324	val: 0.778430	test: 0.732900
MAE train: 0.379334	val: 0.589208	test: 0.574824

Epoch: 119
Loss: 0.2761473134160042
RMSE train: 0.487381	val: 0.780305	test: 0.747297
MAE train: 0.378731	val: 0.589076	test: 0.585170

Epoch: 120
Loss: 0.2786542464579855
RMSE train: 0.487687	val: 0.773733	test: 0.738456
MAE train: 0.383143	val: 0.590068	test: 0.579978

Epoch: 121
Loss: 0.2800262261714254
RMSE train: 0.474618	val: 0.789660	test: 0.725736
MAE train: 0.369056	val: 0.594709	test: 0.568975

Epoch: 122
Loss: 0.27399588163409916
RMSE train: 0.490685	val: 0.814608	test: 0.746701
MAE train: 0.378699	val: 0.613061	test: 0.581256

Epoch: 123
Loss: 0.28623837551900316
RMSE train: 0.464483	val: 0.788334	test: 0.731108
MAE train: 0.359835	val: 0.588715	test: 0.573187

Epoch: 124
Loss: 0.27013511104243143
RMSE train: 0.480161	val: 0.788938	test: 0.757775
MAE train: 0.373738	val: 0.601912	test: 0.593387

Epoch: 125
Loss: 0.2775008838091578
RMSE train: 0.466345	val: 0.779334	test: 0.734974
MAE train: 0.362333	val: 0.591760	test: 0.574538

Epoch: 126
Loss: 0.25948055194956915
RMSE train: 0.495564	val: 0.778143	test: 0.743967
MAE train: 0.383720	val: 0.593047	test: 0.581188

Epoch: 127
Loss: 0.2565116531082562
RMSE train: 0.459333	val: 0.764860	test: 0.742554
MAE train: 0.357082	val: 0.577250	test: 0.577304

Epoch: 128
Loss: 0.25910088207040516
RMSE train: 0.461251	val: 0.780971	test: 0.739571
MAE train: 0.355533	val: 0.589161	test: 0.574671

Epoch: 129
Loss: 0.26517667302063536
RMSE train: 0.461650	val: 0.773634	test: 0.743510
MAE train: 0.359149	val: 0.586326	test: 0.578211

Epoch: 130
Loss: 0.25944954263312475
RMSE train: 0.463021	val: 0.775573	test: 0.749247
MAE train: 0.361022	val: 0.592505	test: 0.585405

Epoch: 131
Loss: 0.2620223302926336
RMSE train: 0.464097	val: 0.780083	test: 0.745476
MAE train: 0.362795	val: 0.589677	test: 0.581046

Epoch: 132
Loss: 0.26023945425237927
RMSE train: 0.484763	val: 0.778582	test: 0.754795
MAE train: 0.377428	val: 0.591992	test: 0.591662

Epoch: 133
Loss: 0.2540339529514313
RMSE train: 0.466118	val: 0.777752	test: 0.763921
MAE train: 0.361815	val: 0.586412	test: 0.600060

Epoch: 134
Loss: 0.26607852642025265
RMSE train: 0.458860	val: 0.766576	test: 0.749954
MAE train: 0.359583	val: 0.578785	test: 0.580448

Epoch: 135
Loss: 0.2806134575179645
RMSE train: 0.458391	val: 0.773867	test: 0.748135
MAE train: 0.357114	val: 0.586506	test: 0.579830

Epoch: 136
Loss: 0.2672613891107695
RMSE train: 0.462547	val: 0.770077	test: 0.745212
MAE train: 0.363661	val: 0.583149	test: 0.585046

Epoch: 137
Loss: 0.26976118449653896
RMSE train: 0.458510	val: 0.786154	test: 0.752105
MAE train: 0.358606	val: 0.598167	test: 0.585456

Epoch: 138
Loss: 0.2542957088776997
RMSE train: 0.460894	val: 0.793538	test: 0.771063
MAE train: 0.359144	val: 0.608248	test: 0.597696

Epoch: 139
Loss: 0.273931442626885
RMSE train: 0.482406	val: 0.800195	test: 0.771686
MAE train: 0.379327	val: 0.604466	test: 0.601685

Epoch: 140
Loss: 0.2689100259116718
RMSE train: 0.489129	val: 0.820675	test: 0.778091
MAE train: 0.382429	val: 0.623701	test: 0.608497

Epoch: 141
Loss: 0.24904937509979522
RMSE train: 0.463686	val: 0.787164	test: 0.763468
MAE train: 0.361749	val: 0.595604	test: 0.593495

Epoch: 142
Loss: 0.24248338916472026
RMSE train: 0.445688	val: 0.780426	test: 0.748543
MAE train: 0.346218	val: 0.583983	test: 0.586567

Epoch: 143
Loss: 0.2487159531031336
RMSE train: 0.458517	val: 0.788733	test: 0.758066
MAE train: 0.359409	val: 0.595287	test: 0.593768
MAE train: 0.382086	val: 0.676982	test: 0.657142

Epoch: 83
Loss: 0.24868840724229813
RMSE train: 0.420790	val: 0.860153	test: 0.821550
MAE train: 0.339154	val: 0.672077	test: 0.652232

Epoch: 84
Loss: 0.2541816830635071
RMSE train: 0.398952	val: 0.882198	test: 0.839370
MAE train: 0.316025	val: 0.694254	test: 0.672117

Epoch: 85
Loss: 0.2371659544961793
RMSE train: 0.401574	val: 0.883123	test: 0.833173
MAE train: 0.317403	val: 0.679859	test: 0.672313

Epoch: 86
Loss: 0.23828129470348358
RMSE train: 0.392771	val: 0.895260	test: 0.838848
MAE train: 0.309537	val: 0.704806	test: 0.671123

Epoch: 87
Loss: 0.21853564360312053
RMSE train: 0.390301	val: 0.868753	test: 0.835563
MAE train: 0.311600	val: 0.675611	test: 0.670638

Epoch: 88
Loss: 0.22262261701481684
RMSE train: 0.406068	val: 0.857257	test: 0.809821
MAE train: 0.325761	val: 0.665085	test: 0.647241

Epoch: 89
Loss: 0.22784441709518433
RMSE train: 0.385463	val: 0.872671	test: 0.830979
MAE train: 0.305875	val: 0.678875	test: 0.673182

Epoch: 90
Loss: 0.2202578687242099
RMSE train: 0.432219	val: 0.864787	test: 0.827095
MAE train: 0.350667	val: 0.678752	test: 0.664231

Epoch: 91
Loss: 0.20980110125882284
RMSE train: 0.366803	val: 0.861418	test: 0.824703
MAE train: 0.289600	val: 0.673174	test: 0.661370

Epoch: 92
Loss: 0.20446027176720755
RMSE train: 0.379879	val: 0.859341	test: 0.812437
MAE train: 0.302699	val: 0.671901	test: 0.652573

Epoch: 93
Loss: 0.2107859913791929
RMSE train: 0.438244	val: 0.871893	test: 0.823434
MAE train: 0.355599	val: 0.676870	test: 0.662098

Epoch: 94
Loss: 0.2150048017501831
RMSE train: 0.403645	val: 0.871146	test: 0.826106
MAE train: 0.320420	val: 0.680494	test: 0.666829

Epoch: 95
Loss: 0.21038179312433516
RMSE train: 0.386443	val: 0.875360	test: 0.816236
MAE train: 0.309976	val: 0.684499	test: 0.658001

Epoch: 96
Loss: 0.20125456154346466
RMSE train: 0.357892	val: 0.867788	test: 0.805202
MAE train: 0.284863	val: 0.684817	test: 0.642371

Epoch: 97
Loss: 0.19478664227894374
RMSE train: 0.370284	val: 0.887257	test: 0.835232
MAE train: 0.294095	val: 0.694173	test: 0.673249

Epoch: 98
Loss: 0.19865513060774123
RMSE train: 0.393308	val: 0.859688	test: 0.818114
MAE train: 0.318276	val: 0.666257	test: 0.658835

Epoch: 99
Loss: 0.19410164547818048
RMSE train: 0.379404	val: 0.874455	test: 0.823127
MAE train: 0.303035	val: 0.685015	test: 0.661164

Epoch: 100
Loss: 0.20182796567678452
RMSE train: 0.381227	val: 0.876007	test: 0.819659
MAE train: 0.304407	val: 0.687504	test: 0.662268

Epoch: 101
Loss: 0.19163491364036286
RMSE train: 0.378823	val: 0.876050	test: 0.816955
MAE train: 0.301944	val: 0.679696	test: 0.657527

Epoch: 102
Loss: 0.1914123339312417
RMSE train: 0.355292	val: 0.883169	test: 0.822280
MAE train: 0.282673	val: 0.691070	test: 0.657780

Epoch: 103
Loss: 0.18093643124614442
RMSE train: 0.347546	val: 0.870995	test: 0.807068
MAE train: 0.274873	val: 0.674760	test: 0.641357

Epoch: 104
Loss: 0.19138228041785105
RMSE train: 0.339639	val: 0.863977	test: 0.806338
MAE train: 0.269763	val: 0.679482	test: 0.645942

Epoch: 105
Loss: 0.18917734069483622
RMSE train: 0.347126	val: 0.885467	test: 0.822885
MAE train: 0.276421	val: 0.694466	test: 0.658224

Epoch: 106
Loss: 0.16735490943704331
RMSE train: 0.375912	val: 0.867248	test: 0.806233
MAE train: 0.302954	val: 0.681356	test: 0.643727

Epoch: 107
Loss: 0.17698080305542266
RMSE train: 0.345253	val: 0.850964	test: 0.811659
MAE train: 0.277584	val: 0.657545	test: 0.649973

Epoch: 108
Loss: 0.18371846952608653
RMSE train: 0.386857	val: 0.863241	test: 0.816787
MAE train: 0.311976	val: 0.669059	test: 0.659752

Epoch: 109
Loss: 0.18345953417675837
RMSE train: 0.382453	val: 0.868332	test: 0.815865
MAE train: 0.309318	val: 0.680213	test: 0.653149

Epoch: 110
Loss: 0.17285016924142838
RMSE train: 0.389052	val: 0.878368	test: 0.822991
MAE train: 0.310237	val: 0.691002	test: 0.662311

Epoch: 111
Loss: 0.1669179924896785
RMSE train: 0.385832	val: 0.865579	test: 0.808727
MAE train: 0.316139	val: 0.671436	test: 0.641478

Epoch: 112
Loss: 0.17258855061871664
RMSE train: 0.361008	val: 0.912189	test: 0.826175
MAE train: 0.286073	val: 0.719543	test: 0.664881

Epoch: 113
Loss: 0.15134149470499583
RMSE train: 0.314101	val: 0.870623	test: 0.802654
MAE train: 0.248863	val: 0.681885	test: 0.643530

Epoch: 114
Loss: 0.16134593103613173
RMSE train: 0.353061	val: 0.868704	test: 0.809726
MAE train: 0.283405	val: 0.681497	test: 0.644658

Epoch: 115
Loss: 0.1681001175727163
RMSE train: 0.332544	val: 0.862341	test: 0.815986
MAE train: 0.265858	val: 0.673132	test: 0.653732

Epoch: 116
Loss: 0.15358574581997736
RMSE train: 0.346349	val: 0.903187	test: 0.834283
MAE train: 0.275451	val: 0.708602	test: 0.673548

Epoch: 117
Loss: 0.15924559107848577
RMSE train: 0.345692	val: 0.864049	test: 0.813369
MAE train: 0.279460	val: 0.668103	test: 0.654417

Epoch: 118
Loss: 0.1544733734003135
RMSE train: 0.342243	val: 0.892388	test: 0.818033
MAE train: 0.272000	val: 0.704736	test: 0.657112

Epoch: 119
Loss: 0.15799901634454727
RMSE train: 0.310252	val: 0.874781	test: 0.815910
MAE train: 0.246073	val: 0.682112	test: 0.645567

Epoch: 120
Loss: 0.16247034605060304
RMSE train: 0.326592	val: 0.856032	test: 0.801811
MAE train: 0.260127	val: 0.670054	test: 0.639707

Epoch: 121
Loss: 0.14537632146051951
RMSE train: 0.317031	val: 0.855751	test: 0.808179
MAE train: 0.252514	val: 0.671195	test: 0.644998

Epoch: 122
Loss: 0.16005227927650725
RMSE train: 0.347967	val: 0.880045	test: 0.807113
MAE train: 0.278983	val: 0.688998	test: 0.646465

Epoch: 123
Loss: 0.1525455644088132
RMSE train: 0.317695	val: 0.877696	test: 0.812635
MAE train: 0.254465	val: 0.694317	test: 0.649373

Epoch: 124
Loss: 0.14725378049271448
RMSE train: 0.373335	val: 0.882228	test: 0.820674
MAE train: 0.301952	val: 0.695249	test: 0.648736

Epoch: 125
Loss: 0.14013050496578217
RMSE train: 0.333119	val: 0.872310	test: 0.815780
MAE train: 0.266222	val: 0.687577	test: 0.645581

Epoch: 126
Loss: 0.14495160058140755
RMSE train: 0.321776	val: 0.859788	test: 0.815110
MAE train: 0.257459	val: 0.677537	test: 0.651853

Epoch: 127
Loss: 0.13636744714209012
RMSE train: 0.326913	val: 0.859849	test: 0.809588
MAE train: 0.261620	val: 0.675943	test: 0.647380

Epoch: 128
Loss: 0.1435802403305258
RMSE train: 0.297539	val: 0.871230	test: 0.821271
MAE train: 0.234800	val: 0.680446	test: 0.653071

Epoch: 129
Loss: 0.14168286908950126
RMSE train: 0.277866	val: 0.873960	test: 0.823251
MAE train: 0.217823	val: 0.686128	test: 0.657648

Epoch: 130
Loss: 0.13800773130995886
RMSE train: 0.327256	val: 0.856366	test: 0.811327
MAE train: 0.263034	val: 0.671851	test: 0.651049

Epoch: 131
Loss: 0.1426422159586634
RMSE train: 0.300170	val: 0.870408	test: 0.800933
MAE train: 0.238527	val: 0.685095	test: 0.638365

Epoch: 132
Loss: 0.14015038524355208
RMSE train: 0.330306	val: 0.878158	test: 0.813280
MAE train: 0.263987	val: 0.690839	test: 0.647201

Epoch: 133
Loss: 0.13648151765976632
RMSE train: 0.307548	val: 0.864469	test: 0.806060
MAE train: 0.245856	val: 0.679032	test: 0.635468

Epoch: 134
Loss: 0.13342250085302762
RMSE train: 0.303248	val: 0.868729	test: 0.814962
MAE train: 0.241470	val: 0.679177	test: 0.652620

Epoch: 135
Loss: 0.12824516743421555
RMSE train: 0.345393	val: 0.864715	test: 0.807558
MAE train: 0.281060	val: 0.674885	test: 0.646338

Epoch: 136
Loss: 0.13791830358760698
RMSE train: 0.322709	val: 0.858036	test: 0.806050
MAE train: 0.263693	val: 0.666320	test: 0.642119

Epoch: 137
Loss: 0.1406087992446763
RMSE train: 0.302554	val: 0.868197	test: 0.799986
MAE train: 0.242752	val: 0.686837	test: 0.641117

Epoch: 138
Loss: 0.13406649019036973
RMSE train: 0.383838	val: 0.863885	test: 0.811967
MAE train: 0.317658	val: 0.675487	test: 0.646591

Epoch: 139
Loss: 0.13442624839288847
RMSE train: 0.292646	val: 0.864445	test: 0.813076
MAE train: 0.233705	val: 0.681389	test: 0.647614

Epoch: 140
Loss: 0.13114859483071736
RMSE train: 0.310378	val: 0.923270	test: 0.860111
MAE train: 0.245157	val: 0.726166	test: 0.688550

Epoch: 141
Loss: 0.13675572031310626
RMSE train: 0.277245	val: 0.859315	test: 0.807160
MAE train: 0.219697	val: 0.676688	test: 0.647389

Epoch: 142
Loss: 0.1146592992757048
RMSE train: 0.319111	val: 0.851061	test: 0.808809
MAE train: 0.257882	val: 0.660410	test: 0.650985

Early stopping
Best (RMSE):	 train: 0.345253	val: 0.850964	test: 0.811659
Best (MAE):	 train: 0.277584	val: 0.657545	test: 0.649973
All runs completed.

MAE train: 0.427804	val: 0.604473	test: 0.598729

Epoch: 84
Loss: 0.32995360238211496
RMSE train: 0.513808	val: 0.773519	test: 0.749119
MAE train: 0.397981	val: 0.587297	test: 0.588597

Epoch: 85
Loss: 0.3156393936702183
RMSE train: 0.519010	val: 0.787524	test: 0.752732
MAE train: 0.402299	val: 0.598415	test: 0.586254

Epoch: 86
Loss: 0.3406487022127424
RMSE train: 0.531077	val: 0.797414	test: 0.762173
MAE train: 0.409227	val: 0.602136	test: 0.595282

Epoch: 87
Loss: 0.31451682107789175
RMSE train: 0.525296	val: 0.793031	test: 0.768458
MAE train: 0.404195	val: 0.599601	test: 0.598203

Epoch: 88
Loss: 0.3138453108923776
RMSE train: 0.499122	val: 0.764101	test: 0.737155
MAE train: 0.389328	val: 0.574536	test: 0.583432

Epoch: 89
Loss: 0.3141040951013565
RMSE train: 0.523272	val: 0.785043	test: 0.750108
MAE train: 0.404049	val: 0.596130	test: 0.594663

Epoch: 90
Loss: 0.30609440377780367
RMSE train: 0.525879	val: 0.800274	test: 0.750499
MAE train: 0.405180	val: 0.607978	test: 0.588397

Epoch: 91
Loss: 0.3077599970357759
RMSE train: 0.510471	val: 0.768575	test: 0.744218
MAE train: 0.395229	val: 0.583009	test: 0.586221

Epoch: 92
Loss: 0.3095427762184824
RMSE train: 0.483656	val: 0.762281	test: 0.741097
MAE train: 0.374514	val: 0.581161	test: 0.580761

Epoch: 93
Loss: 0.3284336009195873
RMSE train: 0.517769	val: 0.772627	test: 0.755446
MAE train: 0.400041	val: 0.588887	test: 0.591418

Epoch: 94
Loss: 0.3058449115071978
RMSE train: 0.538756	val: 0.808706	test: 0.765409
MAE train: 0.419767	val: 0.614104	test: 0.601758

Epoch: 95
Loss: 0.2970488667488098
RMSE train: 0.513810	val: 0.802067	test: 0.759399
MAE train: 0.395478	val: 0.606884	test: 0.592886

Epoch: 96
Loss: 0.28994310327938627
RMSE train: 0.489374	val: 0.774504	test: 0.746039
MAE train: 0.375119	val: 0.586778	test: 0.588011

Epoch: 97
Loss: 0.2771683580109051
RMSE train: 0.495861	val: 0.777371	test: 0.750957
MAE train: 0.381785	val: 0.587398	test: 0.586334

Epoch: 98
Loss: 0.27909792214632034
RMSE train: 0.510171	val: 0.797045	test: 0.758835
MAE train: 0.394980	val: 0.603374	test: 0.592794

Epoch: 99
Loss: 0.28128665472779957
RMSE train: 0.519795	val: 0.801892	test: 0.768470
MAE train: 0.399312	val: 0.607133	test: 0.600905

Epoch: 100
Loss: 0.31001111013548716
RMSE train: 0.481327	val: 0.767387	test: 0.748968
MAE train: 0.370872	val: 0.580512	test: 0.580713

Epoch: 101
Loss: 0.3007044515439442
RMSE train: 0.501833	val: 0.796354	test: 0.750120
MAE train: 0.389119	val: 0.608438	test: 0.582408

Epoch: 102
Loss: 0.30628344629492077
RMSE train: 0.545472	val: 0.822264	test: 0.766603
MAE train: 0.424099	val: 0.630961	test: 0.601784

Epoch: 103
Loss: 0.2874286323785782
RMSE train: 0.478563	val: 0.762365	test: 0.741438
MAE train: 0.368761	val: 0.582744	test: 0.577040

Epoch: 104
Loss: 0.28483364305325914
RMSE train: 0.503881	val: 0.800860	test: 0.754024
MAE train: 0.388329	val: 0.607810	test: 0.586887

Epoch: 105
Loss: 0.27701148071459364
RMSE train: 0.489709	val: 0.781289	test: 0.746585
MAE train: 0.378384	val: 0.592494	test: 0.579908

Epoch: 106
Loss: 0.27505405460085186
RMSE train: 0.497005	val: 0.796053	test: 0.754052
MAE train: 0.383151	val: 0.602971	test: 0.585474

Epoch: 107
Loss: 0.2803157663771084
RMSE train: 0.505882	val: 0.795781	test: 0.743374
MAE train: 0.392898	val: 0.606633	test: 0.573397

Epoch: 108
Loss: 0.2879220183406557
RMSE train: 0.498086	val: 0.774178	test: 0.752150
MAE train: 0.384455	val: 0.590798	test: 0.587944

Epoch: 109
Loss: 0.2889247462153435
RMSE train: 0.478469	val: 0.766327	test: 0.741072
MAE train: 0.369792	val: 0.582003	test: 0.571968

Epoch: 110
Loss: 0.275107561477593
RMSE train: 0.522330	val: 0.776367	test: 0.748429
MAE train: 0.403815	val: 0.593467	test: 0.585324

Epoch: 111
Loss: 0.28131381954465595
RMSE train: 0.516124	val: 0.797344	test: 0.773227
MAE train: 0.399700	val: 0.608938	test: 0.603336

Epoch: 112
Loss: 0.28363529060568127
RMSE train: 0.488533	val: 0.785126	test: 0.759193
MAE train: 0.379197	val: 0.600538	test: 0.586963

Epoch: 113
Loss: 0.282695186989648
RMSE train: 0.504642	val: 0.793218	test: 0.758398
MAE train: 0.388498	val: 0.601106	test: 0.589149

Epoch: 114
Loss: 0.2822235066975866
RMSE train: 0.527678	val: 0.816921	test: 0.776171
MAE train: 0.411851	val: 0.627474	test: 0.602135

Epoch: 115
Loss: 0.2823097418461527
RMSE train: 0.481166	val: 0.770970	test: 0.746151
MAE train: 0.370329	val: 0.586845	test: 0.574538

Epoch: 116
Loss: 0.27670072338410784
RMSE train: 0.464839	val: 0.766609	test: 0.748521
MAE train: 0.357121	val: 0.583584	test: 0.579288

Epoch: 117
Loss: 0.27503599652222227
RMSE train: 0.495248	val: 0.778171	test: 0.746287
MAE train: 0.384012	val: 0.593151	test: 0.585958

Epoch: 118
Loss: 0.27426004090479444
RMSE train: 0.485562	val: 0.786824	test: 0.764813
MAE train: 0.375694	val: 0.595944	test: 0.593235

Epoch: 119
Loss: 0.31108108375753674
RMSE train: 0.477860	val: 0.766115	test: 0.737056
MAE train: 0.369971	val: 0.582939	test: 0.571853

Epoch: 120
Loss: 0.27737348846026827
RMSE train: 0.461604	val: 0.766774	test: 0.741319
MAE train: 0.354582	val: 0.581478	test: 0.577248

Epoch: 121
Loss: 0.2674979471734592
RMSE train: 0.488562	val: 0.775221	test: 0.745953
MAE train: 0.376863	val: 0.595906	test: 0.581556

Epoch: 122
Loss: 0.2964168967945235
RMSE train: 0.547839	val: 0.829323	test: 0.785808
MAE train: 0.426475	val: 0.630921	test: 0.617448

Epoch: 123
Loss: 0.27285817159073694
RMSE train: 0.533885	val: 0.845841	test: 0.802290
MAE train: 0.417735	val: 0.637293	test: 0.629427

Epoch: 124
Loss: 0.272351684314864
RMSE train: 0.488273	val: 0.781385	test: 0.751741
MAE train: 0.377019	val: 0.597358	test: 0.580782

Epoch: 125
Loss: 0.27458540137325016
RMSE train: 0.480962	val: 0.761477	test: 0.746277
MAE train: 0.372785	val: 0.580538	test: 0.587081

Epoch: 126
Loss: 0.27129235970122473
RMSE train: 0.475116	val: 0.751723	test: 0.759059
MAE train: 0.367878	val: 0.581129	test: 0.590092

Epoch: 127
Loss: 0.2605612331203052
RMSE train: 0.478840	val: 0.772408	test: 0.741703
MAE train: 0.369823	val: 0.588682	test: 0.572270

Epoch: 128
Loss: 0.25681852442877634
RMSE train: 0.473776	val: 0.784944	test: 0.755402
MAE train: 0.365296	val: 0.596006	test: 0.589772

Epoch: 129
Loss: 0.2575244669403349
RMSE train: 0.462644	val: 0.768680	test: 0.735328
MAE train: 0.356661	val: 0.586346	test: 0.569338

Epoch: 130
Loss: 0.25554279876606806
RMSE train: 0.476248	val: 0.782909	test: 0.742861
MAE train: 0.366786	val: 0.593800	test: 0.572454

Epoch: 131
Loss: 0.2641654812863895
RMSE train: 0.468350	val: 0.790384	test: 0.752124
MAE train: 0.359345	val: 0.598985	test: 0.581472

Epoch: 132
Loss: 0.2700010250721659
RMSE train: 0.454619	val: 0.769003	test: 0.736664
MAE train: 0.351135	val: 0.582385	test: 0.564241

Epoch: 133
Loss: 0.2483612820506096
RMSE train: 0.454190	val: 0.772389	test: 0.740217
MAE train: 0.351220	val: 0.587593	test: 0.573726

Epoch: 134
Loss: 0.25002171844244003
RMSE train: 0.473748	val: 0.781104	test: 0.747871
MAE train: 0.365682	val: 0.590443	test: 0.578221

Epoch: 135
Loss: 0.24767540182386125
RMSE train: 0.462064	val: 0.785185	test: 0.748354
MAE train: 0.355838	val: 0.595589	test: 0.582659

Epoch: 136
Loss: 0.2650747746229172
RMSE train: 0.455878	val: 0.779215	test: 0.734933
MAE train: 0.349910	val: 0.591776	test: 0.572473

Epoch: 137
Loss: 0.23992879262992314
RMSE train: 0.466598	val: 0.779765	test: 0.745482
MAE train: 0.359670	val: 0.593762	test: 0.575578

Epoch: 138
Loss: 0.24756470216172083
RMSE train: 0.455467	val: 0.773654	test: 0.739623
MAE train: 0.349813	val: 0.587295	test: 0.570512

Epoch: 139
Loss: 0.2400955164006778
RMSE train: 0.449372	val: 0.770872	test: 0.747114
MAE train: 0.345210	val: 0.586224	test: 0.574557

Epoch: 140
Loss: 0.24278407650334494
RMSE train: 0.447286	val: 0.760986	test: 0.734924
MAE train: 0.345735	val: 0.582509	test: 0.567290

Epoch: 141
Loss: 0.2595436828477042
RMSE train: 0.458806	val: 0.780337	test: 0.735982
MAE train: 0.353923	val: 0.594874	test: 0.568591

Epoch: 142
Loss: 0.23922962376049586
RMSE train: 0.469833	val: 0.787388	test: 0.749943
MAE train: 0.364008	val: 0.590143	test: 0.582333

Epoch: 143
Loss: 0.24068118631839752
RMSE train: 0.444929	val: 0.767765	test: 0.733311
MAE train: 0.342123	val: 0.584748	test: 0.565317
MAE train: 0.404218	val: 0.603560	test: 0.587937

Epoch: 84
Loss: 0.3326034737484796
RMSE train: 0.503609	val: 0.786164	test: 0.762445
MAE train: 0.390624	val: 0.594181	test: 0.591641

Epoch: 85
Loss: 0.33804890726293835
RMSE train: 0.515274	val: 0.785039	test: 0.754712
MAE train: 0.396182	val: 0.589650	test: 0.582935

Epoch: 86
Loss: 0.313016802072525
RMSE train: 0.501383	val: 0.779767	test: 0.752427
MAE train: 0.388457	val: 0.585190	test: 0.587846

Epoch: 87
Loss: 0.31174921563693453
RMSE train: 0.505963	val: 0.791481	test: 0.757095
MAE train: 0.391862	val: 0.593566	test: 0.592135

Epoch: 88
Loss: 0.31581454404762815
RMSE train: 0.512147	val: 0.774109	test: 0.761773
MAE train: 0.395524	val: 0.584970	test: 0.597649

Epoch: 89
Loss: 0.3100536870104926
RMSE train: 0.534519	val: 0.793186	test: 0.772685
MAE train: 0.419193	val: 0.605122	test: 0.607045

Epoch: 90
Loss: 0.3229354407106127
RMSE train: 0.515523	val: 0.785386	test: 0.761118
MAE train: 0.397721	val: 0.592015	test: 0.593198

Epoch: 91
Loss: 0.32145934658391134
RMSE train: 0.507665	val: 0.782093	test: 0.758174
MAE train: 0.396399	val: 0.599954	test: 0.592394

Epoch: 92
Loss: 0.3342436296599252
RMSE train: 0.514151	val: 0.788321	test: 0.754895
MAE train: 0.398556	val: 0.599065	test: 0.590471

Epoch: 93
Loss: 0.3298444684062685
RMSE train: 0.527324	val: 0.794312	test: 0.762081
MAE train: 0.408478	val: 0.599981	test: 0.597912

Epoch: 94
Loss: 0.314307627933366
RMSE train: 0.495929	val: 0.774257	test: 0.756690
MAE train: 0.383847	val: 0.590797	test: 0.590044

Epoch: 95
Loss: 0.32995335119111197
RMSE train: 0.514630	val: 0.796909	test: 0.753631
MAE train: 0.398543	val: 0.603755	test: 0.586212

Epoch: 96
Loss: 0.31156819633075167
RMSE train: 0.514843	val: 0.790910	test: 0.762389
MAE train: 0.404070	val: 0.606359	test: 0.594751

Epoch: 97
Loss: 0.30358279815741945
RMSE train: 0.500803	val: 0.779576	test: 0.744536
MAE train: 0.388846	val: 0.592974	test: 0.577518

Epoch: 98
Loss: 0.3136366776057652
RMSE train: 0.510569	val: 0.789092	test: 0.752095
MAE train: 0.396637	val: 0.597137	test: 0.583823

Epoch: 99
Loss: 0.2974275393145425
RMSE train: 0.501334	val: 0.792922	test: 0.758239
MAE train: 0.387436	val: 0.598858	test: 0.589363

Epoch: 100
Loss: 0.2947539474282946
RMSE train: 0.504628	val: 0.775728	test: 0.749365
MAE train: 0.390917	val: 0.589308	test: 0.587709

Epoch: 101
Loss: 0.3084511767540659
RMSE train: 0.481339	val: 0.775096	test: 0.752050
MAE train: 0.372991	val: 0.585997	test: 0.578465

Epoch: 102
Loss: 0.2870023846626282
RMSE train: 0.508539	val: 0.775155	test: 0.757012
MAE train: 0.396020	val: 0.592305	test: 0.594569

Epoch: 103
Loss: 0.3041450615440096
RMSE train: 0.468387	val: 0.773375	test: 0.752163
MAE train: 0.359946	val: 0.583565	test: 0.579269

Epoch: 104
Loss: 0.304321499807494
RMSE train: 0.504113	val: 0.779303	test: 0.767231
MAE train: 0.390916	val: 0.599160	test: 0.603558

Epoch: 105
Loss: 0.29891716475997654
RMSE train: 0.498254	val: 0.775908	test: 0.748407
MAE train: 0.385879	val: 0.588204	test: 0.584559

Epoch: 106
Loss: 0.2845739402941295
RMSE train: 0.476245	val: 0.764765	test: 0.754769
MAE train: 0.367583	val: 0.580495	test: 0.584811

Epoch: 107
Loss: 0.28098389293466297
RMSE train: 0.487410	val: 0.767440	test: 0.749758
MAE train: 0.376786	val: 0.586688	test: 0.585570

Epoch: 108
Loss: 0.27790562382766176
RMSE train: 0.478016	val: 0.781320	test: 0.753882
MAE train: 0.368324	val: 0.595249	test: 0.584957

Epoch: 109
Loss: 0.26265340937035425
RMSE train: 0.481582	val: 0.776048	test: 0.750118
MAE train: 0.370886	val: 0.592048	test: 0.585597

Epoch: 110
Loss: 0.299456533576761
RMSE train: 0.482146	val: 0.787465	test: 0.757520
MAE train: 0.372488	val: 0.597759	test: 0.588802

Epoch: 111
Loss: 0.2717446173940386
RMSE train: 0.483930	val: 0.796855	test: 0.760446
MAE train: 0.373778	val: 0.605658	test: 0.587210

Epoch: 112
Loss: 0.2731088325381279
RMSE train: 0.479732	val: 0.773126	test: 0.752315
MAE train: 0.370115	val: 0.585819	test: 0.585877

Epoch: 113
Loss: 0.2733565622142383
RMSE train: 0.481042	val: 0.771490	test: 0.764606
MAE train: 0.372094	val: 0.584590	test: 0.587213

Epoch: 114
Loss: 0.27942732189382824
RMSE train: 0.495812	val: 0.766540	test: 0.745553
MAE train: 0.386088	val: 0.591334	test: 0.582861

Epoch: 115
Loss: 0.2619220497352736
RMSE train: 0.475060	val: 0.756283	test: 0.756486
MAE train: 0.367497	val: 0.579678	test: 0.586762

Epoch: 116
Loss: 0.27153754021440235
RMSE train: 0.472588	val: 0.772815	test: 0.750582
MAE train: 0.365832	val: 0.596376	test: 0.577048

Epoch: 117
Loss: 0.27552483124392374
RMSE train: 0.501891	val: 0.796150	test: 0.762648
MAE train: 0.391562	val: 0.605599	test: 0.594860

Epoch: 118
Loss: 0.27143005281686783
RMSE train: 0.466029	val: 0.793727	test: 0.759341
MAE train: 0.360027	val: 0.598329	test: 0.583723

Epoch: 119
Loss: 0.26155572491032736
RMSE train: 0.483070	val: 0.774409	test: 0.756424
MAE train: 0.375372	val: 0.591703	test: 0.587468

Epoch: 120
Loss: 0.2731172793677875
RMSE train: 0.465589	val: 0.780460	test: 0.763123
MAE train: 0.362008	val: 0.595426	test: 0.598708

Epoch: 121
Loss: 0.2752979504210608
RMSE train: 0.499314	val: 0.778722	test: 0.786188
MAE train: 0.394012	val: 0.600905	test: 0.619385

Epoch: 122
Loss: 0.26481134444475174
RMSE train: 0.479492	val: 0.772612	test: 0.760782
MAE train: 0.374811	val: 0.594370	test: 0.596009

Epoch: 123
Loss: 0.27785076413835796
RMSE train: 0.472462	val: 0.772583	test: 0.750611
MAE train: 0.367346	val: 0.589851	test: 0.586482

Epoch: 124
Loss: 0.2590662645442145
RMSE train: 0.462579	val: 0.759064	test: 0.762690
MAE train: 0.359375	val: 0.583157	test: 0.597582

Epoch: 125
Loss: 0.25737414509058
RMSE train: 0.454841	val: 0.780212	test: 0.761529
MAE train: 0.351428	val: 0.596375	test: 0.589094

Epoch: 126
Loss: 0.25891335947172983
RMSE train: 0.456608	val: 0.771655	test: 0.755788
MAE train: 0.354000	val: 0.587979	test: 0.584693

Epoch: 127
Loss: 0.2611724148903574
RMSE train: 0.454633	val: 0.793483	test: 0.765027
MAE train: 0.350973	val: 0.597142	test: 0.592120

Epoch: 128
Loss: 0.25285026111773085
RMSE train: 0.475677	val: 0.774482	test: 0.767114
MAE train: 0.367277	val: 0.599340	test: 0.598477

Epoch: 129
Loss: 0.24599131303174154
RMSE train: 0.469791	val: 0.768383	test: 0.756620
MAE train: 0.370264	val: 0.593025	test: 0.596293

Epoch: 130
Loss: 0.2464184303368841
RMSE train: 0.444991	val: 0.752626	test: 0.750487
MAE train: 0.344328	val: 0.573181	test: 0.586728

Epoch: 131
Loss: 0.26005784422159195
RMSE train: 0.474008	val: 0.782702	test: 0.764702
MAE train: 0.369593	val: 0.594973	test: 0.594851

Epoch: 132
Loss: 0.2565154071365084
RMSE train: 0.468944	val: 0.775807	test: 0.763523
MAE train: 0.365854	val: 0.599918	test: 0.593778

Epoch: 133
Loss: 0.2627877763339451
RMSE train: 0.465687	val: 0.761570	test: 0.758726
MAE train: 0.361110	val: 0.580475	test: 0.591649

Epoch: 134
Loss: 0.27521214527743204
RMSE train: 0.449183	val: 0.763278	test: 0.761773
MAE train: 0.348962	val: 0.584247	test: 0.592047

Epoch: 135
Loss: 0.2615510406238692
RMSE train: 0.458188	val: 0.762398	test: 0.759881
MAE train: 0.359198	val: 0.590192	test: 0.592522

Epoch: 136
Loss: 0.24581721957240785
RMSE train: 0.442944	val: 0.771558	test: 0.759400
MAE train: 0.343939	val: 0.589876	test: 0.582328

Epoch: 137
Loss: 0.2608161526066916
RMSE train: 0.432639	val: 0.762620	test: 0.762947
MAE train: 0.333190	val: 0.579504	test: 0.582807

Epoch: 138
Loss: 0.2601563824074609
RMSE train: 0.443536	val: 0.774125	test: 0.751503
MAE train: 0.345390	val: 0.585097	test: 0.582552

Epoch: 139
Loss: 0.24642301563705718
RMSE train: 0.449146	val: 0.770238	test: 0.750622
MAE train: 0.348981	val: 0.585437	test: 0.584326

Epoch: 140
Loss: 0.2410576194524765
RMSE train: 0.431280	val: 0.762806	test: 0.750825
MAE train: 0.332327	val: 0.574847	test: 0.580337

Epoch: 141
Loss: 0.25765123537608553
RMSE train: 0.448773	val: 0.769725	test: 0.743691
MAE train: 0.351011	val: 0.583960	test: 0.578508

Epoch: 142
Loss: 0.24256738488163268
RMSE train: 0.449256	val: 0.770799	test: 0.762847
MAE train: 0.347133	val: 0.586370	test: 0.595401

Epoch: 143
Loss: 0.2511888103825705
RMSE train: 0.479834	val: 0.793025	test: 0.775087
MAE train: 0.370075	val: 0.600236	test: 0.600553

Epoch: 143
Loss: 0.10958102771214076
RMSE train: 0.326375	val: 1.069383	test: 1.030734
MAE train: 0.270612	val: 0.849625	test: 0.832173

Epoch: 144
Loss: 0.09836208234940257
RMSE train: 0.338512	val: 1.022964	test: 1.004202
MAE train: 0.282236	val: 0.822379	test: 0.811987

Epoch: 145
Loss: 0.09974752313324384
RMSE train: 0.386683	val: 1.046976	test: 1.004987
MAE train: 0.329414	val: 0.840100	test: 0.812502

Epoch: 146
Loss: 0.10065441685063499
RMSE train: 0.300345	val: 1.103111	test: 1.061849
MAE train: 0.246149	val: 0.874310	test: 0.853631

Epoch: 147
Loss: 0.10213968636734146
RMSE train: 0.285464	val: 1.079940	test: 1.021641
MAE train: 0.233869	val: 0.853169	test: 0.820714

Epoch: 148
Loss: 0.11081840097904205
RMSE train: 0.308794	val: 1.058389	test: 1.025853
MAE train: 0.254595	val: 0.853972	test: 0.826593

Epoch: 149
Loss: 0.11922187730669975
RMSE train: 0.395047	val: 1.088776	test: 1.045040
MAE train: 0.333968	val: 0.870456	test: 0.847765

Epoch: 150
Loss: 0.11236932447978429
RMSE train: 0.323982	val: 1.101274	test: 1.068752
MAE train: 0.266822	val: 0.892025	test: 0.857924

Epoch: 151
Loss: 0.1056485463465963
RMSE train: 0.408451	val: 1.048920	test: 1.012419
MAE train: 0.348178	val: 0.843937	test: 0.822659

Epoch: 152
Loss: 0.09994878247380257
RMSE train: 0.346299	val: 1.053821	test: 1.013007
MAE train: 0.289748	val: 0.840824	test: 0.814887

Epoch: 153
Loss: 0.09868574195674487
RMSE train: 0.340349	val: 1.088111	test: 1.051541
MAE train: 0.283030	val: 0.869401	test: 0.857859

Epoch: 154
Loss: 0.09782630577683449
RMSE train: 0.346514	val: 1.065212	test: 1.040364
MAE train: 0.289532	val: 0.862460	test: 0.848942

Epoch: 155
Loss: 0.08932240945952279
RMSE train: 0.278441	val: 1.068943	test: 1.027588
MAE train: 0.225843	val: 0.844939	test: 0.827606

Epoch: 156
Loss: 0.08882812091282435
RMSE train: 0.314815	val: 1.055696	test: 1.007320
MAE train: 0.257723	val: 0.846160	test: 0.806522

Epoch: 157
Loss: 0.09018514731100627
RMSE train: 0.476262	val: 1.123473	test: 1.114028
MAE train: 0.416789	val: 0.930601	test: 0.896394

Epoch: 158
Loss: 0.09251035590256963
RMSE train: 0.358067	val: 1.098241	test: 1.082796
MAE train: 0.304049	val: 0.893365	test: 0.876190

Epoch: 159
Loss: 0.09577290713787079
RMSE train: 0.394323	val: 1.059526	test: 1.045725
MAE train: 0.336281	val: 0.859241	test: 0.846551

Epoch: 160
Loss: 0.09951622571263995
RMSE train: 0.299408	val: 1.070096	test: 1.009473
MAE train: 0.247949	val: 0.848111	test: 0.812518

Epoch: 161
Loss: 0.09852774441242218
RMSE train: 0.312377	val: 1.092250	test: 1.044913
MAE train: 0.257348	val: 0.859485	test: 0.845972

Epoch: 162
Loss: 0.0873363810990538
RMSE train: 0.307622	val: 1.125612	test: 1.050071
MAE train: 0.247528	val: 0.884326	test: 0.838477

Epoch: 163
Loss: 0.09796023688146047
RMSE train: 0.373712	val: 1.061463	test: 1.038790
MAE train: 0.318757	val: 0.856942	test: 0.838459

Epoch: 164
Loss: 0.08821399030940873
RMSE train: 0.355538	val: 1.084390	test: 1.044923
MAE train: 0.300211	val: 0.868932	test: 0.847215

Epoch: 165
Loss: 0.07919798365661077
RMSE train: 0.366124	val: 1.045800	test: 1.014329
MAE train: 0.310570	val: 0.843962	test: 0.820358

Epoch: 166
Loss: 0.09230568685701915
RMSE train: 0.323184	val: 1.061390	test: 1.014974
MAE train: 0.266173	val: 0.844045	test: 0.812553

Epoch: 167
Loss: 0.09191661381295749
RMSE train: 0.249512	val: 1.086082	test: 1.029511
MAE train: 0.202144	val: 0.856275	test: 0.828164

Epoch: 168
Loss: 0.07769307307898998
RMSE train: 0.311434	val: 1.070053	test: 1.037583
MAE train: 0.259386	val: 0.863559	test: 0.842188

Epoch: 169
Loss: 0.08628349910889353
RMSE train: 0.396929	val: 1.062530	test: 1.038904
MAE train: 0.341501	val: 0.859282	test: 0.841061

Epoch: 170
Loss: 0.08263770703758512
RMSE train: 0.317335	val: 1.056356	test: 1.000325
MAE train: 0.253955	val: 0.840946	test: 0.803569

Epoch: 171
Loss: 0.08495049976876803
RMSE train: 0.333948	val: 1.056767	test: 1.018848
MAE train: 0.278701	val: 0.838814	test: 0.830080

Epoch: 172
Loss: 0.08219116074698311
RMSE train: 0.294602	val: 1.083089	test: 1.042169
MAE train: 0.244861	val: 0.858147	test: 0.841771

Epoch: 173
Loss: 0.08210454136133194
RMSE train: 0.280735	val: 1.072256	test: 1.019979
MAE train: 0.230313	val: 0.846715	test: 0.822055

Epoch: 174
Loss: 0.08215583807655744
RMSE train: 0.318427	val: 1.081391	test: 1.027856
MAE train: 0.265433	val: 0.853107	test: 0.832899

Epoch: 175
Loss: 0.08493294247559138
RMSE train: 0.379278	val: 1.076173	test: 1.039518
MAE train: 0.326125	val: 0.856099	test: 0.841677

Epoch: 176
Loss: 0.07712533165301595
RMSE train: 0.317995	val: 1.056876	test: 1.012821
MAE train: 0.263749	val: 0.839312	test: 0.815362

Epoch: 177
Loss: 0.08250464020030839
RMSE train: 0.454390	val: 1.046469	test: 1.016992
MAE train: 0.399480	val: 0.844578	test: 0.822741

Epoch: 178
Loss: 0.0744763586137976
RMSE train: 0.318958	val: 1.068065	test: 1.025425
MAE train: 0.265157	val: 0.857611	test: 0.830979

Epoch: 179
Loss: 0.07939084566065244
RMSE train: 0.298809	val: 1.079995	test: 1.022068
MAE train: 0.244907	val: 0.857594	test: 0.828603

Early stopping
Best (RMSE):	 train: 0.338512	val: 1.022964	test: 1.004202
Best (MAE):	 train: 0.282236	val: 0.822379	test: 0.811987
All runs completed.


Epoch: 144
Loss: 0.24611730022089823
RMSE train: 0.474780	val: 0.796830	test: 0.778621
MAE train: 0.372347	val: 0.606977	test: 0.609563

Epoch: 145
Loss: 0.24749143421649933
RMSE train: 0.446363	val: 0.783226	test: 0.766673
MAE train: 0.346585	val: 0.593268	test: 0.591912

Epoch: 146
Loss: 0.24443509642566955
RMSE train: 0.445815	val: 0.768343	test: 0.746241
MAE train: 0.349299	val: 0.581449	test: 0.580185

Epoch: 147
Loss: 0.2549649423786572
RMSE train: 0.458993	val: 0.789392	test: 0.747559
MAE train: 0.356891	val: 0.596746	test: 0.583677

Epoch: 148
Loss: 0.23569474795034953
RMSE train: 0.467339	val: 0.782374	test: 0.767599
MAE train: 0.365506	val: 0.594385	test: 0.596040

Epoch: 149
Loss: 0.25314843228885103
RMSE train: 0.446697	val: 0.765063	test: 0.749291
MAE train: 0.347695	val: 0.583336	test: 0.580469

Epoch: 150
Loss: 0.24656009461198533
RMSE train: 0.463250	val: 0.803713	test: 0.760305
MAE train: 0.363966	val: 0.613224	test: 0.590766

Epoch: 151
Loss: 0.2419760184628623
RMSE train: 0.448204	val: 0.769676	test: 0.765271
MAE train: 0.350058	val: 0.588456	test: 0.592217

Epoch: 152
Loss: 0.2507157304457256
RMSE train: 0.459197	val: 0.785492	test: 0.761215
MAE train: 0.357640	val: 0.601162	test: 0.593891

Epoch: 153
Loss: 0.23993895096438272
RMSE train: 0.432807	val: 0.763924	test: 0.747755
MAE train: 0.338104	val: 0.581895	test: 0.579430

Epoch: 154
Loss: 0.23976743114846094
RMSE train: 0.470521	val: 0.789748	test: 0.752447
MAE train: 0.367090	val: 0.601232	test: 0.594346

Epoch: 155
Loss: 0.22952135332993098
RMSE train: 0.471566	val: 0.798246	test: 0.757749
MAE train: 0.368139	val: 0.612887	test: 0.592245

Epoch: 156
Loss: 0.2368045917579106
RMSE train: 0.448923	val: 0.780852	test: 0.758677
MAE train: 0.348049	val: 0.594628	test: 0.581925

Epoch: 157
Loss: 0.24534172671181814
RMSE train: 0.441363	val: 0.769437	test: 0.739796
MAE train: 0.345499	val: 0.584657	test: 0.574060

Epoch: 158
Loss: 0.25107438223702566
RMSE train: 0.451309	val: 0.782197	test: 0.758936
MAE train: 0.350099	val: 0.600066	test: 0.593765

Epoch: 159
Loss: 0.22880111741168158
RMSE train: 0.451490	val: 0.785296	test: 0.769047
MAE train: 0.351916	val: 0.600782	test: 0.599905

Epoch: 160
Loss: 0.23654942640236445
RMSE train: 0.441672	val: 0.774433	test: 0.746155
MAE train: 0.343791	val: 0.595017	test: 0.582921

Epoch: 161
Loss: 0.24055317576442445
RMSE train: 0.438352	val: 0.766480	test: 0.746074
MAE train: 0.341367	val: 0.588352	test: 0.575815

Epoch: 162
Loss: 0.2243248320051602
RMSE train: 0.462672	val: 0.799559	test: 0.761579
MAE train: 0.362285	val: 0.606156	test: 0.592368

Epoch: 163
Loss: 0.2587486526795796
RMSE train: 0.424255	val: 0.759802	test: 0.748037
MAE train: 0.331213	val: 0.586058	test: 0.570343

Epoch: 164
Loss: 0.25402653004441944
RMSE train: 0.460740	val: 0.776919	test: 0.749716
MAE train: 0.361217	val: 0.599270	test: 0.579619

Epoch: 165
Loss: 0.22574138641357422
RMSE train: 0.431690	val: 0.761591	test: 0.755502
MAE train: 0.335710	val: 0.585566	test: 0.578930

Epoch: 166
Loss: 0.24182236620358058
RMSE train: 0.435679	val: 0.770947	test: 0.750055
MAE train: 0.339677	val: 0.590624	test: 0.578082

Epoch: 167
Loss: 0.21896628503288543
RMSE train: 0.431979	val: 0.771007	test: 0.749664
MAE train: 0.336113	val: 0.592044	test: 0.579450

Epoch: 168
Loss: 0.22077091570411408
RMSE train: 0.433254	val: 0.777812	test: 0.749981
MAE train: 0.337718	val: 0.596082	test: 0.580072

Epoch: 169
Loss: 0.2330974925841604
RMSE train: 0.438331	val: 0.780233	test: 0.741887
MAE train: 0.341492	val: 0.596929	test: 0.571873

Epoch: 170
Loss: 0.22626570292881557
RMSE train: 0.418649	val: 0.769502	test: 0.737431
MAE train: 0.324356	val: 0.590842	test: 0.564584

Epoch: 171
Loss: 0.2258814802127225
RMSE train: 0.415780	val: 0.763322	test: 0.749563
MAE train: 0.322527	val: 0.583526	test: 0.579204

Epoch: 172
Loss: 0.22799245374543325
RMSE train: 0.430079	val: 0.774781	test: 0.746284
MAE train: 0.333453	val: 0.585676	test: 0.582929

Epoch: 173
Loss: 0.23464503777878626
RMSE train: 0.455765	val: 0.768117	test: 0.764117
MAE train: 0.354101	val: 0.586780	test: 0.591974

Epoch: 174
Loss: 0.23375138001782553
RMSE train: 0.413933	val: 0.759246	test: 0.751221
MAE train: 0.320974	val: 0.575381	test: 0.582595

Epoch: 175
Loss: 0.21221058389970235
RMSE train: 0.417909	val: 0.758794	test: 0.740437
MAE train: 0.324779	val: 0.581342	test: 0.568645

Epoch: 176
Loss: 0.22762053247009004
RMSE train: 0.409141	val: 0.762479	test: 0.748020
MAE train: 0.317599	val: 0.577093	test: 0.577594

Epoch: 177
Loss: 0.216565517442567
RMSE train: 0.422416	val: 0.777973	test: 0.750500
MAE train: 0.327195	val: 0.590100	test: 0.580486

Epoch: 178
Loss: 0.2258454986980983
RMSE train: 0.409336	val: 0.734923	test: 0.748150
MAE train: 0.318399	val: 0.562225	test: 0.571323

Epoch: 179
Loss: 0.22016815202576773
RMSE train: 0.421433	val: 0.751708	test: 0.744772
MAE train: 0.329057	val: 0.575624	test: 0.573902

Epoch: 180
Loss: 0.22142026041235244
RMSE train: 0.422572	val: 0.781943	test: 0.746580
MAE train: 0.330293	val: 0.590297	test: 0.578827

Epoch: 181
Loss: 0.21437517340694154
RMSE train: 0.419992	val: 0.774439	test: 0.743312
MAE train: 0.327082	val: 0.580865	test: 0.573737

Epoch: 182
Loss: 0.2169731312564441
RMSE train: 0.416180	val: 0.748221	test: 0.751305
MAE train: 0.324214	val: 0.574152	test: 0.574410

Epoch: 183
Loss: 0.20400325528212956
RMSE train: 0.413374	val: 0.750724	test: 0.764581
MAE train: 0.320200	val: 0.573932	test: 0.584677

Epoch: 184
Loss: 0.2078085094690323
RMSE train: 0.412497	val: 0.779883	test: 0.751008
MAE train: 0.319767	val: 0.588155	test: 0.581916

Epoch: 185
Loss: 0.21507699574742997
RMSE train: 0.409775	val: 0.772268	test: 0.757682
MAE train: 0.317817	val: 0.583204	test: 0.585880

Epoch: 186
Loss: 0.21375091906104768
RMSE train: 0.418971	val: 0.764073	test: 0.753384
MAE train: 0.325333	val: 0.578135	test: 0.581449

Epoch: 187
Loss: 0.2200278490781784
RMSE train: 0.399491	val: 0.757699	test: 0.735702
MAE train: 0.309633	val: 0.571992	test: 0.569915

Epoch: 188
Loss: 0.21165219162191665
RMSE train: 0.421711	val: 0.793833	test: 0.744163
MAE train: 0.328426	val: 0.602860	test: 0.580722

Epoch: 189
Loss: 0.2096442346061979
RMSE train: 0.416629	val: 0.759022	test: 0.742173
MAE train: 0.324940	val: 0.583009	test: 0.572449

Epoch: 190
Loss: 0.21290613498006547
RMSE train: 0.421433	val: 0.776837	test: 0.755741
MAE train: 0.326524	val: 0.585192	test: 0.581397

Epoch: 191
Loss: 0.20654926661934173
RMSE train: 0.400644	val: 0.757123	test: 0.741774
MAE train: 0.309043	val: 0.572789	test: 0.569025

Epoch: 192
Loss: 0.2140700018831662
RMSE train: 0.417336	val: 0.763577	test: 0.742689
MAE train: 0.326478	val: 0.583564	test: 0.575435

Epoch: 193
Loss: 0.21388262936047145
RMSE train: 0.414038	val: 0.761857	test: 0.748288
MAE train: 0.320975	val: 0.578473	test: 0.574288

Epoch: 194
Loss: 0.21142558114869253
RMSE train: 0.415636	val: 0.775910	test: 0.740116
MAE train: 0.323544	val: 0.588193	test: 0.575642

Epoch: 195
Loss: 0.21443469609533036
RMSE train: 0.410121	val: 0.754882	test: 0.751580
MAE train: 0.318518	val: 0.573214	test: 0.581545

Epoch: 196
Loss: 0.1994198283978871
RMSE train: 0.394205	val: 0.748137	test: 0.749810
MAE train: 0.305383	val: 0.568225	test: 0.575450

Epoch: 197
Loss: 0.2100166603922844
RMSE train: 0.408429	val: 0.760802	test: 0.748848
MAE train: 0.318349	val: 0.580998	test: 0.572210

Epoch: 198
Loss: 0.2118220520871026
RMSE train: 0.408103	val: 0.759501	test: 0.743415
MAE train: 0.314909	val: 0.577846	test: 0.569854

Epoch: 199
Loss: 0.2269683171595846
RMSE train: 0.412078	val: 0.781664	test: 0.739127
MAE train: 0.320346	val: 0.589021	test: 0.571525

Epoch: 200
Loss: 0.20838490660701478
RMSE train: 0.395954	val: 0.746750	test: 0.739236
MAE train: 0.307927	val: 0.570208	test: 0.569714

Epoch: 201
Loss: 0.2033709276999746
RMSE train: 0.430075	val: 0.783959	test: 0.763716
MAE train: 0.337625	val: 0.601243	test: 0.589122

Epoch: 202
Loss: 0.21164653130940028
RMSE train: 0.407929	val: 0.771536	test: 0.740753
MAE train: 0.315891	val: 0.586093	test: 0.574851

Epoch: 203
Loss: 0.21123206509011133
RMSE train: 0.385080	val: 0.763733	test: 0.733769
MAE train: 0.298449	val: 0.579983	test: 0.564883

Epoch: 144
Loss: 0.23645827174186707
RMSE train: 0.455146	val: 0.789256	test: 0.758157
MAE train: 0.352453	val: 0.602733	test: 0.580240

Epoch: 145
Loss: 0.24331753700971603
RMSE train: 0.427874	val: 0.758312	test: 0.746726
MAE train: 0.330670	val: 0.577329	test: 0.574334

Epoch: 146
Loss: 0.2354384224329676
RMSE train: 0.432283	val: 0.747525	test: 0.737231
MAE train: 0.336310	val: 0.568017	test: 0.577815

Epoch: 147
Loss: 0.2518602928944996
RMSE train: 0.446111	val: 0.762447	test: 0.755034
MAE train: 0.346988	val: 0.586642	test: 0.593017

Epoch: 148
Loss: 0.22711477960859025
RMSE train: 0.441709	val: 0.771002	test: 0.759817
MAE train: 0.342718	val: 0.585947	test: 0.589477

Epoch: 149
Loss: 0.23058673207248961
RMSE train: 0.447871	val: 0.768773	test: 0.752351
MAE train: 0.349107	val: 0.593842	test: 0.585852

Epoch: 150
Loss: 0.2259353527000972
RMSE train: 0.423234	val: 0.755377	test: 0.762211
MAE train: 0.327948	val: 0.575272	test: 0.591494

Epoch: 151
Loss: 0.23357340480600083
RMSE train: 0.438966	val: 0.759927	test: 0.747757
MAE train: 0.340564	val: 0.583664	test: 0.582466

Epoch: 152
Loss: 0.22635216691664287
RMSE train: 0.431431	val: 0.758290	test: 0.760504
MAE train: 0.336498	val: 0.577845	test: 0.588557

Epoch: 153
Loss: 0.23381918562310083
RMSE train: 0.482265	val: 0.812981	test: 0.792282
MAE train: 0.377650	val: 0.618122	test: 0.616028

Epoch: 154
Loss: 0.24311320270810807
RMSE train: 0.433259	val: 0.783549	test: 0.751505
MAE train: 0.335384	val: 0.593904	test: 0.584306

Epoch: 155
Loss: 0.22289116467748368
RMSE train: 0.419667	val: 0.764601	test: 0.761794
MAE train: 0.323190	val: 0.583972	test: 0.589424

Epoch: 156
Loss: 0.22573102372033255
RMSE train: 0.438095	val: 0.775315	test: 0.750254
MAE train: 0.341971	val: 0.589721	test: 0.584276

Epoch: 157
Loss: 0.23057797870465688
RMSE train: 0.425166	val: 0.768534	test: 0.755812
MAE train: 0.328426	val: 0.586915	test: 0.585394

Epoch: 158
Loss: 0.21090170741081238
RMSE train: 0.419851	val: 0.760922	test: 0.761987
MAE train: 0.325558	val: 0.583192	test: 0.594009

Epoch: 159
Loss: 0.2325499483517238
RMSE train: 0.439463	val: 0.758706	test: 0.758985
MAE train: 0.343171	val: 0.578304	test: 0.594485

Epoch: 160
Loss: 0.23085673046963556
RMSE train: 0.416716	val: 0.762331	test: 0.756140
MAE train: 0.322154	val: 0.585822	test: 0.582898

Epoch: 161
Loss: 0.22945813515356608
RMSE train: 0.435395	val: 0.777420	test: 0.746767
MAE train: 0.337433	val: 0.594743	test: 0.578561

Epoch: 162
Loss: 0.2297599400792803
RMSE train: 0.410077	val: 0.757237	test: 0.757771
MAE train: 0.314961	val: 0.583181	test: 0.580759

Epoch: 163
Loss: 0.22109469239200866
RMSE train: 0.431442	val: 0.767935	test: 0.747574
MAE train: 0.337609	val: 0.590567	test: 0.584824

Epoch: 164
Loss: 0.215747808771474
RMSE train: 0.406261	val: 0.759950	test: 0.760248
MAE train: 0.312522	val: 0.579809	test: 0.586685

Epoch: 165
Loss: 0.2556104521666254
RMSE train: 0.415060	val: 0.750927	test: 0.752940
MAE train: 0.321798	val: 0.578262	test: 0.582145

Epoch: 166
Loss: 0.22077459309782302
RMSE train: 0.424059	val: 0.766314	test: 0.752759
MAE train: 0.328002	val: 0.588605	test: 0.580669

Epoch: 167
Loss: 0.2070551301751818
RMSE train: 0.415275	val: 0.770224	test: 0.758383
MAE train: 0.322138	val: 0.585672	test: 0.584633

Epoch: 168
Loss: 0.22466063499450684
RMSE train: 0.431157	val: 0.774372	test: 0.749457
MAE train: 0.332635	val: 0.591257	test: 0.582590

Epoch: 169
Loss: 0.22741480703864778
RMSE train: 0.412474	val: 0.745778	test: 0.756160
MAE train: 0.320719	val: 0.573770	test: 0.587003

Epoch: 170
Loss: 0.21108304815632956
RMSE train: 0.446683	val: 0.783023	test: 0.758877
MAE train: 0.346158	val: 0.599507	test: 0.589025

Epoch: 171
Loss: 0.22202234821660177
RMSE train: 0.420032	val: 0.755937	test: 0.753346
MAE train: 0.325492	val: 0.572759	test: 0.581235

Epoch: 172
Loss: 0.22526916010039194
RMSE train: 0.428773	val: 0.771078	test: 0.759461
MAE train: 0.332615	val: 0.591971	test: 0.584122

Epoch: 173
Loss: 0.2205313178045409
RMSE train: 0.411692	val: 0.761446	test: 0.757885
MAE train: 0.318466	val: 0.576327	test: 0.587427

Epoch: 174
Loss: 0.22651920041867665
RMSE train: 0.427058	val: 0.754092	test: 0.747678
MAE train: 0.330148	val: 0.583983	test: 0.585353

Epoch: 175
Loss: 0.21382084382431849
RMSE train: 0.416152	val: 0.761241	test: 0.751364
MAE train: 0.321789	val: 0.578816	test: 0.578876

Epoch: 176
Loss: 0.20627682602831296
RMSE train: 0.415730	val: 0.746550	test: 0.754558
MAE train: 0.323480	val: 0.571322	test: 0.581436

Epoch: 177
Loss: 0.21734432982546942
RMSE train: 0.413785	val: 0.745802	test: 0.763070
MAE train: 0.321340	val: 0.573970	test: 0.594329

Epoch: 178
Loss: 0.21591679006814957
RMSE train: 0.419516	val: 0.747817	test: 0.745906
MAE train: 0.327369	val: 0.573700	test: 0.583171

Epoch: 179
Loss: 0.2101814874580928
RMSE train: 0.405678	val: 0.744454	test: 0.754136
MAE train: 0.314748	val: 0.579442	test: 0.581874

Epoch: 180
Loss: 0.21378860835518157
RMSE train: 0.423611	val: 0.776605	test: 0.767274
MAE train: 0.329205	val: 0.592771	test: 0.599922

Epoch: 181
Loss: 0.21457172291619436
RMSE train: 0.400229	val: 0.736992	test: 0.745444
MAE train: 0.307885	val: 0.564903	test: 0.572895

Epoch: 182
Loss: 0.20583863130637578
RMSE train: 0.428313	val: 0.738750	test: 0.764923
MAE train: 0.334697	val: 0.571127	test: 0.592866

Epoch: 183
Loss: 0.2068986679826464
RMSE train: 0.406306	val: 0.763925	test: 0.744969
MAE train: 0.313141	val: 0.578636	test: 0.571604

Epoch: 184
Loss: 0.21055122997079576
RMSE train: 0.414421	val: 0.772139	test: 0.748041
MAE train: 0.320629	val: 0.590274	test: 0.574511

Epoch: 185
Loss: 0.2042195594736508
RMSE train: 0.400189	val: 0.745363	test: 0.751558
MAE train: 0.308366	val: 0.572051	test: 0.575423

Epoch: 186
Loss: 0.22101306383098876
RMSE train: 0.404666	val: 0.744088	test: 0.747292
MAE train: 0.313728	val: 0.577525	test: 0.575576

Epoch: 187
Loss: 0.2227333230631692
RMSE train: 0.414707	val: 0.774247	test: 0.761389
MAE train: 0.318536	val: 0.594752	test: 0.577500

Epoch: 188
Loss: 0.2148417979478836
RMSE train: 0.394295	val: 0.755976	test: 0.771943
MAE train: 0.301217	val: 0.576205	test: 0.586776

Epoch: 189
Loss: 0.2293455143060003
RMSE train: 0.431484	val: 0.752964	test: 0.756143
MAE train: 0.338767	val: 0.581884	test: 0.589125

Epoch: 190
Loss: 0.22281995522124426
RMSE train: 0.405750	val: 0.740558	test: 0.759031
MAE train: 0.313033	val: 0.567116	test: 0.589326

Epoch: 191
Loss: 0.21300814087901795
RMSE train: 0.417277	val: 0.763995	test: 0.746433
MAE train: 0.322664	val: 0.581042	test: 0.575570

Epoch: 192
Loss: 0.2317577227950096
RMSE train: 0.410556	val: 0.751499	test: 0.746440
MAE train: 0.317820	val: 0.575662	test: 0.580631

Epoch: 193
Loss: 0.21151839090245111
RMSE train: 0.396725	val: 0.738202	test: 0.749857
MAE train: 0.306773	val: 0.565039	test: 0.584215

Epoch: 194
Loss: 0.20849528908729553
RMSE train: 0.412987	val: 0.744411	test: 0.742995
MAE train: 0.320495	val: 0.570015	test: 0.575719

Epoch: 195
Loss: 0.21018556186131068
RMSE train: 0.402320	val: 0.753793	test: 0.754378
MAE train: 0.310028	val: 0.575646	test: 0.583081

Epoch: 196
Loss: 0.2087687232664653
RMSE train: 0.399887	val: 0.734979	test: 0.753815
MAE train: 0.308560	val: 0.563109	test: 0.580600

Epoch: 197
Loss: 0.20411800380264009
RMSE train: 0.401978	val: 0.736929	test: 0.760063
MAE train: 0.309382	val: 0.567855	test: 0.589228

Epoch: 198
Loss: 0.19412872408117568
RMSE train: 0.413032	val: 0.757007	test: 0.775369
MAE train: 0.319603	val: 0.585617	test: 0.598012

Epoch: 199
Loss: 0.19938417098351888
RMSE train: 0.402861	val: 0.745267	test: 0.762894
MAE train: 0.310829	val: 0.570912	test: 0.587951

Epoch: 200
Loss: 0.20666950728212083
RMSE train: 0.394914	val: 0.738370	test: 0.753524
MAE train: 0.303592	val: 0.573133	test: 0.582369

Epoch: 201
Loss: 0.20207551760332926
RMSE train: 0.408904	val: 0.747105	test: 0.750696
MAE train: 0.314647	val: 0.574209	test: 0.581382

Epoch: 202
Loss: 0.21027515189988272
RMSE train: 0.384734	val: 0.751743	test: 0.746925
MAE train: 0.293643	val: 0.575318	test: 0.574664

Epoch: 203
Loss: 0.20140299839632853
RMSE train: 0.404447	val: 0.762826	test: 0.743301
MAE train: 0.311724	val: 0.580103	test: 0.577289

Epoch: 144
Loss: 0.24774818335260665
RMSE train: 0.461946	val: 0.772775	test: 0.739076
MAE train: 0.355108	val: 0.588223	test: 0.571857

Epoch: 145
Loss: 0.22189104769911086
RMSE train: 0.436155	val: 0.771065	test: 0.731905
MAE train: 0.334414	val: 0.584209	test: 0.561540

Epoch: 146
Loss: 0.22957101038524083
RMSE train: 0.448518	val: 0.762517	test: 0.740504
MAE train: 0.347851	val: 0.581935	test: 0.573599

Epoch: 147
Loss: 0.22708542006356375
RMSE train: 0.448630	val: 0.755897	test: 0.733077
MAE train: 0.348416	val: 0.577455	test: 0.570130

Epoch: 148
Loss: 0.2243502140045166
RMSE train: 0.463775	val: 0.796084	test: 0.758320
MAE train: 0.358202	val: 0.604463	test: 0.587971

Epoch: 149
Loss: 0.2385116538831166
RMSE train: 0.446958	val: 0.760753	test: 0.722337
MAE train: 0.343570	val: 0.580666	test: 0.556039

Epoch: 150
Loss: 0.24443221624408448
RMSE train: 0.433003	val: 0.763754	test: 0.731176
MAE train: 0.329809	val: 0.578168	test: 0.564572

Epoch: 151
Loss: 0.24169534870556422
RMSE train: 0.451970	val: 0.780765	test: 0.736204
MAE train: 0.344453	val: 0.590755	test: 0.562786

Epoch: 152
Loss: 0.24775087194783346
RMSE train: 0.453556	val: 0.778106	test: 0.731822
MAE train: 0.348149	val: 0.583958	test: 0.563929

Epoch: 153
Loss: 0.239528694323131
RMSE train: 0.456298	val: 0.767436	test: 0.729371
MAE train: 0.354330	val: 0.585840	test: 0.561734

Epoch: 154
Loss: 0.23622134647199086
RMSE train: 0.443982	val: 0.763591	test: 0.733071
MAE train: 0.340524	val: 0.582829	test: 0.567576

Epoch: 155
Loss: 0.2366432398557663
RMSE train: 0.447675	val: 0.768159	test: 0.731365
MAE train: 0.346514	val: 0.583586	test: 0.564658

Epoch: 156
Loss: 0.24002725737435476
RMSE train: 0.431668	val: 0.756901	test: 0.734738
MAE train: 0.331336	val: 0.577585	test: 0.566333

Epoch: 157
Loss: 0.23645849100181035
RMSE train: 0.461589	val: 0.762935	test: 0.750233
MAE train: 0.358203	val: 0.576869	test: 0.574357

Epoch: 158
Loss: 0.25397870051009314
RMSE train: 0.436398	val: 0.768025	test: 0.738789
MAE train: 0.335418	val: 0.578376	test: 0.563763

Epoch: 159
Loss: 0.23112995709691728
RMSE train: 0.435692	val: 0.751261	test: 0.730775
MAE train: 0.336946	val: 0.566371	test: 0.563495

Epoch: 160
Loss: 0.23585062899759837
RMSE train: 0.448041	val: 0.782252	test: 0.750989
MAE train: 0.342989	val: 0.586714	test: 0.576295

Epoch: 161
Loss: 0.23553870618343353
RMSE train: 0.440091	val: 0.764540	test: 0.747913
MAE train: 0.337638	val: 0.579985	test: 0.575628

Epoch: 162
Loss: 0.2300956259880747
RMSE train: 0.461919	val: 0.792507	test: 0.755405
MAE train: 0.355328	val: 0.598468	test: 0.583066

Epoch: 163
Loss: 0.23531310898917063
RMSE train: 0.453810	val: 0.779019	test: 0.744475
MAE train: 0.348503	val: 0.583898	test: 0.574981

Epoch: 164
Loss: 0.24140371488673346
RMSE train: 0.441277	val: 0.772413	test: 0.737131
MAE train: 0.339076	val: 0.587674	test: 0.569879

Epoch: 165
Loss: 0.2361438891717366
RMSE train: 0.453261	val: 0.791429	test: 0.761649
MAE train: 0.350416	val: 0.597242	test: 0.584677

Epoch: 166
Loss: 0.22744156313794
RMSE train: 0.449773	val: 0.759869	test: 0.734780
MAE train: 0.346178	val: 0.576992	test: 0.568917

Epoch: 167
Loss: 0.22520502018077032
RMSE train: 0.425728	val: 0.753265	test: 0.729762
MAE train: 0.327569	val: 0.572081	test: 0.564087

Epoch: 168
Loss: 0.217459075152874
RMSE train: 0.450876	val: 0.787128	test: 0.750400
MAE train: 0.348407	val: 0.595095	test: 0.580228

Epoch: 169
Loss: 0.2299196571111679
RMSE train: 0.431587	val: 0.743581	test: 0.731135
MAE train: 0.331275	val: 0.565095	test: 0.571382

Epoch: 170
Loss: 0.22500525414943695
RMSE train: 0.434311	val: 0.760859	test: 0.745027
MAE train: 0.333334	val: 0.580722	test: 0.577206

Epoch: 171
Loss: 0.2206491870539529
RMSE train: 0.440874	val: 0.779664	test: 0.755793
MAE train: 0.341509	val: 0.595775	test: 0.588304

Epoch: 172
Loss: 0.224133839564664
RMSE train: 0.435151	val: 0.773123	test: 0.737160
MAE train: 0.335383	val: 0.587020	test: 0.566943

Epoch: 173
Loss: 0.23783829595361436
RMSE train: 0.468720	val: 0.806352	test: 0.762328
MAE train: 0.359993	val: 0.611429	test: 0.592776

Epoch: 174
Loss: 0.23416919154780252
RMSE train: 0.427488	val: 0.765248	test: 0.738845
MAE train: 0.330219	val: 0.580448	test: 0.567028

Epoch: 175
Loss: 0.21839957790715353
RMSE train: 0.439217	val: 0.761270	test: 0.726980
MAE train: 0.338410	val: 0.578116	test: 0.558986

Epoch: 176
Loss: 0.23934113766465867
RMSE train: 0.436073	val: 0.779366	test: 0.734140
MAE train: 0.338509	val: 0.589574	test: 0.562951

Epoch: 177
Loss: 0.2215855728302683
RMSE train: 0.439351	val: 0.774126	test: 0.735857
MAE train: 0.342150	val: 0.584421	test: 0.567685

Epoch: 178
Loss: 0.214778870344162
RMSE train: 0.445380	val: 0.791670	test: 0.743577
MAE train: 0.345045	val: 0.594057	test: 0.572117

Epoch: 179
Loss: 0.2121461821453912
RMSE train: 0.427989	val: 0.779002	test: 0.747958
MAE train: 0.330456	val: 0.590789	test: 0.579271

Epoch: 180
Loss: 0.21135270489113672
RMSE train: 0.430561	val: 0.774062	test: 0.729256
MAE train: 0.332500	val: 0.581765	test: 0.562364

Epoch: 181
Loss: 0.21597713977098465
RMSE train: 0.412182	val: 0.770052	test: 0.729155
MAE train: 0.316901	val: 0.586019	test: 0.562606

Epoch: 182
Loss: 0.21320982703140803
RMSE train: 0.402930	val: 0.754799	test: 0.737569
MAE train: 0.308372	val: 0.568292	test: 0.569807

Epoch: 183
Loss: 0.22251976707151958
RMSE train: 0.437059	val: 0.775977	test: 0.735157
MAE train: 0.339507	val: 0.587926	test: 0.567838

Epoch: 184
Loss: 0.21030608671052115
RMSE train: 0.420952	val: 0.759742	test: 0.728103
MAE train: 0.324027	val: 0.573845	test: 0.563038

Epoch: 185
Loss: 0.2216013308082308
RMSE train: 0.409449	val: 0.757349	test: 0.720164
MAE train: 0.315085	val: 0.572397	test: 0.557370

Epoch: 186
Loss: 0.20408514780657633
RMSE train: 0.429237	val: 0.779231	test: 0.734285
MAE train: 0.331928	val: 0.589553	test: 0.566829

Epoch: 187
Loss: 0.21608353619064605
RMSE train: 0.411286	val: 0.758950	test: 0.722407
MAE train: 0.319267	val: 0.577257	test: 0.558891

Epoch: 188
Loss: 0.20521165536982672
RMSE train: 0.435671	val: 0.791946	test: 0.741873
MAE train: 0.338322	val: 0.603939	test: 0.563526

Epoch: 189
Loss: 0.20669271371194295
RMSE train: 0.426544	val: 0.791539	test: 0.738610
MAE train: 0.329836	val: 0.607884	test: 0.562948

Epoch: 190
Loss: 0.2026275715657643
RMSE train: 0.410273	val: 0.786335	test: 0.740870
MAE train: 0.314117	val: 0.597854	test: 0.568345

Epoch: 191
Loss: 0.21429319786173956
RMSE train: 0.408997	val: 0.758736	test: 0.719961
MAE train: 0.316864	val: 0.578957	test: 0.555760

Epoch: 192
Loss: 0.20271573747907365
RMSE train: 0.417413	val: 0.766840	test: 0.731078
MAE train: 0.322912	val: 0.587331	test: 0.564253

Epoch: 193
Loss: 0.2143550632255418
RMSE train: 0.395509	val: 0.749537	test: 0.730592
MAE train: 0.304148	val: 0.575941	test: 0.562469

Epoch: 194
Loss: 0.20099946856498718
RMSE train: 0.410540	val: 0.758957	test: 0.721935
MAE train: 0.316962	val: 0.582431	test: 0.563851

Epoch: 195
Loss: 0.2032490766474179
RMSE train: 0.413437	val: 0.768639	test: 0.729308
MAE train: 0.316431	val: 0.579685	test: 0.559450

Epoch: 196
Loss: 0.210320559995515
RMSE train: 0.418130	val: 0.773169	test: 0.718502
MAE train: 0.319707	val: 0.585223	test: 0.551692

Epoch: 197
Loss: 0.2137163749762944
RMSE train: 0.440302	val: 0.796796	test: 0.744387
MAE train: 0.341820	val: 0.600803	test: 0.573555

Epoch: 198
Loss: 0.19518911306347167
RMSE train: 0.426743	val: 0.777542	test: 0.741925
MAE train: 0.328826	val: 0.589146	test: 0.580674

Epoch: 199
Loss: 0.22079697783504212
RMSE train: 0.423777	val: 0.767976	test: 0.737331
MAE train: 0.329122	val: 0.588345	test: 0.561896

Epoch: 200
Loss: 0.20422938146761485
RMSE train: 0.405817	val: 0.773288	test: 0.736975
MAE train: 0.312360	val: 0.584788	test: 0.561159

Epoch: 201
Loss: 0.20124380609818868
RMSE train: 0.415137	val: 0.758728	test: 0.727582
MAE train: 0.318511	val: 0.580956	test: 0.565032

Epoch: 202
Loss: 0.2068430483341217
RMSE train: 0.414219	val: 0.786151	test: 0.748312
MAE train: 0.319134	val: 0.591341	test: 0.567848

Epoch: 203
Loss: 0.20188522871051515
RMSE train: 0.416486	val: 0.798627	test: 0.754172
MAE train: 0.322751	val: 0.604290	test: 0.573323

Epoch: 204
Loss: 0.19451566679137094
RMSE train: 0.408591	val: 0.769206	test: 0.735362
MAE train: 0.314300	val: 0.584240	test: 0.563077

Early stopping
Best (RMSE):	 train: 0.431587	val: 0.743581	test: 0.731135
Best (MAE):	 train: 0.331275	val: 0.565095	test: 0.571382


Epoch: 204
Loss: 0.21227035777909414
RMSE train: 0.416329	val: 0.763923	test: 0.736881
MAE train: 0.326157	val: 0.587379	test: 0.573079

Epoch: 205
Loss: 0.20538218745163508
RMSE train: 0.415252	val: 0.761071	test: 0.746350
MAE train: 0.321768	val: 0.583623	test: 0.577798

Epoch: 206
Loss: 0.2096442079969815
RMSE train: 0.404469	val: 0.764892	test: 0.752205
MAE train: 0.313854	val: 0.589428	test: 0.580785

Epoch: 207
Loss: 0.20765516800539835
RMSE train: 0.418453	val: 0.784528	test: 0.748255
MAE train: 0.326044	val: 0.601009	test: 0.577861

Epoch: 208
Loss: 0.21343366482428142
RMSE train: 0.395169	val: 0.756199	test: 0.750970
MAE train: 0.306153	val: 0.575744	test: 0.583182

Epoch: 209
Loss: 0.20114633973155702
RMSE train: 0.415763	val: 0.790634	test: 0.746829
MAE train: 0.323854	val: 0.596897	test: 0.579657

Epoch: 210
Loss: 0.21098971047571727
RMSE train: 0.405140	val: 0.772775	test: 0.740047
MAE train: 0.315229	val: 0.587320	test: 0.576004

Epoch: 211
Loss: 0.21455335723502295
RMSE train: 0.396067	val: 0.766109	test: 0.734938
MAE train: 0.307986	val: 0.582563	test: 0.569852

Epoch: 212
Loss: 0.21224711622510636
RMSE train: 0.391993	val: 0.763320	test: 0.739137
MAE train: 0.304061	val: 0.578841	test: 0.572177

Epoch: 213
Loss: 0.214871774826731
RMSE train: 0.396739	val: 0.761361	test: 0.747225
MAE train: 0.308843	val: 0.581216	test: 0.578639

Early stopping
Best (RMSE):	 train: 0.409336	val: 0.734923	test: 0.748150
Best (MAE):	 train: 0.318399	val: 0.562225	test: 0.571323


Epoch: 204
Loss: 0.20979711626257216
RMSE train: 0.396551	val: 0.757918	test: 0.748542
MAE train: 0.304486	val: 0.577721	test: 0.579028

Epoch: 205
Loss: 0.1915747725537845
RMSE train: 0.403136	val: 0.755213	test: 0.739258
MAE train: 0.311201	val: 0.582157	test: 0.574528

Epoch: 206
Loss: 0.18146474765879766
RMSE train: 0.396110	val: 0.741070	test: 0.738740
MAE train: 0.305801	val: 0.573729	test: 0.574616

Epoch: 207
Loss: 0.1919176025050027
RMSE train: 0.393061	val: 0.754091	test: 0.738100
MAE train: 0.302301	val: 0.578993	test: 0.568686

Epoch: 208
Loss: 0.18207333981990814
RMSE train: 0.385344	val: 0.732587	test: 0.734648
MAE train: 0.295925	val: 0.561817	test: 0.565635

Epoch: 209
Loss: 0.18440711391823633
RMSE train: 0.393277	val: 0.753935	test: 0.749509
MAE train: 0.302336	val: 0.577293	test: 0.576073

Epoch: 210
Loss: 0.2019121370145253
RMSE train: 0.392570	val: 0.747359	test: 0.747647
MAE train: 0.302270	val: 0.572508	test: 0.574770

Epoch: 211
Loss: 0.19335823612553732
RMSE train: 0.387097	val: 0.729995	test: 0.744761
MAE train: 0.299207	val: 0.564235	test: 0.580090

Epoch: 212
Loss: 0.19606196028845652
RMSE train: 0.400560	val: 0.766383	test: 0.752187
MAE train: 0.308411	val: 0.585326	test: 0.580387

Epoch: 213
Loss: 0.2093982526234218
RMSE train: 0.406505	val: 0.763825	test: 0.745229
MAE train: 0.315024	val: 0.588660	test: 0.578415

Epoch: 214
Loss: 0.20054673829248973
RMSE train: 0.395568	val: 0.742425	test: 0.740723
MAE train: 0.305957	val: 0.574054	test: 0.574892

Epoch: 215
Loss: 0.19816647789308003
RMSE train: 0.388549	val: 0.746081	test: 0.749398
MAE train: 0.297597	val: 0.574428	test: 0.578651

Epoch: 216
Loss: 0.19027316038097655
RMSE train: 0.388749	val: 0.735205	test: 0.755530
MAE train: 0.299580	val: 0.565858	test: 0.583998

Epoch: 217
Loss: 0.203141131571361
RMSE train: 0.402096	val: 0.745048	test: 0.734950
MAE train: 0.310356	val: 0.574020	test: 0.569477

Epoch: 218
Loss: 0.1928464291351182
RMSE train: 0.373739	val: 0.729117	test: 0.750597
MAE train: 0.286915	val: 0.562097	test: 0.566889

Epoch: 219
Loss: 0.1971528593982969
RMSE train: 0.383906	val: 0.742963	test: 0.746016
MAE train: 0.295609	val: 0.567551	test: 0.571204

Epoch: 220
Loss: 0.17828063187854631
RMSE train: 0.396760	val: 0.754972	test: 0.746507
MAE train: 0.306837	val: 0.584696	test: 0.572666

Epoch: 221
Loss: 0.187495299748012
RMSE train: 0.381654	val: 0.767019	test: 0.759365
MAE train: 0.295228	val: 0.586750	test: 0.578765

Epoch: 222
Loss: 0.18599969893693924
RMSE train: 0.385115	val: 0.726973	test: 0.742296
MAE train: 0.297742	val: 0.560173	test: 0.571599

Epoch: 223
Loss: 0.18001746918473924
RMSE train: 0.389576	val: 0.754533	test: 0.751803
MAE train: 0.301359	val: 0.575339	test: 0.575890

Epoch: 224
Loss: 0.19613732716866902
RMSE train: 0.398854	val: 0.749006	test: 0.742489
MAE train: 0.307504	val: 0.573218	test: 0.572727

Epoch: 225
Loss: 0.18337144702672958
RMSE train: 0.371349	val: 0.733137	test: 0.737270
MAE train: 0.282207	val: 0.560940	test: 0.567701

Epoch: 226
Loss: 0.18194041294710978
RMSE train: 0.385104	val: 0.748139	test: 0.743308
MAE train: 0.296430	val: 0.571953	test: 0.575928

Epoch: 227
Loss: 0.1899066184248243
RMSE train: 0.367303	val: 0.740625	test: 0.741580
MAE train: 0.281024	val: 0.561780	test: 0.570176

Epoch: 228
Loss: 0.18435203390462057
RMSE train: 0.400529	val: 0.750634	test: 0.742833
MAE train: 0.308007	val: 0.577577	test: 0.579859

Epoch: 229
Loss: 0.18416541601930345
RMSE train: 0.378827	val: 0.744586	test: 0.741463
MAE train: 0.290521	val: 0.573011	test: 0.569948

Epoch: 230
Loss: 0.18260180737291062
RMSE train: 0.401558	val: 0.758672	test: 0.760601
MAE train: 0.309993	val: 0.579834	test: 0.579841

Epoch: 231
Loss: 0.18615836650133133
RMSE train: 0.370633	val: 0.735791	test: 0.740296
MAE train: 0.283902	val: 0.562649	test: 0.569005

Epoch: 232
Loss: 0.20519945451191493
RMSE train: 0.386711	val: 0.759656	test: 0.745026
MAE train: 0.298788	val: 0.576883	test: 0.572498

Epoch: 233
Loss: 0.20042541516678675
RMSE train: 0.410407	val: 0.749226	test: 0.731623
MAE train: 0.319113	val: 0.579998	test: 0.569979

Epoch: 234
Loss: 0.19232735995735442
RMSE train: 0.378209	val: 0.760085	test: 0.753939
MAE train: 0.290693	val: 0.581106	test: 0.577177

Epoch: 235
Loss: 0.19736558624676295
RMSE train: 0.391224	val: 0.744162	test: 0.739960
MAE train: 0.300856	val: 0.572747	test: 0.568484

Epoch: 236
Loss: 0.17964520198958261
RMSE train: 0.369953	val: 0.732330	test: 0.747468
MAE train: 0.282509	val: 0.556367	test: 0.577864

Epoch: 237
Loss: 0.1974490944828306
RMSE train: 0.398254	val: 0.752774	test: 0.746914
MAE train: 0.309665	val: 0.584920	test: 0.575170

Epoch: 238
Loss: 0.18908981127398355
RMSE train: 0.390985	val: 0.769415	test: 0.743474
MAE train: 0.301424	val: 0.594688	test: 0.572560

Epoch: 239
Loss: 0.17392769668783462
RMSE train: 0.392809	val: 0.748195	test: 0.738635
MAE train: 0.306321	val: 0.581174	test: 0.573847

Epoch: 240
Loss: 0.17415648379496165
RMSE train: 0.372365	val: 0.738939	test: 0.737011
MAE train: 0.285883	val: 0.567430	test: 0.566574

Epoch: 241
Loss: 0.17503415473869868
RMSE train: 0.370940	val: 0.752163	test: 0.740502
MAE train: 0.283596	val: 0.572740	test: 0.571562

Epoch: 242
Loss: 0.18341631761619023
RMSE train: 0.383521	val: 0.741817	test: 0.748336
MAE train: 0.295627	val: 0.574594	test: 0.577691

Epoch: 243
Loss: 0.17518810821431025
RMSE train: 0.371530	val: 0.747296	test: 0.754397
MAE train: 0.286433	val: 0.576842	test: 0.582138

Epoch: 244
Loss: 0.19269568898848125
RMSE train: 0.374791	val: 0.759506	test: 0.758521
MAE train: 0.287540	val: 0.575649	test: 0.579941

Epoch: 245
Loss: 0.1848888173699379
RMSE train: 0.389167	val: 0.743411	test: 0.736600
MAE train: 0.302234	val: 0.570141	test: 0.564664

Epoch: 246
Loss: 0.1893354845898492
RMSE train: 0.381924	val: 0.746201	test: 0.745025
MAE train: 0.293268	val: 0.574415	test: 0.568759

Epoch: 247
Loss: 0.18652315331356867
RMSE train: 0.375159	val: 0.746317	test: 0.739444
MAE train: 0.288489	val: 0.567811	test: 0.565861

Epoch: 248
Loss: 0.19353481914315904
RMSE train: 0.396295	val: 0.742947	test: 0.739389
MAE train: 0.308661	val: 0.575473	test: 0.572847

Epoch: 249
Loss: 0.182673057275159
RMSE train: 0.395014	val: 0.753870	test: 0.745159
MAE train: 0.306496	val: 0.581490	test: 0.574327

Epoch: 250
Loss: 0.18813450847353255
RMSE train: 0.388042	val: 0.744436	test: 0.749642
MAE train: 0.300238	val: 0.573697	test: 0.577471

Epoch: 251
Loss: 0.18698890187910625
RMSE train: 0.387528	val: 0.757458	test: 0.741726
MAE train: 0.299975	val: 0.584173	test: 0.571694

Epoch: 252
Loss: 0.18346985216651643
RMSE train: 0.373226	val: 0.762249	test: 0.760739
MAE train: 0.287571	val: 0.573643	test: 0.586727

Epoch: 253
Loss: 0.1826272074665342
RMSE train: 0.381285	val: 0.751185	test: 0.748781
MAE train: 0.294488	val: 0.575299	test: 0.571577

Epoch: 254
Loss: 0.17567446934325354
RMSE train: 0.378766	val: 0.765183	test: 0.755144
MAE train: 0.293899	val: 0.583998	test: 0.575670

Epoch: 255
Loss: 0.1766932031938008
RMSE train: 0.382173	val: 0.749737	test: 0.727733
MAE train: 0.296244	val: 0.581347	test: 0.562148

Epoch: 256
Loss: 0.1715593391231128
RMSE train: 0.353141	val: 0.732406	test: 0.732381
MAE train: 0.270415	val: 0.561580	test: 0.559551

Epoch: 257
Loss: 0.167905400374106
RMSE train: 0.373317	val: 0.751657	test: 0.727422
MAE train: 0.287957	val: 0.573209	test: 0.561159

Early stopping
Best (RMSE):	 train: 0.385115	val: 0.726973	test: 0.742296
Best (MAE):	 train: 0.297742	val: 0.560173	test: 0.571599
All runs completed.
