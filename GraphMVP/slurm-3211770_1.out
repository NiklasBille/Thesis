>>> Starting run for dataset: freesolv
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running RANDOM configs_static_noise_experiments/GraphCL/freesolv/noise=0.0.yml on cuda:0
Running RANDOM configs_static_noise_experiments/GraphCL/freesolv/noise=0.05.yml on cuda:1
Running RANDOM configs_static_noise_experiments/GraphCL/freesolv/noise=0.1.yml on cuda:2
Running RANDOM configs_static_noise_experiments/GraphCL/freesolv/noise=0.2.yml on cuda:3
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.05.yml --runseed 4 --device cuda:1
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.05.yml --runseed 5 --device cuda:1
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.2.yml --runseed 4 --device cuda:3
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.05.yml --runseed 6 --device cuda:1
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.2.yml --runseed 5 --device cuda:3
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.1.yml --runseed 4 --device cuda:2
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.2.yml --runseed 6 --device cuda:3
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.0.yml --runseed 4 --device cuda:0
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.1.yml --runseed 5 --device cuda:2
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.0.yml --runseed 5 --device cuda:0
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.1.yml --runseed 6 --device cuda:2
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.0.yml --runseed 6 --device cuda:0
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/freesolv/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/freesolv/noise=0.0/freesolv_scaff_6_26-05_11-28-50  ]
[ Using Seed :  6  ]
[ Using device :  cuda:0  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 20.23494052886963
RMSE train: 4.419964	val: 8.340148	test: 6.355283
MAE train: 3.539600	val: 6.378007	test: 5.419292

Epoch: 2
Loss: 19.098875045776367
RMSE train: 4.365584	val: 8.254533	test: 6.441258
MAE train: 3.512180	val: 6.311938	test: 5.518513

Epoch: 3
Loss: 18.11124897003174
RMSE train: 4.191540	val: 8.072554	test: 6.385686
MAE train: 3.363443	val: 6.175782	test: 5.470549

Epoch: 4
Loss: 17.047837257385254
RMSE train: 4.015712	val: 7.847368	test: 6.263491
MAE train: 3.222794	val: 5.999016	test: 5.354789

Epoch: 5
Loss: 15.578721523284912
RMSE train: 3.816828	val: 7.559303	test: 6.125601
MAE train: 3.078871	val: 5.780372	test: 5.219875

Epoch: 6
Loss: 14.150105953216553
RMSE train: 3.583063	val: 7.236117	test: 6.028179
MAE train: 2.928551	val: 5.545690	test: 5.134713

Epoch: 7
Loss: 12.988377094268799
RMSE train: 3.378695	val: 6.907814	test: 5.987774
MAE train: 2.813660	val: 5.322461	test: 5.111504

Epoch: 8
Loss: 12.076764583587646
RMSE train: 3.179426	val: 6.561956	test: 5.987198
MAE train: 2.696650	val: 5.107839	test: 5.117768

Epoch: 9
Loss: 11.367822170257568
RMSE train: 3.030701	val: 6.261604	test: 6.023981
MAE train: 2.573435	val: 4.940691	test: 5.175885

Epoch: 10
Loss: 11.291643619537354
RMSE train: 2.928781	val: 6.007510	test: 6.011403
MAE train: 2.475692	val: 4.775145	test: 5.209322

Epoch: 11
Loss: 10.26224422454834
RMSE train: 2.944588	val: 5.958854	test: 6.012715
MAE train: 2.514375	val: 4.822951	test: 5.237881

Epoch: 12
Loss: 9.692842960357666
RMSE train: 3.005990	val: 6.073041	test: 5.980046
MAE train: 2.611748	val: 5.028976	test: 5.235475

Epoch: 13
Loss: 9.517021179199219
RMSE train: 3.071989	val: 6.256311	test: 5.968539
MAE train: 2.702252	val: 5.254634	test: 5.220865

Epoch: 14
Loss: 8.953977584838867
RMSE train: 3.114492	val: 6.398193	test: 5.946873
MAE train: 2.756451	val: 5.429447	test: 5.225627

Epoch: 15
Loss: 8.75728702545166
RMSE train: 3.097083	val: 6.444430	test: 5.930734
MAE train: 2.732471	val: 5.486214	test: 5.209511

Epoch: 16
Loss: 7.9696364402771
RMSE train: 3.063351	val: 6.385309	test: 5.909193
MAE train: 2.700341	val: 5.456172	test: 5.185130

Epoch: 17
Loss: 7.809771537780762
RMSE train: 3.034706	val: 6.293181	test: 5.904349
MAE train: 2.686876	val: 5.388207	test: 5.181149

Epoch: 18
Loss: 7.336585521697998
RMSE train: 3.047949	val: 6.269086	test: 5.947682
MAE train: 2.719906	val: 5.381831	test: 5.223732

Epoch: 19
Loss: 7.015319347381592
RMSE train: 3.017485	val: 6.225698	test: 5.907845
MAE train: 2.708917	val: 5.342457	test: 5.195812

Epoch: 20
Loss: 6.2978105545043945
RMSE train: 2.992627	val: 6.215531	test: 5.872184
MAE train: 2.696219	val: 5.333349	test: 5.181603

Epoch: 21
Loss: 6.1359710693359375
RMSE train: 2.986994	val: 6.191745	test: 5.773067
MAE train: 2.696470	val: 5.305428	test: 5.097948

Epoch: 22
Loss: 6.045663595199585
RMSE train: 2.951870	val: 6.165198	test: 5.623934
MAE train: 2.653819	val: 5.267500	test: 4.960589
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/freesolv/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/freesolv/noise=0.0/freesolv_scaff_5_26-05_11-28-50  ]
[ Using Seed :  5  ]
[ Using device :  cuda:0  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 21.056089401245117
RMSE train: 5.113491	val: 8.596300	test: 7.001340
MAE train: 4.306968	val: 6.754116	test: 6.180608

Epoch: 2
Loss: 19.548344612121582
RMSE train: 5.390679	val: 8.602182	test: 7.125247
MAE train: 4.588808	val: 6.865360	test: 6.309204

Epoch: 3
Loss: 18.539462089538574
RMSE train: 5.411447	val: 8.447449	test: 7.006472
MAE train: 4.592979	val: 6.739133	test: 6.174950

Epoch: 4
Loss: 17.305830001831055
RMSE train: 5.229731	val: 8.191768	test: 6.802284
MAE train: 4.409625	val: 6.470913	test: 5.944377

Epoch: 5
Loss: 16.50790023803711
RMSE train: 4.897257	val: 7.946669	test: 6.650334
MAE train: 4.112307	val: 6.218960	test: 5.787458

Epoch: 6
Loss: 15.163496494293213
RMSE train: 4.526890	val: 7.755186	test: 6.551037
MAE train: 3.804234	val: 6.051410	test: 5.706801

Epoch: 7
Loss: 14.595690727233887
RMSE train: 4.267916	val: 7.640453	test: 6.495752
MAE train: 3.603042	val: 6.008415	test: 5.669346

Epoch: 8
Loss: 13.504086971282959
RMSE train: 4.103597	val: 7.554777	test: 6.477211
MAE train: 3.495557	val: 6.036308	test: 5.688685

Epoch: 9
Loss: 13.348252773284912
RMSE train: 3.992115	val: 7.446941	test: 6.369034
MAE train: 3.426131	val: 6.028725	test: 5.623005

Epoch: 10
Loss: 11.83036756515503
RMSE train: 3.860559	val: 7.281062	test: 6.194140
MAE train: 3.330797	val: 5.960302	test: 5.485658

Epoch: 11
Loss: 11.799421310424805
RMSE train: 3.674246	val: 7.014542	test: 5.976982
MAE train: 3.179939	val: 5.782388	test: 5.301485

Epoch: 12
Loss: 10.872439861297607
RMSE train: 3.475818	val: 6.587436	test: 5.640004
MAE train: 3.004170	val: 5.424099	test: 4.973402

Epoch: 13
Loss: 10.462078094482422
RMSE train: 3.264283	val: 6.099305	test: 5.304685
MAE train: 2.787209	val: 4.955372	test: 4.644077

Epoch: 14
Loss: 9.550227165222168
RMSE train: 3.108770	val: 5.675522	test: 5.064800
MAE train: 2.642573	val: 4.512869	test: 4.385670

Epoch: 15
Loss: 8.996240139007568
RMSE train: 2.964976	val: 5.418623	test: 4.918679
MAE train: 2.516429	val: 4.241830	test: 4.219800

Epoch: 16
Loss: 8.58867359161377
RMSE train: 2.932571	val: 5.381868	test: 4.943623
MAE train: 2.510707	val: 4.241486	test: 4.222446

Epoch: 17
Loss: 8.02025055885315
RMSE train: 2.960838	val: 5.481106	test: 5.049483
MAE train: 2.564537	val: 4.407741	test: 4.321206

Epoch: 18
Loss: 7.710946321487427
RMSE train: 3.016858	val: 5.602499	test: 5.163532
MAE train: 2.638277	val: 4.587343	test: 4.438707

Epoch: 19
Loss: 7.530282020568848
RMSE train: 3.036901	val: 5.699130	test: 5.253631
MAE train: 2.672635	val: 4.720062	test: 4.569342

Epoch: 20
Loss: 7.090773582458496
RMSE train: 3.046250	val: 5.730119	test: 5.286994
MAE train: 2.685377	val: 4.789600	test: 4.642295

Epoch: 21
Loss: 6.590723514556885
RMSE train: 3.044116	val: 5.763389	test: 5.257162
MAE train: 2.674920	val: 4.844665	test: 4.632906

Epoch: 22
Loss: 6.202141761779785
RMSE train: 3.001452	val: 5.764744	test: 5.210685
MAE train: 2.631461	val: 4.830844	test: 4.580655Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/freesolv/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/freesolv/noise=0.0/freesolv_scaff_4_26-05_11-28-50  ]
[ Using Seed :  4  ]
[ Using device :  cuda:0  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 21.202117919921875
RMSE train: 4.551854	val: 8.999084	test: 7.084273
MAE train: 3.627421	val: 6.916473	test: 6.243788

Epoch: 2
Loss: 20.108320236206055
RMSE train: 4.550369	val: 8.935580	test: 7.105470
MAE train: 3.668709	val: 6.867830	test: 6.257944

Epoch: 3
Loss: 19.074345588684082
RMSE train: 4.512097	val: 8.934952	test: 7.157242
MAE train: 3.664412	val: 6.956362	test: 6.310008

Epoch: 4
Loss: 17.96513557434082
RMSE train: 4.435168	val: 8.934757	test: 7.215102
MAE train: 3.623532	val: 7.078942	test: 6.354942

Epoch: 5
Loss: 16.675238609313965
RMSE train: 4.312723	val: 8.908389	test: 7.259742
MAE train: 3.547323	val: 7.169648	test: 6.389558

Epoch: 6
Loss: 15.55175495147705
RMSE train: 4.136216	val: 8.850095	test: 7.283987
MAE train: 3.435539	val: 7.256015	test: 6.416716

Epoch: 7
Loss: 14.252321720123291
RMSE train: 3.880475	val: 8.654608	test: 7.200886
MAE train: 3.260226	val: 7.209178	test: 6.341787

Epoch: 8
Loss: 13.057811737060547
RMSE train: 3.567470	val: 8.274407	test: 7.021595
MAE train: 3.045412	val: 6.958012	test: 6.161073

Epoch: 9
Loss: 12.214727878570557
RMSE train: 3.286127	val: 7.856508	test: 6.776269
MAE train: 2.833977	val: 6.604830	test: 5.898422

Epoch: 10
Loss: 11.437840461730957
RMSE train: 3.086178	val: 7.463183	test: 6.484855
MAE train: 2.675451	val: 6.220252	test: 5.618818

Epoch: 11
Loss: 10.804983615875244
RMSE train: 2.983402	val: 7.205996	test: 6.251068
MAE train: 2.584506	val: 6.000560	test: 5.404970

Epoch: 12
Loss: 11.022026538848877
RMSE train: 2.890548	val: 7.072292	test: 6.015920
MAE train: 2.507404	val: 5.972068	test: 5.217419

Epoch: 13
Loss: 9.82545804977417
RMSE train: 2.897943	val: 7.013286	test: 5.857840
MAE train: 2.538384	val: 6.035658	test: 5.066149

Epoch: 14
Loss: 9.62860107421875
RMSE train: 2.917616	val: 6.946335	test: 5.702145
MAE train: 2.568198	val: 6.071609	test: 4.922543

Epoch: 15
Loss: 9.263096809387207
RMSE train: 2.932087	val: 6.912516	test: 5.592000
MAE train: 2.573268	val: 6.076160	test: 4.819725

Epoch: 16
Loss: 8.781351566314697
RMSE train: 2.982044	val: 6.940460	test: 5.538952
MAE train: 2.619380	val: 6.123974	test: 4.781312

Epoch: 17
Loss: 8.415178537368774
RMSE train: 2.991602	val: 6.921141	test: 5.457554
MAE train: 2.632048	val: 6.109199	test: 4.697642

Epoch: 18
Loss: 7.758150100708008
RMSE train: 2.982805	val: 6.910707	test: 5.392336
MAE train: 2.631970	val: 6.090816	test: 4.617181

Epoch: 19
Loss: 7.652626991271973
RMSE train: 2.923286	val: 6.772161	test: 5.270475
MAE train: 2.578417	val: 5.937675	test: 4.483310

Epoch: 20
Loss: 7.1105546951293945
RMSE train: 2.866364	val: 6.656984	test: 5.169970
MAE train: 2.526777	val: 5.783287	test: 4.382028

Epoch: 21
Loss: 6.797802209854126
RMSE train: 2.804551	val: 6.534383	test: 5.066353
MAE train: 2.471617	val: 5.638922	test: 4.294234

Epoch: 22
Loss: 6.5160605907440186
RMSE train: 2.752587	val: 6.425639	test: 5.006605
MAE train: 2.424699	val: 5.527101	test: 4.252500Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/freesolv/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/freesolv/noise=0.1/freesolv_scaff_4_26-05_11-28-50  ]
[ Using Seed :  4  ]
[ Using device :  cuda:2  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 21.536246299743652
RMSE train: 4.712034	val: 8.855163	test: 6.952995
MAE train: 3.776081	val: 6.738725	test: 6.018243

Epoch: 2
Loss: 20.40738868713379
RMSE train: 4.692330	val: 8.834912	test: 7.073042
MAE train: 3.766572	val: 6.731109	test: 6.169947

Epoch: 3
Loss: 19.66115665435791
RMSE train: 4.684879	val: 8.786947	test: 7.149960
MAE train: 3.773207	val: 6.739836	test: 6.253349

Epoch: 4
Loss: 19.134818077087402
RMSE train: 4.695346	val: 8.750330	test: 7.180008
MAE train: 3.806471	val: 6.763147	test: 6.287215

Epoch: 5
Loss: 18.415477752685547
RMSE train: 4.708675	val: 8.716907	test: 7.193802
MAE train: 3.846531	val: 6.770583	test: 6.301459

Epoch: 6
Loss: 17.632695198059082
RMSE train: 4.705661	val: 8.667000	test: 7.188577
MAE train: 3.871483	val: 6.760639	test: 6.302001

Epoch: 7
Loss: 16.85749626159668
RMSE train: 4.709245	val: 8.630972	test: 7.190793
MAE train: 3.914418	val: 6.801554	test: 6.317934

Epoch: 8
Loss: 16.322474479675293
RMSE train: 4.654584	val: 8.552882	test: 7.140374
MAE train: 3.899873	val: 6.788460	test: 6.285053

Epoch: 9
Loss: 15.238610744476318
RMSE train: 4.573513	val: 8.446845	test: 7.063708
MAE train: 3.856917	val: 6.761012	test: 6.231599

Epoch: 10
Loss: 14.745552062988281
RMSE train: 4.382248	val: 8.284554	test: 6.934341
MAE train: 3.701530	val: 6.650575	test: 6.124926

Epoch: 11
Loss: 13.809369564056396
RMSE train: 4.166926	val: 8.106383	test: 6.821538
MAE train: 3.516822	val: 6.529859	test: 6.035572

Epoch: 12
Loss: 12.83051872253418
RMSE train: 3.957392	val: 7.870640	test: 6.672778
MAE train: 3.328902	val: 6.341021	test: 5.908015

Epoch: 13
Loss: 12.285677433013916
RMSE train: 3.761718	val: 7.568167	test: 6.489950
MAE train: 3.155761	val: 6.092686	test: 5.743967

Epoch: 14
Loss: 11.858991146087646
RMSE train: 3.588870	val: 7.272399	test: 6.341885
MAE train: 3.009248	val: 5.856369	test: 5.598934

Epoch: 15
Loss: 11.235458374023438
RMSE train: 3.447824	val: 7.001376	test: 6.219223
MAE train: 2.890815	val: 5.677274	test: 5.460828

Epoch: 16
Loss: 10.650812149047852
RMSE train: 3.372481	val: 6.881897	test: 6.183129
MAE train: 2.829436	val: 5.622676	test: 5.397085

Epoch: 17
Loss: 10.118439674377441
RMSE train: 3.311961	val: 6.813637	test: 6.122672
MAE train: 2.781710	val: 5.581652	test: 5.317937

Epoch: 18
Loss: 9.766038417816162
RMSE train: 3.257889	val: 6.815406	test: 6.069617
MAE train: 2.735373	val: 5.578506	test: 5.249016

Epoch: 19
Loss: 9.12879228591919
RMSE train: 3.226791	val: 6.856033	test: 6.052619
MAE train: 2.712449	val: 5.632611	test: 5.239268

Epoch: 20
Loss: 8.703485012054443
RMSE train: 3.240074	val: 6.945983	test: 6.079019
MAE train: 2.729215	val: 5.729375	test: 5.298228

Epoch: 21
Loss: 8.06907033920288
RMSE train: 3.254884	val: 7.021927	test: 6.103709
MAE train: 2.749085	val: 5.814210	test: 5.354962

Epoch: 22
Loss: 7.489335536956787Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:3
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/freesolv/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/freesolv/noise=0.2/freesolv_scaff_5_26-05_11-28-50  ]
[ Using Seed :  5  ]
[ Using device :  cuda:3  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 21.114930152893066
RMSE train: 4.594851	val: 8.630996	test: 6.621974
MAE train: 3.660593	val: 6.634763	test: 5.677472

Epoch: 2
Loss: 20.43930149078369
RMSE train: 4.572256	val: 8.585757	test: 6.677946
MAE train: 3.644336	val: 6.556393	test: 5.735962

Epoch: 3
Loss: 19.573381423950195
RMSE train: 4.512949	val: 8.531767	test: 6.711430
MAE train: 3.596439	val: 6.479869	test: 5.781192

Epoch: 4
Loss: 18.847851753234863
RMSE train: 4.470565	val: 8.471635	test: 6.750863
MAE train: 3.566028	val: 6.395823	test: 5.839118

Epoch: 5
Loss: 18.316842079162598
RMSE train: 4.440526	val: 8.426478	test: 6.799567
MAE train: 3.551397	val: 6.401737	test: 5.905408

Epoch: 6
Loss: 17.74679660797119
RMSE train: 4.430521	val: 8.393095	test: 6.850246
MAE train: 3.558856	val: 6.441001	test: 5.970447

Epoch: 7
Loss: 17.041797637939453
RMSE train: 4.433585	val: 8.363433	test: 6.906726
MAE train: 3.584993	val: 6.472395	test: 6.034697

Epoch: 8
Loss: 15.928533554077148
RMSE train: 4.408729	val: 8.316975	test: 6.944438
MAE train: 3.587288	val: 6.491021	test: 6.078797

Epoch: 9
Loss: 15.485180377960205
RMSE train: 4.348329	val: 8.259829	test: 6.957311
MAE train: 3.559559	val: 6.499128	test: 6.099161

Epoch: 10
Loss: 14.858859539031982
RMSE train: 4.243582	val: 8.192977	test: 6.953257
MAE train: 3.486804	val: 6.522642	test: 6.109468

Epoch: 11
Loss: 14.577329635620117
RMSE train: 4.129160	val: 8.086483	test: 6.917467
MAE train: 3.401374	val: 6.506022	test: 6.084885

Epoch: 12
Loss: 13.55659532546997
RMSE train: 4.011364	val: 7.972629	test: 6.857229
MAE train: 3.308727	val: 6.446818	test: 6.031284

Epoch: 13
Loss: 12.751960754394531
RMSE train: 3.903117	val: 7.858848	test: 6.797036
MAE train: 3.223752	val: 6.373621	test: 5.980091

Epoch: 14
Loss: 12.442853450775146
RMSE train: 3.798788	val: 7.740698	test: 6.733193
MAE train: 3.137810	val: 6.272309	test: 5.924578

Epoch: 15
Loss: 11.802594184875488
RMSE train: 3.689225	val: 7.612060	test: 6.634377
MAE train: 3.045209	val: 6.160038	test: 5.835268

Epoch: 16
Loss: 11.16786003112793
RMSE train: 3.588942	val: 7.489158	test: 6.514564
MAE train: 2.959671	val: 6.027441	test: 5.722848

Epoch: 17
Loss: 10.446614265441895
RMSE train: 3.476873	val: 7.343673	test: 6.390669
MAE train: 2.863765	val: 5.877342	test: 5.602941

Epoch: 18
Loss: 10.185731410980225
RMSE train: 3.389769	val: 7.215432	test: 6.265248
MAE train: 2.793572	val: 5.742219	test: 5.476869

Epoch: 19
Loss: 9.874893188476562
RMSE train: 3.295789	val: 7.116028	test: 6.172555
MAE train: 2.721427	val: 5.662797	test: 5.392032

Epoch: 20
Loss: 9.382028579711914
RMSE train: 3.216122	val: 7.051823	test: 6.116397
MAE train: 2.660692	val: 5.617694	test: 5.338100

Epoch: 21
Loss: 8.67725920677185
RMSE train: 3.107033	val: 6.986254	test: 6.057017
MAE train: 2.569181	val: 5.588020	test: 5.287764

Epoch: 22
Loss: 7.993580341339111Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/freesolv/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/freesolv/noise=0.1/freesolv_scaff_6_26-05_11-28-50  ]
[ Using Seed :  6  ]
[ Using device :  cuda:2  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 20.751551628112793
RMSE train: 4.623989	val: 8.546972	test: 6.814771
MAE train: 3.747378	val: 6.593436	test: 5.922942

Epoch: 2
Loss: 20.151366233825684
RMSE train: 4.490612	val: 8.452101	test: 6.700946
MAE train: 3.619078	val: 6.487694	test: 5.799900

Epoch: 3
Loss: 19.209805488586426
RMSE train: 4.378709	val: 8.373098	test: 6.678848
MAE train: 3.519028	val: 6.393408	test: 5.794540

Epoch: 4
Loss: 18.491780281066895
RMSE train: 4.297763	val: 8.288568	test: 6.708640
MAE train: 3.458674	val: 6.371739	test: 5.849301

Epoch: 5
Loss: 17.71662139892578
RMSE train: 4.226091	val: 8.189029	test: 6.756688
MAE train: 3.406149	val: 6.364983	test: 5.919071

Epoch: 6
Loss: 17.076138496398926
RMSE train: 4.152553	val: 8.062335	test: 6.787964
MAE train: 3.358074	val: 6.342422	test: 5.963145

Epoch: 7
Loss: 15.786840915679932
RMSE train: 4.067372	val: 7.895309	test: 6.802745
MAE train: 3.312735	val: 6.304077	test: 5.990311

Epoch: 8
Loss: 15.052381038665771
RMSE train: 3.986595	val: 7.662587	test: 6.816103
MAE train: 3.281359	val: 6.217784	test: 6.004545

Epoch: 9
Loss: 14.32640266418457
RMSE train: 3.874422	val: 7.404352	test: 6.787169
MAE train: 3.222815	val: 6.094231	test: 5.971289

Epoch: 10
Loss: 13.68056869506836
RMSE train: 3.765771	val: 7.174978	test: 6.736883
MAE train: 3.173699	val: 5.972740	test: 5.923847

Epoch: 11
Loss: 13.08684492111206
RMSE train: 3.647049	val: 6.901888	test: 6.638644
MAE train: 3.099831	val: 5.793817	test: 5.831090

Epoch: 12
Loss: 12.11627197265625
RMSE train: 3.545498	val: 6.648999	test: 6.541671
MAE train: 3.026950	val: 5.615084	test: 5.737522

Epoch: 13
Loss: 11.65665864944458
RMSE train: 3.470288	val: 6.505076	test: 6.458466
MAE train: 2.975786	val: 5.500538	test: 5.669419

Epoch: 14
Loss: 10.853549003601074
RMSE train: 3.392579	val: 6.376020	test: 6.393804
MAE train: 2.917605	val: 5.400322	test: 5.619042

Epoch: 15
Loss: 10.08885669708252
RMSE train: 3.318895	val: 6.282587	test: 6.341015
MAE train: 2.857559	val: 5.323343	test: 5.581441

Epoch: 16
Loss: 9.492496013641357
RMSE train: 3.250601	val: 6.196351	test: 6.314353
MAE train: 2.805919	val: 5.253871	test: 5.566005

Epoch: 17
Loss: 9.52697467803955
RMSE train: 3.211536	val: 6.164809	test: 6.346847
MAE train: 2.779208	val: 5.243140	test: 5.607074

Epoch: 18
Loss: 9.047534465789795
RMSE train: 3.193550	val: 6.149646	test: 6.405324
MAE train: 2.771573	val: 5.248005	test: 5.672158

Epoch: 19
Loss: 8.380844116210938
RMSE train: 3.206183	val: 6.162110	test: 6.480499
MAE train: 2.789984	val: 5.284891	test: 5.746804

Epoch: 20
Loss: 8.06666374206543
RMSE train: 3.251059	val: 6.266392	test: 6.570619
MAE train: 2.837713	val: 5.421542	test: 5.851343

Epoch: 21
Loss: 7.450453281402588
RMSE train: 3.268822	val: 6.388164	test: 6.629532
MAE train: 2.858070	val: 5.567928	test: 5.929252

Epoch: 22
Loss: 7.088100433349609Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/freesolv/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/freesolv/noise=0.05/freesolv_scaff_6_26-05_11-28-50  ]
[ Using Seed :  6  ]
[ Using device :  cuda:1  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 20.816099166870117
RMSE train: 4.639754	val: 8.595457	test: 6.923727
MAE train: 3.779655	val: 6.630177	test: 6.053001

Epoch: 2
Loss: 20.041675567626953
RMSE train: 4.540525	val: 8.511742	test: 6.809485
MAE train: 3.685057	val: 6.522017	test: 5.923958

Epoch: 3
Loss: 19.156328201293945
RMSE train: 4.471676	val: 8.414827	test: 6.727679
MAE train: 3.636865	val: 6.411113	test: 5.843053

Epoch: 4
Loss: 18.22850799560547
RMSE train: 4.401948	val: 8.344216	test: 6.683079
MAE train: 3.589929	val: 6.376599	test: 5.815935

Epoch: 5
Loss: 17.409679889678955
RMSE train: 4.343598	val: 8.286176	test: 6.664381
MAE train: 3.568168	val: 6.411442	test: 5.821250

Epoch: 6
Loss: 16.510929107666016
RMSE train: 4.276290	val: 8.229031	test: 6.680276
MAE train: 3.555169	val: 6.494666	test: 5.860288

Epoch: 7
Loss: 15.026552200317383
RMSE train: 4.195975	val: 8.140398	test: 6.690702
MAE train: 3.539047	val: 6.545857	test: 5.889262

Epoch: 8
Loss: 14.184037685394287
RMSE train: 4.102510	val: 7.985486	test: 6.670840
MAE train: 3.512001	val: 6.514397	test: 5.884487

Epoch: 9
Loss: 13.313303470611572
RMSE train: 3.972839	val: 7.781269	test: 6.605036
MAE train: 3.444124	val: 6.410030	test: 5.824759

Epoch: 10
Loss: 12.694384098052979
RMSE train: 3.826477	val: 7.584591	test: 6.510547
MAE train: 3.349025	val: 6.292758	test: 5.732770

Epoch: 11
Loss: 11.715246200561523
RMSE train: 3.670606	val: 7.341593	test: 6.328816
MAE train: 3.237687	val: 6.097360	test: 5.548828

Epoch: 12
Loss: 11.109354019165039
RMSE train: 3.530940	val: 7.097438	test: 6.124616
MAE train: 3.128625	val: 5.870050	test: 5.335193

Epoch: 13
Loss: 10.958741188049316
RMSE train: 3.443408	val: 6.965341	test: 5.973147
MAE train: 3.055002	val: 5.704768	test: 5.188114

Epoch: 14
Loss: 10.242431640625
RMSE train: 3.368575	val: 6.888203	test: 5.862751
MAE train: 2.981580	val: 5.572850	test: 5.079080

Epoch: 15
Loss: 9.422806739807129
RMSE train: 3.354844	val: 6.881882	test: 5.804248
MAE train: 2.960337	val: 5.515786	test: 5.006348

Epoch: 16
Loss: 8.990830421447754
RMSE train: 3.354875	val: 6.921221	test: 5.823195
MAE train: 2.966676	val: 5.541565	test: 5.008827

Epoch: 17
Loss: 8.65112018585205
RMSE train: 3.391331	val: 7.032851	test: 5.908539
MAE train: 3.017957	val: 5.652413	test: 5.093952

Epoch: 18
Loss: 8.0923433303833
RMSE train: 3.399747	val: 7.117823	test: 5.987743
MAE train: 3.042951	val: 5.773439	test: 5.174983

Epoch: 19
Loss: 7.737902641296387
RMSE train: 3.361929	val: 7.126920	test: 6.020897
MAE train: 3.022527	val: 5.802422	test: 5.196266

Epoch: 20
Loss: 7.488205194473267
RMSE train: 3.335192	val: 7.148210	test: 6.033178
MAE train: 3.006615	val: 5.843988	test: 5.210415

Epoch: 21
Loss: 7.080910682678223
RMSE train: 3.290022	val: 7.141564	test: 6.005495
MAE train: 2.972501	val: 5.866995	test: 5.192544

Epoch: 22
Loss: 6.558694362640381Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/freesolv/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/freesolv/noise=0.1/freesolv_scaff_5_26-05_11-28-50  ]
[ Using Seed :  5  ]
[ Using device :  cuda:2  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 21.149999618530273
RMSE train: 4.488054	val: 8.660775	test: 6.647912
MAE train: 3.554599	val: 6.652344	test: 5.702555

Epoch: 2
Loss: 20.281763076782227
RMSE train: 4.490021	val: 8.632177	test: 6.731362
MAE train: 3.566021	val: 6.592105	test: 5.792770

Epoch: 3
Loss: 19.25508975982666
RMSE train: 4.420615	val: 8.570534	test: 6.761794
MAE train: 3.520695	val: 6.537312	test: 5.836646

Epoch: 4
Loss: 18.67769718170166
RMSE train: 4.353643	val: 8.513101	test: 6.798447
MAE train: 3.473970	val: 6.491482	test: 5.891226

Epoch: 5
Loss: 17.95764446258545
RMSE train: 4.290374	val: 8.441232	test: 6.801865
MAE train: 3.429026	val: 6.422069	test: 5.909306

Epoch: 6
Loss: 17.29270362854004
RMSE train: 4.246151	val: 8.379118	test: 6.813806
MAE train: 3.402688	val: 6.366277	test: 5.939484

Epoch: 7
Loss: 16.35230588912964
RMSE train: 4.191421	val: 8.313902	test: 6.794582
MAE train: 3.366132	val: 6.315626	test: 5.940815

Epoch: 8
Loss: 15.564734935760498
RMSE train: 4.127202	val: 8.255141	test: 6.728619
MAE train: 3.328854	val: 6.298862	test: 5.903420

Epoch: 9
Loss: 14.960461616516113
RMSE train: 4.047188	val: 8.184836	test: 6.622099
MAE train: 3.280808	val: 6.276620	test: 5.821149

Epoch: 10
Loss: 14.3143630027771
RMSE train: 3.951937	val: 8.113911	test: 6.528966
MAE train: 3.217057	val: 6.258603	test: 5.741879

Epoch: 11
Loss: 13.591333389282227
RMSE train: 3.848140	val: 7.997090	test: 6.440471
MAE train: 3.146637	val: 6.224530	test: 5.659081

Epoch: 12
Loss: 12.66356897354126
RMSE train: 3.729992	val: 7.828160	test: 6.321937
MAE train: 3.056461	val: 6.122043	test: 5.538554

Epoch: 13
Loss: 12.100454807281494
RMSE train: 3.615355	val: 7.678791	test: 6.219290
MAE train: 2.969400	val: 6.023720	test: 5.427058

Epoch: 14
Loss: 11.521448612213135
RMSE train: 3.544130	val: 7.619619	test: 6.168808
MAE train: 2.925558	val: 5.988329	test: 5.380245

Epoch: 15
Loss: 11.140218257904053
RMSE train: 3.484037	val: 7.589019	test: 6.126360
MAE train: 2.884676	val: 5.975373	test: 5.342086

Epoch: 16
Loss: 10.59445571899414
RMSE train: 3.433892	val: 7.573444	test: 6.088104
MAE train: 2.846108	val: 5.994638	test: 5.303052

Epoch: 17
Loss: 9.922138690948486
RMSE train: 3.399247	val: 7.586713	test: 6.084134
MAE train: 2.819786	val: 6.036902	test: 5.301298

Epoch: 18
Loss: 9.358036994934082
RMSE train: 3.344929	val: 7.580863	test: 6.082939
MAE train: 2.775283	val: 6.052800	test: 5.301705

Epoch: 19
Loss: 9.465662002563477
RMSE train: 3.289315	val: 7.606921	test: 6.106291
MAE train: 2.734424	val: 6.105693	test: 5.334295

Epoch: 20
Loss: 8.66342306137085
RMSE train: 3.240031	val: 7.630513	test: 6.134973
MAE train: 2.699477	val: 6.157436	test: 5.380860

Epoch: 21
Loss: 8.249697208404541
RMSE train: 3.181735	val: 7.598816	test: 6.151691
MAE train: 2.657678	val: 6.167104	test: 5.410404

Epoch: 22
Loss: 7.697149276733398Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:3
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/freesolv/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/freesolv/noise=0.2/freesolv_scaff_4_26-05_11-28-50  ]
[ Using Seed :  4  ]
[ Using device :  cuda:3  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 21.532870292663574
RMSE train: 4.500819	val: 8.758024	test: 6.731025
MAE train: 3.576077	val: 6.679339	test: 5.779064

Epoch: 2
Loss: 20.679850578308105
RMSE train: 4.448094	val: 8.690682	test: 6.751018
MAE train: 3.540942	val: 6.595670	test: 5.794340

Epoch: 3
Loss: 19.987476348876953
RMSE train: 4.440511	val: 8.618991	test: 6.751722
MAE train: 3.550119	val: 6.518828	test: 5.787495

Epoch: 4
Loss: 19.506696701049805
RMSE train: 4.420879	val: 8.537373	test: 6.714268
MAE train: 3.539960	val: 6.426114	test: 5.747691

Epoch: 5
Loss: 19.067689895629883
RMSE train: 4.378190	val: 8.441205	test: 6.644603
MAE train: 3.505343	val: 6.325159	test: 5.681519

Epoch: 6
Loss: 18.4866304397583
RMSE train: 4.317021	val: 8.334047	test: 6.562394
MAE train: 3.454302	val: 6.222132	test: 5.604811

Epoch: 7
Loss: 17.91989231109619
RMSE train: 4.260429	val: 8.204969	test: 6.459731
MAE train: 3.412240	val: 6.114273	test: 5.506145

Epoch: 8
Loss: 17.38282012939453
RMSE train: 4.201342	val: 8.077411	test: 6.364694
MAE train: 3.368689	val: 6.001055	test: 5.420271

Epoch: 9
Loss: 16.5702862739563
RMSE train: 4.145453	val: 7.946538	test: 6.287607
MAE train: 3.333245	val: 5.881938	test: 5.361235

Epoch: 10
Loss: 15.971735000610352
RMSE train: 4.067460	val: 7.809518	test: 6.219023
MAE train: 3.281471	val: 5.772596	test: 5.315777

Epoch: 11
Loss: 15.27536153793335
RMSE train: 3.980604	val: 7.647023	test: 6.158034
MAE train: 3.220951	val: 5.706207	test: 5.283458

Epoch: 12
Loss: 14.374399662017822
RMSE train: 3.902528	val: 7.451854	test: 6.097462
MAE train: 3.162153	val: 5.695194	test: 5.259809

Epoch: 13
Loss: 13.751098155975342
RMSE train: 3.831995	val: 7.242094	test: 6.028345
MAE train: 3.108810	val: 5.641732	test: 5.221031

Epoch: 14
Loss: 12.929165840148926
RMSE train: 3.752170	val: 7.044494	test: 5.935953
MAE train: 3.057273	val: 5.598754	test: 5.166852

Epoch: 15
Loss: 12.508850574493408
RMSE train: 3.655591	val: 6.844232	test: 5.786198
MAE train: 2.987488	val: 5.486584	test: 5.054891

Epoch: 16
Loss: 12.349504470825195
RMSE train: 3.572000	val: 6.749155	test: 5.703902
MAE train: 2.926236	val: 5.448300	test: 5.006739

Epoch: 17
Loss: 11.71331787109375
RMSE train: 3.514791	val: 6.705365	test: 5.633213
MAE train: 2.891581	val: 5.410324	test: 4.960427

Epoch: 18
Loss: 11.053013324737549
RMSE train: 3.478123	val: 6.680519	test: 5.573703
MAE train: 2.871341	val: 5.355219	test: 4.914606

Epoch: 19
Loss: 10.779127359390259
RMSE train: 3.457175	val: 6.647814	test: 5.541676
MAE train: 2.872777	val: 5.311419	test: 4.899787

Epoch: 20
Loss: 9.767496109008789
RMSE train: 3.465541	val: 6.646580	test: 5.533743
MAE train: 2.903267	val: 5.301736	test: 4.912749

Epoch: 21
Loss: 9.300382137298584
RMSE train: 3.482108	val: 6.672700	test: 5.561221
MAE train: 2.942960	val: 5.334596	test: 4.956198

Epoch: 22
Loss: 8.794555187225342Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/freesolv/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/freesolv/noise=0.05/freesolv_scaff_4_26-05_11-28-50  ]
[ Using Seed :  4  ]
[ Using device :  cuda:1  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 21.591498374938965
RMSE train: 4.714312	val: 8.781207	test: 6.885860
MAE train: 3.756397	val: 6.687013	test: 5.954161

Epoch: 2
Loss: 20.515432357788086
RMSE train: 4.694355	val: 8.753701	test: 6.994538
MAE train: 3.760797	val: 6.660161	test: 6.087357

Epoch: 3
Loss: 19.44900894165039
RMSE train: 4.701656	val: 8.701481	test: 7.030956
MAE train: 3.791478	val: 6.595368	test: 6.122044

Epoch: 4
Loss: 18.84354877471924
RMSE train: 4.712919	val: 8.670968	test: 7.038594
MAE train: 3.828717	val: 6.561419	test: 6.130060

Epoch: 5
Loss: 18.07603931427002
RMSE train: 4.732895	val: 8.648676	test: 7.041589
MAE train: 3.885158	val: 6.581899	test: 6.146921

Epoch: 6
Loss: 17.283228874206543
RMSE train: 4.727803	val: 8.611801	test: 7.024795
MAE train: 3.924418	val: 6.619974	test: 6.151381

Epoch: 7
Loss: 16.304123878479004
RMSE train: 4.704503	val: 8.562237	test: 6.985036
MAE train: 3.943464	val: 6.665686	test: 6.133631

Epoch: 8
Loss: 15.672319412231445
RMSE train: 4.594245	val: 8.450161	test: 6.847727
MAE train: 3.870760	val: 6.632716	test: 6.025000

Epoch: 9
Loss: 14.810017585754395
RMSE train: 4.446400	val: 8.271746	test: 6.651629
MAE train: 3.761504	val: 6.530983	test: 5.859731

Epoch: 10
Loss: 14.021396160125732
RMSE train: 4.220784	val: 8.042277	test: 6.435522
MAE train: 3.576811	val: 6.368154	test: 5.678692

Epoch: 11
Loss: 13.264894485473633
RMSE train: 3.927835	val: 7.810768	test: 6.248134
MAE train: 3.315998	val: 6.192753	test: 5.516640

Epoch: 12
Loss: 12.403410911560059
RMSE train: 3.651726	val: 7.553377	test: 6.109954
MAE train: 3.073785	val: 6.006453	test: 5.395978

Epoch: 13
Loss: 11.328126907348633
RMSE train: 3.384886	val: 7.231738	test: 5.948353
MAE train: 2.838201	val: 5.760715	test: 5.243154

Epoch: 14
Loss: 10.637022972106934
RMSE train: 3.182906	val: 6.897758	test: 5.816210
MAE train: 2.660751	val: 5.537668	test: 5.106169

Epoch: 15
Loss: 10.233531951904297
RMSE train: 3.025643	val: 6.583963	test: 5.728656
MAE train: 2.532835	val: 5.330911	test: 4.992706

Epoch: 16
Loss: 10.295156478881836
RMSE train: 2.929489	val: 6.492866	test: 5.737844
MAE train: 2.457473	val: 5.303265	test: 4.999429

Epoch: 17
Loss: 9.355443477630615
RMSE train: 2.862033	val: 6.487864	test: 5.796060
MAE train: 2.400638	val: 5.334648	test: 5.051129

Epoch: 18
Loss: 8.919244289398193
RMSE train: 2.834509	val: 6.566094	test: 5.906492
MAE train: 2.391386	val: 5.443707	test: 5.154065

Epoch: 19
Loss: 8.765433549880981
RMSE train: 2.823051	val: 6.652683	test: 6.005269
MAE train: 2.392601	val: 5.571761	test: 5.245380

Epoch: 20
Loss: 8.198198795318604
RMSE train: 2.837381	val: 6.770163	test: 6.107658
MAE train: 2.417597	val: 5.720900	test: 5.351834

Epoch: 21
Loss: 7.817044973373413
RMSE train: 2.855658	val: 6.923041	test: 6.169248
MAE train: 2.449562	val: 5.872017	test: 5.416097

Epoch: 22
Loss: 7.112719774246216Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:3
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/freesolv/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/freesolv/noise=0.2/freesolv_scaff_6_26-05_11-28-50  ]
[ Using Seed :  6  ]
[ Using device :  cuda:3  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 20.99567222595215
RMSE train: 4.458073	val: 8.512903	test: 6.747856
MAE train: 3.548206	val: 6.578713	test: 5.851432

Epoch: 2
Loss: 20.294854164123535
RMSE train: 4.361411	val: 8.457694	test: 6.647254
MAE train: 3.463020	val: 6.504068	test: 5.745443

Epoch: 3
Loss: 19.576236724853516
RMSE train: 4.333641	val: 8.400448	test: 6.640333
MAE train: 3.444454	val: 6.424275	test: 5.728381

Epoch: 4
Loss: 19.005942344665527
RMSE train: 4.301407	val: 8.325437	test: 6.609246
MAE train: 3.426463	val: 6.341549	test: 5.686960

Epoch: 5
Loss: 18.42504072189331
RMSE train: 4.248276	val: 8.241075	test: 6.573764
MAE train: 3.391654	val: 6.261703	test: 5.645805

Epoch: 6
Loss: 17.839646339416504
RMSE train: 4.195812	val: 8.146928	test: 6.537401
MAE train: 3.362710	val: 6.194315	test: 5.609024

Epoch: 7
Loss: 17.08868980407715
RMSE train: 4.148299	val: 8.047266	test: 6.497314
MAE train: 3.344563	val: 6.142245	test: 5.578129

Epoch: 8
Loss: 16.027822494506836
RMSE train: 4.090480	val: 7.903273	test: 6.445055
MAE train: 3.315078	val: 6.070237	test: 5.526392

Epoch: 9
Loss: 15.284790992736816
RMSE train: 4.024951	val: 7.717475	test: 6.361161
MAE train: 3.278891	val: 5.978239	test: 5.435894

Epoch: 10
Loss: 14.553686618804932
RMSE train: 3.950870	val: 7.509159	test: 6.287027
MAE train: 3.238521	val: 5.894184	test: 5.357386

Epoch: 11
Loss: 13.769781589508057
RMSE train: 3.872436	val: 7.271086	test: 6.231874
MAE train: 3.197915	val: 5.797601	test: 5.297209

Epoch: 12
Loss: 13.277435302734375
RMSE train: 3.808128	val: 7.008828	test: 6.115658
MAE train: 3.169022	val: 5.649032	test: 5.191654

Epoch: 13
Loss: 12.3962984085083
RMSE train: 3.754145	val: 6.826190	test: 5.980634
MAE train: 3.143154	val: 5.520473	test: 5.082738

Epoch: 14
Loss: 11.931243896484375
RMSE train: 3.707352	val: 6.674704	test: 5.809906
MAE train: 3.121014	val: 5.370397	test: 4.944265

Epoch: 15
Loss: 11.593873500823975
RMSE train: 3.683811	val: 6.575327	test: 5.647710
MAE train: 3.119025	val: 5.268170	test: 4.804644

Epoch: 16
Loss: 10.954574584960938
RMSE train: 3.672432	val: 6.478022	test: 5.584599
MAE train: 3.131007	val: 5.177580	test: 4.760678

Epoch: 17
Loss: 10.299197673797607
RMSE train: 3.645093	val: 6.381528	test: 5.525337
MAE train: 3.122836	val: 5.096295	test: 4.720126

Epoch: 18
Loss: 9.874513626098633
RMSE train: 3.627151	val: 6.329037	test: 5.497305
MAE train: 3.121302	val: 5.042507	test: 4.709608

Epoch: 19
Loss: 9.420880794525146
RMSE train: 3.579641	val: 6.339576	test: 5.576870
MAE train: 3.086153	val: 5.081607	test: 4.814424

Epoch: 20
Loss: 8.765500545501709
RMSE train: 3.554879	val: 6.383454	test: 5.679731
MAE train: 3.074474	val: 5.149828	test: 4.932755

Epoch: 21
Loss: 8.701054811477661
RMSE train: 3.527100	val: 6.395619	test: 5.750151
MAE train: 3.059895	val: 5.232847	test: 5.017452

Epoch: 22
Loss: 8.102787494659424Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/freesolv/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/freesolv/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/freesolv/noise=0.05/freesolv_scaff_5_26-05_11-28-50  ]
[ Using Seed :  5  ]
[ Using device :  cuda:1  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 20.957051277160645
RMSE train: 4.479456	val: 8.642880	test: 6.652674
MAE train: 3.543321	val: 6.615619	test: 5.712506

Epoch: 2
Loss: 20.091629028320312
RMSE train: 4.538782	val: 8.599193	test: 6.684289
MAE train: 3.629783	val: 6.558333	test: 5.748968

Epoch: 3
Loss: 19.002488136291504
RMSE train: 4.524354	val: 8.525276	test: 6.679868
MAE train: 3.639273	val: 6.495210	test: 5.760105

Epoch: 4
Loss: 18.542574882507324
RMSE train: 4.468811	val: 8.434351	test: 6.654699
MAE train: 3.606392	val: 6.426598	test: 5.746820

Epoch: 5
Loss: 17.326831817626953
RMSE train: 4.383755	val: 8.307985	test: 6.577176
MAE train: 3.542462	val: 6.323015	test: 5.680291

Epoch: 6
Loss: 16.735941410064697
RMSE train: 4.304548	val: 8.168419	test: 6.515290
MAE train: 3.490434	val: 6.237125	test: 5.636206

Epoch: 7
Loss: 15.682485103607178
RMSE train: 4.185865	val: 8.012349	test: 6.453844
MAE train: 3.408847	val: 6.196461	test: 5.595670

Epoch: 8
Loss: 14.778080940246582
RMSE train: 4.047619	val: 7.847216	test: 6.373753
MAE train: 3.313745	val: 6.150307	test: 5.530491

Epoch: 9
Loss: 14.169062614440918
RMSE train: 3.894276	val: 7.674571	test: 6.307575
MAE train: 3.202529	val: 6.099011	test: 5.477271

Epoch: 10
Loss: 13.342880249023438
RMSE train: 3.723452	val: 7.508272	test: 6.255258
MAE train: 3.076125	val: 6.026947	test: 5.428906

Epoch: 11
Loss: 12.925395965576172
RMSE train: 3.571143	val: 7.360391	test: 6.209799
MAE train: 2.969694	val: 5.954878	test: 5.386363

Epoch: 12
Loss: 11.979902744293213
RMSE train: 3.442687	val: 7.218059	test: 6.192734
MAE train: 2.876892	val: 5.874271	test: 5.360121

Epoch: 13
Loss: 10.936127185821533
RMSE train: 3.324573	val: 7.094199	test: 6.216660
MAE train: 2.776525	val: 5.799859	test: 5.378933

Epoch: 14
Loss: 10.477093696594238
RMSE train: 3.238036	val: 7.010044	test: 6.224374
MAE train: 2.695443	val: 5.742385	test: 5.397117

Epoch: 15
Loss: 10.040992736816406
RMSE train: 3.186748	val: 6.961779	test: 6.225177
MAE train: 2.660202	val: 5.705689	test: 5.399336

Epoch: 16
Loss: 9.755542278289795
RMSE train: 3.152801	val: 6.944087	test: 6.242712
MAE train: 2.643541	val: 5.700030	test: 5.419448

Epoch: 17
Loss: 8.952463626861572
RMSE train: 3.105918	val: 6.922518	test: 6.254888
MAE train: 2.619604	val: 5.703316	test: 5.451361

Epoch: 18
Loss: 8.20862340927124
RMSE train: 3.056795	val: 6.887688	test: 6.244325
MAE train: 2.591454	val: 5.707146	test: 5.465838

Epoch: 19
Loss: 8.583383560180664
RMSE train: 3.014911	val: 6.861085	test: 6.229990
MAE train: 2.574621	val: 5.720992	test: 5.480437

Epoch: 20
Loss: 7.8109235763549805
RMSE train: 2.952134	val: 6.783719	test: 6.174414
MAE train: 2.527093	val: 5.678459	test: 5.443036

Epoch: 21
Loss: 7.277909755706787
RMSE train: 2.873130	val: 6.652690	test: 6.096957
MAE train: 2.463328	val: 5.590606	test: 5.366734

Epoch: 22
Loss: 6.946219444274902

Epoch: 23
Loss: 6.2295520305633545
RMSE train: 2.883818	val: 5.729897	test: 5.138186
MAE train: 2.523918	val: 4.746726	test: 4.485843

Epoch: 24
Loss: 5.827056646347046
RMSE train: 2.748270	val: 5.667538	test: 5.022922
MAE train: 2.394578	val: 4.627557	test: 4.319133

Epoch: 25
Loss: 5.4555840492248535
RMSE train: 2.700415	val: 5.655458	test: 4.972557
MAE train: 2.364895	val: 4.585201	test: 4.237900

Epoch: 26
Loss: 4.9831438064575195
RMSE train: 2.676258	val: 5.637836	test: 4.934449
MAE train: 2.350969	val: 4.575460	test: 4.183911

Epoch: 27
Loss: 5.014128923416138
RMSE train: 2.665073	val: 5.622144	test: 4.906204
MAE train: 2.347473	val: 4.573163	test: 4.148678

Epoch: 28
Loss: 4.699386835098267
RMSE train: 2.620161	val: 5.541268	test: 4.847285
MAE train: 2.307577	val: 4.499492	test: 4.090925

Epoch: 29
Loss: 4.384002208709717
RMSE train: 2.565230	val: 5.459047	test: 4.770765
MAE train: 2.256456	val: 4.417140	test: 4.019350

Epoch: 30
Loss: 4.133908152580261
RMSE train: 2.481967	val: 5.365989	test: 4.698031
MAE train: 2.167371	val: 4.310161	test: 3.958605

Epoch: 31
Loss: 3.5576295852661133
RMSE train: 2.393838	val: 5.265307	test: 4.620144
MAE train: 2.083656	val: 4.185652	test: 3.881291

Epoch: 32
Loss: 3.733706831932068
RMSE train: 2.312798	val: 5.188370	test: 4.553297
MAE train: 2.010118	val: 4.077526	test: 3.815569

Epoch: 33
Loss: 3.5111621618270874
RMSE train: 2.212498	val: 5.109409	test: 4.443206
MAE train: 1.905302	val: 3.967689	test: 3.706983

Epoch: 34
Loss: 3.155854821205139
RMSE train: 2.149221	val: 5.075424	test: 4.387325
MAE train: 1.838752	val: 3.924535	test: 3.661140

Epoch: 35
Loss: 3.036364793777466
RMSE train: 2.066041	val: 5.023133	test: 4.328907
MAE train: 1.761477	val: 3.862466	test: 3.598273

Epoch: 36
Loss: 2.839109778404236
RMSE train: 1.969564	val: 4.972436	test: 4.284335
MAE train: 1.665615	val: 3.799561	test: 3.549053

Epoch: 37
Loss: 2.6479926109313965
RMSE train: 1.904447	val: 4.971157	test: 4.286392
MAE train: 1.602517	val: 3.778333	test: 3.554130

Epoch: 38
Loss: 2.710817337036133
RMSE train: 1.818374	val: 4.957146	test: 4.254985
MAE train: 1.523773	val: 3.768035	test: 3.518511

Epoch: 39
Loss: 2.7000975608825684
RMSE train: 1.701998	val: 4.885803	test: 4.169830
MAE train: 1.413704	val: 3.672938	test: 3.415093

Epoch: 40
Loss: 2.1364697217941284
RMSE train: 1.596220	val: 4.798146	test: 4.076056
MAE train: 1.302787	val: 3.568783	test: 3.296306

Epoch: 41
Loss: 2.1266562938690186
RMSE train: 1.502173	val: 4.677239	test: 3.946662
MAE train: 1.197737	val: 3.386812	test: 3.133425

Epoch: 42
Loss: 2.015355348587036
RMSE train: 1.443113	val: 4.575941	test: 3.832884
MAE train: 1.130776	val: 3.242643	test: 2.992409

Epoch: 43
Loss: 1.8620067834854126
RMSE train: 1.436065	val: 4.583997	test: 3.809893
MAE train: 1.125557	val: 3.241959	test: 2.957479

Epoch: 44
Loss: 1.6840745210647583
RMSE train: 1.455001	val: 4.628892	test: 3.832310
MAE train: 1.148141	val: 3.317475	test: 2.995328

Epoch: 45
Loss: 1.7036153078079224
RMSE train: 1.479849	val: 4.675838	test: 3.872754
MAE train: 1.190187	val: 3.416376	test: 3.056484

Epoch: 46
Loss: 1.4467818140983582
RMSE train: 1.484271	val: 4.672112	test: 3.858949
MAE train: 1.197749	val: 3.416405	test: 3.043689

Epoch: 47
Loss: 1.738086998462677
RMSE train: 1.413801	val: 4.599885	test: 3.782445
MAE train: 1.107691	val: 3.300931	test: 2.962292

Epoch: 48
Loss: 1.4200546741485596
RMSE train: 1.389414	val: 4.544866	test: 3.759670
MAE train: 1.070961	val: 3.214123	test: 2.931642

Epoch: 49
Loss: 1.506624162197113
RMSE train: 1.335677	val: 4.456120	test: 3.732183
MAE train: 1.020296	val: 3.161223	test: 2.894948

Epoch: 50
Loss: 1.3147403001785278
RMSE train: 1.275646	val: 4.351511	test: 3.684571
MAE train: 0.966410	val: 3.091702	test: 2.845372

Epoch: 51
Loss: 1.4169074892997742
RMSE train: 1.238824	val: 4.274095	test: 3.674006
MAE train: 0.939712	val: 3.040491	test: 2.828258

Epoch: 52
Loss: 1.3834784030914307
RMSE train: 1.220217	val: 4.278883	test: 3.666382
MAE train: 0.930077	val: 3.096417	test: 2.820753

Epoch: 53
Loss: 1.4025956392288208
RMSE train: 1.216476	val: 4.318059	test: 3.675059
MAE train: 0.935899	val: 3.187162	test: 2.848275

Epoch: 54
Loss: 1.2553369998931885
RMSE train: 1.149468	val: 4.285145	test: 3.588574
MAE train: 0.886093	val: 3.141819	test: 2.775375

Epoch: 55
Loss: 1.3927710056304932
RMSE train: 1.079712	val: 4.215325	test: 3.486361
MAE train: 0.828521	val: 3.014379	test: 2.655459

Epoch: 56
Loss: 1.208648920059204
RMSE train: 1.035427	val: 4.162114	test: 3.422084
MAE train: 0.790609	val: 2.913164	test: 2.567477

Epoch: 57
Loss: 1.2904411554336548
RMSE train: 1.028815	val: 4.132367	test: 3.416990
MAE train: 0.785226	val: 2.848101	test: 2.551413

Epoch: 58
Loss: 1.2922952771186829
RMSE train: 0.994288	val: 4.096898	test: 3.371190
MAE train: 0.753105	val: 2.808481	test: 2.514018

Epoch: 59
Loss: 1.2402718663215637
RMSE train: 0.961112	val: 4.093754	test: 3.334112
MAE train: 0.715565	val: 2.804472	test: 2.473938

Epoch: 60
Loss: 1.2331586480140686
RMSE train: 0.915669	val: 4.055396	test: 3.287959
MAE train: 0.685195	val: 2.775699	test: 2.427181

Epoch: 61
Loss: 1.0517942011356354
RMSE train: 0.910050	val: 4.071454	test: 3.265933
MAE train: 0.683527	val: 2.808347	test: 2.394826

Epoch: 62
Loss: 1.144604116678238
RMSE train: 0.957458	val: 4.114566	test: 3.277442
MAE train: 0.716707	val: 2.894279	test: 2.410725

Epoch: 63
Loss: 1.1269431710243225
RMSE train: 0.986112	val: 4.137068	test: 3.313811
MAE train: 0.731818	val: 2.937124	test: 2.456776

Epoch: 64
Loss: 1.2795524597167969
RMSE train: 1.002017	val: 4.155851	test: 3.382627
MAE train: 0.745436	val: 2.990388	test: 2.512460

Epoch: 65
Loss: 1.1106846928596497
RMSE train: 0.994192	val: 4.155270	test: 3.395695
MAE train: 0.741040	val: 2.979838	test: 2.512512

Epoch: 66
Loss: 1.0641924738883972
RMSE train: 0.945313	val: 4.142337	test: 3.367004
MAE train: 0.709712	val: 2.913682	test: 2.481061

Epoch: 67
Loss: 1.1619502305984497
RMSE train: 0.910752	val: 4.130415	test: 3.333688
MAE train: 0.687752	val: 2.885633	test: 2.455294

Epoch: 68
Loss: 1.1237998008728027
RMSE train: 0.890243	val: 4.103971	test: 3.318692
MAE train: 0.675885	val: 2.859951	test: 2.446091

Epoch: 69
Loss: 1.117285430431366
RMSE train: 0.918377	val: 4.143994	test: 3.326475
MAE train: 0.695181	val: 2.940325	test: 2.451225

Epoch: 70
Loss: 1.176331877708435
RMSE train: 1.001501	val: 4.264275	test: 3.383945
MAE train: 0.750703	val: 3.088116	test: 2.502103

Epoch: 71
Loss: 1.10933518409729
RMSE train: 1.015343	val: 4.327239	test: 3.406155
MAE train: 0.751779	val: 3.136107	test: 2.535044

Epoch: 72
Loss: 1.0956470370292664
RMSE train: 0.949856	val: 4.235016	test: 3.347440
MAE train: 0.706989	val: 3.018270	test: 2.476229

Epoch: 73
Loss: 0.9712754487991333
RMSE train: 0.869322	val: 4.103241	test: 3.301156
MAE train: 0.661044	val: 2.878783	test: 2.429466

Epoch: 74
Loss: 1.1790570616722107
RMSE train: 0.828446	val: 4.026685	test: 3.299333
MAE train: 0.633977	val: 2.758832	test: 2.430325

Epoch: 75
Loss: 1.0761430263519287
RMSE train: 0.788990	val: 3.987669	test: 3.307133
MAE train: 0.600710	val: 2.677369	test: 2.437034

Epoch: 76
Loss: 1.0441125631332397
RMSE train: 0.804489	val: 4.012643	test: 3.341510
MAE train: 0.609348	val: 2.706356	test: 2.468949

Epoch: 77
Loss: 1.1712546944618225
RMSE train: 0.834432	val: 4.068019	test: 3.339962
MAE train: 0.625326	val: 2.785882	test: 2.472156

Epoch: 78
Loss: 0.8812737166881561
RMSE train: 0.882552	val: 4.129233	test: 3.325099
MAE train: 0.638970	val: 2.849211	test: 2.468603

Epoch: 79
Loss: 1.1556238532066345
RMSE train: 0.917512	val: 4.148112	test: 3.309804
MAE train: 0.646109	val: 2.869382	test: 2.468937

Epoch: 80
Loss: 1.0066019594669342
RMSE train: 0.909542	val: 4.096580	test: 3.287701
MAE train: 0.639724	val: 2.848749	test: 2.458151

Epoch: 81
Loss: 1.0772752165794373
RMSE train: 0.882199	val: 4.016747	test: 3.275073
MAE train: 0.640151	val: 2.829066	test: 2.440460

Epoch: 82
Loss: 1.3021304607391357
RMSE train: 0.897659	val: 3.998527	test: 3.287241
MAE train: 0.668046	val: 2.828735	test: 2.465687

Epoch: 83
Loss: 0.9374649822711945
RMSE train: 0.888285	val: 3.973014	test: 3.280679
MAE train: 0.668867	val: 2.804429	test: 2.472504
Epoch: 23
Loss: 5.55510687828064
RMSE train: 2.949807	val: 6.111933	test: 5.483517
MAE train: 2.636750	val: 5.185433	test: 4.829938

Epoch: 24
Loss: 5.308077812194824
RMSE train: 2.893317	val: 6.003231	test: 5.334202
MAE train: 2.582304	val: 5.041504	test: 4.687084

Epoch: 25
Loss: 4.957732677459717
RMSE train: 2.839986	val: 5.896540	test: 5.232759
MAE train: 2.537978	val: 4.897856	test: 4.588173

Epoch: 26
Loss: 4.685535669326782
RMSE train: 2.738663	val: 5.783098	test: 5.111931
MAE train: 2.447783	val: 4.749801	test: 4.462477

Epoch: 27
Loss: 4.467269420623779
RMSE train: 2.735820	val: 5.775021	test: 5.044575
MAE train: 2.446210	val: 4.700521	test: 4.389882

Epoch: 28
Loss: 4.233391761779785
RMSE train: 2.754302	val: 5.788929	test: 5.001442
MAE train: 2.453498	val: 4.705184	test: 4.343795

Epoch: 29
Loss: 4.1453611850738525
RMSE train: 2.753752	val: 5.826583	test: 4.957416
MAE train: 2.434729	val: 4.734344	test: 4.297181

Epoch: 30
Loss: 3.793115973472595
RMSE train: 2.612795	val: 5.710300	test: 4.824480
MAE train: 2.298371	val: 4.612214	test: 4.154522

Epoch: 31
Loss: 4.117002010345459
RMSE train: 2.456253	val: 5.552580	test: 4.667607
MAE train: 2.151812	val: 4.449378	test: 3.985966

Epoch: 32
Loss: 3.391049385070801
RMSE train: 2.275303	val: 5.337067	test: 4.497518
MAE train: 1.981227	val: 4.218695	test: 3.794270

Epoch: 33
Loss: 3.16326105594635
RMSE train: 2.116549	val: 5.148368	test: 4.352514
MAE train: 1.831811	val: 4.004325	test: 3.619519

Epoch: 34
Loss: 3.2184526920318604
RMSE train: 2.004559	val: 5.010114	test: 4.267134
MAE train: 1.729312	val: 3.827588	test: 3.509686

Epoch: 35
Loss: 2.9810872077941895
RMSE train: 1.947282	val: 4.966610	test: 4.225572
MAE train: 1.672082	val: 3.759621	test: 3.460287

Epoch: 36
Loss: 2.92247474193573
RMSE train: 1.938409	val: 4.990350	test: 4.226699
MAE train: 1.657483	val: 3.771135	test: 3.466816

Epoch: 37
Loss: 2.4051430225372314
RMSE train: 1.928588	val: 4.995083	test: 4.219296
MAE train: 1.637725	val: 3.782228	test: 3.463469

Epoch: 38
Loss: 2.2390143871307373
RMSE train: 1.893086	val: 4.984152	test: 4.212190
MAE train: 1.597935	val: 3.780192	test: 3.461766

Epoch: 39
Loss: 2.1895272731781006
RMSE train: 1.826408	val: 4.940889	test: 4.200307
MAE train: 1.536768	val: 3.747254	test: 3.464149

Epoch: 40
Loss: 2.239501714706421
RMSE train: 1.702619	val: 4.822658	test: 4.111777
MAE train: 1.416281	val: 3.629114	test: 3.372660

Epoch: 41
Loss: 1.8938352465629578
RMSE train: 1.577781	val: 4.707536	test: 3.988158
MAE train: 1.294102	val: 3.506543	test: 3.243823

Epoch: 42
Loss: 1.8253006339073181
RMSE train: 1.472335	val: 4.599344	test: 3.890637
MAE train: 1.197445	val: 3.399319	test: 3.147173

Epoch: 43
Loss: 2.0268841981887817
RMSE train: 1.411105	val: 4.547590	test: 3.802471
MAE train: 1.134163	val: 3.299159	test: 3.044923

Epoch: 44
Loss: 1.7459917068481445
RMSE train: 1.402831	val: 4.575317	test: 3.758140
MAE train: 1.112689	val: 3.257125	test: 2.969336

Epoch: 45
Loss: 1.7020260691642761
RMSE train: 1.417313	val: 4.610168	test: 3.754308
MAE train: 1.113536	val: 3.248349	test: 2.935374

Epoch: 46
Loss: 1.6197721362113953
RMSE train: 1.414777	val: 4.606749	test: 3.742241
MAE train: 1.107527	val: 3.246348	test: 2.915737

Epoch: 47
Loss: 1.5891155004501343
RMSE train: 1.390950	val: 4.577879	test: 3.708024
MAE train: 1.084786	val: 3.223928	test: 2.869020

Epoch: 48
Loss: 1.7680521607398987
RMSE train: 1.376238	val: 4.565077	test: 3.661718
MAE train: 1.080430	val: 3.216727	test: 2.826209

Epoch: 49
Loss: 1.510767936706543
RMSE train: 1.380625	val: 4.576781	test: 3.633498
MAE train: 1.072770	val: 3.219136	test: 2.805163

Epoch: 50
Loss: 1.4239185452461243
RMSE train: 1.331213	val: 4.507605	test: 3.555982
MAE train: 1.008800	val: 3.139845	test: 2.725407

Epoch: 51
Loss: 1.5096060037612915
RMSE train: 1.281235	val: 4.480984	test: 3.532661
MAE train: 0.964564	val: 3.102554	test: 2.671422

Epoch: 52
Loss: 1.3807722330093384
RMSE train: 1.173324	val: 4.412058	test: 3.481858
MAE train: 0.882549	val: 3.044933	test: 2.601532

Epoch: 53
Loss: 1.453343689441681
RMSE train: 1.088077	val: 4.336718	test: 3.403063
MAE train: 0.818936	val: 2.987618	test: 2.520675

Epoch: 54
Loss: 1.2822590470314026
RMSE train: 1.031702	val: 4.277260	test: 3.338298
MAE train: 0.769901	val: 2.926656	test: 2.460379

Epoch: 55
Loss: 1.4094437956809998
RMSE train: 1.017487	val: 4.290932	test: 3.317824
MAE train: 0.754371	val: 2.953188	test: 2.427842

Epoch: 56
Loss: 1.057865858078003
RMSE train: 0.986102	val: 4.267361	test: 3.299377
MAE train: 0.731052	val: 2.969357	test: 2.402736

Epoch: 57
Loss: 1.33589106798172
RMSE train: 0.944735	val: 4.238200	test: 3.268127
MAE train: 0.696599	val: 2.957183	test: 2.371666

Epoch: 58
Loss: 1.119858741760254
RMSE train: 0.903758	val: 4.198101	test: 3.240171
MAE train: 0.661156	val: 2.927627	test: 2.350302

Epoch: 59
Loss: 1.2544089555740356
RMSE train: 0.913543	val: 4.190799	test: 3.224266
MAE train: 0.669950	val: 2.926631	test: 2.343699

Epoch: 60
Loss: 1.1391041278839111
RMSE train: 0.950486	val: 4.198360	test: 3.229260
MAE train: 0.699973	val: 2.916254	test: 2.346425

Epoch: 61
Loss: 1.137899488210678
RMSE train: 0.981816	val: 4.186801	test: 3.216276
MAE train: 0.720572	val: 2.886635	test: 2.329890

Epoch: 62
Loss: 1.1739715933799744
RMSE train: 1.050557	val: 4.220459	test: 3.215719
MAE train: 0.771821	val: 2.905583	test: 2.324496

Epoch: 63
Loss: 1.250443458557129
RMSE train: 1.035692	val: 4.197650	test: 3.193019
MAE train: 0.768579	val: 2.904618	test: 2.328111

Epoch: 64
Loss: 1.1832681894302368
RMSE train: 0.994116	val: 4.155532	test: 3.185415
MAE train: 0.739692	val: 2.894201	test: 2.338925

Epoch: 65
Loss: 1.0967733263969421
RMSE train: 0.982561	val: 4.171128	test: 3.201916
MAE train: 0.730341	val: 2.881998	test: 2.343612

Epoch: 66
Loss: 1.1181603074073792
RMSE train: 0.942289	val: 4.181481	test: 3.205411
MAE train: 0.696326	val: 2.873235	test: 2.337603

Epoch: 67
Loss: 1.1216444969177246
RMSE train: 0.918416	val: 4.229713	test: 3.208612
MAE train: 0.680272	val: 2.895025	test: 2.334287

Epoch: 68
Loss: 1.0223205983638763
RMSE train: 0.884313	val: 4.255404	test: 3.169879
MAE train: 0.652070	val: 2.886753	test: 2.289382

Epoch: 69
Loss: 1.268353521823883
RMSE train: 0.903155	val: 4.297320	test: 3.173039
MAE train: 0.666647	val: 2.911049	test: 2.292154

Epoch: 70
Loss: 0.9938250482082367
RMSE train: 0.929354	val: 4.312661	test: 3.174677
MAE train: 0.678270	val: 2.911581	test: 2.305419

Epoch: 71
Loss: 1.0941222608089447
RMSE train: 0.949245	val: 4.297548	test: 3.185135
MAE train: 0.693680	val: 2.923432	test: 2.328254

Epoch: 72
Loss: 1.0610862076282501
RMSE train: 0.900042	val: 4.228914	test: 3.144407
MAE train: 0.660763	val: 2.880275	test: 2.292224

Epoch: 73
Loss: 1.0375916063785553
RMSE train: 0.831186	val: 4.150892	test: 3.104002
MAE train: 0.611948	val: 2.834303	test: 2.250109

Epoch: 74
Loss: 0.9185938537120819
RMSE train: 0.774193	val: 4.079881	test: 3.071315
MAE train: 0.562609	val: 2.797708	test: 2.217861

Epoch: 75
Loss: 0.9443470239639282
RMSE train: 0.776878	val: 4.088178	test: 3.071933
MAE train: 0.563995	val: 2.794723	test: 2.208346

Epoch: 76
Loss: 0.912329912185669
RMSE train: 0.783889	val: 4.123855	test: 3.083316
MAE train: 0.571043	val: 2.826598	test: 2.224214

Epoch: 77
Loss: 0.979572057723999
RMSE train: 0.826332	val: 4.201761	test: 3.114291
MAE train: 0.605155	val: 2.883103	test: 2.253853

Epoch: 78
Loss: 1.0355623662471771
RMSE train: 0.865462	val: 4.254281	test: 3.138420
MAE train: 0.638139	val: 2.914116	test: 2.268540

Epoch: 79
Loss: 0.8842407166957855
RMSE train: 0.893341	val: 4.312590	test: 3.169820
MAE train: 0.652225	val: 2.956092	test: 2.291387

Epoch: 80
Loss: 1.0104370713233948
RMSE train: 0.886585	val: 4.307768	test: 3.173025
MAE train: 0.645873	val: 2.957735	test: 2.294360

Epoch: 81
Loss: 1.0613691210746765
RMSE train: 0.848453	val: 4.266214	test: 3.165176
MAE train: 0.617470	val: 2.937242	test: 2.283279

Epoch: 82
Loss: 0.9777794480323792
RMSE train: 0.791202	val: 4.200053	test: 3.127097
MAE train: 0.578547	val: 2.881820	test: 2.244367

Epoch: 83
Loss: 1.023722529411316
RMSE train: 0.757912	val: 4.144910	test: 3.095259
MAE train: 0.561110	val: 2.812812	test: 2.214903

Epoch: 23
Loss: 6.362271308898926
RMSE train: 2.717793	val: 6.363097	test: 4.969854
MAE train: 2.397696	val: 5.447481	test: 4.212621

Epoch: 24
Loss: 6.1163599491119385
RMSE train: 2.629119	val: 6.221631	test: 4.889630
MAE train: 2.314922	val: 5.299016	test: 4.164156

Epoch: 25
Loss: 5.734918117523193
RMSE train: 2.545747	val: 6.110841	test: 4.828816
MAE train: 2.234194	val: 5.185634	test: 4.126297

Epoch: 26
Loss: 5.431489706039429
RMSE train: 2.449895	val: 5.969111	test: 4.769892
MAE train: 2.134599	val: 5.050808	test: 4.077280

Epoch: 27
Loss: 5.228844404220581
RMSE train: 2.374361	val: 5.857623	test: 4.734046
MAE train: 2.058513	val: 4.918337	test: 4.035217

Epoch: 28
Loss: 4.882511138916016
RMSE train: 2.317465	val: 5.824743	test: 4.709477
MAE train: 2.008101	val: 4.861449	test: 4.000955

Epoch: 29
Loss: 4.725240588188171
RMSE train: 2.275304	val: 5.793381	test: 4.681549
MAE train: 1.968488	val: 4.813883	test: 3.960582

Epoch: 30
Loss: 4.475257635116577
RMSE train: 2.256179	val: 5.821932	test: 4.676970
MAE train: 1.951233	val: 4.824493	test: 3.931006

Epoch: 31
Loss: 4.116424322128296
RMSE train: 2.215177	val: 5.801688	test: 4.648733
MAE train: 1.914183	val: 4.823851	test: 3.904670

Epoch: 32
Loss: 3.857132911682129
RMSE train: 2.183767	val: 5.772392	test: 4.613963
MAE train: 1.881945	val: 4.809476	test: 3.873231

Epoch: 33
Loss: 3.5704880952835083
RMSE train: 2.142860	val: 5.724628	test: 4.563339
MAE train: 1.843538	val: 4.802744	test: 3.829206

Epoch: 34
Loss: 3.340163230895996
RMSE train: 2.084588	val: 5.629120	test: 4.478023
MAE train: 1.789253	val: 4.725954	test: 3.750728

Epoch: 35
Loss: 3.2034908533096313
RMSE train: 2.005767	val: 5.483093	test: 4.351229
MAE train: 1.714562	val: 4.549191	test: 3.626751

Epoch: 36
Loss: 3.1387884616851807
RMSE train: 1.948104	val: 5.346688	test: 4.245025
MAE train: 1.657869	val: 4.384324	test: 3.522551

Epoch: 37
Loss: 2.7530341148376465
RMSE train: 1.928165	val: 5.248445	test: 4.173970
MAE train: 1.638112	val: 4.255449	test: 3.446411

Epoch: 38
Loss: 2.722956418991089
RMSE train: 1.902344	val: 5.171696	test: 4.132936
MAE train: 1.610450	val: 4.135877	test: 3.381900

Epoch: 39
Loss: 2.521791100502014
RMSE train: 1.820433	val: 5.056285	test: 4.053218
MAE train: 1.522798	val: 3.927008	test: 3.274317

Epoch: 40
Loss: 2.3203203678131104
RMSE train: 1.723613	val: 4.943991	test: 3.966682
MAE train: 1.421604	val: 3.702988	test: 3.178462

Epoch: 41
Loss: 2.23842191696167
RMSE train: 1.616629	val: 4.850443	test: 3.875037
MAE train: 1.308492	val: 3.512026	test: 3.083875

Epoch: 42
Loss: 2.2855924367904663
RMSE train: 1.574221	val: 4.830886	test: 3.822108
MAE train: 1.258830	val: 3.433842	test: 3.021578

Epoch: 43
Loss: 2.047247350215912
RMSE train: 1.525095	val: 4.757623	test: 3.733110
MAE train: 1.210220	val: 3.339678	test: 2.928043

Epoch: 44
Loss: 2.007465958595276
RMSE train: 1.453456	val: 4.656316	test: 3.626834
MAE train: 1.143044	val: 3.233001	test: 2.816257

Epoch: 45
Loss: 1.8774978518486023
RMSE train: 1.363707	val: 4.515969	test: 3.517953
MAE train: 1.065295	val: 3.108599	test: 2.712909

Epoch: 46
Loss: 1.7666656970977783
RMSE train: 1.330528	val: 4.394977	test: 3.455667
MAE train: 1.041439	val: 3.024292	test: 2.670266

Epoch: 47
Loss: 1.8428065776824951
RMSE train: 1.243587	val: 4.325073	test: 3.421402
MAE train: 0.961751	val: 2.922019	test: 2.637205

Epoch: 48
Loss: 1.6013312935829163
RMSE train: 1.243609	val: 4.375292	test: 3.458098
MAE train: 0.945104	val: 2.910087	test: 2.654108

Epoch: 49
Loss: 1.641206681728363
RMSE train: 1.261152	val: 4.418426	test: 3.487362
MAE train: 0.948489	val: 2.920319	test: 2.666789

Epoch: 50
Loss: 1.4588362574577332
RMSE train: 1.258619	val: 4.407774	test: 3.472431
MAE train: 0.953751	val: 2.938421	test: 2.650961

Epoch: 51
Loss: 1.6755316257476807
RMSE train: 1.235735	val: 4.351762	test: 3.427503
MAE train: 0.935041	val: 2.922066	test: 2.612588

Epoch: 52
Loss: 1.3225252628326416
RMSE train: 1.171711	val: 4.226076	test: 3.350346
MAE train: 0.882431	val: 2.831816	test: 2.552002

Epoch: 53
Loss: 1.2142937183380127
RMSE train: 1.114852	val: 4.152342	test: 3.299431
MAE train: 0.840628	val: 2.772004	test: 2.517193

Epoch: 54
Loss: 1.340893566608429
RMSE train: 1.053787	val: 4.057413	test: 3.228818
MAE train: 0.795963	val: 2.689488	test: 2.448830

Epoch: 55
Loss: 1.2564168572425842
RMSE train: 1.051508	val: 4.058699	test: 3.206732
MAE train: 0.795054	val: 2.664958	test: 2.419026

Epoch: 56
Loss: 1.4925933480262756
RMSE train: 1.064706	val: 4.136404	test: 3.232048
MAE train: 0.812492	val: 2.724682	test: 2.441416

Epoch: 57
Loss: 1.1482272148132324
RMSE train: 1.084478	val: 4.249728	test: 3.263297
MAE train: 0.818408	val: 2.793110	test: 2.472444

Epoch: 58
Loss: 1.144264280796051
RMSE train: 1.069367	val: 4.305525	test: 3.280183
MAE train: 0.791851	val: 2.855118	test: 2.493156

Epoch: 59
Loss: 1.2713833451271057
RMSE train: 1.019849	val: 4.260744	test: 3.252445
MAE train: 0.743941	val: 2.818971	test: 2.464005

Epoch: 60
Loss: 1.2356928586959839
RMSE train: 0.958174	val: 4.154098	test: 3.187108
MAE train: 0.687735	val: 2.741587	test: 2.386532

Epoch: 61
Loss: 1.1712538003921509
RMSE train: 0.901179	val: 4.015833	test: 3.116122
MAE train: 0.636266	val: 2.659130	test: 2.318180

Epoch: 62
Loss: 1.0477384328842163
RMSE train: 0.865568	val: 3.892103	test: 3.049340
MAE train: 0.613610	val: 2.596657	test: 2.283376

Epoch: 63
Loss: 1.2686033248901367
RMSE train: 0.865387	val: 3.846084	test: 3.019306
MAE train: 0.619712	val: 2.572932	test: 2.287231

Epoch: 64
Loss: 1.2852903008460999
RMSE train: 0.899013	val: 3.869728	test: 3.019022
MAE train: 0.653595	val: 2.599920	test: 2.291867

Epoch: 65
Loss: 1.169491171836853
RMSE train: 0.899140	val: 3.902434	test: 3.036488
MAE train: 0.657370	val: 2.625712	test: 2.304433

Epoch: 66
Loss: 1.043643832206726
RMSE train: 0.868287	val: 3.911910	test: 3.055233
MAE train: 0.635990	val: 2.600675	test: 2.310198

Epoch: 67
Loss: 1.158768355846405
RMSE train: 0.841165	val: 3.876958	test: 3.040944
MAE train: 0.617324	val: 2.520947	test: 2.276951

Epoch: 68
Loss: 1.0740822851657867
RMSE train: 0.830803	val: 3.827818	test: 3.008603
MAE train: 0.607619	val: 2.464533	test: 2.268501

Epoch: 69
Loss: 1.3050278425216675
RMSE train: 0.811704	val: 3.799858	test: 3.014962
MAE train: 0.587636	val: 2.459726	test: 2.282082

Epoch: 70
Loss: 1.0403617024421692
RMSE train: 0.834095	val: 3.819404	test: 3.043171
MAE train: 0.603344	val: 2.471913	test: 2.303760

Epoch: 71
Loss: 1.0176002383232117
RMSE train: 0.834129	val: 3.846614	test: 3.062954
MAE train: 0.598241	val: 2.527563	test: 2.335498

Epoch: 72
Loss: 0.9363018572330475
RMSE train: 0.852040	val: 3.893214	test: 3.064869
MAE train: 0.615083	val: 2.600398	test: 2.338685

Epoch: 73
Loss: 1.006162017583847
RMSE train: 0.871143	val: 3.940068	test: 3.051872
MAE train: 0.635439	val: 2.643838	test: 2.324012

Epoch: 74
Loss: 0.9529776275157928
RMSE train: 0.887345	val: 3.987510	test: 3.031906
MAE train: 0.649505	val: 2.687816	test: 2.294806

Epoch: 75
Loss: 0.9873417913913727
RMSE train: 0.856467	val: 4.002642	test: 3.018329
MAE train: 0.630344	val: 2.703920	test: 2.285247

Epoch: 76
Loss: 0.9614226520061493
RMSE train: 0.837944	val: 4.018644	test: 3.011489
MAE train: 0.618452	val: 2.725184	test: 2.277618

Epoch: 77
Loss: 0.8895785510540009
RMSE train: 0.832199	val: 3.994876	test: 2.990762
MAE train: 0.614800	val: 2.725666	test: 2.253927

Epoch: 78
Loss: 1.0109330713748932
RMSE train: 0.841777	val: 3.995211	test: 3.011165
MAE train: 0.635101	val: 2.706842	test: 2.262119

Epoch: 79
Loss: 0.9648341238498688
RMSE train: 0.868217	val: 4.035685	test: 3.042454
MAE train: 0.653490	val: 2.737588	test: 2.269955

Epoch: 80
Loss: 0.9863040447235107
RMSE train: 0.903953	val: 4.074457	test: 3.074184
MAE train: 0.677602	val: 2.754889	test: 2.290261

Epoch: 81
Loss: 0.8444603681564331
RMSE train: 0.940445	val: 4.103582	test: 3.103016
MAE train: 0.708596	val: 2.768909	test: 2.324382

Epoch: 82
Loss: 1.024716705083847
RMSE train: 0.934803	val: 4.103093	test: 3.114718
MAE train: 0.690136	val: 2.786777	test: 2.329794

Epoch: 83
Loss: 1.0074072480201721
RMSE train: 0.898306	val: 4.040355	test: 3.094840
MAE train: 0.650883	val: 2.743306	test: 2.317105
RMSE train: 3.197136	val: 6.962421	test: 6.055935
MAE train: 2.700495	val: 5.773429	test: 5.327767

Epoch: 23
Loss: 7.311582088470459
RMSE train: 3.100522	val: 6.833807	test: 5.977143
MAE train: 2.620269	val: 5.661031	test: 5.265628

Epoch: 24
Loss: 6.884554386138916
RMSE train: 2.935707	val: 6.633239	test: 5.827821
MAE train: 2.474037	val: 5.453164	test: 5.116202

Epoch: 25
Loss: 6.528593063354492
RMSE train: 2.789943	val: 6.435910	test: 5.657732
MAE train: 2.350339	val: 5.238868	test: 4.942328

Epoch: 26
Loss: 6.181164979934692
RMSE train: 2.660638	val: 6.253271	test: 5.526189
MAE train: 2.237356	val: 5.043663	test: 4.806653

Epoch: 27
Loss: 6.008280992507935
RMSE train: 2.560156	val: 6.124563	test: 5.382965
MAE train: 2.140914	val: 4.898190	test: 4.655451

Epoch: 28
Loss: 5.637650728225708
RMSE train: 2.473916	val: 6.043984	test: 5.233603
MAE train: 2.053312	val: 4.804748	test: 4.488937

Epoch: 29
Loss: 5.441941738128662
RMSE train: 2.404118	val: 5.971892	test: 5.106609
MAE train: 1.983377	val: 4.721803	test: 4.350693

Epoch: 30
Loss: 5.0972418785095215
RMSE train: 2.333277	val: 5.907036	test: 5.006122
MAE train: 1.914219	val: 4.644828	test: 4.242077

Epoch: 31
Loss: 4.889366030693054
RMSE train: 2.257920	val: 5.803695	test: 4.887110
MAE train: 1.844273	val: 4.529594	test: 4.117028

Epoch: 32
Loss: 4.5058674812316895
RMSE train: 2.159350	val: 5.634346	test: 4.740864
MAE train: 1.755155	val: 4.342579	test: 3.964390

Epoch: 33
Loss: 4.219721436500549
RMSE train: 2.073389	val: 5.489618	test: 4.632910
MAE train: 1.678323	val: 4.165622	test: 3.864888

Epoch: 34
Loss: 4.010802984237671
RMSE train: 1.986180	val: 5.336772	test: 4.516209
MAE train: 1.603464	val: 3.993195	test: 3.763841

Epoch: 35
Loss: 3.877680778503418
RMSE train: 1.928119	val: 5.234825	test: 4.436328
MAE train: 1.551664	val: 3.889063	test: 3.687218

Epoch: 36
Loss: 3.3774901628494263
RMSE train: 1.897077	val: 5.182890	test: 4.403197
MAE train: 1.520875	val: 3.836531	test: 3.650354

Epoch: 37
Loss: 3.2834584712982178
RMSE train: 1.865474	val: 5.170297	test: 4.346963
MAE train: 1.491371	val: 3.802612	test: 3.577023

Epoch: 38
Loss: 3.300959348678589
RMSE train: 1.819541	val: 5.146200	test: 4.267194
MAE train: 1.449897	val: 3.752541	test: 3.486776

Epoch: 39
Loss: 3.0081307888031006
RMSE train: 1.783683	val: 5.123091	test: 4.208635
MAE train: 1.419023	val: 3.703213	test: 3.422451

Epoch: 40
Loss: 2.7719842195510864
RMSE train: 1.711456	val: 5.087883	test: 4.125203
MAE train: 1.353653	val: 3.625049	test: 3.339892

Epoch: 41
Loss: 2.575748920440674
RMSE train: 1.624035	val: 5.022480	test: 4.036385
MAE train: 1.278205	val: 3.541199	test: 3.250129

Epoch: 42
Loss: 2.89962637424469
RMSE train: 1.548669	val: 4.986438	test: 3.974901
MAE train: 1.210650	val: 3.524582	test: 3.194621

Epoch: 43
Loss: 2.5099871158599854
RMSE train: 1.497229	val: 4.990698	test: 3.882659
MAE train: 1.160765	val: 3.515936	test: 3.092275

Epoch: 44
Loss: 2.296226382255554
RMSE train: 1.436434	val: 4.983466	test: 3.797387
MAE train: 1.108676	val: 3.473211	test: 2.992979

Epoch: 45
Loss: 2.6413391828536987
RMSE train: 1.320037	val: 4.823026	test: 3.639894
MAE train: 1.015361	val: 3.329908	test: 2.830808

Epoch: 46
Loss: 2.0473544001579285
RMSE train: 1.195384	val: 4.607810	test: 3.481217
MAE train: 0.920543	val: 3.168112	test: 2.679546

Epoch: 47
Loss: 2.3102285861968994
RMSE train: 1.120939	val: 4.451775	test: 3.400148
MAE train: 0.867416	val: 3.084394	test: 2.619590

Epoch: 48
Loss: 1.939444899559021
RMSE train: 1.084449	val: 4.373839	test: 3.377169
MAE train: 0.842356	val: 3.055207	test: 2.619590

Epoch: 49
Loss: 1.9768556952476501
RMSE train: 1.052024	val: 4.330111	test: 3.343555
MAE train: 0.816747	val: 3.029485	test: 2.594219

Epoch: 50
Loss: 2.0251355171203613
RMSE train: 1.046004	val: 4.337958	test: 3.330175
MAE train: 0.810063	val: 3.024559	test: 2.580122

Epoch: 51
Loss: 1.934489667415619
RMSE train: 1.085924	val: 4.447218	test: 3.353280
MAE train: 0.840032	val: 3.082547	test: 2.589637

Epoch: 52
Loss: 1.7875357270240784
RMSE train: 1.102194	val: 4.457451	test: 3.342263
MAE train: 0.854686	val: 3.095440	test: 2.586786

Epoch: 53
Loss: 1.850952923297882
RMSE train: 1.081864	val: 4.357459	test: 3.298851
MAE train: 0.841102	val: 3.037425	test: 2.551121

Epoch: 54
Loss: 1.7139623165130615
RMSE train: 1.009634	val: 4.204470	test: 3.223576
MAE train: 0.782941	val: 2.962627	test: 2.491736

Epoch: 55
Loss: 1.774143636226654
RMSE train: 0.958995	val: 4.098554	test: 3.201232
MAE train: 0.748206	val: 2.932000	test: 2.487906

Epoch: 56
Loss: 1.724791705608368
RMSE train: 0.944047	val: 4.101216	test: 3.219790
MAE train: 0.739921	val: 2.960029	test: 2.505504

Epoch: 57
Loss: 1.7387471199035645
RMSE train: 0.932391	val: 4.131459	test: 3.210269
MAE train: 0.734371	val: 2.953512	test: 2.476877

Epoch: 58
Loss: 1.8351711630821228
RMSE train: 0.924454	val: 4.198172	test: 3.234571
MAE train: 0.722377	val: 2.960011	test: 2.466708

Epoch: 59
Loss: 1.7695621252059937
RMSE train: 0.932746	val: 4.249507	test: 3.269761
MAE train: 0.719638	val: 2.963131	test: 2.482156

Epoch: 60
Loss: 1.6328150629997253
RMSE train: 0.923320	val: 4.223016	test: 3.266558
MAE train: 0.705263	val: 2.971443	test: 2.481693

Epoch: 61
Loss: 1.6886368989944458
RMSE train: 0.883212	val: 4.096350	test: 3.235676
MAE train: 0.676581	val: 2.829698	test: 2.480444

Epoch: 62
Loss: 1.510016918182373
RMSE train: 0.875213	val: 4.014310	test: 3.225354
MAE train: 0.674959	val: 2.762106	test: 2.487096

Epoch: 63
Loss: 1.4552860260009766
RMSE train: 0.893262	val: 4.006887	test: 3.223152
MAE train: 0.693396	val: 2.744901	test: 2.486741

Epoch: 64
Loss: 1.5065963864326477
RMSE train: 0.900233	val: 4.024815	test: 3.228784
MAE train: 0.704050	val: 2.764673	test: 2.494203

Epoch: 65
Loss: 1.403496503829956
RMSE train: 0.923119	val: 4.101984	test: 3.260433
MAE train: 0.726847	val: 2.832195	test: 2.535450

Epoch: 66
Loss: 1.3774654269218445
RMSE train: 0.982230	val: 4.267986	test: 3.341991
MAE train: 0.774776	val: 2.961210	test: 2.622650

Epoch: 67
Loss: 1.2878102660179138
RMSE train: 0.973886	val: 4.338408	test: 3.354185
MAE train: 0.764323	val: 3.008477	test: 2.636100

Epoch: 68
Loss: 1.4465389251708984
RMSE train: 0.937627	val: 4.362797	test: 3.350551
MAE train: 0.728988	val: 3.026267	test: 2.632664

Epoch: 69
Loss: 1.5416773557662964
RMSE train: 0.871000	val: 4.301852	test: 3.322517
MAE train: 0.675104	val: 2.985273	test: 2.599671

Epoch: 70
Loss: 1.1782300472259521
RMSE train: 0.808319	val: 4.156239	test: 3.269007
MAE train: 0.624743	val: 2.896361	test: 2.541206

Epoch: 71
Loss: 1.4476848244667053
RMSE train: 0.798538	val: 4.115251	test: 3.252259
MAE train: 0.615833	val: 2.863771	test: 2.521306

Epoch: 72
Loss: 1.3524603247642517
RMSE train: 0.805010	val: 4.145336	test: 3.277467
MAE train: 0.629922	val: 2.872500	test: 2.539568

Epoch: 73
Loss: 1.3694247007369995
RMSE train: 0.857243	val: 4.274322	test: 3.341359
MAE train: 0.671012	val: 2.961663	test: 2.591288

Epoch: 74
Loss: 1.293740153312683
RMSE train: 0.906825	val: 4.350254	test: 3.343706
MAE train: 0.703428	val: 3.001627	test: 2.583942

Epoch: 75
Loss: 1.2972967028617859
RMSE train: 0.900055	val: 4.323111	test: 3.303349
MAE train: 0.694375	val: 2.990427	test: 2.547491

Epoch: 76
Loss: 1.359893560409546
RMSE train: 0.842942	val: 4.204564	test: 3.217521
MAE train: 0.654663	val: 2.912114	test: 2.478692

Epoch: 77
Loss: 1.2377433776855469
RMSE train: 0.760041	val: 4.090365	test: 3.130877
MAE train: 0.604356	val: 2.828550	test: 2.399624

Epoch: 78
Loss: 1.2230948209762573
RMSE train: 0.726968	val: 4.020106	test: 3.051201
MAE train: 0.583530	val: 2.754159	test: 2.313306

Epoch: 79
Loss: 1.081945776939392
RMSE train: 0.744535	val: 4.022756	test: 3.045429
MAE train: 0.599256	val: 2.737127	test: 2.290387

Epoch: 80
Loss: 1.228119969367981
RMSE train: 0.794404	val: 4.079554	test: 3.138188
MAE train: 0.642337	val: 2.786962	test: 2.396274

Epoch: 81
Loss: 1.180582582950592
RMSE train: 0.826062	val: 4.100405	test: 3.246502
MAE train: 0.670490	val: 2.821318	test: 2.518405

Epoch: 82
Loss: 1.0035608112812042
RMSE train: 0.871263	val: 4.158577	test: 3.397783
MAE train: 0.707470	val: 2.877083	test: 2.681879

Epoch: 83
Loss: 1.1327043175697327
RMSE train: 2.986443	val: 6.914632	test: 6.035790
MAE train: 2.469276	val: 5.557032	test: 5.273608

Epoch: 23
Loss: 7.979865550994873
RMSE train: 2.841479	val: 6.803968	test: 5.991215
MAE train: 2.345633	val: 5.497255	test: 5.241161

Epoch: 24
Loss: 7.4818642139434814
RMSE train: 2.690139	val: 6.686861	test: 5.951635
MAE train: 2.214724	val: 5.414748	test: 5.212376

Epoch: 25
Loss: 7.045503377914429
RMSE train: 2.573738	val: 6.612265	test: 5.925741
MAE train: 2.115877	val: 5.348544	test: 5.187145

Epoch: 26
Loss: 6.408079385757446
RMSE train: 2.486333	val: 6.570548	test: 5.912519
MAE train: 2.040300	val: 5.314313	test: 5.175498

Epoch: 27
Loss: 6.341146230697632
RMSE train: 2.394437	val: 6.509595	test: 5.818071
MAE train: 1.951180	val: 5.242050	test: 5.073973

Epoch: 28
Loss: 6.0476744174957275
RMSE train: 2.312261	val: 6.424131	test: 5.712195
MAE train: 1.877038	val: 5.139754	test: 4.954091

Epoch: 29
Loss: 5.570681095123291
RMSE train: 2.272458	val: 6.378992	test: 5.588385
MAE train: 1.849011	val: 5.084380	test: 4.816380

Epoch: 30
Loss: 5.104352712631226
RMSE train: 2.199505	val: 6.279573	test: 5.437715
MAE train: 1.788260	val: 5.003462	test: 4.655777

Epoch: 31
Loss: 5.123892545700073
RMSE train: 2.126802	val: 6.209177	test: 5.361431
MAE train: 1.724986	val: 4.948059	test: 4.577631

Epoch: 32
Loss: 4.875970840454102
RMSE train: 2.069858	val: 6.150753	test: 5.299780
MAE train: 1.678555	val: 4.907541	test: 4.519021

Epoch: 33
Loss: 4.390486001968384
RMSE train: 1.960071	val: 6.021538	test: 5.200703
MAE train: 1.583033	val: 4.792802	test: 4.422595

Epoch: 34
Loss: 4.384274482727051
RMSE train: 1.881629	val: 5.899367	test: 5.146641
MAE train: 1.515769	val: 4.684913	test: 4.375509

Epoch: 35
Loss: 4.1475337743759155
RMSE train: 1.807036	val: 5.775689	test: 5.086416
MAE train: 1.446888	val: 4.568470	test: 4.320652

Epoch: 36
Loss: 3.7360132932662964
RMSE train: 1.732743	val: 5.646965	test: 4.994704
MAE train: 1.379126	val: 4.440161	test: 4.225008

Epoch: 37
Loss: 3.7163548469543457
RMSE train: 1.705368	val: 5.612591	test: 4.973157
MAE train: 1.357963	val: 4.405813	test: 4.204410

Epoch: 38
Loss: 3.4649386405944824
RMSE train: 1.692346	val: 5.623225	test: 4.956679
MAE train: 1.345126	val: 4.420069	test: 4.185005

Epoch: 39
Loss: 3.335868716239929
RMSE train: 1.637399	val: 5.583168	test: 4.909282
MAE train: 1.293056	val: 4.373435	test: 4.131286

Epoch: 40
Loss: 3.0361329317092896
RMSE train: 1.531593	val: 5.480070	test: 4.788147
MAE train: 1.197699	val: 4.214055	test: 3.991768

Epoch: 41
Loss: 2.875113606452942
RMSE train: 1.400307	val: 5.304104	test: 4.584993
MAE train: 1.079594	val: 3.943669	test: 3.768527

Epoch: 42
Loss: 2.906938910484314
RMSE train: 1.318099	val: 5.192690	test: 4.429399
MAE train: 1.005858	val: 3.774511	test: 3.617194

Epoch: 43
Loss: 2.8669434785842896
RMSE train: 1.276464	val: 5.108392	test: 4.336395
MAE train: 0.972297	val: 3.633481	test: 3.526011

Epoch: 44
Loss: 2.8911837339401245
RMSE train: 1.277968	val: 5.077524	test: 4.317541
MAE train: 0.978366	val: 3.577379	test: 3.511687

Epoch: 45
Loss: 2.5797001123428345
RMSE train: 1.290760	val: 5.116992	test: 4.378491
MAE train: 0.995819	val: 3.599170	test: 3.565143

Epoch: 46
Loss: 2.340204954147339
RMSE train: 1.290395	val: 5.177481	test: 4.451928
MAE train: 0.997593	val: 3.653641	test: 3.634404

Epoch: 47
Loss: 2.3169963359832764
RMSE train: 1.289453	val: 5.265952	test: 4.501365
MAE train: 0.995949	val: 3.715157	test: 3.681348

Epoch: 48
Loss: 2.3584547638893127
RMSE train: 1.259784	val: 5.315377	test: 4.476761
MAE train: 0.970587	val: 3.724472	test: 3.647786

Epoch: 49
Loss: 2.2747896909713745
RMSE train: 1.221210	val: 5.311693	test: 4.388982
MAE train: 0.939920	val: 3.693085	test: 3.554170

Epoch: 50
Loss: 1.8868741989135742
RMSE train: 1.145456	val: 5.252277	test: 4.259571
MAE train: 0.878232	val: 3.627006	test: 3.427921

Epoch: 51
Loss: 2.001405417919159
RMSE train: 1.090129	val: 5.175029	test: 4.134357
MAE train: 0.836727	val: 3.550251	test: 3.307225

Epoch: 52
Loss: 2.113597869873047
RMSE train: 1.047992	val: 5.049887	test: 4.003702
MAE train: 0.800155	val: 3.450869	test: 3.180564

Epoch: 53
Loss: 1.886601209640503
RMSE train: 1.013177	val: 4.931372	test: 3.896307
MAE train: 0.774491	val: 3.374892	test: 3.068918

Epoch: 54
Loss: 2.1645343899726868
RMSE train: 1.009848	val: 4.870869	test: 3.837278
MAE train: 0.770406	val: 3.338863	test: 3.008369

Epoch: 55
Loss: 1.799598515033722
RMSE train: 0.974019	val: 4.854136	test: 3.799160
MAE train: 0.742203	val: 3.323689	test: 2.969477

Epoch: 56
Loss: 1.7584362626075745
RMSE train: 0.935911	val: 4.867976	test: 3.796914
MAE train: 0.712335	val: 3.314464	test: 2.963921

Epoch: 57
Loss: 1.9913510084152222
RMSE train: 0.913730	val: 4.911959	test: 3.828328
MAE train: 0.695446	val: 3.310415	test: 2.991516

Epoch: 58
Loss: 1.7107165455818176
RMSE train: 0.917253	val: 4.989357	test: 3.895246
MAE train: 0.698240	val: 3.344138	test: 3.050979

Epoch: 59
Loss: 1.673166036605835
RMSE train: 0.919345	val: 5.050687	test: 3.950692
MAE train: 0.700805	val: 3.374729	test: 3.097523

Epoch: 60
Loss: 1.626810610294342
RMSE train: 0.923321	val: 5.078660	test: 3.990222
MAE train: 0.707863	val: 3.405980	test: 3.135072

Epoch: 61
Loss: 1.5762810111045837
RMSE train: 0.909065	val: 5.068878	test: 4.010940
MAE train: 0.696975	val: 3.407735	test: 3.159489

Epoch: 62
Loss: 1.4531501531600952
RMSE train: 0.883852	val: 5.010144	test: 3.980812
MAE train: 0.676959	val: 3.357834	test: 3.135206

Epoch: 63
Loss: 1.529110610485077
RMSE train: 0.867724	val: 4.959900	test: 3.960576
MAE train: 0.665787	val: 3.323492	test: 3.119724

Epoch: 64
Loss: 1.6482199430465698
RMSE train: 0.876234	val: 4.951481	test: 3.975546
MAE train: 0.675142	val: 3.318331	test: 3.135103

Epoch: 65
Loss: 1.59456467628479
RMSE train: 0.874414	val: 4.925625	test: 3.975267
MAE train: 0.675975	val: 3.294498	test: 3.138253

Epoch: 66
Loss: 1.5538015961647034
RMSE train: 0.861720	val: 4.885177	test: 3.976787
MAE train: 0.664799	val: 3.272246	test: 3.150003

Epoch: 67
Loss: 1.4338876008987427
RMSE train: 0.841726	val: 4.830951	test: 3.956047
MAE train: 0.645936	val: 3.241595	test: 3.145249

Epoch: 68
Loss: 1.590848684310913
RMSE train: 0.838701	val: 4.819950	test: 3.948240
MAE train: 0.643948	val: 3.240348	test: 3.145762

Epoch: 69
Loss: 1.3787450790405273
RMSE train: 0.832726	val: 4.834067	test: 3.923143
MAE train: 0.642247	val: 3.265955	test: 3.120630

Epoch: 70
Loss: 1.3261998295783997
RMSE train: 0.812063	val: 4.820681	test: 3.882309
MAE train: 0.628112	val: 3.278576	test: 3.074001

Epoch: 71
Loss: 1.299941062927246
RMSE train: 0.797519	val: 4.816582	test: 3.835933
MAE train: 0.619832	val: 3.283301	test: 3.020502

Epoch: 72
Loss: 1.3316627740859985
RMSE train: 0.805497	val: 4.842050	test: 3.847820
MAE train: 0.628354	val: 3.307415	test: 3.028868

Epoch: 73
Loss: 1.2347212433815002
RMSE train: 0.863527	val: 4.913028	test: 3.911399
MAE train: 0.675488	val: 3.338891	test: 3.089857

Epoch: 74
Loss: 1.34672349691391
RMSE train: 0.935620	val: 5.020051	test: 3.993219
MAE train: 0.734255	val: 3.398656	test: 3.161012

Epoch: 75
Loss: 1.2670466303825378
RMSE train: 0.974251	val: 5.107066	test: 4.060287
MAE train: 0.763662	val: 3.463712	test: 3.218690

Epoch: 76
Loss: 1.2804575562477112
RMSE train: 0.943429	val: 5.117167	test: 4.108465
MAE train: 0.742195	val: 3.454744	test: 3.263602

Epoch: 77
Loss: 1.2810136675834656
RMSE train: 0.875748	val: 5.100193	test: 4.130062
MAE train: 0.688468	val: 3.409203	test: 3.275255

Epoch: 78
Loss: 1.111977756023407
RMSE train: 0.866261	val: 5.142597	test: 4.189965
MAE train: 0.682910	val: 3.431490	test: 3.330278

Epoch: 79
Loss: 1.2214648723602295
RMSE train: 0.896196	val: 5.213136	test: 4.244438
MAE train: 0.707860	val: 3.472411	test: 3.365091

Epoch: 80
Loss: 1.2661151885986328
RMSE train: 0.970328	val: 5.292124	test: 4.245083
MAE train: 0.766212	val: 3.534886	test: 3.346698

Epoch: 81
Loss: 1.1007279753684998
RMSE train: 0.965631	val: 5.285098	test: 4.160677
MAE train: 0.760013	val: 3.525444	test: 3.243858

Epoch: 82
Loss: 1.4168851375579834
RMSE train: 0.866783	val: 5.202466	test: 4.034579
MAE train: 0.680027	val: 3.424563	test: 3.127096

Epoch: 83
Loss: 1.2670745849609375
RMSE train: 3.280992	val: 6.538833	test: 6.669433
MAE train: 2.869056	val: 5.723948	test: 5.993606

Epoch: 23
Loss: 6.629748821258545
RMSE train: 3.247558	val: 6.580205	test: 6.640788
MAE train: 2.837598	val: 5.772331	test: 5.979924

Epoch: 24
Loss: 6.57440972328186
RMSE train: 3.144545	val: 6.488700	test: 6.542734
MAE train: 2.740802	val: 5.685911	test: 5.879524

Epoch: 25
Loss: 6.4629411697387695
RMSE train: 3.053725	val: 6.355708	test: 6.412773
MAE train: 2.661263	val: 5.539871	test: 5.741006

Epoch: 26
Loss: 5.921582460403442
RMSE train: 2.936477	val: 6.133158	test: 6.220856
MAE train: 2.556606	val: 5.290689	test: 5.536585

Epoch: 27
Loss: 5.48642635345459
RMSE train: 2.812795	val: 5.894995	test: 6.026560
MAE train: 2.442943	val: 5.008995	test: 5.326124

Epoch: 28
Loss: 5.252496957778931
RMSE train: 2.706737	val: 5.718348	test: 5.879581
MAE train: 2.342903	val: 4.795434	test: 5.176193

Epoch: 29
Loss: 4.806857109069824
RMSE train: 2.618577	val: 5.582045	test: 5.760755
MAE train: 2.256785	val: 4.630482	test: 5.066874

Epoch: 30
Loss: 4.969316005706787
RMSE train: 2.575138	val: 5.520285	test: 5.671553
MAE train: 2.215144	val: 4.540696	test: 4.991532

Epoch: 31
Loss: 4.430852651596069
RMSE train: 2.499753	val: 5.416161	test: 5.557496
MAE train: 2.145072	val: 4.388387	test: 4.887289

Epoch: 32
Loss: 3.968337297439575
RMSE train: 2.419420	val: 5.343613	test: 5.480734
MAE train: 2.072193	val: 4.287895	test: 4.814879

Epoch: 33
Loss: 3.9212260246276855
RMSE train: 2.344231	val: 5.316240	test: 5.397160
MAE train: 2.001390	val: 4.241764	test: 4.735847

Epoch: 34
Loss: 3.6758627891540527
RMSE train: 2.275179	val: 5.281254	test: 5.308110
MAE train: 1.933667	val: 4.186724	test: 4.643651

Epoch: 35
Loss: 3.3939257860183716
RMSE train: 2.208249	val: 5.221196	test: 5.235536
MAE train: 1.871959	val: 4.138548	test: 4.570315

Epoch: 36
Loss: 3.2439240217208862
RMSE train: 2.153481	val: 5.154849	test: 5.172381
MAE train: 1.824122	val: 4.104788	test: 4.502023

Epoch: 37
Loss: 2.9825682640075684
RMSE train: 2.099485	val: 5.112160	test: 5.114641
MAE train: 1.770528	val: 4.083284	test: 4.434422

Epoch: 38
Loss: 3.061711311340332
RMSE train: 2.000410	val: 4.977084	test: 5.016466
MAE train: 1.670592	val: 3.975870	test: 4.325303

Epoch: 39
Loss: 2.7486836910247803
RMSE train: 1.910356	val: 4.889316	test: 4.917425
MAE train: 1.581490	val: 3.891863	test: 4.218868

Epoch: 40
Loss: 2.643415331840515
RMSE train: 1.834611	val: 4.821606	test: 4.837508
MAE train: 1.506788	val: 3.812035	test: 4.143451

Epoch: 41
Loss: 2.4791259765625
RMSE train: 1.736919	val: 4.777112	test: 4.749916
MAE train: 1.418228	val: 3.736437	test: 4.061974

Epoch: 42
Loss: 2.567488670349121
RMSE train: 1.641033	val: 4.744992	test: 4.691102
MAE train: 1.333257	val: 3.687425	test: 4.001118

Epoch: 43
Loss: 2.428446888923645
RMSE train: 1.542122	val: 4.636058	test: 4.615060
MAE train: 1.243628	val: 3.562539	test: 3.925894

Epoch: 44
Loss: 2.3259732127189636
RMSE train: 1.478785	val: 4.573590	test: 4.542659
MAE train: 1.184086	val: 3.477616	test: 3.849481

Epoch: 45
Loss: 1.9997654557228088
RMSE train: 1.431133	val: 4.518947	test: 4.480480
MAE train: 1.136306	val: 3.420182	test: 3.781400

Epoch: 46
Loss: 2.0731253623962402
RMSE train: 1.377262	val: 4.495439	test: 4.437598
MAE train: 1.084123	val: 3.387774	test: 3.731977

Epoch: 47
Loss: 2.063922166824341
RMSE train: 1.337806	val: 4.457990	test: 4.409449
MAE train: 1.048168	val: 3.357854	test: 3.704604

Epoch: 48
Loss: 1.8687743544578552
RMSE train: 1.302134	val: 4.424244	test: 4.388108
MAE train: 1.022807	val: 3.345568	test: 3.688493

Epoch: 49
Loss: 1.7656018733978271
RMSE train: 1.274302	val: 4.364769	test: 4.369850
MAE train: 1.002755	val: 3.305453	test: 3.681217

Epoch: 50
Loss: 1.8518778085708618
RMSE train: 1.275902	val: 4.401360	test: 4.352408
MAE train: 1.008503	val: 3.329286	test: 3.667730

Epoch: 51
Loss: 1.733600914478302
RMSE train: 1.268973	val: 4.436512	test: 4.321117
MAE train: 0.998886	val: 3.353355	test: 3.641590

Epoch: 52
Loss: 1.6412999629974365
RMSE train: 1.229265	val: 4.396635	test: 4.254259
MAE train: 0.964580	val: 3.326560	test: 3.579437

Epoch: 53
Loss: 1.542451798915863
RMSE train: 1.122189	val: 4.266501	test: 4.151220
MAE train: 0.874020	val: 3.249672	test: 3.500231

Epoch: 54
Loss: 1.5578004121780396
RMSE train: 1.035527	val: 4.150674	test: 4.063032
MAE train: 0.799698	val: 3.185667	test: 3.442241

Epoch: 55
Loss: 1.5088459849357605
RMSE train: 0.997152	val: 4.096604	test: 4.022734
MAE train: 0.765668	val: 3.154898	test: 3.414352

Epoch: 56
Loss: 1.5156526565551758
RMSE train: 0.980231	val: 4.032666	test: 3.989439
MAE train: 0.752574	val: 3.101596	test: 3.390806

Epoch: 57
Loss: 1.5161131620407104
RMSE train: 0.999276	val: 4.039977	test: 3.981929
MAE train: 0.770171	val: 3.065752	test: 3.384730

Epoch: 58
Loss: 1.4611607193946838
RMSE train: 1.031513	val: 4.085527	test: 3.982145
MAE train: 0.798870	val: 3.061737	test: 3.388812

Epoch: 59
Loss: 1.4444294571876526
RMSE train: 1.054341	val: 4.137760	test: 3.998446
MAE train: 0.823214	val: 3.093130	test: 3.409413

Epoch: 60
Loss: 1.2956045866012573
RMSE train: 1.060384	val: 4.183035	test: 4.006036
MAE train: 0.833682	val: 3.119376	test: 3.421411

Epoch: 61
Loss: 1.3869138360023499
RMSE train: 1.060816	val: 4.239843	test: 3.978428
MAE train: 0.834699	val: 3.148598	test: 3.395270

Epoch: 62
Loss: 1.3171619176864624
RMSE train: 1.038091	val: 4.193428	test: 3.968458
MAE train: 0.815316	val: 3.145472	test: 3.392074

Epoch: 63
Loss: 1.415714144706726
RMSE train: 0.990076	val: 4.026680	test: 3.960022
MAE train: 0.774171	val: 3.042114	test: 3.386829

Epoch: 64
Loss: 1.3826900124549866
RMSE train: 0.997142	val: 3.921125	test: 3.967294
MAE train: 0.775391	val: 2.946984	test: 3.392352

Epoch: 65
Loss: 1.561275064945221
RMSE train: 1.040467	val: 3.938966	test: 3.987436
MAE train: 0.811698	val: 2.946853	test: 3.414810

Epoch: 66
Loss: 1.2125455141067505
RMSE train: 1.112627	val: 4.041254	test: 4.025862
MAE train: 0.883081	val: 3.022357	test: 3.447775

Epoch: 67
Loss: 1.388437271118164
RMSE train: 1.167418	val: 4.124988	test: 4.041314
MAE train: 0.934786	val: 3.088498	test: 3.461102

Epoch: 68
Loss: 1.4735925793647766
RMSE train: 1.118419	val: 4.029279	test: 3.960582
MAE train: 0.882240	val: 3.035176	test: 3.390807

Epoch: 69
Loss: 1.2335794568061829
RMSE train: 0.949481	val: 3.787322	test: 3.824417
MAE train: 0.736819	val: 2.866493	test: 3.275251

Epoch: 70
Loss: 1.1896193027496338
RMSE train: 0.823366	val: 3.625808	test: 3.718169
MAE train: 0.637250	val: 2.757636	test: 3.183432

Epoch: 71
Loss: 1.3031777739524841
RMSE train: 0.750624	val: 3.560093	test: 3.666010
MAE train: 0.578180	val: 2.721012	test: 3.151030

Epoch: 72
Loss: 1.294475018978119
RMSE train: 0.766761	val: 3.637561	test: 3.709600
MAE train: 0.586546	val: 2.798480	test: 3.198560

Epoch: 73
Loss: 1.190333366394043
RMSE train: 0.814438	val: 3.734122	test: 3.816790
MAE train: 0.622955	val: 2.868965	test: 3.295892

Epoch: 74
Loss: 1.293616533279419
RMSE train: 0.899663	val: 3.883044	test: 3.887674
MAE train: 0.696318	val: 2.942476	test: 3.352729

Epoch: 75
Loss: 1.1346884965896606
RMSE train: 0.953233	val: 3.946026	test: 3.904048
MAE train: 0.742155	val: 2.962130	test: 3.353323

Epoch: 76
Loss: 1.167483389377594
RMSE train: 0.948662	val: 3.902911	test: 3.858914
MAE train: 0.740440	val: 2.924652	test: 3.294111

Epoch: 77
Loss: 1.038550704717636
RMSE train: 0.916085	val: 3.832366	test: 3.805972
MAE train: 0.711736	val: 2.890154	test: 3.233006

Epoch: 78
Loss: 1.1911271810531616
RMSE train: 0.955077	val: 3.873280	test: 3.807733
MAE train: 0.736546	val: 2.969754	test: 3.220245

Epoch: 79
Loss: 1.1390224993228912
RMSE train: 0.991903	val: 3.912059	test: 3.802000
MAE train: 0.763814	val: 3.004850	test: 3.194629

Epoch: 80
Loss: 1.193314790725708
RMSE train: 1.017451	val: 3.971476	test: 3.811246
MAE train: 0.794423	val: 3.022072	test: 3.193688

Epoch: 81
Loss: 1.2127771973609924
RMSE train: 1.049412	val: 4.037302	test: 3.814463
MAE train: 0.825511	val: 3.036975	test: 3.207762

Epoch: 82
Loss: 0.9962221682071686
RMSE train: 1.020291	val: 4.031933	test: 3.799852
MAE train: 0.801087	val: 3.009267	test: 3.206801

Epoch: 83
Loss: 1.1769227385520935
RMSE train: 3.246788	val: 7.143694	test: 5.968147
MAE train: 2.928256	val: 5.890188	test: 5.164184

Epoch: 23
Loss: 6.2543933391571045
RMSE train: 3.179055	val: 7.090599	test: 5.880323
MAE train: 2.859237	val: 5.839303	test: 5.080781

Epoch: 24
Loss: 6.137302398681641
RMSE train: 3.051123	val: 6.928238	test: 5.735185
MAE train: 2.731728	val: 5.659015	test: 4.919320

Epoch: 25
Loss: 5.772483587265015
RMSE train: 2.936978	val: 6.778746	test: 5.603772
MAE train: 2.619875	val: 5.473363	test: 4.779659

Epoch: 26
Loss: 5.335553884506226
RMSE train: 2.831390	val: 6.628924	test: 5.463163
MAE train: 2.513848	val: 5.292691	test: 4.639347

Epoch: 27
Loss: 5.104063391685486
RMSE train: 2.734617	val: 6.519478	test: 5.341107
MAE train: 2.413580	val: 5.165922	test: 4.524259

Epoch: 28
Loss: 4.948043346405029
RMSE train: 2.641612	val: 6.432384	test: 5.261573
MAE train: 2.316004	val: 5.070281	test: 4.449732

Epoch: 29
Loss: 4.397094249725342
RMSE train: 2.554405	val: 6.395791	test: 5.227404
MAE train: 2.229088	val: 5.025359	test: 4.411706

Epoch: 30
Loss: 4.515291690826416
RMSE train: 2.470026	val: 6.372787	test: 5.211421
MAE train: 2.144092	val: 4.984990	test: 4.388875

Epoch: 31
Loss: 4.139808535575867
RMSE train: 2.353234	val: 6.304522	test: 5.156219
MAE train: 2.023867	val: 4.898044	test: 4.326953

Epoch: 32
Loss: 3.9522186517715454
RMSE train: 2.240653	val: 6.237958	test: 5.101496
MAE train: 1.912448	val: 4.813380	test: 4.268245

Epoch: 33
Loss: 3.575529932975769
RMSE train: 2.139983	val: 6.134750	test: 5.016799
MAE train: 1.812652	val: 4.713624	test: 4.198203

Epoch: 34
Loss: 3.337156653404236
RMSE train: 2.036803	val: 5.984964	test: 4.926082
MAE train: 1.709276	val: 4.557862	test: 4.117433

Epoch: 35
Loss: 3.1702821254730225
RMSE train: 1.980298	val: 5.903666	test: 4.858915
MAE train: 1.659057	val: 4.461906	test: 4.059514

Epoch: 36
Loss: 3.09789502620697
RMSE train: 1.953320	val: 5.840584	test: 4.801229
MAE train: 1.631224	val: 4.397405	test: 4.005147

Epoch: 37
Loss: 2.8538968563079834
RMSE train: 1.924201	val: 5.771476	test: 4.717721
MAE train: 1.593021	val: 4.315422	test: 3.921165

Epoch: 38
Loss: 2.5818846225738525
RMSE train: 1.888075	val: 5.675337	test: 4.613977
MAE train: 1.549559	val: 4.208702	test: 3.820676

Epoch: 39
Loss: 2.6851004362106323
RMSE train: 1.873648	val: 5.664779	test: 4.532418
MAE train: 1.537951	val: 4.164448	test: 3.747295

Epoch: 40
Loss: 2.4703495502471924
RMSE train: 1.847834	val: 5.625523	test: 4.477294
MAE train: 1.511495	val: 4.095973	test: 3.694531

Epoch: 41
Loss: 2.15320360660553
RMSE train: 1.798052	val: 5.591262	test: 4.433445
MAE train: 1.461090	val: 4.022220	test: 3.656167

Epoch: 42
Loss: 2.1748640537261963
RMSE train: 1.684139	val: 5.513783	test: 4.373406
MAE train: 1.355224	val: 3.912724	test: 3.602326

Epoch: 43
Loss: 2.1046985387802124
RMSE train: 1.555373	val: 5.399082	test: 4.316981
MAE train: 1.243943	val: 3.789624	test: 3.554127

Epoch: 44
Loss: 1.8821220993995667
RMSE train: 1.447071	val: 5.297341	test: 4.254103
MAE train: 1.149344	val: 3.679871	test: 3.496525

Epoch: 45
Loss: 1.712832510471344
RMSE train: 1.368638	val: 5.249816	test: 4.219680
MAE train: 1.081295	val: 3.626689	test: 3.463910

Epoch: 46
Loss: 1.8625566959381104
RMSE train: 1.306063	val: 5.224205	test: 4.164808
MAE train: 1.030300	val: 3.580322	test: 3.407139

Epoch: 47
Loss: 1.8698770403862
RMSE train: 1.287884	val: 5.206476	test: 4.135709
MAE train: 1.011355	val: 3.561556	test: 3.370517

Epoch: 48
Loss: 1.468855381011963
RMSE train: 1.280045	val: 5.200426	test: 4.096132
MAE train: 1.001351	val: 3.562706	test: 3.325020

Epoch: 49
Loss: 1.5303081274032593
RMSE train: 1.220515	val: 5.142160	test: 4.013629
MAE train: 0.948813	val: 3.517488	test: 3.234282

Epoch: 50
Loss: 1.4860965609550476
RMSE train: 1.177618	val: 5.109162	test: 3.948265
MAE train: 0.909609	val: 3.495911	test: 3.161271

Epoch: 51
Loss: 1.5639399290084839
RMSE train: 1.193099	val: 5.149680	test: 3.942698
MAE train: 0.920103	val: 3.529702	test: 3.149915

Epoch: 52
Loss: 1.3707996010780334
RMSE train: 1.189645	val: 5.144311	test: 3.935421
MAE train: 0.918375	val: 3.543470	test: 3.131608

Epoch: 53
Loss: 1.2298169136047363
RMSE train: 1.135037	val: 5.099902	test: 3.933366
MAE train: 0.878693	val: 3.543142	test: 3.134657

Epoch: 54
Loss: 1.4314592480659485
RMSE train: 1.103884	val: 5.077252	test: 3.945465
MAE train: 0.857919	val: 3.559874	test: 3.145503

Epoch: 55
Loss: 1.3733788132667542
RMSE train: 1.073261	val: 5.039681	test: 3.970189
MAE train: 0.838505	val: 3.559256	test: 3.166486

Epoch: 56
Loss: 1.2812423706054688
RMSE train: 1.041872	val: 5.025956	test: 3.989643
MAE train: 0.816840	val: 3.566654	test: 3.190211

Epoch: 57
Loss: 1.2554494738578796
RMSE train: 1.029378	val: 5.022604	test: 3.996753
MAE train: 0.812942	val: 3.560401	test: 3.201298

Epoch: 58
Loss: 1.2009716629981995
RMSE train: 0.998064	val: 4.985184	test: 3.975294
MAE train: 0.791368	val: 3.535820	test: 3.184050

Epoch: 59
Loss: 1.419411301612854
RMSE train: 0.993558	val: 4.982468	test: 3.982779
MAE train: 0.786553	val: 3.556722	test: 3.203130

Epoch: 60
Loss: 1.208739995956421
RMSE train: 0.996631	val: 4.950208	test: 3.971643
MAE train: 0.785132	val: 3.555219	test: 3.194209

Epoch: 61
Loss: 1.2485234141349792
RMSE train: 0.984055	val: 4.879346	test: 3.901298
MAE train: 0.771904	val: 3.504120	test: 3.103943

Epoch: 62
Loss: 1.0843408703804016
RMSE train: 0.992105	val: 4.844241	test: 3.849073
MAE train: 0.773160	val: 3.468556	test: 3.024857

Epoch: 63
Loss: 1.1529432535171509
RMSE train: 0.989865	val: 4.805787	test: 3.815463
MAE train: 0.765745	val: 3.421364	test: 2.972172

Epoch: 64
Loss: 1.0806750655174255
RMSE train: 0.958015	val: 4.757118	test: 3.754430
MAE train: 0.744376	val: 3.349412	test: 2.906194

Epoch: 65
Loss: 1.2353512048721313
RMSE train: 0.946875	val: 4.745796	test: 3.730729
MAE train: 0.743191	val: 3.285235	test: 2.885883

Epoch: 66
Loss: 1.1726244688034058
RMSE train: 0.955984	val: 4.785206	test: 3.715720
MAE train: 0.754037	val: 3.256613	test: 2.884664

Epoch: 67
Loss: 1.0052777826786041
RMSE train: 0.965581	val: 4.817427	test: 3.703188
MAE train: 0.762586	val: 3.245920	test: 2.881033

Epoch: 68
Loss: 1.2042596936225891
RMSE train: 1.005291	val: 4.858146	test: 3.719084
MAE train: 0.794530	val: 3.270396	test: 2.908152

Epoch: 69
Loss: 0.9759802222251892
RMSE train: 0.981485	val: 4.833150	test: 3.697177
MAE train: 0.775264	val: 3.260487	test: 2.889286

Epoch: 70
Loss: 1.065230369567871
RMSE train: 0.947516	val: 4.806750	test: 3.671401
MAE train: 0.749608	val: 3.259322	test: 2.875029

Epoch: 71
Loss: 1.0438371300697327
RMSE train: 0.891385	val: 4.764875	test: 3.633627
MAE train: 0.702954	val: 3.249198	test: 2.852131

Epoch: 72
Loss: 1.1005255579948425
RMSE train: 0.886570	val: 4.778349	test: 3.645798
MAE train: 0.695320	val: 3.286081	test: 2.862783

Epoch: 73
Loss: 1.189137578010559
RMSE train: 0.879064	val: 4.788915	test: 3.683204
MAE train: 0.684182	val: 3.325693	test: 2.882609

Epoch: 74
Loss: 1.1416134238243103
RMSE train: 0.923033	val: 4.822364	test: 3.733483
MAE train: 0.720895	val: 3.382484	test: 2.932000

Epoch: 75
Loss: 1.0402707159519196
RMSE train: 0.985045	val: 4.866931	test: 3.782203
MAE train: 0.766136	val: 3.405647	test: 2.971258

Epoch: 76
Loss: 1.0555843710899353
RMSE train: 1.022012	val: 4.898397	test: 3.807824
MAE train: 0.792820	val: 3.410845	test: 2.982129

Epoch: 77
Loss: 1.071907877922058
RMSE train: 1.007080	val: 4.897411	test: 3.804765
MAE train: 0.778104	val: 3.400947	test: 2.980300

Epoch: 78
Loss: 1.2091537117958069
RMSE train: 1.008038	val: 4.916606	test: 3.813054
MAE train: 0.783504	val: 3.423004	test: 3.000938

Epoch: 79
Loss: 0.9776840209960938
RMSE train: 1.010120	val: 4.905231	test: 3.830207
MAE train: 0.795168	val: 3.419229	test: 3.036049

Epoch: 80
Loss: 1.0430934727191925
RMSE train: 0.994413	val: 4.882348	test: 3.834465
MAE train: 0.787762	val: 3.389868	test: 3.041515

Epoch: 81
Loss: 0.9408624172210693
RMSE train: 0.956305	val: 4.854646	test: 3.810701
MAE train: 0.763055	val: 3.334254	test: 3.011393

Epoch: 82
Loss: 0.9568181931972504
RMSE train: 0.909627	val: 4.797749	test: 3.751295
MAE train: 0.730076	val: 3.248209	test: 2.946583

Epoch: 83
Loss: 0.9009720683097839
RMSE train: 3.467809	val: 6.646025	test: 5.542794
MAE train: 2.949789	val: 5.334736	test: 4.949698

Epoch: 23
Loss: 8.493144512176514
RMSE train: 3.405563	val: 6.569408	test: 5.514491
MAE train: 2.906813	val: 5.314989	test: 4.934291

Epoch: 24
Loss: 8.093407154083252
RMSE train: 3.320435	val: 6.455754	test: 5.434801
MAE train: 2.839167	val: 5.230908	test: 4.857197

Epoch: 25
Loss: 7.750209093093872
RMSE train: 3.221641	val: 6.287800	test: 5.300856
MAE train: 2.753689	val: 5.072744	test: 4.720383

Epoch: 26
Loss: 7.263513088226318
RMSE train: 3.112711	val: 6.141335	test: 5.165274
MAE train: 2.656827	val: 4.908842	test: 4.579886

Epoch: 27
Loss: 6.8801586627960205
RMSE train: 2.990427	val: 5.951474	test: 5.003319
MAE train: 2.548153	val: 4.644371	test: 4.387314

Epoch: 28
Loss: 6.468363285064697
RMSE train: 2.876247	val: 5.782941	test: 4.822671
MAE train: 2.444090	val: 4.367256	test: 4.149188

Epoch: 29
Loss: 6.03782320022583
RMSE train: 2.769960	val: 5.624916	test: 4.635705
MAE train: 2.342904	val: 4.189309	test: 3.937177

Epoch: 30
Loss: 6.315904855728149
RMSE train: 2.733873	val: 5.628809	test: 4.547659
MAE train: 2.317768	val: 4.132010	test: 3.837422

Epoch: 31
Loss: 5.599267959594727
RMSE train: 2.686776	val: 5.638045	test: 4.507298
MAE train: 2.281059	val: 4.100704	test: 3.799612

Epoch: 32
Loss: 5.466682195663452
RMSE train: 2.604708	val: 5.595915	test: 4.483814
MAE train: 2.207538	val: 4.022616	test: 3.783211

Epoch: 33
Loss: 5.376708745956421
RMSE train: 2.497604	val: 5.478699	test: 4.437918
MAE train: 2.106903	val: 3.912228	test: 3.750202

Epoch: 34
Loss: 4.766130208969116
RMSE train: 2.372707	val: 5.310883	test: 4.344484
MAE train: 1.987646	val: 3.761928	test: 3.667408

Epoch: 35
Loss: 4.763687372207642
RMSE train: 2.207591	val: 5.075351	test: 4.185897
MAE train: 1.831134	val: 3.579266	test: 3.516087

Epoch: 36
Loss: 4.172842025756836
RMSE train: 2.063706	val: 4.869818	test: 4.074506
MAE train: 1.693961	val: 3.415195	test: 3.396088

Epoch: 37
Loss: 4.086292386054993
RMSE train: 1.970091	val: 4.779440	test: 3.978618
MAE train: 1.611873	val: 3.356255	test: 3.295959

Epoch: 38
Loss: 3.577571749687195
RMSE train: 1.905705	val: 4.748374	test: 3.919589
MAE train: 1.554050	val: 3.344421	test: 3.227475

Epoch: 39
Loss: 3.5774126052856445
RMSE train: 1.849673	val: 4.765370	test: 3.891205
MAE train: 1.507809	val: 3.360307	test: 3.192840

Epoch: 40
Loss: 3.471278429031372
RMSE train: 1.785391	val: 4.818983	test: 3.908042
MAE train: 1.458283	val: 3.370818	test: 3.213662

Epoch: 41
Loss: 3.210199236869812
RMSE train: 1.722593	val: 4.868958	test: 3.935478
MAE train: 1.409614	val: 3.359686	test: 3.260911

Epoch: 42
Loss: 3.34759259223938
RMSE train: 1.670222	val: 4.962733	test: 3.940576
MAE train: 1.369746	val: 3.361395	test: 3.268202

Epoch: 43
Loss: 2.8789113759994507
RMSE train: 1.646258	val: 5.076404	test: 3.947588
MAE train: 1.353546	val: 3.413664	test: 3.260641

Epoch: 44
Loss: 3.125196695327759
RMSE train: 1.580846	val: 5.081343	test: 3.916986
MAE train: 1.292375	val: 3.418753	test: 3.217989

Epoch: 45
Loss: 3.0057952404022217
RMSE train: 1.506685	val: 5.011636	test: 3.818466
MAE train: 1.223890	val: 3.387442	test: 3.110285

Epoch: 46
Loss: 2.6177014112472534
RMSE train: 1.429442	val: 4.892765	test: 3.680506
MAE train: 1.145287	val: 3.317307	test: 2.959894

Epoch: 47
Loss: 2.848227858543396
RMSE train: 1.434694	val: 4.937281	test: 3.724380
MAE train: 1.155275	val: 3.374316	test: 2.991402

Epoch: 48
Loss: 2.515612483024597
RMSE train: 1.449284	val: 5.056535	test: 3.835344
MAE train: 1.182215	val: 3.470635	test: 3.090115

Epoch: 49
Loss: 2.3941296339035034
RMSE train: 1.445522	val: 5.142842	test: 3.897543
MAE train: 1.186177	val: 3.536465	test: 3.147107

Epoch: 50
Loss: 2.3219045996665955
RMSE train: 1.411042	val: 5.155687	test: 3.919303
MAE train: 1.158704	val: 3.547788	test: 3.167577

Epoch: 51
Loss: 2.162710726261139
RMSE train: 1.366440	val: 5.197740	test: 3.929803
MAE train: 1.121022	val: 3.560613	test: 3.176141

Epoch: 52
Loss: 2.149992823600769
RMSE train: 1.303267	val: 5.170324	test: 3.871791
MAE train: 1.064879	val: 3.514223	test: 3.125207

Epoch: 53
Loss: 2.0555878281593323
RMSE train: 1.201131	val: 5.043375	test: 3.696336
MAE train: 0.967110	val: 3.408386	test: 2.967198

Epoch: 54
Loss: 2.0881553888320923
RMSE train: 1.124109	val: 4.929766	test: 3.551083
MAE train: 0.897139	val: 3.331761	test: 2.831647

Epoch: 55
Loss: 2.2116938829421997
RMSE train: 1.097008	val: 4.917926	test: 3.497943
MAE train: 0.877842	val: 3.322063	test: 2.780603

Epoch: 56
Loss: 2.1563026905059814
RMSE train: 1.076668	val: 4.962000	test: 3.475543
MAE train: 0.863765	val: 3.349107	test: 2.752320

Epoch: 57
Loss: 2.118175208568573
RMSE train: 1.076970	val: 5.005802	test: 3.484948
MAE train: 0.865799	val: 3.372630	test: 2.757232

Epoch: 58
Loss: 1.9113103747367859
RMSE train: 1.051416	val: 4.962093	test: 3.455861
MAE train: 0.846169	val: 3.345704	test: 2.727730

Epoch: 59
Loss: 1.9277228116989136
RMSE train: 1.045119	val: 4.945706	test: 3.438883
MAE train: 0.841140	val: 3.333577	test: 2.703328

Epoch: 60
Loss: 1.6906123161315918
RMSE train: 1.005287	val: 4.831479	test: 3.418435
MAE train: 0.805982	val: 3.270582	test: 2.682160

Epoch: 61
Loss: 1.7155420184135437
RMSE train: 0.950796	val: 4.699015	test: 3.381649
MAE train: 0.759428	val: 3.228879	test: 2.645588

Epoch: 62
Loss: 1.7572234869003296
RMSE train: 0.935410	val: 4.652734	test: 3.413196
MAE train: 0.748276	val: 3.266971	test: 2.663622

Epoch: 63
Loss: 1.6421106457710266
RMSE train: 0.958302	val: 4.608774	test: 3.430559
MAE train: 0.764424	val: 3.286741	test: 2.675384

Epoch: 64
Loss: 1.7104216814041138
RMSE train: 0.977744	val: 4.603557	test: 3.443503
MAE train: 0.779183	val: 3.328543	test: 2.689169

Epoch: 65
Loss: 1.5645166039466858
RMSE train: 0.965129	val: 4.558452	test: 3.473962
MAE train: 0.763978	val: 3.350290	test: 2.723911

Epoch: 66
Loss: 1.6179081797599792
RMSE train: 0.932499	val: 4.545617	test: 3.530082
MAE train: 0.743631	val: 3.352545	test: 2.788048

Epoch: 67
Loss: 1.5831685066223145
RMSE train: 0.920685	val: 4.596874	test: 3.590886
MAE train: 0.741042	val: 3.349003	test: 2.849628

Epoch: 68
Loss: 1.5156790018081665
RMSE train: 0.919646	val: 4.649122	test: 3.610004
MAE train: 0.741232	val: 3.333762	test: 2.861170

Epoch: 69
Loss: 1.6263217329978943
RMSE train: 0.923125	val: 4.704003	test: 3.555229
MAE train: 0.742716	val: 3.312988	test: 2.817830

Epoch: 70
Loss: 1.4346947073936462
RMSE train: 0.895547	val: 4.659136	test: 3.458365
MAE train: 0.715494	val: 3.253900	test: 2.725047

Epoch: 71
Loss: 1.477276623249054
RMSE train: 0.894016	val: 4.622084	test: 3.369971
MAE train: 0.715603	val: 3.214284	test: 2.624461

Epoch: 72
Loss: 1.4404813647270203
RMSE train: 0.878008	val: 4.590398	test: 3.419258
MAE train: 0.703201	val: 3.195924	test: 2.660879

Epoch: 73
Loss: 1.3775010704994202
RMSE train: 0.842455	val: 4.572877	test: 3.524569
MAE train: 0.668992	val: 3.188065	test: 2.765931

Epoch: 74
Loss: 1.4610841870307922
RMSE train: 0.834524	val: 4.592334	test: 3.631077
MAE train: 0.656146	val: 3.215962	test: 2.865335

Epoch: 75
Loss: 1.3426828384399414
RMSE train: 0.844955	val: 4.618444	test: 3.659083
MAE train: 0.663925	val: 3.260866	test: 2.885364

Epoch: 76
Loss: 1.4454161524772644
RMSE train: 0.854890	val: 4.646077	test: 3.653171
MAE train: 0.672796	val: 3.289486	test: 2.873637

Epoch: 77
Loss: 1.5642229318618774
RMSE train: 0.870000	val: 4.715346	test: 3.653142
MAE train: 0.689833	val: 3.321337	test: 2.873789

Epoch: 78
Loss: 1.2656883597373962
RMSE train: 0.892959	val: 4.772429	test: 3.643870
MAE train: 0.713489	val: 3.375344	test: 2.870462

Epoch: 79
Loss: 1.1122826933860779
RMSE train: 0.890881	val: 4.783562	test: 3.639185
MAE train: 0.714953	val: 3.408950	test: 2.874690

Epoch: 80
Loss: 1.196377158164978
RMSE train: 0.893312	val: 4.828243	test: 3.618432
MAE train: 0.716127	val: 3.438592	test: 2.862049

Epoch: 81
Loss: 1.2596248388290405
RMSE train: 0.852990	val: 4.832520	test: 3.597710
MAE train: 0.675800	val: 3.440659	test: 2.840202

Epoch: 82
Loss: 1.2682812213897705
RMSE train: 0.866842	val: 4.880464	test: 3.588939
MAE train: 0.680045	val: 3.463290	test: 2.820928

Epoch: 83
Loss: 1.1866710186004639
RMSE train: 2.857419	val: 7.006611	test: 6.166994
MAE train: 2.471718	val: 5.926300	test: 5.412478

Epoch: 23
Loss: 7.042126417160034
RMSE train: 2.777668	val: 6.938606	test: 6.105394
MAE train: 2.401563	val: 5.856284	test: 5.342866

Epoch: 24
Loss: 6.387454986572266
RMSE train: 2.672592	val: 6.818062	test: 6.007825
MAE train: 2.299736	val: 5.737517	test: 5.242895

Epoch: 25
Loss: 6.006476640701294
RMSE train: 2.567304	val: 6.622374	test: 5.832136
MAE train: 2.192536	val: 5.523246	test: 5.063135

Epoch: 26
Loss: 5.819264888763428
RMSE train: 2.478863	val: 6.452151	test: 5.683072
MAE train: 2.101491	val: 5.331040	test: 4.907457

Epoch: 27
Loss: 5.883231163024902
RMSE train: 2.404291	val: 6.292441	test: 5.521808
MAE train: 2.034554	val: 5.152335	test: 4.753806

Epoch: 28
Loss: 5.164720773696899
RMSE train: 2.377390	val: 6.199445	test: 5.390578
MAE train: 2.008083	val: 5.012970	test: 4.631352

Epoch: 29
Loss: 5.2586939334869385
RMSE train: 2.391810	val: 6.178365	test: 5.286010
MAE train: 2.023835	val: 4.937501	test: 4.526584

Epoch: 30
Loss: 4.878521203994751
RMSE train: 2.390858	val: 6.145720	test: 5.174938
MAE train: 2.026988	val: 4.856338	test: 4.415836

Epoch: 31
Loss: 4.882416605949402
RMSE train: 2.356012	val: 6.059854	test: 5.021228
MAE train: 2.000476	val: 4.714122	test: 4.247672

Epoch: 32
Loss: 4.440792083740234
RMSE train: 2.284716	val: 5.934126	test: 4.876117
MAE train: 1.931752	val: 4.573904	test: 4.092374

Epoch: 33
Loss: 4.140411853790283
RMSE train: 2.229935	val: 5.837062	test: 4.819326
MAE train: 1.880178	val: 4.491104	test: 4.039262

Epoch: 34
Loss: 3.6587167978286743
RMSE train: 2.148970	val: 5.697507	test: 4.771052
MAE train: 1.804526	val: 4.376263	test: 4.000465

Epoch: 35
Loss: 3.5402671098709106
RMSE train: 2.024353	val: 5.478582	test: 4.666852
MAE train: 1.681901	val: 4.181595	test: 3.911986

Epoch: 36
Loss: 3.3402531147003174
RMSE train: 1.908220	val: 5.299972	test: 4.580253
MAE train: 1.566792	val: 4.014914	test: 3.839375

Epoch: 37
Loss: 3.1031718254089355
RMSE train: 1.826121	val: 5.218514	test: 4.548691
MAE train: 1.484310	val: 3.954166	test: 3.824036

Epoch: 38
Loss: 3.0470041036605835
RMSE train: 1.767089	val: 5.219745	test: 4.514651
MAE train: 1.428895	val: 3.949659	test: 3.803212

Epoch: 39
Loss: 2.831167697906494
RMSE train: 1.710109	val: 5.222629	test: 4.454234
MAE train: 1.371031	val: 3.922496	test: 3.754989

Epoch: 40
Loss: 2.7572120428085327
RMSE train: 1.705560	val: 5.272549	test: 4.427184
MAE train: 1.365626	val: 3.939319	test: 3.741412

Epoch: 41
Loss: 2.566849708557129
RMSE train: 1.687418	val: 5.282567	test: 4.390514
MAE train: 1.345045	val: 3.921093	test: 3.713101

Epoch: 42
Loss: 2.6477099657058716
RMSE train: 1.652983	val: 5.245669	test: 4.353675
MAE train: 1.315329	val: 3.877292	test: 3.677261

Epoch: 43
Loss: 2.2257537841796875
RMSE train: 1.610293	val: 5.170292	test: 4.271577
MAE train: 1.275727	val: 3.799190	test: 3.598959

Epoch: 44
Loss: 2.267512321472168
RMSE train: 1.545736	val: 5.041271	test: 4.160437
MAE train: 1.219239	val: 3.681305	test: 3.495436

Epoch: 45
Loss: 2.013460636138916
RMSE train: 1.442246	val: 4.828701	test: 4.014356
MAE train: 1.129995	val: 3.487432	test: 3.357876

Epoch: 46
Loss: 1.9908703565597534
RMSE train: 1.360559	val: 4.671893	test: 3.914470
MAE train: 1.056719	val: 3.320242	test: 3.257284

Epoch: 47
Loss: 2.1068575382232666
RMSE train: 1.305437	val: 4.584526	test: 3.847179
MAE train: 1.005769	val: 3.202194	test: 3.190797

Epoch: 48
Loss: 1.695528268814087
RMSE train: 1.257100	val: 4.512988	test: 3.805264
MAE train: 0.963915	val: 3.116429	test: 3.158209

Epoch: 49
Loss: 1.9123762249946594
RMSE train: 1.220720	val: 4.489933	test: 3.789086
MAE train: 0.938478	val: 3.079073	test: 3.155328

Epoch: 50
Loss: 2.012782335281372
RMSE train: 1.185365	val: 4.476814	test: 3.739763
MAE train: 0.910604	val: 3.056252	test: 3.117091

Epoch: 51
Loss: 1.7645751237869263
RMSE train: 1.187406	val: 4.556646	test: 3.725131
MAE train: 0.913244	val: 3.100041	test: 3.103509

Epoch: 52
Loss: 1.807056725025177
RMSE train: 1.173136	val: 4.529473	test: 3.657325
MAE train: 0.904551	val: 3.077139	test: 3.039309

Epoch: 53
Loss: 1.7774365544319153
RMSE train: 1.117224	val: 4.378784	test: 3.537040
MAE train: 0.861434	val: 2.970031	test: 2.926446

Epoch: 54
Loss: 1.6314029693603516
RMSE train: 1.065034	val: 4.261298	test: 3.459248
MAE train: 0.820443	val: 2.880723	test: 2.864991

Epoch: 55
Loss: 1.567670464515686
RMSE train: 1.048600	val: 4.176909	test: 3.441247
MAE train: 0.805594	val: 2.825661	test: 2.847135

Epoch: 56
Loss: 1.5876808166503906
RMSE train: 1.014243	val: 4.110224	test: 3.356977
MAE train: 0.779363	val: 2.783048	test: 2.749714

Epoch: 57
Loss: 1.5259804725646973
RMSE train: 0.992001	val: 4.101549	test: 3.269256
MAE train: 0.760032	val: 2.777024	test: 2.646387

Epoch: 58
Loss: 1.6340093612670898
RMSE train: 0.975397	val: 4.086569	test: 3.161848
MAE train: 0.749460	val: 2.751868	test: 2.520768

Epoch: 59
Loss: 1.5698090195655823
RMSE train: 0.943266	val: 4.034498	test: 3.095592
MAE train: 0.734134	val: 2.706094	test: 2.436825

Epoch: 60
Loss: 1.430981159210205
RMSE train: 0.897649	val: 3.967001	test: 3.090338
MAE train: 0.702392	val: 2.655355	test: 2.435225

Epoch: 61
Loss: 1.54477459192276
RMSE train: 0.850098	val: 3.856956	test: 3.102076
MAE train: 0.663390	val: 2.569970	test: 2.465746

Epoch: 62
Loss: 1.3462030291557312
RMSE train: 0.851690	val: 3.796634	test: 3.151613
MAE train: 0.662458	val: 2.519983	test: 2.523964

Epoch: 63
Loss: 1.3536269664764404
RMSE train: 0.845970	val: 3.717461	test: 3.124984
MAE train: 0.651335	val: 2.484134	test: 2.520056

Epoch: 64
Loss: 1.4785909056663513
RMSE train: 0.813835	val: 3.663110	test: 3.088349
MAE train: 0.627608	val: 2.459052	test: 2.485028

Epoch: 65
Loss: 1.1927434802055359
RMSE train: 0.787001	val: 3.668319	test: 3.070982
MAE train: 0.609376	val: 2.460435	test: 2.459852

Epoch: 66
Loss: 1.41441011428833
RMSE train: 0.821212	val: 3.819587	test: 3.105483
MAE train: 0.639912	val: 2.575315	test: 2.484575

Epoch: 67
Loss: 1.312936544418335
RMSE train: 0.867048	val: 3.981869	test: 3.127610
MAE train: 0.675524	val: 2.679885	test: 2.501159

Epoch: 68
Loss: 1.4316431879997253
RMSE train: 0.843076	val: 4.001228	test: 3.076983
MAE train: 0.657094	val: 2.677137	test: 2.440660

Epoch: 69
Loss: 1.4750553965568542
RMSE train: 0.785588	val: 3.925825	test: 3.011101
MAE train: 0.610125	val: 2.614730	test: 2.366679

Epoch: 70
Loss: 1.317484736442566
RMSE train: 0.715793	val: 3.752673	test: 2.959364
MAE train: 0.550309	val: 2.492893	test: 2.327287

Epoch: 71
Loss: 1.370000422000885
RMSE train: 0.729080	val: 3.717789	test: 2.966602
MAE train: 0.546449	val: 2.490978	test: 2.352245

Epoch: 72
Loss: 1.3775742650032043
RMSE train: 0.768868	val: 3.826085	test: 3.069382
MAE train: 0.583748	val: 2.590615	test: 2.461067

Epoch: 73
Loss: 1.2090970873832703
RMSE train: 0.828520	val: 3.980397	test: 3.188920
MAE train: 0.640036	val: 2.691576	test: 2.572554

Epoch: 74
Loss: 1.2024332284927368
RMSE train: 0.897356	val: 4.091808	test: 3.268476
MAE train: 0.695583	val: 2.754649	test: 2.646266

Epoch: 75
Loss: 1.3052331805229187
RMSE train: 0.917736	val: 4.103019	test: 3.290350
MAE train: 0.709594	val: 2.738769	test: 2.663261

Epoch: 76
Loss: 1.3208636045455933
RMSE train: 0.872066	val: 4.038880	test: 3.218836
MAE train: 0.673212	val: 2.663721	test: 2.585327

Epoch: 77
Loss: 1.1818915605545044
RMSE train: 0.795854	val: 3.989159	test: 3.082411
MAE train: 0.613800	val: 2.605206	test: 2.430468

Epoch: 78
Loss: 1.3157967925071716
RMSE train: 0.735337	val: 3.951758	test: 2.968164
MAE train: 0.566291	val: 2.542920	test: 2.280375

Epoch: 79
Loss: 1.0339648723602295
RMSE train: 0.726675	val: 3.961482	test: 2.924142
MAE train: 0.560510	val: 2.532046	test: 2.202609

Epoch: 80
Loss: 1.1571762561798096
RMSE train: 0.741609	val: 4.067620	test: 3.005194
MAE train: 0.572182	val: 2.630034	test: 2.299874

Epoch: 81
Loss: 1.0536388158798218
RMSE train: 0.751422	val: 4.135210	test: 3.112405
MAE train: 0.582559	val: 2.763310	test: 2.453380

Epoch: 82
Loss: 1.1207644939422607
RMSE train: 0.809275	val: 4.277263	test: 3.258093
MAE train: 0.631274	val: 2.930746	test: 2.613113

Epoch: 83
Loss: 1.0574584007263184
RMSE train: 3.096781	val: 7.477998	test: 6.130041
MAE train: 2.592414	val: 6.095716	test: 5.391240

Epoch: 23
Loss: 7.478618144989014
RMSE train: 3.007348	val: 7.288372	test: 6.065370
MAE train: 2.526444	val: 5.958611	test: 5.331567

Epoch: 24
Loss: 6.854045867919922
RMSE train: 2.912834	val: 7.110261	test: 6.013000
MAE train: 2.452000	val: 5.833556	test: 5.288501

Epoch: 25
Loss: 6.391892194747925
RMSE train: 2.777136	val: 6.868307	test: 5.878973
MAE train: 2.332781	val: 5.601569	test: 5.156490

Epoch: 26
Loss: 5.905735015869141
RMSE train: 2.678066	val: 6.714231	test: 5.768916
MAE train: 2.248751	val: 5.421414	test: 5.048032

Epoch: 27
Loss: 6.123940944671631
RMSE train: 2.605429	val: 6.611649	test: 5.698094
MAE train: 2.186342	val: 5.311102	test: 4.984403

Epoch: 28
Loss: 5.9795918464660645
RMSE train: 2.565435	val: 6.544200	test: 5.638947
MAE train: 2.155666	val: 5.247179	test: 4.931774

Epoch: 29
Loss: 5.221442937850952
RMSE train: 2.563361	val: 6.568400	test: 5.640503
MAE train: 2.155715	val: 5.275372	test: 4.941603

Epoch: 30
Loss: 4.741107940673828
RMSE train: 2.540603	val: 6.579446	test: 5.657958
MAE train: 2.135499	val: 5.326962	test: 4.970067

Epoch: 31
Loss: 4.7577149868011475
RMSE train: 2.468856	val: 6.522838	test: 5.628442
MAE train: 2.064426	val: 5.319285	test: 4.948382

Epoch: 32
Loss: 4.30825400352478
RMSE train: 2.396025	val: 6.437137	test: 5.566280
MAE train: 1.995419	val: 5.274871	test: 4.887897

Epoch: 33
Loss: 4.301005601882935
RMSE train: 2.271197	val: 6.252173	test: 5.415536
MAE train: 1.878382	val: 5.098645	test: 4.727020

Epoch: 34
Loss: 4.173348426818848
RMSE train: 2.105488	val: 6.005966	test: 5.217886
MAE train: 1.722714	val: 4.838473	test: 4.501072

Epoch: 35
Loss: 3.9038867950439453
RMSE train: 1.953550	val: 5.792078	test: 5.031401
MAE train: 1.575799	val: 4.585073	test: 4.285537

Epoch: 36
Loss: 3.5104598999023438
RMSE train: 1.844323	val: 5.658799	test: 4.896464
MAE train: 1.475711	val: 4.392699	test: 4.129568

Epoch: 37
Loss: 3.4994324445724487
RMSE train: 1.775875	val: 5.617001	test: 4.838301
MAE train: 1.414911	val: 4.299211	test: 4.061192

Epoch: 38
Loss: 3.1599971055984497
RMSE train: 1.732718	val: 5.617967	test: 4.788921
MAE train: 1.375139	val: 4.231510	test: 4.000488

Epoch: 39
Loss: 3.0332974195480347
RMSE train: 1.710918	val: 5.613191	test: 4.736768
MAE train: 1.356846	val: 4.185642	test: 3.952241

Epoch: 40
Loss: 2.6972103118896484
RMSE train: 1.637141	val: 5.502586	test: 4.616790
MAE train: 1.291740	val: 4.017847	test: 3.823690

Epoch: 41
Loss: 2.7316569089889526
RMSE train: 1.546575	val: 5.331688	test: 4.505381
MAE train: 1.213108	val: 3.806650	test: 3.708859

Epoch: 42
Loss: 2.659441113471985
RMSE train: 1.501738	val: 5.235210	test: 4.446492
MAE train: 1.170911	val: 3.695046	test: 3.650588

Epoch: 43
Loss: 2.422894239425659
RMSE train: 1.479335	val: 5.181920	test: 4.423709
MAE train: 1.150783	val: 3.633408	test: 3.639541

Epoch: 44
Loss: 2.4481488466262817
RMSE train: 1.451397	val: 5.110547	test: 4.403269
MAE train: 1.125913	val: 3.569275	test: 3.635284

Epoch: 45
Loss: 2.3662991523742676
RMSE train: 1.422860	val: 5.068163	test: 4.387917
MAE train: 1.098885	val: 3.526981	test: 3.620742

Epoch: 46
Loss: 2.431515693664551
RMSE train: 1.425310	val: 5.117458	test: 4.404772
MAE train: 1.107386	val: 3.552088	test: 3.631154

Epoch: 47
Loss: 2.2415857315063477
RMSE train: 1.430814	val: 5.210654	test: 4.412420
MAE train: 1.117046	val: 3.564520	test: 3.617793

Epoch: 48
Loss: 2.0298047065734863
RMSE train: 1.404948	val: 5.233438	test: 4.375702
MAE train: 1.098719	val: 3.530258	test: 3.564865

Epoch: 49
Loss: 1.9897997975349426
RMSE train: 1.350121	val: 5.201432	test: 4.309997
MAE train: 1.053753	val: 3.475852	test: 3.498573

Epoch: 50
Loss: 1.8847345113754272
RMSE train: 1.270822	val: 5.135127	test: 4.244636
MAE train: 0.984883	val: 3.406608	test: 3.431464

Epoch: 51
Loss: 2.0017433166503906
RMSE train: 1.223088	val: 5.102592	test: 4.220658
MAE train: 0.941704	val: 3.391493	test: 3.394143

Epoch: 52
Loss: 1.7977941632270813
RMSE train: 1.155442	val: 5.037114	test: 4.198366
MAE train: 0.887467	val: 3.365960	test: 3.359015

Epoch: 53
Loss: 1.8306910395622253
RMSE train: 1.070210	val: 4.949934	test: 4.146542
MAE train: 0.824287	val: 3.306330	test: 3.300083

Epoch: 54
Loss: 1.857050359249115
RMSE train: 1.042791	val: 4.949662	test: 4.125331
MAE train: 0.803369	val: 3.311715	test: 3.274681

Epoch: 55
Loss: 1.7853054404258728
RMSE train: 0.999868	val: 4.924642	test: 4.061286
MAE train: 0.771538	val: 3.280710	test: 3.206594

Epoch: 56
Loss: 1.7377209663391113
RMSE train: 0.957909	val: 4.872527	test: 3.970077
MAE train: 0.737401	val: 3.231125	test: 3.122548

Epoch: 57
Loss: 1.7322861552238464
RMSE train: 0.934553	val: 4.895455	test: 3.932954
MAE train: 0.715080	val: 3.245538	test: 3.077932

Epoch: 58
Loss: 1.675069510936737
RMSE train: 0.920669	val: 4.917685	test: 3.928409
MAE train: 0.699080	val: 3.268759	test: 3.075694

Epoch: 59
Loss: 1.6838228106498718
RMSE train: 0.902957	val: 4.911510	test: 3.925911
MAE train: 0.685288	val: 3.285660	test: 3.090723

Epoch: 60
Loss: 1.5468156337738037
RMSE train: 0.914252	val: 4.934081	test: 3.955855
MAE train: 0.695518	val: 3.345456	test: 3.134995

Epoch: 61
Loss: 1.4433314800262451
RMSE train: 0.914017	val: 4.914757	test: 3.980529
MAE train: 0.698645	val: 3.362470	test: 3.175008

Epoch: 62
Loss: 1.5974202752113342
RMSE train: 0.885110	val: 4.837506	test: 3.950368
MAE train: 0.677754	val: 3.324336	test: 3.154583

Epoch: 63
Loss: 1.4886807203292847
RMSE train: 0.879368	val: 4.801912	test: 3.905061
MAE train: 0.679168	val: 3.324474	test: 3.113473

Epoch: 64
Loss: 1.5398086309432983
RMSE train: 0.892660	val: 4.812081	test: 3.854484
MAE train: 0.695116	val: 3.346821	test: 3.047276

Epoch: 65
Loss: 1.455136239528656
RMSE train: 0.866118	val: 4.749099	test: 3.779424
MAE train: 0.676577	val: 3.295753	test: 2.954103

Epoch: 66
Loss: 1.5005027651786804
RMSE train: 0.849185	val: 4.742214	test: 3.751546
MAE train: 0.662557	val: 3.274781	test: 2.921107

Epoch: 67
Loss: 1.3093453645706177
RMSE train: 0.809246	val: 4.704832	test: 3.712973
MAE train: 0.630512	val: 3.227142	test: 2.879191

Epoch: 68
Loss: 1.2629263401031494
RMSE train: 0.775409	val: 4.663192	test: 3.701867
MAE train: 0.604314	val: 3.188510	test: 2.873669

Epoch: 69
Loss: 1.3226147890090942
RMSE train: 0.767945	val: 4.652796	test: 3.707610
MAE train: 0.597460	val: 3.189285	test: 2.891873

Epoch: 70
Loss: 1.2887731194496155
RMSE train: 0.757244	val: 4.635523	test: 3.730748
MAE train: 0.583626	val: 3.183548	test: 2.929371

Epoch: 71
Loss: 1.2083575129508972
RMSE train: 0.777076	val: 4.659054	test: 3.788496
MAE train: 0.600575	val: 3.218501	test: 2.986640

Epoch: 72
Loss: 1.248558759689331
RMSE train: 0.771813	val: 4.638365	test: 3.831436
MAE train: 0.598129	val: 3.216043	test: 3.028852

Epoch: 73
Loss: 1.2589581608772278
RMSE train: 0.786847	val: 4.644413	test: 3.892621
MAE train: 0.611427	val: 3.242265	test: 3.088232

Epoch: 74
Loss: 1.2555280327796936
RMSE train: 0.812304	val: 4.668454	test: 3.946248
MAE train: 0.631025	val: 3.290587	test: 3.144845

Epoch: 75
Loss: 1.1286478638648987
RMSE train: 0.850539	val: 4.694292	test: 3.960449
MAE train: 0.659046	val: 3.333648	test: 3.165686

Epoch: 76
Loss: 1.3554208278656006
RMSE train: 0.837560	val: 4.689090	test: 3.970505
MAE train: 0.649715	val: 3.316286	test: 3.175408

Epoch: 77
Loss: 1.1829379200935364
RMSE train: 0.805645	val: 4.646084	test: 3.979623
MAE train: 0.626418	val: 3.276718	test: 3.185170

Epoch: 78
Loss: 1.024846613407135
RMSE train: 0.795784	val: 4.664201	test: 4.018935
MAE train: 0.616198	val: 3.270110	test: 3.217552

Epoch: 79
Loss: 1.1928009390830994
RMSE train: 0.801977	val: 4.714485	test: 4.085711
MAE train: 0.625383	val: 3.292111	test: 3.280324

Epoch: 80
Loss: 1.2650349736213684
RMSE train: 0.899932	val: 4.886581	test: 4.203326
MAE train: 0.709926	val: 3.412611	test: 3.397874

Epoch: 81
Loss: 1.0950440168380737
RMSE train: 0.944769	val: 4.976413	test: 4.253612
MAE train: 0.746248	val: 3.496783	test: 3.457153

Epoch: 82
Loss: 1.3033460974693298
RMSE train: 0.890891	val: 4.929505	test: 4.236460
MAE train: 0.704661	val: 3.505386	test: 3.448801

Epoch: 83
Loss: 1.2295689582824707
RMSE train: 3.475631	val: 6.373643	test: 5.747350
MAE train: 3.019249	val: 5.249530	test: 5.030416

Epoch: 23
Loss: 7.533122539520264
RMSE train: 3.384855	val: 6.285500	test: 5.690261
MAE train: 2.938504	val: 5.180199	test: 4.992193

Epoch: 24
Loss: 7.318344593048096
RMSE train: 3.240561	val: 6.104085	test: 5.585453
MAE train: 2.806007	val: 5.005585	test: 4.925117

Epoch: 25
Loss: 7.0659425258636475
RMSE train: 3.127349	val: 5.958497	test: 5.509274
MAE train: 2.700901	val: 4.867646	test: 4.872407

Epoch: 26
Loss: 6.648536920547485
RMSE train: 3.009161	val: 5.753282	test: 5.419374
MAE train: 2.594292	val: 4.674191	test: 4.798710

Epoch: 27
Loss: 6.336187124252319
RMSE train: 2.898561	val: 5.544132	test: 5.326935
MAE train: 2.492865	val: 4.469936	test: 4.721141

Epoch: 28
Loss: 5.8454365730285645
RMSE train: 2.824931	val: 5.385615	test: 5.280807
MAE train: 2.428098	val: 4.346499	test: 4.681103

Epoch: 29
Loss: 5.587996482849121
RMSE train: 2.778832	val: 5.349152	test: 5.290818
MAE train: 2.390433	val: 4.352465	test: 4.693302

Epoch: 30
Loss: 5.425788164138794
RMSE train: 2.768669	val: 5.360797	test: 5.287648
MAE train: 2.388263	val: 4.359564	test: 4.693708

Epoch: 31
Loss: 4.831151962280273
RMSE train: 2.690360	val: 5.312134	test: 5.214315
MAE train: 2.314972	val: 4.264257	test: 4.624398

Epoch: 32
Loss: 4.86221170425415
RMSE train: 2.630472	val: 5.304745	test: 5.131136
MAE train: 2.256877	val: 4.166041	test: 4.533051

Epoch: 33
Loss: 4.7760326862335205
RMSE train: 2.587250	val: 5.316551	test: 5.050853
MAE train: 2.216545	val: 4.141576	test: 4.431953

Epoch: 34
Loss: 4.168986916542053
RMSE train: 2.498322	val: 5.252535	test: 4.948764
MAE train: 2.128750	val: 4.057101	test: 4.299493

Epoch: 35
Loss: 3.9455173015594482
RMSE train: 2.389242	val: 5.127960	test: 4.871150
MAE train: 2.026675	val: 3.921609	test: 4.197331

Epoch: 36
Loss: 3.7906839847564697
RMSE train: 2.257288	val: 4.966269	test: 4.792197
MAE train: 1.905876	val: 3.755310	test: 4.096821

Epoch: 37
Loss: 3.5672818422317505
RMSE train: 2.137311	val: 4.857968	test: 4.722701
MAE train: 1.789715	val: 3.653500	test: 4.017392

Epoch: 38
Loss: 3.4503493309020996
RMSE train: 2.029069	val: 4.760335	test: 4.685869
MAE train: 1.687260	val: 3.590633	test: 3.972477

Epoch: 39
Loss: 3.676950454711914
RMSE train: 1.932123	val: 4.728370	test: 4.639452
MAE train: 1.592762	val: 3.575410	test: 3.932326

Epoch: 40
Loss: 3.0337469577789307
RMSE train: 1.830926	val: 4.680909	test: 4.571599
MAE train: 1.490827	val: 3.531314	test: 3.855568

Epoch: 41
Loss: 2.92565381526947
RMSE train: 1.724086	val: 4.621382	test: 4.478503
MAE train: 1.387375	val: 3.437135	test: 3.714850

Epoch: 42
Loss: 2.930191397666931
RMSE train: 1.665475	val: 4.605084	test: 4.435599
MAE train: 1.334020	val: 3.415298	test: 3.661592

Epoch: 43
Loss: 3.024632692337036
RMSE train: 1.632685	val: 4.639297	test: 4.450903
MAE train: 1.312577	val: 3.411405	test: 3.625306

Epoch: 44
Loss: 2.670524001121521
RMSE train: 1.644879	val: 4.738902	test: 4.497846
MAE train: 1.329743	val: 3.496210	test: 3.663153

Epoch: 45
Loss: 2.652868151664734
RMSE train: 1.616994	val: 4.775288	test: 4.492359
MAE train: 1.304808	val: 3.537982	test: 3.675714

Epoch: 46
Loss: 2.5068119764328003
RMSE train: 1.602551	val: 4.813611	test: 4.472684
MAE train: 1.294863	val: 3.569300	test: 3.645303

Epoch: 47
Loss: 2.479838252067566
RMSE train: 1.591141	val: 4.814569	test: 4.475028
MAE train: 1.291423	val: 3.568872	test: 3.644805

Epoch: 48
Loss: 2.1359318494796753
RMSE train: 1.538711	val: 4.733841	test: 4.440536
MAE train: 1.250270	val: 3.510619	test: 3.603186

Epoch: 49
Loss: 2.2434598803520203
RMSE train: 1.445530	val: 4.602067	test: 4.405179
MAE train: 1.168936	val: 3.395782	test: 3.489904

Epoch: 50
Loss: 2.053839921951294
RMSE train: 1.354708	val: 4.517935	test: 4.405155
MAE train: 1.088701	val: 3.382217	test: 3.380416

Epoch: 51
Loss: 2.235107958316803
RMSE train: 1.279287	val: 4.430671	test: 4.440142
MAE train: 1.019497	val: 3.363540	test: 3.343924

Epoch: 52
Loss: 1.8135713338851929
RMSE train: 1.195575	val: 4.353650	test: 4.516534
MAE train: 0.940092	val: 3.371004	test: 3.345568

Epoch: 53
Loss: 1.8150363564491272
RMSE train: 1.137849	val: 4.419772	test: 4.642868
MAE train: 0.892601	val: 3.466755	test: 3.403572

Epoch: 54
Loss: 2.101053774356842
RMSE train: 1.151174	val: 4.514691	test: 4.615241
MAE train: 0.904946	val: 3.520569	test: 3.415346

Epoch: 55
Loss: 1.8023489117622375
RMSE train: 1.193369	val: 4.618734	test: 4.528859
MAE train: 0.941772	val: 3.575254	test: 3.392964

Epoch: 56
Loss: 1.8082115054130554
RMSE train: 1.209546	val: 4.647162	test: 4.441240
MAE train: 0.952858	val: 3.580457	test: 3.340221

Epoch: 57
Loss: 1.812812089920044
RMSE train: 1.165447	val: 4.571755	test: 4.411981
MAE train: 0.918513	val: 3.508898	test: 3.292894

Epoch: 58
Loss: 1.8725269436836243
RMSE train: 1.074775	val: 4.430914	test: 4.416650
MAE train: 0.848044	val: 3.411114	test: 3.247477

Epoch: 59
Loss: 1.6439056396484375
RMSE train: 0.987428	val: 4.287536	test: 4.533577
MAE train: 0.778279	val: 3.348456	test: 3.282057

Epoch: 60
Loss: 1.6839927434921265
RMSE train: 0.988509	val: 4.301686	test: 4.602524
MAE train: 0.781369	val: 3.352695	test: 3.329624

Epoch: 61
Loss: 1.8111152052879333
RMSE train: 1.021315	val: 4.410687	test: 4.665308
MAE train: 0.811254	val: 3.419931	test: 3.401652

Epoch: 62
Loss: 1.5213158130645752
RMSE train: 1.019645	val: 4.455702	test: 4.723471
MAE train: 0.813447	val: 3.472090	test: 3.446712

Epoch: 63
Loss: 1.6632438898086548
RMSE train: 1.035327	val: 4.468567	test: 4.772145
MAE train: 0.828099	val: 3.462637	test: 3.506091

Epoch: 64
Loss: 1.4771403074264526
RMSE train: 1.044304	val: 4.487938	test: 4.714756
MAE train: 0.837388	val: 3.469758	test: 3.475093

Epoch: 65
Loss: 1.5834296345710754
RMSE train: 1.050562	val: 4.487326	test: 4.670813
MAE train: 0.844088	val: 3.450569	test: 3.459486

Epoch: 66
Loss: 1.4672175645828247
RMSE train: 1.061580	val: 4.484690	test: 4.671949
MAE train: 0.855908	val: 3.410127	test: 3.460882

Epoch: 67
Loss: 1.6600506901741028
RMSE train: 1.088058	val: 4.510927	test: 4.698243
MAE train: 0.881845	val: 3.407805	test: 3.478280

Epoch: 68
Loss: 1.6287123560905457
RMSE train: 1.090591	val: 4.455627	test: 4.816330
MAE train: 0.880478	val: 3.342342	test: 3.553409

Epoch: 69
Loss: 1.2683095335960388
RMSE train: 1.049753	val: 4.387104	test: 5.002583
MAE train: 0.843295	val: 3.313322	test: 3.631214

Epoch: 70
Loss: 1.5282754302024841
RMSE train: 1.069502	val: 4.422632	test: 5.207684
MAE train: 0.861361	val: 3.365603	test: 3.733083

Epoch: 71
Loss: 1.3584668040275574
RMSE train: 1.095354	val: 4.423245	test: 5.182290
MAE train: 0.882809	val: 3.360535	test: 3.736149

Epoch: 72
Loss: 1.4056719541549683
RMSE train: 1.063631	val: 4.349085	test: 5.096028
MAE train: 0.853879	val: 3.316175	test: 3.676172

Epoch: 73
Loss: 1.2605466842651367
RMSE train: 1.024801	val: 4.276939	test: 5.027148
MAE train: 0.821414	val: 3.309928	test: 3.604860

Epoch: 74
Loss: 1.504739224910736
RMSE train: 1.017753	val: 4.250570	test: 4.940125
MAE train: 0.815373	val: 3.290673	test: 3.540740

Epoch: 75
Loss: 1.3549367189407349
RMSE train: 1.013893	val: 4.231736	test: 4.750529
MAE train: 0.812682	val: 3.221785	test: 3.419353

Epoch: 76
Loss: 1.4306253790855408
RMSE train: 1.010280	val: 4.250976	test: 4.579680
MAE train: 0.807509	val: 3.207488	test: 3.315902

Epoch: 77
Loss: 1.3012593388557434
RMSE train: 1.024439	val: 4.295854	test: 4.511379
MAE train: 0.817708	val: 3.213047	test: 3.288280

Epoch: 78
Loss: 1.4313960671424866
RMSE train: 1.026564	val: 4.342702	test: 4.482195
MAE train: 0.816627	val: 3.221732	test: 3.277700

Epoch: 79
Loss: 1.2737540006637573
RMSE train: 1.000814	val: 4.330800	test: 4.491221
MAE train: 0.794060	val: 3.188610	test: 3.278868

Epoch: 80
Loss: 1.318726897239685
RMSE train: 0.953748	val: 4.315011	test: 4.548313
MAE train: 0.758920	val: 3.190628	test: 3.310347

Epoch: 81
Loss: 1.2056156396865845
RMSE train: 0.916964	val: 4.255460	test: 4.549507
MAE train: 0.733706	val: 3.162173	test: 3.314921

Epoch: 82
Loss: 1.1230053305625916
RMSE train: 0.938479	val: 4.315268	test: 4.528510
MAE train: 0.749894	val: 3.227032	test: 3.309610

Epoch: 83
Loss: 1.34418785572052
RMSE train: 2.822015	val: 6.564535	test: 6.020871
MAE train: 2.423040	val: 5.514334	test: 5.295329

Epoch: 23
Loss: 6.57361102104187
RMSE train: 2.782131	val: 6.493337	test: 5.895626
MAE train: 2.388902	val: 5.430283	test: 5.178051

Epoch: 24
Loss: 6.196914911270142
RMSE train: 2.705016	val: 6.388806	test: 5.721937
MAE train: 2.314070	val: 5.314602	test: 5.014767

Epoch: 25
Loss: 5.878735542297363
RMSE train: 2.647413	val: 6.310406	test: 5.575505
MAE train: 2.254801	val: 5.227489	test: 4.877055

Epoch: 26
Loss: 5.445224285125732
RMSE train: 2.587405	val: 6.239605	test: 5.442378
MAE train: 2.191619	val: 5.144659	test: 4.753470

Epoch: 27
Loss: 5.402174711227417
RMSE train: 2.512253	val: 6.126631	test: 5.316933
MAE train: 2.119930	val: 5.030874	test: 4.631550

Epoch: 28
Loss: 5.2411088943481445
RMSE train: 2.433688	val: 6.008184	test: 5.239037
MAE train: 2.053004	val: 4.918915	test: 4.551264

Epoch: 29
Loss: 4.655969619750977
RMSE train: 2.338126	val: 5.885312	test: 5.162297
MAE train: 1.970925	val: 4.790988	test: 4.465889

Epoch: 30
Loss: 4.333550691604614
RMSE train: 2.241129	val: 5.772751	test: 5.114391
MAE train: 1.886840	val: 4.686759	test: 4.409210

Epoch: 31
Loss: 4.498309135437012
RMSE train: 2.134531	val: 5.652418	test: 5.055000
MAE train: 1.789049	val: 4.565926	test: 4.343413

Epoch: 32
Loss: 3.7877038717269897
RMSE train: 2.077403	val: 5.592903	test: 5.036802
MAE train: 1.736594	val: 4.510456	test: 4.311502

Epoch: 33
Loss: 3.771520495414734
RMSE train: 1.961829	val: 5.496783	test: 4.990810
MAE train: 1.624480	val: 4.371859	test: 4.258902

Epoch: 34
Loss: 3.517746686935425
RMSE train: 1.861190	val: 5.438519	test: 4.964312
MAE train: 1.531663	val: 4.280459	test: 4.230041

Epoch: 35
Loss: 3.343938946723938
RMSE train: 1.785347	val: 5.416280	test: 4.978356
MAE train: 1.456326	val: 4.251178	test: 4.239208

Epoch: 36
Loss: 3.203688144683838
RMSE train: 1.762319	val: 5.440638	test: 4.984708
MAE train: 1.430102	val: 4.264385	test: 4.242440

Epoch: 37
Loss: 2.8815581798553467
RMSE train: 1.740185	val: 5.458111	test: 4.917158
MAE train: 1.403208	val: 4.286316	test: 4.177124

Epoch: 38
Loss: 2.691119074821472
RMSE train: 1.701793	val: 5.417303	test: 4.795959
MAE train: 1.361905	val: 4.223017	test: 4.054658

Epoch: 39
Loss: 2.8847025632858276
RMSE train: 1.640537	val: 5.342668	test: 4.659770
MAE train: 1.307078	val: 4.101354	test: 3.904333

Epoch: 40
Loss: 2.538669466972351
RMSE train: 1.542653	val: 5.208608	test: 4.533531
MAE train: 1.222685	val: 3.881749	test: 3.754402

Epoch: 41
Loss: 2.563849449157715
RMSE train: 1.471779	val: 5.082122	test: 4.415708
MAE train: 1.157264	val: 3.692723	test: 3.611409

Epoch: 42
Loss: 2.2601059675216675
RMSE train: 1.459810	val: 5.027010	test: 4.365065
MAE train: 1.138004	val: 3.591162	test: 3.542950

Epoch: 43
Loss: 2.175341546535492
RMSE train: 1.448110	val: 5.001942	test: 4.344983
MAE train: 1.120012	val: 3.558774	test: 3.519248

Epoch: 44
Loss: 2.0902233123779297
RMSE train: 1.440967	val: 5.029800	test: 4.359923
MAE train: 1.111561	val: 3.638704	test: 3.530558

Epoch: 45
Loss: 1.934857428073883
RMSE train: 1.388247	val: 5.044784	test: 4.356644
MAE train: 1.064831	val: 3.681966	test: 3.522306

Epoch: 46
Loss: 1.799281656742096
RMSE train: 1.313362	val: 5.041006	test: 4.336109
MAE train: 1.007185	val: 3.695080	test: 3.492285

Epoch: 47
Loss: 1.797974407672882
RMSE train: 1.295265	val: 5.089397	test: 4.358868
MAE train: 0.999335	val: 3.767782	test: 3.499589

Epoch: 48
Loss: 1.7423865795135498
RMSE train: 1.251695	val: 5.073796	test: 4.355356
MAE train: 0.968692	val: 3.765128	test: 3.494296

Epoch: 49
Loss: 1.5843945145606995
RMSE train: 1.195405	val: 5.007357	test: 4.305125
MAE train: 0.925249	val: 3.689676	test: 3.456784

Epoch: 50
Loss: 1.671994686126709
RMSE train: 1.152875	val: 4.935917	test: 4.245525
MAE train: 0.890555	val: 3.613610	test: 3.418575

Epoch: 51
Loss: 1.44242525100708
RMSE train: 1.139896	val: 4.905669	test: 4.204116
MAE train: 0.877990	val: 3.567021	test: 3.383546

Epoch: 52
Loss: 1.6075657606124878
RMSE train: 1.139379	val: 4.869016	test: 4.143813
MAE train: 0.877728	val: 3.507046	test: 3.317661

Epoch: 53
Loss: 1.6441414952278137
RMSE train: 1.085922	val: 4.789611	test: 4.040676
MAE train: 0.836312	val: 3.370681	test: 3.204957

Epoch: 54
Loss: 1.51943439245224
RMSE train: 1.065750	val: 4.780728	test: 3.967400
MAE train: 0.823909	val: 3.310464	test: 3.119691

Epoch: 55
Loss: 1.5729084014892578
RMSE train: 1.014411	val: 4.746927	test: 3.900056
MAE train: 0.779674	val: 3.217518	test: 3.042389

Epoch: 56
Loss: 1.660144329071045
RMSE train: 1.013737	val: 4.735313	test: 3.884817
MAE train: 0.772516	val: 3.174116	test: 3.040808

Epoch: 57
Loss: 1.446918249130249
RMSE train: 0.997117	val: 4.746082	test: 3.900021
MAE train: 0.758116	val: 3.181497	test: 3.063752

Epoch: 58
Loss: 1.5297717452049255
RMSE train: 0.969309	val: 4.736012	test: 3.930459
MAE train: 0.736560	val: 3.203249	test: 3.111050

Epoch: 59
Loss: 1.2104576230049133
RMSE train: 0.950580	val: 4.746063	test: 4.010435
MAE train: 0.728757	val: 3.266399	test: 3.207170

Epoch: 60
Loss: 1.3925071358680725
RMSE train: 0.928363	val: 4.762900	test: 4.068803
MAE train: 0.717560	val: 3.315756	test: 3.261934

Epoch: 61
Loss: 1.2904817461967468
RMSE train: 0.892977	val: 4.715768	test: 4.044381
MAE train: 0.694578	val: 3.245342	test: 3.220566

Epoch: 62
Loss: 1.3547014594078064
RMSE train: 0.846636	val: 4.648264	test: 3.982734
MAE train: 0.661157	val: 3.162858	test: 3.155370

Epoch: 63
Loss: 1.141430377960205
RMSE train: 0.841777	val: 4.653542	test: 3.934343
MAE train: 0.660336	val: 3.154164	test: 3.107402

Epoch: 64
Loss: 1.3034618496894836
RMSE train: 0.843049	val: 4.692864	test: 3.902158
MAE train: 0.661453	val: 3.181461	test: 3.065820

Epoch: 65
Loss: 1.2419854402542114
RMSE train: 0.822819	val: 4.703832	test: 3.867400
MAE train: 0.641639	val: 3.173429	test: 3.025393

Epoch: 66
Loss: 1.2140003442764282
RMSE train: 0.799163	val: 4.692304	test: 3.825663
MAE train: 0.623209	val: 3.114136	test: 2.981706

Epoch: 67
Loss: 1.199852615594864
RMSE train: 0.759216	val: 4.630093	test: 3.758609
MAE train: 0.595438	val: 2.998370	test: 2.925472

Epoch: 68
Loss: 1.204846978187561
RMSE train: 0.732641	val: 4.572558	test: 3.707586
MAE train: 0.574689	val: 2.911946	test: 2.882194

Epoch: 69
Loss: 1.0979326963424683
RMSE train: 0.736505	val: 4.538280	test: 3.690914
MAE train: 0.577736	val: 2.854755	test: 2.868610

Epoch: 70
Loss: 0.9237622916698456
RMSE train: 0.737294	val: 4.529335	test: 3.685342
MAE train: 0.572952	val: 2.871910	test: 2.863903

Epoch: 71
Loss: 1.088545799255371
RMSE train: 0.730671	val: 4.519273	test: 3.685815
MAE train: 0.566790	val: 2.888106	test: 2.865335

Epoch: 72
Loss: 1.0145076513290405
RMSE train: 0.721606	val: 4.518041	test: 3.687742
MAE train: 0.559649	val: 2.932019	test: 2.862667

Epoch: 73
Loss: 1.093508541584015
RMSE train: 0.734714	val: 4.519112	test: 3.684799
MAE train: 0.560921	val: 2.961512	test: 2.848581

Epoch: 74
Loss: 1.1001296043395996
RMSE train: 0.784672	val: 4.582434	test: 3.716432
MAE train: 0.593069	val: 3.040983	test: 2.864649

Epoch: 75
Loss: 0.97392338514328
RMSE train: 0.866013	val: 4.686883	test: 3.751739
MAE train: 0.656708	val: 3.149309	test: 2.884663

Epoch: 76
Loss: 1.193529188632965
RMSE train: 0.873888	val: 4.711440	test: 3.756159
MAE train: 0.664850	val: 3.164212	test: 2.891745

Epoch: 77
Loss: 1.141334056854248
RMSE train: 0.833422	val: 4.653178	test: 3.728146
MAE train: 0.636713	val: 3.103083	test: 2.880769

Epoch: 78
Loss: 0.9512373208999634
RMSE train: 0.789074	val: 4.591680	test: 3.707017
MAE train: 0.607313	val: 3.046605	test: 2.876456

Epoch: 79
Loss: 1.005868673324585
RMSE train: 0.741757	val: 4.518844	test: 3.678399
MAE train: 0.574769	val: 2.954893	test: 2.852013

Epoch: 80
Loss: 1.2148167490959167
RMSE train: 0.788964	val: 4.608201	test: 3.717250
MAE train: 0.608502	val: 3.034523	test: 2.874937

Epoch: 81
Loss: 1.0244053602218628
RMSE train: 0.856255	val: 4.713426	test: 3.745460
MAE train: 0.663124	val: 3.147937	test: 2.885278

Epoch: 82
Loss: 1.2387446165084839
RMSE train: 0.847904	val: 4.723607	test: 3.797034
MAE train: 0.655988	val: 3.220900	test: 2.935722

Epoch: 83
Loss: 1.133267343044281

Epoch: 84
Loss: 0.9212334454059601
RMSE train: 0.857580	val: 3.953319	test: 3.256119
MAE train: 0.649352	val: 2.780956	test: 2.459204

Epoch: 85
Loss: 1.0433744192123413
RMSE train: 0.808925	val: 3.947032	test: 3.198443
MAE train: 0.606681	val: 2.745785	test: 2.406101

Epoch: 86
Loss: 0.9248099625110626
RMSE train: 0.777473	val: 3.970078	test: 3.142946
MAE train: 0.577022	val: 2.722234	test: 2.345253

Epoch: 87
Loss: 1.1274262070655823
RMSE train: 0.756328	val: 4.012432	test: 3.116390
MAE train: 0.561993	val: 2.746552	test: 2.308790

Epoch: 88
Loss: 0.885331779718399
RMSE train: 0.753700	val: 4.029373	test: 3.100234
MAE train: 0.561193	val: 2.776030	test: 2.285160

Epoch: 89
Loss: 0.9851627051830292
RMSE train: 0.754165	val: 4.016120	test: 3.074424
MAE train: 0.567733	val: 2.774579	test: 2.271289

Epoch: 90
Loss: 1.0152104198932648
RMSE train: 0.694012	val: 3.966728	test: 3.025870
MAE train: 0.525627	val: 2.679366	test: 2.223800

Epoch: 91
Loss: 0.9783273637294769
RMSE train: 0.638357	val: 3.959182	test: 3.006131
MAE train: 0.483475	val: 2.582830	test: 2.175604

Epoch: 92
Loss: 0.9729198515415192
RMSE train: 0.666916	val: 3.985443	test: 3.007193
MAE train: 0.502428	val: 2.562253	test: 2.159625

Epoch: 93
Loss: 0.81843101978302
RMSE train: 0.668852	val: 4.011756	test: 3.041255
MAE train: 0.506899	val: 2.598828	test: 2.191598

Epoch: 94
Loss: 0.9734002947807312
RMSE train: 0.731110	val: 4.069241	test: 3.133304
MAE train: 0.558176	val: 2.726527	test: 2.278002

Epoch: 95
Loss: 0.8923894166946411
RMSE train: 0.807088	val: 4.134192	test: 3.193954
MAE train: 0.606209	val: 2.828066	test: 2.333695

Epoch: 96
Loss: 0.918998122215271
RMSE train: 0.863132	val: 4.184043	test: 3.215851
MAE train: 0.637986	val: 2.906717	test: 2.355983

Epoch: 97
Loss: 1.0671293139457703
RMSE train: 0.870663	val: 4.165403	test: 3.199305
MAE train: 0.640231	val: 2.902068	test: 2.340048

Epoch: 98
Loss: 0.8296975195407867
RMSE train: 0.788947	val: 4.073616	test: 3.151394
MAE train: 0.584073	val: 2.835251	test: 2.294733

Epoch: 99
Loss: 0.9306805431842804
RMSE train: 0.684178	val: 4.004572	test: 3.116913
MAE train: 0.506054	val: 2.772119	test: 2.275781

Epoch: 100
Loss: 0.8659909665584564
RMSE train: 0.657466	val: 4.022431	test: 3.148241
MAE train: 0.482800	val: 2.758522	test: 2.318851

Epoch: 101
Loss: 1.016915887594223
RMSE train: 0.718262	val: 4.089509	test: 3.217502
MAE train: 0.525026	val: 2.813756	test: 2.384009

Epoch: 102
Loss: 0.9966203272342682
RMSE train: 0.792702	val: 4.174442	test: 3.271704
MAE train: 0.578509	val: 2.879243	test: 2.433500

Epoch: 103
Loss: 0.8295789062976837
RMSE train: 0.845061	val: 4.237714	test: 3.295697
MAE train: 0.612627	val: 2.909609	test: 2.455221

Epoch: 104
Loss: 0.8046908378601074
RMSE train: 0.819024	val: 4.228132	test: 3.303924
MAE train: 0.600319	val: 2.910401	test: 2.467109

Epoch: 105
Loss: 0.8360234498977661
RMSE train: 0.771684	val: 4.163087	test: 3.298217
MAE train: 0.570380	val: 2.846999	test: 2.465583

Epoch: 106
Loss: 0.8671600520610809
RMSE train: 0.762186	val: 4.121114	test: 3.321739
MAE train: 0.565215	val: 2.823887	test: 2.504037

Epoch: 107
Loss: 0.8332687318325043
RMSE train: 0.768861	val: 4.099532	test: 3.340335
MAE train: 0.574019	val: 2.812374	test: 2.532576

Epoch: 108
Loss: 0.7247755229473114
RMSE train: 0.775610	val: 4.102939	test: 3.375683
MAE train: 0.577844	val: 2.821453	test: 2.576964

Epoch: 109
Loss: 0.7633688747882843
RMSE train: 0.746810	val: 4.060080	test: 3.354689
MAE train: 0.554453	val: 2.798606	test: 2.552936

Epoch: 110
Loss: 0.990781307220459
RMSE train: 0.698798	val: 4.003946	test: 3.287726
MAE train: 0.517012	val: 2.786068	test: 2.475562

Epoch: 111
Loss: 0.7590432167053223
RMSE train: 0.681807	val: 3.987908	test: 3.226246
MAE train: 0.505513	val: 2.765820	test: 2.408186

Epoch: 112
Loss: 0.8355255424976349
RMSE train: 0.698148	val: 4.019977	test: 3.231661
MAE train: 0.515825	val: 2.783137	test: 2.410455

Epoch: 113
Loss: 0.8371875584125519
RMSE train: 0.712319	val: 4.053723	test: 3.258618
MAE train: 0.530279	val: 2.811876	test: 2.443131

Epoch: 114
Loss: 0.7128919064998627
RMSE train: 0.707751	val: 4.063544	test: 3.263850
MAE train: 0.533667	val: 2.832397	test: 2.462298

Epoch: 115
Loss: 0.7961390316486359
RMSE train: 0.701967	val: 4.072618	test: 3.292348
MAE train: 0.530195	val: 2.848816	test: 2.496374

Epoch: 116
Loss: 0.8335865139961243
RMSE train: 0.675980	val: 4.059612	test: 3.286774
MAE train: 0.509707	val: 2.772573	test: 2.476930

Epoch: 117
Loss: 0.7501956820487976
RMSE train: 0.702303	val: 4.076148	test: 3.281440
MAE train: 0.521939	val: 2.754609	test: 2.449836

Epoch: 118
Loss: 0.7795451283454895
RMSE train: 0.756739	val: 4.124119	test: 3.334556
MAE train: 0.565657	val: 2.782114	test: 2.493195

Epoch: 119
Loss: 0.7615560591220856
RMSE train: 0.839286	val: 4.180039	test: 3.410486
MAE train: 0.632881	val: 2.851291	test: 2.569371

Epoch: 120
Loss: 0.7185763418674469
RMSE train: 0.914666	val: 4.188586	test: 3.452762
MAE train: 0.693857	val: 2.856512	test: 2.598189

Epoch: 121
Loss: 0.8171093761920929
RMSE train: 0.929080	val: 4.195180	test: 3.440920
MAE train: 0.709166	val: 2.838266	test: 2.558923

Early stopping
Best (RMSE):	 train: 0.808925	val: 3.947032	test: 3.198443
Best (MAE):	 train: 0.606681	val: 2.745785	test: 2.406101


Epoch: 84
Loss: 0.8824479281902313
RMSE train: 0.876189	val: 4.014718	test: 3.102595
MAE train: 0.631480	val: 2.738660	test: 2.339104

Epoch: 85
Loss: 0.9953505098819733
RMSE train: 0.830438	val: 3.977744	test: 3.107423
MAE train: 0.593417	val: 2.698308	test: 2.364680

Epoch: 86
Loss: 0.8447074890136719
RMSE train: 0.792838	val: 3.938491	test: 3.116605
MAE train: 0.559962	val: 2.641006	test: 2.379450

Epoch: 87
Loss: 0.9244413375854492
RMSE train: 0.748700	val: 3.885205	test: 3.090217
MAE train: 0.534654	val: 2.614800	test: 2.373587

Epoch: 88
Loss: 0.8509344160556793
RMSE train: 0.709989	val: 3.797225	test: 3.040856
MAE train: 0.511896	val: 2.519864	test: 2.336986

Epoch: 89
Loss: 0.8964112401008606
RMSE train: 0.709498	val: 3.741830	test: 2.978359
MAE train: 0.521027	val: 2.453097	test: 2.273858

Epoch: 90
Loss: 0.9755479693412781
RMSE train: 0.743071	val: 3.755574	test: 2.953394
MAE train: 0.554043	val: 2.454333	test: 2.243747

Epoch: 91
Loss: 0.8858048617839813
RMSE train: 0.772895	val: 3.864139	test: 2.985482
MAE train: 0.577856	val: 2.555712	test: 2.245083

Epoch: 92
Loss: 0.8019472360610962
RMSE train: 0.800594	val: 3.986718	test: 3.025858
MAE train: 0.590203	val: 2.654713	test: 2.246062

Epoch: 93
Loss: 0.8743977546691895
RMSE train: 0.818822	val: 4.034426	test: 3.051645
MAE train: 0.594688	val: 2.697597	test: 2.238519

Epoch: 94
Loss: 0.8435874581336975
RMSE train: 0.797866	val: 4.023455	test: 3.060839
MAE train: 0.570120	val: 2.706619	test: 2.251435

Epoch: 95
Loss: 0.8435933887958527
RMSE train: 0.748785	val: 3.929070	test: 3.033842
MAE train: 0.541742	val: 2.690469	test: 2.297995

Epoch: 96
Loss: 0.8342824280261993
RMSE train: 0.739137	val: 3.878703	test: 3.019593
MAE train: 0.539517	val: 2.693032	test: 2.318319

Epoch: 97
Loss: 0.9874311983585358
RMSE train: 0.749221	val: 3.881780	test: 3.001150
MAE train: 0.549863	val: 2.658711	test: 2.260686

Epoch: 98
Loss: 0.8595388233661652
RMSE train: 0.789612	val: 3.948218	test: 3.004094
MAE train: 0.586881	val: 2.708267	test: 2.192393

Epoch: 99
Loss: 0.9298087060451508
RMSE train: 0.848545	val: 4.029083	test: 3.017524
MAE train: 0.631626	val: 2.766596	test: 2.186792

Epoch: 100
Loss: 0.8407685160636902
RMSE train: 0.876965	val: 4.069120	test: 3.003917
MAE train: 0.644133	val: 2.779969	test: 2.173614

Epoch: 101
Loss: 0.890377551317215
RMSE train: 0.888470	val: 4.073593	test: 3.003720
MAE train: 0.634962	val: 2.769016	test: 2.204599

Epoch: 102
Loss: 0.8815020024776459
RMSE train: 0.862469	val: 4.021463	test: 2.986071
MAE train: 0.612315	val: 2.728943	test: 2.218248

Epoch: 103
Loss: 0.837944746017456
RMSE train: 0.780875	val: 3.918926	test: 2.927959
MAE train: 0.565255	val: 2.666177	test: 2.157004

Epoch: 104
Loss: 0.9121163487434387
RMSE train: 0.707873	val: 3.844881	test: 2.906553
MAE train: 0.515174	val: 2.588574	test: 2.122783

Epoch: 105
Loss: 0.7687185704708099
RMSE train: 0.740268	val: 3.861904	test: 2.928481
MAE train: 0.538035	val: 2.604212	test: 2.154595

Epoch: 106
Loss: 0.807580292224884
RMSE train: 0.745240	val: 3.822511	test: 2.927762
MAE train: 0.542445	val: 2.580199	test: 2.157552

Epoch: 107
Loss: 0.8113932907581329
RMSE train: 0.735061	val: 3.781485	test: 2.923115
MAE train: 0.540022	val: 2.525000	test: 2.156512

Epoch: 108
Loss: 0.8998089134693146
RMSE train: 0.741846	val: 3.798392	test: 2.927440
MAE train: 0.546013	val: 2.537279	test: 2.181729

Epoch: 109
Loss: 0.7931929230690002
RMSE train: 0.784874	val: 3.854743	test: 2.919272
MAE train: 0.576918	val: 2.660534	test: 2.223258

Epoch: 110
Loss: 0.7550379931926727
RMSE train: 0.764254	val: 3.856195	test: 2.903604
MAE train: 0.548527	val: 2.637164	test: 2.190497

Epoch: 111
Loss: 0.8520767092704773
RMSE train: 0.718846	val: 3.853063	test: 2.909817
MAE train: 0.512245	val: 2.623844	test: 2.188477

Epoch: 112
Loss: 0.6897091567516327
RMSE train: 0.696766	val: 3.852677	test: 2.904426
MAE train: 0.497030	val: 2.630196	test: 2.164854

Epoch: 113
Loss: 0.8352022171020508
RMSE train: 0.689805	val: 3.848590	test: 2.900826
MAE train: 0.499980	val: 2.600412	test: 2.161544

Epoch: 114
Loss: 0.8614369332790375
RMSE train: 0.700964	val: 3.878706	test: 2.918529
MAE train: 0.518365	val: 2.592862	test: 2.179161

Epoch: 115
Loss: 0.7303926944732666
RMSE train: 0.682737	val: 3.878069	test: 2.924880
MAE train: 0.509086	val: 2.568195	test: 2.195251

Epoch: 116
Loss: 0.7934245765209198
RMSE train: 0.676056	val: 3.909641	test: 2.933589
MAE train: 0.500338	val: 2.580863	test: 2.191832

Epoch: 117
Loss: 0.6701445877552032
RMSE train: 0.691493	val: 3.932078	test: 2.947499
MAE train: 0.501204	val: 2.605280	test: 2.194622

Epoch: 118
Loss: 0.6551699638366699
RMSE train: 0.732198	val: 3.970257	test: 2.951027
MAE train: 0.524910	val: 2.609525	test: 2.157734

Epoch: 119
Loss: 0.7182321548461914
RMSE train: 0.723068	val: 3.938658	test: 2.933848
MAE train: 0.522781	val: 2.567295	test: 2.138226

Epoch: 120
Loss: 0.7387877106666565
RMSE train: 0.709779	val: 3.930189	test: 2.946491
MAE train: 0.521217	val: 2.556674	test: 2.166006

Epoch: 121
Loss: 0.7943784296512604
RMSE train: 0.694054	val: 3.913234	test: 2.948171
MAE train: 0.508962	val: 2.526443	test: 2.160054

Epoch: 122
Loss: 0.6308777332305908
RMSE train: 0.697565	val: 3.917746	test: 2.947754
MAE train: 0.502935	val: 2.515291	test: 2.120763

Epoch: 123
Loss: 0.8585826754570007
RMSE train: 0.705033	val: 3.940013	test: 2.943790
MAE train: 0.518194	val: 2.520080	test: 2.103053

Epoch: 124
Loss: 0.7355077862739563
RMSE train: 0.746561	val: 4.014284	test: 2.937923
MAE train: 0.551795	val: 2.598591	test: 2.097378

Early stopping
Best (RMSE):	 train: 0.709498	val: 3.741830	test: 2.978359
Best (MAE):	 train: 0.521027	val: 2.453097	test: 2.273858


Epoch: 84
Loss: 1.021357238292694
RMSE train: 0.792115	val: 4.131304	test: 3.106400
MAE train: 0.591176	val: 2.769907	test: 2.230763

Epoch: 85
Loss: 0.9137679636478424
RMSE train: 0.799175	val: 4.077938	test: 3.110241
MAE train: 0.599783	val: 2.722611	test: 2.260479

Epoch: 86
Loss: 0.9273926019668579
RMSE train: 0.875657	val: 4.102640	test: 3.149565
MAE train: 0.660713	val: 2.730271	test: 2.312047

Epoch: 87
Loss: 0.9637426733970642
RMSE train: 0.977080	val: 4.203557	test: 3.195200
MAE train: 0.738489	val: 2.814373	test: 2.357271

Epoch: 88
Loss: 0.8373931646347046
RMSE train: 1.061804	val: 4.323720	test: 3.227173
MAE train: 0.803225	val: 2.903914	test: 2.374070

Epoch: 89
Loss: 0.9878488183021545
RMSE train: 1.052335	val: 4.363173	test: 3.211288
MAE train: 0.787470	val: 2.926572	test: 2.345585

Epoch: 90
Loss: 1.1323177814483643
RMSE train: 0.957648	val: 4.293358	test: 3.156758
MAE train: 0.705008	val: 2.841476	test: 2.273178

Epoch: 91
Loss: 0.8731711804866791
RMSE train: 0.894088	val: 4.230327	test: 3.126207
MAE train: 0.653468	val: 2.781854	test: 2.230400

Epoch: 92
Loss: 0.963749885559082
RMSE train: 0.824343	val: 4.133086	test: 3.094386
MAE train: 0.605348	val: 2.726427	test: 2.199689

Epoch: 93
Loss: 0.8028143346309662
RMSE train: 0.750093	val: 4.039076	test: 3.086063
MAE train: 0.560036	val: 2.697936	test: 2.218237

Epoch: 94
Loss: 0.8490005433559418
RMSE train: 0.761931	val: 4.037035	test: 3.111838
MAE train: 0.575731	val: 2.724171	test: 2.261409

Epoch: 95
Loss: 0.8689529001712799
RMSE train: 0.820789	val: 4.086331	test: 3.160391
MAE train: 0.624225	val: 2.778751	test: 2.318007

Epoch: 96
Loss: 0.8622756004333496
RMSE train: 0.892530	val: 4.145401	test: 3.202631
MAE train: 0.678590	val: 2.821139	test: 2.348417

Epoch: 97
Loss: 0.7982140481472015
RMSE train: 0.961240	val: 4.222032	test: 3.216206
MAE train: 0.724037	val: 2.855401	test: 2.343857

Epoch: 98
Loss: 0.8527050912380219
RMSE train: 0.911216	val: 4.177934	test: 3.157491
MAE train: 0.674494	val: 2.809317	test: 2.270937

Epoch: 99
Loss: 0.7878806293010712
RMSE train: 0.794204	val: 4.075675	test: 3.077740
MAE train: 0.583423	val: 2.717009	test: 2.179630

Epoch: 100
Loss: 0.82175612449646
RMSE train: 0.688875	val: 3.997229	test: 3.021285
MAE train: 0.509164	val: 2.667271	test: 2.130447

Epoch: 101
Loss: 0.8070666491985321
RMSE train: 0.659081	val: 3.981970	test: 2.998136
MAE train: 0.492329	val: 2.661558	test: 2.117499

Epoch: 102
Loss: 0.8474732935428619
RMSE train: 0.697746	val: 4.045515	test: 3.025710
MAE train: 0.533576	val: 2.709756	test: 2.148890

Epoch: 103
Loss: 0.9493330121040344
RMSE train: 0.735355	val: 4.106051	test: 3.055202
MAE train: 0.570351	val: 2.738094	test: 2.188587

Epoch: 104
Loss: 0.82589390873909
RMSE train: 0.772238	val: 4.133084	test: 3.065858
MAE train: 0.591391	val: 2.725346	test: 2.203712

Epoch: 105
Loss: 0.7992556691169739
RMSE train: 0.779029	val: 4.123844	test: 3.074732
MAE train: 0.598597	val: 2.714279	test: 2.222015

Epoch: 106
Loss: 0.9116180539131165
RMSE train: 0.777253	val: 4.111143	test: 3.063041
MAE train: 0.591485	val: 2.710370	test: 2.209678

Epoch: 107
Loss: 0.7370871305465698
RMSE train: 0.754614	val: 4.089983	test: 3.031877
MAE train: 0.570196	val: 2.709410	test: 2.172511

Epoch: 108
Loss: 0.6526727080345154
RMSE train: 0.731311	val: 4.060137	test: 3.001994
MAE train: 0.555502	val: 2.703815	test: 2.154862

Epoch: 109
Loss: 0.9105883836746216
RMSE train: 0.765664	val: 4.075708	test: 3.013561
MAE train: 0.583210	val: 2.729461	test: 2.163651

Epoch: 110
Loss: 0.8632670342922211
RMSE train: 0.840668	val: 4.163044	test: 3.053719
MAE train: 0.636837	val: 2.771367	test: 2.186376

Epoch: 111
Loss: 0.8342655301094055
RMSE train: 0.938281	val: 4.287754	test: 3.089750
MAE train: 0.704916	val: 2.811128	test: 2.203463

Epoch: 112
Loss: 0.7963340282440186
RMSE train: 0.972235	val: 4.329647	test: 3.089674
MAE train: 0.726615	val: 2.817385	test: 2.192336

Epoch: 113
Loss: 0.8329217731952667
RMSE train: 0.961394	val: 4.298608	test: 3.071915
MAE train: 0.709108	val: 2.798271	test: 2.186366

Epoch: 114
Loss: 0.792466938495636
RMSE train: 0.855620	val: 4.184528	test: 3.026053
MAE train: 0.628381	val: 2.758924	test: 2.158109

Epoch: 115
Loss: 0.7271399796009064
RMSE train: 0.725066	val: 4.072123	test: 2.970681
MAE train: 0.541591	val: 2.735035	test: 2.122928

Epoch: 116
Loss: 0.8390809595584869
RMSE train: 0.635558	val: 4.044229	test: 2.959360
MAE train: 0.476182	val: 2.724662	test: 2.098243

Epoch: 117
Loss: 0.8762844204902649
RMSE train: 0.656952	val: 4.086099	test: 2.981845
MAE train: 0.486825	val: 2.733726	test: 2.094555

Epoch: 118
Loss: 0.7194578349590302
RMSE train: 0.751688	val: 4.157776	test: 3.014458
MAE train: 0.562145	val: 2.766324	test: 2.115187

Epoch: 119
Loss: 0.7579658627510071
RMSE train: 0.754696	val: 4.165775	test: 3.008085
MAE train: 0.566945	val: 2.785597	test: 2.127832

Epoch: 120
Loss: 0.7884213030338287
RMSE train: 0.702329	val: 4.090455	test: 2.985732
MAE train: 0.530818	val: 2.760132	test: 2.128347

Epoch: 121
Loss: 0.7590939402580261
RMSE train: 0.666327	val: 4.000963	test: 2.959989
MAE train: 0.503517	val: 2.725079	test: 2.130904

Epoch: 122
Loss: 0.76664137840271
RMSE train: 0.670332	val: 3.990470	test: 2.960937
MAE train: 0.508636	val: 2.716179	test: 2.137673

Epoch: 123
Loss: 0.685809999704361
RMSE train: 0.695255	val: 4.036784	test: 2.984538
MAE train: 0.531863	val: 2.723795	test: 2.144258

Epoch: 124
Loss: 0.7112207114696503
RMSE train: 0.799267	val: 4.124028	test: 3.039526
MAE train: 0.609230	val: 2.760378	test: 2.184926

Epoch: 125
Loss: 0.7848294377326965
RMSE train: 0.869927	val: 4.175196	test: 3.051752
MAE train: 0.661106	val: 2.797187	test: 2.183541

Epoch: 126
Loss: 0.7100482881069183
RMSE train: 0.822638	val: 4.149667	test: 3.007827
MAE train: 0.630738	val: 2.812509	test: 2.145533

Epoch: 127
Loss: 0.6528084576129913
RMSE train: 0.773927	val: 4.091819	test: 2.936999
MAE train: 0.595140	val: 2.791965	test: 2.110277

Epoch: 128
Loss: 0.8333175182342529
RMSE train: 0.798083	val: 4.077335	test: 2.903340
MAE train: 0.600415	val: 2.756540	test: 2.077189

Epoch: 129
Loss: 0.7085217535495758
RMSE train: 0.785018	val: 4.087286	test: 2.895023
MAE train: 0.582020	val: 2.711406	test: 2.027347

Epoch: 130
Loss: 0.7045890092849731
RMSE train: 0.813355	val: 4.135577	test: 2.919870
MAE train: 0.608128	val: 2.716042	test: 2.025209

Epoch: 131
Loss: 0.7369286119937897
RMSE train: 0.798790	val: 4.114549	test: 2.915285
MAE train: 0.607587	val: 2.699719	test: 2.022492

Epoch: 132
Loss: 0.7975454032421112
RMSE train: 0.687602	val: 4.008511	test: 2.878019
MAE train: 0.533146	val: 2.642394	test: 2.000947

Epoch: 133
Loss: 0.6535068154335022
RMSE train: 0.650772	val: 3.988324	test: 2.894275
MAE train: 0.510679	val: 2.649245	test: 2.032007

Epoch: 134
Loss: 0.6996122002601624
RMSE train: 0.683801	val: 4.012261	test: 2.932380
MAE train: 0.538532	val: 2.677265	test: 2.088520

Epoch: 135
Loss: 0.7063961625099182
RMSE train: 0.704840	val: 4.047845	test: 2.959745
MAE train: 0.549240	val: 2.699240	test: 2.097388

Epoch: 136
Loss: 0.7563612163066864
RMSE train: 0.762554	val: 4.120681	test: 2.992927
MAE train: 0.579252	val: 2.744285	test: 2.106473

Early stopping
Best (RMSE):	 train: 0.659081	val: 3.981970	test: 2.998136
Best (MAE):	 train: 0.492329	val: 2.661558	test: 2.117499
All runs completed.

RMSE train: 0.818152	val: 5.130152	test: 3.920156
MAE train: 0.637597	val: 3.378416	test: 3.023751

Epoch: 84
Loss: 1.1888673901557922
RMSE train: 0.807879	val: 5.084065	test: 3.848388
MAE train: 0.628087	val: 3.359708	test: 2.958729

Epoch: 85
Loss: 1.1782814860343933
RMSE train: 0.775889	val: 5.047660	test: 3.790214
MAE train: 0.600973	val: 3.346970	test: 2.912098

Epoch: 86
Loss: 1.1293081045150757
RMSE train: 0.743832	val: 5.017454	test: 3.750419
MAE train: 0.576618	val: 3.335986	test: 2.879458

Epoch: 87
Loss: 1.1908278465270996
RMSE train: 0.745760	val: 5.034175	test: 3.769908
MAE train: 0.578399	val: 3.363399	test: 2.913581

Epoch: 88
Loss: 1.0945003628730774
RMSE train: 0.767948	val: 5.066526	test: 3.850119
MAE train: 0.592906	val: 3.395808	test: 3.004783

Epoch: 89
Loss: 1.1892330646514893
RMSE train: 0.760580	val: 5.044239	test: 3.896604
MAE train: 0.593211	val: 3.374451	test: 3.056891

Epoch: 90
Loss: 1.0181336104869843
RMSE train: 0.753694	val: 5.023508	test: 3.922425
MAE train: 0.591342	val: 3.355327	test: 3.085797

Epoch: 91
Loss: 1.0479634404182434
RMSE train: 0.707536	val: 4.971086	test: 3.913287
MAE train: 0.555436	val: 3.293650	test: 3.083475

Epoch: 92
Loss: 1.4162452220916748
RMSE train: 0.684609	val: 4.925868	test: 3.875102
MAE train: 0.532381	val: 3.254855	test: 3.047705

Epoch: 93
Loss: 1.0231912434101105
RMSE train: 0.695568	val: 4.907590	test: 3.852120
MAE train: 0.539846	val: 3.252549	test: 3.018624

Epoch: 94
Loss: 1.1561419367790222
RMSE train: 0.729576	val: 4.892881	test: 3.818361
MAE train: 0.568037	val: 3.251806	test: 2.980068

Epoch: 95
Loss: 1.0532203316688538
RMSE train: 0.735663	val: 4.864916	test: 3.754927
MAE train: 0.575252	val: 3.243121	test: 2.918896

Epoch: 96
Loss: 1.1784367561340332
RMSE train: 0.770807	val: 4.847741	test: 3.713174
MAE train: 0.603155	val: 3.258248	test: 2.885573

Epoch: 97
Loss: 1.161712884902954
RMSE train: 0.853992	val: 4.870468	test: 3.703761
MAE train: 0.653058	val: 3.319326	test: 2.881978

Epoch: 98
Loss: 1.1110003590583801
RMSE train: 0.869066	val: 4.918933	test: 3.752202
MAE train: 0.657974	val: 3.337337	test: 2.923269

Epoch: 99
Loss: 0.9835033714771271
RMSE train: 0.795613	val: 4.907094	test: 3.771159
MAE train: 0.616111	val: 3.271563	test: 2.945867

Epoch: 100
Loss: 0.9267058372497559
RMSE train: 0.762940	val: 4.958189	test: 3.837209
MAE train: 0.602566	val: 3.281712	test: 3.001830

Epoch: 101
Loss: 0.9969349503517151
RMSE train: 0.764897	val: 5.040383	test: 3.918682
MAE train: 0.606797	val: 3.340910	test: 3.068733

Epoch: 102
Loss: 1.0048438906669617
RMSE train: 0.798839	val: 5.097156	test: 3.987619
MAE train: 0.631675	val: 3.392846	test: 3.126086

Epoch: 103
Loss: 1.1118920743465424
RMSE train: 0.812617	val: 5.099015	test: 3.994093
MAE train: 0.638660	val: 3.395971	test: 3.124127

Epoch: 104
Loss: 0.9949523210525513
RMSE train: 0.793240	val: 5.092208	test: 3.951062
MAE train: 0.619404	val: 3.397986	test: 3.087598

Epoch: 105
Loss: 1.112480878829956
RMSE train: 0.696771	val: 4.996385	test: 3.845196
MAE train: 0.532106	val: 3.308142	test: 3.001084

Epoch: 106
Loss: 0.774709552526474
RMSE train: 0.671729	val: 4.947675	test: 3.794496
MAE train: 0.512886	val: 3.275478	test: 2.954181

Epoch: 107
Loss: 0.8946408629417419
RMSE train: 0.690486	val: 4.938475	test: 3.773669
MAE train: 0.523528	val: 3.278492	test: 2.930463

Epoch: 108
Loss: 0.786903589963913
RMSE train: 0.742244	val: 4.950528	test: 3.779213
MAE train: 0.567257	val: 3.291975	test: 2.927591

Epoch: 109
Loss: 1.0656322240829468
RMSE train: 0.778506	val: 4.966966	test: 3.810576
MAE train: 0.603433	val: 3.294778	test: 2.958817

Epoch: 110
Loss: 0.9470968246459961
RMSE train: 0.789173	val: 4.981964	test: 3.826668
MAE train: 0.619127	val: 3.271340	test: 2.974257

Epoch: 111
Loss: 0.9748559892177582
RMSE train: 0.753246	val: 4.965968	test: 3.839033
MAE train: 0.593374	val: 3.229167	test: 2.990564

Epoch: 112
Loss: 0.812044084072113
RMSE train: 0.717747	val: 4.963385	test: 3.848672
MAE train: 0.565232	val: 3.214634	test: 2.998463

Epoch: 113
Loss: 0.8044134676456451
RMSE train: 0.692602	val: 4.981969	test: 3.832494
MAE train: 0.544201	val: 3.220498	test: 2.971524

Epoch: 114
Loss: 0.9206244051456451
RMSE train: 0.680912	val: 5.004385	test: 3.846694
MAE train: 0.535882	val: 3.225244	test: 2.978770

Epoch: 115
Loss: 0.8906946182250977
RMSE train: 0.712819	val: 5.030377	test: 3.872225
MAE train: 0.559192	val: 3.257296	test: 2.994505

Epoch: 116
Loss: 0.8617645800113678
RMSE train: 0.785291	val: 5.081362	test: 3.923174
MAE train: 0.615356	val: 3.299773	test: 3.034397

Epoch: 117
Loss: 0.7837221920490265
RMSE train: 0.844496	val: 5.106352	test: 3.956760
MAE train: 0.664052	val: 3.325501	test: 3.066572

Epoch: 118
Loss: 0.8513129353523254
RMSE train: 0.817826	val: 5.069382	test: 3.944778
MAE train: 0.641612	val: 3.296351	test: 3.058191

Epoch: 119
Loss: 0.8445794880390167
RMSE train: 0.730343	val: 5.008875	test: 3.906077
MAE train: 0.572631	val: 3.259154	test: 3.027367

Epoch: 120
Loss: 0.8874971568584442
RMSE train: 0.699172	val: 5.005634	test: 3.918835
MAE train: 0.544007	val: 3.266321	test: 3.047950

Epoch: 121
Loss: 0.8851823806762695
RMSE train: 0.736166	val: 5.018800	test: 3.929374
MAE train: 0.561575	val: 3.286704	test: 3.059490

Early stopping
Best (RMSE):	 train: 0.797519	val: 4.816582	test: 3.835933
Best (MAE):	 train: 0.619832	val: 3.283301	test: 3.020502

RMSE train: 0.877918	val: 4.189634	test: 3.451445
MAE train: 0.708349	val: 2.913819	test: 2.747269

Epoch: 84
Loss: 1.0345205068588257
RMSE train: 0.867337	val: 4.189342	test: 3.396456
MAE train: 0.696949	val: 2.908183	test: 2.682922

Epoch: 85
Loss: 1.069115310907364
RMSE train: 0.854242	val: 4.187449	test: 3.312486
MAE train: 0.684090	val: 2.865979	test: 2.579393

Epoch: 86
Loss: 1.027766853570938
RMSE train: 0.845305	val: 4.232089	test: 3.273463
MAE train: 0.675464	val: 2.849650	test: 2.503844

Epoch: 87
Loss: 0.8467035889625549
RMSE train: 0.844548	val: 4.277717	test: 3.287554
MAE train: 0.675882	val: 2.842456	test: 2.501187

Epoch: 88
Loss: 1.072609305381775
RMSE train: 0.862643	val: 4.335189	test: 3.332807
MAE train: 0.689605	val: 2.861442	test: 2.538372

Epoch: 89
Loss: 1.0343373715877533
RMSE train: 0.865303	val: 4.347981	test: 3.361944
MAE train: 0.691202	val: 2.884338	test: 2.569353

Epoch: 90
Loss: 1.001358538866043
RMSE train: 0.886904	val: 4.388507	test: 3.389670
MAE train: 0.709944	val: 2.931777	test: 2.596923

Epoch: 91
Loss: 1.1926831603050232
RMSE train: 0.888600	val: 4.342960	test: 3.363455
MAE train: 0.714187	val: 2.905669	test: 2.574426

Epoch: 92
Loss: 0.9702232182025909
RMSE train: 0.894272	val: 4.302525	test: 3.351226
MAE train: 0.719218	val: 2.885339	test: 2.574663

Epoch: 93
Loss: 1.0117857456207275
RMSE train: 0.883201	val: 4.277451	test: 3.330791
MAE train: 0.709901	val: 2.867005	test: 2.555800

Epoch: 94
Loss: 0.9421409666538239
RMSE train: 0.850681	val: 4.247030	test: 3.310363
MAE train: 0.684481	val: 2.845550	test: 2.533952

Epoch: 95
Loss: 0.9061141908168793
RMSE train: 0.804061	val: 4.201961	test: 3.289720
MAE train: 0.647846	val: 2.790947	test: 2.509095

Epoch: 96
Loss: 0.9822641611099243
RMSE train: 0.822020	val: 4.262447	test: 3.347126
MAE train: 0.661287	val: 2.804714	test: 2.548093

Epoch: 97
Loss: 0.9506100714206696
RMSE train: 0.847184	val: 4.320032	test: 3.377972
MAE train: 0.678475	val: 2.840313	test: 2.575268

Epoch: 98
Loss: 0.9259112775325775
RMSE train: 0.811542	val: 4.273285	test: 3.329375
MAE train: 0.648729	val: 2.820063	test: 2.535740

Epoch: 99
Loss: 0.8921751976013184
RMSE train: 0.759327	val: 4.190577	test: 3.279450
MAE train: 0.604437	val: 2.818054	test: 2.507088

Epoch: 100
Loss: 0.9061987698078156
RMSE train: 0.739630	val: 4.147802	test: 3.273282
MAE train: 0.589685	val: 2.822185	test: 2.526581

Epoch: 101
Loss: 0.9114010632038116
RMSE train: 0.750320	val: 4.173605	test: 3.263490
MAE train: 0.599065	val: 2.859190	test: 2.522737

Epoch: 102
Loss: 0.91893669962883
RMSE train: 0.737412	val: 4.142404	test: 3.230295
MAE train: 0.590869	val: 2.845870	test: 2.480194

Epoch: 103
Loss: 1.1182114481925964
RMSE train: 0.766445	val: 4.177872	test: 3.208214
MAE train: 0.614842	val: 2.878110	test: 2.447585

Epoch: 104
Loss: 0.8831817507743835
RMSE train: 0.758607	val: 4.152548	test: 3.184121
MAE train: 0.606746	val: 2.876326	test: 2.423890

Epoch: 105
Loss: 0.8532941937446594
RMSE train: 0.724460	val: 4.093895	test: 3.171327
MAE train: 0.579487	val: 2.850201	test: 2.418766

Epoch: 106
Loss: 0.7800657153129578
RMSE train: 0.690671	val: 4.073917	test: 3.159592
MAE train: 0.552645	val: 2.837196	test: 2.407510

Epoch: 107
Loss: 0.9391923546791077
RMSE train: 0.666412	val: 4.076098	test: 3.139642
MAE train: 0.531809	val: 2.802804	test: 2.380285

Epoch: 108
Loss: 0.8643798232078552
RMSE train: 0.658586	val: 4.090360	test: 3.093704
MAE train: 0.524064	val: 2.777949	test: 2.322224

Epoch: 109
Loss: 0.8115326762199402
RMSE train: 0.647074	val: 4.089578	test: 3.026017
MAE train: 0.512850	val: 2.777100	test: 2.247916

Epoch: 110
Loss: 0.802003026008606
RMSE train: 0.683735	val: 4.140457	test: 3.010166
MAE train: 0.540611	val: 2.821899	test: 2.221475

Epoch: 111
Loss: 0.8558148741722107
RMSE train: 0.728690	val: 4.197138	test: 3.037783
MAE train: 0.576705	val: 2.863279	test: 2.242880

Epoch: 112
Loss: 0.7904157340526581
RMSE train: 0.740275	val: 4.206181	test: 3.061472
MAE train: 0.588072	val: 2.866563	test: 2.265796

Epoch: 113
Loss: 0.9097161293029785
RMSE train: 0.759996	val: 4.266149	test: 3.117196
MAE train: 0.605720	val: 2.920459	test: 2.324301

Epoch: 114
Loss: 0.8653214275836945
RMSE train: 0.739258	val: 4.284637	test: 3.159722
MAE train: 0.590252	val: 2.933776	test: 2.390307

Epoch: 115
Loss: 0.7972041964530945
RMSE train: 0.695432	val: 4.246848	test: 3.166888
MAE train: 0.552863	val: 2.924114	test: 2.416966

Epoch: 116
Loss: 0.8070979118347168
RMSE train: 0.693429	val: 4.270969	test: 3.162938
MAE train: 0.545245	val: 2.933392	test: 2.408364

Epoch: 117
Loss: 0.8391911685466766
RMSE train: 0.718603	val: 4.364311	test: 3.193917
MAE train: 0.562907	val: 2.987645	test: 2.424007

Epoch: 118
Loss: 0.8771951794624329
RMSE train: 0.699319	val: 4.400221	test: 3.191724
MAE train: 0.550613	val: 2.992927	test: 2.404684

Epoch: 119
Loss: 0.8891811370849609
RMSE train: 0.729507	val: 4.491347	test: 3.224290
MAE train: 0.576029	val: 3.041283	test: 2.420023

Epoch: 120
Loss: 0.8105179965496063
RMSE train: 0.706030	val: 4.481189	test: 3.231841
MAE train: 0.563659	val: 3.021072	test: 2.426359

Epoch: 121
Loss: 0.8525263965129852
RMSE train: 0.638988	val: 4.395915	test: 3.193975
MAE train: 0.511546	val: 2.963696	test: 2.390433

Early stopping
Best (RMSE):	 train: 0.893262	val: 4.006887	test: 3.223152
Best (MAE):	 train: 0.693396	val: 2.744901	test: 2.486741

RMSE train: 0.985860	val: 4.014244	test: 3.837713
MAE train: 0.769039	val: 2.993937	test: 3.254782

Epoch: 84
Loss: 1.2241220474243164
RMSE train: 0.963754	val: 4.007554	test: 3.863577
MAE train: 0.745459	val: 3.011578	test: 3.288648

Epoch: 85
Loss: 1.0451012253761292
RMSE train: 0.931829	val: 3.996637	test: 3.884911
MAE train: 0.720966	val: 3.033260	test: 3.312356

Epoch: 86
Loss: 1.165522277355194
RMSE train: 0.951300	val: 4.091641	test: 3.853886
MAE train: 0.737761	val: 3.098197	test: 3.271682

Epoch: 87
Loss: 0.9752238988876343
RMSE train: 0.943158	val: 4.118248	test: 3.813683
MAE train: 0.725365	val: 3.090697	test: 3.218926

Epoch: 88
Loss: 1.030773103237152
RMSE train: 0.935295	val: 4.102110	test: 3.786208
MAE train: 0.714851	val: 3.076499	test: 3.181452

Epoch: 89
Loss: 1.0339968800544739
RMSE train: 0.909650	val: 4.007852	test: 3.746900
MAE train: 0.694518	val: 3.016340	test: 3.146403

Epoch: 90
Loss: 1.1360667943954468
RMSE train: 0.883482	val: 3.924614	test: 3.745589
MAE train: 0.677805	val: 2.952799	test: 3.161158

Epoch: 91
Loss: 0.8843372464179993
RMSE train: 0.851638	val: 3.823143	test: 3.730129
MAE train: 0.656620	val: 2.887537	test: 3.165000

Epoch: 92
Loss: 1.0986446738243103
RMSE train: 0.827301	val: 3.749618	test: 3.715257
MAE train: 0.642999	val: 2.847319	test: 3.157299

Epoch: 93
Loss: 1.1071731448173523
RMSE train: 0.823626	val: 3.754006	test: 3.728143
MAE train: 0.653663	val: 2.835783	test: 3.174929

Epoch: 94
Loss: 0.9740128815174103
RMSE train: 0.807320	val: 3.753752	test: 3.734736
MAE train: 0.648886	val: 2.834641	test: 3.176456

Epoch: 95
Loss: 0.9561953246593475
RMSE train: 0.814244	val: 3.781722	test: 3.712939
MAE train: 0.654411	val: 2.886343	test: 3.151114

Epoch: 96
Loss: 0.9648143947124481
RMSE train: 0.854460	val: 3.871101	test: 3.700725
MAE train: 0.682458	val: 2.981502	test: 3.130306

Epoch: 97
Loss: 0.8571489453315735
RMSE train: 0.925579	val: 3.983615	test: 3.693388
MAE train: 0.727053	val: 3.079230	test: 3.110158

Epoch: 98
Loss: 1.1861433684825897
RMSE train: 0.950318	val: 4.027502	test: 3.670189
MAE train: 0.742899	val: 3.080904	test: 3.079064

Epoch: 99
Loss: 0.9947990477085114
RMSE train: 0.899974	val: 3.944735	test: 3.622626
MAE train: 0.714779	val: 2.982613	test: 3.026341

Epoch: 100
Loss: 1.1511850357055664
RMSE train: 0.832204	val: 3.838949	test: 3.619973
MAE train: 0.671422	val: 2.865883	test: 3.019491

Epoch: 101
Loss: 0.899013876914978
RMSE train: 0.795055	val: 3.753378	test: 3.626662
MAE train: 0.640705	val: 2.791137	test: 3.032499

Epoch: 102
Loss: 0.8821928203105927
RMSE train: 0.793975	val: 3.708426	test: 3.659976
MAE train: 0.630656	val: 2.777736	test: 3.075678

Epoch: 103
Loss: 0.9549731612205505
RMSE train: 0.806986	val: 3.723112	test: 3.670192
MAE train: 0.637914	val: 2.796938	test: 3.096759

Epoch: 104
Loss: 0.9720323085784912
RMSE train: 0.814850	val: 3.775351	test: 3.643230
MAE train: 0.640444	val: 2.862020	test: 3.071615

Epoch: 105
Loss: 1.0169240236282349
RMSE train: 0.858985	val: 3.908651	test: 3.650009
MAE train: 0.677178	val: 2.911726	test: 3.071382

Epoch: 106
Loss: 0.9963842034339905
RMSE train: 0.852879	val: 3.949734	test: 3.647783
MAE train: 0.663330	val: 2.926644	test: 3.075918

Epoch: 107
Loss: 0.7934248447418213
RMSE train: 0.818592	val: 3.940626	test: 3.622366
MAE train: 0.625380	val: 2.919493	test: 3.061436

Epoch: 108
Loss: 0.7369265854358673
RMSE train: 0.759419	val: 3.853133	test: 3.570386
MAE train: 0.569874	val: 2.888881	test: 3.027980

Epoch: 109
Loss: 0.7896016240119934
RMSE train: 0.708421	val: 3.751481	test: 3.521943
MAE train: 0.535691	val: 2.817858	test: 2.987375

Epoch: 110
Loss: 0.9673129320144653
RMSE train: 0.759708	val: 3.740148	test: 3.549025
MAE train: 0.584205	val: 2.802131	test: 3.009060

Epoch: 111
Loss: 0.8494983017444611
RMSE train: 0.797833	val: 3.725884	test: 3.611042
MAE train: 0.619555	val: 2.791103	test: 3.060526

Epoch: 112
Loss: 0.9786063432693481
RMSE train: 0.834220	val: 3.752040	test: 3.654544
MAE train: 0.652234	val: 2.836512	test: 3.093816

Epoch: 113
Loss: 0.8513628840446472
RMSE train: 0.824021	val: 3.768400	test: 3.666251
MAE train: 0.642017	val: 2.880566	test: 3.108078

Epoch: 114
Loss: 0.7798159420490265
RMSE train: 0.800820	val: 3.777359	test: 3.660191
MAE train: 0.623106	val: 2.925484	test: 3.104199

Epoch: 115
Loss: 0.8118420243263245
RMSE train: 0.764027	val: 3.754392	test: 3.667770
MAE train: 0.596854	val: 2.946310	test: 3.112025

Epoch: 116
Loss: 0.8474196791648865
RMSE train: 0.764883	val: 3.755563	test: 3.707360
MAE train: 0.605424	val: 2.931947	test: 3.152952

Epoch: 117
Loss: 0.8250231146812439
RMSE train: 0.803033	val: 3.773116	test: 3.752687
MAE train: 0.638494	val: 2.917724	test: 3.193773

Epoch: 118
Loss: 0.7807960510253906
RMSE train: 0.847470	val: 3.802790	test: 3.767573
MAE train: 0.675992	val: 2.929608	test: 3.205059

Epoch: 119
Loss: 0.7550308108329773
RMSE train: 0.863343	val: 3.792728	test: 3.739583
MAE train: 0.693543	val: 2.905434	test: 3.171945

Epoch: 120
Loss: 0.8071628212928772
RMSE train: 0.849524	val: 3.746515	test: 3.710993
MAE train: 0.686840	val: 2.858165	test: 3.148539

Epoch: 121
Loss: 0.8798322379589081
RMSE train: 0.822330	val: 3.698957	test: 3.679969
MAE train: 0.663774	val: 2.811773	test: 3.122472

Early stopping
Best (RMSE):	 train: 0.750624	val: 3.560093	test: 3.666010
Best (MAE):	 train: 0.578180	val: 2.721012	test: 3.151030

RMSE train: 0.843635	val: 4.345038	test: 3.296584
MAE train: 0.660757	val: 3.026786	test: 2.652626

Epoch: 84
Loss: 0.9843283295631409
RMSE train: 0.852505	val: 4.357945	test: 3.273698
MAE train: 0.669692	val: 3.032468	test: 2.622870

Epoch: 85
Loss: 0.9299679100513458
RMSE train: 0.859675	val: 4.362195	test: 3.239144
MAE train: 0.673334	val: 3.006985	test: 2.570183

Epoch: 86
Loss: 0.9749479293823242
RMSE train: 0.844558	val: 4.378794	test: 3.214887
MAE train: 0.658599	val: 2.998131	test: 2.530881

Epoch: 87
Loss: 0.9999758005142212
RMSE train: 0.808861	val: 4.347209	test: 3.187603
MAE train: 0.629995	val: 2.944658	test: 2.487460

Epoch: 88
Loss: 1.014035940170288
RMSE train: 0.815789	val: 4.326097	test: 3.210655
MAE train: 0.634623	val: 2.912799	test: 2.513128

Epoch: 89
Loss: 1.027321457862854
RMSE train: 0.836746	val: 4.320898	test: 3.271694
MAE train: 0.654633	val: 2.902250	test: 2.584174

Epoch: 90
Loss: 0.9925059676170349
RMSE train: 0.908881	val: 4.407864	test: 3.379878
MAE train: 0.714695	val: 2.960857	test: 2.700484

Epoch: 91
Loss: 1.081235945224762
RMSE train: 0.936303	val: 4.385406	test: 3.377817
MAE train: 0.738961	val: 2.908334	test: 2.712335

Epoch: 92
Loss: 1.0642961263656616
RMSE train: 0.903172	val: 4.271583	test: 3.292239
MAE train: 0.711686	val: 2.798833	test: 2.632121

Epoch: 93
Loss: 0.9232655763626099
RMSE train: 0.850461	val: 4.161228	test: 3.203501
MAE train: 0.669995	val: 2.704134	test: 2.542033

Epoch: 94
Loss: 0.9526394605636597
RMSE train: 0.777550	val: 4.104462	test: 3.123005
MAE train: 0.612357	val: 2.667414	test: 2.466866

Epoch: 95
Loss: 0.9064290523529053
RMSE train: 0.719724	val: 4.071356	test: 3.085886
MAE train: 0.564087	val: 2.693494	test: 2.444358

Epoch: 96
Loss: 1.053074598312378
RMSE train: 0.723838	val: 4.100748	test: 3.140103
MAE train: 0.564457	val: 2.752170	test: 2.508613

Epoch: 97
Loss: 1.0773748755455017
RMSE train: 0.733897	val: 4.111140	test: 3.143803
MAE train: 0.575029	val: 2.779703	test: 2.510610

Epoch: 98
Loss: 0.9495232105255127
RMSE train: 0.751384	val: 4.118770	test: 3.123616
MAE train: 0.592409	val: 2.781726	test: 2.472716

Epoch: 99
Loss: 0.9669840335845947
RMSE train: 0.761729	val: 4.150914	test: 3.170896
MAE train: 0.601552	val: 2.820012	test: 2.513257

Epoch: 100
Loss: 0.9573038816452026
RMSE train: 0.769221	val: 4.199940	test: 3.262674
MAE train: 0.603831	val: 2.886712	test: 2.605606

Epoch: 101
Loss: 0.9760551154613495
RMSE train: 0.767612	val: 4.239001	test: 3.297372
MAE train: 0.594195	val: 2.927959	test: 2.642817

Epoch: 102
Loss: 0.8933005928993225
RMSE train: 0.745876	val: 4.213113	test: 3.319882
MAE train: 0.579323	val: 2.936246	test: 2.669945

Epoch: 103
Loss: 0.9462806582450867
RMSE train: 0.741348	val: 4.194533	test: 3.303545
MAE train: 0.579482	val: 2.932308	test: 2.648604

Epoch: 104
Loss: 0.9136264324188232
RMSE train: 0.731649	val: 4.177191	test: 3.273274
MAE train: 0.574451	val: 2.954310	test: 2.634987

Epoch: 105
Loss: 0.955725222826004
RMSE train: 0.696580	val: 4.143833	test: 3.243156
MAE train: 0.545555	val: 3.009127	test: 2.647845

Epoch: 106
Loss: 0.899614691734314
RMSE train: 0.683438	val: 4.120045	test: 3.212006
MAE train: 0.536027	val: 3.009862	test: 2.615180

Epoch: 107
Loss: 0.9993048906326294
RMSE train: 0.671214	val: 4.021834	test: 3.183516
MAE train: 0.529330	val: 2.890497	test: 2.563274

Epoch: 108
Loss: 0.903746634721756
RMSE train: 0.685368	val: 3.986250	test: 3.177232
MAE train: 0.543712	val: 2.803825	test: 2.539633

Epoch: 109
Loss: 0.8390518128871918
RMSE train: 0.658922	val: 3.944269	test: 3.151565
MAE train: 0.524595	val: 2.744887	test: 2.514561

Epoch: 110
Loss: 0.8666359186172485
RMSE train: 0.654680	val: 3.939139	test: 3.105384
MAE train: 0.522633	val: 2.700793	test: 2.453447

Epoch: 111
Loss: 0.7539506554603577
RMSE train: 0.662578	val: 3.970800	test: 3.066706
MAE train: 0.527393	val: 2.699971	test: 2.394589

Epoch: 112
Loss: 0.7143823802471161
RMSE train: 0.641675	val: 3.959011	test: 3.039774
MAE train: 0.509282	val: 2.694196	test: 2.370885

Epoch: 113
Loss: 0.8572869896888733
RMSE train: 0.626740	val: 3.959079	test: 3.039614
MAE train: 0.494601	val: 2.718141	test: 2.382769

Epoch: 114
Loss: 0.9441493451595306
RMSE train: 0.638672	val: 4.022534	test: 3.068784
MAE train: 0.496074	val: 2.781001	test: 2.426842

Epoch: 115
Loss: 0.7710631489753723
RMSE train: 0.643119	val: 4.042376	test: 3.082977
MAE train: 0.497326	val: 2.830086	test: 2.467003

Epoch: 116
Loss: 0.9007876217365265
RMSE train: 0.660381	val: 4.069670	test: 3.118107
MAE train: 0.511765	val: 2.826172	test: 2.494501

Epoch: 117
Loss: 0.8935019075870514
RMSE train: 0.696705	val: 4.118692	test: 3.148853
MAE train: 0.543239	val: 2.787556	test: 2.467832

Epoch: 118
Loss: 0.7837662696838379
RMSE train: 0.696519	val: 4.113103	test: 3.135030
MAE train: 0.543598	val: 2.713186	test: 2.416189

Epoch: 119
Loss: 0.9018854796886444
RMSE train: 0.735559	val: 4.151361	test: 3.134962
MAE train: 0.574051	val: 2.710761	test: 2.402667

Epoch: 120
Loss: 0.7682795226573944
RMSE train: 0.722158	val: 4.121442	test: 3.107281
MAE train: 0.563491	val: 2.677643	test: 2.380110

Epoch: 121
Loss: 0.8237143158912659
RMSE train: 0.686600	val: 4.045472	test: 3.086686
MAE train: 0.538721	val: 2.633868	test: 2.371313

Early stopping
Best (RMSE):	 train: 0.813835	val: 3.663110	test: 3.088349
Best (MAE):	 train: 0.627608	val: 2.459052	test: 2.485028

RMSE train: 0.848338	val: 4.727068	test: 3.853371
MAE train: 0.651209	val: 3.252697	test: 2.994671

Epoch: 84
Loss: 1.0588064193725586
RMSE train: 0.830212	val: 4.673432	test: 3.819141
MAE train: 0.629593	val: 3.182566	test: 2.960742

Epoch: 85
Loss: 0.9349391758441925
RMSE train: 0.836479	val: 4.650337	test: 3.799426
MAE train: 0.635131	val: 3.141320	test: 2.939094

Epoch: 86
Loss: 1.0317414104938507
RMSE train: 0.836396	val: 4.646240	test: 3.790807
MAE train: 0.643861	val: 3.117345	test: 2.929317

Epoch: 87
Loss: 0.9068493545055389
RMSE train: 0.836563	val: 4.679525	test: 3.780131
MAE train: 0.644867	val: 3.121526	test: 2.924597

Epoch: 88
Loss: 1.0140550136566162
RMSE train: 0.850301	val: 4.717819	test: 3.776803
MAE train: 0.652721	val: 3.132267	test: 2.933034

Epoch: 89
Loss: 0.8650727868080139
RMSE train: 0.830042	val: 4.695332	test: 3.774789
MAE train: 0.639786	val: 3.117303	test: 2.948471

Epoch: 90
Loss: 0.8731686472892761
RMSE train: 0.796238	val: 4.650077	test: 3.800103
MAE train: 0.619253	val: 3.112195	test: 2.990541

Epoch: 91
Loss: 0.922725647687912
RMSE train: 0.797869	val: 4.640470	test: 3.838084
MAE train: 0.623974	val: 3.146212	test: 3.038499

Epoch: 92
Loss: 1.1097666025161743
RMSE train: 0.828907	val: 4.649697	test: 3.850615
MAE train: 0.646981	val: 3.172902	test: 3.041281

Epoch: 93
Loss: 0.8626902103424072
RMSE train: 0.892184	val: 4.709371	test: 3.843662
MAE train: 0.689918	val: 3.244847	test: 3.020464

Epoch: 94
Loss: 0.9741761982440948
RMSE train: 0.889198	val: 4.679687	test: 3.778310
MAE train: 0.685494	val: 3.177306	test: 2.951750

Epoch: 95
Loss: 0.9101573824882507
RMSE train: 0.845742	val: 4.599975	test: 3.706858
MAE train: 0.650894	val: 3.048884	test: 2.871831

Epoch: 96
Loss: 0.967769593000412
RMSE train: 0.758782	val: 4.497431	test: 3.620713
MAE train: 0.577839	val: 2.951735	test: 2.782808

Epoch: 97
Loss: 1.0105416178703308
RMSE train: 0.741415	val: 4.472229	test: 3.579635
MAE train: 0.562827	val: 2.912719	test: 2.722736

Epoch: 98
Loss: 0.9433743953704834
RMSE train: 0.753902	val: 4.497071	test: 3.564868
MAE train: 0.570838	val: 2.915066	test: 2.683749

Epoch: 99
Loss: 0.8437086939811707
RMSE train: 0.767576	val: 4.521215	test: 3.585730
MAE train: 0.582807	val: 2.931507	test: 2.694472

Epoch: 100
Loss: 0.791944682598114
RMSE train: 0.820050	val: 4.584905	test: 3.647599
MAE train: 0.624670	val: 3.042725	test: 2.768203

Epoch: 101
Loss: 0.9313217103481293
RMSE train: 0.870445	val: 4.638021	test: 3.673333
MAE train: 0.668485	val: 3.140069	test: 2.812352

Epoch: 102
Loss: 0.9081192910671234
RMSE train: 0.900738	val: 4.690545	test: 3.690200
MAE train: 0.690319	val: 3.198260	test: 2.842243

Epoch: 103
Loss: 0.8787039816379547
RMSE train: 0.903348	val: 4.704418	test: 3.706323
MAE train: 0.690142	val: 3.189274	test: 2.865041

Epoch: 104
Loss: 0.9219237565994263
RMSE train: 0.837295	val: 4.644651	test: 3.692732
MAE train: 0.643350	val: 3.105730	test: 2.865183

Epoch: 105
Loss: 0.9455045759677887
RMSE train: 0.728917	val: 4.538662	test: 3.669794
MAE train: 0.560952	val: 2.982850	test: 2.874030

Epoch: 106
Loss: 0.7883661091327667
RMSE train: 0.698192	val: 4.518848	test: 3.696799
MAE train: 0.538830	val: 2.952704	test: 2.901348

Epoch: 107
Loss: 0.7930895686149597
RMSE train: 0.735854	val: 4.591324	test: 3.726200
MAE train: 0.570579	val: 3.016700	test: 2.906332

Epoch: 108
Loss: 0.7912189364433289
RMSE train: 0.804551	val: 4.688236	test: 3.768437
MAE train: 0.626249	val: 3.108117	test: 2.928715

Epoch: 109
Loss: 0.8765604496002197
RMSE train: 0.844619	val: 4.755663	test: 3.806029
MAE train: 0.656483	val: 3.210564	test: 2.973722

Epoch: 110
Loss: 0.8730550110340118
RMSE train: 0.835781	val: 4.747995	test: 3.763141
MAE train: 0.647506	val: 3.205994	test: 2.923777

Epoch: 111
Loss: 0.8003360033035278
RMSE train: 0.763412	val: 4.661979	test: 3.673372
MAE train: 0.592250	val: 3.111172	test: 2.819946

Epoch: 112
Loss: 0.7877559065818787
RMSE train: 0.694600	val: 4.598244	test: 3.623419
MAE train: 0.543609	val: 3.061785	test: 2.754781

Epoch: 113
Loss: 0.6692831814289093
RMSE train: 0.659412	val: 4.601697	test: 3.617864
MAE train: 0.516581	val: 3.093443	test: 2.757445

Epoch: 114
Loss: 0.8263702690601349
RMSE train: 0.631087	val: 4.584926	test: 3.651755
MAE train: 0.495945	val: 3.128107	test: 2.804743

Epoch: 115
Loss: 0.8910135924816132
RMSE train: 0.682491	val: 4.624669	test: 3.728183
MAE train: 0.536317	val: 3.195321	test: 2.885322

Epoch: 116
Loss: 0.6978082656860352
RMSE train: 0.732522	val: 4.666588	test: 3.778471
MAE train: 0.572103	val: 3.234861	test: 2.936627

Epoch: 117
Loss: 0.7453649044036865
RMSE train: 0.770035	val: 4.698559	test: 3.794325
MAE train: 0.602958	val: 3.250235	test: 2.944999

Epoch: 118
Loss: 0.6714611947536469
RMSE train: 0.730844	val: 4.648930	test: 3.764903
MAE train: 0.570455	val: 3.154340	test: 2.929399

Epoch: 119
Loss: 0.7714222073554993
RMSE train: 0.658514	val: 4.559502	test: 3.665482
MAE train: 0.511638	val: 3.007377	test: 2.834713

Epoch: 120
Loss: 0.8217392563819885
RMSE train: 0.634012	val: 4.517434	test: 3.595229
MAE train: 0.492043	val: 2.942861	test: 2.762304

Epoch: 121
Loss: 0.7562902569770813
RMSE train: 0.658385	val: 4.522771	test: 3.546091
MAE train: 0.506090	val: 2.929962	test: 2.677761

Epoch: 122
Loss: 0.7546869218349457
RMSE train: 0.688273	val: 4.554962	test: 3.555857
MAE train: 0.527386	val: 2.945357	test: 2.663470

Epoch: 123
Loss: 0.7543769180774689
RMSE train: 0.732413	val: 4.609483	test: 3.606646
MAE train: 0.563330	val: 3.008351	test: 2.705942

Epoch: 124
Loss: 0.656927227973938
RMSE train: 0.768901	val: 4.664923	test: 3.689406
MAE train: 0.593880	val: 3.124099	test: 2.817470

Epoch: 125
Loss: 0.7085818648338318
RMSE train: 0.736161	val: 4.623392	test: 3.724739
MAE train: 0.572835	val: 3.108183	test: 2.888425

Epoch: 126
Loss: 0.7329785227775574
RMSE train: 0.714876	val: 4.598168	test: 3.730799
MAE train: 0.562703	val: 3.078361	test: 2.902034

Epoch: 127
Loss: 0.7665217220783234
RMSE train: 0.675364	val: 4.565302	test: 3.714381
MAE train: 0.531916	val: 3.021119	test: 2.888606

Epoch: 128
Loss: 0.7210134863853455
RMSE train: 0.684300	val: 4.607253	test: 3.727109
MAE train: 0.535752	val: 3.093031	test: 2.901356

Epoch: 129
Loss: 0.6913956701755524
RMSE train: 0.683604	val: 4.626483	test: 3.737064
MAE train: 0.528022	val: 3.138092	test: 2.919565

Epoch: 130
Loss: 0.7487930357456207
RMSE train: 0.705834	val: 4.664205	test: 3.739914
MAE train: 0.540934	val: 3.171916	test: 2.926804

Epoch: 131
Loss: 0.6827707588672638
RMSE train: 0.772071	val: 4.711172	test: 3.724491
MAE train: 0.586042	val: 3.186760	test: 2.904623

Epoch: 132
Loss: 0.6911061704158783
RMSE train: 0.792135	val: 4.716803	test: 3.702340
MAE train: 0.595912	val: 3.167324	test: 2.888481

Early stopping
Best (RMSE):	 train: 0.741415	val: 4.472229	test: 3.579635
Best (MAE):	 train: 0.562827	val: 2.912719	test: 2.722736

RMSE train: 0.819400	val: 4.836924	test: 4.140590
MAE train: 0.640269	val: 3.429317	test: 3.351165

Epoch: 84
Loss: 1.0403491854667664
RMSE train: 0.745642	val: 4.718259	test: 4.018464
MAE train: 0.579325	val: 3.322167	test: 3.220187

Epoch: 85
Loss: 1.0892598330974579
RMSE train: 0.738433	val: 4.674315	test: 3.914819
MAE train: 0.573016	val: 3.275599	test: 3.104067

Epoch: 86
Loss: 1.0545006394386292
RMSE train: 0.731284	val: 4.656512	test: 3.898768
MAE train: 0.569252	val: 3.254510	test: 3.081317

Epoch: 87
Loss: 1.0920276641845703
RMSE train: 0.738975	val: 4.704541	test: 3.933923
MAE train: 0.579863	val: 3.266599	test: 3.114505

Epoch: 88
Loss: 1.0365320444107056
RMSE train: 0.764073	val: 4.776685	test: 3.990067
MAE train: 0.611441	val: 3.314033	test: 3.168136

Epoch: 89
Loss: 0.9474830627441406
RMSE train: 0.795761	val: 4.807563	test: 4.027923
MAE train: 0.641103	val: 3.339988	test: 3.211549

Epoch: 90
Loss: 1.0868890881538391
RMSE train: 0.813840	val: 4.812332	test: 4.054030
MAE train: 0.658613	val: 3.354820	test: 3.243669

Epoch: 91
Loss: 1.0726786851882935
RMSE train: 0.797636	val: 4.763668	test: 4.036334
MAE train: 0.646022	val: 3.331882	test: 3.232210

Epoch: 92
Loss: 1.2197880744934082
RMSE train: 0.788824	val: 4.710819	test: 3.980627
MAE train: 0.632998	val: 3.278300	test: 3.182459

Epoch: 93
Loss: 0.9914297461509705
RMSE train: 0.789384	val: 4.681557	test: 3.935356
MAE train: 0.632888	val: 3.249954	test: 3.142198

Epoch: 94
Loss: 0.9828737676143646
RMSE train: 0.799972	val: 4.678221	test: 3.872873
MAE train: 0.640053	val: 3.261983	test: 3.074811

Epoch: 95
Loss: 1.1340317130088806
RMSE train: 0.801247	val: 4.667356	test: 3.826930
MAE train: 0.630746	val: 3.255618	test: 3.027328

Epoch: 96
Loss: 1.044081211090088
RMSE train: 0.753768	val: 4.614506	test: 3.789236
MAE train: 0.585714	val: 3.201766	test: 2.988208

Epoch: 97
Loss: 1.0090185403823853
RMSE train: 0.781756	val: 4.713238	test: 3.845152
MAE train: 0.594292	val: 3.260099	test: 3.032573

Epoch: 98
Loss: 0.8881185054779053
RMSE train: 0.788073	val: 4.782604	test: 3.891863
MAE train: 0.595032	val: 3.294882	test: 3.074208

Epoch: 99
Loss: 0.8746539056301117
RMSE train: 0.737711	val: 4.745832	test: 3.913092
MAE train: 0.567848	val: 3.271269	test: 3.099811

Epoch: 100
Loss: 0.9532619416713715
RMSE train: 0.765972	val: 4.801749	test: 3.989187
MAE train: 0.595867	val: 3.330569	test: 3.183978

Epoch: 101
Loss: 0.9266217947006226
RMSE train: 0.784019	val: 4.803348	test: 4.027589
MAE train: 0.605671	val: 3.381482	test: 3.231069

Epoch: 102
Loss: 0.8893715441226959
RMSE train: 0.789143	val: 4.786677	test: 4.033944
MAE train: 0.605238	val: 3.410028	test: 3.243380

Epoch: 103
Loss: 0.9247854948043823
RMSE train: 0.772365	val: 4.742679	test: 4.010029
MAE train: 0.596646	val: 3.377445	test: 3.225604

Epoch: 104
Loss: 0.9963691532611847
RMSE train: 0.755175	val: 4.721195	test: 3.980949
MAE train: 0.589853	val: 3.358682	test: 3.197748

Epoch: 105
Loss: 0.9118197560310364
RMSE train: 0.711675	val: 4.666288	test: 3.946039
MAE train: 0.556115	val: 3.322959	test: 3.167680

Epoch: 106
Loss: 0.8691336810588837
RMSE train: 0.691684	val: 4.621667	test: 3.884152
MAE train: 0.537659	val: 3.287321	test: 3.104086

Epoch: 107
Loss: 0.7933264672756195
RMSE train: 0.708206	val: 4.627156	test: 3.817191
MAE train: 0.547892	val: 3.277271	test: 3.034574

Epoch: 108
Loss: 0.8549782931804657
RMSE train: 0.764922	val: 4.675802	test: 3.801899
MAE train: 0.590005	val: 3.300521	test: 3.010379

Epoch: 109
Loss: 0.8600782155990601
RMSE train: 0.798706	val: 4.724715	test: 3.800992
MAE train: 0.615001	val: 3.299937	test: 3.003475

Epoch: 110
Loss: 0.8851196765899658
RMSE train: 0.801122	val: 4.761462	test: 3.807221
MAE train: 0.624430	val: 3.285601	test: 2.999275

Epoch: 111
Loss: 0.833787202835083
RMSE train: 0.792001	val: 4.772429	test: 3.820160
MAE train: 0.621848	val: 3.273153	test: 3.007118

Epoch: 112
Loss: 0.6719954609870911
RMSE train: 0.781997	val: 4.777392	test: 3.833620
MAE train: 0.617410	val: 3.270568	test: 3.021048

Epoch: 113
Loss: 0.7815246880054474
RMSE train: 0.800185	val: 4.808666	test: 3.876889
MAE train: 0.625031	val: 3.292468	test: 3.069665

Epoch: 114
Loss: 0.8976947069168091
RMSE train: 0.775892	val: 4.759365	test: 3.877625
MAE train: 0.607463	val: 3.267288	test: 3.069371

Epoch: 115
Loss: 0.929512619972229
RMSE train: 0.809862	val: 4.790991	test: 3.887561
MAE train: 0.635272	val: 3.305729	test: 3.071564

Epoch: 116
Loss: 0.7903778553009033
RMSE train: 0.833854	val: 4.829601	test: 3.888143
MAE train: 0.657892	val: 3.346218	test: 3.068079

Epoch: 117
Loss: 0.7810362577438354
RMSE train: 0.882554	val: 4.902247	test: 3.898570
MAE train: 0.698248	val: 3.401621	test: 3.075788

Epoch: 118
Loss: 0.7842037975788116
RMSE train: 0.874624	val: 4.857713	test: 3.899198
MAE train: 0.681851	val: 3.360490	test: 3.089478

Epoch: 119
Loss: 0.8270739316940308
RMSE train: 0.853874	val: 4.813463	test: 3.911070
MAE train: 0.663038	val: 3.337365	test: 3.112338

Epoch: 120
Loss: 0.8123046457767487
RMSE train: 0.838101	val: 4.808549	test: 3.920801
MAE train: 0.644812	val: 3.342949	test: 3.129979

Epoch: 121
Loss: 0.9294983148574829
RMSE train: 0.802969	val: 4.767188	test: 3.885911
MAE train: 0.612052	val: 3.316561	test: 3.097924

Epoch: 122
Loss: 0.7246718108654022
RMSE train: 0.727332	val: 4.674413	test: 3.800063
MAE train: 0.549402	val: 3.251236	test: 3.013493

Epoch: 123
Loss: 0.7602268159389496
RMSE train: 0.660669	val: 4.577060	test: 3.725695
MAE train: 0.502468	val: 3.191766	test: 2.947074

Epoch: 124
Loss: 0.7637596726417542
RMSE train: 0.630177	val: 4.541903	test: 3.705769
MAE train: 0.488500	val: 3.182204	test: 2.932453

Epoch: 125
Loss: 0.7622548341751099
RMSE train: 0.611571	val: 4.468268	test: 3.690875
MAE train: 0.478381	val: 3.158701	test: 2.924368

Epoch: 126
Loss: 0.7318410575389862
RMSE train: 0.628242	val: 4.485001	test: 3.721678
MAE train: 0.494811	val: 3.180418	test: 2.951811

Epoch: 127
Loss: 0.7718475759029388
RMSE train: 0.639034	val: 4.496675	test: 3.764187
MAE train: 0.508674	val: 3.186688	test: 2.992469

Epoch: 128
Loss: 0.7977527678012848
RMSE train: 0.710816	val: 4.610822	test: 3.834302
MAE train: 0.571166	val: 3.256081	test: 3.060345

Epoch: 129
Loss: 0.823896586894989
RMSE train: 0.823065	val: 4.749149	test: 3.938652
MAE train: 0.649472	val: 3.346045	test: 3.153745

Epoch: 130
Loss: 0.8340354561805725
RMSE train: 0.830347	val: 4.762002	test: 3.925044
MAE train: 0.649074	val: 3.330228	test: 3.133851

Epoch: 131
Loss: 0.7306138873100281
RMSE train: 0.841187	val: 4.762984	test: 3.902987
MAE train: 0.657326	val: 3.316134	test: 3.101259

Epoch: 132
Loss: 0.7147167325019836
RMSE train: 0.798220	val: 4.701444	test: 3.847765
MAE train: 0.627285	val: 3.265778	test: 3.040406

Epoch: 133
Loss: 0.733431488275528
RMSE train: 0.765752	val: 4.649065	test: 3.832568
MAE train: 0.605967	val: 3.228868	test: 3.026191

Epoch: 134
Loss: 0.6738100349903107
RMSE train: 0.723206	val: 4.599894	test: 3.823719
MAE train: 0.578652	val: 3.194524	test: 3.022746

Epoch: 135
Loss: 0.7754025459289551
RMSE train: 0.741914	val: 4.602355	test: 3.860598
MAE train: 0.599684	val: 3.197555	test: 3.063988

Epoch: 136
Loss: 0.6899466812610626
RMSE train: 0.753065	val: 4.596804	test: 3.889078
MAE train: 0.611700	val: 3.202072	test: 3.097702

Epoch: 137
Loss: 0.7390807867050171
RMSE train: 0.806842	val: 4.683946	test: 3.913439
MAE train: 0.650006	val: 3.272952	test: 3.134076

Epoch: 138
Loss: 0.6410116851329803
RMSE train: 0.820435	val: 4.714485	test: 3.891848
MAE train: 0.652130	val: 3.294520	test: 3.120727

Epoch: 139
Loss: 0.6986810863018036
RMSE train: 0.786066	val: 4.667779	test: 3.816530
MAE train: 0.616037	val: 3.262153	test: 3.050628

Epoch: 140
Loss: 0.6928499042987823
RMSE train: 0.661507	val: 4.517259	test: 3.691053
MAE train: 0.522624	val: 3.169317	test: 2.932833

Epoch: 141
Loss: 0.9062197506427765
RMSE train: 0.631866	val: 4.482754	test: 3.622526
MAE train: 0.505082	val: 3.168345	test: 2.866318

Epoch: 142
Loss: 0.6368028223514557
RMSE train: 0.585063	val: 4.391353	test: 3.587906
MAE train: 0.471249	val: 3.125453	test: 2.837519

Epoch: 143
Loss: 0.6353250741958618
RMSE train: 0.608458	val: 4.391486	test: 3.586495
MAE train: 0.492323	val: 3.131706	test: 2.843589
RMSE train: 0.902908	val: 4.901631	test: 3.556213
MAE train: 0.696680	val: 3.477048	test: 2.786573

Epoch: 84
Loss: 1.5069244503974915
RMSE train: 0.932011	val: 4.853098	test: 3.481156
MAE train: 0.715874	val: 3.455946	test: 2.712813

Epoch: 85
Loss: 1.3232824206352234
RMSE train: 0.946440	val: 4.783395	test: 3.412887
MAE train: 0.727936	val: 3.418210	test: 2.642104

Epoch: 86
Loss: 1.1892893314361572
RMSE train: 0.890567	val: 4.651269	test: 3.352292
MAE train: 0.694740	val: 3.347337	test: 2.590806

Epoch: 87
Loss: 1.1622441411018372
RMSE train: 0.827250	val: 4.537040	test: 3.344913
MAE train: 0.651380	val: 3.277745	test: 2.595365

Epoch: 88
Loss: 1.2635441422462463
RMSE train: 0.801232	val: 4.484625	test: 3.397817
MAE train: 0.636988	val: 3.235712	test: 2.647431

Epoch: 89
Loss: 1.2038767337799072
RMSE train: 0.806981	val: 4.527313	test: 3.550094
MAE train: 0.646782	val: 3.247812	test: 2.789508

Epoch: 90
Loss: 1.2970802783966064
RMSE train: 0.833847	val: 4.589821	test: 3.626084
MAE train: 0.672193	val: 3.246902	test: 2.850240

Epoch: 91
Loss: 1.4045965671539307
RMSE train: 0.854536	val: 4.634132	test: 3.636662
MAE train: 0.690248	val: 3.261037	test: 2.865007

Epoch: 92
Loss: 1.0792485475540161
RMSE train: 0.859115	val: 4.673544	test: 3.603285
MAE train: 0.694821	val: 3.287719	test: 2.836567

Epoch: 93
Loss: 1.1545284986495972
RMSE train: 0.862133	val: 4.729238	test: 3.590967
MAE train: 0.696896	val: 3.338377	test: 2.821425

Epoch: 94
Loss: 1.2408385872840881
RMSE train: 0.855527	val: 4.815368	test: 3.604169
MAE train: 0.690245	val: 3.408860	test: 2.833138

Epoch: 95
Loss: 1.0734055638313293
RMSE train: 0.818102	val: 4.827369	test: 3.660466
MAE train: 0.660019	val: 3.440874	test: 2.889868

Epoch: 96
Loss: 1.072584331035614
RMSE train: 0.813638	val: 4.896685	test: 3.752226
MAE train: 0.660110	val: 3.502568	test: 2.986013

Epoch: 97
Loss: 1.192708432674408
RMSE train: 0.850235	val: 4.989951	test: 3.825517
MAE train: 0.696630	val: 3.559497	test: 3.070032

Epoch: 98
Loss: 1.0424420833587646
RMSE train: 0.852962	val: 4.979781	test: 3.812448
MAE train: 0.701532	val: 3.538251	test: 3.056817

Epoch: 99
Loss: 1.0593262314796448
RMSE train: 0.831285	val: 4.943774	test: 3.777346
MAE train: 0.681246	val: 3.502268	test: 3.019175

Epoch: 100
Loss: 1.0690920650959015
RMSE train: 0.821761	val: 4.918534	test: 3.724392
MAE train: 0.667914	val: 3.465099	test: 2.958501

Epoch: 101
Loss: 0.9312775135040283
RMSE train: 0.794069	val: 4.831039	test: 3.594213
MAE train: 0.637267	val: 3.382267	test: 2.823450

Epoch: 102
Loss: 0.9351693093776703
RMSE train: 0.778538	val: 4.742804	test: 3.472288
MAE train: 0.617772	val: 3.301972	test: 2.711891

Epoch: 103
Loss: 0.9587117731571198
RMSE train: 0.795564	val: 4.694415	test: 3.430964
MAE train: 0.631810	val: 3.275608	test: 2.678752

Epoch: 104
Loss: 0.982254832983017
RMSE train: 0.799690	val: 4.657958	test: 3.458103
MAE train: 0.640474	val: 3.256119	test: 2.697186

Epoch: 105
Loss: 1.0113216042518616
RMSE train: 0.759333	val: 4.592608	test: 3.470786
MAE train: 0.610853	val: 3.217479	test: 2.703369

Epoch: 106
Loss: 0.8725787401199341
RMSE train: 0.715443	val: 4.548646	test: 3.471601
MAE train: 0.574463	val: 3.216847	test: 2.698286

Epoch: 107
Loss: 0.9049687683582306
RMSE train: 0.666618	val: 4.462623	test: 3.447797
MAE train: 0.534318	val: 3.186680	test: 2.672115

Epoch: 108
Loss: 0.9888356328010559
RMSE train: 0.660551	val: 4.491892	test: 3.468113
MAE train: 0.527054	val: 3.228076	test: 2.691737

Epoch: 109
Loss: 0.9339117705821991
RMSE train: 0.678843	val: 4.508512	test: 3.440303
MAE train: 0.541181	val: 3.262026	test: 2.672923

Epoch: 110
Loss: 0.9663141965866089
RMSE train: 0.756612	val: 4.624553	test: 3.468587
MAE train: 0.602720	val: 3.327106	test: 2.709296

Epoch: 111
Loss: 0.9375124275684357
RMSE train: 0.802587	val: 4.718054	test: 3.508996
MAE train: 0.632497	val: 3.374470	test: 2.755235

Epoch: 112
Loss: 1.0037002861499786
RMSE train: 0.798611	val: 4.747031	test: 3.525819
MAE train: 0.625247	val: 3.391820	test: 2.781477

Epoch: 113
Loss: 0.9847574234008789
RMSE train: 0.761095	val: 4.728636	test: 3.544455
MAE train: 0.597164	val: 3.399955	test: 2.805608

Epoch: 114
Loss: 0.9656620621681213
RMSE train: 0.705080	val: 4.654031	test: 3.548706
MAE train: 0.560306	val: 3.392065	test: 2.812707

Epoch: 115
Loss: 0.8271651268005371
RMSE train: 0.687595	val: 4.612517	test: 3.554081
MAE train: 0.551468	val: 3.388938	test: 2.815501

Epoch: 116
Loss: 0.9782489240169525
RMSE train: 0.707741	val: 4.606085	test: 3.527547
MAE train: 0.569489	val: 3.376325	test: 2.790414

Epoch: 117
Loss: 0.859749048948288
RMSE train: 0.726257	val: 4.597773	test: 3.465932
MAE train: 0.584963	val: 3.335240	test: 2.734321

Epoch: 118
Loss: 0.934230387210846
RMSE train: 0.692396	val: 4.519726	test: 3.377577
MAE train: 0.557449	val: 3.270203	test: 2.651402

Epoch: 119
Loss: 0.8616890907287598
RMSE train: 0.693357	val: 4.553530	test: 3.389917
MAE train: 0.556631	val: 3.294743	test: 2.657355

Epoch: 120
Loss: 0.8391794860363007
RMSE train: 0.714861	val: 4.614695	test: 3.413720
MAE train: 0.574539	val: 3.324918	test: 2.671436

Epoch: 121
Loss: 0.9499544501304626
RMSE train: 0.762838	val: 4.715761	test: 3.501441
MAE train: 0.614900	val: 3.402799	test: 2.741307

Epoch: 122
Loss: 0.9594310820102692
RMSE train: 0.819701	val: 4.814462	test: 3.585807
MAE train: 0.657352	val: 3.467914	test: 2.816010

Epoch: 123
Loss: 0.9603065848350525
RMSE train: 0.824776	val: 4.819580	test: 3.640555
MAE train: 0.663952	val: 3.461700	test: 2.862598

Epoch: 124
Loss: 0.91977858543396
RMSE train: 0.774420	val: 4.715924	test: 3.616391
MAE train: 0.627889	val: 3.372930	test: 2.833859

Epoch: 125
Loss: 0.9120218455791473
RMSE train: 0.740472	val: 4.627675	test: 3.585880
MAE train: 0.601611	val: 3.297102	test: 2.797482

Epoch: 126
Loss: 0.8311215937137604
RMSE train: 0.738913	val: 4.614371	test: 3.604699
MAE train: 0.601381	val: 3.292534	test: 2.802417

Epoch: 127
Loss: 0.9846935868263245
RMSE train: 0.750516	val: 4.610828	test: 3.642413
MAE train: 0.614449	val: 3.302867	test: 2.820259

Epoch: 128
Loss: 0.8800764083862305
RMSE train: 0.807980	val: 4.714967	test: 3.704252
MAE train: 0.658583	val: 3.367173	test: 2.866205

Epoch: 129
Loss: 0.8058743476867676
RMSE train: 0.801764	val: 4.728334	test: 3.711675
MAE train: 0.649780	val: 3.380000	test: 2.870446

Epoch: 130
Loss: 0.7581803798675537
RMSE train: 0.784232	val: 4.718974	test: 3.698286
MAE train: 0.632876	val: 3.385645	test: 2.863111

Epoch: 131
Loss: 1.0119081437587738
RMSE train: 0.780728	val: 4.722366	test: 3.692333
MAE train: 0.626553	val: 3.392997	test: 2.871504

Epoch: 132
Loss: 0.8138827979564667
RMSE train: 0.824153	val: 4.786762	test: 3.746842
MAE train: 0.663276	val: 3.411364	test: 2.943245

Epoch: 133
Loss: 0.6582393944263458
RMSE train: 0.855138	val: 4.834830	test: 3.765988
MAE train: 0.686445	val: 3.429128	test: 2.978044

Epoch: 134
Loss: 0.7457074522972107
RMSE train: 0.834875	val: 4.828195	test: 3.774872
MAE train: 0.672221	val: 3.433555	test: 2.999425

Epoch: 135
Loss: 0.7411618828773499
RMSE train: 0.777301	val: 4.771935	test: 3.749895
MAE train: 0.625973	val: 3.401537	test: 2.982882

Epoch: 136
Loss: 0.7731173038482666
RMSE train: 0.707234	val: 4.691730	test: 3.689841
MAE train: 0.572141	val: 3.356318	test: 2.927579

Epoch: 137
Loss: 0.7891978621482849
RMSE train: 0.657309	val: 4.638716	test: 3.616569
MAE train: 0.532185	val: 3.338192	test: 2.857163

Epoch: 138
Loss: 0.7802616953849792
RMSE train: 0.680931	val: 4.668508	test: 3.581018
MAE train: 0.543148	val: 3.384716	test: 2.814030

Epoch: 139
Loss: 0.8660398721694946
RMSE train: 0.755351	val: 4.777172	test: 3.625395
MAE train: 0.598364	val: 3.442714	test: 2.851651

Epoch: 140
Loss: 0.8548217117786407
RMSE train: 0.823398	val: 4.870834	test: 3.680045
MAE train: 0.653241	val: 3.476578	test: 2.900393

Epoch: 141
Loss: 0.720165342092514
RMSE train: 0.829118	val: 4.903488	test: 3.715030
MAE train: 0.660385	val: 3.472816	test: 2.940737

Epoch: 142
Loss: 0.8771955668926239
RMSE train: 0.800982	val: 4.897541	test: 3.733698
MAE train: 0.635809	val: 3.470023	test: 2.962825

Early stopping
Best (RMSE):	 train: 0.666618	val: 4.462623	test: 3.447797
Best (MAE):	 train: 0.534318	val: 3.186680	test: 2.672115

RMSE train: 0.909390	val: 4.762846	test: 3.717604
MAE train: 0.728865	val: 3.190509	test: 2.904038

Epoch: 84
Loss: 1.259619116783142
RMSE train: 0.919778	val: 4.760467	test: 3.694207
MAE train: 0.733473	val: 3.177931	test: 2.886410

Epoch: 85
Loss: 0.9606538116931915
RMSE train: 0.919175	val: 4.769136	test: 3.689454
MAE train: 0.726804	val: 3.200172	test: 2.893239

Epoch: 86
Loss: 1.0110287070274353
RMSE train: 0.931975	val: 4.813592	test: 3.679096
MAE train: 0.736778	val: 3.226975	test: 2.888410

Epoch: 87
Loss: 0.8722712695598602
RMSE train: 0.890789	val: 4.767834	test: 3.615506
MAE train: 0.704830	val: 3.196765	test: 2.818723

Epoch: 88
Loss: 1.026608258485794
RMSE train: 0.857664	val: 4.696888	test: 3.528936
MAE train: 0.674515	val: 3.169824	test: 2.737876

Epoch: 89
Loss: 0.8074952661991119
RMSE train: 0.801862	val: 4.583571	test: 3.437793
MAE train: 0.627093	val: 3.126748	test: 2.648398

Epoch: 90
Loss: 1.0084964334964752
RMSE train: 0.803241	val: 4.576214	test: 3.437948
MAE train: 0.624589	val: 3.142870	test: 2.633604

Epoch: 91
Loss: 0.8327550888061523
RMSE train: 0.779981	val: 4.560295	test: 3.427609
MAE train: 0.601620	val: 3.133464	test: 2.632468

Epoch: 92
Loss: 0.934932291507721
RMSE train: 0.778602	val: 4.590161	test: 3.448728
MAE train: 0.601826	val: 3.155871	test: 2.667718

Epoch: 93
Loss: 0.9023087024688721
RMSE train: 0.767690	val: 4.624398	test: 3.497722
MAE train: 0.604116	val: 3.187800	test: 2.707628

Epoch: 94
Loss: 0.9257718324661255
RMSE train: 0.765285	val: 4.646855	test: 3.547284
MAE train: 0.604361	val: 3.199907	test: 2.734053

Epoch: 95
Loss: 0.902825802564621
RMSE train: 0.765680	val: 4.642493	test: 3.550729
MAE train: 0.601452	val: 3.205755	test: 2.734024

Epoch: 96
Loss: 0.9752352237701416
RMSE train: 0.767429	val: 4.652125	test: 3.528359
MAE train: 0.597564	val: 3.219857	test: 2.732033

Epoch: 97
Loss: 0.7798091769218445
RMSE train: 0.821303	val: 4.696095	test: 3.528795
MAE train: 0.632885	val: 3.242892	test: 2.745789

Epoch: 98
Loss: 0.9034534096717834
RMSE train: 0.862504	val: 4.719443	test: 3.529140
MAE train: 0.658966	val: 3.254107	test: 2.753355

Epoch: 99
Loss: 0.9320751428604126
RMSE train: 0.833262	val: 4.689554	test: 3.515992
MAE train: 0.642037	val: 3.247061	test: 2.751843

Epoch: 100
Loss: 0.9923253953456879
RMSE train: 0.768302	val: 4.613822	test: 3.536209
MAE train: 0.598479	val: 3.233334	test: 2.779712

Epoch: 101
Loss: 0.8939777314662933
RMSE train: 0.693520	val: 4.544186	test: 3.545243
MAE train: 0.546006	val: 3.205154	test: 2.792289

Epoch: 102
Loss: 0.7846840023994446
RMSE train: 0.702384	val: 4.515633	test: 3.571513
MAE train: 0.548915	val: 3.194566	test: 2.811204

Epoch: 103
Loss: 0.7833970785140991
RMSE train: 0.753089	val: 4.548980	test: 3.615590
MAE train: 0.586700	val: 3.224587	test: 2.842265

Epoch: 104
Loss: 1.068979263305664
RMSE train: 0.760562	val: 4.591811	test: 3.622345
MAE train: 0.601896	val: 3.251741	test: 2.857586

Epoch: 105
Loss: 0.8402113616466522
RMSE train: 0.803331	val: 4.634040	test: 3.599897
MAE train: 0.640922	val: 3.243608	test: 2.831179

Epoch: 106
Loss: 0.9160069525241852
RMSE train: 0.803093	val: 4.627856	test: 3.555709
MAE train: 0.638117	val: 3.229028	test: 2.796338

Epoch: 107
Loss: 0.7355898320674896
RMSE train: 0.775683	val: 4.575063	test: 3.490727
MAE train: 0.609758	val: 3.192740	test: 2.739274

Epoch: 108
Loss: 0.7471236288547516
RMSE train: 0.751211	val: 4.535280	test: 3.475460
MAE train: 0.581424	val: 3.183520	test: 2.748577

Epoch: 109
Loss: 0.7352752983570099
RMSE train: 0.754285	val: 4.514401	test: 3.473148
MAE train: 0.582051	val: 3.169570	test: 2.752876

Epoch: 110
Loss: 1.0009048581123352
RMSE train: 0.801713	val: 4.506359	test: 3.477567
MAE train: 0.626124	val: 3.141934	test: 2.755015

Epoch: 111
Loss: 0.7634463906288147
RMSE train: 0.811464	val: 4.451026	test: 3.483648
MAE train: 0.637936	val: 3.104949	test: 2.751407

Epoch: 112
Loss: 0.8556111752986908
RMSE train: 0.832598	val: 4.431699	test: 3.497155
MAE train: 0.653626	val: 3.095328	test: 2.762683

Epoch: 113
Loss: 0.8074521124362946
RMSE train: 0.818293	val: 4.457120	test: 3.508337
MAE train: 0.639844	val: 3.095418	test: 2.773248

Epoch: 114
Loss: 0.7606988847255707
RMSE train: 0.818933	val: 4.523719	test: 3.548788
MAE train: 0.638720	val: 3.128754	test: 2.805759

Epoch: 115
Loss: 0.8374199569225311
RMSE train: 0.810439	val: 4.565297	test: 3.576016
MAE train: 0.635184	val: 3.151055	test: 2.823501

Epoch: 116
Loss: 0.7919284105300903
RMSE train: 0.800820	val: 4.577444	test: 3.577219
MAE train: 0.635532	val: 3.142521	test: 2.819751

Epoch: 117
Loss: 0.7286232709884644
RMSE train: 0.775355	val: 4.572327	test: 3.565182
MAE train: 0.623616	val: 3.119351	test: 2.798237

Epoch: 118
Loss: 0.648498147726059
RMSE train: 0.769160	val: 4.595372	test: 3.560521
MAE train: 0.620329	val: 3.123472	test: 2.795263

Epoch: 119
Loss: 0.6957691013813019
RMSE train: 0.768128	val: 4.611852	test: 3.526243
MAE train: 0.617491	val: 3.106453	test: 2.753130

Epoch: 120
Loss: 0.8252131938934326
RMSE train: 0.756556	val: 4.586715	test: 3.480088
MAE train: 0.609441	val: 3.084749	test: 2.697217

Epoch: 121
Loss: 0.732016384601593
RMSE train: 0.743135	val: 4.558029	test: 3.443542
MAE train: 0.593135	val: 3.065879	test: 2.655328

Epoch: 122
Loss: 0.7024289965629578
RMSE train: 0.757703	val: 4.569719	test: 3.425638
MAE train: 0.596406	val: 3.070100	test: 2.630680

Epoch: 123
Loss: 0.9367013275623322
RMSE train: 0.725608	val: 4.549171	test: 3.382364
MAE train: 0.566500	val: 3.062011	test: 2.606123

Epoch: 124
Loss: 0.7633028924465179
RMSE train: 0.724581	val: 4.567443	test: 3.367537
MAE train: 0.555619	val: 3.077933	test: 2.601681

Epoch: 125
Loss: 0.6310389041900635
RMSE train: 0.766246	val: 4.603147	test: 3.373753
MAE train: 0.569139	val: 3.097496	test: 2.621125

Epoch: 126
Loss: 0.7624154984951019
RMSE train: 0.745382	val: 4.586178	test: 3.376229
MAE train: 0.553042	val: 3.101283	test: 2.633162

Epoch: 127
Loss: 0.8464879095554352
RMSE train: 0.738870	val: 4.604071	test: 3.425558
MAE train: 0.572990	val: 3.112420	test: 2.655070

Epoch: 128
Loss: 0.6272858381271362
RMSE train: 0.757368	val: 4.625174	test: 3.476423
MAE train: 0.602137	val: 3.120754	test: 2.677108

Epoch: 129
Loss: 0.6752196848392487
RMSE train: 0.797086	val: 4.661035	test: 3.499840
MAE train: 0.638021	val: 3.133862	test: 2.696310

Epoch: 130
Loss: 0.6786772608757019
RMSE train: 0.778517	val: 4.629529	test: 3.471209
MAE train: 0.624621	val: 3.104815	test: 2.679200

Epoch: 131
Loss: 0.8076447248458862
RMSE train: 0.726673	val: 4.556392	test: 3.422136
MAE train: 0.574944	val: 3.073843	test: 2.663808

Epoch: 132
Loss: 0.6654724180698395
RMSE train: 0.694732	val: 4.493358	test: 3.366531
MAE train: 0.549361	val: 3.046846	test: 2.624916

Epoch: 133
Loss: 0.6285116076469421
RMSE train: 0.698044	val: 4.457753	test: 3.342453
MAE train: 0.546613	val: 3.018210	test: 2.608683

Epoch: 134
Loss: 0.7784460783004761
RMSE train: 0.693672	val: 4.423636	test: 3.353643
MAE train: 0.550509	val: 2.978192	test: 2.611946

Epoch: 135
Loss: 0.5553778409957886
RMSE train: 0.650682	val: 4.353795	test: 3.346763
MAE train: 0.517352	val: 2.945461	test: 2.604086

Epoch: 136
Loss: 0.6791914999485016
RMSE train: 0.642260	val: 4.261818	test: 3.371395
MAE train: 0.500942	val: 2.896398	test: 2.613984

Epoch: 137
Loss: 0.6321241855621338
RMSE train: 0.706911	val: 4.268135	test: 3.417702
MAE train: 0.532020	val: 2.911572	test: 2.652718

Epoch: 138
Loss: 0.6654233038425446
RMSE train: 0.727327	val: 4.334328	test: 3.429514
MAE train: 0.560786	val: 2.956922	test: 2.672170

Epoch: 139
Loss: 0.7466722428798676
RMSE train: 0.755081	val: 4.488592	test: 3.430549
MAE train: 0.596000	val: 3.053831	test: 2.694535

Epoch: 140
Loss: 0.6991422772407532
RMSE train: 0.803769	val: 4.583810	test: 3.433768
MAE train: 0.621187	val: 3.109962	test: 2.713277

Epoch: 141
Loss: 0.6889538764953613
RMSE train: 0.812520	val: 4.592640	test: 3.422222
MAE train: 0.617493	val: 3.123890	test: 2.707350

Epoch: 142
Loss: 0.5903202891349792
RMSE train: 0.766936	val: 4.580526	test: 3.422150
MAE train: 0.586704	val: 3.132945	test: 2.700748

Epoch: 143
Loss: 0.6708438396453857
RMSE train: 0.659269	val: 4.529800	test: 3.413207
MAE train: 0.516063	val: 3.104145	test: 2.684781
RMSE train: 0.999344	val: 4.417588	test: 4.567486
MAE train: 0.794907	val: 3.312425	test: 3.340209

Epoch: 84
Loss: 1.3969435095787048
RMSE train: 1.054361	val: 4.463092	test: 4.531099
MAE train: 0.832667	val: 3.333039	test: 3.355961

Epoch: 85
Loss: 1.286995768547058
RMSE train: 1.032676	val: 4.460851	test: 4.586818
MAE train: 0.814258	val: 3.339679	test: 3.404149

Epoch: 86
Loss: 1.1032791137695312
RMSE train: 0.979781	val: 4.440521	test: 4.536475
MAE train: 0.771655	val: 3.343739	test: 3.382230

Epoch: 87
Loss: 1.2058922052383423
RMSE train: 0.883855	val: 4.386368	test: 4.644591
MAE train: 0.701170	val: 3.344716	test: 3.409155

Epoch: 88
Loss: 1.1735817193984985
RMSE train: 0.813566	val: 4.337204	test: 4.668404
MAE train: 0.646121	val: 3.363371	test: 3.388265

Epoch: 89
Loss: 1.0687631368637085
RMSE train: 0.780630	val: 4.242010	test: 4.704714
MAE train: 0.620096	val: 3.309562	test: 3.396909

Epoch: 90
Loss: 1.2383825778961182
RMSE train: 0.763086	val: 4.163285	test: 4.705998
MAE train: 0.604202	val: 3.248779	test: 3.394792

Epoch: 91
Loss: 1.0197807252407074
RMSE train: 0.758776	val: 4.108669	test: 4.632356
MAE train: 0.600146	val: 3.187332	test: 3.349433

Epoch: 92
Loss: 1.352963387966156
RMSE train: 0.800255	val: 4.109035	test: 4.590495
MAE train: 0.635154	val: 3.163119	test: 3.339816

Epoch: 93
Loss: 1.140069842338562
RMSE train: 0.854255	val: 4.171247	test: 4.478582
MAE train: 0.679776	val: 3.192948	test: 3.282148

Epoch: 94
Loss: 1.065919816493988
RMSE train: 0.840987	val: 4.192856	test: 4.457959
MAE train: 0.670094	val: 3.248427	test: 3.250408

Epoch: 95
Loss: 1.0658264756202698
RMSE train: 0.841965	val: 4.208729	test: 4.383526
MAE train: 0.666289	val: 3.255204	test: 3.208793

Epoch: 96
Loss: 1.14468514919281
RMSE train: 0.818452	val: 4.172311	test: 4.401059
MAE train: 0.643668	val: 3.210505	test: 3.234742

Epoch: 97
Loss: 1.1272770762443542
RMSE train: 0.811185	val: 4.134673	test: 4.428099
MAE train: 0.634741	val: 3.152814	test: 3.273519

Epoch: 98
Loss: 1.1468469500541687
RMSE train: 0.796949	val: 4.098346	test: 4.570572
MAE train: 0.630706	val: 3.139837	test: 3.357518

Epoch: 99
Loss: 1.0659438967704773
RMSE train: 0.818757	val: 4.157504	test: 4.696647
MAE train: 0.655058	val: 3.204221	test: 3.427773

Epoch: 100
Loss: 1.2746870517730713
RMSE train: 0.871098	val: 4.265161	test: 4.762069
MAE train: 0.707820	val: 3.308994	test: 3.466331

Epoch: 101
Loss: 1.0658572316169739
RMSE train: 0.916337	val: 4.290325	test: 4.703322
MAE train: 0.747468	val: 3.327515	test: 3.445040

Epoch: 102
Loss: 0.9304421544075012
RMSE train: 0.924277	val: 4.254050	test: 4.622947
MAE train: 0.754082	val: 3.288944	test: 3.403324

Epoch: 103
Loss: 1.10804283618927
RMSE train: 0.935652	val: 4.267808	test: 4.552942
MAE train: 0.761336	val: 3.276082	test: 3.363924

Epoch: 104
Loss: 1.0403406620025635
RMSE train: 0.926916	val: 4.298506	test: 4.415092
MAE train: 0.749379	val: 3.299003	test: 3.288280

Epoch: 105
Loss: 0.9985522627830505
RMSE train: 0.893846	val: 4.319644	test: 4.365084
MAE train: 0.720826	val: 3.323695	test: 3.258363

Epoch: 106
Loss: 1.0479859113693237
RMSE train: 0.817730	val: 4.255789	test: 4.368090
MAE train: 0.659098	val: 3.289605	test: 3.251279

Epoch: 107
Loss: 1.0161595940589905
RMSE train: 0.778186	val: 4.213838	test: 4.409609
MAE train: 0.627829	val: 3.261509	test: 3.273789

Epoch: 108
Loss: 1.0695510506629944
RMSE train: 0.766539	val: 4.217257	test: 4.448465
MAE train: 0.618176	val: 3.281711	test: 3.305641

Epoch: 109
Loss: 0.8109952509403229
RMSE train: 0.821342	val: 4.279001	test: 4.511613
MAE train: 0.663103	val: 3.305522	test: 3.370914

Epoch: 110
Loss: 1.151611566543579
RMSE train: 0.925545	val: 4.357000	test: 4.563192
MAE train: 0.746137	val: 3.307886	test: 3.432575

Epoch: 111
Loss: 0.9150916337966919
RMSE train: 0.922703	val: 4.378483	test: 4.644272
MAE train: 0.737949	val: 3.330500	test: 3.476427

Epoch: 112
Loss: 1.0479248762130737
RMSE train: 0.901235	val: 4.320600	test: 4.465201
MAE train: 0.710144	val: 3.287773	test: 3.342170

Epoch: 113
Loss: 0.9707529842853546
RMSE train: 0.836821	val: 4.250983	test: 4.359951
MAE train: 0.654560	val: 3.275261	test: 3.254584

Epoch: 114
Loss: 0.9039017856121063
RMSE train: 0.817645	val: 4.247429	test: 4.324198
MAE train: 0.641054	val: 3.277162	test: 3.209623

Epoch: 115
Loss: 0.8366187512874603
RMSE train: 0.802721	val: 4.254345	test: 4.371375
MAE train: 0.636433	val: 3.280756	test: 3.224026

Epoch: 116
Loss: 1.0713585019111633
RMSE train: 0.822039	val: 4.294998	test: 4.459083
MAE train: 0.658674	val: 3.298127	test: 3.286686

Epoch: 117
Loss: 0.8721258342266083
RMSE train: 0.899375	val: 4.371295	test: 4.463008
MAE train: 0.721062	val: 3.292745	test: 3.319294

Epoch: 118
Loss: 0.9219366312026978
RMSE train: 0.932844	val: 4.384168	test: 4.387201
MAE train: 0.741845	val: 3.240092	test: 3.294411

Epoch: 119
Loss: 0.8997726142406464
RMSE train: 0.889722	val: 4.290906	test: 4.338834
MAE train: 0.705134	val: 3.137635	test: 3.266854

Epoch: 120
Loss: 0.9870737493038177
RMSE train: 0.794384	val: 4.169150	test: 4.467065
MAE train: 0.636329	val: 3.080355	test: 3.314593

Epoch: 121
Loss: 0.8991282880306244
RMSE train: 0.748057	val: 4.082120	test: 4.545717
MAE train: 0.599135	val: 3.009429	test: 3.357295

Epoch: 122
Loss: 0.9055086672306061
RMSE train: 0.773742	val: 4.120656	test: 4.586864
MAE train: 0.622438	val: 3.017791	test: 3.375291

Epoch: 123
Loss: 1.0488843321800232
RMSE train: 0.786696	val: 4.148011	test: 4.508428
MAE train: 0.634208	val: 3.063055	test: 3.304412

Epoch: 124
Loss: 0.8678233921527863
RMSE train: 0.826248	val: 4.217860	test: 4.306280
MAE train: 0.665611	val: 3.125431	test: 3.171887

Epoch: 125
Loss: 0.824320912361145
RMSE train: 0.911447	val: 4.369141	test: 4.137690
MAE train: 0.723509	val: 3.215190	test: 3.066397

Epoch: 126
Loss: 0.9068959951400757
RMSE train: 0.914416	val: 4.447089	test: 4.164982
MAE train: 0.720129	val: 3.307900	test: 3.062678

Epoch: 127
Loss: 0.8621880114078522
RMSE train: 0.906559	val: 4.464375	test: 4.278746
MAE train: 0.712209	val: 3.358256	test: 3.129974

Epoch: 128
Loss: 0.7970143556594849
RMSE train: 0.885217	val: 4.476451	test: 4.437336
MAE train: 0.693442	val: 3.416297	test: 3.219719

Epoch: 129
Loss: 0.8050670623779297
RMSE train: 0.874694	val: 4.469597	test: 4.455280
MAE train: 0.684444	val: 3.414371	test: 3.229858

Epoch: 130
Loss: 0.8574458658695221
RMSE train: 0.833273	val: 4.388130	test: 4.380398
MAE train: 0.649756	val: 3.311421	test: 3.175707

Epoch: 131
Loss: 0.8560492396354675
RMSE train: 0.771029	val: 4.239786	test: 4.348056
MAE train: 0.607086	val: 3.144063	test: 3.146309

Epoch: 132
Loss: 0.7920975387096405
RMSE train: 0.726651	val: 4.159700	test: 4.424055
MAE train: 0.580114	val: 3.060325	test: 3.186584

Epoch: 133
Loss: 0.7843499779701233
RMSE train: 0.743067	val: 4.185841	test: 4.444688
MAE train: 0.598455	val: 3.072629	test: 3.213554

Epoch: 134
Loss: 0.7556203007698059
RMSE train: 0.779321	val: 4.213651	test: 4.420025
MAE train: 0.630556	val: 3.069142	test: 3.224916

Epoch: 135
Loss: 0.778187483549118
RMSE train: 0.782304	val: 4.156135	test: 4.414098
MAE train: 0.631739	val: 3.004332	test: 3.220577

Epoch: 136
Loss: 0.7385580837726593
RMSE train: 0.764519	val: 4.116539	test: 4.509832
MAE train: 0.616120	val: 2.975531	test: 3.265054

Epoch: 137
Loss: 0.7253716886043549
RMSE train: 0.782434	val: 4.151790	test: 4.606446
MAE train: 0.628926	val: 3.002553	test: 3.317232

Epoch: 138
Loss: 0.7476060390472412
RMSE train: 0.788019	val: 4.259937	test: 4.819038
MAE train: 0.634592	val: 3.103952	test: 3.434878

Epoch: 139
Loss: 0.8807568848133087
RMSE train: 0.792065	val: 4.318755	test: 4.872692
MAE train: 0.639462	val: 3.185006	test: 3.475991

Epoch: 140
Loss: 0.8130372762680054
RMSE train: 0.768706	val: 4.380005	test: 5.092759
MAE train: 0.625442	val: 3.309015	test: 3.585423

Epoch: 141
Loss: 0.7120934426784515
RMSE train: 0.764498	val: 4.358858	test: 5.136875
MAE train: 0.622743	val: 3.290285	test: 3.598561

Epoch: 142
Loss: 0.7016851603984833
RMSE train: 0.749473	val: 4.228807	test: 4.961353
MAE train: 0.608553	val: 3.152445	test: 3.490091

Epoch: 143
Loss: 0.808012992143631
RMSE train: 0.712324	val: 4.084908	test: 4.811514
MAE train: 0.572836	val: 3.031957	test: 3.389250

Epoch: 144
Loss: 0.6585333943367004
RMSE train: 0.589761	val: 4.520897	test: 3.426083
MAE train: 0.459744	val: 3.101616	test: 2.690084

Epoch: 145
Loss: 0.6235924661159515
RMSE train: 0.599502	val: 4.561743	test: 3.443645
MAE train: 0.468205	val: 3.135493	test: 2.711466

Epoch: 146
Loss: 0.6764211356639862
RMSE train: 0.655446	val: 4.629101	test: 3.462910
MAE train: 0.520927	val: 3.162202	test: 2.728138

Epoch: 147
Loss: 0.597521036863327
RMSE train: 0.721491	val: 4.675882	test: 3.461008
MAE train: 0.574039	val: 3.169883	test: 2.718710

Epoch: 148
Loss: 0.6214791238307953
RMSE train: 0.782703	val: 4.687732	test: 3.445049
MAE train: 0.621018	val: 3.163045	test: 2.702497

Epoch: 149
Loss: 0.7018790543079376
RMSE train: 0.774247	val: 4.634290	test: 3.410770
MAE train: 0.609114	val: 3.137903	test: 2.668520

Epoch: 150
Loss: 0.7680397033691406
RMSE train: 0.757961	val: 4.605827	test: 3.401996
MAE train: 0.589889	val: 3.126422	test: 2.663919

Epoch: 151
Loss: 0.6213753372430801
RMSE train: 0.752374	val: 4.599775	test: 3.409672
MAE train: 0.582122	val: 3.133299	test: 2.665877

Epoch: 152
Loss: 0.6258274614810944
RMSE train: 0.748367	val: 4.581972	test: 3.423853
MAE train: 0.573805	val: 3.142050	test: 2.662502

Epoch: 153
Loss: 0.6211861073970795
RMSE train: 0.741628	val: 4.565155	test: 3.427110
MAE train: 0.552561	val: 3.131264	test: 2.647350

Epoch: 154
Loss: 0.6066561937332153
RMSE train: 0.740679	val: 4.520767	test: 3.422995
MAE train: 0.541487	val: 3.090601	test: 2.639085

Epoch: 155
Loss: 0.6429970562458038
RMSE train: 0.736629	val: 4.498320	test: 3.408055
MAE train: 0.545506	val: 3.072568	test: 2.621484

Epoch: 156
Loss: 0.7003998160362244
RMSE train: 0.722465	val: 4.488677	test: 3.375287
MAE train: 0.554954	val: 3.075956	test: 2.608276

Epoch: 157
Loss: 0.7296290993690491
RMSE train: 0.714438	val: 4.460275	test: 3.325525
MAE train: 0.552129	val: 3.078836	test: 2.601311

Epoch: 158
Loss: 0.6244788467884064
RMSE train: 0.711651	val: 4.456962	test: 3.320902
MAE train: 0.551835	val: 3.094907	test: 2.609459

Epoch: 159
Loss: 0.529581144452095
RMSE train: 0.689333	val: 4.430767	test: 3.337232
MAE train: 0.543060	val: 3.079264	test: 2.619951

Epoch: 160
Loss: 0.507251426577568
RMSE train: 0.711837	val: 4.439082	test: 3.385159
MAE train: 0.566655	val: 3.073145	test: 2.637367

Epoch: 161
Loss: 0.5623345375061035
RMSE train: 0.726647	val: 4.447872	test: 3.423179
MAE train: 0.579068	val: 3.074791	test: 2.665986

Epoch: 162
Loss: 0.7416369915008545
RMSE train: 0.736347	val: 4.484098	test: 3.441479
MAE train: 0.580411	val: 3.080785	test: 2.686150

Epoch: 163
Loss: 0.5451489388942719
RMSE train: 0.721561	val: 4.455007	test: 3.415328
MAE train: 0.558025	val: 3.058391	test: 2.660474

Epoch: 164
Loss: 0.6113372445106506
RMSE train: 0.713098	val: 4.448138	test: 3.361170
MAE train: 0.544256	val: 3.052462	test: 2.609076

Epoch: 165
Loss: 0.5586854517459869
RMSE train: 0.717959	val: 4.450553	test: 3.296553
MAE train: 0.537945	val: 3.029159	test: 2.545468

Epoch: 166
Loss: 0.5189274251461029
RMSE train: 0.691243	val: 4.421151	test: 3.238649
MAE train: 0.520992	val: 2.990330	test: 2.488838

Epoch: 167
Loss: 0.6281735301017761
RMSE train: 0.609467	val: 4.393052	test: 3.235139
MAE train: 0.468543	val: 2.974635	test: 2.479273

Epoch: 168
Loss: 0.5026546865701675
RMSE train: 0.567660	val: 4.414262	test: 3.278546
MAE train: 0.448335	val: 2.985712	test: 2.508128

Epoch: 169
Loss: 0.5697448551654816
RMSE train: 0.568125	val: 4.415097	test: 3.333877
MAE train: 0.450051	val: 2.997184	test: 2.560614

Epoch: 170
Loss: 0.5953995287418365
RMSE train: 0.654147	val: 4.499761	test: 3.382945
MAE train: 0.517976	val: 3.037679	test: 2.621255

Epoch: 171
Loss: 0.5571343451738358
RMSE train: 0.719886	val: 4.568332	test: 3.411298
MAE train: 0.564696	val: 3.069173	test: 2.656880

Early stopping
Best (RMSE):	 train: 0.642260	val: 4.261818	test: 3.371395
Best (MAE):	 train: 0.500942	val: 2.896398	test: 2.613984


Epoch: 144
Loss: 0.9357833862304688
RMSE train: 0.701440	val: 4.506760	test: 3.636400
MAE train: 0.565726	val: 3.191513	test: 2.893203

Epoch: 145
Loss: 0.5887093842029572
RMSE train: 0.754441	val: 4.561501	test: 3.654677
MAE train: 0.598495	val: 3.223414	test: 2.913048

Epoch: 146
Loss: 0.7206578552722931
RMSE train: 0.721678	val: 4.521956	test: 3.613765
MAE train: 0.569667	val: 3.197343	test: 2.869803

Epoch: 147
Loss: 0.6924980282783508
RMSE train: 0.640211	val: 4.432501	test: 3.563578
MAE train: 0.503414	val: 3.128164	test: 2.817901

Epoch: 148
Loss: 0.6281024515628815
RMSE train: 0.596532	val: 4.393345	test: 3.518130
MAE train: 0.471009	val: 3.093605	test: 2.778597

Epoch: 149
Loss: 0.5940388143062592
RMSE train: 0.604882	val: 4.428038	test: 3.521485
MAE train: 0.476295	val: 3.105199	test: 2.781130

Epoch: 150
Loss: 0.7666535973548889
RMSE train: 0.642230	val: 4.492395	test: 3.565422
MAE train: 0.499489	val: 3.154879	test: 2.820185

Epoch: 151
Loss: 0.5712830424308777
RMSE train: 0.682125	val: 4.545750	test: 3.605046
MAE train: 0.525686	val: 3.193364	test: 2.849574

Epoch: 152
Loss: 0.6320855617523193
RMSE train: 0.662308	val: 4.504298	test: 3.596082
MAE train: 0.513246	val: 3.185759	test: 2.836677

Epoch: 153
Loss: 0.6776803135871887
RMSE train: 0.666275	val: 4.535523	test: 3.601359
MAE train: 0.521587	val: 3.203132	test: 2.831819

Epoch: 154
Loss: 0.5578416734933853
RMSE train: 0.673009	val: 4.579605	test: 3.640310
MAE train: 0.535362	val: 3.221050	test: 2.867537

Epoch: 155
Loss: 0.6367955207824707
RMSE train: 0.674592	val: 4.609615	test: 3.670381
MAE train: 0.539787	val: 3.223213	test: 2.897220

Epoch: 156
Loss: 0.6264181137084961
RMSE train: 0.634493	val: 4.571094	test: 3.669217
MAE train: 0.507906	val: 3.187934	test: 2.895266

Epoch: 157
Loss: 0.6428650915622711
RMSE train: 0.589742	val: 4.520009	test: 3.656600
MAE train: 0.466075	val: 3.157868	test: 2.891983

Epoch: 158
Loss: 0.5974366664886475
RMSE train: 0.592800	val: 4.516127	test: 3.666058
MAE train: 0.458129	val: 3.183585	test: 2.907624

Epoch: 159
Loss: 0.5925992429256439
RMSE train: 0.597556	val: 4.530565	test: 3.693828
MAE train: 0.460702	val: 3.193384	test: 2.932441

Epoch: 160
Loss: 0.6421505212783813
RMSE train: 0.654614	val: 4.657362	test: 3.754342
MAE train: 0.515959	val: 3.253371	test: 2.985790

Epoch: 161
Loss: 0.6278475224971771
RMSE train: 0.653413	val: 4.696903	test: 3.762147
MAE train: 0.514882	val: 3.272586	test: 2.992973

Epoch: 162
Loss: 0.6158300340175629
RMSE train: 0.590172	val: 4.645846	test: 3.737667
MAE train: 0.464816	val: 3.236748	test: 2.969638

Epoch: 163
Loss: 0.6120579838752747
RMSE train: 0.567408	val: 4.626227	test: 3.746440
MAE train: 0.446410	val: 3.226056	test: 2.979528

Epoch: 164
Loss: 0.6369113624095917
RMSE train: 0.568703	val: 4.599494	test: 3.753707
MAE train: 0.446248	val: 3.207017	test: 2.980624

Epoch: 165
Loss: 0.5351622104644775
RMSE train: 0.591968	val: 4.585150	test: 3.766798
MAE train: 0.467331	val: 3.195315	test: 2.985512

Epoch: 166
Loss: 0.6646845936775208
RMSE train: 0.610245	val: 4.569654	test: 3.779959
MAE train: 0.481889	val: 3.178579	test: 2.997615

Epoch: 167
Loss: 0.5326550006866455
RMSE train: 0.674356	val: 4.635972	test: 3.808449
MAE train: 0.529526	val: 3.213573	test: 3.033940

Epoch: 168
Loss: 0.6194747686386108
RMSE train: 0.673163	val: 4.656765	test: 3.809533
MAE train: 0.527888	val: 3.214484	test: 3.044971

Epoch: 169
Loss: 0.5472098886966705
RMSE train: 0.658019	val: 4.658088	test: 3.790522
MAE train: 0.521715	val: 3.200813	test: 3.036169

Epoch: 170
Loss: 0.6251951158046722
RMSE train: 0.623846	val: 4.640099	test: 3.729878
MAE train: 0.498027	val: 3.187891	test: 2.982068

Epoch: 171
Loss: 0.5545259714126587
RMSE train: 0.634732	val: 4.647639	test: 3.705872
MAE train: 0.503496	val: 3.198757	test: 2.956146

Epoch: 172
Loss: 0.5906969010829926
RMSE train: 0.674102	val: 4.691242	test: 3.680014
MAE train: 0.516991	val: 3.234299	test: 2.930724

Epoch: 173
Loss: 0.5634135901927948
RMSE train: 0.699843	val: 4.702943	test: 3.632516
MAE train: 0.516755	val: 3.243252	test: 2.881111

Epoch: 174
Loss: 0.6912015080451965
RMSE train: 0.660416	val: 4.663144	test: 3.583447
MAE train: 0.489950	val: 3.211188	test: 2.828010

Epoch: 175
Loss: 0.7304149866104126
RMSE train: 0.589675	val: 4.565950	test: 3.548484
MAE train: 0.451487	val: 3.143148	test: 2.787471

Epoch: 176
Loss: 0.6171779930591583
RMSE train: 0.541253	val: 4.480086	test: 3.551480
MAE train: 0.431166	val: 3.074446	test: 2.786254

Epoch: 177
Loss: 0.5673373937606812
RMSE train: 0.570326	val: 4.474051	test: 3.598477
MAE train: 0.462859	val: 3.069748	test: 2.827053

Early stopping
Best (RMSE):	 train: 0.585063	val: 4.391353	test: 3.587906
Best (MAE):	 train: 0.471249	val: 3.125453	test: 2.837519
All runs completed.
All runs completed.


Epoch: 144
Loss: 0.746344268321991
RMSE train: 0.711356	val: 3.979234	test: 4.590487
MAE train: 0.565769	val: 2.938149	test: 3.259721

Epoch: 145
Loss: 0.7036872804164886
RMSE train: 0.700925	val: 3.960190	test: 4.454439
MAE train: 0.553846	val: 2.968188	test: 3.172848

Epoch: 146
Loss: 0.8381098210811615
RMSE train: 0.663587	val: 4.055102	test: 4.482332
MAE train: 0.526484	val: 3.097553	test: 3.171926

Epoch: 147
Loss: 0.7147403657436371
RMSE train: 0.694065	val: 4.230473	test: 4.485233
MAE train: 0.548335	val: 3.275146	test: 3.181104

Epoch: 148
Loss: 0.8350813388824463
RMSE train: 0.728197	val: 4.335226	test: 4.557070
MAE train: 0.576332	val: 3.379965	test: 3.240104

Epoch: 149
Loss: 0.6902317702770233
RMSE train: 0.733274	val: 4.394358	test: 4.666376
MAE train: 0.578842	val: 3.432803	test: 3.307679

Epoch: 150
Loss: 0.7225033044815063
RMSE train: 0.721965	val: 4.302631	test: 4.592963
MAE train: 0.563415	val: 3.310843	test: 3.301415

Epoch: 151
Loss: 0.7372187674045563
RMSE train: 0.748156	val: 4.201553	test: 4.387096
MAE train: 0.579409	val: 3.111937	test: 3.253621

Epoch: 152
Loss: 0.7732858955860138
RMSE train: 0.771422	val: 4.185256	test: 4.381343
MAE train: 0.598718	val: 3.013548	test: 3.294467

Epoch: 153
Loss: 0.6666814982891083
RMSE train: 0.758546	val: 4.174871	test: 4.510737
MAE train: 0.595290	val: 3.010110	test: 3.380569

Epoch: 154
Loss: 0.6941518187522888
RMSE train: 0.739640	val: 4.205487	test: 4.684282
MAE train: 0.592052	val: 3.076752	test: 3.464372

Epoch: 155
Loss: 0.6843581199645996
RMSE train: 0.782432	val: 4.266225	test: 4.708782
MAE train: 0.639009	val: 3.150287	test: 3.485631

Epoch: 156
Loss: 0.6779651641845703
RMSE train: 0.883009	val: 4.346359	test: 4.642309
MAE train: 0.723383	val: 3.202724	test: 3.460324

Epoch: 157
Loss: 0.7733393609523773
RMSE train: 0.885737	val: 4.314831	test: 4.632064
MAE train: 0.722040	val: 3.183519	test: 3.441367

Epoch: 158
Loss: 0.621351957321167
RMSE train: 0.830719	val: 4.213880	test: 4.554675
MAE train: 0.673046	val: 3.101453	test: 3.387809

Epoch: 159
Loss: 0.735165148973465
RMSE train: 0.784647	val: 4.148275	test: 4.454960
MAE train: 0.633313	val: 3.031300	test: 3.324894

Epoch: 160
Loss: 0.6298549473285675
RMSE train: 0.768097	val: 4.172851	test: 4.422017
MAE train: 0.616605	val: 3.042719	test: 3.290672

Epoch: 161
Loss: 0.6893535554409027
RMSE train: 0.788266	val: 4.208573	test: 4.276305
MAE train: 0.620354	val: 3.016218	test: 3.205809

Epoch: 162
Loss: 0.827530026435852
RMSE train: 0.783846	val: 4.238044	test: 4.156075
MAE train: 0.603978	val: 3.029536	test: 3.109754

Epoch: 163
Loss: 0.692962110042572
RMSE train: 0.744945	val: 4.222463	test: 4.192020
MAE train: 0.574092	val: 3.047409	test: 3.089528

Epoch: 164
Loss: 0.5741235017776489
RMSE train: 0.698324	val: 4.200583	test: 4.340932
MAE train: 0.543200	val: 3.097957	test: 3.153981

Epoch: 165
Loss: 0.5740233957767487
RMSE train: 0.655677	val: 4.218355	test: 4.460593
MAE train: 0.512827	val: 3.189448	test: 3.211932

Epoch: 166
Loss: 0.7778840065002441
RMSE train: 0.604766	val: 4.217422	test: 4.617622
MAE train: 0.479282	val: 3.255023	test: 3.284256

Epoch: 167
Loss: 0.66588094830513
RMSE train: 0.595505	val: 4.162645	test: 4.485023
MAE train: 0.472338	val: 3.200355	test: 3.224656

Epoch: 168
Loss: 0.5950020551681519
RMSE train: 0.650119	val: 4.142631	test: 4.299703
MAE train: 0.511043	val: 3.139169	test: 3.139672

Epoch: 169
Loss: 0.5981618463993073
RMSE train: 0.669188	val: 4.145208	test: 4.289938
MAE train: 0.529318	val: 3.146176	test: 3.131655

Epoch: 170
Loss: 0.5640189051628113
RMSE train: 0.714049	val: 4.145740	test: 4.249827
MAE train: 0.564439	val: 3.114811	test: 3.115570

Epoch: 171
Loss: 0.5924996435642242
RMSE train: 0.697403	val: 4.113707	test: 4.356930
MAE train: 0.555630	val: 3.119305	test: 3.163118

Epoch: 172
Loss: 0.6846373081207275
RMSE train: 0.705433	val: 4.100597	test: 4.364180
MAE train: 0.559409	val: 3.094773	test: 3.174553

Epoch: 173
Loss: 0.6313930153846741
RMSE train: 0.739701	val: 4.183257	test: 4.455539
MAE train: 0.586845	val: 3.161643	test: 3.220840

Epoch: 174
Loss: 0.6490706503391266
RMSE train: 0.758165	val: 4.314107	test: 4.544848
MAE train: 0.598313	val: 3.302698	test: 3.264104

Epoch: 175
Loss: 0.6825034916400909
RMSE train: 0.756144	val: 4.513973	test: 4.771330
MAE train: 0.590927	val: 3.500525	test: 3.362093

Epoch: 176
Loss: 0.6263729333877563
RMSE train: 0.721788	val: 4.608632	test: 4.898683
MAE train: 0.557213	val: 3.603830	test: 3.420399

Epoch: 177
Loss: 0.6002438366413116
RMSE train: 0.706039	val: 4.545539	test: 4.779493
MAE train: 0.539433	val: 3.526576	test: 3.388419

Epoch: 178
Loss: 0.6088669300079346
RMSE train: 0.667339	val: 4.579732	test: 4.986654
MAE train: 0.517556	val: 3.609743	test: 3.497397

Epoch: 179
Loss: 0.8089529573917389
RMSE train: 0.712040	val: 4.631546	test: 5.114126
MAE train: 0.561110	val: 3.655596	test: 3.567051

Epoch: 180
Loss: 0.6273070573806763
RMSE train: 0.760808	val: 4.510585	test: 4.792223
MAE train: 0.600305	val: 3.496544	test: 3.430127

Early stopping
Best (RMSE):	 train: 0.700925	val: 3.960190	test: 4.454439
Best (MAE):	 train: 0.553846	val: 2.968188	test: 3.172848
All runs completed.
