>>> Starting run for dataset: bbbp
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running SCAFF configs_split_experiments/GraphMVP/bbbp/scaff/train_prop=0.6.yml on cuda:0
Running SCAFF configs_split_experiments/GraphMVP/bbbp/scaff/train_prop=0.7.yml on cuda:1
Running SCAFF configs_split_experiments/GraphMVP/bbbp/scaff/train_prop=0.8.yml on cuda:2
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphMVP/bbbp/scaff/train_prop=0.8.yml --runseed 4 --device cuda:2
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphMVP/bbbp/scaff/train_prop=0.8.yml --runseed 5 --device cuda:2
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphMVP/bbbp/scaff/train_prop=0.8.yml --runseed 6 --device cuda:2
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphMVP/bbbp/scaff/train_prop=0.6.yml --runseed 4 --device cuda:0
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphMVP/bbbp/scaff/train_prop=0.7.yml --runseed 4 --device cuda:1
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphMVP/bbbp/scaff/train_prop=0.6.yml --runseed 5 --device cuda:0
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphMVP/bbbp/scaff/train_prop=0.7.yml --runseed 5 --device cuda:1
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphMVP/bbbp/scaff/train_prop=0.6.yml --runseed 6 --device cuda:0
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphMVP/bbbp/scaff/train_prop=0.7.yml --runseed 6 --device cuda:1
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] [11:11:03] WARNING: not removing hydrogen atom without neighborsWARNING: not removing hydrogen atom without neighbors

[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
[11:11:03] WARNING: not removing hydrogen atom without neighbors
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/bbbp/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/split/GraphMVP/bbbp/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/bbbp/scaff/train_prop=0.6/bbbp_scaff_5_26-05_11-11-03  ]
[ Using Seed :  5  ]
[ Using device :  cuda:0  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6689306592043536
0 is invalid
0
Some target is missing!
Missing ratio: 1.000000
Traceback (most recent call last):
  File "molecule_finetune.py", line 263, in <module>
    val_result, val_target, val_pred, val_loss = eval(model, device, val_loader, compute_loss=True)
  File "molecule_finetune.py", line 107, in eval
    return {'ROC': sum(roc_list) / len(roc_list), 'PRC': sum(prc_list) / len(prc_list)}, y_true, y_scores, total_loss/len(loader)
ZeroDivisionError: division by zero
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/bbbp/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/split/GraphMVP/bbbp/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/bbbp/scaff/train_prop=0.6/bbbp_scaff_4_26-05_11-11-03  ]
[ Using Seed :  4  ]
[ Using device :  cuda:0  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6343986656486543
0 is invalid
0
Some target is missing!
Missing ratio: 1.000000
Traceback (most recent call last):
  File "molecule_finetune.py", line 263, in <module>
    val_result, val_target, val_pred, val_loss = eval(model, device, val_loader, compute_loss=True)
  File "molecule_finetune.py", line 107, in eval
    return {'ROC': sum(roc_list) / len(roc_list), 'PRC': sum(prc_list) / len(prc_list)}, y_true, y_scores, total_loss/len(loader)
ZeroDivisionError: division by zero
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/bbbp/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/split/GraphMVP/bbbp/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/bbbp/scaff/train_prop=0.6/bbbp_scaff_6_26-05_11-11-03  ]
[ Using Seed :  6  ]
[ Using device :  cuda:0  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6662659271589905
0 is invalid
0
Some target is missing!
Missing ratio: 1.000000
Traceback (most recent call last):
  File "molecule_finetune.py", line 263, in <module>
    val_result, val_target, val_pred, val_loss = eval(model, device, val_loader, compute_loss=True)
  File "molecule_finetune.py", line 107, in eval
    return {'ROC': sum(roc_list) / len(roc_list), 'PRC': sum(prc_list) / len(prc_list)}, y_true, y_scores, total_loss/len(loader)
ZeroDivisionError: division by zero
All runs completed.
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/bbbp/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/split/GraphMVP/bbbp/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/bbbp/scaff/train_prop=0.7/bbbp_scaff_4_26-05_11-11-03  ]
[ Using Seed :  4  ]
[ Using device :  cuda:1  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6204939063815947
ROC train: 0.811949	val: 0.935509	test: 0.734274
PRC train: 0.943137	val: 0.990318	test: 0.625894

Epoch: 2
Loss: 0.5210731237867039
ROC train: 0.845098	val: 0.946276	test: 0.750582
PRC train: 0.952702	val: 0.992713	test: 0.658967

Epoch: 3
Loss: 0.43832081896954045
ROC train: 0.873403	val: 0.942169	test: 0.746729
PRC train: 0.960760	val: 0.991990	test: 0.636489

Epoch: 4
Loss: 0.37321611264873633
ROC train: 0.901541	val: 0.952714	test: 0.762948
PRC train: 0.970530	val: 0.993262	test: 0.625848

Epoch: 5
Loss: 0.333232106681094
ROC train: 0.916474	val: 0.964702	test: 0.755600
PRC train: 0.975909	val: 0.995560	test: 0.627726

Epoch: 6
Loss: 0.3148616648131857
ROC train: 0.928027	val: 0.958375	test: 0.762993
PRC train: 0.979015	val: 0.994480	test: 0.650220

Epoch: 7
Loss: 0.2974696785376326
ROC train: 0.922360	val: 0.945943	test: 0.764830
PRC train: 0.976983	val: 0.991841	test: 0.655170

Epoch: 8
Loss: 0.27792528458402416
ROC train: 0.935841	val: 0.947608	test: 0.766039
PRC train: 0.981219	val: 0.991626	test: 0.646181

Epoch: 9
Loss: 0.27091654962468986
ROC train: 0.941416	val: 0.956488	test: 0.773208
PRC train: 0.982741	val: 0.993937	test: 0.682886

Epoch: 10
Loss: 0.2703408426065664
ROC train: 0.942034	val: 0.962482	test: 0.785573
PRC train: 0.981528	val: 0.995037	test: 0.700814

Epoch: 11
Loss: 0.24320490884770687
ROC train: 0.949555	val: 0.964147	test: 0.779167
PRC train: 0.985371	val: 0.995085	test: 0.689568

Epoch: 12
Loss: 0.24437609591941478
ROC train: 0.947510	val: 0.967144	test: 0.780511
PRC train: 0.984536	val: 0.995424	test: 0.672268

Epoch: 13
Loss: 0.24074053565856804
ROC train: 0.948225	val: 0.966256	test: 0.771192
PRC train: 0.984925	val: 0.995244	test: 0.669120

Epoch: 14
Loss: 0.23261073231552618
ROC train: 0.952833	val: 0.957709	test: 0.780421
PRC train: 0.985684	val: 0.993955	test: 0.687674

Epoch: 15
Loss: 0.2201306271385658
ROC train: 0.957831	val: 0.953047	test: 0.779480
PRC train: 0.987609	val: 0.992766	test: 0.685209

Epoch: 16
Loss: 0.2198144557567898
ROC train: 0.953070	val: 0.953380	test: 0.778047
PRC train: 0.986249	val: 0.992543	test: 0.685360

Epoch: 17
Loss: 0.22397515727252218
ROC train: 0.961331	val: 0.951604	test: 0.782079
PRC train: 0.988955	val: 0.991555	test: 0.688055

Epoch: 18
Loss: 0.21797994154924646
ROC train: 0.959828	val: 0.958930	test: 0.783333
PRC train: 0.987683	val: 0.994018	test: 0.701107

Epoch: 19
Loss: 0.21110573992803464
ROC train: 0.961871	val: 0.961594	test: 0.785349
PRC train: 0.988591	val: 0.994344	test: 0.673655

Epoch: 20
Loss: 0.20231728110465277
ROC train: 0.961959	val: 0.959596	test: 0.777912
PRC train: 0.988605	val: 0.994154	test: 0.668637

Epoch: 21
Loss: 0.21219327935224022
ROC train: 0.962305	val: 0.949717	test: 0.777599
PRC train: 0.988879	val: 0.992778	test: 0.670411

Epoch: 22
Loss: 0.20386724593972247
ROC train: 0.965942	val: 0.965812	test: 0.778943
PRC train: 0.989927	val: 0.995159	test: 0.683848

Epoch: 23
Loss: 0.18414965775092365
ROC train: 0.970455	val: 0.956044	test: 0.783871
PRC train: 0.991877	val: 0.993248	test: 0.689973

Epoch: 24
Loss: 0.18358623611345873
ROC train: 0.971640	val: 0.952936	test: 0.783557
PRC train: 0.992170	val: 0.992437	test: 0.688175

Epoch: 25
Loss: 0.18558887929697823
ROC train: 0.973670	val: 0.945277	test: 0.780018
PRC train: 0.992868	val: 0.988656	test: 0.677803

Epoch: 26
Loss: 0.18382920416847628
ROC train: 0.974153	val: 0.949162	test: 0.774194
PRC train: 0.993279	val: 0.989829	test: 0.662422

Epoch: 27
Loss: 0.17837368129955153
ROC train: 0.974621	val: 0.957931	test: 0.783109
PRC train: 0.993152	val: 0.993075	test: 0.689447

Epoch: 28
Loss: 0.19219999646819477
ROC train: 0.978128	val: 0.950161	test: 0.784857
PRC train: 0.994218	val: 0.991579	test: 0.683284

Epoch: 29
Loss: 0.1721289222932301
ROC train: 0.976836	val: 0.941170	test: 0.781631
PRC train: 0.993842	val: 0.989894	test: 0.703930

Epoch: 30
Loss: 0.16085827293990987
ROC train: 0.979240	val: 0.941392	test: 0.775806
PRC train: 0.994741	val: 0.988633	test: 0.678500

Epoch: 31
Loss: 0.1585756432650602
ROC train: 0.980581	val: 0.947719	test: 0.769131
PRC train: 0.995252	val: 0.990419	test: 0.670459

Epoch: 32
Loss: 0.17107862476562216
ROC train: 0.981478	val: 0.958708	test: 0.777375
PRC train: 0.995422	val: 0.993877	test: 0.669812

Epoch: 33
Loss: 0.16806453171723099Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/bbbp/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/split/GraphMVP/bbbp/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/bbbp/scaff/train_prop=0.7/bbbp_scaff_6_26-05_11-11-03  ]
[ Using Seed :  6  ]
[ Using device :  cuda:1  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6570277886794935
ROC train: 0.786901	val: 0.913531	test: 0.730108
PRC train: 0.932099	val: 0.985442	test: 0.598102

Epoch: 2
Loss: 0.5562350823951073
ROC train: 0.827871	val: 0.945055	test: 0.756720
PRC train: 0.945099	val: 0.992171	test: 0.656487

Epoch: 3
Loss: 0.46393612248599053
ROC train: 0.851631	val: 0.953380	test: 0.765009
PRC train: 0.951897	val: 0.993969	test: 0.680587

Epoch: 4
Loss: 0.39046114764360923
ROC train: 0.877312	val: 0.941614	test: 0.768907
PRC train: 0.963432	val: 0.991961	test: 0.656956

Epoch: 5
Loss: 0.3538969501465414
ROC train: 0.900742	val: 0.936064	test: 0.762903
PRC train: 0.970652	val: 0.990197	test: 0.629764

Epoch: 6
Loss: 0.3360594274105026
ROC train: 0.915683	val: 0.944278	test: 0.770968
PRC train: 0.975881	val: 0.992014	test: 0.661077

Epoch: 7
Loss: 0.3076691677926388
ROC train: 0.922250	val: 0.945388	test: 0.773566
PRC train: 0.977544	val: 0.992490	test: 0.682051

Epoch: 8
Loss: 0.28443139605610784
ROC train: 0.921943	val: 0.932290	test: 0.769444
PRC train: 0.976980	val: 0.989482	test: 0.668144

Epoch: 9
Loss: 0.28801474506091346
ROC train: 0.931535	val: 0.933178	test: 0.766935
PRC train: 0.980127	val: 0.989631	test: 0.667663

Epoch: 10
Loss: 0.2727115258986018
ROC train: 0.938155	val: 0.942724	test: 0.778360
PRC train: 0.982334	val: 0.991208	test: 0.695948

Epoch: 11
Loss: 0.24532504512566186
ROC train: 0.941368	val: 0.951160	test: 0.782213
PRC train: 0.982999	val: 0.992874	test: 0.706658

Epoch: 12
Loss: 0.2570512172491159
ROC train: 0.944733	val: 0.952825	test: 0.784140
PRC train: 0.983471	val: 0.993436	test: 0.720159

Epoch: 13
Loss: 0.24525178704545578
ROC train: 0.946869	val: 0.950605	test: 0.785977
PRC train: 0.983940	val: 0.992993	test: 0.722326

Epoch: 14
Loss: 0.23740181487910658
ROC train: 0.950041	val: 0.954490	test: 0.780959
PRC train: 0.984598	val: 0.993515	test: 0.719302

Epoch: 15
Loss: 0.23081372420674487
ROC train: 0.952038	val: 0.952270	test: 0.786738
PRC train: 0.985402	val: 0.993284	test: 0.726247

Epoch: 16
Loss: 0.23573353481570491
ROC train: 0.954932	val: 0.954268	test: 0.787366
PRC train: 0.986981	val: 0.993565	test: 0.719082

Epoch: 17
Loss: 0.23107878865075512
ROC train: 0.959607	val: 0.960373	test: 0.786783
PRC train: 0.988283	val: 0.994657	test: 0.714888

Epoch: 18
Loss: 0.23019090472498802
ROC train: 0.955318	val: 0.951493	test: 0.788396
PRC train: 0.987029	val: 0.993239	test: 0.714147

Epoch: 19
Loss: 0.21833534995259696
ROC train: 0.962066	val: 0.946498	test: 0.782706
PRC train: 0.989575	val: 0.992099	test: 0.706183

Epoch: 20
Loss: 0.21979060707894768
ROC train: 0.959190	val: 0.951160	test: 0.792652
PRC train: 0.988510	val: 0.993170	test: 0.724198

Epoch: 21
Loss: 0.2065420799192159
ROC train: 0.965576	val: 0.945610	test: 0.778047
PRC train: 0.990463	val: 0.992112	test: 0.715650

Epoch: 22
Loss: 0.20493187308392127
ROC train: 0.964788	val: 0.944056	test: 0.785394
PRC train: 0.990464	val: 0.991817	test: 0.714704

Epoch: 23
Loss: 0.20148389548755277
ROC train: 0.968740	val: 0.952492	test: 0.784857
PRC train: 0.991769	val: 0.993220	test: 0.709692

Epoch: 24
Loss: 0.20844303516378812
ROC train: 0.970938	val: 0.948940	test: 0.786694
PRC train: 0.992255	val: 0.992462	test: 0.709931

Epoch: 25
Loss: 0.19553677060947475
ROC train: 0.970434	val: 0.951271	test: 0.788620
PRC train: 0.992255	val: 0.993008	test: 0.701887

Epoch: 26
Loss: 0.1989180024732485
ROC train: 0.973118	val: 0.940393	test: 0.786335
PRC train: 0.992701	val: 0.990457	test: 0.710146

Epoch: 27
Loss: 0.2090957240345932
ROC train: 0.967098	val: 0.939061	test: 0.785484
PRC train: 0.991070	val: 0.989796	test: 0.712186

Epoch: 28
Loss: 0.1999725154405323
ROC train: 0.967089	val: 0.941947	test: 0.784319
PRC train: 0.991024	val: 0.990836	test: 0.720594

Epoch: 29
Loss: 0.19367945691332655
ROC train: 0.973561	val: 0.941059	test: 0.789292
PRC train: 0.992527	val: 0.990226	test: 0.719369

Epoch: 30
Loss: 0.18068183834772203
ROC train: 0.975086	val: 0.924853	test: 0.771237
PRC train: 0.993581	val: 0.987564	test: 0.680892

Epoch: 31
Loss: 0.1869222419826401
ROC train: 0.975101	val: 0.939172	test: 0.766667
PRC train: 0.993866	val: 0.990221	test: 0.667442

Epoch: 32
Loss: 0.1884425354741506
ROC train: 0.975132	val: 0.948496	test: 0.784857
PRC train: 0.993622	val: 0.992483	test: 0.711191

Epoch: 33
Loss: 0.17899095997227796Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/bbbp/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/split/GraphMVP/bbbp/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/bbbp/scaff/train_prop=0.7/bbbp_scaff_5_26-05_11-11-03  ]
[ Using Seed :  5  ]
[ Using device :  cuda:1  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6545622829837092
ROC train: 0.784571	val: 0.903097	test: 0.721416
PRC train: 0.926514	val: 0.983849	test: 0.602127

Epoch: 2
Loss: 0.549613738561835
ROC train: 0.834370	val: 0.949384	test: 0.745475
PRC train: 0.943845	val: 0.993281	test: 0.634304

Epoch: 3
Loss: 0.4615156715603748
ROC train: 0.854955	val: 0.945055	test: 0.740591
PRC train: 0.952202	val: 0.992767	test: 0.628077

Epoch: 4
Loss: 0.3984414776429881
ROC train: 0.873467	val: 0.945055	test: 0.750224
PRC train: 0.960059	val: 0.992298	test: 0.620762

Epoch: 5
Loss: 0.34457204594227925
ROC train: 0.896098	val: 0.949051	test: 0.762276
PRC train: 0.968639	val: 0.992801	test: 0.657402

Epoch: 6
Loss: 0.3185411157585882
ROC train: 0.910080	val: 0.954934	test: 0.767966
PRC train: 0.973176	val: 0.993871	test: 0.669897

Epoch: 7
Loss: 0.29530328134943273
ROC train: 0.915741	val: 0.948274	test: 0.771640
PRC train: 0.974553	val: 0.992345	test: 0.651072

Epoch: 8
Loss: 0.27894228576112706
ROC train: 0.928596	val: 0.944056	test: 0.770654
PRC train: 0.979000	val: 0.991847	test: 0.665575

Epoch: 9
Loss: 0.2667907230879971
ROC train: 0.931232	val: 0.948052	test: 0.778987
PRC train: 0.979314	val: 0.992747	test: 0.702509

Epoch: 10
Loss: 0.2668971398896479
ROC train: 0.938560	val: 0.960040	test: 0.785036
PRC train: 0.981998	val: 0.994782	test: 0.714169

Epoch: 11
Loss: 0.26128059515852703
ROC train: 0.932607	val: 0.962815	test: 0.783692
PRC train: 0.979858	val: 0.995323	test: 0.708534

Epoch: 12
Loss: 0.25461278755031164
ROC train: 0.946500	val: 0.962038	test: 0.778763
PRC train: 0.984187	val: 0.995173	test: 0.703028

Epoch: 13
Loss: 0.23947939628164996
ROC train: 0.944104	val: 0.960595	test: 0.789023
PRC train: 0.982676	val: 0.995062	test: 0.727511

Epoch: 14
Loss: 0.23508737126343102
ROC train: 0.951343	val: 0.969919	test: 0.792159
PRC train: 0.984693	val: 0.996290	test: 0.735043

Epoch: 15
Loss: 0.22559392460854352
ROC train: 0.954360	val: 0.964924	test: 0.794668
PRC train: 0.986488	val: 0.995372	test: 0.740998

Epoch: 16
Loss: 0.22070333083252738
ROC train: 0.955862	val: 0.965923	test: 0.792608
PRC train: 0.986911	val: 0.995494	test: 0.736490

Epoch: 17
Loss: 0.2227157732123107
ROC train: 0.956184	val: 0.963592	test: 0.794489
PRC train: 0.987092	val: 0.995218	test: 0.738340

Epoch: 18
Loss: 0.21410097598074881
ROC train: 0.958040	val: 0.961927	test: 0.779615
PRC train: 0.988526	val: 0.994944	test: 0.712101

Epoch: 19
Loss: 0.21424467370626887
ROC train: 0.962710	val: 0.960706	test: 0.785932
PRC train: 0.989546	val: 0.994650	test: 0.717260

Epoch: 20
Loss: 0.21061397432623655
ROC train: 0.964467	val: 0.951493	test: 0.785484
PRC train: 0.989730	val: 0.993036	test: 0.723159

Epoch: 21
Loss: 0.20067440872217632
ROC train: 0.967234	val: 0.950161	test: 0.778181
PRC train: 0.990387	val: 0.992825	test: 0.715278

Epoch: 22
Loss: 0.2083108066073807
ROC train: 0.962097	val: 0.941503	test: 0.774686
PRC train: 0.989331	val: 0.991403	test: 0.688324

Epoch: 23
Loss: 0.19214426205454804
ROC train: 0.970879	val: 0.940615	test: 0.773297
PRC train: 0.991960	val: 0.990803	test: 0.689426

Epoch: 24
Loss: 0.19452126744256623
ROC train: 0.966195	val: 0.946054	test: 0.783781
PRC train: 0.990344	val: 0.991718	test: 0.696549

Epoch: 25
Loss: 0.20010657330677772
ROC train: 0.967697	val: 0.949495	test: 0.782841
PRC train: 0.990608	val: 0.993021	test: 0.712565

Epoch: 26
Loss: 0.18240464331172968
ROC train: 0.964178	val: 0.960262	test: 0.784588
PRC train: 0.989668	val: 0.994768	test: 0.689443

Epoch: 27
Loss: 0.16839943819541217
ROC train: 0.973703	val: 0.953602	test: 0.790188
PRC train: 0.992902	val: 0.993039	test: 0.720253

Epoch: 28
Loss: 0.191382187537791
ROC train: 0.974082	val: 0.948940	test: 0.778405
PRC train: 0.993385	val: 0.992173	test: 0.693921

Epoch: 29
Loss: 0.18893904067466702
ROC train: 0.976758	val: 0.950716	test: 0.787545
PRC train: 0.993938	val: 0.993069	test: 0.712906

Epoch: 30
Loss: 0.18425101043158
ROC train: 0.977983	val: 0.953158	test: 0.786514
PRC train: 0.994411	val: 0.993695	test: 0.717224

Epoch: 31
Loss: 0.18295299240956475
ROC train: 0.978474	val: 0.951382	test: 0.785439
PRC train: 0.994671	val: 0.993054	test: 0.720751

Epoch: 32
Loss: 0.17691756343931075
ROC train: 0.981236	val: 0.941614	test: 0.780914
PRC train: 0.995410	val: 0.990540	test: 0.692073

Epoch: 33
Loss: 0.1579804039725398Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/bbbp/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/split/GraphMVP/bbbp/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/bbbp/scaff/train_prop=0.8/bbbp_scaff_6_26-05_11-11-03  ]
[ Using Seed :  6  ]
[ Using device :  cuda:2  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6474915523212582
ROC train: 0.789866	val: 0.897822	test: 0.591242
PRC train: 0.942372	val: 0.854509	test: 0.604325

Epoch: 2
Loss: 0.5315642506115078
ROC train: 0.833256	val: 0.918197	test: 0.629051
PRC train: 0.953612	val: 0.886121	test: 0.671992

Epoch: 3
Loss: 0.44079453394662044
ROC train: 0.860855	val: 0.913580	test: 0.643036
PRC train: 0.963000	val: 0.861152	test: 0.668196

Epoch: 4
Loss: 0.3952145590066685
ROC train: 0.881133	val: 0.898525	test: 0.654610
PRC train: 0.967622	val: 0.824866	test: 0.674377

Epoch: 5
Loss: 0.3373989110799829
ROC train: 0.905082	val: 0.894209	test: 0.672550
PRC train: 0.976133	val: 0.804546	test: 0.718222

Epoch: 6
Loss: 0.3070232936192277
ROC train: 0.921199	val: 0.894710	test: 0.689622
PRC train: 0.980002	val: 0.801919	test: 0.743025

Epoch: 7
Loss: 0.2881369398748032
ROC train: 0.924634	val: 0.900733	test: 0.692901
PRC train: 0.980744	val: 0.814424	test: 0.748801

Epoch: 8
Loss: 0.2668897926943012
ROC train: 0.930353	val: 0.911573	test: 0.695120
PRC train: 0.982374	val: 0.839191	test: 0.752316

Epoch: 9
Loss: 0.26820105191579213
ROC train: 0.928601	val: 0.919301	test: 0.689525
PRC train: 0.981380	val: 0.856897	test: 0.733501

Epoch: 10
Loss: 0.24927320041054118
ROC train: 0.936541	val: 0.920104	test: 0.693769
PRC train: 0.983776	val: 0.865844	test: 0.755118

Epoch: 11
Loss: 0.24191813408496715
ROC train: 0.936193	val: 0.914684	test: 0.696952
PRC train: 0.983669	val: 0.872237	test: 0.758324

Epoch: 12
Loss: 0.2300204069592943
ROC train: 0.944009	val: 0.907859	test: 0.695505
PRC train: 0.985727	val: 0.845345	test: 0.762451

Epoch: 13
Loss: 0.23559066956072883
ROC train: 0.943256	val: 0.916190	test: 0.701871
PRC train: 0.985049	val: 0.860070	test: 0.757350

Epoch: 14
Loss: 0.2309896382927892
ROC train: 0.946406	val: 0.930041	test: 0.685185
PRC train: 0.986433	val: 0.901023	test: 0.737803

Epoch: 15
Loss: 0.2284626826211778
ROC train: 0.954780	val: 0.925926	test: 0.706115
PRC train: 0.987995	val: 0.890340	test: 0.767198

Epoch: 16
Loss: 0.2102969976507532
ROC train: 0.954371	val: 0.931848	test: 0.707272
PRC train: 0.987921	val: 0.903480	test: 0.765417

Epoch: 17
Loss: 0.19573372121738375
ROC train: 0.946656	val: 0.933153	test: 0.680845
PRC train: 0.986696	val: 0.899483	test: 0.725733

Epoch: 18
Loss: 0.19967145166491768
ROC train: 0.958032	val: 0.920707	test: 0.700328
PRC train: 0.989113	val: 0.892237	test: 0.760766

Epoch: 19
Loss: 0.19719261025592752
ROC train: 0.962482	val: 0.928335	test: 0.708140
PRC train: 0.990749	val: 0.909962	test: 0.767473

Epoch: 20
Loss: 0.1940909439143274
ROC train: 0.964861	val: 0.904145	test: 0.702836
PRC train: 0.992252	val: 0.853998	test: 0.753932

Epoch: 21
Loss: 0.18359858340607746
ROC train: 0.959838	val: 0.929941	test: 0.703800
PRC train: 0.989818	val: 0.904498	test: 0.760150

Epoch: 22
Loss: 0.1965898310251694
ROC train: 0.966556	val: 0.921510	test: 0.712770
PRC train: 0.991917	val: 0.888042	test: 0.762797

Epoch: 23
Loss: 0.1870897690016298
ROC train: 0.967376	val: 0.911472	test: 0.706887
PRC train: 0.992481	val: 0.867026	test: 0.745050

Epoch: 24
Loss: 0.18481304214574162
ROC train: 0.969114	val: 0.923417	test: 0.698881
PRC train: 0.992495	val: 0.886870	test: 0.746806

Epoch: 25
Loss: 0.18851103823322884
ROC train: 0.966180	val: 0.927231	test: 0.705826
PRC train: 0.991391	val: 0.886574	test: 0.750018

Epoch: 26
Loss: 0.17927723803203946
ROC train: 0.969161	val: 0.921710	test: 0.709780
PRC train: 0.992710	val: 0.887388	test: 0.754548

Epoch: 27
Loss: 0.18045749389044172
ROC train: 0.972538	val: 0.932149	test: 0.711034
PRC train: 0.993852	val: 0.908115	test: 0.762348

Epoch: 28
Loss: 0.17817406589617532
ROC train: 0.964415	val: 0.928536	test: 0.703125
PRC train: 0.991600	val: 0.903197	test: 0.753427

Epoch: 29
Loss: 0.1782696082199757
ROC train: 0.971352	val: 0.917093	test: 0.694059
PRC train: 0.993673	val: 0.874466	test: 0.738310

Epoch: 30
Loss: 0.17747172422921423
ROC train: 0.975182	val: 0.922011	test: 0.714603
PRC train: 0.994371	val: 0.894600	test: 0.766832

Epoch: 31
Loss: 0.17121528357482957
ROC train: 0.974433	val: 0.922112	test: 0.692419
PRC train: 0.994230	val: 0.888637	test: 0.731334

Epoch: 32
Loss: 0.160892943893255
ROC train: 0.973611	val: 0.925926	test: 0.707755
PRC train: 0.993804	val: 0.898293	test: 0.737828

Epoch: 33
Loss: 0.16150852978259014Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/bbbp/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/split/GraphMVP/bbbp/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/bbbp/scaff/train_prop=0.8/bbbp_scaff_5_26-05_11-11-03  ]
[ Using Seed :  5  ]
[ Using device :  cuda:2  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.641714201112223
ROC train: 0.801519	val: 0.908762	test: 0.595197
PRC train: 0.940005	val: 0.819310	test: 0.637559

Epoch: 2
Loss: 0.5235877527872395
ROC train: 0.840539	val: 0.914484	test: 0.600694
PRC train: 0.950648	val: 0.844476	test: 0.640048

Epoch: 3
Loss: 0.436297747681032
ROC train: 0.862070	val: 0.904748	test: 0.634645
PRC train: 0.961891	val: 0.830067	test: 0.665130

Epoch: 4
Loss: 0.357607219845214
ROC train: 0.877693	val: 0.903744	test: 0.637828
PRC train: 0.966894	val: 0.810578	test: 0.653939

Epoch: 5
Loss: 0.3234306751111152
ROC train: 0.897782	val: 0.898625	test: 0.671682
PRC train: 0.974227	val: 0.800828	test: 0.722135

Epoch: 6
Loss: 0.3079319795247496
ROC train: 0.913812	val: 0.909365	test: 0.682774
PRC train: 0.978155	val: 0.806511	test: 0.731089

Epoch: 7
Loss: 0.28795504380004416
ROC train: 0.921579	val: 0.910770	test: 0.681906
PRC train: 0.979800	val: 0.810392	test: 0.726997

Epoch: 8
Loss: 0.25647278400525614
ROC train: 0.924857	val: 0.912075	test: 0.695795
PRC train: 0.980806	val: 0.823392	test: 0.743172

Epoch: 9
Loss: 0.2510003525765752
ROC train: 0.935509	val: 0.932350	test: 0.694927
PRC train: 0.983784	val: 0.894432	test: 0.747821

Epoch: 10
Loss: 0.2568597687400365
ROC train: 0.935893	val: 0.924119	test: 0.694155
PRC train: 0.983674	val: 0.882279	test: 0.749197

Epoch: 11
Loss: 0.23926440742795854
ROC train: 0.947568	val: 0.930643	test: 0.699846
PRC train: 0.987074	val: 0.903155	test: 0.764562

Epoch: 12
Loss: 0.22699698924750722
ROC train: 0.949422	val: 0.928536	test: 0.707948
PRC train: 0.987020	val: 0.903750	test: 0.773537

Epoch: 13
Loss: 0.21621190367471188
ROC train: 0.950957	val: 0.932751	test: 0.706404
PRC train: 0.987341	val: 0.914179	test: 0.763451

Epoch: 14
Loss: 0.21248168579191937
ROC train: 0.956653	val: 0.921911	test: 0.717207
PRC train: 0.988784	val: 0.886187	test: 0.772752

Epoch: 15
Loss: 0.21476433111337828
ROC train: 0.955437	val: 0.920606	test: 0.705729
PRC train: 0.988604	val: 0.869486	test: 0.756813

Epoch: 16
Loss: 0.2177708276972741
ROC train: 0.958556	val: 0.916391	test: 0.701871
PRC train: 0.989274	val: 0.881164	test: 0.746233

Epoch: 17
Loss: 0.2054668027491149
ROC train: 0.954404	val: 0.935060	test: 0.694252
PRC train: 0.988058	val: 0.907608	test: 0.730686

Epoch: 18
Loss: 0.1862008631741691
ROC train: 0.962993	val: 0.933052	test: 0.713542
PRC train: 0.990230	val: 0.903720	test: 0.764366

Epoch: 19
Loss: 0.19556809703243655
ROC train: 0.967883	val: 0.927833	test: 0.717593
PRC train: 0.991693	val: 0.894704	test: 0.769562

Epoch: 20
Loss: 0.18199564687660166
ROC train: 0.968231	val: 0.933956	test: 0.707562
PRC train: 0.992198	val: 0.899291	test: 0.749759

Epoch: 21
Loss: 0.19140531630018778
ROC train: 0.970784	val: 0.925223	test: 0.709394
PRC train: 0.993042	val: 0.888226	test: 0.746983

Epoch: 22
Loss: 0.18406784395497539
ROC train: 0.968978	val: 0.926528	test: 0.679012
PRC train: 0.992759	val: 0.886075	test: 0.704209

Epoch: 23
Loss: 0.18891904279387922
ROC train: 0.968385	val: 0.899528	test: 0.687404
PRC train: 0.992531	val: 0.818937	test: 0.721230

Epoch: 24
Loss: 0.18501396252127789
ROC train: 0.969765	val: 0.928034	test: 0.699267
PRC train: 0.992845	val: 0.888991	test: 0.712701

Epoch: 25
Loss: 0.17897791994355974
ROC train: 0.973094	val: 0.914082	test: 0.705536
PRC train: 0.994022	val: 0.851881	test: 0.724475

Epoch: 26
Loss: 0.17199373520283207
ROC train: 0.974965	val: 0.909365	test: 0.714603
PRC train: 0.994606	val: 0.839672	test: 0.739421

Epoch: 27
Loss: 0.1759380876419657
ROC train: 0.975466	val: 0.930342	test: 0.704668
PRC train: 0.994318	val: 0.897494	test: 0.732525

Epoch: 28
Loss: 0.1704161917075046
ROC train: 0.976592	val: 0.932550	test: 0.707176
PRC train: 0.994073	val: 0.899840	test: 0.751172

Epoch: 29
Loss: 0.15821704612701729
ROC train: 0.977247	val: 0.931547	test: 0.703993
PRC train: 0.994468	val: 0.891780	test: 0.741437

Epoch: 30
Loss: 0.1688557518845888
ROC train: 0.976296	val: 0.927632	test: 0.691647
PRC train: 0.994412	val: 0.877793	test: 0.699079

Epoch: 31
Loss: 0.16358914047149972
ROC train: 0.978618	val: 0.927231	test: 0.709394
PRC train: 0.995091	val: 0.882906	test: 0.732778

Epoch: 32
Loss: 0.15421448800845997
ROC train: 0.981969	val: 0.935863	test: 0.711709
PRC train: 0.995899	val: 0.894223	test: 0.737547

Epoch: 33
Loss: 0.1528253069935049Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/bbbp/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/split/GraphMVP/bbbp/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/bbbp/scaff/train_prop=0.8/bbbp_scaff_4_26-05_11-11-03  ]
[ Using Seed :  4  ]
[ Using device :  cuda:2  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.615215851416589
ROC train: 0.817281	val: 0.905350	test: 0.617863
PRC train: 0.952675	val: 0.855937	test: 0.666300

Epoch: 2
Loss: 0.5008982831133124
ROC train: 0.854207	val: 0.911071	test: 0.626543
PRC train: 0.960879	val: 0.854452	test: 0.650063

Epoch: 3
Loss: 0.4143169900328392
ROC train: 0.880985	val: 0.909666	test: 0.652296
PRC train: 0.970455	val: 0.844319	test: 0.679376

Epoch: 4
Loss: 0.36146664741219814
ROC train: 0.903641	val: 0.900833	test: 0.654514
PRC train: 0.975728	val: 0.802248	test: 0.681978

Epoch: 5
Loss: 0.3170079961378229
ROC train: 0.910031	val: 0.920807	test: 0.641879
PRC train: 0.977171	val: 0.846998	test: 0.667802

Epoch: 6
Loss: 0.2872161291783974
ROC train: 0.926422	val: 0.917997	test: 0.685475
PRC train: 0.982384	val: 0.838614	test: 0.735153

Epoch: 7
Loss: 0.28455774359349595
ROC train: 0.922687	val: 0.915989	test: 0.674576
PRC train: 0.980674	val: 0.833490	test: 0.718282

Epoch: 8
Loss: 0.26231592297912926
ROC train: 0.933162	val: 0.917896	test: 0.670814
PRC train: 0.983340	val: 0.847619	test: 0.712956

Epoch: 9
Loss: 0.2452691989725287
ROC train: 0.933064	val: 0.928837	test: 0.662905
PRC train: 0.982965	val: 0.874078	test: 0.698744

Epoch: 10
Loss: 0.22411751315251668
ROC train: 0.941432	val: 0.934759	test: 0.667149
PRC train: 0.986094	val: 0.896115	test: 0.723563

Epoch: 11
Loss: 0.2333128441566499
ROC train: 0.947572	val: 0.935863	test: 0.678627
PRC train: 0.987331	val: 0.903125	test: 0.736549

Epoch: 12
Loss: 0.22050646535600477
ROC train: 0.948501	val: 0.933454	test: 0.658854
PRC train: 0.988050	val: 0.895586	test: 0.709250

Epoch: 13
Loss: 0.2106242205496087
ROC train: 0.955253	val: 0.938272	test: 0.686921
PRC train: 0.988994	val: 0.894399	test: 0.733856

Epoch: 14
Loss: 0.21402084022087578
ROC train: 0.958406	val: 0.932049	test: 0.705440
PRC train: 0.989291	val: 0.868809	test: 0.752016

Epoch: 15
Loss: 0.19793423928942921
ROC train: 0.956621	val: 0.928234	test: 0.687693
PRC train: 0.989705	val: 0.873961	test: 0.716990

Epoch: 16
Loss: 0.21976192928927726
ROC train: 0.961861	val: 0.933554	test: 0.690683
PRC train: 0.991207	val: 0.904315	test: 0.731176

Epoch: 17
Loss: 0.2013085681413254
ROC train: 0.963290	val: 0.926428	test: 0.688368
PRC train: 0.991117	val: 0.896274	test: 0.741344

Epoch: 18
Loss: 0.20586241596288404
ROC train: 0.966536	val: 0.926829	test: 0.683642
PRC train: 0.991805	val: 0.882817	test: 0.731577

Epoch: 19
Loss: 0.18997868641414112
ROC train: 0.960659	val: 0.919000	test: 0.678627
PRC train: 0.990647	val: 0.862173	test: 0.714401

Epoch: 20
Loss: 0.1960199959166994
ROC train: 0.967246	val: 0.933755	test: 0.680073
PRC train: 0.991868	val: 0.908656	test: 0.707876

Epoch: 21
Loss: 0.18516846707394066
ROC train: 0.968538	val: 0.931145	test: 0.696566
PRC train: 0.992236	val: 0.895923	test: 0.725041

Epoch: 22
Loss: 0.18158074686361966
ROC train: 0.970416	val: 0.930844	test: 0.707176
PRC train: 0.992380	val: 0.897079	test: 0.744820

Epoch: 23
Loss: 0.17953723529734786
ROC train: 0.971541	val: 0.931246	test: 0.686535
PRC train: 0.993607	val: 0.900265	test: 0.711916

Epoch: 24
Loss: 0.18891480720521767
ROC train: 0.971556	val: 0.927030	test: 0.682002
PRC train: 0.993402	val: 0.881533	test: 0.700610

Epoch: 25
Loss: 0.1722088364043563
ROC train: 0.970168	val: 0.929740	test: 0.686343
PRC train: 0.992793	val: 0.897227	test: 0.717375

Epoch: 26
Loss: 0.17563014062687146
ROC train: 0.975028	val: 0.927833	test: 0.699942
PRC train: 0.994089	val: 0.895337	test: 0.727962

Epoch: 27
Loss: 0.15987183581416087
ROC train: 0.975447	val: 0.938773	test: 0.709394
PRC train: 0.993930	val: 0.914274	test: 0.748709

Epoch: 28
Loss: 0.17185167162229306
ROC train: 0.978400	val: 0.931747	test: 0.691840
PRC train: 0.995027	val: 0.894156	test: 0.712403

Epoch: 29
Loss: 0.16141071624389655
ROC train: 0.977561	val: 0.934457	test: 0.676505
PRC train: 0.995001	val: 0.892720	test: 0.696764

Epoch: 30
Loss: 0.17449274565738918
ROC train: 0.975624	val: 0.912978	test: 0.695312
PRC train: 0.994567	val: 0.845434	test: 0.717921

Epoch: 31
Loss: 0.16099809463790368
ROC train: 0.978801	val: 0.928937	test: 0.682099
PRC train: 0.995392	val: 0.894007	test: 0.701042

Epoch: 32
Loss: 0.16758478637342591
ROC train: 0.977982	val: 0.927331	test: 0.672357
PRC train: 0.995436	val: 0.891519	test: 0.675736

Epoch: 33
Loss: 0.16433989524392625
ROC train: 0.979172	val: 0.942280	test: 0.795520
PRC train: 0.994887	val: 0.989876	test: 0.712083

Epoch: 34
Loss: 0.16769515825542713
ROC train: 0.979214	val: 0.941614	test: 0.789875
PRC train: 0.995070	val: 0.990191	test: 0.712967

Epoch: 35
Loss: 0.1667999633308054
ROC train: 0.983030	val: 0.940615	test: 0.774821
PRC train: 0.995981	val: 0.991009	test: 0.671906

Epoch: 36
Loss: 0.16854893622800526
ROC train: 0.982765	val: 0.952270	test: 0.799194
PRC train: 0.995674	val: 0.993011	test: 0.713906

Epoch: 37
Loss: 0.1650377621705094
ROC train: 0.982116	val: 0.942946	test: 0.790233
PRC train: 0.995635	val: 0.990521	test: 0.707547

Epoch: 38
Loss: 0.15886029580104807
ROC train: 0.984353	val: 0.936397	test: 0.777061
PRC train: 0.996274	val: 0.987796	test: 0.668251

Epoch: 39
Loss: 0.16284121226459045
ROC train: 0.986365	val: 0.926296	test: 0.763217
PRC train: 0.996797	val: 0.986190	test: 0.643605

Epoch: 40
Loss: 0.16034663313735464
ROC train: 0.984164	val: 0.947830	test: 0.790412
PRC train: 0.996068	val: 0.992440	test: 0.705264

Epoch: 41
Loss: 0.1637537949495473
ROC train: 0.986453	val: 0.944278	test: 0.796102
PRC train: 0.996692	val: 0.991650	test: 0.711071

Epoch: 42
Loss: 0.14309361003304172
ROC train: 0.980972	val: 0.929848	test: 0.786246
PRC train: 0.995500	val: 0.988418	test: 0.699402

Epoch: 43
Loss: 0.15548204984861216
ROC train: 0.987639	val: 0.921190	test: 0.772760
PRC train: 0.997167	val: 0.987021	test: 0.657010

Epoch: 44
Loss: 0.15885581868036244
ROC train: 0.987890	val: 0.936397	test: 0.778674
PRC train: 0.997176	val: 0.989038	test: 0.666390

Epoch: 45
Loss: 0.14779261481337266
ROC train: 0.987448	val: 0.937729	test: 0.771461
PRC train: 0.997025	val: 0.989110	test: 0.649178

Epoch: 46
Loss: 0.1416747081486688
ROC train: 0.990483	val: 0.932401	test: 0.775762
PRC train: 0.997836	val: 0.988429	test: 0.675590

Epoch: 47
Loss: 0.1574671590897351
ROC train: 0.988724	val: 0.943612	test: 0.784050
PRC train: 0.997368	val: 0.991505	test: 0.680275

Epoch: 48
Loss: 0.13834652638697134
ROC train: 0.990843	val: 0.923299	test: 0.781228
PRC train: 0.997922	val: 0.986242	test: 0.681321

Epoch: 49
Loss: 0.13612727710582864
ROC train: 0.993028	val: 0.934177	test: 0.784767
PRC train: 0.998408	val: 0.987931	test: 0.703472

Epoch: 50
Loss: 0.12044353976921846
ROC train: 0.992891	val: 0.942724	test: 0.788665
PRC train: 0.998378	val: 0.990117	test: 0.702514

Epoch: 51
Loss: 0.1408837054559541
ROC train: 0.994186	val: 0.939727	test: 0.782437
PRC train: 0.998688	val: 0.989986	test: 0.680892

Epoch: 52
Loss: 0.13557309157886435
ROC train: 0.993842	val: 0.924409	test: 0.767384
PRC train: 0.998600	val: 0.985953	test: 0.652709

Epoch: 53
Loss: 0.13083876796237728
ROC train: 0.991556	val: 0.933622	test: 0.779659
PRC train: 0.998072	val: 0.987741	test: 0.670248

Epoch: 54
Loss: 0.13412988600560108
ROC train: 0.993305	val: 0.939394	test: 0.779346
PRC train: 0.998485	val: 0.988569	test: 0.671748

Epoch: 55
Loss: 0.12464522284022593
ROC train: 0.994290	val: 0.942169	test: 0.792518
PRC train: 0.998701	val: 0.991073	test: 0.687812

Epoch: 56
Loss: 0.13786464634774817
ROC train: 0.995322	val: 0.922411	test: 0.778450
PRC train: 0.998960	val: 0.987117	test: 0.666260

Epoch: 57
Loss: 0.13033108704338048
ROC train: 0.994318	val: 0.915862	test: 0.780914
PRC train: 0.998733	val: 0.985029	test: 0.674295

Epoch: 58
Loss: 0.12379050940930791
ROC train: 0.994806	val: 0.924853	test: 0.786201
PRC train: 0.998792	val: 0.986393	test: 0.690276

Epoch: 59
Loss: 0.12633395966105518
ROC train: 0.993990	val: 0.931846	test: 0.775896
PRC train: 0.998591	val: 0.986215	test: 0.683161

Epoch: 60
Loss: 0.12071005086600578
ROC train: 0.994384	val: 0.922300	test: 0.758020
PRC train: 0.998704	val: 0.984983	test: 0.629920

Epoch: 61
Loss: 0.12624757085860014
ROC train: 0.996488	val: 0.930292	test: 0.772222
PRC train: 0.999219	val: 0.988302	test: 0.666059

Epoch: 62
Loss: 0.10270600502798771
ROC train: 0.996403	val: 0.939838	test: 0.780869
PRC train: 0.999201	val: 0.990231	test: 0.686674

Epoch: 63
Loss: 0.12318375584796148
ROC train: 0.995903	val: 0.937396	test: 0.776299
PRC train: 0.999084	val: 0.989821	test: 0.674958

Epoch: 64
Loss: 0.10846549958841036
ROC train: 0.994929	val: 0.929182	test: 0.763172
PRC train: 0.998865	val: 0.988305	test: 0.652574

Epoch: 65
Loss: 0.1203053980520345
ROC train: 0.994109	val: 0.921412	test: 0.769310
PRC train: 0.998685	val: 0.987088	test: 0.655761

Epoch: 66
Loss: 0.10253922756096025
ROC train: 0.996551	val: 0.939616	test: 0.781810
PRC train: 0.999235	val: 0.989712	test: 0.678477

Epoch: 67
Loss: 0.11770699358809132
ROC train: 0.996790	val: 0.922633	test: 0.768996
PRC train: 0.999289	val: 0.986910	test: 0.655327

Epoch: 68
Loss: 0.1140063814593036
ROC train: 0.995770	val: 0.928627	test: 0.773790
PRC train: 0.999036	val: 0.989118	test: 0.654185

Epoch: 69
Loss: 0.10211699035552364
ROC train: 0.995191	val: 0.943834	test: 0.786290
PRC train: 0.998893	val: 0.991095	test: 0.675062

Epoch: 70
Loss: 0.11324473031174788
ROC train: 0.996530	val: 0.935398	test: 0.781676
PRC train: 0.999219	val: 0.989606	test: 0.676335

Epoch: 71
Loss: 0.1142888788455847
ROC train: 0.997148	val: 0.922744	test: 0.768772
PRC train: 0.999360	val: 0.988043	test: 0.660601

Epoch: 72
Loss: 0.11168263424069509
ROC train: 0.995875	val: 0.929293	test: 0.786828
PRC train: 0.999067	val: 0.988875	test: 0.700577

Epoch: 73
Loss: 0.10279915454816951
ROC train: 0.997101	val: 0.925963	test: 0.777285
PRC train: 0.999349	val: 0.986932	test: 0.667071

Epoch: 74
Loss: 0.09955415947103528
ROC train: 0.997586	val: 0.912532	test: 0.751747
PRC train: 0.999466	val: 0.984323	test: 0.619323

Epoch: 75
Loss: 0.10265943510808535
ROC train: 0.997471	val: 0.931735	test: 0.776030
PRC train: 0.999434	val: 0.987654	test: 0.660359

Epoch: 76
Loss: 0.10137195277300863
ROC train: 0.997818	val: 0.928738	test: 0.775179
PRC train: 0.999514	val: 0.988141	test: 0.656489

Epoch: 77
Loss: 0.10091018705283579
ROC train: 0.996714	val: 0.922744	test: 0.766443
PRC train: 0.999270	val: 0.987764	test: 0.632360

Epoch: 78
Loss: 0.09901248101515181
ROC train: 0.996610	val: 0.938173	test: 0.774418
PRC train: 0.999243	val: 0.990450	test: 0.656179

Epoch: 79
Loss: 0.10177611552184666
ROC train: 0.995633	val: 0.930958	test: 0.767159
PRC train: 0.999015	val: 0.987454	test: 0.634652

Epoch: 80
Loss: 0.0982288532246927
ROC train: 0.997550	val: 0.933733	test: 0.773073
PRC train: 0.999456	val: 0.987623	test: 0.654746

Epoch: 81
Loss: 0.0859715757756317
ROC train: 0.998370	val: 0.931291	test: 0.771685
PRC train: 0.999637	val: 0.987384	test: 0.658488

Epoch: 82
Loss: 0.08535330947569758
ROC train: 0.998285	val: 0.935176	test: 0.774552
PRC train: 0.999619	val: 0.989485	test: 0.674603

Epoch: 83
Loss: 0.1079926962680079
ROC train: 0.998435	val: 0.928516	test: 0.769892
PRC train: 0.999650	val: 0.988626	test: 0.649262

Epoch: 84
Loss: 0.10365376026937993
ROC train: 0.997676	val: 0.917083	test: 0.748029
PRC train: 0.999477	val: 0.988140	test: 0.607717

Epoch: 85
Loss: 0.09314682705755672
ROC train: 0.997648	val: 0.932290	test: 0.765412
PRC train: 0.999467	val: 0.990190	test: 0.646851

Epoch: 86
Loss: 0.09291575746235396
ROC train: 0.996205	val: 0.933844	test: 0.774059
PRC train: 0.999152	val: 0.990212	test: 0.665489

Epoch: 87
Loss: 0.10606319846928047
ROC train: 0.997835	val: 0.926962	test: 0.771057
PRC train: 0.999519	val: 0.987102	test: 0.658837

Epoch: 88
Loss: 0.11290736109246768
ROC train: 0.996993	val: 0.918970	test: 0.770027
PRC train: 0.999333	val: 0.986806	test: 0.669826

Epoch: 89
Loss: 0.10599029638460551
ROC train: 0.994320	val: 0.886336	test: 0.734588
PRC train: 0.998704	val: 0.982391	test: 0.607759

Epoch: 90
Loss: 0.10107236903838525
ROC train: 0.998392	val: 0.912310	test: 0.750045
PRC train: 0.999640	val: 0.986072	test: 0.618673

Epoch: 91
Loss: 0.0954963787110888
ROC train: 0.997924	val: 0.932401	test: 0.761649
PRC train: 0.999541	val: 0.989942	test: 0.646666

Epoch: 92
Loss: 0.09038224744578051
ROC train: 0.997675	val: 0.942058	test: 0.762724
PRC train: 0.999477	val: 0.991972	test: 0.649063

Epoch: 93
Loss: 0.08809895344176544
ROC train: 0.998728	val: 0.934177	test: 0.767070
PRC train: 0.999715	val: 0.990296	test: 0.647163

Epoch: 94
Loss: 0.0945910652006413
ROC train: 0.980835	val: 0.951826	test: 0.780556
PRC train: 0.995217	val: 0.992295	test: 0.666323

Epoch: 34
Loss: 0.16328132846852347
ROC train: 0.985073	val: 0.934288	test: 0.765457
PRC train: 0.996509	val: 0.985209	test: 0.661768

Epoch: 35
Loss: 0.1673068562428095
ROC train: 0.982687	val: 0.950494	test: 0.774328
PRC train: 0.995853	val: 0.989991	test: 0.659036

Epoch: 36
Loss: 0.17400295734464702
ROC train: 0.978751	val: 0.942058	test: 0.784364
PRC train: 0.994528	val: 0.985763	test: 0.698790

Epoch: 37
Loss: 0.1666376548333407
ROC train: 0.984912	val: 0.944944	test: 0.780242
PRC train: 0.996342	val: 0.988198	test: 0.692667

Epoch: 38
Loss: 0.1614482667445433
ROC train: 0.982074	val: 0.946609	test: 0.774194
PRC train: 0.995530	val: 0.989439	test: 0.687473

Epoch: 39
Loss: 0.1542951548238963
ROC train: 0.985451	val: 0.942169	test: 0.761649
PRC train: 0.996520	val: 0.988616	test: 0.646507

Epoch: 40
Loss: 0.16261918679085086
ROC train: 0.984388	val: 0.937618	test: 0.766667
PRC train: 0.996306	val: 0.984700	test: 0.634305

Epoch: 41
Loss: 0.16750477084342466
ROC train: 0.986634	val: 0.941947	test: 0.776613
PRC train: 0.996804	val: 0.989703	test: 0.690443

Epoch: 42
Loss: 0.16422249061325947
ROC train: 0.987582	val: 0.945388	test: 0.780332
PRC train: 0.997123	val: 0.989492	test: 0.687099

Epoch: 43
Loss: 0.15120571584121367
ROC train: 0.988994	val: 0.944611	test: 0.794086
PRC train: 0.997401	val: 0.989657	test: 0.702134

Epoch: 44
Loss: 0.13985767781912037
ROC train: 0.990663	val: 0.942391	test: 0.769355
PRC train: 0.997868	val: 0.986002	test: 0.646457

Epoch: 45
Loss: 0.14606316806302255
ROC train: 0.982590	val: 0.932512	test: 0.776254
PRC train: 0.995935	val: 0.983333	test: 0.687222

Epoch: 46
Loss: 0.14819712275023744
ROC train: 0.990627	val: 0.937507	test: 0.790636
PRC train: 0.997867	val: 0.988290	test: 0.683511

Epoch: 47
Loss: 0.1520978869853881
ROC train: 0.990390	val: 0.941503	test: 0.778808
PRC train: 0.997748	val: 0.990110	test: 0.663178

Epoch: 48
Loss: 0.1434991599020511
ROC train: 0.991339	val: 0.941614	test: 0.782841
PRC train: 0.998008	val: 0.985548	test: 0.671618

Epoch: 49
Loss: 0.14740328153609356
ROC train: 0.991332	val: 0.935731	test: 0.792294
PRC train: 0.998002	val: 0.984092	test: 0.707039

Epoch: 50
Loss: 0.1528742468747736
ROC train: 0.989473	val: 0.948163	test: 0.792473
PRC train: 0.997521	val: 0.991116	test: 0.700387

Epoch: 51
Loss: 0.1338785171522723
ROC train: 0.985713	val: 0.953824	test: 0.783513
PRC train: 0.996492	val: 0.992374	test: 0.671717

Epoch: 52
Loss: 0.13509841189606012
ROC train: 0.990717	val: 0.951826	test: 0.779749
PRC train: 0.997808	val: 0.991927	test: 0.670395

Epoch: 53
Loss: 0.1180312021471903
ROC train: 0.994341	val: 0.941170	test: 0.776613
PRC train: 0.998733	val: 0.988354	test: 0.666547

Epoch: 54
Loss: 0.13508065610410788
ROC train: 0.993201	val: 0.936175	test: 0.776971
PRC train: 0.998471	val: 0.986257	test: 0.660033

Epoch: 55
Loss: 0.13536906854005706
ROC train: 0.993908	val: 0.929515	test: 0.785349
PRC train: 0.998627	val: 0.984989	test: 0.679546

Epoch: 56
Loss: 0.1251861898062024
ROC train: 0.994430	val: 0.930736	test: 0.782034
PRC train: 0.998757	val: 0.981992	test: 0.659005

Epoch: 57
Loss: 0.12739944116987653
ROC train: 0.992149	val: 0.931180	test: 0.774418
PRC train: 0.998152	val: 0.986782	test: 0.655585

Epoch: 58
Loss: 0.12904088947332812
ROC train: 0.993738	val: 0.940060	test: 0.778987
PRC train: 0.998540	val: 0.987126	test: 0.674189

Epoch: 59
Loss: 0.1514396689907368
ROC train: 0.994155	val: 0.924742	test: 0.767384
PRC train: 0.998682	val: 0.981690	test: 0.661357

Epoch: 60
Loss: 0.1225958641142338
ROC train: 0.994465	val: 0.907315	test: 0.771281
PRC train: 0.998751	val: 0.981056	test: 0.668111

Epoch: 61
Loss: 0.13060050153044198
ROC train: 0.991199	val: 0.906871	test: 0.778987
PRC train: 0.997981	val: 0.979847	test: 0.681170

Epoch: 62
Loss: 0.12437673455112637
ROC train: 0.993453	val: 0.928072	test: 0.793459
PRC train: 0.998556	val: 0.984311	test: 0.704417

Epoch: 63
Loss: 0.12656068894976447
ROC train: 0.994897	val: 0.932512	test: 0.768772
PRC train: 0.998841	val: 0.985786	test: 0.669239

Epoch: 64
Loss: 0.12387844305122088
ROC train: 0.995449	val: 0.935731	test: 0.773477
PRC train: 0.998979	val: 0.986841	test: 0.682388

Epoch: 65
Loss: 0.12867092354367785
ROC train: 0.994567	val: 0.943945	test: 0.780690
PRC train: 0.998776	val: 0.988927	test: 0.687514

Epoch: 66
Loss: 0.10358167490855018
ROC train: 0.994684	val: 0.939283	test: 0.777957
PRC train: 0.998809	val: 0.988054	test: 0.693140

Epoch: 67
Loss: 0.12931753653410397
ROC train: 0.995747	val: 0.930403	test: 0.781855
PRC train: 0.999053	val: 0.985119	test: 0.675016

Epoch: 68
Loss: 0.1230910991223752
ROC train: 0.994359	val: 0.935509	test: 0.770341
PRC train: 0.998711	val: 0.985211	test: 0.629519

Epoch: 69
Loss: 0.10876759477013308
ROC train: 0.994349	val: 0.932401	test: 0.763082
PRC train: 0.998731	val: 0.988184	test: 0.654618

Epoch: 70
Loss: 0.11128592493578032
ROC train: 0.995065	val: 0.936064	test: 0.775179
PRC train: 0.998893	val: 0.986530	test: 0.660995

Epoch: 71
Loss: 0.11908958978645938
ROC train: 0.994251	val: 0.924631	test: 0.750134
PRC train: 0.998698	val: 0.979399	test: 0.631894

Epoch: 72
Loss: 0.10878851101134564
ROC train: 0.995053	val: 0.910534	test: 0.749597
PRC train: 0.998886	val: 0.976954	test: 0.641743

Epoch: 73
Loss: 0.09852775501553511
ROC train: 0.997187	val: 0.917638	test: 0.771998
PRC train: 0.999373	val: 0.980668	test: 0.643201

Epoch: 74
Loss: 0.10296911323431186
ROC train: 0.997354	val: 0.930958	test: 0.789561
PRC train: 0.999410	val: 0.986383	test: 0.667028

Epoch: 75
Loss: 0.09634384783840853
ROC train: 0.996325	val: 0.935731	test: 0.785887
PRC train: 0.999187	val: 0.987778	test: 0.666396

Epoch: 76
Loss: 0.12489244520022347
ROC train: 0.996432	val: 0.925852	test: 0.776478
PRC train: 0.999194	val: 0.986948	test: 0.643570

Epoch: 77
Loss: 0.10463975923287827
ROC train: 0.994280	val: 0.931402	test: 0.768683
PRC train: 0.998677	val: 0.989093	test: 0.633401

Epoch: 78
Loss: 0.09826917528669755
ROC train: 0.996780	val: 0.936841	test: 0.785081
PRC train: 0.999270	val: 0.987864	test: 0.661981

Epoch: 79
Loss: 0.10273551608658758
ROC train: 0.996961	val: 0.932956	test: 0.784812
PRC train: 0.999317	val: 0.984831	test: 0.676754

Epoch: 80
Loss: 0.0955264674335163
ROC train: 0.996994	val: 0.929071	test: 0.775403
PRC train: 0.999331	val: 0.985169	test: 0.666531

Epoch: 81
Loss: 0.09029207859865584
ROC train: 0.997194	val: 0.918304	test: 0.775941
PRC train: 0.999377	val: 0.981435	test: 0.664402

Epoch: 82
Loss: 0.10342568449784072
ROC train: 0.996279	val: 0.914308	test: 0.785573
PRC train: 0.999116	val: 0.980714	test: 0.685655

Epoch: 83
Loss: 0.08769249908554905
ROC train: 0.998042	val: 0.919858	test: 0.773118
PRC train: 0.999561	val: 0.976917	test: 0.646814

Epoch: 84
Loss: 0.09234846726784907
ROC train: 0.997345	val: 0.920302	test: 0.769803
PRC train: 0.999407	val: 0.978858	test: 0.635446

Epoch: 85
Loss: 0.08975936780638738
ROC train: 0.998039	val: 0.922744	test: 0.782975
PRC train: 0.999563	val: 0.981234	test: 0.678924

Epoch: 86
Loss: 0.0886891509458173
ROC train: 0.997843	val: 0.929959	test: 0.788396
PRC train: 0.999519	val: 0.981721	test: 0.685346

Epoch: 87
Loss: 0.08371984382129298
ROC train: 0.997536	val: 0.920524	test: 0.766084
PRC train: 0.999450	val: 0.982130	test: 0.660129

Epoch: 88
Loss: 0.0970460988495307
ROC train: 0.997999	val: 0.911422	test: 0.767473
PRC train: 0.999554	val: 0.978588	test: 0.642989

Epoch: 89
Loss: 0.0875624992196663
ROC train: 0.996670	val: 0.925186	test: 0.768459
PRC train: 0.999262	val: 0.980005	test: 0.632257

Epoch: 90
Loss: 0.09778668222479692
ROC train: 0.998453	val: 0.928516	test: 0.769176
PRC train: 0.999657	val: 0.984407	test: 0.647601

Epoch: 91
Loss: 0.08978048747412155
ROC train: 0.998374	val: 0.927961	test: 0.769668
PRC train: 0.999636	val: 0.985677	test: 0.672385

Epoch: 92
Loss: 0.09171672593396686
ROC train: 0.997419	val: 0.923077	test: 0.765860
PRC train: 0.999419	val: 0.983722	test: 0.668265

Epoch: 93
Loss: 0.09163107551947346
ROC train: 0.998276	val: 0.933622	test: 0.785887
PRC train: 0.999617	val: 0.987167	test: 0.689316

Epoch: 94
Loss: 0.09404339872637747
ROC train: 0.979497	val: 0.947830	test: 0.789964
PRC train: 0.994980	val: 0.992050	test: 0.713631

Epoch: 34
Loss: 0.18167539825155798
ROC train: 0.980679	val: 0.946387	test: 0.793414
PRC train: 0.995318	val: 0.991268	test: 0.680269

Epoch: 35
Loss: 0.16835422059393282
ROC train: 0.981201	val: 0.945055	test: 0.780511
PRC train: 0.995468	val: 0.990801	test: 0.666136

Epoch: 36
Loss: 0.16390148704728574
ROC train: 0.982303	val: 0.940060	test: 0.763575
PRC train: 0.995767	val: 0.989728	test: 0.656771

Epoch: 37
Loss: 0.16868481788188283
ROC train: 0.983894	val: 0.953158	test: 0.778495
PRC train: 0.996154	val: 0.992842	test: 0.667997

Epoch: 38
Loss: 0.17318635722172207
ROC train: 0.984650	val: 0.943057	test: 0.769041
PRC train: 0.996443	val: 0.991040	test: 0.678431

Epoch: 39
Loss: 0.16273459885554403
ROC train: 0.984101	val: 0.933844	test: 0.769713
PRC train: 0.996253	val: 0.988836	test: 0.685311

Epoch: 40
Loss: 0.1554455770382411
ROC train: 0.985029	val: 0.947719	test: 0.786783
PRC train: 0.996429	val: 0.991873	test: 0.698620

Epoch: 41
Loss: 0.1600768655166062
ROC train: 0.985916	val: 0.946387	test: 0.785618
PRC train: 0.996649	val: 0.992247	test: 0.702005

Epoch: 42
Loss: 0.1541900211195484
ROC train: 0.985629	val: 0.945721	test: 0.786559
PRC train: 0.996560	val: 0.992156	test: 0.703536

Epoch: 43
Loss: 0.1523933996864053
ROC train: 0.987041	val: 0.949606	test: 0.784588
PRC train: 0.996979	val: 0.992381	test: 0.690092

Epoch: 44
Loss: 0.1361365489458057
ROC train: 0.988153	val: 0.930847	test: 0.763934
PRC train: 0.997254	val: 0.987812	test: 0.645175

Epoch: 45
Loss: 0.1585758871315812
ROC train: 0.986896	val: 0.927073	test: 0.768459
PRC train: 0.996955	val: 0.986132	test: 0.656984

Epoch: 46
Loss: 0.14689993111019073
ROC train: 0.986267	val: 0.947608	test: 0.778987
PRC train: 0.996751	val: 0.991325	test: 0.671983

Epoch: 47
Loss: 0.139697074805632
ROC train: 0.988414	val: 0.958486	test: 0.773522
PRC train: 0.997281	val: 0.994027	test: 0.650086

Epoch: 48
Loss: 0.14028349296999523
ROC train: 0.990703	val: 0.946276	test: 0.771729
PRC train: 0.997855	val: 0.991258	test: 0.652393

Epoch: 49
Loss: 0.141929822785714
ROC train: 0.988906	val: 0.941059	test: 0.778539
PRC train: 0.997410	val: 0.990577	test: 0.674532

Epoch: 50
Loss: 0.13920024399927725
ROC train: 0.988738	val: 0.937174	test: 0.785305
PRC train: 0.997350	val: 0.989468	test: 0.686350

Epoch: 51
Loss: 0.14183061255208554
ROC train: 0.989941	val: 0.939949	test: 0.785081
PRC train: 0.997647	val: 0.988846	test: 0.655380

Epoch: 52
Loss: 0.12898801526922715
ROC train: 0.989773	val: 0.944722	test: 0.787590
PRC train: 0.997644	val: 0.990033	test: 0.674916

Epoch: 53
Loss: 0.14874967731153466
ROC train: 0.992820	val: 0.933844	test: 0.772088
PRC train: 0.998385	val: 0.987473	test: 0.658223

Epoch: 54
Loss: 0.1197487499498678
ROC train: 0.990329	val: 0.937174	test: 0.772267
PRC train: 0.997837	val: 0.989177	test: 0.686101

Epoch: 55
Loss: 0.13076299022279028
ROC train: 0.989066	val: 0.938617	test: 0.781541
PRC train: 0.997487	val: 0.989298	test: 0.697324

Epoch: 56
Loss: 0.13552569408883142
ROC train: 0.991688	val: 0.923965	test: 0.760349
PRC train: 0.998090	val: 0.984916	test: 0.622922

Epoch: 57
Loss: 0.14330743328167028
ROC train: 0.989717	val: 0.928627	test: 0.763217
PRC train: 0.997595	val: 0.987569	test: 0.635351

Epoch: 58
Loss: 0.12491636245016942
ROC train: 0.992670	val: 0.949384	test: 0.778271
PRC train: 0.998307	val: 0.992339	test: 0.654204

Epoch: 59
Loss: 0.14359565856101134
ROC train: 0.992496	val: 0.931624	test: 0.774910
PRC train: 0.998300	val: 0.989475	test: 0.650595

Epoch: 60
Loss: 0.12436849438466523
ROC train: 0.993150	val: 0.945055	test: 0.779749
PRC train: 0.998442	val: 0.990886	test: 0.680689

Epoch: 61
Loss: 0.1272941546087222
ROC train: 0.994320	val: 0.931957	test: 0.772670
PRC train: 0.998718	val: 0.987474	test: 0.677392

Epoch: 62
Loss: 0.12871269738048954
ROC train: 0.994933	val: 0.933289	test: 0.774731
PRC train: 0.998852	val: 0.988166	test: 0.670955

Epoch: 63
Loss: 0.13007850605632879
ROC train: 0.993087	val: 0.944389	test: 0.778853
PRC train: 0.998397	val: 0.990968	test: 0.672727

Epoch: 64
Loss: 0.12112464748460365
ROC train: 0.994840	val: 0.939283	test: 0.770923
PRC train: 0.998835	val: 0.989313	test: 0.654563

Epoch: 65
Loss: 0.1124957705916252
ROC train: 0.995557	val: 0.938617	test: 0.769668
PRC train: 0.999008	val: 0.987969	test: 0.651952

Epoch: 66
Loss: 0.11701644469282253
ROC train: 0.995093	val: 0.939061	test: 0.774597
PRC train: 0.998891	val: 0.988470	test: 0.645830

Epoch: 67
Loss: 0.10608093606564217
ROC train: 0.993412	val: 0.932179	test: 0.769444
PRC train: 0.998484	val: 0.986842	test: 0.625320

Epoch: 68
Loss: 0.11908953318341144
ROC train: 0.995407	val: 0.932623	test: 0.765502
PRC train: 0.998975	val: 0.986479	test: 0.631767

Epoch: 69
Loss: 0.11185311545844484
ROC train: 0.995753	val: 0.937840	test: 0.775134
PRC train: 0.999037	val: 0.988037	test: 0.629712

Epoch: 70
Loss: 0.10900372402404364
ROC train: 0.995447	val: 0.935842	test: 0.780959
PRC train: 0.998989	val: 0.987610	test: 0.652730

Epoch: 71
Loss: 0.10638223580779226
ROC train: 0.995442	val: 0.922633	test: 0.774955
PRC train: 0.998991	val: 0.985484	test: 0.663162

Epoch: 72
Loss: 0.1165128707401542
ROC train: 0.989889	val: 0.940726	test: 0.777912
PRC train: 0.997656	val: 0.990548	test: 0.688967

Epoch: 73
Loss: 0.12292103413552931
ROC train: 0.996818	val: 0.941281	test: 0.776299
PRC train: 0.999289	val: 0.988024	test: 0.656191

Epoch: 74
Loss: 0.11259554161491098
ROC train: 0.994827	val: 0.937174	test: 0.769803
PRC train: 0.998834	val: 0.986562	test: 0.621872

Epoch: 75
Loss: 0.10654987216279045
ROC train: 0.996528	val: 0.936064	test: 0.772581
PRC train: 0.999220	val: 0.988558	test: 0.638953

Epoch: 76
Loss: 0.09738624855245899
ROC train: 0.996745	val: 0.932734	test: 0.774238
PRC train: 0.999273	val: 0.988068	test: 0.643395

Epoch: 77
Loss: 0.11490693676078867
ROC train: 0.996085	val: 0.930292	test: 0.763127
PRC train: 0.999125	val: 0.987005	test: 0.622096

Epoch: 78
Loss: 0.12554137804971677
ROC train: 0.994997	val: 0.922633	test: 0.766935
PRC train: 0.998879	val: 0.982283	test: 0.633927

Epoch: 79
Loss: 0.09917607858849777
ROC train: 0.995462	val: 0.923410	test: 0.778674
PRC train: 0.998974	val: 0.982496	test: 0.659937

Epoch: 80
Loss: 0.09330079291712938
ROC train: 0.994465	val: 0.924076	test: 0.768952
PRC train: 0.998756	val: 0.984997	test: 0.645631

Epoch: 81
Loss: 0.11329445580802187
ROC train: 0.995961	val: 0.919969	test: 0.772133
PRC train: 0.999096	val: 0.984222	test: 0.662168

Epoch: 82
Loss: 0.1025646320203966
ROC train: 0.996642	val: 0.926185	test: 0.772357
PRC train: 0.999242	val: 0.983506	test: 0.653270

Epoch: 83
Loss: 0.11616606947790843
ROC train: 0.995976	val: 0.928183	test: 0.760797
PRC train: 0.999094	val: 0.984177	test: 0.642832

Epoch: 84
Loss: 0.10320318876839692
ROC train: 0.996106	val: 0.926740	test: 0.761425
PRC train: 0.999135	val: 0.985119	test: 0.654182

Epoch: 85
Loss: 0.09618654744125087
ROC train: 0.997077	val: 0.934177	test: 0.761962
PRC train: 0.999347	val: 0.988059	test: 0.627124

Epoch: 86
Loss: 0.0940772898892595
ROC train: 0.996643	val: 0.941392	test: 0.765591
PRC train: 0.999244	val: 0.989598	test: 0.632324

Epoch: 87
Loss: 0.09570342777004533
ROC train: 0.997108	val: 0.943057	test: 0.772043
PRC train: 0.999347	val: 0.990526	test: 0.663355

Epoch: 88
Loss: 0.10329717106632313
ROC train: 0.997352	val: 0.929293	test: 0.768145
PRC train: 0.999395	val: 0.985906	test: 0.650983

Epoch: 89
Loss: 0.0976219226355171
ROC train: 0.997154	val: 0.918304	test: 0.760349
PRC train: 0.999357	val: 0.984072	test: 0.651894

Epoch: 90
Loss: 0.08652649498298025
ROC train: 0.997059	val: 0.925519	test: 0.770744
PRC train: 0.999341	val: 0.985338	test: 0.672665

Epoch: 91
Loss: 0.0971834171361511
ROC train: 0.996719	val: 0.902986	test: 0.751344
PRC train: 0.999264	val: 0.975420	test: 0.620943

Epoch: 92
Loss: 0.09415369326226375
ROC train: 0.997886	val: 0.906205	test: 0.759498
PRC train: 0.999529	val: 0.974246	test: 0.624770

Epoch: 93
Loss: 0.09322402817059233
ROC train: 0.997630	val: 0.923188	test: 0.767832
PRC train: 0.999475	val: 0.982697	test: 0.657127

Epoch: 94
Loss: 0.09143137174767295
ROC train: 0.976866	val: 0.922212	test: 0.706308
PRC train: 0.994893	val: 0.882490	test: 0.733865

Epoch: 34
Loss: 0.16105286258471455
ROC train: 0.978438	val: 0.929138	test: 0.707948
PRC train: 0.995528	val: 0.888084	test: 0.741901

Epoch: 35
Loss: 0.15897725809310387
ROC train: 0.977429	val: 0.909666	test: 0.709105
PRC train: 0.995199	val: 0.864183	test: 0.752464

Epoch: 36
Loss: 0.1570195663435135
ROC train: 0.979665	val: 0.912978	test: 0.700231
PRC train: 0.995679	val: 0.870477	test: 0.727541

Epoch: 37
Loss: 0.1485746484176461
ROC train: 0.982100	val: 0.925625	test: 0.695023
PRC train: 0.996336	val: 0.886899	test: 0.708701

Epoch: 38
Loss: 0.17155434480456175
ROC train: 0.981591	val: 0.915688	test: 0.707272
PRC train: 0.996224	val: 0.866601	test: 0.723279

Epoch: 39
Loss: 0.16150340149678338
ROC train: 0.969360	val: 0.916692	test: 0.662230
PRC train: 0.993376	val: 0.855707	test: 0.679060

Epoch: 40
Loss: 0.16458248500583453
ROC train: 0.980099	val: 0.917796	test: 0.700135
PRC train: 0.995528	val: 0.884024	test: 0.735972

Epoch: 41
Loss: 0.1501320367891794
ROC train: 0.981287	val: 0.915287	test: 0.692805
PRC train: 0.996029	val: 0.856568	test: 0.705852

Epoch: 42
Loss: 0.15020731125655404
ROC train: 0.984430	val: 0.902941	test: 0.685089
PRC train: 0.996770	val: 0.831127	test: 0.700461

Epoch: 43
Loss: 0.15127278015355292
ROC train: 0.982849	val: 0.889993	test: 0.688272
PRC train: 0.996416	val: 0.826396	test: 0.712236

Epoch: 44
Loss: 0.13501570545681715
ROC train: 0.984939	val: 0.924220	test: 0.698785
PRC train: 0.996787	val: 0.881956	test: 0.715520

Epoch: 45
Loss: 0.14210720340522826
ROC train: 0.983405	val: 0.900733	test: 0.683931
PRC train: 0.996516	val: 0.822015	test: 0.691540

Epoch: 46
Loss: 0.1308291361975288
ROC train: 0.987208	val: 0.910971	test: 0.706308
PRC train: 0.997448	val: 0.839273	test: 0.737781

Epoch: 47
Loss: 0.14611219149722898
ROC train: 0.981511	val: 0.918097	test: 0.695795
PRC train: 0.996161	val: 0.856793	test: 0.696405

Epoch: 48
Loss: 0.15108089276734835
ROC train: 0.985958	val: 0.912577	test: 0.698881
PRC train: 0.997171	val: 0.851603	test: 0.729548

Epoch: 49
Loss: 0.13251754176302508
ROC train: 0.987213	val: 0.908963	test: 0.692805
PRC train: 0.997452	val: 0.857275	test: 0.726197

Epoch: 50
Loss: 0.13259383662607635
ROC train: 0.987271	val: 0.915186	test: 0.703607
PRC train: 0.997383	val: 0.863108	test: 0.737680

Epoch: 51
Loss: 0.13675419618722834
ROC train: 0.986607	val: 0.925023	test: 0.708526
PRC train: 0.997241	val: 0.863940	test: 0.740548

Epoch: 52
Loss: 0.12845379623655095
ROC train: 0.989791	val: 0.910870	test: 0.698302
PRC train: 0.998005	val: 0.849461	test: 0.724436

Epoch: 53
Loss: 0.12219210693722905
ROC train: 0.988457	val: 0.905049	test: 0.702160
PRC train: 0.997700	val: 0.848289	test: 0.717619

Epoch: 54
Loss: 0.13359828469345728
ROC train: 0.988323	val: 0.915186	test: 0.712963
PRC train: 0.997642	val: 0.873568	test: 0.742947

Epoch: 55
Loss: 0.13251140475777465
ROC train: 0.991852	val: 0.909164	test: 0.712963
PRC train: 0.998398	val: 0.845449	test: 0.722181

Epoch: 56
Loss: 0.11431762727728449
ROC train: 0.991310	val: 0.902539	test: 0.690972
PRC train: 0.998276	val: 0.838095	test: 0.698638

Epoch: 57
Loss: 0.12951284402167554
ROC train: 0.989772	val: 0.904848	test: 0.700714
PRC train: 0.997959	val: 0.852557	test: 0.715382

Epoch: 58
Loss: 0.11167292475141288
ROC train: 0.992991	val: 0.908863	test: 0.687982
PRC train: 0.998622	val: 0.847074	test: 0.692279

Epoch: 59
Loss: 0.11364143170675935
ROC train: 0.992117	val: 0.915688	test: 0.694541
PRC train: 0.998459	val: 0.862285	test: 0.725597

Epoch: 60
Loss: 0.13246674292756774
ROC train: 0.993283	val: 0.905149	test: 0.701775
PRC train: 0.998687	val: 0.841624	test: 0.715995

Epoch: 61
Loss: 0.1233309613102399
ROC train: 0.993758	val: 0.907658	test: 0.698013
PRC train: 0.998794	val: 0.842491	test: 0.712911

Epoch: 62
Loss: 0.12464148655158401
ROC train: 0.992399	val: 0.912476	test: 0.691647
PRC train: 0.998508	val: 0.864901	test: 0.696234

Epoch: 63
Loss: 0.1115267203691914
ROC train: 0.993692	val: 0.915287	test: 0.707851
PRC train: 0.998779	val: 0.858113	test: 0.707298

Epoch: 64
Loss: 0.12602356143705654
ROC train: 0.993255	val: 0.913781	test: 0.709394
PRC train: 0.998694	val: 0.870064	test: 0.722149

Epoch: 65
Loss: 0.11157918442327364
ROC train: 0.993211	val: 0.914182	test: 0.693287
PRC train: 0.998684	val: 0.861510	test: 0.714038

Epoch: 66
Loss: 0.11286318870555752
ROC train: 0.994472	val: 0.902339	test: 0.695795
PRC train: 0.998930	val: 0.841280	test: 0.714529

Epoch: 67
Loss: 0.11026373002908328
ROC train: 0.990424	val: 0.889792	test: 0.673032
PRC train: 0.998102	val: 0.814990	test: 0.698099

Epoch: 68
Loss: 0.10193747002885768
ROC train: 0.993830	val: 0.898525	test: 0.695120
PRC train: 0.998811	val: 0.821915	test: 0.716839

Epoch: 69
Loss: 0.11365089163999578
ROC train: 0.994065	val: 0.894510	test: 0.683353
PRC train: 0.998862	val: 0.837023	test: 0.687086

Epoch: 70
Loss: 0.11411371512402158
ROC train: 0.993579	val: 0.893406	test: 0.665316
PRC train: 0.998765	val: 0.837925	test: 0.698955

Epoch: 71
Loss: 0.11275003073239111
ROC train: 0.994990	val: 0.906956	test: 0.700714
PRC train: 0.999032	val: 0.847608	test: 0.722688

Epoch: 72
Loss: 0.10228207202331617
ROC train: 0.994854	val: 0.910067	test: 0.703125
PRC train: 0.999001	val: 0.838263	test: 0.722189

Epoch: 73
Loss: 0.10579561013297277
ROC train: 0.995229	val: 0.902740	test: 0.707948
PRC train: 0.999067	val: 0.821222	test: 0.706682

Epoch: 74
Loss: 0.09357645990874162
ROC train: 0.995429	val: 0.910770	test: 0.681327
PRC train: 0.999123	val: 0.840439	test: 0.691041

Epoch: 75
Loss: 0.09894440159047513
ROC train: 0.996151	val: 0.908461	test: 0.696277
PRC train: 0.999261	val: 0.829876	test: 0.721386

Epoch: 76
Loss: 0.08880787631258229
ROC train: 0.997140	val: 0.905049	test: 0.702643
PRC train: 0.999449	val: 0.824996	test: 0.716892

Epoch: 77
Loss: 0.09581863706205677
ROC train: 0.996134	val: 0.900030	test: 0.683063
PRC train: 0.999257	val: 0.840554	test: 0.703726

Epoch: 78
Loss: 0.11239123265974207
ROC train: 0.996935	val: 0.895413	test: 0.687404
PRC train: 0.999408	val: 0.827471	test: 0.681183

Epoch: 79
Loss: 0.10143149979105116
ROC train: 0.996277	val: 0.914985	test: 0.696373
PRC train: 0.999283	val: 0.834882	test: 0.708112

Epoch: 80
Loss: 0.10284014687074346
ROC train: 0.995549	val: 0.914584	test: 0.689911
PRC train: 0.999147	val: 0.837621	test: 0.724341

Epoch: 81
Loss: 0.09880714460718636
ROC train: 0.996568	val: 0.896015	test: 0.673804
PRC train: 0.999340	val: 0.816710	test: 0.696488

Epoch: 82
Loss: 0.10665792325642602
ROC train: 0.996871	val: 0.895112	test: 0.670621
PRC train: 0.999399	val: 0.829336	test: 0.683218

Epoch: 83
Loss: 0.10450229339588225
ROC train: 0.997206	val: 0.901034	test: 0.666667
PRC train: 0.999468	val: 0.826647	test: 0.663208

Epoch: 84
Loss: 0.09612181406723622
ROC train: 0.996625	val: 0.898223	test: 0.689718
PRC train: 0.999345	val: 0.808376	test: 0.688189

Epoch: 85
Loss: 0.10992284610308409
ROC train: 0.994022	val: 0.899026	test: 0.692226
PRC train: 0.998851	val: 0.804778	test: 0.703167

Epoch: 86
Loss: 0.10005808593199735
ROC train: 0.997471	val: 0.900432	test: 0.673418
PRC train: 0.999518	val: 0.815065	test: 0.681315

Epoch: 87
Loss: 0.09266834779492458
ROC train: 0.997478	val: 0.887283	test: 0.669174
PRC train: 0.999518	val: 0.809644	test: 0.671942

Epoch: 88
Loss: 0.09068555843613156
ROC train: 0.995736	val: 0.904748	test: 0.694059
PRC train: 0.999189	val: 0.818890	test: 0.701232

Epoch: 89
Loss: 0.10338571190991915
ROC train: 0.997038	val: 0.916491	test: 0.717110
PRC train: 0.999432	val: 0.848182	test: 0.727531

Epoch: 90
Loss: 0.08346982799360494
ROC train: 0.996995	val: 0.896617	test: 0.694348
PRC train: 0.999425	val: 0.811293	test: 0.695697

Epoch: 91
Loss: 0.08156548302006358
ROC train: 0.996721	val: 0.906554	test: 0.682099
PRC train: 0.999378	val: 0.827976	test: 0.695791

Epoch: 92
Loss: 0.077935556092493
ROC train: 0.997976	val: 0.898625	test: 0.674769
PRC train: 0.999617	val: 0.812917	test: 0.670880

Epoch: 93
Loss: 0.08123633551576324
ROC train: 0.994574	val: 0.875539	test: 0.633681
PRC train: 0.998958	val: 0.773146	test: 0.644423

Epoch: 94
Loss: 0.09116124186952841
ROC train: 0.981745	val: 0.930543	test: 0.702836
PRC train: 0.996108	val: 0.893367	test: 0.724835

Epoch: 34
Loss: 0.14673070436589802
ROC train: 0.982430	val: 0.915287	test: 0.687500
PRC train: 0.996378	val: 0.862981	test: 0.693710

Epoch: 35
Loss: 0.15403844602705405
ROC train: 0.975672	val: 0.916893	test: 0.667052
PRC train: 0.994261	val: 0.837624	test: 0.665763

Epoch: 36
Loss: 0.15403896322002253
ROC train: 0.981688	val: 0.921911	test: 0.703800
PRC train: 0.995649	val: 0.877685	test: 0.735658

Epoch: 37
Loss: 0.14181804646864807
ROC train: 0.984833	val: 0.918900	test: 0.712770
PRC train: 0.996638	val: 0.877201	test: 0.741813

Epoch: 38
Loss: 0.1530573077951687
ROC train: 0.985987	val: 0.912978	test: 0.700328
PRC train: 0.997057	val: 0.870030	test: 0.714076

Epoch: 39
Loss: 0.15600042821443422
ROC train: 0.987666	val: 0.924922	test: 0.700521
PRC train: 0.997446	val: 0.882045	test: 0.709524

Epoch: 40
Loss: 0.1407679828986719
ROC train: 0.987907	val: 0.920907	test: 0.716049
PRC train: 0.997538	val: 0.869647	test: 0.730737

Epoch: 41
Loss: 0.1482471196909269
ROC train: 0.987920	val: 0.916090	test: 0.733121
PRC train: 0.997512	val: 0.858201	test: 0.748050

Epoch: 42
Loss: 0.14669114755650733
ROC train: 0.988549	val: 0.925223	test: 0.711613
PRC train: 0.997696	val: 0.872114	test: 0.724762

Epoch: 43
Loss: 0.15155645161880613
ROC train: 0.987380	val: 0.898424	test: 0.697724
PRC train: 0.997494	val: 0.849741	test: 0.699557

Epoch: 44
Loss: 0.13345269539881052
ROC train: 0.987015	val: 0.914885	test: 0.678530
PRC train: 0.997309	val: 0.870035	test: 0.684223

Epoch: 45
Loss: 0.13182993209480265
ROC train: 0.989455	val: 0.920807	test: 0.686150
PRC train: 0.997807	val: 0.881776	test: 0.696463

Epoch: 46
Loss: 0.13895559705584612
ROC train: 0.988864	val: 0.923116	test: 0.694637
PRC train: 0.997666	val: 0.876023	test: 0.707355

Epoch: 47
Loss: 0.1264203158234662
ROC train: 0.988811	val: 0.927733	test: 0.700424
PRC train: 0.997629	val: 0.877135	test: 0.717040

Epoch: 48
Loss: 0.14085225571170684
ROC train: 0.987689	val: 0.915487	test: 0.680748
PRC train: 0.997355	val: 0.862675	test: 0.682532

Epoch: 49
Loss: 0.12643014129874683
ROC train: 0.989902	val: 0.920104	test: 0.695505
PRC train: 0.997827	val: 0.866355	test: 0.723783

Epoch: 50
Loss: 0.1303992293922299
ROC train: 0.988898	val: 0.905751	test: 0.682870
PRC train: 0.997707	val: 0.841219	test: 0.705608

Epoch: 51
Loss: 0.1322578897712836
ROC train: 0.990939	val: 0.882164	test: 0.672840
PRC train: 0.998177	val: 0.808217	test: 0.690754

Epoch: 52
Loss: 0.11925727144648378
ROC train: 0.990671	val: 0.910368	test: 0.692901
PRC train: 0.998050	val: 0.840513	test: 0.720588

Epoch: 53
Loss: 0.1124320544372711
ROC train: 0.987927	val: 0.908160	test: 0.692708
PRC train: 0.997417	val: 0.841135	test: 0.711450

Epoch: 54
Loss: 0.11096717924945185
ROC train: 0.990993	val: 0.912677	test: 0.677276
PRC train: 0.998148	val: 0.859464	test: 0.683356

Epoch: 55
Loss: 0.12063813324051721
ROC train: 0.993789	val: 0.911673	test: 0.695505
PRC train: 0.998713	val: 0.862882	test: 0.715552

Epoch: 56
Loss: 0.1191623383417216
ROC train: 0.993121	val: 0.910368	test: 0.712288
PRC train: 0.998614	val: 0.852409	test: 0.723834

Epoch: 57
Loss: 0.10610510796925977
ROC train: 0.992706	val: 0.911372	test: 0.690201
PRC train: 0.998547	val: 0.859667	test: 0.698119

Epoch: 58
Loss: 0.10776429392317341
ROC train: 0.993883	val: 0.903342	test: 0.693962
PRC train: 0.998814	val: 0.851487	test: 0.711098

Epoch: 59
Loss: 0.10850385718503293
ROC train: 0.993796	val: 0.897220	test: 0.715567
PRC train: 0.998780	val: 0.826562	test: 0.739880

Epoch: 60
Loss: 0.1195672375439272
ROC train: 0.993670	val: 0.903041	test: 0.714699
PRC train: 0.998765	val: 0.839094	test: 0.731757

Epoch: 61
Loss: 0.11499714572230545
ROC train: 0.993733	val: 0.894409	test: 0.688657
PRC train: 0.998756	val: 0.828525	test: 0.693188

Epoch: 62
Loss: 0.12009998144726522
ROC train: 0.994032	val: 0.890294	test: 0.668306
PRC train: 0.998841	val: 0.819214	test: 0.677060

Epoch: 63
Loss: 0.11446552571358153
ROC train: 0.992194	val: 0.904446	test: 0.706694
PRC train: 0.998458	val: 0.845476	test: 0.700790

Epoch: 64
Loss: 0.11484012003861539
ROC train: 0.992361	val: 0.900833	test: 0.719618
PRC train: 0.998441	val: 0.831800	test: 0.737300

Epoch: 65
Loss: 0.10270344295167277
ROC train: 0.994127	val: 0.909264	test: 0.695312
PRC train: 0.998825	val: 0.840815	test: 0.712056

Epoch: 66
Loss: 0.11862656771406237
ROC train: 0.995622	val: 0.904748	test: 0.686053
PRC train: 0.999155	val: 0.836378	test: 0.692778

Epoch: 67
Loss: 0.11242736292318918
ROC train: 0.993005	val: 0.896517	test: 0.678337
PRC train: 0.998652	val: 0.817464	test: 0.693299

Epoch: 68
Loss: 0.11143741336718833
ROC train: 0.995831	val: 0.909064	test: 0.704572
PRC train: 0.999202	val: 0.831949	test: 0.715517

Epoch: 69
Loss: 0.09815963136623825
ROC train: 0.992954	val: 0.901736	test: 0.705633
PRC train: 0.998585	val: 0.808807	test: 0.708164

Epoch: 70
Loss: 0.11100580536353025
ROC train: 0.996363	val: 0.873432	test: 0.695216
PRC train: 0.999308	val: 0.770438	test: 0.696913

Epoch: 71
Loss: 0.10517461789257086
ROC train: 0.995199	val: 0.896015	test: 0.699460
PRC train: 0.999071	val: 0.825855	test: 0.708162

Epoch: 72
Loss: 0.10415139263055544
ROC train: 0.995948	val: 0.905751	test: 0.705536
PRC train: 0.999225	val: 0.843195	test: 0.720639

Epoch: 73
Loss: 0.09442270650992117
ROC train: 0.996171	val: 0.885878	test: 0.706115
PRC train: 0.999272	val: 0.806229	test: 0.715745

Epoch: 74
Loss: 0.0917360779527711
ROC train: 0.997112	val: 0.898023	test: 0.689525
PRC train: 0.999444	val: 0.837021	test: 0.690787

Epoch: 75
Loss: 0.08874993632749488
ROC train: 0.997196	val: 0.900231	test: 0.682774
PRC train: 0.999465	val: 0.833298	test: 0.683114

Epoch: 76
Loss: 0.08810025039624837
ROC train: 0.997718	val: 0.900432	test: 0.686343
PRC train: 0.999568	val: 0.821815	test: 0.675599

Epoch: 77
Loss: 0.08409613829714235
ROC train: 0.997182	val: 0.908361	test: 0.692901
PRC train: 0.999465	val: 0.829054	test: 0.694824

Epoch: 78
Loss: 0.10007215618266382
ROC train: 0.997286	val: 0.902138	test: 0.702932
PRC train: 0.999479	val: 0.821908	test: 0.703445

Epoch: 79
Loss: 0.0938210811394767
ROC train: 0.996960	val: 0.894911	test: 0.700810
PRC train: 0.999414	val: 0.791573	test: 0.706598

Epoch: 80
Loss: 0.09738470673695783
ROC train: 0.997186	val: 0.891699	test: 0.697917
PRC train: 0.999466	val: 0.812430	test: 0.704917

Epoch: 81
Loss: 0.09270729687841575
ROC train: 0.997900	val: 0.894911	test: 0.682099
PRC train: 0.999596	val: 0.823492	test: 0.679912

Epoch: 82
Loss: 0.10449159095390267
ROC train: 0.997965	val: 0.896919	test: 0.692612
PRC train: 0.999609	val: 0.824890	test: 0.692196

Epoch: 83
Loss: 0.0866530743590337
ROC train: 0.997698	val: 0.890696	test: 0.697917
PRC train: 0.999562	val: 0.842818	test: 0.706295

Epoch: 84
Loss: 0.09136215039266217
ROC train: 0.997774	val: 0.893707	test: 0.684028
PRC train: 0.999580	val: 0.821021	test: 0.679608

Epoch: 85
Loss: 0.09311086020558447
ROC train: 0.996332	val: 0.893907	test: 0.691551
PRC train: 0.999295	val: 0.800996	test: 0.707637

Epoch: 86
Loss: 0.08492129403063627
ROC train: 0.996722	val: 0.890294	test: 0.693094
PRC train: 0.999361	val: 0.800252	test: 0.691852

Epoch: 87
Loss: 0.08512884160103515
ROC train: 0.998029	val: 0.898223	test: 0.695505
PRC train: 0.999620	val: 0.826882	test: 0.682032

Epoch: 88
Loss: 0.09294524327279051
ROC train: 0.997401	val: 0.891900	test: 0.679977
PRC train: 0.999509	val: 0.833178	test: 0.685397

Epoch: 89
Loss: 0.08822614238501444
ROC train: 0.997618	val: 0.874034	test: 0.658179
PRC train: 0.999546	val: 0.787996	test: 0.656742

Epoch: 90
Loss: 0.08104533020077996
ROC train: 0.997747	val: 0.891800	test: 0.669560
PRC train: 0.999576	val: 0.811596	test: 0.663993

Epoch: 91
Loss: 0.09226421430421325
ROC train: 0.997864	val: 0.902439	test: 0.695891
PRC train: 0.999593	val: 0.844546	test: 0.703192

Epoch: 92
Loss: 0.07836561557971077
ROC train: 0.995597	val: 0.886982	test: 0.698013
PRC train: 0.999140	val: 0.795647	test: 0.713718

Epoch: 93
Loss: 0.10606693037168169
ROC train: 0.997406	val: 0.900130	test: 0.688850
PRC train: 0.999501	val: 0.792390	test: 0.697191

Epoch: 94
Loss: 0.08117526103300668
ROC train: 0.982677	val: 0.936465	test: 0.702450
PRC train: 0.996253	val: 0.909900	test: 0.725069

Epoch: 34
Loss: 0.1526540463872772
ROC train: 0.976612	val: 0.943491	test: 0.710262
PRC train: 0.994306	val: 0.929755	test: 0.758026

Epoch: 35
Loss: 0.14073591388378398
ROC train: 0.982650	val: 0.921911	test: 0.680941
PRC train: 0.996376	val: 0.896144	test: 0.704729

Epoch: 36
Loss: 0.15528009637098256
ROC train: 0.985938	val: 0.928134	test: 0.670621
PRC train: 0.997171	val: 0.896815	test: 0.687860

Epoch: 37
Loss: 0.14343542018563812
ROC train: 0.982459	val: 0.932450	test: 0.696373
PRC train: 0.996229	val: 0.902828	test: 0.733957

Epoch: 38
Loss: 0.13414059105128082
ROC train: 0.985857	val: 0.927733	test: 0.681520
PRC train: 0.997077	val: 0.888793	test: 0.684062

Epoch: 39
Loss: 0.14525854770538346
ROC train: 0.982900	val: 0.932149	test: 0.695795
PRC train: 0.996341	val: 0.903811	test: 0.712573

Epoch: 40
Loss: 0.14947455319940847
ROC train: 0.985104	val: 0.918398	test: 0.684124
PRC train: 0.996883	val: 0.867697	test: 0.710390

Epoch: 41
Loss: 0.1423355743634033
ROC train: 0.987164	val: 0.919402	test: 0.666763
PRC train: 0.997417	val: 0.876293	test: 0.683407

Epoch: 42
Loss: 0.1388250332363001
ROC train: 0.986668	val: 0.912275	test: 0.682485
PRC train: 0.997325	val: 0.847522	test: 0.698236

Epoch: 43
Loss: 0.15972608701715135
ROC train: 0.987593	val: 0.921610	test: 0.692033
PRC train: 0.997465	val: 0.884833	test: 0.728614

Epoch: 44
Loss: 0.15491003955214833
ROC train: 0.989030	val: 0.926428	test: 0.711902
PRC train: 0.997736	val: 0.874044	test: 0.742767

Epoch: 45
Loss: 0.14417025915967427
ROC train: 0.989023	val: 0.933253	test: 0.700135
PRC train: 0.997809	val: 0.893878	test: 0.710042

Epoch: 46
Loss: 0.12416560268223258
ROC train: 0.989373	val: 0.927933	test: 0.699074
PRC train: 0.997852	val: 0.893733	test: 0.716320

Epoch: 47
Loss: 0.12487922381088805
ROC train: 0.990132	val: 0.936866	test: 0.678337
PRC train: 0.998018	val: 0.907987	test: 0.678573

Epoch: 48
Loss: 0.12001419218265026
ROC train: 0.990302	val: 0.923918	test: 0.679109
PRC train: 0.998089	val: 0.891082	test: 0.697413

Epoch: 49
Loss: 0.13696589140299528
ROC train: 0.992273	val: 0.922814	test: 0.698495
PRC train: 0.998503	val: 0.885831	test: 0.708708

Epoch: 50
Loss: 0.12118259258064902
ROC train: 0.992452	val: 0.928837	test: 0.708526
PRC train: 0.998535	val: 0.887650	test: 0.704147

Epoch: 51
Loss: 0.13190895339183542
ROC train: 0.992891	val: 0.923818	test: 0.707658
PRC train: 0.998620	val: 0.873031	test: 0.705317

Epoch: 52
Loss: 0.12462316757109967
ROC train: 0.993295	val: 0.922112	test: 0.700039
PRC train: 0.998702	val: 0.872325	test: 0.705535

Epoch: 53
Loss: 0.11775032225136252
ROC train: 0.993212	val: 0.917695	test: 0.696952
PRC train: 0.998694	val: 0.858284	test: 0.678152

Epoch: 54
Loss: 0.12703164579128362
ROC train: 0.991940	val: 0.930643	test: 0.680170
PRC train: 0.998420	val: 0.887584	test: 0.674754

Epoch: 55
Loss: 0.12584199502723556
ROC train: 0.993221	val: 0.921710	test: 0.707079
PRC train: 0.998670	val: 0.869948	test: 0.708207

Epoch: 56
Loss: 0.12626609318952306
ROC train: 0.992936	val: 0.927030	test: 0.703125
PRC train: 0.998595	val: 0.876352	test: 0.724401

Epoch: 57
Loss: 0.1268757663358632
ROC train: 0.992810	val: 0.918599	test: 0.702160
PRC train: 0.998575	val: 0.878956	test: 0.721980

Epoch: 58
Loss: 0.11427623318351013
ROC train: 0.993978	val: 0.920506	test: 0.699363
PRC train: 0.998824	val: 0.880148	test: 0.719493

Epoch: 59
Loss: 0.1163036486508946
ROC train: 0.994671	val: 0.914584	test: 0.681134
PRC train: 0.998947	val: 0.855933	test: 0.677408

Epoch: 60
Loss: 0.11270297717904129
ROC train: 0.995391	val: 0.924420	test: 0.698110
PRC train: 0.999104	val: 0.887279	test: 0.707648

Epoch: 61
Loss: 0.10967022871298993
ROC train: 0.994493	val: 0.926629	test: 0.709877
PRC train: 0.998902	val: 0.894450	test: 0.723199

Epoch: 62
Loss: 0.12212819430531288
ROC train: 0.995047	val: 0.905751	test: 0.688465
PRC train: 0.999019	val: 0.854122	test: 0.701568

Epoch: 63
Loss: 0.1049915389690346
ROC train: 0.994465	val: 0.923517	test: 0.691551
PRC train: 0.998919	val: 0.873761	test: 0.711883

Epoch: 64
Loss: 0.11363946501677984
ROC train: 0.994687	val: 0.898926	test: 0.656154
PRC train: 0.998979	val: 0.827889	test: 0.681954

Epoch: 65
Loss: 0.10463475302349601
ROC train: 0.991094	val: 0.923918	test: 0.668306
PRC train: 0.998272	val: 0.887960	test: 0.691943

Epoch: 66
Loss: 0.10179056531895544
ROC train: 0.995885	val: 0.932751	test: 0.672743
PRC train: 0.999214	val: 0.899714	test: 0.682108

Epoch: 67
Loss: 0.11205683553532578
ROC train: 0.994811	val: 0.917997	test: 0.706501
PRC train: 0.998976	val: 0.868916	test: 0.728661

Epoch: 68
Loss: 0.10870302997892822
ROC train: 0.994051	val: 0.926327	test: 0.669271
PRC train: 0.998849	val: 0.885068	test: 0.693547

Epoch: 69
Loss: 0.10098964712851351
ROC train: 0.995286	val: 0.911372	test: 0.645351
PRC train: 0.999083	val: 0.839638	test: 0.643577

Epoch: 70
Loss: 0.10305343680886239
ROC train: 0.996102	val: 0.917194	test: 0.690394
PRC train: 0.999247	val: 0.877053	test: 0.701464

Epoch: 71
Loss: 0.11050961999177292
ROC train: 0.995848	val: 0.915989	test: 0.683931
PRC train: 0.999193	val: 0.880127	test: 0.703993

Epoch: 72
Loss: 0.10920293460932065
ROC train: 0.996601	val: 0.912275	test: 0.694637
PRC train: 0.999334	val: 0.872021	test: 0.709338

Epoch: 73
Loss: 0.08838127580731756
ROC train: 0.996049	val: 0.922814	test: 0.690683
PRC train: 0.999191	val: 0.874463	test: 0.687151

Epoch: 74
Loss: 0.0971462270674474
ROC train: 0.994078	val: 0.929740	test: 0.692515
PRC train: 0.998715	val: 0.899476	test: 0.706772

Epoch: 75
Loss: 0.10572105907070621
ROC train: 0.996625	val: 0.922714	test: 0.692805
PRC train: 0.999331	val: 0.889478	test: 0.702028

Epoch: 76
Loss: 0.10553034894494505
ROC train: 0.997639	val: 0.923818	test: 0.688947
PRC train: 0.999550	val: 0.882717	test: 0.707286

Epoch: 77
Loss: 0.08707053426730572
ROC train: 0.997144	val: 0.924019	test: 0.702932
PRC train: 0.999456	val: 0.883880	test: 0.728119

Epoch: 78
Loss: 0.10952365395780585
ROC train: 0.996845	val: 0.918699	test: 0.700714
PRC train: 0.999388	val: 0.882547	test: 0.724933

Epoch: 79
Loss: 0.10337561614854882
ROC train: 0.996683	val: 0.921811	test: 0.700617
PRC train: 0.999353	val: 0.890720	test: 0.715543

Epoch: 80
Loss: 0.07755847343107816
ROC train: 0.997287	val: 0.905249	test: 0.690683
PRC train: 0.999485	val: 0.857443	test: 0.702228

Epoch: 81
Loss: 0.08912788367024461
ROC train: 0.996784	val: 0.916692	test: 0.667342
PRC train: 0.999375	val: 0.879031	test: 0.671397

Epoch: 82
Loss: 0.0927378505111555
ROC train: 0.994660	val: 0.911573	test: 0.653260
PRC train: 0.998947	val: 0.869064	test: 0.657668

Epoch: 83
Loss: 0.09970936307331049
ROC train: 0.996699	val: 0.893205	test: 0.682967
PRC train: 0.999365	val: 0.848187	test: 0.687778

Epoch: 84
Loss: 0.08790100790519785
ROC train: 0.996002	val: 0.909264	test: 0.734086
PRC train: 0.999201	val: 0.884413	test: 0.746898

Epoch: 85
Loss: 0.08339007324521512
ROC train: 0.996925	val: 0.923517	test: 0.713542
PRC train: 0.999410	val: 0.893944	test: 0.707166

Epoch: 86
Loss: 0.09348741631109318
ROC train: 0.997803	val: 0.908662	test: 0.710262
PRC train: 0.999579	val: 0.846252	test: 0.723841

Epoch: 87
Loss: 0.08449481507441158
ROC train: 0.997785	val: 0.913179	test: 0.698978
PRC train: 0.999580	val: 0.843577	test: 0.707207

Epoch: 88
Loss: 0.09439050839233262
ROC train: 0.997788	val: 0.911774	test: 0.681424
PRC train: 0.999580	val: 0.844592	test: 0.682416

Epoch: 89
Loss: 0.09147661294595898
ROC train: 0.997245	val: 0.892000	test: 0.674961
PRC train: 0.999474	val: 0.811577	test: 0.675807

Epoch: 90
Loss: 0.09524428032154583
ROC train: 0.995995	val: 0.918900	test: 0.703897
PRC train: 0.999219	val: 0.863542	test: 0.714719

Epoch: 91
Loss: 0.08596986720783704
ROC train: 0.997110	val: 0.918800	test: 0.684992
PRC train: 0.999446	val: 0.854492	test: 0.675464

Epoch: 92
Loss: 0.08637893090616973
ROC train: 0.997743	val: 0.906855	test: 0.687114
PRC train: 0.999572	val: 0.834632	test: 0.697481

Epoch: 93
Loss: 0.09285265374175214
ROC train: 0.997796	val: 0.899528	test: 0.676794
PRC train: 0.999579	val: 0.843062	test: 0.687783

Epoch: 94
Loss: 0.07314730861016348
ROC train: 0.998402	val: 0.915751	test: 0.765412
PRC train: 0.999643	val: 0.986095	test: 0.639478

Epoch: 95
Loss: 0.07763634546912733
ROC train: 0.998486	val: 0.918415	test: 0.769220
PRC train: 0.999663	val: 0.985696	test: 0.642942

Epoch: 96
Loss: 0.08365082564163095
ROC train: 0.997319	val: 0.929182	test: 0.760797
PRC train: 0.999387	val: 0.988954	test: 0.628097

Epoch: 97
Loss: 0.07603002433673452
ROC train: 0.998631	val: 0.945277	test: 0.764382
PRC train: 0.999694	val: 0.991884	test: 0.627414

Epoch: 98
Loss: 0.07343751199846354
ROC train: 0.998642	val: 0.936286	test: 0.764068
PRC train: 0.999698	val: 0.989620	test: 0.630280

Epoch: 99
Loss: 0.08214982053188767
ROC train: 0.998715	val: 0.917305	test: 0.758020
PRC train: 0.999712	val: 0.985896	test: 0.633312

Epoch: 100
Loss: 0.08145949205224613
ROC train: 0.998024	val: 0.887668	test: 0.733961
PRC train: 0.999561	val: 0.980040	test: 0.606764

Epoch: 101
Loss: 0.09333278155972284
ROC train: 0.998670	val: 0.922522	test: 0.765950
PRC train: 0.999704	val: 0.987058	test: 0.644706

Epoch: 102
Loss: 0.08007205497190936
ROC train: 0.998538	val: 0.923521	test: 0.755287
PRC train: 0.999665	val: 0.987360	test: 0.599564

Epoch: 103
Loss: 0.07464968078039183
ROC train: 0.998899	val: 0.923854	test: 0.747401
PRC train: 0.999754	val: 0.986756	test: 0.608439

Epoch: 104
Loss: 0.08250376121222376
ROC train: 0.998893	val: 0.925741	test: 0.761201
PRC train: 0.999753	val: 0.987390	test: 0.628252

Epoch: 105
Loss: 0.07342530241819088
ROC train: 0.998619	val: 0.919747	test: 0.750045
PRC train: 0.999691	val: 0.985046	test: 0.614342

Epoch: 106
Loss: 0.07747950351048966
ROC train: 0.998843	val: 0.915751	test: 0.751299
PRC train: 0.999741	val: 0.986790	test: 0.623949

Epoch: 107
Loss: 0.08561476900501092
ROC train: 0.998377	val: 0.929848	test: 0.773566
PRC train: 0.999635	val: 0.989602	test: 0.672149

Epoch: 108
Loss: 0.092927423550542
ROC train: 0.999010	val: 0.927073	test: 0.772849
PRC train: 0.999777	val: 0.987850	test: 0.657677

Epoch: 109
Loss: 0.09096272401368556
ROC train: 0.999001	val: 0.902986	test: 0.753674
PRC train: 0.999777	val: 0.979234	test: 0.618419

Epoch: 110
Loss: 0.07105870455703807
ROC train: 0.997718	val: 0.919636	test: 0.751254
PRC train: 0.999483	val: 0.982812	test: 0.627933

Epoch: 111
Loss: 0.08228388561278469
ROC train: 0.998692	val: 0.927184	test: 0.758692
PRC train: 0.999709	val: 0.987976	test: 0.624639

Epoch: 112
Loss: 0.07196483790381852
ROC train: 0.999166	val: 0.913864	test: 0.753271
PRC train: 0.999812	val: 0.986687	test: 0.611264

Epoch: 113
Loss: 0.07266570991978556
ROC train: 0.998922	val: 0.920857	test: 0.769400
PRC train: 0.999758	val: 0.987715	test: 0.657764

Epoch: 114
Loss: 0.07981940179186846
ROC train: 0.999034	val: 0.934732	test: 0.774776
PRC train: 0.999783	val: 0.989829	test: 0.644968

Epoch: 115
Loss: 0.0728275191264441
ROC train: 0.999089	val: 0.931846	test: 0.760887
PRC train: 0.999794	val: 0.988169	test: 0.624998

Epoch: 116
Loss: 0.0670643826612115
ROC train: 0.998420	val: 0.920413	test: 0.758961
PRC train: 0.999644	val: 0.986507	test: 0.631950

Epoch: 117
Loss: 0.06696767534908771
ROC train: 0.998489	val: 0.937285	test: 0.764740
PRC train: 0.999659	val: 0.991337	test: 0.644799

Epoch: 118
Loss: 0.0704439285172832
ROC train: 0.998397	val: 0.940726	test: 0.773208
PRC train: 0.999634	val: 0.991916	test: 0.649642

Epoch: 119
Loss: 0.07330045927477012
ROC train: 0.998955	val: 0.932290	test: 0.764606
PRC train: 0.999764	val: 0.989792	test: 0.628920

Epoch: 120
Loss: 0.08466278657254102
ROC train: 0.998945	val: 0.927961	test: 0.749014
PRC train: 0.999764	val: 0.988401	test: 0.620419

Early stopping
Best (ROC):	 train: 0.951343	val: 0.969919	test: 0.792159
Best (PRC):	 train: 0.984693	val: 0.996290	test: 0.735043

ROC train: 0.998103	val: 0.915751	test: 0.776165
PRC train: 0.999579	val: 0.982796	test: 0.657869

Epoch: 95
Loss: 0.09099460163650779
ROC train: 0.997891	val: 0.920080	test: 0.769848
PRC train: 0.999532	val: 0.984955	test: 0.645614

Epoch: 96
Loss: 0.09220087157479329
ROC train: 0.997026	val: 0.941725	test: 0.786918
PRC train: 0.999328	val: 0.989022	test: 0.678429

Epoch: 97
Loss: 0.09196290787670491
ROC train: 0.997728	val: 0.926962	test: 0.771326
PRC train: 0.999497	val: 0.977566	test: 0.654596

Epoch: 98
Loss: 0.08648796383481146
ROC train: 0.998594	val: 0.917860	test: 0.758065
PRC train: 0.999686	val: 0.979914	test: 0.629353

Epoch: 99
Loss: 0.09767369433803186
ROC train: 0.997507	val: 0.921745	test: 0.775045
PRC train: 0.999444	val: 0.987112	test: 0.671770

Epoch: 100
Loss: 0.10083178771961276
ROC train: 0.996350	val: 0.937729	test: 0.779884
PRC train: 0.999165	val: 0.990390	test: 0.663228

Epoch: 101
Loss: 0.08803873084465315
ROC train: 0.998169	val: 0.930625	test: 0.767563
PRC train: 0.999591	val: 0.987456	test: 0.635708

Epoch: 102
Loss: 0.08392749237502041
ROC train: 0.998713	val: 0.932290	test: 0.771326
PRC train: 0.999713	val: 0.986349	test: 0.643535

Epoch: 103
Loss: 0.06825958736831646
ROC train: 0.998566	val: 0.929626	test: 0.777643
PRC train: 0.999681	val: 0.986850	test: 0.662557

Epoch: 104
Loss: 0.08232558491312574
ROC train: 0.998336	val: 0.915973	test: 0.774149
PRC train: 0.999630	val: 0.982073	test: 0.658766

Epoch: 105
Loss: 0.08240209110852591
ROC train: 0.998804	val: 0.917083	test: 0.769041
PRC train: 0.999734	val: 0.981859	test: 0.646806

Epoch: 106
Loss: 0.08789201269224729
ROC train: 0.998973	val: 0.925963	test: 0.773880
PRC train: 0.999770	val: 0.983090	test: 0.648491

Epoch: 107
Loss: 0.0761888802298223
ROC train: 0.998130	val: 0.918304	test: 0.779884
PRC train: 0.999582	val: 0.984862	test: 0.685982

Epoch: 108
Loss: 0.08101887589955924
ROC train: 0.998212	val: 0.907204	test: 0.769982
PRC train: 0.999601	val: 0.980151	test: 0.660265

Epoch: 109
Loss: 0.06895461391827465
ROC train: 0.998789	val: 0.915307	test: 0.776030
PRC train: 0.999730	val: 0.978642	test: 0.660348

Epoch: 110
Loss: 0.08211168277852343
ROC train: 0.998738	val: 0.930625	test: 0.773925
PRC train: 0.999719	val: 0.986454	test: 0.654580

Epoch: 111
Loss: 0.07713879302519473
ROC train: 0.997925	val: 0.908980	test: 0.758602
PRC train: 0.999538	val: 0.983677	test: 0.611688

Epoch: 112
Loss: 0.08259261163100924
ROC train: 0.998538	val: 0.933178	test: 0.772222
PRC train: 0.999673	val: 0.986416	test: 0.624399

Epoch: 113
Loss: 0.08195202621978565
ROC train: 0.999176	val: 0.933178	test: 0.773208
PRC train: 0.999815	val: 0.986175	test: 0.654710

Epoch: 114
Loss: 0.08613637549284875
ROC train: 0.998690	val: 0.921079	test: 0.762724
PRC train: 0.999707	val: 0.983968	test: 0.653654

Epoch: 115
Loss: 0.07387566927325619
ROC train: 0.998898	val: 0.928072	test: 0.766219
PRC train: 0.999754	val: 0.985251	test: 0.632908

Epoch: 116
Loss: 0.07327211715751254
ROC train: 0.998408	val: 0.936841	test: 0.771909
PRC train: 0.999643	val: 0.987271	test: 0.641886

Epoch: 117
Loss: 0.08281129323637201
ROC train: 0.998978	val: 0.931402	test: 0.768235
PRC train: 0.999771	val: 0.986753	test: 0.653148

Epoch: 118
Loss: 0.07005197038443721
ROC train: 0.998860	val: 0.931180	test: 0.768324
PRC train: 0.999743	val: 0.988065	test: 0.662956

Epoch: 119
Loss: 0.07713149800246193
ROC train: 0.999128	val: 0.931624	test: 0.778315
PRC train: 0.999803	val: 0.986113	test: 0.677466

Epoch: 120
Loss: 0.06784859070475896
ROC train: 0.998697	val: 0.927739	test: 0.773073
PRC train: 0.999704	val: 0.984300	test: 0.672389

Early stopping
Best (ROC):	 train: 0.947510	val: 0.967144	test: 0.780511
Best (PRC):	 train: 0.984536	val: 0.995424	test: 0.672268

ROC train: 0.994229	val: 0.919747	test: 0.767832
PRC train: 0.998721	val: 0.984426	test: 0.642157

Epoch: 95
Loss: 0.09727052781583795
ROC train: 0.998097	val: 0.917083	test: 0.764292
PRC train: 0.999576	val: 0.984501	test: 0.626028

Epoch: 96
Loss: 0.09509345886603073
ROC train: 0.997462	val: 0.923410	test: 0.761828
PRC train: 0.999432	val: 0.985748	test: 0.643696

Epoch: 97
Loss: 0.08835407133761573
ROC train: 0.998003	val: 0.933955	test: 0.768369
PRC train: 0.999553	val: 0.986109	test: 0.656614

Epoch: 98
Loss: 0.08740069011747899
ROC train: 0.997861	val: 0.924076	test: 0.778405
PRC train: 0.999519	val: 0.982327	test: 0.663783

Epoch: 99
Loss: 0.09364907248107303
ROC train: 0.998734	val: 0.921079	test: 0.765995
PRC train: 0.999719	val: 0.982523	test: 0.645727

Epoch: 100
Loss: 0.08817403356549149
ROC train: 0.998550	val: 0.924853	test: 0.770520
PRC train: 0.999676	val: 0.986134	test: 0.658557

Epoch: 101
Loss: 0.07887645336922612
ROC train: 0.998810	val: 0.931957	test: 0.786425
PRC train: 0.999732	val: 0.985907	test: 0.674879

Epoch: 102
Loss: 0.08495539095897625
ROC train: 0.998550	val: 0.929848	test: 0.774373
PRC train: 0.999677	val: 0.986047	test: 0.649244

Epoch: 103
Loss: 0.08797197812510262
ROC train: 0.998174	val: 0.927073	test: 0.762769
PRC train: 0.999594	val: 0.984557	test: 0.627678

Epoch: 104
Loss: 0.08859070086929054
ROC train: 0.998369	val: 0.927073	test: 0.767742
PRC train: 0.999636	val: 0.984048	test: 0.647059

Epoch: 105
Loss: 0.07990999555230059
ROC train: 0.998642	val: 0.924631	test: 0.765233
PRC train: 0.999698	val: 0.985491	test: 0.644972

Epoch: 106
Loss: 0.06837867043796407
ROC train: 0.998845	val: 0.922744	test: 0.772849
PRC train: 0.999742	val: 0.984597	test: 0.647814

Epoch: 107
Loss: 0.07412623015099012
ROC train: 0.998904	val: 0.921967	test: 0.773925
PRC train: 0.999755	val: 0.984069	test: 0.646905

Epoch: 108
Loss: 0.0802140059981619
ROC train: 0.998670	val: 0.927739	test: 0.771774
PRC train: 0.999704	val: 0.983481	test: 0.643635

Epoch: 109
Loss: 0.08184429507405072
ROC train: 0.998861	val: 0.926185	test: 0.759812
PRC train: 0.999747	val: 0.982567	test: 0.617842

Epoch: 110
Loss: 0.07915085459112443
ROC train: 0.998682	val: 0.929293	test: 0.767339
PRC train: 0.999705	val: 0.983366	test: 0.632203

Epoch: 111
Loss: 0.06728823792825556
ROC train: 0.998764	val: 0.920080	test: 0.767608
PRC train: 0.999725	val: 0.982752	test: 0.628092

Epoch: 112
Loss: 0.0657338511690352
ROC train: 0.998619	val: 0.922522	test: 0.773925
PRC train: 0.999692	val: 0.983344	test: 0.629795

Epoch: 113
Loss: 0.07106787192231155
ROC train: 0.998861	val: 0.923743	test: 0.770072
PRC train: 0.999745	val: 0.983850	test: 0.621004

Epoch: 114
Loss: 0.07609333281515213
ROC train: 0.998700	val: 0.926629	test: 0.772715
PRC train: 0.999709	val: 0.983050	test: 0.636760

Epoch: 115
Loss: 0.07352019166996256
ROC train: 0.999161	val: 0.922078	test: 0.772088
PRC train: 0.999811	val: 0.980278	test: 0.635066

Epoch: 116
Loss: 0.07953627682558657
ROC train: 0.998771	val: 0.917527	test: 0.774462
PRC train: 0.999727	val: 0.977811	test: 0.631583

Epoch: 117
Loss: 0.09707661519059423
ROC train: 0.998250	val: 0.920635	test: 0.758109
PRC train: 0.999612	val: 0.980771	test: 0.617441

Epoch: 118
Loss: 0.08088560479146771
ROC train: 0.997017	val: 0.903430	test: 0.734005
PRC train: 0.999329	val: 0.978406	test: 0.567851

Epoch: 119
Loss: 0.08652417635493671
ROC train: 0.998665	val: 0.913753	test: 0.754794
PRC train: 0.999702	val: 0.982935	test: 0.620694

Epoch: 120
Loss: 0.07755034908062562
ROC train: 0.992121	val: 0.909091	test: 0.757930
PRC train: 0.998064	val: 0.981993	test: 0.665738

Early stopping
Best (ROC):	 train: 0.959607	val: 0.960373	test: 0.786783
Best (PRC):	 train: 0.988283	val: 0.994657	test: 0.714888
All runs completed.

ROC train: 0.998141	val: 0.884071	test: 0.667535
PRC train: 0.999644	val: 0.809359	test: 0.660964

Epoch: 95
Loss: 0.09317124699195758
ROC train: 0.997366	val: 0.904647	test: 0.677855
PRC train: 0.999497	val: 0.848089	test: 0.681493

Epoch: 96
Loss: 0.08156429611198349
ROC train: 0.998113	val: 0.890394	test: 0.657407
PRC train: 0.999640	val: 0.823581	test: 0.656807

Epoch: 97
Loss: 0.08749585117021859
ROC train: 0.998304	val: 0.887383	test: 0.664159
PRC train: 0.999676	val: 0.821895	test: 0.660929

Epoch: 98
Loss: 0.09137732090337139
ROC train: 0.997852	val: 0.893104	test: 0.681038
PRC train: 0.999590	val: 0.817705	test: 0.664337

Epoch: 99
Loss: 0.07533048824444251
ROC train: 0.996899	val: 0.896116	test: 0.673611
PRC train: 0.999406	val: 0.816030	test: 0.660342

Epoch: 100
Loss: 0.08704734664008607
ROC train: 0.998400	val: 0.903643	test: 0.678434
PRC train: 0.999694	val: 0.822696	test: 0.679665

Epoch: 101
Loss: 0.08578009792064582
ROC train: 0.998000	val: 0.904145	test: 0.682099
PRC train: 0.999621	val: 0.828242	test: 0.681569

Epoch: 102
Loss: 0.09205760232707814
ROC train: 0.997425	val: 0.889993	test: 0.685667
PRC train: 0.999508	val: 0.802780	test: 0.680492

Epoch: 103
Loss: 0.08972252697629433
ROC train: 0.996871	val: 0.899127	test: 0.665606
PRC train: 0.999402	val: 0.829261	test: 0.670502

Epoch: 104
Loss: 0.08633200453099736
ROC train: 0.997798	val: 0.899629	test: 0.688754
PRC train: 0.999575	val: 0.839643	test: 0.705413

Epoch: 105
Loss: 0.08505566371288024
ROC train: 0.998186	val: 0.911874	test: 0.688175
PRC train: 0.999652	val: 0.864851	test: 0.685132

Epoch: 106
Loss: 0.07986819581885238
ROC train: 0.997568	val: 0.896517	test: 0.656925
PRC train: 0.999528	val: 0.838924	test: 0.642777

Epoch: 107
Loss: 0.08879018588050958
ROC train: 0.998595	val: 0.881361	test: 0.665895
PRC train: 0.999730	val: 0.807099	test: 0.651222

Epoch: 108
Loss: 0.08210511567785107
ROC train: 0.998387	val: 0.901235	test: 0.672357
PRC train: 0.999694	val: 0.841431	test: 0.660626

Epoch: 109
Loss: 0.07400549537564145
ROC train: 0.997474	val: 0.906153	test: 0.686343
PRC train: 0.999522	val: 0.847159	test: 0.684218

Epoch: 110
Loss: 0.0841334603684067
ROC train: 0.998805	val: 0.899428	test: 0.685089
PRC train: 0.999773	val: 0.821276	test: 0.673907

Epoch: 111
Loss: 0.08548091340929527
ROC train: 0.997723	val: 0.890796	test: 0.669367
PRC train: 0.999562	val: 0.826605	test: 0.661472

Epoch: 112
Loss: 0.09160588128273892
ROC train: 0.998624	val: 0.896718	test: 0.663677
PRC train: 0.999738	val: 0.838512	test: 0.658363

Epoch: 113
Loss: 0.0759168898384264
ROC train: 0.998189	val: 0.891197	test: 0.679880
PRC train: 0.999658	val: 0.820068	test: 0.674197

Epoch: 114
Loss: 0.08255883720057336
ROC train: 0.996935	val: 0.907156	test: 0.691358
PRC train: 0.999413	val: 0.844079	test: 0.662850

Epoch: 115
Loss: 0.07300132146373019
ROC train: 0.997289	val: 0.899829	test: 0.715374
PRC train: 0.999479	val: 0.845313	test: 0.704827

Epoch: 116
Loss: 0.08670353237386307
ROC train: 0.998731	val: 0.902539	test: 0.658758
PRC train: 0.999758	val: 0.855108	test: 0.656427

Epoch: 117
Loss: 0.0710072358509045
ROC train: 0.998665	val: 0.907759	test: 0.663773
PRC train: 0.999744	val: 0.861748	test: 0.669325

Epoch: 118
Loss: 0.06794039312789289
ROC train: 0.998792	val: 0.905350	test: 0.677373
PRC train: 0.999769	val: 0.863385	test: 0.673715

Epoch: 119
Loss: 0.06553608750576491
ROC train: 0.998796	val: 0.905751	test: 0.673225
PRC train: 0.999770	val: 0.852569	test: 0.669307

Epoch: 120
Loss: 0.06718245456372654
ROC train: 0.998646	val: 0.901034	test: 0.657118
PRC train: 0.999741	val: 0.841619	test: 0.646317

Early stopping
Best (ROC):	 train: 0.946656	val: 0.933153	test: 0.680845
Best (PRC):	 train: 0.986696	val: 0.899483	test: 0.725733

ROC train: 0.998162	val: 0.878551	test: 0.692805
PRC train: 0.999651	val: 0.778827	test: 0.697241

Epoch: 95
Loss: 0.08881131370473665
ROC train: 0.997719	val: 0.901335	test: 0.677083
PRC train: 0.999568	val: 0.817659	test: 0.687495

Epoch: 96
Loss: 0.08200744501081707
ROC train: 0.997528	val: 0.903443	test: 0.697049
PRC train: 0.999522	val: 0.816349	test: 0.710391

Epoch: 97
Loss: 0.0881550220381705
ROC train: 0.997782	val: 0.885476	test: 0.699846
PRC train: 0.999568	val: 0.802385	test: 0.709218

Epoch: 98
Loss: 0.08599363030560747
ROC train: 0.998320	val: 0.893205	test: 0.680556
PRC train: 0.999678	val: 0.803847	test: 0.684358

Epoch: 99
Loss: 0.08300094815862458
ROC train: 0.998324	val: 0.892703	test: 0.685764
PRC train: 0.999681	val: 0.820881	test: 0.674799

Epoch: 100
Loss: 0.09078888791440108
ROC train: 0.998300	val: 0.886881	test: 0.663291
PRC train: 0.999675	val: 0.793411	test: 0.653423

Epoch: 101
Loss: 0.07774158560690303
ROC train: 0.997472	val: 0.890093	test: 0.673515
PRC train: 0.999517	val: 0.808523	test: 0.672304

Epoch: 102
Loss: 0.08183341530307722
ROC train: 0.997259	val: 0.899428	test: 0.701100
PRC train: 0.999469	val: 0.814988	test: 0.683135

Epoch: 103
Loss: 0.09463515296186147
ROC train: 0.998746	val: 0.876543	test: 0.679784
PRC train: 0.999763	val: 0.785654	test: 0.677991

Epoch: 104
Loss: 0.07715863598807124
ROC train: 0.998683	val: 0.890595	test: 0.668210
PRC train: 0.999749	val: 0.831401	test: 0.677672

Epoch: 105
Loss: 0.07932340701745463
ROC train: 0.998571	val: 0.892101	test: 0.668403
PRC train: 0.999728	val: 0.824254	test: 0.670867

Epoch: 106
Loss: 0.08159405575857588
ROC train: 0.998290	val: 0.880357	test: 0.689043
PRC train: 0.999677	val: 0.791629	test: 0.691268

Epoch: 107
Loss: 0.07848613213269265
ROC train: 0.998112	val: 0.898725	test: 0.692130
PRC train: 0.999646	val: 0.808803	test: 0.689415

Epoch: 108
Loss: 0.07947710774469577
ROC train: 0.998857	val: 0.886881	test: 0.685571
PRC train: 0.999783	val: 0.784202	test: 0.676328

Epoch: 109
Loss: 0.07845473085811229
ROC train: 0.998865	val: 0.880658	test: 0.669753
PRC train: 0.999784	val: 0.788080	test: 0.662937

Epoch: 110
Loss: 0.07887130754773564
ROC train: 0.998753	val: 0.894008	test: 0.681906
PRC train: 0.999763	val: 0.813358	test: 0.664284

Epoch: 111
Loss: 0.07721109387221424
ROC train: 0.998488	val: 0.887183	test: 0.683546
PRC train: 0.999713	val: 0.811476	test: 0.689788

Epoch: 112
Loss: 0.08007307295199341
ROC train: 0.998949	val: 0.882666	test: 0.680170
PRC train: 0.999800	val: 0.786597	test: 0.686021

Epoch: 113
Loss: 0.06345581169957103
ROC train: 0.998694	val: 0.879554	test: 0.688561
PRC train: 0.999751	val: 0.768084	test: 0.678721

Epoch: 114
Loss: 0.07380673613309796
ROC train: 0.998820	val: 0.874435	test: 0.689815
PRC train: 0.999776	val: 0.762637	test: 0.683417

Epoch: 115
Loss: 0.07160618292604512
ROC train: 0.999088	val: 0.875339	test: 0.685185
PRC train: 0.999827	val: 0.774017	test: 0.685374

Epoch: 116
Loss: 0.08486673466290225
ROC train: 0.999112	val: 0.884673	test: 0.687018
PRC train: 0.999830	val: 0.794147	test: 0.695689

Epoch: 117
Loss: 0.08293984628561273
ROC train: 0.998529	val: 0.879554	test: 0.681617
PRC train: 0.999719	val: 0.769938	test: 0.688224

Epoch: 118
Loss: 0.07286379125748975
ROC train: 0.998137	val: 0.903342	test: 0.706597
PRC train: 0.999647	val: 0.834906	test: 0.706306

Epoch: 119
Loss: 0.07978238024187626
ROC train: 0.998795	val: 0.895714	test: 0.714313
PRC train: 0.999771	val: 0.806626	test: 0.703255

Epoch: 120
Loss: 0.0796049203988611
ROC train: 0.998935	val: 0.888688	test: 0.687500
PRC train: 0.999795	val: 0.781426	test: 0.674342

Early stopping
Best (ROC):	 train: 0.981969	val: 0.935863	test: 0.711709
Best (PRC):	 train: 0.995899	val: 0.894223	test: 0.737547

ROC train: 0.997925	val: 0.911272	test: 0.678723
PRC train: 0.999598	val: 0.859303	test: 0.682442

Epoch: 95
Loss: 0.09271166162789947
ROC train: 0.996471	val: 0.910368	test: 0.684124
PRC train: 0.999334	val: 0.859135	test: 0.681663

Epoch: 96
Loss: 0.09202745165746741
ROC train: 0.991770	val: 0.897922	test: 0.654514
PRC train: 0.998356	val: 0.830559	test: 0.655129

Epoch: 97
Loss: 0.07918603268670635
ROC train: 0.997850	val: 0.901134	test: 0.650752
PRC train: 0.999584	val: 0.821269	test: 0.651758

Epoch: 98
Loss: 0.08854506205495981
ROC train: 0.997284	val: 0.916692	test: 0.693191
PRC train: 0.999467	val: 0.876778	test: 0.708741

Epoch: 99
Loss: 0.07914769241329433
ROC train: 0.996690	val: 0.914283	test: 0.697531
PRC train: 0.999353	val: 0.856119	test: 0.701559

Epoch: 100
Loss: 0.07270947017854094
ROC train: 0.998139	val: 0.904246	test: 0.693191
PRC train: 0.999640	val: 0.844952	test: 0.685788

Epoch: 101
Loss: 0.07206453810335485
ROC train: 0.997977	val: 0.906153	test: 0.660783
PRC train: 0.999612	val: 0.853853	test: 0.665682

Epoch: 102
Loss: 0.07199442462763421
ROC train: 0.998414	val: 0.905852	test: 0.665992
PRC train: 0.999695	val: 0.863015	test: 0.669824

Epoch: 103
Loss: 0.08049724047811495
ROC train: 0.998219	val: 0.910168	test: 0.670910
PRC train: 0.999657	val: 0.866604	test: 0.665066

Epoch: 104
Loss: 0.08214559762211995
ROC train: 0.998374	val: 0.910268	test: 0.686535
PRC train: 0.999690	val: 0.867472	test: 0.684107

Epoch: 105
Loss: 0.0713119125934811
ROC train: 0.997169	val: 0.891097	test: 0.697434
PRC train: 0.999451	val: 0.847257	test: 0.699690

Epoch: 106
Loss: 0.08522806013976662
ROC train: 0.997307	val: 0.910168	test: 0.692998
PRC train: 0.999475	val: 0.860116	test: 0.702309

Epoch: 107
Loss: 0.07355344731420359
ROC train: 0.998494	val: 0.907558	test: 0.684992
PRC train: 0.999709	val: 0.839427	test: 0.690052

Epoch: 108
Loss: 0.07880684666929501
ROC train: 0.998665	val: 0.901034	test: 0.650945
PRC train: 0.999746	val: 0.829065	test: 0.651983

Epoch: 109
Loss: 0.07242132388953978
ROC train: 0.995957	val: 0.901235	test: 0.635995
PRC train: 0.999228	val: 0.841476	test: 0.632734

Epoch: 110
Loss: 0.06872070480411319
ROC train: 0.997479	val: 0.904045	test: 0.663387
PRC train: 0.999512	val: 0.868264	test: 0.670450

Epoch: 111
Loss: 0.07198747446336937
ROC train: 0.998837	val: 0.903945	test: 0.685860
PRC train: 0.999776	val: 0.864570	test: 0.682123

Epoch: 112
Loss: 0.06607306327654892
ROC train: 0.998694	val: 0.917695	test: 0.687500
PRC train: 0.999748	val: 0.876125	test: 0.671343

Epoch: 113
Loss: 0.0669577872663807
ROC train: 0.998788	val: 0.913781	test: 0.675637
PRC train: 0.999768	val: 0.870091	test: 0.669511

Epoch: 114
Loss: 0.07607619115565833
ROC train: 0.998987	val: 0.908662	test: 0.684510
PRC train: 0.999806	val: 0.861396	test: 0.687277

Epoch: 115
Loss: 0.0837310662059231
ROC train: 0.999080	val: 0.917093	test: 0.682967
PRC train: 0.999824	val: 0.871034	test: 0.682690

Epoch: 116
Loss: 0.07182645978680353
ROC train: 0.998792	val: 0.915186	test: 0.695891
PRC train: 0.999767	val: 0.873746	test: 0.701696

Epoch: 117
Loss: 0.06301051330393836
ROC train: 0.998733	val: 0.913881	test: 0.697917
PRC train: 0.999759	val: 0.850454	test: 0.695975

Epoch: 118
Loss: 0.09016476475085344
ROC train: 0.999076	val: 0.916290	test: 0.696277
PRC train: 0.999823	val: 0.872800	test: 0.694422

Epoch: 119
Loss: 0.06645345482882122
ROC train: 0.998090	val: 0.913380	test: 0.666763
PRC train: 0.999637	val: 0.885906	test: 0.681721

Epoch: 120
Loss: 0.07496505287652133
ROC train: 0.998651	val: 0.905149	test: 0.675733
PRC train: 0.999743	val: 0.850098	test: 0.666431

Early stopping
Best (ROC):	 train: 0.976612	val: 0.943491	test: 0.710262
Best (PRC):	 train: 0.994306	val: 0.929755	test: 0.758026
All runs completed.
