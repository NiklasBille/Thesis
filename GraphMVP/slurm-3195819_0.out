>>> Starting run for dataset: esol
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running SCAFF configs_split_experiments/GraphCL/esol/scaff/train_prop=0.6.yml on cuda:0
Running SCAFF configs_split_experiments/GraphCL/esol/scaff/train_prop=0.7.yml on cuda:1
Running SCAFF configs_split_experiments/GraphCL/esol/scaff/train_prop=0.8.yml on cuda:2
Starting process for seed 1: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/esol/scaff/train_prop=0.7.yml --runseed 1 --device cuda:1
Starting process for seed 2: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/esol/scaff/train_prop=0.7.yml --runseed 2 --device cuda:1
Starting process for seed 1: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/esol/scaff/train_prop=0.8.yml --runseed 1 --device cuda:2
Starting process for seed 3: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/esol/scaff/train_prop=0.7.yml --runseed 3 --device cuda:1
Starting process for seed 2: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/esol/scaff/train_prop=0.8.yml --runseed 2 --device cuda:2
Starting process for seed 3: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/esol/scaff/train_prop=0.8.yml --runseed 3 --device cuda:2
Starting process for seed 1: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/esol/scaff/train_prop=0.6.yml --runseed 1 --device cuda:0
Starting process for seed 2: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/esol/scaff/train_prop=0.6.yml --runseed 2 --device cuda:0
Starting process for seed 3: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/esol/scaff/train_prop=0.6.yml --runseed 3 --device cuda:0
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/esol/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/esol/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/esol/scaff/train_prop=0.6/esol_scaff_3_20-05_14-43-15  ]
[ Using Seed :  3  ]
[ Using device :  cuda:0  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 11.574758529663086
RMSE train: 3.071248	val: 3.593768	test: 4.054128
MAE train: 2.466709	val: 3.099727	test: 3.536331

Epoch: 2
Loss: 10.431230545043945
RMSE train: 2.996385	val: 3.640222	test: 4.135941
MAE train: 2.405175	val: 3.152375	test: 3.682452

Epoch: 3
Loss: 9.598259290059408
RMSE train: 2.946299	val: 3.723028	test: 4.247112
MAE train: 2.389441	val: 3.240099	test: 3.828270

Epoch: 4
Loss: 8.9075927734375
RMSE train: 2.905381	val: 3.809235	test: 4.348848
MAE train: 2.369497	val: 3.332365	test: 3.961607

Epoch: 5
Loss: 8.328323046366373
RMSE train: 2.793926	val: 3.741284	test: 4.246071
MAE train: 2.262494	val: 3.287360	test: 3.878985

Epoch: 6
Loss: 7.656147162119548
RMSE train: 2.722694	val: 3.659265	test: 4.112401
MAE train: 2.200295	val: 3.226884	test: 3.759812

Epoch: 7
Loss: 6.940022945404053
RMSE train: 2.743962	val: 3.673710	test: 4.113941
MAE train: 2.256835	val: 3.241864	test: 3.770067

Epoch: 8
Loss: 6.653781255086263
RMSE train: 2.737684	val: 3.675319	test: 4.098555
MAE train: 2.294631	val: 3.240874	test: 3.749064

Epoch: 9
Loss: 6.187081654866536
RMSE train: 2.697452	val: 3.636134	test: 4.033359
MAE train: 2.290678	val: 3.202712	test: 3.674765

Epoch: 10
Loss: 5.7347175280253095
RMSE train: 2.656868	val: 3.519699	test: 3.874989
MAE train: 2.287979	val: 3.094401	test: 3.505417

Epoch: 11
Loss: 5.266638437906901
RMSE train: 2.592497	val: 3.429849	test: 3.741844
MAE train: 2.247803	val: 3.014081	test: 3.371008

Epoch: 12
Loss: 4.743741909662883
RMSE train: 2.509530	val: 3.306171	test: 3.564339
MAE train: 2.177710	val: 2.902180	test: 3.191763

Epoch: 13
Loss: 4.355195840199788
RMSE train: 2.384158	val: 3.196939	test: 3.416330
MAE train: 2.062316	val: 2.805174	test: 3.036557

Epoch: 14
Loss: 4.020613829294841
RMSE train: 2.301748	val: 3.112258	test: 3.300262
MAE train: 1.997161	val: 2.730003	test: 2.915679

Epoch: 15
Loss: 3.5506526629130044
RMSE train: 2.264285	val: 3.029160	test: 3.195787
MAE train: 1.974267	val: 2.656003	test: 2.809552

Epoch: 16
Loss: 3.341334899266561
RMSE train: 2.173617	val: 2.917131	test: 3.043380
MAE train: 1.907282	val: 2.550194	test: 2.646744

Epoch: 17
Loss: 2.9622104167938232
RMSE train: 2.082888	val: 2.817627	test: 2.901650
MAE train: 1.834017	val: 2.456127	test: 2.500476

Epoch: 18
Loss: 2.655074119567871
RMSE train: 1.939515	val: 2.684280	test: 2.738679
MAE train: 1.699794	val: 2.334739	test: 2.334159

Epoch: 19
Loss: 2.39336105187734
RMSE train: 1.749969	val: 2.515665	test: 2.548598
MAE train: 1.496213	val: 2.175408	test: 2.152343

Epoch: 20
Loss: 2.1078892151514688
RMSE train: 1.584157	val: 2.394689	test: 2.407963
MAE train: 1.315222	val: 2.059972	test: 2.020573

Epoch: 21
Loss: 1.884081482887268
RMSE train: 1.468293	val: 2.337538	test: 2.367426
MAE train: 1.205236	val: 2.016597	test: 1.971325

Epoch: 22
Loss: 1.7407861948013306
RMSE train: 1.355933	val: 2.255273	test: 2.291017
MAE train: 1.090977	val: 1.945337	test: 1.878642

Epoch: 23
Loss: 1.5257632335027058Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/esol/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/esol/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/esol/scaff/train_prop=0.6/esol_scaff_2_20-05_14-43-15  ]
[ Using Seed :  2  ]
[ Using device :  cuda:0  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 11.789557139078775
RMSE train: 3.064899	val: 3.641999	test: 4.138130
MAE train: 2.440102	val: 3.139084	test: 3.617708

Epoch: 2
Loss: 10.938993453979492
RMSE train: 2.934237	val: 3.651551	test: 4.222351
MAE train: 2.317748	val: 3.155511	test: 3.765149

Epoch: 3
Loss: 10.330741882324219
RMSE train: 2.856428	val: 3.758108	test: 4.371642
MAE train: 2.259535	val: 3.275939	test: 3.962452

Epoch: 4
Loss: 9.433197657267252
RMSE train: 2.788899	val: 3.880147	test: 4.474104
MAE train: 2.228103	val: 3.446743	test: 4.099183

Epoch: 5
Loss: 8.626571496327719
RMSE train: 2.763037	val: 3.951430	test: 4.525515
MAE train: 2.230103	val: 3.541177	test: 4.172230

Epoch: 6
Loss: 7.988963762919108
RMSE train: 2.702010	val: 3.918416	test: 4.453011
MAE train: 2.194224	val: 3.514429	test: 4.104797

Epoch: 7
Loss: 7.39111868540446
RMSE train: 2.622394	val: 3.830905	test: 4.325503
MAE train: 2.136484	val: 3.427428	test: 3.971880

Epoch: 8
Loss: 6.939943154652913
RMSE train: 2.588482	val: 3.768172	test: 4.215349
MAE train: 2.144558	val: 3.361518	test: 3.853867

Epoch: 9
Loss: 6.477013111114502
RMSE train: 2.547121	val: 3.712398	test: 4.136775
MAE train: 2.130212	val: 3.304466	test: 3.772231

Epoch: 10
Loss: 5.891703764597575
RMSE train: 2.545081	val: 3.650285	test: 4.052473
MAE train: 2.157811	val: 3.243275	test: 3.687394

Epoch: 11
Loss: 5.3564833005269366
RMSE train: 2.494938	val: 3.575627	test: 3.959043
MAE train: 2.128110	val: 3.173616	test: 3.593358

Epoch: 12
Loss: 5.115268230438232
RMSE train: 2.451011	val: 3.504434	test: 3.870156
MAE train: 2.102019	val: 3.110475	test: 3.506784

Epoch: 13
Loss: 4.570894241333008
RMSE train: 2.393009	val: 3.452430	test: 3.805973
MAE train: 2.052029	val: 3.067835	test: 3.444358

Epoch: 14
Loss: 4.418949445088704
RMSE train: 2.366466	val: 3.370125	test: 3.694404
MAE train: 2.033329	val: 2.998689	test: 3.327574

Epoch: 15
Loss: 3.923264980316162
RMSE train: 2.327077	val: 3.302445	test: 3.601080
MAE train: 1.991202	val: 2.935107	test: 3.225768

Epoch: 16
Loss: 3.490814288457235
RMSE train: 2.184132	val: 3.155831	test: 3.425158
MAE train: 1.832488	val: 2.790908	test: 3.027165

Epoch: 17
Loss: 3.1413885752360025
RMSE train: 2.056715	val: 2.996830	test: 3.221287
MAE train: 1.719750	val: 2.637367	test: 2.805118

Epoch: 18
Loss: 2.853524605433146
RMSE train: 1.970356	val: 2.867475	test: 3.038238
MAE train: 1.658971	val: 2.502750	test: 2.609277

Epoch: 19
Loss: 2.615018685658773
RMSE train: 1.863797	val: 2.830333	test: 2.998138
MAE train: 1.564521	val: 2.455211	test: 2.567185

Epoch: 20
Loss: 2.3264936606089273
RMSE train: 1.731057	val: 2.737878	test: 2.893718
MAE train: 1.437461	val: 2.355848	test: 2.466426

Epoch: 21
Loss: 2.0966771841049194
RMSE train: 1.552512	val: 2.606731	test: 2.731020
MAE train: 1.261516	val: 2.214339	test: 2.303977

Epoch: 22
Loss: 1.847051461537679
RMSE train: 1.386401	val: 2.456723	test: 2.518840
MAE train: 1.106363	val: 2.043921	test: 2.105496

Epoch: 23
Loss: 1.6447073618570964Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/esol/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/esol/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/esol/scaff/train_prop=0.6/esol_scaff_1_20-05_14-43-15  ]
[ Using Seed :  1  ]
[ Using device :  cuda:0  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 11.95510991414388
RMSE train: 3.464802	val: 4.033064	test: 4.511199
MAE train: 2.893625	val: 3.585654	test: 4.078322

Epoch: 2
Loss: 10.70425001780192
RMSE train: 3.310351	val: 3.920083	test: 4.490544
MAE train: 2.754540	val: 3.467610	test: 4.081006

Epoch: 3
Loss: 9.984432220458984
RMSE train: 3.174096	val: 3.818149	test: 4.415072
MAE train: 2.631227	val: 3.353360	test: 4.015401

Epoch: 4
Loss: 9.215395132700602
RMSE train: 3.021660	val: 3.754693	test: 4.360712
MAE train: 2.479322	val: 3.296071	test: 3.967350

Epoch: 5
Loss: 8.528218905131022
RMSE train: 2.950155	val: 3.757109	test: 4.344760
MAE train: 2.423663	val: 3.335303	test: 3.971621

Epoch: 6
Loss: 7.958081245422363
RMSE train: 2.870515	val: 3.732275	test: 4.271562
MAE train: 2.383108	val: 3.330504	test: 3.910677

Epoch: 7
Loss: 7.3824334144592285
RMSE train: 2.791533	val: 3.660917	test: 4.134567
MAE train: 2.346401	val: 3.244086	test: 3.759331

Epoch: 8
Loss: 6.981526851654053
RMSE train: 2.707736	val: 3.585555	test: 3.999406
MAE train: 2.290778	val: 3.150041	test: 3.606023

Epoch: 9
Loss: 6.5056694348653155
RMSE train: 2.597597	val: 3.481709	test: 3.858780
MAE train: 2.194730	val: 3.043584	test: 3.457915

Epoch: 10
Loss: 5.985054969787598
RMSE train: 2.472809	val: 3.353591	test: 3.725544
MAE train: 2.071304	val: 2.920752	test: 3.323971

Epoch: 11
Loss: 5.458571592966716
RMSE train: 2.342803	val: 3.214373	test: 3.574108
MAE train: 1.941888	val: 2.788021	test: 3.162850

Epoch: 12
Loss: 4.918011983235677
RMSE train: 2.197939	val: 3.069867	test: 3.406133
MAE train: 1.798406	val: 2.653471	test: 2.978008

Epoch: 13
Loss: 4.548080285390218
RMSE train: 2.135414	val: 2.981838	test: 3.275402
MAE train: 1.758022	val: 2.573123	test: 2.833892

Epoch: 14
Loss: 4.284663677215576
RMSE train: 2.111972	val: 2.929835	test: 3.190712
MAE train: 1.758581	val: 2.519186	test: 2.745366

Epoch: 15
Loss: 3.86311936378479
RMSE train: 2.043554	val: 2.880426	test: 3.129573
MAE train: 1.695367	val: 2.467987	test: 2.686128

Epoch: 16
Loss: 3.671546777089437
RMSE train: 1.920269	val: 2.776133	test: 3.005177
MAE train: 1.575420	val: 2.371300	test: 2.557610

Epoch: 17
Loss: 3.2884109020233154
RMSE train: 1.780485	val: 2.625344	test: 2.807377
MAE train: 1.445266	val: 2.233060	test: 2.347231

Epoch: 18
Loss: 2.920795281728109
RMSE train: 1.669033	val: 2.524857	test: 2.671266
MAE train: 1.337911	val: 2.140204	test: 2.213520

Epoch: 19
Loss: 2.668074369430542
RMSE train: 1.631211	val: 2.469454	test: 2.610170
MAE train: 1.315624	val: 2.093256	test: 2.159623

Epoch: 20
Loss: 2.3354918162027993
RMSE train: 1.606025	val: 2.425024	test: 2.556108
MAE train: 1.307581	val: 2.046380	test: 2.112712

Epoch: 21
Loss: 2.0880608558654785
RMSE train: 1.555861	val: 2.318035	test: 2.413469
MAE train: 1.252464	val: 1.945187	test: 1.973364

Epoch: 22
Loss: 1.924291968345642
RMSE train: 1.395898	val: 2.210082	test: 2.307786
MAE train: 1.090869	val: 1.854481	test: 1.875959

Epoch: 23
Loss: 1.7885897556940715Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/esol/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/esol/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/esol/scaff/train_prop=0.7/esol_scaff_2_20-05_14-43-15  ]
[ Using Seed :  2  ]
[ Using device :  cuda:1  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 12.977683782577515
RMSE train: 3.112062	val: 3.549095	test: 4.260824
MAE train: 2.515388	val: 3.070200	test: 3.715305

Epoch: 2
Loss: 11.798686981201172
RMSE train: 3.083049	val: 3.612253	test: 4.331973
MAE train: 2.494807	val: 3.147093	test: 3.885223

Epoch: 3
Loss: 11.07446813583374
RMSE train: 3.117256	val: 3.767304	test: 4.418935
MAE train: 2.560514	val: 3.335540	test: 4.043936

Epoch: 4
Loss: 9.046691417694092
RMSE train: 2.962213	val: 3.654653	test: 4.255858
MAE train: 2.437006	val: 3.256623	test: 3.921428

Epoch: 5
Loss: 8.74613094329834
RMSE train: 2.804165	val: 3.529747	test: 4.076013
MAE train: 2.298375	val: 3.185713	test: 3.767377

Epoch: 6
Loss: 7.662146091461182
RMSE train: 2.694722	val: 3.434061	test: 3.936294
MAE train: 2.198509	val: 3.106630	test: 3.620549

Epoch: 7
Loss: 7.801474690437317
RMSE train: 2.582174	val: 3.190850	test: 3.667507
MAE train: 2.110566	val: 2.849166	test: 3.329269

Epoch: 8
Loss: 6.544246554374695
RMSE train: 2.576036	val: 3.011194	test: 3.460107
MAE train: 2.107034	val: 2.647875	test: 3.108004

Epoch: 9
Loss: 5.975438475608826
RMSE train: 2.530074	val: 3.083504	test: 3.504141
MAE train: 2.075222	val: 2.737576	test: 3.168521

Epoch: 10
Loss: 5.837163090705872
RMSE train: 2.433753	val: 3.120526	test: 3.537395
MAE train: 1.991311	val: 2.784291	test: 3.224293

Epoch: 11
Loss: 4.920264959335327
RMSE train: 2.344319	val: 3.048164	test: 3.442029
MAE train: 1.905948	val: 2.719080	test: 3.130076

Epoch: 12
Loss: 4.56799054145813
RMSE train: 2.142019	val: 2.828352	test: 3.224022
MAE train: 1.731233	val: 2.497943	test: 2.915476

Epoch: 13
Loss: 4.341863393783569
RMSE train: 2.058942	val: 2.725161	test: 3.122273
MAE train: 1.664017	val: 2.380153	test: 2.809426

Epoch: 14
Loss: 3.132737457752228
RMSE train: 1.991871	val: 2.612825	test: 2.966659
MAE train: 1.605560	val: 2.273261	test: 2.646350

Epoch: 15
Loss: 3.134204864501953
RMSE train: 1.947130	val: 2.482245	test: 2.798930
MAE train: 1.548190	val: 2.133806	test: 2.464662

Epoch: 16
Loss: 3.2269471287727356
RMSE train: 1.785907	val: 2.364847	test: 2.684656
MAE train: 1.394887	val: 2.024666	test: 2.356720

Epoch: 17
Loss: 2.6913811564445496
RMSE train: 1.655207	val: 2.293777	test: 2.616646
MAE train: 1.287217	val: 1.993446	test: 2.295070

Epoch: 18
Loss: 2.6904433965682983
RMSE train: 1.564624	val: 2.317097	test: 2.618780
MAE train: 1.228811	val: 2.032541	test: 2.300950

Epoch: 19
Loss: 2.077536702156067
RMSE train: 1.436162	val: 2.200390	test: 2.455520
MAE train: 1.105331	val: 1.908534	test: 2.130697

Epoch: 20
Loss: 1.778640240430832
RMSE train: 1.269593	val: 1.877648	test: 2.102993
MAE train: 0.953617	val: 1.586914	test: 1.773945

Epoch: 21
Loss: 1.7676431238651276
RMSE train: 1.170536	val: 1.643095	test: 1.826829
MAE train: 0.879591	val: 1.352328	test: 1.488474

Epoch: 22
Loss: 1.4634094834327698
RMSE train: 1.049965	val: 1.544562	test: 1.723676
MAE train: 0.772031	val: 1.246926	test: 1.361908

Epoch: 23
Loss: 1.8854933977127075Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/esol/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/esol/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/esol/scaff/train_prop=0.7/esol_scaff_3_20-05_14-43-15  ]
[ Using Seed :  3  ]
[ Using device :  cuda:1  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 12.265387296676636
RMSE train: 3.103022	val: 3.482218	test: 4.137332
MAE train: 2.505325	val: 3.007386	test: 3.613406

Epoch: 2
Loss: 10.83462905883789
RMSE train: 3.067253	val: 3.571140	test: 4.201339
MAE train: 2.480761	val: 3.105322	test: 3.759128

Epoch: 3
Loss: 8.758712887763977
RMSE train: 3.069318	val: 3.666416	test: 4.271284
MAE train: 2.494721	val: 3.207912	test: 3.886783

Epoch: 4
Loss: 8.657783031463623
RMSE train: 3.129241	val: 3.842847	test: 4.412387
MAE train: 2.582507	val: 3.405722	test: 4.076578

Epoch: 5
Loss: 7.20478355884552
RMSE train: 3.077188	val: 3.734288	test: 4.259832
MAE train: 2.569911	val: 3.320510	test: 3.935989

Epoch: 6
Loss: 8.612553238868713
RMSE train: 3.029543	val: 3.528957	test: 3.999337
MAE train: 2.565316	val: 3.117871	test: 3.678224

Epoch: 7
Loss: 6.887597322463989
RMSE train: 2.801001	val: 3.389369	test: 3.841303
MAE train: 2.349143	val: 3.002934	test: 3.543659

Epoch: 8
Loss: 6.0059932470321655
RMSE train: 2.593879	val: 3.315277	test: 3.741646
MAE train: 2.139897	val: 2.963305	test: 3.460971

Epoch: 9
Loss: 5.459209680557251
RMSE train: 2.525486	val: 3.226208	test: 3.649239
MAE train: 2.089898	val: 2.857781	test: 3.368068

Epoch: 10
Loss: 5.836593866348267
RMSE train: 2.482454	val: 3.113333	test: 3.517476
MAE train: 2.061710	val: 2.742401	test: 3.223434

Epoch: 11
Loss: 4.809159159660339
RMSE train: 2.387362	val: 2.893140	test: 3.236562
MAE train: 1.974003	val: 2.562236	test: 2.918757

Epoch: 12
Loss: 4.003578007221222
RMSE train: 2.220467	val: 2.779055	test: 3.145719
MAE train: 1.815026	val: 2.451570	test: 2.836402

Epoch: 13
Loss: 3.6287911534309387
RMSE train: 2.085673	val: 2.636366	test: 2.999929
MAE train: 1.677800	val: 2.312740	test: 2.685861

Epoch: 14
Loss: 2.93396058678627
RMSE train: 1.880409	val: 2.464274	test: 2.812522
MAE train: 1.480320	val: 2.156534	test: 2.501209

Epoch: 15
Loss: 2.9843254685401917
RMSE train: 1.769089	val: 2.355604	test: 2.707413
MAE train: 1.389119	val: 2.067097	test: 2.394828

Epoch: 16
Loss: 3.055065870285034
RMSE train: 1.724903	val: 2.354940	test: 2.689605
MAE train: 1.350988	val: 2.072524	test: 2.376442

Epoch: 17
Loss: 2.3286032378673553
RMSE train: 1.591986	val: 2.180507	test: 2.515638
MAE train: 1.233723	val: 1.870537	test: 2.193934

Epoch: 18
Loss: 2.8148745596408844
RMSE train: 1.473947	val: 1.914539	test: 2.193575
MAE train: 1.129419	val: 1.587043	test: 1.834555

Epoch: 19
Loss: 1.9452607333660126
RMSE train: 1.312406	val: 1.698581	test: 1.946848
MAE train: 0.993008	val: 1.380845	test: 1.606572

Epoch: 20
Loss: 2.852746933698654
RMSE train: 1.300195	val: 1.696594	test: 1.949888
MAE train: 0.993031	val: 1.384596	test: 1.625893

Epoch: 21
Loss: 1.4322707206010818
RMSE train: 1.189027	val: 1.725747	test: 1.981085
MAE train: 0.896320	val: 1.465844	test: 1.665527

Epoch: 22
Loss: 1.5071237087249756
RMSE train: 1.152922	val: 1.819187	test: 2.092576
MAE train: 0.866948	val: 1.563108	test: 1.754573

Epoch: 23
Loss: 1.145186185836792Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/esol/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/esol/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/esol/scaff/train_prop=0.7/esol_scaff_1_20-05_14-43-15  ]
[ Using Seed :  1  ]
[ Using device :  cuda:1  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 12.079454183578491
RMSE train: 3.431798	val: 3.836416	test: 4.499008
MAE train: 2.873168	val: 3.422576	test: 4.047761

Epoch: 2
Loss: 11.245049953460693
RMSE train: 3.305611	val: 3.811094	test: 4.466223
MAE train: 2.764467	val: 3.406949	test: 4.065324

Epoch: 3
Loss: 12.615082263946533
RMSE train: 3.168914	val: 3.756296	test: 4.397111
MAE train: 2.630589	val: 3.368735	test: 4.031579

Epoch: 4
Loss: 8.200433492660522
RMSE train: 3.039999	val: 3.738044	test: 4.339144
MAE train: 2.498234	val: 3.367907	test: 4.010944

Epoch: 5
Loss: 7.320366859436035
RMSE train: 2.927126	val: 3.721458	test: 4.284042
MAE train: 2.386958	val: 3.370586	test: 3.991625

Epoch: 6
Loss: 8.241998672485352
RMSE train: 2.789235	val: 3.598013	test: 4.116822
MAE train: 2.276091	val: 3.245252	test: 3.825191

Epoch: 7
Loss: 6.053184807300568
RMSE train: 2.617234	val: 3.375317	test: 3.850886
MAE train: 2.137990	val: 3.021770	test: 3.547987

Epoch: 8
Loss: 7.158761382102966
RMSE train: 2.486250	val: 3.267952	test: 3.719069
MAE train: 2.027487	val: 2.935772	test: 3.421778

Epoch: 9
Loss: 5.746092438697815
RMSE train: 2.370775	val: 3.144832	test: 3.581923
MAE train: 1.924763	val: 2.819774	test: 3.283896

Epoch: 10
Loss: 5.519223093986511
RMSE train: 2.247121	val: 3.022758	test: 3.452419
MAE train: 1.812799	val: 2.689227	test: 3.153033

Epoch: 11
Loss: 4.513135731220245
RMSE train: 2.133805	val: 2.892978	test: 3.326937
MAE train: 1.715325	val: 2.554934	test: 3.016303

Epoch: 12
Loss: 4.066709220409393
RMSE train: 2.022581	val: 2.717216	test: 3.110754
MAE train: 1.626315	val: 2.389271	test: 2.777680

Epoch: 13
Loss: 3.6650807857513428
RMSE train: 1.898097	val: 2.565828	test: 2.863966
MAE train: 1.518689	val: 2.247271	test: 2.512764

Epoch: 14
Loss: 4.999625742435455
RMSE train: 1.823380	val: 2.550915	test: 2.785117
MAE train: 1.454181	val: 2.232968	test: 2.432546

Epoch: 15
Loss: 3.2683833241462708
RMSE train: 1.725269	val: 2.551161	test: 2.799415
MAE train: 1.360393	val: 2.235393	test: 2.469084

Epoch: 16
Loss: 3.0660915970802307
RMSE train: 1.618989	val: 2.398225	test: 2.605529
MAE train: 1.244793	val: 2.069196	test: 2.271628

Epoch: 17
Loss: 2.5951815843582153
RMSE train: 1.575770	val: 2.268749	test: 2.452350
MAE train: 1.214123	val: 1.925800	test: 2.106730

Epoch: 18
Loss: 2.524908721446991
RMSE train: 1.460449	val: 2.245635	test: 2.406741
MAE train: 1.109631	val: 1.906748	test: 2.015255

Epoch: 19
Loss: 2.550309121608734
RMSE train: 1.297823	val: 2.084539	test: 2.263159
MAE train: 0.959165	val: 1.742836	test: 1.850467

Epoch: 20
Loss: 1.8399644196033478
RMSE train: 1.178645	val: 1.845188	test: 1.995473
MAE train: 0.873016	val: 1.551817	test: 1.605966

Epoch: 21
Loss: 1.708033710718155
RMSE train: 1.118629	val: 1.691176	test: 1.826721
MAE train: 0.843375	val: 1.386071	test: 1.467846

Epoch: 22
Loss: 1.380142942070961
RMSE train: 1.213966	val: 1.868335	test: 1.976436
MAE train: 0.952141	val: 1.531574	test: 1.611583

Epoch: 23
Loss: 1.2649723142385483Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/esol/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/esol/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/esol/scaff/train_prop=0.8/esol_scaff_3_20-05_14-43-15  ]
[ Using Seed :  3  ]
[ Using device :  cuda:2  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 12.03889799118042
RMSE train: 3.111992	val: 3.906978	test: 4.069940
MAE train: 2.526388	val: 3.446975	test: 3.535714

Epoch: 2
Loss: 10.54490351676941
RMSE train: 3.092113	val: 4.092198	test: 4.174267
MAE train: 2.523800	val: 3.736925	test: 3.752688

Epoch: 3
Loss: 9.47701621055603
RMSE train: 2.962462	val: 4.049394	test: 4.109441
MAE train: 2.412845	val: 3.739339	test: 3.741618

Epoch: 4
Loss: 8.658843994140625
RMSE train: 2.810449	val: 3.794817	test: 3.905951
MAE train: 2.298273	val: 3.487314	test: 3.545548

Epoch: 5
Loss: 7.864168047904968
RMSE train: 2.785506	val: 3.682967	test: 3.897536
MAE train: 2.312097	val: 3.373993	test: 3.563525

Epoch: 6
Loss: 7.234266400337219
RMSE train: 2.767227	val: 3.643379	test: 3.899381
MAE train: 2.336082	val: 3.346894	test: 3.573477

Epoch: 7
Loss: 6.376073002815247
RMSE train: 2.647575	val: 3.528339	test: 3.771470
MAE train: 2.226332	val: 3.243940	test: 3.445633

Epoch: 8
Loss: 5.9618319272994995
RMSE train: 2.522473	val: 3.412686	test: 3.619222
MAE train: 2.111332	val: 3.142921	test: 3.294458

Epoch: 9
Loss: 5.379524111747742
RMSE train: 2.448627	val: 3.364310	test: 3.574995
MAE train: 2.053992	val: 3.089400	test: 3.275331

Epoch: 10
Loss: 4.751945734024048
RMSE train: 2.405816	val: 3.299989	test: 3.513199
MAE train: 2.032327	val: 3.025960	test: 3.220050

Epoch: 11
Loss: 4.272144198417664
RMSE train: 2.230803	val: 3.040266	test: 3.233319
MAE train: 1.866088	val: 2.752916	test: 2.916313

Epoch: 12
Loss: 3.7480133771896362
RMSE train: 2.020260	val: 2.691088	test: 2.863096
MAE train: 1.666364	val: 2.417728	test: 2.520310

Epoch: 13
Loss: 3.3316845297813416
RMSE train: 1.909671	val: 2.660388	test: 2.798872
MAE train: 1.577899	val: 2.387086	test: 2.479651

Epoch: 14
Loss: 2.82545268535614
RMSE train: 1.671491	val: 2.474326	test: 2.555839
MAE train: 1.347309	val: 2.190898	test: 2.248333

Epoch: 15
Loss: 2.5825300812721252
RMSE train: 1.411763	val: 2.095174	test: 2.147477
MAE train: 1.094542	val: 1.807357	test: 1.818464

Epoch: 16
Loss: 2.080913484096527
RMSE train: 1.381342	val: 2.051971	test: 2.126729
MAE train: 1.062726	val: 1.778433	test: 1.809674

Epoch: 17
Loss: 1.8046149909496307
RMSE train: 1.323564	val: 1.900242	test: 2.043268
MAE train: 1.017026	val: 1.646194	test: 1.707719

Epoch: 18
Loss: 1.6068736016750336
RMSE train: 1.137680	val: 1.780105	test: 1.900880
MAE train: 0.857992	val: 1.500429	test: 1.552393

Epoch: 19
Loss: 1.4613588452339172
RMSE train: 0.914120	val: 1.429481	test: 1.537055
MAE train: 0.676150	val: 1.165759	test: 1.200485

Epoch: 20
Loss: 1.1561135202646255
RMSE train: 0.922451	val: 1.351460	test: 1.445618
MAE train: 0.707550	val: 1.076595	test: 1.133697

Epoch: 21
Loss: 1.1007646322250366
RMSE train: 0.791115	val: 1.413888	test: 1.395684
MAE train: 0.582445	val: 1.145564	test: 1.077114

Epoch: 22
Loss: 1.0390777587890625
RMSE train: 0.814119	val: 1.405449	test: 1.401830
MAE train: 0.606253	val: 1.142098	test: 1.103625

Epoch: 23
Loss: 1.0065964758396149Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/esol/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/esol/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/esol/scaff/train_prop=0.8/esol_scaff_2_20-05_14-43-15  ]
[ Using Seed :  2  ]
[ Using device :  cuda:2  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 12.423888683319092
RMSE train: 3.200147	val: 4.077549	test: 4.246868
MAE train: 2.602615	val: 3.580737	test: 3.729465

Epoch: 2
Loss: 11.281179428100586
RMSE train: 3.136699	val: 4.185893	test: 4.320796
MAE train: 2.559499	val: 3.787300	test: 3.911840

Epoch: 3
Loss: 10.12462568283081
RMSE train: 3.128561	val: 4.219082	test: 4.391402
MAE train: 2.569079	val: 3.882768	test: 4.048344

Epoch: 4
Loss: 9.327015399932861
RMSE train: 3.149796	val: 4.238393	test: 4.475745
MAE train: 2.645256	val: 3.950529	test: 4.185270

Epoch: 5
Loss: 8.40995991230011
RMSE train: 2.963167	val: 4.062113	test: 4.367763
MAE train: 2.458210	val: 3.778086	test: 4.077535

Epoch: 6
Loss: 7.724686861038208
RMSE train: 2.809138	val: 3.837794	test: 4.170146
MAE train: 2.318469	val: 3.542477	test: 3.853283

Epoch: 7
Loss: 6.9101656675338745
RMSE train: 2.624549	val: 3.565971	test: 3.887176
MAE train: 2.155384	val: 3.250621	test: 3.557977

Epoch: 8
Loss: 6.1237804889678955
RMSE train: 2.434756	val: 3.319962	test: 3.597410
MAE train: 1.986851	val: 3.001941	test: 3.261352

Epoch: 9
Loss: 5.511314511299133
RMSE train: 2.309507	val: 3.208472	test: 3.479105
MAE train: 1.881952	val: 2.908785	test: 3.148225

Epoch: 10
Loss: 5.163831949234009
RMSE train: 2.211914	val: 3.106119	test: 3.354288
MAE train: 1.809415	val: 2.823339	test: 3.020501

Epoch: 11
Loss: 4.369029641151428
RMSE train: 2.145669	val: 2.948291	test: 3.195669
MAE train: 1.779834	val: 2.663440	test: 2.860523

Epoch: 12
Loss: 4.045438170433044
RMSE train: 2.071615	val: 2.750273	test: 2.997755
MAE train: 1.742396	val: 2.445421	test: 2.659365

Epoch: 13
Loss: 3.5868342518806458
RMSE train: 1.927724	val: 2.613152	test: 2.864831
MAE train: 1.597295	val: 2.291452	test: 2.541081

Epoch: 14
Loss: 3.097405791282654
RMSE train: 1.806077	val: 2.454862	test: 2.682556
MAE train: 1.489842	val: 2.155049	test: 2.336714

Epoch: 15
Loss: 2.740137279033661
RMSE train: 1.741311	val: 2.457021	test: 2.607255
MAE train: 1.414935	val: 2.141844	test: 2.264552

Epoch: 16
Loss: 2.3132266998291016
RMSE train: 1.603493	val: 2.327310	test: 2.460547
MAE train: 1.267279	val: 1.980214	test: 2.082438

Epoch: 17
Loss: 1.9905171990394592
RMSE train: 1.421882	val: 2.028675	test: 2.160410
MAE train: 1.125817	val: 1.695390	test: 1.794489

Epoch: 18
Loss: 1.8212014734745026
RMSE train: 1.287421	val: 1.828307	test: 1.954161
MAE train: 1.007410	val: 1.486258	test: 1.583402

Epoch: 19
Loss: 1.4680885076522827
RMSE train: 1.174978	val: 1.711995	test: 1.839099
MAE train: 0.917911	val: 1.380133	test: 1.485846

Epoch: 20
Loss: 1.3572374284267426
RMSE train: 1.076925	val: 1.613649	test: 1.710517
MAE train: 0.836856	val: 1.295875	test: 1.385852

Epoch: 21
Loss: 1.2039707601070404
RMSE train: 1.042573	val: 1.618711	test: 1.694615
MAE train: 0.809818	val: 1.289409	test: 1.381640

Epoch: 22
Loss: 1.1409716308116913
RMSE train: 0.893328	val: 1.390940	test: 1.487502
MAE train: 0.682131	val: 1.092612	test: 1.191759

Epoch: 23
Loss: 1.009772464632988Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/esol/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/esol/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/esol/scaff/train_prop=0.8/esol_scaff_1_20-05_14-43-15  ]
[ Using Seed :  1  ]
[ Using device :  cuda:2  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 12.242236137390137
RMSE train: 3.288486	val: 4.214312	test: 4.286156
MAE train: 2.716998	val: 3.821832	test: 3.781340

Epoch: 2
Loss: 11.370502233505249
RMSE train: 3.127516	val: 4.161803	test: 4.235049
MAE train: 2.571803	val: 3.812472	test: 3.800143

Epoch: 3
Loss: 9.923600673675537
RMSE train: 3.065294	val: 4.276682	test: 4.325235
MAE train: 2.510571	val: 3.977342	test: 3.957524

Epoch: 4
Loss: 8.955033898353577
RMSE train: 3.018002	val: 4.322757	test: 4.374833
MAE train: 2.479106	val: 4.062494	test: 4.049367

Epoch: 5
Loss: 8.282372117042542
RMSE train: 2.893583	val: 4.135558	test: 4.249418
MAE train: 2.379541	val: 3.867104	test: 3.931855

Epoch: 6
Loss: 7.5460251569747925
RMSE train: 2.771540	val: 3.830582	test: 4.036200
MAE train: 2.305643	val: 3.541225	test: 3.722950

Epoch: 7
Loss: 7.087041139602661
RMSE train: 2.688349	val: 3.626298	test: 3.861055
MAE train: 2.258786	val: 3.335339	test: 3.539111

Epoch: 8
Loss: 6.394812107086182
RMSE train: 2.529549	val: 3.451801	test: 3.649623
MAE train: 2.095660	val: 3.173680	test: 3.313620

Epoch: 9
Loss: 5.608577251434326
RMSE train: 2.430926	val: 3.370787	test: 3.503993
MAE train: 2.001181	val: 3.098498	test: 3.172117

Epoch: 10
Loss: 5.0491825342178345
RMSE train: 2.392786	val: 3.363108	test: 3.517251
MAE train: 1.985553	val: 3.090520	test: 3.226215

Epoch: 11
Loss: 4.552121043205261
RMSE train: 2.121137	val: 2.951172	test: 3.083923
MAE train: 1.740419	val: 2.662467	test: 2.757620

Epoch: 12
Loss: 4.211354076862335
RMSE train: 1.952031	val: 2.687834	test: 2.821462
MAE train: 1.584201	val: 2.376509	test: 2.484724

Epoch: 13
Loss: 3.7099573612213135
RMSE train: 1.816411	val: 2.587866	test: 2.744963
MAE train: 1.451530	val: 2.285899	test: 2.438263

Epoch: 14
Loss: 3.1936196088790894
RMSE train: 1.634603	val: 2.348416	test: 2.487335
MAE train: 1.275878	val: 2.029192	test: 2.160467

Epoch: 15
Loss: 2.786067485809326
RMSE train: 1.334674	val: 1.897614	test: 2.045372
MAE train: 0.987698	val: 1.579173	test: 1.670721

Epoch: 16
Loss: 2.4294344186782837
RMSE train: 1.223473	val: 1.817449	test: 2.009922
MAE train: 0.902807	val: 1.506175	test: 1.657758

Epoch: 17
Loss: 2.03384730219841
RMSE train: 1.223376	val: 1.830933	test: 2.027823
MAE train: 0.916195	val: 1.521897	test: 1.690422

Epoch: 18
Loss: 1.874608725309372
RMSE train: 1.053623	val: 1.482236	test: 1.717772
MAE train: 0.763562	val: 1.225116	test: 1.358577

Epoch: 19
Loss: 1.601664960384369
RMSE train: 0.951238	val: 1.426693	test: 1.644895
MAE train: 0.683661	val: 1.155500	test: 1.308523

Epoch: 20
Loss: 1.3904750049114227
RMSE train: 0.921104	val: 1.474666	test: 1.672517
MAE train: 0.669619	val: 1.186605	test: 1.351995

Epoch: 21
Loss: 1.3729394376277924
RMSE train: 0.792303	val: 1.308413	test: 1.421630
MAE train: 0.595344	val: 1.037913	test: 1.123320

Epoch: 22
Loss: 1.2415214776992798
RMSE train: 0.771646	val: 1.231553	test: 1.269660
MAE train: 0.589797	val: 0.988080	test: 0.998182

Epoch: 23
Loss: 1.1006413698196411
RMSE train: 1.173671	val: 2.112865	test: 2.137181
MAE train: 0.914832	val: 1.810547	test: 1.724591

Epoch: 24
Loss: 1.3476388851801555
RMSE train: 1.074611	val: 2.029190	test: 2.059381
MAE train: 0.823041	val: 1.726310	test: 1.659964

Epoch: 25
Loss: 1.314121127128601
RMSE train: 1.051822	val: 1.991134	test: 2.036676
MAE train: 0.805254	val: 1.691535	test: 1.652241

Epoch: 26
Loss: 1.233002503712972
RMSE train: 0.991055	val: 1.940812	test: 1.965983
MAE train: 0.742111	val: 1.643243	test: 1.601944

Epoch: 27
Loss: 1.063199798266093
RMSE train: 0.925572	val: 1.831282	test: 1.822764
MAE train: 0.685888	val: 1.547555	test: 1.477244

Epoch: 28
Loss: 0.9501768350601196
RMSE train: 0.840919	val: 1.755350	test: 1.725355
MAE train: 0.623535	val: 1.471492	test: 1.385495

Epoch: 29
Loss: 0.812005897363027
RMSE train: 0.787368	val: 1.761865	test: 1.752817
MAE train: 0.590914	val: 1.472000	test: 1.402724

Epoch: 30
Loss: 0.8503681421279907
RMSE train: 0.782936	val: 1.773780	test: 1.757849
MAE train: 0.592957	val: 1.496527	test: 1.411325

Epoch: 31
Loss: 0.7902478575706482
RMSE train: 0.742249	val: 1.739545	test: 1.728492
MAE train: 0.554416	val: 1.475330	test: 1.384380

Epoch: 32
Loss: 0.6751342415809631
RMSE train: 0.685695	val: 1.695510	test: 1.660018
MAE train: 0.505715	val: 1.433654	test: 1.323112

Epoch: 33
Loss: 0.7688456376393636
RMSE train: 0.678970	val: 1.680111	test: 1.566555
MAE train: 0.501661	val: 1.425171	test: 1.270131

Epoch: 34
Loss: 0.6698303620020548
RMSE train: 0.697322	val: 1.661371	test: 1.510738
MAE train: 0.520061	val: 1.393312	test: 1.214380

Epoch: 35
Loss: 0.6259944041570028
RMSE train: 0.677722	val: 1.656795	test: 1.510962
MAE train: 0.496354	val: 1.375003	test: 1.204202

Epoch: 36
Loss: 0.6963829000790914
RMSE train: 0.716738	val: 1.683119	test: 1.524411
MAE train: 0.523234	val: 1.404327	test: 1.219507

Epoch: 37
Loss: 0.5669771830240885
RMSE train: 0.799357	val: 1.690765	test: 1.509911
MAE train: 0.596845	val: 1.409411	test: 1.211275

Epoch: 38
Loss: 0.6206359267234802
RMSE train: 0.708850	val: 1.704398	test: 1.566606
MAE train: 0.537699	val: 1.425287	test: 1.227148

Epoch: 39
Loss: 0.6934329867362976
RMSE train: 0.741833	val: 1.716624	test: 1.628562
MAE train: 0.570447	val: 1.430826	test: 1.265331

Epoch: 40
Loss: 0.5798173944155375
RMSE train: 0.704119	val: 1.684404	test: 1.586162
MAE train: 0.530537	val: 1.401560	test: 1.242521

Epoch: 41
Loss: 0.594140887260437
RMSE train: 0.680149	val: 1.662404	test: 1.534337
MAE train: 0.513879	val: 1.391148	test: 1.213626

Epoch: 42
Loss: 0.5651985208193461
RMSE train: 0.652639	val: 1.653967	test: 1.521865
MAE train: 0.503104	val: 1.378358	test: 1.218234

Epoch: 43
Loss: 0.6149301330248514
RMSE train: 0.646399	val: 1.696003	test: 1.587650
MAE train: 0.506684	val: 1.402712	test: 1.252836

Epoch: 44
Loss: 0.551609734694163
RMSE train: 0.696875	val: 1.727555	test: 1.622249
MAE train: 0.547351	val: 1.433976	test: 1.263330

Epoch: 45
Loss: 0.5119027892748514
RMSE train: 0.690077	val: 1.662542	test: 1.558298
MAE train: 0.532827	val: 1.384665	test: 1.218095

Epoch: 46
Loss: 0.6437985499699911
RMSE train: 0.627203	val: 1.610383	test: 1.480420
MAE train: 0.475286	val: 1.337679	test: 1.177832

Epoch: 47
Loss: 0.6040838956832886
RMSE train: 0.541811	val: 1.672488	test: 1.556087
MAE train: 0.410429	val: 1.377342	test: 1.245976

Epoch: 48
Loss: 0.529254138469696
RMSE train: 0.586431	val: 1.661398	test: 1.502280
MAE train: 0.439564	val: 1.393984	test: 1.214969

Epoch: 49
Loss: 0.5505972901980082
RMSE train: 0.624191	val: 1.625845	test: 1.460369
MAE train: 0.462966	val: 1.374611	test: 1.188969

Epoch: 50
Loss: 0.5466187000274658
RMSE train: 0.601517	val: 1.647298	test: 1.517572
MAE train: 0.453837	val: 1.383370	test: 1.216501

Epoch: 51
Loss: 0.50885009765625
RMSE train: 0.608989	val: 1.705439	test: 1.580169
MAE train: 0.472252	val: 1.426763	test: 1.244184

Epoch: 52
Loss: 0.6009878317515055
RMSE train: 0.619663	val: 1.702712	test: 1.547839
MAE train: 0.465854	val: 1.438285	test: 1.221879

Epoch: 53
Loss: 0.5469773014386495
RMSE train: 0.632632	val: 1.709987	test: 1.538339
MAE train: 0.469157	val: 1.444708	test: 1.217896

Epoch: 54
Loss: 0.5370745956897736
RMSE train: 0.627467	val: 1.706860	test: 1.550613
MAE train: 0.466991	val: 1.430808	test: 1.233050

Epoch: 55
Loss: 0.49108825127283734
RMSE train: 0.574282	val: 1.686489	test: 1.543462
MAE train: 0.429801	val: 1.412063	test: 1.237540

Epoch: 56
Loss: 0.4422744909922282
RMSE train: 0.536382	val: 1.704817	test: 1.575647
MAE train: 0.404578	val: 1.411017	test: 1.263603

Epoch: 57
Loss: 0.48466113209724426
RMSE train: 0.536760	val: 1.694898	test: 1.572777
MAE train: 0.400047	val: 1.403819	test: 1.256195

Epoch: 58
Loss: 0.4168507754802704
RMSE train: 0.550346	val: 1.671040	test: 1.566810
MAE train: 0.411311	val: 1.383626	test: 1.247238

Epoch: 59
Loss: 0.4704607327779134
RMSE train: 0.577810	val: 1.657232	test: 1.603776
MAE train: 0.432825	val: 1.363647	test: 1.261398

Epoch: 60
Loss: 0.46078357100486755
RMSE train: 0.596451	val: 1.623301	test: 1.591926
MAE train: 0.450571	val: 1.334945	test: 1.257202

Epoch: 61
Loss: 0.4951846202214559
RMSE train: 0.574150	val: 1.592701	test: 1.517784
MAE train: 0.430578	val: 1.322763	test: 1.224287

Epoch: 62
Loss: 0.45166879892349243
RMSE train: 0.586269	val: 1.595936	test: 1.510575
MAE train: 0.439510	val: 1.330530	test: 1.211953

Epoch: 63
Loss: 0.46604838967323303
RMSE train: 0.533524	val: 1.602749	test: 1.550055
MAE train: 0.399677	val: 1.333783	test: 1.231516

Epoch: 64
Loss: 0.5104527572790781
RMSE train: 0.539801	val: 1.610131	test: 1.600329
MAE train: 0.411874	val: 1.318576	test: 1.251251

Epoch: 65
Loss: 0.5223081509272257
RMSE train: 0.563272	val: 1.564124	test: 1.536327
MAE train: 0.427854	val: 1.281080	test: 1.210238

Epoch: 66
Loss: 0.4852462112903595
RMSE train: 0.552092	val: 1.573014	test: 1.514460
MAE train: 0.413325	val: 1.303891	test: 1.200894

Epoch: 67
Loss: 0.3883617917696635
RMSE train: 0.585237	val: 1.601300	test: 1.547670
MAE train: 0.431961	val: 1.344280	test: 1.226202

Epoch: 68
Loss: 0.36279357473055523
RMSE train: 0.594449	val: 1.670468	test: 1.594613
MAE train: 0.437164	val: 1.405170	test: 1.260418

Epoch: 69
Loss: 0.4300983150800069
RMSE train: 0.563481	val: 1.630540	test: 1.548625
MAE train: 0.409917	val: 1.377097	test: 1.230346

Epoch: 70
Loss: 0.40429457028706867
RMSE train: 0.548680	val: 1.613860	test: 1.539117
MAE train: 0.407829	val: 1.346930	test: 1.211684

Epoch: 71
Loss: 0.4754830499490102
RMSE train: 0.558530	val: 1.688715	test: 1.573354
MAE train: 0.420378	val: 1.386627	test: 1.214597

Epoch: 72
Loss: 0.4001689354578654
RMSE train: 0.557331	val: 1.736573	test: 1.583777
MAE train: 0.407513	val: 1.412420	test: 1.236237

Epoch: 73
Loss: 0.4778035581111908
RMSE train: 0.521408	val: 1.599001	test: 1.519061
MAE train: 0.382083	val: 1.335696	test: 1.197513

Epoch: 74
Loss: 0.3906491796175639
RMSE train: 0.531507	val: 1.527561	test: 1.518652
MAE train: 0.394363	val: 1.269914	test: 1.205933

Epoch: 75
Loss: 0.37554537256558734
RMSE train: 0.491141	val: 1.508711	test: 1.533027
MAE train: 0.367364	val: 1.229093	test: 1.224596

Epoch: 76
Loss: 0.44152942299842834
RMSE train: 0.499439	val: 1.513956	test: 1.546345
MAE train: 0.378643	val: 1.218561	test: 1.235539

Epoch: 77
Loss: 0.39710745215415955
RMSE train: 0.497300	val: 1.515136	test: 1.539083
MAE train: 0.375732	val: 1.217374	test: 1.230357

Epoch: 78
Loss: 0.36073845624923706
RMSE train: 0.500444	val: 1.498430	test: 1.490168
MAE train: 0.369342	val: 1.213415	test: 1.198319

Epoch: 79
Loss: 0.3604326645533244
RMSE train: 0.492311	val: 1.510437	test: 1.467126
MAE train: 0.362262	val: 1.223857	test: 1.188458

Epoch: 80
Loss: 0.44888126850128174
RMSE train: 0.525858	val: 1.540066	test: 1.438848
MAE train: 0.383791	val: 1.245231	test: 1.165674

Epoch: 81
Loss: 0.3715011576811473
RMSE train: 0.504778	val: 1.541687	test: 1.501818
MAE train: 0.376842	val: 1.251602	test: 1.186627

Epoch: 82
Loss: 0.38120315472284955
RMSE train: 0.515315	val: 1.562272	test: 1.505717
MAE train: 0.389090	val: 1.266768	test: 1.173918

Epoch: 83
Loss: 0.4023652970790863
RMSE train: 0.494518	val: 1.558427	test: 1.504291
MAE train: 0.374993	val: 1.266694	test: 1.187380
RMSE train: 1.260116	val: 2.329528	test: 2.332754
MAE train: 0.980142	val: 1.920156	test: 1.937679

Epoch: 24
Loss: 1.463684876759847
RMSE train: 1.176613	val: 2.252591	test: 2.220997
MAE train: 0.896116	val: 1.848438	test: 1.835673

Epoch: 25
Loss: 1.4580616156260173
RMSE train: 1.060283	val: 2.129613	test: 2.037945
MAE train: 0.801082	val: 1.753874	test: 1.640202

Epoch: 26
Loss: 1.2853216330210369
RMSE train: 0.978029	val: 2.082865	test: 1.954527
MAE train: 0.721983	val: 1.714298	test: 1.564214

Epoch: 27
Loss: 1.123759110768636
RMSE train: 0.890509	val: 2.073399	test: 1.917801
MAE train: 0.648840	val: 1.700202	test: 1.542452

Epoch: 28
Loss: 1.054435710112254
RMSE train: 0.853692	val: 2.049709	test: 1.912728
MAE train: 0.627324	val: 1.674874	test: 1.530303

Epoch: 29
Loss: 0.8475185831387838
RMSE train: 0.859462	val: 2.025996	test: 1.900581
MAE train: 0.636145	val: 1.658617	test: 1.518814

Epoch: 30
Loss: 0.8203778664271036
RMSE train: 0.891488	val: 2.054058	test: 1.921464
MAE train: 0.670150	val: 1.696561	test: 1.544340

Epoch: 31
Loss: 0.9092970490455627
RMSE train: 0.922134	val: 2.068495	test: 1.931828
MAE train: 0.703634	val: 1.703849	test: 1.570374

Epoch: 32
Loss: 0.7465433279673258
RMSE train: 0.915940	val: 2.040836	test: 1.893569
MAE train: 0.690274	val: 1.675795	test: 1.523141

Epoch: 33
Loss: 0.7515087525049845
RMSE train: 0.849857	val: 2.030812	test: 1.855565
MAE train: 0.627128	val: 1.672198	test: 1.481286

Epoch: 34
Loss: 0.6258899370829264
RMSE train: 0.753458	val: 2.022451	test: 1.797694
MAE train: 0.552125	val: 1.664968	test: 1.424229

Epoch: 35
Loss: 0.74203888575236
RMSE train: 0.713174	val: 2.015133	test: 1.771345
MAE train: 0.528687	val: 1.656874	test: 1.399928

Epoch: 36
Loss: 0.6928109725316366
RMSE train: 0.735656	val: 2.013366	test: 1.823848
MAE train: 0.552532	val: 1.662366	test: 1.436638

Epoch: 37
Loss: 0.7427003781000773
RMSE train: 0.718641	val: 2.017908	test: 1.824051
MAE train: 0.547858	val: 1.665076	test: 1.442008

Epoch: 38
Loss: 0.6267593701680502
RMSE train: 0.662283	val: 2.003746	test: 1.800177
MAE train: 0.504342	val: 1.642171	test: 1.406006

Epoch: 39
Loss: 0.6253555019696554
RMSE train: 0.622731	val: 1.982469	test: 1.804911
MAE train: 0.470953	val: 1.615295	test: 1.402550

Epoch: 40
Loss: 0.6321268479029337
RMSE train: 0.617084	val: 1.922913	test: 1.769267
MAE train: 0.455340	val: 1.578780	test: 1.381779

Epoch: 41
Loss: 0.5811638832092285
RMSE train: 0.604703	val: 1.869076	test: 1.722495
MAE train: 0.444519	val: 1.540791	test: 1.350717

Epoch: 42
Loss: 0.5800129969914755
RMSE train: 0.601110	val: 1.868052	test: 1.701692
MAE train: 0.446411	val: 1.535429	test: 1.338766

Epoch: 43
Loss: 0.5909401575724283
RMSE train: 0.568463	val: 1.881414	test: 1.696253
MAE train: 0.427921	val: 1.535763	test: 1.323690

Epoch: 44
Loss: 0.5335168043772379
RMSE train: 0.569420	val: 1.920886	test: 1.729185
MAE train: 0.425778	val: 1.553449	test: 1.331351

Epoch: 45
Loss: 0.5228454073270162
RMSE train: 0.636438	val: 1.933281	test: 1.755098
MAE train: 0.475641	val: 1.558461	test: 1.349094

Epoch: 46
Loss: 0.5854885578155518
RMSE train: 0.652342	val: 1.901773	test: 1.739781
MAE train: 0.478621	val: 1.542949	test: 1.347032

Epoch: 47
Loss: 0.5766334136327108
RMSE train: 0.642225	val: 1.869766	test: 1.708204
MAE train: 0.473504	val: 1.513631	test: 1.323331

Epoch: 48
Loss: 0.549369215965271
RMSE train: 0.655087	val: 1.850149	test: 1.711821
MAE train: 0.476042	val: 1.488455	test: 1.314204

Epoch: 49
Loss: 0.5155789653460184
RMSE train: 0.647827	val: 1.860013	test: 1.706512
MAE train: 0.470070	val: 1.491694	test: 1.299233

Epoch: 50
Loss: 0.561260998249054
RMSE train: 0.674311	val: 1.881228	test: 1.690077
MAE train: 0.492066	val: 1.511097	test: 1.312580

Epoch: 51
Loss: 0.5069470902283987
RMSE train: 0.721916	val: 1.844739	test: 1.685393
MAE train: 0.519189	val: 1.477165	test: 1.300413

Epoch: 52
Loss: 0.6379771927992502
RMSE train: 0.652610	val: 1.839549	test: 1.687035
MAE train: 0.478294	val: 1.462970	test: 1.289142

Epoch: 53
Loss: 0.5149220029513041
RMSE train: 0.571437	val: 1.867758	test: 1.652997
MAE train: 0.411466	val: 1.492082	test: 1.272987

Epoch: 54
Loss: 0.4809282720088959
RMSE train: 0.623859	val: 1.915478	test: 1.690130
MAE train: 0.450716	val: 1.542287	test: 1.343717

Epoch: 55
Loss: 0.5502283175786337
RMSE train: 0.660654	val: 1.895311	test: 1.729630
MAE train: 0.483969	val: 1.523997	test: 1.358823

Epoch: 56
Loss: 0.4611588219801585
RMSE train: 0.740707	val: 1.853645	test: 1.680609
MAE train: 0.525643	val: 1.461704	test: 1.275584

Epoch: 57
Loss: 0.5240733126799265
RMSE train: 0.628520	val: 1.820925	test: 1.654314
MAE train: 0.452759	val: 1.434319	test: 1.260226

Epoch: 58
Loss: 0.4970828394095103
RMSE train: 0.602555	val: 1.850812	test: 1.692156
MAE train: 0.433173	val: 1.476052	test: 1.344439

Epoch: 59
Loss: 0.46387208501497906
RMSE train: 0.539911	val: 1.823105	test: 1.613585
MAE train: 0.383150	val: 1.436524	test: 1.252935

Epoch: 60
Loss: 0.4581296344598134
RMSE train: 0.552419	val: 1.803095	test: 1.624488
MAE train: 0.398016	val: 1.412279	test: 1.229729

Epoch: 61
Loss: 0.4522160490353902
RMSE train: 0.610448	val: 1.808918	test: 1.657636
MAE train: 0.451164	val: 1.427273	test: 1.265422

Epoch: 62
Loss: 0.4560873508453369
RMSE train: 0.623124	val: 1.826346	test: 1.693574
MAE train: 0.470840	val: 1.463394	test: 1.314438

Epoch: 63
Loss: 0.42108407616615295
RMSE train: 0.600435	val: 1.806520	test: 1.652613
MAE train: 0.448092	val: 1.449483	test: 1.264778

Epoch: 64
Loss: 0.4754105707009633
RMSE train: 0.668473	val: 1.825545	test: 1.668192
MAE train: 0.490259	val: 1.478010	test: 1.283960

Epoch: 65
Loss: 0.4963754018147786
RMSE train: 0.737610	val: 1.835565	test: 1.690020
MAE train: 0.538009	val: 1.491267	test: 1.308707

Epoch: 66
Loss: 0.4482301672299703
RMSE train: 0.667241	val: 1.821950	test: 1.671434
MAE train: 0.486931	val: 1.469665	test: 1.286273

Epoch: 67
Loss: 0.47144174575805664
RMSE train: 0.592991	val: 1.825589	test: 1.659558
MAE train: 0.449275	val: 1.453282	test: 1.273731

Epoch: 68
Loss: 0.4104754726092021
RMSE train: 0.642199	val: 1.836343	test: 1.688668
MAE train: 0.491847	val: 1.466142	test: 1.314451

Epoch: 69
Loss: 0.4199889600276947
RMSE train: 0.637902	val: 1.827157	test: 1.676000
MAE train: 0.473790	val: 1.451064	test: 1.323007

Epoch: 70
Loss: 0.41055821379025775
RMSE train: 0.587194	val: 1.800252	test: 1.619918
MAE train: 0.433659	val: 1.415831	test: 1.243689

Epoch: 71
Loss: 0.4006715416908264
RMSE train: 0.603512	val: 1.823791	test: 1.708444
MAE train: 0.449499	val: 1.423542	test: 1.320355

Epoch: 72
Loss: 0.3839787344137828
RMSE train: 0.582477	val: 1.790553	test: 1.655011
MAE train: 0.442449	val: 1.425507	test: 1.282730

Epoch: 73
Loss: 0.4482846260070801
RMSE train: 0.559052	val: 1.752109	test: 1.619107
MAE train: 0.415596	val: 1.397212	test: 1.254799

Epoch: 74
Loss: 0.4267817835013072
RMSE train: 0.575583	val: 1.728213	test: 1.614082
MAE train: 0.428726	val: 1.380407	test: 1.240890

Epoch: 75
Loss: 0.42929744720458984
RMSE train: 0.560635	val: 1.712715	test: 1.573384
MAE train: 0.411707	val: 1.361721	test: 1.209007

Epoch: 76
Loss: 0.46127506097157794
RMSE train: 0.554656	val: 1.729284	test: 1.586125
MAE train: 0.402126	val: 1.376161	test: 1.218249

Epoch: 77
Loss: 0.42893680930137634
RMSE train: 0.606852	val: 1.756806	test: 1.650694
MAE train: 0.439365	val: 1.415215	test: 1.294495

Epoch: 78
Loss: 0.4266290068626404
RMSE train: 0.577646	val: 1.730275	test: 1.617826
MAE train: 0.419855	val: 1.387243	test: 1.255761

Epoch: 79
Loss: 0.37483763694763184
RMSE train: 0.586500	val: 1.697063	test: 1.584505
MAE train: 0.422052	val: 1.348493	test: 1.214584

Epoch: 80
Loss: 0.36973562836647034
RMSE train: 0.612174	val: 1.709722	test: 1.617110
MAE train: 0.437236	val: 1.371265	test: 1.253854

Epoch: 81
Loss: 0.337043156226476
RMSE train: 0.604894	val: 1.731557	test: 1.683104
MAE train: 0.435423	val: 1.381592	test: 1.295092

Epoch: 82
Loss: 0.3610448936621348
RMSE train: 0.600972	val: 1.749842	test: 1.691193
MAE train: 0.429004	val: 1.392927	test: 1.302874

Epoch: 83
Loss: 0.35851991176605225
RMSE train: 0.587920	val: 1.744446	test: 1.641056
MAE train: 0.420221	val: 1.400680	test: 1.294338
RMSE train: 1.244514	val: 2.140255	test: 2.242652
MAE train: 0.936197	val: 1.790126	test: 1.825854

Epoch: 24
Loss: 1.5034502347310383
RMSE train: 1.127881	val: 2.100096	test: 2.155592
MAE train: 0.818659	val: 1.759872	test: 1.760086

Epoch: 25
Loss: 1.4554917812347412
RMSE train: 1.022933	val: 2.094001	test: 2.087978
MAE train: 0.740422	val: 1.740742	test: 1.691918

Epoch: 26
Loss: 1.2793250878651936
RMSE train: 0.912142	val: 1.987740	test: 1.902746
MAE train: 0.655382	val: 1.654777	test: 1.536169

Epoch: 27
Loss: 1.1009168028831482
RMSE train: 0.846279	val: 1.895427	test: 1.772871
MAE train: 0.609252	val: 1.572961	test: 1.423273

Epoch: 28
Loss: 1.1054884592692058
RMSE train: 0.794195	val: 1.910532	test: 1.796246
MAE train: 0.572495	val: 1.563091	test: 1.426610

Epoch: 29
Loss: 0.9515246152877808
RMSE train: 0.825798	val: 2.008587	test: 1.854361
MAE train: 0.618942	val: 1.618723	test: 1.472807

Epoch: 30
Loss: 0.8466788331667582
RMSE train: 0.732753	val: 1.990299	test: 1.806962
MAE train: 0.539855	val: 1.607261	test: 1.421807

Epoch: 31
Loss: 0.7927620609601339
RMSE train: 0.679236	val: 1.963686	test: 1.797783
MAE train: 0.496824	val: 1.580335	test: 1.401579

Epoch: 32
Loss: 0.8214933474858602
RMSE train: 0.657351	val: 1.966525	test: 1.807785
MAE train: 0.477902	val: 1.578246	test: 1.407273

Epoch: 33
Loss: 0.7346724271774292
RMSE train: 0.665558	val: 2.025084	test: 1.826716
MAE train: 0.477948	val: 1.644332	test: 1.443069

Epoch: 34
Loss: 0.7586067318916321
RMSE train: 0.599841	val: 2.040542	test: 1.825499
MAE train: 0.439720	val: 1.659898	test: 1.427372

Epoch: 35
Loss: 0.6357529163360596
RMSE train: 0.603202	val: 2.077378	test: 2.001491
MAE train: 0.452234	val: 1.634138	test: 1.555349

Epoch: 36
Loss: 0.6855201323827108
RMSE train: 0.617347	val: 1.935641	test: 1.771538
MAE train: 0.465347	val: 1.542030	test: 1.361155

Epoch: 37
Loss: 0.7056287328402201
RMSE train: 0.617499	val: 1.909717	test: 1.690684
MAE train: 0.473605	val: 1.533132	test: 1.323414

Epoch: 38
Loss: 0.6061796347300211
RMSE train: 0.615281	val: 1.898808	test: 1.684374
MAE train: 0.472342	val: 1.514424	test: 1.316107

Epoch: 39
Loss: 0.6361131270726522
RMSE train: 0.611568	val: 1.816962	test: 1.628997
MAE train: 0.469149	val: 1.449293	test: 1.277231

Epoch: 40
Loss: 0.6373614072799683
RMSE train: 0.590081	val: 1.791559	test: 1.602065
MAE train: 0.447178	val: 1.429992	test: 1.259061

Epoch: 41
Loss: 0.6129583915074667
RMSE train: 0.628966	val: 1.815961	test: 1.632762
MAE train: 0.473023	val: 1.468804	test: 1.305823

Epoch: 42
Loss: 0.6321690479914347
RMSE train: 0.563806	val: 1.777718	test: 1.649003
MAE train: 0.416726	val: 1.420167	test: 1.293603

Epoch: 43
Loss: 0.5708646774291992
RMSE train: 0.597517	val: 1.781445	test: 1.649122
MAE train: 0.443793	val: 1.428460	test: 1.291350

Epoch: 44
Loss: 0.6154660582542419
RMSE train: 0.645817	val: 1.741955	test: 1.670052
MAE train: 0.489026	val: 1.398336	test: 1.285922

Epoch: 45
Loss: 0.5724320610364279
RMSE train: 0.739599	val: 1.751511	test: 1.675875
MAE train: 0.553226	val: 1.427941	test: 1.309436

Epoch: 46
Loss: 0.5683167378107706
RMSE train: 0.717617	val: 1.766788	test: 1.689531
MAE train: 0.536233	val: 1.442569	test: 1.319248

Epoch: 47
Loss: 0.5414600769678751
RMSE train: 0.646514	val: 1.832394	test: 1.728952
MAE train: 0.492801	val: 1.472553	test: 1.344082

Epoch: 48
Loss: 0.6083455880482992
RMSE train: 0.613618	val: 1.821414	test: 1.698871
MAE train: 0.472446	val: 1.457709	test: 1.313615

Epoch: 49
Loss: 0.5602124730745951
RMSE train: 0.626371	val: 1.796219	test: 1.667789
MAE train: 0.474927	val: 1.433053	test: 1.295153

Epoch: 50
Loss: 0.47654882073402405
RMSE train: 0.607107	val: 1.767442	test: 1.621442
MAE train: 0.443171	val: 1.407624	test: 1.283796

Epoch: 51
Loss: 0.5770469605922699
RMSE train: 0.546733	val: 1.752035	test: 1.610320
MAE train: 0.405347	val: 1.380769	test: 1.249827

Epoch: 52
Loss: 0.5420467654863993
RMSE train: 0.543105	val: 1.716154	test: 1.593232
MAE train: 0.403582	val: 1.362226	test: 1.242031

Epoch: 53
Loss: 0.5145600934823354
RMSE train: 0.549282	val: 1.683704	test: 1.589737
MAE train: 0.409430	val: 1.340694	test: 1.240386

Epoch: 54
Loss: 0.5055461923281351
RMSE train: 0.535595	val: 1.688897	test: 1.608758
MAE train: 0.397709	val: 1.350877	test: 1.262873

Epoch: 55
Loss: 0.48982057968775433
RMSE train: 0.537093	val: 1.746165	test: 1.725105
MAE train: 0.406813	val: 1.389747	test: 1.357640

Epoch: 56
Loss: 0.498194952805837
RMSE train: 0.547072	val: 1.715254	test: 1.657150
MAE train: 0.406118	val: 1.367917	test: 1.286789

Epoch: 57
Loss: 0.4618774652481079
RMSE train: 0.589238	val: 1.730978	test: 1.615330
MAE train: 0.427359	val: 1.377010	test: 1.261586

Epoch: 58
Loss: 0.5620448589324951
RMSE train: 0.589116	val: 1.729178	test: 1.604741
MAE train: 0.444370	val: 1.372549	test: 1.239728

Epoch: 59
Loss: 0.5075712502002716
RMSE train: 0.599055	val: 1.754474	test: 1.655325
MAE train: 0.449750	val: 1.384217	test: 1.277384

Epoch: 60
Loss: 0.46177197496096295
RMSE train: 0.539457	val: 1.695506	test: 1.599041
MAE train: 0.402642	val: 1.351739	test: 1.254073

Epoch: 61
Loss: 0.4694552222887675
RMSE train: 0.551687	val: 1.668601	test: 1.597869
MAE train: 0.408229	val: 1.329425	test: 1.264861

Epoch: 62
Loss: 0.46866103013356525
RMSE train: 0.538669	val: 1.675222	test: 1.700044
MAE train: 0.407968	val: 1.332066	test: 1.331964

Epoch: 63
Loss: 0.46327746907869977
RMSE train: 0.550868	val: 1.616125	test: 1.650073
MAE train: 0.412678	val: 1.282676	test: 1.285588

Epoch: 64
Loss: 0.4336337248484294
RMSE train: 0.633463	val: 1.594442	test: 1.570385
MAE train: 0.463392	val: 1.262578	test: 1.227190

Epoch: 65
Loss: 0.4534907341003418
RMSE train: 0.575097	val: 1.588425	test: 1.581443
MAE train: 0.426760	val: 1.266782	test: 1.235199

Epoch: 66
Loss: 0.4957929750283559
RMSE train: 0.513118	val: 1.639700	test: 1.653686
MAE train: 0.386097	val: 1.310054	test: 1.294139

Epoch: 67
Loss: 0.41461028655370075
RMSE train: 0.531705	val: 1.632171	test: 1.654821
MAE train: 0.398422	val: 1.307079	test: 1.292870

Epoch: 68
Loss: 0.46790529290835065
RMSE train: 0.537076	val: 1.646235	test: 1.653592
MAE train: 0.400860	val: 1.314301	test: 1.292544

Epoch: 69
Loss: 0.42936662832895917
RMSE train: 0.557341	val: 1.670486	test: 1.642368
MAE train: 0.416342	val: 1.340170	test: 1.285353

Epoch: 70
Loss: 0.42972729603449505
RMSE train: 0.526484	val: 1.835466	test: 1.785535
MAE train: 0.396771	val: 1.419984	test: 1.386670

Epoch: 71
Loss: 0.43220486243565875
RMSE train: 0.551830	val: 1.872865	test: 1.843505
MAE train: 0.423275	val: 1.444302	test: 1.432593

Epoch: 72
Loss: 0.43687963485717773
RMSE train: 0.564055	val: 1.783993	test: 1.739606
MAE train: 0.422080	val: 1.422643	test: 1.363160

Epoch: 73
Loss: 0.3852205276489258
RMSE train: 0.543482	val: 1.862962	test: 1.796969
MAE train: 0.400446	val: 1.464181	test: 1.400721

Epoch: 74
Loss: 0.4207417865594228
RMSE train: 0.541131	val: 1.890356	test: 1.801961
MAE train: 0.400397	val: 1.494226	test: 1.403109

Epoch: 75
Loss: 0.5568939447402954
RMSE train: 0.542139	val: 1.910379	test: 1.784758
MAE train: 0.394793	val: 1.526633	test: 1.390889

Epoch: 76
Loss: 0.4212034046649933
RMSE train: 0.574877	val: 1.898628	test: 1.768228
MAE train: 0.420507	val: 1.534534	test: 1.376673

Epoch: 77
Loss: 0.4468987484773
RMSE train: 0.591851	val: 1.974743	test: 1.849418
MAE train: 0.432933	val: 1.559208	test: 1.427598

Epoch: 78
Loss: 0.4318672815958659
RMSE train: 0.610573	val: 1.951884	test: 1.846327
MAE train: 0.462675	val: 1.523219	test: 1.423873

Epoch: 79
Loss: 0.3927264412244161
RMSE train: 0.526135	val: 1.921280	test: 1.794139
MAE train: 0.396601	val: 1.505258	test: 1.397744

Epoch: 80
Loss: 0.4281247556209564
RMSE train: 0.495739	val: 1.889500	test: 1.772676
MAE train: 0.369993	val: 1.486509	test: 1.393057

Epoch: 81
Loss: 0.39101247986157733
RMSE train: 0.515828	val: 1.830573	test: 1.744589
MAE train: 0.378082	val: 1.447752	test: 1.376435

Epoch: 82
Loss: 0.37491756677627563
RMSE train: 0.568891	val: 1.829510	test: 1.751646
MAE train: 0.413208	val: 1.440551	test: 1.374691

Epoch: 83
Loss: 0.3762740393479665
RMSE train: 0.488175	val: 1.877815	test: 1.751740
MAE train: 0.356020	val: 1.498018	test: 1.378103
RMSE train: 1.070821	val: 1.601424	test: 1.800074
MAE train: 0.792849	val: 1.309260	test: 1.426681

Epoch: 24
Loss: 1.500447690486908
RMSE train: 1.153436	val: 1.667937	test: 1.819758
MAE train: 0.884640	val: 1.410105	test: 1.476084

Epoch: 25
Loss: 1.198457956314087
RMSE train: 0.947473	val: 1.659688	test: 1.865912
MAE train: 0.709925	val: 1.379372	test: 1.500827

Epoch: 26
Loss: 0.9806353002786636
RMSE train: 0.919296	val: 1.683656	test: 1.920618
MAE train: 0.683440	val: 1.391355	test: 1.552420

Epoch: 27
Loss: 1.1920258402824402
RMSE train: 0.921715	val: 1.578289	test: 1.773503
MAE train: 0.680973	val: 1.270832	test: 1.433405

Epoch: 28
Loss: 1.1574169993400574
RMSE train: 0.816391	val: 1.456951	test: 1.628248
MAE train: 0.620988	val: 1.187176	test: 1.306383

Epoch: 29
Loss: 1.2057313919067383
RMSE train: 0.824961	val: 1.342523	test: 1.491067
MAE train: 0.646508	val: 1.082724	test: 1.195011

Epoch: 30
Loss: 1.2653294205665588
RMSE train: 0.735914	val: 1.416776	test: 1.516253
MAE train: 0.577212	val: 1.125391	test: 1.234544

Epoch: 31
Loss: 1.1167982369661331
RMSE train: 0.734964	val: 1.497956	test: 1.756636
MAE train: 0.558065	val: 1.194934	test: 1.417302

Epoch: 32
Loss: 1.6855651140213013
RMSE train: 0.833780	val: 1.612469	test: 1.832891
MAE train: 0.651049	val: 1.321498	test: 1.500007

Epoch: 33
Loss: 1.04497130215168
RMSE train: 0.741944	val: 1.479627	test: 1.657643
MAE train: 0.564007	val: 1.201438	test: 1.324848

Epoch: 34
Loss: 0.812063992023468
RMSE train: 0.741214	val: 1.393717	test: 1.549126
MAE train: 0.579215	val: 1.117559	test: 1.224480

Epoch: 35
Loss: 1.0166163742542267
RMSE train: 0.735201	val: 1.438070	test: 1.559828
MAE train: 0.565850	val: 1.163518	test: 1.240263

Epoch: 36
Loss: 1.0361087620258331
RMSE train: 0.776365	val: 1.386311	test: 1.482539
MAE train: 0.596008	val: 1.145716	test: 1.173225

Epoch: 37
Loss: 0.8800502270460129
RMSE train: 0.749112	val: 1.318812	test: 1.453097
MAE train: 0.576574	val: 1.087777	test: 1.165700

Epoch: 38
Loss: 0.8007293790578842
RMSE train: 0.768358	val: 1.338082	test: 1.481599
MAE train: 0.591253	val: 1.095210	test: 1.188917

Epoch: 39
Loss: 1.1662596315145493
RMSE train: 0.702713	val: 1.362024	test: 1.488853
MAE train: 0.546455	val: 1.111063	test: 1.182127

Epoch: 40
Loss: 0.869737446308136
RMSE train: 0.745390	val: 1.462809	test: 1.538040
MAE train: 0.584743	val: 1.183691	test: 1.205616

Epoch: 41
Loss: 0.8605155199766159
RMSE train: 0.769775	val: 1.430896	test: 1.573973
MAE train: 0.603998	val: 1.144884	test: 1.211893

Epoch: 42
Loss: 0.9862083792686462
RMSE train: 0.699223	val: 1.329217	test: 1.463716
MAE train: 0.541563	val: 1.043925	test: 1.152831

Epoch: 43
Loss: 0.809468224644661
RMSE train: 0.697845	val: 1.233388	test: 1.304604
MAE train: 0.548145	val: 0.942462	test: 1.032855

Epoch: 44
Loss: 0.7175458073616028
RMSE train: 0.723308	val: 1.317209	test: 1.392160
MAE train: 0.557804	val: 1.019036	test: 1.111117

Epoch: 45
Loss: 0.7464926391839981
RMSE train: 0.769889	val: 1.380461	test: 1.479563
MAE train: 0.591026	val: 1.079020	test: 1.178792

Epoch: 46
Loss: 0.8252176195383072
RMSE train: 0.798108	val: 1.392356	test: 1.522926
MAE train: 0.613461	val: 1.112941	test: 1.209378

Epoch: 47
Loss: 0.7527279257774353
RMSE train: 0.731855	val: 1.317354	test: 1.458865
MAE train: 0.563905	val: 1.043564	test: 1.148569

Epoch: 48
Loss: 0.7418262511491776
RMSE train: 0.719347	val: 1.291383	test: 1.431105
MAE train: 0.551747	val: 1.022035	test: 1.123617

Epoch: 49
Loss: 0.8706054389476776
RMSE train: 0.686741	val: 1.235242	test: 1.335711
MAE train: 0.522053	val: 0.978633	test: 1.059222

Epoch: 50
Loss: 0.7012090682983398
RMSE train: 0.625229	val: 1.197178	test: 1.230590
MAE train: 0.482839	val: 0.931283	test: 0.983849

Epoch: 51
Loss: 0.6160523891448975
RMSE train: 0.650506	val: 1.268096	test: 1.276923
MAE train: 0.501871	val: 0.981072	test: 1.015591

Epoch: 52
Loss: 0.8664821684360504
RMSE train: 0.595379	val: 1.247454	test: 1.310180
MAE train: 0.454412	val: 0.966401	test: 1.043456

Epoch: 53
Loss: 0.7496938705444336
RMSE train: 0.708153	val: 1.321893	test: 1.436570
MAE train: 0.528812	val: 1.070211	test: 1.167213

Epoch: 54
Loss: 0.902659684419632
RMSE train: 0.762779	val: 1.438252	test: 1.634154
MAE train: 0.572298	val: 1.186831	test: 1.307591

Epoch: 55
Loss: 0.6823442280292511
RMSE train: 0.812756	val: 1.459868	test: 1.725072
MAE train: 0.616036	val: 1.172471	test: 1.368234

Epoch: 56
Loss: 0.7725468128919601
RMSE train: 0.761500	val: 1.423438	test: 1.661846
MAE train: 0.583530	val: 1.145350	test: 1.311666

Epoch: 57
Loss: 0.6996991783380508
RMSE train: 0.726452	val: 1.355250	test: 1.594397
MAE train: 0.555520	val: 1.093342	test: 1.239081

Epoch: 58
Loss: 0.751629114151001
RMSE train: 0.767004	val: 1.340137	test: 1.579666
MAE train: 0.577737	val: 1.084732	test: 1.229688

Epoch: 59
Loss: 0.7759317755699158
RMSE train: 0.755386	val: 1.333976	test: 1.561859
MAE train: 0.566529	val: 1.081523	test: 1.234390

Epoch: 60
Loss: 0.8025757074356079
RMSE train: 0.708492	val: 1.350433	test: 1.549746
MAE train: 0.523529	val: 1.091819	test: 1.244840

Epoch: 61
Loss: 0.6219334751367569
RMSE train: 0.686313	val: 1.193020	test: 1.332807
MAE train: 0.530138	val: 0.946194	test: 1.057246

Epoch: 62
Loss: 0.6643313467502594
RMSE train: 0.634284	val: 1.138824	test: 1.283611
MAE train: 0.490618	val: 0.908731	test: 1.018921

Epoch: 63
Loss: 0.6432165950536728
RMSE train: 0.652257	val: 1.134932	test: 1.302202
MAE train: 0.506272	val: 0.929131	test: 1.027992

Epoch: 64
Loss: 0.8261576741933823
RMSE train: 0.638481	val: 1.179821	test: 1.376071
MAE train: 0.492802	val: 0.975330	test: 1.082321

Epoch: 65
Loss: 0.5994899868965149
RMSE train: 0.689265	val: 1.206470	test: 1.386856
MAE train: 0.518627	val: 0.974162	test: 1.095119

Epoch: 66
Loss: 0.6414233893156052
RMSE train: 0.639221	val: 1.180117	test: 1.316006
MAE train: 0.485305	val: 0.937172	test: 1.038008

Epoch: 67
Loss: 0.7198067009449005
RMSE train: 0.641083	val: 1.217425	test: 1.325298
MAE train: 0.493318	val: 0.954464	test: 1.038323

Epoch: 68
Loss: 0.8401783853769302
RMSE train: 0.667379	val: 1.293504	test: 1.407122
MAE train: 0.515523	val: 1.033814	test: 1.120502

Epoch: 69
Loss: 0.6155438497662544
RMSE train: 0.726752	val: 1.381535	test: 1.511213
MAE train: 0.556624	val: 1.093518	test: 1.215039

Epoch: 70
Loss: 0.6253151744604111
RMSE train: 0.703258	val: 1.333803	test: 1.422825
MAE train: 0.530548	val: 1.037449	test: 1.135808

Epoch: 71
Loss: 0.5660476535558701
RMSE train: 0.627809	val: 1.255215	test: 1.263727
MAE train: 0.490194	val: 0.952781	test: 1.011010

Epoch: 72
Loss: 0.537691205739975
RMSE train: 0.650873	val: 1.215994	test: 1.206126
MAE train: 0.514635	val: 0.913977	test: 0.974651

Epoch: 73
Loss: 0.6650345623493195
RMSE train: 0.579474	val: 1.176026	test: 1.209692
MAE train: 0.456899	val: 0.907542	test: 0.975611

Epoch: 74
Loss: 0.5282911583781242
RMSE train: 0.590463	val: 1.255491	test: 1.344106
MAE train: 0.445259	val: 0.974280	test: 1.063661

Epoch: 75
Loss: 0.8065598160028458
RMSE train: 0.688296	val: 1.377837	test: 1.514512
MAE train: 0.509907	val: 1.094503	test: 1.196207

Epoch: 76
Loss: 0.5035332255065441
RMSE train: 0.622950	val: 1.290616	test: 1.392544
MAE train: 0.471728	val: 1.035790	test: 1.101989

Epoch: 77
Loss: 0.5285812467336655
RMSE train: 0.582399	val: 1.211615	test: 1.302744
MAE train: 0.455047	val: 0.972557	test: 1.044777

Epoch: 78
Loss: 0.602847620844841
RMSE train: 0.589876	val: 1.209473	test: 1.340312
MAE train: 0.458236	val: 0.979046	test: 1.083229

Epoch: 79
Loss: 0.9498040229082108
RMSE train: 0.643828	val: 1.244771	test: 1.388994
MAE train: 0.483041	val: 1.003598	test: 1.080771

Epoch: 80
Loss: 0.5517732203006744
RMSE train: 0.675229	val: 1.232036	test: 1.351913
MAE train: 0.518677	val: 0.984945	test: 1.058919

Epoch: 81
Loss: 0.5579959154129028
RMSE train: 0.573150	val: 1.158349	test: 1.299798
MAE train: 0.438161	val: 0.927613	test: 1.035145

Epoch: 82
Loss: 0.6311481669545174
RMSE train: 0.574155	val: 1.176802	test: 1.361819
MAE train: 0.435687	val: 0.930957	test: 1.081851

Epoch: 83
Loss: 0.5770417973399162
RMSE train: 0.619171	val: 1.216193	test: 1.438125
MAE train: 0.473536	val: 0.977521	test: 1.145342

Epoch: 84
Loss: 0.569037452340126
RMSE train: 1.119103	val: 1.752873	test: 1.837961
MAE train: 0.836767	val: 1.418826	test: 1.485536

Epoch: 24
Loss: 1.3837257027626038
RMSE train: 0.994352	val: 1.569148	test: 1.650336
MAE train: 0.757866	val: 1.288624	test: 1.316491

Epoch: 25
Loss: 1.1322354823350906
RMSE train: 1.002427	val: 1.704979	test: 1.756092
MAE train: 0.759593	val: 1.418825	test: 1.415855

Epoch: 26
Loss: 1.2773089408874512
RMSE train: 1.052391	val: 1.887974	test: 1.971543
MAE train: 0.818173	val: 1.542736	test: 1.581698

Epoch: 27
Loss: 1.0267495959997177
RMSE train: 0.815729	val: 1.418359	test: 1.521689
MAE train: 0.618002	val: 1.144335	test: 1.198114

Epoch: 28
Loss: 1.1185240745544434
RMSE train: 0.809685	val: 1.339448	test: 1.399167
MAE train: 0.617274	val: 1.046683	test: 1.144389

Epoch: 29
Loss: 1.0507247298955917
RMSE train: 0.712639	val: 1.380267	test: 1.452605
MAE train: 0.526646	val: 1.083011	test: 1.180403

Epoch: 30
Loss: 1.0139439553022385
RMSE train: 0.816001	val: 1.516824	test: 1.584890
MAE train: 0.609770	val: 1.196143	test: 1.274985

Epoch: 31
Loss: 1.067933201789856
RMSE train: 0.780631	val: 1.477247	test: 1.543221
MAE train: 0.591222	val: 1.177508	test: 1.220698

Epoch: 32
Loss: 1.0579891949892044
RMSE train: 0.767197	val: 1.464175	test: 1.586141
MAE train: 0.586067	val: 1.152160	test: 1.235986

Epoch: 33
Loss: 1.0220493078231812
RMSE train: 0.729484	val: 1.408191	test: 1.517247
MAE train: 0.564386	val: 1.113349	test: 1.189380

Epoch: 34
Loss: 0.7936985194683075
RMSE train: 0.741494	val: 1.317239	test: 1.420891
MAE train: 0.579613	val: 1.044022	test: 1.155893

Epoch: 35
Loss: 1.3422666490077972
RMSE train: 0.727972	val: 1.360520	test: 1.471387
MAE train: 0.559965	val: 1.022756	test: 1.208514

Epoch: 36
Loss: 0.8492400348186493
RMSE train: 0.830006	val: 1.648914	test: 1.757407
MAE train: 0.630333	val: 1.230464	test: 1.417543

Epoch: 37
Loss: 0.9341103732585907
RMSE train: 0.770641	val: 1.457270	test: 1.601851
MAE train: 0.587566	val: 1.092314	test: 1.297077

Epoch: 38
Loss: 0.8069034814834595
RMSE train: 0.716677	val: 1.230574	test: 1.372134
MAE train: 0.548475	val: 0.947054	test: 1.119644

Epoch: 39
Loss: 1.0872887670993805
RMSE train: 0.723024	val: 1.322668	test: 1.480999
MAE train: 0.557611	val: 1.042063	test: 1.193134

Epoch: 40
Loss: 0.7836864441633224
RMSE train: 0.897947	val: 1.692583	test: 1.787165
MAE train: 0.710675	val: 1.370328	test: 1.444130

Epoch: 41
Loss: 0.8134743422269821
RMSE train: 0.715347	val: 1.477452	test: 1.503944
MAE train: 0.546151	val: 1.164287	test: 1.220541

Epoch: 42
Loss: 0.9573909193277359
RMSE train: 0.708046	val: 1.329629	test: 1.434257
MAE train: 0.533592	val: 1.045509	test: 1.148417

Epoch: 43
Loss: 0.843528226017952
RMSE train: 0.806211	val: 1.406413	test: 1.593317
MAE train: 0.615489	val: 1.098765	test: 1.243237

Epoch: 44
Loss: 0.8682926744222641
RMSE train: 0.740655	val: 1.275044	test: 1.472484
MAE train: 0.557449	val: 1.023433	test: 1.172888

Epoch: 45
Loss: 1.1724801808595657
RMSE train: 0.712168	val: 1.302658	test: 1.485612
MAE train: 0.535904	val: 1.038516	test: 1.189105

Epoch: 46
Loss: 0.750937283039093
RMSE train: 0.744069	val: 1.529940	test: 1.721479
MAE train: 0.563769	val: 1.201919	test: 1.383591

Epoch: 47
Loss: 0.8119144588708878
RMSE train: 0.722079	val: 1.419445	test: 1.558468
MAE train: 0.553804	val: 1.110672	test: 1.245365

Epoch: 48
Loss: 0.9452769607305527
RMSE train: 0.717199	val: 1.238169	test: 1.311670
MAE train: 0.563443	val: 0.956182	test: 1.073506

Epoch: 49
Loss: 0.7303863316774368
RMSE train: 0.792769	val: 1.224607	test: 1.305510
MAE train: 0.628290	val: 0.943841	test: 1.078105

Epoch: 50
Loss: 0.7479276210069656
RMSE train: 0.669299	val: 1.331498	test: 1.479193
MAE train: 0.509630	val: 1.037498	test: 1.197465

Epoch: 51
Loss: 1.1125055104494095
RMSE train: 0.682452	val: 1.387603	test: 1.527705
MAE train: 0.521745	val: 1.063942	test: 1.232395

Epoch: 52
Loss: 0.7196841090917587
RMSE train: 0.696246	val: 1.308985	test: 1.388451
MAE train: 0.539881	val: 1.017216	test: 1.129020

Epoch: 53
Loss: 0.6622762680053711
RMSE train: 0.641287	val: 1.242903	test: 1.294921
MAE train: 0.499374	val: 0.955883	test: 1.054717

Epoch: 54
Loss: 0.825692355632782
RMSE train: 0.633167	val: 1.387806	test: 1.453834
MAE train: 0.492297	val: 1.045443	test: 1.169591

Epoch: 55
Loss: 0.7899323850870132
RMSE train: 0.732067	val: 1.467871	test: 1.564186
MAE train: 0.579276	val: 1.118886	test: 1.253181

Epoch: 56
Loss: 0.7975892424583435
RMSE train: 0.696405	val: 1.369628	test: 1.475759
MAE train: 0.553038	val: 1.034462	test: 1.198337

Epoch: 57
Loss: 0.9262464195489883
RMSE train: 0.704565	val: 1.297036	test: 1.414926
MAE train: 0.562438	val: 0.979717	test: 1.148602

Epoch: 58
Loss: 0.7307779341936111
RMSE train: 0.703272	val: 1.317557	test: 1.471822
MAE train: 0.558198	val: 1.002155	test: 1.195632

Epoch: 59
Loss: 0.9353166669607162
RMSE train: 0.727960	val: 1.357630	test: 1.506268
MAE train: 0.571652	val: 1.049845	test: 1.231025

Epoch: 60
Loss: 0.7197285741567612
RMSE train: 0.695657	val: 1.322804	test: 1.437505
MAE train: 0.539083	val: 1.028739	test: 1.155131

Epoch: 61
Loss: 0.6840231716632843
RMSE train: 0.631063	val: 1.257287	test: 1.341810
MAE train: 0.475823	val: 0.979844	test: 1.075756

Epoch: 62
Loss: 0.6168854832649231
RMSE train: 0.652415	val: 1.269771	test: 1.366424
MAE train: 0.488367	val: 0.994097	test: 1.087057

Epoch: 63
Loss: 0.6242926567792892
RMSE train: 0.696731	val: 1.309540	test: 1.437539
MAE train: 0.525740	val: 1.032130	test: 1.148015

Epoch: 64
Loss: 0.7088282257318497
RMSE train: 0.665021	val: 1.279432	test: 1.413733
MAE train: 0.498549	val: 0.993587	test: 1.143150

Epoch: 65
Loss: 0.8615557551383972
RMSE train: 0.634950	val: 1.334760	test: 1.496261
MAE train: 0.480496	val: 1.016823	test: 1.211077

Epoch: 66
Loss: 0.6317406296730042
RMSE train: 0.592708	val: 1.302300	test: 1.463821
MAE train: 0.443979	val: 1.005313	test: 1.177343

Epoch: 67
Loss: 0.5793787688016891
RMSE train: 0.632781	val: 1.275349	test: 1.437561
MAE train: 0.474734	val: 0.991188	test: 1.153792

Epoch: 68
Loss: 0.7886562943458557
RMSE train: 0.662293	val: 1.297076	test: 1.467708
MAE train: 0.496101	val: 1.017340	test: 1.164527

Epoch: 69
Loss: 0.6390543729066849
RMSE train: 0.740273	val: 1.363982	test: 1.538362
MAE train: 0.565774	val: 1.064576	test: 1.213325

Epoch: 70
Loss: 1.028121992945671
RMSE train: 0.678383	val: 1.274140	test: 1.416782
MAE train: 0.506569	val: 0.998276	test: 1.134236

Epoch: 71
Loss: 0.6284223198890686
RMSE train: 0.660022	val: 1.236143	test: 1.370638
MAE train: 0.484874	val: 0.977279	test: 1.088416

Epoch: 72
Loss: 0.7932212799787521
RMSE train: 0.625193	val: 1.344372	test: 1.511215
MAE train: 0.461553	val: 1.075667	test: 1.199289

Epoch: 73
Loss: 0.7321349084377289
RMSE train: 0.712511	val: 1.470927	test: 1.650612
MAE train: 0.533431	val: 1.173729	test: 1.285347

Epoch: 74
Loss: 0.6214130893349648
RMSE train: 0.697179	val: 1.308785	test: 1.478433
MAE train: 0.513523	val: 1.044306	test: 1.139235

Epoch: 75
Loss: 0.6245933771133423
RMSE train: 0.643579	val: 1.242324	test: 1.376801
MAE train: 0.471974	val: 0.984993	test: 1.085675

Epoch: 76
Loss: 0.6625385731458664
RMSE train: 0.608992	val: 1.309614	test: 1.457259
MAE train: 0.466851	val: 1.032415	test: 1.163610

Epoch: 77
Loss: 0.5936877653002739
RMSE train: 0.568210	val: 1.274541	test: 1.380035
MAE train: 0.443131	val: 0.990485	test: 1.109871

Epoch: 78
Loss: 0.5366790220141411
RMSE train: 0.573306	val: 1.233124	test: 1.296185
MAE train: 0.448086	val: 0.932654	test: 1.030930

Epoch: 79
Loss: 0.6730927526950836
RMSE train: 0.565981	val: 1.247185	test: 1.324055
MAE train: 0.454404	val: 0.927864	test: 1.063603

Epoch: 80
Loss: 0.653733491897583
RMSE train: 0.589938	val: 1.288389	test: 1.421089
MAE train: 0.468737	val: 0.957069	test: 1.146119

Epoch: 81
Loss: 0.6268864721059799
RMSE train: 0.583362	val: 1.200146	test: 1.312379
MAE train: 0.453620	val: 0.894426	test: 1.052052

Epoch: 82
Loss: 0.5533338263630867
RMSE train: 0.591551	val: 1.201483	test: 1.335968
MAE train: 0.450644	val: 0.917464	test: 1.066797

Epoch: 83
Loss: 0.5928927287459373
RMSE train: 0.577045	val: 1.240665	test: 1.437375
MAE train: 0.435579	val: 0.962553	test: 1.157026
RMSE train: 1.072590	val: 1.576912	test: 1.879022
MAE train: 0.815875	val: 1.292972	test: 1.542181

Epoch: 24
Loss: 1.126127079129219
RMSE train: 0.979042	val: 1.499502	test: 1.819810
MAE train: 0.761927	val: 1.214586	test: 1.468560

Epoch: 25
Loss: 1.1545754820108414
RMSE train: 0.912579	val: 1.541272	test: 1.872369
MAE train: 0.698860	val: 1.254391	test: 1.486834

Epoch: 26
Loss: 1.1665329337120056
RMSE train: 0.878220	val: 1.487355	test: 1.791879
MAE train: 0.682831	val: 1.218047	test: 1.441153

Epoch: 27
Loss: 1.3645817637443542
RMSE train: 0.868051	val: 1.415799	test: 1.656064
MAE train: 0.676909	val: 1.157944	test: 1.351863

Epoch: 28
Loss: 1.1695610731840134
RMSE train: 0.888366	val: 1.459975	test: 1.644378
MAE train: 0.684830	val: 1.181761	test: 1.345751

Epoch: 29
Loss: 1.2177076637744904
RMSE train: 0.866915	val: 1.556785	test: 1.778682
MAE train: 0.684644	val: 1.262243	test: 1.467434

Epoch: 30
Loss: 0.9161181896924973
RMSE train: 0.922273	val: 1.506558	test: 1.732172
MAE train: 0.729510	val: 1.242821	test: 1.395784

Epoch: 31
Loss: 1.0143933147192001
RMSE train: 0.970411	val: 1.457024	test: 1.685338
MAE train: 0.777961	val: 1.193703	test: 1.347548

Epoch: 32
Loss: 0.9328676462173462
RMSE train: 0.718110	val: 1.420562	test: 1.681191
MAE train: 0.552696	val: 1.156662	test: 1.346246

Epoch: 33
Loss: 1.0922328233718872
RMSE train: 0.723737	val: 1.455750	test: 1.689312
MAE train: 0.553988	val: 1.176245	test: 1.350644

Epoch: 34
Loss: 0.9060075581073761
RMSE train: 0.758481	val: 1.543777	test: 1.763422
MAE train: 0.579860	val: 1.218523	test: 1.385073

Epoch: 35
Loss: 0.8676232099533081
RMSE train: 0.864534	val: 1.471786	test: 1.709395
MAE train: 0.683874	val: 1.181682	test: 1.332588

Epoch: 36
Loss: 0.7749499380588531
RMSE train: 0.810831	val: 1.396827	test: 1.647927
MAE train: 0.639896	val: 1.133452	test: 1.317579

Epoch: 37
Loss: 0.9005538523197174
RMSE train: 0.731033	val: 1.346899	test: 1.555795
MAE train: 0.569355	val: 1.066628	test: 1.234097

Epoch: 38
Loss: 0.7130702584981918
RMSE train: 0.690449	val: 1.442463	test: 1.599697
MAE train: 0.533087	val: 1.130155	test: 1.265874

Epoch: 39
Loss: 0.7906233817338943
RMSE train: 0.690247	val: 1.427521	test: 1.537813
MAE train: 0.527342	val: 1.124686	test: 1.222656

Epoch: 40
Loss: 1.0057650953531265
RMSE train: 0.704999	val: 1.390350	test: 1.511028
MAE train: 0.529747	val: 1.143999	test: 1.205782

Epoch: 41
Loss: 0.7928033173084259
RMSE train: 0.708143	val: 1.325878	test: 1.433528
MAE train: 0.536037	val: 1.077557	test: 1.134951

Epoch: 42
Loss: 0.6737907752394676
RMSE train: 0.709530	val: 1.225995	test: 1.312380
MAE train: 0.545304	val: 0.957302	test: 1.038605

Epoch: 43
Loss: 0.8299340009689331
RMSE train: 0.685402	val: 1.214489	test: 1.355384
MAE train: 0.521907	val: 0.967802	test: 1.088198

Epoch: 44
Loss: 0.6732693761587143
RMSE train: 0.707012	val: 1.314855	test: 1.502506
MAE train: 0.544079	val: 1.057860	test: 1.220316

Epoch: 45
Loss: 0.7204213440418243
RMSE train: 0.748791	val: 1.368423	test: 1.548648
MAE train: 0.572490	val: 1.107962	test: 1.266343

Epoch: 46
Loss: 0.7497604638338089
RMSE train: 0.771525	val: 1.349947	test: 1.564847
MAE train: 0.588654	val: 1.097371	test: 1.280090

Epoch: 47
Loss: 0.6077782288193703
RMSE train: 0.773590	val: 1.357176	test: 1.629828
MAE train: 0.589678	val: 1.098742	test: 1.320634

Epoch: 48
Loss: 0.8779510855674744
RMSE train: 0.699098	val: 1.283285	test: 1.496545
MAE train: 0.523315	val: 1.037201	test: 1.214741

Epoch: 49
Loss: 1.1793069541454315
RMSE train: 0.655580	val: 1.264026	test: 1.415061
MAE train: 0.493464	val: 1.019128	test: 1.146333

Epoch: 50
Loss: 0.7362382709980011
RMSE train: 0.660730	val: 1.305391	test: 1.475752
MAE train: 0.502240	val: 1.063740	test: 1.197025

Epoch: 51
Loss: 0.8145193159580231
RMSE train: 0.749274	val: 1.341176	test: 1.508170
MAE train: 0.568775	val: 1.063713	test: 1.223763

Epoch: 52
Loss: 0.6707350015640259
RMSE train: 0.761200	val: 1.259351	test: 1.345461
MAE train: 0.581602	val: 1.000400	test: 1.077531

Epoch: 53
Loss: 0.6738660484552383
RMSE train: 0.784930	val: 1.229277	test: 1.309068
MAE train: 0.603719	val: 0.977170	test: 1.048543

Epoch: 54
Loss: 0.6999921798706055
RMSE train: 0.708126	val: 1.271430	test: 1.453137
MAE train: 0.532134	val: 1.045319	test: 1.151638

Epoch: 55
Loss: 0.7386583238840103
RMSE train: 0.724907	val: 1.372337	test: 1.626477
MAE train: 0.557107	val: 1.117803	test: 1.292317

Epoch: 56
Loss: 0.7440173178911209
RMSE train: 0.725433	val: 1.386604	test: 1.599441
MAE train: 0.556791	val: 1.134348	test: 1.285537

Epoch: 57
Loss: 0.8046130836009979
RMSE train: 0.723883	val: 1.353318	test: 1.484145
MAE train: 0.562537	val: 1.068189	test: 1.160573

Epoch: 58
Loss: 0.8912624269723892
RMSE train: 0.665102	val: 1.318797	test: 1.460083
MAE train: 0.514768	val: 1.044021	test: 1.150972

Epoch: 59
Loss: 0.6479410976171494
RMSE train: 0.634374	val: 1.322202	test: 1.505052
MAE train: 0.481560	val: 1.074587	test: 1.199666

Epoch: 60
Loss: 1.2485281080007553
RMSE train: 0.602524	val: 1.241518	test: 1.382484
MAE train: 0.464052	val: 1.001988	test: 1.112896

Epoch: 61
Loss: 0.7193833589553833
RMSE train: 0.773641	val: 1.282017	test: 1.401873
MAE train: 0.608466	val: 1.023743	test: 1.113493

Epoch: 62
Loss: 0.825036808848381
RMSE train: 0.684793	val: 1.242539	test: 1.366305
MAE train: 0.534202	val: 0.985707	test: 1.091572

Epoch: 63
Loss: 0.6364581137895584
RMSE train: 0.682480	val: 1.368461	test: 1.551039
MAE train: 0.514993	val: 1.075948	test: 1.284231

Epoch: 64
Loss: 0.7225505858659744
RMSE train: 0.607129	val: 1.419000	test: 1.599340
MAE train: 0.449406	val: 1.098859	test: 1.307800

Epoch: 65
Loss: 0.8388362377882004
RMSE train: 0.680015	val: 1.333585	test: 1.465460
MAE train: 0.506881	val: 1.052578	test: 1.178183

Epoch: 66
Loss: 0.6605872958898544
RMSE train: 0.689341	val: 1.297528	test: 1.417841
MAE train: 0.512447	val: 1.018531	test: 1.149643

Epoch: 67
Loss: 0.6106856763362885
RMSE train: 0.620349	val: 1.239582	test: 1.373711
MAE train: 0.464013	val: 0.992908	test: 1.113653

Epoch: 68
Loss: 0.6450396701693535
RMSE train: 0.573934	val: 1.216397	test: 1.330392
MAE train: 0.425001	val: 0.969600	test: 1.072818

Epoch: 69
Loss: 0.6617990732192993
RMSE train: 0.587040	val: 1.244123	test: 1.349594
MAE train: 0.433507	val: 0.984960	test: 1.090276

Epoch: 70
Loss: 0.8128355890512466
RMSE train: 0.634057	val: 1.265373	test: 1.397690
MAE train: 0.462005	val: 1.024590	test: 1.133876

Epoch: 71
Loss: 0.6827780604362488
RMSE train: 0.704629	val: 1.332619	test: 1.521078
MAE train: 0.512309	val: 1.095009	test: 1.231897

Epoch: 72
Loss: 0.5634842962026596
RMSE train: 0.750099	val: 1.349873	test: 1.545330
MAE train: 0.542163	val: 1.119471	test: 1.261721

Epoch: 73
Loss: 0.7162257581949234
RMSE train: 0.686353	val: 1.302564	test: 1.466013
MAE train: 0.513578	val: 1.077061	test: 1.205902

Epoch: 74
Loss: 0.695133775472641
RMSE train: 0.676195	val: 1.420984	test: 1.588054
MAE train: 0.513780	val: 1.162906	test: 1.295724

Epoch: 75
Loss: 0.6514523327350616
RMSE train: 0.809495	val: 1.527551	test: 1.702095
MAE train: 0.620217	val: 1.244656	test: 1.384622

Epoch: 76
Loss: 0.8359923362731934
RMSE train: 0.738572	val: 1.410852	test: 1.520600
MAE train: 0.562917	val: 1.157887	test: 1.251641

Epoch: 77
Loss: 0.5891904383897781
RMSE train: 0.726939	val: 1.303534	test: 1.347998
MAE train: 0.543255	val: 1.061416	test: 1.098180

Epoch: 78
Loss: 0.6747608929872513
RMSE train: 0.676533	val: 1.273607	test: 1.317611
MAE train: 0.507308	val: 1.046888	test: 1.081750

Epoch: 79
Loss: 0.5905761569738388
RMSE train: 0.623658	val: 1.230410	test: 1.264350
MAE train: 0.478292	val: 1.006718	test: 1.034624

Epoch: 80
Loss: 1.0691091120243073
RMSE train: 0.624005	val: 1.245772	test: 1.298633
MAE train: 0.465022	val: 1.014218	test: 1.061518

Epoch: 81
Loss: 0.7519258409738541
RMSE train: 0.650671	val: 1.318162	test: 1.405394
MAE train: 0.474732	val: 1.072185	test: 1.148471

Epoch: 82
Loss: 0.5748972296714783
RMSE train: 0.687917	val: 1.295746	test: 1.404978
MAE train: 0.513488	val: 1.040552	test: 1.147169

Epoch: 83
Loss: 0.6921603232622147
RMSE train: 0.599051	val: 1.216442	test: 1.339836
MAE train: 0.450237	val: 0.970531	test: 1.104872
RMSE train: 0.827259	val: 1.390841	test: 1.377433
MAE train: 0.618289	val: 1.121111	test: 1.076916

Epoch: 24
Loss: 0.8520445227622986
RMSE train: 0.840611	val: 1.347805	test: 1.316421
MAE train: 0.633767	val: 1.080988	test: 1.011933

Epoch: 25
Loss: 0.7500628232955933
RMSE train: 0.797788	val: 1.445153	test: 1.394455
MAE train: 0.592635	val: 1.174911	test: 1.085736

Epoch: 26
Loss: 0.7459878176450729
RMSE train: 0.762012	val: 1.453390	test: 1.358709
MAE train: 0.560541	val: 1.160119	test: 1.082569

Epoch: 27
Loss: 0.7369751930236816
RMSE train: 0.771600	val: 1.233767	test: 1.229037
MAE train: 0.579659	val: 0.972273	test: 0.972362

Epoch: 28
Loss: 0.7843377441167831
RMSE train: 0.711442	val: 1.382437	test: 1.279606
MAE train: 0.519514	val: 1.104943	test: 0.997370

Epoch: 29
Loss: 0.7739300280809402
RMSE train: 0.664293	val: 1.310472	test: 1.193101
MAE train: 0.501907	val: 1.058023	test: 0.927794

Epoch: 30
Loss: 0.7166672199964523
RMSE train: 0.741682	val: 1.258170	test: 1.284374
MAE train: 0.546357	val: 1.031431	test: 0.976353

Epoch: 31
Loss: 0.6234875321388245
RMSE train: 0.724525	val: 1.292113	test: 1.271000
MAE train: 0.539282	val: 1.015633	test: 0.979178

Epoch: 32
Loss: 0.6397206634283066
RMSE train: 0.709207	val: 1.311238	test: 1.279278
MAE train: 0.526308	val: 1.031448	test: 0.983477

Epoch: 33
Loss: 0.6806777566671371
RMSE train: 0.718838	val: 1.278553	test: 1.294061
MAE train: 0.531533	val: 1.047704	test: 0.970196

Epoch: 34
Loss: 0.5986989438533783
RMSE train: 0.670345	val: 1.277510	test: 1.254242
MAE train: 0.503373	val: 1.019511	test: 0.971218

Epoch: 35
Loss: 0.5969104170799255
RMSE train: 0.782117	val: 1.329960	test: 1.381113
MAE train: 0.584390	val: 1.094105	test: 1.042638

Epoch: 36
Loss: 0.6276623159646988
RMSE train: 0.819387	val: 1.356782	test: 1.379597
MAE train: 0.605238	val: 1.071547	test: 1.042220

Epoch: 37
Loss: 0.5819800347089767
RMSE train: 0.776682	val: 1.386343	test: 1.418446
MAE train: 0.581685	val: 1.111540	test: 1.092009

Epoch: 38
Loss: 0.5798288434743881
RMSE train: 0.702056	val: 1.322656	test: 1.323754
MAE train: 0.521860	val: 1.047155	test: 1.022707

Epoch: 39
Loss: 0.6276001483201981
RMSE train: 0.725350	val: 1.342616	test: 1.325041
MAE train: 0.542918	val: 1.066554	test: 1.016085

Epoch: 40
Loss: 0.5457312166690826
RMSE train: 0.800656	val: 1.395853	test: 1.447552
MAE train: 0.594227	val: 1.135857	test: 1.092777

Epoch: 41
Loss: 0.6249585747718811
RMSE train: 0.650400	val: 1.276589	test: 1.250127
MAE train: 0.483346	val: 1.034708	test: 0.961627

Epoch: 42
Loss: 0.5605477839708328
RMSE train: 0.649941	val: 1.176797	test: 1.226206
MAE train: 0.491363	val: 0.939461	test: 0.933479

Epoch: 43
Loss: 0.5692773014307022
RMSE train: 0.598426	val: 1.295085	test: 1.236123
MAE train: 0.435828	val: 1.059161	test: 0.922257

Epoch: 44
Loss: 0.6457399874925613
RMSE train: 0.644251	val: 1.249919	test: 1.263320
MAE train: 0.469606	val: 1.000531	test: 0.936219

Epoch: 45
Loss: 0.6143848299980164
RMSE train: 0.631320	val: 1.252647	test: 1.214768
MAE train: 0.463217	val: 0.992726	test: 0.948629

Epoch: 46
Loss: 0.5829924121499062
RMSE train: 0.695098	val: 1.271021	test: 1.259291
MAE train: 0.503125	val: 1.001385	test: 0.960781

Epoch: 47
Loss: 0.5386942699551582
RMSE train: 0.697340	val: 1.304138	test: 1.312302
MAE train: 0.510099	val: 1.044025	test: 0.990025

Epoch: 48
Loss: 0.5575455874204636
RMSE train: 0.649094	val: 1.243274	test: 1.249808
MAE train: 0.481869	val: 0.983539	test: 0.959539

Epoch: 49
Loss: 0.48584113270044327
RMSE train: 0.665496	val: 1.269334	test: 1.286089
MAE train: 0.487588	val: 1.009099	test: 0.987420

Epoch: 50
Loss: 0.5035810992121696
RMSE train: 0.672091	val: 1.286607	test: 1.281757
MAE train: 0.487171	val: 1.031190	test: 0.985750

Epoch: 51
Loss: 0.5034995004534721
RMSE train: 0.640888	val: 1.234632	test: 1.219906
MAE train: 0.462650	val: 0.984620	test: 0.938258

Epoch: 52
Loss: 0.47206054627895355
RMSE train: 0.618325	val: 1.205832	test: 1.206609
MAE train: 0.451662	val: 0.973692	test: 0.916162

Epoch: 53
Loss: 0.45536694675683975
RMSE train: 0.639087	val: 1.241770	test: 1.247313
MAE train: 0.466075	val: 1.003267	test: 0.934982

Epoch: 54
Loss: 0.43680739402770996
RMSE train: 0.647057	val: 1.214686	test: 1.228446
MAE train: 0.480435	val: 0.977637	test: 0.925298

Epoch: 55
Loss: 0.4714055135846138
RMSE train: 0.622526	val: 1.158083	test: 1.214568
MAE train: 0.461240	val: 0.967826	test: 0.913282

Epoch: 56
Loss: 0.42080362141132355
RMSE train: 0.625212	val: 1.197014	test: 1.226707
MAE train: 0.458905	val: 0.988939	test: 0.945057

Epoch: 57
Loss: 0.4375074803829193
RMSE train: 0.667089	val: 1.230019	test: 1.251040
MAE train: 0.494454	val: 0.996167	test: 0.972916

Epoch: 58
Loss: 0.4396389573812485
RMSE train: 0.693525	val: 1.276507	test: 1.301543
MAE train: 0.506205	val: 1.060949	test: 0.981465

Epoch: 59
Loss: 0.4457462206482887
RMSE train: 0.653791	val: 1.196882	test: 1.235859
MAE train: 0.483518	val: 0.980823	test: 0.961216

Epoch: 60
Loss: 0.438280925154686
RMSE train: 0.630309	val: 1.196411	test: 1.241315
MAE train: 0.471410	val: 0.982248	test: 0.974589

Epoch: 61
Loss: 0.40372204780578613
RMSE train: 0.646504	val: 1.387831	test: 1.336774
MAE train: 0.473648	val: 1.096777	test: 1.053511

Epoch: 62
Loss: 0.4589716047048569
RMSE train: 0.640821	val: 1.295888	test: 1.277740
MAE train: 0.469290	val: 1.052131	test: 0.993731

Epoch: 63
Loss: 0.44267695397138596
RMSE train: 0.628762	val: 1.217793	test: 1.212967
MAE train: 0.462014	val: 0.984352	test: 0.935788

Epoch: 64
Loss: 0.38077671080827713
RMSE train: 0.636034	val: 1.256511	test: 1.257347
MAE train: 0.464815	val: 1.014547	test: 0.958290

Epoch: 65
Loss: 0.41014987975358963
RMSE train: 0.589720	val: 1.184653	test: 1.223779
MAE train: 0.433647	val: 0.969217	test: 0.943690

Epoch: 66
Loss: 0.5316452011466026
RMSE train: 0.559388	val: 1.207194	test: 1.211568
MAE train: 0.417375	val: 0.956433	test: 0.968099

Epoch: 67
Loss: 0.46297869831323624
RMSE train: 0.573719	val: 1.173315	test: 1.206313
MAE train: 0.416984	val: 0.973156	test: 0.940330

Epoch: 68
Loss: 0.3891064450144768
RMSE train: 0.565766	val: 1.199705	test: 1.196798
MAE train: 0.410214	val: 0.979310	test: 0.939496

Epoch: 69
Loss: 0.4199993312358856
RMSE train: 0.604627	val: 1.185263	test: 1.203220
MAE train: 0.445597	val: 0.984505	test: 0.923733

Epoch: 70
Loss: 0.40097129344940186
RMSE train: 0.597491	val: 1.186574	test: 1.191841
MAE train: 0.434984	val: 0.978362	test: 0.889550

Epoch: 71
Loss: 0.35788844525814056
RMSE train: 0.558288	val: 1.014401	test: 1.114166
MAE train: 0.418184	val: 0.846780	test: 0.862680

Epoch: 72
Loss: 0.39316973835229874
RMSE train: 0.574251	val: 1.168552	test: 1.140628
MAE train: 0.409340	val: 0.947437	test: 0.878929

Epoch: 73
Loss: 0.42260680347681046
RMSE train: 0.577963	val: 1.194043	test: 1.195097
MAE train: 0.416771	val: 0.969501	test: 0.920969

Epoch: 74
Loss: 0.3725769594311714
RMSE train: 0.545942	val: 1.130834	test: 1.182587
MAE train: 0.396865	val: 0.937677	test: 0.907754

Epoch: 75
Loss: 0.4253307208418846
RMSE train: 0.512285	val: 1.130423	test: 1.134943
MAE train: 0.380670	val: 0.901419	test: 0.908991

Epoch: 76
Loss: 0.38398002833127975
RMSE train: 0.512028	val: 1.069794	test: 1.128554
MAE train: 0.383290	val: 0.874435	test: 0.884537

Epoch: 77
Loss: 0.3859448805451393
RMSE train: 0.500540	val: 1.227969	test: 1.149919
MAE train: 0.373447	val: 0.983355	test: 0.906584

Epoch: 78
Loss: 0.4044961929321289
RMSE train: 0.495215	val: 1.198611	test: 1.132835
MAE train: 0.365053	val: 0.961559	test: 0.882752

Epoch: 79
Loss: 0.36834852397441864
RMSE train: 0.595365	val: 1.179794	test: 1.199083
MAE train: 0.426689	val: 0.981966	test: 0.889299

Epoch: 80
Loss: 0.3551648184657097
RMSE train: 0.582619	val: 1.220446	test: 1.192171
MAE train: 0.416906	val: 0.982630	test: 0.900388

Epoch: 81
Loss: 0.34137724339962006
RMSE train: 0.578725	val: 1.169475	test: 1.170019
MAE train: 0.427085	val: 0.955241	test: 0.884250

Epoch: 82
Loss: 0.3862299695611
RMSE train: 0.646626	val: 1.331459	test: 1.249345
MAE train: 0.478648	val: 1.049060	test: 0.959195

Epoch: 83
Loss: 0.38216759264469147
RMSE train: 0.608774	val: 1.284424	test: 1.194882
MAE train: 0.453742	val: 1.027004	test: 0.926728
RMSE train: 0.828549	val: 1.399237	test: 1.477029
MAE train: 0.607098	val: 1.115532	test: 1.176677

Epoch: 24
Loss: 0.9522566646337509
RMSE train: 0.779473	val: 1.449336	test: 1.500821
MAE train: 0.574601	val: 1.136315	test: 1.213467

Epoch: 25
Loss: 0.8541140705347061
RMSE train: 0.690003	val: 1.389121	test: 1.407199
MAE train: 0.524153	val: 1.081058	test: 1.131396

Epoch: 26
Loss: 0.8074086159467697
RMSE train: 0.696564	val: 1.358660	test: 1.371766
MAE train: 0.528728	val: 1.069250	test: 1.111047

Epoch: 27
Loss: 0.7961976230144501
RMSE train: 0.728826	val: 1.326948	test: 1.357127
MAE train: 0.542540	val: 1.072391	test: 1.090127

Epoch: 28
Loss: 0.7850626558065414
RMSE train: 0.687859	val: 1.262801	test: 1.210966
MAE train: 0.529824	val: 1.029696	test: 0.985807

Epoch: 29
Loss: 0.7531063109636307
RMSE train: 0.694478	val: 1.387311	test: 1.301923
MAE train: 0.525707	val: 1.106889	test: 1.043590

Epoch: 30
Loss: 0.7678952515125275
RMSE train: 0.737283	val: 1.386085	test: 1.339921
MAE train: 0.544038	val: 1.098053	test: 1.058371

Epoch: 31
Loss: 0.7405868768692017
RMSE train: 0.683636	val: 1.382262	test: 1.279736
MAE train: 0.512213	val: 1.102200	test: 1.021968

Epoch: 32
Loss: 0.7273480147123337
RMSE train: 0.723077	val: 1.532753	test: 1.352810
MAE train: 0.557094	val: 1.229506	test: 1.096025

Epoch: 33
Loss: 0.7229093611240387
RMSE train: 0.694416	val: 1.219626	test: 1.241499
MAE train: 0.522158	val: 0.978299	test: 0.977658

Epoch: 34
Loss: 0.7604971528053284
RMSE train: 0.710068	val: 1.317399	test: 1.270258
MAE train: 0.537100	val: 1.047700	test: 0.981805

Epoch: 35
Loss: 0.6852217763662338
RMSE train: 0.745277	val: 1.546362	test: 1.391237
MAE train: 0.566690	val: 1.220281	test: 1.085240

Epoch: 36
Loss: 0.6671599745750427
RMSE train: 0.664369	val: 1.217550	test: 1.180846
MAE train: 0.494542	val: 0.985144	test: 0.909454

Epoch: 37
Loss: 0.6492063850164413
RMSE train: 0.658791	val: 1.209561	test: 1.204826
MAE train: 0.491585	val: 0.978545	test: 0.924140

Epoch: 38
Loss: 0.6038049459457397
RMSE train: 0.654398	val: 1.366831	test: 1.272644
MAE train: 0.506333	val: 1.104617	test: 1.030666

Epoch: 39
Loss: 0.5994589775800705
RMSE train: 0.601658	val: 1.226256	test: 1.205669
MAE train: 0.459986	val: 1.000372	test: 0.956439

Epoch: 40
Loss: 0.5907397419214249
RMSE train: 0.622476	val: 1.231178	test: 1.228836
MAE train: 0.466366	val: 1.010026	test: 0.941249

Epoch: 41
Loss: 0.56608647108078
RMSE train: 0.699945	val: 1.402788	test: 1.290822
MAE train: 0.535254	val: 1.148987	test: 0.997324

Epoch: 42
Loss: 0.653057262301445
RMSE train: 0.589226	val: 1.243817	test: 1.222267
MAE train: 0.446112	val: 1.006681	test: 0.965079

Epoch: 43
Loss: 0.6525867432355881
RMSE train: 0.582794	val: 1.295934	test: 1.261349
MAE train: 0.445926	val: 1.043271	test: 1.011365

Epoch: 44
Loss: 0.6012222766876221
RMSE train: 0.639943	val: 1.344020	test: 1.273647
MAE train: 0.484446	val: 1.079831	test: 0.977359

Epoch: 45
Loss: 0.56794074177742
RMSE train: 0.668493	val: 1.293711	test: 1.289017
MAE train: 0.496029	val: 1.060483	test: 0.983135

Epoch: 46
Loss: 0.6098449975252151
RMSE train: 0.628270	val: 1.363935	test: 1.270490
MAE train: 0.468666	val: 1.104906	test: 0.975624

Epoch: 47
Loss: 0.5144639387726784
RMSE train: 0.618868	val: 1.216633	test: 1.228863
MAE train: 0.458075	val: 1.000904	test: 0.937826

Epoch: 48
Loss: 0.5831863880157471
RMSE train: 0.588288	val: 1.256672	test: 1.235957
MAE train: 0.440164	val: 1.023089	test: 0.974181

Epoch: 49
Loss: 0.5473372638225555
RMSE train: 0.635435	val: 1.329925	test: 1.268942
MAE train: 0.481190	val: 1.070152	test: 1.014368

Epoch: 50
Loss: 0.5435042977333069
RMSE train: 0.614057	val: 1.177089	test: 1.195883
MAE train: 0.453905	val: 0.977631	test: 0.948501

Epoch: 51
Loss: 0.5546259433031082
RMSE train: 0.600932	val: 1.275593	test: 1.235519
MAE train: 0.458748	val: 1.028317	test: 0.993288

Epoch: 52
Loss: 0.4973425194621086
RMSE train: 0.681296	val: 1.431007	test: 1.353217
MAE train: 0.515224	val: 1.131664	test: 1.084235

Epoch: 53
Loss: 0.5374788120388985
RMSE train: 0.605328	val: 1.147147	test: 1.201595
MAE train: 0.448751	val: 0.938079	test: 0.939675

Epoch: 54
Loss: 0.5267089754343033
RMSE train: 0.560918	val: 1.100842	test: 1.124344
MAE train: 0.424207	val: 0.901233	test: 0.884246

Epoch: 55
Loss: 0.5970389395952225
RMSE train: 0.584001	val: 1.275377	test: 1.230667
MAE train: 0.444157	val: 1.045965	test: 0.955192

Epoch: 56
Loss: 0.5150980204343796
RMSE train: 0.619829	val: 1.152378	test: 1.174925
MAE train: 0.449999	val: 0.987387	test: 0.885848

Epoch: 57
Loss: 0.49517112225294113
RMSE train: 0.615405	val: 1.354021	test: 1.284595
MAE train: 0.466841	val: 1.094341	test: 1.014384

Epoch: 58
Loss: 0.45353177189826965
RMSE train: 0.600548	val: 1.198394	test: 1.211378
MAE train: 0.450650	val: 1.002615	test: 0.961688

Epoch: 59
Loss: 0.4856177270412445
RMSE train: 0.687511	val: 1.204483	test: 1.239477
MAE train: 0.511406	val: 1.008430	test: 0.970633

Epoch: 60
Loss: 0.4414464831352234
RMSE train: 0.670036	val: 1.309622	test: 1.268373
MAE train: 0.502155	val: 1.061317	test: 0.993254

Epoch: 61
Loss: 0.44628778100013733
RMSE train: 0.598423	val: 1.262360	test: 1.202389
MAE train: 0.444907	val: 1.035358	test: 0.935215

Epoch: 62
Loss: 0.4736153408885002
RMSE train: 0.556062	val: 1.100801	test: 1.127514
MAE train: 0.406086	val: 0.921085	test: 0.885156

Epoch: 63
Loss: 0.45276422798633575
RMSE train: 0.576409	val: 1.220334	test: 1.186950
MAE train: 0.437621	val: 1.007963	test: 0.929698

Epoch: 64
Loss: 0.43556174635887146
RMSE train: 0.581656	val: 1.265841	test: 1.168240
MAE train: 0.439184	val: 1.045324	test: 0.894707

Epoch: 65
Loss: 0.48886551707983017
RMSE train: 0.583502	val: 1.133683	test: 1.133485
MAE train: 0.436394	val: 0.939986	test: 0.860288

Epoch: 66
Loss: 0.4302464723587036
RMSE train: 0.564828	val: 1.178389	test: 1.168939
MAE train: 0.426874	val: 0.977081	test: 0.902797

Epoch: 67
Loss: 0.4474312588572502
RMSE train: 0.518632	val: 1.257656	test: 1.234313
MAE train: 0.385922	val: 1.035741	test: 0.942383

Epoch: 68
Loss: 0.4164750277996063
RMSE train: 0.514218	val: 1.179625	test: 1.145839
MAE train: 0.382625	val: 0.984341	test: 0.891428

Epoch: 69
Loss: 0.4101059064269066
RMSE train: 0.516784	val: 1.227822	test: 1.153803
MAE train: 0.391271	val: 1.019264	test: 0.889081

Epoch: 70
Loss: 0.3816779628396034
RMSE train: 0.562574	val: 1.249002	test: 1.178784
MAE train: 0.428042	val: 1.041333	test: 0.900885

Epoch: 71
Loss: 0.44586996734142303
RMSE train: 0.569335	val: 1.080995	test: 1.122923
MAE train: 0.415218	val: 0.918855	test: 0.862979

Epoch: 72
Loss: 0.42862722277641296
RMSE train: 0.550345	val: 1.135525	test: 1.176515
MAE train: 0.401499	val: 0.958576	test: 0.910135

Epoch: 73
Loss: 0.401755727827549
RMSE train: 0.532477	val: 1.256892	test: 1.250738
MAE train: 0.391475	val: 1.037404	test: 0.981188

Epoch: 74
Loss: 0.36876654624938965
RMSE train: 0.608836	val: 1.291745	test: 1.293914
MAE train: 0.451814	val: 1.061874	test: 1.010027

Epoch: 75
Loss: 0.43773651123046875
RMSE train: 0.547222	val: 1.144594	test: 1.215868
MAE train: 0.411393	val: 0.970521	test: 0.975250

Epoch: 76
Loss: 0.4769286513328552
RMSE train: 0.491034	val: 1.137238	test: 1.201778
MAE train: 0.367712	val: 0.966934	test: 0.962387

Epoch: 77
Loss: 0.3732564449310303
RMSE train: 0.581780	val: 1.255769	test: 1.287336
MAE train: 0.430546	val: 1.048202	test: 0.977376

Epoch: 78
Loss: 0.38596200942993164
RMSE train: 0.569861	val: 1.193936	test: 1.228384
MAE train: 0.430552	val: 1.006992	test: 0.941121

Epoch: 79
Loss: 0.35076872259378433
RMSE train: 0.515899	val: 1.151720	test: 1.174159
MAE train: 0.395225	val: 0.966912	test: 0.914491

Epoch: 80
Loss: 0.3839273154735565
RMSE train: 0.522796	val: 1.171858	test: 1.178744
MAE train: 0.399302	val: 0.982794	test: 0.903645

Epoch: 81
Loss: 0.4045066237449646
RMSE train: 0.565676	val: 1.220218	test: 1.238716
MAE train: 0.426101	val: 1.041308	test: 0.947274

Epoch: 82
Loss: 0.3931392878293991
RMSE train: 0.527967	val: 1.247077	test: 1.263150
MAE train: 0.397194	val: 1.040635	test: 0.964718

Epoch: 83
Loss: 0.34990982711315155
RMSE train: 0.546389	val: 1.268633	test: 1.284380
MAE train: 0.409301	val: 1.052273	test: 0.990138

Epoch: 84
Loss: 0.36142340302467346
RMSE train: 0.508525	val: 1.535085	test: 1.486387
MAE train: 0.378308	val: 1.246873	test: 1.196615

Epoch: 85
Loss: 0.4222315549850464
RMSE train: 0.552456	val: 1.525735	test: 1.517086
MAE train: 0.409891	val: 1.243698	test: 1.214605

Epoch: 86
Loss: 0.4461572865645091
RMSE train: 0.526779	val: 1.532880	test: 1.564942
MAE train: 0.390386	val: 1.234177	test: 1.252260

Epoch: 87
Loss: 0.3488508661588033
RMSE train: 0.509701	val: 1.523328	test: 1.532434
MAE train: 0.379167	val: 1.231028	test: 1.231615

Epoch: 88
Loss: 0.3846260408560435
RMSE train: 0.522339	val: 1.526230	test: 1.536178
MAE train: 0.384216	val: 1.237029	test: 1.238124

Epoch: 89
Loss: 0.40339428186416626
RMSE train: 0.540119	val: 1.510727	test: 1.544620
MAE train: 0.404863	val: 1.220450	test: 1.234621

Epoch: 90
Loss: 0.3676776587963104
RMSE train: 0.492638	val: 1.530884	test: 1.631859
MAE train: 0.374993	val: 1.211714	test: 1.291123

Epoch: 91
Loss: 0.3398067355155945
RMSE train: 0.518490	val: 1.516969	test: 1.645239
MAE train: 0.387739	val: 1.196970	test: 1.302107

Epoch: 92
Loss: 0.3460131088892619
RMSE train: 0.540312	val: 1.498484	test: 1.576131
MAE train: 0.393917	val: 1.196734	test: 1.254554

Epoch: 93
Loss: 0.34293054540952045
RMSE train: 0.535232	val: 1.525537	test: 1.543277
MAE train: 0.391024	val: 1.221117	test: 1.235588

Epoch: 94
Loss: 0.35033683975537616
RMSE train: 0.526773	val: 1.517089	test: 1.571905
MAE train: 0.387926	val: 1.211004	test: 1.246899

Epoch: 95
Loss: 0.31194673975308734
RMSE train: 0.511790	val: 1.512844	test: 1.559072
MAE train: 0.381120	val: 1.219174	test: 1.239910

Epoch: 96
Loss: 0.32211431364218396
RMSE train: 0.508383	val: 1.501943	test: 1.514754
MAE train: 0.374119	val: 1.215950	test: 1.205516

Epoch: 97
Loss: 0.390452762444814
RMSE train: 0.502455	val: 1.501452	test: 1.499382
MAE train: 0.372698	val: 1.213114	test: 1.192171

Epoch: 98
Loss: 0.3464725414911906
RMSE train: 0.536831	val: 1.518201	test: 1.508979
MAE train: 0.399163	val: 1.213840	test: 1.188460

Epoch: 99
Loss: 0.3793560763200124
RMSE train: 0.509451	val: 1.530041	test: 1.499755
MAE train: 0.377880	val: 1.219044	test: 1.188412

Epoch: 100
Loss: 0.3819725612799327
RMSE train: 0.472734	val: 1.529328	test: 1.524147
MAE train: 0.350642	val: 1.226834	test: 1.212130

Epoch: 101
Loss: 0.3781048158804576
RMSE train: 0.514922	val: 1.587400	test: 1.625968
MAE train: 0.388650	val: 1.294187	test: 1.287073

Epoch: 102
Loss: 0.35509273409843445
RMSE train: 0.521905	val: 1.571438	test: 1.576637
MAE train: 0.385866	val: 1.290704	test: 1.250183

Epoch: 103
Loss: 0.33288877209027606
RMSE train: 0.506877	val: 1.605279	test: 1.591935
MAE train: 0.371942	val: 1.302324	test: 1.265897

Epoch: 104
Loss: 0.34871451059977215
RMSE train: 0.497638	val: 1.644749	test: 1.653891
MAE train: 0.365264	val: 1.328030	test: 1.301837

Epoch: 105
Loss: 0.2996746400992076
RMSE train: 0.562793	val: 1.710469	test: 1.682955
MAE train: 0.420787	val: 1.379908	test: 1.331105

Epoch: 106
Loss: 0.31986575325330097
RMSE train: 0.562395	val: 1.673697	test: 1.597326
MAE train: 0.407933	val: 1.364957	test: 1.269702

Epoch: 107
Loss: 0.3347309430440267
RMSE train: 0.529035	val: 1.669702	test: 1.571195
MAE train: 0.383593	val: 1.362477	test: 1.254020

Epoch: 108
Loss: 0.3699011504650116
RMSE train: 0.476842	val: 1.752361	test: 1.788878
MAE train: 0.360043	val: 1.399705	test: 1.418799

Epoch: 109
Loss: 0.33621026078859967
RMSE train: 0.577435	val: 1.601890	test: 1.674417
MAE train: 0.420745	val: 1.311523	test: 1.326599

Epoch: 110
Loss: 0.32720209161440533
RMSE train: 0.543045	val: 1.569231	test: 1.642936
MAE train: 0.393708	val: 1.258703	test: 1.304969

Epoch: 111
Loss: 0.3318135341008504
RMSE train: 0.485178	val: 1.554752	test: 1.659568
MAE train: 0.351400	val: 1.250892	test: 1.313392

Epoch: 112
Loss: 0.34272510806719464
RMSE train: 0.474008	val: 1.568231	test: 1.712994
MAE train: 0.346693	val: 1.255478	test: 1.348902

Epoch: 113
Loss: 0.33260804414749146
RMSE train: 0.498427	val: 1.600976	test: 1.766535
MAE train: 0.368602	val: 1.276020	test: 1.378285

Epoch: 114
Loss: 0.3100009361902873
RMSE train: 0.511067	val: 1.560445	test: 1.640947
MAE train: 0.380504	val: 1.225324	test: 1.289768

Epoch: 115
Loss: 0.3290782372156779
RMSE train: 0.507070	val: 1.551973	test: 1.595831
MAE train: 0.371376	val: 1.216117	test: 1.264960

Epoch: 116
Loss: 0.3331311841805776
RMSE train: 0.471058	val: 1.589429	test: 1.701338
MAE train: 0.354731	val: 1.237861	test: 1.333877

Epoch: 117
Loss: 0.3327968418598175
RMSE train: 0.430004	val: 1.588035	test: 1.649557
MAE train: 0.324132	val: 1.232428	test: 1.307438

Epoch: 118
Loss: 0.27875857055187225
RMSE train: 0.465529	val: 1.549570	test: 1.553135
MAE train: 0.339514	val: 1.214027	test: 1.233657

Epoch: 119
Loss: 0.31529147426287335
RMSE train: 0.476773	val: 1.554764	test: 1.589071
MAE train: 0.352582	val: 1.213703	test: 1.254249

Epoch: 120
Loss: 0.31926490863164264
RMSE train: 0.446724	val: 1.570215	test: 1.598846
MAE train: 0.332786	val: 1.213893	test: 1.261471

Epoch: 121
Loss: 0.3097299039363861
RMSE train: 0.445834	val: 1.586302	test: 1.502519
MAE train: 0.334066	val: 1.235677	test: 1.207760

Early stopping
Best (RMSE):	 train: 0.500444	val: 1.498430	test: 1.490168
Best (MAE):	 train: 0.369342	val: 1.213415	test: 1.198319

RMSE train: 0.746360	val: 1.289511	test: 1.307664
MAE train: 0.571841	val: 1.035783	test: 1.023584

Epoch: 24
Loss: 0.9734636098146439
RMSE train: 0.705346	val: 1.229806	test: 1.308746
MAE train: 0.546566	val: 0.975445	test: 1.086375

Epoch: 25
Loss: 0.9006032645702362
RMSE train: 0.724500	val: 1.304345	test: 1.297902
MAE train: 0.561406	val: 1.035087	test: 1.072235

Epoch: 26
Loss: 0.8302189707756042
RMSE train: 0.686393	val: 1.202328	test: 1.209611
MAE train: 0.525908	val: 0.955697	test: 0.955872

Epoch: 27
Loss: 0.7963978201150894
RMSE train: 0.682600	val: 1.149690	test: 1.203713
MAE train: 0.524431	val: 0.906908	test: 0.974317

Epoch: 28
Loss: 0.8098654896020889
RMSE train: 0.690461	val: 1.238003	test: 1.283754
MAE train: 0.521897	val: 0.985401	test: 1.058157

Epoch: 29
Loss: 0.7919744700193405
RMSE train: 0.727984	val: 1.388341	test: 1.449876
MAE train: 0.553519	val: 1.090608	test: 1.176605

Epoch: 30
Loss: 0.7927680760622025
RMSE train: 0.757351	val: 1.419343	test: 1.463446
MAE train: 0.585422	val: 1.128781	test: 1.183946

Epoch: 31
Loss: 0.7270182371139526
RMSE train: 0.629731	val: 1.175062	test: 1.273343
MAE train: 0.484106	val: 0.943760	test: 0.998929

Epoch: 32
Loss: 0.7303569912910461
RMSE train: 0.641071	val: 1.364725	test: 1.308128
MAE train: 0.484957	val: 1.076820	test: 1.068405

Epoch: 33
Loss: 0.7836855053901672
RMSE train: 0.651490	val: 1.207884	test: 1.222553
MAE train: 0.490660	val: 0.971402	test: 0.964463

Epoch: 34
Loss: 0.6975586861371994
RMSE train: 0.672170	val: 1.108812	test: 1.243696
MAE train: 0.506187	val: 0.907798	test: 0.981557

Epoch: 35
Loss: 0.693779781460762
RMSE train: 0.696312	val: 1.210345	test: 1.291490
MAE train: 0.531435	val: 0.951415	test: 1.027387

Epoch: 36
Loss: 0.6882638484239578
RMSE train: 0.676332	val: 1.245249	test: 1.288977
MAE train: 0.508611	val: 0.994243	test: 1.035031

Epoch: 37
Loss: 0.6624418497085571
RMSE train: 0.682145	val: 1.234824	test: 1.289030
MAE train: 0.513682	val: 1.000751	test: 1.022442

Epoch: 38
Loss: 0.6459868252277374
RMSE train: 0.634264	val: 1.158640	test: 1.239270
MAE train: 0.481942	val: 0.934504	test: 0.978448

Epoch: 39
Loss: 0.6452991366386414
RMSE train: 0.591443	val: 1.087510	test: 1.180389
MAE train: 0.448334	val: 0.881440	test: 0.920676

Epoch: 40
Loss: 0.6644114702939987
RMSE train: 0.604322	val: 1.030373	test: 1.200564
MAE train: 0.451835	val: 0.843988	test: 0.932791

Epoch: 41
Loss: 0.6744055598974228
RMSE train: 0.673191	val: 1.236111	test: 1.329321
MAE train: 0.509124	val: 0.990760	test: 1.042150

Epoch: 42
Loss: 0.6362167000770569
RMSE train: 0.687183	val: 1.236152	test: 1.361566
MAE train: 0.514988	val: 1.012695	test: 1.053977

Epoch: 43
Loss: 0.5639569014310837
RMSE train: 0.656376	val: 1.077996	test: 1.278440
MAE train: 0.475568	val: 0.923581	test: 0.983915

Epoch: 44
Loss: 0.6040516644716263
RMSE train: 0.623531	val: 1.129454	test: 1.259677
MAE train: 0.456976	val: 0.927718	test: 0.967823

Epoch: 45
Loss: 0.5996380001306534
RMSE train: 0.610364	val: 1.158475	test: 1.243560
MAE train: 0.444235	val: 0.952301	test: 0.962240

Epoch: 46
Loss: 0.6073234528303146
RMSE train: 0.576540	val: 1.085082	test: 1.143421
MAE train: 0.428514	val: 0.872483	test: 0.893384

Epoch: 47
Loss: 0.5837917178869247
RMSE train: 0.571356	val: 1.145945	test: 1.195939
MAE train: 0.423296	val: 0.952976	test: 0.933543

Epoch: 48
Loss: 0.5607058554887772
RMSE train: 0.545367	val: 1.137907	test: 1.159480
MAE train: 0.407900	val: 0.936519	test: 0.900835

Epoch: 49
Loss: 0.5876486897468567
RMSE train: 0.551787	val: 1.247245	test: 1.165136
MAE train: 0.413953	val: 0.986655	test: 0.919815

Epoch: 50
Loss: 0.6002404987812042
RMSE train: 0.610227	val: 1.297045	test: 1.197490
MAE train: 0.462305	val: 1.011612	test: 0.921258

Epoch: 51
Loss: 0.5420684069395065
RMSE train: 0.584195	val: 1.034017	test: 1.144907
MAE train: 0.442992	val: 0.842776	test: 0.907417

Epoch: 52
Loss: 0.5442180782556534
RMSE train: 0.540228	val: 1.184412	test: 1.118450
MAE train: 0.414793	val: 0.913197	test: 0.902868

Epoch: 53
Loss: 0.5404125899076462
RMSE train: 0.537896	val: 1.196854	test: 1.126244
MAE train: 0.409482	val: 0.954067	test: 0.865219

Epoch: 54
Loss: 0.5172552764415741
RMSE train: 0.533708	val: 1.107148	test: 1.122105
MAE train: 0.405613	val: 0.903853	test: 0.861048

Epoch: 55
Loss: 0.5060818642377853
RMSE train: 0.563094	val: 1.202483	test: 1.156422
MAE train: 0.429736	val: 0.964960	test: 0.888288

Epoch: 56
Loss: 0.5162599012255669
RMSE train: 0.585119	val: 1.181707	test: 1.195824
MAE train: 0.432684	val: 0.968526	test: 0.919251

Epoch: 57
Loss: 0.4928400442004204
RMSE train: 0.549553	val: 1.198013	test: 1.175286
MAE train: 0.406839	val: 0.970973	test: 0.903238

Epoch: 58
Loss: 0.5014001056551933
RMSE train: 0.538093	val: 1.184093	test: 1.131720
MAE train: 0.403910	val: 0.945881	test: 0.874536

Epoch: 59
Loss: 0.5215786397457123
RMSE train: 0.525001	val: 1.093721	test: 1.115219
MAE train: 0.386017	val: 0.883274	test: 0.861984

Epoch: 60
Loss: 0.4789144992828369
RMSE train: 0.546619	val: 1.059385	test: 1.134776
MAE train: 0.397039	val: 0.883944	test: 0.868572

Epoch: 61
Loss: 0.4802888557314873
RMSE train: 0.659347	val: 1.273236	test: 1.264784
MAE train: 0.499435	val: 0.998931	test: 0.943275

Epoch: 62
Loss: 0.5386399626731873
RMSE train: 0.554782	val: 1.094064	test: 1.144887
MAE train: 0.410985	val: 0.912760	test: 0.855823

Epoch: 63
Loss: 0.48473475873470306
RMSE train: 0.518790	val: 1.082409	test: 1.109473
MAE train: 0.386117	val: 0.873381	test: 0.849570

Epoch: 64
Loss: 0.4678626209497452
RMSE train: 0.619024	val: 1.306392	test: 1.256987
MAE train: 0.471373	val: 1.042750	test: 0.945510

Epoch: 65
Loss: 0.4721943885087967
RMSE train: 0.546206	val: 1.228769	test: 1.188373
MAE train: 0.411492	val: 0.980780	test: 0.900905

Epoch: 66
Loss: 0.4779154881834984
RMSE train: 0.519734	val: 1.107117	test: 1.115909
MAE train: 0.386254	val: 0.897626	test: 0.858236

Epoch: 67
Loss: 0.5026661828160286
RMSE train: 0.579898	val: 1.246145	test: 1.235966
MAE train: 0.439255	val: 1.017864	test: 0.935225

Epoch: 68
Loss: 0.4229630008339882
RMSE train: 0.541400	val: 1.177378	test: 1.186800
MAE train: 0.393181	val: 0.990466	test: 0.911338

Epoch: 69
Loss: 0.47229525446891785
RMSE train: 0.495002	val: 1.223670	test: 1.149365
MAE train: 0.370144	val: 0.982644	test: 0.904490

Epoch: 70
Loss: 0.47418251633644104
RMSE train: 0.535601	val: 1.242492	test: 1.194515
MAE train: 0.398180	val: 0.994087	test: 0.925768

Epoch: 71
Loss: 0.48518193513154984
RMSE train: 0.606896	val: 1.297378	test: 1.274877
MAE train: 0.455377	val: 1.049120	test: 0.974127

Epoch: 72
Loss: 0.4999588131904602
RMSE train: 0.493023	val: 1.207154	test: 1.143641
MAE train: 0.366785	val: 0.978895	test: 0.890121

Epoch: 73
Loss: 0.4600181132555008
RMSE train: 0.500601	val: 1.168801	test: 1.140766
MAE train: 0.373110	val: 0.956834	test: 0.899762

Epoch: 74
Loss: 0.4212837293744087
RMSE train: 0.552176	val: 1.283614	test: 1.258816
MAE train: 0.418101	val: 1.031887	test: 0.984508

Epoch: 75
Loss: 0.4339253902435303
RMSE train: 0.520169	val: 1.217964	test: 1.200862
MAE train: 0.390671	val: 0.983203	test: 0.938542

Epoch: 76
Loss: 0.40440845489501953
RMSE train: 0.483274	val: 1.159195	test: 1.158377
MAE train: 0.362151	val: 0.947502	test: 0.910432

Epoch: 77
Loss: 0.40804920345544815
RMSE train: 0.538860	val: 1.337189	test: 1.216691
MAE train: 0.410477	val: 1.071574	test: 0.948147

Epoch: 78
Loss: 0.42072273790836334
RMSE train: 0.528584	val: 1.242161	test: 1.191257
MAE train: 0.397352	val: 1.004622	test: 0.925916

Epoch: 79
Loss: 0.4037545248866081
RMSE train: 0.499411	val: 1.134789	test: 1.138763
MAE train: 0.374036	val: 0.937543	test: 0.884450

Epoch: 80
Loss: 0.40072067826986313
RMSE train: 0.544279	val: 1.299253	test: 1.233372
MAE train: 0.419752	val: 1.044748	test: 0.949827

Epoch: 81
Loss: 0.4080716669559479
RMSE train: 0.520666	val: 1.204045	test: 1.180582
MAE train: 0.392775	val: 0.984025	test: 0.921509

Epoch: 82
Loss: 0.36910346895456314
RMSE train: 0.545864	val: 1.155994	test: 1.154255
MAE train: 0.407096	val: 0.951995	test: 0.886993

Epoch: 83
Loss: 0.3868550509214401
RMSE train: 0.626003	val: 1.264982	test: 1.257499
MAE train: 0.468764	val: 1.031570	test: 0.945322

Epoch: 84
Loss: 0.36782971024513245
RMSE train: 0.528003	val: 1.878196	test: 1.713158
MAE train: 0.384625	val: 1.537701	test: 1.352864

Epoch: 85
Loss: 0.3683045009771983
RMSE train: 0.523986	val: 1.820923	test: 1.686071
MAE train: 0.384405	val: 1.493811	test: 1.327280

Epoch: 86
Loss: 0.37982555230458576
RMSE train: 0.538771	val: 1.821492	test: 1.686193
MAE train: 0.411886	val: 1.476077	test: 1.311776

Epoch: 87
Loss: 0.3320133884747823
RMSE train: 0.469337	val: 1.861866	test: 1.681808
MAE train: 0.341869	val: 1.498543	test: 1.319329

Epoch: 88
Loss: 0.36545825004577637
RMSE train: 0.500014	val: 1.843239	test: 1.657798
MAE train: 0.371295	val: 1.467820	test: 1.299110

Epoch: 89
Loss: 0.3845720887184143
RMSE train: 0.519300	val: 1.818571	test: 1.664630
MAE train: 0.381211	val: 1.436308	test: 1.291031

Epoch: 90
Loss: 0.4123292068640391
RMSE train: 0.476256	val: 1.845252	test: 1.686186
MAE train: 0.358150	val: 1.469325	test: 1.311993

Epoch: 91
Loss: 0.431400587161382
RMSE train: 0.454274	val: 1.874790	test: 1.706791
MAE train: 0.341401	val: 1.511325	test: 1.335006

Epoch: 92
Loss: 0.4080839554468791
RMSE train: 0.486850	val: 1.807003	test: 1.664081
MAE train: 0.361147	val: 1.466102	test: 1.299188

Epoch: 93
Loss: 0.3629130820433299
RMSE train: 0.504343	val: 1.818231	test: 1.708457
MAE train: 0.377894	val: 1.463376	test: 1.326558

Epoch: 94
Loss: 0.40992921590805054
RMSE train: 0.524239	val: 1.808275	test: 1.733690
MAE train: 0.392798	val: 1.446001	test: 1.349864

Epoch: 95
Loss: 0.30920764803886414
RMSE train: 0.529564	val: 1.757577	test: 1.687436
MAE train: 0.390341	val: 1.417167	test: 1.320266

Epoch: 96
Loss: 0.3290146390597026
RMSE train: 0.478590	val: 1.787201	test: 1.750200
MAE train: 0.356659	val: 1.428452	test: 1.378330

Epoch: 97
Loss: 0.34490222732226056
RMSE train: 0.481371	val: 1.746544	test: 1.727009
MAE train: 0.356712	val: 1.399107	test: 1.363281

Epoch: 98
Loss: 0.36536742250124615
RMSE train: 0.533131	val: 1.699921	test: 1.688603
MAE train: 0.396333	val: 1.371874	test: 1.322920

Epoch: 99
Loss: 0.3160601357618968
RMSE train: 0.542410	val: 1.721765	test: 1.692458
MAE train: 0.407611	val: 1.382108	test: 1.320365

Epoch: 100
Loss: 0.3371654947598775
RMSE train: 0.500018	val: 1.845229	test: 1.759633
MAE train: 0.375691	val: 1.463672	test: 1.372193

Epoch: 101
Loss: 0.3118380010128021
RMSE train: 0.477188	val: 1.942710	test: 1.835543
MAE train: 0.361245	val: 1.517278	test: 1.424248

Epoch: 102
Loss: 0.3223459720611572
RMSE train: 0.582114	val: 1.814901	test: 1.722399
MAE train: 0.442981	val: 1.452521	test: 1.332683

Epoch: 103
Loss: 0.3527188499768575
RMSE train: 0.562188	val: 1.849323	test: 1.733362
MAE train: 0.429864	val: 1.464677	test: 1.338336

Epoch: 104
Loss: 0.3426199058691661
RMSE train: 0.498581	val: 1.973115	test: 1.801207
MAE train: 0.378114	val: 1.526009	test: 1.388519

Epoch: 105
Loss: 0.39430739482243854
RMSE train: 0.486695	val: 1.935850	test: 1.757391
MAE train: 0.368188	val: 1.502849	test: 1.354174

Epoch: 106
Loss: 0.304170697927475
RMSE train: 0.500120	val: 1.871779	test: 1.714663
MAE train: 0.376296	val: 1.477809	test: 1.325569

Epoch: 107
Loss: 0.31257455547650653
RMSE train: 0.482643	val: 1.781176	test: 1.687592
MAE train: 0.360587	val: 1.434558	test: 1.315504

Epoch: 108
Loss: 0.319060484568278
RMSE train: 0.468268	val: 1.700800	test: 1.709098
MAE train: 0.354271	val: 1.369057	test: 1.346868

Epoch: 109
Loss: 0.3215885857741038
RMSE train: 0.461576	val: 1.727865	test: 1.713686
MAE train: 0.352700	val: 1.369884	test: 1.353191

Epoch: 110
Loss: 0.3608544071515401
RMSE train: 0.464967	val: 1.826294	test: 1.721593
MAE train: 0.360294	val: 1.432925	test: 1.358946

Epoch: 111
Loss: 0.3206785221894582
RMSE train: 0.483302	val: 1.830892	test: 1.681404
MAE train: 0.369383	val: 1.427071	test: 1.320145

Epoch: 112
Loss: 0.36053500572840375
RMSE train: 0.500696	val: 1.813497	test: 1.706295
MAE train: 0.373890	val: 1.402267	test: 1.334878

Epoch: 113
Loss: 0.3635217746098836
RMSE train: 0.508271	val: 1.831949	test: 1.683318
MAE train: 0.377828	val: 1.426452	test: 1.306797

Epoch: 114
Loss: 0.39749008417129517
RMSE train: 0.442954	val: 1.933031	test: 1.720142
MAE train: 0.335568	val: 1.500376	test: 1.343798

Epoch: 115
Loss: 0.36127497752507526
RMSE train: 0.522809	val: 1.850413	test: 1.689769
MAE train: 0.390045	val: 1.467311	test: 1.317220

Epoch: 116
Loss: 0.32403846581776935
RMSE train: 0.563680	val: 1.838650	test: 1.700275
MAE train: 0.427895	val: 1.454020	test: 1.325693

Epoch: 117
Loss: 0.27886635065078735
RMSE train: 0.521500	val: 1.888704	test: 1.686176
MAE train: 0.390694	val: 1.469989	test: 1.316436

Epoch: 118
Loss: 0.3082470695177714
RMSE train: 0.522427	val: 1.916263	test: 1.658671
MAE train: 0.375411	val: 1.504658	test: 1.293177

Epoch: 119
Loss: 0.3029215137163798
RMSE train: 0.462897	val: 1.951798	test: 1.788630
MAE train: 0.347522	val: 1.477816	test: 1.393367

Epoch: 120
Loss: 0.39923563599586487
RMSE train: 0.546658	val: 1.784144	test: 1.697338
MAE train: 0.410511	val: 1.395617	test: 1.315519

Epoch: 121
Loss: 0.34592026472091675
RMSE train: 0.540141	val: 1.759360	test: 1.645744
MAE train: 0.396133	val: 1.414783	test: 1.279765

Early stopping
Best (RMSE):	 train: 0.575097	val: 1.588425	test: 1.581443
Best (MAE):	 train: 0.426760	val: 1.266782	test: 1.235199


Epoch: 84
Loss: 0.3977496226628621
RMSE train: 0.534259	val: 1.718741	test: 1.618897
MAE train: 0.397809	val: 1.360184	test: 1.254517

Epoch: 85
Loss: 0.3889382680257161
RMSE train: 0.572762	val: 1.708706	test: 1.617546
MAE train: 0.434101	val: 1.353678	test: 1.258847

Epoch: 86
Loss: 0.3877199590206146
RMSE train: 0.567259	val: 1.715061	test: 1.564373
MAE train: 0.406623	val: 1.349058	test: 1.228684

Epoch: 87
Loss: 0.3819308280944824
RMSE train: 0.539851	val: 1.720588	test: 1.513093
MAE train: 0.383847	val: 1.335043	test: 1.189553

Epoch: 88
Loss: 0.3989179730415344
RMSE train: 0.470118	val: 1.755920	test: 1.571457
MAE train: 0.339253	val: 1.351957	test: 1.192893

Epoch: 89
Loss: 0.3950748046239217
RMSE train: 0.577392	val: 1.714697	test: 1.630428
MAE train: 0.423828	val: 1.321910	test: 1.249193

Epoch: 90
Loss: 0.4081806540489197
RMSE train: 0.579828	val: 1.709292	test: 1.621521
MAE train: 0.424553	val: 1.316466	test: 1.255639

Epoch: 91
Loss: 0.3733282784620921
RMSE train: 0.441112	val: 1.791409	test: 1.628905
MAE train: 0.327312	val: 1.381133	test: 1.268919

Epoch: 92
Loss: 0.39202232162157696
RMSE train: 0.517272	val: 1.748503	test: 1.572297
MAE train: 0.378926	val: 1.351219	test: 1.205939

Epoch: 93
Loss: 0.3225868046283722
RMSE train: 0.572385	val: 1.749245	test: 1.577720
MAE train: 0.419818	val: 1.353550	test: 1.206975

Epoch: 94
Loss: 0.3940393030643463
RMSE train: 0.502879	val: 1.767788	test: 1.574041
MAE train: 0.372176	val: 1.373518	test: 1.191721

Epoch: 95
Loss: 0.3311131000518799
RMSE train: 0.522527	val: 1.765831	test: 1.601544
MAE train: 0.380924	val: 1.377312	test: 1.206681

Epoch: 96
Loss: 0.3530528247356415
RMSE train: 0.616040	val: 1.781730	test: 1.649231
MAE train: 0.444499	val: 1.393463	test: 1.272694

Epoch: 97
Loss: 0.3350073496500651
RMSE train: 0.592867	val: 1.804298	test: 1.649165
MAE train: 0.432884	val: 1.419547	test: 1.265954

Epoch: 98
Loss: 0.3559429347515106
RMSE train: 0.570003	val: 1.845859	test: 1.704088
MAE train: 0.420046	val: 1.454986	test: 1.308486

Epoch: 99
Loss: 0.33852145075798035
RMSE train: 0.624507	val: 1.862082	test: 1.779911
MAE train: 0.449096	val: 1.457261	test: 1.364244

Epoch: 100
Loss: 0.39475451906522113
RMSE train: 0.614606	val: 1.759744	test: 1.641573
MAE train: 0.431618	val: 1.388394	test: 1.264225

Epoch: 101
Loss: 0.3433965841929118
RMSE train: 0.527895	val: 1.732010	test: 1.623234
MAE train: 0.377681	val: 1.360026	test: 1.234774

Epoch: 102
Loss: 0.3284180859724681
RMSE train: 0.567215	val: 1.718524	test: 1.617121
MAE train: 0.412374	val: 1.340227	test: 1.225403

Epoch: 103
Loss: 0.3507684866587321
RMSE train: 0.660409	val: 1.764426	test: 1.692307
MAE train: 0.471057	val: 1.417576	test: 1.323413

Epoch: 104
Loss: 0.3493403494358063
RMSE train: 0.554663	val: 1.691934	test: 1.603659
MAE train: 0.414298	val: 1.322074	test: 1.225909

Epoch: 105
Loss: 0.31993570923805237
RMSE train: 0.458979	val: 1.704005	test: 1.594721
MAE train: 0.342317	val: 1.341599	test: 1.211735

Epoch: 106
Loss: 0.3333415985107422
RMSE train: 0.537721	val: 1.695227	test: 1.569843
MAE train: 0.380688	val: 1.327960	test: 1.217267

Epoch: 107
Loss: 0.2866869072119395
RMSE train: 0.590609	val: 1.661749	test: 1.586258
MAE train: 0.425086	val: 1.296195	test: 1.225068

Epoch: 108
Loss: 0.34686581293741864
RMSE train: 0.510835	val: 1.670708	test: 1.539199
MAE train: 0.367284	val: 1.298253	test: 1.177565

Epoch: 109
Loss: 0.32403616110483807
RMSE train: 0.485812	val: 1.703224	test: 1.496180
MAE train: 0.356190	val: 1.329282	test: 1.167748

Epoch: 110
Loss: 0.3137456774711609
RMSE train: 0.447273	val: 1.676122	test: 1.528135
MAE train: 0.329721	val: 1.311105	test: 1.175393

Epoch: 111
Loss: 0.2705709636211395
RMSE train: 0.469189	val: 1.665179	test: 1.555082
MAE train: 0.358037	val: 1.303525	test: 1.183865

Epoch: 112
Loss: 0.3975325326124827
RMSE train: 0.546582	val: 1.677418	test: 1.563015
MAE train: 0.406516	val: 1.313578	test: 1.197451

Epoch: 113
Loss: 0.3345101277033488
RMSE train: 0.502733	val: 1.686611	test: 1.540229
MAE train: 0.365409	val: 1.321470	test: 1.182342

Epoch: 114
Loss: 0.47782357533772785
RMSE train: 0.542922	val: 1.695961	test: 1.544929
MAE train: 0.385113	val: 1.323971	test: 1.188041

Epoch: 115
Loss: 0.29348230361938477
RMSE train: 0.513604	val: 1.686300	test: 1.562934
MAE train: 0.374649	val: 1.315775	test: 1.198143

Epoch: 116
Loss: 0.3126641313234965
RMSE train: 0.499528	val: 1.709975	test: 1.612670
MAE train: 0.370978	val: 1.340867	test: 1.226790

Epoch: 117
Loss: 0.306178480386734
RMSE train: 0.491008	val: 1.751141	test: 1.663531
MAE train: 0.365790	val: 1.372542	test: 1.266002

Epoch: 118
Loss: 0.27683956424395245
RMSE train: 0.564554	val: 1.745865	test: 1.664929
MAE train: 0.393394	val: 1.371820	test: 1.299134

Epoch: 119
Loss: 0.31553223729133606
RMSE train: 0.517156	val: 1.770110	test: 1.646643
MAE train: 0.369120	val: 1.383386	test: 1.255398

Epoch: 120
Loss: 0.303591122229894
RMSE train: 0.467802	val: 1.861472	test: 1.719994
MAE train: 0.340203	val: 1.436013	test: 1.309782

Epoch: 121
Loss: 0.34715892871220905
RMSE train: 0.464551	val: 1.800263	test: 1.640051
MAE train: 0.344963	val: 1.403419	test: 1.249204

Epoch: 122
Loss: 0.3237365782260895
RMSE train: 0.514160	val: 1.751323	test: 1.631894
MAE train: 0.377743	val: 1.363901	test: 1.257631

Epoch: 123
Loss: 0.2792867124080658
RMSE train: 0.453909	val: 1.770367	test: 1.634124
MAE train: 0.343399	val: 1.374197	test: 1.243033

Epoch: 124
Loss: 0.27995310723781586
RMSE train: 0.474694	val: 1.800437	test: 1.672668
MAE train: 0.349596	val: 1.390469	test: 1.266939

Epoch: 125
Loss: 0.34753134349981946
RMSE train: 0.514166	val: 1.760251	test: 1.639250
MAE train: 0.366336	val: 1.358966	test: 1.245978

Epoch: 126
Loss: 0.33556610345840454
RMSE train: 0.611801	val: 1.751685	test: 1.666569
MAE train: 0.409125	val: 1.347447	test: 1.318858

Epoch: 127
Loss: 0.32891152302424115
RMSE train: 0.465763	val: 1.731590	test: 1.565023
MAE train: 0.334336	val: 1.342145	test: 1.203701

Epoch: 128
Loss: 0.3430713315804799
RMSE train: 0.446693	val: 1.759237	test: 1.628498
MAE train: 0.330169	val: 1.370143	test: 1.244613

Epoch: 129
Loss: 0.30395732323328656
RMSE train: 0.538623	val: 1.734925	test: 1.636158
MAE train: 0.388381	val: 1.348579	test: 1.264594

Epoch: 130
Loss: 0.3259713848431905
RMSE train: 0.490534	val: 1.740913	test: 1.654364
MAE train: 0.376455	val: 1.358457	test: 1.267242

Epoch: 131
Loss: 0.36462830503781635
RMSE train: 0.427432	val: 1.767551	test: 1.660271
MAE train: 0.319055	val: 1.378797	test: 1.269646

Epoch: 132
Loss: 0.2756912012894948
RMSE train: 0.476004	val: 1.752893	test: 1.659400
MAE train: 0.342920	val: 1.362880	test: 1.260588

Epoch: 133
Loss: 0.3422568043073018
RMSE train: 0.497277	val: 1.723176	test: 1.611595
MAE train: 0.355068	val: 1.341194	test: 1.222242

Epoch: 134
Loss: 0.2585558195908864
RMSE train: 0.489539	val: 1.737727	test: 1.605439
MAE train: 0.351889	val: 1.357202	test: 1.223125

Epoch: 135
Loss: 0.27856337030728656
RMSE train: 0.477960	val: 1.763583	test: 1.639220
MAE train: 0.348645	val: 1.378585	test: 1.246642

Epoch: 136
Loss: 0.27368325491746265
RMSE train: 0.534546	val: 1.738110	test: 1.647997
MAE train: 0.387101	val: 1.358328	test: 1.279829

Epoch: 137
Loss: 0.28898483514785767
RMSE train: 0.563610	val: 1.765192	test: 1.703363
MAE train: 0.416252	val: 1.372166	test: 1.308244

Epoch: 138
Loss: 0.27036193013191223
RMSE train: 0.562685	val: 1.800959	test: 1.714633
MAE train: 0.396701	val: 1.390397	test: 1.308976

Epoch: 139
Loss: 0.25974009931087494
RMSE train: 0.484503	val: 1.868117	test: 1.716628
MAE train: 0.351148	val: 1.440130	test: 1.313775

Epoch: 140
Loss: 0.3044474422931671
RMSE train: 0.500476	val: 1.769303	test: 1.663655
MAE train: 0.354608	val: 1.374583	test: 1.276273

Epoch: 141
Loss: 0.297065148750941
RMSE train: 0.506185	val: 1.814703	test: 1.761541
MAE train: 0.382067	val: 1.405576	test: 1.344811

Epoch: 142
Loss: 0.28719361623128253
RMSE train: 0.438016	val: 1.818899	test: 1.701355
MAE train: 0.322892	val: 1.419243	test: 1.296321

Early stopping
Best (RMSE):	 train: 0.590609	val: 1.661749	test: 1.586258
Best (MAE):	 train: 0.425086	val: 1.296195	test: 1.225068
All runs completed.


Epoch: 84
Loss: 0.5408036559820175
RMSE train: 0.686708	val: 1.276638	test: 1.493620
MAE train: 0.519001	val: 1.003417	test: 1.178535

Epoch: 85
Loss: 0.5942975729703903
RMSE train: 0.715109	val: 1.278653	test: 1.460865
MAE train: 0.535142	val: 1.001590	test: 1.151593

Epoch: 86
Loss: 0.4810716062784195
RMSE train: 0.718311	val: 1.334738	test: 1.513334
MAE train: 0.533082	val: 1.051375	test: 1.185907

Epoch: 87
Loss: 0.5737980753183365
RMSE train: 0.717948	val: 1.419372	test: 1.632671
MAE train: 0.528614	val: 1.123731	test: 1.264359

Epoch: 88
Loss: 0.6743159219622612
RMSE train: 0.725131	val: 1.402628	test: 1.666872
MAE train: 0.534761	val: 1.106309	test: 1.282090

Epoch: 89
Loss: 0.7453422546386719
RMSE train: 0.764789	val: 1.397384	test: 1.673815
MAE train: 0.579286	val: 1.102766	test: 1.299818

Epoch: 90
Loss: 0.6368488371372223
RMSE train: 0.610366	val: 1.195773	test: 1.402951
MAE train: 0.459124	val: 0.951812	test: 1.118341

Epoch: 91
Loss: 0.5059394463896751
RMSE train: 0.591080	val: 1.176044	test: 1.356766
MAE train: 0.452982	val: 0.906370	test: 1.106429

Epoch: 92
Loss: 0.5058189257979393
RMSE train: 0.575818	val: 1.170352	test: 1.390910
MAE train: 0.426733	val: 0.930154	test: 1.133088

Epoch: 93
Loss: 0.6941531300544739
RMSE train: 0.610847	val: 1.191781	test: 1.438965
MAE train: 0.461542	val: 0.966349	test: 1.155148

Epoch: 94
Loss: 0.6588779538869858
RMSE train: 0.580891	val: 1.172871	test: 1.365437
MAE train: 0.437238	val: 0.934660	test: 1.096506

Epoch: 95
Loss: 0.47903842478990555
RMSE train: 0.609232	val: 1.168188	test: 1.304807
MAE train: 0.453762	val: 0.915692	test: 1.057763

Epoch: 96
Loss: 0.6256894469261169
RMSE train: 0.625813	val: 1.183775	test: 1.316215
MAE train: 0.463429	val: 0.940746	test: 1.058931

Epoch: 97
Loss: 0.722206175327301
RMSE train: 0.599482	val: 1.261181	test: 1.423280
MAE train: 0.456180	val: 0.996514	test: 1.139631

Epoch: 98
Loss: 0.6878216490149498
RMSE train: 0.663177	val: 1.352029	test: 1.528685
MAE train: 0.502396	val: 1.066290	test: 1.207494

Epoch: 99
Loss: 0.5727257877588272
RMSE train: 0.706438	val: 1.394808	test: 1.551868
MAE train: 0.538462	val: 1.106515	test: 1.225395

Epoch: 100
Loss: 0.4706890657544136
RMSE train: 0.575602	val: 1.291717	test: 1.373611
MAE train: 0.420358	val: 1.019759	test: 1.108574

Epoch: 101
Loss: 0.5264928042888641
RMSE train: 0.573904	val: 1.238953	test: 1.307164
MAE train: 0.435454	val: 0.951979	test: 1.069038

Epoch: 102
Loss: 0.5986844599246979
RMSE train: 0.590026	val: 1.223567	test: 1.304253
MAE train: 0.440503	val: 0.961197	test: 1.079113

Epoch: 103
Loss: 0.5245946645736694
RMSE train: 0.656091	val: 1.284337	test: 1.422058
MAE train: 0.481686	val: 1.018860	test: 1.135290

Epoch: 104
Loss: 0.4910481795668602
RMSE train: 0.654811	val: 1.342017	test: 1.503705
MAE train: 0.484824	val: 1.058179	test: 1.184602

Epoch: 105
Loss: 0.5935461670160294
RMSE train: 0.645505	val: 1.331334	test: 1.461567
MAE train: 0.482026	val: 1.056676	test: 1.163990

Epoch: 106
Loss: 0.6167262047529221
RMSE train: 0.573194	val: 1.229085	test: 1.334960
MAE train: 0.428985	val: 0.958439	test: 1.079883

Epoch: 107
Loss: 0.5870031788945198
RMSE train: 0.533524	val: 1.203852	test: 1.324272
MAE train: 0.404501	val: 0.930711	test: 1.068661

Epoch: 108
Loss: 0.7156697437167168
RMSE train: 0.566448	val: 1.210074	test: 1.357137
MAE train: 0.424277	val: 0.949151	test: 1.085501

Epoch: 109
Loss: 0.5493356063961983
RMSE train: 0.644580	val: 1.286729	test: 1.442085
MAE train: 0.463916	val: 1.029923	test: 1.160456

Epoch: 110
Loss: 0.5860990509390831
RMSE train: 0.600952	val: 1.248332	test: 1.403966
MAE train: 0.436521	val: 1.012057	test: 1.126149

Epoch: 111
Loss: 0.5196112394332886
RMSE train: 0.572640	val: 1.254803	test: 1.478187
MAE train: 0.425821	val: 0.994824	test: 1.187040

Epoch: 112
Loss: 0.5223433747887611
RMSE train: 0.570239	val: 1.255523	test: 1.470391
MAE train: 0.427613	val: 0.995400	test: 1.185630

Epoch: 113
Loss: 0.6224655136466026
RMSE train: 0.617596	val: 1.232089	test: 1.413431
MAE train: 0.462759	val: 0.994801	test: 1.128952

Epoch: 114
Loss: 0.7507267445325851
RMSE train: 0.691193	val: 1.318576	test: 1.505952
MAE train: 0.522488	val: 1.054545	test: 1.186908

Epoch: 115
Loss: 0.4208632856607437
RMSE train: 0.565705	val: 1.241675	test: 1.392308
MAE train: 0.415934	val: 0.993284	test: 1.116317

Epoch: 116
Loss: 0.5763478726148605
RMSE train: 0.551960	val: 1.250799	test: 1.442189
MAE train: 0.405802	val: 0.992445	test: 1.165482

Epoch: 117
Loss: 0.6463882774114609
RMSE train: 0.615382	val: 1.335207	test: 1.558498
MAE train: 0.457996	val: 1.044195	test: 1.241493

Epoch: 118
Loss: 0.46414073556661606
RMSE train: 0.645012	val: 1.290992	test: 1.479141
MAE train: 0.471547	val: 1.010036	test: 1.185029

Epoch: 119
Loss: 0.6173722222447395
RMSE train: 0.611621	val: 1.220922	test: 1.346494
MAE train: 0.445487	val: 0.951046	test: 1.088183

Epoch: 120
Loss: 0.5648873373866081
RMSE train: 0.573999	val: 1.194195	test: 1.311831
MAE train: 0.419275	val: 0.918976	test: 1.057628

Epoch: 121
Loss: 0.5158075243234634
RMSE train: 0.576925	val: 1.183461	test: 1.318904
MAE train: 0.425048	val: 0.922666	test: 1.068624

Epoch: 122
Loss: 0.49791841953992844
RMSE train: 0.674464	val: 1.278000	test: 1.456218
MAE train: 0.498173	val: 1.001071	test: 1.152166

Epoch: 123
Loss: 0.8284427970647812
RMSE train: 0.652623	val: 1.297745	test: 1.474849
MAE train: 0.484234	val: 1.020207	test: 1.171992

Epoch: 124
Loss: 0.6030638366937637
RMSE train: 0.677760	val: 1.287677	test: 1.446326
MAE train: 0.500650	val: 1.022305	test: 1.166097

Epoch: 125
Loss: 0.6803373917937279
RMSE train: 0.696267	val: 1.255483	test: 1.398261
MAE train: 0.505608	val: 0.989172	test: 1.135529

Epoch: 126
Loss: 0.475713774561882
RMSE train: 0.634341	val: 1.237302	test: 1.408189
MAE train: 0.479522	val: 0.966275	test: 1.131898

Epoch: 127
Loss: 0.602936714887619
RMSE train: 0.578391	val: 1.250372	test: 1.429531
MAE train: 0.430410	val: 0.962285	test: 1.159916

Epoch: 128
Loss: 0.6426873952150345
RMSE train: 0.527375	val: 1.170312	test: 1.326319
MAE train: 0.390211	val: 0.890462	test: 1.080806

Epoch: 129
Loss: 0.5716566070914268
RMSE train: 0.536756	val: 1.175161	test: 1.319028
MAE train: 0.397017	val: 0.916380	test: 1.068414

Epoch: 130
Loss: 0.5635436251759529
RMSE train: 0.566944	val: 1.224115	test: 1.385638
MAE train: 0.427255	val: 0.965531	test: 1.116910

Early stopping
Best (RMSE):	 train: 0.609232	val: 1.168188	test: 1.304807
Best (MAE):	 train: 0.453762	val: 0.915692	test: 1.057763


Epoch: 84
Loss: 0.593444749712944
RMSE train: 0.630928	val: 1.284982	test: 1.451766
MAE train: 0.477114	val: 0.987533	test: 1.160849

Epoch: 85
Loss: 0.8156725913286209
RMSE train: 0.652023	val: 1.219531	test: 1.319768
MAE train: 0.500974	val: 0.984618	test: 1.071794

Epoch: 86
Loss: 0.5215039998292923
RMSE train: 0.647865	val: 1.204634	test: 1.321259
MAE train: 0.493116	val: 0.979186	test: 1.079600

Epoch: 87
Loss: 0.6344735324382782
RMSE train: 0.649834	val: 1.220233	test: 1.369733
MAE train: 0.486462	val: 0.995287	test: 1.118683

Epoch: 88
Loss: 0.5650717690587044
RMSE train: 0.584109	val: 1.195023	test: 1.339472
MAE train: 0.449854	val: 0.963358	test: 1.093100

Epoch: 89
Loss: 0.7345359921455383
RMSE train: 0.605151	val: 1.300238	test: 1.464247
MAE train: 0.457867	val: 1.020896	test: 1.173159

Epoch: 90
Loss: 0.5064611732959747
RMSE train: 0.619400	val: 1.219734	test: 1.344643
MAE train: 0.468561	val: 0.949703	test: 1.083059

Epoch: 91
Loss: 0.49751051515340805
RMSE train: 0.643025	val: 1.165377	test: 1.248215
MAE train: 0.488837	val: 0.909811	test: 1.017707

Epoch: 92
Loss: 0.8974804282188416
RMSE train: 0.606563	val: 1.176083	test: 1.253746
MAE train: 0.453940	val: 0.931504	test: 1.019053

Epoch: 93
Loss: 0.518072597682476
RMSE train: 0.571032	val: 1.180295	test: 1.272393
MAE train: 0.433413	val: 0.950756	test: 1.035256

Epoch: 94
Loss: 0.49434737116098404
RMSE train: 0.589856	val: 1.248317	test: 1.380142
MAE train: 0.459883	val: 0.993535	test: 1.121787

Epoch: 95
Loss: 0.8168479353189468
RMSE train: 0.538094	val: 1.219639	test: 1.354444
MAE train: 0.412763	val: 0.970278	test: 1.102835

Epoch: 96
Loss: 0.6404831781983376
RMSE train: 0.661080	val: 1.183222	test: 1.256025
MAE train: 0.514393	val: 0.922039	test: 1.006175

Epoch: 97
Loss: 0.5439928621053696
RMSE train: 0.638528	val: 1.159992	test: 1.212182
MAE train: 0.500413	val: 0.922512	test: 0.970661

Epoch: 98
Loss: 0.6435634046792984
RMSE train: 0.613486	val: 1.179568	test: 1.267024
MAE train: 0.462136	val: 0.951244	test: 1.032781

Epoch: 99
Loss: 0.5643939971923828
RMSE train: 0.635090	val: 1.222183	test: 1.341009
MAE train: 0.462668	val: 0.990799	test: 1.088193

Epoch: 100
Loss: 0.5134430304169655
RMSE train: 0.587499	val: 1.239679	test: 1.345642
MAE train: 0.434541	val: 0.997615	test: 1.101612

Epoch: 101
Loss: 0.47130290418863297
RMSE train: 0.554908	val: 1.239268	test: 1.323899
MAE train: 0.411245	val: 1.001353	test: 1.088558

Epoch: 102
Loss: 0.528986357152462
RMSE train: 0.558818	val: 1.215206	test: 1.307326
MAE train: 0.414687	val: 0.975728	test: 1.065819

Epoch: 103
Loss: 0.5146477967500687
RMSE train: 0.533102	val: 1.179903	test: 1.283268
MAE train: 0.397738	val: 0.933966	test: 1.043518

Epoch: 104
Loss: 0.4821818396449089
RMSE train: 0.580826	val: 1.183675	test: 1.301060
MAE train: 0.436869	val: 0.951667	test: 1.056817

Epoch: 105
Loss: 0.6322925165295601
RMSE train: 0.608243	val: 1.190167	test: 1.355312
MAE train: 0.464334	val: 0.968616	test: 1.106815

Epoch: 106
Loss: 0.6479618698358536
RMSE train: 0.572756	val: 1.210237	test: 1.420522
MAE train: 0.426156	val: 0.970307	test: 1.160046

Epoch: 107
Loss: 0.692101500928402
RMSE train: 0.589577	val: 1.218727	test: 1.352799
MAE train: 0.438795	val: 0.994317	test: 1.098585

Epoch: 108
Loss: 0.5789903923869133
RMSE train: 0.586130	val: 1.209836	test: 1.306475
MAE train: 0.433933	val: 0.968038	test: 1.063158

Epoch: 109
Loss: 0.7910967767238617
RMSE train: 0.597361	val: 1.212646	test: 1.328024
MAE train: 0.434493	val: 0.984333	test: 1.057913

Epoch: 110
Loss: 0.5347502306103706
RMSE train: 0.615547	val: 1.211689	test: 1.350821
MAE train: 0.464999	val: 0.995218	test: 1.069628

Epoch: 111
Loss: 0.8095272779464722
RMSE train: 0.642242	val: 1.227384	test: 1.395820
MAE train: 0.489928	val: 0.997436	test: 1.119340

Epoch: 112
Loss: 0.4905456677079201
RMSE train: 0.656697	val: 1.241835	test: 1.372424
MAE train: 0.514048	val: 0.984942	test: 1.111603

Epoch: 113
Loss: 0.48350412398576736
RMSE train: 0.570840	val: 1.219859	test: 1.308867
MAE train: 0.443836	val: 0.940611	test: 1.054009

Epoch: 114
Loss: 0.6003798842430115
RMSE train: 0.560396	val: 1.217378	test: 1.286580
MAE train: 0.431772	val: 0.936684	test: 1.030051

Epoch: 115
Loss: 0.7057510688900948
RMSE train: 0.515602	val: 1.187699	test: 1.255172
MAE train: 0.395344	val: 0.920464	test: 1.013492

Epoch: 116
Loss: 0.4711778610944748
RMSE train: 0.576888	val: 1.224510	test: 1.356168
MAE train: 0.439084	val: 0.981625	test: 1.088704

Epoch: 117
Loss: 0.5638500601053238
RMSE train: 0.584278	val: 1.190520	test: 1.291940
MAE train: 0.433024	val: 0.978262	test: 1.028148

Epoch: 118
Loss: 0.4394589886069298
RMSE train: 0.590335	val: 1.177664	test: 1.245915
MAE train: 0.443812	val: 0.949143	test: 1.005489

Epoch: 119
Loss: 0.5617015361785889
RMSE train: 0.542832	val: 1.206540	test: 1.260182
MAE train: 0.411439	val: 0.948666	test: 1.029951

Epoch: 120
Loss: 0.5632346346974373
RMSE train: 0.554996	val: 1.270447	test: 1.309023
MAE train: 0.426048	val: 0.992493	test: 1.075385

Epoch: 121
Loss: 0.41925840824842453
RMSE train: 0.541427	val: 1.287355	test: 1.364295
MAE train: 0.409355	val: 1.006083	test: 1.109292

Epoch: 122
Loss: 0.49615616351366043
RMSE train: 0.527244	val: 1.207861	test: 1.261163
MAE train: 0.394954	val: 0.969765	test: 1.022888

Epoch: 123
Loss: 0.48661088198423386
RMSE train: 0.552163	val: 1.195447	test: 1.240105
MAE train: 0.407843	val: 0.961635	test: 0.997587

Epoch: 124
Loss: 0.4621979594230652
RMSE train: 0.577098	val: 1.190598	test: 1.251339
MAE train: 0.426336	val: 0.957235	test: 1.012292

Epoch: 125
Loss: 0.5463091060519218
RMSE train: 0.527546	val: 1.190979	test: 1.273379
MAE train: 0.388090	val: 0.950342	test: 1.032534

Epoch: 126
Loss: 0.5213594511151314
RMSE train: 0.552946	val: 1.326392	test: 1.465446
MAE train: 0.408218	val: 1.040625	test: 1.204132

Epoch: 127
Loss: 0.50691919028759
RMSE train: 0.632625	val: 1.313070	test: 1.471956
MAE train: 0.458970	val: 1.042948	test: 1.207273

Epoch: 128
Loss: 0.6094023361802101
RMSE train: 0.652649	val: 1.236212	test: 1.348598
MAE train: 0.472270	val: 0.989605	test: 1.097296

Epoch: 129
Loss: 0.4837137684226036
RMSE train: 0.610557	val: 1.206154	test: 1.277722
MAE train: 0.455431	val: 0.953788	test: 1.030100

Epoch: 130
Loss: 0.49055279791355133
RMSE train: 0.517971	val: 1.193552	test: 1.317012
MAE train: 0.387809	val: 0.935475	test: 1.047843

Epoch: 131
Loss: 0.4785817489027977
RMSE train: 0.553058	val: 1.212557	test: 1.354103
MAE train: 0.420296	val: 0.953199	test: 1.084985

Epoch: 132
Loss: 0.5784372985363007
RMSE train: 0.645946	val: 1.215296	test: 1.355685
MAE train: 0.484585	val: 0.978453	test: 1.086099

Early stopping
Best (RMSE):	 train: 0.638528	val: 1.159992	test: 1.212182
Best (MAE):	 train: 0.500413	val: 0.922512	test: 0.970661


Epoch: 84
Loss: 0.4000537768006325
RMSE train: 0.546927	val: 1.229995	test: 1.259274
MAE train: 0.409797	val: 1.013710	test: 0.972059

Epoch: 85
Loss: 0.4516735076904297
RMSE train: 0.574419	val: 1.287327	test: 1.307732
MAE train: 0.429658	val: 1.045971	test: 0.993802

Epoch: 86
Loss: 0.3771861717104912
RMSE train: 0.552725	val: 1.188429	test: 1.242339
MAE train: 0.418414	val: 0.972649	test: 0.931451

Epoch: 87
Loss: 0.39312414079904556
RMSE train: 0.549099	val: 1.236230	test: 1.259111
MAE train: 0.417038	val: 1.024783	test: 0.954255

Epoch: 88
Loss: 0.3770543709397316
RMSE train: 0.551217	val: 1.254940	test: 1.284752
MAE train: 0.418124	val: 1.047644	test: 0.995008

Epoch: 89
Loss: 0.48884308338165283
RMSE train: 0.524330	val: 1.314055	test: 1.305578
MAE train: 0.401318	val: 1.075907	test: 1.000630

Epoch: 90
Loss: 0.41574833542108536
RMSE train: 0.554692	val: 1.296470	test: 1.290505
MAE train: 0.424186	val: 1.071606	test: 0.997772

Epoch: 91
Loss: 0.3858351334929466
RMSE train: 0.573047	val: 1.381012	test: 1.350220
MAE train: 0.428792	val: 1.130564	test: 1.042695

Epoch: 92
Loss: 0.3542768210172653
RMSE train: 0.486929	val: 1.327303	test: 1.323357
MAE train: 0.365145	val: 1.081846	test: 0.990836

Epoch: 93
Loss: 0.40012774616479874
RMSE train: 0.522026	val: 1.154371	test: 1.203055
MAE train: 0.400233	val: 0.982828	test: 0.931373

Epoch: 94
Loss: 0.3478739708662033
RMSE train: 0.588228	val: 1.339003	test: 1.325544
MAE train: 0.458326	val: 1.102654	test: 1.020750

Epoch: 95
Loss: 0.3791022524237633
RMSE train: 0.508145	val: 1.172979	test: 1.217987
MAE train: 0.382483	val: 0.995621	test: 0.940877

Epoch: 96
Loss: 0.36405670642852783
RMSE train: 0.498045	val: 1.169975	test: 1.205582
MAE train: 0.374614	val: 0.988711	test: 0.943147

Epoch: 97
Loss: 0.3752172365784645
RMSE train: 0.529235	val: 1.334332	test: 1.279558
MAE train: 0.407743	val: 1.094669	test: 1.001906

Epoch: 98
Loss: 0.34532325714826584
RMSE train: 0.494322	val: 1.146619	test: 1.143951
MAE train: 0.376420	val: 0.943524	test: 0.889833

Epoch: 99
Loss: 0.3578127399086952
RMSE train: 0.485970	val: 1.229372	test: 1.180900
MAE train: 0.380109	val: 1.011955	test: 0.933738

Epoch: 100
Loss: 0.33852385729551315
RMSE train: 0.534967	val: 1.330870	test: 1.273759
MAE train: 0.421317	val: 1.085634	test: 0.986646

Epoch: 101
Loss: 0.33918052911758423
RMSE train: 0.487459	val: 1.208499	test: 1.197716
MAE train: 0.378634	val: 1.000812	test: 0.937231

Epoch: 102
Loss: 0.3328970596194267
RMSE train: 0.477918	val: 1.216865	test: 1.211133
MAE train: 0.376516	val: 1.005361	test: 0.958405

Epoch: 103
Loss: 0.3406493216753006
RMSE train: 0.538202	val: 1.381592	test: 1.362885
MAE train: 0.422511	val: 1.114426	test: 1.052558

Epoch: 104
Loss: 0.3225509747862816
RMSE train: 0.513372	val: 1.170519	test: 1.192823
MAE train: 0.396356	val: 0.990840	test: 0.943283

Epoch: 105
Loss: 0.350857175886631
RMSE train: 0.531394	val: 1.152554	test: 1.185444
MAE train: 0.405580	val: 0.980355	test: 0.924152

Epoch: 106
Loss: 0.3832341507077217
RMSE train: 0.541123	val: 1.231829	test: 1.225018
MAE train: 0.408301	val: 1.016453	test: 0.955796

Epoch: 107
Loss: 0.34749671816825867
RMSE train: 0.527719	val: 1.088386	test: 1.164915
MAE train: 0.392795	val: 0.933982	test: 0.911724

Epoch: 108
Loss: 0.33469822257757187
RMSE train: 0.485253	val: 1.212049	test: 1.244794
MAE train: 0.381175	val: 1.008649	test: 0.990861

Epoch: 109
Loss: 0.3030441999435425
RMSE train: 0.502580	val: 1.218219	test: 1.226236
MAE train: 0.397849	val: 1.013647	test: 0.978660

Epoch: 110
Loss: 0.34096813946962357
RMSE train: 0.487217	val: 1.119632	test: 1.158268
MAE train: 0.375117	val: 0.953684	test: 0.916043

Epoch: 111
Loss: 0.31697651743888855
RMSE train: 0.473861	val: 1.154389	test: 1.154855
MAE train: 0.365474	val: 0.965684	test: 0.894441

Epoch: 112
Loss: 0.35939984768629074
RMSE train: 0.541262	val: 1.176527	test: 1.195491
MAE train: 0.417687	val: 0.975731	test: 0.886600

Epoch: 113
Loss: 0.34091801941394806
RMSE train: 0.530445	val: 1.127354	test: 1.159375
MAE train: 0.410239	val: 0.942062	test: 0.886112

Epoch: 114
Loss: 0.3598415330052376
RMSE train: 0.477755	val: 1.241887	test: 1.285453
MAE train: 0.370100	val: 1.009713	test: 1.004080

Epoch: 115
Loss: 0.3581244647502899
RMSE train: 0.485828	val: 1.170079	test: 1.206715
MAE train: 0.362196	val: 0.997511	test: 0.941948

Epoch: 116
Loss: 0.35275010019540787
RMSE train: 0.517812	val: 1.173091	test: 1.208678
MAE train: 0.392544	val: 0.997310	test: 0.949891

Epoch: 117
Loss: 0.3743632212281227
RMSE train: 0.525298	val: 1.413977	test: 1.383721
MAE train: 0.397714	val: 1.142160	test: 1.067871

Epoch: 118
Loss: 0.37258773297071457
RMSE train: 0.545668	val: 1.215867	test: 1.256444
MAE train: 0.411635	val: 1.009018	test: 0.977605

Epoch: 119
Loss: 0.4053187295794487
RMSE train: 0.497007	val: 1.177428	test: 1.199342
MAE train: 0.382529	val: 0.959993	test: 0.942026

Epoch: 120
Loss: 0.32385244965553284
RMSE train: 0.480074	val: 1.302967	test: 1.276836
MAE train: 0.366073	val: 1.065217	test: 1.009456

Epoch: 121
Loss: 0.36032000184059143
RMSE train: 0.517835	val: 1.140786	test: 1.193385
MAE train: 0.388018	val: 0.939821	test: 0.924352

Early stopping
Best (RMSE):	 train: 0.569335	val: 1.080995	test: 1.122923
Best (MAE):	 train: 0.415218	val: 0.918855	test: 0.862979


Epoch: 84
Loss: 0.3951192796230316
RMSE train: 0.577117	val: 1.238515	test: 1.189450
MAE train: 0.436394	val: 1.000761	test: 0.897069

Epoch: 85
Loss: 0.43576904386281967
RMSE train: 0.505415	val: 1.129151	test: 1.095287
MAE train: 0.377515	val: 0.918570	test: 0.850312

Epoch: 86
Loss: 0.39333076775074005
RMSE train: 0.498934	val: 1.292736	test: 1.178597
MAE train: 0.368491	val: 1.028219	test: 0.908400

Epoch: 87
Loss: 0.3661135211586952
RMSE train: 0.517538	val: 1.181751	test: 1.202381
MAE train: 0.388946	val: 0.976117	test: 0.929818

Epoch: 88
Loss: 0.42034807801246643
RMSE train: 0.465985	val: 1.246388	test: 1.189890
MAE train: 0.350636	val: 0.997801	test: 0.918990

Epoch: 89
Loss: 0.3862026780843735
RMSE train: 0.518343	val: 1.279170	test: 1.202190
MAE train: 0.389350	val: 1.037975	test: 0.926736

Epoch: 90
Loss: 0.40219367295503616
RMSE train: 0.516605	val: 1.242130	test: 1.169633
MAE train: 0.380987	val: 0.991273	test: 0.889920

Epoch: 91
Loss: 0.3571985065937042
RMSE train: 0.518634	val: 1.235105	test: 1.160407
MAE train: 0.381900	val: 0.978824	test: 0.863833

Epoch: 92
Loss: 0.359112411737442
RMSE train: 0.507569	val: 1.125320	test: 1.131105
MAE train: 0.374685	val: 0.922501	test: 0.867574

Epoch: 93
Loss: 0.3996402993798256
RMSE train: 0.493963	val: 1.171474	test: 1.155170
MAE train: 0.367426	val: 0.964591	test: 0.892571

Epoch: 94
Loss: 0.3733966127038002
RMSE train: 0.551043	val: 1.261173	test: 1.232664
MAE train: 0.416133	val: 1.029215	test: 0.958533

Epoch: 95
Loss: 0.35315845906734467
RMSE train: 0.502875	val: 1.212196	test: 1.160058
MAE train: 0.374037	val: 0.989883	test: 0.900341

Epoch: 96
Loss: 0.3965604081749916
RMSE train: 0.545398	val: 1.271003	test: 1.195843
MAE train: 0.410486	val: 1.031190	test: 0.928272

Epoch: 97
Loss: 0.36719033867120743
RMSE train: 0.537867	val: 1.287994	test: 1.208971
MAE train: 0.406014	val: 1.047221	test: 0.927943

Epoch: 98
Loss: 0.34828585386276245
RMSE train: 0.458271	val: 1.193386	test: 1.137790
MAE train: 0.342733	val: 0.969949	test: 0.884762

Epoch: 99
Loss: 0.3324749693274498
RMSE train: 0.513840	val: 1.249304	test: 1.169825
MAE train: 0.385560	val: 1.000563	test: 0.906549

Epoch: 100
Loss: 0.3649835139513016
RMSE train: 0.536081	val: 1.353038	test: 1.209367
MAE train: 0.410992	val: 1.075933	test: 0.926242

Epoch: 101
Loss: 0.36402687430381775
RMSE train: 0.486962	val: 1.114263	test: 1.125125
MAE train: 0.359904	val: 0.929088	test: 0.878149

Epoch: 102
Loss: 0.3642354905605316
RMSE train: 0.469572	val: 1.288583	test: 1.231703
MAE train: 0.354833	val: 1.043894	test: 0.940612

Epoch: 103
Loss: 0.3657550811767578
RMSE train: 0.502683	val: 1.218518	test: 1.270951
MAE train: 0.372254	val: 0.997866	test: 1.006295

Epoch: 104
Loss: 0.39975325018167496
RMSE train: 0.486384	val: 1.197538	test: 1.242868
MAE train: 0.362011	val: 0.980656	test: 0.980475

Epoch: 105
Loss: 0.31443871557712555
RMSE train: 0.503234	val: 1.341791	test: 1.249867
MAE train: 0.377964	val: 1.085103	test: 0.941475

Epoch: 106
Loss: 0.37400010973215103
RMSE train: 0.569789	val: 1.251022	test: 1.248248
MAE train: 0.424006	val: 1.038553	test: 0.935950

Epoch: 107
Loss: 0.37451307475566864
RMSE train: 0.497265	val: 1.183891	test: 1.195648
MAE train: 0.369643	val: 0.982777	test: 0.913694

Epoch: 108
Loss: 0.33134160935878754
RMSE train: 0.530588	val: 1.366738	test: 1.276645
MAE train: 0.398263	val: 1.102551	test: 0.978410

Epoch: 109
Loss: 0.3524149879813194
RMSE train: 0.519380	val: 1.173813	test: 1.257623
MAE train: 0.370686	val: 0.966482	test: 0.983759

Epoch: 110
Loss: 0.3436600938439369
RMSE train: 0.454062	val: 1.165344	test: 1.208787
MAE train: 0.336909	val: 0.940551	test: 0.946769

Epoch: 111
Loss: 0.327280230820179
RMSE train: 0.471016	val: 1.237592	test: 1.219355
MAE train: 0.359782	val: 0.998081	test: 0.938433

Epoch: 112
Loss: 0.27799708396196365
RMSE train: 0.488739	val: 1.162557	test: 1.194134
MAE train: 0.366352	val: 0.962250	test: 0.923523

Epoch: 113
Loss: 0.33608780056238174
RMSE train: 0.458155	val: 1.164603	test: 1.147790
MAE train: 0.345457	val: 0.954083	test: 0.893267

Epoch: 114
Loss: 0.32410772889852524
RMSE train: 0.466901	val: 1.171033	test: 1.159849
MAE train: 0.349548	val: 0.946240	test: 0.890064

Epoch: 115
Loss: 0.30230166390538216
RMSE train: 0.479282	val: 1.153190	test: 1.167388
MAE train: 0.357869	val: 0.950496	test: 0.887029

Epoch: 116
Loss: 0.33069106936454773
RMSE train: 0.436469	val: 1.177643	test: 1.145379
MAE train: 0.328909	val: 0.955275	test: 0.883767

Epoch: 117
Loss: 0.35446397960186005
RMSE train: 0.429064	val: 1.202824	test: 1.165937
MAE train: 0.323475	val: 0.966594	test: 0.905533

Epoch: 118
Loss: 0.32844342291355133
RMSE train: 0.444120	val: 1.169357	test: 1.163721
MAE train: 0.340811	val: 0.955821	test: 0.904955

Epoch: 119
Loss: 0.31583400070667267
RMSE train: 0.450275	val: 1.125541	test: 1.144250
MAE train: 0.335146	val: 0.934741	test: 0.884332

Epoch: 120
Loss: 0.3044968247413635
RMSE train: 0.454676	val: 1.086335	test: 1.115792
MAE train: 0.337382	val: 0.903444	test: 0.865507

Epoch: 121
Loss: 0.31454768031835556
RMSE train: 0.440018	val: 1.201172	test: 1.142173
MAE train: 0.341127	val: 0.983752	test: 0.866753

Early stopping
Best (RMSE):	 train: 0.604322	val: 1.030373	test: 1.200564
Best (MAE):	 train: 0.451835	val: 0.843988	test: 0.932791

RMSE train: 0.644249	val: 1.190643	test: 1.372584
MAE train: 0.490976	val: 0.982533	test: 1.110373

Epoch: 85
Loss: 0.5440077111124992
RMSE train: 0.606682	val: 1.163866	test: 1.300604
MAE train: 0.466597	val: 0.961493	test: 1.057139

Epoch: 86
Loss: 0.4895708039402962
RMSE train: 0.580427	val: 1.222254	test: 1.359357
MAE train: 0.450967	val: 0.972752	test: 1.102195

Epoch: 87
Loss: 0.5534703359007835
RMSE train: 0.575545	val: 1.246865	test: 1.389575
MAE train: 0.436819	val: 0.995309	test: 1.120775

Epoch: 88
Loss: 0.5728898718953133
RMSE train: 0.646267	val: 1.277943	test: 1.444104
MAE train: 0.485917	val: 1.037170	test: 1.150271

Epoch: 89
Loss: 0.7409154027700424
RMSE train: 0.771241	val: 1.410704	test: 1.639099
MAE train: 0.566573	val: 1.141728	test: 1.324654

Epoch: 90
Loss: 0.6327550411224365
RMSE train: 0.708096	val: 1.323830	test: 1.509506
MAE train: 0.533396	val: 1.068913	test: 1.217340

Epoch: 91
Loss: 0.5767814666032791
RMSE train: 0.659231	val: 1.319961	test: 1.522913
MAE train: 0.496646	val: 1.034754	test: 1.212748

Epoch: 92
Loss: 0.751994326710701
RMSE train: 0.593403	val: 1.214940	test: 1.370973
MAE train: 0.445909	val: 0.971465	test: 1.087987

Epoch: 93
Loss: 0.6109722629189491
RMSE train: 0.611004	val: 1.209496	test: 1.299098
MAE train: 0.455814	val: 0.983358	test: 1.055505

Epoch: 94
Loss: 0.5939298570156097
RMSE train: 0.637522	val: 1.216729	test: 1.332997
MAE train: 0.471478	val: 0.990818	test: 1.074086

Epoch: 95
Loss: 0.49960483610630035
RMSE train: 0.688240	val: 1.264220	test: 1.434518
MAE train: 0.531924	val: 1.003580	test: 1.137483

Epoch: 96
Loss: 0.6092638671398163
RMSE train: 0.689167	val: 1.265796	test: 1.478243
MAE train: 0.537268	val: 1.001955	test: 1.167634

Epoch: 97
Loss: 0.5991226732730865
RMSE train: 0.646273	val: 1.232619	test: 1.452630
MAE train: 0.490885	val: 0.992514	test: 1.149893

Epoch: 98
Loss: 0.4821021035313606
RMSE train: 0.678442	val: 1.242462	test: 1.427233
MAE train: 0.500036	val: 0.997579	test: 1.140281

Epoch: 99
Loss: 0.5572275966405869
RMSE train: 0.608516	val: 1.221472	test: 1.414591
MAE train: 0.454873	val: 0.967710	test: 1.113594

Epoch: 100
Loss: 0.7547958940267563
RMSE train: 0.641545	val: 1.272942	test: 1.516237
MAE train: 0.481629	val: 0.994596	test: 1.185710

Epoch: 101
Loss: 0.5528867989778519
RMSE train: 0.756041	val: 1.348325	test: 1.641014
MAE train: 0.547707	val: 1.065553	test: 1.265656

Epoch: 102
Loss: 1.308394879102707
RMSE train: 0.670331	val: 1.179367	test: 1.398645
MAE train: 0.505942	val: 0.960061	test: 1.085784

Epoch: 103
Loss: 0.5644945353269577
RMSE train: 0.714911	val: 1.142020	test: 1.333794
MAE train: 0.538904	val: 0.948614	test: 1.059034

Epoch: 104
Loss: 0.8034982830286026
RMSE train: 0.670110	val: 1.202622	test: 1.402973
MAE train: 0.512210	val: 0.993822	test: 1.124435

Epoch: 105
Loss: 0.5615366026759148
RMSE train: 0.657020	val: 1.251880	test: 1.441840
MAE train: 0.493739	val: 1.023738	test: 1.158618

Epoch: 106
Loss: 0.48705827444791794
RMSE train: 0.574922	val: 1.212675	test: 1.370417
MAE train: 0.435714	val: 0.965910	test: 1.099097

Epoch: 107
Loss: 0.4290410876274109
RMSE train: 0.536556	val: 1.116523	test: 1.249170
MAE train: 0.408931	val: 0.896159	test: 1.003834

Epoch: 108
Loss: 0.6526229754090309
RMSE train: 0.553047	val: 1.114302	test: 1.265719
MAE train: 0.428819	val: 0.895589	test: 1.018172

Epoch: 109
Loss: 0.6289719715714455
RMSE train: 0.589251	val: 1.140538	test: 1.300483
MAE train: 0.456328	val: 0.915707	test: 1.036078

Epoch: 110
Loss: 0.579516813158989
RMSE train: 0.581371	val: 1.095538	test: 1.270374
MAE train: 0.438421	val: 0.880913	test: 1.001787

Epoch: 111
Loss: 0.6533873826265335
RMSE train: 0.629714	val: 1.089323	test: 1.249690
MAE train: 0.473379	val: 0.880241	test: 1.005884

Epoch: 112
Loss: 0.6989833414554596
RMSE train: 0.636588	val: 1.126547	test: 1.295629
MAE train: 0.496971	val: 0.927929	test: 1.045055

Epoch: 113
Loss: 0.5327074229717255
RMSE train: 0.662957	val: 1.214838	test: 1.426036
MAE train: 0.513196	val: 0.984154	test: 1.145882

Epoch: 114
Loss: 0.43584755808115005
RMSE train: 0.603406	val: 1.198518	test: 1.375725
MAE train: 0.466534	val: 0.989191	test: 1.111734

Epoch: 115
Loss: 0.5653652176260948
RMSE train: 0.568161	val: 1.165613	test: 1.317021
MAE train: 0.439759	val: 0.964117	test: 1.067451

Epoch: 116
Loss: 0.5083441957831383
RMSE train: 0.595387	val: 1.129680	test: 1.286693
MAE train: 0.454474	val: 0.934091	test: 1.054326

Epoch: 117
Loss: 0.8455264717340469
RMSE train: 0.609724	val: 1.144662	test: 1.286141
MAE train: 0.457301	val: 0.932958	test: 1.034873

Epoch: 118
Loss: 0.614723302423954
RMSE train: 0.610157	val: 1.207449	test: 1.278280
MAE train: 0.474369	val: 0.941357	test: 1.014188

Epoch: 119
Loss: 0.782604269683361
RMSE train: 0.616272	val: 1.274429	test: 1.351554
MAE train: 0.470515	val: 0.994313	test: 1.097443

Epoch: 120
Loss: 0.5519073605537415
RMSE train: 0.561646	val: 1.087457	test: 1.213237
MAE train: 0.426464	val: 0.860476	test: 0.982635

Epoch: 121
Loss: 0.6028314605355263
RMSE train: 0.606808	val: 1.096075	test: 1.227657
MAE train: 0.454494	val: 0.887585	test: 0.971599

Epoch: 122
Loss: 0.4612414985895157
RMSE train: 0.593990	val: 1.119180	test: 1.274197
MAE train: 0.442967	val: 0.921207	test: 0.994195

Epoch: 123
Loss: 0.4093291088938713
RMSE train: 0.609082	val: 1.160426	test: 1.315217
MAE train: 0.460418	val: 0.952452	test: 1.004327

Epoch: 124
Loss: 0.49729256331920624
RMSE train: 0.585138	val: 1.148376	test: 1.318539
MAE train: 0.445508	val: 0.931552	test: 1.024406

Epoch: 125
Loss: 0.7267783135175705
RMSE train: 0.555629	val: 1.143663	test: 1.320122
MAE train: 0.423642	val: 0.911628	test: 1.063078

Epoch: 126
Loss: 0.6195504888892174
RMSE train: 0.520552	val: 1.164802	test: 1.326769
MAE train: 0.396427	val: 0.910583	test: 1.079448

Epoch: 127
Loss: 0.5522022172808647
RMSE train: 0.605838	val: 1.186746	test: 1.313497
MAE train: 0.459654	val: 0.944914	test: 1.052572

Epoch: 128
Loss: 0.536254033446312
RMSE train: 0.584854	val: 1.204444	test: 1.314514
MAE train: 0.454923	val: 0.950908	test: 1.055294

Epoch: 129
Loss: 0.49835021048784256
RMSE train: 0.544808	val: 1.175416	test: 1.345584
MAE train: 0.416104	val: 0.922342	test: 1.068846

Epoch: 130
Loss: 0.5157256498932838
RMSE train: 0.540881	val: 1.147886	test: 1.364527
MAE train: 0.398380	val: 0.915447	test: 1.066706

Epoch: 131
Loss: 0.7904100716114044
RMSE train: 0.554318	val: 1.163850	test: 1.422414
MAE train: 0.405400	val: 0.942778	test: 1.118969

Epoch: 132
Loss: 0.5299316644668579
RMSE train: 0.567147	val: 1.200486	test: 1.479460
MAE train: 0.430356	val: 0.955359	test: 1.169511

Epoch: 133
Loss: 0.4422604590654373
RMSE train: 0.606341	val: 1.298673	test: 1.546482
MAE train: 0.475635	val: 1.025679	test: 1.233009

Epoch: 134
Loss: 0.5381620600819588
RMSE train: 0.536360	val: 1.189285	test: 1.403308
MAE train: 0.415507	val: 0.940742	test: 1.114854

Epoch: 135
Loss: 0.5174506679177284
RMSE train: 0.561019	val: 1.111011	test: 1.321720
MAE train: 0.426020	val: 0.895595	test: 1.044742

Epoch: 136
Loss: 0.5506358742713928
RMSE train: 0.594253	val: 1.152858	test: 1.369299
MAE train: 0.446200	val: 0.943534	test: 1.069894

Epoch: 137
Loss: 0.4862920045852661
RMSE train: 0.532960	val: 1.163113	test: 1.399244
MAE train: 0.400367	val: 0.935703	test: 1.109292

Epoch: 138
Loss: 0.5086845308542252
RMSE train: 0.524998	val: 1.210303	test: 1.461028
MAE train: 0.397642	val: 0.960702	test: 1.167523

Epoch: 139
Loss: 0.5286984220147133
RMSE train: 0.517481	val: 1.163330	test: 1.367820
MAE train: 0.388460	val: 0.932005	test: 1.068491

Epoch: 140
Loss: 0.4704075828194618
RMSE train: 0.589153	val: 1.181101	test: 1.406291
MAE train: 0.449559	val: 0.948147	test: 1.095306

Epoch: 141
Loss: 0.41176997870206833
RMSE train: 0.547994	val: 1.143655	test: 1.345219
MAE train: 0.417156	val: 0.909660	test: 1.056571

Epoch: 142
Loss: 0.5029188618063927
RMSE train: 0.556101	val: 1.117298	test: 1.267668
MAE train: 0.420653	val: 0.890390	test: 1.023539

Epoch: 143
Loss: 0.48178158700466156
RMSE train: 0.551695	val: 1.133072	test: 1.281401
MAE train: 0.428718	val: 0.903897	test: 1.021150

Epoch: 144
Loss: 0.4503975212574005
RMSE train: 0.603589	val: 1.178626	test: 1.329478
MAE train: 0.480761	val: 0.919575	test: 1.051857

Epoch: 145
Loss: 0.4868827909231186
RMSE train: 0.514731	val: 1.144000	test: 1.306792
MAE train: 0.394625	val: 0.914600	test: 1.045079

Epoch: 146
Loss: 0.4921450763940811
RMSE train: 0.538544	val: 1.108846	test: 1.255510
MAE train: 0.405668	val: 0.907850	test: 1.003203

Epoch: 147
Loss: 0.5685827657580376
RMSE train: 0.587554	val: 1.146967	test: 1.308978
MAE train: 0.447326	val: 0.940363	test: 1.040414

Epoch: 148
Loss: 0.5177805125713348
RMSE train: 0.624428	val: 1.170862	test: 1.405019
MAE train: 0.484245	val: 0.950071	test: 1.101513

Epoch: 149
Loss: 0.7195164039731026
RMSE train: 0.527563	val: 1.141855	test: 1.368257
MAE train: 0.391106	val: 0.924841	test: 1.084006

Epoch: 150
Loss: 0.567726731300354
RMSE train: 0.544938	val: 1.121134	test: 1.257312
MAE train: 0.422762	val: 0.903536	test: 0.995075

Epoch: 151
Loss: 0.4663158059120178
RMSE train: 0.647283	val: 1.179317	test: 1.305049
MAE train: 0.516310	val: 0.938843	test: 1.032509

Epoch: 152
Loss: 0.43636947125196457
RMSE train: 0.655144	val: 1.229746	test: 1.352044
MAE train: 0.520752	val: 0.978864	test: 1.073814

Epoch: 153
Loss: 0.43682195246219635
RMSE train: 0.558556	val: 1.171930	test: 1.322539
MAE train: 0.428472	val: 0.921875	test: 1.044068

Epoch: 154
Loss: 0.4367406442761421
RMSE train: 0.520525	val: 1.150666	test: 1.313551
MAE train: 0.386955	val: 0.918802	test: 1.041578

Epoch: 155
Loss: 0.6259094327688217
RMSE train: 0.577048	val: 1.173111	test: 1.334035
MAE train: 0.428485	val: 0.949104	test: 1.060381

Early stopping
Best (RMSE):	 train: 0.561646	val: 1.087457	test: 1.213237
Best (MAE):	 train: 0.426464	val: 0.860476	test: 0.982635
All runs completed.


Epoch: 84
Loss: 0.3844224363565445
RMSE train: 0.579505	val: 1.085269	test: 1.164282
MAE train: 0.426117	val: 0.915586	test: 0.886013

Epoch: 85
Loss: 0.43297724425792694
RMSE train: 0.566786	val: 1.197348	test: 1.171219
MAE train: 0.426259	val: 0.953298	test: 0.916561

Epoch: 86
Loss: 0.39204344898462296
RMSE train: 0.513458	val: 1.182110	test: 1.131260
MAE train: 0.395240	val: 0.943893	test: 0.917278

Epoch: 87
Loss: 0.3806215822696686
RMSE train: 0.561468	val: 1.032755	test: 1.158810
MAE train: 0.427860	val: 0.839333	test: 0.886739

Epoch: 88
Loss: 0.4089270979166031
RMSE train: 0.511092	val: 1.103888	test: 1.119000
MAE train: 0.397780	val: 0.905760	test: 0.861015

Epoch: 89
Loss: 0.44629430025815964
RMSE train: 0.472936	val: 1.059125	test: 1.099366
MAE train: 0.358056	val: 0.861334	test: 0.866863

Epoch: 90
Loss: 0.4073086604475975
RMSE train: 0.552908	val: 1.036348	test: 1.137565
MAE train: 0.407954	val: 0.874117	test: 0.858306

Epoch: 91
Loss: 0.43714873492717743
RMSE train: 0.546174	val: 1.129052	test: 1.165261
MAE train: 0.406861	val: 0.952481	test: 0.871243

Epoch: 92
Loss: 0.3863399401307106
RMSE train: 0.496884	val: 1.164210	test: 1.186037
MAE train: 0.362505	val: 0.951461	test: 0.926694

Epoch: 93
Loss: 0.38900773972272873
RMSE train: 0.521709	val: 1.038580	test: 1.174288
MAE train: 0.388091	val: 0.872944	test: 0.910547

Epoch: 94
Loss: 0.3656337931752205
RMSE train: 0.579241	val: 1.195638	test: 1.236935
MAE train: 0.430764	val: 0.977415	test: 0.948429

Epoch: 95
Loss: 0.34438298642635345
RMSE train: 0.501936	val: 1.097015	test: 1.168135
MAE train: 0.368775	val: 0.917515	test: 0.890274

Epoch: 96
Loss: 0.34980709105730057
RMSE train: 0.491508	val: 1.050471	test: 1.147022
MAE train: 0.361928	val: 0.895681	test: 0.881067

Epoch: 97
Loss: 0.3146772161126137
RMSE train: 0.584456	val: 1.181440	test: 1.239117
MAE train: 0.425676	val: 0.998967	test: 0.949411

Epoch: 98
Loss: 0.3505143001675606
RMSE train: 0.569825	val: 1.184628	test: 1.241079
MAE train: 0.418209	val: 0.994786	test: 0.956017

Epoch: 99
Loss: 0.3160471022129059
RMSE train: 0.537093	val: 1.175311	test: 1.206717
MAE train: 0.391696	val: 0.962847	test: 0.934680

Epoch: 100
Loss: 0.4126907214522362
RMSE train: 0.545003	val: 1.148458	test: 1.198978
MAE train: 0.397616	val: 0.979317	test: 0.913949

Epoch: 101
Loss: 0.33639752864837646
RMSE train: 0.567256	val: 1.207259	test: 1.240312
MAE train: 0.413844	val: 1.018341	test: 0.953158

Epoch: 102
Loss: 0.32630931586027145
RMSE train: 0.507352	val: 1.091377	test: 1.184883
MAE train: 0.378236	val: 0.931994	test: 0.920779

Epoch: 103
Loss: 0.3177865594625473
RMSE train: 0.516264	val: 1.125736	test: 1.223706
MAE train: 0.377772	val: 0.959858	test: 0.950362

Epoch: 104
Loss: 0.4190335124731064
RMSE train: 0.563084	val: 1.216146	test: 1.278616
MAE train: 0.404577	val: 1.005685	test: 0.979110

Epoch: 105
Loss: 0.3649553284049034
RMSE train: 0.525323	val: 0.997534	test: 1.162134
MAE train: 0.391680	val: 0.856157	test: 0.909260

Epoch: 106
Loss: 0.3322747200727463
RMSE train: 0.525656	val: 1.097292	test: 1.151121
MAE train: 0.407992	val: 0.923655	test: 0.893082

Epoch: 107
Loss: 0.36206554621458054
RMSE train: 0.501590	val: 1.049204	test: 1.099962
MAE train: 0.393321	val: 0.880674	test: 0.860458

Epoch: 108
Loss: 0.2848656699061394
RMSE train: 0.492426	val: 1.028368	test: 1.116463
MAE train: 0.367349	val: 0.854463	test: 0.860097

Epoch: 109
Loss: 0.34256797283887863
RMSE train: 0.520937	val: 1.177547	test: 1.177813
MAE train: 0.381174	val: 0.969012	test: 0.877349

Epoch: 110
Loss: 0.32631465792655945
RMSE train: 0.545182	val: 1.272583	test: 1.230014
MAE train: 0.398051	val: 1.018247	test: 0.937476

Epoch: 111
Loss: 0.3093073442578316
RMSE train: 0.491840	val: 1.085068	test: 1.170517
MAE train: 0.364740	val: 0.905054	test: 0.902940

Epoch: 112
Loss: 0.33510690927505493
RMSE train: 0.492454	val: 1.076371	test: 1.176409
MAE train: 0.367107	val: 0.900292	test: 0.907994

Epoch: 113
Loss: 0.31114010512828827
RMSE train: 0.535801	val: 1.208396	test: 1.210014
MAE train: 0.395124	val: 0.979930	test: 0.920560

Epoch: 114
Loss: 0.3732365444302559
RMSE train: 0.556822	val: 1.180510	test: 1.223063
MAE train: 0.406605	val: 0.962117	test: 0.917731

Epoch: 115
Loss: 0.3242040351033211
RMSE train: 0.518330	val: 1.166554	test: 1.201889
MAE train: 0.376060	val: 0.971337	test: 0.909567

Epoch: 116
Loss: 0.31550247222185135
RMSE train: 0.527561	val: 1.225705	test: 1.191829
MAE train: 0.394292	val: 0.995837	test: 0.923533

Epoch: 117
Loss: 0.3354501873254776
RMSE train: 0.492399	val: 1.151500	test: 1.173912
MAE train: 0.368982	val: 0.951582	test: 0.910651

Epoch: 118
Loss: 0.32488812506198883
RMSE train: 0.536963	val: 1.178625	test: 1.234400
MAE train: 0.392087	val: 0.981459	test: 0.929415

Epoch: 119
Loss: 0.3035881593823433
RMSE train: 0.521963	val: 1.155669	test: 1.222441
MAE train: 0.372722	val: 0.950770	test: 0.913408

Epoch: 120
Loss: 0.29252148419618607
RMSE train: 0.513178	val: 1.115899	test: 1.187450
MAE train: 0.371534	val: 0.934544	test: 0.879935

Epoch: 121
Loss: 0.2825888469815254
RMSE train: 0.501971	val: 1.067265	test: 1.153356
MAE train: 0.365056	val: 0.901372	test: 0.873389

Epoch: 122
Loss: 0.29148951172828674
RMSE train: 0.483688	val: 1.127585	test: 1.161148
MAE train: 0.366975	val: 0.943839	test: 0.894382

Epoch: 123
Loss: 0.3168387785553932
RMSE train: 0.454753	val: 1.130039	test: 1.179392
MAE train: 0.336074	val: 0.948209	test: 0.917070

Epoch: 124
Loss: 0.2965649999678135
RMSE train: 0.493824	val: 1.219185	test: 1.256442
MAE train: 0.361619	val: 1.018974	test: 0.976942

Epoch: 125
Loss: 0.33570949733257294
RMSE train: 0.499249	val: 1.173483	test: 1.206232
MAE train: 0.367723	val: 0.971518	test: 0.937740

Epoch: 126
Loss: 0.28277958184480667
RMSE train: 0.470904	val: 1.139909	test: 1.163268
MAE train: 0.349249	val: 0.926219	test: 0.904673

Epoch: 127
Loss: 0.2866518720984459
RMSE train: 0.500767	val: 1.187490	test: 1.192482
MAE train: 0.366949	val: 0.978565	test: 0.903327

Epoch: 128
Loss: 0.30159562826156616
RMSE train: 0.535925	val: 1.228697	test: 1.228787
MAE train: 0.388447	val: 1.003790	test: 0.927211

Epoch: 129
Loss: 0.27499645203351974
RMSE train: 0.501462	val: 1.123452	test: 1.153784
MAE train: 0.372820	val: 0.922417	test: 0.881202

Epoch: 130
Loss: 0.3029283583164215
RMSE train: 0.474472	val: 1.158629	test: 1.147731
MAE train: 0.348433	val: 0.937292	test: 0.882558

Epoch: 131
Loss: 0.3257715255022049
RMSE train: 0.516803	val: 1.217991	test: 1.203823
MAE train: 0.385653	val: 0.979295	test: 0.919430

Epoch: 132
Loss: 0.29545821249485016
RMSE train: 0.470984	val: 1.071037	test: 1.152393
MAE train: 0.346655	val: 0.878753	test: 0.892445

Epoch: 133
Loss: 0.29772067815065384
RMSE train: 0.464348	val: 1.105905	test: 1.160772
MAE train: 0.345311	val: 0.904202	test: 0.887570

Epoch: 134
Loss: 0.26316435635089874
RMSE train: 0.510535	val: 1.123878	test: 1.200889
MAE train: 0.383976	val: 0.943502	test: 0.895616

Epoch: 135
Loss: 0.26463520899415016
RMSE train: 0.497493	val: 1.155430	test: 1.207965
MAE train: 0.374818	val: 0.964797	test: 0.910829

Epoch: 136
Loss: 0.25645119696855545
RMSE train: 0.498628	val: 1.286318	test: 1.282766
MAE train: 0.370615	val: 1.030843	test: 0.978277

Epoch: 137
Loss: 0.2794947177171707
RMSE train: 0.472476	val: 1.180757	test: 1.225417
MAE train: 0.346324	val: 0.963124	test: 0.941232

Epoch: 138
Loss: 0.29104817286133766
RMSE train: 0.518288	val: 1.157503	test: 1.222477
MAE train: 0.382986	val: 0.963244	test: 0.929742

Epoch: 139
Loss: 0.2412962056696415
RMSE train: 0.527214	val: 1.161178	test: 1.220473
MAE train: 0.390004	val: 0.951905	test: 0.919146

Epoch: 140
Loss: 0.27301614731550217
RMSE train: 0.456686	val: 1.154639	test: 1.180684
MAE train: 0.340954	val: 0.921074	test: 0.919821

Early stopping
Best (RMSE):	 train: 0.525323	val: 0.997534	test: 1.162134
Best (MAE):	 train: 0.391680	val: 0.856157	test: 0.909260
All runs completed.
