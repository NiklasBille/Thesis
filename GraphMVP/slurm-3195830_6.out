>>> Starting run for dataset: lipo
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running RANDOM configs_static_noise_experiments/GraphCL/lipo/noise=0.0.yml on cuda:0
Running RANDOM configs_static_noise_experiments/GraphCL/lipo/noise=0.05.yml on cuda:1
Running RANDOM configs_static_noise_experiments/GraphCL/lipo/noise=0.1.yml on cuda:2
Running RANDOM configs_static_noise_experiments/GraphCL/lipo/noise=0.2.yml on cuda:3
Starting process for seed 1: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.1.yml --runseed 1 --device cuda:2
Starting process for seed 1: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.2.yml --runseed 1 --device cuda:3
Starting process for seed 2: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.1.yml --runseed 2 --device cuda:2
Starting process for seed 2: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.2.yml --runseed 2 --device cuda:3
Starting process for seed 3: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.1.yml --runseed 3 --device cuda:2
Starting process for seed 1: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.05.yml --runseed 1 --device cuda:1
Starting process for seed 3: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.2.yml --runseed 3 --device cuda:3
Starting process for seed 2: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.05.yml --runseed 2 --device cuda:1
Starting process for seed 3: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.05.yml --runseed 3 --device cuda:1
Starting process for seed 1: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.0.yml --runseed 1 --device cuda:0
Starting process for seed 2: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.0.yml --runseed 2 --device cuda:0
Starting process for seed 3: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.0.yml --runseed 3 --device cuda:0
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/lipophilicity/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/lipophilicity/noise=0.0/lipophilicity_scaff_1_20-05_14-43-31  ]
[ Using Seed :  1  ]
[ Using device :  cuda:0  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.032868044716971
RMSE train: 2.106877	val: 2.094124	test: 2.186906
MAE train: 1.855289	val: 1.812544	test: 1.937030

Epoch: 2
Loss: 3.1137449400765553
RMSE train: 1.845906	val: 1.849103	test: 1.935862
MAE train: 1.619588	val: 1.588121	test: 1.703547

Epoch: 3
Loss: 1.8754147802080428
RMSE train: 1.307801	val: 1.316525	test: 1.339409
MAE train: 1.104713	val: 1.101091	test: 1.135074

Epoch: 4
Loss: 1.1732775483812605
RMSE train: 0.892403	val: 0.982347	test: 0.922823
MAE train: 0.710863	val: 0.789258	test: 0.736663

Epoch: 5
Loss: 0.9077736820493426
RMSE train: 0.852851	val: 0.943904	test: 0.875061
MAE train: 0.667987	val: 0.756348	test: 0.703046

Epoch: 6
Loss: 0.8368283467633384
RMSE train: 0.944814	val: 1.032344	test: 0.945836
MAE train: 0.737355	val: 0.826087	test: 0.760950

Epoch: 7
Loss: 0.8435785940715245
RMSE train: 0.846671	val: 0.941176	test: 0.859584
MAE train: 0.661232	val: 0.749802	test: 0.694783

Epoch: 8
Loss: 0.802156639950616
RMSE train: 0.790593	val: 0.883444	test: 0.829242
MAE train: 0.617049	val: 0.695746	test: 0.660766

Epoch: 9
Loss: 0.7538842218262809
RMSE train: 0.836019	val: 0.948906	test: 0.860102
MAE train: 0.652496	val: 0.757329	test: 0.695478

Epoch: 10
Loss: 0.7559899262019566
RMSE train: 0.775854	val: 0.886240	test: 0.815294
MAE train: 0.607556	val: 0.704787	test: 0.654658

Epoch: 11
Loss: 0.6920963823795319
RMSE train: 0.838083	val: 0.933310	test: 0.856232
MAE train: 0.651086	val: 0.740060	test: 0.688710

Epoch: 12
Loss: 0.7152522930077144
RMSE train: 0.758536	val: 0.869308	test: 0.806046
MAE train: 0.590575	val: 0.688741	test: 0.642817

Epoch: 13
Loss: 0.6831566265651158
RMSE train: 0.736861	val: 0.864764	test: 0.805590
MAE train: 0.578136	val: 0.682303	test: 0.649687

Epoch: 14
Loss: 0.6598970890045166
RMSE train: 0.736732	val: 0.863677	test: 0.807997
MAE train: 0.573548	val: 0.682005	test: 0.654969

Epoch: 15
Loss: 0.663007310458592
RMSE train: 0.759107	val: 0.907026	test: 0.831273
MAE train: 0.593419	val: 0.721950	test: 0.678045

Epoch: 16
Loss: 0.6343711018562317
RMSE train: 0.755319	val: 0.893558	test: 0.829051
MAE train: 0.588460	val: 0.704689	test: 0.671390

Epoch: 17
Loss: 0.6266841632979256
RMSE train: 0.697701	val: 0.835351	test: 0.787054
MAE train: 0.540456	val: 0.654178	test: 0.628099

Epoch: 18
Loss: 0.6185764457498278
RMSE train: 0.734827	val: 0.867303	test: 0.806282
MAE train: 0.574310	val: 0.678246	test: 0.653663

Epoch: 19
Loss: 0.6331413813999721
RMSE train: 0.672675	val: 0.823440	test: 0.779342
MAE train: 0.520221	val: 0.636749	test: 0.624147

Epoch: 20
Loss: 0.6094309964350292
RMSE train: 0.722473	val: 0.885979	test: 0.832371
MAE train: 0.560188	val: 0.698319	test: 0.669238

Epoch: 21
Loss: 0.5899569072893688
RMSE train: 0.689376	val: 0.837681	test: 0.794919
MAE train: 0.536723	val: 0.658417	test: 0.629022

Epoch: 22
Loss: 0.5797032926763807
RMSE train: 0.698314	val: 0.855991	test: 0.792004
MAE train: 0.546837	val: 0.675576	test: 0.642128Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/lipophilicity/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/lipophilicity/noise=0.0/lipophilicity_scaff_2_20-05_14-43-31  ]
[ Using Seed :  2  ]
[ Using device :  cuda:0  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 4.74642060484205
RMSE train: 2.056508	val: 2.044824	test: 2.151407
MAE train: 1.813436	val: 1.782263	test: 1.911216

Epoch: 2
Loss: 3.046364256313869
RMSE train: 1.918043	val: 1.901990	test: 2.011942
MAE train: 1.686185	val: 1.651943	test: 1.787089

Epoch: 3
Loss: 1.9096720048359461
RMSE train: 1.297177	val: 1.330441	test: 1.371542
MAE train: 1.089034	val: 1.119100	test: 1.161105

Epoch: 4
Loss: 1.1817493012973241
RMSE train: 1.021577	val: 1.098784	test: 1.084050
MAE train: 0.835065	val: 0.899223	test: 0.896832

Epoch: 5
Loss: 0.9120716622897557
RMSE train: 0.856170	val: 0.954917	test: 0.871412
MAE train: 0.674587	val: 0.764439	test: 0.692663

Epoch: 6
Loss: 0.8609330313546317
RMSE train: 0.881103	val: 0.978676	test: 0.888198
MAE train: 0.685482	val: 0.775807	test: 0.704928

Epoch: 7
Loss: 0.8220389187335968
RMSE train: 0.831200	val: 0.929330	test: 0.850015
MAE train: 0.647465	val: 0.738980	test: 0.673593

Epoch: 8
Loss: 0.7886046426636832
RMSE train: 0.817148	val: 0.937745	test: 0.847900
MAE train: 0.638416	val: 0.752862	test: 0.683713

Epoch: 9
Loss: 0.7486112841538021
RMSE train: 0.799873	val: 0.898287	test: 0.815980
MAE train: 0.623546	val: 0.721573	test: 0.653651

Epoch: 10
Loss: 0.721544508423124
RMSE train: 0.769768	val: 0.892574	test: 0.828315
MAE train: 0.598995	val: 0.704619	test: 0.663323

Epoch: 11
Loss: 0.7009302122252328
RMSE train: 0.793180	val: 0.909957	test: 0.827513
MAE train: 0.617642	val: 0.724744	test: 0.663137

Epoch: 12
Loss: 0.7285085107598986
RMSE train: 0.751543	val: 0.884019	test: 0.812466
MAE train: 0.584456	val: 0.699399	test: 0.646365

Epoch: 13
Loss: 0.6843747283731189
RMSE train: 0.808764	val: 0.920846	test: 0.832673
MAE train: 0.625406	val: 0.723172	test: 0.676453

Epoch: 14
Loss: 0.6432586141995021
RMSE train: 0.728680	val: 0.870312	test: 0.808897
MAE train: 0.569668	val: 0.684079	test: 0.648610

Epoch: 15
Loss: 0.6307202492441449
RMSE train: 0.753979	val: 0.884945	test: 0.814197
MAE train: 0.585325	val: 0.697264	test: 0.652899

Epoch: 16
Loss: 0.6129284756524223
RMSE train: 0.792241	val: 0.939449	test: 0.850183
MAE train: 0.619450	val: 0.750386	test: 0.686198

Epoch: 17
Loss: 0.6107631964342934
RMSE train: 0.700792	val: 0.844679	test: 0.793472
MAE train: 0.539692	val: 0.663577	test: 0.643104

Epoch: 18
Loss: 0.6030706720692771
RMSE train: 0.735768	val: 0.887849	test: 0.823094
MAE train: 0.571518	val: 0.701523	test: 0.664428

Epoch: 19
Loss: 0.5823868662118912
RMSE train: 0.719653	val: 0.889279	test: 0.834192
MAE train: 0.557187	val: 0.698065	test: 0.679299

Epoch: 20
Loss: 0.6151662809508187
RMSE train: 0.732912	val: 0.885368	test: 0.814019
MAE train: 0.567884	val: 0.688262	test: 0.665358

Epoch: 21
Loss: 0.556526705622673
RMSE train: 0.686431	val: 0.846956	test: 0.782005
MAE train: 0.532182	val: 0.660425	test: 0.630279

Epoch: 22
Loss: 0.5617174250738961
RMSE train: 0.696065	val: 0.862074	test: 0.797792
MAE train: 0.539496	val: 0.676915	test: 0.643531Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/lipophilicity/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/lipophilicity/noise=0.0/lipophilicity_scaff_3_20-05_14-43-31  ]
[ Using Seed :  3  ]
[ Using device :  cuda:0  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.1869639839444845
RMSE train: 2.411717	val: 2.366906	test: 2.474696
MAE train: 2.182119	val: 2.112706	test: 2.245083

Epoch: 2
Loss: 3.2221172026225497
RMSE train: 1.831099	val: 1.798472	test: 1.888442
MAE train: 1.612463	val: 1.566907	test: 1.657745

Epoch: 3
Loss: 2.010270152773176
RMSE train: 1.268058	val: 1.300331	test: 1.328683
MAE train: 1.061390	val: 1.080889	test: 1.115178

Epoch: 4
Loss: 1.2756612939493996
RMSE train: 1.068381	val: 1.108136	test: 1.125061
MAE train: 0.887590	val: 0.910669	test: 0.940097

Epoch: 5
Loss: 0.966332882642746
RMSE train: 0.869129	val: 0.958449	test: 0.920815
MAE train: 0.686852	val: 0.759205	test: 0.741356

Epoch: 6
Loss: 0.8225183146340507
RMSE train: 0.853974	val: 0.944261	test: 0.866524
MAE train: 0.659654	val: 0.757907	test: 0.695321

Epoch: 7
Loss: 0.8464634375912803
RMSE train: 0.823034	val: 0.941730	test: 0.877461
MAE train: 0.640005	val: 0.749291	test: 0.704339

Epoch: 8
Loss: 0.7734926513263157
RMSE train: 0.846732	val: 0.950286	test: 0.873127
MAE train: 0.655390	val: 0.755244	test: 0.702598

Epoch: 9
Loss: 0.735871183020728
RMSE train: 0.788004	val: 0.902121	test: 0.857058
MAE train: 0.617409	val: 0.717122	test: 0.691186

Epoch: 10
Loss: 0.7255276569298336
RMSE train: 0.797209	val: 0.921168	test: 0.880022
MAE train: 0.618748	val: 0.728549	test: 0.706967

Epoch: 11
Loss: 0.6938866589750562
RMSE train: 0.744878	val: 0.876678	test: 0.843130
MAE train: 0.581965	val: 0.694675	test: 0.677663

Epoch: 12
Loss: 0.6948337299483163
RMSE train: 0.770689	val: 0.900244	test: 0.850346
MAE train: 0.603476	val: 0.720842	test: 0.683414

Epoch: 13
Loss: 0.7102091610431671
RMSE train: 0.752459	val: 0.915972	test: 0.855342
MAE train: 0.584948	val: 0.723922	test: 0.690949

Epoch: 14
Loss: 0.6718655909810748
RMSE train: 0.740725	val: 0.884906	test: 0.829853
MAE train: 0.575705	val: 0.696478	test: 0.669612

Epoch: 15
Loss: 0.6625165385859353
RMSE train: 0.719313	val: 0.889365	test: 0.827685
MAE train: 0.561102	val: 0.702530	test: 0.665285

Epoch: 16
Loss: 0.622066570179803
RMSE train: 0.722461	val: 0.870730	test: 0.821552
MAE train: 0.566134	val: 0.683158	test: 0.661341

Epoch: 17
Loss: 0.6091544500419072
RMSE train: 0.727179	val: 0.881079	test: 0.838527
MAE train: 0.566165	val: 0.691849	test: 0.678152

Epoch: 18
Loss: 0.6261489774499621
RMSE train: 0.701416	val: 0.869275	test: 0.823585
MAE train: 0.543920	val: 0.685811	test: 0.663722

Epoch: 19
Loss: 0.5925736491169248
RMSE train: 0.761884	val: 0.889843	test: 0.832635
MAE train: 0.595963	val: 0.695342	test: 0.676807

Epoch: 20
Loss: 0.5989559548241752
RMSE train: 0.691992	val: 0.856461	test: 0.823381
MAE train: 0.538242	val: 0.668159	test: 0.665584

Epoch: 21
Loss: 0.5829694483961377
RMSE train: 0.708279	val: 0.894888	test: 0.842250
MAE train: 0.547473	val: 0.696048	test: 0.680780

Epoch: 22
Loss: 0.5850107329232352
RMSE train: 0.712575	val: 0.879124	test: 0.827237
MAE train: 0.554841	val: 0.696148	test: 0.665026Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/lipophilicity/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/lipophilicity/noise=0.05/lipophilicity_scaff_3_20-05_14-43-31  ]
[ Using Seed :  3  ]
[ Using device :  cuda:1  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.38106506211417
RMSE train: 2.263587	val: 2.183819	test: 2.300358
MAE train: 2.017547	val: 1.903369	test: 2.052987

Epoch: 2
Loss: 3.4970243658338274
RMSE train: 1.965683	val: 1.804138	test: 1.898475
MAE train: 1.726476	val: 1.541464	test: 1.665664

Epoch: 3
Loss: 2.285339832305908
RMSE train: 1.464409	val: 1.378885	test: 1.426534
MAE train: 1.247110	val: 1.158688	test: 1.209182

Epoch: 4
Loss: 1.4835736538682665
RMSE train: 1.131543	val: 1.115728	test: 1.096365
MAE train: 0.939287	val: 0.916522	test: 0.903518

Epoch: 5
Loss: 1.1553968361445837
RMSE train: 0.974084	val: 1.039004	test: 0.960344
MAE train: 0.782540	val: 0.826908	test: 0.767127

Epoch: 6
Loss: 1.0519846166883196
RMSE train: 0.938237	val: 1.058144	test: 0.939972
MAE train: 0.740516	val: 0.830311	test: 0.745166

Epoch: 7
Loss: 1.0694315007754736
RMSE train: 0.952134	val: 1.078853	test: 0.955116
MAE train: 0.747935	val: 0.841808	test: 0.758915

Epoch: 8
Loss: 0.9435263276100159
RMSE train: 0.901865	val: 1.066872	test: 0.954428
MAE train: 0.708621	val: 0.836315	test: 0.758311

Epoch: 9
Loss: 0.9585175343922206
RMSE train: 0.901281	val: 1.067869	test: 0.947402
MAE train: 0.709827	val: 0.836036	test: 0.754892

Epoch: 10
Loss: 0.9014919144766671
RMSE train: 0.863685	val: 1.031543	test: 0.928025
MAE train: 0.684425	val: 0.812019	test: 0.745269

Epoch: 11
Loss: 0.8469861362661634
RMSE train: 0.847793	val: 1.021402	test: 0.916497
MAE train: 0.669937	val: 0.808429	test: 0.733555

Epoch: 12
Loss: 0.8624290185315269
RMSE train: 0.885747	val: 1.024898	test: 0.918424
MAE train: 0.693416	val: 0.801046	test: 0.737239

Epoch: 13
Loss: 0.8064251797539848
RMSE train: 0.829757	val: 0.991195	test: 0.886109
MAE train: 0.650999	val: 0.783222	test: 0.700484

Epoch: 14
Loss: 0.7793399095535278
RMSE train: 0.843289	val: 1.029138	test: 0.908738
MAE train: 0.663919	val: 0.806846	test: 0.725374

Epoch: 15
Loss: 0.8100040682724544
RMSE train: 0.814475	val: 1.006610	test: 0.912058
MAE train: 0.637258	val: 0.796007	test: 0.715868

Epoch: 16
Loss: 0.7835762032440731
RMSE train: 0.781560	val: 0.965188	test: 0.883104
MAE train: 0.617542	val: 0.764458	test: 0.701422

Epoch: 17
Loss: 0.799319075686591
RMSE train: 0.842824	val: 1.052626	test: 0.933970
MAE train: 0.659356	val: 0.833212	test: 0.736625

Epoch: 18
Loss: 0.7199105364935738
RMSE train: 0.794416	val: 1.003570	test: 0.900209
MAE train: 0.624270	val: 0.787834	test: 0.726004

Epoch: 19
Loss: 0.7444338202476501
RMSE train: 0.777139	val: 0.992713	test: 0.879673
MAE train: 0.609004	val: 0.790954	test: 0.698650

Epoch: 20
Loss: 0.7324451676436833
RMSE train: 0.787752	val: 1.020884	test: 0.909026
MAE train: 0.618446	val: 0.815306	test: 0.717982

Epoch: 21
Loss: 0.7140723381723676
RMSE train: 0.783728	val: 1.004972	test: 0.885221
MAE train: 0.619332	val: 0.791589	test: 0.705627Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/lipophilicity/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/lipophilicity/noise=0.1/lipophilicity_scaff_1_20-05_14-43-31  ]
[ Using Seed :  1  ]
[ Using device :  cuda:2  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.005537816456386
RMSE train: 2.244751	val: 2.268657	test: 2.371852
MAE train: 2.000796	val: 1.988363	test: 2.130216

Epoch: 2
Loss: 3.306834578514099
RMSE train: 1.845606	val: 1.793842	test: 1.872941
MAE train: 1.604898	val: 1.517178	test: 1.633753

Epoch: 3
Loss: 2.1004409108843123
RMSE train: 1.390728	val: 1.333828	test: 1.353656
MAE train: 1.176131	val: 1.116155	test: 1.135616

Epoch: 4
Loss: 1.4645888294492448
RMSE train: 1.147174	val: 1.171223	test: 1.140841
MAE train: 0.951473	val: 0.962883	test: 0.942417

Epoch: 5
Loss: 1.1866876568113054
RMSE train: 1.019815	val: 1.095934	test: 1.019838
MAE train: 0.820991	val: 0.875038	test: 0.822439

Epoch: 6
Loss: 1.1143795549869537
RMSE train: 1.000007	val: 1.092654	test: 1.003782
MAE train: 0.796508	val: 0.867412	test: 0.806006

Epoch: 7
Loss: 1.104641160794667
RMSE train: 0.971994	val: 1.101022	test: 1.003280
MAE train: 0.778744	val: 0.864968	test: 0.809201

Epoch: 8
Loss: 1.0496203473636083
RMSE train: 0.945642	val: 1.049859	test: 0.984947
MAE train: 0.761510	val: 0.840598	test: 0.793590

Epoch: 9
Loss: 1.0165632920605796
RMSE train: 0.936830	val: 1.073067	test: 0.976105
MAE train: 0.747785	val: 0.844222	test: 0.785142

Epoch: 10
Loss: 0.9818267226219177
RMSE train: 0.912553	val: 1.070740	test: 0.965939
MAE train: 0.729264	val: 0.843874	test: 0.784818

Epoch: 11
Loss: 0.9331471196242741
RMSE train: 0.945573	val: 1.132211	test: 1.023080
MAE train: 0.747354	val: 0.890239	test: 0.819480

Epoch: 12
Loss: 0.9241963454655239
RMSE train: 0.891052	val: 1.095858	test: 0.993169
MAE train: 0.707821	val: 0.868479	test: 0.801259

Epoch: 13
Loss: 0.9144681819847652
RMSE train: 0.865844	val: 1.078960	test: 0.969009
MAE train: 0.689025	val: 0.853991	test: 0.784058

Epoch: 14
Loss: 0.8783613911696843
RMSE train: 0.863586	val: 1.051402	test: 0.945555
MAE train: 0.688983	val: 0.829522	test: 0.755247

Epoch: 15
Loss: 0.8556071732725415
RMSE train: 0.845500	val: 1.065474	test: 0.986735
MAE train: 0.676539	val: 0.843025	test: 0.799642

Epoch: 16
Loss: 0.8556347617081234
RMSE train: 0.831926	val: 1.014394	test: 0.938009
MAE train: 0.665018	val: 0.816466	test: 0.759638

Epoch: 17
Loss: 0.8277300681386676
RMSE train: 0.823271	val: 1.040553	test: 0.939246
MAE train: 0.655518	val: 0.833523	test: 0.753976

Epoch: 18
Loss: 0.7810941849436078
RMSE train: 0.810924	val: 1.016355	test: 0.932214
MAE train: 0.650215	val: 0.818315	test: 0.748855

Epoch: 19
Loss: 0.8097468997750964
RMSE train: 0.823826	val: 1.047783	test: 0.953145
MAE train: 0.658390	val: 0.834161	test: 0.768978

Epoch: 20
Loss: 0.7731860109737941
RMSE train: 0.778998	val: 0.998066	test: 0.926883
MAE train: 0.620778	val: 0.802020	test: 0.737011

Epoch: 21
Loss: 0.7702244988509587
RMSE train: 0.780587	val: 1.064446	test: 0.968487
MAE train: 0.621293	val: 0.847789	test: 0.778737Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/lipophilicity/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/lipophilicity/noise=0.1/lipophilicity_scaff_2_20-05_14-43-31  ]
[ Using Seed :  2  ]
[ Using device :  cuda:2  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.062682083674839
RMSE train: 2.186104	val: 2.260897	test: 2.349227
MAE train: 1.937486	val: 1.985934	test: 2.105940

Epoch: 2
Loss: 3.2676481689725603
RMSE train: 1.838194	val: 1.781636	test: 1.839467
MAE train: 1.593709	val: 1.515754	test: 1.585746

Epoch: 3
Loss: 2.165685398238046
RMSE train: 1.442628	val: 1.354369	test: 1.377827
MAE train: 1.219729	val: 1.127991	test: 1.143767

Epoch: 4
Loss: 1.4846207244055611
RMSE train: 1.127077	val: 1.106630	test: 1.047651
MAE train: 0.931942	val: 0.895911	test: 0.851687

Epoch: 5
Loss: 1.2289023995399475
RMSE train: 1.013954	val: 1.109010	test: 0.990490
MAE train: 0.824211	val: 0.878634	test: 0.797004

Epoch: 6
Loss: 1.1507892523493086
RMSE train: 0.988966	val: 1.135099	test: 1.023260
MAE train: 0.791412	val: 0.887295	test: 0.825603

Epoch: 7
Loss: 1.0769037476607732
RMSE train: 0.973188	val: 1.169586	test: 1.030637
MAE train: 0.771008	val: 0.915915	test: 0.821059

Epoch: 8
Loss: 1.0478018522262573
RMSE train: 0.934449	val: 1.123826	test: 1.021289
MAE train: 0.747157	val: 0.878418	test: 0.823327

Epoch: 9
Loss: 0.9848542085715702
RMSE train: 0.912345	val: 1.073025	test: 0.964630
MAE train: 0.728783	val: 0.842736	test: 0.778248

Epoch: 10
Loss: 1.015976790870939
RMSE train: 0.913829	val: 1.079396	test: 0.995086
MAE train: 0.731812	val: 0.844724	test: 0.798994

Epoch: 11
Loss: 0.9361449820654733
RMSE train: 0.873566	val: 1.059717	test: 0.961372
MAE train: 0.693957	val: 0.830581	test: 0.770172

Epoch: 12
Loss: 0.9395329781941005
RMSE train: 0.878824	val: 1.107568	test: 1.017822
MAE train: 0.697770	val: 0.870379	test: 0.811526

Epoch: 13
Loss: 0.8835739663669041
RMSE train: 0.866046	val: 1.141695	test: 1.039421
MAE train: 0.693447	val: 0.892173	test: 0.830970

Epoch: 14
Loss: 0.8726644601140704
RMSE train: 0.834805	val: 1.127255	test: 1.020004
MAE train: 0.662967	val: 0.884545	test: 0.807873

Epoch: 15
Loss: 0.8568369235311236
RMSE train: 0.831328	val: 1.092519	test: 0.998274
MAE train: 0.664075	val: 0.859845	test: 0.796295

Epoch: 16
Loss: 0.8134129600865501
RMSE train: 0.807200	val: 1.058587	test: 0.980982
MAE train: 0.644128	val: 0.828455	test: 0.782433

Epoch: 17
Loss: 0.8048943834645408
RMSE train: 0.832150	val: 1.017788	test: 0.930170
MAE train: 0.667001	val: 0.808654	test: 0.745408

Epoch: 18
Loss: 0.777329649244036
RMSE train: 0.789306	val: 1.033927	test: 0.947561
MAE train: 0.628450	val: 0.815536	test: 0.757327

Epoch: 19
Loss: 0.7563608969960894
RMSE train: 0.785213	val: 1.103802	test: 1.003137
MAE train: 0.626094	val: 0.870824	test: 0.804327

Epoch: 20
Loss: 0.7719194165297917
RMSE train: 0.774644	val: 1.149933	test: 1.076027
MAE train: 0.617437	val: 0.894408	test: 0.865591

Epoch: 21
Loss: 0.7444153938974652
RMSE train: 0.769993	val: 1.051930	test: 0.965242
MAE train: 0.611281	val: 0.825278	test: 0.767593Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/lipophilicity/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/lipophilicity/noise=0.1/lipophilicity_scaff_3_20-05_14-43-31  ]
[ Using Seed :  3  ]
[ Using device :  cuda:2  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.387527908597674
RMSE train: 2.192187	val: 2.175881	test: 2.291037
MAE train: 1.944936	val: 1.893885	test: 2.041062

Epoch: 2
Loss: 3.5442395891462053
RMSE train: 1.868026	val: 1.766724	test: 1.863307
MAE train: 1.627056	val: 1.507981	test: 1.627885

Epoch: 3
Loss: 2.3485255326543535
RMSE train: 1.521735	val: 1.444683	test: 1.499002
MAE train: 1.297994	val: 1.217273	test: 1.276642

Epoch: 4
Loss: 1.5679812942232405
RMSE train: 1.149625	val: 1.181524	test: 1.148664
MAE train: 0.953840	val: 0.980377	test: 0.950286

Epoch: 5
Loss: 1.2584298253059387
RMSE train: 1.046556	val: 1.090917	test: 1.028584
MAE train: 0.848811	val: 0.880568	test: 0.829490

Epoch: 6
Loss: 1.1723210896764482
RMSE train: 1.003601	val: 1.085322	test: 0.967103
MAE train: 0.807387	val: 0.852749	test: 0.773974

Epoch: 7
Loss: 1.1776517246450697
RMSE train: 0.996198	val: 1.133020	test: 1.002251
MAE train: 0.793368	val: 0.878284	test: 0.797474

Epoch: 8
Loss: 1.0513802894524165
RMSE train: 0.957591	val: 1.082080	test: 0.971431
MAE train: 0.762699	val: 0.849633	test: 0.777019

Epoch: 9
Loss: 1.0901434549263544
RMSE train: 0.958224	val: 1.128711	test: 1.003712
MAE train: 0.763458	val: 0.878015	test: 0.799283

Epoch: 10
Loss: 1.0491489001682825
RMSE train: 0.926600	val: 1.044892	test: 0.969589
MAE train: 0.746163	val: 0.824088	test: 0.774336

Epoch: 11
Loss: 0.9513272557939801
RMSE train: 0.899690	val: 1.029629	test: 0.947821
MAE train: 0.721461	val: 0.803634	test: 0.756046

Epoch: 12
Loss: 0.9741239207131522
RMSE train: 0.912131	val: 1.087852	test: 0.973384
MAE train: 0.724831	val: 0.838513	test: 0.766085

Epoch: 13
Loss: 0.9310610166617802
RMSE train: 0.875565	val: 1.054630	test: 0.962216
MAE train: 0.695760	val: 0.817188	test: 0.759911

Epoch: 14
Loss: 0.8721432345254081
RMSE train: 0.859367	val: 1.061955	test: 0.962745
MAE train: 0.688459	val: 0.823925	test: 0.760114

Epoch: 15
Loss: 0.9037227715764727
RMSE train: 0.844932	val: 1.042949	test: 0.950810
MAE train: 0.674364	val: 0.817270	test: 0.749723

Epoch: 16
Loss: 0.9001250948224749
RMSE train: 0.831544	val: 1.012359	test: 0.936952
MAE train: 0.666564	val: 0.787441	test: 0.742211

Epoch: 17
Loss: 0.8900878088814872
RMSE train: 0.830227	val: 1.051432	test: 0.937426
MAE train: 0.664933	val: 0.826095	test: 0.746283

Epoch: 18
Loss: 0.8186355999537877
RMSE train: 0.836728	val: 1.042501	test: 0.950453
MAE train: 0.672473	val: 0.812741	test: 0.756782

Epoch: 19
Loss: 0.8167511948517391
RMSE train: 0.806135	val: 1.041656	test: 0.967253
MAE train: 0.644269	val: 0.820435	test: 0.757115

Epoch: 20
Loss: 0.7960818409919739
RMSE train: 0.792885	val: 1.044873	test: 0.976314
MAE train: 0.632996	val: 0.817268	test: 0.766964

Epoch: 21
Loss: 0.7806544601917267
RMSE train: 0.767861	val: 1.043314	test: 0.960069
MAE train: 0.616794	val: 0.815340	test: 0.753660Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/lipophilicity/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/lipophilicity/noise=0.05/lipophilicity_scaff_2_20-05_14-43-31  ]
[ Using Seed :  2  ]
[ Using device :  cuda:1  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.0337047917502264
RMSE train: 2.309141	val: 2.330432	test: 2.433419
MAE train: 2.066070	val: 2.063041	test: 2.196231

Epoch: 2
Loss: 3.223944289343698
RMSE train: 1.871774	val: 1.681008	test: 1.738525
MAE train: 1.629323	val: 1.417985	test: 1.483549

Epoch: 3
Loss: 2.0887697339057922
RMSE train: 1.459730	val: 1.370601	test: 1.412458
MAE train: 1.234981	val: 1.130677	test: 1.187710

Epoch: 4
Loss: 1.4080370920045036
RMSE train: 1.063947	val: 1.073975	test: 1.049016
MAE train: 0.871654	val: 0.877732	test: 0.867870

Epoch: 5
Loss: 1.105266468865531
RMSE train: 0.960309	val: 1.016986	test: 0.937338
MAE train: 0.776700	val: 0.819295	test: 0.761945

Epoch: 6
Loss: 1.0417002269199915
RMSE train: 0.933765	val: 1.042461	test: 0.964283
MAE train: 0.737563	val: 0.814305	test: 0.771938

Epoch: 7
Loss: 0.9700858124664852
RMSE train: 0.953117	val: 1.066771	test: 0.956651
MAE train: 0.744414	val: 0.836474	test: 0.759232

Epoch: 8
Loss: 0.9487671341214862
RMSE train: 0.885805	val: 1.007224	test: 0.941497
MAE train: 0.695425	val: 0.784343	test: 0.754037

Epoch: 9
Loss: 0.9004304919924054
RMSE train: 0.877715	val: 0.977551	test: 0.905435
MAE train: 0.693470	val: 0.774441	test: 0.738916

Epoch: 10
Loss: 0.908282437494823
RMSE train: 0.852214	val: 0.979135	test: 0.936067
MAE train: 0.673926	val: 0.771491	test: 0.753232

Epoch: 11
Loss: 0.8503891740526471
RMSE train: 0.862114	val: 0.948570	test: 0.935663
MAE train: 0.693417	val: 0.769121	test: 0.753181

Epoch: 12
Loss: 0.8386445428643908
RMSE train: 0.829012	val: 0.939683	test: 0.893349
MAE train: 0.662017	val: 0.751303	test: 0.723041

Epoch: 13
Loss: 0.8194053556237902
RMSE train: 0.820938	val: 0.966381	test: 0.905595
MAE train: 0.654731	val: 0.761617	test: 0.727584

Epoch: 14
Loss: 0.8054289392062596
RMSE train: 0.797048	val: 0.979961	test: 0.908886
MAE train: 0.630447	val: 0.770955	test: 0.732089

Epoch: 15
Loss: 0.7913572064467839
RMSE train: 0.808481	val: 0.949859	test: 0.883119
MAE train: 0.640423	val: 0.752076	test: 0.706170

Epoch: 16
Loss: 0.7498631136757987
RMSE train: 0.788981	val: 0.956844	test: 0.894634
MAE train: 0.624791	val: 0.750961	test: 0.712421

Epoch: 17
Loss: 0.7621144482067653
RMSE train: 0.789616	val: 0.953795	test: 0.893287
MAE train: 0.627017	val: 0.759348	test: 0.725252

Epoch: 18
Loss: 0.7388125572885785
RMSE train: 0.766609	val: 0.914433	test: 0.901305
MAE train: 0.611658	val: 0.730934	test: 0.716128

Epoch: 19
Loss: 0.6970402640955788
RMSE train: 0.772987	val: 0.932094	test: 0.882780
MAE train: 0.616190	val: 0.741568	test: 0.712785

Epoch: 20
Loss: 0.728482050555093
RMSE train: 0.780451	val: 0.989573	test: 0.946307
MAE train: 0.615586	val: 0.778763	test: 0.745480

Epoch: 21
Loss: 0.6846164039203099
RMSE train: 0.782277	val: 0.940734	test: 0.883620
MAE train: 0.618835	val: 0.750010	test: 0.710819Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/lipophilicity/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/lipophilicity/noise=0.05/lipophilicity_scaff_1_20-05_14-43-31  ]
[ Using Seed :  1  ]
[ Using device :  cuda:1  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 4.977647015026638
RMSE train: 2.139699	val: 2.160957	test: 2.246337
MAE train: 1.895017	val: 1.889255	test: 2.011776

Epoch: 2
Loss: 3.2583234139851163
RMSE train: 1.787689	val: 1.677184	test: 1.746676
MAE train: 1.546698	val: 1.420857	test: 1.508505

Epoch: 3
Loss: 2.025828182697296
RMSE train: 1.479483	val: 1.383119	test: 1.413981
MAE train: 1.260583	val: 1.168721	test: 1.192002

Epoch: 4
Loss: 1.3673272388322013
RMSE train: 1.078744	val: 1.095904	test: 1.053672
MAE train: 0.895254	val: 0.907250	test: 0.863669

Epoch: 5
Loss: 1.1101415497916085
RMSE train: 0.969427	val: 1.076113	test: 0.977510
MAE train: 0.771390	val: 0.839271	test: 0.784064

Epoch: 6
Loss: 1.007484495639801
RMSE train: 0.935385	val: 1.025092	test: 0.908348
MAE train: 0.744088	val: 0.804675	test: 0.734649

Epoch: 7
Loss: 0.9521831018584115
RMSE train: 0.909144	val: 1.020459	test: 0.933131
MAE train: 0.723320	val: 0.810864	test: 0.759539

Epoch: 8
Loss: 0.9353327708584922
RMSE train: 0.894255	val: 1.003560	test: 0.913154
MAE train: 0.713342	val: 0.794198	test: 0.732031

Epoch: 9
Loss: 0.9254009766238076
RMSE train: 0.891199	val: 1.048119	test: 0.932558
MAE train: 0.698482	val: 0.832412	test: 0.751070

Epoch: 10
Loss: 0.8894699769360679
RMSE train: 0.875411	val: 1.001761	test: 0.885596
MAE train: 0.692084	val: 0.789164	test: 0.721398

Epoch: 11
Loss: 0.8568085219178881
RMSE train: 0.917516	val: 1.089194	test: 0.961039
MAE train: 0.714397	val: 0.857503	test: 0.772017

Epoch: 12
Loss: 0.8348181375435421
RMSE train: 0.842378	val: 1.021913	test: 0.911948
MAE train: 0.663762	val: 0.805603	test: 0.731850

Epoch: 13
Loss: 0.8133768609591893
RMSE train: 0.820853	val: 0.974486	test: 0.869083
MAE train: 0.652380	val: 0.769286	test: 0.709233

Epoch: 14
Loss: 0.8022966129439217
RMSE train: 0.827055	val: 0.983448	test: 0.886048
MAE train: 0.650915	val: 0.777893	test: 0.719312

Epoch: 15
Loss: 0.7824713970933642
RMSE train: 0.830948	val: 1.000554	test: 0.894076
MAE train: 0.660544	val: 0.788777	test: 0.723934

Epoch: 16
Loss: 0.7901426340852465
RMSE train: 0.829023	val: 0.994955	test: 0.890936
MAE train: 0.649226	val: 0.790771	test: 0.716859

Epoch: 17
Loss: 0.768261581659317
RMSE train: 0.791861	val: 0.961060	test: 0.873003
MAE train: 0.629972	val: 0.757528	test: 0.712023

Epoch: 18
Loss: 0.7614989919321877
RMSE train: 0.785179	val: 0.929130	test: 0.867729
MAE train: 0.629265	val: 0.741761	test: 0.698533

Epoch: 19
Loss: 0.7345127846513476
RMSE train: 0.793853	val: 0.996939	test: 0.892674
MAE train: 0.627292	val: 0.791266	test: 0.723743

Epoch: 20
Loss: 0.7330202886036464
RMSE train: 0.778454	val: 0.989425	test: 0.875938
MAE train: 0.608736	val: 0.785213	test: 0.708597

Epoch: 21
Loss: 0.6953286273138863
RMSE train: 0.797959	val: 0.995721	test: 0.890693
MAE train: 0.624801	val: 0.788866	test: 0.721526Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:3
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/lipophilicity/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/lipophilicity/noise=0.2/lipophilicity_scaff_3_20-05_14-43-31  ]
[ Using Seed :  3  ]
[ Using device :  cuda:3  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.422386544091361
RMSE train: 2.103188	val: 2.157155	test: 2.244531
MAE train: 1.854655	val: 1.878063	test: 1.995726

Epoch: 2
Loss: 3.6133128234318326
RMSE train: 1.917260	val: 1.919729	test: 1.968821
MAE train: 1.672470	val: 1.648906	test: 1.719562

Epoch: 3
Loss: 2.4147533178329468
RMSE train: 1.662328	val: 1.786672	test: 1.838469
MAE train: 1.425294	val: 1.523626	test: 1.597058

Epoch: 4
Loss: 1.6325746519224984
RMSE train: 1.294274	val: 1.372886	test: 1.343411
MAE train: 1.090633	val: 1.142944	test: 1.114792

Epoch: 5
Loss: 1.3086878572191512
RMSE train: 1.107057	val: 1.185846	test: 1.104507
MAE train: 0.913605	val: 0.968704	test: 0.901558

Epoch: 6
Loss: 1.237985440662929
RMSE train: 1.052411	val: 1.143685	test: 1.035963
MAE train: 0.856404	val: 0.920209	test: 0.838240

Epoch: 7
Loss: 1.2276612562792641
RMSE train: 1.052029	val: 1.213798	test: 1.074580
MAE train: 0.831286	val: 0.948007	test: 0.861469

Epoch: 8
Loss: 1.1485806277820043
RMSE train: 1.022790	val: 1.215465	test: 1.084052
MAE train: 0.816121	val: 0.946732	test: 0.872896

Epoch: 9
Loss: 1.1542177540915353
RMSE train: 0.998121	val: 1.211366	test: 1.103598
MAE train: 0.806747	val: 0.941078	test: 0.889568

Epoch: 10
Loss: 1.1343824991158076
RMSE train: 0.991235	val: 1.101076	test: 1.054187
MAE train: 0.802102	val: 0.892930	test: 0.850134

Epoch: 11
Loss: 1.0925065577030182
RMSE train: 0.968678	val: 1.080983	test: 1.026897
MAE train: 0.779862	val: 0.869910	test: 0.825784

Epoch: 12
Loss: 1.0572316774300166
RMSE train: 0.996460	val: 1.271281	test: 1.148874
MAE train: 0.791322	val: 0.993946	test: 0.918091

Epoch: 13
Loss: 1.0363108004842485
RMSE train: 0.937405	val: 1.075739	test: 0.987679
MAE train: 0.753950	val: 0.846653	test: 0.798058

Epoch: 14
Loss: 0.9733530112675258
RMSE train: 0.919571	val: 1.162426	test: 1.083180
MAE train: 0.737400	val: 0.902374	test: 0.872914

Epoch: 15
Loss: 0.9709592078413282
RMSE train: 0.906482	val: 1.118222	test: 1.030035
MAE train: 0.725804	val: 0.865610	test: 0.830306

Epoch: 16
Loss: 0.996702424117497
RMSE train: 0.899597	val: 1.086630	test: 0.983439
MAE train: 0.717401	val: 0.845826	test: 0.791648

Epoch: 17
Loss: 0.9903374527181897
RMSE train: 0.889704	val: 1.175323	test: 1.082105
MAE train: 0.710971	val: 0.914244	test: 0.870789

Epoch: 18
Loss: 0.9193046391010284
RMSE train: 0.867314	val: 1.089949	test: 0.958250
MAE train: 0.699472	val: 0.847728	test: 0.766320

Epoch: 19
Loss: 0.8919563123158046
RMSE train: 0.856760	val: 1.055341	test: 0.959035
MAE train: 0.692016	val: 0.839991	test: 0.780944

Epoch: 20
Loss: 0.9064254973615918
RMSE train: 0.853721	val: 1.061540	test: 0.985946
MAE train: 0.687545	val: 0.850601	test: 0.801524

Epoch: 21
Loss: 0.8932881695883614
RMSE train: 0.826821	val: 1.050934	test: 0.981514
MAE train: 0.667803	val: 0.842056	test: 0.799163Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:3
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/lipophilicity/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/lipophilicity/noise=0.2/lipophilicity_scaff_1_20-05_14-43-31  ]
[ Using Seed :  1  ]
[ Using device :  cuda:3  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.071109397070749
RMSE train: 2.187114	val: 2.249650	test: 2.352199
MAE train: 1.942444	val: 1.975924	test: 2.116627

Epoch: 2
Loss: 3.3885204451424733
RMSE train: 1.872135	val: 1.820127	test: 1.882012
MAE train: 1.627854	val: 1.553227	test: 1.637795

Epoch: 3
Loss: 2.159336124147688
RMSE train: 1.460169	val: 1.381101	test: 1.373682
MAE train: 1.236422	val: 1.160777	test: 1.144193

Epoch: 4
Loss: 1.5517393010003226
RMSE train: 1.204273	val: 1.211077	test: 1.143986
MAE train: 1.001763	val: 0.992084	test: 0.937931

Epoch: 5
Loss: 1.276665014880044
RMSE train: 1.073128	val: 1.136554	test: 1.016482
MAE train: 0.876752	val: 0.908470	test: 0.821083

Epoch: 6
Loss: 1.2489757197243827
RMSE train: 1.055076	val: 1.198969	test: 1.062086
MAE train: 0.843363	val: 0.944020	test: 0.848941

Epoch: 7
Loss: 1.1898215838841029
RMSE train: 1.020526	val: 1.182841	test: 1.048762
MAE train: 0.819078	val: 0.931392	test: 0.847318

Epoch: 8
Loss: 1.1733327678271703
RMSE train: 1.008622	val: 1.151491	test: 1.024362
MAE train: 0.810219	val: 0.913293	test: 0.825437

Epoch: 9
Loss: 1.1459379962512426
RMSE train: 0.989975	val: 1.132213	test: 1.011619
MAE train: 0.795002	val: 0.895682	test: 0.817456

Epoch: 10
Loss: 1.1191976581301009
RMSE train: 0.981439	val: 1.161735	test: 1.028092
MAE train: 0.778615	val: 0.912225	test: 0.827151

Epoch: 11
Loss: 1.0437727102211543
RMSE train: 0.960938	val: 1.162808	test: 1.020113
MAE train: 0.763163	val: 0.911913	test: 0.816487

Epoch: 12
Loss: 1.046342249427523
RMSE train: 0.949165	val: 1.173323	test: 1.024758
MAE train: 0.752790	val: 0.919966	test: 0.821960

Epoch: 13
Loss: 1.0397876713957106
RMSE train: 0.952389	val: 1.221725	test: 1.066303
MAE train: 0.750772	val: 0.957548	test: 0.852194

Epoch: 14
Loss: 0.9858164616993496
RMSE train: 0.912489	val: 1.156500	test: 1.013859
MAE train: 0.724840	val: 0.910227	test: 0.811315

Epoch: 15
Loss: 0.9549498770918164
RMSE train: 0.898963	val: 1.152492	test: 1.006728
MAE train: 0.716313	val: 0.905141	test: 0.805703

Epoch: 16
Loss: 0.967409257377897
RMSE train: 0.883495	val: 1.105101	test: 0.977102
MAE train: 0.710692	val: 0.872150	test: 0.787390

Epoch: 17
Loss: 0.9081972965172359
RMSE train: 0.868002	val: 1.088314	test: 0.965113
MAE train: 0.695502	val: 0.856806	test: 0.769628

Epoch: 18
Loss: 0.8866143524646759
RMSE train: 0.843875	val: 1.088070	test: 0.963721
MAE train: 0.674324	val: 0.849579	test: 0.778425

Epoch: 19
Loss: 0.871387141091483
RMSE train: 0.843285	val: 1.241638	test: 1.080776
MAE train: 0.671514	val: 0.972588	test: 0.858151

Epoch: 20
Loss: 0.823949796812875
RMSE train: 0.851963	val: 1.241064	test: 1.086111
MAE train: 0.677587	val: 0.976738	test: 0.858408

Epoch: 21
Loss: 0.8330250254699162
RMSE train: 0.803955	val: 1.125343	test: 0.993642
MAE train: 0.645556	val: 0.891900	test: 0.795106Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/lipo/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:3
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/lipophilicity/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/lipophilicity/noise=0.2/lipophilicity_scaff_2_20-05_14-43-31  ]
[ Using Seed :  2  ]
[ Using device :  cuda:3  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.070234996931894
RMSE train: 2.210580	val: 2.307797	test: 2.409737
MAE train: 1.964499	val: 2.034385	test: 2.173756

Epoch: 2
Loss: 3.3502638169697354
RMSE train: 1.919610	val: 1.907241	test: 1.974522
MAE train: 1.673971	val: 1.637003	test: 1.727745

Epoch: 3
Loss: 2.237403469426291
RMSE train: 1.595793	val: 1.630926	test: 1.678824
MAE train: 1.359959	val: 1.376013	test: 1.428091

Epoch: 4
Loss: 1.6024796877588545
RMSE train: 1.215729	val: 1.246702	test: 1.233068
MAE train: 1.013022	val: 1.035444	test: 1.015891

Epoch: 5
Loss: 1.3417853798185075
RMSE train: 1.103164	val: 1.147474	test: 1.104894
MAE train: 0.901757	val: 0.934551	test: 0.905348

Epoch: 6
Loss: 1.3088749136243547
RMSE train: 1.068617	val: 1.134951	test: 1.053053
MAE train: 0.859454	val: 0.913258	test: 0.855745

Epoch: 7
Loss: 1.2034006374222892
RMSE train: 1.050925	val: 1.101025	test: 1.029645
MAE train: 0.850939	val: 0.891403	test: 0.838957

Epoch: 8
Loss: 1.1933481437819344
RMSE train: 1.024705	val: 1.099638	test: 1.015224
MAE train: 0.827004	val: 0.887778	test: 0.831918

Epoch: 9
Loss: 1.132770368031093
RMSE train: 1.009985	val: 1.101039	test: 1.015220
MAE train: 0.809241	val: 0.885566	test: 0.831360

Epoch: 10
Loss: 1.1724384767668588
RMSE train: 0.982577	val: 1.093189	test: 1.033414
MAE train: 0.787834	val: 0.878606	test: 0.833889

Epoch: 11
Loss: 1.103786723954337
RMSE train: 0.987135	val: 1.093507	test: 0.999396
MAE train: 0.787744	val: 0.873381	test: 0.814206

Epoch: 12
Loss: 1.1075274518557958
RMSE train: 0.965739	val: 1.078744	test: 0.985816
MAE train: 0.767227	val: 0.848753	test: 0.797040

Epoch: 13
Loss: 1.06181977050645
RMSE train: 0.955177	val: 1.101143	test: 1.036121
MAE train: 0.767434	val: 0.894724	test: 0.844149

Epoch: 14
Loss: 1.0170839726924896
RMSE train: 0.942490	val: 1.092954	test: 0.981836
MAE train: 0.748937	val: 0.866696	test: 0.798431

Epoch: 15
Loss: 1.0096024657998766
RMSE train: 0.906151	val: 1.058229	test: 0.983733
MAE train: 0.724248	val: 0.840846	test: 0.808030

Epoch: 16
Loss: 0.9798203749316079
RMSE train: 0.893207	val: 1.084066	test: 0.981363
MAE train: 0.712120	val: 0.849324	test: 0.797141

Epoch: 17
Loss: 0.9263534418174199
RMSE train: 0.892092	val: 1.069139	test: 0.975945
MAE train: 0.711317	val: 0.842841	test: 0.798378

Epoch: 18
Loss: 0.9371454119682312
RMSE train: 0.879305	val: 1.080112	test: 0.975813
MAE train: 0.697635	val: 0.842070	test: 0.793310

Epoch: 19
Loss: 0.8989847557885307
RMSE train: 0.867092	val: 1.078688	test: 0.983951
MAE train: 0.689871	val: 0.838895	test: 0.801234

Epoch: 20
Loss: 0.8953662855284554
RMSE train: 0.834222	val: 1.043059	test: 0.965441
MAE train: 0.665256	val: 0.808930	test: 0.783040

Epoch: 21
Loss: 0.8673148623534611
RMSE train: 0.871600	val: 1.144540	test: 1.041760
MAE train: 0.690856	val: 0.890933	test: 0.829487

Epoch: 23
Loss: 0.5929151177406311
RMSE train: 0.667692	val: 0.819189	test: 0.777245
MAE train: 0.521282	val: 0.641845	test: 0.627335

Epoch: 24
Loss: 0.5525197280304772
RMSE train: 0.670960	val: 0.837301	test: 0.769713
MAE train: 0.522978	val: 0.661893	test: 0.618272

Epoch: 25
Loss: 0.5374271018164498
RMSE train: 0.706080	val: 0.867020	test: 0.806972
MAE train: 0.548356	val: 0.676926	test: 0.654720

Epoch: 26
Loss: 0.5477464411939893
RMSE train: 0.687732	val: 0.862337	test: 0.804478
MAE train: 0.531216	val: 0.675851	test: 0.653899

Epoch: 27
Loss: 0.5163872497422355
RMSE train: 0.637066	val: 0.805780	test: 0.769751
MAE train: 0.493145	val: 0.621305	test: 0.612128

Epoch: 28
Loss: 0.5446162543126515
RMSE train: 0.648680	val: 0.824025	test: 0.771991
MAE train: 0.500770	val: 0.636364	test: 0.619413

Epoch: 29
Loss: 0.5012224955218179
RMSE train: 0.621748	val: 0.800738	test: 0.763016
MAE train: 0.475828	val: 0.613113	test: 0.610724

Epoch: 30
Loss: 0.4989697252001081
RMSE train: 0.675938	val: 0.857192	test: 0.793823
MAE train: 0.521915	val: 0.664443	test: 0.639979

Epoch: 31
Loss: 0.48880984101976666
RMSE train: 0.620555	val: 0.801299	test: 0.770121
MAE train: 0.476270	val: 0.615155	test: 0.612900

Epoch: 32
Loss: 0.48566882950919016
RMSE train: 0.664783	val: 0.839445	test: 0.794726
MAE train: 0.515059	val: 0.653298	test: 0.637526

Epoch: 33
Loss: 0.4824651948043278
RMSE train: 0.622910	val: 0.795442	test: 0.769828
MAE train: 0.485405	val: 0.609749	test: 0.612937

Epoch: 34
Loss: 0.4966008024556296
RMSE train: 0.652506	val: 0.837409	test: 0.803810
MAE train: 0.504640	val: 0.652141	test: 0.645949

Epoch: 35
Loss: 0.470907245363508
RMSE train: 0.628204	val: 0.806772	test: 0.782580
MAE train: 0.484543	val: 0.614313	test: 0.624646

Epoch: 36
Loss: 0.4998379967042378
RMSE train: 0.630646	val: 0.813834	test: 0.788953
MAE train: 0.485112	val: 0.622392	test: 0.635618

Epoch: 37
Loss: 0.49496010158743176
RMSE train: 0.607099	val: 0.797374	test: 0.768810
MAE train: 0.470484	val: 0.617098	test: 0.610829

Epoch: 38
Loss: 0.46881348959037233
RMSE train: 0.613744	val: 0.797595	test: 0.769394
MAE train: 0.475182	val: 0.610158	test: 0.611028

Epoch: 39
Loss: 0.47194976253168924
RMSE train: 0.628791	val: 0.808761	test: 0.773518
MAE train: 0.491497	val: 0.619890	test: 0.617947

Epoch: 40
Loss: 0.4382045567035675
RMSE train: 0.606119	val: 0.799726	test: 0.757218
MAE train: 0.468538	val: 0.612520	test: 0.604320

Epoch: 41
Loss: 0.4707381214414324
RMSE train: 0.591904	val: 0.788011	test: 0.765634
MAE train: 0.457174	val: 0.600381	test: 0.611044

Epoch: 42
Loss: 0.42422604986599516
RMSE train: 0.589162	val: 0.791871	test: 0.762816
MAE train: 0.451637	val: 0.599992	test: 0.609024

Epoch: 43
Loss: 0.4441991661276136
RMSE train: 0.620618	val: 0.805467	test: 0.763799
MAE train: 0.478075	val: 0.609388	test: 0.610908

Epoch: 44
Loss: 0.42465280635016306
RMSE train: 0.601077	val: 0.803248	test: 0.757987
MAE train: 0.462537	val: 0.610756	test: 0.600588

Epoch: 45
Loss: 0.436663629753249
RMSE train: 0.591959	val: 0.789372	test: 0.753638
MAE train: 0.458392	val: 0.600076	test: 0.601066

Epoch: 46
Loss: 0.4330185524054936
RMSE train: 0.587565	val: 0.788061	test: 0.756473
MAE train: 0.457481	val: 0.600907	test: 0.600682

Epoch: 47
Loss: 0.4130472370556423
RMSE train: 0.581552	val: 0.792506	test: 0.756169
MAE train: 0.449217	val: 0.602703	test: 0.600393

Epoch: 48
Loss: 0.4091230609587261
RMSE train: 0.604844	val: 0.816020	test: 0.766465
MAE train: 0.467061	val: 0.624777	test: 0.609095

Epoch: 49
Loss: 0.41048626388822285
RMSE train: 0.576510	val: 0.779659	test: 0.765145
MAE train: 0.443840	val: 0.592219	test: 0.604350

Epoch: 50
Loss: 0.41351046732493807
RMSE train: 0.569799	val: 0.781419	test: 0.758680
MAE train: 0.443606	val: 0.592214	test: 0.602997

Epoch: 51
Loss: 0.38892504572868347
RMSE train: 0.568768	val: 0.783670	test: 0.753253
MAE train: 0.440010	val: 0.593365	test: 0.602786

Epoch: 52
Loss: 0.3947785943746567
RMSE train: 0.572155	val: 0.774658	test: 0.751057
MAE train: 0.444853	val: 0.587960	test: 0.596938

Epoch: 53
Loss: 0.40819427158151356
RMSE train: 0.577719	val: 0.772849	test: 0.760218
MAE train: 0.446493	val: 0.586060	test: 0.600006

Epoch: 54
Loss: 0.4032398270709174
RMSE train: 0.640013	val: 0.857507	test: 0.814205
MAE train: 0.496224	val: 0.649682	test: 0.650100

Epoch: 55
Loss: 0.40394824530397144
RMSE train: 0.546284	val: 0.768600	test: 0.749526
MAE train: 0.420534	val: 0.585268	test: 0.592949

Epoch: 56
Loss: 0.39919807442596983
RMSE train: 0.578781	val: 0.781065	test: 0.750298
MAE train: 0.450310	val: 0.595095	test: 0.593656

Epoch: 57
Loss: 0.37901528605393003
RMSE train: 0.544361	val: 0.757536	test: 0.755833
MAE train: 0.419867	val: 0.572427	test: 0.597526

Epoch: 58
Loss: 0.3838663697242737
RMSE train: 0.551126	val: 0.766972	test: 0.751292
MAE train: 0.427563	val: 0.579487	test: 0.599669

Epoch: 59
Loss: 0.3950431091444833
RMSE train: 0.562099	val: 0.785597	test: 0.745697
MAE train: 0.436792	val: 0.594169	test: 0.589596

Epoch: 60
Loss: 0.36980740087372915
RMSE train: 0.538574	val: 0.766109	test: 0.756281
MAE train: 0.415858	val: 0.582192	test: 0.601169

Epoch: 61
Loss: 0.38649967525686535
RMSE train: 0.562080	val: 0.778982	test: 0.755477
MAE train: 0.436864	val: 0.588122	test: 0.600852

Epoch: 62
Loss: 0.37545500482831684
RMSE train: 0.543977	val: 0.760815	test: 0.752583
MAE train: 0.417416	val: 0.578191	test: 0.595457

Epoch: 63
Loss: 0.37055290171078276
RMSE train: 0.558514	val: 0.792145	test: 0.784330
MAE train: 0.429917	val: 0.591128	test: 0.612686

Epoch: 64
Loss: 0.36436877718993593
RMSE train: 0.547589	val: 0.760690	test: 0.773564
MAE train: 0.427678	val: 0.570442	test: 0.613325

Epoch: 65
Loss: 0.3961338698863983
RMSE train: 0.531034	val: 0.761034	test: 0.749455
MAE train: 0.407697	val: 0.565536	test: 0.590350

Epoch: 66
Loss: 0.36473606739725384
RMSE train: 0.542680	val: 0.781273	test: 0.759130
MAE train: 0.418457	val: 0.586562	test: 0.598317

Epoch: 67
Loss: 0.34277127257415224
RMSE train: 0.559449	val: 0.793166	test: 0.782966
MAE train: 0.432603	val: 0.598855	test: 0.612557

Epoch: 68
Loss: 0.3527222488607679
RMSE train: 0.538997	val: 0.776586	test: 0.763469
MAE train: 0.416503	val: 0.583852	test: 0.599637

Epoch: 69
Loss: 0.3573008349963597
RMSE train: 0.523260	val: 0.762823	test: 0.756794
MAE train: 0.402747	val: 0.570060	test: 0.591453

Epoch: 70
Loss: 0.3532783197505133
RMSE train: 0.555260	val: 0.787778	test: 0.773815
MAE train: 0.430322	val: 0.592051	test: 0.609062

Epoch: 71
Loss: 0.36668427501405987
RMSE train: 0.527250	val: 0.756225	test: 0.748106
MAE train: 0.410265	val: 0.572990	test: 0.589226

Epoch: 72
Loss: 0.36901286244392395
RMSE train: 0.529948	val: 0.768704	test: 0.763127
MAE train: 0.411227	val: 0.581781	test: 0.600425

Epoch: 73
Loss: 0.3401686762060438
RMSE train: 0.527220	val: 0.777720	test: 0.762699
MAE train: 0.406506	val: 0.587374	test: 0.595150

Epoch: 74
Loss: 0.33917788309710367
RMSE train: 0.521311	val: 0.772688	test: 0.763514
MAE train: 0.403924	val: 0.578781	test: 0.599814

Epoch: 75
Loss: 0.33418604199375423
RMSE train: 0.498226	val: 0.739654	test: 0.739359
MAE train: 0.380331	val: 0.558327	test: 0.579788

Epoch: 76
Loss: 0.33071322313376833
RMSE train: 0.532858	val: 0.775103	test: 0.750808
MAE train: 0.412249	val: 0.582787	test: 0.586705

Epoch: 77
Loss: 0.33626570871898104
RMSE train: 0.532435	val: 0.757608	test: 0.751373
MAE train: 0.411891	val: 0.574233	test: 0.591611

Epoch: 78
Loss: 0.3213709465094975
RMSE train: 0.516630	val: 0.744461	test: 0.744402
MAE train: 0.398015	val: 0.561136	test: 0.580944

Epoch: 79
Loss: 0.3307391937289919
RMSE train: 0.516298	val: 0.763788	test: 0.756561
MAE train: 0.397505	val: 0.570101	test: 0.590619

Epoch: 80
Loss: 0.34036271486963543
RMSE train: 0.525032	val: 0.781986	test: 0.753038
MAE train: 0.407672	val: 0.590700	test: 0.593890

Epoch: 81
Loss: 0.32391829043626785
RMSE train: 0.501723	val: 0.750391	test: 0.747562
MAE train: 0.386990	val: 0.565854	test: 0.582941

Epoch: 82
Loss: 0.3230081924370357
RMSE train: 0.500357	val: 0.745019	test: 0.741114
MAE train: 0.385860	val: 0.565006	test: 0.579715

Epoch: 83
Loss: 0.30890073095049175
RMSE train: 0.522232	val: 0.759343	test: 0.749085

Epoch: 22
Loss: 0.7260829848902566
RMSE train: 0.759213	val: 1.079786	test: 1.004736
MAE train: 0.606773	val: 0.847332	test: 0.814674

Epoch: 23
Loss: 0.7268390953540802
RMSE train: 0.738427	val: 1.062730	test: 0.976775
MAE train: 0.588925	val: 0.832854	test: 0.787700

Epoch: 24
Loss: 0.7025070062705449
RMSE train: 0.739664	val: 1.102138	test: 1.002664
MAE train: 0.587727	val: 0.866688	test: 0.796827

Epoch: 25
Loss: 0.6825982587678092
RMSE train: 0.704006	val: 1.055404	test: 0.977348
MAE train: 0.560006	val: 0.833772	test: 0.778429

Epoch: 26
Loss: 0.6414485786642347
RMSE train: 0.715433	val: 1.123540	test: 1.053622
MAE train: 0.567608	val: 0.881218	test: 0.839320

Epoch: 27
Loss: 0.622402834040778
RMSE train: 0.683641	val: 1.017482	test: 0.943030
MAE train: 0.544898	val: 0.800757	test: 0.748431

Epoch: 28
Loss: 0.6023104275975909
RMSE train: 0.679992	val: 0.988302	test: 0.914880
MAE train: 0.539506	val: 0.786492	test: 0.725443

Epoch: 29
Loss: 0.6105264978749412
RMSE train: 0.676467	val: 0.979998	test: 0.914889
MAE train: 0.539844	val: 0.781755	test: 0.728376

Epoch: 30
Loss: 0.6044200829097203
RMSE train: 0.669866	val: 1.017044	test: 0.929004
MAE train: 0.531224	val: 0.801793	test: 0.736473

Epoch: 31
Loss: 0.5888535146202359
RMSE train: 0.657146	val: 1.058792	test: 0.972830
MAE train: 0.521610	val: 0.846197	test: 0.772707

Epoch: 32
Loss: 0.5955916089670998
RMSE train: 0.646980	val: 1.014532	test: 0.928125
MAE train: 0.514604	val: 0.804630	test: 0.742252

Epoch: 33
Loss: 0.569261542388371
RMSE train: 0.656650	val: 1.080257	test: 0.979340
MAE train: 0.522758	val: 0.866240	test: 0.775256

Epoch: 34
Loss: 0.5611165634223393
RMSE train: 0.628206	val: 1.044176	test: 0.978649
MAE train: 0.497682	val: 0.836865	test: 0.769230

Epoch: 35
Loss: 0.5554501371724265
RMSE train: 0.637311	val: 0.969839	test: 0.909903
MAE train: 0.511773	val: 0.775956	test: 0.724717

Epoch: 36
Loss: 0.5448920279741287
RMSE train: 0.616897	val: 1.000900	test: 0.935400
MAE train: 0.490801	val: 0.803251	test: 0.732080

Epoch: 37
Loss: 0.5468775055238179
RMSE train: 0.590953	val: 1.014894	test: 0.921398
MAE train: 0.469726	val: 0.813276	test: 0.726780

Epoch: 38
Loss: 0.5022865640265601
RMSE train: 0.612768	val: 1.026719	test: 0.935742
MAE train: 0.486827	val: 0.825180	test: 0.746218

Epoch: 39
Loss: 0.500671026962144
RMSE train: 0.588545	val: 1.042562	test: 0.956193
MAE train: 0.468956	val: 0.828056	test: 0.755235

Epoch: 40
Loss: 0.48294939952237265
RMSE train: 0.559683	val: 1.070445	test: 0.981636
MAE train: 0.443804	val: 0.847211	test: 0.771688

Epoch: 41
Loss: 0.47852628997394014
RMSE train: 0.602945	val: 1.132923	test: 1.037250
MAE train: 0.476833	val: 0.901172	test: 0.820552

Epoch: 42
Loss: 0.45375857821532656
RMSE train: 0.585563	val: 1.094141	test: 1.003714
MAE train: 0.460726	val: 0.868169	test: 0.788626

Epoch: 43
Loss: 0.4216796393905367
RMSE train: 0.535508	val: 0.978712	test: 0.922471
MAE train: 0.422834	val: 0.785290	test: 0.728893

Epoch: 44
Loss: 0.4484462503876005
RMSE train: 0.538571	val: 1.032546	test: 0.943574
MAE train: 0.426797	val: 0.816495	test: 0.753949

Epoch: 45
Loss: 0.4473577397210257
RMSE train: 0.555523	val: 0.999023	test: 0.917830
MAE train: 0.438293	val: 0.804134	test: 0.719666

Epoch: 46
Loss: 0.4331837275198528
RMSE train: 0.591658	val: 1.099253	test: 1.003524
MAE train: 0.471964	val: 0.873023	test: 0.783234

Epoch: 47
Loss: 0.405831407223429
RMSE train: 0.634732	val: 1.143326	test: 1.046577
MAE train: 0.506711	val: 0.913603	test: 0.822440

Epoch: 48
Loss: 0.3814708026392119
RMSE train: 0.506191	val: 1.079295	test: 0.996028
MAE train: 0.403253	val: 0.854939	test: 0.788476

Epoch: 49
Loss: 0.41450958592551096
RMSE train: 0.512890	val: 0.995850	test: 0.927210
MAE train: 0.405037	val: 0.798688	test: 0.724098

Epoch: 50
Loss: 0.3834768533706665
RMSE train: 0.541965	val: 1.049398	test: 0.972074
MAE train: 0.434726	val: 0.833428	test: 0.762370

Epoch: 51
Loss: 0.37848352108682903
RMSE train: 0.516673	val: 1.066765	test: 0.983610
MAE train: 0.410166	val: 0.851072	test: 0.773005

Epoch: 52
Loss: 0.3704483870949064
RMSE train: 0.482670	val: 1.054464	test: 0.988856
MAE train: 0.378878	val: 0.838570	test: 0.768035

Epoch: 53
Loss: 0.37349223451954977
RMSE train: 0.496387	val: 1.006866	test: 0.939991
MAE train: 0.392688	val: 0.802463	test: 0.747947

Epoch: 54
Loss: 0.3726502465350287
RMSE train: 0.466379	val: 0.967761	test: 0.907899
MAE train: 0.370503	val: 0.772672	test: 0.713927

Epoch: 55
Loss: 0.3422527185508183
RMSE train: 0.501586	val: 1.062758	test: 0.969778
MAE train: 0.398711	val: 0.843944	test: 0.758278

Epoch: 56
Loss: 0.31706168183258604
RMSE train: 0.435036	val: 1.047847	test: 0.969989
MAE train: 0.342367	val: 0.840437	test: 0.761343

Epoch: 57
Loss: 0.32650570571422577
RMSE train: 0.439657	val: 0.987240	test: 0.917076
MAE train: 0.347877	val: 0.790963	test: 0.714697

Epoch: 58
Loss: 0.30455731068338665
RMSE train: 0.432050	val: 1.035656	test: 0.940866
MAE train: 0.342301	val: 0.826887	test: 0.730178

Epoch: 59
Loss: 0.3246885985136032
RMSE train: 0.437716	val: 0.986507	test: 0.916610
MAE train: 0.347371	val: 0.784743	test: 0.721030

Epoch: 60
Loss: 0.31619579238551004
RMSE train: 0.445817	val: 0.979929	test: 0.895034
MAE train: 0.351969	val: 0.782927	test: 0.699664

Epoch: 61
Loss: 0.33055771461554934
RMSE train: 0.406938	val: 0.999254	test: 0.918382
MAE train: 0.319834	val: 0.800110	test: 0.721267

Epoch: 62
Loss: 0.30537443501608713
RMSE train: 0.431691	val: 0.972856	test: 0.897136
MAE train: 0.342628	val: 0.781791	test: 0.712727

Epoch: 63
Loss: 0.28539103269577026
RMSE train: 0.404608	val: 1.004308	test: 0.929374
MAE train: 0.319607	val: 0.805367	test: 0.728629

Epoch: 64
Loss: 0.28424147197178434
RMSE train: 0.413118	val: 0.989498	test: 0.913313
MAE train: 0.329326	val: 0.790660	test: 0.723672

Epoch: 65
Loss: 0.2942658130611692
RMSE train: 0.407411	val: 1.038414	test: 0.949308
MAE train: 0.323002	val: 0.830283	test: 0.739365

Epoch: 66
Loss: 0.28067302278109957
RMSE train: 0.462028	val: 1.056188	test: 0.969584
MAE train: 0.371305	val: 0.843121	test: 0.756259

Epoch: 67
Loss: 0.257511243224144
RMSE train: 0.417851	val: 0.998442	test: 0.916680
MAE train: 0.331292	val: 0.799271	test: 0.714536

Epoch: 68
Loss: 0.2691535609109061
RMSE train: 0.413072	val: 1.016579	test: 0.943015
MAE train: 0.328848	val: 0.811085	test: 0.737809

Epoch: 69
Loss: 0.27138917999608175
RMSE train: 0.390021	val: 1.003766	test: 0.932750
MAE train: 0.305915	val: 0.805489	test: 0.726574

Epoch: 70
Loss: 0.27817269201789585
RMSE train: 0.451822	val: 1.031784	test: 0.949124
MAE train: 0.359489	val: 0.828221	test: 0.742404

Epoch: 71
Loss: 0.26144624820777346
RMSE train: 0.390067	val: 1.053445	test: 0.967935
MAE train: 0.310760	val: 0.837018	test: 0.755999

Epoch: 72
Loss: 0.24847884901932307
RMSE train: 0.380137	val: 1.004040	test: 0.933131
MAE train: 0.301441	val: 0.802870	test: 0.723314

Epoch: 73
Loss: 0.24842161472354615
RMSE train: 0.379446	val: 1.004143	test: 0.927474
MAE train: 0.300940	val: 0.802605	test: 0.724931

Epoch: 74
Loss: 0.24262843600341252
RMSE train: 0.375439	val: 1.001386	test: 0.915978
MAE train: 0.297895	val: 0.797596	test: 0.720545

Epoch: 75
Loss: 0.24407356871025904
RMSE train: 0.348534	val: 0.997557	test: 0.916302
MAE train: 0.274499	val: 0.796313	test: 0.715402

Epoch: 76
Loss: 0.23454186000994273
RMSE train: 0.401163	val: 1.051167	test: 0.954938
MAE train: 0.318601	val: 0.833931	test: 0.749080

Epoch: 77
Loss: 0.24526951994214738
RMSE train: 0.410536	val: 1.072493	test: 0.977691
MAE train: 0.331551	val: 0.857314	test: 0.765545

Epoch: 78
Loss: 0.21198358599628722
RMSE train: 0.371180	val: 0.974558	test: 0.906818
MAE train: 0.293042	val: 0.774744	test: 0.716745

Epoch: 79
Loss: 0.22069240574325835
RMSE train: 0.364470	val: 0.990320	test: 0.915261
MAE train: 0.289501	val: 0.792399	test: 0.713519

Epoch: 80
Loss: 0.22469642332621984
RMSE train: 0.325001	val: 0.994256	test: 0.921018
MAE train: 0.256199	val: 0.791655	test: 0.719447

Epoch: 81
Loss: 0.22893049248627254
RMSE train: 0.397168	val: 1.012976	test: 0.936690
MAE train: 0.316895	val: 0.807483	test: 0.731889

Epoch: 82
Loss: 0.21967057138681412
RMSE train: 0.340115	val: 0.988831	test: 0.917920

Epoch: 22
Loss: 0.695174195936748
RMSE train: 0.762807	val: 0.990062	test: 0.890830
MAE train: 0.603381	val: 0.787982	test: 0.712058

Epoch: 23
Loss: 0.6919664016791752
RMSE train: 0.744649	val: 0.946484	test: 0.863939
MAE train: 0.589231	val: 0.751070	test: 0.690975

Epoch: 24
Loss: 0.6296908536127636
RMSE train: 0.737614	val: 0.990008	test: 0.896795
MAE train: 0.582786	val: 0.787029	test: 0.718001

Epoch: 25
Loss: 0.6364808806351253
RMSE train: 0.707800	val: 0.926076	test: 0.865212
MAE train: 0.558881	val: 0.737728	test: 0.691684

Epoch: 26
Loss: 0.6625087899821145
RMSE train: 0.752343	val: 0.941614	test: 0.856397
MAE train: 0.591698	val: 0.742526	test: 0.689699

Epoch: 27
Loss: 0.6287941464356014
RMSE train: 0.742848	val: 0.975464	test: 0.890470
MAE train: 0.583257	val: 0.770057	test: 0.708099

Epoch: 28
Loss: 0.6139814981392452
RMSE train: 0.752564	val: 0.969171	test: 0.884521
MAE train: 0.588859	val: 0.768923	test: 0.691105

Epoch: 29
Loss: 0.6073660595076424
RMSE train: 0.684817	val: 0.924044	test: 0.858077
MAE train: 0.541857	val: 0.726256	test: 0.675262

Epoch: 30
Loss: 0.5829685074942452
RMSE train: 0.658361	val: 0.928521	test: 0.864133
MAE train: 0.514495	val: 0.730167	test: 0.687259

Epoch: 31
Loss: 0.5919787819896426
RMSE train: 0.696030	val: 0.920417	test: 0.834823
MAE train: 0.548449	val: 0.726325	test: 0.662891

Epoch: 32
Loss: 0.5486077112810952
RMSE train: 0.745709	val: 1.006344	test: 0.918530
MAE train: 0.582967	val: 0.792392	test: 0.719699

Epoch: 33
Loss: 0.5459610521793365
RMSE train: 0.663296	val: 0.925127	test: 0.836559
MAE train: 0.522192	val: 0.732041	test: 0.656889

Epoch: 34
Loss: 0.5270212015935353
RMSE train: 0.654634	val: 0.926223	test: 0.836532
MAE train: 0.514462	val: 0.728072	test: 0.665367

Epoch: 35
Loss: 0.5296421838658196
RMSE train: 0.636579	val: 0.917753	test: 0.835530
MAE train: 0.503180	val: 0.719258	test: 0.653059

Epoch: 36
Loss: 0.5343108262334552
RMSE train: 0.647523	val: 0.952622	test: 0.858388
MAE train: 0.509276	val: 0.751047	test: 0.677363

Epoch: 37
Loss: 0.49848098840032307
RMSE train: 0.690967	val: 0.948254	test: 0.845287
MAE train: 0.541718	val: 0.750104	test: 0.657963

Epoch: 38
Loss: 0.49383047435964855
RMSE train: 0.621659	val: 0.898609	test: 0.816914
MAE train: 0.490707	val: 0.702101	test: 0.648435

Epoch: 39
Loss: 0.5070318111351558
RMSE train: 0.608864	val: 0.912444	test: 0.827165
MAE train: 0.478175	val: 0.716645	test: 0.649611

Epoch: 40
Loss: 0.4933220446109772
RMSE train: 0.692235	val: 0.996052	test: 0.905327
MAE train: 0.543025	val: 0.788997	test: 0.702205

Epoch: 41
Loss: 0.4960110400404249
RMSE train: 0.632446	val: 0.942162	test: 0.853792
MAE train: 0.496679	val: 0.740137	test: 0.663270

Epoch: 42
Loss: 0.4590810409614018
RMSE train: 0.617258	val: 0.918760	test: 0.831319
MAE train: 0.486270	val: 0.722374	test: 0.657549

Epoch: 43
Loss: 0.4530703531844275
RMSE train: 0.572716	val: 0.884621	test: 0.811614
MAE train: 0.453070	val: 0.690020	test: 0.644559

Epoch: 44
Loss: 0.4585873229163034
RMSE train: 0.581036	val: 0.904824	test: 0.833031
MAE train: 0.458519	val: 0.707857	test: 0.651268

Epoch: 45
Loss: 0.44610847532749176
RMSE train: 0.599060	val: 0.915575	test: 0.845754
MAE train: 0.472815	val: 0.716974	test: 0.667738

Epoch: 46
Loss: 0.45784576875822885
RMSE train: 0.584787	val: 0.899506	test: 0.835407
MAE train: 0.458876	val: 0.714784	test: 0.642904

Epoch: 47
Loss: 0.43805831245013643
RMSE train: 0.604407	val: 0.951886	test: 0.861573
MAE train: 0.476199	val: 0.756660	test: 0.684237

Epoch: 48
Loss: 0.4248541125229427
RMSE train: 0.563929	val: 0.895543	test: 0.809436
MAE train: 0.444387	val: 0.708186	test: 0.637456

Epoch: 49
Loss: 0.4225903217281614
RMSE train: 0.546339	val: 0.902001	test: 0.824971
MAE train: 0.429620	val: 0.709494	test: 0.643067

Epoch: 50
Loss: 0.4036783512149538
RMSE train: 0.586534	val: 0.931791	test: 0.845642
MAE train: 0.463203	val: 0.729472	test: 0.666706

Epoch: 51
Loss: 0.3833732030221394
RMSE train: 0.549151	val: 0.923091	test: 0.825452
MAE train: 0.432663	val: 0.726229	test: 0.648234

Epoch: 52
Loss: 0.40079669441495624
RMSE train: 0.519790	val: 0.880426	test: 0.790691
MAE train: 0.408015	val: 0.692247	test: 0.621679

Epoch: 53
Loss: 0.4029575458594731
RMSE train: 0.636533	val: 0.990949	test: 0.900126
MAE train: 0.499919	val: 0.784481	test: 0.699808

Epoch: 54
Loss: 0.3826207731451307
RMSE train: 0.590963	val: 0.913636	test: 0.817320
MAE train: 0.465414	val: 0.714632	test: 0.645562

Epoch: 55
Loss: 0.3908257761171886
RMSE train: 0.617405	val: 0.919141	test: 0.827407
MAE train: 0.489744	val: 0.723774	test: 0.657176

Epoch: 56
Loss: 0.35698130301066805
RMSE train: 0.590061	val: 0.966556	test: 0.878485
MAE train: 0.465418	val: 0.758742	test: 0.681590

Epoch: 57
Loss: 0.36650954825537546
RMSE train: 0.552066	val: 0.939164	test: 0.845265
MAE train: 0.434687	val: 0.736693	test: 0.656087

Epoch: 58
Loss: 0.36059379151889254
RMSE train: 0.533517	val: 0.913783	test: 0.828959
MAE train: 0.420326	val: 0.717363	test: 0.651655

Epoch: 59
Loss: 0.34468656352588106
RMSE train: 0.524552	val: 0.908504	test: 0.829999
MAE train: 0.413546	val: 0.707006	test: 0.648242

Epoch: 60
Loss: 0.3411972458873476
RMSE train: 0.528023	val: 0.930945	test: 0.857658
MAE train: 0.416329	val: 0.731026	test: 0.674687

Epoch: 61
Loss: 0.331881120800972
RMSE train: 0.499831	val: 0.899654	test: 0.827715
MAE train: 0.392777	val: 0.696923	test: 0.653574

Epoch: 62
Loss: 0.34070264441626413
RMSE train: 0.559599	val: 0.918931	test: 0.842668
MAE train: 0.441609	val: 0.723640	test: 0.659723

Epoch: 63
Loss: 0.3298592290707997
RMSE train: 0.518208	val: 0.888497	test: 0.831765
MAE train: 0.408567	val: 0.702269	test: 0.650354

Epoch: 64
Loss: 0.3183087429830006
RMSE train: 0.618053	val: 0.994494	test: 0.912102
MAE train: 0.491603	val: 0.780417	test: 0.719874

Epoch: 65
Loss: 0.3313615939446858
RMSE train: 0.471246	val: 0.898853	test: 0.839586
MAE train: 0.370487	val: 0.695487	test: 0.666965

Epoch: 66
Loss: 0.33029368094035555
RMSE train: 0.487943	val: 0.907864	test: 0.842490
MAE train: 0.383140	val: 0.706676	test: 0.654748

Epoch: 67
Loss: 0.32944334191935404
RMSE train: 0.475632	val: 0.859144	test: 0.814902
MAE train: 0.375812	val: 0.675439	test: 0.645762

Epoch: 68
Loss: 0.3229291843516486
RMSE train: 0.508007	val: 0.948703	test: 0.878611
MAE train: 0.399267	val: 0.744722	test: 0.686708

Epoch: 69
Loss: 0.3109676795346396
RMSE train: 0.531320	val: 0.928724	test: 0.852970
MAE train: 0.415491	val: 0.730545	test: 0.674926

Epoch: 70
Loss: 0.307305317904268
RMSE train: 0.486198	val: 0.918932	test: 0.842041
MAE train: 0.383866	val: 0.720219	test: 0.658075

Epoch: 71
Loss: 0.28006945869752337
RMSE train: 0.459367	val: 0.891178	test: 0.821014
MAE train: 0.361698	val: 0.693424	test: 0.647215

Epoch: 72
Loss: 0.29017929094178335
RMSE train: 0.482323	val: 0.923293	test: 0.844814
MAE train: 0.379119	val: 0.720674	test: 0.668033

Epoch: 73
Loss: 0.2875294823731695
RMSE train: 0.495366	val: 0.927268	test: 0.850499
MAE train: 0.388135	val: 0.731240	test: 0.667147

Epoch: 74
Loss: 0.30494909839970724
RMSE train: 0.460389	val: 0.880879	test: 0.803186
MAE train: 0.363550	val: 0.695388	test: 0.633024

Epoch: 75
Loss: 0.29694160180432455
RMSE train: 0.495263	val: 0.933435	test: 0.863536
MAE train: 0.388699	val: 0.736153	test: 0.674016

Epoch: 76
Loss: 0.2896547349435942
RMSE train: 0.569519	val: 0.971584	test: 0.886925
MAE train: 0.455844	val: 0.767240	test: 0.692709

Epoch: 77
Loss: 0.28283065451042994
RMSE train: 0.516432	val: 0.958318	test: 0.881548
MAE train: 0.409987	val: 0.751501	test: 0.703185

Epoch: 78
Loss: 0.27915556196655544
RMSE train: 0.515844	val: 0.947218	test: 0.860134
MAE train: 0.409843	val: 0.750962	test: 0.678371

Epoch: 79
Loss: 0.27079679391213823
RMSE train: 0.410230	val: 0.886610	test: 0.825081
MAE train: 0.323437	val: 0.689145	test: 0.652428

Epoch: 80
Loss: 0.24661751091480255
RMSE train: 0.498055	val: 0.946388	test: 0.865215
MAE train: 0.393046	val: 0.740987	test: 0.686621

Epoch: 81
Loss: 0.2657322425927435
RMSE train: 0.455313	val: 0.944361	test: 0.866258
MAE train: 0.360729	val: 0.747776	test: 0.686646

Epoch: 82
Loss: 0.2496355761374746
RMSE train: 0.511176	val: 0.953689	test: 0.878229

Epoch: 22
Loss: 0.6829054611069816
RMSE train: 0.730615	val: 0.929824	test: 0.899812
MAE train: 0.578821	val: 0.735242	test: 0.728010

Epoch: 23
Loss: 0.7008894767080035
RMSE train: 0.748348	val: 0.966103	test: 0.903989
MAE train: 0.595532	val: 0.762951	test: 0.732772

Epoch: 24
Loss: 0.6481299826077053
RMSE train: 0.718559	val: 0.902690	test: 0.877271
MAE train: 0.572552	val: 0.712555	test: 0.705284

Epoch: 25
Loss: 0.62624420438494
RMSE train: 0.707296	val: 0.901109	test: 0.872090
MAE train: 0.562174	val: 0.715319	test: 0.693820

Epoch: 26
Loss: 0.6125280346189227
RMSE train: 0.705007	val: 0.896039	test: 0.869948
MAE train: 0.558817	val: 0.705686	test: 0.695079

Epoch: 27
Loss: 0.5845790441547122
RMSE train: 0.683990	val: 0.908928	test: 0.878051
MAE train: 0.540742	val: 0.711829	test: 0.701378

Epoch: 28
Loss: 0.5738063348191125
RMSE train: 0.673137	val: 0.900727	test: 0.860580
MAE train: 0.532272	val: 0.718287	test: 0.690025

Epoch: 29
Loss: 0.5580256964479174
RMSE train: 0.676636	val: 0.889158	test: 0.860309
MAE train: 0.536088	val: 0.705535	test: 0.690303

Epoch: 30
Loss: 0.5497961810656956
RMSE train: 0.684735	val: 0.923344	test: 0.885914
MAE train: 0.541969	val: 0.727850	test: 0.711298

Epoch: 31
Loss: 0.5675875395536423
RMSE train: 0.647601	val: 0.901288	test: 0.873789
MAE train: 0.514686	val: 0.712385	test: 0.694096

Epoch: 32
Loss: 0.5539200582674572
RMSE train: 0.651967	val: 0.891375	test: 0.875787
MAE train: 0.520580	val: 0.698316	test: 0.695638

Epoch: 33
Loss: 0.5218730249575206
RMSE train: 0.648264	val: 0.917914	test: 0.882045
MAE train: 0.516190	val: 0.729113	test: 0.705209

Epoch: 34
Loss: 0.5070749478680747
RMSE train: 0.631985	val: 0.888688	test: 0.890806
MAE train: 0.501475	val: 0.700837	test: 0.700485

Epoch: 35
Loss: 0.5242275872400829
RMSE train: 0.635989	val: 0.901483	test: 0.872200
MAE train: 0.506537	val: 0.715367	test: 0.689481

Epoch: 36
Loss: 0.5094524430377143
RMSE train: 0.631311	val: 0.909369	test: 0.876802
MAE train: 0.503734	val: 0.726516	test: 0.700395

Epoch: 37
Loss: 0.5050932105098452
RMSE train: 0.613820	val: 0.891478	test: 0.855101
MAE train: 0.486021	val: 0.698518	test: 0.678349

Epoch: 38
Loss: 0.49339739978313446
RMSE train: 0.614251	val: 0.901560	test: 0.865541
MAE train: 0.487985	val: 0.713696	test: 0.682554

Epoch: 39
Loss: 0.49175554301057545
RMSE train: 0.641943	val: 0.937229	test: 0.880664
MAE train: 0.511582	val: 0.746446	test: 0.701785

Epoch: 40
Loss: 0.4517642302172525
RMSE train: 0.613884	val: 0.914137	test: 0.880520
MAE train: 0.485630	val: 0.724112	test: 0.696630

Epoch: 41
Loss: 0.45427801992212025
RMSE train: 0.615702	val: 0.925689	test: 0.887058
MAE train: 0.490134	val: 0.733078	test: 0.708709

Epoch: 42
Loss: 0.4489896936076028
RMSE train: 0.578015	val: 0.900532	test: 0.871096
MAE train: 0.459082	val: 0.707954	test: 0.690279

Epoch: 43
Loss: 0.42219746112823486
RMSE train: 0.572671	val: 0.878315	test: 0.859917
MAE train: 0.455327	val: 0.698096	test: 0.682700

Epoch: 44
Loss: 0.4286345882075174
RMSE train: 0.590196	val: 0.903177	test: 0.874028
MAE train: 0.470282	val: 0.711182	test: 0.702133

Epoch: 45
Loss: 0.443758208836828
RMSE train: 0.585429	val: 0.868577	test: 0.848110
MAE train: 0.464909	val: 0.692813	test: 0.669857

Epoch: 46
Loss: 0.4244650559765952
RMSE train: 0.575085	val: 0.905289	test: 0.871821
MAE train: 0.457691	val: 0.703532	test: 0.694630

Epoch: 47
Loss: 0.39250252502305166
RMSE train: 0.545808	val: 0.893401	test: 0.858240
MAE train: 0.432395	val: 0.703727	test: 0.691219

Epoch: 48
Loss: 0.39043364354542326
RMSE train: 0.527043	val: 0.855241	test: 0.840689
MAE train: 0.417094	val: 0.675278	test: 0.668989

Epoch: 49
Loss: 0.4282785675355366
RMSE train: 0.562021	val: 0.865066	test: 0.867420
MAE train: 0.451137	val: 0.677844	test: 0.689264

Epoch: 50
Loss: 0.4071962514093944
RMSE train: 0.547952	val: 0.867367	test: 0.857272
MAE train: 0.435002	val: 0.685154	test: 0.683776

Epoch: 51
Loss: 0.39785648243767874
RMSE train: 0.539995	val: 0.907023	test: 0.869373
MAE train: 0.429347	val: 0.718256	test: 0.688545

Epoch: 52
Loss: 0.3854308383805411
RMSE train: 0.512176	val: 0.884408	test: 0.869579
MAE train: 0.404790	val: 0.701250	test: 0.684962

Epoch: 53
Loss: 0.3922648898192814
RMSE train: 0.534989	val: 0.868412	test: 0.843166
MAE train: 0.429221	val: 0.691570	test: 0.672130

Epoch: 54
Loss: 0.3545643410512379
RMSE train: 0.532051	val: 0.887072	test: 0.854804
MAE train: 0.421700	val: 0.700086	test: 0.676876

Epoch: 55
Loss: 0.3588901438883373
RMSE train: 0.525249	val: 0.881665	test: 0.856453
MAE train: 0.416866	val: 0.696900	test: 0.681559

Epoch: 56
Loss: 0.3505840280226299
RMSE train: 0.543910	val: 0.875814	test: 0.856132
MAE train: 0.435809	val: 0.687677	test: 0.685955

Epoch: 57
Loss: 0.34662494701998575
RMSE train: 0.504641	val: 0.877888	test: 0.849073
MAE train: 0.401150	val: 0.694837	test: 0.670702

Epoch: 58
Loss: 0.34311368422848837
RMSE train: 0.506642	val: 0.884265	test: 0.843770
MAE train: 0.401479	val: 0.700436	test: 0.660080

Epoch: 59
Loss: 0.3454709755522864
RMSE train: 0.498331	val: 0.895404	test: 0.864702
MAE train: 0.397384	val: 0.711282	test: 0.680492

Epoch: 60
Loss: 0.3313512908560889
RMSE train: 0.486397	val: 0.874938	test: 0.843408
MAE train: 0.387399	val: 0.691789	test: 0.667015

Epoch: 61
Loss: 0.3193771413394383
RMSE train: 0.464529	val: 0.863344	test: 0.834388
MAE train: 0.368966	val: 0.680752	test: 0.658029

Epoch: 62
Loss: 0.3274025470018387
RMSE train: 0.482517	val: 0.893834	test: 0.856251
MAE train: 0.384349	val: 0.702324	test: 0.682993

Epoch: 63
Loss: 0.30462015526635305
RMSE train: 0.502393	val: 0.927043	test: 0.899474
MAE train: 0.399810	val: 0.732599	test: 0.707627

Epoch: 64
Loss: 0.3236829361745289
RMSE train: 0.485319	val: 0.868017	test: 0.845439
MAE train: 0.388768	val: 0.681649	test: 0.671896

Epoch: 65
Loss: 0.30420425959995817
RMSE train: 0.479119	val: 0.867774	test: 0.836441
MAE train: 0.381149	val: 0.685914	test: 0.663424

Epoch: 66
Loss: 0.3047841659614018
RMSE train: 0.495477	val: 0.884149	test: 0.848156
MAE train: 0.396208	val: 0.697957	test: 0.677446

Epoch: 67
Loss: 0.2767673134803772
RMSE train: 0.465279	val: 0.852203	test: 0.827388
MAE train: 0.371886	val: 0.667682	test: 0.656203

Epoch: 68
Loss: 0.2819628651653017
RMSE train: 0.476361	val: 0.850524	test: 0.836983
MAE train: 0.381213	val: 0.665834	test: 0.677239

Epoch: 69
Loss: 0.303682379424572
RMSE train: 0.510861	val: 0.900479	test: 0.879523
MAE train: 0.410568	val: 0.718049	test: 0.698005

Epoch: 70
Loss: 0.295885722552027
RMSE train: 0.457937	val: 0.864281	test: 0.855866
MAE train: 0.365817	val: 0.681733	test: 0.675557

Epoch: 71
Loss: 0.28059275661196026
RMSE train: 0.434171	val: 0.832674	test: 0.807095
MAE train: 0.344126	val: 0.648504	test: 0.651980

Epoch: 72
Loss: 0.27300700758184704
RMSE train: 0.463707	val: 0.876772	test: 0.844079
MAE train: 0.370409	val: 0.687959	test: 0.673002

Epoch: 73
Loss: 0.2805269977876118
RMSE train: 0.415755	val: 0.850388	test: 0.834668
MAE train: 0.328401	val: 0.669239	test: 0.662095

Epoch: 74
Loss: 0.28503348252602984
RMSE train: 0.461924	val: 0.886566	test: 0.843565
MAE train: 0.365272	val: 0.698996	test: 0.671045

Epoch: 75
Loss: 0.2682048412305968
RMSE train: 0.442951	val: 0.888585	test: 0.869914
MAE train: 0.354537	val: 0.702327	test: 0.683144

Epoch: 76
Loss: 0.26647061641727177
RMSE train: 0.426462	val: 0.868862	test: 0.837058
MAE train: 0.339705	val: 0.681205	test: 0.663232

Epoch: 77
Loss: 0.28398420342377256
RMSE train: 0.503410	val: 0.951657	test: 0.911439
MAE train: 0.407158	val: 0.755359	test: 0.722128

Epoch: 78
Loss: 0.2626525493604796
RMSE train: 0.453907	val: 0.855300	test: 0.829381
MAE train: 0.359616	val: 0.670910	test: 0.666709

Epoch: 79
Loss: 0.25137463212013245
RMSE train: 0.488960	val: 0.887442	test: 0.862432
MAE train: 0.389993	val: 0.701408	test: 0.681322

Epoch: 80
Loss: 0.24351405565227782
RMSE train: 0.419681	val: 0.840040	test: 0.816373
MAE train: 0.334456	val: 0.657508	test: 0.646629

Epoch: 81
Loss: 0.24949488788843155
RMSE train: 0.447460	val: 0.844180	test: 0.829065
MAE train: 0.359256	val: 0.663061	test: 0.666252

Epoch: 82
Loss: 0.2517343780824116
RMSE train: 0.418249	val: 0.866788	test: 0.830195

Epoch: 22
Loss: 0.7428297357899802
RMSE train: 0.767319	val: 1.039286	test: 0.971447
MAE train: 0.612594	val: 0.812798	test: 0.766388

Epoch: 23
Loss: 0.7338594113077436
RMSE train: 0.758668	val: 0.989302	test: 0.927463
MAE train: 0.609102	val: 0.777065	test: 0.733576

Epoch: 24
Loss: 0.6937768672193799
RMSE train: 0.748490	val: 1.063564	test: 0.984183
MAE train: 0.597771	val: 0.827925	test: 0.780621

Epoch: 25
Loss: 0.6829555077212197
RMSE train: 0.730094	val: 1.061809	test: 0.978271
MAE train: 0.586541	val: 0.828155	test: 0.774462

Epoch: 26
Loss: 0.7085515260696411
RMSE train: 0.724171	val: 1.015604	test: 0.950520
MAE train: 0.581789	val: 0.791577	test: 0.759476

Epoch: 27
Loss: 0.6771126346928733
RMSE train: 0.718947	val: 0.962687	test: 0.905322
MAE train: 0.577354	val: 0.764217	test: 0.715821

Epoch: 28
Loss: 0.6467748688799995
RMSE train: 0.730837	val: 1.041704	test: 0.950989
MAE train: 0.582948	val: 0.815637	test: 0.753365

Epoch: 29
Loss: 0.6368106986795153
RMSE train: 0.698746	val: 1.059594	test: 0.980788
MAE train: 0.557484	val: 0.825578	test: 0.775525

Epoch: 30
Loss: 0.6327772928135735
RMSE train: 0.712922	val: 1.122999	test: 1.031792
MAE train: 0.566264	val: 0.883366	test: 0.813274

Epoch: 31
Loss: 0.5946357590811593
RMSE train: 0.665978	val: 1.007999	test: 0.930895
MAE train: 0.532868	val: 0.791414	test: 0.735063

Epoch: 32
Loss: 0.5704117992094585
RMSE train: 0.685844	val: 1.057858	test: 0.980055
MAE train: 0.543194	val: 0.821603	test: 0.770948

Epoch: 33
Loss: 0.5956535360642842
RMSE train: 0.670526	val: 1.023882	test: 0.953716
MAE train: 0.533851	val: 0.800787	test: 0.758035

Epoch: 34
Loss: 0.5693950993674142
RMSE train: 0.652237	val: 0.991426	test: 0.913177
MAE train: 0.523977	val: 0.783550	test: 0.723145

Epoch: 35
Loss: 0.5491019274507251
RMSE train: 0.635862	val: 1.026235	test: 0.944136
MAE train: 0.508094	val: 0.807421	test: 0.742741

Epoch: 36
Loss: 0.529906126005309
RMSE train: 0.616733	val: 0.995639	test: 0.930229
MAE train: 0.492942	val: 0.783499	test: 0.738129

Epoch: 37
Loss: 0.5356588448796954
RMSE train: 0.647246	val: 1.029696	test: 0.959822
MAE train: 0.519049	val: 0.813430	test: 0.756336

Epoch: 38
Loss: 0.5231893126453672
RMSE train: 0.625326	val: 1.008725	test: 0.946516
MAE train: 0.497268	val: 0.789868	test: 0.754564

Epoch: 39
Loss: 0.523909907255854
RMSE train: 0.614555	val: 1.002807	test: 0.937086
MAE train: 0.492946	val: 0.789413	test: 0.739073

Epoch: 40
Loss: 0.4988140506403787
RMSE train: 0.607486	val: 1.041723	test: 0.972887
MAE train: 0.483930	val: 0.815829	test: 0.761761

Epoch: 41
Loss: 0.49961898156574797
RMSE train: 0.581559	val: 0.977533	test: 0.929047
MAE train: 0.462768	val: 0.768673	test: 0.737269

Epoch: 42
Loss: 0.47410707601479124
RMSE train: 0.589423	val: 0.965327	test: 0.912787
MAE train: 0.472756	val: 0.756660	test: 0.722437

Epoch: 43
Loss: 0.44703940195696695
RMSE train: 0.581426	val: 1.015352	test: 0.944270
MAE train: 0.465125	val: 0.797225	test: 0.744169

Epoch: 44
Loss: 0.47509206192834036
RMSE train: 0.564255	val: 1.027099	test: 0.954091
MAE train: 0.448527	val: 0.800787	test: 0.755439

Epoch: 45
Loss: 0.4387422651052475
RMSE train: 0.579734	val: 1.006814	test: 0.940081
MAE train: 0.464990	val: 0.789104	test: 0.744047

Epoch: 46
Loss: 0.46633833221026827
RMSE train: 0.581961	val: 1.001199	test: 0.932759
MAE train: 0.464007	val: 0.785923	test: 0.732868

Epoch: 47
Loss: 0.43703881331852507
RMSE train: 0.541017	val: 1.010527	test: 0.943559
MAE train: 0.432211	val: 0.797050	test: 0.735870

Epoch: 48
Loss: 0.4278044764484678
RMSE train: 0.530781	val: 1.015913	test: 0.955309
MAE train: 0.422338	val: 0.793107	test: 0.757433

Epoch: 49
Loss: 0.4156018091099603
RMSE train: 0.542286	val: 1.064228	test: 0.997527
MAE train: 0.434318	val: 0.830967	test: 0.787278

Epoch: 50
Loss: 0.3833341428211757
RMSE train: 0.540979	val: 1.097219	test: 1.036388
MAE train: 0.431237	val: 0.855005	test: 0.819666

Epoch: 51
Loss: 0.3870637203965868
RMSE train: 0.531518	val: 1.002406	test: 0.957378
MAE train: 0.424385	val: 0.788280	test: 0.736263

Epoch: 52
Loss: 0.41276911752564566
RMSE train: 0.533469	val: 1.009492	test: 0.948829
MAE train: 0.424082	val: 0.785844	test: 0.741580

Epoch: 53
Loss: 0.384077787399292
RMSE train: 0.510071	val: 1.025285	test: 0.970692
MAE train: 0.406671	val: 0.799226	test: 0.773286

Epoch: 54
Loss: 0.3711456188133785
RMSE train: 0.540639	val: 1.018629	test: 0.965329
MAE train: 0.428456	val: 0.788204	test: 0.765463

Epoch: 55
Loss: 0.39735583535262514
RMSE train: 0.602166	val: 1.095908	test: 1.018800
MAE train: 0.483034	val: 0.859472	test: 0.803618

Epoch: 56
Loss: 0.35029945203236174
RMSE train: 0.516392	val: 1.036606	test: 0.981479
MAE train: 0.413793	val: 0.808806	test: 0.770104

Epoch: 57
Loss: 0.37539148330688477
RMSE train: 0.527589	val: 1.009378	test: 0.943658
MAE train: 0.422645	val: 0.784440	test: 0.745745

Epoch: 58
Loss: 0.35650824436119627
RMSE train: 0.460215	val: 0.999011	test: 0.953884
MAE train: 0.364952	val: 0.769684	test: 0.760109

Epoch: 59
Loss: 0.3344347466315542
RMSE train: 0.495919	val: 1.055031	test: 0.986512
MAE train: 0.397716	val: 0.824509	test: 0.776605

Epoch: 60
Loss: 0.3280221628291266
RMSE train: 0.488821	val: 1.061346	test: 1.002813
MAE train: 0.391746	val: 0.824553	test: 0.793092

Epoch: 61
Loss: 0.3154136666229793
RMSE train: 0.458052	val: 1.041200	test: 0.979592
MAE train: 0.364219	val: 0.803010	test: 0.774107

Epoch: 62
Loss: 0.3239583522081375
RMSE train: 0.532515	val: 1.049580	test: 0.982157
MAE train: 0.426911	val: 0.815738	test: 0.775246

Epoch: 63
Loss: 0.33384446799755096
RMSE train: 0.541421	val: 1.022772	test: 0.963353
MAE train: 0.430439	val: 0.796509	test: 0.760580

Epoch: 64
Loss: 0.3243063347680228
RMSE train: 0.448557	val: 0.974941	test: 0.912369
MAE train: 0.354422	val: 0.759319	test: 0.720513

Epoch: 65
Loss: 0.3201622601066317
RMSE train: 0.452377	val: 1.026364	test: 0.959097
MAE train: 0.359975	val: 0.793784	test: 0.765912

Epoch: 66
Loss: 0.3387260266712734
RMSE train: 0.435558	val: 1.011046	test: 0.944222
MAE train: 0.345868	val: 0.786554	test: 0.744146

Epoch: 67
Loss: 0.3068320634109633
RMSE train: 0.436108	val: 0.951877	test: 0.900499
MAE train: 0.345356	val: 0.746655	test: 0.712847

Epoch: 68
Loss: 0.2713137428675379
RMSE train: 0.451145	val: 1.026597	test: 0.962946
MAE train: 0.361033	val: 0.791634	test: 0.760951

Epoch: 69
Loss: 0.30054292934281485
RMSE train: 0.486623	val: 1.049023	test: 0.965716
MAE train: 0.390341	val: 0.814606	test: 0.761862

Epoch: 70
Loss: 0.27831012649195536
RMSE train: 0.408760	val: 1.022925	test: 0.957817
MAE train: 0.325948	val: 0.792910	test: 0.748104

Epoch: 71
Loss: 0.259156023817403
RMSE train: 0.436667	val: 1.014390	test: 0.945355
MAE train: 0.349746	val: 0.790489	test: 0.747322

Epoch: 72
Loss: 0.27997499065739767
RMSE train: 0.449614	val: 1.081957	test: 1.012313
MAE train: 0.359820	val: 0.844774	test: 0.804339

Epoch: 73
Loss: 0.2696680415953909
RMSE train: 0.409602	val: 1.039654	test: 0.971440
MAE train: 0.325964	val: 0.808004	test: 0.770081

Epoch: 74
Loss: 0.2731223851442337
RMSE train: 0.472731	val: 0.996332	test: 0.923833
MAE train: 0.376795	val: 0.784763	test: 0.722419

Epoch: 75
Loss: 0.2903156110218593
RMSE train: 0.463612	val: 1.077010	test: 1.008471
MAE train: 0.372963	val: 0.832022	test: 0.796659

Epoch: 76
Loss: 0.2613224131720407
RMSE train: 0.425519	val: 1.034721	test: 0.979803
MAE train: 0.337045	val: 0.803427	test: 0.780454

Epoch: 77
Loss: 0.26811183989048004
RMSE train: 0.437616	val: 1.041456	test: 0.972860
MAE train: 0.349556	val: 0.801653	test: 0.768586

Epoch: 78
Loss: 0.26106647295611246
RMSE train: 0.432285	val: 1.014217	test: 0.953605
MAE train: 0.347131	val: 0.790438	test: 0.754036

Epoch: 79
Loss: 0.25109618263585226
RMSE train: 0.390524	val: 1.025906	test: 0.953524
MAE train: 0.308962	val: 0.792722	test: 0.765194

Epoch: 80
Loss: 0.2491484433412552
RMSE train: 0.418092	val: 1.007144	test: 0.955352
MAE train: 0.329168	val: 0.785595	test: 0.761129

Epoch: 81
Loss: 0.2440249611224447
RMSE train: 0.392417	val: 1.001430	test: 0.931121
MAE train: 0.312206	val: 0.778911	test: 0.737560

Epoch: 82
Loss: 0.22905871378523962
RMSE train: 0.387865	val: 1.031732	test: 0.968363

Epoch: 22
Loss: 0.6945799844605582
RMSE train: 0.734554	val: 0.965463	test: 0.866285
MAE train: 0.582545	val: 0.757492	test: 0.704573

Epoch: 23
Loss: 0.6772608629294804
RMSE train: 0.741858	val: 0.938914	test: 0.860636
MAE train: 0.586856	val: 0.752254	test: 0.702358

Epoch: 24
Loss: 0.6285352962357658
RMSE train: 0.728294	val: 0.966010	test: 0.869281
MAE train: 0.575578	val: 0.758372	test: 0.704720

Epoch: 25
Loss: 0.6039324126073292
RMSE train: 0.706266	val: 0.955758	test: 0.860899
MAE train: 0.554136	val: 0.755992	test: 0.698543

Epoch: 26
Loss: 0.6346309014729091
RMSE train: 0.697814	val: 0.907449	test: 0.839500
MAE train: 0.554373	val: 0.711811	test: 0.682273

Epoch: 27
Loss: 0.6169608959129879
RMSE train: 0.723643	val: 0.918995	test: 0.840435
MAE train: 0.570582	val: 0.730053	test: 0.672928

Epoch: 28
Loss: 0.6161618913922992
RMSE train: 0.678887	val: 0.944062	test: 0.864505
MAE train: 0.534031	val: 0.742033	test: 0.700113

Epoch: 29
Loss: 0.5599798049245562
RMSE train: 0.674074	val: 0.922602	test: 0.833175
MAE train: 0.531078	val: 0.724482	test: 0.676508

Epoch: 30
Loss: 0.5826406968491418
RMSE train: 0.680181	val: 0.928899	test: 0.836791
MAE train: 0.535626	val: 0.731993	test: 0.681603

Epoch: 31
Loss: 0.5627618900367192
RMSE train: 0.695108	val: 0.976519	test: 0.886636
MAE train: 0.545807	val: 0.770019	test: 0.726821

Epoch: 32
Loss: 0.5554445556231907
RMSE train: 0.673603	val: 0.941346	test: 0.855191
MAE train: 0.529247	val: 0.746422	test: 0.684531

Epoch: 33
Loss: 0.5508422191653933
RMSE train: 0.659448	val: 0.919327	test: 0.840444
MAE train: 0.519305	val: 0.721498	test: 0.684642

Epoch: 34
Loss: 0.526798991220338
RMSE train: 0.634299	val: 0.910891	test: 0.835132
MAE train: 0.500401	val: 0.718410	test: 0.682795

Epoch: 35
Loss: 0.49692135623523165
RMSE train: 0.626026	val: 0.897665	test: 0.816051
MAE train: 0.492028	val: 0.699305	test: 0.655376

Epoch: 36
Loss: 0.5066559165716171
RMSE train: 0.657262	val: 0.952208	test: 0.860219
MAE train: 0.516922	val: 0.749880	test: 0.698214

Epoch: 37
Loss: 0.48818477988243103
RMSE train: 0.654778	val: 0.930235	test: 0.856708
MAE train: 0.512270	val: 0.736681	test: 0.694215

Epoch: 38
Loss: 0.5240235903433391
RMSE train: 0.647847	val: 0.943370	test: 0.871665
MAE train: 0.510619	val: 0.739142	test: 0.709423

Epoch: 39
Loss: 0.5010651328733989
RMSE train: 0.631563	val: 0.947367	test: 0.863703
MAE train: 0.495703	val: 0.741368	test: 0.688315

Epoch: 40
Loss: 0.49617244941847666
RMSE train: 0.596675	val: 0.889467	test: 0.826311
MAE train: 0.471299	val: 0.690750	test: 0.660122

Epoch: 41
Loss: 0.4729128863130297
RMSE train: 0.617445	val: 0.893411	test: 0.826626
MAE train: 0.483772	val: 0.702831	test: 0.666972

Epoch: 42
Loss: 0.44777371627943857
RMSE train: 0.629569	val: 0.931728	test: 0.857369
MAE train: 0.496591	val: 0.732256	test: 0.692850

Epoch: 43
Loss: 0.4667819333927972
RMSE train: 0.563408	val: 0.897086	test: 0.839045
MAE train: 0.443725	val: 0.700768	test: 0.677593

Epoch: 44
Loss: 0.4347813533885138
RMSE train: 0.598975	val: 0.895375	test: 0.838459
MAE train: 0.471861	val: 0.706850	test: 0.676458

Epoch: 45
Loss: 0.43026889647756306
RMSE train: 0.629796	val: 0.966429	test: 0.881189
MAE train: 0.498485	val: 0.744733	test: 0.710843

Epoch: 46
Loss: 0.42882714314120157
RMSE train: 0.599434	val: 0.916640	test: 0.850747
MAE train: 0.468656	val: 0.714056	test: 0.687591

Epoch: 47
Loss: 0.44903180854661123
RMSE train: 0.585419	val: 0.872874	test: 0.834046
MAE train: 0.466540	val: 0.687535	test: 0.684379

Epoch: 48
Loss: 0.41352093645504545
RMSE train: 0.548545	val: 0.875655	test: 0.820596
MAE train: 0.431496	val: 0.684035	test: 0.661714

Epoch: 49
Loss: 0.4048860903297152
RMSE train: 0.563311	val: 0.861206	test: 0.828459
MAE train: 0.448690	val: 0.674832	test: 0.657564

Epoch: 50
Loss: 0.3962779236691339
RMSE train: 0.561957	val: 0.886972	test: 0.829419
MAE train: 0.440870	val: 0.699267	test: 0.669809

Epoch: 51
Loss: 0.3847478841032301
RMSE train: 0.552097	val: 0.896212	test: 0.850130
MAE train: 0.436439	val: 0.707079	test: 0.691192

Epoch: 52
Loss: 0.3774271479674748
RMSE train: 0.538183	val: 0.872370	test: 0.823055
MAE train: 0.425302	val: 0.681542	test: 0.666694

Epoch: 53
Loss: 0.39635605045727323
RMSE train: 0.517469	val: 0.861385	test: 0.816977
MAE train: 0.407711	val: 0.684594	test: 0.654543

Epoch: 54
Loss: 0.39302835294178556
RMSE train: 0.556309	val: 0.866647	test: 0.823246
MAE train: 0.442911	val: 0.683786	test: 0.669471

Epoch: 55
Loss: 0.37730886893612997
RMSE train: 0.559595	val: 0.874250	test: 0.839883
MAE train: 0.442923	val: 0.687272	test: 0.680815

Epoch: 56
Loss: 0.3766193091869354
RMSE train: 0.568655	val: 0.916293	test: 0.846466
MAE train: 0.446724	val: 0.728555	test: 0.684978

Epoch: 57
Loss: 0.4308927719082151
RMSE train: 0.518806	val: 0.865703	test: 0.808290
MAE train: 0.406851	val: 0.680923	test: 0.645994

Epoch: 58
Loss: 0.37153082873140064
RMSE train: 0.503201	val: 0.856885	test: 0.819927
MAE train: 0.397429	val: 0.680123	test: 0.656806

Epoch: 59
Loss: 0.33829911053180695
RMSE train: 0.551593	val: 0.929612	test: 0.868201
MAE train: 0.436014	val: 0.732940	test: 0.700308

Epoch: 60
Loss: 0.33751876865114483
RMSE train: 0.493553	val: 0.872978	test: 0.826489
MAE train: 0.388835	val: 0.684913	test: 0.666365

Epoch: 61
Loss: 0.31969451691423145
RMSE train: 0.524875	val: 0.904353	test: 0.845293
MAE train: 0.413141	val: 0.713179	test: 0.693910

Epoch: 62
Loss: 0.3142566829919815
RMSE train: 0.476951	val: 0.864338	test: 0.821687
MAE train: 0.376719	val: 0.678215	test: 0.664376

Epoch: 63
Loss: 0.31199532960142407
RMSE train: 0.466477	val: 0.866514	test: 0.825069
MAE train: 0.366085	val: 0.681042	test: 0.668929

Epoch: 64
Loss: 0.3435952897582735
RMSE train: 0.468375	val: 0.866108	test: 0.827028
MAE train: 0.369294	val: 0.675361	test: 0.666449

Epoch: 65
Loss: 0.30635350942611694
RMSE train: 0.466134	val: 0.872127	test: 0.824465
MAE train: 0.366698	val: 0.683208	test: 0.667876

Epoch: 66
Loss: 0.3143251176391329
RMSE train: 0.470905	val: 0.873819	test: 0.818376
MAE train: 0.373322	val: 0.687924	test: 0.666713

Epoch: 67
Loss: 0.3010766272033964
RMSE train: 0.445822	val: 0.866726	test: 0.819316
MAE train: 0.348966	val: 0.676641	test: 0.653852

Epoch: 68
Loss: 0.29496112146547865
RMSE train: 0.510008	val: 0.928544	test: 0.863648
MAE train: 0.404657	val: 0.722193	test: 0.702344

Epoch: 69
Loss: 0.3009596850190844
RMSE train: 0.470755	val: 0.882951	test: 0.834505
MAE train: 0.372494	val: 0.691980	test: 0.676800

Epoch: 70
Loss: 0.2981932099376406
RMSE train: 0.444065	val: 0.857549	test: 0.820769
MAE train: 0.348099	val: 0.675068	test: 0.666503

Epoch: 71
Loss: 0.2907008230686188
RMSE train: 0.464169	val: 0.857304	test: 0.814995
MAE train: 0.369247	val: 0.673723	test: 0.665825

Epoch: 72
Loss: 0.28064359937395367
RMSE train: 0.429329	val: 0.866259	test: 0.816025
MAE train: 0.338712	val: 0.683348	test: 0.659177

Epoch: 73
Loss: 0.2955966836639813
RMSE train: 0.429226	val: 0.852751	test: 0.820739
MAE train: 0.336707	val: 0.664261	test: 0.660833

Epoch: 74
Loss: 0.269545605140073
RMSE train: 0.445965	val: 0.888245	test: 0.826199
MAE train: 0.351789	val: 0.693900	test: 0.665923

Epoch: 75
Loss: 0.28560884296894073
RMSE train: 0.439874	val: 0.868118	test: 0.825857
MAE train: 0.347948	val: 0.676774	test: 0.678239

Epoch: 76
Loss: 0.27846688883645193
RMSE train: 0.485400	val: 0.913183	test: 0.857921
MAE train: 0.384130	val: 0.710007	test: 0.696472

Epoch: 77
Loss: 0.2752733241234507
RMSE train: 0.410608	val: 0.855840	test: 0.825180
MAE train: 0.323654	val: 0.661767	test: 0.666435

Epoch: 78
Loss: 0.2691475365843092
RMSE train: 0.437009	val: 0.851405	test: 0.820645
MAE train: 0.345324	val: 0.666383	test: 0.663675

Epoch: 79
Loss: 0.24795701461178915
RMSE train: 0.399686	val: 0.855561	test: 0.822456
MAE train: 0.313835	val: 0.662493	test: 0.665184

Epoch: 80
Loss: 0.24839080231530325
RMSE train: 0.437221	val: 0.868377	test: 0.821398
MAE train: 0.343944	val: 0.679031	test: 0.659748

Epoch: 81
Loss: 0.24383731292826788
RMSE train: 0.427767	val: 0.882203	test: 0.824206
MAE train: 0.337626	val: 0.690508	test: 0.667505

Epoch: 82
Loss: 0.2616424486041069
RMSE train: 0.398953	val: 0.859891	test: 0.821261

Epoch: 23
Loss: 0.5519409307411739
RMSE train: 0.689502	val: 0.841875	test: 0.819854
MAE train: 0.542540	val: 0.656073	test: 0.660276

Epoch: 24
Loss: 0.5450960866042546
RMSE train: 0.670299	val: 0.851395	test: 0.808007
MAE train: 0.527114	val: 0.665576	test: 0.653330

Epoch: 25
Loss: 0.5450544910771506
RMSE train: 0.670859	val: 0.849742	test: 0.806409
MAE train: 0.520832	val: 0.661337	test: 0.654748

Epoch: 26
Loss: 0.5155579021998814
RMSE train: 0.660313	val: 0.828339	test: 0.805857
MAE train: 0.519880	val: 0.644514	test: 0.650655

Epoch: 27
Loss: 0.5347496058259692
RMSE train: 0.674844	val: 0.855335	test: 0.802721
MAE train: 0.524456	val: 0.661910	test: 0.649916

Epoch: 28
Loss: 0.5247912385634014
RMSE train: 0.653556	val: 0.850655	test: 0.793346
MAE train: 0.510768	val: 0.660131	test: 0.640797

Epoch: 29
Loss: 0.5074797400406429
RMSE train: 0.644687	val: 0.819487	test: 0.792770
MAE train: 0.504117	val: 0.634342	test: 0.639446

Epoch: 30
Loss: 0.49325796961784363
RMSE train: 0.643631	val: 0.824035	test: 0.795095
MAE train: 0.502913	val: 0.637675	test: 0.639132

Epoch: 31
Loss: 0.5019034679446902
RMSE train: 0.653967	val: 0.841398	test: 0.801128
MAE train: 0.510042	val: 0.648269	test: 0.650242

Epoch: 32
Loss: 0.48825994772570475
RMSE train: 0.647969	val: 0.855686	test: 0.810114
MAE train: 0.501677	val: 0.664158	test: 0.656154

Epoch: 33
Loss: 0.5025627208607537
RMSE train: 0.653967	val: 0.862506	test: 0.811105
MAE train: 0.506597	val: 0.663825	test: 0.657683

Epoch: 34
Loss: 0.5048936882189342
RMSE train: 0.652087	val: 0.847513	test: 0.814048
MAE train: 0.506062	val: 0.650226	test: 0.658025

Epoch: 35
Loss: 0.5019500106573105
RMSE train: 0.622230	val: 0.824428	test: 0.793391
MAE train: 0.488766	val: 0.630834	test: 0.636707

Epoch: 36
Loss: 0.45563257379191263
RMSE train: 0.629598	val: 0.839259	test: 0.788595
MAE train: 0.488003	val: 0.641646	test: 0.635511

Epoch: 37
Loss: 0.46347567439079285
RMSE train: 0.626656	val: 0.839685	test: 0.794722
MAE train: 0.487128	val: 0.640635	test: 0.638201

Epoch: 38
Loss: 0.473718666604587
RMSE train: 0.630572	val: 0.844243	test: 0.809309
MAE train: 0.490588	val: 0.644371	test: 0.659901

Epoch: 39
Loss: 0.4744430035352707
RMSE train: 0.616457	val: 0.828442	test: 0.796689
MAE train: 0.478240	val: 0.635766	test: 0.646040

Epoch: 40
Loss: 0.4415868031127112
RMSE train: 0.644518	val: 0.840290	test: 0.794792
MAE train: 0.501838	val: 0.643354	test: 0.642534

Epoch: 41
Loss: 0.45210585423878263
RMSE train: 0.604292	val: 0.825662	test: 0.785498
MAE train: 0.465986	val: 0.636300	test: 0.620208

Epoch: 42
Loss: 0.45862859913281034
RMSE train: 0.618602	val: 0.822826	test: 0.793925
MAE train: 0.482275	val: 0.632959	test: 0.639004

Epoch: 43
Loss: 0.4333616409982954
RMSE train: 0.595779	val: 0.822137	test: 0.785219
MAE train: 0.460621	val: 0.631244	test: 0.631746

Epoch: 44
Loss: 0.4297708294221333
RMSE train: 0.603663	val: 0.820315	test: 0.782802
MAE train: 0.474021	val: 0.627945	test: 0.619610

Epoch: 45
Loss: 0.42888060212135315
RMSE train: 0.599823	val: 0.820022	test: 0.785291
MAE train: 0.466163	val: 0.627956	test: 0.639876

Epoch: 46
Loss: 0.4177246519497463
RMSE train: 0.592148	val: 0.820436	test: 0.784099
MAE train: 0.456236	val: 0.628702	test: 0.634908

Epoch: 47
Loss: 0.4259874863283975
RMSE train: 0.600307	val: 0.797322	test: 0.780237
MAE train: 0.473315	val: 0.614598	test: 0.625696

Epoch: 48
Loss: 0.4377755182129996
RMSE train: 0.582999	val: 0.806408	test: 0.773273
MAE train: 0.455267	val: 0.617422	test: 0.617352

Epoch: 49
Loss: 0.4292977069105421
RMSE train: 0.632645	val: 0.838003	test: 0.806539
MAE train: 0.495530	val: 0.647630	test: 0.644335

Epoch: 50
Loss: 0.4303426061357771
RMSE train: 0.595341	val: 0.815542	test: 0.783398
MAE train: 0.464543	val: 0.626344	test: 0.619654

Epoch: 51
Loss: 0.4054444261959621
RMSE train: 0.611392	val: 0.810544	test: 0.789177
MAE train: 0.479725	val: 0.617803	test: 0.636537

Epoch: 52
Loss: 0.40341546492917196
RMSE train: 0.607314	val: 0.843558	test: 0.792119
MAE train: 0.471185	val: 0.644780	test: 0.634582

Epoch: 53
Loss: 0.399775287934712
RMSE train: 0.591533	val: 0.821338	test: 0.784407
MAE train: 0.458291	val: 0.630884	test: 0.631910

Epoch: 54
Loss: 0.39026886224746704
RMSE train: 0.560860	val: 0.777499	test: 0.759474
MAE train: 0.436874	val: 0.590076	test: 0.603526

Epoch: 55
Loss: 0.4049201969589506
RMSE train: 0.587982	val: 0.812730	test: 0.783801
MAE train: 0.457127	val: 0.617476	test: 0.632920

Epoch: 56
Loss: 0.3868518876177924
RMSE train: 0.582601	val: 0.800829	test: 0.783046
MAE train: 0.458059	val: 0.609931	test: 0.616530

Epoch: 57
Loss: 0.38547338971069883
RMSE train: 0.568935	val: 0.796760	test: 0.765407
MAE train: 0.442909	val: 0.597421	test: 0.603363

Epoch: 58
Loss: 0.3747305806194033
RMSE train: 0.575151	val: 0.820794	test: 0.773447
MAE train: 0.447612	val: 0.624385	test: 0.619837

Epoch: 59
Loss: 0.3656154934849058
RMSE train: 0.578171	val: 0.786477	test: 0.765128
MAE train: 0.454294	val: 0.605602	test: 0.611911

Epoch: 60
Loss: 0.38590396514960695
RMSE train: 0.553624	val: 0.783694	test: 0.761384
MAE train: 0.430009	val: 0.594416	test: 0.605267

Epoch: 61
Loss: 0.38454404899052214
RMSE train: 0.582620	val: 0.791590	test: 0.766594
MAE train: 0.456146	val: 0.602055	test: 0.615968

Epoch: 62
Loss: 0.37493168456213816
RMSE train: 0.559957	val: 0.784466	test: 0.757132
MAE train: 0.433697	val: 0.594872	test: 0.609174

Epoch: 63
Loss: 0.39964776805468966
RMSE train: 0.577505	val: 0.779741	test: 0.762145
MAE train: 0.450188	val: 0.592676	test: 0.610315

Epoch: 64
Loss: 0.3769637261118208
RMSE train: 0.579700	val: 0.794391	test: 0.765719
MAE train: 0.453696	val: 0.599376	test: 0.612707

Epoch: 65
Loss: 0.352290724005018
RMSE train: 0.572076	val: 0.807195	test: 0.765274
MAE train: 0.443683	val: 0.615611	test: 0.603715

Epoch: 66
Loss: 0.38595848211220335
RMSE train: 0.529742	val: 0.773929	test: 0.751528
MAE train: 0.410753	val: 0.581816	test: 0.589120

Epoch: 67
Loss: 0.3695638711963381
RMSE train: 0.564093	val: 0.791535	test: 0.768370
MAE train: 0.441182	val: 0.604271	test: 0.614255

Epoch: 68
Loss: 0.36301409772464205
RMSE train: 0.540770	val: 0.792358	test: 0.759605
MAE train: 0.420150	val: 0.597234	test: 0.601300

Epoch: 69
Loss: 0.390200817159244
RMSE train: 0.540536	val: 0.780344	test: 0.757000
MAE train: 0.420267	val: 0.589811	test: 0.602798

Epoch: 70
Loss: 0.38845456498009817
RMSE train: 0.541998	val: 0.774258	test: 0.760641
MAE train: 0.421190	val: 0.591071	test: 0.604137

Epoch: 71
Loss: 0.37359206804207395
RMSE train: 0.562331	val: 0.816044	test: 0.781219
MAE train: 0.433230	val: 0.622774	test: 0.617666

Epoch: 72
Loss: 0.34250966140202116
RMSE train: 0.568527	val: 0.802770	test: 0.771231
MAE train: 0.443133	val: 0.610101	test: 0.614567

Epoch: 73
Loss: 0.34805634192058016
RMSE train: 0.516221	val: 0.770346	test: 0.754881
MAE train: 0.398490	val: 0.584867	test: 0.594446

Epoch: 74
Loss: 0.35848083240645273
RMSE train: 0.538174	val: 0.767107	test: 0.756949
MAE train: 0.418511	val: 0.581449	test: 0.602668

Epoch: 75
Loss: 0.34818697401455473
RMSE train: 0.534515	val: 0.774030	test: 0.749689
MAE train: 0.416034	val: 0.584806	test: 0.591355

Epoch: 76
Loss: 0.3537468356745584
RMSE train: 0.547225	val: 0.775760	test: 0.761394
MAE train: 0.424008	val: 0.588923	test: 0.606515

Epoch: 77
Loss: 0.34761160824980053
RMSE train: 0.540304	val: 0.787129	test: 0.754500
MAE train: 0.417185	val: 0.600741	test: 0.593691

Epoch: 78
Loss: 0.3276994526386261
RMSE train: 0.539055	val: 0.788869	test: 0.767322
MAE train: 0.417619	val: 0.596279	test: 0.609243

Epoch: 79
Loss: 0.36519147881439756
RMSE train: 0.549573	val: 0.833913	test: 0.790107
MAE train: 0.427293	val: 0.633029	test: 0.626206

Epoch: 80
Loss: 0.3278094700404576
RMSE train: 0.513998	val: 0.764859	test: 0.750962
MAE train: 0.397311	val: 0.582140	test: 0.584215

Epoch: 81
Loss: 0.3309985505683081
RMSE train: 0.530774	val: 0.785428	test: 0.757225
MAE train: 0.412006	val: 0.593830	test: 0.601561

Epoch: 82
Loss: 0.3188785655157907
RMSE train: 0.549008	val: 0.822502	test: 0.775567
MAE train: 0.427391	val: 0.625352	test: 0.608641

Epoch: 83
Loss: 0.3301073546920504
RMSE train: 0.531167	val: 0.803084	test: 0.756985

Epoch: 23
Loss: 0.5492024293967656
RMSE train: 0.680813	val: 0.842374	test: 0.784334
MAE train: 0.529457	val: 0.658309	test: 0.633437

Epoch: 24
Loss: 0.5288054708923612
RMSE train: 0.712051	val: 0.873768	test: 0.804140
MAE train: 0.549524	val: 0.685782	test: 0.644643

Epoch: 25
Loss: 0.5571003194366183
RMSE train: 0.711192	val: 0.891279	test: 0.819730
MAE train: 0.554480	val: 0.696721	test: 0.666751

Epoch: 26
Loss: 0.5248335536037173
RMSE train: 0.672928	val: 0.854223	test: 0.783420
MAE train: 0.522534	val: 0.658715	test: 0.633336

Epoch: 27
Loss: 0.5115578387464795
RMSE train: 0.694914	val: 0.865882	test: 0.800673
MAE train: 0.539616	val: 0.671710	test: 0.651528

Epoch: 28
Loss: 0.5069467127323151
RMSE train: 0.685400	val: 0.875209	test: 0.810639
MAE train: 0.534329	val: 0.674770	test: 0.658130

Epoch: 29
Loss: 0.5182609047208514
RMSE train: 0.659758	val: 0.837546	test: 0.772225
MAE train: 0.509837	val: 0.644254	test: 0.620350

Epoch: 30
Loss: 0.5295751563140324
RMSE train: 0.643254	val: 0.833123	test: 0.776929
MAE train: 0.497442	val: 0.643549	test: 0.630386

Epoch: 31
Loss: 0.5260480897767204
RMSE train: 0.677113	val: 0.852561	test: 0.791846
MAE train: 0.526704	val: 0.656417	test: 0.640201

Epoch: 32
Loss: 0.48180840057986124
RMSE train: 0.657659	val: 0.849631	test: 0.782732
MAE train: 0.507805	val: 0.658615	test: 0.634228

Epoch: 33
Loss: 0.5150410234928131
RMSE train: 0.642395	val: 0.829698	test: 0.770354
MAE train: 0.499620	val: 0.635734	test: 0.626820

Epoch: 34
Loss: 0.49187313445976805
RMSE train: 0.656360	val: 0.862843	test: 0.772214
MAE train: 0.508825	val: 0.663921	test: 0.616443

Epoch: 35
Loss: 0.47257573902606964
RMSE train: 0.641789	val: 0.845186	test: 0.768422
MAE train: 0.500162	val: 0.640464	test: 0.615989

Epoch: 36
Loss: 0.46534526135240284
RMSE train: 0.635610	val: 0.840344	test: 0.764917
MAE train: 0.492225	val: 0.634906	test: 0.607814

Epoch: 37
Loss: 0.4864369354077748
RMSE train: 0.624782	val: 0.831482	test: 0.762387
MAE train: 0.487140	val: 0.627945	test: 0.616527

Epoch: 38
Loss: 0.4656892269849777
RMSE train: 0.613404	val: 0.821738	test: 0.754726
MAE train: 0.479410	val: 0.623676	test: 0.605417

Epoch: 39
Loss: 0.44466271145003183
RMSE train: 0.650326	val: 0.872575	test: 0.800701
MAE train: 0.503832	val: 0.657393	test: 0.644592

Epoch: 40
Loss: 0.4621259421110153
RMSE train: 0.608773	val: 0.827983	test: 0.759465
MAE train: 0.472914	val: 0.622867	test: 0.609017

Epoch: 41
Loss: 0.4376897428716932
RMSE train: 0.620651	val: 0.822299	test: 0.763905
MAE train: 0.481974	val: 0.625317	test: 0.615266

Epoch: 42
Loss: 0.42625273977007183
RMSE train: 0.600433	val: 0.805933	test: 0.762999
MAE train: 0.465492	val: 0.615976	test: 0.612563

Epoch: 43
Loss: 0.43697700117315563
RMSE train: 0.630900	val: 0.825337	test: 0.788438
MAE train: 0.490025	val: 0.637384	test: 0.634777

Epoch: 44
Loss: 0.42192415254456656
RMSE train: 0.597911	val: 0.786581	test: 0.760784
MAE train: 0.461070	val: 0.608096	test: 0.612081

Epoch: 45
Loss: 0.4447904782635825
RMSE train: 0.619651	val: 0.800176	test: 0.772758
MAE train: 0.480627	val: 0.617777	test: 0.622228

Epoch: 46
Loss: 0.4211591737610953
RMSE train: 0.614842	val: 0.812500	test: 0.772650
MAE train: 0.476936	val: 0.625824	test: 0.619346

Epoch: 47
Loss: 0.4086392138685499
RMSE train: 0.599588	val: 0.807821	test: 0.766737
MAE train: 0.465523	val: 0.617499	test: 0.618462

Epoch: 48
Loss: 0.43827701466424124
RMSE train: 0.639366	val: 0.854300	test: 0.799221
MAE train: 0.497137	val: 0.654831	test: 0.643148

Epoch: 49
Loss: 0.40842829857553753
RMSE train: 0.616152	val: 0.844710	test: 0.783294
MAE train: 0.479410	val: 0.640557	test: 0.633173

Epoch: 50
Loss: 0.41260094940662384
RMSE train: 0.598625	val: 0.826396	test: 0.773323
MAE train: 0.466723	val: 0.634262	test: 0.610902

Epoch: 51
Loss: 0.3993337345974786
RMSE train: 0.591225	val: 0.807807	test: 0.766434
MAE train: 0.457005	val: 0.612959	test: 0.615062

Epoch: 52
Loss: 0.41258420263017925
RMSE train: 0.590872	val: 0.805047	test: 0.757222
MAE train: 0.459126	val: 0.608414	test: 0.608913

Epoch: 53
Loss: 0.3932147238935743
RMSE train: 0.581262	val: 0.801895	test: 0.745277
MAE train: 0.448124	val: 0.613123	test: 0.598426

Epoch: 54
Loss: 0.3862223433596747
RMSE train: 0.580793	val: 0.787122	test: 0.745094
MAE train: 0.450115	val: 0.598545	test: 0.599087

Epoch: 55
Loss: 0.37999857536384035
RMSE train: 0.584607	val: 0.801693	test: 0.760053
MAE train: 0.451495	val: 0.618172	test: 0.611525

Epoch: 56
Loss: 0.3928306294339044
RMSE train: 0.573822	val: 0.794077	test: 0.750895
MAE train: 0.446383	val: 0.610684	test: 0.605726

Epoch: 57
Loss: 0.3856679137263979
RMSE train: 0.602595	val: 0.816117	test: 0.772212
MAE train: 0.468218	val: 0.625964	test: 0.610714

Epoch: 58
Loss: 0.39067533186503817
RMSE train: 0.559773	val: 0.770210	test: 0.733394
MAE train: 0.433537	val: 0.589068	test: 0.586968

Epoch: 59
Loss: 0.3800052745001657
RMSE train: 0.599541	val: 0.827180	test: 0.771041
MAE train: 0.461396	val: 0.624995	test: 0.619650

Epoch: 60
Loss: 0.38735790124961306
RMSE train: 0.558961	val: 0.793044	test: 0.751700
MAE train: 0.431834	val: 0.606082	test: 0.600308

Epoch: 61
Loss: 0.3651225098541805
RMSE train: 0.561973	val: 0.796548	test: 0.757959
MAE train: 0.434865	val: 0.607515	test: 0.608659

Epoch: 62
Loss: 0.37579070031642914
RMSE train: 0.575839	val: 0.805186	test: 0.769350
MAE train: 0.448141	val: 0.617098	test: 0.622370

Epoch: 63
Loss: 0.3705275697367532
RMSE train: 0.571793	val: 0.801463	test: 0.758843
MAE train: 0.443191	val: 0.618084	test: 0.613598

Epoch: 64
Loss: 0.3666644585984094
RMSE train: 0.554454	val: 0.777112	test: 0.735306
MAE train: 0.431918	val: 0.600468	test: 0.590388

Epoch: 65
Loss: 0.3481116550309317
RMSE train: 0.550857	val: 0.781966	test: 0.738656
MAE train: 0.427950	val: 0.593851	test: 0.595804

Epoch: 66
Loss: 0.3594300491469247
RMSE train: 0.547116	val: 0.792272	test: 0.741640
MAE train: 0.425214	val: 0.604583	test: 0.598015

Epoch: 67
Loss: 0.35098910331726074
RMSE train: 0.552362	val: 0.787828	test: 0.741754
MAE train: 0.430959	val: 0.603204	test: 0.598783

Epoch: 68
Loss: 0.3637239507266453
RMSE train: 0.568077	val: 0.813927	test: 0.754997
MAE train: 0.439812	val: 0.623673	test: 0.608313

Epoch: 69
Loss: 0.35694352643830435
RMSE train: 0.548127	val: 0.783281	test: 0.738007
MAE train: 0.425106	val: 0.596375	test: 0.590148

Epoch: 70
Loss: 0.34221960391317097
RMSE train: 0.547971	val: 0.787426	test: 0.746472
MAE train: 0.423058	val: 0.599546	test: 0.587173

Epoch: 71
Loss: 0.36192889511585236
RMSE train: 0.528131	val: 0.784145	test: 0.740021
MAE train: 0.408494	val: 0.592348	test: 0.587393

Epoch: 72
Loss: 0.3437711958374296
RMSE train: 0.535612	val: 0.780008	test: 0.738604
MAE train: 0.415626	val: 0.592972	test: 0.594048

Epoch: 73
Loss: 0.34776630784784046
RMSE train: 0.542150	val: 0.798324	test: 0.758571
MAE train: 0.419553	val: 0.604976	test: 0.605688

Epoch: 74
Loss: 0.3479962625673839
RMSE train: 0.540154	val: 0.800823	test: 0.749646
MAE train: 0.419593	val: 0.600166	test: 0.595449

Epoch: 75
Loss: 0.34004213341644834
RMSE train: 0.506209	val: 0.767216	test: 0.728702
MAE train: 0.391793	val: 0.574653	test: 0.579864

Epoch: 76
Loss: 0.34825789076941355
RMSE train: 0.555508	val: 0.795872	test: 0.746767
MAE train: 0.430840	val: 0.608014	test: 0.601070

Epoch: 77
Loss: 0.3481981818165098
RMSE train: 0.560544	val: 0.793085	test: 0.755889
MAE train: 0.436454	val: 0.604639	test: 0.603039

Epoch: 78
Loss: 0.3207458427974156
RMSE train: 0.525284	val: 0.769712	test: 0.734943
MAE train: 0.408183	val: 0.581532	test: 0.580315

Epoch: 79
Loss: 0.31809426844120026
RMSE train: 0.545609	val: 0.788434	test: 0.745263
MAE train: 0.425654	val: 0.599697	test: 0.583885

Epoch: 80
Loss: 0.3293705497469221
RMSE train: 0.520609	val: 0.769016	test: 0.731393
MAE train: 0.407351	val: 0.586785	test: 0.582684

Epoch: 81
Loss: 0.3323898868901389
RMSE train: 0.544475	val: 0.805640	test: 0.748706
MAE train: 0.424276	val: 0.612047	test: 0.594486

Epoch: 82
Loss: 0.3242004279579435
RMSE train: 0.530885	val: 0.804157	test: 0.755322
MAE train: 0.411628	val: 0.608305	test: 0.595514

Epoch: 83
Loss: 0.30870538098471506
RMSE train: 0.512309	val: 0.780989	test: 0.734567

Epoch: 22
Loss: 0.8281088599136898
RMSE train: 0.789293	val: 1.084074	test: 0.962508
MAE train: 0.638143	val: 0.855648	test: 0.774325

Epoch: 23
Loss: 0.8107571303844452
RMSE train: 0.775855	val: 1.127766	test: 1.008138
MAE train: 0.623304	val: 0.889168	test: 0.811257

Epoch: 24
Loss: 0.7693794582571302
RMSE train: 0.763689	val: 1.139497	test: 1.007567
MAE train: 0.612027	val: 0.894891	test: 0.814304

Epoch: 25
Loss: 0.7243967396872384
RMSE train: 0.757186	val: 1.152871	test: 1.015963
MAE train: 0.606343	val: 0.907900	test: 0.821415

Epoch: 26
Loss: 0.7328216433525085
RMSE train: 0.766095	val: 1.090186	test: 0.987265
MAE train: 0.617146	val: 0.873605	test: 0.801883

Epoch: 27
Loss: 0.7303789598601205
RMSE train: 0.754362	val: 1.060048	test: 0.955942
MAE train: 0.604500	val: 0.844798	test: 0.765237

Epoch: 28
Loss: 0.7198318455900464
RMSE train: 0.714676	val: 1.094230	test: 0.983327
MAE train: 0.572446	val: 0.863142	test: 0.793747

Epoch: 29
Loss: 0.6533404546124595
RMSE train: 0.733230	val: 1.050745	test: 0.964360
MAE train: 0.591674	val: 0.832884	test: 0.778978

Epoch: 30
Loss: 0.6493655826364245
RMSE train: 0.693568	val: 1.021392	test: 0.935977
MAE train: 0.554477	val: 0.811274	test: 0.750836

Epoch: 31
Loss: 0.6440252917153495
RMSE train: 0.695658	val: 1.057503	test: 0.964609
MAE train: 0.559353	val: 0.838538	test: 0.778113

Epoch: 32
Loss: 0.6165630604539599
RMSE train: 0.660179	val: 1.095304	test: 0.976475
MAE train: 0.527125	val: 0.858659	test: 0.783884

Epoch: 33
Loss: 0.6058599310261863
RMSE train: 0.673050	val: 1.101071	test: 0.986862
MAE train: 0.538008	val: 0.870244	test: 0.799180

Epoch: 34
Loss: 0.5957314372062683
RMSE train: 0.667248	val: 1.083615	test: 0.987005
MAE train: 0.534677	val: 0.855524	test: 0.798561

Epoch: 35
Loss: 0.5601581973688943
RMSE train: 0.634801	val: 1.113440	test: 0.992369
MAE train: 0.508695	val: 0.883844	test: 0.800004

Epoch: 36
Loss: 0.5334964139120919
RMSE train: 0.615006	val: 1.062006	test: 0.959767
MAE train: 0.491784	val: 0.839987	test: 0.767098

Epoch: 37
Loss: 0.5492674240044185
RMSE train: 0.619914	val: 1.040323	test: 0.948659
MAE train: 0.494382	val: 0.828771	test: 0.758757

Epoch: 38
Loss: 0.543163001537323
RMSE train: 0.593396	val: 1.045725	test: 0.950863
MAE train: 0.471861	val: 0.824938	test: 0.765967

Epoch: 39
Loss: 0.5196761552776609
RMSE train: 0.598288	val: 1.045954	test: 0.959045
MAE train: 0.477510	val: 0.831067	test: 0.767514

Epoch: 40
Loss: 0.49348769230501993
RMSE train: 0.594216	val: 1.133660	test: 1.011130
MAE train: 0.474722	val: 0.896095	test: 0.810127

Epoch: 41
Loss: 0.4494396969676018
RMSE train: 0.552456	val: 1.057465	test: 0.956306
MAE train: 0.439366	val: 0.829797	test: 0.763955

Epoch: 42
Loss: 0.4487678217036383
RMSE train: 0.569342	val: 1.062678	test: 0.950856
MAE train: 0.453956	val: 0.843724	test: 0.765204

Epoch: 43
Loss: 0.4798728674650192
RMSE train: 0.560912	val: 1.033025	test: 0.957485
MAE train: 0.449463	val: 0.823791	test: 0.774961

Epoch: 44
Loss: 0.4366953543254307
RMSE train: 0.536748	val: 1.053749	test: 0.972801
MAE train: 0.428820	val: 0.850227	test: 0.780462

Epoch: 45
Loss: 0.43345535014356884
RMSE train: 0.532740	val: 1.067627	test: 0.964177
MAE train: 0.425839	val: 0.847039	test: 0.773435

Epoch: 46
Loss: 0.4099638398204531
RMSE train: 0.572607	val: 1.063554	test: 0.975718
MAE train: 0.460849	val: 0.848563	test: 0.789215

Epoch: 47
Loss: 0.4133829027414322
RMSE train: 0.519602	val: 1.031388	test: 0.944928
MAE train: 0.413138	val: 0.821496	test: 0.764320

Epoch: 48
Loss: 0.39945654358182636
RMSE train: 0.521869	val: 1.047328	test: 0.955689
MAE train: 0.416336	val: 0.828139	test: 0.759985

Epoch: 49
Loss: 0.4063792441572462
RMSE train: 0.544173	val: 1.060217	test: 0.986923
MAE train: 0.436610	val: 0.858683	test: 0.797347

Epoch: 50
Loss: 0.3778309353760311
RMSE train: 0.491477	val: 1.031114	test: 0.962945
MAE train: 0.387919	val: 0.834175	test: 0.774122

Epoch: 51
Loss: 0.37092575643743786
RMSE train: 0.480432	val: 1.060182	test: 0.958541
MAE train: 0.381617	val: 0.842286	test: 0.770202

Epoch: 52
Loss: 0.37504599136965616
RMSE train: 0.498969	val: 1.095117	test: 0.980984
MAE train: 0.396714	val: 0.869314	test: 0.791012

Epoch: 53
Loss: 0.3781746115003313
RMSE train: 0.480380	val: 1.066615	test: 0.985997
MAE train: 0.382437	val: 0.859947	test: 0.802967

Epoch: 54
Loss: 0.359485781618527
RMSE train: 0.512358	val: 1.055837	test: 0.951336
MAE train: 0.408087	val: 0.843936	test: 0.760257

Epoch: 55
Loss: 0.34339443274906706
RMSE train: 0.468059	val: 1.095014	test: 0.976264
MAE train: 0.371403	val: 0.863905	test: 0.778304

Epoch: 56
Loss: 0.3307341124330248
RMSE train: 0.475478	val: 1.048432	test: 0.959009
MAE train: 0.378275	val: 0.841807	test: 0.774737

Epoch: 57
Loss: 0.36111571959086824
RMSE train: 0.513522	val: 1.070125	test: 0.950358
MAE train: 0.406094	val: 0.845907	test: 0.755755

Epoch: 58
Loss: 0.33633333444595337
RMSE train: 0.458086	val: 1.054190	test: 0.944314
MAE train: 0.366764	val: 0.835604	test: 0.755157

Epoch: 59
Loss: 0.3187434524297714
RMSE train: 0.476488	val: 1.105177	test: 0.979123
MAE train: 0.382355	val: 0.867233	test: 0.782512

Epoch: 60
Loss: 0.3082453714949744
RMSE train: 0.517112	val: 1.035986	test: 0.949282
MAE train: 0.418784	val: 0.826182	test: 0.753692

Epoch: 61
Loss: 0.2980727106332779
RMSE train: 0.451919	val: 1.038516	test: 0.946157
MAE train: 0.359322	val: 0.826992	test: 0.762381

Epoch: 62
Loss: 0.29337779858282637
RMSE train: 0.426077	val: 1.060683	test: 0.962419
MAE train: 0.339910	val: 0.834277	test: 0.774185

Epoch: 63
Loss: 0.291835045175893
RMSE train: 0.405371	val: 1.075874	test: 0.971194
MAE train: 0.322463	val: 0.848069	test: 0.778638

Epoch: 64
Loss: 0.27830763693366734
RMSE train: 0.432743	val: 1.086128	test: 0.980684
MAE train: 0.343887	val: 0.857420	test: 0.786018

Epoch: 65
Loss: 0.2623247972556523
RMSE train: 0.408545	val: 1.070072	test: 0.961233
MAE train: 0.325669	val: 0.843400	test: 0.768546

Epoch: 66
Loss: 0.268309157873903
RMSE train: 0.445829	val: 1.103770	test: 0.986713
MAE train: 0.358127	val: 0.869876	test: 0.788560

Epoch: 67
Loss: 0.2778861033064978
RMSE train: 0.393926	val: 1.039381	test: 0.940514
MAE train: 0.311184	val: 0.817286	test: 0.743827

Epoch: 68
Loss: 0.26575542773519245
RMSE train: 0.457133	val: 1.066496	test: 0.959351
MAE train: 0.363302	val: 0.837227	test: 0.762537

Epoch: 69
Loss: 0.25286497282130377
RMSE train: 0.401500	val: 1.045077	test: 0.948621
MAE train: 0.317330	val: 0.823459	test: 0.758411

Epoch: 70
Loss: 0.2539928374545915
RMSE train: 0.464000	val: 1.051760	test: 0.966199
MAE train: 0.377856	val: 0.827607	test: 0.769770

Epoch: 71
Loss: 0.2429554345352309
RMSE train: 0.400502	val: 1.072679	test: 0.962234
MAE train: 0.317695	val: 0.847128	test: 0.767506

Epoch: 72
Loss: 0.24349793472460338
RMSE train: 0.449116	val: 1.056198	test: 0.955101
MAE train: 0.353192	val: 0.831822	test: 0.761473

Epoch: 73
Loss: 0.2347081782562392
RMSE train: 0.401752	val: 1.029913	test: 0.935167
MAE train: 0.318847	val: 0.811046	test: 0.747304

Epoch: 74
Loss: 0.2372200105871473
RMSE train: 0.390481	val: 1.039406	test: 0.937764
MAE train: 0.310336	val: 0.817827	test: 0.752644

Epoch: 75
Loss: 0.2281405425497464
RMSE train: 0.377923	val: 1.050714	test: 0.945420
MAE train: 0.298553	val: 0.820289	test: 0.751646

Epoch: 76
Loss: 0.22895450783627375
RMSE train: 0.407974	val: 1.052674	test: 0.972306
MAE train: 0.324605	val: 0.837116	test: 0.782086

Epoch: 77
Loss: 0.21178952604532242
RMSE train: 0.389496	val: 1.047860	test: 0.949133
MAE train: 0.308880	val: 0.820861	test: 0.759879

Epoch: 78
Loss: 0.21969151496887207
RMSE train: 0.382179	val: 1.038343	test: 0.946190
MAE train: 0.304417	val: 0.832010	test: 0.763291

Epoch: 79
Loss: 0.2100527925150735
RMSE train: 0.350280	val: 1.046011	test: 0.951929
MAE train: 0.277807	val: 0.829115	test: 0.766301

Epoch: 80
Loss: 0.21317006328276225
RMSE train: 0.401376	val: 1.020407	test: 0.934676
MAE train: 0.321010	val: 0.815512	test: 0.751818

Epoch: 81
Loss: 0.20583062725407736
RMSE train: 0.354314	val: 1.041813	test: 0.958253
MAE train: 0.282143	val: 0.823378	test: 0.764504

Epoch: 82
Loss: 0.2010833120771817
RMSE train: 0.356464	val: 1.062384	test: 0.964662

Epoch: 22
Loss: 0.8360496376241956
RMSE train: 0.809187	val: 1.073351	test: 0.980628
MAE train: 0.647251	val: 0.837120	test: 0.787986

Epoch: 23
Loss: 0.8532142128263202
RMSE train: 0.837390	val: 1.126736	test: 1.016589
MAE train: 0.666686	val: 0.876842	test: 0.813345

Epoch: 24
Loss: 0.7803670593670436
RMSE train: 0.788992	val: 1.072541	test: 0.972284
MAE train: 0.629321	val: 0.827071	test: 0.779772

Epoch: 25
Loss: 0.7902156966073173
RMSE train: 0.769925	val: 1.083094	test: 0.984728
MAE train: 0.612093	val: 0.834937	test: 0.794815

Epoch: 26
Loss: 0.7491771365915026
RMSE train: 0.816228	val: 1.125599	test: 1.023781
MAE train: 0.648847	val: 0.871592	test: 0.815428

Epoch: 27
Loss: 0.7220966645649501
RMSE train: 0.762606	val: 1.115434	test: 1.029143
MAE train: 0.609141	val: 0.865705	test: 0.828926

Epoch: 28
Loss: 0.6956026617969785
RMSE train: 0.732951	val: 1.176606	test: 1.104204
MAE train: 0.585404	val: 0.905427	test: 0.891211

Epoch: 29
Loss: 0.6655144350869315
RMSE train: 0.723927	val: 1.099149	test: 1.016662
MAE train: 0.576778	val: 0.846377	test: 0.814503

Epoch: 30
Loss: 0.6819362129483905
RMSE train: 0.701918	val: 1.206397	test: 1.131779
MAE train: 0.560235	val: 0.925988	test: 0.898154

Epoch: 31
Loss: 0.6694331765174866
RMSE train: 0.677758	val: 1.066053	test: 0.987789
MAE train: 0.540353	val: 0.826491	test: 0.794398

Epoch: 32
Loss: 0.6416712062699454
RMSE train: 0.681577	val: 1.072796	test: 1.004468
MAE train: 0.542326	val: 0.843667	test: 0.822662

Epoch: 33
Loss: 0.626455524138042
RMSE train: 0.681439	val: 1.100986	test: 1.031550
MAE train: 0.541147	val: 0.862135	test: 0.850036

Epoch: 34
Loss: 0.6101224763052804
RMSE train: 0.660755	val: 1.144118	test: 1.062221
MAE train: 0.526317	val: 0.891904	test: 0.868941

Epoch: 35
Loss: 0.6261537202766964
RMSE train: 0.680032	val: 1.117703	test: 1.040599
MAE train: 0.538432	val: 0.865937	test: 0.839405

Epoch: 36
Loss: 0.5932576954364777
RMSE train: 0.669211	val: 1.105546	test: 1.043175
MAE train: 0.528079	val: 0.867838	test: 0.853719

Epoch: 37
Loss: 0.5861373926912036
RMSE train: 0.672248	val: 1.096111	test: 1.025284
MAE train: 0.533326	val: 0.854680	test: 0.831921

Epoch: 38
Loss: 0.5639796257019043
RMSE train: 0.642787	val: 1.090223	test: 1.052011
MAE train: 0.510108	val: 0.885484	test: 0.858307

Epoch: 39
Loss: 0.5490243434906006
RMSE train: 0.603149	val: 1.067108	test: 1.043105
MAE train: 0.479192	val: 0.878921	test: 0.846991

Epoch: 40
Loss: 0.5161938049963543
RMSE train: 0.590915	val: 1.078341	test: 1.062004
MAE train: 0.469075	val: 0.885232	test: 0.860526

Epoch: 41
Loss: 0.4899578349930899
RMSE train: 0.610154	val: 1.086823	test: 1.035109
MAE train: 0.484677	val: 0.882749	test: 0.841934

Epoch: 42
Loss: 0.5071473483528409
RMSE train: 0.604584	val: 1.087346	test: 1.027735
MAE train: 0.481581	val: 0.866602	test: 0.841783

Epoch: 43
Loss: 0.4704005207334246
RMSE train: 0.593943	val: 1.109503	test: 1.048117
MAE train: 0.472435	val: 0.876375	test: 0.861993

Epoch: 44
Loss: 0.4793581707136972
RMSE train: 0.576093	val: 1.048669	test: 0.988031
MAE train: 0.459528	val: 0.851731	test: 0.807750

Epoch: 45
Loss: 0.46138656990868704
RMSE train: 0.588059	val: 1.014977	test: 0.957398
MAE train: 0.466258	val: 0.821535	test: 0.782126

Epoch: 46
Loss: 0.46406872996262144
RMSE train: 0.557659	val: 1.080755	test: 1.026892
MAE train: 0.443506	val: 0.873584	test: 0.837618

Epoch: 47
Loss: 0.40898203424045015
RMSE train: 0.541427	val: 1.062141	test: 1.001239
MAE train: 0.429952	val: 0.848573	test: 0.811894

Epoch: 48
Loss: 0.39749806587185177
RMSE train: 0.573982	val: 1.084106	test: 1.018415
MAE train: 0.458942	val: 0.872939	test: 0.833289

Epoch: 49
Loss: 0.39859971191201893
RMSE train: 0.499820	val: 1.079774	test: 1.027746
MAE train: 0.397770	val: 0.870347	test: 0.833681

Epoch: 50
Loss: 0.3925029954739979
RMSE train: 0.526217	val: 1.061013	test: 1.001824
MAE train: 0.418320	val: 0.854412	test: 0.817619

Epoch: 51
Loss: 0.3914106466940471
RMSE train: 0.544965	val: 1.110264	test: 1.035902
MAE train: 0.433197	val: 0.877181	test: 0.844097

Epoch: 52
Loss: 0.3816929353134973
RMSE train: 0.488683	val: 1.107352	test: 1.062917
MAE train: 0.388008	val: 0.903814	test: 0.863114

Epoch: 53
Loss: 0.38048526644706726
RMSE train: 0.592780	val: 1.116249	test: 1.050066
MAE train: 0.468149	val: 0.900342	test: 0.853106

Epoch: 54
Loss: 0.376247763633728
RMSE train: 0.521020	val: 1.070812	test: 1.009843
MAE train: 0.414126	val: 0.857933	test: 0.817687

Epoch: 55
Loss: 0.3546662777662277
RMSE train: 0.479518	val: 1.030361	test: 0.979510
MAE train: 0.378608	val: 0.827842	test: 0.798032

Epoch: 56
Loss: 0.3305093262876783
RMSE train: 0.505656	val: 1.072354	test: 1.019243
MAE train: 0.401119	val: 0.866959	test: 0.827852

Epoch: 57
Loss: 0.32316821813583374
RMSE train: 0.472826	val: 1.070063	test: 1.013425
MAE train: 0.375503	val: 0.865398	test: 0.824462

Epoch: 58
Loss: 0.3300475063068526
RMSE train: 0.475298	val: 1.033872	test: 0.970032
MAE train: 0.375645	val: 0.838622	test: 0.788982

Epoch: 59
Loss: 0.33682592851775034
RMSE train: 0.467444	val: 1.057913	test: 0.994192
MAE train: 0.369116	val: 0.848227	test: 0.811997

Epoch: 60
Loss: 0.32645555266312193
RMSE train: 0.426807	val: 1.045045	test: 0.988134
MAE train: 0.336736	val: 0.851371	test: 0.809874

Epoch: 61
Loss: 0.30182046975408283
RMSE train: 0.418943	val: 1.056125	test: 1.006399
MAE train: 0.331833	val: 0.862077	test: 0.818107

Epoch: 62
Loss: 0.30803267657756805
RMSE train: 0.464432	val: 1.089923	test: 1.027503
MAE train: 0.367750	val: 0.884497	test: 0.838729

Epoch: 63
Loss: 0.29581611284187864
RMSE train: 0.460532	val: 1.050463	test: 0.994228
MAE train: 0.366353	val: 0.853909	test: 0.808580

Epoch: 64
Loss: 0.29182203114032745
RMSE train: 0.471757	val: 1.112148	test: 1.054654
MAE train: 0.377259	val: 0.897540	test: 0.858580

Epoch: 65
Loss: 0.28511172107287813
RMSE train: 0.466029	val: 1.069955	test: 1.019966
MAE train: 0.371528	val: 0.880714	test: 0.836010

Epoch: 66
Loss: 0.28217695334127973
RMSE train: 0.417340	val: 1.044514	test: 0.998785
MAE train: 0.330805	val: 0.843704	test: 0.809211

Epoch: 67
Loss: 0.2713081017136574
RMSE train: 0.411121	val: 0.997233	test: 0.937142
MAE train: 0.325123	val: 0.801092	test: 0.759720

Epoch: 68
Loss: 0.2692372830850737
RMSE train: 0.407712	val: 1.053366	test: 0.980191
MAE train: 0.323235	val: 0.837609	test: 0.800428

Epoch: 69
Loss: 0.27066815750939505
RMSE train: 0.473701	val: 1.061128	test: 0.981213
MAE train: 0.379701	val: 0.835966	test: 0.791614

Epoch: 70
Loss: 0.26423405004399164
RMSE train: 0.538007	val: 1.112698	test: 1.038382
MAE train: 0.427205	val: 0.875658	test: 0.838826

Epoch: 71
Loss: 0.25300837574260576
RMSE train: 0.430416	val: 1.037477	test: 0.973290
MAE train: 0.340209	val: 0.837747	test: 0.795450

Epoch: 72
Loss: 0.2478589415550232
RMSE train: 0.417993	val: 1.068601	test: 1.002279
MAE train: 0.330747	val: 0.848989	test: 0.810174

Epoch: 73
Loss: 0.2439276363168444
RMSE train: 0.408435	val: 1.050995	test: 0.986443
MAE train: 0.322680	val: 0.833366	test: 0.796500

Epoch: 74
Loss: 0.24009879039866583
RMSE train: 0.403197	val: 1.022622	test: 0.957448
MAE train: 0.320299	val: 0.817647	test: 0.777626

Epoch: 75
Loss: 0.23944059227194106
RMSE train: 0.374186	val: 1.071306	test: 1.011073
MAE train: 0.296705	val: 0.854647	test: 0.824019

Epoch: 76
Loss: 0.21677477764231817
RMSE train: 0.367581	val: 1.083213	test: 1.021861
MAE train: 0.291747	val: 0.867122	test: 0.834028

Epoch: 77
Loss: 0.26494345707552774
RMSE train: 0.418333	val: 1.083723	test: 1.003506
MAE train: 0.330531	val: 0.854479	test: 0.814181

Epoch: 78
Loss: 0.2341775777084487
RMSE train: 0.446758	val: 1.059455	test: 0.998172
MAE train: 0.351954	val: 0.856676	test: 0.812799

Epoch: 79
Loss: 0.22625311676945006
RMSE train: 0.392672	val: 1.011302	test: 0.951815
MAE train: 0.309644	val: 0.817683	test: 0.782535

Epoch: 80
Loss: 0.2307118166770254
RMSE train: 0.356823	val: 1.047638	test: 1.002438
MAE train: 0.281699	val: 0.855892	test: 0.819795

Epoch: 81
Loss: 0.22046866374356405
RMSE train: 0.377732	val: 1.069407	test: 1.022904
MAE train: 0.299652	val: 0.869931	test: 0.827761

Epoch: 82
Loss: 0.2193770855665207
RMSE train: 0.424329	val: 1.063709	test: 1.004355

Epoch: 22
Loss: 0.8305634302752358
RMSE train: 0.820342	val: 1.077925	test: 0.981196
MAE train: 0.663888	val: 0.857798	test: 0.793440

Epoch: 23
Loss: 0.8012227501188006
RMSE train: 0.811521	val: 1.064475	test: 0.988245
MAE train: 0.655903	val: 0.849468	test: 0.802605

Epoch: 24
Loss: 0.8073209268706185
RMSE train: 0.802010	val: 1.080296	test: 1.031173
MAE train: 0.642501	val: 0.870394	test: 0.837768

Epoch: 25
Loss: 0.7711217701435089
RMSE train: 0.783705	val: 1.044935	test: 0.978345
MAE train: 0.632929	val: 0.838272	test: 0.802705

Epoch: 26
Loss: 0.74973811847823
RMSE train: 0.753814	val: 1.042099	test: 0.956800
MAE train: 0.605195	val: 0.821350	test: 0.776828

Epoch: 27
Loss: 0.748597149338041
RMSE train: 0.751275	val: 1.047560	test: 0.975340
MAE train: 0.604952	val: 0.836263	test: 0.793005

Epoch: 28
Loss: 0.6865555005414146
RMSE train: 0.738556	val: 1.038389	test: 0.989252
MAE train: 0.591841	val: 0.837416	test: 0.805193

Epoch: 29
Loss: 0.6767223349639347
RMSE train: 0.710144	val: 1.046846	test: 0.978630
MAE train: 0.568063	val: 0.828184	test: 0.789505

Epoch: 30
Loss: 0.686122464282172
RMSE train: 0.703212	val: 1.042900	test: 0.968505
MAE train: 0.563342	val: 0.819317	test: 0.785045

Epoch: 31
Loss: 0.6532790958881378
RMSE train: 0.687226	val: 1.044308	test: 0.976540
MAE train: 0.549392	val: 0.832206	test: 0.790013

Epoch: 32
Loss: 0.6220864611012595
RMSE train: 0.706951	val: 1.041344	test: 0.968356
MAE train: 0.566139	val: 0.827168	test: 0.782416

Epoch: 33
Loss: 0.6052813231945038
RMSE train: 0.668448	val: 1.065577	test: 1.017991
MAE train: 0.533690	val: 0.837949	test: 0.826009

Epoch: 34
Loss: 0.6182277181318828
RMSE train: 0.660624	val: 1.088137	test: 1.004073
MAE train: 0.528455	val: 0.842352	test: 0.808943

Epoch: 35
Loss: 0.5687411682946342
RMSE train: 0.637211	val: 1.051124	test: 0.984032
MAE train: 0.510643	val: 0.825660	test: 0.798406

Epoch: 36
Loss: 0.5703957315002169
RMSE train: 0.642706	val: 1.105690	test: 1.054389
MAE train: 0.513635	val: 0.853095	test: 0.854268

Epoch: 37
Loss: 0.5415390368018832
RMSE train: 0.625397	val: 1.038745	test: 0.941237
MAE train: 0.498558	val: 0.811543	test: 0.757601

Epoch: 38
Loss: 0.5361957422324589
RMSE train: 0.613459	val: 1.079802	test: 0.980109
MAE train: 0.487040	val: 0.834222	test: 0.784607

Epoch: 39
Loss: 0.5018077492713928
RMSE train: 0.613494	val: 1.023497	test: 0.953666
MAE train: 0.487578	val: 0.814700	test: 0.774364

Epoch: 40
Loss: 0.5032387844153813
RMSE train: 0.582556	val: 1.058427	test: 0.978631
MAE train: 0.464460	val: 0.824051	test: 0.801393

Epoch: 41
Loss: 0.5347358520541873
RMSE train: 0.595734	val: 1.046889	test: 0.967358
MAE train: 0.473769	val: 0.817320	test: 0.782350

Epoch: 42
Loss: 0.48319304628031595
RMSE train: 0.597360	val: 1.063650	test: 1.000140
MAE train: 0.475698	val: 0.830188	test: 0.809905

Epoch: 43
Loss: 0.46649792790412903
RMSE train: 0.562118	val: 1.062512	test: 0.991712
MAE train: 0.450389	val: 0.821774	test: 0.803288

Epoch: 44
Loss: 0.45455025349344524
RMSE train: 0.544304	val: 1.044897	test: 0.976476
MAE train: 0.432976	val: 0.809781	test: 0.788964

Epoch: 45
Loss: 0.42049975054604666
RMSE train: 0.537339	val: 1.004529	test: 0.946587
MAE train: 0.428829	val: 0.792540	test: 0.763562

Epoch: 46
Loss: 0.4289620156799044
RMSE train: 0.540140	val: 1.033926	test: 0.951372
MAE train: 0.431648	val: 0.808432	test: 0.767435

Epoch: 47
Loss: 0.4081604778766632
RMSE train: 0.518704	val: 1.029524	test: 0.974460
MAE train: 0.410438	val: 0.809209	test: 0.784315

Epoch: 48
Loss: 0.3938191256352833
RMSE train: 0.530641	val: 1.050161	test: 0.979485
MAE train: 0.423375	val: 0.815946	test: 0.793871

Epoch: 49
Loss: 0.40098745269434793
RMSE train: 0.513186	val: 1.064187	test: 1.002868
MAE train: 0.409203	val: 0.820695	test: 0.807778

Epoch: 50
Loss: 0.38181738555431366
RMSE train: 0.482166	val: 1.057940	test: 0.996075
MAE train: 0.385454	val: 0.821061	test: 0.805331

Epoch: 51
Loss: 0.37894529742853983
RMSE train: 0.549618	val: 1.038011	test: 0.986824
MAE train: 0.442307	val: 0.838972	test: 0.803444

Epoch: 52
Loss: 0.4025599317891257
RMSE train: 0.539141	val: 1.038908	test: 0.968695
MAE train: 0.428707	val: 0.816442	test: 0.781009

Epoch: 53
Loss: 0.38736375527722494
RMSE train: 0.481566	val: 1.038852	test: 0.980830
MAE train: 0.384487	val: 0.811491	test: 0.798066

Epoch: 54
Loss: 0.3730508450950895
RMSE train: 0.450151	val: 1.044648	test: 1.003844
MAE train: 0.358668	val: 0.815072	test: 0.812953

Epoch: 55
Loss: 0.35068794233458384
RMSE train: 0.485577	val: 1.013435	test: 0.965704
MAE train: 0.390008	val: 0.801826	test: 0.788532

Epoch: 56
Loss: 0.34597618665013996
RMSE train: 0.470322	val: 1.029676	test: 0.960597
MAE train: 0.374009	val: 0.799564	test: 0.775024

Epoch: 57
Loss: 0.3475514394896371
RMSE train: 0.443545	val: 1.094102	test: 1.042200
MAE train: 0.352773	val: 0.846623	test: 0.843126

Epoch: 58
Loss: 0.33549059288842337
RMSE train: 0.541996	val: 1.150550	test: 1.074912
MAE train: 0.436237	val: 0.884228	test: 0.862018

Epoch: 59
Loss: 0.3135083626423563
RMSE train: 0.418483	val: 1.000771	test: 0.933924
MAE train: 0.334090	val: 0.781784	test: 0.758687

Epoch: 60
Loss: 0.30514085718563627
RMSE train: 0.445600	val: 1.039464	test: 0.966882
MAE train: 0.355642	val: 0.801217	test: 0.779355

Epoch: 61
Loss: 0.31378048019749777
RMSE train: 0.440100	val: 1.021990	test: 0.994643
MAE train: 0.348951	val: 0.813801	test: 0.802687

Epoch: 62
Loss: 0.2992795280047825
RMSE train: 0.416341	val: 1.033541	test: 0.976810
MAE train: 0.329589	val: 0.805135	test: 0.787711

Epoch: 63
Loss: 0.29817551374435425
RMSE train: 0.512522	val: 1.022728	test: 0.941334
MAE train: 0.407871	val: 0.802401	test: 0.760801

Epoch: 64
Loss: 0.29962688258716036
RMSE train: 0.440441	val: 1.079574	test: 1.007185
MAE train: 0.353581	val: 0.831895	test: 0.813647

Epoch: 65
Loss: 0.26767455360719133
RMSE train: 0.480057	val: 1.068511	test: 0.990295
MAE train: 0.388685	val: 0.824528	test: 0.797843

Epoch: 66
Loss: 0.29052899458578657
RMSE train: 0.387061	val: 1.038856	test: 0.975266
MAE train: 0.309955	val: 0.805378	test: 0.792863

Epoch: 67
Loss: 0.2723902634211949
RMSE train: 0.400385	val: 1.007872	test: 0.970166
MAE train: 0.319161	val: 0.810448	test: 0.793536

Epoch: 68
Loss: 0.25677184760570526
RMSE train: 0.399659	val: 0.992653	test: 0.949346
MAE train: 0.317737	val: 0.785960	test: 0.774664

Epoch: 69
Loss: 0.26040807579244885
RMSE train: 0.545180	val: 1.058291	test: 0.973406
MAE train: 0.439224	val: 0.822388	test: 0.784570

Epoch: 70
Loss: 0.2691443881818226
RMSE train: 0.377755	val: 1.036899	test: 0.963533
MAE train: 0.302561	val: 0.800050	test: 0.779356

Epoch: 71
Loss: 0.2590843119791576
RMSE train: 0.404316	val: 0.996570	test: 0.945007
MAE train: 0.322771	val: 0.788790	test: 0.768453

Epoch: 72
Loss: 0.25397065175431116
RMSE train: 0.416851	val: 1.034077	test: 0.991875
MAE train: 0.333305	val: 0.804757	test: 0.804920

Epoch: 73
Loss: 0.2476900251848357
RMSE train: 0.428918	val: 1.013445	test: 0.944172
MAE train: 0.343877	val: 0.792379	test: 0.767304

Epoch: 74
Loss: 0.23899236640759877
RMSE train: 0.455686	val: 1.019290	test: 0.949089
MAE train: 0.365229	val: 0.793356	test: 0.765713

Epoch: 75
Loss: 0.23501889194761003
RMSE train: 0.399460	val: 1.076045	test: 1.011930
MAE train: 0.322250	val: 0.828517	test: 0.814766

Epoch: 76
Loss: 0.2356670583997454
RMSE train: 0.369100	val: 1.004798	test: 0.950109
MAE train: 0.295312	val: 0.791315	test: 0.774448

Epoch: 77
Loss: 0.25285758823156357
RMSE train: 0.390072	val: 1.072619	test: 1.003017
MAE train: 0.308410	val: 0.825123	test: 0.810536

Epoch: 78
Loss: 0.25260522429432186
RMSE train: 0.448177	val: 1.012626	test: 0.973615
MAE train: 0.353496	val: 0.795647	test: 0.789770

Epoch: 79
Loss: 0.23602017547403062
RMSE train: 0.441476	val: 1.066437	test: 1.012533
MAE train: 0.356959	val: 0.819330	test: 0.815225

Epoch: 80
Loss: 0.22741724018539702
RMSE train: 0.376090	val: 1.002670	test: 0.983076
MAE train: 0.299168	val: 0.807075	test: 0.798245

Epoch: 81
Loss: 0.21699385132108415
RMSE train: 0.392067	val: 1.030504	test: 0.979548
MAE train: 0.313547	val: 0.807144	test: 0.794899

Epoch: 82
Loss: 0.20047813334635325
RMSE train: 0.360549	val: 1.009608	test: 0.998138

Epoch: 22
Loss: 0.7649290433951786
RMSE train: 0.771088	val: 1.007557	test: 0.915210
MAE train: 0.618029	val: 0.802273	test: 0.735983

Epoch: 23
Loss: 0.7469705896718162
RMSE train: 0.762412	val: 1.021515	test: 0.948037
MAE train: 0.613572	val: 0.821852	test: 0.761143

Epoch: 24
Loss: 0.6690104667629514
RMSE train: 0.748595	val: 1.007419	test: 0.924646
MAE train: 0.597058	val: 0.810955	test: 0.742272

Epoch: 25
Loss: 0.6961940314088549
RMSE train: 0.747062	val: 1.009746	test: 0.911212
MAE train: 0.593791	val: 0.806496	test: 0.727069

Epoch: 26
Loss: 0.701201971088137
RMSE train: 0.716782	val: 0.995441	test: 0.910724
MAE train: 0.572099	val: 0.792911	test: 0.729493

Epoch: 27
Loss: 0.6818705924919674
RMSE train: 0.744384	val: 1.003474	test: 0.906674
MAE train: 0.592384	val: 0.795598	test: 0.728903

Epoch: 28
Loss: 0.6780389079025814
RMSE train: 0.715693	val: 0.961359	test: 0.885399
MAE train: 0.569425	val: 0.762322	test: 0.710937

Epoch: 29
Loss: 0.6413464971951076
RMSE train: 0.697770	val: 0.981655	test: 0.888214
MAE train: 0.559985	val: 0.786136	test: 0.716046

Epoch: 30
Loss: 0.6154616049357823
RMSE train: 0.684535	val: 0.994052	test: 0.909933
MAE train: 0.546053	val: 0.788207	test: 0.724626

Epoch: 31
Loss: 0.6393126334462848
RMSE train: 0.686077	val: 1.036449	test: 0.956215
MAE train: 0.545264	val: 0.821177	test: 0.760708

Epoch: 32
Loss: 0.5937255365507943
RMSE train: 0.667217	val: 0.969737	test: 0.881276
MAE train: 0.531058	val: 0.770204	test: 0.702185

Epoch: 33
Loss: 0.5797327671732221
RMSE train: 0.643221	val: 0.972636	test: 0.896247
MAE train: 0.512574	val: 0.772907	test: 0.717179

Epoch: 34
Loss: 0.5682426393032074
RMSE train: 0.643339	val: 0.977002	test: 0.907252
MAE train: 0.514273	val: 0.778660	test: 0.725669

Epoch: 35
Loss: 0.5518761000462941
RMSE train: 0.639193	val: 0.980715	test: 0.897502
MAE train: 0.507462	val: 0.782500	test: 0.717940

Epoch: 36
Loss: 0.5378913496221814
RMSE train: 0.616812	val: 0.968331	test: 0.889311
MAE train: 0.491574	val: 0.776025	test: 0.707306

Epoch: 37
Loss: 0.5214972964354924
RMSE train: 0.610701	val: 0.968291	test: 0.897633
MAE train: 0.486150	val: 0.783014	test: 0.714443

Epoch: 38
Loss: 0.5309417120047978
RMSE train: 0.612669	val: 0.971578	test: 0.911647
MAE train: 0.488814	val: 0.784305	test: 0.731381

Epoch: 39
Loss: 0.5087039875132697
RMSE train: 0.636743	val: 0.990039	test: 0.911852
MAE train: 0.507319	val: 0.786225	test: 0.726448

Epoch: 40
Loss: 0.5098797572510583
RMSE train: 0.594622	val: 0.963378	test: 0.901636
MAE train: 0.473939	val: 0.774717	test: 0.719530

Epoch: 41
Loss: 0.4795866289309093
RMSE train: 0.568084	val: 0.974694	test: 0.907397
MAE train: 0.449005	val: 0.776358	test: 0.725355

Epoch: 42
Loss: 0.47354487649032045
RMSE train: 0.585751	val: 0.957332	test: 0.884596
MAE train: 0.466006	val: 0.771502	test: 0.712992

Epoch: 43
Loss: 0.4986200226204736
RMSE train: 0.576096	val: 0.994590	test: 0.909457
MAE train: 0.457451	val: 0.796004	test: 0.730229

Epoch: 44
Loss: 0.4611717824425016
RMSE train: 0.585477	val: 0.962709	test: 0.891744
MAE train: 0.465619	val: 0.773368	test: 0.711043

Epoch: 45
Loss: 0.4437688482659204
RMSE train: 0.592454	val: 1.021954	test: 0.927536
MAE train: 0.472765	val: 0.808588	test: 0.753899

Epoch: 46
Loss: 0.4289102107286453
RMSE train: 0.543663	val: 0.971071	test: 0.902466
MAE train: 0.432105	val: 0.774924	test: 0.724272

Epoch: 47
Loss: 0.44436990576131
RMSE train: 0.536675	val: 0.954840	test: 0.883825
MAE train: 0.429200	val: 0.764214	test: 0.711764

Epoch: 48
Loss: 0.4103791905300958
RMSE train: 0.518529	val: 0.973857	test: 0.891936
MAE train: 0.412168	val: 0.774296	test: 0.718716

Epoch: 49
Loss: 0.4061236338956015
RMSE train: 0.527168	val: 0.973913	test: 0.911201
MAE train: 0.418907	val: 0.775199	test: 0.731487

Epoch: 50
Loss: 0.4106098328317915
RMSE train: 0.509160	val: 0.974459	test: 0.899241
MAE train: 0.403897	val: 0.774253	test: 0.723675

Epoch: 51
Loss: 0.3931885595832552
RMSE train: 0.507029	val: 0.978977	test: 0.914442
MAE train: 0.399899	val: 0.780971	test: 0.736224

Epoch: 52
Loss: 0.38903371563979555
RMSE train: 0.502370	val: 0.999608	test: 0.916685
MAE train: 0.397348	val: 0.796689	test: 0.739400

Epoch: 53
Loss: 0.3898750713893345
RMSE train: 0.489494	val: 0.989165	test: 0.900326
MAE train: 0.388257	val: 0.783493	test: 0.727587

Epoch: 54
Loss: 0.3762425184249878
RMSE train: 0.484704	val: 0.963804	test: 0.876726
MAE train: 0.383574	val: 0.767890	test: 0.704404

Epoch: 55
Loss: 0.3740032911300659
RMSE train: 0.458897	val: 0.972756	test: 0.891312
MAE train: 0.362059	val: 0.769578	test: 0.712276

Epoch: 56
Loss: 0.35845884467874256
RMSE train: 0.511239	val: 0.984880	test: 0.887426
MAE train: 0.405196	val: 0.778382	test: 0.715835

Epoch: 57
Loss: 0.40636724872248514
RMSE train: 0.508743	val: 0.961715	test: 0.872729
MAE train: 0.403848	val: 0.766169	test: 0.693363

Epoch: 58
Loss: 0.33612140800271717
RMSE train: 0.467725	val: 0.954773	test: 0.879184
MAE train: 0.370989	val: 0.757274	test: 0.706035

Epoch: 59
Loss: 0.33337278025490896
RMSE train: 0.500129	val: 1.000566	test: 0.919136
MAE train: 0.399391	val: 0.792766	test: 0.744681

Epoch: 60
Loss: 0.3335264665739877
RMSE train: 0.485949	val: 0.965381	test: 0.886670
MAE train: 0.385039	val: 0.775359	test: 0.715346

Epoch: 61
Loss: 0.3226860761642456
RMSE train: 0.454874	val: 0.982041	test: 0.913925
MAE train: 0.356220	val: 0.777725	test: 0.730713

Epoch: 62
Loss: 0.3115282271589552
RMSE train: 0.466611	val: 0.990958	test: 0.901581
MAE train: 0.368388	val: 0.780625	test: 0.725724

Epoch: 63
Loss: 0.3269165796892984
RMSE train: 0.528943	val: 1.008878	test: 0.916610
MAE train: 0.424879	val: 0.800069	test: 0.739260

Epoch: 64
Loss: 0.31850743506635937
RMSE train: 0.436561	val: 0.973754	test: 0.888019
MAE train: 0.346400	val: 0.770588	test: 0.718346

Epoch: 65
Loss: 0.29092042573860716
RMSE train: 0.435023	val: 0.980234	test: 0.880933
MAE train: 0.343510	val: 0.774483	test: 0.713065

Epoch: 66
Loss: 0.3007366231509617
RMSE train: 0.407453	val: 0.978304	test: 0.884314
MAE train: 0.320333	val: 0.765005	test: 0.712793

Epoch: 67
Loss: 0.2928970158100128
RMSE train: 0.416885	val: 0.957479	test: 0.871976
MAE train: 0.330946	val: 0.758654	test: 0.700507

Epoch: 68
Loss: 0.29379238933324814
RMSE train: 0.445646	val: 0.977165	test: 0.898013
MAE train: 0.352531	val: 0.772435	test: 0.721893

Epoch: 69
Loss: 0.2776891163417271
RMSE train: 0.399302	val: 0.958749	test: 0.869474
MAE train: 0.315599	val: 0.756288	test: 0.703562

Epoch: 70
Loss: 0.2692134327122143
RMSE train: 0.391120	val: 0.954705	test: 0.882674
MAE train: 0.309030	val: 0.758338	test: 0.710328

Epoch: 71
Loss: 0.2648257442883083
RMSE train: 0.408842	val: 0.961632	test: 0.899041
MAE train: 0.323816	val: 0.767536	test: 0.722110

Epoch: 72
Loss: 0.27217219344207216
RMSE train: 0.408070	val: 0.956543	test: 0.879474
MAE train: 0.322551	val: 0.763278	test: 0.706615

Epoch: 73
Loss: 0.26822537396635326
RMSE train: 0.386211	val: 0.983999	test: 0.892772
MAE train: 0.303414	val: 0.774933	test: 0.721426

Epoch: 74
Loss: 0.2611174668584551
RMSE train: 0.404810	val: 0.958299	test: 0.883667
MAE train: 0.322166	val: 0.761769	test: 0.711770

Epoch: 75
Loss: 0.29462158999272753
RMSE train: 0.427174	val: 0.990049	test: 0.885329
MAE train: 0.340060	val: 0.782673	test: 0.717433

Epoch: 76
Loss: 0.2800095911536898
RMSE train: 0.429804	val: 1.011928	test: 0.909753
MAE train: 0.340211	val: 0.801243	test: 0.734975

Epoch: 77
Loss: 0.26794502458402086
RMSE train: 0.403011	val: 0.979189	test: 0.899483
MAE train: 0.319921	val: 0.778035	test: 0.729111

Epoch: 78
Loss: 0.25640097366912024
RMSE train: 0.397728	val: 0.968646	test: 0.898474
MAE train: 0.313315	val: 0.773326	test: 0.724368

Epoch: 79
Loss: 0.2327873972909791
RMSE train: 0.383081	val: 0.967671	test: 0.881005
MAE train: 0.303032	val: 0.766899	test: 0.715557

Epoch: 80
Loss: 0.23308215822492326
RMSE train: 0.398242	val: 0.963674	test: 0.866564
MAE train: 0.316281	val: 0.762237	test: 0.696820

Epoch: 81
Loss: 0.2339581400156021
RMSE train: 0.388217	val: 0.975746	test: 0.882476
MAE train: 0.309369	val: 0.766863	test: 0.716694

Epoch: 82
Loss: 0.23281077508415496
RMSE train: 0.392013	val: 0.978615	test: 0.885736
MAE train: 0.308264	val: 0.801201	test: 0.771023

Epoch: 83
Loss: 0.2413485614316804
RMSE train: 0.349037	val: 0.973024	test: 0.904569
MAE train: 0.275510	val: 0.760436	test: 0.714756

Epoch: 84
Loss: 0.21591381409338542
RMSE train: 0.511353	val: 1.105804	test: 1.036334
MAE train: 0.422577	val: 0.863508	test: 0.821292

Epoch: 85
Loss: 0.2143454392041479
RMSE train: 0.355143	val: 0.967255	test: 0.902188
MAE train: 0.279950	val: 0.756417	test: 0.719574

Epoch: 86
Loss: 0.21400354376861028
RMSE train: 0.377588	val: 1.021861	test: 0.952131
MAE train: 0.301249	val: 0.795659	test: 0.755692

Epoch: 87
Loss: 0.21856779498713358
RMSE train: 0.378969	val: 1.021534	test: 0.949798
MAE train: 0.301649	val: 0.799211	test: 0.751167

Epoch: 88
Loss: 0.2327162687267576
RMSE train: 0.354461	val: 1.018603	test: 0.950823
MAE train: 0.278685	val: 0.783900	test: 0.756011

Epoch: 89
Loss: 0.22899714750902994
RMSE train: 0.381270	val: 1.001070	test: 0.929948
MAE train: 0.300780	val: 0.782554	test: 0.738126

Epoch: 90
Loss: 0.21715408457177027
RMSE train: 0.396772	val: 1.031210	test: 0.965951
MAE train: 0.310994	val: 0.801612	test: 0.770467

Epoch: 91
Loss: 0.21033640844481333
RMSE train: 0.464260	val: 1.012799	test: 0.937709
MAE train: 0.366323	val: 0.794249	test: 0.740075

Epoch: 92
Loss: 0.2164044263107436
RMSE train: 0.332920	val: 0.990797	test: 0.920827
MAE train: 0.262029	val: 0.763762	test: 0.731425

Epoch: 93
Loss: 0.21326484850474767
RMSE train: 0.330240	val: 0.993567	test: 0.932163
MAE train: 0.258548	val: 0.766582	test: 0.739166

Epoch: 94
Loss: 0.21305201521941594
RMSE train: 0.414950	val: 1.009172	test: 0.943487
MAE train: 0.329874	val: 0.787670	test: 0.747443

Epoch: 95
Loss: 0.221453563443252
RMSE train: 0.346935	val: 1.025807	test: 0.953786
MAE train: 0.271928	val: 0.797093	test: 0.755780

Epoch: 96
Loss: 0.19488417676516942
RMSE train: 0.338726	val: 0.965430	test: 0.905493
MAE train: 0.266834	val: 0.757367	test: 0.725388

Epoch: 97
Loss: 0.19688176150832856
RMSE train: 0.343270	val: 1.023006	test: 0.959456
MAE train: 0.270027	val: 0.795759	test: 0.761489

Epoch: 98
Loss: 0.18151285073586873
RMSE train: 0.298767	val: 1.005216	test: 0.933492
MAE train: 0.234513	val: 0.778984	test: 0.740829

Epoch: 99
Loss: 0.19138690616403306
RMSE train: 0.397656	val: 1.006343	test: 0.938169
MAE train: 0.321672	val: 0.781452	test: 0.738704

Epoch: 100
Loss: 0.18916862990174974
RMSE train: 0.343060	val: 0.962541	test: 0.880948
MAE train: 0.271925	val: 0.761086	test: 0.693543

Epoch: 101
Loss: 0.18515393350805556
RMSE train: 0.356672	val: 1.014783	test: 0.941122
MAE train: 0.282312	val: 0.786890	test: 0.742267

Epoch: 102
Loss: 0.1756197756954602
RMSE train: 0.315946	val: 0.997796	test: 0.939581
MAE train: 0.249169	val: 0.775525	test: 0.743783

Epoch: 103
Loss: 0.196685152394431
RMSE train: 0.425819	val: 1.092950	test: 1.029278
MAE train: 0.335255	val: 0.845361	test: 0.813830

Epoch: 104
Loss: 0.18226772121020726
RMSE train: 0.416938	val: 1.066281	test: 1.003788
MAE train: 0.338173	val: 0.821844	test: 0.798085

Epoch: 105
Loss: 0.17648412712982722
RMSE train: 0.317269	val: 0.979706	test: 0.913570
MAE train: 0.251855	val: 0.761550	test: 0.725896

Epoch: 106
Loss: 0.18054288093532836
RMSE train: 0.369540	val: 0.986890	test: 0.912676
MAE train: 0.292850	val: 0.771160	test: 0.727764

Epoch: 107
Loss: 0.1729655819279807
RMSE train: 0.290844	val: 0.971981	test: 0.905670
MAE train: 0.228915	val: 0.756930	test: 0.715053

Epoch: 108
Loss: 0.1713187513606889
RMSE train: 0.309408	val: 1.030272	test: 0.971504
MAE train: 0.243573	val: 0.792462	test: 0.775553

Epoch: 109
Loss: 0.1693368468965803
RMSE train: 0.342973	val: 0.976980	test: 0.897077
MAE train: 0.270295	val: 0.766356	test: 0.710553

Epoch: 110
Loss: 0.168165993477617
RMSE train: 0.419566	val: 1.048194	test: 0.962334
MAE train: 0.335747	val: 0.814628	test: 0.756420

Epoch: 111
Loss: 0.1725081524678639
RMSE train: 0.421915	val: 1.047216	test: 0.963520
MAE train: 0.340554	val: 0.810312	test: 0.759038

Epoch: 112
Loss: 0.16273074064935958
RMSE train: 0.281201	val: 0.984071	test: 0.925950
MAE train: 0.220981	val: 0.763199	test: 0.737597

Epoch: 113
Loss: 0.17230801710060664
RMSE train: 0.314970	val: 0.982041	test: 0.923084
MAE train: 0.247226	val: 0.767414	test: 0.731185

Epoch: 114
Loss: 0.17921641149691173
RMSE train: 0.356618	val: 0.977609	test: 0.907466
MAE train: 0.281163	val: 0.771505	test: 0.723429

Epoch: 115
Loss: 0.167750783264637
RMSE train: 0.346426	val: 1.024718	test: 0.942514
MAE train: 0.276163	val: 0.799989	test: 0.741698

Epoch: 116
Loss: 0.1582873554102012
RMSE train: 0.328992	val: 1.029783	test: 0.989520
MAE train: 0.259027	val: 0.796901	test: 0.782506

Epoch: 117
Loss: 0.15823247549789293
RMSE train: 0.301838	val: 0.980469	test: 0.919593
MAE train: 0.236935	val: 0.764148	test: 0.726396

Epoch: 118
Loss: 0.15087613410183362
RMSE train: 0.379478	val: 1.022416	test: 0.940369
MAE train: 0.305089	val: 0.795310	test: 0.740989

Epoch: 119
Loss: 0.15133107187492506
RMSE train: 0.341289	val: 1.014538	test: 0.947336
MAE train: 0.268495	val: 0.788254	test: 0.749762

Epoch: 120
Loss: 0.15258258794035232
RMSE train: 0.281948	val: 0.984281	test: 0.933452
MAE train: 0.220823	val: 0.761696	test: 0.745484

Epoch: 121
Loss: 0.17333903163671494
RMSE train: 0.302278	val: 1.003876	test: 0.949222
MAE train: 0.237928	val: 0.780472	test: 0.753936

Early stopping
Best (RMSE):	 train: 0.436108	val: 0.951877	test: 0.900499
Best (MAE):	 train: 0.345356	val: 0.746655	test: 0.712847

MAE train: 0.403930	val: 0.753549	test: 0.696423

Epoch: 83
Loss: 0.2687981277704239
RMSE train: 0.499737	val: 0.983968	test: 0.902169
MAE train: 0.398717	val: 0.780037	test: 0.704527

Epoch: 84
Loss: 0.25574960985354017
RMSE train: 0.492309	val: 0.954337	test: 0.881657
MAE train: 0.391445	val: 0.758046	test: 0.698895

Epoch: 85
Loss: 0.2358978624854769
RMSE train: 0.385811	val: 0.866282	test: 0.797217
MAE train: 0.301856	val: 0.678688	test: 0.637424

Epoch: 86
Loss: 0.23578152805566788
RMSE train: 0.491228	val: 0.931757	test: 0.844678
MAE train: 0.388509	val: 0.733862	test: 0.660041

Epoch: 87
Loss: 0.22499518628631318
RMSE train: 0.426399	val: 0.921647	test: 0.841421
MAE train: 0.336779	val: 0.727220	test: 0.664089

Epoch: 88
Loss: 0.24151824840477534
RMSE train: 0.472168	val: 0.975960	test: 0.896495
MAE train: 0.373635	val: 0.764017	test: 0.710685

Epoch: 89
Loss: 0.2488196896655219
RMSE train: 0.435599	val: 0.967955	test: 0.890578
MAE train: 0.343495	val: 0.764043	test: 0.695750

Epoch: 90
Loss: 0.23333936078207834
RMSE train: 0.406128	val: 0.901429	test: 0.818521
MAE train: 0.319253	val: 0.712847	test: 0.649036

Epoch: 91
Loss: 0.238307495202337
RMSE train: 0.554721	val: 0.958970	test: 0.874697
MAE train: 0.444791	val: 0.758240	test: 0.687451

Epoch: 92
Loss: 0.23372693146978105
RMSE train: 0.391057	val: 0.909806	test: 0.840073
MAE train: 0.309382	val: 0.716003	test: 0.659982

Epoch: 93
Loss: 0.2186075331909316
RMSE train: 0.450609	val: 0.951726	test: 0.878613
MAE train: 0.360773	val: 0.754512	test: 0.694624

Epoch: 94
Loss: 0.211836399776595
RMSE train: 0.467439	val: 0.962322	test: 0.880588
MAE train: 0.375134	val: 0.762026	test: 0.696870

Epoch: 95
Loss: 0.21683131903409958
RMSE train: 0.383080	val: 0.920899	test: 0.835193
MAE train: 0.302635	val: 0.727339	test: 0.658109

Epoch: 96
Loss: 0.20608518485512053
RMSE train: 0.432917	val: 0.952784	test: 0.877293
MAE train: 0.341284	val: 0.749839	test: 0.696733

Epoch: 97
Loss: 0.21771132520266942
RMSE train: 0.465678	val: 0.926153	test: 0.837897
MAE train: 0.369497	val: 0.732162	test: 0.664778

Epoch: 98
Loss: 0.19440471593822753
RMSE train: 0.388851	val: 0.953917	test: 0.877633
MAE train: 0.309184	val: 0.752677	test: 0.694240

Epoch: 99
Loss: 0.20344094612768718
RMSE train: 0.404654	val: 0.912597	test: 0.828462
MAE train: 0.321854	val: 0.722702	test: 0.650061

Epoch: 100
Loss: 0.19777603979621614
RMSE train: 0.373998	val: 0.906548	test: 0.821011
MAE train: 0.293721	val: 0.717358	test: 0.645667

Epoch: 101
Loss: 0.20885077544621058
RMSE train: 0.477762	val: 0.961827	test: 0.875058
MAE train: 0.379948	val: 0.763956	test: 0.687770

Epoch: 102
Loss: 0.1916042502437319
RMSE train: 0.403173	val: 0.927378	test: 0.832782
MAE train: 0.316827	val: 0.733791	test: 0.665717

Epoch: 103
Loss: 0.2000689676829747
RMSE train: 0.376226	val: 0.915204	test: 0.835579
MAE train: 0.296979	val: 0.721930	test: 0.668424

Epoch: 104
Loss: 0.18671713130814688
RMSE train: 0.377615	val: 0.897081	test: 0.806654
MAE train: 0.296011	val: 0.711078	test: 0.642692

Epoch: 105
Loss: 0.19543264912707464
RMSE train: 0.360177	val: 0.889577	test: 0.813419
MAE train: 0.285889	val: 0.700117	test: 0.646670

Epoch: 106
Loss: 0.1934186252100127
RMSE train: 0.445290	val: 0.946495	test: 0.850940
MAE train: 0.356118	val: 0.753807	test: 0.675578

Epoch: 107
Loss: 0.18602034556014196
RMSE train: 0.353815	val: 0.886375	test: 0.795767
MAE train: 0.279421	val: 0.695870	test: 0.641378

Epoch: 108
Loss: 0.1920563546674592
RMSE train: 0.377525	val: 0.900346	test: 0.818764
MAE train: 0.299610	val: 0.709043	test: 0.655546

Epoch: 109
Loss: 0.18457349283354624
RMSE train: 0.338063	val: 0.878376	test: 0.793960
MAE train: 0.266192	val: 0.696099	test: 0.633114

Epoch: 110
Loss: 0.18614515555756433
RMSE train: 0.388160	val: 0.940150	test: 0.841165
MAE train: 0.308081	val: 0.747674	test: 0.669927

Epoch: 111
Loss: 0.2012661812560899
RMSE train: 0.447776	val: 0.952801	test: 0.849710
MAE train: 0.356663	val: 0.754237	test: 0.672281

Epoch: 112
Loss: 0.18855202623776027
RMSE train: 0.359609	val: 0.887261	test: 0.802875
MAE train: 0.283920	val: 0.701622	test: 0.642517

Epoch: 113
Loss: 0.19455346252237046
RMSE train: 0.362346	val: 0.869506	test: 0.799270
MAE train: 0.290139	val: 0.684199	test: 0.647261

Epoch: 114
Loss: 0.2021866270474025
RMSE train: 0.395393	val: 0.894385	test: 0.819848
MAE train: 0.315881	val: 0.706923	test: 0.658165

Epoch: 115
Loss: 0.17731614730187825
RMSE train: 0.356692	val: 0.897564	test: 0.811792
MAE train: 0.282573	val: 0.706851	test: 0.641186

Epoch: 116
Loss: 0.17224817084414618
RMSE train: 0.375364	val: 0.921874	test: 0.845081
MAE train: 0.297996	val: 0.723151	test: 0.672692

Epoch: 117
Loss: 0.16792647221258708
RMSE train: 0.375001	val: 0.919416	test: 0.825398
MAE train: 0.296820	val: 0.725648	test: 0.656405

Epoch: 118
Loss: 0.17281859900270188
RMSE train: 0.425195	val: 0.949283	test: 0.854020
MAE train: 0.342277	val: 0.750908	test: 0.674341

Epoch: 119
Loss: 0.1755199623959405
RMSE train: 0.397735	val: 0.942994	test: 0.850008
MAE train: 0.316916	val: 0.746343	test: 0.669987

Epoch: 120
Loss: 0.17247089317866734
RMSE train: 0.370423	val: 0.953577	test: 0.864386
MAE train: 0.291898	val: 0.749020	test: 0.685539

Epoch: 121
Loss: 0.18190741006817138
RMSE train: 0.336030	val: 0.908423	test: 0.821080
MAE train: 0.264061	val: 0.715069	test: 0.649549

Early stopping
Best (RMSE):	 train: 0.475632	val: 0.859144	test: 0.814902
Best (MAE):	 train: 0.375812	val: 0.675439	test: 0.645762

MAE train: 0.334813	val: 0.852584	test: 0.812956

Epoch: 83
Loss: 0.21229377069643565
RMSE train: 0.377618	val: 1.022100	test: 0.968984
MAE train: 0.299656	val: 0.824137	test: 0.789122

Epoch: 84
Loss: 0.20195396457399642
RMSE train: 0.381790	val: 1.035954	test: 0.987574
MAE train: 0.303103	val: 0.836863	test: 0.804468

Epoch: 85
Loss: 0.1998860463500023
RMSE train: 0.370332	val: 1.034162	test: 0.984058
MAE train: 0.290973	val: 0.835762	test: 0.801938

Epoch: 86
Loss: 0.20227088566337312
RMSE train: 0.354299	val: 1.074739	test: 1.021669
MAE train: 0.280543	val: 0.862724	test: 0.831777

Epoch: 87
Loss: 0.19428474668945586
RMSE train: 0.408354	val: 1.086443	test: 1.022672
MAE train: 0.322376	val: 0.871819	test: 0.828013

Epoch: 88
Loss: 0.1992866631065096
RMSE train: 0.367300	val: 1.149447	test: 1.095554
MAE train: 0.288706	val: 0.928960	test: 0.890438

Epoch: 89
Loss: 0.19376638212374278
RMSE train: 0.323508	val: 1.107590	test: 1.074619
MAE train: 0.256183	val: 0.903795	test: 0.871218

Epoch: 90
Loss: 0.1885464063712529
RMSE train: 0.407201	val: 1.050070	test: 0.981971
MAE train: 0.321307	val: 0.837774	test: 0.796847

Epoch: 91
Loss: 0.1856843520488058
RMSE train: 0.382278	val: 1.054790	test: 0.994735
MAE train: 0.302386	val: 0.841834	test: 0.810781

Epoch: 92
Loss: 0.1829682184117181
RMSE train: 0.366273	val: 1.093970	test: 1.039829
MAE train: 0.287116	val: 0.879883	test: 0.840793

Epoch: 93
Loss: 0.18021294474601746
RMSE train: 0.327274	val: 1.069072	test: 1.024776
MAE train: 0.259060	val: 0.871385	test: 0.829789

Epoch: 94
Loss: 0.16998810533966338
RMSE train: 0.363858	val: 1.077540	test: 1.026975
MAE train: 0.285161	val: 0.862242	test: 0.830431

Epoch: 95
Loss: 0.17736087313720159
RMSE train: 0.344755	val: 0.998699	test: 0.940569
MAE train: 0.270076	val: 0.800526	test: 0.762987

Epoch: 96
Loss: 0.1600602407540594
RMSE train: 0.315481	val: 1.059137	test: 1.001553
MAE train: 0.248288	val: 0.842603	test: 0.807359

Epoch: 97
Loss: 0.1751019401209695
RMSE train: 0.363398	val: 1.026026	test: 0.962474
MAE train: 0.286201	val: 0.830808	test: 0.782778

Epoch: 98
Loss: 0.17601780274084636
RMSE train: 0.330187	val: 1.051860	test: 0.999070
MAE train: 0.260014	val: 0.849396	test: 0.816495

Epoch: 99
Loss: 0.1577168410377843
RMSE train: 0.340271	val: 1.010661	test: 0.957153
MAE train: 0.269030	val: 0.822110	test: 0.779381

Epoch: 100
Loss: 0.16106512716838292
RMSE train: 0.306457	val: 1.038006	test: 0.987833
MAE train: 0.242245	val: 0.840414	test: 0.806191

Epoch: 101
Loss: 0.16052734958274023
RMSE train: 0.377170	val: 1.049265	test: 0.987436
MAE train: 0.295467	val: 0.852104	test: 0.803678

Epoch: 102
Loss: 0.17127263865300588
RMSE train: 0.335357	val: 1.073511	test: 1.030112
MAE train: 0.263826	val: 0.869174	test: 0.837416

Epoch: 103
Loss: 0.15936954958098276
RMSE train: 0.359554	val: 1.035865	test: 0.991722
MAE train: 0.278758	val: 0.847266	test: 0.809006

Epoch: 104
Loss: 0.17573938412325724
RMSE train: 0.417440	val: 1.019053	test: 0.957163
MAE train: 0.325194	val: 0.826170	test: 0.780066

Epoch: 105
Loss: 0.16253324385200227
RMSE train: 0.355810	val: 1.052838	test: 0.992553
MAE train: 0.276998	val: 0.836910	test: 0.800762

Epoch: 106
Loss: 0.16228180804422923
RMSE train: 0.367139	val: 1.070515	test: 1.002752
MAE train: 0.286333	val: 0.855494	test: 0.810238

Epoch: 107
Loss: 0.1559002473950386
RMSE train: 0.376104	val: 1.015455	test: 0.940738
MAE train: 0.293554	val: 0.814695	test: 0.761029

Epoch: 108
Loss: 0.16895798593759537
RMSE train: 0.359088	val: 1.020306	test: 0.955742
MAE train: 0.281039	val: 0.817861	test: 0.774136

Epoch: 109
Loss: 0.15309222361871175
RMSE train: 0.322093	val: 1.026031	test: 0.975201
MAE train: 0.251986	val: 0.833391	test: 0.795612

Epoch: 110
Loss: 0.1459965461066791
RMSE train: 0.363325	val: 1.006116	test: 0.938261
MAE train: 0.281994	val: 0.803847	test: 0.759409

Epoch: 111
Loss: 0.14140629608716285
RMSE train: 0.327290	val: 1.018184	test: 0.958392
MAE train: 0.252782	val: 0.818566	test: 0.776812

Epoch: 112
Loss: 0.14343416477952683
RMSE train: 0.332114	val: 1.047049	test: 0.979998
MAE train: 0.260208	val: 0.835637	test: 0.789868

Epoch: 113
Loss: 0.1425103566476277
RMSE train: 0.340446	val: 1.020212	test: 0.958266
MAE train: 0.267682	val: 0.825292	test: 0.777604

Epoch: 114
Loss: 0.1400584221950599
RMSE train: 0.338526	val: 1.065958	test: 0.988785
MAE train: 0.261736	val: 0.843436	test: 0.794869

Epoch: 115
Loss: 0.1383566835096904
RMSE train: 0.323363	val: 1.027852	test: 0.978797
MAE train: 0.256664	val: 0.829130	test: 0.793760

Epoch: 116
Loss: 0.13578359835914203
RMSE train: 0.337505	val: 1.049069	test: 1.001043
MAE train: 0.265647	val: 0.855749	test: 0.809601

Epoch: 117
Loss: 0.15093580207654408
RMSE train: 0.315071	val: 1.073991	test: 1.013544
MAE train: 0.249439	val: 0.847410	test: 0.816739

Epoch: 118
Loss: 0.1382497740643365
RMSE train: 0.325702	val: 1.021634	test: 0.979858
MAE train: 0.264284	val: 0.836284	test: 0.794801

Epoch: 119
Loss: 0.1412982147719179
RMSE train: 0.320874	val: 1.049863	test: 0.982810
MAE train: 0.251659	val: 0.842087	test: 0.793546

Epoch: 120
Loss: 0.13851068967155047
RMSE train: 0.357487	val: 1.037590	test: 0.966518
MAE train: 0.281004	val: 0.825701	test: 0.775605

Epoch: 121
Loss: 0.14067530312708446
RMSE train: 0.318077	val: 1.014143	test: 0.972395
MAE train: 0.248228	val: 0.822780	test: 0.786911

Early stopping
Best (RMSE):	 train: 0.411121	val: 0.997233	test: 0.937142
Best (MAE):	 train: 0.325123	val: 0.801092	test: 0.759720

MAE train: 0.408696	val: 0.580798	test: 0.585107

Epoch: 84
Loss: 0.31151055863925387
RMSE train: 0.504014	val: 0.766711	test: 0.749990
MAE train: 0.388133	val: 0.577681	test: 0.588283

Epoch: 85
Loss: 0.3088967502117157
RMSE train: 0.513825	val: 0.773072	test: 0.764025
MAE train: 0.398574	val: 0.580591	test: 0.600117

Epoch: 86
Loss: 0.3166840225458145
RMSE train: 0.486408	val: 0.745346	test: 0.756002
MAE train: 0.377551	val: 0.566207	test: 0.591186

Epoch: 87
Loss: 0.3161220593111856
RMSE train: 0.497575	val: 0.747880	test: 0.754604
MAE train: 0.382311	val: 0.562438	test: 0.591815

Epoch: 88
Loss: 0.3156512188059943
RMSE train: 0.491907	val: 0.758040	test: 0.756242
MAE train: 0.379236	val: 0.566308	test: 0.593140

Epoch: 89
Loss: 0.30647638865879606
RMSE train: 0.485558	val: 0.761236	test: 0.739397
MAE train: 0.372939	val: 0.573551	test: 0.575735

Epoch: 90
Loss: 0.311472760779517
RMSE train: 0.523110	val: 0.782186	test: 0.766998
MAE train: 0.406177	val: 0.581727	test: 0.602630

Epoch: 91
Loss: 0.31980076857975553
RMSE train: 0.540492	val: 0.789173	test: 0.752245
MAE train: 0.417027	val: 0.590978	test: 0.589224

Epoch: 92
Loss: 0.331425384751388
RMSE train: 0.495975	val: 0.738480	test: 0.749122
MAE train: 0.383310	val: 0.555460	test: 0.586573

Epoch: 93
Loss: 0.30201331845351626
RMSE train: 0.547441	val: 0.779620	test: 0.781343
MAE train: 0.424018	val: 0.589500	test: 0.614816

Epoch: 94
Loss: 0.3321214424712317
RMSE train: 0.498984	val: 0.763508	test: 0.750826
MAE train: 0.385698	val: 0.572257	test: 0.578414

Epoch: 95
Loss: 0.3100979221718652
RMSE train: 0.518284	val: 0.795290	test: 0.771024
MAE train: 0.399842	val: 0.592173	test: 0.594026

Epoch: 96
Loss: 0.30492831979479107
RMSE train: 0.485999	val: 0.755099	test: 0.744376
MAE train: 0.374839	val: 0.565832	test: 0.574603

Epoch: 97
Loss: 0.3044542074203491
RMSE train: 0.490620	val: 0.771224	test: 0.745472
MAE train: 0.380489	val: 0.571209	test: 0.575175

Epoch: 98
Loss: 0.29631850123405457
RMSE train: 0.495464	val: 0.773324	test: 0.748231
MAE train: 0.382083	val: 0.578401	test: 0.583306

Epoch: 99
Loss: 0.2898905266608511
RMSE train: 0.461973	val: 0.748035	test: 0.750417
MAE train: 0.356413	val: 0.561351	test: 0.581528

Epoch: 100
Loss: 0.2980571634003094
RMSE train: 0.479294	val: 0.753711	test: 0.732318
MAE train: 0.371626	val: 0.562809	test: 0.568839

Epoch: 101
Loss: 0.2925082583512579
RMSE train: 0.519131	val: 0.809347	test: 0.774885
MAE train: 0.402388	val: 0.606781	test: 0.603331

Epoch: 102
Loss: 0.2861632651516369
RMSE train: 0.466764	val: 0.745379	test: 0.761915
MAE train: 0.358101	val: 0.561589	test: 0.585369

Epoch: 103
Loss: 0.2898689316851752
RMSE train: 0.472781	val: 0.754480	test: 0.735759
MAE train: 0.364809	val: 0.565733	test: 0.576206

Epoch: 104
Loss: 0.3025906469140734
RMSE train: 0.477022	val: 0.751658	test: 0.737984
MAE train: 0.369598	val: 0.562688	test: 0.566938

Epoch: 105
Loss: 0.2832449270146234
RMSE train: 0.470860	val: 0.770376	test: 0.744430
MAE train: 0.366345	val: 0.570998	test: 0.580714

Epoch: 106
Loss: 0.2791594820363181
RMSE train: 0.467442	val: 0.761071	test: 0.734237
MAE train: 0.362178	val: 0.571301	test: 0.573040

Epoch: 107
Loss: 0.29681683012417387
RMSE train: 0.490738	val: 0.787844	test: 0.748989
MAE train: 0.381571	val: 0.589661	test: 0.582300

Epoch: 108
Loss: 0.2712045429008348
RMSE train: 0.467964	val: 0.762767	test: 0.752948
MAE train: 0.359882	val: 0.567439	test: 0.579736

Epoch: 109
Loss: 0.2793766441089766
RMSE train: 0.470974	val: 0.767473	test: 0.754111
MAE train: 0.363378	val: 0.568583	test: 0.579877

Epoch: 110
Loss: 0.2645194487912314
RMSE train: 0.453962	val: 0.754457	test: 0.739343
MAE train: 0.349239	val: 0.562990	test: 0.573951

Epoch: 111
Loss: 0.2757028043270111
RMSE train: 0.463504	val: 0.760435	test: 0.739032
MAE train: 0.358397	val: 0.571753	test: 0.574798

Epoch: 112
Loss: 0.287331291607448
RMSE train: 0.464281	val: 0.756486	test: 0.730283
MAE train: 0.360217	val: 0.569047	test: 0.575331

Epoch: 113
Loss: 0.30534955859184265
RMSE train: 0.480466	val: 0.750934	test: 0.742907
MAE train: 0.371591	val: 0.567508	test: 0.580764

Epoch: 114
Loss: 0.2909182033368519
RMSE train: 0.466067	val: 0.747174	test: 0.743221
MAE train: 0.360941	val: 0.562360	test: 0.583852

Epoch: 115
Loss: 0.2850715166756085
RMSE train: 0.476341	val: 0.760720	test: 0.747406
MAE train: 0.369826	val: 0.573501	test: 0.580129

Epoch: 116
Loss: 0.272480557007449
RMSE train: 0.464813	val: 0.767085	test: 0.742998
MAE train: 0.359232	val: 0.577771	test: 0.579712

Epoch: 117
Loss: 0.2870651047144617
RMSE train: 0.473506	val: 0.758252	test: 0.739633
MAE train: 0.365409	val: 0.563875	test: 0.576668

Epoch: 118
Loss: 0.24954175310475485
RMSE train: 0.496668	val: 0.790804	test: 0.768673
MAE train: 0.387638	val: 0.588565	test: 0.597434

Epoch: 119
Loss: 0.2657896067414965
RMSE train: 0.464997	val: 0.766400	test: 0.753458
MAE train: 0.359942	val: 0.569976	test: 0.583709

Epoch: 120
Loss: 0.29320136244807926
RMSE train: 0.470415	val: 0.756939	test: 0.753214
MAE train: 0.363646	val: 0.566992	test: 0.587080

Epoch: 121
Loss: 0.2898011250155313
RMSE train: 0.478577	val: 0.775724	test: 0.747808
MAE train: 0.372805	val: 0.589843	test: 0.579126

Epoch: 122
Loss: 0.27662382061992374
RMSE train: 0.442713	val: 0.747467	test: 0.747355
MAE train: 0.341478	val: 0.561277	test: 0.571500

Epoch: 123
Loss: 0.27251788867371424
RMSE train: 0.452027	val: 0.743283	test: 0.733264
MAE train: 0.349247	val: 0.555179	test: 0.567042

Epoch: 124
Loss: 0.2709693546806063
RMSE train: 0.479465	val: 0.755214	test: 0.746136
MAE train: 0.374840	val: 0.570855	test: 0.583687

Epoch: 125
Loss: 0.2726537872638021
RMSE train: 0.455543	val: 0.764146	test: 0.740860
MAE train: 0.351563	val: 0.573562	test: 0.572496

Epoch: 126
Loss: 0.2637779340147972
RMSE train: 0.446892	val: 0.751631	test: 0.736027
MAE train: 0.346451	val: 0.566752	test: 0.563687

Epoch: 127
Loss: 0.24833089858293533
RMSE train: 0.464020	val: 0.769818	test: 0.736694
MAE train: 0.359893	val: 0.571070	test: 0.564641

Early stopping
Best (RMSE):	 train: 0.495975	val: 0.738480	test: 0.749122
Best (MAE):	 train: 0.383310	val: 0.555460	test: 0.586573

MAE train: 0.311682	val: 0.764777	test: 0.715662

Epoch: 83
Loss: 0.23762525192328862
RMSE train: 0.386087	val: 0.975932	test: 0.883086
MAE train: 0.307630	val: 0.772436	test: 0.713041

Epoch: 84
Loss: 0.2264501228928566
RMSE train: 0.345052	val: 0.959016	test: 0.876042
MAE train: 0.271322	val: 0.757196	test: 0.705165

Epoch: 85
Loss: 0.23326427063771657
RMSE train: 0.392177	val: 0.983067	test: 0.889652
MAE train: 0.308907	val: 0.778354	test: 0.719342

Epoch: 86
Loss: 0.22130020707845688
RMSE train: 0.353073	val: 0.965239	test: 0.894140
MAE train: 0.278777	val: 0.756809	test: 0.717002

Epoch: 87
Loss: 0.2165705912879535
RMSE train: 0.351493	val: 0.969054	test: 0.894541
MAE train: 0.277136	val: 0.764753	test: 0.725097

Epoch: 88
Loss: 0.21007207993950164
RMSE train: 0.412092	val: 0.985216	test: 0.895678
MAE train: 0.330435	val: 0.776351	test: 0.720567

Epoch: 89
Loss: 0.2077443919011525
RMSE train: 0.354164	val: 0.959582	test: 0.889469
MAE train: 0.278390	val: 0.756046	test: 0.714844

Epoch: 90
Loss: 0.20579012909105845
RMSE train: 0.359080	val: 0.988002	test: 0.901838
MAE train: 0.282723	val: 0.770012	test: 0.727738

Epoch: 91
Loss: 0.20551553581442153
RMSE train: 0.390744	val: 0.964170	test: 0.871784
MAE train: 0.309731	val: 0.760878	test: 0.707027

Epoch: 92
Loss: 0.199689687362739
RMSE train: 0.325615	val: 0.945416	test: 0.881907
MAE train: 0.253988	val: 0.744464	test: 0.711977

Epoch: 93
Loss: 0.19801545994622366
RMSE train: 0.488931	val: 0.988332	test: 0.879558
MAE train: 0.395820	val: 0.776361	test: 0.714059

Epoch: 94
Loss: 0.1966210656932422
RMSE train: 0.393180	val: 0.972748	test: 0.881474
MAE train: 0.312427	val: 0.764719	test: 0.712873

Epoch: 95
Loss: 0.1837447945560728
RMSE train: 0.350131	val: 0.967669	test: 0.880422
MAE train: 0.276323	val: 0.762527	test: 0.715514

Epoch: 96
Loss: 0.18935812690428325
RMSE train: 0.367946	val: 0.973712	test: 0.891993
MAE train: 0.288666	val: 0.770340	test: 0.724112

Epoch: 97
Loss: 0.19366654115063803
RMSE train: 0.307818	val: 0.972535	test: 0.891234
MAE train: 0.243243	val: 0.768540	test: 0.720507

Epoch: 98
Loss: 0.18190142938068934
RMSE train: 0.323657	val: 0.977683	test: 0.885427
MAE train: 0.254835	val: 0.773688	test: 0.712715

Epoch: 99
Loss: 0.18144479819706508
RMSE train: 0.345519	val: 0.972300	test: 0.891968
MAE train: 0.273351	val: 0.777428	test: 0.717059

Epoch: 100
Loss: 0.19735046263251985
RMSE train: 0.337958	val: 0.963879	test: 0.879326
MAE train: 0.266146	val: 0.761752	test: 0.705174

Epoch: 101
Loss: 0.18976994284561702
RMSE train: 0.348908	val: 0.981168	test: 0.886439
MAE train: 0.277177	val: 0.763625	test: 0.719400

Epoch: 102
Loss: 0.18850298651627131
RMSE train: 0.332168	val: 0.979623	test: 0.887919
MAE train: 0.261088	val: 0.770418	test: 0.714594

Epoch: 103
Loss: 0.175904495375497
RMSE train: 0.344362	val: 0.954552	test: 0.870571
MAE train: 0.270835	val: 0.759556	test: 0.704774

Epoch: 104
Loss: 0.16631949905838286
RMSE train: 0.366781	val: 0.988765	test: 0.895053
MAE train: 0.291458	val: 0.774864	test: 0.724981

Epoch: 105
Loss: 0.16742316952773503
RMSE train: 0.293510	val: 0.969011	test: 0.892004
MAE train: 0.230724	val: 0.758328	test: 0.722117

Epoch: 106
Loss: 0.1606984425868307
RMSE train: 0.334057	val: 0.978547	test: 0.894739
MAE train: 0.266180	val: 0.780685	test: 0.721664

Epoch: 107
Loss: 0.17396060803106853
RMSE train: 0.342198	val: 0.958965	test: 0.879657
MAE train: 0.270158	val: 0.762368	test: 0.709286

Epoch: 108
Loss: 0.15764230170420238
RMSE train: 0.284125	val: 0.962404	test: 0.878787
MAE train: 0.225031	val: 0.752245	test: 0.706800

Epoch: 109
Loss: 0.1603359335235187
RMSE train: 0.287041	val: 0.961942	test: 0.888903
MAE train: 0.226575	val: 0.756891	test: 0.719592

Epoch: 110
Loss: 0.157401371215071
RMSE train: 0.350279	val: 0.957737	test: 0.873506
MAE train: 0.277156	val: 0.758377	test: 0.710441

Epoch: 111
Loss: 0.15872533300093242
RMSE train: 0.292840	val: 0.966038	test: 0.881244
MAE train: 0.231947	val: 0.762798	test: 0.715040

Epoch: 112
Loss: 0.15545216149517468
RMSE train: 0.301548	val: 0.972297	test: 0.890081
MAE train: 0.236395	val: 0.771573	test: 0.718482

Epoch: 113
Loss: 0.14616123693329947
RMSE train: 0.338950	val: 0.980901	test: 0.899320
MAE train: 0.267038	val: 0.781148	test: 0.727011

Epoch: 114
Loss: 0.14448661037853786
RMSE train: 0.277069	val: 0.959113	test: 0.880332
MAE train: 0.216604	val: 0.757728	test: 0.707901

Epoch: 115
Loss: 0.15054957887956075
RMSE train: 0.311589	val: 0.958810	test: 0.880709
MAE train: 0.246335	val: 0.762210	test: 0.713952

Epoch: 116
Loss: 0.1457452124782971
RMSE train: 0.341173	val: 0.958033	test: 0.872000
MAE train: 0.269547	val: 0.757472	test: 0.710287

Epoch: 117
Loss: 0.14607206944908416
RMSE train: 0.304243	val: 0.954564	test: 0.867205
MAE train: 0.240137	val: 0.752824	test: 0.699595

Epoch: 118
Loss: 0.1494377308658191
RMSE train: 0.296498	val: 0.958471	test: 0.876736
MAE train: 0.230763	val: 0.760672	test: 0.708320

Epoch: 119
Loss: 0.14137677262936318
RMSE train: 0.294400	val: 0.947488	test: 0.869734
MAE train: 0.232779	val: 0.752132	test: 0.703134

Epoch: 120
Loss: 0.15519984545452253
RMSE train: 0.302571	val: 0.948209	test: 0.872602
MAE train: 0.237554	val: 0.751660	test: 0.706045

Epoch: 121
Loss: 0.1511393945131983
RMSE train: 0.266065	val: 0.959827	test: 0.880992
MAE train: 0.209475	val: 0.758244	test: 0.713902

Epoch: 122
Loss: 0.1626004076429776
RMSE train: 0.299540	val: 0.969871	test: 0.887903
MAE train: 0.236874	val: 0.767930	test: 0.721238

Epoch: 123
Loss: 0.14039287396839686
RMSE train: 0.280683	val: 0.959876	test: 0.885248
MAE train: 0.218248	val: 0.754869	test: 0.719068

Epoch: 124
Loss: 0.13564770030123846
RMSE train: 0.301390	val: 0.963634	test: 0.875871
MAE train: 0.238265	val: 0.754383	test: 0.710349

Epoch: 125
Loss: 0.13335200452378818
RMSE train: 0.277726	val: 0.959373	test: 0.874020
MAE train: 0.218586	val: 0.754645	test: 0.704745

Epoch: 126
Loss: 0.13398842247469084
RMSE train: 0.299379	val: 0.971468	test: 0.881910
MAE train: 0.236330	val: 0.764406	test: 0.717406

Epoch: 127
Loss: 0.12555449455976486
RMSE train: 0.293986	val: 0.953044	test: 0.874937
MAE train: 0.230287	val: 0.752387	test: 0.704724

Early stopping
Best (RMSE):	 train: 0.325615	val: 0.945416	test: 0.881907
Best (MAE):	 train: 0.253988	val: 0.744464	test: 0.711977

MAE train: 0.287612	val: 0.802992	test: 0.809330

Epoch: 83
Loss: 0.205693889941488
RMSE train: 0.434964	val: 0.990775	test: 0.953553
MAE train: 0.349788	val: 0.792706	test: 0.774621

Epoch: 84
Loss: 0.1949379656996046
RMSE train: 0.371943	val: 1.006733	test: 0.983189
MAE train: 0.296921	val: 0.800298	test: 0.791133

Epoch: 85
Loss: 0.1942239265356745
RMSE train: 0.313422	val: 1.012149	test: 0.969058
MAE train: 0.249411	val: 0.791115	test: 0.787521

Epoch: 86
Loss: 0.20350664002554758
RMSE train: 0.361786	val: 0.996068	test: 0.939700
MAE train: 0.290753	val: 0.784339	test: 0.764006

Epoch: 87
Loss: 0.19025576753275736
RMSE train: 0.424934	val: 1.016336	test: 0.945880
MAE train: 0.340176	val: 0.806245	test: 0.766132

Epoch: 88
Loss: 0.2117349835378783
RMSE train: 0.326154	val: 1.058407	test: 1.000443
MAE train: 0.260668	val: 0.816296	test: 0.804181

Epoch: 89
Loss: 0.21223900467157364
RMSE train: 0.406600	val: 1.107197	test: 1.045613
MAE train: 0.328279	val: 0.858719	test: 0.838395

Epoch: 90
Loss: 0.18457840170179093
RMSE train: 0.348131	val: 1.063822	test: 1.016635
MAE train: 0.276976	val: 0.832037	test: 0.813011

Epoch: 91
Loss: 0.19392744132450648
RMSE train: 0.341213	val: 1.056965	test: 1.019633
MAE train: 0.269856	val: 0.819451	test: 0.820390

Epoch: 92
Loss: 0.19432961727891648
RMSE train: 0.316233	val: 1.001011	test: 0.967581
MAE train: 0.251449	val: 0.794208	test: 0.783126

Epoch: 93
Loss: 0.180049866437912
RMSE train: 0.421137	val: 1.037351	test: 0.985308
MAE train: 0.333904	val: 0.812422	test: 0.795242

Epoch: 94
Loss: 0.1753025363598551
RMSE train: 0.352696	val: 1.012057	test: 0.969327
MAE train: 0.280685	val: 0.792504	test: 0.780852

Epoch: 95
Loss: 0.18322751032454626
RMSE train: 0.360363	val: 1.053750	test: 0.982564
MAE train: 0.286430	val: 0.822064	test: 0.790004

Epoch: 96
Loss: 0.17889285939080374
RMSE train: 0.323727	val: 1.024826	test: 0.965609
MAE train: 0.258688	val: 0.805381	test: 0.781915

Epoch: 97
Loss: 0.18110663230930055
RMSE train: 0.382121	val: 1.096129	test: 1.031372
MAE train: 0.299780	val: 0.843517	test: 0.825122

Epoch: 98
Loss: 0.1725821133170809
RMSE train: 0.376474	val: 1.032535	test: 0.966908
MAE train: 0.301590	val: 0.808068	test: 0.781976

Epoch: 99
Loss: 0.18113129373107636
RMSE train: 0.379212	val: 1.011305	test: 0.932757
MAE train: 0.303376	val: 0.795640	test: 0.756168

Epoch: 100
Loss: 0.17011129749672754
RMSE train: 0.348354	val: 1.003832	test: 0.961434
MAE train: 0.277103	val: 0.809372	test: 0.790273

Epoch: 101
Loss: 0.15982108350311006
RMSE train: 0.301381	val: 0.987945	test: 0.934454
MAE train: 0.238518	val: 0.782882	test: 0.759181

Epoch: 102
Loss: 0.1611427403986454
RMSE train: 0.306776	val: 1.028558	test: 0.956964
MAE train: 0.242543	val: 0.808607	test: 0.778536

Epoch: 103
Loss: 0.14957410363214357
RMSE train: 0.315810	val: 1.060908	test: 0.985405
MAE train: 0.249417	val: 0.825232	test: 0.794755

Epoch: 104
Loss: 0.14663905863251006
RMSE train: 0.367167	val: 1.018524	test: 0.943809
MAE train: 0.290055	val: 0.806239	test: 0.768290

Epoch: 105
Loss: 0.15988147258758545
RMSE train: 0.314714	val: 1.002881	test: 0.942319
MAE train: 0.250490	val: 0.795412	test: 0.768748

Epoch: 106
Loss: 0.16124571434089116
RMSE train: 0.371812	val: 1.013813	test: 0.949347
MAE train: 0.291581	val: 0.804026	test: 0.770859

Epoch: 107
Loss: 0.14866437124354498
RMSE train: 0.361310	val: 1.011076	test: 0.933171
MAE train: 0.285423	val: 0.798284	test: 0.754065

Epoch: 108
Loss: 0.15486755860703333
RMSE train: 0.310436	val: 1.000800	test: 0.952032
MAE train: 0.249871	val: 0.792995	test: 0.774277

Epoch: 109
Loss: 0.15380061630691802
RMSE train: 0.385403	val: 1.006658	test: 0.939880
MAE train: 0.303877	val: 0.802699	test: 0.763799

Epoch: 110
Loss: 0.14614108843462809
RMSE train: 0.350545	val: 1.039675	test: 0.963724
MAE train: 0.280120	val: 0.814386	test: 0.779050

Epoch: 111
Loss: 0.1528900840452739
RMSE train: 0.353307	val: 1.024089	test: 0.947102
MAE train: 0.283100	val: 0.805721	test: 0.765994

Epoch: 112
Loss: 0.14722424266593798
RMSE train: 0.333850	val: 1.035013	test: 0.971876
MAE train: 0.261855	val: 0.811905	test: 0.784510

Epoch: 113
Loss: 0.1505963701222624
RMSE train: 0.313365	val: 1.034104	test: 0.967886
MAE train: 0.246809	val: 0.812350	test: 0.781883

Epoch: 114
Loss: 0.1638208723493985
RMSE train: 0.360503	val: 1.058760	test: 0.992983
MAE train: 0.282905	val: 0.835945	test: 0.802688

Epoch: 115
Loss: 0.14548811529363906
RMSE train: 0.349964	val: 0.997800	test: 0.936882
MAE train: 0.278837	val: 0.802635	test: 0.766793

Epoch: 116
Loss: 0.14231786291514123
RMSE train: 0.360049	val: 1.020307	test: 0.957105
MAE train: 0.279001	val: 0.811465	test: 0.778362

Epoch: 117
Loss: 0.13955972396901675
RMSE train: 0.298508	val: 1.052725	test: 0.986807
MAE train: 0.232688	val: 0.826053	test: 0.797822

Epoch: 118
Loss: 0.1462993717619351
RMSE train: 0.354376	val: 1.095846	test: 1.036367
MAE train: 0.283188	val: 0.851561	test: 0.833319

Epoch: 119
Loss: 0.1372249759733677
RMSE train: 0.307883	val: 1.036588	test: 0.970623
MAE train: 0.240302	val: 0.816106	test: 0.785032

Epoch: 120
Loss: 0.132725611861263
RMSE train: 0.322641	val: 1.012756	test: 0.973934
MAE train: 0.262416	val: 0.816190	test: 0.788705

Epoch: 121
Loss: 0.14363565189497812
RMSE train: 0.297769	val: 1.027453	test: 0.952125
MAE train: 0.236100	val: 0.807989	test: 0.771206

Epoch: 122
Loss: 0.12978224296654975
RMSE train: 0.327880	val: 1.031133	test: 0.950124
MAE train: 0.257729	val: 0.818881	test: 0.772611

Epoch: 123
Loss: 0.13274161251527922
RMSE train: 0.320576	val: 1.025717	test: 0.962340
MAE train: 0.253816	val: 0.820251	test: 0.785041

Epoch: 124
Loss: 0.1301293559372425
RMSE train: 0.310716	val: 1.015236	test: 0.954805
MAE train: 0.242549	val: 0.805669	test: 0.776065

Epoch: 125
Loss: 0.12413265290004867
RMSE train: 0.343111	val: 1.023494	test: 0.959438
MAE train: 0.269852	val: 0.819829	test: 0.782272

Epoch: 126
Loss: 0.12675605607884272
RMSE train: 0.346075	val: 1.024203	test: 0.968386
MAE train: 0.273303	val: 0.824294	test: 0.791851

Epoch: 127
Loss: 0.14831428761993135
RMSE train: 0.296832	val: 1.017954	test: 0.959001
MAE train: 0.237627	val: 0.812558	test: 0.780909

Epoch: 128
Loss: 0.1378809346684388
RMSE train: 0.311353	val: 1.015913	test: 0.959887
MAE train: 0.245549	val: 0.822219	test: 0.789604

Epoch: 129
Loss: 0.11817625377859388
RMSE train: 0.288449	val: 1.046366	test: 0.979863
MAE train: 0.230075	val: 0.823429	test: 0.795871

Epoch: 130
Loss: 0.12459020582692963
RMSE train: 0.313495	val: 1.014584	test: 0.956378
MAE train: 0.248385	val: 0.811967	test: 0.779823

Epoch: 131
Loss: 0.12225186079740524
RMSE train: 0.295133	val: 1.023349	test: 0.963512
MAE train: 0.232534	val: 0.809160	test: 0.783707

Epoch: 132
Loss: 0.11437915744526046
RMSE train: 0.297539	val: 1.006546	test: 0.952822
MAE train: 0.236960	val: 0.813476	test: 0.782749

Epoch: 133
Loss: 0.11858131044677325
RMSE train: 0.291510	val: 1.026924	test: 0.963731
MAE train: 0.231247	val: 0.811004	test: 0.783155

Epoch: 134
Loss: 0.12015293166041374
RMSE train: 0.294353	val: 1.032729	test: 0.959800
MAE train: 0.228276	val: 0.815933	test: 0.779673

Epoch: 135
Loss: 0.12109283890042986
RMSE train: 0.283593	val: 1.029347	test: 0.974777
MAE train: 0.225015	val: 0.816057	test: 0.791907

Epoch: 136
Loss: 0.12055485802037376
RMSE train: 0.311969	val: 1.046782	test: 0.985676
MAE train: 0.244622	val: 0.820976	test: 0.800616

Early stopping
Best (RMSE):	 train: 0.301381	val: 0.987945	test: 0.934454
Best (MAE):	 train: 0.238518	val: 0.782882	test: 0.759181

MAE train: 0.268406	val: 0.793786	test: 0.721192

Epoch: 83
Loss: 0.21322495916060039
RMSE train: 0.347625	val: 0.998267	test: 0.921482
MAE train: 0.273922	val: 0.794748	test: 0.723799

Epoch: 84
Loss: 0.20971239996807917
RMSE train: 0.435376	val: 1.056023	test: 0.966580
MAE train: 0.358628	val: 0.842876	test: 0.751176

Epoch: 85
Loss: 0.2031222635081836
RMSE train: 0.342489	val: 1.034596	test: 0.944242
MAE train: 0.270758	val: 0.823846	test: 0.741485

Epoch: 86
Loss: 0.20750218949147634
RMSE train: 0.338193	val: 0.993069	test: 0.920305
MAE train: 0.266368	val: 0.784748	test: 0.723195

Epoch: 87
Loss: 0.2041080817580223
RMSE train: 0.322374	val: 1.011221	test: 0.928148
MAE train: 0.253226	val: 0.806846	test: 0.723055

Epoch: 88
Loss: 0.18867063256246702
RMSE train: 0.315264	val: 0.972636	test: 0.907760
MAE train: 0.248403	val: 0.777987	test: 0.710361

Epoch: 89
Loss: 0.18109010798590525
RMSE train: 0.338507	val: 1.025352	test: 0.928983
MAE train: 0.268341	val: 0.818150	test: 0.726331

Epoch: 90
Loss: 0.19785849217857635
RMSE train: 0.386963	val: 1.061223	test: 0.977730
MAE train: 0.310372	val: 0.845779	test: 0.760881

Epoch: 91
Loss: 0.20375486143997737
RMSE train: 0.334782	val: 1.057939	test: 0.964493
MAE train: 0.263747	val: 0.837681	test: 0.753841

Epoch: 92
Loss: 0.19872018375567027
RMSE train: 0.336458	val: 1.013249	test: 0.948656
MAE train: 0.268376	val: 0.808545	test: 0.743854

Epoch: 93
Loss: 0.1863992235490254
RMSE train: 0.323452	val: 0.975997	test: 0.907968
MAE train: 0.254134	val: 0.782340	test: 0.713752

Epoch: 94
Loss: 0.18133959706340516
RMSE train: 0.327285	val: 1.008182	test: 0.940235
MAE train: 0.259605	val: 0.802445	test: 0.736747

Epoch: 95
Loss: 0.17975376759256637
RMSE train: 0.310722	val: 0.997003	test: 0.930177
MAE train: 0.243345	val: 0.795649	test: 0.730968

Epoch: 96
Loss: 0.18033927572625025
RMSE train: 0.320682	val: 0.986997	test: 0.925678
MAE train: 0.253216	val: 0.787017	test: 0.722994

Epoch: 97
Loss: 0.17137465945311955
RMSE train: 0.301449	val: 0.984209	test: 0.913051
MAE train: 0.237630	val: 0.787317	test: 0.712143

Epoch: 98
Loss: 0.17962275445461273
RMSE train: 0.340300	val: 1.030615	test: 0.946069
MAE train: 0.271759	val: 0.813404	test: 0.740131

Epoch: 99
Loss: 0.16972982351269042
RMSE train: 0.327155	val: 0.986537	test: 0.910608
MAE train: 0.256642	val: 0.789006	test: 0.711562

Epoch: 100
Loss: 0.16768026884113038
RMSE train: 0.309111	val: 1.037991	test: 0.966292
MAE train: 0.246312	val: 0.819679	test: 0.755325

Epoch: 101
Loss: 0.16347605202879226
RMSE train: 0.377311	val: 1.056776	test: 0.972650
MAE train: 0.303285	val: 0.839877	test: 0.763782

Epoch: 102
Loss: 0.15979885629245213
RMSE train: 0.357379	val: 1.002653	test: 0.939401
MAE train: 0.282578	val: 0.796082	test: 0.732483

Epoch: 103
Loss: 0.16916597528117044
RMSE train: 0.331220	val: 0.988845	test: 0.921761
MAE train: 0.262540	val: 0.791871	test: 0.723971

Epoch: 104
Loss: 0.18136424784149444
RMSE train: 0.313483	val: 1.029083	test: 0.948449
MAE train: 0.246317	val: 0.817411	test: 0.743070

Epoch: 105
Loss: 0.1638387707727296
RMSE train: 0.298864	val: 1.009273	test: 0.942223
MAE train: 0.235224	val: 0.804114	test: 0.742139

Epoch: 106
Loss: 0.17718148125069483
RMSE train: 0.331700	val: 0.953688	test: 0.893574
MAE train: 0.263274	val: 0.761797	test: 0.709019

Epoch: 107
Loss: 0.15461890612329757
RMSE train: 0.344515	val: 1.033980	test: 0.968506
MAE train: 0.277476	val: 0.818913	test: 0.754812

Epoch: 108
Loss: 0.16636560644422257
RMSE train: 0.367079	val: 1.063833	test: 0.980906
MAE train: 0.293837	val: 0.847769	test: 0.765891

Epoch: 109
Loss: 0.1577760119523321
RMSE train: 0.299037	val: 1.012022	test: 0.941251
MAE train: 0.237193	val: 0.799901	test: 0.737583

Epoch: 110
Loss: 0.14815437846950122
RMSE train: 0.301138	val: 1.040506	test: 0.949927
MAE train: 0.237043	val: 0.832058	test: 0.746261

Epoch: 111
Loss: 0.15126126844968116
RMSE train: 0.290384	val: 1.059833	test: 0.979103
MAE train: 0.231130	val: 0.842472	test: 0.770369

Epoch: 112
Loss: 0.13332855435354368
RMSE train: 0.273396	val: 0.963561	test: 0.900054
MAE train: 0.215596	val: 0.763880	test: 0.710047

Epoch: 113
Loss: 0.14148020850760595
RMSE train: 0.330191	val: 1.024986	test: 0.939722
MAE train: 0.257547	val: 0.814340	test: 0.734811

Epoch: 114
Loss: 0.14176681424890245
RMSE train: 0.274724	val: 0.951785	test: 0.885262
MAE train: 0.213616	val: 0.763076	test: 0.704348

Epoch: 115
Loss: 0.141617444476911
RMSE train: 0.285480	val: 1.030161	test: 0.943334
MAE train: 0.222372	val: 0.816588	test: 0.739458

Epoch: 116
Loss: 0.1351012789777347
RMSE train: 0.320741	val: 0.989898	test: 0.910951
MAE train: 0.257030	val: 0.788489	test: 0.714392

Epoch: 117
Loss: 0.15052173605987004
RMSE train: 0.275985	val: 1.046906	test: 0.968484
MAE train: 0.218124	val: 0.831005	test: 0.760940

Epoch: 118
Loss: 0.13791182690433093
RMSE train: 0.295325	val: 1.010583	test: 0.924594
MAE train: 0.235288	val: 0.805518	test: 0.730871

Epoch: 119
Loss: 0.12389724754861423
RMSE train: 0.262812	val: 0.990034	test: 0.915057
MAE train: 0.207142	val: 0.791261	test: 0.723401

Epoch: 120
Loss: 0.12184645820941244
RMSE train: 0.294395	val: 1.004433	test: 0.921414
MAE train: 0.234347	val: 0.797744	test: 0.725433

Epoch: 121
Loss: 0.13001873131309236
RMSE train: 0.287263	val: 1.013225	test: 0.930853
MAE train: 0.226149	val: 0.804823	test: 0.734303

Epoch: 122
Loss: 0.13297290621059282
RMSE train: 0.326608	val: 1.017284	test: 0.936835
MAE train: 0.262420	val: 0.811905	test: 0.740034

Epoch: 123
Loss: 0.13467036666614668
RMSE train: 0.253358	val: 1.002049	test: 0.935028
MAE train: 0.201849	val: 0.794610	test: 0.732868

Epoch: 124
Loss: 0.1345849729010037
RMSE train: 0.337985	val: 1.055398	test: 0.974098
MAE train: 0.270862	val: 0.842787	test: 0.766983

Epoch: 125
Loss: 0.1228812752025468
RMSE train: 0.243438	val: 1.010497	test: 0.938248
MAE train: 0.193230	val: 0.801827	test: 0.735176

Epoch: 126
Loss: 0.12203577586582728
RMSE train: 0.343815	val: 1.026571	test: 0.936063
MAE train: 0.270599	val: 0.815389	test: 0.737106

Epoch: 127
Loss: 0.14537131254162108
RMSE train: 0.267636	val: 0.994071	test: 0.921991
MAE train: 0.212478	val: 0.792372	test: 0.730964

Epoch: 128
Loss: 0.13142744877508708
RMSE train: 0.271411	val: 1.003299	test: 0.932026
MAE train: 0.216110	val: 0.797102	test: 0.731614

Epoch: 129
Loss: 0.12299128834690366
RMSE train: 0.269506	val: 0.986754	test: 0.913241
MAE train: 0.213561	val: 0.786570	test: 0.722762

Epoch: 130
Loss: 0.12173025895442281
RMSE train: 0.327582	val: 1.062231	test: 0.978618
MAE train: 0.262943	val: 0.839132	test: 0.769070

Epoch: 131
Loss: 0.11554304510354996
RMSE train: 0.305092	val: 1.045412	test: 0.955028
MAE train: 0.240977	val: 0.829956	test: 0.754785

Epoch: 132
Loss: 0.12181529083422252
RMSE train: 0.344627	val: 1.020235	test: 0.932204
MAE train: 0.271029	val: 0.810750	test: 0.734807

Epoch: 133
Loss: 0.1350233554840088
RMSE train: 0.377011	val: 1.070153	test: 0.981750
MAE train: 0.308588	val: 0.850166	test: 0.769200

Epoch: 134
Loss: 0.14031348749995232
RMSE train: 0.259755	val: 1.003256	test: 0.938694
MAE train: 0.203615	val: 0.802147	test: 0.734873

Epoch: 135
Loss: 0.13439304381608963
RMSE train: 0.298140	val: 1.030703	test: 0.947833
MAE train: 0.236162	val: 0.818720	test: 0.749604

Epoch: 136
Loss: 0.12844143595014298
RMSE train: 0.331391	val: 1.077299	test: 0.996565
MAE train: 0.263688	val: 0.853204	test: 0.780519

Epoch: 137
Loss: 0.14230113423296384
RMSE train: 0.316422	val: 0.992827	test: 0.918784
MAE train: 0.255902	val: 0.782820	test: 0.728245

Epoch: 138
Loss: 0.11891998297401837
RMSE train: 0.306629	val: 0.974994	test: 0.897584
MAE train: 0.246268	val: 0.775764	test: 0.708083

Epoch: 139
Loss: 0.11340691149234772
RMSE train: 0.290666	val: 0.999458	test: 0.917133
MAE train: 0.230272	val: 0.795309	test: 0.724115

Epoch: 140
Loss: 0.11510758314813886
RMSE train: 0.307796	val: 1.048731	test: 0.958861
MAE train: 0.245719	val: 0.830838	test: 0.756126

Epoch: 141
Loss: 0.10751753885831151
RMSE train: 0.261160	val: 0.988589	test: 0.907908
MAE train: 0.205111	val: 0.791035	test: 0.719695

Epoch: 142
Loss: 0.10404583119920321
RMSE train: 0.318657	val: 1.003233	test: 0.922301

MAE train: 0.333760	val: 0.683198	test: 0.663886

Epoch: 83
Loss: 0.24318986598934447
RMSE train: 0.495176	val: 0.921890	test: 0.863005
MAE train: 0.400847	val: 0.731179	test: 0.683090

Epoch: 84
Loss: 0.24633969047239848
RMSE train: 0.411121	val: 0.863812	test: 0.827109
MAE train: 0.325856	val: 0.685463	test: 0.659293

Epoch: 85
Loss: 0.23565206357410975
RMSE train: 0.409784	val: 0.870634	test: 0.829494
MAE train: 0.324955	val: 0.687562	test: 0.663910

Epoch: 86
Loss: 0.2348693162202835
RMSE train: 0.420167	val: 0.872837	test: 0.846081
MAE train: 0.333287	val: 0.688894	test: 0.671891

Epoch: 87
Loss: 0.2298567412155015
RMSE train: 0.445003	val: 0.884057	test: 0.840198
MAE train: 0.353610	val: 0.701705	test: 0.667316

Epoch: 88
Loss: 0.22461365056889399
RMSE train: 0.382215	val: 0.846295	test: 0.808757
MAE train: 0.301705	val: 0.670509	test: 0.642560

Epoch: 89
Loss: 0.21960049335445678
RMSE train: 0.411185	val: 0.856135	test: 0.820588
MAE train: 0.326231	val: 0.675733	test: 0.654569

Epoch: 90
Loss: 0.22083102485963277
RMSE train: 0.396395	val: 0.870631	test: 0.838862
MAE train: 0.314269	val: 0.683968	test: 0.669760

Epoch: 91
Loss: 0.21823352894612721
RMSE train: 0.412398	val: 0.904511	test: 0.869186
MAE train: 0.326492	val: 0.713088	test: 0.683386

Epoch: 92
Loss: 0.21800656403814042
RMSE train: 0.415940	val: 0.893817	test: 0.858477
MAE train: 0.330120	val: 0.707956	test: 0.678811

Epoch: 93
Loss: 0.2129521518945694
RMSE train: 0.387905	val: 0.847439	test: 0.815619
MAE train: 0.308112	val: 0.664152	test: 0.650154

Epoch: 94
Loss: 0.20722115359136037
RMSE train: 0.409492	val: 0.862993	test: 0.832468
MAE train: 0.328199	val: 0.678556	test: 0.663989

Epoch: 95
Loss: 0.20980783232620784
RMSE train: 0.391447	val: 0.856537	test: 0.819334
MAE train: 0.311994	val: 0.675863	test: 0.650432

Epoch: 96
Loss: 0.20474550766604288
RMSE train: 0.370126	val: 0.839659	test: 0.818929
MAE train: 0.295305	val: 0.660419	test: 0.653827

Epoch: 97
Loss: 0.2017028714929308
RMSE train: 0.380247	val: 0.876818	test: 0.833335
MAE train: 0.302297	val: 0.695486	test: 0.660725

Epoch: 98
Loss: 0.19853732947792327
RMSE train: 0.369554	val: 0.856483	test: 0.820761
MAE train: 0.295571	val: 0.674202	test: 0.652743

Epoch: 99
Loss: 0.18906899222305842
RMSE train: 0.356285	val: 0.847088	test: 0.813352
MAE train: 0.282652	val: 0.667012	test: 0.644951

Epoch: 100
Loss: 0.188164516219071
RMSE train: 0.359609	val: 0.844814	test: 0.821588
MAE train: 0.284026	val: 0.662163	test: 0.654404

Epoch: 101
Loss: 0.19101981659020698
RMSE train: 0.387299	val: 0.885611	test: 0.845072
MAE train: 0.306440	val: 0.693797	test: 0.663443

Epoch: 102
Loss: 0.19170422213418142
RMSE train: 0.370652	val: 0.862498	test: 0.832776
MAE train: 0.294459	val: 0.679742	test: 0.662397

Epoch: 103
Loss: 0.18228465531553542
RMSE train: 0.336333	val: 0.843908	test: 0.814302
MAE train: 0.266097	val: 0.659212	test: 0.650395

Epoch: 104
Loss: 0.1922973170876503
RMSE train: 0.377680	val: 0.853461	test: 0.823318
MAE train: 0.298596	val: 0.669339	test: 0.649245

Epoch: 105
Loss: 0.196396142244339
RMSE train: 0.352650	val: 0.844819	test: 0.820703
MAE train: 0.278832	val: 0.663698	test: 0.652518

Epoch: 106
Loss: 0.1870530311550413
RMSE train: 0.394187	val: 0.863772	test: 0.825504
MAE train: 0.313608	val: 0.679262	test: 0.656940

Epoch: 107
Loss: 0.17515975875513895
RMSE train: 0.438341	val: 0.896588	test: 0.849609
MAE train: 0.345861	val: 0.704892	test: 0.675716

Epoch: 108
Loss: 0.20116530571665084
RMSE train: 0.349224	val: 0.847340	test: 0.813661
MAE train: 0.279437	val: 0.661989	test: 0.641984

Epoch: 109
Loss: 0.1759492467556681
RMSE train: 0.370222	val: 0.864217	test: 0.830750
MAE train: 0.292967	val: 0.676200	test: 0.662686

Epoch: 110
Loss: 0.18512688683612005
RMSE train: 0.335024	val: 0.874416	test: 0.822656
MAE train: 0.263829	val: 0.682854	test: 0.640880

Epoch: 111
Loss: 0.17271687196833746
RMSE train: 0.368532	val: 0.832563	test: 0.799333
MAE train: 0.294004	val: 0.647972	test: 0.636958

Epoch: 112
Loss: 0.16955832711287908
RMSE train: 0.408726	val: 0.895226	test: 0.837598
MAE train: 0.329335	val: 0.699139	test: 0.665228

Epoch: 113
Loss: 0.16882398937429702
RMSE train: 0.329786	val: 0.861288	test: 0.808750
MAE train: 0.261324	val: 0.674610	test: 0.632777

Epoch: 114
Loss: 0.17453256568738393
RMSE train: 0.340022	val: 0.869249	test: 0.827438
MAE train: 0.267007	val: 0.672508	test: 0.652261

Epoch: 115
Loss: 0.1640533826180867
RMSE train: 0.365500	val: 0.881406	test: 0.830592
MAE train: 0.287923	val: 0.694726	test: 0.649428

Epoch: 116
Loss: 0.1724834612437657
RMSE train: 0.331917	val: 0.859218	test: 0.817618
MAE train: 0.259643	val: 0.673279	test: 0.646466

Epoch: 117
Loss: 0.17574281245470047
RMSE train: 0.347657	val: 0.838384	test: 0.815806
MAE train: 0.277145	val: 0.657983	test: 0.651967

Epoch: 118
Loss: 0.16722552265439714
RMSE train: 0.342892	val: 0.877216	test: 0.834206
MAE train: 0.271568	val: 0.687454	test: 0.664748

Epoch: 119
Loss: 0.15987179800868034
RMSE train: 0.365176	val: 0.844322	test: 0.827858
MAE train: 0.293908	val: 0.648415	test: 0.663941

Epoch: 120
Loss: 0.1645065418311528
RMSE train: 0.337844	val: 0.873186	test: 0.830854
MAE train: 0.267107	val: 0.686558	test: 0.654653

Epoch: 121
Loss: 0.16375440784863063
RMSE train: 0.362455	val: 0.852258	test: 0.804624
MAE train: 0.287166	val: 0.672561	test: 0.636717

Epoch: 122
Loss: 0.16911774660859788
RMSE train: 0.372239	val: 0.872089	test: 0.828616
MAE train: 0.294275	val: 0.684380	test: 0.656010

Epoch: 123
Loss: 0.17730080442769186
RMSE train: 0.334515	val: 0.822614	test: 0.798052
MAE train: 0.265431	val: 0.645093	test: 0.635714

Epoch: 124
Loss: 0.16370275084461486
RMSE train: 0.366349	val: 0.896573	test: 0.846225
MAE train: 0.288682	val: 0.700453	test: 0.671443

Epoch: 125
Loss: 0.16407537992511476
RMSE train: 0.313773	val: 0.866061	test: 0.816845
MAE train: 0.247554	val: 0.677856	test: 0.642995

Epoch: 126
Loss: 0.15549776064498083
RMSE train: 0.352563	val: 0.875310	test: 0.825831
MAE train: 0.280731	val: 0.685198	test: 0.658347

Epoch: 127
Loss: 0.16399043798446655
RMSE train: 0.319165	val: 0.843480	test: 0.814576
MAE train: 0.252749	val: 0.665541	test: 0.652504

Epoch: 128
Loss: 0.14723596615450724
RMSE train: 0.335821	val: 0.831716	test: 0.802903
MAE train: 0.270062	val: 0.653122	test: 0.640878

Epoch: 129
Loss: 0.14426329093320028
RMSE train: 0.312980	val: 0.836166	test: 0.817010
MAE train: 0.247803	val: 0.654422	test: 0.649359

Epoch: 130
Loss: 0.14197566785982677
RMSE train: 0.364796	val: 0.870278	test: 0.827054
MAE train: 0.289240	val: 0.683691	test: 0.656115

Epoch: 131
Loss: 0.1392190626689366
RMSE train: 0.344185	val: 0.863714	test: 0.819251
MAE train: 0.270751	val: 0.676917	test: 0.653796

Epoch: 132
Loss: 0.14734397296394622
RMSE train: 0.324769	val: 0.861728	test: 0.809241
MAE train: 0.256000	val: 0.674305	test: 0.636224

Epoch: 133
Loss: 0.14288675306098803
RMSE train: 0.351371	val: 0.866651	test: 0.821644
MAE train: 0.280926	val: 0.678118	test: 0.648190

Epoch: 134
Loss: 0.13543231253113067
RMSE train: 0.307379	val: 0.861547	test: 0.812338
MAE train: 0.245193	val: 0.676737	test: 0.642361

Epoch: 135
Loss: 0.14619120529719762
RMSE train: 0.301945	val: 0.867554	test: 0.830381
MAE train: 0.238320	val: 0.681024	test: 0.660518

Epoch: 136
Loss: 0.14311768699969565
RMSE train: 0.311720	val: 0.829457	test: 0.800968
MAE train: 0.247921	val: 0.651766	test: 0.639432

Epoch: 137
Loss: 0.16495408224208014
RMSE train: 0.308310	val: 0.830791	test: 0.809998
MAE train: 0.245484	val: 0.651908	test: 0.646651

Epoch: 138
Loss: 0.14132571380053247
RMSE train: 0.370073	val: 0.878776	test: 0.825834
MAE train: 0.291256	val: 0.693163	test: 0.652518

Epoch: 139
Loss: 0.1441069319844246
RMSE train: 0.299441	val: 0.847407	test: 0.818375
MAE train: 0.236778	val: 0.668102	test: 0.647176

Epoch: 140
Loss: 0.15085581690073013
RMSE train: 0.321964	val: 0.838120	test: 0.803047
MAE train: 0.254484	val: 0.655853	test: 0.636976

Epoch: 141
Loss: 0.1452269729759012
RMSE train: 0.349616	val: 0.865545	test: 0.808312
MAE train: 0.276081	val: 0.684716	test: 0.639417

Epoch: 142
Loss: 0.13678251313311712
RMSE train: 0.338233	val: 0.867909	test: 0.816263
MAE train: 0.270524	val: 0.685689	test: 0.645311
MAE train: 0.284273	val: 0.831829	test: 0.776267

Epoch: 83
Loss: 0.18903755822352
RMSE train: 0.384677	val: 1.062855	test: 0.956358
MAE train: 0.304417	val: 0.833693	test: 0.764351

Epoch: 84
Loss: 0.18852806517056056
RMSE train: 0.342392	val: 1.014883	test: 0.927082
MAE train: 0.273131	val: 0.803437	test: 0.743925

Epoch: 85
Loss: 0.18742349318095616
RMSE train: 0.379807	val: 1.036483	test: 0.943619
MAE train: 0.300999	val: 0.824199	test: 0.759311

Epoch: 86
Loss: 0.19142613027776992
RMSE train: 0.331333	val: 1.045172	test: 0.948972
MAE train: 0.264063	val: 0.819175	test: 0.759352

Epoch: 87
Loss: 0.19827665920768464
RMSE train: 0.352312	val: 1.043687	test: 0.948631
MAE train: 0.279782	val: 0.821406	test: 0.759159

Epoch: 88
Loss: 0.19085931777954102
RMSE train: 0.327940	val: 1.032768	test: 0.941246
MAE train: 0.260425	val: 0.813508	test: 0.760182

Epoch: 89
Loss: 0.18801254353352956
RMSE train: 0.368597	val: 1.029368	test: 0.940144
MAE train: 0.299026	val: 0.817066	test: 0.757399

Epoch: 90
Loss: 0.1872046770794051
RMSE train: 0.352223	val: 1.049201	test: 0.954758
MAE train: 0.280607	val: 0.823325	test: 0.764183

Epoch: 91
Loss: 0.1778312889592988
RMSE train: 0.428689	val: 1.069964	test: 0.953731
MAE train: 0.337649	val: 0.842701	test: 0.761012

Epoch: 92
Loss: 0.16851559281349182
RMSE train: 0.449838	val: 1.033371	test: 0.947821
MAE train: 0.372503	val: 0.823738	test: 0.759911

Epoch: 93
Loss: 0.18087695219687053
RMSE train: 0.348984	val: 1.019220	test: 0.927888
MAE train: 0.281682	val: 0.805283	test: 0.739865

Epoch: 94
Loss: 0.1702377966472081
RMSE train: 0.359214	val: 1.054617	test: 0.952297
MAE train: 0.288234	val: 0.835474	test: 0.765139

Epoch: 95
Loss: 0.17299385581697738
RMSE train: 0.364538	val: 1.053161	test: 0.944942
MAE train: 0.287047	val: 0.832186	test: 0.753641

Epoch: 96
Loss: 0.1697895069207464
RMSE train: 0.356311	val: 1.045575	test: 0.940848
MAE train: 0.286716	val: 0.829231	test: 0.756538

Epoch: 97
Loss: 0.18118559143372945
RMSE train: 0.313966	val: 1.039159	test: 0.946786
MAE train: 0.249720	val: 0.823971	test: 0.763000

Epoch: 98
Loss: 0.16120607086590358
RMSE train: 0.333357	val: 1.007773	test: 0.917898
MAE train: 0.265975	val: 0.801671	test: 0.732887

Epoch: 99
Loss: 0.16737547729696547
RMSE train: 0.325346	val: 1.047886	test: 0.957512
MAE train: 0.257174	val: 0.822814	test: 0.768700

Epoch: 100
Loss: 0.1697823309472629
RMSE train: 0.401021	val: 1.069908	test: 0.964706
MAE train: 0.317233	val: 0.840274	test: 0.768046

Epoch: 101
Loss: 0.1641093066760472
RMSE train: 0.367301	val: 1.044665	test: 0.953088
MAE train: 0.295962	val: 0.821008	test: 0.762413

Epoch: 102
Loss: 0.16569492806281363
RMSE train: 0.330725	val: 1.065034	test: 0.956114
MAE train: 0.262417	val: 0.836707	test: 0.769995

Epoch: 103
Loss: 0.15648226599608148
RMSE train: 0.378081	val: 1.001181	test: 0.924070
MAE train: 0.309647	val: 0.802597	test: 0.737933

Epoch: 104
Loss: 0.15179968731743948
RMSE train: 0.328941	val: 1.050715	test: 0.942361
MAE train: 0.263089	val: 0.829346	test: 0.753233

Epoch: 105
Loss: 0.14390836230346135
RMSE train: 0.318393	val: 1.069463	test: 0.963456
MAE train: 0.256142	val: 0.839201	test: 0.773399

Epoch: 106
Loss: 0.1533528779234205
RMSE train: 0.354609	val: 1.020092	test: 0.932855
MAE train: 0.282394	val: 0.808205	test: 0.740817

Epoch: 107
Loss: 0.15997941685574396
RMSE train: 0.374794	val: 1.058105	test: 0.968262
MAE train: 0.307513	val: 0.835842	test: 0.783097

Epoch: 108
Loss: 0.14029554863061225
RMSE train: 0.308205	val: 1.041067	test: 0.942969
MAE train: 0.247730	val: 0.820719	test: 0.758973

Epoch: 109
Loss: 0.14262613015515463
RMSE train: 0.365790	val: 1.053310	test: 0.960737
MAE train: 0.301386	val: 0.830262	test: 0.774200

Epoch: 110
Loss: 0.14394171323095048
RMSE train: 0.368235	val: 1.065079	test: 0.958211
MAE train: 0.289014	val: 0.837207	test: 0.767236

Epoch: 111
Loss: 0.13754163363150188
RMSE train: 0.303147	val: 1.033835	test: 0.938039
MAE train: 0.243416	val: 0.815690	test: 0.750200

Epoch: 112
Loss: 0.13653169093387468
RMSE train: 0.293678	val: 1.067531	test: 0.969506
MAE train: 0.232002	val: 0.838260	test: 0.777148

Epoch: 113
Loss: 0.1364214244697775
RMSE train: 0.388517	val: 1.047533	test: 0.953423
MAE train: 0.321285	val: 0.825034	test: 0.763476

Epoch: 114
Loss: 0.1319599790232522
RMSE train: 0.320605	val: 1.080121	test: 0.968229
MAE train: 0.257367	val: 0.847359	test: 0.777597

Epoch: 115
Loss: 0.13954383613807814
RMSE train: 0.337015	val: 1.035026	test: 0.937738
MAE train: 0.271890	val: 0.815827	test: 0.749447

Epoch: 116
Loss: 0.13571877564702714
RMSE train: 0.337298	val: 1.036951	test: 0.948071
MAE train: 0.266523	val: 0.815224	test: 0.758603

Epoch: 117
Loss: 0.13333651583109582
RMSE train: 0.319208	val: 1.074870	test: 0.977346
MAE train: 0.248088	val: 0.842965	test: 0.782125

Epoch: 118
Loss: 0.14577934358801162
RMSE train: 0.312449	val: 1.073732	test: 0.974484
MAE train: 0.248025	val: 0.841983	test: 0.783907

Epoch: 119
Loss: 0.13851045657481467
RMSE train: 0.361198	val: 1.126648	test: 1.020440
MAE train: 0.278890	val: 0.880705	test: 0.817819

Epoch: 120
Loss: 0.14169016959411757
RMSE train: 0.383059	val: 1.031162	test: 0.948945
MAE train: 0.318319	val: 0.814476	test: 0.758579

Epoch: 121
Loss: 0.13547689520886966
RMSE train: 0.351173	val: 1.030743	test: 0.946357
MAE train: 0.288537	val: 0.812520	test: 0.750720

Epoch: 122
Loss: 0.15030086359807424
RMSE train: 0.299013	val: 1.067968	test: 0.971562
MAE train: 0.241428	val: 0.839167	test: 0.781871

Epoch: 123
Loss: 0.1323519710983549
RMSE train: 0.289836	val: 1.058844	test: 0.965511
MAE train: 0.234422	val: 0.831521	test: 0.776034

Epoch: 124
Loss: 0.12240374247942652
RMSE train: 0.322309	val: 1.095285	test: 0.995202
MAE train: 0.252854	val: 0.858308	test: 0.797079

Epoch: 125
Loss: 0.12146006311689105
RMSE train: 0.305505	val: 1.069141	test: 0.972025
MAE train: 0.247926	val: 0.838372	test: 0.781243

Epoch: 126
Loss: 0.11924208381346293
RMSE train: 0.306810	val: 1.041010	test: 0.953182
MAE train: 0.244395	val: 0.821356	test: 0.763992

Epoch: 127
Loss: 0.11809654853173665
RMSE train: 0.314569	val: 1.096250	test: 0.999785
MAE train: 0.244787	val: 0.857272	test: 0.802267

Epoch: 128
Loss: 0.11260132385151726
RMSE train: 0.356696	val: 1.020273	test: 0.932407
MAE train: 0.293775	val: 0.807346	test: 0.744359

Epoch: 129
Loss: 0.12271517408745629
RMSE train: 0.301185	val: 1.013626	test: 0.929932
MAE train: 0.245483	val: 0.803483	test: 0.743780

Epoch: 130
Loss: 0.12365891731211118
RMSE train: 0.269642	val: 1.041523	test: 0.951963
MAE train: 0.215536	val: 0.819054	test: 0.759478

Epoch: 131
Loss: 0.1408336247716631
RMSE train: 0.317597	val: 1.036901	test: 0.945625
MAE train: 0.259804	val: 0.820730	test: 0.751735

Epoch: 132
Loss: 0.11437634431890079
RMSE train: 0.350047	val: 1.108012	test: 1.000322
MAE train: 0.273923	val: 0.868972	test: 0.796172

Epoch: 133
Loss: 0.1294526926108769
RMSE train: 0.281642	val: 1.068229	test: 0.970680
MAE train: 0.224423	val: 0.843062	test: 0.780799

Epoch: 134
Loss: 0.11133359798363276
RMSE train: 0.303976	val: 1.048210	test: 0.949838
MAE train: 0.243661	val: 0.828832	test: 0.761214

Epoch: 135
Loss: 0.11412998874272619
RMSE train: 0.335829	val: 1.088168	test: 0.992308
MAE train: 0.266382	val: 0.852884	test: 0.787787

Epoch: 136
Loss: 0.12355268959488187
RMSE train: 0.342872	val: 1.029551	test: 0.944515
MAE train: 0.284731	val: 0.823555	test: 0.759099

Epoch: 137
Loss: 0.11114095364298139
RMSE train: 0.338069	val: 1.043212	test: 0.954962
MAE train: 0.276430	val: 0.832033	test: 0.766404

Epoch: 138
Loss: 0.10509779251047544
RMSE train: 0.283451	val: 1.071071	test: 0.973748
MAE train: 0.225007	val: 0.843148	test: 0.778774

Early stopping
Best (RMSE):	 train: 0.378081	val: 1.001181	test: 0.924070
Best (MAE):	 train: 0.309647	val: 0.802597	test: 0.737933

MAE train: 0.313244	val: 0.675251	test: 0.651739

Epoch: 83
Loss: 0.24500275190387452
RMSE train: 0.424304	val: 0.871495	test: 0.836236
MAE train: 0.335574	val: 0.683425	test: 0.674718

Epoch: 84
Loss: 0.24651983699628285
RMSE train: 0.418470	val: 0.891165	test: 0.849477
MAE train: 0.331049	val: 0.697906	test: 0.688053

Epoch: 85
Loss: 0.243363724223205
RMSE train: 0.454471	val: 0.898837	test: 0.840690
MAE train: 0.359707	val: 0.703852	test: 0.675472

Epoch: 86
Loss: 0.25212341227701734
RMSE train: 0.400908	val: 0.890125	test: 0.853769
MAE train: 0.317603	val: 0.688746	test: 0.687873

Epoch: 87
Loss: 0.2577608121292932
RMSE train: 0.437809	val: 0.896675	test: 0.851010
MAE train: 0.347524	val: 0.708555	test: 0.696932

Epoch: 88
Loss: 0.25117489376238417
RMSE train: 0.399621	val: 0.853060	test: 0.820498
MAE train: 0.317424	val: 0.674923	test: 0.657183

Epoch: 89
Loss: 0.23375025817326137
RMSE train: 0.432027	val: 0.909384	test: 0.845998
MAE train: 0.341063	val: 0.710681	test: 0.684324

Epoch: 90
Loss: 0.252765562917505
RMSE train: 0.397473	val: 0.889662	test: 0.841366
MAE train: 0.313332	val: 0.695744	test: 0.680448

Epoch: 91
Loss: 0.21734191370861872
RMSE train: 0.385207	val: 0.863380	test: 0.828383
MAE train: 0.303516	val: 0.677539	test: 0.664235

Epoch: 92
Loss: 0.22452810619558608
RMSE train: 0.401482	val: 0.852307	test: 0.823033
MAE train: 0.317617	val: 0.668861	test: 0.663701

Epoch: 93
Loss: 0.22141388484409877
RMSE train: 0.433788	val: 0.892977	test: 0.866987
MAE train: 0.343471	val: 0.693033	test: 0.699525

Epoch: 94
Loss: 0.2238016203045845
RMSE train: 0.409552	val: 0.868523	test: 0.834222
MAE train: 0.324005	val: 0.679441	test: 0.676310

Epoch: 95
Loss: 0.2137986347079277
RMSE train: 0.404343	val: 0.865874	test: 0.844393
MAE train: 0.320521	val: 0.678364	test: 0.683371

Epoch: 96
Loss: 0.22202247274773462
RMSE train: 0.478804	val: 0.936441	test: 0.882756
MAE train: 0.382394	val: 0.737895	test: 0.717427

Epoch: 97
Loss: 0.2249749270933015
RMSE train: 0.419359	val: 0.870335	test: 0.827413
MAE train: 0.331396	val: 0.679196	test: 0.676304

Epoch: 98
Loss: 0.21073091349431447
RMSE train: 0.391674	val: 0.865620	test: 0.828596
MAE train: 0.309085	val: 0.681184	test: 0.672876

Epoch: 99
Loss: 0.20542192139795848
RMSE train: 0.392016	val: 0.869145	test: 0.832745
MAE train: 0.311966	val: 0.679870	test: 0.677443

Epoch: 100
Loss: 0.22362794301339559
RMSE train: 0.351717	val: 0.880843	test: 0.840650
MAE train: 0.277670	val: 0.687281	test: 0.677592

Epoch: 101
Loss: 0.19426856988242694
RMSE train: 0.359051	val: 0.882238	test: 0.829373
MAE train: 0.283433	val: 0.681448	test: 0.675096

Epoch: 102
Loss: 0.19708340082849776
RMSE train: 0.400972	val: 0.901896	test: 0.856711
MAE train: 0.315210	val: 0.700570	test: 0.695285

Epoch: 103
Loss: 0.19782227384192602
RMSE train: 0.375334	val: 0.880357	test: 0.852172
MAE train: 0.299213	val: 0.693030	test: 0.694860

Epoch: 104
Loss: 0.19430411607027054
RMSE train: 0.355969	val: 0.875512	test: 0.835966
MAE train: 0.279899	val: 0.684709	test: 0.673926

Epoch: 105
Loss: 0.18296748719045094
RMSE train: 0.341869	val: 0.861603	test: 0.822336
MAE train: 0.270118	val: 0.676103	test: 0.662659

Epoch: 106
Loss: 0.1938483023217746
RMSE train: 0.391953	val: 0.877647	test: 0.838645
MAE train: 0.310332	val: 0.684132	test: 0.676739

Epoch: 107
Loss: 0.18815636741263525
RMSE train: 0.358007	val: 0.870795	test: 0.848240
MAE train: 0.282217	val: 0.680709	test: 0.687812

Epoch: 108
Loss: 0.1780297617827143
RMSE train: 0.320318	val: 0.849518	test: 0.821447
MAE train: 0.251325	val: 0.662710	test: 0.662564

Epoch: 109
Loss: 0.17507432188306535
RMSE train: 0.348653	val: 0.870067	test: 0.845552
MAE train: 0.277330	val: 0.674146	test: 0.683843

Epoch: 110
Loss: 0.16940764763525554
RMSE train: 0.361632	val: 0.890323	test: 0.848976
MAE train: 0.284700	val: 0.694297	test: 0.691762

Epoch: 111
Loss: 0.17480618400233133
RMSE train: 0.323479	val: 0.878673	test: 0.840191
MAE train: 0.254578	val: 0.676767	test: 0.675230

Epoch: 112
Loss: 0.1773128371153559
RMSE train: 0.354592	val: 0.890257	test: 0.849826
MAE train: 0.279088	val: 0.696007	test: 0.690914

Epoch: 113
Loss: 0.17046670296362468
RMSE train: 0.376040	val: 0.864047	test: 0.836519
MAE train: 0.301552	val: 0.677195	test: 0.681410

Epoch: 114
Loss: 0.17198796144553594
RMSE train: 0.353391	val: 0.880126	test: 0.848594
MAE train: 0.280524	val: 0.680985	test: 0.682542

Epoch: 115
Loss: 0.16114471639905656
RMSE train: 0.352331	val: 0.867463	test: 0.838599
MAE train: 0.279932	val: 0.673388	test: 0.679254

Epoch: 116
Loss: 0.1544847089265074
RMSE train: 0.313099	val: 0.874064	test: 0.838603
MAE train: 0.247052	val: 0.675056	test: 0.680174

Epoch: 117
Loss: 0.17075277119874954
RMSE train: 0.333659	val: 0.864747	test: 0.821715
MAE train: 0.265061	val: 0.678000	test: 0.661708

Epoch: 118
Loss: 0.16326824043478286
RMSE train: 0.339279	val: 0.877233	test: 0.847761
MAE train: 0.264947	val: 0.678509	test: 0.687248

Epoch: 119
Loss: 0.15563240540879114
RMSE train: 0.310401	val: 0.860943	test: 0.814973
MAE train: 0.244490	val: 0.672473	test: 0.650879

Epoch: 120
Loss: 0.17292975740773336
RMSE train: 0.354052	val: 0.876834	test: 0.834907
MAE train: 0.280066	val: 0.684425	test: 0.678024

Epoch: 121
Loss: 0.1626096772296088
RMSE train: 0.323865	val: 0.859723	test: 0.843820
MAE train: 0.257082	val: 0.677529	test: 0.685308

Epoch: 122
Loss: 0.18590165036065237
RMSE train: 0.366672	val: 0.890394	test: 0.851310
MAE train: 0.286724	val: 0.690123	test: 0.689982

Epoch: 123
Loss: 0.17582858886037553
RMSE train: 0.303130	val: 0.858190	test: 0.830095
MAE train: 0.239351	val: 0.670084	test: 0.677285

Epoch: 124
Loss: 0.16257261059113912
RMSE train: 0.362294	val: 0.892823	test: 0.858500
MAE train: 0.284761	val: 0.692057	test: 0.693686

Epoch: 125
Loss: 0.15232731721230916
RMSE train: 0.287558	val: 0.854696	test: 0.823136
MAE train: 0.227625	val: 0.666143	test: 0.657419

Epoch: 126
Loss: 0.15439326103244508
RMSE train: 0.336491	val: 0.867556	test: 0.842577
MAE train: 0.266867	val: 0.679433	test: 0.682922

Epoch: 127
Loss: 0.14681013460670198
RMSE train: 0.320771	val: 0.862818	test: 0.840731
MAE train: 0.255618	val: 0.680043	test: 0.677912

Epoch: 128
Loss: 0.1519640013575554
RMSE train: 0.356409	val: 0.870672	test: 0.833217
MAE train: 0.284542	val: 0.683942	test: 0.677940

Epoch: 129
Loss: 0.15086227761847631
RMSE train: 0.302842	val: 0.879740	test: 0.846268
MAE train: 0.238988	val: 0.685931	test: 0.686449

Epoch: 130
Loss: 0.15151464885899
RMSE train: 0.337201	val: 0.862932	test: 0.834404
MAE train: 0.269694	val: 0.681754	test: 0.668584

Epoch: 131
Loss: 0.16742824018001556
RMSE train: 0.306564	val: 0.868789	test: 0.825661
MAE train: 0.242660	val: 0.682284	test: 0.662492

Epoch: 132
Loss: 0.15437145211866923
RMSE train: 0.354840	val: 0.896150	test: 0.852629
MAE train: 0.279464	val: 0.691731	test: 0.694829

Epoch: 133
Loss: 0.1549418345093727
RMSE train: 0.335398	val: 0.870737	test: 0.847276
MAE train: 0.265358	val: 0.679836	test: 0.688569

Epoch: 134
Loss: 0.13980338190283095
RMSE train: 0.308862	val: 0.880372	test: 0.849485
MAE train: 0.244680	val: 0.683066	test: 0.693230

Epoch: 135
Loss: 0.1356678775378636
RMSE train: 0.325240	val: 0.856970	test: 0.840741
MAE train: 0.261252	val: 0.674476	test: 0.679843

Epoch: 136
Loss: 0.1531429695231574
RMSE train: 0.320609	val: 0.873743	test: 0.837079
MAE train: 0.253167	val: 0.679577	test: 0.676249

Epoch: 137
Loss: 0.1306762019438403
RMSE train: 0.435299	val: 0.924024	test: 0.876792
MAE train: 0.349591	val: 0.717162	test: 0.708657

Epoch: 138
Loss: 0.1313555490757738
RMSE train: 0.307264	val: 0.849907	test: 0.823488
MAE train: 0.243641	val: 0.664110	test: 0.668687

Epoch: 139
Loss: 0.1297330403966563
RMSE train: 0.301142	val: 0.874014	test: 0.834202
MAE train: 0.238993	val: 0.679495	test: 0.676446

Epoch: 140
Loss: 0.14036379701324872
RMSE train: 0.340654	val: 0.886167	test: 0.841937
MAE train: 0.266977	val: 0.687698	test: 0.680321

Epoch: 141
Loss: 0.13264907045023783
RMSE train: 0.294731	val: 0.863042	test: 0.821819
MAE train: 0.230816	val: 0.671222	test: 0.665084

Epoch: 142
Loss: 0.13414637478334562
RMSE train: 0.310396	val: 0.877327	test: 0.833555
MAE train: 0.246303	val: 0.685006	test: 0.676094All runs completed.


Epoch: 143
Loss: 0.14662051360521997
RMSE train: 0.318319	val: 0.884414	test: 0.823355
MAE train: 0.253474	val: 0.684126	test: 0.666818

Early stopping
Best (RMSE):	 train: 0.320318	val: 0.849518	test: 0.821447
Best (MAE):	 train: 0.251325	val: 0.662710	test: 0.662564

MAE train: 0.397651	val: 0.590229	test: 0.583010

Epoch: 84
Loss: 0.32123230397701263
RMSE train: 0.568910	val: 0.818253	test: 0.767577
MAE train: 0.443075	val: 0.625327	test: 0.613054

Epoch: 85
Loss: 0.2986599121774946
RMSE train: 0.528215	val: 0.786920	test: 0.741556
MAE train: 0.409730	val: 0.593132	test: 0.589385

Epoch: 86
Loss: 0.3092298571552549
RMSE train: 0.525563	val: 0.768265	test: 0.725995
MAE train: 0.410636	val: 0.588198	test: 0.581197

Epoch: 87
Loss: 0.30791244975158144
RMSE train: 0.507372	val: 0.766025	test: 0.726229
MAE train: 0.393063	val: 0.583689	test: 0.578443

Epoch: 88
Loss: 0.3206899570567267
RMSE train: 0.532810	val: 0.788287	test: 0.741207
MAE train: 0.414840	val: 0.607673	test: 0.592528

Epoch: 89
Loss: 0.31030580401420593
RMSE train: 0.503090	val: 0.772842	test: 0.729757
MAE train: 0.390333	val: 0.583011	test: 0.578127

Epoch: 90
Loss: 0.32215347247464315
RMSE train: 0.515871	val: 0.773435	test: 0.730929
MAE train: 0.401648	val: 0.588771	test: 0.574828

Epoch: 91
Loss: 0.3057367652654648
RMSE train: 0.518086	val: 0.795351	test: 0.739749
MAE train: 0.401515	val: 0.604859	test: 0.585798

Epoch: 92
Loss: 0.30342010302203043
RMSE train: 0.519696	val: 0.795172	test: 0.735854
MAE train: 0.403590	val: 0.600603	test: 0.584863

Epoch: 93
Loss: 0.2896212616137096
RMSE train: 0.501074	val: 0.775232	test: 0.720297
MAE train: 0.389899	val: 0.585036	test: 0.564537

Epoch: 94
Loss: 0.30074343191725866
RMSE train: 0.514965	val: 0.786712	test: 0.733703
MAE train: 0.402178	val: 0.600259	test: 0.584032

Epoch: 95
Loss: 0.310970385159765
RMSE train: 0.532934	val: 0.800459	test: 0.740861
MAE train: 0.415266	val: 0.610423	test: 0.586226

Epoch: 96
Loss: 0.31999356618949343
RMSE train: 0.495570	val: 0.782521	test: 0.725440
MAE train: 0.384118	val: 0.592307	test: 0.572350

Epoch: 97
Loss: 0.2870141086833818
RMSE train: 0.505384	val: 0.777622	test: 0.729548
MAE train: 0.391519	val: 0.593086	test: 0.573237

Epoch: 98
Loss: 0.2903302822794233
RMSE train: 0.486069	val: 0.754161	test: 0.719159
MAE train: 0.375791	val: 0.577314	test: 0.566897

Epoch: 99
Loss: 0.31742980969803675
RMSE train: 0.514320	val: 0.774071	test: 0.734748
MAE train: 0.400163	val: 0.591493	test: 0.581836

Epoch: 100
Loss: 0.30727859905787874
RMSE train: 0.544352	val: 0.786292	test: 0.762644
MAE train: 0.425279	val: 0.598309	test: 0.604040

Epoch: 101
Loss: 0.309493111712592
RMSE train: 0.504739	val: 0.776576	test: 0.749022
MAE train: 0.391187	val: 0.592107	test: 0.592794

Epoch: 102
Loss: 0.280057324894837
RMSE train: 0.494217	val: 0.775703	test: 0.735891
MAE train: 0.383979	val: 0.586875	test: 0.576959

Epoch: 103
Loss: 0.2951277047395706
RMSE train: 0.479723	val: 0.760114	test: 0.734369
MAE train: 0.373542	val: 0.574364	test: 0.577679

Epoch: 104
Loss: 0.32206624852759497
RMSE train: 0.502645	val: 0.776564	test: 0.737213
MAE train: 0.393765	val: 0.589604	test: 0.587340

Epoch: 105
Loss: 0.29154630111796515
RMSE train: 0.488097	val: 0.770715	test: 0.734024
MAE train: 0.379993	val: 0.586741	test: 0.573003

Epoch: 106
Loss: 0.2922835147806576
RMSE train: 0.481139	val: 0.766206	test: 0.725867
MAE train: 0.374692	val: 0.576189	test: 0.571187

Epoch: 107
Loss: 0.2786713072231838
RMSE train: 0.490819	val: 0.765883	test: 0.728711
MAE train: 0.381276	val: 0.576907	test: 0.570573

Epoch: 108
Loss: 0.28420488962105345
RMSE train: 0.464667	val: 0.769000	test: 0.734340
MAE train: 0.358098	val: 0.583720	test: 0.566172

Epoch: 109
Loss: 0.2833143557821001
RMSE train: 0.474103	val: 0.757980	test: 0.724650
MAE train: 0.369321	val: 0.572816	test: 0.574547

Epoch: 110
Loss: 0.2758092624800546
RMSE train: 0.466243	val: 0.758847	test: 0.721646
MAE train: 0.362684	val: 0.571438	test: 0.567745

Epoch: 111
Loss: 0.2682299486228398
RMSE train: 0.458176	val: 0.751130	test: 0.729161
MAE train: 0.356363	val: 0.568086	test: 0.575954

Epoch: 112
Loss: 0.27344283035823275
RMSE train: 0.476480	val: 0.776607	test: 0.742759
MAE train: 0.369339	val: 0.590036	test: 0.582867

Epoch: 113
Loss: 0.2710454006280218
RMSE train: 0.484736	val: 0.754954	test: 0.725202
MAE train: 0.378220	val: 0.579539	test: 0.573871

Epoch: 114
Loss: 0.26697784449372974
RMSE train: 0.475712	val: 0.769723	test: 0.745800
MAE train: 0.368196	val: 0.580512	test: 0.576369

Epoch: 115
Loss: 0.28947887995413374
RMSE train: 0.472200	val: 0.770577	test: 0.728939
MAE train: 0.366579	val: 0.582794	test: 0.568367

Epoch: 116
Loss: 0.26785353996923994
RMSE train: 0.492249	val: 0.764015	test: 0.731466
MAE train: 0.382708	val: 0.583150	test: 0.579528

Epoch: 117
Loss: 0.27160680294036865
RMSE train: 0.487845	val: 0.779825	test: 0.747625
MAE train: 0.378213	val: 0.595951	test: 0.587204

Epoch: 118
Loss: 0.2542473280004093
RMSE train: 0.517885	val: 0.805767	test: 0.766740
MAE train: 0.406651	val: 0.613906	test: 0.599891

Epoch: 119
Loss: 0.26015675919396536
RMSE train: 0.459181	val: 0.765140	test: 0.723383
MAE train: 0.355476	val: 0.580288	test: 0.564951

Epoch: 120
Loss: 0.2670970782637596
RMSE train: 0.476730	val: 0.775009	test: 0.739795
MAE train: 0.370154	val: 0.583643	test: 0.581150

Epoch: 121
Loss: 0.25530795220817837
RMSE train: 0.472625	val: 0.763437	test: 0.734174
MAE train: 0.370681	val: 0.581875	test: 0.587914

Epoch: 122
Loss: 0.2746345783982958
RMSE train: 0.466598	val: 0.760788	test: 0.719614
MAE train: 0.363616	val: 0.575528	test: 0.566275

Epoch: 123
Loss: 0.2615221398217337
RMSE train: 0.471410	val: 0.771101	test: 0.732712
MAE train: 0.366640	val: 0.586458	test: 0.576058

Epoch: 124
Loss: 0.26439120301178526
RMSE train: 0.484239	val: 0.779368	test: 0.755529
MAE train: 0.375939	val: 0.594527	test: 0.594263

Epoch: 125
Loss: 0.26160849737269537
RMSE train: 0.477247	val: 0.768259	test: 0.736738
MAE train: 0.370919	val: 0.586202	test: 0.578521

Epoch: 126
Loss: 0.2764760521905763
RMSE train: 0.462603	val: 0.761011	test: 0.740812
MAE train: 0.358280	val: 0.579344	test: 0.583273

Epoch: 127
Loss: 0.26461139002016615
RMSE train: 0.464722	val: 0.785116	test: 0.735862
MAE train: 0.359990	val: 0.594819	test: 0.576344

Epoch: 128
Loss: 0.2742215852652277
RMSE train: 0.484820	val: 0.784997	test: 0.749924
MAE train: 0.379596	val: 0.588581	test: 0.581140

Epoch: 129
Loss: 0.2976276416863714
RMSE train: 0.494018	val: 0.794580	test: 0.755048
MAE train: 0.383435	val: 0.599432	test: 0.592450

Epoch: 130
Loss: 0.26919119379350115
RMSE train: 0.483640	val: 0.790307	test: 0.745281
MAE train: 0.374140	val: 0.607400	test: 0.578960

Epoch: 131
Loss: 0.26832703607422964
RMSE train: 0.494724	val: 0.801189	test: 0.771028
MAE train: 0.384682	val: 0.609463	test: 0.598581

Epoch: 132
Loss: 0.2589373045733997
RMSE train: 0.463212	val: 0.759221	test: 0.741965
MAE train: 0.359695	val: 0.582025	test: 0.586302

Epoch: 133
Loss: 0.25986777565308977
RMSE train: 0.448432	val: 0.756785	test: 0.729206
MAE train: 0.348255	val: 0.572873	test: 0.575991

Epoch: 134
Loss: 0.26346192189625334
RMSE train: 0.467732	val: 0.769075	test: 0.730443
MAE train: 0.363336	val: 0.585891	test: 0.573665

Epoch: 135
Loss: 0.2511375333581652
RMSE train: 0.452069	val: 0.761376	test: 0.719658
MAE train: 0.352229	val: 0.577487	test: 0.567637

Epoch: 136
Loss: 0.25408236363104414
RMSE train: 0.487434	val: 0.780038	test: 0.752262
MAE train: 0.381749	val: 0.595790	test: 0.594141

Epoch: 137
Loss: 0.25379134182419094
RMSE train: 0.430795	val: 0.759157	test: 0.740019
MAE train: 0.331793	val: 0.574854	test: 0.577530

Epoch: 138
Loss: 0.24877907867942536
RMSE train: 0.456702	val: 0.751212	test: 0.723236
MAE train: 0.352583	val: 0.577021	test: 0.569417

Epoch: 139
Loss: 0.2570977711251804
RMSE train: 0.455026	val: 0.773863	test: 0.740765
MAE train: 0.352438	val: 0.581987	test: 0.579079

Epoch: 140
Loss: 0.24384543938296183
RMSE train: 0.446678	val: 0.766202	test: 0.737992
MAE train: 0.346785	val: 0.583005	test: 0.581071

Epoch: 141
Loss: 0.23418000446898596
RMSE train: 0.448510	val: 0.772766	test: 0.733789
MAE train: 0.349535	val: 0.591118	test: 0.574749

Epoch: 142
Loss: 0.22866716448749816
RMSE train: 0.433633	val: 0.766572	test: 0.726147
MAE train: 0.335622	val: 0.580792	test: 0.570577

Epoch: 143
Loss: 0.2477720986519541
RMSE train: 0.436683	val: 0.757484	test: 0.723518
MAE train: 0.339388	val: 0.579237	test: 0.569759
MAE train: 0.412956	val: 0.614575	test: 0.597329

Epoch: 84
Loss: 0.31289535548005787
RMSE train: 0.529523	val: 0.781704	test: 0.745563
MAE train: 0.411030	val: 0.590292	test: 0.593342

Epoch: 85
Loss: 0.32677597658974783
RMSE train: 0.556716	val: 0.828641	test: 0.801741
MAE train: 0.432437	val: 0.635016	test: 0.630080

Epoch: 86
Loss: 0.3345720682825361
RMSE train: 0.533101	val: 0.795623	test: 0.759756
MAE train: 0.415577	val: 0.601871	test: 0.602246

Epoch: 87
Loss: 0.3491045321737017
RMSE train: 0.506324	val: 0.767894	test: 0.747202
MAE train: 0.391732	val: 0.584089	test: 0.588759

Epoch: 88
Loss: 0.34278955842767445
RMSE train: 0.534659	val: 0.794866	test: 0.761828
MAE train: 0.415500	val: 0.604181	test: 0.599550

Epoch: 89
Loss: 0.31462109088897705
RMSE train: 0.524094	val: 0.792562	test: 0.762712
MAE train: 0.407471	val: 0.601835	test: 0.595041

Epoch: 90
Loss: 0.3138870256287711
RMSE train: 0.509911	val: 0.784947	test: 0.755957
MAE train: 0.395879	val: 0.599911	test: 0.601329

Epoch: 91
Loss: 0.3166533410549164
RMSE train: 0.523925	val: 0.789706	test: 0.767851
MAE train: 0.406463	val: 0.599543	test: 0.608332

Epoch: 92
Loss: 0.32652412567819866
RMSE train: 0.508335	val: 0.772590	test: 0.752909
MAE train: 0.396800	val: 0.586779	test: 0.597625

Epoch: 93
Loss: 0.30516763776540756
RMSE train: 0.502349	val: 0.773172	test: 0.753307
MAE train: 0.389582	val: 0.594616	test: 0.598417

Epoch: 94
Loss: 0.356056524174554
RMSE train: 0.510910	val: 0.786335	test: 0.748986
MAE train: 0.397116	val: 0.589459	test: 0.596483

Epoch: 95
Loss: 0.33314421347209383
RMSE train: 0.509258	val: 0.783331	test: 0.759329
MAE train: 0.395828	val: 0.596514	test: 0.604638

Epoch: 96
Loss: 0.29793025978973936
RMSE train: 0.519078	val: 0.800527	test: 0.767796
MAE train: 0.400843	val: 0.609215	test: 0.604746

Epoch: 97
Loss: 0.30847962626389097
RMSE train: 0.519831	val: 0.801846	test: 0.768576
MAE train: 0.402720	val: 0.605705	test: 0.606170

Epoch: 98
Loss: 0.3099107603941645
RMSE train: 0.498529	val: 0.776223	test: 0.757099
MAE train: 0.387222	val: 0.587339	test: 0.601656

Epoch: 99
Loss: 0.292928335922105
RMSE train: 0.488726	val: 0.771603	test: 0.748356
MAE train: 0.378782	val: 0.585096	test: 0.587569

Epoch: 100
Loss: 0.30018733122519087
RMSE train: 0.508946	val: 0.794852	test: 0.762195
MAE train: 0.395157	val: 0.600731	test: 0.607956

Epoch: 101
Loss: 0.28686518647841047
RMSE train: 0.508753	val: 0.799737	test: 0.751589
MAE train: 0.391784	val: 0.609428	test: 0.594081

Epoch: 102
Loss: 0.28666644224098753
RMSE train: 0.502986	val: 0.769757	test: 0.738199
MAE train: 0.390087	val: 0.582362	test: 0.582423

Epoch: 103
Loss: 0.30916476249694824
RMSE train: 0.537195	val: 0.818092	test: 0.786190
MAE train: 0.418933	val: 0.628422	test: 0.621212

Epoch: 104
Loss: 0.2948018227304731
RMSE train: 0.506197	val: 0.783590	test: 0.753733
MAE train: 0.391889	val: 0.602125	test: 0.586295

Epoch: 105
Loss: 0.292744663144861
RMSE train: 0.494625	val: 0.785114	test: 0.756341
MAE train: 0.384391	val: 0.599737	test: 0.597972

Epoch: 106
Loss: 0.3083104970199721
RMSE train: 0.482181	val: 0.767908	test: 0.749574
MAE train: 0.374768	val: 0.582235	test: 0.590372

Epoch: 107
Loss: 0.28691463598183226
RMSE train: 0.501069	val: 0.799354	test: 0.765413
MAE train: 0.388248	val: 0.605956	test: 0.601624

Epoch: 108
Loss: 0.2921006881764957
RMSE train: 0.485201	val: 0.773223	test: 0.750289
MAE train: 0.374124	val: 0.585568	test: 0.588923

Epoch: 109
Loss: 0.2867833354643413
RMSE train: 0.507785	val: 0.780361	test: 0.752497
MAE train: 0.394812	val: 0.591292	test: 0.590207

Epoch: 110
Loss: 0.3006301149725914
RMSE train: 0.504526	val: 0.786405	test: 0.764717
MAE train: 0.389715	val: 0.601959	test: 0.603093

Epoch: 111
Loss: 0.29272160679101944
RMSE train: 0.477986	val: 0.762031	test: 0.740457
MAE train: 0.371532	val: 0.579233	test: 0.579759

Epoch: 112
Loss: 0.28118174948862623
RMSE train: 0.511913	val: 0.796593	test: 0.754074
MAE train: 0.398758	val: 0.608419	test: 0.600542

Epoch: 113
Loss: 0.2825073259217398
RMSE train: 0.491703	val: 0.782806	test: 0.753930
MAE train: 0.380629	val: 0.592985	test: 0.592762

Epoch: 114
Loss: 0.28636327918086735
RMSE train: 0.492879	val: 0.800081	test: 0.773214
MAE train: 0.381009	val: 0.610088	test: 0.597934

Epoch: 115
Loss: 0.2818221982036318
RMSE train: 0.483303	val: 0.786546	test: 0.747308
MAE train: 0.376105	val: 0.596621	test: 0.587882

Epoch: 116
Loss: 0.2798069992235729
RMSE train: 0.476718	val: 0.772555	test: 0.735846
MAE train: 0.370329	val: 0.586425	test: 0.586395

Epoch: 117
Loss: 0.2801613264850208
RMSE train: 0.481237	val: 0.778120	test: 0.751227
MAE train: 0.372539	val: 0.597033	test: 0.586732

Epoch: 118
Loss: 0.28124979457684923
RMSE train: 0.482815	val: 0.776740	test: 0.747362
MAE train: 0.374412	val: 0.589273	test: 0.587125

Epoch: 119
Loss: 0.2692467155201094
RMSE train: 0.465205	val: 0.756400	test: 0.741380
MAE train: 0.360611	val: 0.578074	test: 0.582766

Epoch: 120
Loss: 0.2738386243581772
RMSE train: 0.462882	val: 0.764767	test: 0.738965
MAE train: 0.356726	val: 0.582829	test: 0.578191

Epoch: 121
Loss: 0.2666408675057547
RMSE train: 0.496497	val: 0.787769	test: 0.751444
MAE train: 0.386412	val: 0.604469	test: 0.582878

Epoch: 122
Loss: 0.2701704981071608
RMSE train: 0.464820	val: 0.762794	test: 0.746945
MAE train: 0.356956	val: 0.577448	test: 0.577393

Epoch: 123
Loss: 0.2620492268885885
RMSE train: 0.481027	val: 0.774389	test: 0.742268
MAE train: 0.372924	val: 0.594033	test: 0.580463

Epoch: 124
Loss: 0.25819081387349535
RMSE train: 0.453640	val: 0.753835	test: 0.737900
MAE train: 0.351236	val: 0.572780	test: 0.574062

Epoch: 125
Loss: 0.2804517980132784
RMSE train: 0.470407	val: 0.769406	test: 0.751326
MAE train: 0.366735	val: 0.589583	test: 0.588925

Epoch: 126
Loss: 0.26625570335558485
RMSE train: 0.484181	val: 0.813100	test: 0.776722
MAE train: 0.374909	val: 0.615990	test: 0.605338

Epoch: 127
Loss: 0.2736461822475706
RMSE train: 0.511035	val: 0.795497	test: 0.760343
MAE train: 0.397874	val: 0.606812	test: 0.601114

Epoch: 128
Loss: 0.2629563489130565
RMSE train: 0.457551	val: 0.767607	test: 0.740314
MAE train: 0.353320	val: 0.585215	test: 0.583328

Epoch: 129
Loss: 0.2598585124526705
RMSE train: 0.448077	val: 0.768491	test: 0.736053
MAE train: 0.345605	val: 0.581740	test: 0.574481

Epoch: 130
Loss: 0.27186714857816696
RMSE train: 0.469389	val: 0.784816	test: 0.732315
MAE train: 0.364668	val: 0.592635	test: 0.574463

Epoch: 131
Loss: 0.26005925451006207
RMSE train: 0.468824	val: 0.792777	test: 0.741941
MAE train: 0.362490	val: 0.605708	test: 0.582575

Epoch: 132
Loss: 0.2545233347586223
RMSE train: 0.448459	val: 0.768398	test: 0.726258
MAE train: 0.347720	val: 0.585355	test: 0.572672

Epoch: 133
Loss: 0.2568215344633375
RMSE train: 0.456896	val: 0.772594	test: 0.730898
MAE train: 0.354633	val: 0.592629	test: 0.576715

Epoch: 134
Loss: 0.2520420178771019
RMSE train: 0.455851	val: 0.760355	test: 0.725089
MAE train: 0.356145	val: 0.578624	test: 0.571748

Epoch: 135
Loss: 0.2564297009791647
RMSE train: 0.440876	val: 0.755346	test: 0.724639
MAE train: 0.341619	val: 0.572428	test: 0.571831

Epoch: 136
Loss: 0.2453721155013357
RMSE train: 0.473852	val: 0.785247	test: 0.750931
MAE train: 0.366450	val: 0.596204	test: 0.587591

Epoch: 137
Loss: 0.24829577335289546
RMSE train: 0.488006	val: 0.820428	test: 0.770613
MAE train: 0.379250	val: 0.626389	test: 0.606280

Epoch: 138
Loss: 0.25016855342047556
RMSE train: 0.466115	val: 0.778399	test: 0.739488
MAE train: 0.362392	val: 0.596404	test: 0.584788

Epoch: 139
Loss: 0.2485420416508402
RMSE train: 0.454962	val: 0.763708	test: 0.744135
MAE train: 0.354345	val: 0.577427	test: 0.588878

Epoch: 140
Loss: 0.2624029368162155
RMSE train: 0.462897	val: 0.763047	test: 0.745740
MAE train: 0.363238	val: 0.588051	test: 0.589826

Epoch: 141
Loss: 0.25262593903711866
RMSE train: 0.470913	val: 0.803398	test: 0.766656
MAE train: 0.366891	val: 0.616679	test: 0.600236

Epoch: 142
Loss: 0.25682064571550917
RMSE train: 0.454034	val: 0.788191	test: 0.746512
MAE train: 0.354228	val: 0.602493	test: 0.588772

Epoch: 143
Loss: 0.2563378768307822
RMSE train: 0.439680	val: 0.767123	test: 0.740108
MAE train: 0.340236	val: 0.589984	test: 0.582459MAE train: 0.249140	val: 0.797565	test: 0.729944

Epoch: 143
Loss: 0.11399890588862556
RMSE train: 0.302928	val: 1.037816	test: 0.954210
MAE train: 0.238457	val: 0.821031	test: 0.751037

Epoch: 144
Loss: 0.1077333933540753
RMSE train: 0.260923	val: 1.034706	test: 0.950261
MAE train: 0.206590	val: 0.820299	test: 0.748208

Epoch: 145
Loss: 0.10958054129566465
RMSE train: 0.293097	val: 1.046126	test: 0.970090
MAE train: 0.230469	val: 0.831062	test: 0.766328

Epoch: 146
Loss: 0.10454115590878896
RMSE train: 0.265371	val: 1.006238	test: 0.931196
MAE train: 0.209499	val: 0.798747	test: 0.738890

Epoch: 147
Loss: 0.1029716432094574
RMSE train: 0.262780	val: 0.967884	test: 0.902449
MAE train: 0.207055	val: 0.767274	test: 0.716643

Epoch: 148
Loss: 0.11444233730435371
RMSE train: 0.296852	val: 1.040142	test: 0.961066
MAE train: 0.236124	val: 0.823129	test: 0.759721

Epoch: 149
Loss: 0.09951405599713326
RMSE train: 0.277446	val: 1.031145	test: 0.956486
MAE train: 0.216741	val: 0.820133	test: 0.750763

Early stopping
Best (RMSE):	 train: 0.274724	val: 0.951785	test: 0.885262
Best (MAE):	 train: 0.213616	val: 0.763076	test: 0.704348
All runs completed.


Epoch: 143
Loss: 0.1397942735680512
RMSE train: 0.376329	val: 0.896014	test: 0.830917
MAE train: 0.300658	val: 0.706455	test: 0.649157

Epoch: 144
Loss: 0.12990804176245416
RMSE train: 0.276392	val: 0.824531	test: 0.792066
MAE train: 0.220336	val: 0.642315	test: 0.624927

Epoch: 145
Loss: 0.12707876254405295
RMSE train: 0.305122	val: 0.833487	test: 0.807103
MAE train: 0.243503	val: 0.659477	test: 0.634998

Epoch: 146
Loss: 0.12183244526386261
RMSE train: 0.308109	val: 0.846113	test: 0.798498
MAE train: 0.241533	val: 0.661706	test: 0.624983

Epoch: 147
Loss: 0.12825113322053636
RMSE train: 0.294151	val: 0.846103	test: 0.797244
MAE train: 0.232368	val: 0.661264	test: 0.631385

Epoch: 148
Loss: 0.1395379432610103
RMSE train: 0.288961	val: 0.857235	test: 0.819159
MAE train: 0.228406	val: 0.676957	test: 0.650221

Epoch: 149
Loss: 0.1236257196537086
RMSE train: 0.295322	val: 0.823718	test: 0.797043
MAE train: 0.234055	val: 0.647222	test: 0.632995

Epoch: 150
Loss: 0.12469652827296938
RMSE train: 0.276050	val: 0.845514	test: 0.804410
MAE train: 0.217072	val: 0.664815	test: 0.637101

Epoch: 151
Loss: 0.12192114549023765
RMSE train: 0.305924	val: 0.841559	test: 0.799155
MAE train: 0.242971	val: 0.654021	test: 0.633162

Epoch: 152
Loss: 0.1339763631778104
RMSE train: 0.378543	val: 0.882184	test: 0.824197
MAE train: 0.303930	val: 0.692634	test: 0.650414

Epoch: 153
Loss: 0.13108387004051889
RMSE train: 0.287048	val: 0.844957	test: 0.807110
MAE train: 0.225738	val: 0.659827	test: 0.638122

Epoch: 154
Loss: 0.12589028211576597
RMSE train: 0.288550	val: 0.854106	test: 0.808463
MAE train: 0.227808	val: 0.667962	test: 0.641203

Epoch: 155
Loss: 0.11732709567461695
RMSE train: 0.342215	val: 0.862867	test: 0.810986
MAE train: 0.266841	val: 0.679996	test: 0.638175

Epoch: 156
Loss: 0.12275560040559087
RMSE train: 0.287867	val: 0.832035	test: 0.792602
MAE train: 0.226981	val: 0.650524	test: 0.628137

Epoch: 157
Loss: 0.12089857139757701
RMSE train: 0.305314	val: 0.848574	test: 0.816984
MAE train: 0.243675	val: 0.663509	test: 0.655086

Epoch: 158
Loss: 0.12655526931796754
RMSE train: 0.345772	val: 0.891201	test: 0.826456
MAE train: 0.275103	val: 0.699342	test: 0.649957

Early stopping
Best (RMSE):	 train: 0.334515	val: 0.822614	test: 0.798052
Best (MAE):	 train: 0.265431	val: 0.645093	test: 0.635714
All runs completed.


Epoch: 144
Loss: 0.24225554402385438
RMSE train: 0.445560	val: 0.781854	test: 0.736304
MAE train: 0.344323	val: 0.595714	test: 0.572061

Epoch: 145
Loss: 0.22452718977417266
RMSE train: 0.450354	val: 0.772566	test: 0.729451
MAE train: 0.350413	val: 0.591059	test: 0.570759

Epoch: 146
Loss: 0.24545225713934218
RMSE train: 0.444456	val: 0.782693	test: 0.745190
MAE train: 0.346296	val: 0.602140	test: 0.581341

Epoch: 147
Loss: 0.24591810681990214
RMSE train: 0.459067	val: 0.794987	test: 0.741620
MAE train: 0.356596	val: 0.610115	test: 0.579567

Epoch: 148
Loss: 0.2546628915837833
RMSE train: 0.456099	val: 0.781811	test: 0.741718
MAE train: 0.354472	val: 0.599670	test: 0.579817

Epoch: 149
Loss: 0.25516401337725775
RMSE train: 0.442058	val: 0.768651	test: 0.733902
MAE train: 0.341411	val: 0.590693	test: 0.577368

Epoch: 150
Loss: 0.2571980261376926
RMSE train: 0.460193	val: 0.778838	test: 0.734804
MAE train: 0.356565	val: 0.597457	test: 0.574842

Epoch: 151
Loss: 0.2780117190309933
RMSE train: 0.443703	val: 0.772181	test: 0.736370
MAE train: 0.344304	val: 0.588655	test: 0.564800

Epoch: 152
Loss: 0.25001325245414463
RMSE train: 0.436968	val: 0.776667	test: 0.728626
MAE train: 0.339961	val: 0.600055	test: 0.569979

Epoch: 153
Loss: 0.22865389713219234
RMSE train: 0.443279	val: 0.755105	test: 0.726613
MAE train: 0.344533	val: 0.570728	test: 0.568150

Epoch: 154
Loss: 0.23392708706004278
RMSE train: 0.424741	val: 0.775784	test: 0.739825
MAE train: 0.326844	val: 0.583485	test: 0.577429

Epoch: 155
Loss: 0.2236178261893136
RMSE train: 0.467209	val: 0.796478	test: 0.734495
MAE train: 0.366522	val: 0.612196	test: 0.577240

Epoch: 156
Loss: 0.22649881882326944
RMSE train: 0.415400	val: 0.753888	test: 0.717641
MAE train: 0.320868	val: 0.572137	test: 0.555961

Epoch: 157
Loss: 0.22658355640513556
RMSE train: 0.425845	val: 0.762224	test: 0.723663
MAE train: 0.330435	val: 0.581986	test: 0.563966

Epoch: 158
Loss: 0.2304665446281433
RMSE train: 0.426625	val: 0.773210	test: 0.728990
MAE train: 0.329788	val: 0.590785	test: 0.566415

Epoch: 159
Loss: 0.2259723533477102
RMSE train: 0.454331	val: 0.791443	test: 0.746196
MAE train: 0.354055	val: 0.607717	test: 0.577484

Early stopping
Best (RMSE):	 train: 0.453640	val: 0.753835	test: 0.737900
Best (MAE):	 train: 0.351236	val: 0.572780	test: 0.574062


Epoch: 144
Loss: 0.24813751450606755
RMSE train: 0.442095	val: 0.763007	test: 0.733009
MAE train: 0.341958	val: 0.583835	test: 0.574983

Epoch: 145
Loss: 0.25067512691020966
RMSE train: 0.447058	val: 0.744578	test: 0.722784
MAE train: 0.346472	val: 0.567447	test: 0.569792

Epoch: 146
Loss: 0.2308363180075373
RMSE train: 0.455018	val: 0.777020	test: 0.740801
MAE train: 0.352157	val: 0.593170	test: 0.582681

Epoch: 147
Loss: 0.22793106521878922
RMSE train: 0.441018	val: 0.772770	test: 0.741446
MAE train: 0.342606	val: 0.589680	test: 0.574939

Epoch: 148
Loss: 0.22640174627304077
RMSE train: 0.439466	val: 0.766482	test: 0.725245
MAE train: 0.341064	val: 0.586045	test: 0.568254

Epoch: 149
Loss: 0.24330995338303701
RMSE train: 0.450654	val: 0.781687	test: 0.736821
MAE train: 0.348536	val: 0.597630	test: 0.576642

Epoch: 150
Loss: 0.24045470250504358
RMSE train: 0.451139	val: 0.768679	test: 0.743624
MAE train: 0.350345	val: 0.587107	test: 0.584542

Epoch: 151
Loss: 0.24949290496962412
RMSE train: 0.446660	val: 0.768244	test: 0.727309
MAE train: 0.345941	val: 0.585125	test: 0.568889

Epoch: 152
Loss: 0.2367145727787699
RMSE train: 0.456689	val: 0.778850	test: 0.749435
MAE train: 0.353845	val: 0.587958	test: 0.581297

Epoch: 153
Loss: 0.2337367076958929
RMSE train: 0.442705	val: 0.771058	test: 0.725631
MAE train: 0.341582	val: 0.585684	test: 0.570564

Epoch: 154
Loss: 0.23233283630439214
RMSE train: 0.439501	val: 0.766895	test: 0.732157
MAE train: 0.340444	val: 0.577345	test: 0.571653

Epoch: 155
Loss: 0.2250164481145995
RMSE train: 0.441622	val: 0.773842	test: 0.722504
MAE train: 0.340342	val: 0.584910	test: 0.566181

Epoch: 156
Loss: 0.24018256153379167
RMSE train: 0.434807	val: 0.771415	test: 0.718542
MAE train: 0.336384	val: 0.585730	test: 0.564207

Epoch: 157
Loss: 0.22766525511230742
RMSE train: 0.446865	val: 0.755475	test: 0.734910
MAE train: 0.345468	val: 0.574387	test: 0.575253

Epoch: 158
Loss: 0.23327852679150446
RMSE train: 0.440201	val: 0.766131	test: 0.732427
MAE train: 0.341734	val: 0.584877	test: 0.578064

Epoch: 159
Loss: 0.22458373648779734
RMSE train: 0.433263	val: 0.762755	test: 0.723767
MAE train: 0.334522	val: 0.581912	test: 0.568432

Epoch: 160
Loss: 0.2262853511742183
RMSE train: 0.434747	val: 0.766338	test: 0.720213
MAE train: 0.337958	val: 0.582607	test: 0.567151

Epoch: 161
Loss: 0.22931833565235138
RMSE train: 0.427935	val: 0.762390	test: 0.726923
MAE train: 0.330884	val: 0.578992	test: 0.572680

Epoch: 162
Loss: 0.21202229176248824
RMSE train: 0.448910	val: 0.787765	test: 0.738173
MAE train: 0.349571	val: 0.598963	test: 0.583242

Epoch: 163
Loss: 0.21686140767165593
RMSE train: 0.420651	val: 0.751572	test: 0.726144
MAE train: 0.324286	val: 0.566120	test: 0.569855

Epoch: 164
Loss: 0.21957519756896154
RMSE train: 0.425633	val: 0.750975	test: 0.722920
MAE train: 0.329701	val: 0.571233	test: 0.572121

Epoch: 165
Loss: 0.21487740320818766
RMSE train: 0.447130	val: 0.765277	test: 0.722561
MAE train: 0.346245	val: 0.583739	test: 0.565305

Epoch: 166
Loss: 0.21963075016226088
RMSE train: 0.432062	val: 0.773322	test: 0.738135
MAE train: 0.338076	val: 0.587153	test: 0.576774

Epoch: 167
Loss: 0.22730862668582372
RMSE train: 0.444753	val: 0.784278	test: 0.742145
MAE train: 0.344854	val: 0.592424	test: 0.578267

Epoch: 168
Loss: 0.21403535349028452
RMSE train: 0.433199	val: 0.772912	test: 0.736386
MAE train: 0.335720	val: 0.578554	test: 0.574855

Epoch: 169
Loss: 0.21045220217534474
RMSE train: 0.422664	val: 0.766439	test: 0.723725
MAE train: 0.327622	val: 0.575028	test: 0.563069

Epoch: 170
Loss: 0.22501099854707718
RMSE train: 0.427121	val: 0.765492	test: 0.730046
MAE train: 0.332173	val: 0.575960	test: 0.563515

Epoch: 171
Loss: 0.2160280623606273
RMSE train: 0.410163	val: 0.762251	test: 0.726553
MAE train: 0.317987	val: 0.576937	test: 0.565129

Epoch: 172
Loss: 0.22704752747501647
RMSE train: 0.412063	val: 0.750587	test: 0.728206
MAE train: 0.319923	val: 0.571338	test: 0.563799

Epoch: 173
Loss: 0.21853877497570856
RMSE train: 0.463788	val: 0.788029	test: 0.762904
MAE train: 0.365687	val: 0.594646	test: 0.586242

Epoch: 174
Loss: 0.22211878533874238
RMSE train: 0.452844	val: 0.785425	test: 0.760971
MAE train: 0.352355	val: 0.598994	test: 0.596375

Epoch: 175
Loss: 0.2069291259561266
RMSE train: 0.422471	val: 0.763073	test: 0.729859
MAE train: 0.326616	val: 0.577377	test: 0.566358

Epoch: 176
Loss: 0.19815490820578166
RMSE train: 0.450700	val: 0.781512	test: 0.747991
MAE train: 0.352336	val: 0.597424	test: 0.582371

Epoch: 177
Loss: 0.20540752155440195
RMSE train: 0.439534	val: 0.769781	test: 0.739628
MAE train: 0.342213	val: 0.592080	test: 0.577521

Epoch: 178
Loss: 0.21291256376675197
RMSE train: 0.405614	val: 0.760311	test: 0.729278
MAE train: 0.313655	val: 0.575283	test: 0.569411

Epoch: 179
Loss: 0.22620138100215367
RMSE train: 0.438902	val: 0.764736	test: 0.734377
MAE train: 0.342507	val: 0.582420	test: 0.576826

Epoch: 180
Loss: 0.21482492131846292
RMSE train: 0.413710	val: 0.753612	test: 0.731586
MAE train: 0.320648	val: 0.574964	test: 0.572034

Early stopping
Best (RMSE):	 train: 0.447058	val: 0.744578	test: 0.722784
Best (MAE):	 train: 0.346472	val: 0.567447	test: 0.569792
All runs completed.
