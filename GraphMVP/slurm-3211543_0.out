>>> Starting run for dataset: esol
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running RANDOM configs_static_noise_experiments/GraphMVP/esol/noise=0.0.yml on cuda:0
Running RANDOM configs_static_noise_experiments/GraphMVP/esol/noise=0.05.yml on cuda:1
Running RANDOM configs_static_noise_experiments/GraphMVP/esol/noise=0.1.yml on cuda:2
Running RANDOM configs_static_noise_experiments/GraphMVP/esol/noise=0.2.yml on cuda:3
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphMVP/esol/noise=0.0.yml --runseed 4 --device cuda:0
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphMVP/esol/noise=0.0.yml --runseed 5 --device cuda:0
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphMVP/esol/noise=0.0.yml --runseed 6 --device cuda:0
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphMVP/esol/noise=0.1.yml --runseed 4 --device cuda:2
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphMVP/esol/noise=0.05.yml --runseed 4 --device cuda:1
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphMVP/esol/noise=0.1.yml --runseed 5 --device cuda:2
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphMVP/esol/noise=0.05.yml --runseed 5 --device cuda:1
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphMVP/esol/noise=0.2.yml --runseed 4 --device cuda:3
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphMVP/esol/noise=0.1.yml --runseed 6 --device cuda:2
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphMVP/esol/noise=0.05.yml --runseed 6 --device cuda:1
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphMVP/esol/noise=0.2.yml --runseed 5 --device cuda:3
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphMVP/esol/noise=0.2.yml --runseed 6 --device cuda:3
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/esol/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/static-noise/GraphMVP/esol/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/esol/noise=0.0/esol_scaff_6_26-05_11-13-30  ]
[ Using Seed :  6  ]
[ Using device :  cuda:0  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 12.013425827026367
RMSE train: 3.287642	val: 4.152284	test: 4.194670
MAE train: 2.737678	val: 3.788735	test: 3.798206

Epoch: 2
Loss: 10.944839715957642
RMSE train: 3.094876	val: 4.088199	test: 4.153102
MAE train: 2.548657	val: 3.800846	test: 3.834965

Epoch: 3
Loss: 9.11015260219574
RMSE train: 2.880036	val: 4.054607	test: 4.143773
MAE train: 2.307229	val: 3.757736	test: 3.824340

Epoch: 4
Loss: 8.202714681625366
RMSE train: 2.483980	val: 3.514706	test: 3.630819
MAE train: 1.959954	val: 3.231429	test: 3.316988

Epoch: 5
Loss: 7.654040455818176
RMSE train: 2.165856	val: 3.017853	test: 3.145538
MAE train: 1.703778	val: 2.737994	test: 2.810584

Epoch: 6
Loss: 7.195631265640259
RMSE train: 2.227648	val: 3.229601	test: 3.308113
MAE train: 1.757264	val: 2.953435	test: 2.974850

Epoch: 7
Loss: 6.178324222564697
RMSE train: 2.356909	val: 3.520772	test: 3.581348
MAE train: 1.879876	val: 3.276740	test: 3.274083

Epoch: 8
Loss: 5.614045977592468
RMSE train: 2.168349	val: 3.142843	test: 3.240296
MAE train: 1.728642	val: 2.889485	test: 2.928398

Epoch: 9
Loss: 5.132590413093567
RMSE train: 2.068046	val: 2.887307	test: 3.038460
MAE train: 1.681678	val: 2.655355	test: 2.722018

Epoch: 10
Loss: 4.49704372882843
RMSE train: 2.041355	val: 2.922455	test: 3.100541
MAE train: 1.680781	val: 2.706894	test: 2.806007

Epoch: 11
Loss: 4.084192991256714
RMSE train: 1.888042	val: 2.778590	test: 2.946482
MAE train: 1.533662	val: 2.564188	test: 2.658329

Epoch: 12
Loss: 3.580931842327118
RMSE train: 1.693188	val: 2.489906	test: 2.649793
MAE train: 1.361892	val: 2.261214	test: 2.344951

Epoch: 13
Loss: 3.1222949028015137
RMSE train: 1.649264	val: 2.399660	test: 2.584971
MAE train: 1.345899	val: 2.169118	test: 2.282128

Epoch: 14
Loss: 2.581003963947296
RMSE train: 1.590217	val: 2.322498	test: 2.541353
MAE train: 1.279482	val: 2.090389	test: 2.229448

Epoch: 15
Loss: 2.1304692327976227
RMSE train: 1.324108	val: 1.993696	test: 2.194428
MAE train: 1.017020	val: 1.747974	test: 1.887672

Epoch: 16
Loss: 1.7459320425987244
RMSE train: 1.185302	val: 1.754060	test: 1.863366
MAE train: 0.952853	val: 1.488516	test: 1.569725

Epoch: 17
Loss: 1.4715063124895096
RMSE train: 1.210008	val: 1.841032	test: 1.961905
MAE train: 0.960418	val: 1.608117	test: 1.673760

Epoch: 18
Loss: 1.3155412673950195
RMSE train: 1.163053	val: 1.734201	test: 1.892941
MAE train: 0.902317	val: 1.523650	test: 1.597183

Epoch: 19
Loss: 1.1140520721673965
RMSE train: 0.980208	val: 1.528234	test: 1.617531
MAE train: 0.770611	val: 1.283203	test: 1.330670

Epoch: 20
Loss: 0.9621985703706741
RMSE train: 0.957600	val: 1.595869	test: 1.656229
MAE train: 0.732149	val: 1.320145	test: 1.354368

Epoch: 21
Loss: 0.8621240258216858
RMSE train: 0.906273	val: 1.427795	test: 1.560672
MAE train: 0.687242	val: 1.201950	test: 1.257057

Epoch: 22
Loss: 0.7456960380077362
RMSE train: 0.885407	val: 1.438995	test: 1.589750
MAE train: 0.663250	val: 1.229835	test: 1.306856

Epoch: 23
Loss: 0.7399446219205856Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/esol/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/static-noise/GraphMVP/esol/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/esol/noise=0.0/esol_scaff_4_26-05_11-13-30  ]
[ Using Seed :  4  ]
[ Using device :  cuda:0  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 11.848098278045654
RMSE train: 3.504769	val: 4.285882	test: 4.350394
MAE train: 2.964770	val: 3.952944	test: 3.940349

Epoch: 2
Loss: 10.087511777877808
RMSE train: 3.317299	val: 4.229819	test: 4.247360
MAE train: 2.850012	val: 3.986496	test: 3.929832

Epoch: 3
Loss: 9.043285608291626
RMSE train: 3.102683	val: 4.144528	test: 4.173890
MAE train: 2.655112	val: 3.891683	test: 3.863708

Epoch: 4
Loss: 8.08313000202179
RMSE train: 2.761812	val: 3.837593	test: 3.938312
MAE train: 2.300364	val: 3.574585	test: 3.642767

Epoch: 5
Loss: 7.473158359527588
RMSE train: 2.436568	val: 3.344256	test: 3.486414
MAE train: 2.001998	val: 3.053497	test: 3.157871

Epoch: 6
Loss: 6.657300591468811
RMSE train: 2.341842	val: 3.220108	test: 3.372441
MAE train: 1.931446	val: 2.934700	test: 3.080997

Epoch: 7
Loss: 6.0292675495147705
RMSE train: 2.295888	val: 3.201368	test: 3.342616
MAE train: 1.892972	val: 2.930884	test: 3.061532

Epoch: 8
Loss: 5.2482699155807495
RMSE train: 2.311893	val: 3.225752	test: 3.356024
MAE train: 1.945806	val: 2.983712	test: 3.086066

Epoch: 9
Loss: 4.729094624519348
RMSE train: 2.256710	val: 3.193030	test: 3.317902
MAE train: 1.886748	val: 2.943512	test: 3.055594

Epoch: 10
Loss: 4.399214088916779
RMSE train: 2.126580	val: 2.977009	test: 3.119862
MAE train: 1.776099	val: 2.739736	test: 2.847498

Epoch: 11
Loss: 3.834508180618286
RMSE train: 1.957880	val: 2.711462	test: 2.864871
MAE train: 1.631369	val: 2.462122	test: 2.577220

Epoch: 12
Loss: 3.345960557460785
RMSE train: 1.939694	val: 2.767146	test: 2.904927
MAE train: 1.587839	val: 2.513808	test: 2.629131

Epoch: 13
Loss: 2.9320985078811646
RMSE train: 1.833878	val: 2.560662	test: 2.681493
MAE train: 1.531211	val: 2.310451	test: 2.401389

Epoch: 14
Loss: 2.4084951877593994
RMSE train: 1.670105	val: 2.380306	test: 2.529350
MAE train: 1.381793	val: 2.130193	test: 2.254175

Epoch: 15
Loss: 2.01970574259758
RMSE train: 1.514253	val: 2.181079	test: 2.334125
MAE train: 1.231739	val: 1.940375	test: 2.058843

Epoch: 16
Loss: 1.6757729351520538
RMSE train: 1.384201	val: 2.016247	test: 2.142117
MAE train: 1.125071	val: 1.763956	test: 1.874443

Epoch: 17
Loss: 1.4183842837810516
RMSE train: 1.243911	val: 1.805092	test: 1.935401
MAE train: 1.007191	val: 1.555798	test: 1.649567

Epoch: 18
Loss: 1.206439420580864
RMSE train: 1.113105	val: 1.659837	test: 1.800398
MAE train: 0.894196	val: 1.422711	test: 1.505707

Epoch: 19
Loss: 1.0768889635801315
RMSE train: 1.095748	val: 1.719268	test: 1.824959
MAE train: 0.866630	val: 1.468564	test: 1.552522

Epoch: 20
Loss: 0.8806468844413757
RMSE train: 1.029928	val: 1.605490	test: 1.672835
MAE train: 0.787381	val: 1.338798	test: 1.384346

Epoch: 21
Loss: 0.7961992919445038
RMSE train: 0.922037	val: 1.463796	test: 1.552371
MAE train: 0.699770	val: 1.217894	test: 1.256118

Epoch: 22
Loss: 0.7045280188322067
RMSE train: 0.915603	val: 1.510492	test: 1.567295
MAE train: 0.695344	val: 1.259934	test: 1.282627

Epoch: 23
Loss: 0.6688317656517029Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/esol/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/static-noise/GraphMVP/esol/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/esol/noise=0.0/esol_scaff_5_26-05_11-13-30  ]
[ Using Seed :  5  ]
[ Using device :  cuda:0  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 12.268494844436646
RMSE train: 3.372539	val: 4.110158	test: 4.193553
MAE train: 2.881417	val: 3.809967	test: 3.845572

Epoch: 2
Loss: 10.68088173866272
RMSE train: 3.266416	val: 4.251469	test: 4.306768
MAE train: 2.806945	val: 4.006688	test: 4.002845

Epoch: 3
Loss: 9.733617305755615
RMSE train: 3.165768	val: 4.313063	test: 4.393805
MAE train: 2.700244	val: 4.049757	test: 4.102864

Epoch: 4
Loss: 8.378237128257751
RMSE train: 2.955562	val: 4.131770	test: 4.246305
MAE train: 2.472542	val: 3.864736	test: 3.960170

Epoch: 5
Loss: 8.077040791511536
RMSE train: 2.572035	val: 3.518940	test: 3.654056
MAE train: 2.129063	val: 3.228164	test: 3.324465

Epoch: 6
Loss: 7.08423113822937
RMSE train: 2.447214	val: 3.298948	test: 3.438286
MAE train: 2.033289	val: 3.034750	test: 3.131822

Epoch: 7
Loss: 6.549736380577087
RMSE train: 2.403644	val: 3.279896	test: 3.408922
MAE train: 1.993699	val: 3.020425	test: 3.114637

Epoch: 8
Loss: 5.8519065380096436
RMSE train: 2.306187	val: 3.222171	test: 3.348215
MAE train: 1.929542	val: 2.975835	test: 3.064963

Epoch: 9
Loss: 5.513926982879639
RMSE train: 2.259097	val: 3.183759	test: 3.340209
MAE train: 1.885544	val: 2.941929	test: 3.047553

Epoch: 10
Loss: 4.84114933013916
RMSE train: 2.117317	val: 2.957673	test: 3.147498
MAE train: 1.764998	val: 2.702939	test: 2.846109

Epoch: 11
Loss: 4.504296243190765
RMSE train: 2.005255	val: 2.773932	test: 2.950973
MAE train: 1.676858	val: 2.514328	test: 2.669312

Epoch: 12
Loss: 3.653398036956787
RMSE train: 1.848667	val: 2.553929	test: 2.680828
MAE train: 1.530156	val: 2.264510	test: 2.389490

Epoch: 13
Loss: 3.204024612903595
RMSE train: 1.830854	val: 2.545221	test: 2.650404
MAE train: 1.499181	val: 2.272033	test: 2.369586

Epoch: 14
Loss: 2.789120376110077
RMSE train: 1.771742	val: 2.529589	test: 2.645266
MAE train: 1.457624	val: 2.278287	test: 2.369803

Epoch: 15
Loss: 2.4018355011940002
RMSE train: 1.720948	val: 2.431595	test: 2.564786
MAE train: 1.434803	val: 2.186296	test: 2.284950

Epoch: 16
Loss: 1.9775319993495941
RMSE train: 1.677625	val: 2.336269	test: 2.487157
MAE train: 1.397498	val: 2.082520	test: 2.201792

Epoch: 17
Loss: 1.682628571987152
RMSE train: 1.538495	val: 2.096869	test: 2.236862
MAE train: 1.292921	val: 1.828527	test: 1.946279

Epoch: 18
Loss: 1.4488577246665955
RMSE train: 1.498625	val: 2.096966	test: 2.259421
MAE train: 1.244177	val: 1.835912	test: 1.970154

Epoch: 19
Loss: 1.2972935140132904
RMSE train: 1.319162	val: 1.921757	test: 2.060693
MAE train: 1.064473	val: 1.659179	test: 1.755737

Epoch: 20
Loss: 1.056823804974556
RMSE train: 1.204255	val: 1.899467	test: 1.940573
MAE train: 0.950930	val: 1.604980	test: 1.637524

Epoch: 21
Loss: 0.8874190300703049
RMSE train: 1.100205	val: 1.713469	test: 1.754854
MAE train: 0.864839	val: 1.471002	test: 1.475200

Epoch: 22
Loss: 0.7863977700471878
RMSE train: 0.987543	val: 1.563764	test: 1.614224
MAE train: 0.758147	val: 1.319014	test: 1.320447

Epoch: 23
Loss: 0.6724728494882584Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/esol/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/static-noise/GraphMVP/esol/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/esol/noise=0.05/esol_scaff_4_26-05_11-13-30  ]
[ Using Seed :  4  ]
[ Using device :  cuda:1  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 11.887118101119995
RMSE train: 3.453567	val: 4.261499	test: 4.347960
MAE train: 2.822049	val: 3.823665	test: 3.835748

Epoch: 2
Loss: 10.604169368743896
RMSE train: 3.216709	val: 4.107570	test: 4.193098
MAE train: 2.609237	val: 3.753906	test: 3.799996

Epoch: 3
Loss: 9.755890846252441
RMSE train: 3.042936	val: 3.980524	test: 4.102411
MAE train: 2.452912	val: 3.657766	test: 3.730075

Epoch: 4
Loss: 8.62833046913147
RMSE train: 2.756423	val: 3.658276	test: 3.778799
MAE train: 2.208494	val: 3.363392	test: 3.434868

Epoch: 5
Loss: 7.817020893096924
RMSE train: 2.442800	val: 3.242730	test: 3.333780
MAE train: 1.958729	val: 2.965578	test: 2.980698

Epoch: 6
Loss: 7.0313438177108765
RMSE train: 2.357997	val: 3.113781	test: 3.170822
MAE train: 1.912592	val: 2.832217	test: 2.798079

Epoch: 7
Loss: 6.474973678588867
RMSE train: 2.406033	val: 3.321766	test: 3.367440
MAE train: 1.934062	val: 3.051230	test: 3.005027

Epoch: 8
Loss: 5.91386878490448
RMSE train: 2.337539	val: 3.207464	test: 3.279293
MAE train: 1.895122	val: 2.962847	test: 2.947594

Epoch: 9
Loss: 5.069350242614746
RMSE train: 2.113581	val: 2.827331	test: 2.910905
MAE train: 1.713102	val: 2.588953	test: 2.570221

Epoch: 10
Loss: 4.526291012763977
RMSE train: 1.989593	val: 2.581043	test: 2.709136
MAE train: 1.612923	val: 2.344231	test: 2.369855

Epoch: 11
Loss: 4.183688700199127
RMSE train: 1.959228	val: 2.588511	test: 2.773219
MAE train: 1.593097	val: 2.338601	test: 2.436575

Epoch: 12
Loss: 3.5758240818977356
RMSE train: 1.789744	val: 2.354258	test: 2.524821
MAE train: 1.422965	val: 2.118060	test: 2.163539

Epoch: 13
Loss: 3.094254434108734
RMSE train: 1.756459	val: 2.261496	test: 2.419060
MAE train: 1.406942	val: 2.016922	test: 2.072322

Epoch: 14
Loss: 2.637815535068512
RMSE train: 1.652428	val: 2.138940	test: 2.304311
MAE train: 1.336090	val: 1.881885	test: 1.988785

Epoch: 15
Loss: 2.282553970813751
RMSE train: 1.415902	val: 1.809095	test: 1.976346
MAE train: 1.139453	val: 1.562299	test: 1.659035

Epoch: 16
Loss: 1.9203616082668304
RMSE train: 1.438439	val: 1.875126	test: 2.042982
MAE train: 1.143605	val: 1.620348	test: 1.740170

Epoch: 17
Loss: 1.6243740916252136
RMSE train: 1.307366	val: 1.691453	test: 1.854437
MAE train: 1.030424	val: 1.439551	test: 1.547970

Epoch: 18
Loss: 1.5048960149288177
RMSE train: 1.205737	val: 1.531495	test: 1.655791
MAE train: 0.954161	val: 1.270998	test: 1.359617

Epoch: 19
Loss: 1.1607380211353302
RMSE train: 1.151833	val: 1.565466	test: 1.684515
MAE train: 0.893827	val: 1.303614	test: 1.395012

Epoch: 20
Loss: 1.1104002892971039
RMSE train: 0.986618	val: 1.321142	test: 1.482529
MAE train: 0.743238	val: 1.093923	test: 1.171661

Epoch: 21
Loss: 1.004690334200859
RMSE train: 0.895784	val: 1.323235	test: 1.406033
MAE train: 0.668373	val: 1.062943	test: 1.115458

Epoch: 22
Loss: 0.8403482884168625
RMSE train: 0.820540	val: 1.211616	test: 1.358278Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/esol/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/static-noise/GraphMVP/esol/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/esol/noise=0.05/esol_scaff_5_26-05_11-13-30  ]
[ Using Seed :  5  ]
[ Using device :  cuda:1  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 11.67917251586914
RMSE train: 3.247582	val: 4.070738	test: 4.097218
MAE train: 2.663576	val: 3.727441	test: 3.686472

Epoch: 2
Loss: 10.097005128860474
RMSE train: 3.094301	val: 3.982634	test: 3.999964
MAE train: 2.510365	val: 3.683043	test: 3.643927

Epoch: 3
Loss: 9.248619794845581
RMSE train: 2.965321	val: 3.915871	test: 3.962763
MAE train: 2.392349	val: 3.617859	test: 3.605446

Epoch: 4
Loss: 8.225761651992798
RMSE train: 2.699476	val: 3.679200	test: 3.755491
MAE train: 2.161370	val: 3.397459	test: 3.394691

Epoch: 5
Loss: 7.382944464683533
RMSE train: 2.393923	val: 3.277740	test: 3.348823
MAE train: 1.919820	val: 3.020440	test: 2.995242

Epoch: 6
Loss: 6.642432928085327
RMSE train: 2.193252	val: 2.955726	test: 3.001392
MAE train: 1.758080	val: 2.699929	test: 2.643188

Epoch: 7
Loss: 6.001009702682495
RMSE train: 2.202140	val: 3.006416	test: 3.049983
MAE train: 1.746524	val: 2.734791	test: 2.687912

Epoch: 8
Loss: 5.400752663612366
RMSE train: 2.178181	val: 2.924725	test: 2.985089
MAE train: 1.731788	val: 2.638626	test: 2.620474

Epoch: 9
Loss: 4.877220273017883
RMSE train: 2.100280	val: 2.775980	test: 2.879559
MAE train: 1.697647	val: 2.507285	test: 2.533713

Epoch: 10
Loss: 4.306404411792755
RMSE train: 2.027118	val: 2.680194	test: 2.802495
MAE train: 1.650635	val: 2.415173	test: 2.448746

Epoch: 11
Loss: 3.990847170352936
RMSE train: 1.907038	val: 2.514492	test: 2.674784
MAE train: 1.534984	val: 2.230318	test: 2.292820

Epoch: 12
Loss: 3.453339099884033
RMSE train: 1.753580	val: 2.289931	test: 2.482190
MAE train: 1.412182	val: 2.037442	test: 2.114276

Epoch: 13
Loss: 2.8804938793182373
RMSE train: 1.641892	val: 2.194814	test: 2.384734
MAE train: 1.302427	val: 1.953718	test: 2.020002

Epoch: 14
Loss: 2.5448132157325745
RMSE train: 1.518128	val: 1.957879	test: 2.136753
MAE train: 1.206925	val: 1.709453	test: 1.795834

Epoch: 15
Loss: 2.1355402171611786
RMSE train: 1.434132	val: 1.953995	test: 2.107662
MAE train: 1.133914	val: 1.715073	test: 1.773249

Epoch: 16
Loss: 1.808485209941864
RMSE train: 1.344445	val: 1.918033	test: 2.072306
MAE train: 1.037862	val: 1.688219	test: 1.707785

Epoch: 17
Loss: 1.5157396346330643
RMSE train: 1.200691	val: 1.584205	test: 1.774916
MAE train: 0.924746	val: 1.359579	test: 1.445728

Epoch: 18
Loss: 1.3146197199821472
RMSE train: 1.142305	val: 1.583568	test: 1.769499
MAE train: 0.869073	val: 1.360294	test: 1.441226

Epoch: 19
Loss: 1.1849157810211182
RMSE train: 1.067444	val: 1.534431	test: 1.711595
MAE train: 0.792836	val: 1.322561	test: 1.376363

Epoch: 20
Loss: 0.9781086444854736
RMSE train: 0.978727	val: 1.355899	test: 1.460362
MAE train: 0.740569	val: 1.099043	test: 1.135424

Epoch: 21
Loss: 0.9894617050886154
RMSE train: 0.839739	val: 1.197607	test: 1.366908
MAE train: 0.622345	val: 0.993615	test: 1.065358

Epoch: 22
Loss: 0.8113080263137817
RMSE train: 0.848671	val: 1.204057	test: 1.353199Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/esol/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/static-noise/GraphMVP/esol/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/esol/noise=0.1/esol_scaff_6_26-05_11-13-30  ]
[ Using Seed :  6  ]
[ Using device :  cuda:2  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 11.262634515762329
RMSE train: 2.904408	val: 3.362614	test: 3.421893
MAE train: 2.326470	val: 3.010154	test: 2.985646

Epoch: 2
Loss: 10.10502314567566
RMSE train: 2.789210	val: 3.218074	test: 3.264085
MAE train: 2.214001	val: 2.907856	test: 2.891009

Epoch: 3
Loss: 8.731629371643066
RMSE train: 2.736589	val: 3.286363	test: 3.337756
MAE train: 2.162199	val: 2.960415	test: 2.901961

Epoch: 4
Loss: 7.9806952476501465
RMSE train: 2.610846	val: 3.126362	test: 3.289368
MAE train: 2.099630	val: 2.820214	test: 2.860686

Epoch: 5
Loss: 7.1102988719940186
RMSE train: 2.305818	val: 2.724652	test: 2.917585
MAE train: 1.836809	val: 2.389767	test: 2.459834

Epoch: 6
Loss: 6.362672686576843
RMSE train: 2.149057	val: 2.649029	test: 2.810276
MAE train: 1.681406	val: 2.303162	test: 2.373753

Epoch: 7
Loss: 6.053710699081421
RMSE train: 2.094508	val: 2.685956	test: 2.817669
MAE train: 1.625971	val: 2.344975	test: 2.394159

Epoch: 8
Loss: 5.166463494300842
RMSE train: 1.971042	val: 2.513807	test: 2.636888
MAE train: 1.542054	val: 2.208924	test: 2.241380

Epoch: 9
Loss: 4.7554837465286255
RMSE train: 1.891505	val: 2.427253	test: 2.562211
MAE train: 1.499700	val: 2.139200	test: 2.185615

Epoch: 10
Loss: 4.0594300627708435
RMSE train: 1.862555	val: 2.421175	test: 2.578480
MAE train: 1.501703	val: 2.128585	test: 2.218719

Epoch: 11
Loss: 3.6197166442871094
RMSE train: 1.802614	val: 2.292153	test: 2.461786
MAE train: 1.456194	val: 1.993065	test: 2.089086

Epoch: 12
Loss: 3.098425328731537
RMSE train: 1.688308	val: 2.235791	test: 2.416042
MAE train: 1.344255	val: 1.943057	test: 2.047458

Epoch: 13
Loss: 2.750328779220581
RMSE train: 1.599154	val: 2.204530	test: 2.396627
MAE train: 1.290742	val: 1.915249	test: 2.043120

Epoch: 14
Loss: 2.355305105447769
RMSE train: 1.612841	val: 2.193073	test: 2.359208
MAE train: 1.320652	val: 1.914493	test: 2.017824

Epoch: 15
Loss: 1.9498020112514496
RMSE train: 1.702732	val: 2.260897	test: 2.392452
MAE train: 1.412074	val: 1.988467	test: 2.071052

Epoch: 16
Loss: 1.69331756234169
RMSE train: 1.486840	val: 1.997035	test: 2.104345
MAE train: 1.211414	val: 1.720448	test: 1.770913

Epoch: 17
Loss: 1.4249707460403442
RMSE train: 1.213270	val: 1.640915	test: 1.760765
MAE train: 0.974272	val: 1.362628	test: 1.384807

Epoch: 18
Loss: 1.278320163488388
RMSE train: 1.169537	val: 1.713160	test: 1.806627
MAE train: 0.909610	val: 1.440719	test: 1.476355

Epoch: 19
Loss: 1.2109245359897614
RMSE train: 1.039012	val: 1.486936	test: 1.594846
MAE train: 0.825306	val: 1.231033	test: 1.241159

Epoch: 20
Loss: 1.0794974863529205
RMSE train: 0.895756	val: 1.413462	test: 1.513259
MAE train: 0.693772	val: 1.196809	test: 1.179832

Epoch: 21
Loss: 0.9453054815530777
RMSE train: 0.844041	val: 1.460029	test: 1.546476
MAE train: 0.645092	val: 1.234481	test: 1.218843

Epoch: 22
Loss: 0.8711856603622437
RMSE train: 0.839978	val: 1.412291	test: 1.479770Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/esol/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/static-noise/GraphMVP/esol/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/esol/noise=0.1/esol_scaff_5_26-05_11-13-30  ]
[ Using Seed :  5  ]
[ Using device :  cuda:2  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 11.708005666732788
RMSE train: 3.287470	val: 4.054810	test: 4.079910
MAE train: 2.705307	val: 3.704345	test: 3.661397

Epoch: 2
Loss: 10.171194076538086
RMSE train: 3.148484	val: 3.912956	test: 3.926242
MAE train: 2.565400	val: 3.605425	test: 3.561489

Epoch: 3
Loss: 9.354063868522644
RMSE train: 3.012951	val: 3.830673	test: 3.846162
MAE train: 2.440640	val: 3.526859	test: 3.495832

Epoch: 4
Loss: 8.333799958229065
RMSE train: 2.759966	val: 3.693326	test: 3.722262
MAE train: 2.223748	val: 3.394918	test: 3.379693

Epoch: 5
Loss: 7.478302359580994
RMSE train: 2.417555	val: 3.292195	test: 3.331855
MAE train: 1.945876	val: 3.009593	test: 2.982401

Epoch: 6
Loss: 6.726863503456116
RMSE train: 2.212118	val: 2.991861	test: 3.023492
MAE train: 1.757618	val: 2.712322	test: 2.643039

Epoch: 7
Loss: 6.083885073661804
RMSE train: 2.186031	val: 2.931416	test: 2.974927
MAE train: 1.713768	val: 2.643370	test: 2.598242

Epoch: 8
Loss: 5.509067893028259
RMSE train: 2.179773	val: 2.883603	test: 2.947197
MAE train: 1.725319	val: 2.600350	test: 2.587919

Epoch: 9
Loss: 4.964297294616699
RMSE train: 2.127496	val: 2.818177	test: 2.911631
MAE train: 1.717620	val: 2.543403	test: 2.563173

Epoch: 10
Loss: 4.441320538520813
RMSE train: 2.044836	val: 2.751812	test: 2.856319
MAE train: 1.665939	val: 2.472678	test: 2.514086

Epoch: 11
Loss: 4.040594637393951
RMSE train: 1.989817	val: 2.699944	test: 2.800778
MAE train: 1.613055	val: 2.410489	test: 2.428144

Epoch: 12
Loss: 3.545003354549408
RMSE train: 1.883812	val: 2.543309	test: 2.669029
MAE train: 1.535297	val: 2.265985	test: 2.286546

Epoch: 13
Loss: 2.924363672733307
RMSE train: 1.809066	val: 2.496911	test: 2.630245
MAE train: 1.463563	val: 2.229714	test: 2.270794

Epoch: 14
Loss: 2.603269398212433
RMSE train: 1.585091	val: 2.067587	test: 2.201692
MAE train: 1.278578	val: 1.807643	test: 1.863534

Epoch: 15
Loss: 2.171869456768036
RMSE train: 1.449456	val: 1.987332	test: 2.099264
MAE train: 1.156430	val: 1.716489	test: 1.756356

Epoch: 16
Loss: 1.8925830125808716
RMSE train: 1.374756	val: 1.915707	test: 2.007968
MAE train: 1.082647	val: 1.658836	test: 1.650050

Epoch: 17
Loss: 1.6027859151363373
RMSE train: 1.274121	val: 1.637722	test: 1.748611
MAE train: 1.004340	val: 1.362007	test: 1.427171

Epoch: 18
Loss: 1.4012944400310516
RMSE train: 1.208280	val: 1.710687	test: 1.802238
MAE train: 0.935243	val: 1.443137	test: 1.487243

Epoch: 19
Loss: 1.3003503382205963
RMSE train: 1.097087	val: 1.599876	test: 1.697800
MAE train: 0.846060	val: 1.345929	test: 1.366419

Epoch: 20
Loss: 1.0448969900608063
RMSE train: 1.066453	val: 1.480829	test: 1.556263
MAE train: 0.820561	val: 1.221772	test: 1.219338

Epoch: 21
Loss: 1.0365355610847473
RMSE train: 0.961757	val: 1.460999	test: 1.543996
MAE train: 0.726820	val: 1.217334	test: 1.215875

Epoch: 22
Loss: 0.8462101519107819
RMSE train: 0.869980	val: 1.364295	test: 1.473822Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/esol/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/static-noise/GraphMVP/esol/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/esol/noise=0.1/esol_scaff_4_26-05_11-13-30  ]
[ Using Seed :  4  ]
[ Using device :  cuda:2  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 11.923345565795898
RMSE train: 3.517123	val: 4.293718	test: 4.381248
MAE train: 2.881156	val: 3.846135	test: 3.855782

Epoch: 2
Loss: 10.74929428100586
RMSE train: 3.343734	val: 4.163900	test: 4.242547
MAE train: 2.724506	val: 3.786654	test: 3.828651

Epoch: 3
Loss: 9.961872816085815
RMSE train: 3.203559	val: 4.096300	test: 4.192471
MAE train: 2.603418	val: 3.756974	test: 3.823269

Epoch: 4
Loss: 8.87542986869812
RMSE train: 2.952603	val: 3.862102	test: 3.968918
MAE train: 2.385964	val: 3.538984	test: 3.625503

Epoch: 5
Loss: 8.070119738578796
RMSE train: 2.620075	val: 3.453561	test: 3.544729
MAE train: 2.113475	val: 3.153509	test: 3.205302

Epoch: 6
Loss: 7.260496735572815
RMSE train: 2.380361	val: 3.036625	test: 3.096260
MAE train: 1.928077	val: 2.752083	test: 2.751845

Epoch: 7
Loss: 6.6346564292907715
RMSE train: 2.289400	val: 2.931477	test: 2.981815
MAE train: 1.825729	val: 2.642219	test: 2.609230

Epoch: 8
Loss: 6.057332277297974
RMSE train: 2.185724	val: 2.758890	test: 2.837775
MAE train: 1.761983	val: 2.492914	test: 2.481731

Epoch: 9
Loss: 5.303166031837463
RMSE train: 2.077035	val: 2.608969	test: 2.711216
MAE train: 1.674975	val: 2.344472	test: 2.349774

Epoch: 10
Loss: 4.69620418548584
RMSE train: 2.002112	val: 2.487608	test: 2.628038
MAE train: 1.617949	val: 2.228378	test: 2.274904

Epoch: 11
Loss: 4.313335597515106
RMSE train: 1.924193	val: 2.383585	test: 2.569350
MAE train: 1.571930	val: 2.111066	test: 2.222112

Epoch: 12
Loss: 3.6890673637390137
RMSE train: 1.795570	val: 2.182223	test: 2.370083
MAE train: 1.462383	val: 1.924492	test: 2.006588

Epoch: 13
Loss: 3.2099876403808594
RMSE train: 1.854344	val: 2.320747	test: 2.465785
MAE train: 1.501265	val: 2.046040	test: 2.093047

Epoch: 14
Loss: 2.7258501052856445
RMSE train: 1.744284	val: 2.285327	test: 2.416411
MAE train: 1.405859	val: 2.008623	test: 2.067816

Epoch: 15
Loss: 2.3889761865139008
RMSE train: 1.457455	val: 1.882844	test: 2.025168
MAE train: 1.165820	val: 1.595617	test: 1.660194

Epoch: 16
Loss: 2.0131793916225433
RMSE train: 1.418304	val: 1.866599	test: 2.022067
MAE train: 1.120045	val: 1.575665	test: 1.662158

Epoch: 17
Loss: 1.7204105854034424
RMSE train: 1.330720	val: 1.765825	test: 1.910710
MAE train: 1.038542	val: 1.476613	test: 1.552072

Epoch: 18
Loss: 1.591824859380722
RMSE train: 1.222535	val: 1.580229	test: 1.678244
MAE train: 0.956105	val: 1.296522	test: 1.357068

Epoch: 19
Loss: 1.331215262413025
RMSE train: 1.151736	val: 1.566530	test: 1.629015
MAE train: 0.893245	val: 1.286410	test: 1.327656

Epoch: 20
Loss: 1.2336426973342896
RMSE train: 1.057368	val: 1.433895	test: 1.525992
MAE train: 0.801058	val: 1.180504	test: 1.211918

Epoch: 21
Loss: 1.0707592070102692
RMSE train: 0.990487	val: 1.461746	test: 1.497884
MAE train: 0.742875	val: 1.175417	test: 1.183768

Epoch: 22
Loss: 0.9993829727172852
RMSE train: 0.863330	val: 1.370540	test: 1.441165Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/esol/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:3
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/static-noise/GraphMVP/esol/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/esol/noise=0.2/esol_scaff_6_26-05_11-13-30  ]
[ Using Seed :  6  ]
[ Using device :  cuda:3  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 11.290613889694214
RMSE train: 3.044147	val: 3.329975	test: 3.380295
MAE train: 2.459159	val: 2.976072	test: 2.943713

Epoch: 2
Loss: 10.21182656288147
RMSE train: 2.960354	val: 3.104412	test: 3.132045
MAE train: 2.380470	val: 2.779094	test: 2.747825

Epoch: 3
Loss: 8.922072649002075
RMSE train: 2.891185	val: 3.029450	test: 3.104813
MAE train: 2.322089	val: 2.715029	test: 2.691633

Epoch: 4
Loss: 8.149404287338257
RMSE train: 2.757452	val: 2.941171	test: 3.076291
MAE train: 2.222120	val: 2.626710	test: 2.637355

Epoch: 5
Loss: 7.285235285758972
RMSE train: 2.467808	val: 2.652648	test: 2.817590
MAE train: 1.991933	val: 2.349694	test: 2.371813

Epoch: 6
Loss: 6.61278223991394
RMSE train: 2.276948	val: 2.565124	test: 2.729682
MAE train: 1.825353	val: 2.264523	test: 2.279477

Epoch: 7
Loss: 6.245097994804382
RMSE train: 2.164471	val: 2.447282	test: 2.625112
MAE train: 1.724944	val: 2.123477	test: 2.161721

Epoch: 8
Loss: 5.3469743728637695
RMSE train: 2.061197	val: 2.269782	test: 2.464291
MAE train: 1.645505	val: 1.951512	test: 2.025211

Epoch: 9
Loss: 5.026637554168701
RMSE train: 1.952203	val: 2.131645	test: 2.359879
MAE train: 1.562129	val: 1.838470	test: 1.929196

Epoch: 10
Loss: 4.304932117462158
RMSE train: 1.901123	val: 2.106385	test: 2.358890
MAE train: 1.537812	val: 1.807760	test: 1.926601

Epoch: 11
Loss: 3.794945776462555
RMSE train: 1.887579	val: 2.208998	test: 2.463534
MAE train: 1.524147	val: 1.952002	test: 2.070398

Epoch: 12
Loss: 3.293781876564026
RMSE train: 1.793022	val: 2.207180	test: 2.464147
MAE train: 1.429917	val: 1.953900	test: 2.073792

Epoch: 13
Loss: 2.9884976744651794
RMSE train: 1.678550	val: 1.996104	test: 2.261944
MAE train: 1.344674	val: 1.749524	test: 1.872169

Epoch: 14
Loss: 2.5478230714797974
RMSE train: 1.589645	val: 1.913258	test: 2.165918
MAE train: 1.274541	val: 1.670920	test: 1.786126

Epoch: 15
Loss: 2.206537753343582
RMSE train: 1.600935	val: 2.055733	test: 2.268124
MAE train: 1.286604	val: 1.811299	test: 1.911409

Epoch: 16
Loss: 1.9018490612506866
RMSE train: 1.416603	val: 1.728924	test: 1.952204
MAE train: 1.133908	val: 1.475835	test: 1.582134

Epoch: 17
Loss: 1.6717624068260193
RMSE train: 1.209528	val: 1.428066	test: 1.664073
MAE train: 0.956838	val: 1.184466	test: 1.293863

Epoch: 18
Loss: 1.5955398976802826
RMSE train: 1.196229	val: 1.553507	test: 1.753140
MAE train: 0.940331	val: 1.304239	test: 1.399559

Epoch: 19
Loss: 1.4098444283008575
RMSE train: 1.040875	val: 1.352331	test: 1.534152
MAE train: 0.817158	val: 1.101837	test: 1.167090

Epoch: 20
Loss: 1.2149615585803986
RMSE train: 1.006164	val: 1.445895	test: 1.614993
MAE train: 0.785621	val: 1.205876	test: 1.229911

Epoch: 21
Loss: 1.1066090911626816
RMSE train: 0.931020	val: 1.384751	test: 1.602715
MAE train: 0.711311	val: 1.163758	test: 1.217370

Epoch: 22
Loss: 1.0748230963945389
RMSE train: 0.936727	val: 1.365575	test: 1.570363Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/esol/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/static-noise/GraphMVP/esol/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/esol/noise=0.05/esol_scaff_6_26-05_11-13-30  ]
[ Using Seed :  6  ]
[ Using device :  cuda:1  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 11.240230798721313
RMSE train: 2.839396	val: 3.396617	test: 3.451945
MAE train: 2.267580	val: 3.047003	test: 3.020497

Epoch: 2
Loss: 9.985690355300903
RMSE train: 2.706386	val: 3.280892	test: 3.314878
MAE train: 2.142903	val: 2.971981	test: 2.942248

Epoch: 3
Loss: 8.55453872680664
RMSE train: 2.635095	val: 3.297955	test: 3.356427
MAE train: 2.073727	val: 2.987446	test: 2.941855

Epoch: 4
Loss: 7.84725284576416
RMSE train: 2.494679	val: 3.152205	test: 3.316831
MAE train: 1.987255	val: 2.865736	test: 2.919726

Epoch: 5
Loss: 6.9806026220321655
RMSE train: 2.108254	val: 2.613212	test: 2.801947
MAE train: 1.639565	val: 2.294824	test: 2.381384

Epoch: 6
Loss: 6.199994206428528
RMSE train: 1.992411	val: 2.566539	test: 2.717563
MAE train: 1.522665	val: 2.247308	test: 2.303382

Epoch: 7
Loss: 5.895662784576416
RMSE train: 2.015811	val: 2.713888	test: 2.849181
MAE train: 1.546387	val: 2.423547	test: 2.464297

Epoch: 8
Loss: 5.011329770088196
RMSE train: 1.991158	val: 2.700960	test: 2.836503
MAE train: 1.567215	val: 2.453456	test: 2.494323

Epoch: 9
Loss: 4.631929337978363
RMSE train: 1.917900	val: 2.575102	test: 2.726589
MAE train: 1.528103	val: 2.322465	test: 2.392007

Epoch: 10
Loss: 3.894139289855957
RMSE train: 1.864358	val: 2.497115	test: 2.695631
MAE train: 1.523497	val: 2.251898	test: 2.382362

Epoch: 11
Loss: 3.4557936787605286
RMSE train: 1.773284	val: 2.332780	test: 2.561076
MAE train: 1.439616	val: 2.075630	test: 2.225576

Epoch: 12
Loss: 2.984024703502655
RMSE train: 1.683084	val: 2.257057	test: 2.486174
MAE train: 1.346590	val: 1.994008	test: 2.146052

Epoch: 13
Loss: 2.6044322848320007
RMSE train: 1.574813	val: 2.109273	test: 2.346078
MAE train: 1.283454	val: 1.860274	test: 2.033357

Epoch: 14
Loss: 2.1889489889144897
RMSE train: 1.520420	val: 2.036733	test: 2.240361
MAE train: 1.237226	val: 1.779738	test: 1.939593

Epoch: 15
Loss: 1.7950755059719086
RMSE train: 1.508704	val: 2.016108	test: 2.207419
MAE train: 1.234695	val: 1.763515	test: 1.913353

Epoch: 16
Loss: 1.549238681793213
RMSE train: 1.312856	val: 1.683433	test: 1.907188
MAE train: 1.082782	val: 1.447639	test: 1.590272

Epoch: 17
Loss: 1.3282930850982666
RMSE train: 1.169026	val: 1.561939	test: 1.760751
MAE train: 0.936137	val: 1.326152	test: 1.430647

Epoch: 18
Loss: 1.1879594326019287
RMSE train: 1.139549	val: 1.646380	test: 1.787279
MAE train: 0.874134	val: 1.381646	test: 1.486026

Epoch: 19
Loss: 1.0730123668909073
RMSE train: 0.987137	val: 1.409245	test: 1.567550
MAE train: 0.785088	val: 1.145946	test: 1.245531

Epoch: 20
Loss: 0.9472592622041702
RMSE train: 0.874593	val: 1.281250	test: 1.457155
MAE train: 0.672724	val: 1.068606	test: 1.168074

Epoch: 21
Loss: 0.8289451450109482
RMSE train: 0.740801	val: 1.180201	test: 1.352201
MAE train: 0.558229	val: 0.993509	test: 1.086446

Epoch: 22
Loss: 0.7612105160951614
RMSE train: 0.626774	val: 1.128572	test: 1.284823Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/esol/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:3
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/static-noise/GraphMVP/esol/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/esol/noise=0.2/esol_scaff_4_26-05_11-13-30  ]
[ Using Seed :  4  ]
[ Using device :  cuda:3  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 11.99092173576355
RMSE train: 3.544287	val: 4.291050	test: 4.377836
MAE train: 2.907304	val: 3.836769	test: 3.838195

Epoch: 2
Loss: 10.888057470321655
RMSE train: 3.359121	val: 4.122661	test: 4.185076
MAE train: 2.742353	val: 3.729341	test: 3.739328

Epoch: 3
Loss: 10.101510286331177
RMSE train: 3.249044	val: 4.108948	test: 4.171154
MAE train: 2.658414	val: 3.761929	test: 3.772710

Epoch: 4
Loss: 9.15126085281372
RMSE train: 3.047197	val: 4.034877	test: 4.102159
MAE train: 2.486590	val: 3.709613	test: 3.731069

Epoch: 5
Loss: 8.252203345298767
RMSE train: 2.758196	val: 3.753823	test: 3.787077
MAE train: 2.243269	val: 3.449144	test: 3.431249

Epoch: 6
Loss: 7.554219007492065
RMSE train: 2.536183	val: 3.286434	test: 3.290494
MAE train: 2.069674	val: 2.999810	test: 2.919290

Epoch: 7
Loss: 6.824069499969482
RMSE train: 2.436278	val: 3.090971	test: 3.068473
MAE train: 1.968421	val: 2.774324	test: 2.644396

Epoch: 8
Loss: 6.259436011314392
RMSE train: 2.282306	val: 2.855341	test: 2.852185
MAE train: 1.835271	val: 2.538756	test: 2.426287

Epoch: 9
Loss: 5.458920657634735
RMSE train: 2.100511	val: 2.568142	test: 2.595472
MAE train: 1.686097	val: 2.240068	test: 2.154400

Epoch: 10
Loss: 4.920870125293732
RMSE train: 1.971270	val: 2.255085	test: 2.328342
MAE train: 1.585618	val: 1.947195	test: 1.923214

Epoch: 11
Loss: 4.505640387535095
RMSE train: 1.910954	val: 2.136307	test: 2.269236
MAE train: 1.541954	val: 1.858799	test: 1.897911

Epoch: 12
Loss: 3.8429555892944336
RMSE train: 1.741407	val: 1.993773	test: 2.148323
MAE train: 1.388509	val: 1.692476	test: 1.741314

Epoch: 13
Loss: 3.4343918561935425
RMSE train: 1.729130	val: 2.043629	test: 2.203358
MAE train: 1.377612	val: 1.705247	test: 1.756352

Epoch: 14
Loss: 2.951642632484436
RMSE train: 1.721394	val: 2.135674	test: 2.288484
MAE train: 1.381430	val: 1.847223	test: 1.889040

Epoch: 15
Loss: 2.5454495549201965
RMSE train: 1.496370	val: 1.843402	test: 1.982942
MAE train: 1.189134	val: 1.562321	test: 1.613252

Epoch: 16
Loss: 2.182738572359085
RMSE train: 1.431151	val: 1.757225	test: 1.901104
MAE train: 1.132992	val: 1.475531	test: 1.540634

Epoch: 17
Loss: 1.9508857429027557
RMSE train: 1.410613	val: 1.862837	test: 1.998073
MAE train: 1.114981	val: 1.589205	test: 1.606954

Epoch: 18
Loss: 1.711646556854248
RMSE train: 1.305623	val: 1.804196	test: 1.907525
MAE train: 1.027301	val: 1.528747	test: 1.549794

Epoch: 19
Loss: 1.4457022547721863
RMSE train: 1.222343	val: 1.804033	test: 1.849259
MAE train: 0.950448	val: 1.491145	test: 1.493271

Epoch: 20
Loss: 1.3809063136577606
RMSE train: 1.123753	val: 1.784901	test: 1.819556
MAE train: 0.858718	val: 1.443227	test: 1.420696

Epoch: 21
Loss: 1.2126568257808685
RMSE train: 0.972518	val: 1.540493	test: 1.545804
MAE train: 0.739523	val: 1.252964	test: 1.197367

Epoch: 22
Loss: 1.0996312350034714
RMSE train: 0.913319	val: 1.470455	test: 1.485557Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/esol/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:3
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/static-noise/GraphMVP/esol/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/esol/noise=0.2/esol_scaff_5_26-05_11-13-30  ]
[ Using Seed :  5  ]
[ Using device :  cuda:3  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 11.766710758209229
RMSE train: 3.378081	val: 4.038263	test: 4.062596
MAE train: 2.783841	val: 3.677048	test: 3.634463

Epoch: 2
Loss: 10.429025173187256
RMSE train: 3.303536	val: 3.851066	test: 3.867054
MAE train: 2.702875	val: 3.521505	test: 3.482890

Epoch: 3
Loss: 9.680007934570312
RMSE train: 3.188670	val: 3.769122	test: 3.788428
MAE train: 2.596068	val: 3.447641	test: 3.409457

Epoch: 4
Loss: 8.75960087776184
RMSE train: 2.964496	val: 3.698505	test: 3.712288
MAE train: 2.399804	val: 3.387513	test: 3.326980

Epoch: 5
Loss: 7.876800537109375
RMSE train: 2.664455	val: 3.417741	test: 3.432885
MAE train: 2.161947	val: 3.128204	test: 3.061382

Epoch: 6
Loss: 7.0514914989471436
RMSE train: 2.423425	val: 3.077664	test: 3.086715
MAE train: 1.965715	val: 2.801015	test: 2.723106

Epoch: 7
Loss: 6.4048073291778564
RMSE train: 2.300163	val: 2.838919	test: 2.849427
MAE train: 1.860368	val: 2.555080	test: 2.473375

Epoch: 8
Loss: 5.857687592506409
RMSE train: 2.303402	val: 2.726701	test: 2.753592
MAE train: 1.872319	val: 2.446190	test: 2.375602

Epoch: 9
Loss: 5.189194202423096
RMSE train: 2.225731	val: 2.487637	test: 2.545433
MAE train: 1.814649	val: 2.221266	test: 2.186361

Epoch: 10
Loss: 4.687247633934021
RMSE train: 2.133479	val: 2.328571	test: 2.406765
MAE train: 1.745545	val: 2.047759	test: 2.040983

Epoch: 11
Loss: 4.312870621681213
RMSE train: 2.029985	val: 2.217282	test: 2.298589
MAE train: 1.644978	val: 1.935901	test: 1.931691

Epoch: 12
Loss: 3.748714029788971
RMSE train: 1.957591	val: 2.145083	test: 2.261891
MAE train: 1.601591	val: 1.869424	test: 1.876319

Epoch: 13
Loss: 3.1271714568138123
RMSE train: 1.943168	val: 2.274305	test: 2.412036
MAE train: 1.578679	val: 1.997070	test: 2.025950

Epoch: 14
Loss: 2.7905537486076355
RMSE train: 1.802640	val: 2.047508	test: 2.198826
MAE train: 1.472494	val: 1.770693	test: 1.804466

Epoch: 15
Loss: 2.405512511730194
RMSE train: 1.601434	val: 1.821519	test: 1.961381
MAE train: 1.299266	val: 1.527232	test: 1.570495

Epoch: 16
Loss: 2.1054561138153076
RMSE train: 1.469703	val: 1.752920	test: 1.897196
MAE train: 1.159063	val: 1.475875	test: 1.510789

Epoch: 17
Loss: 1.7759103178977966
RMSE train: 1.380600	val: 1.599012	test: 1.767621
MAE train: 1.082051	val: 1.343269	test: 1.384004

Epoch: 18
Loss: 1.5795779824256897
RMSE train: 1.236031	val: 1.460919	test: 1.618868
MAE train: 0.968958	val: 1.221425	test: 1.246556

Epoch: 19
Loss: 1.454808384180069
RMSE train: 1.140604	val: 1.396006	test: 1.552826
MAE train: 0.881986	val: 1.159227	test: 1.201130

Epoch: 20
Loss: 1.263039618730545
RMSE train: 1.062286	val: 1.288290	test: 1.440395
MAE train: 0.835150	val: 1.064878	test: 1.099763

Epoch: 21
Loss: 1.1711638867855072
RMSE train: 0.942416	val: 1.196101	test: 1.359963
MAE train: 0.726534	val: 0.984892	test: 1.051260

Epoch: 22
Loss: 1.0939676463603973
RMSE train: 0.856499	val: 1.155036	test: 1.299449
RMSE train: 0.905339	val: 1.513364	test: 1.521342
MAE train: 0.697141	val: 1.235681	test: 1.233367

Epoch: 24
Loss: 0.6637711375951767
RMSE train: 0.887007	val: 1.420718	test: 1.488915
MAE train: 0.668219	val: 1.184680	test: 1.198479

Epoch: 25
Loss: 0.6141127794981003
RMSE train: 0.833148	val: 1.280325	test: 1.367778
MAE train: 0.616133	val: 1.035159	test: 1.063421

Epoch: 26
Loss: 0.5739262402057648
RMSE train: 0.669870	val: 1.366096	test: 1.322376
MAE train: 0.485110	val: 1.079000	test: 1.021826

Epoch: 27
Loss: 0.5491664111614227
RMSE train: 0.677006	val: 1.312846	test: 1.276845
MAE train: 0.492894	val: 1.029856	test: 0.980123

Epoch: 28
Loss: 0.5863042101264
RMSE train: 0.654071	val: 1.151447	test: 1.186394
MAE train: 0.467847	val: 0.922020	test: 0.905190

Epoch: 29
Loss: 0.524296760559082
RMSE train: 0.691758	val: 1.409948	test: 1.313881
MAE train: 0.511985	val: 1.131394	test: 1.037713

Epoch: 30
Loss: 0.534639447927475
RMSE train: 0.577129	val: 1.281130	test: 1.176946
MAE train: 0.423422	val: 1.006685	test: 0.921828

Epoch: 31
Loss: 0.46329081803560257
RMSE train: 0.662578	val: 1.215109	test: 1.150160
MAE train: 0.473943	val: 0.952181	test: 0.898458

Epoch: 32
Loss: 0.4679761081933975
RMSE train: 0.546291	val: 1.179431	test: 1.129894
MAE train: 0.395041	val: 0.925492	test: 0.881014

Epoch: 33
Loss: 0.4921598806977272
RMSE train: 0.609421	val: 1.304244	test: 1.217582
MAE train: 0.444363	val: 1.012097	test: 0.971928

Epoch: 34
Loss: 0.5174465253949165
RMSE train: 0.711875	val: 1.259100	test: 1.229898
MAE train: 0.532937	val: 1.000962	test: 0.966872

Epoch: 35
Loss: 0.4527917057275772
RMSE train: 0.742197	val: 1.426598	test: 1.348891
MAE train: 0.553596	val: 1.151723	test: 1.096038

Epoch: 36
Loss: 0.46990616619586945
RMSE train: 0.642566	val: 1.261756	test: 1.182320
MAE train: 0.479281	val: 0.990195	test: 0.930941

Epoch: 37
Loss: 0.48264777660369873
RMSE train: 0.585604	val: 1.060633	test: 1.069530
MAE train: 0.441930	val: 0.862431	test: 0.803002

Epoch: 38
Loss: 0.4388887584209442
RMSE train: 0.657807	val: 1.215092	test: 1.215129
MAE train: 0.487791	val: 0.988845	test: 0.941729

Epoch: 39
Loss: 0.4412171319127083
RMSE train: 0.522371	val: 1.100034	test: 1.067755
MAE train: 0.379268	val: 0.883203	test: 0.798885

Epoch: 40
Loss: 0.43618006259202957
RMSE train: 0.503709	val: 1.139095	test: 1.082210
MAE train: 0.366487	val: 0.892474	test: 0.818382

Epoch: 41
Loss: 0.4435427635908127
RMSE train: 0.591565	val: 1.233465	test: 1.163424
MAE train: 0.430716	val: 0.973931	test: 0.910778

Epoch: 42
Loss: 0.42984484136104584
RMSE train: 0.552078	val: 1.152071	test: 1.129339
MAE train: 0.401187	val: 0.912150	test: 0.856071

Epoch: 43
Loss: 0.4155869036912918
RMSE train: 0.548065	val: 1.236768	test: 1.133660
MAE train: 0.408717	val: 0.961232	test: 0.882648

Epoch: 44
Loss: 0.4373767822980881
RMSE train: 0.487585	val: 1.114211	test: 1.096584
MAE train: 0.362028	val: 0.882706	test: 0.831979

Epoch: 45
Loss: 0.44008514285087585
RMSE train: 0.580460	val: 1.209050	test: 1.154374
MAE train: 0.429359	val: 0.955467	test: 0.899582

Epoch: 46
Loss: 0.4934232458472252
RMSE train: 0.546430	val: 1.104704	test: 1.098663
MAE train: 0.399362	val: 0.892177	test: 0.841994

Epoch: 47
Loss: 0.4143759161233902
RMSE train: 0.559739	val: 1.163239	test: 1.143878
MAE train: 0.407189	val: 0.927067	test: 0.882868

Epoch: 48
Loss: 0.44783470779657364
RMSE train: 0.568176	val: 1.191788	test: 1.157994
MAE train: 0.416706	val: 0.957851	test: 0.896886

Epoch: 49
Loss: 0.41622210294008255
RMSE train: 0.447793	val: 1.087454	test: 1.053819
MAE train: 0.331598	val: 0.883934	test: 0.793002

Epoch: 50
Loss: 0.3603780269622803
RMSE train: 0.454647	val: 1.103413	test: 1.067969
MAE train: 0.339160	val: 0.887892	test: 0.808158

Epoch: 51
Loss: 0.3868408724665642
RMSE train: 0.517235	val: 1.051270	test: 1.066229
MAE train: 0.369219	val: 0.860993	test: 0.808975

Epoch: 52
Loss: 0.38880830258131027
RMSE train: 0.499816	val: 1.035582	test: 1.048699
MAE train: 0.353996	val: 0.850346	test: 0.794438

Epoch: 53
Loss: 0.40072985738515854
RMSE train: 0.483674	val: 1.050527	test: 1.071231
MAE train: 0.348668	val: 0.856337	test: 0.803627

Epoch: 54
Loss: 0.3885173425078392
RMSE train: 0.565430	val: 1.153424	test: 1.145786
MAE train: 0.412450	val: 0.931651	test: 0.882340

Epoch: 55
Loss: 0.37492892891168594
RMSE train: 0.543784	val: 1.132483	test: 1.099859
MAE train: 0.395164	val: 0.912194	test: 0.834074

Epoch: 56
Loss: 0.36087973415851593
RMSE train: 0.511833	val: 1.075331	test: 1.100947
MAE train: 0.363636	val: 0.879920	test: 0.812289

Epoch: 57
Loss: 0.3541616052389145
RMSE train: 0.573396	val: 1.190912	test: 1.172303
MAE train: 0.412470	val: 0.938973	test: 0.894058

Epoch: 58
Loss: 0.3851677253842354
RMSE train: 0.536104	val: 1.137789	test: 1.148442
MAE train: 0.386573	val: 0.896272	test: 0.880213

Epoch: 59
Loss: 0.3575781285762787
RMSE train: 0.602733	val: 1.142099	test: 1.181883
MAE train: 0.442464	val: 0.938240	test: 0.903686

Epoch: 60
Loss: 0.33682776987552643
RMSE train: 0.711184	val: 1.304724	test: 1.288204
MAE train: 0.526920	val: 1.061049	test: 1.018055

Epoch: 61
Loss: 0.34704048931598663
RMSE train: 0.462613	val: 1.122312	test: 1.108649
MAE train: 0.336653	val: 0.901352	test: 0.841687

Epoch: 62
Loss: 0.3477623164653778
RMSE train: 0.437944	val: 1.149400	test: 1.127433
MAE train: 0.315772	val: 0.914552	test: 0.853914

Epoch: 63
Loss: 0.34431231021881104
RMSE train: 0.547416	val: 1.136685	test: 1.134858
MAE train: 0.387906	val: 0.920795	test: 0.877574

Epoch: 64
Loss: 0.37224573642015457
RMSE train: 0.496327	val: 1.116213	test: 1.100334
MAE train: 0.354641	val: 0.887281	test: 0.830066

Epoch: 65
Loss: 0.31406597048044205
RMSE train: 0.511048	val: 1.150307	test: 1.132380
MAE train: 0.371098	val: 0.920851	test: 0.863700

Epoch: 66
Loss: 0.3529997766017914
RMSE train: 0.491409	val: 1.080913	test: 1.096943
MAE train: 0.359292	val: 0.879421	test: 0.823851

Epoch: 67
Loss: 0.4292694851756096
RMSE train: 0.472358	val: 1.096598	test: 1.075301
MAE train: 0.347863	val: 0.866033	test: 0.807600

Epoch: 68
Loss: 0.3138124719262123
RMSE train: 0.494417	val: 1.079839	test: 1.099863
MAE train: 0.358205	val: 0.861861	test: 0.837589

Epoch: 69
Loss: 0.354844406247139
RMSE train: 0.484505	val: 1.097980	test: 1.129629
MAE train: 0.350642	val: 0.878478	test: 0.851379

Epoch: 70
Loss: 0.3265207037329674
RMSE train: 0.473753	val: 1.083406	test: 1.106087
MAE train: 0.344377	val: 0.872186	test: 0.841542

Epoch: 71
Loss: 0.32331695407629013
RMSE train: 0.457174	val: 1.029303	test: 1.092855
MAE train: 0.329181	val: 0.832165	test: 0.802809

Epoch: 72
Loss: 0.34136948361992836
RMSE train: 0.497856	val: 1.121297	test: 1.144376
MAE train: 0.354770	val: 0.897277	test: 0.851335

Epoch: 73
Loss: 0.32279209047555923
RMSE train: 0.596130	val: 1.252572	test: 1.228879
MAE train: 0.425384	val: 1.004850	test: 0.975305

Epoch: 74
Loss: 0.3031797520816326
RMSE train: 0.456481	val: 1.026767	test: 1.071674
MAE train: 0.328462	val: 0.845568	test: 0.803386

Epoch: 75
Loss: 0.3490341156721115
RMSE train: 0.534738	val: 1.212083	test: 1.172375
MAE train: 0.379227	val: 0.957564	test: 0.916979

Epoch: 76
Loss: 0.3165254592895508
RMSE train: 0.500027	val: 1.134966	test: 1.117649
MAE train: 0.358668	val: 0.898573	test: 0.862886

Epoch: 77
Loss: 0.28927383944392204
RMSE train: 0.462314	val: 1.030797	test: 1.052993
MAE train: 0.331868	val: 0.828599	test: 0.791915

Epoch: 78
Loss: 0.3288298100233078
RMSE train: 0.546136	val: 1.112982	test: 1.123601
MAE train: 0.391412	val: 0.900318	test: 0.859507

Epoch: 79
Loss: 0.3114830404520035
RMSE train: 0.449185	val: 1.109079	test: 1.112726
MAE train: 0.324059	val: 0.880781	test: 0.824710

Epoch: 80
Loss: 0.29776477813720703
RMSE train: 0.418618	val: 1.004685	test: 1.068644
MAE train: 0.299538	val: 0.817811	test: 0.784663

Epoch: 81
Loss: 0.35347244143486023
RMSE train: 0.476553	val: 1.045463	test: 1.094578
MAE train: 0.342999	val: 0.850512	test: 0.824749

Epoch: 82
Loss: 0.3159331977367401
RMSE train: 0.447040	val: 1.096181	test: 1.100781
MAE train: 0.324501	val: 0.882776	test: 0.821167

Epoch: 83
Loss: 0.3289997801184654
RMSE train: 0.410062	val: 1.029614	test: 1.064371
MAE train: 0.299793	val: 0.837922	test: 0.782530
RMSE train: 0.684255	val: 1.244951	test: 1.334865
MAE train: 0.502793	val: 1.031142	test: 1.043699

Epoch: 24
Loss: 0.6374399811029434
RMSE train: 0.621694	val: 1.225947	test: 1.219679
MAE train: 0.468048	val: 0.999411	test: 0.929444

Epoch: 25
Loss: 0.6145806014537811
RMSE train: 0.659486	val: 1.245937	test: 1.318176
MAE train: 0.480807	val: 1.019805	test: 1.024244

Epoch: 26
Loss: 0.5975166708230972
RMSE train: 0.644952	val: 1.124048	test: 1.228898
MAE train: 0.466334	val: 0.942732	test: 0.947876

Epoch: 27
Loss: 0.5720284879207611
RMSE train: 0.674998	val: 1.350339	test: 1.278591
MAE train: 0.507272	val: 1.092306	test: 1.015550

Epoch: 28
Loss: 0.5662201195955276
RMSE train: 0.731151	val: 1.397790	test: 1.331454
MAE train: 0.542575	val: 1.141180	test: 1.052975

Epoch: 29
Loss: 0.5361660271883011
RMSE train: 0.744534	val: 1.358259	test: 1.354370
MAE train: 0.550760	val: 1.136209	test: 1.061787

Epoch: 30
Loss: 0.56624486297369
RMSE train: 0.557500	val: 1.121611	test: 1.122345
MAE train: 0.411737	val: 0.929067	test: 0.881506

Epoch: 31
Loss: 0.5632898211479187
RMSE train: 0.525575	val: 1.149347	test: 1.119089
MAE train: 0.390782	val: 0.938245	test: 0.874384

Epoch: 32
Loss: 0.5007382184267044
RMSE train: 0.585918	val: 1.096394	test: 1.134426
MAE train: 0.429816	val: 0.910850	test: 0.875896

Epoch: 33
Loss: 0.48258768022060394
RMSE train: 0.523413	val: 1.041366	test: 1.102866
MAE train: 0.376617	val: 0.876820	test: 0.834964

Epoch: 34
Loss: 0.4798246771097183
RMSE train: 0.537764	val: 1.202188	test: 1.129473
MAE train: 0.392750	val: 0.972149	test: 0.853171

Epoch: 35
Loss: 0.5910226702690125
RMSE train: 0.487078	val: 1.081153	test: 1.101103
MAE train: 0.363699	val: 0.894400	test: 0.833453

Epoch: 36
Loss: 0.4194843992590904
RMSE train: 0.515042	val: 1.099795	test: 1.128397
MAE train: 0.384110	val: 0.914050	test: 0.855484

Epoch: 37
Loss: 0.4363711252808571
RMSE train: 0.541586	val: 1.142519	test: 1.157927
MAE train: 0.398996	val: 0.933459	test: 0.886423

Epoch: 38
Loss: 0.4026453495025635
RMSE train: 0.496433	val: 1.032238	test: 1.091504
MAE train: 0.355489	val: 0.848861	test: 0.839217

Epoch: 39
Loss: 0.46129487454891205
RMSE train: 0.545874	val: 1.095790	test: 1.157400
MAE train: 0.393249	val: 0.897486	test: 0.897484

Epoch: 40
Loss: 0.4421219229698181
RMSE train: 0.522069	val: 1.131695	test: 1.158864
MAE train: 0.378864	val: 0.910025	test: 0.899409

Epoch: 41
Loss: 0.46538493037223816
RMSE train: 0.517605	val: 1.126007	test: 1.136934
MAE train: 0.376200	val: 0.904287	test: 0.874384

Epoch: 42
Loss: 0.41552069038152695
RMSE train: 0.527634	val: 1.137047	test: 1.129056
MAE train: 0.387987	val: 0.913931	test: 0.874399

Epoch: 43
Loss: 0.388866126537323
RMSE train: 0.510262	val: 1.076883	test: 1.096891
MAE train: 0.370312	val: 0.865920	test: 0.847088

Epoch: 44
Loss: 0.4079854488372803
RMSE train: 0.525304	val: 1.134455	test: 1.123201
MAE train: 0.384149	val: 0.903296	test: 0.874663

Epoch: 45
Loss: 0.4029776155948639
RMSE train: 0.487656	val: 1.145443	test: 1.113573
MAE train: 0.361336	val: 0.904135	test: 0.867098

Epoch: 46
Loss: 0.414002925157547
RMSE train: 0.462834	val: 1.142647	test: 1.111461
MAE train: 0.339449	val: 0.899275	test: 0.869416

Epoch: 47
Loss: 0.41449081152677536
RMSE train: 0.437753	val: 1.081393	test: 1.093445
MAE train: 0.326283	val: 0.872622	test: 0.860285

Epoch: 48
Loss: 0.3715825825929642
RMSE train: 0.467608	val: 1.157034	test: 1.120467
MAE train: 0.355589	val: 0.923069	test: 0.866930

Epoch: 49
Loss: 0.40803050249814987
RMSE train: 0.443041	val: 1.045334	test: 1.106108
MAE train: 0.338028	val: 0.856652	test: 0.852055

Epoch: 50
Loss: 0.38831330835819244
RMSE train: 0.539645	val: 1.169522	test: 1.154484
MAE train: 0.395020	val: 0.951481	test: 0.893319

Epoch: 51
Loss: 0.3859099820256233
RMSE train: 0.489334	val: 1.135479	test: 1.142759
MAE train: 0.355525	val: 0.932842	test: 0.874588

Epoch: 52
Loss: 0.3874253034591675
RMSE train: 0.471903	val: 1.165107	test: 1.135865
MAE train: 0.338465	val: 0.932579	test: 0.878974

Epoch: 53
Loss: 0.3378245085477829
RMSE train: 0.477915	val: 1.079906	test: 1.121859
MAE train: 0.341246	val: 0.865300	test: 0.881688

Epoch: 54
Loss: 0.3789661154150963
RMSE train: 0.490696	val: 1.072427	test: 1.131183
MAE train: 0.349420	val: 0.895018	test: 0.870967

Epoch: 55
Loss: 0.36622514575719833
RMSE train: 0.554971	val: 1.128316	test: 1.158777
MAE train: 0.410618	val: 0.947277	test: 0.897257

Epoch: 56
Loss: 0.34907788783311844
RMSE train: 0.452999	val: 1.098434	test: 1.097777
MAE train: 0.333356	val: 0.899861	test: 0.842749

Epoch: 57
Loss: 0.3530009761452675
RMSE train: 0.502530	val: 1.202843	test: 1.162538
MAE train: 0.375332	val: 0.976388	test: 0.908950

Epoch: 58
Loss: 0.3758867233991623
RMSE train: 0.459702	val: 1.049128	test: 1.088501
MAE train: 0.338902	val: 0.874185	test: 0.830357

Epoch: 59
Loss: 0.3342312127351761
RMSE train: 0.440519	val: 1.027714	test: 1.061462
MAE train: 0.319166	val: 0.843573	test: 0.811144

Epoch: 60
Loss: 0.32315056025981903
RMSE train: 0.539225	val: 1.173757	test: 1.155655
MAE train: 0.390868	val: 0.936014	test: 0.908195

Epoch: 61
Loss: 0.3490966409444809
RMSE train: 0.477118	val: 1.090595	test: 1.153543
MAE train: 0.355733	val: 0.894006	test: 0.879745

Epoch: 62
Loss: 0.3540467619895935
RMSE train: 0.515420	val: 1.132404	test: 1.154935
MAE train: 0.376412	val: 0.922520	test: 0.903151

Epoch: 63
Loss: 0.3970213979482651
RMSE train: 0.493413	val: 1.066835	test: 1.120842
MAE train: 0.353001	val: 0.876770	test: 0.862179

Epoch: 64
Loss: 0.40522217750549316
RMSE train: 0.477870	val: 1.019647	test: 1.117727
MAE train: 0.344214	val: 0.850117	test: 0.837512

Epoch: 65
Loss: 0.3382146954536438
RMSE train: 0.555805	val: 1.226684	test: 1.184414
MAE train: 0.399710	val: 0.992419	test: 0.922866

Epoch: 66
Loss: 0.3203897476196289
RMSE train: 0.465678	val: 1.072545	test: 1.111176
MAE train: 0.340143	val: 0.881096	test: 0.837522

Epoch: 67
Loss: 0.3533179685473442
RMSE train: 0.491213	val: 1.104959	test: 1.124183
MAE train: 0.355421	val: 0.909229	test: 0.862119

Epoch: 68
Loss: 0.3335913345217705
RMSE train: 0.488306	val: 1.100660	test: 1.113721
MAE train: 0.356685	val: 0.900978	test: 0.847144

Epoch: 69
Loss: 0.3083302229642868
RMSE train: 0.404117	val: 1.084215	test: 1.097883
MAE train: 0.295944	val: 0.870532	test: 0.845379

Epoch: 70
Loss: 0.3435979038476944
RMSE train: 0.504202	val: 1.158266	test: 1.140302
MAE train: 0.356822	val: 0.927920	test: 0.885795

Epoch: 71
Loss: 0.3827841728925705
RMSE train: 0.422346	val: 1.013240	test: 1.074251
MAE train: 0.304493	val: 0.829590	test: 0.830609

Epoch: 72
Loss: 0.3195232227444649
RMSE train: 0.419544	val: 0.950534	test: 1.065032
MAE train: 0.301974	val: 0.807068	test: 0.818785

Epoch: 73
Loss: 0.332933209836483
RMSE train: 0.530044	val: 1.155819	test: 1.192064
MAE train: 0.397560	val: 0.939739	test: 0.943164

Epoch: 74
Loss: 0.31454703211784363
RMSE train: 0.422357	val: 1.043065	test: 1.142156
MAE train: 0.316762	val: 0.863730	test: 0.868694

Epoch: 75
Loss: 0.3219844922423363
RMSE train: 0.491868	val: 1.029470	test: 1.147089
MAE train: 0.348803	val: 0.867478	test: 0.873608

Epoch: 76
Loss: 0.31235481053590775
RMSE train: 0.503895	val: 1.150321	test: 1.182941
MAE train: 0.368283	val: 0.939594	test: 0.917793

Epoch: 77
Loss: 0.3224564343690872
RMSE train: 0.392534	val: 1.099871	test: 1.078041
MAE train: 0.292778	val: 0.881501	test: 0.809066

Epoch: 78
Loss: 0.32824282348155975
RMSE train: 0.392819	val: 0.991296	test: 1.062468
MAE train: 0.285791	val: 0.823144	test: 0.812863

Epoch: 79
Loss: 0.3157604858279228
RMSE train: 0.403048	val: 1.045032	test: 1.094416
MAE train: 0.292612	val: 0.858635	test: 0.842127

Epoch: 80
Loss: 0.33249257504940033
RMSE train: 0.420284	val: 1.038571	test: 1.107024
MAE train: 0.303564	val: 0.861678	test: 0.852140

Epoch: 81
Loss: 0.29603729397058487
RMSE train: 0.430495	val: 1.034106	test: 1.105159
MAE train: 0.305040	val: 0.855437	test: 0.864023

Epoch: 82
Loss: 0.31389760971069336
RMSE train: 0.408774	val: 1.157259	test: 1.191778
MAE train: 0.296091	val: 0.920469	test: 0.927158

Epoch: 83
Loss: 0.296983040869236
RMSE train: 0.387240	val: 1.088212	test: 1.145777
MAE train: 0.291936	val: 0.877279	test: 0.884062
RMSE train: 0.813545	val: 1.462600	test: 1.474455
MAE train: 0.615133	val: 1.193117	test: 1.170114

Epoch: 24
Loss: 0.6525920331478119
RMSE train: 0.686315	val: 1.144013	test: 1.209905
MAE train: 0.515476	val: 0.949350	test: 0.923844

Epoch: 25
Loss: 0.596671886742115
RMSE train: 0.794877	val: 1.301944	test: 1.393982
MAE train: 0.587319	val: 1.066362	test: 1.104007

Epoch: 26
Loss: 0.5454783886671066
RMSE train: 0.641202	val: 1.174556	test: 1.216482
MAE train: 0.466663	val: 0.947508	test: 0.940726

Epoch: 27
Loss: 0.6125291585922241
RMSE train: 0.695783	val: 1.319842	test: 1.292791
MAE train: 0.520977	val: 1.063962	test: 1.003456

Epoch: 28
Loss: 0.5346669107675552
RMSE train: 0.797304	val: 1.304024	test: 1.357926
MAE train: 0.586992	val: 1.087049	test: 1.061808

Epoch: 29
Loss: 0.5279115438461304
RMSE train: 0.623093	val: 1.214493	test: 1.167689
MAE train: 0.461579	val: 0.974417	test: 0.912090

Epoch: 30
Loss: 0.4901992902159691
RMSE train: 0.679567	val: 1.411195	test: 1.294779
MAE train: 0.513137	val: 1.117893	test: 1.030127

Epoch: 31
Loss: 0.5224382132291794
RMSE train: 0.555561	val: 1.173692	test: 1.155899
MAE train: 0.404270	val: 0.959978	test: 0.877553

Epoch: 32
Loss: 0.48160988837480545
RMSE train: 0.516374	val: 1.113162	test: 1.109277
MAE train: 0.379789	val: 0.904218	test: 0.856210

Epoch: 33
Loss: 0.48685163259506226
RMSE train: 0.660970	val: 1.325850	test: 1.302240
MAE train: 0.493380	val: 1.063529	test: 1.032064

Epoch: 34
Loss: 0.4529833570122719
RMSE train: 0.587550	val: 1.152379	test: 1.210810
MAE train: 0.427778	val: 0.944682	test: 0.934237

Epoch: 35
Loss: 0.4931853488087654
RMSE train: 0.546760	val: 1.153872	test: 1.180197
MAE train: 0.403936	val: 0.938014	test: 0.906075

Epoch: 36
Loss: 0.4750903248786926
RMSE train: 0.528207	val: 1.166024	test: 1.149138
MAE train: 0.381729	val: 0.951751	test: 0.890623

Epoch: 37
Loss: 0.4544535502791405
RMSE train: 0.523790	val: 1.051732	test: 1.095448
MAE train: 0.370949	val: 0.863963	test: 0.837345

Epoch: 38
Loss: 0.46619825065135956
RMSE train: 0.553040	val: 1.130393	test: 1.152249
MAE train: 0.395406	val: 0.918416	test: 0.888764

Epoch: 39
Loss: 0.4412877932190895
RMSE train: 0.634647	val: 1.270324	test: 1.272723
MAE train: 0.476162	val: 1.018813	test: 0.993211

Epoch: 40
Loss: 0.4523939788341522
RMSE train: 0.546483	val: 1.051248	test: 1.125647
MAE train: 0.401577	val: 0.874773	test: 0.856943

Epoch: 41
Loss: 0.4357490763068199
RMSE train: 0.627943	val: 1.194539	test: 1.192185
MAE train: 0.470098	val: 0.961327	test: 0.929899

Epoch: 42
Loss: 0.45742470026016235
RMSE train: 0.611445	val: 1.287756	test: 1.245014
MAE train: 0.454627	val: 1.023790	test: 0.981785

Epoch: 43
Loss: 0.4480767995119095
RMSE train: 0.536095	val: 1.065503	test: 1.097703
MAE train: 0.398387	val: 0.862075	test: 0.851811

Epoch: 44
Loss: 0.42760898917913437
RMSE train: 0.527011	val: 1.153084	test: 1.142375
MAE train: 0.383772	val: 0.923705	test: 0.873373

Epoch: 45
Loss: 0.50430928170681
RMSE train: 0.526999	val: 1.172955	test: 1.177453
MAE train: 0.380643	val: 0.933997	test: 0.905199

Epoch: 46
Loss: 0.42372556775808334
RMSE train: 0.485751	val: 1.142104	test: 1.121845
MAE train: 0.362054	val: 0.899797	test: 0.874904

Epoch: 47
Loss: 0.4589189812541008
RMSE train: 0.516164	val: 1.079400	test: 1.138411
MAE train: 0.371617	val: 0.883698	test: 0.884663

Epoch: 48
Loss: 0.454343318939209
RMSE train: 0.638243	val: 1.200245	test: 1.243225
MAE train: 0.463274	val: 0.963800	test: 0.977852

Epoch: 49
Loss: 0.438163623213768
RMSE train: 0.531579	val: 1.047276	test: 1.128390
MAE train: 0.381779	val: 0.863321	test: 0.864919

Epoch: 50
Loss: 0.3818916380405426
RMSE train: 0.601154	val: 1.055491	test: 1.146705
MAE train: 0.436963	val: 0.861962	test: 0.880772

Epoch: 51
Loss: 0.3875817358493805
RMSE train: 0.564894	val: 1.111576	test: 1.153525
MAE train: 0.404136	val: 0.902355	test: 0.891633

Epoch: 52
Loss: 0.39069703221321106
RMSE train: 0.491043	val: 1.122230	test: 1.146770
MAE train: 0.350332	val: 0.890189	test: 0.883484

Epoch: 53
Loss: 0.38821590691804886
RMSE train: 0.466530	val: 1.030754	test: 1.126121
MAE train: 0.334064	val: 0.837834	test: 0.867263

Epoch: 54
Loss: 0.3794470876455307
RMSE train: 0.504442	val: 1.092215	test: 1.135569
MAE train: 0.360681	val: 0.884503	test: 0.877923

Epoch: 55
Loss: 0.36397169530391693
RMSE train: 0.433287	val: 1.092997	test: 1.141090
MAE train: 0.319132	val: 0.875955	test: 0.870858

Epoch: 56
Loss: 0.3502107039093971
RMSE train: 0.485881	val: 1.212724	test: 1.189547
MAE train: 0.353406	val: 0.957344	test: 0.931057

Epoch: 57
Loss: 0.3271045535802841
RMSE train: 0.477597	val: 1.129806	test: 1.130962
MAE train: 0.351536	val: 0.903407	test: 0.888915

Epoch: 58
Loss: 0.37119947373867035
RMSE train: 0.517546	val: 1.089478	test: 1.112319
MAE train: 0.372957	val: 0.885826	test: 0.867882

Epoch: 59
Loss: 0.3716188073158264
RMSE train: 0.506901	val: 1.098326	test: 1.107953
MAE train: 0.361779	val: 0.894527	test: 0.855428

Epoch: 60
Loss: 0.36156777292490005
RMSE train: 0.460381	val: 1.102282	test: 1.083555
MAE train: 0.337431	val: 0.887549	test: 0.835066

Epoch: 61
Loss: 0.36334237456321716
RMSE train: 0.442103	val: 1.048583	test: 1.053521
MAE train: 0.318181	val: 0.858228	test: 0.787722

Epoch: 62
Loss: 0.3473617359995842
RMSE train: 0.447406	val: 1.029042	test: 1.048968
MAE train: 0.324749	val: 0.839653	test: 0.785345

Epoch: 63
Loss: 0.395520456135273
RMSE train: 0.529057	val: 1.089363	test: 1.106776
MAE train: 0.384247	val: 0.887544	test: 0.853169

Epoch: 64
Loss: 0.324391134083271
RMSE train: 0.548803	val: 1.101372	test: 1.133933
MAE train: 0.400241	val: 0.911534	test: 0.867934

Epoch: 65
Loss: 0.33707527071237564
RMSE train: 0.573406	val: 1.169729	test: 1.174250
MAE train: 0.416168	val: 0.941580	test: 0.915336

Epoch: 66
Loss: 0.31667330861091614
RMSE train: 0.529422	val: 1.190668	test: 1.162524
MAE train: 0.387488	val: 0.968471	test: 0.895692

Epoch: 67
Loss: 0.3762170523405075
RMSE train: 0.482819	val: 1.170095	test: 1.145001
MAE train: 0.359319	val: 0.918038	test: 0.864076

Epoch: 68
Loss: 0.36724382638931274
RMSE train: 0.466230	val: 1.094815	test: 1.106517
MAE train: 0.334087	val: 0.880548	test: 0.828432

Epoch: 69
Loss: 0.3366231098771095
RMSE train: 0.450606	val: 1.123815	test: 1.117312
MAE train: 0.333698	val: 0.895798	test: 0.824648

Epoch: 70
Loss: 0.3539852350950241
RMSE train: 0.415266	val: 1.200781	test: 1.100575
MAE train: 0.300984	val: 0.925094	test: 0.841434

Epoch: 71
Loss: 0.34881874173879623
RMSE train: 0.452293	val: 1.027871	test: 1.088934
MAE train: 0.318780	val: 0.845485	test: 0.821076

Epoch: 72
Loss: 0.3504195362329483
RMSE train: 0.460739	val: 1.114966	test: 1.115397
MAE train: 0.325543	val: 0.903267	test: 0.850214

Epoch: 73
Loss: 0.3172837346792221
RMSE train: 0.490621	val: 1.238755	test: 1.179084
MAE train: 0.360662	val: 0.983031	test: 0.917330

Epoch: 74
Loss: 0.3185035213828087
RMSE train: 0.521527	val: 1.176291	test: 1.174454
MAE train: 0.379654	val: 0.956339	test: 0.901761

Epoch: 75
Loss: 0.33622507005929947
RMSE train: 0.542159	val: 1.106863	test: 1.134897
MAE train: 0.389016	val: 0.887626	test: 0.873875

Epoch: 76
Loss: 0.3085997700691223
RMSE train: 0.420126	val: 1.048273	test: 1.080017
MAE train: 0.303314	val: 0.835534	test: 0.802200

Epoch: 77
Loss: 0.3300674632191658
RMSE train: 0.499962	val: 1.125266	test: 1.129278
MAE train: 0.366861	val: 0.905516	test: 0.876700

Epoch: 78
Loss: 0.31027213484048843
RMSE train: 0.466878	val: 1.061342	test: 1.086494
MAE train: 0.332388	val: 0.841923	test: 0.836186

Epoch: 79
Loss: 0.28853554651141167
RMSE train: 0.440309	val: 1.021709	test: 1.072199
MAE train: 0.315903	val: 0.830089	test: 0.795571

Epoch: 80
Loss: 0.3249383866786957
RMSE train: 0.499386	val: 1.199069	test: 1.143894
MAE train: 0.375377	val: 0.964357	test: 0.899297

Epoch: 81
Loss: 0.2937552258372307
RMSE train: 0.432881	val: 1.067169	test: 1.108988
MAE train: 0.317449	val: 0.865184	test: 0.813945

Epoch: 82
Loss: 0.3074098825454712
RMSE train: 0.473089	val: 1.103745	test: 1.119736
MAE train: 0.340120	val: 0.899771	test: 0.845671

Epoch: 83
Loss: 0.32514023780822754
RMSE train: 0.435750	val: 1.092753	test: 1.090892
MAE train: 0.315652	val: 0.893542	test: 0.829770
MAE train: 0.615023	val: 0.991280	test: 1.059729

Epoch: 23
Loss: 0.7506129741668701
RMSE train: 0.917765	val: 1.247456	test: 1.398746
MAE train: 0.687761	val: 0.988008	test: 1.118777

Epoch: 24
Loss: 0.8171473443508148
RMSE train: 0.711341	val: 1.081596	test: 1.207220
MAE train: 0.528764	val: 0.878370	test: 0.960123

Epoch: 25
Loss: 0.6896959543228149
RMSE train: 0.655643	val: 1.154567	test: 1.195865
MAE train: 0.486617	val: 0.948600	test: 0.967141

Epoch: 26
Loss: 0.682623103260994
RMSE train: 0.696617	val: 1.124912	test: 1.163965
MAE train: 0.517248	val: 0.913156	test: 0.935746

Epoch: 27
Loss: 0.7694549113512039
RMSE train: 0.668317	val: 1.077776	test: 1.138090
MAE train: 0.496322	val: 0.879703	test: 0.885913

Epoch: 28
Loss: 0.7097890079021454
RMSE train: 0.691658	val: 1.173837	test: 1.192594
MAE train: 0.521220	val: 0.943331	test: 0.988459

Epoch: 29
Loss: 0.5984555333852768
RMSE train: 0.657007	val: 1.183127	test: 1.225613
MAE train: 0.490861	val: 0.962043	test: 0.990646

Epoch: 30
Loss: 0.6351899802684784
RMSE train: 0.687785	val: 1.199844	test: 1.263385
MAE train: 0.518761	val: 0.984770	test: 1.008150

Epoch: 31
Loss: 0.6066402792930603
RMSE train: 0.633759	val: 1.081116	test: 1.132937
MAE train: 0.476738	val: 0.887023	test: 0.897499

Epoch: 32
Loss: 0.595551148056984
RMSE train: 0.612019	val: 1.072268	test: 1.132881
MAE train: 0.456565	val: 0.882856	test: 0.887162

Epoch: 33
Loss: 0.6104966104030609
RMSE train: 0.671797	val: 1.121885	test: 1.200776
MAE train: 0.502934	val: 0.907847	test: 0.939613

Epoch: 34
Loss: 0.6011991202831268
RMSE train: 0.649632	val: 1.129492	test: 1.185846
MAE train: 0.488557	val: 0.929798	test: 0.930601

Epoch: 35
Loss: 0.5680171325802803
RMSE train: 0.763486	val: 1.247461	test: 1.283386
MAE train: 0.587320	val: 1.002693	test: 1.039064

Epoch: 36
Loss: 0.5736898183822632
RMSE train: 0.734417	val: 1.119598	test: 1.243468
MAE train: 0.574496	val: 0.911137	test: 0.963344

Epoch: 37
Loss: 0.5595381408929825
RMSE train: 0.698484	val: 1.126170	test: 1.273175
MAE train: 0.538532	val: 0.920997	test: 1.001443

Epoch: 38
Loss: 0.5402911007404327
RMSE train: 0.722591	val: 1.255904	test: 1.330144
MAE train: 0.556292	val: 1.011136	test: 1.089916

Epoch: 39
Loss: 0.5957194864749908
RMSE train: 0.635160	val: 1.059672	test: 1.152011
MAE train: 0.487485	val: 0.877005	test: 0.900413

Epoch: 40
Loss: 0.5029204040765762
RMSE train: 0.811560	val: 1.262063	test: 1.357724
MAE train: 0.609796	val: 1.033108	test: 1.072758

Epoch: 41
Loss: 0.538999155163765
RMSE train: 0.676288	val: 1.169300	test: 1.238844
MAE train: 0.514950	val: 0.963427	test: 0.973150

Epoch: 42
Loss: 0.5154440850019455
RMSE train: 0.547980	val: 1.056923	test: 1.125908
MAE train: 0.418149	val: 0.876368	test: 0.888915

Epoch: 43
Loss: 0.4595709592103958
RMSE train: 0.649454	val: 1.134130	test: 1.184090
MAE train: 0.493381	val: 0.918133	test: 0.950890

Epoch: 44
Loss: 0.4496207684278488
RMSE train: 0.604947	val: 1.087631	test: 1.168418
MAE train: 0.451084	val: 0.901183	test: 0.915776

Epoch: 45
Loss: 0.4519145116209984
RMSE train: 0.528621	val: 1.076657	test: 1.138800
MAE train: 0.393523	val: 0.877106	test: 0.909616

Epoch: 46
Loss: 0.4487728253006935
RMSE train: 0.524827	val: 1.126404	test: 1.159791
MAE train: 0.392684	val: 0.905034	test: 0.943543

Epoch: 47
Loss: 0.4462372958660126
RMSE train: 0.612875	val: 1.137538	test: 1.200766
MAE train: 0.459890	val: 0.921400	test: 0.938069

Epoch: 48
Loss: 0.4740944132208824
RMSE train: 0.520919	val: 1.128483	test: 1.168071
MAE train: 0.392105	val: 0.905750	test: 0.929708

Epoch: 49
Loss: 0.4175857901573181
RMSE train: 0.494523	val: 1.098961	test: 1.157880
MAE train: 0.375770	val: 0.903359	test: 0.922561

Epoch: 50
Loss: 0.45521486550569534
RMSE train: 0.603243	val: 1.157306	test: 1.221155
MAE train: 0.457951	val: 0.944471	test: 0.953259

Epoch: 51
Loss: 0.42476221174001694
RMSE train: 0.621179	val: 1.143245	test: 1.198860
MAE train: 0.468216	val: 0.923180	test: 0.953985

Epoch: 52
Loss: 0.4553542733192444
RMSE train: 0.490895	val: 1.004993	test: 1.094700
MAE train: 0.362789	val: 0.821850	test: 0.850702

Epoch: 53
Loss: 0.47853583097457886
RMSE train: 0.540263	val: 1.103130	test: 1.173766
MAE train: 0.406471	val: 0.886516	test: 0.924092

Epoch: 54
Loss: 0.3869098722934723
RMSE train: 0.508041	val: 1.118076	test: 1.169781
MAE train: 0.382873	val: 0.900566	test: 0.913011

Epoch: 55
Loss: 0.39523351937532425
RMSE train: 0.569757	val: 1.106989	test: 1.158685
MAE train: 0.425941	val: 0.878915	test: 0.905567

Epoch: 56
Loss: 0.41026290506124496
RMSE train: 0.509317	val: 1.068544	test: 1.114496
MAE train: 0.383130	val: 0.867247	test: 0.862690

Epoch: 57
Loss: 0.3642442673444748
RMSE train: 0.516147	val: 1.144987	test: 1.157987
MAE train: 0.387234	val: 0.921866	test: 0.911165

Epoch: 58
Loss: 0.4211520031094551
RMSE train: 0.460557	val: 1.100943	test: 1.116138
MAE train: 0.345093	val: 0.888001	test: 0.890327

Epoch: 59
Loss: 0.3979355990886688
RMSE train: 0.510501	val: 1.097847	test: 1.120204
MAE train: 0.376401	val: 0.885177	test: 0.892747

Epoch: 60
Loss: 0.3621561527252197
RMSE train: 0.523547	val: 1.070544	test: 1.106873
MAE train: 0.393876	val: 0.883790	test: 0.843515

Epoch: 61
Loss: 0.40751293301582336
RMSE train: 0.488439	val: 1.042898	test: 1.087479
MAE train: 0.361753	val: 0.839226	test: 0.836414

Epoch: 62
Loss: 0.369898684322834
RMSE train: 0.562913	val: 1.118317	test: 1.159807
MAE train: 0.423844	val: 0.891643	test: 0.913770

Epoch: 63
Loss: 0.41997892409563065
RMSE train: 0.561561	val: 1.080059	test: 1.174788
MAE train: 0.415677	val: 0.871745	test: 0.889918

Epoch: 64
Loss: 0.3687005490064621
RMSE train: 0.597041	val: 1.177152	test: 1.211766
MAE train: 0.451992	val: 0.937420	test: 0.971332

Epoch: 65
Loss: 0.3919253796339035
RMSE train: 0.453439	val: 1.071248	test: 1.111638
MAE train: 0.341264	val: 0.878120	test: 0.869935

Epoch: 66
Loss: 0.36329619586467743
RMSE train: 0.608768	val: 1.176525	test: 1.222691
MAE train: 0.464287	val: 0.959906	test: 0.946859

Epoch: 67
Loss: 0.34539568424224854
RMSE train: 0.462414	val: 1.069750	test: 1.110485
MAE train: 0.347577	val: 0.882205	test: 0.858464

Epoch: 68
Loss: 0.351953886449337
RMSE train: 0.455322	val: 1.091559	test: 1.116755
MAE train: 0.343164	val: 0.886445	test: 0.870192

Epoch: 69
Loss: 0.3354504033923149
RMSE train: 0.567482	val: 1.116754	test: 1.191241
MAE train: 0.417816	val: 0.917138	test: 0.901355

Epoch: 70
Loss: 0.4208660274744034
RMSE train: 0.589340	val: 1.156061	test: 1.207915
MAE train: 0.439140	val: 0.930449	test: 0.942374

Epoch: 71
Loss: 0.4209981635212898
RMSE train: 0.470275	val: 1.092821	test: 1.127313
MAE train: 0.352418	val: 0.886803	test: 0.869773

Epoch: 72
Loss: 0.33586040884256363
RMSE train: 0.514411	val: 1.050387	test: 1.150486
MAE train: 0.357070	val: 0.868574	test: 0.844842

Epoch: 73
Loss: 0.3937976732850075
RMSE train: 0.551555	val: 1.095075	test: 1.130383
MAE train: 0.411732	val: 0.876335	test: 0.888417

Epoch: 74
Loss: 0.34899088740348816
RMSE train: 0.453746	val: 1.026075	test: 1.097467
MAE train: 0.347205	val: 0.861660	test: 0.822726

Epoch: 75
Loss: 0.3154548481106758
RMSE train: 0.492738	val: 1.128694	test: 1.127911
MAE train: 0.368813	val: 0.899479	test: 0.882853

Epoch: 76
Loss: 0.3306988626718521
RMSE train: 0.477969	val: 1.129703	test: 1.141496
MAE train: 0.355165	val: 0.901729	test: 0.914211

Epoch: 77
Loss: 0.39086484909057617
RMSE train: 0.408980	val: 1.036965	test: 1.133569
MAE train: 0.304119	val: 0.852877	test: 0.888143

Epoch: 78
Loss: 0.3309689834713936
RMSE train: 0.444946	val: 1.067034	test: 1.130358
MAE train: 0.331103	val: 0.864143	test: 0.881759

Epoch: 79
Loss: 0.29783330112695694
RMSE train: 0.391792	val: 1.019253	test: 1.104215
MAE train: 0.296703	val: 0.857116	test: 0.855800

Epoch: 80
Loss: 0.264778696000576
RMSE train: 0.456545	val: 1.071652	test: 1.135988
MAE train: 0.345804	val: 0.877502	test: 0.899208

Epoch: 81
Loss: 0.28542231023311615
RMSE train: 0.435013	val: 1.073326	test: 1.139199
MAE train: 0.330112	val: 0.883310	test: 0.899641

Epoch: 82
Loss: 0.2980972155928612
RMSE train: 0.491982	val: 1.033418	test: 1.129480
MAE train: 0.363152	val: 0.861292	test: 0.865806

Epoch: 83
Loss: 0.3091369792819023
MAE train: 0.639220	val: 1.000106	test: 1.037572

Epoch: 23
Loss: 0.7554826736450195
RMSE train: 0.831720	val: 1.225807	test: 1.322393
MAE train: 0.626503	val: 1.002347	test: 1.034861

Epoch: 24
Loss: 0.728635624051094
RMSE train: 0.725607	val: 1.085266	test: 1.220502
MAE train: 0.548258	val: 0.908269	test: 0.966386

Epoch: 25
Loss: 0.7312770932912827
RMSE train: 0.781584	val: 1.181455	test: 1.317256
MAE train: 0.585249	val: 0.984462	test: 1.022887

Epoch: 26
Loss: 0.690587505698204
RMSE train: 0.783388	val: 1.207360	test: 1.297789
MAE train: 0.588786	val: 0.996722	test: 1.004670

Epoch: 27
Loss: 0.6275377422571182
RMSE train: 0.722930	val: 1.148582	test: 1.224188
MAE train: 0.545427	val: 0.964002	test: 0.959874

Epoch: 28
Loss: 0.6425462365150452
RMSE train: 0.685871	val: 1.145103	test: 1.238675
MAE train: 0.514499	val: 0.958364	test: 0.962077

Epoch: 29
Loss: 0.6030852645635605
RMSE train: 0.735175	val: 1.159601	test: 1.265880
MAE train: 0.557873	val: 0.951049	test: 0.979627

Epoch: 30
Loss: 0.623725950717926
RMSE train: 0.723365	val: 1.099592	test: 1.239897
MAE train: 0.543160	val: 0.903301	test: 0.962777

Epoch: 31
Loss: 0.6080025881528854
RMSE train: 0.798414	val: 1.254758	test: 1.402465
MAE train: 0.615659	val: 1.044216	test: 1.060384

Epoch: 32
Loss: 0.6099992841482162
RMSE train: 0.744002	val: 1.150831	test: 1.309127
MAE train: 0.568927	val: 0.968257	test: 1.013314

Epoch: 33
Loss: 0.578385055065155
RMSE train: 0.825203	val: 1.223080	test: 1.349578
MAE train: 0.636385	val: 1.022369	test: 1.022798

Epoch: 34
Loss: 0.5312568619847298
RMSE train: 0.776767	val: 1.305177	test: 1.379028
MAE train: 0.591018	val: 1.087324	test: 1.075625

Epoch: 35
Loss: 0.5658950060606003
RMSE train: 0.635357	val: 1.120060	test: 1.209844
MAE train: 0.483453	val: 0.929606	test: 0.934034

Epoch: 36
Loss: 0.5705983638763428
RMSE train: 0.616338	val: 1.063008	test: 1.181448
MAE train: 0.463174	val: 0.881474	test: 0.916397

Epoch: 37
Loss: 0.5354859307408333
RMSE train: 0.735452	val: 1.219675	test: 1.314399
MAE train: 0.556257	val: 1.020184	test: 1.012102

Epoch: 38
Loss: 0.5037471204996109
RMSE train: 0.641394	val: 1.112588	test: 1.187323
MAE train: 0.488217	val: 0.921499	test: 0.921693

Epoch: 39
Loss: 0.5358602628111839
RMSE train: 0.643356	val: 1.089788	test: 1.198090
MAE train: 0.486821	val: 0.904301	test: 0.926609

Epoch: 40
Loss: 0.5531424283981323
RMSE train: 0.698847	val: 1.157363	test: 1.282611
MAE train: 0.529625	val: 0.970620	test: 0.973974

Epoch: 41
Loss: 0.4987689107656479
RMSE train: 0.583757	val: 1.035178	test: 1.169930
MAE train: 0.446440	val: 0.874363	test: 0.901825

Epoch: 42
Loss: 0.5356360003352165
RMSE train: 0.627258	val: 1.108463	test: 1.182733
MAE train: 0.474089	val: 0.909298	test: 0.919901

Epoch: 43
Loss: 0.5085974633693695
RMSE train: 0.586106	val: 1.045691	test: 1.185175
MAE train: 0.439801	val: 0.871073	test: 0.914766

Epoch: 44
Loss: 0.4767889231443405
RMSE train: 0.536988	val: 1.039905	test: 1.170181
MAE train: 0.403587	val: 0.846234	test: 0.905843

Epoch: 45
Loss: 0.48206817358732224
RMSE train: 0.567359	val: 1.068417	test: 1.189149
MAE train: 0.421173	val: 0.886756	test: 0.922610

Epoch: 46
Loss: 0.455030120909214
RMSE train: 0.752324	val: 1.223140	test: 1.310567
MAE train: 0.575981	val: 1.027349	test: 1.010319

Epoch: 47
Loss: 0.41898466646671295
RMSE train: 0.703231	val: 1.178929	test: 1.295402
MAE train: 0.535991	val: 0.987565	test: 0.982375

Epoch: 48
Loss: 0.43404992669820786
RMSE train: 0.603745	val: 1.115698	test: 1.226607
MAE train: 0.460984	val: 0.931355	test: 0.930717

Epoch: 49
Loss: 0.46020785719156265
RMSE train: 0.579657	val: 1.076136	test: 1.202142
MAE train: 0.439925	val: 0.885119	test: 0.925207

Epoch: 50
Loss: 0.43038417398929596
RMSE train: 0.604631	val: 1.053342	test: 1.203981
MAE train: 0.444589	val: 0.870795	test: 0.928276

Epoch: 51
Loss: 0.4018741473555565
RMSE train: 0.707082	val: 1.191720	test: 1.305245
MAE train: 0.537943	val: 0.998065	test: 1.005728

Epoch: 52
Loss: 0.40876690298318863
RMSE train: 0.588208	val: 1.130338	test: 1.236478
MAE train: 0.452784	val: 0.944131	test: 0.961667

Epoch: 53
Loss: 0.4015221521258354
RMSE train: 0.510755	val: 1.076357	test: 1.168837
MAE train: 0.396448	val: 0.892812	test: 0.918294

Epoch: 54
Loss: 0.39493002742528915
RMSE train: 0.572713	val: 1.062717	test: 1.209867
MAE train: 0.414556	val: 0.875397	test: 0.925425

Epoch: 55
Loss: 0.49552274495363235
RMSE train: 0.624245	val: 1.084740	test: 1.218930
MAE train: 0.472748	val: 0.887870	test: 0.932033

Epoch: 56
Loss: 0.39731772243976593
RMSE train: 0.720883	val: 1.233773	test: 1.340894
MAE train: 0.560193	val: 1.034795	test: 1.019311

Epoch: 57
Loss: 0.45567433536052704
RMSE train: 0.743683	val: 1.218485	test: 1.341667
MAE train: 0.577445	val: 1.036331	test: 1.004213

Epoch: 58
Loss: 0.39677999168634415
RMSE train: 0.687752	val: 1.116796	test: 1.247960
MAE train: 0.524399	val: 0.932741	test: 0.938463

Epoch: 59
Loss: 0.3839833587408066
RMSE train: 0.618049	val: 1.082946	test: 1.199171
MAE train: 0.467759	val: 0.888960	test: 0.916060

Epoch: 60
Loss: 0.37220608443021774
RMSE train: 0.596859	val: 1.067287	test: 1.202960
MAE train: 0.447395	val: 0.891173	test: 0.916192

Epoch: 61
Loss: 0.3781404197216034
RMSE train: 0.614299	val: 1.110342	test: 1.249649
MAE train: 0.456030	val: 0.926273	test: 0.952303

Epoch: 62
Loss: 0.4072181358933449
RMSE train: 0.622651	val: 1.120862	test: 1.256106
MAE train: 0.470483	val: 0.920371	test: 0.953142

Epoch: 63
Loss: 0.3403186798095703
RMSE train: 0.659318	val: 1.187603	test: 1.305942
MAE train: 0.498481	val: 0.997284	test: 0.993432

Epoch: 64
Loss: 0.36548446863889694
RMSE train: 0.625785	val: 1.122055	test: 1.269088
MAE train: 0.474170	val: 0.941341	test: 0.961647

Epoch: 65
Loss: 0.3513863831758499
RMSE train: 0.500999	val: 1.018881	test: 1.192090
MAE train: 0.386629	val: 0.843384	test: 0.927238

Epoch: 66
Loss: 0.33321313560009
RMSE train: 0.635305	val: 1.201456	test: 1.313916
MAE train: 0.476787	val: 1.006088	test: 0.999350

Epoch: 67
Loss: 0.35974228382110596
RMSE train: 0.545440	val: 1.092028	test: 1.210077
MAE train: 0.405902	val: 0.905437	test: 0.911243

Epoch: 68
Loss: 0.38518156111240387
RMSE train: 0.546252	val: 1.103118	test: 1.206190
MAE train: 0.414289	val: 0.912263	test: 0.918088

Epoch: 69
Loss: 0.37255188822746277
RMSE train: 0.646404	val: 1.250226	test: 1.321769
MAE train: 0.462466	val: 1.028963	test: 1.022904

Epoch: 70
Loss: 0.33706343173980713
RMSE train: 0.555583	val: 1.076942	test: 1.171526
MAE train: 0.423265	val: 0.888925	test: 0.905860

Epoch: 71
Loss: 0.33563023805618286
RMSE train: 0.393195	val: 1.022701	test: 1.165957
MAE train: 0.303540	val: 0.851821	test: 0.905949

Epoch: 72
Loss: 0.3177185729146004
RMSE train: 0.581913	val: 1.179642	test: 1.263939
MAE train: 0.445901	val: 0.977294	test: 0.974458

Epoch: 73
Loss: 0.3341224119067192
RMSE train: 0.593294	val: 1.149184	test: 1.256082
MAE train: 0.438639	val: 0.963173	test: 0.950524

Epoch: 74
Loss: 0.3510954976081848
RMSE train: 0.437799	val: 0.996694	test: 1.148184
MAE train: 0.328407	val: 0.829418	test: 0.912163

Epoch: 75
Loss: 0.3483949676156044
RMSE train: 0.442279	val: 1.059398	test: 1.182298
MAE train: 0.339405	val: 0.871899	test: 0.937258

Epoch: 76
Loss: 0.33700869232416153
RMSE train: 0.458713	val: 1.021794	test: 1.150998
MAE train: 0.344742	val: 0.863012	test: 0.894937

Epoch: 77
Loss: 0.28239889070391655
RMSE train: 0.574180	val: 1.141229	test: 1.257871
MAE train: 0.429243	val: 0.951226	test: 0.941742

Epoch: 78
Loss: 0.35018691420555115
RMSE train: 0.492157	val: 1.030501	test: 1.179512
MAE train: 0.373782	val: 0.859534	test: 0.912426

Epoch: 79
Loss: 0.2962791286408901
RMSE train: 0.542688	val: 1.063152	test: 1.196479
MAE train: 0.411700	val: 0.877387	test: 0.918574

Epoch: 80
Loss: 0.3389527201652527
RMSE train: 0.468692	val: 1.011020	test: 1.166427
MAE train: 0.358207	val: 0.836957	test: 0.898746

Epoch: 81
Loss: 0.31329941749572754
RMSE train: 0.430484	val: 0.989710	test: 1.159553
MAE train: 0.331075	val: 0.829951	test: 0.892050

Epoch: 82
Loss: 0.29673586040735245
RMSE train: 0.534884	val: 1.085128	test: 1.238912
MAE train: 0.408193	val: 0.911680	test: 0.925859

Epoch: 83
Loss: 0.3073519095778465
MAE train: 0.658640	val: 1.151104	test: 1.122159

Epoch: 23
Loss: 0.8352300822734833
RMSE train: 0.832425	val: 1.315235	test: 1.380259
MAE train: 0.645326	val: 1.101050	test: 1.072029

Epoch: 24
Loss: 0.7933551222085953
RMSE train: 0.916390	val: 1.451078	test: 1.499909
MAE train: 0.714014	val: 1.212683	test: 1.183931

Epoch: 25
Loss: 0.7619577050209045
RMSE train: 0.819902	val: 1.351268	test: 1.408628
MAE train: 0.642820	val: 1.129211	test: 1.093614

Epoch: 26
Loss: 0.7561186105012894
RMSE train: 0.833327	val: 1.289152	test: 1.357286
MAE train: 0.650308	val: 1.074439	test: 1.045509

Epoch: 27
Loss: 0.7916376888751984
RMSE train: 0.891811	val: 1.365350	test: 1.428839
MAE train: 0.680017	val: 1.135828	test: 1.102228

Epoch: 28
Loss: 0.708817183971405
RMSE train: 0.750655	val: 1.280554	test: 1.314400
MAE train: 0.581250	val: 1.055655	test: 1.011469

Epoch: 29
Loss: 0.6384665966033936
RMSE train: 0.842829	val: 1.435889	test: 1.411925
MAE train: 0.655138	val: 1.155544	test: 1.120199

Epoch: 30
Loss: 0.6429259181022644
RMSE train: 0.852170	val: 1.322381	test: 1.365140
MAE train: 0.664827	val: 1.105835	test: 1.049901

Epoch: 31
Loss: 0.6440497934818268
RMSE train: 0.764483	val: 1.241450	test: 1.298568
MAE train: 0.593536	val: 1.039547	test: 1.011776

Epoch: 32
Loss: 0.6892829537391663
RMSE train: 0.685355	val: 1.278770	test: 1.320007
MAE train: 0.531658	val: 1.048476	test: 1.052928

Epoch: 33
Loss: 0.5869064629077911
RMSE train: 0.727381	val: 1.366769	test: 1.404886
MAE train: 0.561081	val: 1.135438	test: 1.097940

Epoch: 34
Loss: 0.6544937640428543
RMSE train: 0.666543	val: 1.202236	test: 1.285367
MAE train: 0.503316	val: 1.022847	test: 1.002112

Epoch: 35
Loss: 0.5469458252191544
RMSE train: 0.595616	val: 1.142556	test: 1.232135
MAE train: 0.455268	val: 0.950620	test: 0.974324

Epoch: 36
Loss: 0.5815597474575043
RMSE train: 0.610177	val: 1.239883	test: 1.310920
MAE train: 0.467633	val: 1.010626	test: 1.036777

Epoch: 37
Loss: 0.5417696684598923
RMSE train: 0.582600	val: 1.187101	test: 1.259331
MAE train: 0.446637	val: 0.975219	test: 0.988882

Epoch: 38
Loss: 0.5377938449382782
RMSE train: 0.634726	val: 1.238748	test: 1.314868
MAE train: 0.491102	val: 1.027640	test: 1.026786

Epoch: 39
Loss: 0.516339011490345
RMSE train: 0.647914	val: 1.229864	test: 1.298340
MAE train: 0.500941	val: 1.015567	test: 1.012340

Epoch: 40
Loss: 0.5039685070514679
RMSE train: 0.590404	val: 1.229227	test: 1.305854
MAE train: 0.462515	val: 1.008512	test: 1.023704

Epoch: 41
Loss: 0.45979084074497223
RMSE train: 0.694564	val: 1.348404	test: 1.409245
MAE train: 0.544647	val: 1.114250	test: 1.105846

Epoch: 42
Loss: 0.464297890663147
RMSE train: 0.671169	val: 1.309782	test: 1.383326
MAE train: 0.521120	val: 1.088410	test: 1.079763

Epoch: 43
Loss: 0.49511442333459854
RMSE train: 0.609510	val: 1.250092	test: 1.320113
MAE train: 0.463087	val: 1.043039	test: 1.016922

Epoch: 44
Loss: 0.5758170410990715
RMSE train: 0.693991	val: 1.333996	test: 1.398562
MAE train: 0.541210	val: 1.096742	test: 1.115669

Epoch: 45
Loss: 0.4478503242135048
RMSE train: 0.647033	val: 1.218664	test: 1.330810
MAE train: 0.511663	val: 1.012785	test: 1.057278

Epoch: 46
Loss: 0.46073320508003235
RMSE train: 0.747934	val: 1.406361	test: 1.480454
MAE train: 0.589140	val: 1.159633	test: 1.153914

Epoch: 47
Loss: 0.4465692341327667
RMSE train: 0.684128	val: 1.268456	test: 1.366687
MAE train: 0.530623	val: 1.054237	test: 1.059762

Epoch: 48
Loss: 0.4591049626469612
RMSE train: 0.675926	val: 1.185874	test: 1.315864
MAE train: 0.515680	val: 0.989006	test: 1.033015

Epoch: 49
Loss: 0.44173578172922134
RMSE train: 0.682474	val: 1.270136	test: 1.380374
MAE train: 0.532635	val: 1.045275	test: 1.098692

Epoch: 50
Loss: 0.451157383620739
RMSE train: 0.548634	val: 1.138636	test: 1.255787
MAE train: 0.417892	val: 0.955603	test: 0.989073

Epoch: 51
Loss: 0.4038214161992073
RMSE train: 0.550876	val: 1.100012	test: 1.220136
MAE train: 0.421124	val: 0.916984	test: 0.968475

Epoch: 52
Loss: 0.3892560601234436
RMSE train: 0.710044	val: 1.323327	test: 1.415855
MAE train: 0.559394	val: 1.100779	test: 1.119799

Epoch: 53
Loss: 0.41324926912784576
RMSE train: 0.703933	val: 1.338175	test: 1.430089
MAE train: 0.550891	val: 1.118040	test: 1.117108

Epoch: 54
Loss: 0.37224502861499786
RMSE train: 0.572593	val: 1.147552	test: 1.252501
MAE train: 0.444295	val: 0.949534	test: 0.992544

Epoch: 55
Loss: 0.3926510587334633
RMSE train: 0.617423	val: 1.226746	test: 1.307088
MAE train: 0.488559	val: 1.015548	test: 1.035903

Epoch: 56
Loss: 0.3936278000473976
RMSE train: 0.602854	val: 1.249866	test: 1.317556
MAE train: 0.474747	val: 1.037591	test: 1.030310

Epoch: 57
Loss: 0.38190044462680817
RMSE train: 0.549897	val: 1.170618	test: 1.257407
MAE train: 0.421478	val: 0.968819	test: 0.986646

Epoch: 58
Loss: 0.36185336858034134
RMSE train: 0.492957	val: 1.180521	test: 1.267866
MAE train: 0.385640	val: 0.958135	test: 1.001230

Epoch: 59
Loss: 0.3921659141778946
RMSE train: 0.512614	val: 1.196984	test: 1.281464
MAE train: 0.403628	val: 0.968049	test: 1.008518

Epoch: 60
Loss: 0.3451251685619354
RMSE train: 0.525391	val: 1.175321	test: 1.251333
MAE train: 0.412592	val: 0.966039	test: 0.978621

Epoch: 61
Loss: 0.37685002386569977
RMSE train: 0.542539	val: 1.183386	test: 1.250288
MAE train: 0.421627	val: 0.983410	test: 0.972504

Epoch: 62
Loss: 0.3692672550678253
RMSE train: 0.677695	val: 1.349958	test: 1.404954
MAE train: 0.532644	val: 1.092423	test: 1.117986

Epoch: 63
Loss: 0.38466352969408035
RMSE train: 0.557045	val: 1.174880	test: 1.279043
MAE train: 0.431238	val: 0.973782	test: 1.003995

Epoch: 64
Loss: 0.3597187176346779
RMSE train: 0.477452	val: 1.099851	test: 1.212383
MAE train: 0.360724	val: 0.919234	test: 0.961387

Epoch: 65
Loss: 0.3228169083595276
RMSE train: 0.523583	val: 1.215100	test: 1.286219
MAE train: 0.408344	val: 1.007940	test: 1.021196

Epoch: 66
Loss: 0.36328113079071045
RMSE train: 0.445548	val: 1.146431	test: 1.238609
MAE train: 0.343053	val: 0.945449	test: 0.987515

Epoch: 67
Loss: 0.3436281383037567
RMSE train: 0.508154	val: 1.168775	test: 1.276059
MAE train: 0.397052	val: 0.957174	test: 1.015446

Epoch: 68
Loss: 0.32427648454904556
RMSE train: 0.616355	val: 1.314960	test: 1.416938
MAE train: 0.487676	val: 1.077203	test: 1.110568

Epoch: 69
Loss: 0.3260238617658615
RMSE train: 0.501735	val: 1.136881	test: 1.270873
MAE train: 0.396265	val: 0.931691	test: 1.002942

Epoch: 70
Loss: 0.3143857792019844
RMSE train: 0.499566	val: 1.131313	test: 1.294159
MAE train: 0.391921	val: 0.936553	test: 1.017189

Epoch: 71
Loss: 0.3256058767437935
RMSE train: 0.534954	val: 1.211046	test: 1.349448
MAE train: 0.416868	val: 0.998830	test: 1.052708

Epoch: 72
Loss: 0.3166768252849579
RMSE train: 0.497811	val: 1.144524	test: 1.283785
MAE train: 0.380201	val: 0.958264	test: 1.004887

Epoch: 73
Loss: 0.31449057906866074
RMSE train: 0.502188	val: 1.173320	test: 1.307231
MAE train: 0.384664	val: 0.955284	test: 1.020388

Epoch: 74
Loss: 0.3845405727624893
RMSE train: 0.539449	val: 1.211253	test: 1.339293
MAE train: 0.429154	val: 1.005037	test: 1.043279

Epoch: 75
Loss: 0.2917889580130577
RMSE train: 0.688613	val: 1.400471	test: 1.473994
MAE train: 0.544193	val: 1.169547	test: 1.165185

Epoch: 76
Loss: 0.3410992994904518
RMSE train: 0.611655	val: 1.255076	test: 1.372496
MAE train: 0.481928	val: 1.038395	test: 1.085145

Epoch: 77
Loss: 0.3112494871020317
RMSE train: 0.544531	val: 1.206046	test: 1.346568
MAE train: 0.421315	val: 0.999054	test: 1.060103

Epoch: 78
Loss: 0.2777753174304962
RMSE train: 0.487561	val: 1.219742	test: 1.323743
MAE train: 0.376360	val: 0.988953	test: 1.054551

Epoch: 79
Loss: 0.2880393862724304
RMSE train: 0.406446	val: 1.164107	test: 1.278620
MAE train: 0.316956	val: 0.969839	test: 0.994248

Epoch: 80
Loss: 0.3247987926006317
RMSE train: 0.523051	val: 1.225458	test: 1.333040
MAE train: 0.405473	val: 1.014512	test: 1.039363

Epoch: 81
Loss: 0.3564366400241852
RMSE train: 0.520797	val: 1.189601	test: 1.295386
MAE train: 0.398293	val: 1.004504	test: 1.007792

Epoch: 82
Loss: 0.28408070281147957
RMSE train: 0.538680	val: 1.278472	test: 1.381218
MAE train: 0.410336	val: 1.064066	test: 1.096559

Epoch: 83
Loss: 0.30326882749795914
MAE train: 0.728767	val: 1.127526	test: 1.153948

Epoch: 23
Loss: 1.032448649406433
RMSE train: 0.928940	val: 1.395126	test: 1.560864
MAE train: 0.719895	val: 1.163237	test: 1.197825

Epoch: 24
Loss: 0.9700683355331421
RMSE train: 0.813107	val: 1.208514	test: 1.409049
MAE train: 0.623850	val: 0.994738	test: 1.084080

Epoch: 25
Loss: 0.8745569288730621
RMSE train: 0.792661	val: 1.165649	test: 1.406066
MAE train: 0.607338	val: 0.954403	test: 1.104434

Epoch: 26
Loss: 0.8991710245609283
RMSE train: 0.829060	val: 1.222316	test: 1.423747
MAE train: 0.648298	val: 1.000582	test: 1.109316

Epoch: 27
Loss: 0.8858489394187927
RMSE train: 0.872607	val: 1.287062	test: 1.465017
MAE train: 0.685407	val: 1.035687	test: 1.147787

Epoch: 28
Loss: 0.8047921359539032
RMSE train: 0.884444	val: 1.381874	test: 1.559549
MAE train: 0.697995	val: 1.136357	test: 1.221449

Epoch: 29
Loss: 0.7683975696563721
RMSE train: 0.932473	val: 1.508527	test: 1.637966
MAE train: 0.739413	val: 1.206256	test: 1.278219

Epoch: 30
Loss: 0.7552623301744461
RMSE train: 0.882926	val: 1.288238	test: 1.475493
MAE train: 0.697740	val: 1.026671	test: 1.134554

Epoch: 31
Loss: 0.7809213846921921
RMSE train: 0.888251	val: 1.356278	test: 1.525796
MAE train: 0.705410	val: 1.082637	test: 1.182956

Epoch: 32
Loss: 0.8086356818675995
RMSE train: 0.832227	val: 1.441264	test: 1.587339
MAE train: 0.651040	val: 1.148649	test: 1.236790

Epoch: 33
Loss: 0.6479207128286362
RMSE train: 0.822078	val: 1.555987	test: 1.698512
MAE train: 0.643655	val: 1.248471	test: 1.306343

Epoch: 34
Loss: 0.727437362074852
RMSE train: 0.782949	val: 1.458653	test: 1.610153
MAE train: 0.610744	val: 1.195086	test: 1.237982

Epoch: 35
Loss: 0.6850123107433319
RMSE train: 0.671328	val: 1.264562	test: 1.410588
MAE train: 0.522616	val: 1.024772	test: 1.095218

Epoch: 36
Loss: 0.6612479537725449
RMSE train: 0.686885	val: 1.282683	test: 1.398311
MAE train: 0.543252	val: 1.053841	test: 1.086682

Epoch: 37
Loss: 0.6476048529148102
RMSE train: 0.712996	val: 1.277335	test: 1.401754
MAE train: 0.567539	val: 1.055854	test: 1.103415

Epoch: 38
Loss: 0.6047375053167343
RMSE train: 0.750820	val: 1.324714	test: 1.463904
MAE train: 0.596575	val: 1.100978	test: 1.155745

Epoch: 39
Loss: 0.6099236309528351
RMSE train: 0.731915	val: 1.288219	test: 1.436439
MAE train: 0.568945	val: 1.078117	test: 1.116726

Epoch: 40
Loss: 0.5868678390979767
RMSE train: 0.643848	val: 1.231731	test: 1.405869
MAE train: 0.504499	val: 1.025549	test: 1.095764

Epoch: 41
Loss: 0.5963195860385895
RMSE train: 0.738753	val: 1.470082	test: 1.590406
MAE train: 0.582952	val: 1.206455	test: 1.230230

Epoch: 42
Loss: 0.5591742694377899
RMSE train: 0.652174	val: 1.332904	test: 1.465992
MAE train: 0.511408	val: 1.104541	test: 1.140769

Epoch: 43
Loss: 0.5689736753702164
RMSE train: 0.625239	val: 1.253541	test: 1.390390
MAE train: 0.484041	val: 1.035941	test: 1.095930

Epoch: 44
Loss: 0.6413113176822662
RMSE train: 0.597229	val: 1.289984	test: 1.393412
MAE train: 0.469123	val: 1.066639	test: 1.130204

Epoch: 45
Loss: 0.5458330512046814
RMSE train: 0.547403	val: 1.195757	test: 1.336911
MAE train: 0.426867	val: 0.984674	test: 1.073852

Epoch: 46
Loss: 0.5099107399582863
RMSE train: 0.574977	val: 1.235319	test: 1.390570
MAE train: 0.450336	val: 1.012938	test: 1.091725

Epoch: 47
Loss: 0.4990035966038704
RMSE train: 0.691500	val: 1.370044	test: 1.510710
MAE train: 0.544586	val: 1.118946	test: 1.165037

Epoch: 48
Loss: 0.5034653842449188
RMSE train: 0.709246	val: 1.305986	test: 1.469563
MAE train: 0.558909	val: 1.079953	test: 1.155179

Epoch: 49
Loss: 0.5136578306555748
RMSE train: 0.767212	val: 1.399887	test: 1.528546
MAE train: 0.604945	val: 1.172283	test: 1.211379

Epoch: 50
Loss: 0.5285437703132629
RMSE train: 0.627025	val: 1.249603	test: 1.384901
MAE train: 0.490366	val: 1.032025	test: 1.076204

Epoch: 51
Loss: 0.44174864143133163
RMSE train: 0.597775	val: 1.253560	test: 1.376324
MAE train: 0.468420	val: 1.033217	test: 1.062516

Epoch: 52
Loss: 0.4530903398990631
RMSE train: 0.681189	val: 1.421675	test: 1.542108
MAE train: 0.535894	val: 1.190300	test: 1.213625

Epoch: 53
Loss: 0.4366181194782257
RMSE train: 0.696287	val: 1.448213	test: 1.555161
MAE train: 0.548017	val: 1.191675	test: 1.211464

Epoch: 54
Loss: 0.44257570058107376
RMSE train: 0.537344	val: 1.268369	test: 1.377324
MAE train: 0.421594	val: 1.040603	test: 1.076814

Epoch: 55
Loss: 0.45747410506010056
RMSE train: 0.552875	val: 1.306239	test: 1.400690
MAE train: 0.436663	val: 1.069131	test: 1.092689

Epoch: 56
Loss: 0.4721822887659073
RMSE train: 0.576530	val: 1.355502	test: 1.450062
MAE train: 0.449581	val: 1.112493	test: 1.123970

Epoch: 57
Loss: 0.4281505346298218
RMSE train: 0.549885	val: 1.278299	test: 1.377595
MAE train: 0.412097	val: 1.046995	test: 1.073397

Epoch: 58
Loss: 0.39331022650003433
RMSE train: 0.511626	val: 1.342269	test: 1.416347
MAE train: 0.403174	val: 1.102680	test: 1.104671

Epoch: 59
Loss: 0.4180411323904991
RMSE train: 0.576770	val: 1.434082	test: 1.503126
MAE train: 0.459988	val: 1.176808	test: 1.159079

Epoch: 60
Loss: 0.410271055996418
RMSE train: 0.569983	val: 1.366736	test: 1.448812
MAE train: 0.454070	val: 1.121359	test: 1.119097

Epoch: 61
Loss: 0.42645514756441116
RMSE train: 0.613851	val: 1.374539	test: 1.462380
MAE train: 0.486869	val: 1.124400	test: 1.121429

Epoch: 62
Loss: 0.4178234711289406
RMSE train: 0.723944	val: 1.621161	test: 1.675617
MAE train: 0.581324	val: 1.311654	test: 1.310676

Epoch: 63
Loss: 0.4433318227529526
RMSE train: 0.546219	val: 1.385812	test: 1.478433
MAE train: 0.428172	val: 1.131617	test: 1.138943

Epoch: 64
Loss: 0.38863348960876465
RMSE train: 0.448178	val: 1.237496	test: 1.359895
MAE train: 0.343846	val: 1.016522	test: 1.097782

Epoch: 65
Loss: 0.3305351659655571
RMSE train: 0.532098	val: 1.373496	test: 1.454669
MAE train: 0.418169	val: 1.143541	test: 1.164838

Epoch: 66
Loss: 0.4243174344301224
RMSE train: 0.490524	val: 1.228486	test: 1.342255
MAE train: 0.386996	val: 1.020241	test: 1.054161

Epoch: 67
Loss: 0.4069272205233574
RMSE train: 0.517166	val: 1.264956	test: 1.386390
MAE train: 0.403431	val: 1.044937	test: 1.077163

Epoch: 68
Loss: 0.34978433698415756
RMSE train: 0.604449	val: 1.444792	test: 1.551291
MAE train: 0.476639	val: 1.197009	test: 1.186209

Epoch: 69
Loss: 0.33974798768758774
RMSE train: 0.515414	val: 1.335124	test: 1.466567
MAE train: 0.406176	val: 1.109983	test: 1.130225

Epoch: 70
Loss: 0.33743778616189957
RMSE train: 0.451475	val: 1.407285	test: 1.524024
MAE train: 0.353755	val: 1.128082	test: 1.162490

Epoch: 71
Loss: 0.3804468959569931
RMSE train: 0.472778	val: 1.352220	test: 1.489531
MAE train: 0.371201	val: 1.099025	test: 1.140722

Epoch: 72
Loss: 0.33537548780441284
RMSE train: 0.522799	val: 1.362718	test: 1.493237
MAE train: 0.407252	val: 1.125738	test: 1.173702

Epoch: 73
Loss: 0.3376910090446472
RMSE train: 0.435837	val: 1.346550	test: 1.457296
MAE train: 0.331893	val: 1.085457	test: 1.118052

Epoch: 74
Loss: 0.38877538591623306
RMSE train: 0.410595	val: 1.337468	test: 1.452081
MAE train: 0.317886	val: 1.078767	test: 1.106642

Epoch: 75
Loss: 0.3429705426096916
RMSE train: 0.502657	val: 1.344632	test: 1.461233
MAE train: 0.396882	val: 1.131591	test: 1.129575

Epoch: 76
Loss: 0.3247968927025795
RMSE train: 0.464452	val: 1.299894	test: 1.428891
MAE train: 0.367180	val: 1.094934	test: 1.104540

Epoch: 77
Loss: 0.3378181904554367
RMSE train: 0.514998	val: 1.307728	test: 1.426103
MAE train: 0.406199	val: 1.092394	test: 1.118788

Epoch: 78
Loss: 0.324675090610981
RMSE train: 0.539605	val: 1.333622	test: 1.435878
MAE train: 0.427191	val: 1.102340	test: 1.137272

Epoch: 79
Loss: 0.2987965866923332
RMSE train: 0.503923	val: 1.320371	test: 1.422513
MAE train: 0.410811	val: 1.107417	test: 1.104825

Epoch: 80
Loss: 0.36223430931568146
RMSE train: 0.689091	val: 1.465389	test: 1.567662
MAE train: 0.548358	val: 1.218001	test: 1.210642

Epoch: 81
Loss: 0.34284447133541107
RMSE train: 0.656388	val: 1.477485	test: 1.592866
MAE train: 0.512363	val: 1.225924	test: 1.222162

Epoch: 82
Loss: 0.3341701626777649
RMSE train: 0.497028	val: 1.434428	test: 1.534786
MAE train: 0.392607	val: 1.196470	test: 1.190248

Epoch: 83
Loss: 0.3107192739844322
MAE train: 0.663286	val: 1.141547	test: 1.125654

Epoch: 23
Loss: 0.8160383403301239
RMSE train: 0.843862	val: 1.366060	test: 1.426053
MAE train: 0.641258	val: 1.144775	test: 1.119715

Epoch: 24
Loss: 0.7956917136907578
RMSE train: 0.759179	val: 1.336460	test: 1.393681
MAE train: 0.578942	val: 1.098133	test: 1.106146

Epoch: 25
Loss: 0.8079310208559036
RMSE train: 0.730937	val: 1.291183	test: 1.389701
MAE train: 0.550281	val: 1.085342	test: 1.104858

Epoch: 26
Loss: 0.7392813563346863
RMSE train: 0.782784	val: 1.331481	test: 1.410641
MAE train: 0.595555	val: 1.115362	test: 1.118771

Epoch: 27
Loss: 0.697538748383522
RMSE train: 0.660360	val: 1.175068	test: 1.279802
MAE train: 0.498622	val: 0.968324	test: 1.018492

Epoch: 28
Loss: 0.681242972612381
RMSE train: 0.669485	val: 1.162320	test: 1.269230
MAE train: 0.501593	val: 0.964819	test: 1.007784

Epoch: 29
Loss: 0.6435043662786484
RMSE train: 0.740958	val: 1.289551	test: 1.368657
MAE train: 0.564877	val: 1.054032	test: 1.051779

Epoch: 30
Loss: 0.6149312555789948
RMSE train: 0.633085	val: 1.147435	test: 1.264107
MAE train: 0.477914	val: 0.946404	test: 1.001095

Epoch: 31
Loss: 0.6032462567090988
RMSE train: 0.676017	val: 1.301465	test: 1.400120
MAE train: 0.519339	val: 1.078297	test: 1.086669

Epoch: 32
Loss: 0.6549010723829269
RMSE train: 0.734974	val: 1.374925	test: 1.489426
MAE train: 0.562557	val: 1.152177	test: 1.150689

Epoch: 33
Loss: 0.6210324764251709
RMSE train: 0.737568	val: 1.323349	test: 1.426850
MAE train: 0.570222	val: 1.109535	test: 1.098950

Epoch: 34
Loss: 0.5709780976176262
RMSE train: 0.714025	val: 1.380167	test: 1.448472
MAE train: 0.557673	val: 1.139390	test: 1.134286

Epoch: 35
Loss: 0.5656893625855446
RMSE train: 0.659488	val: 1.344340	test: 1.438010
MAE train: 0.505865	val: 1.107737	test: 1.107954

Epoch: 36
Loss: 0.5662963539361954
RMSE train: 0.658190	val: 1.269628	test: 1.385987
MAE train: 0.506534	val: 1.056113	test: 1.064575

Epoch: 37
Loss: 0.5681073218584061
RMSE train: 0.747270	val: 1.324016	test: 1.418788
MAE train: 0.580349	val: 1.097581	test: 1.102883

Epoch: 38
Loss: 0.5225568413734436
RMSE train: 0.577299	val: 1.188475	test: 1.303155
MAE train: 0.438923	val: 0.985954	test: 1.017708

Epoch: 39
Loss: 0.5296960100531578
RMSE train: 0.593910	val: 1.241239	test: 1.332848
MAE train: 0.456798	val: 1.016441	test: 1.014640

Epoch: 40
Loss: 0.5942665040493011
RMSE train: 0.696138	val: 1.279193	test: 1.379858
MAE train: 0.538649	val: 1.071887	test: 1.056165

Epoch: 41
Loss: 0.5089523494243622
RMSE train: 0.674739	val: 1.272719	test: 1.389729
MAE train: 0.510319	val: 1.067622	test: 1.058553

Epoch: 42
Loss: 0.5769829601049423
RMSE train: 0.632003	val: 1.307366	test: 1.417388
MAE train: 0.486934	val: 1.067678	test: 1.090246

Epoch: 43
Loss: 0.5206829607486725
RMSE train: 0.565601	val: 1.202361	test: 1.328718
MAE train: 0.434726	val: 0.997093	test: 1.036833

Epoch: 44
Loss: 0.524618960916996
RMSE train: 0.532095	val: 1.184660	test: 1.302228
MAE train: 0.407092	val: 0.979566	test: 1.013341

Epoch: 45
Loss: 0.4450961649417877
RMSE train: 0.513192	val: 1.108752	test: 1.242223
MAE train: 0.379737	val: 0.926643	test: 0.990119

Epoch: 46
Loss: 0.465689554810524
RMSE train: 0.632289	val: 1.293037	test: 1.331110
MAE train: 0.492489	val: 1.045174	test: 1.069232

Epoch: 47
Loss: 0.4858138635754585
RMSE train: 0.635652	val: 1.248358	test: 1.328710
MAE train: 0.487509	val: 1.035922	test: 1.046877

Epoch: 48
Loss: 0.45707837492227554
RMSE train: 0.519440	val: 1.167992	test: 1.271035
MAE train: 0.400019	val: 0.957813	test: 1.004071

Epoch: 49
Loss: 0.45243339240550995
RMSE train: 0.557166	val: 1.258278	test: 1.322218
MAE train: 0.435050	val: 1.005586	test: 1.035228

Epoch: 50
Loss: 0.39956188201904297
RMSE train: 0.629324	val: 1.217154	test: 1.300699
MAE train: 0.491247	val: 1.009353	test: 1.020080

Epoch: 51
Loss: 0.4020872265100479
RMSE train: 0.749580	val: 1.320487	test: 1.387840
MAE train: 0.591243	val: 1.106012	test: 1.076143

Epoch: 52
Loss: 0.4271067604422569
RMSE train: 0.677162	val: 1.326539	test: 1.394865
MAE train: 0.532943	val: 1.084445	test: 1.068935

Epoch: 53
Loss: 0.39965013414621353
RMSE train: 0.562567	val: 1.222228	test: 1.313859
MAE train: 0.435843	val: 0.997661	test: 1.013370

Epoch: 54
Loss: 0.43264780193567276
RMSE train: 0.579649	val: 1.181781	test: 1.300114
MAE train: 0.452739	val: 0.977978	test: 1.010393

Epoch: 55
Loss: 0.5428209975361824
RMSE train: 0.537562	val: 1.150570	test: 1.261798
MAE train: 0.420777	val: 0.915716	test: 1.005464

Epoch: 56
Loss: 0.4085783585906029
RMSE train: 0.611591	val: 1.251602	test: 1.322548
MAE train: 0.483197	val: 1.021973	test: 1.032100

Epoch: 57
Loss: 0.4774913862347603
RMSE train: 0.554384	val: 1.195893	test: 1.269224
MAE train: 0.434280	val: 0.975951	test: 0.997179

Epoch: 58
Loss: 0.42358727753162384
RMSE train: 0.461559	val: 1.133628	test: 1.200435
MAE train: 0.363636	val: 0.904580	test: 0.959568

Epoch: 59
Loss: 0.4030655771493912
RMSE train: 0.611432	val: 1.329368	test: 1.360004
MAE train: 0.472691	val: 1.046815	test: 1.060990

Epoch: 60
Loss: 0.39040539413690567
RMSE train: 0.560015	val: 1.233485	test: 1.308173
MAE train: 0.412987	val: 1.022674	test: 1.009323

Epoch: 61
Loss: 0.3698873966932297
RMSE train: 0.497734	val: 1.171750	test: 1.244371
MAE train: 0.380530	val: 0.945468	test: 0.979656

Epoch: 62
Loss: 0.4358496367931366
RMSE train: 0.493351	val: 1.264479	test: 1.300947
MAE train: 0.383228	val: 0.988630	test: 1.017537

Epoch: 63
Loss: 0.34983638674020767
RMSE train: 0.521911	val: 1.167320	test: 1.260758
MAE train: 0.396363	val: 0.958145	test: 0.976247

Epoch: 64
Loss: 0.3647589012980461
RMSE train: 0.433189	val: 1.184002	test: 1.303378
MAE train: 0.334083	val: 0.932527	test: 1.011527

Epoch: 65
Loss: 0.3559722751379013
RMSE train: 0.364026	val: 1.126757	test: 1.278380
MAE train: 0.282322	val: 0.876165	test: 1.013141

Epoch: 66
Loss: 0.3319314792752266
RMSE train: 0.566311	val: 1.204284	test: 1.287272
MAE train: 0.433259	val: 0.997851	test: 1.002174

Epoch: 67
Loss: 0.346914142370224
RMSE train: 0.486902	val: 1.285276	test: 1.339662
MAE train: 0.376849	val: 1.006919	test: 1.046624

Epoch: 68
Loss: 0.38792792707681656
RMSE train: 0.403728	val: 1.143557	test: 1.222338
MAE train: 0.317697	val: 0.906028	test: 0.971236

Epoch: 69
Loss: 0.3821662738919258
RMSE train: 0.597192	val: 1.252113	test: 1.301441
MAE train: 0.445686	val: 1.031849	test: 1.007382

Epoch: 70
Loss: 0.35179373621940613
RMSE train: 0.558811	val: 1.228748	test: 1.264556
MAE train: 0.435978	val: 0.964523	test: 0.986813

Epoch: 71
Loss: 0.34702980518341064
RMSE train: 0.375733	val: 1.157718	test: 1.236765
MAE train: 0.293380	val: 0.918513	test: 0.982678

Epoch: 72
Loss: 0.3264026716351509
RMSE train: 0.569595	val: 1.292923	test: 1.338481
MAE train: 0.445056	val: 1.036848	test: 1.026017

Epoch: 73
Loss: 0.33024735003709793
RMSE train: 0.668325	val: 1.373468	test: 1.427300
MAE train: 0.514012	val: 1.141349	test: 1.073897

Epoch: 74
Loss: 0.36416780203580856
RMSE train: 0.388945	val: 1.106024	test: 1.222895
MAE train: 0.292435	val: 0.891946	test: 0.967978

Epoch: 75
Loss: 0.34397466480731964
RMSE train: 0.377572	val: 1.379119	test: 1.390520
MAE train: 0.287124	val: 1.056799	test: 1.111856

Epoch: 76
Loss: 0.3390480503439903
RMSE train: 0.411854	val: 1.173623	test: 1.233639
MAE train: 0.318116	val: 0.933331	test: 0.982455

Epoch: 77
Loss: 0.30127082765102386
RMSE train: 0.467740	val: 1.190781	test: 1.254457
MAE train: 0.362687	val: 0.971716	test: 0.988757

Epoch: 78
Loss: 0.36148152500391006
RMSE train: 0.419957	val: 1.178974	test: 1.252708
MAE train: 0.328163	val: 0.935483	test: 0.990342

Epoch: 79
Loss: 0.2935601696372032
RMSE train: 0.519894	val: 1.232497	test: 1.285072
MAE train: 0.408685	val: 0.965408	test: 1.005795

Epoch: 80
Loss: 0.31900011375546455
RMSE train: 0.494943	val: 1.208844	test: 1.286694
MAE train: 0.391227	val: 0.953779	test: 1.010137

Epoch: 81
Loss: 0.32727867364883423
RMSE train: 0.410952	val: 1.126174	test: 1.247724
MAE train: 0.325320	val: 0.893449	test: 0.995107

Epoch: 82
Loss: 0.3153406009078026
RMSE train: 0.538787	val: 1.191354	test: 1.318152
MAE train: 0.415813	val: 0.989603	test: 1.013002

Epoch: 83
Loss: 0.3243282660841942
MAE train: 0.649490	val: 1.101038	test: 1.154314

Epoch: 23
Loss: 0.8272744566202164
RMSE train: 0.888129	val: 1.350265	test: 1.438266
MAE train: 0.665696	val: 1.050604	test: 1.146563

Epoch: 24
Loss: 0.883940115571022
RMSE train: 0.818864	val: 1.237301	test: 1.309026
MAE train: 0.601825	val: 0.976233	test: 1.061063

Epoch: 25
Loss: 0.7774816155433655
RMSE train: 0.761799	val: 1.334628	test: 1.343712
MAE train: 0.560673	val: 1.068507	test: 1.089491

Epoch: 26
Loss: 0.7336098551750183
RMSE train: 0.762310	val: 1.296636	test: 1.301156
MAE train: 0.560368	val: 1.036360	test: 1.054889

Epoch: 27
Loss: 0.864898219704628
RMSE train: 0.707143	val: 1.209324	test: 1.234907
MAE train: 0.523297	val: 0.985764	test: 1.017750

Epoch: 28
Loss: 0.780491977930069
RMSE train: 0.624985	val: 1.272593	test: 1.322243
MAE train: 0.467419	val: 1.018099	test: 1.090886

Epoch: 29
Loss: 0.6523114293813705
RMSE train: 0.623235	val: 1.285797	test: 1.346247
MAE train: 0.460646	val: 1.046221	test: 1.093723

Epoch: 30
Loss: 0.7150145918130875
RMSE train: 0.672159	val: 1.238856	test: 1.287593
MAE train: 0.499883	val: 1.019565	test: 1.032581

Epoch: 31
Loss: 0.6793500632047653
RMSE train: 0.649864	val: 1.168194	test: 1.213319
MAE train: 0.484558	val: 0.968937	test: 0.970529

Epoch: 32
Loss: 0.6798770874738693
RMSE train: 0.698515	val: 1.197486	test: 1.237391
MAE train: 0.531754	val: 0.966866	test: 0.996270

Epoch: 33
Loss: 0.6964094638824463
RMSE train: 0.755243	val: 1.242059	test: 1.304319
MAE train: 0.574216	val: 1.001110	test: 1.035707

Epoch: 34
Loss: 0.6357174515724182
RMSE train: 0.696030	val: 1.211263	test: 1.267895
MAE train: 0.535952	val: 0.992978	test: 1.009925

Epoch: 35
Loss: 0.6001678109169006
RMSE train: 0.757943	val: 1.325388	test: 1.348758
MAE train: 0.585521	val: 1.058880	test: 1.088402

Epoch: 36
Loss: 0.644785538315773
RMSE train: 0.752655	val: 1.171579	test: 1.256700
MAE train: 0.591475	val: 0.948710	test: 1.005176

Epoch: 37
Loss: 0.5745719820261002
RMSE train: 0.663788	val: 1.111998	test: 1.241534
MAE train: 0.506327	val: 0.920467	test: 1.013205

Epoch: 38
Loss: 0.5791085064411163
RMSE train: 0.688475	val: 1.287699	test: 1.360152
MAE train: 0.526076	val: 1.047501	test: 1.119842

Epoch: 39
Loss: 0.6250928789377213
RMSE train: 0.613882	val: 1.155125	test: 1.212370
MAE train: 0.470001	val: 0.939947	test: 0.990761

Epoch: 40
Loss: 0.5794003903865814
RMSE train: 0.687766	val: 1.293879	test: 1.312604
MAE train: 0.520021	val: 1.031512	test: 1.044417

Epoch: 41
Loss: 0.596008375287056
RMSE train: 0.594969	val: 1.223948	test: 1.262986
MAE train: 0.443738	val: 0.994059	test: 1.017323

Epoch: 42
Loss: 0.556911289691925
RMSE train: 0.543790	val: 1.146327	test: 1.248068
MAE train: 0.410775	val: 0.939599	test: 1.018150

Epoch: 43
Loss: 0.4923674911260605
RMSE train: 0.533078	val: 1.143880	test: 1.254432
MAE train: 0.398590	val: 0.932683	test: 1.037558

Epoch: 44
Loss: 0.49051570147275925
RMSE train: 0.525764	val: 1.067469	test: 1.205965
MAE train: 0.394015	val: 0.883083	test: 0.990502

Epoch: 45
Loss: 0.4950304999947548
RMSE train: 0.529778	val: 1.056472	test: 1.202530
MAE train: 0.398821	val: 0.882678	test: 0.976232

Epoch: 46
Loss: 0.48606326431035995
RMSE train: 0.520838	val: 1.136542	test: 1.247861
MAE train: 0.393546	val: 0.935431	test: 1.006386

Epoch: 47
Loss: 0.4836443066596985
RMSE train: 0.644663	val: 1.196632	test: 1.270006
MAE train: 0.481707	val: 0.955516	test: 1.013010

Epoch: 48
Loss: 0.48700304329395294
RMSE train: 0.525557	val: 1.083019	test: 1.198404
MAE train: 0.395737	val: 0.874622	test: 0.962835

Epoch: 49
Loss: 0.4714973643422127
RMSE train: 0.550408	val: 1.136729	test: 1.242043
MAE train: 0.414892	val: 0.923476	test: 0.995638

Epoch: 50
Loss: 0.49596938490867615
RMSE train: 0.649568	val: 1.220836	test: 1.265624
MAE train: 0.491909	val: 0.979109	test: 1.007021

Epoch: 51
Loss: 0.4613742530345917
RMSE train: 0.599238	val: 1.151464	test: 1.202632
MAE train: 0.450558	val: 0.936348	test: 0.970268

Epoch: 52
Loss: 0.4638064056634903
RMSE train: 0.528459	val: 1.114211	test: 1.189406
MAE train: 0.398481	val: 0.918101	test: 0.960569

Epoch: 53
Loss: 0.5065065622329712
RMSE train: 0.588724	val: 1.203421	test: 1.270946
MAE train: 0.446985	val: 0.969937	test: 1.009945

Epoch: 54
Loss: 0.4327859506011009
RMSE train: 0.572286	val: 1.145561	test: 1.223895
MAE train: 0.439797	val: 0.926829	test: 0.979780

Epoch: 55
Loss: 0.42582544684410095
RMSE train: 0.535826	val: 1.109448	test: 1.190593
MAE train: 0.406195	val: 0.904041	test: 0.956488

Epoch: 56
Loss: 0.4500514194369316
RMSE train: 0.465025	val: 1.096240	test: 1.193561
MAE train: 0.356213	val: 0.891747	test: 0.959830

Epoch: 57
Loss: 0.41273070871829987
RMSE train: 0.504078	val: 1.175197	test: 1.253421
MAE train: 0.383677	val: 0.954252	test: 0.993442

Epoch: 58
Loss: 0.41460342705249786
RMSE train: 0.462901	val: 1.125455	test: 1.218510
MAE train: 0.353159	val: 0.923633	test: 0.977620

Epoch: 59
Loss: 0.40483947843313217
RMSE train: 0.473404	val: 1.164117	test: 1.254500
MAE train: 0.357313	val: 0.960817	test: 1.015803

Epoch: 60
Loss: 0.3929490000009537
RMSE train: 0.471301	val: 1.127491	test: 1.193480
MAE train: 0.356379	val: 0.930177	test: 0.961823

Epoch: 61
Loss: 0.41013315320014954
RMSE train: 0.437188	val: 1.118743	test: 1.194459
MAE train: 0.326064	val: 0.909235	test: 0.952439

Epoch: 62
Loss: 0.3994044214487076
RMSE train: 0.467924	val: 1.192019	test: 1.255624
MAE train: 0.351352	val: 0.959138	test: 1.008090

Epoch: 63
Loss: 0.41156094521284103
RMSE train: 0.512144	val: 1.146513	test: 1.221363
MAE train: 0.383756	val: 0.930458	test: 0.978375

Epoch: 64
Loss: 0.40070902556180954
RMSE train: 0.539824	val: 1.193680	test: 1.266348
MAE train: 0.405223	val: 0.955916	test: 1.009910

Epoch: 65
Loss: 0.4045770689845085
RMSE train: 0.522099	val: 1.176988	test: 1.247457
MAE train: 0.401231	val: 0.949368	test: 0.992312

Epoch: 66
Loss: 0.3902832567691803
RMSE train: 0.637528	val: 1.256487	test: 1.309780
MAE train: 0.489873	val: 1.009567	test: 1.045174

Epoch: 67
Loss: 0.3616267442703247
RMSE train: 0.548787	val: 1.166197	test: 1.242165
MAE train: 0.416764	val: 0.940936	test: 0.992201

Epoch: 68
Loss: 0.3882255032658577
RMSE train: 0.498705	val: 1.172743	test: 1.268266
MAE train: 0.378380	val: 0.953076	test: 1.016404

Epoch: 69
Loss: 0.3327540159225464
RMSE train: 0.533674	val: 1.155631	test: 1.248112
MAE train: 0.396301	val: 0.953964	test: 0.993582

Epoch: 70
Loss: 0.45970238745212555
RMSE train: 0.491180	val: 1.198423	test: 1.292882
MAE train: 0.359806	val: 0.973610	test: 1.029818

Epoch: 71
Loss: 0.439154677093029
RMSE train: 0.425939	val: 1.221066	test: 1.341301
MAE train: 0.322047	val: 0.984248	test: 1.047726

Epoch: 72
Loss: 0.35035691410303116
RMSE train: 0.450280	val: 1.090813	test: 1.190976
MAE train: 0.325710	val: 0.885204	test: 0.948906

Epoch: 73
Loss: 0.3718130514025688
RMSE train: 0.494209	val: 1.171267	test: 1.256628
MAE train: 0.368937	val: 0.948462	test: 0.990813

Epoch: 74
Loss: 0.3679705411195755
RMSE train: 0.423811	val: 1.101942	test: 1.179095
MAE train: 0.321001	val: 0.897688	test: 0.945835

Epoch: 75
Loss: 0.31100548803806305
RMSE train: 0.413263	val: 1.177276	test: 1.234913
MAE train: 0.316719	val: 0.956963	test: 0.987333

Epoch: 76
Loss: 0.3214287832379341
RMSE train: 0.455839	val: 1.222237	test: 1.266953
MAE train: 0.344417	val: 0.988768	test: 1.016035

Epoch: 77
Loss: 0.37644343823194504
RMSE train: 0.390075	val: 1.089161	test: 1.175707
MAE train: 0.292297	val: 0.897574	test: 0.952558

Epoch: 78
Loss: 0.34691233932971954
RMSE train: 0.404307	val: 1.081938	test: 1.187771
MAE train: 0.303536	val: 0.879762	test: 0.959544

Epoch: 79
Loss: 0.3254845440387726
RMSE train: 0.396565	val: 1.090045	test: 1.215737
MAE train: 0.301984	val: 0.882622	test: 0.976288

Epoch: 80
Loss: 0.28415582329034805
RMSE train: 0.415133	val: 1.110017	test: 1.243896
MAE train: 0.312312	val: 0.904569	test: 0.987689

Epoch: 81
Loss: 0.31400639563798904
RMSE train: 0.421831	val: 1.132309	test: 1.256730
MAE train: 0.320550	val: 0.928559	test: 0.993046

Epoch: 82
Loss: 0.30509258806705475
RMSE train: 0.444605	val: 1.109351	test: 1.211414
MAE train: 0.336987	val: 0.919281	test: 0.962482

Epoch: 83
Loss: 0.2890271060168743
MAE train: 0.480245	val: 0.934000	test: 0.995834

Epoch: 23
Loss: 0.7431258112192154
RMSE train: 0.732436	val: 1.156769	test: 1.296612
MAE train: 0.558676	val: 0.978235	test: 0.994946

Epoch: 24
Loss: 0.7305750399827957
RMSE train: 0.857204	val: 1.359070	test: 1.427704
MAE train: 0.658921	val: 1.147137	test: 1.133400

Epoch: 25
Loss: 0.6793678402900696
RMSE train: 0.744013	val: 1.163869	test: 1.276915
MAE train: 0.575920	val: 0.977249	test: 0.995556

Epoch: 26
Loss: 0.660865031182766
RMSE train: 0.736372	val: 1.179491	test: 1.285874
MAE train: 0.574214	val: 0.969610	test: 1.011289

Epoch: 27
Loss: 0.6623687595129013
RMSE train: 0.775549	val: 1.231461	test: 1.328089
MAE train: 0.588432	val: 1.008676	test: 1.046666

Epoch: 28
Loss: 0.6342620700597763
RMSE train: 0.633930	val: 1.116921	test: 1.224751
MAE train: 0.479714	val: 0.931579	test: 0.967554

Epoch: 29
Loss: 0.5613465309143066
RMSE train: 0.673555	val: 1.229758	test: 1.283490
MAE train: 0.504837	val: 0.990118	test: 1.027834

Epoch: 30
Loss: 0.5828983783721924
RMSE train: 0.734321	val: 1.187594	test: 1.283477
MAE train: 0.548431	val: 0.993971	test: 1.003814

Epoch: 31
Loss: 0.5733141899108887
RMSE train: 0.617162	val: 1.040560	test: 1.177501
MAE train: 0.460735	val: 0.866207	test: 0.937095

Epoch: 32
Loss: 0.6339371353387833
RMSE train: 0.583197	val: 1.121588	test: 1.269729
MAE train: 0.442434	val: 0.923117	test: 1.042452

Epoch: 33
Loss: 0.55280402302742
RMSE train: 0.694494	val: 1.231420	test: 1.338146
MAE train: 0.522938	val: 1.018940	test: 1.049335

Epoch: 34
Loss: 0.5900217890739441
RMSE train: 0.730490	val: 1.199825	test: 1.330586
MAE train: 0.550081	val: 0.989279	test: 1.040464

Epoch: 35
Loss: 0.49642302840948105
RMSE train: 0.607195	val: 1.077297	test: 1.246363
MAE train: 0.470419	val: 0.891729	test: 0.993791

Epoch: 36
Loss: 0.5213096141815186
RMSE train: 0.639348	val: 1.185303	test: 1.320548
MAE train: 0.492540	val: 0.966432	test: 1.060680

Epoch: 37
Loss: 0.4758892357349396
RMSE train: 0.627572	val: 1.116785	test: 1.247032
MAE train: 0.483457	val: 0.925817	test: 0.988886

Epoch: 38
Loss: 0.4858338385820389
RMSE train: 0.684051	val: 1.179179	test: 1.307080
MAE train: 0.529465	val: 0.976316	test: 1.017328

Epoch: 39
Loss: 0.4698765203356743
RMSE train: 0.754607	val: 1.242474	test: 1.367684
MAE train: 0.578890	val: 1.025319	test: 1.065228

Epoch: 40
Loss: 0.46257608383893967
RMSE train: 0.586312	val: 1.068610	test: 1.217657
MAE train: 0.444193	val: 0.892328	test: 0.947042

Epoch: 41
Loss: 0.44946257770061493
RMSE train: 0.622549	val: 1.164272	test: 1.275877
MAE train: 0.469142	val: 0.962595	test: 1.012543

Epoch: 42
Loss: 0.45313459634780884
RMSE train: 0.585695	val: 1.094350	test: 1.237337
MAE train: 0.444246	val: 0.915273	test: 0.972473

Epoch: 43
Loss: 0.43263423442840576
RMSE train: 0.595443	val: 1.091063	test: 1.224117
MAE train: 0.435752	val: 0.920592	test: 0.953321

Epoch: 44
Loss: 0.5440507382154465
RMSE train: 0.586690	val: 1.186933	test: 1.339990
MAE train: 0.438633	val: 0.974669	test: 1.087691

Epoch: 45
Loss: 0.40640317648649216
RMSE train: 0.528894	val: 0.987420	test: 1.217277
MAE train: 0.403936	val: 0.834555	test: 0.943760

Epoch: 46
Loss: 0.4382186606526375
RMSE train: 0.608658	val: 1.082283	test: 1.263193
MAE train: 0.465970	val: 0.897161	test: 1.007837

Epoch: 47
Loss: 0.3947002440690994
RMSE train: 0.614973	val: 1.101696	test: 1.278871
MAE train: 0.470776	val: 0.907292	test: 1.028301

Epoch: 48
Loss: 0.40109042078256607
RMSE train: 0.562368	val: 0.991564	test: 1.187852
MAE train: 0.424873	val: 0.832831	test: 0.942449

Epoch: 49
Loss: 0.4101756364107132
RMSE train: 0.576904	val: 1.054417	test: 1.229203
MAE train: 0.441943	val: 0.882148	test: 0.982429

Epoch: 50
Loss: 0.4192093014717102
RMSE train: 0.522370	val: 0.978934	test: 1.165724
MAE train: 0.387158	val: 0.817365	test: 0.921152

Epoch: 51
Loss: 0.3778371214866638
RMSE train: 0.475173	val: 0.966085	test: 1.166394
MAE train: 0.348754	val: 0.794010	test: 0.932067

Epoch: 52
Loss: 0.3554365336894989
RMSE train: 0.536492	val: 1.031430	test: 1.213123
MAE train: 0.400271	val: 0.853283	test: 0.962627

Epoch: 53
Loss: 0.3724343851208687
RMSE train: 0.594839	val: 1.080195	test: 1.243275
MAE train: 0.445336	val: 0.907337	test: 0.980358

Epoch: 54
Loss: 0.366121806204319
RMSE train: 0.463092	val: 1.006299	test: 1.206043
MAE train: 0.347712	val: 0.837193	test: 0.967288

Epoch: 55
Loss: 0.37281110137701035
RMSE train: 0.448091	val: 1.074667	test: 1.281746
MAE train: 0.338649	val: 0.888922	test: 1.023933

Epoch: 56
Loss: 0.38431021571159363
RMSE train: 0.516964	val: 1.091031	test: 1.253194
MAE train: 0.390971	val: 0.912130	test: 1.001707

Epoch: 57
Loss: 0.35032444447278976
RMSE train: 0.504684	val: 1.033957	test: 1.192133
MAE train: 0.368379	val: 0.872477	test: 0.944837

Epoch: 58
Loss: 0.3534111827611923
RMSE train: 0.436045	val: 1.066470	test: 1.230341
MAE train: 0.328068	val: 0.879144	test: 0.980467

Epoch: 59
Loss: 0.3597731739282608
RMSE train: 0.480356	val: 1.097937	test: 1.244588
MAE train: 0.365889	val: 0.900098	test: 0.992433

Epoch: 60
Loss: 0.3389809653162956
RMSE train: 0.411496	val: 0.984658	test: 1.173780
MAE train: 0.310409	val: 0.817157	test: 0.912521

Epoch: 61
Loss: 0.364503413438797
RMSE train: 0.412734	val: 0.969031	test: 1.168607
MAE train: 0.309113	val: 0.810401	test: 0.919850

Epoch: 62
Loss: 0.35497234761714935
RMSE train: 0.545753	val: 1.098044	test: 1.284387
MAE train: 0.423691	val: 0.909873	test: 1.022596

Epoch: 63
Loss: 0.36208200454711914
RMSE train: 0.510562	val: 1.027846	test: 1.215304
MAE train: 0.381267	val: 0.857919	test: 0.957045

Epoch: 64
Loss: 0.3272751718759537
RMSE train: 0.456381	val: 1.035933	test: 1.251032
MAE train: 0.331505	val: 0.857627	test: 1.009306

Epoch: 65
Loss: 0.31386350095272064
RMSE train: 0.540115	val: 1.151861	test: 1.320406
MAE train: 0.406297	val: 0.948949	test: 1.073829

Epoch: 66
Loss: 0.32656584680080414
RMSE train: 0.473815	val: 1.038728	test: 1.236227
MAE train: 0.350797	val: 0.872793	test: 0.993214

Epoch: 67
Loss: 0.3113168999552727
RMSE train: 0.438321	val: 0.982568	test: 1.202669
MAE train: 0.329261	val: 0.813376	test: 0.953739

Epoch: 68
Loss: 0.30881523340940475
RMSE train: 0.525534	val: 1.054207	test: 1.235994
MAE train: 0.402645	val: 0.871287	test: 0.979327

Epoch: 69
Loss: 0.32553325593471527
RMSE train: 0.511909	val: 1.032646	test: 1.215593
MAE train: 0.384259	val: 0.844304	test: 0.949434

Epoch: 70
Loss: 0.2883755713701248
RMSE train: 0.462881	val: 0.977291	test: 1.184984
MAE train: 0.351395	val: 0.796581	test: 0.921249

Epoch: 71
Loss: 0.34201111644506454
RMSE train: 0.521129	val: 1.022953	test: 1.200883
MAE train: 0.384496	val: 0.846983	test: 0.937968

Epoch: 72
Loss: 0.3250035047531128
RMSE train: 0.479544	val: 0.981984	test: 1.167394
MAE train: 0.356129	val: 0.817942	test: 0.894088

Epoch: 73
Loss: 0.30849697813391685
RMSE train: 0.420818	val: 1.073970	test: 1.243762
MAE train: 0.311962	val: 0.881859	test: 0.979971

Epoch: 74
Loss: 0.35241393744945526
RMSE train: 0.441496	val: 1.018461	test: 1.211824
MAE train: 0.339497	val: 0.849227	test: 0.960237

Epoch: 75
Loss: 0.282107412815094
RMSE train: 0.561765	val: 1.106373	test: 1.278918
MAE train: 0.421450	val: 0.918604	test: 1.003100

Epoch: 76
Loss: 0.3342290222644806
RMSE train: 0.554488	val: 1.125664	test: 1.322620
MAE train: 0.425888	val: 0.926672	test: 1.056313

Epoch: 77
Loss: 0.29457004368305206
RMSE train: 0.543949	val: 1.092496	test: 1.292747
MAE train: 0.420742	val: 0.910104	test: 1.016743

Epoch: 78
Loss: 0.27164436131715775
RMSE train: 0.595990	val: 1.150972	test: 1.329138
MAE train: 0.442906	val: 0.945545	test: 1.050392

Epoch: 79
Loss: 0.29073067009449005
RMSE train: 0.402730	val: 1.010065	test: 1.248431
MAE train: 0.307732	val: 0.837789	test: 0.983245

Epoch: 80
Loss: 0.31754156947135925
RMSE train: 0.497506	val: 1.144197	test: 1.379007
MAE train: 0.373244	val: 0.954497	test: 1.058947

Epoch: 81
Loss: 0.34166256338357925
RMSE train: 0.463102	val: 1.033281	test: 1.250874
MAE train: 0.339272	val: 0.859767	test: 0.980364

Epoch: 82
Loss: 0.26925643533468246
RMSE train: 0.381392	val: 0.971573	test: 1.158862
MAE train: 0.276812	val: 0.821850	test: 0.898989

Epoch: 83
Loss: 0.2831493020057678
MAE train: 0.657875	val: 0.943927	test: 1.032259

Epoch: 23
Loss: 0.9910823404788971
RMSE train: 0.848539	val: 1.216720	test: 1.324492
MAE train: 0.653595	val: 0.969298	test: 1.050218

Epoch: 24
Loss: 0.9602338671684265
RMSE train: 0.748427	val: 1.278386	test: 1.363413
MAE train: 0.568337	val: 1.018515	test: 1.104502

Epoch: 25
Loss: 0.9349792450666428
RMSE train: 0.769392	val: 1.148348	test: 1.276424
MAE train: 0.580299	val: 0.942884	test: 1.040979

Epoch: 26
Loss: 0.8909087181091309
RMSE train: 0.815728	val: 1.180429	test: 1.292840
MAE train: 0.622830	val: 0.971078	test: 1.043817

Epoch: 27
Loss: 0.876998245716095
RMSE train: 0.805101	val: 1.253574	test: 1.350685
MAE train: 0.624484	val: 1.040970	test: 1.073930

Epoch: 28
Loss: 0.8083110004663467
RMSE train: 0.749966	val: 1.175279	test: 1.283099
MAE train: 0.580048	val: 0.969312	test: 1.028349

Epoch: 29
Loss: 0.7725511789321899
RMSE train: 0.788080	val: 1.198522	test: 1.275335
MAE train: 0.613651	val: 0.974854	test: 1.038901

Epoch: 30
Loss: 0.7789615988731384
RMSE train: 0.734294	val: 1.228048	test: 1.282156
MAE train: 0.570176	val: 0.990689	test: 1.046021

Epoch: 31
Loss: 0.7633903622627258
RMSE train: 0.732734	val: 1.246826	test: 1.311596
MAE train: 0.571883	val: 1.018101	test: 1.051831

Epoch: 32
Loss: 0.7909860461950302
RMSE train: 0.774487	val: 1.186490	test: 1.275864
MAE train: 0.600016	val: 0.982674	test: 1.026308

Epoch: 33
Loss: 0.736918494105339
RMSE train: 0.827477	val: 1.189568	test: 1.285771
MAE train: 0.643027	val: 0.993938	test: 1.030925

Epoch: 34
Loss: 0.7171496152877808
RMSE train: 0.707167	val: 1.147120	test: 1.257377
MAE train: 0.549989	val: 0.945523	test: 1.013946

Epoch: 35
Loss: 0.676202729344368
RMSE train: 0.689269	val: 1.176496	test: 1.287023
MAE train: 0.545074	val: 0.967148	test: 1.042850

Epoch: 36
Loss: 0.6517614871263504
RMSE train: 0.620872	val: 1.140771	test: 1.289867
MAE train: 0.477183	val: 0.939563	test: 1.030040

Epoch: 37
Loss: 0.6777393221855164
RMSE train: 0.652728	val: 1.117273	test: 1.268770
MAE train: 0.505650	val: 0.910438	test: 1.031694

Epoch: 38
Loss: 0.6179166585206985
RMSE train: 0.659487	val: 1.117615	test: 1.259215
MAE train: 0.507508	val: 0.917017	test: 1.033808

Epoch: 39
Loss: 0.6505561172962189
RMSE train: 0.598003	val: 1.207779	test: 1.298724
MAE train: 0.472692	val: 0.971871	test: 1.074566

Epoch: 40
Loss: 0.6916307955980301
RMSE train: 0.658222	val: 1.282411	test: 1.342346
MAE train: 0.524593	val: 1.043617	test: 1.121819

Epoch: 41
Loss: 0.585794523358345
RMSE train: 0.691506	val: 1.134780	test: 1.284813
MAE train: 0.534375	val: 0.931400	test: 1.051332

Epoch: 42
Loss: 0.6569414436817169
RMSE train: 0.680917	val: 1.341710	test: 1.365522
MAE train: 0.530714	val: 1.105583	test: 1.126784

Epoch: 43
Loss: 0.6311526894569397
RMSE train: 0.681930	val: 1.171402	test: 1.260560
MAE train: 0.536290	val: 0.957758	test: 1.032515

Epoch: 44
Loss: 0.6164129823446274
RMSE train: 0.642140	val: 1.116569	test: 1.215110
MAE train: 0.509874	val: 0.915775	test: 1.009572

Epoch: 45
Loss: 0.5550267621874809
RMSE train: 0.558063	val: 1.111737	test: 1.236485
MAE train: 0.437370	val: 0.895445	test: 1.009811

Epoch: 46
Loss: 0.5306629687547684
RMSE train: 0.681595	val: 1.228981	test: 1.290517
MAE train: 0.549419	val: 0.988076	test: 1.057813

Epoch: 47
Loss: 0.5292690843343735
RMSE train: 0.767131	val: 1.203841	test: 1.289375
MAE train: 0.618256	val: 0.976146	test: 1.055118

Epoch: 48
Loss: 0.5014773681759834
RMSE train: 0.624493	val: 1.103043	test: 1.218300
MAE train: 0.497804	val: 0.898992	test: 1.006213

Epoch: 49
Loss: 0.553971566259861
RMSE train: 0.604361	val: 1.216768	test: 1.276626
MAE train: 0.490599	val: 0.986771	test: 1.045208

Epoch: 50
Loss: 0.501324437558651
RMSE train: 0.694396	val: 1.172106	test: 1.274402
MAE train: 0.547521	val: 0.957542	test: 1.035784

Epoch: 51
Loss: 0.5036488473415375
RMSE train: 0.677139	val: 1.134800	test: 1.247610
MAE train: 0.535242	val: 0.925084	test: 1.024004

Epoch: 52
Loss: 0.4726387485861778
RMSE train: 0.648961	val: 1.184456	test: 1.244592
MAE train: 0.525417	val: 0.952938	test: 1.033450

Epoch: 53
Loss: 0.4708678424358368
RMSE train: 0.523907	val: 1.138237	test: 1.205356
MAE train: 0.415583	val: 0.920137	test: 0.996139

Epoch: 54
Loss: 0.49926017969846725
RMSE train: 0.502574	val: 1.138419	test: 1.199960
MAE train: 0.397184	val: 0.922694	test: 0.997352

Epoch: 55
Loss: 0.6084851697087288
RMSE train: 0.491347	val: 1.344838	test: 1.328835
MAE train: 0.385135	val: 1.096636	test: 1.100704

Epoch: 56
Loss: 0.4726260229945183
RMSE train: 0.529677	val: 1.233496	test: 1.266755
MAE train: 0.419427	val: 1.002611	test: 1.019238

Epoch: 57
Loss: 0.5109690353274345
RMSE train: 0.603246	val: 1.167936	test: 1.215566
MAE train: 0.480946	val: 0.943396	test: 0.983611

Epoch: 58
Loss: 0.4765172526240349
RMSE train: 0.502377	val: 1.141040	test: 1.228545
MAE train: 0.370722	val: 0.933825	test: 0.991531

Epoch: 59
Loss: 0.4524970203638077
RMSE train: 0.610008	val: 1.338842	test: 1.290375
MAE train: 0.480518	val: 1.083002	test: 1.071161

Epoch: 60
Loss: 0.42755987495183945
RMSE train: 0.556613	val: 1.241935	test: 1.253718
MAE train: 0.438403	val: 0.993293	test: 1.028121

Epoch: 61
Loss: 0.45750506967306137
RMSE train: 0.509724	val: 1.132902	test: 1.236945
MAE train: 0.380718	val: 0.924632	test: 1.013337

Epoch: 62
Loss: 0.49248527735471725
RMSE train: 0.498725	val: 1.182942	test: 1.253747
MAE train: 0.391884	val: 0.950287	test: 1.061909

Epoch: 63
Loss: 0.39937932044267654
RMSE train: 0.528546	val: 1.163122	test: 1.279503
MAE train: 0.408040	val: 0.951818	test: 1.043878

Epoch: 64
Loss: 0.41230564564466476
RMSE train: 0.416500	val: 1.317340	test: 1.380280
MAE train: 0.332069	val: 1.056569	test: 1.151659

Epoch: 65
Loss: 0.40575893223285675
RMSE train: 0.497417	val: 1.244775	test: 1.309957
MAE train: 0.399872	val: 0.992087	test: 1.123480

Epoch: 66
Loss: 0.37033191323280334
RMSE train: 0.591257	val: 1.105660	test: 1.251106
MAE train: 0.431581	val: 0.916955	test: 1.026999

Epoch: 67
Loss: 0.3987361416220665
RMSE train: 0.514811	val: 1.298385	test: 1.340959
MAE train: 0.412633	val: 1.037465	test: 1.114581

Epoch: 68
Loss: 0.4487524852156639
RMSE train: 0.499579	val: 1.238468	test: 1.304879
MAE train: 0.399243	val: 0.999153	test: 1.075720

Epoch: 69
Loss: 0.3944816216826439
RMSE train: 0.605314	val: 1.238635	test: 1.338616
MAE train: 0.440577	val: 1.045563	test: 1.052526

Epoch: 70
Loss: 0.37885963171720505
RMSE train: 0.554404	val: 1.348993	test: 1.335489
MAE train: 0.443604	val: 1.077648	test: 1.114414

Epoch: 71
Loss: 0.38526400178670883
RMSE train: 0.409248	val: 1.413123	test: 1.397623
MAE train: 0.326569	val: 1.130647	test: 1.160884

Epoch: 72
Loss: 0.35558056086301804
RMSE train: 0.519020	val: 1.216440	test: 1.274079
MAE train: 0.401965	val: 0.990372	test: 1.053730

Epoch: 73
Loss: 0.37837090343236923
RMSE train: 0.639829	val: 1.259895	test: 1.319811
MAE train: 0.490083	val: 1.027696	test: 1.072866

Epoch: 74
Loss: 0.40857766568660736
RMSE train: 0.402639	val: 1.224572	test: 1.267001
MAE train: 0.314122	val: 0.972932	test: 1.047945

Epoch: 75
Loss: 0.376958392560482
RMSE train: 0.403754	val: 1.359162	test: 1.360458
MAE train: 0.320391	val: 1.106348	test: 1.112698

Epoch: 76
Loss: 0.36663618683815
RMSE train: 0.469857	val: 1.232841	test: 1.292491
MAE train: 0.364861	val: 0.997766	test: 1.035139

Epoch: 77
Loss: 0.32835716009140015
RMSE train: 0.511480	val: 1.225507	test: 1.270073
MAE train: 0.403828	val: 0.987259	test: 1.034629

Epoch: 78
Loss: 0.4096597656607628
RMSE train: 0.455374	val: 1.216434	test: 1.262311
MAE train: 0.350081	val: 0.974335	test: 1.042163

Epoch: 79
Loss: 0.3317093774676323
RMSE train: 0.551278	val: 1.210682	test: 1.263217
MAE train: 0.426920	val: 0.967695	test: 1.027720

Epoch: 80
Loss: 0.3667236417531967
RMSE train: 0.453851	val: 1.376134	test: 1.302056
MAE train: 0.363752	val: 1.106057	test: 1.095869

Epoch: 81
Loss: 0.3619122803211212
RMSE train: 0.374145	val: 1.365704	test: 1.321959
MAE train: 0.294468	val: 1.084350	test: 1.109949

Epoch: 82
Loss: 0.3466091603040695
RMSE train: 0.537505	val: 1.119206	test: 1.219649
MAE train: 0.392401	val: 0.917201	test: 0.992847

Epoch: 83
Loss: 0.3400428742170334
RMSE train: 0.480541	val: 1.238668	test: 1.298952
MAE train: 0.703032	val: 1.178824	test: 1.179821

Epoch: 23
Loss: 0.9531171023845673
RMSE train: 0.986531	val: 1.415077	test: 1.451107
MAE train: 0.763868	val: 1.151710	test: 1.137068

Epoch: 24
Loss: 0.9978377819061279
RMSE train: 0.932029	val: 1.438817	test: 1.472876
MAE train: 0.722703	val: 1.166357	test: 1.179602

Epoch: 25
Loss: 0.9570525884628296
RMSE train: 0.892221	val: 1.355384	test: 1.339735
MAE train: 0.690986	val: 1.103993	test: 1.086773

Epoch: 26
Loss: 0.9479891061782837
RMSE train: 0.981408	val: 1.481552	test: 1.494252
MAE train: 0.757923	val: 1.210891	test: 1.167233

Epoch: 27
Loss: 0.9955036342144012
RMSE train: 0.843726	val: 1.486390	test: 1.490222
MAE train: 0.649921	val: 1.188947	test: 1.188589

Epoch: 28
Loss: 0.9175812900066376
RMSE train: 0.793340	val: 1.311282	test: 1.290753
MAE train: 0.615816	val: 1.064285	test: 1.074509

Epoch: 29
Loss: 0.8221554011106491
RMSE train: 0.790046	val: 1.316389	test: 1.308335
MAE train: 0.617051	val: 1.080857	test: 1.078889

Epoch: 30
Loss: 0.7435085773468018
RMSE train: 0.814272	val: 1.355558	test: 1.354540
MAE train: 0.640379	val: 1.119822	test: 1.088858

Epoch: 31
Loss: 0.7634562999010086
RMSE train: 0.785625	val: 1.375883	test: 1.355126
MAE train: 0.617708	val: 1.146859	test: 1.060053

Epoch: 32
Loss: 0.7572558373212814
RMSE train: 0.833793	val: 1.557077	test: 1.526390
MAE train: 0.656583	val: 1.282051	test: 1.210521

Epoch: 33
Loss: 0.7973176538944244
RMSE train: 0.803133	val: 1.413411	test: 1.418474
MAE train: 0.629851	val: 1.158558	test: 1.119138

Epoch: 34
Loss: 0.7035861909389496
RMSE train: 0.680301	val: 1.320740	test: 1.351563
MAE train: 0.515305	val: 1.085626	test: 1.072644

Epoch: 35
Loss: 0.7042431831359863
RMSE train: 0.755891	val: 1.316225	test: 1.333616
MAE train: 0.591769	val: 1.091612	test: 1.073112

Epoch: 36
Loss: 0.766206219792366
RMSE train: 0.785582	val: 1.394303	test: 1.427114
MAE train: 0.617858	val: 1.142021	test: 1.147624

Epoch: 37
Loss: 0.6769958436489105
RMSE train: 0.662169	val: 1.281524	test: 1.348257
MAE train: 0.502971	val: 1.062256	test: 1.075198

Epoch: 38
Loss: 0.6553453356027603
RMSE train: 0.704035	val: 1.306770	test: 1.299817
MAE train: 0.549611	val: 1.075685	test: 1.054462

Epoch: 39
Loss: 0.6937629878520966
RMSE train: 0.697505	val: 1.320576	test: 1.354757
MAE train: 0.552746	val: 1.080184	test: 1.088928

Epoch: 40
Loss: 0.6746320277452469
RMSE train: 0.665602	val: 1.356247	test: 1.371110
MAE train: 0.507964	val: 1.118362	test: 1.084128

Epoch: 41
Loss: 0.6729383915662766
RMSE train: 0.652690	val: 1.371702	test: 1.372999
MAE train: 0.497681	val: 1.129808	test: 1.081515

Epoch: 42
Loss: 0.5956730395555496
RMSE train: 0.645929	val: 1.318715	test: 1.320588
MAE train: 0.507118	val: 1.095238	test: 1.059546

Epoch: 43
Loss: 0.5988957732915878
RMSE train: 0.648811	val: 1.244288	test: 1.287715
MAE train: 0.511282	val: 1.050934	test: 1.029015

Epoch: 44
Loss: 0.5602017939090729
RMSE train: 0.666378	val: 1.214547	test: 1.289463
MAE train: 0.516756	val: 1.022751	test: 1.025605

Epoch: 45
Loss: 0.6167370229959488
RMSE train: 0.677674	val: 1.240338	test: 1.319855
MAE train: 0.536200	val: 1.028860	test: 1.053778

Epoch: 46
Loss: 0.5413684546947479
RMSE train: 0.635441	val: 1.283808	test: 1.319071
MAE train: 0.508934	val: 1.050656	test: 1.060009

Epoch: 47
Loss: 0.5432984977960587
RMSE train: 0.702775	val: 1.365285	test: 1.389406
MAE train: 0.561951	val: 1.126603	test: 1.090981

Epoch: 48
Loss: 0.5646530538797379
RMSE train: 0.685250	val: 1.363175	test: 1.397416
MAE train: 0.545972	val: 1.121809	test: 1.099007

Epoch: 49
Loss: 0.4713510349392891
RMSE train: 0.657650	val: 1.284882	test: 1.337473
MAE train: 0.523937	val: 1.061440	test: 1.075577

Epoch: 50
Loss: 0.5318292677402496
RMSE train: 0.578914	val: 1.224776	test: 1.272479
MAE train: 0.457896	val: 1.005664	test: 1.033888

Epoch: 51
Loss: 0.5455963686108589
RMSE train: 0.699872	val: 1.377013	test: 1.384605
MAE train: 0.545287	val: 1.129788	test: 1.096311

Epoch: 52
Loss: 0.532446563243866
RMSE train: 0.648604	val: 1.413193	test: 1.420284
MAE train: 0.504784	val: 1.161570	test: 1.132401

Epoch: 53
Loss: 0.516589343547821
RMSE train: 0.561972	val: 1.289466	test: 1.289173
MAE train: 0.444456	val: 1.053359	test: 1.046764

Epoch: 54
Loss: 0.5178457349538803
RMSE train: 0.563226	val: 1.282550	test: 1.296133
MAE train: 0.447784	val: 1.035361	test: 1.033883

Epoch: 55
Loss: 0.47619660198688507
RMSE train: 0.650771	val: 1.379989	test: 1.400185
MAE train: 0.509771	val: 1.134705	test: 1.099105

Epoch: 56
Loss: 0.47293948382139206
RMSE train: 0.548436	val: 1.267984	test: 1.281933
MAE train: 0.424402	val: 1.051429	test: 1.016922

Epoch: 57
Loss: 0.43369901925325394
RMSE train: 0.596050	val: 1.339029	test: 1.312742
MAE train: 0.476711	val: 1.099702	test: 1.052140

Epoch: 58
Loss: 0.45175884664058685
RMSE train: 0.549920	val: 1.310350	test: 1.307545
MAE train: 0.435198	val: 1.090596	test: 1.046477

Epoch: 59
Loss: 0.4125467613339424
RMSE train: 0.484830	val: 1.245816	test: 1.268881
MAE train: 0.369764	val: 1.033723	test: 1.026341

Epoch: 60
Loss: 0.45144617557525635
RMSE train: 0.533325	val: 1.267959	test: 1.327209
MAE train: 0.409790	val: 1.048208	test: 1.056024

Epoch: 61
Loss: 0.4404236301779747
RMSE train: 0.468710	val: 1.222486	test: 1.308359
MAE train: 0.366315	val: 1.010103	test: 1.035598

Epoch: 62
Loss: 0.4356982037425041
RMSE train: 0.540840	val: 1.272218	test: 1.298751
MAE train: 0.423046	val: 1.039812	test: 1.038759

Epoch: 63
Loss: 0.4373958185315132
RMSE train: 0.671624	val: 1.541647	test: 1.562283
MAE train: 0.512910	val: 1.263553	test: 1.238357

Epoch: 64
Loss: 0.4338988661766052
RMSE train: 0.612308	val: 1.410058	test: 1.422328
MAE train: 0.480846	val: 1.172736	test: 1.113672

Epoch: 65
Loss: 0.4429052025079727
RMSE train: 0.451081	val: 1.303453	test: 1.313653
MAE train: 0.353929	val: 1.084399	test: 1.040535

Epoch: 66
Loss: 0.4256187006831169
RMSE train: 0.567529	val: 1.442076	test: 1.481244
MAE train: 0.441152	val: 1.196890	test: 1.185244

Epoch: 67
Loss: 0.44228432327508926
RMSE train: 0.500223	val: 1.328104	test: 1.355765
MAE train: 0.385658	val: 1.105939	test: 1.076417

Epoch: 68
Loss: 0.4059124141931534
RMSE train: 0.425125	val: 1.267433	test: 1.291464
MAE train: 0.328513	val: 1.042698	test: 1.050174

Epoch: 69
Loss: 0.36915286630392075
RMSE train: 0.496050	val: 1.418713	test: 1.451055
MAE train: 0.371708	val: 1.148284	test: 1.175980

Epoch: 70
Loss: 0.49545570462942123
RMSE train: 0.521085	val: 1.348606	test: 1.366836
MAE train: 0.395883	val: 1.119949	test: 1.100314

Epoch: 71
Loss: 0.47310128808021545
RMSE train: 0.413482	val: 1.272082	test: 1.312493
MAE train: 0.314291	val: 1.051574	test: 1.077393

Epoch: 72
Loss: 0.3950783461332321
RMSE train: 0.478096	val: 1.304763	test: 1.426127
MAE train: 0.345411	val: 1.064185	test: 1.148072

Epoch: 73
Loss: 0.399410180747509
RMSE train: 0.514998	val: 1.301721	test: 1.333108
MAE train: 0.401618	val: 1.054392	test: 1.075075

Epoch: 74
Loss: 0.39771606773138046
RMSE train: 0.484594	val: 1.257456	test: 1.328377
MAE train: 0.377152	val: 1.051824	test: 1.073388

Epoch: 75
Loss: 0.3426174446940422
RMSE train: 0.500622	val: 1.275412	test: 1.330067
MAE train: 0.399480	val: 1.064947	test: 1.076330

Epoch: 76
Loss: 0.34218862652778625
RMSE train: 0.623308	val: 1.427911	test: 1.401017
MAE train: 0.480354	val: 1.146119	test: 1.114776

Epoch: 77
Loss: 0.4483557268977165
RMSE train: 0.557944	val: 1.319126	test: 1.338670
MAE train: 0.438517	val: 1.102064	test: 1.070151

Epoch: 78
Loss: 0.35908904671669006
RMSE train: 0.529171	val: 1.269195	test: 1.346933
MAE train: 0.411428	val: 1.056255	test: 1.087644

Epoch: 79
Loss: 0.3561447858810425
RMSE train: 0.444360	val: 1.193229	test: 1.259535
MAE train: 0.342011	val: 0.982814	test: 1.022385

Epoch: 80
Loss: 0.343825601041317
RMSE train: 0.396977	val: 1.185786	test: 1.261723
MAE train: 0.303014	val: 0.988607	test: 1.026442

Epoch: 81
Loss: 0.3313649296760559
RMSE train: 0.524151	val: 1.348634	test: 1.385370
MAE train: 0.419734	val: 1.106381	test: 1.102672

Epoch: 82
Loss: 0.33628397434949875
RMSE train: 0.554983	val: 1.380334	test: 1.436609
MAE train: 0.434508	val: 1.137580	test: 1.140588

Epoch: 83
Loss: 0.3363919109106064

Epoch: 84
Loss: 0.3007783554494381
RMSE train: 0.470517	val: 1.154439	test: 1.164338
MAE train: 0.345589	val: 0.936910	test: 0.899594

Epoch: 85
Loss: 0.25845347344875336
RMSE train: 0.434959	val: 1.033485	test: 1.122092
MAE train: 0.313382	val: 0.875292	test: 0.854207

Epoch: 86
Loss: 0.3256800100207329
RMSE train: 0.405591	val: 1.053368	test: 1.117723
MAE train: 0.293546	val: 0.865826	test: 0.859886

Epoch: 87
Loss: 0.2960413992404938
RMSE train: 0.419830	val: 1.090201	test: 1.132505
MAE train: 0.305182	val: 0.889760	test: 0.872914

Epoch: 88
Loss: 0.2834484167397022
RMSE train: 0.430946	val: 1.080597	test: 1.148464
MAE train: 0.306773	val: 0.893617	test: 0.879864

Epoch: 89
Loss: 0.3022514805197716
RMSE train: 0.446713	val: 1.056431	test: 1.142948
MAE train: 0.316489	val: 0.891674	test: 0.871683

Epoch: 90
Loss: 0.29040496051311493
RMSE train: 0.420901	val: 1.111723	test: 1.126217
MAE train: 0.304964	val: 0.899833	test: 0.871806

Epoch: 91
Loss: 0.29742633551359177
RMSE train: 0.400235	val: 1.138454	test: 1.160710
MAE train: 0.291428	val: 0.900406	test: 0.880202

Epoch: 92
Loss: 0.27916502952575684
RMSE train: 0.450925	val: 1.002277	test: 1.123929
MAE train: 0.323562	val: 0.849378	test: 0.854209

Epoch: 93
Loss: 0.2913719117641449
RMSE train: 0.465335	val: 1.028685	test: 1.148933
MAE train: 0.334346	val: 0.877319	test: 0.876614

Epoch: 94
Loss: 0.2649969831109047
RMSE train: 0.430922	val: 1.112133	test: 1.167075
MAE train: 0.316807	val: 0.905074	test: 0.895847

Epoch: 95
Loss: 0.2541065700352192
RMSE train: 0.393600	val: 1.076199	test: 1.145010
MAE train: 0.291027	val: 0.867139	test: 0.866774

Epoch: 96
Loss: 0.2756839394569397
RMSE train: 0.441471	val: 1.073535	test: 1.173898
MAE train: 0.319808	val: 0.890812	test: 0.902871

Epoch: 97
Loss: 0.28891341388225555
RMSE train: 0.409035	val: 1.026370	test: 1.157985
MAE train: 0.292992	val: 0.864642	test: 0.880130

Epoch: 98
Loss: 0.2547367662191391
RMSE train: 0.464011	val: 1.101486	test: 1.206777
MAE train: 0.331188	val: 0.907184	test: 0.926321

Epoch: 99
Loss: 0.23995567485690117
RMSE train: 0.414841	val: 1.068905	test: 1.168258
MAE train: 0.295675	val: 0.875215	test: 0.886057

Epoch: 100
Loss: 0.25848206132650375
RMSE train: 0.446225	val: 1.154373	test: 1.173093
MAE train: 0.320072	val: 0.924979	test: 0.904018

Epoch: 101
Loss: 0.2865997403860092
RMSE train: 0.408896	val: 0.998872	test: 1.117749
MAE train: 0.286671	val: 0.843367	test: 0.836186

Epoch: 102
Loss: 0.2538582719862461
RMSE train: 0.385955	val: 1.001757	test: 1.117987
MAE train: 0.283054	val: 0.839166	test: 0.834340

Epoch: 103
Loss: 0.24450420588254929
RMSE train: 0.443231	val: 1.130695	test: 1.132847
MAE train: 0.323181	val: 0.899580	test: 0.865055

Epoch: 104
Loss: 0.24898162484169006
RMSE train: 0.384814	val: 1.098676	test: 1.113800
MAE train: 0.284747	val: 0.871188	test: 0.836764

Epoch: 105
Loss: 0.2662789337337017
RMSE train: 0.403675	val: 0.973997	test: 1.085681
MAE train: 0.298991	val: 0.822304	test: 0.831182

Epoch: 106
Loss: 0.27263758704066277
RMSE train: 0.403575	val: 0.961483	test: 1.061435
MAE train: 0.296206	val: 0.825224	test: 0.804422

Epoch: 107
Loss: 0.2530273012816906
RMSE train: 0.374666	val: 0.985232	test: 1.075719
MAE train: 0.277933	val: 0.825680	test: 0.821555

Epoch: 108
Loss: 0.2468927539885044
RMSE train: 0.403057	val: 1.063760	test: 1.147095
MAE train: 0.290976	val: 0.870610	test: 0.877627

Epoch: 109
Loss: 0.27117710560560226
RMSE train: 0.388110	val: 1.045631	test: 1.181956
MAE train: 0.286330	val: 0.877691	test: 0.901591

Epoch: 110
Loss: 0.3133164793252945
RMSE train: 0.379739	val: 1.088877	test: 1.176537
MAE train: 0.274423	val: 0.891928	test: 0.905594

Epoch: 111
Loss: 0.24401848763227463
RMSE train: 0.401291	val: 1.035599	test: 1.111898
MAE train: 0.287810	val: 0.860037	test: 0.848225

Epoch: 112
Loss: 0.26884206756949425
RMSE train: 0.364316	val: 0.989326	test: 1.079679
MAE train: 0.268922	val: 0.815704	test: 0.818869

Epoch: 113
Loss: 0.37925153970718384
RMSE train: 0.390536	val: 1.163090	test: 1.137364
MAE train: 0.287378	val: 0.901863	test: 0.862343

Epoch: 114
Loss: 0.24060394242405891
RMSE train: 0.353975	val: 1.091085	test: 1.112884
MAE train: 0.264714	val: 0.855929	test: 0.840651

Epoch: 115
Loss: 0.2609372138977051
RMSE train: 0.466342	val: 1.132313	test: 1.182233
MAE train: 0.344949	val: 0.927027	test: 0.913150

Epoch: 116
Loss: 0.27333107963204384
RMSE train: 0.392435	val: 1.037781	test: 1.123251
MAE train: 0.288485	val: 0.849510	test: 0.865751

Epoch: 117
Loss: 0.2595035769045353
RMSE train: 0.376715	val: 0.999814	test: 1.101594
MAE train: 0.278346	val: 0.838343	test: 0.845599

Epoch: 118
Loss: 0.23234814777970314
RMSE train: 0.412585	val: 1.079467	test: 1.123952
MAE train: 0.299717	val: 0.886341	test: 0.865687

Epoch: 119
Loss: 0.26125650107860565
RMSE train: 0.388028	val: 1.026779	test: 1.141532
MAE train: 0.287898	val: 0.844157	test: 0.880065

Epoch: 120
Loss: 0.22893399372696877
RMSE train: 0.423919	val: 1.017064	test: 1.141852
MAE train: 0.306640	val: 0.854552	test: 0.870341

Epoch: 121
Loss: 0.24895844608545303
RMSE train: 0.459821	val: 1.187835	test: 1.184219
MAE train: 0.335740	val: 0.951671	test: 0.922258

Early stopping
Best (RMSE):	 train: 0.419544	val: 0.950534	test: 1.065032
Best (MAE):	 train: 0.301974	val: 0.807068	test: 0.818785

MAE train: 0.368300	val: 1.004657	test: 1.042513

Epoch: 84
Loss: 0.31774771958589554
RMSE train: 0.417832	val: 1.515296	test: 1.481801
MAE train: 0.335657	val: 1.233664	test: 1.195937

Epoch: 85
Loss: 0.27909260988235474
RMSE train: 0.404274	val: 1.296175	test: 1.344986
MAE train: 0.319922	val: 1.052942	test: 1.082176

Epoch: 86
Loss: 0.2840672470629215
RMSE train: 0.470088	val: 1.249438	test: 1.271763
MAE train: 0.373581	val: 1.000931	test: 1.048675

Epoch: 87
Loss: 0.3091900870203972
RMSE train: 0.427219	val: 1.281427	test: 1.274801
MAE train: 0.341037	val: 1.030992	test: 1.050168

Epoch: 88
Loss: 0.28093549236655235
RMSE train: 0.356604	val: 1.215794	test: 1.259927
MAE train: 0.275734	val: 0.977198	test: 1.027790

Epoch: 89
Loss: 0.31305602192878723
RMSE train: 0.465531	val: 1.235082	test: 1.260167
MAE train: 0.367828	val: 0.993119	test: 1.051003

Epoch: 90
Loss: 0.32408296316862106
RMSE train: 0.455514	val: 1.197136	test: 1.259337
MAE train: 0.355071	val: 0.967243	test: 1.045142

Epoch: 91
Loss: 0.30433620512485504
RMSE train: 0.451828	val: 1.267226	test: 1.298360
MAE train: 0.359158	val: 1.019215	test: 1.066573

Epoch: 92
Loss: 0.3074565455317497
RMSE train: 0.429502	val: 1.178913	test: 1.266818
MAE train: 0.324173	val: 0.956121	test: 1.028786

Epoch: 93
Loss: 0.2945266589522362
RMSE train: 0.458378	val: 1.184560	test: 1.239940
MAE train: 0.344077	val: 0.948650	test: 1.032668

Epoch: 94
Loss: 0.29125451296567917
RMSE train: 0.404229	val: 1.247891	test: 1.264512
MAE train: 0.314079	val: 0.997456	test: 1.029550

Epoch: 95
Loss: 0.3111971989274025
RMSE train: 0.395015	val: 1.268914	test: 1.274395
MAE train: 0.309078	val: 1.021949	test: 1.035843

Epoch: 96
Loss: 0.2532525025308132
RMSE train: 0.512441	val: 1.371975	test: 1.312374
MAE train: 0.397442	val: 1.111661	test: 1.097424

Epoch: 97
Loss: 0.31329871714115143
RMSE train: 0.435126	val: 1.240994	test: 1.241550
MAE train: 0.339399	val: 0.994174	test: 1.027905

Epoch: 98
Loss: 0.2906063571572304
RMSE train: 0.456042	val: 1.175392	test: 1.219427
MAE train: 0.350109	val: 0.945605	test: 0.998266

Epoch: 99
Loss: 0.28547415882349014
RMSE train: 0.399822	val: 1.302442	test: 1.299006
MAE train: 0.310452	val: 1.034767	test: 1.072953

Epoch: 100
Loss: 0.29865898191928864
RMSE train: 0.421731	val: 1.325095	test: 1.286993
MAE train: 0.331765	val: 1.051551	test: 1.070432

Epoch: 101
Loss: 0.3076218143105507
RMSE train: 0.471446	val: 1.195808	test: 1.225424
MAE train: 0.353954	val: 0.955084	test: 1.000440

Epoch: 102
Loss: 0.2703262269496918
RMSE train: 0.381854	val: 1.367597	test: 1.282827
MAE train: 0.295016	val: 1.094827	test: 1.079394

Epoch: 103
Loss: 0.3540734499692917
RMSE train: 0.298565	val: 1.278230	test: 1.287308
MAE train: 0.227228	val: 1.022228	test: 1.032251

Epoch: 104
Loss: 0.28894174844026566
RMSE train: 0.483005	val: 1.209369	test: 1.256821
MAE train: 0.361853	val: 0.975125	test: 1.018938

Epoch: 105
Loss: 0.28548893332481384
RMSE train: 0.359574	val: 1.381380	test: 1.324512
MAE train: 0.286939	val: 1.091628	test: 1.109551

Epoch: 106
Loss: 0.28233809024095535
RMSE train: 0.334743	val: 1.326752	test: 1.325571
MAE train: 0.265193	val: 1.055853	test: 1.088046

Epoch: 107
Loss: 0.26840297877788544
RMSE train: 0.467088	val: 1.177698	test: 1.262300
MAE train: 0.345970	val: 0.965645	test: 1.018104

Epoch: 108
Loss: 0.2658602222800255
RMSE train: 0.336154	val: 1.258096	test: 1.296384
MAE train: 0.261644	val: 0.994919	test: 1.073250

Epoch: 109
Loss: 0.27036186307668686
RMSE train: 0.342692	val: 1.438692	test: 1.373404
MAE train: 0.271977	val: 1.137290	test: 1.154669

Epoch: 110
Loss: 0.25053533911705017
RMSE train: 0.346125	val: 1.317771	test: 1.350795
MAE train: 0.261353	val: 1.068216	test: 1.094231

Epoch: 111
Loss: 0.2556995674967766
RMSE train: 0.417417	val: 1.435588	test: 1.331636
MAE train: 0.325267	val: 1.140660	test: 1.133741

Epoch: 112
Loss: 0.268545288592577
RMSE train: 0.300716	val: 1.337496	test: 1.301062
MAE train: 0.234995	val: 1.055078	test: 1.083505

Epoch: 113
Loss: 0.23277129977941513
RMSE train: 0.276424	val: 1.375673	test: 1.337751
MAE train: 0.215804	val: 1.090466	test: 1.101265

Epoch: 114
Loss: 0.26080934703350067
RMSE train: 0.311554	val: 1.484036	test: 1.376438
MAE train: 0.249374	val: 1.172700	test: 1.153761

Epoch: 115
Loss: 0.23859857022762299
RMSE train: 0.288734	val: 1.384251	test: 1.362733
MAE train: 0.218870	val: 1.102914	test: 1.112376

Epoch: 116
Loss: 0.2449021115899086
RMSE train: 0.292304	val: 1.441147	test: 1.383926
MAE train: 0.225444	val: 1.138775	test: 1.162791

Epoch: 117
Loss: 0.2300104685127735
RMSE train: 0.418328	val: 1.493168	test: 1.370490
MAE train: 0.313934	val: 1.181039	test: 1.178558

Epoch: 118
Loss: 0.2750827334821224
RMSE train: 0.332064	val: 1.448265	test: 1.386640
MAE train: 0.252089	val: 1.167836	test: 1.151832

Epoch: 119
Loss: 0.22654393687844276
RMSE train: 0.399272	val: 1.416732	test: 1.338689
MAE train: 0.315093	val: 1.125123	test: 1.135244

Epoch: 120
Loss: 0.22448090463876724
RMSE train: 0.461986	val: 1.294688	test: 1.279402
MAE train: 0.355154	val: 1.033565	test: 1.086287

Epoch: 121
Loss: 0.21230674535036087
RMSE train: 0.358281	val: 1.347817	test: 1.312640
MAE train: 0.279104	val: 1.076623	test: 1.104750

Early stopping
Best (RMSE):	 train: 0.624493	val: 1.103043	test: 1.218300
Best (MAE):	 train: 0.497804	val: 0.898992	test: 1.006213

RMSE train: 0.386361	val: 1.347131	test: 1.352753
MAE train: 0.291724	val: 1.099031	test: 1.098949

Epoch: 84
Loss: 0.31175733357667923
RMSE train: 0.391376	val: 1.265959	test: 1.322324
MAE train: 0.291382	val: 1.038946	test: 1.083039

Epoch: 85
Loss: 0.3347812592983246
RMSE train: 0.525213	val: 1.296937	test: 1.389886
MAE train: 0.404814	val: 1.085610	test: 1.124331

Epoch: 86
Loss: 0.32841022312641144
RMSE train: 0.530393	val: 1.360585	test: 1.391707
MAE train: 0.414970	val: 1.100977	test: 1.113369

Epoch: 87
Loss: 0.3359941691160202
RMSE train: 0.431788	val: 1.452462	test: 1.391066
MAE train: 0.325178	val: 1.177188	test: 1.134349

Epoch: 88
Loss: 0.325626403093338
RMSE train: 0.470387	val: 1.249100	test: 1.314734
MAE train: 0.365182	val: 1.036263	test: 1.047660

Epoch: 89
Loss: 0.3190022259950638
RMSE train: 0.492623	val: 1.261726	test: 1.365220
MAE train: 0.368557	val: 1.032807	test: 1.082793

Epoch: 90
Loss: 0.3231166899204254
RMSE train: 0.442261	val: 1.325981	test: 1.346297
MAE train: 0.335779	val: 1.078685	test: 1.099494

Epoch: 91
Loss: 0.3249475881457329
RMSE train: 0.501087	val: 1.260699	test: 1.352766
MAE train: 0.386444	val: 1.042130	test: 1.071677

Epoch: 92
Loss: 0.34018929302692413
RMSE train: 0.480113	val: 1.322825	test: 1.392954
MAE train: 0.374057	val: 1.091771	test: 1.110788

Epoch: 93
Loss: 0.3630964830517769
RMSE train: 0.394840	val: 1.372430	test: 1.376067
MAE train: 0.303821	val: 1.109355	test: 1.100432

Epoch: 94
Loss: 0.3217850774526596
RMSE train: 0.344651	val: 1.284569	test: 1.330979
MAE train: 0.267604	val: 1.060385	test: 1.081874

Epoch: 95
Loss: 0.3350900635123253
RMSE train: 0.391290	val: 1.326174	test: 1.376645
MAE train: 0.300351	val: 1.110006	test: 1.105581

Epoch: 96
Loss: 0.3319365829229355
RMSE train: 0.322050	val: 1.344743	test: 1.396469
MAE train: 0.248291	val: 1.096212	test: 1.134002

Epoch: 97
Loss: 0.3064878396689892
RMSE train: 0.370503	val: 1.332467	test: 1.380688
MAE train: 0.282785	val: 1.089418	test: 1.126506

Epoch: 98
Loss: 0.3443584330379963
RMSE train: 0.609337	val: 1.546692	test: 1.577185
MAE train: 0.470336	val: 1.251191	test: 1.231492

Epoch: 99
Loss: 0.2821406088769436
RMSE train: 0.541931	val: 1.440879	test: 1.501446
MAE train: 0.413625	val: 1.173291	test: 1.175861

Epoch: 100
Loss: 0.30956626683473587
RMSE train: 0.475486	val: 1.380102	test: 1.438302
MAE train: 0.373672	val: 1.101849	test: 1.125232

Epoch: 101
Loss: 0.26667315140366554
RMSE train: 0.537118	val: 1.416531	test: 1.457314
MAE train: 0.428891	val: 1.127108	test: 1.154019

Epoch: 102
Loss: 0.3213260844349861
RMSE train: 0.415986	val: 1.272244	test: 1.354792
MAE train: 0.325800	val: 1.038882	test: 1.068434

Epoch: 103
Loss: 0.28427036851644516
RMSE train: 0.501020	val: 1.366946	test: 1.438613
MAE train: 0.379841	val: 1.125061	test: 1.132482

Epoch: 104
Loss: 0.2688834182918072
RMSE train: 0.487423	val: 1.396808	test: 1.445736
MAE train: 0.375020	val: 1.151260	test: 1.151514

Epoch: 105
Loss: 0.2701515108346939
RMSE train: 0.358852	val: 1.292512	test: 1.354557
MAE train: 0.271056	val: 1.068856	test: 1.081154

Epoch: 106
Loss: 0.269240889698267
RMSE train: 0.380357	val: 1.343023	test: 1.400994
MAE train: 0.288338	val: 1.093830	test: 1.111080

Epoch: 107
Loss: 0.28052058815956116
RMSE train: 0.441505	val: 1.407299	test: 1.485302
MAE train: 0.328227	val: 1.156259	test: 1.178507

Epoch: 108
Loss: 0.3046758249402046
RMSE train: 0.374928	val: 1.332106	test: 1.411119
MAE train: 0.283108	val: 1.096066	test: 1.117991

Epoch: 109
Loss: 0.2523651048541069
RMSE train: 0.370640	val: 1.349437	test: 1.426017
MAE train: 0.282196	val: 1.098307	test: 1.137828

Epoch: 110
Loss: 0.28189198672771454
RMSE train: 0.359691	val: 1.273452	test: 1.386426
MAE train: 0.272356	val: 1.063711	test: 1.110865

Epoch: 111
Loss: 0.2780971825122833
RMSE train: 0.455449	val: 1.337143	test: 1.400103
MAE train: 0.357805	val: 1.089670	test: 1.121346

Epoch: 112
Loss: 0.2627897709608078
RMSE train: 0.333029	val: 1.263088	test: 1.330135
MAE train: 0.257540	val: 1.024231	test: 1.090215

Epoch: 113
Loss: 0.24077537655830383
RMSE train: 0.372707	val: 1.264252	test: 1.329121
MAE train: 0.291425	val: 1.046732	test: 1.071872

Epoch: 114
Loss: 0.2353750355541706
RMSE train: 0.431759	val: 1.345841	test: 1.415234
MAE train: 0.337370	val: 1.121442	test: 1.133430

Epoch: 115
Loss: 0.2664804980158806
RMSE train: 0.390241	val: 1.308063	test: 1.394977
MAE train: 0.295410	val: 1.096473	test: 1.116374

Epoch: 116
Loss: 0.22680748999118805
RMSE train: 0.405121	val: 1.339575	test: 1.387932
MAE train: 0.304929	val: 1.079844	test: 1.115480

Epoch: 117
Loss: 0.24466881155967712
RMSE train: 0.387212	val: 1.326230	test: 1.378778
MAE train: 0.297287	val: 1.075996	test: 1.099233

Epoch: 118
Loss: 0.26555832475423813
RMSE train: 0.519666	val: 1.522575	test: 1.560762
MAE train: 0.396701	val: 1.251147	test: 1.240884

Epoch: 119
Loss: 0.24802901968359947
RMSE train: 0.447390	val: 1.446645	test: 1.486197
MAE train: 0.334609	val: 1.183191	test: 1.172577

Epoch: 120
Loss: 0.2415580116212368
RMSE train: 0.317317	val: 1.301402	test: 1.353470
MAE train: 0.237460	val: 1.070532	test: 1.079177

Epoch: 121
Loss: 0.2974756434559822
RMSE train: 0.404748	val: 1.405975	test: 1.458922
MAE train: 0.298532	val: 1.161615	test: 1.159184

Early stopping
Best (RMSE):	 train: 0.396977	val: 1.185786	test: 1.261723
Best (MAE):	 train: 0.303014	val: 0.988607	test: 1.026442

RMSE train: 0.467815	val: 1.170764	test: 1.320856
MAE train: 0.342706	val: 0.954115	test: 1.038631

Epoch: 84
Loss: 0.2859290838241577
RMSE train: 0.350885	val: 1.034749	test: 1.245610
MAE train: 0.262405	val: 0.838962	test: 0.965224

Epoch: 85
Loss: 0.33201125264167786
RMSE train: 0.412138	val: 1.046904	test: 1.197737
MAE train: 0.309094	val: 0.885935	test: 0.937211

Epoch: 86
Loss: 0.3326101005077362
RMSE train: 0.437011	val: 1.097397	test: 1.241570
MAE train: 0.329169	val: 0.911503	test: 0.977021

Epoch: 87
Loss: 0.25951965898275375
RMSE train: 0.359938	val: 1.028400	test: 1.208241
MAE train: 0.272867	val: 0.854399	test: 0.938598

Epoch: 88
Loss: 0.2695760168135166
RMSE train: 0.545897	val: 1.186089	test: 1.317584
MAE train: 0.402137	val: 0.950162	test: 1.050973

Epoch: 89
Loss: 0.2799099311232567
RMSE train: 0.466217	val: 1.046434	test: 1.201200
MAE train: 0.347385	val: 0.869042	test: 0.930625

Epoch: 90
Loss: 0.2589912824332714
RMSE train: 0.439875	val: 1.052678	test: 1.214303
MAE train: 0.323819	val: 0.876578	test: 0.937826

Epoch: 91
Loss: 0.2267472855746746
RMSE train: 0.500031	val: 1.152345	test: 1.306876
MAE train: 0.363355	val: 0.930374	test: 1.033322

Epoch: 92
Loss: 0.25394346937537193
RMSE train: 0.430388	val: 1.038912	test: 1.215732
MAE train: 0.307764	val: 0.857187	test: 0.917136

Epoch: 93
Loss: 0.24070512875914574
RMSE train: 0.505468	val: 1.125348	test: 1.270360
MAE train: 0.380959	val: 0.939843	test: 0.985467

Epoch: 94
Loss: 0.25593071058392525
RMSE train: 0.402142	val: 1.046731	test: 1.209877
MAE train: 0.303837	val: 0.870370	test: 0.940031

Epoch: 95
Loss: 0.2969142273068428
RMSE train: 0.389659	val: 1.008835	test: 1.194401
MAE train: 0.294200	val: 0.844000	test: 0.932483

Epoch: 96
Loss: 0.28108326718211174
RMSE train: 0.454724	val: 1.021279	test: 1.196355
MAE train: 0.337008	val: 0.856490	test: 0.925319

Epoch: 97
Loss: 0.25560086220502853
RMSE train: 0.393699	val: 1.011196	test: 1.188689
MAE train: 0.299517	val: 0.826029	test: 0.918599

Epoch: 98
Loss: 0.2186249978840351
RMSE train: 0.481616	val: 1.137543	test: 1.269772
MAE train: 0.371717	val: 0.918313	test: 1.001511

Epoch: 99
Loss: 0.2941197454929352
RMSE train: 0.445477	val: 1.040275	test: 1.189797
MAE train: 0.333945	val: 0.875195	test: 0.923928

Epoch: 100
Loss: 0.2482980564236641
RMSE train: 0.402059	val: 1.049378	test: 1.216460
MAE train: 0.301416	val: 0.873690	test: 0.955958

Epoch: 101
Loss: 0.2633543461561203
RMSE train: 0.574810	val: 1.200137	test: 1.321833
MAE train: 0.427245	val: 1.012973	test: 1.034364

Epoch: 102
Loss: 0.22846455872058868
RMSE train: 0.373680	val: 1.013778	test: 1.227139
MAE train: 0.269326	val: 0.837932	test: 0.925896

Epoch: 103
Loss: 0.27170608565211296
RMSE train: 0.373435	val: 1.102854	test: 1.302495
MAE train: 0.271167	val: 0.871093	test: 1.007251

Epoch: 104
Loss: 0.24081259965896606
RMSE train: 0.345085	val: 1.009767	test: 1.209072
MAE train: 0.256488	val: 0.845826	test: 0.920671

Epoch: 105
Loss: 0.2413811832666397
RMSE train: 0.365594	val: 1.026928	test: 1.203820
MAE train: 0.276665	val: 0.856490	test: 0.939624

Epoch: 106
Loss: 0.21763335540890694
RMSE train: 0.366077	val: 1.025884	test: 1.215336
MAE train: 0.266814	val: 0.843380	test: 0.951295

Epoch: 107
Loss: 0.2317701391875744
RMSE train: 0.361414	val: 1.002700	test: 1.199385
MAE train: 0.267481	val: 0.826676	test: 0.914438

Epoch: 108
Loss: 0.2363874800503254
RMSE train: 0.350966	val: 0.998024	test: 1.178475
MAE train: 0.257443	val: 0.828474	test: 0.906230

Epoch: 109
Loss: 0.23524268716573715
RMSE train: 0.427196	val: 1.176787	test: 1.330803
MAE train: 0.316601	val: 0.945400	test: 1.020935

Epoch: 110
Loss: 0.2403687909245491
RMSE train: 0.398324	val: 1.074900	test: 1.236096
MAE train: 0.300593	val: 0.896820	test: 0.943941

Epoch: 111
Loss: 0.22132692858576775
RMSE train: 0.396675	val: 1.071486	test: 1.221193
MAE train: 0.294601	val: 0.893986	test: 0.944753

Epoch: 112
Loss: 0.20450345426797867
RMSE train: 0.311745	val: 1.046358	test: 1.252556
MAE train: 0.237506	val: 0.853430	test: 0.959467

Epoch: 113
Loss: 0.244902815669775
RMSE train: 0.465928	val: 1.142779	test: 1.304821
MAE train: 0.347169	val: 0.943225	test: 1.013846

Epoch: 114
Loss: 0.25665855407714844
RMSE train: 0.357775	val: 1.084477	test: 1.271336
MAE train: 0.256900	val: 0.887142	test: 0.977817

Epoch: 115
Loss: 0.2119249776005745
RMSE train: 0.313387	val: 1.048936	test: 1.272000
MAE train: 0.239221	val: 0.842199	test: 0.967109

Epoch: 116
Loss: 0.2410251721739769
RMSE train: 0.396230	val: 1.135802	test: 1.301283
MAE train: 0.286499	val: 0.921743	test: 1.012809

Epoch: 117
Loss: 0.24993539229035378
RMSE train: 0.370590	val: 1.056255	test: 1.232264
MAE train: 0.268787	val: 0.863812	test: 0.944670

Epoch: 118
Loss: 0.20753238722682
RMSE train: 0.320494	val: 0.970946	test: 1.133245
MAE train: 0.228987	val: 0.807328	test: 0.864040

Epoch: 119
Loss: 0.21769429743289948
RMSE train: 0.456208	val: 1.072991	test: 1.205930
MAE train: 0.333334	val: 0.886233	test: 0.931475

Epoch: 120
Loss: 0.22798219323158264
RMSE train: 0.351677	val: 1.054163	test: 1.206584
MAE train: 0.258590	val: 0.836802	test: 0.924866

Epoch: 121
Loss: 0.21418533101677895
RMSE train: 0.321579	val: 1.071123	test: 1.259729
MAE train: 0.244687	val: 0.848203	test: 0.967233

Early stopping
Best (RMSE):	 train: 0.475173	val: 0.966085	test: 1.166394
Best (MAE):	 train: 0.348754	val: 0.794010	test: 0.932067


Epoch: 84
Loss: 0.28106969594955444
RMSE train: 0.410366	val: 1.065824	test: 1.077623
MAE train: 0.297283	val: 0.859236	test: 0.795472

Epoch: 85
Loss: 0.3256135508418083
RMSE train: 0.475646	val: 1.108503	test: 1.126599
MAE train: 0.339706	val: 0.899188	test: 0.853886

Epoch: 86
Loss: 0.282311275601387
RMSE train: 0.450129	val: 1.101938	test: 1.099282
MAE train: 0.326499	val: 0.889068	test: 0.832827

Epoch: 87
Loss: 0.31430109590291977
RMSE train: 0.469576	val: 1.004894	test: 1.044383
MAE train: 0.333968	val: 0.825020	test: 0.778190

Epoch: 88
Loss: 0.3104560971260071
RMSE train: 0.483724	val: 1.079655	test: 1.090252
MAE train: 0.342757	val: 0.890850	test: 0.818428

Epoch: 89
Loss: 0.30713415890932083
RMSE train: 0.459872	val: 1.124016	test: 1.087425
MAE train: 0.327835	val: 0.910404	test: 0.815221

Epoch: 90
Loss: 0.3175211474299431
RMSE train: 0.503842	val: 1.116976	test: 1.104847
MAE train: 0.357697	val: 0.910833	test: 0.836659

Epoch: 91
Loss: 0.2876303791999817
RMSE train: 0.439840	val: 1.073374	test: 1.068368
MAE train: 0.318939	val: 0.872626	test: 0.802391

Epoch: 92
Loss: 0.32019221782684326
RMSE train: 0.477517	val: 1.112104	test: 1.098934
MAE train: 0.344875	val: 0.896483	test: 0.842184

Epoch: 93
Loss: 0.2711559496819973
RMSE train: 0.451318	val: 1.135161	test: 1.120519
MAE train: 0.328003	val: 0.902884	test: 0.847571

Epoch: 94
Loss: 0.28902046382427216
RMSE train: 0.459734	val: 1.087013	test: 1.082397
MAE train: 0.325441	val: 0.868226	test: 0.819255

Epoch: 95
Loss: 0.3090471960604191
RMSE train: 0.458515	val: 1.011255	test: 1.051405
MAE train: 0.322492	val: 0.838841	test: 0.779857

Epoch: 96
Loss: 0.28351879119873047
RMSE train: 0.445091	val: 1.142226	test: 1.106067
MAE train: 0.324456	val: 0.915490	test: 0.832221

Epoch: 97
Loss: 0.3192061707377434
RMSE train: 0.467995	val: 1.131790	test: 1.113025
MAE train: 0.337656	val: 0.902553	test: 0.841026

Epoch: 98
Loss: 0.27348586171865463
RMSE train: 0.523337	val: 1.115900	test: 1.111035
MAE train: 0.374495	val: 0.900042	test: 0.849187

Epoch: 99
Loss: 0.24498369544744492
RMSE train: 0.437575	val: 1.022238	test: 1.050937
MAE train: 0.310843	val: 0.831713	test: 0.783286

Epoch: 100
Loss: 0.2914619818329811
RMSE train: 0.469301	val: 1.137337	test: 1.118329
MAE train: 0.338004	val: 0.901556	test: 0.852711

Epoch: 101
Loss: 0.23776070028543472
RMSE train: 0.468755	val: 1.142725	test: 1.114039
MAE train: 0.340408	val: 0.906313	test: 0.848345

Epoch: 102
Loss: 0.27938730269670486
RMSE train: 0.388879	val: 0.996447	test: 1.027382
MAE train: 0.277772	val: 0.815919	test: 0.759492

Epoch: 103
Loss: 0.31822704523801804
RMSE train: 0.371496	val: 1.085409	test: 1.081052
MAE train: 0.270579	val: 0.869910	test: 0.804844

Epoch: 104
Loss: 0.30998292565345764
RMSE train: 0.420154	val: 1.204485	test: 1.138612
MAE train: 0.309987	val: 0.953005	test: 0.861563

Epoch: 105
Loss: 0.2711271047592163
RMSE train: 0.378653	val: 1.068972	test: 1.052186
MAE train: 0.278121	val: 0.856806	test: 0.771690

Epoch: 106
Loss: 0.2528780773282051
RMSE train: 0.411503	val: 1.131949	test: 1.082693
MAE train: 0.294891	val: 0.885572	test: 0.809687

Epoch: 107
Loss: 0.26169469207525253
RMSE train: 0.431714	val: 1.143045	test: 1.110024
MAE train: 0.309254	val: 0.885223	test: 0.839516

Epoch: 108
Loss: 0.25796475261449814
RMSE train: 0.538027	val: 1.253104	test: 1.193304
MAE train: 0.385101	val: 0.974860	test: 0.928986

Epoch: 109
Loss: 0.25680892914533615
RMSE train: 0.485551	val: 1.107876	test: 1.115697
MAE train: 0.349277	val: 0.883726	test: 0.843815

Epoch: 110
Loss: 0.2615089975297451
RMSE train: 0.448291	val: 1.128416	test: 1.086479
MAE train: 0.323613	val: 0.894042	test: 0.815298

Epoch: 111
Loss: 0.27228377759456635
RMSE train: 0.394006	val: 1.119529	test: 1.062840
MAE train: 0.282013	val: 0.886683	test: 0.786025

Epoch: 112
Loss: 0.23705419898033142
RMSE train: 0.391374	val: 1.041912	test: 1.039798
MAE train: 0.276686	val: 0.845511	test: 0.777354

Epoch: 113
Loss: 0.22628958150744438
RMSE train: 0.402668	val: 1.086880	test: 1.056640
MAE train: 0.294027	val: 0.879121	test: 0.783813

Epoch: 114
Loss: 0.26229220628738403
RMSE train: 0.365923	val: 1.069306	test: 1.068130
MAE train: 0.268618	val: 0.867167	test: 0.781761

Epoch: 115
Loss: 0.26274265348911285
RMSE train: 0.476620	val: 1.123689	test: 1.098357
MAE train: 0.343992	val: 0.908614	test: 0.819473

Epoch: 116
Loss: 0.23616144806146622
RMSE train: 0.485517	val: 1.056317	test: 1.105555
MAE train: 0.344677	val: 0.865982	test: 0.827695

Epoch: 117
Loss: 0.2734397426247597
RMSE train: 0.552272	val: 1.181384	test: 1.180978
MAE train: 0.392005	val: 0.944793	test: 0.904973

Epoch: 118
Loss: 0.2729771099984646
RMSE train: 0.497219	val: 1.106045	test: 1.150560
MAE train: 0.357051	val: 0.891128	test: 0.861361

Epoch: 119
Loss: 0.267465528100729
RMSE train: 0.468187	val: 1.069662	test: 1.110540
MAE train: 0.331832	val: 0.865246	test: 0.827051

Epoch: 120
Loss: 0.25407154113054276
RMSE train: 0.431663	val: 1.105377	test: 1.082423
MAE train: 0.305815	val: 0.885319	test: 0.816731

Epoch: 121
Loss: 0.2466970682144165
RMSE train: 0.370638	val: 0.995137	test: 1.068606
MAE train: 0.269604	val: 0.821128	test: 0.791342

Epoch: 122
Loss: 0.24224118888378143
RMSE train: 0.414154	val: 1.110919	test: 1.086077
MAE train: 0.293255	val: 0.886762	test: 0.816363

Epoch: 123
Loss: 0.25964586436748505
RMSE train: 0.376020	val: 1.001347	test: 1.107842
MAE train: 0.284671	val: 0.821816	test: 0.818241

Epoch: 124
Loss: 0.26123762130737305
RMSE train: 0.485691	val: 1.172777	test: 1.161708
MAE train: 0.352159	val: 0.943608	test: 0.888206

Epoch: 125
Loss: 0.25180069357156754
RMSE train: 0.375921	val: 1.046898	test: 1.079999
MAE train: 0.270729	val: 0.853494	test: 0.800415

Epoch: 126
Loss: 0.2519826740026474
RMSE train: 0.340823	val: 1.074847	test: 1.108092
MAE train: 0.252812	val: 0.880552	test: 0.808759

Epoch: 127
Loss: 0.24002595618367195
RMSE train: 0.504768	val: 1.178128	test: 1.146481
MAE train: 0.363862	val: 0.942338	test: 0.885586

Epoch: 128
Loss: 0.26314693316817284
RMSE train: 0.362524	val: 1.011311	test: 1.047970
MAE train: 0.262398	val: 0.824814	test: 0.761709

Epoch: 129
Loss: 0.2550628185272217
RMSE train: 0.399964	val: 1.116166	test: 1.092779
MAE train: 0.284070	val: 0.884680	test: 0.798719

Epoch: 130
Loss: 0.24151799082756042
RMSE train: 0.466992	val: 1.126519	test: 1.096765
MAE train: 0.327739	val: 0.898765	test: 0.834576

Epoch: 131
Loss: 0.2644850015640259
RMSE train: 0.408300	val: 0.968373	test: 1.071474
MAE train: 0.287498	val: 0.802783	test: 0.787915

Epoch: 132
Loss: 0.2382115162909031
RMSE train: 0.446121	val: 1.134855	test: 1.117124
MAE train: 0.324450	val: 0.909480	test: 0.853888

Epoch: 133
Loss: 0.27039142325520515
RMSE train: 0.396019	val: 1.036361	test: 1.075843
MAE train: 0.281941	val: 0.836644	test: 0.800668

Epoch: 134
Loss: 0.2623848244547844
RMSE train: 0.390266	val: 0.975793	test: 1.059545
MAE train: 0.279184	val: 0.800286	test: 0.782331

Epoch: 135
Loss: 0.29008958116173744
RMSE train: 0.409502	val: 1.155097	test: 1.112094
MAE train: 0.290976	val: 0.911588	test: 0.845889

Epoch: 136
Loss: 0.25104156881570816
RMSE train: 0.392533	val: 1.039349	test: 1.091767
MAE train: 0.276990	val: 0.848795	test: 0.820268

Epoch: 137
Loss: 0.24589066579937935
RMSE train: 0.403140	val: 1.101205	test: 1.142838
MAE train: 0.288714	val: 0.891813	test: 0.879614

Epoch: 138
Loss: 0.22810719162225723
RMSE train: 0.416558	val: 1.117641	test: 1.104820
MAE train: 0.299378	val: 0.901185	test: 0.840620

Epoch: 139
Loss: 0.2594917081296444
RMSE train: 0.385938	val: 1.048931	test: 1.083010
MAE train: 0.278182	val: 0.857198	test: 0.811312

Epoch: 140
Loss: 0.25192683190107346
RMSE train: 0.464200	val: 1.100193	test: 1.125175
MAE train: 0.331240	val: 0.895650	test: 0.854618

Epoch: 141
Loss: 0.23204616457223892
RMSE train: 0.388027	val: 1.115690	test: 1.089589
MAE train: 0.277802	val: 0.887603	test: 0.821096

Epoch: 142
Loss: 0.19380497559905052
RMSE train: 0.392675	val: 1.073554	test: 1.059655
MAE train: 0.273858	val: 0.857971	test: 0.795752

Epoch: 143
Loss: 0.2343253567814827
RMSE train: 0.429529	val: 1.105283	test: 1.080355
MAE train: 0.301102	val: 0.879891	test: 0.809256

Epoch: 144
Loss: 0.22427363321185112

Epoch: 84
Loss: 0.31083691865205765
RMSE train: 0.410740	val: 1.039431	test: 1.070470
MAE train: 0.303093	val: 0.846367	test: 0.802686

Epoch: 85
Loss: 0.2724448852241039
RMSE train: 0.474264	val: 1.120636	test: 1.119073
MAE train: 0.343402	val: 0.904230	test: 0.860330

Epoch: 86
Loss: 0.27375902235507965
RMSE train: 0.424980	val: 1.091006	test: 1.094851
MAE train: 0.308508	val: 0.885863	test: 0.828154

Epoch: 87
Loss: 0.30119694024324417
RMSE train: 0.423218	val: 1.080840	test: 1.074086
MAE train: 0.307123	val: 0.886925	test: 0.817090

Epoch: 88
Loss: 0.2813163213431835
RMSE train: 0.452634	val: 1.103680	test: 1.096057
MAE train: 0.328964	val: 0.894320	test: 0.845280

Epoch: 89
Loss: 0.2994290590286255
RMSE train: 0.399452	val: 1.041214	test: 1.067368
MAE train: 0.288615	val: 0.838327	test: 0.806694

Epoch: 90
Loss: 0.2930981516838074
RMSE train: 0.459839	val: 1.154514	test: 1.125301
MAE train: 0.332002	val: 0.932828	test: 0.869060

Epoch: 91
Loss: 0.28915489092469215
RMSE train: 0.469895	val: 1.162756	test: 1.143654
MAE train: 0.338114	val: 0.947595	test: 0.887128

Epoch: 92
Loss: 0.26244601234793663
RMSE train: 0.395561	val: 1.077604	test: 1.074122
MAE train: 0.287886	val: 0.866126	test: 0.820338

Epoch: 93
Loss: 0.2953713610768318
RMSE train: 0.440496	val: 1.105234	test: 1.099191
MAE train: 0.313064	val: 0.889046	test: 0.847626

Epoch: 94
Loss: 0.2899922952055931
RMSE train: 0.412220	val: 1.049964	test: 1.117466
MAE train: 0.300999	val: 0.857517	test: 0.833449

Epoch: 95
Loss: 0.294265553355217
RMSE train: 0.432202	val: 1.092399	test: 1.139389
MAE train: 0.316783	val: 0.898215	test: 0.864653

Epoch: 96
Loss: 0.27373991161584854
RMSE train: 0.483000	val: 1.101807	test: 1.158432
MAE train: 0.356980	val: 0.905293	test: 0.888836

Epoch: 97
Loss: 0.2709300145506859
RMSE train: 0.412300	val: 0.986346	test: 1.103803
MAE train: 0.301440	val: 0.811047	test: 0.826466

Epoch: 98
Loss: 0.33941003680229187
RMSE train: 0.446733	val: 1.090950	test: 1.134973
MAE train: 0.326372	val: 0.876999	test: 0.880117

Epoch: 99
Loss: 0.26629170030355453
RMSE train: 0.459726	val: 1.167146	test: 1.156909
MAE train: 0.335463	val: 0.924940	test: 0.906627

Epoch: 100
Loss: 0.3007172420620918
RMSE train: 0.391981	val: 1.046049	test: 1.097325
MAE train: 0.283503	val: 0.833131	test: 0.821691

Epoch: 101
Loss: 0.2500026635825634
RMSE train: 0.400689	val: 1.116044	test: 1.124524
MAE train: 0.293318	val: 0.891525	test: 0.852013

Epoch: 102
Loss: 0.3043500855565071
RMSE train: 0.380453	val: 1.083908	test: 1.108351
MAE train: 0.276654	val: 0.869527	test: 0.832116

Epoch: 103
Loss: 0.27771009877324104
RMSE train: 0.411294	val: 1.121458	test: 1.096103
MAE train: 0.304144	val: 0.898449	test: 0.829634

Epoch: 104
Loss: 0.25624237582087517
RMSE train: 0.478860	val: 1.154668	test: 1.135388
MAE train: 0.342172	val: 0.951109	test: 0.846880

Epoch: 105
Loss: 0.27482326328754425
RMSE train: 0.384148	val: 1.062566	test: 1.056141
MAE train: 0.275202	val: 0.852192	test: 0.794499

Epoch: 106
Loss: 0.26601284742355347
RMSE train: 0.425312	val: 1.009119	test: 1.063002
MAE train: 0.305873	val: 0.835942	test: 0.792926

Epoch: 107
Loss: 0.28327931836247444
RMSE train: 0.387576	val: 1.029930	test: 1.062794
MAE train: 0.287029	val: 0.848277	test: 0.774418

Epoch: 108
Loss: 0.30787543207407
RMSE train: 0.375295	val: 1.088950	test: 1.073597
MAE train: 0.282824	val: 0.871503	test: 0.797893

Epoch: 109
Loss: 0.2813543602824211
RMSE train: 0.440532	val: 1.122810	test: 1.113639
MAE train: 0.321303	val: 0.916169	test: 0.842391

Epoch: 110
Loss: 0.25872213393449783
RMSE train: 0.497903	val: 1.130132	test: 1.155374
MAE train: 0.354479	val: 0.934638	test: 0.876094

Epoch: 111
Loss: 0.29421376436948776
RMSE train: 0.395035	val: 1.128021	test: 1.116431
MAE train: 0.282525	val: 0.896753	test: 0.840542

Epoch: 112
Loss: 0.2789064943790436
RMSE train: 0.382030	val: 1.115368	test: 1.111810
MAE train: 0.278074	val: 0.883682	test: 0.848136

Epoch: 113
Loss: 0.2576882988214493
RMSE train: 0.425635	val: 1.113606	test: 1.108827
MAE train: 0.302104	val: 0.889386	test: 0.845894

Epoch: 114
Loss: 0.233839500695467
RMSE train: 0.432079	val: 1.087475	test: 1.082575
MAE train: 0.306714	val: 0.867220	test: 0.823575

Epoch: 115
Loss: 0.26090846955776215
RMSE train: 0.411325	val: 1.149017	test: 1.118405
MAE train: 0.298701	val: 0.902169	test: 0.849669

Epoch: 116
Loss: 0.2403348796069622
RMSE train: 0.417677	val: 1.105265	test: 1.123269
MAE train: 0.303858	val: 0.887015	test: 0.840045

Epoch: 117
Loss: 0.36170585453510284
RMSE train: 0.388669	val: 1.112669	test: 1.117513
MAE train: 0.287124	val: 0.869713	test: 0.836162

Epoch: 118
Loss: 0.22295714914798737
RMSE train: 0.461439	val: 1.095795	test: 1.152376
MAE train: 0.328808	val: 0.891787	test: 0.868128

Epoch: 119
Loss: 0.27451998367905617
RMSE train: 0.434745	val: 1.072020	test: 1.108922
MAE train: 0.309668	val: 0.867329	test: 0.842526

Epoch: 120
Loss: 0.2419002503156662
RMSE train: 0.363568	val: 1.029381	test: 1.086804
MAE train: 0.264648	val: 0.825780	test: 0.816080

Epoch: 121
Loss: 0.2925388887524605
RMSE train: 0.434156	val: 1.114014	test: 1.150711
MAE train: 0.314547	val: 0.902562	test: 0.879834

Epoch: 122
Loss: 0.241063691675663
RMSE train: 0.362137	val: 0.979003	test: 1.081845
MAE train: 0.266241	val: 0.811297	test: 0.813846

Epoch: 123
Loss: 0.22878533974289894
RMSE train: 0.358171	val: 1.060477	test: 1.102693
MAE train: 0.260490	val: 0.857870	test: 0.830299

Epoch: 124
Loss: 0.22485338523983955
RMSE train: 0.378749	val: 1.078740	test: 1.149418
MAE train: 0.277500	val: 0.868813	test: 0.874145

Epoch: 125
Loss: 0.21556277945637703
RMSE train: 0.387194	val: 1.055834	test: 1.120554
MAE train: 0.280174	val: 0.846081	test: 0.848142

Epoch: 126
Loss: 0.2655784823000431
RMSE train: 0.389161	val: 1.097095	test: 1.116485
MAE train: 0.279805	val: 0.867929	test: 0.853673

Epoch: 127
Loss: 0.24643125012516975
RMSE train: 0.413181	val: 1.092723	test: 1.129522
MAE train: 0.295591	val: 0.870255	test: 0.857423

Epoch: 128
Loss: 0.256601307541132
RMSE train: 0.351537	val: 1.089059	test: 1.098845
MAE train: 0.255489	val: 0.856782	test: 0.833099

Epoch: 129
Loss: 0.26444753259420395
RMSE train: 0.346041	val: 1.039439	test: 1.091264
MAE train: 0.252226	val: 0.828161	test: 0.815768

Epoch: 130
Loss: 0.2712015360593796
RMSE train: 0.411286	val: 1.069833	test: 1.114180
MAE train: 0.295312	val: 0.874336	test: 0.834081

Epoch: 131
Loss: 0.2486184760928154
RMSE train: 0.343935	val: 1.104996	test: 1.136933
MAE train: 0.261839	val: 0.874725	test: 0.856183

Epoch: 132
Loss: 0.26184386759996414
RMSE train: 0.348917	val: 1.108599	test: 1.116447
MAE train: 0.255973	val: 0.867975	test: 0.828868

Epoch: 133
Loss: 0.23142918199300766
RMSE train: 0.469809	val: 1.166276	test: 1.141677
MAE train: 0.337724	val: 0.930769	test: 0.872293

Epoch: 134
Loss: 0.2646898478269577
RMSE train: 0.362625	val: 0.996476	test: 1.078335
MAE train: 0.267912	val: 0.801125	test: 0.808022

Epoch: 135
Loss: 0.23056980967521667
RMSE train: 0.353740	val: 1.071232	test: 1.093563
MAE train: 0.259904	val: 0.848657	test: 0.817412

Epoch: 136
Loss: 0.23046975582838058
RMSE train: 0.387374	val: 1.141640	test: 1.146598
MAE train: 0.283034	val: 0.902083	test: 0.871096

Epoch: 137
Loss: 0.23502471297979355
RMSE train: 0.373179	val: 1.028424	test: 1.120046
MAE train: 0.277415	val: 0.836067	test: 0.831885

Epoch: 138
Loss: 0.2577027939260006
RMSE train: 0.420948	val: 1.151944	test: 1.155956
MAE train: 0.307322	val: 0.915181	test: 0.877967

Epoch: 139
Loss: 0.23170602694153786
RMSE train: 0.387088	val: 1.027218	test: 1.098239
MAE train: 0.280384	val: 0.840181	test: 0.817668

Epoch: 140
Loss: 0.23318789526820183
RMSE train: 0.403254	val: 1.047556	test: 1.105138
MAE train: 0.288602	val: 0.853908	test: 0.826891

Epoch: 141
Loss: 0.23153721913695335
RMSE train: 0.398477	val: 1.085168	test: 1.107928
MAE train: 0.283921	val: 0.869347	test: 0.831448

Epoch: 142
Loss: 0.2392340525984764
RMSE train: 0.430576	val: 1.110148	test: 1.126552
MAE train: 0.308597	val: 0.896215	test: 0.844778

Epoch: 143
Loss: 0.21756337583065033
RMSE train: 0.408655	val: 1.130262	test: 1.103585
MAE train: 0.297147	val: 0.896895	test: 0.833061

Epoch: 144
Loss: 0.26382169872522354
RMSE train: 0.464729	val: 1.020939	test: 1.192649
MAE train: 0.352150	val: 0.848271	test: 0.915731

Epoch: 84
Loss: 0.2817669063806534
RMSE train: 0.459736	val: 1.058507	test: 1.200743
MAE train: 0.351937	val: 0.876676	test: 0.930649

Epoch: 85
Loss: 0.2738569676876068
RMSE train: 0.397709	val: 0.990845	test: 1.158197
MAE train: 0.297217	val: 0.828751	test: 0.910858

Epoch: 86
Loss: 0.27490607649087906
RMSE train: 0.474839	val: 1.043368	test: 1.192125
MAE train: 0.358374	val: 0.873935	test: 0.901075

Epoch: 87
Loss: 0.27316687256097794
RMSE train: 0.502286	val: 1.100202	test: 1.224965
MAE train: 0.386525	val: 0.922739	test: 0.927955

Epoch: 88
Loss: 0.2490500621497631
RMSE train: 0.454923	val: 1.048177	test: 1.179178
MAE train: 0.347983	val: 0.884676	test: 0.905728

Epoch: 89
Loss: 0.27655431628227234
RMSE train: 0.524635	val: 1.083387	test: 1.205313
MAE train: 0.401913	val: 0.905814	test: 0.913103

Epoch: 90
Loss: 0.32692278176546097
RMSE train: 0.522543	val: 1.067301	test: 1.205812
MAE train: 0.401801	val: 0.886674	test: 0.913886

Epoch: 91
Loss: 0.2932514436542988
RMSE train: 0.488338	val: 1.037529	test: 1.192579
MAE train: 0.368464	val: 0.858632	test: 0.913197

Epoch: 92
Loss: 0.26362941414117813
RMSE train: 0.540050	val: 1.076709	test: 1.219174
MAE train: 0.410996	val: 0.890202	test: 0.934308

Epoch: 93
Loss: 0.25771645084023476
RMSE train: 0.461045	val: 0.985979	test: 1.162174
MAE train: 0.349021	val: 0.816311	test: 0.898011

Epoch: 94
Loss: 0.2790624350309372
RMSE train: 0.566741	val: 1.118365	test: 1.226288
MAE train: 0.435482	val: 0.914666	test: 0.937113

Epoch: 95
Loss: 0.2566569782793522
RMSE train: 0.560338	val: 1.137026	test: 1.238956
MAE train: 0.426094	val: 0.940663	test: 0.947405

Epoch: 96
Loss: 0.25728273019194603
RMSE train: 0.491481	val: 1.028449	test: 1.167617
MAE train: 0.367284	val: 0.855072	test: 0.885448

Epoch: 97
Loss: 0.3166268467903137
RMSE train: 0.468272	val: 1.050326	test: 1.192713
MAE train: 0.349982	val: 0.877626	test: 0.909253

Epoch: 98
Loss: 0.25021842867136
RMSE train: 0.451140	val: 1.034237	test: 1.177574
MAE train: 0.345368	val: 0.858609	test: 0.912190

Epoch: 99
Loss: 0.24370024353265762
RMSE train: 0.332988	val: 0.934713	test: 1.121824
MAE train: 0.249460	val: 0.770791	test: 0.904064

Epoch: 100
Loss: 0.2576424963772297
RMSE train: 0.463195	val: 1.040105	test: 1.180633
MAE train: 0.354509	val: 0.854219	test: 0.903968

Epoch: 101
Loss: 0.2828194499015808
RMSE train: 0.471246	val: 1.052540	test: 1.193376
MAE train: 0.350637	val: 0.886125	test: 0.919021

Epoch: 102
Loss: 0.2528548501431942
RMSE train: 0.498554	val: 1.047188	test: 1.171861
MAE train: 0.379069	val: 0.862549	test: 0.891489

Epoch: 103
Loss: 0.31287578493356705
RMSE train: 0.419157	val: 1.028169	test: 1.171978
MAE train: 0.303271	val: 0.850903	test: 0.900692

Epoch: 104
Loss: 0.2433597333729267
RMSE train: 0.468118	val: 1.076531	test: 1.191426
MAE train: 0.351427	val: 0.898112	test: 0.917919

Epoch: 105
Loss: 0.248598612844944
RMSE train: 0.390916	val: 1.042917	test: 1.145082
MAE train: 0.301428	val: 0.842738	test: 0.892850

Epoch: 106
Loss: 0.2656979076564312
RMSE train: 0.453120	val: 1.037989	test: 1.166262
MAE train: 0.345244	val: 0.869761	test: 0.896104

Epoch: 107
Loss: 0.2323586270213127
RMSE train: 0.555743	val: 1.155744	test: 1.250159
MAE train: 0.415128	val: 0.975440	test: 0.959324

Epoch: 108
Loss: 0.21751044690608978
RMSE train: 0.429404	val: 1.057254	test: 1.148368
MAE train: 0.323777	val: 0.867971	test: 0.890720

Epoch: 109
Loss: 0.225737065076828
RMSE train: 0.406986	val: 1.100695	test: 1.171347
MAE train: 0.315928	val: 0.891297	test: 0.902431

Epoch: 110
Loss: 0.233582254499197
RMSE train: 0.378776	val: 1.028879	test: 1.154280
MAE train: 0.287623	val: 0.863163	test: 0.894595

Epoch: 111
Loss: 0.2334129512310028
RMSE train: 0.441680	val: 1.057483	test: 1.158555
MAE train: 0.340977	val: 0.864896	test: 0.897413

Epoch: 112
Loss: 0.24166517332196236
RMSE train: 0.359266	val: 1.006980	test: 1.126803
MAE train: 0.270724	val: 0.829995	test: 0.887478

Epoch: 113
Loss: 0.23976870253682137
RMSE train: 0.355817	val: 1.002503	test: 1.122744
MAE train: 0.263170	val: 0.836622	test: 0.887768

Epoch: 114
Loss: 0.2341560274362564
RMSE train: 0.366526	val: 1.077804	test: 1.158504
MAE train: 0.275953	val: 0.870288	test: 0.922048

Epoch: 115
Loss: 0.20941035449504852
RMSE train: 0.342620	val: 1.003839	test: 1.177880
MAE train: 0.256718	val: 0.853105	test: 0.918364

Epoch: 116
Loss: 0.24065359309315681
RMSE train: 0.364554	val: 1.011942	test: 1.158544
MAE train: 0.266437	val: 0.848789	test: 0.900651

Epoch: 117
Loss: 0.2299380525946617
RMSE train: 0.321196	val: 1.003624	test: 1.153599
MAE train: 0.236010	val: 0.815515	test: 0.911534

Epoch: 118
Loss: 0.25206590443849564
RMSE train: 0.344317	val: 1.007576	test: 1.149095
MAE train: 0.257525	val: 0.830107	test: 0.887584

Epoch: 119
Loss: 0.20161336287856102
RMSE train: 0.446289	val: 1.129825	test: 1.203356
MAE train: 0.348537	val: 0.919037	test: 0.932486

Epoch: 120
Loss: 0.20300618559122086
RMSE train: 0.427181	val: 1.041588	test: 1.182741
MAE train: 0.323018	val: 0.871741	test: 0.920564

Epoch: 121
Loss: 0.2101583443582058
RMSE train: 0.411522	val: 1.027674	test: 1.170379
MAE train: 0.307483	val: 0.868993	test: 0.902102

Epoch: 122
Loss: 0.19552943482995033
RMSE train: 0.404507	val: 1.036160	test: 1.173557
MAE train: 0.309914	val: 0.864572	test: 0.889048

Epoch: 123
Loss: 0.22663090378046036
RMSE train: 0.421219	val: 1.026362	test: 1.216248
MAE train: 0.316832	val: 0.856450	test: 0.910507

Epoch: 124
Loss: 0.2347460277378559
RMSE train: 0.580909	val: 1.157104	test: 1.293606
MAE train: 0.438319	val: 0.966284	test: 0.974482

Epoch: 125
Loss: 0.22657570615410805
RMSE train: 0.399294	val: 1.040401	test: 1.198907
MAE train: 0.301977	val: 0.871258	test: 0.908287

Epoch: 126
Loss: 0.23604832217097282
RMSE train: 0.408809	val: 1.104152	test: 1.215853
MAE train: 0.321037	val: 0.898870	test: 0.920820

Epoch: 127
Loss: 0.273549921810627
RMSE train: 0.458361	val: 1.074469	test: 1.198796
MAE train: 0.345668	val: 0.883934	test: 0.904699

Epoch: 128
Loss: 0.185699712485075
RMSE train: 0.371676	val: 1.023248	test: 1.187017
MAE train: 0.256734	val: 0.870274	test: 0.915760

Epoch: 129
Loss: 0.1955743134021759
RMSE train: 0.380547	val: 1.077039	test: 1.206958
MAE train: 0.289159	val: 0.881064	test: 0.943502

Epoch: 130
Loss: 0.1791982725262642
RMSE train: 0.273264	val: 1.027202	test: 1.218785
MAE train: 0.210653	val: 0.838429	test: 0.985836

Epoch: 131
Loss: 0.19187524542212486
RMSE train: 0.367928	val: 1.035406	test: 1.205806
MAE train: 0.270423	val: 0.859032	test: 0.950299

Epoch: 132
Loss: 0.18617935478687286
RMSE train: 0.420125	val: 1.070725	test: 1.214047
MAE train: 0.309691	val: 0.877477	test: 0.945624

Epoch: 133
Loss: 0.17663684859871864
RMSE train: 0.356756	val: 0.995803	test: 1.173545
MAE train: 0.257155	val: 0.840899	test: 0.906549

Epoch: 134
Loss: 0.20663943886756897
RMSE train: 0.380419	val: 1.064100	test: 1.206049
MAE train: 0.291323	val: 0.870094	test: 0.926543

Early stopping
Best (RMSE):	 train: 0.332988	val: 0.934713	test: 1.121824
Best (MAE):	 train: 0.249460	val: 0.770791	test: 0.904064

RMSE train: 0.398698	val: 1.211971	test: 1.336995
MAE train: 0.305541	val: 0.987665	test: 1.050828

Epoch: 84
Loss: 0.3264339193701744
RMSE train: 0.339423	val: 1.098932	test: 1.223881
MAE train: 0.257697	val: 0.901899	test: 0.974599

Epoch: 85
Loss: 0.3372609466314316
RMSE train: 0.458597	val: 1.124568	test: 1.218612
MAE train: 0.343433	val: 0.929159	test: 0.970976

Epoch: 86
Loss: 0.3273615837097168
RMSE train: 0.513606	val: 1.234756	test: 1.339234
MAE train: 0.383137	val: 0.985375	test: 1.060069

Epoch: 87
Loss: 0.31468380987644196
RMSE train: 0.375666	val: 1.155766	test: 1.308536
MAE train: 0.288149	val: 0.946764	test: 1.021625

Epoch: 88
Loss: 0.3179791048169136
RMSE train: 0.561119	val: 1.174130	test: 1.288599
MAE train: 0.413499	val: 0.941472	test: 1.007027

Epoch: 89
Loss: 0.2900509312748909
RMSE train: 0.452052	val: 1.103246	test: 1.232542
MAE train: 0.329138	val: 0.878533	test: 0.967313

Epoch: 90
Loss: 0.2811398208141327
RMSE train: 0.407849	val: 1.176431	test: 1.316168
MAE train: 0.307815	val: 0.929414	test: 1.042377

Epoch: 91
Loss: 0.2880803346633911
RMSE train: 0.527269	val: 1.216922	test: 1.322144
MAE train: 0.401183	val: 0.960505	test: 1.036699

Epoch: 92
Loss: 0.2743474170565605
RMSE train: 0.433717	val: 1.149998	test: 1.265220
MAE train: 0.333630	val: 0.930569	test: 0.994314

Epoch: 93
Loss: 0.347690112888813
RMSE train: 0.421985	val: 1.229719	test: 1.331569
MAE train: 0.307144	val: 0.984101	test: 1.047148

Epoch: 94
Loss: 0.2708890251815319
RMSE train: 0.363540	val: 1.149163	test: 1.268880
MAE train: 0.274852	val: 0.931159	test: 0.996346

Epoch: 95
Loss: 0.294454462826252
RMSE train: 0.398877	val: 1.152708	test: 1.258812
MAE train: 0.300797	val: 0.944805	test: 0.992863

Epoch: 96
Loss: 0.28605836629867554
RMSE train: 0.360285	val: 1.178590	test: 1.329683
MAE train: 0.273938	val: 0.956504	test: 1.050586

Epoch: 97
Loss: 0.29692306742072105
RMSE train: 0.358088	val: 1.135688	test: 1.298875
MAE train: 0.268869	val: 0.923848	test: 1.035130

Epoch: 98
Loss: 0.32559407129883766
RMSE train: 0.389846	val: 1.155438	test: 1.307079
MAE train: 0.293676	val: 0.941842	test: 1.030242

Epoch: 99
Loss: 0.26276515051722527
RMSE train: 0.336859	val: 1.093139	test: 1.277138
MAE train: 0.254106	val: 0.904406	test: 0.991422

Epoch: 100
Loss: 0.2906913086771965
RMSE train: 0.369845	val: 1.176503	test: 1.351193
MAE train: 0.276344	val: 0.942810	test: 1.034103

Epoch: 101
Loss: 0.29012398421764374
RMSE train: 0.412534	val: 1.121422	test: 1.243934
MAE train: 0.314535	val: 0.912854	test: 0.974889

Epoch: 102
Loss: 0.2693874426186085
RMSE train: 0.386692	val: 1.055916	test: 1.195009
MAE train: 0.289140	val: 0.868448	test: 0.934547

Epoch: 103
Loss: 0.270848598331213
RMSE train: 0.448117	val: 1.082366	test: 1.195110
MAE train: 0.333900	val: 0.888191	test: 0.937283

Epoch: 104
Loss: 0.2740742154419422
RMSE train: 0.436824	val: 1.145385	test: 1.244425
MAE train: 0.334244	val: 0.926125	test: 0.985943

Epoch: 105
Loss: 0.23746568709611893
RMSE train: 0.334643	val: 1.076002	test: 1.221971
MAE train: 0.254856	val: 0.880932	test: 0.961719

Epoch: 106
Loss: 0.25724751502275467
RMSE train: 0.373472	val: 1.092901	test: 1.226321
MAE train: 0.279893	val: 0.877557	test: 0.966842

Epoch: 107
Loss: 0.26823437213897705
RMSE train: 0.460107	val: 1.139383	test: 1.255924
MAE train: 0.334972	val: 0.910456	test: 0.981622

Epoch: 108
Loss: 0.2650480605661869
RMSE train: 0.398425	val: 1.165828	test: 1.324288
MAE train: 0.296262	val: 0.933412	test: 1.036796

Epoch: 109
Loss: 0.240667887032032
RMSE train: 0.345219	val: 1.148137	test: 1.312176
MAE train: 0.260478	val: 0.929011	test: 1.034978

Epoch: 110
Loss: 0.27812908217310905
RMSE train: 0.337064	val: 1.088679	test: 1.250597
MAE train: 0.250701	val: 0.892812	test: 0.991534

Epoch: 111
Loss: 0.25415772572159767
RMSE train: 0.326234	val: 1.179306	test: 1.347723
MAE train: 0.249346	val: 0.948438	test: 1.055686

Epoch: 112
Loss: 0.25376465171575546
RMSE train: 0.280913	val: 1.101363	test: 1.285289
MAE train: 0.219715	val: 0.898878	test: 0.995881

Epoch: 113
Loss: 0.24914366751909256
RMSE train: 0.406676	val: 1.187313	test: 1.291076
MAE train: 0.291262	val: 0.950882	test: 1.010859

Epoch: 114
Loss: 0.21781658753752708
RMSE train: 0.346502	val: 1.089363	test: 1.209228
MAE train: 0.257139	val: 0.889669	test: 0.947279

Epoch: 115
Loss: 0.261525746434927
RMSE train: 0.382236	val: 1.182452	test: 1.289056
MAE train: 0.277107	val: 0.939740	test: 1.005250

Epoch: 116
Loss: 0.21482901647686958
RMSE train: 0.359269	val: 1.143515	test: 1.220062
MAE train: 0.267851	val: 0.923633	test: 0.970879

Epoch: 117
Loss: 0.22343885526061058
RMSE train: 0.304686	val: 1.101001	test: 1.189249
MAE train: 0.233617	val: 0.892289	test: 0.937940

Epoch: 118
Loss: 0.25713737681508064
RMSE train: 0.370852	val: 1.178801	test: 1.235493
MAE train: 0.279173	val: 0.947904	test: 0.971374

Epoch: 119
Loss: 0.23522072285413742
RMSE train: 0.399594	val: 1.169086	test: 1.244303
MAE train: 0.296853	val: 0.946825	test: 0.970780

Epoch: 120
Loss: 0.2295687049627304
RMSE train: 0.379892	val: 1.158421	test: 1.250672
MAE train: 0.289616	val: 0.932848	test: 0.978145

Epoch: 121
Loss: 0.27264072000980377
RMSE train: 0.440312	val: 1.194294	test: 1.295326
MAE train: 0.332458	val: 0.968211	test: 1.011588

Epoch: 122
Loss: 0.23194723948836327
RMSE train: 0.409123	val: 1.148765	test: 1.259745
MAE train: 0.308114	val: 0.936962	test: 0.986329

Epoch: 123
Loss: 0.20294931903481483
RMSE train: 0.410561	val: 1.217123	test: 1.292182
MAE train: 0.311995	val: 0.971725	test: 1.023157

Epoch: 124
Loss: 0.23919029161334038
RMSE train: 0.464852	val: 1.217993	test: 1.287838
MAE train: 0.351567	val: 0.978498	test: 1.028500

Epoch: 125
Loss: 0.2501522898674011
RMSE train: 0.358999	val: 1.143578	test: 1.261968
MAE train: 0.267311	val: 0.925667	test: 1.004758

Epoch: 126
Loss: 0.2308153584599495
RMSE train: 0.336301	val: 1.104248	test: 1.213779
MAE train: 0.254068	val: 0.895081	test: 0.946774

Epoch: 127
Loss: 0.24548153951764107
RMSE train: 0.413373	val: 1.203843	test: 1.290390
MAE train: 0.308152	val: 0.956636	test: 1.011628

Epoch: 128
Loss: 0.22864849120378494
RMSE train: 0.298042	val: 1.080019	test: 1.193069
MAE train: 0.225492	val: 0.875604	test: 0.931015

Epoch: 129
Loss: 0.22441274300217628
RMSE train: 0.347752	val: 1.146349	test: 1.239870
MAE train: 0.265969	val: 0.919084	test: 0.977562

Epoch: 130
Loss: 0.25520480796694756
RMSE train: 0.344520	val: 1.163196	test: 1.273106
MAE train: 0.261466	val: 0.934622	test: 0.998526

Epoch: 131
Loss: 0.22208786755800247
RMSE train: 0.289592	val: 1.073291	test: 1.217273
MAE train: 0.217842	val: 0.869839	test: 0.973311

Epoch: 132
Loss: 0.18092555925250053
RMSE train: 0.366297	val: 1.111542	test: 1.231212
MAE train: 0.271251	val: 0.907837	test: 0.977618

Epoch: 133
Loss: 0.2193417027592659
RMSE train: 0.325559	val: 1.096047	test: 1.224530
MAE train: 0.244656	val: 0.890357	test: 0.969800

Epoch: 134
Loss: 0.2180097997188568
RMSE train: 0.425343	val: 1.201323	test: 1.323314
MAE train: 0.314855	val: 0.951527	test: 1.050155

Epoch: 135
Loss: 0.21475108340382576
RMSE train: 0.373691	val: 1.074509	test: 1.217179
MAE train: 0.280028	val: 0.877848	test: 0.963850

Epoch: 136
Loss: 0.22566035389900208
RMSE train: 0.382072	val: 1.142413	test: 1.280250
MAE train: 0.291045	val: 0.912956	test: 1.002393

Epoch: 137
Loss: 0.20763354003429413
RMSE train: 0.382586	val: 1.169458	test: 1.296835
MAE train: 0.289256	val: 0.933794	test: 1.024140

Early stopping
Best (RMSE):	 train: 0.386692	val: 1.055916	test: 1.195009
Best (MAE):	 train: 0.289140	val: 0.868448	test: 0.934547

RMSE train: 0.515719	val: 1.111757	test: 1.160207
MAE train: 0.385568	val: 0.908595	test: 0.912525

Epoch: 84
Loss: 0.31631217151880264
RMSE train: 0.416795	val: 1.036098	test: 1.085481
MAE train: 0.320785	val: 0.859600	test: 0.847135

Epoch: 85
Loss: 0.3424610197544098
RMSE train: 0.450991	val: 1.027145	test: 1.106759
MAE train: 0.342468	val: 0.867981	test: 0.842832

Epoch: 86
Loss: 0.31055784225463867
RMSE train: 0.523739	val: 1.127759	test: 1.188236
MAE train: 0.393307	val: 0.921985	test: 0.923265

Epoch: 87
Loss: 0.2981594651937485
RMSE train: 0.366647	val: 1.094993	test: 1.169075
MAE train: 0.285822	val: 0.909449	test: 0.916566

Epoch: 88
Loss: 0.29822875559329987
RMSE train: 0.482487	val: 1.029744	test: 1.124131
MAE train: 0.354458	val: 0.843110	test: 0.854350

Epoch: 89
Loss: 0.2707864046096802
RMSE train: 0.422297	val: 0.986906	test: 1.102570
MAE train: 0.312995	val: 0.815880	test: 0.831663

Epoch: 90
Loss: 0.24828583002090454
RMSE train: 0.353995	val: 1.004222	test: 1.102552
MAE train: 0.268173	val: 0.812913	test: 0.849974

Epoch: 91
Loss: 0.27442440390586853
RMSE train: 0.523085	val: 1.128731	test: 1.219814
MAE train: 0.401782	val: 0.907715	test: 0.936390

Epoch: 92
Loss: 0.2773354947566986
RMSE train: 0.462449	val: 1.120634	test: 1.191356
MAE train: 0.359097	val: 0.905428	test: 0.924538

Epoch: 93
Loss: 0.3387378752231598
RMSE train: 0.446972	val: 1.108687	test: 1.179140
MAE train: 0.327912	val: 0.895798	test: 0.933699

Epoch: 94
Loss: 0.27084968611598015
RMSE train: 0.455806	val: 1.110964	test: 1.188948
MAE train: 0.335827	val: 0.907338	test: 0.915461

Epoch: 95
Loss: 0.2947973161935806
RMSE train: 0.442155	val: 1.103570	test: 1.201636
MAE train: 0.328196	val: 0.907721	test: 0.923887

Epoch: 96
Loss: 0.2729741930961609
RMSE train: 0.366803	val: 1.139551	test: 1.262550
MAE train: 0.277320	val: 0.909462	test: 0.992211

Epoch: 97
Loss: 0.2803320400416851
RMSE train: 0.347699	val: 1.080965	test: 1.195478
MAE train: 0.267250	val: 0.886191	test: 0.935654

Epoch: 98
Loss: 0.2996067591011524
RMSE train: 0.399120	val: 1.033597	test: 1.149044
MAE train: 0.300664	val: 0.881094	test: 0.882287

Epoch: 99
Loss: 0.24609170854091644
RMSE train: 0.397593	val: 1.027251	test: 1.148175
MAE train: 0.297251	val: 0.862457	test: 0.886893

Epoch: 100
Loss: 0.26204754039645195
RMSE train: 0.392878	val: 1.080319	test: 1.187625
MAE train: 0.295206	val: 0.870751	test: 0.925954

Epoch: 101
Loss: 0.29497621208429337
RMSE train: 0.452543	val: 1.079257	test: 1.185600
MAE train: 0.347044	val: 0.878345	test: 0.914609

Epoch: 102
Loss: 0.25576861947774887
RMSE train: 0.401778	val: 1.028817	test: 1.150348
MAE train: 0.296676	val: 0.836283	test: 0.907882

Epoch: 103
Loss: 0.2690277136862278
RMSE train: 0.387974	val: 1.016307	test: 1.134707
MAE train: 0.285648	val: 0.836008	test: 0.859502

Epoch: 104
Loss: 0.2492937222123146
RMSE train: 0.398232	val: 1.057869	test: 1.144894
MAE train: 0.298942	val: 0.867156	test: 0.879380

Epoch: 105
Loss: 0.22651755809783936
RMSE train: 0.363665	val: 1.066155	test: 1.152698
MAE train: 0.269106	val: 0.867809	test: 0.902034

Epoch: 106
Loss: 0.24225594848394394
RMSE train: 0.363037	val: 1.010947	test: 1.140205
MAE train: 0.267405	val: 0.830152	test: 0.886016

Epoch: 107
Loss: 0.24262376874685287
RMSE train: 0.459240	val: 1.057069	test: 1.180634
MAE train: 0.337628	val: 0.878060	test: 0.906265

Epoch: 108
Loss: 0.25360875576734543
RMSE train: 0.395883	val: 1.070169	test: 1.209115
MAE train: 0.291985	val: 0.869952	test: 0.945151

Epoch: 109
Loss: 0.24388012290000916
RMSE train: 0.379488	val: 1.007257	test: 1.142512
MAE train: 0.278424	val: 0.836455	test: 0.880433

Epoch: 110
Loss: 0.25856780633330345
RMSE train: 0.424570	val: 1.009224	test: 1.140620
MAE train: 0.307468	val: 0.820964	test: 0.854395

Epoch: 111
Loss: 0.24776946008205414
RMSE train: 0.405554	val: 1.048132	test: 1.141963
MAE train: 0.307160	val: 0.839186	test: 0.894261

Epoch: 112
Loss: 0.25869492068886757
RMSE train: 0.328712	val: 0.977583	test: 1.091209
MAE train: 0.247602	val: 0.813429	test: 0.820183

Epoch: 113
Loss: 0.23801109567284584
RMSE train: 0.547775	val: 1.103523	test: 1.176862
MAE train: 0.397754	val: 0.873395	test: 0.905558

Epoch: 114
Loss: 0.22503744065761566
RMSE train: 0.397098	val: 1.012534	test: 1.112901
MAE train: 0.289482	val: 0.840384	test: 0.844551

Epoch: 115
Loss: 0.24986735731363297
RMSE train: 0.407405	val: 1.050390	test: 1.139538
MAE train: 0.301235	val: 0.862558	test: 0.888650

Epoch: 116
Loss: 0.21478629112243652
RMSE train: 0.362846	val: 1.069649	test: 1.159569
MAE train: 0.275058	val: 0.879955	test: 0.887156

Epoch: 117
Loss: 0.22701697796583176
RMSE train: 0.342861	val: 1.064927	test: 1.148705
MAE train: 0.255678	val: 0.879376	test: 0.877241

Epoch: 118
Loss: 0.25447307527065277
RMSE train: 0.397782	val: 1.069055	test: 1.144706
MAE train: 0.296723	val: 0.882562	test: 0.883760

Epoch: 119
Loss: 0.2354924939572811
RMSE train: 0.371703	val: 1.019536	test: 1.134917
MAE train: 0.271665	val: 0.859414	test: 0.859294

Epoch: 120
Loss: 0.2102500945329666
RMSE train: 0.359824	val: 1.025732	test: 1.137904
MAE train: 0.270283	val: 0.858349	test: 0.872678

Epoch: 121
Loss: 0.2634609527885914
RMSE train: 0.367447	val: 1.025590	test: 1.137704
MAE train: 0.282161	val: 0.860090	test: 0.877390

Epoch: 122
Loss: 0.2202206514775753
RMSE train: 0.415458	val: 1.039169	test: 1.137563
MAE train: 0.319747	val: 0.867940	test: 0.868467

Epoch: 123
Loss: 0.21080242469906807
RMSE train: 0.416866	val: 1.061577	test: 1.145557
MAE train: 0.315483	val: 0.883690	test: 0.841330

Epoch: 124
Loss: 0.22222699970006943
RMSE train: 0.429226	val: 1.058670	test: 1.140332
MAE train: 0.321656	val: 0.869974	test: 0.852289

Epoch: 125
Loss: 0.2540379874408245
RMSE train: 0.379698	val: 1.048079	test: 1.136547
MAE train: 0.282232	val: 0.844126	test: 0.891560

Epoch: 126
Loss: 0.2181035466492176
RMSE train: 0.363547	val: 1.027381	test: 1.144235
MAE train: 0.263263	val: 0.860987	test: 0.849208

Epoch: 127
Loss: 0.21724780276417732
RMSE train: 0.384550	val: 1.017941	test: 1.111479
MAE train: 0.280759	val: 0.849855	test: 0.861306

Epoch: 128
Loss: 0.22421368211507797
RMSE train: 0.336385	val: 1.065510	test: 1.181922
MAE train: 0.259812	val: 0.899390	test: 0.902762

Epoch: 129
Loss: 0.23038994893431664
RMSE train: 0.354270	val: 1.023076	test: 1.113332
MAE train: 0.269738	val: 0.847317	test: 0.851582

Epoch: 130
Loss: 0.23098190128803253
RMSE train: 0.319446	val: 1.016811	test: 1.096752
MAE train: 0.240407	val: 0.828052	test: 0.861324

Epoch: 131
Loss: 0.23106799274683
RMSE train: 0.278414	val: 0.997220	test: 1.087911
MAE train: 0.212531	val: 0.812865	test: 0.842540

Epoch: 132
Loss: 0.2059924155473709
RMSE train: 0.440442	val: 1.052106	test: 1.134288
MAE train: 0.325336	val: 0.864864	test: 0.865380

Epoch: 133
Loss: 0.2393617331981659
RMSE train: 0.315019	val: 0.987315	test: 1.099398
MAE train: 0.235623	val: 0.821216	test: 0.851888

Epoch: 134
Loss: 0.21817245334386826
RMSE train: 0.400046	val: 1.090784	test: 1.184545
MAE train: 0.296471	val: 0.881467	test: 0.941478

Epoch: 135
Loss: 0.22199733927845955
RMSE train: 0.329459	val: 0.977734	test: 1.125370
MAE train: 0.248147	val: 0.801701	test: 0.863647

Epoch: 136
Loss: 0.23747623711824417
RMSE train: 0.372688	val: 0.990033	test: 1.130971
MAE train: 0.268101	val: 0.816320	test: 0.880127

Epoch: 137
Loss: 0.2119566649198532
RMSE train: 0.386931	val: 1.071125	test: 1.154590
MAE train: 0.287725	val: 0.866566	test: 0.907870

Epoch: 138
Loss: 0.2151830792427063
RMSE train: 0.324396	val: 1.068566	test: 1.181789
MAE train: 0.251191	val: 0.877094	test: 0.905195

Epoch: 139
Loss: 0.2131757289171219
RMSE train: 0.313398	val: 1.010338	test: 1.129807
MAE train: 0.234957	val: 0.832395	test: 0.889634

Epoch: 140
Loss: 0.21666499972343445
RMSE train: 0.321663	val: 0.998648	test: 1.121621
MAE train: 0.243592	val: 0.828309	test: 0.871641

Epoch: 141
Loss: 0.19982639700174332
RMSE train: 0.284051	val: 1.051061	test: 1.120973
MAE train: 0.219453	val: 0.856619	test: 0.898464

Epoch: 142
Loss: 0.21202905848622322
RMSE train: 0.314793	val: 1.001908	test: 1.096587
MAE train: 0.239144	val: 0.822859	test: 0.866237

Epoch: 143
Loss: 0.21352149546146393
RMSE train: 0.385333	val: 1.034086	test: 1.144749
RMSE train: 0.570076	val: 1.546729	test: 1.588308
MAE train: 0.451318	val: 1.280406	test: 1.261189

Epoch: 84
Loss: 0.32018037885427475
RMSE train: 0.517935	val: 1.351009	test: 1.413625
MAE train: 0.400045	val: 1.129499	test: 1.109087

Epoch: 85
Loss: 0.3601459786295891
RMSE train: 0.424809	val: 1.283546	test: 1.367798
MAE train: 0.328921	val: 1.077300	test: 1.069648

Epoch: 86
Loss: 0.3514881059527397
RMSE train: 0.391372	val: 1.282612	test: 1.357607
MAE train: 0.301635	val: 1.066861	test: 1.061055

Epoch: 87
Loss: 0.2767470180988312
RMSE train: 0.352302	val: 1.154759	test: 1.254570
MAE train: 0.277005	val: 0.947461	test: 0.998685

Epoch: 88
Loss: 0.2989976927638054
RMSE train: 0.487824	val: 1.439519	test: 1.498915
MAE train: 0.379415	val: 1.158381	test: 1.158411

Epoch: 89
Loss: 0.3206466734409332
RMSE train: 0.364333	val: 1.187288	test: 1.314123
MAE train: 0.281512	val: 0.981770	test: 1.010920

Epoch: 90
Loss: 0.2810099497437477
RMSE train: 0.319066	val: 1.217741	test: 1.311989
MAE train: 0.245231	val: 0.999591	test: 1.022378

Epoch: 91
Loss: 0.2458721101284027
RMSE train: 0.371670	val: 1.280497	test: 1.325973
MAE train: 0.292233	val: 1.037629	test: 1.052441

Epoch: 92
Loss: 0.31331995129585266
RMSE train: 0.395577	val: 1.211453	test: 1.309915
MAE train: 0.300274	val: 1.001722	test: 1.016377

Epoch: 93
Loss: 0.2744864337146282
RMSE train: 0.506231	val: 1.453285	test: 1.519566
MAE train: 0.391034	val: 1.185884	test: 1.144785

Epoch: 94
Loss: 0.3021535277366638
RMSE train: 0.482568	val: 1.357234	test: 1.419128
MAE train: 0.373517	val: 1.108024	test: 1.071671

Epoch: 95
Loss: 0.30673662573099136
RMSE train: 0.542130	val: 1.362164	test: 1.430664
MAE train: 0.420844	val: 1.116791	test: 1.078074

Epoch: 96
Loss: 0.32300473749637604
RMSE train: 0.631906	val: 1.548688	test: 1.608913
MAE train: 0.478800	val: 1.262845	test: 1.228309

Epoch: 97
Loss: 0.28907765448093414
RMSE train: 0.482846	val: 1.357932	test: 1.443729
MAE train: 0.380070	val: 1.120421	test: 1.095388

Epoch: 98
Loss: 0.23798846825957298
RMSE train: 0.593824	val: 1.451554	test: 1.502997
MAE train: 0.452478	val: 1.175688	test: 1.135502

Epoch: 99
Loss: 0.335765790194273
RMSE train: 0.372401	val: 1.201173	test: 1.280502
MAE train: 0.289065	val: 0.982916	test: 1.010466

Epoch: 100
Loss: 0.2903987094759941
RMSE train: 0.296253	val: 1.220665	test: 1.298929
MAE train: 0.232169	val: 0.974594	test: 1.028129

Epoch: 101
Loss: 0.31152062118053436
RMSE train: 0.556185	val: 1.423350	test: 1.490638
MAE train: 0.410942	val: 1.176094	test: 1.146212

Epoch: 102
Loss: 0.2571128234267235
RMSE train: 0.403016	val: 1.201074	test: 1.299783
MAE train: 0.304049	val: 0.981732	test: 1.011042

Epoch: 103
Loss: 0.2539985552430153
RMSE train: 0.461678	val: 1.406572	test: 1.462368
MAE train: 0.344473	val: 1.122637	test: 1.114431

Epoch: 104
Loss: 0.24864761903882027
RMSE train: 0.467748	val: 1.414133	test: 1.473655
MAE train: 0.365497	val: 1.153393	test: 1.110420

Epoch: 105
Loss: 0.26271242275834084
RMSE train: 0.431904	val: 1.310510	test: 1.390915
MAE train: 0.337717	val: 1.096268	test: 1.064526

Epoch: 106
Loss: 0.22681738436222076
RMSE train: 0.467832	val: 1.365272	test: 1.419701
MAE train: 0.347586	val: 1.114791	test: 1.090272

Epoch: 107
Loss: 0.2515888027846813
RMSE train: 0.352666	val: 1.232797	test: 1.288468
MAE train: 0.272429	val: 1.008940	test: 1.020176

Epoch: 108
Loss: 0.22491803765296936
RMSE train: 0.318764	val: 1.206344	test: 1.280932
MAE train: 0.243619	val: 0.989675	test: 1.009657

Epoch: 109
Loss: 0.26858286187052727
RMSE train: 0.376565	val: 1.423350	test: 1.442662
MAE train: 0.280786	val: 1.139888	test: 1.114846

Epoch: 110
Loss: 0.2620803378522396
RMSE train: 0.329709	val: 1.279961	test: 1.334278
MAE train: 0.261719	val: 1.047555	test: 1.038657

Epoch: 111
Loss: 0.22245750576257706
RMSE train: 0.389737	val: 1.335277	test: 1.400857
MAE train: 0.308694	val: 1.098127	test: 1.062298

Epoch: 112
Loss: 0.2124170996248722
RMSE train: 0.380428	val: 1.297107	test: 1.365870
MAE train: 0.305124	val: 1.055136	test: 1.066794

Epoch: 113
Loss: 0.24779920279979706
RMSE train: 0.380401	val: 1.291736	test: 1.362487
MAE train: 0.307275	val: 1.046450	test: 1.087013

Epoch: 114
Loss: 0.2595214955508709
RMSE train: 0.409231	val: 1.392525	test: 1.426796
MAE train: 0.305305	val: 1.114483	test: 1.138401

Epoch: 115
Loss: 0.23432554304599762
RMSE train: 0.284580	val: 1.180274	test: 1.248934
MAE train: 0.218514	val: 0.962384	test: 0.991493

Epoch: 116
Loss: 0.26964984834194183
RMSE train: 0.408417	val: 1.313294	test: 1.355795
MAE train: 0.313023	val: 1.075199	test: 1.052788

Epoch: 117
Loss: 0.25315655022859573
RMSE train: 0.488556	val: 1.391570	test: 1.405419
MAE train: 0.360883	val: 1.112684	test: 1.096367

Epoch: 118
Loss: 0.23658407479524612
RMSE train: 0.303582	val: 1.089375	test: 1.171866
MAE train: 0.240696	val: 0.892274	test: 0.936352

Epoch: 119
Loss: 0.22519145533442497
RMSE train: 0.330775	val: 1.365822	test: 1.419187
MAE train: 0.252293	val: 1.097897	test: 1.078526

Epoch: 120
Loss: 0.25984663143754005
RMSE train: 0.359824	val: 1.326394	test: 1.338053
MAE train: 0.268280	val: 1.055007	test: 1.054137

Epoch: 121
Loss: 0.23037117719650269
RMSE train: 0.339727	val: 1.240952	test: 1.260056
MAE train: 0.266945	val: 0.993609	test: 1.011814

Epoch: 122
Loss: 0.2685302644968033
RMSE train: 0.374612	val: 1.337713	test: 1.391154
MAE train: 0.290913	val: 1.098132	test: 1.055814

Epoch: 123
Loss: 0.27609533816576004
RMSE train: 0.480072	val: 1.462716	test: 1.522839
MAE train: 0.354022	val: 1.190240	test: 1.147333

Epoch: 124
Loss: 0.24365856125950813
RMSE train: 0.336058	val: 1.269113	test: 1.332504
MAE train: 0.272424	val: 1.034235	test: 1.022475

Epoch: 125
Loss: 0.23554028943181038
RMSE train: 0.448710	val: 1.413042	test: 1.450677
MAE train: 0.347505	val: 1.141433	test: 1.105214

Epoch: 126
Loss: 0.22089172154664993
RMSE train: 0.411052	val: 1.355362	test: 1.423463
MAE train: 0.312096	val: 1.092775	test: 1.067222

Epoch: 127
Loss: 0.21600046381354332
RMSE train: 0.366699	val: 1.297723	test: 1.381837
MAE train: 0.281954	val: 1.063235	test: 1.038218

Epoch: 128
Loss: 0.19113044068217278
RMSE train: 0.375747	val: 1.382593	test: 1.451426
MAE train: 0.295270	val: 1.124771	test: 1.081797

Epoch: 129
Loss: 0.194324042648077
RMSE train: 0.435748	val: 1.379015	test: 1.432146
MAE train: 0.343211	val: 1.119486	test: 1.082456

Epoch: 130
Loss: 0.22305436804890633
RMSE train: 0.395335	val: 1.249971	test: 1.304882
MAE train: 0.296716	val: 1.007846	test: 1.019138

Epoch: 131
Loss: 0.20282632857561111
RMSE train: 0.337165	val: 1.253202	test: 1.309529
MAE train: 0.256143	val: 1.024014	test: 1.008829

Epoch: 132
Loss: 0.21650390326976776
RMSE train: 0.315427	val: 1.247807	test: 1.322996
MAE train: 0.245395	val: 1.021326	test: 1.006266

Epoch: 133
Loss: 0.21272090077400208
RMSE train: 0.451691	val: 1.380870	test: 1.427890
MAE train: 0.339515	val: 1.110537	test: 1.080566

Epoch: 134
Loss: 0.18549202755093575
RMSE train: 0.360946	val: 1.225155	test: 1.283684
MAE train: 0.274274	val: 0.980219	test: 0.998761

Epoch: 135
Loss: 0.2056574523448944
RMSE train: 0.274987	val: 1.267280	test: 1.296610
MAE train: 0.218007	val: 1.014591	test: 1.021750

Epoch: 136
Loss: 0.21175190061330795
RMSE train: 0.349375	val: 1.361332	test: 1.369338
MAE train: 0.264511	val: 1.084207	test: 1.065971

Epoch: 137
Loss: 0.21204420551657677
RMSE train: 0.322527	val: 1.256101	test: 1.301823
MAE train: 0.248958	val: 1.006729	test: 1.009211

Epoch: 138
Loss: 0.21423903480172157
RMSE train: 0.336383	val: 1.336675	test: 1.343553
MAE train: 0.267118	val: 1.066187	test: 1.044189

Epoch: 139
Loss: 0.18197765946388245
RMSE train: 0.360020	val: 1.354868	test: 1.356634
MAE train: 0.270423	val: 1.079260	test: 1.048183

Epoch: 140
Loss: 0.2119796797633171
RMSE train: 0.326631	val: 1.240748	test: 1.299446
MAE train: 0.253345	val: 1.010262	test: 0.990683

Epoch: 141
Loss: 0.19514276459813118
RMSE train: 0.339468	val: 1.304439	test: 1.349590
MAE train: 0.255794	val: 1.084690	test: 1.028733

Epoch: 142
Loss: 0.18622973933815956
RMSE train: 0.303237	val: 1.242602	test: 1.264896
MAE train: 0.227315	val: 0.997040	test: 0.998253

Epoch: 143
Loss: 0.18442368134856224
RMSE train: 0.272837	val: 1.133206	test: 1.207215
RMSE train: 0.567067	val: 1.314950	test: 1.367912
MAE train: 0.431691	val: 1.068061	test: 1.081231

Epoch: 84
Loss: 0.30558645725250244
RMSE train: 0.475356	val: 1.171128	test: 1.250242
MAE train: 0.361691	val: 0.962935	test: 0.976216

Epoch: 85
Loss: 0.338851235806942
RMSE train: 0.538345	val: 1.328973	test: 1.369618
MAE train: 0.426960	val: 1.075267	test: 1.088563

Epoch: 86
Loss: 0.3247907906770706
RMSE train: 0.429544	val: 1.181351	test: 1.250328
MAE train: 0.328305	val: 0.977020	test: 0.977733

Epoch: 87
Loss: 0.26625798642635345
RMSE train: 0.375702	val: 1.104545	test: 1.208943
MAE train: 0.289449	val: 0.929105	test: 0.952531

Epoch: 88
Loss: 0.27971913665533066
RMSE train: 0.525438	val: 1.299424	test: 1.353145
MAE train: 0.396934	val: 1.037608	test: 1.073758

Epoch: 89
Loss: 0.2803766056895256
RMSE train: 0.380701	val: 1.117312	test: 1.242867
MAE train: 0.291438	val: 0.943971	test: 0.961281

Epoch: 90
Loss: 0.2708817087113857
RMSE train: 0.477639	val: 1.242246	test: 1.336258
MAE train: 0.368891	val: 1.016574	test: 1.033689

Epoch: 91
Loss: 0.237419281154871
RMSE train: 0.463045	val: 1.255710	test: 1.329783
MAE train: 0.356198	val: 1.012510	test: 1.053147

Epoch: 92
Loss: 0.3024391308426857
RMSE train: 0.448810	val: 1.230028	test: 1.311999
MAE train: 0.343168	val: 1.017070	test: 1.030919

Epoch: 93
Loss: 0.26500508934259415
RMSE train: 0.619123	val: 1.422094	test: 1.477686
MAE train: 0.487363	val: 1.173998	test: 1.178451

Epoch: 94
Loss: 0.28529365360736847
RMSE train: 0.467014	val: 1.250653	test: 1.324588
MAE train: 0.360341	val: 1.005977	test: 1.041423

Epoch: 95
Loss: 0.310284823179245
RMSE train: 0.444679	val: 1.201254	test: 1.290450
MAE train: 0.349611	val: 0.973938	test: 1.009421

Epoch: 96
Loss: 0.31152939796447754
RMSE train: 0.579259	val: 1.300761	test: 1.382949
MAE train: 0.444143	val: 1.061552	test: 1.092494

Epoch: 97
Loss: 0.26957015693187714
RMSE train: 0.476506	val: 1.166911	test: 1.290940
MAE train: 0.372703	val: 0.970861	test: 1.006960

Epoch: 98
Loss: 0.24225475639104843
RMSE train: 0.577192	val: 1.303707	test: 1.388294
MAE train: 0.469685	val: 1.060525	test: 1.094993

Epoch: 99
Loss: 0.3058485947549343
RMSE train: 0.407158	val: 1.175340	test: 1.271453
MAE train: 0.319672	val: 0.986472	test: 0.991798

Epoch: 100
Loss: 0.26403507590293884
RMSE train: 0.362625	val: 1.180466	test: 1.225829
MAE train: 0.280631	val: 0.964458	test: 0.966261

Epoch: 101
Loss: 0.2876177653670311
RMSE train: 0.552943	val: 1.321013	test: 1.385462
MAE train: 0.417312	val: 1.093185	test: 1.086465

Epoch: 102
Loss: 0.23233017325401306
RMSE train: 0.470997	val: 1.223510	test: 1.331088
MAE train: 0.357153	val: 1.002122	test: 1.038814

Epoch: 103
Loss: 0.26414714753627777
RMSE train: 0.537289	val: 1.301104	test: 1.381312
MAE train: 0.396359	val: 1.027227	test: 1.079595

Epoch: 104
Loss: 0.26633962243795395
RMSE train: 0.486377	val: 1.254558	test: 1.357857
MAE train: 0.380013	val: 1.019156	test: 1.059878

Epoch: 105
Loss: 0.22682273387908936
RMSE train: 0.482902	val: 1.287354	test: 1.363807
MAE train: 0.384778	val: 1.049698	test: 1.076201

Epoch: 106
Loss: 0.212125051766634
RMSE train: 0.388141	val: 1.160454	test: 1.243953
MAE train: 0.287862	val: 0.964009	test: 0.975989

Epoch: 107
Loss: 0.22546467930078506
RMSE train: 0.347782	val: 1.114818	test: 1.216896
MAE train: 0.263408	val: 0.931494	test: 0.954925

Epoch: 108
Loss: 0.2395014762878418
RMSE train: 0.368757	val: 1.167878	test: 1.229448
MAE train: 0.279359	val: 0.961394	test: 0.974132

Epoch: 109
Loss: 0.22046325355768204
RMSE train: 0.352420	val: 1.162903	test: 1.205625
MAE train: 0.264727	val: 0.934408	test: 0.958684

Epoch: 110
Loss: 0.24057446792721748
RMSE train: 0.464872	val: 1.269853	test: 1.326201
MAE train: 0.353099	val: 1.030617	test: 1.045674

Epoch: 111
Loss: 0.21926956996321678
RMSE train: 0.467831	val: 1.295068	test: 1.371421
MAE train: 0.355328	val: 1.056522	test: 1.076517

Epoch: 112
Loss: 0.21481552347540855
RMSE train: 0.338691	val: 1.114333	test: 1.254503
MAE train: 0.261990	val: 0.903010	test: 0.978380

Epoch: 113
Loss: 0.22504764050245285
RMSE train: 0.448212	val: 1.251406	test: 1.362374
MAE train: 0.358303	val: 1.002909	test: 1.074738

Epoch: 114
Loss: 0.2522246539592743
RMSE train: 0.405903	val: 1.168263	test: 1.268251
MAE train: 0.307464	val: 0.933854	test: 1.012844

Epoch: 115
Loss: 0.22487960010766983
RMSE train: 0.386434	val: 1.101448	test: 1.222371
MAE train: 0.282276	val: 0.920381	test: 0.961217

Epoch: 116
Loss: 0.25128673762083054
RMSE train: 0.553076	val: 1.271834	test: 1.342137
MAE train: 0.412008	val: 1.042307	test: 1.070073

Epoch: 117
Loss: 0.25281746312975883
RMSE train: 0.384490	val: 1.104434	test: 1.238390
MAE train: 0.284137	val: 0.932602	test: 0.977154

Epoch: 118
Loss: 0.2079925872385502
RMSE train: 0.306412	val: 1.082191	test: 1.188429
MAE train: 0.231580	val: 0.915514	test: 0.926331

Epoch: 119
Loss: 0.22095345705747604
RMSE train: 0.435306	val: 1.215950	test: 1.291972
MAE train: 0.329065	val: 0.990443	test: 1.014164

Epoch: 120
Loss: 0.2339322343468666
RMSE train: 0.355505	val: 1.125930	test: 1.232449
MAE train: 0.268685	val: 0.920916	test: 0.972327

Epoch: 121
Loss: 0.21112652868032455
RMSE train: 0.323286	val: 1.131235	test: 1.256512
MAE train: 0.243280	val: 0.929155	test: 1.002165

Epoch: 122
Loss: 0.24935032799839973
RMSE train: 0.445035	val: 1.201630	test: 1.331706
MAE train: 0.347306	val: 1.001024	test: 1.046360

Epoch: 123
Loss: 0.26801057159900665
RMSE train: 0.449362	val: 1.229932	test: 1.348720
MAE train: 0.345848	val: 1.035020	test: 1.054681

Epoch: 124
Loss: 0.22966331988573074
RMSE train: 0.386398	val: 1.096575	test: 1.263016
MAE train: 0.290911	val: 0.922180	test: 0.976326

Epoch: 125
Loss: 0.23653363436460495
RMSE train: 0.567735	val: 1.347574	test: 1.442248
MAE train: 0.429865	val: 1.096690	test: 1.128554

Epoch: 126
Loss: 0.20780957117676735
RMSE train: 0.433554	val: 1.221948	test: 1.340541
MAE train: 0.338240	val: 0.994482	test: 1.036512

Epoch: 127
Loss: 0.20146486163139343
RMSE train: 0.465344	val: 1.230298	test: 1.339891
MAE train: 0.352688	val: 1.007778	test: 1.045831

Epoch: 128
Loss: 0.1904820166528225
RMSE train: 0.487822	val: 1.230746	test: 1.338114
MAE train: 0.371227	val: 1.010061	test: 1.045948

Epoch: 129
Loss: 0.19814180210232735
RMSE train: 0.553483	val: 1.309302	test: 1.395015
MAE train: 0.424783	val: 1.072715	test: 1.101641

Epoch: 130
Loss: 0.20466721057891846
RMSE train: 0.407338	val: 1.146856	test: 1.254501
MAE train: 0.302258	val: 0.954045	test: 0.978865

Epoch: 131
Loss: 0.19572313502430916
RMSE train: 0.370239	val: 1.132424	test: 1.255652
MAE train: 0.283236	val: 0.955162	test: 0.974541

Epoch: 132
Loss: 0.20389259979128838
RMSE train: 0.409148	val: 1.253819	test: 1.337795
MAE train: 0.314368	val: 1.019513	test: 1.060890

Epoch: 133
Loss: 0.20807494223117828
RMSE train: 0.359743	val: 1.225027	test: 1.308026
MAE train: 0.268261	val: 0.986295	test: 1.043705

Epoch: 134
Loss: 0.19650357216596603
RMSE train: 0.330139	val: 1.150699	test: 1.254748
MAE train: 0.242923	val: 0.943926	test: 0.992126

Epoch: 135
Loss: 0.1948336847126484
RMSE train: 0.260151	val: 1.147343	test: 1.239807
MAE train: 0.198435	val: 0.939618	test: 0.982909

Epoch: 136
Loss: 0.211727574467659
RMSE train: 0.305098	val: 1.175266	test: 1.233494
MAE train: 0.223926	val: 0.966534	test: 0.977244

Epoch: 137
Loss: 0.20529982820153236
RMSE train: 0.337927	val: 1.129054	test: 1.215151
MAE train: 0.249808	val: 0.961297	test: 0.944382

Epoch: 138
Loss: 0.20445922389626503
RMSE train: 0.384534	val: 1.231484	test: 1.277560
MAE train: 0.284814	val: 0.984718	test: 1.022805

Epoch: 139
Loss: 0.18808262795209885
RMSE train: 0.324493	val: 1.177952	test: 1.264492
MAE train: 0.239000	val: 0.953466	test: 1.002232

Epoch: 140
Loss: 0.21218203380703926
RMSE train: 0.342738	val: 1.113698	test: 1.232739
MAE train: 0.258679	val: 0.927188	test: 0.963592

Epoch: 141
Loss: 0.17376067861914635
RMSE train: 0.503291	val: 1.312204	test: 1.385798
MAE train: 0.383276	val: 1.073241	test: 1.104216

Epoch: 142
Loss: 0.1749766431748867
RMSE train: 0.335893	val: 1.130200	test: 1.241270
MAE train: 0.254822	val: 0.932657	test: 0.972066

Epoch: 143
Loss: 0.18317533284425735
RMSE train: 0.468028	val: 1.178277	test: 1.292500
MAE train: 0.271180	val: 0.855788	test: 0.873784

Epoch: 144
Loss: 0.19429586082696915
RMSE train: 0.448121	val: 1.155151	test: 1.243467
MAE train: 0.328673	val: 0.917193	test: 0.980965

Epoch: 145
Loss: 0.19588736072182655
RMSE train: 0.343897	val: 1.049741	test: 1.160269
MAE train: 0.263631	val: 0.850046	test: 0.916293

Epoch: 146
Loss: 0.1995156854391098
RMSE train: 0.437755	val: 1.059051	test: 1.156474
MAE train: 0.339488	val: 0.864931	test: 0.890616

Epoch: 147
Loss: 0.19711900502443314
RMSE train: 0.426554	val: 1.029017	test: 1.142599
MAE train: 0.314014	val: 0.846236	test: 0.879449

Early stopping
Best (RMSE):	 train: 0.328712	val: 0.977583	test: 1.091209
Best (MAE):	 train: 0.247602	val: 0.813429	test: 0.820183

RMSE train: 0.384717	val: 1.043229	test: 1.092655
MAE train: 0.274561	val: 0.836005	test: 0.816615

Epoch: 145
Loss: 0.24712306633591652
RMSE train: 0.444034	val: 1.114350	test: 1.140126
MAE train: 0.314300	val: 0.894796	test: 0.863757

Epoch: 146
Loss: 0.24664008244872093
RMSE train: 0.385948	val: 1.064503	test: 1.102271
MAE train: 0.277689	val: 0.863668	test: 0.826632

Epoch: 147
Loss: 0.23852012678980827
RMSE train: 0.409280	val: 1.129070	test: 1.136599
MAE train: 0.294554	val: 0.909680	test: 0.864497

Epoch: 148
Loss: 0.24533960223197937
RMSE train: 0.364012	val: 1.103774	test: 1.142887
MAE train: 0.269803	val: 0.884489	test: 0.863172

Epoch: 149
Loss: 0.2289263792335987
RMSE train: 0.358261	val: 1.094869	test: 1.114575
MAE train: 0.263753	val: 0.875717	test: 0.839768

Epoch: 150
Loss: 0.21413053944706917
RMSE train: 0.457245	val: 1.122554	test: 1.156181
MAE train: 0.329892	val: 0.918973	test: 0.871072

Epoch: 151
Loss: 0.24305082857608795
RMSE train: 0.381790	val: 0.998901	test: 1.112403
MAE train: 0.276249	val: 0.824228	test: 0.821479

Epoch: 152
Loss: 0.25445743277668953
RMSE train: 0.426071	val: 1.082043	test: 1.135531
MAE train: 0.304710	val: 0.879144	test: 0.851748

Epoch: 153
Loss: 0.2769295684993267
RMSE train: 0.320272	val: 1.031849	test: 1.097446
MAE train: 0.234979	val: 0.816640	test: 0.823401

Epoch: 154
Loss: 0.2171948328614235
RMSE train: 0.323015	val: 1.058679	test: 1.121410
MAE train: 0.244939	val: 0.825029	test: 0.847332

Epoch: 155
Loss: 0.2130184955894947
RMSE train: 0.354092	val: 1.013623	test: 1.089091
MAE train: 0.260731	val: 0.820120	test: 0.812366

Epoch: 156
Loss: 0.20665553212165833
RMSE train: 0.347577	val: 1.051880	test: 1.104358
MAE train: 0.252521	val: 0.843913	test: 0.825229

Epoch: 157
Loss: 0.21705643460154533
RMSE train: 0.332992	val: 1.103676	test: 1.160784
MAE train: 0.241175	val: 0.882643	test: 0.869746

Early stopping
Best (RMSE):	 train: 0.362137	val: 0.979003	test: 1.081845
Best (MAE):	 train: 0.266241	val: 0.811297	test: 0.813846

RMSE train: 0.453891	val: 1.187417	test: 1.280294
MAE train: 0.357795	val: 0.960285	test: 0.990889

Epoch: 84
Loss: 0.28636420890688896
RMSE train: 0.360387	val: 1.261909	test: 1.310477
MAE train: 0.286749	val: 0.969875	test: 1.012062

Epoch: 85
Loss: 0.300658144056797
RMSE train: 0.400984	val: 1.162626	test: 1.255824
MAE train: 0.311624	val: 0.945013	test: 0.977978

Epoch: 86
Loss: 0.24163918942213058
RMSE train: 0.523138	val: 1.231994	test: 1.310058
MAE train: 0.406443	val: 0.999999	test: 1.016883

Epoch: 87
Loss: 0.2788989245891571
RMSE train: 0.498855	val: 1.242776	test: 1.307120
MAE train: 0.393071	val: 0.995300	test: 1.021993

Epoch: 88
Loss: 0.2503579221665859
RMSE train: 0.470198	val: 1.186724	test: 1.267229
MAE train: 0.371264	val: 0.967616	test: 0.991778

Epoch: 89
Loss: 0.2812984958291054
RMSE train: 0.580314	val: 1.287733	test: 1.335770
MAE train: 0.456610	val: 1.037038	test: 1.024452

Epoch: 90
Loss: 0.3014693260192871
RMSE train: 0.463711	val: 1.191088	test: 1.270913
MAE train: 0.359449	val: 0.956867	test: 0.979404

Epoch: 91
Loss: 0.2886570580303669
RMSE train: 0.457744	val: 1.187594	test: 1.251068
MAE train: 0.358625	val: 0.946916	test: 0.961960

Epoch: 92
Loss: 0.2432618848979473
RMSE train: 0.525156	val: 1.227521	test: 1.286492
MAE train: 0.411332	val: 0.986993	test: 0.990667

Epoch: 93
Loss: 0.2624880522489548
RMSE train: 0.368156	val: 1.123407	test: 1.200036
MAE train: 0.286654	val: 0.883953	test: 0.944342

Epoch: 94
Loss: 0.26083771139383316
RMSE train: 0.386490	val: 1.218377	test: 1.242971
MAE train: 0.301531	val: 0.940148	test: 0.983770

Epoch: 95
Loss: 0.2668379098176956
RMSE train: 0.380991	val: 1.163771	test: 1.197775
MAE train: 0.294962	val: 0.914160	test: 0.959869

Epoch: 96
Loss: 0.26158544793725014
RMSE train: 0.446074	val: 1.267481	test: 1.258922
MAE train: 0.346058	val: 0.985287	test: 0.983965

Epoch: 97
Loss: 0.2946609556674957
RMSE train: 0.360269	val: 1.126677	test: 1.197620
MAE train: 0.274748	val: 0.890654	test: 0.942505

Epoch: 98
Loss: 0.22349508851766586
RMSE train: 0.388389	val: 1.099201	test: 1.200114
MAE train: 0.292742	val: 0.879550	test: 0.941959

Epoch: 99
Loss: 0.24902602285146713
RMSE train: 0.396318	val: 1.199755	test: 1.236399
MAE train: 0.310935	val: 0.925739	test: 0.969246

Epoch: 100
Loss: 0.24451685696840286
RMSE train: 0.343304	val: 1.200957	test: 1.224537
MAE train: 0.274259	val: 0.929653	test: 0.967098

Epoch: 101
Loss: 0.2670774981379509
RMSE train: 0.437268	val: 1.185331	test: 1.248158
MAE train: 0.344926	val: 0.937391	test: 0.975407

Epoch: 102
Loss: 0.24799180403351784
RMSE train: 0.493911	val: 1.251018	test: 1.279219
MAE train: 0.376773	val: 0.967314	test: 0.999520

Epoch: 103
Loss: 0.31355930119752884
RMSE train: 0.340956	val: 1.079116	test: 1.164715
MAE train: 0.262630	val: 0.839627	test: 0.935790

Epoch: 104
Loss: 0.25098391622304916
RMSE train: 0.494239	val: 1.292986	test: 1.294951
MAE train: 0.386555	val: 1.003739	test: 1.013246

Epoch: 105
Loss: 0.24528737366199493
RMSE train: 0.345538	val: 1.181097	test: 1.218724
MAE train: 0.267128	val: 0.913922	test: 0.967677

Epoch: 106
Loss: 0.23197558522224426
RMSE train: 0.397263	val: 1.103589	test: 1.196057
MAE train: 0.300808	val: 0.900989	test: 0.947589

Epoch: 107
Loss: 0.23477303981781006
RMSE train: 0.474352	val: 1.210226	test: 1.274362
MAE train: 0.369768	val: 0.992207	test: 0.988477

Epoch: 108
Loss: 0.22331032156944275
RMSE train: 0.338134	val: 1.173157	test: 1.233843
MAE train: 0.262209	val: 0.907615	test: 0.979864

Epoch: 109
Loss: 0.2284195013344288
RMSE train: 0.378979	val: 1.254707	test: 1.271215
MAE train: 0.295716	val: 0.969580	test: 1.003654

Epoch: 110
Loss: 0.2543981969356537
RMSE train: 0.350139	val: 1.113316	test: 1.190653
MAE train: 0.273683	val: 0.875274	test: 0.943419

Epoch: 111
Loss: 0.23656804487109184
RMSE train: 0.489446	val: 1.277966	test: 1.301934
MAE train: 0.380493	val: 1.012669	test: 1.004526

Epoch: 112
Loss: 0.23447174578905106
RMSE train: 0.518085	val: 1.213381	test: 1.272282
MAE train: 0.409294	val: 0.983035	test: 0.974808

Epoch: 113
Loss: 0.25173505395650864
RMSE train: 0.376978	val: 1.075753	test: 1.172865
MAE train: 0.292823	val: 0.857177	test: 0.929705

Epoch: 114
Loss: 0.22078394889831543
RMSE train: 0.353761	val: 1.240710	test: 1.262015
MAE train: 0.275354	val: 0.960202	test: 1.006465

Epoch: 115
Loss: 0.21744952723383904
RMSE train: 0.307716	val: 1.071545	test: 1.199837
MAE train: 0.234356	val: 0.842940	test: 0.955283

Epoch: 116
Loss: 0.247800100594759
RMSE train: 0.337266	val: 1.100441	test: 1.232710
MAE train: 0.252885	val: 0.867412	test: 0.978839

Epoch: 117
Loss: 0.2253705896437168
RMSE train: 0.362446	val: 1.217377	test: 1.281226
MAE train: 0.270713	val: 0.953344	test: 1.020938

Epoch: 118
Loss: 0.25013670697808266
RMSE train: 0.382685	val: 1.187550	test: 1.237017
MAE train: 0.296194	val: 0.943419	test: 0.975506

Epoch: 119
Loss: 0.2285001501441002
RMSE train: 0.366747	val: 1.188495	test: 1.258263
MAE train: 0.288417	val: 0.946189	test: 0.986281

Epoch: 120
Loss: 0.20801717415452003
RMSE train: 0.450865	val: 1.189833	test: 1.299690
MAE train: 0.345754	val: 0.962484	test: 1.002863

Epoch: 121
Loss: 0.19107645377516747
RMSE train: 0.399170	val: 1.119227	test: 1.256504
MAE train: 0.301122	val: 0.907051	test: 0.983417

Epoch: 122
Loss: 0.18195021897554398
RMSE train: 0.305573	val: 1.078500	test: 1.249564
MAE train: 0.235284	val: 0.850609	test: 1.000863

Epoch: 123
Loss: 0.20236504822969437
RMSE train: 0.385423	val: 1.206371	test: 1.323160
MAE train: 0.296911	val: 0.923561	test: 1.050318

Epoch: 124
Loss: 0.24316633120179176
RMSE train: 0.400993	val: 1.101802	test: 1.260408
MAE train: 0.291835	val: 0.897807	test: 0.985919

Epoch: 125
Loss: 0.22045506164431572
RMSE train: 0.315062	val: 1.069601	test: 1.220414
MAE train: 0.239191	val: 0.858618	test: 0.971944

Epoch: 126
Loss: 0.22310497239232063
RMSE train: 0.299394	val: 1.195554	test: 1.274479
MAE train: 0.229422	val: 0.944226	test: 1.007899

Epoch: 127
Loss: 0.264974907040596
RMSE train: 0.322123	val: 1.152105	test: 1.275178
MAE train: 0.254523	val: 0.912056	test: 1.012048

Epoch: 128
Loss: 0.19802625104784966
RMSE train: 0.405158	val: 1.104401	test: 1.275290
MAE train: 0.298317	val: 0.909316	test: 0.997941

Epoch: 129
Loss: 0.20689907297492027
RMSE train: 0.395342	val: 1.162691	test: 1.321505
MAE train: 0.293786	val: 0.941679	test: 1.029561

Epoch: 130
Loss: 0.19358933717012405
RMSE train: 0.258249	val: 1.230204	test: 1.358907
MAE train: 0.197875	val: 0.938370	test: 1.081996

Epoch: 131
Loss: 0.18430564925074577
RMSE train: 0.296940	val: 1.123245	test: 1.256396
MAE train: 0.225507	val: 0.860321	test: 1.004001

Epoch: 132
Loss: 0.17524294927716255
RMSE train: 0.304696	val: 1.112781	test: 1.237997
MAE train: 0.232289	val: 0.861208	test: 0.986944

Epoch: 133
Loss: 0.17270798236131668
RMSE train: 0.323130	val: 1.072435	test: 1.217025
MAE train: 0.247622	val: 0.846015	test: 0.962378

Epoch: 134
Loss: 0.2053603008389473
RMSE train: 0.375118	val: 1.177848	test: 1.268934
MAE train: 0.294316	val: 0.934678	test: 0.982195

Epoch: 135
Loss: 0.23752833157777786
RMSE train: 0.391964	val: 1.088005	test: 1.248005
MAE train: 0.295795	val: 0.892494	test: 0.960238

Epoch: 136
Loss: 0.1965060830116272
RMSE train: 0.452597	val: 1.158139	test: 1.307695
MAE train: 0.336054	val: 0.953260	test: 0.997711

Epoch: 137
Loss: 0.20919855311512947
RMSE train: 0.405115	val: 1.211250	test: 1.318512
MAE train: 0.309739	val: 0.948242	test: 1.015742

Epoch: 138
Loss: 0.2104545421898365
RMSE train: 0.272717	val: 1.105660	test: 1.247161
MAE train: 0.213071	val: 0.872697	test: 0.969925

Epoch: 139
Loss: 0.2258545570075512
RMSE train: 0.484956	val: 1.239089	test: 1.380487
MAE train: 0.370796	val: 1.005328	test: 1.047545

Epoch: 140
Loss: 0.20321431383490562
RMSE train: 0.252697	val: 1.070120	test: 1.272053
MAE train: 0.196903	val: 0.844715	test: 0.993316

Epoch: 141
Loss: 0.21686648577451706
RMSE train: 0.253066	val: 1.042887	test: 1.250105
MAE train: 0.196476	val: 0.832505	test: 0.977392

Epoch: 142
Loss: 0.1867719180881977
RMSE train: 0.371143	val: 1.135939	test: 1.298144
MAE train: 0.281901	val: 0.919640	test: 0.992440

Epoch: 143
Loss: 0.20045076683163643
RMSE train: 0.316002	val: 1.013520	test: 1.215673All runs completed.

MAE train: 0.218370	val: 0.902660	test: 0.971587

Epoch: 144
Loss: 0.18425244092941284
RMSE train: 0.425420	val: 1.449620	test: 1.456921
MAE train: 0.332898	val: 1.180926	test: 1.116359

Epoch: 145
Loss: 0.23945913463830948
RMSE train: 0.276254	val: 1.268006	test: 1.283785
MAE train: 0.215598	val: 1.021599	test: 0.995292

Epoch: 146
Loss: 0.19930920004844666
RMSE train: 0.272572	val: 1.333728	test: 1.335516
MAE train: 0.210901	val: 1.052202	test: 1.035140

Epoch: 147
Loss: 0.18109974265098572
RMSE train: 0.360641	val: 1.312780	test: 1.340004
MAE train: 0.261553	val: 1.029913	test: 1.024107

Epoch: 148
Loss: 0.160186056047678
RMSE train: 0.310093	val: 1.175222	test: 1.233969
MAE train: 0.227182	val: 0.958767	test: 0.966163

Epoch: 149
Loss: 0.24430012330412865
RMSE train: 0.305452	val: 1.208769	test: 1.238260
MAE train: 0.230535	val: 0.984147	test: 0.986267

Epoch: 150
Loss: 0.214340940117836
RMSE train: 0.449845	val: 1.341405	test: 1.371472
MAE train: 0.336189	val: 1.086377	test: 1.058803

Epoch: 151
Loss: 0.2706386260688305
RMSE train: 0.364361	val: 1.262834	test: 1.309688
MAE train: 0.273339	val: 1.035710	test: 1.024174

Epoch: 152
Loss: 0.18475057184696198
RMSE train: 0.314258	val: 1.356086	test: 1.366026
MAE train: 0.233510	val: 1.083663	test: 1.080803

Epoch: 153
Loss: 0.20377282053232193
RMSE train: 0.239488	val: 1.262794	test: 1.279497
MAE train: 0.189132	val: 1.038665	test: 1.000798

Early stopping
Best (RMSE):	 train: 0.303582	val: 1.089375	test: 1.171866
Best (MAE):	 train: 0.240696	val: 0.892274	test: 0.936352

RMSE train: 0.384125	val: 1.107413	test: 1.079182
MAE train: 0.278504	val: 0.880231	test: 0.800439

Epoch: 145
Loss: 0.2642243355512619
RMSE train: 0.372218	val: 1.160853	test: 1.100572
MAE train: 0.274406	val: 0.913291	test: 0.832086

Epoch: 146
Loss: 0.20872559398412704
RMSE train: 0.378773	val: 1.115712	test: 1.098289
MAE train: 0.274571	val: 0.886590	test: 0.830519

Epoch: 147
Loss: 0.22058265283703804
RMSE train: 0.461956	val: 1.212737	test: 1.141743
MAE train: 0.331614	val: 0.960494	test: 0.889884

Epoch: 148
Loss: 0.2152218259871006
RMSE train: 0.424053	val: 1.060609	test: 1.080469
MAE train: 0.300189	val: 0.883594	test: 0.809437

Epoch: 149
Loss: 0.2132527343928814
RMSE train: 0.435127	val: 1.093380	test: 1.104407
MAE train: 0.313975	val: 0.907427	test: 0.841093

Epoch: 150
Loss: 0.23977163061499596
RMSE train: 0.457647	val: 1.163100	test: 1.157082
MAE train: 0.332593	val: 0.944782	test: 0.900156

Epoch: 151
Loss: 0.21927078440785408
RMSE train: 0.396967	val: 1.041059	test: 1.087669
MAE train: 0.282754	val: 0.850691	test: 0.818226

Epoch: 152
Loss: 0.22156201675534248
RMSE train: 0.408527	val: 1.046011	test: 1.068033
MAE train: 0.287288	val: 0.851055	test: 0.798213

Epoch: 153
Loss: 0.2159058265388012
RMSE train: 0.440298	val: 1.118283	test: 1.086079
MAE train: 0.314064	val: 0.899228	test: 0.814514

Epoch: 154
Loss: 0.22456364706158638
RMSE train: 0.396579	val: 1.045886	test: 1.089098
MAE train: 0.282815	val: 0.854855	test: 0.815821

Epoch: 155
Loss: 0.21074678376317024
RMSE train: 0.401583	val: 1.120253	test: 1.112495
MAE train: 0.286826	val: 0.894672	test: 0.837770

Epoch: 156
Loss: 0.21724602207541466
RMSE train: 0.415682	val: 1.129326	test: 1.107312
MAE train: 0.293254	val: 0.896840	test: 0.838596

Epoch: 157
Loss: 0.22267146781086922
RMSE train: 0.378874	val: 1.087925	test: 1.086177
MAE train: 0.263137	val: 0.856894	test: 0.817380

Epoch: 158
Loss: 0.26700330153107643
RMSE train: 0.368472	val: 1.050056	test: 1.108288
MAE train: 0.265282	val: 0.848293	test: 0.832165

Epoch: 159
Loss: 0.23449068889021873
RMSE train: 0.352535	val: 1.124434	test: 1.117215
MAE train: 0.255860	val: 0.902423	test: 0.837303

Epoch: 160
Loss: 0.23049571365118027
RMSE train: 0.390951	val: 1.147979	test: 1.138031
MAE train: 0.276186	val: 0.929665	test: 0.861214

Epoch: 161
Loss: 0.21650657430291176
RMSE train: 0.343063	val: 1.034777	test: 1.085438
MAE train: 0.247384	val: 0.856315	test: 0.818773

Epoch: 162
Loss: 0.18745407834649086
RMSE train: 0.341545	val: 1.161328	test: 1.131489
MAE train: 0.241596	val: 0.926651	test: 0.844522

Epoch: 163
Loss: 0.2053172066807747
RMSE train: 0.362703	val: 1.090154	test: 1.077377
MAE train: 0.254836	val: 0.895075	test: 0.794561

Epoch: 164
Loss: 0.22941849380731583
RMSE train: 0.386586	val: 1.049351	test: 1.075670
MAE train: 0.274584	val: 0.866404	test: 0.794479

Epoch: 165
Loss: 0.19617841020226479
RMSE train: 0.336667	val: 1.152464	test: 1.140178
MAE train: 0.249412	val: 0.918889	test: 0.848297

Epoch: 166
Loss: 0.2224097065627575
RMSE train: 0.351655	val: 1.073433	test: 1.102660
MAE train: 0.256313	val: 0.877669	test: 0.823854

Early stopping
Best (RMSE):	 train: 0.408300	val: 0.968373	test: 1.071474
Best (MAE):	 train: 0.287498	val: 0.802783	test: 0.787915
All runs completed.
All runs completed.

MAE train: 0.351891	val: 0.945250	test: 1.018331

Epoch: 144
Loss: 0.17750175297260284
RMSE train: 0.556603	val: 1.325285	test: 1.412499
MAE train: 0.434540	val: 1.079229	test: 1.127290

Epoch: 145
Loss: 0.21122200787067413
RMSE train: 0.415418	val: 1.209108	test: 1.322331
MAE train: 0.320663	val: 0.980702	test: 1.036297

Epoch: 146
Loss: 0.18058834969997406
RMSE train: 0.448671	val: 1.253455	test: 1.345310
MAE train: 0.341703	val: 0.992538	test: 1.056672

Epoch: 147
Loss: 0.17024977132678032
RMSE train: 0.434703	val: 1.210780	test: 1.319780
MAE train: 0.329513	val: 0.983727	test: 1.032244

Epoch: 148
Loss: 0.14624746143817902
RMSE train: 0.411629	val: 1.193548	test: 1.312830
MAE train: 0.306921	val: 0.990067	test: 1.028826

Epoch: 149
Loss: 0.25139423832297325
RMSE train: 0.398024	val: 1.228409	test: 1.328681
MAE train: 0.293565	val: 1.004817	test: 1.052464

Epoch: 150
Loss: 0.21038565039634705
RMSE train: 0.413751	val: 1.263152	test: 1.352441
MAE train: 0.305081	val: 1.032001	test: 1.064982

Epoch: 151
Loss: 0.25261978432536125
RMSE train: 0.352336	val: 1.190615	test: 1.293064
MAE train: 0.257636	val: 0.990437	test: 1.007756

Epoch: 152
Loss: 0.1979464329779148
RMSE train: 0.404233	val: 1.248030	test: 1.331932
MAE train: 0.289494	val: 1.000624	test: 1.045937

Epoch: 153
Loss: 0.19286823645234108
RMSE train: 0.386618	val: 1.285066	test: 1.390997
MAE train: 0.303541	val: 1.046540	test: 1.090285

Early stopping
Best (RMSE):	 train: 0.306412	val: 1.082191	test: 1.188429
Best (MAE):	 train: 0.231580	val: 0.915514	test: 0.926331

MAE train: 0.235117	val: 0.819673	test: 0.948641

Epoch: 144
Loss: 0.21435802429914474
RMSE train: 0.377794	val: 1.232923	test: 1.315174
MAE train: 0.287622	val: 0.967742	test: 1.001197

Epoch: 145
Loss: 0.1804550476372242
RMSE train: 0.324030	val: 1.028603	test: 1.204138
MAE train: 0.227583	val: 0.834206	test: 0.947502

Epoch: 146
Loss: 0.20038384571671486
RMSE train: 0.350556	val: 1.136364	test: 1.266500
MAE train: 0.259892	val: 0.897405	test: 0.996526

Epoch: 147
Loss: 0.22219473123550415
RMSE train: 0.296687	val: 1.222109	test: 1.293066
MAE train: 0.222167	val: 0.942874	test: 1.018089

Epoch: 148
Loss: 0.20065395534038544
RMSE train: 0.236281	val: 1.113715	test: 1.190335
MAE train: 0.181803	val: 0.867043	test: 0.949508

Epoch: 149
Loss: 0.18268441408872604
RMSE train: 0.355296	val: 1.146004	test: 1.214757
MAE train: 0.268588	val: 0.887881	test: 0.954335

Epoch: 150
Loss: 0.2210834138095379
RMSE train: 0.265080	val: 1.120862	test: 1.184354
MAE train: 0.201212	val: 0.867032	test: 0.952899

Epoch: 151
Loss: 0.19986720383167267
RMSE train: 0.312231	val: 1.170315	test: 1.203989
MAE train: 0.245294	val: 0.917569	test: 0.953381

Epoch: 152
Loss: 0.1968793049454689
RMSE train: 0.331335	val: 1.125700	test: 1.209120
MAE train: 0.255567	val: 0.883046	test: 0.953385

Epoch: 153
Loss: 0.16436200216412544
RMSE train: 0.294991	val: 1.025536	test: 1.203478
MAE train: 0.205783	val: 0.818316	test: 0.964573

Epoch: 154
Loss: 0.2008073516190052
RMSE train: 0.343027	val: 1.166354	test: 1.287722
MAE train: 0.261790	val: 0.899215	test: 1.007928

Epoch: 155
Loss: 0.22025664523243904
RMSE train: 0.291006	val: 1.202213	test: 1.308759
MAE train: 0.226672	val: 0.909657	test: 1.024074

Epoch: 156
Loss: 0.17692895978689194
RMSE train: 0.297213	val: 1.091577	test: 1.237257
MAE train: 0.228188	val: 0.856462	test: 0.976794

Epoch: 157
Loss: 0.20679891481995583
RMSE train: 0.472847	val: 1.392436	test: 1.396300
MAE train: 0.356208	val: 1.078096	test: 1.070096

Epoch: 158
Loss: 0.2060101553797722
RMSE train: 0.289999	val: 1.042529	test: 1.155869
MAE train: 0.214611	val: 0.827650	test: 0.916022

Epoch: 159
Loss: 0.19033366814255714
RMSE train: 0.431364	val: 1.159503	test: 1.244244
MAE train: 0.328830	val: 0.939184	test: 0.959716

Epoch: 160
Loss: 0.19172574952244759
RMSE train: 0.397279	val: 1.194043	test: 1.279324
MAE train: 0.302454	val: 0.949037	test: 0.989086

Epoch: 161
Loss: 0.16866131871938705
RMSE train: 0.263760	val: 1.075539	test: 1.232063
MAE train: 0.196100	val: 0.838205	test: 0.972691

Epoch: 162
Loss: 0.21471673250198364
RMSE train: 0.381700	val: 1.178404	test: 1.300532
MAE train: 0.293375	val: 0.940917	test: 1.004607

Epoch: 163
Loss: 0.14723562076687813
RMSE train: 0.251891	val: 1.107411	test: 1.240384
MAE train: 0.196543	val: 0.867364	test: 0.987975

Epoch: 164
Loss: 0.1843258962035179
RMSE train: 0.261961	val: 1.069761	test: 1.209487
MAE train: 0.199426	val: 0.846473	test: 0.961769

Epoch: 165
Loss: 0.20255309343338013
RMSE train: 0.384413	val: 1.145189	test: 1.263597
MAE train: 0.289192	val: 0.932365	test: 0.968322

Epoch: 166
Loss: 0.15928104147315025
RMSE train: 0.310560	val: 1.137935	test: 1.236067
MAE train: 0.239654	val: 0.895702	test: 0.952313

Epoch: 167
Loss: 0.14909428544342518
RMSE train: 0.370459	val: 1.314664	test: 1.323404
MAE train: 0.276007	val: 1.014686	test: 1.030119

Epoch: 168
Loss: 0.15638046711683273
RMSE train: 0.295542	val: 1.044673	test: 1.190385
MAE train: 0.218204	val: 0.827225	test: 0.935670

Epoch: 169
Loss: 0.19297656416893005
RMSE train: 0.384757	val: 1.094567	test: 1.227959
MAE train: 0.280909	val: 0.889710	test: 0.950491

Epoch: 170
Loss: 0.16192996874451637
RMSE train: 0.323692	val: 1.158506	test: 1.247013
MAE train: 0.251656	val: 0.902135	test: 0.973050

Epoch: 171
Loss: 0.1954817771911621
RMSE train: 0.210692	val: 1.067788	test: 1.196201
MAE train: 0.162613	val: 0.835750	test: 0.948876

Epoch: 172
Loss: 0.26693058013916016
RMSE train: 0.428112	val: 1.198566	test: 1.301774
MAE train: 0.314760	val: 0.956595	test: 1.000321

Epoch: 173
Loss: 0.21637460216879845
RMSE train: 0.384985	val: 1.191056	test: 1.286218
MAE train: 0.275144	val: 0.928439	test: 0.996599

Epoch: 174
Loss: 0.1882471963763237
RMSE train: 0.257915	val: 1.040991	test: 1.170045
MAE train: 0.194514	val: 0.817420	test: 0.930739

Epoch: 175
Loss: 0.2152298428118229
RMSE train: 0.360095	val: 1.440710	test: 1.371305
MAE train: 0.280172	val: 1.084181	test: 1.090143

Epoch: 176
Loss: 0.1877632774412632
RMSE train: 0.289739	val: 1.102393	test: 1.228952
MAE train: 0.218030	val: 0.867726	test: 0.967736

Epoch: 177
Loss: 0.18600502610206604
RMSE train: 0.285755	val: 1.081898	test: 1.214948
MAE train: 0.217136	val: 0.864443	test: 0.944052

Epoch: 178
Loss: 0.2048046998679638
RMSE train: 0.442732	val: 1.313294	test: 1.341277
MAE train: 0.338341	val: 1.029167	test: 1.037734

Early stopping
Best (RMSE):	 train: 0.316002	val: 1.013520	test: 1.215673
Best (MAE):	 train: 0.235117	val: 0.819673	test: 0.948641
All runs completed.
