>>> Starting run for dataset: lipo
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running RANDOM configs_split_experiments/GraphCL/lipo/random/train_prop=0.6.yml on cuda:0
Running RANDOM configs_split_experiments/GraphCL/lipo/random/train_prop=0.7.yml on cuda:1
Running RANDOM configs_split_experiments/GraphCL/lipo/random/train_prop=0.8.yml on cuda:2
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/lipo/random/train_prop=0.8.yml --runseed 4 --device cuda:2
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/lipo/random/train_prop=0.8.yml --runseed 5 --device cuda:2
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/lipo/random/train_prop=0.8.yml --runseed 6 --device cuda:2
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/lipo/random/train_prop=0.6.yml --runseed 4 --device cuda:0
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/lipo/random/train_prop=0.6.yml --runseed 5 --device cuda:0
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/lipo/random/train_prop=0.6.yml --runseed 6 --device cuda:0
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/lipo/random/train_prop=0.7.yml --runseed 4 --device cuda:1
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/lipo/random/train_prop=0.7.yml --runseed 5 --device cuda:1
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/lipo/random/train_prop=0.7.yml --runseed 6 --device cuda:1
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/lipo/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/lipo/random/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/lipo/random/train_prop=0.6/lipophilicity_random_4_26-05_09-47-41  ]
[ Using Seed :  4  ]
[ Using device :  cuda:0  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.437810897827148
RMSE train: 2.426360	val: 2.463114	test: 2.463354
MAE train: 2.186149	val: 2.224772	test: 2.212546

Epoch: 2
Loss: 3.958935046195984
RMSE train: 2.179324	val: 2.210841	test: 2.205211
MAE train: 1.943770	val: 1.979548	test: 1.959236

Epoch: 3
Loss: 2.811845588684082
RMSE train: 1.776170	val: 1.822331	test: 1.805736
MAE train: 1.563929	val: 1.606899	test: 1.573606

Epoch: 4
Loss: 1.9331795215606689
RMSE train: 1.411866	val: 1.450456	test: 1.441474
MAE train: 1.210998	val: 1.240901	test: 1.226746

Epoch: 5
Loss: 1.3124078214168549
RMSE train: 1.009473	val: 1.050500	test: 1.060776
MAE train: 0.834191	val: 0.858345	test: 0.883223

Epoch: 6
Loss: 1.006410425901413
RMSE train: 0.893610	val: 0.926513	test: 0.949137
MAE train: 0.721325	val: 0.742614	test: 0.768647

Epoch: 7
Loss: 0.8747555315494537
RMSE train: 0.850536	val: 0.879509	test: 0.914057
MAE train: 0.673627	val: 0.693555	test: 0.727939

Epoch: 8
Loss: 0.8355429530143738
RMSE train: 0.829990	val: 0.855866	test: 0.907575
MAE train: 0.647159	val: 0.664976	test: 0.715396

Epoch: 9
Loss: 0.806640875339508
RMSE train: 0.810240	val: 0.845080	test: 0.886404
MAE train: 0.637989	val: 0.659710	test: 0.700774

Epoch: 10
Loss: 0.7811682403087616
RMSE train: 0.811781	val: 0.845859	test: 0.877731
MAE train: 0.640247	val: 0.651411	test: 0.689494

Epoch: 11
Loss: 0.7322466313838959
RMSE train: 0.812490	val: 0.841594	test: 0.887311
MAE train: 0.640575	val: 0.653737	test: 0.700720

Epoch: 12
Loss: 0.7035925447940826
RMSE train: 0.764731	val: 0.816496	test: 0.841311
MAE train: 0.604693	val: 0.622258	test: 0.658349

Epoch: 13
Loss: 0.7133947134017944
RMSE train: 0.755849	val: 0.802188	test: 0.849101
MAE train: 0.592854	val: 0.616231	test: 0.663186

Epoch: 14
Loss: 0.6936598777770996
RMSE train: 0.767999	val: 0.815529	test: 0.850760
MAE train: 0.607813	val: 0.631871	test: 0.669371

Epoch: 15
Loss: 0.683177363872528
RMSE train: 0.735116	val: 0.791534	test: 0.834339
MAE train: 0.579493	val: 0.614839	test: 0.652262

Epoch: 16
Loss: 0.6610388875007629
RMSE train: 0.762888	val: 0.809332	test: 0.853518
MAE train: 0.601253	val: 0.627858	test: 0.661862

Epoch: 17
Loss: 0.6477422714233398
RMSE train: 0.759762	val: 0.810156	test: 0.844870
MAE train: 0.596532	val: 0.623166	test: 0.655584

Epoch: 18
Loss: 0.6498688042163849
RMSE train: 0.749902	val: 0.802183	test: 0.847473
MAE train: 0.585056	val: 0.618812	test: 0.655022

Epoch: 19
Loss: 0.6212012797594071
RMSE train: 0.725609	val: 0.790887	test: 0.818743
MAE train: 0.573657	val: 0.607598	test: 0.637417

Epoch: 20
Loss: 0.6036134570837021
RMSE train: 0.711033	val: 0.785228	test: 0.808513
MAE train: 0.561539	val: 0.597331	test: 0.623368

Epoch: 21
Loss: 0.5975511193275451
RMSE train: 0.729347	val: 0.795041	test: 0.831658
MAE train: 0.576813	val: 0.617297	test: 0.648006

Epoch: 22
Loss: 0.596094137430191
RMSE train: 0.701313	val: 0.774980	test: 0.805589
MAE train: 0.554376	val: 0.593736	test: 0.623360Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/lipo/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/lipo/random/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/lipo/random/train_prop=0.6/lipophilicity_random_6_26-05_09-47-41  ]
[ Using Seed :  6  ]
[ Using device :  cuda:0  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 6.051488828659058
RMSE train: 2.573153	val: 2.612271	test: 2.606169
MAE train: 2.332896	val: 2.376497	test: 2.353170

Epoch: 2
Loss: 4.471044397354126
RMSE train: 2.238175	val: 2.279209	test: 2.269551
MAE train: 1.998726	val: 2.043803	test: 2.017312

Epoch: 3
Loss: 3.2763405323028563
RMSE train: 1.939137	val: 1.991327	test: 1.970386
MAE train: 1.722671	val: 1.774433	test: 1.738750

Epoch: 4
Loss: 2.295895314216614
RMSE train: 1.577055	val: 1.639968	test: 1.607897
MAE train: 1.372756	val: 1.425843	test: 1.388639

Epoch: 5
Loss: 1.5994961500167846
RMSE train: 1.218277	val: 1.274176	test: 1.257612
MAE train: 1.026236	val: 1.066913	test: 1.056066

Epoch: 6
Loss: 1.1330595910549164
RMSE train: 0.981916	val: 1.036641	test: 1.037517
MAE train: 0.807448	val: 0.838865	test: 0.851780

Epoch: 7
Loss: 0.9208176910877228
RMSE train: 0.867445	val: 0.907849	test: 0.939123
MAE train: 0.688683	val: 0.712974	test: 0.749752

Epoch: 8
Loss: 0.8192050576210022
RMSE train: 0.825821	val: 0.853807	test: 0.918127
MAE train: 0.649481	val: 0.663964	test: 0.719316

Epoch: 9
Loss: 0.8023209869861603
RMSE train: 0.882427	val: 0.895232	test: 0.957727
MAE train: 0.684430	val: 0.690039	test: 0.747572

Epoch: 10
Loss: 0.7804882526397705
RMSE train: 0.796159	val: 0.830953	test: 0.884411
MAE train: 0.628901	val: 0.642877	test: 0.695404

Epoch: 11
Loss: 0.7464376866817475
RMSE train: 0.771136	val: 0.814143	test: 0.860834
MAE train: 0.605801	val: 0.625393	test: 0.678233

Epoch: 12
Loss: 0.7204539597034454
RMSE train: 0.770725	val: 0.815730	test: 0.866126
MAE train: 0.610624	val: 0.631118	test: 0.684535

Epoch: 13
Loss: 0.7189072668552399
RMSE train: 0.789874	val: 0.830138	test: 0.878078
MAE train: 0.622316	val: 0.643026	test: 0.686680

Epoch: 14
Loss: 0.7071281969547272
RMSE train: 0.736099	val: 0.801944	test: 0.844547
MAE train: 0.582543	val: 0.618400	test: 0.663057

Epoch: 15
Loss: 0.660306304693222
RMSE train: 0.764410	val: 0.820464	test: 0.861037
MAE train: 0.603091	val: 0.629363	test: 0.670681

Epoch: 16
Loss: 0.6525982677936554
RMSE train: 0.721762	val: 0.789933	test: 0.830282
MAE train: 0.568475	val: 0.607050	test: 0.648734

Epoch: 17
Loss: 0.6399091899394989
RMSE train: 0.737714	val: 0.805052	test: 0.835149
MAE train: 0.580127	val: 0.615470	test: 0.653572

Epoch: 18
Loss: 0.618834501504898
RMSE train: 0.733295	val: 0.791631	test: 0.843241
MAE train: 0.572936	val: 0.607338	test: 0.649941

Epoch: 19
Loss: 0.6281067728996277
RMSE train: 0.707431	val: 0.780221	test: 0.813342
MAE train: 0.555717	val: 0.596330	test: 0.630565

Epoch: 20
Loss: 0.6020651400089264
RMSE train: 0.737764	val: 0.804092	test: 0.837535
MAE train: 0.581690	val: 0.617961	test: 0.649260

Epoch: 21
Loss: 0.5964569866657257
RMSE train: 0.710109	val: 0.792908	test: 0.817738
MAE train: 0.560782	val: 0.602318	test: 0.634411

Epoch: 22
Loss: 0.6037164002656936
RMSE train: 0.752795	val: 0.814397	test: 0.858806
MAE train: 0.593727	val: 0.628536	test: 0.660207Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/lipo/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/lipo/random/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/lipo/random/train_prop=0.6/lipophilicity_random_5_26-05_09-47-41  ]
[ Using Seed :  5  ]
[ Using device :  cuda:0  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.5737205982208256
RMSE train: 2.402739	val: 2.445171	test: 2.421641
MAE train: 2.167144	val: 2.214525	test: 2.169190

Epoch: 2
Loss: 4.0703027725219725
RMSE train: 2.035473	val: 2.071592	test: 2.060540
MAE train: 1.803711	val: 1.841601	test: 1.811360

Epoch: 3
Loss: 2.9147106885910032
RMSE train: 1.739781	val: 1.787170	test: 1.768190
MAE train: 1.523976	val: 1.566895	test: 1.535181

Epoch: 4
Loss: 2.0105018258094787
RMSE train: 1.322303	val: 1.373878	test: 1.366867
MAE train: 1.129349	val: 1.156857	test: 1.159085

Epoch: 5
Loss: 1.3904452919960022
RMSE train: 1.014674	val: 1.059904	test: 1.077138
MAE train: 0.833518	val: 0.856269	test: 0.885260

Epoch: 6
Loss: 1.0353151977062225
RMSE train: 0.845412	val: 0.871192	test: 0.920875
MAE train: 0.669767	val: 0.681952	test: 0.728971

Epoch: 7
Loss: 0.8997743487358093
RMSE train: 0.843748	val: 0.858348	test: 0.924420
MAE train: 0.659305	val: 0.668782	test: 0.725700

Epoch: 8
Loss: 0.7925946474075317
RMSE train: 0.832508	val: 0.855229	test: 0.913339
MAE train: 0.645770	val: 0.654909	test: 0.712330

Epoch: 9
Loss: 0.7921578049659729
RMSE train: 0.819382	val: 0.834152	test: 0.907444
MAE train: 0.641706	val: 0.643692	test: 0.708611

Epoch: 10
Loss: 0.7633263647556305
RMSE train: 0.854412	val: 0.857212	test: 0.930858
MAE train: 0.664098	val: 0.668526	test: 0.723341

Epoch: 11
Loss: 0.7339893579483032
RMSE train: 0.826289	val: 0.836726	test: 0.908581
MAE train: 0.644170	val: 0.651999	test: 0.706648

Epoch: 12
Loss: 0.720834344625473
RMSE train: 0.832782	val: 0.838874	test: 0.909195
MAE train: 0.644946	val: 0.651253	test: 0.705027

Epoch: 13
Loss: 0.7611489772796631
RMSE train: 0.828915	val: 0.851608	test: 0.917495
MAE train: 0.639476	val: 0.658057	test: 0.711844

Epoch: 14
Loss: 0.7075287342071533
RMSE train: 0.772964	val: 0.804628	test: 0.862821
MAE train: 0.597502	val: 0.618283	test: 0.668858

Epoch: 15
Loss: 0.6768674373626709
RMSE train: 0.723740	val: 0.770223	test: 0.816520
MAE train: 0.570955	val: 0.591091	test: 0.638645

Epoch: 16
Loss: 0.6639143884181976
RMSE train: 0.720599	val: 0.777228	test: 0.806068
MAE train: 0.562596	val: 0.593547	test: 0.628496

Epoch: 17
Loss: 0.6526845932006836
RMSE train: 0.714580	val: 0.769302	test: 0.812932
MAE train: 0.563907	val: 0.587150	test: 0.637827

Epoch: 18
Loss: 0.6420407652854919
RMSE train: 0.704007	val: 0.754509	test: 0.804502
MAE train: 0.554000	val: 0.581390	test: 0.621914

Epoch: 19
Loss: 0.6155757188796998
RMSE train: 0.785529	val: 0.823931	test: 0.863803
MAE train: 0.609154	val: 0.630549	test: 0.659608

Epoch: 20
Loss: 0.6339246273040772
RMSE train: 0.714855	val: 0.768099	test: 0.812442
MAE train: 0.559351	val: 0.585087	test: 0.629475

Epoch: 21
Loss: 0.6079323828220368
RMSE train: 0.702618	val: 0.758171	test: 0.801148
MAE train: 0.552917	val: 0.580939	test: 0.616109

Epoch: 22
Loss: 0.5903469771146774
RMSE train: 0.675278	val: 0.762730	test: 0.779871
MAE train: 0.535501	val: 0.575468	test: 0.602912Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/lipo/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/lipo/random/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/lipo/random/train_prop=0.7/lipophilicity_random_5_26-05_09-47-41  ]
[ Using Seed :  5  ]
[ Using device :  cuda:1  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.439654111862183
RMSE train: 2.239737	val: 2.338516	test: 2.268423
MAE train: 2.002528	val: 2.095581	test: 2.018109

Epoch: 2
Loss: 3.750683764616648
RMSE train: 1.993994	val: 2.082706	test: 2.009752
MAE train: 1.764008	val: 1.841623	test: 1.769038

Epoch: 3
Loss: 2.4675807654857635
RMSE train: 1.778477	val: 1.848908	test: 1.782977
MAE train: 1.571682	val: 1.627842	test: 1.562783

Epoch: 4
Loss: 1.6034883360068004
RMSE train: 1.157254	val: 1.227229	test: 1.183582
MAE train: 0.966688	val: 1.026899	test: 0.994123

Epoch: 5
Loss: 1.0904841125011444
RMSE train: 0.885543	val: 0.949121	test: 0.942276
MAE train: 0.708032	val: 0.763498	test: 0.753480

Epoch: 6
Loss: 0.8993140558401743
RMSE train: 0.846685	val: 0.905115	test: 0.910521
MAE train: 0.659587	val: 0.713259	test: 0.713627

Epoch: 7
Loss: 0.8400900214910507
RMSE train: 0.873325	val: 0.922954	test: 0.937382
MAE train: 0.678094	val: 0.728254	test: 0.733622

Epoch: 8
Loss: 0.8196630378564199
RMSE train: 0.820351	val: 0.881047	test: 0.890949
MAE train: 0.640920	val: 0.697566	test: 0.699979

Epoch: 9
Loss: 0.7787658721208572
RMSE train: 0.795240	val: 0.862264	test: 0.855542
MAE train: 0.622193	val: 0.679436	test: 0.673678

Epoch: 10
Loss: 0.743499830365181
RMSE train: 0.804499	val: 0.854365	test: 0.871685
MAE train: 0.624582	val: 0.675076	test: 0.682895

Epoch: 11
Loss: 0.7041770120461782
RMSE train: 0.764645	val: 0.833543	test: 0.823012
MAE train: 0.591859	val: 0.649149	test: 0.648455

Epoch: 12
Loss: 0.7035607596238455
RMSE train: 0.780047	val: 0.853627	test: 0.853578
MAE train: 0.606845	val: 0.670569	test: 0.663791

Epoch: 13
Loss: 0.6937215576569239
RMSE train: 0.737545	val: 0.822538	test: 0.806335
MAE train: 0.578173	val: 0.647612	test: 0.631859

Epoch: 14
Loss: 0.698319653669993
RMSE train: 0.772511	val: 0.839031	test: 0.837204
MAE train: 0.604064	val: 0.659235	test: 0.651236

Epoch: 15
Loss: 0.6545296311378479
RMSE train: 0.727680	val: 0.812458	test: 0.789756
MAE train: 0.565580	val: 0.628099	test: 0.615375

Epoch: 16
Loss: 0.6583362619082133
RMSE train: 0.727386	val: 0.802434	test: 0.771652
MAE train: 0.569645	val: 0.623961	test: 0.602100

Epoch: 17
Loss: 0.6524702658255895
RMSE train: 0.712974	val: 0.804394	test: 0.780018
MAE train: 0.558940	val: 0.629165	test: 0.612176

Epoch: 18
Loss: 0.6419920672972997
RMSE train: 0.724481	val: 0.808059	test: 0.779685
MAE train: 0.568714	val: 0.626516	test: 0.608396

Epoch: 19
Loss: 0.603923887014389
RMSE train: 0.739396	val: 0.819214	test: 0.809182
MAE train: 0.575834	val: 0.631372	test: 0.632141

Epoch: 20
Loss: 0.5916300565004349
RMSE train: 0.701871	val: 0.791323	test: 0.778518
MAE train: 0.544679	val: 0.608584	test: 0.610708

Epoch: 21
Loss: 0.5831120659907659
RMSE train: 0.708920	val: 0.805860	test: 0.784349
MAE train: 0.553449	val: 0.619067	test: 0.609795

Epoch: 22
Loss: 0.5757974485556284
RMSE train: 0.693642	val: 0.787420	test: 0.754605
MAE train: 0.541992	val: 0.602727	test: 0.587346Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/lipo/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/lipo/random/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/lipo/random/train_prop=0.7/lipophilicity_random_6_26-05_09-47-41  ]
[ Using Seed :  6  ]
[ Using device :  cuda:1  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.842098315556844
RMSE train: 2.513039	val: 2.621007	test: 2.554466
MAE train: 2.271902	val: 2.368546	test: 2.299299

Epoch: 2
Loss: 4.112918337186177
RMSE train: 2.211355	val: 2.296223	test: 2.233602
MAE train: 1.982877	val: 2.056931	test: 1.995155

Epoch: 3
Loss: 2.745934545993805
RMSE train: 1.681321	val: 1.755086	test: 1.708176
MAE train: 1.474133	val: 1.533118	test: 1.489580

Epoch: 4
Loss: 1.7819144825140636
RMSE train: 1.114890	val: 1.195161	test: 1.154516
MAE train: 0.931231	val: 0.986718	test: 0.967479

Epoch: 5
Loss: 1.204615294933319
RMSE train: 0.893179	val: 0.976270	test: 0.932875
MAE train: 0.719957	val: 0.785961	test: 0.762075

Epoch: 6
Loss: 0.911530484755834
RMSE train: 0.906733	val: 0.953291	test: 0.947666
MAE train: 0.702982	val: 0.754809	test: 0.744526

Epoch: 7
Loss: 0.8558138410250345
RMSE train: 0.835758	val: 0.888839	test: 0.883600
MAE train: 0.644567	val: 0.693552	test: 0.692374

Epoch: 8
Loss: 0.8085000912348429
RMSE train: 0.815514	val: 0.889759	test: 0.877188
MAE train: 0.632847	val: 0.698754	test: 0.685437

Epoch: 9
Loss: 0.7569987177848816
RMSE train: 0.793775	val: 0.862202	test: 0.845670
MAE train: 0.619617	val: 0.676390	test: 0.663367

Epoch: 10
Loss: 0.7512869834899902
RMSE train: 0.762478	val: 0.854530	test: 0.832968
MAE train: 0.598081	val: 0.666613	test: 0.651723

Epoch: 11
Loss: 0.700071781873703
RMSE train: 0.764013	val: 0.851477	test: 0.831547
MAE train: 0.600251	val: 0.664306	test: 0.651342

Epoch: 12
Loss: 0.6985099166631699
RMSE train: 0.779357	val: 0.850486	test: 0.839200
MAE train: 0.611138	val: 0.662976	test: 0.657691

Epoch: 13
Loss: 0.6928749034802119
RMSE train: 0.746816	val: 0.840275	test: 0.810523
MAE train: 0.586361	val: 0.650467	test: 0.633898

Epoch: 14
Loss: 0.6475238005320231
RMSE train: 0.732598	val: 0.825811	test: 0.805403
MAE train: 0.574084	val: 0.637120	test: 0.634367

Epoch: 15
Loss: 0.6415249953667322
RMSE train: 0.710578	val: 0.809614	test: 0.776649
MAE train: 0.559516	val: 0.618251	test: 0.606612

Epoch: 16
Loss: 0.6202096740404764
RMSE train: 0.755562	val: 0.838303	test: 0.819884
MAE train: 0.590706	val: 0.648288	test: 0.637148

Epoch: 17
Loss: 0.6246352394421896
RMSE train: 0.712353	val: 0.819577	test: 0.797090
MAE train: 0.561166	val: 0.629859	test: 0.624078

Epoch: 18
Loss: 0.6015005658070246
RMSE train: 0.722275	val: 0.826730	test: 0.797953
MAE train: 0.566247	val: 0.630135	test: 0.623859

Epoch: 19
Loss: 0.6385079870621363
RMSE train: 0.694614	val: 0.804100	test: 0.773454
MAE train: 0.546260	val: 0.614850	test: 0.606861

Epoch: 20
Loss: 0.5910227547089258
RMSE train: 0.708042	val: 0.809400	test: 0.789657
MAE train: 0.557898	val: 0.623069	test: 0.614545

Epoch: 21
Loss: 0.5956998318433762
RMSE train: 0.694773	val: 0.806721	test: 0.768793
MAE train: 0.546924	val: 0.616810	test: 0.601478

Epoch: 22
Loss: 0.580952192346255
RMSE train: 0.685286	val: 0.795549	test: 0.769392
MAE train: 0.540122	val: 0.605407	test: 0.601341Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/lipo/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/lipo/random/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/lipo/random/train_prop=0.7/lipophilicity_random_4_26-05_09-47-41  ]
[ Using Seed :  4  ]
[ Using device :  cuda:1  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.233063578605652
RMSE train: 2.289379	val: 2.382692	test: 2.324251
MAE train: 2.046352	val: 2.132252	test: 2.070631

Epoch: 2
Loss: 3.58636611700058
RMSE train: 2.042122	val: 2.132900	test: 2.062406
MAE train: 1.810491	val: 1.886194	test: 1.817888

Epoch: 3
Loss: 2.3727795084317527
RMSE train: 1.731304	val: 1.814334	test: 1.749256
MAE train: 1.519284	val: 1.587591	test: 1.522484

Epoch: 4
Loss: 1.495959500471751
RMSE train: 1.133457	val: 1.207954	test: 1.154871
MAE train: 0.940105	val: 0.996923	test: 0.964393

Epoch: 5
Loss: 1.068411409854889
RMSE train: 0.947322	val: 1.020977	test: 0.969474
MAE train: 0.768746	val: 0.827817	test: 0.794376

Epoch: 6
Loss: 0.9224178791046143
RMSE train: 0.911215	val: 0.979590	test: 0.937365
MAE train: 0.741319	val: 0.799776	test: 0.778446

Epoch: 7
Loss: 0.8472054451704025
RMSE train: 0.828005	val: 0.884948	test: 0.869577
MAE train: 0.644170	val: 0.703725	test: 0.686316

Epoch: 8
Loss: 0.7977775732676188
RMSE train: 0.806824	val: 0.865252	test: 0.852350
MAE train: 0.630558	val: 0.683894	test: 0.671528

Epoch: 9
Loss: 0.7563493500153223
RMSE train: 0.804259	val: 0.860359	test: 0.855210
MAE train: 0.626210	val: 0.680816	test: 0.672736

Epoch: 10
Loss: 0.7340747465689977
RMSE train: 0.792336	val: 0.852011	test: 0.835450
MAE train: 0.615669	val: 0.668435	test: 0.650329

Epoch: 11
Loss: 0.7370202491680781
RMSE train: 0.801272	val: 0.858177	test: 0.849456
MAE train: 0.622475	val: 0.672922	test: 0.661672

Epoch: 12
Loss: 0.6988367338975271
RMSE train: 0.739437	val: 0.814591	test: 0.795908
MAE train: 0.581926	val: 0.636508	test: 0.625960

Epoch: 13
Loss: 0.6706770956516266
RMSE train: 0.734492	val: 0.817047	test: 0.798570
MAE train: 0.574601	val: 0.639337	test: 0.623528

Epoch: 14
Loss: 0.6937799354394277
RMSE train: 0.750676	val: 0.817845	test: 0.801981
MAE train: 0.587056	val: 0.638146	test: 0.627698

Epoch: 15
Loss: 0.6396676053603491
RMSE train: 0.744780	val: 0.811823	test: 0.781979
MAE train: 0.587869	val: 0.636012	test: 0.619830

Epoch: 16
Loss: 0.6887330164511999
RMSE train: 0.829667	val: 0.893394	test: 0.890457
MAE train: 0.641404	val: 0.693972	test: 0.684714

Epoch: 17
Loss: 0.6358410815397898
RMSE train: 0.719641	val: 0.808355	test: 0.772301
MAE train: 0.566237	val: 0.628772	test: 0.608526

Epoch: 18
Loss: 0.6167277346054713
RMSE train: 0.720565	val: 0.794624	test: 0.773927
MAE train: 0.565310	val: 0.617134	test: 0.604852

Epoch: 19
Loss: 0.6045089860757192
RMSE train: 0.702691	val: 0.783164	test: 0.761394
MAE train: 0.547145	val: 0.605751	test: 0.589515

Epoch: 20
Loss: 0.6016995360453924
RMSE train: 0.703348	val: 0.776494	test: 0.762120
MAE train: 0.549536	val: 0.602918	test: 0.591135

Epoch: 21
Loss: 0.5722859973708788
RMSE train: 0.693004	val: 0.777528	test: 0.768961
MAE train: 0.545122	val: 0.603870	test: 0.601314

Epoch: 22
Loss: 0.5949246635039648
RMSE train: 0.678023	val: 0.777707	test: 0.747285
MAE train: 0.532379	val: 0.598673	test: 0.582488Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/lipo/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/lipo/random/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/lipo/random/train_prop=0.8/lipophilicity_random_4_26-05_09-47-41  ]
[ Using Seed :  4  ]
[ Using device :  cuda:2  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.225655044828143
RMSE train: 2.107944	val: 2.126742	test: 2.149467
MAE train: 1.861514	val: 1.862827	test: 1.904255

Epoch: 2
Loss: 3.369029266493661
RMSE train: 1.796563	val: 1.818530	test: 1.837898
MAE train: 1.566266	val: 1.572669	test: 1.608065

Epoch: 3
Loss: 2.0376525350979398
RMSE train: 1.342326	val: 1.381264	test: 1.391743
MAE train: 1.138437	val: 1.164567	test: 1.186298

Epoch: 4
Loss: 1.2742120282990592
RMSE train: 0.941964	val: 0.994303	test: 0.964661
MAE train: 0.766688	val: 0.800445	test: 0.798179

Epoch: 5
Loss: 0.9228839916842324
RMSE train: 0.861992	val: 0.945907	test: 0.921838
MAE train: 0.677746	val: 0.745230	test: 0.714729

Epoch: 6
Loss: 0.8047616566930499
RMSE train: 0.839415	val: 0.945921	test: 0.895162
MAE train: 0.652445	val: 0.736448	test: 0.706769

Epoch: 7
Loss: 0.7968166470527649
RMSE train: 0.827250	val: 0.937063	test: 0.887722
MAE train: 0.644585	val: 0.733629	test: 0.690472

Epoch: 8
Loss: 0.803081499678748
RMSE train: 0.870912	val: 0.972450	test: 0.905662
MAE train: 0.677450	val: 0.758758	test: 0.695217

Epoch: 9
Loss: 0.7557410555226463
RMSE train: 0.917875	val: 1.022096	test: 0.975947
MAE train: 0.717872	val: 0.809040	test: 0.765318

Epoch: 10
Loss: 0.7468731573649815
RMSE train: 0.803430	val: 0.921534	test: 0.862791
MAE train: 0.622881	val: 0.716746	test: 0.667387

Epoch: 11
Loss: 0.756718214069094
RMSE train: 0.767179	val: 0.872923	test: 0.814978
MAE train: 0.598223	val: 0.688729	test: 0.634170

Epoch: 12
Loss: 0.6795167071478707
RMSE train: 0.749475	val: 0.856528	test: 0.788384
MAE train: 0.580094	val: 0.668024	test: 0.611463

Epoch: 13
Loss: 0.6910330866064344
RMSE train: 0.733150	val: 0.851581	test: 0.798409
MAE train: 0.567518	val: 0.661776	test: 0.620781

Epoch: 14
Loss: 0.6619725908551898
RMSE train: 0.739749	val: 0.848010	test: 0.792581
MAE train: 0.574404	val: 0.661622	test: 0.608059

Epoch: 15
Loss: 0.6588721701077053
RMSE train: 0.718560	val: 0.830829	test: 0.762037
MAE train: 0.558381	val: 0.650883	test: 0.586726

Epoch: 16
Loss: 0.6520409647907529
RMSE train: 0.719543	val: 0.852648	test: 0.765920
MAE train: 0.558110	val: 0.662911	test: 0.587858

Epoch: 17
Loss: 0.6208360322884151
RMSE train: 0.742014	val: 0.871423	test: 0.777908
MAE train: 0.576423	val: 0.678639	test: 0.587663

Epoch: 18
Loss: 0.6293168706553323
RMSE train: 0.721655	val: 0.839568	test: 0.748844
MAE train: 0.557284	val: 0.654037	test: 0.572248

Epoch: 19
Loss: 0.6058830193110875
RMSE train: 0.704649	val: 0.832111	test: 0.746922
MAE train: 0.550166	val: 0.652572	test: 0.569972

Epoch: 20
Loss: 0.6135679398264203
RMSE train: 0.728256	val: 0.858018	test: 0.771374
MAE train: 0.566823	val: 0.666047	test: 0.589013

Epoch: 21
Loss: 0.5936331834111895
RMSE train: 0.705069	val: 0.840658	test: 0.744702
MAE train: 0.551598	val: 0.650526	test: 0.573861

Epoch: 22
Loss: 0.6114161674465451
RMSE train: 0.723558	val: 0.866777	test: 0.788760
MAE train: 0.564820	val: 0.678547	test: 0.604174Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/lipo/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/lipo/random/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/lipo/random/train_prop=0.8/lipophilicity_random_6_26-05_09-47-41  ]
[ Using Seed :  6  ]
[ Using device :  cuda:2  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.778412103652954
RMSE train: 2.435531	val: 2.453509	test: 2.464965
MAE train: 2.198824	val: 2.192800	test: 2.220825

Epoch: 2
Loss: 3.794210842677525
RMSE train: 1.995885	val: 2.018732	test: 2.025492
MAE train: 1.765691	val: 1.762566	test: 1.792942

Epoch: 3
Loss: 2.3937585779598782
RMSE train: 1.558719	val: 1.577502	test: 1.591294
MAE train: 1.347023	val: 1.351819	test: 1.377901

Epoch: 4
Loss: 1.4462129473686218
RMSE train: 1.129367	val: 1.173962	test: 1.165143
MAE train: 0.940220	val: 0.966320	test: 0.980253

Epoch: 5
Loss: 0.9601627673421588
RMSE train: 0.847045	val: 0.921554	test: 0.905372
MAE train: 0.667110	val: 0.724876	test: 0.699830

Epoch: 6
Loss: 0.881252429315022
RMSE train: 0.872464	val: 0.971975	test: 0.910609
MAE train: 0.675814	val: 0.773560	test: 0.694450

Epoch: 7
Loss: 0.8423691477094378
RMSE train: 0.797712	val: 0.908586	test: 0.861916
MAE train: 0.622580	val: 0.720969	test: 0.667302

Epoch: 8
Loss: 0.7854832410812378
RMSE train: 0.844810	val: 0.934882	test: 0.872847
MAE train: 0.660074	val: 0.736458	test: 0.671835

Epoch: 9
Loss: 0.7765368819236755
RMSE train: 0.792608	val: 0.905253	test: 0.828625
MAE train: 0.617100	val: 0.709681	test: 0.632329

Epoch: 10
Loss: 0.7906346874577659
RMSE train: 0.764802	val: 0.885231	test: 0.815686
MAE train: 0.596333	val: 0.696256	test: 0.634620

Epoch: 11
Loss: 0.709818708045142
RMSE train: 0.770578	val: 0.878375	test: 0.806254
MAE train: 0.603776	val: 0.695657	test: 0.632356

Epoch: 12
Loss: 0.684032712663923
RMSE train: 0.793290	val: 0.914058	test: 0.841053
MAE train: 0.618643	val: 0.712670	test: 0.642363

Epoch: 13
Loss: 0.7144528712545123
RMSE train: 0.737987	val: 0.859936	test: 0.785382
MAE train: 0.577880	val: 0.672878	test: 0.611041

Epoch: 14
Loss: 0.6844454620565686
RMSE train: 0.765819	val: 0.892501	test: 0.830979
MAE train: 0.597587	val: 0.706210	test: 0.633900

Epoch: 15
Loss: 0.6400443698678698
RMSE train: 0.740911	val: 0.854802	test: 0.786854
MAE train: 0.581569	val: 0.677083	test: 0.614678

Epoch: 16
Loss: 0.6508553496428898
RMSE train: 0.743188	val: 0.874005	test: 0.812493
MAE train: 0.580093	val: 0.685091	test: 0.621561

Epoch: 17
Loss: 0.6452161584581647
RMSE train: 0.767918	val: 0.891601	test: 0.820169
MAE train: 0.601689	val: 0.700883	test: 0.625508

Epoch: 18
Loss: 0.6216030759470803
RMSE train: 0.711243	val: 0.842769	test: 0.773084
MAE train: 0.556712	val: 0.657875	test: 0.593526

Epoch: 19
Loss: 0.609669451202665
RMSE train: 0.736563	val: 0.872994	test: 0.802232
MAE train: 0.578773	val: 0.682394	test: 0.609018

Epoch: 20
Loss: 0.6316388079098293
RMSE train: 0.704129	val: 0.856878	test: 0.771538
MAE train: 0.549694	val: 0.666231	test: 0.594778

Epoch: 21
Loss: 0.5991639282022204
RMSE train: 0.697211	val: 0.843652	test: 0.774290
MAE train: 0.543170	val: 0.654883	test: 0.593110

Epoch: 22
Loss: 0.6006198674440384
RMSE train: 0.692485	val: 0.838059	test: 0.782731
MAE train: 0.544985	val: 0.656974	test: 0.605354Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/lipo/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/lipo/random/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/lipo/random/train_prop=0.8/lipophilicity_random_5_26-05_09-47-41  ]
[ Using Seed :  5  ]
[ Using device :  cuda:2  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.364467518670218
RMSE train: 2.209221	val: 2.228859	test: 2.219572
MAE train: 1.970905	val: 1.969325	test: 1.975144

Epoch: 2
Loss: 3.5676343611308505
RMSE train: 1.933895	val: 1.957677	test: 1.958196
MAE train: 1.708828	val: 1.710285	test: 1.727047

Epoch: 3
Loss: 2.126374508653368
RMSE train: 1.339330	val: 1.391424	test: 1.384336
MAE train: 1.127079	val: 1.158932	test: 1.170112

Epoch: 4
Loss: 1.308921686240605
RMSE train: 0.998334	val: 1.065981	test: 1.069687
MAE train: 0.807821	val: 0.857629	test: 0.878516

Epoch: 5
Loss: 0.9082751018660409
RMSE train: 0.859129	val: 0.962317	test: 0.918663
MAE train: 0.676004	val: 0.750540	test: 0.728196

Epoch: 6
Loss: 0.8953561016491481
RMSE train: 0.887728	val: 1.001181	test: 0.937824
MAE train: 0.693794	val: 0.784513	test: 0.718775

Epoch: 7
Loss: 0.8048490285873413
RMSE train: 0.940065	val: 1.064423	test: 0.977592
MAE train: 0.739909	val: 0.843736	test: 0.756302

Epoch: 8
Loss: 0.7651962893349784
RMSE train: 0.791164	val: 0.901932	test: 0.821224
MAE train: 0.618501	val: 0.707951	test: 0.646855

Epoch: 9
Loss: 0.7372872573988778
RMSE train: 0.837586	val: 0.954828	test: 0.893081
MAE train: 0.655264	val: 0.747024	test: 0.687038

Epoch: 10
Loss: 0.7514145970344543
RMSE train: 0.794257	val: 0.927241	test: 0.835167
MAE train: 0.621114	val: 0.718422	test: 0.643779

Epoch: 11
Loss: 0.7260895669460297
RMSE train: 0.776731	val: 0.901138	test: 0.807264
MAE train: 0.608370	val: 0.705588	test: 0.635335

Epoch: 12
Loss: 0.7145224298749652
RMSE train: 0.747355	val: 0.875093	test: 0.803757
MAE train: 0.583506	val: 0.691371	test: 0.628760

Epoch: 13
Loss: 0.7036000234740121
RMSE train: 0.761219	val: 0.889747	test: 0.822479
MAE train: 0.591293	val: 0.688624	test: 0.631717

Epoch: 14
Loss: 0.6805599502154759
RMSE train: 0.796297	val: 0.929387	test: 0.851192
MAE train: 0.621597	val: 0.726316	test: 0.647522

Epoch: 15
Loss: 0.702085439647947
RMSE train: 0.846619	val: 0.971707	test: 0.904299
MAE train: 0.661910	val: 0.755564	test: 0.695927

Epoch: 16
Loss: 0.6717866914612907
RMSE train: 0.763792	val: 0.910006	test: 0.830380
MAE train: 0.598193	val: 0.705827	test: 0.640908

Epoch: 17
Loss: 0.6532337239810398
RMSE train: 0.754844	val: 0.883954	test: 0.820908
MAE train: 0.593783	val: 0.694216	test: 0.628956

Epoch: 18
Loss: 0.6225149759224483
RMSE train: 0.761161	val: 0.895600	test: 0.818570
MAE train: 0.595555	val: 0.697632	test: 0.622839

Epoch: 19
Loss: 0.6181732245853969
RMSE train: 0.738890	val: 0.884303	test: 0.781880
MAE train: 0.578619	val: 0.681592	test: 0.607204

Epoch: 20
Loss: 0.5967978558370045
RMSE train: 0.729841	val: 0.879558	test: 0.763852
MAE train: 0.572107	val: 0.673303	test: 0.589826

Epoch: 21
Loss: 0.5931520334311894
RMSE train: 0.703226	val: 0.846735	test: 0.756250
MAE train: 0.549326	val: 0.655230	test: 0.584671

Epoch: 22
Loss: 0.5891049930027553
RMSE train: 0.698935	val: 0.837834	test: 0.757306
MAE train: 0.548714	val: 0.645489	test: 0.578020

Epoch: 23
Loss: 0.5844661355018616
RMSE train: 0.729715	val: 0.794327	test: 0.836497
MAE train: 0.571182	val: 0.613610	test: 0.643255

Epoch: 24
Loss: 0.550706896185875
RMSE train: 0.710147	val: 0.785497	test: 0.810781
MAE train: 0.559803	val: 0.602394	test: 0.625922

Epoch: 25
Loss: 0.576206910610199
RMSE train: 0.695919	val: 0.778633	test: 0.794697
MAE train: 0.548944	val: 0.596067	test: 0.618645

Epoch: 26
Loss: 0.568131685256958
RMSE train: 0.669162	val: 0.776829	test: 0.783995
MAE train: 0.527285	val: 0.584848	test: 0.607541

Epoch: 27
Loss: 0.5500417917966842
RMSE train: 0.709394	val: 0.786037	test: 0.821090
MAE train: 0.559912	val: 0.603450	test: 0.638678

Epoch: 28
Loss: 0.557569545507431
RMSE train: 0.683569	val: 0.764926	test: 0.801084
MAE train: 0.540203	val: 0.587375	test: 0.621790

Epoch: 29
Loss: 0.5409263104200364
RMSE train: 0.685800	val: 0.771663	test: 0.794256
MAE train: 0.542270	val: 0.585946	test: 0.615965

Epoch: 30
Loss: 0.5191251248121261
RMSE train: 0.660402	val: 0.754554	test: 0.773860
MAE train: 0.520746	val: 0.573252	test: 0.595799

Epoch: 31
Loss: 0.5098090529441833
RMSE train: 0.677251	val: 0.773110	test: 0.800550
MAE train: 0.531947	val: 0.581193	test: 0.614214

Epoch: 32
Loss: 0.5166819602251053
RMSE train: 0.659455	val: 0.754973	test: 0.786335
MAE train: 0.520063	val: 0.570986	test: 0.606878

Epoch: 33
Loss: 0.5043794989585877
RMSE train: 0.695774	val: 0.789550	test: 0.811451
MAE train: 0.550414	val: 0.600968	test: 0.626383

Epoch: 34
Loss: 0.4901741862297058
RMSE train: 0.674768	val: 0.770408	test: 0.795014
MAE train: 0.533371	val: 0.584423	test: 0.613162

Epoch: 35
Loss: 0.5176763951778411
RMSE train: 0.698425	val: 0.778359	test: 0.816512
MAE train: 0.547803	val: 0.600407	test: 0.631339

Epoch: 36
Loss: 0.48020587861537933
RMSE train: 0.671542	val: 0.765083	test: 0.791112
MAE train: 0.531634	val: 0.583516	test: 0.611864

Epoch: 37
Loss: 0.4624380886554718
RMSE train: 0.701188	val: 0.782489	test: 0.818065
MAE train: 0.550348	val: 0.596361	test: 0.624795

Epoch: 38
Loss: 0.4712393760681152
RMSE train: 0.691963	val: 0.776901	test: 0.816768
MAE train: 0.542416	val: 0.597229	test: 0.623450

Epoch: 39
Loss: 0.46279181241989137
RMSE train: 0.669318	val: 0.758025	test: 0.795853
MAE train: 0.528226	val: 0.570560	test: 0.607109

Epoch: 40
Loss: 0.4575623840093613
RMSE train: 0.650100	val: 0.754564	test: 0.773919
MAE train: 0.514611	val: 0.565066	test: 0.589623

Epoch: 41
Loss: 0.4657765805721283
RMSE train: 0.639303	val: 0.735387	test: 0.766038
MAE train: 0.500457	val: 0.560197	test: 0.585584

Epoch: 42
Loss: 0.46645253002643583
RMSE train: 0.667091	val: 0.746908	test: 0.792315
MAE train: 0.523113	val: 0.572142	test: 0.606304

Epoch: 43
Loss: 0.4580905556678772
RMSE train: 0.632550	val: 0.733398	test: 0.767495
MAE train: 0.499283	val: 0.557460	test: 0.587782

Epoch: 44
Loss: 0.44544294774532317
RMSE train: 0.632694	val: 0.731696	test: 0.768395
MAE train: 0.499164	val: 0.549535	test: 0.588266

Epoch: 45
Loss: 0.43922958970069886
RMSE train: 0.651909	val: 0.753843	test: 0.785639
MAE train: 0.514308	val: 0.567952	test: 0.598346

Epoch: 46
Loss: 0.43530348539352415
RMSE train: 0.650146	val: 0.749332	test: 0.785930
MAE train: 0.510066	val: 0.565314	test: 0.598559

Epoch: 47
Loss: 0.44445700347423556
RMSE train: 0.645217	val: 0.744367	test: 0.784561
MAE train: 0.505658	val: 0.561572	test: 0.596190

Epoch: 48
Loss: 0.4216391026973724
RMSE train: 0.615253	val: 0.734288	test: 0.747425
MAE train: 0.483171	val: 0.548401	test: 0.570666

Epoch: 49
Loss: 0.4260771363973618
RMSE train: 0.607043	val: 0.721605	test: 0.753282
MAE train: 0.478076	val: 0.542670	test: 0.574287

Epoch: 50
Loss: 0.42661207616329194
RMSE train: 0.615978	val: 0.734932	test: 0.753379
MAE train: 0.485160	val: 0.548072	test: 0.571861

Epoch: 51
Loss: 0.40447880923748014
RMSE train: 0.602992	val: 0.726740	test: 0.740037
MAE train: 0.476673	val: 0.543587	test: 0.566164

Epoch: 52
Loss: 0.399453866481781
RMSE train: 0.616993	val: 0.740424	test: 0.762528
MAE train: 0.489160	val: 0.549018	test: 0.584237

Epoch: 53
Loss: 0.40391563773155215
RMSE train: 0.592206	val: 0.728758	test: 0.740001
MAE train: 0.466462	val: 0.540171	test: 0.565496

Epoch: 54
Loss: 0.4139135807752609
RMSE train: 0.591316	val: 0.717699	test: 0.740360
MAE train: 0.467259	val: 0.537856	test: 0.568749

Epoch: 55
Loss: 0.3959421306848526
RMSE train: 0.576150	val: 0.713105	test: 0.733114
MAE train: 0.454597	val: 0.528770	test: 0.558876

Epoch: 56
Loss: 0.40414205491542815
RMSE train: 0.619836	val: 0.743683	test: 0.770889
MAE train: 0.488141	val: 0.556457	test: 0.592709

Epoch: 57
Loss: 0.39810691475868226
RMSE train: 0.564901	val: 0.707224	test: 0.731323
MAE train: 0.446469	val: 0.528929	test: 0.560381

Epoch: 58
Loss: 0.39113942086696624
RMSE train: 0.623008	val: 0.753678	test: 0.773409
MAE train: 0.493450	val: 0.554396	test: 0.591112

Epoch: 59
Loss: 0.3813993901014328
RMSE train: 0.587135	val: 0.732351	test: 0.747292
MAE train: 0.466237	val: 0.547170	test: 0.572629

Epoch: 60
Loss: 0.38313219249248504
RMSE train: 0.603732	val: 0.730409	test: 0.764754
MAE train: 0.473650	val: 0.548274	test: 0.582160

Epoch: 61
Loss: 0.3915021985769272
RMSE train: 0.586820	val: 0.721382	test: 0.743677
MAE train: 0.461350	val: 0.542488	test: 0.568468

Epoch: 62
Loss: 0.3696091860532761
RMSE train: 0.582694	val: 0.722067	test: 0.742856
MAE train: 0.460650	val: 0.537946	test: 0.565062

Epoch: 63
Loss: 0.3644097089767456
RMSE train: 0.582620	val: 0.724012	test: 0.745955
MAE train: 0.459181	val: 0.530305	test: 0.564916

Epoch: 64
Loss: 0.39309189319610593
RMSE train: 0.560687	val: 0.705346	test: 0.734186
MAE train: 0.444094	val: 0.526261	test: 0.557453

Epoch: 65
Loss: 0.3743948698043823
RMSE train: 0.560856	val: 0.707090	test: 0.729864
MAE train: 0.443927	val: 0.527678	test: 0.554808

Epoch: 66
Loss: 0.36611777544021606
RMSE train: 0.567923	val: 0.707208	test: 0.725289
MAE train: 0.447207	val: 0.523394	test: 0.553361

Epoch: 67
Loss: 0.3621269315481186
RMSE train: 0.560632	val: 0.710602	test: 0.726076
MAE train: 0.444844	val: 0.525787	test: 0.553254

Epoch: 68
Loss: 0.3543757826089859
RMSE train: 0.550705	val: 0.717212	test: 0.728420
MAE train: 0.435948	val: 0.533874	test: 0.557781

Epoch: 69
Loss: 0.35409807562828066
RMSE train: 0.562898	val: 0.712850	test: 0.727576
MAE train: 0.443822	val: 0.526461	test: 0.557217

Epoch: 70
Loss: 0.345661923289299
RMSE train: 0.554807	val: 0.712430	test: 0.731256
MAE train: 0.437304	val: 0.525545	test: 0.559259

Epoch: 71
Loss: 0.34871222376823424
RMSE train: 0.560733	val: 0.719869	test: 0.728519
MAE train: 0.440874	val: 0.522851	test: 0.557000

Epoch: 72
Loss: 0.36322674453258513
RMSE train: 0.553631	val: 0.713737	test: 0.715567
MAE train: 0.438811	val: 0.533403	test: 0.551325

Epoch: 73
Loss: 0.35171028077602384
RMSE train: 0.538934	val: 0.709174	test: 0.724234
MAE train: 0.424254	val: 0.528033	test: 0.549121

Epoch: 74
Loss: 0.33186595141887665
RMSE train: 0.546152	val: 0.713739	test: 0.719765
MAE train: 0.428487	val: 0.522748	test: 0.550197

Epoch: 75
Loss: 0.32508677393198016
RMSE train: 0.555739	val: 0.720325	test: 0.730840
MAE train: 0.437841	val: 0.528077	test: 0.548036

Epoch: 76
Loss: 0.3333912998437881
RMSE train: 0.555150	val: 0.716723	test: 0.731952
MAE train: 0.438407	val: 0.528607	test: 0.554214

Epoch: 77
Loss: 0.31815245747566223
RMSE train: 0.540922	val: 0.711488	test: 0.731220
MAE train: 0.427917	val: 0.527458	test: 0.556685

Epoch: 78
Loss: 0.33233997225761414
RMSE train: 0.554152	val: 0.735707	test: 0.731778
MAE train: 0.436807	val: 0.530737	test: 0.556290

Epoch: 79
Loss: 0.3345714628696442
RMSE train: 0.508848	val: 0.684334	test: 0.709685
MAE train: 0.399386	val: 0.507502	test: 0.536802

Epoch: 80
Loss: 0.32758685350418093
RMSE train: 0.552139	val: 0.715177	test: 0.728757
MAE train: 0.436785	val: 0.532585	test: 0.557923

Epoch: 81
Loss: 0.3213687360286713
RMSE train: 0.542350	val: 0.714074	test: 0.722483
MAE train: 0.428214	val: 0.524264	test: 0.550071

Epoch: 82
Loss: 0.3157923430204391
RMSE train: 0.529467	val: 0.703973	test: 0.717317
MAE train: 0.417961	val: 0.519289	test: 0.549357

Epoch: 83
Loss: 0.3243107318878174
RMSE train: 0.515498	val: 0.691814	test: 0.696520

Epoch: 23
Loss: 0.5818229764699936
RMSE train: 0.691086	val: 0.776723	test: 0.809296
MAE train: 0.545630	val: 0.592892	test: 0.630241

Epoch: 24
Loss: 0.5848113775253296
RMSE train: 0.746303	val: 0.812600	test: 0.845749
MAE train: 0.587287	val: 0.618720	test: 0.650953

Epoch: 25
Loss: 0.5674142479896546
RMSE train: 0.690247	val: 0.771207	test: 0.808593
MAE train: 0.542755	val: 0.583485	test: 0.623446

Epoch: 26
Loss: 0.5765892893075943
RMSE train: 0.719190	val: 0.792668	test: 0.830250
MAE train: 0.562773	val: 0.601715	test: 0.637989

Epoch: 27
Loss: 0.5501248002052307
RMSE train: 0.702392	val: 0.791774	test: 0.806789
MAE train: 0.554306	val: 0.597207	test: 0.627061

Epoch: 28
Loss: 0.5473308145999909
RMSE train: 0.662346	val: 0.765161	test: 0.781630
MAE train: 0.524340	val: 0.583407	test: 0.606732

Epoch: 29
Loss: 0.5445772141218186
RMSE train: 0.667226	val: 0.759009	test: 0.784270
MAE train: 0.526743	val: 0.576907	test: 0.613838

Epoch: 30
Loss: 0.5267284661531448
RMSE train: 0.661355	val: 0.761862	test: 0.783366
MAE train: 0.515900	val: 0.568508	test: 0.604579

Epoch: 31
Loss: 0.5389740496873856
RMSE train: 0.661300	val: 0.758111	test: 0.781104
MAE train: 0.522279	val: 0.576125	test: 0.610924

Epoch: 32
Loss: 0.49243910014629366
RMSE train: 0.692928	val: 0.775156	test: 0.810420
MAE train: 0.543669	val: 0.588658	test: 0.622131

Epoch: 33
Loss: 0.5033919841051102
RMSE train: 0.637059	val: 0.746723	test: 0.759005
MAE train: 0.505290	val: 0.562360	test: 0.591355

Epoch: 34
Loss: 0.4963586837053299
RMSE train: 0.630812	val: 0.746490	test: 0.756082
MAE train: 0.497178	val: 0.562261	test: 0.587658

Epoch: 35
Loss: 0.487999302148819
RMSE train: 0.626115	val: 0.743031	test: 0.745415
MAE train: 0.495158	val: 0.562676	test: 0.583338

Epoch: 36
Loss: 0.4787887066602707
RMSE train: 0.629394	val: 0.740585	test: 0.748081
MAE train: 0.497881	val: 0.556448	test: 0.578542

Epoch: 37
Loss: 0.48138711154460906
RMSE train: 0.627936	val: 0.740423	test: 0.750261
MAE train: 0.495641	val: 0.558563	test: 0.584988

Epoch: 38
Loss: 0.4769918203353882
RMSE train: 0.611570	val: 0.732028	test: 0.743887
MAE train: 0.479401	val: 0.548479	test: 0.578422

Epoch: 39
Loss: 0.468246591091156
RMSE train: 0.607059	val: 0.730269	test: 0.736976
MAE train: 0.480341	val: 0.545205	test: 0.571701

Epoch: 40
Loss: 0.47167096436023714
RMSE train: 0.620020	val: 0.727447	test: 0.751119
MAE train: 0.489234	val: 0.546901	test: 0.580502

Epoch: 41
Loss: 0.4618830233812332
RMSE train: 0.620663	val: 0.740495	test: 0.750389
MAE train: 0.488216	val: 0.554711	test: 0.575650

Epoch: 42
Loss: 0.44608933925628663
RMSE train: 0.623854	val: 0.744237	test: 0.758664
MAE train: 0.485644	val: 0.554886	test: 0.579396

Epoch: 43
Loss: 0.4622678875923157
RMSE train: 0.611733	val: 0.737101	test: 0.747958
MAE train: 0.483763	val: 0.555382	test: 0.575263

Epoch: 44
Loss: 0.44858518242836
RMSE train: 0.614485	val: 0.734193	test: 0.751794
MAE train: 0.482827	val: 0.551876	test: 0.578821

Epoch: 45
Loss: 0.45186729431152345
RMSE train: 0.605520	val: 0.728546	test: 0.754541
MAE train: 0.474517	val: 0.549851	test: 0.582101

Epoch: 46
Loss: 0.44725150167942046
RMSE train: 0.596332	val: 0.733667	test: 0.740922
MAE train: 0.467144	val: 0.544826	test: 0.568920

Epoch: 47
Loss: 0.44103367924690245
RMSE train: 0.599766	val: 0.726481	test: 0.738327
MAE train: 0.476521	val: 0.545783	test: 0.569569

Epoch: 48
Loss: 0.4197003483772278
RMSE train: 0.581415	val: 0.727447	test: 0.731182
MAE train: 0.459129	val: 0.540199	test: 0.562912

Epoch: 49
Loss: 0.4303033649921417
RMSE train: 0.588220	val: 0.737518	test: 0.739604
MAE train: 0.464388	val: 0.551889	test: 0.569555

Epoch: 50
Loss: 0.4216936320066452
RMSE train: 0.580644	val: 0.723994	test: 0.736315
MAE train: 0.457986	val: 0.540670	test: 0.561789

Epoch: 51
Loss: 0.3995454251766205
RMSE train: 0.590862	val: 0.738438	test: 0.728975
MAE train: 0.468605	val: 0.555638	test: 0.561697

Epoch: 52
Loss: 0.4140400797128677
RMSE train: 0.563100	val: 0.712366	test: 0.727210
MAE train: 0.443361	val: 0.531523	test: 0.554128

Epoch: 53
Loss: 0.4050442039966583
RMSE train: 0.590670	val: 0.733493	test: 0.736258
MAE train: 0.464031	val: 0.548085	test: 0.563114

Epoch: 54
Loss: 0.401949605345726
RMSE train: 0.572647	val: 0.720738	test: 0.720493
MAE train: 0.451293	val: 0.539431	test: 0.554339

Epoch: 55
Loss: 0.40703747272491453
RMSE train: 0.575742	val: 0.729465	test: 0.732303
MAE train: 0.454867	val: 0.545074	test: 0.560396

Epoch: 56
Loss: 0.39383832812309266
RMSE train: 0.574047	val: 0.722865	test: 0.724639
MAE train: 0.450404	val: 0.537019	test: 0.555363

Epoch: 57
Loss: 0.3858949035406113
RMSE train: 0.569257	val: 0.722436	test: 0.720746
MAE train: 0.449503	val: 0.538463	test: 0.552794

Epoch: 58
Loss: 0.38035314083099364
RMSE train: 0.563399	val: 0.719880	test: 0.720868
MAE train: 0.444592	val: 0.538694	test: 0.554130

Epoch: 59
Loss: 0.37178064286708834
RMSE train: 0.539844	val: 0.707865	test: 0.712117
MAE train: 0.424940	val: 0.526169	test: 0.545231

Epoch: 60
Loss: 0.38226038217544556
RMSE train: 0.559217	val: 0.719218	test: 0.716745
MAE train: 0.440303	val: 0.532910	test: 0.550830

Epoch: 61
Loss: 0.38350414037704467
RMSE train: 0.548351	val: 0.717733	test: 0.715797
MAE train: 0.431699	val: 0.536692	test: 0.549411

Epoch: 62
Loss: 0.37192573547363283
RMSE train: 0.547453	val: 0.714038	test: 0.721784
MAE train: 0.428353	val: 0.528091	test: 0.550794

Epoch: 63
Loss: 0.3812344342470169
RMSE train: 0.550092	val: 0.719921	test: 0.730855
MAE train: 0.433055	val: 0.538217	test: 0.561508

Epoch: 64
Loss: 0.3607546776533127
RMSE train: 0.536420	val: 0.709976	test: 0.717479
MAE train: 0.422770	val: 0.529164	test: 0.546844

Epoch: 65
Loss: 0.3782024264335632
RMSE train: 0.554801	val: 0.730704	test: 0.720573
MAE train: 0.440892	val: 0.539329	test: 0.549259

Epoch: 66
Loss: 0.36520753502845765
RMSE train: 0.546861	val: 0.715028	test: 0.728740
MAE train: 0.432793	val: 0.539966	test: 0.560640

Epoch: 67
Loss: 0.35900667905807493
RMSE train: 0.525737	val: 0.701231	test: 0.721841
MAE train: 0.414820	val: 0.517577	test: 0.545566

Epoch: 68
Loss: 0.3681346237659454
RMSE train: 0.543503	val: 0.722744	test: 0.731186
MAE train: 0.430073	val: 0.535251	test: 0.555579

Epoch: 69
Loss: 0.3643253892660141
RMSE train: 0.528082	val: 0.708529	test: 0.714626
MAE train: 0.417766	val: 0.528798	test: 0.542420

Epoch: 70
Loss: 0.353918531537056
RMSE train: 0.541918	val: 0.718673	test: 0.726588
MAE train: 0.424523	val: 0.531063	test: 0.545153

Epoch: 71
Loss: 0.35802477300167085
RMSE train: 0.538224	val: 0.711331	test: 0.724653
MAE train: 0.424445	val: 0.525671	test: 0.547628

Epoch: 72
Loss: 0.3312079757452011
RMSE train: 0.503494	val: 0.693933	test: 0.703418
MAE train: 0.396884	val: 0.511563	test: 0.533709

Epoch: 73
Loss: 0.35055424869060514
RMSE train: 0.543186	val: 0.712963	test: 0.730165
MAE train: 0.426165	val: 0.532080	test: 0.548026

Epoch: 74
Loss: 0.33414298593997954
RMSE train: 0.512985	val: 0.693984	test: 0.717526
MAE train: 0.405199	val: 0.516227	test: 0.539422

Epoch: 75
Loss: 0.3510276973247528
RMSE train: 0.527915	val: 0.720425	test: 0.722592
MAE train: 0.413477	val: 0.523020	test: 0.546451

Epoch: 76
Loss: 0.3439517080783844
RMSE train: 0.544713	val: 0.716543	test: 0.718381
MAE train: 0.433167	val: 0.538670	test: 0.548446

Epoch: 77
Loss: 0.32850357294082644
RMSE train: 0.536392	val: 0.725998	test: 0.722825
MAE train: 0.423923	val: 0.536020	test: 0.546203

Epoch: 78
Loss: 0.3442771345376968
RMSE train: 0.527479	val: 0.716782	test: 0.712442
MAE train: 0.418636	val: 0.536626	test: 0.544726

Epoch: 79
Loss: 0.32696660459041593
RMSE train: 0.508800	val: 0.718904	test: 0.711071
MAE train: 0.398985	val: 0.528361	test: 0.535318

Epoch: 80
Loss: 0.324229696393013
RMSE train: 0.506514	val: 0.702416	test: 0.698191
MAE train: 0.398982	val: 0.522644	test: 0.534462

Epoch: 81
Loss: 0.33959430158138276
RMSE train: 0.516589	val: 0.705645	test: 0.697604
MAE train: 0.407197	val: 0.523449	test: 0.534758

Epoch: 82
Loss: 0.3272965997457504
RMSE train: 0.525712	val: 0.716912	test: 0.708211
MAE train: 0.412683	val: 0.527148	test: 0.536205

Epoch: 83
Loss: 0.3157294377684593
RMSE train: 0.507753	val: 0.700329	test: 0.703583

Epoch: 23
Loss: 0.5717189997434616
RMSE train: 0.678488	val: 0.751397	test: 0.787281
MAE train: 0.533993	val: 0.569329	test: 0.612373

Epoch: 24
Loss: 0.5667260199785232
RMSE train: 0.660266	val: 0.735120	test: 0.774627
MAE train: 0.519727	val: 0.558975	test: 0.598290

Epoch: 25
Loss: 0.5667455285787583
RMSE train: 0.656338	val: 0.733352	test: 0.774773
MAE train: 0.514946	val: 0.558284	test: 0.597605

Epoch: 26
Loss: 0.5471970736980438
RMSE train: 0.673676	val: 0.749346	test: 0.780183
MAE train: 0.532272	val: 0.569539	test: 0.600176

Epoch: 27
Loss: 0.5517347902059555
RMSE train: 0.677672	val: 0.741782	test: 0.781880
MAE train: 0.533266	val: 0.572492	test: 0.607184

Epoch: 28
Loss: 0.540800678730011
RMSE train: 0.663552	val: 0.744614	test: 0.777276
MAE train: 0.521722	val: 0.566995	test: 0.597763

Epoch: 29
Loss: 0.5393274456262589
RMSE train: 0.636592	val: 0.730299	test: 0.755339
MAE train: 0.503143	val: 0.554819	test: 0.577612

Epoch: 30
Loss: 0.5334405064582824
RMSE train: 0.665854	val: 0.738259	test: 0.778096
MAE train: 0.522341	val: 0.562993	test: 0.603485

Epoch: 31
Loss: 0.5312256455421448
RMSE train: 0.645577	val: 0.737145	test: 0.767705
MAE train: 0.506571	val: 0.559805	test: 0.592793

Epoch: 32
Loss: 0.5132810056209565
RMSE train: 0.624850	val: 0.720211	test: 0.740851
MAE train: 0.489914	val: 0.543161	test: 0.572926

Epoch: 33
Loss: 0.49943129122257235
RMSE train: 0.652043	val: 0.733728	test: 0.770281
MAE train: 0.510225	val: 0.557931	test: 0.595853

Epoch: 34
Loss: 0.485768136382103
RMSE train: 0.614109	val: 0.715390	test: 0.742173
MAE train: 0.482430	val: 0.541487	test: 0.572317

Epoch: 35
Loss: 0.48749748766422274
RMSE train: 0.639576	val: 0.725634	test: 0.754577
MAE train: 0.500843	val: 0.546489	test: 0.579880

Epoch: 36
Loss: 0.4883363574743271
RMSE train: 0.617736	val: 0.714877	test: 0.741763
MAE train: 0.484159	val: 0.542700	test: 0.570120

Epoch: 37
Loss: 0.48306054174900054
RMSE train: 0.638801	val: 0.734436	test: 0.764980
MAE train: 0.502600	val: 0.553554	test: 0.585026

Epoch: 38
Loss: 0.46621311604976656
RMSE train: 0.618075	val: 0.714109	test: 0.742883
MAE train: 0.489116	val: 0.546126	test: 0.571934

Epoch: 39
Loss: 0.4686255782842636
RMSE train: 0.617575	val: 0.721032	test: 0.735792
MAE train: 0.485032	val: 0.547891	test: 0.563805

Epoch: 40
Loss: 0.4579967588186264
RMSE train: 0.605551	val: 0.713496	test: 0.732620
MAE train: 0.478430	val: 0.539775	test: 0.566956

Epoch: 41
Loss: 0.43789013624191286
RMSE train: 0.607566	val: 0.729279	test: 0.742868
MAE train: 0.482115	val: 0.551013	test: 0.570997

Epoch: 42
Loss: 0.45381312966346743
RMSE train: 0.606910	val: 0.704519	test: 0.740931
MAE train: 0.473550	val: 0.530742	test: 0.571059

Epoch: 43
Loss: 0.4575708121061325
RMSE train: 0.610635	val: 0.717162	test: 0.736800
MAE train: 0.481372	val: 0.542933	test: 0.571756

Epoch: 44
Loss: 0.45893708765506747
RMSE train: 0.609329	val: 0.714065	test: 0.748613
MAE train: 0.475801	val: 0.542079	test: 0.576271

Epoch: 45
Loss: 0.41656085550785066
RMSE train: 0.624562	val: 0.724947	test: 0.749336
MAE train: 0.492220	val: 0.544396	test: 0.572690

Epoch: 46
Loss: 0.43558900654315946
RMSE train: 0.603140	val: 0.714847	test: 0.735188
MAE train: 0.474141	val: 0.539541	test: 0.563989

Epoch: 47
Loss: 0.43681375980377196
RMSE train: 0.610849	val: 0.715893	test: 0.749088
MAE train: 0.475717	val: 0.538070	test: 0.570928

Epoch: 48
Loss: 0.43600831031799314
RMSE train: 0.583162	val: 0.710026	test: 0.726416
MAE train: 0.458523	val: 0.531739	test: 0.551656

Epoch: 49
Loss: 0.40545965135097506
RMSE train: 0.563416	val: 0.700012	test: 0.714867
MAE train: 0.444892	val: 0.530890	test: 0.546108

Epoch: 50
Loss: 0.4138457953929901
RMSE train: 0.577845	val: 0.712453	test: 0.727346
MAE train: 0.453796	val: 0.529105	test: 0.556255

Epoch: 51
Loss: 0.40870550870895384
RMSE train: 0.564429	val: 0.696058	test: 0.718576
MAE train: 0.445123	val: 0.522629	test: 0.545233

Epoch: 52
Loss: 0.4109123468399048
RMSE train: 0.574475	val: 0.698371	test: 0.732395
MAE train: 0.448126	val: 0.519381	test: 0.557367

Epoch: 53
Loss: 0.40632507503032683
RMSE train: 0.586685	val: 0.706417	test: 0.737370
MAE train: 0.461296	val: 0.527394	test: 0.561150

Epoch: 54
Loss: 0.39950422644615174
RMSE train: 0.591476	val: 0.715900	test: 0.740500
MAE train: 0.470262	val: 0.545178	test: 0.572822

Epoch: 55
Loss: 0.3833161503076553
RMSE train: 0.564575	val: 0.713424	test: 0.726364
MAE train: 0.445001	val: 0.532890	test: 0.552226

Epoch: 56
Loss: 0.3914049416780472
RMSE train: 0.595042	val: 0.711782	test: 0.743293
MAE train: 0.467152	val: 0.531984	test: 0.568794

Epoch: 57
Loss: 0.3974755108356476
RMSE train: 0.577536	val: 0.730862	test: 0.736907
MAE train: 0.460476	val: 0.553155	test: 0.563716

Epoch: 58
Loss: 0.398864883184433
RMSE train: 0.558307	val: 0.706053	test: 0.717836
MAE train: 0.442726	val: 0.531836	test: 0.553710

Epoch: 59
Loss: 0.3818217635154724
RMSE train: 0.537674	val: 0.689558	test: 0.705437
MAE train: 0.423618	val: 0.508952	test: 0.534793

Epoch: 60
Loss: 0.3712625175714493
RMSE train: 0.546772	val: 0.694720	test: 0.713983
MAE train: 0.428379	val: 0.514332	test: 0.545102

Epoch: 61
Loss: 0.38209547102451324
RMSE train: 0.555399	val: 0.709132	test: 0.713252
MAE train: 0.434671	val: 0.528168	test: 0.546534

Epoch: 62
Loss: 0.3712901324033737
RMSE train: 0.556435	val: 0.713876	test: 0.714453
MAE train: 0.440236	val: 0.532331	test: 0.548206

Epoch: 63
Loss: 0.36028291285037994
RMSE train: 0.551188	val: 0.705334	test: 0.720868
MAE train: 0.432411	val: 0.524587	test: 0.551351

Epoch: 64
Loss: 0.36844741404056547
RMSE train: 0.551692	val: 0.698098	test: 0.718322
MAE train: 0.437128	val: 0.521483	test: 0.548896

Epoch: 65
Loss: 0.3590260028839111
RMSE train: 0.551573	val: 0.703095	test: 0.718855
MAE train: 0.433031	val: 0.526192	test: 0.549523

Epoch: 66
Loss: 0.37179377377033235
RMSE train: 0.571895	val: 0.705972	test: 0.733273
MAE train: 0.447829	val: 0.525341	test: 0.561116

Epoch: 67
Loss: 0.3579288959503174
RMSE train: 0.534795	val: 0.689295	test: 0.712011
MAE train: 0.421646	val: 0.515997	test: 0.543435

Epoch: 68
Loss: 0.3523632824420929
RMSE train: 0.534022	val: 0.705166	test: 0.712854
MAE train: 0.421656	val: 0.520369	test: 0.541926

Epoch: 69
Loss: 0.3760612219572067
RMSE train: 0.581182	val: 0.722650	test: 0.743231
MAE train: 0.455241	val: 0.544089	test: 0.574088

Epoch: 70
Loss: 0.35114026367664336
RMSE train: 0.532252	val: 0.701784	test: 0.706556
MAE train: 0.421429	val: 0.522512	test: 0.538971

Epoch: 71
Loss: 0.34418228566646575
RMSE train: 0.528049	val: 0.692125	test: 0.722842
MAE train: 0.416332	val: 0.507301	test: 0.545829

Epoch: 72
Loss: 0.35048431158065796
RMSE train: 0.548260	val: 0.699323	test: 0.736926
MAE train: 0.429346	val: 0.522069	test: 0.565493

Epoch: 73
Loss: 0.3431410163640976
RMSE train: 0.558942	val: 0.711876	test: 0.730466
MAE train: 0.440151	val: 0.532389	test: 0.550038

Epoch: 74
Loss: 0.35250906050205233
RMSE train: 0.537281	val: 0.698571	test: 0.727709
MAE train: 0.420282	val: 0.515205	test: 0.553344

Epoch: 75
Loss: 0.3315084606409073
RMSE train: 0.546994	val: 0.709528	test: 0.733804
MAE train: 0.429330	val: 0.525319	test: 0.563382

Epoch: 76
Loss: 0.3242676883935928
RMSE train: 0.541876	val: 0.707450	test: 0.732421
MAE train: 0.426260	val: 0.522754	test: 0.558348

Epoch: 77
Loss: 0.3309475421905518
RMSE train: 0.522037	val: 0.691883	test: 0.698295
MAE train: 0.415572	val: 0.514359	test: 0.536146

Epoch: 78
Loss: 0.33231604695320127
RMSE train: 0.535609	val: 0.700345	test: 0.715989
MAE train: 0.421507	val: 0.520848	test: 0.548665

Epoch: 79
Loss: 0.32342479526996615
RMSE train: 0.535182	val: 0.701629	test: 0.717455
MAE train: 0.421909	val: 0.514501	test: 0.544511

Epoch: 80
Loss: 0.3274617463350296
RMSE train: 0.517529	val: 0.687129	test: 0.721042
MAE train: 0.408158	val: 0.512016	test: 0.548178

Epoch: 81
Loss: 0.33677734434604645
RMSE train: 0.518984	val: 0.698576	test: 0.705456
MAE train: 0.409518	val: 0.515708	test: 0.537282

Epoch: 82
Loss: 0.3204915881156921
RMSE train: 0.519421	val: 0.700866	test: 0.702260
MAE train: 0.408634	val: 0.513914	test: 0.537661

Epoch: 83
Loss: 0.30777007937431333
RMSE train: 0.511195	val: 0.688655	test: 0.708383

Epoch: 23
Loss: 0.5701893344521523
RMSE train: 0.668252	val: 0.769163	test: 0.739586
MAE train: 0.526909	val: 0.594226	test: 0.577048

Epoch: 24
Loss: 0.5804799621303877
RMSE train: 0.686372	val: 0.785829	test: 0.758501
MAE train: 0.540069	val: 0.607332	test: 0.592198

Epoch: 25
Loss: 0.5698054134845734
RMSE train: 0.688979	val: 0.782561	test: 0.749281
MAE train: 0.537550	val: 0.595109	test: 0.582261

Epoch: 26
Loss: 0.5538331891099612
RMSE train: 0.651933	val: 0.771262	test: 0.729694
MAE train: 0.510931	val: 0.586632	test: 0.567281

Epoch: 27
Loss: 0.5716396272182465
RMSE train: 0.678790	val: 0.772349	test: 0.756060
MAE train: 0.529046	val: 0.590874	test: 0.587189

Epoch: 28
Loss: 0.5504394869009653
RMSE train: 0.644549	val: 0.758819	test: 0.731739
MAE train: 0.504628	val: 0.579049	test: 0.565791

Epoch: 29
Loss: 0.538215217490991
RMSE train: 0.643472	val: 0.755961	test: 0.732095
MAE train: 0.503652	val: 0.577876	test: 0.569501

Epoch: 30
Loss: 0.5188325047492981
RMSE train: 0.675799	val: 0.781865	test: 0.739789
MAE train: 0.527603	val: 0.595864	test: 0.573689

Epoch: 31
Loss: 0.5335629930098852
RMSE train: 0.639572	val: 0.751423	test: 0.737699
MAE train: 0.502547	val: 0.574801	test: 0.575836

Epoch: 32
Loss: 0.520883691807588
RMSE train: 0.636485	val: 0.760027	test: 0.724594
MAE train: 0.501442	val: 0.572301	test: 0.564310

Epoch: 33
Loss: 0.49618346492449444
RMSE train: 0.649053	val: 0.783878	test: 0.732577
MAE train: 0.509356	val: 0.592425	test: 0.571920

Epoch: 34
Loss: 0.49824756383895874
RMSE train: 0.636303	val: 0.759288	test: 0.724286
MAE train: 0.496321	val: 0.572542	test: 0.565736

Epoch: 35
Loss: 0.5040306150913239
RMSE train: 0.623469	val: 0.746784	test: 0.713164
MAE train: 0.486286	val: 0.559639	test: 0.550500

Epoch: 36
Loss: 0.49606456855932873
RMSE train: 0.628638	val: 0.747390	test: 0.718289
MAE train: 0.491039	val: 0.558224	test: 0.555017

Epoch: 37
Loss: 0.47508370876312256
RMSE train: 0.619434	val: 0.751396	test: 0.725522
MAE train: 0.479880	val: 0.560446	test: 0.566077

Epoch: 38
Loss: 0.49285904069741565
RMSE train: 0.633319	val: 0.757693	test: 0.733794
MAE train: 0.498625	val: 0.574559	test: 0.570368

Epoch: 39
Loss: 0.46628472457329434
RMSE train: 0.633224	val: 0.782481	test: 0.728920
MAE train: 0.499069	val: 0.587938	test: 0.577651

Epoch: 40
Loss: 0.4626091991861661
RMSE train: 0.626255	val: 0.754118	test: 0.724259
MAE train: 0.490850	val: 0.565392	test: 0.563246

Epoch: 41
Loss: 0.44625552246967953
RMSE train: 0.598319	val: 0.734633	test: 0.704214
MAE train: 0.471062	val: 0.546233	test: 0.550182

Epoch: 42
Loss: 0.46458879858255386
RMSE train: 0.600757	val: 0.734748	test: 0.707477
MAE train: 0.472509	val: 0.546650	test: 0.553560

Epoch: 43
Loss: 0.4407208065191905
RMSE train: 0.579937	val: 0.729244	test: 0.699376
MAE train: 0.452721	val: 0.542244	test: 0.542218

Epoch: 44
Loss: 0.4483707199494044
RMSE train: 0.593880	val: 0.736259	test: 0.704793
MAE train: 0.469707	val: 0.548933	test: 0.554330

Epoch: 45
Loss: 0.47331958760817844
RMSE train: 0.593718	val: 0.734777	test: 0.700308
MAE train: 0.467573	val: 0.556109	test: 0.547862

Epoch: 46
Loss: 0.43300047020117444
RMSE train: 0.603088	val: 0.735038	test: 0.704892
MAE train: 0.476121	val: 0.554321	test: 0.553480

Epoch: 47
Loss: 0.4214539974927902
RMSE train: 0.580420	val: 0.735476	test: 0.689014
MAE train: 0.454131	val: 0.545642	test: 0.540740

Epoch: 48
Loss: 0.4267343208193779
RMSE train: 0.591873	val: 0.722700	test: 0.691395
MAE train: 0.463827	val: 0.544444	test: 0.540785

Epoch: 49
Loss: 0.4066876918077469
RMSE train: 0.579611	val: 0.717456	test: 0.699246
MAE train: 0.451572	val: 0.537263	test: 0.540984

Epoch: 50
Loss: 0.4175736630956332
RMSE train: 0.582082	val: 0.716296	test: 0.694746
MAE train: 0.459440	val: 0.539476	test: 0.545712

Epoch: 51
Loss: 0.421657291551431
RMSE train: 0.589587	val: 0.727153	test: 0.693601
MAE train: 0.460656	val: 0.541726	test: 0.536918

Epoch: 52
Loss: 0.39454710483551025
RMSE train: 0.588113	val: 0.735975	test: 0.705299
MAE train: 0.464461	val: 0.555832	test: 0.556027

Epoch: 53
Loss: 0.42931672434012097
RMSE train: 0.582201	val: 0.715425	test: 0.688637
MAE train: 0.453712	val: 0.531285	test: 0.532736

Epoch: 54
Loss: 0.3988628139098485
RMSE train: 0.573815	val: 0.733659	test: 0.695798
MAE train: 0.448611	val: 0.543347	test: 0.543387

Epoch: 55
Loss: 0.41188214470942813
RMSE train: 0.569890	val: 0.727597	test: 0.690686
MAE train: 0.446141	val: 0.540437	test: 0.537895

Epoch: 56
Loss: 0.395560363928477
RMSE train: 0.558719	val: 0.714009	test: 0.686621
MAE train: 0.435834	val: 0.537910	test: 0.532835

Epoch: 57
Loss: 0.3965686559677124
RMSE train: 0.563136	val: 0.704654	test: 0.685352
MAE train: 0.440802	val: 0.529998	test: 0.534495

Epoch: 58
Loss: 0.3981659139196078
RMSE train: 0.570114	val: 0.715657	test: 0.693276
MAE train: 0.449702	val: 0.537126	test: 0.547069

Epoch: 59
Loss: 0.4108375782767932
RMSE train: 0.571303	val: 0.706407	test: 0.692877
MAE train: 0.447840	val: 0.533550	test: 0.543728

Epoch: 60
Loss: 0.38660187770922977
RMSE train: 0.549005	val: 0.702881	test: 0.670561
MAE train: 0.427667	val: 0.524766	test: 0.515828

Epoch: 61
Loss: 0.3792320713400841
RMSE train: 0.570529	val: 0.714918	test: 0.691791
MAE train: 0.446933	val: 0.531382	test: 0.536525

Epoch: 62
Loss: 0.3782656913002332
RMSE train: 0.562085	val: 0.709235	test: 0.687018
MAE train: 0.441832	val: 0.529813	test: 0.537466

Epoch: 63
Loss: 0.3829363062977791
RMSE train: 0.549413	val: 0.708965	test: 0.689214
MAE train: 0.431613	val: 0.530429	test: 0.529792

Epoch: 64
Loss: 0.36803074181079865
RMSE train: 0.546343	val: 0.707513	test: 0.674027
MAE train: 0.425101	val: 0.516360	test: 0.523139

Epoch: 65
Loss: 0.37031514445940655
RMSE train: 0.547848	val: 0.715629	test: 0.681600
MAE train: 0.428354	val: 0.534794	test: 0.530193

Epoch: 66
Loss: 0.36032602936029434
RMSE train: 0.534476	val: 0.716057	test: 0.674365
MAE train: 0.419239	val: 0.534406	test: 0.525746

Epoch: 67
Loss: 0.35617466767628986
RMSE train: 0.544439	val: 0.707922	test: 0.676232
MAE train: 0.425843	val: 0.528501	test: 0.525007

Epoch: 68
Loss: 0.3524354075392087
RMSE train: 0.538001	val: 0.706859	test: 0.679940
MAE train: 0.421655	val: 0.519269	test: 0.527018

Epoch: 69
Loss: 0.35170572251081467
RMSE train: 0.568617	val: 0.720871	test: 0.702440
MAE train: 0.444627	val: 0.534272	test: 0.546232

Epoch: 70
Loss: 0.35283583650986355
RMSE train: 0.526401	val: 0.700265	test: 0.666737
MAE train: 0.412013	val: 0.510122	test: 0.519079

Epoch: 71
Loss: 0.35781029363473255
RMSE train: 0.527231	val: 0.708981	test: 0.671665
MAE train: 0.412713	val: 0.520915	test: 0.519519

Epoch: 72
Loss: 0.35167787969112396
RMSE train: 0.553073	val: 0.704423	test: 0.685947
MAE train: 0.431148	val: 0.517927	test: 0.528909

Epoch: 73
Loss: 0.3411736637353897
RMSE train: 0.519889	val: 0.699110	test: 0.662926
MAE train: 0.406472	val: 0.514533	test: 0.512568

Epoch: 74
Loss: 0.3556351512670517
RMSE train: 0.544050	val: 0.716600	test: 0.675896
MAE train: 0.425848	val: 0.536409	test: 0.518156

Epoch: 75
Loss: 0.3457706794142723
RMSE train: 0.555919	val: 0.730779	test: 0.703164
MAE train: 0.434186	val: 0.553187	test: 0.546302

Epoch: 76
Loss: 0.3473631888628006
RMSE train: 0.524939	val: 0.705786	test: 0.669632
MAE train: 0.409712	val: 0.511170	test: 0.511222

Epoch: 77
Loss: 0.33615433673063916
RMSE train: 0.529329	val: 0.706701	test: 0.676535
MAE train: 0.414660	val: 0.519449	test: 0.516032

Epoch: 78
Loss: 0.3312017470598221
RMSE train: 0.538420	val: 0.717820	test: 0.681622
MAE train: 0.425063	val: 0.525039	test: 0.524411

Epoch: 79
Loss: 0.3468552529811859
RMSE train: 0.522779	val: 0.708398	test: 0.670174
MAE train: 0.410006	val: 0.522481	test: 0.508428

Epoch: 80
Loss: 0.3353673964738846
RMSE train: 0.521651	val: 0.706506	test: 0.669239
MAE train: 0.408454	val: 0.517567	test: 0.513815

Epoch: 81
Loss: 0.33507145196199417
RMSE train: 0.542945	val: 0.707821	test: 0.695234
MAE train: 0.425086	val: 0.520898	test: 0.534260

Epoch: 82
Loss: 0.33363425234953564
RMSE train: 0.531821	val: 0.704369	test: 0.667119
MAE train: 0.417316	val: 0.519521	test: 0.517061

Epoch: 83
Loss: 0.31898072610298794
RMSE train: 0.521309	val: 0.712668	test: 0.676852

Epoch: 23
Loss: 0.5736329977711042
RMSE train: 0.678605	val: 0.793447	test: 0.762124
MAE train: 0.537468	val: 0.604393	test: 0.596601

Epoch: 24
Loss: 0.5503527323404948
RMSE train: 0.732959	val: 0.827741	test: 0.815137
MAE train: 0.575124	val: 0.629888	test: 0.636968

Epoch: 25
Loss: 0.531775638461113
RMSE train: 0.723713	val: 0.841928	test: 0.795351
MAE train: 0.563615	val: 0.631471	test: 0.608817

Epoch: 26
Loss: 0.5494565963745117
RMSE train: 0.665420	val: 0.790545	test: 0.755864
MAE train: 0.526309	val: 0.595512	test: 0.587915

Epoch: 27
Loss: 0.5307735179861387
RMSE train: 0.701559	val: 0.810085	test: 0.779864
MAE train: 0.549010	val: 0.606699	test: 0.604829

Epoch: 28
Loss: 0.5240337948004404
RMSE train: 0.673105	val: 0.798172	test: 0.753770
MAE train: 0.526385	val: 0.586935	test: 0.584762

Epoch: 29
Loss: 0.5133263890941938
RMSE train: 0.650082	val: 0.781006	test: 0.728329
MAE train: 0.511889	val: 0.586038	test: 0.571290

Epoch: 30
Loss: 0.5183948129415512
RMSE train: 0.662932	val: 0.780137	test: 0.732751
MAE train: 0.523642	val: 0.590475	test: 0.572833

Epoch: 31
Loss: 0.5132586484154066
RMSE train: 0.652538	val: 0.762055	test: 0.732980
MAE train: 0.509208	val: 0.569677	test: 0.568218

Epoch: 32
Loss: 0.5002563993136088
RMSE train: 0.666192	val: 0.782625	test: 0.745472
MAE train: 0.522512	val: 0.587708	test: 0.581296

Epoch: 33
Loss: 0.4923989251255989
RMSE train: 0.645846	val: 0.771702	test: 0.726898
MAE train: 0.504966	val: 0.569079	test: 0.564886

Epoch: 34
Loss: 0.48861803114414215
RMSE train: 0.640556	val: 0.779787	test: 0.732014
MAE train: 0.503966	val: 0.585850	test: 0.570113

Epoch: 35
Loss: 0.49464980016152066
RMSE train: 0.626224	val: 0.760976	test: 0.716757
MAE train: 0.494415	val: 0.572980	test: 0.559707

Epoch: 36
Loss: 0.4882054477930069
RMSE train: 0.642302	val: 0.764986	test: 0.730386
MAE train: 0.510744	val: 0.573773	test: 0.575062

Epoch: 37
Loss: 0.46351240823666257
RMSE train: 0.636037	val: 0.748334	test: 0.728215
MAE train: 0.497908	val: 0.562641	test: 0.564574

Epoch: 38
Loss: 0.4642922381560008
RMSE train: 0.640454	val: 0.760463	test: 0.729199
MAE train: 0.501050	val: 0.566868	test: 0.559052

Epoch: 39
Loss: 0.4520786330103874
RMSE train: 0.632823	val: 0.765230	test: 0.735583
MAE train: 0.495550	val: 0.570369	test: 0.562660

Epoch: 40
Loss: 0.46521274745464325
RMSE train: 0.648899	val: 0.766873	test: 0.745978
MAE train: 0.507521	val: 0.567278	test: 0.578576

Epoch: 41
Loss: 0.45086172719796497
RMSE train: 0.621059	val: 0.753873	test: 0.721714
MAE train: 0.488333	val: 0.559198	test: 0.558487

Epoch: 42
Loss: 0.46134166916211444
RMSE train: 0.616461	val: 0.758693	test: 0.718418
MAE train: 0.483924	val: 0.561099	test: 0.554351

Epoch: 43
Loss: 0.4565911491711934
RMSE train: 0.608392	val: 0.758292	test: 0.710305
MAE train: 0.476375	val: 0.563663	test: 0.553890

Epoch: 44
Loss: 0.444119227429231
RMSE train: 0.613692	val: 0.739384	test: 0.708158
MAE train: 0.484059	val: 0.547890	test: 0.551589

Epoch: 45
Loss: 0.4263974477847417
RMSE train: 0.632452	val: 0.746098	test: 0.727919
MAE train: 0.496857	val: 0.558694	test: 0.563501

Epoch: 46
Loss: 0.43543824553489685
RMSE train: 0.597664	val: 0.737138	test: 0.700286
MAE train: 0.469469	val: 0.545229	test: 0.544433

Epoch: 47
Loss: 0.42817681034406024
RMSE train: 0.601688	val: 0.753343	test: 0.709123
MAE train: 0.473916	val: 0.555773	test: 0.541617

Epoch: 48
Loss: 0.41792217393716175
RMSE train: 0.609387	val: 0.764754	test: 0.711348
MAE train: 0.476269	val: 0.561607	test: 0.548622

Epoch: 49
Loss: 0.42327965547641117
RMSE train: 0.598638	val: 0.753687	test: 0.710021
MAE train: 0.473855	val: 0.557265	test: 0.552286

Epoch: 50
Loss: 0.42239052802324295
RMSE train: 0.593928	val: 0.736596	test: 0.704128
MAE train: 0.462453	val: 0.545414	test: 0.541299

Epoch: 51
Loss: 0.413198026518027
RMSE train: 0.607444	val: 0.745721	test: 0.704771
MAE train: 0.473204	val: 0.551733	test: 0.545799

Epoch: 52
Loss: 0.4189464847246806
RMSE train: 0.583795	val: 0.734119	test: 0.698044
MAE train: 0.456329	val: 0.545187	test: 0.540622

Epoch: 53
Loss: 0.4049940382440885
RMSE train: 0.601109	val: 0.745195	test: 0.704987
MAE train: 0.469992	val: 0.544816	test: 0.551430

Epoch: 54
Loss: 0.39300113668044406
RMSE train: 0.590224	val: 0.750200	test: 0.704639
MAE train: 0.462272	val: 0.553560	test: 0.547953

Epoch: 55
Loss: 0.3867809573809306
RMSE train: 0.580618	val: 0.746068	test: 0.694906
MAE train: 0.455052	val: 0.552098	test: 0.542198

Epoch: 56
Loss: 0.3949509263038635
RMSE train: 0.576003	val: 0.746875	test: 0.699320
MAE train: 0.451568	val: 0.548559	test: 0.543037

Epoch: 57
Loss: 0.3902209873000781
RMSE train: 0.583018	val: 0.748502	test: 0.713542
MAE train: 0.454386	val: 0.544064	test: 0.549634

Epoch: 58
Loss: 0.3862359474102656
RMSE train: 0.580837	val: 0.748497	test: 0.705678
MAE train: 0.455067	val: 0.542246	test: 0.543115

Epoch: 59
Loss: 0.38790907214085263
RMSE train: 0.572428	val: 0.736222	test: 0.699643
MAE train: 0.448091	val: 0.541198	test: 0.542744

Epoch: 60
Loss: 0.39499028275410336
RMSE train: 0.568447	val: 0.727638	test: 0.699111
MAE train: 0.444326	val: 0.536003	test: 0.532944

Epoch: 61
Loss: 0.36809083074331284
RMSE train: 0.571202	val: 0.736822	test: 0.706622
MAE train: 0.443460	val: 0.538974	test: 0.539134

Epoch: 62
Loss: 0.38822376479705173
RMSE train: 0.608002	val: 0.752330	test: 0.731384
MAE train: 0.475287	val: 0.557572	test: 0.558660

Epoch: 63
Loss: 0.3772018700838089
RMSE train: 0.553881	val: 0.740772	test: 0.689720
MAE train: 0.434014	val: 0.540114	test: 0.531915

Epoch: 64
Loss: 0.3709761053323746
RMSE train: 0.579096	val: 0.744321	test: 0.698085
MAE train: 0.452555	val: 0.545638	test: 0.536509

Epoch: 65
Loss: 0.36429765075445175
RMSE train: 0.593877	val: 0.752644	test: 0.713699
MAE train: 0.464837	val: 0.554522	test: 0.550011

Epoch: 66
Loss: 0.36635852108399075
RMSE train: 0.543092	val: 0.718411	test: 0.676519
MAE train: 0.426263	val: 0.525516	test: 0.518792

Epoch: 67
Loss: 0.3563011363148689
RMSE train: 0.548765	val: 0.725290	test: 0.681716
MAE train: 0.430732	val: 0.527985	test: 0.525782

Epoch: 68
Loss: 0.35771268357833225
RMSE train: 0.560339	val: 0.725623	test: 0.688322
MAE train: 0.440300	val: 0.534765	test: 0.528066

Epoch: 69
Loss: 0.3512986972928047
RMSE train: 0.533923	val: 0.730719	test: 0.676766
MAE train: 0.418636	val: 0.532707	test: 0.520965

Epoch: 70
Loss: 0.3390984982252121
RMSE train: 0.557911	val: 0.740134	test: 0.693996
MAE train: 0.436082	val: 0.535877	test: 0.533396

Epoch: 71
Loss: 0.352794590095679
RMSE train: 0.569016	val: 0.752455	test: 0.701976
MAE train: 0.445669	val: 0.547176	test: 0.544795

Epoch: 72
Loss: 0.3518933678666751
RMSE train: 0.583500	val: 0.750897	test: 0.724671
MAE train: 0.458339	val: 0.551830	test: 0.560656

Epoch: 73
Loss: 0.3317042837540309
RMSE train: 0.555469	val: 0.738545	test: 0.695821
MAE train: 0.436752	val: 0.530801	test: 0.532514

Epoch: 74
Loss: 0.3465273752808571
RMSE train: 0.556944	val: 0.752234	test: 0.702659
MAE train: 0.434729	val: 0.550285	test: 0.538300

Epoch: 75
Loss: 0.33488689611355466
RMSE train: 0.536049	val: 0.726480	test: 0.679024
MAE train: 0.418962	val: 0.525265	test: 0.520916

Epoch: 76
Loss: 0.3488781526684761
RMSE train: 0.530588	val: 0.731154	test: 0.688536
MAE train: 0.416159	val: 0.542746	test: 0.533118

Epoch: 77
Loss: 0.32819867382446927
RMSE train: 0.528386	val: 0.734634	test: 0.686775
MAE train: 0.414222	val: 0.536808	test: 0.532331

Epoch: 78
Loss: 0.33226694415013
RMSE train: 0.530404	val: 0.730547	test: 0.694383
MAE train: 0.416409	val: 0.531493	test: 0.532272

Epoch: 79
Loss: 0.34034210443496704
RMSE train: 0.542432	val: 0.735948	test: 0.707188
MAE train: 0.423602	val: 0.547798	test: 0.540933

Epoch: 80
Loss: 0.3173014869292577
RMSE train: 0.527718	val: 0.743492	test: 0.693002
MAE train: 0.412539	val: 0.528290	test: 0.532639

Epoch: 81
Loss: 0.32037749141454697
RMSE train: 0.541674	val: 0.741165	test: 0.693252
MAE train: 0.426286	val: 0.530474	test: 0.531516

Epoch: 82
Loss: 0.33081983278195065
RMSE train: 0.523430	val: 0.734613	test: 0.689351
MAE train: 0.412944	val: 0.529193	test: 0.531373

Epoch: 83
Loss: 0.3257235139608383
RMSE train: 0.529669	val: 0.728485	test: 0.691931

Epoch: 23
Loss: 0.5933985561132431
RMSE train: 0.682224	val: 0.778215	test: 0.747477
MAE train: 0.537000	val: 0.600630	test: 0.585562

Epoch: 24
Loss: 0.5471427092949549
RMSE train: 0.710096	val: 0.800217	test: 0.770870
MAE train: 0.557808	val: 0.615980	test: 0.599841

Epoch: 25
Loss: 0.5405934527516365
RMSE train: 0.669436	val: 0.772633	test: 0.750213
MAE train: 0.525316	val: 0.594066	test: 0.581702

Epoch: 26
Loss: 0.5502126713593801
RMSE train: 0.670379	val: 0.770880	test: 0.735967
MAE train: 0.524042	val: 0.588187	test: 0.569209

Epoch: 27
Loss: 0.5249785954753557
RMSE train: 0.637514	val: 0.755969	test: 0.726341
MAE train: 0.501081	val: 0.573813	test: 0.564116

Epoch: 28
Loss: 0.515356625119845
RMSE train: 0.649448	val: 0.758766	test: 0.737376
MAE train: 0.508730	val: 0.577839	test: 0.572589

Epoch: 29
Loss: 0.51258767892917
RMSE train: 0.639069	val: 0.745080	test: 0.725961
MAE train: 0.501587	val: 0.567697	test: 0.557631

Epoch: 30
Loss: 0.49633671591679257
RMSE train: 0.670058	val: 0.774330	test: 0.754856
MAE train: 0.522777	val: 0.591046	test: 0.580691

Epoch: 31
Loss: 0.4833719755212466
RMSE train: 0.652965	val: 0.757064	test: 0.745919
MAE train: 0.512456	val: 0.577027	test: 0.574309

Epoch: 32
Loss: 0.4799576848745346
RMSE train: 0.631284	val: 0.751016	test: 0.726784
MAE train: 0.493872	val: 0.568427	test: 0.561693

Epoch: 33
Loss: 0.49809595197439194
RMSE train: 0.621841	val: 0.738385	test: 0.724127
MAE train: 0.486955	val: 0.558290	test: 0.558180

Epoch: 34
Loss: 0.48763277381658554
RMSE train: 0.627508	val: 0.750583	test: 0.717756
MAE train: 0.489596	val: 0.561460	test: 0.554151

Epoch: 35
Loss: 0.46982089430093765
RMSE train: 0.621109	val: 0.753058	test: 0.724816
MAE train: 0.486996	val: 0.570793	test: 0.557352

Epoch: 36
Loss: 0.48083746433258057
RMSE train: 0.618595	val: 0.738778	test: 0.715721
MAE train: 0.485384	val: 0.551075	test: 0.548825

Epoch: 37
Loss: 0.4576043685277303
RMSE train: 0.651097	val: 0.752542	test: 0.741425
MAE train: 0.507555	val: 0.568614	test: 0.568562

Epoch: 38
Loss: 0.45997416973114014
RMSE train: 0.622319	val: 0.754042	test: 0.722128
MAE train: 0.490591	val: 0.565160	test: 0.563314

Epoch: 39
Loss: 0.4667261764407158
RMSE train: 0.614214	val: 0.742670	test: 0.726077
MAE train: 0.481705	val: 0.560983	test: 0.560328

Epoch: 40
Loss: 0.4423701614141464
RMSE train: 0.603263	val: 0.736340	test: 0.705252
MAE train: 0.470195	val: 0.546126	test: 0.542069

Epoch: 41
Loss: 0.4473342075943947
RMSE train: 0.619803	val: 0.745118	test: 0.721433
MAE train: 0.485372	val: 0.561195	test: 0.552579

Epoch: 42
Loss: 0.4343334635098775
RMSE train: 0.608726	val: 0.740205	test: 0.721008
MAE train: 0.483631	val: 0.568144	test: 0.567856

Epoch: 43
Loss: 0.439404937128226
RMSE train: 0.599652	val: 0.731223	test: 0.706232
MAE train: 0.468747	val: 0.545876	test: 0.547628

Epoch: 44
Loss: 0.43107908467451733
RMSE train: 0.594588	val: 0.724621	test: 0.705478
MAE train: 0.462960	val: 0.536709	test: 0.539602

Epoch: 45
Loss: 0.4335092678666115
RMSE train: 0.617941	val: 0.741188	test: 0.716863
MAE train: 0.486628	val: 0.553833	test: 0.558700

Epoch: 46
Loss: 0.4213273897767067
RMSE train: 0.580240	val: 0.721723	test: 0.694549
MAE train: 0.455803	val: 0.538465	test: 0.535695

Epoch: 47
Loss: 0.42054034272829693
RMSE train: 0.594532	val: 0.722435	test: 0.706627
MAE train: 0.468843	val: 0.537123	test: 0.547726

Epoch: 48
Loss: 0.413016510506471
RMSE train: 0.588656	val: 0.726978	test: 0.704788
MAE train: 0.463479	val: 0.540026	test: 0.548056

Epoch: 49
Loss: 0.406066690882047
RMSE train: 0.578236	val: 0.728080	test: 0.695868
MAE train: 0.454357	val: 0.538412	test: 0.539890

Epoch: 50
Loss: 0.40787039945522946
RMSE train: 0.590151	val: 0.739298	test: 0.699876
MAE train: 0.462235	val: 0.552650	test: 0.538046

Epoch: 51
Loss: 0.39991579949855804
RMSE train: 0.588241	val: 0.724008	test: 0.703151
MAE train: 0.460015	val: 0.539790	test: 0.532872

Epoch: 52
Loss: 0.4055835281809171
RMSE train: 0.572908	val: 0.717586	test: 0.696307
MAE train: 0.445485	val: 0.527475	test: 0.533955

Epoch: 53
Loss: 0.4023073489467303
RMSE train: 0.605111	val: 0.736637	test: 0.713903
MAE train: 0.476577	val: 0.548506	test: 0.555832

Epoch: 54
Loss: 0.4129822825392087
RMSE train: 0.573385	val: 0.719689	test: 0.695443
MAE train: 0.453206	val: 0.536111	test: 0.538786

Epoch: 55
Loss: 0.39687032500902814
RMSE train: 0.564165	val: 0.731167	test: 0.691027
MAE train: 0.442685	val: 0.542305	test: 0.540120

Epoch: 56
Loss: 0.38827912509441376
RMSE train: 0.566491	val: 0.732786	test: 0.702151
MAE train: 0.444518	val: 0.534421	test: 0.534853

Epoch: 57
Loss: 0.40243662148714066
RMSE train: 0.563251	val: 0.718978	test: 0.697921
MAE train: 0.440127	val: 0.521712	test: 0.528603

Epoch: 58
Loss: 0.3847096338868141
RMSE train: 0.559643	val: 0.713570	test: 0.687835
MAE train: 0.438605	val: 0.523914	test: 0.529745

Epoch: 59
Loss: 0.3885868812600772
RMSE train: 0.566761	val: 0.711232	test: 0.696688
MAE train: 0.445280	val: 0.530490	test: 0.539549

Epoch: 60
Loss: 0.38276975105206174
RMSE train: 0.562404	val: 0.716941	test: 0.687063
MAE train: 0.438829	val: 0.528661	test: 0.531749

Epoch: 61
Loss: 0.3762792671720187
RMSE train: 0.554315	val: 0.730237	test: 0.697833
MAE train: 0.434362	val: 0.553559	test: 0.540515

Epoch: 62
Loss: 0.37310734391212463
RMSE train: 0.557317	val: 0.708414	test: 0.684404
MAE train: 0.437582	val: 0.532700	test: 0.522965

Epoch: 63
Loss: 0.36436303953329724
RMSE train: 0.543809	val: 0.712479	test: 0.683274
MAE train: 0.423879	val: 0.523346	test: 0.516353

Epoch: 64
Loss: 0.3660184219479561
RMSE train: 0.554013	val: 0.708581	test: 0.693875
MAE train: 0.436469	val: 0.521858	test: 0.538593

Epoch: 65
Loss: 0.3756830592950185
RMSE train: 0.543575	val: 0.715121	test: 0.694766
MAE train: 0.426371	val: 0.529878	test: 0.535908

Epoch: 66
Loss: 0.35326940069595975
RMSE train: 0.534134	val: 0.709096	test: 0.682511
MAE train: 0.418022	val: 0.521362	test: 0.524986

Epoch: 67
Loss: 0.3745337277650833
RMSE train: 0.553410	val: 0.717661	test: 0.686664
MAE train: 0.434489	val: 0.525739	test: 0.527511

Epoch: 68
Loss: 0.3511331503589948
RMSE train: 0.550502	val: 0.723546	test: 0.687110
MAE train: 0.436141	val: 0.538460	test: 0.537644

Epoch: 69
Loss: 0.3553464536865552
RMSE train: 0.534313	val: 0.701476	test: 0.686698
MAE train: 0.416993	val: 0.516673	test: 0.519361

Epoch: 70
Loss: 0.32340628902117413
RMSE train: 0.544212	val: 0.707698	test: 0.682885
MAE train: 0.429178	val: 0.521901	test: 0.528694

Epoch: 71
Loss: 0.36454962690671283
RMSE train: 0.550159	val: 0.730602	test: 0.707951
MAE train: 0.433141	val: 0.546670	test: 0.546950

Epoch: 72
Loss: 0.3471763754884402
RMSE train: 0.548388	val: 0.723288	test: 0.685606
MAE train: 0.434330	val: 0.529687	test: 0.532602

Epoch: 73
Loss: 0.34307283411423367
RMSE train: 0.518186	val: 0.712657	test: 0.684811
MAE train: 0.403827	val: 0.519943	test: 0.523800

Epoch: 74
Loss: 0.3400677318374316
RMSE train: 0.526759	val: 0.703688	test: 0.689928
MAE train: 0.411994	val: 0.514821	test: 0.525654

Epoch: 75
Loss: 0.3338714987039566
RMSE train: 0.545525	val: 0.709172	test: 0.698477
MAE train: 0.426160	val: 0.514566	test: 0.529770

Epoch: 76
Loss: 0.3139063740770022
RMSE train: 0.518536	val: 0.712238	test: 0.682294
MAE train: 0.406844	val: 0.517315	test: 0.523128

Epoch: 77
Loss: 0.32674720759193104
RMSE train: 0.546381	val: 0.726120	test: 0.713757
MAE train: 0.427079	val: 0.532827	test: 0.540769

Epoch: 78
Loss: 0.33149899790684384
RMSE train: 0.519671	val: 0.708821	test: 0.680952
MAE train: 0.407796	val: 0.522354	test: 0.528638

Epoch: 79
Loss: 0.3347145865360896
RMSE train: 0.522652	val: 0.707716	test: 0.692048
MAE train: 0.411356	val: 0.524035	test: 0.528507

Epoch: 80
Loss: 0.32785369952519733
RMSE train: 0.526142	val: 0.718636	test: 0.693208
MAE train: 0.416442	val: 0.531705	test: 0.539048

Epoch: 81
Loss: 0.30787741889556247
RMSE train: 0.502353	val: 0.704126	test: 0.679915
MAE train: 0.392717	val: 0.511828	test: 0.519501

Epoch: 82
Loss: 0.3145475039879481
RMSE train: 0.518737	val: 0.710110	test: 0.687236
MAE train: 0.408216	val: 0.523583	test: 0.527573

Epoch: 83
Loss: 0.32264623045921326
RMSE train: 0.504872	val: 0.689106	test: 0.671214

Epoch: 23
Loss: 0.5757756637675422
RMSE train: 0.701812	val: 0.838953	test: 0.763659
MAE train: 0.543645	val: 0.654959	test: 0.583075

Epoch: 24
Loss: 0.59042041855199
RMSE train: 0.706574	val: 0.839022	test: 0.740952
MAE train: 0.546104	val: 0.652252	test: 0.567965

Epoch: 25
Loss: 0.6055108564240592
RMSE train: 0.666440	val: 0.817500	test: 0.694572
MAE train: 0.517154	val: 0.629949	test: 0.529000

Epoch: 26
Loss: 0.5698279653276715
RMSE train: 0.702065	val: 0.839175	test: 0.746868
MAE train: 0.547558	val: 0.653118	test: 0.574943

Epoch: 27
Loss: 0.5768129335982459
RMSE train: 0.699275	val: 0.845394	test: 0.745920
MAE train: 0.542574	val: 0.649339	test: 0.567922

Epoch: 28
Loss: 0.5621320222105298
RMSE train: 0.688995	val: 0.829595	test: 0.741284
MAE train: 0.538679	val: 0.639754	test: 0.568000

Epoch: 29
Loss: 0.5501007437705994
RMSE train: 0.707965	val: 0.835561	test: 0.764663
MAE train: 0.548215	val: 0.652774	test: 0.579999

Epoch: 30
Loss: 0.5239922595875603
RMSE train: 0.693462	val: 0.837477	test: 0.739147
MAE train: 0.537938	val: 0.646315	test: 0.568910

Epoch: 31
Loss: 0.5333414162908282
RMSE train: 0.660023	val: 0.819789	test: 0.736702
MAE train: 0.516192	val: 0.629652	test: 0.556816

Epoch: 32
Loss: 0.514179698058537
RMSE train: 0.655308	val: 0.812735	test: 0.712847
MAE train: 0.509179	val: 0.623963	test: 0.542782

Epoch: 33
Loss: 0.4875080670629229
RMSE train: 0.694306	val: 0.844053	test: 0.753528
MAE train: 0.540243	val: 0.648864	test: 0.569497

Epoch: 34
Loss: 0.49003281550748007
RMSE train: 0.667634	val: 0.823499	test: 0.727098
MAE train: 0.516743	val: 0.629408	test: 0.549000

Epoch: 35
Loss: 0.5139937677553722
RMSE train: 0.655164	val: 0.821304	test: 0.710085
MAE train: 0.511107	val: 0.627050	test: 0.540832

Epoch: 36
Loss: 0.4943204713719232
RMSE train: 0.627734	val: 0.810087	test: 0.694563
MAE train: 0.486030	val: 0.613107	test: 0.522695

Epoch: 37
Loss: 0.48372027916567667
RMSE train: 0.643074	val: 0.807605	test: 0.698421
MAE train: 0.500311	val: 0.614205	test: 0.526917

Epoch: 38
Loss: 0.4736180752515793
RMSE train: 0.658834	val: 0.827150	test: 0.734389
MAE train: 0.512566	val: 0.633449	test: 0.555505

Epoch: 39
Loss: 0.4663913867303303
RMSE train: 0.620786	val: 0.793744	test: 0.695936
MAE train: 0.479050	val: 0.599367	test: 0.518973

Epoch: 40
Loss: 0.4726765219654356
RMSE train: 0.662010	val: 0.826493	test: 0.742063
MAE train: 0.513700	val: 0.624667	test: 0.549902

Epoch: 41
Loss: 0.46239253665719715
RMSE train: 0.630542	val: 0.793190	test: 0.686187
MAE train: 0.488520	val: 0.603074	test: 0.510539

Epoch: 42
Loss: 0.46155846118927
RMSE train: 0.647468	val: 0.816109	test: 0.716990
MAE train: 0.503184	val: 0.620574	test: 0.535974

Epoch: 43
Loss: 0.459138212459428
RMSE train: 0.666307	val: 0.827219	test: 0.754048
MAE train: 0.517525	val: 0.632240	test: 0.567355

Epoch: 44
Loss: 0.47788346239498686
RMSE train: 0.638852	val: 0.804710	test: 0.752771
MAE train: 0.494972	val: 0.622082	test: 0.559692

Epoch: 45
Loss: 0.48563335835933685
RMSE train: 0.631754	val: 0.798448	test: 0.712832
MAE train: 0.488835	val: 0.611358	test: 0.539236

Epoch: 46
Loss: 0.44761447821344647
RMSE train: 0.596902	val: 0.780843	test: 0.686799
MAE train: 0.462835	val: 0.591573	test: 0.513681

Epoch: 47
Loss: 0.4354004796062197
RMSE train: 0.614771	val: 0.805218	test: 0.698146
MAE train: 0.477862	val: 0.609635	test: 0.529405

Epoch: 48
Loss: 0.4681808480194637
RMSE train: 0.628565	val: 0.806008	test: 0.719739
MAE train: 0.485850	val: 0.610815	test: 0.540604

Epoch: 49
Loss: 0.43756171635219027
RMSE train: 0.612443	val: 0.786862	test: 0.695179
MAE train: 0.476305	val: 0.596527	test: 0.518545

Epoch: 50
Loss: 0.4234312240566526
RMSE train: 0.616399	val: 0.785899	test: 0.702174
MAE train: 0.479367	val: 0.597542	test: 0.531234

Epoch: 51
Loss: 0.4181277517761503
RMSE train: 0.602327	val: 0.776576	test: 0.695560
MAE train: 0.466594	val: 0.594239	test: 0.522407

Epoch: 52
Loss: 0.4261834238256727
RMSE train: 0.597200	val: 0.773460	test: 0.691941
MAE train: 0.465123	val: 0.592215	test: 0.521689

Epoch: 53
Loss: 0.4205979747431619
RMSE train: 0.609669	val: 0.783038	test: 0.721387
MAE train: 0.474054	val: 0.599290	test: 0.541697

Epoch: 54
Loss: 0.4197843713419778
RMSE train: 0.601793	val: 0.787203	test: 0.670349
MAE train: 0.467201	val: 0.595876	test: 0.503941

Epoch: 55
Loss: 0.42703731783798765
RMSE train: 0.602320	val: 0.772273	test: 0.700639
MAE train: 0.469087	val: 0.593933	test: 0.530363

Epoch: 56
Loss: 0.42470015585422516
RMSE train: 0.567229	val: 0.750699	test: 0.674529
MAE train: 0.440285	val: 0.569846	test: 0.506425

Epoch: 57
Loss: 0.3878179277692522
RMSE train: 0.614556	val: 0.793101	test: 0.706247
MAE train: 0.477545	val: 0.599715	test: 0.528690

Epoch: 58
Loss: 0.3943817487784794
RMSE train: 0.564468	val: 0.751513	test: 0.661629
MAE train: 0.439222	val: 0.569875	test: 0.496001

Epoch: 59
Loss: 0.40796310135296415
RMSE train: 0.570449	val: 0.763499	test: 0.674321
MAE train: 0.441234	val: 0.578223	test: 0.509852

Epoch: 60
Loss: 0.38885368406772614
RMSE train: 0.592284	val: 0.767098	test: 0.690605
MAE train: 0.460692	val: 0.581274	test: 0.521701

Epoch: 61
Loss: 0.4009454527071544
RMSE train: 0.577904	val: 0.766178	test: 0.671867
MAE train: 0.446555	val: 0.574614	test: 0.501649

Epoch: 62
Loss: 0.38684483085359844
RMSE train: 0.580577	val: 0.765830	test: 0.675309
MAE train: 0.451067	val: 0.584345	test: 0.508366

Epoch: 63
Loss: 0.39814257621765137
RMSE train: 0.614416	val: 0.792185	test: 0.725807
MAE train: 0.474678	val: 0.599240	test: 0.542351

Epoch: 64
Loss: 0.3771702689783914
RMSE train: 0.571239	val: 0.745548	test: 0.666440
MAE train: 0.443886	val: 0.571120	test: 0.499151

Epoch: 65
Loss: 0.3770331633942468
RMSE train: 0.604801	val: 0.772993	test: 0.691321
MAE train: 0.472102	val: 0.583630	test: 0.523001

Epoch: 66
Loss: 0.3919624856540135
RMSE train: 0.557273	val: 0.763422	test: 0.664659
MAE train: 0.431408	val: 0.578829	test: 0.502182

Epoch: 67
Loss: 0.37684326086725506
RMSE train: 0.579745	val: 0.779655	test: 0.684486
MAE train: 0.449276	val: 0.586947	test: 0.517184

Epoch: 68
Loss: 0.3845980784722737
RMSE train: 0.549127	val: 0.753670	test: 0.667329
MAE train: 0.426093	val: 0.564241	test: 0.501106

Epoch: 69
Loss: 0.3760208466223308
RMSE train: 0.541682	val: 0.740692	test: 0.659957
MAE train: 0.417397	val: 0.544907	test: 0.490051

Epoch: 70
Loss: 0.376125516636031
RMSE train: 0.547592	val: 0.753497	test: 0.658297
MAE train: 0.427498	val: 0.568022	test: 0.499304

Epoch: 71
Loss: 0.36879666575363707
RMSE train: 0.549762	val: 0.742188	test: 0.670732
MAE train: 0.425691	val: 0.558934	test: 0.502825

Epoch: 72
Loss: 0.3639191857406071
RMSE train: 0.569071	val: 0.736430	test: 0.676291
MAE train: 0.443230	val: 0.565369	test: 0.508649

Epoch: 73
Loss: 0.3524636115346636
RMSE train: 0.568868	val: 0.756153	test: 0.688309
MAE train: 0.442210	val: 0.573027	test: 0.518638

Epoch: 74
Loss: 0.36375124539647785
RMSE train: 0.537261	val: 0.742221	test: 0.654066
MAE train: 0.416020	val: 0.557146	test: 0.493048

Epoch: 75
Loss: 0.35673155316284727
RMSE train: 0.559287	val: 0.758146	test: 0.682670
MAE train: 0.434774	val: 0.570668	test: 0.512736

Epoch: 76
Loss: 0.3621801563671657
RMSE train: 0.554055	val: 0.752797	test: 0.665547
MAE train: 0.430715	val: 0.568682	test: 0.503725

Epoch: 77
Loss: 0.35015813793454853
RMSE train: 0.545191	val: 0.748136	test: 0.668405
MAE train: 0.423851	val: 0.566620	test: 0.503693

Epoch: 78
Loss: 0.3445123497928892
RMSE train: 0.549022	val: 0.747603	test: 0.682081
MAE train: 0.425003	val: 0.557883	test: 0.506182

Epoch: 79
Loss: 0.34705545221056255
RMSE train: 0.541646	val: 0.748801	test: 0.674016
MAE train: 0.419139	val: 0.563494	test: 0.501675

Epoch: 80
Loss: 0.3586190662213734
RMSE train: 0.551798	val: 0.764486	test: 0.662733
MAE train: 0.431040	val: 0.567505	test: 0.506691

Epoch: 81
Loss: 0.3442092942340033
RMSE train: 0.535654	val: 0.749166	test: 0.648875
MAE train: 0.414353	val: 0.566537	test: 0.490396

Epoch: 82
Loss: 0.3682814432041986
RMSE train: 0.534507	val: 0.743982	test: 0.662187
MAE train: 0.414833	val: 0.555707	test: 0.496279

Epoch: 83
Loss: 0.3400588972227914
RMSE train: 0.572252	val: 0.779050	test: 0.702220

Epoch: 23
Loss: 0.5959944214139666
RMSE train: 0.700477	val: 0.842962	test: 0.763072
MAE train: 0.547702	val: 0.651893	test: 0.583574

Epoch: 24
Loss: 0.5854476519993373
RMSE train: 0.708480	val: 0.839646	test: 0.765940
MAE train: 0.553357	val: 0.657893	test: 0.590432

Epoch: 25
Loss: 0.5565939311470304
RMSE train: 0.681098	val: 0.835601	test: 0.740565
MAE train: 0.528667	val: 0.649934	test: 0.569753

Epoch: 26
Loss: 0.5303334444761276
RMSE train: 0.687502	val: 0.828041	test: 0.743209
MAE train: 0.537500	val: 0.640030	test: 0.568976

Epoch: 27
Loss: 0.577446963105883
RMSE train: 0.670261	val: 0.829491	test: 0.740054
MAE train: 0.525061	val: 0.643476	test: 0.578934

Epoch: 28
Loss: 0.5620334723166057
RMSE train: 0.689208	val: 0.838093	test: 0.748173
MAE train: 0.541240	val: 0.644567	test: 0.577744

Epoch: 29
Loss: 0.5345404808010373
RMSE train: 0.679702	val: 0.838932	test: 0.746071
MAE train: 0.529252	val: 0.647372	test: 0.575160

Epoch: 30
Loss: 0.5579237788915634
RMSE train: 0.668174	val: 0.830487	test: 0.742188
MAE train: 0.527264	val: 0.640898	test: 0.580587

Epoch: 31
Loss: 0.5170508857284274
RMSE train: 0.661963	val: 0.823219	test: 0.723581
MAE train: 0.517796	val: 0.630541	test: 0.560078

Epoch: 32
Loss: 0.5507957828896386
RMSE train: 0.680834	val: 0.838617	test: 0.754345
MAE train: 0.528527	val: 0.642413	test: 0.567515

Epoch: 33
Loss: 0.5691312381199428
RMSE train: 0.665155	val: 0.827837	test: 0.727083
MAE train: 0.518087	val: 0.622037	test: 0.553387

Epoch: 34
Loss: 0.531584518296378
RMSE train: 0.655600	val: 0.808684	test: 0.716970
MAE train: 0.512890	val: 0.629449	test: 0.558549

Epoch: 35
Loss: 0.5257932032857623
RMSE train: 0.643501	val: 0.810220	test: 0.719341
MAE train: 0.504890	val: 0.612793	test: 0.554710

Epoch: 36
Loss: 0.4770190737077168
RMSE train: 0.701967	val: 0.871775	test: 0.782149
MAE train: 0.549497	val: 0.671218	test: 0.590073

Epoch: 37
Loss: 0.5063020054783139
RMSE train: 0.631234	val: 0.804005	test: 0.703281
MAE train: 0.490718	val: 0.610616	test: 0.537073

Epoch: 38
Loss: 0.5110537196908679
RMSE train: 0.646668	val: 0.796865	test: 0.719187
MAE train: 0.507396	val: 0.614137	test: 0.543967

Epoch: 39
Loss: 0.4865478447505406
RMSE train: 0.626533	val: 0.787123	test: 0.699418
MAE train: 0.490875	val: 0.597753	test: 0.535028

Epoch: 40
Loss: 0.4842796879155295
RMSE train: 0.649979	val: 0.811927	test: 0.720835
MAE train: 0.509344	val: 0.620618	test: 0.545963

Epoch: 41
Loss: 0.4848544384752001
RMSE train: 0.634710	val: 0.811863	test: 0.721552
MAE train: 0.494048	val: 0.615358	test: 0.542843

Epoch: 42
Loss: 0.48554361292294096
RMSE train: 0.639910	val: 0.798269	test: 0.705946
MAE train: 0.503384	val: 0.605776	test: 0.550567

Epoch: 43
Loss: 0.4700782724789211
RMSE train: 0.659353	val: 0.816581	test: 0.728021
MAE train: 0.516522	val: 0.626571	test: 0.552823

Epoch: 44
Loss: 0.4681457664285387
RMSE train: 0.605167	val: 0.800549	test: 0.680208
MAE train: 0.471195	val: 0.589679	test: 0.522938

Epoch: 45
Loss: 0.4540525823831558
RMSE train: 0.626194	val: 0.801080	test: 0.705807
MAE train: 0.492181	val: 0.607920	test: 0.540105

Epoch: 46
Loss: 0.451201998761722
RMSE train: 0.610839	val: 0.786678	test: 0.689107
MAE train: 0.479347	val: 0.588730	test: 0.527184

Epoch: 47
Loss: 0.45211270451545715
RMSE train: 0.606120	val: 0.782569	test: 0.685903
MAE train: 0.474618	val: 0.588745	test: 0.525792

Epoch: 48
Loss: 0.42587675154209137
RMSE train: 0.635041	val: 0.805958	test: 0.712012
MAE train: 0.496556	val: 0.610409	test: 0.534835

Epoch: 49
Loss: 0.4357228683573859
RMSE train: 0.628950	val: 0.803684	test: 0.706580
MAE train: 0.488832	val: 0.608113	test: 0.533127

Epoch: 50
Loss: 0.4226516719375338
RMSE train: 0.600279	val: 0.789292	test: 0.672595
MAE train: 0.468007	val: 0.586075	test: 0.508775

Epoch: 51
Loss: 0.43434066006115507
RMSE train: 0.605909	val: 0.771496	test: 0.675725
MAE train: 0.471580	val: 0.580955	test: 0.511495

Epoch: 52
Loss: 0.43517254080091206
RMSE train: 0.586895	val: 0.782888	test: 0.692324
MAE train: 0.458198	val: 0.581217	test: 0.519643

Epoch: 53
Loss: 0.40846552167619976
RMSE train: 0.621327	val: 0.803478	test: 0.708410
MAE train: 0.483455	val: 0.603384	test: 0.536754

Epoch: 54
Loss: 0.41244998574256897
RMSE train: 0.613967	val: 0.791089	test: 0.701358
MAE train: 0.478384	val: 0.594178	test: 0.526477

Epoch: 55
Loss: 0.4309735468455723
RMSE train: 0.616146	val: 0.785940	test: 0.704155
MAE train: 0.477992	val: 0.589606	test: 0.528320

Epoch: 56
Loss: 0.4230920374393463
RMSE train: 0.616129	val: 0.799905	test: 0.712823
MAE train: 0.480136	val: 0.605976	test: 0.535014

Epoch: 57
Loss: 0.43405595421791077
RMSE train: 0.587118	val: 0.784013	test: 0.676405
MAE train: 0.454505	val: 0.577845	test: 0.511832

Epoch: 58
Loss: 0.40719298805509296
RMSE train: 0.583166	val: 0.768757	test: 0.681964
MAE train: 0.456302	val: 0.577639	test: 0.516040

Epoch: 59
Loss: 0.3984726220369339
RMSE train: 0.562720	val: 0.756445	test: 0.666401
MAE train: 0.438252	val: 0.564332	test: 0.501855

Epoch: 60
Loss: 0.41682828962802887
RMSE train: 0.589290	val: 0.778655	test: 0.693532
MAE train: 0.460723	val: 0.582653	test: 0.523609

Epoch: 61
Loss: 0.4057630236659731
RMSE train: 0.620847	val: 0.794313	test: 0.724015
MAE train: 0.479598	val: 0.595683	test: 0.533476

Epoch: 62
Loss: 0.38759549387863707
RMSE train: 0.572829	val: 0.755008	test: 0.695846
MAE train: 0.445586	val: 0.573729	test: 0.515923

Epoch: 63
Loss: 0.39529722716127125
RMSE train: 0.592506	val: 0.766725	test: 0.693785
MAE train: 0.458420	val: 0.583031	test: 0.517133

Epoch: 64
Loss: 0.3889405386788504
RMSE train: 0.572246	val: 0.757854	test: 0.662119
MAE train: 0.445620	val: 0.572284	test: 0.504426

Epoch: 65
Loss: 0.399627332176481
RMSE train: 0.587776	val: 0.788921	test: 0.690793
MAE train: 0.450289	val: 0.596301	test: 0.517848

Epoch: 66
Loss: 0.3843476176261902
RMSE train: 0.579686	val: 0.762556	test: 0.686961
MAE train: 0.450657	val: 0.581730	test: 0.512918

Epoch: 67
Loss: 0.37427586742809843
RMSE train: 0.567931	val: 0.750340	test: 0.697377
MAE train: 0.437937	val: 0.572746	test: 0.520086

Epoch: 68
Loss: 0.36665890898023334
RMSE train: 0.569283	val: 0.755480	test: 0.685571
MAE train: 0.442387	val: 0.575388	test: 0.524006

Epoch: 69
Loss: 0.3618479881967817
RMSE train: 0.561419	val: 0.765077	test: 0.668669
MAE train: 0.435869	val: 0.576149	test: 0.503213

Epoch: 70
Loss: 0.37753856607845854
RMSE train: 0.586037	val: 0.777009	test: 0.702726
MAE train: 0.452668	val: 0.588970	test: 0.525564

Epoch: 71
Loss: 0.35741112274783
RMSE train: 0.561618	val: 0.747434	test: 0.675856
MAE train: 0.434409	val: 0.559311	test: 0.500485

Epoch: 72
Loss: 0.3653412141970226
RMSE train: 0.554709	val: 0.758652	test: 0.668539
MAE train: 0.425552	val: 0.562606	test: 0.502425

Epoch: 73
Loss: 0.37359039059707094
RMSE train: 0.554118	val: 0.753658	test: 0.662505
MAE train: 0.430083	val: 0.557482	test: 0.490897

Epoch: 74
Loss: 0.372379794716835
RMSE train: 0.568807	val: 0.764185	test: 0.680560
MAE train: 0.445895	val: 0.579836	test: 0.527727

Epoch: 75
Loss: 0.37047957522528513
RMSE train: 0.544463	val: 0.748097	test: 0.662523
MAE train: 0.420390	val: 0.557418	test: 0.494327

Epoch: 76
Loss: 0.3540422660963876
RMSE train: 0.577726	val: 0.791984	test: 0.681381
MAE train: 0.447577	val: 0.584578	test: 0.505001

Epoch: 77
Loss: 0.374839295233999
RMSE train: 0.564021	val: 0.770009	test: 0.677002
MAE train: 0.434509	val: 0.573080	test: 0.503637

Epoch: 78
Loss: 0.359732306429318
RMSE train: 0.543714	val: 0.762103	test: 0.655182
MAE train: 0.420338	val: 0.562636	test: 0.486769

Epoch: 79
Loss: 0.36414651572704315
RMSE train: 0.535639	val: 0.743866	test: 0.657339
MAE train: 0.417505	val: 0.560381	test: 0.495591

Epoch: 80
Loss: 0.33840055976595196
RMSE train: 0.535738	val: 0.758524	test: 0.657637
MAE train: 0.413778	val: 0.564721	test: 0.490364

Epoch: 81
Loss: 0.3618619974170412
RMSE train: 0.524818	val: 0.752368	test: 0.645705
MAE train: 0.406962	val: 0.557014	test: 0.477602

Epoch: 82
Loss: 0.3534526973962784
RMSE train: 0.548748	val: 0.761051	test: 0.673984
MAE train: 0.425642	val: 0.570907	test: 0.500067

Epoch: 83
Loss: 0.34903930127620697
RMSE train: 0.538352	val: 0.738337	test: 0.658948

Epoch: 23
Loss: 0.5865148731640407
RMSE train: 0.710049	val: 0.849000	test: 0.779522
MAE train: 0.554930	val: 0.658656	test: 0.591658

Epoch: 24
Loss: 0.5858619127954755
RMSE train: 0.698102	val: 0.839580	test: 0.761903
MAE train: 0.543552	val: 0.648985	test: 0.576294

Epoch: 25
Loss: 0.585710095507758
RMSE train: 0.707922	val: 0.849926	test: 0.771727
MAE train: 0.554180	val: 0.663866	test: 0.586432

Epoch: 26
Loss: 0.5403561357940946
RMSE train: 0.706744	val: 0.846246	test: 0.771874
MAE train: 0.553743	val: 0.654162	test: 0.586035

Epoch: 27
Loss: 0.5566257898296628
RMSE train: 0.698023	val: 0.833359	test: 0.761925
MAE train: 0.548460	val: 0.645281	test: 0.573503

Epoch: 28
Loss: 0.5453182799475533
RMSE train: 0.679377	val: 0.830785	test: 0.751068
MAE train: 0.530226	val: 0.640139	test: 0.570278

Epoch: 29
Loss: 0.5280906068427222
RMSE train: 0.705364	val: 0.859714	test: 0.778350
MAE train: 0.555654	val: 0.663619	test: 0.589897

Epoch: 30
Loss: 0.5137267538479396
RMSE train: 0.668476	val: 0.814423	test: 0.741555
MAE train: 0.521819	val: 0.624126	test: 0.561248

Epoch: 31
Loss: 0.5334785687071937
RMSE train: 0.673657	val: 0.821422	test: 0.754800
MAE train: 0.527891	val: 0.631961	test: 0.569824

Epoch: 32
Loss: 0.511236914566585
RMSE train: 0.668694	val: 0.831834	test: 0.746066
MAE train: 0.519948	val: 0.641941	test: 0.560935

Epoch: 33
Loss: 0.4891098120382854
RMSE train: 0.670076	val: 0.824294	test: 0.741174
MAE train: 0.521847	val: 0.638668	test: 0.558442

Epoch: 34
Loss: 0.5084606621946607
RMSE train: 0.670206	val: 0.823275	test: 0.743549
MAE train: 0.521631	val: 0.633060	test: 0.558012

Epoch: 35
Loss: 0.5296088010072708
RMSE train: 0.661512	val: 0.815327	test: 0.728749
MAE train: 0.516814	val: 0.630685	test: 0.540661

Epoch: 36
Loss: 0.5195074315582003
RMSE train: 0.650267	val: 0.817803	test: 0.740943
MAE train: 0.508367	val: 0.628145	test: 0.560806

Epoch: 37
Loss: 0.4751699630703245
RMSE train: 0.645371	val: 0.800952	test: 0.714892
MAE train: 0.504772	val: 0.613427	test: 0.529732

Epoch: 38
Loss: 0.4937447266919272
RMSE train: 0.652218	val: 0.816496	test: 0.726208
MAE train: 0.508230	val: 0.623204	test: 0.538348

Epoch: 39
Loss: 0.49948039863790783
RMSE train: 0.633021	val: 0.806063	test: 0.716190
MAE train: 0.494944	val: 0.614623	test: 0.537380

Epoch: 40
Loss: 0.47885589727333616
RMSE train: 0.643799	val: 0.803902	test: 0.718325
MAE train: 0.503082	val: 0.612350	test: 0.540987

Epoch: 41
Loss: 0.4736116485936301
RMSE train: 0.647165	val: 0.815436	test: 0.723718
MAE train: 0.508740	val: 0.627284	test: 0.547729

Epoch: 42
Loss: 0.4789751448801586
RMSE train: 0.632708	val: 0.792157	test: 0.703265
MAE train: 0.494172	val: 0.603859	test: 0.529237

Epoch: 43
Loss: 0.4679629866565977
RMSE train: 0.602584	val: 0.757790	test: 0.682415
MAE train: 0.468896	val: 0.576941	test: 0.507250

Epoch: 44
Loss: 0.4566029416663306
RMSE train: 0.705823	val: 0.855835	test: 0.777920
MAE train: 0.551590	val: 0.658800	test: 0.587084

Epoch: 45
Loss: 0.43189894940171925
RMSE train: 0.629647	val: 0.804235	test: 0.712628
MAE train: 0.489186	val: 0.609604	test: 0.531112

Epoch: 46
Loss: 0.4472018288714545
RMSE train: 0.617287	val: 0.806709	test: 0.703037
MAE train: 0.481313	val: 0.612441	test: 0.523941

Epoch: 47
Loss: 0.4539608976670674
RMSE train: 0.610985	val: 0.798360	test: 0.698817
MAE train: 0.474329	val: 0.606784	test: 0.521405

Epoch: 48
Loss: 0.43170186451503206
RMSE train: 0.612157	val: 0.781138	test: 0.697365
MAE train: 0.478370	val: 0.595338	test: 0.525783

Epoch: 49
Loss: 0.45318190114838736
RMSE train: 0.598131	val: 0.768074	test: 0.696723
MAE train: 0.465924	val: 0.589207	test: 0.507397

Epoch: 50
Loss: 0.4380241845335279
RMSE train: 0.671034	val: 0.819857	test: 0.754188
MAE train: 0.524393	val: 0.630876	test: 0.568681

Epoch: 51
Loss: 0.44976588019302915
RMSE train: 0.619923	val: 0.799741	test: 0.705901
MAE train: 0.485746	val: 0.610511	test: 0.528876

Epoch: 52
Loss: 0.4408159873315266
RMSE train: 0.589312	val: 0.783747	test: 0.674733
MAE train: 0.460933	val: 0.592969	test: 0.500978

Epoch: 53
Loss: 0.43423896602221895
RMSE train: 0.579545	val: 0.760828	test: 0.685678
MAE train: 0.451999	val: 0.576203	test: 0.510871

Epoch: 54
Loss: 0.44422721437045504
RMSE train: 0.618035	val: 0.786453	test: 0.718114
MAE train: 0.481591	val: 0.600872	test: 0.538654

Epoch: 55
Loss: 0.4323965034314564
RMSE train: 0.620209	val: 0.792444	test: 0.712149
MAE train: 0.482229	val: 0.601350	test: 0.532556

Epoch: 56
Loss: 0.4235548568623407
RMSE train: 0.630784	val: 0.792591	test: 0.718738
MAE train: 0.491543	val: 0.608839	test: 0.546187

Epoch: 57
Loss: 0.4137525111436844
RMSE train: 0.586578	val: 0.767963	test: 0.677364
MAE train: 0.455044	val: 0.571869	test: 0.509634

Epoch: 58
Loss: 0.40545098057815004
RMSE train: 0.579235	val: 0.776138	test: 0.682039
MAE train: 0.453820	val: 0.581789	test: 0.505768

Epoch: 59
Loss: 0.39158057953630176
RMSE train: 0.598603	val: 0.780994	test: 0.698958
MAE train: 0.467831	val: 0.582433	test: 0.517460

Epoch: 60
Loss: 0.408541025859969
RMSE train: 0.628587	val: 0.813190	test: 0.737483
MAE train: 0.489867	val: 0.611689	test: 0.551259

Epoch: 61
Loss: 0.41582013240882326
RMSE train: 0.591724	val: 0.769437	test: 0.692989
MAE train: 0.463585	val: 0.582005	test: 0.513679

Epoch: 62
Loss: 0.39642679904188427
RMSE train: 0.600089	val: 0.779920	test: 0.699214
MAE train: 0.467306	val: 0.587525	test: 0.522069

Epoch: 63
Loss: 0.384928965142795
RMSE train: 0.601529	val: 0.773459	test: 0.697491
MAE train: 0.469400	val: 0.582408	test: 0.516744

Epoch: 64
Loss: 0.38684929055827005
RMSE train: 0.571550	val: 0.769955	test: 0.676518
MAE train: 0.447016	val: 0.575887	test: 0.501139

Epoch: 65
Loss: 0.3988199830055237
RMSE train: 0.575433	val: 0.775978	test: 0.683878
MAE train: 0.450837	val: 0.582583	test: 0.510102

Epoch: 66
Loss: 0.3952783005578177
RMSE train: 0.601474	val: 0.790228	test: 0.718496
MAE train: 0.469950	val: 0.596609	test: 0.537593

Epoch: 67
Loss: 0.36748598303113666
RMSE train: 0.556592	val: 0.763169	test: 0.671594
MAE train: 0.431829	val: 0.567519	test: 0.501230

Epoch: 68
Loss: 0.3626927520547594
RMSE train: 0.573299	val: 0.772629	test: 0.686210
MAE train: 0.446564	val: 0.579331	test: 0.510967

Epoch: 69
Loss: 0.36912178141730173
RMSE train: 0.570878	val: 0.784090	test: 0.684194
MAE train: 0.444090	val: 0.587302	test: 0.510736

Epoch: 70
Loss: 0.3736421912908554
RMSE train: 0.553763	val: 0.753217	test: 0.671775
MAE train: 0.431242	val: 0.565125	test: 0.500939

Epoch: 71
Loss: 0.3728177675179073
RMSE train: 0.549314	val: 0.754323	test: 0.672242
MAE train: 0.426851	val: 0.566550	test: 0.500399

Epoch: 72
Loss: 0.3561899257557733
RMSE train: 0.579956	val: 0.773795	test: 0.683882
MAE train: 0.452321	val: 0.575567	test: 0.509828

Epoch: 73
Loss: 0.35365118086338043
RMSE train: 0.539254	val: 0.745912	test: 0.661725
MAE train: 0.420363	val: 0.551297	test: 0.490150

Epoch: 74
Loss: 0.3627533252750124
RMSE train: 0.557879	val: 0.762504	test: 0.690489
MAE train: 0.435939	val: 0.571811	test: 0.513559

Epoch: 75
Loss: 0.34724243198122295
RMSE train: 0.536434	val: 0.754049	test: 0.682626
MAE train: 0.416687	val: 0.560565	test: 0.506284

Epoch: 76
Loss: 0.35161869653633665
RMSE train: 0.583739	val: 0.796329	test: 0.709411
MAE train: 0.457468	val: 0.598655	test: 0.532561

Epoch: 77
Loss: 0.3779799597603934
RMSE train: 0.601390	val: 0.796257	test: 0.723511
MAE train: 0.471812	val: 0.601721	test: 0.540262

Epoch: 78
Loss: 0.3668970061200006
RMSE train: 0.585925	val: 0.799878	test: 0.716788
MAE train: 0.459917	val: 0.601699	test: 0.535556

Epoch: 79
Loss: 0.35326912999153137
RMSE train: 0.538668	val: 0.769413	test: 0.658992
MAE train: 0.418012	val: 0.567853	test: 0.489310

Epoch: 80
Loss: 0.35246632780347553
RMSE train: 0.544911	val: 0.768387	test: 0.682199
MAE train: 0.427327	val: 0.565109	test: 0.501345

Epoch: 81
Loss: 0.3457203762871878
RMSE train: 0.594200	val: 0.834213	test: 0.726372
MAE train: 0.464366	val: 0.620934	test: 0.540723

Epoch: 82
Loss: 0.339526891708374
RMSE train: 0.532014	val: 0.754752	test: 0.667069
MAE train: 0.414109	val: 0.561793	test: 0.496736

Epoch: 83
Loss: 0.3291161060333252
RMSE train: 0.533010	val: 0.771708	test: 0.662174
MAE train: 0.408514	val: 0.510236	test: 0.531893

Epoch: 84
Loss: 0.31668777763843536
RMSE train: 0.540145	val: 0.711305	test: 0.721347
MAE train: 0.427170	val: 0.521554	test: 0.548250

Epoch: 85
Loss: 0.3362232893705368
RMSE train: 0.526945	val: 0.708787	test: 0.714742
MAE train: 0.414773	val: 0.520515	test: 0.543039

Epoch: 86
Loss: 0.3159225195646286
RMSE train: 0.539202	val: 0.730804	test: 0.720020
MAE train: 0.428110	val: 0.537914	test: 0.553543

Epoch: 87
Loss: 0.3046690791845322
RMSE train: 0.527111	val: 0.711412	test: 0.719218
MAE train: 0.415937	val: 0.524014	test: 0.548193

Epoch: 88
Loss: 0.30259206891059875
RMSE train: 0.512728	val: 0.698943	test: 0.698937
MAE train: 0.405681	val: 0.510596	test: 0.532281

Epoch: 89
Loss: 0.2965789258480072
RMSE train: 0.510742	val: 0.699606	test: 0.706651
MAE train: 0.402135	val: 0.516377	test: 0.537589

Epoch: 90
Loss: 0.29259311556816103
RMSE train: 0.521655	val: 0.702515	test: 0.696980
MAE train: 0.413124	val: 0.513854	test: 0.532060

Epoch: 91
Loss: 0.3037131607532501
RMSE train: 0.514832	val: 0.697035	test: 0.703756
MAE train: 0.402245	val: 0.514883	test: 0.539566

Epoch: 92
Loss: 0.3043141782283783
RMSE train: 0.506898	val: 0.705436	test: 0.705532
MAE train: 0.402006	val: 0.520277	test: 0.536638

Epoch: 93
Loss: 0.2999989718198776
RMSE train: 0.534279	val: 0.719953	test: 0.715407
MAE train: 0.421063	val: 0.530938	test: 0.547006

Epoch: 94
Loss: 0.2991515815258026
RMSE train: 0.516437	val: 0.709211	test: 0.713339
MAE train: 0.407658	val: 0.523354	test: 0.544805

Epoch: 95
Loss: 0.2943055063486099
RMSE train: 0.507332	val: 0.708661	test: 0.710061
MAE train: 0.399975	val: 0.516829	test: 0.539114

Epoch: 96
Loss: 0.3001622200012207
RMSE train: 0.495679	val: 0.691870	test: 0.695739
MAE train: 0.391815	val: 0.505220	test: 0.529171

Epoch: 97
Loss: 0.29576247185468674
RMSE train: 0.493912	val: 0.694638	test: 0.699650
MAE train: 0.391229	val: 0.510423	test: 0.533758

Epoch: 98
Loss: 0.28755777776241304
RMSE train: 0.508796	val: 0.697297	test: 0.706296
MAE train: 0.399967	val: 0.514745	test: 0.538804

Epoch: 99
Loss: 0.2936383545398712
RMSE train: 0.483338	val: 0.686832	test: 0.690281
MAE train: 0.383269	val: 0.505456	test: 0.522119

Epoch: 100
Loss: 0.2811876595020294
RMSE train: 0.493384	val: 0.704739	test: 0.704776
MAE train: 0.388868	val: 0.514861	test: 0.532674

Epoch: 101
Loss: 0.2894067168235779
RMSE train: 0.518158	val: 0.725599	test: 0.712076
MAE train: 0.407253	val: 0.528636	test: 0.542526

Epoch: 102
Loss: 0.2763760477304459
RMSE train: 0.476704	val: 0.689365	test: 0.697110
MAE train: 0.376572	val: 0.509919	test: 0.527046

Epoch: 103
Loss: 0.2858491688966751
RMSE train: 0.514268	val: 0.700025	test: 0.710496
MAE train: 0.405922	val: 0.516558	test: 0.540787

Epoch: 104
Loss: 0.26914292871952056
RMSE train: 0.486756	val: 0.699433	test: 0.698470
MAE train: 0.384004	val: 0.506041	test: 0.529014

Epoch: 105
Loss: 0.2774693310260773
RMSE train: 0.567946	val: 0.759816	test: 0.752380
MAE train: 0.451789	val: 0.570300	test: 0.576819

Epoch: 106
Loss: 0.2769486039876938
RMSE train: 0.533089	val: 0.732389	test: 0.719820
MAE train: 0.421854	val: 0.542220	test: 0.552147

Epoch: 107
Loss: 0.29054526835680006
RMSE train: 0.475231	val: 0.693379	test: 0.693684
MAE train: 0.373437	val: 0.505837	test: 0.517253

Epoch: 108
Loss: 0.27240250557661055
RMSE train: 0.490667	val: 0.711327	test: 0.708203
MAE train: 0.387592	val: 0.521035	test: 0.534697

Epoch: 109
Loss: 0.2843721598386765
RMSE train: 0.476901	val: 0.695935	test: 0.698391
MAE train: 0.377638	val: 0.512077	test: 0.529595

Epoch: 110
Loss: 0.2753341495990753
RMSE train: 0.477777	val: 0.697509	test: 0.698297
MAE train: 0.376307	val: 0.504273	test: 0.523358

Epoch: 111
Loss: 0.2632168591022491
RMSE train: 0.484364	val: 0.701501	test: 0.699266
MAE train: 0.380529	val: 0.516666	test: 0.531590

Epoch: 112
Loss: 0.26773629784584047
RMSE train: 0.472316	val: 0.698295	test: 0.690946
MAE train: 0.372669	val: 0.506701	test: 0.524572

Epoch: 113
Loss: 0.26770084351301193
RMSE train: 0.463509	val: 0.690029	test: 0.687607
MAE train: 0.365421	val: 0.500421	test: 0.520711

Epoch: 114
Loss: 0.25395342260599135
RMSE train: 0.466717	val: 0.698491	test: 0.693322
MAE train: 0.369702	val: 0.509200	test: 0.529528

Epoch: 115
Loss: 0.2743791863322258
RMSE train: 0.474154	val: 0.693110	test: 0.690185
MAE train: 0.374053	val: 0.505879	test: 0.521617

Epoch: 116
Loss: 0.264314678311348
RMSE train: 0.460172	val: 0.695520	test: 0.677451
MAE train: 0.364528	val: 0.502987	test: 0.512304

Epoch: 117
Loss: 0.27309503853321077
RMSE train: 0.513104	val: 0.708214	test: 0.715911
MAE train: 0.403878	val: 0.520255	test: 0.545229

Epoch: 118
Loss: 0.2469887375831604
RMSE train: 0.461677	val: 0.694469	test: 0.682852
MAE train: 0.365702	val: 0.506914	test: 0.513282

Epoch: 119
Loss: 0.25208682417869566
RMSE train: 0.495919	val: 0.703528	test: 0.703098
MAE train: 0.390008	val: 0.518921	test: 0.528991

Epoch: 120
Loss: 0.24622987806797028
RMSE train: 0.464657	val: 0.690892	test: 0.695372
MAE train: 0.364745	val: 0.500952	test: 0.520801

Epoch: 121
Loss: 0.23927612900733947
RMSE train: 0.452891	val: 0.693070	test: 0.692878
MAE train: 0.357402	val: 0.500288	test: 0.518860

Early stopping
Best (RMSE):	 train: 0.508848	val: 0.684334	test: 0.709685
Best (MAE):	 train: 0.399386	val: 0.507502	test: 0.536802

MAE train: 0.399456	val: 0.522241	test: 0.533793

Epoch: 84
Loss: 0.3168884843587875
RMSE train: 0.511292	val: 0.695997	test: 0.702284
MAE train: 0.399951	val: 0.518085	test: 0.534578

Epoch: 85
Loss: 0.3068013578653336
RMSE train: 0.497022	val: 0.706542	test: 0.702306
MAE train: 0.389827	val: 0.517342	test: 0.528143

Epoch: 86
Loss: 0.29945300072431563
RMSE train: 0.495299	val: 0.707098	test: 0.706028
MAE train: 0.389118	val: 0.524191	test: 0.530438

Epoch: 87
Loss: 0.3246017724275589
RMSE train: 0.511535	val: 0.713109	test: 0.709318
MAE train: 0.407467	val: 0.539057	test: 0.545006

Epoch: 88
Loss: 0.30675199031829836
RMSE train: 0.487204	val: 0.705229	test: 0.700447
MAE train: 0.383846	val: 0.523382	test: 0.533015

Epoch: 89
Loss: 0.30353411138057707
RMSE train: 0.493093	val: 0.704075	test: 0.697640
MAE train: 0.388548	val: 0.518264	test: 0.523399

Epoch: 90
Loss: 0.2946872770786285
RMSE train: 0.492832	val: 0.704158	test: 0.703973
MAE train: 0.390777	val: 0.517603	test: 0.524884

Epoch: 91
Loss: 0.29996501803398135
RMSE train: 0.488924	val: 0.704890	test: 0.695921
MAE train: 0.384602	val: 0.522828	test: 0.532779

Epoch: 92
Loss: 0.30084926187992095
RMSE train: 0.463299	val: 0.693811	test: 0.696183
MAE train: 0.362753	val: 0.505485	test: 0.521107

Epoch: 93
Loss: 0.30109753459692
RMSE train: 0.487733	val: 0.705232	test: 0.709212
MAE train: 0.383726	val: 0.524408	test: 0.534926

Epoch: 94
Loss: 0.28080642521381377
RMSE train: 0.474938	val: 0.682370	test: 0.702748
MAE train: 0.373525	val: 0.503795	test: 0.526666

Epoch: 95
Loss: 0.2854566514492035
RMSE train: 0.492139	val: 0.700190	test: 0.708698
MAE train: 0.387685	val: 0.517231	test: 0.533069

Epoch: 96
Loss: 0.29163541793823244
RMSE train: 0.485197	val: 0.710795	test: 0.703481
MAE train: 0.380496	val: 0.519650	test: 0.530324

Epoch: 97
Loss: 0.28897589445114136
RMSE train: 0.476230	val: 0.702069	test: 0.693898
MAE train: 0.372225	val: 0.512474	test: 0.524866

Epoch: 98
Loss: 0.2912833034992218
RMSE train: 0.460467	val: 0.689891	test: 0.686562
MAE train: 0.360709	val: 0.508011	test: 0.516967

Epoch: 99
Loss: 0.27305344939231874
RMSE train: 0.493421	val: 0.702231	test: 0.704198
MAE train: 0.387008	val: 0.518040	test: 0.531601

Epoch: 100
Loss: 0.28837230801582336
RMSE train: 0.486429	val: 0.708557	test: 0.701948
MAE train: 0.379191	val: 0.516091	test: 0.531054

Epoch: 101
Loss: 0.28064342141151427
RMSE train: 0.478469	val: 0.704269	test: 0.701884
MAE train: 0.376583	val: 0.519915	test: 0.531455

Epoch: 102
Loss: 0.2792158842086792
RMSE train: 0.473326	val: 0.703864	test: 0.690336
MAE train: 0.371975	val: 0.515971	test: 0.517698

Epoch: 103
Loss: 0.296314337849617
RMSE train: 0.490095	val: 0.703177	test: 0.713208
MAE train: 0.384166	val: 0.519200	test: 0.532376

Epoch: 104
Loss: 0.28188800662755964
RMSE train: 0.475410	val: 0.689311	test: 0.708443
MAE train: 0.373248	val: 0.506777	test: 0.531745

Epoch: 105
Loss: 0.27378152757883073
RMSE train: 0.473676	val: 0.696254	test: 0.701927
MAE train: 0.370264	val: 0.510488	test: 0.529490

Epoch: 106
Loss: 0.27816406786441805
RMSE train: 0.463845	val: 0.693164	test: 0.689524
MAE train: 0.364179	val: 0.505853	test: 0.518931

Epoch: 107
Loss: 0.28245503902435304
RMSE train: 0.465785	val: 0.697054	test: 0.694237
MAE train: 0.367002	val: 0.507175	test: 0.517150

Epoch: 108
Loss: 0.2583009123802185
RMSE train: 0.468393	val: 0.700413	test: 0.696323
MAE train: 0.366996	val: 0.512101	test: 0.522739

Epoch: 109
Loss: 0.2651021495461464
RMSE train: 0.481614	val: 0.721494	test: 0.701349
MAE train: 0.378885	val: 0.520232	test: 0.527354

Epoch: 110
Loss: 0.26734112948179245
RMSE train: 0.452699	val: 0.696019	test: 0.690238
MAE train: 0.354368	val: 0.507954	test: 0.518281

Epoch: 111
Loss: 0.2710884302854538
RMSE train: 0.453971	val: 0.682615	test: 0.694132
MAE train: 0.356039	val: 0.503495	test: 0.519088

Epoch: 112
Loss: 0.25086589306592944
RMSE train: 0.463619	val: 0.700763	test: 0.695247
MAE train: 0.365609	val: 0.506532	test: 0.517249

Epoch: 113
Loss: 0.2642502188682556
RMSE train: 0.451317	val: 0.680704	test: 0.687678
MAE train: 0.355681	val: 0.502447	test: 0.516779

Epoch: 114
Loss: 0.26546113193035126
RMSE train: 0.462390	val: 0.687558	test: 0.709081
MAE train: 0.359229	val: 0.503736	test: 0.526060

Epoch: 115
Loss: 0.26651381850242617
RMSE train: 0.473013	val: 0.692466	test: 0.703243
MAE train: 0.370078	val: 0.510026	test: 0.528483

Epoch: 116
Loss: 0.2592634528875351
RMSE train: 0.481328	val: 0.693302	test: 0.700061
MAE train: 0.377924	val: 0.513337	test: 0.525089

Epoch: 117
Loss: 0.2578096315264702
RMSE train: 0.450016	val: 0.690850	test: 0.697228
MAE train: 0.350156	val: 0.505385	test: 0.518478

Epoch: 118
Loss: 0.26980261504650116
RMSE train: 0.480284	val: 0.706095	test: 0.702514
MAE train: 0.381957	val: 0.519106	test: 0.524736

Epoch: 119
Loss: 0.2619744285941124
RMSE train: 0.452743	val: 0.683042	test: 0.694608
MAE train: 0.356636	val: 0.501844	test: 0.514918

Epoch: 120
Loss: 0.24385549873113632
RMSE train: 0.438970	val: 0.685627	test: 0.694148
MAE train: 0.345563	val: 0.506157	test: 0.514796

Epoch: 121
Loss: 0.24495606273412704
RMSE train: 0.437896	val: 0.686224	test: 0.687206
MAE train: 0.344886	val: 0.497904	test: 0.506776

Epoch: 122
Loss: 0.25336656868457796
RMSE train: 0.437003	val: 0.686472	test: 0.697235
MAE train: 0.342990	val: 0.501027	test: 0.514913

Epoch: 123
Loss: 0.2340244933962822
RMSE train: 0.459972	val: 0.704577	test: 0.717678
MAE train: 0.360933	val: 0.510289	test: 0.525029

Epoch: 124
Loss: 0.2555339738726616
RMSE train: 0.473618	val: 0.705925	test: 0.714393
MAE train: 0.372611	val: 0.516463	test: 0.531939

Epoch: 125
Loss: 0.25294561237096785
RMSE train: 0.438142	val: 0.686768	test: 0.699789
MAE train: 0.344395	val: 0.497244	test: 0.514174

Epoch: 126
Loss: 0.2394113913178444
RMSE train: 0.451823	val: 0.688332	test: 0.694981
MAE train: 0.356373	val: 0.498586	test: 0.512445

Epoch: 127
Loss: 0.23822346776723863
RMSE train: 0.439580	val: 0.680366	test: 0.688559
MAE train: 0.345610	val: 0.497203	test: 0.513434

Epoch: 128
Loss: 0.23907577991485596
RMSE train: 0.430556	val: 0.679422	test: 0.691332
MAE train: 0.338040	val: 0.498572	test: 0.515504

Epoch: 129
Loss: 0.233905827999115
RMSE train: 0.438923	val: 0.671941	test: 0.694670
MAE train: 0.344041	val: 0.493894	test: 0.513547

Epoch: 130
Loss: 0.23739380091428758
RMSE train: 0.438512	val: 0.690049	test: 0.693266
MAE train: 0.344798	val: 0.491785	test: 0.505738

Epoch: 131
Loss: 0.2384967789053917
RMSE train: 0.434542	val: 0.685606	test: 0.698163
MAE train: 0.342031	val: 0.499660	test: 0.516594

Epoch: 132
Loss: 0.23226124197244644
RMSE train: 0.438567	val: 0.689699	test: 0.689392
MAE train: 0.346084	val: 0.498960	test: 0.510027

Epoch: 133
Loss: 0.24702938050031661
RMSE train: 0.439552	val: 0.684663	test: 0.697005
MAE train: 0.346917	val: 0.509776	test: 0.523776

Epoch: 134
Loss: 0.23473187834024428
RMSE train: 0.424212	val: 0.671324	test: 0.687449
MAE train: 0.332067	val: 0.486376	test: 0.510281

Epoch: 135
Loss: 0.22490809261798858
RMSE train: 0.432573	val: 0.677712	test: 0.693500
MAE train: 0.339239	val: 0.496216	test: 0.518150

Epoch: 136
Loss: 0.22874007523059844
RMSE train: 0.437175	val: 0.682820	test: 0.679431
MAE train: 0.343144	val: 0.491526	test: 0.504481

Epoch: 137
Loss: 0.2293379560112953
RMSE train: 0.422352	val: 0.678257	test: 0.676765
MAE train: 0.331233	val: 0.492701	test: 0.502379

Epoch: 138
Loss: 0.23355999141931533
RMSE train: 0.433831	val: 0.688174	test: 0.689175
MAE train: 0.342450	val: 0.501256	test: 0.513405

Epoch: 139
Loss: 0.23528770208358765
RMSE train: 0.438942	val: 0.678379	test: 0.688842
MAE train: 0.344280	val: 0.488767	test: 0.513216

Epoch: 140
Loss: 0.23104469031095504
RMSE train: 0.436754	val: 0.680671	test: 0.692389
MAE train: 0.343555	val: 0.504614	test: 0.522321

Epoch: 141
Loss: 0.23823875784873963
RMSE train: 0.448617	val: 0.685612	test: 0.694692
MAE train: 0.350689	val: 0.495318	test: 0.521064

Epoch: 142
Loss: 0.23231865614652633
RMSE train: 0.449672	val: 0.699610	test: 0.693773
MAE train: 0.353713	val: 0.502493	test: 0.516738

Epoch: 143
Loss: 0.23080264180898666
RMSE train: 0.417117	val: 0.669264	test: 0.678909
MAE train: 0.326879	val: 0.488109	test: 0.507937
MAE train: 0.402838	val: 0.513522	test: 0.536823

Epoch: 84
Loss: 0.30888239443302157
RMSE train: 0.496866	val: 0.692920	test: 0.697075
MAE train: 0.392226	val: 0.510071	test: 0.529967

Epoch: 85
Loss: 0.3347773224115372
RMSE train: 0.513332	val: 0.695229	test: 0.718796
MAE train: 0.404048	val: 0.509447	test: 0.542275

Epoch: 86
Loss: 0.34288170337677004
RMSE train: 0.557469	val: 0.722732	test: 0.743526
MAE train: 0.439237	val: 0.533750	test: 0.569394

Epoch: 87
Loss: 0.30546735227108
RMSE train: 0.511526	val: 0.687157	test: 0.711621
MAE train: 0.402570	val: 0.513208	test: 0.540812

Epoch: 88
Loss: 0.31960389018058777
RMSE train: 0.484995	val: 0.671436	test: 0.694110
MAE train: 0.380839	val: 0.493617	test: 0.529699

Epoch: 89
Loss: 0.29675533920526503
RMSE train: 0.524141	val: 0.698391	test: 0.715430
MAE train: 0.413317	val: 0.514303	test: 0.541888

Epoch: 90
Loss: 0.3100862443447113
RMSE train: 0.510000	val: 0.700494	test: 0.711386
MAE train: 0.401686	val: 0.519607	test: 0.540116

Epoch: 91
Loss: 0.3165412455797195
RMSE train: 0.505955	val: 0.688803	test: 0.710109
MAE train: 0.395996	val: 0.506067	test: 0.534622

Epoch: 92
Loss: 0.2980546087026596
RMSE train: 0.495752	val: 0.688130	test: 0.703986
MAE train: 0.391191	val: 0.510372	test: 0.535206

Epoch: 93
Loss: 0.29574928283691404
RMSE train: 0.501648	val: 0.687547	test: 0.702953
MAE train: 0.396526	val: 0.502863	test: 0.533357

Epoch: 94
Loss: 0.30464098155498504
RMSE train: 0.502758	val: 0.698127	test: 0.715503
MAE train: 0.394372	val: 0.512447	test: 0.536976

Epoch: 95
Loss: 0.31308883130550386
RMSE train: 0.490739	val: 0.698434	test: 0.690412
MAE train: 0.387372	val: 0.508484	test: 0.521858

Epoch: 96
Loss: 0.278568834066391
RMSE train: 0.487916	val: 0.689361	test: 0.697459
MAE train: 0.385661	val: 0.508770	test: 0.529697

Epoch: 97
Loss: 0.2870918959379196
RMSE train: 0.465090	val: 0.678579	test: 0.694546
MAE train: 0.365899	val: 0.495304	test: 0.520681

Epoch: 98
Loss: 0.2845674306154251
RMSE train: 0.511216	val: 0.705174	test: 0.704175
MAE train: 0.401771	val: 0.517617	test: 0.535725

Epoch: 99
Loss: 0.293358701467514
RMSE train: 0.477071	val: 0.680548	test: 0.688909
MAE train: 0.376309	val: 0.495736	test: 0.525621

Epoch: 100
Loss: 0.2939609557390213
RMSE train: 0.475524	val: 0.686953	test: 0.700264
MAE train: 0.375977	val: 0.502314	test: 0.525965

Epoch: 101
Loss: 0.277115298807621
RMSE train: 0.482141	val: 0.701727	test: 0.694048
MAE train: 0.380295	val: 0.509275	test: 0.525914

Epoch: 102
Loss: 0.27334995567798615
RMSE train: 0.473353	val: 0.685107	test: 0.694843
MAE train: 0.370945	val: 0.495989	test: 0.527136

Epoch: 103
Loss: 0.27259603440761565
RMSE train: 0.471600	val: 0.691209	test: 0.698919
MAE train: 0.371326	val: 0.505929	test: 0.524185

Epoch: 104
Loss: 0.2678541511297226
RMSE train: 0.473695	val: 0.687976	test: 0.701349
MAE train: 0.372707	val: 0.502008	test: 0.523384

Epoch: 105
Loss: 0.2820603370666504
RMSE train: 0.479341	val: 0.690719	test: 0.698499
MAE train: 0.376596	val: 0.499545	test: 0.525574

Epoch: 106
Loss: 0.29245133996009826
RMSE train: 0.473021	val: 0.684856	test: 0.690977
MAE train: 0.371098	val: 0.498458	test: 0.523951

Epoch: 107
Loss: 0.2804976314306259
RMSE train: 0.464956	val: 0.684042	test: 0.680610
MAE train: 0.367649	val: 0.499924	test: 0.513002

Epoch: 108
Loss: 0.26506142020225526
RMSE train: 0.465029	val: 0.690625	test: 0.699859
MAE train: 0.366455	val: 0.500875	test: 0.523489

Epoch: 109
Loss: 0.27751565277576445
RMSE train: 0.462395	val: 0.684565	test: 0.693315
MAE train: 0.363111	val: 0.500598	test: 0.525227

Epoch: 110
Loss: 0.26926229596138
RMSE train: 0.454572	val: 0.678846	test: 0.687062
MAE train: 0.357487	val: 0.498828	test: 0.521560

Epoch: 111
Loss: 0.26927401721477506
RMSE train: 0.476330	val: 0.693419	test: 0.696267
MAE train: 0.374745	val: 0.506020	test: 0.521124

Epoch: 112
Loss: 0.25291216373443604
RMSE train: 0.501974	val: 0.718450	test: 0.718308
MAE train: 0.393815	val: 0.528167	test: 0.544149

Epoch: 113
Loss: 0.2591347649693489
RMSE train: 0.512390	val: 0.718122	test: 0.722474
MAE train: 0.400780	val: 0.528202	test: 0.548784

Epoch: 114
Loss: 0.26572274714708327
RMSE train: 0.472460	val: 0.685226	test: 0.695739
MAE train: 0.373240	val: 0.508730	test: 0.523274

Epoch: 115
Loss: 0.2696181520819664
RMSE train: 0.453801	val: 0.679164	test: 0.694870
MAE train: 0.357917	val: 0.500994	test: 0.521844

Epoch: 116
Loss: 0.26407831311225893
RMSE train: 0.451618	val: 0.673483	test: 0.692993
MAE train: 0.354158	val: 0.493142	test: 0.520566

Epoch: 117
Loss: 0.26388263404369355
RMSE train: 0.446964	val: 0.669371	test: 0.689442
MAE train: 0.349722	val: 0.488332	test: 0.521063

Epoch: 118
Loss: 0.2734517753124237
RMSE train: 0.444864	val: 0.682240	test: 0.689390
MAE train: 0.350126	val: 0.498122	test: 0.515561

Epoch: 119
Loss: 0.2604308038949966
RMSE train: 0.468923	val: 0.689274	test: 0.692187
MAE train: 0.367663	val: 0.502739	test: 0.523518

Epoch: 120
Loss: 0.2560680001974106
RMSE train: 0.470989	val: 0.689170	test: 0.701014
MAE train: 0.372393	val: 0.506649	test: 0.530740

Epoch: 121
Loss: 0.2522899180650711
RMSE train: 0.470479	val: 0.692782	test: 0.696982
MAE train: 0.368741	val: 0.506528	test: 0.525422

Epoch: 122
Loss: 0.2601227179169655
RMSE train: 0.488007	val: 0.687564	test: 0.709870
MAE train: 0.384775	val: 0.514948	test: 0.539697

Epoch: 123
Loss: 0.25107343792915343
RMSE train: 0.453082	val: 0.684485	test: 0.682748
MAE train: 0.355649	val: 0.496467	test: 0.515675

Epoch: 124
Loss: 0.24259910881519317
RMSE train: 0.449841	val: 0.674623	test: 0.691292
MAE train: 0.351783	val: 0.497593	test: 0.521061

Epoch: 125
Loss: 0.2507915019989014
RMSE train: 0.433150	val: 0.670839	test: 0.684420
MAE train: 0.339237	val: 0.493421	test: 0.510256

Epoch: 126
Loss: 0.246947680413723
RMSE train: 0.444314	val: 0.686607	test: 0.680936
MAE train: 0.348580	val: 0.497141	test: 0.516929

Epoch: 127
Loss: 0.24810748398303986
RMSE train: 0.441802	val: 0.684127	test: 0.684256
MAE train: 0.347750	val: 0.497913	test: 0.518345

Epoch: 128
Loss: 0.23676573038101195
RMSE train: 0.454070	val: 0.696007	test: 0.695806
MAE train: 0.356258	val: 0.512525	test: 0.528553

Epoch: 129
Loss: 0.2573002889752388
RMSE train: 0.468229	val: 0.697653	test: 0.693285
MAE train: 0.369054	val: 0.511036	test: 0.528226

Epoch: 130
Loss: 0.2414409339427948
RMSE train: 0.434115	val: 0.682292	test: 0.693005
MAE train: 0.342422	val: 0.504298	test: 0.518060

Epoch: 131
Loss: 0.2394850254058838
RMSE train: 0.434759	val: 0.676611	test: 0.675319
MAE train: 0.339853	val: 0.491337	test: 0.511071

Epoch: 132
Loss: 0.24838921427726746
RMSE train: 0.449558	val: 0.688617	test: 0.676676
MAE train: 0.354022	val: 0.502621	test: 0.512897

Epoch: 133
Loss: 0.23664116561412812
RMSE train: 0.422195	val: 0.683292	test: 0.684190
MAE train: 0.332676	val: 0.494797	test: 0.509388

Epoch: 134
Loss: 0.2327202007174492
RMSE train: 0.450093	val: 0.694613	test: 0.691008
MAE train: 0.353639	val: 0.500889	test: 0.522549

Epoch: 135
Loss: 0.2320503756403923
RMSE train: 0.438881	val: 0.680711	test: 0.690154
MAE train: 0.345745	val: 0.494830	test: 0.519027

Epoch: 136
Loss: 0.2329549416899681
RMSE train: 0.422933	val: 0.682719	test: 0.684909
MAE train: 0.331984	val: 0.492646	test: 0.511552

Epoch: 137
Loss: 0.23281281739473342
RMSE train: 0.436795	val: 0.680679	test: 0.689707
MAE train: 0.342651	val: 0.495944	test: 0.520293

Epoch: 138
Loss: 0.24023654460906982
RMSE train: 0.442073	val: 0.680475	test: 0.684977
MAE train: 0.348206	val: 0.498928	test: 0.519914

Epoch: 139
Loss: 0.23873382657766343
RMSE train: 0.433303	val: 0.692329	test: 0.684549
MAE train: 0.341643	val: 0.503986	test: 0.516353

Epoch: 140
Loss: 0.22581785768270493
RMSE train: 0.424196	val: 0.683447	test: 0.680208
MAE train: 0.333104	val: 0.495502	test: 0.510949

Epoch: 141
Loss: 0.22293752580881118
RMSE train: 0.438975	val: 0.695131	test: 0.693543
MAE train: 0.342088	val: 0.506344	test: 0.523119

Epoch: 142
Loss: 0.226738440990448
RMSE train: 0.444931	val: 0.687655	test: 0.691521
MAE train: 0.350262	val: 0.497793	test: 0.523514

Epoch: 143
Loss: 0.22675391733646394
RMSE train: 0.438106	val: 0.681572	test: 0.685595
MAE train: 0.343716	val: 0.490589	test: 0.514803

Epoch: 144
Loss: 0.2249186486005783
RMSE train: 0.421177	val: 0.676742	test: 0.673689
MAE train: 0.329484	val: 0.493660	test: 0.509720

Epoch: 145
Loss: 0.22622331380844116
RMSE train: 0.446401	val: 0.692159	test: 0.690540
MAE train: 0.353218	val: 0.509194	test: 0.523332

Epoch: 146
Loss: 0.22952610850334168
RMSE train: 0.427062	val: 0.682039	test: 0.681700
MAE train: 0.336038	val: 0.491604	test: 0.514935

Epoch: 147
Loss: 0.22583794146776198
RMSE train: 0.426877	val: 0.683016	test: 0.683802
MAE train: 0.335803	val: 0.500639	test: 0.517991

Epoch: 148
Loss: 0.22310501486063003
RMSE train: 0.435676	val: 0.683085	test: 0.680523
MAE train: 0.344059	val: 0.502890	test: 0.515770

Epoch: 149
Loss: 0.22494245022535325
RMSE train: 0.425574	val: 0.674405	test: 0.680082
MAE train: 0.333292	val: 0.485745	test: 0.510049

Epoch: 150
Loss: 0.22069980204105377
RMSE train: 0.427197	val: 0.676691	test: 0.680757
MAE train: 0.335297	val: 0.493749	test: 0.511703

Epoch: 151
Loss: 0.21315387338399888
RMSE train: 0.426712	val: 0.685353	test: 0.678500
MAE train: 0.338169	val: 0.498191	test: 0.510765

Epoch: 152
Loss: 0.22579595744609832
RMSE train: 0.423329	val: 0.685781	test: 0.682779
MAE train: 0.334454	val: 0.500981	test: 0.512302

Early stopping
Best (RMSE):	 train: 0.446964	val: 0.669371	test: 0.689442
Best (MAE):	 train: 0.349722	val: 0.488332	test: 0.521063

MAE train: 0.410404	val: 0.525726	test: 0.521229

Epoch: 84
Loss: 0.3223219042023023
RMSE train: 0.498485	val: 0.694619	test: 0.658914
MAE train: 0.391620	val: 0.511039	test: 0.503399

Epoch: 85
Loss: 0.30960149814685184
RMSE train: 0.506855	val: 0.699356	test: 0.663540
MAE train: 0.400518	val: 0.510591	test: 0.504898

Epoch: 86
Loss: 0.31743300954500836
RMSE train: 0.512791	val: 0.707124	test: 0.677078
MAE train: 0.403441	val: 0.525443	test: 0.522852

Epoch: 87
Loss: 0.31498240182797116
RMSE train: 0.501439	val: 0.709192	test: 0.662282
MAE train: 0.394081	val: 0.517750	test: 0.507712

Epoch: 88
Loss: 0.3174258420864741
RMSE train: 0.501806	val: 0.690987	test: 0.654651
MAE train: 0.395075	val: 0.505492	test: 0.495627

Epoch: 89
Loss: 0.30545928329229355
RMSE train: 0.491389	val: 0.694273	test: 0.660467
MAE train: 0.386423	val: 0.511128	test: 0.500121

Epoch: 90
Loss: 0.32414015630880993
RMSE train: 0.503233	val: 0.710065	test: 0.665418
MAE train: 0.398364	val: 0.523106	test: 0.516258

Epoch: 91
Loss: 0.31324740250905353
RMSE train: 0.538762	val: 0.733775	test: 0.682928
MAE train: 0.421466	val: 0.528831	test: 0.527758

Epoch: 92
Loss: 0.30594495435555774
RMSE train: 0.488244	val: 0.707029	test: 0.665282
MAE train: 0.381391	val: 0.516821	test: 0.511912

Epoch: 93
Loss: 0.3028901368379593
RMSE train: 0.487858	val: 0.693751	test: 0.663263
MAE train: 0.382443	val: 0.506935	test: 0.507281

Epoch: 94
Loss: 0.2935964837670326
RMSE train: 0.506023	val: 0.711542	test: 0.674121
MAE train: 0.399730	val: 0.527843	test: 0.516990

Epoch: 95
Loss: 0.28306729222337407
RMSE train: 0.483890	val: 0.693327	test: 0.669940
MAE train: 0.378602	val: 0.508803	test: 0.512457

Epoch: 96
Loss: 0.29875413576761883
RMSE train: 0.476213	val: 0.699404	test: 0.662740
MAE train: 0.373756	val: 0.501871	test: 0.506476

Epoch: 97
Loss: 0.2903650974233945
RMSE train: 0.491080	val: 0.711032	test: 0.673320
MAE train: 0.386225	val: 0.515799	test: 0.514423

Epoch: 98
Loss: 0.29815344760815304
RMSE train: 0.485624	val: 0.699934	test: 0.670543
MAE train: 0.382383	val: 0.500864	test: 0.511288

Epoch: 99
Loss: 0.284915787478288
RMSE train: 0.502756	val: 0.717162	test: 0.678009
MAE train: 0.396705	val: 0.524747	test: 0.523545

Epoch: 100
Loss: 0.287892176459233
RMSE train: 0.513762	val: 0.725458	test: 0.690635
MAE train: 0.408157	val: 0.540899	test: 0.535019

Epoch: 101
Loss: 0.2865569666028023
RMSE train: 0.507053	val: 0.721176	test: 0.689744
MAE train: 0.397969	val: 0.533346	test: 0.525462

Epoch: 102
Loss: 0.2849334701895714
RMSE train: 0.467306	val: 0.694285	test: 0.658453
MAE train: 0.367247	val: 0.502030	test: 0.499052

Epoch: 103
Loss: 0.2822584832708041
RMSE train: 0.460920	val: 0.686937	test: 0.653504
MAE train: 0.362656	val: 0.497640	test: 0.500899

Epoch: 104
Loss: 0.27662619575858116
RMSE train: 0.490206	val: 0.698984	test: 0.670397
MAE train: 0.387892	val: 0.506752	test: 0.513379

Epoch: 105
Loss: 0.2804640755057335
RMSE train: 0.471343	val: 0.691565	test: 0.668071
MAE train: 0.371161	val: 0.497352	test: 0.506020

Epoch: 106
Loss: 0.2692812445263068
RMSE train: 0.480407	val: 0.700023	test: 0.663166
MAE train: 0.378012	val: 0.511711	test: 0.508691

Epoch: 107
Loss: 0.2761864612499873
RMSE train: 0.476556	val: 0.704712	test: 0.665832
MAE train: 0.373455	val: 0.506777	test: 0.506520

Epoch: 108
Loss: 0.2682385245958964
RMSE train: 0.476351	val: 0.703139	test: 0.679236
MAE train: 0.371120	val: 0.505233	test: 0.513231

Epoch: 109
Loss: 0.27091972157359123
RMSE train: 0.468322	val: 0.690551	test: 0.658614
MAE train: 0.369246	val: 0.504612	test: 0.503044

Epoch: 110
Loss: 0.2742028074959914
RMSE train: 0.453902	val: 0.686546	test: 0.644122
MAE train: 0.356924	val: 0.498456	test: 0.492987

Epoch: 111
Loss: 0.25915976613759995
RMSE train: 0.471262	val: 0.711898	test: 0.673504
MAE train: 0.369285	val: 0.512221	test: 0.510443

Epoch: 112
Loss: 0.2708655136326949
RMSE train: 0.463605	val: 0.695321	test: 0.659955
MAE train: 0.363386	val: 0.506091	test: 0.497644

Epoch: 113
Loss: 0.27010958393414813
RMSE train: 0.468993	val: 0.708124	test: 0.674803
MAE train: 0.368375	val: 0.518687	test: 0.514891

Epoch: 114
Loss: 0.2676274813711643
RMSE train: 0.466106	val: 0.689695	test: 0.663861
MAE train: 0.367439	val: 0.499631	test: 0.503582

Epoch: 115
Loss: 0.26980480303366977
RMSE train: 0.495136	val: 0.720261	test: 0.685258
MAE train: 0.393197	val: 0.538179	test: 0.530662

Epoch: 116
Loss: 0.26900341734290123
RMSE train: 0.474745	val: 0.701257	test: 0.671653
MAE train: 0.373570	val: 0.508622	test: 0.510367

Epoch: 117
Loss: 0.2640773418049018
RMSE train: 0.472841	val: 0.701439	test: 0.672477
MAE train: 0.372983	val: 0.514157	test: 0.514922

Epoch: 118
Loss: 0.25702959299087524
RMSE train: 0.461626	val: 0.683844	test: 0.667501
MAE train: 0.362543	val: 0.493407	test: 0.505651

Epoch: 119
Loss: 0.25973570098479587
RMSE train: 0.449097	val: 0.698479	test: 0.669143
MAE train: 0.351811	val: 0.503642	test: 0.502988

Epoch: 120
Loss: 0.2663472356895606
RMSE train: 0.446618	val: 0.680529	test: 0.667699
MAE train: 0.350812	val: 0.493698	test: 0.504552

Epoch: 121
Loss: 0.25133179873228073
RMSE train: 0.455851	val: 0.692097	test: 0.670721
MAE train: 0.360498	val: 0.500304	test: 0.512112

Epoch: 122
Loss: 0.2610290100177129
RMSE train: 0.442719	val: 0.685890	test: 0.664227
MAE train: 0.347831	val: 0.501159	test: 0.505473

Epoch: 123
Loss: 0.2534468322992325
RMSE train: 0.454822	val: 0.677907	test: 0.660190
MAE train: 0.358608	val: 0.493891	test: 0.505255

Epoch: 124
Loss: 0.2476570966343085
RMSE train: 0.454970	val: 0.697503	test: 0.676003
MAE train: 0.358465	val: 0.514150	test: 0.517493

Epoch: 125
Loss: 0.24577425047755241
RMSE train: 0.445812	val: 0.678793	test: 0.656434
MAE train: 0.349361	val: 0.490868	test: 0.497771

Epoch: 126
Loss: 0.26273705437779427
RMSE train: 0.437653	val: 0.668605	test: 0.659090
MAE train: 0.339466	val: 0.488035	test: 0.495917

Epoch: 127
Loss: 0.25288844108581543
RMSE train: 0.458229	val: 0.688885	test: 0.677215
MAE train: 0.358513	val: 0.506283	test: 0.518826

Epoch: 128
Loss: 0.2472259203592936
RMSE train: 0.439686	val: 0.686131	test: 0.656131
MAE train: 0.348313	val: 0.497152	test: 0.503850

Epoch: 129
Loss: 0.24760140726963678
RMSE train: 0.464536	val: 0.701416	test: 0.673982
MAE train: 0.368399	val: 0.517471	test: 0.521903

Epoch: 130
Loss: 0.24268439908822378
RMSE train: 0.459871	val: 0.710678	test: 0.668900
MAE train: 0.359454	val: 0.520765	test: 0.509978

Epoch: 131
Loss: 0.2436196357011795
RMSE train: 0.462565	val: 0.709752	test: 0.678731
MAE train: 0.361465	val: 0.512480	test: 0.513310

Epoch: 132
Loss: 0.2399944228430589
RMSE train: 0.451121	val: 0.700994	test: 0.662512
MAE train: 0.355996	val: 0.506075	test: 0.506583

Epoch: 133
Loss: 0.25418805703520775
RMSE train: 0.433716	val: 0.684345	test: 0.659632
MAE train: 0.339113	val: 0.494261	test: 0.500674

Epoch: 134
Loss: 0.23982704058289528
RMSE train: 0.462258	val: 0.692294	test: 0.657362
MAE train: 0.363415	val: 0.506173	test: 0.507057

Epoch: 135
Loss: 0.2467195838689804
RMSE train: 0.455866	val: 0.704960	test: 0.663971
MAE train: 0.358908	val: 0.504314	test: 0.510008

Epoch: 136
Loss: 0.23721159373720488
RMSE train: 0.437390	val: 0.696151	test: 0.657669
MAE train: 0.340280	val: 0.498563	test: 0.498832

Epoch: 137
Loss: 0.23213206231594086
RMSE train: 0.430115	val: 0.692062	test: 0.654730
MAE train: 0.334950	val: 0.495507	test: 0.497253

Epoch: 138
Loss: 0.2384793944656849
RMSE train: 0.437060	val: 0.676154	test: 0.654672
MAE train: 0.342207	val: 0.487581	test: 0.495478

Epoch: 139
Loss: 0.2351744311551253
RMSE train: 0.436609	val: 0.694415	test: 0.659319
MAE train: 0.341295	val: 0.494850	test: 0.499598

Epoch: 140
Loss: 0.23652214805285135
RMSE train: 0.433343	val: 0.693679	test: 0.663244
MAE train: 0.338631	val: 0.495141	test: 0.505618

Epoch: 141
Loss: 0.22559740270177522
RMSE train: 0.428301	val: 0.691930	test: 0.654097
MAE train: 0.335255	val: 0.493093	test: 0.496592

Epoch: 142
Loss: 0.23466615627209345
RMSE train: 0.433112	val: 0.690751	test: 0.674469
MAE train: 0.338889	val: 0.500276	test: 0.510887

Epoch: 143
Loss: 0.2361909138659636
RMSE train: 0.441098	val: 0.698489	test: 0.678053
MAE train: 0.346427	val: 0.506460	test: 0.515491
MAE train: 0.395616	val: 0.503028	test: 0.517255

Epoch: 84
Loss: 0.3098107998569806
RMSE train: 0.510994	val: 0.698739	test: 0.685046
MAE train: 0.402459	val: 0.514917	test: 0.527268

Epoch: 85
Loss: 0.31372474879026413
RMSE train: 0.520123	val: 0.718579	test: 0.692979
MAE train: 0.409024	val: 0.528476	test: 0.536229

Epoch: 86
Loss: 0.313909371693929
RMSE train: 0.512614	val: 0.699683	test: 0.683607
MAE train: 0.404721	val: 0.523384	test: 0.524436

Epoch: 87
Loss: 0.31179030736287433
RMSE train: 0.531647	val: 0.714462	test: 0.683127
MAE train: 0.416507	val: 0.515109	test: 0.527104

Epoch: 88
Loss: 0.30968280633290607
RMSE train: 0.497682	val: 0.707226	test: 0.693375
MAE train: 0.389205	val: 0.517026	test: 0.520629

Epoch: 89
Loss: 0.30768680075804394
RMSE train: 0.524415	val: 0.722881	test: 0.696429
MAE train: 0.414624	val: 0.536640	test: 0.540402

Epoch: 90
Loss: 0.3046141987045606
RMSE train: 0.519445	val: 0.705976	test: 0.696008
MAE train: 0.406681	val: 0.516502	test: 0.531712

Epoch: 91
Loss: 0.31325919677813846
RMSE train: 0.500576	val: 0.698649	test: 0.673923
MAE train: 0.393545	val: 0.515156	test: 0.520422

Epoch: 92
Loss: 0.3047959754864375
RMSE train: 0.516733	val: 0.696309	test: 0.694622
MAE train: 0.403509	val: 0.510645	test: 0.533240

Epoch: 93
Loss: 0.30786119153102237
RMSE train: 0.500873	val: 0.693969	test: 0.689505
MAE train: 0.394498	val: 0.515265	test: 0.527812

Epoch: 94
Loss: 0.3024785965681076
RMSE train: 0.498543	val: 0.685572	test: 0.684784
MAE train: 0.393504	val: 0.504947	test: 0.531408

Epoch: 95
Loss: 0.2931450369457404
RMSE train: 0.490672	val: 0.694529	test: 0.676411
MAE train: 0.386228	val: 0.503325	test: 0.520640

Epoch: 96
Loss: 0.30504631871978444
RMSE train: 0.482450	val: 0.686415	test: 0.676802
MAE train: 0.378278	val: 0.503643	test: 0.514373

Epoch: 97
Loss: 0.2885957919061184
RMSE train: 0.498475	val: 0.692257	test: 0.680728
MAE train: 0.390877	val: 0.505064	test: 0.522771

Epoch: 98
Loss: 0.2862382307648659
RMSE train: 0.499820	val: 0.686498	test: 0.683632
MAE train: 0.393260	val: 0.493524	test: 0.526348

Epoch: 99
Loss: 0.28492730607589084
RMSE train: 0.497340	val: 0.691399	test: 0.684472
MAE train: 0.391531	val: 0.510115	test: 0.518986

Epoch: 100
Loss: 0.275882251560688
RMSE train: 0.472775	val: 0.689612	test: 0.672321
MAE train: 0.368421	val: 0.505542	test: 0.512616

Epoch: 101
Loss: 0.2802687721947829
RMSE train: 0.484640	val: 0.686850	test: 0.662791
MAE train: 0.381171	val: 0.498156	test: 0.513446

Epoch: 102
Loss: 0.29403969645500183
RMSE train: 0.517200	val: 0.707305	test: 0.712876
MAE train: 0.407386	val: 0.522892	test: 0.548005

Epoch: 103
Loss: 0.2972774803638458
RMSE train: 0.479127	val: 0.692671	test: 0.689612
MAE train: 0.374951	val: 0.504860	test: 0.520083

Epoch: 104
Loss: 0.2785343440870444
RMSE train: 0.514879	val: 0.696889	test: 0.690814
MAE train: 0.404350	val: 0.507288	test: 0.526317

Epoch: 105
Loss: 0.29287509992718697
RMSE train: 0.466402	val: 0.693852	test: 0.670469
MAE train: 0.364891	val: 0.513859	test: 0.514378

Epoch: 106
Loss: 0.2704436456163724
RMSE train: 0.500642	val: 0.695729	test: 0.691123
MAE train: 0.397197	val: 0.513593	test: 0.530375

Epoch: 107
Loss: 0.2757326550781727
RMSE train: 0.471386	val: 0.687415	test: 0.666112
MAE train: 0.369779	val: 0.496709	test: 0.510031

Epoch: 108
Loss: 0.26986800010005635
RMSE train: 0.476169	val: 0.689903	test: 0.685438
MAE train: 0.375000	val: 0.506923	test: 0.525470

Epoch: 109
Loss: 0.2656281900902589
RMSE train: 0.468085	val: 0.674781	test: 0.672554
MAE train: 0.369382	val: 0.486559	test: 0.517132

Epoch: 110
Loss: 0.26069246232509613
RMSE train: 0.476286	val: 0.697696	test: 0.670855
MAE train: 0.373696	val: 0.514491	test: 0.516588

Epoch: 111
Loss: 0.2672797491153081
RMSE train: 0.464126	val: 0.679804	test: 0.670735
MAE train: 0.365686	val: 0.501940	test: 0.512871

Epoch: 112
Loss: 0.2728956217567126
RMSE train: 0.493869	val: 0.701772	test: 0.681511
MAE train: 0.388856	val: 0.515980	test: 0.518152

Epoch: 113
Loss: 0.25345464795827866
RMSE train: 0.461260	val: 0.687357	test: 0.670744
MAE train: 0.362692	val: 0.506563	test: 0.514910

Epoch: 114
Loss: 0.26271580159664154
RMSE train: 0.453973	val: 0.678845	test: 0.655640
MAE train: 0.355586	val: 0.494206	test: 0.510029

Epoch: 115
Loss: 0.260182399302721
RMSE train: 0.472539	val: 0.684537	test: 0.666104
MAE train: 0.371414	val: 0.498147	test: 0.511740

Epoch: 116
Loss: 0.2668379781146844
RMSE train: 0.468783	val: 0.684252	test: 0.666001
MAE train: 0.368734	val: 0.504894	test: 0.512417

Epoch: 117
Loss: 0.2636512170235316
RMSE train: 0.473996	val: 0.686778	test: 0.669129
MAE train: 0.374297	val: 0.504885	test: 0.512755

Epoch: 118
Loss: 0.2650940182308356
RMSE train: 0.461436	val: 0.678976	test: 0.669460
MAE train: 0.364269	val: 0.499873	test: 0.519422

Epoch: 119
Loss: 0.24996707340081534
RMSE train: 0.469008	val: 0.677340	test: 0.669546
MAE train: 0.366947	val: 0.486132	test: 0.512219

Epoch: 120
Loss: 0.2471980651219686
RMSE train: 0.452998	val: 0.687083	test: 0.665114
MAE train: 0.354122	val: 0.494192	test: 0.512129

Epoch: 121
Loss: 0.25121305634578067
RMSE train: 0.453309	val: 0.673502	test: 0.662540
MAE train: 0.358206	val: 0.495161	test: 0.510601

Epoch: 122
Loss: 0.24367877965172133
RMSE train: 0.441569	val: 0.685796	test: 0.667188
MAE train: 0.345816	val: 0.498522	test: 0.508045

Epoch: 123
Loss: 0.2485761654873689
RMSE train: 0.457530	val: 0.681598	test: 0.668421
MAE train: 0.360353	val: 0.503943	test: 0.515473

Epoch: 124
Loss: 0.23368298510710397
RMSE train: 0.456225	val: 0.689287	test: 0.681204
MAE train: 0.359717	val: 0.508740	test: 0.526262

Epoch: 125
Loss: 0.2503717839717865
RMSE train: 0.434655	val: 0.677900	test: 0.664609
MAE train: 0.340103	val: 0.488205	test: 0.507757

Epoch: 126
Loss: 0.25425710156559944
RMSE train: 0.469207	val: 0.700628	test: 0.671904
MAE train: 0.367579	val: 0.503536	test: 0.517349

Epoch: 127
Loss: 0.25214431807398796
RMSE train: 0.451240	val: 0.676569	test: 0.668130
MAE train: 0.356527	val: 0.496894	test: 0.514528

Epoch: 128
Loss: 0.25012238323688507
RMSE train: 0.440973	val: 0.676506	test: 0.654918
MAE train: 0.346138	val: 0.493378	test: 0.505806

Epoch: 129
Loss: 0.2377524438003699
RMSE train: 0.450024	val: 0.682925	test: 0.665282
MAE train: 0.353992	val: 0.491142	test: 0.508744

Epoch: 130
Loss: 0.24009999136130014
RMSE train: 0.448979	val: 0.691416	test: 0.676546
MAE train: 0.351418	val: 0.504114	test: 0.517478

Epoch: 131
Loss: 0.24767140299081802
RMSE train: 0.431725	val: 0.685408	test: 0.660259
MAE train: 0.338351	val: 0.495084	test: 0.502311

Epoch: 132
Loss: 0.23205178355177244
RMSE train: 0.444962	val: 0.681765	test: 0.662055
MAE train: 0.350319	val: 0.493136	test: 0.509402

Epoch: 133
Loss: 0.2388927328089873
RMSE train: 0.447793	val: 0.684916	test: 0.671727
MAE train: 0.351494	val: 0.488107	test: 0.515335

Epoch: 134
Loss: 0.24202094102899233
RMSE train: 0.440928	val: 0.674926	test: 0.665579
MAE train: 0.346922	val: 0.483691	test: 0.513566

Epoch: 135
Loss: 0.2387218934794267
RMSE train: 0.445779	val: 0.675160	test: 0.661480
MAE train: 0.349821	val: 0.488199	test: 0.508749

Epoch: 136
Loss: 0.2402421422302723
RMSE train: 0.454866	val: 0.691259	test: 0.665945
MAE train: 0.358026	val: 0.496373	test: 0.509416

Epoch: 137
Loss: 0.2437725489338239
RMSE train: 0.449430	val: 0.691786	test: 0.669736
MAE train: 0.350924	val: 0.504360	test: 0.515563

Epoch: 138
Loss: 0.23660813396175703
RMSE train: 0.432672	val: 0.685709	test: 0.649638
MAE train: 0.340963	val: 0.486602	test: 0.495625

Epoch: 139
Loss: 0.23966962471604347
RMSE train: 0.439291	val: 0.694154	test: 0.671706
MAE train: 0.344552	val: 0.499294	test: 0.518578

Epoch: 140
Loss: 0.225576713681221
RMSE train: 0.435503	val: 0.686096	test: 0.657001
MAE train: 0.342656	val: 0.488126	test: 0.502957

Epoch: 141
Loss: 0.2307645926872889
RMSE train: 0.431651	val: 0.697349	test: 0.661227
MAE train: 0.338162	val: 0.502769	test: 0.506977

Epoch: 142
Loss: 0.22981036081910133
RMSE train: 0.460048	val: 0.708089	test: 0.687384
MAE train: 0.361271	val: 0.508412	test: 0.524000

Epoch: 143
Loss: 0.23405392716328302
RMSE train: 0.437934	val: 0.692317	test: 0.660720
MAE train: 0.343706	val: 0.495543	test: 0.506679
MAE train: 0.415683	val: 0.531710	test: 0.528643

Epoch: 84
Loss: 0.31126853575309116
RMSE train: 0.527619	val: 0.715235	test: 0.679429
MAE train: 0.414905	val: 0.525134	test: 0.520861

Epoch: 85
Loss: 0.3272736296057701
RMSE train: 0.536705	val: 0.713913	test: 0.694976
MAE train: 0.422480	val: 0.524150	test: 0.534614

Epoch: 86
Loss: 0.3152591461936633
RMSE train: 0.513849	val: 0.724482	test: 0.680814
MAE train: 0.402046	val: 0.524266	test: 0.518763

Epoch: 87
Loss: 0.3210109795133273
RMSE train: 0.509671	val: 0.713934	test: 0.674522
MAE train: 0.400891	val: 0.524948	test: 0.516069

Epoch: 88
Loss: 0.32487452526887256
RMSE train: 0.526540	val: 0.726627	test: 0.691166
MAE train: 0.413948	val: 0.526488	test: 0.533378

Epoch: 89
Loss: 0.324548843006293
RMSE train: 0.512565	val: 0.725264	test: 0.681484
MAE train: 0.401938	val: 0.516007	test: 0.523221

Epoch: 90
Loss: 0.32029422620932263
RMSE train: 0.548598	val: 0.738752	test: 0.703657
MAE train: 0.429716	val: 0.535384	test: 0.540131

Epoch: 91
Loss: 0.3115980500976245
RMSE train: 0.498747	val: 0.716672	test: 0.679686
MAE train: 0.392273	val: 0.518766	test: 0.519125

Epoch: 92
Loss: 0.3071550751725833
RMSE train: 0.505502	val: 0.715256	test: 0.675838
MAE train: 0.397166	val: 0.521007	test: 0.520948

Epoch: 93
Loss: 0.2983255634705226
RMSE train: 0.520144	val: 0.722889	test: 0.686322
MAE train: 0.407777	val: 0.520264	test: 0.524064

Epoch: 94
Loss: 0.2993001714348793
RMSE train: 0.500130	val: 0.729438	test: 0.680634
MAE train: 0.393750	val: 0.519022	test: 0.521523

Epoch: 95
Loss: 0.2920203705628713
RMSE train: 0.511529	val: 0.726482	test: 0.688282
MAE train: 0.401849	val: 0.519913	test: 0.527024

Epoch: 96
Loss: 0.29737138499816257
RMSE train: 0.516074	val: 0.736075	test: 0.695564
MAE train: 0.403407	val: 0.533149	test: 0.529775

Epoch: 97
Loss: 0.30431338275472325
RMSE train: 0.520034	val: 0.743986	test: 0.692124
MAE train: 0.407275	val: 0.538326	test: 0.526819

Epoch: 98
Loss: 0.29492533579468727
RMSE train: 0.509237	val: 0.716742	test: 0.678033
MAE train: 0.399831	val: 0.518338	test: 0.526447

Epoch: 99
Loss: 0.29477964465816814
RMSE train: 0.501372	val: 0.711189	test: 0.680001
MAE train: 0.394432	val: 0.515624	test: 0.522823

Epoch: 100
Loss: 0.28819840028882027
RMSE train: 0.523774	val: 0.744279	test: 0.696872
MAE train: 0.413221	val: 0.541048	test: 0.541190

Epoch: 101
Loss: 0.2875176469484965
RMSE train: 0.492047	val: 0.711278	test: 0.671153
MAE train: 0.386157	val: 0.511291	test: 0.511996

Epoch: 102
Loss: 0.2873271467785041
RMSE train: 0.500894	val: 0.722409	test: 0.683226
MAE train: 0.394996	val: 0.521676	test: 0.528122

Epoch: 103
Loss: 0.2850581593811512
RMSE train: 0.487461	val: 0.719331	test: 0.677325
MAE train: 0.381420	val: 0.516125	test: 0.519741

Epoch: 104
Loss: 0.2857622268299262
RMSE train: 0.536088	val: 0.735466	test: 0.709486
MAE train: 0.421489	val: 0.534732	test: 0.544264

Epoch: 105
Loss: 0.27778638899326324
RMSE train: 0.487451	val: 0.721061	test: 0.676488
MAE train: 0.382654	val: 0.514420	test: 0.516619

Epoch: 106
Loss: 0.26991796121001244
RMSE train: 0.497633	val: 0.719758	test: 0.680869
MAE train: 0.393906	val: 0.525234	test: 0.523032

Epoch: 107
Loss: 0.2741395483414332
RMSE train: 0.478081	val: 0.718895	test: 0.680981
MAE train: 0.373832	val: 0.515372	test: 0.516152

Epoch: 108
Loss: 0.28048326075077057
RMSE train: 0.519570	val: 0.723825	test: 0.698935
MAE train: 0.409093	val: 0.523594	test: 0.535247

Epoch: 109
Loss: 0.28186282763878506
RMSE train: 0.501432	val: 0.712871	test: 0.681361
MAE train: 0.392625	val: 0.513267	test: 0.520571

Epoch: 110
Loss: 0.27666578938563663
RMSE train: 0.481743	val: 0.707070	test: 0.680436
MAE train: 0.379042	val: 0.509319	test: 0.520255

Epoch: 111
Loss: 0.2645321575303872
RMSE train: 0.474924	val: 0.713307	test: 0.672273
MAE train: 0.372516	val: 0.511432	test: 0.514122

Epoch: 112
Loss: 0.27975674470265705
RMSE train: 0.474285	val: 0.704664	test: 0.674974
MAE train: 0.373259	val: 0.501892	test: 0.513355

Epoch: 113
Loss: 0.2632143708566825
RMSE train: 0.478523	val: 0.728684	test: 0.680589
MAE train: 0.373995	val: 0.517643	test: 0.518495

Epoch: 114
Loss: 0.2627629240353902
RMSE train: 0.490509	val: 0.721991	test: 0.678703
MAE train: 0.385878	val: 0.524249	test: 0.518214

Epoch: 115
Loss: 0.26810433094700176
RMSE train: 0.472235	val: 0.706132	test: 0.673659
MAE train: 0.370994	val: 0.503379	test: 0.515836

Epoch: 116
Loss: 0.2640937305986881
RMSE train: 0.467296	val: 0.708433	test: 0.670077
MAE train: 0.366982	val: 0.507873	test: 0.513997

Epoch: 117
Loss: 0.262457437813282
RMSE train: 0.455685	val: 0.703589	test: 0.658120
MAE train: 0.358552	val: 0.500608	test: 0.502323

Epoch: 118
Loss: 0.2623215975860755
RMSE train: 0.492066	val: 0.714305	test: 0.695135
MAE train: 0.383465	val: 0.517589	test: 0.527974

Epoch: 119
Loss: 0.2609279739360015
RMSE train: 0.465943	val: 0.713025	test: 0.670138
MAE train: 0.365299	val: 0.506315	test: 0.509103

Epoch: 120
Loss: 0.2525435611605644
RMSE train: 0.493664	val: 0.726544	test: 0.690568
MAE train: 0.386193	val: 0.518165	test: 0.527946

Epoch: 121
Loss: 0.25943688675761223
RMSE train: 0.480509	val: 0.712050	test: 0.679737
MAE train: 0.375659	val: 0.515798	test: 0.516542

Epoch: 122
Loss: 0.2596805803477764
RMSE train: 0.476281	val: 0.714169	test: 0.675952
MAE train: 0.375869	val: 0.511355	test: 0.519187

Epoch: 123
Loss: 0.2535576621691386
RMSE train: 0.461501	val: 0.708721	test: 0.666403
MAE train: 0.362475	val: 0.510082	test: 0.509001

Epoch: 124
Loss: 0.2589082717895508
RMSE train: 0.444820	val: 0.700159	test: 0.670083
MAE train: 0.347786	val: 0.504315	test: 0.506835

Epoch: 125
Loss: 0.24662199119726816
RMSE train: 0.473299	val: 0.713918	test: 0.669439
MAE train: 0.370866	val: 0.505462	test: 0.508629

Epoch: 126
Loss: 0.2519627201060454
RMSE train: 0.472108	val: 0.722156	test: 0.682368
MAE train: 0.369102	val: 0.512659	test: 0.519211

Epoch: 127
Loss: 0.2491053268313408
RMSE train: 0.472893	val: 0.714423	test: 0.685415
MAE train: 0.370998	val: 0.514241	test: 0.523050

Epoch: 128
Loss: 0.2431086984773477
RMSE train: 0.454667	val: 0.710213	test: 0.672750
MAE train: 0.356639	val: 0.501224	test: 0.513074

Epoch: 129
Loss: 0.24287782609462738
RMSE train: 0.444929	val: 0.707500	test: 0.665023
MAE train: 0.347151	val: 0.492394	test: 0.504105

Epoch: 130
Loss: 0.24684510131676993
RMSE train: 0.444381	val: 0.718474	test: 0.654323
MAE train: 0.348189	val: 0.502055	test: 0.495007

Epoch: 131
Loss: 0.24215689425667128
RMSE train: 0.453260	val: 0.715827	test: 0.674995
MAE train: 0.354420	val: 0.510954	test: 0.511563

Epoch: 132
Loss: 0.24615338320533434
RMSE train: 0.479322	val: 0.720600	test: 0.684251
MAE train: 0.376808	val: 0.516225	test: 0.523562

Epoch: 133
Loss: 0.2491837739944458
RMSE train: 0.459257	val: 0.727278	test: 0.661035
MAE train: 0.359378	val: 0.513146	test: 0.502599

Epoch: 134
Loss: 0.25646231571833294
RMSE train: 0.456331	val: 0.723299	test: 0.674501
MAE train: 0.357147	val: 0.518988	test: 0.517406

Epoch: 135
Loss: 0.24380532652139664
RMSE train: 0.455445	val: 0.731708	test: 0.675647
MAE train: 0.356508	val: 0.522009	test: 0.509782

Epoch: 136
Loss: 0.2522045349081357
RMSE train: 0.453478	val: 0.725963	test: 0.680601
MAE train: 0.353079	val: 0.521328	test: 0.519791

Epoch: 137
Loss: 0.24514527370532355
RMSE train: 0.439231	val: 0.697197	test: 0.655113
MAE train: 0.344687	val: 0.499985	test: 0.498143

Epoch: 138
Loss: 0.23369077717264494
RMSE train: 0.453037	val: 0.717293	test: 0.660667
MAE train: 0.356498	val: 0.507687	test: 0.502987

Epoch: 139
Loss: 0.2298099361360073
RMSE train: 0.432348	val: 0.710153	test: 0.665071
MAE train: 0.339701	val: 0.507983	test: 0.501835

Epoch: 140
Loss: 0.24386711666981378
RMSE train: 0.453344	val: 0.716274	test: 0.673055
MAE train: 0.355905	val: 0.513529	test: 0.518003

Epoch: 141
Loss: 0.24157447243730226
RMSE train: 0.448516	val: 0.705279	test: 0.660193
MAE train: 0.352594	val: 0.509783	test: 0.509367

Epoch: 142
Loss: 0.2359069101512432
RMSE train: 0.459777	val: 0.716529	test: 0.680230
MAE train: 0.358149	val: 0.516477	test: 0.517747

Epoch: 143
Loss: 0.2298511415719986
RMSE train: 0.446974	val: 0.713471	test: 0.660820
MAE train: 0.351560	val: 0.504986	test: 0.505636
MAE train: 0.447035	val: 0.584519	test: 0.531009

Epoch: 84
Loss: 0.3236042133399418
RMSE train: 0.529154	val: 0.760508	test: 0.654947
MAE train: 0.411362	val: 0.558005	test: 0.488969

Epoch: 85
Loss: 0.32943206812654224
RMSE train: 0.525898	val: 0.756198	test: 0.661933
MAE train: 0.407098	val: 0.558857	test: 0.491676

Epoch: 86
Loss: 0.32250453318868366
RMSE train: 0.561227	val: 0.774929	test: 0.700746
MAE train: 0.437213	val: 0.581250	test: 0.534011

Epoch: 87
Loss: 0.34097859263420105
RMSE train: 0.523726	val: 0.748770	test: 0.653777
MAE train: 0.405912	val: 0.561722	test: 0.493920

Epoch: 88
Loss: 0.332929042833192
RMSE train: 0.550064	val: 0.751761	test: 0.687772
MAE train: 0.429275	val: 0.565059	test: 0.522297

Epoch: 89
Loss: 0.3210651023047311
RMSE train: 0.565590	val: 0.769279	test: 0.699030
MAE train: 0.442437	val: 0.581242	test: 0.528440

Epoch: 90
Loss: 0.3341521514313562
RMSE train: 0.543183	val: 0.767964	test: 0.677093
MAE train: 0.424268	val: 0.569517	test: 0.507645

Epoch: 91
Loss: 0.3353785446711949
RMSE train: 0.521064	val: 0.745605	test: 0.647179
MAE train: 0.405724	val: 0.555589	test: 0.483510

Epoch: 92
Loss: 0.334280897464071
RMSE train: 0.531591	val: 0.744217	test: 0.674746
MAE train: 0.414325	val: 0.553860	test: 0.503510

Epoch: 93
Loss: 0.33046006304877146
RMSE train: 0.531684	val: 0.756233	test: 0.663871
MAE train: 0.413672	val: 0.569723	test: 0.502224

Epoch: 94
Loss: 0.3174337404114859
RMSE train: 0.546013	val: 0.770459	test: 0.677186
MAE train: 0.423205	val: 0.570010	test: 0.512786

Epoch: 95
Loss: 0.320756482226508
RMSE train: 0.509550	val: 0.743445	test: 0.653297
MAE train: 0.394657	val: 0.555584	test: 0.490783

Epoch: 96
Loss: 0.3387384084718568
RMSE train: 0.506481	val: 0.742084	test: 0.652440
MAE train: 0.393390	val: 0.553195	test: 0.492769

Epoch: 97
Loss: 0.30353096766131266
RMSE train: 0.524345	val: 0.756269	test: 0.674175
MAE train: 0.407045	val: 0.561403	test: 0.506691

Epoch: 98
Loss: 0.33763575341020313
RMSE train: 0.534394	val: 0.764839	test: 0.669882
MAE train: 0.417543	val: 0.570436	test: 0.505091

Epoch: 99
Loss: 0.3241920535053526
RMSE train: 0.507177	val: 0.744565	test: 0.662907
MAE train: 0.394343	val: 0.550964	test: 0.499081

Epoch: 100
Loss: 0.3436424051012312
RMSE train: 0.535328	val: 0.754425	test: 0.667902
MAE train: 0.416266	val: 0.564592	test: 0.503885

Epoch: 101
Loss: 0.33522634846823557
RMSE train: 0.510428	val: 0.752571	test: 0.674051
MAE train: 0.395732	val: 0.558059	test: 0.500961

Epoch: 102
Loss: 0.31259716195719583
RMSE train: 0.538884	val: 0.761036	test: 0.684488
MAE train: 0.421379	val: 0.569187	test: 0.516245

Epoch: 103
Loss: 0.29329439039741245
RMSE train: 0.508488	val: 0.734082	test: 0.653070
MAE train: 0.395081	val: 0.547897	test: 0.488078

Epoch: 104
Loss: 0.31534118950366974
RMSE train: 0.502054	val: 0.728585	test: 0.652254
MAE train: 0.391375	val: 0.545747	test: 0.484872

Epoch: 105
Loss: 0.31594338161604746
RMSE train: 0.498414	val: 0.742585	test: 0.643228
MAE train: 0.387224	val: 0.555436	test: 0.485192

Epoch: 106
Loss: 0.31621706060000826
RMSE train: 0.528670	val: 0.756063	test: 0.691079
MAE train: 0.409866	val: 0.568499	test: 0.508482

Epoch: 107
Loss: 0.31037588736840654
RMSE train: 0.497354	val: 0.732678	test: 0.644575
MAE train: 0.385752	val: 0.546497	test: 0.473163

Epoch: 108
Loss: 0.3193396819489343
RMSE train: 0.509802	val: 0.752886	test: 0.656380
MAE train: 0.398821	val: 0.557478	test: 0.494553

Epoch: 109
Loss: 0.3057888075709343
RMSE train: 0.517509	val: 0.762431	test: 0.673320
MAE train: 0.402725	val: 0.562561	test: 0.503132

Epoch: 110
Loss: 0.3079348142657961
RMSE train: 0.499378	val: 0.745721	test: 0.650751
MAE train: 0.389248	val: 0.549064	test: 0.480983

Epoch: 111
Loss: 0.28715615080935614
RMSE train: 0.494719	val: 0.744552	test: 0.645479
MAE train: 0.384732	val: 0.556581	test: 0.485714

Epoch: 112
Loss: 0.3032149544784001
RMSE train: 0.570553	val: 0.784905	test: 0.734128
MAE train: 0.451172	val: 0.584603	test: 0.558369

Epoch: 113
Loss: 0.30719919396298273
RMSE train: 0.491694	val: 0.735260	test: 0.644428
MAE train: 0.383528	val: 0.544044	test: 0.483847

Epoch: 114
Loss: 0.2932582955275263
RMSE train: 0.519838	val: 0.753284	test: 0.666983
MAE train: 0.406119	val: 0.555522	test: 0.500379

Epoch: 115
Loss: 0.29246185294219423
RMSE train: 0.493127	val: 0.747854	test: 0.647626
MAE train: 0.383578	val: 0.556205	test: 0.479772

Epoch: 116
Loss: 0.29194783738681246
RMSE train: 0.493622	val: 0.747709	test: 0.651366
MAE train: 0.387188	val: 0.557667	test: 0.489123

Epoch: 117
Loss: 0.2848532370158604
RMSE train: 0.491392	val: 0.741642	test: 0.641770
MAE train: 0.383614	val: 0.552002	test: 0.476633

Epoch: 118
Loss: 0.29438348753111704
RMSE train: 0.502662	val: 0.759253	test: 0.640901
MAE train: 0.390482	val: 0.558271	test: 0.477667

Epoch: 119
Loss: 0.2879938387445041
RMSE train: 0.520994	val: 0.755064	test: 0.685032
MAE train: 0.407058	val: 0.565753	test: 0.509949

Epoch: 120
Loss: 0.29820986092090607
RMSE train: 0.473324	val: 0.724053	test: 0.631839
MAE train: 0.368817	val: 0.538318	test: 0.467633

Epoch: 121
Loss: 0.28547977975436617
RMSE train: 0.482573	val: 0.749980	test: 0.658222
MAE train: 0.373790	val: 0.552910	test: 0.486084

Epoch: 122
Loss: 0.29032125430447714
RMSE train: 0.518304	val: 0.738053	test: 0.668504
MAE train: 0.405195	val: 0.555331	test: 0.498033

Epoch: 123
Loss: 0.27397524459021433
RMSE train: 0.471786	val: 0.719201	test: 0.634162
MAE train: 0.365925	val: 0.536933	test: 0.466500

Epoch: 124
Loss: 0.29261596500873566
RMSE train: 0.512944	val: 0.748434	test: 0.655215
MAE train: 0.399561	val: 0.560774	test: 0.485176

Epoch: 125
Loss: 0.29727126338652204
RMSE train: 0.495975	val: 0.735296	test: 0.639594
MAE train: 0.387792	val: 0.553144	test: 0.475984

Epoch: 126
Loss: 0.28564453125
RMSE train: 0.515358	val: 0.744831	test: 0.654965
MAE train: 0.401559	val: 0.560405	test: 0.491958

Epoch: 127
Loss: 0.29198612911360605
RMSE train: 0.491832	val: 0.737825	test: 0.643226
MAE train: 0.382848	val: 0.551485	test: 0.480745

Epoch: 128
Loss: 0.3080124727317265
RMSE train: 0.487652	val: 0.708557	test: 0.628410
MAE train: 0.381723	val: 0.533249	test: 0.464629

Epoch: 129
Loss: 0.2791378838675363
RMSE train: 0.486676	val: 0.723268	test: 0.637399
MAE train: 0.379585	val: 0.548636	test: 0.479858

Epoch: 130
Loss: 0.28699468608413425
RMSE train: 0.490316	val: 0.717099	test: 0.650821
MAE train: 0.380515	val: 0.542663	test: 0.485874

Epoch: 131
Loss: 0.26909840532711576
RMSE train: 0.478540	val: 0.735432	test: 0.616901
MAE train: 0.370840	val: 0.543331	test: 0.459779

Epoch: 132
Loss: 0.29249209591320585
RMSE train: 0.482924	val: 0.716917	test: 0.628611
MAE train: 0.378659	val: 0.541739	test: 0.474368

Epoch: 133
Loss: 0.2728377027170999
RMSE train: 0.465012	val: 0.703562	test: 0.623720
MAE train: 0.364590	val: 0.534466	test: 0.468704

Epoch: 134
Loss: 0.2972895024078233
RMSE train: 0.487986	val: 0.729609	test: 0.642991
MAE train: 0.381818	val: 0.556642	test: 0.480529

Epoch: 135
Loss: 0.2789307140878269
RMSE train: 0.454971	val: 0.745755	test: 0.631063
MAE train: 0.352046	val: 0.541074	test: 0.466251

Epoch: 136
Loss: 0.26661783882549833
RMSE train: 0.484315	val: 0.735148	test: 0.641227
MAE train: 0.376503	val: 0.549540	test: 0.484983

Epoch: 137
Loss: 0.2671702376433781
RMSE train: 0.471990	val: 0.738134	test: 0.632424
MAE train: 0.368994	val: 0.545178	test: 0.475435

Epoch: 138
Loss: 0.2549587530749185
RMSE train: 0.469093	val: 0.753663	test: 0.640824
MAE train: 0.366198	val: 0.553533	test: 0.474954

Epoch: 139
Loss: 0.26021569647959303
RMSE train: 0.460520	val: 0.733163	test: 0.631797
MAE train: 0.360317	val: 0.541473	test: 0.468918

Epoch: 140
Loss: 0.2669999258858817
RMSE train: 0.462655	val: 0.745178	test: 0.631420
MAE train: 0.360563	val: 0.550734	test: 0.465622

Epoch: 141
Loss: 0.2617956112538065
RMSE train: 0.479961	val: 0.754382	test: 0.645502
MAE train: 0.373684	val: 0.553695	test: 0.482074

Epoch: 142
Loss: 0.25475791309561047
RMSE train: 0.468782	val: 0.728417	test: 0.632251
MAE train: 0.366962	val: 0.545032	test: 0.475962

Epoch: 143
Loss: 0.25880507699080874
RMSE train: 0.469183	val: 0.723461	test: 0.646119
MAE train: 0.364302	val: 0.545245	test: 0.479996
MAE train: 0.418581	val: 0.557739	test: 0.499612

Epoch: 84
Loss: 0.3463400346892221
RMSE train: 0.531787	val: 0.735208	test: 0.664712
MAE train: 0.410650	val: 0.558290	test: 0.501799

Epoch: 85
Loss: 0.3371244583811079
RMSE train: 0.535701	val: 0.753923	test: 0.662340
MAE train: 0.412489	val: 0.554347	test: 0.490020

Epoch: 86
Loss: 0.3330900775534766
RMSE train: 0.573759	val: 0.771972	test: 0.693676
MAE train: 0.447256	val: 0.583624	test: 0.517513

Epoch: 87
Loss: 0.34874649133001057
RMSE train: 0.519725	val: 0.732552	test: 0.640035
MAE train: 0.402227	val: 0.542093	test: 0.481248

Epoch: 88
Loss: 0.3686151312930243
RMSE train: 0.542053	val: 0.741298	test: 0.671328
MAE train: 0.421782	val: 0.560379	test: 0.507078

Epoch: 89
Loss: 0.36654579639434814
RMSE train: 0.570697	val: 0.784412	test: 0.692658
MAE train: 0.443809	val: 0.588715	test: 0.511775

Epoch: 90
Loss: 0.32750202289649416
RMSE train: 0.552311	val: 0.778795	test: 0.667901
MAE train: 0.426214	val: 0.573485	test: 0.498231

Epoch: 91
Loss: 0.32663540116378237
RMSE train: 0.545855	val: 0.750948	test: 0.664185
MAE train: 0.428388	val: 0.562618	test: 0.508602

Epoch: 92
Loss: 0.32688479125499725
RMSE train: 0.527828	val: 0.750256	test: 0.668484
MAE train: 0.408820	val: 0.561224	test: 0.502182

Epoch: 93
Loss: 0.3375735070024218
RMSE train: 0.533884	val: 0.756546	test: 0.655931
MAE train: 0.412694	val: 0.556664	test: 0.492641

Epoch: 94
Loss: 0.3441140800714493
RMSE train: 0.569993	val: 0.788636	test: 0.717102
MAE train: 0.438421	val: 0.590371	test: 0.526070

Epoch: 95
Loss: 0.32925379489149365
RMSE train: 0.542908	val: 0.757583	test: 0.670456
MAE train: 0.422037	val: 0.559406	test: 0.493582

Epoch: 96
Loss: 0.3308377798114504
RMSE train: 0.531710	val: 0.753532	test: 0.674111
MAE train: 0.410711	val: 0.564516	test: 0.496936

Epoch: 97
Loss: 0.3205930697066443
RMSE train: 0.510762	val: 0.731688	test: 0.640126
MAE train: 0.397347	val: 0.542893	test: 0.478967

Epoch: 98
Loss: 0.29759397889886585
RMSE train: 0.521102	val: 0.741579	test: 0.658020
MAE train: 0.404407	val: 0.552374	test: 0.488685

Epoch: 99
Loss: 0.3197696826287678
RMSE train: 0.509663	val: 0.751371	test: 0.647682
MAE train: 0.393828	val: 0.550759	test: 0.477641

Epoch: 100
Loss: 0.31745015936238424
RMSE train: 0.511355	val: 0.737668	test: 0.648382
MAE train: 0.397619	val: 0.556794	test: 0.488753

Epoch: 101
Loss: 0.32980339229106903
RMSE train: 0.505849	val: 0.735789	test: 0.651827
MAE train: 0.391558	val: 0.550487	test: 0.485917

Epoch: 102
Loss: 0.3088148683309555
RMSE train: 0.512707	val: 0.742877	test: 0.666633
MAE train: 0.399039	val: 0.558027	test: 0.503022

Epoch: 103
Loss: 0.31582921956266674
RMSE train: 0.545368	val: 0.743871	test: 0.697008
MAE train: 0.422044	val: 0.560113	test: 0.515627

Epoch: 104
Loss: 0.30860918015241623
RMSE train: 0.511776	val: 0.729842	test: 0.657746
MAE train: 0.395255	val: 0.547308	test: 0.483745

Epoch: 105
Loss: 0.3145623632839748
RMSE train: 0.520920	val: 0.720932	test: 0.672590
MAE train: 0.405933	val: 0.550184	test: 0.502276

Epoch: 106
Loss: 0.31297558333192554
RMSE train: 0.514595	val: 0.738702	test: 0.672874
MAE train: 0.398011	val: 0.562873	test: 0.506275

Epoch: 107
Loss: 0.31656551148210255
RMSE train: 0.506182	val: 0.726255	test: 0.651881
MAE train: 0.395809	val: 0.543031	test: 0.486457

Epoch: 108
Loss: 0.31997865651335033
RMSE train: 0.507130	val: 0.736103	test: 0.654270
MAE train: 0.393337	val: 0.544395	test: 0.484773

Epoch: 109
Loss: 0.2950100653937885
RMSE train: 0.540003	val: 0.745999	test: 0.678457
MAE train: 0.420592	val: 0.555851	test: 0.502159

Epoch: 110
Loss: 0.31178069753306253
RMSE train: 0.500422	val: 0.724913	test: 0.653718
MAE train: 0.386397	val: 0.543481	test: 0.486196

Epoch: 111
Loss: 0.29608755665166037
RMSE train: 0.514285	val: 0.748766	test: 0.663188
MAE train: 0.398191	val: 0.558464	test: 0.494982

Epoch: 112
Loss: 0.29226109066179823
RMSE train: 0.485600	val: 0.739946	test: 0.621680
MAE train: 0.376181	val: 0.540951	test: 0.465153

Epoch: 113
Loss: 0.28804147030626026
RMSE train: 0.507795	val: 0.739379	test: 0.660337
MAE train: 0.396299	val: 0.546202	test: 0.488833

Epoch: 114
Loss: 0.29841047312532154
RMSE train: 0.514929	val: 0.717948	test: 0.656455
MAE train: 0.397372	val: 0.545813	test: 0.490427

Epoch: 115
Loss: 0.29999974370002747
RMSE train: 0.501375	val: 0.713940	test: 0.645052
MAE train: 0.389886	val: 0.535750	test: 0.477665

Epoch: 116
Loss: 0.3044893039124353
RMSE train: 0.491196	val: 0.728770	test: 0.639534
MAE train: 0.381639	val: 0.537768	test: 0.478677

Epoch: 117
Loss: 0.29864373803138733
RMSE train: 0.504241	val: 0.747958	test: 0.640073
MAE train: 0.392372	val: 0.551832	test: 0.479187

Epoch: 118
Loss: 0.2782930965934481
RMSE train: 0.487336	val: 0.724445	test: 0.630796
MAE train: 0.378193	val: 0.535921	test: 0.474541

Epoch: 119
Loss: 0.2733553020017488
RMSE train: 0.495387	val: 0.723798	test: 0.647727
MAE train: 0.383603	val: 0.541156	test: 0.484249

Epoch: 120
Loss: 0.2739051250474794
RMSE train: 0.475064	val: 0.712643	test: 0.627816
MAE train: 0.368503	val: 0.530579	test: 0.468498

Epoch: 121
Loss: 0.2680148790989603
RMSE train: 0.476711	val: 0.728361	test: 0.625732
MAE train: 0.370483	val: 0.541374	test: 0.471542

Epoch: 122
Loss: 0.26711736832346233
RMSE train: 0.509185	val: 0.749184	test: 0.662196
MAE train: 0.395284	val: 0.553498	test: 0.499638

Epoch: 123
Loss: 0.30736187526157926
RMSE train: 0.495395	val: 0.745326	test: 0.665709
MAE train: 0.386235	val: 0.558277	test: 0.507750

Epoch: 124
Loss: 0.2824494679059301
RMSE train: 0.491760	val: 0.727025	test: 0.629166
MAE train: 0.382093	val: 0.533738	test: 0.472527

Epoch: 125
Loss: 0.2811027563044003
RMSE train: 0.464055	val: 0.706707	test: 0.622379
MAE train: 0.359556	val: 0.524655	test: 0.462008

Epoch: 126
Loss: 0.26840145140886307
RMSE train: 0.505197	val: 0.735037	test: 0.672152
MAE train: 0.391119	val: 0.556898	test: 0.503317

Epoch: 127
Loss: 0.29952803573438097
RMSE train: 0.498212	val: 0.732465	test: 0.647596
MAE train: 0.387360	val: 0.547530	test: 0.485524

Epoch: 128
Loss: 0.2709462344646454
RMSE train: 0.473982	val: 0.719826	test: 0.632303
MAE train: 0.364912	val: 0.533766	test: 0.469417

Epoch: 129
Loss: 0.26697592969451633
RMSE train: 0.485262	val: 0.733170	test: 0.644650
MAE train: 0.375045	val: 0.538316	test: 0.481598

Epoch: 130
Loss: 0.2706644630857876
RMSE train: 0.475229	val: 0.733275	test: 0.640965
MAE train: 0.368556	val: 0.539353	test: 0.474168

Epoch: 131
Loss: 0.2671216034463474
RMSE train: 0.471499	val: 0.738187	test: 0.642737
MAE train: 0.365829	val: 0.539036	test: 0.473266

Epoch: 132
Loss: 0.2597823292016983
RMSE train: 0.482651	val: 0.741902	test: 0.647768
MAE train: 0.375965	val: 0.541576	test: 0.479324

Epoch: 133
Loss: 0.2721151454108102
RMSE train: 0.457656	val: 0.721453	test: 0.618773
MAE train: 0.353714	val: 0.526780	test: 0.458530

Epoch: 134
Loss: 0.27454705855676104
RMSE train: 0.484460	val: 0.731392	test: 0.651643
MAE train: 0.378805	val: 0.558044	test: 0.490513

Epoch: 135
Loss: 0.2650629888687815
RMSE train: 0.468619	val: 0.719892	test: 0.639279
MAE train: 0.364704	val: 0.540831	test: 0.475303

Epoch: 136
Loss: 0.26278397866657804
RMSE train: 0.488962	val: 0.739228	test: 0.644217
MAE train: 0.381780	val: 0.544784	test: 0.482501

Epoch: 137
Loss: 0.2582232164485114
RMSE train: 0.461305	val: 0.726055	test: 0.610760
MAE train: 0.357718	val: 0.538373	test: 0.455347

Epoch: 138
Loss: 0.2825583006654467
RMSE train: 0.492323	val: 0.729679	test: 0.660237
MAE train: 0.381717	val: 0.551197	test: 0.502350

Epoch: 139
Loss: 0.26019569273505894
RMSE train: 0.491206	val: 0.720829	test: 0.657552
MAE train: 0.380042	val: 0.533191	test: 0.490494

Epoch: 140
Loss: 0.26555596930640085
RMSE train: 0.458332	val: 0.716429	test: 0.628709
MAE train: 0.354227	val: 0.529115	test: 0.465833

Epoch: 141
Loss: 0.26068930860076633
RMSE train: 0.464666	val: 0.730798	test: 0.641810
MAE train: 0.360328	val: 0.537448	test: 0.474092

Epoch: 142
Loss: 0.2686473737869944
RMSE train: 0.489444	val: 0.730431	test: 0.647231
MAE train: 0.381533	val: 0.544699	test: 0.481915

Epoch: 143
Loss: 0.24214590979473932
RMSE train: 0.458297	val: 0.700580	test: 0.633274
MAE train: 0.353573	val: 0.536329	test: 0.466591

Epoch: 144
Loss: 0.23243523389101028
RMSE train: 0.450791	val: 0.704284	test: 0.672595
MAE train: 0.352798	val: 0.507576	test: 0.508200

Epoch: 145
Loss: 0.23402322207887968
RMSE train: 0.422940	val: 0.691970	test: 0.662425
MAE train: 0.329081	val: 0.500647	test: 0.498068

Epoch: 146
Loss: 0.2206385980049769
RMSE train: 0.423205	val: 0.684199	test: 0.662362
MAE train: 0.330431	val: 0.489191	test: 0.499856

Epoch: 147
Loss: 0.22182459011673927
RMSE train: 0.443732	val: 0.712164	test: 0.674536
MAE train: 0.347549	val: 0.507423	test: 0.510560

Epoch: 148
Loss: 0.23838686694701514
RMSE train: 0.417346	val: 0.684517	test: 0.653696
MAE train: 0.327623	val: 0.491306	test: 0.494441

Epoch: 149
Loss: 0.24169802169005075
RMSE train: 0.455940	val: 0.710362	test: 0.678490
MAE train: 0.354953	val: 0.518012	test: 0.516471

Epoch: 150
Loss: 0.23097407817840576
RMSE train: 0.416080	val: 0.680151	test: 0.649475
MAE train: 0.325965	val: 0.489034	test: 0.492618

Epoch: 151
Loss: 0.21220171079039574
RMSE train: 0.422137	val: 0.690798	test: 0.658378
MAE train: 0.329713	val: 0.493049	test: 0.496418

Epoch: 152
Loss: 0.2160879150032997
RMSE train: 0.414425	val: 0.686345	test: 0.647747
MAE train: 0.324321	val: 0.494164	test: 0.488122

Epoch: 153
Loss: 0.21624271695812544
RMSE train: 0.426430	val: 0.689670	test: 0.654229
MAE train: 0.336132	val: 0.498703	test: 0.501447

Epoch: 154
Loss: 0.21612671265999475
RMSE train: 0.416209	val: 0.687557	test: 0.646806
MAE train: 0.325836	val: 0.500136	test: 0.494755

Epoch: 155
Loss: 0.22274016837279
RMSE train: 0.421747	val: 0.691223	test: 0.662984
MAE train: 0.330479	val: 0.503465	test: 0.501515

Epoch: 156
Loss: 0.21881472319364548
RMSE train: 0.420203	val: 0.685415	test: 0.662139
MAE train: 0.328977	val: 0.491989	test: 0.501373

Epoch: 157
Loss: 0.21839114278554916
RMSE train: 0.428367	val: 0.706452	test: 0.658577
MAE train: 0.335513	val: 0.513775	test: 0.500496

Epoch: 158
Loss: 0.21789688368638357
RMSE train: 0.400683	val: 0.675022	test: 0.652215
MAE train: 0.314477	val: 0.489014	test: 0.495011

Epoch: 159
Loss: 0.22040271138151488
RMSE train: 0.419644	val: 0.692510	test: 0.662432
MAE train: 0.327015	val: 0.508379	test: 0.505291

Epoch: 160
Loss: 0.22098886469999948
RMSE train: 0.404211	val: 0.680366	test: 0.651576
MAE train: 0.315179	val: 0.487104	test: 0.489806

Epoch: 161
Loss: 0.21446413298447928
RMSE train: 0.442098	val: 0.702681	test: 0.665278
MAE train: 0.348470	val: 0.508090	test: 0.504970

Early stopping
Best (RMSE):	 train: 0.437653	val: 0.668605	test: 0.659090
Best (MAE):	 train: 0.339466	val: 0.488035	test: 0.495917

MAE train: 0.416945	val: 0.571073	test: 0.496909

Epoch: 84
Loss: 0.3567095398902893
RMSE train: 0.549055	val: 0.780095	test: 0.686977
MAE train: 0.429663	val: 0.576828	test: 0.513976

Epoch: 85
Loss: 0.3468923951898302
RMSE train: 0.530441	val: 0.773560	test: 0.666985
MAE train: 0.414323	val: 0.564613	test: 0.492907

Epoch: 86
Loss: 0.3223574789507048
RMSE train: 0.547466	val: 0.771481	test: 0.684361
MAE train: 0.427734	val: 0.572167	test: 0.508791

Epoch: 87
Loss: 0.34925562143325806
RMSE train: 0.574087	val: 0.798772	test: 0.695488
MAE train: 0.450808	val: 0.598308	test: 0.519730

Epoch: 88
Loss: 0.340250836951392
RMSE train: 0.551809	val: 0.785941	test: 0.685375
MAE train: 0.431166	val: 0.581146	test: 0.522540

Epoch: 89
Loss: 0.3473911668573107
RMSE train: 0.582453	val: 0.796278	test: 0.718691
MAE train: 0.456180	val: 0.594589	test: 0.536192

Epoch: 90
Loss: 0.320500659091132
RMSE train: 0.519655	val: 0.768789	test: 0.648033
MAE train: 0.403917	val: 0.563191	test: 0.485803

Epoch: 91
Loss: 0.32003985877547947
RMSE train: 0.543243	val: 0.781093	test: 0.684526
MAE train: 0.423089	val: 0.573767	test: 0.507502

Epoch: 92
Loss: 0.3372190126350948
RMSE train: 0.564372	val: 0.780799	test: 0.679955
MAE train: 0.444403	val: 0.584646	test: 0.511538

Epoch: 93
Loss: 0.32973686286381315
RMSE train: 0.524859	val: 0.753226	test: 0.653325
MAE train: 0.410028	val: 0.552583	test: 0.486106

Epoch: 94
Loss: 0.3278466526951109
RMSE train: 0.545701	val: 0.771628	test: 0.677651
MAE train: 0.425095	val: 0.575594	test: 0.510337

Epoch: 95
Loss: 0.34191499437604633
RMSE train: 0.552888	val: 0.776893	test: 0.678933
MAE train: 0.433032	val: 0.580598	test: 0.511614

Epoch: 96
Loss: 0.3354507620845522
RMSE train: 0.531291	val: 0.781219	test: 0.671412
MAE train: 0.416522	val: 0.579679	test: 0.497281

Epoch: 97
Loss: 0.31887852613415035
RMSE train: 0.611067	val: 0.822499	test: 0.749537
MAE train: 0.479788	val: 0.618762	test: 0.558219

Epoch: 98
Loss: 0.33390915180955616
RMSE train: 0.510432	val: 0.746454	test: 0.649482
MAE train: 0.397163	val: 0.551711	test: 0.481597

Epoch: 99
Loss: 0.3170511679989951
RMSE train: 0.506049	val: 0.744683	test: 0.651010
MAE train: 0.395312	val: 0.552380	test: 0.486866

Epoch: 100
Loss: 0.3215610001768385
RMSE train: 0.503239	val: 0.754536	test: 0.654649
MAE train: 0.393015	val: 0.551708	test: 0.488304

Epoch: 101
Loss: 0.30647421947547365
RMSE train: 0.535613	val: 0.780486	test: 0.689745
MAE train: 0.419070	val: 0.578254	test: 0.511512

Epoch: 102
Loss: 0.2922542563506535
RMSE train: 0.548005	val: 0.785227	test: 0.697506
MAE train: 0.429372	val: 0.584036	test: 0.522109

Epoch: 103
Loss: 0.31948422321251463
RMSE train: 0.506742	val: 0.737689	test: 0.662129
MAE train: 0.396235	val: 0.546453	test: 0.485674

Epoch: 104
Loss: 0.3074346642409052
RMSE train: 0.504470	val: 0.744527	test: 0.657718
MAE train: 0.394746	val: 0.552652	test: 0.486166

Epoch: 105
Loss: 0.3018373165811811
RMSE train: 0.515604	val: 0.750822	test: 0.661847
MAE train: 0.402101	val: 0.558764	test: 0.490375

Epoch: 106
Loss: 0.32255579956940245
RMSE train: 0.517206	val: 0.753744	test: 0.656023
MAE train: 0.403421	val: 0.558494	test: 0.493456

Epoch: 107
Loss: 0.3098428270646504
RMSE train: 0.519919	val: 0.745757	test: 0.658957
MAE train: 0.407434	val: 0.553286	test: 0.485321

Epoch: 108
Loss: 0.30378439490284237
RMSE train: 0.555464	val: 0.786366	test: 0.696098
MAE train: 0.436506	val: 0.587025	test: 0.530058

Epoch: 109
Loss: 0.2914675325155258
RMSE train: 0.518577	val: 0.750044	test: 0.661607
MAE train: 0.404237	val: 0.557304	test: 0.492305

Epoch: 110
Loss: 0.30196867670331684
RMSE train: 0.515121	val: 0.751668	test: 0.666675
MAE train: 0.403038	val: 0.562533	test: 0.492384

Epoch: 111
Loss: 0.29565164021083284
RMSE train: 0.523484	val: 0.745430	test: 0.660923
MAE train: 0.408410	val: 0.557000	test: 0.487922

Epoch: 112
Loss: 0.31529627314635683
RMSE train: 0.510058	val: 0.747998	test: 0.667142
MAE train: 0.397103	val: 0.561341	test: 0.488461

Epoch: 113
Loss: 0.30989089395318714
RMSE train: 0.498727	val: 0.762484	test: 0.645426
MAE train: 0.389623	val: 0.558159	test: 0.476816

Epoch: 114
Loss: 0.29890365472861696
RMSE train: 0.487691	val: 0.744010	test: 0.656657
MAE train: 0.380520	val: 0.549664	test: 0.479606

Epoch: 115
Loss: 0.3007522353104183
RMSE train: 0.490551	val: 0.746460	test: 0.652380
MAE train: 0.382664	val: 0.549735	test: 0.480279

Epoch: 116
Loss: 0.30820452741214205
RMSE train: 0.518537	val: 0.763812	test: 0.669265
MAE train: 0.407471	val: 0.568425	test: 0.497573

Epoch: 117
Loss: 0.304989935031959
RMSE train: 0.493060	val: 0.742018	test: 0.652284
MAE train: 0.384549	val: 0.553242	test: 0.479892

Epoch: 118
Loss: 0.2907160371541977
RMSE train: 0.503644	val: 0.776736	test: 0.653651
MAE train: 0.392819	val: 0.558124	test: 0.487926

Epoch: 119
Loss: 0.29274725168943405
RMSE train: 0.476723	val: 0.750992	test: 0.643422
MAE train: 0.373746	val: 0.553969	test: 0.484085

Epoch: 120
Loss: 0.2781163996883801
RMSE train: 0.504878	val: 0.769677	test: 0.631196
MAE train: 0.393601	val: 0.561079	test: 0.467454

Epoch: 121
Loss: 0.3047579271452768
RMSE train: 0.503619	val: 0.755916	test: 0.668865
MAE train: 0.390223	val: 0.562628	test: 0.490397

Epoch: 122
Loss: 0.2909542577607291
RMSE train: 0.503748	val: 0.773235	test: 0.627480
MAE train: 0.392813	val: 0.565626	test: 0.471701

Epoch: 123
Loss: 0.29535740720374243
RMSE train: 0.505853	val: 0.757468	test: 0.645678
MAE train: 0.394936	val: 0.561565	test: 0.479180

Epoch: 124
Loss: 0.29444617671625956
RMSE train: 0.504480	val: 0.766886	test: 0.656082
MAE train: 0.394100	val: 0.571948	test: 0.488762

Epoch: 125
Loss: 0.2895475998520851
RMSE train: 0.484995	val: 0.759873	test: 0.627162
MAE train: 0.379288	val: 0.550791	test: 0.466354

Epoch: 126
Loss: 0.279839246400765
RMSE train: 0.501414	val: 0.786363	test: 0.673638
MAE train: 0.389459	val: 0.580288	test: 0.500717

Epoch: 127
Loss: 0.27102717544351307
RMSE train: 0.473713	val: 0.737063	test: 0.641341
MAE train: 0.370487	val: 0.544630	test: 0.472788

Epoch: 128
Loss: 0.2741582968405315
RMSE train: 0.491027	val: 0.743165	test: 0.655678
MAE train: 0.383178	val: 0.552869	test: 0.477499

Epoch: 129
Loss: 0.2750411863837923
RMSE train: 0.462844	val: 0.735672	test: 0.615122
MAE train: 0.360329	val: 0.538177	test: 0.454438

Epoch: 130
Loss: 0.300379247537681
RMSE train: 0.473891	val: 0.754231	test: 0.648242
MAE train: 0.368125	val: 0.554465	test: 0.476377

Epoch: 131
Loss: 0.270189069211483
RMSE train: 0.495542	val: 0.774930	test: 0.680880
MAE train: 0.388902	val: 0.565965	test: 0.500498

Epoch: 132
Loss: 0.26034797515187946
RMSE train: 0.482849	val: 0.759962	test: 0.660655
MAE train: 0.378419	val: 0.557824	test: 0.481435

Epoch: 133
Loss: 0.2623289078474045
RMSE train: 0.479657	val: 0.758209	test: 0.636282
MAE train: 0.376248	val: 0.547242	test: 0.469516

Epoch: 134
Loss: 0.25986178325755255
RMSE train: 0.456319	val: 0.741590	test: 0.628844
MAE train: 0.356135	val: 0.536687	test: 0.465648

Epoch: 135
Loss: 0.26153609263045446
RMSE train: 0.505657	val: 0.772185	test: 0.662292
MAE train: 0.396926	val: 0.562330	test: 0.489409

Epoch: 136
Loss: 0.25588709115982056
RMSE train: 0.455357	val: 0.750075	test: 0.647931
MAE train: 0.353320	val: 0.545098	test: 0.475598

Epoch: 137
Loss: 0.2831992304750851
RMSE train: 0.465960	val: 0.761881	test: 0.639868
MAE train: 0.364794	val: 0.559442	test: 0.473091

Epoch: 138
Loss: 0.27465978264808655
RMSE train: 0.466718	val: 0.756795	test: 0.644179
MAE train: 0.365467	val: 0.561551	test: 0.484497

Epoch: 139
Loss: 0.2776709177664348
RMSE train: 0.500287	val: 0.750625	test: 0.659680
MAE train: 0.395346	val: 0.552175	test: 0.497360

Epoch: 140
Loss: 0.27786597183772493
RMSE train: 0.472505	val: 0.748202	test: 0.637371
MAE train: 0.372546	val: 0.547787	test: 0.476228

Epoch: 141
Loss: 0.27237734624317717
RMSE train: 0.474499	val: 0.770419	test: 0.654423
MAE train: 0.370305	val: 0.560749	test: 0.483249

Epoch: 142
Loss: 0.2674639821052551
RMSE train: 0.482170	val: 0.740244	test: 0.646496
MAE train: 0.378263	val: 0.551065	test: 0.485000

Epoch: 143
Loss: 0.2532423713377544
RMSE train: 0.456084	val: 0.741830	test: 0.641537
MAE train: 0.356907	val: 0.537150	test: 0.474566

Epoch: 144
Loss: 0.22650331556797026
RMSE train: 0.411374	val: 0.677483	test: 0.673335
MAE train: 0.322406	val: 0.492653	test: 0.503045

Epoch: 145
Loss: 0.21689763516187668
RMSE train: 0.414315	val: 0.687856	test: 0.676591
MAE train: 0.326300	val: 0.497848	test: 0.500617

Epoch: 146
Loss: 0.2131008416414261
RMSE train: 0.421732	val: 0.671691	test: 0.684640
MAE train: 0.332848	val: 0.492834	test: 0.508736

Epoch: 147
Loss: 0.21501463204622268
RMSE train: 0.405079	val: 0.676021	test: 0.673562
MAE train: 0.317940	val: 0.489931	test: 0.500275

Epoch: 148
Loss: 0.22188047766685487
RMSE train: 0.413871	val: 0.678799	test: 0.679811
MAE train: 0.324800	val: 0.490523	test: 0.506620

Epoch: 149
Loss: 0.2196260780096054
RMSE train: 0.420753	val: 0.683420	test: 0.675314
MAE train: 0.330027	val: 0.492962	test: 0.503940

Epoch: 150
Loss: 0.21811244189739226
RMSE train: 0.428417	val: 0.682800	test: 0.677523
MAE train: 0.335064	val: 0.494556	test: 0.509710

Epoch: 151
Loss: 0.22071328908205032
RMSE train: 0.424760	val: 0.682571	test: 0.677399
MAE train: 0.334436	val: 0.489695	test: 0.509169

Epoch: 152
Loss: 0.2167096421122551
RMSE train: 0.399223	val: 0.671689	test: 0.676498
MAE train: 0.311598	val: 0.485214	test: 0.501315

Epoch: 153
Loss: 0.22474664002656936
RMSE train: 0.410206	val: 0.681466	test: 0.687002
MAE train: 0.322154	val: 0.488350	test: 0.501853

Epoch: 154
Loss: 0.2085077866911888
RMSE train: 0.420156	val: 0.675650	test: 0.672779
MAE train: 0.327953	val: 0.485213	test: 0.503437

Epoch: 155
Loss: 0.2060576558113098
RMSE train: 0.405546	val: 0.678999	test: 0.666465
MAE train: 0.317725	val: 0.488132	test: 0.497919

Epoch: 156
Loss: 0.21267743408679962
RMSE train: 0.410725	val: 0.669717	test: 0.671084
MAE train: 0.321529	val: 0.487998	test: 0.502756

Epoch: 157
Loss: 0.21104493588209153
RMSE train: 0.422054	val: 0.686500	test: 0.681944
MAE train: 0.331068	val: 0.494769	test: 0.508998

Epoch: 158
Loss: 0.21947510540485382
RMSE train: 0.418460	val: 0.675930	test: 0.681189
MAE train: 0.328080	val: 0.492078	test: 0.507261

Epoch: 159
Loss: 0.21673296838998796
RMSE train: 0.397579	val: 0.670280	test: 0.673863
MAE train: 0.311013	val: 0.489038	test: 0.501431

Epoch: 160
Loss: 0.19939652681350709
RMSE train: 0.396233	val: 0.679450	test: 0.674688
MAE train: 0.308886	val: 0.491872	test: 0.501274

Epoch: 161
Loss: 0.19816861152648926
RMSE train: 0.410267	val: 0.670766	test: 0.670207
MAE train: 0.322909	val: 0.489949	test: 0.500180

Epoch: 162
Loss: 0.20385447442531585
RMSE train: 0.391834	val: 0.661701	test: 0.676939
MAE train: 0.307873	val: 0.477975	test: 0.501023

Epoch: 163
Loss: 0.2100067541003227
RMSE train: 0.411820	val: 0.676043	test: 0.678749
MAE train: 0.324617	val: 0.484463	test: 0.501888

Epoch: 164
Loss: 0.19934052973985672
RMSE train: 0.402316	val: 0.670068	test: 0.677887
MAE train: 0.314576	val: 0.481687	test: 0.503622

Epoch: 165
Loss: 0.21314390152692794
RMSE train: 0.395607	val: 0.663697	test: 0.673536
MAE train: 0.309285	val: 0.478519	test: 0.496112

Epoch: 166
Loss: 0.20494623184204103
RMSE train: 0.401619	val: 0.660163	test: 0.677500
MAE train: 0.314460	val: 0.477557	test: 0.500587

Epoch: 167
Loss: 0.1958585113286972
RMSE train: 0.421388	val: 0.698473	test: 0.680691
MAE train: 0.329747	val: 0.498864	test: 0.508913

Epoch: 168
Loss: 0.2093188002705574
RMSE train: 0.409225	val: 0.692522	test: 0.684257
MAE train: 0.320639	val: 0.505825	test: 0.509785

Epoch: 169
Loss: 0.21108753085136414
RMSE train: 0.390090	val: 0.677173	test: 0.677600
MAE train: 0.306108	val: 0.488115	test: 0.502447

Epoch: 170
Loss: 0.20256697684526442
RMSE train: 0.410963	val: 0.685026	test: 0.681698
MAE train: 0.323768	val: 0.491322	test: 0.506293

Epoch: 171
Loss: 0.19170130491256715
RMSE train: 0.406421	val: 0.674762	test: 0.685969
MAE train: 0.320234	val: 0.486136	test: 0.505681

Epoch: 172
Loss: 0.20386491268873214
RMSE train: 0.399876	val: 0.673314	test: 0.678776
MAE train: 0.313930	val: 0.485501	test: 0.500078

Epoch: 173
Loss: 0.20092718452215194
RMSE train: 0.397096	val: 0.671640	test: 0.678379
MAE train: 0.311360	val: 0.492443	test: 0.503647

Epoch: 174
Loss: 0.19730516374111176
RMSE train: 0.385219	val: 0.673522	test: 0.673219
MAE train: 0.301694	val: 0.482671	test: 0.495856

Epoch: 175
Loss: 0.2022177457809448
RMSE train: 0.398495	val: 0.678991	test: 0.679125
MAE train: 0.312003	val: 0.488410	test: 0.504534

Epoch: 176
Loss: 0.19893524795770645
RMSE train: 0.395412	val: 0.681557	test: 0.664316
MAE train: 0.311251	val: 0.489592	test: 0.502890

Epoch: 177
Loss: 0.20869995504617692
RMSE train: 0.415494	val: 0.676547	test: 0.680642
MAE train: 0.324360	val: 0.489698	test: 0.511776

Epoch: 178
Loss: 0.19383023530244828
RMSE train: 0.405697	val: 0.684942	test: 0.684406
MAE train: 0.316061	val: 0.496004	test: 0.513725

Epoch: 179
Loss: 0.19408282041549682
RMSE train: 0.395696	val: 0.676669	test: 0.670515
MAE train: 0.310382	val: 0.484752	test: 0.498496

Epoch: 180
Loss: 0.1977192461490631
RMSE train: 0.384694	val: 0.669786	test: 0.669958
MAE train: 0.299686	val: 0.489920	test: 0.502231

Epoch: 181
Loss: 0.18779468536376953
RMSE train: 0.379505	val: 0.665605	test: 0.657928
MAE train: 0.296029	val: 0.477763	test: 0.485957

Epoch: 182
Loss: 0.19598426073789596
RMSE train: 0.399018	val: 0.679750	test: 0.672865
MAE train: 0.309868	val: 0.485430	test: 0.501029

Epoch: 183
Loss: 0.18742229640483857
RMSE train: 0.386850	val: 0.671172	test: 0.664716
MAE train: 0.302935	val: 0.482450	test: 0.500271

Epoch: 184
Loss: 0.19110038876533508
RMSE train: 0.397333	val: 0.671456	test: 0.670973
MAE train: 0.312706	val: 0.483870	test: 0.501189

Epoch: 185
Loss: 0.18099809288978577
RMSE train: 0.378157	val: 0.661627	test: 0.665755
MAE train: 0.296125	val: 0.475276	test: 0.495839

Epoch: 186
Loss: 0.19641835987567902
RMSE train: 0.393862	val: 0.665209	test: 0.666798
MAE train: 0.304740	val: 0.482993	test: 0.503106

Epoch: 187
Loss: 0.18877895325422286
RMSE train: 0.399858	val: 0.679270	test: 0.678305
MAE train: 0.311515	val: 0.487655	test: 0.507382

Epoch: 188
Loss: 0.1921950474381447
RMSE train: 0.386432	val: 0.677913	test: 0.670593
MAE train: 0.301257	val: 0.494309	test: 0.502226

Epoch: 189
Loss: 0.17933601140975952
RMSE train: 0.374157	val: 0.682885	test: 0.666122
MAE train: 0.291343	val: 0.491272	test: 0.496613

Epoch: 190
Loss: 0.19239456355571746
RMSE train: 0.386997	val: 0.666629	test: 0.672523
MAE train: 0.303507	val: 0.484907	test: 0.501146

Epoch: 191
Loss: 0.1866031214594841
RMSE train: 0.380195	val: 0.669599	test: 0.676346
MAE train: 0.297901	val: 0.480453	test: 0.499261

Epoch: 192
Loss: 0.18549350649118423
RMSE train: 0.377937	val: 0.674406	test: 0.667395
MAE train: 0.296867	val: 0.481371	test: 0.491571

Epoch: 193
Loss: 0.17723124772310256
RMSE train: 0.384936	val: 0.679910	test: 0.661316
MAE train: 0.301128	val: 0.489182	test: 0.496721

Epoch: 194
Loss: 0.18495896309614182
RMSE train: 0.395093	val: 0.671779	test: 0.674942
MAE train: 0.311116	val: 0.488491	test: 0.508111

Epoch: 195
Loss: 0.18626968711614608
RMSE train: 0.381983	val: 0.675532	test: 0.681524
MAE train: 0.299482	val: 0.489919	test: 0.506426

Epoch: 196
Loss: 0.17837154269218444
RMSE train: 0.392482	val: 0.688601	test: 0.687783
MAE train: 0.308042	val: 0.494299	test: 0.511306

Epoch: 197
Loss: 0.1757970467209816
RMSE train: 0.368898	val: 0.675271	test: 0.669916
MAE train: 0.289689	val: 0.480449	test: 0.493570

Epoch: 198
Loss: 0.17482240945100785
RMSE train: 0.374774	val: 0.674786	test: 0.670559
MAE train: 0.289966	val: 0.483783	test: 0.502167

Epoch: 199
Loss: 0.17240252792835237
RMSE train: 0.380189	val: 0.679834	test: 0.672783
MAE train: 0.298492	val: 0.483576	test: 0.498170

Epoch: 200
Loss: 0.1786270409822464
RMSE train: 0.370190	val: 0.670723	test: 0.682673
MAE train: 0.287894	val: 0.479933	test: 0.504303

Epoch: 201
Loss: 0.1805408388376236
RMSE train: 0.382561	val: 0.675993	test: 0.672771
MAE train: 0.302808	val: 0.488433	test: 0.500200

Early stopping
Best (RMSE):	 train: 0.401619	val: 0.660163	test: 0.677500
Best (MAE):	 train: 0.314460	val: 0.477557	test: 0.500587
All runs completed.


Epoch: 144
Loss: 0.2238065004348755
RMSE train: 0.447941	val: 0.698631	test: 0.655692
MAE train: 0.354447	val: 0.504678	test: 0.508550

Epoch: 145
Loss: 0.2168218418955803
RMSE train: 0.463068	val: 0.704395	test: 0.695752
MAE train: 0.362175	val: 0.505727	test: 0.531755

Epoch: 146
Loss: 0.2237911857664585
RMSE train: 0.422852	val: 0.682383	test: 0.652154
MAE train: 0.332108	val: 0.482056	test: 0.503074

Epoch: 147
Loss: 0.2236903247733911
RMSE train: 0.442636	val: 0.672125	test: 0.659376
MAE train: 0.350751	val: 0.483605	test: 0.510751

Epoch: 148
Loss: 0.23343449582656225
RMSE train: 0.421994	val: 0.677521	test: 0.660666
MAE train: 0.331148	val: 0.485614	test: 0.506907

Epoch: 149
Loss: 0.2264541250964006
RMSE train: 0.438177	val: 0.700842	test: 0.665370
MAE train: 0.344363	val: 0.503003	test: 0.509589

Epoch: 150
Loss: 0.21505330627163252
RMSE train: 0.445487	val: 0.699206	test: 0.674846
MAE train: 0.349629	val: 0.499038	test: 0.515518

Epoch: 151
Loss: 0.22121359159549078
RMSE train: 0.415542	val: 0.675680	test: 0.648847
MAE train: 0.326882	val: 0.484959	test: 0.502577

Epoch: 152
Loss: 0.21473595996697745
RMSE train: 0.435995	val: 0.690247	test: 0.659520
MAE train: 0.343215	val: 0.492481	test: 0.508293

Epoch: 153
Loss: 0.21700936804215112
RMSE train: 0.431651	val: 0.695440	test: 0.658719
MAE train: 0.339061	val: 0.499340	test: 0.506087

Epoch: 154
Loss: 0.22550870726505914
RMSE train: 0.428141	val: 0.699100	test: 0.669620
MAE train: 0.336151	val: 0.504943	test: 0.512737

Epoch: 155
Loss: 0.2255822146932284
RMSE train: 0.427894	val: 0.682248	test: 0.659447
MAE train: 0.337248	val: 0.491448	test: 0.508384

Epoch: 156
Loss: 0.21910187850395837
RMSE train: 0.419520	val: 0.674673	test: 0.646348
MAE train: 0.330302	val: 0.479191	test: 0.497913

Epoch: 157
Loss: 0.2199610024690628
RMSE train: 0.449747	val: 0.700901	test: 0.689442
MAE train: 0.353184	val: 0.508049	test: 0.524574

Epoch: 158
Loss: 0.2246918926636378
RMSE train: 0.430570	val: 0.692204	test: 0.672936
MAE train: 0.335352	val: 0.493108	test: 0.512558

Epoch: 159
Loss: 0.21498137339949608
RMSE train: 0.417184	val: 0.676695	test: 0.650918
MAE train: 0.326826	val: 0.479737	test: 0.498366

Epoch: 160
Loss: 0.20703018953402838
RMSE train: 0.431223	val: 0.684333	test: 0.663676
MAE train: 0.338512	val: 0.489754	test: 0.509508

Epoch: 161
Loss: 0.2133505940437317
RMSE train: 0.411274	val: 0.671675	test: 0.656889
MAE train: 0.322426	val: 0.483251	test: 0.502925

Epoch: 162
Loss: 0.21819429472088814
RMSE train: 0.420939	val: 0.691784	test: 0.669468
MAE train: 0.328826	val: 0.504141	test: 0.507648

Epoch: 163
Loss: 0.20663253838817278
RMSE train: 0.419710	val: 0.676986	test: 0.666655
MAE train: 0.329240	val: 0.495152	test: 0.505442

Epoch: 164
Loss: 0.21157706901431084
RMSE train: 0.423882	val: 0.686062	test: 0.657399
MAE train: 0.330832	val: 0.483940	test: 0.499567

Epoch: 165
Loss: 0.21769450356562933
RMSE train: 0.405982	val: 0.673247	test: 0.647991
MAE train: 0.317867	val: 0.482714	test: 0.495298

Epoch: 166
Loss: 0.21349251146117845
RMSE train: 0.422383	val: 0.683998	test: 0.669794
MAE train: 0.331783	val: 0.498404	test: 0.512716

Epoch: 167
Loss: 0.19748660549521446
RMSE train: 0.401597	val: 0.671177	test: 0.654933
MAE train: 0.314547	val: 0.489449	test: 0.496050

Epoch: 168
Loss: 0.20940431331594786
RMSE train: 0.429467	val: 0.696375	test: 0.658162
MAE train: 0.338376	val: 0.495667	test: 0.498807

Epoch: 169
Loss: 0.20444861551125845
RMSE train: 0.400819	val: 0.672265	test: 0.648093
MAE train: 0.315275	val: 0.483262	test: 0.496901

Epoch: 170
Loss: 0.20187521974245706
RMSE train: 0.406130	val: 0.667382	test: 0.648935
MAE train: 0.318620	val: 0.478681	test: 0.496596

Epoch: 171
Loss: 0.21324511741598448
RMSE train: 0.417524	val: 0.692116	test: 0.678755
MAE train: 0.327885	val: 0.491351	test: 0.512504

Epoch: 172
Loss: 0.200364600867033
RMSE train: 0.403068	val: 0.685274	test: 0.662032
MAE train: 0.315275	val: 0.493101	test: 0.501865

Epoch: 173
Loss: 0.20187617714206377
RMSE train: 0.409360	val: 0.681134	test: 0.661935
MAE train: 0.323802	val: 0.487046	test: 0.504191

Epoch: 174
Loss: 0.20539959271748862
RMSE train: 0.395515	val: 0.666307	test: 0.645619
MAE train: 0.310415	val: 0.475629	test: 0.493547

Epoch: 175
Loss: 0.19428914040327072
RMSE train: 0.414775	val: 0.676909	test: 0.657126
MAE train: 0.325395	val: 0.485424	test: 0.502706

Epoch: 176
Loss: 0.19302441303928694
RMSE train: 0.391465	val: 0.674789	test: 0.650015
MAE train: 0.307116	val: 0.486950	test: 0.492976

Epoch: 177
Loss: 0.1950544354816278
RMSE train: 0.405323	val: 0.689786	test: 0.657754
MAE train: 0.317516	val: 0.492809	test: 0.499088

Epoch: 178
Loss: 0.19038180013497671
RMSE train: 0.400748	val: 0.685726	test: 0.656117
MAE train: 0.313373	val: 0.495776	test: 0.497795

Epoch: 179
Loss: 0.1942835027972857
RMSE train: 0.402398	val: 0.672196	test: 0.655003
MAE train: 0.317578	val: 0.487228	test: 0.501958

Epoch: 180
Loss: 0.1960070418814818
RMSE train: 0.403123	val: 0.688010	test: 0.653807
MAE train: 0.315859	val: 0.505721	test: 0.500320

Epoch: 181
Loss: 0.19811359172066054
RMSE train: 0.414557	val: 0.689410	test: 0.658899
MAE train: 0.325539	val: 0.500032	test: 0.505414

Epoch: 182
Loss: 0.19427779068549475
RMSE train: 0.429873	val: 0.699437	test: 0.689441
MAE train: 0.338510	val: 0.510828	test: 0.527251

Epoch: 183
Loss: 0.20054466277360916
RMSE train: 0.397591	val: 0.677097	test: 0.658915
MAE train: 0.312777	val: 0.488516	test: 0.501130

Epoch: 184
Loss: 0.19001546874642372
RMSE train: 0.444973	val: 0.698388	test: 0.683640
MAE train: 0.351928	val: 0.503411	test: 0.520671

Epoch: 185
Loss: 0.1936016802986463
RMSE train: 0.402861	val: 0.679402	test: 0.652996
MAE train: 0.313922	val: 0.485214	test: 0.495504

Epoch: 186
Loss: 0.18772185842196146
RMSE train: 0.389167	val: 0.672333	test: 0.649993
MAE train: 0.303137	val: 0.478128	test: 0.496069

Epoch: 187
Loss: 0.19321214531858763
RMSE train: 0.394533	val: 0.671706	test: 0.656124
MAE train: 0.308229	val: 0.482854	test: 0.501122

Epoch: 188
Loss: 0.20911414548754692
RMSE train: 0.383780	val: 0.673339	test: 0.650213
MAE train: 0.300159	val: 0.474735	test: 0.493706

Epoch: 189
Loss: 0.19051124900579453
RMSE train: 0.387163	val: 0.670848	test: 0.651346
MAE train: 0.304897	val: 0.480950	test: 0.493011

Epoch: 190
Loss: 0.19340775534510612
RMSE train: 0.410697	val: 0.700994	test: 0.672363
MAE train: 0.321754	val: 0.502051	test: 0.517608

Epoch: 191
Loss: 0.188205619653066
RMSE train: 0.384289	val: 0.673524	test: 0.649506
MAE train: 0.300813	val: 0.486725	test: 0.497038

Epoch: 192
Loss: 0.18552323430776596
RMSE train: 0.391218	val: 0.681698	test: 0.655445
MAE train: 0.306815	val: 0.494762	test: 0.501069

Epoch: 193
Loss: 0.18527550746997198
RMSE train: 0.384398	val: 0.676838	test: 0.651057
MAE train: 0.299959	val: 0.489286	test: 0.497960

Epoch: 194
Loss: 0.18756306916475296
RMSE train: 0.402813	val: 0.696767	test: 0.663599
MAE train: 0.317105	val: 0.494705	test: 0.504782

Epoch: 195
Loss: 0.18220670272906622
RMSE train: 0.386018	val: 0.673873	test: 0.646004
MAE train: 0.303278	val: 0.480320	test: 0.492659

Epoch: 196
Loss: 0.1807182841002941
RMSE train: 0.392694	val: 0.684866	test: 0.657480
MAE train: 0.308397	val: 0.489833	test: 0.499419

Epoch: 197
Loss: 0.18483051657676697
RMSE train: 0.374495	val: 0.673691	test: 0.659026
MAE train: 0.292882	val: 0.482745	test: 0.497390

Epoch: 198
Loss: 0.19677073260148367
RMSE train: 0.396679	val: 0.691127	test: 0.677735
MAE train: 0.311296	val: 0.495542	test: 0.513153

Epoch: 199
Loss: 0.18180049459139505
RMSE train: 0.388778	val: 0.680527	test: 0.653344
MAE train: 0.304902	val: 0.483824	test: 0.498078

Epoch: 200
Loss: 0.18670805419484773
RMSE train: 0.391008	val: 0.673780	test: 0.659759
MAE train: 0.306650	val: 0.484562	test: 0.499477

Epoch: 201
Loss: 0.1900403822461764
RMSE train: 0.408772	val: 0.693035	test: 0.679394
MAE train: 0.319542	val: 0.490542	test: 0.520311

Epoch: 202
Loss: 0.18432437380154928
RMSE train: 0.390730	val: 0.667639	test: 0.651889
MAE train: 0.307964	val: 0.481360	test: 0.498262

Epoch: 203
Loss: 0.19196226075291634
RMSE train: 0.397674	val: 0.695319	test: 0.674743
MAE train: 0.313692	val: 0.496400	test: 0.510406

Epoch: 144
Loss: 0.22416173294186592
RMSE train: 0.436955	val: 0.722997	test: 0.665652
MAE train: 0.341821	val: 0.506655	test: 0.508150

Epoch: 145
Loss: 0.22672793517510095
RMSE train: 0.436056	val: 0.703717	test: 0.664745
MAE train: 0.341870	val: 0.504133	test: 0.503903

Epoch: 146
Loss: 0.22414244463046393
RMSE train: 0.436682	val: 0.717321	test: 0.667087
MAE train: 0.339827	val: 0.508455	test: 0.506192

Epoch: 147
Loss: 0.226930753638347
RMSE train: 0.443539	val: 0.712660	test: 0.677335
MAE train: 0.345515	val: 0.502384	test: 0.513183

Epoch: 148
Loss: 0.225522859642903
RMSE train: 0.429558	val: 0.713207	test: 0.667151
MAE train: 0.335972	val: 0.496023	test: 0.508876

Epoch: 149
Loss: 0.22242486973603567
RMSE train: 0.462414	val: 0.724849	test: 0.671968
MAE train: 0.364282	val: 0.528496	test: 0.514776

Epoch: 150
Loss: 0.22566979254285494
RMSE train: 0.419530	val: 0.708988	test: 0.664783
MAE train: 0.327487	val: 0.503920	test: 0.503624

Epoch: 151
Loss: 0.22974729289611182
RMSE train: 0.451422	val: 0.714851	test: 0.676509
MAE train: 0.353185	val: 0.509773	test: 0.515889

Epoch: 152
Loss: 0.23011283949017525
RMSE train: 0.440699	val: 0.702410	test: 0.665575
MAE train: 0.345603	val: 0.506432	test: 0.506605

Epoch: 153
Loss: 0.22085592399040857
RMSE train: 0.433943	val: 0.716729	test: 0.663727
MAE train: 0.341504	val: 0.513232	test: 0.507612

Epoch: 154
Loss: 0.22229027996460596
RMSE train: 0.435801	val: 0.694594	test: 0.669608
MAE train: 0.340678	val: 0.501330	test: 0.509869

Epoch: 155
Loss: 0.21872382114330927
RMSE train: 0.442012	val: 0.700476	test: 0.664272
MAE train: 0.345258	val: 0.499695	test: 0.505611

Epoch: 156
Loss: 0.22942382469773293
RMSE train: 0.443562	val: 0.716921	test: 0.668309
MAE train: 0.348662	val: 0.503592	test: 0.510092

Epoch: 157
Loss: 0.22347857430577278
RMSE train: 0.434368	val: 0.689997	test: 0.675916
MAE train: 0.338606	val: 0.492609	test: 0.514469

Epoch: 158
Loss: 0.21613416448235512
RMSE train: 0.458347	val: 0.715387	test: 0.679062
MAE train: 0.360570	val: 0.509260	test: 0.517734

Epoch: 159
Loss: 0.214676051090161
RMSE train: 0.427735	val: 0.707650	test: 0.670459
MAE train: 0.333495	val: 0.503200	test: 0.511671

Epoch: 160
Loss: 0.21243142584959665
RMSE train: 0.434949	val: 0.695526	test: 0.678745
MAE train: 0.339977	val: 0.502130	test: 0.514993

Epoch: 161
Loss: 0.21754528706272444
RMSE train: 0.442084	val: 0.706471	test: 0.684351
MAE train: 0.345378	val: 0.507712	test: 0.519397

Epoch: 162
Loss: 0.21489615986744562
RMSE train: 0.425466	val: 0.710567	test: 0.667355
MAE train: 0.333408	val: 0.503483	test: 0.505942

Epoch: 163
Loss: 0.21649729957183203
RMSE train: 0.409518	val: 0.689297	test: 0.662969
MAE train: 0.320097	val: 0.495680	test: 0.502010

Epoch: 164
Loss: 0.21513735254605612
RMSE train: 0.425183	val: 0.701730	test: 0.670270
MAE train: 0.331201	val: 0.494302	test: 0.510969

Epoch: 165
Loss: 0.2139612970252832
RMSE train: 0.419230	val: 0.715507	test: 0.662213
MAE train: 0.329401	val: 0.515491	test: 0.507035

Epoch: 166
Loss: 0.22748441994190216
RMSE train: 0.445795	val: 0.710363	test: 0.666796
MAE train: 0.351106	val: 0.511786	test: 0.514866

Epoch: 167
Loss: 0.21526539201537767
RMSE train: 0.426933	val: 0.702975	test: 0.658144
MAE train: 0.334784	val: 0.497906	test: 0.500675

Epoch: 168
Loss: 0.21229968965053558
RMSE train: 0.464804	val: 0.713030	test: 0.699435
MAE train: 0.362673	val: 0.517542	test: 0.536346

Epoch: 169
Loss: 0.21515827750166258
RMSE train: 0.422889	val: 0.714170	test: 0.672304
MAE train: 0.329960	val: 0.503501	test: 0.507983

Epoch: 170
Loss: 0.2026826192935308
RMSE train: 0.418683	val: 0.714851	test: 0.668237
MAE train: 0.326550	val: 0.510363	test: 0.504061

Epoch: 171
Loss: 0.20999487737814584
RMSE train: 0.408646	val: 0.700945	test: 0.657168
MAE train: 0.319950	val: 0.498317	test: 0.494167

Epoch: 172
Loss: 0.20515512550870577
RMSE train: 0.421901	val: 0.697297	test: 0.669536
MAE train: 0.328715	val: 0.497457	test: 0.508720

Epoch: 173
Loss: 0.2033605749408404
RMSE train: 0.425855	val: 0.699916	test: 0.672658
MAE train: 0.332850	val: 0.500693	test: 0.511992

Epoch: 174
Loss: 0.2106548249721527
RMSE train: 0.412451	val: 0.709252	test: 0.664428
MAE train: 0.322319	val: 0.497564	test: 0.498611

Epoch: 175
Loss: 0.20007219910621643
RMSE train: 0.405184	val: 0.685372	test: 0.658937
MAE train: 0.316786	val: 0.490218	test: 0.498863

Epoch: 176
Loss: 0.19962962220112482
RMSE train: 0.411209	val: 0.689787	test: 0.652218
MAE train: 0.323232	val: 0.490622	test: 0.491600

Epoch: 177
Loss: 0.19728045289715132
RMSE train: 0.400339	val: 0.707432	test: 0.671979
MAE train: 0.312754	val: 0.500749	test: 0.507256

Epoch: 178
Loss: 0.20202832420667013
RMSE train: 0.418901	val: 0.693869	test: 0.671401
MAE train: 0.328806	val: 0.495466	test: 0.509444

Epoch: 179
Loss: 0.19336241856217384
RMSE train: 0.418306	val: 0.708693	test: 0.675019
MAE train: 0.328955	val: 0.503089	test: 0.508969

Epoch: 180
Loss: 0.19525009393692017
RMSE train: 0.399023	val: 0.707465	test: 0.658198
MAE train: 0.312462	val: 0.492221	test: 0.500640

Epoch: 181
Loss: 0.2107009974618753
RMSE train: 0.418880	val: 0.702495	test: 0.678383
MAE train: 0.328431	val: 0.497479	test: 0.516503

Epoch: 182
Loss: 0.18990889067451158
RMSE train: 0.409756	val: 0.696322	test: 0.680412
MAE train: 0.318579	val: 0.493789	test: 0.512674

Epoch: 183
Loss: 0.19151198863983154
RMSE train: 0.401150	val: 0.699967	test: 0.653703
MAE train: 0.313883	val: 0.487925	test: 0.492621

Epoch: 184
Loss: 0.19577077900369963
RMSE train: 0.419640	val: 0.698066	test: 0.668092
MAE train: 0.328992	val: 0.494556	test: 0.507692

Epoch: 185
Loss: 0.19344582532842955
RMSE train: 0.420158	val: 0.705461	test: 0.668632
MAE train: 0.327946	val: 0.499323	test: 0.507433

Epoch: 186
Loss: 0.1976917659242948
RMSE train: 0.442055	val: 0.705659	test: 0.681610
MAE train: 0.346964	val: 0.507296	test: 0.516143

Epoch: 187
Loss: 0.2030679720143477
RMSE train: 0.400665	val: 0.703036	test: 0.665800
MAE train: 0.311179	val: 0.501040	test: 0.501444

Epoch: 188
Loss: 0.19133102397123972
RMSE train: 0.407573	val: 0.713774	test: 0.663833
MAE train: 0.319127	val: 0.502095	test: 0.499614

Epoch: 189
Loss: 0.20096002146601677
RMSE train: 0.401288	val: 0.701932	test: 0.659577
MAE train: 0.314905	val: 0.496680	test: 0.498138

Epoch: 190
Loss: 0.1938509208460649
RMSE train: 0.415465	val: 0.705431	test: 0.672805
MAE train: 0.326212	val: 0.505325	test: 0.508191

Epoch: 191
Loss: 0.19084927439689636
RMSE train: 0.384594	val: 0.694733	test: 0.658342
MAE train: 0.298990	val: 0.502131	test: 0.497254

Epoch: 192
Loss: 0.19288833066821098
RMSE train: 0.400917	val: 0.688205	test: 0.648886
MAE train: 0.315055	val: 0.493355	test: 0.494077

Epoch: 193
Loss: 0.18513672798871994
RMSE train: 0.402080	val: 0.698227	test: 0.655141
MAE train: 0.316131	val: 0.503723	test: 0.495609

Epoch: 194
Loss: 0.1912569428483645
RMSE train: 0.394340	val: 0.686625	test: 0.661771
MAE train: 0.307658	val: 0.495154	test: 0.497666

Epoch: 195
Loss: 0.18587093179424605
RMSE train: 0.377031	val: 0.687942	test: 0.656899
MAE train: 0.293634	val: 0.490984	test: 0.496803

Epoch: 196
Loss: 0.18873991817235947
RMSE train: 0.395913	val: 0.692297	test: 0.662128
MAE train: 0.309565	val: 0.492885	test: 0.504606

Epoch: 197
Loss: 0.1861288733780384
RMSE train: 0.401656	val: 0.684937	test: 0.660594
MAE train: 0.313942	val: 0.487924	test: 0.503198

Epoch: 198
Loss: 0.18117808302243552
RMSE train: 0.393341	val: 0.692409	test: 0.657504
MAE train: 0.307892	val: 0.492507	test: 0.500761

Epoch: 199
Loss: 0.1851655182739099
RMSE train: 0.392086	val: 0.687684	test: 0.661425
MAE train: 0.306106	val: 0.489941	test: 0.502669

Epoch: 200
Loss: 0.18048851564526558
RMSE train: 0.396837	val: 0.695196	test: 0.669366
MAE train: 0.309845	val: 0.499005	test: 0.504226

Epoch: 201
Loss: 0.1737848035991192
RMSE train: 0.386746	val: 0.685132	test: 0.651024
MAE train: 0.300832	val: 0.481804	test: 0.491630

Epoch: 202
Loss: 0.17940181121230125
RMSE train: 0.388480	val: 0.701292	test: 0.657052
MAE train: 0.303080	val: 0.493710	test: 0.494276

Epoch: 203
Loss: 0.1880092571179072
RMSE train: 0.403583	val: 0.691067	test: 0.644322
MAE train: 0.317964	val: 0.489218	test: 0.489987

Epoch: 204
Loss: 0.18412528187036514
RMSE train: 0.386847	val: 0.686779	test: 0.655635
MAE train: 0.302944	val: 0.487708	test: 0.501253

Epoch: 205
Loss: 0.18003802994887033
RMSE train: 0.382799	val: 0.676398	test: 0.651210
MAE train: 0.298875	val: 0.486998	test: 0.493437

Epoch: 206
Loss: 0.1778405917187532
RMSE train: 0.385479	val: 0.680830	test: 0.657493
MAE train: 0.302970	val: 0.480359	test: 0.502690

Epoch: 207
Loss: 0.17714864139755568
RMSE train: 0.385978	val: 0.678789	test: 0.653573
MAE train: 0.303890	val: 0.488117	test: 0.496077

Epoch: 208
Loss: 0.18368577832976976
RMSE train: 0.386614	val: 0.681094	test: 0.659104
MAE train: 0.305702	val: 0.482694	test: 0.501205

Epoch: 209
Loss: 0.17895847434798876
RMSE train: 0.382726	val: 0.683397	test: 0.650982
MAE train: 0.302010	val: 0.497250	test: 0.496854

Early stopping
Best (RMSE):	 train: 0.395515	val: 0.666307	test: 0.645619
Best (MAE):	 train: 0.310415	val: 0.475629	test: 0.493547


Epoch: 144
Loss: 0.2639873283249991
RMSE train: 0.484150	val: 0.715706	test: 0.662674
MAE train: 0.376378	val: 0.545093	test: 0.495173

Epoch: 145
Loss: 0.25411044167620794
RMSE train: 0.461449	val: 0.728125	test: 0.620115
MAE train: 0.357841	val: 0.546580	test: 0.466664

Epoch: 146
Loss: 0.259912728198937
RMSE train: 0.449403	val: 0.729059	test: 0.623917
MAE train: 0.349720	val: 0.538866	test: 0.467960

Epoch: 147
Loss: 0.2628205216356686
RMSE train: 0.453902	val: 0.714992	test: 0.635062
MAE train: 0.351863	val: 0.531888	test: 0.472616

Epoch: 148
Loss: 0.2812438447560583
RMSE train: 0.469393	val: 0.725053	test: 0.641497
MAE train: 0.366435	val: 0.552916	test: 0.480033

Epoch: 149
Loss: 0.2987465922321592
RMSE train: 0.513792	val: 0.766955	test: 0.665974
MAE train: 0.399516	val: 0.578740	test: 0.513551

Epoch: 150
Loss: 0.2741749180214746
RMSE train: 0.455192	val: 0.721181	test: 0.628844
MAE train: 0.354412	val: 0.530424	test: 0.472926

Epoch: 151
Loss: 0.26258725034339087
RMSE train: 0.453891	val: 0.716411	test: 0.635036
MAE train: 0.352052	val: 0.534345	test: 0.483812

Epoch: 152
Loss: 0.25614868317331585
RMSE train: 0.440656	val: 0.702273	test: 0.625999
MAE train: 0.342128	val: 0.522754	test: 0.465087

Epoch: 153
Loss: 0.26696585012333734
RMSE train: 0.463453	val: 0.718729	test: 0.640565
MAE train: 0.360931	val: 0.543360	test: 0.478658

Epoch: 154
Loss: 0.26531025022268295
RMSE train: 0.474882	val: 0.741103	test: 0.664557
MAE train: 0.369727	val: 0.548907	test: 0.496626

Epoch: 155
Loss: 0.26191927386181696
RMSE train: 0.458258	val: 0.725740	test: 0.641278
MAE train: 0.356639	val: 0.528314	test: 0.477395

Epoch: 156
Loss: 0.24716358206101827
RMSE train: 0.458343	val: 0.721561	test: 0.632324
MAE train: 0.358368	val: 0.534083	test: 0.469765

Epoch: 157
Loss: 0.24553574728114264
RMSE train: 0.443543	val: 0.734424	test: 0.628802
MAE train: 0.343471	val: 0.538229	test: 0.464798

Epoch: 158
Loss: 0.2498537940638406
RMSE train: 0.463909	val: 0.731044	test: 0.637025
MAE train: 0.362893	val: 0.544580	test: 0.475192

Epoch: 159
Loss: 0.24576798187834875
RMSE train: 0.443063	val: 0.719577	test: 0.629902
MAE train: 0.343461	val: 0.536331	test: 0.469587

Epoch: 160
Loss: 0.23897423595190048
RMSE train: 0.464376	val: 0.728857	test: 0.637141
MAE train: 0.364781	val: 0.550206	test: 0.488671

Epoch: 161
Loss: 0.2357438270534788
RMSE train: 0.477776	val: 0.748978	test: 0.656630
MAE train: 0.373388	val: 0.552796	test: 0.497038

Epoch: 162
Loss: 0.24128394999674388
RMSE train: 0.446818	val: 0.714259	test: 0.633046
MAE train: 0.347285	val: 0.527672	test: 0.474442

Epoch: 163
Loss: 0.2427238098212651
RMSE train: 0.441098	val: 0.727633	test: 0.649275
MAE train: 0.342024	val: 0.532375	test: 0.482294

Epoch: 164
Loss: 0.23943507139171874
RMSE train: 0.440808	val: 0.728740	test: 0.621842
MAE train: 0.343149	val: 0.534682	test: 0.468937

Epoch: 165
Loss: 0.2304794437118939
RMSE train: 0.455549	val: 0.733637	test: 0.654417
MAE train: 0.355733	val: 0.545403	test: 0.490066

Epoch: 166
Loss: 0.25286715158394407
RMSE train: 0.446213	val: 0.726353	test: 0.628868
MAE train: 0.348874	val: 0.538238	test: 0.469509

Epoch: 167
Loss: 0.23211681417056493
RMSE train: 0.468357	val: 0.729090	test: 0.656677
MAE train: 0.362420	val: 0.539354	test: 0.487517

Epoch: 168
Loss: 0.23787946892636164
RMSE train: 0.456031	val: 0.738882	test: 0.609033
MAE train: 0.356255	val: 0.533952	test: 0.462500

Epoch: 169
Loss: 0.2398326780114855
RMSE train: 0.476320	val: 0.752060	test: 0.664726
MAE train: 0.369112	val: 0.550458	test: 0.498290

Epoch: 170
Loss: 0.23990474960633687
RMSE train: 0.473616	val: 0.733828	test: 0.661205
MAE train: 0.372926	val: 0.550275	test: 0.492592

Epoch: 171
Loss: 0.23258920865399496
RMSE train: 0.437388	val: 0.713114	test: 0.611087
MAE train: 0.340089	val: 0.525630	test: 0.457045

Epoch: 172
Loss: 0.23001413473061152
RMSE train: 0.446597	val: 0.734936	test: 0.631926
MAE train: 0.346594	val: 0.536757	test: 0.480249

Epoch: 173
Loss: 0.2365634920341628
RMSE train: 0.437111	val: 0.725651	test: 0.614340
MAE train: 0.339729	val: 0.537725	test: 0.466858

Epoch: 174
Loss: 0.23725516136203492
RMSE train: 0.433055	val: 0.711024	test: 0.621085
MAE train: 0.335472	val: 0.527459	test: 0.464482

Epoch: 175
Loss: 0.23938051824058806
RMSE train: 0.440940	val: 0.708844	test: 0.614239
MAE train: 0.343087	val: 0.528669	test: 0.459157

Epoch: 176
Loss: 0.2200251423886844
RMSE train: 0.449630	val: 0.751174	test: 0.635534
MAE train: 0.348924	val: 0.544474	test: 0.477640

Epoch: 177
Loss: 0.23203843938452856
RMSE train: 0.426739	val: 0.723397	test: 0.615335
MAE train: 0.330552	val: 0.528887	test: 0.460179

Epoch: 178
Loss: 0.23054092803171702
RMSE train: 0.423729	val: 0.726854	test: 0.610018
MAE train: 0.328356	val: 0.528275	test: 0.459653

Epoch: 179
Loss: 0.22716172252382552
RMSE train: 0.445483	val: 0.745944	test: 0.630646
MAE train: 0.347191	val: 0.532986	test: 0.472221

Epoch: 180
Loss: 0.23325403034687042
RMSE train: 0.438922	val: 0.718233	test: 0.611764
MAE train: 0.342777	val: 0.529766	test: 0.461181

Epoch: 181
Loss: 0.21608903791223252
RMSE train: 0.424277	val: 0.734235	test: 0.622943
MAE train: 0.329214	val: 0.527051	test: 0.463521

Epoch: 182
Loss: 0.2308760370526995
RMSE train: 0.436547	val: 0.702198	test: 0.616494
MAE train: 0.339423	val: 0.525767	test: 0.460505

Epoch: 183
Loss: 0.2249994852713176
RMSE train: 0.446998	val: 0.725619	test: 0.637116
MAE train: 0.348249	val: 0.531363	test: 0.481157

Epoch: 184
Loss: 0.22455681966883795
RMSE train: 0.456139	val: 0.736983	test: 0.641643
MAE train: 0.355143	val: 0.540113	test: 0.483477

Epoch: 185
Loss: 0.2339095515864236
RMSE train: 0.433537	val: 0.712457	test: 0.616564
MAE train: 0.338787	val: 0.535837	test: 0.458941

Epoch: 186
Loss: 0.2372439961348261
RMSE train: 0.410554	val: 0.718807	test: 0.620728
MAE train: 0.318224	val: 0.523095	test: 0.457113

Epoch: 187
Loss: 0.22390591885362351
RMSE train: 0.437213	val: 0.711992	test: 0.619081
MAE train: 0.341224	val: 0.530009	test: 0.460801

Epoch: 188
Loss: 0.22317421436309814
RMSE train: 0.431764	val: 0.724411	test: 0.623389
MAE train: 0.337898	val: 0.535898	test: 0.471367

Epoch: 189
Loss: 0.21293372235127858
RMSE train: 0.424728	val: 0.733068	test: 0.614785
MAE train: 0.331207	val: 0.543194	test: 0.462595

Epoch: 190
Loss: 0.22054322276796615
RMSE train: 0.426982	val: 0.745944	test: 0.611647
MAE train: 0.331615	val: 0.539662	test: 0.455168

Epoch: 191
Loss: 0.22458795670952117
RMSE train: 0.418960	val: 0.712202	test: 0.606305
MAE train: 0.324673	val: 0.528957	test: 0.452883

Epoch: 192
Loss: 0.22016795511756623
RMSE train: 0.423350	val: 0.722284	test: 0.630457
MAE train: 0.327965	val: 0.531254	test: 0.471852

Epoch: 193
Loss: 0.21417823008128575
RMSE train: 0.430183	val: 0.712316	test: 0.631310
MAE train: 0.335537	val: 0.523811	test: 0.465691

Epoch: 194
Loss: 0.21641009939568384
RMSE train: 0.417896	val: 0.721148	test: 0.613409
MAE train: 0.325674	val: 0.523042	test: 0.459072

Epoch: 195
Loss: 0.21695138726915633
RMSE train: 0.432660	val: 0.739862	test: 0.632137
MAE train: 0.336489	val: 0.534555	test: 0.474263

Epoch: 196
Loss: 0.20979676289217813
RMSE train: 0.395830	val: 0.734434	test: 0.605689
MAE train: 0.309248	val: 0.524058	test: 0.455615

Epoch: 197
Loss: 0.2156947680882045
RMSE train: 0.418482	val: 0.724123	test: 0.607568
MAE train: 0.325840	val: 0.532752	test: 0.455641

Epoch: 198
Loss: 0.22561544924974442
RMSE train: 0.433226	val: 0.732659	test: 0.622910
MAE train: 0.339308	val: 0.540450	test: 0.469445

Epoch: 199
Loss: 0.22661150033984864
RMSE train: 0.429554	val: 0.737759	test: 0.635177
MAE train: 0.334279	val: 0.532093	test: 0.476971

Epoch: 200
Loss: 0.23028961781944549
RMSE train: 0.436191	val: 0.743507	test: 0.617347
MAE train: 0.342066	val: 0.537554	test: 0.464396

Epoch: 201
Loss: 0.21508336812257767
RMSE train: 0.428047	val: 0.733291	test: 0.640377
MAE train: 0.332742	val: 0.534832	test: 0.475407

Epoch: 202
Loss: 0.2312243314726012
RMSE train: 0.420400	val: 0.714590	test: 0.626666
MAE train: 0.329098	val: 0.523022	test: 0.472685

Epoch: 203
Loss: 0.22096383890935353
RMSE train: 0.432929	val: 0.751432	test: 0.637478
MAE train: 0.335921	val: 0.543670	test: 0.476094

Epoch: 144
Loss: 0.25627498009375166
RMSE train: 0.485256	val: 0.724884	test: 0.641337
MAE train: 0.378316	val: 0.536139	test: 0.477736

Epoch: 145
Loss: 0.24420054576226644
RMSE train: 0.461689	val: 0.691297	test: 0.619950
MAE train: 0.357333	val: 0.519983	test: 0.465780

Epoch: 146
Loss: 0.2406862314258303
RMSE train: 0.456275	val: 0.710374	test: 0.627164
MAE train: 0.354627	val: 0.535854	test: 0.472295

Epoch: 147
Loss: 0.24183640416179383
RMSE train: 0.468365	val: 0.726431	test: 0.627663
MAE train: 0.364864	val: 0.537326	test: 0.467233

Epoch: 148
Loss: 0.24584027166877473
RMSE train: 0.451634	val: 0.715037	test: 0.621974
MAE train: 0.350149	val: 0.531492	test: 0.461149

Epoch: 149
Loss: 0.23299991658755712
RMSE train: 0.475658	val: 0.717394	test: 0.648383
MAE train: 0.370105	val: 0.541998	test: 0.482500

Epoch: 150
Loss: 0.24351542549473898
RMSE train: 0.450600	val: 0.726417	test: 0.643491
MAE train: 0.350199	val: 0.548081	test: 0.483463

Epoch: 151
Loss: 0.24392078391143254
RMSE train: 0.469618	val: 0.755025	test: 0.665012
MAE train: 0.364344	val: 0.555236	test: 0.492585

Epoch: 152
Loss: 0.23971819345440185
RMSE train: 0.476817	val: 0.726460	test: 0.651991
MAE train: 0.370998	val: 0.542207	test: 0.483514

Epoch: 153
Loss: 0.2390686571598053
RMSE train: 0.440098	val: 0.727502	test: 0.626631
MAE train: 0.340862	val: 0.523546	test: 0.466825

Epoch: 154
Loss: 0.25247980867113384
RMSE train: 0.460133	val: 0.729251	test: 0.639587
MAE train: 0.357687	val: 0.537926	test: 0.478237

Epoch: 155
Loss: 0.24117999843188695
RMSE train: 0.482930	val: 0.749367	test: 0.656432
MAE train: 0.376478	val: 0.557715	test: 0.492070

Epoch: 156
Loss: 0.23320246913603374
RMSE train: 0.483829	val: 0.734658	test: 0.657693
MAE train: 0.375462	val: 0.549292	test: 0.494574

Epoch: 157
Loss: 0.23570935747453145
RMSE train: 0.450416	val: 0.722198	test: 0.643753
MAE train: 0.348292	val: 0.537745	test: 0.481866

Epoch: 158
Loss: 0.26310585013457705
RMSE train: 0.441759	val: 0.727102	test: 0.623324
MAE train: 0.346123	val: 0.531623	test: 0.464519

Epoch: 159
Loss: 0.24767536457095826
RMSE train: 0.452891	val: 0.731627	test: 0.631251
MAE train: 0.352841	val: 0.545128	test: 0.467615

Epoch: 160
Loss: 0.24224001382078444
RMSE train: 0.437331	val: 0.718751	test: 0.616755
MAE train: 0.340573	val: 0.534430	test: 0.457815

Epoch: 161
Loss: 0.23873245290347508
RMSE train: 0.433600	val: 0.705452	test: 0.610050
MAE train: 0.337764	val: 0.523370	test: 0.457542

Epoch: 162
Loss: 0.2480349913239479
RMSE train: 0.444348	val: 0.720798	test: 0.627350
MAE train: 0.345843	val: 0.544002	test: 0.467740

Epoch: 163
Loss: 0.22887731449944632
RMSE train: 0.434685	val: 0.703833	test: 0.618265
MAE train: 0.338844	val: 0.522909	test: 0.461071

Epoch: 164
Loss: 0.24276043261800492
RMSE train: 0.467931	val: 0.727705	test: 0.658431
MAE train: 0.363730	val: 0.549095	test: 0.496336

Epoch: 165
Loss: 0.23414249931062972
RMSE train: 0.459701	val: 0.714852	test: 0.623850
MAE train: 0.358952	val: 0.533895	test: 0.469796

Epoch: 166
Loss: 0.24375424640519278
RMSE train: 0.457508	val: 0.726612	test: 0.638929
MAE train: 0.354426	val: 0.538686	test: 0.471948

Epoch: 167
Loss: 0.2434607297182083
RMSE train: 0.457797	val: 0.714005	test: 0.642655
MAE train: 0.357292	val: 0.537544	test: 0.479396

Epoch: 168
Loss: 0.22607340876545226
RMSE train: 0.439985	val: 0.712200	test: 0.631199
MAE train: 0.338840	val: 0.528589	test: 0.468943

Epoch: 169
Loss: 0.2396393120288849
RMSE train: 0.453542	val: 0.706759	test: 0.633787
MAE train: 0.353983	val: 0.526982	test: 0.477449

Epoch: 170
Loss: 0.25132404799972263
RMSE train: 0.445241	val: 0.716513	test: 0.624180
MAE train: 0.341786	val: 0.531669	test: 0.463035

Epoch: 171
Loss: 0.23429445390190398
RMSE train: 0.442174	val: 0.707970	test: 0.621935
MAE train: 0.343199	val: 0.522807	test: 0.468896

Epoch: 172
Loss: 0.2451958262494632
RMSE train: 0.449143	val: 0.713982	test: 0.623938
MAE train: 0.346282	val: 0.537070	test: 0.466826

Epoch: 173
Loss: 0.24261948146990367
RMSE train: 0.456392	val: 0.726041	test: 0.627110
MAE train: 0.351683	val: 0.536213	test: 0.471757

Epoch: 174
Loss: 0.23694187934909547
RMSE train: 0.433669	val: 0.714392	test: 0.623509
MAE train: 0.336898	val: 0.533626	test: 0.469520

Epoch: 175
Loss: 0.21955965140036174
RMSE train: 0.448783	val: 0.712208	test: 0.634504
MAE train: 0.347131	val: 0.539213	test: 0.474144

Epoch: 176
Loss: 0.23539924302271434
RMSE train: 0.439739	val: 0.714074	test: 0.632793
MAE train: 0.338896	val: 0.528790	test: 0.474714

Epoch: 177
Loss: 0.22185842373541423
RMSE train: 0.429254	val: 0.725615	test: 0.625457
MAE train: 0.331081	val: 0.534678	test: 0.462584

Epoch: 178
Loss: 0.23213799723557063
RMSE train: 0.441608	val: 0.708053	test: 0.636714
MAE train: 0.338545	val: 0.531673	test: 0.473310

Epoch: 179
Loss: 0.22347712091037206
RMSE train: 0.435508	val: 0.692868	test: 0.623849
MAE train: 0.338128	val: 0.523104	test: 0.465072

Epoch: 180
Loss: 0.23382060761962617
RMSE train: 0.419162	val: 0.688921	test: 0.630663
MAE train: 0.322512	val: 0.528983	test: 0.463109

Epoch: 181
Loss: 0.22472066432237625
RMSE train: 0.442157	val: 0.724978	test: 0.627840
MAE train: 0.347269	val: 0.536791	test: 0.460998

Epoch: 182
Loss: 0.23386939082826888
RMSE train: 0.419407	val: 0.705242	test: 0.614020
MAE train: 0.322327	val: 0.527221	test: 0.452554

Epoch: 183
Loss: 0.22899938694068364
RMSE train: 0.425652	val: 0.717591	test: 0.611273
MAE train: 0.331322	val: 0.534841	test: 0.457656

Epoch: 184
Loss: 0.22433180149112428
RMSE train: 0.425110	val: 0.698386	test: 0.618603
MAE train: 0.331302	val: 0.527568	test: 0.456888

Epoch: 185
Loss: 0.22516644107443945
RMSE train: 0.452083	val: 0.703388	test: 0.649078
MAE train: 0.351114	val: 0.536365	test: 0.486512

Epoch: 186
Loss: 0.2377117265548025
RMSE train: 0.434715	val: 0.695960	test: 0.614914
MAE train: 0.338004	val: 0.534078	test: 0.467455

Epoch: 187
Loss: 0.21476095701966966
RMSE train: 0.418164	val: 0.699569	test: 0.606750
MAE train: 0.324558	val: 0.525217	test: 0.460519

Epoch: 188
Loss: 0.21687081456184387
RMSE train: 0.417296	val: 0.699493	test: 0.617850
MAE train: 0.322139	val: 0.528552	test: 0.463164

Epoch: 189
Loss: 0.21009179098265512
RMSE train: 0.432174	val: 0.687352	test: 0.611428
MAE train: 0.334250	val: 0.520484	test: 0.460838

Epoch: 190
Loss: 0.21564881290708268
RMSE train: 0.415990	val: 0.688799	test: 0.619610
MAE train: 0.322858	val: 0.521089	test: 0.458742

Epoch: 191
Loss: 0.21278173050710134
RMSE train: 0.425871	val: 0.690633	test: 0.605301
MAE train: 0.330353	val: 0.515527	test: 0.450331

Epoch: 192
Loss: 0.21262878605297633
RMSE train: 0.432589	val: 0.711599	test: 0.622367
MAE train: 0.335227	val: 0.529906	test: 0.473391

Epoch: 193
Loss: 0.2260180903332574
RMSE train: 0.417932	val: 0.695800	test: 0.632408
MAE train: 0.323103	val: 0.529819	test: 0.474156

Epoch: 194
Loss: 0.21745087951421738
RMSE train: 0.435746	val: 0.720726	test: 0.613760
MAE train: 0.338470	val: 0.534493	test: 0.457662

Epoch: 195
Loss: 0.21378014236688614
RMSE train: 0.422498	val: 0.707938	test: 0.631903
MAE train: 0.327182	val: 0.535366	test: 0.476471

Epoch: 196
Loss: 0.20974829367228917
RMSE train: 0.414790	val: 0.713371	test: 0.605063
MAE train: 0.322361	val: 0.529956	test: 0.454839

Epoch: 197
Loss: 0.21096564935786383
RMSE train: 0.405847	val: 0.703628	test: 0.626474
MAE train: 0.314686	val: 0.527674	test: 0.466133

Epoch: 198
Loss: 0.19782764677490508
RMSE train: 0.433497	val: 0.719948	test: 0.635721
MAE train: 0.338685	val: 0.536820	test: 0.477710

Epoch: 199
Loss: 0.20565875779305184
RMSE train: 0.413979	val: 0.702758	test: 0.607391
MAE train: 0.320170	val: 0.519160	test: 0.450766

Epoch: 200
Loss: 0.20112765048231399
RMSE train: 0.432953	val: 0.702905	test: 0.626731
MAE train: 0.336065	val: 0.525439	test: 0.468254

Epoch: 201
Loss: 0.2038706892303058
RMSE train: 0.402425	val: 0.682180	test: 0.623876
MAE train: 0.308304	val: 0.506505	test: 0.459041

Epoch: 202
Loss: 0.1998577660747937
RMSE train: 0.428434	val: 0.715493	test: 0.630346
MAE train: 0.332354	val: 0.535611	test: 0.469099

Epoch: 203
Loss: 0.19920911746365683
RMSE train: 0.423297	val: 0.700650	test: 0.631459
MAE train: 0.328125	val: 0.522132	test: 0.472502

Epoch: 144
Loss: 0.2700578857745443
RMSE train: 0.497277	val: 0.758854	test: 0.676900
MAE train: 0.390162	val: 0.559659	test: 0.505335

Epoch: 145
Loss: 0.26925777537482126
RMSE train: 0.481104	val: 0.724276	test: 0.630109
MAE train: 0.375135	val: 0.546534	test: 0.465748

Epoch: 146
Loss: 0.24758871644735336
RMSE train: 0.457749	val: 0.716657	test: 0.618377
MAE train: 0.358557	val: 0.529641	test: 0.453147

Epoch: 147
Loss: 0.2521419557077544
RMSE train: 0.475345	val: 0.734225	test: 0.642713
MAE train: 0.374747	val: 0.544581	test: 0.477860

Epoch: 148
Loss: 0.25010883275951656
RMSE train: 0.449319	val: 0.736882	test: 0.630733
MAE train: 0.351459	val: 0.538330	test: 0.467141

Epoch: 149
Loss: 0.2414615729025432
RMSE train: 0.472259	val: 0.752819	test: 0.650477
MAE train: 0.367520	val: 0.551773	test: 0.482114

Epoch: 150
Loss: 0.2668485769203731
RMSE train: 0.463334	val: 0.736711	test: 0.634219
MAE train: 0.361338	val: 0.546621	test: 0.469597

Epoch: 151
Loss: 0.2589473138962473
RMSE train: 0.481465	val: 0.771474	test: 0.664038
MAE train: 0.377975	val: 0.571279	test: 0.493762

Epoch: 152
Loss: 0.2608276350157602
RMSE train: 0.461968	val: 0.740109	test: 0.628982
MAE train: 0.362110	val: 0.546278	test: 0.471122

Epoch: 153
Loss: 0.2534873943243708
RMSE train: 0.466498	val: 0.754520	test: 0.638774
MAE train: 0.363416	val: 0.550493	test: 0.477577

Epoch: 154
Loss: 0.25124904619795935
RMSE train: 0.447122	val: 0.742722	test: 0.632657
MAE train: 0.348521	val: 0.539356	test: 0.472689

Epoch: 155
Loss: 0.25100001054150717
RMSE train: 0.462098	val: 0.758341	test: 0.641575
MAE train: 0.359504	val: 0.558307	test: 0.479616

Epoch: 156
Loss: 0.24888754103864943
RMSE train: 0.497927	val: 0.775237	test: 0.660992
MAE train: 0.390072	val: 0.575378	test: 0.495390

Epoch: 157
Loss: 0.23253575606005533
RMSE train: 0.438048	val: 0.746780	test: 0.614893
MAE train: 0.341194	val: 0.544336	test: 0.460279

Epoch: 158
Loss: 0.26573713230235235
RMSE train: 0.492186	val: 0.762208	test: 0.677176
MAE train: 0.383492	val: 0.564441	test: 0.511644

Epoch: 159
Loss: 0.23824203440121242
RMSE train: 0.440739	val: 0.732049	test: 0.630104
MAE train: 0.343755	val: 0.532921	test: 0.468057

Epoch: 160
Loss: 0.22957013653857367
RMSE train: 0.457187	val: 0.735919	test: 0.646348
MAE train: 0.357134	val: 0.540161	test: 0.477860

Epoch: 161
Loss: 0.23336509934493474
RMSE train: 0.448135	val: 0.752303	test: 0.621535
MAE train: 0.350953	val: 0.540713	test: 0.458575

Epoch: 162
Loss: 0.23591126714433944
RMSE train: 0.442284	val: 0.738925	test: 0.654546
MAE train: 0.342745	val: 0.543408	test: 0.478489

Epoch: 163
Loss: 0.22881039125578745
RMSE train: 0.426957	val: 0.722956	test: 0.631527
MAE train: 0.331727	val: 0.533301	test: 0.466926

Epoch: 164
Loss: 0.23556050871099746
RMSE train: 0.460631	val: 0.734343	test: 0.630527
MAE train: 0.359294	val: 0.542965	test: 0.467524

Epoch: 165
Loss: 0.24290436186960765
RMSE train: 0.454641	val: 0.752165	test: 0.646298
MAE train: 0.354225	val: 0.551209	test: 0.476327

Epoch: 166
Loss: 0.2317869727100645
RMSE train: 0.485940	val: 0.769108	test: 0.659426
MAE train: 0.381237	val: 0.570207	test: 0.499890

Epoch: 167
Loss: 0.24073024094104767
RMSE train: 0.457128	val: 0.748423	test: 0.657375
MAE train: 0.355341	val: 0.555417	test: 0.482914

Epoch: 168
Loss: 0.2407870250088828
RMSE train: 0.413089	val: 0.716469	test: 0.610134
MAE train: 0.321719	val: 0.523062	test: 0.445447

Epoch: 169
Loss: 0.22503428852983884
RMSE train: 0.454105	val: 0.735954	test: 0.629418
MAE train: 0.355836	val: 0.539815	test: 0.464656

Epoch: 170
Loss: 0.2320624930518014
RMSE train: 0.422739	val: 0.727173	test: 0.629918
MAE train: 0.328506	val: 0.531036	test: 0.461299

Epoch: 171
Loss: 0.2311864463346345
RMSE train: 0.498926	val: 0.775151	test: 0.678624
MAE train: 0.392096	val: 0.576539	test: 0.509391

Epoch: 172
Loss: 0.22891679725476674
RMSE train: 0.456127	val: 0.761699	test: 0.640319
MAE train: 0.357256	val: 0.548269	test: 0.477448

Epoch: 173
Loss: 0.22295729390212468
RMSE train: 0.435463	val: 0.732896	test: 0.626218
MAE train: 0.340199	val: 0.532503	test: 0.472169

Epoch: 174
Loss: 0.24307082487004145
RMSE train: 0.431083	val: 0.723204	test: 0.624925
MAE train: 0.336223	val: 0.530387	test: 0.466368

Epoch: 175
Loss: 0.21347064737762725
RMSE train: 0.419063	val: 0.712819	test: 0.616767
MAE train: 0.326468	val: 0.515180	test: 0.450392

Epoch: 176
Loss: 0.22157196487699235
RMSE train: 0.438858	val: 0.731539	test: 0.619861
MAE train: 0.343361	val: 0.527542	test: 0.458251

Epoch: 177
Loss: 0.23335404800517218
RMSE train: 0.418869	val: 0.718663	test: 0.620102
MAE train: 0.326448	val: 0.530901	test: 0.454329

Epoch: 178
Loss: 0.23580848106316157
RMSE train: 0.436434	val: 0.723702	test: 0.630447
MAE train: 0.341484	val: 0.529195	test: 0.472015

Epoch: 179
Loss: 0.2335422390273639
RMSE train: 0.442079	val: 0.738857	test: 0.635222
MAE train: 0.348366	val: 0.541833	test: 0.472376

Epoch: 180
Loss: 0.2278098857828549
RMSE train: 0.457880	val: 0.748449	test: 0.636495
MAE train: 0.360273	val: 0.549473	test: 0.471962

Epoch: 181
Loss: 0.22803016645567759
RMSE train: 0.454810	val: 0.749604	test: 0.647810
MAE train: 0.356594	val: 0.548416	test: 0.482464

Epoch: 182
Loss: 0.2165722953421729
RMSE train: 0.460399	val: 0.752504	test: 0.648514
MAE train: 0.360871	val: 0.548974	test: 0.481718

Epoch: 183
Loss: 0.21421383853469575
RMSE train: 0.447588	val: 0.748214	test: 0.637601
MAE train: 0.352004	val: 0.542981	test: 0.479372

Epoch: 184
Loss: 0.21114573095526015
RMSE train: 0.428816	val: 0.733100	test: 0.621580
MAE train: 0.334994	val: 0.535346	test: 0.459446

Epoch: 185
Loss: 0.21536843265805924
RMSE train: 0.429149	val: 0.741219	test: 0.625363
MAE train: 0.336273	val: 0.535910	test: 0.460905

Epoch: 186
Loss: 0.22733405551740102
RMSE train: 0.414780	val: 0.733289	test: 0.623782
MAE train: 0.324879	val: 0.531264	test: 0.456279

Epoch: 187
Loss: 0.22264939972332545
RMSE train: 0.426431	val: 0.732312	test: 0.628223
MAE train: 0.332560	val: 0.531048	test: 0.461235

Epoch: 188
Loss: 0.2120916715690068
RMSE train: 0.426646	val: 0.735296	test: 0.615239
MAE train: 0.334267	val: 0.539690	test: 0.456311

Epoch: 189
Loss: 0.20662028236048563
RMSE train: 0.405304	val: 0.726002	test: 0.612226
MAE train: 0.316425	val: 0.521448	test: 0.446331

Epoch: 190
Loss: 0.21290347938026702
RMSE train: 0.470541	val: 0.757981	test: 0.663805
MAE train: 0.372187	val: 0.561130	test: 0.498686

Epoch: 191
Loss: 0.2110503273350852
RMSE train: 0.421942	val: 0.725655	test: 0.622729
MAE train: 0.330781	val: 0.526738	test: 0.460476

Epoch: 192
Loss: 0.21674098074436188
RMSE train: 0.399202	val: 0.723441	test: 0.600021
MAE train: 0.311686	val: 0.517104	test: 0.442021

Epoch: 193
Loss: 0.2127293742128781
RMSE train: 0.425385	val: 0.734043	test: 0.611226
MAE train: 0.333608	val: 0.527848	test: 0.455443

Epoch: 194
Loss: 0.21811340749263763
RMSE train: 0.413662	val: 0.741080	test: 0.628725
MAE train: 0.323702	val: 0.533872	test: 0.462764

Epoch: 195
Loss: 0.22296458376305445
RMSE train: 0.407055	val: 0.733182	test: 0.621069
MAE train: 0.319974	val: 0.523839	test: 0.452658

Epoch: 196
Loss: 0.2215443594115121
RMSE train: 0.442912	val: 0.741536	test: 0.625437
MAE train: 0.348007	val: 0.541688	test: 0.466191

Epoch: 197
Loss: 0.20711259437458857
RMSE train: 0.396535	val: 0.712598	test: 0.599163
MAE train: 0.309624	val: 0.516560	test: 0.439687

Epoch: 198
Loss: 0.204846196940967
RMSE train: 0.444577	val: 0.757837	test: 0.656143
MAE train: 0.347966	val: 0.554959	test: 0.489887

Epoch: 199
Loss: 0.21049175092152186
RMSE train: 0.410428	val: 0.743125	test: 0.612729
MAE train: 0.319769	val: 0.532348	test: 0.452770

Epoch: 200
Loss: 0.20261543456997191
RMSE train: 0.425782	val: 0.756033	test: 0.626346
MAE train: 0.332399	val: 0.542685	test: 0.463383

Epoch: 201
Loss: 0.24887900905949728
RMSE train: 0.422676	val: 0.753278	test: 0.615982
MAE train: 0.331228	val: 0.541680	test: 0.456671

Epoch: 202
Loss: 0.225537648158414
RMSE train: 0.454692	val: 0.774671	test: 0.660395
MAE train: 0.357894	val: 0.557549	test: 0.493166

Epoch: 203
Loss: 0.22887845124517167
RMSE train: 0.467814	val: 0.768119	test: 0.647615
MAE train: 0.368837	val: 0.556457	test: 0.488684

Epoch: 204
Loss: 0.18619039903084436
RMSE train: 0.367611	val: 0.689981	test: 0.647929
MAE train: 0.285556	val: 0.490665	test: 0.486943

Epoch: 205
Loss: 0.1877159650127093
RMSE train: 0.400326	val: 0.702619	test: 0.655742
MAE train: 0.314556	val: 0.506731	test: 0.501088

Epoch: 206
Loss: 0.1959647387266159
RMSE train: 0.395866	val: 0.700102	test: 0.659875
MAE train: 0.309815	val: 0.498407	test: 0.494891

Epoch: 207
Loss: 0.18208159630497298
RMSE train: 0.414551	val: 0.724025	test: 0.659879
MAE train: 0.322166	val: 0.502202	test: 0.498725

Epoch: 208
Loss: 0.18463095525900522
RMSE train: 0.387023	val: 0.685193	test: 0.652966
MAE train: 0.302559	val: 0.487035	test: 0.492975

Epoch: 209
Loss: 0.1759431759516398
RMSE train: 0.380650	val: 0.692606	test: 0.644854
MAE train: 0.296424	val: 0.492363	test: 0.485540

Epoch: 210
Loss: 0.17994487782319388
RMSE train: 0.397172	val: 0.716788	test: 0.676045
MAE train: 0.307976	val: 0.514135	test: 0.510835

Epoch: 211
Loss: 0.1796971596777439
RMSE train: 0.374500	val: 0.696031	test: 0.638909
MAE train: 0.291567	val: 0.493094	test: 0.485044

Epoch: 212
Loss: 0.17578022430340448
RMSE train: 0.395816	val: 0.698060	test: 0.663047
MAE train: 0.308896	val: 0.496209	test: 0.504355

Epoch: 213
Loss: 0.1906801164150238
RMSE train: 0.388454	val: 0.691757	test: 0.656774
MAE train: 0.305416	val: 0.497409	test: 0.504287

Epoch: 214
Loss: 0.18577707807223
RMSE train: 0.377308	val: 0.706380	test: 0.662709
MAE train: 0.295299	val: 0.500355	test: 0.503037

Epoch: 215
Loss: 0.18595920751492181
RMSE train: 0.409109	val: 0.712389	test: 0.680636
MAE train: 0.320133	val: 0.505954	test: 0.511822

Epoch: 216
Loss: 0.17012602711717287
RMSE train: 0.391823	val: 0.698571	test: 0.653630
MAE train: 0.307894	val: 0.493620	test: 0.492755

Epoch: 217
Loss: 0.16903917118906975
RMSE train: 0.372500	val: 0.679721	test: 0.639393
MAE train: 0.289117	val: 0.473950	test: 0.478881

Epoch: 218
Loss: 0.1733260713517666
RMSE train: 0.380213	val: 0.698082	test: 0.643456
MAE train: 0.296017	val: 0.489771	test: 0.485261

Epoch: 219
Loss: 0.17036725829044977
RMSE train: 0.382108	val: 0.694574	test: 0.658252
MAE train: 0.296648	val: 0.492612	test: 0.498348

Epoch: 220
Loss: 0.1686931699514389
RMSE train: 0.373692	val: 0.691088	test: 0.648122
MAE train: 0.291150	val: 0.495421	test: 0.495088

Epoch: 221
Loss: 0.1728300116956234
RMSE train: 0.372065	val: 0.688254	test: 0.646466
MAE train: 0.290966	val: 0.484405	test: 0.491708

Epoch: 222
Loss: 0.17938136185208955
RMSE train: 0.387997	val: 0.705137	test: 0.667800
MAE train: 0.300382	val: 0.500456	test: 0.504010

Epoch: 223
Loss: 0.17118248591820398
RMSE train: 0.373082	val: 0.687772	test: 0.648136
MAE train: 0.292434	val: 0.484774	test: 0.487865

Epoch: 224
Loss: 0.16705428684751192
RMSE train: 0.397571	val: 0.691288	test: 0.668943
MAE train: 0.310428	val: 0.489615	test: 0.508988

Epoch: 225
Loss: 0.1691552313665549
RMSE train: 0.378006	val: 0.695364	test: 0.651788
MAE train: 0.296301	val: 0.492313	test: 0.496125

Epoch: 226
Loss: 0.17492119098703066
RMSE train: 0.362900	val: 0.682614	test: 0.645045
MAE train: 0.282853	val: 0.486786	test: 0.483078

Epoch: 227
Loss: 0.16883684198061624
RMSE train: 0.379415	val: 0.688751	test: 0.645138
MAE train: 0.296512	val: 0.490268	test: 0.485751

Epoch: 228
Loss: 0.16456417987744013
RMSE train: 0.367453	val: 0.695460	test: 0.649223
MAE train: 0.285795	val: 0.496996	test: 0.489630

Epoch: 229
Loss: 0.1754489429295063
RMSE train: 0.380004	val: 0.701665	test: 0.643842
MAE train: 0.297750	val: 0.487207	test: 0.489000

Epoch: 230
Loss: 0.16985111683607101
RMSE train: 0.363066	val: 0.690918	test: 0.647014
MAE train: 0.282365	val: 0.482929	test: 0.487631

Epoch: 231
Loss: 0.1716379721959432
RMSE train: 0.389786	val: 0.690231	test: 0.659782
MAE train: 0.303042	val: 0.491138	test: 0.501231

Epoch: 232
Loss: 0.16975446542104086
RMSE train: 0.391365	val: 0.714847	test: 0.668727
MAE train: 0.303565	val: 0.510471	test: 0.505683

Epoch: 233
Loss: 0.15639522423346838
RMSE train: 0.367802	val: 0.692650	test: 0.646035
MAE train: 0.286092	val: 0.493958	test: 0.490075

Epoch: 234
Loss: 0.16338103637099266
RMSE train: 0.387287	val: 0.693729	test: 0.649370
MAE train: 0.302802	val: 0.491309	test: 0.492306

Epoch: 235
Loss: 0.1720518966515859
RMSE train: 0.354550	val: 0.693392	test: 0.643780
MAE train: 0.276381	val: 0.493827	test: 0.486872

Epoch: 236
Loss: 0.16530842954913774
RMSE train: 0.371407	val: 0.689103	test: 0.655908
MAE train: 0.287594	val: 0.493537	test: 0.498446

Epoch: 237
Loss: 0.1675809510052204
RMSE train: 0.380209	val: 0.694942	test: 0.657806
MAE train: 0.296032	val: 0.496736	test: 0.499571

Epoch: 238
Loss: 0.16521630808711052
RMSE train: 0.363743	val: 0.699718	test: 0.654042
MAE train: 0.283394	val: 0.494708	test: 0.490866

Epoch: 239
Loss: 0.16129464035232863
RMSE train: 0.395707	val: 0.692766	test: 0.667001
MAE train: 0.310080	val: 0.492886	test: 0.504724

Epoch: 240
Loss: 0.1669215758641561
RMSE train: 0.382913	val: 0.716084	test: 0.670498
MAE train: 0.298988	val: 0.503668	test: 0.505914

Epoch: 241
Loss: 0.1617866431673368
RMSE train: 0.356077	val: 0.690019	test: 0.648851
MAE train: 0.275061	val: 0.490301	test: 0.490489

Epoch: 242
Loss: 0.17515763516227403
RMSE train: 0.353925	val: 0.696924	test: 0.645126
MAE train: 0.273679	val: 0.492010	test: 0.483877

Epoch: 243
Loss: 0.16280097141861916
RMSE train: 0.383671	val: 0.695906	test: 0.658462
MAE train: 0.300678	val: 0.495932	test: 0.498477

Epoch: 244
Loss: 0.16384949162602425
RMSE train: 0.363596	val: 0.688181	test: 0.653997
MAE train: 0.283758	val: 0.491346	test: 0.494587

Epoch: 245
Loss: 0.16157038882374763
RMSE train: 0.381675	val: 0.699736	test: 0.648180
MAE train: 0.298825	val: 0.491878	test: 0.491914

Epoch: 246
Loss: 0.16357101127505302
RMSE train: 0.392972	val: 0.704459	test: 0.668924
MAE train: 0.307389	val: 0.504654	test: 0.507416

Epoch: 247
Loss: 0.1503694790105025
RMSE train: 0.379525	val: 0.694068	test: 0.658025
MAE train: 0.297682	val: 0.494193	test: 0.500401

Epoch: 248
Loss: 0.15910240138570467
RMSE train: 0.368788	val: 0.692956	test: 0.651074
MAE train: 0.285470	val: 0.488785	test: 0.495301

Epoch: 249
Loss: 0.15738361018399397
RMSE train: 0.366579	val: 0.688058	test: 0.648817
MAE train: 0.285735	val: 0.489892	test: 0.491644

Epoch: 250
Loss: 0.1636078959951798
RMSE train: 0.355212	val: 0.688433	test: 0.646402
MAE train: 0.276256	val: 0.488984	test: 0.487984

Epoch: 251
Loss: 0.1691911481320858
RMSE train: 0.373844	val: 0.694295	test: 0.643906
MAE train: 0.294060	val: 0.499149	test: 0.491044

Epoch: 252
Loss: 0.15850571915507317
RMSE train: 0.381105	val: 0.697184	test: 0.664369
MAE train: 0.296979	val: 0.497497	test: 0.506898

Early stopping
Best (RMSE):	 train: 0.372500	val: 0.679721	test: 0.639393
Best (MAE):	 train: 0.289117	val: 0.473950	test: 0.478881
All runs completed.


Epoch: 204
Loss: 0.2153493623648371
RMSE train: 0.413096	val: 0.752869	test: 0.616733
MAE train: 0.322921	val: 0.538775	test: 0.463877

Epoch: 205
Loss: 0.211653294307845
RMSE train: 0.426616	val: 0.744016	test: 0.609068
MAE train: 0.332531	val: 0.533307	test: 0.460801

Epoch: 206
Loss: 0.21196735330990382
RMSE train: 0.414389	val: 0.735521	test: 0.615080
MAE train: 0.323546	val: 0.526974	test: 0.460645

Epoch: 207
Loss: 0.2023134816970144
RMSE train: 0.415747	val: 0.733487	test: 0.626009
MAE train: 0.324943	val: 0.534688	test: 0.472333

Epoch: 208
Loss: 0.22441992589405604
RMSE train: 0.422442	val: 0.718538	test: 0.611100
MAE train: 0.330844	val: 0.532333	test: 0.466734

Epoch: 209
Loss: 0.22677752801350184
RMSE train: 0.438141	val: 0.744421	test: 0.654022
MAE train: 0.341639	val: 0.545990	test: 0.494623

Epoch: 210
Loss: 0.20344412326812744
RMSE train: 0.410939	val: 0.732214	test: 0.621870
MAE train: 0.320369	val: 0.533788	test: 0.466432

Epoch: 211
Loss: 0.20678221540791647
RMSE train: 0.411978	val: 0.741690	test: 0.610798
MAE train: 0.321156	val: 0.544223	test: 0.461680

Epoch: 212
Loss: 0.1970720248562949
RMSE train: 0.419819	val: 0.739454	test: 0.636834
MAE train: 0.327397	val: 0.534242	test: 0.476379

Epoch: 213
Loss: 0.2233868326459612
RMSE train: 0.412593	val: 0.744852	test: 0.616405
MAE train: 0.321632	val: 0.532018	test: 0.469843

Epoch: 214
Loss: 0.21166897352252687
RMSE train: 0.412973	val: 0.759655	test: 0.636255
MAE train: 0.321529	val: 0.540164	test: 0.482298

Epoch: 215
Loss: 0.2048359534570149
RMSE train: 0.415628	val: 0.739722	test: 0.622571
MAE train: 0.327235	val: 0.533959	test: 0.470491

Epoch: 216
Loss: 0.20780023400272643
RMSE train: 0.412523	val: 0.752826	test: 0.635440
MAE train: 0.319621	val: 0.543582	test: 0.476414

Epoch: 217
Loss: 0.2107294574379921
RMSE train: 0.429284	val: 0.743345	test: 0.611864
MAE train: 0.334684	val: 0.529817	test: 0.464524

Early stopping
Best (RMSE):	 train: 0.436547	val: 0.702198	test: 0.616494
Best (MAE):	 train: 0.339423	val: 0.525767	test: 0.460505


Epoch: 204
Loss: 0.2310723907181195
RMSE train: 0.515034	val: 0.818284	test: 0.697142
MAE train: 0.406923	val: 0.592361	test: 0.524093

Epoch: 205
Loss: 0.21515455096960068
RMSE train: 0.416999	val: 0.718564	test: 0.616960
MAE train: 0.327283	val: 0.521481	test: 0.452552

Epoch: 206
Loss: 0.2160687276295253
RMSE train: 0.414648	val: 0.716016	test: 0.622689
MAE train: 0.324838	val: 0.521563	test: 0.457460

Epoch: 207
Loss: 0.2091025145990508
RMSE train: 0.426495	val: 0.739683	test: 0.630290
MAE train: 0.333009	val: 0.533838	test: 0.471138

Epoch: 208
Loss: 0.20659695885011128
RMSE train: 0.416395	val: 0.732961	test: 0.604386
MAE train: 0.326647	val: 0.528667	test: 0.446140

Epoch: 209
Loss: 0.21926993238074438
RMSE train: 0.439517	val: 0.746514	test: 0.631560
MAE train: 0.345366	val: 0.546721	test: 0.476192

Epoch: 210
Loss: 0.23512499353715352
RMSE train: 0.403052	val: 0.720404	test: 0.605845
MAE train: 0.315725	val: 0.521205	test: 0.442822

Epoch: 211
Loss: 0.19632713390248163
RMSE train: 0.430514	val: 0.733275	test: 0.624762
MAE train: 0.337026	val: 0.531597	test: 0.465262

Epoch: 212
Loss: 0.20141239357846125
RMSE train: 0.413027	val: 0.712981	test: 0.614621
MAE train: 0.324608	val: 0.520736	test: 0.453201

Epoch: 213
Loss: 0.19937613500016077
RMSE train: 0.444523	val: 0.745144	test: 0.644899
MAE train: 0.349542	val: 0.546720	test: 0.481169

Epoch: 214
Loss: 0.20418453642300197
RMSE train: 0.406328	val: 0.725459	test: 0.613176
MAE train: 0.316776	val: 0.525949	test: 0.450683

Epoch: 215
Loss: 0.20628939249685832
RMSE train: 0.426865	val: 0.748050	test: 0.625821
MAE train: 0.333919	val: 0.535359	test: 0.463668

Epoch: 216
Loss: 0.19598315443311418
RMSE train: 0.405323	val: 0.705494	test: 0.612725
MAE train: 0.316555	val: 0.514662	test: 0.445661

Epoch: 217
Loss: 0.19650654920509883
RMSE train: 0.421819	val: 0.734253	test: 0.624176
MAE train: 0.330323	val: 0.528428	test: 0.455992

Epoch: 218
Loss: 0.20042492129973002
RMSE train: 0.385398	val: 0.700658	test: 0.605047
MAE train: 0.299534	val: 0.500848	test: 0.436125

Epoch: 219
Loss: 0.19365579634904861
RMSE train: 0.406901	val: 0.718833	test: 0.614947
MAE train: 0.318097	val: 0.518010	test: 0.454020

Epoch: 220
Loss: 0.19530644480671203
RMSE train: 0.409843	val: 0.720796	test: 0.616504
MAE train: 0.320632	val: 0.521187	test: 0.456719

Epoch: 221
Loss: 0.204585785312312
RMSE train: 0.423395	val: 0.729793	test: 0.629783
MAE train: 0.330057	val: 0.527564	test: 0.472023

Epoch: 222
Loss: 0.19601736004863465
RMSE train: 0.394175	val: 0.701119	test: 0.601498
MAE train: 0.307897	val: 0.510127	test: 0.441480

Epoch: 223
Loss: 0.1862451508641243
RMSE train: 0.404979	val: 0.732245	test: 0.621591
MAE train: 0.316633	val: 0.530935	test: 0.459759

Epoch: 224
Loss: 0.18762244816337312
RMSE train: 0.418300	val: 0.744608	test: 0.632001
MAE train: 0.329706	val: 0.536689	test: 0.467334

Epoch: 225
Loss: 0.19072030165365764
RMSE train: 0.403474	val: 0.731443	test: 0.604811
MAE train: 0.313622	val: 0.528107	test: 0.450672

Epoch: 226
Loss: 0.20544428271906717
RMSE train: 0.389197	val: 0.722551	test: 0.614457
MAE train: 0.303503	val: 0.519001	test: 0.450398

Epoch: 227
Loss: 0.1988136289375169
RMSE train: 0.407864	val: 0.734207	test: 0.603799
MAE train: 0.318914	val: 0.527312	test: 0.449165

Epoch: 228
Loss: 0.1916963415486472
RMSE train: 0.410254	val: 0.733846	test: 0.623948
MAE train: 0.320308	val: 0.531994	test: 0.466531

Epoch: 229
Loss: 0.20315161240952356
RMSE train: 0.405465	val: 0.735638	test: 0.627765
MAE train: 0.315697	val: 0.527377	test: 0.462262

Epoch: 230
Loss: 0.1970332573567118
RMSE train: 0.398919	val: 0.744932	test: 0.623443
MAE train: 0.311697	val: 0.533275	test: 0.460931

Epoch: 231
Loss: 0.19743085971900395
RMSE train: 0.432335	val: 0.753614	test: 0.634813
MAE train: 0.341067	val: 0.543298	test: 0.475561

Epoch: 232
Loss: 0.19907958273376739
RMSE train: 0.387803	val: 0.714062	test: 0.604854
MAE train: 0.301565	val: 0.514766	test: 0.442947

Epoch: 233
Loss: 0.20275379291602544
RMSE train: 0.406838	val: 0.730872	test: 0.626667
MAE train: 0.317827	val: 0.524013	test: 0.464701

Epoch: 234
Loss: 0.19845187025410788
RMSE train: 0.407638	val: 0.728275	test: 0.619256
MAE train: 0.319737	val: 0.523167	test: 0.459294

Epoch: 235
Loss: 0.19624773412942886
RMSE train: 0.409230	val: 0.714405	test: 0.601122
MAE train: 0.318214	val: 0.525122	test: 0.453487

Epoch: 236
Loss: 0.20499953627586365
RMSE train: 0.409609	val: 0.754082	test: 0.628349
MAE train: 0.320064	val: 0.539223	test: 0.470405

Epoch: 237
Loss: 0.20692638839994157
RMSE train: 0.410683	val: 0.755570	test: 0.618585
MAE train: 0.321830	val: 0.543686	test: 0.462959

Epoch: 238
Loss: 0.19232512691191264
RMSE train: 0.386091	val: 0.733550	test: 0.607259
MAE train: 0.301787	val: 0.522630	test: 0.448735

Epoch: 239
Loss: 0.1902889119727271
RMSE train: 0.399189	val: 0.732771	test: 0.615911
MAE train: 0.312142	val: 0.523736	test: 0.461950

Epoch: 240
Loss: 0.1894525374685015
RMSE train: 0.403288	val: 0.724995	test: 0.616872
MAE train: 0.315284	val: 0.526434	test: 0.461913

Epoch: 241
Loss: 0.18523340778691427
RMSE train: 0.378659	val: 0.705287	test: 0.601466
MAE train: 0.294658	val: 0.511404	test: 0.444638

Epoch: 242
Loss: 0.1930790671280452
RMSE train: 0.394716	val: 0.726572	test: 0.619766
MAE train: 0.310056	val: 0.525407	test: 0.462539

Epoch: 243
Loss: 0.18800894064562662
RMSE train: 0.408910	val: 0.729864	test: 0.619909
MAE train: 0.320346	val: 0.535125	test: 0.470864

Epoch: 244
Loss: 0.18065510796649115
RMSE train: 0.396301	val: 0.726445	test: 0.623152
MAE train: 0.310389	val: 0.526708	test: 0.465758

Epoch: 245
Loss: 0.18397843199116842
RMSE train: 0.392560	val: 0.718226	test: 0.605142
MAE train: 0.306054	val: 0.526644	test: 0.449549

Epoch: 246
Loss: 0.19683122315577098
RMSE train: 0.389979	val: 0.718350	test: 0.600783
MAE train: 0.305647	val: 0.519804	test: 0.445972

Epoch: 247
Loss: 0.20781961934907095
RMSE train: 0.413278	val: 0.707727	test: 0.612417
MAE train: 0.323385	val: 0.519740	test: 0.451071

Epoch: 248
Loss: 0.20487430478845323
RMSE train: 0.391827	val: 0.718609	test: 0.608772
MAE train: 0.306959	val: 0.514651	test: 0.446773

Epoch: 249
Loss: 0.1855285827602659
RMSE train: 0.394229	val: 0.726536	test: 0.617359
MAE train: 0.309789	val: 0.526633	test: 0.455397

Epoch: 250
Loss: 0.1932951169354575
RMSE train: 0.378527	val: 0.728749	test: 0.619063
MAE train: 0.293665	val: 0.528371	test: 0.461337

Epoch: 251
Loss: 0.1904023289680481
RMSE train: 0.404914	val: 0.739766	test: 0.620763
MAE train: 0.317902	val: 0.527771	test: 0.462890

Epoch: 252
Loss: 0.1773324151124273
RMSE train: 0.375353	val: 0.708885	test: 0.619753
MAE train: 0.291825	val: 0.519926	test: 0.449895

Epoch: 253
Loss: 0.1837890414255006
RMSE train: 0.403486	val: 0.726303	test: 0.621844
MAE train: 0.317562	val: 0.520044	test: 0.461646

Early stopping
Best (RMSE):	 train: 0.385398	val: 0.700658	test: 0.605047
Best (MAE):	 train: 0.299534	val: 0.500848	test: 0.436125


Epoch: 204
Loss: 0.22289766264813288
RMSE train: 0.404532	val: 0.690076	test: 0.606471
MAE train: 0.315394	val: 0.517313	test: 0.455874

Epoch: 205
Loss: 0.21112418174743652
RMSE train: 0.410448	val: 0.711734	test: 0.619326
MAE train: 0.317231	val: 0.528045	test: 0.469870

Epoch: 206
Loss: 0.2049240299633571
RMSE train: 0.401572	val: 0.690400	test: 0.612795
MAE train: 0.311788	val: 0.519125	test: 0.459592

Epoch: 207
Loss: 0.21353393154484884
RMSE train: 0.414945	val: 0.701277	test: 0.605249
MAE train: 0.322224	val: 0.520677	test: 0.459851

Epoch: 208
Loss: 0.211329029074737
RMSE train: 0.419028	val: 0.699871	test: 0.617239
MAE train: 0.325346	val: 0.523129	test: 0.469799

Epoch: 209
Loss: 0.21698673601661408
RMSE train: 0.412653	val: 0.712158	test: 0.611878
MAE train: 0.320964	val: 0.531753	test: 0.462245

Epoch: 210
Loss: 0.20547146988766535
RMSE train: 0.404146	val: 0.701417	test: 0.615542
MAE train: 0.314937	val: 0.528926	test: 0.465280

Epoch: 211
Loss: 0.2112118410212653
RMSE train: 0.410982	val: 0.696598	test: 0.598166
MAE train: 0.318856	val: 0.520992	test: 0.453099

Epoch: 212
Loss: 0.2035562757934843
RMSE train: 0.406777	val: 0.705051	test: 0.604788
MAE train: 0.315059	val: 0.527227	test: 0.455764

Epoch: 213
Loss: 0.2015044391155243
RMSE train: 0.390623	val: 0.681505	test: 0.604475
MAE train: 0.303678	val: 0.509570	test: 0.448999

Epoch: 214
Loss: 0.19677309479032243
RMSE train: 0.412973	val: 0.701988	test: 0.622909
MAE train: 0.319695	val: 0.526083	test: 0.469034

Epoch: 215
Loss: 0.2050986417702266
RMSE train: 0.417012	val: 0.702630	test: 0.622115
MAE train: 0.325943	val: 0.520083	test: 0.466412

Epoch: 216
Loss: 0.2180441596678325
RMSE train: 0.397787	val: 0.687408	test: 0.619769
MAE train: 0.307158	val: 0.513863	test: 0.470655

Epoch: 217
Loss: 0.19889227194445475
RMSE train: 0.412766	val: 0.692452	test: 0.611651
MAE train: 0.318369	val: 0.512965	test: 0.457327

Epoch: 218
Loss: 0.2598778158426285
RMSE train: 0.429506	val: 0.716718	test: 0.636142
MAE train: 0.336938	val: 0.539436	test: 0.484327

Epoch: 219
Loss: 0.2349515493427004
RMSE train: 0.428493	val: 0.694557	test: 0.605401
MAE train: 0.333599	val: 0.520357	test: 0.455497

Epoch: 220
Loss: 0.22745415249041148
RMSE train: 0.425359	val: 0.709881	test: 0.606934
MAE train: 0.333945	val: 0.530354	test: 0.457636

Epoch: 221
Loss: 0.21520011020558222
RMSE train: 0.411531	val: 0.725178	test: 0.610753
MAE train: 0.319941	val: 0.532752	test: 0.459719

Epoch: 222
Loss: 0.22606247237750463
RMSE train: 0.409895	val: 0.705771	test: 0.605865
MAE train: 0.319810	val: 0.521345	test: 0.452281

Epoch: 223
Loss: 0.21550925821065903
RMSE train: 0.392788	val: 0.693529	test: 0.623426
MAE train: 0.300407	val: 0.508737	test: 0.459707

Epoch: 224
Loss: 0.21214922623974936
RMSE train: 0.421358	val: 0.707316	test: 0.595084
MAE train: 0.328155	val: 0.520110	test: 0.447282

Epoch: 225
Loss: 0.20125762586082732
RMSE train: 0.391533	val: 0.702389	test: 0.591721
MAE train: 0.303110	val: 0.518502	test: 0.445319

Epoch: 226
Loss: 0.18728491025311605
RMSE train: 0.408023	val: 0.701194	test: 0.604924
MAE train: 0.316104	val: 0.526739	test: 0.454204

Epoch: 227
Loss: 0.19591233134269714
RMSE train: 0.395966	val: 0.681793	test: 0.596914
MAE train: 0.306760	val: 0.510109	test: 0.448395

Epoch: 228
Loss: 0.19063666888645717
RMSE train: 0.394891	val: 0.700078	test: 0.604080
MAE train: 0.305295	val: 0.520685	test: 0.457352

Epoch: 229
Loss: 0.19637698467288697
RMSE train: 0.409866	val: 0.696027	test: 0.605221
MAE train: 0.318410	val: 0.517605	test: 0.454740

Epoch: 230
Loss: 0.19235986577613012
RMSE train: 0.380162	val: 0.680819	test: 0.587980
MAE train: 0.293751	val: 0.505606	test: 0.435346

Epoch: 231
Loss: 0.19487143201487406
RMSE train: 0.406233	val: 0.706603	test: 0.607529
MAE train: 0.315860	val: 0.525128	test: 0.453311

Epoch: 232
Loss: 0.1918278442961829
RMSE train: 0.375688	val: 0.682860	test: 0.608148
MAE train: 0.289844	val: 0.510160	test: 0.447117

Epoch: 233
Loss: 0.19061375196490968
RMSE train: 0.385153	val: 0.707558	test: 0.609626
MAE train: 0.299168	val: 0.518673	test: 0.453894

Epoch: 234
Loss: 0.18618657333510263
RMSE train: 0.397078	val: 0.690605	test: 0.612076
MAE train: 0.310387	val: 0.515094	test: 0.459492

Epoch: 235
Loss: 0.1803507006594113
RMSE train: 0.393094	val: 0.693052	test: 0.616784
MAE train: 0.303941	val: 0.519322	test: 0.460839

Epoch: 236
Loss: 0.18733283025877817
RMSE train: 0.387555	val: 0.692908	test: 0.626472
MAE train: 0.296080	val: 0.519408	test: 0.466938

Epoch: 237
Loss: 0.18637336364814214
RMSE train: 0.402834	val: 0.704069	test: 0.601765
MAE train: 0.314420	val: 0.525676	test: 0.448424

Epoch: 238
Loss: 0.18102323476757323
RMSE train: 0.398774	val: 0.694529	test: 0.604514
MAE train: 0.310320	val: 0.520239	test: 0.453574

Epoch: 239
Loss: 0.1991447220955576
RMSE train: 0.375946	val: 0.679788	test: 0.594231
MAE train: 0.290417	val: 0.511204	test: 0.437431

Epoch: 240
Loss: 0.19930258286850794
RMSE train: 0.403288	val: 0.696163	test: 0.611429
MAE train: 0.312624	val: 0.523575	test: 0.454200

Epoch: 241
Loss: 0.18408214300870895
RMSE train: 0.382256	val: 0.699919	test: 0.609688
MAE train: 0.297009	val: 0.520587	test: 0.452868

Epoch: 242
Loss: 0.18148719945124217
RMSE train: 0.377767	val: 0.701877	test: 0.599387
MAE train: 0.292610	val: 0.517103	test: 0.449266

Epoch: 243
Loss: 0.1760919519833156
RMSE train: 0.390749	val: 0.706291	test: 0.609643
MAE train: 0.302767	val: 0.523444	test: 0.450488

Epoch: 244
Loss: 0.18343231294836318
RMSE train: 0.362636	val: 0.696002	test: 0.605248
MAE train: 0.279685	val: 0.513291	test: 0.440821

Epoch: 245
Loss: 0.18570836527006968
RMSE train: 0.405147	val: 0.704492	test: 0.623065
MAE train: 0.314622	val: 0.524530	test: 0.461624

Epoch: 246
Loss: 0.18065872362681798
RMSE train: 0.394483	val: 0.709649	test: 0.625770
MAE train: 0.303108	val: 0.528871	test: 0.469332

Epoch: 247
Loss: 0.17453789285251073
RMSE train: 0.424141	val: 0.700767	test: 0.640549
MAE train: 0.330997	val: 0.532290	test: 0.485443

Epoch: 248
Loss: 0.19171974062919617
RMSE train: 0.401559	val: 0.691969	test: 0.616730
MAE train: 0.311694	val: 0.524588	test: 0.465672

Epoch: 249
Loss: 0.1748028780732836
RMSE train: 0.376456	val: 0.681391	test: 0.610062
MAE train: 0.292659	val: 0.504245	test: 0.450646

Epoch: 250
Loss: 0.18506019030298507
RMSE train: 0.380605	val: 0.700484	test: 0.604871
MAE train: 0.295293	val: 0.515766	test: 0.454876

Epoch: 251
Loss: 0.19004123764378683
RMSE train: 0.392264	val: 0.695340	test: 0.612920
MAE train: 0.305905	val: 0.515150	test: 0.460529

Epoch: 252
Loss: 0.18046380153724126
RMSE train: 0.388369	val: 0.699551	test: 0.618474
MAE train: 0.301936	val: 0.518670	test: 0.463493

Epoch: 253
Loss: 0.1808744360293661
RMSE train: 0.382995	val: 0.691220	test: 0.620552
MAE train: 0.297993	val: 0.520315	test: 0.462615

Epoch: 254
Loss: 0.18333713710308075
RMSE train: 0.404309	val: 0.719732	test: 0.627866
MAE train: 0.315333	val: 0.526925	test: 0.472486

Epoch: 255
Loss: 0.17874289623328618
RMSE train: 0.392952	val: 0.700968	test: 0.614513
MAE train: 0.300941	val: 0.522915	test: 0.458584

Epoch: 256
Loss: 0.19071563546146666
RMSE train: 0.383949	val: 0.692643	test: 0.607777
MAE train: 0.293534	val: 0.511359	test: 0.448055

Epoch: 257
Loss: 0.18432791105338506
RMSE train: 0.390318	val: 0.680608	test: 0.620951
MAE train: 0.300286	val: 0.518138	test: 0.456910

Epoch: 258
Loss: 0.1917430864913123
RMSE train: 0.392777	val: 0.699377	test: 0.615006
MAE train: 0.304478	val: 0.520255	test: 0.457608

Epoch: 259
Loss: 0.17960173849548614
RMSE train: 0.428902	val: 0.733941	test: 0.661505
MAE train: 0.339138	val: 0.547262	test: 0.496555

Epoch: 260
Loss: 0.17921516192810877
RMSE train: 0.368819	val: 0.693646	test: 0.599105
MAE train: 0.284842	val: 0.512297	test: 0.439385

Epoch: 261
Loss: 0.17701649772269384
RMSE train: 0.384221	val: 0.688657	test: 0.602670
MAE train: 0.299312	val: 0.511991	test: 0.446114

Epoch: 262
Loss: 0.1664473797593798
RMSE train: 0.362534	val: 0.686059	test: 0.609022
MAE train: 0.280566	val: 0.508722	test: 0.444294

Epoch: 263
Loss: 0.17593134407486236
RMSE train: 0.379253	val: 0.711347	test: 0.621403
MAE train: 0.291306	val: 0.520648	test: 0.456570

Epoch: 264
Loss: 0.17367700593812124
RMSE train: 0.373290	val: 0.710386	test: 0.609105
MAE train: 0.288765	val: 0.511373	test: 0.454055

Epoch: 265
Loss: 0.17150540437017167
RMSE train: 0.355541	val: 0.695899	test: 0.595895
MAE train: 0.272860	val: 0.509971	test: 0.440864

Epoch: 266
Loss: 0.1705475758228983
RMSE train: 0.382603	val: 0.693160	test: 0.613245
MAE train: 0.296342	val: 0.510668	test: 0.456052

Epoch: 267
Loss: 0.19735812608684813
RMSE train: 0.379815	val: 0.709725	test: 0.599007
MAE train: 0.294961	val: 0.521699	test: 0.452640

Epoch: 268
Loss: 0.18453780455248697
RMSE train: 0.382964	val: 0.714504	test: 0.652746
MAE train: 0.296018	val: 0.525786	test: 0.484052

Epoch: 269
Loss: 0.18623065096991404
RMSE train: 0.399513	val: 0.714931	test: 0.630466
MAE train: 0.311449	val: 0.521797	test: 0.470057

Epoch: 270
Loss: 0.17791789663689478
RMSE train: 0.377722	val: 0.705541	test: 0.616312
MAE train: 0.292146	val: 0.519422	test: 0.460445

Epoch: 271
Loss: 0.19617414687361037
RMSE train: 0.388445	val: 0.709386	test: 0.617034
MAE train: 0.304428	val: 0.522974	test: 0.456502

Epoch: 272
Loss: 0.19155024311372212
RMSE train: 0.400899	val: 0.711969	test: 0.600168
MAE train: 0.311679	val: 0.531593	test: 0.444246

Epoch: 273
Loss: 0.18039238559348242
RMSE train: 0.364538	val: 0.689951	test: 0.586966
MAE train: 0.281659	val: 0.507730	test: 0.434529

Epoch: 274
Loss: 0.17484725266695023
RMSE train: 0.380013	val: 0.711985	test: 0.624847
MAE train: 0.293038	val: 0.527261	test: 0.468229

Early stopping
Best (RMSE):	 train: 0.375946	val: 0.679788	test: 0.594231
Best (MAE):	 train: 0.290417	val: 0.511204	test: 0.437431
All runs completed.
