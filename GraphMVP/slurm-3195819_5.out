>>> Starting run for dataset: hiv
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running SCAFF configs_split_experiments/GraphCL/hiv/scaff/train_prop=0.6.yml on cuda:0
Running SCAFF configs_split_experiments/GraphCL/hiv/scaff/train_prop=0.7.yml on cuda:1
Running SCAFF configs_split_experiments/GraphCL/hiv/scaff/train_prop=0.8.yml on cuda:2
Starting process for seed 1: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/hiv/scaff/train_prop=0.7.yml --runseed 1 --device cuda:1
Starting process for seed 1: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/hiv/scaff/train_prop=0.8.yml --runseed 1 --device cuda:2
Starting process for seed 2: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/hiv/scaff/train_prop=0.7.yml --runseed 2 --device cuda:1
Starting process for seed 2: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/hiv/scaff/train_prop=0.8.yml --runseed 2 --device cuda:2
Starting process for seed 3: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/hiv/scaff/train_prop=0.7.yml --runseed 3 --device cuda:1
Starting process for seed 3: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/hiv/scaff/train_prop=0.8.yml --runseed 3 --device cuda:2
Starting process for seed 1: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/hiv/scaff/train_prop=0.6.yml --runseed 1 --device cuda:0
Starting process for seed 2: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/hiv/scaff/train_prop=0.6.yml --runseed 2 --device cuda:0
Starting process for seed 3: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/hiv/scaff/train_prop=0.6.yml --runseed 3 --device cuda:0
[14:43:28] WARNING: not removing hydrogen atom without neighbors
[14:43:28] WARNING: not removing hydrogen atom without neighbors
[14:43:28] WARNING: not removing hydrogen atom without neighbors
[14:43:28] WARNING: not removing hydrogen atom without neighbors
[14:43:28] WARNING: not removing hydrogen atom without neighbors
[14:43:28] WARNING: not removing hydrogen atom without neighbors
[14:43:28] WARNING: not removing hydrogen atom without neighbors
[14:43:28] WARNING: not removing hydrogen atom without neighbors
[14:43:28] WARNING: not removing hydrogen atom without neighbors
[14:43:28] WARNING: not removing hydrogen atom without neighbors
[14:43:28] WARNING: not removing hydrogen atom without neighbors
[14:43:28] WARNING: not removing hydrogen atom without neighbors
[14:43:28] WARNING: not removing hydrogen atom without neighbors
[14:43:28] WARNING: not removing hydrogen atom without neighbors
[14:43:28] WARNING: not removing hydrogen atom without neighbors
[14:43:28] WARNING: not removing hydrogen atom without neighbors
[14:43:28] WARNING: not removing hydrogen atom without neighbors
[14:43:28] WARNING: not removing hydrogen atom without neighbors
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/hiv/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: hiv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/hiv/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/hiv/scaff/train_prop=0.6/hiv_scaff_2_20-05_14-43-16  ]
[ Using Seed :  2  ]
[ Using device :  cuda:0  ]
Dataset: hiv
Data: Data(edge_attr=[2259376, 2], edge_index=[2, 2259376], id=[41127], x=[1049163, 2], y=[41127])
MoleculeDataset(41127)
split via scaffold
Data(edge_attr=[32, 2], edge_index=[2, 32], id=[1], x=[16, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.2842418501795363
ROC train: 0.743307	val: 0.649628	test: 0.697880
PRC train: 0.195652	val: 0.115935	test: 0.167082

Epoch: 2
Loss: 0.14114210968522753
ROC train: 0.816690	val: 0.664245	test: 0.722976
PRC train: 0.334792	val: 0.165200	test: 0.174705

Epoch: 3
Loss: 0.13272656929948645
ROC train: 0.829576	val: 0.656794	test: 0.718799
PRC train: 0.398989	val: 0.186405	test: 0.246150

Epoch: 4
Loss: 0.12824992090406398
ROC train: 0.832288	val: 0.676321	test: 0.740150
PRC train: 0.402006	val: 0.184487	test: 0.263571

Epoch: 5
Loss: 0.12483057245658319
ROC train: 0.846959	val: 0.671631	test: 0.729921
PRC train: 0.427429	val: 0.194386	test: 0.218768

Epoch: 6
Loss: 0.12197182251978848
ROC train: 0.860472	val: 0.682368	test: 0.740826
PRC train: 0.442725	val: 0.198603	test: 0.177655

Epoch: 7
Loss: 0.11985868405081763
ROC train: 0.856453	val: 0.681835	test: 0.739762
PRC train: 0.431005	val: 0.154250	test: 0.192011

Epoch: 8
Loss: 0.11865870462602414
ROC train: 0.860452	val: 0.667872	test: 0.721326
PRC train: 0.406160	val: 0.110685	test: 0.130676

Epoch: 9
Loss: 0.11622493777331049
ROC train: 0.876777	val: 0.680452	test: 0.741910
PRC train: 0.510153	val: 0.207385	test: 0.275053

Epoch: 10
Loss: 0.11543110993326007
ROC train: 0.883884	val: 0.683180	test: 0.739872
PRC train: 0.511251	val: 0.198574	test: 0.253236

Epoch: 11
Loss: 0.11373753477173958
ROC train: 0.873366	val: 0.687686	test: 0.724145
PRC train: 0.485436	val: 0.188689	test: 0.243766

Epoch: 12
Loss: 0.1103897758810127
ROC train: 0.891620	val: 0.687281	test: 0.733600
PRC train: 0.531776	val: 0.190521	test: 0.196606

Epoch: 13
Loss: 0.10905428478306356
ROC train: 0.888675	val: 0.692728	test: 0.746983
PRC train: 0.508067	val: 0.170754	test: 0.234485

Epoch: 14
Loss: 0.11011109367040177
ROC train: 0.892284	val: 0.683014	test: 0.727224
PRC train: 0.470999	val: 0.166753	test: 0.140732

Epoch: 15
Loss: 0.10795016272515799
ROC train: 0.894117	val: 0.698070	test: 0.752759
PRC train: 0.511409	val: 0.204014	test: 0.239604

Epoch: 16
Loss: 0.1082435215711293
ROC train: 0.905693	val: 0.689246	test: 0.734806
PRC train: 0.561384	val: 0.208215	test: 0.185195

Epoch: 17
Loss: 0.10521532023504722
ROC train: 0.907248	val: 0.688485	test: 0.733195
PRC train: 0.554222	val: 0.189296	test: 0.179630

Epoch: 18
Loss: 0.10673336817284512
ROC train: 0.898658	val: 0.685758	test: 0.734808
PRC train: 0.528529	val: 0.169689	test: 0.181496

Epoch: 19
Loss: 0.10601233506920725
ROC train: 0.908334	val: 0.687452	test: 0.740799
PRC train: 0.571334	val: 0.210742	test: 0.227856

Epoch: 20
Loss: 0.10359277825593816
ROC train: 0.914132	val: 0.687283	test: 0.740734
PRC train: 0.573544	val: 0.193347	test: 0.234590

Epoch: 21
Loss: 0.10280090449340903
ROC train: 0.903842	val: 0.676419	test: 0.732641
PRC train: 0.560766	val: 0.182963	test: 0.212203

Epoch: 22
Loss: 0.10393338307806457
ROC train: 0.906308	val: 0.689624	test: 0.745911
PRC train: 0.555524	val: 0.215507	test: 0.237553

Epoch: 23
Loss: 0.10189978103938696
ROC train: 0.915957	val: 0.696331	test: 0.740886
PRC train: 0.581276	val: 0.202609	test: 0.168789

Epoch: 24
Loss: 0.09936998465914054
ROC train: 0.925464	val: 0.689588	test: 0.748350
PRC train: 0.608790	val: 0.204275	test: 0.206262

Epoch: 25
Loss: 0.10033363977137973
ROC train: 0.923665	val: 0.691671	test: 0.740054
PRC train: 0.600773	val: 0.218060	test: 0.215112

Epoch: 26
Loss: 0.09704283159060315
ROC train: 0.923798	val: 0.695265	test: 0.747935
PRC train: 0.596647	val: 0.215556	test: 0.215736

Epoch: 27
Loss: 0.09836423918157065
ROC train: 0.931988	val: 0.704545	test: 0.754787
PRC train: 0.613737	val: 0.218116	test: 0.216722

Epoch: 28
Loss: 0.09872674499744839
ROC train: 0.926368	val: 0.696528	test: 0.759051
PRC train: 0.604626	val: 0.240469	test: 0.238799

Epoch: 29
Loss: 0.09739566342488444
ROC train: 0.927966	val: 0.702967	test: 0.750981
PRC train: 0.579742	val: 0.203839	test: 0.224048

Epoch: 30
Loss: 0.09746441064338222
ROC train: 0.933789	val: 0.700522	test: 0.758869
PRC train: 0.628491	val: 0.215511	test: 0.224291

Epoch: 31
Loss: 0.09746191853928517
ROC train: 0.930957	val: 0.702524	test: 0.765334
PRC train: 0.626730	val: 0.230388	test: 0.215460

Epoch: 32
Loss: 0.09812464447602004
ROC train: 0.934522	val: 0.691663	test: 0.748485
PRC train: 0.626596	val: 0.214083	test: 0.197690

Epoch: 33
Loss: 0.09563505626200366Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/hiv/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: hiv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/hiv/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/hiv/scaff/train_prop=0.6/hiv_scaff_3_20-05_14-43-16  ]
[ Using Seed :  3  ]
[ Using device :  cuda:0  ]
Dataset: hiv
Data: Data(edge_attr=[2259376, 2], edge_index=[2, 2259376], id=[41127], x=[1049163, 2], y=[41127])
MoleculeDataset(41127)
split via scaffold
Data(edge_attr=[32, 2], edge_index=[2, 32], id=[1], x=[16, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.2945338524900209
ROC train: 0.776088	val: 0.679674	test: 0.658982
PRC train: 0.234503	val: 0.124392	test: 0.133880

Epoch: 2
Loss: 0.1400094983406002
ROC train: 0.808320	val: 0.689496	test: 0.725096
PRC train: 0.334555	val: 0.182769	test: 0.209631

Epoch: 3
Loss: 0.13300471359249927
ROC train: 0.823375	val: 0.669413	test: 0.733675
PRC train: 0.381455	val: 0.171250	test: 0.265036

Epoch: 4
Loss: 0.1302995633249627
ROC train: 0.832749	val: 0.682671	test: 0.719535
PRC train: 0.395710	val: 0.195807	test: 0.156106

Epoch: 5
Loss: 0.12567571661966057
ROC train: 0.839331	val: 0.675201	test: 0.745449
PRC train: 0.409904	val: 0.191317	test: 0.226838

Epoch: 6
Loss: 0.1262826041901254
ROC train: 0.854765	val: 0.681358	test: 0.743576
PRC train: 0.437641	val: 0.200016	test: 0.246536

Epoch: 7
Loss: 0.12213183021739676
ROC train: 0.854153	val: 0.682510	test: 0.751470
PRC train: 0.460399	val: 0.190957	test: 0.244570

Epoch: 8
Loss: 0.1195645414781351
ROC train: 0.865152	val: 0.701790	test: 0.757028
PRC train: 0.472489	val: 0.199440	test: 0.235859

Epoch: 9
Loss: 0.11596949212190372
ROC train: 0.861881	val: 0.678023	test: 0.717198
PRC train: 0.460919	val: 0.179298	test: 0.124495

Epoch: 10
Loss: 0.11646485648760761
ROC train: 0.872832	val: 0.676279	test: 0.744094
PRC train: 0.476347	val: 0.177658	test: 0.219897

Epoch: 11
Loss: 0.11449047489372376
ROC train: 0.875947	val: 0.686463	test: 0.731492
PRC train: 0.512809	val: 0.206396	test: 0.245536

Epoch: 12
Loss: 0.11351365116338845
ROC train: 0.878223	val: 0.686436	test: 0.739389
PRC train: 0.510297	val: 0.178847	test: 0.243699

Epoch: 13
Loss: 0.11204013865660777
ROC train: 0.890039	val: 0.687954	test: 0.731220
PRC train: 0.487386	val: 0.199419	test: 0.139266

Epoch: 14
Loss: 0.11033205579241695
ROC train: 0.887635	val: 0.690036	test: 0.728950
PRC train: 0.521367	val: 0.219692	test: 0.254097

Epoch: 15
Loss: 0.10922148515082826
ROC train: 0.902381	val: 0.696524	test: 0.731493
PRC train: 0.541260	val: 0.199405	test: 0.236699

Epoch: 16
Loss: 0.10780088660184539
ROC train: 0.901029	val: 0.691544	test: 0.749467
PRC train: 0.538797	val: 0.203518	test: 0.223558

Epoch: 17
Loss: 0.10617039526218483
ROC train: 0.900086	val: 0.686379	test: 0.732467
PRC train: 0.536033	val: 0.192357	test: 0.274365

Epoch: 18
Loss: 0.10782744591269097
ROC train: 0.900748	val: 0.702123	test: 0.739454
PRC train: 0.544969	val: 0.220983	test: 0.213744

Epoch: 19
Loss: 0.10506956712484507
ROC train: 0.908875	val: 0.691171	test: 0.739703
PRC train: 0.564335	val: 0.215463	test: 0.174268

Epoch: 20
Loss: 0.10329239883045276
ROC train: 0.911791	val: 0.695084	test: 0.742246
PRC train: 0.580277	val: 0.237225	test: 0.200795

Epoch: 21
Loss: 0.10427973177148162
ROC train: 0.912237	val: 0.705608	test: 0.741330
PRC train: 0.582861	val: 0.219931	test: 0.234226

Epoch: 22
Loss: 0.10254894440986564
ROC train: 0.914925	val: 0.702924	test: 0.736593
PRC train: 0.579993	val: 0.231586	test: 0.204826

Epoch: 23
Loss: 0.10146002671646895
ROC train: 0.915737	val: 0.694358	test: 0.729331
PRC train: 0.581978	val: 0.217964	test: 0.173800

Epoch: 24
Loss: 0.1024523327937765
ROC train: 0.915640	val: 0.701815	test: 0.741939
PRC train: 0.586536	val: 0.212491	test: 0.200084

Epoch: 25
Loss: 0.10162334890369484
ROC train: 0.923476	val: 0.706675	test: 0.752063
PRC train: 0.611293	val: 0.207455	test: 0.232119

Epoch: 26
Loss: 0.10038389012337703
ROC train: 0.926646	val: 0.701331	test: 0.733080
PRC train: 0.611294	val: 0.219818	test: 0.216162

Epoch: 27
Loss: 0.09962975498732038
ROC train: 0.923222	val: 0.703937	test: 0.750804
PRC train: 0.596450	val: 0.215703	test: 0.257643

Epoch: 28
Loss: 0.1000906114517673
ROC train: 0.922673	val: 0.690789	test: 0.751639
PRC train: 0.605885	val: 0.227437	test: 0.202051

Epoch: 29
Loss: 0.09848561209655612
ROC train: 0.931779	val: 0.706228	test: 0.739449
PRC train: 0.627728	val: 0.242124	test: 0.240685

Epoch: 30
Loss: 0.095186272474935
ROC train: 0.932241	val: 0.698618	test: 0.729318
PRC train: 0.619963	val: 0.221632	test: 0.162210

Epoch: 31
Loss: 0.09482540706982878
ROC train: 0.934910	val: 0.693737	test: 0.740666
PRC train: 0.632566	val: 0.226916	test: 0.231410

Epoch: 32
Loss: 0.09667173605639237
ROC train: 0.936937	val: 0.701776	test: 0.740370
PRC train: 0.632977	val: 0.248899	test: 0.247656

Epoch: 33
Loss: 0.09619050573372653Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/hiv/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: hiv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/hiv/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/hiv/scaff/train_prop=0.6/hiv_scaff_1_20-05_14-43-16  ]
[ Using Seed :  1  ]
[ Using device :  cuda:0  ]
Dataset: hiv
Data: Data(edge_attr=[2259376, 2], edge_index=[2, 2259376], id=[41127], x=[1049163, 2], y=[41127])
MoleculeDataset(41127)
split via scaffold
Data(edge_attr=[32, 2], edge_index=[2, 32], id=[1], x=[16, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.28860968487797223
ROC train: 0.780957	val: 0.681602	test: 0.730075
PRC train: 0.258222	val: 0.180097	test: 0.183311

Epoch: 2
Loss: 0.14281755107171332
ROC train: 0.805533	val: 0.683388	test: 0.718900
PRC train: 0.304634	val: 0.175996	test: 0.177468

Epoch: 3
Loss: 0.13229488841526355
ROC train: 0.836163	val: 0.672354	test: 0.727681
PRC train: 0.376433	val: 0.186653	test: 0.212510

Epoch: 4
Loss: 0.12902582522998185
ROC train: 0.838316	val: 0.690564	test: 0.728446
PRC train: 0.396800	val: 0.185656	test: 0.249882

Epoch: 5
Loss: 0.12547147532442973
ROC train: 0.840159	val: 0.683206	test: 0.743877
PRC train: 0.392764	val: 0.149351	test: 0.240650

Epoch: 6
Loss: 0.12420025561063887
ROC train: 0.860534	val: 0.690426	test: 0.751810
PRC train: 0.449207	val: 0.186633	test: 0.248413

Epoch: 7
Loss: 0.12132883823686332
ROC train: 0.861709	val: 0.688774	test: 0.733429
PRC train: 0.447531	val: 0.184745	test: 0.184262

Epoch: 8
Loss: 0.1182657973134901
ROC train: 0.871893	val: 0.695975	test: 0.741467
PRC train: 0.485015	val: 0.192651	test: 0.225010

Epoch: 9
Loss: 0.11683575119250436
ROC train: 0.872281	val: 0.695616	test: 0.730827
PRC train: 0.482280	val: 0.195058	test: 0.200913

Epoch: 10
Loss: 0.11721731291680369
ROC train: 0.881030	val: 0.707624	test: 0.756500
PRC train: 0.495810	val: 0.190463	test: 0.206808

Epoch: 11
Loss: 0.11387151445092461
ROC train: 0.863819	val: 0.696875	test: 0.730904
PRC train: 0.470127	val: 0.198260	test: 0.265170

Epoch: 12
Loss: 0.11290285398702699
ROC train: 0.886027	val: 0.696951	test: 0.744630
PRC train: 0.520812	val: 0.227712	test: 0.251598

Epoch: 13
Loss: 0.11283134932267215
ROC train: 0.897052	val: 0.707126	test: 0.757431
PRC train: 0.523015	val: 0.234417	test: 0.218640

Epoch: 14
Loss: 0.11044431690315452
ROC train: 0.897935	val: 0.708055	test: 0.746901
PRC train: 0.539868	val: 0.218657	test: 0.220983

Epoch: 15
Loss: 0.1089577614958546
ROC train: 0.903158	val: 0.704819	test: 0.769972
PRC train: 0.545053	val: 0.189412	test: 0.200493

Epoch: 16
Loss: 0.10798665768905041
ROC train: 0.896888	val: 0.710423	test: 0.769971
PRC train: 0.533915	val: 0.220231	test: 0.228225

Epoch: 17
Loss: 0.1077866788909283
ROC train: 0.898484	val: 0.697165	test: 0.745197
PRC train: 0.552124	val: 0.221647	test: 0.235839

Epoch: 18
Loss: 0.1050537313093038
ROC train: 0.907689	val: 0.711009	test: 0.756282
PRC train: 0.562877	val: 0.208429	test: 0.241727

Epoch: 19
Loss: 0.10568129153109651
ROC train: 0.908311	val: 0.714462	test: 0.748010
PRC train: 0.555793	val: 0.219431	test: 0.198833

Epoch: 20
Loss: 0.10516806478030982
ROC train: 0.910465	val: 0.705399	test: 0.750910
PRC train: 0.577867	val: 0.239157	test: 0.203916

Epoch: 21
Loss: 0.10226441913329687
ROC train: 0.912048	val: 0.699360	test: 0.747996
PRC train: 0.578666	val: 0.231164	test: 0.192792

Epoch: 22
Loss: 0.10369667547188578
ROC train: 0.916260	val: 0.702413	test: 0.754314
PRC train: 0.581060	val: 0.224124	test: 0.217522

Epoch: 23
Loss: 0.10342192488615917
ROC train: 0.921062	val: 0.711604	test: 0.741825
PRC train: 0.599568	val: 0.218345	test: 0.220353

Epoch: 24
Loss: 0.09944462114192615
ROC train: 0.921243	val: 0.698987	test: 0.736302
PRC train: 0.605589	val: 0.237467	test: 0.205440

Epoch: 25
Loss: 0.10194251143946985
ROC train: 0.923088	val: 0.705251	test: 0.753638
PRC train: 0.603228	val: 0.233904	test: 0.189784

Epoch: 26
Loss: 0.10007659254824504
ROC train: 0.924015	val: 0.697239	test: 0.733364
PRC train: 0.599310	val: 0.196623	test: 0.191161

Epoch: 27
Loss: 0.09708619651621474
ROC train: 0.930334	val: 0.698861	test: 0.753089
PRC train: 0.614330	val: 0.222752	test: 0.235885

Epoch: 28
Loss: 0.09884425907945488
ROC train: 0.928722	val: 0.696147	test: 0.744377
PRC train: 0.606598	val: 0.214731	test: 0.250824

Epoch: 29
Loss: 0.09774414954597577
ROC train: 0.934678	val: 0.714162	test: 0.762069
PRC train: 0.619547	val: 0.210783	test: 0.232065

Epoch: 30
Loss: 0.09801928204465733
ROC train: 0.933804	val: 0.713808	test: 0.750771
PRC train: 0.617136	val: 0.241165	test: 0.227281

Epoch: 31
Loss: 0.09545510203919422
ROC train: 0.931961	val: 0.700314	test: 0.757973
PRC train: 0.610351	val: 0.220563	test: 0.190751

Epoch: 32
Loss: 0.09639252021102929
ROC train: 0.934169	val: 0.707075	test: 0.754441
PRC train: 0.628501	val: 0.257774	test: 0.187680

Epoch: 33
Loss: 0.09306927219920609Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/hiv/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: hiv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/hiv/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/hiv/scaff/train_prop=0.7/hiv_scaff_1_20-05_14-43-16  ]
[ Using Seed :  1  ]
[ Using device :  cuda:1  ]
Dataset: hiv
Data: Data(edge_attr=[2259376, 2], edge_index=[2, 2259376], id=[41127], x=[1049163, 2], y=[41127])
MoleculeDataset(41127)
split via scaffold
Data(edge_attr=[32, 2], edge_index=[2, 32], id=[1], x=[16, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.267273991900637
ROC train: 0.775760	val: 0.685612	test: 0.682199
PRC train: 0.254463	val: 0.173648	test: 0.167964

Epoch: 2
Loss: 0.14152250044170667
ROC train: 0.796580	val: 0.697061	test: 0.731345
PRC train: 0.316837	val: 0.231980	test: 0.220559

Epoch: 3
Loss: 0.13482046157398336
ROC train: 0.809880	val: 0.702765	test: 0.735823
PRC train: 0.339193	val: 0.225189	test: 0.184768

Epoch: 4
Loss: 0.1315453174850552
ROC train: 0.823346	val: 0.700930	test: 0.725252
PRC train: 0.371884	val: 0.256583	test: 0.178385

Epoch: 5
Loss: 0.12840852146467618
ROC train: 0.835195	val: 0.701169	test: 0.709192
PRC train: 0.387431	val: 0.217223	test: 0.162753

Epoch: 6
Loss: 0.1249096195545545
ROC train: 0.840388	val: 0.708189	test: 0.742776
PRC train: 0.402962	val: 0.256277	test: 0.178739

Epoch: 7
Loss: 0.12456153284396732
ROC train: 0.853561	val: 0.693153	test: 0.712631
PRC train: 0.417157	val: 0.247157	test: 0.156534

Epoch: 8
Loss: 0.12228541109267321
ROC train: 0.849395	val: 0.707735	test: 0.728508
PRC train: 0.426623	val: 0.287194	test: 0.217175

Epoch: 9
Loss: 0.11932134231391328
ROC train: 0.863470	val: 0.716923	test: 0.719573
PRC train: 0.447325	val: 0.267304	test: 0.150017

Epoch: 10
Loss: 0.11837657483381393
ROC train: 0.868748	val: 0.706467	test: 0.737010
PRC train: 0.457431	val: 0.260012	test: 0.172816

Epoch: 11
Loss: 0.11763891515022491
ROC train: 0.871235	val: 0.723160	test: 0.743573
PRC train: 0.454696	val: 0.289265	test: 0.219848

Epoch: 12
Loss: 0.1168718184664521
ROC train: 0.873491	val: 0.729571	test: 0.737796
PRC train: 0.481559	val: 0.251980	test: 0.116471

Epoch: 13
Loss: 0.11369332066595648
ROC train: 0.886655	val: 0.716573	test: 0.745434
PRC train: 0.473443	val: 0.284517	test: 0.218544

Epoch: 14
Loss: 0.11420996797455348
ROC train: 0.882650	val: 0.721060	test: 0.747146
PRC train: 0.487671	val: 0.326554	test: 0.204764

Epoch: 15
Loss: 0.11134018938946508
ROC train: 0.884331	val: 0.718151	test: 0.758648
PRC train: 0.482554	val: 0.285132	test: 0.181925

Epoch: 16
Loss: 0.11128056617590731
ROC train: 0.888884	val: 0.727721	test: 0.738507
PRC train: 0.502329	val: 0.301317	test: 0.189306

Epoch: 17
Loss: 0.11137433324347148
ROC train: 0.895701	val: 0.725400	test: 0.740761
PRC train: 0.526865	val: 0.278583	test: 0.146727

Epoch: 18
Loss: 0.1116689897214661
ROC train: 0.899682	val: 0.715978	test: 0.719796
PRC train: 0.528693	val: 0.262842	test: 0.146219

Epoch: 19
Loss: 0.10888406023204002
ROC train: 0.897431	val: 0.729528	test: 0.750962
PRC train: 0.520359	val: 0.295611	test: 0.211424

Epoch: 20
Loss: 0.10865970697240725
ROC train: 0.902667	val: 0.724171	test: 0.763836
PRC train: 0.529468	val: 0.304005	test: 0.191110

Epoch: 21
Loss: 0.1063866677218812
ROC train: 0.901315	val: 0.713256	test: 0.746524
PRC train: 0.536100	val: 0.273489	test: 0.154176

Epoch: 22
Loss: 0.10618029674483202
ROC train: 0.899289	val: 0.733273	test: 0.745621
PRC train: 0.539026	val: 0.302607	test: 0.188595

Epoch: 23
Loss: 0.1054226469412393
ROC train: 0.915943	val: 0.723847	test: 0.728122
PRC train: 0.572816	val: 0.289845	test: 0.148311

Epoch: 24
Loss: 0.1041185555543166
ROC train: 0.911287	val: 0.718602	test: 0.751437
PRC train: 0.574290	val: 0.291376	test: 0.154990

Epoch: 25
Loss: 0.10364849695365058
ROC train: 0.908564	val: 0.724213	test: 0.738642
PRC train: 0.549729	val: 0.275216	test: 0.184750

Epoch: 26
Loss: 0.10331931450088847
ROC train: 0.913536	val: 0.705845	test: 0.733298
PRC train: 0.572633	val: 0.286135	test: 0.164079

Epoch: 27
Loss: 0.10266941870338146
ROC train: 0.918663	val: 0.714429	test: 0.743493
PRC train: 0.577765	val: 0.313846	test: 0.155683

Epoch: 28
Loss: 0.10097897467192933
ROC train: 0.919287	val: 0.701753	test: 0.736585
PRC train: 0.578607	val: 0.291680	test: 0.162473

Epoch: 29
Loss: 0.10195198640945931
ROC train: 0.922316	val: 0.731572	test: 0.760496
PRC train: 0.586397	val: 0.305816	test: 0.207332

Epoch: 30
Loss: 0.10097271926288252
ROC train: 0.918393	val: 0.722173	test: 0.755882
PRC train: 0.580014	val: 0.289430	test: 0.166767

Epoch: 31
Loss: 0.10002359758694157
ROC train: 0.918551	val: 0.701814	test: 0.748029
PRC train: 0.558852	val: 0.295011	test: 0.174279

Epoch: 32
Loss: 0.09968603346960364
ROC train: 0.923520	val: 0.713895	test: 0.747544
PRC train: 0.590791	val: 0.289859	test: 0.161925

Epoch: 33
Loss: 0.09805336920158451Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/hiv/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: hiv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/hiv/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/hiv/scaff/train_prop=0.7/hiv_scaff_3_20-05_14-43-16  ]
[ Using Seed :  3  ]
[ Using device :  cuda:1  ]
Dataset: hiv
Data: Data(edge_attr=[2259376, 2], edge_index=[2, 2259376], id=[41127], x=[1049163, 2], y=[41127])
MoleculeDataset(41127)
split via scaffold
Data(edge_attr=[32, 2], edge_index=[2, 32], id=[1], x=[16, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.27250695809475384
ROC train: 0.783084	val: 0.698498	test: 0.632297
PRC train: 0.267090	val: 0.184213	test: 0.136801

Epoch: 2
Loss: 0.14188551432106097
ROC train: 0.792054	val: 0.692659	test: 0.667385
PRC train: 0.286119	val: 0.204124	test: 0.175010

Epoch: 3
Loss: 0.13385695132703723
ROC train: 0.809630	val: 0.701973	test: 0.713981
PRC train: 0.331841	val: 0.237750	test: 0.163434

Epoch: 4
Loss: 0.13201146516042844
ROC train: 0.823557	val: 0.715581	test: 0.699971
PRC train: 0.377198	val: 0.290533	test: 0.222033

Epoch: 5
Loss: 0.12774496114762468
ROC train: 0.828820	val: 0.706336	test: 0.711144
PRC train: 0.380444	val: 0.282552	test: 0.213704

Epoch: 6
Loss: 0.1265678912515672
ROC train: 0.842650	val: 0.714978	test: 0.730777
PRC train: 0.400276	val: 0.301852	test: 0.213842

Epoch: 7
Loss: 0.12465694523807457
ROC train: 0.849737	val: 0.710523	test: 0.713511
PRC train: 0.429649	val: 0.299735	test: 0.197388

Epoch: 8
Loss: 0.12145960508253555
ROC train: 0.860000	val: 0.718268	test: 0.713089
PRC train: 0.448139	val: 0.274787	test: 0.166604

Epoch: 9
Loss: 0.12062522531280961
ROC train: 0.858566	val: 0.710674	test: 0.714357
PRC train: 0.465551	val: 0.304910	test: 0.200703

Epoch: 10
Loss: 0.11865843918811984
ROC train: 0.855946	val: 0.709723	test: 0.731976
PRC train: 0.431580	val: 0.311367	test: 0.202112

Epoch: 11
Loss: 0.1178814512317219
ROC train: 0.863556	val: 0.716172	test: 0.723064
PRC train: 0.462799	val: 0.307456	test: 0.213123

Epoch: 12
Loss: 0.11482041997776453
ROC train: 0.874094	val: 0.705094	test: 0.723186
PRC train: 0.498865	val: 0.271652	test: 0.163273

Epoch: 13
Loss: 0.11522087255346203
ROC train: 0.874046	val: 0.717300	test: 0.731333
PRC train: 0.496969	val: 0.268638	test: 0.172016

Epoch: 14
Loss: 0.11308418258062913
ROC train: 0.873764	val: 0.717757	test: 0.743167
PRC train: 0.504254	val: 0.284518	test: 0.163737

Epoch: 15
Loss: 0.1137692146833875
ROC train: 0.875220	val: 0.717449	test: 0.726715
PRC train: 0.492056	val: 0.265357	test: 0.140726

Epoch: 16
Loss: 0.11323480895782147
ROC train: 0.872025	val: 0.696224	test: 0.747458
PRC train: 0.488415	val: 0.280891	test: 0.187209

Epoch: 17
Loss: 0.11163942096715442
ROC train: 0.883518	val: 0.703402	test: 0.758341
PRC train: 0.487655	val: 0.312775	test: 0.211769

Epoch: 18
Loss: 0.11040798890379729
ROC train: 0.885222	val: 0.693924	test: 0.731618
PRC train: 0.515828	val: 0.293118	test: 0.205749

Epoch: 19
Loss: 0.11021086777806635
ROC train: 0.889198	val: 0.712681	test: 0.716123
PRC train: 0.534212	val: 0.261885	test: 0.099631

Epoch: 20
Loss: 0.10814402841730415
ROC train: 0.894315	val: 0.702602	test: 0.712699
PRC train: 0.544427	val: 0.280969	test: 0.131672

Epoch: 21
Loss: 0.10707434500052579
ROC train: 0.900274	val: 0.707565	test: 0.742974
PRC train: 0.541442	val: 0.273092	test: 0.143026

Epoch: 22
Loss: 0.10528964104529936
ROC train: 0.905948	val: 0.709945	test: 0.744812
PRC train: 0.550025	val: 0.252566	test: 0.092836

Epoch: 23
Loss: 0.10478738903658422
ROC train: 0.900552	val: 0.722768	test: 0.732401
PRC train: 0.541037	val: 0.277621	test: 0.129111

Epoch: 24
Loss: 0.10556895604995635
ROC train: 0.904759	val: 0.698278	test: 0.744032
PRC train: 0.551638	val: 0.275110	test: 0.129152

Epoch: 25
Loss: 0.10422265802572671
ROC train: 0.907683	val: 0.721378	test: 0.742822
PRC train: 0.560258	val: 0.274481	test: 0.154701

Epoch: 26
Loss: 0.10622312849818323
ROC train: 0.910905	val: 0.711948	test: 0.734597
PRC train: 0.568103	val: 0.290955	test: 0.169506

Epoch: 27
Loss: 0.10437073405766452
ROC train: 0.914467	val: 0.715573	test: 0.745958
PRC train: 0.576208	val: 0.279651	test: 0.158239

Epoch: 28
Loss: 0.10277870335559361
ROC train: 0.910282	val: 0.695273	test: 0.721217
PRC train: 0.569400	val: 0.300061	test: 0.141844

Epoch: 29
Loss: 0.1019399000300702
ROC train: 0.918681	val: 0.701974	test: 0.725780
PRC train: 0.580605	val: 0.260482	test: 0.123514

Epoch: 30
Loss: 0.10198014564693475
ROC train: 0.914803	val: 0.707789	test: 0.745208
PRC train: 0.584458	val: 0.269529	test: 0.129610

Epoch: 31
Loss: 0.10109209575001767
ROC train: 0.917931	val: 0.715387	test: 0.759106
PRC train: 0.590384	val: 0.295539	test: 0.167115

Epoch: 32
Loss: 0.10040762988819693
ROC train: 0.923108	val: 0.711149	test: 0.729388
PRC train: 0.583517	val: 0.288275	test: 0.138199

Epoch: 33
Loss: 0.10044919652740095Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/hiv/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: hiv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/hiv/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/hiv/scaff/train_prop=0.7/hiv_scaff_2_20-05_14-43-16  ]
[ Using Seed :  2  ]
[ Using device :  cuda:1  ]
Dataset: hiv
Data: Data(edge_attr=[2259376, 2], edge_index=[2, 2259376], id=[41127], x=[1049163, 2], y=[41127])
MoleculeDataset(41127)
split via scaffold
Data(edge_attr=[32, 2], edge_index=[2, 32], id=[1], x=[16, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.2657998876705095
ROC train: 0.772934	val: 0.690466	test: 0.722699
PRC train: 0.239803	val: 0.179507	test: 0.198305

Epoch: 2
Loss: 0.14044173080084474
ROC train: 0.789069	val: 0.721291	test: 0.711374
PRC train: 0.300119	val: 0.235890	test: 0.191604

Epoch: 3
Loss: 0.13234518235026818
ROC train: 0.823809	val: 0.711114	test: 0.710023
PRC train: 0.360552	val: 0.257055	test: 0.174094

Epoch: 4
Loss: 0.13107565697174103
ROC train: 0.821881	val: 0.703803	test: 0.713805
PRC train: 0.373299	val: 0.287079	test: 0.206539

Epoch: 5
Loss: 0.12708105823216706
ROC train: 0.841055	val: 0.698807	test: 0.696976
PRC train: 0.403300	val: 0.275856	test: 0.172011

Epoch: 6
Loss: 0.1253213603054052
ROC train: 0.839293	val: 0.709049	test: 0.712369
PRC train: 0.412032	val: 0.283191	test: 0.170914

Epoch: 7
Loss: 0.12266768092867307
ROC train: 0.846468	val: 0.704036	test: 0.701534
PRC train: 0.431984	val: 0.280976	test: 0.179640

Epoch: 8
Loss: 0.12069713646018451
ROC train: 0.854136	val: 0.703575	test: 0.739596
PRC train: 0.389157	val: 0.280689	test: 0.171229

Epoch: 9
Loss: 0.11929847442188633
ROC train: 0.852360	val: 0.693939	test: 0.741281
PRC train: 0.438365	val: 0.266708	test: 0.196060

Epoch: 10
Loss: 0.11851094051194572
ROC train: 0.863534	val: 0.704989	test: 0.732768
PRC train: 0.461441	val: 0.267587	test: 0.174390

Epoch: 11
Loss: 0.11548910201232596
ROC train: 0.866623	val: 0.692693	test: 0.714721
PRC train: 0.462265	val: 0.256133	test: 0.167383

Epoch: 12
Loss: 0.11498365948984068
ROC train: 0.868966	val: 0.710760	test: 0.749174
PRC train: 0.471065	val: 0.262517	test: 0.202782

Epoch: 13
Loss: 0.11401656036218834
ROC train: 0.881214	val: 0.711805	test: 0.760933
PRC train: 0.493670	val: 0.303518	test: 0.171645

Epoch: 14
Loss: 0.11464841921928655
ROC train: 0.881213	val: 0.717294	test: 0.743485
PRC train: 0.509340	val: 0.297019	test: 0.199043

Epoch: 15
Loss: 0.11163160982732404
ROC train: 0.887987	val: 0.710972	test: 0.751920
PRC train: 0.521601	val: 0.289541	test: 0.209540

Epoch: 16
Loss: 0.11093680160932257
ROC train: 0.884088	val: 0.702194	test: 0.702560
PRC train: 0.506806	val: 0.243911	test: 0.085780

Epoch: 17
Loss: 0.11046303058472574
ROC train: 0.890299	val: 0.707116	test: 0.753510
PRC train: 0.511299	val: 0.255575	test: 0.131632

Epoch: 18
Loss: 0.108298813609817
ROC train: 0.898289	val: 0.704795	test: 0.751586
PRC train: 0.533116	val: 0.292661	test: 0.181364

Epoch: 19
Loss: 0.10808430843926174
ROC train: 0.891382	val: 0.702784	test: 0.737647
PRC train: 0.512525	val: 0.290473	test: 0.115391

Epoch: 20
Loss: 0.10883491560839287
ROC train: 0.903085	val: 0.703307	test: 0.747833
PRC train: 0.557333	val: 0.282804	test: 0.170359

Epoch: 21
Loss: 0.10849837217336104
ROC train: 0.904972	val: 0.692681	test: 0.770096
PRC train: 0.550824	val: 0.233329	test: 0.151808

Epoch: 22
Loss: 0.10478358629501801
ROC train: 0.904879	val: 0.707152	test: 0.744804
PRC train: 0.549649	val: 0.285230	test: 0.132970

Epoch: 23
Loss: 0.10525774590154068
ROC train: 0.904063	val: 0.716295	test: 0.744012
PRC train: 0.552634	val: 0.293478	test: 0.184646

Epoch: 24
Loss: 0.10458386079109785
ROC train: 0.910048	val: 0.685619	test: 0.737174
PRC train: 0.558851	val: 0.267944	test: 0.134444

Epoch: 25
Loss: 0.10499940909824129
ROC train: 0.916255	val: 0.706213	test: 0.756238
PRC train: 0.572433	val: 0.263096	test: 0.134804

Epoch: 26
Loss: 0.10258648304630401
ROC train: 0.906247	val: 0.710020	test: 0.776735
PRC train: 0.545345	val: 0.275048	test: 0.181753

Epoch: 27
Loss: 0.10539486834823765
ROC train: 0.917195	val: 0.708726	test: 0.759328
PRC train: 0.589117	val: 0.280540	test: 0.178539

Epoch: 28
Loss: 0.10335214729400258
ROC train: 0.916592	val: 0.702679	test: 0.758672
PRC train: 0.565199	val: 0.301498	test: 0.185214

Epoch: 29
Loss: 0.10079552492356532
ROC train: 0.920410	val: 0.701992	test: 0.766920
PRC train: 0.580043	val: 0.307078	test: 0.199172

Epoch: 30
Loss: 0.1007895631952736
ROC train: 0.916740	val: 0.697447	test: 0.749866
PRC train: 0.600288	val: 0.274348	test: 0.144576

Epoch: 31
Loss: 0.10114604372581532
ROC train: 0.919808	val: 0.695031	test: 0.755464
PRC train: 0.592076	val: 0.266971	test: 0.143263

Epoch: 32
Loss: 0.09953430290101639
ROC train: 0.905388	val: 0.702485	test: 0.735444
PRC train: 0.543133	val: 0.244108	test: 0.108461

Epoch: 33
Loss: 0.09918247653945234Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/hiv/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: hiv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/hiv/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/hiv/scaff/train_prop=0.8/hiv_scaff_1_20-05_14-43-16  ]
[ Using Seed :  1  ]
[ Using device :  cuda:2  ]
Dataset: hiv
Data: Data(edge_attr=[2259376, 2], edge_index=[2, 2259376], id=[41127], x=[1049163, 2], y=[41127])
MoleculeDataset(41127)
split via scaffold
Data(edge_attr=[32, 2], edge_index=[2, 32], id=[1], x=[16, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.25458463205000387
ROC train: 0.760864	val: 0.756758	test: 0.691444
PRC train: 0.236171	val: 0.196511	test: 0.169800

Epoch: 2
Loss: 0.1416562754181346
ROC train: 0.773864	val: 0.730670	test: 0.710504
PRC train: 0.259401	val: 0.240633	test: 0.180562

Epoch: 3
Loss: 0.13631098835137043
ROC train: 0.796006	val: 0.762150	test: 0.726968
PRC train: 0.311324	val: 0.316212	test: 0.214037

Epoch: 4
Loss: 0.13231760907860393
ROC train: 0.796299	val: 0.740627	test: 0.721899
PRC train: 0.356608	val: 0.269843	test: 0.224476

Epoch: 5
Loss: 0.12911422256437222
ROC train: 0.817325	val: 0.745732	test: 0.711781
PRC train: 0.369668	val: 0.273366	test: 0.178564

Epoch: 6
Loss: 0.12791547759801386
ROC train: 0.810559	val: 0.749893	test: 0.742720
PRC train: 0.346737	val: 0.343492	test: 0.261753

Epoch: 7
Loss: 0.12556601251554259
ROC train: 0.832020	val: 0.780194	test: 0.752040
PRC train: 0.402034	val: 0.353875	test: 0.231267

Epoch: 8
Loss: 0.12511123410315125
ROC train: 0.836493	val: 0.762839	test: 0.755839
PRC train: 0.384705	val: 0.278301	test: 0.209210

Epoch: 9
Loss: 0.12264383571563783
ROC train: 0.847322	val: 0.787334	test: 0.736579
PRC train: 0.427775	val: 0.299534	test: 0.157174

Epoch: 10
Loss: 0.12199149306833336
ROC train: 0.839613	val: 0.741065	test: 0.725958
PRC train: 0.414585	val: 0.297188	test: 0.147040

Epoch: 11
Loss: 0.12075371689953461
ROC train: 0.851693	val: 0.765383	test: 0.703486
PRC train: 0.445180	val: 0.322939	test: 0.176521

Epoch: 12
Loss: 0.11817166501747237
ROC train: 0.862485	val: 0.773225	test: 0.725850
PRC train: 0.460376	val: 0.367460	test: 0.164015

Epoch: 13
Loss: 0.11587439102835106
ROC train: 0.864409	val: 0.776391	test: 0.743081
PRC train: 0.456947	val: 0.322760	test: 0.149214

Epoch: 14
Loss: 0.11577669894274135
ROC train: 0.862467	val: 0.765098	test: 0.756629
PRC train: 0.434892	val: 0.349360	test: 0.222941

Epoch: 15
Loss: 0.11489527882944027
ROC train: 0.863675	val: 0.798879	test: 0.752431
PRC train: 0.469683	val: 0.358913	test: 0.186298

Epoch: 16
Loss: 0.11373739970882674
ROC train: 0.866528	val: 0.772171	test: 0.744435
PRC train: 0.448165	val: 0.347727	test: 0.189916

Epoch: 17
Loss: 0.11269013776070688
ROC train: 0.871535	val: 0.772689	test: 0.756600
PRC train: 0.493784	val: 0.347466	test: 0.194518

Epoch: 18
Loss: 0.11198829288329674
ROC train: 0.874523	val: 0.783586	test: 0.750084
PRC train: 0.465785	val: 0.350244	test: 0.182475

Epoch: 19
Loss: 0.11190128829195183
ROC train: 0.880198	val: 0.772297	test: 0.741028
PRC train: 0.509835	val: 0.346143	test: 0.143269

Epoch: 20
Loss: 0.1096649241215242
ROC train: 0.886620	val: 0.783124	test: 0.760073
PRC train: 0.516632	val: 0.312744	test: 0.187929

Epoch: 21
Loss: 0.10971329443764666
ROC train: 0.888799	val: 0.797227	test: 0.758472
PRC train: 0.500298	val: 0.366444	test: 0.201243

Epoch: 22
Loss: 0.10989493479289628
ROC train: 0.889522	val: 0.780641	test: 0.753052
PRC train: 0.507041	val: 0.367392	test: 0.221758

Epoch: 23
Loss: 0.10838271890508853
ROC train: 0.889775	val: 0.790647	test: 0.764903
PRC train: 0.513697	val: 0.344516	test: 0.195226

Epoch: 24
Loss: 0.10680030074587052
ROC train: 0.894815	val: 0.768454	test: 0.736615
PRC train: 0.536060	val: 0.353528	test: 0.184585

Epoch: 25
Loss: 0.10588788309473773
ROC train: 0.898781	val: 0.779593	test: 0.738552
PRC train: 0.547171	val: 0.357559	test: 0.168772

Epoch: 26
Loss: 0.10650182606194512
ROC train: 0.881351	val: 0.774728	test: 0.784285
PRC train: 0.514985	val: 0.365432	test: 0.165913

Epoch: 27
Loss: 0.10493742713261354
ROC train: 0.896928	val: 0.779762	test: 0.768790
PRC train: 0.544441	val: 0.363759	test: 0.172429

Epoch: 28
Loss: 0.10488027171067404
ROC train: 0.909868	val: 0.772352	test: 0.758182
PRC train: 0.559755	val: 0.307975	test: 0.149534

Epoch: 29
Loss: 0.1041029638067681
ROC train: 0.903376	val: 0.758181	test: 0.771755
PRC train: 0.553098	val: 0.346529	test: 0.166035

Epoch: 30
Loss: 0.10269999840549335
ROC train: 0.914791	val: 0.803287	test: 0.742878
PRC train: 0.554076	val: 0.359610	test: 0.178299

Epoch: 31
Loss: 0.10340884908187961
ROC train: 0.906797	val: 0.770705	test: 0.748230
PRC train: 0.547372	val: 0.309640	test: 0.214662

Epoch: 32
Loss: 0.10295982929391143
ROC train: 0.912516	val: 0.789756	test: 0.760019
PRC train: 0.572699	val: 0.393855	test: 0.188076

Epoch: 33
Loss: 0.10172712112236619Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/hiv/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: hiv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/hiv/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/hiv/scaff/train_prop=0.8/hiv_scaff_3_20-05_14-43-16  ]
[ Using Seed :  3  ]
[ Using device :  cuda:2  ]
Dataset: hiv
Data: Data(edge_attr=[2259376, 2], edge_index=[2, 2259376], id=[41127], x=[1049163, 2], y=[41127])
MoleculeDataset(41127)
split via scaffold
Data(edge_attr=[32, 2], edge_index=[2, 32], id=[1], x=[16, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.25635985459278676
ROC train: 0.762734	val: 0.770291	test: 0.694730
PRC train: 0.257768	val: 0.228034	test: 0.176119

Epoch: 2
Loss: 0.13944928108778326
ROC train: 0.774872	val: 0.742391	test: 0.727202
PRC train: 0.258498	val: 0.236107	test: 0.204624

Epoch: 3
Loss: 0.13223363916620523
ROC train: 0.801700	val: 0.786241	test: 0.715029
PRC train: 0.351189	val: 0.268856	test: 0.174833

Epoch: 4
Loss: 0.13243018809370824
ROC train: 0.802228	val: 0.760147	test: 0.715736
PRC train: 0.354591	val: 0.290205	test: 0.243808

Epoch: 5
Loss: 0.13027891976073022
ROC train: 0.816914	val: 0.768430	test: 0.717258
PRC train: 0.373150	val: 0.279153	test: 0.142050

Epoch: 6
Loss: 0.12774492700044815
ROC train: 0.818784	val: 0.767459	test: 0.729653
PRC train: 0.355515	val: 0.332521	test: 0.208226

Epoch: 7
Loss: 0.12560024687499083
ROC train: 0.828385	val: 0.749678	test: 0.737411
PRC train: 0.400974	val: 0.294923	test: 0.223354

Epoch: 8
Loss: 0.12330128242158878
ROC train: 0.840473	val: 0.766130	test: 0.739939
PRC train: 0.412273	val: 0.259433	test: 0.164255

Epoch: 9
Loss: 0.12217020418426602
ROC train: 0.835638	val: 0.740680	test: 0.722003
PRC train: 0.392690	val: 0.201450	test: 0.104924

Epoch: 10
Loss: 0.12099334141449217
ROC train: 0.844225	val: 0.762793	test: 0.751432
PRC train: 0.453843	val: 0.326717	test: 0.205816

Epoch: 11
Loss: 0.11907186036863361
ROC train: 0.852548	val: 0.767082	test: 0.732425
PRC train: 0.449826	val: 0.346210	test: 0.212163

Epoch: 12
Loss: 0.11744345114658927
ROC train: 0.856570	val: 0.780818	test: 0.734481
PRC train: 0.468312	val: 0.382132	test: 0.198052

Epoch: 13
Loss: 0.11629628914722653
ROC train: 0.861806	val: 0.753916	test: 0.749964
PRC train: 0.463559	val: 0.333768	test: 0.153216

Epoch: 14
Loss: 0.11415676141107942
ROC train: 0.864147	val: 0.759602	test: 0.735644
PRC train: 0.462010	val: 0.316570	test: 0.141839

Epoch: 15
Loss: 0.11458285330904855
ROC train: 0.869200	val: 0.761975	test: 0.751269
PRC train: 0.487505	val: 0.340295	test: 0.194790

Epoch: 16
Loss: 0.11315553151593676
ROC train: 0.864386	val: 0.776789	test: 0.774221
PRC train: 0.489873	val: 0.387650	test: 0.251843

Epoch: 17
Loss: 0.1128977920236635
ROC train: 0.876461	val: 0.789171	test: 0.758242
PRC train: 0.503381	val: 0.352824	test: 0.220434

Epoch: 18
Loss: 0.11231390656774608
ROC train: 0.879245	val: 0.780117	test: 0.749777
PRC train: 0.503980	val: 0.353617	test: 0.165119

Epoch: 19
Loss: 0.11066153358771254
ROC train: 0.881166	val: 0.783889	test: 0.759285
PRC train: 0.505204	val: 0.357531	test: 0.224685

Epoch: 20
Loss: 0.10999560717495145
ROC train: 0.878844	val: 0.772386	test: 0.713872
PRC train: 0.511524	val: 0.363090	test: 0.209783

Epoch: 21
Loss: 0.10873111171980179
ROC train: 0.890621	val: 0.772646	test: 0.735294
PRC train: 0.535323	val: 0.346319	test: 0.197202

Epoch: 22
Loss: 0.10941747167265829
ROC train: 0.892198	val: 0.798146	test: 0.752326
PRC train: 0.521950	val: 0.345878	test: 0.179071

Epoch: 23
Loss: 0.10972145522364002
ROC train: 0.889744	val: 0.776489	test: 0.732478
PRC train: 0.523176	val: 0.338507	test: 0.139008

Epoch: 24
Loss: 0.10654581145829305
ROC train: 0.891752	val: 0.771492	test: 0.748010
PRC train: 0.530741	val: 0.342788	test: 0.208525

Epoch: 25
Loss: 0.1062525164294839
ROC train: 0.893575	val: 0.774324	test: 0.761423
PRC train: 0.526177	val: 0.356481	test: 0.229179

Epoch: 26
Loss: 0.10630091287924458
ROC train: 0.898194	val: 0.794637	test: 0.764930
PRC train: 0.542330	val: 0.371623	test: 0.214034

Epoch: 27
Loss: 0.1055247000925034
ROC train: 0.909567	val: 0.796930	test: 0.742287
PRC train: 0.557782	val: 0.355397	test: 0.189811

Epoch: 28
Loss: 0.10455878370231504
ROC train: 0.907736	val: 0.799680	test: 0.758504
PRC train: 0.547714	val: 0.330286	test: 0.186879

Epoch: 29
Loss: 0.1038011742847727
ROC train: 0.910467	val: 0.782502	test: 0.749555
PRC train: 0.567520	val: 0.364107	test: 0.189628

Epoch: 30
Loss: 0.10426325569571863
ROC train: 0.906039	val: 0.787028	test: 0.766747
PRC train: 0.552302	val: 0.339586	test: 0.192528

Epoch: 31
Loss: 0.10126695077037148
ROC train: 0.914183	val: 0.802420	test: 0.760418
PRC train: 0.576407	val: 0.317641	test: 0.159653

Epoch: 32
Loss: 0.10236193606389958
ROC train: 0.910801	val: 0.786663	test: 0.766604
PRC train: 0.574443	val: 0.331298	test: 0.191901

Epoch: 33
Loss: 0.10218337570614432Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/hiv/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: hiv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/hiv/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/hiv/scaff/train_prop=0.8/hiv_scaff_2_20-05_14-43-16  ]
[ Using Seed :  2  ]
[ Using device :  cuda:2  ]
Dataset: hiv
Data: Data(edge_attr=[2259376, 2], edge_index=[2, 2259376], id=[41127], x=[1049163, 2], y=[41127])
MoleculeDataset(41127)
split via scaffold
Data(edge_attr=[32, 2], edge_index=[2, 32], id=[1], x=[16, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.25000347984829346
ROC train: 0.764278	val: 0.761188	test: 0.625657
PRC train: 0.221857	val: 0.189442	test: 0.114750

Epoch: 2
Loss: 0.1404242912615072
ROC train: 0.789586	val: 0.771023	test: 0.728927
PRC train: 0.297414	val: 0.250100	test: 0.176701

Epoch: 3
Loss: 0.13515157755197135
ROC train: 0.812001	val: 0.763932	test: 0.701786
PRC train: 0.334840	val: 0.249762	test: 0.151632

Epoch: 4
Loss: 0.13107188289426958
ROC train: 0.817991	val: 0.770631	test: 0.717577
PRC train: 0.338266	val: 0.303242	test: 0.171193

Epoch: 5
Loss: 0.12821494061699384
ROC train: 0.823985	val: 0.779780	test: 0.712766
PRC train: 0.387321	val: 0.324161	test: 0.188673

Epoch: 6
Loss: 0.12746814633215633
ROC train: 0.831341	val: 0.777178	test: 0.734647
PRC train: 0.396728	val: 0.335913	test: 0.198531

Epoch: 7
Loss: 0.12487873758902493
ROC train: 0.833959	val: 0.778433	test: 0.699652
PRC train: 0.423989	val: 0.323123	test: 0.164049

Epoch: 8
Loss: 0.12164973631429922
ROC train: 0.844408	val: 0.746574	test: 0.744644
PRC train: 0.423281	val: 0.285540	test: 0.118840

Epoch: 9
Loss: 0.12253425084756843
ROC train: 0.838324	val: 0.799964	test: 0.748384
PRC train: 0.418571	val: 0.284867	test: 0.190930

Epoch: 10
Loss: 0.11920074164604462
ROC train: 0.854300	val: 0.792533	test: 0.742523
PRC train: 0.425137	val: 0.336869	test: 0.238421

Epoch: 11
Loss: 0.11882125782430195
ROC train: 0.855054	val: 0.770830	test: 0.724111
PRC train: 0.465848	val: 0.350366	test: 0.179895

Epoch: 12
Loss: 0.11679852806003357
ROC train: 0.856323	val: 0.769511	test: 0.764980
PRC train: 0.443779	val: 0.350360	test: 0.157599

Epoch: 13
Loss: 0.11712882797359603
ROC train: 0.862002	val: 0.752391	test: 0.764758
PRC train: 0.471179	val: 0.355626	test: 0.192952

Epoch: 14
Loss: 0.11630426066869405
ROC train: 0.862624	val: 0.781896	test: 0.740870
PRC train: 0.476287	val: 0.343853	test: 0.200327

Epoch: 15
Loss: 0.11312879883123637
ROC train: 0.874824	val: 0.780399	test: 0.749922
PRC train: 0.499023	val: 0.342109	test: 0.190159

Epoch: 16
Loss: 0.11322928954366814
ROC train: 0.871860	val: 0.785387	test: 0.748404
PRC train: 0.498900	val: 0.354203	test: 0.151514

Epoch: 17
Loss: 0.11138938359269956
ROC train: 0.882297	val: 0.776158	test: 0.750308
PRC train: 0.512634	val: 0.365257	test: 0.192152

Epoch: 18
Loss: 0.1110909923917041
ROC train: 0.878054	val: 0.786183	test: 0.735134
PRC train: 0.496251	val: 0.316717	test: 0.118106

Epoch: 19
Loss: 0.11040942029832286
ROC train: 0.883715	val: 0.785313	test: 0.737973
PRC train: 0.509367	val: 0.340229	test: 0.170863

Epoch: 20
Loss: 0.11004233789019305
ROC train: 0.881848	val: 0.790766	test: 0.739078
PRC train: 0.504528	val: 0.298772	test: 0.177836

Epoch: 21
Loss: 0.1090272481768357
ROC train: 0.887077	val: 0.776672	test: 0.765453
PRC train: 0.515239	val: 0.359350	test: 0.160312

Epoch: 22
Loss: 0.10795982275838421
ROC train: 0.884536	val: 0.772732	test: 0.728900
PRC train: 0.520132	val: 0.335089	test: 0.189743

Epoch: 23
Loss: 0.107731444593029
ROC train: 0.891598	val: 0.789294	test: 0.758051
PRC train: 0.529799	val: 0.343874	test: 0.144132

Epoch: 24
Loss: 0.10850158842724567
ROC train: 0.888149	val: 0.763678	test: 0.734263
PRC train: 0.521290	val: 0.338488	test: 0.152622

Epoch: 25
Loss: 0.10559874019325892
ROC train: 0.892785	val: 0.759722	test: 0.735806
PRC train: 0.542229	val: 0.322668	test: 0.160564

Epoch: 26
Loss: 0.10536659747834715
ROC train: 0.893194	val: 0.763221	test: 0.734022
PRC train: 0.525420	val: 0.354187	test: 0.218952

Epoch: 27
Loss: 0.10523319270344979
ROC train: 0.894301	val: 0.751791	test: 0.717312
PRC train: 0.531923	val: 0.286533	test: 0.128002

Epoch: 28
Loss: 0.10622259943147605
ROC train: 0.904219	val: 0.780497	test: 0.746787
PRC train: 0.559412	val: 0.355143	test: 0.170693

Epoch: 29
Loss: 0.10573789404189429
ROC train: 0.905516	val: 0.794135	test: 0.748205
PRC train: 0.574417	val: 0.353080	test: 0.195993

Epoch: 30
Loss: 0.10494685882430578
ROC train: 0.899492	val: 0.781532	test: 0.738446
PRC train: 0.544977	val: 0.352788	test: 0.236012

Epoch: 31
Loss: 0.10268730196502142
ROC train: 0.910162	val: 0.782095	test: 0.748439
PRC train: 0.541166	val: 0.359228	test: 0.165492

Epoch: 32
Loss: 0.10326136001507052
ROC train: 0.913105	val: 0.789900	test: 0.750070
PRC train: 0.570896	val: 0.308224	test: 0.147866

Epoch: 33
Loss: 0.1021772342986633
ROC train: 0.936720	val: 0.698583	test: 0.759484
PRC train: 0.630190	val: 0.208160	test: 0.217948

Epoch: 34
Loss: 0.09731783612197793
ROC train: 0.935949	val: 0.697721	test: 0.756709
PRC train: 0.619845	val: 0.206716	test: 0.235622

Epoch: 35
Loss: 0.09524091836116629
ROC train: 0.935801	val: 0.698788	test: 0.756689
PRC train: 0.615157	val: 0.217187	test: 0.215650

Epoch: 36
Loss: 0.09507995131368059
ROC train: 0.944011	val: 0.690718	test: 0.758999
PRC train: 0.652950	val: 0.222755	test: 0.220846

Epoch: 37
Loss: 0.09330998665371933
ROC train: 0.941442	val: 0.701573	test: 0.749881
PRC train: 0.632852	val: 0.208493	test: 0.199131

Epoch: 38
Loss: 0.09166981224831987
ROC train: 0.947186	val: 0.694037	test: 0.741883
PRC train: 0.640903	val: 0.219579	test: 0.203600

Epoch: 39
Loss: 0.09455761938357497
ROC train: 0.942084	val: 0.699303	test: 0.756545
PRC train: 0.639804	val: 0.232215	test: 0.239247

Epoch: 40
Loss: 0.09169711080900673
ROC train: 0.948597	val: 0.695114	test: 0.762451
PRC train: 0.670874	val: 0.240421	test: 0.258289

Epoch: 41
Loss: 0.09162533286457225
ROC train: 0.934850	val: 0.691115	test: 0.709633
PRC train: 0.606869	val: 0.196994	test: 0.160121

Epoch: 42
Loss: 0.09188232301236404
ROC train: 0.945502	val: 0.686908	test: 0.742323
PRC train: 0.643453	val: 0.208065	test: 0.228338

Epoch: 43
Loss: 0.09075852713261062
ROC train: 0.951234	val: 0.694227	test: 0.746514
PRC train: 0.667377	val: 0.217119	test: 0.218975

Epoch: 44
Loss: 0.08925698296147686
ROC train: 0.953254	val: 0.686972	test: 0.734346
PRC train: 0.666782	val: 0.210642	test: 0.182213

Epoch: 45
Loss: 0.08962481746689664
ROC train: 0.953838	val: 0.695153	test: 0.743168
PRC train: 0.669064	val: 0.214793	test: 0.205847

Epoch: 46
Loss: 0.08913934041512174
ROC train: 0.941814	val: 0.681258	test: 0.738186
PRC train: 0.643828	val: 0.202837	test: 0.188668

Epoch: 47
Loss: 0.08864355981742801
ROC train: 0.949160	val: 0.692652	test: 0.754557
PRC train: 0.647760	val: 0.198006	test: 0.254100

Epoch: 48
Loss: 0.08638786077221239
ROC train: 0.957135	val: 0.685418	test: 0.749043
PRC train: 0.678989	val: 0.217920	test: 0.226375

Epoch: 49
Loss: 0.08924397345904614
ROC train: 0.956139	val: 0.692158	test: 0.740558
PRC train: 0.675271	val: 0.225831	test: 0.235885

Epoch: 50
Loss: 0.08654415942813765
ROC train: 0.959691	val: 0.693348	test: 0.744854
PRC train: 0.691686	val: 0.205695	test: 0.178778

Epoch: 51
Loss: 0.08592133258415331
ROC train: 0.962950	val: 0.695308	test: 0.750298
PRC train: 0.700995	val: 0.227155	test: 0.192045

Epoch: 52
Loss: 0.08663077871878029
ROC train: 0.963350	val: 0.698968	test: 0.749207
PRC train: 0.706654	val: 0.240859	test: 0.230277

Epoch: 53
Loss: 0.08515593986681907
ROC train: 0.961609	val: 0.684159	test: 0.734116
PRC train: 0.681979	val: 0.202460	test: 0.167722

Epoch: 54
Loss: 0.08442952975998316
ROC train: 0.960437	val: 0.690409	test: 0.752050
PRC train: 0.685839	val: 0.192321	test: 0.230276

Epoch: 55
Loss: 0.08447430999574941
ROC train: 0.963867	val: 0.693447	test: 0.739963
PRC train: 0.697667	val: 0.219769	test: 0.217433

Epoch: 56
Loss: 0.0833070304771439
ROC train: 0.964468	val: 0.696911	test: 0.745585
PRC train: 0.706012	val: 0.217267	test: 0.203837

Epoch: 57
Loss: 0.08500307457142263
ROC train: 0.963378	val: 0.700730	test: 0.752258
PRC train: 0.703940	val: 0.229728	test: 0.262772

Epoch: 58
Loss: 0.08271362915206669
ROC train: 0.963750	val: 0.686923	test: 0.741697
PRC train: 0.704621	val: 0.216535	test: 0.162822

Epoch: 59
Loss: 0.08315904001763051
ROC train: 0.965811	val: 0.692390	test: 0.752662
PRC train: 0.716451	val: 0.223298	test: 0.201028

Epoch: 60
Loss: 0.08318110232048455
ROC train: 0.953303	val: 0.686243	test: 0.733240
PRC train: 0.686395	val: 0.208081	test: 0.150919

Epoch: 61
Loss: 0.08370993689699663
ROC train: 0.967555	val: 0.694409	test: 0.745527
PRC train: 0.712281	val: 0.220906	test: 0.195202

Epoch: 62
Loss: 0.08096856807038239
ROC train: 0.968218	val: 0.687893	test: 0.749113
PRC train: 0.717297	val: 0.226167	test: 0.228305

Epoch: 63
Loss: 0.08330474832344759
ROC train: 0.968340	val: 0.694511	test: 0.748129
PRC train: 0.716036	val: 0.226178	test: 0.255993

Epoch: 64
Loss: 0.0817443503553311
ROC train: 0.970012	val: 0.692490	test: 0.742673
PRC train: 0.718468	val: 0.211416	test: 0.193882

Epoch: 65
Loss: 0.08129462743760904
ROC train: 0.964977	val: 0.702159	test: 0.753364
PRC train: 0.698441	val: 0.215545	test: 0.240435

Epoch: 66
Loss: 0.08119444812854439
ROC train: 0.971475	val: 0.701730	test: 0.750446
PRC train: 0.733830	val: 0.233009	test: 0.225162

Epoch: 67
Loss: 0.07911886095060476
ROC train: 0.972095	val: 0.707818	test: 0.755621
PRC train: 0.741052	val: 0.248025	test: 0.201998

Epoch: 68
Loss: 0.0795136394555296
ROC train: 0.971870	val: 0.699367	test: 0.750761
PRC train: 0.738044	val: 0.245347	test: 0.223689

Epoch: 69
Loss: 0.07922232391812249
ROC train: 0.969885	val: 0.694442	test: 0.744172
PRC train: 0.721210	val: 0.209073	test: 0.201086

Epoch: 70
Loss: 0.07872053969951785
ROC train: 0.974419	val: 0.685044	test: 0.751894
PRC train: 0.751779	val: 0.232297	test: 0.219097

Epoch: 71
Loss: 0.07856414997886119
ROC train: 0.974460	val: 0.687092	test: 0.749236
PRC train: 0.737860	val: 0.190048	test: 0.177127

Epoch: 72
Loss: 0.07648572607900592
ROC train: 0.971084	val: 0.692099	test: 0.750584
PRC train: 0.737890	val: 0.211527	test: 0.179815

Epoch: 73
Loss: 0.07852667910422184
ROC train: 0.974789	val: 0.696769	test: 0.749231
PRC train: 0.747406	val: 0.199194	test: 0.187959

Epoch: 74
Loss: 0.07690792931718392
ROC train: 0.973791	val: 0.691189	test: 0.755865
PRC train: 0.750828	val: 0.229281	test: 0.251066

Epoch: 75
Loss: 0.07762140555964255
ROC train: 0.976159	val: 0.701973	test: 0.750316
PRC train: 0.756646	val: 0.242695	test: 0.207148

Epoch: 76
Loss: 0.07579149797833104
ROC train: 0.976925	val: 0.697439	test: 0.747638
PRC train: 0.754385	val: 0.225102	test: 0.188389

Epoch: 77
Loss: 0.07626175756928087
ROC train: 0.977094	val: 0.697370	test: 0.741662
PRC train: 0.751808	val: 0.210258	test: 0.185621

Epoch: 78
Loss: 0.07701080240065027
ROC train: 0.978684	val: 0.698452	test: 0.750956
PRC train: 0.775892	val: 0.235705	test: 0.202066

Epoch: 79
Loss: 0.07619240048492906
ROC train: 0.976719	val: 0.693635	test: 0.738684
PRC train: 0.751403	val: 0.213572	test: 0.149301

Epoch: 80
Loss: 0.07548115723493545
ROC train: 0.977934	val: 0.702616	test: 0.760169
PRC train: 0.770080	val: 0.229449	test: 0.215092

Epoch: 81
Loss: 0.07527510180718285
ROC train: 0.976757	val: 0.693530	test: 0.747734
PRC train: 0.747682	val: 0.213194	test: 0.162086

Epoch: 82
Loss: 0.07549307050996115
ROC train: 0.974597	val: 0.690004	test: 0.732529
PRC train: 0.752722	val: 0.241960	test: 0.236564

Epoch: 83
Loss: 0.07390968352184067
ROC train: 0.977305	val: 0.686886	test: 0.752087
PRC train: 0.756996	val: 0.230697	test: 0.208306

Epoch: 84
Loss: 0.07629290019586374
ROC train: 0.981376	val: 0.692137	test: 0.745441
PRC train: 0.787934	val: 0.233967	test: 0.205536

Epoch: 85
Loss: 0.07108483269071407
ROC train: 0.982160	val: 0.690604	test: 0.744259
PRC train: 0.797527	val: 0.240852	test: 0.218676

Epoch: 86
Loss: 0.07267419048193768
ROC train: 0.980266	val: 0.693017	test: 0.749328
PRC train: 0.776132	val: 0.244916	test: 0.199270

Epoch: 87
Loss: 0.0731104479828799
ROC train: 0.978455	val: 0.693077	test: 0.750212
PRC train: 0.771506	val: 0.241903	test: 0.173276

Epoch: 88
Loss: 0.07425871044329771
ROC train: 0.977452	val: 0.690038	test: 0.724172
PRC train: 0.763838	val: 0.220377	test: 0.128510

Epoch: 89
Loss: 0.07408451057790873
ROC train: 0.978734	val: 0.689440	test: 0.735543
PRC train: 0.771668	val: 0.223466	test: 0.171552

Epoch: 90
Loss: 0.07170226946714708
ROC train: 0.982982	val: 0.689092	test: 0.739861
PRC train: 0.796860	val: 0.211839	test: 0.183030

Epoch: 91
Loss: 0.0715778871770012
ROC train: 0.979309	val: 0.686546	test: 0.745626
PRC train: 0.775866	val: 0.209754	test: 0.149896

Epoch: 92
Loss: 0.07260770231146349
ROC train: 0.982725	val: 0.692513	test: 0.748309
PRC train: 0.790260	val: 0.228337	test: 0.182311

Epoch: 93
Loss: 0.07089329597506347
ROC train: 0.982413	val: 0.689581	test: 0.748770
PRC train: 0.792497	val: 0.212560	test: 0.180705
ROC train: 0.934982	val: 0.699466	test: 0.751493
PRC train: 0.632562	val: 0.222932	test: 0.203689

Epoch: 34
Loss: 0.09421572314353677
ROC train: 0.935955	val: 0.688936	test: 0.739040
PRC train: 0.633218	val: 0.228406	test: 0.199087

Epoch: 35
Loss: 0.0931929863157583
ROC train: 0.942098	val: 0.704664	test: 0.748084
PRC train: 0.641477	val: 0.225788	test: 0.234988

Epoch: 36
Loss: 0.09505996379267219
ROC train: 0.942943	val: 0.702609	test: 0.754158
PRC train: 0.644547	val: 0.233276	test: 0.220862

Epoch: 37
Loss: 0.09502458084197116
ROC train: 0.942491	val: 0.698211	test: 0.746521
PRC train: 0.642925	val: 0.221757	test: 0.195104

Epoch: 38
Loss: 0.09346663301262725
ROC train: 0.948253	val: 0.690181	test: 0.745768
PRC train: 0.659580	val: 0.225001	test: 0.227677

Epoch: 39
Loss: 0.09183097566506915
ROC train: 0.945956	val: 0.681637	test: 0.742969
PRC train: 0.657656	val: 0.221545	test: 0.184755

Epoch: 40
Loss: 0.08995705922022208
ROC train: 0.943634	val: 0.700794	test: 0.743569
PRC train: 0.642114	val: 0.236018	test: 0.183709

Epoch: 41
Loss: 0.0912185878087741
ROC train: 0.949942	val: 0.708101	test: 0.739552
PRC train: 0.656240	val: 0.233510	test: 0.169353

Epoch: 42
Loss: 0.09210465495584323
ROC train: 0.948460	val: 0.701977	test: 0.754440
PRC train: 0.657422	val: 0.242282	test: 0.209677

Epoch: 43
Loss: 0.09089126468018059
ROC train: 0.954368	val: 0.701495	test: 0.743418
PRC train: 0.674449	val: 0.235245	test: 0.193376

Epoch: 44
Loss: 0.08848333816159337
ROC train: 0.954383	val: 0.703043	test: 0.747895
PRC train: 0.666111	val: 0.225218	test: 0.199433

Epoch: 45
Loss: 0.08756638598636873
ROC train: 0.953974	val: 0.702543	test: 0.734880
PRC train: 0.665712	val: 0.224393	test: 0.177869

Epoch: 46
Loss: 0.08851703066358826
ROC train: 0.953954	val: 0.691110	test: 0.744727
PRC train: 0.675438	val: 0.232563	test: 0.185100

Epoch: 47
Loss: 0.08896311466497103
ROC train: 0.953805	val: 0.699810	test: 0.742551
PRC train: 0.672582	val: 0.233846	test: 0.166208

Epoch: 48
Loss: 0.0860801977260989
ROC train: 0.954687	val: 0.689635	test: 0.735398
PRC train: 0.672274	val: 0.235844	test: 0.193117

Epoch: 49
Loss: 0.08820713921754525
ROC train: 0.956563	val: 0.709677	test: 0.752368
PRC train: 0.682340	val: 0.222905	test: 0.167273

Epoch: 50
Loss: 0.08784168306575281
ROC train: 0.957221	val: 0.697039	test: 0.748644
PRC train: 0.679156	val: 0.239087	test: 0.216522

Epoch: 51
Loss: 0.08674351259747771
ROC train: 0.958809	val: 0.694095	test: 0.738486
PRC train: 0.696552	val: 0.232176	test: 0.216211

Epoch: 52
Loss: 0.08797252724962817
ROC train: 0.959360	val: 0.690339	test: 0.739646
PRC train: 0.694802	val: 0.222291	test: 0.182907

Epoch: 53
Loss: 0.08500417065922689
ROC train: 0.960098	val: 0.689819	test: 0.721024
PRC train: 0.701089	val: 0.224399	test: 0.193118

Epoch: 54
Loss: 0.0850205806002755
ROC train: 0.956414	val: 0.700355	test: 0.730956
PRC train: 0.683666	val: 0.230306	test: 0.186069

Epoch: 55
Loss: 0.08530467924577186
ROC train: 0.963686	val: 0.698556	test: 0.748072
PRC train: 0.712406	val: 0.216810	test: 0.197266

Epoch: 56
Loss: 0.08460968410404104
ROC train: 0.959334	val: 0.696034	test: 0.744598
PRC train: 0.707283	val: 0.241474	test: 0.190241

Epoch: 57
Loss: 0.0833110316963731
ROC train: 0.965987	val: 0.696398	test: 0.746002
PRC train: 0.709011	val: 0.228331	test: 0.189859

Epoch: 58
Loss: 0.08348364612133552
ROC train: 0.958023	val: 0.702864	test: 0.741640
PRC train: 0.704307	val: 0.224486	test: 0.165333

Epoch: 59
Loss: 0.08288493519654755
ROC train: 0.965418	val: 0.708872	test: 0.759309
PRC train: 0.715690	val: 0.220498	test: 0.211009

Epoch: 60
Loss: 0.08413053500853455
ROC train: 0.966788	val: 0.698766	test: 0.745203
PRC train: 0.715875	val: 0.213669	test: 0.151581

Epoch: 61
Loss: 0.0825099852049673
ROC train: 0.968122	val: 0.696779	test: 0.754519
PRC train: 0.720546	val: 0.216462	test: 0.194319

Epoch: 62
Loss: 0.08358573158586713
ROC train: 0.969427	val: 0.700941	test: 0.756423
PRC train: 0.726814	val: 0.234772	test: 0.181069

Epoch: 63
Loss: 0.08140221659591178
ROC train: 0.970032	val: 0.699134	test: 0.749465
PRC train: 0.733141	val: 0.225073	test: 0.213903

Epoch: 64
Loss: 0.08182701909587592
ROC train: 0.970689	val: 0.693440	test: 0.735876
PRC train: 0.735708	val: 0.224371	test: 0.179249

Epoch: 65
Loss: 0.08121463946857334
ROC train: 0.969794	val: 0.699130	test: 0.756946
PRC train: 0.732401	val: 0.225336	test: 0.219694

Epoch: 66
Loss: 0.07906763241771539
ROC train: 0.971416	val: 0.710905	test: 0.749605
PRC train: 0.739849	val: 0.233897	test: 0.212540

Epoch: 67
Loss: 0.07911578950312348
ROC train: 0.970281	val: 0.698761	test: 0.747657
PRC train: 0.732134	val: 0.212754	test: 0.171463

Epoch: 68
Loss: 0.0801126866142109
ROC train: 0.973466	val: 0.714835	test: 0.767278
PRC train: 0.745090	val: 0.211244	test: 0.234078

Epoch: 69
Loss: 0.07802602751944526
ROC train: 0.971634	val: 0.699018	test: 0.748399
PRC train: 0.740083	val: 0.213825	test: 0.202155

Epoch: 70
Loss: 0.07983863455939975
ROC train: 0.970417	val: 0.697549	test: 0.750709
PRC train: 0.739069	val: 0.229022	test: 0.213646

Epoch: 71
Loss: 0.07731639584133504
ROC train: 0.974970	val: 0.694682	test: 0.753081
PRC train: 0.758873	val: 0.212328	test: 0.203426

Epoch: 72
Loss: 0.0794702561947425
ROC train: 0.974939	val: 0.702992	test: 0.765980
PRC train: 0.760013	val: 0.218585	test: 0.222712

Epoch: 73
Loss: 0.07675785066491357
ROC train: 0.971510	val: 0.700866	test: 0.756617
PRC train: 0.739073	val: 0.212869	test: 0.246073

Epoch: 74
Loss: 0.07924996543349705
ROC train: 0.974345	val: 0.694604	test: 0.747838
PRC train: 0.760159	val: 0.218998	test: 0.179778

Epoch: 75
Loss: 0.07627096481768471
ROC train: 0.976059	val: 0.720864	test: 0.768133
PRC train: 0.767063	val: 0.215577	test: 0.229695

Epoch: 76
Loss: 0.0774725866332541
ROC train: 0.977197	val: 0.694388	test: 0.744253
PRC train: 0.769123	val: 0.200796	test: 0.212641

Epoch: 77
Loss: 0.07468154645537965
ROC train: 0.975758	val: 0.702102	test: 0.759880
PRC train: 0.755296	val: 0.210667	test: 0.236641

Epoch: 78
Loss: 0.0770945960190833
ROC train: 0.975276	val: 0.694548	test: 0.748390
PRC train: 0.757733	val: 0.174930	test: 0.191732

Epoch: 79
Loss: 0.07497377267483306
ROC train: 0.976508	val: 0.698744	test: 0.743207
PRC train: 0.767342	val: 0.202850	test: 0.156519

Epoch: 80
Loss: 0.07489112045697228
ROC train: 0.979093	val: 0.697046	test: 0.760350
PRC train: 0.777548	val: 0.226093	test: 0.198650

Epoch: 81
Loss: 0.0744406946012357
ROC train: 0.978502	val: 0.703631	test: 0.755332
PRC train: 0.778400	val: 0.214325	test: 0.202872

Epoch: 82
Loss: 0.07567078581280612
ROC train: 0.979577	val: 0.695466	test: 0.753606
PRC train: 0.781560	val: 0.216380	test: 0.194828

Epoch: 83
Loss: 0.07338653148239649
ROC train: 0.979862	val: 0.706334	test: 0.755411
PRC train: 0.784012	val: 0.242094	test: 0.230223

Epoch: 84
Loss: 0.07381744042665336
ROC train: 0.980217	val: 0.692984	test: 0.739699
PRC train: 0.780929	val: 0.199772	test: 0.162605

Epoch: 85
Loss: 0.07249153552261198
ROC train: 0.973554	val: 0.702343	test: 0.734481
PRC train: 0.756437	val: 0.204491	test: 0.164363

Epoch: 86
Loss: 0.07372470040268178
ROC train: 0.979716	val: 0.707393	test: 0.752069
PRC train: 0.788066	val: 0.252180	test: 0.218180

Epoch: 87
Loss: 0.07369799980862599
ROC train: 0.979705	val: 0.697730	test: 0.757389
PRC train: 0.786905	val: 0.205398	test: 0.204290

Epoch: 88
Loss: 0.07128911374558342
ROC train: 0.977795	val: 0.699563	test: 0.751597
PRC train: 0.772694	val: 0.223218	test: 0.199942

Epoch: 89
Loss: 0.07207810639726733
ROC train: 0.979250	val: 0.704609	test: 0.756640
PRC train: 0.781462	val: 0.232066	test: 0.211618

Epoch: 90
Loss: 0.07204963248506267
ROC train: 0.976510	val: 0.706249	test: 0.756336
PRC train: 0.763712	val: 0.205529	test: 0.197203

Epoch: 91
Loss: 0.07153151503369973
ROC train: 0.980246	val: 0.694704	test: 0.760033
PRC train: 0.783775	val: 0.229572	test: 0.191473

Epoch: 92
Loss: 0.07313737802950541
ROC train: 0.982081	val: 0.704449	test: 0.758579
PRC train: 0.794958	val: 0.243704	test: 0.213637

Epoch: 93
Loss: 0.07138600964740305
ROC train: 0.984118	val: 0.705610	test: 0.757910
PRC train: 0.804866	val: 0.217884	test: 0.215616

Epoch: 94
Loss: 0.07059865488348839
ROC train: 0.931857	val: 0.692423	test: 0.741698
PRC train: 0.627933	val: 0.232959	test: 0.182763

Epoch: 34
Loss: 0.09467899258806294
ROC train: 0.936218	val: 0.705703	test: 0.724033
PRC train: 0.630408	val: 0.241895	test: 0.184563

Epoch: 35
Loss: 0.09577589461524602
ROC train: 0.938719	val: 0.706249	test: 0.740468
PRC train: 0.632014	val: 0.222332	test: 0.233324

Epoch: 36
Loss: 0.09385851977635004
ROC train: 0.936113	val: 0.698304	test: 0.729737
PRC train: 0.638608	val: 0.210366	test: 0.206729

Epoch: 37
Loss: 0.09440895531625541
ROC train: 0.935518	val: 0.702605	test: 0.727397
PRC train: 0.635803	val: 0.231161	test: 0.214875

Epoch: 38
Loss: 0.09237083227488863
ROC train: 0.943350	val: 0.694789	test: 0.726451
PRC train: 0.657118	val: 0.227858	test: 0.205797

Epoch: 39
Loss: 0.09242135838286716
ROC train: 0.945027	val: 0.702690	test: 0.722938
PRC train: 0.661185	val: 0.228525	test: 0.184440

Epoch: 40
Loss: 0.09212896465416812
ROC train: 0.946865	val: 0.698562	test: 0.731403
PRC train: 0.662444	val: 0.229588	test: 0.233141

Epoch: 41
Loss: 0.09066058693850672
ROC train: 0.947228	val: 0.708168	test: 0.733692
PRC train: 0.665373	val: 0.227071	test: 0.191064

Epoch: 42
Loss: 0.09037209312615199
ROC train: 0.933038	val: 0.705656	test: 0.727334
PRC train: 0.619048	val: 0.213063	test: 0.172908

Epoch: 43
Loss: 0.08973930476428987
ROC train: 0.942932	val: 0.698130	test: 0.738877
PRC train: 0.657522	val: 0.227545	test: 0.244956

Epoch: 44
Loss: 0.0906511301872609
ROC train: 0.949520	val: 0.706620	test: 0.739679
PRC train: 0.670262	val: 0.219208	test: 0.229106

Epoch: 45
Loss: 0.08932099260591687
ROC train: 0.949271	val: 0.711590	test: 0.734424
PRC train: 0.665092	val: 0.228076	test: 0.160472

Epoch: 46
Loss: 0.08835573991749104
ROC train: 0.948362	val: 0.708119	test: 0.736825
PRC train: 0.678711	val: 0.241056	test: 0.216893

Epoch: 47
Loss: 0.0878011650330266
ROC train: 0.952450	val: 0.699149	test: 0.734949
PRC train: 0.672366	val: 0.227928	test: 0.227532

Epoch: 48
Loss: 0.08732981791524237
ROC train: 0.954875	val: 0.717400	test: 0.736000
PRC train: 0.690578	val: 0.218954	test: 0.185770

Epoch: 49
Loss: 0.08677742903657402
ROC train: 0.958721	val: 0.710319	test: 0.728859
PRC train: 0.693924	val: 0.221275	test: 0.205255

Epoch: 50
Loss: 0.08736647649843224
ROC train: 0.955897	val: 0.699203	test: 0.726386
PRC train: 0.691775	val: 0.231749	test: 0.205301

Epoch: 51
Loss: 0.08624498584514342
ROC train: 0.959666	val: 0.712012	test: 0.741222
PRC train: 0.696236	val: 0.238847	test: 0.240210

Epoch: 52
Loss: 0.0849615830822947
ROC train: 0.951595	val: 0.707559	test: 0.734043
PRC train: 0.681270	val: 0.220782	test: 0.192919

Epoch: 53
Loss: 0.08874345196076638
ROC train: 0.960921	val: 0.706522	test: 0.735975
PRC train: 0.707721	val: 0.244313	test: 0.198986

Epoch: 54
Loss: 0.08467383737559159
ROC train: 0.957256	val: 0.702714	test: 0.731841
PRC train: 0.686576	val: 0.198398	test: 0.171643

Epoch: 55
Loss: 0.08720315621820983
ROC train: 0.952870	val: 0.706957	test: 0.742010
PRC train: 0.678538	val: 0.230693	test: 0.201597

Epoch: 56
Loss: 0.08511330652420239
ROC train: 0.958754	val: 0.705628	test: 0.749892
PRC train: 0.698791	val: 0.218741	test: 0.238179

Epoch: 57
Loss: 0.08286232518353448
ROC train: 0.962849	val: 0.697495	test: 0.734910
PRC train: 0.703144	val: 0.219374	test: 0.212623

Epoch: 58
Loss: 0.08436920108629903
ROC train: 0.963667	val: 0.704766	test: 0.740172
PRC train: 0.714595	val: 0.224780	test: 0.210721

Epoch: 59
Loss: 0.08370480231100313
ROC train: 0.958758	val: 0.687699	test: 0.728682
PRC train: 0.695116	val: 0.232571	test: 0.181016

Epoch: 60
Loss: 0.08544033826232357
ROC train: 0.966187	val: 0.699262	test: 0.739325
PRC train: 0.720838	val: 0.249233	test: 0.203969

Epoch: 61
Loss: 0.08241474320804039
ROC train: 0.965248	val: 0.710418	test: 0.748086
PRC train: 0.722172	val: 0.231212	test: 0.239804

Epoch: 62
Loss: 0.0839037114032691
ROC train: 0.966546	val: 0.700594	test: 0.752548
PRC train: 0.726728	val: 0.222716	test: 0.219247

Epoch: 63
Loss: 0.08090433393034041
ROC train: 0.966227	val: 0.711278	test: 0.753277
PRC train: 0.727028	val: 0.233207	test: 0.228228

Epoch: 64
Loss: 0.08120242994452759
ROC train: 0.964626	val: 0.701286	test: 0.733571
PRC train: 0.713503	val: 0.234618	test: 0.214334

Epoch: 65
Loss: 0.081926226542398
ROC train: 0.969830	val: 0.702226	test: 0.736487
PRC train: 0.735269	val: 0.191252	test: 0.212732

Epoch: 66
Loss: 0.07842719915492859
ROC train: 0.970897	val: 0.709156	test: 0.737686
PRC train: 0.739181	val: 0.218861	test: 0.221712

Epoch: 67
Loss: 0.08155999898130264
ROC train: 0.971024	val: 0.707348	test: 0.735347
PRC train: 0.738908	val: 0.224800	test: 0.206131

Epoch: 68
Loss: 0.0806617198476004
ROC train: 0.967742	val: 0.706705	test: 0.729915
PRC train: 0.727341	val: 0.225073	test: 0.199172

Epoch: 69
Loss: 0.08072933769930149
ROC train: 0.972370	val: 0.728152	test: 0.752675
PRC train: 0.736438	val: 0.224044	test: 0.251102

Epoch: 70
Loss: 0.08036246543673681
ROC train: 0.974225	val: 0.704957	test: 0.728729
PRC train: 0.750969	val: 0.224073	test: 0.187514

Epoch: 71
Loss: 0.07800025376557213
ROC train: 0.974978	val: 0.708761	test: 0.731582
PRC train: 0.761776	val: 0.233546	test: 0.215236

Epoch: 72
Loss: 0.07900070578076981
ROC train: 0.974552	val: 0.713366	test: 0.740175
PRC train: 0.758611	val: 0.216575	test: 0.210737

Epoch: 73
Loss: 0.07835438725496781
ROC train: 0.974358	val: 0.709279	test: 0.736052
PRC train: 0.742895	val: 0.233124	test: 0.239132

Epoch: 74
Loss: 0.07722203519745556
ROC train: 0.973364	val: 0.700601	test: 0.730616
PRC train: 0.760588	val: 0.240897	test: 0.190716

Epoch: 75
Loss: 0.07705597061022983
ROC train: 0.973607	val: 0.710346	test: 0.741332
PRC train: 0.753837	val: 0.247572	test: 0.184836

Epoch: 76
Loss: 0.07714666333894066
ROC train: 0.974512	val: 0.708951	test: 0.759148
PRC train: 0.757041	val: 0.253726	test: 0.241028

Epoch: 77
Loss: 0.07579111821463223
ROC train: 0.971270	val: 0.693484	test: 0.738667
PRC train: 0.750308	val: 0.244786	test: 0.201594

Epoch: 78
Loss: 0.07587015455399934
ROC train: 0.978952	val: 0.702641	test: 0.745572
PRC train: 0.780406	val: 0.241808	test: 0.229465

Epoch: 79
Loss: 0.07596489752407602
ROC train: 0.976135	val: 0.716814	test: 0.744619
PRC train: 0.761328	val: 0.243519	test: 0.247595

Epoch: 80
Loss: 0.07536614529382066
ROC train: 0.976238	val: 0.714879	test: 0.735543
PRC train: 0.779602	val: 0.226813	test: 0.218957

Epoch: 81
Loss: 0.07388171251575917
ROC train: 0.978104	val: 0.714861	test: 0.742137
PRC train: 0.775564	val: 0.232357	test: 0.210450

Epoch: 82
Loss: 0.0744303757480203
ROC train: 0.979484	val: 0.713045	test: 0.731579
PRC train: 0.781859	val: 0.241213	test: 0.196806

Epoch: 83
Loss: 0.07442052107725601
ROC train: 0.973977	val: 0.708523	test: 0.738655
PRC train: 0.760565	val: 0.233826	test: 0.192451

Epoch: 84
Loss: 0.07497421140781364
ROC train: 0.979524	val: 0.717161	test: 0.742680
PRC train: 0.784693	val: 0.231044	test: 0.215834

Epoch: 85
Loss: 0.07501075170266776
ROC train: 0.978805	val: 0.698591	test: 0.752721
PRC train: 0.781627	val: 0.231831	test: 0.206606

Epoch: 86
Loss: 0.07262465541697545
ROC train: 0.981610	val: 0.713812	test: 0.750128
PRC train: 0.791887	val: 0.244257	test: 0.258446

Epoch: 87
Loss: 0.07303814156529932
ROC train: 0.982288	val: 0.706618	test: 0.741079
PRC train: 0.803009	val: 0.230147	test: 0.193409

Epoch: 88
Loss: 0.07330599964361673
ROC train: 0.979001	val: 0.705768	test: 0.751287
PRC train: 0.783187	val: 0.229500	test: 0.191701

Epoch: 89
Loss: 0.07217382681283199
ROC train: 0.981165	val: 0.715280	test: 0.755327
PRC train: 0.786791	val: 0.245614	test: 0.218442

Epoch: 90
Loss: 0.0709226005313083
ROC train: 0.982005	val: 0.709494	test: 0.745833
PRC train: 0.795972	val: 0.247115	test: 0.240871

Epoch: 91
Loss: 0.07210520792354892
ROC train: 0.980641	val: 0.716614	test: 0.750055
PRC train: 0.789684	val: 0.229350	test: 0.183229

Epoch: 92
Loss: 0.07100971619912635
ROC train: 0.982761	val: 0.711123	test: 0.741569
PRC train: 0.806299	val: 0.230435	test: 0.240706

Epoch: 93
Loss: 0.07009275306156998
ROC train: 0.980258	val: 0.720658	test: 0.750785
PRC train: 0.797187	val: 0.241496	test: 0.241690

Epoch: 94
Loss: 0.07100314054570235
ROC train: 0.933576	val: 0.722736	test: 0.748427
PRC train: 0.613003	val: 0.297841	test: 0.183889

Epoch: 34
Loss: 0.10006671903470564
ROC train: 0.932672	val: 0.724655	test: 0.749578
PRC train: 0.611184	val: 0.299016	test: 0.137613

Epoch: 35
Loss: 0.09743826007584815
ROC train: 0.925450	val: 0.720737	test: 0.729169
PRC train: 0.596942	val: 0.308051	test: 0.188078

Epoch: 36
Loss: 0.0984951438350253
ROC train: 0.934483	val: 0.701570	test: 0.762363
PRC train: 0.609461	val: 0.263604	test: 0.187671

Epoch: 37
Loss: 0.0973816426881253
ROC train: 0.933990	val: 0.722369	test: 0.742003
PRC train: 0.615755	val: 0.279055	test: 0.191791

Epoch: 38
Loss: 0.0963463876746629
ROC train: 0.934492	val: 0.706788	test: 0.744028
PRC train: 0.612168	val: 0.287004	test: 0.170991

Epoch: 39
Loss: 0.09655195589739121
ROC train: 0.935880	val: 0.713888	test: 0.753118
PRC train: 0.613242	val: 0.308572	test: 0.166900

Epoch: 40
Loss: 0.09634813334177574
ROC train: 0.935622	val: 0.713950	test: 0.739670
PRC train: 0.636150	val: 0.275996	test: 0.145106

Epoch: 41
Loss: 0.09448657686066679
ROC train: 0.934365	val: 0.730986	test: 0.749677
PRC train: 0.638254	val: 0.299996	test: 0.169178

Epoch: 42
Loss: 0.09522845811079961
ROC train: 0.937925	val: 0.718782	test: 0.765788
PRC train: 0.620937	val: 0.309962	test: 0.214264

Epoch: 43
Loss: 0.09388074146094846
ROC train: 0.941359	val: 0.735006	test: 0.748476
PRC train: 0.626824	val: 0.301371	test: 0.178858

Epoch: 44
Loss: 0.09448199655486302
ROC train: 0.931196	val: 0.716893	test: 0.755723
PRC train: 0.612180	val: 0.277805	test: 0.169511

Epoch: 45
Loss: 0.09343001209345421
ROC train: 0.946793	val: 0.717819	test: 0.764728
PRC train: 0.650075	val: 0.306351	test: 0.194640

Epoch: 46
Loss: 0.09542278873577253
ROC train: 0.946008	val: 0.723450	test: 0.775800
PRC train: 0.642262	val: 0.315985	test: 0.203311

Epoch: 47
Loss: 0.0921839718290625
ROC train: 0.943634	val: 0.720941	test: 0.739474
PRC train: 0.652183	val: 0.283684	test: 0.157360

Epoch: 48
Loss: 0.09134706032310738
ROC train: 0.945645	val: 0.716144	test: 0.748082
PRC train: 0.662841	val: 0.285459	test: 0.160342

Epoch: 49
Loss: 0.09226622023005353
ROC train: 0.946112	val: 0.721799	test: 0.772689
PRC train: 0.638534	val: 0.283813	test: 0.148274

Epoch: 50
Loss: 0.09187615527616033
ROC train: 0.953617	val: 0.723947	test: 0.754492
PRC train: 0.670638	val: 0.304932	test: 0.169651

Epoch: 51
Loss: 0.09068798713342234
ROC train: 0.951816	val: 0.721931	test: 0.760974
PRC train: 0.662206	val: 0.281227	test: 0.182931

Epoch: 52
Loss: 0.09014304852358421
ROC train: 0.949560	val: 0.724249	test: 0.754781
PRC train: 0.671564	val: 0.278353	test: 0.147203

Epoch: 53
Loss: 0.08824032560855011
ROC train: 0.956156	val: 0.726956	test: 0.762862
PRC train: 0.677450	val: 0.298974	test: 0.180826

Epoch: 54
Loss: 0.0892092659636701
ROC train: 0.958412	val: 0.732049	test: 0.756626
PRC train: 0.684298	val: 0.266925	test: 0.120262

Epoch: 55
Loss: 0.08863762888665419
ROC train: 0.953206	val: 0.723392	test: 0.760748
PRC train: 0.653382	val: 0.281579	test: 0.162517

Epoch: 56
Loss: 0.08990657516465497
ROC train: 0.954638	val: 0.711694	test: 0.759146
PRC train: 0.673430	val: 0.269636	test: 0.142923

Epoch: 57
Loss: 0.08775223259879357
ROC train: 0.955613	val: 0.711093	test: 0.780327
PRC train: 0.671459	val: 0.301247	test: 0.182258

Epoch: 58
Loss: 0.08853206964030821
ROC train: 0.954644	val: 0.723285	test: 0.751869
PRC train: 0.674232	val: 0.290516	test: 0.156747

Epoch: 59
Loss: 0.0864262306514384
ROC train: 0.956355	val: 0.730652	test: 0.746576
PRC train: 0.678382	val: 0.290368	test: 0.115351

Epoch: 60
Loss: 0.08638758241950156
ROC train: 0.956074	val: 0.712054	test: 0.761718
PRC train: 0.680585	val: 0.256005	test: 0.154646

Epoch: 61
Loss: 0.08628616119806733
ROC train: 0.961722	val: 0.720428	test: 0.778648
PRC train: 0.700857	val: 0.299955	test: 0.170927

Epoch: 62
Loss: 0.08703959490065223
ROC train: 0.958777	val: 0.714691	test: 0.757997
PRC train: 0.694632	val: 0.289543	test: 0.170186

Epoch: 63
Loss: 0.08495750826818686
ROC train: 0.961359	val: 0.720540	test: 0.760947
PRC train: 0.696257	val: 0.301897	test: 0.164731

Epoch: 64
Loss: 0.0851228640852545
ROC train: 0.956645	val: 0.717436	test: 0.769965
PRC train: 0.697489	val: 0.289123	test: 0.152840

Epoch: 65
Loss: 0.08316687065276193
ROC train: 0.957757	val: 0.710992	test: 0.741551
PRC train: 0.693022	val: 0.263919	test: 0.127490

Epoch: 66
Loss: 0.08531417388254012
ROC train: 0.959107	val: 0.712034	test: 0.758527
PRC train: 0.694465	val: 0.261339	test: 0.160384

Epoch: 67
Loss: 0.08576538874462883
ROC train: 0.962973	val: 0.713796	test: 0.741601
PRC train: 0.697847	val: 0.271244	test: 0.141484

Epoch: 68
Loss: 0.0829861403778792
ROC train: 0.958432	val: 0.722982	test: 0.753917
PRC train: 0.686593	val: 0.268923	test: 0.167359

Epoch: 69
Loss: 0.08245408756157978
ROC train: 0.968428	val: 0.711959	test: 0.761824
PRC train: 0.715870	val: 0.275324	test: 0.146019

Epoch: 70
Loss: 0.08255807521037388
ROC train: 0.967546	val: 0.721771	test: 0.762216
PRC train: 0.726234	val: 0.282395	test: 0.142947

Epoch: 71
Loss: 0.08242663518709456
ROC train: 0.970403	val: 0.711577	test: 0.756492
PRC train: 0.738210	val: 0.279908	test: 0.150066

Epoch: 72
Loss: 0.08102400663200025
ROC train: 0.967287	val: 0.727164	test: 0.752819
PRC train: 0.718142	val: 0.280549	test: 0.134881

Epoch: 73
Loss: 0.08068960132177301
ROC train: 0.968887	val: 0.721939	test: 0.754833
PRC train: 0.723986	val: 0.283873	test: 0.124192

Epoch: 74
Loss: 0.08013654283038409
ROC train: 0.966402	val: 0.719644	test: 0.756890
PRC train: 0.714710	val: 0.277307	test: 0.128886

Epoch: 75
Loss: 0.0800926878703528
ROC train: 0.968464	val: 0.703819	test: 0.764851
PRC train: 0.718781	val: 0.269799	test: 0.165999

Epoch: 76
Loss: 0.08045428120216301
ROC train: 0.973808	val: 0.718846	test: 0.769043
PRC train: 0.751970	val: 0.294518	test: 0.153644

Epoch: 77
Loss: 0.07987596038971255
ROC train: 0.970487	val: 0.727048	test: 0.753834
PRC train: 0.745413	val: 0.293404	test: 0.159300

Epoch: 78
Loss: 0.08027745587525588
ROC train: 0.971699	val: 0.726812	test: 0.768059
PRC train: 0.747974	val: 0.278320	test: 0.139541

Epoch: 79
Loss: 0.0797016001755114
ROC train: 0.972113	val: 0.714389	test: 0.766734
PRC train: 0.740714	val: 0.273549	test: 0.159744

Epoch: 80
Loss: 0.07736555868326045
ROC train: 0.971565	val: 0.708282	test: 0.751504
PRC train: 0.746007	val: 0.276081	test: 0.132341

Epoch: 81
Loss: 0.07751098540659898
ROC train: 0.973827	val: 0.709640	test: 0.761830
PRC train: 0.747141	val: 0.294466	test: 0.154767

Epoch: 82
Loss: 0.0786722607040404
ROC train: 0.976150	val: 0.718517	test: 0.772997
PRC train: 0.754629	val: 0.285069	test: 0.162586

Epoch: 83
Loss: 0.07796447820724317
ROC train: 0.974002	val: 0.722933	test: 0.763055
PRC train: 0.753313	val: 0.295704	test: 0.173479

Epoch: 84
Loss: 0.07728672768386989
ROC train: 0.977327	val: 0.710037	test: 0.755477
PRC train: 0.773897	val: 0.287867	test: 0.144916

Epoch: 85
Loss: 0.07816802400022121
ROC train: 0.972400	val: 0.707637	test: 0.769645
PRC train: 0.746835	val: 0.275873	test: 0.133577

Epoch: 86
Loss: 0.07753973279527282
ROC train: 0.975301	val: 0.715292	test: 0.772944
PRC train: 0.758947	val: 0.279994	test: 0.146017

Epoch: 87
Loss: 0.0764549009036921
ROC train: 0.975969	val: 0.731976	test: 0.766370
PRC train: 0.766471	val: 0.299320	test: 0.145608

Epoch: 88
Loss: 0.0752597577363757
ROC train: 0.976445	val: 0.699594	test: 0.756471
PRC train: 0.758443	val: 0.257306	test: 0.145743

Epoch: 89
Loss: 0.07588590520636827
ROC train: 0.979030	val: 0.722897	test: 0.767740
PRC train: 0.780113	val: 0.294240	test: 0.150535

Epoch: 90
Loss: 0.07526831990120757
ROC train: 0.977797	val: 0.710425	test: 0.782140
PRC train: 0.775137	val: 0.272200	test: 0.143110

Epoch: 91
Loss: 0.07662575930796554
ROC train: 0.978472	val: 0.727316	test: 0.778589
PRC train: 0.774034	val: 0.282332	test: 0.157201

Epoch: 92
Loss: 0.07554274289626195
ROC train: 0.975860	val: 0.708926	test: 0.758559
PRC train: 0.772257	val: 0.281509	test: 0.146316

Epoch: 93
Loss: 0.07443526678007523
ROC train: 0.979810	val: 0.727266	test: 0.769555
PRC train: 0.780579	val: 0.290165	test: 0.152073

Epoch: 94
Loss: 0.07350544175136259
ROC train: 0.920735	val: 0.712860	test: 0.740640
PRC train: 0.595870	val: 0.267657	test: 0.129973

Epoch: 34
Loss: 0.10144613432260732
ROC train: 0.926169	val: 0.717480	test: 0.753494
PRC train: 0.602002	val: 0.285043	test: 0.150845

Epoch: 35
Loss: 0.0997021428157386
ROC train: 0.921444	val: 0.705222	test: 0.710413
PRC train: 0.593003	val: 0.238108	test: 0.100270

Epoch: 36
Loss: 0.09941641383670805
ROC train: 0.927859	val: 0.716867	test: 0.747539
PRC train: 0.603212	val: 0.267884	test: 0.152851

Epoch: 37
Loss: 0.09925089233429873
ROC train: 0.929152	val: 0.708066	test: 0.731486
PRC train: 0.606933	val: 0.287960	test: 0.179311

Epoch: 38
Loss: 0.09775454735507429
ROC train: 0.928105	val: 0.695544	test: 0.730270
PRC train: 0.605589	val: 0.264831	test: 0.102228

Epoch: 39
Loss: 0.09748367499514492
ROC train: 0.929419	val: 0.723987	test: 0.727452
PRC train: 0.606461	val: 0.303944	test: 0.170967

Epoch: 40
Loss: 0.09476108456731618
ROC train: 0.930759	val: 0.700935	test: 0.721657
PRC train: 0.608543	val: 0.247940	test: 0.105154

Epoch: 41
Loss: 0.09630029936764349
ROC train: 0.933000	val: 0.718967	test: 0.747277
PRC train: 0.622278	val: 0.289472	test: 0.135366

Epoch: 42
Loss: 0.09701645494177895
ROC train: 0.933378	val: 0.709786	test: 0.730826
PRC train: 0.608939	val: 0.288716	test: 0.162867

Epoch: 43
Loss: 0.0973408814715407
ROC train: 0.940936	val: 0.713723	test: 0.740895
PRC train: 0.633652	val: 0.264264	test: 0.145869

Epoch: 44
Loss: 0.09544212659271595
ROC train: 0.939317	val: 0.703521	test: 0.724075
PRC train: 0.623938	val: 0.253324	test: 0.112554

Epoch: 45
Loss: 0.09511917145611491
ROC train: 0.930570	val: 0.721652	test: 0.739543
PRC train: 0.614918	val: 0.279424	test: 0.147631

Epoch: 46
Loss: 0.09480895274297103
ROC train: 0.940010	val: 0.715766	test: 0.747344
PRC train: 0.629895	val: 0.269326	test: 0.125016

Epoch: 47
Loss: 0.09392434616229116
ROC train: 0.941592	val: 0.703218	test: 0.735939
PRC train: 0.630087	val: 0.246747	test: 0.099223

Epoch: 48
Loss: 0.09416734686597047
ROC train: 0.944555	val: 0.701120	test: 0.731381
PRC train: 0.632149	val: 0.271194	test: 0.108404

Epoch: 49
Loss: 0.09241139523682602
ROC train: 0.945059	val: 0.714796	test: 0.752371
PRC train: 0.643246	val: 0.288416	test: 0.159618

Epoch: 50
Loss: 0.09220935891711124
ROC train: 0.947227	val: 0.724876	test: 0.750176
PRC train: 0.641454	val: 0.301266	test: 0.169836

Epoch: 51
Loss: 0.09110469200263631
ROC train: 0.948630	val: 0.718125	test: 0.735976
PRC train: 0.653841	val: 0.268011	test: 0.125422

Epoch: 52
Loss: 0.09025939715244909
ROC train: 0.951217	val: 0.714399	test: 0.757668
PRC train: 0.664776	val: 0.274839	test: 0.110149

Epoch: 53
Loss: 0.0905931983734528
ROC train: 0.941906	val: 0.722012	test: 0.773969
PRC train: 0.639921	val: 0.269479	test: 0.163913

Epoch: 54
Loss: 0.09083649923744551
ROC train: 0.950055	val: 0.711353	test: 0.761583
PRC train: 0.664440	val: 0.276894	test: 0.161568

Epoch: 55
Loss: 0.09127468480815175
ROC train: 0.953267	val: 0.709510	test: 0.751445
PRC train: 0.669824	val: 0.269537	test: 0.113597

Epoch: 56
Loss: 0.08940051351635707
ROC train: 0.952039	val: 0.717816	test: 0.732078
PRC train: 0.667851	val: 0.279497	test: 0.118123

Epoch: 57
Loss: 0.08914486769424129
ROC train: 0.953791	val: 0.708890	test: 0.754508
PRC train: 0.653291	val: 0.295756	test: 0.158711

Epoch: 58
Loss: 0.08915130360484026
ROC train: 0.946079	val: 0.687335	test: 0.740514
PRC train: 0.646098	val: 0.245723	test: 0.138451

Epoch: 59
Loss: 0.08922161407092313
ROC train: 0.957874	val: 0.711102	test: 0.747310
PRC train: 0.681891	val: 0.271410	test: 0.128774

Epoch: 60
Loss: 0.08950089354184985
ROC train: 0.957722	val: 0.713518	test: 0.734067
PRC train: 0.682881	val: 0.272933	test: 0.121891

Epoch: 61
Loss: 0.08786809577355169
ROC train: 0.952727	val: 0.704880	test: 0.716176
PRC train: 0.659044	val: 0.226968	test: 0.083978

Epoch: 62
Loss: 0.08837010551013398
ROC train: 0.954528	val: 0.707308	test: 0.739101
PRC train: 0.675188	val: 0.263044	test: 0.103901

Epoch: 63
Loss: 0.08693093587400205
ROC train: 0.956316	val: 0.712290	test: 0.740838
PRC train: 0.685633	val: 0.261340	test: 0.117327

Epoch: 64
Loss: 0.0859521258170168
ROC train: 0.960102	val: 0.710877	test: 0.721849
PRC train: 0.686715	val: 0.245889	test: 0.107215

Epoch: 65
Loss: 0.08557090813101494
ROC train: 0.961632	val: 0.712599	test: 0.747512
PRC train: 0.694139	val: 0.277451	test: 0.148689

Epoch: 66
Loss: 0.08695134019036709
ROC train: 0.960121	val: 0.707090	test: 0.745084
PRC train: 0.691370	val: 0.271517	test: 0.148137

Epoch: 67
Loss: 0.08424746316100015
ROC train: 0.960098	val: 0.704061	test: 0.737429
PRC train: 0.702610	val: 0.289535	test: 0.130997

Epoch: 68
Loss: 0.08712857556782391
ROC train: 0.963791	val: 0.718399	test: 0.762430
PRC train: 0.695512	val: 0.308134	test: 0.162293

Epoch: 69
Loss: 0.08397271473066627
ROC train: 0.962874	val: 0.711312	test: 0.745422
PRC train: 0.697090	val: 0.243875	test: 0.113070

Epoch: 70
Loss: 0.08336185583987528
ROC train: 0.964109	val: 0.704394	test: 0.751412
PRC train: 0.713850	val: 0.279347	test: 0.114854

Epoch: 71
Loss: 0.08347991327060358
ROC train: 0.962763	val: 0.717534	test: 0.751102
PRC train: 0.692969	val: 0.254132	test: 0.142834

Epoch: 72
Loss: 0.08451251255576375
ROC train: 0.966022	val: 0.714386	test: 0.742077
PRC train: 0.706821	val: 0.282094	test: 0.140817

Epoch: 73
Loss: 0.0851799736828971
ROC train: 0.964979	val: 0.696002	test: 0.722449
PRC train: 0.707476	val: 0.259110	test: 0.106584

Epoch: 74
Loss: 0.08392372650258334
ROC train: 0.966964	val: 0.727556	test: 0.750640
PRC train: 0.723522	val: 0.278317	test: 0.117837

Epoch: 75
Loss: 0.08220206605459701
ROC train: 0.965026	val: 0.719489	test: 0.748948
PRC train: 0.703546	val: 0.277043	test: 0.138730

Epoch: 76
Loss: 0.08172679271643282
ROC train: 0.965554	val: 0.702085	test: 0.721192
PRC train: 0.705385	val: 0.246305	test: 0.112938

Epoch: 77
Loss: 0.08138741429987474
ROC train: 0.964954	val: 0.698986	test: 0.744042
PRC train: 0.707795	val: 0.271058	test: 0.116577

Epoch: 78
Loss: 0.08125743481148681
ROC train: 0.967690	val: 0.699765	test: 0.752500
PRC train: 0.730237	val: 0.267806	test: 0.114001

Epoch: 79
Loss: 0.07993975643097526
ROC train: 0.970005	val: 0.700951	test: 0.735313
PRC train: 0.730615	val: 0.263084	test: 0.136559

Epoch: 80
Loss: 0.07931949221192142
ROC train: 0.967813	val: 0.703726	test: 0.720829
PRC train: 0.735644	val: 0.262241	test: 0.133457

Epoch: 81
Loss: 0.0793894149886083
ROC train: 0.970999	val: 0.701630	test: 0.742003
PRC train: 0.732912	val: 0.245036	test: 0.132736

Epoch: 82
Loss: 0.07947108298692522
ROC train: 0.972228	val: 0.699132	test: 0.723288
PRC train: 0.732295	val: 0.278222	test: 0.140821

Epoch: 83
Loss: 0.07959317528543737
ROC train: 0.970479	val: 0.700633	test: 0.722904
PRC train: 0.730291	val: 0.213400	test: 0.091031

Epoch: 84
Loss: 0.07983996017085156
ROC train: 0.974300	val: 0.710690	test: 0.742124
PRC train: 0.743882	val: 0.261049	test: 0.132280

Epoch: 85
Loss: 0.07802854721241717
ROC train: 0.971403	val: 0.706370	test: 0.718849
PRC train: 0.738106	val: 0.259822	test: 0.098909

Epoch: 86
Loss: 0.07914900878874163
ROC train: 0.974552	val: 0.714763	test: 0.729361
PRC train: 0.754858	val: 0.267570	test: 0.109525

Epoch: 87
Loss: 0.07731937802069087
ROC train: 0.973636	val: 0.718398	test: 0.739517
PRC train: 0.748567	val: 0.261383	test: 0.099765

Epoch: 88
Loss: 0.07740466563421428
ROC train: 0.976351	val: 0.718886	test: 0.736180
PRC train: 0.752412	val: 0.299336	test: 0.147850

Epoch: 89
Loss: 0.07801432716912621
ROC train: 0.975576	val: 0.714950	test: 0.741669
PRC train: 0.753569	val: 0.268573	test: 0.128235

Epoch: 90
Loss: 0.0765808819788157
ROC train: 0.975407	val: 0.711728	test: 0.747915
PRC train: 0.761875	val: 0.274181	test: 0.131941

Epoch: 91
Loss: 0.07665535286540244
ROC train: 0.977120	val: 0.709395	test: 0.738026
PRC train: 0.760675	val: 0.265438	test: 0.136460

Epoch: 92
Loss: 0.07919678253688917
ROC train: 0.976899	val: 0.718867	test: 0.739906
PRC train: 0.760877	val: 0.253620	test: 0.111800

Epoch: 93
Loss: 0.07544462321127329
ROC train: 0.977297	val: 0.713956	test: 0.741112
PRC train: 0.762228	val: 0.251760	test: 0.115926


ROC train: 0.922371	val: 0.705147	test: 0.762766
PRC train: 0.593355	val: 0.303761	test: 0.200023

Epoch: 34
Loss: 0.09864827804217614
ROC train: 0.926572	val: 0.704807	test: 0.752129
PRC train: 0.593771	val: 0.292230	test: 0.204775

Epoch: 35
Loss: 0.09757366130985047
ROC train: 0.930015	val: 0.701101	test: 0.759667
PRC train: 0.614438	val: 0.258627	test: 0.138888

Epoch: 36
Loss: 0.09978996588349122
ROC train: 0.923036	val: 0.702559	test: 0.742249
PRC train: 0.606121	val: 0.251759	test: 0.165605

Epoch: 37
Loss: 0.09702085472655914
ROC train: 0.926851	val: 0.713031	test: 0.752524
PRC train: 0.615467	val: 0.246431	test: 0.140488

Epoch: 38
Loss: 0.09761260225678674
ROC train: 0.929538	val: 0.715863	test: 0.773770
PRC train: 0.613772	val: 0.280399	test: 0.196082

Epoch: 39
Loss: 0.09779120839035371
ROC train: 0.931303	val: 0.709979	test: 0.756929
PRC train: 0.621793	val: 0.297240	test: 0.171151

Epoch: 40
Loss: 0.09525016318713284
ROC train: 0.929660	val: 0.693021	test: 0.733712
PRC train: 0.597512	val: 0.221937	test: 0.167513

Epoch: 41
Loss: 0.09561877394546696
ROC train: 0.941142	val: 0.709871	test: 0.752653
PRC train: 0.637922	val: 0.290981	test: 0.177669

Epoch: 42
Loss: 0.09604219802162743
ROC train: 0.936948	val: 0.718217	test: 0.753659
PRC train: 0.633268	val: 0.263357	test: 0.144761

Epoch: 43
Loss: 0.0934410839714714
ROC train: 0.944902	val: 0.719948	test: 0.747989
PRC train: 0.642155	val: 0.260324	test: 0.143475

Epoch: 44
Loss: 0.09319685977868249
ROC train: 0.931697	val: 0.723331	test: 0.764377
PRC train: 0.611889	val: 0.275061	test: 0.204630

Epoch: 45
Loss: 0.0932779857544736
ROC train: 0.945956	val: 0.712375	test: 0.760655
PRC train: 0.651799	val: 0.308074	test: 0.199573

Epoch: 46
Loss: 0.09256375485033487
ROC train: 0.944434	val: 0.697930	test: 0.748738
PRC train: 0.653387	val: 0.265728	test: 0.139697

Epoch: 47
Loss: 0.0920971888048082
ROC train: 0.944620	val: 0.717963	test: 0.752110
PRC train: 0.629179	val: 0.299136	test: 0.216265

Epoch: 48
Loss: 0.09132792755764131
ROC train: 0.943917	val: 0.694476	test: 0.762899
PRC train: 0.637536	val: 0.276826	test: 0.174648

Epoch: 49
Loss: 0.09255702946047473
ROC train: 0.946265	val: 0.704008	test: 0.759123
PRC train: 0.659762	val: 0.259251	test: 0.166514

Epoch: 50
Loss: 0.09133590024989623
ROC train: 0.947543	val: 0.707933	test: 0.770004
PRC train: 0.643227	val: 0.250292	test: 0.172553

Epoch: 51
Loss: 0.09159486618672046
ROC train: 0.951464	val: 0.703535	test: 0.762784
PRC train: 0.669562	val: 0.269097	test: 0.139288

Epoch: 52
Loss: 0.08915167707943204
ROC train: 0.946838	val: 0.697932	test: 0.754173
PRC train: 0.660372	val: 0.268292	test: 0.151486

Epoch: 53
Loss: 0.09011797881404969
ROC train: 0.950151	val: 0.702755	test: 0.758276
PRC train: 0.645773	val: 0.280542	test: 0.126148

Epoch: 54
Loss: 0.08871679251490491
ROC train: 0.953418	val: 0.693559	test: 0.765681
PRC train: 0.674358	val: 0.280158	test: 0.183252

Epoch: 55
Loss: 0.08703535313112878
ROC train: 0.953161	val: 0.709369	test: 0.756248
PRC train: 0.678436	val: 0.293124	test: 0.181656

Epoch: 56
Loss: 0.09000421322196231
ROC train: 0.956185	val: 0.720954	test: 0.764052
PRC train: 0.675552	val: 0.274968	test: 0.180620

Epoch: 57
Loss: 0.0882734640440269
ROC train: 0.955872	val: 0.695712	test: 0.746719
PRC train: 0.679687	val: 0.257322	test: 0.143720

Epoch: 58
Loss: 0.08691846309663534
ROC train: 0.952672	val: 0.709201	test: 0.754144
PRC train: 0.678582	val: 0.251468	test: 0.164877

Epoch: 59
Loss: 0.08717805157933085
ROC train: 0.956128	val: 0.704766	test: 0.758365
PRC train: 0.687772	val: 0.270526	test: 0.125264

Epoch: 60
Loss: 0.0870585419941272
ROC train: 0.956258	val: 0.708331	test: 0.770090
PRC train: 0.674332	val: 0.288705	test: 0.206852

Epoch: 61
Loss: 0.08643826082422398
ROC train: 0.957329	val: 0.699286	test: 0.776617
PRC train: 0.696799	val: 0.270872	test: 0.151057

Epoch: 62
Loss: 0.0858901423270632
ROC train: 0.956506	val: 0.703886	test: 0.760800
PRC train: 0.658918	val: 0.265352	test: 0.156376

Epoch: 63
Loss: 0.08553298182975021
ROC train: 0.961220	val: 0.712076	test: 0.763158
PRC train: 0.700259	val: 0.224174	test: 0.132204

Epoch: 64
Loss: 0.08454306855384079
ROC train: 0.964628	val: 0.716176	test: 0.775089
PRC train: 0.712213	val: 0.310677	test: 0.199323

Epoch: 65
Loss: 0.08323711676131162
ROC train: 0.961884	val: 0.709415	test: 0.758624
PRC train: 0.701732	val: 0.262199	test: 0.122157

Epoch: 66
Loss: 0.08356547853274053
ROC train: 0.956618	val: 0.699223	test: 0.753055
PRC train: 0.694497	val: 0.245376	test: 0.123821

Epoch: 67
Loss: 0.0841784821838601
ROC train: 0.965243	val: 0.708200	test: 0.753440
PRC train: 0.715459	val: 0.258896	test: 0.163953

Epoch: 68
Loss: 0.08390981742789382
ROC train: 0.965847	val: 0.708354	test: 0.772099
PRC train: 0.711429	val: 0.262619	test: 0.163261

Epoch: 69
Loss: 0.08374442608003245
ROC train: 0.967217	val: 0.723586	test: 0.771036
PRC train: 0.719311	val: 0.277379	test: 0.163024

Epoch: 70
Loss: 0.08404385719842343
ROC train: 0.967506	val: 0.706622	test: 0.753499
PRC train: 0.725709	val: 0.278824	test: 0.144216

Epoch: 71
Loss: 0.08089213858149003
ROC train: 0.967671	val: 0.715059	test: 0.769610
PRC train: 0.720708	val: 0.281290	test: 0.180273

Epoch: 72
Loss: 0.08116118028548484
ROC train: 0.965434	val: 0.717309	test: 0.767150
PRC train: 0.718227	val: 0.257181	test: 0.130369

Epoch: 73
Loss: 0.08109105573506292
ROC train: 0.971208	val: 0.723021	test: 0.762138
PRC train: 0.743484	val: 0.284729	test: 0.183800

Epoch: 74
Loss: 0.08206879262101241
ROC train: 0.967128	val: 0.704056	test: 0.749968
PRC train: 0.717719	val: 0.254544	test: 0.122148

Epoch: 75
Loss: 0.082310300931466
ROC train: 0.968421	val: 0.704090	test: 0.764915
PRC train: 0.724910	val: 0.238497	test: 0.139942

Epoch: 76
Loss: 0.0799362505792599
ROC train: 0.972686	val: 0.710189	test: 0.756957
PRC train: 0.750790	val: 0.261769	test: 0.158950

Epoch: 77
Loss: 0.08082496601047168
ROC train: 0.967385	val: 0.725895	test: 0.755842
PRC train: 0.716207	val: 0.276219	test: 0.168196

Epoch: 78
Loss: 0.07938193107168984
ROC train: 0.971794	val: 0.700236	test: 0.760391
PRC train: 0.748718	val: 0.259360	test: 0.159942

Epoch: 79
Loss: 0.07951381929024731
ROC train: 0.971795	val: 0.710685	test: 0.752437
PRC train: 0.739797	val: 0.269652	test: 0.154596

Epoch: 80
Loss: 0.07986335378432625
ROC train: 0.972365	val: 0.710262	test: 0.751786
PRC train: 0.744680	val: 0.243052	test: 0.126510

Epoch: 81
Loss: 0.08116006868409711
ROC train: 0.972191	val: 0.722287	test: 0.742768
PRC train: 0.750620	val: 0.257790	test: 0.124939

Epoch: 82
Loss: 0.08003871027801437
ROC train: 0.970726	val: 0.701486	test: 0.778925
PRC train: 0.741361	val: 0.245989	test: 0.165511

Epoch: 83
Loss: 0.07864261251551576
ROC train: 0.973946	val: 0.718145	test: 0.749360
PRC train: 0.744794	val: 0.276774	test: 0.175462

Epoch: 84
Loss: 0.07783461980625772
ROC train: 0.975421	val: 0.708950	test: 0.751633
PRC train: 0.748481	val: 0.255217	test: 0.147411

Epoch: 85
Loss: 0.07999118320532439
ROC train: 0.973897	val: 0.711599	test: 0.765253
PRC train: 0.757023	val: 0.279384	test: 0.132173

Epoch: 86
Loss: 0.07663833780084026
ROC train: 0.975260	val: 0.699676	test: 0.761333
PRC train: 0.754873	val: 0.269883	test: 0.158147

Epoch: 87
Loss: 0.07644821582154158
ROC train: 0.975872	val: 0.696915	test: 0.759253
PRC train: 0.763532	val: 0.262322	test: 0.148693

Epoch: 88
Loss: 0.0774145091504767
ROC train: 0.973973	val: 0.694139	test: 0.751995
PRC train: 0.752266	val: 0.249557	test: 0.159934

Epoch: 89
Loss: 0.07656559431026126
ROC train: 0.976378	val: 0.709721	test: 0.756935
PRC train: 0.770067	val: 0.277122	test: 0.173140

Epoch: 90
Loss: 0.07658983431230788
ROC train: 0.977667	val: 0.707927	test: 0.763703
PRC train: 0.775615	val: 0.251882	test: 0.152596

Epoch: 91
Loss: 0.07598027943906802
ROC train: 0.977572	val: 0.707759	test: 0.766265
PRC train: 0.777781	val: 0.273823	test: 0.176452

Epoch: 92
Loss: 0.07408044305611211
ROC train: 0.978071	val: 0.720254	test: 0.747562
PRC train: 0.774954	val: 0.266865	test: 0.136428

Epoch: 93
Loss: 0.07449790658488863
ROC train: 0.979780	val: 0.720911	test: 0.767002
PRC train: 0.787107	val: 0.252812	test: 0.152744

Epoch: 94
Loss: 0.07333989351195418
ROC train: 0.915572	val: 0.770640	test: 0.763474
PRC train: 0.574468	val: 0.349392	test: 0.167575

Epoch: 34
Loss: 0.10128035671684889
ROC train: 0.910038	val: 0.765248	test: 0.768053
PRC train: 0.569463	val: 0.343746	test: 0.185891

Epoch: 35
Loss: 0.09974911122763301
ROC train: 0.918789	val: 0.775389	test: 0.749464
PRC train: 0.567378	val: 0.368788	test: 0.221204

Epoch: 36
Loss: 0.0993442045660662
ROC train: 0.923061	val: 0.800929	test: 0.769445
PRC train: 0.585262	val: 0.370317	test: 0.210263

Epoch: 37
Loss: 0.0998534077286032
ROC train: 0.922421	val: 0.791477	test: 0.759974
PRC train: 0.594610	val: 0.353192	test: 0.203977

Epoch: 38
Loss: 0.09948218373933025
ROC train: 0.917573	val: 0.789413	test: 0.763476
PRC train: 0.595870	val: 0.345061	test: 0.171229

Epoch: 39
Loss: 0.09867300479723685
ROC train: 0.931968	val: 0.810360	test: 0.753035
PRC train: 0.620318	val: 0.372715	test: 0.203906

Epoch: 40
Loss: 0.09795759077170978
ROC train: 0.930142	val: 0.800595	test: 0.769389
PRC train: 0.605036	val: 0.372665	test: 0.193690

Epoch: 41
Loss: 0.09761499718630404
ROC train: 0.928016	val: 0.754954	test: 0.751059
PRC train: 0.609534	val: 0.335939	test: 0.160729

Epoch: 42
Loss: 0.09638091728929302
ROC train: 0.929792	val: 0.783571	test: 0.748636
PRC train: 0.613949	val: 0.348051	test: 0.146095

Epoch: 43
Loss: 0.0962812507552541
ROC train: 0.929621	val: 0.781192	test: 0.751724
PRC train: 0.616815	val: 0.350250	test: 0.191449

Epoch: 44
Loss: 0.09516369162004393
ROC train: 0.932920	val: 0.797941	test: 0.756513
PRC train: 0.616687	val: 0.339252	test: 0.165528

Epoch: 45
Loss: 0.09581006224421774
ROC train: 0.926683	val: 0.778724	test: 0.770175
PRC train: 0.608132	val: 0.341722	test: 0.141830

Epoch: 46
Loss: 0.09531098670285865
ROC train: 0.932364	val: 0.783470	test: 0.730889
PRC train: 0.616370	val: 0.342799	test: 0.197111

Epoch: 47
Loss: 0.09490515043951475
ROC train: 0.917174	val: 0.762799	test: 0.750648
PRC train: 0.596864	val: 0.310779	test: 0.155774

Epoch: 48
Loss: 0.09460176070406708
ROC train: 0.936938	val: 0.770726	test: 0.747859
PRC train: 0.621916	val: 0.364882	test: 0.188058

Epoch: 49
Loss: 0.09253074084133416
ROC train: 0.942783	val: 0.795620	test: 0.742119
PRC train: 0.648762	val: 0.335621	test: 0.183853

Epoch: 50
Loss: 0.09288421779299644
ROC train: 0.937152	val: 0.786443	test: 0.745926
PRC train: 0.640510	val: 0.360523	test: 0.167439

Epoch: 51
Loss: 0.09325864924505525
ROC train: 0.938779	val: 0.784863	test: 0.750702
PRC train: 0.629467	val: 0.327655	test: 0.155954

Epoch: 52
Loss: 0.09291846033128594
ROC train: 0.940528	val: 0.750069	test: 0.760013
PRC train: 0.640766	val: 0.345888	test: 0.191674

Epoch: 53
Loss: 0.09263226350020774
ROC train: 0.946686	val: 0.774523	test: 0.752367
PRC train: 0.652657	val: 0.353936	test: 0.172270

Epoch: 54
Loss: 0.09192600612394004
ROC train: 0.943591	val: 0.775359	test: 0.767332
PRC train: 0.656589	val: 0.363295	test: 0.188228

Epoch: 55
Loss: 0.09180358852703849
ROC train: 0.943026	val: 0.783360	test: 0.757993
PRC train: 0.644859	val: 0.336154	test: 0.168418

Epoch: 56
Loss: 0.09053073708138826
ROC train: 0.948966	val: 0.794514	test: 0.741552
PRC train: 0.658870	val: 0.328415	test: 0.160546

Epoch: 57
Loss: 0.08986080801872334
ROC train: 0.948617	val: 0.776715	test: 0.741339
PRC train: 0.651195	val: 0.342347	test: 0.159378

Epoch: 58
Loss: 0.09033692125989454
ROC train: 0.952149	val: 0.783528	test: 0.734514
PRC train: 0.652504	val: 0.340143	test: 0.175546

Epoch: 59
Loss: 0.09012048210661795
ROC train: 0.950431	val: 0.755695	test: 0.743894
PRC train: 0.671119	val: 0.330863	test: 0.165465

Epoch: 60
Loss: 0.0887920238037776
ROC train: 0.949648	val: 0.794358	test: 0.759020
PRC train: 0.674027	val: 0.311946	test: 0.168972

Epoch: 61
Loss: 0.0887826330908304
ROC train: 0.954446	val: 0.775016	test: 0.753201
PRC train: 0.683474	val: 0.368150	test: 0.196535

Epoch: 62
Loss: 0.08721187727645618
ROC train: 0.955717	val: 0.774296	test: 0.746442
PRC train: 0.670245	val: 0.337781	test: 0.178741

Epoch: 63
Loss: 0.089232303564773
ROC train: 0.957179	val: 0.780824	test: 0.736640
PRC train: 0.698996	val: 0.330428	test: 0.166848

Epoch: 64
Loss: 0.08731089994946942
ROC train: 0.954170	val: 0.771489	test: 0.740310
PRC train: 0.685079	val: 0.288596	test: 0.157104

Epoch: 65
Loss: 0.08574260020725902
ROC train: 0.962309	val: 0.787643	test: 0.749590
PRC train: 0.703246	val: 0.348177	test: 0.180184

Epoch: 66
Loss: 0.08574750772054655
ROC train: 0.959361	val: 0.786771	test: 0.752465
PRC train: 0.685555	val: 0.340804	test: 0.198603

Epoch: 67
Loss: 0.08625115599249246
ROC train: 0.956274	val: 0.790681	test: 0.749398
PRC train: 0.689844	val: 0.333126	test: 0.186715

Epoch: 68
Loss: 0.08541875114881496
ROC train: 0.958083	val: 0.784716	test: 0.734062
PRC train: 0.698576	val: 0.315725	test: 0.179704

Epoch: 69
Loss: 0.08569847900233826
ROC train: 0.963641	val: 0.777530	test: 0.747009
PRC train: 0.714378	val: 0.318785	test: 0.183309

Epoch: 70
Loss: 0.08409848047226508
ROC train: 0.959478	val: 0.787705	test: 0.741231
PRC train: 0.707293	val: 0.323343	test: 0.167587

Epoch: 71
Loss: 0.08279418298206802
ROC train: 0.964607	val: 0.778032	test: 0.741189
PRC train: 0.714563	val: 0.340680	test: 0.174712

Epoch: 72
Loss: 0.08315509400918646
ROC train: 0.963351	val: 0.776216	test: 0.737952
PRC train: 0.706413	val: 0.310531	test: 0.177491

Epoch: 73
Loss: 0.08351173147048256
ROC train: 0.956283	val: 0.768917	test: 0.718305
PRC train: 0.683896	val: 0.283801	test: 0.144298

Epoch: 74
Loss: 0.08270727591074672
ROC train: 0.963415	val: 0.777793	test: 0.744966
PRC train: 0.704273	val: 0.314994	test: 0.207740

Epoch: 75
Loss: 0.08280670120531615
ROC train: 0.965253	val: 0.783286	test: 0.777408
PRC train: 0.719804	val: 0.321426	test: 0.189677

Epoch: 76
Loss: 0.08076347115211893
ROC train: 0.964821	val: 0.777058	test: 0.747210
PRC train: 0.707339	val: 0.311132	test: 0.196788

Epoch: 77
Loss: 0.08225408061856937
ROC train: 0.969321	val: 0.786180	test: 0.749841
PRC train: 0.731651	val: 0.328039	test: 0.165546

Epoch: 78
Loss: 0.08140748934509692
ROC train: 0.967763	val: 0.764220	test: 0.770476
PRC train: 0.728221	val: 0.321084	test: 0.232288

Epoch: 79
Loss: 0.07968651075189205
ROC train: 0.965173	val: 0.764856	test: 0.748769
PRC train: 0.717368	val: 0.315503	test: 0.174492

Epoch: 80
Loss: 0.08048203719537905
ROC train: 0.968547	val: 0.779052	test: 0.742251
PRC train: 0.741774	val: 0.294468	test: 0.172878

Epoch: 81
Loss: 0.0804992936198242
ROC train: 0.969281	val: 0.769624	test: 0.723332
PRC train: 0.730001	val: 0.302474	test: 0.152564

Epoch: 82
Loss: 0.07988086610363446
ROC train: 0.966684	val: 0.797662	test: 0.725534
PRC train: 0.725414	val: 0.315795	test: 0.165619

Epoch: 83
Loss: 0.07829775720267795
ROC train: 0.973271	val: 0.809763	test: 0.747770
PRC train: 0.752748	val: 0.321005	test: 0.171725

Epoch: 84
Loss: 0.08044345152556001
ROC train: 0.967335	val: 0.779808	test: 0.750820
PRC train: 0.738852	val: 0.305923	test: 0.160739

Epoch: 85
Loss: 0.07946208067535665
ROC train: 0.973585	val: 0.789866	test: 0.730115
PRC train: 0.752617	val: 0.307987	test: 0.157192

Epoch: 86
Loss: 0.07844897764963532
ROC train: 0.970177	val: 0.798507	test: 0.740489
PRC train: 0.750349	val: 0.322878	test: 0.175055

Epoch: 87
Loss: 0.07920479835849387
ROC train: 0.975236	val: 0.788473	test: 0.739284
PRC train: 0.760195	val: 0.291521	test: 0.147911

Epoch: 88
Loss: 0.0779076262021409
ROC train: 0.970632	val: 0.802224	test: 0.742994
PRC train: 0.749796	val: 0.333393	test: 0.157561

Epoch: 89
Loss: 0.0768314998466265
ROC train: 0.975591	val: 0.776005	test: 0.737110
PRC train: 0.765925	val: 0.322353	test: 0.182024

Epoch: 90
Loss: 0.0779551113993175
ROC train: 0.976932	val: 0.793280	test: 0.750443
PRC train: 0.770153	val: 0.337978	test: 0.188128

Epoch: 91
Loss: 0.07585396910857872
ROC train: 0.974318	val: 0.779851	test: 0.732166
PRC train: 0.748246	val: 0.292335	test: 0.152960

Epoch: 92
Loss: 0.0764499605030313
ROC train: 0.976895	val: 0.791088	test: 0.742946
PRC train: 0.772994	val: 0.295236	test: 0.166144

Epoch: 93
Loss: 0.07573246836049373
ROC train: 0.976091	val: 0.778917	test: 0.732316
PRC train: 0.766871	val: 0.297704	test: 0.177347

Epoch: 94
Loss: 0.07709558015116692
ROC train: 0.911687	val: 0.774762	test: 0.761340
PRC train: 0.570278	val: 0.336602	test: 0.200661

Epoch: 34
Loss: 0.10157886095549543
ROC train: 0.917384	val: 0.781229	test: 0.738915
PRC train: 0.578185	val: 0.326978	test: 0.170182

Epoch: 35
Loss: 0.10115101717025253
ROC train: 0.922046	val: 0.768537	test: 0.741173
PRC train: 0.591406	val: 0.327578	test: 0.181385

Epoch: 36
Loss: 0.0998223835187816
ROC train: 0.915413	val: 0.775711	test: 0.762234
PRC train: 0.563142	val: 0.322950	test: 0.174348

Epoch: 37
Loss: 0.10010828717022872
ROC train: 0.923167	val: 0.776219	test: 0.774387
PRC train: 0.587594	val: 0.357954	test: 0.195896

Epoch: 38
Loss: 0.09949242841347646
ROC train: 0.922199	val: 0.773837	test: 0.756119
PRC train: 0.591269	val: 0.361341	test: 0.179771

Epoch: 39
Loss: 0.09859418157425683
ROC train: 0.926898	val: 0.787423	test: 0.768910
PRC train: 0.603265	val: 0.337139	test: 0.179323

Epoch: 40
Loss: 0.09764708181776471
ROC train: 0.928964	val: 0.778265	test: 0.759961
PRC train: 0.604605	val: 0.346827	test: 0.189139

Epoch: 41
Loss: 0.0980966450069409
ROC train: 0.930133	val: 0.781385	test: 0.746814
PRC train: 0.610109	val: 0.336075	test: 0.175424

Epoch: 42
Loss: 0.09659458403250443
ROC train: 0.930998	val: 0.800699	test: 0.749993
PRC train: 0.594082	val: 0.358340	test: 0.181465

Epoch: 43
Loss: 0.0978139360552007
ROC train: 0.931567	val: 0.779713	test: 0.749194
PRC train: 0.599874	val: 0.346345	test: 0.197615

Epoch: 44
Loss: 0.0967062970233177
ROC train: 0.935174	val: 0.780432	test: 0.745816
PRC train: 0.625616	val: 0.338221	test: 0.200540

Epoch: 45
Loss: 0.09615606932176624
ROC train: 0.935871	val: 0.785411	test: 0.759739
PRC train: 0.620769	val: 0.328274	test: 0.180634

Epoch: 46
Loss: 0.09608189323268237
ROC train: 0.936684	val: 0.778954	test: 0.748831
PRC train: 0.610599	val: 0.362231	test: 0.221738

Epoch: 47
Loss: 0.09588037047180534
ROC train: 0.936229	val: 0.774324	test: 0.735107
PRC train: 0.618822	val: 0.326574	test: 0.159165

Epoch: 48
Loss: 0.0957884429326439
ROC train: 0.937427	val: 0.785500	test: 0.757305
PRC train: 0.615983	val: 0.314292	test: 0.195356

Epoch: 49
Loss: 0.09334668299308295
ROC train: 0.940699	val: 0.768693	test: 0.748728
PRC train: 0.632769	val: 0.321932	test: 0.160687

Epoch: 50
Loss: 0.09407831683791121
ROC train: 0.940569	val: 0.784272	test: 0.737523
PRC train: 0.645913	val: 0.335548	test: 0.174807

Epoch: 51
Loss: 0.09346456836637794
ROC train: 0.942607	val: 0.767180	test: 0.759820
PRC train: 0.631957	val: 0.322063	test: 0.236518

Epoch: 52
Loss: 0.09350671999657917
ROC train: 0.940527	val: 0.783096	test: 0.755090
PRC train: 0.631587	val: 0.312487	test: 0.200685

Epoch: 53
Loss: 0.09194366106048076
ROC train: 0.941446	val: 0.779229	test: 0.739686
PRC train: 0.643776	val: 0.302973	test: 0.156639

Epoch: 54
Loss: 0.0914966128262144
ROC train: 0.946875	val: 0.790932	test: 0.748958
PRC train: 0.657491	val: 0.344378	test: 0.199567

Epoch: 55
Loss: 0.09168740881047123
ROC train: 0.949149	val: 0.785457	test: 0.757881
PRC train: 0.658232	val: 0.335808	test: 0.183084

Epoch: 56
Loss: 0.08987476482500284
ROC train: 0.951138	val: 0.783669	test: 0.756944
PRC train: 0.661816	val: 0.331378	test: 0.196247

Epoch: 57
Loss: 0.09021885087351023
ROC train: 0.946012	val: 0.791618	test: 0.734280
PRC train: 0.658548	val: 0.292068	test: 0.163581

Epoch: 58
Loss: 0.09144687396327315
ROC train: 0.949437	val: 0.785139	test: 0.759486
PRC train: 0.660952	val: 0.350908	test: 0.196050

Epoch: 59
Loss: 0.08923639665404387
ROC train: 0.951001	val: 0.772762	test: 0.749373
PRC train: 0.671694	val: 0.363998	test: 0.199803

Epoch: 60
Loss: 0.08850443353572138
ROC train: 0.953645	val: 0.766341	test: 0.750936
PRC train: 0.670487	val: 0.321637	test: 0.183610

Epoch: 61
Loss: 0.08828366160132865
ROC train: 0.951552	val: 0.767925	test: 0.761687
PRC train: 0.655731	val: 0.322045	test: 0.197138

Epoch: 62
Loss: 0.09020358744364049
ROC train: 0.949206	val: 0.762569	test: 0.772707
PRC train: 0.663107	val: 0.315806	test: 0.202355

Epoch: 63
Loss: 0.08753726111309494
ROC train: 0.953812	val: 0.768613	test: 0.748856
PRC train: 0.663486	val: 0.353264	test: 0.192805

Epoch: 64
Loss: 0.0877182684676273
ROC train: 0.955333	val: 0.782818	test: 0.736957
PRC train: 0.675820	val: 0.297048	test: 0.159448

Epoch: 65
Loss: 0.08710925255677503
ROC train: 0.956682	val: 0.771936	test: 0.736071
PRC train: 0.681260	val: 0.311352	test: 0.181481

Epoch: 66
Loss: 0.08690051113178968
ROC train: 0.957493	val: 0.771234	test: 0.746196
PRC train: 0.684189	val: 0.305370	test: 0.176797

Epoch: 67
Loss: 0.08406373996830946
ROC train: 0.955827	val: 0.766476	test: 0.761596
PRC train: 0.687880	val: 0.303402	test: 0.178749

Epoch: 68
Loss: 0.0862789797798606
ROC train: 0.958125	val: 0.772141	test: 0.752853
PRC train: 0.683900	val: 0.323866	test: 0.179463

Epoch: 69
Loss: 0.08435376659468645
ROC train: 0.959684	val: 0.779079	test: 0.746326
PRC train: 0.685095	val: 0.318959	test: 0.162164

Epoch: 70
Loss: 0.08519374324525068
ROC train: 0.962827	val: 0.757330	test: 0.751855
PRC train: 0.702581	val: 0.320939	test: 0.148969

Epoch: 71
Loss: 0.08486130384712962
ROC train: 0.963872	val: 0.772055	test: 0.735820
PRC train: 0.699816	val: 0.322679	test: 0.139815

Epoch: 72
Loss: 0.08476587558475863
ROC train: 0.961117	val: 0.782420	test: 0.758110
PRC train: 0.697523	val: 0.321542	test: 0.159103

Epoch: 73
Loss: 0.08506750627735221
ROC train: 0.962831	val: 0.776936	test: 0.763000
PRC train: 0.705642	val: 0.343137	test: 0.204132

Epoch: 74
Loss: 0.08289986131039337
ROC train: 0.964178	val: 0.768531	test: 0.762608
PRC train: 0.711983	val: 0.320740	test: 0.178846

Epoch: 75
Loss: 0.08336534736273177
ROC train: 0.962209	val: 0.783672	test: 0.740287
PRC train: 0.704122	val: 0.320566	test: 0.190463

Epoch: 76
Loss: 0.08237940672885209
ROC train: 0.965162	val: 0.778834	test: 0.753255
PRC train: 0.719695	val: 0.295753	test: 0.166897

Epoch: 77
Loss: 0.08210483888092912
ROC train: 0.967549	val: 0.787738	test: 0.773592
PRC train: 0.721516	val: 0.331292	test: 0.186063

Epoch: 78
Loss: 0.08138924169614391
ROC train: 0.966857	val: 0.776342	test: 0.755563
PRC train: 0.724313	val: 0.297852	test: 0.173877

Epoch: 79
Loss: 0.0811722331531478
ROC train: 0.969139	val: 0.780628	test: 0.763881
PRC train: 0.725398	val: 0.329285	test: 0.187061

Epoch: 80
Loss: 0.08186096156375625
ROC train: 0.966499	val: 0.781679	test: 0.765778
PRC train: 0.727014	val: 0.329820	test: 0.197825

Epoch: 81
Loss: 0.08090979736994239
ROC train: 0.968490	val: 0.772003	test: 0.754615
PRC train: 0.728555	val: 0.302614	test: 0.170629

Epoch: 82
Loss: 0.07982954685212375
ROC train: 0.971387	val: 0.801446	test: 0.761378
PRC train: 0.744299	val: 0.312252	test: 0.161479

Epoch: 83
Loss: 0.08117876766474394
ROC train: 0.968789	val: 0.789955	test: 0.756915
PRC train: 0.732159	val: 0.322083	test: 0.154605

Epoch: 84
Loss: 0.07909709783293899
ROC train: 0.967686	val: 0.763668	test: 0.740563
PRC train: 0.725338	val: 0.289396	test: 0.124327

Epoch: 85
Loss: 0.08051220592376353
ROC train: 0.969928	val: 0.767903	test: 0.760179
PRC train: 0.730944	val: 0.319455	test: 0.174693

Epoch: 86
Loss: 0.07898327032202838
ROC train: 0.969893	val: 0.771822	test: 0.761471
PRC train: 0.737803	val: 0.313075	test: 0.197323

Epoch: 87
Loss: 0.07890548891230552
ROC train: 0.968556	val: 0.771749	test: 0.735624
PRC train: 0.730523	val: 0.295919	test: 0.143327

Epoch: 88
Loss: 0.0802437532941486
ROC train: 0.971995	val: 0.777894	test: 0.767985
PRC train: 0.747422	val: 0.343798	test: 0.215621

Epoch: 89
Loss: 0.07813665979784029
ROC train: 0.970664	val: 0.774217	test: 0.748585
PRC train: 0.732991	val: 0.314198	test: 0.173541

Epoch: 90
Loss: 0.0777458740418613
ROC train: 0.972780	val: 0.763120	test: 0.754731
PRC train: 0.751997	val: 0.302796	test: 0.195849

Epoch: 91
Loss: 0.07819622018469115
ROC train: 0.971830	val: 0.768084	test: 0.746393
PRC train: 0.750502	val: 0.305326	test: 0.170188

Epoch: 92
Loss: 0.07770571487703559
ROC train: 0.975368	val: 0.781801	test: 0.747473
PRC train: 0.765265	val: 0.286465	test: 0.180460

Epoch: 93
Loss: 0.07723553563061461
ROC train: 0.976014	val: 0.770506	test: 0.751465
PRC train: 0.767100	val: 0.301204	test: 0.170117

Epoch: 94
Loss: 0.076718701946952
ROC train: 0.912772	val: 0.778219	test: 0.766175
PRC train: 0.546127	val: 0.301221	test: 0.171782

Epoch: 34
Loss: 0.10267655009157012
ROC train: 0.915945	val: 0.762624	test: 0.752299
PRC train: 0.583075	val: 0.344189	test: 0.175764

Epoch: 35
Loss: 0.1004231624046623
ROC train: 0.919111	val: 0.774725	test: 0.754636
PRC train: 0.584199	val: 0.333250	test: 0.186949

Epoch: 36
Loss: 0.10110783465279695
ROC train: 0.920644	val: 0.786737	test: 0.752738
PRC train: 0.594992	val: 0.320403	test: 0.179435

Epoch: 37
Loss: 0.10028705111859984
ROC train: 0.918174	val: 0.786709	test: 0.757753
PRC train: 0.577857	val: 0.313462	test: 0.162196

Epoch: 38
Loss: 0.09970252993258741
ROC train: 0.926884	val: 0.792062	test: 0.766369
PRC train: 0.597318	val: 0.296582	test: 0.172796

Epoch: 39
Loss: 0.09979959681590343
ROC train: 0.917063	val: 0.757661	test: 0.743705
PRC train: 0.557618	val: 0.229602	test: 0.120380

Epoch: 40
Loss: 0.09878006341971089
ROC train: 0.927767	val: 0.791922	test: 0.757253
PRC train: 0.611021	val: 0.326664	test: 0.169983

Epoch: 41
Loss: 0.09742953372184979
ROC train: 0.922531	val: 0.786348	test: 0.749634
PRC train: 0.591748	val: 0.300386	test: 0.203043

Epoch: 42
Loss: 0.09663159525054538
ROC train: 0.929846	val: 0.794024	test: 0.758358
PRC train: 0.598293	val: 0.286372	test: 0.155298

Epoch: 43
Loss: 0.0970146721939885
ROC train: 0.929719	val: 0.774318	test: 0.762881
PRC train: 0.614111	val: 0.302039	test: 0.144359

Epoch: 44
Loss: 0.09642155460742483
ROC train: 0.932453	val: 0.785696	test: 0.764356
PRC train: 0.595253	val: 0.296970	test: 0.204665

Epoch: 45
Loss: 0.09582949106979204
ROC train: 0.929212	val: 0.781936	test: 0.775919
PRC train: 0.609713	val: 0.306341	test: 0.183573

Epoch: 46
Loss: 0.09446596932288497
ROC train: 0.931977	val: 0.799138	test: 0.761849
PRC train: 0.623511	val: 0.320292	test: 0.158280

Epoch: 47
Loss: 0.09518675981295013
ROC train: 0.933516	val: 0.773708	test: 0.755916
PRC train: 0.605851	val: 0.279822	test: 0.149281

Epoch: 48
Loss: 0.09486210154374011
ROC train: 0.936271	val: 0.788877	test: 0.764766
PRC train: 0.626510	val: 0.284945	test: 0.139096

Epoch: 49
Loss: 0.09278898950791735
ROC train: 0.932102	val: 0.786186	test: 0.751975
PRC train: 0.594727	val: 0.267325	test: 0.152186

Epoch: 50
Loss: 0.0933627263063089
ROC train: 0.942441	val: 0.789888	test: 0.777647
PRC train: 0.641564	val: 0.343284	test: 0.221022

Epoch: 51
Loss: 0.09242227418899795
ROC train: 0.940920	val: 0.802148	test: 0.758134
PRC train: 0.632999	val: 0.284464	test: 0.149421

Epoch: 52
Loss: 0.09393892324302973
ROC train: 0.942007	val: 0.784358	test: 0.755843
PRC train: 0.635359	val: 0.312731	test: 0.140891

Epoch: 53
Loss: 0.09182908701584243
ROC train: 0.944538	val: 0.795029	test: 0.756538
PRC train: 0.627142	val: 0.292681	test: 0.148094

Epoch: 54
Loss: 0.09239516423333846
ROC train: 0.943858	val: 0.789376	test: 0.776850
PRC train: 0.649185	val: 0.306172	test: 0.166372

Epoch: 55
Loss: 0.09114468888259336
ROC train: 0.935788	val: 0.779345	test: 0.764623
PRC train: 0.609352	val: 0.270848	test: 0.193895

Epoch: 56
Loss: 0.09141169769745751
ROC train: 0.948129	val: 0.788479	test: 0.767440
PRC train: 0.655969	val: 0.317555	test: 0.170594

Epoch: 57
Loss: 0.09061724370592886
ROC train: 0.945771	val: 0.776360	test: 0.775919
PRC train: 0.659380	val: 0.315863	test: 0.164863

Epoch: 58
Loss: 0.0909803704044779
ROC train: 0.951481	val: 0.789260	test: 0.770403
PRC train: 0.665394	val: 0.287226	test: 0.177654

Epoch: 59
Loss: 0.0891879679903107
ROC train: 0.948757	val: 0.782521	test: 0.763421
PRC train: 0.659596	val: 0.318174	test: 0.183650

Epoch: 60
Loss: 0.08908871488374064
ROC train: 0.954957	val: 0.782707	test: 0.785504
PRC train: 0.683500	val: 0.316909	test: 0.208883

Epoch: 61
Loss: 0.08844253128661889
ROC train: 0.955123	val: 0.796269	test: 0.772713
PRC train: 0.674641	val: 0.311059	test: 0.206760

Epoch: 62
Loss: 0.08890405835249711
ROC train: 0.953929	val: 0.785019	test: 0.775137
PRC train: 0.674417	val: 0.288326	test: 0.173403

Epoch: 63
Loss: 0.08797319768281718
ROC train: 0.955693	val: 0.804273	test: 0.769656
PRC train: 0.681078	val: 0.315261	test: 0.173265

Epoch: 64
Loss: 0.08847710589772656
ROC train: 0.956815	val: 0.792325	test: 0.759370
PRC train: 0.681611	val: 0.310058	test: 0.156093

Epoch: 65
Loss: 0.08618325049073035
ROC train: 0.956249	val: 0.797711	test: 0.768103
PRC train: 0.680797	val: 0.285496	test: 0.184217

Epoch: 66
Loss: 0.0860561290722179
ROC train: 0.954068	val: 0.787983	test: 0.778723
PRC train: 0.681113	val: 0.305820	test: 0.168088

Epoch: 67
Loss: 0.08631472939292899
ROC train: 0.959074	val: 0.788397	test: 0.763364
PRC train: 0.703720	val: 0.297415	test: 0.161322

Epoch: 68
Loss: 0.08541514744339535
ROC train: 0.958316	val: 0.782652	test: 0.757091
PRC train: 0.695075	val: 0.310756	test: 0.160788

Epoch: 69
Loss: 0.08574755803067152
ROC train: 0.959676	val: 0.793718	test: 0.768387
PRC train: 0.693889	val: 0.316793	test: 0.195809

Epoch: 70
Loss: 0.0838300111843366
ROC train: 0.962739	val: 0.788292	test: 0.784980
PRC train: 0.702557	val: 0.270175	test: 0.174077

Epoch: 71
Loss: 0.08526440308767945
ROC train: 0.957645	val: 0.777245	test: 0.769302
PRC train: 0.689363	val: 0.295686	test: 0.173032

Epoch: 72
Loss: 0.0838163147222823
ROC train: 0.961903	val: 0.788152	test: 0.765202
PRC train: 0.708990	val: 0.315072	test: 0.181794

Epoch: 73
Loss: 0.08391724180686777
ROC train: 0.961118	val: 0.791694	test: 0.765360
PRC train: 0.708488	val: 0.306895	test: 0.148647

Epoch: 74
Loss: 0.08309770026938605
ROC train: 0.960309	val: 0.794361	test: 0.769115
PRC train: 0.694012	val: 0.274215	test: 0.189906

Epoch: 75
Loss: 0.08468180943117493
ROC train: 0.964000	val: 0.801786	test: 0.767506
PRC train: 0.718345	val: 0.308271	test: 0.170159

Epoch: 76
Loss: 0.08378909570115962
ROC train: 0.961827	val: 0.806664	test: 0.759995
PRC train: 0.702758	val: 0.292732	test: 0.182199

Epoch: 77
Loss: 0.08259631296255933
ROC train: 0.966177	val: 0.800488	test: 0.779152
PRC train: 0.723551	val: 0.329615	test: 0.194351

Epoch: 78
Loss: 0.08263604202747067
ROC train: 0.964644	val: 0.788571	test: 0.760538
PRC train: 0.722449	val: 0.250056	test: 0.143280

Epoch: 79
Loss: 0.08213366703868379
ROC train: 0.965517	val: 0.797160	test: 0.769978
PRC train: 0.720486	val: 0.285091	test: 0.156167

Epoch: 80
Loss: 0.08222839720089224
ROC train: 0.965555	val: 0.798859	test: 0.767006
PRC train: 0.722866	val: 0.302330	test: 0.192077

Epoch: 81
Loss: 0.0820942407245694
ROC train: 0.967287	val: 0.801450	test: 0.773541
PRC train: 0.731160	val: 0.310716	test: 0.211676

Epoch: 82
Loss: 0.0807209846291226
ROC train: 0.967255	val: 0.789392	test: 0.743027
PRC train: 0.732022	val: 0.272010	test: 0.125684

Epoch: 83
Loss: 0.07986448176903788
ROC train: 0.967256	val: 0.787490	test: 0.774053
PRC train: 0.718079	val: 0.308900	test: 0.212177

Epoch: 84
Loss: 0.07927023974979093
ROC train: 0.971190	val: 0.799882	test: 0.777080
PRC train: 0.742594	val: 0.308153	test: 0.192134

Epoch: 85
Loss: 0.0786210891304068
ROC train: 0.971303	val: 0.810041	test: 0.765395
PRC train: 0.742476	val: 0.278891	test: 0.155458

Epoch: 86
Loss: 0.07886144704711036
ROC train: 0.968739	val: 0.791789	test: 0.758024
PRC train: 0.725696	val: 0.269701	test: 0.144303

Epoch: 87
Loss: 0.07863082994273105
ROC train: 0.970220	val: 0.786584	test: 0.780307
PRC train: 0.739945	val: 0.294260	test: 0.184688

Epoch: 88
Loss: 0.07943324744639269
ROC train: 0.972223	val: 0.799517	test: 0.780285
PRC train: 0.745426	val: 0.307577	test: 0.182720

Epoch: 89
Loss: 0.07833697408495392
ROC train: 0.973305	val: 0.785678	test: 0.788491
PRC train: 0.754033	val: 0.264708	test: 0.152902

Epoch: 90
Loss: 0.07796336620791688
ROC train: 0.973953	val: 0.795785	test: 0.767114
PRC train: 0.760132	val: 0.301797	test: 0.189472

Epoch: 91
Loss: 0.07645302407839492
ROC train: 0.974099	val: 0.802680	test: 0.779177
PRC train: 0.763658	val: 0.308043	test: 0.159099

Epoch: 92
Loss: 0.07682707049157182
ROC train: 0.972452	val: 0.784330	test: 0.775878
PRC train: 0.750574	val: 0.290394	test: 0.152792

Epoch: 93
Loss: 0.07660433608867785
ROC train: 0.975346	val: 0.799490	test: 0.761150
PRC train: 0.761552	val: 0.288873	test: 0.190807

Epoch: 94
Loss: 0.07719340860155371

Epoch: 94
Loss: 0.0717320804518648
ROC train: 0.983034	val: 0.689761	test: 0.758384
PRC train: 0.798865	val: 0.239446	test: 0.230176

Epoch: 95
Loss: 0.07026613136473887
ROC train: 0.983773	val: 0.697258	test: 0.760829
PRC train: 0.807967	val: 0.235149	test: 0.204257

Epoch: 96
Loss: 0.06957204506570862
ROC train: 0.984551	val: 0.695864	test: 0.754783
PRC train: 0.810320	val: 0.223600	test: 0.183065

Epoch: 97
Loss: 0.07070838705211682
ROC train: 0.981471	val: 0.698093	test: 0.753059
PRC train: 0.786715	val: 0.256149	test: 0.211209

Epoch: 98
Loss: 0.07079712725386705
ROC train: 0.981031	val: 0.694441	test: 0.756437
PRC train: 0.791127	val: 0.237318	test: 0.191852

Epoch: 99
Loss: 0.06996657536203757
ROC train: 0.984262	val: 0.694228	test: 0.750384
PRC train: 0.810428	val: 0.225054	test: 0.174909

Epoch: 100
Loss: 0.07015534536308674
ROC train: 0.980713	val: 0.690499	test: 0.752937
PRC train: 0.793327	val: 0.209135	test: 0.168927

Epoch: 101
Loss: 0.0674518890379645
ROC train: 0.984894	val: 0.703945	test: 0.763110
PRC train: 0.809094	val: 0.229812	test: 0.186126

Epoch: 102
Loss: 0.06907561183369046
ROC train: 0.980655	val: 0.696945	test: 0.743399
PRC train: 0.782859	val: 0.217620	test: 0.186001

Epoch: 103
Loss: 0.06839850976662526
ROC train: 0.985451	val: 0.699065	test: 0.759278
PRC train: 0.818686	val: 0.237040	test: 0.204522

Epoch: 104
Loss: 0.06664257229649487
ROC train: 0.986234	val: 0.701575	test: 0.755788
PRC train: 0.819322	val: 0.224264	test: 0.221216

Epoch: 105
Loss: 0.06559496903835355
ROC train: 0.985008	val: 0.693199	test: 0.744977
PRC train: 0.811491	val: 0.222372	test: 0.208328

Epoch: 106
Loss: 0.06537469480607663
ROC train: 0.986404	val: 0.694813	test: 0.730058
PRC train: 0.824599	val: 0.225658	test: 0.162835

Epoch: 107
Loss: 0.0657900637653659
ROC train: 0.986038	val: 0.690460	test: 0.736294
PRC train: 0.821113	val: 0.225541	test: 0.156090

Epoch: 108
Loss: 0.06636711868753198
ROC train: 0.987491	val: 0.695150	test: 0.745047
PRC train: 0.832350	val: 0.230531	test: 0.205538

Epoch: 109
Loss: 0.0645668476596654
ROC train: 0.984439	val: 0.692249	test: 0.728857
PRC train: 0.801926	val: 0.213489	test: 0.174440

Epoch: 110
Loss: 0.07041920971270212
ROC train: 0.986450	val: 0.703989	test: 0.747689
PRC train: 0.821233	val: 0.244572	test: 0.193391

Epoch: 111
Loss: 0.0649321594485881
ROC train: 0.986190	val: 0.695686	test: 0.736000
PRC train: 0.832015	val: 0.228476	test: 0.173717

Epoch: 112
Loss: 0.06473882652945005
ROC train: 0.988269	val: 0.701428	test: 0.742630
PRC train: 0.841348	val: 0.233161	test: 0.182250

Epoch: 113
Loss: 0.06534220476790126
ROC train: 0.987677	val: 0.705191	test: 0.745230
PRC train: 0.833301	val: 0.236911	test: 0.216937

Epoch: 114
Loss: 0.06443578986599052
ROC train: 0.986602	val: 0.697984	test: 0.742272
PRC train: 0.828056	val: 0.232540	test: 0.162778

Epoch: 115
Loss: 0.06713900685614343
ROC train: 0.988303	val: 0.698368	test: 0.746503
PRC train: 0.840202	val: 0.239703	test: 0.190606

Epoch: 116
Loss: 0.06321669705914885
ROC train: 0.988104	val: 0.699842	test: 0.751065
PRC train: 0.839990	val: 0.240171	test: 0.194165

Epoch: 117
Loss: 0.06263297266485746
ROC train: 0.986778	val: 0.702724	test: 0.740364
PRC train: 0.824937	val: 0.212974	test: 0.186056

Epoch: 118
Loss: 0.0645372509876266
ROC train: 0.989391	val: 0.703168	test: 0.750230
PRC train: 0.845332	val: 0.228535	test: 0.187338

Epoch: 119
Loss: 0.06376540065835533
ROC train: 0.989371	val: 0.692204	test: 0.731214
PRC train: 0.851478	val: 0.223316	test: 0.171857

Epoch: 120
Loss: 0.06334130911649953
ROC train: 0.988640	val: 0.696797	test: 0.742623
PRC train: 0.849200	val: 0.239234	test: 0.212074

Early stopping
Best (ROC):	 train: 0.972095	val: 0.707818	test: 0.755621
Best (PRC):	 train: 0.741052	val: 0.248025	test: 0.201998

ROC train: 0.981227	val: 0.688447	test: 0.734096
PRC train: 0.799333	val: 0.223542	test: 0.162767

Epoch: 95
Loss: 0.06990779229444948
ROC train: 0.983976	val: 0.706697	test: 0.762630
PRC train: 0.810159	val: 0.238344	test: 0.206317

Epoch: 96
Loss: 0.07034926900752682
ROC train: 0.984479	val: 0.697491	test: 0.752819
PRC train: 0.809921	val: 0.212298	test: 0.185821

Epoch: 97
Loss: 0.06848135491693226
ROC train: 0.985845	val: 0.703173	test: 0.755756
PRC train: 0.822925	val: 0.230568	test: 0.190764

Epoch: 98
Loss: 0.06807352759515597
ROC train: 0.985051	val: 0.686833	test: 0.745290
PRC train: 0.811679	val: 0.188939	test: 0.173749

Epoch: 99
Loss: 0.07076079983539871
ROC train: 0.986084	val: 0.694972	test: 0.738624
PRC train: 0.827285	val: 0.216895	test: 0.142668

Epoch: 100
Loss: 0.06794938396106079
ROC train: 0.984905	val: 0.692406	test: 0.757342
PRC train: 0.815719	val: 0.218112	test: 0.210556

Epoch: 101
Loss: 0.06765000894222552
ROC train: 0.984927	val: 0.695558	test: 0.748551
PRC train: 0.810797	val: 0.219505	test: 0.182469

Epoch: 102
Loss: 0.0682684324446154
ROC train: 0.986100	val: 0.701042	test: 0.755779
PRC train: 0.828975	val: 0.231414	test: 0.185764

Epoch: 103
Loss: 0.06531270115459958
ROC train: 0.984302	val: 0.690450	test: 0.743785
PRC train: 0.815924	val: 0.200621	test: 0.153478

Epoch: 104
Loss: 0.06771160071120895
ROC train: 0.984192	val: 0.698234	test: 0.761296
PRC train: 0.814193	val: 0.229057	test: 0.221088

Epoch: 105
Loss: 0.06939812870073571
ROC train: 0.986336	val: 0.699505	test: 0.752896
PRC train: 0.827869	val: 0.217276	test: 0.162399

Epoch: 106
Loss: 0.06620791445693787
ROC train: 0.987306	val: 0.693002	test: 0.753767
PRC train: 0.831121	val: 0.217316	test: 0.198903

Epoch: 107
Loss: 0.0664864500004213
ROC train: 0.985306	val: 0.704490	test: 0.772134
PRC train: 0.819569	val: 0.224755	test: 0.234595

Epoch: 108
Loss: 0.06515258586854915
ROC train: 0.987240	val: 0.693683	test: 0.740407
PRC train: 0.832872	val: 0.215451	test: 0.170099

Epoch: 109
Loss: 0.063821902365739
ROC train: 0.987288	val: 0.693667	test: 0.746794
PRC train: 0.833746	val: 0.196236	test: 0.154606

Epoch: 110
Loss: 0.06346052812971692
ROC train: 0.987598	val: 0.696809	test: 0.757617
PRC train: 0.836471	val: 0.212371	test: 0.194790

Epoch: 111
Loss: 0.06638880599698774
ROC train: 0.988086	val: 0.703087	test: 0.752092
PRC train: 0.839709	val: 0.241016	test: 0.188749

Epoch: 112
Loss: 0.06660885390925218
ROC train: 0.988733	val: 0.696068	test: 0.751519
PRC train: 0.845270	val: 0.224957	test: 0.184671

Epoch: 113
Loss: 0.06451494514563778
ROC train: 0.986614	val: 0.697617	test: 0.751225
PRC train: 0.836328	val: 0.218747	test: 0.188362

Epoch: 114
Loss: 0.06586051652735052
ROC train: 0.987262	val: 0.694374	test: 0.744498
PRC train: 0.826792	val: 0.222920	test: 0.175099

Epoch: 115
Loss: 0.0638251895150411
ROC train: 0.988530	val: 0.700052	test: 0.748090
PRC train: 0.846287	val: 0.203396	test: 0.199691

Epoch: 116
Loss: 0.06393259300236911
ROC train: 0.988263	val: 0.690268	test: 0.751427
PRC train: 0.835168	val: 0.197645	test: 0.198338

Epoch: 117
Loss: 0.06474832386138762
ROC train: 0.987366	val: 0.700036	test: 0.745248
PRC train: 0.839854	val: 0.210062	test: 0.164981

Epoch: 118
Loss: 0.06096628435101476
ROC train: 0.989818	val: 0.701423	test: 0.746843
PRC train: 0.860349	val: 0.188899	test: 0.181677

Epoch: 119
Loss: 0.062482468617082884
ROC train: 0.988826	val: 0.694275	test: 0.747101
PRC train: 0.851497	val: 0.241651	test: 0.181480

Epoch: 120
Loss: 0.06363433219313949
ROC train: 0.988862	val: 0.707964	test: 0.752376
PRC train: 0.850609	val: 0.222977	test: 0.187358

Early stopping
Best (ROC):	 train: 0.976059	val: 0.720864	test: 0.768133
Best (PRC):	 train: 0.767063	val: 0.215577	test: 0.229695

ROC train: 0.983283	val: 0.716298	test: 0.738207
PRC train: 0.800852	val: 0.227526	test: 0.230232

Epoch: 95
Loss: 0.07055657374007039
ROC train: 0.982589	val: 0.715701	test: 0.754789
PRC train: 0.807413	val: 0.258311	test: 0.245059

Epoch: 96
Loss: 0.06953416884371928
ROC train: 0.981471	val: 0.710092	test: 0.740315
PRC train: 0.800720	val: 0.227188	test: 0.222974

Epoch: 97
Loss: 0.0696869940922196
ROC train: 0.984417	val: 0.710344	test: 0.758273
PRC train: 0.804464	val: 0.253049	test: 0.233601

Epoch: 98
Loss: 0.06828656439380397
ROC train: 0.984760	val: 0.718544	test: 0.747779
PRC train: 0.819788	val: 0.238603	test: 0.208960

Epoch: 99
Loss: 0.07010380966998886
ROC train: 0.984362	val: 0.704534	test: 0.733657
PRC train: 0.811550	val: 0.213910	test: 0.169571

Epoch: 100
Loss: 0.067316592378819
ROC train: 0.984909	val: 0.702899	test: 0.725961
PRC train: 0.822394	val: 0.225155	test: 0.177974

Epoch: 101
Loss: 0.06876926597312595
ROC train: 0.983764	val: 0.718475	test: 0.749457
PRC train: 0.810924	val: 0.235599	test: 0.197168

Epoch: 102
Loss: 0.06876465052940148
ROC train: 0.983845	val: 0.709047	test: 0.745214
PRC train: 0.815614	val: 0.245259	test: 0.193866

Epoch: 103
Loss: 0.0685648539843687
ROC train: 0.985080	val: 0.706206	test: 0.752069
PRC train: 0.819243	val: 0.254737	test: 0.207902

Epoch: 104
Loss: 0.06670464465382511
ROC train: 0.983422	val: 0.703764	test: 0.744670
PRC train: 0.810323	val: 0.227476	test: 0.185969

Epoch: 105
Loss: 0.0679621092307863
ROC train: 0.983745	val: 0.705666	test: 0.755919
PRC train: 0.808021	val: 0.233635	test: 0.226958

Epoch: 106
Loss: 0.06748218639795471
ROC train: 0.986370	val: 0.707778	test: 0.749522
PRC train: 0.822227	val: 0.252230	test: 0.211646

Epoch: 107
Loss: 0.06704460018361641
ROC train: 0.986136	val: 0.701349	test: 0.738882
PRC train: 0.831198	val: 0.225042	test: 0.186276

Epoch: 108
Loss: 0.06760389563939798
ROC train: 0.985426	val: 0.715772	test: 0.761172
PRC train: 0.819508	val: 0.242191	test: 0.202891

Epoch: 109
Loss: 0.06573017035839213
ROC train: 0.987287	val: 0.702681	test: 0.743654
PRC train: 0.835197	val: 0.241074	test: 0.235637

Epoch: 110
Loss: 0.06629663594953099
ROC train: 0.986407	val: 0.710863	test: 0.749185
PRC train: 0.826157	val: 0.243842	test: 0.232282

Epoch: 111
Loss: 0.0646447334984798
ROC train: 0.985008	val: 0.710212	test: 0.755078
PRC train: 0.812409	val: 0.260054	test: 0.234404

Epoch: 112
Loss: 0.06503947066957509
ROC train: 0.988417	val: 0.707239	test: 0.736738
PRC train: 0.839625	val: 0.232765	test: 0.244566

Epoch: 113
Loss: 0.06470080983961396
ROC train: 0.985755	val: 0.710398	test: 0.751688
PRC train: 0.823120	val: 0.245416	test: 0.235643

Epoch: 114
Loss: 0.0631354449851153
ROC train: 0.987339	val: 0.706460	test: 0.745204
PRC train: 0.838538	val: 0.255704	test: 0.236887

Epoch: 115
Loss: 0.06520063011382998
ROC train: 0.988421	val: 0.712780	test: 0.740446
PRC train: 0.847985	val: 0.256624	test: 0.179579

Epoch: 116
Loss: 0.06561315539091772
ROC train: 0.989352	val: 0.705318	test: 0.740991
PRC train: 0.855347	val: 0.241467	test: 0.207230

Epoch: 117
Loss: 0.06384244755196379
ROC train: 0.988782	val: 0.707800	test: 0.738978
PRC train: 0.851302	val: 0.246766	test: 0.215671

Epoch: 118
Loss: 0.06306226499501356
ROC train: 0.987976	val: 0.712443	test: 0.765895
PRC train: 0.844373	val: 0.240943	test: 0.257903

Epoch: 119
Loss: 0.0646111586949458
ROC train: 0.988564	val: 0.710948	test: 0.754652
PRC train: 0.850129	val: 0.245760	test: 0.200538

Epoch: 120
Loss: 0.06292929032241336
ROC train: 0.988156	val: 0.709845	test: 0.744020
PRC train: 0.846999	val: 0.244508	test: 0.211299

Early stopping
Best (ROC):	 train: 0.972370	val: 0.728152	test: 0.752675
Best (PRC):	 train: 0.736438	val: 0.224044	test: 0.251102
All runs completed.
Epoch: 94
Loss: 0.07652329256985028
ROC train: 0.974256	val: 0.707428	test: 0.756425
PRC train: 0.753284	val: 0.259059	test: 0.114436

Epoch: 95
Loss: 0.07741575829372932
ROC train: 0.977693	val: 0.721115	test: 0.744651
PRC train: 0.761779	val: 0.275578	test: 0.134609

Epoch: 96
Loss: 0.07605916974876276
ROC train: 0.976499	val: 0.717578	test: 0.749316
PRC train: 0.769137	val: 0.268699	test: 0.121044

Epoch: 97
Loss: 0.0738956530007673
ROC train: 0.978270	val: 0.710155	test: 0.739133
PRC train: 0.776220	val: 0.266083	test: 0.123626

Epoch: 98
Loss: 0.07467378281115057
ROC train: 0.978738	val: 0.719217	test: 0.739852
PRC train: 0.777498	val: 0.256152	test: 0.134519

Epoch: 99
Loss: 0.07214867840313313
ROC train: 0.978801	val: 0.705760	test: 0.720846
PRC train: 0.774135	val: 0.224774	test: 0.092781

Epoch: 100
Loss: 0.07506984225235298
ROC train: 0.978256	val: 0.704675	test: 0.749079
PRC train: 0.771137	val: 0.279810	test: 0.147133

Epoch: 101
Loss: 0.07383741056085062
ROC train: 0.980424	val: 0.716549	test: 0.747105
PRC train: 0.771311	val: 0.274445	test: 0.130877

Epoch: 102
Loss: 0.073748972049259
ROC train: 0.977955	val: 0.711261	test: 0.740721
PRC train: 0.774783	val: 0.288672	test: 0.148150

Epoch: 103
Loss: 0.07207630956751639
ROC train: 0.979101	val: 0.706967	test: 0.740891
PRC train: 0.776187	val: 0.258041	test: 0.130326

Epoch: 104
Loss: 0.07357511199364397
ROC train: 0.979666	val: 0.696980	test: 0.743964
PRC train: 0.784229	val: 0.244448	test: 0.131764

Epoch: 105
Loss: 0.07244905315648728
ROC train: 0.979681	val: 0.703118	test: 0.752324
PRC train: 0.784915	val: 0.275268	test: 0.153381

Epoch: 106
Loss: 0.07188481566536624
ROC train: 0.982828	val: 0.715666	test: 0.727399
PRC train: 0.795417	val: 0.252268	test: 0.129525

Epoch: 107
Loss: 0.0722255292082791
ROC train: 0.982689	val: 0.711813	test: 0.743805
PRC train: 0.794799	val: 0.251928	test: 0.129486

Epoch: 108
Loss: 0.07164110732600293
ROC train: 0.983299	val: 0.722866	test: 0.732444
PRC train: 0.801704	val: 0.289862	test: 0.132875

Epoch: 109
Loss: 0.07183002970005371
ROC train: 0.981765	val: 0.714180	test: 0.747074
PRC train: 0.797971	val: 0.269758	test: 0.132743

Epoch: 110
Loss: 0.07053146225736776
ROC train: 0.981772	val: 0.699373	test: 0.734185
PRC train: 0.786998	val: 0.264137	test: 0.135909

Epoch: 111
Loss: 0.07111288184436111
ROC train: 0.983433	val: 0.704647	test: 0.726415
PRC train: 0.804331	val: 0.262337	test: 0.117217

Epoch: 112
Loss: 0.07189555386668892
ROC train: 0.983004	val: 0.714996	test: 0.732349
PRC train: 0.806929	val: 0.280345	test: 0.122667

Epoch: 113
Loss: 0.06959702324449325
ROC train: 0.985526	val: 0.706686	test: 0.731197
PRC train: 0.819932	val: 0.276721	test: 0.120118

Epoch: 114
Loss: 0.06948136606610879
ROC train: 0.981978	val: 0.703194	test: 0.704218
PRC train: 0.796373	val: 0.237926	test: 0.089331

Epoch: 115
Loss: 0.07023741268322385
ROC train: 0.982701	val: 0.705793	test: 0.732281
PRC train: 0.802256	val: 0.271163	test: 0.126299

Epoch: 116
Loss: 0.07052410040584095
ROC train: 0.981515	val: 0.689881	test: 0.721987
PRC train: 0.799904	val: 0.222461	test: 0.087663

Epoch: 117
Loss: 0.06913524348264904
ROC train: 0.985774	val: 0.723023	test: 0.735438
PRC train: 0.820892	val: 0.250036	test: 0.101048

Epoch: 118
Loss: 0.06811662693562258
ROC train: 0.984825	val: 0.708422	test: 0.716740
PRC train: 0.817410	val: 0.259084	test: 0.125716

Epoch: 119
Loss: 0.06852213779793444
ROC train: 0.985335	val: 0.715040	test: 0.722536
PRC train: 0.808394	val: 0.237244	test: 0.102987

Epoch: 120
Loss: 0.06907581287286603
ROC train: 0.982456	val: 0.702645	test: 0.739781
PRC train: 0.802330	val: 0.243953	test: 0.111404

Early stopping
Best (ROC):	 train: 0.966964	val: 0.727556	test: 0.750640
Best (PRC):	 train: 0.723522	val: 0.278317	test: 0.117837

ROC train: 0.973989	val: 0.791780	test: 0.743852
PRC train: 0.760235	val: 0.273897	test: 0.160045

Epoch: 95
Loss: 0.0752368015893203
ROC train: 0.975670	val: 0.785730	test: 0.734246
PRC train: 0.769167	val: 0.272588	test: 0.170291

Epoch: 96
Loss: 0.07614658763226954
ROC train: 0.976969	val: 0.787374	test: 0.740497
PRC train: 0.774321	val: 0.279858	test: 0.156605

Epoch: 97
Loss: 0.07396778017395934
ROC train: 0.979348	val: 0.774734	test: 0.724912
PRC train: 0.785146	val: 0.264999	test: 0.147312

Epoch: 98
Loss: 0.07593142380100387
ROC train: 0.979443	val: 0.800093	test: 0.744597
PRC train: 0.789195	val: 0.314676	test: 0.162342

Epoch: 99
Loss: 0.0726142795574131
ROC train: 0.978559	val: 0.794098	test: 0.763012
PRC train: 0.779520	val: 0.311051	test: 0.176297

Epoch: 100
Loss: 0.07324667493741097
ROC train: 0.976534	val: 0.794667	test: 0.769903
PRC train: 0.767908	val: 0.323067	test: 0.201569

Epoch: 101
Loss: 0.07387450295838047
ROC train: 0.980156	val: 0.793792	test: 0.756092
PRC train: 0.787350	val: 0.297314	test: 0.165466

Epoch: 102
Loss: 0.07375246061068684
ROC train: 0.981283	val: 0.797781	test: 0.754646
PRC train: 0.796084	val: 0.302616	test: 0.165610

Epoch: 103
Loss: 0.07232358674496026
ROC train: 0.978865	val: 0.784370	test: 0.748800
PRC train: 0.784262	val: 0.296477	test: 0.169907

Epoch: 104
Loss: 0.07184818070773653
ROC train: 0.981237	val: 0.785509	test: 0.754404
PRC train: 0.785252	val: 0.292592	test: 0.176642

Epoch: 105
Loss: 0.07098296891486754
ROC train: 0.978979	val: 0.796716	test: 0.737059
PRC train: 0.781576	val: 0.298109	test: 0.173131

Epoch: 106
Loss: 0.07314440889594301
ROC train: 0.981786	val: 0.792463	test: 0.748614
PRC train: 0.797237	val: 0.288765	test: 0.187674

Epoch: 107
Loss: 0.07027169506060323
ROC train: 0.980904	val: 0.770735	test: 0.743661
PRC train: 0.791296	val: 0.274750	test: 0.163467

Epoch: 108
Loss: 0.07242354528803413
ROC train: 0.980412	val: 0.781470	test: 0.757168
PRC train: 0.791074	val: 0.287800	test: 0.177053

Epoch: 109
Loss: 0.07092622614418807
ROC train: 0.980186	val: 0.784943	test: 0.759161
PRC train: 0.794184	val: 0.319362	test: 0.182283

Epoch: 110
Loss: 0.07087421219243513
ROC train: 0.982037	val: 0.773298	test: 0.759275
PRC train: 0.802869	val: 0.287742	test: 0.179314

Epoch: 111
Loss: 0.0690390615286051
ROC train: 0.981423	val: 0.780953	test: 0.739157
PRC train: 0.791299	val: 0.263944	test: 0.153177

Epoch: 112
Loss: 0.06903269006695166
ROC train: 0.981515	val: 0.782199	test: 0.756158
PRC train: 0.800577	val: 0.291438	test: 0.175507

Epoch: 113
Loss: 0.0719496347638337
ROC train: 0.983096	val: 0.786394	test: 0.758578
PRC train: 0.805593	val: 0.272746	test: 0.167737

Epoch: 114
Loss: 0.0702101941419674
ROC train: 0.984622	val: 0.796722	test: 0.760152
PRC train: 0.809884	val: 0.275682	test: 0.158872

Epoch: 115
Loss: 0.06857408796709531
ROC train: 0.982626	val: 0.778108	test: 0.742921
PRC train: 0.807774	val: 0.315061	test: 0.190707

Epoch: 116
Loss: 0.07142478468966158
ROC train: 0.981874	val: 0.797898	test: 0.756930
PRC train: 0.798293	val: 0.263002	test: 0.182523

Epoch: 117
Loss: 0.06850512064329939
ROC train: 0.983949	val: 0.790408	test: 0.758997
PRC train: 0.821741	val: 0.267365	test: 0.162445

Epoch: 118
Loss: 0.06879920181007997
ROC train: 0.984752	val: 0.779805	test: 0.756094
PRC train: 0.816120	val: 0.265138	test: 0.178494

Epoch: 119
Loss: 0.0672614854481481
ROC train: 0.986316	val: 0.772012	test: 0.751486
PRC train: 0.830216	val: 0.282524	test: 0.188612

Epoch: 120
Loss: 0.06686923914508963
ROC train: 0.983383	val: 0.780760	test: 0.740028
PRC train: 0.806654	val: 0.262250	test: 0.167897

Early stopping
Best (ROC):	 train: 0.931968	val: 0.810360	test: 0.753035
Best (PRC):	 train: 0.620318	val: 0.372715	test: 0.203906

ROC train: 0.974832	val: 0.779419	test: 0.747625
PRC train: 0.761581	val: 0.299654	test: 0.159382

Epoch: 95
Loss: 0.07556633224662292
ROC train: 0.975211	val: 0.782392	test: 0.750445
PRC train: 0.767109	val: 0.302094	test: 0.177857

Epoch: 96
Loss: 0.0755366536082549
ROC train: 0.975396	val: 0.789875	test: 0.751990
PRC train: 0.750930	val: 0.314823	test: 0.187790

Epoch: 97
Loss: 0.07691989810501412
ROC train: 0.975656	val: 0.764866	test: 0.747546
PRC train: 0.761803	val: 0.284809	test: 0.144071

Epoch: 98
Loss: 0.07550381616544928
ROC train: 0.975650	val: 0.759351	test: 0.738751
PRC train: 0.769538	val: 0.270762	test: 0.164061

Epoch: 99
Loss: 0.07350726234213231
ROC train: 0.976229	val: 0.772040	test: 0.740903
PRC train: 0.762662	val: 0.279404	test: 0.172318

Epoch: 100
Loss: 0.07362628035765109
ROC train: 0.978381	val: 0.778145	test: 0.765639
PRC train: 0.775033	val: 0.306033	test: 0.199244

Epoch: 101
Loss: 0.07475841385990441
ROC train: 0.977903	val: 0.772478	test: 0.749850
PRC train: 0.777246	val: 0.297511	test: 0.177874

Epoch: 102
Loss: 0.0732986539112775
ROC train: 0.979155	val: 0.776764	test: 0.744864
PRC train: 0.786059	val: 0.329619	test: 0.165965

Epoch: 103
Loss: 0.0731732797412332
ROC train: 0.980090	val: 0.781042	test: 0.769515
PRC train: 0.783441	val: 0.301919	test: 0.205679

Epoch: 104
Loss: 0.07394597155437198
ROC train: 0.978435	val: 0.779097	test: 0.759273
PRC train: 0.777227	val: 0.314011	test: 0.190536

Epoch: 105
Loss: 0.07365635329523439
ROC train: 0.980004	val: 0.785292	test: 0.758377
PRC train: 0.793967	val: 0.293568	test: 0.184615

Epoch: 106
Loss: 0.07215997538367509
ROC train: 0.980078	val: 0.767499	test: 0.749800
PRC train: 0.788878	val: 0.285713	test: 0.208015

Epoch: 107
Loss: 0.07246939067731334
ROC train: 0.979408	val: 0.772046	test: 0.746536
PRC train: 0.789640	val: 0.291882	test: 0.169406

Epoch: 108
Loss: 0.07133110011738197
ROC train: 0.979899	val: 0.777493	test: 0.746457
PRC train: 0.786772	val: 0.281197	test: 0.177091

Epoch: 109
Loss: 0.07295089778508985
ROC train: 0.981979	val: 0.772897	test: 0.756747
PRC train: 0.792159	val: 0.301920	test: 0.209346

Epoch: 110
Loss: 0.0724626339938592
ROC train: 0.981696	val: 0.761620	test: 0.756930
PRC train: 0.795005	val: 0.288784	test: 0.178635

Epoch: 111
Loss: 0.0712160822502351
ROC train: 0.980271	val: 0.772223	test: 0.755407
PRC train: 0.793800	val: 0.290492	test: 0.192218

Epoch: 112
Loss: 0.07128369580428986
ROC train: 0.981479	val: 0.770892	test: 0.755768
PRC train: 0.799626	val: 0.308156	test: 0.175905

Epoch: 113
Loss: 0.07063374868839274
ROC train: 0.981390	val: 0.791437	test: 0.763331
PRC train: 0.794481	val: 0.273958	test: 0.171042

Epoch: 114
Loss: 0.07065729314817523
ROC train: 0.981679	val: 0.776498	test: 0.761255
PRC train: 0.804426	val: 0.304021	test: 0.181889

Epoch: 115
Loss: 0.07248187312233438
ROC train: 0.982632	val: 0.784101	test: 0.762792
PRC train: 0.805344	val: 0.308619	test: 0.174434

Epoch: 116
Loss: 0.07247245835373525
ROC train: 0.983959	val: 0.783026	test: 0.762427
PRC train: 0.813390	val: 0.296617	test: 0.174284

Epoch: 117
Loss: 0.06933818311717323
ROC train: 0.984520	val: 0.790402	test: 0.769675
PRC train: 0.821381	val: 0.314414	test: 0.179398

Epoch: 118
Loss: 0.06941639820716541
ROC train: 0.983475	val: 0.783081	test: 0.750492
PRC train: 0.810061	val: 0.299918	test: 0.163289

Epoch: 119
Loss: 0.06785481466687016
ROC train: 0.982991	val: 0.776626	test: 0.752440
PRC train: 0.805161	val: 0.308809	test: 0.182950

Epoch: 120
Loss: 0.06840176645762654
ROC train: 0.982168	val: 0.778222	test: 0.757444
PRC train: 0.802156	val: 0.280977	test: 0.169541

Early stopping
Best (ROC):	 train: 0.914183	val: 0.802420	test: 0.760418
Best (PRC):	 train: 0.576407	val: 0.317641	test: 0.159653

ROC train: 0.972806	val: 0.805308	test: 0.764783
PRC train: 0.742153	val: 0.283301	test: 0.201762

Epoch: 95
Loss: 0.07668826831389276
ROC train: 0.973955	val: 0.796403	test: 0.771280
PRC train: 0.761837	val: 0.276435	test: 0.166953

Epoch: 96
Loss: 0.07569172989280344
ROC train: 0.978091	val: 0.798951	test: 0.765903
PRC train: 0.777594	val: 0.286547	test: 0.160210

Epoch: 97
Loss: 0.07662526516009951
ROC train: 0.975204	val: 0.790981	test: 0.766247
PRC train: 0.768354	val: 0.270321	test: 0.175494

Epoch: 98
Loss: 0.07549591470614292
ROC train: 0.976243	val: 0.800200	test: 0.772296
PRC train: 0.767157	val: 0.268563	test: 0.150962

Epoch: 99
Loss: 0.07608745163598779
ROC train: 0.974944	val: 0.777410	test: 0.757419
PRC train: 0.763451	val: 0.277650	test: 0.162396

Epoch: 100
Loss: 0.07373685584877111
ROC train: 0.977168	val: 0.790766	test: 0.765766
PRC train: 0.775679	val: 0.285829	test: 0.165328

Epoch: 101
Loss: 0.0735317280515269
ROC train: 0.979353	val: 0.796921	test: 0.770123
PRC train: 0.786978	val: 0.275948	test: 0.168223

Epoch: 102
Loss: 0.07501875880837461
ROC train: 0.978899	val: 0.790427	test: 0.772833
PRC train: 0.779429	val: 0.271482	test: 0.163444

Epoch: 103
Loss: 0.07436178695077594
ROC train: 0.978854	val: 0.806958	test: 0.755275
PRC train: 0.780582	val: 0.278407	test: 0.141453

Epoch: 104
Loss: 0.07336669782898177
ROC train: 0.979688	val: 0.798023	test: 0.757956
PRC train: 0.789032	val: 0.275450	test: 0.173785

Epoch: 105
Loss: 0.07274500311324496
ROC train: 0.981165	val: 0.801979	test: 0.768031
PRC train: 0.794164	val: 0.307984	test: 0.161456

Epoch: 106
Loss: 0.07302829937466995
ROC train: 0.981522	val: 0.787839	test: 0.759739
PRC train: 0.795303	val: 0.270075	test: 0.154870

Epoch: 107
Loss: 0.07345799339515498
ROC train: 0.980617	val: 0.789848	test: 0.775993
PRC train: 0.786572	val: 0.292798	test: 0.185928

Epoch: 108
Loss: 0.07160117210616841
ROC train: 0.979904	val: 0.802870	test: 0.752179
PRC train: 0.792700	val: 0.274650	test: 0.147615

Epoch: 109
Loss: 0.0736088217590957
ROC train: 0.980787	val: 0.796854	test: 0.748659
PRC train: 0.793199	val: 0.259013	test: 0.139289

Epoch: 110
Loss: 0.07311603956532485
ROC train: 0.980804	val: 0.793161	test: 0.760339
PRC train: 0.789121	val: 0.287918	test: 0.165051

Epoch: 111
Loss: 0.07107538822935157
ROC train: 0.981465	val: 0.798406	test: 0.749001
PRC train: 0.798746	val: 0.279893	test: 0.189944

Epoch: 112
Loss: 0.07142894163754719
ROC train: 0.982972	val: 0.785145	test: 0.763018
PRC train: 0.804283	val: 0.244361	test: 0.146451

Epoch: 113
Loss: 0.06978063815394747
ROC train: 0.980107	val: 0.798421	test: 0.772806
PRC train: 0.797155	val: 0.282302	test: 0.157032

Epoch: 114
Loss: 0.06962543411378523
ROC train: 0.979545	val: 0.787337	test: 0.751256
PRC train: 0.781322	val: 0.256305	test: 0.137615

Epoch: 115
Loss: 0.06916006260999599
ROC train: 0.983150	val: 0.794943	test: 0.743948
PRC train: 0.807865	val: 0.264629	test: 0.143702

Epoch: 116
Loss: 0.06891258801246265
ROC train: 0.983819	val: 0.802564	test: 0.770847
PRC train: 0.814303	val: 0.276408	test: 0.149611

Epoch: 117
Loss: 0.07165947518263062
ROC train: 0.984225	val: 0.791933	test: 0.759590
PRC train: 0.817217	val: 0.262640	test: 0.151237

Epoch: 118
Loss: 0.06836312406263118
ROC train: 0.983914	val: 0.790965	test: 0.763568
PRC train: 0.811648	val: 0.272340	test: 0.153281

Epoch: 119
Loss: 0.06844750998441315
ROC train: 0.980419	val: 0.795849	test: 0.753259
PRC train: 0.799473	val: 0.263198	test: 0.165743

Epoch: 120
Loss: 0.06878158489194171
ROC train: 0.983901	val: 0.797781	test: 0.764970
PRC train: 0.809211	val: 0.270224	test: 0.164846

Early stopping
Best (ROC):	 train: 0.971303	val: 0.810041	test: 0.765395
Best (PRC):	 train: 0.742476	val: 0.278891	test: 0.155458
All runs completed.

ROC train: 0.980565	val: 0.727189	test: 0.760817
PRC train: 0.790209	val: 0.299035	test: 0.167588

Epoch: 95
Loss: 0.07390183818570142
ROC train: 0.980769	val: 0.718836	test: 0.760395
PRC train: 0.793671	val: 0.281566	test: 0.158190

Epoch: 96
Loss: 0.07277396110332432
ROC train: 0.978187	val: 0.731455	test: 0.763069
PRC train: 0.773192	val: 0.276295	test: 0.159695

Epoch: 97
Loss: 0.07305403299668665
ROC train: 0.978536	val: 0.712765	test: 0.769625
PRC train: 0.783747	val: 0.279324	test: 0.149203

Epoch: 98
Loss: 0.0743976798536924
ROC train: 0.977556	val: 0.712166	test: 0.763709
PRC train: 0.787974	val: 0.275686	test: 0.138211

Epoch: 99
Loss: 0.07386988066164585
ROC train: 0.979091	val: 0.710979	test: 0.772705
PRC train: 0.792133	val: 0.297018	test: 0.178504

Epoch: 100
Loss: 0.07140664913339854
ROC train: 0.981421	val: 0.712739	test: 0.770363
PRC train: 0.795356	val: 0.265440	test: 0.132798

Epoch: 101
Loss: 0.07185775969549438
ROC train: 0.982114	val: 0.710944	test: 0.762935
PRC train: 0.800236	val: 0.278121	test: 0.128904

Epoch: 102
Loss: 0.07256200783549498
ROC train: 0.981855	val: 0.713395	test: 0.760177
PRC train: 0.792851	val: 0.274661	test: 0.127378

Epoch: 103
Loss: 0.0702318326030094
ROC train: 0.982749	val: 0.707703	test: 0.768891
PRC train: 0.797710	val: 0.269465	test: 0.134161

Epoch: 104
Loss: 0.07114379828363285
ROC train: 0.983474	val: 0.724662	test: 0.786712
PRC train: 0.804715	val: 0.294897	test: 0.163310

Epoch: 105
Loss: 0.07054346241134798
ROC train: 0.982801	val: 0.717480	test: 0.770655
PRC train: 0.801689	val: 0.276281	test: 0.132847

Epoch: 106
Loss: 0.06964931291685168
ROC train: 0.981675	val: 0.730136	test: 0.783523
PRC train: 0.787031	val: 0.294405	test: 0.160824

Epoch: 107
Loss: 0.07180697720533145
ROC train: 0.983731	val: 0.736391	test: 0.767303
PRC train: 0.805676	val: 0.285986	test: 0.140320

Epoch: 108
Loss: 0.06822808089133141
ROC train: 0.983437	val: 0.715718	test: 0.752189
PRC train: 0.799950	val: 0.244770	test: 0.103757

Epoch: 109
Loss: 0.07059928236460149
ROC train: 0.983407	val: 0.721504	test: 0.754800
PRC train: 0.804279	val: 0.266444	test: 0.151787

Epoch: 110
Loss: 0.070634327813543
ROC train: 0.983693	val: 0.715493	test: 0.734448
PRC train: 0.805648	val: 0.239232	test: 0.115424

Epoch: 111
Loss: 0.0691014415513901
ROC train: 0.985089	val: 0.716441	test: 0.773882
PRC train: 0.814442	val: 0.287075	test: 0.149730

Epoch: 112
Loss: 0.06849393269490653
ROC train: 0.983835	val: 0.713711	test: 0.763572
PRC train: 0.815191	val: 0.267032	test: 0.140999

Epoch: 113
Loss: 0.06953111742593364
ROC train: 0.985005	val: 0.713413	test: 0.773554
PRC train: 0.818135	val: 0.255168	test: 0.139596

Epoch: 114
Loss: 0.06740685455588344
ROC train: 0.985657	val: 0.721083	test: 0.780821
PRC train: 0.821906	val: 0.288468	test: 0.153656

Epoch: 115
Loss: 0.06594842846578765
ROC train: 0.983977	val: 0.713157	test: 0.759479
PRC train: 0.814699	val: 0.273618	test: 0.150923

Epoch: 116
Loss: 0.06778350731953518
ROC train: 0.987795	val: 0.723642	test: 0.764219
PRC train: 0.837207	val: 0.283283	test: 0.152040

Epoch: 117
Loss: 0.06607099940232475
ROC train: 0.985776	val: 0.709310	test: 0.763426
PRC train: 0.825481	val: 0.272211	test: 0.145419

Epoch: 118
Loss: 0.06922392044333862
ROC train: 0.985559	val: 0.710588	test: 0.773864
PRC train: 0.823620	val: 0.277062	test: 0.146408

Epoch: 119
Loss: 0.06630259297638379
ROC train: 0.986669	val: 0.715272	test: 0.767979
PRC train: 0.835353	val: 0.287626	test: 0.141231

Epoch: 120
Loss: 0.06711225243272324
ROC train: 0.983867	val: 0.716942	test: 0.770185
PRC train: 0.817348	val: 0.289917	test: 0.164771

Epoch: 121
Loss: 0.0683417799214117
ROC train: 0.988074	val: 0.728759	test: 0.769075
PRC train: 0.836624	val: 0.275716	test: 0.144808

Epoch: 122
Loss: 0.06409494153048642
ROC train: 0.986150	val: 0.712763	test: 0.772332
PRC train: 0.829330	val: 0.257379	test: 0.139966

Epoch: 123
Loss: 0.06580111712782381
ROC train: 0.987430	val: 0.705445	test: 0.770040
PRC train: 0.832906	val: 0.277983	test: 0.136229

Epoch: 124
Loss: 0.06523423404280859
ROC train: 0.987536	val: 0.709349	test: 0.753660
PRC train: 0.833375	val: 0.266900	test: 0.145929

Epoch: 125
Loss: 0.06612936120945721
ROC train: 0.987086	val: 0.714310	test: 0.762415
PRC train: 0.829893	val: 0.253841	test: 0.130102

Epoch: 126
Loss: 0.06620219216263559
ROC train: 0.988064	val: 0.723587	test: 0.774844
PRC train: 0.834800	val: 0.282428	test: 0.152499

Epoch: 127
Loss: 0.06503590003594859
ROC train: 0.987177	val: 0.708007	test: 0.766992
PRC train: 0.833783	val: 0.267773	test: 0.143506

Epoch: 128
Loss: 0.06540500390731643
ROC train: 0.987553	val: 0.717356	test: 0.770671
PRC train: 0.834843	val: 0.245017	test: 0.147768

Epoch: 129
Loss: 0.06293985876389575
ROC train: 0.989152	val: 0.706587	test: 0.765202
PRC train: 0.849580	val: 0.269133	test: 0.131245

Epoch: 130
Loss: 0.06284148552651184
ROC train: 0.988581	val: 0.718083	test: 0.784139
PRC train: 0.846093	val: 0.275783	test: 0.149518

Epoch: 131
Loss: 0.06334713790678889
ROC train: 0.989393	val: 0.709904	test: 0.761535
PRC train: 0.848532	val: 0.258619	test: 0.140914

Epoch: 132
Loss: 0.06301957644968953
ROC train: 0.990247	val: 0.720708	test: 0.772209
PRC train: 0.859758	val: 0.240877	test: 0.131086

Epoch: 133
Loss: 0.06295096495129934
ROC train: 0.988226	val: 0.708023	test: 0.749191
PRC train: 0.845428	val: 0.267142	test: 0.124778

Epoch: 134
Loss: 0.06247174841811118
ROC train: 0.988745	val: 0.709995	test: 0.769413
PRC train: 0.847426	val: 0.254194	test: 0.129431

Epoch: 135
Loss: 0.06138010616567528
ROC train: 0.989431	val: 0.718709	test: 0.769690
PRC train: 0.849580	val: 0.281905	test: 0.146364

Epoch: 136
Loss: 0.061821494420035365
ROC train: 0.989164	val: 0.714258	test: 0.760423
PRC train: 0.851411	val: 0.250892	test: 0.133491

Epoch: 137
Loss: 0.062434941487451864
ROC train: 0.989531	val: 0.715883	test: 0.768461
PRC train: 0.850262	val: 0.250369	test: 0.140887

Epoch: 138
Loss: 0.0625271813858985
ROC train: 0.989332	val: 0.721467	test: 0.750575
PRC train: 0.853540	val: 0.264814	test: 0.127445

Epoch: 139
Loss: 0.06045320806709285
ROC train: 0.989438	val: 0.715844	test: 0.763316
PRC train: 0.855298	val: 0.260876	test: 0.132783

Epoch: 140
Loss: 0.061701396117327695
ROC train: 0.988995	val: 0.722245	test: 0.772538
PRC train: 0.851199	val: 0.278099	test: 0.137717

Epoch: 141
Loss: 0.06115135793383905
ROC train: 0.989990	val: 0.715863	test: 0.763205
PRC train: 0.850972	val: 0.244972	test: 0.132604

Epoch: 142
Loss: 0.06235038040368809
ROC train: 0.990841	val: 0.707132	test: 0.769405
PRC train: 0.864301	val: 0.259043	test: 0.152087

Early stopping
Best (ROC):	 train: 0.983731	val: 0.736391	test: 0.767303
Best (PRC):	 train: 0.805676	val: 0.285986	test: 0.140320

ROC train: 0.981003	val: 0.714181	test: 0.768712
PRC train: 0.778256	val: 0.239373	test: 0.135937

Epoch: 95
Loss: 0.07410404652493585
ROC train: 0.980771	val: 0.733801	test: 0.761587
PRC train: 0.790458	val: 0.288227	test: 0.170694

Epoch: 96
Loss: 0.07362104508366984
ROC train: 0.979168	val: 0.726857	test: 0.758463
PRC train: 0.774370	val: 0.278223	test: 0.158486

Epoch: 97
Loss: 0.07364348160765799
ROC train: 0.981188	val: 0.717490	test: 0.776706
PRC train: 0.792454	val: 0.276925	test: 0.157703

Epoch: 98
Loss: 0.0704905779166054
ROC train: 0.979809	val: 0.710983	test: 0.772872
PRC train: 0.791386	val: 0.268324	test: 0.153129

Epoch: 99
Loss: 0.07245514428809381
ROC train: 0.979596	val: 0.715462	test: 0.764460
PRC train: 0.788007	val: 0.278148	test: 0.144579

Epoch: 100
Loss: 0.07218100501959113
ROC train: 0.981768	val: 0.708646	test: 0.757534
PRC train: 0.797053	val: 0.251572	test: 0.134593

Epoch: 101
Loss: 0.07147161796752546
ROC train: 0.982434	val: 0.719711	test: 0.785018
PRC train: 0.802311	val: 0.279345	test: 0.175869

Epoch: 102
Loss: 0.07164151945456779
ROC train: 0.983194	val: 0.720716	test: 0.769044
PRC train: 0.801856	val: 0.282499	test: 0.160344

Epoch: 103
Loss: 0.07065803697071008
ROC train: 0.982269	val: 0.725920	test: 0.765035
PRC train: 0.795802	val: 0.268552	test: 0.157910

Epoch: 104
Loss: 0.07233745423444782
ROC train: 0.982272	val: 0.708119	test: 0.758237
PRC train: 0.810501	val: 0.281923	test: 0.172615

Epoch: 105
Loss: 0.07018827995018477
ROC train: 0.982309	val: 0.719539	test: 0.768140
PRC train: 0.805524	val: 0.298313	test: 0.196815

Epoch: 106
Loss: 0.07193855545595802
ROC train: 0.983750	val: 0.720484	test: 0.763374
PRC train: 0.817118	val: 0.259406	test: 0.148055

Epoch: 107
Loss: 0.07065601513934157
ROC train: 0.980354	val: 0.721228	test: 0.744875
PRC train: 0.788259	val: 0.264207	test: 0.146920

Epoch: 108
Loss: 0.07047227029640916
ROC train: 0.982618	val: 0.718122	test: 0.759558
PRC train: 0.804319	val: 0.224927	test: 0.122559

Epoch: 109
Loss: 0.06988478765717233
ROC train: 0.985261	val: 0.729888	test: 0.754781
PRC train: 0.817810	val: 0.273895	test: 0.151505

Epoch: 110
Loss: 0.07095341299858535
ROC train: 0.985867	val: 0.713952	test: 0.754754
PRC train: 0.824127	val: 0.258595	test: 0.142518

Epoch: 111
Loss: 0.07067284902818609
ROC train: 0.983516	val: 0.727280	test: 0.772638
PRC train: 0.807798	val: 0.300841	test: 0.170341

Epoch: 112
Loss: 0.06878964509491713
ROC train: 0.983102	val: 0.710197	test: 0.752088
PRC train: 0.810850	val: 0.247076	test: 0.132415

Epoch: 113
Loss: 0.06951571116760617
ROC train: 0.985242	val: 0.734111	test: 0.748781
PRC train: 0.821466	val: 0.265139	test: 0.119707

Epoch: 114
Loss: 0.06789076145491972
ROC train: 0.986152	val: 0.719661	test: 0.765359
PRC train: 0.820310	val: 0.280289	test: 0.185831

Epoch: 115
Loss: 0.06757714819380274
ROC train: 0.983079	val: 0.708396	test: 0.751677
PRC train: 0.810174	val: 0.244013	test: 0.155182

Epoch: 116
Loss: 0.06663158881071445
ROC train: 0.986947	val: 0.715840	test: 0.773721
PRC train: 0.824228	val: 0.274006	test: 0.171039

Epoch: 117
Loss: 0.06672691741074739
ROC train: 0.986056	val: 0.712822	test: 0.760527
PRC train: 0.827731	val: 0.265586	test: 0.153281

Epoch: 118
Loss: 0.0684935828825031
ROC train: 0.986262	val: 0.719748	test: 0.765015
PRC train: 0.820249	val: 0.270955	test: 0.148145

Epoch: 119
Loss: 0.06661366068408493
ROC train: 0.984455	val: 0.719912	test: 0.775346
PRC train: 0.812735	val: 0.295170	test: 0.187997

Epoch: 120
Loss: 0.06823496891197023
ROC train: 0.985890	val: 0.721619	test: 0.750783
PRC train: 0.827712	val: 0.243588	test: 0.110542

Epoch: 121
Loss: 0.06628353185359587
ROC train: 0.987198	val: 0.710403	test: 0.749552
PRC train: 0.833694	val: 0.242224	test: 0.115554

Epoch: 122
Loss: 0.06590085509478837
ROC train: 0.985277	val: 0.705370	test: 0.782160
PRC train: 0.827528	val: 0.271819	test: 0.181767

Epoch: 123
Loss: 0.06489866859853702
ROC train: 0.987618	val: 0.719630	test: 0.769836
PRC train: 0.841680	val: 0.272915	test: 0.150225

Epoch: 124
Loss: 0.06575680467697534
ROC train: 0.987711	val: 0.721197	test: 0.768699
PRC train: 0.841866	val: 0.269545	test: 0.156150

Epoch: 125
Loss: 0.0655946365941892
ROC train: 0.987946	val: 0.714774	test: 0.772617
PRC train: 0.845704	val: 0.254697	test: 0.150625

Epoch: 126
Loss: 0.06432344817180624
ROC train: 0.987688	val: 0.718204	test: 0.762794
PRC train: 0.837106	val: 0.278047	test: 0.173437

Epoch: 127
Loss: 0.0661117350899239
ROC train: 0.987406	val: 0.703592	test: 0.776563
PRC train: 0.841890	val: 0.247747	test: 0.151681

Epoch: 128
Loss: 0.065719785714266
ROC train: 0.985247	val: 0.715091	test: 0.747741
PRC train: 0.819311	val: 0.258965	test: 0.155101

Epoch: 129
Loss: 0.06393533434502821
ROC train: 0.988482	val: 0.712932	test: 0.740288
PRC train: 0.843997	val: 0.253779	test: 0.158178

Epoch: 130
Loss: 0.06374046550375313
ROC train: 0.989185	val: 0.719773	test: 0.756110
PRC train: 0.853309	val: 0.269167	test: 0.160596

Epoch: 131
Loss: 0.06346137867992241
ROC train: 0.987184	val: 0.722145	test: 0.755384
PRC train: 0.843141	val: 0.240564	test: 0.132796

Epoch: 132
Loss: 0.06353676036864175
ROC train: 0.988307	val: 0.715948	test: 0.736425
PRC train: 0.849082	val: 0.246828	test: 0.128039

Epoch: 133
Loss: 0.06380748594855198
ROC train: 0.989642	val: 0.727574	test: 0.758693
PRC train: 0.853474	val: 0.221463	test: 0.125973

Epoch: 134
Loss: 0.0647285013432892
ROC train: 0.990003	val: 0.724510	test: 0.763841
PRC train: 0.858902	val: 0.265837	test: 0.153653

Epoch: 135
Loss: 0.06310588040804939
ROC train: 0.989607	val: 0.722741	test: 0.774178
PRC train: 0.856374	val: 0.273135	test: 0.166963

Epoch: 136
Loss: 0.06176198863439378
ROC train: 0.989708	val: 0.705666	test: 0.765913
PRC train: 0.859427	val: 0.267830	test: 0.168737

Epoch: 137
Loss: 0.062130130102816906
ROC train: 0.989782	val: 0.723131	test: 0.756276
PRC train: 0.854969	val: 0.244118	test: 0.128393

Epoch: 138
Loss: 0.06194955141220866
ROC train: 0.986641	val: 0.703217	test: 0.768404
PRC train: 0.835655	val: 0.225345	test: 0.140982

Epoch: 139
Loss: 0.061251688312075704
ROC train: 0.988955	val: 0.708164	test: 0.768229
PRC train: 0.844618	val: 0.247532	test: 0.150059

Epoch: 140
Loss: 0.06223086311120461
ROC train: 0.989594	val: 0.707979	test: 0.777361
PRC train: 0.858475	val: 0.249051	test: 0.179810

Epoch: 141
Loss: 0.060872081599859025
ROC train: 0.988924	val: 0.712617	test: 0.764108
PRC train: 0.857465	val: 0.234468	test: 0.174142

Epoch: 142
Loss: 0.05950453964744672
ROC train: 0.991212	val: 0.718645	test: 0.764693
PRC train: 0.869271	val: 0.254358	test: 0.189434

Epoch: 143
Loss: 0.06003932741258392
ROC train: 0.991146	val: 0.706828	test: 0.761820
PRC train: 0.869596	val: 0.246797	test: 0.154501

Epoch: 144
Loss: 0.06176212141227287
ROC train: 0.990780	val: 0.713543	test: 0.761116
PRC train: 0.868865	val: 0.255698	test: 0.160624

Epoch: 145
Loss: 0.059258873273853756
ROC train: 0.990699	val: 0.718149	test: 0.768293
PRC train: 0.868625	val: 0.279939	test: 0.200660

Epoch: 146
Loss: 0.058873953252853765
ROC train: 0.990088	val: 0.699236	test: 0.765081
PRC train: 0.863263	val: 0.239075	test: 0.136758

Epoch: 147
Loss: 0.05999409680875203
ROC train: 0.990754	val: 0.709622	test: 0.757021
PRC train: 0.869149	val: 0.232852	test: 0.135691

Epoch: 148
Loss: 0.0595245900350083
ROC train: 0.991355	val: 0.709135	test: 0.754677
PRC train: 0.874337	val: 0.240637	test: 0.147816

Early stopping
Best (ROC):	 train: 0.985242	val: 0.734111	test: 0.748781
Best (PRC):	 train: 0.821466	val: 0.265139	test: 0.119707
All runs completed.
