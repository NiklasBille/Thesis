>>> Starting run for dataset: esol
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running RANDOM configs_static_noise_experiments/GraphCL/esol/noise=0.0.yml on cuda:0
Running RANDOM configs_static_noise_experiments/GraphCL/esol/noise=0.05.yml on cuda:1
Running RANDOM configs_static_noise_experiments/GraphCL/esol/noise=0.1.yml on cuda:2
Running RANDOM configs_static_noise_experiments/GraphCL/esol/noise=0.2.yml on cuda:3
Starting process for seed 1: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.0.yml --runseed 1 --device cuda:0
Starting process for seed 2: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.0.yml --runseed 2 --device cuda:0
Starting process for seed 3: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.0.yml --runseed 3 --device cuda:0
Starting process for seed 1: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.2.yml --runseed 1 --device cuda:3Starting process for seed 1: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.05.yml --runseed 1 --device cuda:1

Starting process for seed 2: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.2.yml --runseed 2 --device cuda:3
Starting process for seed 2: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.05.yml --runseed 2 --device cuda:1
Starting process for seed 3: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.2.yml --runseed 3 --device cuda:3
Starting process for seed 3: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.05.yml --runseed 3 --device cuda:1
Starting process for seed 1: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.1.yml --runseed 1 --device cuda:2
Starting process for seed 2: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.1.yml --runseed 2 --device cuda:2
Starting process for seed 3: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.1.yml --runseed 3 --device cuda:2
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/esol/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/esol/noise=0.0/esol_scaff_1_20-05_14-43-31  ]
[ Using Seed :  1  ]
[ Using device :  cuda:0  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 12.241846799850464
RMSE train: 3.287439	val: 4.213537	test: 4.285330
MAE train: 2.715833	val: 3.821334	test: 3.781186

Epoch: 2
Loss: 11.368069410324097
RMSE train: 3.132503	val: 4.169676	test: 4.239278
MAE train: 2.576391	val: 3.821701	test: 3.806384

Epoch: 3
Loss: 9.928165912628174
RMSE train: 3.068712	val: 4.283286	test: 4.324579
MAE train: 2.512656	val: 3.985633	test: 3.957094

Epoch: 4
Loss: 8.949111580848694
RMSE train: 3.037670	val: 4.356874	test: 4.400817
MAE train: 2.500626	val: 4.100115	test: 4.079444

Epoch: 5
Loss: 8.284820795059204
RMSE train: 2.937913	val: 4.185468	test: 4.307140
MAE train: 2.429625	val: 3.928183	test: 3.998679

Epoch: 6
Loss: 7.529090166091919
RMSE train: 2.847878	val: 3.923566	test: 4.142056
MAE train: 2.388039	val: 3.647896	test: 3.846405

Epoch: 7
Loss: 7.081191539764404
RMSE train: 2.734490	val: 3.698214	test: 3.922949
MAE train: 2.309214	val: 3.418268	test: 3.613648

Epoch: 8
Loss: 6.39107358455658
RMSE train: 2.521669	val: 3.434778	test: 3.618238
MAE train: 2.095263	val: 3.165830	test: 3.282821

Epoch: 9
Loss: 5.606252193450928
RMSE train: 2.396807	val: 3.343806	test: 3.463693
MAE train: 1.975627	val: 3.082114	test: 3.135316

Epoch: 10
Loss: 5.0723748207092285
RMSE train: 2.380941	val: 3.381308	test: 3.519275
MAE train: 1.974251	val: 3.123054	test: 3.229598

Epoch: 11
Loss: 4.55821031332016
RMSE train: 2.118334	val: 2.966538	test: 3.090363
MAE train: 1.732871	val: 2.690092	test: 2.757921

Epoch: 12
Loss: 4.21350884437561
RMSE train: 1.932057	val: 2.685022	test: 2.798695
MAE train: 1.556187	val: 2.383088	test: 2.446899

Epoch: 13
Loss: 3.709749221801758
RMSE train: 1.794214	val: 2.590357	test: 2.728635
MAE train: 1.424152	val: 2.286401	test: 2.419294

Epoch: 14
Loss: 3.200154662132263
RMSE train: 1.476394	val: 2.083915	test: 2.233415
MAE train: 1.117105	val: 1.770021	test: 1.876211

Epoch: 15
Loss: 2.77219820022583
RMSE train: 1.282671	val: 1.760654	test: 1.941701
MAE train: 0.952979	val: 1.466189	test: 1.552216

Epoch: 16
Loss: 2.417255222797394
RMSE train: 1.272489	val: 1.801878	test: 2.015994
MAE train: 0.967407	val: 1.495090	test: 1.647686

Epoch: 17
Loss: 2.0014695823192596
RMSE train: 1.158433	val: 1.622420	test: 1.838577
MAE train: 0.862682	val: 1.347191	test: 1.482409

Epoch: 18
Loss: 1.8624523878097534
RMSE train: 0.978832	val: 1.374871	test: 1.625519
MAE train: 0.712744	val: 1.121932	test: 1.284672

Epoch: 19
Loss: 1.5910695791244507
RMSE train: 0.887360	val: 1.414648	test: 1.605576
MAE train: 0.643488	val: 1.126518	test: 1.285381

Epoch: 20
Loss: 1.4073417484760284
RMSE train: 0.821219	val: 1.397034	test: 1.571181
MAE train: 0.605989	val: 1.100907	test: 1.272320

Epoch: 21
Loss: 1.337803214788437
RMSE train: 0.780957	val: 1.306844	test: 1.482229
MAE train: 0.589482	val: 1.036422	test: 1.177876

Epoch: 22
Loss: 1.1952678561210632
RMSE train: 0.766663	val: 1.368527	test: 1.399839
MAE train: 0.580749	val: 1.067070	test: 1.102973

Epoch: 23
Loss: 1.0429408550262451Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/esol/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/esol/noise=0.0/esol_scaff_3_20-05_14-43-31  ]
[ Using Seed :  3  ]
[ Using device :  cuda:0  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 12.039077997207642
RMSE train: 3.106620	val: 3.902163	test: 4.065426
MAE train: 2.520758	val: 3.440917	test: 3.529586

Epoch: 2
Loss: 10.546354293823242
RMSE train: 3.078812	val: 4.078815	test: 4.159832
MAE train: 2.509444	val: 3.720883	test: 3.735059

Epoch: 3
Loss: 9.478596210479736
RMSE train: 2.939953	val: 4.021249	test: 4.075747
MAE train: 2.387558	val: 3.705771	test: 3.698408

Epoch: 4
Loss: 8.661001205444336
RMSE train: 2.786899	val: 3.771574	test: 3.866989
MAE train: 2.272040	val: 3.458520	test: 3.494850

Epoch: 5
Loss: 7.869765758514404
RMSE train: 2.752313	val: 3.639857	test: 3.838686
MAE train: 2.275488	val: 3.323769	test: 3.496231

Epoch: 6
Loss: 7.246010065078735
RMSE train: 2.750060	val: 3.624561	test: 3.871834
MAE train: 2.316158	val: 3.323747	test: 3.545646

Epoch: 7
Loss: 6.378421425819397
RMSE train: 2.621469	val: 3.504169	test: 3.735725
MAE train: 2.194575	val: 3.215066	test: 3.404669

Epoch: 8
Loss: 5.962866306304932
RMSE train: 2.487691	val: 3.368504	test: 3.570313
MAE train: 2.067535	val: 3.091148	test: 3.236185

Epoch: 9
Loss: 5.383676052093506
RMSE train: 2.435898	val: 3.341712	test: 3.547779
MAE train: 2.040073	val: 3.065470	test: 3.242619

Epoch: 10
Loss: 4.747454762458801
RMSE train: 2.392028	val: 3.283517	test: 3.488278
MAE train: 2.020601	val: 3.010001	test: 3.196377

Epoch: 11
Loss: 4.267232060432434
RMSE train: 2.230428	val: 3.041524	test: 3.231221
MAE train: 1.871860	val: 2.759978	test: 2.919637

Epoch: 12
Loss: 3.7514269948005676
RMSE train: 2.029220	val: 2.713538	test: 2.891369
MAE train: 1.680404	val: 2.443338	test: 2.557827

Epoch: 13
Loss: 3.325455665588379
RMSE train: 1.867015	val: 2.636994	test: 2.795788
MAE train: 1.535956	val: 2.368113	test: 2.481526

Epoch: 14
Loss: 2.820846140384674
RMSE train: 1.623761	val: 2.422398	test: 2.532477
MAE train: 1.297828	val: 2.145644	test: 2.222609

Epoch: 15
Loss: 2.557573437690735
RMSE train: 1.414306	val: 2.090276	test: 2.166663
MAE train: 1.103183	val: 1.800509	test: 1.831762

Epoch: 16
Loss: 2.091614931821823
RMSE train: 1.383016	val: 2.026053	test: 2.118558
MAE train: 1.071322	val: 1.760555	test: 1.794568

Epoch: 17
Loss: 1.8031371235847473
RMSE train: 1.349030	val: 1.960595	test: 2.092239
MAE train: 1.053834	val: 1.699685	test: 1.759474

Epoch: 18
Loss: 1.6075783669948578
RMSE train: 1.165410	val: 1.822770	test: 1.909006
MAE train: 0.875008	val: 1.534158	test: 1.554313

Epoch: 19
Loss: 1.4711765050888062
RMSE train: 0.907362	val: 1.401231	test: 1.467475
MAE train: 0.677229	val: 1.136694	test: 1.135716

Epoch: 20
Loss: 1.1739725470542908
RMSE train: 0.909754	val: 1.345110	test: 1.423060
MAE train: 0.700296	val: 1.077327	test: 1.106086

Epoch: 21
Loss: 1.1140332370996475
RMSE train: 0.823539	val: 1.437386	test: 1.420967
MAE train: 0.611503	val: 1.164885	test: 1.094286

Epoch: 22
Loss: 1.0409429967403412
RMSE train: 0.872545	val: 1.399168	test: 1.418786
MAE train: 0.659049	val: 1.131417	test: 1.102406

Epoch: 23
Loss: 0.9893690347671509Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/esol/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/esol/noise=0.0/esol_scaff_2_20-05_14-43-31  ]
[ Using Seed :  2  ]
[ Using device :  cuda:0  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 12.424087524414062
RMSE train: 3.204362	val: 4.088537	test: 4.255516
MAE train: 2.606447	val: 3.596740	test: 3.741804

Epoch: 2
Loss: 11.280456781387329
RMSE train: 3.144485	val: 4.199000	test: 4.332024
MAE train: 2.568091	val: 3.800794	test: 3.925385

Epoch: 3
Loss: 10.123215913772583
RMSE train: 3.137177	val: 4.251091	test: 4.427588
MAE train: 2.579459	val: 3.916095	test: 4.095345

Epoch: 4
Loss: 9.314859867095947
RMSE train: 3.177810	val: 4.323566	test: 4.562468
MAE train: 2.669168	val: 4.036689	test: 4.284311

Epoch: 5
Loss: 8.410003066062927
RMSE train: 2.964294	val: 4.100810	test: 4.376705
MAE train: 2.450837	val: 3.820830	test: 4.078133

Epoch: 6
Loss: 7.725629210472107
RMSE train: 2.793313	val: 3.831761	test: 4.140476
MAE train: 2.298648	val: 3.532566	test: 3.798230

Epoch: 7
Loss: 6.89765989780426
RMSE train: 2.556057	val: 3.486833	test: 3.789889
MAE train: 2.086200	val: 3.165402	test: 3.447935

Epoch: 8
Loss: 6.124315857887268
RMSE train: 2.420513	val: 3.296594	test: 3.565343
MAE train: 1.978312	val: 2.972181	test: 3.228170

Epoch: 9
Loss: 5.4931358098983765
RMSE train: 2.363107	val: 3.270411	test: 3.524411
MAE train: 1.953238	val: 2.970227	test: 3.203518

Epoch: 10
Loss: 5.1608768701553345
RMSE train: 2.298380	val: 3.234489	test: 3.455534
MAE train: 1.907148	val: 2.949108	test: 3.140451

Epoch: 11
Loss: 4.361662268638611
RMSE train: 2.246092	val: 3.099033	test: 3.301291
MAE train: 1.882446	val: 2.810966	test: 2.981449

Epoch: 12
Loss: 4.047953486442566
RMSE train: 2.154464	val: 2.907633	test: 3.106581
MAE train: 1.812146	val: 2.587289	test: 2.775777

Epoch: 13
Loss: 3.5692219138145447
RMSE train: 2.068291	val: 2.861258	test: 3.069770
MAE train: 1.720342	val: 2.531933	test: 2.757683

Epoch: 14
Loss: 3.069305181503296
RMSE train: 1.917069	val: 2.663021	test: 2.847531
MAE train: 1.602084	val: 2.354712	test: 2.516657

Epoch: 15
Loss: 2.753799319267273
RMSE train: 1.749490	val: 2.514535	test: 2.624129
MAE train: 1.427900	val: 2.204285	test: 2.294396

Epoch: 16
Loss: 2.296935558319092
RMSE train: 1.616936	val: 2.359636	test: 2.451919
MAE train: 1.293514	val: 2.011990	test: 2.098756

Epoch: 17
Loss: 1.9484588205814362
RMSE train: 1.449602	val: 2.048224	test: 2.172276
MAE train: 1.164901	val: 1.720992	test: 1.818376

Epoch: 18
Loss: 1.7983074486255646
RMSE train: 1.286640	val: 1.852046	test: 1.981375
MAE train: 1.009731	val: 1.511066	test: 1.611310

Epoch: 19
Loss: 1.453184813261032
RMSE train: 1.134296	val: 1.702155	test: 1.812892
MAE train: 0.883756	val: 1.386343	test: 1.477598

Epoch: 20
Loss: 1.3101554811000824
RMSE train: 1.002516	val: 1.575948	test: 1.669548
MAE train: 0.770524	val: 1.258104	test: 1.362328

Epoch: 21
Loss: 1.1742964088916779
RMSE train: 0.925900	val: 1.525773	test: 1.578572
MAE train: 0.700375	val: 1.204205	test: 1.282658

Epoch: 22
Loss: 1.1349909007549286
RMSE train: 0.795896	val: 1.304704	test: 1.366442
MAE train: 0.600876	val: 1.029900	test: 1.080701

Epoch: 23
Loss: 0.9704363495111465Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/esol/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/esol/noise=0.05/esol_scaff_1_20-05_14-43-31  ]
[ Using Seed :  1  ]
[ Using device :  cuda:1  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 11.45988917350769
RMSE train: 3.144986	val: 3.977871	test: 4.058563
MAE train: 2.585588	val: 3.570533	test: 3.542550

Epoch: 2
Loss: 10.225627899169922
RMSE train: 3.027878	val: 4.030153	test: 4.130453
MAE train: 2.478543	val: 3.641794	test: 3.675028

Epoch: 3
Loss: 9.475133419036865
RMSE train: 2.911292	val: 4.008176	test: 4.119038
MAE train: 2.375647	val: 3.633653	test: 3.724934

Epoch: 4
Loss: 8.555011510848999
RMSE train: 2.760955	val: 3.819347	test: 3.924171
MAE train: 2.240270	val: 3.488280	test: 3.571705

Epoch: 5
Loss: 7.6400816440582275
RMSE train: 2.621562	val: 3.558654	test: 3.687589
MAE train: 2.115074	val: 3.248222	test: 3.332772

Epoch: 6
Loss: 7.0475322008132935
RMSE train: 2.532796	val: 3.449084	test: 3.652992
MAE train: 2.042467	val: 3.132259	test: 3.305953

Epoch: 7
Loss: 6.517350912094116
RMSE train: 2.450261	val: 3.297640	test: 3.546165
MAE train: 1.975410	val: 2.975881	test: 3.194835

Epoch: 8
Loss: 6.026956081390381
RMSE train: 2.332051	val: 3.124281	test: 3.340821
MAE train: 1.875056	val: 2.808431	test: 2.981975

Epoch: 9
Loss: 5.370225667953491
RMSE train: 2.190037	val: 2.953858	test: 3.124193
MAE train: 1.749738	val: 2.640546	test: 2.769514

Epoch: 10
Loss: 4.734908878803253
RMSE train: 2.049039	val: 2.770215	test: 2.906972
MAE train: 1.629252	val: 2.467180	test: 2.541672

Epoch: 11
Loss: 4.388891816139221
RMSE train: 1.928894	val: 2.604760	test: 2.762637
MAE train: 1.527842	val: 2.297253	test: 2.413287

Epoch: 12
Loss: 3.7897619009017944
RMSE train: 1.877328	val: 2.528913	test: 2.728470
MAE train: 1.498795	val: 2.195244	test: 2.360571

Epoch: 13
Loss: 3.4610632061958313
RMSE train: 1.758274	val: 2.298753	test: 2.503578
MAE train: 1.394567	val: 1.938707	test: 2.115357

Epoch: 14
Loss: 3.0842976570129395
RMSE train: 1.666594	val: 2.184646	test: 2.363214
MAE train: 1.320387	val: 1.840562	test: 1.974803

Epoch: 15
Loss: 2.6646631956100464
RMSE train: 1.577515	val: 2.139162	test: 2.278131
MAE train: 1.246586	val: 1.799850	test: 1.894777

Epoch: 16
Loss: 2.3173803091049194
RMSE train: 1.394395	val: 2.007705	test: 2.119241
MAE train: 1.076017	val: 1.662027	test: 1.718461

Epoch: 17
Loss: 2.104230910539627
RMSE train: 1.191961	val: 1.763248	test: 1.857700
MAE train: 0.907044	val: 1.428396	test: 1.469537

Epoch: 18
Loss: 1.7886241674423218
RMSE train: 1.059537	val: 1.646482	test: 1.739210
MAE train: 0.806317	val: 1.328944	test: 1.366681

Epoch: 19
Loss: 1.7596543431282043
RMSE train: 1.004401	val: 1.629596	test: 1.685652
MAE train: 0.769473	val: 1.305726	test: 1.327824

Epoch: 20
Loss: 1.5356789231300354
RMSE train: 0.938488	val: 1.478978	test: 1.501651
MAE train: 0.714553	val: 1.202638	test: 1.184172

Epoch: 21
Loss: 1.4006984531879425
RMSE train: 0.877655	val: 1.451407	test: 1.491224
MAE train: 0.666048	val: 1.158291	test: 1.172181

Epoch: 22
Loss: 1.1572007685899734
RMSE train: 0.829210	val: 1.382567	test: 1.438875Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/esol/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/esol/noise=0.1/esol_scaff_2_20-05_14-43-31  ]
[ Using Seed :  2  ]
[ Using device :  cuda:2  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 11.568729162216187
RMSE train: 3.251390	val: 3.929537	test: 4.070982
MAE train: 2.650633	val: 3.493668	test: 3.536521

Epoch: 2
Loss: 10.563724994659424
RMSE train: 3.171205	val: 3.804438	test: 3.942252
MAE train: 2.587310	val: 3.350991	test: 3.436458

Epoch: 3
Loss: 9.373839616775513
RMSE train: 3.079427	val: 3.790624	test: 3.889715
MAE train: 2.508110	val: 3.364644	test: 3.427345

Epoch: 4
Loss: 8.639428853988647
RMSE train: 2.973864	val: 3.852102	test: 3.896705
MAE train: 2.417704	val: 3.488311	test: 3.477156

Epoch: 5
Loss: 7.814743280410767
RMSE train: 2.860742	val: 3.935842	test: 3.908342
MAE train: 2.330620	val: 3.643124	test: 3.539877

Epoch: 6
Loss: 7.359634518623352
RMSE train: 2.683255	val: 3.809286	test: 3.763995
MAE train: 2.176621	val: 3.522638	test: 3.405034

Epoch: 7
Loss: 6.572842717170715
RMSE train: 2.516716	val: 3.561275	test: 3.525779
MAE train: 2.032132	val: 3.276124	test: 3.152503

Epoch: 8
Loss: 5.862290143966675
RMSE train: 2.369892	val: 3.303334	test: 3.325926
MAE train: 1.901088	val: 3.015724	test: 2.942174

Epoch: 9
Loss: 5.44388222694397
RMSE train: 2.319522	val: 3.209447	test: 3.272250
MAE train: 1.864824	val: 2.926323	test: 2.884937

Epoch: 10
Loss: 4.939488887786865
RMSE train: 2.286657	val: 3.185123	test: 3.253379
MAE train: 1.837750	val: 2.903431	test: 2.868031

Epoch: 11
Loss: 4.399807453155518
RMSE train: 2.151742	val: 3.124873	test: 3.146296
MAE train: 1.717946	val: 2.835031	test: 2.735792

Epoch: 12
Loss: 3.9219855666160583
RMSE train: 1.945388	val: 2.836305	test: 2.867025
MAE train: 1.542857	val: 2.541684	test: 2.472299

Epoch: 13
Loss: 3.462949216365814
RMSE train: 1.722469	val: 2.402774	test: 2.443019
MAE train: 1.363026	val: 2.115055	test: 2.086030

Epoch: 14
Loss: 3.047869384288788
RMSE train: 1.615431	val: 2.354130	test: 2.328085
MAE train: 1.269758	val: 2.049562	test: 1.975956

Epoch: 15
Loss: 2.7533052563667297
RMSE train: 1.519087	val: 2.383901	test: 2.293951
MAE train: 1.191250	val: 2.062496	test: 1.941427

Epoch: 16
Loss: 2.3324891328811646
RMSE train: 1.402745	val: 2.202129	test: 2.095132
MAE train: 1.107826	val: 1.864095	test: 1.758083

Epoch: 17
Loss: 2.3046595752239227
RMSE train: 1.258198	val: 1.869870	test: 1.768572
MAE train: 0.985457	val: 1.525758	test: 1.423464

Epoch: 18
Loss: 1.927782267332077
RMSE train: 1.158318	val: 1.681422	test: 1.572048
MAE train: 0.894001	val: 1.300554	test: 1.212334

Epoch: 19
Loss: 1.8659596145153046
RMSE train: 1.079326	val: 1.552376	test: 1.493853
MAE train: 0.836878	val: 1.181410	test: 1.153712

Epoch: 20
Loss: 1.6162478029727936
RMSE train: 1.108633	val: 1.491537	test: 1.543496
MAE train: 0.857762	val: 1.152211	test: 1.200224

Epoch: 21
Loss: 1.5071594417095184
RMSE train: 1.048209	val: 1.398809	test: 1.446179
MAE train: 0.806760	val: 1.091995	test: 1.127946

Epoch: 22
Loss: 1.3786775469779968
RMSE train: 0.979779	val: 1.396039	test: 1.393836Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:3
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/esol/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/esol/noise=0.2/esol_scaff_2_20-05_14-43-31  ]
[ Using Seed :  2  ]
[ Using device :  cuda:3  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 11.661525249481201
RMSE train: 3.244411	val: 3.901739	test: 4.034977
MAE train: 2.648008	val: 3.451238	test: 3.481063

Epoch: 2
Loss: 10.860531568527222
RMSE train: 3.196685	val: 3.678598	test: 3.819202
MAE train: 2.610920	val: 3.184886	test: 3.256244

Epoch: 3
Loss: 9.614802837371826
RMSE train: 3.138708	val: 3.564765	test: 3.673343
MAE train: 2.568824	val: 3.078804	test: 3.121365

Epoch: 4
Loss: 8.950035572052002
RMSE train: 3.025104	val: 3.513023	test: 3.578799
MAE train: 2.476411	val: 3.062187	test: 3.050302

Epoch: 5
Loss: 8.048346400260925
RMSE train: 2.881779	val: 3.474665	test: 3.505047
MAE train: 2.346589	val: 3.073889	test: 3.032586

Epoch: 6
Loss: 7.5870198011398315
RMSE train: 2.743056	val: 3.413844	test: 3.417842
MAE train: 2.217682	val: 3.057349	test: 2.981773

Epoch: 7
Loss: 6.907421827316284
RMSE train: 2.596225	val: 3.235078	test: 3.253007
MAE train: 2.090680	val: 2.891130	test: 2.828942

Epoch: 8
Loss: 6.247036099433899
RMSE train: 2.404371	val: 3.002058	test: 3.055720
MAE train: 1.912780	val: 2.656073	test: 2.621236

Epoch: 9
Loss: 5.65224814414978
RMSE train: 2.263533	val: 2.879825	test: 2.947719
MAE train: 1.788013	val: 2.535958	test: 2.503809

Epoch: 10
Loss: 5.257486343383789
RMSE train: 2.184834	val: 2.853118	test: 2.915744
MAE train: 1.723411	val: 2.516150	test: 2.470578

Epoch: 11
Loss: 4.671181559562683
RMSE train: 2.099072	val: 2.877975	test: 2.902105
MAE train: 1.650145	val: 2.543816	test: 2.442089

Epoch: 12
Loss: 4.154760420322418
RMSE train: 1.956667	val: 2.700057	test: 2.744010
MAE train: 1.525025	val: 2.346175	test: 2.280308

Epoch: 13
Loss: 3.774117350578308
RMSE train: 1.834269	val: 2.464127	test: 2.528427
MAE train: 1.429529	val: 2.092054	test: 2.076014

Epoch: 14
Loss: 3.3829290866851807
RMSE train: 1.742399	val: 2.156175	test: 2.207908
MAE train: 1.363628	val: 1.782171	test: 1.761245

Epoch: 15
Loss: 3.0764031410217285
RMSE train: 1.587597	val: 1.820886	test: 1.879876
MAE train: 1.235813	val: 1.479932	test: 1.465487

Epoch: 16
Loss: 2.653143286705017
RMSE train: 1.466782	val: 1.701531	test: 1.764267
MAE train: 1.149663	val: 1.401703	test: 1.401298

Epoch: 17
Loss: 2.5838943123817444
RMSE train: 1.382524	val: 1.645762	test: 1.719095
MAE train: 1.081214	val: 1.363327	test: 1.389835

Epoch: 18
Loss: 2.174343913793564
RMSE train: 1.288061	val: 1.596555	test: 1.636542
MAE train: 1.003699	val: 1.311410	test: 1.306555

Epoch: 19
Loss: 2.1873967051506042
RMSE train: 1.189789	val: 1.569541	test: 1.601300
MAE train: 0.929144	val: 1.239327	test: 1.270444

Epoch: 20
Loss: 2.000414490699768
RMSE train: 1.140717	val: 1.592916	test: 1.688901
MAE train: 0.903149	val: 1.313753	test: 1.334012

Epoch: 21
Loss: 1.8107353448867798
RMSE train: 1.087970	val: 1.601157	test: 1.694003
MAE train: 0.863447	val: 1.318953	test: 1.332645

Epoch: 22
Loss: 1.6942370235919952
RMSE train: 1.020050	val: 1.547492	test: 1.625426Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/esol/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/esol/noise=0.1/esol_scaff_3_20-05_14-43-31  ]
[ Using Seed :  3  ]
[ Using device :  cuda:2  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 12.65580129623413
RMSE train: 3.580918	val: 4.376924	test: 4.466252
MAE train: 3.006719	val: 3.965784	test: 3.989776

Epoch: 2
Loss: 11.52125597000122
RMSE train: 3.380282	val: 4.281780	test: 4.350141
MAE train: 2.816511	val: 3.909962	test: 3.918104

Epoch: 3
Loss: 10.456088542938232
RMSE train: 3.198406	val: 4.198754	test: 4.265279
MAE train: 2.633581	val: 3.839349	test: 3.870531

Epoch: 4
Loss: 9.596334218978882
RMSE train: 3.129767	val: 4.249628	test: 4.313278
MAE train: 2.586691	val: 3.913242	test: 3.951402

Epoch: 5
Loss: 8.781104326248169
RMSE train: 3.064186	val: 4.257757	test: 4.325680
MAE train: 2.540288	val: 3.936073	test: 3.991443

Epoch: 6
Loss: 8.062888503074646
RMSE train: 2.910849	val: 4.056376	test: 4.150010
MAE train: 2.399776	val: 3.734267	test: 3.824785

Epoch: 7
Loss: 7.705757260322571
RMSE train: 2.740419	val: 3.820351	test: 3.947812
MAE train: 2.240110	val: 3.512930	test: 3.617226

Epoch: 8
Loss: 6.863232493400574
RMSE train: 2.563377	val: 3.584218	test: 3.733571
MAE train: 2.071533	val: 3.293327	test: 3.395766

Epoch: 9
Loss: 6.0015681982040405
RMSE train: 2.381624	val: 3.329782	test: 3.503017
MAE train: 1.897354	val: 3.032971	test: 3.150966

Epoch: 10
Loss: 5.605423331260681
RMSE train: 2.233244	val: 3.123289	test: 3.316509
MAE train: 1.760781	val: 2.830007	test: 2.945956

Epoch: 11
Loss: 4.96039891242981
RMSE train: 2.142523	val: 3.040542	test: 3.263260
MAE train: 1.701321	val: 2.729202	test: 2.911709

Epoch: 12
Loss: 4.3977362513542175
RMSE train: 2.104206	val: 2.981938	test: 3.201360
MAE train: 1.690222	val: 2.684069	test: 2.864544

Epoch: 13
Loss: 3.994325041770935
RMSE train: 2.002506	val: 2.791307	test: 2.977005
MAE train: 1.594563	val: 2.509808	test: 2.630129

Epoch: 14
Loss: 3.5531248450279236
RMSE train: 1.780245	val: 2.450976	test: 2.609714
MAE train: 1.379503	val: 2.168849	test: 2.253564

Epoch: 15
Loss: 3.174307346343994
RMSE train: 1.631711	val: 2.209122	test: 2.379109
MAE train: 1.246319	val: 1.929369	test: 2.009003

Epoch: 16
Loss: 2.8433967232704163
RMSE train: 1.530781	val: 2.045783	test: 2.250012
MAE train: 1.167175	val: 1.762110	test: 1.857395

Epoch: 17
Loss: 2.5639922618865967
RMSE train: 1.397393	val: 1.899921	test: 2.040356
MAE train: 1.056206	val: 1.595947	test: 1.654685

Epoch: 18
Loss: 2.225018560886383
RMSE train: 1.234256	val: 1.822529	test: 1.836228
MAE train: 0.933735	val: 1.466137	test: 1.470783

Epoch: 19
Loss: 2.1126749515533447
RMSE train: 1.106273	val: 1.652970	test: 1.624785
MAE train: 0.846183	val: 1.314813	test: 1.290052

Epoch: 20
Loss: 1.8027613461017609
RMSE train: 1.083774	val: 1.462566	test: 1.447266
MAE train: 0.840425	val: 1.157360	test: 1.134022

Epoch: 21
Loss: 1.7748156189918518
RMSE train: 1.032759	val: 1.578985	test: 1.448095
MAE train: 0.802973	val: 1.211119	test: 1.135200

Epoch: 22
Loss: 1.6177937388420105
RMSE train: 0.992277	val: 1.626803	test: 1.448299Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/esol/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/esol/noise=0.1/esol_scaff_1_20-05_14-43-31  ]
[ Using Seed :  1  ]
[ Using device :  cuda:2  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 11.429764986038208
RMSE train: 3.187729	val: 3.897490	test: 3.984965
MAE train: 2.639180	val: 3.473743	test: 3.447267

Epoch: 2
Loss: 10.355093717575073
RMSE train: 3.089456	val: 3.851440	test: 3.923351
MAE train: 2.550414	val: 3.458656	test: 3.419429

Epoch: 3
Loss: 9.606674432754517
RMSE train: 3.052424	val: 3.862440	test: 3.912306
MAE train: 2.528034	val: 3.502125	test: 3.475179

Epoch: 4
Loss: 8.787365913391113
RMSE train: 2.971412	val: 3.823577	test: 3.846813
MAE train: 2.466684	val: 3.487318	test: 3.456972

Epoch: 5
Loss: 7.838678240776062
RMSE train: 2.794440	val: 3.624026	test: 3.638919
MAE train: 2.307600	val: 3.302088	test: 3.255036

Epoch: 6
Loss: 7.220598101615906
RMSE train: 2.634186	val: 3.419582	test: 3.476969
MAE train: 2.166402	val: 3.096586	test: 3.099817

Epoch: 7
Loss: 6.655927419662476
RMSE train: 2.541914	val: 3.330676	test: 3.433791
MAE train: 2.095798	val: 3.012531	test: 3.060728

Epoch: 8
Loss: 6.124197244644165
RMSE train: 2.473791	val: 3.359828	test: 3.470602
MAE train: 2.035967	val: 3.052750	test: 3.103599

Epoch: 9
Loss: 5.546916365623474
RMSE train: 2.359746	val: 3.351632	test: 3.468461
MAE train: 1.929835	val: 3.047266	test: 3.114432

Epoch: 10
Loss: 4.799447476863861
RMSE train: 2.201687	val: 3.210614	test: 3.326870
MAE train: 1.786218	val: 2.896503	test: 2.976773

Epoch: 11
Loss: 4.460312604904175
RMSE train: 2.008454	val: 2.915407	test: 3.028381
MAE train: 1.600476	val: 2.594420	test: 2.682153

Epoch: 12
Loss: 3.8861979246139526
RMSE train: 1.864188	val: 2.656900	test: 2.776355
MAE train: 1.469128	val: 2.336330	test: 2.427993

Epoch: 13
Loss: 3.579117774963379
RMSE train: 1.729328	val: 2.463130	test: 2.605576
MAE train: 1.353502	val: 2.125229	test: 2.229970

Epoch: 14
Loss: 3.1936779022216797
RMSE train: 1.599576	val: 2.339624	test: 2.505673
MAE train: 1.245254	val: 1.997918	test: 2.103727

Epoch: 15
Loss: 2.897771954536438
RMSE train: 1.445178	val: 2.116059	test: 2.275306
MAE train: 1.115988	val: 1.762953	test: 1.855699

Epoch: 16
Loss: 2.463441014289856
RMSE train: 1.321885	val: 2.026309	test: 2.141720
MAE train: 1.004086	val: 1.654767	test: 1.713961

Epoch: 17
Loss: 2.216208189725876
RMSE train: 1.213059	val: 1.846961	test: 1.971274
MAE train: 0.928738	val: 1.504284	test: 1.569753

Epoch: 18
Loss: 1.9761632978916168
RMSE train: 1.127667	val: 1.774354	test: 1.887780
MAE train: 0.873561	val: 1.452687	test: 1.495226

Epoch: 19
Loss: 1.96951562166214
RMSE train: 1.029635	val: 1.781565	test: 1.803973
MAE train: 0.798685	val: 1.386100	test: 1.425325

Epoch: 20
Loss: 1.7253515422344208
RMSE train: 0.975354	val: 1.787388	test: 1.715760
MAE train: 0.765199	val: 1.374026	test: 1.377706

Epoch: 21
Loss: 1.5349750518798828
RMSE train: 0.935149	val: 1.839028	test: 1.746493
MAE train: 0.723385	val: 1.418032	test: 1.391120

Epoch: 22
Loss: 1.3174150586128235
RMSE train: 0.895939	val: 1.760779	test: 1.678015Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:3
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/esol/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/esol/noise=0.2/esol_scaff_3_20-05_14-43-31  ]
[ Using Seed :  3  ]
[ Using device :  cuda:3  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 12.69971227645874
RMSE train: 3.557472	val: 4.394271	test: 4.468809
MAE train: 2.975651	val: 3.975899	test: 3.971891

Epoch: 2
Loss: 11.671275615692139
RMSE train: 3.389674	val: 4.260358	test: 4.316977
MAE train: 2.815112	val: 3.854856	test: 3.843572

Epoch: 3
Loss: 10.63172721862793
RMSE train: 3.250166	val: 4.029880	test: 4.111898
MAE train: 2.680286	val: 3.613181	test: 3.649364

Epoch: 4
Loss: 9.815145492553711
RMSE train: 3.203214	val: 3.907153	test: 3.989411
MAE train: 2.643379	val: 3.491633	test: 3.535951

Epoch: 5
Loss: 9.168285846710205
RMSE train: 3.132756	val: 3.870860	test: 3.930993
MAE train: 2.583301	val: 3.455682	test: 3.495137

Epoch: 6
Loss: 8.504111289978027
RMSE train: 3.027509	val: 3.869291	test: 3.933464
MAE train: 2.494474	val: 3.465364	test: 3.530251

Epoch: 7
Loss: 8.085612416267395
RMSE train: 2.875842	val: 3.852472	test: 3.928470
MAE train: 2.358714	val: 3.462712	test: 3.547412

Epoch: 8
Loss: 7.366422057151794
RMSE train: 2.678602	val: 3.762848	test: 3.854315
MAE train: 2.171872	val: 3.376602	test: 3.464673

Epoch: 9
Loss: 6.399532079696655
RMSE train: 2.454374	val: 3.475759	test: 3.552082
MAE train: 1.954355	val: 3.092212	test: 3.130312

Epoch: 10
Loss: 5.857683777809143
RMSE train: 2.256355	val: 3.217843	test: 3.278822
MAE train: 1.771361	val: 2.832514	test: 2.837634

Epoch: 11
Loss: 5.351807951927185
RMSE train: 2.131665	val: 3.169318	test: 3.234111
MAE train: 1.667760	val: 2.771722	test: 2.781598

Epoch: 12
Loss: 4.827390193939209
RMSE train: 2.051501	val: 3.278246	test: 3.415967
MAE train: 1.604826	val: 2.891292	test: 2.998825

Epoch: 13
Loss: 4.4500192403793335
RMSE train: 1.991534	val: 3.250380	test: 3.427290
MAE train: 1.557680	val: 2.893591	test: 3.037024

Epoch: 14
Loss: 3.907299757003784
RMSE train: 1.872210	val: 2.937927	test: 3.105272
MAE train: 1.454095	val: 2.556206	test: 2.654040

Epoch: 15
Loss: 3.611799478530884
RMSE train: 1.715956	val: 2.615817	test: 2.751631
MAE train: 1.325472	val: 2.236325	test: 2.291529

Epoch: 16
Loss: 3.1307332515716553
RMSE train: 1.570554	val: 2.330968	test: 2.445785
MAE train: 1.212198	val: 1.992961	test: 1.998917

Epoch: 17
Loss: 3.03082811832428
RMSE train: 1.457758	val: 2.181094	test: 2.228940
MAE train: 1.124462	val: 1.825255	test: 1.795381

Epoch: 18
Loss: 2.6001391410827637
RMSE train: 1.358376	val: 2.300462	test: 2.221867
MAE train: 1.049264	val: 1.862041	test: 1.782609

Epoch: 19
Loss: 2.342897355556488
RMSE train: 1.255823	val: 2.356183	test: 2.200131
MAE train: 0.968545	val: 1.873629	test: 1.726844

Epoch: 20
Loss: 2.159044235944748
RMSE train: 1.186968	val: 2.327249	test: 2.190510
MAE train: 0.909480	val: 1.839845	test: 1.709787

Epoch: 21
Loss: 1.9829286634922028
RMSE train: 1.139747	val: 2.302707	test: 2.165672
MAE train: 0.872919	val: 1.822537	test: 1.700199

Epoch: 22
Loss: 1.8809839189052582
RMSE train: 1.086012	val: 2.302523	test: 2.060039Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/esol/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/esol/noise=0.05/esol_scaff_3_20-05_14-43-31  ]
[ Using Seed :  3  ]
[ Using device :  cuda:1  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 12.58519721031189
RMSE train: 3.487477	val: 4.338822	test: 4.443552
MAE train: 2.897012	val: 3.909725	test: 3.961134

Epoch: 2
Loss: 11.462485074996948
RMSE train: 3.301724	val: 4.279922	test: 4.359058
MAE train: 2.717312	val: 3.895033	test: 3.923488

Epoch: 3
Loss: 10.319397926330566
RMSE train: 3.198453	val: 4.348774	test: 4.391872
MAE train: 2.616481	val: 4.014183	test: 4.001238

Epoch: 4
Loss: 9.470982074737549
RMSE train: 3.137115	val: 4.424729	test: 4.447512
MAE train: 2.572777	val: 4.122521	test: 4.095431

Epoch: 5
Loss: 8.585041046142578
RMSE train: 3.069067	val: 4.377361	test: 4.438010
MAE train: 2.531311	val: 4.089281	test: 4.131447

Epoch: 6
Loss: 7.946770787239075
RMSE train: 2.860212	val: 3.981218	test: 4.109122
MAE train: 2.355458	val: 3.663051	test: 3.793417

Epoch: 7
Loss: 7.468616366386414
RMSE train: 2.650612	val: 3.605379	test: 3.805948
MAE train: 2.167105	val: 3.276606	test: 3.469010

Epoch: 8
Loss: 6.679231405258179
RMSE train: 2.492561	val: 3.427017	test: 3.678315
MAE train: 2.025459	val: 3.107163	test: 3.336685

Epoch: 9
Loss: 5.920370578765869
RMSE train: 2.363593	val: 3.377631	test: 3.625454
MAE train: 1.894499	val: 3.071138	test: 3.271179

Epoch: 10
Loss: 5.445567727088928
RMSE train: 2.222439	val: 3.213729	test: 3.475082
MAE train: 1.772000	val: 2.910355	test: 3.109098

Epoch: 11
Loss: 4.815304756164551
RMSE train: 2.145679	val: 3.059965	test: 3.320088
MAE train: 1.718460	val: 2.743875	test: 2.974866

Epoch: 12
Loss: 4.343905329704285
RMSE train: 2.090342	val: 2.923349	test: 3.125503
MAE train: 1.678219	val: 2.631185	test: 2.795303

Epoch: 13
Loss: 3.838291883468628
RMSE train: 1.931670	val: 2.667586	test: 2.815586
MAE train: 1.528415	val: 2.384674	test: 2.464213

Epoch: 14
Loss: 3.409206807613373
RMSE train: 1.729729	val: 2.390711	test: 2.504987
MAE train: 1.344902	val: 2.096296	test: 2.135427

Epoch: 15
Loss: 3.086851119995117
RMSE train: 1.603512	val: 2.230744	test: 2.354574
MAE train: 1.225748	val: 1.942448	test: 1.994570

Epoch: 16
Loss: 2.658830374479294
RMSE train: 1.538343	val: 2.061690	test: 2.208102
MAE train: 1.173061	val: 1.786671	test: 1.847426

Epoch: 17
Loss: 2.5058191120624542
RMSE train: 1.351940	val: 1.815298	test: 1.887442
MAE train: 1.019099	val: 1.542611	test: 1.555719

Epoch: 18
Loss: 2.065897047519684
RMSE train: 1.165112	val: 1.603447	test: 1.618572
MAE train: 0.870485	val: 1.309970	test: 1.283270

Epoch: 19
Loss: 1.9583410322666168
RMSE train: 1.068945	val: 1.482988	test: 1.527489
MAE train: 0.811678	val: 1.206324	test: 1.227776

Epoch: 20
Loss: 1.6668386161327362
RMSE train: 1.111112	val: 1.441329	test: 1.540971
MAE train: 0.867329	val: 1.147284	test: 1.231105

Epoch: 21
Loss: 1.611410677433014
RMSE train: 1.027457	val: 1.445864	test: 1.474105
MAE train: 0.805761	val: 1.143310	test: 1.184518

Epoch: 22
Loss: 1.4072188436985016
RMSE train: 0.947773	val: 1.535434	test: 1.427917Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:3
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/esol/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/esol/noise=0.2/esol_scaff_1_20-05_14-43-31  ]
[ Using Seed :  1  ]
[ Using device :  cuda:3  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 11.466374158859253
RMSE train: 3.335773	val: 3.927241	test: 4.000775
MAE train: 2.768579	val: 3.505199	test: 3.461946

Epoch: 2
Loss: 10.548509359359741
RMSE train: 3.195197	val: 3.846792	test: 3.904556
MAE train: 2.643697	val: 3.442743	test: 3.393526

Epoch: 3
Loss: 9.894898176193237
RMSE train: 3.120897	val: 3.767214	test: 3.811561
MAE train: 2.588833	val: 3.384552	test: 3.337457

Epoch: 4
Loss: 9.041271686553955
RMSE train: 3.041508	val: 3.741503	test: 3.758917
MAE train: 2.530309	val: 3.388342	test: 3.323436

Epoch: 5
Loss: 8.12941837310791
RMSE train: 2.877921	val: 3.590813	test: 3.608672
MAE train: 2.388629	val: 3.237602	test: 3.191827

Epoch: 6
Loss: 7.453298091888428
RMSE train: 2.674472	val: 3.325123	test: 3.365499
MAE train: 2.203998	val: 2.941943	test: 2.953288

Epoch: 7
Loss: 6.8522549867630005
RMSE train: 2.547171	val: 3.122552	test: 3.173911
MAE train: 2.088190	val: 2.720231	test: 2.765702

Epoch: 8
Loss: 6.333095669746399
RMSE train: 2.414144	val: 3.003396	test: 3.060108
MAE train: 1.967948	val: 2.597928	test: 2.658624

Epoch: 9
Loss: 5.748825430870056
RMSE train: 2.303419	val: 2.939036	test: 3.008955
MAE train: 1.871072	val: 2.544390	test: 2.614047

Epoch: 10
Loss: 5.170256614685059
RMSE train: 2.206882	val: 2.869241	test: 2.963459
MAE train: 1.792708	val: 2.497256	test: 2.578265

Epoch: 11
Loss: 4.656222820281982
RMSE train: 2.064814	val: 2.759154	test: 2.837531
MAE train: 1.656739	val: 2.377845	test: 2.435322

Epoch: 12
Loss: 4.16721785068512
RMSE train: 1.920544	val: 2.600720	test: 2.668498
MAE train: 1.529438	val: 2.220035	test: 2.255635

Epoch: 13
Loss: 3.8730721473693848
RMSE train: 1.799892	val: 2.463125	test: 2.530520
MAE train: 1.428081	val: 2.076746	test: 2.096178

Epoch: 14
Loss: 3.426357686519623
RMSE train: 1.693311	val: 2.433939	test: 2.482305
MAE train: 1.340266	val: 2.034751	test: 2.026807

Epoch: 15
Loss: 3.040679931640625
RMSE train: 1.555621	val: 2.371420	test: 2.400197
MAE train: 1.222137	val: 1.957279	test: 1.922323

Epoch: 16
Loss: 2.6336183547973633
RMSE train: 1.424446	val: 2.264064	test: 2.183125
MAE train: 1.106740	val: 1.811512	test: 1.734243

Epoch: 17
Loss: 2.463835656642914
RMSE train: 1.326717	val: 2.108582	test: 2.015740
MAE train: 1.024382	val: 1.665384	test: 1.595564

Epoch: 18
Loss: 2.092672199010849
RMSE train: 1.224839	val: 2.081794	test: 1.876419
MAE train: 0.945412	val: 1.595935	test: 1.489086

Epoch: 19
Loss: 2.1669141054153442
RMSE train: 1.117482	val: 2.280181	test: 1.806573
MAE train: 0.861988	val: 1.689622	test: 1.428411

Epoch: 20
Loss: 1.8550274670124054
RMSE train: 1.066506	val: 2.578241	test: 1.924363
MAE train: 0.830776	val: 1.907353	test: 1.510834

Epoch: 21
Loss: 1.7244835793972015
RMSE train: 1.050761	val: 2.337905	test: 1.869974
MAE train: 0.824097	val: 1.779060	test: 1.467755

Epoch: 22
Loss: 1.5694068670272827
RMSE train: 1.054280	val: 2.142483	test: 1.802776Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/esol/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/esol/noise=0.05/esol_scaff_2_20-05_14-43-31  ]
[ Using Seed :  2  ]
[ Using device :  cuda:1  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 11.585439682006836
RMSE train: 3.075356	val: 3.799589	test: 3.970409
MAE train: 2.475284	val: 3.323125	test: 3.416401

Epoch: 2
Loss: 10.530095100402832
RMSE train: 2.957341	val: 3.731929	test: 3.891258
MAE train: 2.379249	val: 3.276918	test: 3.393276

Epoch: 3
Loss: 9.294471263885498
RMSE train: 2.869740	val: 3.672030	test: 3.801290
MAE train: 2.311107	val: 3.272209	test: 3.368357

Epoch: 4
Loss: 8.527912378311157
RMSE train: 2.776551	val: 3.545243	test: 3.652156
MAE train: 2.228290	val: 3.206358	test: 3.260961

Epoch: 5
Loss: 7.67065954208374
RMSE train: 2.726496	val: 3.483781	test: 3.571077
MAE train: 2.192802	val: 3.188841	test: 3.201314

Epoch: 6
Loss: 7.195836663246155
RMSE train: 2.674274	val: 3.408189	test: 3.526959
MAE train: 2.164926	val: 3.127826	test: 3.175702

Epoch: 7
Loss: 6.53208065032959
RMSE train: 2.544921	val: 3.154857	test: 3.288702
MAE train: 2.064970	val: 2.864476	test: 2.903162

Epoch: 8
Loss: 5.720256805419922
RMSE train: 2.484959	val: 3.084233	test: 3.239552
MAE train: 2.032456	val: 2.800166	test: 2.856192

Epoch: 9
Loss: 5.3295851945877075
RMSE train: 2.495262	val: 3.111932	test: 3.289301
MAE train: 2.074657	val: 2.838166	test: 2.920958

Epoch: 10
Loss: 4.835025906562805
RMSE train: 2.443744	val: 3.102117	test: 3.280002
MAE train: 2.041050	val: 2.828054	test: 2.920245

Epoch: 11
Loss: 4.261864602565765
RMSE train: 2.269076	val: 2.914596	test: 3.050045
MAE train: 1.859214	val: 2.611904	test: 2.646543

Epoch: 12
Loss: 3.6850464940071106
RMSE train: 2.041766	val: 2.652515	test: 2.775475
MAE train: 1.643026	val: 2.358610	test: 2.387373

Epoch: 13
Loss: 3.345269799232483
RMSE train: 1.792699	val: 2.346852	test: 2.443457
MAE train: 1.425587	val: 2.057814	test: 2.082657

Epoch: 14
Loss: 2.946417272090912
RMSE train: 1.652940	val: 2.124964	test: 2.194583
MAE train: 1.300579	val: 1.828501	test: 1.839565

Epoch: 15
Loss: 2.592642068862915
RMSE train: 1.508668	val: 1.994153	test: 2.075273
MAE train: 1.179759	val: 1.682433	test: 1.729688

Epoch: 16
Loss: 2.189559757709503
RMSE train: 1.377713	val: 1.842106	test: 1.967997
MAE train: 1.074700	val: 1.537675	test: 1.611663

Epoch: 17
Loss: 2.128297358751297
RMSE train: 1.237367	val: 1.544477	test: 1.649382
MAE train: 0.963089	val: 1.258565	test: 1.313279

Epoch: 18
Loss: 1.7143138349056244
RMSE train: 1.155235	val: 1.458670	test: 1.537834
MAE train: 0.891623	val: 1.188149	test: 1.213988

Epoch: 19
Loss: 1.669610857963562
RMSE train: 1.063371	val: 1.425202	test: 1.484675
MAE train: 0.816528	val: 1.162265	test: 1.163592

Epoch: 20
Loss: 1.4209854304790497
RMSE train: 1.066269	val: 1.408974	test: 1.491372
MAE train: 0.811268	val: 1.163098	test: 1.170004

Epoch: 21
Loss: 1.3624063730239868
RMSE train: 1.000576	val: 1.306179	test: 1.374178
MAE train: 0.766384	val: 1.072510	test: 1.066894

Epoch: 22
Loss: 1.2208904325962067
RMSE train: 0.896824	val: 1.331819	test: 1.301618
RMSE train: 0.724903	val: 1.371294	test: 1.360460
MAE train: 0.547782	val: 1.070112	test: 1.072801

Epoch: 24
Loss: 0.9727452397346497
RMSE train: 0.686455	val: 1.324240	test: 1.371501
MAE train: 0.524293	val: 1.017728	test: 1.114892

Epoch: 25
Loss: 0.8456906825304031
RMSE train: 0.746259	val: 1.345030	test: 1.352667
MAE train: 0.580072	val: 1.047341	test: 1.107610

Epoch: 26
Loss: 0.8816100060939789
RMSE train: 0.682409	val: 1.203550	test: 1.245767
MAE train: 0.525899	val: 0.925113	test: 1.003299

Epoch: 27
Loss: 0.7781345844268799
RMSE train: 0.672069	val: 1.379046	test: 1.313128
MAE train: 0.511369	val: 1.065238	test: 1.077243

Epoch: 28
Loss: 0.8001436591148376
RMSE train: 0.687222	val: 1.324090	test: 1.327240
MAE train: 0.520562	val: 1.033836	test: 1.079541

Epoch: 29
Loss: 0.7682299762964249
RMSE train: 0.690498	val: 1.283314	test: 1.347105
MAE train: 0.527164	val: 1.005833	test: 1.074943

Epoch: 30
Loss: 0.7976037114858627
RMSE train: 0.792314	val: 1.455126	test: 1.475650
MAE train: 0.617633	val: 1.113310	test: 1.191918

Epoch: 31
Loss: 0.7287836223840714
RMSE train: 0.626434	val: 1.206928	test: 1.269155
MAE train: 0.479943	val: 0.938814	test: 1.008775

Epoch: 32
Loss: 0.7254195809364319
RMSE train: 0.625599	val: 1.302236	test: 1.266265
MAE train: 0.477717	val: 1.002851	test: 1.026172

Epoch: 33
Loss: 0.7707696855068207
RMSE train: 0.686103	val: 1.240226	test: 1.260501
MAE train: 0.523557	val: 0.995666	test: 0.980068

Epoch: 34
Loss: 0.7215048223733902
RMSE train: 0.649069	val: 1.086017	test: 1.205218
MAE train: 0.487492	val: 0.880758	test: 0.952702

Epoch: 35
Loss: 0.7021027952432632
RMSE train: 0.631699	val: 1.207900	test: 1.214685
MAE train: 0.482769	val: 0.964003	test: 0.967325

Epoch: 36
Loss: 0.6715991199016571
RMSE train: 0.643721	val: 1.331983	test: 1.237688
MAE train: 0.487974	val: 1.033320	test: 0.998053

Epoch: 37
Loss: 0.6544632464647293
RMSE train: 0.649852	val: 1.263432	test: 1.215809
MAE train: 0.491696	val: 1.001642	test: 0.953620

Epoch: 38
Loss: 0.6391157954931259
RMSE train: 0.613500	val: 1.203665	test: 1.197361
MAE train: 0.467652	val: 0.969843	test: 0.938571

Epoch: 39
Loss: 0.6603799015283585
RMSE train: 0.595767	val: 1.164109	test: 1.177792
MAE train: 0.454840	val: 0.941644	test: 0.930604

Epoch: 40
Loss: 0.6499571800231934
RMSE train: 0.585359	val: 1.121772	test: 1.174857
MAE train: 0.437969	val: 0.894684	test: 0.943008

Epoch: 41
Loss: 0.6599439680576324
RMSE train: 0.639431	val: 1.339995	test: 1.261596
MAE train: 0.484880	val: 1.062967	test: 1.003302

Epoch: 42
Loss: 0.6166512221097946
RMSE train: 0.676961	val: 1.347049	test: 1.308707
MAE train: 0.513352	val: 1.078261	test: 1.016688

Epoch: 43
Loss: 0.5714272856712341
RMSE train: 0.630585	val: 1.102110	test: 1.209659
MAE train: 0.467748	val: 0.920666	test: 0.944950

Epoch: 44
Loss: 0.570879265666008
RMSE train: 0.581229	val: 1.096195	test: 1.172061
MAE train: 0.427170	val: 0.899822	test: 0.921438

Epoch: 45
Loss: 0.5644483715295792
RMSE train: 0.559911	val: 1.192346	test: 1.167231
MAE train: 0.413138	val: 0.950303	test: 0.922161

Epoch: 46
Loss: 0.5861819088459015
RMSE train: 0.556204	val: 1.173739	test: 1.152300
MAE train: 0.412177	val: 0.927378	test: 0.921388

Epoch: 47
Loss: 0.5381863415241241
RMSE train: 0.566809	val: 1.195710	test: 1.209060
MAE train: 0.420152	val: 0.976465	test: 0.959400

Epoch: 48
Loss: 0.5539599061012268
RMSE train: 0.544235	val: 1.257150	test: 1.196213
MAE train: 0.411912	val: 1.023917	test: 0.941584

Epoch: 49
Loss: 0.574135422706604
RMSE train: 0.534362	val: 1.294818	test: 1.174574
MAE train: 0.404935	val: 1.058409	test: 0.928286

Epoch: 50
Loss: 0.5815490633249283
RMSE train: 0.611634	val: 1.333291	test: 1.211676
MAE train: 0.467835	val: 1.083414	test: 0.943044

Epoch: 51
Loss: 0.5192812904715538
RMSE train: 0.566340	val: 1.110404	test: 1.155125
MAE train: 0.424339	val: 0.913327	test: 0.928808

Epoch: 52
Loss: 0.5239965096116066
RMSE train: 0.545793	val: 1.292941	test: 1.184538
MAE train: 0.416030	val: 1.049576	test: 0.947261

Epoch: 53
Loss: 0.5034523531794548
RMSE train: 0.569197	val: 1.264184	test: 1.205074
MAE train: 0.427262	val: 1.038347	test: 0.963680

Epoch: 54
Loss: 0.513263426721096
RMSE train: 0.561612	val: 1.213572	test: 1.192585
MAE train: 0.422678	val: 0.998421	test: 0.951419

Epoch: 55
Loss: 0.4796251580119133
RMSE train: 0.647182	val: 1.406047	test: 1.286957
MAE train: 0.496329	val: 1.151196	test: 1.036155

Epoch: 56
Loss: 0.5024759396910667
RMSE train: 0.639367	val: 1.236805	test: 1.224493
MAE train: 0.477600	val: 1.024204	test: 0.959973

Epoch: 57
Loss: 0.5011827424168587
RMSE train: 0.594161	val: 1.217633	test: 1.189204
MAE train: 0.445789	val: 1.012014	test: 0.933334

Epoch: 58
Loss: 0.5114460587501526
RMSE train: 0.580717	val: 1.280814	test: 1.156077
MAE train: 0.444438	val: 1.054021	test: 0.920801

Epoch: 59
Loss: 0.5094651281833649
RMSE train: 0.541996	val: 1.105704	test: 1.094014
MAE train: 0.403807	val: 0.896797	test: 0.877684

Epoch: 60
Loss: 0.4994911849498749
RMSE train: 0.525221	val: 1.048561	test: 1.086060
MAE train: 0.393085	val: 0.851856	test: 0.854838

Epoch: 61
Loss: 0.4776856601238251
RMSE train: 0.576815	val: 1.245422	test: 1.137339
MAE train: 0.449786	val: 1.017534	test: 0.887135

Epoch: 62
Loss: 0.5288083851337433
RMSE train: 0.517802	val: 1.019174	test: 1.076059
MAE train: 0.385798	val: 0.846454	test: 0.844226

Epoch: 63
Loss: 0.48749910295009613
RMSE train: 0.522626	val: 1.051392	test: 1.102420
MAE train: 0.391246	val: 0.861059	test: 0.878243

Epoch: 64
Loss: 0.47355319559574127
RMSE train: 0.545763	val: 1.219004	test: 1.176078
MAE train: 0.412789	val: 1.012788	test: 0.907719

Epoch: 65
Loss: 0.5151865556836128
RMSE train: 0.551882	val: 1.222444	test: 1.171178
MAE train: 0.415396	val: 1.012076	test: 0.894586

Epoch: 66
Loss: 0.4837452471256256
RMSE train: 0.598905	val: 1.095473	test: 1.140779
MAE train: 0.437828	val: 0.907803	test: 0.887075

Epoch: 67
Loss: 0.50148805975914
RMSE train: 0.594463	val: 1.280045	test: 1.247098
MAE train: 0.447500	val: 1.035082	test: 0.965026

Epoch: 68
Loss: 0.42621057480573654
RMSE train: 0.540395	val: 1.221472	test: 1.196277
MAE train: 0.395592	val: 1.016506	test: 0.928255

Epoch: 69
Loss: 0.450024351477623
RMSE train: 0.492669	val: 1.211789	test: 1.146836
MAE train: 0.365829	val: 0.985587	test: 0.916320

Epoch: 70
Loss: 0.4568101614713669
RMSE train: 0.525287	val: 1.246132	test: 1.182291
MAE train: 0.388057	val: 1.023142	test: 0.937368

Epoch: 71
Loss: 0.4646598771214485
RMSE train: 0.598546	val: 1.350065	test: 1.259179
MAE train: 0.451984	val: 1.089456	test: 0.982123

Epoch: 72
Loss: 0.5085677728056908
RMSE train: 0.484211	val: 1.201875	test: 1.115543
MAE train: 0.357344	val: 0.986623	test: 0.877855

Epoch: 73
Loss: 0.44483156502246857
RMSE train: 0.517053	val: 1.147979	test: 1.120566
MAE train: 0.384702	val: 0.951025	test: 0.888990

Epoch: 74
Loss: 0.4766947999596596
RMSE train: 0.524978	val: 1.308494	test: 1.187089
MAE train: 0.400896	val: 1.070317	test: 0.931704

Epoch: 75
Loss: 0.446389876306057
RMSE train: 0.512692	val: 1.231841	test: 1.167030
MAE train: 0.384181	val: 1.021282	test: 0.912826

Epoch: 76
Loss: 0.433986097574234
RMSE train: 0.525851	val: 1.192136	test: 1.193846
MAE train: 0.386817	val: 0.987469	test: 0.941105

Epoch: 77
Loss: 0.4276067838072777
RMSE train: 0.549086	val: 1.349252	test: 1.210905
MAE train: 0.419660	val: 1.106494	test: 0.946995

Epoch: 78
Loss: 0.4460064321756363
RMSE train: 0.581470	val: 1.211815	test: 1.212024
MAE train: 0.430098	val: 1.002271	test: 0.929393

Epoch: 79
Loss: 0.39857571572065353
RMSE train: 0.527735	val: 1.091124	test: 1.099996
MAE train: 0.393519	val: 0.904373	test: 0.857724

Epoch: 80
Loss: 0.4107494279742241
RMSE train: 0.556302	val: 1.286669	test: 1.190502
MAE train: 0.420429	val: 1.060994	test: 0.917912

Epoch: 81
Loss: 0.4074708819389343
RMSE train: 0.590798	val: 1.277504	test: 1.226830
MAE train: 0.451646	val: 1.053261	test: 0.944786

Epoch: 82
Loss: 0.39516719430685043
RMSE train: 0.518462	val: 1.117251	test: 1.109735
MAE train: 0.388147	val: 0.937546	test: 0.855771

Epoch: 83
Loss: 0.4045076444745064
RMSE train: 0.559976	val: 1.213173	test: 1.192328
MAE train: 0.417154	val: 1.000964	test: 0.903298
RMSE train: 0.769014	val: 1.341735	test: 1.388875
MAE train: 0.573246	val: 1.066064	test: 1.097121

Epoch: 24
Loss: 0.9541856497526169
RMSE train: 0.719967	val: 1.367172	test: 1.395638
MAE train: 0.531643	val: 1.089121	test: 1.108612

Epoch: 25
Loss: 0.8827093094587326
RMSE train: 0.707018	val: 1.285907	test: 1.319627
MAE train: 0.542576	val: 1.010579	test: 1.038299

Epoch: 26
Loss: 0.7790166288614273
RMSE train: 0.693342	val: 1.310502	test: 1.324232
MAE train: 0.515196	val: 1.039175	test: 1.041066

Epoch: 27
Loss: 0.7684410810470581
RMSE train: 0.730896	val: 1.323958	test: 1.358053
MAE train: 0.542875	val: 1.044078	test: 1.072412

Epoch: 28
Loss: 0.8151697069406509
RMSE train: 0.662657	val: 1.252597	test: 1.219296
MAE train: 0.511086	val: 1.013484	test: 0.992945

Epoch: 29
Loss: 0.7550994604825974
RMSE train: 0.754016	val: 1.491149	test: 1.388848
MAE train: 0.582113	val: 1.178486	test: 1.128153

Epoch: 30
Loss: 0.7483258992433548
RMSE train: 0.742188	val: 1.358381	test: 1.358132
MAE train: 0.557639	val: 1.062004	test: 1.069270

Epoch: 31
Loss: 0.72987300157547
RMSE train: 0.658595	val: 1.255782	test: 1.228278
MAE train: 0.493322	val: 0.990666	test: 0.968367

Epoch: 32
Loss: 0.7256633192300797
RMSE train: 0.693812	val: 1.465745	test: 1.326665
MAE train: 0.534041	val: 1.170998	test: 1.093836

Epoch: 33
Loss: 0.7052810490131378
RMSE train: 0.699029	val: 1.197199	test: 1.235987
MAE train: 0.531123	val: 0.958312	test: 0.954552

Epoch: 34
Loss: 0.7536226361989975
RMSE train: 0.742341	val: 1.296800	test: 1.296679
MAE train: 0.564992	val: 1.018142	test: 0.999966

Epoch: 35
Loss: 0.6795264184474945
RMSE train: 0.748614	val: 1.569226	test: 1.427895
MAE train: 0.576888	val: 1.238131	test: 1.118970

Epoch: 36
Loss: 0.6527422219514847
RMSE train: 0.641219	val: 1.229891	test: 1.218746
MAE train: 0.479075	val: 0.990171	test: 0.926379

Epoch: 37
Loss: 0.6420409381389618
RMSE train: 0.651891	val: 1.242187	test: 1.256152
MAE train: 0.481431	val: 0.989957	test: 0.959169

Epoch: 38
Loss: 0.602633997797966
RMSE train: 0.675984	val: 1.443113	test: 1.343818
MAE train: 0.521139	val: 1.172676	test: 1.075419

Epoch: 39
Loss: 0.6062416732311249
RMSE train: 0.575974	val: 1.315750	test: 1.263306
MAE train: 0.438379	val: 1.081839	test: 0.998325

Epoch: 40
Loss: 0.5953148156404495
RMSE train: 0.607334	val: 1.300393	test: 1.261433
MAE train: 0.460799	val: 1.071257	test: 0.966616

Epoch: 41
Loss: 0.5742668807506561
RMSE train: 0.647636	val: 1.366860	test: 1.282014
MAE train: 0.495193	val: 1.122939	test: 0.995569

Epoch: 42
Loss: 0.6035013049840927
RMSE train: 0.555301	val: 1.315175	test: 1.260474
MAE train: 0.420810	val: 1.077017	test: 1.005638

Epoch: 43
Loss: 0.6691521555185318
RMSE train: 0.568700	val: 1.334868	test: 1.277830
MAE train: 0.430983	val: 1.083429	test: 1.011135

Epoch: 44
Loss: 0.5934876203536987
RMSE train: 0.679556	val: 1.386053	test: 1.321366
MAE train: 0.520682	val: 1.119502	test: 1.012753

Epoch: 45
Loss: 0.5661550909280777
RMSE train: 0.588971	val: 1.237669	test: 1.213120
MAE train: 0.438258	val: 1.021813	test: 0.910005

Epoch: 46
Loss: 0.5823735594749451
RMSE train: 0.569697	val: 1.320595	test: 1.240286
MAE train: 0.425047	val: 1.078371	test: 0.965921

Epoch: 47
Loss: 0.5527375936508179
RMSE train: 0.612781	val: 1.231231	test: 1.219283
MAE train: 0.456266	val: 1.009832	test: 0.918077

Epoch: 48
Loss: 0.5801958665251732
RMSE train: 0.593088	val: 1.222359	test: 1.218215
MAE train: 0.445234	val: 0.997624	test: 0.913142

Epoch: 49
Loss: 0.5551326870918274
RMSE train: 0.644769	val: 1.310628	test: 1.281000
MAE train: 0.484761	val: 1.057634	test: 0.972789

Epoch: 50
Loss: 0.5847581028938293
RMSE train: 0.653790	val: 1.197247	test: 1.277471
MAE train: 0.479579	val: 1.010996	test: 0.990655

Epoch: 51
Loss: 0.568841241300106
RMSE train: 0.593199	val: 1.306081	test: 1.289156
MAE train: 0.452878	val: 1.065695	test: 1.024355

Epoch: 52
Loss: 0.5018658712506294
RMSE train: 0.654417	val: 1.403865	test: 1.389035
MAE train: 0.495124	val: 1.132571	test: 1.096430

Epoch: 53
Loss: 0.5147618427872658
RMSE train: 0.605498	val: 1.177912	test: 1.264487
MAE train: 0.451192	val: 0.982802	test: 0.971405

Epoch: 54
Loss: 0.5382829830050468
RMSE train: 0.564428	val: 1.144863	test: 1.200712
MAE train: 0.430214	val: 0.952471	test: 0.946563

Epoch: 55
Loss: 0.5623039603233337
RMSE train: 0.618538	val: 1.440278	test: 1.382785
MAE train: 0.474044	val: 1.161627	test: 1.071203

Epoch: 56
Loss: 0.5096405521035194
RMSE train: 0.587626	val: 1.233749	test: 1.288193
MAE train: 0.430107	val: 1.042090	test: 0.953391

Epoch: 57
Loss: 0.5113411247730255
RMSE train: 0.591043	val: 1.353209	test: 1.351818
MAE train: 0.447177	val: 1.104317	test: 1.023550

Epoch: 58
Loss: 0.46893879771232605
RMSE train: 0.576372	val: 1.211494	test: 1.262018
MAE train: 0.437209	val: 1.012669	test: 0.973344

Epoch: 59
Loss: 0.46347422897815704
RMSE train: 0.634849	val: 1.192884	test: 1.252344
MAE train: 0.476891	val: 1.000675	test: 0.961540

Epoch: 60
Loss: 0.4516051858663559
RMSE train: 0.639515	val: 1.354189	test: 1.311484
MAE train: 0.477662	val: 1.113363	test: 1.024563

Epoch: 61
Loss: 0.4510166198015213
RMSE train: 0.657842	val: 1.357891	test: 1.336915
MAE train: 0.493501	val: 1.103788	test: 1.035011

Epoch: 62
Loss: 0.46674592792987823
RMSE train: 0.615352	val: 1.216981	test: 1.268509
MAE train: 0.457033	val: 1.026457	test: 0.984428

Epoch: 63
Loss: 0.4577588140964508
RMSE train: 0.611690	val: 1.308352	test: 1.318613
MAE train: 0.460274	val: 1.071273	test: 1.022462

Epoch: 64
Loss: 0.4528067037463188
RMSE train: 0.646591	val: 1.409597	test: 1.315448
MAE train: 0.496005	val: 1.143124	test: 1.022318

Epoch: 65
Loss: 0.4845331087708473
RMSE train: 0.590272	val: 1.211205	test: 1.208598
MAE train: 0.448676	val: 0.998656	test: 0.917752

Epoch: 66
Loss: 0.4622427970170975
RMSE train: 0.557775	val: 1.179246	test: 1.228569
MAE train: 0.424932	val: 0.987944	test: 0.937920

Epoch: 67
Loss: 0.4658632203936577
RMSE train: 0.558775	val: 1.307323	test: 1.331766
MAE train: 0.423370	val: 1.075343	test: 1.003213

Epoch: 68
Loss: 0.4088503047823906
RMSE train: 0.536649	val: 1.204898	test: 1.219647
MAE train: 0.400826	val: 1.032393	test: 0.930402

Epoch: 69
Loss: 0.41143641620874405
RMSE train: 0.510768	val: 1.243547	test: 1.206875
MAE train: 0.385739	val: 1.046343	test: 0.930790

Epoch: 70
Loss: 0.40159403532743454
RMSE train: 0.552929	val: 1.247329	test: 1.193688
MAE train: 0.420949	val: 1.045978	test: 0.920947

Epoch: 71
Loss: 0.4386081174015999
RMSE train: 0.566745	val: 1.117863	test: 1.167017
MAE train: 0.416472	val: 0.957954	test: 0.908419

Epoch: 72
Loss: 0.4172188565135002
RMSE train: 0.578259	val: 1.170851	test: 1.230272
MAE train: 0.426823	val: 0.997779	test: 0.964218

Epoch: 73
Loss: 0.4233497828245163
RMSE train: 0.573362	val: 1.287835	test: 1.292946
MAE train: 0.426689	val: 1.067779	test: 1.015476

Epoch: 74
Loss: 0.3552826792001724
RMSE train: 0.625326	val: 1.306026	test: 1.328434
MAE train: 0.474462	val: 1.072829	test: 1.034333

Epoch: 75
Loss: 0.44268517196178436
RMSE train: 0.545045	val: 1.116586	test: 1.221939
MAE train: 0.408312	val: 0.955740	test: 0.980355

Epoch: 76
Loss: 0.478042371571064
RMSE train: 0.485489	val: 1.181369	test: 1.246086
MAE train: 0.362408	val: 0.995868	test: 0.975252

Epoch: 77
Loss: 0.3799660876393318
RMSE train: 0.560973	val: 1.265373	test: 1.304411
MAE train: 0.419011	val: 1.060399	test: 0.985512

Epoch: 78
Loss: 0.3664700463414192
RMSE train: 0.557810	val: 1.217122	test: 1.252764
MAE train: 0.422963	val: 1.027466	test: 0.960767

Epoch: 79
Loss: 0.37575314939022064
RMSE train: 0.525210	val: 1.167569	test: 1.203401
MAE train: 0.402623	val: 0.985959	test: 0.932118

Epoch: 80
Loss: 0.4020107164978981
RMSE train: 0.534079	val: 1.157318	test: 1.205084
MAE train: 0.408296	val: 0.987577	test: 0.936087

Epoch: 81
Loss: 0.4220113158226013
RMSE train: 0.555485	val: 1.254085	test: 1.316882
MAE train: 0.422193	val: 1.070571	test: 1.008421

Epoch: 82
Loss: 0.42418068647384644
RMSE train: 0.532792	val: 1.302538	test: 1.341741
MAE train: 0.402626	val: 1.074224	test: 1.013673

Epoch: 83
Loss: 0.350490041077137
RMSE train: 0.545235	val: 1.251940	test: 1.307983
MAE train: 0.411182	val: 1.036352	test: 0.990293
RMSE train: 0.831813	val: 1.382676	test: 1.362862
MAE train: 0.622403	val: 1.130977	test: 1.054046

Epoch: 24
Loss: 0.8669010996818542
RMSE train: 0.782685	val: 1.280954	test: 1.248799
MAE train: 0.599968	val: 1.032409	test: 0.962553

Epoch: 25
Loss: 0.7780813723802567
RMSE train: 0.750849	val: 1.348167	test: 1.294016
MAE train: 0.558056	val: 1.095611	test: 0.986251

Epoch: 26
Loss: 0.7305149883031845
RMSE train: 0.731059	val: 1.385233	test: 1.308925
MAE train: 0.535041	val: 1.102987	test: 1.032584

Epoch: 27
Loss: 0.7441230416297913
RMSE train: 0.760985	val: 1.172720	test: 1.191803
MAE train: 0.568616	val: 0.925740	test: 0.936390

Epoch: 28
Loss: 0.757652685046196
RMSE train: 0.674411	val: 1.313195	test: 1.226028
MAE train: 0.507619	val: 1.055255	test: 0.963114

Epoch: 29
Loss: 0.7867024093866348
RMSE train: 0.635834	val: 1.333004	test: 1.203349
MAE train: 0.483196	val: 1.076667	test: 0.968468

Epoch: 30
Loss: 0.6959894299507141
RMSE train: 0.738369	val: 1.304422	test: 1.279057
MAE train: 0.547522	val: 1.082847	test: 0.987437

Epoch: 31
Loss: 0.6430624425411224
RMSE train: 0.689903	val: 1.315449	test: 1.235825
MAE train: 0.516356	val: 1.052694	test: 0.959122

Epoch: 32
Loss: 0.6551942676305771
RMSE train: 0.667810	val: 1.313365	test: 1.238282
MAE train: 0.498700	val: 1.047612	test: 0.971870

Epoch: 33
Loss: 0.6753697544336319
RMSE train: 0.695432	val: 1.283721	test: 1.275332
MAE train: 0.518549	val: 1.061664	test: 0.975166

Epoch: 34
Loss: 0.6165510714054108
RMSE train: 0.648710	val: 1.302601	test: 1.249301
MAE train: 0.490304	val: 1.047619	test: 0.993202

Epoch: 35
Loss: 0.6414478868246078
RMSE train: 0.714092	val: 1.299131	test: 1.285549
MAE train: 0.532551	val: 1.045764	test: 0.967756

Epoch: 36
Loss: 0.6464495211839676
RMSE train: 0.726969	val: 1.295157	test: 1.266175
MAE train: 0.536959	val: 1.021679	test: 0.942643

Epoch: 37
Loss: 0.567734956741333
RMSE train: 0.672643	val: 1.276305	test: 1.264654
MAE train: 0.491262	val: 1.029010	test: 0.954601

Epoch: 38
Loss: 0.5832736939191818
RMSE train: 0.643905	val: 1.308768	test: 1.236429
MAE train: 0.476220	val: 1.042058	test: 0.953848

Epoch: 39
Loss: 0.6540351957082748
RMSE train: 0.699120	val: 1.404090	test: 1.293487
MAE train: 0.517826	val: 1.111391	test: 1.013736

Epoch: 40
Loss: 0.5498523861169815
RMSE train: 0.767031	val: 1.418842	test: 1.408861
MAE train: 0.566176	val: 1.132347	test: 1.072356

Epoch: 41
Loss: 0.6171194016933441
RMSE train: 0.632770	val: 1.293220	test: 1.242719
MAE train: 0.468823	val: 1.036209	test: 0.968029

Epoch: 42
Loss: 0.5591422766447067
RMSE train: 0.680005	val: 1.226545	test: 1.268188
MAE train: 0.499663	val: 0.997849	test: 0.956368

Epoch: 43
Loss: 0.5617735385894775
RMSE train: 0.647648	val: 1.381546	test: 1.300326
MAE train: 0.476113	val: 1.107045	test: 1.004524

Epoch: 44
Loss: 0.5943802148103714
RMSE train: 0.704249	val: 1.307029	test: 1.326292
MAE train: 0.515237	val: 1.086707	test: 0.992531

Epoch: 45
Loss: 0.6349741891026497
RMSE train: 0.659212	val: 1.228410	test: 1.215240
MAE train: 0.491241	val: 0.994878	test: 0.942812

Epoch: 46
Loss: 0.5522600337862968
RMSE train: 0.714806	val: 1.282574	test: 1.276398
MAE train: 0.518015	val: 1.039942	test: 0.963081

Epoch: 47
Loss: 0.5380645096302032
RMSE train: 0.742742	val: 1.321495	test: 1.358979
MAE train: 0.546520	val: 1.103152	test: 1.017433

Epoch: 48
Loss: 0.5827211886644363
RMSE train: 0.690122	val: 1.288504	test: 1.290612
MAE train: 0.515929	val: 1.032907	test: 1.003646

Epoch: 49
Loss: 0.5049586966633797
RMSE train: 0.687613	val: 1.320144	test: 1.327680
MAE train: 0.503853	val: 1.070401	test: 1.019235

Epoch: 50
Loss: 0.5326310023665428
RMSE train: 0.641934	val: 1.254524	test: 1.256943
MAE train: 0.469428	val: 1.035646	test: 0.962559

Epoch: 51
Loss: 0.5143795758485794
RMSE train: 0.632696	val: 1.205590	test: 1.207250
MAE train: 0.461594	val: 0.994059	test: 0.920838

Epoch: 52
Loss: 0.46352197974920273
RMSE train: 0.582755	val: 1.219444	test: 1.166305
MAE train: 0.431968	val: 0.985551	test: 0.898826

Epoch: 53
Loss: 0.4675462618470192
RMSE train: 0.578982	val: 1.225411	test: 1.176827
MAE train: 0.426932	val: 0.995973	test: 0.892285

Epoch: 54
Loss: 0.4350355416536331
RMSE train: 0.596635	val: 1.187301	test: 1.192638
MAE train: 0.447616	val: 0.956846	test: 0.919787

Epoch: 55
Loss: 0.4681144058704376
RMSE train: 0.577529	val: 1.166008	test: 1.217296
MAE train: 0.437022	val: 0.962955	test: 0.947538

Epoch: 56
Loss: 0.4398755431175232
RMSE train: 0.600336	val: 1.154804	test: 1.243867
MAE train: 0.438905	val: 0.980179	test: 0.968550

Epoch: 57
Loss: 0.44527002424001694
RMSE train: 0.623618	val: 1.223568	test: 1.244617
MAE train: 0.462260	val: 0.988797	test: 0.978603

Epoch: 58
Loss: 0.42826370149850845
RMSE train: 0.719340	val: 1.305421	test: 1.342658
MAE train: 0.522291	val: 1.068229	test: 1.008981

Epoch: 59
Loss: 0.4185709282755852
RMSE train: 0.674428	val: 1.182504	test: 1.262794
MAE train: 0.499135	val: 0.986651	test: 0.947043

Epoch: 60
Loss: 0.4386722072958946
RMSE train: 0.603732	val: 1.173777	test: 1.205791
MAE train: 0.451447	val: 0.971625	test: 0.933318

Epoch: 61
Loss: 0.41686058789491653
RMSE train: 0.629778	val: 1.307444	test: 1.281742
MAE train: 0.465906	val: 1.057699	test: 0.990732

Epoch: 62
Loss: 0.4811808094382286
RMSE train: 0.620586	val: 1.234498	test: 1.234068
MAE train: 0.456884	val: 1.017796	test: 0.949631

Epoch: 63
Loss: 0.4612090364098549
RMSE train: 0.619541	val: 1.278550	test: 1.238456
MAE train: 0.447655	val: 1.033842	test: 0.960326

Epoch: 64
Loss: 0.3798513635993004
RMSE train: 0.642497	val: 1.231279	test: 1.253230
MAE train: 0.468063	val: 1.015105	test: 0.934233

Epoch: 65
Loss: 0.41907402127981186
RMSE train: 0.604670	val: 1.195905	test: 1.226608
MAE train: 0.443165	val: 0.970850	test: 0.936469

Epoch: 66
Loss: 0.5195462629199028
RMSE train: 0.563531	val: 1.250336	test: 1.228979
MAE train: 0.417524	val: 0.999487	test: 0.980511

Epoch: 67
Loss: 0.4688795357942581
RMSE train: 0.573809	val: 1.128991	test: 1.168714
MAE train: 0.421305	val: 0.950907	test: 0.906136

Epoch: 68
Loss: 0.40489496290683746
RMSE train: 0.541346	val: 1.200255	test: 1.172684
MAE train: 0.399416	val: 0.966004	test: 0.939769

Epoch: 69
Loss: 0.41737617552280426
RMSE train: 0.567124	val: 1.133607	test: 1.172670
MAE train: 0.419942	val: 0.944290	test: 0.925972

Epoch: 70
Loss: 0.3956369310617447
RMSE train: 0.564982	val: 1.153526	test: 1.180682
MAE train: 0.409721	val: 0.985712	test: 0.907137

Epoch: 71
Loss: 0.37797480821609497
RMSE train: 0.510947	val: 1.080491	test: 1.111360
MAE train: 0.381563	val: 0.891050	test: 0.883556

Epoch: 72
Loss: 0.3804793506860733
RMSE train: 0.550561	val: 1.174674	test: 1.163051
MAE train: 0.394484	val: 0.961224	test: 0.921824

Epoch: 73
Loss: 0.41196849942207336
RMSE train: 0.581617	val: 1.205808	test: 1.222425
MAE train: 0.417273	val: 1.001804	test: 0.945610

Epoch: 74
Loss: 0.3927411809563637
RMSE train: 0.529675	val: 1.154002	test: 1.164029
MAE train: 0.388284	val: 0.956634	test: 0.903717

Epoch: 75
Loss: 0.427755169570446
RMSE train: 0.517386	val: 1.142419	test: 1.120612
MAE train: 0.386255	val: 0.934707	test: 0.886914

Epoch: 76
Loss: 0.39420489966869354
RMSE train: 0.515022	val: 1.056012	test: 1.110634
MAE train: 0.387536	val: 0.883467	test: 0.867720

Epoch: 77
Loss: 0.3792451396584511
RMSE train: 0.513775	val: 1.210796	test: 1.159111
MAE train: 0.384147	val: 0.996011	test: 0.913270

Epoch: 78
Loss: 0.3924121931195259
RMSE train: 0.526684	val: 1.163912	test: 1.146668
MAE train: 0.388900	val: 0.958784	test: 0.888240

Epoch: 79
Loss: 0.37713107466697693
RMSE train: 0.579736	val: 1.164086	test: 1.178808
MAE train: 0.423424	val: 0.974635	test: 0.904578

Epoch: 80
Loss: 0.3752058818936348
RMSE train: 0.584992	val: 1.189927	test: 1.195461
MAE train: 0.424987	val: 0.998747	test: 0.912734

Epoch: 81
Loss: 0.35941115766763687
RMSE train: 0.534360	val: 1.135393	test: 1.137087
MAE train: 0.398850	val: 0.944317	test: 0.875960

Epoch: 82
Loss: 0.377156525850296
RMSE train: 0.649681	val: 1.347702	test: 1.251362
MAE train: 0.480853	val: 1.096226	test: 0.974801

Epoch: 83
Loss: 0.38231730461120605
RMSE train: 0.615730	val: 1.236013	test: 1.211894
MAE train: 0.451514	val: 1.019274	test: 0.939158
MAE train: 0.640065	val: 1.111075	test: 1.120350

Epoch: 23
Loss: 1.2415679693222046
RMSE train: 0.820795	val: 1.448305	test: 1.448282
MAE train: 0.635050	val: 1.141125	test: 1.147679

Epoch: 24
Loss: 1.124832034111023
RMSE train: 0.872403	val: 1.531521	test: 1.470144
MAE train: 0.672091	val: 1.174650	test: 1.157113

Epoch: 25
Loss: 1.0616327822208405
RMSE train: 0.820608	val: 1.419871	test: 1.380666
MAE train: 0.642739	val: 1.137702	test: 1.087113

Epoch: 26
Loss: 0.9750831127166748
RMSE train: 0.814327	val: 1.410594	test: 1.328636
MAE train: 0.644603	val: 1.127160	test: 1.051809

Epoch: 27
Loss: 1.077364906668663
RMSE train: 0.835275	val: 1.618871	test: 1.424180
MAE train: 0.659535	val: 1.246420	test: 1.113645

Epoch: 28
Loss: 0.9551985859870911
RMSE train: 0.783889	val: 1.514988	test: 1.377212
MAE train: 0.620938	val: 1.192699	test: 1.083334

Epoch: 29
Loss: 0.9322467744350433
RMSE train: 0.770271	val: 1.439272	test: 1.355576
MAE train: 0.614363	val: 1.144353	test: 1.056673

Epoch: 30
Loss: 0.89609594643116
RMSE train: 0.823183	val: 1.562603	test: 1.536711
MAE train: 0.636043	val: 1.175836	test: 1.177714

Epoch: 31
Loss: 0.9269565939903259
RMSE train: 0.790407	val: 1.438260	test: 1.460410
MAE train: 0.609679	val: 1.119646	test: 1.096212

Epoch: 32
Loss: 0.9040446728467941
RMSE train: 0.775649	val: 1.294633	test: 1.309591
MAE train: 0.605502	val: 1.035940	test: 1.010946

Epoch: 33
Loss: 0.9351208806037903
RMSE train: 0.785905	val: 1.502201	test: 1.389359
MAE train: 0.614089	val: 1.142730	test: 1.071681

Epoch: 34
Loss: 0.8309363275766373
RMSE train: 0.817533	val: 1.518284	test: 1.419285
MAE train: 0.632952	val: 1.193516	test: 1.098175

Epoch: 35
Loss: 0.7924363017082214
RMSE train: 0.718765	val: 1.302606	test: 1.302755
MAE train: 0.558827	val: 1.032926	test: 1.024633

Epoch: 36
Loss: 0.8327477276325226
RMSE train: 0.736149	val: 1.428611	test: 1.412963
MAE train: 0.565615	val: 1.077755	test: 1.101477

Epoch: 37
Loss: 0.8534395843744278
RMSE train: 0.883547	val: 1.672382	test: 1.650752
MAE train: 0.676928	val: 1.248943	test: 1.298953

Epoch: 38
Loss: 0.8085815012454987
RMSE train: 0.703311	val: 1.209378	test: 1.247398
MAE train: 0.552954	val: 0.987425	test: 0.950602

Epoch: 39
Loss: 0.7968344390392303
RMSE train: 0.678550	val: 1.318594	test: 1.307502
MAE train: 0.526126	val: 1.054224	test: 0.978270

Epoch: 40
Loss: 0.7022218257188797
RMSE train: 0.753314	val: 1.496521	test: 1.452926
MAE train: 0.582830	val: 1.196866	test: 1.111523

Epoch: 41
Loss: 0.718906432390213
RMSE train: 0.653706	val: 1.366187	test: 1.345481
MAE train: 0.506981	val: 1.090921	test: 1.008155

Epoch: 42
Loss: 0.6752760112285614
RMSE train: 0.647741	val: 1.385202	test: 1.377033
MAE train: 0.498034	val: 1.056494	test: 1.012125

Epoch: 43
Loss: 0.6909559220075607
RMSE train: 0.738018	val: 1.456319	test: 1.446698
MAE train: 0.554636	val: 1.105305	test: 1.069240

Epoch: 44
Loss: 0.6501971632242203
RMSE train: 0.721644	val: 1.391223	test: 1.381031
MAE train: 0.555549	val: 1.089257	test: 1.042640

Epoch: 45
Loss: 0.6669311672449112
RMSE train: 0.634716	val: 1.255901	test: 1.254058
MAE train: 0.500070	val: 0.980629	test: 0.953639

Epoch: 46
Loss: 0.7029508650302887
RMSE train: 0.652997	val: 1.370350	test: 1.369296
MAE train: 0.500553	val: 1.035019	test: 1.014065

Epoch: 47
Loss: 0.7139352709054947
RMSE train: 0.819713	val: 1.628878	test: 1.587037
MAE train: 0.627370	val: 1.260912	test: 1.215587

Epoch: 48
Loss: 0.6049721837043762
RMSE train: 0.704755	val: 1.405940	test: 1.384335
MAE train: 0.540510	val: 1.119032	test: 1.054082

Epoch: 49
Loss: 0.6126526147127151
RMSE train: 0.629395	val: 1.344924	test: 1.316104
MAE train: 0.485238	val: 1.044820	test: 0.992729

Epoch: 50
Loss: 0.5934732630848885
RMSE train: 0.683715	val: 1.446382	test: 1.417170
MAE train: 0.523875	val: 1.094850	test: 1.056301

Epoch: 51
Loss: 0.5846360474824905
RMSE train: 0.684519	val: 1.365272	test: 1.380673
MAE train: 0.529094	val: 1.069177	test: 1.016401

Epoch: 52
Loss: 0.589365616440773
RMSE train: 0.753276	val: 1.462728	test: 1.462547
MAE train: 0.571621	val: 1.133405	test: 1.088107

Epoch: 53
Loss: 0.5783895701169968
RMSE train: 0.706274	val: 1.459749	test: 1.415298
MAE train: 0.526564	val: 1.114289	test: 1.056680

Epoch: 54
Loss: 0.5469301640987396
RMSE train: 0.642773	val: 1.367328	test: 1.333725
MAE train: 0.488690	val: 1.069799	test: 0.998205

Epoch: 55
Loss: 0.5325937420129776
RMSE train: 0.644479	val: 1.363459	test: 1.331579
MAE train: 0.488866	val: 1.081548	test: 0.983216

Epoch: 56
Loss: 0.5961789190769196
RMSE train: 0.666294	val: 1.417632	test: 1.388872
MAE train: 0.503394	val: 1.113293	test: 1.028902

Epoch: 57
Loss: 0.5186254158616066
RMSE train: 0.614493	val: 1.342125	test: 1.347621
MAE train: 0.469649	val: 1.053190	test: 1.018438

Epoch: 58
Loss: 0.5537597984075546
RMSE train: 0.638099	val: 1.383107	test: 1.383001
MAE train: 0.485575	val: 1.064024	test: 1.041184

Epoch: 59
Loss: 0.5193390995264053
RMSE train: 0.605925	val: 1.399315	test: 1.402356
MAE train: 0.462647	val: 1.081092	test: 1.057122

Epoch: 60
Loss: 0.5106053352355957
RMSE train: 0.641395	val: 1.412842	test: 1.419821
MAE train: 0.493282	val: 1.127451	test: 1.066330

Epoch: 61
Loss: 0.4872998371720314
RMSE train: 0.598039	val: 1.286411	test: 1.315366
MAE train: 0.465781	val: 1.053610	test: 0.998120

Epoch: 62
Loss: 0.4936893507838249
RMSE train: 0.601141	val: 1.388348	test: 1.384009
MAE train: 0.459710	val: 1.086736	test: 1.058498

Epoch: 63
Loss: 0.5148360878229141
RMSE train: 0.651081	val: 1.373431	test: 1.383124
MAE train: 0.494416	val: 1.066063	test: 1.020415

Epoch: 64
Loss: 0.4488952234387398
RMSE train: 0.577682	val: 1.265668	test: 1.307464
MAE train: 0.443882	val: 1.002436	test: 0.962212

Epoch: 65
Loss: 0.4537013918161392
RMSE train: 0.606703	val: 1.354299	test: 1.360466
MAE train: 0.461771	val: 1.024683	test: 1.016583

Epoch: 66
Loss: 0.4529871344566345
RMSE train: 0.671462	val: 1.399473	test: 1.418226
MAE train: 0.507866	val: 1.066293	test: 1.045034

Epoch: 67
Loss: 0.4741416722536087
RMSE train: 0.651722	val: 1.431520	test: 1.433024
MAE train: 0.494897	val: 1.089359	test: 1.063771

Epoch: 68
Loss: 0.4211657643318176
RMSE train: 0.644736	val: 1.510769	test: 1.474889
MAE train: 0.477479	val: 1.127588	test: 1.090126

Epoch: 69
Loss: 0.47170496731996536
RMSE train: 0.592014	val: 1.407227	test: 1.388545
MAE train: 0.446006	val: 1.069657	test: 1.016606

Epoch: 70
Loss: 0.4602303206920624
RMSE train: 0.567966	val: 1.362621	test: 1.332478
MAE train: 0.434789	val: 1.062571	test: 0.993818

Epoch: 71
Loss: 0.4234130010008812
RMSE train: 0.610854	val: 1.437553	test: 1.373184
MAE train: 0.458695	val: 1.100514	test: 1.034802

Epoch: 72
Loss: 0.42747630178928375
RMSE train: 0.527662	val: 1.307747	test: 1.295109
MAE train: 0.404088	val: 1.029480	test: 0.973321

Epoch: 73
Loss: 0.42979295551776886
RMSE train: 0.563413	val: 1.356801	test: 1.343441
MAE train: 0.431206	val: 1.068967	test: 1.003812

Epoch: 74
Loss: 0.3652110621333122
RMSE train: 0.550364	val: 1.326507	test: 1.331186
MAE train: 0.421129	val: 1.058705	test: 1.001303

Epoch: 75
Loss: 0.4367513135075569
RMSE train: 0.605014	val: 1.372491	test: 1.343870
MAE train: 0.454541	val: 1.074486	test: 1.012778

Epoch: 76
Loss: 0.4142378196120262
RMSE train: 0.628690	val: 1.490090	test: 1.417838
MAE train: 0.468749	val: 1.124852	test: 1.042812

Epoch: 77
Loss: 0.44643350690603256
RMSE train: 0.530988	val: 1.331932	test: 1.313224
MAE train: 0.405564	val: 1.029335	test: 0.964069

Epoch: 78
Loss: 0.3666469231247902
RMSE train: 0.515998	val: 1.270331	test: 1.270148
MAE train: 0.394684	val: 0.991994	test: 0.947763

Epoch: 79
Loss: 0.38168173283338547
RMSE train: 0.544201	val: 1.371680	test: 1.344921
MAE train: 0.414877	val: 1.051362	test: 1.005096

Epoch: 80
Loss: 0.37834804505109787
RMSE train: 0.621902	val: 1.444121	test: 1.428416
MAE train: 0.474999	val: 1.107805	test: 1.062544

Epoch: 81
Loss: 0.4270608499646187
RMSE train: 0.574514	val: 1.376433	test: 1.378548
MAE train: 0.436387	val: 1.059659	test: 1.020192

Epoch: 82
Loss: 0.395995557308197
RMSE train: 0.642212	val: 1.425420	test: 1.418936
MAE train: 0.473256	val: 1.082837	test: 1.042144

Epoch: 83
Loss: 0.3890816196799278
RMSE train: 0.609772	val: 1.401995	test: 1.382007
MAE train: 0.756401	val: 1.091666	test: 1.071605

Epoch: 23
Loss: 1.2912945747375488
RMSE train: 0.999993	val: 1.546188	test: 1.490610
MAE train: 0.780232	val: 1.247277	test: 1.165103

Epoch: 24
Loss: 1.4402973055839539
RMSE train: 0.923624	val: 1.475046	test: 1.411034
MAE train: 0.718434	val: 1.152589	test: 1.109253

Epoch: 25
Loss: 1.1979871988296509
RMSE train: 0.926790	val: 1.506611	test: 1.424089
MAE train: 0.720984	val: 1.173768	test: 1.116151

Epoch: 26
Loss: 1.1922373175621033
RMSE train: 0.925797	val: 1.548824	test: 1.492730
MAE train: 0.714950	val: 1.248646	test: 1.166264

Epoch: 27
Loss: 1.1515729129314423
RMSE train: 0.824466	val: 1.528456	test: 1.412355
MAE train: 0.638836	val: 1.192649	test: 1.077436

Epoch: 28
Loss: 1.1433532536029816
RMSE train: 0.795040	val: 1.416121	test: 1.323177
MAE train: 0.617297	val: 1.115403	test: 1.026097

Epoch: 29
Loss: 1.0602414309978485
RMSE train: 0.788231	val: 1.307469	test: 1.288307
MAE train: 0.614709	val: 1.046696	test: 1.001648

Epoch: 30
Loss: 1.1159708797931671
RMSE train: 0.819881	val: 1.351404	test: 1.346599
MAE train: 0.635332	val: 1.092639	test: 1.033502

Epoch: 31
Loss: 1.0233792215585709
RMSE train: 0.911202	val: 1.461727	test: 1.433971
MAE train: 0.716823	val: 1.185769	test: 1.099216

Epoch: 32
Loss: 0.9873014986515045
RMSE train: 0.813388	val: 1.440752	test: 1.323173
MAE train: 0.634333	val: 1.155002	test: 1.043966

Epoch: 33
Loss: 1.0201102197170258
RMSE train: 0.775366	val: 1.393886	test: 1.278002
MAE train: 0.604669	val: 1.139104	test: 1.001511

Epoch: 34
Loss: 0.9299676418304443
RMSE train: 0.799262	val: 1.514086	test: 1.356804
MAE train: 0.625207	val: 1.240985	test: 1.035593

Epoch: 35
Loss: 0.8238223344087601
RMSE train: 0.789209	val: 1.477403	test: 1.368986
MAE train: 0.617002	val: 1.200914	test: 1.050388

Epoch: 36
Loss: 0.8665312975645065
RMSE train: 0.748339	val: 1.530083	test: 1.381884
MAE train: 0.582757	val: 1.222931	test: 1.083478

Epoch: 37
Loss: 0.8396458923816681
RMSE train: 0.824711	val: 1.599468	test: 1.463798
MAE train: 0.654787	val: 1.275701	test: 1.130042

Epoch: 38
Loss: 0.781608060002327
RMSE train: 0.742092	val: 1.464202	test: 1.347718
MAE train: 0.584351	val: 1.163367	test: 1.051175

Epoch: 39
Loss: 0.8231356292963028
RMSE train: 0.676020	val: 1.486749	test: 1.298296
MAE train: 0.527521	val: 1.187281	test: 1.026918

Epoch: 40
Loss: 0.6873770207166672
RMSE train: 0.852382	val: 1.657810	test: 1.503185
MAE train: 0.679639	val: 1.359338	test: 1.159048

Epoch: 41
Loss: 0.7606582194566727
RMSE train: 0.768327	val: 1.500961	test: 1.385565
MAE train: 0.604961	val: 1.223142	test: 1.064716

Epoch: 42
Loss: 0.7682752758264542
RMSE train: 0.634773	val: 1.496751	test: 1.304290
MAE train: 0.497495	val: 1.187449	test: 1.040356

Epoch: 43
Loss: 0.7092647105455399
RMSE train: 0.768216	val: 1.585580	test: 1.357546
MAE train: 0.602003	val: 1.260149	test: 1.067077

Epoch: 44
Loss: 0.7937644720077515
RMSE train: 0.856543	val: 1.538344	test: 1.425126
MAE train: 0.682373	val: 1.242670	test: 1.105930

Epoch: 45
Loss: 0.6057441234588623
RMSE train: 0.699107	val: 1.472268	test: 1.377779
MAE train: 0.551273	val: 1.134553	test: 1.088358

Epoch: 46
Loss: 0.7000798434019089
RMSE train: 0.772211	val: 1.473887	test: 1.376197
MAE train: 0.610797	val: 1.164805	test: 1.075756

Epoch: 47
Loss: 0.681265726685524
RMSE train: 0.838390	val: 1.544135	test: 1.438492
MAE train: 0.671405	val: 1.242792	test: 1.113369

Epoch: 48
Loss: 0.6421839892864227
RMSE train: 0.886138	val: 1.702550	test: 1.516482
MAE train: 0.706982	val: 1.404343	test: 1.164679

Epoch: 49
Loss: 0.7135759592056274
RMSE train: 0.750187	val: 1.467065	test: 1.384102
MAE train: 0.601942	val: 1.195022	test: 1.064846

Epoch: 50
Loss: 0.6054502576589584
RMSE train: 0.719672	val: 1.424715	test: 1.393705
MAE train: 0.572163	val: 1.165411	test: 1.080471

Epoch: 51
Loss: 0.6453278511762619
RMSE train: 0.686600	val: 1.379419	test: 1.367703
MAE train: 0.545957	val: 1.113178	test: 1.092520

Epoch: 52
Loss: 0.6523777842521667
RMSE train: 0.690181	val: 1.462430	test: 1.420784
MAE train: 0.548054	val: 1.201224	test: 1.144312

Epoch: 53
Loss: 0.5533707812428474
RMSE train: 0.649179	val: 1.381046	test: 1.382893
MAE train: 0.514942	val: 1.123734	test: 1.087146

Epoch: 54
Loss: 0.546615906059742
RMSE train: 0.683808	val: 1.485398	test: 1.415793
MAE train: 0.537572	val: 1.202299	test: 1.111658

Epoch: 55
Loss: 0.5553701743483543
RMSE train: 0.751971	val: 1.474887	test: 1.439808
MAE train: 0.599375	val: 1.203883	test: 1.127502

Epoch: 56
Loss: 0.6119252443313599
RMSE train: 0.714924	val: 1.412273	test: 1.363882
MAE train: 0.566295	val: 1.140135	test: 1.056511

Epoch: 57
Loss: 0.5939604938030243
RMSE train: 0.721091	val: 1.731179	test: 1.560707
MAE train: 0.562566	val: 1.376878	test: 1.273750

Epoch: 58
Loss: 0.6078952848911285
RMSE train: 0.683441	val: 1.349607	test: 1.391693
MAE train: 0.523967	val: 1.092306	test: 1.116729

Epoch: 59
Loss: 0.6179098635911942
RMSE train: 0.666761	val: 1.387023	test: 1.388048
MAE train: 0.518539	val: 1.114143	test: 1.112463

Epoch: 60
Loss: 0.6738226115703583
RMSE train: 0.749660	val: 1.547173	test: 1.472548
MAE train: 0.575215	val: 1.252368	test: 1.179933

Epoch: 61
Loss: 0.5702103674411774
RMSE train: 0.636991	val: 1.305292	test: 1.334904
MAE train: 0.500924	val: 1.034837	test: 1.088356

Epoch: 62
Loss: 0.6190846040844917
RMSE train: 0.555594	val: 1.497852	test: 1.412946
MAE train: 0.437522	val: 1.211834	test: 1.155746

Epoch: 63
Loss: 0.5731069147586823
RMSE train: 0.664906	val: 1.493668	test: 1.443031
MAE train: 0.526011	val: 1.195367	test: 1.135540

Epoch: 64
Loss: 0.5485996454954147
RMSE train: 0.715046	val: 1.459732	test: 1.431784
MAE train: 0.550288	val: 1.175930	test: 1.112551

Epoch: 65
Loss: 0.5067457184195518
RMSE train: 0.619229	val: 1.489484	test: 1.425822
MAE train: 0.487641	val: 1.199244	test: 1.123608

Epoch: 66
Loss: 0.5139042139053345
RMSE train: 0.671628	val: 1.543686	test: 1.498359
MAE train: 0.533530	val: 1.245907	test: 1.171222

Epoch: 67
Loss: 0.455569364130497
RMSE train: 0.689022	val: 1.480083	test: 1.404619
MAE train: 0.546165	val: 1.204786	test: 1.103787

Epoch: 68
Loss: 0.5155320763587952
RMSE train: 0.603613	val: 1.621155	test: 1.447163
MAE train: 0.477077	val: 1.283800	test: 1.185793

Epoch: 69
Loss: 0.5492576360702515
RMSE train: 0.635863	val: 1.381176	test: 1.342556
MAE train: 0.507212	val: 1.120742	test: 1.078125

Epoch: 70
Loss: 0.47125330567359924
RMSE train: 0.713870	val: 1.565006	test: 1.471241
MAE train: 0.565936	val: 1.279392	test: 1.149698

Epoch: 71
Loss: 0.4567677304148674
RMSE train: 0.554005	val: 1.456088	test: 1.336305
MAE train: 0.431004	val: 1.169644	test: 1.075844

Epoch: 72
Loss: 0.4746490865945816
RMSE train: 0.603033	val: 1.459468	test: 1.385174
MAE train: 0.474680	val: 1.160915	test: 1.115838

Epoch: 73
Loss: 0.4798649251461029
RMSE train: 0.676272	val: 1.611962	test: 1.516306
MAE train: 0.529274	val: 1.293216	test: 1.197282

Epoch: 74
Loss: 0.4600442126393318
RMSE train: 0.568808	val: 1.422624	test: 1.372652
MAE train: 0.446622	val: 1.141317	test: 1.101786

Epoch: 75
Loss: 0.4654151722788811
RMSE train: 0.637686	val: 1.384781	test: 1.367969
MAE train: 0.499271	val: 1.105305	test: 1.093205

Epoch: 76
Loss: 0.413934588432312
RMSE train: 0.685868	val: 1.585769	test: 1.468331
MAE train: 0.533389	val: 1.278325	test: 1.156247

Epoch: 77
Loss: 0.46289999783039093
RMSE train: 0.605262	val: 1.566337	test: 1.425145
MAE train: 0.482460	val: 1.273996	test: 1.130382

Epoch: 78
Loss: 0.4031648710370064
RMSE train: 0.631476	val: 1.683321	test: 1.435503
MAE train: 0.494140	val: 1.363146	test: 1.142631

Epoch: 79
Loss: 0.42340049892663956
RMSE train: 0.625022	val: 1.509149	test: 1.390110
MAE train: 0.497132	val: 1.225836	test: 1.097953

Epoch: 80
Loss: 0.429635152220726
RMSE train: 0.615248	val: 1.556627	test: 1.421825
MAE train: 0.494291	val: 1.267592	test: 1.123870

Epoch: 81
Loss: 0.42015422135591507
RMSE train: 0.561334	val: 1.428289	test: 1.376354
MAE train: 0.448510	val: 1.156882	test: 1.097490

Epoch: 82
Loss: 0.44388447701931
RMSE train: 0.597171	val: 1.710905	test: 1.469840
MAE train: 0.459165	val: 1.372575	test: 1.193844

Epoch: 83
Loss: 0.40656449645757675
RMSE train: 0.549991	val: 1.484678	test: 1.385011
MAE train: 0.811593	val: 1.249068	test: 1.288932

Epoch: 23
Loss: 1.5544849336147308
RMSE train: 0.993746	val: 1.466789	test: 1.565886
MAE train: 0.787973	val: 1.211237	test: 1.216737

Epoch: 24
Loss: 1.6099914610385895
RMSE train: 0.971485	val: 1.437443	test: 1.527501
MAE train: 0.764896	val: 1.188058	test: 1.167460

Epoch: 25
Loss: 1.4722571074962616
RMSE train: 0.930136	val: 1.360498	test: 1.504425
MAE train: 0.733629	val: 1.123574	test: 1.174530

Epoch: 26
Loss: 1.419359177350998
RMSE train: 0.896920	val: 1.351612	test: 1.484515
MAE train: 0.705201	val: 1.121953	test: 1.149723

Epoch: 27
Loss: 1.3314623832702637
RMSE train: 0.880185	val: 1.398462	test: 1.449754
MAE train: 0.692914	val: 1.146072	test: 1.162455

Epoch: 28
Loss: 1.298488289117813
RMSE train: 0.848137	val: 1.372100	test: 1.446385
MAE train: 0.669818	val: 1.096943	test: 1.176866

Epoch: 29
Loss: 1.2393585443496704
RMSE train: 0.826553	val: 1.386527	test: 1.469010
MAE train: 0.655089	val: 1.081403	test: 1.213386

Epoch: 30
Loss: 1.2241755425930023
RMSE train: 0.837918	val: 1.416466	test: 1.504844
MAE train: 0.650387	val: 1.153914	test: 1.200063

Epoch: 31
Loss: 1.160176545381546
RMSE train: 0.902822	val: 1.767305	test: 1.708559
MAE train: 0.701914	val: 1.461766	test: 1.345380

Epoch: 32
Loss: 1.1323379874229431
RMSE train: 0.893213	val: 1.816592	test: 1.674870
MAE train: 0.708735	val: 1.491095	test: 1.304733

Epoch: 33
Loss: 1.1564911305904388
RMSE train: 0.790320	val: 1.662436	test: 1.586848
MAE train: 0.611359	val: 1.369523	test: 1.232873

Epoch: 34
Loss: 1.0448179841041565
RMSE train: 0.795147	val: 1.610022	test: 1.522868
MAE train: 0.617484	val: 1.310976	test: 1.188941

Epoch: 35
Loss: 1.0186471194028854
RMSE train: 0.815545	val: 1.595924	test: 1.497636
MAE train: 0.637361	val: 1.310512	test: 1.155997

Epoch: 36
Loss: 1.000273734331131
RMSE train: 0.777866	val: 1.595654	test: 1.504319
MAE train: 0.604631	val: 1.337780	test: 1.205920

Epoch: 37
Loss: 1.061449185013771
RMSE train: 0.755131	val: 1.493097	test: 1.384360
MAE train: 0.587434	val: 1.215674	test: 1.118334

Epoch: 38
Loss: 0.9540001899003983
RMSE train: 0.772896	val: 1.481070	test: 1.396659
MAE train: 0.605211	val: 1.194361	test: 1.136118

Epoch: 39
Loss: 0.9503307044506073
RMSE train: 0.724822	val: 1.454225	test: 1.495665
MAE train: 0.563976	val: 1.202181	test: 1.218129

Epoch: 40
Loss: 0.8294863253831863
RMSE train: 0.705734	val: 1.473534	test: 1.487161
MAE train: 0.544486	val: 1.233078	test: 1.218321

Epoch: 41
Loss: 0.8381897360086441
RMSE train: 0.719242	val: 1.554934	test: 1.502090
MAE train: 0.557185	val: 1.311831	test: 1.226897

Epoch: 42
Loss: 0.8730920255184174
RMSE train: 0.667075	val: 1.533904	test: 1.472659
MAE train: 0.521649	val: 1.299653	test: 1.193224

Epoch: 43
Loss: 0.8170478940010071
RMSE train: 0.730477	val: 1.501159	test: 1.436790
MAE train: 0.580340	val: 1.265097	test: 1.172929

Epoch: 44
Loss: 0.8207727521657944
RMSE train: 0.779242	val: 1.540360	test: 1.482291
MAE train: 0.621267	val: 1.308945	test: 1.224995

Epoch: 45
Loss: 0.7933171838521957
RMSE train: 0.750008	val: 1.460855	test: 1.447080
MAE train: 0.591931	val: 1.218134	test: 1.182549

Epoch: 46
Loss: 0.795555904507637
RMSE train: 0.843675	val: 1.580864	test: 1.502340
MAE train: 0.662068	val: 1.346624	test: 1.212670

Epoch: 47
Loss: 0.7928826361894608
RMSE train: 0.722079	val: 1.583703	test: 1.541515
MAE train: 0.564085	val: 1.360315	test: 1.257318

Epoch: 48
Loss: 0.7278871238231659
RMSE train: 0.664732	val: 1.552209	test: 1.489193
MAE train: 0.519339	val: 1.298489	test: 1.209761

Epoch: 49
Loss: 0.7440599054098129
RMSE train: 0.626525	val: 1.553964	test: 1.434179
MAE train: 0.491302	val: 1.235939	test: 1.145637

Epoch: 50
Loss: 0.7283303141593933
RMSE train: 0.659055	val: 1.593691	test: 1.473199
MAE train: 0.520099	val: 1.255521	test: 1.134420

Epoch: 51
Loss: 0.7359398752450943
RMSE train: 0.648902	val: 1.606256	test: 1.518714
MAE train: 0.515109	val: 1.308098	test: 1.175112

Epoch: 52
Loss: 0.7297647297382355
RMSE train: 0.630836	val: 1.660312	test: 1.573128
MAE train: 0.502768	val: 1.390615	test: 1.234730

Epoch: 53
Loss: 0.6400806903839111
RMSE train: 0.646283	val: 1.714659	test: 1.609860
MAE train: 0.512302	val: 1.445687	test: 1.258729

Epoch: 54
Loss: 0.6178926080465317
RMSE train: 0.675139	val: 1.640575	test: 1.502488
MAE train: 0.529665	val: 1.344246	test: 1.176997

Epoch: 55
Loss: 0.6550716161727905
RMSE train: 0.627815	val: 1.445162	test: 1.412585
MAE train: 0.494591	val: 1.200577	test: 1.139448

Epoch: 56
Loss: 0.6424757987260818
RMSE train: 0.646410	val: 1.431634	test: 1.357975
MAE train: 0.514867	val: 1.157552	test: 1.076498

Epoch: 57
Loss: 0.6550340950489044
RMSE train: 0.656670	val: 1.498716	test: 1.393523
MAE train: 0.518402	val: 1.171839	test: 1.058953

Epoch: 58
Loss: 0.599448099732399
RMSE train: 0.551446	val: 1.383355	test: 1.402473
MAE train: 0.438929	val: 1.166043	test: 1.104209

Epoch: 59
Loss: 0.623228907585144
RMSE train: 0.520534	val: 1.328807	test: 1.335378
MAE train: 0.409923	val: 1.108799	test: 1.044654

Epoch: 60
Loss: 0.5749554634094238
RMSE train: 0.617920	val: 1.450403	test: 1.340688
MAE train: 0.486270	val: 1.145632	test: 1.030394

Epoch: 61
Loss: 0.5417870283126831
RMSE train: 0.540300	val: 1.388702	test: 1.318511
MAE train: 0.421900	val: 1.132828	test: 1.049153

Epoch: 62
Loss: 0.6153638362884521
RMSE train: 0.522962	val: 1.370169	test: 1.311352
MAE train: 0.408601	val: 1.117731	test: 1.046517

Epoch: 63
Loss: 0.5130264014005661
RMSE train: 0.568225	val: 1.474298	test: 1.364111
MAE train: 0.447197	val: 1.207602	test: 1.058159

Epoch: 64
Loss: 0.5797463357448578
RMSE train: 0.649232	val: 1.652706	test: 1.531365
MAE train: 0.509771	val: 1.364953	test: 1.189140

Epoch: 65
Loss: 0.5221804529428482
RMSE train: 0.570847	val: 1.637667	test: 1.531978
MAE train: 0.444761	val: 1.369455	test: 1.180496

Epoch: 66
Loss: 0.5578496158123016
RMSE train: 0.503432	val: 1.555302	test: 1.457960
MAE train: 0.402799	val: 1.296210	test: 1.124098

Epoch: 67
Loss: 0.4960888624191284
RMSE train: 0.615528	val: 1.733948	test: 1.623428
MAE train: 0.485533	val: 1.455094	test: 1.221234

Epoch: 68
Loss: 0.509630024433136
RMSE train: 0.648575	val: 1.678498	test: 1.575045
MAE train: 0.515574	val: 1.405319	test: 1.192373

Epoch: 69
Loss: 0.5174193382263184
RMSE train: 0.521492	val: 1.371382	test: 1.337108
MAE train: 0.415674	val: 1.103693	test: 1.066774

Epoch: 70
Loss: 0.5139817297458649
RMSE train: 0.508892	val: 1.512648	test: 1.406760
MAE train: 0.405353	val: 1.212272	test: 1.081561

Epoch: 71
Loss: 0.5228230133652687
RMSE train: 0.547124	val: 1.749203	test: 1.661274
MAE train: 0.427796	val: 1.462119	test: 1.267099

Epoch: 72
Loss: 0.49714453518390656
RMSE train: 0.530860	val: 1.688679	test: 1.611417
MAE train: 0.418256	val: 1.405187	test: 1.225612

Epoch: 73
Loss: 0.5045887008309364
RMSE train: 0.632779	val: 1.748170	test: 1.618608
MAE train: 0.487955	val: 1.415204	test: 1.204339

Epoch: 74
Loss: 0.4571237340569496
RMSE train: 0.605682	val: 1.595355	test: 1.495459
MAE train: 0.475614	val: 1.317953	test: 1.147860

Epoch: 75
Loss: 0.48104050755500793
RMSE train: 0.535483	val: 1.499055	test: 1.410157
MAE train: 0.422419	val: 1.213389	test: 1.079266

Epoch: 76
Loss: 0.44573407620191574
RMSE train: 0.574418	val: 1.659266	test: 1.509973
MAE train: 0.450453	val: 1.324789	test: 1.133146

Epoch: 77
Loss: 0.4797254949808121
RMSE train: 0.526107	val: 1.573717	test: 1.466368
MAE train: 0.416496	val: 1.267015	test: 1.098459

Epoch: 78
Loss: 0.44884829968214035
RMSE train: 0.510663	val: 1.498482	test: 1.385410
MAE train: 0.402469	val: 1.188625	test: 1.055544

Epoch: 79
Loss: 0.4316268861293793
RMSE train: 0.554370	val: 1.617529	test: 1.442479
MAE train: 0.435452	val: 1.282104	test: 1.080807

Epoch: 80
Loss: 0.46457475423812866
RMSE train: 0.536935	val: 1.577236	test: 1.433239
MAE train: 0.423207	val: 1.260814	test: 1.088678

Epoch: 81
Loss: 0.44220664352178574
RMSE train: 0.506658	val: 1.479504	test: 1.382215
MAE train: 0.397190	val: 1.190192	test: 1.068722

Epoch: 82
Loss: 0.43038227409124374
RMSE train: 0.552966	val: 1.591479	test: 1.424048
MAE train: 0.434732	val: 1.273194	test: 1.125612

Epoch: 83
Loss: 0.4253470376133919
RMSE train: 0.576438	val: 1.614190	test: 1.505473
MAE train: 0.699453	val: 1.364809	test: 1.329094

Epoch: 23
Loss: 1.365208625793457
RMSE train: 0.873969	val: 1.850856	test: 1.689876
MAE train: 0.685556	val: 1.437151	test: 1.374662

Epoch: 24
Loss: 1.3185600936412811
RMSE train: 0.857709	val: 1.710607	test: 1.585443
MAE train: 0.673163	val: 1.324053	test: 1.276786

Epoch: 25
Loss: 1.1470726132392883
RMSE train: 0.855939	val: 1.500222	test: 1.478656
MAE train: 0.665298	val: 1.193915	test: 1.184558

Epoch: 26
Loss: 1.0742792636156082
RMSE train: 0.814933	val: 1.583145	test: 1.523853
MAE train: 0.636402	val: 1.220987	test: 1.200607

Epoch: 27
Loss: 1.0996914058923721
RMSE train: 0.775290	val: 1.527997	test: 1.483351
MAE train: 0.608238	val: 1.173335	test: 1.183961

Epoch: 28
Loss: 1.100977897644043
RMSE train: 0.728901	val: 1.681059	test: 1.616711
MAE train: 0.565942	val: 1.283926	test: 1.266225

Epoch: 29
Loss: 0.9843849688768387
RMSE train: 0.819013	val: 1.816438	test: 1.759526
MAE train: 0.630691	val: 1.409525	test: 1.359887

Epoch: 30
Loss: 0.9492515027523041
RMSE train: 0.783039	val: 1.659163	test: 1.619875
MAE train: 0.603297	val: 1.306484	test: 1.242020

Epoch: 31
Loss: 0.9944125860929489
RMSE train: 0.757009	val: 1.578171	test: 1.581929
MAE train: 0.581348	val: 1.221848	test: 1.184727

Epoch: 32
Loss: 0.8781202733516693
RMSE train: 0.749315	val: 1.669626	test: 1.703754
MAE train: 0.577352	val: 1.251303	test: 1.286753

Epoch: 33
Loss: 0.875695064663887
RMSE train: 0.754448	val: 1.700201	test: 1.677779
MAE train: 0.587830	val: 1.269786	test: 1.269321

Epoch: 34
Loss: 0.8969786763191223
RMSE train: 0.897058	val: 1.759285	test: 1.776437
MAE train: 0.696684	val: 1.370547	test: 1.339351

Epoch: 35
Loss: 0.8630408048629761
RMSE train: 0.762586	val: 1.568075	test: 1.599640
MAE train: 0.591329	val: 1.201658	test: 1.195533

Epoch: 36
Loss: 0.8700530529022217
RMSE train: 0.680415	val: 1.585645	test: 1.583565
MAE train: 0.534025	val: 1.184369	test: 1.211133

Epoch: 37
Loss: 0.8403377085924149
RMSE train: 0.770949	val: 1.678182	test: 1.671490
MAE train: 0.599118	val: 1.305762	test: 1.285324

Epoch: 38
Loss: 0.8048320412635803
RMSE train: 0.718861	val: 1.374844	test: 1.363727
MAE train: 0.569970	val: 1.082085	test: 1.056738

Epoch: 39
Loss: 0.8171081393957138
RMSE train: 0.634096	val: 1.518781	test: 1.432794
MAE train: 0.506867	val: 1.158269	test: 1.083383

Epoch: 40
Loss: 0.7608744651079178
RMSE train: 0.608487	val: 1.476131	test: 1.440958
MAE train: 0.480120	val: 1.155348	test: 1.099902

Epoch: 41
Loss: 0.7792138308286667
RMSE train: 0.636509	val: 1.498388	test: 1.470658
MAE train: 0.503778	val: 1.177684	test: 1.127104

Epoch: 42
Loss: 0.6812763214111328
RMSE train: 0.677165	val: 1.627142	test: 1.540399
MAE train: 0.520945	val: 1.229806	test: 1.151022

Epoch: 43
Loss: 0.6930853724479675
RMSE train: 0.657441	val: 1.485656	test: 1.489879
MAE train: 0.507658	val: 1.134185	test: 1.122137

Epoch: 44
Loss: 0.6668957024812698
RMSE train: 0.746414	val: 1.503875	test: 1.586356
MAE train: 0.582502	val: 1.207805	test: 1.219905

Epoch: 45
Loss: 0.6426837891340256
RMSE train: 0.687252	val: 1.514955	test: 1.549551
MAE train: 0.533112	val: 1.156817	test: 1.173263

Epoch: 46
Loss: 0.7099197208881378
RMSE train: 0.622771	val: 1.576757	test: 1.543839
MAE train: 0.484472	val: 1.183584	test: 1.156590

Epoch: 47
Loss: 0.6663684397935867
RMSE train: 0.685100	val: 1.642874	test: 1.592868
MAE train: 0.533676	val: 1.261986	test: 1.218095

Epoch: 48
Loss: 0.611877977848053
RMSE train: 0.672833	val: 1.449642	test: 1.443324
MAE train: 0.524003	val: 1.129810	test: 1.097233

Epoch: 49
Loss: 0.6029632538557053
RMSE train: 0.611929	val: 1.487649	test: 1.425414
MAE train: 0.470749	val: 1.144897	test: 1.077228

Epoch: 50
Loss: 0.6008371114730835
RMSE train: 0.654669	val: 1.593573	test: 1.568497
MAE train: 0.511044	val: 1.186127	test: 1.169091

Epoch: 51
Loss: 0.5945324450731277
RMSE train: 0.632510	val: 1.482768	test: 1.503546
MAE train: 0.497152	val: 1.121844	test: 1.118277

Epoch: 52
Loss: 0.5698770135641098
RMSE train: 0.664365	val: 1.522627	test: 1.492011
MAE train: 0.515579	val: 1.134229	test: 1.106338

Epoch: 53
Loss: 0.590220183134079
RMSE train: 0.704901	val: 1.720616	test: 1.582531
MAE train: 0.537322	val: 1.284997	test: 1.167213

Epoch: 54
Loss: 0.5672942101955414
RMSE train: 0.652057	val: 1.631228	test: 1.575887
MAE train: 0.502451	val: 1.238003	test: 1.163394

Epoch: 55
Loss: 0.5971390530467033
RMSE train: 0.665888	val: 1.607225	test: 1.621805
MAE train: 0.515284	val: 1.240002	test: 1.211200

Epoch: 56
Loss: 0.5929197892546654
RMSE train: 0.706989	val: 1.685806	test: 1.666549
MAE train: 0.542197	val: 1.265500	test: 1.232517

Epoch: 57
Loss: 0.5112654715776443
RMSE train: 0.688493	val: 1.636409	test: 1.628752
MAE train: 0.533006	val: 1.228261	test: 1.213265

Epoch: 58
Loss: 0.5466471686959267
RMSE train: 0.698871	val: 1.524951	test: 1.622414
MAE train: 0.540949	val: 1.176019	test: 1.195863

Epoch: 59
Loss: 0.5345454141497612
RMSE train: 0.738255	val: 1.633261	test: 1.704526
MAE train: 0.563253	val: 1.220799	test: 1.238102

Epoch: 60
Loss: 0.5468057096004486
RMSE train: 0.769738	val: 1.614936	test: 1.675237
MAE train: 0.587954	val: 1.226275	test: 1.231985

Epoch: 61
Loss: 0.493227519094944
RMSE train: 0.610605	val: 1.374530	test: 1.432698
MAE train: 0.479849	val: 1.068012	test: 1.051912

Epoch: 62
Loss: 0.4985387548804283
RMSE train: 0.645269	val: 1.671546	test: 1.555510
MAE train: 0.493052	val: 1.257252	test: 1.156924

Epoch: 63
Loss: 0.5296756401658058
RMSE train: 0.595569	val: 1.599970	test: 1.503450
MAE train: 0.456468	val: 1.205427	test: 1.107940

Epoch: 64
Loss: 0.4068170413374901
RMSE train: 0.522185	val: 1.348313	test: 1.370716
MAE train: 0.406125	val: 1.056679	test: 1.020503

Epoch: 65
Loss: 0.4678465276956558
RMSE train: 0.544012	val: 1.521255	test: 1.450655
MAE train: 0.419936	val: 1.138742	test: 1.062688

Epoch: 66
Loss: 0.41256552934646606
RMSE train: 0.557433	val: 1.641260	test: 1.461578
MAE train: 0.435959	val: 1.235242	test: 1.092695

Epoch: 67
Loss: 0.478072352707386
RMSE train: 0.520116	val: 1.586436	test: 1.424259
MAE train: 0.407997	val: 1.230302	test: 1.071680

Epoch: 68
Loss: 0.41520361602306366
RMSE train: 0.565913	val: 1.704991	test: 1.515102
MAE train: 0.442333	val: 1.292777	test: 1.123090

Epoch: 69
Loss: 0.45021431148052216
RMSE train: 0.616216	val: 1.597619	test: 1.513906
MAE train: 0.467683	val: 1.199676	test: 1.095790

Epoch: 70
Loss: 0.43624401092529297
RMSE train: 0.672007	val: 1.606335	test: 1.587617
MAE train: 0.510270	val: 1.217585	test: 1.163616

Epoch: 71
Loss: 0.4437284916639328
RMSE train: 0.658812	val: 1.635839	test: 1.591792
MAE train: 0.502185	val: 1.211590	test: 1.180084

Epoch: 72
Loss: 0.43209799379110336
RMSE train: 0.549788	val: 1.506032	test: 1.504048
MAE train: 0.429115	val: 1.121860	test: 1.109667

Epoch: 73
Loss: 0.4120824337005615
RMSE train: 0.572135	val: 1.555566	test: 1.551600
MAE train: 0.446744	val: 1.162527	test: 1.131768

Epoch: 74
Loss: 0.39857136458158493
RMSE train: 0.467219	val: 1.493520	test: 1.481643
MAE train: 0.368299	val: 1.126016	test: 1.087567

Epoch: 75
Loss: 0.4168795570731163
RMSE train: 0.481054	val: 1.507976	test: 1.475311
MAE train: 0.378270	val: 1.138507	test: 1.086175

Epoch: 76
Loss: 0.38922490924596786
RMSE train: 0.559240	val: 1.697183	test: 1.571232
MAE train: 0.431195	val: 1.266542	test: 1.167683

Epoch: 77
Loss: 0.3901655897498131
RMSE train: 0.457084	val: 1.494416	test: 1.411899
MAE train: 0.362361	val: 1.133088	test: 1.062904

Epoch: 78
Loss: 0.3468688502907753
RMSE train: 0.479714	val: 1.390175	test: 1.412271
MAE train: 0.380527	val: 1.060869	test: 1.046344

Epoch: 79
Loss: 0.3912961483001709
RMSE train: 0.539899	val: 1.492148	test: 1.500921
MAE train: 0.425499	val: 1.128347	test: 1.096537

Epoch: 80
Loss: 0.3716742843389511
RMSE train: 0.629265	val: 1.625049	test: 1.589299
MAE train: 0.488845	val: 1.220261	test: 1.167159

Epoch: 81
Loss: 0.4328434392809868
RMSE train: 0.543753	val: 1.426648	test: 1.443987
MAE train: 0.430319	val: 1.104012	test: 1.065437

Epoch: 82
Loss: 0.3632032498717308
RMSE train: 0.629583	val: 1.589507	test: 1.625498
MAE train: 0.487722	val: 1.198259	test: 1.177396

Epoch: 83
Loss: 0.37830641120672226
RMSE train: 0.605555	val: 1.580401	test: 1.623223
MAE train: 0.826379	val: 1.640249	test: 1.417720

Epoch: 23
Loss: 1.53302800655365
RMSE train: 1.080635	val: 2.256442	test: 1.807106
MAE train: 0.847715	val: 1.725418	test: 1.440671

Epoch: 24
Loss: 1.5039379596710205
RMSE train: 1.130981	val: 2.063062	test: 1.652562
MAE train: 0.901131	val: 1.574551	test: 1.302643

Epoch: 25
Loss: 1.2668331563472748
RMSE train: 1.093165	val: 1.810268	test: 1.584220
MAE train: 0.868538	val: 1.374708	test: 1.249175

Epoch: 26
Loss: 1.3251831829547882
RMSE train: 0.920327	val: 2.058424	test: 1.899364
MAE train: 0.711808	val: 1.578779	test: 1.465806

Epoch: 27
Loss: 1.384206473827362
RMSE train: 0.911712	val: 2.017660	test: 1.940056
MAE train: 0.710501	val: 1.572295	test: 1.506196

Epoch: 28
Loss: 1.1965032368898392
RMSE train: 0.904584	val: 2.017154	test: 1.820478
MAE train: 0.713084	val: 1.547377	test: 1.386475

Epoch: 29
Loss: 1.1432479172945023
RMSE train: 0.841145	val: 2.106381	test: 1.920834
MAE train: 0.656833	val: 1.628895	test: 1.443156

Epoch: 30
Loss: 1.0953626334667206
RMSE train: 0.801964	val: 2.070753	test: 1.951315
MAE train: 0.623424	val: 1.610093	test: 1.489409

Epoch: 31
Loss: 1.083184540271759
RMSE train: 0.803919	val: 1.901528	test: 1.885237
MAE train: 0.620734	val: 1.509322	test: 1.432860

Epoch: 32
Loss: 1.0800874829292297
RMSE train: 0.782235	val: 1.751940	test: 1.766703
MAE train: 0.603820	val: 1.388254	test: 1.357293

Epoch: 33
Loss: 1.019138127565384
RMSE train: 0.803125	val: 1.817227	test: 1.762525
MAE train: 0.620981	val: 1.404780	test: 1.354935

Epoch: 34
Loss: 0.9644895642995834
RMSE train: 0.875188	val: 1.881224	test: 1.851656
MAE train: 0.675267	val: 1.487076	test: 1.426861

Epoch: 35
Loss: 0.9948229789733887
RMSE train: 0.901669	val: 1.917603	test: 1.974648
MAE train: 0.692467	val: 1.581551	test: 1.557921

Epoch: 36
Loss: 0.9646725803613663
RMSE train: 0.884984	val: 1.991742	test: 2.073002
MAE train: 0.681544	val: 1.657166	test: 1.655896

Epoch: 37
Loss: 0.9670788943767548
RMSE train: 0.873728	val: 2.146459	test: 2.211117
MAE train: 0.672507	val: 1.769202	test: 1.761031

Epoch: 38
Loss: 0.8121276795864105
RMSE train: 0.755112	val: 1.972426	test: 2.013382
MAE train: 0.579278	val: 1.588545	test: 1.573819

Epoch: 39
Loss: 0.8398528844118118
RMSE train: 0.715091	val: 1.969103	test: 2.011512
MAE train: 0.546339	val: 1.575451	test: 1.566752

Epoch: 40
Loss: 0.8788087218999863
RMSE train: 0.720366	val: 1.926691	test: 1.971651
MAE train: 0.548996	val: 1.557263	test: 1.545223

Epoch: 41
Loss: 0.8315751254558563
RMSE train: 0.771226	val: 2.053508	test: 2.082401
MAE train: 0.591680	val: 1.682798	test: 1.647558

Epoch: 42
Loss: 0.7504077255725861
RMSE train: 0.728266	val: 1.956541	test: 2.001022
MAE train: 0.555830	val: 1.587761	test: 1.541439

Epoch: 43
Loss: 0.8154349774122238
RMSE train: 0.711996	val: 1.935982	test: 2.033885
MAE train: 0.545232	val: 1.603754	test: 1.616508

Epoch: 44
Loss: 0.7593534588813782
RMSE train: 0.733132	val: 2.042433	test: 2.167875
MAE train: 0.563153	val: 1.732740	test: 1.781576

Epoch: 45
Loss: 0.7195604145526886
RMSE train: 0.685309	val: 2.006641	test: 2.116210
MAE train: 0.526939	val: 1.675555	test: 1.697274

Epoch: 46
Loss: 0.7890366464853287
RMSE train: 0.650578	val: 1.955752	test: 2.033459
MAE train: 0.504477	val: 1.595037	test: 1.579314

Epoch: 47
Loss: 0.8448268473148346
RMSE train: 0.611911	val: 1.882972	test: 1.946132
MAE train: 0.470332	val: 1.550438	test: 1.547683

Epoch: 48
Loss: 0.6740013360977173
RMSE train: 0.628561	val: 1.936809	test: 1.988279
MAE train: 0.483109	val: 1.594071	test: 1.584691

Epoch: 49
Loss: 0.7059003710746765
RMSE train: 0.649182	val: 2.064425	test: 2.082038
MAE train: 0.499083	val: 1.655885	test: 1.617954

Epoch: 50
Loss: 0.7269923537969589
RMSE train: 0.616526	val: 1.983247	test: 2.020914
MAE train: 0.472715	val: 1.582887	test: 1.564179

Epoch: 51
Loss: 0.6495018303394318
RMSE train: 0.593950	val: 1.936854	test: 1.975849
MAE train: 0.455347	val: 1.531539	test: 1.520113

Epoch: 52
Loss: 0.6534391045570374
RMSE train: 0.598864	val: 2.035628	test: 2.047753
MAE train: 0.468699	val: 1.604814	test: 1.557873

Epoch: 53
Loss: 0.6479980945587158
RMSE train: 0.597326	val: 1.911259	test: 1.948726
MAE train: 0.463556	val: 1.510887	test: 1.482955

Epoch: 54
Loss: 0.6151061952114105
RMSE train: 0.619786	val: 1.866582	test: 1.956261
MAE train: 0.473048	val: 1.512534	test: 1.530527

Epoch: 55
Loss: 0.6816366016864777
RMSE train: 0.627194	val: 1.986949	test: 2.059154
MAE train: 0.482559	val: 1.616402	test: 1.614059

Epoch: 56
Loss: 0.6192630082368851
RMSE train: 0.631585	val: 1.947981	test: 1.980074
MAE train: 0.486206	val: 1.550635	test: 1.511759

Epoch: 57
Loss: 0.6298277825117111
RMSE train: 0.659931	val: 1.993699	test: 2.015200
MAE train: 0.512636	val: 1.596310	test: 1.559121

Epoch: 58
Loss: 0.6086480468511581
RMSE train: 0.716258	val: 2.036751	test: 2.113086
MAE train: 0.552182	val: 1.671115	test: 1.658005

Epoch: 59
Loss: 0.5851250737905502
RMSE train: 0.621245	val: 2.102671	test: 2.103442
MAE train: 0.477372	val: 1.654221	test: 1.570645

Epoch: 60
Loss: 0.6392351984977722
RMSE train: 0.622433	val: 2.060881	test: 2.126923
MAE train: 0.477611	val: 1.643555	test: 1.608039

Epoch: 61
Loss: 0.5494515597820282
RMSE train: 0.664282	val: 2.003583	test: 2.115073
MAE train: 0.500632	val: 1.666204	test: 1.667099

Epoch: 62
Loss: 0.5504487752914429
RMSE train: 0.558144	val: 2.007050	test: 2.036557
MAE train: 0.428468	val: 1.579779	test: 1.533870

Epoch: 63
Loss: 0.5279160067439079
RMSE train: 0.550300	val: 2.109581	test: 2.088846
MAE train: 0.425326	val: 1.658224	test: 1.561764

Epoch: 64
Loss: 0.5075949877500534
RMSE train: 0.551169	val: 1.857564	test: 1.950810
MAE train: 0.422908	val: 1.478125	test: 1.465458

Epoch: 65
Loss: 0.5514656454324722
RMSE train: 0.515889	val: 1.966081	test: 2.032269
MAE train: 0.397298	val: 1.547361	test: 1.518786

Epoch: 66
Loss: 0.5028332993388176
RMSE train: 0.506632	val: 2.009788	test: 2.024545
MAE train: 0.390425	val: 1.558445	test: 1.505453

Epoch: 67
Loss: 0.4979969933629036
RMSE train: 0.515466	val: 1.915734	test: 1.954913
MAE train: 0.392780	val: 1.519779	test: 1.471844

Epoch: 68
Loss: 0.5185747742652893
RMSE train: 0.548996	val: 1.990939	test: 2.008152
MAE train: 0.416158	val: 1.592297	test: 1.525338

Epoch: 69
Loss: 0.4869414418935776
RMSE train: 0.506106	val: 1.948119	test: 1.941686
MAE train: 0.390305	val: 1.539681	test: 1.463499

Epoch: 70
Loss: 0.5159495696425438
RMSE train: 0.474302	val: 2.006193	test: 1.982962
MAE train: 0.365222	val: 1.591271	test: 1.506790

Epoch: 71
Loss: 0.45260039716959
RMSE train: 0.543298	val: 2.180860	test: 2.197119
MAE train: 0.418399	val: 1.737577	test: 1.695301

Epoch: 72
Loss: 0.5000732094049454
RMSE train: 0.509786	val: 2.015080	test: 2.063694
MAE train: 0.392980	val: 1.612171	test: 1.578142

Epoch: 73
Loss: 0.4860266000032425
RMSE train: 0.489026	val: 1.895418	test: 1.920335
MAE train: 0.375346	val: 1.491768	test: 1.439997

Epoch: 74
Loss: 0.4097984656691551
RMSE train: 0.489447	val: 1.920897	test: 1.966269
MAE train: 0.376502	val: 1.527949	test: 1.498404

Epoch: 75
Loss: 0.4483264610171318
RMSE train: 0.542276	val: 1.868852	test: 1.934090
MAE train: 0.417513	val: 1.509240	test: 1.484372

Epoch: 76
Loss: 0.4301983416080475
RMSE train: 0.555175	val: 2.020277	test: 2.074819
MAE train: 0.430346	val: 1.631924	test: 1.607983

Epoch: 77
Loss: 0.44902222603559494
RMSE train: 0.555200	val: 2.024847	test: 2.110009
MAE train: 0.426115	val: 1.634212	test: 1.626691

Epoch: 78
Loss: 0.43725868314504623
RMSE train: 0.563602	val: 1.925301	test: 2.041624
MAE train: 0.426958	val: 1.587766	test: 1.585350

Epoch: 79
Loss: 0.4374701902270317
RMSE train: 0.561918	val: 1.944264	test: 2.033187
MAE train: 0.420527	val: 1.571435	test: 1.567412

Epoch: 80
Loss: 0.41472330689430237
RMSE train: 0.548074	val: 2.040428	test: 2.065797
MAE train: 0.413950	val: 1.637196	test: 1.594338

Epoch: 81
Loss: 0.4275183528661728
RMSE train: 0.510603	val: 1.836677	test: 1.914403
MAE train: 0.393045	val: 1.522097	test: 1.485286

Epoch: 82
Loss: 0.4376252591609955
RMSE train: 0.528564	val: 1.805892	test: 1.876331
MAE train: 0.407626	val: 1.480761	test: 1.433059

Epoch: 83
Loss: 0.36391404271125793
RMSE train: 0.529409	val: 1.790734	test: 1.820798
MAE train: 0.836835	val: 1.815397	test: 1.612113

Epoch: 23
Loss: 1.6795561611652374
RMSE train: 1.026990	val: 2.283148	test: 2.065249
MAE train: 0.787050	val: 1.803457	test: 1.580100

Epoch: 24
Loss: 1.6981920003890991
RMSE train: 1.008753	val: 2.110342	test: 2.039043
MAE train: 0.768699	val: 1.720267	test: 1.549695

Epoch: 25
Loss: 1.6756552755832672
RMSE train: 1.005853	val: 2.141339	test: 2.142830
MAE train: 0.767486	val: 1.813895	test: 1.669688

Epoch: 26
Loss: 1.4903099536895752
RMSE train: 0.939054	val: 2.000553	test: 1.962007
MAE train: 0.732184	val: 1.642192	test: 1.511081

Epoch: 27
Loss: 1.4374221563339233
RMSE train: 0.928494	val: 1.962652	test: 1.853505
MAE train: 0.727898	val: 1.546362	test: 1.415970

Epoch: 28
Loss: 1.3363410532474518
RMSE train: 0.947094	val: 1.688765	test: 1.653008
MAE train: 0.735302	val: 1.359367	test: 1.287670

Epoch: 29
Loss: 1.3597412705421448
RMSE train: 0.861353	val: 1.733811	test: 1.710307
MAE train: 0.662215	val: 1.436876	test: 1.322995

Epoch: 30
Loss: 1.2533594965934753
RMSE train: 0.892995	val: 1.861398	test: 1.816963
MAE train: 0.686444	val: 1.579613	test: 1.435772

Epoch: 31
Loss: 1.2528603971004486
RMSE train: 0.842503	val: 1.740581	test: 1.710866
MAE train: 0.651650	val: 1.469128	test: 1.377816

Epoch: 32
Loss: 1.1699170172214508
RMSE train: 0.804302	val: 1.602675	test: 1.622820
MAE train: 0.624066	val: 1.332592	test: 1.312409

Epoch: 33
Loss: 1.1070349514484406
RMSE train: 0.777913	val: 1.491472	test: 1.539988
MAE train: 0.608131	val: 1.215284	test: 1.228520

Epoch: 34
Loss: 1.1184685230255127
RMSE train: 0.760196	val: 1.412965	test: 1.471765
MAE train: 0.594455	val: 1.154699	test: 1.167571

Epoch: 35
Loss: 1.0949699729681015
RMSE train: 0.848434	val: 1.603175	test: 1.566074
MAE train: 0.673659	val: 1.340626	test: 1.239998

Epoch: 36
Loss: 1.0213296115398407
RMSE train: 0.818188	val: 1.695830	test: 1.620662
MAE train: 0.653519	val: 1.429361	test: 1.298585

Epoch: 37
Loss: 1.0342310816049576
RMSE train: 0.739186	val: 1.683500	test: 1.599244
MAE train: 0.582185	val: 1.413900	test: 1.274983

Epoch: 38
Loss: 0.969224214553833
RMSE train: 0.826920	val: 1.870694	test: 1.783412
MAE train: 0.647653	val: 1.589930	test: 1.420703

Epoch: 39
Loss: 0.9401133209466934
RMSE train: 0.919776	val: 2.129989	test: 2.049395
MAE train: 0.723168	val: 1.809681	test: 1.640914

Epoch: 40
Loss: 0.9646636843681335
RMSE train: 0.792294	val: 1.990990	test: 1.946461
MAE train: 0.632728	val: 1.685225	test: 1.549412

Epoch: 41
Loss: 0.9254870563745499
RMSE train: 0.702240	val: 1.766737	test: 1.726778
MAE train: 0.553932	val: 1.482001	test: 1.365003

Epoch: 42
Loss: 1.0380904376506805
RMSE train: 0.750485	val: 1.819665	test: 1.718159
MAE train: 0.580049	val: 1.518105	test: 1.330036

Epoch: 43
Loss: 0.8759762495756149
RMSE train: 0.765848	val: 1.778872	test: 1.768571
MAE train: 0.595442	val: 1.540053	test: 1.409463

Epoch: 44
Loss: 0.8149503767490387
RMSE train: 0.666490	val: 1.614837	test: 1.643903
MAE train: 0.525542	val: 1.387191	test: 1.335570

Epoch: 45
Loss: 0.8347751200199127
RMSE train: 0.633738	val: 1.641944	test: 1.645895
MAE train: 0.499235	val: 1.403040	test: 1.311330

Epoch: 46
Loss: 0.7764185816049576
RMSE train: 0.686084	val: 1.735796	test: 1.739128
MAE train: 0.543800	val: 1.498013	test: 1.394551

Epoch: 47
Loss: 0.7449733167886734
RMSE train: 0.703582	val: 1.696471	test: 1.712735
MAE train: 0.555025	val: 1.479096	test: 1.396115

Epoch: 48
Loss: 0.7851320058107376
RMSE train: 0.667366	val: 1.683307	test: 1.608117
MAE train: 0.530589	val: 1.446803	test: 1.269483

Epoch: 49
Loss: 0.7393560111522675
RMSE train: 0.735146	val: 1.719495	test: 1.610403
MAE train: 0.584161	val: 1.482568	test: 1.291748

Epoch: 50
Loss: 0.7492565661668777
RMSE train: 0.736379	val: 1.653741	test: 1.557257
MAE train: 0.582732	val: 1.419790	test: 1.272978

Epoch: 51
Loss: 0.7216535359621048
RMSE train: 0.671216	val: 1.715531	test: 1.650780
MAE train: 0.533337	val: 1.460752	test: 1.280442

Epoch: 52
Loss: 0.6857178807258606
RMSE train: 0.653544	val: 1.889707	test: 1.875779
MAE train: 0.524273	val: 1.610687	test: 1.473246

Epoch: 53
Loss: 0.7377631664276123
RMSE train: 0.625286	val: 1.821517	test: 1.822689
MAE train: 0.498709	val: 1.568113	test: 1.441289

Epoch: 54
Loss: 0.6655951738357544
RMSE train: 0.581237	val: 1.734678	test: 1.711586
MAE train: 0.462733	val: 1.485715	test: 1.320092

Epoch: 55
Loss: 0.647438257932663
RMSE train: 0.620549	val: 1.734322	test: 1.730493
MAE train: 0.487590	val: 1.488262	test: 1.338874

Epoch: 56
Loss: 0.6601229906082153
RMSE train: 0.594834	val: 1.787143	test: 1.777128
MAE train: 0.467776	val: 1.545154	test: 1.386779

Epoch: 57
Loss: 0.6664834022521973
RMSE train: 0.568527	val: 1.760084	test: 1.743414
MAE train: 0.452839	val: 1.509269	test: 1.339663

Epoch: 58
Loss: 0.6805395931005478
RMSE train: 0.627188	val: 1.773609	test: 1.766048
MAE train: 0.494614	val: 1.528630	test: 1.380089

Epoch: 59
Loss: 0.6135319322347641
RMSE train: 0.583539	val: 1.707888	test: 1.718909
MAE train: 0.459680	val: 1.460666	test: 1.331900

Epoch: 60
Loss: 0.6247634738683701
RMSE train: 0.519903	val: 1.603245	test: 1.659242
MAE train: 0.412371	val: 1.375741	test: 1.288831

Epoch: 61
Loss: 0.6431731730699539
RMSE train: 0.579795	val: 1.837031	test: 1.902132
MAE train: 0.446480	val: 1.578389	test: 1.507585

Epoch: 62
Loss: 0.6679130494594574
RMSE train: 0.666905	val: 1.947645	test: 1.971448
MAE train: 0.526402	val: 1.684569	test: 1.560675

Epoch: 63
Loss: 0.6280283331871033
RMSE train: 0.618961	val: 1.676205	test: 1.702001
MAE train: 0.483900	val: 1.459932	test: 1.342515

Epoch: 64
Loss: 0.604422315955162
RMSE train: 0.603284	val: 1.577626	test: 1.590894
MAE train: 0.468779	val: 1.368010	test: 1.252703

Epoch: 65
Loss: 0.6359943002462387
RMSE train: 0.549283	val: 1.667594	test: 1.628585
MAE train: 0.431667	val: 1.435823	test: 1.236579

Epoch: 66
Loss: 0.5809758454561234
RMSE train: 0.574401	val: 1.759927	test: 1.752610
MAE train: 0.452522	val: 1.536134	test: 1.363851

Epoch: 67
Loss: 0.5430624112486839
RMSE train: 0.660838	val: 1.903219	test: 1.933724
MAE train: 0.521288	val: 1.661478	test: 1.570433

Epoch: 68
Loss: 0.5188821628689766
RMSE train: 0.668995	val: 1.946820	test: 1.996450
MAE train: 0.536605	val: 1.689422	test: 1.616370

Epoch: 69
Loss: 0.5264618247747421
RMSE train: 0.715837	val: 1.951462	test: 2.011955
MAE train: 0.561764	val: 1.694442	test: 1.623399

Epoch: 70
Loss: 0.5140017420053482
RMSE train: 0.677651	val: 1.881302	test: 1.968810
MAE train: 0.525340	val: 1.644485	test: 1.593013

Epoch: 71
Loss: 0.504156693816185
RMSE train: 0.557820	val: 1.745228	test: 1.811863
MAE train: 0.442918	val: 1.514271	test: 1.430563

Epoch: 72
Loss: 0.5122586339712143
RMSE train: 0.581088	val: 1.901538	test: 1.918521
MAE train: 0.463194	val: 1.652623	test: 1.539537

Epoch: 73
Loss: 0.5093964859843254
RMSE train: 0.511713	val: 1.712145	test: 1.735779
MAE train: 0.393195	val: 1.501125	test: 1.397338

Epoch: 74
Loss: 0.502961203455925
RMSE train: 0.449219	val: 1.621440	test: 1.638814
MAE train: 0.356926	val: 1.414807	test: 1.264538

Epoch: 75
Loss: 0.508596695959568
RMSE train: 0.451510	val: 1.639151	test: 1.682175
MAE train: 0.360660	val: 1.421218	test: 1.292520

Epoch: 76
Loss: 0.4990486428141594
RMSE train: 0.528461	val: 1.639626	test: 1.687342
MAE train: 0.413947	val: 1.411753	test: 1.334784

Epoch: 77
Loss: 0.48089374601840973
RMSE train: 0.573622	val: 1.745078	test: 1.752299
MAE train: 0.448873	val: 1.494172	test: 1.362995

Epoch: 78
Loss: 0.4127798303961754
RMSE train: 0.592691	val: 1.908198	test: 1.893648
MAE train: 0.465061	val: 1.655375	test: 1.529664

Epoch: 79
Loss: 0.4773188978433609
RMSE train: 0.528445	val: 1.831142	test: 1.817439
MAE train: 0.409809	val: 1.592820	test: 1.459032

Epoch: 80
Loss: 0.44383562356233597
RMSE train: 0.486257	val: 1.700418	test: 1.690158
MAE train: 0.383042	val: 1.470028	test: 1.303148

Epoch: 81
Loss: 0.48872654139995575
RMSE train: 0.568064	val: 1.828406	test: 1.827557
MAE train: 0.436194	val: 1.585005	test: 1.463752

Epoch: 82
Loss: 0.43860650807619095
RMSE train: 0.478478	val: 1.728934	test: 1.740990
MAE train: 0.374268	val: 1.487107	test: 1.342278

Epoch: 83
Loss: 0.3956504240632057
RMSE train: 0.446872	val: 1.725906	test: 1.749135
MAE train: 0.682854	val: 1.113283	test: 1.025416

Epoch: 23
Loss: 1.1988833844661713
RMSE train: 0.900421	val: 1.359001	test: 1.348515
MAE train: 0.695068	val: 1.161626	test: 1.078786

Epoch: 24
Loss: 1.2053316831588745
RMSE train: 0.822134	val: 1.288731	test: 1.280453
MAE train: 0.640871	val: 1.077030	test: 1.012370

Epoch: 25
Loss: 1.0734354108572006
RMSE train: 0.803080	val: 1.334205	test: 1.286368
MAE train: 0.621946	val: 1.109883	test: 1.011101

Epoch: 26
Loss: 1.0646989047527313
RMSE train: 0.836699	val: 1.311163	test: 1.313745
MAE train: 0.647314	val: 1.109532	test: 1.054676

Epoch: 27
Loss: 0.9554036110639572
RMSE train: 0.834498	val: 1.349526	test: 1.343924
MAE train: 0.650017	val: 1.132379	test: 1.066305

Epoch: 28
Loss: 0.9568646252155304
RMSE train: 0.810874	val: 1.294362	test: 1.340747
MAE train: 0.631535	val: 1.067980	test: 1.035078

Epoch: 29
Loss: 0.9844338744878769
RMSE train: 0.795452	val: 1.209786	test: 1.299992
MAE train: 0.619526	val: 0.982209	test: 0.983026

Epoch: 30
Loss: 0.9916032999753952
RMSE train: 0.842981	val: 1.181708	test: 1.291897
MAE train: 0.638328	val: 0.961370	test: 0.985037

Epoch: 31
Loss: 0.9588252454996109
RMSE train: 0.815236	val: 1.335732	test: 1.299803
MAE train: 0.633468	val: 1.099361	test: 1.005586

Epoch: 32
Loss: 0.8952288031578064
RMSE train: 0.749476	val: 1.269381	test: 1.238740
MAE train: 0.583457	val: 1.050181	test: 0.941168

Epoch: 33
Loss: 0.9054909497499466
RMSE train: 0.710872	val: 1.170932	test: 1.232644
MAE train: 0.560325	val: 0.963238	test: 0.951886

Epoch: 34
Loss: 0.7937431335449219
RMSE train: 0.711778	val: 1.273319	test: 1.310597
MAE train: 0.562127	val: 1.052441	test: 1.024015

Epoch: 35
Loss: 0.8258069008588791
RMSE train: 0.775012	val: 1.226889	test: 1.316574
MAE train: 0.607465	val: 0.981469	test: 0.991881

Epoch: 36
Loss: 0.7208645939826965
RMSE train: 0.805926	val: 1.278565	test: 1.349252
MAE train: 0.629326	val: 1.036499	test: 1.024025

Epoch: 37
Loss: 0.7597908228635788
RMSE train: 0.769559	val: 1.295553	test: 1.306086
MAE train: 0.599607	val: 1.060088	test: 1.008980

Epoch: 38
Loss: 0.711571455001831
RMSE train: 0.735938	val: 1.259244	test: 1.281621
MAE train: 0.577384	val: 1.019887	test: 0.980669

Epoch: 39
Loss: 0.7670684903860092
RMSE train: 0.711303	val: 1.183950	test: 1.279107
MAE train: 0.556644	val: 0.946533	test: 0.968122

Epoch: 40
Loss: 0.6764822751283646
RMSE train: 0.679726	val: 1.260631	test: 1.289154
MAE train: 0.530727	val: 0.994218	test: 0.965902

Epoch: 41
Loss: 0.7044614404439926
RMSE train: 0.694670	val: 1.261687	test: 1.296594
MAE train: 0.544566	val: 1.007017	test: 0.962302

Epoch: 42
Loss: 0.7463581562042236
RMSE train: 0.635176	val: 1.187658	test: 1.288240
MAE train: 0.507877	val: 0.968899	test: 0.985920

Epoch: 43
Loss: 0.7178822755813599
RMSE train: 0.700276	val: 1.315121	test: 1.376852
MAE train: 0.556399	val: 1.071535	test: 1.064882

Epoch: 44
Loss: 0.6716967225074768
RMSE train: 0.788340	val: 1.342831	test: 1.400009
MAE train: 0.623491	val: 1.069270	test: 1.054450

Epoch: 45
Loss: 0.6444921493530273
RMSE train: 0.583920	val: 1.143357	test: 1.248897
MAE train: 0.463418	val: 0.956430	test: 0.979826

Epoch: 46
Loss: 0.6813759058713913
RMSE train: 0.566993	val: 1.138135	test: 1.219392
MAE train: 0.448585	val: 0.945133	test: 0.976967

Epoch: 47
Loss: 0.6807431727647781
RMSE train: 0.594669	val: 1.069609	test: 1.158866
MAE train: 0.472954	val: 0.888107	test: 0.896371

Epoch: 48
Loss: 0.6246014982461929
RMSE train: 0.650560	val: 1.096262	test: 1.191941
MAE train: 0.513115	val: 0.896824	test: 0.893696

Epoch: 49
Loss: 0.5921623259782791
RMSE train: 0.602920	val: 1.042327	test: 1.166818
MAE train: 0.475859	val: 0.859207	test: 0.903193

Epoch: 50
Loss: 0.5801003277301788
RMSE train: 0.658946	val: 1.056996	test: 1.212105
MAE train: 0.522106	val: 0.863588	test: 0.943384

Epoch: 51
Loss: 0.5698050707578659
RMSE train: 0.720600	val: 1.211387	test: 1.298810
MAE train: 0.570631	val: 0.968624	test: 0.982855

Epoch: 52
Loss: 0.5842598676681519
RMSE train: 0.693991	val: 1.266201	test: 1.313567
MAE train: 0.548840	val: 1.017054	test: 0.985609

Epoch: 53
Loss: 0.5983170419931412
RMSE train: 0.675090	val: 1.219563	test: 1.305301
MAE train: 0.528806	val: 1.002833	test: 0.989481

Epoch: 54
Loss: 0.5642235726118088
RMSE train: 0.623123	val: 1.274756	test: 1.301159
MAE train: 0.490726	val: 1.056867	test: 1.007178

Epoch: 55
Loss: 0.6121333688497543
RMSE train: 0.624818	val: 1.172353	test: 1.265139
MAE train: 0.493906	val: 0.967499	test: 0.962143

Epoch: 56
Loss: 0.5199849531054497
RMSE train: 0.595400	val: 1.175943	test: 1.244812
MAE train: 0.465131	val: 0.972489	test: 0.940192

Epoch: 57
Loss: 0.5740252286195755
RMSE train: 0.562486	val: 1.264176	test: 1.291484
MAE train: 0.439012	val: 1.071635	test: 1.035980

Epoch: 58
Loss: 0.5454551130533218
RMSE train: 0.630529	val: 1.149012	test: 1.262589
MAE train: 0.495577	val: 0.986649	test: 0.964735

Epoch: 59
Loss: 0.5315780788660049
RMSE train: 0.638405	val: 1.264974	test: 1.307007
MAE train: 0.506123	val: 1.035105	test: 0.979582

Epoch: 60
Loss: 0.6119193136692047
RMSE train: 0.746856	val: 1.376828	test: 1.408043
MAE train: 0.596227	val: 1.085029	test: 1.035565

Epoch: 61
Loss: 0.5459382981061935
RMSE train: 0.703134	val: 1.160862	test: 1.342624
MAE train: 0.545869	val: 0.939280	test: 1.016939

Epoch: 62
Loss: 0.5745758190751076
RMSE train: 0.598319	val: 1.305789	test: 1.340435
MAE train: 0.477676	val: 1.078209	test: 1.086179

Epoch: 63
Loss: 0.5612562596797943
RMSE train: 0.616233	val: 1.205326	test: 1.281686
MAE train: 0.482848	val: 0.984569	test: 0.950547

Epoch: 64
Loss: 0.5122161135077477
RMSE train: 0.580181	val: 1.113105	test: 1.238514
MAE train: 0.452964	val: 0.940187	test: 0.953375

Epoch: 65
Loss: 0.46632616221904755
RMSE train: 0.540653	val: 1.284957	test: 1.303466
MAE train: 0.430743	val: 1.079424	test: 1.061367

Epoch: 66
Loss: 0.4961961433291435
RMSE train: 0.578639	val: 1.227507	test: 1.294386
MAE train: 0.456804	val: 1.015005	test: 1.014137

Epoch: 67
Loss: 0.47561075538396835
RMSE train: 0.588495	val: 1.172676	test: 1.286512
MAE train: 0.463537	val: 0.960699	test: 0.996063

Epoch: 68
Loss: 0.4444407597184181
RMSE train: 0.523517	val: 1.144698	test: 1.268351
MAE train: 0.412771	val: 0.955971	test: 1.005882

Epoch: 69
Loss: 0.49442108720541
RMSE train: 0.543633	val: 1.091221	test: 1.228428
MAE train: 0.426796	val: 0.893598	test: 0.935355

Epoch: 70
Loss: 0.44671180844306946
RMSE train: 0.607349	val: 1.241925	test: 1.307860
MAE train: 0.478261	val: 0.994332	test: 0.965696

Epoch: 71
Loss: 0.48578988015651703
RMSE train: 0.576049	val: 1.172929	test: 1.281827
MAE train: 0.459187	val: 0.943994	test: 0.967621

Epoch: 72
Loss: 0.4484007656574249
RMSE train: 0.615391	val: 1.219953	test: 1.351765
MAE train: 0.487191	val: 1.002996	test: 1.013125

Epoch: 73
Loss: 0.4456423670053482
RMSE train: 0.607227	val: 1.299369	test: 1.367843
MAE train: 0.473115	val: 1.041315	test: 1.051130

Epoch: 74
Loss: 0.44290971755981445
RMSE train: 0.548727	val: 1.151546	test: 1.254905
MAE train: 0.436866	val: 0.942052	test: 0.960181

Epoch: 75
Loss: 0.46473728865385056
RMSE train: 0.567333	val: 1.083283	test: 1.238285
MAE train: 0.440848	val: 0.903382	test: 0.950219

Epoch: 76
Loss: 0.4430759847164154
RMSE train: 0.535591	val: 1.237066	test: 1.261056
MAE train: 0.414903	val: 1.000280	test: 0.966773

Epoch: 77
Loss: 0.4054402932524681
RMSE train: 0.547030	val: 1.211291	test: 1.279439
MAE train: 0.427979	val: 0.979463	test: 0.954918

Epoch: 78
Loss: 0.37760601937770844
RMSE train: 0.527620	val: 1.179183	test: 1.230633
MAE train: 0.416506	val: 0.971364	test: 0.929192

Epoch: 79
Loss: 0.39737875759601593
RMSE train: 0.555435	val: 1.176628	test: 1.267793
MAE train: 0.435897	val: 0.959540	test: 0.955116

Epoch: 80
Loss: 0.4672832041978836
RMSE train: 0.571543	val: 1.201530	test: 1.322594
MAE train: 0.448902	val: 0.967361	test: 0.981563

Epoch: 81
Loss: 0.4324556738138199
RMSE train: 0.577979	val: 1.200342	test: 1.342316
MAE train: 0.448285	val: 0.992287	test: 0.999721

Epoch: 82
Loss: 0.43409552425146103
RMSE train: 0.583273	val: 1.237967	test: 1.308729
MAE train: 0.457572	val: 0.997173	test: 1.006177

Epoch: 83
Loss: 0.37867452949285507

MAE train: 0.767998	val: 1.258030	test: 1.141535

Epoch: 23
Loss: 1.4640499949455261
RMSE train: 0.958780	val: 1.408668	test: 1.415682
MAE train: 0.730117	val: 1.134482	test: 1.090755

Epoch: 24
Loss: 1.389317125082016
RMSE train: 0.908973	val: 1.442651	test: 1.446021
MAE train: 0.694628	val: 1.136484	test: 1.130065

Epoch: 25
Loss: 1.2690317034721375
RMSE train: 0.882224	val: 1.467941	test: 1.461787
MAE train: 0.680324	val: 1.131809	test: 1.153334

Epoch: 26
Loss: 1.1367120146751404
RMSE train: 0.855854	val: 1.526920	test: 1.541755
MAE train: 0.657609	val: 1.175180	test: 1.227132

Epoch: 27
Loss: 1.1592732965946198
RMSE train: 0.915583	val: 1.473787	test: 1.573155
MAE train: 0.700712	val: 1.163386	test: 1.236004

Epoch: 28
Loss: 1.1116853505373
RMSE train: 0.818484	val: 1.371170	test: 1.500175
MAE train: 0.631370	val: 1.103098	test: 1.187659

Epoch: 29
Loss: 1.119589477777481
RMSE train: 0.793453	val: 1.425686	test: 1.470361
MAE train: 0.611434	val: 1.141149	test: 1.206674

Epoch: 30
Loss: 1.1741407960653305
RMSE train: 0.799801	val: 1.398426	test: 1.433969
MAE train: 0.618754	val: 1.118835	test: 1.147261

Epoch: 31
Loss: 1.030636727809906
RMSE train: 0.766659	val: 1.404572	test: 1.435058
MAE train: 0.591941	val: 1.127553	test: 1.134561

Epoch: 32
Loss: 0.9648428559303284
RMSE train: 0.781267	val: 1.435581	test: 1.438612
MAE train: 0.600235	val: 1.146136	test: 1.152384

Epoch: 33
Loss: 0.9608279317617416
RMSE train: 0.734987	val: 1.452678	test: 1.469679
MAE train: 0.566736	val: 1.170652	test: 1.188534

Epoch: 34
Loss: 0.9769735038280487
RMSE train: 0.760403	val: 1.486605	test: 1.510870
MAE train: 0.587295	val: 1.188932	test: 1.204300

Epoch: 35
Loss: 0.9867367744445801
RMSE train: 0.778636	val: 1.488676	test: 1.504339
MAE train: 0.605905	val: 1.201307	test: 1.201544

Epoch: 36
Loss: 0.7977035939693451
RMSE train: 0.755393	val: 1.376711	test: 1.427010
MAE train: 0.584043	val: 1.110811	test: 1.134105

Epoch: 37
Loss: 0.945515587925911
RMSE train: 0.798722	val: 1.367399	test: 1.452599
MAE train: 0.613956	val: 1.094657	test: 1.126006

Epoch: 38
Loss: 0.8772192001342773
RMSE train: 0.837637	val: 1.396332	test: 1.498217
MAE train: 0.647989	val: 1.119859	test: 1.148020

Epoch: 39
Loss: 0.8042125999927521
RMSE train: 0.783354	val: 1.392037	test: 1.471833
MAE train: 0.607606	val: 1.073832	test: 1.161057

Epoch: 40
Loss: 0.910211831331253
RMSE train: 0.766633	val: 1.380400	test: 1.478950
MAE train: 0.592712	val: 1.054768	test: 1.172015

Epoch: 41
Loss: 0.8344952315092087
RMSE train: 0.845678	val: 1.448863	test: 1.556746
MAE train: 0.655199	val: 1.124800	test: 1.179608

Epoch: 42
Loss: 0.7908822745084763
RMSE train: 0.821718	val: 1.468627	test: 1.545260
MAE train: 0.633520	val: 1.127218	test: 1.189416

Epoch: 43
Loss: 0.827732041478157
RMSE train: 0.724122	val: 1.345854	test: 1.435253
MAE train: 0.557890	val: 1.083918	test: 1.089701

Epoch: 44
Loss: 0.7393507063388824
RMSE train: 0.650403	val: 1.197686	test: 1.324247
MAE train: 0.505116	val: 0.992132	test: 1.027666

Epoch: 45
Loss: 0.7382554262876511
RMSE train: 0.708531	val: 1.314913	test: 1.418210
MAE train: 0.548117	val: 1.063626	test: 1.084798

Epoch: 46
Loss: 0.7298731356859207
RMSE train: 0.707379	val: 1.332723	test: 1.423648
MAE train: 0.550169	val: 1.067636	test: 1.107480

Epoch: 47
Loss: 0.6184794455766678
RMSE train: 0.641261	val: 1.217173	test: 1.363845
MAE train: 0.492802	val: 0.997810	test: 1.074139

Epoch: 48
Loss: 0.6512873470783234
RMSE train: 0.747035	val: 1.399689	test: 1.432633
MAE train: 0.583946	val: 1.120514	test: 1.123004

Epoch: 49
Loss: 0.7029148638248444
RMSE train: 0.717973	val: 1.342312	test: 1.420440
MAE train: 0.555701	val: 1.101110	test: 1.109951

Epoch: 50
Loss: 0.7082379162311554
RMSE train: 0.637880	val: 1.252914	test: 1.416519
MAE train: 0.493552	val: 1.036283	test: 1.117314

Epoch: 51
Loss: 0.6895837634801865
RMSE train: 0.645007	val: 1.311332	test: 1.434449
MAE train: 0.506269	val: 1.066370	test: 1.123395

Epoch: 52
Loss: 0.5958213359117508
RMSE train: 0.637368	val: 1.311838	test: 1.401079
MAE train: 0.501425	val: 1.045427	test: 1.108745

Epoch: 53
Loss: 0.6744093149900436
RMSE train: 0.643833	val: 1.217755	test: 1.324481
MAE train: 0.503430	val: 0.985282	test: 1.043550

Epoch: 54
Loss: 0.6248715817928314
RMSE train: 0.678078	val: 1.256687	test: 1.341358
MAE train: 0.531716	val: 1.001505	test: 1.025328

Epoch: 55
Loss: 0.5835639834403992
RMSE train: 0.703361	val: 1.367827	test: 1.411971
MAE train: 0.552667	val: 1.066821	test: 1.088445

Epoch: 56
Loss: 0.6364687979221344
RMSE train: 0.676015	val: 1.269748	test: 1.416330
MAE train: 0.535524	val: 1.025280	test: 1.078510

Epoch: 57
Loss: 0.607396200299263
RMSE train: 0.651869	val: 1.273973	test: 1.403248
MAE train: 0.515003	val: 1.030322	test: 1.084572

Epoch: 58
Loss: 0.5814686119556427
RMSE train: 0.704807	val: 1.306478	test: 1.439394
MAE train: 0.551911	val: 1.072068	test: 1.128944

Epoch: 59
Loss: 0.5899407863616943
RMSE train: 0.582332	val: 1.252665	test: 1.414139
MAE train: 0.454677	val: 1.036872	test: 1.126567

Epoch: 60
Loss: 0.4868745282292366
RMSE train: 0.541496	val: 1.351660	test: 1.484204
MAE train: 0.424384	val: 1.095345	test: 1.194158

Epoch: 61
Loss: 0.5778559297323227
RMSE train: 0.635003	val: 1.457847	test: 1.562310
MAE train: 0.494888	val: 1.186340	test: 1.200881

Epoch: 62
Loss: 0.5894891619682312
RMSE train: 0.643651	val: 1.332004	test: 1.458762
MAE train: 0.502244	val: 1.086443	test: 1.125626

Epoch: 63
Loss: 0.5289772003889084
RMSE train: 0.546025	val: 1.211918	test: 1.328227
MAE train: 0.422337	val: 0.990788	test: 1.046872

Epoch: 64
Loss: 0.5130639672279358
RMSE train: 0.623589	val: 1.310232	test: 1.406386
MAE train: 0.477463	val: 1.064135	test: 1.081701

Epoch: 65
Loss: 0.5640135109424591
RMSE train: 0.603732	val: 1.373335	test: 1.415063
MAE train: 0.468864	val: 1.089060	test: 1.114340

Epoch: 66
Loss: 0.5168224051594734
RMSE train: 0.627883	val: 1.309583	test: 1.390341
MAE train: 0.488614	val: 1.057857	test: 1.087930

Epoch: 67
Loss: 0.5784886553883553
RMSE train: 0.750216	val: 1.472769	test: 1.551802
MAE train: 0.593503	val: 1.187531	test: 1.175079

Epoch: 68
Loss: 0.5241561383008957
RMSE train: 0.741192	val: 1.459762	test: 1.533617
MAE train: 0.587560	val: 1.156447	test: 1.162360

Epoch: 69
Loss: 0.468441978096962
RMSE train: 0.656222	val: 1.391099	test: 1.484324
MAE train: 0.516861	val: 1.113889	test: 1.132247

Epoch: 70
Loss: 0.4599504694342613
RMSE train: 0.637100	val: 1.383373	test: 1.493200
MAE train: 0.494096	val: 1.136056	test: 1.132547

Epoch: 71
Loss: 0.44113682210445404
RMSE train: 0.534722	val: 1.367058	test: 1.441405
MAE train: 0.418645	val: 1.093240	test: 1.112144

Epoch: 72
Loss: 0.48724180459976196
RMSE train: 0.543819	val: 1.301196	test: 1.385487
MAE train: 0.424690	val: 1.045640	test: 1.087327

Epoch: 73
Loss: 0.4236787110567093
RMSE train: 0.530489	val: 1.220275	test: 1.338338
MAE train: 0.412277	val: 0.985859	test: 1.066282

Epoch: 74
Loss: 0.4938473775982857
RMSE train: 0.545659	val: 1.198744	test: 1.374125
MAE train: 0.429307	val: 0.989224	test: 1.097771

Epoch: 75
Loss: 0.45902062952518463
RMSE train: 0.527575	val: 1.210610	test: 1.466905
MAE train: 0.417684	val: 1.032550	test: 1.135937

Epoch: 76
Loss: 0.46410083025693893
RMSE train: 0.529079	val: 1.258226	test: 1.400143
MAE train: 0.415737	val: 1.042744	test: 1.105760

Epoch: 77
Loss: 0.40975361317396164
RMSE train: 0.528583	val: 1.382565	test: 1.452324
MAE train: 0.415195	val: 1.110987	test: 1.142500

Epoch: 78
Loss: 0.43308669328689575
RMSE train: 0.567819	val: 1.209609	test: 1.366703
MAE train: 0.435880	val: 1.006178	test: 1.076724

Epoch: 79
Loss: 0.42372001707553864
RMSE train: 0.598803	val: 1.300992	test: 1.426143
MAE train: 0.466518	val: 1.069523	test: 1.106774

Epoch: 80
Loss: 0.4032807722687721
RMSE train: 0.536767	val: 1.291925	test: 1.384122
MAE train: 0.422242	val: 1.057991	test: 1.082993

Epoch: 81
Loss: 0.4035670608282089
RMSE train: 0.572565	val: 1.236337	test: 1.420438
MAE train: 0.437992	val: 1.021999	test: 1.101609

Epoch: 82
Loss: 0.36312398314476013
RMSE train: 0.625926	val: 1.332986	test: 1.461145
MAE train: 0.493922	val: 1.069053	test: 1.115694

Epoch: 83
Loss: 0.43004802614450455
RMSE train: 0.579598	val: 1.319962	test: 1.435233
MAE train: 0.732719	val: 1.214052	test: 1.151081

Epoch: 23
Loss: 1.2793620228767395
RMSE train: 0.913023	val: 1.516384	test: 1.428411
MAE train: 0.698303	val: 1.221307	test: 1.141087

Epoch: 24
Loss: 1.3261695504188538
RMSE train: 0.861458	val: 1.529078	test: 1.468838
MAE train: 0.660560	val: 1.208815	test: 1.179378

Epoch: 25
Loss: 1.1854868233203888
RMSE train: 0.855640	val: 1.447996	test: 1.464970
MAE train: 0.663595	val: 1.122233	test: 1.204318

Epoch: 26
Loss: 1.1359856724739075
RMSE train: 0.837424	val: 1.463425	test: 1.431488
MAE train: 0.651417	val: 1.167162	test: 1.201429

Epoch: 27
Loss: 1.1139827966690063
RMSE train: 0.810972	val: 1.457557	test: 1.358357
MAE train: 0.630494	val: 1.191862	test: 1.121724

Epoch: 28
Loss: 1.0146240144968033
RMSE train: 0.870195	val: 1.476852	test: 1.355047
MAE train: 0.686395	val: 1.166020	test: 1.126662

Epoch: 29
Loss: 1.0182050168514252
RMSE train: 0.813957	val: 1.342427	test: 1.270011
MAE train: 0.642413	val: 1.044836	test: 1.064644

Epoch: 30
Loss: 0.9280150234699249
RMSE train: 0.772776	val: 1.348357	test: 1.250995
MAE train: 0.594800	val: 1.057870	test: 1.031673

Epoch: 31
Loss: 0.9830605238676071
RMSE train: 0.778312	val: 1.249921	test: 1.227515
MAE train: 0.611448	val: 0.998687	test: 0.991968

Epoch: 32
Loss: 0.9007949084043503
RMSE train: 0.764774	val: 1.367384	test: 1.352110
MAE train: 0.587221	val: 1.111160	test: 1.068341

Epoch: 33
Loss: 0.8529528826475143
RMSE train: 0.760340	val: 1.423550	test: 1.399450
MAE train: 0.588995	val: 1.134035	test: 1.099360

Epoch: 34
Loss: 0.8905265033245087
RMSE train: 0.709408	val: 1.304483	test: 1.286786
MAE train: 0.555798	val: 1.050492	test: 1.031113

Epoch: 35
Loss: 0.8540981709957123
RMSE train: 0.690919	val: 1.257365	test: 1.255997
MAE train: 0.541712	val: 1.004874	test: 1.015246

Epoch: 36
Loss: 0.7817985415458679
RMSE train: 0.655373	val: 1.265365	test: 1.252998
MAE train: 0.509521	val: 1.006556	test: 1.010861

Epoch: 37
Loss: 0.7606126666069031
RMSE train: 0.674234	val: 1.314370	test: 1.310094
MAE train: 0.520952	val: 1.080525	test: 1.042046

Epoch: 38
Loss: 0.8645653575658798
RMSE train: 0.698956	val: 1.364194	test: 1.365330
MAE train: 0.539954	val: 1.127821	test: 1.104926

Epoch: 39
Loss: 0.7023495882749557
RMSE train: 0.671196	val: 1.414488	test: 1.408691
MAE train: 0.534335	val: 1.137175	test: 1.135026

Epoch: 40
Loss: 0.7117087692022324
RMSE train: 0.686049	val: 1.349932	test: 1.366137
MAE train: 0.531722	val: 1.130896	test: 1.094830

Epoch: 41
Loss: 0.7516587525606155
RMSE train: 0.672431	val: 1.296082	test: 1.325266
MAE train: 0.522069	val: 1.085427	test: 1.055617

Epoch: 42
Loss: 0.8004667609930038
RMSE train: 0.639034	val: 1.401084	test: 1.406118
MAE train: 0.497674	val: 1.147987	test: 1.085377

Epoch: 43
Loss: 0.7395356744527817
RMSE train: 0.720663	val: 1.479490	test: 1.421400
MAE train: 0.542193	val: 1.228459	test: 1.120872

Epoch: 44
Loss: 0.6698518693447113
RMSE train: 0.652866	val: 1.285712	test: 1.271580
MAE train: 0.510086	val: 1.077841	test: 1.014391

Epoch: 45
Loss: 0.6911081671714783
RMSE train: 0.635377	val: 1.286163	test: 1.266670
MAE train: 0.491016	val: 1.045522	test: 0.988619

Epoch: 46
Loss: 0.6971990615129471
RMSE train: 0.612683	val: 1.297134	test: 1.268854
MAE train: 0.472206	val: 1.044943	test: 0.990507

Epoch: 47
Loss: 0.6233499497175217
RMSE train: 0.595585	val: 1.211378	test: 1.204153
MAE train: 0.466900	val: 0.978893	test: 0.968588

Epoch: 48
Loss: 0.61021688580513
RMSE train: 0.640541	val: 1.382138	test: 1.332692
MAE train: 0.499704	val: 1.121130	test: 1.058732

Epoch: 49
Loss: 0.6286419481039047
RMSE train: 0.625748	val: 1.333401	test: 1.315302
MAE train: 0.488330	val: 1.097438	test: 1.038745

Epoch: 50
Loss: 0.6728128641843796
RMSE train: 0.620411	val: 1.292790	test: 1.311808
MAE train: 0.484665	val: 1.065005	test: 1.060297

Epoch: 51
Loss: 0.6292784214019775
RMSE train: 0.584157	val: 1.334212	test: 1.367420
MAE train: 0.458472	val: 1.086150	test: 1.101282

Epoch: 52
Loss: 0.6021017581224442
RMSE train: 0.564796	val: 1.366805	test: 1.358875
MAE train: 0.440715	val: 1.127958	test: 1.073329

Epoch: 53
Loss: 0.614787146449089
RMSE train: 0.606340	val: 1.388968	test: 1.374469
MAE train: 0.468882	val: 1.157329	test: 1.072548

Epoch: 54
Loss: 0.5719058662652969
RMSE train: 0.565934	val: 1.329713	test: 1.323630
MAE train: 0.440747	val: 1.086574	test: 1.025388

Epoch: 55
Loss: 0.5484209507703781
RMSE train: 0.585798	val: 1.387268	test: 1.380037
MAE train: 0.458627	val: 1.125292	test: 1.061129

Epoch: 56
Loss: 0.5376359671354294
RMSE train: 0.601342	val: 1.294236	test: 1.323350
MAE train: 0.462051	val: 1.082258	test: 1.027854

Epoch: 57
Loss: 0.5356336236000061
RMSE train: 0.580490	val: 1.321043	test: 1.332518
MAE train: 0.450018	val: 1.074942	test: 1.026278

Epoch: 58
Loss: 0.5649215430021286
RMSE train: 0.633063	val: 1.350242	test: 1.361217
MAE train: 0.485493	val: 1.097403	test: 1.043022

Epoch: 59
Loss: 0.506513237953186
RMSE train: 0.585723	val: 1.300160	test: 1.347534
MAE train: 0.451597	val: 1.077178	test: 1.037093

Epoch: 60
Loss: 0.5488961264491081
RMSE train: 0.531730	val: 1.374818	test: 1.389695
MAE train: 0.416696	val: 1.112821	test: 1.104460

Epoch: 61
Loss: 0.5478988587856293
RMSE train: 0.623957	val: 1.441508	test: 1.490053
MAE train: 0.479827	val: 1.186627	test: 1.139824

Epoch: 62
Loss: 0.5265141725540161
RMSE train: 0.548748	val: 1.319812	test: 1.347021
MAE train: 0.428137	val: 1.056266	test: 1.057317

Epoch: 63
Loss: 0.5456709861755371
RMSE train: 0.515299	val: 1.288413	test: 1.277994
MAE train: 0.406414	val: 1.036747	test: 1.029550

Epoch: 64
Loss: 0.5072829648852348
RMSE train: 0.540109	val: 1.302488	test: 1.321775
MAE train: 0.420683	val: 1.082041	test: 1.034619

Epoch: 65
Loss: 0.48109685629606247
RMSE train: 0.527628	val: 1.346488	test: 1.332701
MAE train: 0.413930	val: 1.093884	test: 1.060691

Epoch: 66
Loss: 0.47407373785972595
RMSE train: 0.545220	val: 1.256705	test: 1.289982
MAE train: 0.431605	val: 1.046983	test: 1.039080

Epoch: 67
Loss: 0.526458777487278
RMSE train: 0.518382	val: 1.294670	test: 1.325740
MAE train: 0.399575	val: 1.076588	test: 1.033636

Epoch: 68
Loss: 0.44493602216243744
RMSE train: 0.545540	val: 1.353190	test: 1.383872
MAE train: 0.424015	val: 1.095057	test: 1.054869

Epoch: 69
Loss: 0.4629068300127983
RMSE train: 0.554856	val: 1.298650	test: 1.353009
MAE train: 0.430547	val: 1.053836	test: 1.030229

Epoch: 70
Loss: 0.45036766678094864
RMSE train: 0.590504	val: 1.304064	test: 1.384124
MAE train: 0.451221	val: 1.076790	test: 1.065837

Epoch: 71
Loss: 0.4642247334122658
RMSE train: 0.467248	val: 1.245510	test: 1.341911
MAE train: 0.368357	val: 1.016474	test: 1.067988

Epoch: 72
Loss: 0.45567286014556885
RMSE train: 0.496502	val: 1.267843	test: 1.352259
MAE train: 0.384412	val: 1.041422	test: 1.074454

Epoch: 73
Loss: 0.3974079191684723
RMSE train: 0.515193	val: 1.226443	test: 1.320484
MAE train: 0.397608	val: 1.024268	test: 1.061180

Epoch: 74
Loss: 0.47346973419189453
RMSE train: 0.467912	val: 1.236907	test: 1.307320
MAE train: 0.368921	val: 1.016080	test: 1.041251

Epoch: 75
Loss: 0.4445361942052841
RMSE train: 0.476656	val: 1.086894	test: 1.214030
MAE train: 0.384501	val: 0.912416	test: 0.982031

Epoch: 76
Loss: 0.40458596497774124
RMSE train: 0.474498	val: 1.158731	test: 1.250141
MAE train: 0.383048	val: 0.944871	test: 1.026798

Epoch: 77
Loss: 0.4332744628190994
RMSE train: 0.495244	val: 1.255432	test: 1.304974
MAE train: 0.387021	val: 1.025579	test: 1.054301

Epoch: 78
Loss: 0.36881517618894577
RMSE train: 0.469467	val: 1.192930	test: 1.251030
MAE train: 0.370138	val: 1.006467	test: 1.000115

Epoch: 79
Loss: 0.38428356498479843
RMSE train: 0.463916	val: 1.242045	test: 1.286526
MAE train: 0.362273	val: 1.041867	test: 1.023426

Epoch: 80
Loss: 0.3603349030017853
RMSE train: 0.560192	val: 1.395211	test: 1.408548
MAE train: 0.427901	val: 1.132259	test: 1.083778

Epoch: 81
Loss: 0.4055272489786148
RMSE train: 0.521651	val: 1.269555	test: 1.334524
MAE train: 0.402762	val: 1.053432	test: 1.022366

Epoch: 82
Loss: 0.3638981953263283
RMSE train: 0.633070	val: 1.437714	test: 1.506008
MAE train: 0.483419	val: 1.191416	test: 1.148447

Epoch: 83
Loss: 0.41383780539035797
RMSE train: 0.555440	val: 1.366420	test: 1.411750

Epoch: 84
Loss: 0.3997955992817879
RMSE train: 0.626467	val: 1.341869	test: 1.274123
MAE train: 0.474539	val: 1.089171	test: 0.969482

Epoch: 85
Loss: 0.44989771395921707
RMSE train: 0.504811	val: 1.178613	test: 1.121553
MAE train: 0.376681	val: 0.965570	test: 0.874907

Epoch: 86
Loss: 0.3899684175848961
RMSE train: 0.493042	val: 1.181061	test: 1.121840
MAE train: 0.367512	val: 0.961282	test: 0.881429

Epoch: 87
Loss: 0.3608274608850479
RMSE train: 0.520112	val: 1.189583	test: 1.171878
MAE train: 0.394791	val: 0.979642	test: 0.895940

Epoch: 88
Loss: 0.4361044690012932
RMSE train: 0.523764	val: 1.234790	test: 1.162899
MAE train: 0.400448	val: 1.010549	test: 0.893586

Epoch: 89
Loss: 0.3897855579853058
RMSE train: 0.590278	val: 1.237006	test: 1.198761
MAE train: 0.438931	val: 1.023828	test: 0.920275

Epoch: 90
Loss: 0.41621729731559753
RMSE train: 0.585900	val: 1.265282	test: 1.236725
MAE train: 0.432604	val: 1.031478	test: 0.943054

Epoch: 91
Loss: 0.39227496087551117
RMSE train: 0.575432	val: 1.267798	test: 1.246713
MAE train: 0.423123	val: 1.023589	test: 0.939242

Epoch: 92
Loss: 0.34663327783346176
RMSE train: 0.544697	val: 1.131787	test: 1.177266
MAE train: 0.405622	val: 0.950166	test: 0.895122

Epoch: 93
Loss: 0.3845159485936165
RMSE train: 0.526215	val: 1.196224	test: 1.159230
MAE train: 0.400803	val: 0.996494	test: 0.901780

Epoch: 94
Loss: 0.3677109628915787
RMSE train: 0.551944	val: 1.228768	test: 1.221589
MAE train: 0.418214	val: 1.026695	test: 0.940836

Epoch: 95
Loss: 0.3453769236803055
RMSE train: 0.451552	val: 1.181783	test: 1.132053
MAE train: 0.335664	val: 0.976682	test: 0.892398

Epoch: 96
Loss: 0.3876868933439255
RMSE train: 0.506205	val: 1.177843	test: 1.157086
MAE train: 0.381720	val: 0.974486	test: 0.901872

Epoch: 97
Loss: 0.37024103850126266
RMSE train: 0.492741	val: 1.208016	test: 1.160105
MAE train: 0.374839	val: 0.994619	test: 0.902980

Epoch: 98
Loss: 0.3808661624789238
RMSE train: 0.452252	val: 1.191125	test: 1.128641
MAE train: 0.340704	val: 0.986336	test: 0.890575

Epoch: 99
Loss: 0.3486398607492447
RMSE train: 0.494700	val: 1.196718	test: 1.155557
MAE train: 0.375111	val: 0.976728	test: 0.906980

Epoch: 100
Loss: 0.3819480612874031
RMSE train: 0.508803	val: 1.323971	test: 1.171737
MAE train: 0.388861	val: 1.077132	test: 0.912330

Epoch: 101
Loss: 0.36067892611026764
RMSE train: 0.505529	val: 1.153426	test: 1.147018
MAE train: 0.375490	val: 0.957477	test: 0.878371

Epoch: 102
Loss: 0.36901170015335083
RMSE train: 0.489562	val: 1.234867	test: 1.151677
MAE train: 0.368999	val: 1.012824	test: 0.896187

Epoch: 103
Loss: 0.37566521018743515
RMSE train: 0.507996	val: 1.191192	test: 1.165986
MAE train: 0.381576	val: 0.978175	test: 0.927289

Epoch: 104
Loss: 0.3885529041290283
RMSE train: 0.511198	val: 1.184565	test: 1.167934
MAE train: 0.379221	val: 0.980789	test: 0.921130

Epoch: 105
Loss: 0.32972896099090576
RMSE train: 0.524451	val: 1.338051	test: 1.209399
MAE train: 0.387490	val: 1.089119	test: 0.921854

Epoch: 106
Loss: 0.3859860524535179
RMSE train: 0.589870	val: 1.330925	test: 1.262685
MAE train: 0.433022	val: 1.069633	test: 0.944523

Epoch: 107
Loss: 0.3932420164346695
RMSE train: 0.550988	val: 1.174795	test: 1.221384
MAE train: 0.400507	val: 0.974938	test: 0.914554

Epoch: 108
Loss: 0.34119613468647003
RMSE train: 0.587866	val: 1.412659	test: 1.291668
MAE train: 0.441180	val: 1.144831	test: 1.005322

Epoch: 109
Loss: 0.39324451982975006
RMSE train: 0.516306	val: 1.159959	test: 1.219914
MAE train: 0.380538	val: 0.966392	test: 0.941544

Epoch: 110
Loss: 0.34860292822122574
RMSE train: 0.484183	val: 1.134850	test: 1.177615
MAE train: 0.361099	val: 0.946095	test: 0.915284

Epoch: 111
Loss: 0.3390129506587982
RMSE train: 0.492654	val: 1.229600	test: 1.186638
MAE train: 0.378995	val: 1.010164	test: 0.923484

Epoch: 112
Loss: 0.29301074892282486
RMSE train: 0.495233	val: 1.157368	test: 1.169432
MAE train: 0.373369	val: 0.972866	test: 0.908290

Epoch: 113
Loss: 0.3340677469968796
RMSE train: 0.474913	val: 1.192501	test: 1.139153
MAE train: 0.359484	val: 0.986300	test: 0.900441

Epoch: 114
Loss: 0.323163166642189
RMSE train: 0.477555	val: 1.172651	test: 1.140391
MAE train: 0.355149	val: 0.952725	test: 0.882320

Epoch: 115
Loss: 0.30770333856344223
RMSE train: 0.473357	val: 1.162547	test: 1.141360
MAE train: 0.358081	val: 0.955102	test: 0.872440

Epoch: 116
Loss: 0.342478409409523
RMSE train: 0.434745	val: 1.178947	test: 1.115745
MAE train: 0.330210	val: 0.971444	test: 0.870380

Epoch: 117
Loss: 0.357643760740757
RMSE train: 0.457670	val: 1.166741	test: 1.137354
MAE train: 0.344532	val: 0.955631	test: 0.901643

Epoch: 118
Loss: 0.33491089940071106
RMSE train: 0.453393	val: 1.190629	test: 1.167520
MAE train: 0.348708	val: 0.978269	test: 0.920758

Epoch: 119
Loss: 0.32127197086811066
RMSE train: 0.452139	val: 1.171464	test: 1.165298
MAE train: 0.339216	val: 0.960735	test: 0.906131

Epoch: 120
Loss: 0.29995741695165634
RMSE train: 0.463514	val: 1.109339	test: 1.136354
MAE train: 0.346305	val: 0.906107	test: 0.886045

Epoch: 121
Loss: 0.32799897342920303
RMSE train: 0.469097	val: 1.248846	test: 1.143413
MAE train: 0.367218	val: 1.029847	test: 0.875082

Early stopping
Best (RMSE):	 train: 0.517802	val: 1.019174	test: 1.076059
Best (MAE):	 train: 0.385798	val: 0.846454	test: 0.844226

MAE train: 0.454347	val: 1.334911	test: 1.136295

Epoch: 84
Loss: 0.4128454849123955
RMSE train: 0.544195	val: 1.710286	test: 1.607649
MAE train: 0.424172	val: 1.445607	test: 1.238876

Epoch: 85
Loss: 0.3930474817752838
RMSE train: 0.448938	val: 1.586707	test: 1.490074
MAE train: 0.349033	val: 1.312087	test: 1.106007

Epoch: 86
Loss: 0.4177651107311249
RMSE train: 0.473700	val: 1.502314	test: 1.399669
MAE train: 0.368109	val: 1.202409	test: 1.066486

Epoch: 87
Loss: 0.40867476910352707
RMSE train: 0.442025	val: 1.439838	test: 1.359808
MAE train: 0.352999	val: 1.169873	test: 1.052751

Epoch: 88
Loss: 0.44911182671785355
RMSE train: 0.461172	val: 1.489585	test: 1.422160
MAE train: 0.361587	val: 1.243536	test: 1.116599

Epoch: 89
Loss: 0.45914750546216965
RMSE train: 0.506376	val: 1.531015	test: 1.452203
MAE train: 0.390192	val: 1.249675	test: 1.094329

Epoch: 90
Loss: 0.38342054188251495
RMSE train: 0.426813	val: 1.401796	test: 1.332795
MAE train: 0.333681	val: 1.119318	test: 1.061265

Epoch: 91
Loss: 0.4132215976715088
RMSE train: 0.538067	val: 1.573053	test: 1.482393
MAE train: 0.410673	val: 1.287370	test: 1.171698

Epoch: 92
Loss: 0.415401928126812
RMSE train: 0.512209	val: 1.574891	test: 1.436513
MAE train: 0.394596	val: 1.224031	test: 1.071643

Epoch: 93
Loss: 0.36379358172416687
RMSE train: 0.431251	val: 1.587299	test: 1.458598
MAE train: 0.335917	val: 1.249552	test: 1.083777

Epoch: 94
Loss: 0.370692178606987
RMSE train: 0.539747	val: 1.772044	test: 1.665822
MAE train: 0.423921	val: 1.458699	test: 1.253566

Epoch: 95
Loss: 0.38671115785837173
RMSE train: 0.518192	val: 1.732945	test: 1.563788
MAE train: 0.409525	val: 1.381420	test: 1.155237

Epoch: 96
Loss: 0.3926362544298172
RMSE train: 0.529288	val: 1.741079	test: 1.628100
MAE train: 0.409786	val: 1.447809	test: 1.231792

Epoch: 97
Loss: 0.394285149872303
RMSE train: 0.508363	val: 1.725720	test: 1.650600
MAE train: 0.386455	val: 1.462157	test: 1.275237

Epoch: 98
Loss: 0.38917363435029984
RMSE train: 0.438629	val: 1.460541	test: 1.412789
MAE train: 0.337793	val: 1.157044	test: 1.088527

Epoch: 99
Loss: 0.3738580569624901
RMSE train: 0.495481	val: 1.568845	test: 1.552152
MAE train: 0.384006	val: 1.301142	test: 1.227584

Epoch: 100
Loss: 0.46205831319093704
RMSE train: 0.489905	val: 1.440756	test: 1.432360
MAE train: 0.386832	val: 1.171197	test: 1.127974

Epoch: 101
Loss: 0.3765040785074234
RMSE train: 0.503754	val: 1.536391	test: 1.461325
MAE train: 0.386751	val: 1.208501	test: 1.141984

Epoch: 102
Loss: 0.34721389412879944
RMSE train: 0.538565	val: 1.779600	test: 1.746074
MAE train: 0.405460	val: 1.477115	test: 1.356809

Epoch: 103
Loss: 0.3842778503894806
RMSE train: 0.443769	val: 1.575186	test: 1.501726
MAE train: 0.343739	val: 1.282928	test: 1.182403

Epoch: 104
Loss: 0.39538343995809555
RMSE train: 0.452175	val: 1.526527	test: 1.391163
MAE train: 0.346501	val: 1.191067	test: 1.057892

Epoch: 105
Loss: 0.364408515393734
RMSE train: 0.424483	val: 1.492649	test: 1.391481
MAE train: 0.328656	val: 1.188795	test: 1.113771

Epoch: 106
Loss: 0.35671114176511765
RMSE train: 0.461500	val: 1.612457	test: 1.466477
MAE train: 0.353248	val: 1.296269	test: 1.143653

Epoch: 107
Loss: 0.3296816423535347
RMSE train: 0.485838	val: 1.573735	test: 1.467701
MAE train: 0.376505	val: 1.244114	test: 1.135383

Epoch: 108
Loss: 0.3237861916422844
RMSE train: 0.441507	val: 1.434596	test: 1.402948
MAE train: 0.353234	val: 1.138240	test: 1.126458

Epoch: 109
Loss: 0.35008157789707184
RMSE train: 0.439555	val: 1.448731	test: 1.381988
MAE train: 0.346575	val: 1.138548	test: 1.085624

Epoch: 110
Loss: 0.3099083676934242
RMSE train: 0.473430	val: 1.528892	test: 1.411976
MAE train: 0.370240	val: 1.218242	test: 1.089091

Epoch: 111
Loss: 0.2984137684106827
RMSE train: 0.451513	val: 1.493142	test: 1.444454
MAE train: 0.347906	val: 1.237883	test: 1.161741

Epoch: 112
Loss: 0.3598339930176735
RMSE train: 0.464376	val: 1.475340	test: 1.416755
MAE train: 0.353518	val: 1.180460	test: 1.091064

Epoch: 113
Loss: 0.31219058856368065
RMSE train: 0.421642	val: 1.537241	test: 1.458263
MAE train: 0.320034	val: 1.235998	test: 1.096162

Epoch: 114
Loss: 0.35159920901060104
RMSE train: 0.486357	val: 1.650920	test: 1.625667
MAE train: 0.365369	val: 1.411024	test: 1.283147

Epoch: 115
Loss: 0.2986399382352829
RMSE train: 0.464662	val: 1.532601	test: 1.523009
MAE train: 0.351528	val: 1.290938	test: 1.200712

Epoch: 116
Loss: 0.27438752353191376
RMSE train: 0.362020	val: 1.384380	test: 1.347735
MAE train: 0.281875	val: 1.105629	test: 1.053496

Epoch: 117
Loss: 0.29836374148726463
RMSE train: 0.409325	val: 1.459593	test: 1.421926
MAE train: 0.319738	val: 1.205772	test: 1.093468

Epoch: 118
Loss: 0.2978864423930645
RMSE train: 0.435411	val: 1.419034	test: 1.406173
MAE train: 0.338643	val: 1.128155	test: 1.077131

Epoch: 119
Loss: 0.2823985889554024
RMSE train: 0.415289	val: 1.332325	test: 1.370515
MAE train: 0.328417	val: 1.056400	test: 1.075996

Epoch: 120
Loss: 0.29105551540851593
RMSE train: 0.448733	val: 1.496921	test: 1.488239
MAE train: 0.346474	val: 1.210920	test: 1.104959

Epoch: 121
Loss: 0.33716288954019547
RMSE train: 0.542002	val: 1.560427	test: 1.563312
MAE train: 0.409306	val: 1.303349	test: 1.160058

Early stopping
Best (RMSE):	 train: 0.520534	val: 1.328807	test: 1.335378
Best (MAE):	 train: 0.409923	val: 1.108799	test: 1.044654

MAE train: 0.434099	val: 1.209472	test: 1.129370

Epoch: 84
Loss: 0.4199489876627922
RMSE train: 0.625750	val: 1.506984	test: 1.480606
MAE train: 0.497012	val: 1.238233	test: 1.166089

Epoch: 85
Loss: 0.3916464298963547
RMSE train: 0.528650	val: 1.496399	test: 1.389484
MAE train: 0.420094	val: 1.216360	test: 1.113274

Epoch: 86
Loss: 0.38036762177944183
RMSE train: 0.526005	val: 1.456629	test: 1.380534
MAE train: 0.412279	val: 1.167830	test: 1.120283

Epoch: 87
Loss: 0.3750833347439766
RMSE train: 0.524049	val: 1.382465	test: 1.360407
MAE train: 0.417004	val: 1.138611	test: 1.095539

Epoch: 88
Loss: 0.40193382650613785
RMSE train: 0.607943	val: 1.474237	test: 1.424127
MAE train: 0.484127	val: 1.205841	test: 1.117373

Epoch: 89
Loss: 0.44846945255994797
RMSE train: 0.544111	val: 1.526611	test: 1.440672
MAE train: 0.424141	val: 1.239650	test: 1.135164

Epoch: 90
Loss: 0.3741510435938835
RMSE train: 0.581170	val: 1.488472	test: 1.472035
MAE train: 0.463568	val: 1.229043	test: 1.150931

Epoch: 91
Loss: 0.3855638653039932
RMSE train: 0.712979	val: 1.618481	test: 1.586838
MAE train: 0.571006	val: 1.335961	test: 1.237159

Epoch: 92
Loss: 0.4045439660549164
RMSE train: 0.486845	val: 1.449691	test: 1.397835
MAE train: 0.390300	val: 1.181358	test: 1.118306

Epoch: 93
Loss: 0.32865097373723984
RMSE train: 0.530130	val: 1.419554	test: 1.447914
MAE train: 0.419506	val: 1.181702	test: 1.142827

Epoch: 94
Loss: 0.3658803924918175
RMSE train: 0.600140	val: 1.530288	test: 1.486226
MAE train: 0.464020	val: 1.229412	test: 1.171793

Epoch: 95
Loss: 0.3465239033102989
RMSE train: 0.460735	val: 1.454077	test: 1.373669
MAE train: 0.363270	val: 1.179601	test: 1.099448

Epoch: 96
Loss: 0.3682689070701599
RMSE train: 0.531518	val: 1.460935	test: 1.465525
MAE train: 0.398606	val: 1.199329	test: 1.146632

Epoch: 97
Loss: 0.37337805330753326
RMSE train: 0.465815	val: 1.417325	test: 1.413650
MAE train: 0.365554	val: 1.176373	test: 1.125587

Epoch: 98
Loss: 0.3653551861643791
RMSE train: 0.524387	val: 1.557227	test: 1.522864
MAE train: 0.407939	val: 1.245842	test: 1.208055

Epoch: 99
Loss: 0.28897665441036224
RMSE train: 0.584234	val: 1.511465	test: 1.533948
MAE train: 0.447398	val: 1.235413	test: 1.199125

Epoch: 100
Loss: 0.47514718025922775
RMSE train: 0.509822	val: 1.403971	test: 1.428741
MAE train: 0.400494	val: 1.163959	test: 1.133396

Epoch: 101
Loss: 0.3357827365398407
RMSE train: 0.539697	val: 1.532422	test: 1.508336
MAE train: 0.418202	val: 1.244765	test: 1.189491

Epoch: 102
Loss: 0.3060494139790535
RMSE train: 0.546343	val: 1.385584	test: 1.448713
MAE train: 0.411183	val: 1.131151	test: 1.125881

Epoch: 103
Loss: 0.35440076142549515
RMSE train: 0.531037	val: 1.420334	test: 1.374489
MAE train: 0.415936	val: 1.162393	test: 1.082110

Epoch: 104
Loss: 0.39039453119039536
RMSE train: 0.482953	val: 1.417334	test: 1.381100
MAE train: 0.376918	val: 1.158158	test: 1.087230

Epoch: 105
Loss: 0.3377026468515396
RMSE train: 0.459916	val: 1.355891	test: 1.367423
MAE train: 0.360331	val: 1.128068	test: 1.080408

Epoch: 106
Loss: 0.3495185747742653
RMSE train: 0.476985	val: 1.493341	test: 1.434429
MAE train: 0.368056	val: 1.206385	test: 1.116770

Epoch: 107
Loss: 0.2920474708080292
RMSE train: 0.501592	val: 1.447938	test: 1.449423
MAE train: 0.397182	val: 1.200016	test: 1.134022

Epoch: 108
Loss: 0.3362491950392723
RMSE train: 0.472414	val: 1.428628	test: 1.407728
MAE train: 0.377572	val: 1.167074	test: 1.103186

Epoch: 109
Loss: 0.346086710691452
RMSE train: 0.463727	val: 1.419854	test: 1.375522
MAE train: 0.362391	val: 1.147034	test: 1.093207

Epoch: 110
Loss: 0.27601369470357895
RMSE train: 0.456383	val: 1.372510	test: 1.391760
MAE train: 0.354156	val: 1.126529	test: 1.113654

Epoch: 111
Loss: 0.2578182481229305
RMSE train: 0.439450	val: 1.390807	test: 1.400031
MAE train: 0.345618	val: 1.144088	test: 1.127240

Epoch: 112
Loss: 0.3517388552427292
RMSE train: 0.470420	val: 1.411745	test: 1.404647
MAE train: 0.371773	val: 1.151917	test: 1.126287

Epoch: 113
Loss: 0.29258668050169945
RMSE train: 0.485810	val: 1.434418	test: 1.410643
MAE train: 0.376632	val: 1.149793	test: 1.137218

Epoch: 114
Loss: 0.310495987534523
RMSE train: 0.563715	val: 1.426758	test: 1.438982
MAE train: 0.423864	val: 1.167524	test: 1.136385

Epoch: 115
Loss: 0.2661699689924717
RMSE train: 0.525780	val: 1.379498	test: 1.434838
MAE train: 0.406141	val: 1.153312	test: 1.130238

Epoch: 116
Loss: 0.2964451313018799
RMSE train: 0.524531	val: 1.445195	test: 1.447757
MAE train: 0.403622	val: 1.182245	test: 1.134132

Epoch: 117
Loss: 0.3296948969364166
RMSE train: 0.482885	val: 1.312838	test: 1.372293
MAE train: 0.374933	val: 1.067892	test: 1.101333

Epoch: 118
Loss: 0.27069902047514915
RMSE train: 0.519757	val: 1.439391	test: 1.441953
MAE train: 0.400433	val: 1.168720	test: 1.144989

Epoch: 119
Loss: 0.28984520584344864
RMSE train: 0.515415	val: 1.462179	test: 1.466831
MAE train: 0.405937	val: 1.213126	test: 1.154547

Epoch: 120
Loss: 0.2634374871850014
RMSE train: 0.507908	val: 1.427754	test: 1.426160
MAE train: 0.393739	val: 1.192964	test: 1.126400

Epoch: 121
Loss: 0.32298918813467026
RMSE train: 0.560966	val: 1.420033	test: 1.433334
MAE train: 0.427596	val: 1.190473	test: 1.125737

Early stopping
Best (RMSE):	 train: 0.636991	val: 1.305292	test: 1.334904
Best (MAE):	 train: 0.500924	val: 1.034837	test: 1.088356

MAE train: 0.403335	val: 1.417357	test: 1.367006

Epoch: 84
Loss: 0.40002071112394333
RMSE train: 0.480309	val: 1.519325	test: 1.649693
MAE train: 0.368679	val: 1.247614	test: 1.252318

Epoch: 85
Loss: 0.42899830639362335
RMSE train: 0.541774	val: 1.659273	test: 1.796877
MAE train: 0.400614	val: 1.373367	test: 1.393836

Epoch: 86
Loss: 0.4181661158800125
RMSE train: 0.530364	val: 1.840772	test: 1.927226
MAE train: 0.403384	val: 1.471867	test: 1.497837

Epoch: 87
Loss: 0.42392198741436005
RMSE train: 0.507260	val: 1.914148	test: 2.012528
MAE train: 0.393529	val: 1.540650	test: 1.577639

Epoch: 88
Loss: 0.40650857239961624
RMSE train: 0.517588	val: 1.841677	test: 1.937612
MAE train: 0.399357	val: 1.490299	test: 1.502919

Epoch: 89
Loss: 0.36449574679136276
RMSE train: 0.457303	val: 1.910141	test: 1.946921
MAE train: 0.359835	val: 1.516571	test: 1.521814

Epoch: 90
Loss: 0.39686399698257446
RMSE train: 0.434936	val: 1.696080	test: 1.765678
MAE train: 0.344455	val: 1.348734	test: 1.357041

Epoch: 91
Loss: 0.4023975431919098
RMSE train: 0.484800	val: 1.756792	test: 1.813867
MAE train: 0.383382	val: 1.404638	test: 1.372177

Epoch: 92
Loss: 0.37801069021224976
RMSE train: 0.532400	val: 1.749927	test: 1.823801
MAE train: 0.417910	val: 1.405438	test: 1.380017

Epoch: 93
Loss: 0.39923448115587234
RMSE train: 0.471935	val: 1.592977	test: 1.688783
MAE train: 0.373382	val: 1.276540	test: 1.280356

Epoch: 94
Loss: 0.33995381742715836
RMSE train: 0.445071	val: 1.762203	test: 1.807885
MAE train: 0.346721	val: 1.385857	test: 1.386227

Epoch: 95
Loss: 0.3716252073645592
RMSE train: 0.502628	val: 1.782648	test: 1.849442
MAE train: 0.392881	val: 1.411154	test: 1.413493

Epoch: 96
Loss: 0.3913123905658722
RMSE train: 0.472900	val: 1.858517	test: 1.928428
MAE train: 0.370809	val: 1.475166	test: 1.477516

Epoch: 97
Loss: 0.3949607387185097
RMSE train: 0.483611	val: 1.972376	test: 2.017180
MAE train: 0.370273	val: 1.588265	test: 1.565510

Epoch: 98
Loss: 0.3622486963868141
RMSE train: 0.485564	val: 1.859968	test: 1.885896
MAE train: 0.372294	val: 1.502962	test: 1.467138

Epoch: 99
Loss: 0.36520833522081375
RMSE train: 0.502893	val: 1.805749	test: 1.864348
MAE train: 0.386970	val: 1.442121	test: 1.445088

Epoch: 100
Loss: 0.35496170818805695
RMSE train: 0.482923	val: 1.747061	test: 1.825117
MAE train: 0.374035	val: 1.410954	test: 1.416633

Epoch: 101
Loss: 0.3790831193327904
RMSE train: 0.517988	val: 1.794494	test: 1.899218
MAE train: 0.389528	val: 1.490873	test: 1.502247

Epoch: 102
Loss: 0.3402513563632965
RMSE train: 0.513347	val: 1.799919	test: 1.857424
MAE train: 0.387285	val: 1.423373	test: 1.431202

Epoch: 103
Loss: 0.34501566737890244
RMSE train: 0.532099	val: 1.793495	test: 1.870081
MAE train: 0.402945	val: 1.460032	test: 1.438647

Epoch: 104
Loss: 0.3027161732316017
RMSE train: 0.510834	val: 1.757035	test: 1.842327
MAE train: 0.390283	val: 1.445522	test: 1.421550

Epoch: 105
Loss: 0.3174138143658638
RMSE train: 0.425129	val: 1.657085	test: 1.737997
MAE train: 0.335563	val: 1.351160	test: 1.339454

Epoch: 106
Loss: 0.3342651054263115
RMSE train: 0.453734	val: 1.630395	test: 1.693649
MAE train: 0.362821	val: 1.310108	test: 1.296123

Epoch: 107
Loss: 0.3520224839448929
RMSE train: 0.508883	val: 1.733458	test: 1.768980
MAE train: 0.393260	val: 1.390886	test: 1.356159

Epoch: 108
Loss: 0.34033723920583725
RMSE train: 0.467928	val: 1.636664	test: 1.700763
MAE train: 0.366737	val: 1.336422	test: 1.318726

Epoch: 109
Loss: 0.3284395858645439
RMSE train: 0.405035	val: 1.681029	test: 1.743729
MAE train: 0.320368	val: 1.345036	test: 1.367433

Epoch: 110
Loss: 0.319644071161747
RMSE train: 0.456859	val: 1.765205	test: 1.810513
MAE train: 0.352789	val: 1.418497	test: 1.403909

Epoch: 111
Loss: 0.30531545728445053
RMSE train: 0.504750	val: 1.739053	test: 1.812425
MAE train: 0.384407	val: 1.407799	test: 1.384982

Epoch: 112
Loss: 0.31371574848890305
RMSE train: 0.443518	val: 1.843766	test: 1.912734
MAE train: 0.335915	val: 1.483834	test: 1.483765

Epoch: 113
Loss: 0.3095118924975395
RMSE train: 0.483682	val: 1.928385	test: 2.025654
MAE train: 0.361291	val: 1.601141	test: 1.603688

Epoch: 114
Loss: 0.2876376807689667
RMSE train: 0.490544	val: 1.757979	test: 1.859820
MAE train: 0.374802	val: 1.440759	test: 1.426944

Epoch: 115
Loss: 0.32552294433116913
RMSE train: 0.540044	val: 1.946855	test: 2.009550
MAE train: 0.407973	val: 1.549107	test: 1.541209

Epoch: 116
Loss: 0.27975061535835266
RMSE train: 0.470762	val: 1.658991	test: 1.764067
MAE train: 0.362996	val: 1.349594	test: 1.335349

Epoch: 117
Loss: 0.31177328526973724
RMSE train: 0.452453	val: 1.665778	test: 1.735606
MAE train: 0.354210	val: 1.327041	test: 1.323384

Epoch: 118
Loss: 0.2752463035285473
RMSE train: 0.498426	val: 1.829052	test: 1.864011
MAE train: 0.384642	val: 1.455684	test: 1.442248

Epoch: 119
Loss: 0.26901668310165405
RMSE train: 0.471924	val: 1.755047	test: 1.821695
MAE train: 0.365748	val: 1.407839	test: 1.415122

Epoch: 120
Loss: 0.2870241329073906
RMSE train: 0.472127	val: 1.742053	test: 1.835835
MAE train: 0.367573	val: 1.420231	test: 1.440250

Epoch: 121
Loss: 0.3351820409297943
RMSE train: 0.503977	val: 1.701596	test: 1.785487
MAE train: 0.392095	val: 1.368938	test: 1.365292

Early stopping
Best (RMSE):	 train: 0.480309	val: 1.519325	test: 1.649693
Best (MAE):	 train: 0.368679	val: 1.247614	test: 1.252318

MAE train: 0.348710	val: 1.478933	test: 1.345060

Epoch: 84
Loss: 0.429178424179554
RMSE train: 0.526046	val: 1.766567	test: 1.787057
MAE train: 0.404201	val: 1.525250	test: 1.428680

Epoch: 85
Loss: 0.42803342640399933
RMSE train: 0.588047	val: 1.905531	test: 1.901149
MAE train: 0.448009	val: 1.650796	test: 1.527400

Epoch: 86
Loss: 0.4221299737691879
RMSE train: 0.520600	val: 1.851842	test: 1.841000
MAE train: 0.399485	val: 1.596144	test: 1.447706

Epoch: 87
Loss: 0.3524997606873512
RMSE train: 0.509108	val: 1.844783	test: 1.840499
MAE train: 0.390366	val: 1.596887	test: 1.446869

Epoch: 88
Loss: 0.38135815411806107
RMSE train: 0.485037	val: 1.731566	test: 1.736983
MAE train: 0.375861	val: 1.502715	test: 1.345644

Epoch: 89
Loss: 0.3519907146692276
RMSE train: 0.527232	val: 1.773851	test: 1.770891
MAE train: 0.407117	val: 1.552990	test: 1.401647

Epoch: 90
Loss: 0.39498187601566315
RMSE train: 0.499262	val: 1.787220	test: 1.759091
MAE train: 0.392165	val: 1.561261	test: 1.382155

Epoch: 91
Loss: 0.36761462688446045
RMSE train: 0.496571	val: 1.705284	test: 1.694263
MAE train: 0.384247	val: 1.486791	test: 1.303563

Epoch: 92
Loss: 0.3885239064693451
RMSE train: 0.487950	val: 1.538068	test: 1.585269
MAE train: 0.375568	val: 1.345578	test: 1.239088

Epoch: 93
Loss: 0.41757682710886
RMSE train: 0.476289	val: 1.627133	test: 1.674064
MAE train: 0.364760	val: 1.427056	test: 1.307555

Epoch: 94
Loss: 0.366401769220829
RMSE train: 0.476942	val: 1.761662	test: 1.781880
MAE train: 0.364532	val: 1.512847	test: 1.365764

Epoch: 95
Loss: 0.5047904551029205
RMSE train: 0.516497	val: 1.769546	test: 1.790500
MAE train: 0.386929	val: 1.544200	test: 1.398892

Epoch: 96
Loss: 0.3969461917877197
RMSE train: 0.549593	val: 1.840232	test: 1.861524
MAE train: 0.414125	val: 1.561013	test: 1.409779

Epoch: 97
Loss: 0.3681298792362213
RMSE train: 0.516537	val: 1.756039	test: 1.792544
MAE train: 0.398012	val: 1.541479	test: 1.391632

Epoch: 98
Loss: 0.3688740208745003
RMSE train: 0.532631	val: 1.776270	test: 1.798858
MAE train: 0.404071	val: 1.562403	test: 1.418877

Epoch: 99
Loss: 0.35494233667850494
RMSE train: 0.458838	val: 1.730003	test: 1.747271
MAE train: 0.350484	val: 1.501023	test: 1.329638

Epoch: 100
Loss: 0.36532940715551376
RMSE train: 0.517402	val: 1.696292	test: 1.719128
MAE train: 0.389081	val: 1.493186	test: 1.341285

Epoch: 101
Loss: 0.36594144999980927
RMSE train: 0.536646	val: 1.661771	test: 1.685282
MAE train: 0.406584	val: 1.457822	test: 1.305657

Epoch: 102
Loss: 0.3444060832262039
RMSE train: 0.449556	val: 1.633546	test: 1.660375
MAE train: 0.347751	val: 1.416272	test: 1.287363

Epoch: 103
Loss: 0.3774336948990822
RMSE train: 0.501098	val: 1.732967	test: 1.732734
MAE train: 0.377920	val: 1.514908	test: 1.369506

Epoch: 104
Loss: 0.33680569380521774
RMSE train: 0.448561	val: 1.588942	test: 1.620631
MAE train: 0.352323	val: 1.384497	test: 1.272238

Epoch: 105
Loss: 0.3336254805326462
RMSE train: 0.467606	val: 1.696966	test: 1.735214
MAE train: 0.364687	val: 1.471202	test: 1.352756

Epoch: 106
Loss: 0.35505571961402893
RMSE train: 0.549759	val: 1.807242	test: 1.836625
MAE train: 0.419170	val: 1.562109	test: 1.433310

Epoch: 107
Loss: 0.33114371448755264
RMSE train: 0.473133	val: 1.648405	test: 1.697049
MAE train: 0.366651	val: 1.432913	test: 1.328956

Epoch: 108
Loss: 0.3598000109195709
RMSE train: 0.445786	val: 1.759569	test: 1.771771
MAE train: 0.343343	val: 1.518730	test: 1.391929

Epoch: 109
Loss: 0.34505263715982437
RMSE train: 0.488770	val: 1.873369	test: 1.870811
MAE train: 0.367491	val: 1.620107	test: 1.477602

Epoch: 110
Loss: 0.3338473439216614
RMSE train: 0.497946	val: 1.793976	test: 1.802199
MAE train: 0.384027	val: 1.552548	test: 1.389738

Epoch: 111
Loss: 0.359873428940773
RMSE train: 0.434781	val: 1.652824	test: 1.667641
MAE train: 0.333904	val: 1.428374	test: 1.274350

Epoch: 112
Loss: 0.3535298854112625
RMSE train: 0.482762	val: 1.492548	test: 1.550149
MAE train: 0.367128	val: 1.294559	test: 1.253729

Epoch: 113
Loss: 0.3392874673008919
RMSE train: 0.405893	val: 1.569008	test: 1.597012
MAE train: 0.315134	val: 1.358317	test: 1.235331

Epoch: 114
Loss: 0.343169242143631
RMSE train: 0.466601	val: 1.740812	test: 1.755443
MAE train: 0.355973	val: 1.503189	test: 1.338426

Epoch: 115
Loss: 0.3350359573960304
RMSE train: 0.509178	val: 1.782487	test: 1.799551
MAE train: 0.377484	val: 1.542179	test: 1.413481

Epoch: 116
Loss: 0.3289695307612419
RMSE train: 0.490046	val: 1.723320	test: 1.732526
MAE train: 0.374092	val: 1.495160	test: 1.364662

Epoch: 117
Loss: 0.28954777121543884
RMSE train: 0.498421	val: 1.678633	test: 1.690881
MAE train: 0.377718	val: 1.439722	test: 1.291864

Epoch: 118
Loss: 0.3287782669067383
RMSE train: 0.468747	val: 1.620042	test: 1.655770
MAE train: 0.356882	val: 1.391865	test: 1.257605

Epoch: 119
Loss: 0.3144253194332123
RMSE train: 0.460370	val: 1.686062	test: 1.725749
MAE train: 0.347074	val: 1.466420	test: 1.353990

Epoch: 120
Loss: 0.3017655238509178
RMSE train: 0.566094	val: 1.969856	test: 1.957314
MAE train: 0.433300	val: 1.687053	test: 1.530107

Epoch: 121
Loss: 0.35927820950746536
RMSE train: 0.538639	val: 1.898703	test: 1.865552
MAE train: 0.406006	val: 1.609635	test: 1.437436

Early stopping
Best (RMSE):	 train: 0.760196	val: 1.412965	test: 1.471765
Best (MAE):	 train: 0.594455	val: 1.154699	test: 1.167571
RMSE train: 0.560369	val: 1.193701	test: 1.298018
MAE train: 0.447176	val: 0.964816	test: 0.981937

Epoch: 84
Loss: 0.4002334997057915
RMSE train: 0.631634	val: 1.321531	test: 1.433875
MAE train: 0.487616	val: 1.070920	test: 1.063041

Epoch: 85
Loss: 0.4097750410437584
RMSE train: 0.538910	val: 1.214125	test: 1.300987
MAE train: 0.422392	val: 0.962653	test: 0.972380

Epoch: 86
Loss: 0.40459977090358734
RMSE train: 0.593639	val: 1.288741	test: 1.328481
MAE train: 0.461530	val: 1.019500	test: 1.009441

Epoch: 87
Loss: 0.37764347344636917
RMSE train: 0.578629	val: 1.261451	test: 1.334295
MAE train: 0.458611	val: 1.045093	test: 0.994365

Epoch: 88
Loss: 0.390775203704834
RMSE train: 0.566517	val: 1.254903	test: 1.310032
MAE train: 0.445086	val: 1.047679	test: 0.983773

Epoch: 89
Loss: 0.4515383392572403
RMSE train: 0.489758	val: 1.207703	test: 1.255544
MAE train: 0.386282	val: 0.967806	test: 0.967984

Epoch: 90
Loss: 0.38597236573696136
RMSE train: 0.538238	val: 1.211211	test: 1.328644
MAE train: 0.421857	val: 0.998640	test: 1.007532

Epoch: 91
Loss: 0.3630325496196747
RMSE train: 0.642968	val: 1.333706	test: 1.376723
MAE train: 0.508716	val: 1.066844	test: 1.020171

Epoch: 92
Loss: 0.3577663078904152
RMSE train: 0.518767	val: 1.279675	test: 1.288555
MAE train: 0.410222	val: 1.013729	test: 0.984307

Epoch: 93
Loss: 0.3351399824023247
RMSE train: 0.533798	val: 1.205238	test: 1.294537
MAE train: 0.425113	val: 0.980774	test: 0.974396

Epoch: 94
Loss: 0.3696756288409233
RMSE train: 0.551926	val: 1.252131	test: 1.294571
MAE train: 0.436959	val: 0.989869	test: 0.974516

Epoch: 95
Loss: 0.3400909826159477
RMSE train: 0.466877	val: 1.311838	test: 1.288872
MAE train: 0.359479	val: 1.066687	test: 1.019692

Epoch: 96
Loss: 0.38147423416376114
RMSE train: 0.559262	val: 1.208403	test: 1.350073
MAE train: 0.412646	val: 1.010411	test: 1.035509

Epoch: 97
Loss: 0.3692966103553772
RMSE train: 0.499281	val: 1.155190	test: 1.262783
MAE train: 0.391784	val: 0.972780	test: 0.956074

Epoch: 98
Loss: 0.39516420662403107
RMSE train: 0.490882	val: 1.254599	test: 1.291063
MAE train: 0.384378	val: 1.029893	test: 0.997896

Epoch: 99
Loss: 0.3155362755060196
RMSE train: 0.545003	val: 1.164262	test: 1.335764
MAE train: 0.410192	val: 0.971827	test: 1.016494

Epoch: 100
Loss: 0.485902801156044
RMSE train: 0.498942	val: 1.119133	test: 1.268203
MAE train: 0.388732	val: 0.926647	test: 0.949322

Epoch: 101
Loss: 0.3266039416193962
RMSE train: 0.566198	val: 1.455655	test: 1.397321
MAE train: 0.434351	val: 1.160316	test: 1.108556

Epoch: 102
Loss: 0.3310862332582474
RMSE train: 0.538870	val: 1.214433	test: 1.340576
MAE train: 0.409007	val: 1.008085	test: 1.010359

Epoch: 103
Loss: 0.3556959182024002
RMSE train: 0.479501	val: 1.177405	test: 1.257156
MAE train: 0.374529	val: 0.962068	test: 0.956364

Epoch: 104
Loss: 0.356611430644989
RMSE train: 0.524294	val: 1.305035	test: 1.302948
MAE train: 0.398059	val: 1.024366	test: 1.005988

Epoch: 105
Loss: 0.3150797188282013
RMSE train: 0.445617	val: 1.123112	test: 1.232826
MAE train: 0.351968	val: 0.912161	test: 0.957925

Epoch: 106
Loss: 0.340474508702755
RMSE train: 0.445245	val: 1.197810	test: 1.237646
MAE train: 0.352352	val: 0.962861	test: 0.961899

Epoch: 107
Loss: 0.29725781828165054
RMSE train: 0.466972	val: 1.208793	test: 1.244824
MAE train: 0.366928	val: 0.975528	test: 0.954917

Epoch: 108
Loss: 0.3182604908943176
RMSE train: 0.448795	val: 1.164168	test: 1.234227
MAE train: 0.353720	val: 0.960781	test: 0.943996

Epoch: 109
Loss: 0.33371178060770035
RMSE train: 0.428322	val: 1.119536	test: 1.215296
MAE train: 0.336584	val: 0.922556	test: 0.941221

Epoch: 110
Loss: 0.2546625882387161
RMSE train: 0.448181	val: 1.200606	test: 1.251180
MAE train: 0.352743	val: 0.983328	test: 0.970064

Epoch: 111
Loss: 0.2974989712238312
RMSE train: 0.498921	val: 1.137074	test: 1.254762
MAE train: 0.392718	val: 0.930970	test: 0.932383

Epoch: 112
Loss: 0.3654993027448654
RMSE train: 0.485159	val: 1.107628	test: 1.233979
MAE train: 0.381671	val: 0.919025	test: 0.927450

Epoch: 113
Loss: 0.2754812054336071
RMSE train: 0.471207	val: 1.223424	test: 1.238386
MAE train: 0.370619	val: 0.989189	test: 0.934971

Epoch: 114
Loss: 0.32720018178224564
RMSE train: 0.508270	val: 1.261875	test: 1.273496
MAE train: 0.396602	val: 1.014697	test: 0.961073

Epoch: 115
Loss: 0.28075096011161804
RMSE train: 0.488909	val: 1.208037	test: 1.291830
MAE train: 0.388155	val: 1.002026	test: 0.975889

Epoch: 116
Loss: 0.29104389250278473
RMSE train: 0.540505	val: 1.291221	test: 1.339933
MAE train: 0.421571	val: 1.023662	test: 0.997952

Epoch: 117
Loss: 0.32860416918992996
RMSE train: 0.498966	val: 1.228476	test: 1.314120
MAE train: 0.394816	val: 0.991444	test: 0.979579

Epoch: 118
Loss: 0.2685822732746601
RMSE train: 0.439313	val: 1.181079	test: 1.270835
MAE train: 0.350664	val: 0.960227	test: 0.961282

Epoch: 119
Loss: 0.28197505325078964
RMSE train: 0.482981	val: 1.232254	test: 1.297503
MAE train: 0.381664	val: 0.982397	test: 0.977290

Epoch: 120
Loss: 0.26265496760606766
RMSE train: 0.490375	val: 1.234585	test: 1.293066
MAE train: 0.387231	val: 0.993701	test: 0.968464

Epoch: 121
Loss: 0.33925461024045944
RMSE train: 0.472350	val: 1.273066	test: 1.329372
MAE train: 0.372213	val: 1.020456	test: 0.987685

Early stopping
Best (RMSE):	 train: 0.602920	val: 1.042327	test: 1.166818
Best (MAE):	 train: 0.475859	val: 0.859207	test: 0.903193

MAE train: 0.428611	val: 1.092990	test: 1.071171

Epoch: 84
Loss: 0.3821145296096802
RMSE train: 0.544159	val: 1.307862	test: 1.371683
MAE train: 0.419181	val: 1.055753	test: 1.052441

Epoch: 85
Loss: 0.3899633511900902
RMSE train: 0.583521	val: 1.414424	test: 1.469977
MAE train: 0.446823	val: 1.152926	test: 1.131962

Epoch: 86
Loss: 0.33340463787317276
RMSE train: 0.461260	val: 1.301645	test: 1.395765
MAE train: 0.359047	val: 1.066195	test: 1.101340

Epoch: 87
Loss: 0.35224785655736923
RMSE train: 0.498218	val: 1.331168	test: 1.427536
MAE train: 0.385867	val: 1.107895	test: 1.126685

Epoch: 88
Loss: 0.3655063658952713
RMSE train: 0.536777	val: 1.367111	test: 1.402197
MAE train: 0.419329	val: 1.104035	test: 1.100246

Epoch: 89
Loss: 0.3141537830233574
RMSE train: 0.487509	val: 1.194898	test: 1.249210
MAE train: 0.382667	val: 0.984238	test: 0.973548

Epoch: 90
Loss: 0.34949349611997604
RMSE train: 0.459987	val: 1.225318	test: 1.258477
MAE train: 0.357098	val: 1.008434	test: 0.973571

Epoch: 91
Loss: 0.3212262839078903
RMSE train: 0.533490	val: 1.341700	test: 1.351945
MAE train: 0.411114	val: 1.082110	test: 1.054691

Epoch: 92
Loss: 0.3075384795665741
RMSE train: 0.479781	val: 1.165734	test: 1.254985
MAE train: 0.378089	val: 0.954464	test: 1.028422

Epoch: 93
Loss: 0.365917332470417
RMSE train: 0.483307	val: 1.180980	test: 1.259865
MAE train: 0.376476	val: 0.972939	test: 0.993435

Epoch: 94
Loss: 0.37102780118584633
RMSE train: 0.509866	val: 1.328203	test: 1.366369
MAE train: 0.398341	val: 1.083340	test: 1.057537

Epoch: 95
Loss: 0.40147851407527924
RMSE train: 0.482163	val: 1.328300	test: 1.395508
MAE train: 0.373959	val: 1.113390	test: 1.082839

Epoch: 96
Loss: 0.3229067549109459
RMSE train: 0.521293	val: 1.473637	test: 1.500832
MAE train: 0.404027	val: 1.177311	test: 1.178917

Epoch: 97
Loss: 0.357238344848156
RMSE train: 0.488175	val: 1.319321	test: 1.377261
MAE train: 0.380500	val: 1.097615	test: 1.094151

Epoch: 98
Loss: 0.34291572123765945
RMSE train: 0.474422	val: 1.296408	test: 1.364386
MAE train: 0.373638	val: 1.087477	test: 1.064834

Epoch: 99
Loss: 0.306966133415699
RMSE train: 0.471994	val: 1.334996	test: 1.398603
MAE train: 0.369503	val: 1.095760	test: 1.080531

Epoch: 100
Loss: 0.32221732288599014
RMSE train: 0.562116	val: 1.387239	test: 1.457633
MAE train: 0.430946	val: 1.153270	test: 1.097344

Epoch: 101
Loss: 0.32466942071914673
RMSE train: 0.501567	val: 1.312846	test: 1.379146
MAE train: 0.390058	val: 1.072748	test: 1.054049

Epoch: 102
Loss: 0.3240717202425003
RMSE train: 0.414408	val: 1.162964	test: 1.227932
MAE train: 0.330009	val: 0.946407	test: 0.992503

Epoch: 103
Loss: 0.2941545993089676
RMSE train: 0.476542	val: 1.230907	test: 1.271169
MAE train: 0.379729	val: 1.009241	test: 0.999578

Epoch: 104
Loss: 0.3340683802962303
RMSE train: 0.451545	val: 1.137668	test: 1.222792
MAE train: 0.360300	val: 0.951714	test: 0.989853

Epoch: 105
Loss: 0.3172511123120785
RMSE train: 0.425182	val: 1.197874	test: 1.291994
MAE train: 0.332648	val: 1.006911	test: 1.028546

Epoch: 106
Loss: 0.30903130769729614
RMSE train: 0.496451	val: 1.312825	test: 1.378621
MAE train: 0.386910	val: 1.097912	test: 1.071690

Epoch: 107
Loss: 0.32826387882232666
RMSE train: 0.504465	val: 1.359055	test: 1.393894
MAE train: 0.392002	val: 1.107518	test: 1.073138

Epoch: 108
Loss: 0.29266420751810074
RMSE train: 0.404505	val: 1.185546	test: 1.242295
MAE train: 0.322169	val: 0.977761	test: 0.983082

Epoch: 109
Loss: 0.33902662992477417
RMSE train: 0.511752	val: 1.328500	test: 1.375183
MAE train: 0.401720	val: 1.100653	test: 1.054056

Epoch: 110
Loss: 0.27935289591550827
RMSE train: 0.551073	val: 1.409922	test: 1.463150
MAE train: 0.419611	val: 1.142451	test: 1.124775

Epoch: 111
Loss: 0.33975108712911606
RMSE train: 0.471276	val: 1.331968	test: 1.419949
MAE train: 0.359904	val: 1.085427	test: 1.108897

Epoch: 112
Loss: 0.29269784688949585
RMSE train: 0.438095	val: 1.229917	test: 1.308079
MAE train: 0.338508	val: 1.019195	test: 1.020695

Epoch: 113
Loss: 0.3096977174282074
RMSE train: 0.453422	val: 1.231475	test: 1.309695
MAE train: 0.355793	val: 1.017570	test: 1.024353

Epoch: 114
Loss: 0.2830202244222164
RMSE train: 0.428955	val: 1.175619	test: 1.263399
MAE train: 0.333492	val: 0.959628	test: 0.993670

Epoch: 115
Loss: 0.3369765803217888
RMSE train: 0.436732	val: 1.234113	test: 1.318263
MAE train: 0.334287	val: 1.005253	test: 1.013637

Epoch: 116
Loss: 0.27805251628160477
RMSE train: 0.442388	val: 1.272102	test: 1.348848
MAE train: 0.338721	val: 1.043218	test: 1.028200

Epoch: 117
Loss: 0.28834884613752365
RMSE train: 0.432492	val: 1.334656	test: 1.416939
MAE train: 0.335944	val: 1.066210	test: 1.115806

Epoch: 118
Loss: 0.32601257413625717
RMSE train: 0.466569	val: 1.276529	test: 1.356837
MAE train: 0.371181	val: 1.021392	test: 1.082417

Epoch: 119
Loss: 0.2691528722643852
RMSE train: 0.433129	val: 1.144940	test: 1.268356
MAE train: 0.338603	val: 0.954384	test: 1.001967

Epoch: 120
Loss: 0.32612908631563187
RMSE train: 0.434336	val: 1.362914	test: 1.427635
MAE train: 0.338141	val: 1.087185	test: 1.123969

Epoch: 121
Loss: 0.30631426349282265
RMSE train: 0.472461	val: 1.310248	test: 1.398001
MAE train: 0.360946	val: 1.048254	test: 1.108415

Early stopping
Best (RMSE):	 train: 0.476656	val: 1.086894	test: 1.214030
Best (MAE):	 train: 0.384501	val: 0.912416	test: 0.982031
All runs completed.


Epoch: 84
Loss: 0.3944981321692467
RMSE train: 0.544902	val: 1.232235	test: 1.314221
MAE train: 0.410916	val: 1.009477	test: 0.984462

Epoch: 85
Loss: 0.4446282759308815
RMSE train: 0.620670	val: 1.364030	test: 1.439111
MAE train: 0.460000	val: 1.080629	test: 1.068312

Epoch: 86
Loss: 0.35610564053058624
RMSE train: 0.603403	val: 1.235875	test: 1.356979
MAE train: 0.455386	val: 1.007430	test: 1.005947

Epoch: 87
Loss: 0.3980320692062378
RMSE train: 0.555423	val: 1.254129	test: 1.353379
MAE train: 0.427350	val: 1.050177	test: 1.032220

Epoch: 88
Loss: 0.38246387988328934
RMSE train: 0.525143	val: 1.246837	test: 1.357902
MAE train: 0.406542	val: 1.051251	test: 1.035872

Epoch: 89
Loss: 0.5056822225451469
RMSE train: 0.513013	val: 1.231911	test: 1.300799
MAE train: 0.397776	val: 1.023472	test: 0.992208

Epoch: 90
Loss: 0.41154929995536804
RMSE train: 0.569049	val: 1.307037	test: 1.330405
MAE train: 0.441270	val: 1.072625	test: 1.018393

Epoch: 91
Loss: 0.38586076349020004
RMSE train: 0.535626	val: 1.361265	test: 1.378222
MAE train: 0.413826	val: 1.104387	test: 1.039099

Epoch: 92
Loss: 0.34253621101379395
RMSE train: 0.469466	val: 1.306807	test: 1.357787
MAE train: 0.356950	val: 1.049845	test: 0.993712

Epoch: 93
Loss: 0.39442921429872513
RMSE train: 0.539984	val: 1.185555	test: 1.263706
MAE train: 0.418319	val: 0.992786	test: 0.958491

Epoch: 94
Loss: 0.34880033135414124
RMSE train: 0.615773	val: 1.250630	test: 1.277013
MAE train: 0.487805	val: 1.038078	test: 0.983965

Epoch: 95
Loss: 0.35940103977918625
RMSE train: 0.513672	val: 1.121398	test: 1.197810
MAE train: 0.388289	val: 0.945515	test: 0.920862

Epoch: 96
Loss: 0.38073547184467316
RMSE train: 0.497545	val: 1.166436	test: 1.212372
MAE train: 0.376151	val: 0.982501	test: 0.945037

Epoch: 97
Loss: 0.3661595359444618
RMSE train: 0.528378	val: 1.269456	test: 1.246673
MAE train: 0.416595	val: 1.054090	test: 0.985927

Epoch: 98
Loss: 0.3387160524725914
RMSE train: 0.491726	val: 1.107342	test: 1.142637
MAE train: 0.382544	val: 0.916675	test: 0.903151

Epoch: 99
Loss: 0.3429497256875038
RMSE train: 0.484199	val: 1.206343	test: 1.207283
MAE train: 0.381490	val: 1.008999	test: 0.947824

Epoch: 100
Loss: 0.3566681817173958
RMSE train: 0.510980	val: 1.272468	test: 1.290272
MAE train: 0.402760	val: 1.065119	test: 0.994751

Epoch: 101
Loss: 0.3422875702381134
RMSE train: 0.485101	val: 1.159758	test: 1.217682
MAE train: 0.378519	val: 0.983797	test: 0.969208

Epoch: 102
Loss: 0.32656243443489075
RMSE train: 0.475045	val: 1.181686	test: 1.241813
MAE train: 0.376746	val: 0.995551	test: 0.982473

Epoch: 103
Loss: 0.36292384564876556
RMSE train: 0.485017	val: 1.283987	test: 1.347355
MAE train: 0.377255	val: 1.069381	test: 1.037189

Epoch: 104
Loss: 0.32692377269268036
RMSE train: 0.523715	val: 1.159501	test: 1.257944
MAE train: 0.400721	val: 0.990633	test: 0.987209

Epoch: 105
Loss: 0.35264045745134354
RMSE train: 0.531105	val: 1.191611	test: 1.275278
MAE train: 0.404548	val: 1.004903	test: 0.993256

Epoch: 106
Loss: 0.3682422935962677
RMSE train: 0.512010	val: 1.226814	test: 1.245810
MAE train: 0.390055	val: 1.030936	test: 0.967042

Epoch: 107
Loss: 0.34155768156051636
RMSE train: 0.514987	val: 1.108980	test: 1.198442
MAE train: 0.388491	val: 0.952813	test: 0.937021

Epoch: 108
Loss: 0.342618927359581
RMSE train: 0.504528	val: 1.228040	test: 1.270726
MAE train: 0.403126	val: 1.022390	test: 1.000590

Epoch: 109
Loss: 0.31099362671375275
RMSE train: 0.507689	val: 1.213220	test: 1.238906
MAE train: 0.406118	val: 1.010314	test: 0.983756

Epoch: 110
Loss: 0.32986266911029816
RMSE train: 0.501137	val: 1.116318	test: 1.174707
MAE train: 0.386141	val: 0.960018	test: 0.922123

Epoch: 111
Loss: 0.3362252488732338
RMSE train: 0.518340	val: 1.195631	test: 1.200403
MAE train: 0.397973	val: 1.002959	test: 0.927577

Epoch: 112
Loss: 0.34595731645822525
RMSE train: 0.566915	val: 1.214461	test: 1.224449
MAE train: 0.434205	val: 0.992831	test: 0.916423

Epoch: 113
Loss: 0.3507356122136116
RMSE train: 0.547604	val: 1.179154	test: 1.189001
MAE train: 0.422738	val: 0.976022	test: 0.908665

Epoch: 114
Loss: 0.36252565681934357
RMSE train: 0.488463	val: 1.302492	test: 1.348190
MAE train: 0.377613	val: 1.062011	test: 1.025249

Epoch: 115
Loss: 0.3661998063325882
RMSE train: 0.507556	val: 1.217056	test: 1.263571
MAE train: 0.382566	val: 1.039792	test: 0.976769

Epoch: 116
Loss: 0.3522563949227333
RMSE train: 0.511433	val: 1.182890	test: 1.226186
MAE train: 0.393743	val: 1.009081	test: 0.963641

Epoch: 117
Loss: 0.37703726440668106
RMSE train: 0.512026	val: 1.309353	test: 1.298061
MAE train: 0.389897	val: 1.090212	test: 1.002324

Epoch: 118
Loss: 0.3582891598343849
RMSE train: 0.582544	val: 1.197951	test: 1.265720
MAE train: 0.444686	val: 1.002603	test: 0.972538

Epoch: 119
Loss: 0.4053870216012001
RMSE train: 0.532024	val: 1.183382	test: 1.239153
MAE train: 0.412218	val: 0.984783	test: 0.962440

Epoch: 120
Loss: 0.333302766084671
RMSE train: 0.474545	val: 1.194150	test: 1.246172
MAE train: 0.364570	val: 1.003695	test: 0.982543

Epoch: 121
Loss: 0.3486819863319397
RMSE train: 0.526255	val: 1.079808	test: 1.172678
MAE train: 0.395739	val: 0.910970	test: 0.915602

Epoch: 122
Loss: 0.32530680298805237
RMSE train: 0.534697	val: 1.269279	test: 1.256490
MAE train: 0.416460	val: 1.060827	test: 0.978213

Epoch: 123
Loss: 0.3208889588713646
RMSE train: 0.459431	val: 1.093592	test: 1.165194
MAE train: 0.349907	val: 0.928373	test: 0.906262

Epoch: 124
Loss: 0.35977816581726074
RMSE train: 0.519733	val: 1.084666	test: 1.155029
MAE train: 0.395635	val: 0.922256	test: 0.896186

Epoch: 125
Loss: 0.30525557696819305
RMSE train: 0.528696	val: 1.248532	test: 1.243182
MAE train: 0.415014	val: 1.038727	test: 0.976707

Epoch: 126
Loss: 0.33537862449884415
RMSE train: 0.463424	val: 1.086409	test: 1.185658
MAE train: 0.351239	val: 0.912035	test: 0.943133

Epoch: 127
Loss: 0.3089611157774925
RMSE train: 0.530671	val: 1.099581	test: 1.194308
MAE train: 0.389792	val: 0.923205	test: 0.920628

Epoch: 128
Loss: 0.3260292708873749
RMSE train: 0.527640	val: 1.168260	test: 1.221650
MAE train: 0.400168	val: 0.971488	test: 0.945986

Epoch: 129
Loss: 0.2961435988545418
RMSE train: 0.456519	val: 1.052210	test: 1.149229
MAE train: 0.346666	val: 0.881030	test: 0.909303

Epoch: 130
Loss: 0.2992406561970711
RMSE train: 0.528852	val: 1.170003	test: 1.190574
MAE train: 0.410859	val: 0.976366	test: 0.919774

Epoch: 131
Loss: 0.3461090922355652
RMSE train: 0.512648	val: 1.147070	test: 1.181691
MAE train: 0.396246	val: 0.957927	test: 0.917607

Epoch: 132
Loss: 0.3028126209974289
RMSE train: 0.439701	val: 1.149162	test: 1.195369
MAE train: 0.341236	val: 0.965519	test: 0.956660

Epoch: 133
Loss: 0.28481996059417725
RMSE train: 0.509182	val: 1.260552	test: 1.300553
MAE train: 0.392300	val: 1.067223	test: 1.016955

Epoch: 134
Loss: 0.3314339742064476
RMSE train: 0.564356	val: 1.278369	test: 1.299120
MAE train: 0.434641	val: 1.069589	test: 1.011916

Epoch: 135
Loss: 0.3118540719151497
RMSE train: 0.454631	val: 1.157688	test: 1.190021
MAE train: 0.351595	val: 0.966552	test: 0.934282

Epoch: 136
Loss: 0.30910421162843704
RMSE train: 0.483412	val: 1.177173	test: 1.187166
MAE train: 0.375517	val: 0.992002	test: 0.930172

Epoch: 137
Loss: 0.36954598128795624
RMSE train: 0.542133	val: 1.172651	test: 1.215329
MAE train: 0.412838	val: 0.975828	test: 0.920910

Epoch: 138
Loss: 0.2748161256313324
RMSE train: 0.510041	val: 1.152832	test: 1.199416
MAE train: 0.384220	val: 0.958049	test: 0.916265

Epoch: 139
Loss: 0.29825951904058456
RMSE train: 0.476568	val: 1.136659	test: 1.177944
MAE train: 0.359484	val: 0.943087	test: 0.913985

Epoch: 140
Loss: 0.2948392480611801
RMSE train: 0.543288	val: 1.291413	test: 1.264504
MAE train: 0.415251	val: 1.078783	test: 0.975792

Epoch: 141
Loss: 0.32085250318050385
RMSE train: 0.574306	val: 1.187409	test: 1.243998
MAE train: 0.438538	val: 0.994233	test: 0.933376

Epoch: 142
Loss: 0.3312521427869797
RMSE train: 0.522911	val: 1.137378	test: 1.186357
MAE train: 0.399181	val: 0.948506	test: 0.905056

Epoch: 143
Loss: 0.27225761860609055
RMSE train: 0.509001	val: 1.208429	test: 1.206081
MAE train: 0.383854	val: 1.011375	test: 0.940788

Epoch: 144
Loss: 0.2904978096485138

Epoch: 84
Loss: 0.3735116794705391
RMSE train: 0.620113	val: 1.171335	test: 1.229415
MAE train: 0.448841	val: 0.999563	test: 0.929937

Epoch: 85
Loss: 0.4093824550509453
RMSE train: 0.593497	val: 1.270032	test: 1.251904
MAE train: 0.431655	val: 1.022992	test: 0.977103

Epoch: 86
Loss: 0.39879047125577927
RMSE train: 0.504796	val: 1.096731	test: 1.135123
MAE train: 0.387936	val: 0.901314	test: 0.918659

Epoch: 87
Loss: 0.3703645318746567
RMSE train: 0.540843	val: 1.027685	test: 1.144731
MAE train: 0.412256	val: 0.857963	test: 0.891514

Epoch: 88
Loss: 0.3878064826130867
RMSE train: 0.516735	val: 1.111193	test: 1.124500
MAE train: 0.406224	val: 0.919380	test: 0.886643

Epoch: 89
Loss: 0.43093402683734894
RMSE train: 0.494060	val: 1.065668	test: 1.128689
MAE train: 0.377154	val: 0.876963	test: 0.902492

Epoch: 90
Loss: 0.39615871012210846
RMSE train: 0.526528	val: 1.124467	test: 1.145772
MAE train: 0.394939	val: 0.952231	test: 0.893885

Epoch: 91
Loss: 0.4229314625263214
RMSE train: 0.523215	val: 1.180054	test: 1.176390
MAE train: 0.390478	val: 0.988928	test: 0.913495

Epoch: 92
Loss: 0.3872431665658951
RMSE train: 0.503515	val: 1.226894	test: 1.182595
MAE train: 0.369604	val: 1.014930	test: 0.939183

Epoch: 93
Loss: 0.36950284242630005
RMSE train: 0.527468	val: 1.053922	test: 1.148903
MAE train: 0.396968	val: 0.882139	test: 0.902921

Epoch: 94
Loss: 0.382728174328804
RMSE train: 0.545084	val: 1.222277	test: 1.189404
MAE train: 0.407941	val: 1.018516	test: 0.926531

Epoch: 95
Loss: 0.3409029245376587
RMSE train: 0.513074	val: 1.110586	test: 1.156784
MAE train: 0.382170	val: 0.941775	test: 0.881913

Epoch: 96
Loss: 0.37040358036756516
RMSE train: 0.491505	val: 1.097790	test: 1.138474
MAE train: 0.363689	val: 0.935229	test: 0.879951

Epoch: 97
Loss: 0.3125438466668129
RMSE train: 0.539718	val: 1.193020	test: 1.194593
MAE train: 0.398713	val: 1.000588	test: 0.937626

Epoch: 98
Loss: 0.3567441925406456
RMSE train: 0.526917	val: 1.191721	test: 1.198974
MAE train: 0.393840	val: 0.989154	test: 0.947802

Epoch: 99
Loss: 0.322971872985363
RMSE train: 0.513838	val: 1.178120	test: 1.175178
MAE train: 0.382836	val: 0.975853	test: 0.917171

Epoch: 100
Loss: 0.407552070915699
RMSE train: 0.514363	val: 1.142952	test: 1.159511
MAE train: 0.381813	val: 0.970388	test: 0.893859

Epoch: 101
Loss: 0.33107105642557144
RMSE train: 0.548007	val: 1.168710	test: 1.193910
MAE train: 0.404038	val: 0.990825	test: 0.894381

Epoch: 102
Loss: 0.3386159613728523
RMSE train: 0.501798	val: 1.072918	test: 1.145715
MAE train: 0.377155	val: 0.917493	test: 0.888142

Epoch: 103
Loss: 0.336347833275795
RMSE train: 0.529357	val: 1.138764	test: 1.191190
MAE train: 0.394970	val: 0.957611	test: 0.922647

Epoch: 104
Loss: 0.40331656485795975
RMSE train: 0.569290	val: 1.164040	test: 1.232961
MAE train: 0.417880	val: 0.977249	test: 0.939687

Epoch: 105
Loss: 0.37431750446558
RMSE train: 0.494396	val: 1.014708	test: 1.141974
MAE train: 0.370870	val: 0.857848	test: 0.907371

Epoch: 106
Loss: 0.32938747853040695
RMSE train: 0.572568	val: 1.229885	test: 1.205570
MAE train: 0.443368	val: 0.997294	test: 0.956507

Epoch: 107
Loss: 0.36285581439733505
RMSE train: 0.498053	val: 1.074890	test: 1.128507
MAE train: 0.394993	val: 0.876369	test: 0.904674

Epoch: 108
Loss: 0.29589899629354477
RMSE train: 0.466310	val: 1.125651	test: 1.110397
MAE train: 0.359361	val: 0.914996	test: 0.898354

Epoch: 109
Loss: 0.32454607635736465
RMSE train: 0.521234	val: 1.172334	test: 1.159087
MAE train: 0.395877	val: 0.961484	test: 0.893085

Epoch: 110
Loss: 0.31903671845793724
RMSE train: 0.478102	val: 1.136350	test: 1.146395
MAE train: 0.358144	val: 0.936119	test: 0.903988

Epoch: 111
Loss: 0.3255629390478134
RMSE train: 0.493100	val: 1.104915	test: 1.153943
MAE train: 0.370800	val: 0.929433	test: 0.894776

Epoch: 112
Loss: 0.3355075344443321
RMSE train: 0.523446	val: 1.142537	test: 1.174334
MAE train: 0.389974	val: 0.963283	test: 0.886975

Epoch: 113
Loss: 0.3205745071172714
RMSE train: 0.543667	val: 1.183892	test: 1.179851
MAE train: 0.402527	val: 0.990950	test: 0.883855

Epoch: 114
Loss: 0.37040797621011734
RMSE train: 0.566161	val: 1.184473	test: 1.190066
MAE train: 0.416872	val: 0.991530	test: 0.879609

Epoch: 115
Loss: 0.3190896660089493
RMSE train: 0.556104	val: 1.213987	test: 1.191623
MAE train: 0.403802	val: 1.011089	test: 0.886902

Epoch: 116
Loss: 0.2852046340703964
RMSE train: 0.546141	val: 1.202199	test: 1.171859
MAE train: 0.406244	val: 1.001455	test: 0.895772

Epoch: 117
Loss: 0.3163699805736542
RMSE train: 0.506851	val: 1.118477	test: 1.167770
MAE train: 0.378475	val: 0.951112	test: 0.898441

Epoch: 118
Loss: 0.3255452811717987
RMSE train: 0.557964	val: 1.233112	test: 1.268306
MAE train: 0.409594	val: 1.008327	test: 0.966656

Epoch: 119
Loss: 0.2986645773053169
RMSE train: 0.507624	val: 1.151830	test: 1.205308
MAE train: 0.369274	val: 0.942625	test: 0.920187

Epoch: 120
Loss: 0.30810797214508057
RMSE train: 0.543637	val: 1.115936	test: 1.201487
MAE train: 0.396430	val: 0.948595	test: 0.896438

Epoch: 121
Loss: 0.29626111686229706
RMSE train: 0.497262	val: 1.160558	test: 1.226911
MAE train: 0.372772	val: 0.958004	test: 0.951105

Epoch: 122
Loss: 0.3360457867383957
RMSE train: 0.483238	val: 1.147429	test: 1.236610
MAE train: 0.364349	val: 0.970907	test: 0.959552

Epoch: 123
Loss: 0.3551653251051903
RMSE train: 0.515082	val: 1.200281	test: 1.306804
MAE train: 0.375186	val: 1.014509	test: 1.001632

Epoch: 124
Loss: 0.30100494623184204
RMSE train: 0.528407	val: 1.310159	test: 1.366470
MAE train: 0.380416	val: 1.065373	test: 1.054792

Epoch: 125
Loss: 0.33460966497659683
RMSE train: 0.480507	val: 1.066509	test: 1.159388
MAE train: 0.351238	val: 0.898070	test: 0.901379

Epoch: 126
Loss: 0.29298723489046097
RMSE train: 0.489316	val: 1.148134	test: 1.148042
MAE train: 0.363881	val: 0.944512	test: 0.887818

Epoch: 127
Loss: 0.28032538294792175
RMSE train: 0.503563	val: 1.165322	test: 1.160831
MAE train: 0.374011	val: 0.960360	test: 0.885800

Epoch: 128
Loss: 0.29769905656576157
RMSE train: 0.506324	val: 1.153179	test: 1.164656
MAE train: 0.374170	val: 0.951589	test: 0.878494

Epoch: 129
Loss: 0.28405801951885223
RMSE train: 0.521888	val: 1.142434	test: 1.160478
MAE train: 0.383746	val: 0.955087	test: 0.865144

Epoch: 130
Loss: 0.30403758585453033
RMSE train: 0.480392	val: 1.203095	test: 1.158995
MAE train: 0.353414	val: 0.992479	test: 0.908328

Epoch: 131
Loss: 0.3265584111213684
RMSE train: 0.511238	val: 1.135434	test: 1.169896
MAE train: 0.374578	val: 0.959484	test: 0.896566

Epoch: 132
Loss: 0.28391747176647186
RMSE train: 0.477231	val: 1.041580	test: 1.137739
MAE train: 0.350383	val: 0.886466	test: 0.878807

Epoch: 133
Loss: 0.2928961217403412
RMSE train: 0.473421	val: 1.113743	test: 1.152584
MAE train: 0.354103	val: 0.935913	test: 0.887159

Epoch: 134
Loss: 0.28979144245386124
RMSE train: 0.546977	val: 1.126178	test: 1.190843
MAE train: 0.406723	val: 0.952278	test: 0.879528

Epoch: 135
Loss: 0.2691905200481415
RMSE train: 0.512774	val: 1.243940	test: 1.184200
MAE train: 0.390767	val: 1.022939	test: 0.901136

Epoch: 136
Loss: 0.2664150185883045
RMSE train: 0.494629	val: 1.270496	test: 1.192745
MAE train: 0.366555	val: 1.037534	test: 0.920940

Epoch: 137
Loss: 0.29240782558918
RMSE train: 0.543819	val: 1.213625	test: 1.206352
MAE train: 0.395680	val: 1.005393	test: 0.904828

Epoch: 138
Loss: 0.3003505654633045
RMSE train: 0.532996	val: 1.209605	test: 1.202323
MAE train: 0.393604	val: 0.985848	test: 0.911044

Epoch: 139
Loss: 0.23076987639069557
RMSE train: 0.482763	val: 1.097742	test: 1.142241
MAE train: 0.360380	val: 0.907282	test: 0.876789

Epoch: 140
Loss: 0.2777530997991562
RMSE train: 0.465024	val: 1.133006	test: 1.142473
MAE train: 0.347963	val: 0.927545	test: 0.893364

Early stopping
Best (RMSE):	 train: 0.494396	val: 1.014708	test: 1.141974
Best (MAE):	 train: 0.370870	val: 0.857848	test: 0.907371

MAE train: 0.457863	val: 1.065254	test: 1.034085

Epoch: 84
Loss: 0.36204537749290466
RMSE train: 0.507149	val: 1.336269	test: 1.310733
MAE train: 0.388083	val: 1.029375	test: 0.997073

Epoch: 85
Loss: 0.35809047520160675
RMSE train: 0.469375	val: 1.259048	test: 1.243649
MAE train: 0.363665	val: 0.987443	test: 0.952233

Epoch: 86
Loss: 0.34873034805059433
RMSE train: 0.505225	val: 1.297218	test: 1.280263
MAE train: 0.385370	val: 1.021181	test: 0.979244

Epoch: 87
Loss: 0.37550079822540283
RMSE train: 0.565905	val: 1.299730	test: 1.306818
MAE train: 0.433707	val: 1.029382	test: 0.979837

Epoch: 88
Loss: 0.3779387027025223
RMSE train: 0.581445	val: 1.425523	test: 1.387342
MAE train: 0.436930	val: 1.104959	test: 1.064616

Epoch: 89
Loss: 0.35613782703876495
RMSE train: 0.531959	val: 1.383019	test: 1.357309
MAE train: 0.401158	val: 1.080591	test: 1.036001

Epoch: 90
Loss: 0.36481087654829025
RMSE train: 0.575835	val: 1.382950	test: 1.361540
MAE train: 0.436146	val: 1.079072	test: 1.024471

Epoch: 91
Loss: 0.33951982855796814
RMSE train: 0.556154	val: 1.346297	test: 1.317956
MAE train: 0.426328	val: 1.042324	test: 0.977537

Epoch: 92
Loss: 0.37586162239313126
RMSE train: 0.542862	val: 1.317137	test: 1.288807
MAE train: 0.421170	val: 1.024957	test: 0.969753

Epoch: 93
Loss: 0.40181611478328705
RMSE train: 0.614607	val: 1.370153	test: 1.348669
MAE train: 0.472413	val: 1.060105	test: 1.004980

Epoch: 94
Loss: 0.33480358868837357
RMSE train: 0.602705	val: 1.491703	test: 1.408327
MAE train: 0.458547	val: 1.124857	test: 1.062465

Epoch: 95
Loss: 0.3541029542684555
RMSE train: 0.527331	val: 1.246429	test: 1.265174
MAE train: 0.406384	val: 1.008849	test: 0.934529

Epoch: 96
Loss: 0.3613612726330757
RMSE train: 0.575406	val: 1.372615	test: 1.355009
MAE train: 0.428725	val: 1.055388	test: 0.996753

Epoch: 97
Loss: 0.36320292949676514
RMSE train: 0.576072	val: 1.368005	test: 1.344521
MAE train: 0.428015	val: 1.065018	test: 1.000930

Epoch: 98
Loss: 0.31331486999988556
RMSE train: 0.479981	val: 1.332143	test: 1.311930
MAE train: 0.370948	val: 1.043212	test: 0.993074

Epoch: 99
Loss: 0.3502803072333336
RMSE train: 0.502500	val: 1.464509	test: 1.410140
MAE train: 0.383562	val: 1.109450	test: 1.068338

Epoch: 100
Loss: 0.34435269236564636
RMSE train: 0.495760	val: 1.306252	test: 1.281897
MAE train: 0.383939	val: 1.020514	test: 0.974925

Epoch: 101
Loss: 0.3093169927597046
RMSE train: 0.472807	val: 1.290705	test: 1.268363
MAE train: 0.369035	val: 1.016653	test: 0.974318

Epoch: 102
Loss: 0.3054675906896591
RMSE train: 0.556488	val: 1.384429	test: 1.352882
MAE train: 0.418803	val: 1.064519	test: 1.028347

Epoch: 103
Loss: 0.3005887567996979
RMSE train: 0.551963	val: 1.290053	test: 1.323279
MAE train: 0.417897	val: 1.027825	test: 0.983859

Epoch: 104
Loss: 0.32097988575696945
RMSE train: 0.515270	val: 1.370185	test: 1.349914
MAE train: 0.387182	val: 1.064757	test: 1.024763

Epoch: 105
Loss: 0.3214610069990158
RMSE train: 0.476476	val: 1.262404	test: 1.282838
MAE train: 0.364073	val: 1.034109	test: 0.958346

Epoch: 106
Loss: 0.30365993827581406
RMSE train: 0.627913	val: 1.468580	test: 1.433892
MAE train: 0.454104	val: 1.136581	test: 1.075573

Epoch: 107
Loss: 0.3670317530632019
RMSE train: 0.632838	val: 1.420138	test: 1.407709
MAE train: 0.456686	val: 1.116278	test: 1.035742

Epoch: 108
Loss: 0.327914334833622
RMSE train: 0.542928	val: 1.253508	test: 1.301248
MAE train: 0.409408	val: 1.040750	test: 0.951336

Epoch: 109
Loss: 0.2951681390404701
RMSE train: 0.502097	val: 1.394606	test: 1.358358
MAE train: 0.381223	val: 1.095637	test: 1.041644

Epoch: 110
Loss: 0.29105889052152634
RMSE train: 0.462781	val: 1.229139	test: 1.257500
MAE train: 0.361552	val: 0.998980	test: 0.979420

Epoch: 111
Loss: 0.3413674458861351
RMSE train: 0.532957	val: 1.247604	test: 1.287850
MAE train: 0.413567	val: 1.008996	test: 0.988023

Epoch: 112
Loss: 0.3091810345649719
RMSE train: 0.538208	val: 1.470897	test: 1.449513
MAE train: 0.396055	val: 1.137035	test: 1.119835

Epoch: 113
Loss: 0.33322257548570633
RMSE train: 0.517352	val: 1.266077	test: 1.310465
MAE train: 0.392728	val: 1.033609	test: 0.977718

Epoch: 114
Loss: 0.31675318628549576
RMSE train: 0.555865	val: 1.366788	test: 1.365662
MAE train: 0.420937	val: 1.043900	test: 1.017924

Epoch: 115
Loss: 0.3639647886157036
RMSE train: 0.562525	val: 1.461751	test: 1.416495
MAE train: 0.420238	val: 1.098775	test: 1.072064

Epoch: 116
Loss: 0.2903964929282665
RMSE train: 0.526383	val: 1.157093	test: 1.224438
MAE train: 0.403068	val: 0.955251	test: 0.904917

Epoch: 117
Loss: 0.32051799818873405
RMSE train: 0.535769	val: 1.420422	test: 1.375255
MAE train: 0.402652	val: 1.094515	test: 1.043134

Epoch: 118
Loss: 0.3009164556860924
RMSE train: 0.533955	val: 1.365004	test: 1.342314
MAE train: 0.403800	val: 1.057177	test: 1.016555

Epoch: 119
Loss: 0.2768840752542019
RMSE train: 0.461057	val: 1.259944	test: 1.268840
MAE train: 0.357856	val: 1.002702	test: 0.969270

Epoch: 120
Loss: 0.3095431253314018
RMSE train: 0.519775	val: 1.442386	test: 1.398168
MAE train: 0.393408	val: 1.108254	test: 1.069859

Epoch: 121
Loss: 0.2881436049938202
RMSE train: 0.477622	val: 1.268539	test: 1.268468
MAE train: 0.376269	val: 1.017691	test: 0.953678

Epoch: 122
Loss: 0.2877974882721901
RMSE train: 0.453539	val: 1.238046	test: 1.239276
MAE train: 0.357676	val: 1.010018	test: 0.942659

Epoch: 123
Loss: 0.27701471373438835
RMSE train: 0.491135	val: 1.413285	test: 1.358398
MAE train: 0.376822	val: 1.094574	test: 1.035056

Epoch: 124
Loss: 0.34472136199474335
RMSE train: 0.485162	val: 1.334739	test: 1.315637
MAE train: 0.382010	val: 1.038201	test: 0.986452

Epoch: 125
Loss: 0.2793532498180866
RMSE train: 0.482110	val: 1.221020	test: 1.237632
MAE train: 0.379498	val: 0.974521	test: 0.941725

Epoch: 126
Loss: 0.28678521513938904
RMSE train: 0.541780	val: 1.436414	test: 1.372919
MAE train: 0.399725	val: 1.097384	test: 1.054758

Epoch: 127
Loss: 0.2691425606608391
RMSE train: 0.511237	val: 1.339734	test: 1.341015
MAE train: 0.378174	val: 1.033798	test: 0.989813

Epoch: 128
Loss: 0.2748701274394989
RMSE train: 0.462779	val: 1.325145	test: 1.331113
MAE train: 0.354674	val: 1.033201	test: 0.966492

Epoch: 129
Loss: 0.2662573270499706
RMSE train: 0.543781	val: 1.432639	test: 1.392720
MAE train: 0.412557	val: 1.082084	test: 1.020936

Epoch: 130
Loss: 0.2425958439707756
RMSE train: 0.524263	val: 1.358733	test: 1.328990
MAE train: 0.403884	val: 1.046962	test: 0.988963

Epoch: 131
Loss: 0.2531032972037792
RMSE train: 0.532762	val: 1.372292	test: 1.348474
MAE train: 0.403629	val: 1.054506	test: 0.999424

Epoch: 132
Loss: 0.2634879946708679
RMSE train: 0.605310	val: 1.438117	test: 1.401568
MAE train: 0.443590	val: 1.097239	test: 1.037112

Epoch: 133
Loss: 0.25217435136437416
RMSE train: 0.532974	val: 1.394278	test: 1.350977
MAE train: 0.396217	val: 1.062517	test: 1.023746

Epoch: 134
Loss: 0.22747085988521576
RMSE train: 0.469810	val: 1.334560	test: 1.312951
MAE train: 0.358390	val: 1.034297	test: 1.007050

Epoch: 135
Loss: 0.24079974368214607
RMSE train: 0.475289	val: 1.311356	test: 1.305173
MAE train: 0.366894	val: 1.028417	test: 0.990693

Epoch: 136
Loss: 0.2879943922162056
RMSE train: 0.500219	val: 1.320612	test: 1.303041
MAE train: 0.391578	val: 1.026853	test: 0.998715

Epoch: 137
Loss: 0.27339015528559685
RMSE train: 0.427110	val: 1.401827	test: 1.369887
MAE train: 0.335271	val: 1.120622	test: 1.076034

Epoch: 138
Loss: 0.2196677289903164
RMSE train: 0.412591	val: 1.259772	test: 1.259435
MAE train: 0.323888	val: 0.998383	test: 0.976134

Epoch: 139
Loss: 0.3150339610874653
RMSE train: 0.472754	val: 1.327713	test: 1.311008
MAE train: 0.369055	val: 1.031111	test: 1.004453

Epoch: 140
Loss: 0.26437920331954956
RMSE train: 0.522499	val: 1.404771	test: 1.362192
MAE train: 0.393344	val: 1.076220	test: 1.034611

Epoch: 141
Loss: 0.22845294699072838
RMSE train: 0.481853	val: 1.260143	test: 1.271776
MAE train: 0.365253	val: 1.001159	test: 0.964643

Epoch: 142
Loss: 0.25532690063118935
RMSE train: 0.440550	val: 1.303685	test: 1.293292
MAE train: 0.338276	val: 1.022737	test: 0.994554

Epoch: 143
Loss: 0.25070155411958694
RMSE train: 0.442653	val: 1.352432	test: 1.341487
MAE train: 0.330667	val: 1.048323	test: 1.026980
MAE train: 0.475041	val: 1.191198	test: 1.178396

Epoch: 84
Loss: 0.33777502179145813
RMSE train: 0.535980	val: 1.458828	test: 1.518506
MAE train: 0.421979	val: 1.139005	test: 1.105492

Epoch: 85
Loss: 0.35071171820163727
RMSE train: 0.548701	val: 1.486643	test: 1.520770
MAE train: 0.425835	val: 1.147712	test: 1.102538

Epoch: 86
Loss: 0.36342234164476395
RMSE train: 0.556981	val: 1.553873	test: 1.562089
MAE train: 0.425875	val: 1.167535	test: 1.129990

Epoch: 87
Loss: 0.37239981442689896
RMSE train: 0.547756	val: 1.540894	test: 1.558401
MAE train: 0.425029	val: 1.156972	test: 1.123582

Epoch: 88
Loss: 0.3554658740758896
RMSE train: 0.559102	val: 1.598181	test: 1.556677
MAE train: 0.432397	val: 1.187551	test: 1.135697

Epoch: 89
Loss: 0.3450518324971199
RMSE train: 0.547186	val: 1.605899	test: 1.576709
MAE train: 0.422550	val: 1.194823	test: 1.144712

Epoch: 90
Loss: 0.33911698311567307
RMSE train: 0.545124	val: 1.553727	test: 1.527163
MAE train: 0.416413	val: 1.159483	test: 1.098491

Epoch: 91
Loss: 0.3275427743792534
RMSE train: 0.515326	val: 1.529894	test: 1.482893
MAE train: 0.398290	val: 1.151175	test: 1.075509

Epoch: 92
Loss: 0.3484899625182152
RMSE train: 0.567983	val: 1.543843	test: 1.498608
MAE train: 0.434352	val: 1.154182	test: 1.081459

Epoch: 93
Loss: 0.35194994509220123
RMSE train: 0.539341	val: 1.456671	test: 1.458526
MAE train: 0.419780	val: 1.102446	test: 1.054100

Epoch: 94
Loss: 0.33689557760953903
RMSE train: 0.587189	val: 1.653228	test: 1.565667
MAE train: 0.449811	val: 1.215597	test: 1.158987

Epoch: 95
Loss: 0.3438275009393692
RMSE train: 0.636060	val: 1.554223	test: 1.567895
MAE train: 0.501371	val: 1.156497	test: 1.140481

Epoch: 96
Loss: 0.3545367270708084
RMSE train: 0.652616	val: 1.533001	test: 1.592118
MAE train: 0.510118	val: 1.172490	test: 1.152202

Epoch: 97
Loss: 0.35673753172159195
RMSE train: 0.644467	val: 1.572577	test: 1.604057
MAE train: 0.494316	val: 1.177881	test: 1.169509

Epoch: 98
Loss: 0.30450544506311417
RMSE train: 0.589863	val: 1.588542	test: 1.576110
MAE train: 0.456587	val: 1.197834	test: 1.175213

Epoch: 99
Loss: 0.3246275708079338
RMSE train: 0.596646	val: 1.749331	test: 1.618345
MAE train: 0.461145	val: 1.309476	test: 1.212571

Epoch: 100
Loss: 0.33226118981838226
RMSE train: 0.607588	val: 1.654092	test: 1.633383
MAE train: 0.472763	val: 1.241530	test: 1.226016

Epoch: 101
Loss: 0.33305035531520844
RMSE train: 0.515434	val: 1.478244	test: 1.533337
MAE train: 0.406107	val: 1.128000	test: 1.139571

Epoch: 102
Loss: 0.3374238610267639
RMSE train: 0.593818	val: 1.576992	test: 1.544351
MAE train: 0.441141	val: 1.163556	test: 1.144607

Epoch: 103
Loss: 0.31210170686244965
RMSE train: 0.524697	val: 1.461903	test: 1.473879
MAE train: 0.398295	val: 1.086498	test: 1.084576

Epoch: 104
Loss: 0.28475745022296906
RMSE train: 0.423197	val: 1.441472	test: 1.451871
MAE train: 0.334134	val: 1.075451	test: 1.092923

Epoch: 105
Loss: 0.33427929133176804
RMSE train: 0.465639	val: 1.479270	test: 1.508897
MAE train: 0.365052	val: 1.095106	test: 1.119866

Epoch: 106
Loss: 0.31212449818849564
RMSE train: 0.504706	val: 1.463346	test: 1.458642
MAE train: 0.385982	val: 1.084590	test: 1.091531

Epoch: 107
Loss: 0.30775172263383865
RMSE train: 0.521262	val: 1.517344	test: 1.514313
MAE train: 0.382381	val: 1.117006	test: 1.129673

Epoch: 108
Loss: 0.29325616359710693
RMSE train: 0.487209	val: 1.434189	test: 1.498327
MAE train: 0.372060	val: 1.111602	test: 1.098257

Epoch: 109
Loss: 0.2731042355298996
RMSE train: 0.419246	val: 1.554529	test: 1.487217
MAE train: 0.328780	val: 1.174080	test: 1.115461

Epoch: 110
Loss: 0.2813549339771271
RMSE train: 0.447179	val: 1.451020	test: 1.404865
MAE train: 0.352042	val: 1.089497	test: 1.043654

Epoch: 111
Loss: 0.3284391313791275
RMSE train: 0.554379	val: 1.454599	test: 1.484945
MAE train: 0.421485	val: 1.106259	test: 1.068858

Epoch: 112
Loss: 0.30357129871845245
RMSE train: 0.493423	val: 1.596564	test: 1.535842
MAE train: 0.367619	val: 1.159063	test: 1.126952

Epoch: 113
Loss: 0.3010910525918007
RMSE train: 0.478048	val: 1.426962	test: 1.477250
MAE train: 0.361127	val: 1.081601	test: 1.072648

Epoch: 114
Loss: 0.3100637122988701
RMSE train: 0.537122	val: 1.560845	test: 1.582600
MAE train: 0.416701	val: 1.164243	test: 1.141845

Epoch: 115
Loss: 0.35769473761320114
RMSE train: 0.632510	val: 1.707140	test: 1.658126
MAE train: 0.479757	val: 1.257531	test: 1.214036

Epoch: 116
Loss: 0.29402966797351837
RMSE train: 0.485790	val: 1.333078	test: 1.410542
MAE train: 0.379594	val: 1.044376	test: 1.036270

Epoch: 117
Loss: 0.2977322190999985
RMSE train: 0.572288	val: 1.613780	test: 1.629660
MAE train: 0.440497	val: 1.205590	test: 1.190823

Epoch: 118
Loss: 0.2685100808739662
RMSE train: 0.515915	val: 1.566913	test: 1.560175
MAE train: 0.399946	val: 1.162723	test: 1.121866

Epoch: 119
Loss: 0.24700549617409706
RMSE train: 0.421776	val: 1.443167	test: 1.421792
MAE train: 0.336535	val: 1.080301	test: 1.042756

Epoch: 120
Loss: 0.29552531242370605
RMSE train: 0.505435	val: 1.570417	test: 1.537482
MAE train: 0.393730	val: 1.154110	test: 1.130577

Epoch: 121
Loss: 0.2988433912396431
RMSE train: 0.485314	val: 1.385803	test: 1.444849
MAE train: 0.374251	val: 1.059412	test: 1.047499

Epoch: 122
Loss: 0.26959456503391266
RMSE train: 0.424347	val: 1.426723	test: 1.424515
MAE train: 0.322855	val: 1.058314	test: 1.046511

Epoch: 123
Loss: 0.2692970745265484
RMSE train: 0.474318	val: 1.578368	test: 1.527357
MAE train: 0.363201	val: 1.154634	test: 1.113829

Epoch: 124
Loss: 0.319926667958498
RMSE train: 0.435788	val: 1.461300	test: 1.473075
MAE train: 0.340482	val: 1.107425	test: 1.080958

Epoch: 125
Loss: 0.26859113946557045
RMSE train: 0.421939	val: 1.393869	test: 1.424866
MAE train: 0.330325	val: 1.073103	test: 1.045946

Epoch: 126
Loss: 0.27762240916490555
RMSE train: 0.543422	val: 1.598652	test: 1.589936
MAE train: 0.416080	val: 1.183531	test: 1.156537

Epoch: 127
Loss: 0.23349867016077042
RMSE train: 0.523922	val: 1.531802	test: 1.557689
MAE train: 0.397653	val: 1.168926	test: 1.151154

Epoch: 128
Loss: 0.2434442825615406
RMSE train: 0.533807	val: 1.551955	test: 1.577650
MAE train: 0.402922	val: 1.185645	test: 1.173314

Epoch: 129
Loss: 0.2559530436992645
RMSE train: 0.532912	val: 1.522937	test: 1.538748
MAE train: 0.404085	val: 1.149052	test: 1.128506

Epoch: 130
Loss: 0.2193748988211155
RMSE train: 0.503279	val: 1.465260	test: 1.485722
MAE train: 0.386029	val: 1.094674	test: 1.081597

Epoch: 131
Loss: 0.2567794770002365
RMSE train: 0.588793	val: 1.610926	test: 1.643714
MAE train: 0.437575	val: 1.194314	test: 1.207680

Epoch: 132
Loss: 0.2551906853914261
RMSE train: 0.566361	val: 1.558156	test: 1.625871
MAE train: 0.427029	val: 1.188004	test: 1.187065

Epoch: 133
Loss: 0.2559904009103775
RMSE train: 0.471005	val: 1.584848	test: 1.561680
MAE train: 0.356657	val: 1.174771	test: 1.172794

Epoch: 134
Loss: 0.23236488923430443
RMSE train: 0.476095	val: 1.513013	test: 1.517200
MAE train: 0.366112	val: 1.124726	test: 1.103303

Epoch: 135
Loss: 0.2339952401816845
RMSE train: 0.489629	val: 1.501282	test: 1.521942
MAE train: 0.371889	val: 1.144607	test: 1.106271

Epoch: 136
Loss: 0.25899725034832954
RMSE train: 0.440206	val: 1.548372	test: 1.491808
MAE train: 0.332947	val: 1.147484	test: 1.087964

Epoch: 137
Loss: 0.2747640460729599
RMSE train: 0.426081	val: 1.409769	test: 1.385200
MAE train: 0.328854	val: 1.066284	test: 1.032286

Epoch: 138
Loss: 0.23194466158747673
RMSE train: 0.520247	val: 1.517768	test: 1.521948
MAE train: 0.390752	val: 1.128000	test: 1.127553

Epoch: 139
Loss: 0.2955375052988529
RMSE train: 0.511310	val: 1.561612	test: 1.539570
MAE train: 0.381953	val: 1.157476	test: 1.128398

Epoch: 140
Loss: 0.2715645730495453
RMSE train: 0.469741	val: 1.462890	test: 1.469737
MAE train: 0.358783	val: 1.080413	test: 1.069608

Epoch: 141
Loss: 0.21390045061707497
RMSE train: 0.442906	val: 1.435414	test: 1.453767
MAE train: 0.336731	val: 1.072257	test: 1.065940

Epoch: 142
Loss: 0.2521776780486107
RMSE train: 0.413642	val: 1.404609	test: 1.387494
MAE train: 0.318159	val: 1.059338	test: 1.027634

Epoch: 143
Loss: 0.222006194293499
RMSE train: 0.434972	val: 1.463923	test: 1.401211
MAE train: 0.331312	val: 1.103208	test: 1.050728
MAE train: 0.454524	val: 1.041400	test: 1.092558

Epoch: 84
Loss: 0.37987084686756134
RMSE train: 0.531073	val: 1.204112	test: 1.370691
MAE train: 0.409346	val: 1.006927	test: 1.052865

Epoch: 85
Loss: 0.42858733236789703
RMSE train: 0.576486	val: 1.280744	test: 1.395367
MAE train: 0.442488	val: 1.033665	test: 1.076552

Epoch: 86
Loss: 0.3841847702860832
RMSE train: 0.540207	val: 1.257429	test: 1.389242
MAE train: 0.413707	val: 1.015366	test: 1.094583

Epoch: 87
Loss: 0.3456398621201515
RMSE train: 0.475961	val: 1.227590	test: 1.392245
MAE train: 0.361965	val: 1.024748	test: 1.092917

Epoch: 88
Loss: 0.3628024682402611
RMSE train: 0.558149	val: 1.307683	test: 1.419730
MAE train: 0.432427	val: 1.047958	test: 1.109054

Epoch: 89
Loss: 0.3518115133047104
RMSE train: 0.534449	val: 1.191715	test: 1.328549
MAE train: 0.408854	val: 0.984169	test: 1.054345

Epoch: 90
Loss: 0.38789229094982147
RMSE train: 0.499112	val: 1.157188	test: 1.312407
MAE train: 0.382080	val: 0.980044	test: 1.049703

Epoch: 91
Loss: 0.3711801916360855
RMSE train: 0.582723	val: 1.355511	test: 1.492815
MAE train: 0.451333	val: 1.123720	test: 1.159045

Epoch: 92
Loss: 0.3407459706068039
RMSE train: 0.520872	val: 1.280995	test: 1.463658
MAE train: 0.391918	val: 1.072289	test: 1.166960

Epoch: 93
Loss: 0.3469855263829231
RMSE train: 0.470335	val: 1.252710	test: 1.452979
MAE train: 0.366858	val: 1.042263	test: 1.167562

Epoch: 94
Loss: 0.3728106766939163
RMSE train: 0.587404	val: 1.354943	test: 1.502134
MAE train: 0.455540	val: 1.104244	test: 1.175195

Epoch: 95
Loss: 0.47433875501155853
RMSE train: 0.593235	val: 1.424449	test: 1.554324
MAE train: 0.439343	val: 1.173087	test: 1.212972

Epoch: 96
Loss: 0.36825039237737656
RMSE train: 0.537707	val: 1.519678	test: 1.608753
MAE train: 0.408785	val: 1.218196	test: 1.260057

Epoch: 97
Loss: 0.36531632393598557
RMSE train: 0.605069	val: 1.305747	test: 1.430085
MAE train: 0.470219	val: 1.066975	test: 1.117708

Epoch: 98
Loss: 0.33967822045087814
RMSE train: 0.641929	val: 1.318856	test: 1.434854
MAE train: 0.498318	val: 1.090700	test: 1.102046

Epoch: 99
Loss: 0.34738142788410187
RMSE train: 0.571681	val: 1.252874	test: 1.379836
MAE train: 0.446875	val: 1.035581	test: 1.074215

Epoch: 100
Loss: 0.2993774265050888
RMSE train: 0.582613	val: 1.198583	test: 1.342853
MAE train: 0.451606	val: 1.003985	test: 1.044917

Epoch: 101
Loss: 0.33612680435180664
RMSE train: 0.611857	val: 1.245118	test: 1.370598
MAE train: 0.468931	val: 1.035554	test: 1.049740

Epoch: 102
Loss: 0.30669622868299484
RMSE train: 0.505071	val: 1.173467	test: 1.310781
MAE train: 0.390255	val: 0.975803	test: 1.019997

Epoch: 103
Loss: 0.33838099241256714
RMSE train: 0.562476	val: 1.279188	test: 1.370622
MAE train: 0.437991	val: 1.039804	test: 1.054660

Epoch: 104
Loss: 0.33433452993631363
RMSE train: 0.511786	val: 1.253290	test: 1.341791
MAE train: 0.399915	val: 1.019671	test: 1.053853

Epoch: 105
Loss: 0.3078029900789261
RMSE train: 0.511280	val: 1.277440	test: 1.393524
MAE train: 0.398537	val: 1.069945	test: 1.083640

Epoch: 106
Loss: 0.31580889970064163
RMSE train: 0.583913	val: 1.408202	test: 1.521496
MAE train: 0.455634	val: 1.138082	test: 1.161661

Epoch: 107
Loss: 0.3415578678250313
RMSE train: 0.540228	val: 1.266860	test: 1.427780
MAE train: 0.424809	val: 1.067524	test: 1.119723

Epoch: 108
Loss: 0.3078334704041481
RMSE train: 0.452230	val: 1.222718	test: 1.429166
MAE train: 0.359254	val: 1.030371	test: 1.128130

Epoch: 109
Loss: 0.3431427925825119
RMSE train: 0.492312	val: 1.322456	test: 1.505859
MAE train: 0.391998	val: 1.087105	test: 1.174952

Epoch: 110
Loss: 0.3013867437839508
RMSE train: 0.557664	val: 1.305807	test: 1.435800
MAE train: 0.426675	val: 1.087105	test: 1.114242

Epoch: 111
Loss: 0.3198761120438576
RMSE train: 0.511942	val: 1.305638	test: 1.413335
MAE train: 0.397417	val: 1.064286	test: 1.106110

Epoch: 112
Loss: 0.3526524007320404
RMSE train: 0.448340	val: 1.253627	test: 1.353265
MAE train: 0.346414	val: 1.050879	test: 1.064785

Epoch: 113
Loss: 0.3208024129271507
RMSE train: 0.508033	val: 1.354165	test: 1.445016
MAE train: 0.395594	val: 1.121860	test: 1.136255

Epoch: 114
Loss: 0.29010673612356186
RMSE train: 0.490346	val: 1.318153	test: 1.417450
MAE train: 0.386153	val: 1.077526	test: 1.113045

Epoch: 115
Loss: 0.31053753197193146
RMSE train: 0.480625	val: 1.281934	test: 1.374516
MAE train: 0.376934	val: 1.034835	test: 1.078708

Epoch: 116
Loss: 0.30567771941423416
RMSE train: 0.527393	val: 1.265570	test: 1.374914
MAE train: 0.406010	val: 1.029390	test: 1.056851

Epoch: 117
Loss: 0.2715432941913605
RMSE train: 0.522864	val: 1.334933	test: 1.413438
MAE train: 0.403829	val: 1.083006	test: 1.133756

Epoch: 118
Loss: 0.29030753672122955
RMSE train: 0.580155	val: 1.268312	test: 1.400375
MAE train: 0.452845	val: 1.012353	test: 1.097995

Epoch: 119
Loss: 0.2998554855585098
RMSE train: 0.496582	val: 1.173520	test: 1.337272
MAE train: 0.389521	val: 0.961787	test: 1.062420

Epoch: 120
Loss: 0.3224092200398445
RMSE train: 0.586852	val: 1.318572	test: 1.424486
MAE train: 0.453879	val: 1.061906	test: 1.123115

Epoch: 121
Loss: 0.30482812225818634
RMSE train: 0.589251	val: 1.178348	test: 1.327106
MAE train: 0.448569	val: 0.947917	test: 1.040899

Epoch: 122
Loss: 0.2762584909796715
RMSE train: 0.478140	val: 1.056233	test: 1.270363
MAE train: 0.365372	val: 0.880764	test: 1.020703

Epoch: 123
Loss: 0.2892669588327408
RMSE train: 0.525913	val: 1.344186	test: 1.459120
MAE train: 0.404064	val: 1.084996	test: 1.156292

Epoch: 124
Loss: 0.32112785428762436
RMSE train: 0.577091	val: 1.268220	test: 1.412677
MAE train: 0.438658	val: 1.027704	test: 1.095138

Epoch: 125
Loss: 0.27003196626901627
RMSE train: 0.516338	val: 1.230523	test: 1.396639
MAE train: 0.384468	val: 1.028724	test: 1.085613

Epoch: 126
Loss: 0.2923549711704254
RMSE train: 0.476620	val: 1.247650	test: 1.395675
MAE train: 0.361917	val: 1.043578	test: 1.089949

Epoch: 127
Loss: 0.2731994315981865
RMSE train: 0.520259	val: 1.306901	test: 1.438843
MAE train: 0.407614	val: 1.070503	test: 1.128814

Epoch: 128
Loss: 0.2954583466053009
RMSE train: 0.526353	val: 1.274208	test: 1.413415
MAE train: 0.405062	val: 1.062878	test: 1.118211

Epoch: 129
Loss: 0.2772926576435566
RMSE train: 0.489454	val: 1.257271	test: 1.421343
MAE train: 0.371782	val: 1.052721	test: 1.135174

Epoch: 130
Loss: 0.3194766789674759
RMSE train: 0.468097	val: 1.311929	test: 1.495109
MAE train: 0.360883	val: 1.086642	test: 1.183396

Epoch: 131
Loss: 0.23070812970399857
RMSE train: 0.462966	val: 1.374693	test: 1.531603
MAE train: 0.345957	val: 1.149202	test: 1.208003

Epoch: 132
Loss: 0.25292814522981644
RMSE train: 0.435495	val: 1.329909	test: 1.470207
MAE train: 0.330424	val: 1.124936	test: 1.159930

Epoch: 133
Loss: 0.24924179911613464
RMSE train: 0.457579	val: 1.328096	test: 1.450881
MAE train: 0.353272	val: 1.075199	test: 1.144811

Epoch: 134
Loss: 0.2648274041712284
RMSE train: 0.518902	val: 1.248244	test: 1.375741
MAE train: 0.386276	val: 1.035028	test: 1.063110

Epoch: 135
Loss: 0.2651263475418091
RMSE train: 0.493319	val: 1.245760	test: 1.379121
MAE train: 0.375582	val: 1.018307	test: 1.059919

Epoch: 136
Loss: 0.2610264867544174
RMSE train: 0.461754	val: 1.260069	test: 1.389046
MAE train: 0.364018	val: 1.032305	test: 1.095897

Epoch: 137
Loss: 0.2605530694127083
RMSE train: 0.533405	val: 1.208627	test: 1.394527
MAE train: 0.409920	val: 0.997604	test: 1.096138

Epoch: 138
Loss: 0.2844226583838463
RMSE train: 0.538713	val: 1.303187	test: 1.477611
MAE train: 0.410139	val: 1.082483	test: 1.136985

Epoch: 139
Loss: 0.3052702359855175
RMSE train: 0.487884	val: 1.295679	test: 1.489236
MAE train: 0.369003	val: 1.063713	test: 1.151424

Epoch: 140
Loss: 0.26994094997644424
RMSE train: 0.540282	val: 1.204156	test: 1.409618
MAE train: 0.398083	val: 1.010367	test: 1.102483

Epoch: 141
Loss: 0.26715007424354553
RMSE train: 0.513492	val: 1.308191	test: 1.462883
MAE train: 0.404360	val: 1.050788	test: 1.152010

Epoch: 142
Loss: 0.2776057906448841
RMSE train: 0.459370	val: 1.199850	test: 1.370793
MAE train: 0.356990	val: 0.996336	test: 1.067129

Epoch: 143
Loss: 0.25979363173246384
RMSE train: 0.514740	val: 1.240753	test: 1.412941
MAE train: 0.378615	val: 1.037258	test: 1.090872

Epoch: 144
Loss: 0.2488229051232338
RMSE train: 0.475515	val: 1.297623	test: 1.315294
MAE train: 0.356498	val: 1.022951	test: 1.002751

Epoch: 145
Loss: 0.22133273258805275
RMSE train: 0.516746	val: 1.475180	test: 1.415412
MAE train: 0.380396	val: 1.136925	test: 1.081172

Epoch: 146
Loss: 0.29355333000421524
RMSE train: 0.483607	val: 1.328783	test: 1.340607
MAE train: 0.370130	val: 1.034137	test: 1.020883

Epoch: 147
Loss: 0.2466246448457241
RMSE train: 0.514966	val: 1.370754	test: 1.368087
MAE train: 0.401479	val: 1.077982	test: 1.025297

Epoch: 148
Loss: 0.25559699535369873
RMSE train: 0.407092	val: 1.321374	test: 1.306933
MAE train: 0.319463	val: 1.059400	test: 1.006633

Epoch: 149
Loss: 0.21527772396802902
RMSE train: 0.424744	val: 1.285701	test: 1.268717
MAE train: 0.328319	val: 1.037407	test: 0.976532

Epoch: 150
Loss: 0.2286204695701599
RMSE train: 0.478153	val: 1.280525	test: 1.269547
MAE train: 0.369432	val: 1.024850	test: 0.966391

Epoch: 151
Loss: 0.20857606828212738
RMSE train: 0.436719	val: 1.228752	test: 1.245571
MAE train: 0.340145	val: 0.994096	test: 0.932126

Early stopping
Best (RMSE):	 train: 0.526383	val: 1.157093	test: 1.224438
Best (MAE):	 train: 0.403068	val: 0.955251	test: 0.904917


Epoch: 144
Loss: 0.23568829894065857
RMSE train: 0.436574	val: 1.448658	test: 1.428686
MAE train: 0.334440	val: 1.088256	test: 1.044409

Epoch: 145
Loss: 0.22821945697069168
RMSE train: 0.492207	val: 1.575297	test: 1.500004
MAE train: 0.369827	val: 1.171025	test: 1.103434

Epoch: 146
Loss: 0.26946284994482994
RMSE train: 0.472470	val: 1.548595	test: 1.503638
MAE train: 0.356530	val: 1.145217	test: 1.089753

Epoch: 147
Loss: 0.24346542730927467
RMSE train: 0.568513	val: 1.649768	test: 1.682128
MAE train: 0.426959	val: 1.251187	test: 1.224279

Epoch: 148
Loss: 0.2754347026348114
RMSE train: 0.507623	val: 1.628401	test: 1.573838
MAE train: 0.379027	val: 1.215754	test: 1.168029

Epoch: 149
Loss: 0.21273991838097572
RMSE train: 0.525624	val: 1.657994	test: 1.585332
MAE train: 0.388634	val: 1.243807	test: 1.188283

Epoch: 150
Loss: 0.2475040704011917
RMSE train: 0.465965	val: 1.479726	test: 1.491105
MAE train: 0.351110	val: 1.126667	test: 1.073068

Epoch: 151
Loss: 0.22021763399243355
RMSE train: 0.487304	val: 1.542207	test: 1.522441
MAE train: 0.363437	val: 1.144130	test: 1.086962

Early stopping
Best (RMSE):	 train: 0.485790	val: 1.333078	test: 1.410542
Best (MAE):	 train: 0.379594	val: 1.044376	test: 1.036270
All runs completed.


Epoch: 144
Loss: 0.2771771177649498
RMSE train: 0.497155	val: 1.252982	test: 1.417862
MAE train: 0.382636	val: 1.046933	test: 1.093128

Epoch: 145
Loss: 0.2507886365056038
RMSE train: 0.442470	val: 1.180542	test: 1.378413
MAE train: 0.343696	val: 0.991704	test: 1.081761

Epoch: 146
Loss: 0.24350152909755707
RMSE train: 0.569874	val: 1.315496	test: 1.456254
MAE train: 0.443072	val: 1.075186	test: 1.128764

Epoch: 147
Loss: 0.24729183688759804
RMSE train: 0.443998	val: 1.170333	test: 1.340227
MAE train: 0.344814	val: 0.981390	test: 1.050711

Epoch: 148
Loss: 0.26418379321694374
RMSE train: 0.441358	val: 1.213060	test: 1.377553
MAE train: 0.343746	val: 1.024426	test: 1.077623

Epoch: 149
Loss: 0.24831288307905197
RMSE train: 0.446746	val: 1.206663	test: 1.362372
MAE train: 0.347830	val: 1.020546	test: 1.070128

Epoch: 150
Loss: 0.2128598988056183
RMSE train: 0.383797	val: 1.175953	test: 1.361351
MAE train: 0.311263	val: 0.988332	test: 1.087785

Epoch: 151
Loss: 0.22474053874611855
RMSE train: 0.371030	val: 1.209964	test: 1.360546
MAE train: 0.298266	val: 1.010179	test: 1.078546

Epoch: 152
Loss: 0.2241579331457615
RMSE train: 0.473511	val: 1.249421	test: 1.377628
MAE train: 0.356380	val: 1.054369	test: 1.066485

Epoch: 153
Loss: 0.26693031936883926
RMSE train: 0.410283	val: 1.204286	test: 1.358465
MAE train: 0.314393	val: 1.029462	test: 1.047834

Epoch: 154
Loss: 0.2346816062927246
RMSE train: 0.380547	val: 1.271970	test: 1.443533
MAE train: 0.301362	val: 1.060037	test: 1.138812

Epoch: 155
Loss: 0.2579292766749859
RMSE train: 0.468886	val: 1.188140	test: 1.347381
MAE train: 0.372544	val: 0.977366	test: 1.068674

Epoch: 156
Loss: 0.21897400170564651
RMSE train: 0.501040	val: 1.165486	test: 1.329283
MAE train: 0.396742	val: 0.956420	test: 1.039566

Epoch: 157
Loss: 0.21936318278312683
RMSE train: 0.453112	val: 1.106203	test: 1.304670
MAE train: 0.348885	val: 0.924574	test: 1.008284

Early stopping
Best (RMSE):	 train: 0.478140	val: 1.056233	test: 1.270363
Best (MAE):	 train: 0.365372	val: 0.880764	test: 1.020703
All runs completed.

RMSE train: 0.545359	val: 1.144842	test: 1.195374
MAE train: 0.396536	val: 0.968486	test: 0.901827

Epoch: 145
Loss: 0.2727997824549675
RMSE train: 0.478911	val: 1.066512	test: 1.136579
MAE train: 0.356728	val: 0.896456	test: 0.905785

Epoch: 146
Loss: 0.29633889347314835
RMSE train: 0.458810	val: 1.250580	test: 1.241635
MAE train: 0.353902	val: 1.051762	test: 0.970010

Epoch: 147
Loss: 0.2856878899037838
RMSE train: 0.478955	val: 1.104555	test: 1.161861
MAE train: 0.372378	val: 0.915093	test: 0.901788

Epoch: 148
Loss: 0.26651934534311295
RMSE train: 0.461812	val: 1.071530	test: 1.162068
MAE train: 0.355342	val: 0.887406	test: 0.924073

Epoch: 149
Loss: 0.2576330527663231
RMSE train: 0.476708	val: 1.193233	test: 1.236334
MAE train: 0.365027	val: 0.984334	test: 0.968183

Epoch: 150
Loss: 0.2704169377684593
RMSE train: 0.477114	val: 1.163326	test: 1.192008
MAE train: 0.370554	val: 0.950208	test: 0.926547

Epoch: 151
Loss: 0.27549368515610695
RMSE train: 0.461562	val: 1.157700	test: 1.187352
MAE train: 0.356169	val: 0.954707	test: 0.923879

Epoch: 152
Loss: 0.31099289655685425
RMSE train: 0.443067	val: 1.198220	test: 1.241447
MAE train: 0.344012	val: 0.987047	test: 0.970411

Epoch: 153
Loss: 0.29172200709581375
RMSE train: 0.442451	val: 1.194455	test: 1.271026
MAE train: 0.339681	val: 0.990500	test: 0.991587

Epoch: 154
Loss: 0.2873213402926922
RMSE train: 0.482711	val: 1.188050	test: 1.232579
MAE train: 0.376298	val: 1.001179	test: 0.962762

Epoch: 155
Loss: 0.2891887463629246
RMSE train: 0.458525	val: 1.087287	test: 1.165603
MAE train: 0.344615	val: 0.910022	test: 0.917812

Epoch: 156
Loss: 0.2937197983264923
RMSE train: 0.487190	val: 1.186546	test: 1.183503
MAE train: 0.370317	val: 0.980351	test: 0.946957

Epoch: 157
Loss: 0.30209581553936005
RMSE train: 0.466589	val: 1.132409	test: 1.161904
MAE train: 0.354888	val: 0.922377	test: 0.934151

Epoch: 158
Loss: 0.32009030133485794
RMSE train: 0.450001	val: 1.147892	test: 1.183321
MAE train: 0.347786	val: 0.946866	test: 0.932144

Epoch: 159
Loss: 0.2734104320406914
RMSE train: 0.446763	val: 1.186249	test: 1.256624
MAE train: 0.341922	val: 0.992810	test: 0.977113

Epoch: 160
Loss: 0.28415946662425995
RMSE train: 0.475605	val: 1.123772	test: 1.224274
MAE train: 0.359258	val: 0.948707	test: 0.968972

Epoch: 161
Loss: 0.2740277126431465
RMSE train: 0.445334	val: 1.080332	test: 1.184511
MAE train: 0.338381	val: 0.897346	test: 0.949335

Epoch: 162
Loss: 0.2653649263083935
RMSE train: 0.456508	val: 1.138719	test: 1.218835
MAE train: 0.351087	val: 0.936018	test: 0.966767

Epoch: 163
Loss: 0.2723492197692394
RMSE train: 0.502674	val: 1.114987	test: 1.185249
MAE train: 0.389384	val: 0.915865	test: 0.932885

Epoch: 164
Loss: 0.26643140241503716
RMSE train: 0.432769	val: 1.010237	test: 1.126886
MAE train: 0.328799	val: 0.837365	test: 0.901602

Epoch: 165
Loss: 0.30093495547771454
RMSE train: 0.522117	val: 1.202891	test: 1.232654
MAE train: 0.398694	val: 0.991247	test: 0.944621

Epoch: 166
Loss: 0.2457486242055893
RMSE train: 0.510722	val: 1.081125	test: 1.182520
MAE train: 0.380734	val: 0.910833	test: 0.906446

Epoch: 167
Loss: 0.2658984027802944
RMSE train: 0.467566	val: 1.108167	test: 1.158605
MAE train: 0.354460	val: 0.933821	test: 0.904460

Epoch: 168
Loss: 0.29700250551104546
RMSE train: 0.504594	val: 1.147717	test: 1.200218
MAE train: 0.373863	val: 0.957663	test: 0.926050

Epoch: 169
Loss: 0.24566831067204475
RMSE train: 0.540004	val: 1.168171	test: 1.231682
MAE train: 0.395455	val: 0.978066	test: 0.942244

Epoch: 170
Loss: 0.2731950283050537
RMSE train: 0.461650	val: 1.151673	test: 1.202824
MAE train: 0.346043	val: 0.959895	test: 0.950202

Epoch: 171
Loss: 0.30429714918136597
RMSE train: 0.468254	val: 1.152438	test: 1.169159
MAE train: 0.357030	val: 0.965610	test: 0.950714

Epoch: 172
Loss: 0.28220342844724655
RMSE train: 0.484091	val: 1.053083	test: 1.159658
MAE train: 0.363940	val: 0.891369	test: 0.926014

Epoch: 173
Loss: 0.2582710236310959
RMSE train: 0.458373	val: 1.197257	test: 1.241434
MAE train: 0.347687	val: 1.004124	test: 0.967746

Epoch: 174
Loss: 0.2984882593154907
RMSE train: 0.455443	val: 1.243975	test: 1.267171
MAE train: 0.340045	val: 1.046630	test: 1.001419

Epoch: 175
Loss: 0.2564576417207718
RMSE train: 0.451152	val: 1.192762	test: 1.219669
MAE train: 0.343601	val: 1.006462	test: 0.984888

Epoch: 176
Loss: 0.25077711790800095
RMSE train: 0.437146	val: 1.205674	test: 1.203489
MAE train: 0.340771	val: 0.994267	test: 0.968333

Epoch: 177
Loss: 0.2643110007047653
RMSE train: 0.417904	val: 1.080432	test: 1.126767
MAE train: 0.320618	val: 0.895331	test: 0.890884

Epoch: 178
Loss: 0.2642286382615566
RMSE train: 0.437286	val: 1.159311	test: 1.143714
MAE train: 0.335098	val: 0.966382	test: 0.901260

Epoch: 179
Loss: 0.30741896852850914
RMSE train: 0.464149	val: 1.134462	test: 1.162800
MAE train: 0.344029	val: 0.953734	test: 0.906195

Epoch: 180
Loss: 0.2964984402060509
RMSE train: 0.474269	val: 1.129189	test: 1.189650
MAE train: 0.357525	val: 0.961375	test: 0.936448

Epoch: 181
Loss: 0.26534488052129745
RMSE train: 0.454018	val: 1.290689	test: 1.331412
MAE train: 0.344847	val: 1.069311	test: 1.013340

Epoch: 182
Loss: 0.2819719761610031
RMSE train: 0.430252	val: 1.157654	test: 1.194091
MAE train: 0.329053	val: 0.968348	test: 0.943849

Epoch: 183
Loss: 0.27501627802848816
RMSE train: 0.517245	val: 1.095109	test: 1.196359
MAE train: 0.379847	val: 0.923150	test: 0.933959

Epoch: 184
Loss: 0.27220889180898666
RMSE train: 0.519297	val: 1.215105	test: 1.238430
MAE train: 0.388125	val: 0.991997	test: 0.957663

Epoch: 185
Loss: 0.2870902940630913
RMSE train: 0.423634	val: 1.094614	test: 1.139907
MAE train: 0.322445	val: 0.925899	test: 0.894769

Epoch: 186
Loss: 0.2332000471651554
RMSE train: 0.464905	val: 1.119002	test: 1.161751
MAE train: 0.358669	val: 0.946800	test: 0.912266

Epoch: 187
Loss: 0.21630115061998367
RMSE train: 0.459899	val: 1.086306	test: 1.148886
MAE train: 0.351758	val: 0.911966	test: 0.898568

Epoch: 188
Loss: 0.23151906952261925
RMSE train: 0.462780	val: 1.117895	test: 1.165024
MAE train: 0.350832	val: 0.926121	test: 0.888939

Epoch: 189
Loss: 0.245611060410738
RMSE train: 0.451359	val: 1.116487	test: 1.171203
MAE train: 0.337933	val: 0.925693	test: 0.892285

Epoch: 190
Loss: 0.29388612508773804
RMSE train: 0.495745	val: 1.143317	test: 1.190330
MAE train: 0.372667	val: 0.949266	test: 0.909535

Epoch: 191
Loss: 0.2193433716893196
RMSE train: 0.476661	val: 1.083955	test: 1.169589
MAE train: 0.356932	val: 0.909652	test: 0.892641

Epoch: 192
Loss: 0.2538120895624161
RMSE train: 0.433336	val: 1.090071	test: 1.143175
MAE train: 0.332548	val: 0.914558	test: 0.881599

Epoch: 193
Loss: 0.2588929682970047
RMSE train: 0.465701	val: 1.101929	test: 1.148328
MAE train: 0.354823	val: 0.922930	test: 0.865781

Epoch: 194
Loss: 0.2285469025373459
RMSE train: 0.470243	val: 1.089996	test: 1.152477
MAE train: 0.357734	val: 0.909708	test: 0.865119

Epoch: 195
Loss: 0.24681537598371506
RMSE train: 0.456703	val: 1.105199	test: 1.144487
MAE train: 0.353159	val: 0.924436	test: 0.881097

Epoch: 196
Loss: 0.22715110331773758
RMSE train: 0.453995	val: 1.091711	test: 1.176352
MAE train: 0.345450	val: 0.924587	test: 0.935843

Epoch: 197
Loss: 0.276418536901474
RMSE train: 0.475683	val: 1.145114	test: 1.235043
MAE train: 0.357303	val: 0.975468	test: 0.965468

Epoch: 198
Loss: 0.24994581565260887
RMSE train: 0.481665	val: 1.178150	test: 1.249544
MAE train: 0.363911	val: 0.988549	test: 0.964280

Epoch: 199
Loss: 0.2550055719912052
RMSE train: 0.427106	val: 1.067386	test: 1.159407
MAE train: 0.319321	val: 0.907488	test: 0.902406

Early stopping
Best (RMSE):	 train: 0.432769	val: 1.010237	test: 1.126886
Best (MAE):	 train: 0.328799	val: 0.837365	test: 0.901602
All runs completed.
