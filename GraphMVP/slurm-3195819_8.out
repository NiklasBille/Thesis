>>> Starting run for dataset: tox21
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running SCAFF configs_split_experiments/GraphCL/tox21/scaff/train_prop=0.6.yml on cuda:0
Running SCAFF configs_split_experiments/GraphCL/tox21/scaff/train_prop=0.7.yml on cuda:1
Running SCAFF configs_split_experiments/GraphCL/tox21/scaff/train_prop=0.8.yml on cuda:2
Starting process for seed 1: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/tox21/scaff/train_prop=0.6.yml --runseed 1 --device cuda:0
Starting process for seed 1: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/tox21/scaff/train_prop=0.8.yml --runseed 1 --device cuda:2
Starting process for seed 2: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/tox21/scaff/train_prop=0.6.yml --runseed 2 --device cuda:0
Starting process for seed 2: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/tox21/scaff/train_prop=0.8.yml --runseed 2 --device cuda:2
Starting process for seed 3: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/tox21/scaff/train_prop=0.6.yml --runseed 3 --device cuda:0
Starting process for seed 3: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/tox21/scaff/train_prop=0.8.yml --runseed 3 --device cuda:2
Starting process for seed 1: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/tox21/scaff/train_prop=0.7.yml --runseed 1 --device cuda:1
Starting process for seed 2: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/tox21/scaff/train_prop=0.7.yml --runseed 2 --device cuda:1
Starting process for seed 3: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/tox21/scaff/train_prop=0.7.yml --runseed 3 --device cuda:1
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] [14:43:16] WARNING: not removing hydrogen atom without neighborsWARNING: not removing hydrogen atom without neighbors

[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] [14:43:16] WARNING: not removing hydrogen atom without neighborsWARNING: not removing hydrogen atom without neighbors

Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/tox21/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/tox21/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/tox21/scaff/train_prop=0.6/tox21_scaff_1_20-05_14-43-16  ]
[ Using Seed :  1  ]
[ Using device :  cuda:0  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5605073549754315
ROC train: 0.675639	val: 0.607275	test: 0.609335
PRC train: 0.182360	val: 0.192802	test: 0.170937

Epoch: 2
Loss: 0.35853339278073265
ROC train: 0.729859	val: 0.610973	test: 0.590874
PRC train: 0.237518	val: 0.204556	test: 0.178790

Epoch: 3
Loss: 0.25092698016032966
ROC train: 0.772285	val: 0.693332	test: 0.677780
PRC train: 0.274221	val: 0.243952	test: 0.220753

Epoch: 4
Loss: 0.20573181785441258
ROC train: 0.795174	val: 0.713773	test: 0.688289
PRC train: 0.321705	val: 0.272992	test: 0.247329

Epoch: 5
Loss: 0.18789661168615215
ROC train: 0.819093	val: 0.734005	test: 0.701431
PRC train: 0.346841	val: 0.302502	test: 0.263818

Epoch: 6
Loss: 0.1770833731475561
ROC train: 0.828329	val: 0.709087	test: 0.682679
PRC train: 0.365968	val: 0.269002	test: 0.236109

Epoch: 7
Loss: 0.17398392201991422
ROC train: 0.848156	val: 0.735906	test: 0.700255
PRC train: 0.418638	val: 0.293930	test: 0.262967

Epoch: 8
Loss: 0.17165821525008892
ROC train: 0.846947	val: 0.693863	test: 0.659529
PRC train: 0.419588	val: 0.265379	test: 0.229461

Epoch: 9
Loss: 0.16788220437320917
ROC train: 0.845161	val: 0.701175	test: 0.687626
PRC train: 0.419801	val: 0.269518	test: 0.244741

Epoch: 10
Loss: 0.1688688455687198
ROC train: 0.868471	val: 0.727494	test: 0.699462
PRC train: 0.454609	val: 0.269770	test: 0.243054

Epoch: 11
Loss: 0.16164065425934313
ROC train: 0.870745	val: 0.751792	test: 0.720715
PRC train: 0.469006	val: 0.302592	test: 0.266790

Epoch: 12
Loss: 0.16027715430319814
ROC train: 0.874641	val: 0.740692	test: 0.713907
PRC train: 0.498340	val: 0.294566	test: 0.263524

Epoch: 13
Loss: 0.15669810316815
ROC train: 0.882597	val: 0.745001	test: 0.713586
PRC train: 0.515333	val: 0.301568	test: 0.267032

Epoch: 14
Loss: 0.15306889355677583
ROC train: 0.881393	val: 0.742917	test: 0.710944
PRC train: 0.509533	val: 0.296588	test: 0.268049

Epoch: 15
Loss: 0.15368641664991
ROC train: 0.887155	val: 0.740220	test: 0.717774
PRC train: 0.536065	val: 0.299893	test: 0.264139

Epoch: 16
Loss: 0.15132741291116827
ROC train: 0.884673	val: 0.734370	test: 0.712347
PRC train: 0.521834	val: 0.290204	test: 0.261549

Epoch: 17
Loss: 0.1533676603079492
ROC train: 0.887258	val: 0.738058	test: 0.723450
PRC train: 0.526140	val: 0.308129	test: 0.293894

Epoch: 18
Loss: 0.149985302939075
ROC train: 0.890999	val: 0.753926	test: 0.726462
PRC train: 0.541432	val: 0.318146	test: 0.286416

Epoch: 19
Loss: 0.14744712090284115
ROC train: 0.898059	val: 0.754147	test: 0.732151
PRC train: 0.570140	val: 0.308838	test: 0.284381

Epoch: 20
Loss: 0.14718033199942412
ROC train: 0.899215	val: 0.748790	test: 0.725627
PRC train: 0.568558	val: 0.309011	test: 0.287655

Epoch: 21
Loss: 0.14413731010058195
ROC train: 0.897860	val: 0.742798	test: 0.715697
PRC train: 0.571463	val: 0.291517	test: 0.274560

Epoch: 22
Loss: 0.14690206951122753
ROC train: 0.902313	val: 0.747890	test: 0.716686
PRC train: 0.581140	val: 0.308819	test: 0.266808

Epoch: 23
Loss: 0.14437387644059027
ROC train: 0.902459	val: 0.736843	test: 0.710722
PRC train: 0.584660	val: 0.295197	test: 0.274093

Epoch: 24
Loss: 0.14410693866126242
ROC train: 0.886525	val: 0.737780	test: 0.714399
PRC train: 0.528934	val: 0.296255	test: 0.270424

Epoch: 25
Loss: 0.1426183693934394
ROC train: 0.908984	val: 0.738279	test: 0.710261
PRC train: 0.586636	val: 0.295410	test: 0.265193

Epoch: 26
Loss: 0.1408897304002958
ROC train: 0.910163	val: 0.751121	test: 0.717698
PRC train: 0.599427	val: 0.311175	test: 0.280386

Epoch: 27
Loss: 0.13991583524855625
ROC train: 0.912376	val: 0.754891	test: 0.722925
PRC train: 0.608700	val: 0.322822	test: 0.286320

Epoch: 28
Loss: 0.1402512519420917
ROC train: 0.912745	val: 0.753429	test: 0.725782
PRC train: 0.611212	val: 0.324357	test: 0.293258

Epoch: 29
Loss: 0.13846289412925064
ROC train: 0.913559	val: 0.741254	test: 0.707137
PRC train: 0.606790	val: 0.302826	test: 0.273723

Epoch: 30
Loss: 0.13829988267921092
ROC train: 0.915888	val: 0.738102	test: 0.714715
PRC train: 0.615664	val: 0.310767	test: 0.282949

Epoch: 31
Loss: 0.13639981751982294
ROC train: 0.918847	val: 0.749138	test: 0.720157
PRC train: 0.624580	val: 0.318818	test: 0.284981

Epoch: 32
Loss: 0.13781467756266855
ROC train: 0.918321	val: 0.725546	test: 0.700533
PRC train: 0.628388	val: 0.290162	test: 0.268294

Epoch: 33
Loss: 0.1354457158422299Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/tox21/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/tox21/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/tox21/scaff/train_prop=0.6/tox21_scaff_3_20-05_14-43-16  ]
[ Using Seed :  3  ]
[ Using device :  cuda:0  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5721192364338349
ROC train: 0.677784	val: 0.623017	test: 0.604326
PRC train: 0.196948	val: 0.200826	test: 0.180980

Epoch: 2
Loss: 0.3666318780631962
ROC train: 0.726843	val: 0.650110	test: 0.625196
PRC train: 0.253628	val: 0.216903	test: 0.215362

Epoch: 3
Loss: 0.2627755108607478
ROC train: 0.777628	val: 0.690893	test: 0.667106
PRC train: 0.311448	val: 0.238041	test: 0.229999

Epoch: 4
Loss: 0.21518889241272865
ROC train: 0.801292	val: 0.700711	test: 0.672131
PRC train: 0.334800	val: 0.255761	test: 0.237503

Epoch: 5
Loss: 0.19386890371882912
ROC train: 0.808998	val: 0.712641	test: 0.695351
PRC train: 0.357452	val: 0.275331	test: 0.263150

Epoch: 6
Loss: 0.18432557655414347
ROC train: 0.832578	val: 0.725086	test: 0.697428
PRC train: 0.400408	val: 0.293149	test: 0.276688

Epoch: 7
Loss: 0.17514846840822634
ROC train: 0.837833	val: 0.715289	test: 0.691473
PRC train: 0.413075	val: 0.282242	test: 0.260439

Epoch: 8
Loss: 0.17310353039005436
ROC train: 0.848914	val: 0.735158	test: 0.711121
PRC train: 0.434294	val: 0.303825	test: 0.282959

Epoch: 9
Loss: 0.16898234689779915
ROC train: 0.853444	val: 0.732607	test: 0.709466
PRC train: 0.438081	val: 0.291278	test: 0.277955

Epoch: 10
Loss: 0.16484017238333556
ROC train: 0.865000	val: 0.738213	test: 0.715428
PRC train: 0.480750	val: 0.299262	test: 0.277899

Epoch: 11
Loss: 0.16329380380837022
ROC train: 0.866835	val: 0.712907	test: 0.696916
PRC train: 0.471198	val: 0.275479	test: 0.262840

Epoch: 12
Loss: 0.15918732576980857
ROC train: 0.872201	val: 0.740461	test: 0.714386
PRC train: 0.501260	val: 0.291601	test: 0.271896

Epoch: 13
Loss: 0.15822403787295794
ROC train: 0.876479	val: 0.737999	test: 0.715750
PRC train: 0.511811	val: 0.286117	test: 0.268392

Epoch: 14
Loss: 0.1562673047056415
ROC train: 0.884908	val: 0.741789	test: 0.716029
PRC train: 0.542668	val: 0.291289	test: 0.270246

Epoch: 15
Loss: 0.15352969498438446
ROC train: 0.886584	val: 0.740115	test: 0.713232
PRC train: 0.543330	val: 0.302328	test: 0.279460

Epoch: 16
Loss: 0.15162470659763988
ROC train: 0.885354	val: 0.751635	test: 0.724995
PRC train: 0.534003	val: 0.309908	test: 0.287668

Epoch: 17
Loss: 0.14929846780637093
ROC train: 0.891452	val: 0.752488	test: 0.725378
PRC train: 0.563878	val: 0.301668	test: 0.281845

Epoch: 18
Loss: 0.15029593288905235
ROC train: 0.895170	val: 0.744646	test: 0.721616
PRC train: 0.566406	val: 0.305346	test: 0.284496

Epoch: 19
Loss: 0.1475010034195727
ROC train: 0.895666	val: 0.749563	test: 0.719368
PRC train: 0.568147	val: 0.298068	test: 0.275679

Epoch: 20
Loss: 0.1448647239525194
ROC train: 0.898300	val: 0.752476	test: 0.722691
PRC train: 0.578089	val: 0.298533	test: 0.295579

Epoch: 21
Loss: 0.1456641861486002
ROC train: 0.899023	val: 0.743990	test: 0.723685
PRC train: 0.587279	val: 0.293257	test: 0.279647

Epoch: 22
Loss: 0.14485342658551661
ROC train: 0.898201	val: 0.751235	test: 0.729519
PRC train: 0.582707	val: 0.307986	test: 0.290956

Epoch: 23
Loss: 0.14241566428378083
ROC train: 0.901231	val: 0.754614	test: 0.732414
PRC train: 0.591790	val: 0.312322	test: 0.300347

Epoch: 24
Loss: 0.1415435684774729
ROC train: 0.903803	val: 0.750979	test: 0.724721
PRC train: 0.597693	val: 0.303038	test: 0.287397

Epoch: 25
Loss: 0.1403281135626634
ROC train: 0.907994	val: 0.743814	test: 0.720207
PRC train: 0.599146	val: 0.301033	test: 0.286976

Epoch: 26
Loss: 0.14091922207880042
ROC train: 0.909863	val: 0.746472	test: 0.727484
PRC train: 0.614583	val: 0.296144	test: 0.284776

Epoch: 27
Loss: 0.13826652893815072
ROC train: 0.912702	val: 0.756036	test: 0.731133
PRC train: 0.625736	val: 0.316424	test: 0.295130

Epoch: 28
Loss: 0.13620375335723553
ROC train: 0.912333	val: 0.748856	test: 0.730970
PRC train: 0.622224	val: 0.304321	test: 0.284704

Epoch: 29
Loss: 0.1387555877634549
ROC train: 0.918847	val: 0.749253	test: 0.725846
PRC train: 0.639763	val: 0.301198	test: 0.284734

Epoch: 30
Loss: 0.13594713511288403
ROC train: 0.917961	val: 0.747018	test: 0.725469
PRC train: 0.642036	val: 0.296491	test: 0.282014

Epoch: 31
Loss: 0.13874018261371202
ROC train: 0.922220	val: 0.758524	test: 0.736184
PRC train: 0.647250	val: 0.314711	test: 0.297540

Epoch: 32
Loss: 0.1382052202893512
ROC train: 0.918419	val: 0.740137	test: 0.725061
PRC train: 0.643109	val: 0.291936	test: 0.280062

Epoch: 33
Loss: 0.1332758266268293Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/tox21/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/tox21/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/tox21/scaff/train_prop=0.6/tox21_scaff_2_20-05_14-43-16  ]
[ Using Seed :  2  ]
[ Using device :  cuda:0  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5677460659736372
ROC train: 0.647898	val: 0.610598	test: 0.599722
PRC train: 0.177825	val: 0.194214	test: 0.175030

Epoch: 2
Loss: 0.36976305412082516
ROC train: 0.736332	val: 0.657198	test: 0.641573
PRC train: 0.247949	val: 0.205838	test: 0.199797

Epoch: 3
Loss: 0.2611765455395525
ROC train: 0.779076	val: 0.709917	test: 0.674825
PRC train: 0.309269	val: 0.241416	test: 0.235953

Epoch: 4
Loss: 0.21310636364619703
ROC train: 0.798829	val: 0.702912	test: 0.680510
PRC train: 0.330884	val: 0.254732	test: 0.239765

Epoch: 5
Loss: 0.18968314333648253
ROC train: 0.821180	val: 0.722020	test: 0.691936
PRC train: 0.365902	val: 0.275911	test: 0.254700

Epoch: 6
Loss: 0.18146365930530897
ROC train: 0.839077	val: 0.721577	test: 0.695220
PRC train: 0.409409	val: 0.277200	test: 0.259619

Epoch: 7
Loss: 0.17503502127651585
ROC train: 0.846248	val: 0.723479	test: 0.701281
PRC train: 0.428159	val: 0.272893	test: 0.269177

Epoch: 8
Loss: 0.1681119799437118
ROC train: 0.845328	val: 0.732268	test: 0.708894
PRC train: 0.423537	val: 0.299285	test: 0.273974

Epoch: 9
Loss: 0.16580824370198477
ROC train: 0.858859	val: 0.733929	test: 0.705627
PRC train: 0.460816	val: 0.297086	test: 0.270552

Epoch: 10
Loss: 0.16457779557611138
ROC train: 0.867618	val: 0.739111	test: 0.705750
PRC train: 0.479960	val: 0.295308	test: 0.270084

Epoch: 11
Loss: 0.15891404175868595
ROC train: 0.868703	val: 0.729587	test: 0.699627
PRC train: 0.486376	val: 0.295865	test: 0.265722

Epoch: 12
Loss: 0.15888315351189197
ROC train: 0.876653	val: 0.739077	test: 0.702016
PRC train: 0.512198	val: 0.298566	test: 0.265651

Epoch: 13
Loss: 0.15714561724097798
ROC train: 0.880469	val: 0.746332	test: 0.718168
PRC train: 0.523146	val: 0.318169	test: 0.285767

Epoch: 14
Loss: 0.15343090086301023
ROC train: 0.882430	val: 0.739609	test: 0.716088
PRC train: 0.534539	val: 0.300417	test: 0.282728

Epoch: 15
Loss: 0.15207086579979712
ROC train: 0.883329	val: 0.735272	test: 0.711558
PRC train: 0.549057	val: 0.296896	test: 0.287165

Epoch: 16
Loss: 0.15298337065372372
ROC train: 0.886308	val: 0.749793	test: 0.719377
PRC train: 0.547842	val: 0.300973	test: 0.274702

Epoch: 17
Loss: 0.1480900115951144
ROC train: 0.890789	val: 0.751724	test: 0.714903
PRC train: 0.562060	val: 0.296830	test: 0.277082

Epoch: 18
Loss: 0.14666149002948559
ROC train: 0.892788	val: 0.760470	test: 0.730163
PRC train: 0.568793	val: 0.324881	test: 0.293792

Epoch: 19
Loss: 0.14556806361650862
ROC train: 0.896684	val: 0.750217	test: 0.719360
PRC train: 0.576922	val: 0.296724	test: 0.274647

Epoch: 20
Loss: 0.14535952924071566
ROC train: 0.899954	val: 0.745019	test: 0.713755
PRC train: 0.582500	val: 0.300199	test: 0.284216

Epoch: 21
Loss: 0.14433030206499003
ROC train: 0.900358	val: 0.749047	test: 0.721664
PRC train: 0.584206	val: 0.309246	test: 0.293998

Epoch: 22
Loss: 0.14367791381930042
ROC train: 0.903719	val: 0.750814	test: 0.727326
PRC train: 0.584823	val: 0.306171	test: 0.280398

Epoch: 23
Loss: 0.14443866352796414
ROC train: 0.903119	val: 0.750033	test: 0.721892
PRC train: 0.588162	val: 0.307997	test: 0.284134

Epoch: 24
Loss: 0.14153395383046483
ROC train: 0.908488	val: 0.745395	test: 0.716601
PRC train: 0.602611	val: 0.315726	test: 0.289464

Epoch: 25
Loss: 0.13903979983041773
ROC train: 0.909635	val: 0.748193	test: 0.725749
PRC train: 0.613252	val: 0.315369	test: 0.292736

Epoch: 26
Loss: 0.1391319339556682
ROC train: 0.911526	val: 0.748360	test: 0.722095
PRC train: 0.615006	val: 0.315853	test: 0.288118

Epoch: 27
Loss: 0.13882429103858296
ROC train: 0.913719	val: 0.742218	test: 0.722762
PRC train: 0.625741	val: 0.306989	test: 0.278536

Epoch: 28
Loss: 0.13639357078380976
ROC train: 0.915810	val: 0.740261	test: 0.724530
PRC train: 0.627783	val: 0.323876	test: 0.298614

Epoch: 29
Loss: 0.13789398614776294
ROC train: 0.917046	val: 0.751202	test: 0.717411
PRC train: 0.629273	val: 0.328496	test: 0.289261

Epoch: 30
Loss: 0.13812181355734798
ROC train: 0.916220	val: 0.754419	test: 0.728187
PRC train: 0.626839	val: 0.321390	test: 0.291011

Epoch: 31
Loss: 0.1351357875141985
ROC train: 0.918977	val: 0.746871	test: 0.726107
PRC train: 0.643064	val: 0.311711	test: 0.289459

Epoch: 32
Loss: 0.13439790825637363
ROC train: 0.920289	val: 0.748706	test: 0.722767
PRC train: 0.647171	val: 0.315305	test: 0.288008

Epoch: 33
Loss: 0.13331750367933926Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/tox21/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/tox21/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/tox21/scaff/train_prop=0.7/tox21_scaff_1_20-05_14-43-16  ]
[ Using Seed :  1  ]
[ Using device :  cuda:1  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5400158318870749
ROC train: 0.678779	val: 0.568134	test: 0.570200
PRC train: 0.198017	val: 0.189448	test: 0.164736

Epoch: 2
Loss: 0.33147851291540914
ROC train: 0.755659	val: 0.693522	test: 0.672290
PRC train: 0.276980	val: 0.245034	test: 0.231973

Epoch: 3
Loss: 0.24005261064821515
ROC train: 0.787718	val: 0.664306	test: 0.655724
PRC train: 0.307099	val: 0.249154	test: 0.228626

Epoch: 4
Loss: 0.20566649612425583
ROC train: 0.811784	val: 0.697828	test: 0.689700
PRC train: 0.344144	val: 0.280525	test: 0.265323

Epoch: 5
Loss: 0.1924796879566915
ROC train: 0.828783	val: 0.716596	test: 0.695808
PRC train: 0.372178	val: 0.302372	test: 0.277825

Epoch: 6
Loss: 0.18647536628235872
ROC train: 0.838625	val: 0.739331	test: 0.724130
PRC train: 0.391188	val: 0.301758	test: 0.288578

Epoch: 7
Loss: 0.18116756425464237
ROC train: 0.851448	val: 0.733073	test: 0.727407
PRC train: 0.433392	val: 0.315402	test: 0.299885

Epoch: 8
Loss: 0.17742338827941684
ROC train: 0.855796	val: 0.728653	test: 0.727201
PRC train: 0.432930	val: 0.300926	test: 0.283147

Epoch: 9
Loss: 0.17531732803646358
ROC train: 0.859842	val: 0.714888	test: 0.705478
PRC train: 0.451119	val: 0.291211	test: 0.268240

Epoch: 10
Loss: 0.17345913710060448
ROC train: 0.870192	val: 0.725696	test: 0.724168
PRC train: 0.488897	val: 0.312331	test: 0.305937

Epoch: 11
Loss: 0.16905517525376693
ROC train: 0.871755	val: 0.740141	test: 0.723652
PRC train: 0.492850	val: 0.320362	test: 0.287231

Epoch: 12
Loss: 0.16770210382989237
ROC train: 0.878839	val: 0.743627	test: 0.740382
PRC train: 0.512852	val: 0.327072	test: 0.310019

Epoch: 13
Loss: 0.16517973713050335
ROC train: 0.878614	val: 0.746580	test: 0.732637
PRC train: 0.502562	val: 0.287234	test: 0.286597

Epoch: 14
Loss: 0.16529883276635438
ROC train: 0.883584	val: 0.735411	test: 0.726605
PRC train: 0.532359	val: 0.319623	test: 0.300220

Epoch: 15
Loss: 0.16212441955229967
ROC train: 0.890361	val: 0.752679	test: 0.733702
PRC train: 0.558310	val: 0.337351	test: 0.326037

Epoch: 16
Loss: 0.1622809972998198
ROC train: 0.889253	val: 0.761500	test: 0.745179
PRC train: 0.552088	val: 0.337314	test: 0.318481

Epoch: 17
Loss: 0.16133761845957714
ROC train: 0.892610	val: 0.736561	test: 0.734853
PRC train: 0.558697	val: 0.326371	test: 0.314898

Epoch: 18
Loss: 0.1577858816486274
ROC train: 0.896218	val: 0.744576	test: 0.737906
PRC train: 0.574361	val: 0.329077	test: 0.318690

Epoch: 19
Loss: 0.1569207500589459
ROC train: 0.896238	val: 0.742802	test: 0.745658
PRC train: 0.582047	val: 0.338993	test: 0.326487

Epoch: 20
Loss: 0.15410404352836526
ROC train: 0.898678	val: 0.749137	test: 0.737409
PRC train: 0.590462	val: 0.334046	test: 0.318344

Epoch: 21
Loss: 0.15591224361614442
ROC train: 0.896956	val: 0.743726	test: 0.737691
PRC train: 0.569243	val: 0.314908	test: 0.312714

Epoch: 22
Loss: 0.15298426761445227
ROC train: 0.902577	val: 0.747917	test: 0.736372
PRC train: 0.597155	val: 0.327537	test: 0.316483

Epoch: 23
Loss: 0.15288320198381491
ROC train: 0.902931	val: 0.746442	test: 0.738063
PRC train: 0.599714	val: 0.311059	test: 0.309147

Epoch: 24
Loss: 0.15096766701625278
ROC train: 0.906705	val: 0.744044	test: 0.739477
PRC train: 0.601095	val: 0.316713	test: 0.301618

Epoch: 25
Loss: 0.15230883135211915
ROC train: 0.911646	val: 0.753045	test: 0.744691
PRC train: 0.617229	val: 0.349118	test: 0.333514

Epoch: 26
Loss: 0.15016047898566692
ROC train: 0.912412	val: 0.748816	test: 0.737618
PRC train: 0.625455	val: 0.327561	test: 0.318407

Epoch: 27
Loss: 0.14785029962760474
ROC train: 0.909426	val: 0.748498	test: 0.743170
PRC train: 0.608443	val: 0.326385	test: 0.321163

Epoch: 28
Loss: 0.14879831688972228
ROC train: 0.913457	val: 0.750236	test: 0.736694
PRC train: 0.627548	val: 0.331617	test: 0.320892

Epoch: 29
Loss: 0.14438823129128336
ROC train: 0.912157	val: 0.741653	test: 0.729266
PRC train: 0.606872	val: 0.331878	test: 0.313803

Epoch: 30
Loss: 0.1444045429030143
ROC train: 0.918716	val: 0.758926	test: 0.744548
PRC train: 0.642177	val: 0.342631	test: 0.327528

Epoch: 31
Loss: 0.14411947220098204
ROC train: 0.920236	val: 0.750098	test: 0.744168
PRC train: 0.653687	val: 0.341426	test: 0.321923

Epoch: 32
Loss: 0.143613377995277
ROC train: 0.922301	val: 0.747445	test: 0.737919
PRC train: 0.652766	val: 0.326436	test: 0.312589

Epoch: 33
Loss: 0.1413197581338594Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/tox21/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/tox21/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/tox21/scaff/train_prop=0.7/tox21_scaff_3_20-05_14-43-16  ]
[ Using Seed :  3  ]
[ Using device :  cuda:1  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5535997997782395
ROC train: 0.695958	val: 0.618743	test: 0.626232
PRC train: 0.222924	val: 0.202368	test: 0.194282

Epoch: 2
Loss: 0.34608840480223524
ROC train: 0.737896	val: 0.639438	test: 0.623881
PRC train: 0.260016	val: 0.214321	test: 0.196776

Epoch: 3
Loss: 0.2465570923168488
ROC train: 0.787916	val: 0.687581	test: 0.665930
PRC train: 0.304805	val: 0.237947	test: 0.226999

Epoch: 4
Loss: 0.20879651964952978
ROC train: 0.811915	val: 0.712529	test: 0.688308
PRC train: 0.359763	val: 0.285036	test: 0.265453

Epoch: 5
Loss: 0.1923319142220619
ROC train: 0.827376	val: 0.710632	test: 0.697949
PRC train: 0.386128	val: 0.259079	test: 0.251054

Epoch: 6
Loss: 0.1857772390893972
ROC train: 0.835688	val: 0.736736	test: 0.710389
PRC train: 0.408253	val: 0.302835	test: 0.284207

Epoch: 7
Loss: 0.18258819588546757
ROC train: 0.844166	val: 0.737868	test: 0.710826
PRC train: 0.417512	val: 0.306283	test: 0.292755

Epoch: 8
Loss: 0.18204941314266476
ROC train: 0.848066	val: 0.741510	test: 0.723231
PRC train: 0.434410	val: 0.308414	test: 0.304385

Epoch: 9
Loss: 0.1759060518265439
ROC train: 0.855907	val: 0.744072	test: 0.725383
PRC train: 0.457194	val: 0.314585	test: 0.313393

Epoch: 10
Loss: 0.17339664203945332
ROC train: 0.864587	val: 0.747014	test: 0.723002
PRC train: 0.484488	val: 0.291051	test: 0.306897

Epoch: 11
Loss: 0.16972981702493953
ROC train: 0.869996	val: 0.740625	test: 0.722898
PRC train: 0.494638	val: 0.306704	test: 0.303046

Epoch: 12
Loss: 0.16831596420695152
ROC train: 0.875733	val: 0.749530	test: 0.731233
PRC train: 0.513677	val: 0.319089	test: 0.321942

Epoch: 13
Loss: 0.16655595759387765
ROC train: 0.875738	val: 0.753502	test: 0.734128
PRC train: 0.514432	val: 0.321670	test: 0.323218

Epoch: 14
Loss: 0.166583392347911
ROC train: 0.875949	val: 0.740319	test: 0.720825
PRC train: 0.517025	val: 0.295210	test: 0.293846

Epoch: 15
Loss: 0.161751880675418
ROC train: 0.884018	val: 0.748534	test: 0.728471
PRC train: 0.548568	val: 0.312947	test: 0.330072

Epoch: 16
Loss: 0.15881609929745685
ROC train: 0.889077	val: 0.755397	test: 0.740303
PRC train: 0.561539	val: 0.323593	test: 0.335080

Epoch: 17
Loss: 0.15910347183909182
ROC train: 0.890528	val: 0.752269	test: 0.731160
PRC train: 0.567644	val: 0.324218	test: 0.336694

Epoch: 18
Loss: 0.1580621215768427
ROC train: 0.892008	val: 0.753384	test: 0.731579
PRC train: 0.573683	val: 0.332006	test: 0.331274

Epoch: 19
Loss: 0.15803573290822956
ROC train: 0.891069	val: 0.751935	test: 0.737002
PRC train: 0.568805	val: 0.305775	test: 0.328877

Epoch: 20
Loss: 0.15409680621433244
ROC train: 0.896505	val: 0.749755	test: 0.722896
PRC train: 0.575298	val: 0.332043	test: 0.313345

Epoch: 21
Loss: 0.1540153904569793
ROC train: 0.900956	val: 0.752284	test: 0.732920
PRC train: 0.598134	val: 0.324262	test: 0.318701

Epoch: 22
Loss: 0.15431071020857984
ROC train: 0.903083	val: 0.749747	test: 0.729611
PRC train: 0.600659	val: 0.317672	test: 0.320501

Epoch: 23
Loss: 0.1514819655484834
ROC train: 0.906913	val: 0.743336	test: 0.721596
PRC train: 0.596187	val: 0.284910	test: 0.302963

Epoch: 24
Loss: 0.15012518181846748
ROC train: 0.905982	val: 0.741154	test: 0.723189
PRC train: 0.593120	val: 0.308995	test: 0.302721

Epoch: 25
Loss: 0.14864907580669576
ROC train: 0.910015	val: 0.738116	test: 0.722781
PRC train: 0.613710	val: 0.307453	test: 0.311743

Epoch: 26
Loss: 0.14961270314506345
ROC train: 0.910798	val: 0.752330	test: 0.739949
PRC train: 0.616652	val: 0.338832	test: 0.343125

Epoch: 27
Loss: 0.1456186025294343
ROC train: 0.911202	val: 0.743212	test: 0.730317
PRC train: 0.624636	val: 0.311841	test: 0.324839

Epoch: 28
Loss: 0.14772037277562755
ROC train: 0.915711	val: 0.749074	test: 0.733829
PRC train: 0.633935	val: 0.311481	test: 0.323568

Epoch: 29
Loss: 0.15039888938983198
ROC train: 0.912555	val: 0.737785	test: 0.727377
PRC train: 0.605071	val: 0.292124	test: 0.306814

Epoch: 30
Loss: 0.14644849174848826
ROC train: 0.916395	val: 0.743775	test: 0.729798
PRC train: 0.630295	val: 0.313953	test: 0.311452

Epoch: 31
Loss: 0.1445969730579671
ROC train: 0.916715	val: 0.732274	test: 0.728603
PRC train: 0.622498	val: 0.289774	test: 0.302728

Epoch: 32
Loss: 0.14373622765647612
ROC train: 0.921548	val: 0.740976	test: 0.730968
PRC train: 0.649373	val: 0.315380	test: 0.309391

Epoch: 33
Loss: 0.1419636270609326Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/tox21/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/tox21/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/tox21/scaff/train_prop=0.7/tox21_scaff_2_20-05_14-43-16  ]
[ Using Seed :  2  ]
[ Using device :  cuda:1  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.550101565439956
ROC train: 0.702400	val: 0.649421	test: 0.643421
PRC train: 0.234431	val: 0.234559	test: 0.204819

Epoch: 2
Loss: 0.34458116199306343
ROC train: 0.752013	val: 0.661216	test: 0.648348
PRC train: 0.268525	val: 0.204965	test: 0.208961

Epoch: 3
Loss: 0.24622414152304142
ROC train: 0.779415	val: 0.668423	test: 0.665605
PRC train: 0.305567	val: 0.212878	test: 0.218749

Epoch: 4
Loss: 0.20976105499596
ROC train: 0.802871	val: 0.699869	test: 0.689685
PRC train: 0.340104	val: 0.263095	test: 0.257311

Epoch: 5
Loss: 0.19326022564110376
ROC train: 0.826146	val: 0.718193	test: 0.705810
PRC train: 0.380341	val: 0.294674	test: 0.270987

Epoch: 6
Loss: 0.18608767131597084
ROC train: 0.836750	val: 0.736638	test: 0.723647
PRC train: 0.404370	val: 0.311752	test: 0.294191

Epoch: 7
Loss: 0.1822285779356234
ROC train: 0.846938	val: 0.735301	test: 0.715213
PRC train: 0.414575	val: 0.295943	test: 0.292188

Epoch: 8
Loss: 0.17914704238490278
ROC train: 0.854548	val: 0.741326	test: 0.728057
PRC train: 0.441359	val: 0.317932	test: 0.306320

Epoch: 9
Loss: 0.17716853114050107
ROC train: 0.860684	val: 0.734851	test: 0.718955
PRC train: 0.453569	val: 0.286735	test: 0.290480

Epoch: 10
Loss: 0.1721410719389599
ROC train: 0.865863	val: 0.738303	test: 0.717768
PRC train: 0.473680	val: 0.311592	test: 0.311197

Epoch: 11
Loss: 0.16993742855970836
ROC train: 0.868547	val: 0.746889	test: 0.726335
PRC train: 0.492194	val: 0.331302	test: 0.319512

Epoch: 12
Loss: 0.1669307439478758
ROC train: 0.877968	val: 0.743532	test: 0.722924
PRC train: 0.520171	val: 0.323166	test: 0.311965

Epoch: 13
Loss: 0.1654941797272491
ROC train: 0.879329	val: 0.746778	test: 0.726769
PRC train: 0.514625	val: 0.328275	test: 0.319622

Epoch: 14
Loss: 0.1634333698365515
ROC train: 0.884096	val: 0.745777	test: 0.727683
PRC train: 0.535072	val: 0.313780	test: 0.320932

Epoch: 15
Loss: 0.1624409512827016
ROC train: 0.882239	val: 0.752121	test: 0.732288
PRC train: 0.532315	val: 0.310754	test: 0.313274

Epoch: 16
Loss: 0.15913674595161564
ROC train: 0.888411	val: 0.754193	test: 0.737430
PRC train: 0.556300	val: 0.327412	test: 0.321575

Epoch: 17
Loss: 0.15808249768584356
ROC train: 0.891615	val: 0.750117	test: 0.735501
PRC train: 0.567677	val: 0.324490	test: 0.319678

Epoch: 18
Loss: 0.15907344961697786
ROC train: 0.893445	val: 0.751361	test: 0.727683
PRC train: 0.564922	val: 0.310562	test: 0.302000

Epoch: 19
Loss: 0.1568026425054825
ROC train: 0.892981	val: 0.756021	test: 0.740753
PRC train: 0.569706	val: 0.317870	test: 0.323086

Epoch: 20
Loss: 0.15575486563963925
ROC train: 0.897959	val: 0.755932	test: 0.737836
PRC train: 0.587773	val: 0.345603	test: 0.335612

Epoch: 21
Loss: 0.1541248514625576
ROC train: 0.899634	val: 0.757722	test: 0.739200
PRC train: 0.592915	val: 0.346981	test: 0.332313

Epoch: 22
Loss: 0.15350578978328475
ROC train: 0.903126	val: 0.746556	test: 0.741529
PRC train: 0.595256	val: 0.323775	test: 0.321762

Epoch: 23
Loss: 0.1515362332686959
ROC train: 0.903209	val: 0.756462	test: 0.738621
PRC train: 0.601130	val: 0.340174	test: 0.315323

Epoch: 24
Loss: 0.14981987597508153
ROC train: 0.906024	val: 0.744051	test: 0.735616
PRC train: 0.603887	val: 0.329397	test: 0.305433

Epoch: 25
Loss: 0.15002909869610923
ROC train: 0.907960	val: 0.743240	test: 0.728069
PRC train: 0.611395	val: 0.328171	test: 0.311228

Epoch: 26
Loss: 0.14786313105471535
ROC train: 0.912750	val: 0.743254	test: 0.735266
PRC train: 0.620665	val: 0.343967	test: 0.312943

Epoch: 27
Loss: 0.14629503227816842
ROC train: 0.913065	val: 0.743372	test: 0.729253
PRC train: 0.623736	val: 0.333006	test: 0.317893

Epoch: 28
Loss: 0.14711186793617703
ROC train: 0.912307	val: 0.744593	test: 0.729156
PRC train: 0.620112	val: 0.339491	test: 0.306758

Epoch: 29
Loss: 0.14642427015609608
ROC train: 0.916788	val: 0.744477	test: 0.732400
PRC train: 0.638431	val: 0.328025	test: 0.320284

Epoch: 30
Loss: 0.14376059461384907
ROC train: 0.917902	val: 0.740807	test: 0.737641
PRC train: 0.638440	val: 0.340951	test: 0.325853

Epoch: 31
Loss: 0.1424331991140831
ROC train: 0.919943	val: 0.747583	test: 0.736827
PRC train: 0.643109	val: 0.334476	test: 0.325244

Epoch: 32
Loss: 0.14442213748386598
ROC train: 0.923788	val: 0.741488	test: 0.735038
PRC train: 0.647223	val: 0.335986	test: 0.329888

Epoch: 33
Loss: 0.14257346459477466Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/tox21/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/tox21/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/tox21/scaff/train_prop=0.8/tox21_scaff_2_20-05_14-43-16  ]
[ Using Seed :  2  ]
[ Using device :  cuda:2  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.532251125164001
ROC train: 0.692466	val: 0.632435	test: 0.581278
PRC train: 0.216430	val: 0.208728	test: 0.190393

Epoch: 2
Loss: 0.3235603473924726
ROC train: 0.751517	val: 0.703410	test: 0.643402
PRC train: 0.267960	val: 0.267629	test: 0.229622

Epoch: 3
Loss: 0.23780105940461663
ROC train: 0.788937	val: 0.734966	test: 0.675682
PRC train: 0.328338	val: 0.293839	test: 0.293294

Epoch: 4
Loss: 0.20858959664613622
ROC train: 0.806814	val: 0.755713	test: 0.703261
PRC train: 0.343088	val: 0.299155	test: 0.306667

Epoch: 5
Loss: 0.1987203468217312
ROC train: 0.826930	val: 0.761727	test: 0.720510
PRC train: 0.386019	val: 0.317852	test: 0.325029

Epoch: 6
Loss: 0.19413640514546224
ROC train: 0.834930	val: 0.741750	test: 0.683329
PRC train: 0.395952	val: 0.303921	test: 0.316481

Epoch: 7
Loss: 0.1889691959544505
ROC train: 0.849688	val: 0.770715	test: 0.714966
PRC train: 0.429365	val: 0.343927	test: 0.328917

Epoch: 8
Loss: 0.18587857536470356
ROC train: 0.854595	val: 0.778299	test: 0.720677
PRC train: 0.432463	val: 0.359062	test: 0.334693

Epoch: 9
Loss: 0.18221445038713438
ROC train: 0.858068	val: 0.772826	test: 0.726573
PRC train: 0.456987	val: 0.341712	test: 0.342880

Epoch: 10
Loss: 0.18073321029247455
ROC train: 0.859978	val: 0.774800	test: 0.711903
PRC train: 0.460051	val: 0.354908	test: 0.339301

Epoch: 11
Loss: 0.1811727728692784
ROC train: 0.869327	val: 0.776904	test: 0.720955
PRC train: 0.489108	val: 0.340094	test: 0.343121

Epoch: 12
Loss: 0.17530346099698005
ROC train: 0.874349	val: 0.772181	test: 0.736483
PRC train: 0.507616	val: 0.343288	test: 0.362630

Epoch: 13
Loss: 0.17647402866563427
ROC train: 0.872066	val: 0.777734	test: 0.729311
PRC train: 0.498696	val: 0.344793	test: 0.345449

Epoch: 14
Loss: 0.17375429555438734
ROC train: 0.878119	val: 0.774096	test: 0.733580
PRC train: 0.517204	val: 0.349439	test: 0.360892

Epoch: 15
Loss: 0.17131447159125507
ROC train: 0.882901	val: 0.773957	test: 0.727108
PRC train: 0.538371	val: 0.347147	test: 0.352529

Epoch: 16
Loss: 0.17129047578659634
ROC train: 0.883841	val: 0.773486	test: 0.725782
PRC train: 0.541213	val: 0.355761	test: 0.351865

Epoch: 17
Loss: 0.16856646631153385
ROC train: 0.886594	val: 0.776095	test: 0.731666
PRC train: 0.545875	val: 0.355764	test: 0.365956

Epoch: 18
Loss: 0.16664023073969284
ROC train: 0.889070	val: 0.774838	test: 0.730701
PRC train: 0.554959	val: 0.349741	test: 0.347983

Epoch: 19
Loss: 0.1678181947165434
ROC train: 0.892360	val: 0.784488	test: 0.728978
PRC train: 0.562903	val: 0.353971	test: 0.369580

Epoch: 20
Loss: 0.16455196605930772
ROC train: 0.895919	val: 0.768507	test: 0.719361
PRC train: 0.566199	val: 0.331101	test: 0.329827

Epoch: 21
Loss: 0.1640529762207809
ROC train: 0.899795	val: 0.776559	test: 0.728439
PRC train: 0.581076	val: 0.358846	test: 0.363061

Epoch: 22
Loss: 0.16180231540862003
ROC train: 0.899938	val: 0.783748	test: 0.736998
PRC train: 0.580659	val: 0.354621	test: 0.354983

Epoch: 23
Loss: 0.15921828519953998
ROC train: 0.898936	val: 0.773530	test: 0.737597
PRC train: 0.585607	val: 0.350328	test: 0.339753

Epoch: 24
Loss: 0.15875565939664166
ROC train: 0.903115	val: 0.775901	test: 0.729283
PRC train: 0.592883	val: 0.352513	test: 0.350064

Epoch: 25
Loss: 0.15923925363979127
ROC train: 0.898594	val: 0.774498	test: 0.729623
PRC train: 0.575734	val: 0.338077	test: 0.346818

Epoch: 26
Loss: 0.15761700747569768
ROC train: 0.904281	val: 0.781101	test: 0.730290
PRC train: 0.591094	val: 0.348123	test: 0.345715

Epoch: 27
Loss: 0.15654382925371518
ROC train: 0.909681	val: 0.782471	test: 0.743256
PRC train: 0.619969	val: 0.364186	test: 0.361815

Epoch: 28
Loss: 0.1545055924443374
ROC train: 0.912884	val: 0.776300	test: 0.739060
PRC train: 0.617394	val: 0.353090	test: 0.363796

Epoch: 29
Loss: 0.15700261533288817
ROC train: 0.911957	val: 0.778944	test: 0.732702
PRC train: 0.618393	val: 0.361001	test: 0.367004

Epoch: 30
Loss: 0.15403871128307756
ROC train: 0.913181	val: 0.778865	test: 0.735683
PRC train: 0.614736	val: 0.360488	test: 0.352580

Epoch: 31
Loss: 0.15405579253106322
ROC train: 0.912534	val: 0.784347	test: 0.748155
PRC train: 0.628411	val: 0.371930	test: 0.368976

Epoch: 32
Loss: 0.15141292703111425
ROC train: 0.914442	val: 0.772306	test: 0.734039
PRC train: 0.628861	val: 0.357510	test: 0.348566

Epoch: 33
Loss: 0.15133369348106274Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/tox21/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/tox21/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/tox21/scaff/train_prop=0.8/tox21_scaff_3_20-05_14-43-16  ]
[ Using Seed :  3  ]
[ Using device :  cuda:2  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5305712807695536
ROC train: 0.686363	val: 0.562259	test: 0.563917
PRC train: 0.197683	val: 0.178449	test: 0.166849

Epoch: 2
Loss: 0.3262435098208279
ROC train: 0.756517	val: 0.716440	test: 0.655655
PRC train: 0.274251	val: 0.282343	test: 0.235609

Epoch: 3
Loss: 0.2367016450280131
ROC train: 0.787927	val: 0.751565	test: 0.698035
PRC train: 0.310103	val: 0.283147	test: 0.272139

Epoch: 4
Loss: 0.2095253517841419
ROC train: 0.794221	val: 0.723393	test: 0.689154
PRC train: 0.325188	val: 0.258019	test: 0.284768

Epoch: 5
Loss: 0.2030050739423301
ROC train: 0.821284	val: 0.762365	test: 0.717313
PRC train: 0.375099	val: 0.313823	test: 0.323691

Epoch: 6
Loss: 0.19541424289714013
ROC train: 0.834920	val: 0.776552	test: 0.730897
PRC train: 0.392331	val: 0.334409	test: 0.344953

Epoch: 7
Loss: 0.191055685219497
ROC train: 0.842859	val: 0.766986	test: 0.728772
PRC train: 0.421809	val: 0.327545	test: 0.348439

Epoch: 8
Loss: 0.18658342418675825
ROC train: 0.842023	val: 0.768947	test: 0.727976
PRC train: 0.427964	val: 0.340300	test: 0.346744

Epoch: 9
Loss: 0.18471856098022818
ROC train: 0.856837	val: 0.765825	test: 0.737307
PRC train: 0.462687	val: 0.344471	test: 0.353143

Epoch: 10
Loss: 0.18188588110311868
ROC train: 0.859009	val: 0.776124	test: 0.730193
PRC train: 0.465809	val: 0.343047	test: 0.336804

Epoch: 11
Loss: 0.17889500657276725
ROC train: 0.867239	val: 0.763219	test: 0.729865
PRC train: 0.483101	val: 0.338194	test: 0.359174

Epoch: 12
Loss: 0.17649096979371948
ROC train: 0.870299	val: 0.758763	test: 0.746174
PRC train: 0.499964	val: 0.334593	test: 0.372038

Epoch: 13
Loss: 0.17466647798737703
ROC train: 0.869067	val: 0.771126	test: 0.742509
PRC train: 0.498095	val: 0.344876	test: 0.370029

Epoch: 14
Loss: 0.17437871671399555
ROC train: 0.876779	val: 0.771874	test: 0.733754
PRC train: 0.518536	val: 0.341350	test: 0.356162

Epoch: 15
Loss: 0.17141239651555334
ROC train: 0.879102	val: 0.760491	test: 0.749785
PRC train: 0.534285	val: 0.347222	test: 0.376008

Epoch: 16
Loss: 0.16983109945381397
ROC train: 0.883866	val: 0.764083	test: 0.747086
PRC train: 0.537686	val: 0.348422	test: 0.372183

Epoch: 17
Loss: 0.1689482254364552
ROC train: 0.882862	val: 0.764703	test: 0.747958
PRC train: 0.545971	val: 0.346950	test: 0.371596

Epoch: 18
Loss: 0.16857388454969222
ROC train: 0.888256	val: 0.767229	test: 0.747811
PRC train: 0.550348	val: 0.337397	test: 0.360049

Epoch: 19
Loss: 0.1664033091446254
ROC train: 0.888314	val: 0.776243	test: 0.750300
PRC train: 0.562215	val: 0.350237	test: 0.368892

Epoch: 20
Loss: 0.1650235679155731
ROC train: 0.891912	val: 0.771340	test: 0.743658
PRC train: 0.561702	val: 0.343940	test: 0.362638

Epoch: 21
Loss: 0.16309686825880848
ROC train: 0.895575	val: 0.770752	test: 0.740183
PRC train: 0.575102	val: 0.342819	test: 0.344980

Epoch: 22
Loss: 0.16252390698359595
ROC train: 0.898309	val: 0.778822	test: 0.742149
PRC train: 0.578701	val: 0.351935	test: 0.373743

Epoch: 23
Loss: 0.16203538646044927
ROC train: 0.901197	val: 0.770022	test: 0.742146
PRC train: 0.590592	val: 0.343618	test: 0.349255

Epoch: 24
Loss: 0.15982806719693268
ROC train: 0.901873	val: 0.768216	test: 0.744023
PRC train: 0.593952	val: 0.340108	test: 0.354701

Epoch: 25
Loss: 0.15798692606659662
ROC train: 0.899553	val: 0.760535	test: 0.740041
PRC train: 0.572345	val: 0.320151	test: 0.352686

Epoch: 26
Loss: 0.15716341561445427
ROC train: 0.907972	val: 0.775547	test: 0.751246
PRC train: 0.607003	val: 0.339528	test: 0.357358

Epoch: 27
Loss: 0.15706140138634556
ROC train: 0.909258	val: 0.776197	test: 0.747779
PRC train: 0.609752	val: 0.354395	test: 0.365658

Epoch: 28
Loss: 0.15639713772588323
ROC train: 0.909057	val: 0.775494	test: 0.755401
PRC train: 0.607733	val: 0.343587	test: 0.371785

Epoch: 29
Loss: 0.15561450006889832
ROC train: 0.912010	val: 0.771785	test: 0.752960
PRC train: 0.614922	val: 0.338514	test: 0.360153

Epoch: 30
Loss: 0.1552408661395959
ROC train: 0.914278	val: 0.768078	test: 0.745460
PRC train: 0.616174	val: 0.343354	test: 0.350330

Epoch: 31
Loss: 0.15327241057040644
ROC train: 0.916867	val: 0.768585	test: 0.747468
PRC train: 0.624861	val: 0.337383	test: 0.340465

Epoch: 32
Loss: 0.152132497719228
ROC train: 0.917361	val: 0.775703	test: 0.747999
PRC train: 0.628305	val: 0.346259	test: 0.370910

Epoch: 33
Loss: 0.1516403448222784Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/tox21/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/tox21/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/tox21/scaff/train_prop=0.8/tox21_scaff_1_20-05_14-43-16  ]
[ Using Seed :  1  ]
[ Using device :  cuda:2  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5215559402178632
ROC train: 0.665648	val: 0.559068	test: 0.553945
PRC train: 0.190402	val: 0.171454	test: 0.168453

Epoch: 2
Loss: 0.3130272352505817
ROC train: 0.753868	val: 0.699289	test: 0.650429
PRC train: 0.260555	val: 0.247906	test: 0.230011

Epoch: 3
Loss: 0.2329510684672644
ROC train: 0.791609	val: 0.735785	test: 0.672504
PRC train: 0.314609	val: 0.282647	test: 0.276540

Epoch: 4
Loss: 0.20683242494623028
ROC train: 0.817362	val: 0.758825	test: 0.706275
PRC train: 0.361493	val: 0.311821	test: 0.298098

Epoch: 5
Loss: 0.19802471015709638
ROC train: 0.835240	val: 0.767144	test: 0.711525
PRC train: 0.373495	val: 0.326224	test: 0.304588

Epoch: 6
Loss: 0.19255861920246353
ROC train: 0.836628	val: 0.763730	test: 0.721703
PRC train: 0.388014	val: 0.323893	test: 0.316197

Epoch: 7
Loss: 0.18989315023712383
ROC train: 0.845692	val: 0.771653	test: 0.718412
PRC train: 0.412432	val: 0.343105	test: 0.322725

Epoch: 8
Loss: 0.1870857237033305
ROC train: 0.857455	val: 0.771201	test: 0.720560
PRC train: 0.444734	val: 0.323243	test: 0.329176

Epoch: 9
Loss: 0.18445417204508419
ROC train: 0.854374	val: 0.774209	test: 0.737427
PRC train: 0.444354	val: 0.331486	test: 0.333070

Epoch: 10
Loss: 0.18368683467520172
ROC train: 0.867896	val: 0.767876	test: 0.720723
PRC train: 0.472223	val: 0.332864	test: 0.328930

Epoch: 11
Loss: 0.17862820159949513
ROC train: 0.870918	val: 0.765672	test: 0.729225
PRC train: 0.479552	val: 0.338224	test: 0.348307

Epoch: 12
Loss: 0.17822232683380335
ROC train: 0.869242	val: 0.772120	test: 0.725005
PRC train: 0.485356	val: 0.346164	test: 0.337175

Epoch: 13
Loss: 0.17575749453180947
ROC train: 0.878320	val: 0.772565	test: 0.735867
PRC train: 0.513221	val: 0.359160	test: 0.358955

Epoch: 14
Loss: 0.17183643108377022
ROC train: 0.882221	val: 0.760972	test: 0.720459
PRC train: 0.516359	val: 0.338733	test: 0.339603

Epoch: 15
Loss: 0.17238863052200287
ROC train: 0.882280	val: 0.763767	test: 0.731504
PRC train: 0.508602	val: 0.329537	test: 0.342967

Epoch: 16
Loss: 0.1705259194987628
ROC train: 0.879871	val: 0.776676	test: 0.734543
PRC train: 0.515634	val: 0.356568	test: 0.358876

Epoch: 17
Loss: 0.1716231083390431
ROC train: 0.884548	val: 0.764455	test: 0.729290
PRC train: 0.533002	val: 0.343689	test: 0.343139

Epoch: 18
Loss: 0.1687627114497084
ROC train: 0.890677	val: 0.770115	test: 0.739126
PRC train: 0.548849	val: 0.352050	test: 0.361047

Epoch: 19
Loss: 0.16641413579771885
ROC train: 0.891458	val: 0.767481	test: 0.729191
PRC train: 0.551031	val: 0.338332	test: 0.337740

Epoch: 20
Loss: 0.16421963614838475
ROC train: 0.896085	val: 0.767495	test: 0.731154
PRC train: 0.558716	val: 0.349766	test: 0.355963

Epoch: 21
Loss: 0.16424968252428143
ROC train: 0.893020	val: 0.773592	test: 0.742351
PRC train: 0.567411	val: 0.366374	test: 0.357002

Epoch: 22
Loss: 0.16313352170783016
ROC train: 0.896848	val: 0.762865	test: 0.737687
PRC train: 0.571943	val: 0.347604	test: 0.360405

Epoch: 23
Loss: 0.16016811963251062
ROC train: 0.901937	val: 0.772956	test: 0.738814
PRC train: 0.585305	val: 0.358070	test: 0.359444

Epoch: 24
Loss: 0.15994828681037826
ROC train: 0.903509	val: 0.767422	test: 0.728684
PRC train: 0.576044	val: 0.354040	test: 0.356390

Epoch: 25
Loss: 0.1590079894946287
ROC train: 0.906694	val: 0.779119	test: 0.737041
PRC train: 0.596817	val: 0.367811	test: 0.369548

Epoch: 26
Loss: 0.15864954852729585
ROC train: 0.903239	val: 0.777491	test: 0.730055
PRC train: 0.583391	val: 0.370101	test: 0.358819

Epoch: 27
Loss: 0.15834882195125058
ROC train: 0.906199	val: 0.771445	test: 0.741282
PRC train: 0.588668	val: 0.354514	test: 0.369722

Epoch: 28
Loss: 0.15630677290238895
ROC train: 0.910650	val: 0.761778	test: 0.741763
PRC train: 0.600452	val: 0.360511	test: 0.351843

Epoch: 29
Loss: 0.1554470281396944
ROC train: 0.912591	val: 0.772510	test: 0.742437
PRC train: 0.611710	val: 0.380529	test: 0.351470

Epoch: 30
Loss: 0.15491530522326616
ROC train: 0.914516	val: 0.775055	test: 0.736610
PRC train: 0.619875	val: 0.365185	test: 0.361056

Epoch: 31
Loss: 0.15434813839422692
ROC train: 0.915117	val: 0.777084	test: 0.741485
PRC train: 0.621632	val: 0.365439	test: 0.356257

Epoch: 32
Loss: 0.15206032249068638
ROC train: 0.917006	val: 0.774537	test: 0.734399
PRC train: 0.633797	val: 0.366139	test: 0.350981

Epoch: 33
Loss: 0.15227588377803297
ROC train: 0.917087	val: 0.747580	test: 0.719961
PRC train: 0.632046	val: 0.312625	test: 0.283604

Epoch: 34
Loss: 0.13292289429282325
ROC train: 0.922381	val: 0.739519	test: 0.711989
PRC train: 0.639141	val: 0.314522	test: 0.285408

Epoch: 35
Loss: 0.13343089076309345
ROC train: 0.922282	val: 0.737582	test: 0.713829
PRC train: 0.639955	val: 0.297288	test: 0.278802

Epoch: 36
Loss: 0.1346489946478023
ROC train: 0.925311	val: 0.742577	test: 0.709232
PRC train: 0.646674	val: 0.305558	test: 0.272600

Epoch: 37
Loss: 0.1320275699341667
ROC train: 0.928161	val: 0.741863	test: 0.712901
PRC train: 0.655009	val: 0.305502	test: 0.276255

Epoch: 38
Loss: 0.13023288628948576
ROC train: 0.928642	val: 0.747203	test: 0.715080
PRC train: 0.660484	val: 0.304120	test: 0.276377

Epoch: 39
Loss: 0.1326325659848246
ROC train: 0.927751	val: 0.742208	test: 0.715289
PRC train: 0.648064	val: 0.312597	test: 0.285587

Epoch: 40
Loss: 0.13053320061955145
ROC train: 0.929037	val: 0.742874	test: 0.716467
PRC train: 0.661318	val: 0.315942	test: 0.284017

Epoch: 41
Loss: 0.13097965680780121
ROC train: 0.931882	val: 0.752185	test: 0.726002
PRC train: 0.665703	val: 0.324486	test: 0.292665

Epoch: 42
Loss: 0.13181103588320658
ROC train: 0.933021	val: 0.740245	test: 0.717277
PRC train: 0.672736	val: 0.308614	test: 0.281000

Epoch: 43
Loss: 0.12987854542229643
ROC train: 0.932823	val: 0.757246	test: 0.732786
PRC train: 0.673303	val: 0.324571	test: 0.292558

Epoch: 44
Loss: 0.13140257319979293
ROC train: 0.930260	val: 0.732723	test: 0.701429
PRC train: 0.659471	val: 0.288649	test: 0.267845

Epoch: 45
Loss: 0.1292918074847231
ROC train: 0.936253	val: 0.744647	test: 0.721580
PRC train: 0.677245	val: 0.303423	test: 0.281683

Epoch: 46
Loss: 0.1281118775086598
ROC train: 0.937598	val: 0.753255	test: 0.722173
PRC train: 0.683989	val: 0.315824	test: 0.281622

Epoch: 47
Loss: 0.12555891648085274
ROC train: 0.937815	val: 0.739337	test: 0.710385
PRC train: 0.683075	val: 0.302262	test: 0.271703

Epoch: 48
Loss: 0.12754940183193236
ROC train: 0.937035	val: 0.735861	test: 0.712646
PRC train: 0.682618	val: 0.306273	test: 0.287579

Epoch: 49
Loss: 0.12525039449847988
ROC train: 0.938812	val: 0.720392	test: 0.708406
PRC train: 0.688623	val: 0.293645	test: 0.266867

Epoch: 50
Loss: 0.12394815180359153
ROC train: 0.942632	val: 0.738128	test: 0.710971
PRC train: 0.694742	val: 0.308385	test: 0.276002

Epoch: 51
Loss: 0.12178965911774706
ROC train: 0.942505	val: 0.735040	test: 0.712898
PRC train: 0.700159	val: 0.302538	test: 0.280284

Epoch: 52
Loss: 0.12272127930986992
ROC train: 0.945791	val: 0.735501	test: 0.703288
PRC train: 0.710388	val: 0.297512	test: 0.270610

Epoch: 53
Loss: 0.12044864831535573
ROC train: 0.946953	val: 0.742867	test: 0.708358
PRC train: 0.717674	val: 0.318266	test: 0.286781

Epoch: 54
Loss: 0.12139583451281558
ROC train: 0.945895	val: 0.740226	test: 0.709875
PRC train: 0.717243	val: 0.297670	test: 0.272792

Epoch: 55
Loss: 0.12093432725858347
ROC train: 0.941536	val: 0.719369	test: 0.690929
PRC train: 0.701429	val: 0.285742	test: 0.260681

Epoch: 56
Loss: 0.11872166834783532
ROC train: 0.949092	val: 0.730692	test: 0.697311
PRC train: 0.722696	val: 0.296286	test: 0.268437

Epoch: 57
Loss: 0.11995730964689559
ROC train: 0.949277	val: 0.731868	test: 0.704537
PRC train: 0.725476	val: 0.296677	test: 0.272676

Epoch: 58
Loss: 0.12032182450837589
ROC train: 0.950767	val: 0.720495	test: 0.701396
PRC train: 0.730637	val: 0.290247	test: 0.270533

Epoch: 59
Loss: 0.1210553115324673
ROC train: 0.950023	val: 0.738093	test: 0.708588
PRC train: 0.729886	val: 0.312737	test: 0.281604

Epoch: 60
Loss: 0.1184433725714378
ROC train: 0.950131	val: 0.724679	test: 0.703151
PRC train: 0.730394	val: 0.300593	test: 0.282884

Epoch: 61
Loss: 0.11977302882345187
ROC train: 0.951256	val: 0.744655	test: 0.710226
PRC train: 0.731315	val: 0.307076	test: 0.275639

Epoch: 62
Loss: 0.11909187377482759
ROC train: 0.950525	val: 0.736524	test: 0.711434
PRC train: 0.732175	val: 0.318751	test: 0.287215

Epoch: 63
Loss: 0.11664559062205548
ROC train: 0.954162	val: 0.721717	test: 0.698222
PRC train: 0.738342	val: 0.296496	test: 0.269265

Epoch: 64
Loss: 0.11707439904568939
ROC train: 0.953963	val: 0.729375	test: 0.700565
PRC train: 0.744182	val: 0.294924	test: 0.275220

Epoch: 65
Loss: 0.11491620910262702
ROC train: 0.956766	val: 0.730999	test: 0.703575
PRC train: 0.757330	val: 0.302715	test: 0.278546

Epoch: 66
Loss: 0.11441018735974366
ROC train: 0.955185	val: 0.732330	test: 0.710647
PRC train: 0.753806	val: 0.300491	test: 0.278941

Epoch: 67
Loss: 0.11521421904461714
ROC train: 0.956924	val: 0.733944	test: 0.699460
PRC train: 0.752743	val: 0.301090	test: 0.270241

Epoch: 68
Loss: 0.11165528787014024
ROC train: 0.957186	val: 0.733741	test: 0.707409
PRC train: 0.756142	val: 0.314614	test: 0.283263

Epoch: 69
Loss: 0.11207272023267158
ROC train: 0.958255	val: 0.735278	test: 0.704567
PRC train: 0.764183	val: 0.301625	test: 0.267379

Epoch: 70
Loss: 0.11155935721973166
ROC train: 0.957832	val: 0.728693	test: 0.699325
PRC train: 0.757709	val: 0.295287	test: 0.276043

Epoch: 71
Loss: 0.11255865237128063
ROC train: 0.960481	val: 0.724073	test: 0.704312
PRC train: 0.771560	val: 0.298011	test: 0.277140

Epoch: 72
Loss: 0.11175815937360194
ROC train: 0.962198	val: 0.726613	test: 0.698689
PRC train: 0.778142	val: 0.301384	test: 0.276961

Epoch: 73
Loss: 0.10873802302461891
ROC train: 0.961313	val: 0.735177	test: 0.698517
PRC train: 0.776719	val: 0.294118	test: 0.277474

Epoch: 74
Loss: 0.1125686487596124
ROC train: 0.960384	val: 0.712768	test: 0.696587
PRC train: 0.766452	val: 0.292389	test: 0.288173

Epoch: 75
Loss: 0.11107183910507827
ROC train: 0.962524	val: 0.702256	test: 0.686116
PRC train: 0.775757	val: 0.281361	test: 0.258336

Epoch: 76
Loss: 0.11173051374666723
ROC train: 0.963697	val: 0.734292	test: 0.699408
PRC train: 0.782851	val: 0.294139	test: 0.261909

Epoch: 77
Loss: 0.11042742684465717
ROC train: 0.963518	val: 0.727085	test: 0.701727
PRC train: 0.780986	val: 0.298084	test: 0.276992

Epoch: 78
Loss: 0.10958743884659775
ROC train: 0.963423	val: 0.724008	test: 0.694755
PRC train: 0.783114	val: 0.285539	test: 0.265966

Epoch: 79
Loss: 0.10692967973021664
ROC train: 0.965415	val: 0.734262	test: 0.711073
PRC train: 0.793260	val: 0.303926	test: 0.269320

Epoch: 80
Loss: 0.10756474168926983
ROC train: 0.966708	val: 0.719325	test: 0.699525
PRC train: 0.797824	val: 0.297435	test: 0.280270

Epoch: 81
Loss: 0.10728652624386531
ROC train: 0.965961	val: 0.717145	test: 0.690395
PRC train: 0.791720	val: 0.291261	test: 0.268327

Epoch: 82
Loss: 0.1052515055654555
ROC train: 0.968185	val: 0.719492	test: 0.695090
PRC train: 0.804152	val: 0.291847	test: 0.274968

Epoch: 83
Loss: 0.10488249396716891
ROC train: 0.954921	val: 0.711701	test: 0.688675
PRC train: 0.765510	val: 0.284835	test: 0.270221

Epoch: 84
Loss: 0.10426206939833918
ROC train: 0.968759	val: 0.720929	test: 0.700201
PRC train: 0.806793	val: 0.287900	test: 0.282168

Epoch: 85
Loss: 0.1045761183104594
ROC train: 0.966963	val: 0.716116	test: 0.687055
PRC train: 0.791949	val: 0.280329	test: 0.267036

Epoch: 86
Loss: 0.10649096640973822
ROC train: 0.970514	val: 0.723360	test: 0.699418
PRC train: 0.811843	val: 0.290269	test: 0.269059

Epoch: 87
Loss: 0.10469440379764072
ROC train: 0.968242	val: 0.710781	test: 0.691810
PRC train: 0.803380	val: 0.273531	test: 0.260758

Epoch: 88
Loss: 0.1023323294627944
ROC train: 0.969691	val: 0.719449	test: 0.699300
PRC train: 0.804791	val: 0.287532	test: 0.260515

Epoch: 89
Loss: 0.10224701538597443
ROC train: 0.970913	val: 0.727309	test: 0.700155
PRC train: 0.815334	val: 0.289442	test: 0.268682

Epoch: 90
Loss: 0.10065087283960611
ROC train: 0.971760	val: 0.725533	test: 0.690676
PRC train: 0.820242	val: 0.285883	test: 0.259952

Epoch: 91
Loss: 0.10190587859814534
ROC train: 0.972096	val: 0.721569	test: 0.699928
PRC train: 0.817663	val: 0.293194	test: 0.270488

Epoch: 92
Loss: 0.09985047631993223
ROC train: 0.973511	val: 0.729539	test: 0.693267
PRC train: 0.823697	val: 0.288503	test: 0.262797

Epoch: 93
Loss: 0.10030585307932868
ROC train: 0.971136	val: 0.710745	test: 0.685779
PRC train: 0.815086	val: 0.273465	test: 0.258752

Epoch: 94
Loss: 0.09743518143767195
ROC train: 0.923401	val: 0.745251	test: 0.725501
PRC train: 0.651882	val: 0.304528	test: 0.287375

Epoch: 34
Loss: 0.13215538418156056
ROC train: 0.924572	val: 0.755346	test: 0.735873
PRC train: 0.655329	val: 0.312035	test: 0.300269

Epoch: 35
Loss: 0.13177438426098959
ROC train: 0.926439	val: 0.754295	test: 0.730845
PRC train: 0.665153	val: 0.319815	test: 0.299151

Epoch: 36
Loss: 0.13351053419790773
ROC train: 0.926173	val: 0.748206	test: 0.731307
PRC train: 0.649549	val: 0.287363	test: 0.278551

Epoch: 37
Loss: 0.1320754891888088
ROC train: 0.929167	val: 0.755697	test: 0.728845
PRC train: 0.665789	val: 0.309409	test: 0.288680

Epoch: 38
Loss: 0.13041521134672052
ROC train: 0.926598	val: 0.747015	test: 0.729338
PRC train: 0.661900	val: 0.297301	test: 0.285327

Epoch: 39
Loss: 0.1317017841904542
ROC train: 0.930971	val: 0.743807	test: 0.727670
PRC train: 0.671420	val: 0.303955	test: 0.290281

Epoch: 40
Loss: 0.1309784197231509
ROC train: 0.933751	val: 0.743438	test: 0.723185
PRC train: 0.679767	val: 0.296746	test: 0.278717

Epoch: 41
Loss: 0.1297566846653568
ROC train: 0.933718	val: 0.746343	test: 0.718337
PRC train: 0.683634	val: 0.303382	test: 0.279113

Epoch: 42
Loss: 0.12745034129377142
ROC train: 0.936989	val: 0.743317	test: 0.723535
PRC train: 0.690515	val: 0.291348	test: 0.283775

Epoch: 43
Loss: 0.12756799613367623
ROC train: 0.936206	val: 0.747292	test: 0.726712
PRC train: 0.691366	val: 0.309037	test: 0.295719

Epoch: 44
Loss: 0.12690384711738814
ROC train: 0.939423	val: 0.739106	test: 0.723242
PRC train: 0.696340	val: 0.294257	test: 0.275437

Epoch: 45
Loss: 0.12360602452324611
ROC train: 0.939421	val: 0.746965	test: 0.732662
PRC train: 0.700041	val: 0.308033	test: 0.294946

Epoch: 46
Loss: 0.12445789302665256
ROC train: 0.942032	val: 0.743479	test: 0.731545
PRC train: 0.704902	val: 0.287874	test: 0.281704

Epoch: 47
Loss: 0.12266645039550396
ROC train: 0.941670	val: 0.742730	test: 0.717052
PRC train: 0.704235	val: 0.288876	test: 0.268592

Epoch: 48
Loss: 0.12643768520886975
ROC train: 0.941868	val: 0.731065	test: 0.717392
PRC train: 0.709607	val: 0.291314	test: 0.276037

Epoch: 49
Loss: 0.12298642370271368
ROC train: 0.943282	val: 0.741737	test: 0.722623
PRC train: 0.708596	val: 0.289389	test: 0.269755

Epoch: 50
Loss: 0.12156743920623712
ROC train: 0.945529	val: 0.744812	test: 0.724108
PRC train: 0.720248	val: 0.301446	test: 0.290790

Epoch: 51
Loss: 0.12173811333726003
ROC train: 0.945451	val: 0.729931	test: 0.715101
PRC train: 0.720285	val: 0.279443	test: 0.272612

Epoch: 52
Loss: 0.11967879950363305
ROC train: 0.946204	val: 0.741453	test: 0.715407
PRC train: 0.721385	val: 0.289778	test: 0.267589

Epoch: 53
Loss: 0.12062135565593086
ROC train: 0.949618	val: 0.746830	test: 0.728021
PRC train: 0.732121	val: 0.294044	test: 0.272656

Epoch: 54
Loss: 0.11815554417429301
ROC train: 0.951421	val: 0.743327	test: 0.722581
PRC train: 0.731781	val: 0.299700	test: 0.275865

Epoch: 55
Loss: 0.1214740212792529
ROC train: 0.950599	val: 0.744110	test: 0.723737
PRC train: 0.728284	val: 0.296035	test: 0.273600

Epoch: 56
Loss: 0.1207511989975767
ROC train: 0.949630	val: 0.737646	test: 0.727287
PRC train: 0.726062	val: 0.310203	test: 0.296856

Epoch: 57
Loss: 0.11817390948842589
ROC train: 0.951099	val: 0.725481	test: 0.711011
PRC train: 0.735444	val: 0.290106	test: 0.275284

Epoch: 58
Loss: 0.11929286564591554
ROC train: 0.953703	val: 0.740860	test: 0.721338
PRC train: 0.746283	val: 0.297508	test: 0.281776

Epoch: 59
Loss: 0.11636895152734861
ROC train: 0.955167	val: 0.744159	test: 0.727101
PRC train: 0.746347	val: 0.305523	test: 0.282862

Epoch: 60
Loss: 0.11568708627482654
ROC train: 0.954747	val: 0.737064	test: 0.721510
PRC train: 0.750255	val: 0.297396	test: 0.281782

Epoch: 61
Loss: 0.11575652791902473
ROC train: 0.955259	val: 0.737477	test: 0.718937
PRC train: 0.751021	val: 0.289535	test: 0.269840

Epoch: 62
Loss: 0.11498219694813419
ROC train: 0.957278	val: 0.734923	test: 0.716850
PRC train: 0.754385	val: 0.295637	test: 0.276658

Epoch: 63
Loss: 0.11443629753063433
ROC train: 0.957616	val: 0.736698	test: 0.722961
PRC train: 0.759700	val: 0.296730	test: 0.278905

Epoch: 64
Loss: 0.11190002055308411
ROC train: 0.957374	val: 0.736545	test: 0.722096
PRC train: 0.758327	val: 0.285516	test: 0.265348

Epoch: 65
Loss: 0.11352310933463662
ROC train: 0.958051	val: 0.741579	test: 0.727918
PRC train: 0.757990	val: 0.313220	test: 0.287675

Epoch: 66
Loss: 0.11206299594364498
ROC train: 0.958069	val: 0.730798	test: 0.714517
PRC train: 0.761532	val: 0.287736	test: 0.277248

Epoch: 67
Loss: 0.11243411806474765
ROC train: 0.960530	val: 0.737312	test: 0.716896
PRC train: 0.767840	val: 0.296801	test: 0.273361

Epoch: 68
Loss: 0.10990895647728176
ROC train: 0.960974	val: 0.737106	test: 0.717288
PRC train: 0.773074	val: 0.290298	test: 0.280678

Epoch: 69
Loss: 0.11110931082818262
ROC train: 0.961907	val: 0.745392	test: 0.723829
PRC train: 0.779703	val: 0.292942	test: 0.277299

Epoch: 70
Loss: 0.11203303609410231
ROC train: 0.961635	val: 0.737023	test: 0.715755
PRC train: 0.774651	val: 0.301837	test: 0.280047

Epoch: 71
Loss: 0.11030603834121455
ROC train: 0.961605	val: 0.745973	test: 0.727107
PRC train: 0.777466	val: 0.304521	test: 0.284607

Epoch: 72
Loss: 0.10850229111187026
ROC train: 0.962005	val: 0.731730	test: 0.713275
PRC train: 0.770476	val: 0.302702	test: 0.286222

Epoch: 73
Loss: 0.10835669900591648
ROC train: 0.964295	val: 0.731289	test: 0.713107
PRC train: 0.788463	val: 0.285780	test: 0.274253

Epoch: 74
Loss: 0.10767289384638866
ROC train: 0.965084	val: 0.743526	test: 0.723395
PRC train: 0.788488	val: 0.301543	test: 0.282017

Epoch: 75
Loss: 0.11019024470278033
ROC train: 0.966422	val: 0.731559	test: 0.714191
PRC train: 0.796752	val: 0.281185	test: 0.278484

Epoch: 76
Loss: 0.10728121624042143
ROC train: 0.965472	val: 0.733838	test: 0.717180
PRC train: 0.788334	val: 0.282781	test: 0.265692

Epoch: 77
Loss: 0.10559535346351592
ROC train: 0.967257	val: 0.731237	test: 0.714687
PRC train: 0.797537	val: 0.293126	test: 0.275073

Epoch: 78
Loss: 0.10515477058093792
ROC train: 0.967080	val: 0.732655	test: 0.716363
PRC train: 0.796914	val: 0.290194	test: 0.265332

Epoch: 79
Loss: 0.10231974346823597
ROC train: 0.968883	val: 0.735295	test: 0.711219
PRC train: 0.803701	val: 0.285864	test: 0.260181

Epoch: 80
Loss: 0.10308964204154888
ROC train: 0.969250	val: 0.731111	test: 0.714133
PRC train: 0.807745	val: 0.297010	test: 0.285966

Epoch: 81
Loss: 0.10427673809703304
ROC train: 0.970247	val: 0.737422	test: 0.720542
PRC train: 0.809350	val: 0.295873	test: 0.290198

Epoch: 82
Loss: 0.10505583554846433
ROC train: 0.968888	val: 0.724058	test: 0.706148
PRC train: 0.804751	val: 0.267698	test: 0.267645

Epoch: 83
Loss: 0.10591823406004651
ROC train: 0.969096	val: 0.728454	test: 0.717920
PRC train: 0.803986	val: 0.283359	test: 0.278804

Epoch: 84
Loss: 0.10533631066288408
ROC train: 0.969846	val: 0.730862	test: 0.706738
PRC train: 0.810557	val: 0.286894	test: 0.281026

Epoch: 85
Loss: 0.10333733986618032
ROC train: 0.971416	val: 0.736891	test: 0.718225
PRC train: 0.818297	val: 0.292721	test: 0.277881

Epoch: 86
Loss: 0.10196007064788343
ROC train: 0.969189	val: 0.732797	test: 0.713903
PRC train: 0.811661	val: 0.284283	test: 0.277411

Epoch: 87
Loss: 0.10107087596638176
ROC train: 0.970842	val: 0.717283	test: 0.696709
PRC train: 0.812395	val: 0.274407	test: 0.267249

Epoch: 88
Loss: 0.1023246154931935
ROC train: 0.972789	val: 0.724757	test: 0.709605
PRC train: 0.827916	val: 0.289505	test: 0.278779

Epoch: 89
Loss: 0.09875467791263509
ROC train: 0.972683	val: 0.725937	test: 0.709431
PRC train: 0.827532	val: 0.282839	test: 0.264814

Epoch: 90
Loss: 0.10019423625470943
ROC train: 0.972551	val: 0.732164	test: 0.714691
PRC train: 0.824614	val: 0.288348	test: 0.276409

Epoch: 91
Loss: 0.09841259114270781
ROC train: 0.973358	val: 0.725095	test: 0.705112
PRC train: 0.824169	val: 0.283869	test: 0.277149

Epoch: 92
Loss: 0.09859448794104274
ROC train: 0.974466	val: 0.737197	test: 0.725703
PRC train: 0.833408	val: 0.294547	test: 0.284624

Epoch: 93
Loss: 0.0993643344242608
ROC train: 0.973608	val: 0.728767	test: 0.710433
PRC train: 0.829803	val: 0.289723	test: 0.263247

Epoch: 94
Loss: 0.0978385617970972
ROC train: 0.924038	val: 0.746617	test: 0.731384
PRC train: 0.651962	val: 0.317756	test: 0.288125

Epoch: 34
Loss: 0.13065401787083164
ROC train: 0.924574	val: 0.749804	test: 0.726455
PRC train: 0.652439	val: 0.315890	test: 0.285657

Epoch: 35
Loss: 0.13179634503950302
ROC train: 0.926299	val: 0.748610	test: 0.730627
PRC train: 0.657420	val: 0.315969	test: 0.294892

Epoch: 36
Loss: 0.13213631557233982
ROC train: 0.928305	val: 0.738065	test: 0.717670
PRC train: 0.658292	val: 0.297989	test: 0.274500

Epoch: 37
Loss: 0.13072957916304317
ROC train: 0.929726	val: 0.729411	test: 0.714054
PRC train: 0.660535	val: 0.302835	test: 0.283640

Epoch: 38
Loss: 0.12744985562211433
ROC train: 0.931357	val: 0.734991	test: 0.727523
PRC train: 0.668589	val: 0.299832	test: 0.289457

Epoch: 39
Loss: 0.13023659866801007
ROC train: 0.932455	val: 0.728277	test: 0.709654
PRC train: 0.671663	val: 0.283500	test: 0.261905

Epoch: 40
Loss: 0.1283929543846629
ROC train: 0.931504	val: 0.741514	test: 0.731429
PRC train: 0.674745	val: 0.313444	test: 0.296390

Epoch: 41
Loss: 0.12696120551690396
ROC train: 0.932352	val: 0.732969	test: 0.708966
PRC train: 0.671066	val: 0.298136	test: 0.278728

Epoch: 42
Loss: 0.12651803491357316
ROC train: 0.933510	val: 0.736537	test: 0.714704
PRC train: 0.679701	val: 0.295356	test: 0.279120

Epoch: 43
Loss: 0.12713231373835485
ROC train: 0.934985	val: 0.743022	test: 0.735456
PRC train: 0.680631	val: 0.317150	test: 0.307204

Epoch: 44
Loss: 0.1280140913984569
ROC train: 0.938417	val: 0.730518	test: 0.714907
PRC train: 0.697941	val: 0.293560	test: 0.274676

Epoch: 45
Loss: 0.12500947232011425
ROC train: 0.939999	val: 0.727080	test: 0.714478
PRC train: 0.700270	val: 0.289575	test: 0.278231

Epoch: 46
Loss: 0.12272353325852367
ROC train: 0.939936	val: 0.735339	test: 0.715815
PRC train: 0.699917	val: 0.306419	test: 0.287101

Epoch: 47
Loss: 0.1260768633908019
ROC train: 0.939687	val: 0.733979	test: 0.719438
PRC train: 0.702663	val: 0.300000	test: 0.283950

Epoch: 48
Loss: 0.12351270175455613
ROC train: 0.940985	val: 0.732734	test: 0.711858
PRC train: 0.703701	val: 0.279427	test: 0.260110

Epoch: 49
Loss: 0.12051154323777098
ROC train: 0.945313	val: 0.733687	test: 0.716500
PRC train: 0.712058	val: 0.305288	test: 0.294210

Epoch: 50
Loss: 0.12038535877149345
ROC train: 0.944885	val: 0.741485	test: 0.717196
PRC train: 0.716356	val: 0.310450	test: 0.284797

Epoch: 51
Loss: 0.1214525174876081
ROC train: 0.947271	val: 0.732650	test: 0.714872
PRC train: 0.721288	val: 0.293062	test: 0.274513

Epoch: 52
Loss: 0.11788787180537569
ROC train: 0.947742	val: 0.738285	test: 0.710772
PRC train: 0.724446	val: 0.291642	test: 0.272352

Epoch: 53
Loss: 0.12190282964654879
ROC train: 0.946064	val: 0.733594	test: 0.717686
PRC train: 0.719879	val: 0.299986	test: 0.273547

Epoch: 54
Loss: 0.12114303022876671
ROC train: 0.945287	val: 0.730050	test: 0.715870
PRC train: 0.715345	val: 0.301686	test: 0.283949

Epoch: 55
Loss: 0.11795271259197894
ROC train: 0.948588	val: 0.731843	test: 0.713934
PRC train: 0.726793	val: 0.296532	test: 0.279824

Epoch: 56
Loss: 0.11781921201801272
ROC train: 0.952571	val: 0.729088	test: 0.709439
PRC train: 0.744403	val: 0.290655	test: 0.283467

Epoch: 57
Loss: 0.11877490352109284
ROC train: 0.951645	val: 0.730166	test: 0.712865
PRC train: 0.735406	val: 0.309089	test: 0.289322

Epoch: 58
Loss: 0.1189044196387727
ROC train: 0.953716	val: 0.737926	test: 0.713076
PRC train: 0.742149	val: 0.299588	test: 0.282339

Epoch: 59
Loss: 0.11704168224511276
ROC train: 0.952712	val: 0.724699	test: 0.707376
PRC train: 0.743854	val: 0.293270	test: 0.273682

Epoch: 60
Loss: 0.1160979191704989
ROC train: 0.952634	val: 0.730412	test: 0.710564
PRC train: 0.740023	val: 0.316816	test: 0.288403

Epoch: 61
Loss: 0.11596071926029851
ROC train: 0.953520	val: 0.741641	test: 0.718122
PRC train: 0.743871	val: 0.314535	test: 0.287723

Epoch: 62
Loss: 0.11292771042360818
ROC train: 0.955784	val: 0.727409	test: 0.699091
PRC train: 0.753662	val: 0.295124	test: 0.285517

Epoch: 63
Loss: 0.11346062747274119
ROC train: 0.957220	val: 0.735339	test: 0.712443
PRC train: 0.755464	val: 0.297894	test: 0.273228

Epoch: 64
Loss: 0.11306884202748171
ROC train: 0.957400	val: 0.733236	test: 0.706711
PRC train: 0.757764	val: 0.287084	test: 0.276510

Epoch: 65
Loss: 0.11013339420861948
ROC train: 0.959350	val: 0.734685	test: 0.710925
PRC train: 0.766619	val: 0.308812	test: 0.290484

Epoch: 66
Loss: 0.11101134595742516
ROC train: 0.958991	val: 0.736563	test: 0.711986
PRC train: 0.765282	val: 0.304266	test: 0.279110

Epoch: 67
Loss: 0.11228336035354113
ROC train: 0.960140	val: 0.716179	test: 0.705038
PRC train: 0.766573	val: 0.290597	test: 0.280021

Epoch: 68
Loss: 0.11200855977234173
ROC train: 0.961432	val: 0.737659	test: 0.717351
PRC train: 0.779323	val: 0.307523	test: 0.289654

Epoch: 69
Loss: 0.11189534224773627
ROC train: 0.960953	val: 0.716692	test: 0.701267
PRC train: 0.773033	val: 0.280187	test: 0.269576

Epoch: 70
Loss: 0.11036775726339272
ROC train: 0.963296	val: 0.730527	test: 0.715162
PRC train: 0.782501	val: 0.291994	test: 0.276470

Epoch: 71
Loss: 0.10866322939382303
ROC train: 0.955147	val: 0.719527	test: 0.709362
PRC train: 0.751441	val: 0.294312	test: 0.285264

Epoch: 72
Loss: 0.11061070084925444
ROC train: 0.959402	val: 0.728792	test: 0.717137
PRC train: 0.768367	val: 0.292266	test: 0.273308

Epoch: 73
Loss: 0.10963577083744884
ROC train: 0.964301	val: 0.727546	test: 0.715475
PRC train: 0.788930	val: 0.294517	test: 0.277125

Epoch: 74
Loss: 0.10783442571923152
ROC train: 0.963898	val: 0.723385	test: 0.707033
PRC train: 0.783542	val: 0.293343	test: 0.275628

Epoch: 75
Loss: 0.11043789827630351
ROC train: 0.963620	val: 0.716287	test: 0.702793
PRC train: 0.781558	val: 0.270521	test: 0.260864

Epoch: 76
Loss: 0.10668840883093299
ROC train: 0.964292	val: 0.735925	test: 0.715501
PRC train: 0.792757	val: 0.300902	test: 0.272668

Epoch: 77
Loss: 0.1065713829593589
ROC train: 0.967113	val: 0.732188	test: 0.712333
PRC train: 0.798865	val: 0.286219	test: 0.269424

Epoch: 78
Loss: 0.10459571788500654
ROC train: 0.967920	val: 0.719622	test: 0.695097
PRC train: 0.805686	val: 0.277745	test: 0.259031

Epoch: 79
Loss: 0.10500399182473905
ROC train: 0.968172	val: 0.713985	test: 0.699940
PRC train: 0.806702	val: 0.283553	test: 0.274606

Epoch: 80
Loss: 0.10355849560421546
ROC train: 0.969393	val: 0.726713	test: 0.708329
PRC train: 0.811932	val: 0.283121	test: 0.280597

Epoch: 81
Loss: 0.10251954162728184
ROC train: 0.970119	val: 0.718012	test: 0.700500
PRC train: 0.814983	val: 0.283483	test: 0.283480

Epoch: 82
Loss: 0.10399972912062799
ROC train: 0.971044	val: 0.719390	test: 0.709383
PRC train: 0.820645	val: 0.285191	test: 0.286357

Epoch: 83
Loss: 0.1031402855977729
ROC train: 0.970320	val: 0.723774	test: 0.711864
PRC train: 0.815434	val: 0.282102	test: 0.277523

Epoch: 84
Loss: 0.10078615152291462
ROC train: 0.970604	val: 0.724911	test: 0.705582
PRC train: 0.818280	val: 0.279210	test: 0.273059

Epoch: 85
Loss: 0.10249327763704874
ROC train: 0.972147	val: 0.726946	test: 0.712329
PRC train: 0.826819	val: 0.284095	test: 0.277692

Epoch: 86
Loss: 0.10222371327958273
ROC train: 0.972299	val: 0.717415	test: 0.703418
PRC train: 0.827267	val: 0.278263	test: 0.278659

Epoch: 87
Loss: 0.10067019621141801
ROC train: 0.972153	val: 0.717552	test: 0.703510
PRC train: 0.823697	val: 0.278926	test: 0.269545

Epoch: 88
Loss: 0.09960147323029815
ROC train: 0.971455	val: 0.714028	test: 0.702206
PRC train: 0.820838	val: 0.288096	test: 0.276254

Epoch: 89
Loss: 0.0996687176116606
ROC train: 0.972614	val: 0.728423	test: 0.713283
PRC train: 0.820208	val: 0.287337	test: 0.269689

Epoch: 90
Loss: 0.09809779034814876
ROC train: 0.974256	val: 0.717764	test: 0.704963
PRC train: 0.834586	val: 0.279928	test: 0.258783

Epoch: 91
Loss: 0.09925688348709205
ROC train: 0.973830	val: 0.722418	test: 0.710203
PRC train: 0.832249	val: 0.291073	test: 0.288164

Epoch: 92
Loss: 0.09624913540100255
ROC train: 0.975618	val: 0.716511	test: 0.702767
PRC train: 0.842285	val: 0.287285	test: 0.274877

Epoch: 93
Loss: 0.09664060342221903
ROC train: 0.975998	val: 0.727899	test: 0.717461
PRC train: 0.846024	val: 0.287715	test: 0.286094

Epoch: 94
Loss: 0.09900930056376381
ROC train: 0.923624	val: 0.745420	test: 0.729955
PRC train: 0.650545	val: 0.315045	test: 0.319635

Epoch: 34
Loss: 0.1416767095820507
ROC train: 0.925287	val: 0.744750	test: 0.728853
PRC train: 0.661622	val: 0.310020	test: 0.317979

Epoch: 35
Loss: 0.14071293626130144
ROC train: 0.925124	val: 0.745465	test: 0.730892
PRC train: 0.654970	val: 0.310988	test: 0.316457

Epoch: 36
Loss: 0.13993022756312576
ROC train: 0.926907	val: 0.751213	test: 0.741379
PRC train: 0.671296	val: 0.329000	test: 0.325180

Epoch: 37
Loss: 0.1407523732952993
ROC train: 0.930468	val: 0.739555	test: 0.729918
PRC train: 0.678401	val: 0.315525	test: 0.312141

Epoch: 38
Loss: 0.13776151067000264
ROC train: 0.931855	val: 0.734326	test: 0.719573
PRC train: 0.681276	val: 0.304981	test: 0.310706

Epoch: 39
Loss: 0.1357529073757907
ROC train: 0.931027	val: 0.732582	test: 0.721749
PRC train: 0.679563	val: 0.305714	test: 0.311740

Epoch: 40
Loss: 0.13569412674705575
ROC train: 0.935881	val: 0.741293	test: 0.728517
PRC train: 0.691406	val: 0.309346	test: 0.319263

Epoch: 41
Loss: 0.13587797426237744
ROC train: 0.934188	val: 0.732657	test: 0.722459
PRC train: 0.690595	val: 0.311521	test: 0.302010

Epoch: 42
Loss: 0.1358155003700828
ROC train: 0.935613	val: 0.737280	test: 0.720534
PRC train: 0.697186	val: 0.303720	test: 0.296077

Epoch: 43
Loss: 0.13532578769854053
ROC train: 0.937928	val: 0.734337	test: 0.719592
PRC train: 0.701097	val: 0.317601	test: 0.321774

Epoch: 44
Loss: 0.1341013560894699
ROC train: 0.941151	val: 0.731300	test: 0.721051
PRC train: 0.713416	val: 0.302994	test: 0.305323

Epoch: 45
Loss: 0.1337783578377182
ROC train: 0.940008	val: 0.737020	test: 0.725055
PRC train: 0.708517	val: 0.313931	test: 0.324553

Epoch: 46
Loss: 0.13073343287958325
ROC train: 0.941503	val: 0.732977	test: 0.720396
PRC train: 0.709824	val: 0.301843	test: 0.321931

Epoch: 47
Loss: 0.1330111870260782
ROC train: 0.943475	val: 0.735291	test: 0.721436
PRC train: 0.717885	val: 0.311593	test: 0.325811

Epoch: 48
Loss: 0.13141260985744152
ROC train: 0.943788	val: 0.740194	test: 0.729570
PRC train: 0.722959	val: 0.325954	test: 0.338311

Epoch: 49
Loss: 0.13011663252888164
ROC train: 0.945538	val: 0.741576	test: 0.721882
PRC train: 0.727361	val: 0.321209	test: 0.308388

Epoch: 50
Loss: 0.13046612983551076
ROC train: 0.945075	val: 0.738460	test: 0.720670
PRC train: 0.725682	val: 0.321107	test: 0.328180

Epoch: 51
Loss: 0.12948704809049094
ROC train: 0.947255	val: 0.744843	test: 0.723892
PRC train: 0.735098	val: 0.341876	test: 0.312124

Epoch: 52
Loss: 0.1287032924438757
ROC train: 0.947761	val: 0.744276	test: 0.728039
PRC train: 0.734694	val: 0.324424	test: 0.311307

Epoch: 53
Loss: 0.12946505540243924
ROC train: 0.948938	val: 0.738459	test: 0.724845
PRC train: 0.737892	val: 0.317564	test: 0.312419

Epoch: 54
Loss: 0.12492181437221132
ROC train: 0.950313	val: 0.736018	test: 0.711553
PRC train: 0.746004	val: 0.307237	test: 0.311162

Epoch: 55
Loss: 0.12695205102496399
ROC train: 0.950411	val: 0.736795	test: 0.721016
PRC train: 0.740245	val: 0.320129	test: 0.332307

Epoch: 56
Loss: 0.12628688778055944
ROC train: 0.952680	val: 0.738931	test: 0.726983
PRC train: 0.748112	val: 0.320838	test: 0.328429

Epoch: 57
Loss: 0.12390688534879941
ROC train: 0.953270	val: 0.734366	test: 0.718418
PRC train: 0.758707	val: 0.311104	test: 0.318013

Epoch: 58
Loss: 0.12360739678082816
ROC train: 0.954926	val: 0.737057	test: 0.715187
PRC train: 0.760031	val: 0.326057	test: 0.316991

Epoch: 59
Loss: 0.12187008245567106
ROC train: 0.956046	val: 0.729592	test: 0.710163
PRC train: 0.768115	val: 0.323140	test: 0.327068

Epoch: 60
Loss: 0.12294098898534132
ROC train: 0.956177	val: 0.730786	test: 0.713733
PRC train: 0.769072	val: 0.316120	test: 0.317258

Epoch: 61
Loss: 0.1211421720988349
ROC train: 0.955402	val: 0.738028	test: 0.715180
PRC train: 0.767258	val: 0.320005	test: 0.316951

Epoch: 62
Loss: 0.1204500704705256
ROC train: 0.957092	val: 0.730085	test: 0.716790
PRC train: 0.773393	val: 0.304835	test: 0.308918

Epoch: 63
Loss: 0.12094024572133813
ROC train: 0.958417	val: 0.738902	test: 0.724334
PRC train: 0.777652	val: 0.316068	test: 0.309024

Epoch: 64
Loss: 0.12051328183889343
ROC train: 0.960578	val: 0.739047	test: 0.717716
PRC train: 0.783254	val: 0.332148	test: 0.322604

Epoch: 65
Loss: 0.11701650688409097
ROC train: 0.960817	val: 0.734761	test: 0.709149
PRC train: 0.785944	val: 0.319559	test: 0.312801

Epoch: 66
Loss: 0.11724741274846767
ROC train: 0.961468	val: 0.736560	test: 0.714662
PRC train: 0.785816	val: 0.316236	test: 0.307533

Epoch: 67
Loss: 0.11645160582548172
ROC train: 0.960077	val: 0.733269	test: 0.713474
PRC train: 0.780945	val: 0.334663	test: 0.304334

Epoch: 68
Loss: 0.11733060319575944
ROC train: 0.963019	val: 0.730364	test: 0.712243
PRC train: 0.789306	val: 0.303412	test: 0.300342

Epoch: 69
Loss: 0.11603534054802025
ROC train: 0.963306	val: 0.742581	test: 0.716491
PRC train: 0.795590	val: 0.327834	test: 0.318296

Epoch: 70
Loss: 0.11475245285326108
ROC train: 0.964709	val: 0.738664	test: 0.710071
PRC train: 0.800133	val: 0.314448	test: 0.313902

Epoch: 71
Loss: 0.11593300963032599
ROC train: 0.964698	val: 0.724094	test: 0.708247
PRC train: 0.801308	val: 0.318276	test: 0.306579

Epoch: 72
Loss: 0.11644325289544012
ROC train: 0.966439	val: 0.739996	test: 0.716834
PRC train: 0.808085	val: 0.321797	test: 0.313850

Epoch: 73
Loss: 0.11381998236821884
ROC train: 0.967210	val: 0.736428	test: 0.712035
PRC train: 0.804754	val: 0.301586	test: 0.309561

Epoch: 74
Loss: 0.11501885810888605
ROC train: 0.965437	val: 0.732863	test: 0.713156
PRC train: 0.801593	val: 0.319678	test: 0.318921

Epoch: 75
Loss: 0.11253009109852231
ROC train: 0.967840	val: 0.729658	test: 0.708699
PRC train: 0.809668	val: 0.308431	test: 0.315353

Epoch: 76
Loss: 0.11263900759937724
ROC train: 0.967961	val: 0.733403	test: 0.714661
PRC train: 0.808178	val: 0.324478	test: 0.315571

Epoch: 77
Loss: 0.11079679047071248
ROC train: 0.969973	val: 0.724165	test: 0.701083
PRC train: 0.821228	val: 0.302783	test: 0.299721

Epoch: 78
Loss: 0.11068759333834698
ROC train: 0.970371	val: 0.737450	test: 0.715178
PRC train: 0.820189	val: 0.326325	test: 0.316005

Epoch: 79
Loss: 0.1098372150226306
ROC train: 0.969245	val: 0.740319	test: 0.718416
PRC train: 0.819232	val: 0.316969	test: 0.318766

Epoch: 80
Loss: 0.10826795226543508
ROC train: 0.969265	val: 0.740996	test: 0.714391
PRC train: 0.823957	val: 0.321339	test: 0.315206

Epoch: 81
Loss: 0.10957040659535372
ROC train: 0.971038	val: 0.733514	test: 0.710615
PRC train: 0.825817	val: 0.328958	test: 0.313993

Epoch: 82
Loss: 0.10613478890673739
ROC train: 0.972809	val: 0.727891	test: 0.709273
PRC train: 0.832714	val: 0.330357	test: 0.315625

Epoch: 83
Loss: 0.10637673541099182
ROC train: 0.973310	val: 0.724742	test: 0.701180
PRC train: 0.834967	val: 0.317526	test: 0.309788

Epoch: 84
Loss: 0.10793235378324563
ROC train: 0.972731	val: 0.736339	test: 0.716732
PRC train: 0.833905	val: 0.309390	test: 0.316793

Epoch: 85
Loss: 0.10603773423747569
ROC train: 0.973248	val: 0.724841	test: 0.702268
PRC train: 0.832507	val: 0.311264	test: 0.310783

Epoch: 86
Loss: 0.10717916997782923
ROC train: 0.974477	val: 0.730304	test: 0.718248
PRC train: 0.840347	val: 0.315969	test: 0.316202

Epoch: 87
Loss: 0.10566955950217767
ROC train: 0.975394	val: 0.728072	test: 0.713673
PRC train: 0.844001	val: 0.299820	test: 0.305495

Epoch: 88
Loss: 0.10561115618322967
ROC train: 0.974801	val: 0.727451	test: 0.709666
PRC train: 0.838453	val: 0.297754	test: 0.305336

Epoch: 89
Loss: 0.1066061516583091
ROC train: 0.974813	val: 0.733850	test: 0.721148
PRC train: 0.842226	val: 0.303789	test: 0.313861

Epoch: 90
Loss: 0.10413741978344639
ROC train: 0.976802	val: 0.730774	test: 0.713669
PRC train: 0.852087	val: 0.316302	test: 0.310184

Epoch: 91
Loss: 0.10417626145674555
ROC train: 0.976315	val: 0.727159	test: 0.709456
PRC train: 0.849627	val: 0.310847	test: 0.289700

Epoch: 92
Loss: 0.1025919828055827
ROC train: 0.977150	val: 0.734269	test: 0.718304
PRC train: 0.852600	val: 0.310687	test: 0.307158

Epoch: 93
Loss: 0.10145478650844474
ROC train: 0.978357	val: 0.730647	test: 0.711742
PRC train: 0.857081	val: 0.311394	test: 0.308095

Epoch: 94
Loss: 0.10136252997784391
ROC train: 0.925402	val: 0.742818	test: 0.732646
PRC train: 0.661518	val: 0.346573	test: 0.331547

Epoch: 34
Loss: 0.14059460143166255
ROC train: 0.924077	val: 0.743660	test: 0.737106
PRC train: 0.653443	val: 0.342041	test: 0.329780

Epoch: 35
Loss: 0.1424438607975173
ROC train: 0.929707	val: 0.745259	test: 0.734979
PRC train: 0.672373	val: 0.335730	test: 0.320266

Epoch: 36
Loss: 0.14049484344542928
ROC train: 0.928251	val: 0.749217	test: 0.742093
PRC train: 0.669660	val: 0.335674	test: 0.327358

Epoch: 37
Loss: 0.13763929497817476
ROC train: 0.929357	val: 0.748610	test: 0.743880
PRC train: 0.674324	val: 0.352288	test: 0.329979

Epoch: 38
Loss: 0.1382129532452158
ROC train: 0.931026	val: 0.743992	test: 0.734738
PRC train: 0.678478	val: 0.338141	test: 0.313965

Epoch: 39
Loss: 0.13773455791967446
ROC train: 0.934356	val: 0.738896	test: 0.735981
PRC train: 0.688449	val: 0.343598	test: 0.328614

Epoch: 40
Loss: 0.1378742359326518
ROC train: 0.932617	val: 0.751399	test: 0.740212
PRC train: 0.685022	val: 0.342775	test: 0.330498

Epoch: 41
Loss: 0.13422337797099343
ROC train: 0.934753	val: 0.728943	test: 0.733625
PRC train: 0.692906	val: 0.332056	test: 0.316122

Epoch: 42
Loss: 0.1366030782642363
ROC train: 0.935759	val: 0.737369	test: 0.735521
PRC train: 0.687402	val: 0.337738	test: 0.333567

Epoch: 43
Loss: 0.1343090141583611
ROC train: 0.936723	val: 0.748207	test: 0.740062
PRC train: 0.693137	val: 0.344189	test: 0.332315

Epoch: 44
Loss: 0.13444044559177454
ROC train: 0.940201	val: 0.736636	test: 0.726845
PRC train: 0.707546	val: 0.329830	test: 0.319856

Epoch: 45
Loss: 0.13207204796932134
ROC train: 0.938231	val: 0.728460	test: 0.729141
PRC train: 0.692058	val: 0.326292	test: 0.321043

Epoch: 46
Loss: 0.13074505076206422
ROC train: 0.939881	val: 0.728620	test: 0.714356
PRC train: 0.706446	val: 0.310274	test: 0.311566

Epoch: 47
Loss: 0.1296267778343338
ROC train: 0.942781	val: 0.730789	test: 0.729513
PRC train: 0.718463	val: 0.316225	test: 0.323894

Epoch: 48
Loss: 0.1294085257106935
ROC train: 0.940971	val: 0.727807	test: 0.725302
PRC train: 0.704561	val: 0.317288	test: 0.320083

Epoch: 49
Loss: 0.13083032219076535
ROC train: 0.945261	val: 0.729378	test: 0.719224
PRC train: 0.726056	val: 0.339400	test: 0.318928

Epoch: 50
Loss: 0.13016533476880346
ROC train: 0.944436	val: 0.733455	test: 0.724059
PRC train: 0.720801	val: 0.335897	test: 0.323759

Epoch: 51
Loss: 0.13038214244249113
ROC train: 0.948065	val: 0.729866	test: 0.726675
PRC train: 0.729447	val: 0.340357	test: 0.322212

Epoch: 52
Loss: 0.128023747607644
ROC train: 0.946327	val: 0.725883	test: 0.728779
PRC train: 0.724666	val: 0.329472	test: 0.311723

Epoch: 53
Loss: 0.1285364087109761
ROC train: 0.945456	val: 0.727081	test: 0.721689
PRC train: 0.728079	val: 0.315853	test: 0.316085

Epoch: 54
Loss: 0.12586412182329554
ROC train: 0.949589	val: 0.731732	test: 0.724532
PRC train: 0.733566	val: 0.327276	test: 0.329412

Epoch: 55
Loss: 0.1239868188529169
ROC train: 0.952664	val: 0.726690	test: 0.724237
PRC train: 0.749299	val: 0.331139	test: 0.322947

Epoch: 56
Loss: 0.12372846880112714
ROC train: 0.952106	val: 0.726281	test: 0.724246
PRC train: 0.742590	val: 0.331588	test: 0.318470

Epoch: 57
Loss: 0.12397568736088115
ROC train: 0.952564	val: 0.731748	test: 0.731226
PRC train: 0.748430	val: 0.332763	test: 0.329320

Epoch: 58
Loss: 0.12269333616751098
ROC train: 0.954602	val: 0.733365	test: 0.729400
PRC train: 0.756071	val: 0.341566	test: 0.326097

Epoch: 59
Loss: 0.12104622042309097
ROC train: 0.956220	val: 0.722564	test: 0.728692
PRC train: 0.762752	val: 0.343127	test: 0.333050

Epoch: 60
Loss: 0.1223408660972837
ROC train: 0.956878	val: 0.712870	test: 0.726257
PRC train: 0.763322	val: 0.325360	test: 0.323774

Epoch: 61
Loss: 0.11813068512173987
ROC train: 0.955861	val: 0.710236	test: 0.718892
PRC train: 0.750092	val: 0.310045	test: 0.317907

Epoch: 62
Loss: 0.12031312442075137
ROC train: 0.957291	val: 0.717186	test: 0.720826
PRC train: 0.760419	val: 0.312814	test: 0.311455

Epoch: 63
Loss: 0.12101618938014322
ROC train: 0.955785	val: 0.727229	test: 0.728301
PRC train: 0.760066	val: 0.332474	test: 0.328162

Epoch: 64
Loss: 0.12095161730235336
ROC train: 0.954950	val: 0.719897	test: 0.722619
PRC train: 0.757105	val: 0.330135	test: 0.326475

Epoch: 65
Loss: 0.11993391569030912
ROC train: 0.959753	val: 0.726127	test: 0.724944
PRC train: 0.776595	val: 0.334575	test: 0.315305

Epoch: 66
Loss: 0.1191040919732912
ROC train: 0.960941	val: 0.724243	test: 0.719690
PRC train: 0.780233	val: 0.328933	test: 0.322599

Epoch: 67
Loss: 0.11969835512644256
ROC train: 0.962560	val: 0.726250	test: 0.728580
PRC train: 0.783892	val: 0.333630	test: 0.330551

Epoch: 68
Loss: 0.11649154876761365
ROC train: 0.962889	val: 0.730619	test: 0.724122
PRC train: 0.786944	val: 0.333462	test: 0.315498

Epoch: 69
Loss: 0.11477756132998708
ROC train: 0.964517	val: 0.720956	test: 0.720707
PRC train: 0.790785	val: 0.327425	test: 0.324568

Epoch: 70
Loss: 0.11494165988539375
ROC train: 0.963895	val: 0.726060	test: 0.728282
PRC train: 0.790429	val: 0.338764	test: 0.325737

Epoch: 71
Loss: 0.1161011596408217
ROC train: 0.961447	val: 0.709366	test: 0.705801
PRC train: 0.774080	val: 0.311453	test: 0.309291

Epoch: 72
Loss: 0.11667713043105171
ROC train: 0.965987	val: 0.724916	test: 0.724507
PRC train: 0.798627	val: 0.331716	test: 0.327147

Epoch: 73
Loss: 0.11393477899395883
ROC train: 0.966857	val: 0.730281	test: 0.723987
PRC train: 0.802110	val: 0.335864	test: 0.327411

Epoch: 74
Loss: 0.11343973297645053
ROC train: 0.967787	val: 0.726543	test: 0.725024
PRC train: 0.806532	val: 0.335587	test: 0.333778

Epoch: 75
Loss: 0.11247970253026077
ROC train: 0.967522	val: 0.723929	test: 0.724158
PRC train: 0.802182	val: 0.336778	test: 0.333035

Epoch: 76
Loss: 0.11062611330454457
ROC train: 0.968762	val: 0.720428	test: 0.724066
PRC train: 0.810488	val: 0.320541	test: 0.326473

Epoch: 77
Loss: 0.1110378997180757
ROC train: 0.969108	val: 0.716274	test: 0.718228
PRC train: 0.812874	val: 0.322992	test: 0.322347

Epoch: 78
Loss: 0.1096505225921934
ROC train: 0.969935	val: 0.730030	test: 0.725930
PRC train: 0.816835	val: 0.326650	test: 0.326005

Epoch: 79
Loss: 0.1101644540834668
ROC train: 0.967902	val: 0.709988	test: 0.715486
PRC train: 0.807011	val: 0.309253	test: 0.316344

Epoch: 80
Loss: 0.10997484111531675
ROC train: 0.968794	val: 0.718780	test: 0.713336
PRC train: 0.803107	val: 0.317397	test: 0.310205

Epoch: 81
Loss: 0.11152834820987884
ROC train: 0.970244	val: 0.717778	test: 0.709704
PRC train: 0.817072	val: 0.321491	test: 0.311292

Epoch: 82
Loss: 0.11049275105102058
ROC train: 0.970248	val: 0.725114	test: 0.720461
PRC train: 0.819043	val: 0.333687	test: 0.323892

Epoch: 83
Loss: 0.10818027904314655
ROC train: 0.970757	val: 0.718327	test: 0.720499
PRC train: 0.822468	val: 0.330022	test: 0.318316

Epoch: 84
Loss: 0.10950463377295051
ROC train: 0.969774	val: 0.723965	test: 0.724880
PRC train: 0.821748	val: 0.325421	test: 0.320482

Epoch: 85
Loss: 0.10742298400427809
ROC train: 0.973458	val: 0.716838	test: 0.722140
PRC train: 0.837382	val: 0.323917	test: 0.322623

Epoch: 86
Loss: 0.10801484520064074
ROC train: 0.973620	val: 0.729490	test: 0.727917
PRC train: 0.832069	val: 0.329792	test: 0.311737

Epoch: 87
Loss: 0.10506625114291622
ROC train: 0.973917	val: 0.727012	test: 0.736050
PRC train: 0.837410	val: 0.340608	test: 0.326507

Epoch: 88
Loss: 0.1067252345629579
ROC train: 0.973684	val: 0.718535	test: 0.727527
PRC train: 0.836719	val: 0.331637	test: 0.331025

Epoch: 89
Loss: 0.10463304696491611
ROC train: 0.974239	val: 0.724887	test: 0.730975
PRC train: 0.842440	val: 0.328384	test: 0.336443

Epoch: 90
Loss: 0.10596773761176745
ROC train: 0.974189	val: 0.723447	test: 0.722158
PRC train: 0.838922	val: 0.329808	test: 0.323605

Epoch: 91
Loss: 0.1048223946020297
ROC train: 0.976242	val: 0.720709	test: 0.722463
PRC train: 0.847813	val: 0.309855	test: 0.309670

Epoch: 92
Loss: 0.10541470433578679
ROC train: 0.976756	val: 0.729787	test: 0.724644
PRC train: 0.850777	val: 0.335832	test: 0.314413

Epoch: 93
Loss: 0.10110171188347579
ROC train: 0.976824	val: 0.722136	test: 0.725887
PRC train: 0.847678	val: 0.327183	test: 0.316259

Epoch: 94
Loss: 0.101771551371381
ROC train: 0.924975	val: 0.751155	test: 0.735673
PRC train: 0.650009	val: 0.335702	test: 0.324293

Epoch: 34
Loss: 0.1408024118796677
ROC train: 0.925199	val: 0.744530	test: 0.742870
PRC train: 0.657129	val: 0.321275	test: 0.330619

Epoch: 35
Loss: 0.1408803855041721
ROC train: 0.927081	val: 0.755678	test: 0.739279
PRC train: 0.665360	val: 0.344500	test: 0.326048

Epoch: 36
Loss: 0.1391991818986213
ROC train: 0.928143	val: 0.730495	test: 0.737580
PRC train: 0.666063	val: 0.325981	test: 0.315407

Epoch: 37
Loss: 0.1369061416675965
ROC train: 0.931499	val: 0.746507	test: 0.735635
PRC train: 0.679800	val: 0.326983	test: 0.315134

Epoch: 38
Loss: 0.13641348335824002
ROC train: 0.932266	val: 0.749875	test: 0.739319
PRC train: 0.683926	val: 0.340769	test: 0.323690

Epoch: 39
Loss: 0.1372252711202705
ROC train: 0.931767	val: 0.761246	test: 0.746758
PRC train: 0.682205	val: 0.348843	test: 0.335368

Epoch: 40
Loss: 0.13571089507032436
ROC train: 0.932782	val: 0.746529	test: 0.742660
PRC train: 0.682249	val: 0.342434	test: 0.324660

Epoch: 41
Loss: 0.13613539180689538
ROC train: 0.935467	val: 0.739780	test: 0.735382
PRC train: 0.692494	val: 0.337743	test: 0.328189

Epoch: 42
Loss: 0.13323234698258146
ROC train: 0.938855	val: 0.742504	test: 0.732380
PRC train: 0.708926	val: 0.327465	test: 0.322616

Epoch: 43
Loss: 0.1335467307953914
ROC train: 0.940828	val: 0.743820	test: 0.728744
PRC train: 0.706452	val: 0.337612	test: 0.326249

Epoch: 44
Loss: 0.13038206149414006
ROC train: 0.939152	val: 0.740084	test: 0.728874
PRC train: 0.705838	val: 0.322875	test: 0.319984

Epoch: 45
Loss: 0.13173028008261697
ROC train: 0.939410	val: 0.747791	test: 0.738636
PRC train: 0.707032	val: 0.332432	test: 0.321295

Epoch: 46
Loss: 0.13178625391915436
ROC train: 0.940152	val: 0.740058	test: 0.732779
PRC train: 0.708367	val: 0.338761	test: 0.317136

Epoch: 47
Loss: 0.1290079858617226
ROC train: 0.945647	val: 0.741443	test: 0.729885
PRC train: 0.727508	val: 0.325647	test: 0.307222

Epoch: 48
Loss: 0.13220923222689882
ROC train: 0.942686	val: 0.737957	test: 0.725670
PRC train: 0.713408	val: 0.323428	test: 0.310413

Epoch: 49
Loss: 0.1281388170687384
ROC train: 0.945124	val: 0.739605	test: 0.731552
PRC train: 0.716645	val: 0.341184	test: 0.317794

Epoch: 50
Loss: 0.12960741670717513
ROC train: 0.948888	val: 0.760385	test: 0.736207
PRC train: 0.736498	val: 0.350862	test: 0.321506

Epoch: 51
Loss: 0.13010118525567613
ROC train: 0.946904	val: 0.746940	test: 0.726547
PRC train: 0.728148	val: 0.322103	test: 0.303602

Epoch: 52
Loss: 0.12598571326326613
ROC train: 0.950173	val: 0.740867	test: 0.728752
PRC train: 0.738973	val: 0.331063	test: 0.316905

Epoch: 53
Loss: 0.12917899294464338
ROC train: 0.948079	val: 0.730340	test: 0.725615
PRC train: 0.735613	val: 0.338199	test: 0.322593

Epoch: 54
Loss: 0.12739857619962122
ROC train: 0.950389	val: 0.747107	test: 0.729571
PRC train: 0.740014	val: 0.337950	test: 0.320593

Epoch: 55
Loss: 0.12594522314085116
ROC train: 0.950692	val: 0.750612	test: 0.729947
PRC train: 0.742030	val: 0.349958	test: 0.317842

Epoch: 56
Loss: 0.12592774097364215
ROC train: 0.951240	val: 0.740249	test: 0.722110
PRC train: 0.750940	val: 0.335220	test: 0.318114

Epoch: 57
Loss: 0.1213972803847652
ROC train: 0.954127	val: 0.744096	test: 0.731640
PRC train: 0.759225	val: 0.335693	test: 0.319262

Epoch: 58
Loss: 0.12480369460515396
ROC train: 0.955387	val: 0.739854	test: 0.723382
PRC train: 0.759092	val: 0.336091	test: 0.314543

Epoch: 59
Loss: 0.12204098648306196
ROC train: 0.956804	val: 0.742904	test: 0.725629
PRC train: 0.765540	val: 0.339461	test: 0.312040

Epoch: 60
Loss: 0.12354905126071744
ROC train: 0.956441	val: 0.728024	test: 0.716512
PRC train: 0.766271	val: 0.325975	test: 0.306953

Epoch: 61
Loss: 0.12226565274073421
ROC train: 0.957008	val: 0.737323	test: 0.729118
PRC train: 0.768768	val: 0.336877	test: 0.317115

Epoch: 62
Loss: 0.12249414614382688
ROC train: 0.957717	val: 0.735345	test: 0.722172
PRC train: 0.768226	val: 0.340030	test: 0.319887

Epoch: 63
Loss: 0.11849547996741086
ROC train: 0.959131	val: 0.742681	test: 0.729199
PRC train: 0.773784	val: 0.345135	test: 0.324389

Epoch: 64
Loss: 0.11902617676344147
ROC train: 0.956911	val: 0.743528	test: 0.728599
PRC train: 0.768525	val: 0.335540	test: 0.320778

Epoch: 65
Loss: 0.11889224338346166
ROC train: 0.960074	val: 0.729011	test: 0.716356
PRC train: 0.774845	val: 0.328558	test: 0.312011

Epoch: 66
Loss: 0.11940036048200295
ROC train: 0.961019	val: 0.733601	test: 0.715835
PRC train: 0.781723	val: 0.335112	test: 0.320523

Epoch: 67
Loss: 0.11706614594389038
ROC train: 0.962597	val: 0.727102	test: 0.715847
PRC train: 0.788528	val: 0.319837	test: 0.308324

Epoch: 68
Loss: 0.11703796838391076
ROC train: 0.963047	val: 0.732621	test: 0.716265
PRC train: 0.794388	val: 0.338308	test: 0.314083

Epoch: 69
Loss: 0.11467896472069049
ROC train: 0.964106	val: 0.729325	test: 0.720245
PRC train: 0.797591	val: 0.330340	test: 0.317612

Epoch: 70
Loss: 0.11639221145588134
ROC train: 0.962731	val: 0.728376	test: 0.719215
PRC train: 0.785775	val: 0.335255	test: 0.316495

Epoch: 71
Loss: 0.11725096761106055
ROC train: 0.965253	val: 0.731763	test: 0.722164
PRC train: 0.802701	val: 0.324118	test: 0.307842

Epoch: 72
Loss: 0.11299423562256901
ROC train: 0.964369	val: 0.731669	test: 0.713700
PRC train: 0.802062	val: 0.321693	test: 0.308014

Epoch: 73
Loss: 0.11267433336356789
ROC train: 0.966843	val: 0.734300	test: 0.715445
PRC train: 0.807615	val: 0.327691	test: 0.309690

Epoch: 74
Loss: 0.11191686694864553
ROC train: 0.967214	val: 0.733942	test: 0.709887
PRC train: 0.810949	val: 0.316399	test: 0.306893

Epoch: 75
Loss: 0.11254318864868265
ROC train: 0.967921	val: 0.730331	test: 0.715375
PRC train: 0.812447	val: 0.325523	test: 0.307469

Epoch: 76
Loss: 0.11238522698197037
ROC train: 0.968169	val: 0.733462	test: 0.716821
PRC train: 0.814876	val: 0.329047	test: 0.316000

Epoch: 77
Loss: 0.11086158913465492
ROC train: 0.968793	val: 0.730083	test: 0.712440
PRC train: 0.818606	val: 0.317916	test: 0.313446

Epoch: 78
Loss: 0.11140833072055395
ROC train: 0.968745	val: 0.734094	test: 0.717198
PRC train: 0.813339	val: 0.340622	test: 0.325045

Epoch: 79
Loss: 0.11076842572516159
ROC train: 0.969660	val: 0.720504	test: 0.712897
PRC train: 0.819163	val: 0.315197	test: 0.316797

Epoch: 80
Loss: 0.11065405209688518
ROC train: 0.971254	val: 0.719512	test: 0.713745
PRC train: 0.828493	val: 0.315848	test: 0.318086

Epoch: 81
Loss: 0.11126439301244324
ROC train: 0.970104	val: 0.729862	test: 0.715412
PRC train: 0.819906	val: 0.329552	test: 0.318534

Epoch: 82
Loss: 0.10924408548918661
ROC train: 0.969389	val: 0.723084	test: 0.719705
PRC train: 0.822337	val: 0.319180	test: 0.314323

Epoch: 83
Loss: 0.10750613582607181
ROC train: 0.970067	val: 0.728533	test: 0.714920
PRC train: 0.825038	val: 0.326727	test: 0.322652

Epoch: 84
Loss: 0.10608212854168585
ROC train: 0.972853	val: 0.722690	test: 0.707698
PRC train: 0.831772	val: 0.323928	test: 0.307363

Epoch: 85
Loss: 0.10875849632551199
ROC train: 0.973220	val: 0.716994	test: 0.703543
PRC train: 0.836357	val: 0.307626	test: 0.304749

Epoch: 86
Loss: 0.10820012390218059
ROC train: 0.973084	val: 0.718086	test: 0.704852
PRC train: 0.837744	val: 0.315051	test: 0.314353

Epoch: 87
Loss: 0.10506025462888356
ROC train: 0.972814	val: 0.728828	test: 0.716658
PRC train: 0.837847	val: 0.322175	test: 0.327831

Epoch: 88
Loss: 0.10742986612046278
ROC train: 0.973630	val: 0.717755	test: 0.698083
PRC train: 0.836841	val: 0.307147	test: 0.293631

Epoch: 89
Loss: 0.10537995562598614
ROC train: 0.975567	val: 0.720652	test: 0.713995
PRC train: 0.847551	val: 0.325873	test: 0.315415

Epoch: 90
Loss: 0.10269749808135237
ROC train: 0.975330	val: 0.725733	test: 0.710931
PRC train: 0.848424	val: 0.321906	test: 0.312357

Epoch: 91
Loss: 0.10263949691357611
ROC train: 0.974890	val: 0.721366	test: 0.706057
PRC train: 0.846354	val: 0.310201	test: 0.301356

Epoch: 92
Loss: 0.10464798085454663
ROC train: 0.975025	val: 0.732267	test: 0.717874
PRC train: 0.850106	val: 0.319682	test: 0.308679

Epoch: 93
Loss: 0.10320698542530525
ROC train: 0.975569	val: 0.724445	test: 0.709455
PRC train: 0.850443	val: 0.316582	test: 0.307828

Epoch: 94
Loss: 0.10116103336414807
ROC train: 0.920679	val: 0.779205	test: 0.739175
PRC train: 0.641564	val: 0.355663	test: 0.357782

Epoch: 34
Loss: 0.1512485756882844
ROC train: 0.919479	val: 0.762740	test: 0.742757
PRC train: 0.631373	val: 0.321559	test: 0.333226

Epoch: 35
Loss: 0.15038690347762987
ROC train: 0.921563	val: 0.779360	test: 0.737242
PRC train: 0.644150	val: 0.348993	test: 0.346953

Epoch: 36
Loss: 0.14935128074508025
ROC train: 0.923583	val: 0.781839	test: 0.738330
PRC train: 0.649811	val: 0.345434	test: 0.347962

Epoch: 37
Loss: 0.1496141039079624
ROC train: 0.923648	val: 0.772450	test: 0.746403
PRC train: 0.656186	val: 0.374857	test: 0.352551

Epoch: 38
Loss: 0.14759524252987063
ROC train: 0.925622	val: 0.772732	test: 0.736883
PRC train: 0.648208	val: 0.345703	test: 0.353800

Epoch: 39
Loss: 0.14639580600947627
ROC train: 0.928264	val: 0.774474	test: 0.739422
PRC train: 0.665484	val: 0.349446	test: 0.365084

Epoch: 40
Loss: 0.14504657825184272
ROC train: 0.931960	val: 0.777010	test: 0.738037
PRC train: 0.672043	val: 0.353446	test: 0.365421

Epoch: 41
Loss: 0.14466253891525144
ROC train: 0.932051	val: 0.763479	test: 0.727684
PRC train: 0.676502	val: 0.343978	test: 0.355438

Epoch: 42
Loss: 0.14323996174044332
ROC train: 0.932489	val: 0.772134	test: 0.737617
PRC train: 0.681561	val: 0.343047	test: 0.365987

Epoch: 43
Loss: 0.1441149097047845
ROC train: 0.930925	val: 0.767023	test: 0.739007
PRC train: 0.678286	val: 0.363668	test: 0.361009

Epoch: 44
Loss: 0.1421751930830825
ROC train: 0.933411	val: 0.766815	test: 0.737403
PRC train: 0.672042	val: 0.346683	test: 0.343466

Epoch: 45
Loss: 0.14029546621340439
ROC train: 0.939607	val: 0.766988	test: 0.737919
PRC train: 0.700665	val: 0.356597	test: 0.359914

Epoch: 46
Loss: 0.13920123024973063
ROC train: 0.938414	val: 0.766131	test: 0.739391
PRC train: 0.694177	val: 0.335638	test: 0.354646

Epoch: 47
Loss: 0.140113560820061
ROC train: 0.938078	val: 0.769953	test: 0.739734
PRC train: 0.696424	val: 0.360555	test: 0.351400

Epoch: 48
Loss: 0.13974715372121327
ROC train: 0.941447	val: 0.763355	test: 0.743525
PRC train: 0.705861	val: 0.349053	test: 0.342020

Epoch: 49
Loss: 0.13796250397426257
ROC train: 0.941898	val: 0.769373	test: 0.741417
PRC train: 0.708046	val: 0.353275	test: 0.357682

Epoch: 50
Loss: 0.13780003170817945
ROC train: 0.940425	val: 0.766254	test: 0.738088
PRC train: 0.709490	val: 0.344063	test: 0.350827

Epoch: 51
Loss: 0.13751404884457086
ROC train: 0.944012	val: 0.766283	test: 0.736868
PRC train: 0.722836	val: 0.355710	test: 0.364546

Epoch: 52
Loss: 0.13496856582545033
ROC train: 0.945457	val: 0.763267	test: 0.731261
PRC train: 0.721975	val: 0.356962	test: 0.371904

Epoch: 53
Loss: 0.1329618196557564
ROC train: 0.946485	val: 0.768373	test: 0.742301
PRC train: 0.729847	val: 0.360253	test: 0.365689

Epoch: 54
Loss: 0.13250671022292962
ROC train: 0.947114	val: 0.761820	test: 0.749010
PRC train: 0.723638	val: 0.349104	test: 0.377148

Epoch: 55
Loss: 0.13468049898474016
ROC train: 0.945972	val: 0.765066	test: 0.727725
PRC train: 0.722538	val: 0.354997	test: 0.358510

Epoch: 56
Loss: 0.13450561848168238
ROC train: 0.949212	val: 0.771601	test: 0.746481
PRC train: 0.740047	val: 0.363473	test: 0.373667

Epoch: 57
Loss: 0.13098804267998967
ROC train: 0.951184	val: 0.770633	test: 0.741721
PRC train: 0.743240	val: 0.353301	test: 0.365742

Epoch: 58
Loss: 0.1302176128955788
ROC train: 0.953208	val: 0.757483	test: 0.742364
PRC train: 0.750495	val: 0.349311	test: 0.363506

Epoch: 59
Loss: 0.12943617351048184
ROC train: 0.953597	val: 0.763040	test: 0.741522
PRC train: 0.756834	val: 0.356559	test: 0.361256

Epoch: 60
Loss: 0.13019241418067515
ROC train: 0.952149	val: 0.764459	test: 0.735754
PRC train: 0.754015	val: 0.351261	test: 0.363910

Epoch: 61
Loss: 0.13008335038394353
ROC train: 0.952271	val: 0.760102	test: 0.750016
PRC train: 0.751158	val: 0.352800	test: 0.372654

Epoch: 62
Loss: 0.12914098761535814
ROC train: 0.956072	val: 0.763668	test: 0.743391
PRC train: 0.764399	val: 0.356764	test: 0.354839

Epoch: 63
Loss: 0.1283742770653206
ROC train: 0.956285	val: 0.763878	test: 0.741554
PRC train: 0.767666	val: 0.354717	test: 0.360283

Epoch: 64
Loss: 0.12577756056939177
ROC train: 0.959989	val: 0.748658	test: 0.741069
PRC train: 0.778598	val: 0.330352	test: 0.373572

Epoch: 65
Loss: 0.12761315481149219
ROC train: 0.958721	val: 0.757577	test: 0.739801
PRC train: 0.777092	val: 0.350531	test: 0.370386

Epoch: 66
Loss: 0.12600901509895704
ROC train: 0.959739	val: 0.756389	test: 0.744084
PRC train: 0.782857	val: 0.353245	test: 0.374435

Epoch: 67
Loss: 0.12469385431636061
ROC train: 0.958667	val: 0.757997	test: 0.741134
PRC train: 0.774901	val: 0.351798	test: 0.358651

Epoch: 68
Loss: 0.12515372828552673
ROC train: 0.960001	val: 0.764626	test: 0.741684
PRC train: 0.782827	val: 0.355127	test: 0.360978

Epoch: 69
Loss: 0.12471200192117607
ROC train: 0.960330	val: 0.764688	test: 0.733643
PRC train: 0.781816	val: 0.353243	test: 0.365478

Epoch: 70
Loss: 0.12546767026752337
ROC train: 0.961377	val: 0.762113	test: 0.736131
PRC train: 0.782200	val: 0.345261	test: 0.352021

Epoch: 71
Loss: 0.12321163149651243
ROC train: 0.962434	val: 0.756494	test: 0.734035
PRC train: 0.792697	val: 0.352827	test: 0.367027

Epoch: 72
Loss: 0.12136794768156095
ROC train: 0.961345	val: 0.769744	test: 0.746870
PRC train: 0.788881	val: 0.356789	test: 0.365046

Epoch: 73
Loss: 0.12113587672820092
ROC train: 0.962542	val: 0.752847	test: 0.741533
PRC train: 0.783349	val: 0.352161	test: 0.367235

Epoch: 74
Loss: 0.11896777699290138
ROC train: 0.964606	val: 0.767740	test: 0.742595
PRC train: 0.803211	val: 0.368888	test: 0.373354

Epoch: 75
Loss: 0.11992225188516156
ROC train: 0.965057	val: 0.764093	test: 0.742688
PRC train: 0.799638	val: 0.364298	test: 0.369540

Epoch: 76
Loss: 0.11820175267535954
ROC train: 0.966602	val: 0.754060	test: 0.737777
PRC train: 0.813315	val: 0.359520	test: 0.357204

Epoch: 77
Loss: 0.1188201716431032
ROC train: 0.965700	val: 0.762309	test: 0.741111
PRC train: 0.807978	val: 0.356147	test: 0.375411

Epoch: 78
Loss: 0.11971332438557254
ROC train: 0.967460	val: 0.753554	test: 0.743634
PRC train: 0.815675	val: 0.354275	test: 0.360324

Epoch: 79
Loss: 0.1187923487548725
ROC train: 0.966965	val: 0.755626	test: 0.731980
PRC train: 0.809006	val: 0.359205	test: 0.372236

Epoch: 80
Loss: 0.11728765116201653
ROC train: 0.969568	val: 0.752535	test: 0.729035
PRC train: 0.826610	val: 0.356778	test: 0.353768

Epoch: 81
Loss: 0.11577525824159209
ROC train: 0.967773	val: 0.768200	test: 0.745102
PRC train: 0.820827	val: 0.372603	test: 0.373153

Epoch: 82
Loss: 0.1126844154351235
ROC train: 0.970547	val: 0.747757	test: 0.736446
PRC train: 0.825906	val: 0.352728	test: 0.350982

Epoch: 83
Loss: 0.11483524505416859
ROC train: 0.971041	val: 0.759539	test: 0.739759
PRC train: 0.831426	val: 0.352063	test: 0.352953

Epoch: 84
Loss: 0.11062978415053813
ROC train: 0.970918	val: 0.762742	test: 0.743082
PRC train: 0.830924	val: 0.357970	test: 0.372025

Epoch: 85
Loss: 0.11253660807476541
ROC train: 0.972273	val: 0.751635	test: 0.741017
PRC train: 0.837208	val: 0.343191	test: 0.362570

Epoch: 86
Loss: 0.11269895103140343
ROC train: 0.972572	val: 0.752159	test: 0.739463
PRC train: 0.837252	val: 0.349866	test: 0.372480

Epoch: 87
Loss: 0.11302403183076999
ROC train: 0.973378	val: 0.761934	test: 0.741380
PRC train: 0.842010	val: 0.361320	test: 0.367076

Epoch: 88
Loss: 0.11310637060905576
ROC train: 0.972710	val: 0.747358	test: 0.738258
PRC train: 0.838653	val: 0.341069	test: 0.357757

Epoch: 89
Loss: 0.1117166715971039
ROC train: 0.972260	val: 0.760571	test: 0.740039
PRC train: 0.837359	val: 0.366442	test: 0.370836

Epoch: 90
Loss: 0.11162225570412432
ROC train: 0.974333	val: 0.754606	test: 0.741336
PRC train: 0.848422	val: 0.356812	test: 0.367333

Epoch: 91
Loss: 0.11047499433109274
ROC train: 0.973808	val: 0.754620	test: 0.733713
PRC train: 0.844162	val: 0.356150	test: 0.349050

Epoch: 92
Loss: 0.10984701856253984
ROC train: 0.974539	val: 0.756162	test: 0.737453
PRC train: 0.848552	val: 0.357056	test: 0.363639

Epoch: 93
Loss: 0.1070015950928479
ROC train: 0.976289	val: 0.760303	test: 0.740617
PRC train: 0.857073	val: 0.367572	test: 0.363926

Epoch: 94
Loss: 0.10763237920346419
ROC train: 0.919217	val: 0.772405	test: 0.745886
PRC train: 0.629183	val: 0.358647	test: 0.370598

Epoch: 34
Loss: 0.15073847867002732
ROC train: 0.920347	val: 0.767065	test: 0.742221
PRC train: 0.641900	val: 0.367128	test: 0.362325

Epoch: 35
Loss: 0.1511092773126218
ROC train: 0.919685	val: 0.770328	test: 0.746060
PRC train: 0.631581	val: 0.370150	test: 0.367599

Epoch: 36
Loss: 0.15060944828775522
ROC train: 0.925057	val: 0.769080	test: 0.728833
PRC train: 0.646803	val: 0.361447	test: 0.355356

Epoch: 37
Loss: 0.14832010276095464
ROC train: 0.923501	val: 0.776546	test: 0.739952
PRC train: 0.643489	val: 0.372167	test: 0.359801

Epoch: 38
Loss: 0.14840702483249113
ROC train: 0.927157	val: 0.779450	test: 0.740264
PRC train: 0.652708	val: 0.378101	test: 0.366747

Epoch: 39
Loss: 0.1463404270756133
ROC train: 0.929741	val: 0.773242	test: 0.751026
PRC train: 0.667875	val: 0.363092	test: 0.369385

Epoch: 40
Loss: 0.14546353336171378
ROC train: 0.929649	val: 0.767451	test: 0.742064
PRC train: 0.661809	val: 0.355516	test: 0.362686

Epoch: 41
Loss: 0.14505112627937225
ROC train: 0.931172	val: 0.763843	test: 0.726697
PRC train: 0.671220	val: 0.365814	test: 0.352548

Epoch: 42
Loss: 0.14491744565241177
ROC train: 0.928859	val: 0.765847	test: 0.736050
PRC train: 0.659348	val: 0.369124	test: 0.357916

Epoch: 43
Loss: 0.1439801882351912
ROC train: 0.936006	val: 0.767884	test: 0.732300
PRC train: 0.684386	val: 0.364240	test: 0.353706

Epoch: 44
Loss: 0.14421939658711116
ROC train: 0.935529	val: 0.780154	test: 0.740480
PRC train: 0.686964	val: 0.364632	test: 0.351011

Epoch: 45
Loss: 0.14298505932233593
ROC train: 0.937359	val: 0.773023	test: 0.745564
PRC train: 0.695704	val: 0.370854	test: 0.355074

Epoch: 46
Loss: 0.1400435329578071
ROC train: 0.937204	val: 0.779452	test: 0.741920
PRC train: 0.698542	val: 0.379376	test: 0.361201

Epoch: 47
Loss: 0.1383413262913592
ROC train: 0.940225	val: 0.767797	test: 0.742239
PRC train: 0.701391	val: 0.365023	test: 0.356989

Epoch: 48
Loss: 0.13873295919681644
ROC train: 0.941401	val: 0.765809	test: 0.750579
PRC train: 0.703289	val: 0.369619	test: 0.354435

Epoch: 49
Loss: 0.13892851558941502
ROC train: 0.940899	val: 0.754721	test: 0.731896
PRC train: 0.695765	val: 0.370664	test: 0.359746

Epoch: 50
Loss: 0.13784344468739435
ROC train: 0.943401	val: 0.762836	test: 0.732563
PRC train: 0.714324	val: 0.372066	test: 0.350977

Epoch: 51
Loss: 0.13772277969792943
ROC train: 0.944187	val: 0.763806	test: 0.723028
PRC train: 0.712932	val: 0.373645	test: 0.359279

Epoch: 52
Loss: 0.1381008388522008
ROC train: 0.942349	val: 0.765110	test: 0.734077
PRC train: 0.703837	val: 0.362862	test: 0.367681

Epoch: 53
Loss: 0.13696392364709162
ROC train: 0.944035	val: 0.761617	test: 0.738990
PRC train: 0.718062	val: 0.364316	test: 0.366268

Epoch: 54
Loss: 0.1353978557931411
ROC train: 0.947060	val: 0.770073	test: 0.742591
PRC train: 0.730960	val: 0.380435	test: 0.372949

Epoch: 55
Loss: 0.13404875646008396
ROC train: 0.950486	val: 0.760216	test: 0.739857
PRC train: 0.737315	val: 0.361985	test: 0.348043

Epoch: 56
Loss: 0.13507242401801361
ROC train: 0.948643	val: 0.762376	test: 0.739805
PRC train: 0.731317	val: 0.369470	test: 0.362752

Epoch: 57
Loss: 0.13692577916575518
ROC train: 0.949967	val: 0.766280	test: 0.733095
PRC train: 0.739420	val: 0.375470	test: 0.354388

Epoch: 58
Loss: 0.13217923529820458
ROC train: 0.951676	val: 0.770378	test: 0.736933
PRC train: 0.740335	val: 0.364746	test: 0.368998

Epoch: 59
Loss: 0.12944554209525974
ROC train: 0.952511	val: 0.773770	test: 0.736177
PRC train: 0.750268	val: 0.376978	test: 0.359550

Epoch: 60
Loss: 0.13037189992710893
ROC train: 0.951943	val: 0.764711	test: 0.734912
PRC train: 0.744129	val: 0.365621	test: 0.358149

Epoch: 61
Loss: 0.13058414571923982
ROC train: 0.954341	val: 0.764833	test: 0.734233
PRC train: 0.761483	val: 0.358625	test: 0.343801

Epoch: 62
Loss: 0.12956362660599224
ROC train: 0.955505	val: 0.765145	test: 0.736140
PRC train: 0.759569	val: 0.358492	test: 0.350978

Epoch: 63
Loss: 0.12772587783288064
ROC train: 0.956071	val: 0.763769	test: 0.741027
PRC train: 0.762744	val: 0.367038	test: 0.362866

Epoch: 64
Loss: 0.12735260972013557
ROC train: 0.957657	val: 0.772221	test: 0.727368
PRC train: 0.768974	val: 0.376049	test: 0.358619

Epoch: 65
Loss: 0.1264904626493191
ROC train: 0.959384	val: 0.769424	test: 0.727684
PRC train: 0.775038	val: 0.379947	test: 0.358681

Epoch: 66
Loss: 0.127565593187593
ROC train: 0.958749	val: 0.775732	test: 0.740834
PRC train: 0.776715	val: 0.379654	test: 0.353431

Epoch: 67
Loss: 0.12662605681352715
ROC train: 0.959261	val: 0.757461	test: 0.725080
PRC train: 0.778680	val: 0.385357	test: 0.360245

Epoch: 68
Loss: 0.1254245858333286
ROC train: 0.961439	val: 0.770576	test: 0.738949
PRC train: 0.786365	val: 0.374279	test: 0.365069

Epoch: 69
Loss: 0.1241831431315214
ROC train: 0.956458	val: 0.750232	test: 0.727157
PRC train: 0.759099	val: 0.349055	test: 0.348608

Epoch: 70
Loss: 0.123918243992893
ROC train: 0.962367	val: 0.771045	test: 0.738395
PRC train: 0.793179	val: 0.373123	test: 0.371994

Epoch: 71
Loss: 0.12301978464871689
ROC train: 0.963490	val: 0.763652	test: 0.727359
PRC train: 0.791109	val: 0.361559	test: 0.357780

Epoch: 72
Loss: 0.12122670235647519
ROC train: 0.964215	val: 0.762733	test: 0.736907
PRC train: 0.794347	val: 0.369975	test: 0.361176

Epoch: 73
Loss: 0.1217240633399787
ROC train: 0.964185	val: 0.761162	test: 0.738800
PRC train: 0.794967	val: 0.366593	test: 0.357663

Epoch: 74
Loss: 0.12034572436447911
ROC train: 0.965561	val: 0.763385	test: 0.727662
PRC train: 0.796971	val: 0.367503	test: 0.347393

Epoch: 75
Loss: 0.12088616717677947
ROC train: 0.964049	val: 0.768321	test: 0.736069
PRC train: 0.800340	val: 0.367425	test: 0.355839

Epoch: 76
Loss: 0.11965902997057515
ROC train: 0.965652	val: 0.773667	test: 0.739214
PRC train: 0.802319	val: 0.360925	test: 0.368280

Epoch: 77
Loss: 0.11934824121315098
ROC train: 0.964427	val: 0.756658	test: 0.731721
PRC train: 0.794296	val: 0.364358	test: 0.351562

Epoch: 78
Loss: 0.11953191994920073
ROC train: 0.966719	val: 0.764925	test: 0.737955
PRC train: 0.802461	val: 0.377155	test: 0.377072

Epoch: 79
Loss: 0.11687518533999787
ROC train: 0.967483	val: 0.759435	test: 0.731728
PRC train: 0.809907	val: 0.354419	test: 0.359854

Epoch: 80
Loss: 0.1182338433415579
ROC train: 0.967971	val: 0.763207	test: 0.738326
PRC train: 0.811102	val: 0.373936	test: 0.364193

Epoch: 81
Loss: 0.11688084768589983
ROC train: 0.969386	val: 0.753956	test: 0.728267
PRC train: 0.820426	val: 0.361992	test: 0.354502

Epoch: 82
Loss: 0.11422140683534893
ROC train: 0.971582	val: 0.758438	test: 0.734611
PRC train: 0.828495	val: 0.370141	test: 0.357357

Epoch: 83
Loss: 0.11582824431438997
ROC train: 0.971660	val: 0.766927	test: 0.738083
PRC train: 0.831185	val: 0.380371	test: 0.359194

Epoch: 84
Loss: 0.11508323435000792
ROC train: 0.971450	val: 0.755138	test: 0.730732
PRC train: 0.822302	val: 0.374363	test: 0.359220

Epoch: 85
Loss: 0.11318024177109326
ROC train: 0.970508	val: 0.763261	test: 0.728918
PRC train: 0.825977	val: 0.376122	test: 0.349442

Epoch: 86
Loss: 0.11458373529986911
ROC train: 0.972780	val: 0.755142	test: 0.739759
PRC train: 0.832671	val: 0.366425	test: 0.348201

Epoch: 87
Loss: 0.11193836844042668
ROC train: 0.974188	val: 0.755021	test: 0.734864
PRC train: 0.839923	val: 0.354680	test: 0.340874

Epoch: 88
Loss: 0.11221552765788402
ROC train: 0.973852	val: 0.763313	test: 0.744908
PRC train: 0.840081	val: 0.363037	test: 0.348308

Epoch: 89
Loss: 0.11066529037376802
ROC train: 0.973900	val: 0.758820	test: 0.740636
PRC train: 0.838441	val: 0.363817	test: 0.347817

Epoch: 90
Loss: 0.10967634794832853
ROC train: 0.973918	val: 0.767664	test: 0.745083
PRC train: 0.843321	val: 0.389070	test: 0.361569

Epoch: 91
Loss: 0.10982283170126785
ROC train: 0.976271	val: 0.755833	test: 0.730270
PRC train: 0.851289	val: 0.355363	test: 0.348782

Epoch: 92
Loss: 0.11086740802970137
ROC train: 0.974703	val: 0.759786	test: 0.733963
PRC train: 0.842773	val: 0.368095	test: 0.367258

Epoch: 93
Loss: 0.10942091627523375
ROC train: 0.977350	val: 0.755816	test: 0.740130
PRC train: 0.855952	val: 0.372112	test: 0.364516

Epoch: 94
Loss: 0.10849731522243836
ROC train: 0.919514	val: 0.770609	test: 0.742542
PRC train: 0.641014	val: 0.334797	test: 0.359764

Epoch: 34
Loss: 0.1505214462664837
ROC train: 0.922013	val: 0.778147	test: 0.741180
PRC train: 0.645710	val: 0.354406	test: 0.363472

Epoch: 35
Loss: 0.14800405292854676
ROC train: 0.923414	val: 0.773972	test: 0.748992
PRC train: 0.654337	val: 0.352283	test: 0.364871

Epoch: 36
Loss: 0.148232710718261
ROC train: 0.924264	val: 0.773509	test: 0.752200
PRC train: 0.650239	val: 0.355152	test: 0.364110

Epoch: 37
Loss: 0.1504887736288821
ROC train: 0.924143	val: 0.775402	test: 0.743285
PRC train: 0.639837	val: 0.353825	test: 0.366480

Epoch: 38
Loss: 0.14540366258596904
ROC train: 0.928301	val: 0.761538	test: 0.740783
PRC train: 0.656904	val: 0.346898	test: 0.348795

Epoch: 39
Loss: 0.14642488762683892
ROC train: 0.928746	val: 0.771507	test: 0.745809
PRC train: 0.675228	val: 0.362524	test: 0.371786

Epoch: 40
Loss: 0.1454081981296474
ROC train: 0.931864	val: 0.784281	test: 0.745755
PRC train: 0.678755	val: 0.368204	test: 0.374116

Epoch: 41
Loss: 0.14303057017002016
ROC train: 0.933683	val: 0.769424	test: 0.740207
PRC train: 0.679372	val: 0.353713	test: 0.349205

Epoch: 42
Loss: 0.14468622540284046
ROC train: 0.934024	val: 0.772934	test: 0.740781
PRC train: 0.681464	val: 0.353800	test: 0.359948

Epoch: 43
Loss: 0.14124753605208532
ROC train: 0.934475	val: 0.788290	test: 0.749473
PRC train: 0.689895	val: 0.373112	test: 0.375387

Epoch: 44
Loss: 0.1422157949673879
ROC train: 0.936372	val: 0.778530	test: 0.745224
PRC train: 0.696977	val: 0.365123	test: 0.357078

Epoch: 45
Loss: 0.1399038005179395
ROC train: 0.936174	val: 0.774637	test: 0.734121
PRC train: 0.689859	val: 0.345964	test: 0.349271

Epoch: 46
Loss: 0.13966923379975174
ROC train: 0.937370	val: 0.783306	test: 0.756792
PRC train: 0.698444	val: 0.364431	test: 0.371985

Epoch: 47
Loss: 0.1396847376629123
ROC train: 0.939302	val: 0.779497	test: 0.744546
PRC train: 0.701632	val: 0.367189	test: 0.358666

Epoch: 48
Loss: 0.13950617139154745
ROC train: 0.941416	val: 0.782006	test: 0.737917
PRC train: 0.705643	val: 0.377000	test: 0.357665

Epoch: 49
Loss: 0.13823568042244622
ROC train: 0.942875	val: 0.781280	test: 0.741614
PRC train: 0.716891	val: 0.377250	test: 0.374433

Epoch: 50
Loss: 0.13722399772413263
ROC train: 0.942799	val: 0.782137	test: 0.748098
PRC train: 0.717166	val: 0.365669	test: 0.372615

Epoch: 51
Loss: 0.1360131094899488
ROC train: 0.945350	val: 0.782307	test: 0.752163
PRC train: 0.717912	val: 0.368730	test: 0.376347

Epoch: 52
Loss: 0.13653678283142073
ROC train: 0.947439	val: 0.777444	test: 0.746783
PRC train: 0.730885	val: 0.367665	test: 0.360893

Epoch: 53
Loss: 0.13377500371469472
ROC train: 0.947461	val: 0.773907	test: 0.743949
PRC train: 0.730426	val: 0.368651	test: 0.375101

Epoch: 54
Loss: 0.13306477201468214
ROC train: 0.947962	val: 0.786112	test: 0.747340
PRC train: 0.734604	val: 0.372286	test: 0.365233

Epoch: 55
Loss: 0.1341625038286536
ROC train: 0.949644	val: 0.773896	test: 0.751322
PRC train: 0.737583	val: 0.361493	test: 0.375882

Epoch: 56
Loss: 0.13476805158107819
ROC train: 0.949551	val: 0.769763	test: 0.747299
PRC train: 0.736160	val: 0.349214	test: 0.362609

Epoch: 57
Loss: 0.1324077597164424
ROC train: 0.951242	val: 0.771123	test: 0.748186
PRC train: 0.749070	val: 0.366980	test: 0.365017

Epoch: 58
Loss: 0.13184361346054077
ROC train: 0.953719	val: 0.777873	test: 0.748306
PRC train: 0.756516	val: 0.375070	test: 0.369293

Epoch: 59
Loss: 0.13006195643593638
ROC train: 0.953607	val: 0.781956	test: 0.749313
PRC train: 0.760272	val: 0.366857	test: 0.365335

Epoch: 60
Loss: 0.12928737686683747
ROC train: 0.954591	val: 0.784271	test: 0.753987
PRC train: 0.755552	val: 0.381815	test: 0.376802

Epoch: 61
Loss: 0.12900465672838826
ROC train: 0.954960	val: 0.775572	test: 0.743406
PRC train: 0.752692	val: 0.368147	test: 0.363636

Epoch: 62
Loss: 0.13062289378991024
ROC train: 0.956859	val: 0.778337	test: 0.749690
PRC train: 0.766782	val: 0.379496	test: 0.362331

Epoch: 63
Loss: 0.12845999208254394
ROC train: 0.957218	val: 0.781039	test: 0.744653
PRC train: 0.770120	val: 0.369500	test: 0.357380

Epoch: 64
Loss: 0.12765636209505926
ROC train: 0.959879	val: 0.775056	test: 0.737431
PRC train: 0.778307	val: 0.373427	test: 0.353681

Epoch: 65
Loss: 0.12585558341041947
ROC train: 0.960239	val: 0.777836	test: 0.757054
PRC train: 0.782653	val: 0.366015	test: 0.362233

Epoch: 66
Loss: 0.12525124467157756
ROC train: 0.958507	val: 0.769390	test: 0.737488
PRC train: 0.767561	val: 0.351275	test: 0.355675

Epoch: 67
Loss: 0.1243574054980848
ROC train: 0.961167	val: 0.778480	test: 0.741857
PRC train: 0.783158	val: 0.362339	test: 0.359235

Epoch: 68
Loss: 0.12591113434266396
ROC train: 0.961209	val: 0.773317	test: 0.736475
PRC train: 0.785273	val: 0.357739	test: 0.364062

Epoch: 69
Loss: 0.12373359642105967
ROC train: 0.961930	val: 0.772236	test: 0.739165
PRC train: 0.784502	val: 0.372448	test: 0.351787

Epoch: 70
Loss: 0.12404065620254018
ROC train: 0.963889	val: 0.780050	test: 0.745089
PRC train: 0.795447	val: 0.367891	test: 0.361930

Epoch: 71
Loss: 0.12150362910365706
ROC train: 0.962939	val: 0.775631	test: 0.740434
PRC train: 0.795871	val: 0.351948	test: 0.362289

Epoch: 72
Loss: 0.12181957046765322
ROC train: 0.963893	val: 0.779414	test: 0.746864
PRC train: 0.797127	val: 0.381142	test: 0.369975

Epoch: 73
Loss: 0.12195251444875727
ROC train: 0.964765	val: 0.776052	test: 0.745519
PRC train: 0.800324	val: 0.373699	test: 0.371617

Epoch: 74
Loss: 0.11901916058658218
ROC train: 0.967205	val: 0.769100	test: 0.742047
PRC train: 0.808338	val: 0.370890	test: 0.365718

Epoch: 75
Loss: 0.11972525374683576
ROC train: 0.966370	val: 0.787250	test: 0.745997
PRC train: 0.809894	val: 0.370964	test: 0.358893

Epoch: 76
Loss: 0.11996543549996204
ROC train: 0.965833	val: 0.782531	test: 0.732359
PRC train: 0.805576	val: 0.384662	test: 0.355622

Epoch: 77
Loss: 0.11905080588871333
ROC train: 0.967699	val: 0.770557	test: 0.736436
PRC train: 0.810293	val: 0.343864	test: 0.345764

Epoch: 78
Loss: 0.11826075012510444
ROC train: 0.966727	val: 0.772710	test: 0.733338
PRC train: 0.807942	val: 0.359874	test: 0.350663

Epoch: 79
Loss: 0.12050712421787761
ROC train: 0.968446	val: 0.775079	test: 0.737239
PRC train: 0.819686	val: 0.369426	test: 0.372403

Epoch: 80
Loss: 0.11487859992153361
ROC train: 0.970212	val: 0.773283	test: 0.740086
PRC train: 0.823451	val: 0.374235	test: 0.362354

Epoch: 81
Loss: 0.11570611990484064
ROC train: 0.970549	val: 0.771483	test: 0.732622
PRC train: 0.822019	val: 0.361138	test: 0.354593

Epoch: 82
Loss: 0.11544992306467142
ROC train: 0.970541	val: 0.770315	test: 0.740858
PRC train: 0.826627	val: 0.367301	test: 0.366883

Epoch: 83
Loss: 0.11420261399869881
ROC train: 0.971525	val: 0.775470	test: 0.739990
PRC train: 0.828049	val: 0.364063	test: 0.368741

Epoch: 84
Loss: 0.11492477610720402
ROC train: 0.971662	val: 0.772581	test: 0.737629
PRC train: 0.831955	val: 0.362446	test: 0.359903

Epoch: 85
Loss: 0.11341593609648029
ROC train: 0.971630	val: 0.771443	test: 0.742327
PRC train: 0.829345	val: 0.362057	test: 0.359575

Epoch: 86
Loss: 0.11278015085998083
ROC train: 0.972201	val: 0.768240	test: 0.725990
PRC train: 0.837544	val: 0.353188	test: 0.338964

Epoch: 87
Loss: 0.11161475200488338
ROC train: 0.972541	val: 0.769714	test: 0.732516
PRC train: 0.836979	val: 0.368257	test: 0.350060

Epoch: 88
Loss: 0.11201705478887794
ROC train: 0.974732	val: 0.765159	test: 0.730702
PRC train: 0.845164	val: 0.360995	test: 0.355335

Epoch: 89
Loss: 0.11113397766094113
ROC train: 0.975459	val: 0.764957	test: 0.735838
PRC train: 0.848706	val: 0.351576	test: 0.348921

Epoch: 90
Loss: 0.11111396517602666
ROC train: 0.975710	val: 0.775308	test: 0.742509
PRC train: 0.848833	val: 0.360516	test: 0.361278

Epoch: 91
Loss: 0.10994121973332317
ROC train: 0.973477	val: 0.771182	test: 0.736805
PRC train: 0.838035	val: 0.361526	test: 0.359315

Epoch: 92
Loss: 0.10806146015386568
ROC train: 0.976117	val: 0.781236	test: 0.739833
PRC train: 0.852481	val: 0.374203	test: 0.365536

Epoch: 93
Loss: 0.10947590876994631
ROC train: 0.976250	val: 0.765469	test: 0.731243
PRC train: 0.853303	val: 0.363722	test: 0.360153

Epoch: 94
Loss: 0.11032558706588873
ROC train: 0.973413	val: 0.728105	test: 0.695736
PRC train: 0.824896	val: 0.286099	test: 0.264904

Epoch: 95
Loss: 0.10012721758492012
ROC train: 0.971634	val: 0.690822	test: 0.677516
PRC train: 0.811730	val: 0.269688	test: 0.253633

Epoch: 96
Loss: 0.10136604894900618
ROC train: 0.974895	val: 0.721459	test: 0.698221
PRC train: 0.832851	val: 0.279933	test: 0.264779

Epoch: 97
Loss: 0.10038293501437003
ROC train: 0.975050	val: 0.723839	test: 0.704568
PRC train: 0.836691	val: 0.286030	test: 0.271372

Epoch: 98
Loss: 0.09857024479979853
ROC train: 0.976145	val: 0.714835	test: 0.692992
PRC train: 0.841140	val: 0.264520	test: 0.252956

Epoch: 99
Loss: 0.09874282503739741
ROC train: 0.975763	val: 0.717825	test: 0.690014
PRC train: 0.832022	val: 0.282119	test: 0.260284

Epoch: 100
Loss: 0.09663178666142228
ROC train: 0.976470	val: 0.725903	test: 0.694864
PRC train: 0.841527	val: 0.282274	test: 0.263747

Epoch: 101
Loss: 0.09670432335239354
ROC train: 0.976689	val: 0.724614	test: 0.696298
PRC train: 0.842718	val: 0.274867	test: 0.256597

Epoch: 102
Loss: 0.09571710012801449
ROC train: 0.977611	val: 0.722799	test: 0.693294
PRC train: 0.847773	val: 0.280272	test: 0.256857

Epoch: 103
Loss: 0.09505213220122406
ROC train: 0.976700	val: 0.719386	test: 0.697703
PRC train: 0.840328	val: 0.291319	test: 0.261024

Epoch: 104
Loss: 0.09626943680252122
ROC train: 0.974847	val: 0.715435	test: 0.691971
PRC train: 0.836111	val: 0.286414	test: 0.264511

Epoch: 105
Loss: 0.096090805450503
ROC train: 0.977972	val: 0.729907	test: 0.698955
PRC train: 0.847136	val: 0.291893	test: 0.266273

Epoch: 106
Loss: 0.09658940809983456
ROC train: 0.977674	val: 0.727346	test: 0.701736
PRC train: 0.849328	val: 0.284539	test: 0.260202

Epoch: 107
Loss: 0.0939892455745364
ROC train: 0.978416	val: 0.729982	test: 0.694221
PRC train: 0.850672	val: 0.278699	test: 0.254473

Epoch: 108
Loss: 0.09323043756029047
ROC train: 0.979647	val: 0.723469	test: 0.698700
PRC train: 0.861458	val: 0.293894	test: 0.273359

Epoch: 109
Loss: 0.0916192644302496
ROC train: 0.979665	val: 0.724998	test: 0.694798
PRC train: 0.858450	val: 0.283854	test: 0.255825

Epoch: 110
Loss: 0.09096268459387599
ROC train: 0.980746	val: 0.721836	test: 0.698118
PRC train: 0.864340	val: 0.282976	test: 0.274488

Epoch: 111
Loss: 0.09409366460968065
ROC train: 0.980998	val: 0.714012	test: 0.693998
PRC train: 0.865452	val: 0.268952	test: 0.256282

Epoch: 112
Loss: 0.09178279578499023
ROC train: 0.981245	val: 0.726550	test: 0.695280
PRC train: 0.869414	val: 0.278520	test: 0.255358

Epoch: 113
Loss: 0.0921818879737855
ROC train: 0.980946	val: 0.724361	test: 0.697129
PRC train: 0.863737	val: 0.287759	test: 0.260370

Epoch: 114
Loss: 0.09003337177895139
ROC train: 0.981025	val: 0.724704	test: 0.694929
PRC train: 0.865038	val: 0.286635	test: 0.265283

Epoch: 115
Loss: 0.09105411817742444
ROC train: 0.981035	val: 0.725266	test: 0.699891
PRC train: 0.870311	val: 0.286903	test: 0.264430

Epoch: 116
Loss: 0.09107419899840155
ROC train: 0.982534	val: 0.727795	test: 0.698664
PRC train: 0.870884	val: 0.291200	test: 0.262371

Epoch: 117
Loss: 0.09104827712794579
ROC train: 0.982076	val: 0.732012	test: 0.700469
PRC train: 0.873462	val: 0.285702	test: 0.261002

Epoch: 118
Loss: 0.09030074881369501
ROC train: 0.982787	val: 0.725480	test: 0.697854
PRC train: 0.874371	val: 0.285960	test: 0.265619

Epoch: 119
Loss: 0.08818248877601917
ROC train: 0.983990	val: 0.722354	test: 0.699883
PRC train: 0.882986	val: 0.270870	test: 0.251638

Epoch: 120
Loss: 0.08833495929860806
ROC train: 0.983483	val: 0.714111	test: 0.690757
PRC train: 0.881634	val: 0.277119	test: 0.255274

Early stopping
Best (ROC):	 train: 0.932823	val: 0.757246	test: 0.732786
Best (PRC):	 train: 0.673303	val: 0.324571	test: 0.292558

ROC train: 0.975906	val: 0.722702	test: 0.712159
PRC train: 0.841907	val: 0.277826	test: 0.268353

Epoch: 95
Loss: 0.09663018942768586
ROC train: 0.976001	val: 0.728377	test: 0.714647
PRC train: 0.838373	val: 0.287659	test: 0.284543

Epoch: 96
Loss: 0.09589318106721578
ROC train: 0.975158	val: 0.724171	test: 0.707660
PRC train: 0.835139	val: 0.281824	test: 0.261829

Epoch: 97
Loss: 0.09760298975781023
ROC train: 0.977131	val: 0.728183	test: 0.714092
PRC train: 0.848241	val: 0.278186	test: 0.268125

Epoch: 98
Loss: 0.09504921058874131
ROC train: 0.976443	val: 0.727819	test: 0.714532
PRC train: 0.846019	val: 0.280090	test: 0.261167

Epoch: 99
Loss: 0.0956641025661995
ROC train: 0.977897	val: 0.725488	test: 0.712151
PRC train: 0.851569	val: 0.284332	test: 0.268416

Epoch: 100
Loss: 0.09476248835928232
ROC train: 0.978031	val: 0.727981	test: 0.712523
PRC train: 0.852498	val: 0.284051	test: 0.266500

Epoch: 101
Loss: 0.0971116168311831
ROC train: 0.977816	val: 0.719908	test: 0.716677
PRC train: 0.852672	val: 0.283879	test: 0.277640

Epoch: 102
Loss: 0.09477863523970087
ROC train: 0.978640	val: 0.720550	test: 0.711575
PRC train: 0.853729	val: 0.280230	test: 0.275348

Epoch: 103
Loss: 0.09407617398523749
ROC train: 0.979462	val: 0.713602	test: 0.707545
PRC train: 0.859072	val: 0.267048	test: 0.251471

Epoch: 104
Loss: 0.09335685728978098
ROC train: 0.978316	val: 0.723918	test: 0.716971
PRC train: 0.853618	val: 0.274891	test: 0.264215

Epoch: 105
Loss: 0.09261558990568652
ROC train: 0.979737	val: 0.729259	test: 0.713922
PRC train: 0.862146	val: 0.285615	test: 0.277527

Epoch: 106
Loss: 0.09501859629823946
ROC train: 0.980075	val: 0.726543	test: 0.711919
PRC train: 0.863476	val: 0.272878	test: 0.255130

Epoch: 107
Loss: 0.09288920682203929
ROC train: 0.980596	val: 0.723690	test: 0.714695
PRC train: 0.861372	val: 0.272151	test: 0.264546

Epoch: 108
Loss: 0.0903306750770347
ROC train: 0.980657	val: 0.726071	test: 0.709456
PRC train: 0.865114	val: 0.275280	test: 0.260693

Epoch: 109
Loss: 0.09265266485783584
ROC train: 0.982058	val: 0.726715	test: 0.713768
PRC train: 0.871566	val: 0.273494	test: 0.255954

Epoch: 110
Loss: 0.09096165121586237
ROC train: 0.981820	val: 0.729417	test: 0.715784
PRC train: 0.871795	val: 0.275200	test: 0.265152

Epoch: 111
Loss: 0.0897451013863994
ROC train: 0.982335	val: 0.722841	test: 0.712399
PRC train: 0.875339	val: 0.282027	test: 0.269348

Epoch: 112
Loss: 0.09300038951877515
ROC train: 0.982023	val: 0.718449	test: 0.705847
PRC train: 0.872694	val: 0.272862	test: 0.255762

Epoch: 113
Loss: 0.09203011255765739
ROC train: 0.981599	val: 0.716048	test: 0.700481
PRC train: 0.868723	val: 0.245502	test: 0.254127

Epoch: 114
Loss: 0.08915383949898721
ROC train: 0.982639	val: 0.727109	test: 0.708060
PRC train: 0.878083	val: 0.278585	test: 0.269827

Epoch: 115
Loss: 0.08929967247962627
ROC train: 0.983976	val: 0.723645	test: 0.710186
PRC train: 0.884769	val: 0.277727	test: 0.266058

Epoch: 116
Loss: 0.08606112367753334
ROC train: 0.983733	val: 0.724209	test: 0.704210
PRC train: 0.877641	val: 0.288488	test: 0.270666

Epoch: 117
Loss: 0.0871254130543055
ROC train: 0.983050	val: 0.712200	test: 0.705383
PRC train: 0.876861	val: 0.259040	test: 0.249148

Epoch: 118
Loss: 0.08563039782298887
ROC train: 0.984537	val: 0.704780	test: 0.697846
PRC train: 0.885988	val: 0.251867	test: 0.254667

Epoch: 119
Loss: 0.08627915263488886
ROC train: 0.984254	val: 0.721079	test: 0.709665
PRC train: 0.881691	val: 0.270601	test: 0.260328

Epoch: 120
Loss: 0.0872757633686196
ROC train: 0.984587	val: 0.712526	test: 0.703681
PRC train: 0.886179	val: 0.268086	test: 0.259746

Early stopping
Best (ROC):	 train: 0.922220	val: 0.758524	test: 0.736184
Best (PRC):	 train: 0.647250	val: 0.314711	test: 0.297540

ROC train: 0.975978	val: 0.713919	test: 0.703514
PRC train: 0.843108	val: 0.290933	test: 0.278544

Epoch: 95
Loss: 0.09676113208835398
ROC train: 0.976161	val: 0.719027	test: 0.708584
PRC train: 0.842132	val: 0.287536	test: 0.275887

Epoch: 96
Loss: 0.09627102518729834
ROC train: 0.976401	val: 0.719923	test: 0.709013
PRC train: 0.842114	val: 0.283606	test: 0.289345

Epoch: 97
Loss: 0.09745322064343494
ROC train: 0.975993	val: 0.725255	test: 0.707810
PRC train: 0.843765	val: 0.294197	test: 0.282542

Epoch: 98
Loss: 0.09521571200560741
ROC train: 0.977761	val: 0.717093	test: 0.700471
PRC train: 0.850325	val: 0.292842	test: 0.276993

Epoch: 99
Loss: 0.09579509967395682
ROC train: 0.977222	val: 0.722131	test: 0.699018
PRC train: 0.850309	val: 0.290971	test: 0.279801

Epoch: 100
Loss: 0.09549378601633983
ROC train: 0.977371	val: 0.726807	test: 0.711944
PRC train: 0.849664	val: 0.307058	test: 0.278361

Epoch: 101
Loss: 0.095655188187531
ROC train: 0.977470	val: 0.716894	test: 0.701610
PRC train: 0.848919	val: 0.279334	test: 0.269997

Epoch: 102
Loss: 0.09458578926912915
ROC train: 0.978398	val: 0.722773	test: 0.705305
PRC train: 0.855839	val: 0.313947	test: 0.280758

Epoch: 103
Loss: 0.09146473088980997
ROC train: 0.980266	val: 0.718525	test: 0.702559
PRC train: 0.861492	val: 0.296131	test: 0.279441

Epoch: 104
Loss: 0.091842785899908
ROC train: 0.979869	val: 0.719024	test: 0.701418
PRC train: 0.861141	val: 0.285200	test: 0.269437

Epoch: 105
Loss: 0.09425986559916345
ROC train: 0.979485	val: 0.714980	test: 0.695899
PRC train: 0.858919	val: 0.278631	test: 0.265001

Epoch: 106
Loss: 0.0936760340394601
ROC train: 0.980215	val: 0.710718	test: 0.689390
PRC train: 0.867465	val: 0.269927	test: 0.243493

Epoch: 107
Loss: 0.09188232865662006
ROC train: 0.979700	val: 0.719517	test: 0.701158
PRC train: 0.865038	val: 0.283253	test: 0.257668

Epoch: 108
Loss: 0.09294445537135192
ROC train: 0.980702	val: 0.712008	test: 0.697383
PRC train: 0.866138	val: 0.278342	test: 0.259679

Epoch: 109
Loss: 0.09327972035715856
ROC train: 0.980183	val: 0.715895	test: 0.703360
PRC train: 0.863042	val: 0.281073	test: 0.261338

Epoch: 110
Loss: 0.09115377309300417
ROC train: 0.981400	val: 0.708883	test: 0.703154
PRC train: 0.868497	val: 0.269850	test: 0.260727

Epoch: 111
Loss: 0.08888095820922154
ROC train: 0.981982	val: 0.720625	test: 0.710769
PRC train: 0.874028	val: 0.290650	test: 0.274005

Epoch: 112
Loss: 0.08772925930392228
ROC train: 0.981470	val: 0.715617	test: 0.701142
PRC train: 0.871228	val: 0.281459	test: 0.265742

Epoch: 113
Loss: 0.08708940063789665
ROC train: 0.981865	val: 0.714661	test: 0.709641
PRC train: 0.874704	val: 0.275017	test: 0.256806

Epoch: 114
Loss: 0.0875462542577302
ROC train: 0.983079	val: 0.708360	test: 0.699835
PRC train: 0.878681	val: 0.300962	test: 0.264416

Epoch: 115
Loss: 0.0883307594813341
ROC train: 0.982978	val: 0.717332	test: 0.699480
PRC train: 0.879782	val: 0.289739	test: 0.264543

Epoch: 116
Loss: 0.0864874716114725
ROC train: 0.983421	val: 0.710640	test: 0.694509
PRC train: 0.880748	val: 0.296318	test: 0.275941

Epoch: 117
Loss: 0.08808316177587185
ROC train: 0.983277	val: 0.714265	test: 0.702490
PRC train: 0.880290	val: 0.280719	test: 0.261018

Epoch: 118
Loss: 0.08722586399479415
ROC train: 0.983877	val: 0.716512	test: 0.698898
PRC train: 0.883688	val: 0.286353	test: 0.263286

Epoch: 119
Loss: 0.08752041563764265
ROC train: 0.983898	val: 0.711349	test: 0.697757
PRC train: 0.884977	val: 0.277663	test: 0.265950

Epoch: 120
Loss: 0.08360959096125341
ROC train: 0.984372	val: 0.715887	test: 0.699051
PRC train: 0.888569	val: 0.285440	test: 0.271381

Early stopping
Best (ROC):	 train: 0.892788	val: 0.760470	test: 0.730163
Best (PRC):	 train: 0.568793	val: 0.324881	test: 0.293792
All runs completed.

ROC train: 0.977952	val: 0.739310	test: 0.711472
PRC train: 0.857447	val: 0.302775	test: 0.300272

Epoch: 95
Loss: 0.0992768445374045
ROC train: 0.977879	val: 0.737462	test: 0.713916
PRC train: 0.860524	val: 0.310243	test: 0.311373

Epoch: 96
Loss: 0.10055671286589205
ROC train: 0.978533	val: 0.723908	test: 0.709024
PRC train: 0.861452	val: 0.315800	test: 0.319061

Epoch: 97
Loss: 0.1026305493895086
ROC train: 0.979513	val: 0.735059	test: 0.708367
PRC train: 0.864784	val: 0.320949	test: 0.309800

Epoch: 98
Loss: 0.09850422185264027
ROC train: 0.979307	val: 0.727316	test: 0.699487
PRC train: 0.864545	val: 0.289340	test: 0.281099

Epoch: 99
Loss: 0.10018649218899921
ROC train: 0.978486	val: 0.716030	test: 0.698754
PRC train: 0.857795	val: 0.302939	test: 0.289182

Epoch: 100
Loss: 0.10026158072999122
ROC train: 0.979321	val: 0.724667	test: 0.703914
PRC train: 0.862710	val: 0.293465	test: 0.302981

Epoch: 101
Loss: 0.09797087284873272
ROC train: 0.980054	val: 0.727723	test: 0.704224
PRC train: 0.862581	val: 0.313052	test: 0.308052

Epoch: 102
Loss: 0.09766411740309952
ROC train: 0.980390	val: 0.731532	test: 0.710139
PRC train: 0.868937	val: 0.308710	test: 0.294802

Epoch: 103
Loss: 0.09688484326489409
ROC train: 0.981716	val: 0.722227	test: 0.697958
PRC train: 0.875512	val: 0.303740	test: 0.294638

Epoch: 104
Loss: 0.09743923018938844
ROC train: 0.981810	val: 0.724238	test: 0.700834
PRC train: 0.874679	val: 0.303625	test: 0.295721

Epoch: 105
Loss: 0.09790919490902515
ROC train: 0.982016	val: 0.726185	test: 0.704660
PRC train: 0.878585	val: 0.307201	test: 0.301621

Epoch: 106
Loss: 0.09451001146416046
ROC train: 0.983562	val: 0.728977	test: 0.704873
PRC train: 0.885780	val: 0.316432	test: 0.298947

Epoch: 107
Loss: 0.09372484276319938
ROC train: 0.981771	val: 0.727780	test: 0.704732
PRC train: 0.878520	val: 0.329421	test: 0.307207

Epoch: 108
Loss: 0.09221289934664051
ROC train: 0.982505	val: 0.733664	test: 0.712147
PRC train: 0.878849	val: 0.334899	test: 0.309244

Epoch: 109
Loss: 0.09589944492068554
ROC train: 0.983947	val: 0.726164	test: 0.705533
PRC train: 0.887839	val: 0.299209	test: 0.285852

Epoch: 110
Loss: 0.09347360690166245
ROC train: 0.982874	val: 0.731501	test: 0.708943
PRC train: 0.879488	val: 0.298159	test: 0.290929

Epoch: 111
Loss: 0.09551055050641909
ROC train: 0.984270	val: 0.731633	test: 0.716219
PRC train: 0.890641	val: 0.323102	test: 0.311999

Epoch: 112
Loss: 0.09192922942450649
ROC train: 0.985233	val: 0.724158	test: 0.708110
PRC train: 0.895209	val: 0.300549	test: 0.287091

Epoch: 113
Loss: 0.09413392678041897
ROC train: 0.984083	val: 0.723566	test: 0.713708
PRC train: 0.890182	val: 0.302484	test: 0.311788

Epoch: 114
Loss: 0.09406500114302045
ROC train: 0.983354	val: 0.723747	test: 0.702074
PRC train: 0.883042	val: 0.307998	test: 0.306009

Epoch: 115
Loss: 0.09225784237747105
ROC train: 0.985663	val: 0.727089	test: 0.710348
PRC train: 0.896280	val: 0.309385	test: 0.311992

Epoch: 116
Loss: 0.09012534532112752
ROC train: 0.985469	val: 0.726356	test: 0.717030
PRC train: 0.897551	val: 0.301771	test: 0.298334

Epoch: 117
Loss: 0.09152018415978871
ROC train: 0.985250	val: 0.725305	test: 0.713786
PRC train: 0.892104	val: 0.328444	test: 0.322127

Epoch: 118
Loss: 0.09101243700140316
ROC train: 0.986054	val: 0.720760	test: 0.709918
PRC train: 0.900871	val: 0.314232	test: 0.314606

Epoch: 119
Loss: 0.08951296579156684
ROC train: 0.985551	val: 0.716671	test: 0.704830
PRC train: 0.894845	val: 0.302667	test: 0.292737

Epoch: 120
Loss: 0.09018898296775783
ROC train: 0.985264	val: 0.714105	test: 0.702140
PRC train: 0.896715	val: 0.299225	test: 0.312246

Early stopping
Best (ROC):	 train: 0.889077	val: 0.755397	test: 0.740303
Best (PRC):	 train: 0.561539	val: 0.323593	test: 0.335080

ROC train: 0.976928	val: 0.721370	test: 0.724493
PRC train: 0.852267	val: 0.325777	test: 0.322639

Epoch: 95
Loss: 0.10190199863662
ROC train: 0.976082	val: 0.713120	test: 0.706803
PRC train: 0.842103	val: 0.327755	test: 0.315029

Epoch: 96
Loss: 0.10152926865411994
ROC train: 0.978462	val: 0.715620	test: 0.724148
PRC train: 0.858016	val: 0.316585	test: 0.313073

Epoch: 97
Loss: 0.10170145752301575
ROC train: 0.976846	val: 0.713574	test: 0.717357
PRC train: 0.848356	val: 0.323377	test: 0.319570

Epoch: 98
Loss: 0.10131522089413378
ROC train: 0.979097	val: 0.709138	test: 0.713531
PRC train: 0.864285	val: 0.315866	test: 0.311569

Epoch: 99
Loss: 0.10209765048471486
ROC train: 0.979837	val: 0.719729	test: 0.718001
PRC train: 0.865321	val: 0.320437	test: 0.318965

Epoch: 100
Loss: 0.09664246337994488
ROC train: 0.978930	val: 0.715976	test: 0.715874
PRC train: 0.862999	val: 0.327192	test: 0.321068

Epoch: 101
Loss: 0.0985117642313673
ROC train: 0.980663	val: 0.715466	test: 0.710610
PRC train: 0.871759	val: 0.315340	test: 0.312015

Epoch: 102
Loss: 0.09872846587413733
ROC train: 0.980313	val: 0.718291	test: 0.711765
PRC train: 0.865383	val: 0.324123	test: 0.306114

Epoch: 103
Loss: 0.09804651898495936
ROC train: 0.981566	val: 0.707386	test: 0.707783
PRC train: 0.873567	val: 0.308757	test: 0.296303

Epoch: 104
Loss: 0.09639514277904221
ROC train: 0.981664	val: 0.706075	test: 0.705985
PRC train: 0.872770	val: 0.318391	test: 0.306400

Epoch: 105
Loss: 0.09644564829231853
ROC train: 0.980075	val: 0.704195	test: 0.705540
PRC train: 0.866832	val: 0.326023	test: 0.308459

Epoch: 106
Loss: 0.09521904088920084
ROC train: 0.981325	val: 0.710119	test: 0.711142
PRC train: 0.872916	val: 0.327639	test: 0.312326

Epoch: 107
Loss: 0.09414511117271745
ROC train: 0.981224	val: 0.702593	test: 0.714101
PRC train: 0.871267	val: 0.296355	test: 0.306069

Epoch: 108
Loss: 0.09767849612483365
ROC train: 0.980909	val: 0.712308	test: 0.714823
PRC train: 0.875937	val: 0.314416	test: 0.309354

Epoch: 109
Loss: 0.09610709146338602
ROC train: 0.981721	val: 0.711840	test: 0.714623
PRC train: 0.872927	val: 0.317433	test: 0.324560

Epoch: 110
Loss: 0.09486162996257562
ROC train: 0.982184	val: 0.718095	test: 0.715968
PRC train: 0.879403	val: 0.310295	test: 0.312123

Epoch: 111
Loss: 0.09381822249530818
ROC train: 0.981364	val: 0.718862	test: 0.724489
PRC train: 0.874005	val: 0.322861	test: 0.325058

Epoch: 112
Loss: 0.09497212762156106
ROC train: 0.983925	val: 0.702842	test: 0.707741
PRC train: 0.884807	val: 0.301962	test: 0.309174

Epoch: 113
Loss: 0.09339951500196376
ROC train: 0.984084	val: 0.711338	test: 0.718733
PRC train: 0.889827	val: 0.321558	test: 0.314990

Epoch: 114
Loss: 0.09038867891458191
ROC train: 0.983664	val: 0.709409	test: 0.716752
PRC train: 0.884554	val: 0.308382	test: 0.313213

Epoch: 115
Loss: 0.08996953593387765
ROC train: 0.984391	val: 0.707578	test: 0.712142
PRC train: 0.887070	val: 0.314262	test: 0.309421

Epoch: 116
Loss: 0.09073726213799939
ROC train: 0.984507	val: 0.712157	test: 0.714380
PRC train: 0.893391	val: 0.318356	test: 0.314694

Epoch: 117
Loss: 0.09091376908540597
ROC train: 0.985472	val: 0.711242	test: 0.707175
PRC train: 0.896295	val: 0.306447	test: 0.300353

Epoch: 118
Loss: 0.08933035534263059
ROC train: 0.984095	val: 0.709351	test: 0.715312
PRC train: 0.885039	val: 0.315382	test: 0.315844

Epoch: 119
Loss: 0.08989851512696223
ROC train: 0.985978	val: 0.714247	test: 0.712752
PRC train: 0.897875	val: 0.304495	test: 0.304874

Epoch: 120
Loss: 0.0902798502865141
ROC train: 0.985903	val: 0.710599	test: 0.714230
PRC train: 0.897382	val: 0.314345	test: 0.308736

Early stopping
Best (ROC):	 train: 0.899634	val: 0.757722	test: 0.739200
Best (PRC):	 train: 0.592915	val: 0.346981	test: 0.332313

ROC train: 0.977193	val: 0.725981	test: 0.710909
PRC train: 0.857645	val: 0.315557	test: 0.315354

Epoch: 95
Loss: 0.10000456858337486
ROC train: 0.977929	val: 0.728878	test: 0.705138
PRC train: 0.863139	val: 0.316366	test: 0.314300

Epoch: 96
Loss: 0.09909643214010953
ROC train: 0.977137	val: 0.732598	test: 0.716559
PRC train: 0.859479	val: 0.321771	test: 0.315806

Epoch: 97
Loss: 0.09902709612835194
ROC train: 0.977483	val: 0.695161	test: 0.684385
PRC train: 0.863153	val: 0.270203	test: 0.288652

Epoch: 98
Loss: 0.1027557229415661
ROC train: 0.977899	val: 0.722413	test: 0.703833
PRC train: 0.861103	val: 0.313480	test: 0.298629

Epoch: 99
Loss: 0.1006728581203657
ROC train: 0.978791	val: 0.722618	test: 0.700748
PRC train: 0.866887	val: 0.315700	test: 0.310991

Epoch: 100
Loss: 0.09973553609668472
ROC train: 0.978888	val: 0.718805	test: 0.710061
PRC train: 0.866591	val: 0.309952	test: 0.317275

Epoch: 101
Loss: 0.09942200968177882
ROC train: 0.979616	val: 0.717638	test: 0.706563
PRC train: 0.871445	val: 0.326175	test: 0.315659

Epoch: 102
Loss: 0.09840602993169739
ROC train: 0.980059	val: 0.724303	test: 0.709622
PRC train: 0.872754	val: 0.331008	test: 0.317559

Epoch: 103
Loss: 0.09796234937868956
ROC train: 0.980858	val: 0.712655	test: 0.695625
PRC train: 0.873728	val: 0.301167	test: 0.300489

Epoch: 104
Loss: 0.09721748452574502
ROC train: 0.980098	val: 0.717928	test: 0.703898
PRC train: 0.870392	val: 0.324148	test: 0.306196

Epoch: 105
Loss: 0.09687247477426117
ROC train: 0.981046	val: 0.709931	test: 0.703963
PRC train: 0.876420	val: 0.302371	test: 0.315557

Epoch: 106
Loss: 0.0977165045234021
ROC train: 0.980813	val: 0.719299	test: 0.707820
PRC train: 0.873635	val: 0.301836	test: 0.304963

Epoch: 107
Loss: 0.09497754010839064
ROC train: 0.980618	val: 0.714149	test: 0.693768
PRC train: 0.876194	val: 0.301936	test: 0.297107

Epoch: 108
Loss: 0.09396389335008672
ROC train: 0.982016	val: 0.708737	test: 0.699762
PRC train: 0.879436	val: 0.308939	test: 0.311432

Epoch: 109
Loss: 0.09513386867492551
ROC train: 0.981958	val: 0.720862	test: 0.707486
PRC train: 0.882644	val: 0.318924	test: 0.313108

Epoch: 110
Loss: 0.09384884307134277
ROC train: 0.983637	val: 0.712065	test: 0.700202
PRC train: 0.887395	val: 0.300571	test: 0.306042

Epoch: 111
Loss: 0.09304378687164837
ROC train: 0.983237	val: 0.725661	test: 0.711126
PRC train: 0.889097	val: 0.327225	test: 0.324972

Epoch: 112
Loss: 0.09448785060368885
ROC train: 0.984070	val: 0.725220	test: 0.708550
PRC train: 0.893026	val: 0.329380	test: 0.311931

Epoch: 113
Loss: 0.09357218042782182
ROC train: 0.983250	val: 0.714288	test: 0.701377
PRC train: 0.888152	val: 0.305922	test: 0.306998

Epoch: 114
Loss: 0.09381468590353945
ROC train: 0.983932	val: 0.706165	test: 0.706813
PRC train: 0.888046	val: 0.304585	test: 0.299266

Epoch: 115
Loss: 0.09251918158452653
ROC train: 0.984841	val: 0.707567	test: 0.704130
PRC train: 0.896370	val: 0.293756	test: 0.299965

Epoch: 116
Loss: 0.09182124526163812
ROC train: 0.985116	val: 0.714673	test: 0.708405
PRC train: 0.896504	val: 0.304403	test: 0.305129

Epoch: 117
Loss: 0.09181471396089314
ROC train: 0.984234	val: 0.702941	test: 0.701217
PRC train: 0.890117	val: 0.306638	test: 0.304869

Epoch: 118
Loss: 0.08964850382040451
ROC train: 0.985135	val: 0.709059	test: 0.703718
PRC train: 0.899005	val: 0.296575	test: 0.308860

Epoch: 119
Loss: 0.09093037164179765
ROC train: 0.985170	val: 0.714083	test: 0.699749
PRC train: 0.899452	val: 0.298612	test: 0.305535

Epoch: 120
Loss: 0.09072547739497275
ROC train: 0.985893	val: 0.718727	test: 0.706171
PRC train: 0.901097	val: 0.310281	test: 0.311740

Early stopping
Best (ROC):	 train: 0.889253	val: 0.761500	test: 0.745179
Best (PRC):	 train: 0.552088	val: 0.337314	test: 0.318481
All runs completed.

ROC train: 0.977229	val: 0.751748	test: 0.738994
PRC train: 0.861905	val: 0.355852	test: 0.360388

Epoch: 95
Loss: 0.10808783054649228
ROC train: 0.976724	val: 0.758920	test: 0.737747
PRC train: 0.856129	val: 0.363216	test: 0.366352

Epoch: 96
Loss: 0.10685634422960337
ROC train: 0.977487	val: 0.759478	test: 0.742470
PRC train: 0.862506	val: 0.362107	test: 0.367871

Epoch: 97
Loss: 0.10733325314642946
ROC train: 0.978356	val: 0.750082	test: 0.739998
PRC train: 0.864438	val: 0.350707	test: 0.362310

Epoch: 98
Loss: 0.10580881149556756
ROC train: 0.978715	val: 0.760189	test: 0.744654
PRC train: 0.868457	val: 0.359000	test: 0.361620

Epoch: 99
Loss: 0.10454926316734568
ROC train: 0.979406	val: 0.750036	test: 0.738064
PRC train: 0.867776	val: 0.364292	test: 0.362550

Epoch: 100
Loss: 0.10512410693793207
ROC train: 0.977995	val: 0.747056	test: 0.732847
PRC train: 0.863653	val: 0.352783	test: 0.349737

Epoch: 101
Loss: 0.10522517233126762
ROC train: 0.978873	val: 0.745356	test: 0.727466
PRC train: 0.867775	val: 0.335515	test: 0.332054

Epoch: 102
Loss: 0.10422193340241448
ROC train: 0.977926	val: 0.761591	test: 0.743392
PRC train: 0.863669	val: 0.354074	test: 0.357167

Epoch: 103
Loss: 0.10170430557972496
ROC train: 0.979801	val: 0.752766	test: 0.734660
PRC train: 0.871918	val: 0.366515	test: 0.362363

Epoch: 104
Loss: 0.10150569546092329
ROC train: 0.980398	val: 0.755932	test: 0.725373
PRC train: 0.872609	val: 0.362595	test: 0.360675

Epoch: 105
Loss: 0.10158378488254176
ROC train: 0.980146	val: 0.754643	test: 0.733071
PRC train: 0.872744	val: 0.352924	test: 0.355074

Epoch: 106
Loss: 0.10277345046595379
ROC train: 0.980468	val: 0.756566	test: 0.732377
PRC train: 0.874736	val: 0.354017	test: 0.355551

Epoch: 107
Loss: 0.10039233922291473
ROC train: 0.981381	val: 0.749967	test: 0.731990
PRC train: 0.879868	val: 0.346017	test: 0.355274

Epoch: 108
Loss: 0.10025601139114558
ROC train: 0.980983	val: 0.752473	test: 0.736301
PRC train: 0.879854	val: 0.344987	test: 0.349877

Epoch: 109
Loss: 0.09977784137868972
ROC train: 0.982633	val: 0.749840	test: 0.734370
PRC train: 0.888569	val: 0.354843	test: 0.347026

Epoch: 110
Loss: 0.10049343665821686
ROC train: 0.981693	val: 0.747526	test: 0.729965
PRC train: 0.884731	val: 0.353646	test: 0.358541

Epoch: 111
Loss: 0.1002208690375393
ROC train: 0.982048	val: 0.755823	test: 0.733928
PRC train: 0.883678	val: 0.362969	test: 0.361341

Epoch: 112
Loss: 0.10031792314481379
ROC train: 0.981625	val: 0.761534	test: 0.727968
PRC train: 0.878922	val: 0.363065	test: 0.362328

Epoch: 113
Loss: 0.09675143275625293
ROC train: 0.983699	val: 0.757558	test: 0.727688
PRC train: 0.892002	val: 0.355965	test: 0.357762

Epoch: 114
Loss: 0.09569581868486539
ROC train: 0.982710	val: 0.753446	test: 0.734002
PRC train: 0.890960	val: 0.343597	test: 0.344838

Epoch: 115
Loss: 0.09714248196047778
ROC train: 0.984786	val: 0.745275	test: 0.729414
PRC train: 0.896486	val: 0.340949	test: 0.343442

Epoch: 116
Loss: 0.09590307328119757
ROC train: 0.983311	val: 0.750491	test: 0.741473
PRC train: 0.889114	val: 0.352758	test: 0.351022

Epoch: 117
Loss: 0.09520981290903574
ROC train: 0.984419	val: 0.753180	test: 0.741871
PRC train: 0.896954	val: 0.351109	test: 0.359930

Epoch: 118
Loss: 0.09528591200825667
ROC train: 0.985186	val: 0.762696	test: 0.736742
PRC train: 0.900795	val: 0.368908	test: 0.358186

Epoch: 119
Loss: 0.093634642006719
ROC train: 0.983981	val: 0.763149	test: 0.739783
PRC train: 0.897123	val: 0.350892	test: 0.359542

Epoch: 120
Loss: 0.09329023365529851
ROC train: 0.985563	val: 0.747809	test: 0.723965
PRC train: 0.902970	val: 0.345867	test: 0.332764

Early stopping
Best (ROC):	 train: 0.892360	val: 0.784488	test: 0.728978
Best (PRC):	 train: 0.562903	val: 0.353971	test: 0.369580

ROC train: 0.977916	val: 0.755638	test: 0.737190
PRC train: 0.857092	val: 0.360356	test: 0.357765

Epoch: 95
Loss: 0.10621637730201172
ROC train: 0.978188	val: 0.752415	test: 0.740480
PRC train: 0.863210	val: 0.374188	test: 0.363641

Epoch: 96
Loss: 0.10732365319453685
ROC train: 0.976655	val: 0.734891	test: 0.729719
PRC train: 0.851848	val: 0.363035	test: 0.356797

Epoch: 97
Loss: 0.10778465657466649
ROC train: 0.978902	val: 0.755569	test: 0.733713
PRC train: 0.865275	val: 0.381380	test: 0.357043

Epoch: 98
Loss: 0.10786964270208285
ROC train: 0.976551	val: 0.742481	test: 0.724770
PRC train: 0.849843	val: 0.351918	test: 0.340779

Epoch: 99
Loss: 0.10453958065838448
ROC train: 0.979399	val: 0.755295	test: 0.741301
PRC train: 0.864958	val: 0.369725	test: 0.363535

Epoch: 100
Loss: 0.10620598796739848
ROC train: 0.979566	val: 0.750259	test: 0.734804
PRC train: 0.868432	val: 0.366813	test: 0.359337

Epoch: 101
Loss: 0.10425275372111843
ROC train: 0.980093	val: 0.761158	test: 0.733875
PRC train: 0.869866	val: 0.386704	test: 0.360254

Epoch: 102
Loss: 0.10278773645150388
ROC train: 0.980420	val: 0.765265	test: 0.730008
PRC train: 0.866667	val: 0.381944	test: 0.349900

Epoch: 103
Loss: 0.1050907074889305
ROC train: 0.979318	val: 0.758852	test: 0.743883
PRC train: 0.869505	val: 0.376619	test: 0.356953

Epoch: 104
Loss: 0.10317311287537467
ROC train: 0.980612	val: 0.758899	test: 0.733026
PRC train: 0.872154	val: 0.365414	test: 0.346142

Epoch: 105
Loss: 0.10305842447565683
ROC train: 0.981246	val: 0.760420	test: 0.733954
PRC train: 0.875765	val: 0.371775	test: 0.344029

Epoch: 106
Loss: 0.10091879546541158
ROC train: 0.982391	val: 0.769121	test: 0.726462
PRC train: 0.882460	val: 0.381007	test: 0.346006

Epoch: 107
Loss: 0.10134550803969747
ROC train: 0.983144	val: 0.758270	test: 0.739353
PRC train: 0.883081	val: 0.386491	test: 0.363432

Epoch: 108
Loss: 0.10156486442783227
ROC train: 0.982826	val: 0.757307	test: 0.731337
PRC train: 0.886977	val: 0.378052	test: 0.352049

Epoch: 109
Loss: 0.10183668754268634
ROC train: 0.983101	val: 0.758604	test: 0.733225
PRC train: 0.886806	val: 0.371615	test: 0.348331

Epoch: 110
Loss: 0.09892210840751785
ROC train: 0.983067	val: 0.761061	test: 0.738778
PRC train: 0.883286	val: 0.374301	test: 0.352993

Epoch: 111
Loss: 0.10018783666637528
ROC train: 0.982818	val: 0.746726	test: 0.724746
PRC train: 0.881654	val: 0.365643	test: 0.333859

Epoch: 112
Loss: 0.09947844691571216
ROC train: 0.983029	val: 0.758746	test: 0.725618
PRC train: 0.887209	val: 0.374003	test: 0.328213

Epoch: 113
Loss: 0.09920325625525457
ROC train: 0.983871	val: 0.758672	test: 0.732806
PRC train: 0.886880	val: 0.361962	test: 0.345924

Epoch: 114
Loss: 0.09799964163501931
ROC train: 0.983959	val: 0.763969	test: 0.730988
PRC train: 0.890663	val: 0.374533	test: 0.338730

Epoch: 115
Loss: 0.096746118923838
ROC train: 0.985690	val: 0.749792	test: 0.726745
PRC train: 0.898848	val: 0.362585	test: 0.344937

Epoch: 116
Loss: 0.09653538219134136
ROC train: 0.985295	val: 0.758278	test: 0.731894
PRC train: 0.899005	val: 0.371165	test: 0.357116

Epoch: 117
Loss: 0.09241564135986363
ROC train: 0.986082	val: 0.760267	test: 0.726240
PRC train: 0.904598	val: 0.376225	test: 0.345598

Epoch: 118
Loss: 0.09506097693820342
ROC train: 0.986159	val: 0.748503	test: 0.726017
PRC train: 0.901545	val: 0.367197	test: 0.337677

Epoch: 119
Loss: 0.09587017574735805
ROC train: 0.984945	val: 0.763815	test: 0.733191
PRC train: 0.899390	val: 0.384133	test: 0.346865

Epoch: 120
Loss: 0.09501113475673764
ROC train: 0.985220	val: 0.758677	test: 0.735534
PRC train: 0.898787	val: 0.377088	test: 0.356436

Early stopping
Best (ROC):	 train: 0.935529	val: 0.780154	test: 0.740480
Best (PRC):	 train: 0.686964	val: 0.364632	test: 0.351011

ROC train: 0.977323	val: 0.768808	test: 0.732684
PRC train: 0.857683	val: 0.365968	test: 0.355609

Epoch: 95
Loss: 0.10910416854538657
ROC train: 0.977993	val: 0.773125	test: 0.734946
PRC train: 0.862091	val: 0.365019	test: 0.352421

Epoch: 96
Loss: 0.10661896783754476
ROC train: 0.977897	val: 0.768054	test: 0.736276
PRC train: 0.862703	val: 0.359413	test: 0.347096

Epoch: 97
Loss: 0.10714129060480335
ROC train: 0.978115	val: 0.765047	test: 0.738441
PRC train: 0.861991	val: 0.366632	test: 0.342560

Epoch: 98
Loss: 0.10686847817524599
ROC train: 0.977883	val: 0.763843	test: 0.747374
PRC train: 0.857581	val: 0.358815	test: 0.364943

Epoch: 99
Loss: 0.10712261857359348
ROC train: 0.977592	val: 0.762478	test: 0.724184
PRC train: 0.854724	val: 0.353564	test: 0.350914

Epoch: 100
Loss: 0.10610175161182303
ROC train: 0.978218	val: 0.768738	test: 0.737830
PRC train: 0.868090	val: 0.351155	test: 0.333090

Epoch: 101
Loss: 0.10478091491610746
ROC train: 0.979085	val: 0.774979	test: 0.733411
PRC train: 0.868800	val: 0.374838	test: 0.344918

Epoch: 102
Loss: 0.10573891837477056
ROC train: 0.980397	val: 0.768948	test: 0.730982
PRC train: 0.873292	val: 0.365703	test: 0.344409

Epoch: 103
Loss: 0.1043326257163544
ROC train: 0.980330	val: 0.769717	test: 0.737340
PRC train: 0.874816	val: 0.363618	test: 0.350396

Epoch: 104
Loss: 0.10492555848158469
ROC train: 0.981472	val: 0.771002	test: 0.729963
PRC train: 0.878079	val: 0.372995	test: 0.350490

Epoch: 105
Loss: 0.10305084936366508
ROC train: 0.981098	val: 0.768293	test: 0.729447
PRC train: 0.878145	val: 0.360226	test: 0.343534

Epoch: 106
Loss: 0.1023506685787342
ROC train: 0.982307	val: 0.771885	test: 0.743772
PRC train: 0.882692	val: 0.357109	test: 0.350155

Epoch: 107
Loss: 0.10203019471158341
ROC train: 0.981884	val: 0.764244	test: 0.733538
PRC train: 0.882841	val: 0.359348	test: 0.346828

Epoch: 108
Loss: 0.10227982019393726
ROC train: 0.982244	val: 0.765190	test: 0.737306
PRC train: 0.883498	val: 0.346590	test: 0.339664

Epoch: 109
Loss: 0.0999538221695894
ROC train: 0.981726	val: 0.770490	test: 0.740909
PRC train: 0.880509	val: 0.375765	test: 0.362760

Epoch: 110
Loss: 0.09867066037891276
ROC train: 0.983450	val: 0.761786	test: 0.736643
PRC train: 0.888062	val: 0.362884	test: 0.354424

Epoch: 111
Loss: 0.10022598898583217
ROC train: 0.983408	val: 0.764804	test: 0.746858
PRC train: 0.887292	val: 0.357201	test: 0.348586

Epoch: 112
Loss: 0.09894508915495695
ROC train: 0.982028	val: 0.757571	test: 0.736747
PRC train: 0.882445	val: 0.321454	test: 0.315589

Epoch: 113
Loss: 0.09879575006907387
ROC train: 0.983779	val: 0.766132	test: 0.731647
PRC train: 0.890022	val: 0.371286	test: 0.359371

Epoch: 114
Loss: 0.09759307199175792
ROC train: 0.983524	val: 0.758910	test: 0.739043
PRC train: 0.892673	val: 0.358361	test: 0.350925

Epoch: 115
Loss: 0.09791052436889518
ROC train: 0.982663	val: 0.762200	test: 0.729872
PRC train: 0.882270	val: 0.367377	test: 0.346579

Epoch: 116
Loss: 0.09561119191826593
ROC train: 0.983590	val: 0.764237	test: 0.735678
PRC train: 0.890870	val: 0.371651	test: 0.346613

Epoch: 117
Loss: 0.09616079817874308
ROC train: 0.984924	val: 0.768433	test: 0.729313
PRC train: 0.896633	val: 0.379931	test: 0.335923

Epoch: 118
Loss: 0.09787478584138574
ROC train: 0.985598	val: 0.763349	test: 0.733770
PRC train: 0.899104	val: 0.366478	test: 0.340078

Epoch: 119
Loss: 0.09497776052633246
ROC train: 0.984466	val: 0.763142	test: 0.737500
PRC train: 0.899800	val: 0.357651	test: 0.340695

Epoch: 120
Loss: 0.09652787782562088
ROC train: 0.986188	val: 0.769466	test: 0.735202
PRC train: 0.904275	val: 0.379258	test: 0.345434

Early stopping
Best (ROC):	 train: 0.934475	val: 0.788290	test: 0.749473
Best (PRC):	 train: 0.689895	val: 0.373112	test: 0.375387
All runs completed.
