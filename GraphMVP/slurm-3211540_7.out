>>> Starting run for dataset: lipo
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running RANDOM configs_split_experiments/GraphMVP/lipo/random/train_prop=0.6.yml on cuda:0
Running RANDOM configs_split_experiments/GraphMVP/lipo/random/train_prop=0.7.yml on cuda:1
Running RANDOM configs_split_experiments/GraphMVP/lipo/random/train_prop=0.8.yml on cuda:2
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/lipo/random/train_prop=0.7.yml --runseed 4 --device cuda:1
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/lipo/random/train_prop=0.8.yml --runseed 4 --device cuda:2
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/lipo/random/train_prop=0.7.yml --runseed 5 --device cuda:1
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/lipo/random/train_prop=0.8.yml --runseed 5 --device cuda:2
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/lipo/random/train_prop=0.7.yml --runseed 6 --device cuda:1
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/lipo/random/train_prop=0.8.yml --runseed 6 --device cuda:2
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/lipo/random/train_prop=0.6.yml --runseed 4 --device cuda:0
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/lipo/random/train_prop=0.6.yml --runseed 5 --device cuda:0
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/lipo/random/train_prop=0.6.yml --runseed 6 --device cuda:0
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/lipo/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/lipo/random/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/lipo/random/train_prop=0.6/lipophilicity_random_6_26-05_11-07-47  ]
[ Using Seed :  6  ]
[ Using device :  cuda:0  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.319939613342285
RMSE train: 2.072084	val: 2.113291	test: 2.103390
MAE train: 1.824221	val: 1.858424	test: 1.847407

Epoch: 2
Loss: 3.878569149971008
RMSE train: 1.848612	val: 1.882141	test: 1.867682
MAE train: 1.614755	val: 1.638778	test: 1.627872

Epoch: 3
Loss: 2.7852108001708986
RMSE train: 1.521455	val: 1.563682	test: 1.546250
MAE train: 1.307348	val: 1.333452	test: 1.331445

Epoch: 4
Loss: 1.8857633471488953
RMSE train: 1.332246	val: 1.367952	test: 1.376360
MAE train: 1.126050	val: 1.149016	test: 1.170684

Epoch: 5
Loss: 1.2664450049400329
RMSE train: 1.063450	val: 1.098929	test: 1.137629
MAE train: 0.872560	val: 0.893662	test: 0.937343

Epoch: 6
Loss: 0.9247169435024262
RMSE train: 0.874339	val: 0.924268	test: 0.966950
MAE train: 0.704325	val: 0.735524	test: 0.785457

Epoch: 7
Loss: 0.814453536272049
RMSE train: 0.812587	val: 0.857573	test: 0.914199
MAE train: 0.646656	val: 0.670862	test: 0.730963

Epoch: 8
Loss: 0.7518867552280426
RMSE train: 0.819851	val: 0.853483	test: 0.918097
MAE train: 0.650296	val: 0.664289	test: 0.728956

Epoch: 9
Loss: 0.6961483955383301
RMSE train: 0.747117	val: 0.798155	test: 0.862612
MAE train: 0.591012	val: 0.608475	test: 0.683622

Epoch: 10
Loss: 0.6814880192279815
RMSE train: 0.748127	val: 0.810634	test: 0.862043
MAE train: 0.599880	val: 0.631720	test: 0.686071

Epoch: 11
Loss: 0.6772939920425415
RMSE train: 0.736295	val: 0.778351	test: 0.858906
MAE train: 0.575597	val: 0.593643	test: 0.668775

Epoch: 12
Loss: 0.6418259799480438
RMSE train: 0.757848	val: 0.824365	test: 0.859421
MAE train: 0.604125	val: 0.642806	test: 0.691018

Epoch: 13
Loss: 0.6155328929424286
RMSE train: 0.691510	val: 0.750189	test: 0.811524
MAE train: 0.544136	val: 0.567851	test: 0.634070

Epoch: 14
Loss: 0.5912221729755401
RMSE train: 0.707403	val: 0.776086	test: 0.826177
MAE train: 0.558508	val: 0.592986	test: 0.648412

Epoch: 15
Loss: 0.582870626449585
RMSE train: 0.673472	val: 0.754758	test: 0.797488
MAE train: 0.534055	val: 0.573384	test: 0.626332

Epoch: 16
Loss: 0.5663432538509369
RMSE train: 0.699197	val: 0.754572	test: 0.821029
MAE train: 0.542517	val: 0.571972	test: 0.627775

Epoch: 17
Loss: 0.563948518037796
RMSE train: 0.676267	val: 0.745167	test: 0.788140
MAE train: 0.533716	val: 0.561999	test: 0.614620

Epoch: 18
Loss: 0.5442073494195938
RMSE train: 0.661610	val: 0.742034	test: 0.784728
MAE train: 0.523545	val: 0.554983	test: 0.606510

Epoch: 19
Loss: 0.5347032815217971
RMSE train: 0.629136	val: 0.715603	test: 0.760816
MAE train: 0.498438	val: 0.535906	test: 0.590298

Epoch: 20
Loss: 0.5155048906803131
RMSE train: 0.644404	val: 0.733753	test: 0.768188
MAE train: 0.509523	val: 0.553474	test: 0.592197

Epoch: 21
Loss: 0.5140206485986709
RMSE train: 0.629479	val: 0.730664	test: 0.765636
MAE train: 0.498448	val: 0.544628	test: 0.588434

Epoch: 22
Loss: 0.5153404951095581
RMSE train: 0.623959	val: 0.716872	test: 0.758065
MAE train: 0.493748	val: 0.545219	test: 0.590144Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/lipo/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/lipo/random/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/lipo/random/train_prop=0.6/lipophilicity_random_4_26-05_11-07-47  ]
[ Using Seed :  4  ]
[ Using device :  cuda:0  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.501951408386231
RMSE train: 2.220134	val: 2.273528	test: 2.250372
MAE train: 1.974489	val: 2.028014	test: 1.995414

Epoch: 2
Loss: 4.0592018365859985
RMSE train: 1.955368	val: 2.007188	test: 1.976535
MAE train: 1.722136	val: 1.766507	test: 1.734318

Epoch: 3
Loss: 2.8714775323867796
RMSE train: 1.451270	val: 1.502413	test: 1.491266
MAE train: 1.239513	val: 1.274562	test: 1.272603

Epoch: 4
Loss: 2.0048617839813234
RMSE train: 1.123088	val: 1.182246	test: 1.186606
MAE train: 0.935390	val: 0.984695	test: 0.993231

Epoch: 5
Loss: 1.3376193046569824
RMSE train: 0.950633	val: 1.008549	test: 1.040152
MAE train: 0.779373	val: 0.820751	test: 0.859018

Epoch: 6
Loss: 0.9815578937530518
RMSE train: 0.789009	val: 0.840649	test: 0.895617
MAE train: 0.629576	val: 0.663682	test: 0.719997

Epoch: 7
Loss: 0.8101252973079681
RMSE train: 0.751718	val: 0.809621	test: 0.858584
MAE train: 0.597962	val: 0.633193	test: 0.688551

Epoch: 8
Loss: 0.7241498708724976
RMSE train: 0.749889	val: 0.804802	test: 0.872397
MAE train: 0.595455	val: 0.628591	test: 0.688062

Epoch: 9
Loss: 0.6804234206676483
RMSE train: 0.730616	val: 0.797398	test: 0.848438
MAE train: 0.578469	val: 0.615794	test: 0.669919

Epoch: 10
Loss: 0.6678184926509857
RMSE train: 0.721615	val: 0.767718	test: 0.842184
MAE train: 0.564570	val: 0.585436	test: 0.653786

Epoch: 11
Loss: 0.6537594795227051
RMSE train: 0.706918	val: 0.762485	test: 0.820786
MAE train: 0.556692	val: 0.580045	test: 0.639309

Epoch: 12
Loss: 0.6439669311046601
RMSE train: 0.714317	val: 0.769246	test: 0.844934
MAE train: 0.560946	val: 0.588582	test: 0.657471

Epoch: 13
Loss: 0.5952037632465362
RMSE train: 0.691841	val: 0.751078	test: 0.802863
MAE train: 0.544965	val: 0.581263	test: 0.634512

Epoch: 14
Loss: 0.609528711438179
RMSE train: 0.669374	val: 0.726739	test: 0.795999
MAE train: 0.524284	val: 0.546600	test: 0.625012

Epoch: 15
Loss: 0.5921704649925232
RMSE train: 0.652749	val: 0.729061	test: 0.775358
MAE train: 0.514744	val: 0.548983	test: 0.607876

Epoch: 16
Loss: 0.5614533066749573
RMSE train: 0.669451	val: 0.740232	test: 0.800107
MAE train: 0.526393	val: 0.556475	test: 0.619096

Epoch: 17
Loss: 0.5785419702529907
RMSE train: 0.651417	val: 0.727055	test: 0.775068
MAE train: 0.509725	val: 0.545578	test: 0.603994

Epoch: 18
Loss: 0.5765211284160614
RMSE train: 0.631214	val: 0.708246	test: 0.767553
MAE train: 0.493807	val: 0.530426	test: 0.595466

Epoch: 19
Loss: 0.5303315341472625
RMSE train: 0.627252	val: 0.717914	test: 0.775009
MAE train: 0.494634	val: 0.540704	test: 0.602015

Epoch: 20
Loss: 0.5160915046930313
RMSE train: 0.661141	val: 0.730121	test: 0.795629
MAE train: 0.515567	val: 0.547907	test: 0.613007

Epoch: 21
Loss: 0.5191047459840774
RMSE train: 0.626116	val: 0.708267	test: 0.766617
MAE train: 0.487860	val: 0.526247	test: 0.591392

Epoch: 22
Loss: 0.48830479979515073
RMSE train: 0.634373	val: 0.705499	test: 0.770399
MAE train: 0.493140	val: 0.525432	test: 0.589644Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/lipo/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/lipo/random/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/lipo/random/train_prop=0.6/lipophilicity_random_5_26-05_11-07-47  ]
[ Using Seed :  5  ]
[ Using device :  cuda:0  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.015224838256836
RMSE train: 2.092102	val: 2.133360	test: 2.126289
MAE train: 1.847951	val: 1.882364	test: 1.878268

Epoch: 2
Loss: 3.612890195846558
RMSE train: 1.728457	val: 1.771592	test: 1.757508
MAE train: 1.496737	val: 1.527740	test: 1.517143

Epoch: 3
Loss: 2.550132918357849
RMSE train: 1.413037	val: 1.466307	test: 1.454538
MAE train: 1.203231	val: 1.239435	test: 1.241768

Epoch: 4
Loss: 1.7390791535377503
RMSE train: 1.118832	val: 1.156555	test: 1.185915
MAE train: 0.928370	val: 0.960078	test: 0.993113

Epoch: 5
Loss: 1.1952608823776245
RMSE train: 0.950110	val: 0.986142	test: 1.045689
MAE train: 0.773415	val: 0.801704	test: 0.856335

Epoch: 6
Loss: 0.9134734809398651
RMSE train: 0.826433	val: 0.855803	test: 0.943161
MAE train: 0.657743	val: 0.675907	test: 0.754304

Epoch: 7
Loss: 0.7653558254241943
RMSE train: 0.757783	val: 0.796924	test: 0.885318
MAE train: 0.600877	val: 0.609254	test: 0.696276

Epoch: 8
Loss: 0.7275835812091828
RMSE train: 0.750581	val: 0.803155	test: 0.885885
MAE train: 0.588965	val: 0.612603	test: 0.699107

Epoch: 9
Loss: 0.696512120962143
RMSE train: 0.746503	val: 0.797260	test: 0.876513
MAE train: 0.587470	val: 0.614147	test: 0.683331

Epoch: 10
Loss: 0.6567651510238648
RMSE train: 0.727923	val: 0.771176	test: 0.848501
MAE train: 0.569728	val: 0.585462	test: 0.663118

Epoch: 11
Loss: 0.6538303017616272
RMSE train: 0.702726	val: 0.758241	test: 0.829871
MAE train: 0.548673	val: 0.571427	test: 0.651678

Epoch: 12
Loss: 0.6363190233707428
RMSE train: 0.700221	val: 0.753560	test: 0.833747
MAE train: 0.548031	val: 0.567958	test: 0.649731

Epoch: 13
Loss: 0.6222374558448791
RMSE train: 0.684907	val: 0.746502	test: 0.809545
MAE train: 0.542098	val: 0.564952	test: 0.634641

Epoch: 14
Loss: 0.6157442390918731
RMSE train: 0.677152	val: 0.742177	test: 0.809870
MAE train: 0.532142	val: 0.566825	test: 0.639400

Epoch: 15
Loss: 0.5515558809041977
RMSE train: 0.681963	val: 0.750240	test: 0.809471
MAE train: 0.538776	val: 0.574074	test: 0.635279

Epoch: 16
Loss: 0.5748121917247773
RMSE train: 0.672954	val: 0.729594	test: 0.812201
MAE train: 0.524059	val: 0.554953	test: 0.624761

Epoch: 17
Loss: 0.5590060174465179
RMSE train: 0.653322	val: 0.714333	test: 0.783142
MAE train: 0.512147	val: 0.537081	test: 0.610127

Epoch: 18
Loss: 0.5577506244182586
RMSE train: 0.648101	val: 0.741983	test: 0.795736
MAE train: 0.513144	val: 0.568333	test: 0.629537

Epoch: 19
Loss: 0.5302765041589736
RMSE train: 0.642568	val: 0.722012	test: 0.779900
MAE train: 0.500000	val: 0.542802	test: 0.605526

Epoch: 20
Loss: 0.547319957613945
RMSE train: 0.643535	val: 0.723896	test: 0.790878
MAE train: 0.501191	val: 0.547228	test: 0.615200

Epoch: 21
Loss: 0.5421135067939759
RMSE train: 0.627098	val: 0.710095	test: 0.763378
MAE train: 0.493973	val: 0.534285	test: 0.598086

Epoch: 22
Loss: 0.5123458057641983
RMSE train: 0.619089	val: 0.701616	test: 0.760987
MAE train: 0.483202	val: 0.526998	test: 0.588215Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/lipo/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/lipo/random/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/lipo/random/train_prop=0.7/lipophilicity_random_4_26-05_11-07-47  ]
[ Using Seed :  4  ]
[ Using device :  cuda:1  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.360025962193807
RMSE train: 2.191123	val: 2.288604	test: 2.223646
MAE train: 1.944592	val: 2.030619	test: 1.969285

Epoch: 2
Loss: 3.7206563353538513
RMSE train: 1.788674	val: 1.866011	test: 1.807171
MAE train: 1.559458	val: 1.625727	test: 1.577944

Epoch: 3
Loss: 2.452570855617523
RMSE train: 1.231711	val: 1.312195	test: 1.284994
MAE train: 1.032525	val: 1.097092	test: 1.083027

Epoch: 4
Loss: 1.59170859058698
RMSE train: 1.019467	val: 1.096815	test: 1.101119
MAE train: 0.842988	val: 0.910367	test: 0.922254

Epoch: 5
Loss: 1.0450699577728908
RMSE train: 0.826802	val: 0.897464	test: 0.902492
MAE train: 0.662268	val: 0.720658	test: 0.728160

Epoch: 6
Loss: 0.824909453590711
RMSE train: 0.783460	val: 0.864478	test: 0.873005
MAE train: 0.628215	val: 0.692781	test: 0.703416

Epoch: 7
Loss: 0.7485810170571009
RMSE train: 0.754045	val: 0.843172	test: 0.851389
MAE train: 0.599822	val: 0.666877	test: 0.680207

Epoch: 8
Loss: 0.7176923503478368
RMSE train: 0.728050	val: 0.813913	test: 0.818406
MAE train: 0.569981	val: 0.629654	test: 0.648056

Epoch: 9
Loss: 0.6714018881320953
RMSE train: 0.721807	val: 0.803447	test: 0.819477
MAE train: 0.562436	val: 0.622854	test: 0.639152

Epoch: 10
Loss: 0.6366587032874426
RMSE train: 0.703841	val: 0.798477	test: 0.791393
MAE train: 0.547143	val: 0.610588	test: 0.625205

Epoch: 11
Loss: 0.6217962106068929
RMSE train: 0.715751	val: 0.792256	test: 0.797063
MAE train: 0.559754	val: 0.609667	test: 0.627704

Epoch: 12
Loss: 0.6188044647375742
RMSE train: 0.687436	val: 0.785261	test: 0.785591
MAE train: 0.539089	val: 0.605751	test: 0.615508

Epoch: 13
Loss: 0.5818897783756256
RMSE train: 0.678680	val: 0.784060	test: 0.780856
MAE train: 0.528304	val: 0.602529	test: 0.609660

Epoch: 14
Loss: 0.5822226156791052
RMSE train: 0.680759	val: 0.787868	test: 0.782521
MAE train: 0.534159	val: 0.609220	test: 0.610770

Epoch: 15
Loss: 0.5676675289869308
RMSE train: 0.652278	val: 0.755375	test: 0.747883
MAE train: 0.508187	val: 0.572592	test: 0.583559

Epoch: 16
Loss: 0.5641026049852371
RMSE train: 0.670716	val: 0.769469	test: 0.769750
MAE train: 0.522178	val: 0.589583	test: 0.602234

Epoch: 17
Loss: 0.5645554388562838
RMSE train: 0.639145	val: 0.749689	test: 0.752207
MAE train: 0.498573	val: 0.567446	test: 0.586163

Epoch: 18
Loss: 0.5404668052991232
RMSE train: 0.646293	val: 0.752661	test: 0.765731
MAE train: 0.502258	val: 0.570575	test: 0.593724

Epoch: 19
Loss: 0.5367948114871979
RMSE train: 0.640509	val: 0.749257	test: 0.749032
MAE train: 0.496283	val: 0.561335	test: 0.572399

Epoch: 20
Loss: 0.5282089884082476
RMSE train: 0.639092	val: 0.746570	test: 0.757877
MAE train: 0.495006	val: 0.562387	test: 0.582743

Epoch: 21
Loss: 0.5196537598967552
RMSE train: 0.637927	val: 0.750604	test: 0.755308
MAE train: 0.496488	val: 0.567110	test: 0.584300

Epoch: 22
Loss: 0.5358717714746793
RMSE train: 0.624961	val: 0.742939	test: 0.751177
MAE train: 0.487987	val: 0.567156	test: 0.579310Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/lipo/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/lipo/random/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/lipo/random/train_prop=0.7/lipophilicity_random_6_26-05_11-07-47  ]
[ Using Seed :  6  ]
[ Using device :  cuda:1  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.145013729731242
RMSE train: 2.205352	val: 2.300946	test: 2.247973
MAE train: 1.955265	val: 2.043261	test: 1.990090

Epoch: 2
Loss: 3.5542932947476706
RMSE train: 1.713547	val: 1.792991	test: 1.726317
MAE train: 1.480821	val: 1.543411	test: 1.487071

Epoch: 3
Loss: 2.3052529295285544
RMSE train: 1.353583	val: 1.403387	test: 1.360701
MAE train: 1.151432	val: 1.189229	test: 1.164486

Epoch: 4
Loss: 1.4490933517615001
RMSE train: 0.939593	val: 1.000486	test: 1.007919
MAE train: 0.767657	val: 0.821693	test: 0.829466

Epoch: 5
Loss: 0.9909271200497946
RMSE train: 0.821540	val: 0.883481	test: 0.898693
MAE train: 0.657682	val: 0.707453	test: 0.716920

Epoch: 6
Loss: 0.8331888616085052
RMSE train: 0.816593	val: 0.860436	test: 0.911888
MAE train: 0.642747	val: 0.680431	test: 0.708569

Epoch: 7
Loss: 0.7662582496802012
RMSE train: 0.762332	val: 0.828559	test: 0.839559
MAE train: 0.597522	val: 0.658804	test: 0.673983

Epoch: 8
Loss: 0.7041664520899454
RMSE train: 0.741460	val: 0.815343	test: 0.832274
MAE train: 0.577017	val: 0.636365	test: 0.648640

Epoch: 9
Loss: 0.6670380135377248
RMSE train: 0.733517	val: 0.804230	test: 0.817910
MAE train: 0.572584	val: 0.622749	test: 0.632474

Epoch: 10
Loss: 0.6536574512720108
RMSE train: 0.717986	val: 0.789815	test: 0.813694
MAE train: 0.558884	val: 0.607631	test: 0.632900

Epoch: 11
Loss: 0.6352227429548899
RMSE train: 0.709473	val: 0.778869	test: 0.796383
MAE train: 0.549354	val: 0.597300	test: 0.609051

Epoch: 12
Loss: 0.6112778037786484
RMSE train: 0.716795	val: 0.797159	test: 0.821634
MAE train: 0.554884	val: 0.606795	test: 0.627447

Epoch: 13
Loss: 0.5958721165855726
RMSE train: 0.680450	val: 0.751672	test: 0.775546
MAE train: 0.529400	val: 0.574743	test: 0.597524

Epoch: 14
Loss: 0.5717297544082006
RMSE train: 0.670730	val: 0.763045	test: 0.774853
MAE train: 0.518702	val: 0.578059	test: 0.595893

Epoch: 15
Loss: 0.5744525591532389
RMSE train: 0.664879	val: 0.761626	test: 0.775063
MAE train: 0.518075	val: 0.575737	test: 0.592645

Epoch: 16
Loss: 0.5444611782828966
RMSE train: 0.667516	val: 0.765913	test: 0.776704
MAE train: 0.517447	val: 0.572687	test: 0.590025

Epoch: 17
Loss: 0.5606713791688284
RMSE train: 0.663547	val: 0.763967	test: 0.788152
MAE train: 0.516039	val: 0.575784	test: 0.603898

Epoch: 18
Loss: 0.5361035764217377
RMSE train: 0.638924	val: 0.752626	test: 0.753367
MAE train: 0.499749	val: 0.570438	test: 0.581679

Epoch: 19
Loss: 0.5158818066120148
RMSE train: 0.651984	val: 0.772658	test: 0.775973
MAE train: 0.509519	val: 0.587516	test: 0.592359

Epoch: 20
Loss: 0.5229752187927564
RMSE train: 0.652991	val: 0.757619	test: 0.766590
MAE train: 0.506652	val: 0.566541	test: 0.588712

Epoch: 21
Loss: 0.5273583953579267
RMSE train: 0.644212	val: 0.768407	test: 0.772587
MAE train: 0.497758	val: 0.571165	test: 0.591334

Epoch: 22
Loss: 0.487918384373188
RMSE train: 0.641882	val: 0.760039	test: 0.771191
MAE train: 0.496786	val: 0.568197	test: 0.582532Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/lipo/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/lipo/random/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/lipo/random/train_prop=0.7/lipophilicity_random_5_26-05_11-07-47  ]
[ Using Seed :  5  ]
[ Using device :  cuda:1  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 4.880793770154317
RMSE train: 2.005281	val: 2.099307	test: 2.041995
MAE train: 1.754406	val: 1.837789	test: 1.786608

Epoch: 2
Loss: 3.326071798801422
RMSE train: 1.675648	val: 1.738615	test: 1.685894
MAE train: 1.449849	val: 1.501594	test: 1.462174

Epoch: 3
Loss: 2.1140941083431244
RMSE train: 1.198988	val: 1.267012	test: 1.246820
MAE train: 1.000065	val: 1.056551	test: 1.055096

Epoch: 4
Loss: 1.3469218611717224
RMSE train: 1.061078	val: 1.125405	test: 1.117230
MAE train: 0.880596	val: 0.928992	test: 0.932379

Epoch: 5
Loss: 0.9197097569704056
RMSE train: 0.811779	val: 0.899338	test: 0.904570
MAE train: 0.653316	val: 0.726467	test: 0.733193

Epoch: 6
Loss: 0.7819168666998545
RMSE train: 0.772733	val: 0.867543	test: 0.865634
MAE train: 0.611798	val: 0.683555	test: 0.690933

Epoch: 7
Loss: 0.7421241750319799
RMSE train: 0.793739	val: 0.894559	test: 0.891791
MAE train: 0.629207	val: 0.702948	test: 0.712643

Epoch: 8
Loss: 0.712318350871404
RMSE train: 0.730827	val: 0.830850	test: 0.816927
MAE train: 0.575213	val: 0.648639	test: 0.644429

Epoch: 9
Loss: 0.6553400407234827
RMSE train: 0.718977	val: 0.828633	test: 0.813757
MAE train: 0.564270	val: 0.646236	test: 0.639482

Epoch: 10
Loss: 0.6381958077351252
RMSE train: 0.720234	val: 0.811325	test: 0.804642
MAE train: 0.561012	val: 0.620596	test: 0.625348

Epoch: 11
Loss: 0.6344021807114283
RMSE train: 0.705204	val: 0.796908	test: 0.789901
MAE train: 0.549616	val: 0.613461	test: 0.616337

Epoch: 12
Loss: 0.5993538002173106
RMSE train: 0.698104	val: 0.805327	test: 0.790384
MAE train: 0.543931	val: 0.617449	test: 0.614252

Epoch: 13
Loss: 0.6035334169864655
RMSE train: 0.697057	val: 0.789440	test: 0.770369
MAE train: 0.542661	val: 0.600701	test: 0.598119

Epoch: 14
Loss: 0.5703582540154457
RMSE train: 0.652843	val: 0.768611	test: 0.751183
MAE train: 0.511747	val: 0.587526	test: 0.588037

Epoch: 15
Loss: 0.5647751515110334
RMSE train: 0.672050	val: 0.800608	test: 0.769635
MAE train: 0.522878	val: 0.608314	test: 0.599740

Epoch: 16
Loss: 0.5528738970557848
RMSE train: 0.673955	val: 0.772941	test: 0.749674
MAE train: 0.528163	val: 0.586787	test: 0.596121

Epoch: 17
Loss: 0.5413408701618513
RMSE train: 0.659952	val: 0.776254	test: 0.764553
MAE train: 0.508236	val: 0.590519	test: 0.591018

Epoch: 18
Loss: 0.5343260938922564
RMSE train: 0.651809	val: 0.755378	test: 0.730235
MAE train: 0.505544	val: 0.576444	test: 0.575276

Epoch: 19
Loss: 0.5237983291347822
RMSE train: 0.634885	val: 0.746571	test: 0.724624
MAE train: 0.496645	val: 0.572919	test: 0.565126

Epoch: 20
Loss: 0.4934782733519872
RMSE train: 0.632736	val: 0.750507	test: 0.721731
MAE train: 0.490608	val: 0.559614	test: 0.562500

Epoch: 21
Loss: 0.48366413017114
RMSE train: 0.627543	val: 0.755301	test: 0.723773
MAE train: 0.489686	val: 0.571939	test: 0.566413

Epoch: 22
Loss: 0.49471361686786014
RMSE train: 0.613283	val: 0.742750	test: 0.709571
MAE train: 0.476780	val: 0.558691	test: 0.546785Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/lipo/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/lipo/random/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/lipo/random/train_prop=0.8/lipophilicity_random_4_26-05_11-07-47  ]
[ Using Seed :  4  ]
[ Using device :  cuda:2  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.359096731458392
RMSE train: 2.182083	val: 2.179277	test: 2.221466
MAE train: 1.939190	val: 1.920577	test: 1.976948

Epoch: 2
Loss: 3.4484582798821584
RMSE train: 1.535226	val: 1.539105	test: 1.564327
MAE train: 1.315868	val: 1.312784	test: 1.347269

Epoch: 3
Loss: 2.0861624819891795
RMSE train: 1.075061	val: 1.116510	test: 1.141030
MAE train: 0.889707	val: 0.925109	test: 0.953635

Epoch: 4
Loss: 1.2577014480318343
RMSE train: 0.862209	val: 0.951542	test: 0.948830
MAE train: 0.696297	val: 0.774023	test: 0.771269

Epoch: 5
Loss: 0.8811673734869275
RMSE train: 0.795843	val: 0.907501	test: 0.896804
MAE train: 0.623850	val: 0.712478	test: 0.694492

Epoch: 6
Loss: 0.7510925233364105
RMSE train: 0.778277	val: 0.906715	test: 0.860493
MAE train: 0.613975	val: 0.713286	test: 0.671796

Epoch: 7
Loss: 0.7307938720498767
RMSE train: 0.792333	val: 0.910792	test: 0.882475
MAE train: 0.612523	val: 0.708826	test: 0.668032

Epoch: 8
Loss: 0.6934919697897775
RMSE train: 0.744849	val: 0.876853	test: 0.835450
MAE train: 0.581553	val: 0.691828	test: 0.649311

Epoch: 9
Loss: 0.6537722774914333
RMSE train: 0.747195	val: 0.880074	test: 0.840386
MAE train: 0.580209	val: 0.689821	test: 0.648099

Epoch: 10
Loss: 0.6435659655502864
RMSE train: 0.723638	val: 0.863082	test: 0.828161
MAE train: 0.563608	val: 0.675645	test: 0.632736

Epoch: 11
Loss: 0.6338099496705192
RMSE train: 0.738943	val: 0.888374	test: 0.831379
MAE train: 0.575773	val: 0.689344	test: 0.631647

Epoch: 12
Loss: 0.5962925255298615
RMSE train: 0.694455	val: 0.821976	test: 0.786285
MAE train: 0.538503	val: 0.645059	test: 0.600127

Epoch: 13
Loss: 0.5853003944669452
RMSE train: 0.679213	val: 0.816698	test: 0.772269
MAE train: 0.525848	val: 0.641219	test: 0.601547

Epoch: 14
Loss: 0.6063772695405143
RMSE train: 0.709043	val: 0.847960	test: 0.797228
MAE train: 0.546434	val: 0.661880	test: 0.606225

Epoch: 15
Loss: 0.5856362006493977
RMSE train: 0.677417	val: 0.819231	test: 0.773168
MAE train: 0.525739	val: 0.636200	test: 0.590307

Epoch: 16
Loss: 0.5779564806393215
RMSE train: 0.691068	val: 0.833052	test: 0.794376
MAE train: 0.529481	val: 0.646718	test: 0.601475

Epoch: 17
Loss: 0.5726561546325684
RMSE train: 0.664748	val: 0.819642	test: 0.761006
MAE train: 0.516298	val: 0.635330	test: 0.586773

Epoch: 18
Loss: 0.5462615042924881
RMSE train: 0.664213	val: 0.826894	test: 0.759079
MAE train: 0.512090	val: 0.635052	test: 0.575978

Epoch: 19
Loss: 0.5175843302692685
RMSE train: 0.687370	val: 0.847933	test: 0.780517
MAE train: 0.533998	val: 0.657232	test: 0.582350

Epoch: 20
Loss: 0.547911707844053
RMSE train: 0.661596	val: 0.816872	test: 0.754663
MAE train: 0.514021	val: 0.633033	test: 0.573768

Epoch: 21
Loss: 0.5003178524119514
RMSE train: 0.623132	val: 0.775490	test: 0.719784
MAE train: 0.482361	val: 0.600544	test: 0.547649

Epoch: 22
Loss: 0.522611854331834
RMSE train: 0.647615	val: 0.782178	test: 0.753548
MAE train: 0.508542	val: 0.612663	test: 0.580784Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/lipo/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/lipo/random/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/lipo/random/train_prop=0.8/lipophilicity_random_6_26-05_11-07-47  ]
[ Using Seed :  6  ]
[ Using device :  cuda:2  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.068975993565151
RMSE train: 2.119865	val: 2.120568	test: 2.166393
MAE train: 1.874102	val: 1.857029	test: 1.920622

Epoch: 2
Loss: 3.2321070773260936
RMSE train: 1.607543	val: 1.616145	test: 1.632202
MAE train: 1.384250	val: 1.387537	test: 1.412744

Epoch: 3
Loss: 1.9718593103545052
RMSE train: 1.085793	val: 1.123468	test: 1.140864
MAE train: 0.897804	val: 0.941183	test: 0.955525

Epoch: 4
Loss: 1.159746412719999
RMSE train: 0.934320	val: 1.007039	test: 1.021398
MAE train: 0.766432	val: 0.823615	test: 0.850562

Epoch: 5
Loss: 0.8264886907168797
RMSE train: 0.803394	val: 0.909431	test: 0.915382
MAE train: 0.637759	val: 0.725598	test: 0.714677

Epoch: 6
Loss: 0.7783847280911037
RMSE train: 0.769800	val: 0.885283	test: 0.868742
MAE train: 0.613920	val: 0.708112	test: 0.696195

Epoch: 7
Loss: 0.7684390672615596
RMSE train: 0.771839	val: 0.901367	test: 0.875038
MAE train: 0.602426	val: 0.715047	test: 0.690324

Epoch: 8
Loss: 0.7256697586604527
RMSE train: 0.753544	val: 0.864062	test: 0.845777
MAE train: 0.595351	val: 0.682816	test: 0.664317

Epoch: 9
Loss: 0.7094042812074933
RMSE train: 0.738209	val: 0.869087	test: 0.835314
MAE train: 0.585003	val: 0.680133	test: 0.650959

Epoch: 10
Loss: 0.687369840485709
RMSE train: 0.746446	val: 0.877510	test: 0.850850
MAE train: 0.579286	val: 0.688312	test: 0.663588

Epoch: 11
Loss: 0.6368698520319802
RMSE train: 0.717275	val: 0.837033	test: 0.811615
MAE train: 0.565396	val: 0.648969	test: 0.642865

Epoch: 12
Loss: 0.6229818207877023
RMSE train: 0.698919	val: 0.825257	test: 0.794095
MAE train: 0.545781	val: 0.635992	test: 0.614174

Epoch: 13
Loss: 0.6576771565846035
RMSE train: 0.689509	val: 0.834061	test: 0.797601
MAE train: 0.535786	val: 0.646294	test: 0.611788

Epoch: 14
Loss: 0.6018629499844143
RMSE train: 0.670584	val: 0.807662	test: 0.778193
MAE train: 0.523726	val: 0.630599	test: 0.603524

Epoch: 15
Loss: 0.6029875789369855
RMSE train: 0.674110	val: 0.823412	test: 0.759878
MAE train: 0.527473	val: 0.641274	test: 0.589433

Epoch: 16
Loss: 0.5773169696331024
RMSE train: 0.669060	val: 0.817276	test: 0.771664
MAE train: 0.520072	val: 0.635153	test: 0.593085

Epoch: 17
Loss: 0.5587646620614188
RMSE train: 0.648325	val: 0.796586	test: 0.747145
MAE train: 0.505788	val: 0.614706	test: 0.577281

Epoch: 18
Loss: 0.5532217792102269
RMSE train: 0.650668	val: 0.807441	test: 0.768474
MAE train: 0.505948	val: 0.631015	test: 0.589003

Epoch: 19
Loss: 0.5304518576179232
RMSE train: 0.656063	val: 0.797158	test: 0.768601
MAE train: 0.517863	val: 0.624734	test: 0.591723

Epoch: 20
Loss: 0.5523313369069781
RMSE train: 0.638596	val: 0.787270	test: 0.744977
MAE train: 0.497186	val: 0.615971	test: 0.572269

Epoch: 21
Loss: 0.5333928146532604
RMSE train: 0.656869	val: 0.783249	test: 0.752736
MAE train: 0.515223	val: 0.606228	test: 0.578917

Epoch: 22
Loss: 0.5249569075448173
RMSE train: 0.628030	val: 0.785058	test: 0.749979
MAE train: 0.491541	val: 0.607217	test: 0.567528Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/lipo/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/lipo/random/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/lipo/random/train_prop=0.8/lipophilicity_random_5_26-05_11-07-47  ]
[ Using Seed :  5  ]
[ Using device :  cuda:2  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 4.785060882568359
RMSE train: 1.930900	val: 1.945713	test: 1.973025
MAE train: 1.686207	val: 1.679436	test: 1.730216

Epoch: 2
Loss: 3.0986084767750333
RMSE train: 1.513286	val: 1.515877	test: 1.558812
MAE train: 1.287296	val: 1.291436	test: 1.345270

Epoch: 3
Loss: 1.8311089277267456
RMSE train: 1.092529	val: 1.130611	test: 1.157083
MAE train: 0.901697	val: 0.936282	test: 0.972085

Epoch: 4
Loss: 1.1112718965326036
RMSE train: 0.855106	val: 0.969758	test: 0.943874
MAE train: 0.677788	val: 0.771470	test: 0.745225

Epoch: 5
Loss: 0.8701025460447583
RMSE train: 0.811577	val: 0.906678	test: 0.891072
MAE train: 0.644489	val: 0.727943	test: 0.718695

Epoch: 6
Loss: 0.8246993252209255
RMSE train: 0.814903	val: 0.931077	test: 0.894624
MAE train: 0.631932	val: 0.733753	test: 0.697879

Epoch: 7
Loss: 0.7544677470411573
RMSE train: 0.811747	val: 0.934825	test: 0.893880
MAE train: 0.626527	val: 0.730246	test: 0.690555

Epoch: 8
Loss: 0.714086502790451
RMSE train: 0.724102	val: 0.833921	test: 0.797901
MAE train: 0.567016	val: 0.658516	test: 0.631523

Epoch: 9
Loss: 0.6801124215126038
RMSE train: 0.743406	val: 0.859360	test: 0.830156
MAE train: 0.576321	val: 0.673419	test: 0.650207

Epoch: 10
Loss: 0.6520884931087494
RMSE train: 0.758244	val: 0.880698	test: 0.823857
MAE train: 0.588468	val: 0.682117	test: 0.630016

Epoch: 11
Loss: 0.654655030795506
RMSE train: 0.720228	val: 0.840208	test: 0.801241
MAE train: 0.565168	val: 0.660154	test: 0.631985

Epoch: 12
Loss: 0.6551545688084194
RMSE train: 0.730085	val: 0.864148	test: 0.809893
MAE train: 0.567193	val: 0.673701	test: 0.618938

Epoch: 13
Loss: 0.613685959151813
RMSE train: 0.702650	val: 0.852599	test: 0.799145
MAE train: 0.542617	val: 0.665950	test: 0.617948

Epoch: 14
Loss: 0.6056963971682957
RMSE train: 0.698473	val: 0.832537	test: 0.756704
MAE train: 0.539365	val: 0.639975	test: 0.578391

Epoch: 15
Loss: 0.6009715029171535
RMSE train: 0.713488	val: 0.841123	test: 0.779844
MAE train: 0.549089	val: 0.651361	test: 0.593022

Epoch: 16
Loss: 0.591659528868539
RMSE train: 0.664007	val: 0.780639	test: 0.739107
MAE train: 0.516720	val: 0.608336	test: 0.575636

Epoch: 17
Loss: 0.5614525271313531
RMSE train: 0.665773	val: 0.810743	test: 0.746099
MAE train: 0.518861	val: 0.627555	test: 0.574387

Epoch: 18
Loss: 0.540525055357388
RMSE train: 0.677234	val: 0.830479	test: 0.759670
MAE train: 0.525606	val: 0.638592	test: 0.575678

Epoch: 19
Loss: 0.4945248918873923
RMSE train: 0.637449	val: 0.791027	test: 0.725328
MAE train: 0.494736	val: 0.611103	test: 0.557774

Epoch: 20
Loss: 0.5255129933357239
RMSE train: 0.640131	val: 0.785451	test: 0.743097
MAE train: 0.493675	val: 0.606891	test: 0.566888

Epoch: 21
Loss: 0.5094954179865974
RMSE train: 0.622753	val: 0.776749	test: 0.710100
MAE train: 0.482747	val: 0.598925	test: 0.541607

Epoch: 22
Loss: 0.53435175546578
RMSE train: 0.648267	val: 0.779211	test: 0.747280
MAE train: 0.505852	val: 0.611655	test: 0.577444

Epoch: 23
Loss: 0.498467817902565
RMSE train: 0.617136	val: 0.708645	test: 0.753080
MAE train: 0.483178	val: 0.528348	test: 0.582668

Epoch: 24
Loss: 0.4834537208080292
RMSE train: 0.647066	val: 0.736382	test: 0.774666
MAE train: 0.512513	val: 0.557564	test: 0.595189

Epoch: 25
Loss: 0.4775850296020508
RMSE train: 0.607307	val: 0.714578	test: 0.753114
MAE train: 0.478082	val: 0.533478	test: 0.583364

Epoch: 26
Loss: 0.4941964417695999
RMSE train: 0.589933	val: 0.690310	test: 0.732351
MAE train: 0.462627	val: 0.515359	test: 0.566728

Epoch: 27
Loss: 0.4608175575733185
RMSE train: 0.597179	val: 0.701941	test: 0.747004
MAE train: 0.469956	val: 0.521386	test: 0.568673

Epoch: 28
Loss: 0.453227561712265
RMSE train: 0.586287	val: 0.688946	test: 0.727866
MAE train: 0.460941	val: 0.512388	test: 0.554960

Epoch: 29
Loss: 0.433145871758461
RMSE train: 0.594737	val: 0.709353	test: 0.742551
MAE train: 0.464016	val: 0.529908	test: 0.570224

Epoch: 30
Loss: 0.4443425625562668
RMSE train: 0.579950	val: 0.699564	test: 0.736138
MAE train: 0.452802	val: 0.515184	test: 0.558931

Epoch: 31
Loss: 0.4452175468206406
RMSE train: 0.610780	val: 0.707689	test: 0.746880
MAE train: 0.477494	val: 0.529501	test: 0.571864

Epoch: 32
Loss: 0.4294758915901184
RMSE train: 0.580234	val: 0.710468	test: 0.730975
MAE train: 0.455046	val: 0.525815	test: 0.559238

Epoch: 33
Loss: 0.43345009386539457
RMSE train: 0.572137	val: 0.696951	test: 0.723413
MAE train: 0.448877	val: 0.516806	test: 0.554424

Epoch: 34
Loss: 0.43805829286575315
RMSE train: 0.569451	val: 0.686945	test: 0.724947
MAE train: 0.448984	val: 0.512807	test: 0.557412

Epoch: 35
Loss: 0.41543286144733427
RMSE train: 0.566178	val: 0.693421	test: 0.730599
MAE train: 0.444491	val: 0.520369	test: 0.568376

Epoch: 36
Loss: 0.42329393327236176
RMSE train: 0.577538	val: 0.701219	test: 0.733604
MAE train: 0.453988	val: 0.526613	test: 0.576752

Epoch: 37
Loss: 0.40519056022167205
RMSE train: 0.578795	val: 0.709753	test: 0.734734
MAE train: 0.451162	val: 0.527982	test: 0.568185

Epoch: 38
Loss: 0.4087220221757889
RMSE train: 0.591004	val: 0.693534	test: 0.740259
MAE train: 0.460563	val: 0.518195	test: 0.562080

Epoch: 39
Loss: 0.39893475770950315
RMSE train: 0.548895	val: 0.683827	test: 0.719466
MAE train: 0.431857	val: 0.504070	test: 0.549539

Epoch: 40
Loss: 0.4079484283924103
RMSE train: 0.585616	val: 0.704416	test: 0.725906
MAE train: 0.456152	val: 0.529581	test: 0.561495

Epoch: 41
Loss: 0.4016402274370193
RMSE train: 0.564388	val: 0.691899	test: 0.721680
MAE train: 0.438751	val: 0.518760	test: 0.558694

Epoch: 42
Loss: 0.39026656150817873
RMSE train: 0.543451	val: 0.684007	test: 0.713096
MAE train: 0.425097	val: 0.508424	test: 0.550164

Epoch: 43
Loss: 0.3908806383609772
RMSE train: 0.561597	val: 0.684194	test: 0.729429
MAE train: 0.439416	val: 0.509909	test: 0.547356

Epoch: 44
Loss: 0.3746523350477219
RMSE train: 0.547694	val: 0.700473	test: 0.723261
MAE train: 0.432134	val: 0.522900	test: 0.557844

Epoch: 45
Loss: 0.38121907114982606
RMSE train: 0.524826	val: 0.685374	test: 0.711424
MAE train: 0.409649	val: 0.505376	test: 0.541304

Epoch: 46
Loss: 0.38784179389476775
RMSE train: 0.540104	val: 0.694284	test: 0.726586
MAE train: 0.424740	val: 0.511147	test: 0.553340

Epoch: 47
Loss: 0.3783366322517395
RMSE train: 0.551058	val: 0.684322	test: 0.707322
MAE train: 0.433708	val: 0.506224	test: 0.539360

Epoch: 48
Loss: 0.3588512450456619
RMSE train: 0.533511	val: 0.681149	test: 0.702326
MAE train: 0.415558	val: 0.509223	test: 0.541504

Epoch: 49
Loss: 0.3694644242525101
RMSE train: 0.537219	val: 0.679098	test: 0.701126
MAE train: 0.422509	val: 0.509311	test: 0.537818

Epoch: 50
Loss: 0.36908715069293974
RMSE train: 0.565344	val: 0.697693	test: 0.731101
MAE train: 0.440918	val: 0.516664	test: 0.552315

Epoch: 51
Loss: 0.35954104363918304
RMSE train: 0.550744	val: 0.687407	test: 0.723314
MAE train: 0.431061	val: 0.506461	test: 0.549101

Epoch: 52
Loss: 0.3693592220544815
RMSE train: 0.529299	val: 0.680062	test: 0.706870
MAE train: 0.415645	val: 0.508328	test: 0.546334

Epoch: 53
Loss: 0.33879463374614716
RMSE train: 0.513432	val: 0.667393	test: 0.696707
MAE train: 0.400050	val: 0.490439	test: 0.529924

Epoch: 54
Loss: 0.3243998497724533
RMSE train: 0.531373	val: 0.686587	test: 0.708041
MAE train: 0.417035	val: 0.510700	test: 0.538455

Epoch: 55
Loss: 0.32919231951236727
RMSE train: 0.508425	val: 0.675191	test: 0.696991
MAE train: 0.395322	val: 0.497121	test: 0.530526

Epoch: 56
Loss: 0.3482794016599655
RMSE train: 0.519167	val: 0.670060	test: 0.711776
MAE train: 0.403936	val: 0.497489	test: 0.538963

Epoch: 57
Loss: 0.3390251100063324
RMSE train: 0.515986	val: 0.678609	test: 0.705775
MAE train: 0.404717	val: 0.504543	test: 0.538743

Epoch: 58
Loss: 0.3298994779586792
RMSE train: 0.509383	val: 0.676101	test: 0.703392
MAE train: 0.397686	val: 0.499878	test: 0.538831

Epoch: 59
Loss: 0.32384522557258605
RMSE train: 0.505200	val: 0.670968	test: 0.705985
MAE train: 0.394014	val: 0.492554	test: 0.530042

Epoch: 60
Loss: 0.3258615046739578
RMSE train: 0.520410	val: 0.685567	test: 0.710697
MAE train: 0.406537	val: 0.503692	test: 0.540012

Epoch: 61
Loss: 0.3267660021781921
RMSE train: 0.501714	val: 0.663112	test: 0.694199
MAE train: 0.389749	val: 0.485513	test: 0.525014

Epoch: 62
Loss: 0.32107230126857755
RMSE train: 0.483141	val: 0.679361	test: 0.694760
MAE train: 0.379213	val: 0.494152	test: 0.519337

Epoch: 63
Loss: 0.3256029799580574
RMSE train: 0.489263	val: 0.660744	test: 0.695041
MAE train: 0.380354	val: 0.483456	test: 0.525144

Epoch: 64
Loss: 0.3105207920074463
RMSE train: 0.503650	val: 0.677632	test: 0.705522
MAE train: 0.390842	val: 0.496008	test: 0.533043

Epoch: 65
Loss: 0.3198291748762131
RMSE train: 0.516801	val: 0.682603	test: 0.707952
MAE train: 0.403159	val: 0.502546	test: 0.533292

Epoch: 66
Loss: 0.3104659736156464
RMSE train: 0.512664	val: 0.682530	test: 0.710906
MAE train: 0.403781	val: 0.510983	test: 0.548488

Epoch: 67
Loss: 0.3103320896625519
RMSE train: 0.493580	val: 0.659764	test: 0.694200
MAE train: 0.385738	val: 0.487102	test: 0.530336

Epoch: 68
Loss: 0.3145085066556931
RMSE train: 0.492360	val: 0.673261	test: 0.690555
MAE train: 0.381750	val: 0.493940	test: 0.522219

Epoch: 69
Loss: 0.3151422917842865
RMSE train: 0.516779	val: 0.691704	test: 0.703775
MAE train: 0.406029	val: 0.512430	test: 0.532480

Epoch: 70
Loss: 0.3012975037097931
RMSE train: 0.483280	val: 0.664709	test: 0.679116
MAE train: 0.375330	val: 0.488531	test: 0.515805

Epoch: 71
Loss: 0.30375467240810394
RMSE train: 0.458725	val: 0.655027	test: 0.675643
MAE train: 0.359234	val: 0.480866	test: 0.512641

Epoch: 72
Loss: 0.29728258550167086
RMSE train: 0.475432	val: 0.662544	test: 0.686838
MAE train: 0.372100	val: 0.488642	test: 0.518339

Epoch: 73
Loss: 0.29749571084976195
RMSE train: 0.470234	val: 0.655074	test: 0.684938
MAE train: 0.368163	val: 0.483461	test: 0.521493

Epoch: 74
Loss: 0.2957478165626526
RMSE train: 0.470501	val: 0.661977	test: 0.684120
MAE train: 0.365389	val: 0.490882	test: 0.522174

Epoch: 75
Loss: 0.2895605146884918
RMSE train: 0.502243	val: 0.683432	test: 0.707303
MAE train: 0.390090	val: 0.503420	test: 0.534280

Epoch: 76
Loss: 0.28360296189785006
RMSE train: 0.462258	val: 0.648058	test: 0.685497
MAE train: 0.361020	val: 0.480325	test: 0.521546

Epoch: 77
Loss: 0.2881599307060242
RMSE train: 0.489218	val: 0.673780	test: 0.696223
MAE train: 0.385094	val: 0.498345	test: 0.525176

Epoch: 78
Loss: 0.29558160305023196
RMSE train: 0.465689	val: 0.664420	test: 0.674469
MAE train: 0.364907	val: 0.485664	test: 0.515330

Epoch: 79
Loss: 0.2780933782458305
RMSE train: 0.457129	val: 0.651706	test: 0.682320
MAE train: 0.353915	val: 0.476080	test: 0.519197

Epoch: 80
Loss: 0.28398858904838564
RMSE train: 0.464475	val: 0.662423	test: 0.678139
MAE train: 0.365653	val: 0.486394	test: 0.518080

Epoch: 81
Loss: 0.27286425828933714
RMSE train: 0.465333	val: 0.667884	test: 0.681726
MAE train: 0.363920	val: 0.486537	test: 0.512288

Epoch: 82
Loss: 0.2914358377456665
RMSE train: 0.495736	val: 0.698268	test: 0.705062
MAE train: 0.387109	val: 0.511595	test: 0.541220

Epoch: 83
Loss: 0.2750323086977005
RMSE train: 0.512672	val: 0.698341	test: 0.711603

Epoch: 23
Loss: 0.4872172802686691
RMSE train: 0.600955	val: 0.697087	test: 0.756564
MAE train: 0.468811	val: 0.523606	test: 0.578537

Epoch: 24
Loss: 0.4974398881196976
RMSE train: 0.603737	val: 0.699734	test: 0.757757
MAE train: 0.473821	val: 0.528583	test: 0.586230

Epoch: 25
Loss: 0.490078529715538
RMSE train: 0.598321	val: 0.695438	test: 0.745780
MAE train: 0.470641	val: 0.524015	test: 0.579521

Epoch: 26
Loss: 0.46862800121307374
RMSE train: 0.615620	val: 0.715211	test: 0.763629
MAE train: 0.483657	val: 0.532213	test: 0.585585

Epoch: 27
Loss: 0.47669307291507723
RMSE train: 0.603168	val: 0.700239	test: 0.753294
MAE train: 0.470130	val: 0.519886	test: 0.582767

Epoch: 28
Loss: 0.4646260768175125
RMSE train: 0.576195	val: 0.689987	test: 0.737242
MAE train: 0.452729	val: 0.514071	test: 0.564759

Epoch: 29
Loss: 0.45580818355083463
RMSE train: 0.581654	val: 0.681606	test: 0.738917
MAE train: 0.453393	val: 0.505320	test: 0.561169

Epoch: 30
Loss: 0.45609551966190337
RMSE train: 0.575037	val: 0.689342	test: 0.730106
MAE train: 0.447814	val: 0.511096	test: 0.560304

Epoch: 31
Loss: 0.4383065938949585
RMSE train: 0.560341	val: 0.670659	test: 0.727396
MAE train: 0.434001	val: 0.495969	test: 0.552103

Epoch: 32
Loss: 0.4389998823404312
RMSE train: 0.565522	val: 0.679171	test: 0.723613
MAE train: 0.446189	val: 0.509391	test: 0.557716

Epoch: 33
Loss: 0.41957026422023774
RMSE train: 0.581738	val: 0.678350	test: 0.741508
MAE train: 0.455478	val: 0.501628	test: 0.567213

Epoch: 34
Loss: 0.4167813420295715
RMSE train: 0.611298	val: 0.708028	test: 0.778231
MAE train: 0.477485	val: 0.526464	test: 0.584080

Epoch: 35
Loss: 0.4411153167486191
RMSE train: 0.580378	val: 0.684464	test: 0.757182
MAE train: 0.453797	val: 0.507553	test: 0.569523

Epoch: 36
Loss: 0.4137802988290787
RMSE train: 0.588635	val: 0.695168	test: 0.748325
MAE train: 0.460086	val: 0.516377	test: 0.566557

Epoch: 37
Loss: 0.3976456791162491
RMSE train: 0.542565	val: 0.671475	test: 0.720838
MAE train: 0.423638	val: 0.497664	test: 0.548178

Epoch: 38
Loss: 0.4144784390926361
RMSE train: 0.549238	val: 0.674503	test: 0.724619
MAE train: 0.428044	val: 0.505863	test: 0.550720

Epoch: 39
Loss: 0.3941423535346985
RMSE train: 0.555936	val: 0.683959	test: 0.728537
MAE train: 0.433350	val: 0.505389	test: 0.551630

Epoch: 40
Loss: 0.3989620476961136
RMSE train: 0.551425	val: 0.694294	test: 0.737054
MAE train: 0.435407	val: 0.518136	test: 0.560624

Epoch: 41
Loss: 0.3950429379940033
RMSE train: 0.554742	val: 0.686736	test: 0.732791
MAE train: 0.432818	val: 0.509552	test: 0.554858

Epoch: 42
Loss: 0.3730179339647293
RMSE train: 0.554013	val: 0.679954	test: 0.729192
MAE train: 0.432791	val: 0.501765	test: 0.547806

Epoch: 43
Loss: 0.3814406394958496
RMSE train: 0.532715	val: 0.672548	test: 0.728160
MAE train: 0.417205	val: 0.500830	test: 0.555746

Epoch: 44
Loss: 0.387945756316185
RMSE train: 0.530188	val: 0.680511	test: 0.711592
MAE train: 0.415060	val: 0.502087	test: 0.536847

Epoch: 45
Loss: 0.368717685341835
RMSE train: 0.536884	val: 0.672225	test: 0.730240
MAE train: 0.419626	val: 0.498759	test: 0.554647

Epoch: 46
Loss: 0.38797277212142944
RMSE train: 0.530950	val: 0.667593	test: 0.714966
MAE train: 0.418753	val: 0.498110	test: 0.548562

Epoch: 47
Loss: 0.37740554809570315
RMSE train: 0.537758	val: 0.681879	test: 0.725671
MAE train: 0.422913	val: 0.503864	test: 0.549425

Epoch: 48
Loss: 0.3585029661655426
RMSE train: 0.549342	val: 0.687072	test: 0.733584
MAE train: 0.429154	val: 0.510982	test: 0.553853

Epoch: 49
Loss: 0.37734563648700714
RMSE train: 0.517515	val: 0.674639	test: 0.719411
MAE train: 0.405768	val: 0.495025	test: 0.533801

Epoch: 50
Loss: 0.3653547078371048
RMSE train: 0.521753	val: 0.679037	test: 0.719840
MAE train: 0.410428	val: 0.498657	test: 0.539587

Epoch: 51
Loss: 0.37315097749233245
RMSE train: 0.585198	val: 0.710568	test: 0.769621
MAE train: 0.459297	val: 0.535061	test: 0.582445

Epoch: 52
Loss: 0.3626128673553467
RMSE train: 0.526933	val: 0.673786	test: 0.731859
MAE train: 0.413620	val: 0.495158	test: 0.549685

Epoch: 53
Loss: 0.35350262820720674
RMSE train: 0.518388	val: 0.670932	test: 0.731583
MAE train: 0.404836	val: 0.489942	test: 0.550877

Epoch: 54
Loss: 0.3549628347158432
RMSE train: 0.504564	val: 0.669932	test: 0.702388
MAE train: 0.395060	val: 0.487839	test: 0.526125

Epoch: 55
Loss: 0.33827107548713686
RMSE train: 0.535369	val: 0.692156	test: 0.727304
MAE train: 0.419612	val: 0.508273	test: 0.546218

Epoch: 56
Loss: 0.34712775349617003
RMSE train: 0.488855	val: 0.663416	test: 0.700376
MAE train: 0.384315	val: 0.486228	test: 0.535133

Epoch: 57
Loss: 0.335859951376915
RMSE train: 0.489559	val: 0.649625	test: 0.701040
MAE train: 0.383989	val: 0.481140	test: 0.536067

Epoch: 58
Loss: 0.34752327501773833
RMSE train: 0.524967	val: 0.668683	test: 0.711105
MAE train: 0.407081	val: 0.491195	test: 0.539241

Epoch: 59
Loss: 0.347616246342659
RMSE train: 0.529594	val: 0.695360	test: 0.738741
MAE train: 0.420057	val: 0.516811	test: 0.567255

Epoch: 60
Loss: 0.3207308351993561
RMSE train: 0.511626	val: 0.663346	test: 0.699993
MAE train: 0.400400	val: 0.492949	test: 0.532748

Epoch: 61
Loss: 0.3261620461940765
RMSE train: 0.513447	val: 0.670396	test: 0.715167
MAE train: 0.398311	val: 0.498684	test: 0.541431

Epoch: 62
Loss: 0.34232768416404724
RMSE train: 0.507717	val: 0.669158	test: 0.711266
MAE train: 0.395113	val: 0.493963	test: 0.540476

Epoch: 63
Loss: 0.3380009740591049
RMSE train: 0.494357	val: 0.668335	test: 0.709596
MAE train: 0.387042	val: 0.492784	test: 0.533406

Epoch: 64
Loss: 0.3301485449075699
RMSE train: 0.496968	val: 0.670774	test: 0.711496
MAE train: 0.391513	val: 0.493159	test: 0.536092

Epoch: 65
Loss: 0.3383104085922241
RMSE train: 0.512965	val: 0.698893	test: 0.712379
MAE train: 0.402497	val: 0.512117	test: 0.539923

Epoch: 66
Loss: 0.3228014975786209
RMSE train: 0.470758	val: 0.657797	test: 0.689157
MAE train: 0.366679	val: 0.481976	test: 0.519545

Epoch: 67
Loss: 0.30614839494228363
RMSE train: 0.476814	val: 0.668905	test: 0.704125
MAE train: 0.373035	val: 0.488083	test: 0.528553

Epoch: 68
Loss: 0.31297974586486815
RMSE train: 0.480731	val: 0.669295	test: 0.689464
MAE train: 0.375833	val: 0.486957	test: 0.520101

Epoch: 69
Loss: 0.31459830701351166
RMSE train: 0.486618	val: 0.658464	test: 0.703084
MAE train: 0.379902	val: 0.480008	test: 0.527672

Epoch: 70
Loss: 0.31523159593343736
RMSE train: 0.493998	val: 0.656921	test: 0.714254
MAE train: 0.389071	val: 0.488429	test: 0.536685

Epoch: 71
Loss: 0.29774224311113356
RMSE train: 0.523879	val: 0.684705	test: 0.724872
MAE train: 0.414035	val: 0.507725	test: 0.540648

Epoch: 72
Loss: 0.3190553903579712
RMSE train: 0.485541	val: 0.664901	test: 0.697701
MAE train: 0.381394	val: 0.492301	test: 0.528693

Epoch: 73
Loss: 0.30437399744987487
RMSE train: 0.474870	val: 0.655149	test: 0.690978
MAE train: 0.374887	val: 0.482635	test: 0.518439

Epoch: 74
Loss: 0.2898718237876892
RMSE train: 0.476112	val: 0.676074	test: 0.691395
MAE train: 0.373826	val: 0.492250	test: 0.521216

Epoch: 75
Loss: 0.2925774961709976
RMSE train: 0.489227	val: 0.677501	test: 0.700864
MAE train: 0.384213	val: 0.488641	test: 0.526905

Epoch: 76
Loss: 0.2882271409034729
RMSE train: 0.496742	val: 0.670496	test: 0.700669
MAE train: 0.393070	val: 0.499236	test: 0.530583

Epoch: 77
Loss: 0.29862514436244963
RMSE train: 0.458398	val: 0.642423	test: 0.696633
MAE train: 0.359240	val: 0.470872	test: 0.523632

Epoch: 78
Loss: 0.2898624151945114
RMSE train: 0.474340	val: 0.662290	test: 0.690359
MAE train: 0.370521	val: 0.483056	test: 0.517231

Epoch: 79
Loss: 0.30257085859775545
RMSE train: 0.466462	val: 0.666247	test: 0.712328
MAE train: 0.364978	val: 0.490122	test: 0.535227

Epoch: 80
Loss: 0.30051063299179076
RMSE train: 0.480014	val: 0.655432	test: 0.691489
MAE train: 0.373641	val: 0.480654	test: 0.519830

Epoch: 81
Loss: 0.27516271024942396
RMSE train: 0.464015	val: 0.647468	test: 0.691314
MAE train: 0.362944	val: 0.472301	test: 0.515702

Epoch: 82
Loss: 0.28744440227746965
RMSE train: 0.472170	val: 0.650625	test: 0.700401
MAE train: 0.371725	val: 0.479798	test: 0.528412

Epoch: 83
Loss: 0.27727596610784533
RMSE train: 0.464112	val: 0.655253	test: 0.687124

Epoch: 23
Loss: 0.4833005517721176
RMSE train: 0.630852	val: 0.704767	test: 0.769954
MAE train: 0.489450	val: 0.530816	test: 0.594787

Epoch: 24
Loss: 0.4858062416315079
RMSE train: 0.657229	val: 0.726603	test: 0.782465
MAE train: 0.516346	val: 0.550841	test: 0.604919

Epoch: 25
Loss: 0.4919353425502777
RMSE train: 0.615666	val: 0.702310	test: 0.768105
MAE train: 0.482035	val: 0.530591	test: 0.590299

Epoch: 26
Loss: 0.4681202441453934
RMSE train: 0.609395	val: 0.691713	test: 0.754751
MAE train: 0.477124	val: 0.520847	test: 0.586697

Epoch: 27
Loss: 0.4662101149559021
RMSE train: 0.626920	val: 0.715445	test: 0.781084
MAE train: 0.492888	val: 0.538062	test: 0.602199

Epoch: 28
Loss: 0.47044356167316437
RMSE train: 0.627235	val: 0.718996	test: 0.760393
MAE train: 0.493128	val: 0.541835	test: 0.587230

Epoch: 29
Loss: 0.46242926716804506
RMSE train: 0.612097	val: 0.698620	test: 0.756757
MAE train: 0.477283	val: 0.523171	test: 0.578724

Epoch: 30
Loss: 0.44344442188739774
RMSE train: 0.588861	val: 0.692474	test: 0.743647
MAE train: 0.465136	val: 0.521839	test: 0.579675

Epoch: 31
Loss: 0.44530248641967773
RMSE train: 0.600417	val: 0.695862	test: 0.760554
MAE train: 0.471073	val: 0.523022	test: 0.588015

Epoch: 32
Loss: 0.43055744767189025
RMSE train: 0.613790	val: 0.714873	test: 0.757294
MAE train: 0.480593	val: 0.534915	test: 0.587350

Epoch: 33
Loss: 0.43931481540203093
RMSE train: 0.605916	val: 0.698340	test: 0.757524
MAE train: 0.475066	val: 0.529462	test: 0.581771

Epoch: 34
Loss: 0.4396519958972931
RMSE train: 0.646174	val: 0.743891	test: 0.787343
MAE train: 0.508402	val: 0.564375	test: 0.602736

Epoch: 35
Loss: 0.4207609236240387
RMSE train: 0.591004	val: 0.693828	test: 0.737757
MAE train: 0.458560	val: 0.512894	test: 0.566071

Epoch: 36
Loss: 0.42556457221508026
RMSE train: 0.596852	val: 0.703996	test: 0.751283
MAE train: 0.469182	val: 0.530419	test: 0.566723

Epoch: 37
Loss: 0.41722466349601744
RMSE train: 0.588429	val: 0.716025	test: 0.756458
MAE train: 0.462908	val: 0.537359	test: 0.584369

Epoch: 38
Loss: 0.40652925670146944
RMSE train: 0.580450	val: 0.690367	test: 0.736474
MAE train: 0.453678	val: 0.515671	test: 0.563303

Epoch: 39
Loss: 0.40447205901145933
RMSE train: 0.588860	val: 0.710902	test: 0.750144
MAE train: 0.459924	val: 0.529762	test: 0.570940

Epoch: 40
Loss: 0.38747645914554596
RMSE train: 0.556081	val: 0.684667	test: 0.716182
MAE train: 0.431383	val: 0.502694	test: 0.548129

Epoch: 41
Loss: 0.3956418424844742
RMSE train: 0.551453	val: 0.688293	test: 0.738852
MAE train: 0.432020	val: 0.509194	test: 0.562675

Epoch: 42
Loss: 0.38815054297447205
RMSE train: 0.574535	val: 0.718224	test: 0.734424
MAE train: 0.445343	val: 0.534310	test: 0.561764

Epoch: 43
Loss: 0.38917967975139617
RMSE train: 0.614256	val: 0.739911	test: 0.774240
MAE train: 0.487303	val: 0.555863	test: 0.590510

Epoch: 44
Loss: 0.38499673306941984
RMSE train: 0.553592	val: 0.687044	test: 0.736840
MAE train: 0.430057	val: 0.515487	test: 0.558502

Epoch: 45
Loss: 0.38779823780059813
RMSE train: 0.557481	val: 0.687641	test: 0.725223
MAE train: 0.434078	val: 0.514952	test: 0.558724

Epoch: 46
Loss: 0.390292164683342
RMSE train: 0.549494	val: 0.685870	test: 0.727193
MAE train: 0.429863	val: 0.511712	test: 0.555535

Epoch: 47
Loss: 0.36696421205997465
RMSE train: 0.550276	val: 0.672218	test: 0.734629
MAE train: 0.427114	val: 0.502319	test: 0.556996

Epoch: 48
Loss: 0.37591047286987306
RMSE train: 0.539024	val: 0.678114	test: 0.715738
MAE train: 0.422769	val: 0.504307	test: 0.542600

Epoch: 49
Loss: 0.35206542909145355
RMSE train: 0.566064	val: 0.690602	test: 0.746476
MAE train: 0.443865	val: 0.516266	test: 0.565467

Epoch: 50
Loss: 0.35978453755378725
RMSE train: 0.545449	val: 0.679677	test: 0.736082
MAE train: 0.426050	val: 0.505968	test: 0.558081

Epoch: 51
Loss: 0.3548844665288925
RMSE train: 0.530582	val: 0.674892	test: 0.719684
MAE train: 0.416729	val: 0.505349	test: 0.541945

Epoch: 52
Loss: 0.35963636040687563
RMSE train: 0.507801	val: 0.656044	test: 0.711298
MAE train: 0.395826	val: 0.486998	test: 0.533677

Epoch: 53
Loss: 0.36657229363918303
RMSE train: 0.513204	val: 0.668073	test: 0.707263
MAE train: 0.402344	val: 0.496952	test: 0.536619

Epoch: 54
Loss: 0.33719616532325747
RMSE train: 0.525713	val: 0.666160	test: 0.730292
MAE train: 0.407948	val: 0.494553	test: 0.548168

Epoch: 55
Loss: 0.3423537194728851
RMSE train: 0.552679	val: 0.698857	test: 0.730634
MAE train: 0.436689	val: 0.520955	test: 0.557396

Epoch: 56
Loss: 0.3416899651288986
RMSE train: 0.605952	val: 0.730455	test: 0.772033
MAE train: 0.483199	val: 0.554091	test: 0.591421

Epoch: 57
Loss: 0.34495090544223783
RMSE train: 0.537479	val: 0.680591	test: 0.724863
MAE train: 0.422842	val: 0.508469	test: 0.549663

Epoch: 58
Loss: 0.3405909076333046
RMSE train: 0.499861	val: 0.657876	test: 0.721139
MAE train: 0.391211	val: 0.491055	test: 0.545705

Epoch: 59
Loss: 0.3309781581163406
RMSE train: 0.530861	val: 0.682725	test: 0.718742
MAE train: 0.419718	val: 0.511601	test: 0.540142

Epoch: 60
Loss: 0.3197414636611938
RMSE train: 0.533467	val: 0.677310	test: 0.724148
MAE train: 0.420138	val: 0.509397	test: 0.548407

Epoch: 61
Loss: 0.3278453588485718
RMSE train: 0.510834	val: 0.659514	test: 0.715884
MAE train: 0.396788	val: 0.487393	test: 0.536546

Epoch: 62
Loss: 0.3111127704381943
RMSE train: 0.512667	val: 0.676100	test: 0.707002
MAE train: 0.399162	val: 0.498736	test: 0.531771

Epoch: 63
Loss: 0.3244039982557297
RMSE train: 0.520979	val: 0.676051	test: 0.724347
MAE train: 0.405443	val: 0.503880	test: 0.545684

Epoch: 64
Loss: 0.3310667186975479
RMSE train: 0.518786	val: 0.668098	test: 0.714681
MAE train: 0.408285	val: 0.496410	test: 0.534816

Epoch: 65
Loss: 0.31647062599658965
RMSE train: 0.522128	val: 0.679954	test: 0.731148
MAE train: 0.409833	val: 0.503713	test: 0.552250

Epoch: 66
Loss: 0.32068548202514646
RMSE train: 0.524299	val: 0.673215	test: 0.722382
MAE train: 0.413439	val: 0.503417	test: 0.553320

Epoch: 67
Loss: 0.3174067199230194
RMSE train: 0.479892	val: 0.651353	test: 0.702992
MAE train: 0.374177	val: 0.482081	test: 0.527284

Epoch: 68
Loss: 0.3067727655172348
RMSE train: 0.511742	val: 0.660868	test: 0.707443
MAE train: 0.399294	val: 0.491989	test: 0.533741

Epoch: 69
Loss: 0.3174103170633316
RMSE train: 0.499726	val: 0.663430	test: 0.722504
MAE train: 0.389782	val: 0.499345	test: 0.541002

Epoch: 70
Loss: 0.30077830255031585
RMSE train: 0.470909	val: 0.648261	test: 0.691445
MAE train: 0.367683	val: 0.479193	test: 0.519762

Epoch: 71
Loss: 0.30955936312675475
RMSE train: 0.500426	val: 0.666546	test: 0.719834
MAE train: 0.391834	val: 0.492390	test: 0.541857

Epoch: 72
Loss: 0.30034475326538085
RMSE train: 0.471622	val: 0.654544	test: 0.699715
MAE train: 0.366517	val: 0.484514	test: 0.524590

Epoch: 73
Loss: 0.29861395359039306
RMSE train: 0.513891	val: 0.678759	test: 0.720497
MAE train: 0.406391	val: 0.504951	test: 0.544244

Epoch: 74
Loss: 0.28615363240242003
RMSE train: 0.482449	val: 0.657758	test: 0.713647
MAE train: 0.379419	val: 0.487113	test: 0.532205

Epoch: 75
Loss: 0.2926072716712952
RMSE train: 0.499756	val: 0.666364	test: 0.708013
MAE train: 0.391392	val: 0.492198	test: 0.536694

Epoch: 76
Loss: 0.29385396987199786
RMSE train: 0.468702	val: 0.654582	test: 0.704616
MAE train: 0.365787	val: 0.478678	test: 0.529114

Epoch: 77
Loss: 0.28481983244419096
RMSE train: 0.479040	val: 0.652374	test: 0.695964
MAE train: 0.377218	val: 0.483594	test: 0.521736

Epoch: 78
Loss: 0.29262363612651826
RMSE train: 0.517611	val: 0.688034	test: 0.724378
MAE train: 0.409053	val: 0.511165	test: 0.544264

Epoch: 79
Loss: 0.28629153668880464
RMSE train: 0.510413	val: 0.675734	test: 0.717678
MAE train: 0.405145	val: 0.504364	test: 0.543872

Epoch: 80
Loss: 0.2872501462697983
RMSE train: 0.475415	val: 0.652931	test: 0.693575
MAE train: 0.371980	val: 0.476998	test: 0.517273

Epoch: 81
Loss: 0.2883618175983429
RMSE train: 0.486883	val: 0.656442	test: 0.701723
MAE train: 0.374545	val: 0.484776	test: 0.530729

Epoch: 82
Loss: 0.2928151935338974
RMSE train: 0.497054	val: 0.663223	test: 0.718177
MAE train: 0.391204	val: 0.492056	test: 0.542025

Epoch: 83
Loss: 0.27689706087112426
RMSE train: 0.489920	val: 0.660365	test: 0.705371

Epoch: 23
Loss: 0.4901660258571307
RMSE train: 0.612757	val: 0.737300	test: 0.737408
MAE train: 0.475097	val: 0.550780	test: 0.565046

Epoch: 24
Loss: 0.47038277983665466
RMSE train: 0.646144	val: 0.763831	test: 0.775780
MAE train: 0.502159	val: 0.570846	test: 0.583396

Epoch: 25
Loss: 0.4795854464173317
RMSE train: 0.620251	val: 0.757698	test: 0.751863
MAE train: 0.479784	val: 0.559409	test: 0.568913

Epoch: 26
Loss: 0.4744051496187846
RMSE train: 0.618607	val: 0.740239	test: 0.746827
MAE train: 0.480203	val: 0.545798	test: 0.563467

Epoch: 27
Loss: 0.4640243624647458
RMSE train: 0.607389	val: 0.755965	test: 0.754687
MAE train: 0.472946	val: 0.566736	test: 0.570364

Epoch: 28
Loss: 0.46170058598121005
RMSE train: 0.584521	val: 0.728901	test: 0.733988
MAE train: 0.457799	val: 0.543041	test: 0.554377

Epoch: 29
Loss: 0.450208343565464
RMSE train: 0.585639	val: 0.728731	test: 0.732039
MAE train: 0.454210	val: 0.537685	test: 0.549702

Epoch: 30
Loss: 0.4395968019962311
RMSE train: 0.582188	val: 0.738570	test: 0.727518
MAE train: 0.452412	val: 0.547899	test: 0.552570

Epoch: 31
Loss: 0.4229739358027776
RMSE train: 0.567158	val: 0.733523	test: 0.720810
MAE train: 0.439570	val: 0.534632	test: 0.542257

Epoch: 32
Loss: 0.4471129427353541
RMSE train: 0.637789	val: 0.777231	test: 0.780871
MAE train: 0.502464	val: 0.575716	test: 0.594510

Epoch: 33
Loss: 0.42579345405101776
RMSE train: 0.583591	val: 0.734115	test: 0.750092
MAE train: 0.451312	val: 0.535904	test: 0.557368

Epoch: 34
Loss: 0.42633315175771713
RMSE train: 0.607590	val: 0.748452	test: 0.756068
MAE train: 0.476428	val: 0.551963	test: 0.567930

Epoch: 35
Loss: 0.4211327979962031
RMSE train: 0.571990	val: 0.735742	test: 0.727167
MAE train: 0.445195	val: 0.541234	test: 0.544950

Epoch: 36
Loss: 0.41886821140845615
RMSE train: 0.575331	val: 0.726447	test: 0.726338
MAE train: 0.448273	val: 0.536707	test: 0.542439

Epoch: 37
Loss: 0.4147113462289174
RMSE train: 0.553666	val: 0.728246	test: 0.725791
MAE train: 0.430820	val: 0.540414	test: 0.538626

Epoch: 38
Loss: 0.40303237239519757
RMSE train: 0.582526	val: 0.732158	test: 0.735734
MAE train: 0.454422	val: 0.537931	test: 0.554181

Epoch: 39
Loss: 0.41321689635515213
RMSE train: 0.617735	val: 0.766517	test: 0.773031
MAE train: 0.483140	val: 0.562717	test: 0.587246

Epoch: 40
Loss: 0.3946317806839943
RMSE train: 0.567878	val: 0.736133	test: 0.738443
MAE train: 0.441075	val: 0.537654	test: 0.552867

Epoch: 41
Loss: 0.3979992518822352
RMSE train: 0.532747	val: 0.716286	test: 0.708121
MAE train: 0.414558	val: 0.513839	test: 0.526827

Epoch: 42
Loss: 0.3814071863889694
RMSE train: 0.519295	val: 0.702260	test: 0.694465
MAE train: 0.403080	val: 0.510698	test: 0.515395

Epoch: 43
Loss: 0.3777959992488225
RMSE train: 0.532362	val: 0.727702	test: 0.708453
MAE train: 0.416336	val: 0.518669	test: 0.526134

Epoch: 44
Loss: 0.3776136015852292
RMSE train: 0.552261	val: 0.727154	test: 0.717202
MAE train: 0.430284	val: 0.529745	test: 0.537448

Epoch: 45
Loss: 0.3823389758666356
RMSE train: 0.511267	val: 0.701435	test: 0.693305
MAE train: 0.399917	val: 0.513953	test: 0.516768

Epoch: 46
Loss: 0.36964651197195053
RMSE train: 0.528083	val: 0.709740	test: 0.705287
MAE train: 0.411590	val: 0.516293	test: 0.525610

Epoch: 47
Loss: 0.37337782482306164
RMSE train: 0.527102	val: 0.703673	test: 0.694361
MAE train: 0.411671	val: 0.508024	test: 0.517171

Epoch: 48
Loss: 0.37553658833106357
RMSE train: 0.561481	val: 0.738403	test: 0.751214
MAE train: 0.436885	val: 0.533948	test: 0.561642

Epoch: 49
Loss: 0.3944171617428462
RMSE train: 0.554462	val: 0.745541	test: 0.715776
MAE train: 0.435501	val: 0.545742	test: 0.539615

Epoch: 50
Loss: 0.37149688601493835
RMSE train: 0.550203	val: 0.718220	test: 0.725383
MAE train: 0.433178	val: 0.516175	test: 0.548569

Epoch: 51
Loss: 0.35568095992008847
RMSE train: 0.520014	val: 0.710272	test: 0.710951
MAE train: 0.405155	val: 0.510562	test: 0.527521

Epoch: 52
Loss: 0.35062409440676373
RMSE train: 0.552580	val: 0.735286	test: 0.733669
MAE train: 0.437426	val: 0.532844	test: 0.557605

Epoch: 53
Loss: 0.3520265966653824
RMSE train: 0.514127	val: 0.712728	test: 0.711143
MAE train: 0.399298	val: 0.514460	test: 0.530011

Epoch: 54
Loss: 0.3276001488169034
RMSE train: 0.501616	val: 0.701357	test: 0.696175
MAE train: 0.391951	val: 0.503349	test: 0.516380

Epoch: 55
Loss: 0.3486206109325091
RMSE train: 0.505138	val: 0.707252	test: 0.689011
MAE train: 0.396792	val: 0.510563	test: 0.514887

Epoch: 56
Loss: 0.3334234034021695
RMSE train: 0.517589	val: 0.717529	test: 0.707635
MAE train: 0.402962	val: 0.509739	test: 0.528638

Epoch: 57
Loss: 0.3342728614807129
RMSE train: 0.512298	val: 0.706148	test: 0.700141
MAE train: 0.400969	val: 0.509152	test: 0.520959

Epoch: 58
Loss: 0.3373989090323448
RMSE train: 0.498461	val: 0.700716	test: 0.692869
MAE train: 0.390137	val: 0.506384	test: 0.509642

Epoch: 59
Loss: 0.32210389773050946
RMSE train: 0.490930	val: 0.699242	test: 0.693235
MAE train: 0.384150	val: 0.502972	test: 0.511593

Epoch: 60
Loss: 0.3340378850698471
RMSE train: 0.484627	val: 0.690733	test: 0.690122
MAE train: 0.378890	val: 0.499257	test: 0.520294

Epoch: 61
Loss: 0.326001921047767
RMSE train: 0.524760	val: 0.718668	test: 0.720479
MAE train: 0.409621	val: 0.516533	test: 0.537158

Epoch: 62
Loss: 0.32443447907765705
RMSE train: 0.502206	val: 0.710982	test: 0.712253
MAE train: 0.395279	val: 0.511795	test: 0.528995

Epoch: 63
Loss: 0.3224176640311877
RMSE train: 0.519037	val: 0.729257	test: 0.714562
MAE train: 0.411669	val: 0.522388	test: 0.538592

Epoch: 64
Loss: 0.3223639751474063
RMSE train: 0.496998	val: 0.704039	test: 0.703017
MAE train: 0.387889	val: 0.510784	test: 0.520841

Epoch: 65
Loss: 0.3119424233833949
RMSE train: 0.497277	val: 0.699381	test: 0.709465
MAE train: 0.389786	val: 0.501497	test: 0.520885

Epoch: 66
Loss: 0.3158022612333298
RMSE train: 0.471537	val: 0.696458	test: 0.688046
MAE train: 0.368605	val: 0.501655	test: 0.511722

Epoch: 67
Loss: 0.3009282449881236
RMSE train: 0.474731	val: 0.693631	test: 0.698869
MAE train: 0.371456	val: 0.498143	test: 0.517212

Epoch: 68
Loss: 0.3019981582959493
RMSE train: 0.505324	val: 0.721715	test: 0.722542
MAE train: 0.396740	val: 0.519531	test: 0.532763

Epoch: 69
Loss: 0.3119785562157631
RMSE train: 0.461959	val: 0.694530	test: 0.690650
MAE train: 0.360876	val: 0.503674	test: 0.510352

Epoch: 70
Loss: 0.31052061294515926
RMSE train: 0.465509	val: 0.686282	test: 0.683415
MAE train: 0.365399	val: 0.493185	test: 0.508896

Epoch: 71
Loss: 0.32112619529167813
RMSE train: 0.510436	val: 0.718131	test: 0.725762
MAE train: 0.400464	val: 0.514528	test: 0.533454

Epoch: 72
Loss: 0.29755737135807675
RMSE train: 0.490957	val: 0.708080	test: 0.699031
MAE train: 0.386416	val: 0.507144	test: 0.517771

Epoch: 73
Loss: 0.304512694478035
RMSE train: 0.533992	val: 0.743604	test: 0.745298
MAE train: 0.422106	val: 0.542722	test: 0.563933

Epoch: 74
Loss: 0.2831834132472674
RMSE train: 0.487085	val: 0.716967	test: 0.714489
MAE train: 0.379737	val: 0.507414	test: 0.535619

Epoch: 75
Loss: 0.2797434901197751
RMSE train: 0.514811	val: 0.726801	test: 0.717570
MAE train: 0.409669	val: 0.528367	test: 0.544249

Epoch: 76
Loss: 0.28492048879464466
RMSE train: 0.481432	val: 0.704361	test: 0.701735
MAE train: 0.376014	val: 0.504073	test: 0.522315

Epoch: 77
Loss: 0.2768313102424145
RMSE train: 0.512331	val: 0.729857	test: 0.724345
MAE train: 0.406275	val: 0.525929	test: 0.550428

Epoch: 78
Loss: 0.2850487679243088
RMSE train: 0.509872	val: 0.739311	test: 0.726248
MAE train: 0.402938	val: 0.534257	test: 0.551272

Epoch: 79
Loss: 0.2825146404405435
RMSE train: 0.445141	val: 0.686244	test: 0.673050
MAE train: 0.347751	val: 0.483696	test: 0.498557

Epoch: 80
Loss: 0.2770106568932533
RMSE train: 0.494347	val: 0.716207	test: 0.724167
MAE train: 0.390107	val: 0.516849	test: 0.541949

Epoch: 81
Loss: 0.2894172978897889
RMSE train: 0.463787	val: 0.697052	test: 0.690454
MAE train: 0.362443	val: 0.503989	test: 0.510481

Epoch: 82
Loss: 0.28991495817899704
RMSE train: 0.502238	val: 0.717858	test: 0.712577
MAE train: 0.397485	val: 0.514342	test: 0.533712

Epoch: 83
Loss: 0.2798551457623641
RMSE train: 0.503499	val: 0.719535	test: 0.710208

Epoch: 23
Loss: 0.48494233936071396
RMSE train: 0.636835	val: 0.746407	test: 0.757549
MAE train: 0.501474	val: 0.574142	test: 0.588952

Epoch: 24
Loss: 0.48937763025363284
RMSE train: 0.609222	val: 0.734521	test: 0.723610
MAE train: 0.480346	val: 0.562921	test: 0.558389

Epoch: 25
Loss: 0.46658646563688916
RMSE train: 0.625715	val: 0.728529	test: 0.751469
MAE train: 0.484972	val: 0.537695	test: 0.574748

Epoch: 26
Loss: 0.46397223323583603
RMSE train: 0.597809	val: 0.717744	test: 0.715751
MAE train: 0.464279	val: 0.535746	test: 0.546460

Epoch: 27
Loss: 0.46221110721429187
RMSE train: 0.590157	val: 0.722190	test: 0.721783
MAE train: 0.459060	val: 0.540009	test: 0.551283

Epoch: 28
Loss: 0.4650256658593814
RMSE train: 0.611824	val: 0.738651	test: 0.733207
MAE train: 0.477162	val: 0.550922	test: 0.557515

Epoch: 29
Loss: 0.4332280382514
RMSE train: 0.582260	val: 0.709652	test: 0.718684
MAE train: 0.452207	val: 0.532209	test: 0.545705

Epoch: 30
Loss: 0.4372861882050832
RMSE train: 0.575249	val: 0.712892	test: 0.716925
MAE train: 0.450668	val: 0.537807	test: 0.547368

Epoch: 31
Loss: 0.4363812853892644
RMSE train: 0.572216	val: 0.706024	test: 0.714801
MAE train: 0.447289	val: 0.530426	test: 0.545187

Epoch: 32
Loss: 0.43780987213055295
RMSE train: 0.567570	val: 0.698696	test: 0.702548
MAE train: 0.442691	val: 0.520596	test: 0.537241

Epoch: 33
Loss: 0.42893898983796436
RMSE train: 0.570179	val: 0.706147	test: 0.722078
MAE train: 0.446522	val: 0.536474	test: 0.556118

Epoch: 34
Loss: 0.4138089567422867
RMSE train: 0.564890	val: 0.699490	test: 0.703863
MAE train: 0.439597	val: 0.519855	test: 0.534892

Epoch: 35
Loss: 0.4175982077916463
RMSE train: 0.587677	val: 0.726743	test: 0.725746
MAE train: 0.461111	val: 0.542104	test: 0.549401

Epoch: 36
Loss: 0.4183052157362302
RMSE train: 0.554453	val: 0.691192	test: 0.711042
MAE train: 0.430337	val: 0.511180	test: 0.543913

Epoch: 37
Loss: 0.42595743884642917
RMSE train: 0.550434	val: 0.698625	test: 0.699118
MAE train: 0.429315	val: 0.521824	test: 0.534139

Epoch: 38
Loss: 0.38907594482103985
RMSE train: 0.536645	val: 0.692497	test: 0.699613
MAE train: 0.414974	val: 0.509835	test: 0.524425

Epoch: 39
Loss: 0.4059568742911021
RMSE train: 0.575898	val: 0.707953	test: 0.718835
MAE train: 0.451320	val: 0.528964	test: 0.551027

Epoch: 40
Loss: 0.3852998564640681
RMSE train: 0.546924	val: 0.700327	test: 0.704404
MAE train: 0.426086	val: 0.517185	test: 0.532112

Epoch: 41
Loss: 0.38613763451576233
RMSE train: 0.531128	val: 0.684024	test: 0.697896
MAE train: 0.415244	val: 0.507369	test: 0.528061

Epoch: 42
Loss: 0.39674992114305496
RMSE train: 0.537964	val: 0.696068	test: 0.688289
MAE train: 0.419040	val: 0.524946	test: 0.526817

Epoch: 43
Loss: 0.37799133360385895
RMSE train: 0.571758	val: 0.707475	test: 0.712994
MAE train: 0.451205	val: 0.525836	test: 0.545985

Epoch: 44
Loss: 0.38895049691200256
RMSE train: 0.557162	val: 0.704398	test: 0.708522
MAE train: 0.436408	val: 0.531222	test: 0.542590

Epoch: 45
Loss: 0.3847266013423602
RMSE train: 0.529883	val: 0.698587	test: 0.704199
MAE train: 0.410794	val: 0.514140	test: 0.532422

Epoch: 46
Loss: 0.3548326964179675
RMSE train: 0.516099	val: 0.689208	test: 0.681208
MAE train: 0.401163	val: 0.504278	test: 0.511729

Epoch: 47
Loss: 0.35793599983056384
RMSE train: 0.547314	val: 0.690766	test: 0.697938
MAE train: 0.429515	val: 0.514123	test: 0.535765

Epoch: 48
Loss: 0.35618333766857785
RMSE train: 0.510458	val: 0.680663	test: 0.687923
MAE train: 0.396814	val: 0.499778	test: 0.516923

Epoch: 49
Loss: 0.3769156262278557
RMSE train: 0.528276	val: 0.692664	test: 0.691072
MAE train: 0.411987	val: 0.513166	test: 0.521104

Epoch: 50
Loss: 0.35137607405583066
RMSE train: 0.542521	val: 0.698148	test: 0.714135
MAE train: 0.422614	val: 0.515795	test: 0.540431

Epoch: 51
Loss: 0.3510857621828715
RMSE train: 0.523619	val: 0.687811	test: 0.681130
MAE train: 0.408211	val: 0.504041	test: 0.513228

Epoch: 52
Loss: 0.34541912873586017
RMSE train: 0.536890	val: 0.700113	test: 0.708709
MAE train: 0.416725	val: 0.515645	test: 0.538213

Epoch: 53
Loss: 0.34667443484067917
RMSE train: 0.537778	val: 0.689130	test: 0.694023
MAE train: 0.416729	val: 0.510145	test: 0.527138

Epoch: 54
Loss: 0.3524053518970807
RMSE train: 0.497716	val: 0.667260	test: 0.679863
MAE train: 0.386927	val: 0.488436	test: 0.513118

Epoch: 55
Loss: 0.3561973397930463
RMSE train: 0.517586	val: 0.677198	test: 0.688174
MAE train: 0.404421	val: 0.501040	test: 0.521917

Epoch: 56
Loss: 0.33813008666038513
RMSE train: 0.504132	val: 0.682150	test: 0.677310
MAE train: 0.392477	val: 0.496121	test: 0.512994

Epoch: 57
Loss: 0.34626534829537076
RMSE train: 0.505921	val: 0.691907	test: 0.685063
MAE train: 0.394451	val: 0.509312	test: 0.514080

Epoch: 58
Loss: 0.3284744769334793
RMSE train: 0.503830	val: 0.680785	test: 0.686421
MAE train: 0.395419	val: 0.494357	test: 0.515965

Epoch: 59
Loss: 0.32453234245379764
RMSE train: 0.524424	val: 0.698124	test: 0.698756
MAE train: 0.409051	val: 0.513913	test: 0.527732

Epoch: 60
Loss: 0.3351256176829338
RMSE train: 0.512017	val: 0.698248	test: 0.685494
MAE train: 0.400022	val: 0.514420	test: 0.510840

Epoch: 61
Loss: 0.32804004351298016
RMSE train: 0.498302	val: 0.679206	test: 0.677951
MAE train: 0.390517	val: 0.500800	test: 0.513161

Epoch: 62
Loss: 0.3191172182559967
RMSE train: 0.527626	val: 0.680504	test: 0.697497
MAE train: 0.414985	val: 0.500450	test: 0.534853

Epoch: 63
Loss: 0.3279946520924568
RMSE train: 0.503694	val: 0.691117	test: 0.677281
MAE train: 0.393699	val: 0.504992	test: 0.508416

Epoch: 64
Loss: 0.31670044114192325
RMSE train: 0.481734	val: 0.675012	test: 0.663987
MAE train: 0.376284	val: 0.491538	test: 0.502100

Epoch: 65
Loss: 0.3188524966438611
RMSE train: 0.508923	val: 0.681948	test: 0.699616
MAE train: 0.396727	val: 0.508426	test: 0.534978

Epoch: 66
Loss: 0.3169286921620369
RMSE train: 0.505668	val: 0.687620	test: 0.693602
MAE train: 0.396144	val: 0.507869	test: 0.516387

Epoch: 67
Loss: 0.31812674552202225
RMSE train: 0.476910	val: 0.661503	test: 0.669449
MAE train: 0.372242	val: 0.490535	test: 0.501782

Epoch: 68
Loss: 0.28996311873197556
RMSE train: 0.456411	val: 0.652738	test: 0.643807
MAE train: 0.355687	val: 0.480142	test: 0.484382

Epoch: 69
Loss: 0.3089012602965037
RMSE train: 0.501964	val: 0.685562	test: 0.683934
MAE train: 0.391176	val: 0.505913	test: 0.510869

Epoch: 70
Loss: 0.30529309312502545
RMSE train: 0.488382	val: 0.672671	test: 0.672590
MAE train: 0.382921	val: 0.495041	test: 0.505039

Epoch: 71
Loss: 0.31184450288613635
RMSE train: 0.540257	val: 0.716425	test: 0.710086
MAE train: 0.430328	val: 0.529850	test: 0.539500

Epoch: 72
Loss: 0.30076994250218075
RMSE train: 0.484731	val: 0.680281	test: 0.671071
MAE train: 0.379988	val: 0.507543	test: 0.507220

Epoch: 73
Loss: 0.3001330917080243
RMSE train: 0.527078	val: 0.695457	test: 0.684050
MAE train: 0.416381	val: 0.509649	test: 0.516209

Epoch: 74
Loss: 0.28849629188577336
RMSE train: 0.489845	val: 0.675280	test: 0.686283
MAE train: 0.381597	val: 0.498724	test: 0.514855

Epoch: 75
Loss: 0.29655783623456955
RMSE train: 0.474979	val: 0.650933	test: 0.649632
MAE train: 0.372090	val: 0.475584	test: 0.493271

Epoch: 76
Loss: 0.29370880623658496
RMSE train: 0.470068	val: 0.663853	test: 0.657298
MAE train: 0.366682	val: 0.483301	test: 0.496569

Epoch: 77
Loss: 0.29395675410827
RMSE train: 0.489046	val: 0.682187	test: 0.661197
MAE train: 0.384112	val: 0.498570	test: 0.498858

Epoch: 78
Loss: 0.30286184201637906
RMSE train: 0.512598	val: 0.703382	test: 0.705558
MAE train: 0.402323	val: 0.521987	test: 0.534812

Epoch: 79
Loss: 0.29612214118242264
RMSE train: 0.489639	val: 0.679502	test: 0.679454
MAE train: 0.383075	val: 0.501038	test: 0.512596

Epoch: 80
Loss: 0.28824519490202266
RMSE train: 0.459741	val: 0.654649	test: 0.659811
MAE train: 0.358893	val: 0.478568	test: 0.496569

Epoch: 81
Loss: 0.29592664539813995
RMSE train: 0.453477	val: 0.664134	test: 0.673587
MAE train: 0.350900	val: 0.484081	test: 0.501908

Epoch: 82
Loss: 0.2741931987305482
RMSE train: 0.462668	val: 0.656979	test: 0.665703
MAE train: 0.361338	val: 0.485958	test: 0.506253

Epoch: 83
Loss: 0.2869838662445545
RMSE train: 0.470471	val: 0.665490	test: 0.664594

Epoch: 23
Loss: 0.4829726169506709
RMSE train: 0.622090	val: 0.739105	test: 0.720788
MAE train: 0.485356	val: 0.556695	test: 0.559601

Epoch: 24
Loss: 0.49826260656118393
RMSE train: 0.609169	val: 0.736163	test: 0.719576
MAE train: 0.473027	val: 0.556705	test: 0.555581

Epoch: 25
Loss: 0.4964378625154495
RMSE train: 0.608899	val: 0.727389	test: 0.707636
MAE train: 0.471723	val: 0.540851	test: 0.549493

Epoch: 26
Loss: 0.4584434578816096
RMSE train: 0.592175	val: 0.721125	test: 0.700324
MAE train: 0.461942	val: 0.542454	test: 0.544740

Epoch: 27
Loss: 0.45243435601393384
RMSE train: 0.602858	val: 0.726382	test: 0.699606
MAE train: 0.465941	val: 0.541703	test: 0.542313

Epoch: 28
Loss: 0.4726157958308856
RMSE train: 0.608901	val: 0.723034	test: 0.710691
MAE train: 0.473200	val: 0.544905	test: 0.556692

Epoch: 29
Loss: 0.45045796781778336
RMSE train: 0.590643	val: 0.728137	test: 0.700963
MAE train: 0.456043	val: 0.542851	test: 0.540523

Epoch: 30
Loss: 0.44272247701883316
RMSE train: 0.625900	val: 0.737071	test: 0.722665
MAE train: 0.491044	val: 0.556727	test: 0.566754

Epoch: 31
Loss: 0.43505995472272235
RMSE train: 0.585248	val: 0.714945	test: 0.706813
MAE train: 0.453730	val: 0.533495	test: 0.547167

Epoch: 32
Loss: 0.43260227888822556
RMSE train: 0.577691	val: 0.714698	test: 0.689489
MAE train: 0.451428	val: 0.539087	test: 0.538355

Epoch: 33
Loss: 0.43575766930977505
RMSE train: 0.585391	val: 0.725165	test: 0.703125
MAE train: 0.454523	val: 0.547179	test: 0.548846

Epoch: 34
Loss: 0.4092496285835902
RMSE train: 0.562873	val: 0.703552	test: 0.689508
MAE train: 0.440014	val: 0.527868	test: 0.535215

Epoch: 35
Loss: 0.40953438977400464
RMSE train: 0.614417	val: 0.745678	test: 0.724094
MAE train: 0.482411	val: 0.570661	test: 0.566926

Epoch: 36
Loss: 0.43571506688992184
RMSE train: 0.579627	val: 0.734405	test: 0.712874
MAE train: 0.451158	val: 0.548481	test: 0.550576

Epoch: 37
Loss: 0.4065300722916921
RMSE train: 0.570189	val: 0.708813	test: 0.692690
MAE train: 0.443901	val: 0.528690	test: 0.535933

Epoch: 38
Loss: 0.41612791270017624
RMSE train: 0.601941	val: 0.723062	test: 0.729416
MAE train: 0.467030	val: 0.543423	test: 0.561843

Epoch: 39
Loss: 0.4108404243985812
RMSE train: 0.573308	val: 0.721521	test: 0.705346
MAE train: 0.446319	val: 0.548101	test: 0.543855

Epoch: 40
Loss: 0.38612230122089386
RMSE train: 0.595540	val: 0.729351	test: 0.720194
MAE train: 0.467547	val: 0.545915	test: 0.559914

Epoch: 41
Loss: 0.39213188737630844
RMSE train: 0.536769	val: 0.698301	test: 0.681122
MAE train: 0.419743	val: 0.517220	test: 0.522216

Epoch: 42
Loss: 0.3951385368903478
RMSE train: 0.540335	val: 0.701098	test: 0.679104
MAE train: 0.422602	val: 0.523396	test: 0.516376

Epoch: 43
Loss: 0.3806094055374463
RMSE train: 0.547848	val: 0.695598	test: 0.696190
MAE train: 0.425772	val: 0.516249	test: 0.530983

Epoch: 44
Loss: 0.3880092849334081
RMSE train: 0.549149	val: 0.684897	test: 0.679798
MAE train: 0.428744	val: 0.504787	test: 0.522450

Epoch: 45
Loss: 0.3935625031590462
RMSE train: 0.587384	val: 0.729320	test: 0.724263
MAE train: 0.460581	val: 0.546091	test: 0.557036

Epoch: 46
Loss: 0.37667902559041977
RMSE train: 0.541031	val: 0.697089	test: 0.687490
MAE train: 0.424593	val: 0.518815	test: 0.530278

Epoch: 47
Loss: 0.371652215719223
RMSE train: 0.533636	val: 0.705233	test: 0.689944
MAE train: 0.415920	val: 0.524528	test: 0.530862

Epoch: 48
Loss: 0.3533647879958153
RMSE train: 0.548015	val: 0.681566	test: 0.686091
MAE train: 0.428901	val: 0.504303	test: 0.527234

Epoch: 49
Loss: 0.35082447280486423
RMSE train: 0.559355	val: 0.715011	test: 0.716710
MAE train: 0.435562	val: 0.533358	test: 0.550356

Epoch: 50
Loss: 0.34954408059517544
RMSE train: 0.503841	val: 0.681133	test: 0.659970
MAE train: 0.392308	val: 0.494835	test: 0.501981

Epoch: 51
Loss: 0.3591528609395027
RMSE train: 0.523181	val: 0.699683	test: 0.682039
MAE train: 0.408175	val: 0.516245	test: 0.524870

Epoch: 52
Loss: 0.3403947229186694
RMSE train: 0.544430	val: 0.704408	test: 0.698516
MAE train: 0.424505	val: 0.520180	test: 0.532347

Epoch: 53
Loss: 0.36432137837012607
RMSE train: 0.534642	val: 0.700404	test: 0.694101
MAE train: 0.421684	val: 0.518978	test: 0.533609

Epoch: 54
Loss: 0.3625018522143364
RMSE train: 0.528762	val: 0.682766	test: 0.686682
MAE train: 0.411188	val: 0.507075	test: 0.526002

Epoch: 55
Loss: 0.34709760546684265
RMSE train: 0.514530	val: 0.694040	test: 0.677863
MAE train: 0.401017	val: 0.509082	test: 0.522290

Epoch: 56
Loss: 0.3366132800777753
RMSE train: 0.521122	val: 0.704311	test: 0.689307
MAE train: 0.405510	val: 0.520767	test: 0.526692

Epoch: 57
Loss: 0.33289871613184613
RMSE train: 0.576964	val: 0.743827	test: 0.732749
MAE train: 0.455002	val: 0.553061	test: 0.564495

Epoch: 58
Loss: 0.3407363047202428
RMSE train: 0.567538	val: 0.719590	test: 0.711936
MAE train: 0.446303	val: 0.532483	test: 0.546967

Epoch: 59
Loss: 0.3238495687643687
RMSE train: 0.516347	val: 0.693021	test: 0.690211
MAE train: 0.402564	val: 0.509571	test: 0.523271

Epoch: 60
Loss: 0.31951995690663654
RMSE train: 0.524699	val: 0.710301	test: 0.705811
MAE train: 0.406033	val: 0.518665	test: 0.531670

Epoch: 61
Loss: 0.32538090149561566
RMSE train: 0.540860	val: 0.714328	test: 0.704460
MAE train: 0.424912	val: 0.525179	test: 0.541885

Epoch: 62
Loss: 0.31434376041094464
RMSE train: 0.498686	val: 0.682282	test: 0.677165
MAE train: 0.390566	val: 0.493474	test: 0.516918

Epoch: 63
Loss: 0.32414960612853366
RMSE train: 0.528290	val: 0.689949	test: 0.700303
MAE train: 0.414594	val: 0.504173	test: 0.530327

Epoch: 64
Loss: 0.30530966569979984
RMSE train: 0.498653	val: 0.683214	test: 0.685844
MAE train: 0.387244	val: 0.499406	test: 0.515268

Epoch: 65
Loss: 0.3267295112212499
RMSE train: 0.512871	val: 0.684419	test: 0.684995
MAE train: 0.399359	val: 0.497803	test: 0.517425

Epoch: 66
Loss: 0.3215307866533597
RMSE train: 0.495003	val: 0.691832	test: 0.680031
MAE train: 0.384836	val: 0.502799	test: 0.511956

Epoch: 67
Loss: 0.3231071134408315
RMSE train: 0.487307	val: 0.684426	test: 0.673585
MAE train: 0.377895	val: 0.496324	test: 0.506584

Epoch: 68
Loss: 0.31862935175498325
RMSE train: 0.523929	val: 0.695902	test: 0.697663
MAE train: 0.408971	val: 0.510249	test: 0.531927

Epoch: 69
Loss: 0.30879434446493786
RMSE train: 0.490739	val: 0.690233	test: 0.673583
MAE train: 0.378204	val: 0.501222	test: 0.506851

Epoch: 70
Loss: 0.3083832760651906
RMSE train: 0.485560	val: 0.677646	test: 0.678198
MAE train: 0.378066	val: 0.496200	test: 0.513728

Epoch: 71
Loss: 0.2958931401371956
RMSE train: 0.502376	val: 0.684260	test: 0.692281
MAE train: 0.389012	val: 0.494641	test: 0.524797

Epoch: 72
Loss: 0.30175145839651424
RMSE train: 0.557572	val: 0.747058	test: 0.740770
MAE train: 0.443369	val: 0.552288	test: 0.575422

Epoch: 73
Loss: 0.2847822109858195
RMSE train: 0.483051	val: 0.689449	test: 0.669961
MAE train: 0.376987	val: 0.497555	test: 0.509770

Epoch: 74
Loss: 0.2952284390727679
RMSE train: 0.489557	val: 0.700861	test: 0.686524
MAE train: 0.385186	val: 0.518552	test: 0.527958

Epoch: 75
Loss: 0.28470250343283016
RMSE train: 0.538749	val: 0.720254	test: 0.713392
MAE train: 0.424824	val: 0.533771	test: 0.548592

Epoch: 76
Loss: 0.30063584198554355
RMSE train: 0.533797	val: 0.723167	test: 0.705475
MAE train: 0.422488	val: 0.536657	test: 0.538926

Epoch: 77
Loss: 0.3188561474283536
RMSE train: 0.501939	val: 0.703095	test: 0.698810
MAE train: 0.393873	val: 0.513736	test: 0.528718

Epoch: 78
Loss: 0.28145646055539447
RMSE train: 0.512823	val: 0.705092	test: 0.694300
MAE train: 0.397085	val: 0.521287	test: 0.527566

Epoch: 79
Loss: 0.297467689961195
RMSE train: 0.539434	val: 0.730481	test: 0.707920
MAE train: 0.428133	val: 0.548772	test: 0.539635

Epoch: 80
Loss: 0.27872658769289654
RMSE train: 0.483502	val: 0.683387	test: 0.685112
MAE train: 0.376719	val: 0.498805	test: 0.515585

Epoch: 81
Loss: 0.2784424163401127
RMSE train: 0.471646	val: 0.689682	test: 0.674961
MAE train: 0.367965	val: 0.509582	test: 0.513489

Epoch: 82
Loss: 0.2765578143298626
RMSE train: 0.476085	val: 0.681166	test: 0.681898
MAE train: 0.366955	val: 0.491588	test: 0.516893

Epoch: 83
Loss: 0.2916013722618421
RMSE train: 0.496945	val: 0.710867	test: 0.694819

Epoch: 23
Loss: 0.49405074545315336
RMSE train: 0.621990	val: 0.772842	test: 0.726262
MAE train: 0.485770	val: 0.595213	test: 0.548114

Epoch: 24
Loss: 0.4961923914296286
RMSE train: 0.640777	val: 0.804524	test: 0.758454
MAE train: 0.494123	val: 0.624650	test: 0.572468

Epoch: 25
Loss: 0.4961751912321363
RMSE train: 0.613775	val: 0.776977	test: 0.731913
MAE train: 0.477329	val: 0.597709	test: 0.549009

Epoch: 26
Loss: 0.4743708883013044
RMSE train: 0.615595	val: 0.781164	test: 0.745001
MAE train: 0.481925	val: 0.601178	test: 0.564538

Epoch: 27
Loss: 0.49065579048224855
RMSE train: 0.628268	val: 0.778599	test: 0.741510
MAE train: 0.490797	val: 0.594147	test: 0.569255

Epoch: 28
Loss: 0.47721451946667265
RMSE train: 0.592319	val: 0.764700	test: 0.721430
MAE train: 0.458102	val: 0.588933	test: 0.540877

Epoch: 29
Loss: 0.4710533320903778
RMSE train: 0.591450	val: 0.765338	test: 0.714127
MAE train: 0.456893	val: 0.587378	test: 0.527127

Epoch: 30
Loss: 0.46258179204804556
RMSE train: 0.612964	val: 0.770111	test: 0.732509
MAE train: 0.480855	val: 0.597792	test: 0.555580

Epoch: 31
Loss: 0.462303353207452
RMSE train: 0.598134	val: 0.766679	test: 0.729300
MAE train: 0.466598	val: 0.586852	test: 0.551407

Epoch: 32
Loss: 0.471469270331519
RMSE train: 0.569586	val: 0.741609	test: 0.719530
MAE train: 0.445592	val: 0.566125	test: 0.534067

Epoch: 33
Loss: 0.48931100964546204
RMSE train: 0.595745	val: 0.759896	test: 0.725652
MAE train: 0.467341	val: 0.582593	test: 0.548157

Epoch: 34
Loss: 0.5010727154357093
RMSE train: 0.610581	val: 0.771974	test: 0.730876
MAE train: 0.474072	val: 0.587050	test: 0.542957

Epoch: 35
Loss: 0.4534930884838104
RMSE train: 0.599096	val: 0.768288	test: 0.722286
MAE train: 0.464031	val: 0.581805	test: 0.539309

Epoch: 36
Loss: 0.43853896856307983
RMSE train: 0.599929	val: 0.772132	test: 0.733985
MAE train: 0.466031	val: 0.599528	test: 0.549761

Epoch: 37
Loss: 0.43143161279814585
RMSE train: 0.570665	val: 0.760228	test: 0.698013
MAE train: 0.442657	val: 0.572485	test: 0.530918

Epoch: 38
Loss: 0.43599227922303335
RMSE train: 0.561606	val: 0.743730	test: 0.696926
MAE train: 0.434650	val: 0.559911	test: 0.521212

Epoch: 39
Loss: 0.41679722496441435
RMSE train: 0.557288	val: 0.733035	test: 0.693096
MAE train: 0.431584	val: 0.558276	test: 0.517680

Epoch: 40
Loss: 0.4121236673423222
RMSE train: 0.572138	val: 0.741969	test: 0.689248
MAE train: 0.440732	val: 0.558557	test: 0.512705

Epoch: 41
Loss: 0.4156138279608318
RMSE train: 0.563751	val: 0.743645	test: 0.686303
MAE train: 0.432829	val: 0.562408	test: 0.509783

Epoch: 42
Loss: 0.41681910838399616
RMSE train: 0.554863	val: 0.728649	test: 0.701803
MAE train: 0.436722	val: 0.550126	test: 0.521623

Epoch: 43
Loss: 0.41386241359370096
RMSE train: 0.574874	val: 0.761566	test: 0.720624
MAE train: 0.447990	val: 0.581865	test: 0.536902

Epoch: 44
Loss: 0.4069157200200217
RMSE train: 0.562313	val: 0.754072	test: 0.689757
MAE train: 0.433330	val: 0.567243	test: 0.515768

Epoch: 45
Loss: 0.3844317751271384
RMSE train: 0.539068	val: 0.721539	test: 0.683444
MAE train: 0.419273	val: 0.549460	test: 0.508821

Epoch: 46
Loss: 0.4163550138473511
RMSE train: 0.538029	val: 0.731921	test: 0.686037
MAE train: 0.421365	val: 0.557630	test: 0.506353

Epoch: 47
Loss: 0.40435592191559927
RMSE train: 0.559927	val: 0.751066	test: 0.698065
MAE train: 0.437114	val: 0.574170	test: 0.522312

Epoch: 48
Loss: 0.3813286913292749
RMSE train: 0.567366	val: 0.747562	test: 0.707839
MAE train: 0.438554	val: 0.560455	test: 0.522086

Epoch: 49
Loss: 0.3864686850990568
RMSE train: 0.573931	val: 0.745578	test: 0.695188
MAE train: 0.445735	val: 0.555886	test: 0.517546

Epoch: 50
Loss: 0.3565079942345619
RMSE train: 0.568767	val: 0.753080	test: 0.711028
MAE train: 0.440399	val: 0.576043	test: 0.531323

Epoch: 51
Loss: 0.3639166887317385
RMSE train: 0.527077	val: 0.709412	test: 0.665636
MAE train: 0.406164	val: 0.533117	test: 0.492525

Epoch: 52
Loss: 0.3813527375459671
RMSE train: 0.536866	val: 0.739016	test: 0.679709
MAE train: 0.419166	val: 0.556578	test: 0.501908

Epoch: 53
Loss: 0.3662524138178144
RMSE train: 0.543911	val: 0.740170	test: 0.699919
MAE train: 0.424505	val: 0.563204	test: 0.526341

Epoch: 54
Loss: 0.38460944167205263
RMSE train: 0.540788	val: 0.728168	test: 0.676830
MAE train: 0.419492	val: 0.549806	test: 0.500596

Epoch: 55
Loss: 0.3558687780584608
RMSE train: 0.513794	val: 0.723112	test: 0.664245
MAE train: 0.400375	val: 0.544673	test: 0.488563

Epoch: 56
Loss: 0.37027388385363985
RMSE train: 0.517323	val: 0.719612	test: 0.669922
MAE train: 0.402156	val: 0.536862	test: 0.495270

Epoch: 57
Loss: 0.3668142855167389
RMSE train: 0.547038	val: 0.730338	test: 0.691792
MAE train: 0.430218	val: 0.552843	test: 0.523666

Epoch: 58
Loss: 0.3797522165945598
RMSE train: 0.527932	val: 0.723726	test: 0.689746
MAE train: 0.411453	val: 0.552304	test: 0.518578

Epoch: 59
Loss: 0.35259371463741573
RMSE train: 0.543088	val: 0.740959	test: 0.703005
MAE train: 0.419414	val: 0.557590	test: 0.524032

Epoch: 60
Loss: 0.35217731978212086
RMSE train: 0.553360	val: 0.753544	test: 0.701290
MAE train: 0.430686	val: 0.563255	test: 0.528427

Epoch: 61
Loss: 0.3531710548060281
RMSE train: 0.525310	val: 0.732736	test: 0.681952
MAE train: 0.405607	val: 0.550385	test: 0.505815

Epoch: 62
Loss: 0.3499916046857834
RMSE train: 0.518673	val: 0.708913	test: 0.677939
MAE train: 0.405910	val: 0.545645	test: 0.511423

Epoch: 63
Loss: 0.3690512031316757
RMSE train: 0.517527	val: 0.728717	test: 0.683842
MAE train: 0.402405	val: 0.556285	test: 0.510224

Epoch: 64
Loss: 0.3556057150874819
RMSE train: 0.540355	val: 0.739266	test: 0.696223
MAE train: 0.417739	val: 0.553479	test: 0.526840

Epoch: 65
Loss: 0.34248324802943636
RMSE train: 0.516683	val: 0.714984	test: 0.672889
MAE train: 0.400288	val: 0.534321	test: 0.501572

Epoch: 66
Loss: 0.3368109954254968
RMSE train: 0.513826	val: 0.737086	test: 0.690018
MAE train: 0.399620	val: 0.547018	test: 0.506817

Epoch: 67
Loss: 0.34070362576416563
RMSE train: 0.502581	val: 0.712917	test: 0.679883
MAE train: 0.391739	val: 0.541823	test: 0.505442

Epoch: 68
Loss: 0.3427490293979645
RMSE train: 0.507163	val: 0.720012	test: 0.668741
MAE train: 0.393662	val: 0.534063	test: 0.498286

Epoch: 69
Loss: 0.3354578507798059
RMSE train: 0.508668	val: 0.703806	test: 0.670007
MAE train: 0.393716	val: 0.523011	test: 0.499674

Epoch: 70
Loss: 0.33585784477846964
RMSE train: 0.542114	val: 0.743612	test: 0.680823
MAE train: 0.420754	val: 0.556802	test: 0.504475

Epoch: 71
Loss: 0.3291188542331968
RMSE train: 0.504961	val: 0.720873	test: 0.660379
MAE train: 0.390097	val: 0.539874	test: 0.492467

Epoch: 72
Loss: 0.33166447068963734
RMSE train: 0.489635	val: 0.711334	test: 0.667055
MAE train: 0.379559	val: 0.528770	test: 0.495627

Epoch: 73
Loss: 0.32572861654417856
RMSE train: 0.498783	val: 0.723373	test: 0.670638
MAE train: 0.384852	val: 0.544230	test: 0.496116

Epoch: 74
Loss: 0.3283349701336452
RMSE train: 0.508886	val: 0.713466	test: 0.681816
MAE train: 0.397036	val: 0.536884	test: 0.506074

Epoch: 75
Loss: 0.3415248159851347
RMSE train: 0.491141	val: 0.698763	test: 0.660399
MAE train: 0.382025	val: 0.518665	test: 0.483675

Epoch: 76
Loss: 0.3238459655216762
RMSE train: 0.488095	val: 0.717409	test: 0.651729
MAE train: 0.376530	val: 0.536520	test: 0.479422

Epoch: 77
Loss: 0.32838734132902964
RMSE train: 0.484351	val: 0.699537	test: 0.651882
MAE train: 0.376387	val: 0.523307	test: 0.483137

Epoch: 78
Loss: 0.31146120173590525
RMSE train: 0.474473	val: 0.692795	test: 0.659546
MAE train: 0.368904	val: 0.520823	test: 0.488306

Epoch: 79
Loss: 0.3251301092760904
RMSE train: 0.497731	val: 0.706363	test: 0.663143
MAE train: 0.389707	val: 0.531003	test: 0.502209

Epoch: 80
Loss: 0.304092104945864
RMSE train: 0.502476	val: 0.725054	test: 0.669890
MAE train: 0.390969	val: 0.539692	test: 0.500043

Epoch: 81
Loss: 0.31405780570847647
RMSE train: 0.501928	val: 0.720592	test: 0.662574
MAE train: 0.387860	val: 0.538452	test: 0.493954

Epoch: 82
Loss: 0.3123191339629037
RMSE train: 0.478831	val: 0.702812	test: 0.664854
MAE train: 0.370509	val: 0.524426	test: 0.489937

Epoch: 83
Loss: 0.3032101360814912
RMSE train: 0.482682	val: 0.714294	test: 0.674200

Epoch: 23
Loss: 0.4879448924745832
RMSE train: 0.684537	val: 0.855691	test: 0.774148
MAE train: 0.530370	val: 0.651163	test: 0.584701

Epoch: 24
Loss: 0.5224740632942745
RMSE train: 0.641288	val: 0.802696	test: 0.733754
MAE train: 0.497020	val: 0.622667	test: 0.557207

Epoch: 25
Loss: 0.5098229220935276
RMSE train: 0.633562	val: 0.780321	test: 0.731239
MAE train: 0.501953	val: 0.610361	test: 0.572517

Epoch: 26
Loss: 0.4785475219999041
RMSE train: 0.626084	val: 0.807963	test: 0.730751
MAE train: 0.484878	val: 0.618266	test: 0.540496

Epoch: 27
Loss: 0.4682235079152243
RMSE train: 0.630319	val: 0.787656	test: 0.714854
MAE train: 0.485292	val: 0.607015	test: 0.546270

Epoch: 28
Loss: 0.484285620706422
RMSE train: 0.644180	val: 0.809783	test: 0.747491
MAE train: 0.497209	val: 0.617507	test: 0.560976

Epoch: 29
Loss: 0.4951153503996985
RMSE train: 0.713390	val: 0.853483	test: 0.815240
MAE train: 0.553958	val: 0.666190	test: 0.621260

Epoch: 30
Loss: 0.4661379264933722
RMSE train: 0.641551	val: 0.831012	test: 0.725369
MAE train: 0.499472	val: 0.628284	test: 0.553731

Epoch: 31
Loss: 0.45497143481458935
RMSE train: 0.625447	val: 0.804975	test: 0.748718
MAE train: 0.485025	val: 0.614430	test: 0.563234

Epoch: 32
Loss: 0.4460394765649523
RMSE train: 0.592130	val: 0.763963	test: 0.698682
MAE train: 0.460104	val: 0.581241	test: 0.535161

Epoch: 33
Loss: 0.43991572516305105
RMSE train: 0.627590	val: 0.797234	test: 0.738978
MAE train: 0.483626	val: 0.610986	test: 0.558873

Epoch: 34
Loss: 0.44895877156938824
RMSE train: 0.612537	val: 0.792548	test: 0.717253
MAE train: 0.474187	val: 0.609679	test: 0.538333

Epoch: 35
Loss: 0.44255636845316204
RMSE train: 0.579592	val: 0.758895	test: 0.712399
MAE train: 0.448134	val: 0.575410	test: 0.539719

Epoch: 36
Loss: 0.42580749094486237
RMSE train: 0.613343	val: 0.787865	test: 0.705370
MAE train: 0.476880	val: 0.596238	test: 0.532414

Epoch: 37
Loss: 0.4232093947274344
RMSE train: 0.570652	val: 0.762858	test: 0.694185
MAE train: 0.445124	val: 0.584619	test: 0.529419

Epoch: 38
Loss: 0.42228606769016813
RMSE train: 0.568271	val: 0.769202	test: 0.698238
MAE train: 0.436144	val: 0.581910	test: 0.522213

Epoch: 39
Loss: 0.42854941317013334
RMSE train: 0.595760	val: 0.769739	test: 0.694108
MAE train: 0.461392	val: 0.583378	test: 0.526538

Epoch: 40
Loss: 0.4279642892735345
RMSE train: 0.617602	val: 0.790796	test: 0.721294
MAE train: 0.482045	val: 0.603691	test: 0.541908

Epoch: 41
Loss: 0.42588432346071514
RMSE train: 0.593430	val: 0.780668	test: 0.692827
MAE train: 0.460040	val: 0.585601	test: 0.523401

Epoch: 42
Loss: 0.40155584045818876
RMSE train: 0.552603	val: 0.764412	test: 0.668704
MAE train: 0.427795	val: 0.577906	test: 0.508746

Epoch: 43
Loss: 0.3878125250339508
RMSE train: 0.543064	val: 0.746631	test: 0.670137
MAE train: 0.419939	val: 0.562294	test: 0.511045

Epoch: 44
Loss: 0.41012528964451384
RMSE train: 0.619102	val: 0.803409	test: 0.736748
MAE train: 0.487277	val: 0.613484	test: 0.565241

Epoch: 45
Loss: 0.44190281203814913
RMSE train: 0.573809	val: 0.759695	test: 0.676307
MAE train: 0.444423	val: 0.574898	test: 0.520110

Epoch: 46
Loss: 0.38283310191971914
RMSE train: 0.585146	val: 0.787248	test: 0.711917
MAE train: 0.454152	val: 0.589532	test: 0.533855

Epoch: 47
Loss: 0.37289322699819294
RMSE train: 0.550476	val: 0.757786	test: 0.672221
MAE train: 0.427597	val: 0.565306	test: 0.509714

Epoch: 48
Loss: 0.4009549617767334
RMSE train: 0.600533	val: 0.798552	test: 0.717615
MAE train: 0.471002	val: 0.601027	test: 0.541375

Epoch: 49
Loss: 0.39584799323763165
RMSE train: 0.546759	val: 0.751594	test: 0.657190
MAE train: 0.422349	val: 0.558094	test: 0.495111

Epoch: 50
Loss: 0.39385335786002024
RMSE train: 0.573239	val: 0.771373	test: 0.695623
MAE train: 0.444670	val: 0.584713	test: 0.523517

Epoch: 51
Loss: 0.37738156744412016
RMSE train: 0.594928	val: 0.772650	test: 0.699793
MAE train: 0.464838	val: 0.585511	test: 0.531872

Epoch: 52
Loss: 0.36911616793700625
RMSE train: 0.527826	val: 0.734678	test: 0.651635
MAE train: 0.408513	val: 0.546766	test: 0.497313

Epoch: 53
Loss: 0.36609290540218353
RMSE train: 0.657153	val: 0.845722	test: 0.773046
MAE train: 0.519097	val: 0.656707	test: 0.588525

Epoch: 54
Loss: 0.37357152359826223
RMSE train: 0.573679	val: 0.764981	test: 0.677771
MAE train: 0.446515	val: 0.574186	test: 0.513117

Epoch: 55
Loss: 0.3859258357967649
RMSE train: 0.565817	val: 0.752546	test: 0.708612
MAE train: 0.437158	val: 0.570637	test: 0.534544

Epoch: 56
Loss: 0.36899263731070925
RMSE train: 0.543480	val: 0.766264	test: 0.668996
MAE train: 0.423268	val: 0.569861	test: 0.502195

Epoch: 57
Loss: 0.3626186741249902
RMSE train: 0.551786	val: 0.763163	test: 0.691517
MAE train: 0.427841	val: 0.564348	test: 0.522706

Epoch: 58
Loss: 0.3377452939748764
RMSE train: 0.537791	val: 0.747457	test: 0.660114
MAE train: 0.416424	val: 0.561467	test: 0.496428

Epoch: 59
Loss: 0.3570401391812733
RMSE train: 0.520561	val: 0.736586	test: 0.647219
MAE train: 0.402638	val: 0.540094	test: 0.485671

Epoch: 60
Loss: 0.3635685976062502
RMSE train: 0.536303	val: 0.758109	test: 0.668663
MAE train: 0.416216	val: 0.565707	test: 0.501844

Epoch: 61
Loss: 0.34551355455602917
RMSE train: 0.503934	val: 0.723339	test: 0.638815
MAE train: 0.389556	val: 0.544070	test: 0.479103

Epoch: 62
Loss: 0.3577504945652826
RMSE train: 0.556777	val: 0.751051	test: 0.690250
MAE train: 0.433541	val: 0.574331	test: 0.526394

Epoch: 63
Loss: 0.34443368869168417
RMSE train: 0.521924	val: 0.728851	test: 0.644331
MAE train: 0.401759	val: 0.546044	test: 0.484769

Epoch: 64
Loss: 0.34075030258723665
RMSE train: 0.511148	val: 0.734767	test: 0.651997
MAE train: 0.392604	val: 0.553228	test: 0.488384

Epoch: 65
Loss: 0.32363938433783396
RMSE train: 0.566228	val: 0.768904	test: 0.702698
MAE train: 0.444452	val: 0.578871	test: 0.530852

Epoch: 66
Loss: 0.3416638438190733
RMSE train: 0.532148	val: 0.759664	test: 0.667319
MAE train: 0.412578	val: 0.569384	test: 0.508124

Epoch: 67
Loss: 0.3288075838770185
RMSE train: 0.503433	val: 0.716689	test: 0.647344
MAE train: 0.393635	val: 0.542872	test: 0.498546

Epoch: 68
Loss: 0.36355977399008615
RMSE train: 0.554593	val: 0.770567	test: 0.693535
MAE train: 0.431706	val: 0.578792	test: 0.521515

Epoch: 69
Loss: 0.34766252764633726
RMSE train: 0.536075	val: 0.740104	test: 0.673801
MAE train: 0.414235	val: 0.551527	test: 0.505298

Epoch: 70
Loss: 0.32904651548181263
RMSE train: 0.496574	val: 0.727836	test: 0.658763
MAE train: 0.385079	val: 0.536332	test: 0.495591

Epoch: 71
Loss: 0.3287562280893326
RMSE train: 0.547059	val: 0.773845	test: 0.691096
MAE train: 0.431296	val: 0.586939	test: 0.524166

Epoch: 72
Loss: 0.313340568116733
RMSE train: 0.536706	val: 0.763850	test: 0.676010
MAE train: 0.420039	val: 0.578487	test: 0.510662

Epoch: 73
Loss: 0.31336080921547754
RMSE train: 0.540515	val: 0.748212	test: 0.684824
MAE train: 0.420264	val: 0.566111	test: 0.521545

Epoch: 74
Loss: 0.32184326222964693
RMSE train: 0.481454	val: 0.717242	test: 0.639198
MAE train: 0.372131	val: 0.532617	test: 0.476994

Epoch: 75
Loss: 0.3212860184056418
RMSE train: 0.529315	val: 0.734504	test: 0.670399
MAE train: 0.409371	val: 0.547304	test: 0.499977

Epoch: 76
Loss: 0.3048455651317324
RMSE train: 0.564977	val: 0.790766	test: 0.677774
MAE train: 0.446514	val: 0.595798	test: 0.520871

Epoch: 77
Loss: 0.3249660623925073
RMSE train: 0.507247	val: 0.745078	test: 0.656319
MAE train: 0.390705	val: 0.550789	test: 0.496399

Epoch: 78
Loss: 0.3378835256610598
RMSE train: 0.556805	val: 0.768123	test: 0.678842
MAE train: 0.438348	val: 0.577739	test: 0.508699

Epoch: 79
Loss: 0.31279354116746355
RMSE train: 0.480737	val: 0.719550	test: 0.638161
MAE train: 0.371907	val: 0.531674	test: 0.473039

Epoch: 80
Loss: 0.31784260060106007
RMSE train: 0.519804	val: 0.748534	test: 0.661835
MAE train: 0.401750	val: 0.558369	test: 0.497670

Epoch: 81
Loss: 0.3054082840681076
RMSE train: 0.506182	val: 0.743200	test: 0.652104
MAE train: 0.390257	val: 0.557467	test: 0.489209

Epoch: 82
Loss: 0.3169902190566063
RMSE train: 0.481135	val: 0.718315	test: 0.619006
MAE train: 0.373340	val: 0.526969	test: 0.461390

Epoch: 83
Loss: 0.30558573454618454
RMSE train: 0.530275	val: 0.754929	test: 0.678825

Epoch: 23
Loss: 0.5164038432495934
RMSE train: 0.628285	val: 0.773597	test: 0.722310
MAE train: 0.485047	val: 0.598871	test: 0.548819

Epoch: 24
Loss: 0.515362486243248
RMSE train: 0.657614	val: 0.809578	test: 0.751020
MAE train: 0.513124	val: 0.635075	test: 0.577635

Epoch: 25
Loss: 0.5172249142612729
RMSE train: 0.674258	val: 0.813185	test: 0.735229
MAE train: 0.526410	val: 0.631114	test: 0.559560

Epoch: 26
Loss: 0.48878635678972515
RMSE train: 0.689523	val: 0.832666	test: 0.780525
MAE train: 0.538906	val: 0.648773	test: 0.590265

Epoch: 27
Loss: 0.5084492287465504
RMSE train: 0.614743	val: 0.764954	test: 0.697710
MAE train: 0.477245	val: 0.586543	test: 0.538798

Epoch: 28
Loss: 0.47811412811279297
RMSE train: 0.620964	val: 0.780714	test: 0.709561
MAE train: 0.479454	val: 0.605723	test: 0.539363

Epoch: 29
Loss: 0.4694546972002302
RMSE train: 0.595184	val: 0.749375	test: 0.676316
MAE train: 0.463406	val: 0.573413	test: 0.515252

Epoch: 30
Loss: 0.45634264392512186
RMSE train: 0.594013	val: 0.757583	test: 0.689404
MAE train: 0.460014	val: 0.582858	test: 0.521474

Epoch: 31
Loss: 0.442143331680979
RMSE train: 0.612298	val: 0.758088	test: 0.696067
MAE train: 0.474633	val: 0.576433	test: 0.522087

Epoch: 32
Loss: 0.44634824991226196
RMSE train: 0.601127	val: 0.756510	test: 0.701114
MAE train: 0.465747	val: 0.584895	test: 0.532634

Epoch: 33
Loss: 0.43184418231248856
RMSE train: 0.577325	val: 0.735269	test: 0.689871
MAE train: 0.446560	val: 0.563577	test: 0.519902

Epoch: 34
Loss: 0.4475782811641693
RMSE train: 0.585718	val: 0.752678	test: 0.693442
MAE train: 0.452769	val: 0.574800	test: 0.527582

Epoch: 35
Loss: 0.44728156498500277
RMSE train: 0.571212	val: 0.740649	test: 0.679323
MAE train: 0.444496	val: 0.567765	test: 0.514139

Epoch: 36
Loss: 0.43358165877205984
RMSE train: 0.574761	val: 0.748032	test: 0.675987
MAE train: 0.445745	val: 0.567244	test: 0.510890

Epoch: 37
Loss: 0.4188662086214338
RMSE train: 0.570199	val: 0.735851	test: 0.685165
MAE train: 0.441197	val: 0.565513	test: 0.513461

Epoch: 38
Loss: 0.4365795339856829
RMSE train: 0.620959	val: 0.781108	test: 0.722637
MAE train: 0.485406	val: 0.597763	test: 0.551329

Epoch: 39
Loss: 0.4317552468606404
RMSE train: 0.605933	val: 0.784751	test: 0.727021
MAE train: 0.471122	val: 0.605636	test: 0.540853

Epoch: 40
Loss: 0.41204184932368143
RMSE train: 0.571543	val: 0.743074	test: 0.684746
MAE train: 0.443812	val: 0.559477	test: 0.509078

Epoch: 41
Loss: 0.42425309334482464
RMSE train: 0.575570	val: 0.746845	test: 0.676225
MAE train: 0.445357	val: 0.565989	test: 0.512043

Epoch: 42
Loss: 0.4180852174758911
RMSE train: 0.583726	val: 0.743350	test: 0.690654
MAE train: 0.453463	val: 0.573586	test: 0.523014

Epoch: 43
Loss: 0.40765891330582754
RMSE train: 0.542598	val: 0.716355	test: 0.655346
MAE train: 0.419675	val: 0.549559	test: 0.494700

Epoch: 44
Loss: 0.38734765563692364
RMSE train: 0.562499	val: 0.741300	test: 0.675913
MAE train: 0.436764	val: 0.570880	test: 0.509705

Epoch: 45
Loss: 0.38100355863571167
RMSE train: 0.546996	val: 0.710134	test: 0.677891
MAE train: 0.421593	val: 0.545599	test: 0.507315

Epoch: 46
Loss: 0.3955499806574413
RMSE train: 0.624212	val: 0.800955	test: 0.753608
MAE train: 0.486502	val: 0.613915	test: 0.565155

Epoch: 47
Loss: 0.4001482427120209
RMSE train: 0.580312	val: 0.748722	test: 0.704376
MAE train: 0.449482	val: 0.574300	test: 0.531864

Epoch: 48
Loss: 0.40365785147462574
RMSE train: 0.591264	val: 0.761685	test: 0.717489
MAE train: 0.457496	val: 0.582760	test: 0.538292

Epoch: 49
Loss: 0.4070034346410206
RMSE train: 0.558928	val: 0.726832	test: 0.680379
MAE train: 0.440374	val: 0.558510	test: 0.521798

Epoch: 50
Loss: 0.4112484816993986
RMSE train: 0.591754	val: 0.780627	test: 0.712227
MAE train: 0.457187	val: 0.588990	test: 0.533657

Epoch: 51
Loss: 0.392083255308015
RMSE train: 0.540368	val: 0.711981	test: 0.658256
MAE train: 0.417749	val: 0.544493	test: 0.497412

Epoch: 52
Loss: 0.37385989299842287
RMSE train: 0.564228	val: 0.736860	test: 0.685270
MAE train: 0.438360	val: 0.557925	test: 0.508443

Epoch: 53
Loss: 0.3799735712153571
RMSE train: 0.552686	val: 0.753173	test: 0.702009
MAE train: 0.428014	val: 0.571985	test: 0.519612

Epoch: 54
Loss: 0.38048101110117777
RMSE train: 0.548830	val: 0.735933	test: 0.671559
MAE train: 0.426210	val: 0.554958	test: 0.505604

Epoch: 55
Loss: 0.36900538206100464
RMSE train: 0.563599	val: 0.730913	test: 0.696107
MAE train: 0.435527	val: 0.554081	test: 0.518056

Epoch: 56
Loss: 0.3717048955815179
RMSE train: 0.546413	val: 0.737571	test: 0.683684
MAE train: 0.422502	val: 0.559280	test: 0.513423

Epoch: 57
Loss: 0.363803825208119
RMSE train: 0.518494	val: 0.713086	test: 0.653938
MAE train: 0.402140	val: 0.534132	test: 0.486114

Epoch: 58
Loss: 0.36533552195344654
RMSE train: 0.540533	val: 0.742908	test: 0.692025
MAE train: 0.415747	val: 0.563220	test: 0.508989

Epoch: 59
Loss: 0.3596225402184895
RMSE train: 0.605286	val: 0.784222	test: 0.729209
MAE train: 0.473748	val: 0.604060	test: 0.551622

Epoch: 60
Loss: 0.3677672381911959
RMSE train: 0.529987	val: 0.734417	test: 0.680749
MAE train: 0.409234	val: 0.549812	test: 0.500854

Epoch: 61
Loss: 0.3704830642257418
RMSE train: 0.512216	val: 0.707999	test: 0.648196
MAE train: 0.397687	val: 0.527196	test: 0.482590

Epoch: 62
Loss: 0.3461784358535494
RMSE train: 0.533144	val: 0.720775	test: 0.668586
MAE train: 0.411582	val: 0.542760	test: 0.500319

Epoch: 63
Loss: 0.3410902576787131
RMSE train: 0.519566	val: 0.717532	test: 0.658265
MAE train: 0.402041	val: 0.539902	test: 0.490087

Epoch: 64
Loss: 0.32829531601497103
RMSE train: 0.517902	val: 0.720310	test: 0.650164
MAE train: 0.402906	val: 0.541401	test: 0.490012

Epoch: 65
Loss: 0.33552985957690645
RMSE train: 0.508350	val: 0.699975	test: 0.651842
MAE train: 0.393201	val: 0.524803	test: 0.484089

Epoch: 66
Loss: 0.3445126712322235
RMSE train: 0.518953	val: 0.723895	test: 0.661212
MAE train: 0.402600	val: 0.544801	test: 0.495436

Epoch: 67
Loss: 0.33756041313920704
RMSE train: 0.507417	val: 0.707807	test: 0.654973
MAE train: 0.394100	val: 0.535425	test: 0.482945

Epoch: 68
Loss: 0.3434497139283589
RMSE train: 0.560350	val: 0.750913	test: 0.719495
MAE train: 0.433255	val: 0.570721	test: 0.535755

Epoch: 69
Loss: 0.3346168334994997
RMSE train: 0.520383	val: 0.719859	test: 0.660669
MAE train: 0.399990	val: 0.541227	test: 0.494064

Epoch: 70
Loss: 0.33065645822456907
RMSE train: 0.541683	val: 0.746572	test: 0.690474
MAE train: 0.420057	val: 0.561990	test: 0.522049

Epoch: 71
Loss: 0.3351215954337801
RMSE train: 0.531141	val: 0.737108	test: 0.692569
MAE train: 0.411443	val: 0.566001	test: 0.523015

Epoch: 72
Loss: 0.3239974613700594
RMSE train: 0.519239	val: 0.717592	test: 0.659071
MAE train: 0.403975	val: 0.543475	test: 0.492354

Epoch: 73
Loss: 0.3401709496974945
RMSE train: 0.519591	val: 0.718872	test: 0.670800
MAE train: 0.401727	val: 0.541364	test: 0.501487

Epoch: 74
Loss: 0.32320210124765125
RMSE train: 0.499835	val: 0.701727	test: 0.637907
MAE train: 0.389597	val: 0.530720	test: 0.478598

Epoch: 75
Loss: 0.3408482202461788
RMSE train: 0.561045	val: 0.771233	test: 0.694508
MAE train: 0.438062	val: 0.578296	test: 0.520718

Epoch: 76
Loss: 0.3188652204615729
RMSE train: 0.521417	val: 0.730609	test: 0.687487
MAE train: 0.400113	val: 0.552287	test: 0.513499

Epoch: 77
Loss: 0.3457643858024052
RMSE train: 0.496446	val: 0.706100	test: 0.646684
MAE train: 0.385918	val: 0.530807	test: 0.485960

Epoch: 78
Loss: 0.3110391561474119
RMSE train: 0.502856	val: 0.715620	test: 0.643704
MAE train: 0.392251	val: 0.547723	test: 0.483929

Epoch: 79
Loss: 0.3073499266590391
RMSE train: 0.466369	val: 0.703797	test: 0.629497
MAE train: 0.360729	val: 0.531839	test: 0.467293

Epoch: 80
Loss: 0.3128941250698907
RMSE train: 0.498429	val: 0.714585	test: 0.660499
MAE train: 0.387292	val: 0.541020	test: 0.492925

Epoch: 81
Loss: 0.29415271218333927
RMSE train: 0.518165	val: 0.746413	test: 0.661907
MAE train: 0.403495	val: 0.562648	test: 0.498723

Epoch: 82
Loss: 0.3108359617846353
RMSE train: 0.495304	val: 0.716819	test: 0.665937
MAE train: 0.383054	val: 0.538737	test: 0.497569

Epoch: 83
Loss: 0.30793233854430063
RMSE train: 0.484195	val: 0.708601	test: 0.646990
MAE train: 0.385454	val: 0.490091	test: 0.531236

Epoch: 84
Loss: 0.2870367974042892
RMSE train: 0.507606	val: 0.665140	test: 0.718623
MAE train: 0.396222	val: 0.493739	test: 0.538203

Epoch: 85
Loss: 0.2790981963276863
RMSE train: 0.463796	val: 0.647614	test: 0.703320
MAE train: 0.360638	val: 0.476194	test: 0.528827

Epoch: 86
Loss: 0.27718169093132017
RMSE train: 0.488060	val: 0.662330	test: 0.706082
MAE train: 0.381957	val: 0.484846	test: 0.532866

Epoch: 87
Loss: 0.2758355289697647
RMSE train: 0.516617	val: 0.671571	test: 0.725221
MAE train: 0.408307	val: 0.502578	test: 0.550739

Epoch: 88
Loss: 0.28546338677406313
RMSE train: 0.457113	val: 0.653867	test: 0.700249
MAE train: 0.355570	val: 0.482704	test: 0.522283

Epoch: 89
Loss: 0.26222738772630694
RMSE train: 0.449340	val: 0.632321	test: 0.697206
MAE train: 0.347856	val: 0.462275	test: 0.520004

Epoch: 90
Loss: 0.2716553181409836
RMSE train: 0.492780	val: 0.671669	test: 0.719679
MAE train: 0.387757	val: 0.494381	test: 0.542688

Epoch: 91
Loss: 0.2621695563197136
RMSE train: 0.508415	val: 0.679542	test: 0.723134
MAE train: 0.401407	val: 0.502771	test: 0.546978

Epoch: 92
Loss: 0.25574285835027694
RMSE train: 0.487990	val: 0.671690	test: 0.724090
MAE train: 0.387904	val: 0.501370	test: 0.546139

Epoch: 93
Loss: 0.2559766203165054
RMSE train: 0.492910	val: 0.677362	test: 0.717144
MAE train: 0.386007	val: 0.502108	test: 0.539383

Epoch: 94
Loss: 0.2605736285448074
RMSE train: 0.455211	val: 0.645086	test: 0.699164
MAE train: 0.352450	val: 0.469579	test: 0.523972

Epoch: 95
Loss: 0.26913361698389054
RMSE train: 0.483380	val: 0.663892	test: 0.702396
MAE train: 0.378347	val: 0.482236	test: 0.528034

Epoch: 96
Loss: 0.24659976065158845
RMSE train: 0.447557	val: 0.640627	test: 0.690096
MAE train: 0.350582	val: 0.467896	test: 0.521913

Epoch: 97
Loss: 0.25491030514240265
RMSE train: 0.444783	val: 0.633409	test: 0.689146
MAE train: 0.347269	val: 0.459185	test: 0.516623

Epoch: 98
Loss: 0.24743201285600663
RMSE train: 0.427733	val: 0.633410	test: 0.689307
MAE train: 0.334053	val: 0.459625	test: 0.511561

Epoch: 99
Loss: 0.25931957811117173
RMSE train: 0.457062	val: 0.645323	test: 0.700758
MAE train: 0.359404	val: 0.472359	test: 0.525519

Epoch: 100
Loss: 0.25317513644695283
RMSE train: 0.429883	val: 0.625039	test: 0.690609
MAE train: 0.334036	val: 0.456700	test: 0.513010

Epoch: 101
Loss: 0.2569823369383812
RMSE train: 0.446135	val: 0.643504	test: 0.692062
MAE train: 0.345927	val: 0.466031	test: 0.515879

Epoch: 102
Loss: 0.2493213087320328
RMSE train: 0.509461	val: 0.683016	test: 0.739016
MAE train: 0.407541	val: 0.509133	test: 0.565610

Epoch: 103
Loss: 0.24999269843101501
RMSE train: 0.445057	val: 0.649660	test: 0.684119
MAE train: 0.350124	val: 0.470719	test: 0.513959

Epoch: 104
Loss: 0.24500953853130342
RMSE train: 0.470689	val: 0.666563	test: 0.705658
MAE train: 0.368394	val: 0.492125	test: 0.530947

Epoch: 105
Loss: 0.2323802039027214
RMSE train: 0.484436	val: 0.681998	test: 0.729561
MAE train: 0.385921	val: 0.505168	test: 0.552209

Epoch: 106
Loss: 0.23616062700748444
RMSE train: 0.481104	val: 0.682997	test: 0.725337
MAE train: 0.380182	val: 0.503128	test: 0.546789

Epoch: 107
Loss: 0.23497071713209153
RMSE train: 0.448768	val: 0.655150	test: 0.695689
MAE train: 0.351134	val: 0.474080	test: 0.522544

Epoch: 108
Loss: 0.2299747496843338
RMSE train: 0.453686	val: 0.656197	test: 0.690735
MAE train: 0.357587	val: 0.480175	test: 0.523042

Epoch: 109
Loss: 0.25073073208332064
RMSE train: 0.501702	val: 0.687868	test: 0.736686
MAE train: 0.394924	val: 0.512334	test: 0.563083

Epoch: 110
Loss: 0.24217253774404526
RMSE train: 0.481396	val: 0.667482	test: 0.724788
MAE train: 0.379437	val: 0.494338	test: 0.546872

Epoch: 111
Loss: 0.23915814012289047
RMSE train: 0.493007	val: 0.681266	test: 0.724938
MAE train: 0.390712	val: 0.506842	test: 0.549090

Epoch: 112
Loss: 0.23077292293310164
RMSE train: 0.459147	val: 0.672409	test: 0.699336
MAE train: 0.359320	val: 0.484780	test: 0.521970

Epoch: 113
Loss: 0.24206102788448333
RMSE train: 0.455350	val: 0.650738	test: 0.700126
MAE train: 0.356758	val: 0.480223	test: 0.528155

Epoch: 114
Loss: 0.2503644347190857
RMSE train: 0.426532	val: 0.643882	test: 0.690151
MAE train: 0.330922	val: 0.470452	test: 0.515586

Epoch: 115
Loss: 0.22962257266044617
RMSE train: 0.471788	val: 0.670434	test: 0.721479
MAE train: 0.373714	val: 0.502657	test: 0.547584

Epoch: 116
Loss: 0.22889622747898103
RMSE train: 0.533804	val: 0.711953	test: 0.761162
MAE train: 0.432239	val: 0.541124	test: 0.584846

Epoch: 117
Loss: 0.22366116642951966
RMSE train: 0.490575	val: 0.677597	test: 0.713802
MAE train: 0.387531	val: 0.503691	test: 0.542604

Epoch: 118
Loss: 0.22925533652305602
RMSE train: 0.424873	val: 0.653893	test: 0.689568
MAE train: 0.329590	val: 0.473121	test: 0.516756

Epoch: 119
Loss: 0.22569006830453872
RMSE train: 0.460464	val: 0.669544	test: 0.692860
MAE train: 0.359018	val: 0.489590	test: 0.518546

Epoch: 120
Loss: 0.2266955316066742
RMSE train: 0.474930	val: 0.691606	test: 0.733050
MAE train: 0.376456	val: 0.505161	test: 0.552066

Epoch: 121
Loss: 0.22288851141929628
RMSE train: 0.517617	val: 0.704183	test: 0.751349
MAE train: 0.415791	val: 0.528134	test: 0.573204

Epoch: 122
Loss: 0.22660696804523467
RMSE train: 0.460126	val: 0.679089	test: 0.727393
MAE train: 0.366705	val: 0.498606	test: 0.544133

Epoch: 123
Loss: 0.232827065885067
RMSE train: 0.473978	val: 0.672453	test: 0.724025
MAE train: 0.374493	val: 0.495445	test: 0.547958

Epoch: 124
Loss: 0.2246738314628601
RMSE train: 0.430422	val: 0.644577	test: 0.688454
MAE train: 0.337407	val: 0.469224	test: 0.514633

Epoch: 125
Loss: 0.2171526923775673
RMSE train: 0.426241	val: 0.646748	test: 0.696719
MAE train: 0.330443	val: 0.466776	test: 0.517787

Epoch: 126
Loss: 0.21614062637090684
RMSE train: 0.392945	val: 0.627531	test: 0.674117
MAE train: 0.304774	val: 0.455417	test: 0.502733

Epoch: 127
Loss: 0.20948788225650788
RMSE train: 0.425348	val: 0.636645	test: 0.686566
MAE train: 0.332489	val: 0.464715	test: 0.512760

Epoch: 128
Loss: 0.21133224964141845
RMSE train: 0.454848	val: 0.674790	test: 0.730137
MAE train: 0.362752	val: 0.498001	test: 0.546080

Epoch: 129
Loss: 0.23686293810606002
RMSE train: 0.466588	val: 0.679200	test: 0.711774
MAE train: 0.372243	val: 0.502085	test: 0.533083

Epoch: 130
Loss: 0.21432901173830032
RMSE train: 0.412140	val: 0.648678	test: 0.690454
MAE train: 0.323353	val: 0.471694	test: 0.516484

Epoch: 131
Loss: 0.21782837063074112
RMSE train: 0.414648	val: 0.640213	test: 0.681228
MAE train: 0.326693	val: 0.466320	test: 0.507615

Epoch: 132
Loss: 0.21390576660633087
RMSE train: 0.439072	val: 0.651428	test: 0.700409
MAE train: 0.342418	val: 0.477346	test: 0.525635

Epoch: 133
Loss: 0.21597703248262407
RMSE train: 0.446162	val: 0.667736	test: 0.704615
MAE train: 0.351740	val: 0.484281	test: 0.528287

Epoch: 134
Loss: 0.21661957651376723
RMSE train: 0.409121	val: 0.646649	test: 0.670796
MAE train: 0.318543	val: 0.465509	test: 0.502552

Epoch: 135
Loss: 0.20541372299194335
RMSE train: 0.470963	val: 0.677254	test: 0.724262
MAE train: 0.373543	val: 0.501953	test: 0.547654

Early stopping
Best (RMSE):	 train: 0.429883	val: 0.625039	test: 0.690609
Best (MAE):	 train: 0.334036	val: 0.456700	test: 0.513010

MAE train: 0.394823	val: 0.516191	test: 0.549758

Epoch: 84
Loss: 0.2766958996653557
RMSE train: 0.436006	val: 0.640015	test: 0.666303
MAE train: 0.338563	val: 0.466070	test: 0.500945

Epoch: 85
Loss: 0.2750117748975754
RMSE train: 0.465687	val: 0.670095	test: 0.697545
MAE train: 0.363137	val: 0.491281	test: 0.526008

Epoch: 86
Loss: 0.2802256762981415
RMSE train: 0.449511	val: 0.644617	test: 0.673630
MAE train: 0.349387	val: 0.469773	test: 0.508038

Epoch: 87
Loss: 0.2776108220219612
RMSE train: 0.446247	val: 0.662925	test: 0.686232
MAE train: 0.348876	val: 0.482527	test: 0.510770

Epoch: 88
Loss: 0.2819253772497177
RMSE train: 0.472584	val: 0.669755	test: 0.688480
MAE train: 0.366178	val: 0.494436	test: 0.527174

Epoch: 89
Loss: 0.2719789132475853
RMSE train: 0.466650	val: 0.666912	test: 0.682813
MAE train: 0.364080	val: 0.486401	test: 0.518123

Epoch: 90
Loss: 0.27127983421087265
RMSE train: 0.481990	val: 0.679562	test: 0.693875
MAE train: 0.378971	val: 0.498510	test: 0.526427

Epoch: 91
Loss: 0.26899402737617495
RMSE train: 0.450557	val: 0.665745	test: 0.670466
MAE train: 0.351965	val: 0.484144	test: 0.509759

Epoch: 92
Loss: 0.2647270247340202
RMSE train: 0.431117	val: 0.659658	test: 0.662366
MAE train: 0.338013	val: 0.480603	test: 0.503524

Epoch: 93
Loss: 0.27453768700361253
RMSE train: 0.464734	val: 0.657481	test: 0.684900
MAE train: 0.364205	val: 0.485322	test: 0.519187

Epoch: 94
Loss: 0.25083590149879453
RMSE train: 0.467580	val: 0.666056	test: 0.676218
MAE train: 0.366904	val: 0.482719	test: 0.510325

Epoch: 95
Loss: 0.2518839806318283
RMSE train: 0.435570	val: 0.651127	test: 0.664430
MAE train: 0.339352	val: 0.470379	test: 0.499782

Epoch: 96
Loss: 0.26421691924333573
RMSE train: 0.462325	val: 0.671011	test: 0.684707
MAE train: 0.360624	val: 0.489972	test: 0.524252

Epoch: 97
Loss: 0.2607394218444824
RMSE train: 0.438579	val: 0.642841	test: 0.672803
MAE train: 0.340673	val: 0.470357	test: 0.512589

Epoch: 98
Loss: 0.24992783218622208
RMSE train: 0.437029	val: 0.657098	test: 0.667697
MAE train: 0.340103	val: 0.477741	test: 0.503401

Epoch: 99
Loss: 0.24775922447443008
RMSE train: 0.466762	val: 0.671515	test: 0.696298
MAE train: 0.364982	val: 0.497940	test: 0.522463

Epoch: 100
Loss: 0.25723675042390826
RMSE train: 0.450681	val: 0.654870	test: 0.680345
MAE train: 0.351392	val: 0.479643	test: 0.511855

Epoch: 101
Loss: 0.24989710450172425
RMSE train: 0.490665	val: 0.684908	test: 0.699484
MAE train: 0.383249	val: 0.504884	test: 0.532744

Epoch: 102
Loss: 0.2684022426605225
RMSE train: 0.433609	val: 0.654907	test: 0.665612
MAE train: 0.337493	val: 0.480411	test: 0.503549

Epoch: 103
Loss: 0.2435392215847969
RMSE train: 0.417223	val: 0.657156	test: 0.654992
MAE train: 0.322432	val: 0.474840	test: 0.494390

Epoch: 104
Loss: 0.2397113099694252
RMSE train: 0.454230	val: 0.685570	test: 0.693805
MAE train: 0.356700	val: 0.501674	test: 0.527434

Epoch: 105
Loss: 0.24703608751296996
RMSE train: 0.409856	val: 0.647728	test: 0.661571
MAE train: 0.317926	val: 0.469537	test: 0.495126

Epoch: 106
Loss: 0.24042804688215255
RMSE train: 0.442210	val: 0.670661	test: 0.677349
MAE train: 0.347288	val: 0.486718	test: 0.514402

Epoch: 107
Loss: 0.23441898375749587
RMSE train: 0.428128	val: 0.659204	test: 0.670368
MAE train: 0.332815	val: 0.477139	test: 0.503350

Epoch: 108
Loss: 0.2356123998761177
RMSE train: 0.484421	val: 0.691395	test: 0.710030
MAE train: 0.379293	val: 0.507245	test: 0.537316

Epoch: 109
Loss: 0.23899076133966446
RMSE train: 0.426839	val: 0.653337	test: 0.665105
MAE train: 0.333845	val: 0.469485	test: 0.497458

Epoch: 110
Loss: 0.24327458143234254
RMSE train: 0.419023	val: 0.636555	test: 0.657458
MAE train: 0.327214	val: 0.465338	test: 0.497138

Epoch: 111
Loss: 0.23639976978302002
RMSE train: 0.432355	val: 0.654819	test: 0.674619
MAE train: 0.335543	val: 0.476914	test: 0.507809

Epoch: 112
Loss: 0.23260766565799712
RMSE train: 0.425006	val: 0.653890	test: 0.666485
MAE train: 0.330964	val: 0.475259	test: 0.498905

Epoch: 113
Loss: 0.23154582679271699
RMSE train: 0.413541	val: 0.647306	test: 0.664938
MAE train: 0.320784	val: 0.469677	test: 0.498750

Epoch: 114
Loss: 0.22828106880187987
RMSE train: 0.411664	val: 0.657027	test: 0.659136
MAE train: 0.321727	val: 0.471919	test: 0.491886

Epoch: 115
Loss: 0.2314760446548462
RMSE train: 0.454002	val: 0.673664	test: 0.682676
MAE train: 0.351396	val: 0.489227	test: 0.513858

Epoch: 116
Loss: 0.23517109155654908
RMSE train: 0.425801	val: 0.653378	test: 0.667809
MAE train: 0.332691	val: 0.472331	test: 0.499888

Epoch: 117
Loss: 0.2298953041434288
RMSE train: 0.406325	val: 0.645400	test: 0.659009
MAE train: 0.314711	val: 0.464299	test: 0.495363

Epoch: 118
Loss: 0.2283878967165947
RMSE train: 0.421645	val: 0.648786	test: 0.659689
MAE train: 0.330247	val: 0.471963	test: 0.493883

Epoch: 119
Loss: 0.2233925223350525
RMSE train: 0.415724	val: 0.662958	test: 0.668382
MAE train: 0.325708	val: 0.480526	test: 0.501462

Epoch: 120
Loss: 0.22562237530946733
RMSE train: 0.471108	val: 0.693030	test: 0.694793
MAE train: 0.375890	val: 0.510287	test: 0.523187

Epoch: 121
Loss: 0.22524774372577666
RMSE train: 0.430326	val: 0.671782	test: 0.672945
MAE train: 0.338029	val: 0.480406	test: 0.507410

Epoch: 122
Loss: 0.2194040074944496
RMSE train: 0.412550	val: 0.654211	test: 0.655907
MAE train: 0.319516	val: 0.469405	test: 0.493789

Epoch: 123
Loss: 0.214951092004776
RMSE train: 0.430018	val: 0.658493	test: 0.671107
MAE train: 0.335866	val: 0.476162	test: 0.509655

Epoch: 124
Loss: 0.22356234788894652
RMSE train: 0.411452	val: 0.638849	test: 0.664496
MAE train: 0.319830	val: 0.464315	test: 0.498835

Epoch: 125
Loss: 0.21650865226984023
RMSE train: 0.409094	val: 0.658561	test: 0.659368
MAE train: 0.318526	val: 0.471236	test: 0.493049

Epoch: 126
Loss: 0.21441321969032287
RMSE train: 0.392250	val: 0.646358	test: 0.652137
MAE train: 0.304727	val: 0.465201	test: 0.490800

Epoch: 127
Loss: 0.21435403674840928
RMSE train: 0.411051	val: 0.656297	test: 0.658366
MAE train: 0.321809	val: 0.469487	test: 0.493659

Epoch: 128
Loss: 0.22450587749481202
RMSE train: 0.462822	val: 0.686765	test: 0.704142
MAE train: 0.362522	val: 0.506904	test: 0.533932

Epoch: 129
Loss: 0.22407927513122558
RMSE train: 0.402654	val: 0.658284	test: 0.662764
MAE train: 0.316136	val: 0.468342	test: 0.501011

Epoch: 130
Loss: 0.2114000141620636
RMSE train: 0.419341	val: 0.662184	test: 0.677091
MAE train: 0.325287	val: 0.474584	test: 0.511701

Epoch: 131
Loss: 0.21503085792064666
RMSE train: 0.395405	val: 0.649415	test: 0.656993
MAE train: 0.307547	val: 0.465527	test: 0.491151

Epoch: 132
Loss: 0.2168835625052452
RMSE train: 0.431767	val: 0.661959	test: 0.677842
MAE train: 0.338362	val: 0.482448	test: 0.506804

Epoch: 133
Loss: 0.2176930695772171
RMSE train: 0.400330	val: 0.647244	test: 0.658372
MAE train: 0.314077	val: 0.465687	test: 0.491680

Epoch: 134
Loss: 0.21242900490760802
RMSE train: 0.392601	val: 0.640261	test: 0.660840
MAE train: 0.306207	val: 0.459453	test: 0.493316

Epoch: 135
Loss: 0.2107450157403946
RMSE train: 0.382048	val: 0.640361	test: 0.664566
MAE train: 0.296421	val: 0.461134	test: 0.496991

Epoch: 136
Loss: 0.20402248948812485
RMSE train: 0.401207	val: 0.652015	test: 0.668176
MAE train: 0.314410	val: 0.469328	test: 0.502049

Epoch: 137
Loss: 0.21547716110944748
RMSE train: 0.400748	val: 0.653823	test: 0.662306
MAE train: 0.312293	val: 0.465859	test: 0.494174

Epoch: 138
Loss: 0.21306179463863373
RMSE train: 0.407694	val: 0.670012	test: 0.670515
MAE train: 0.321466	val: 0.489882	test: 0.512789

Epoch: 139
Loss: 0.21197015792131424
RMSE train: 0.406804	val: 0.672657	test: 0.665935
MAE train: 0.319330	val: 0.484111	test: 0.501864

Epoch: 140
Loss: 0.20690650045871734
RMSE train: 0.379803	val: 0.645713	test: 0.658940
MAE train: 0.296086	val: 0.462015	test: 0.494737

Epoch: 141
Loss: 0.20835984945297242
RMSE train: 0.388377	val: 0.644177	test: 0.666182
MAE train: 0.303512	val: 0.457615	test: 0.494985

Epoch: 142
Loss: 0.20379152446985244
RMSE train: 0.406412	val: 0.668941	test: 0.681137
MAE train: 0.321124	val: 0.480525	test: 0.509882

Epoch: 143
Loss: 0.2088477373123169
RMSE train: 0.431017	val: 0.674542	test: 0.689591
MAE train: 0.341438	val: 0.489729	test: 0.517775
MAE train: 0.362382	val: 0.482283	test: 0.517698

Epoch: 84
Loss: 0.2825805962085724
RMSE train: 0.463350	val: 0.665125	test: 0.696833
MAE train: 0.365022	val: 0.493105	test: 0.518710

Epoch: 85
Loss: 0.28229303658008575
RMSE train: 0.466814	val: 0.654899	test: 0.688347
MAE train: 0.364130	val: 0.480139	test: 0.515381

Epoch: 86
Loss: 0.2744443237781525
RMSE train: 0.478067	val: 0.667337	test: 0.697206
MAE train: 0.375745	val: 0.491570	test: 0.520985

Epoch: 87
Loss: 0.2848357081413269
RMSE train: 0.437214	val: 0.643698	test: 0.687197
MAE train: 0.343083	val: 0.476807	test: 0.517943

Epoch: 88
Loss: 0.2700424835085869
RMSE train: 0.436184	val: 0.649680	test: 0.669836
MAE train: 0.340759	val: 0.475137	test: 0.504516

Epoch: 89
Loss: 0.2691078454256058
RMSE train: 0.442708	val: 0.655386	test: 0.678566
MAE train: 0.348419	val: 0.480115	test: 0.510564

Epoch: 90
Loss: 0.261033695936203
RMSE train: 0.458224	val: 0.652806	test: 0.687722
MAE train: 0.360678	val: 0.481520	test: 0.516416

Epoch: 91
Loss: 0.273405721783638
RMSE train: 0.469584	val: 0.653437	test: 0.703487
MAE train: 0.365087	val: 0.481511	test: 0.534013

Epoch: 92
Loss: 0.2786803662776947
RMSE train: 0.440357	val: 0.647482	test: 0.677391
MAE train: 0.343662	val: 0.469260	test: 0.511079

Epoch: 93
Loss: 0.2581941232085228
RMSE train: 0.453540	val: 0.655444	test: 0.687527
MAE train: 0.356237	val: 0.479986	test: 0.519800

Epoch: 94
Loss: 0.2540307804942131
RMSE train: 0.438586	val: 0.641527	test: 0.676953
MAE train: 0.344768	val: 0.473472	test: 0.509981

Epoch: 95
Loss: 0.26658446043729783
RMSE train: 0.460125	val: 0.667842	test: 0.671607
MAE train: 0.362413	val: 0.487002	test: 0.502072

Epoch: 96
Loss: 0.2605478137731552
RMSE train: 0.535169	val: 0.720552	test: 0.750926
MAE train: 0.428346	val: 0.547756	test: 0.576513

Epoch: 97
Loss: 0.2560552626848221
RMSE train: 0.437740	val: 0.661036	test: 0.687270
MAE train: 0.343805	val: 0.483654	test: 0.512481

Epoch: 98
Loss: 0.2576180562376976
RMSE train: 0.424615	val: 0.652214	test: 0.683914
MAE train: 0.334237	val: 0.477576	test: 0.514380

Epoch: 99
Loss: 0.247548608481884
RMSE train: 0.425889	val: 0.648580	test: 0.689239
MAE train: 0.332746	val: 0.473989	test: 0.510653

Epoch: 100
Loss: 0.2533613875508308
RMSE train: 0.423732	val: 0.645791	test: 0.674307
MAE train: 0.331347	val: 0.473354	test: 0.506236

Epoch: 101
Loss: 0.2524577870965004
RMSE train: 0.430990	val: 0.652304	test: 0.683531
MAE train: 0.336558	val: 0.475821	test: 0.508656

Epoch: 102
Loss: 0.25695172101259234
RMSE train: 0.464527	val: 0.678587	test: 0.695855
MAE train: 0.366243	val: 0.498457	test: 0.525551

Epoch: 103
Loss: 0.2481192782521248
RMSE train: 0.444482	val: 0.669766	test: 0.682981
MAE train: 0.349979	val: 0.488202	test: 0.510121

Epoch: 104
Loss: 0.25000312775373457
RMSE train: 0.430154	val: 0.656636	test: 0.684577
MAE train: 0.337988	val: 0.479958	test: 0.511436

Epoch: 105
Loss: 0.24873490482568741
RMSE train: 0.452834	val: 0.656774	test: 0.693255
MAE train: 0.358287	val: 0.483223	test: 0.521556

Epoch: 106
Loss: 0.25231539756059645
RMSE train: 0.457608	val: 0.690918	test: 0.685253
MAE train: 0.361095	val: 0.501031	test: 0.520030

Epoch: 107
Loss: 0.2706088200211525
RMSE train: 0.427617	val: 0.647292	test: 0.693113
MAE train: 0.333324	val: 0.475478	test: 0.517568

Epoch: 108
Loss: 0.24866794496774675
RMSE train: 0.431316	val: 0.646925	test: 0.661658
MAE train: 0.339234	val: 0.471480	test: 0.500747

Epoch: 109
Loss: 0.24939653575420379
RMSE train: 0.420214	val: 0.643391	test: 0.676713
MAE train: 0.329853	val: 0.468506	test: 0.509767

Epoch: 110
Loss: 0.23665234744548796
RMSE train: 0.434606	val: 0.657470	test: 0.674136
MAE train: 0.342340	val: 0.479538	test: 0.505069

Epoch: 111
Loss: 0.24036299884319307
RMSE train: 0.420585	val: 0.644654	test: 0.682842
MAE train: 0.330046	val: 0.471860	test: 0.510732

Epoch: 112
Loss: 0.23747294694185256
RMSE train: 0.415182	val: 0.647612	test: 0.680597
MAE train: 0.324747	val: 0.468176	test: 0.508447

Epoch: 113
Loss: 0.23697624802589418
RMSE train: 0.411101	val: 0.645001	test: 0.670180
MAE train: 0.320464	val: 0.470539	test: 0.501922

Epoch: 114
Loss: 0.2393035590648651
RMSE train: 0.433690	val: 0.648492	test: 0.683712
MAE train: 0.341363	val: 0.477695	test: 0.512358

Epoch: 115
Loss: 0.24036862403154374
RMSE train: 0.422977	val: 0.647458	test: 0.668636
MAE train: 0.331986	val: 0.468711	test: 0.495861

Epoch: 116
Loss: 0.22565154880285263
RMSE train: 0.425465	val: 0.654107	test: 0.681492
MAE train: 0.333972	val: 0.477183	test: 0.506681

Epoch: 117
Loss: 0.24312526881694793
RMSE train: 0.414616	val: 0.644856	test: 0.691833
MAE train: 0.324875	val: 0.474074	test: 0.517330

Epoch: 118
Loss: 0.22394901961088182
RMSE train: 0.433502	val: 0.656371	test: 0.679831
MAE train: 0.340795	val: 0.487412	test: 0.513463

Epoch: 119
Loss: 0.23883701413869857
RMSE train: 0.420354	val: 0.644311	test: 0.692498
MAE train: 0.328744	val: 0.471963	test: 0.520116

Epoch: 120
Loss: 0.22444702982902526
RMSE train: 0.422348	val: 0.659145	test: 0.679797
MAE train: 0.331013	val: 0.479939	test: 0.510066

Epoch: 121
Loss: 0.229348823428154
RMSE train: 0.397253	val: 0.643318	test: 0.676018
MAE train: 0.308722	val: 0.465827	test: 0.502707

Epoch: 122
Loss: 0.22334988713264464
RMSE train: 0.452576	val: 0.667447	test: 0.697292
MAE train: 0.358154	val: 0.491384	test: 0.525209

Epoch: 123
Loss: 0.22655593007802963
RMSE train: 0.393083	val: 0.636210	test: 0.660096
MAE train: 0.308997	val: 0.463403	test: 0.495470

Epoch: 124
Loss: 0.22303486466407776
RMSE train: 0.409291	val: 0.639376	test: 0.675015
MAE train: 0.319477	val: 0.472539	test: 0.507881

Epoch: 125
Loss: 0.2190499946475029
RMSE train: 0.437791	val: 0.650508	test: 0.687190
MAE train: 0.340604	val: 0.486595	test: 0.520784

Epoch: 126
Loss: 0.2216410905122757
RMSE train: 0.390436	val: 0.638842	test: 0.672969
MAE train: 0.306899	val: 0.461947	test: 0.500627

Epoch: 127
Loss: 0.22332081347703933
RMSE train: 0.414340	val: 0.641529	test: 0.680117
MAE train: 0.326268	val: 0.473348	test: 0.514486

Epoch: 128
Loss: 0.22181787043809892
RMSE train: 0.418172	val: 0.652285	test: 0.669130
MAE train: 0.330341	val: 0.473754	test: 0.507189

Epoch: 129
Loss: 0.2213781952857971
RMSE train: 0.460560	val: 0.678705	test: 0.700903
MAE train: 0.367654	val: 0.497249	test: 0.533135

Epoch: 130
Loss: 0.23127007633447647
RMSE train: 0.404380	val: 0.638543	test: 0.666866
MAE train: 0.316489	val: 0.466361	test: 0.503670

Epoch: 131
Loss: 0.20949384123086928
RMSE train: 0.419429	val: 0.652293	test: 0.666771
MAE train: 0.328818	val: 0.474021	test: 0.503560

Epoch: 132
Loss: 0.21708949655294418
RMSE train: 0.449239	val: 0.670381	test: 0.692181
MAE train: 0.355698	val: 0.493569	test: 0.523547

Epoch: 133
Loss: 0.2147630050778389
RMSE train: 0.403513	val: 0.632589	test: 0.679969
MAE train: 0.315204	val: 0.463540	test: 0.513897

Epoch: 134
Loss: 0.21288203746080397
RMSE train: 0.416551	val: 0.660224	test: 0.693431
MAE train: 0.324886	val: 0.481243	test: 0.514962

Epoch: 135
Loss: 0.21777838170528413
RMSE train: 0.423517	val: 0.649568	test: 0.683635
MAE train: 0.332343	val: 0.477253	test: 0.514702

Epoch: 136
Loss: 0.2083679184317589
RMSE train: 0.429853	val: 0.661532	test: 0.687942
MAE train: 0.337385	val: 0.484423	test: 0.517282

Epoch: 137
Loss: 0.210831917822361
RMSE train: 0.408068	val: 0.649201	test: 0.681277
MAE train: 0.320822	val: 0.475432	test: 0.511431

Epoch: 138
Loss: 0.21977595686912538
RMSE train: 0.436734	val: 0.661517	test: 0.692446
MAE train: 0.344756	val: 0.482090	test: 0.523792

Epoch: 139
Loss: 0.2176019936800003
RMSE train: 0.422204	val: 0.659254	test: 0.679144
MAE train: 0.331864	val: 0.476087	test: 0.507403

Epoch: 140
Loss: 0.21216373145580292
RMSE train: 0.422327	val: 0.664919	test: 0.683645
MAE train: 0.333625	val: 0.484546	test: 0.515540

Epoch: 141
Loss: 0.2071833312511444
RMSE train: 0.380019	val: 0.635249	test: 0.670430
MAE train: 0.296629	val: 0.457739	test: 0.502097

Epoch: 142
Loss: 0.19630940407514572
RMSE train: 0.384716	val: 0.645250	test: 0.665627
MAE train: 0.299897	val: 0.465974	test: 0.502790

Epoch: 143
Loss: 0.2131847470998764
RMSE train: 0.422429	val: 0.649055	test: 0.674432
MAE train: 0.332256	val: 0.473972	test: 0.512478

Epoch: 144
Loss: 0.20824006497859954
RMSE train: 0.436856	val: 0.685026	test: 0.686220
MAE train: 0.346288	val: 0.504448	test: 0.528517

Epoch: 145
Loss: 0.20263128876686096
RMSE train: 0.412980	val: 0.654994	test: 0.670439
MAE train: 0.323562	val: 0.474570	test: 0.504616

Early stopping
Best (RMSE):	 train: 0.419023	val: 0.636555	test: 0.657458
Best (MAE):	 train: 0.327214	val: 0.465338	test: 0.497138

MAE train: 0.395155	val: 0.516522	test: 0.526972

Epoch: 84
Loss: 0.27591124673684436
RMSE train: 0.469753	val: 0.689433	test: 0.688304
MAE train: 0.368139	val: 0.485234	test: 0.516290

Epoch: 85
Loss: 0.2730145404736201
RMSE train: 0.540495	val: 0.749585	test: 0.745014
MAE train: 0.430718	val: 0.544072	test: 0.567936

Epoch: 86
Loss: 0.2747543267905712
RMSE train: 0.452708	val: 0.698481	test: 0.690343
MAE train: 0.353480	val: 0.500835	test: 0.510002

Epoch: 87
Loss: 0.27345216770966846
RMSE train: 0.452925	val: 0.693001	test: 0.692612
MAE train: 0.356009	val: 0.492630	test: 0.510486

Epoch: 88
Loss: 0.2653707427283128
RMSE train: 0.465325	val: 0.692672	test: 0.696386
MAE train: 0.365129	val: 0.489536	test: 0.517619

Epoch: 89
Loss: 0.279587522149086
RMSE train: 0.512870	val: 0.738596	test: 0.738833
MAE train: 0.407866	val: 0.527306	test: 0.552165

Epoch: 90
Loss: 0.2810959244767825
RMSE train: 0.461577	val: 0.689676	test: 0.689301
MAE train: 0.363118	val: 0.495994	test: 0.516700

Epoch: 91
Loss: 0.26318713277578354
RMSE train: 0.440178	val: 0.696615	test: 0.690556
MAE train: 0.345712	val: 0.500330	test: 0.503985

Epoch: 92
Loss: 0.2700616642832756
RMSE train: 0.439873	val: 0.695146	test: 0.683423
MAE train: 0.345223	val: 0.492772	test: 0.506684

Epoch: 93
Loss: 0.25627075632413227
RMSE train: 0.417472	val: 0.661984	test: 0.656191
MAE train: 0.325768	val: 0.462818	test: 0.484422

Epoch: 94
Loss: 0.2472554457684358
RMSE train: 0.435383	val: 0.698399	test: 0.682714
MAE train: 0.341967	val: 0.505849	test: 0.508075

Epoch: 95
Loss: 0.2549600712954998
RMSE train: 0.465026	val: 0.705123	test: 0.705754
MAE train: 0.367330	val: 0.507362	test: 0.531522

Epoch: 96
Loss: 0.2551591843366623
RMSE train: 0.508017	val: 0.729511	test: 0.728232
MAE train: 0.405019	val: 0.528792	test: 0.551922

Epoch: 97
Loss: 0.2664765914281209
RMSE train: 0.471432	val: 0.714816	test: 0.700120
MAE train: 0.372330	val: 0.514244	test: 0.522355

Epoch: 98
Loss: 0.25144259879986447
RMSE train: 0.473677	val: 0.716163	test: 0.705444
MAE train: 0.375398	val: 0.509809	test: 0.528886

Epoch: 99
Loss: 0.245661577830712
RMSE train: 0.487911	val: 0.731597	test: 0.724785
MAE train: 0.389283	val: 0.525873	test: 0.547552

Epoch: 100
Loss: 0.24800284331043562
RMSE train: 0.455801	val: 0.704445	test: 0.711349
MAE train: 0.358250	val: 0.500352	test: 0.530960

Epoch: 101
Loss: 0.25244975338379544
RMSE train: 0.426809	val: 0.675663	test: 0.677609
MAE train: 0.335455	val: 0.478511	test: 0.499949

Epoch: 102
Loss: 0.24675374726454416
RMSE train: 0.462434	val: 0.710603	test: 0.707824
MAE train: 0.367304	val: 0.508834	test: 0.527170

Epoch: 103
Loss: 0.2540428787469864
RMSE train: 0.436011	val: 0.701592	test: 0.698633
MAE train: 0.342391	val: 0.500967	test: 0.516867

Epoch: 104
Loss: 0.2497996836900711
RMSE train: 0.445538	val: 0.684381	test: 0.694245
MAE train: 0.347811	val: 0.491583	test: 0.517766

Epoch: 105
Loss: 0.25164580593506497
RMSE train: 0.416860	val: 0.670050	test: 0.679071
MAE train: 0.326224	val: 0.477924	test: 0.498369

Epoch: 106
Loss: 0.23248942072192827
RMSE train: 0.502677	val: 0.723610	test: 0.730855
MAE train: 0.401393	val: 0.524682	test: 0.555810

Epoch: 107
Loss: 0.2507937401533127
RMSE train: 0.478588	val: 0.716831	test: 0.719351
MAE train: 0.379345	val: 0.512358	test: 0.544255

Epoch: 108
Loss: 0.24150925874710083
RMSE train: 0.396787	val: 0.662297	test: 0.647399
MAE train: 0.309929	val: 0.473761	test: 0.483475

Epoch: 109
Loss: 0.24186478555202484
RMSE train: 0.434405	val: 0.685883	test: 0.684744
MAE train: 0.342258	val: 0.489922	test: 0.507261

Epoch: 110
Loss: 0.23859057078758875
RMSE train: 0.467348	val: 0.723092	test: 0.700307
MAE train: 0.373455	val: 0.517585	test: 0.532589

Epoch: 111
Loss: 0.2395205187300841
RMSE train: 0.407663	val: 0.677550	test: 0.672164
MAE train: 0.318584	val: 0.473779	test: 0.499475

Epoch: 112
Loss: 0.22937801480293274
RMSE train: 0.489561	val: 0.725018	test: 0.738820
MAE train: 0.390183	val: 0.524672	test: 0.564970

Epoch: 113
Loss: 0.22927377745509148
RMSE train: 0.458842	val: 0.709000	test: 0.700610
MAE train: 0.367930	val: 0.506818	test: 0.526329

Epoch: 114
Loss: 0.2260806808869044
RMSE train: 0.445631	val: 0.690286	test: 0.692201
MAE train: 0.354787	val: 0.491550	test: 0.522408

Epoch: 115
Loss: 0.2369569962223371
RMSE train: 0.458741	val: 0.686593	test: 0.701266
MAE train: 0.359839	val: 0.490145	test: 0.536375

Epoch: 116
Loss: 0.23747533559799194
RMSE train: 0.408723	val: 0.676265	test: 0.676071
MAE train: 0.320068	val: 0.483474	test: 0.501529

Epoch: 117
Loss: 0.22243096555272737
RMSE train: 0.432458	val: 0.697861	test: 0.672417
MAE train: 0.342242	val: 0.491241	test: 0.505590

Epoch: 118
Loss: 0.21432875469326973
RMSE train: 0.499308	val: 0.726868	test: 0.730098
MAE train: 0.403236	val: 0.528863	test: 0.561877

Epoch: 119
Loss: 0.23896757513284683
RMSE train: 0.395507	val: 0.665896	test: 0.654488
MAE train: 0.309012	val: 0.463886	test: 0.482254

Epoch: 120
Loss: 0.22648338849345842
RMSE train: 0.482302	val: 0.721679	test: 0.718868
MAE train: 0.384624	val: 0.515604	test: 0.542655

Epoch: 121
Loss: 0.2229137271642685
RMSE train: 0.501699	val: 0.738049	test: 0.748088
MAE train: 0.401053	val: 0.528594	test: 0.572611

Epoch: 122
Loss: 0.22631785894433656
RMSE train: 0.430659	val: 0.686908	test: 0.686298
MAE train: 0.340777	val: 0.488443	test: 0.514438

Epoch: 123
Loss: 0.21332821870843569
RMSE train: 0.439200	val: 0.698265	test: 0.692810
MAE train: 0.348285	val: 0.500916	test: 0.519463

Epoch: 124
Loss: 0.21285784617066383
RMSE train: 0.408006	val: 0.701467	test: 0.685221
MAE train: 0.322214	val: 0.502049	test: 0.509036

Epoch: 125
Loss: 0.2177736647427082
RMSE train: 0.411763	val: 0.680257	test: 0.677107
MAE train: 0.323743	val: 0.479361	test: 0.505971

Epoch: 126
Loss: 0.21410048007965088
RMSE train: 0.478127	val: 0.719525	test: 0.716274
MAE train: 0.383110	val: 0.522558	test: 0.550410

Epoch: 127
Loss: 0.22010391453901926
RMSE train: 0.433093	val: 0.695047	test: 0.695523
MAE train: 0.344571	val: 0.491399	test: 0.519021

Epoch: 128
Loss: 0.22384469583630562
RMSE train: 0.438152	val: 0.681528	test: 0.691305
MAE train: 0.346074	val: 0.487176	test: 0.520649

Early stopping
Best (RMSE):	 train: 0.417472	val: 0.661984	test: 0.656191
Best (MAE):	 train: 0.325768	val: 0.462818	test: 0.484422

MAE train: 0.386085	val: 0.516815	test: 0.515961

Epoch: 84
Loss: 0.27308259531855583
RMSE train: 0.455185	val: 0.677192	test: 0.668644
MAE train: 0.355490	val: 0.498115	test: 0.508912

Epoch: 85
Loss: 0.2702546454966068
RMSE train: 0.470221	val: 0.698071	test: 0.672224
MAE train: 0.367753	val: 0.506037	test: 0.506356

Epoch: 86
Loss: 0.2751487505932649
RMSE train: 0.459805	val: 0.695883	test: 0.685420
MAE train: 0.357602	val: 0.504916	test: 0.512313

Epoch: 87
Loss: 0.27221427982052165
RMSE train: 0.500215	val: 0.703005	test: 0.687202
MAE train: 0.391026	val: 0.515490	test: 0.522352

Epoch: 88
Loss: 0.2661207616329193
RMSE train: 0.463564	val: 0.688188	test: 0.682861
MAE train: 0.362813	val: 0.497396	test: 0.514727

Epoch: 89
Loss: 0.2640252423783143
RMSE train: 0.489719	val: 0.693461	test: 0.694928
MAE train: 0.384087	val: 0.505207	test: 0.524079

Epoch: 90
Loss: 0.2601163052022457
RMSE train: 0.456798	val: 0.689387	test: 0.683953
MAE train: 0.356055	val: 0.499936	test: 0.516054

Epoch: 91
Loss: 0.27763139208157855
RMSE train: 0.495505	val: 0.692128	test: 0.694771
MAE train: 0.389098	val: 0.505455	test: 0.524060

Epoch: 92
Loss: 0.27350129435459775
RMSE train: 0.453592	val: 0.672400	test: 0.663095
MAE train: 0.352861	val: 0.485724	test: 0.497771

Epoch: 93
Loss: 0.2562200513978799
RMSE train: 0.448611	val: 0.668880	test: 0.673658
MAE train: 0.349751	val: 0.480391	test: 0.506191

Epoch: 94
Loss: 0.2578693690399329
RMSE train: 0.443474	val: 0.684418	test: 0.664776
MAE train: 0.348037	val: 0.496032	test: 0.495158

Epoch: 95
Loss: 0.25088347246249515
RMSE train: 0.453527	val: 0.677249	test: 0.672932
MAE train: 0.352593	val: 0.488558	test: 0.506131

Epoch: 96
Loss: 0.25786492849389714
RMSE train: 0.502843	val: 0.718454	test: 0.708980
MAE train: 0.393986	val: 0.524846	test: 0.535374

Epoch: 97
Loss: 0.247617290665706
RMSE train: 0.446614	val: 0.672486	test: 0.653527
MAE train: 0.348952	val: 0.487042	test: 0.488934

Epoch: 98
Loss: 0.2459381322065989
RMSE train: 0.426523	val: 0.666360	test: 0.640573
MAE train: 0.331098	val: 0.481644	test: 0.480558

Epoch: 99
Loss: 0.2526918947696686
RMSE train: 0.456160	val: 0.683763	test: 0.691844
MAE train: 0.354576	val: 0.494467	test: 0.515878

Epoch: 100
Loss: 0.23764613767464957
RMSE train: 0.478762	val: 0.711707	test: 0.692380
MAE train: 0.377438	val: 0.521285	test: 0.520444

Epoch: 101
Loss: 0.26418661947051686
RMSE train: 0.470751	val: 0.683542	test: 0.684189
MAE train: 0.367469	val: 0.495568	test: 0.514747

Epoch: 102
Loss: 0.25679348905881244
RMSE train: 0.428396	val: 0.670669	test: 0.659186
MAE train: 0.333890	val: 0.484068	test: 0.494357

Epoch: 103
Loss: 0.25034429008762044
RMSE train: 0.428643	val: 0.679324	test: 0.659252
MAE train: 0.335476	val: 0.486283	test: 0.491747

Epoch: 104
Loss: 0.25055306653181714
RMSE train: 0.474538	val: 0.698982	test: 0.699326
MAE train: 0.370562	val: 0.512228	test: 0.522125

Epoch: 105
Loss: 0.25042666494846344
RMSE train: 0.445688	val: 0.675454	test: 0.664737
MAE train: 0.348353	val: 0.489242	test: 0.501317

Epoch: 106
Loss: 0.24676856398582458
RMSE train: 0.496055	val: 0.719474	test: 0.712284
MAE train: 0.394873	val: 0.520778	test: 0.541194

Epoch: 107
Loss: 0.25545388956864673
RMSE train: 0.458627	val: 0.686318	test: 0.675673
MAE train: 0.362528	val: 0.505564	test: 0.511403

Epoch: 108
Loss: 0.24765129635731378
RMSE train: 0.490453	val: 0.713049	test: 0.697853
MAE train: 0.388832	val: 0.523419	test: 0.534915

Epoch: 109
Loss: 0.24392476305365562
RMSE train: 0.452826	val: 0.698608	test: 0.664508
MAE train: 0.356539	val: 0.506406	test: 0.503543

Epoch: 110
Loss: 0.2436609404782454
RMSE train: 0.444456	val: 0.675899	test: 0.653661
MAE train: 0.346971	val: 0.486983	test: 0.491833

Epoch: 111
Loss: 0.24387777224183083
RMSE train: 0.486909	val: 0.716515	test: 0.716893
MAE train: 0.384147	val: 0.521078	test: 0.543342

Epoch: 112
Loss: 0.23378503819306692
RMSE train: 0.430450	val: 0.685419	test: 0.664368
MAE train: 0.336494	val: 0.493317	test: 0.500379

Epoch: 113
Loss: 0.23570178697506586
RMSE train: 0.533890	val: 0.756406	test: 0.745831
MAE train: 0.430073	val: 0.564315	test: 0.571788

Epoch: 114
Loss: 0.22888672476013502
RMSE train: 0.430511	val: 0.684081	test: 0.660943
MAE train: 0.333834	val: 0.495540	test: 0.496700

Epoch: 115
Loss: 0.2310147906343142
RMSE train: 0.426442	val: 0.683749	test: 0.657986
MAE train: 0.334316	val: 0.494052	test: 0.495934

Epoch: 116
Loss: 0.23056062186757723
RMSE train: 0.465736	val: 0.705738	test: 0.698137
MAE train: 0.367612	val: 0.512550	test: 0.528656

Epoch: 117
Loss: 0.23511902863780657
RMSE train: 0.456010	val: 0.693865	test: 0.678164
MAE train: 0.358984	val: 0.504944	test: 0.511777

Epoch: 118
Loss: 0.22970102727413177
RMSE train: 0.422713	val: 0.668809	test: 0.655434
MAE train: 0.328275	val: 0.483287	test: 0.490746

Epoch: 119
Loss: 0.23392293602228165
RMSE train: 0.467263	val: 0.704672	test: 0.670488
MAE train: 0.368136	val: 0.514617	test: 0.506170

Epoch: 120
Loss: 0.23347806806365648
RMSE train: 0.464277	val: 0.704985	test: 0.703442
MAE train: 0.364860	val: 0.514794	test: 0.529326

Epoch: 121
Loss: 0.2257809117436409
RMSE train: 0.421331	val: 0.676171	test: 0.668085
MAE train: 0.329874	val: 0.482274	test: 0.499468

Epoch: 122
Loss: 0.2278682105243206
RMSE train: 0.429800	val: 0.695188	test: 0.669500
MAE train: 0.335612	val: 0.503741	test: 0.495854

Epoch: 123
Loss: 0.21955588459968567
RMSE train: 0.450319	val: 0.689533	test: 0.689658
MAE train: 0.354760	val: 0.498471	test: 0.522813

Epoch: 124
Loss: 0.21314775571227074
RMSE train: 0.413260	val: 0.668758	test: 0.654383
MAE train: 0.321291	val: 0.484822	test: 0.485570

Epoch: 125
Loss: 0.21550974870721498
RMSE train: 0.422177	val: 0.671470	test: 0.651967
MAE train: 0.327858	val: 0.483168	test: 0.488237

Epoch: 126
Loss: 0.22523488476872444
RMSE train: 0.450438	val: 0.692860	test: 0.682455
MAE train: 0.357102	val: 0.498454	test: 0.518337

Epoch: 127
Loss: 0.20955898612737656
RMSE train: 0.421379	val: 0.680927	test: 0.673456
MAE train: 0.330753	val: 0.490390	test: 0.503410

Epoch: 128
Loss: 0.21290846914052963
RMSE train: 0.430018	val: 0.672821	test: 0.662523
MAE train: 0.335604	val: 0.489736	test: 0.492817

Epoch: 129
Loss: 0.21026995157202086
RMSE train: 0.407631	val: 0.674098	test: 0.672827
MAE train: 0.316426	val: 0.478300	test: 0.497261

Epoch: 130
Loss: 0.21130157386263212
RMSE train: 0.445008	val: 0.690030	test: 0.687566
MAE train: 0.353068	val: 0.500470	test: 0.516987

Epoch: 131
Loss: 0.2114009509483973
RMSE train: 0.450461	val: 0.692192	test: 0.691309
MAE train: 0.351422	val: 0.506346	test: 0.520088

Epoch: 132
Loss: 0.21993901953101158
RMSE train: 0.434753	val: 0.674918	test: 0.671959
MAE train: 0.341976	val: 0.488445	test: 0.504654

Epoch: 133
Loss: 0.2152633716662725
RMSE train: 0.507059	val: 0.739183	test: 0.728832
MAE train: 0.409347	val: 0.546835	test: 0.560340

Early stopping
Best (RMSE):	 train: 0.426523	val: 0.666360	test: 0.640573
Best (MAE):	 train: 0.331098	val: 0.481644	test: 0.480558

MAE train: 0.374550	val: 0.534866	test: 0.495841

Epoch: 84
Loss: 0.2903903680188315
RMSE train: 0.479072	val: 0.708584	test: 0.657535
MAE train: 0.371820	val: 0.531801	test: 0.489518

Epoch: 85
Loss: 0.2950546624405043
RMSE train: 0.445972	val: 0.695825	test: 0.641606
MAE train: 0.342201	val: 0.516513	test: 0.472959

Epoch: 86
Loss: 0.29903168444122585
RMSE train: 0.504946	val: 0.719861	test: 0.665305
MAE train: 0.395116	val: 0.534627	test: 0.495082

Epoch: 87
Loss: 0.3085720645529883
RMSE train: 0.471271	val: 0.693392	test: 0.641795
MAE train: 0.363381	val: 0.521945	test: 0.469852

Epoch: 88
Loss: 0.3185251610619681
RMSE train: 0.451348	val: 0.683794	test: 0.639606
MAE train: 0.349711	val: 0.506036	test: 0.473250

Epoch: 89
Loss: 0.33944972178765703
RMSE train: 0.471765	val: 0.708232	test: 0.656759
MAE train: 0.367240	val: 0.531390	test: 0.493302

Epoch: 90
Loss: 0.3159752083676202
RMSE train: 0.545766	val: 0.762317	test: 0.715171
MAE train: 0.428611	val: 0.563101	test: 0.544634

Epoch: 91
Loss: 0.29804325316633495
RMSE train: 0.477486	val: 0.713583	test: 0.656473
MAE train: 0.370918	val: 0.530226	test: 0.489474

Epoch: 92
Loss: 0.2967542771782194
RMSE train: 0.450124	val: 0.703576	test: 0.650456
MAE train: 0.348051	val: 0.517387	test: 0.479535

Epoch: 93
Loss: 0.2942572736314365
RMSE train: 0.482504	val: 0.703090	test: 0.640052
MAE train: 0.374529	val: 0.523545	test: 0.481559

Epoch: 94
Loss: 0.27827316735471996
RMSE train: 0.494017	val: 0.729751	test: 0.680834
MAE train: 0.384439	val: 0.540145	test: 0.507775

Epoch: 95
Loss: 0.2907735247697149
RMSE train: 0.450311	val: 0.694502	test: 0.632885
MAE train: 0.348159	val: 0.512113	test: 0.464803

Epoch: 96
Loss: 0.2934803324086325
RMSE train: 0.476694	val: 0.709432	test: 0.658777
MAE train: 0.372217	val: 0.529709	test: 0.492928

Epoch: 97
Loss: 0.30445883529526846
RMSE train: 0.491239	val: 0.723942	test: 0.688500
MAE train: 0.383506	val: 0.541396	test: 0.510055

Epoch: 98
Loss: 0.28047461062669754
RMSE train: 0.468636	val: 0.707065	test: 0.644585
MAE train: 0.363769	val: 0.523606	test: 0.473083

Epoch: 99
Loss: 0.2784140578338078
RMSE train: 0.497265	val: 0.735268	test: 0.667069
MAE train: 0.388233	val: 0.546588	test: 0.488942

Epoch: 100
Loss: 0.2950618096760341
RMSE train: 0.481924	val: 0.731765	test: 0.676783
MAE train: 0.375025	val: 0.544361	test: 0.496729

Epoch: 101
Loss: 0.2847759851387569
RMSE train: 0.452174	val: 0.690298	test: 0.632668
MAE train: 0.350850	val: 0.510031	test: 0.464597

Epoch: 102
Loss: 0.27453504502773285
RMSE train: 0.448220	val: 0.693288	test: 0.650042
MAE train: 0.347486	val: 0.506437	test: 0.482232

Epoch: 103
Loss: 0.27098366405282703
RMSE train: 0.436902	val: 0.693744	test: 0.647704
MAE train: 0.338620	val: 0.516331	test: 0.475862

Epoch: 104
Loss: 0.27567670813628603
RMSE train: 0.464109	val: 0.704561	test: 0.654809
MAE train: 0.361821	val: 0.519922	test: 0.484712

Epoch: 105
Loss: 0.2645684480667114
RMSE train: 0.462140	val: 0.695726	test: 0.671069
MAE train: 0.354684	val: 0.512145	test: 0.487703

Epoch: 106
Loss: 0.267854417009013
RMSE train: 0.468372	val: 0.726099	test: 0.653798
MAE train: 0.364631	val: 0.537351	test: 0.482008

Epoch: 107
Loss: 0.2752059240426336
RMSE train: 0.453894	val: 0.684723	test: 0.658389
MAE train: 0.352367	val: 0.504481	test: 0.486099

Epoch: 108
Loss: 0.28924770653247833
RMSE train: 0.480356	val: 0.691529	test: 0.656727
MAE train: 0.374962	val: 0.521539	test: 0.482656

Epoch: 109
Loss: 0.26766483911446165
RMSE train: 0.470818	val: 0.701495	test: 0.643460
MAE train: 0.364284	val: 0.518656	test: 0.473511

Epoch: 110
Loss: 0.28186867598976406
RMSE train: 0.466021	val: 0.715450	test: 0.666618
MAE train: 0.361422	val: 0.528080	test: 0.492745

Epoch: 111
Loss: 0.25398706857647213
RMSE train: 0.447392	val: 0.709401	test: 0.651917
MAE train: 0.346944	val: 0.520447	test: 0.481649

Epoch: 112
Loss: 0.2515334093144962
RMSE train: 0.477178	val: 0.729978	test: 0.655735
MAE train: 0.373098	val: 0.540204	test: 0.495765

Epoch: 113
Loss: 0.27112630541835514
RMSE train: 0.428711	val: 0.688470	test: 0.634141
MAE train: 0.332637	val: 0.509974	test: 0.464967

Epoch: 114
Loss: 0.2636511187468256
RMSE train: 0.436526	val: 0.690883	test: 0.637735
MAE train: 0.337453	val: 0.506509	test: 0.467666

Epoch: 115
Loss: 0.2766908130475453
RMSE train: 0.462475	val: 0.711941	test: 0.654814
MAE train: 0.359391	val: 0.532767	test: 0.484120

Epoch: 116
Loss: 0.2558722676975386
RMSE train: 0.460120	val: 0.709061	test: 0.659935
MAE train: 0.357365	val: 0.526194	test: 0.484999

Epoch: 117
Loss: 0.25384169284786495
RMSE train: 0.469402	val: 0.703927	test: 0.656313
MAE train: 0.364245	val: 0.526147	test: 0.486098

Epoch: 118
Loss: 0.2528426966496876
RMSE train: 0.481571	val: 0.735132	test: 0.673387
MAE train: 0.378066	val: 0.544678	test: 0.500112

Epoch: 119
Loss: 0.2778297843677657
RMSE train: 0.446916	val: 0.713440	test: 0.651927
MAE train: 0.347138	val: 0.521330	test: 0.483360

Epoch: 120
Loss: 0.26261074521711897
RMSE train: 0.445560	val: 0.695742	test: 0.633596
MAE train: 0.346681	val: 0.512371	test: 0.469270

Epoch: 121
Loss: 0.25454668381384443
RMSE train: 0.423937	val: 0.685509	test: 0.633898
MAE train: 0.329477	val: 0.506118	test: 0.469137

Epoch: 122
Loss: 0.24198392352887563
RMSE train: 0.455534	val: 0.716160	test: 0.644189
MAE train: 0.353685	val: 0.524332	test: 0.476835

Epoch: 123
Loss: 0.2831956882561956
RMSE train: 0.419914	val: 0.684755	test: 0.629039
MAE train: 0.324164	val: 0.502820	test: 0.458841

Early stopping
Best (RMSE):	 train: 0.451348	val: 0.683794	test: 0.639606
Best (MAE):	 train: 0.349711	val: 0.506036	test: 0.473250

MAE train: 0.415065	val: 0.571763	test: 0.514687

Epoch: 84
Loss: 0.3036616263645036
RMSE train: 0.535489	val: 0.763453	test: 0.669615
MAE train: 0.420018	val: 0.567915	test: 0.506173

Epoch: 85
Loss: 0.2971298534955297
RMSE train: 0.499868	val: 0.733772	test: 0.639621
MAE train: 0.385368	val: 0.550415	test: 0.486527

Epoch: 86
Loss: 0.30591084382363726
RMSE train: 0.536360	val: 0.758317	test: 0.677292
MAE train: 0.420207	val: 0.565338	test: 0.518756

Epoch: 87
Loss: 0.29590942178453716
RMSE train: 0.519373	val: 0.763548	test: 0.669886
MAE train: 0.404719	val: 0.574193	test: 0.508689

Epoch: 88
Loss: 0.30341849688972744
RMSE train: 0.557349	val: 0.783250	test: 0.696313
MAE train: 0.445503	val: 0.591819	test: 0.539402

Epoch: 89
Loss: 0.2936705925634929
RMSE train: 0.516714	val: 0.745213	test: 0.665142
MAE train: 0.406985	val: 0.558379	test: 0.507146

Epoch: 90
Loss: 0.29810042679309845
RMSE train: 0.461624	val: 0.705626	test: 0.618049
MAE train: 0.356363	val: 0.523077	test: 0.465030

Epoch: 91
Loss: 0.3030354848929814
RMSE train: 0.497009	val: 0.713061	test: 0.637980
MAE train: 0.387021	val: 0.536157	test: 0.482219

Epoch: 92
Loss: 0.3008765173809869
RMSE train: 0.564761	val: 0.784189	test: 0.715399
MAE train: 0.445096	val: 0.591346	test: 0.553516

Epoch: 93
Loss: 0.29365009708063944
RMSE train: 0.550402	val: 0.772676	test: 0.691650
MAE train: 0.433128	val: 0.578856	test: 0.532647

Epoch: 94
Loss: 0.3016752770968846
RMSE train: 0.498929	val: 0.740276	test: 0.638380
MAE train: 0.386018	val: 0.545901	test: 0.480571

Epoch: 95
Loss: 0.281078077852726
RMSE train: 0.487755	val: 0.730676	test: 0.636484
MAE train: 0.378448	val: 0.544072	test: 0.483689

Epoch: 96
Loss: 0.2913223345364843
RMSE train: 0.485766	val: 0.726804	test: 0.658701
MAE train: 0.377872	val: 0.552100	test: 0.503053

Epoch: 97
Loss: 0.28344523161649704
RMSE train: 0.483222	val: 0.734134	test: 0.637691
MAE train: 0.375711	val: 0.549788	test: 0.481694

Epoch: 98
Loss: 0.2949076401335852
RMSE train: 0.477519	val: 0.723196	test: 0.653907
MAE train: 0.370779	val: 0.536092	test: 0.491277

Epoch: 99
Loss: 0.28838119549410685
RMSE train: 0.457733	val: 0.707598	test: 0.628362
MAE train: 0.354044	val: 0.525953	test: 0.476651

Epoch: 100
Loss: 0.30068689584732056
RMSE train: 0.514551	val: 0.738127	test: 0.663399
MAE train: 0.401102	val: 0.556659	test: 0.503044

Epoch: 101
Loss: 0.30888294747897554
RMSE train: 0.484487	val: 0.717846	test: 0.616485
MAE train: 0.375685	val: 0.545800	test: 0.470084

Epoch: 102
Loss: 0.28413652947970797
RMSE train: 0.499922	val: 0.732265	test: 0.659700
MAE train: 0.390404	val: 0.554466	test: 0.497410

Epoch: 103
Loss: 0.27923743426799774
RMSE train: 0.571324	val: 0.790556	test: 0.729519
MAE train: 0.453774	val: 0.602040	test: 0.561521

Epoch: 104
Loss: 0.27865997701883316
RMSE train: 0.497434	val: 0.732881	test: 0.646073
MAE train: 0.384690	val: 0.552887	test: 0.488183

Epoch: 105
Loss: 0.2782620351229395
RMSE train: 0.469669	val: 0.718743	test: 0.632774
MAE train: 0.363647	val: 0.535862	test: 0.476372

Epoch: 106
Loss: 0.27370946002858026
RMSE train: 0.459358	val: 0.714470	test: 0.635063
MAE train: 0.356499	val: 0.542917	test: 0.473685

Epoch: 107
Loss: 0.275594277041299
RMSE train: 0.456180	val: 0.712675	test: 0.605762
MAE train: 0.353685	val: 0.528474	test: 0.457036

Epoch: 108
Loss: 0.2814908485327448
RMSE train: 0.456690	val: 0.720570	test: 0.639797
MAE train: 0.354120	val: 0.532711	test: 0.479344

Epoch: 109
Loss: 0.263968326151371
RMSE train: 0.472211	val: 0.724149	test: 0.643635
MAE train: 0.364917	val: 0.539118	test: 0.486882

Epoch: 110
Loss: 0.27728079152958734
RMSE train: 0.529706	val: 0.765388	test: 0.686047
MAE train: 0.419205	val: 0.586114	test: 0.524105

Epoch: 111
Loss: 0.2623918258718082
RMSE train: 0.514329	val: 0.745494	test: 0.688529
MAE train: 0.403812	val: 0.568027	test: 0.522580

Epoch: 112
Loss: 0.26754680808101383
RMSE train: 0.528929	val: 0.765056	test: 0.709991
MAE train: 0.415318	val: 0.580909	test: 0.540673

Epoch: 113
Loss: 0.2614494743091719
RMSE train: 0.508224	val: 0.740082	test: 0.671520
MAE train: 0.401781	val: 0.560226	test: 0.516976

Epoch: 114
Loss: 0.26862497734172003
RMSE train: 0.463539	val: 0.722964	test: 0.630755
MAE train: 0.361259	val: 0.543096	test: 0.475348

Epoch: 115
Loss: 0.2631901672908238
RMSE train: 0.513142	val: 0.753332	test: 0.674267
MAE train: 0.398814	val: 0.562940	test: 0.512308

Epoch: 116
Loss: 0.2450514297400202
RMSE train: 0.493209	val: 0.737876	test: 0.661418
MAE train: 0.387207	val: 0.557426	test: 0.500160

Epoch: 117
Loss: 0.258442291191646
RMSE train: 0.449755	val: 0.713579	test: 0.631476
MAE train: 0.347642	val: 0.536934	test: 0.470221

Epoch: 118
Loss: 0.2512744132961546
RMSE train: 0.471102	val: 0.711701	test: 0.630047
MAE train: 0.366161	val: 0.536029	test: 0.469642

Epoch: 119
Loss: 0.2552169497523989
RMSE train: 0.457322	val: 0.718467	test: 0.653998
MAE train: 0.355552	val: 0.532417	test: 0.492707

Epoch: 120
Loss: 0.2536207841975348
RMSE train: 0.454482	val: 0.716802	test: 0.643777
MAE train: 0.352358	val: 0.537990	test: 0.480258

Epoch: 121
Loss: 0.26611526736191343
RMSE train: 0.458937	val: 0.717250	test: 0.634702
MAE train: 0.358765	val: 0.543270	test: 0.472922

Epoch: 122
Loss: 0.27312107171331135
RMSE train: 0.474526	val: 0.726013	test: 0.653200
MAE train: 0.371005	val: 0.549134	test: 0.490392

Epoch: 123
Loss: 0.24471153425318853
RMSE train: 0.481975	val: 0.729796	test: 0.647660
MAE train: 0.376039	val: 0.541494	test: 0.481518

Epoch: 124
Loss: 0.2680411541036197
RMSE train: 0.464458	val: 0.728809	test: 0.662448
MAE train: 0.359746	val: 0.551066	test: 0.492315

Epoch: 125
Loss: 0.2591681735856192
RMSE train: 0.467857	val: 0.730876	test: 0.645315
MAE train: 0.365096	val: 0.541932	test: 0.486733

Early stopping
Best (RMSE):	 train: 0.461624	val: 0.705626	test: 0.618049
Best (MAE):	 train: 0.356363	val: 0.523077	test: 0.465030

MAE train: 0.368967	val: 0.491429	test: 0.498365

Epoch: 84
Loss: 0.2746443673968315
RMSE train: 0.480741	val: 0.681639	test: 0.672552
MAE train: 0.378433	val: 0.500004	test: 0.506549

Epoch: 85
Loss: 0.2752656837304433
RMSE train: 0.466521	val: 0.666212	test: 0.664217
MAE train: 0.363633	val: 0.490922	test: 0.502935

Epoch: 86
Loss: 0.2861290251215299
RMSE train: 0.462108	val: 0.654424	test: 0.664381
MAE train: 0.359198	val: 0.481306	test: 0.500671

Epoch: 87
Loss: 0.29005232204993564
RMSE train: 0.476242	val: 0.670278	test: 0.673494
MAE train: 0.372787	val: 0.492933	test: 0.511575

Epoch: 88
Loss: 0.27443916350603104
RMSE train: 0.472773	val: 0.679234	test: 0.675171
MAE train: 0.371279	val: 0.503162	test: 0.505902

Epoch: 89
Loss: 0.27603475376963615
RMSE train: 0.481696	val: 0.686767	test: 0.685258
MAE train: 0.380772	val: 0.505943	test: 0.509554

Epoch: 90
Loss: 0.26891319329539937
RMSE train: 0.452503	val: 0.668091	test: 0.669883
MAE train: 0.353561	val: 0.486736	test: 0.500920

Epoch: 91
Loss: 0.2754864903787772
RMSE train: 0.456320	val: 0.665659	test: 0.652938
MAE train: 0.358033	val: 0.492993	test: 0.488152

Epoch: 92
Loss: 0.267989382147789
RMSE train: 0.436004	val: 0.647703	test: 0.660204
MAE train: 0.339226	val: 0.478491	test: 0.495531

Epoch: 93
Loss: 0.26026275753974915
RMSE train: 0.455485	val: 0.668635	test: 0.661398
MAE train: 0.358146	val: 0.495420	test: 0.498484

Epoch: 94
Loss: 0.26203781366348267
RMSE train: 0.452860	val: 0.663522	test: 0.675244
MAE train: 0.354116	val: 0.488361	test: 0.511381

Epoch: 95
Loss: 0.2566057729224364
RMSE train: 0.441297	val: 0.665241	test: 0.659892
MAE train: 0.344532	val: 0.488263	test: 0.496827

Epoch: 96
Loss: 0.26426491513848305
RMSE train: 0.466936	val: 0.684906	test: 0.674886
MAE train: 0.364846	val: 0.501121	test: 0.504378

Epoch: 97
Loss: 0.2533692307770252
RMSE train: 0.434051	val: 0.645953	test: 0.657452
MAE train: 0.335943	val: 0.478548	test: 0.490128

Epoch: 98
Loss: 0.2599530095855395
RMSE train: 0.477981	val: 0.691861	test: 0.691364
MAE train: 0.375583	val: 0.510577	test: 0.517358

Epoch: 99
Loss: 0.24839810778697333
RMSE train: 0.478755	val: 0.702662	test: 0.677808
MAE train: 0.376147	val: 0.516353	test: 0.504295

Epoch: 100
Loss: 0.25191496312618256
RMSE train: 0.453325	val: 0.675228	test: 0.667151
MAE train: 0.355717	val: 0.501680	test: 0.500759

Epoch: 101
Loss: 0.25142572323481244
RMSE train: 0.444959	val: 0.663054	test: 0.653698
MAE train: 0.345001	val: 0.490954	test: 0.488427

Epoch: 102
Loss: 0.24826208874583244
RMSE train: 0.430558	val: 0.646458	test: 0.657650
MAE train: 0.335264	val: 0.481056	test: 0.497736

Epoch: 103
Loss: 0.2525950161119302
RMSE train: 0.421595	val: 0.656138	test: 0.658946
MAE train: 0.327365	val: 0.481224	test: 0.491362

Epoch: 104
Loss: 0.24978715802232423
RMSE train: 0.420675	val: 0.650499	test: 0.641753
MAE train: 0.327912	val: 0.473275	test: 0.476921

Epoch: 105
Loss: 0.26453107347091037
RMSE train: 0.421630	val: 0.648855	test: 0.643477
MAE train: 0.328269	val: 0.476719	test: 0.480978

Epoch: 106
Loss: 0.24926290909449259
RMSE train: 0.436411	val: 0.657190	test: 0.666543
MAE train: 0.339919	val: 0.476611	test: 0.497145

Epoch: 107
Loss: 0.25877755259474117
RMSE train: 0.472381	val: 0.701603	test: 0.676108
MAE train: 0.374172	val: 0.516373	test: 0.511853

Epoch: 108
Loss: 0.2506196101506551
RMSE train: 0.468147	val: 0.683830	test: 0.682714
MAE train: 0.369284	val: 0.505685	test: 0.513796

Epoch: 109
Loss: 0.24683321515719095
RMSE train: 0.433649	val: 0.656217	test: 0.663498
MAE train: 0.336015	val: 0.480962	test: 0.501923

Epoch: 110
Loss: 0.2464693933725357
RMSE train: 0.432612	val: 0.656344	test: 0.656215
MAE train: 0.336917	val: 0.480305	test: 0.487288

Epoch: 111
Loss: 0.23635326946775118
RMSE train: 0.416447	val: 0.653690	test: 0.654068
MAE train: 0.324510	val: 0.477774	test: 0.480514

Epoch: 112
Loss: 0.24816777184605598
RMSE train: 0.452731	val: 0.678141	test: 0.674638
MAE train: 0.355030	val: 0.498429	test: 0.504593

Epoch: 113
Loss: 0.246382142106692
RMSE train: 0.429745	val: 0.649346	test: 0.655163
MAE train: 0.334519	val: 0.474343	test: 0.488383

Epoch: 114
Loss: 0.24315020690361658
RMSE train: 0.461161	val: 0.683934	test: 0.678418
MAE train: 0.359797	val: 0.506038	test: 0.502157

Epoch: 115
Loss: 0.2229787533481916
RMSE train: 0.402521	val: 0.647505	test: 0.641779
MAE train: 0.311016	val: 0.473528	test: 0.476546

Epoch: 116
Loss: 0.2408097249766191
RMSE train: 0.406344	val: 0.653050	test: 0.639160
MAE train: 0.316545	val: 0.477378	test: 0.473269

Epoch: 117
Loss: 0.2332561065753301
RMSE train: 0.395515	val: 0.645291	test: 0.641226
MAE train: 0.307577	val: 0.472466	test: 0.474715

Epoch: 118
Loss: 0.23693632086118063
RMSE train: 0.422681	val: 0.655784	test: 0.656061
MAE train: 0.331128	val: 0.476381	test: 0.485118

Epoch: 119
Loss: 0.21853080640236536
RMSE train: 0.422650	val: 0.673445	test: 0.647466
MAE train: 0.331446	val: 0.490185	test: 0.483129

Epoch: 120
Loss: 0.22871892899274826
RMSE train: 0.442959	val: 0.665439	test: 0.656652
MAE train: 0.345781	val: 0.490193	test: 0.491679

Epoch: 121
Loss: 0.22578351199626923
RMSE train: 0.462821	val: 0.688510	test: 0.695018
MAE train: 0.361074	val: 0.508107	test: 0.516745

Epoch: 122
Loss: 0.22521591559052467
RMSE train: 0.402801	val: 0.654931	test: 0.643910
MAE train: 0.313640	val: 0.474281	test: 0.483110

Epoch: 123
Loss: 0.22646926840146384
RMSE train: 0.448364	val: 0.683428	test: 0.681789
MAE train: 0.353697	val: 0.498390	test: 0.509924

Epoch: 124
Loss: 0.22641016418735185
RMSE train: 0.453591	val: 0.696424	test: 0.679187
MAE train: 0.358214	val: 0.505386	test: 0.512071

Epoch: 125
Loss: 0.21737063055237135
RMSE train: 0.408879	val: 0.660227	test: 0.655627
MAE train: 0.319746	val: 0.478260	test: 0.489017

Epoch: 126
Loss: 0.2264102337261041
RMSE train: 0.425794	val: 0.673928	test: 0.659488
MAE train: 0.333193	val: 0.487842	test: 0.487751

Epoch: 127
Loss: 0.22370568414529166
RMSE train: 0.414830	val: 0.664590	test: 0.648839
MAE train: 0.323313	val: 0.481325	test: 0.480425

Epoch: 128
Loss: 0.22294609621167183
RMSE train: 0.420650	val: 0.661956	test: 0.655967
MAE train: 0.328905	val: 0.481508	test: 0.492114

Epoch: 129
Loss: 0.21431100244323412
RMSE train: 0.440523	val: 0.674933	test: 0.673219
MAE train: 0.343602	val: 0.495518	test: 0.504637

Epoch: 130
Loss: 0.2215991715590159
RMSE train: 0.397826	val: 0.653643	test: 0.651762
MAE train: 0.310898	val: 0.473507	test: 0.484034

Epoch: 131
Loss: 0.22522565349936485
RMSE train: 0.410795	val: 0.660483	test: 0.646228
MAE train: 0.319831	val: 0.477824	test: 0.481329

Epoch: 132
Loss: 0.21866215268770853
RMSE train: 0.450167	val: 0.683887	test: 0.678592
MAE train: 0.352764	val: 0.506375	test: 0.509775

Epoch: 133
Loss: 0.21709796786308289
RMSE train: 0.414726	val: 0.666663	test: 0.655518
MAE train: 0.322332	val: 0.484889	test: 0.483759

Epoch: 134
Loss: 0.20827106262246767
RMSE train: 0.425345	val: 0.675987	test: 0.658742
MAE train: 0.330792	val: 0.495696	test: 0.492170

Epoch: 135
Loss: 0.21640942990779877
RMSE train: 0.390078	val: 0.646325	test: 0.641396
MAE train: 0.302783	val: 0.465621	test: 0.476054

Epoch: 136
Loss: 0.20987553521990776
RMSE train: 0.391082	val: 0.657565	test: 0.636528
MAE train: 0.304534	val: 0.475463	test: 0.471078

Epoch: 137
Loss: 0.20737835640708605
RMSE train: 0.418706	val: 0.685310	test: 0.668071
MAE train: 0.327604	val: 0.501034	test: 0.496180

Epoch: 138
Loss: 0.2055246556798617
RMSE train: 0.415641	val: 0.681247	test: 0.652623
MAE train: 0.326140	val: 0.491271	test: 0.490891

Epoch: 139
Loss: 0.2120747131605943
RMSE train: 0.381147	val: 0.649934	test: 0.636616
MAE train: 0.296130	val: 0.472533	test: 0.479109

Epoch: 140
Loss: 0.20317732666929564
RMSE train: 0.421795	val: 0.662846	test: 0.651474
MAE train: 0.327931	val: 0.483674	test: 0.489084

Epoch: 141
Loss: 0.22328285748759905
RMSE train: 0.431933	val: 0.693448	test: 0.659713
MAE train: 0.337060	val: 0.507407	test: 0.484266

Epoch: 142
Loss: 0.21327523762981096
RMSE train: 0.405907	val: 0.668791	test: 0.643341
MAE train: 0.314260	val: 0.486931	test: 0.479552

Epoch: 143
Loss: 0.222422211120526
RMSE train: 0.412860	val: 0.663614	test: 0.656029
MAE train: 0.320215	val: 0.486857	test: 0.491271

Epoch: 144
Loss: 0.2037145271897316
RMSE train: 0.406951	val: 0.664643	test: 0.670032
MAE train: 0.318945	val: 0.480886	test: 0.504535

Epoch: 145
Loss: 0.20797972232103348
RMSE train: 0.373719	val: 0.639005	test: 0.662888
MAE train: 0.291725	val: 0.461302	test: 0.499713

Epoch: 146
Loss: 0.21636841148138047
RMSE train: 0.387123	val: 0.645314	test: 0.663482
MAE train: 0.302291	val: 0.469075	test: 0.499634

Epoch: 147
Loss: 0.2052777349948883
RMSE train: 0.389246	val: 0.652256	test: 0.658709
MAE train: 0.304081	val: 0.471176	test: 0.493658

Epoch: 148
Loss: 0.2046952113509178
RMSE train: 0.377008	val: 0.633801	test: 0.665965
MAE train: 0.295972	val: 0.462681	test: 0.505397

Epoch: 149
Loss: 0.20647145211696624
RMSE train: 0.383857	val: 0.634156	test: 0.666474
MAE train: 0.300417	val: 0.466659	test: 0.503135

Epoch: 150
Loss: 0.2045674979686737
RMSE train: 0.387259	val: 0.654793	test: 0.664660
MAE train: 0.303482	val: 0.473330	test: 0.503088

Epoch: 151
Loss: 0.2036614775657654
RMSE train: 0.434418	val: 0.672614	test: 0.705222
MAE train: 0.343596	val: 0.492517	test: 0.534563

Epoch: 152
Loss: 0.20144591033458709
RMSE train: 0.400791	val: 0.659045	test: 0.670083
MAE train: 0.314862	val: 0.480110	test: 0.506153

Epoch: 153
Loss: 0.20912516862154007
RMSE train: 0.412986	val: 0.661653	test: 0.683130
MAE train: 0.326049	val: 0.480137	test: 0.516695

Epoch: 154
Loss: 0.2082343354821205
RMSE train: 0.418549	val: 0.671389	test: 0.700465
MAE train: 0.329276	val: 0.491036	test: 0.527044

Epoch: 155
Loss: 0.206318262219429
RMSE train: 0.401157	val: 0.645107	test: 0.676724
MAE train: 0.310852	val: 0.471526	test: 0.503367

Epoch: 156
Loss: 0.20914433151483536
RMSE train: 0.391527	val: 0.634303	test: 0.682693
MAE train: 0.306784	val: 0.461457	test: 0.514946

Epoch: 157
Loss: 0.20051272213459015
RMSE train: 0.434952	val: 0.666294	test: 0.695106
MAE train: 0.347437	val: 0.488677	test: 0.528965

Epoch: 158
Loss: 0.20165506303310393
RMSE train: 0.393488	val: 0.654553	test: 0.664730
MAE train: 0.310853	val: 0.476297	test: 0.505360

Epoch: 159
Loss: 0.20511489361524582
RMSE train: 0.375309	val: 0.649092	test: 0.665877
MAE train: 0.295419	val: 0.465332	test: 0.498236

Epoch: 160
Loss: 0.20510514974594116
RMSE train: 0.416471	val: 0.674120	test: 0.701240
MAE train: 0.331224	val: 0.491663	test: 0.527578

Epoch: 161
Loss: 0.19796273857355118
RMSE train: 0.397172	val: 0.667009	test: 0.670789
MAE train: 0.314363	val: 0.483876	test: 0.508585

Epoch: 162
Loss: 0.19378480911254883
RMSE train: 0.376441	val: 0.641474	test: 0.671676
MAE train: 0.296135	val: 0.466843	test: 0.506174

Epoch: 163
Loss: 0.19450474083423613
RMSE train: 0.377397	val: 0.641793	test: 0.659323
MAE train: 0.293946	val: 0.461742	test: 0.494117

Epoch: 164
Loss: 0.20499903708696365
RMSE train: 0.397101	val: 0.661393	test: 0.679327
MAE train: 0.314951	val: 0.479841	test: 0.505581

Epoch: 165
Loss: 0.19054387956857682
RMSE train: 0.385552	val: 0.652962	test: 0.658074
MAE train: 0.301356	val: 0.466329	test: 0.495540

Epoch: 166
Loss: 0.18573656380176545
RMSE train: 0.385044	val: 0.644432	test: 0.676771
MAE train: 0.301951	val: 0.466241	test: 0.508856

Epoch: 167
Loss: 0.1897469565272331
RMSE train: 0.465526	val: 0.687415	test: 0.709319
MAE train: 0.373555	val: 0.507863	test: 0.542008

Epoch: 168
Loss: 0.18682073801755905
RMSE train: 0.419343	val: 0.670899	test: 0.685931
MAE train: 0.334900	val: 0.489400	test: 0.517888

Early stopping
Best (RMSE):	 train: 0.403513	val: 0.632589	test: 0.679969
Best (MAE):	 train: 0.315204	val: 0.463540	test: 0.513897
All runs completed.


Epoch: 144
Loss: 0.2115503984193007
RMSE train: 0.426689	val: 0.685177	test: 0.655502
MAE train: 0.329391	val: 0.498634	test: 0.491175

Epoch: 145
Loss: 0.21139232317606607
RMSE train: 0.392378	val: 0.654386	test: 0.643400
MAE train: 0.302899	val: 0.472521	test: 0.483993

Epoch: 146
Loss: 0.19974190990130106
RMSE train: 0.429015	val: 0.690863	test: 0.670665
MAE train: 0.334433	val: 0.500432	test: 0.492482

Epoch: 147
Loss: 0.2077009677886963
RMSE train: 0.408877	val: 0.666892	test: 0.644427
MAE train: 0.317343	val: 0.486924	test: 0.486456

Epoch: 148
Loss: 0.20954407875736555
RMSE train: 0.435710	val: 0.696600	test: 0.682229
MAE train: 0.341619	val: 0.507680	test: 0.506812

Epoch: 149
Loss: 0.20951625083883604
RMSE train: 0.405649	val: 0.664161	test: 0.654436
MAE train: 0.314619	val: 0.482838	test: 0.486491

Epoch: 150
Loss: 0.19799759487311044
RMSE train: 0.409902	val: 0.662375	test: 0.655625
MAE train: 0.319888	val: 0.482514	test: 0.489375

Epoch: 151
Loss: 0.2019802692035834
RMSE train: 0.377113	val: 0.653825	test: 0.638068
MAE train: 0.292290	val: 0.468041	test: 0.472123

Epoch: 152
Loss: 0.20443875342607498
RMSE train: 0.394458	val: 0.661141	test: 0.638763
MAE train: 0.304487	val: 0.475988	test: 0.475066

Early stopping
Best (RMSE):	 train: 0.395515	val: 0.645291	test: 0.641226
Best (MAE):	 train: 0.307577	val: 0.472466	test: 0.474715
All runs completed.

MAE train: 0.375069	val: 0.535744	test: 0.484340

Epoch: 84
Loss: 0.32323154168469564
RMSE train: 0.572353	val: 0.789022	test: 0.703509
MAE train: 0.453342	val: 0.598730	test: 0.535131

Epoch: 85
Loss: 0.3083556869200298
RMSE train: 0.494661	val: 0.718874	test: 0.644673
MAE train: 0.383157	val: 0.539056	test: 0.484007

Epoch: 86
Loss: 0.3028190221105303
RMSE train: 0.469920	val: 0.697742	test: 0.645428
MAE train: 0.363727	val: 0.531193	test: 0.483431

Epoch: 87
Loss: 0.31219527976853506
RMSE train: 0.524466	val: 0.739829	test: 0.670207
MAE train: 0.407879	val: 0.560850	test: 0.502073

Epoch: 88
Loss: 0.3006222354514258
RMSE train: 0.504843	val: 0.741453	test: 0.683669
MAE train: 0.391554	val: 0.556585	test: 0.504311

Epoch: 89
Loss: 0.2987359826053892
RMSE train: 0.542403	val: 0.779036	test: 0.702357
MAE train: 0.422374	val: 0.587531	test: 0.535240

Epoch: 90
Loss: 0.2817001598221915
RMSE train: 0.475654	val: 0.701611	test: 0.632200
MAE train: 0.365727	val: 0.523470	test: 0.472093

Epoch: 91
Loss: 0.2803300693631172
RMSE train: 0.485528	val: 0.719964	test: 0.654141
MAE train: 0.376612	val: 0.542333	test: 0.484592

Epoch: 92
Loss: 0.29481218116624014
RMSE train: 0.492636	val: 0.733311	test: 0.673882
MAE train: 0.382383	val: 0.555460	test: 0.512237

Epoch: 93
Loss: 0.28935370381389347
RMSE train: 0.561135	val: 0.783579	test: 0.714591
MAE train: 0.442008	val: 0.593361	test: 0.545296

Epoch: 94
Loss: 0.2926464421408517
RMSE train: 0.542998	val: 0.754054	test: 0.695924
MAE train: 0.426117	val: 0.578318	test: 0.533734

Epoch: 95
Loss: 0.29822572427136557
RMSE train: 0.471151	val: 0.719731	test: 0.649785
MAE train: 0.363704	val: 0.539883	test: 0.485270

Epoch: 96
Loss: 0.2841156082493918
RMSE train: 0.479819	val: 0.729237	test: 0.657156
MAE train: 0.374415	val: 0.547243	test: 0.489021

Epoch: 97
Loss: 0.2733703872987202
RMSE train: 0.492998	val: 0.738013	test: 0.679587
MAE train: 0.381859	val: 0.556330	test: 0.511120

Epoch: 98
Loss: 0.29064788669347763
RMSE train: 0.449950	val: 0.686295	test: 0.631615
MAE train: 0.350243	val: 0.520277	test: 0.470593

Epoch: 99
Loss: 0.2622519029038293
RMSE train: 0.473200	val: 0.709806	test: 0.665931
MAE train: 0.367095	val: 0.539195	test: 0.497850

Epoch: 100
Loss: 0.27072480427367346
RMSE train: 0.477074	val: 0.718647	test: 0.656593
MAE train: 0.372102	val: 0.541099	test: 0.494579

Epoch: 101
Loss: 0.27812160764421734
RMSE train: 0.475603	val: 0.705808	test: 0.635730
MAE train: 0.369613	val: 0.529037	test: 0.477382

Epoch: 102
Loss: 0.27621603650706156
RMSE train: 0.458356	val: 0.709480	test: 0.642936
MAE train: 0.354927	val: 0.535828	test: 0.477500

Epoch: 103
Loss: 0.2695995845964977
RMSE train: 0.514336	val: 0.743807	test: 0.672676
MAE train: 0.404766	val: 0.557857	test: 0.507320

Epoch: 104
Loss: 0.27684285811015535
RMSE train: 0.454818	val: 0.708873	test: 0.627028
MAE train: 0.352779	val: 0.528872	test: 0.466383

Epoch: 105
Loss: 0.2623215317726135
RMSE train: 0.456199	val: 0.706608	test: 0.631539
MAE train: 0.356293	val: 0.527745	test: 0.478644

Epoch: 106
Loss: 0.25375025825841085
RMSE train: 0.515949	val: 0.755023	test: 0.672595
MAE train: 0.408359	val: 0.572107	test: 0.519014

Epoch: 107
Loss: 0.24927278608083725
RMSE train: 0.480260	val: 0.726011	test: 0.661701
MAE train: 0.373760	val: 0.550810	test: 0.502393

Epoch: 108
Loss: 0.26665934920310974
RMSE train: 0.442610	val: 0.689345	test: 0.646165
MAE train: 0.342900	val: 0.516896	test: 0.481359

Epoch: 109
Loss: 0.2728477492928505
RMSE train: 0.452368	val: 0.687584	test: 0.630146
MAE train: 0.350144	val: 0.523804	test: 0.473481

Epoch: 110
Loss: 0.2676807450396674
RMSE train: 0.473796	val: 0.716406	test: 0.652709
MAE train: 0.365185	val: 0.536164	test: 0.492175

Epoch: 111
Loss: 0.2729099935718945
RMSE train: 0.442014	val: 0.689312	test: 0.634066
MAE train: 0.342515	val: 0.526394	test: 0.471922

Epoch: 112
Loss: 0.271261351449149
RMSE train: 0.486344	val: 0.738378	test: 0.651522
MAE train: 0.381809	val: 0.554783	test: 0.494702

Epoch: 113
Loss: 0.2724947343979563
RMSE train: 0.493542	val: 0.726988	test: 0.660302
MAE train: 0.381502	val: 0.543480	test: 0.494870

Epoch: 114
Loss: 0.2661390719669206
RMSE train: 0.452730	val: 0.701408	test: 0.622745
MAE train: 0.352826	val: 0.530329	test: 0.465720

Epoch: 115
Loss: 0.266598536499909
RMSE train: 0.531210	val: 0.770869	test: 0.709427
MAE train: 0.418945	val: 0.582617	test: 0.528300

Epoch: 116
Loss: 0.2690677610891206
RMSE train: 0.439072	val: 0.699828	test: 0.621831
MAE train: 0.342906	val: 0.530852	test: 0.464769

Epoch: 117
Loss: 0.2657190369708197
RMSE train: 0.469876	val: 0.729355	test: 0.661178
MAE train: 0.367403	val: 0.547683	test: 0.494350

Epoch: 118
Loss: 0.26788507189069477
RMSE train: 0.486077	val: 0.743355	test: 0.651025
MAE train: 0.379851	val: 0.551249	test: 0.499594

Epoch: 119
Loss: 0.2559023467557771
RMSE train: 0.458589	val: 0.713695	test: 0.644128
MAE train: 0.358059	val: 0.538200	test: 0.481060

Epoch: 120
Loss: 0.2554087308900697
RMSE train: 0.458437	val: 0.698520	test: 0.629470
MAE train: 0.355701	val: 0.529894	test: 0.473140

Epoch: 121
Loss: 0.26136117960725513
RMSE train: 0.446429	val: 0.702283	test: 0.637754
MAE train: 0.349406	val: 0.533857	test: 0.477745

Epoch: 122
Loss: 0.25995892712048124
RMSE train: 0.443328	val: 0.698047	test: 0.628183
MAE train: 0.345772	val: 0.529351	test: 0.471513

Epoch: 123
Loss: 0.26971360615321566
RMSE train: 0.502844	val: 0.733618	test: 0.672717
MAE train: 0.393699	val: 0.557221	test: 0.510720

Epoch: 124
Loss: 0.244502278310912
RMSE train: 0.481657	val: 0.726523	test: 0.657258
MAE train: 0.380145	val: 0.553809	test: 0.501061

Epoch: 125
Loss: 0.24744564720562526
RMSE train: 0.447078	val: 0.711330	test: 0.624023
MAE train: 0.349365	val: 0.538347	test: 0.466816

Epoch: 126
Loss: 0.24879657583577292
RMSE train: 0.465306	val: 0.722245	test: 0.655967
MAE train: 0.361189	val: 0.546693	test: 0.501335

Epoch: 127
Loss: 0.23945762429918563
RMSE train: 0.433470	val: 0.714595	test: 0.634739
MAE train: 0.335452	val: 0.533188	test: 0.475104

Epoch: 128
Loss: 0.24111979135445186
RMSE train: 0.488804	val: 0.739478	test: 0.670225
MAE train: 0.382934	val: 0.554465	test: 0.511717

Epoch: 129
Loss: 0.23804103370223725
RMSE train: 0.423572	val: 0.700191	test: 0.626865
MAE train: 0.328303	val: 0.517806	test: 0.467289

Epoch: 130
Loss: 0.28651097204004017
RMSE train: 0.472174	val: 0.740491	test: 0.664461
MAE train: 0.368958	val: 0.554703	test: 0.507141

Epoch: 131
Loss: 0.25509340316057205
RMSE train: 0.437302	val: 0.709019	test: 0.653056
MAE train: 0.342379	val: 0.537264	test: 0.488914

Epoch: 132
Loss: 0.24811928399971553
RMSE train: 0.445535	val: 0.713281	test: 0.645213
MAE train: 0.347869	val: 0.539194	test: 0.487283

Epoch: 133
Loss: 0.2301159075328282
RMSE train: 0.454069	val: 0.709612	test: 0.637796
MAE train: 0.350685	val: 0.530984	test: 0.481281

Early stopping
Best (RMSE):	 train: 0.449950	val: 0.686295	test: 0.631615
Best (MAE):	 train: 0.350243	val: 0.520277	test: 0.470593
All runs completed.
