>>> Starting run for dataset: esol
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running RANDOM configs_split_experiments/GraphCL/esol/random/train_prop=0.6.yml on cuda:0
Running RANDOM configs_split_experiments/GraphCL/esol/random/train_prop=0.7.yml on cuda:1
Running RANDOM configs_split_experiments/GraphCL/esol/random/train_prop=0.8.yml on cuda:2
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/esol/random/train_prop=0.6.yml --runseed 4 --device cuda:0
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/esol/random/train_prop=0.6.yml --runseed 5 --device cuda:0
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/esol/random/train_prop=0.8.yml --runseed 4 --device cuda:2
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/esol/random/train_prop=0.8.yml --runseed 5 --device cuda:2
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/esol/random/train_prop=0.6.yml --runseed 6 --device cuda:0
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/esol/random/train_prop=0.8.yml --runseed 6 --device cuda:2
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/esol/random/train_prop=0.7.yml --runseed 4 --device cuda:1
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/esol/random/train_prop=0.7.yml --runseed 5 --device cuda:1
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/esol/random/train_prop=0.7.yml --runseed 6 --device cuda:1
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/esol/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/esol/random/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/esol/random/train_prop=0.6/esol_random_5_26-05_09-43-12  ]
[ Using Seed :  5  ]
[ Using device :  cuda:0  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 13.845980326334635
RMSE train: 3.626999	val: 3.493802	test: 3.507265
MAE train: 3.003862	val: 2.907560	test: 2.916007

Epoch: 2
Loss: 12.369619369506836
RMSE train: 3.577445	val: 3.453337	test: 3.457163
MAE train: 2.980116	val: 2.910401	test: 2.879009

Epoch: 3
Loss: 11.570513089497885
RMSE train: 3.480778	val: 3.342168	test: 3.331835
MAE train: 2.915478	val: 2.844377	test: 2.777280

Epoch: 4
Loss: 10.793540636698404
RMSE train: 3.396104	val: 3.281471	test: 3.220294
MAE train: 2.857597	val: 2.822799	test: 2.690508

Epoch: 5
Loss: 10.24249267578125
RMSE train: 3.373688	val: 3.313237	test: 3.160991
MAE train: 2.860426	val: 2.875852	test: 2.654421

Epoch: 6
Loss: 9.27282746632894
RMSE train: 3.250725	val: 3.221760	test: 3.013937
MAE train: 2.751005	val: 2.774692	test: 2.528817

Epoch: 7
Loss: 8.614840825398764
RMSE train: 3.096335	val: 3.072356	test: 2.853584
MAE train: 2.618983	val: 2.638152	test: 2.396675

Epoch: 8
Loss: 8.218001365661621
RMSE train: 2.900283	val: 2.883746	test: 2.663663
MAE train: 2.446928	val: 2.464626	test: 2.221349

Epoch: 9
Loss: 7.502579689025879
RMSE train: 2.719692	val: 2.698166	test: 2.493192
MAE train: 2.280608	val: 2.290119	test: 2.052107

Epoch: 10
Loss: 6.902381420135498
RMSE train: 2.548892	val: 2.510938	test: 2.334567
MAE train: 2.107752	val: 2.101315	test: 1.897057

Epoch: 11
Loss: 6.512320518493652
RMSE train: 2.497542	val: 2.455877	test: 2.297404
MAE train: 2.055423	val: 2.051465	test: 1.871019

Epoch: 12
Loss: 6.072218259175618
RMSE train: 2.489261	val: 2.451327	test: 2.308669
MAE train: 2.059916	val: 2.058383	test: 1.901609

Epoch: 13
Loss: 5.601991971333821
RMSE train: 2.516549	val: 2.493272	test: 2.347976
MAE train: 2.104473	val: 2.115276	test: 1.959125

Epoch: 14
Loss: 5.193378607432048
RMSE train: 2.472859	val: 2.452813	test: 2.306089
MAE train: 2.075445	val: 2.092256	test: 1.932055

Epoch: 15
Loss: 4.860216776529948
RMSE train: 2.372387	val: 2.348526	test: 2.209965
MAE train: 1.983789	val: 1.998003	test: 1.841723

Epoch: 16
Loss: 4.363893191019694
RMSE train: 2.171122	val: 2.131170	test: 2.026297
MAE train: 1.778742	val: 1.776099	test: 1.645634

Epoch: 17
Loss: 4.073782920837402
RMSE train: 1.990465	val: 1.922418	test: 1.874259
MAE train: 1.617411	val: 1.583653	test: 1.513837

Epoch: 18
Loss: 3.5296186606089273
RMSE train: 1.865378	val: 1.791140	test: 1.775385
MAE train: 1.504118	val: 1.461033	test: 1.431539

Epoch: 19
Loss: 3.222275416056315
RMSE train: 1.726915	val: 1.680548	test: 1.661313
MAE train: 1.364293	val: 1.347687	test: 1.311043

Epoch: 20
Loss: 2.808152993520101
RMSE train: 1.637206	val: 1.615801	test: 1.593358
MAE train: 1.283554	val: 1.282413	test: 1.251223

Epoch: 21
Loss: 2.506718158721924
RMSE train: 1.570347	val: 1.541988	test: 1.543036
MAE train: 1.235697	val: 1.223949	test: 1.217117

Epoch: 22
Loss: 2.3211658000946045
RMSE train: 1.480511	val: 1.473733	test: 1.480440
MAE train: 1.165494	val: 1.165372	test: 1.159751

Epoch: 23
Loss: 2.1654383738835654Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/esol/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/esol/random/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/esol/random/train_prop=0.6/esol_random_4_26-05_09-43-12  ]
[ Using Seed :  4  ]
[ Using device :  cuda:0  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 13.991230646769205
RMSE train: 3.598864	val: 3.448001	test: 3.506985
MAE train: 2.964796	val: 2.823225	test: 2.894679

Epoch: 2
Loss: 12.916174252827963
RMSE train: 3.601931	val: 3.448507	test: 3.507394
MAE train: 2.977187	val: 2.855206	test: 2.899188

Epoch: 3
Loss: 11.939080238342285
RMSE train: 3.543085	val: 3.403377	test: 3.427914
MAE train: 2.955407	val: 2.866996	test: 2.832687

Epoch: 4
Loss: 11.075182914733887
RMSE train: 3.486594	val: 3.403422	test: 3.335614
MAE train: 2.918885	val: 2.892632	test: 2.749950

Epoch: 5
Loss: 10.349395751953125
RMSE train: 3.466791	val: 3.462336	test: 3.271152
MAE train: 2.912519	val: 2.924638	test: 2.693389

Epoch: 6
Loss: 9.472618420918783
RMSE train: 3.413922	val: 3.427891	test: 3.191942
MAE train: 2.900641	val: 2.925676	test: 2.655658

Epoch: 7
Loss: 8.90320110321045
RMSE train: 3.361932	val: 3.333873	test: 3.164820
MAE train: 2.897734	val: 2.904898	test: 2.678313

Epoch: 8
Loss: 8.341688791910807
RMSE train: 3.133317	val: 3.059456	test: 2.951418
MAE train: 2.684711	val: 2.663486	test: 2.487596

Epoch: 9
Loss: 7.780088901519775
RMSE train: 2.789860	val: 2.697932	test: 2.621091
MAE train: 2.344461	val: 2.307038	test: 2.173811

Epoch: 10
Loss: 7.25911823908488
RMSE train: 2.517745	val: 2.426870	test: 2.356460
MAE train: 2.066913	val: 2.019505	test: 1.907418

Epoch: 11
Loss: 6.6254951159159345
RMSE train: 2.426833	val: 2.347811	test: 2.280668
MAE train: 1.979212	val: 1.943418	test: 1.845755

Epoch: 12
Loss: 6.334179083506267
RMSE train: 2.450801	val: 2.376678	test: 2.321553
MAE train: 2.006619	val: 1.980179	test: 1.904652

Epoch: 13
Loss: 5.878781318664551
RMSE train: 2.374384	val: 2.321960	test: 2.256354
MAE train: 1.945358	val: 1.932171	test: 1.845232

Epoch: 14
Loss: 5.2588198979695635
RMSE train: 2.238212	val: 2.197910	test: 2.127153
MAE train: 1.831745	val: 1.824294	test: 1.735357

Epoch: 15
Loss: 4.927170117696126
RMSE train: 2.141541	val: 2.086641	test: 2.038356
MAE train: 1.754028	val: 1.732852	test: 1.654993

Epoch: 16
Loss: 4.387700160344441
RMSE train: 2.082370	val: 2.015339	test: 2.003697
MAE train: 1.705374	val: 1.665293	test: 1.632630

Epoch: 17
Loss: 4.1641685962677
RMSE train: 2.016367	val: 1.946985	test: 1.954370
MAE train: 1.637909	val: 1.594377	test: 1.588456

Epoch: 18
Loss: 3.7129900455474854
RMSE train: 1.833110	val: 1.764200	test: 1.771801
MAE train: 1.467004	val: 1.421173	test: 1.431981

Epoch: 19
Loss: 3.2511446475982666
RMSE train: 1.547137	val: 1.499939	test: 1.481793
MAE train: 1.182299	val: 1.170257	test: 1.150384

Epoch: 20
Loss: 2.9368742307027182
RMSE train: 1.409913	val: 1.378974	test: 1.346191
MAE train: 1.063714	val: 1.080279	test: 1.026950

Epoch: 21
Loss: 2.7945284048716226
RMSE train: 1.367137	val: 1.319888	test: 1.321310
MAE train: 1.029632	val: 1.018705	test: 1.023824

Epoch: 22
Loss: 2.399608850479126
RMSE train: 1.279189	val: 1.253148	test: 1.261145
MAE train: 0.953237	val: 0.958117	test: 0.970275

Epoch: 23
Loss: 2.1607149839401245Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/esol/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/esol/random/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/esol/random/train_prop=0.6/esol_random_6_26-05_09-43-12  ]
[ Using Seed :  6  ]
[ Using device :  cuda:0  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 12.910023053487143
RMSE train: 3.345681	val: 3.203896	test: 3.212781
MAE train: 2.742699	val: 2.638994	test: 2.641096

Epoch: 2
Loss: 12.105887730916342
RMSE train: 3.305946	val: 3.159533	test: 3.176957
MAE train: 2.714760	val: 2.614817	test: 2.622578

Epoch: 3
Loss: 11.271875381469727
RMSE train: 3.199454	val: 3.048277	test: 3.070464
MAE train: 2.632185	val: 2.536679	test: 2.539986

Epoch: 4
Loss: 10.41736094156901
RMSE train: 3.113794	val: 2.965662	test: 2.978774
MAE train: 2.572508	val: 2.482641	test: 2.467095

Epoch: 5
Loss: 9.747409184773764
RMSE train: 3.127056	val: 2.981560	test: 2.965008
MAE train: 2.619611	val: 2.526398	test: 2.473033

Epoch: 6
Loss: 9.060282707214355
RMSE train: 3.125275	val: 2.997084	test: 2.959141
MAE train: 2.647233	val: 2.550209	test: 2.490699

Epoch: 7
Loss: 8.33074680964152
RMSE train: 3.110916	val: 2.998226	test: 2.929416
MAE train: 2.666883	val: 2.569606	test: 2.483016

Epoch: 8
Loss: 7.865907351175944
RMSE train: 3.001538	val: 2.919431	test: 2.803272
MAE train: 2.570199	val: 2.503280	test: 2.370984

Epoch: 9
Loss: 7.217685381571452
RMSE train: 2.691856	val: 2.666545	test: 2.488805
MAE train: 2.241780	val: 2.232329	test: 2.039398

Epoch: 10
Loss: 6.649109522501628
RMSE train: 2.553949	val: 2.559119	test: 2.352127
MAE train: 2.090208	val: 2.108065	test: 1.906698

Epoch: 11
Loss: 6.327475865681966
RMSE train: 2.426301	val: 2.397773	test: 2.252840
MAE train: 1.980772	val: 1.978319	test: 1.829021

Epoch: 12
Loss: 5.790462176005046
RMSE train: 2.414104	val: 2.353394	test: 2.255647
MAE train: 1.999832	val: 1.968731	test: 1.844465

Epoch: 13
Loss: 5.263169606526692
RMSE train: 2.425100	val: 2.351525	test: 2.263819
MAE train: 2.030552	val: 1.982847	test: 1.873766

Epoch: 14
Loss: 4.973057587941487
RMSE train: 2.308390	val: 2.239164	test: 2.144294
MAE train: 1.914349	val: 1.862598	test: 1.756349

Epoch: 15
Loss: 4.447213649749756
RMSE train: 2.188023	val: 2.139820	test: 2.042094
MAE train: 1.802410	val: 1.778058	test: 1.654681

Epoch: 16
Loss: 3.9671735763549805
RMSE train: 2.094166	val: 2.067952	test: 1.976260
MAE train: 1.740084	val: 1.741270	test: 1.609434

Epoch: 17
Loss: 3.6313724517822266
RMSE train: 2.012072	val: 1.998499	test: 1.914314
MAE train: 1.674088	val: 1.691209	test: 1.561009

Epoch: 18
Loss: 3.2253552277882895
RMSE train: 1.954626	val: 1.953160	test: 1.872213
MAE train: 1.633720	val: 1.665154	test: 1.534262

Epoch: 19
Loss: 2.9429381688435874
RMSE train: 1.900908	val: 1.905205	test: 1.834026
MAE train: 1.593253	val: 1.626026	test: 1.508640

Epoch: 20
Loss: 2.6455456415812173
RMSE train: 1.740134	val: 1.769165	test: 1.688942
MAE train: 1.431677	val: 1.475227	test: 1.360972

Epoch: 21
Loss: 2.352617859840393
RMSE train: 1.516195	val: 1.557902	test: 1.485947
MAE train: 1.209890	val: 1.250756	test: 1.158221

Epoch: 22
Loss: 2.0399564107259116
RMSE train: 1.341127	val: 1.390221	test: 1.339476
MAE train: 1.063004	val: 1.109139	test: 1.046178

Epoch: 23
Loss: 1.950741171836853Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/esol/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/esol/random/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/esol/random/train_prop=0.7/esol_random_6_26-05_09-43-12  ]
[ Using Seed :  6  ]
[ Using device :  cuda:1  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 11.627623915672302
RMSE train: 3.366705	val: 3.491415	test: 3.210574
MAE train: 2.772203	val: 2.939947	test: 2.663213

Epoch: 2
Loss: 11.387786149978638
RMSE train: 3.263680	val: 3.427760	test: 3.136547
MAE train: 2.681976	val: 2.879177	test: 2.585787

Epoch: 3
Loss: 11.144725561141968
RMSE train: 3.112499	val: 3.226781	test: 2.980387
MAE train: 2.559791	val: 2.701018	test: 2.445648

Epoch: 4
Loss: 8.763989686965942
RMSE train: 3.017708	val: 3.066472	test: 2.854023
MAE train: 2.491369	val: 2.580323	test: 2.363258

Epoch: 5
Loss: 8.782400608062744
RMSE train: 2.859826	val: 2.907281	test: 2.649268
MAE train: 2.344844	val: 2.436372	test: 2.151044

Epoch: 6
Loss: 8.137620449066162
RMSE train: 2.710521	val: 2.778274	test: 2.473712
MAE train: 2.213812	val: 2.311521	test: 1.988413

Epoch: 7
Loss: 7.982522130012512
RMSE train: 2.601563	val: 2.666947	test: 2.364164
MAE train: 2.118467	val: 2.211489	test: 1.898739

Epoch: 8
Loss: 6.639075398445129
RMSE train: 2.572718	val: 2.625041	test: 2.341368
MAE train: 2.088489	val: 2.163106	test: 1.874308

Epoch: 9
Loss: 6.459100127220154
RMSE train: 2.541947	val: 2.597755	test: 2.321844
MAE train: 2.082843	val: 2.165695	test: 1.885826

Epoch: 10
Loss: 5.725328087806702
RMSE train: 2.339135	val: 2.398354	test: 2.141526
MAE train: 1.891960	val: 1.975993	test: 1.718719

Epoch: 11
Loss: 5.35751473903656
RMSE train: 2.191081	val: 2.239848	test: 2.005044
MAE train: 1.770346	val: 1.834346	test: 1.622790

Epoch: 12
Loss: 4.332463622093201
RMSE train: 2.125355	val: 2.198716	test: 1.956125
MAE train: 1.719047	val: 1.812465	test: 1.578100

Epoch: 13
Loss: 3.7340922951698303
RMSE train: 2.138516	val: 2.238087	test: 1.967021
MAE train: 1.739949	val: 1.849581	test: 1.599548

Epoch: 14
Loss: 3.74828040599823
RMSE train: 2.128661	val: 2.234541	test: 1.920119
MAE train: 1.736266	val: 1.860500	test: 1.556333

Epoch: 15
Loss: 2.8057669699192047
RMSE train: 2.077843	val: 2.173983	test: 1.859584
MAE train: 1.682169	val: 1.803219	test: 1.495470

Epoch: 16
Loss: 2.7106831073760986
RMSE train: 1.870126	val: 1.955672	test: 1.693077
MAE train: 1.506184	val: 1.601868	test: 1.370751

Epoch: 17
Loss: 2.46535187959671
RMSE train: 1.672659	val: 1.731707	test: 1.536837
MAE train: 1.331662	val: 1.400540	test: 1.220921

Epoch: 18
Loss: 3.1519325971603394
RMSE train: 1.583804	val: 1.676924	test: 1.501538
MAE train: 1.239613	val: 1.340056	test: 1.158168

Epoch: 19
Loss: 1.8497692048549652
RMSE train: 1.409351	val: 1.519749	test: 1.378231
MAE train: 1.078465	val: 1.185491	test: 1.031310

Epoch: 20
Loss: 1.692599892616272
RMSE train: 1.151346	val: 1.222207	test: 1.149193
MAE train: 0.874384	val: 0.964403	test: 0.867634

Epoch: 21
Loss: 1.915492683649063
RMSE train: 0.998039	val: 1.068978	test: 1.037584
MAE train: 0.771920	val: 0.852040	test: 0.779092

Epoch: 22
Loss: 1.9535456001758575
RMSE train: 1.076057	val: 1.194452	test: 1.099861
MAE train: 0.816512	val: 0.945244	test: 0.831765

Epoch: 23
Loss: 1.7719901204109192Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/esol/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/esol/random/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/esol/random/train_prop=0.7/esol_random_5_26-05_09-43-12  ]
[ Using Seed :  5  ]
[ Using device :  cuda:1  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 13.086208581924438
RMSE train: 3.479374	val: 3.612647	test: 3.344895
MAE train: 2.871667	val: 3.014294	test: 2.776319

Epoch: 2
Loss: 12.847848415374756
RMSE train: 3.460054	val: 3.564292	test: 3.311429
MAE train: 2.890666	val: 3.024922	test: 2.791739

Epoch: 3
Loss: 10.622496604919434
RMSE train: 3.334210	val: 3.397116	test: 3.155140
MAE train: 2.798463	val: 2.913335	test: 2.649893

Epoch: 4
Loss: 10.043404817581177
RMSE train: 3.208595	val: 3.268441	test: 2.995737
MAE train: 2.684133	val: 2.793878	test: 2.477989

Epoch: 5
Loss: 8.833252906799316
RMSE train: 3.159324	val: 3.200174	test: 2.903020
MAE train: 2.642884	val: 2.723355	test: 2.403384

Epoch: 6
Loss: 8.278083562850952
RMSE train: 3.082630	val: 3.090613	test: 2.807781
MAE train: 2.595725	val: 2.643740	test: 2.342502

Epoch: 7
Loss: 7.194337844848633
RMSE train: 2.984729	val: 2.963568	test: 2.727301
MAE train: 2.540020	val: 2.555542	test: 2.299343

Epoch: 8
Loss: 6.787212610244751
RMSE train: 2.836643	val: 2.813403	test: 2.620214
MAE train: 2.410815	val: 2.419801	test: 2.196535

Epoch: 9
Loss: 6.738807559013367
RMSE train: 2.768910	val: 2.740358	test: 2.581231
MAE train: 2.351076	val: 2.347126	test: 2.159910

Epoch: 10
Loss: 5.730356335639954
RMSE train: 2.665214	val: 2.619517	test: 2.499555
MAE train: 2.258109	val: 2.237743	test: 2.099282

Epoch: 11
Loss: 5.773956298828125
RMSE train: 2.669727	val: 2.617563	test: 2.526455
MAE train: 2.248390	val: 2.218388	test: 2.114025

Epoch: 12
Loss: 4.218824476003647
RMSE train: 2.544007	val: 2.503155	test: 2.418738
MAE train: 2.124532	val: 2.098467	test: 2.019036

Epoch: 13
Loss: 4.5358370542526245
RMSE train: 2.318216	val: 2.328244	test: 2.191588
MAE train: 1.948213	val: 1.963221	test: 1.845581

Epoch: 14
Loss: 4.3779794573783875
RMSE train: 2.238600	val: 2.260528	test: 2.120785
MAE train: 1.837670	val: 1.866349	test: 1.731896

Epoch: 15
Loss: 3.6504474878311157
RMSE train: 2.046932	val: 2.092776	test: 1.933976
MAE train: 1.656182	val: 1.705053	test: 1.561275

Epoch: 16
Loss: 3.293610394001007
RMSE train: 1.791518	val: 1.800785	test: 1.694416
MAE train: 1.402908	val: 1.441683	test: 1.342359

Epoch: 17
Loss: 3.222676992416382
RMSE train: 1.546651	val: 1.523787	test: 1.501319
MAE train: 1.177303	val: 1.205838	test: 1.172416

Epoch: 18
Loss: 2.8540400862693787
RMSE train: 1.517131	val: 1.506897	test: 1.465997
MAE train: 1.152413	val: 1.176256	test: 1.129991

Epoch: 19
Loss: 2.3066264390945435
RMSE train: 1.491932	val: 1.517532	test: 1.453367
MAE train: 1.136519	val: 1.170883	test: 1.114511

Epoch: 20
Loss: 2.1778839826583862
RMSE train: 1.348798	val: 1.472062	test: 1.365538
MAE train: 0.999084	val: 1.114356	test: 1.002113

Epoch: 21
Loss: 2.135999321937561
RMSE train: 1.166966	val: 1.276488	test: 1.216926
MAE train: 0.868494	val: 0.974434	test: 0.906476

Epoch: 22
Loss: 1.6867395639419556
RMSE train: 1.121491	val: 1.261538	test: 1.207806
MAE train: 0.839009	val: 0.920122	test: 0.859804

Epoch: 23
Loss: 1.9777939319610596Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/esol/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/esol/random/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/esol/random/train_prop=0.8/esol_random_5_26-05_09-43-12  ]
[ Using Seed :  5  ]
[ Using device :  cuda:2  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 13.266300201416016
RMSE train: 3.444177	val: 3.629219	test: 3.102313
MAE train: 2.829309	val: 3.031935	test: 2.502560

Epoch: 2
Loss: 11.681932926177979
RMSE train: 3.373570	val: 3.507088	test: 3.001972
MAE train: 2.810640	val: 2.944424	test: 2.433125

Epoch: 3
Loss: 10.809090852737427
RMSE train: 3.289848	val: 3.315487	test: 2.898760
MAE train: 2.757007	val: 2.756092	test: 2.364164

Epoch: 4
Loss: 9.70919942855835
RMSE train: 3.206703	val: 3.150948	test: 2.802443
MAE train: 2.681430	val: 2.580669	test: 2.303708

Epoch: 5
Loss: 8.714296460151672
RMSE train: 3.013947	val: 2.928125	test: 2.668159
MAE train: 2.534143	val: 2.422277	test: 2.207358

Epoch: 6
Loss: 7.986423969268799
RMSE train: 2.804140	val: 2.688993	test: 2.510642
MAE train: 2.353202	val: 2.208110	test: 2.073115

Epoch: 7
Loss: 7.206580400466919
RMSE train: 2.563267	val: 2.405085	test: 2.332257
MAE train: 2.107392	val: 1.938693	test: 1.891242

Epoch: 8
Loss: 6.435304164886475
RMSE train: 2.407066	val: 2.237348	test: 2.206326
MAE train: 1.953405	val: 1.766087	test: 1.764770

Epoch: 9
Loss: 5.936916828155518
RMSE train: 2.389750	val: 2.264650	test: 2.165079
MAE train: 1.959931	val: 1.830615	test: 1.767211

Epoch: 10
Loss: 5.643097162246704
RMSE train: 2.354915	val: 2.276591	test: 2.124720
MAE train: 1.940955	val: 1.861122	test: 1.736877

Epoch: 11
Loss: 4.734385848045349
RMSE train: 2.127151	val: 2.020675	test: 1.962425
MAE train: 1.713535	val: 1.600056	test: 1.567042

Epoch: 12
Loss: 4.479793667793274
RMSE train: 1.987386	val: 1.855425	test: 1.867621
MAE train: 1.571376	val: 1.419295	test: 1.470545

Epoch: 13
Loss: 3.8529866337776184
RMSE train: 1.908623	val: 1.822291	test: 1.780386
MAE train: 1.511201	val: 1.418072	test: 1.399376

Epoch: 14
Loss: 3.3213881254196167
RMSE train: 1.782924	val: 1.775680	test: 1.647843
MAE train: 1.406356	val: 1.398076	test: 1.282688

Epoch: 15
Loss: 2.921701431274414
RMSE train: 1.464188	val: 1.453430	test: 1.401753
MAE train: 1.117171	val: 1.101608	test: 1.056649

Epoch: 16
Loss: 2.7010276317596436
RMSE train: 1.347983	val: 1.349996	test: 1.306739
MAE train: 1.024169	val: 1.029017	test: 0.971770

Epoch: 17
Loss: 2.3979892134666443
RMSE train: 1.336845	val: 1.387225	test: 1.265994
MAE train: 1.018746	val: 1.074063	test: 0.943068

Epoch: 18
Loss: 1.9553463160991669
RMSE train: 1.140529	val: 1.202947	test: 1.143663
MAE train: 0.851054	val: 0.901860	test: 0.829902

Epoch: 19
Loss: 1.748002290725708
RMSE train: 0.991680	val: 1.103380	test: 1.044385
MAE train: 0.730338	val: 0.818616	test: 0.737850

Epoch: 20
Loss: 1.5398970246315002
RMSE train: 0.949435	val: 1.090652	test: 1.007979
MAE train: 0.711776	val: 0.820531	test: 0.741914

Epoch: 21
Loss: 1.3648633658885956
RMSE train: 0.864821	val: 1.006794	test: 0.962458
MAE train: 0.643305	val: 0.735760	test: 0.710166

Epoch: 22
Loss: 1.1384420692920685
RMSE train: 0.820845	val: 1.016684	test: 0.898775
MAE train: 0.615601	val: 0.757215	test: 0.683401

Epoch: 23
Loss: 1.1201658844947815Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/esol/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/esol/random/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/esol/random/train_prop=0.7/esol_random_4_26-05_09-43-12  ]
[ Using Seed :  4  ]
[ Using device :  cuda:1  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 13.350824117660522
RMSE train: 3.599344	val: 3.818098	test: 3.476584
MAE train: 2.969315	val: 3.197722	test: 2.882570

Epoch: 2
Loss: 14.08483624458313
RMSE train: 3.509057	val: 3.669334	test: 3.347706
MAE train: 2.904485	val: 3.098527	test: 2.747610

Epoch: 3
Loss: 10.968675136566162
RMSE train: 3.411860	val: 3.568501	test: 3.249325
MAE train: 2.848978	val: 3.046820	test: 2.671126

Epoch: 4
Loss: 9.945963859558105
RMSE train: 3.273994	val: 3.429149	test: 3.101113
MAE train: 2.736365	val: 2.924419	test: 2.553223

Epoch: 5
Loss: 8.764717102050781
RMSE train: 3.154815	val: 3.326070	test: 2.967303
MAE train: 2.614247	val: 2.805850	test: 2.419039

Epoch: 6
Loss: 8.866128206253052
RMSE train: 2.982405	val: 3.143300	test: 2.783838
MAE train: 2.455049	val: 2.638728	test: 2.236966

Epoch: 7
Loss: 7.685681700706482
RMSE train: 2.789076	val: 2.915878	test: 2.597723
MAE train: 2.283735	val: 2.436003	test: 2.078183

Epoch: 8
Loss: 6.8630757331848145
RMSE train: 2.662770	val: 2.737762	test: 2.479282
MAE train: 2.180110	val: 2.288908	test: 1.990632

Epoch: 9
Loss: 6.205798268318176
RMSE train: 2.429716	val: 2.479769	test: 2.263626
MAE train: 1.963599	val: 2.045864	test: 1.794765

Epoch: 10
Loss: 5.7327107191085815
RMSE train: 2.258348	val: 2.279645	test: 2.111473
MAE train: 1.810875	val: 1.868189	test: 1.669858

Epoch: 11
Loss: 5.8148884773254395
RMSE train: 2.137993	val: 2.150185	test: 2.012575
MAE train: 1.701199	val: 1.748745	test: 1.587584

Epoch: 12
Loss: 6.015562176704407
RMSE train: 2.048667	val: 2.050551	test: 1.934941
MAE train: 1.619258	val: 1.666323	test: 1.544187

Epoch: 13
Loss: 3.7285295128822327
RMSE train: 1.877531	val: 1.855060	test: 1.770011
MAE train: 1.470723	val: 1.497381	test: 1.409312

Epoch: 14
Loss: 3.8516685366630554
RMSE train: 1.822158	val: 1.762328	test: 1.724880
MAE train: 1.439428	val: 1.437251	test: 1.391373

Epoch: 15
Loss: 3.7466018199920654
RMSE train: 1.704387	val: 1.696982	test: 1.616730
MAE train: 1.319408	val: 1.354375	test: 1.269624

Epoch: 16
Loss: 3.0835098028182983
RMSE train: 1.541114	val: 1.565482	test: 1.478131
MAE train: 1.168283	val: 1.214462	test: 1.129184

Epoch: 17
Loss: 2.598167598247528
RMSE train: 1.408298	val: 1.425859	test: 1.374657
MAE train: 1.069145	val: 1.100855	test: 1.064109

Epoch: 18
Loss: 2.506333649158478
RMSE train: 1.312124	val: 1.338518	test: 1.305491
MAE train: 0.992694	val: 1.030718	test: 1.004194

Epoch: 19
Loss: 2.0326960384845734
RMSE train: 1.272997	val: 1.278380	test: 1.264358
MAE train: 0.963322	val: 1.012219	test: 0.968743

Epoch: 20
Loss: 2.5851994156837463
RMSE train: 1.234012	val: 1.274397	test: 1.264993
MAE train: 0.922344	val: 0.982180	test: 0.962785

Epoch: 21
Loss: 1.8689569532871246
RMSE train: 1.230449	val: 1.272555	test: 1.301194
MAE train: 0.926812	val: 0.986916	test: 0.980755

Epoch: 22
Loss: 2.036401182413101
RMSE train: 1.221308	val: 1.232884	test: 1.297516
MAE train: 0.927448	val: 0.958370	test: 0.987603

Epoch: 23
Loss: 1.5629633367061615Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/esol/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/esol/random/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/esol/random/train_prop=0.8/esol_random_6_26-05_09-43-12  ]
[ Using Seed :  6  ]
[ Using device :  cuda:2  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 12.639514923095703
RMSE train: 3.392228	val: 3.513508	test: 3.047023
MAE train: 2.802867	val: 2.960874	test: 2.515329

Epoch: 2
Loss: 11.367470026016235
RMSE train: 3.139492	val: 3.293901	test: 2.761683
MAE train: 2.573566	val: 2.744660	test: 2.221463

Epoch: 3
Loss: 10.141577959060669
RMSE train: 2.986170	val: 3.110352	test: 2.626880
MAE train: 2.457532	val: 2.581888	test: 2.136034

Epoch: 4
Loss: 9.32800555229187
RMSE train: 2.930195	val: 2.953072	test: 2.610996
MAE train: 2.429180	val: 2.409108	test: 2.122758

Epoch: 5
Loss: 8.251278281211853
RMSE train: 2.895948	val: 2.846906	test: 2.611976
MAE train: 2.413882	val: 2.334861	test: 2.126453

Epoch: 6
Loss: 7.464420557022095
RMSE train: 2.798530	val: 2.688115	test: 2.524444
MAE train: 2.329579	val: 2.195455	test: 2.058336

Epoch: 7
Loss: 6.908464789390564
RMSE train: 2.736679	val: 2.584039	test: 2.466617
MAE train: 2.276461	val: 2.113158	test: 2.043605

Epoch: 8
Loss: 6.2864673137664795
RMSE train: 2.585680	val: 2.488089	test: 2.321017
MAE train: 2.141546	val: 2.037254	test: 1.899394

Epoch: 9
Loss: 5.6716614961624146
RMSE train: 2.478268	val: 2.388087	test: 2.247677
MAE train: 2.074116	val: 1.985759	test: 1.859768

Epoch: 10
Loss: 4.800203800201416
RMSE train: 2.433058	val: 2.311412	test: 2.230295
MAE train: 2.072312	val: 1.947005	test: 1.880931

Epoch: 11
Loss: 4.438094675540924
RMSE train: 2.336851	val: 2.275469	test: 2.129241
MAE train: 1.986408	val: 1.899026	test: 1.796693

Epoch: 12
Loss: 3.8897844552993774
RMSE train: 2.174721	val: 2.145860	test: 1.989836
MAE train: 1.818348	val: 1.760611	test: 1.646980

Epoch: 13
Loss: 3.4443737268447876
RMSE train: 1.956007	val: 1.907250	test: 1.814915
MAE train: 1.616993	val: 1.539575	test: 1.484950

Epoch: 14
Loss: 2.9953020215034485
RMSE train: 1.727946	val: 1.655247	test: 1.646206
MAE train: 1.390971	val: 1.294368	test: 1.317235

Epoch: 15
Loss: 2.5666118264198303
RMSE train: 1.503074	val: 1.478275	test: 1.427783
MAE train: 1.173141	val: 1.128067	test: 1.110463

Epoch: 16
Loss: 2.2904648184776306
RMSE train: 1.248029	val: 1.248676	test: 1.208506
MAE train: 0.939371	val: 0.912740	test: 0.930378

Epoch: 17
Loss: 1.999631643295288
RMSE train: 1.066087	val: 1.115542	test: 1.052897
MAE train: 0.798255	val: 0.822722	test: 0.810595

Epoch: 18
Loss: 1.6507762372493744
RMSE train: 0.954059	val: 1.055535	test: 0.947422
MAE train: 0.719107	val: 0.790267	test: 0.724004

Epoch: 19
Loss: 1.4574181735515594
RMSE train: 0.945876	val: 1.066521	test: 0.926226
MAE train: 0.708581	val: 0.787902	test: 0.687696

Epoch: 20
Loss: 1.382092922925949
RMSE train: 0.888464	val: 1.036735	test: 0.895823
MAE train: 0.664780	val: 0.764360	test: 0.657193

Epoch: 21
Loss: 1.2288382053375244
RMSE train: 0.843615	val: 1.062947	test: 0.880991
MAE train: 0.637068	val: 0.808993	test: 0.658782

Epoch: 22
Loss: 1.1257955878973007
RMSE train: 0.820246	val: 0.923478	test: 0.902534
MAE train: 0.620863	val: 0.694282	test: 0.667699

Epoch: 23
Loss: 1.0212863981723785Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/esol/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/esol/random/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/esol/random/train_prop=0.8/esol_random_4_26-05_09-43-12  ]
[ Using Seed :  4  ]
[ Using device :  cuda:2  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 13.446179628372192
RMSE train: 3.532297	val: 3.779624	test: 3.121724
MAE train: 2.924603	val: 3.198840	test: 2.528973

Epoch: 2
Loss: 12.250332832336426
RMSE train: 3.423901	val: 3.649721	test: 2.995726
MAE train: 2.871602	val: 3.069097	test: 2.414831

Epoch: 3
Loss: 11.005746603012085
RMSE train: 3.426805	val: 3.571293	test: 2.969753
MAE train: 2.888282	val: 2.978714	test: 2.429425

Epoch: 4
Loss: 9.915832042694092
RMSE train: 3.385501	val: 3.445301	test: 2.924408
MAE train: 2.839863	val: 2.846706	test: 2.380298

Epoch: 5
Loss: 9.133478879928589
RMSE train: 3.237565	val: 3.287383	test: 2.835142
MAE train: 2.719560	val: 2.724645	test: 2.295800

Epoch: 6
Loss: 8.410263299942017
RMSE train: 2.895439	val: 2.897855	test: 2.553710
MAE train: 2.392471	val: 2.360605	test: 2.041880

Epoch: 7
Loss: 7.51265025138855
RMSE train: 2.560608	val: 2.548061	test: 2.259990
MAE train: 2.082373	val: 2.024047	test: 1.801639

Epoch: 8
Loss: 6.913408875465393
RMSE train: 2.427359	val: 2.430327	test: 2.140701
MAE train: 1.968622	val: 1.928483	test: 1.711843

Epoch: 9
Loss: 6.430909514427185
RMSE train: 2.402523	val: 2.395443	test: 2.143750
MAE train: 1.964753	val: 1.925179	test: 1.745619

Epoch: 10
Loss: 5.747354745864868
RMSE train: 2.322401	val: 2.268797	test: 2.122219
MAE train: 1.913725	val: 1.839161	test: 1.756027

Epoch: 11
Loss: 4.932947278022766
RMSE train: 2.341062	val: 2.300674	test: 2.147567
MAE train: 1.965490	val: 1.916355	test: 1.808543

Epoch: 12
Loss: 4.468664050102234
RMSE train: 2.316802	val: 2.298269	test: 2.143942
MAE train: 1.952895	val: 1.929624	test: 1.803429

Epoch: 13
Loss: 4.1285094022750854
RMSE train: 2.165049	val: 2.114876	test: 2.033050
MAE train: 1.820850	val: 1.762877	test: 1.693082

Epoch: 14
Loss: 3.460067868232727
RMSE train: 1.881749	val: 1.832764	test: 1.795191
MAE train: 1.547394	val: 1.490248	test: 1.462631

Epoch: 15
Loss: 3.07195121049881
RMSE train: 1.681008	val: 1.701497	test: 1.595107
MAE train: 1.353779	val: 1.364638	test: 1.277788

Epoch: 16
Loss: 2.6510470509529114
RMSE train: 1.526265	val: 1.562166	test: 1.473638
MAE train: 1.202599	val: 1.203642	test: 1.158900

Epoch: 17
Loss: 2.4041702151298523
RMSE train: 1.411152	val: 1.478795	test: 1.344944
MAE train: 1.101787	val: 1.138205	test: 1.040838

Epoch: 18
Loss: 2.0758703351020813
RMSE train: 1.404227	val: 1.538835	test: 1.314323
MAE train: 1.086563	val: 1.211908	test: 1.021046

Epoch: 19
Loss: 1.7917749285697937
RMSE train: 1.322656	val: 1.414103	test: 1.283030
MAE train: 1.027929	val: 1.101932	test: 0.984906

Epoch: 20
Loss: 1.5880014300346375
RMSE train: 1.090725	val: 1.149428	test: 1.100438
MAE train: 0.835551	val: 0.851512	test: 0.817865

Epoch: 21
Loss: 1.4117613434791565
RMSE train: 1.039641	val: 1.221898	test: 1.011410
MAE train: 0.803803	val: 0.962279	test: 0.768877

Epoch: 22
Loss: 1.2482897639274597
RMSE train: 1.029099	val: 1.165598	test: 1.029029
MAE train: 0.795417	val: 0.907977	test: 0.778591

Epoch: 23
Loss: 1.1346918940544128
RMSE train: 1.350330	val: 1.378095	test: 1.374916
MAE train: 1.038865	val: 1.056343	test: 1.065886

Epoch: 24
Loss: 1.9333093961079915
RMSE train: 1.304666	val: 1.348888	test: 1.348227
MAE train: 0.994037	val: 1.018922	test: 1.038593

Epoch: 25
Loss: 1.8391616344451904
RMSE train: 1.294654	val: 1.331007	test: 1.347400
MAE train: 0.981650	val: 0.994984	test: 1.041934

Epoch: 26
Loss: 1.46162615219752
RMSE train: 1.113150	val: 1.123803	test: 1.182379
MAE train: 0.828709	val: 0.835421	test: 0.884682

Epoch: 27
Loss: 1.4006122748057048
RMSE train: 1.044084	val: 1.061649	test: 1.136112
MAE train: 0.770486	val: 0.803602	test: 0.838084

Epoch: 28
Loss: 1.3182984193166096
RMSE train: 1.000245	val: 1.034435	test: 1.107815
MAE train: 0.749651	val: 0.772591	test: 0.817347

Epoch: 29
Loss: 1.2255452473958333
RMSE train: 0.918961	val: 0.981614	test: 1.048494
MAE train: 0.699887	val: 0.724480	test: 0.766145

Epoch: 30
Loss: 1.0859005848566692
RMSE train: 0.881272	val: 0.950869	test: 1.030705
MAE train: 0.676446	val: 0.700927	test: 0.763058

Epoch: 31
Loss: 0.9458247423171997
RMSE train: 0.863556	val: 0.928074	test: 1.017482
MAE train: 0.663225	val: 0.682394	test: 0.752311

Epoch: 32
Loss: 0.9397946000099182
RMSE train: 0.873976	val: 0.932997	test: 1.025429
MAE train: 0.669882	val: 0.677069	test: 0.753080

Epoch: 33
Loss: 0.9471245606740316
RMSE train: 0.895476	val: 0.959123	test: 1.044826
MAE train: 0.688903	val: 0.692155	test: 0.770173

Epoch: 34
Loss: 0.9354506929715475
RMSE train: 0.943462	val: 1.020297	test: 1.125465
MAE train: 0.738604	val: 0.740162	test: 0.869204

Epoch: 35
Loss: 0.8532875974973043
RMSE train: 0.877836	val: 0.960063	test: 1.084469
MAE train: 0.682031	val: 0.705207	test: 0.824943

Epoch: 36
Loss: 0.821881095568339
RMSE train: 0.775843	val: 0.865503	test: 1.009795
MAE train: 0.589452	val: 0.631203	test: 0.748384

Epoch: 37
Loss: 0.8622523148854574
RMSE train: 0.774428	val: 0.873836	test: 1.022833
MAE train: 0.593617	val: 0.646540	test: 0.767072

Epoch: 38
Loss: 0.8626154462496439
RMSE train: 0.735135	val: 0.842826	test: 1.010071
MAE train: 0.571517	val: 0.631923	test: 0.755591

Epoch: 39
Loss: 0.7636146346728007
RMSE train: 0.724711	val: 0.865880	test: 1.006050
MAE train: 0.576146	val: 0.665548	test: 0.752206

Epoch: 40
Loss: 0.8008062243461609
RMSE train: 0.694819	val: 0.836763	test: 0.957850
MAE train: 0.548439	val: 0.627085	test: 0.715174

Epoch: 41
Loss: 0.7877471049626669
RMSE train: 0.696407	val: 0.840544	test: 0.958256
MAE train: 0.548322	val: 0.636451	test: 0.718621

Epoch: 42
Loss: 0.7141098380088806
RMSE train: 0.678498	val: 0.831229	test: 0.957515
MAE train: 0.527796	val: 0.635729	test: 0.714998

Epoch: 43
Loss: 0.735314150651296
RMSE train: 0.705282	val: 0.850275	test: 0.972087
MAE train: 0.550052	val: 0.641182	test: 0.725887

Epoch: 44
Loss: 0.7046854297320048
RMSE train: 0.766667	val: 0.901744	test: 1.006504
MAE train: 0.599228	val: 0.680255	test: 0.764111

Epoch: 45
Loss: 0.6941397190093994
RMSE train: 0.813687	val: 0.953324	test: 1.031427
MAE train: 0.634410	val: 0.717074	test: 0.798338

Epoch: 46
Loss: 0.6757333874702454
RMSE train: 0.698739	val: 0.844823	test: 0.941662
MAE train: 0.535959	val: 0.626320	test: 0.710151

Epoch: 47
Loss: 0.6948844790458679
RMSE train: 0.767249	val: 0.914417	test: 1.013757
MAE train: 0.594271	val: 0.702008	test: 0.784203

Epoch: 48
Loss: 0.6989938020706177
RMSE train: 0.851337	val: 0.980816	test: 1.072636
MAE train: 0.657943	val: 0.752476	test: 0.823082

Epoch: 49
Loss: 0.6808024247487386
RMSE train: 0.697030	val: 0.816191	test: 0.922201
MAE train: 0.523133	val: 0.590853	test: 0.679916

Epoch: 50
Loss: 0.6596460143725077
RMSE train: 0.640981	val: 0.782258	test: 0.901273
MAE train: 0.483178	val: 0.581205	test: 0.667308

Epoch: 51
Loss: 0.7143669724464417
RMSE train: 0.686975	val: 0.821955	test: 0.957261
MAE train: 0.534467	val: 0.612700	test: 0.714341

Epoch: 52
Loss: 0.6340392827987671
RMSE train: 0.805718	val: 0.931880	test: 1.015460
MAE train: 0.628575	val: 0.705515	test: 0.792961

Epoch: 53
Loss: 0.6410423517227173
RMSE train: 0.704938	val: 0.860128	test: 0.923706
MAE train: 0.527946	val: 0.637092	test: 0.697030

Epoch: 54
Loss: 0.6802649100621542
RMSE train: 0.668500	val: 0.827500	test: 0.906774
MAE train: 0.513964	val: 0.619201	test: 0.696153

Epoch: 55
Loss: 0.6344394087791443
RMSE train: 0.747964	val: 0.888240	test: 0.975935
MAE train: 0.574699	val: 0.665025	test: 0.750615

Epoch: 56
Loss: 0.6072052915891012
RMSE train: 0.656374	val: 0.811623	test: 0.914539
MAE train: 0.499494	val: 0.593637	test: 0.680683

Epoch: 57
Loss: 0.5656031966209412
RMSE train: 0.628360	val: 0.805720	test: 0.903206
MAE train: 0.476865	val: 0.594922	test: 0.666693

Epoch: 58
Loss: 0.6058812737464905
RMSE train: 0.686643	val: 0.859994	test: 0.931865
MAE train: 0.523806	val: 0.634231	test: 0.692259

Epoch: 59
Loss: 0.6205278237660726
RMSE train: 0.689673	val: 0.854816	test: 0.938353
MAE train: 0.531966	val: 0.632235	test: 0.701096

Epoch: 60
Loss: 0.6367521087328593
RMSE train: 0.705054	val: 0.861321	test: 0.945763
MAE train: 0.544883	val: 0.642017	test: 0.714375

Epoch: 61
Loss: 0.5577499071756998
RMSE train: 0.746593	val: 0.889514	test: 0.971096
MAE train: 0.576351	val: 0.667690	test: 0.749034

Epoch: 62
Loss: 0.537403812011083
RMSE train: 0.739082	val: 0.884032	test: 0.947636
MAE train: 0.567335	val: 0.644395	test: 0.724396

Epoch: 63
Loss: 0.5880281925201416
RMSE train: 0.727447	val: 0.874445	test: 0.940822
MAE train: 0.552274	val: 0.628176	test: 0.710645

Epoch: 64
Loss: 0.5789770483970642
RMSE train: 0.738893	val: 0.884714	test: 0.941910
MAE train: 0.559988	val: 0.633307	test: 0.705479

Epoch: 65
Loss: 0.5421585043271383
RMSE train: 0.644840	val: 0.810831	test: 0.872459
MAE train: 0.486348	val: 0.597138	test: 0.642609

Epoch: 66
Loss: 0.64410932858785
RMSE train: 0.629774	val: 0.792379	test: 0.887188
MAE train: 0.476991	val: 0.583411	test: 0.656153

Epoch: 67
Loss: 0.50491796930631
RMSE train: 0.668076	val: 0.829508	test: 0.917726
MAE train: 0.512280	val: 0.614375	test: 0.693520

Epoch: 68
Loss: 0.5920475323994955
RMSE train: 0.656973	val: 0.828419	test: 0.913273
MAE train: 0.507297	val: 0.610362	test: 0.693195

Epoch: 69
Loss: 0.5464252034823099
RMSE train: 0.664576	val: 0.826336	test: 0.893281
MAE train: 0.517277	val: 0.605252	test: 0.679281

Epoch: 70
Loss: 0.5268498162428538
RMSE train: 0.665678	val: 0.827283	test: 0.891380
MAE train: 0.516981	val: 0.613748	test: 0.676852

Epoch: 71
Loss: 0.5087932050228119
RMSE train: 0.644548	val: 0.815018	test: 0.895266
MAE train: 0.500532	val: 0.601198	test: 0.670697

Epoch: 72
Loss: 0.5325340628623962
RMSE train: 0.682112	val: 0.864527	test: 0.947950
MAE train: 0.530738	val: 0.631273	test: 0.717044

Epoch: 73
Loss: 0.5624802112579346
RMSE train: 0.619771	val: 0.801032	test: 0.885408
MAE train: 0.478902	val: 0.578327	test: 0.658200

Epoch: 74
Loss: 0.5096012850602468
RMSE train: 0.667134	val: 0.819706	test: 0.889831
MAE train: 0.510241	val: 0.592285	test: 0.662627

Epoch: 75
Loss: 0.497578223546346
RMSE train: 0.717318	val: 0.862091	test: 0.926291
MAE train: 0.553907	val: 0.629800	test: 0.708611

Epoch: 76
Loss: 0.5087791383266449
RMSE train: 0.760675	val: 0.904821	test: 0.970393
MAE train: 0.587322	val: 0.673802	test: 0.749918

Epoch: 77
Loss: 0.575281540552775
RMSE train: 0.677193	val: 0.830641	test: 0.873656
MAE train: 0.518522	val: 0.606846	test: 0.655609

Epoch: 78
Loss: 0.5009260972340902
RMSE train: 0.749095	val: 0.904730	test: 0.906548
MAE train: 0.566877	val: 0.689021	test: 0.684440

Epoch: 79
Loss: 0.5543620785077413
RMSE train: 0.615855	val: 0.817007	test: 0.880108
MAE train: 0.475554	val: 0.598364	test: 0.670347

Epoch: 80
Loss: 0.4759504695733388
RMSE train: 0.629744	val: 0.832973	test: 0.916780
MAE train: 0.488857	val: 0.615340	test: 0.690727

Epoch: 81
Loss: 0.539079467455546
RMSE train: 0.585856	val: 0.777224	test: 0.860785
MAE train: 0.443833	val: 0.579367	test: 0.638934

Epoch: 82
Loss: 0.49287307262420654
RMSE train: 0.610583	val: 0.794034	test: 0.876925
MAE train: 0.464366	val: 0.601318	test: 0.651730

Epoch: 83
Loss: 0.49733612934748334
RMSE train: 0.597269	val: 0.779889	test: 0.878776
MAE train: 0.460484	val: 0.584383	test: 0.662070

Epoch: 84
Loss: 0.4267057577768962
RMSE train: 1.200435	val: 1.195701	test: 1.207592
MAE train: 0.891237	val: 0.895414	test: 0.916967

Epoch: 24
Loss: 2.0245441993077598
RMSE train: 1.116621	val: 1.141108	test: 1.136503
MAE train: 0.833457	val: 0.864693	test: 0.843217

Epoch: 25
Loss: 1.794520338376363
RMSE train: 1.048902	val: 1.100694	test: 1.095607
MAE train: 0.788084	val: 0.837597	test: 0.817018

Epoch: 26
Loss: 1.562393307685852
RMSE train: 1.061993	val: 1.106811	test: 1.118970
MAE train: 0.797396	val: 0.824194	test: 0.837297

Epoch: 27
Loss: 1.5074698130289714
RMSE train: 1.117340	val: 1.152460	test: 1.175719
MAE train: 0.849642	val: 0.858835	test: 0.898158

Epoch: 28
Loss: 1.3383257786432903
RMSE train: 0.998239	val: 1.043453	test: 1.074364
MAE train: 0.752896	val: 0.775199	test: 0.795213

Epoch: 29
Loss: 1.230897049109141
RMSE train: 0.916519	val: 0.969338	test: 1.004875
MAE train: 0.695213	val: 0.716251	test: 0.752698

Epoch: 30
Loss: 1.107101837793986
RMSE train: 0.910728	val: 0.958827	test: 1.014079
MAE train: 0.693919	val: 0.716047	test: 0.762933

Epoch: 31
Loss: 1.0428619384765625
RMSE train: 0.857610	val: 0.901090	test: 0.977840
MAE train: 0.662762	val: 0.663767	test: 0.728968

Epoch: 32
Loss: 1.0042465130488079
RMSE train: 0.791909	val: 0.854072	test: 0.937735
MAE train: 0.609736	val: 0.637814	test: 0.701321

Epoch: 33
Loss: 0.9796049992243449
RMSE train: 0.736676	val: 0.817785	test: 0.939789
MAE train: 0.576999	val: 0.617930	test: 0.715846

Epoch: 34
Loss: 0.8632553418477377
RMSE train: 0.736544	val: 0.816953	test: 0.926617
MAE train: 0.567148	val: 0.611922	test: 0.708579

Epoch: 35
Loss: 0.8480185270309448
RMSE train: 0.731478	val: 0.821368	test: 0.902648
MAE train: 0.563227	val: 0.613659	test: 0.672477

Epoch: 36
Loss: 0.82903653383255
RMSE train: 0.735139	val: 0.833627	test: 0.927513
MAE train: 0.561451	val: 0.618734	test: 0.690888

Epoch: 37
Loss: 0.8258519570032755
RMSE train: 0.787828	val: 0.889468	test: 0.972470
MAE train: 0.598238	val: 0.662338	test: 0.738765

Epoch: 38
Loss: 0.8043045401573181
RMSE train: 0.785776	val: 0.886294	test: 0.958343
MAE train: 0.599256	val: 0.647463	test: 0.724344

Epoch: 39
Loss: 0.8385583360989889
RMSE train: 0.777386	val: 0.881460	test: 0.955040
MAE train: 0.595203	val: 0.641446	test: 0.725852

Epoch: 40
Loss: 0.7639989852905273
RMSE train: 0.764604	val: 0.884966	test: 0.965521
MAE train: 0.588140	val: 0.641885	test: 0.728529

Epoch: 41
Loss: 0.8066109220186869
RMSE train: 0.626482	val: 0.762472	test: 0.873296
MAE train: 0.480981	val: 0.584848	test: 0.658790

Epoch: 42
Loss: 0.7190122604370117
RMSE train: 0.654283	val: 0.780599	test: 0.884516
MAE train: 0.500549	val: 0.604548	test: 0.669599

Epoch: 43
Loss: 0.7694078286488851
RMSE train: 0.803866	val: 0.890728	test: 0.977538
MAE train: 0.611062	val: 0.662384	test: 0.745899

Epoch: 44
Loss: 0.7905928691228231
RMSE train: 0.809618	val: 0.898884	test: 0.985741
MAE train: 0.623070	val: 0.658539	test: 0.750540

Epoch: 45
Loss: 0.716972827911377
RMSE train: 0.707792	val: 0.815403	test: 0.900551
MAE train: 0.545807	val: 0.610864	test: 0.683531

Epoch: 46
Loss: 0.7330534060796102
RMSE train: 0.672447	val: 0.793843	test: 0.887839
MAE train: 0.521334	val: 0.589985	test: 0.680943

Epoch: 47
Loss: 0.716348926226298
RMSE train: 0.788003	val: 0.904337	test: 1.004975
MAE train: 0.592015	val: 0.662547	test: 0.760233

Epoch: 48
Loss: 0.7273353139559428
RMSE train: 0.670674	val: 0.794576	test: 0.901563
MAE train: 0.514487	val: 0.590248	test: 0.674962

Epoch: 49
Loss: 0.6808472871780396
RMSE train: 0.687516	val: 0.796937	test: 0.906509
MAE train: 0.525745	val: 0.596210	test: 0.680972

Epoch: 50
Loss: 0.6924121379852295
RMSE train: 0.792825	val: 0.886812	test: 0.985260
MAE train: 0.602994	val: 0.655330	test: 0.746512

Epoch: 51
Loss: 0.619095245997111
RMSE train: 0.689082	val: 0.820016	test: 0.899061
MAE train: 0.520190	val: 0.596642	test: 0.674041

Epoch: 52
Loss: 0.6953449050585429
RMSE train: 0.689102	val: 0.823252	test: 0.911105
MAE train: 0.520946	val: 0.601707	test: 0.688196

Epoch: 53
Loss: 0.6890740593274435
RMSE train: 0.784552	val: 0.889239	test: 0.967231
MAE train: 0.594885	val: 0.648232	test: 0.728442

Epoch: 54
Loss: 0.6420944730440775
RMSE train: 0.767513	val: 0.872767	test: 0.930342
MAE train: 0.590408	val: 0.638281	test: 0.694343

Epoch: 55
Loss: 0.6300939321517944
RMSE train: 0.706462	val: 0.822041	test: 0.893934
MAE train: 0.541697	val: 0.605659	test: 0.676354

Epoch: 56
Loss: 0.6502767403920492
RMSE train: 0.690567	val: 0.815660	test: 0.884624
MAE train: 0.527543	val: 0.607276	test: 0.670165

Epoch: 57
Loss: 0.6646503806114197
RMSE train: 0.725007	val: 0.845550	test: 0.906130
MAE train: 0.553104	val: 0.606056	test: 0.673274

Epoch: 58
Loss: 0.6093816558519999
RMSE train: 0.719021	val: 0.835464	test: 0.900107
MAE train: 0.548782	val: 0.598340	test: 0.673825

Epoch: 59
Loss: 0.6337441205978394
RMSE train: 0.674748	val: 0.805935	test: 0.883700
MAE train: 0.513367	val: 0.584947	test: 0.669647

Epoch: 60
Loss: 0.5575001835823059
RMSE train: 0.665191	val: 0.804486	test: 0.872095
MAE train: 0.498626	val: 0.569348	test: 0.656972

Epoch: 61
Loss: 0.6178398529688517
RMSE train: 0.706322	val: 0.832612	test: 0.894753
MAE train: 0.533080	val: 0.577601	test: 0.671169

Epoch: 62
Loss: 0.5271597901980082
RMSE train: 0.715573	val: 0.832384	test: 0.928671
MAE train: 0.538035	val: 0.589566	test: 0.691851

Epoch: 63
Loss: 0.5696864525477091
RMSE train: 0.653547	val: 0.785392	test: 0.885132
MAE train: 0.488683	val: 0.554899	test: 0.658745

Epoch: 64
Loss: 0.5210842887560526
RMSE train: 0.668262	val: 0.803401	test: 0.899299
MAE train: 0.498125	val: 0.567610	test: 0.663001

Epoch: 65
Loss: 0.5920557181040446
RMSE train: 0.696101	val: 0.835755	test: 0.924364
MAE train: 0.519222	val: 0.598529	test: 0.677786

Epoch: 66
Loss: 0.5669588645299276
RMSE train: 0.607603	val: 0.775123	test: 0.830571
MAE train: 0.454489	val: 0.559280	test: 0.614317

Epoch: 67
Loss: 0.6061901847521464
RMSE train: 0.672152	val: 0.830208	test: 0.879665
MAE train: 0.507054	val: 0.601520	test: 0.649671

Epoch: 68
Loss: 0.5198288758595785
RMSE train: 0.673078	val: 0.833949	test: 0.891766
MAE train: 0.507328	val: 0.599721	test: 0.656574

Epoch: 69
Loss: 0.48245853185653687
RMSE train: 0.596588	val: 0.773100	test: 0.838269
MAE train: 0.453161	val: 0.566861	test: 0.616609

Epoch: 70
Loss: 0.5518506368001302
RMSE train: 0.591006	val: 0.761202	test: 0.853607
MAE train: 0.449679	val: 0.555052	test: 0.632061

Epoch: 71
Loss: 0.5017883777618408
RMSE train: 0.618289	val: 0.773533	test: 0.870981
MAE train: 0.470857	val: 0.566148	test: 0.651789

Epoch: 72
Loss: 0.5256709456443787
RMSE train: 0.610371	val: 0.770656	test: 0.842096
MAE train: 0.464725	val: 0.570304	test: 0.623556

Epoch: 73
Loss: 0.5061827500661215
RMSE train: 0.605560	val: 0.765543	test: 0.861038
MAE train: 0.461440	val: 0.553977	test: 0.637982

Epoch: 74
Loss: 0.512753407160441
RMSE train: 0.627092	val: 0.788907	test: 0.882155
MAE train: 0.482418	val: 0.568585	test: 0.654023

Epoch: 75
Loss: 0.566419243812561
RMSE train: 0.610230	val: 0.779790	test: 0.840923
MAE train: 0.466869	val: 0.569800	test: 0.629747

Epoch: 76
Loss: 0.4901479184627533
RMSE train: 0.585865	val: 0.761221	test: 0.824397
MAE train: 0.449203	val: 0.567934	test: 0.621953

Epoch: 77
Loss: 0.49309098720550537
RMSE train: 0.605283	val: 0.780435	test: 0.838797
MAE train: 0.467932	val: 0.581106	test: 0.636093

Epoch: 78
Loss: 0.44040247797966003
RMSE train: 0.639719	val: 0.802663	test: 0.867747
MAE train: 0.487375	val: 0.592284	test: 0.649728

Epoch: 79
Loss: 0.4959459404150645
RMSE train: 0.579186	val: 0.748665	test: 0.829071
MAE train: 0.452355	val: 0.564269	test: 0.619579

Epoch: 80
Loss: 0.49436501661936444
RMSE train: 0.587938	val: 0.751358	test: 0.859745
MAE train: 0.461962	val: 0.570971	test: 0.648906

Epoch: 81
Loss: 0.49686287840207416
RMSE train: 0.642473	val: 0.798565	test: 0.902882
MAE train: 0.495606	val: 0.589302	test: 0.676189

Epoch: 82
Loss: 0.5261239310105642
RMSE train: 0.602159	val: 0.775932	test: 0.846847
MAE train: 0.461426	val: 0.577714	test: 0.631926

Epoch: 83
Loss: 0.5997608800729116
RMSE train: 0.603799	val: 0.795712	test: 0.846493
MAE train: 0.466762	val: 0.606716	test: 0.634941
RMSE train: 1.333571	val: 1.375869	test: 1.364648
MAE train: 1.046404	val: 1.085242	test: 1.076327

Epoch: 24
Loss: 1.6062637170155842
RMSE train: 1.244852	val: 1.311456	test: 1.294831
MAE train: 0.973288	val: 1.029568	test: 1.018735

Epoch: 25
Loss: 1.4530473947525024
RMSE train: 1.154644	val: 1.241084	test: 1.214267
MAE train: 0.897547	val: 0.968635	test: 0.941275

Epoch: 26
Loss: 1.309714635213216
RMSE train: 1.083817	val: 1.162159	test: 1.153547
MAE train: 0.837724	val: 0.905409	test: 0.887159

Epoch: 27
Loss: 1.1037309964497883
RMSE train: 0.981847	val: 1.069065	test: 1.081490
MAE train: 0.748849	val: 0.807979	test: 0.836868

Epoch: 28
Loss: 1.1701240142186482
RMSE train: 0.893135	val: 1.014310	test: 1.021918
MAE train: 0.679456	val: 0.766076	test: 0.775898

Epoch: 29
Loss: 1.0476395686467488
RMSE train: 0.857354	val: 1.006508	test: 1.016194
MAE train: 0.658944	val: 0.768477	test: 0.765797

Epoch: 30
Loss: 0.9589952627817789
RMSE train: 0.759642	val: 0.920529	test: 0.964035
MAE train: 0.583790	val: 0.686753	test: 0.736865

Epoch: 31
Loss: 0.9635273019472758
RMSE train: 0.754088	val: 0.885544	test: 0.969960
MAE train: 0.579947	val: 0.647577	test: 0.737190

Epoch: 32
Loss: 0.9117417335510254
RMSE train: 0.738074	val: 0.867683	test: 0.934141
MAE train: 0.558999	val: 0.626357	test: 0.710767

Epoch: 33
Loss: 0.8382630944252014
RMSE train: 0.793783	val: 0.910541	test: 0.973697
MAE train: 0.600011	val: 0.662303	test: 0.742877

Epoch: 34
Loss: 0.8753099838892618
RMSE train: 0.723330	val: 0.851901	test: 0.917917
MAE train: 0.546343	val: 0.619140	test: 0.687580

Epoch: 35
Loss: 0.7607397437095642
RMSE train: 0.674134	val: 0.811457	test: 0.896632
MAE train: 0.509749	val: 0.602260	test: 0.668997

Epoch: 36
Loss: 0.777431309223175
RMSE train: 0.695379	val: 0.834623	test: 0.909198
MAE train: 0.519414	val: 0.612138	test: 0.674727

Epoch: 37
Loss: 0.7302339275677999
RMSE train: 0.711657	val: 0.853275	test: 0.919301
MAE train: 0.525524	val: 0.613423	test: 0.692693

Epoch: 38
Loss: 0.8400415976842245
RMSE train: 0.713255	val: 0.859953	test: 0.890643
MAE train: 0.535789	val: 0.619454	test: 0.666623

Epoch: 39
Loss: 0.8107722004254659
RMSE train: 0.674165	val: 0.823278	test: 0.887202
MAE train: 0.503946	val: 0.594636	test: 0.668694

Epoch: 40
Loss: 0.7870190739631653
RMSE train: 0.636955	val: 0.802145	test: 0.884260
MAE train: 0.485117	val: 0.592470	test: 0.651398

Epoch: 41
Loss: 0.7161243359247843
RMSE train: 0.638978	val: 0.815038	test: 0.845095
MAE train: 0.483996	val: 0.595875	test: 0.633922

Epoch: 42
Loss: 0.7000566720962524
RMSE train: 0.708803	val: 0.868703	test: 0.876409
MAE train: 0.540624	val: 0.634509	test: 0.658166

Epoch: 43
Loss: 0.6789397398630778
RMSE train: 0.677567	val: 0.834988	test: 0.880433
MAE train: 0.507363	val: 0.609095	test: 0.661994

Epoch: 44
Loss: 0.6894724170366923
RMSE train: 0.676968	val: 0.829338	test: 0.892700
MAE train: 0.502749	val: 0.600280	test: 0.669798

Epoch: 45
Loss: 0.7033163905143738
RMSE train: 0.649170	val: 0.816402	test: 0.838072
MAE train: 0.485642	val: 0.590751	test: 0.639141

Epoch: 46
Loss: 0.682502269744873
RMSE train: 0.652869	val: 0.831784	test: 0.831189
MAE train: 0.491510	val: 0.609709	test: 0.622962

Epoch: 47
Loss: 0.7174509962399801
RMSE train: 0.675283	val: 0.847143	test: 0.875092
MAE train: 0.508015	val: 0.604055	test: 0.649632

Epoch: 48
Loss: 0.7344918251037598
RMSE train: 0.636610	val: 0.800798	test: 0.874093
MAE train: 0.486159	val: 0.579983	test: 0.652307

Epoch: 49
Loss: 0.755499521891276
RMSE train: 0.652337	val: 0.806968	test: 0.864866
MAE train: 0.496136	val: 0.583894	test: 0.638761

Epoch: 50
Loss: 0.6727515061696371
RMSE train: 0.714415	val: 0.886962	test: 0.910100
MAE train: 0.538349	val: 0.649086	test: 0.679361

Epoch: 51
Loss: 0.6467416485150655
RMSE train: 0.727426	val: 0.891480	test: 0.927982
MAE train: 0.553868	val: 0.649465	test: 0.701345

Epoch: 52
Loss: 0.6040237744649252
RMSE train: 0.708158	val: 0.874833	test: 0.907121
MAE train: 0.525603	val: 0.634693	test: 0.684971

Epoch: 53
Loss: 0.6585798859596252
RMSE train: 0.681565	val: 0.847154	test: 0.873288
MAE train: 0.509719	val: 0.603243	test: 0.646109

Epoch: 54
Loss: 0.6313474675019582
RMSE train: 0.700708	val: 0.853990	test: 0.895251
MAE train: 0.526517	val: 0.617888	test: 0.658474

Epoch: 55
Loss: 0.6758616964022318
RMSE train: 0.653101	val: 0.819695	test: 0.880396
MAE train: 0.499930	val: 0.589045	test: 0.647104

Epoch: 56
Loss: 0.6454150279362997
RMSE train: 0.715887	val: 0.885187	test: 0.945823
MAE train: 0.546934	val: 0.636782	test: 0.701946

Epoch: 57
Loss: 0.6148152748743693
RMSE train: 0.682612	val: 0.853819	test: 0.870952
MAE train: 0.514299	val: 0.608726	test: 0.642803

Epoch: 58
Loss: 0.5651688774426779
RMSE train: 0.701216	val: 0.875214	test: 0.869871
MAE train: 0.518522	val: 0.628891	test: 0.639370

Epoch: 59
Loss: 0.5716637372970581
RMSE train: 0.624553	val: 0.803115	test: 0.834833
MAE train: 0.465721	val: 0.591623	test: 0.620859

Epoch: 60
Loss: 0.5429901480674744
RMSE train: 0.582098	val: 0.780389	test: 0.825348
MAE train: 0.443301	val: 0.597034	test: 0.613925

Epoch: 61
Loss: 0.5398104389508566
RMSE train: 0.583157	val: 0.780259	test: 0.816415
MAE train: 0.438357	val: 0.570358	test: 0.602640

Epoch: 62
Loss: 0.5987840294837952
RMSE train: 0.729902	val: 0.886801	test: 0.925055
MAE train: 0.546867	val: 0.647852	test: 0.697462

Epoch: 63
Loss: 0.5871388713518778
RMSE train: 0.768531	val: 0.911791	test: 0.972560
MAE train: 0.574860	val: 0.665631	test: 0.734150

Epoch: 64
Loss: 0.5263404250144958
RMSE train: 0.580457	val: 0.768069	test: 0.818216
MAE train: 0.446461	val: 0.550424	test: 0.603570

Epoch: 65
Loss: 0.5299811959266663
RMSE train: 0.594215	val: 0.798633	test: 0.818463
MAE train: 0.462860	val: 0.583763	test: 0.613319

Epoch: 66
Loss: 0.5505459308624268
RMSE train: 0.582754	val: 0.787824	test: 0.816721
MAE train: 0.442763	val: 0.556847	test: 0.601371

Epoch: 67
Loss: 0.650760551293691
RMSE train: 0.662393	val: 0.839562	test: 0.891405
MAE train: 0.496035	val: 0.607416	test: 0.663768

Epoch: 68
Loss: 0.48048662145932514
RMSE train: 0.604102	val: 0.795304	test: 0.840540
MAE train: 0.452278	val: 0.566853	test: 0.619393

Epoch: 69
Loss: 0.5262309511502584
RMSE train: 0.602474	val: 0.794897	test: 0.847273
MAE train: 0.450284	val: 0.578093	test: 0.619087

Epoch: 70
Loss: 0.4967653453350067
RMSE train: 0.620803	val: 0.813695	test: 0.879017
MAE train: 0.471105	val: 0.584214	test: 0.640553

Epoch: 71
Loss: 0.5332215030988058
RMSE train: 0.627350	val: 0.813277	test: 0.882424
MAE train: 0.476733	val: 0.584763	test: 0.651941

Epoch: 72
Loss: 0.5302305718262991
RMSE train: 0.608275	val: 0.790397	test: 0.849973
MAE train: 0.456738	val: 0.562322	test: 0.623206

Epoch: 73
Loss: 0.46631763378779095
RMSE train: 0.582559	val: 0.780471	test: 0.860537
MAE train: 0.441281	val: 0.564914	test: 0.620175

Epoch: 74
Loss: 0.4848131636778514
RMSE train: 0.603349	val: 0.805808	test: 0.894693
MAE train: 0.456354	val: 0.583522	test: 0.635366

Epoch: 75
Loss: 0.4975353976090749
RMSE train: 0.623516	val: 0.813358	test: 0.881337
MAE train: 0.473083	val: 0.595140	test: 0.636479

Epoch: 76
Loss: 0.47443410754203796
RMSE train: 0.569129	val: 0.772778	test: 0.825596
MAE train: 0.429279	val: 0.563273	test: 0.606215

Epoch: 77
Loss: 0.5684307515621185
RMSE train: 0.521441	val: 0.747384	test: 0.810550
MAE train: 0.399785	val: 0.556669	test: 0.598248

Epoch: 78
Loss: 0.47437186042467755
RMSE train: 0.530701	val: 0.745712	test: 0.832201
MAE train: 0.411266	val: 0.557576	test: 0.609136

Epoch: 79
Loss: 0.4668686588605245
RMSE train: 0.559035	val: 0.755185	test: 0.882180
MAE train: 0.438024	val: 0.561449	test: 0.629703

Epoch: 80
Loss: 0.46012284358342487
RMSE train: 0.531991	val: 0.738448	test: 0.884742
MAE train: 0.417285	val: 0.551815	test: 0.624704

Epoch: 81
Loss: 0.509677787621816
RMSE train: 0.493050	val: 0.728476	test: 0.858009
MAE train: 0.387689	val: 0.545414	test: 0.615195

Epoch: 82
Loss: 0.4214591383934021
RMSE train: 0.523354	val: 0.762995	test: 0.854460
MAE train: 0.405601	val: 0.573514	test: 0.627081

Epoch: 83
Loss: 0.45992226401964825
RMSE train: 0.535599	val: 0.766502	test: 0.863037
MAE train: 0.415926	val: 0.574225	test: 0.630114
RMSE train: 1.383624	val: 1.515334	test: 1.372522
MAE train: 1.056503	val: 1.175214	test: 1.037214

Epoch: 24
Loss: 1.5710553526878357
RMSE train: 1.070648	val: 1.134266	test: 1.083593
MAE train: 0.795670	val: 0.843596	test: 0.796318

Epoch: 25
Loss: 1.4952421188354492
RMSE train: 0.936333	val: 0.936759	test: 0.970289
MAE train: 0.703498	val: 0.719602	test: 0.703103

Epoch: 26
Loss: 1.3550333976745605
RMSE train: 0.928665	val: 0.991184	test: 1.010302
MAE train: 0.712304	val: 0.757198	test: 0.728786

Epoch: 27
Loss: 1.0894420593976974
RMSE train: 0.846220	val: 0.924704	test: 0.978271
MAE train: 0.662819	val: 0.712913	test: 0.742412

Epoch: 28
Loss: 1.1054442524909973
RMSE train: 0.870977	val: 0.919654	test: 0.981642
MAE train: 0.675278	val: 0.728257	test: 0.753310

Epoch: 29
Loss: 1.097425863146782
RMSE train: 0.866580	val: 0.932454	test: 0.973296
MAE train: 0.663321	val: 0.729255	test: 0.739817

Epoch: 30
Loss: 1.228982299566269
RMSE train: 0.843557	val: 0.896966	test: 0.953773
MAE train: 0.654347	val: 0.709737	test: 0.739589

Epoch: 31
Loss: 1.1287302374839783
RMSE train: 0.844303	val: 0.903323	test: 0.924906
MAE train: 0.638944	val: 0.686492	test: 0.707442

Epoch: 32
Loss: 1.0930430591106415
RMSE train: 0.854206	val: 0.917137	test: 0.950820
MAE train: 0.653070	val: 0.718817	test: 0.729562

Epoch: 33
Loss: 1.051504671573639
RMSE train: 0.840637	val: 0.903598	test: 0.946598
MAE train: 0.627258	val: 0.701684	test: 0.702196

Epoch: 34
Loss: 1.0778178870677948
RMSE train: 0.822531	val: 0.866161	test: 0.946592
MAE train: 0.617438	val: 0.688365	test: 0.712953

Epoch: 35
Loss: 1.0781540423631668
RMSE train: 0.811141	val: 0.857288	test: 0.934903
MAE train: 0.605488	val: 0.671518	test: 0.711088

Epoch: 36
Loss: 0.9699755162000656
RMSE train: 0.791373	val: 0.842461	test: 0.924034
MAE train: 0.598600	val: 0.666802	test: 0.716106

Epoch: 37
Loss: 1.0782900750637054
RMSE train: 0.766997	val: 0.825599	test: 0.884946
MAE train: 0.583877	val: 0.652300	test: 0.681759

Epoch: 38
Loss: 0.786775216460228
RMSE train: 0.818492	val: 0.900749	test: 0.906913
MAE train: 0.618753	val: 0.682637	test: 0.697267

Epoch: 39
Loss: 1.1471735835075378
RMSE train: 0.821329	val: 0.904242	test: 0.908313
MAE train: 0.621657	val: 0.691535	test: 0.694694

Epoch: 40
Loss: 1.2444133162498474
RMSE train: 0.761455	val: 0.874209	test: 0.878145
MAE train: 0.579956	val: 0.679210	test: 0.684238

Epoch: 41
Loss: 0.9151046723127365
RMSE train: 0.778986	val: 0.881529	test: 0.887267
MAE train: 0.602268	val: 0.705906	test: 0.693957

Epoch: 42
Loss: 0.9523041397333145
RMSE train: 0.747868	val: 0.872673	test: 0.862620
MAE train: 0.557561	val: 0.667576	test: 0.664711

Epoch: 43
Loss: 0.8087394535541534
RMSE train: 0.774540	val: 0.886025	test: 0.904059
MAE train: 0.574541	val: 0.669144	test: 0.695138

Epoch: 44
Loss: 1.4316133558750153
RMSE train: 0.806520	val: 0.911077	test: 0.923192
MAE train: 0.598363	val: 0.690126	test: 0.697829

Epoch: 45
Loss: 0.916086345911026
RMSE train: 0.743670	val: 0.849840	test: 0.865710
MAE train: 0.559342	val: 0.657504	test: 0.657972

Epoch: 46
Loss: 0.877049595117569
RMSE train: 0.728894	val: 0.861551	test: 0.853403
MAE train: 0.564192	val: 0.680572	test: 0.661499

Epoch: 47
Loss: 1.236573040485382
RMSE train: 0.716088	val: 0.872785	test: 0.853646
MAE train: 0.550269	val: 0.664381	test: 0.653449

Epoch: 48
Loss: 0.923640176653862
RMSE train: 0.863836	val: 0.933419	test: 0.991553
MAE train: 0.636118	val: 0.715343	test: 0.734165

Epoch: 49
Loss: 1.1114459335803986
RMSE train: 0.891461	val: 0.989390	test: 1.046091
MAE train: 0.668416	val: 0.736883	test: 0.768927

Epoch: 50
Loss: 0.9490169435739517
RMSE train: 0.899703	val: 1.028527	test: 1.037827
MAE train: 0.672325	val: 0.773292	test: 0.765834

Epoch: 51
Loss: 0.8602966368198395
RMSE train: 0.821768	val: 0.908547	test: 0.929242
MAE train: 0.626115	val: 0.705250	test: 0.688973

Epoch: 52
Loss: 0.9716423451900482
RMSE train: 0.838712	val: 0.874750	test: 0.934656
MAE train: 0.645899	val: 0.702607	test: 0.713604

Epoch: 53
Loss: 0.9342864900827408
RMSE train: 0.778947	val: 0.840286	test: 0.886750
MAE train: 0.590497	val: 0.646773	test: 0.674554

Epoch: 54
Loss: 0.7510316371917725
RMSE train: 0.748756	val: 0.842309	test: 0.862490
MAE train: 0.559006	val: 0.623370	test: 0.652754

Epoch: 55
Loss: 0.7961218804121017
RMSE train: 0.739348	val: 0.852097	test: 0.853548
MAE train: 0.550900	val: 0.636181	test: 0.637888

Epoch: 56
Loss: 0.7492264062166214
RMSE train: 0.738655	val: 0.844962	test: 0.848737
MAE train: 0.550883	val: 0.637667	test: 0.621305

Epoch: 57
Loss: 0.9669502228498459
RMSE train: 0.753282	val: 0.882494	test: 0.846269
MAE train: 0.558521	val: 0.670297	test: 0.633475

Epoch: 58
Loss: 0.8213476091623306
RMSE train: 0.814751	val: 0.995374	test: 0.871605
MAE train: 0.590895	val: 0.720514	test: 0.643473

Epoch: 59
Loss: 0.8810144513845444
RMSE train: 0.818414	val: 0.978501	test: 0.870667
MAE train: 0.590266	val: 0.738404	test: 0.640896

Epoch: 60
Loss: 0.7361862510442734
RMSE train: 0.810983	val: 0.936363	test: 0.865962
MAE train: 0.607024	val: 0.733776	test: 0.649878

Epoch: 61
Loss: 0.814292311668396
RMSE train: 0.779556	val: 0.883240	test: 0.849902
MAE train: 0.580117	val: 0.675387	test: 0.639895

Epoch: 62
Loss: 0.7894593179225922
RMSE train: 0.725791	val: 0.808618	test: 0.846708
MAE train: 0.552271	val: 0.614727	test: 0.647398

Epoch: 63
Loss: 1.0228414833545685
RMSE train: 0.722388	val: 0.802012	test: 0.858708
MAE train: 0.561373	val: 0.628025	test: 0.669105

Epoch: 64
Loss: 1.1058769226074219
RMSE train: 0.689462	val: 0.765066	test: 0.831026
MAE train: 0.539593	val: 0.618949	test: 0.640149

Epoch: 65
Loss: 0.9377414584159851
RMSE train: 0.680833	val: 0.753542	test: 0.867805
MAE train: 0.532853	val: 0.618192	test: 0.663396

Epoch: 66
Loss: 0.9368453621864319
RMSE train: 0.679799	val: 0.758029	test: 0.848135
MAE train: 0.520011	val: 0.596356	test: 0.643221

Epoch: 67
Loss: 0.7135220319032669
RMSE train: 0.763676	val: 0.840963	test: 0.843719
MAE train: 0.569867	val: 0.639574	test: 0.626351

Epoch: 68
Loss: 0.7173423022031784
RMSE train: 0.784155	val: 0.854675	test: 0.838304
MAE train: 0.586481	val: 0.655736	test: 0.623721

Epoch: 69
Loss: 0.8169782608747482
RMSE train: 0.743885	val: 0.841091	test: 0.824326
MAE train: 0.556475	val: 0.624879	test: 0.617635

Epoch: 70
Loss: 0.8104654997587204
RMSE train: 0.731459	val: 0.818940	test: 0.824475
MAE train: 0.547077	val: 0.611778	test: 0.622042

Epoch: 71
Loss: 0.8095448762178421
RMSE train: 0.691178	val: 0.780728	test: 0.805936
MAE train: 0.526200	val: 0.609022	test: 0.598780

Epoch: 72
Loss: 0.7212781608104706
RMSE train: 0.703359	val: 0.783589	test: 0.815248
MAE train: 0.531243	val: 0.606460	test: 0.596008

Epoch: 73
Loss: 0.9388388991355896
RMSE train: 0.665029	val: 0.746037	test: 0.795163
MAE train: 0.516387	val: 0.572629	test: 0.613318

Epoch: 74
Loss: 0.9635641723871231
RMSE train: 0.683035	val: 0.806820	test: 0.818054
MAE train: 0.542296	val: 0.616525	test: 0.625315

Epoch: 75
Loss: 0.7012125104665756
RMSE train: 0.717845	val: 0.851719	test: 0.803915
MAE train: 0.560072	val: 0.645945	test: 0.611109

Epoch: 76
Loss: 0.8302952945232391
RMSE train: 0.802322	val: 0.935327	test: 0.850060
MAE train: 0.606696	val: 0.700414	test: 0.640598

Epoch: 77
Loss: 0.7608263492584229
RMSE train: 0.837797	val: 0.944225	test: 0.910045
MAE train: 0.622764	val: 0.701759	test: 0.672407

Epoch: 78
Loss: 0.8674066215753555
RMSE train: 0.691339	val: 0.806209	test: 0.830552
MAE train: 0.517951	val: 0.585914	test: 0.623133

Epoch: 79
Loss: 0.890500620007515
RMSE train: 0.667291	val: 0.769052	test: 0.824623
MAE train: 0.518329	val: 0.587017	test: 0.650148

Epoch: 80
Loss: 0.7356756627559662
RMSE train: 0.780615	val: 0.896651	test: 0.903438
MAE train: 0.597282	val: 0.672681	test: 0.695048

Epoch: 81
Loss: 1.2742105722427368
RMSE train: 0.858856	val: 0.951262	test: 0.928323
MAE train: 0.658238	val: 0.726498	test: 0.722957

Epoch: 82
Loss: 0.8713339269161224
RMSE train: 0.777090	val: 0.844511	test: 0.847006
MAE train: 0.590373	val: 0.639524	test: 0.624820

Epoch: 83
Loss: 0.868104875087738
RMSE train: 0.690203	val: 0.767835	test: 0.825441
MAE train: 0.520252	val: 0.568736	test: 0.611455

Epoch: 84
Loss: 0.6729840487241745
RMSE train: 1.128808	val: 1.178381	test: 1.190937
MAE train: 0.836480	val: 0.863872	test: 0.858962

Epoch: 24
Loss: 1.4985897541046143
RMSE train: 1.201418	val: 1.235732	test: 1.226350
MAE train: 0.907783	val: 0.925947	test: 0.916080

Epoch: 25
Loss: 1.3926703929901123
RMSE train: 1.003743	val: 1.061553	test: 1.047459
MAE train: 0.739781	val: 0.781624	test: 0.766729

Epoch: 26
Loss: 1.403112530708313
RMSE train: 0.847194	val: 0.947486	test: 0.935550
MAE train: 0.647271	val: 0.727317	test: 0.697373

Epoch: 27
Loss: 1.165024757385254
RMSE train: 0.837101	val: 0.957729	test: 0.939165
MAE train: 0.640684	val: 0.750430	test: 0.716418

Epoch: 28
Loss: 1.5791873037815094
RMSE train: 0.921671	val: 1.021173	test: 0.994780
MAE train: 0.705676	val: 0.790789	test: 0.757925

Epoch: 29
Loss: 1.0668862611055374
RMSE train: 0.966736	val: 1.086187	test: 1.055940
MAE train: 0.731670	val: 0.820318	test: 0.797620

Epoch: 30
Loss: 1.2376439571380615
RMSE train: 0.979525	val: 1.087065	test: 1.081329
MAE train: 0.736037	val: 0.826094	test: 0.831931

Epoch: 31
Loss: 1.2671259343624115
RMSE train: 0.923097	val: 1.007918	test: 1.033019
MAE train: 0.695782	val: 0.767065	test: 0.779208

Epoch: 32
Loss: 1.2219734489917755
RMSE train: 0.846516	val: 0.938119	test: 0.976302
MAE train: 0.642133	val: 0.734371	test: 0.731968

Epoch: 33
Loss: 1.1455728113651276
RMSE train: 0.811494	val: 0.945731	test: 0.939742
MAE train: 0.615582	val: 0.695229	test: 0.701785

Epoch: 34
Loss: 1.129773035645485
RMSE train: 0.827672	val: 0.936565	test: 0.956241
MAE train: 0.630695	val: 0.688527	test: 0.717841

Epoch: 35
Loss: 1.1173998564481735
RMSE train: 0.851759	val: 0.974404	test: 1.001174
MAE train: 0.644963	val: 0.737673	test: 0.765795

Epoch: 36
Loss: 1.0671363174915314
RMSE train: 0.891965	val: 1.021572	test: 1.064389
MAE train: 0.685991	val: 0.780573	test: 0.819141

Epoch: 37
Loss: 1.0858985632658005
RMSE train: 0.830785	val: 0.945631	test: 1.004833
MAE train: 0.641702	val: 0.733426	test: 0.776006

Epoch: 38
Loss: 1.1136004030704498
RMSE train: 0.805617	val: 0.900992	test: 0.950302
MAE train: 0.612661	val: 0.680772	test: 0.725797

Epoch: 39
Loss: 1.1879731863737106
RMSE train: 0.799750	val: 0.902917	test: 0.941654
MAE train: 0.617404	val: 0.670772	test: 0.715894

Epoch: 40
Loss: 0.9060337245464325
RMSE train: 0.746087	val: 0.840208	test: 0.923468
MAE train: 0.580274	val: 0.645757	test: 0.702814

Epoch: 41
Loss: 1.1015612483024597
RMSE train: 0.779922	val: 0.862299	test: 0.981134
MAE train: 0.609356	val: 0.682166	test: 0.747319

Epoch: 42
Loss: 0.9025283008813858
RMSE train: 0.753735	val: 0.870826	test: 0.968693
MAE train: 0.580834	val: 0.689103	test: 0.737161

Epoch: 43
Loss: 0.9627836495637894
RMSE train: 0.719755	val: 0.853003	test: 0.960047
MAE train: 0.567020	val: 0.671191	test: 0.726305

Epoch: 44
Loss: 0.9550969004631042
RMSE train: 0.704953	val: 0.844980	test: 0.927072
MAE train: 0.543131	val: 0.632637	test: 0.683957

Epoch: 45
Loss: 0.8560821861028671
RMSE train: 0.740689	val: 0.883340	test: 0.921819
MAE train: 0.560878	val: 0.642923	test: 0.682296

Epoch: 46
Loss: 0.8834034502506256
RMSE train: 0.752486	val: 0.857461	test: 0.936654
MAE train: 0.575848	val: 0.647140	test: 0.699015

Epoch: 47
Loss: 0.8543068617582321
RMSE train: 0.752759	val: 0.874826	test: 0.932583
MAE train: 0.566860	val: 0.648930	test: 0.701905

Epoch: 48
Loss: 0.8398135006427765
RMSE train: 0.737382	val: 0.865553	test: 0.938982
MAE train: 0.576618	val: 0.670656	test: 0.712204

Epoch: 49
Loss: 0.8300401568412781
RMSE train: 0.743119	val: 0.891873	test: 0.968172
MAE train: 0.581456	val: 0.701416	test: 0.741764

Epoch: 50
Loss: 0.9338165372610092
RMSE train: 0.757872	val: 0.918783	test: 0.962697
MAE train: 0.581843	val: 0.704108	test: 0.737498

Epoch: 51
Loss: 1.1034793555736542
RMSE train: 0.913237	val: 1.053379	test: 1.045692
MAE train: 0.682368	val: 0.783863	test: 0.775437

Epoch: 52
Loss: 0.7539455443620682
RMSE train: 0.865237	val: 0.978969	test: 1.013489
MAE train: 0.656996	val: 0.744176	test: 0.743937

Epoch: 53
Loss: 1.1229202151298523
RMSE train: 0.780991	val: 0.828512	test: 0.957778
MAE train: 0.605107	val: 0.660478	test: 0.716201

Epoch: 54
Loss: 0.7602891027927399
RMSE train: 0.799666	val: 0.863886	test: 0.955449
MAE train: 0.637142	val: 0.688422	test: 0.742828

Epoch: 55
Loss: 0.799619123339653
RMSE train: 0.756935	val: 0.861472	test: 0.867872
MAE train: 0.582325	val: 0.624293	test: 0.647264

Epoch: 56
Loss: 1.1304022669792175
RMSE train: 0.755347	val: 0.859126	test: 0.879616
MAE train: 0.580440	val: 0.631086	test: 0.660645

Epoch: 57
Loss: 0.8949606567621231
RMSE train: 0.663007	val: 0.752688	test: 0.860620
MAE train: 0.514301	val: 0.577873	test: 0.654159

Epoch: 58
Loss: 1.371124729514122
RMSE train: 0.681216	val: 0.740097	test: 0.847249
MAE train: 0.528213	val: 0.576438	test: 0.641139

Epoch: 59
Loss: 0.735114187002182
RMSE train: 0.762597	val: 0.801724	test: 0.913814
MAE train: 0.597344	val: 0.631040	test: 0.691628

Epoch: 60
Loss: 0.8551023751497269
RMSE train: 0.761736	val: 0.794537	test: 0.925671
MAE train: 0.602348	val: 0.629715	test: 0.696243

Epoch: 61
Loss: 0.7677705436944962
RMSE train: 0.713256	val: 0.776726	test: 0.897839
MAE train: 0.562304	val: 0.599113	test: 0.675517

Epoch: 62
Loss: 0.7687587589025497
RMSE train: 0.705287	val: 0.794158	test: 0.891553
MAE train: 0.546784	val: 0.603907	test: 0.672378

Epoch: 63
Loss: 0.9348185658454895
RMSE train: 0.766275	val: 0.852936	test: 0.906803
MAE train: 0.565822	val: 0.635740	test: 0.665222

Epoch: 64
Loss: 0.8638142347335815
RMSE train: 0.842048	val: 0.944557	test: 0.956091
MAE train: 0.636850	val: 0.709815	test: 0.702656

Epoch: 65
Loss: 1.0602738112211227
RMSE train: 0.767079	val: 0.880779	test: 0.932037
MAE train: 0.601560	val: 0.690341	test: 0.706056

Epoch: 66
Loss: 0.696318507194519
RMSE train: 0.758448	val: 0.864447	test: 0.931544
MAE train: 0.584003	val: 0.668829	test: 0.699194

Epoch: 67
Loss: 0.858049675822258
RMSE train: 0.738615	val: 0.850596	test: 0.904892
MAE train: 0.560043	val: 0.650369	test: 0.672079

Epoch: 68
Loss: 0.9553857743740082
RMSE train: 0.730994	val: 0.813497	test: 0.921481
MAE train: 0.557249	val: 0.622276	test: 0.685241

Epoch: 69
Loss: 0.8148056864738464
RMSE train: 0.785241	val: 0.895661	test: 0.928088
MAE train: 0.582352	val: 0.694020	test: 0.683048

Epoch: 70
Loss: 0.9517881125211716
RMSE train: 0.815193	val: 0.953258	test: 0.959245
MAE train: 0.599785	val: 0.728540	test: 0.710655

Epoch: 71
Loss: 0.7495259344577789
RMSE train: 0.760359	val: 0.858252	test: 0.916897
MAE train: 0.560896	val: 0.661452	test: 0.667032

Epoch: 72
Loss: 0.8892986625432968
RMSE train: 0.703406	val: 0.849871	test: 0.874836
MAE train: 0.532694	val: 0.639653	test: 0.646485

Epoch: 73
Loss: 0.7517361342906952
RMSE train: 0.750832	val: 0.845079	test: 0.924799
MAE train: 0.586270	val: 0.659042	test: 0.678060

Epoch: 74
Loss: 1.273965746164322
RMSE train: 0.726729	val: 0.831075	test: 0.957422
MAE train: 0.585659	val: 0.684780	test: 0.729964

Epoch: 75
Loss: 0.6240808293223381
RMSE train: 0.714723	val: 0.843747	test: 0.920421
MAE train: 0.558306	val: 0.682872	test: 0.673640

Epoch: 76
Loss: 0.7288313210010529
RMSE train: 0.696810	val: 0.817754	test: 0.902739
MAE train: 0.539923	val: 0.652561	test: 0.654062

Epoch: 77
Loss: 0.8651141971349716
RMSE train: 0.704333	val: 0.806420	test: 0.912507
MAE train: 0.535652	val: 0.637181	test: 0.664546

Epoch: 78
Loss: 0.7117033153772354
RMSE train: 0.661482	val: 0.771543	test: 0.858341
MAE train: 0.492908	val: 0.611506	test: 0.627642

Epoch: 79
Loss: 0.7925311625003815
RMSE train: 0.736464	val: 0.865461	test: 0.882327
MAE train: 0.551087	val: 0.655708	test: 0.659543

Epoch: 80
Loss: 0.7882376462221146
RMSE train: 0.734384	val: 0.887267	test: 0.888075
MAE train: 0.553642	val: 0.672928	test: 0.665262

Epoch: 81
Loss: 0.7702651917934418
RMSE train: 0.715065	val: 0.839857	test: 0.880782
MAE train: 0.544917	val: 0.651536	test: 0.657339

Epoch: 82
Loss: 0.9909474104642868
RMSE train: 0.671213	val: 0.817563	test: 0.882043
MAE train: 0.514703	val: 0.628534	test: 0.667234

Epoch: 83
Loss: 0.6505118906497955
RMSE train: 0.673551	val: 0.838114	test: 0.901589
MAE train: 0.521744	val: 0.622505	test: 0.669697

Epoch: 84
Loss: 0.917029857635498
RMSE train: 1.192908	val: 1.200773	test: 1.268588
MAE train: 0.903551	val: 0.919409	test: 0.964191

Epoch: 24
Loss: 1.7574765980243683
RMSE train: 1.075319	val: 1.077780	test: 1.153818
MAE train: 0.831727	val: 0.832292	test: 0.872555

Epoch: 25
Loss: 1.3299517631530762
RMSE train: 0.903073	val: 0.934719	test: 1.053427
MAE train: 0.701552	val: 0.721857	test: 0.780005

Epoch: 26
Loss: 1.3365990817546844
RMSE train: 0.904146	val: 0.915139	test: 1.054333
MAE train: 0.710871	val: 0.718247	test: 0.800532

Epoch: 27
Loss: 1.1823111176490784
RMSE train: 0.951370	val: 0.993742	test: 1.101703
MAE train: 0.741521	val: 0.781496	test: 0.852498

Epoch: 28
Loss: 1.1910092234611511
RMSE train: 0.981961	val: 1.048312	test: 1.125320
MAE train: 0.759408	val: 0.820304	test: 0.867781

Epoch: 29
Loss: 1.213471919298172
RMSE train: 0.826267	val: 0.900585	test: 0.962960
MAE train: 0.628770	val: 0.708588	test: 0.713615

Epoch: 30
Loss: 1.1480820178985596
RMSE train: 0.808249	val: 0.899326	test: 0.956675
MAE train: 0.616912	val: 0.713095	test: 0.725179

Epoch: 31
Loss: 1.18321293592453
RMSE train: 0.892486	val: 0.950429	test: 1.015888
MAE train: 0.686149	val: 0.737186	test: 0.761732

Epoch: 32
Loss: 1.4927278459072113
RMSE train: 0.770637	val: 0.875331	test: 0.921269
MAE train: 0.601109	val: 0.682169	test: 0.699449

Epoch: 33
Loss: 1.100585550069809
RMSE train: 0.746575	val: 0.874193	test: 0.903480
MAE train: 0.588007	val: 0.689097	test: 0.692641

Epoch: 34
Loss: 1.1283141374588013
RMSE train: 0.768050	val: 0.870230	test: 0.907698
MAE train: 0.603379	val: 0.674687	test: 0.691642

Epoch: 35
Loss: 1.097193881869316
RMSE train: 0.792179	val: 0.879502	test: 0.962218
MAE train: 0.619628	val: 0.694791	test: 0.720899

Epoch: 36
Loss: 0.9236855879426003
RMSE train: 0.783855	val: 0.880898	test: 0.953288
MAE train: 0.608106	val: 0.705403	test: 0.720691

Epoch: 37
Loss: 1.2583426237106323
RMSE train: 0.782207	val: 0.866074	test: 0.928276
MAE train: 0.603220	val: 0.679441	test: 0.692575

Epoch: 38
Loss: 1.1457186341285706
RMSE train: 0.741206	val: 0.797809	test: 0.894273
MAE train: 0.572554	val: 0.624340	test: 0.682698

Epoch: 39
Loss: 1.0252832472324371
RMSE train: 0.818544	val: 0.855626	test: 0.917353
MAE train: 0.629500	val: 0.668176	test: 0.689321

Epoch: 40
Loss: 1.213337019085884
RMSE train: 0.960165	val: 1.009209	test: 1.005468
MAE train: 0.752339	val: 0.795929	test: 0.755892

Epoch: 41
Loss: 0.8422423601150513
RMSE train: 0.740464	val: 0.790424	test: 0.840117
MAE train: 0.568240	val: 0.591979	test: 0.626600

Epoch: 42
Loss: 1.0648268908262253
RMSE train: 0.718079	val: 0.751485	test: 0.851299
MAE train: 0.553202	val: 0.574637	test: 0.636629

Epoch: 43
Loss: 0.9666931927204132
RMSE train: 0.875833	val: 0.899143	test: 0.969849
MAE train: 0.684814	val: 0.726809	test: 0.728875

Epoch: 44
Loss: 0.8938708752393723
RMSE train: 0.890278	val: 0.919158	test: 0.981452
MAE train: 0.692612	val: 0.749185	test: 0.740925

Epoch: 45
Loss: 1.0630610883235931
RMSE train: 0.786503	val: 0.806280	test: 0.907441
MAE train: 0.594544	val: 0.644348	test: 0.679444

Epoch: 46
Loss: 0.9295413494110107
RMSE train: 0.775342	val: 0.813148	test: 0.929433
MAE train: 0.580987	val: 0.611797	test: 0.698534

Epoch: 47
Loss: 1.0154246240854263
RMSE train: 0.785637	val: 0.833623	test: 0.930789
MAE train: 0.608400	val: 0.639768	test: 0.713121

Epoch: 48
Loss: 0.8727684319019318
RMSE train: 0.874904	val: 0.910353	test: 0.989528
MAE train: 0.675093	val: 0.698735	test: 0.770602

Epoch: 49
Loss: 0.9801429510116577
RMSE train: 0.832451	val: 0.851827	test: 0.951218
MAE train: 0.637437	val: 0.662326	test: 0.735750

Epoch: 50
Loss: 0.9151348620653152
RMSE train: 0.748065	val: 0.761124	test: 0.871815
MAE train: 0.572482	val: 0.600783	test: 0.670400

Epoch: 51
Loss: 0.9795077741146088
RMSE train: 0.730108	val: 0.742900	test: 0.867764
MAE train: 0.565786	val: 0.588645	test: 0.670239

Epoch: 52
Loss: 1.0252950936555862
RMSE train: 0.705575	val: 0.738259	test: 0.878071
MAE train: 0.551087	val: 0.577120	test: 0.676240

Epoch: 53
Loss: 0.9414740353822708
RMSE train: 0.751838	val: 0.812637	test: 0.903647
MAE train: 0.593662	val: 0.643078	test: 0.691762

Epoch: 54
Loss: 0.9017459750175476
RMSE train: 0.750263	val: 0.785373	test: 0.951785
MAE train: 0.598104	val: 0.615275	test: 0.724226

Epoch: 55
Loss: 1.2742261588573456
RMSE train: 0.775308	val: 0.806500	test: 0.963068
MAE train: 0.608126	val: 0.637237	test: 0.725144

Epoch: 56
Loss: 0.9870158582925797
RMSE train: 0.728325	val: 0.763780	test: 0.918272
MAE train: 0.569683	val: 0.592175	test: 0.690526

Epoch: 57
Loss: 1.1621157824993134
RMSE train: 0.698396	val: 0.757603	test: 0.891880
MAE train: 0.537201	val: 0.580827	test: 0.681931

Epoch: 58
Loss: 0.8160036355257034
RMSE train: 0.721063	val: 0.789659	test: 0.893987
MAE train: 0.556939	val: 0.606361	test: 0.683286

Epoch: 59
Loss: 0.9651175290346146
RMSE train: 0.749662	val: 0.838319	test: 0.894242
MAE train: 0.584597	val: 0.649947	test: 0.682596

Epoch: 60
Loss: 0.884961411356926
RMSE train: 0.726810	val: 0.796600	test: 0.934427
MAE train: 0.580084	val: 0.630971	test: 0.741756

Epoch: 61
Loss: 0.9104982018470764
RMSE train: 0.704548	val: 0.783234	test: 0.933157
MAE train: 0.549562	val: 0.619697	test: 0.729243

Epoch: 62
Loss: 0.7988682836294174
RMSE train: 0.777527	val: 0.877996	test: 0.948383
MAE train: 0.595375	val: 0.678321	test: 0.730129

Epoch: 63
Loss: 0.6792240589857101
RMSE train: 0.762252	val: 0.859373	test: 0.921764
MAE train: 0.582638	val: 0.664790	test: 0.704737

Epoch: 64
Loss: 1.0508087277412415
RMSE train: 0.693758	val: 0.773784	test: 0.867675
MAE train: 0.532368	val: 0.594358	test: 0.658383

Epoch: 65
Loss: 0.7646467685699463
RMSE train: 0.785775	val: 0.872845	test: 0.916053
MAE train: 0.602966	val: 0.668079	test: 0.687249

Epoch: 66
Loss: 0.8229119777679443
RMSE train: 0.833045	val: 0.927953	test: 0.955854
MAE train: 0.638415	val: 0.708985	test: 0.727463

Epoch: 67
Loss: 0.8776543140411377
RMSE train: 0.784470	val: 0.841525	test: 0.934014
MAE train: 0.598225	val: 0.672735	test: 0.714151

Epoch: 68
Loss: 0.7731449455022812
RMSE train: 0.729135	val: 0.789212	test: 0.907916
MAE train: 0.554178	val: 0.625232	test: 0.691811

Epoch: 69
Loss: 0.7226835712790489
RMSE train: 0.752207	val: 0.827317	test: 0.899842
MAE train: 0.570797	val: 0.645483	test: 0.686500

Epoch: 70
Loss: 0.694214716553688
RMSE train: 0.770932	val: 0.860425	test: 0.893564
MAE train: 0.591182	val: 0.656237	test: 0.669937

Epoch: 71
Loss: 0.6101525127887726
RMSE train: 0.729120	val: 0.822868	test: 0.869129
MAE train: 0.564778	val: 0.633361	test: 0.646899

Epoch: 72
Loss: 0.7433067560195923
RMSE train: 0.690874	val: 0.787063	test: 0.881580
MAE train: 0.541284	val: 0.605448	test: 0.667003

Epoch: 73
Loss: 0.7138145267963409
RMSE train: 0.691775	val: 0.795219	test: 0.929949
MAE train: 0.543224	val: 0.608081	test: 0.704205

Epoch: 74
Loss: 0.6735759079456329
RMSE train: 0.665969	val: 0.784467	test: 0.875213
MAE train: 0.514719	val: 0.599436	test: 0.653690

Epoch: 75
Loss: 0.8125253319740295
RMSE train: 0.768329	val: 0.870044	test: 0.922495
MAE train: 0.591501	val: 0.685424	test: 0.690108

Epoch: 76
Loss: 0.60416629165411
RMSE train: 0.712037	val: 0.807919	test: 0.887088
MAE train: 0.553200	val: 0.636463	test: 0.666538

Epoch: 77
Loss: 0.6793126612901688
RMSE train: 0.669087	val: 0.787404	test: 0.841823
MAE train: 0.522962	val: 0.614506	test: 0.634907

Epoch: 78
Loss: 0.7338321357965469
RMSE train: 0.678125	val: 0.797658	test: 0.838956
MAE train: 0.532949	val: 0.629692	test: 0.632779

Epoch: 79
Loss: 0.6839017271995544
RMSE train: 0.670322	val: 0.806166	test: 0.864550
MAE train: 0.526628	val: 0.614041	test: 0.637229

Epoch: 80
Loss: 0.6875241994857788
RMSE train: 0.635367	val: 0.759102	test: 0.857312
MAE train: 0.504720	val: 0.586273	test: 0.636383

Epoch: 81
Loss: 0.6516833454370499
RMSE train: 0.682837	val: 0.776769	test: 0.888068
MAE train: 0.540779	val: 0.618620	test: 0.673333

Epoch: 82
Loss: 0.8726418912410736
RMSE train: 0.710261	val: 0.807651	test: 0.916038
MAE train: 0.557987	val: 0.634777	test: 0.693178

Epoch: 83
Loss: 0.7599617838859558
RMSE train: 0.648904	val: 0.737750	test: 0.886114
MAE train: 0.505528	val: 0.552040	test: 0.646956

Epoch: 84
Loss: 1.0930068492889404
RMSE train: 0.755194	val: 0.941937	test: 0.826687
MAE train: 0.559562	val: 0.681258	test: 0.611582

Epoch: 24
Loss: 0.9718402177095413
RMSE train: 0.720229	val: 0.942400	test: 0.807369
MAE train: 0.534709	val: 0.674730	test: 0.593210

Epoch: 25
Loss: 0.8429757356643677
RMSE train: 0.748131	val: 0.896729	test: 0.862196
MAE train: 0.568225	val: 0.662706	test: 0.622453

Epoch: 26
Loss: 0.9225857108831406
RMSE train: 0.699301	val: 0.901237	test: 0.820174
MAE train: 0.528373	val: 0.674550	test: 0.602799

Epoch: 27
Loss: 0.8833702802658081
RMSE train: 0.723505	val: 0.959368	test: 0.790145
MAE train: 0.543774	val: 0.718596	test: 0.596903

Epoch: 28
Loss: 0.8354018181562424
RMSE train: 0.697259	val: 0.880718	test: 0.810876
MAE train: 0.526103	val: 0.668264	test: 0.602947

Epoch: 29
Loss: 0.7973194867372513
RMSE train: 0.684835	val: 0.859154	test: 0.823631
MAE train: 0.508082	val: 0.664518	test: 0.604955

Epoch: 30
Loss: 0.7707094252109528
RMSE train: 0.712115	val: 0.884979	test: 0.851710
MAE train: 0.530186	val: 0.685318	test: 0.633560

Epoch: 31
Loss: 0.7517726868391037
RMSE train: 0.692616	val: 0.949014	test: 0.789001
MAE train: 0.511128	val: 0.704321	test: 0.600244

Epoch: 32
Loss: 0.810967743396759
RMSE train: 0.705116	val: 0.901307	test: 0.795210
MAE train: 0.525115	val: 0.658320	test: 0.608951

Epoch: 33
Loss: 0.7147606015205383
RMSE train: 0.801670	val: 1.008747	test: 0.846656
MAE train: 0.589544	val: 0.766631	test: 0.629959

Epoch: 34
Loss: 0.7351719737052917
RMSE train: 0.710539	val: 0.925647	test: 0.785358
MAE train: 0.521777	val: 0.698301	test: 0.579615

Epoch: 35
Loss: 0.7306568622589111
RMSE train: 0.683569	val: 0.880607	test: 0.776725
MAE train: 0.509176	val: 0.658941	test: 0.575166

Epoch: 36
Loss: 0.6466286778450012
RMSE train: 0.663697	val: 0.873042	test: 0.745305
MAE train: 0.496606	val: 0.660812	test: 0.565903

Epoch: 37
Loss: 0.6779607683420181
RMSE train: 0.742251	val: 0.970848	test: 0.779440
MAE train: 0.545199	val: 0.736435	test: 0.604736

Epoch: 38
Loss: 0.6480936110019684
RMSE train: 0.746216	val: 0.929978	test: 0.821287
MAE train: 0.555669	val: 0.705103	test: 0.615995

Epoch: 39
Loss: 0.6455113589763641
RMSE train: 0.645356	val: 0.920707	test: 0.755176
MAE train: 0.473322	val: 0.681390	test: 0.572291

Epoch: 40
Loss: 0.659044474363327
RMSE train: 0.615957	val: 0.851873	test: 0.803275
MAE train: 0.460702	val: 0.651578	test: 0.602533

Epoch: 41
Loss: 0.6332996040582657
RMSE train: 0.625143	val: 0.844818	test: 0.825485
MAE train: 0.470228	val: 0.643969	test: 0.606156

Epoch: 42
Loss: 0.6357401907444
RMSE train: 0.662747	val: 0.902413	test: 0.803908
MAE train: 0.489828	val: 0.687028	test: 0.593621

Epoch: 43
Loss: 0.6303576827049255
RMSE train: 0.700196	val: 0.938542	test: 0.800876
MAE train: 0.512765	val: 0.716024	test: 0.583633

Epoch: 44
Loss: 0.6627066880464554
RMSE train: 0.637177	val: 0.850983	test: 0.770175
MAE train: 0.475264	val: 0.635014	test: 0.554118

Epoch: 45
Loss: 0.5747869461774826
RMSE train: 0.612060	val: 0.853731	test: 0.758899
MAE train: 0.461834	val: 0.616565	test: 0.558129

Epoch: 46
Loss: 0.6297514289617538
RMSE train: 0.629555	val: 0.856628	test: 0.786913
MAE train: 0.471560	val: 0.636586	test: 0.564608

Epoch: 47
Loss: 0.6144188642501831
RMSE train: 0.614262	val: 0.872528	test: 0.760795
MAE train: 0.457416	val: 0.654757	test: 0.546507

Epoch: 48
Loss: 0.5730161219835281
RMSE train: 0.602873	val: 0.884494	test: 0.735184
MAE train: 0.447222	val: 0.650575	test: 0.532130

Epoch: 49
Loss: 0.5836601108312607
RMSE train: 0.679074	val: 0.970296	test: 0.772182
MAE train: 0.503594	val: 0.719387	test: 0.557847

Epoch: 50
Loss: 0.6239952147006989
RMSE train: 0.671830	val: 0.896969	test: 0.783946
MAE train: 0.494193	val: 0.674866	test: 0.570109

Epoch: 51
Loss: 0.6053478568792343
RMSE train: 0.664071	val: 0.855740	test: 0.778357
MAE train: 0.494319	val: 0.660735	test: 0.560464

Epoch: 52
Loss: 0.5267382711172104
RMSE train: 0.722283	val: 0.980560	test: 0.777710
MAE train: 0.535365	val: 0.750388	test: 0.569162

Epoch: 53
Loss: 0.5628318190574646
RMSE train: 0.622020	val: 0.852519	test: 0.740493
MAE train: 0.466136	val: 0.643104	test: 0.529173

Epoch: 54
Loss: 0.5458129197359085
RMSE train: 0.646226	val: 0.882054	test: 0.773127
MAE train: 0.480285	val: 0.670316	test: 0.554026

Epoch: 55
Loss: 0.5842791497707367
RMSE train: 0.660822	val: 0.967474	test: 0.739925
MAE train: 0.495872	val: 0.724317	test: 0.534435

Epoch: 56
Loss: 0.5687333792448044
RMSE train: 0.630235	val: 0.918039	test: 0.730269
MAE train: 0.474593	val: 0.673572	test: 0.534866

Epoch: 57
Loss: 0.488803431391716
RMSE train: 0.625703	val: 0.862253	test: 0.753394
MAE train: 0.469505	val: 0.656927	test: 0.552845

Epoch: 58
Loss: 0.532172366976738
RMSE train: 0.619340	val: 0.859194	test: 0.755201
MAE train: 0.472006	val: 0.660785	test: 0.550079

Epoch: 59
Loss: 0.5638043880462646
RMSE train: 0.602874	val: 0.871880	test: 0.736089
MAE train: 0.452054	val: 0.655842	test: 0.527386

Epoch: 60
Loss: 0.504438728094101
RMSE train: 0.642626	val: 0.899162	test: 0.773663
MAE train: 0.474980	val: 0.689247	test: 0.548830

Epoch: 61
Loss: 0.5379182547330856
RMSE train: 0.572920	val: 0.852684	test: 0.739967
MAE train: 0.433146	val: 0.639383	test: 0.535494

Epoch: 62
Loss: 0.535151906311512
RMSE train: 0.578186	val: 0.878978	test: 0.735091
MAE train: 0.439050	val: 0.651021	test: 0.537698

Epoch: 63
Loss: 0.5095270946621895
RMSE train: 0.588605	val: 0.884624	test: 0.736163
MAE train: 0.442426	val: 0.652811	test: 0.534946

Epoch: 64
Loss: 0.53326465934515
RMSE train: 0.589130	val: 0.934318	test: 0.719631
MAE train: 0.449080	val: 0.681045	test: 0.530229

Epoch: 65
Loss: 0.5504564046859741
RMSE train: 0.590305	val: 0.859920	test: 0.743695
MAE train: 0.442448	val: 0.646184	test: 0.532617

Epoch: 66
Loss: 0.501142106950283
RMSE train: 0.620110	val: 0.912216	test: 0.744227
MAE train: 0.466741	val: 0.680296	test: 0.527880

Epoch: 67
Loss: 0.4868009239435196
RMSE train: 0.566250	val: 0.849471	test: 0.714974
MAE train: 0.425212	val: 0.641192	test: 0.514723

Epoch: 68
Loss: 0.5254195109009743
RMSE train: 0.599339	val: 0.864275	test: 0.720420
MAE train: 0.446780	val: 0.667947	test: 0.521647

Epoch: 69
Loss: 0.4611971154808998
RMSE train: 0.589259	val: 0.836073	test: 0.736419
MAE train: 0.442377	val: 0.648271	test: 0.523845

Epoch: 70
Loss: 0.47435807436704636
RMSE train: 0.598505	val: 0.852986	test: 0.745877
MAE train: 0.444836	val: 0.650166	test: 0.537283

Epoch: 71
Loss: 0.4728775918483734
RMSE train: 0.642198	val: 0.900364	test: 0.755503
MAE train: 0.477192	val: 0.694596	test: 0.552527

Epoch: 72
Loss: 0.4411289244890213
RMSE train: 0.623668	val: 0.911765	test: 0.724385
MAE train: 0.464080	val: 0.681439	test: 0.530394

Epoch: 73
Loss: 0.4878772124648094
RMSE train: 0.594919	val: 0.863436	test: 0.718613
MAE train: 0.451868	val: 0.654710	test: 0.521701

Epoch: 74
Loss: 0.41534697264432907
RMSE train: 0.602529	val: 0.856739	test: 0.747815
MAE train: 0.458135	val: 0.647134	test: 0.532548

Epoch: 75
Loss: 0.4782670885324478
RMSE train: 0.623940	val: 0.872525	test: 0.759844
MAE train: 0.468857	val: 0.663343	test: 0.547193

Epoch: 76
Loss: 0.4666924327611923
RMSE train: 0.617504	val: 0.857572	test: 0.751834
MAE train: 0.460894	val: 0.638060	test: 0.538036

Epoch: 77
Loss: 0.4745299071073532
RMSE train: 0.663652	val: 0.988038	test: 0.750055
MAE train: 0.496794	val: 0.732955	test: 0.563123

Epoch: 78
Loss: 0.47743403911590576
RMSE train: 0.590712	val: 0.886636	test: 0.745845
MAE train: 0.446047	val: 0.663622	test: 0.553653

Epoch: 79
Loss: 0.47726966440677643
RMSE train: 0.571255	val: 0.842298	test: 0.746692
MAE train: 0.443747	val: 0.640967	test: 0.562652

Epoch: 80
Loss: 0.5050944834947586
RMSE train: 0.568835	val: 0.869957	test: 0.717342
MAE train: 0.424051	val: 0.653665	test: 0.535181

Epoch: 81
Loss: 0.460212767124176
RMSE train: 0.621844	val: 0.987433	test: 0.738021
MAE train: 0.465167	val: 0.716232	test: 0.562668

Epoch: 82
Loss: 0.42604900896549225
RMSE train: 0.552264	val: 0.819748	test: 0.742323
MAE train: 0.422719	val: 0.627216	test: 0.562319

Epoch: 83
Loss: 0.46688786894083023
RMSE train: 0.529995	val: 0.873882	test: 0.715581
MAE train: 0.413782	val: 0.671007	test: 0.544123

RMSE train: 0.774350	val: 0.974708	test: 0.856838
MAE train: 0.579993	val: 0.720530	test: 0.654597

Epoch: 24
Loss: 1.0504499077796936
RMSE train: 0.858408	val: 0.963596	test: 0.930607
MAE train: 0.634461	val: 0.701191	test: 0.692553

Epoch: 25
Loss: 0.9218786358833313
RMSE train: 0.877846	val: 1.018334	test: 0.925879
MAE train: 0.655807	val: 0.765482	test: 0.679559

Epoch: 26
Loss: 0.8989118337631226
RMSE train: 0.725454	val: 0.928721	test: 0.850200
MAE train: 0.551800	val: 0.664654	test: 0.643155

Epoch: 27
Loss: 0.9240119159221649
RMSE train: 0.700064	val: 0.885525	test: 0.832184
MAE train: 0.544999	val: 0.680178	test: 0.636704

Epoch: 28
Loss: 0.8138816654682159
RMSE train: 0.745411	val: 0.923630	test: 0.856023
MAE train: 0.574489	val: 0.686968	test: 0.639939

Epoch: 29
Loss: 0.8999903500080109
RMSE train: 0.746235	val: 0.934252	test: 0.857983
MAE train: 0.580616	val: 0.707257	test: 0.655482

Epoch: 30
Loss: 0.8278277814388275
RMSE train: 0.664905	val: 0.920565	test: 0.782526
MAE train: 0.502389	val: 0.664894	test: 0.589159

Epoch: 31
Loss: 0.7853402495384216
RMSE train: 0.801430	val: 1.059285	test: 0.866767
MAE train: 0.620218	val: 0.816600	test: 0.658501

Epoch: 32
Loss: 0.8294496834278107
RMSE train: 0.665963	val: 0.909852	test: 0.838844
MAE train: 0.509250	val: 0.683172	test: 0.634260

Epoch: 33
Loss: 0.8096804171800613
RMSE train: 0.670722	val: 0.916599	test: 0.813359
MAE train: 0.502453	val: 0.677904	test: 0.609378

Epoch: 34
Loss: 0.669931173324585
RMSE train: 0.697778	val: 0.888845	test: 0.820758
MAE train: 0.518206	val: 0.667284	test: 0.588990

Epoch: 35
Loss: 0.7147247642278671
RMSE train: 0.650942	val: 0.858292	test: 0.789474
MAE train: 0.487760	val: 0.638283	test: 0.584768

Epoch: 36
Loss: 0.7689273506402969
RMSE train: 0.658968	val: 0.907763	test: 0.776355
MAE train: 0.492715	val: 0.658157	test: 0.581814

Epoch: 37
Loss: 0.6927682757377625
RMSE train: 0.699118	val: 0.940802	test: 0.805830
MAE train: 0.525710	val: 0.696349	test: 0.604801

Epoch: 38
Loss: 0.7221478670835495
RMSE train: 0.760273	val: 0.947067	test: 0.846789
MAE train: 0.575198	val: 0.719070	test: 0.635675

Epoch: 39
Loss: 0.7207227051258087
RMSE train: 0.718721	val: 0.933601	test: 0.846751
MAE train: 0.540031	val: 0.691058	test: 0.636390

Epoch: 40
Loss: 0.644585058093071
RMSE train: 0.817279	val: 1.083362	test: 0.889593
MAE train: 0.631015	val: 0.829957	test: 0.668745

Epoch: 41
Loss: 0.6957961469888687
RMSE train: 0.764369	val: 0.979397	test: 0.866779
MAE train: 0.582721	val: 0.747061	test: 0.640712

Epoch: 42
Loss: 0.646128460764885
RMSE train: 0.763096	val: 0.910533	test: 0.878325
MAE train: 0.577284	val: 0.687471	test: 0.646604

Epoch: 43
Loss: 0.6740998029708862
RMSE train: 0.726866	val: 0.986030	test: 0.821039
MAE train: 0.564218	val: 0.750933	test: 0.626722

Epoch: 44
Loss: 0.680078312754631
RMSE train: 0.649690	val: 0.934941	test: 0.789964
MAE train: 0.494524	val: 0.691542	test: 0.587782

Epoch: 45
Loss: 0.7323518842458725
RMSE train: 0.685159	val: 0.915154	test: 0.801044
MAE train: 0.512728	val: 0.692237	test: 0.593333

Epoch: 46
Loss: 0.6824007779359818
RMSE train: 0.693036	val: 0.893192	test: 0.808547
MAE train: 0.518553	val: 0.666683	test: 0.597039

Epoch: 47
Loss: 0.5983964055776596
RMSE train: 0.680887	val: 0.964220	test: 0.790726
MAE train: 0.524570	val: 0.732021	test: 0.596468

Epoch: 48
Loss: 0.604626290500164
RMSE train: 0.671059	val: 0.905832	test: 0.816225
MAE train: 0.516708	val: 0.685515	test: 0.611069

Epoch: 49
Loss: 0.5928680449724197
RMSE train: 0.688111	val: 0.920467	test: 0.796914
MAE train: 0.520934	val: 0.699127	test: 0.596385

Epoch: 50
Loss: 0.6164745539426804
RMSE train: 0.678674	val: 0.903983	test: 0.811071
MAE train: 0.513841	val: 0.678943	test: 0.613088

Epoch: 51
Loss: 0.6100397557020187
RMSE train: 0.748355	val: 1.030910	test: 0.814655
MAE train: 0.572847	val: 0.802453	test: 0.619702

Epoch: 52
Loss: 0.5650446563959122
RMSE train: 0.571956	val: 0.873307	test: 0.773974
MAE train: 0.442709	val: 0.660133	test: 0.584965

Epoch: 53
Loss: 0.5872181579470634
RMSE train: 0.570049	val: 0.907397	test: 0.766549
MAE train: 0.443027	val: 0.666121	test: 0.584192

Epoch: 54
Loss: 0.6258815675973892
RMSE train: 0.690743	val: 1.014294	test: 0.796013
MAE train: 0.533942	val: 0.761033	test: 0.607410

Epoch: 55
Loss: 0.579380139708519
RMSE train: 0.631046	val: 0.844760	test: 0.828211
MAE train: 0.498877	val: 0.676857	test: 0.620686

Epoch: 56
Loss: 0.6278667151927948
RMSE train: 0.577794	val: 0.811505	test: 0.758935
MAE train: 0.444870	val: 0.628886	test: 0.577161

Epoch: 57
Loss: 0.5507640689611435
RMSE train: 0.693147	val: 0.953028	test: 0.793084
MAE train: 0.539007	val: 0.745995	test: 0.600768

Epoch: 58
Loss: 0.5713860243558884
RMSE train: 0.611722	val: 0.783475	test: 0.794738
MAE train: 0.463176	val: 0.584757	test: 0.568011

Epoch: 59
Loss: 0.5850250720977783
RMSE train: 0.564539	val: 0.822758	test: 0.743643
MAE train: 0.433662	val: 0.601129	test: 0.556489

Epoch: 60
Loss: 0.5312596037983894
RMSE train: 0.689139	val: 1.019081	test: 0.775627
MAE train: 0.538809	val: 0.762730	test: 0.589049

Epoch: 61
Loss: 0.5044588819146156
RMSE train: 0.572198	val: 0.798700	test: 0.775327
MAE train: 0.436528	val: 0.611244	test: 0.580778

Epoch: 62
Loss: 0.5183262377977371
RMSE train: 0.585224	val: 0.852444	test: 0.758104
MAE train: 0.449577	val: 0.641300	test: 0.574872

Epoch: 63
Loss: 0.5345897227525711
RMSE train: 0.612956	val: 0.902422	test: 0.758658
MAE train: 0.471355	val: 0.677274	test: 0.579488

Epoch: 64
Loss: 0.49461499601602554
RMSE train: 0.592248	val: 0.854462	test: 0.809075
MAE train: 0.458004	val: 0.666250	test: 0.590555

Epoch: 65
Loss: 0.5342266038060188
RMSE train: 0.566784	val: 0.908210	test: 0.760378
MAE train: 0.437647	val: 0.664651	test: 0.559681

Epoch: 66
Loss: 0.48987793922424316
RMSE train: 0.614559	val: 0.950128	test: 0.758690
MAE train: 0.470039	val: 0.708521	test: 0.548601

Epoch: 67
Loss: 0.5809282213449478
RMSE train: 0.584659	val: 0.872854	test: 0.754942
MAE train: 0.438336	val: 0.661494	test: 0.544544

Epoch: 68
Loss: 0.4548763707280159
RMSE train: 0.565721	val: 0.906129	test: 0.731303
MAE train: 0.431033	val: 0.678452	test: 0.560076

Epoch: 69
Loss: 0.5410919114947319
RMSE train: 0.574320	val: 0.915514	test: 0.743810
MAE train: 0.441926	val: 0.686015	test: 0.567397

Epoch: 70
Loss: 0.48966626077890396
RMSE train: 0.585470	val: 0.837231	test: 0.802672
MAE train: 0.447252	val: 0.640937	test: 0.593734

Epoch: 71
Loss: 0.5049431473016739
RMSE train: 0.557768	val: 0.832479	test: 0.789121
MAE train: 0.427476	val: 0.634938	test: 0.593735

Epoch: 72
Loss: 0.5236434936523438
RMSE train: 0.606299	val: 0.958050	test: 0.755199
MAE train: 0.471284	val: 0.716170	test: 0.570119

Epoch: 73
Loss: 0.5152567327022552
RMSE train: 0.626578	val: 0.865534	test: 0.772755
MAE train: 0.465081	val: 0.649024	test: 0.560314

Epoch: 74
Loss: 0.5334528833627701
RMSE train: 0.578785	val: 0.849071	test: 0.747978
MAE train: 0.433987	val: 0.637345	test: 0.552319

Epoch: 75
Loss: 0.4703436866402626
RMSE train: 0.723899	val: 1.061429	test: 0.797591
MAE train: 0.548078	val: 0.817623	test: 0.613447

Epoch: 76
Loss: 0.4902893155813217
RMSE train: 0.618634	val: 0.852823	test: 0.786885
MAE train: 0.469486	val: 0.650656	test: 0.577281

Epoch: 77
Loss: 0.4931237921118736
RMSE train: 0.620888	val: 0.881163	test: 0.771206
MAE train: 0.475139	val: 0.675039	test: 0.561361

Epoch: 78
Loss: 0.47208167612552643
RMSE train: 0.589869	val: 0.889577	test: 0.750102
MAE train: 0.451335	val: 0.684953	test: 0.558551

Epoch: 79
Loss: 0.4565121456980705
RMSE train: 0.555940	val: 0.863070	test: 0.745811
MAE train: 0.422633	val: 0.634188	test: 0.557871

Epoch: 80
Loss: 0.5233755335211754
RMSE train: 0.553775	val: 0.842658	test: 0.739697
MAE train: 0.418137	val: 0.623106	test: 0.538985

Epoch: 81
Loss: 0.460620641708374
RMSE train: 0.536538	val: 0.864291	test: 0.734826
MAE train: 0.414402	val: 0.640566	test: 0.545571

Epoch: 82
Loss: 0.47526079416275024
RMSE train: 0.535150	val: 0.814520	test: 0.734045
MAE train: 0.411960	val: 0.616939	test: 0.552871

Epoch: 83
Loss: 0.42798391729593277
RMSE train: 0.561514	val: 0.826847	test: 0.744700
MAE train: 0.432583	val: 0.631034	test: 0.559810
RMSE train: 0.920855	val: 1.010497	test: 0.973518
MAE train: 0.713383	val: 0.791346	test: 0.735395

Epoch: 24
Loss: 0.9594778567552567
RMSE train: 0.822503	val: 0.967073	test: 0.890636
MAE train: 0.641936	val: 0.765213	test: 0.675523

Epoch: 25
Loss: 0.9484608322381973
RMSE train: 0.727058	val: 0.921649	test: 0.820657
MAE train: 0.562142	val: 0.712399	test: 0.637578

Epoch: 26
Loss: 0.9292531162500381
RMSE train: 0.810307	val: 0.964166	test: 0.899155
MAE train: 0.627399	val: 0.745220	test: 0.687950

Epoch: 27
Loss: 0.8965510725975037
RMSE train: 0.911801	val: 1.068406	test: 0.941823
MAE train: 0.709373	val: 0.814482	test: 0.717687

Epoch: 28
Loss: 0.8666675835847855
RMSE train: 0.932696	val: 1.176301	test: 0.924869
MAE train: 0.711247	val: 0.880228	test: 0.712775

Epoch: 29
Loss: 0.8232926577329636
RMSE train: 0.842803	val: 1.059927	test: 0.885579
MAE train: 0.647746	val: 0.820326	test: 0.690998

Epoch: 30
Loss: 0.8162854015827179
RMSE train: 0.797208	val: 0.929225	test: 0.899588
MAE train: 0.611977	val: 0.709564	test: 0.683212

Epoch: 31
Loss: 0.8925101608037949
RMSE train: 0.815884	val: 1.022195	test: 0.861087
MAE train: 0.618579	val: 0.784827	test: 0.647522

Epoch: 32
Loss: 0.7921020537614822
RMSE train: 0.755324	val: 0.938494	test: 0.815068
MAE train: 0.574819	val: 0.730410	test: 0.599911

Epoch: 33
Loss: 0.8721897900104523
RMSE train: 0.782987	val: 0.937982	test: 0.845392
MAE train: 0.589434	val: 0.739789	test: 0.627163

Epoch: 34
Loss: 0.7704112082719803
RMSE train: 0.850942	val: 1.047581	test: 0.881499
MAE train: 0.642230	val: 0.804924	test: 0.668877

Epoch: 35
Loss: 0.7708465903997421
RMSE train: 0.745061	val: 0.937379	test: 0.799696
MAE train: 0.560593	val: 0.694464	test: 0.605344

Epoch: 36
Loss: 0.7342719882726669
RMSE train: 0.791492	val: 0.973748	test: 0.829313
MAE train: 0.593277	val: 0.733557	test: 0.636422

Epoch: 37
Loss: 0.7730587422847748
RMSE train: 0.797640	val: 0.965800	test: 0.862668
MAE train: 0.608161	val: 0.740909	test: 0.644367

Epoch: 38
Loss: 0.7389842122793198
RMSE train: 0.702406	val: 0.937425	test: 0.820407
MAE train: 0.541670	val: 0.733036	test: 0.629817

Epoch: 39
Loss: 0.7143371105194092
RMSE train: 0.734888	val: 0.944842	test: 0.851245
MAE train: 0.564417	val: 0.725613	test: 0.659435

Epoch: 40
Loss: 0.701835960149765
RMSE train: 0.726340	val: 0.897326	test: 0.838047
MAE train: 0.557942	val: 0.687991	test: 0.632883

Epoch: 41
Loss: 0.6656222641468048
RMSE train: 0.693637	val: 0.905997	test: 0.797489
MAE train: 0.535531	val: 0.702412	test: 0.619316

Epoch: 42
Loss: 0.7107740044593811
RMSE train: 0.744047	val: 0.992095	test: 0.787625
MAE train: 0.578147	val: 0.753549	test: 0.580657

Epoch: 43
Loss: 0.7250576764345169
RMSE train: 0.770335	val: 0.948452	test: 0.819880
MAE train: 0.584650	val: 0.728503	test: 0.603200

Epoch: 44
Loss: 0.716576412320137
RMSE train: 0.812338	val: 1.026572	test: 0.846859
MAE train: 0.598033	val: 0.771264	test: 0.641723

Epoch: 45
Loss: 0.6631596237421036
RMSE train: 0.650711	val: 0.853081	test: 0.763802
MAE train: 0.489504	val: 0.664621	test: 0.559225

Epoch: 46
Loss: 0.6845774501562119
RMSE train: 0.642853	val: 0.898458	test: 0.749986
MAE train: 0.492664	val: 0.668309	test: 0.558253

Epoch: 47
Loss: 0.6566045880317688
RMSE train: 0.669357	val: 0.930961	test: 0.768611
MAE train: 0.525222	val: 0.713053	test: 0.586767

Epoch: 48
Loss: 0.6413310170173645
RMSE train: 0.646414	val: 0.854517	test: 0.810360
MAE train: 0.503547	val: 0.659221	test: 0.607667

Epoch: 49
Loss: 0.6162128746509552
RMSE train: 0.642071	val: 0.857300	test: 0.813270
MAE train: 0.497273	val: 0.652419	test: 0.611639

Epoch: 50
Loss: 0.6134535670280457
RMSE train: 0.609513	val: 0.854897	test: 0.772322
MAE train: 0.468222	val: 0.641932	test: 0.582739

Epoch: 51
Loss: 0.6298742741346359
RMSE train: 0.706681	val: 0.932502	test: 0.797616
MAE train: 0.529122	val: 0.707111	test: 0.598220

Epoch: 52
Loss: 0.5725918710231781
RMSE train: 0.788699	val: 0.959775	test: 0.850167
MAE train: 0.586932	val: 0.734824	test: 0.619469

Epoch: 53
Loss: 0.5898645967245102
RMSE train: 0.737884	val: 0.888846	test: 0.823031
MAE train: 0.553589	val: 0.681884	test: 0.591367

Epoch: 54
Loss: 0.5511178523302078
RMSE train: 0.666506	val: 0.870220	test: 0.768832
MAE train: 0.503102	val: 0.665330	test: 0.567747

Epoch: 55
Loss: 0.606992319226265
RMSE train: 0.634965	val: 0.847598	test: 0.758082
MAE train: 0.485916	val: 0.646547	test: 0.560884

Epoch: 56
Loss: 0.5942486971616745
RMSE train: 0.686712	val: 0.889770	test: 0.793660
MAE train: 0.525137	val: 0.684837	test: 0.596460

Epoch: 57
Loss: 0.5563246458768845
RMSE train: 0.674293	val: 0.821597	test: 0.823051
MAE train: 0.520969	val: 0.632786	test: 0.614313

Epoch: 58
Loss: 0.585458442568779
RMSE train: 0.667367	val: 0.877632	test: 0.771232
MAE train: 0.504843	val: 0.680673	test: 0.569086

Epoch: 59
Loss: 0.5154972076416016
RMSE train: 0.670182	val: 0.838919	test: 0.799915
MAE train: 0.506981	val: 0.659499	test: 0.587098

Epoch: 60
Loss: 0.5574436709284782
RMSE train: 0.712918	val: 0.958222	test: 0.800481
MAE train: 0.538077	val: 0.740603	test: 0.616634

Epoch: 61
Loss: 0.5238666161894798
RMSE train: 0.661743	val: 0.884826	test: 0.788565
MAE train: 0.506675	val: 0.688407	test: 0.591496

Epoch: 62
Loss: 0.5551322549581528
RMSE train: 0.674555	val: 0.885083	test: 0.790052
MAE train: 0.509817	val: 0.684690	test: 0.580437

Epoch: 63
Loss: 0.5085383132100105
RMSE train: 0.682976	val: 0.909517	test: 0.777011
MAE train: 0.512776	val: 0.703743	test: 0.576343

Epoch: 64
Loss: 0.5746338218450546
RMSE train: 0.694849	val: 0.915753	test: 0.788977
MAE train: 0.522740	val: 0.706008	test: 0.587057

Epoch: 65
Loss: 0.5490987300872803
RMSE train: 0.677648	val: 0.919041	test: 0.782090
MAE train: 0.521468	val: 0.710136	test: 0.584519

Epoch: 66
Loss: 0.5439193993806839
RMSE train: 0.619697	val: 0.830275	test: 0.752403
MAE train: 0.471202	val: 0.652600	test: 0.573701

Epoch: 67
Loss: 0.5249485820531845
RMSE train: 0.685970	val: 0.875957	test: 0.776849
MAE train: 0.515024	val: 0.676856	test: 0.570073

Epoch: 68
Loss: 0.5041623562574387
RMSE train: 0.625561	val: 0.854263	test: 0.722839
MAE train: 0.471871	val: 0.646492	test: 0.541958

Epoch: 69
Loss: 0.5472380220890045
RMSE train: 0.589836	val: 0.785684	test: 0.738595
MAE train: 0.450820	val: 0.612178	test: 0.555989

Epoch: 70
Loss: 0.5167064219713211
RMSE train: 0.715799	val: 0.906313	test: 0.793688
MAE train: 0.536122	val: 0.688548	test: 0.569420

Epoch: 71
Loss: 0.481142982840538
RMSE train: 0.601376	val: 0.848391	test: 0.719503
MAE train: 0.449167	val: 0.641526	test: 0.517504

Epoch: 72
Loss: 0.49703487008810043
RMSE train: 0.601918	val: 0.787686	test: 0.763435
MAE train: 0.453067	val: 0.598771	test: 0.548299

Epoch: 73
Loss: 0.4757332429289818
RMSE train: 0.673851	val: 0.841642	test: 0.801260
MAE train: 0.505201	val: 0.649746	test: 0.583930

Epoch: 74
Loss: 0.5164849236607552
RMSE train: 0.594481	val: 0.884828	test: 0.737544
MAE train: 0.456767	val: 0.663604	test: 0.558128

Epoch: 75
Loss: 0.48826541006565094
RMSE train: 0.622278	val: 0.853976	test: 0.763090
MAE train: 0.470873	val: 0.656562	test: 0.572536

Epoch: 76
Loss: 0.4905891418457031
RMSE train: 0.652582	val: 0.802286	test: 0.822411
MAE train: 0.499280	val: 0.627226	test: 0.609173

Epoch: 77
Loss: 0.4738517180085182
RMSE train: 0.702753	val: 0.909771	test: 0.794094
MAE train: 0.520652	val: 0.692287	test: 0.592398

Epoch: 78
Loss: 0.4548749178647995
RMSE train: 0.676001	val: 0.895219	test: 0.764879
MAE train: 0.510369	val: 0.685261	test: 0.553292

Epoch: 79
Loss: 0.44496195018291473
RMSE train: 0.639599	val: 0.884574	test: 0.748611
MAE train: 0.487370	val: 0.675539	test: 0.554139

Epoch: 80
Loss: 0.4712899401783943
RMSE train: 0.649505	val: 0.846931	test: 0.776519
MAE train: 0.493202	val: 0.671774	test: 0.575723

Epoch: 81
Loss: 0.4596000537276268
RMSE train: 0.632298	val: 0.818198	test: 0.774542
MAE train: 0.478465	val: 0.644337	test: 0.569889

Epoch: 82
Loss: 0.4714082106947899
RMSE train: 0.637429	val: 0.831659	test: 0.752912
MAE train: 0.476212	val: 0.646299	test: 0.558308

Epoch: 83
Loss: 0.44631973654031754
RMSE train: 0.714334	val: 0.934957	test: 0.766997
MAE train: 0.524102	val: 0.713659	test: 0.576023

Epoch: 84
Loss: 0.42244454224904376
RMSE train: 0.530139	val: 0.757479	test: 0.848702
MAE train: 0.410164	val: 0.558027	test: 0.617958

Epoch: 85
Loss: 0.4027055899302165
RMSE train: 0.549059	val: 0.760716	test: 0.824007
MAE train: 0.425239	val: 0.551801	test: 0.608065

Epoch: 86
Loss: 0.44844608505566913
RMSE train: 0.577949	val: 0.778542	test: 0.820316
MAE train: 0.446594	val: 0.568090	test: 0.615123

Epoch: 87
Loss: 0.41733243068059284
RMSE train: 0.571366	val: 0.776869	test: 0.812500
MAE train: 0.442204	val: 0.578272	test: 0.611309

Epoch: 88
Loss: 0.46061312158902484
RMSE train: 0.557377	val: 0.770541	test: 0.813435
MAE train: 0.425606	val: 0.576468	test: 0.607022

Epoch: 89
Loss: 0.4945759177207947
RMSE train: 0.545191	val: 0.763636	test: 0.823216
MAE train: 0.411268	val: 0.566293	test: 0.604267

Epoch: 90
Loss: 0.3883213798205058
RMSE train: 0.592643	val: 0.796193	test: 0.839080
MAE train: 0.446039	val: 0.581008	test: 0.615354

Epoch: 91
Loss: 0.4589021106561025
RMSE train: 0.570314	val: 0.777537	test: 0.841180
MAE train: 0.428798	val: 0.558566	test: 0.612542

Epoch: 92
Loss: 0.47500768303871155
RMSE train: 0.580321	val: 0.798877	test: 0.916461
MAE train: 0.434861	val: 0.578951	test: 0.648131

Epoch: 93
Loss: 0.39516879121462506
RMSE train: 0.534262	val: 0.756754	test: 0.870693
MAE train: 0.407400	val: 0.549801	test: 0.624724

Epoch: 94
Loss: 0.43150534232457477
RMSE train: 0.536762	val: 0.765673	test: 0.824249
MAE train: 0.409953	val: 0.556522	test: 0.605764

Epoch: 95
Loss: 0.42893242835998535
RMSE train: 0.540059	val: 0.783878	test: 0.843257
MAE train: 0.411087	val: 0.571993	test: 0.616483

Epoch: 96
Loss: 0.4781787196795146
RMSE train: 0.550594	val: 0.804678	test: 0.899252
MAE train: 0.421244	val: 0.594598	test: 0.640886

Epoch: 97
Loss: 0.42103517055511475
RMSE train: 0.540964	val: 0.789024	test: 0.906696
MAE train: 0.417297	val: 0.590747	test: 0.641058

Epoch: 98
Loss: 0.3697030246257782
RMSE train: 0.523509	val: 0.755798	test: 0.848620
MAE train: 0.401105	val: 0.566449	test: 0.619403

Epoch: 99
Loss: 0.41473735372225445
RMSE train: 0.492334	val: 0.746890	test: 0.840381
MAE train: 0.377784	val: 0.554943	test: 0.613796

Epoch: 100
Loss: 0.4154444833596547
RMSE train: 0.521091	val: 0.774883	test: 0.891675
MAE train: 0.399468	val: 0.572695	test: 0.641309

Epoch: 101
Loss: 0.40574605266253155
RMSE train: 0.554172	val: 0.791475	test: 0.910404
MAE train: 0.428547	val: 0.595008	test: 0.649206

Epoch: 102
Loss: 0.4021031856536865
RMSE train: 0.525542	val: 0.780710	test: 0.865574
MAE train: 0.407228	val: 0.582110	test: 0.619304

Epoch: 103
Loss: 0.45351578791936237
RMSE train: 0.521947	val: 0.772420	test: 0.870496
MAE train: 0.404530	val: 0.554119	test: 0.616282

Epoch: 104
Loss: 0.4604976375897725
RMSE train: 0.543044	val: 0.784205	test: 0.876644
MAE train: 0.417556	val: 0.562528	test: 0.632021

Epoch: 105
Loss: 0.40972620248794556
RMSE train: 0.544636	val: 0.791855	test: 0.861797
MAE train: 0.415883	val: 0.584934	test: 0.635682

Epoch: 106
Loss: 0.3853645821412404
RMSE train: 0.505789	val: 0.775085	test: 0.849463
MAE train: 0.396217	val: 0.578023	test: 0.615516

Epoch: 107
Loss: 0.38240788380304974
RMSE train: 0.465302	val: 0.752451	test: 0.811257
MAE train: 0.364137	val: 0.555151	test: 0.590047

Epoch: 108
Loss: 0.3918835520744324
RMSE train: 0.512422	val: 0.765103	test: 0.829852
MAE train: 0.400528	val: 0.563397	test: 0.616124

Epoch: 109
Loss: 0.4524458050727844
RMSE train: 0.569860	val: 0.797910	test: 0.871183
MAE train: 0.447106	val: 0.594159	test: 0.646078

Epoch: 110
Loss: 0.39818477630615234
RMSE train: 0.540838	val: 0.798440	test: 0.866876
MAE train: 0.409563	val: 0.588819	test: 0.621010

Epoch: 111
Loss: 0.4287045995394389
RMSE train: 0.515452	val: 0.772685	test: 0.833780
MAE train: 0.386222	val: 0.562642	test: 0.604002

Epoch: 112
Loss: 0.3845786948998769
RMSE train: 0.562944	val: 0.801529	test: 0.866290
MAE train: 0.421820	val: 0.583611	test: 0.636729

Epoch: 113
Loss: 0.389466126759847
RMSE train: 0.556638	val: 0.797448	test: 0.884626
MAE train: 0.416115	val: 0.579440	test: 0.633687

Epoch: 114
Loss: 0.39310571551322937
RMSE train: 0.523135	val: 0.765959	test: 0.845314
MAE train: 0.398914	val: 0.554474	test: 0.613988

Epoch: 115
Loss: 0.35631290078163147
RMSE train: 0.554487	val: 0.782033	test: 0.843146
MAE train: 0.420807	val: 0.575798	test: 0.622815

Epoch: 116
Loss: 0.35864777366320294
RMSE train: 0.546485	val: 0.783832	test: 0.856748
MAE train: 0.409568	val: 0.575516	test: 0.634222

Epoch: 117
Loss: 0.3464527130126953
RMSE train: 0.512568	val: 0.762034	test: 0.857239
MAE train: 0.389297	val: 0.561970	test: 0.622610

Epoch: 118
Loss: 0.3672730326652527
RMSE train: 0.509315	val: 0.744512	test: 0.846254
MAE train: 0.395035	val: 0.564941	test: 0.629075

Epoch: 119
Loss: 0.4017229775587718
RMSE train: 0.516729	val: 0.749176	test: 0.858607
MAE train: 0.403538	val: 0.564973	test: 0.637120

Epoch: 120
Loss: 0.38247297207514447
RMSE train: 0.493965	val: 0.749612	test: 0.874507
MAE train: 0.384156	val: 0.550958	test: 0.624854

Epoch: 121
Loss: 0.3910858730475108
RMSE train: 0.529350	val: 0.754948	test: 0.874552
MAE train: 0.400704	val: 0.554072	test: 0.637086

Early stopping
Best (RMSE):	 train: 0.493050	val: 0.728476	test: 0.858009
Best (MAE):	 train: 0.387689	val: 0.545414	test: 0.615195

RMSE train: 0.600737	val: 0.790921	test: 0.904039
MAE train: 0.475305	val: 0.593459	test: 0.677598

Epoch: 85
Loss: 0.46725165843963623
RMSE train: 0.573700	val: 0.777463	test: 0.878574
MAE train: 0.453001	val: 0.597306	test: 0.657499

Epoch: 86
Loss: 0.4467300573984782
RMSE train: 0.616528	val: 0.815519	test: 0.883599
MAE train: 0.479282	val: 0.617804	test: 0.664409

Epoch: 87
Loss: 0.4565177857875824
RMSE train: 0.617498	val: 0.808019	test: 0.905320
MAE train: 0.478579	val: 0.593647	test: 0.675182

Epoch: 88
Loss: 0.5027310848236084
RMSE train: 0.674559	val: 0.851184	test: 0.968226
MAE train: 0.514596	val: 0.629142	test: 0.712523

Epoch: 89
Loss: 0.4473173717657725
RMSE train: 0.597721	val: 0.780443	test: 0.893053
MAE train: 0.456543	val: 0.565087	test: 0.656201

Epoch: 90
Loss: 0.4498908619085948
RMSE train: 0.599436	val: 0.794257	test: 0.889409
MAE train: 0.462005	val: 0.592941	test: 0.655010

Epoch: 91
Loss: 0.4520101149876912
RMSE train: 0.574478	val: 0.782050	test: 0.890488
MAE train: 0.447893	val: 0.586419	test: 0.656469

Epoch: 92
Loss: 0.4306127429008484
RMSE train: 0.550349	val: 0.761627	test: 0.900109
MAE train: 0.427008	val: 0.564870	test: 0.651976

Epoch: 93
Loss: 0.5203679104646047
RMSE train: 0.593632	val: 0.776879	test: 0.907375
MAE train: 0.453197	val: 0.567155	test: 0.658270

Epoch: 94
Loss: 0.40480239192644757
RMSE train: 0.681582	val: 0.832930	test: 0.933411
MAE train: 0.517743	val: 0.605274	test: 0.687666

Epoch: 95
Loss: 0.42477965354919434
RMSE train: 0.645986	val: 0.811524	test: 0.913041
MAE train: 0.493612	val: 0.590278	test: 0.671263

Epoch: 96
Loss: 0.44173096617062885
RMSE train: 0.649715	val: 0.817796	test: 0.912947
MAE train: 0.491235	val: 0.590194	test: 0.680739

Epoch: 97
Loss: 0.4539767801761627
RMSE train: 0.688628	val: 0.847927	test: 0.929060
MAE train: 0.522334	val: 0.610933	test: 0.701452

Epoch: 98
Loss: 0.4370264212290446
RMSE train: 0.750191	val: 0.903185	test: 0.960535
MAE train: 0.576341	val: 0.658442	test: 0.727055

Epoch: 99
Loss: 0.4177588125069936
RMSE train: 0.586267	val: 0.785731	test: 0.865256
MAE train: 0.455371	val: 0.566814	test: 0.644169

Epoch: 100
Loss: 0.4237886965274811
RMSE train: 0.550324	val: 0.767354	test: 0.862134
MAE train: 0.427148	val: 0.555317	test: 0.639246

Epoch: 101
Loss: 0.4392656187216441
RMSE train: 0.601287	val: 0.811685	test: 0.889208
MAE train: 0.463827	val: 0.601147	test: 0.663034

Epoch: 102
Loss: 0.4802012840906779
RMSE train: 0.609472	val: 0.820003	test: 0.896863
MAE train: 0.471432	val: 0.604200	test: 0.661987

Epoch: 103
Loss: 0.44355300068855286
RMSE train: 0.544521	val: 0.765142	test: 0.856980
MAE train: 0.424732	val: 0.555067	test: 0.628459

Epoch: 104
Loss: 0.3966483275095622
RMSE train: 0.662859	val: 0.851412	test: 0.957034
MAE train: 0.508767	val: 0.639165	test: 0.710009

Epoch: 105
Loss: 0.45440220832824707
RMSE train: 0.567400	val: 0.772373	test: 0.868438
MAE train: 0.444209	val: 0.570438	test: 0.642823

Epoch: 106
Loss: 0.38672324021657306
RMSE train: 0.559779	val: 0.786367	test: 0.863434
MAE train: 0.445579	val: 0.603917	test: 0.646357

Epoch: 107
Loss: 0.44342732429504395
RMSE train: 0.537803	val: 0.766413	test: 0.859557
MAE train: 0.420128	val: 0.566487	test: 0.634174

Epoch: 108
Loss: 0.47973235448201496
RMSE train: 0.635403	val: 0.831471	test: 0.919980
MAE train: 0.480895	val: 0.609807	test: 0.689669

Epoch: 109
Loss: 0.43587859471638996
RMSE train: 0.573607	val: 0.783059	test: 0.865906
MAE train: 0.436299	val: 0.560789	test: 0.632240

Epoch: 110
Loss: 0.40053046743075055
RMSE train: 0.525688	val: 0.753860	test: 0.838151
MAE train: 0.407976	val: 0.557993	test: 0.612542

Epoch: 111
Loss: 0.39165128270785016
RMSE train: 0.526328	val: 0.752062	test: 0.856570
MAE train: 0.403672	val: 0.548825	test: 0.626121

Epoch: 112
Loss: 0.3742430508136749
RMSE train: 0.576465	val: 0.783638	test: 0.880211
MAE train: 0.447601	val: 0.578585	test: 0.651643

Epoch: 113
Loss: 0.4019436041514079
RMSE train: 0.558490	val: 0.776322	test: 0.858585
MAE train: 0.429250	val: 0.574856	test: 0.635273

Epoch: 114
Loss: 0.4088342785835266
RMSE train: 0.543723	val: 0.769888	test: 0.861568
MAE train: 0.426923	val: 0.569192	test: 0.641116

Epoch: 115
Loss: 0.3861868679523468
RMSE train: 0.543911	val: 0.773744	test: 0.869670
MAE train: 0.423677	val: 0.571557	test: 0.644382

Epoch: 116
Loss: 0.4398167630036672
RMSE train: 0.521055	val: 0.757571	test: 0.854787
MAE train: 0.407206	val: 0.565804	test: 0.632875

Epoch: 117
Loss: 0.36490801970163983
RMSE train: 0.511567	val: 0.747934	test: 0.848929
MAE train: 0.399616	val: 0.558709	test: 0.630470

Epoch: 118
Loss: 0.37236641844113666
RMSE train: 0.497612	val: 0.740224	test: 0.842278
MAE train: 0.388398	val: 0.546140	test: 0.624528

Epoch: 119
Loss: 0.31485681732495624
RMSE train: 0.505610	val: 0.751052	test: 0.853291
MAE train: 0.391711	val: 0.549521	test: 0.626750

Epoch: 120
Loss: 0.42419658104578656
RMSE train: 0.528100	val: 0.760291	test: 0.842162
MAE train: 0.408577	val: 0.554131	test: 0.621474

Epoch: 121
Loss: 0.3488953610261281
RMSE train: 0.533682	val: 0.768141	test: 0.852036
MAE train: 0.416647	val: 0.564938	test: 0.624404

Epoch: 122
Loss: 0.38199617465337116
RMSE train: 0.529917	val: 0.764554	test: 0.834666
MAE train: 0.413499	val: 0.567134	test: 0.614952

Epoch: 123
Loss: 0.40005605419476825
RMSE train: 0.603362	val: 0.808318	test: 0.854619
MAE train: 0.459635	val: 0.574829	test: 0.637268

Epoch: 124
Loss: 0.4166272083918254
RMSE train: 0.650440	val: 0.852457	test: 0.906541
MAE train: 0.492734	val: 0.618398	test: 0.676345

Epoch: 125
Loss: 0.3775687317053477
RMSE train: 0.486062	val: 0.738540	test: 0.831642
MAE train: 0.381181	val: 0.548787	test: 0.611209

Epoch: 126
Loss: 0.34482427438100177
RMSE train: 0.483378	val: 0.744685	test: 0.840068
MAE train: 0.374781	val: 0.555770	test: 0.620514

Epoch: 127
Loss: 0.35640931129455566
RMSE train: 0.534817	val: 0.777794	test: 0.860099
MAE train: 0.413587	val: 0.583434	test: 0.637252

Epoch: 128
Loss: 0.3694211145242055
RMSE train: 0.532673	val: 0.777591	test: 0.859350
MAE train: 0.413474	val: 0.584372	test: 0.632840

Epoch: 129
Loss: 0.3491092324256897
RMSE train: 0.486784	val: 0.757337	test: 0.849750
MAE train: 0.382604	val: 0.569417	test: 0.620163

Epoch: 130
Loss: 0.35440851251284283
RMSE train: 0.501441	val: 0.759961	test: 0.836997
MAE train: 0.388221	val: 0.566933	test: 0.612457

Epoch: 131
Loss: 0.36514238516489667
RMSE train: 0.588288	val: 0.811929	test: 0.869192
MAE train: 0.439591	val: 0.605282	test: 0.633705

Epoch: 132
Loss: 0.3728229800860087
RMSE train: 0.591881	val: 0.806166	test: 0.895248
MAE train: 0.457187	val: 0.611407	test: 0.666041

Epoch: 133
Loss: 0.30574147899945575
RMSE train: 0.529444	val: 0.773989	test: 0.891442
MAE train: 0.409184	val: 0.582492	test: 0.648998

Epoch: 134
Loss: 0.3712691863377889
RMSE train: 0.508302	val: 0.756109	test: 0.862443
MAE train: 0.389415	val: 0.575075	test: 0.636758

Epoch: 135
Loss: 0.30044469237327576
RMSE train: 0.583352	val: 0.803146	test: 0.891596
MAE train: 0.444938	val: 0.595520	test: 0.663114

Epoch: 136
Loss: 0.3665550847848256
RMSE train: 0.549812	val: 0.783998	test: 0.906022
MAE train: 0.418739	val: 0.574827	test: 0.666329

Epoch: 137
Loss: 0.3457055489222209
RMSE train: 0.506338	val: 0.753452	test: 0.890058
MAE train: 0.388785	val: 0.546564	test: 0.644870

Epoch: 138
Loss: 0.3778902490933736
RMSE train: 0.520355	val: 0.757154	test: 0.865464
MAE train: 0.400413	val: 0.561104	test: 0.633032

Epoch: 139
Loss: 0.2969074447949727
RMSE train: 0.573617	val: 0.783972	test: 0.886042
MAE train: 0.446538	val: 0.587000	test: 0.648364

Epoch: 140
Loss: 0.3347354233264923
RMSE train: 0.519216	val: 0.753306	test: 0.873218
MAE train: 0.411271	val: 0.559689	test: 0.637845

Epoch: 141
Loss: 0.3184179464975993
RMSE train: 0.457693	val: 0.729064	test: 0.842797
MAE train: 0.359349	val: 0.548619	test: 0.616674

Epoch: 142
Loss: 0.39901047945022583
RMSE train: 0.537700	val: 0.773248	test: 0.852752
MAE train: 0.410275	val: 0.562047	test: 0.633762

Epoch: 143
Loss: 0.32627073923746747
RMSE train: 0.604839	val: 0.813801	test: 0.895220
MAE train: 0.456880	val: 0.603055	test: 0.667422

Epoch: 144
Loss: 0.3558339675267537
RMSE train: 0.543122	val: 0.760143	test: 0.869887

Epoch: 84
Loss: 0.5118941565354665
RMSE train: 0.563907	val: 0.764624	test: 0.846227
MAE train: 0.440038	val: 0.580670	test: 0.636070

Epoch: 85
Loss: 0.4605754216512044
RMSE train: 0.593304	val: 0.781904	test: 0.891274
MAE train: 0.464173	val: 0.592471	test: 0.668587

Epoch: 86
Loss: 0.4411916335423787
RMSE train: 0.557561	val: 0.740570	test: 0.838897
MAE train: 0.432021	val: 0.560577	test: 0.620756

Epoch: 87
Loss: 0.5862662692864736
RMSE train: 0.600167	val: 0.771560	test: 0.849289
MAE train: 0.457079	val: 0.574806	test: 0.629581

Epoch: 88
Loss: 0.4928925236066182
RMSE train: 0.653267	val: 0.817438	test: 0.912792
MAE train: 0.496681	val: 0.605994	test: 0.688982

Epoch: 89
Loss: 0.4960438907146454
RMSE train: 0.615525	val: 0.773304	test: 0.865361
MAE train: 0.466692	val: 0.563256	test: 0.649720

Epoch: 90
Loss: 0.431637038787206
RMSE train: 0.619997	val: 0.774174	test: 0.857409
MAE train: 0.468861	val: 0.554966	test: 0.642391

Epoch: 91
Loss: 0.396824171145757
RMSE train: 0.594458	val: 0.765669	test: 0.865785
MAE train: 0.453099	val: 0.547912	test: 0.649245

Epoch: 92
Loss: 0.42287150025367737
RMSE train: 0.581279	val: 0.764002	test: 0.848821
MAE train: 0.446207	val: 0.542289	test: 0.634454

Epoch: 93
Loss: 0.48454463481903076
RMSE train: 0.621295	val: 0.801872	test: 0.861731
MAE train: 0.474827	val: 0.569730	test: 0.642679

Epoch: 94
Loss: 0.44675223032633465
RMSE train: 0.620015	val: 0.805282	test: 0.879238
MAE train: 0.472981	val: 0.578836	test: 0.664185

Epoch: 95
Loss: 0.49979954957962036
RMSE train: 0.591494	val: 0.789770	test: 0.867308
MAE train: 0.455337	val: 0.577695	test: 0.653235

Epoch: 96
Loss: 0.4832117060820262
RMSE train: 0.558111	val: 0.772299	test: 0.825489
MAE train: 0.423644	val: 0.583324	test: 0.611582

Epoch: 97
Loss: 0.42753544449806213
RMSE train: 0.531842	val: 0.751689	test: 0.813980
MAE train: 0.408144	val: 0.570936	test: 0.603444

Epoch: 98
Loss: 0.4494690994421641
RMSE train: 0.548642	val: 0.756649	test: 0.833172
MAE train: 0.427127	val: 0.561115	test: 0.613575

Epoch: 99
Loss: 0.46114198366800946
RMSE train: 0.529372	val: 0.737187	test: 0.817995
MAE train: 0.408548	val: 0.549149	test: 0.599324

Epoch: 100
Loss: 0.435043603181839
RMSE train: 0.515507	val: 0.720559	test: 0.813791
MAE train: 0.402643	val: 0.549447	test: 0.602757

Epoch: 101
Loss: 0.46249522765477497
RMSE train: 0.544995	val: 0.717996	test: 0.823963
MAE train: 0.427223	val: 0.545614	test: 0.613692

Epoch: 102
Loss: 0.3691891332467397
RMSE train: 0.575220	val: 0.738536	test: 0.841305
MAE train: 0.452148	val: 0.565536	test: 0.630948

Epoch: 103
Loss: 0.3738543192545573
RMSE train: 0.563238	val: 0.743129	test: 0.830789
MAE train: 0.442372	val: 0.568469	test: 0.620641

Epoch: 104
Loss: 0.5185817778110504
RMSE train: 0.543944	val: 0.746877	test: 0.827495
MAE train: 0.423008	val: 0.566941	test: 0.608285

Epoch: 105
Loss: 0.4193930427233378
RMSE train: 0.524797	val: 0.740131	test: 0.848585
MAE train: 0.407954	val: 0.546180	test: 0.624030

Epoch: 106
Loss: 0.41966164112091064
RMSE train: 0.615277	val: 0.792983	test: 0.878402
MAE train: 0.467504	val: 0.571108	test: 0.644877

Epoch: 107
Loss: 0.4099152286847432
RMSE train: 0.632434	val: 0.805946	test: 0.885392
MAE train: 0.480381	val: 0.588757	test: 0.659862

Epoch: 108
Loss: 0.3966573178768158
RMSE train: 0.576482	val: 0.760739	test: 0.858864
MAE train: 0.442174	val: 0.561139	test: 0.640457

Epoch: 109
Loss: 0.39196057120958966
RMSE train: 0.619562	val: 0.793711	test: 0.869973
MAE train: 0.471794	val: 0.587083	test: 0.641987

Epoch: 110
Loss: 0.4031132360299428
RMSE train: 0.638152	val: 0.813766	test: 0.882503
MAE train: 0.485847	val: 0.594669	test: 0.649341

Epoch: 111
Loss: 0.43505942821502686
RMSE train: 0.574896	val: 0.761697	test: 0.857592
MAE train: 0.438823	val: 0.558579	test: 0.631949

Epoch: 112
Loss: 0.40111368894577026
RMSE train: 0.531934	val: 0.722203	test: 0.829846
MAE train: 0.410217	val: 0.537892	test: 0.608048

Epoch: 113
Loss: 0.4155401289463043
RMSE train: 0.532284	val: 0.736373	test: 0.815714
MAE train: 0.412065	val: 0.567978	test: 0.598419

Epoch: 114
Loss: 0.3538263142108917
RMSE train: 0.513159	val: 0.730093	test: 0.826950
MAE train: 0.405720	val: 0.578282	test: 0.618374

Epoch: 115
Loss: 0.3772050937016805
RMSE train: 0.488827	val: 0.711124	test: 0.841234
MAE train: 0.392008	val: 0.550816	test: 0.615521

Epoch: 116
Loss: 0.401360422372818
RMSE train: 0.483514	val: 0.705822	test: 0.832325
MAE train: 0.374036	val: 0.535066	test: 0.598078

Epoch: 117
Loss: 0.39155758420626324
RMSE train: 0.521149	val: 0.724603	test: 0.847111
MAE train: 0.402559	val: 0.542275	test: 0.614279

Epoch: 118
Loss: 0.4257456362247467
RMSE train: 0.519281	val: 0.730171	test: 0.861940
MAE train: 0.401723	val: 0.548135	test: 0.624748

Epoch: 119
Loss: 0.39094544450442
RMSE train: 0.529751	val: 0.730805	test: 0.851848
MAE train: 0.414033	val: 0.553271	test: 0.629655

Epoch: 120
Loss: 0.35925912857055664
RMSE train: 0.519512	val: 0.727919	test: 0.830016
MAE train: 0.403031	val: 0.560284	test: 0.618291

Epoch: 121
Loss: 0.34670451283454895
RMSE train: 0.496175	val: 0.710478	test: 0.843411
MAE train: 0.394868	val: 0.539122	test: 0.626723

Epoch: 122
Loss: 0.3393811285495758
RMSE train: 0.509586	val: 0.720508	test: 0.853639
MAE train: 0.399590	val: 0.536506	test: 0.626533

Epoch: 123
Loss: 0.3707934220631917
RMSE train: 0.540378	val: 0.752639	test: 0.834609
MAE train: 0.410663	val: 0.555202	test: 0.615304

Epoch: 124
Loss: 0.39126057426134747
RMSE train: 0.520974	val: 0.732188	test: 0.815071
MAE train: 0.399588	val: 0.533105	test: 0.598071

Epoch: 125
Loss: 0.336855947971344
RMSE train: 0.555864	val: 0.750724	test: 0.833193
MAE train: 0.419287	val: 0.537368	test: 0.617296

Epoch: 126
Loss: 0.3538711667060852
RMSE train: 0.561290	val: 0.754929	test: 0.837391
MAE train: 0.431664	val: 0.541537	test: 0.619012

Epoch: 127
Loss: 0.37323059638341266
RMSE train: 0.573783	val: 0.777044	test: 0.833428
MAE train: 0.444464	val: 0.558710	test: 0.611820

Epoch: 128
Loss: 0.3564930856227875
RMSE train: 0.561651	val: 0.788069	test: 0.844524
MAE train: 0.428300	val: 0.564013	test: 0.617070

Epoch: 129
Loss: 0.43899764617284137
RMSE train: 0.524236	val: 0.752660	test: 0.848670
MAE train: 0.403724	val: 0.538676	test: 0.618760

Epoch: 130
Loss: 0.3401143153508504
RMSE train: 0.523709	val: 0.737740	test: 0.841492
MAE train: 0.405666	val: 0.533236	test: 0.612418

Epoch: 131
Loss: 0.3176555236180623
RMSE train: 0.525855	val: 0.741347	test: 0.837143
MAE train: 0.403643	val: 0.537587	test: 0.613817

Epoch: 132
Loss: 0.339760422706604
RMSE train: 0.558685	val: 0.765301	test: 0.860111
MAE train: 0.424513	val: 0.561015	test: 0.636226

Epoch: 133
Loss: 0.33297552665074664
RMSE train: 0.496621	val: 0.715013	test: 0.843797
MAE train: 0.385974	val: 0.527394	test: 0.621293

Epoch: 134
Loss: 0.32966328660647076
RMSE train: 0.469216	val: 0.690301	test: 0.832762
MAE train: 0.372491	val: 0.520916	test: 0.608946

Epoch: 135
Loss: 0.32705609997113544
RMSE train: 0.497561	val: 0.707242	test: 0.828108
MAE train: 0.391510	val: 0.531075	test: 0.607309

Epoch: 136
Loss: 0.31874145070711773
RMSE train: 0.520067	val: 0.732735	test: 0.837117
MAE train: 0.400398	val: 0.537222	test: 0.619777

Epoch: 137
Loss: 0.3627853989601135
RMSE train: 0.481921	val: 0.715672	test: 0.821710
MAE train: 0.373866	val: 0.526256	test: 0.604824

Epoch: 138
Loss: 0.34540825088818866
RMSE train: 0.482044	val: 0.713139	test: 0.799429
MAE train: 0.379356	val: 0.544171	test: 0.597964

Epoch: 139
Loss: 0.3410089612007141
RMSE train: 0.480141	val: 0.715692	test: 0.805961
MAE train: 0.379635	val: 0.545865	test: 0.604488

Epoch: 140
Loss: 0.42892569303512573
RMSE train: 0.480069	val: 0.719008	test: 0.825745
MAE train: 0.380687	val: 0.539758	test: 0.615309

Epoch: 141
Loss: 0.3639555275440216
RMSE train: 0.512467	val: 0.731925	test: 0.818217
MAE train: 0.399338	val: 0.542862	test: 0.605733

Epoch: 142
Loss: 0.33463697632153827
RMSE train: 0.509695	val: 0.737774	test: 0.829992
MAE train: 0.396859	val: 0.535030	test: 0.609292

Epoch: 143
Loss: 0.33573345343271893
RMSE train: 0.578274	val: 0.798557	test: 0.873779
MAE train: 0.435961	val: 0.577915	test: 0.643465

Epoch: 144
Loss: 0.3313228289286296
RMSE train: 0.656774	val: 0.833025	test: 0.880126
MAE train: 0.499859	val: 0.600727	test: 0.653120

Epoch: 85
Loss: 0.6364514827728271
RMSE train: 0.719947	val: 0.893276	test: 0.902907
MAE train: 0.542926	val: 0.650535	test: 0.680954

Epoch: 86
Loss: 0.7811921685934067
RMSE train: 0.738509	val: 0.870449	test: 0.906715
MAE train: 0.562731	val: 0.651263	test: 0.686478

Epoch: 87
Loss: 0.6936192214488983
RMSE train: 0.671838	val: 0.780692	test: 0.861893
MAE train: 0.522529	val: 0.601274	test: 0.638758

Epoch: 88
Loss: 0.5940559804439545
RMSE train: 0.623420	val: 0.723600	test: 0.840840
MAE train: 0.484626	val: 0.568924	test: 0.616867

Epoch: 89
Loss: 0.7565746307373047
RMSE train: 0.625008	val: 0.710918	test: 0.822067
MAE train: 0.474309	val: 0.550380	test: 0.593653

Epoch: 90
Loss: 0.6634878218173981
RMSE train: 0.690801	val: 0.772658	test: 0.849158
MAE train: 0.511927	val: 0.597824	test: 0.601309

Epoch: 91
Loss: 0.6784897297620773
RMSE train: 0.738554	val: 0.828913	test: 0.896097
MAE train: 0.559970	val: 0.643483	test: 0.656804

Epoch: 92
Loss: 0.6172936633229256
RMSE train: 0.683061	val: 0.819758	test: 0.897926
MAE train: 0.526521	val: 0.620075	test: 0.676022

Epoch: 93
Loss: 0.6702268570661545
RMSE train: 0.683745	val: 0.863451	test: 0.904878
MAE train: 0.539040	val: 0.646039	test: 0.680392

Epoch: 94
Loss: 0.6477108299732208
RMSE train: 0.690670	val: 0.875184	test: 0.929372
MAE train: 0.543055	val: 0.685421	test: 0.721681

Epoch: 95
Loss: 0.747216209769249
RMSE train: 0.626337	val: 0.772321	test: 0.871656
MAE train: 0.488015	val: 0.599555	test: 0.669891

Epoch: 96
Loss: 0.6109674721956253
RMSE train: 0.655527	val: 0.764484	test: 0.852299
MAE train: 0.500449	val: 0.593672	test: 0.646281

Epoch: 97
Loss: 0.9999126642942429
RMSE train: 0.662713	val: 0.782555	test: 0.848306
MAE train: 0.495835	val: 0.592124	test: 0.638672

Epoch: 98
Loss: 0.6408739238977432
RMSE train: 0.613975	val: 0.763412	test: 0.830536
MAE train: 0.466341	val: 0.564965	test: 0.618665

Epoch: 99
Loss: 0.6990844309329987
RMSE train: 0.646600	val: 0.807652	test: 0.851887
MAE train: 0.499322	val: 0.603312	test: 0.633004

Epoch: 100
Loss: 0.7311708480119705
RMSE train: 0.729825	val: 0.867179	test: 0.915161
MAE train: 0.572612	val: 0.667077	test: 0.703582

Epoch: 101
Loss: 0.7486866265535355
RMSE train: 0.677390	val: 0.810301	test: 0.849334
MAE train: 0.528440	val: 0.631935	test: 0.653963

Epoch: 102
Loss: 0.75607830286026
RMSE train: 0.728086	val: 0.882771	test: 0.860789
MAE train: 0.557957	val: 0.672344	test: 0.647162

Epoch: 103
Loss: 0.6676401495933533
RMSE train: 0.643605	val: 0.789403	test: 0.816298
MAE train: 0.485988	val: 0.609025	test: 0.598534

Epoch: 104
Loss: 0.644277811050415
RMSE train: 0.672516	val: 0.797848	test: 0.843819
MAE train: 0.504533	val: 0.607728	test: 0.607120

Epoch: 105
Loss: 0.7855102121829987
RMSE train: 0.702563	val: 0.836074	test: 0.863670
MAE train: 0.544882	val: 0.641010	test: 0.632425

Epoch: 106
Loss: 0.8369569033384323
RMSE train: 0.709518	val: 0.886140	test: 0.864368
MAE train: 0.551048	val: 0.671955	test: 0.648820

Epoch: 107
Loss: 0.5837069451808929
RMSE train: 0.648190	val: 0.796446	test: 0.850782
MAE train: 0.512710	val: 0.628207	test: 0.665960

Epoch: 108
Loss: 0.8211584985256195
RMSE train: 0.644481	val: 0.788673	test: 0.818016
MAE train: 0.493831	val: 0.604532	test: 0.616558

Epoch: 109
Loss: 0.5585742592811584
RMSE train: 0.768966	val: 0.891319	test: 0.891274
MAE train: 0.586141	val: 0.661864	test: 0.667936

Epoch: 110
Loss: 0.7098432630300522
RMSE train: 0.797388	val: 0.923925	test: 0.940722
MAE train: 0.600214	val: 0.689796	test: 0.703544

Epoch: 111
Loss: 0.579764187335968
RMSE train: 0.666936	val: 0.808147	test: 0.870368
MAE train: 0.513357	val: 0.616530	test: 0.658896

Epoch: 112
Loss: 0.6823443621397018
RMSE train: 0.640688	val: 0.788619	test: 0.863032
MAE train: 0.495379	val: 0.600213	test: 0.640508

Epoch: 113
Loss: 0.7319465652108192
RMSE train: 0.627044	val: 0.776113	test: 0.862553
MAE train: 0.487017	val: 0.596838	test: 0.650789

Epoch: 114
Loss: 0.7379677221179008
RMSE train: 0.698747	val: 0.840114	test: 0.892233
MAE train: 0.539909	val: 0.628202	test: 0.668244

Epoch: 115
Loss: 0.5477728396654129
RMSE train: 0.753795	val: 0.901321	test: 0.934812
MAE train: 0.581011	val: 0.675188	test: 0.704465

Epoch: 116
Loss: 0.6488928496837616
RMSE train: 0.593777	val: 0.774494	test: 0.846768
MAE train: 0.454218	val: 0.583239	test: 0.629768

Epoch: 117
Loss: 0.9189275950193405
RMSE train: 0.632564	val: 0.785825	test: 0.868108
MAE train: 0.482227	val: 0.601901	test: 0.644645

Epoch: 118
Loss: 0.5734212547540665
RMSE train: 0.798769	val: 0.934662	test: 0.970763
MAE train: 0.596320	val: 0.703850	test: 0.717649

Epoch: 119
Loss: 0.7047266662120819
RMSE train: 0.767317	val: 0.907715	test: 0.948864
MAE train: 0.603396	val: 0.698337	test: 0.712876

Epoch: 120
Loss: 0.7240065634250641
RMSE train: 0.708214	val: 0.890011	test: 0.948641
MAE train: 0.561154	val: 0.670876	test: 0.716431

Epoch: 121
Loss: 0.705808162689209
RMSE train: 0.612528	val: 0.796715	test: 0.910362
MAE train: 0.477665	val: 0.596834	test: 0.688118

Epoch: 122
Loss: 0.6480471938848495
RMSE train: 0.582435	val: 0.725156	test: 0.888979
MAE train: 0.450707	val: 0.550433	test: 0.680710

Epoch: 123
Loss: 0.861208125948906
RMSE train: 0.626247	val: 0.750696	test: 0.922120
MAE train: 0.498204	val: 0.583000	test: 0.713129

Epoch: 124
Loss: 0.5837994292378426
RMSE train: 0.630597	val: 0.798886	test: 0.922962
MAE train: 0.499826	val: 0.610955	test: 0.707694

Early stopping
Best (RMSE):	 train: 0.625008	val: 0.710918	test: 0.822067
Best (MAE):	 train: 0.474309	val: 0.550380	test: 0.593653


Epoch: 84
Loss: 0.4379272237420082
RMSE train: 0.563698	val: 0.955107	test: 0.726250
MAE train: 0.441418	val: 0.673900	test: 0.565734

Epoch: 85
Loss: 0.48755844682455063
RMSE train: 0.595712	val: 0.868801	test: 0.757817
MAE train: 0.458088	val: 0.654391	test: 0.564528

Epoch: 86
Loss: 0.4768712744116783
RMSE train: 0.569540	val: 0.816405	test: 0.791072
MAE train: 0.433330	val: 0.619873	test: 0.586144

Epoch: 87
Loss: 0.4856085404753685
RMSE train: 0.555684	val: 0.920015	test: 0.753620
MAE train: 0.429177	val: 0.686884	test: 0.572746

Epoch: 88
Loss: 0.41258539259433746
RMSE train: 0.541644	val: 0.836300	test: 0.765987
MAE train: 0.421239	val: 0.639520	test: 0.586993

Epoch: 89
Loss: 0.4133949428796768
RMSE train: 0.520404	val: 0.834146	test: 0.755336
MAE train: 0.406841	val: 0.634362	test: 0.585830

Epoch: 90
Loss: 0.4087071195244789
RMSE train: 0.555694	val: 0.884116	test: 0.747042
MAE train: 0.426817	val: 0.666469	test: 0.564604

Epoch: 91
Loss: 0.38887178152799606
RMSE train: 0.539976	val: 0.805944	test: 0.764307
MAE train: 0.412101	val: 0.610206	test: 0.576608

Epoch: 92
Loss: 0.3935002014040947
RMSE train: 0.540067	val: 0.841717	test: 0.733643
MAE train: 0.417877	val: 0.640727	test: 0.567052

Epoch: 93
Loss: 0.41035784780979156
RMSE train: 0.553147	val: 0.872452	test: 0.722775
MAE train: 0.427837	val: 0.659238	test: 0.554115

Epoch: 94
Loss: 0.4258740022778511
RMSE train: 0.542745	val: 0.807005	test: 0.767635
MAE train: 0.420530	val: 0.634875	test: 0.581123

Epoch: 95
Loss: 0.4208739548921585
RMSE train: 0.571957	val: 0.879985	test: 0.771849
MAE train: 0.447717	val: 0.688253	test: 0.595549

Epoch: 96
Loss: 0.45128943026065826
RMSE train: 0.517629	val: 0.824757	test: 0.758938
MAE train: 0.403721	val: 0.629112	test: 0.576312

Epoch: 97
Loss: 0.3988497406244278
RMSE train: 0.519295	val: 0.822505	test: 0.765870
MAE train: 0.405889	val: 0.652465	test: 0.570404

Epoch: 98
Loss: 0.43789148330688477
RMSE train: 0.540873	val: 0.878885	test: 0.713174
MAE train: 0.419245	val: 0.673850	test: 0.545981

Epoch: 99
Loss: 0.47238898277282715
RMSE train: 0.563446	val: 0.875579	test: 0.724959
MAE train: 0.433609	val: 0.678630	test: 0.552369

Epoch: 100
Loss: 0.4163069874048233
RMSE train: 0.558905	val: 0.784233	test: 0.800135
MAE train: 0.430585	val: 0.611642	test: 0.595366

Epoch: 101
Loss: 0.40446098893880844
RMSE train: 0.541609	val: 0.872012	test: 0.750780
MAE train: 0.421668	val: 0.661632	test: 0.552135

Epoch: 102
Loss: 0.41659000515937805
RMSE train: 0.507618	val: 0.856703	test: 0.724575
MAE train: 0.396724	val: 0.647339	test: 0.542008

Epoch: 103
Loss: 0.40062159299850464
RMSE train: 0.557367	val: 0.821206	test: 0.760011
MAE train: 0.420255	val: 0.632041	test: 0.561045

Epoch: 104
Loss: 0.43885837495326996
RMSE train: 0.564313	val: 0.888775	test: 0.748552
MAE train: 0.430515	val: 0.686875	test: 0.570316

Epoch: 105
Loss: 0.40459322929382324
RMSE train: 0.562853	val: 0.839626	test: 0.768071
MAE train: 0.428805	val: 0.633516	test: 0.571574

Epoch: 106
Loss: 0.38509003072977066
RMSE train: 0.552927	val: 0.881821	test: 0.753952
MAE train: 0.426751	val: 0.684285	test: 0.567801

Epoch: 107
Loss: 0.38740837574005127
RMSE train: 0.502158	val: 0.862810	test: 0.726574
MAE train: 0.384623	val: 0.655947	test: 0.544198

Epoch: 108
Loss: 0.38392551243305206
RMSE train: 0.517874	val: 0.836363	test: 0.737083
MAE train: 0.396489	val: 0.633600	test: 0.552149

Epoch: 109
Loss: 0.3512490913271904
RMSE train: 0.511815	val: 0.885976	test: 0.733755
MAE train: 0.400802	val: 0.652445	test: 0.564806

Epoch: 110
Loss: 0.3928200528025627
RMSE train: 0.527433	val: 0.872124	test: 0.760504
MAE train: 0.413555	val: 0.663409	test: 0.578705

Epoch: 111
Loss: 0.4113042578101158
RMSE train: 0.518416	val: 0.818372	test: 0.796542
MAE train: 0.407308	val: 0.635919	test: 0.586790

Epoch: 112
Loss: 0.3709346279501915
RMSE train: 0.558267	val: 0.903080	test: 0.787909
MAE train: 0.431736	val: 0.699945	test: 0.581362

Epoch: 113
Loss: 0.37083324044942856
RMSE train: 0.474601	val: 0.817163	test: 0.721620
MAE train: 0.369967	val: 0.609302	test: 0.543293

Epoch: 114
Loss: 0.38676030933856964
RMSE train: 0.515736	val: 0.844827	test: 0.717787
MAE train: 0.397118	val: 0.628808	test: 0.538882

Epoch: 115
Loss: 0.3631266877055168
RMSE train: 0.533217	val: 0.843663	test: 0.741624
MAE train: 0.410503	val: 0.655017	test: 0.554251

Epoch: 116
Loss: 0.3615771755576134
RMSE train: 0.492188	val: 0.815096	test: 0.745414
MAE train: 0.373859	val: 0.618893	test: 0.551823

Epoch: 117
Loss: 0.37733228504657745
RMSE train: 0.493080	val: 0.853459	test: 0.732950
MAE train: 0.380674	val: 0.646250	test: 0.544604

Epoch: 118
Loss: 0.3481519743800163
RMSE train: 0.510689	val: 0.869546	test: 0.736004
MAE train: 0.394785	val: 0.658201	test: 0.552901

Epoch: 119
Loss: 0.36258330196142197
RMSE train: 0.540485	val: 0.893713	test: 0.747749
MAE train: 0.414939	val: 0.689410	test: 0.562845

Epoch: 120
Loss: 0.34475119411945343
RMSE train: 0.481601	val: 0.817459	test: 0.754492
MAE train: 0.369559	val: 0.617368	test: 0.568256

Epoch: 121
Loss: 0.3317943513393402
RMSE train: 0.490441	val: 0.855508	test: 0.759341
MAE train: 0.383557	val: 0.638872	test: 0.573327

Early stopping
Best (RMSE):	 train: 0.611722	val: 0.783475	test: 0.794738
Best (MAE):	 train: 0.463176	val: 0.584757	test: 0.568011

RMSE train: 0.480778	val: 0.729420	test: 0.795690
MAE train: 0.376625	val: 0.537745	test: 0.588339

Epoch: 145
Loss: 0.35670532782872516
RMSE train: 0.484224	val: 0.728470	test: 0.806839
MAE train: 0.384690	val: 0.551315	test: 0.601326

Epoch: 146
Loss: 0.3568400939305623
RMSE train: 0.557491	val: 0.772360	test: 0.862487
MAE train: 0.433997	val: 0.583555	test: 0.641216

Epoch: 147
Loss: 0.3300562898317973
RMSE train: 0.542068	val: 0.769966	test: 0.838495
MAE train: 0.422337	val: 0.582412	test: 0.621569

Epoch: 148
Loss: 0.3789827028910319
RMSE train: 0.485735	val: 0.733034	test: 0.825770
MAE train: 0.381845	val: 0.557183	test: 0.606238

Epoch: 149
Loss: 0.33694006005922955
RMSE train: 0.492260	val: 0.739592	test: 0.849467
MAE train: 0.379083	val: 0.546123	test: 0.617514

Epoch: 150
Loss: 0.3337293366591136
RMSE train: 0.560198	val: 0.777215	test: 0.839506
MAE train: 0.421273	val: 0.554526	test: 0.608602

Epoch: 151
Loss: 0.39035625259081524
RMSE train: 0.561795	val: 0.772384	test: 0.854155
MAE train: 0.433476	val: 0.567374	test: 0.626440

Epoch: 152
Loss: 0.2988495131333669
RMSE train: 0.497062	val: 0.737376	test: 0.868339
MAE train: 0.390849	val: 0.558413	test: 0.626611

Epoch: 153
Loss: 0.3602956732114156
RMSE train: 0.446564	val: 0.707561	test: 0.827330
MAE train: 0.354834	val: 0.538207	test: 0.604755

Epoch: 154
Loss: 0.3320648868878682
RMSE train: 0.510535	val: 0.747926	test: 0.833980
MAE train: 0.398568	val: 0.565104	test: 0.616059

Epoch: 155
Loss: 0.3227037986119588
RMSE train: 0.489638	val: 0.740041	test: 0.849121
MAE train: 0.387109	val: 0.556458	test: 0.614075

Epoch: 156
Loss: 0.31490635871887207
RMSE train: 0.469630	val: 0.721987	test: 0.838411
MAE train: 0.369156	val: 0.546211	test: 0.609017

Epoch: 157
Loss: 0.305403192838033
RMSE train: 0.504138	val: 0.731876	test: 0.824655
MAE train: 0.390063	val: 0.549555	test: 0.612283

Epoch: 158
Loss: 0.314528355995814
RMSE train: 0.538399	val: 0.757030	test: 0.849845
MAE train: 0.413437	val: 0.572071	test: 0.631241

Epoch: 159
Loss: 0.30118805170059204
RMSE train: 0.460220	val: 0.718798	test: 0.831199
MAE train: 0.355540	val: 0.539818	test: 0.607158

Epoch: 160
Loss: 0.25541652242342633
RMSE train: 0.446896	val: 0.712644	test: 0.812948
MAE train: 0.349603	val: 0.534788	test: 0.592035

Epoch: 161
Loss: 0.36864115794499713
RMSE train: 0.481743	val: 0.721801	test: 0.817910
MAE train: 0.373979	val: 0.535447	test: 0.602264

Epoch: 162
Loss: 0.326656699180603
RMSE train: 0.558947	val: 0.766425	test: 0.864110
MAE train: 0.429589	val: 0.577878	test: 0.642144

Epoch: 163
Loss: 0.33195897936820984
RMSE train: 0.459720	val: 0.726845	test: 0.839062
MAE train: 0.362324	val: 0.546931	test: 0.612022

Epoch: 164
Loss: 0.3879709343115489
RMSE train: 0.483353	val: 0.741375	test: 0.848373
MAE train: 0.375049	val: 0.546129	test: 0.617569

Epoch: 165
Loss: 0.29776742060979206
RMSE train: 0.497471	val: 0.747157	test: 0.821494
MAE train: 0.383398	val: 0.553879	test: 0.603752

Epoch: 166
Loss: 0.29136814177036285
RMSE train: 0.507852	val: 0.753853	test: 0.857181
MAE train: 0.390406	val: 0.553404	test: 0.624960

Epoch: 167
Loss: 0.2766345689694087
RMSE train: 0.473441	val: 0.729897	test: 0.846004
MAE train: 0.368471	val: 0.537737	test: 0.610322

Epoch: 168
Loss: 0.3360930283864339
RMSE train: 0.472873	val: 0.724601	test: 0.831832
MAE train: 0.368546	val: 0.535321	test: 0.598699

Epoch: 169
Loss: 0.35030099749565125
RMSE train: 0.538013	val: 0.765884	test: 0.859543
MAE train: 0.408953	val: 0.568677	test: 0.629409

Early stopping
Best (RMSE):	 train: 0.469216	val: 0.690301	test: 0.832762
Best (MAE):	 train: 0.372491	val: 0.520916	test: 0.608946

RMSE train: 0.738387	val: 0.844562	test: 0.883347
MAE train: 0.557070	val: 0.618855	test: 0.661729

Epoch: 85
Loss: 0.6690346896648407
RMSE train: 0.675830	val: 0.739875	test: 0.803306
MAE train: 0.513415	val: 0.559922	test: 0.613953

Epoch: 86
Loss: 0.7801722437143326
RMSE train: 0.683919	val: 0.753383	test: 0.810072
MAE train: 0.529187	val: 0.599824	test: 0.621442

Epoch: 87
Loss: 0.8883424997329712
RMSE train: 0.652022	val: 0.739421	test: 0.791109
MAE train: 0.493340	val: 0.591441	test: 0.614580

Epoch: 88
Loss: 0.6475980281829834
RMSE train: 0.659397	val: 0.768195	test: 0.798498
MAE train: 0.509672	val: 0.607572	test: 0.632520

Epoch: 89
Loss: 0.7426062226295471
RMSE train: 0.608226	val: 0.745992	test: 0.770844
MAE train: 0.466091	val: 0.593048	test: 0.602947

Epoch: 90
Loss: 0.7036574929952621
RMSE train: 0.597384	val: 0.739340	test: 0.752463
MAE train: 0.458396	val: 0.575752	test: 0.564784

Epoch: 91
Loss: 0.7397379130125046
RMSE train: 0.599368	val: 0.714072	test: 0.772265
MAE train: 0.464787	val: 0.578664	test: 0.592370

Epoch: 92
Loss: 0.7653965801000595
RMSE train: 0.634399	val: 0.738458	test: 0.793756
MAE train: 0.475577	val: 0.574010	test: 0.596158

Epoch: 93
Loss: 0.7445435971021652
RMSE train: 0.749866	val: 0.870379	test: 0.855967
MAE train: 0.551205	val: 0.638024	test: 0.630990

Epoch: 94
Loss: 0.6274852454662323
RMSE train: 0.704352	val: 0.825564	test: 0.813375
MAE train: 0.528582	val: 0.618952	test: 0.600304

Epoch: 95
Loss: 0.6902608573436737
RMSE train: 0.668352	val: 0.815124	test: 0.792692
MAE train: 0.502885	val: 0.607014	test: 0.583147

Epoch: 96
Loss: 0.6681422591209412
RMSE train: 0.657246	val: 0.810618	test: 0.784512
MAE train: 0.492674	val: 0.599145	test: 0.568474

Epoch: 97
Loss: 0.7407897561788559
RMSE train: 0.646571	val: 0.795102	test: 0.761299
MAE train: 0.486081	val: 0.592124	test: 0.541976

Epoch: 98
Loss: 0.5449229925870895
RMSE train: 0.593472	val: 0.743709	test: 0.742995
MAE train: 0.453165	val: 0.543476	test: 0.540022

Epoch: 99
Loss: 0.6293907910585403
RMSE train: 0.604150	val: 0.758023	test: 0.763247
MAE train: 0.457247	val: 0.549044	test: 0.551906

Epoch: 100
Loss: 0.6499626934528351
RMSE train: 0.601175	val: 0.705589	test: 0.769748
MAE train: 0.456317	val: 0.534675	test: 0.556512

Epoch: 101
Loss: 0.5349085852503777
RMSE train: 0.598318	val: 0.686559	test: 0.757093
MAE train: 0.457762	val: 0.537921	test: 0.557209

Epoch: 102
Loss: 0.6313172280788422
RMSE train: 0.621202	val: 0.722745	test: 0.769974
MAE train: 0.474768	val: 0.546829	test: 0.570200

Epoch: 103
Loss: 0.5488895550370216
RMSE train: 0.678843	val: 0.805729	test: 0.786445
MAE train: 0.513526	val: 0.599974	test: 0.589713

Epoch: 104
Loss: 0.5641378313302994
RMSE train: 0.627286	val: 0.730459	test: 0.730697
MAE train: 0.473476	val: 0.560455	test: 0.539975

Epoch: 105
Loss: 0.6237572133541107
RMSE train: 0.641087	val: 0.751354	test: 0.745557
MAE train: 0.475845	val: 0.575131	test: 0.548495

Epoch: 106
Loss: 0.668114572763443
RMSE train: 0.750044	val: 0.880493	test: 0.863866
MAE train: 0.559516	val: 0.652839	test: 0.630387

Epoch: 107
Loss: 0.7934412509202957
RMSE train: 0.629425	val: 0.739815	test: 0.779885
MAE train: 0.473205	val: 0.569781	test: 0.573896

Epoch: 108
Loss: 0.6564643532037735
RMSE train: 0.690307	val: 0.778193	test: 0.864530
MAE train: 0.525614	val: 0.608087	test: 0.662477

Epoch: 109
Loss: 0.7498204410076141
RMSE train: 0.600937	val: 0.721882	test: 0.774767
MAE train: 0.453080	val: 0.565655	test: 0.582743

Epoch: 110
Loss: 0.5552969574928284
RMSE train: 0.704132	val: 0.834618	test: 0.863209
MAE train: 0.531645	val: 0.625119	test: 0.648386

Epoch: 111
Loss: 0.6707184389233589
RMSE train: 0.631773	val: 0.761249	test: 0.813839
MAE train: 0.474283	val: 0.572463	test: 0.607106

Epoch: 112
Loss: 0.6070648729801178
RMSE train: 0.597149	val: 0.723519	test: 0.804295
MAE train: 0.463041	val: 0.564058	test: 0.606186

Epoch: 113
Loss: 0.6497758477926254
RMSE train: 0.598348	val: 0.740666	test: 0.780827
MAE train: 0.459655	val: 0.562280	test: 0.583502

Epoch: 114
Loss: 0.6396965980529785
RMSE train: 0.688270	val: 0.835467	test: 0.810371
MAE train: 0.518072	val: 0.617382	test: 0.607480

Epoch: 115
Loss: 0.8628181964159012
RMSE train: 0.672837	val: 0.787330	test: 0.789002
MAE train: 0.509415	val: 0.593565	test: 0.588519

Epoch: 116
Loss: 0.5319918170571327
RMSE train: 0.600284	val: 0.712748	test: 0.755542
MAE train: 0.456325	val: 0.535045	test: 0.555095

Epoch: 117
Loss: 0.6156561970710754
RMSE train: 0.608469	val: 0.730415	test: 0.790089
MAE train: 0.456913	val: 0.546491	test: 0.594224

Epoch: 118
Loss: 0.5873623043298721
RMSE train: 0.646692	val: 0.788695	test: 0.839990
MAE train: 0.492877	val: 0.589382	test: 0.620148

Epoch: 119
Loss: 0.7309460341930389
RMSE train: 0.667663	val: 0.818576	test: 0.858512
MAE train: 0.507893	val: 0.619078	test: 0.627172

Epoch: 120
Loss: 0.7090975046157837
RMSE train: 0.684022	val: 0.821182	test: 0.827444
MAE train: 0.510475	val: 0.615527	test: 0.592621

Epoch: 121
Loss: 0.5735470205545425
RMSE train: 0.698505	val: 0.808523	test: 0.795785
MAE train: 0.524797	val: 0.612297	test: 0.570876

Epoch: 122
Loss: 0.6406302452087402
RMSE train: 0.692877	val: 0.808535	test: 0.784805
MAE train: 0.516560	val: 0.607312	test: 0.574277

Epoch: 123
Loss: 0.7143734693527222
RMSE train: 0.773601	val: 0.885627	test: 0.856340
MAE train: 0.579103	val: 0.655473	test: 0.636039

Epoch: 124
Loss: 0.5412955805659294
RMSE train: 0.686752	val: 0.812807	test: 0.797847
MAE train: 0.511826	val: 0.607809	test: 0.593429

Epoch: 125
Loss: 0.5576025694608688
RMSE train: 0.652276	val: 0.766188	test: 0.798852
MAE train: 0.490228	val: 0.599854	test: 0.587330

Epoch: 126
Loss: 0.8366253525018692
RMSE train: 0.650133	val: 0.759618	test: 0.805834
MAE train: 0.491878	val: 0.605454	test: 0.600719

Epoch: 127
Loss: 0.7231232523918152
RMSE train: 0.595033	val: 0.733085	test: 0.762701
MAE train: 0.452567	val: 0.563963	test: 0.569594

Epoch: 128
Loss: 0.5878899246454239
RMSE train: 0.785166	val: 0.920556	test: 0.904298
MAE train: 0.600264	val: 0.690986	test: 0.664796

Epoch: 129
Loss: 1.0904728546738625
RMSE train: 0.601717	val: 0.704018	test: 0.794352
MAE train: 0.471564	val: 0.560113	test: 0.602460

Epoch: 130
Loss: 0.7528943121433258
RMSE train: 1.072362	val: 1.057379	test: 1.203913
MAE train: 0.840561	val: 0.827035	test: 0.930544

Epoch: 131
Loss: 1.0371835976839066
RMSE train: 0.857876	val: 0.917492	test: 1.022491
MAE train: 0.695825	val: 0.736318	test: 0.797671

Epoch: 132
Loss: 0.5998373255133629
RMSE train: 0.746496	val: 0.830231	test: 0.940509
MAE train: 0.596457	val: 0.662145	test: 0.736140

Epoch: 133
Loss: 0.9266711622476578
RMSE train: 0.722410	val: 0.783087	test: 0.942433
MAE train: 0.561433	val: 0.607510	test: 0.727158

Epoch: 134
Loss: 0.575008749961853
RMSE train: 0.722014	val: 0.772448	test: 0.908893
MAE train: 0.548664	val: 0.591592	test: 0.679510

Epoch: 135
Loss: 0.6901407986879349
RMSE train: 0.762213	val: 0.824183	test: 0.892076
MAE train: 0.584236	val: 0.647830	test: 0.660761

Epoch: 136
Loss: 0.6429210603237152
RMSE train: 0.799671	val: 0.890590	test: 0.901841
MAE train: 0.579188	val: 0.651969	test: 0.645635

Early stopping
Best (RMSE):	 train: 0.598318	val: 0.686559	test: 0.757093
Best (MAE):	 train: 0.457762	val: 0.537921	test: 0.557209

RMSE train: 0.741425	val: 0.762432	test: 0.942733
MAE train: 0.571377	val: 0.597132	test: 0.675735

Epoch: 85
Loss: 0.7475915998220444
RMSE train: 0.775813	val: 0.851834	test: 0.943850
MAE train: 0.602240	val: 0.676886	test: 0.707769

Epoch: 86
Loss: 0.7377763539552689
RMSE train: 0.812292	val: 0.931739	test: 0.973457
MAE train: 0.622789	val: 0.704584	test: 0.733246

Epoch: 87
Loss: 0.7453020587563515
RMSE train: 0.824459	val: 0.967724	test: 0.976682
MAE train: 0.610906	val: 0.703392	test: 0.720564

Epoch: 88
Loss: 0.9093675911426544
RMSE train: 0.672979	val: 0.775952	test: 0.858854
MAE train: 0.514344	val: 0.587969	test: 0.635406

Epoch: 89
Loss: 0.7796925157308578
RMSE train: 0.658728	val: 0.753635	test: 0.903607
MAE train: 0.513141	val: 0.583912	test: 0.690810

Epoch: 90
Loss: 0.7704762667417526
RMSE train: 0.650658	val: 0.783225	test: 0.876526
MAE train: 0.498306	val: 0.582743	test: 0.646621

Epoch: 91
Loss: 0.6286636292934418
RMSE train: 0.650941	val: 0.748936	test: 0.870598
MAE train: 0.501402	val: 0.581046	test: 0.646385

Epoch: 92
Loss: 0.6514981538057327
RMSE train: 0.639858	val: 0.724361	test: 0.871243
MAE train: 0.493753	val: 0.577017	test: 0.649730

Epoch: 93
Loss: 0.7853459417819977
RMSE train: 0.614865	val: 0.710044	test: 0.838332
MAE train: 0.472835	val: 0.560501	test: 0.628520

Epoch: 94
Loss: 0.7300636023283005
RMSE train: 0.639245	val: 0.728405	test: 0.830821
MAE train: 0.485074	val: 0.567265	test: 0.618167

Epoch: 95
Loss: 0.8661892265081406
RMSE train: 0.668531	val: 0.728613	test: 0.814938
MAE train: 0.517297	val: 0.549921	test: 0.614968

Epoch: 96
Loss: 0.7085976451635361
RMSE train: 0.681202	val: 0.741034	test: 0.843232
MAE train: 0.528326	val: 0.574773	test: 0.625921

Epoch: 97
Loss: 1.1183735877275467
RMSE train: 0.689627	val: 0.744250	test: 0.898764
MAE train: 0.516108	val: 0.574136	test: 0.662565

Epoch: 98
Loss: 0.612994909286499
RMSE train: 0.760805	val: 0.814283	test: 0.961302
MAE train: 0.587735	val: 0.643667	test: 0.722964

Epoch: 99
Loss: 0.7912216037511826
RMSE train: 0.683813	val: 0.795313	test: 0.888720
MAE train: 0.534216	val: 0.597990	test: 0.659525

Epoch: 100
Loss: 0.7721770107746124
RMSE train: 0.713962	val: 0.874771	test: 0.890194
MAE train: 0.549409	val: 0.616500	test: 0.649646

Epoch: 101
Loss: 0.7323186993598938
RMSE train: 0.705120	val: 0.840431	test: 0.867763
MAE train: 0.550539	val: 0.638678	test: 0.637288

Epoch: 102
Loss: 0.6804608702659607
RMSE train: 0.666124	val: 0.774717	test: 0.862694
MAE train: 0.517837	val: 0.605655	test: 0.640062

Epoch: 103
Loss: 0.6089642196893692
RMSE train: 0.624718	val: 0.756722	test: 0.855055
MAE train: 0.483037	val: 0.581958	test: 0.629691

Epoch: 104
Loss: 0.6194841712713242
RMSE train: 0.683886	val: 0.815804	test: 0.882962
MAE train: 0.521003	val: 0.630528	test: 0.655763

Epoch: 105
Loss: 0.6281862109899521
RMSE train: 0.668861	val: 0.796260	test: 0.875331
MAE train: 0.510063	val: 0.615409	test: 0.646022

Epoch: 106
Loss: 0.5492590367794037
RMSE train: 0.625413	val: 0.787991	test: 0.863303
MAE train: 0.477729	val: 0.572731	test: 0.640333

Epoch: 107
Loss: 0.6910500228404999
RMSE train: 0.579898	val: 0.723275	test: 0.868964
MAE train: 0.438688	val: 0.535522	test: 0.645117

Epoch: 108
Loss: 0.5835352167487144
RMSE train: 0.626263	val: 0.720528	test: 0.878655
MAE train: 0.469233	val: 0.558192	test: 0.651541

Epoch: 109
Loss: 0.5380998328328133
RMSE train: 0.724394	val: 0.833110	test: 0.912858
MAE train: 0.557580	val: 0.641804	test: 0.680461

Epoch: 110
Loss: 0.7251792252063751
RMSE train: 0.717458	val: 0.852585	test: 0.903507
MAE train: 0.553283	val: 0.644657	test: 0.681048

Epoch: 111
Loss: 0.5859785676002502
RMSE train: 0.624335	val: 0.746341	test: 0.849462
MAE train: 0.487941	val: 0.582439	test: 0.640728

Epoch: 112
Loss: 0.7660356611013412
RMSE train: 0.603920	val: 0.708200	test: 0.845965
MAE train: 0.459532	val: 0.559727	test: 0.637274

Epoch: 113
Loss: 0.6560229659080505
RMSE train: 0.694701	val: 0.778677	test: 0.914394
MAE train: 0.532790	val: 0.617552	test: 0.686236

Epoch: 114
Loss: 0.7937848269939423
RMSE train: 0.648727	val: 0.745719	test: 0.896400
MAE train: 0.506612	val: 0.581309	test: 0.683662

Epoch: 115
Loss: 0.532106988132
RMSE train: 0.702300	val: 0.809357	test: 0.902181
MAE train: 0.543763	val: 0.616033	test: 0.700792

Epoch: 116
Loss: 0.7179179191589355
RMSE train: 0.640901	val: 0.765854	test: 0.845336
MAE train: 0.486513	val: 0.566343	test: 0.651313

Epoch: 117
Loss: 0.5667333602905273
RMSE train: 0.624024	val: 0.725286	test: 0.830185
MAE train: 0.491603	val: 0.567983	test: 0.626165

Epoch: 118
Loss: 0.6870453208684921
RMSE train: 0.652647	val: 0.750786	test: 0.872396
MAE train: 0.522893	val: 0.598710	test: 0.671143

Epoch: 119
Loss: 0.7840434908866882
RMSE train: 0.650786	val: 0.765997	test: 0.834791
MAE train: 0.508863	val: 0.578548	test: 0.625453

Epoch: 120
Loss: 0.6716330125927925
RMSE train: 0.679841	val: 0.787010	test: 0.854169
MAE train: 0.526300	val: 0.605437	test: 0.631238

Epoch: 121
Loss: 0.674988254904747
RMSE train: 0.677374	val: 0.754530	test: 0.897351
MAE train: 0.521394	val: 0.591401	test: 0.662751

Epoch: 122
Loss: 0.637144461274147
RMSE train: 0.578176	val: 0.673774	test: 0.819665
MAE train: 0.443020	val: 0.521600	test: 0.615046

Epoch: 123
Loss: 0.6200294494628906
RMSE train: 0.570891	val: 0.686658	test: 0.808754
MAE train: 0.439523	val: 0.528630	test: 0.601591

Epoch: 124
Loss: 0.510320670902729
RMSE train: 0.640853	val: 0.757116	test: 0.825844
MAE train: 0.482282	val: 0.575986	test: 0.617530

Epoch: 125
Loss: 0.5295626744627953
RMSE train: 0.715962	val: 0.828345	test: 0.868876
MAE train: 0.546931	val: 0.636820	test: 0.654139

Epoch: 126
Loss: 0.6675918698310852
RMSE train: 0.632492	val: 0.744481	test: 0.820308
MAE train: 0.474400	val: 0.566082	test: 0.609067

Epoch: 127
Loss: 0.5616975203156471
RMSE train: 0.595511	val: 0.705624	test: 0.799701
MAE train: 0.442286	val: 0.529757	test: 0.593465

Epoch: 128
Loss: 0.5173952728509903
RMSE train: 0.652109	val: 0.756338	test: 0.828329
MAE train: 0.475517	val: 0.562669	test: 0.605947

Epoch: 129
Loss: 0.5032422915101051
RMSE train: 0.659591	val: 0.766028	test: 0.828467
MAE train: 0.485366	val: 0.575800	test: 0.602665

Epoch: 130
Loss: 0.5283750295639038
RMSE train: 0.614614	val: 0.713237	test: 0.813447
MAE train: 0.469532	val: 0.556407	test: 0.589962

Epoch: 131
Loss: 0.6026490852236748
RMSE train: 0.599046	val: 0.720745	test: 0.810979
MAE train: 0.462947	val: 0.549454	test: 0.600029

Epoch: 132
Loss: 0.5102507248520851
RMSE train: 0.638467	val: 0.779609	test: 0.868218
MAE train: 0.496749	val: 0.582004	test: 0.654413

Epoch: 133
Loss: 0.5988316535949707
RMSE train: 0.579674	val: 0.723783	test: 0.861937
MAE train: 0.451863	val: 0.545228	test: 0.640386

Epoch: 134
Loss: 0.6615770012140274
RMSE train: 0.581538	val: 0.719512	test: 0.848085
MAE train: 0.457782	val: 0.556344	test: 0.640615

Epoch: 135
Loss: 0.5504590272903442
RMSE train: 0.589362	val: 0.733586	test: 0.826193
MAE train: 0.467022	val: 0.572456	test: 0.635670

Epoch: 136
Loss: 0.563803143799305
RMSE train: 0.640069	val: 0.828468	test: 0.857093
MAE train: 0.509259	val: 0.628786	test: 0.662610

Epoch: 137
Loss: 0.585867278277874
RMSE train: 0.644259	val: 0.841297	test: 0.863246
MAE train: 0.512892	val: 0.627521	test: 0.662048

Epoch: 138
Loss: 0.505190834403038
RMSE train: 0.604348	val: 0.761120	test: 0.850561
MAE train: 0.476156	val: 0.573192	test: 0.643446

Epoch: 139
Loss: 1.0353409796953201
RMSE train: 0.640749	val: 0.779690	test: 0.894870
MAE train: 0.500745	val: 0.594390	test: 0.681894

Epoch: 140
Loss: 0.623814694583416
RMSE train: 0.607327	val: 0.746869	test: 0.834878
MAE train: 0.465461	val: 0.565321	test: 0.630664

Epoch: 141
Loss: 0.5642214864492416
RMSE train: 0.614326	val: 0.778204	test: 0.812680
MAE train: 0.474337	val: 0.575305	test: 0.604216

Epoch: 142
Loss: 0.5127411559224129
RMSE train: 0.620246	val: 0.836989	test: 0.807106
MAE train: 0.469259	val: 0.597258	test: 0.596216

Epoch: 143
Loss: 0.6652088612318039
RMSE train: 0.583995	val: 0.767861	test: 0.784058
MAE train: 0.435678	val: 0.574427	test: 0.585998

Epoch: 144
Loss: 0.6508177891373634
RMSE train: 0.667830	val: 0.812696	test: 0.852281
MAE train: 0.508260	val: 0.620355	test: 0.636747
Epoch: 84
Loss: 0.42618846148252487
RMSE train: 0.573865	val: 0.931984	test: 0.701750
MAE train: 0.432653	val: 0.701418	test: 0.519426

Epoch: 85
Loss: 0.4083210155367851
RMSE train: 0.564085	val: 0.858657	test: 0.704548
MAE train: 0.419748	val: 0.663337	test: 0.519325

Epoch: 86
Loss: 0.46903209388256073
RMSE train: 0.525648	val: 0.837810	test: 0.723810
MAE train: 0.414198	val: 0.634592	test: 0.551212

Epoch: 87
Loss: 0.4235370382666588
RMSE train: 0.544015	val: 0.861668	test: 0.758621
MAE train: 0.441850	val: 0.656468	test: 0.590112

Epoch: 88
Loss: 0.4374672397971153
RMSE train: 0.600876	val: 0.871582	test: 0.771310
MAE train: 0.477542	val: 0.678984	test: 0.577518

Epoch: 89
Loss: 0.4252839833498001
RMSE train: 0.568747	val: 0.819008	test: 0.767844
MAE train: 0.444329	val: 0.636734	test: 0.576755

Epoch: 90
Loss: 0.43468213826417923
RMSE train: 0.533990	val: 0.920265	test: 0.705321
MAE train: 0.408746	val: 0.674334	test: 0.537978

Epoch: 91
Loss: 0.4859714210033417
RMSE train: 0.600832	val: 0.859030	test: 0.751261
MAE train: 0.454562	val: 0.677972	test: 0.550917

Epoch: 92
Loss: 0.41398436576128006
RMSE train: 0.557318	val: 0.848449	test: 0.725812
MAE train: 0.423533	val: 0.656588	test: 0.529218

Epoch: 93
Loss: 0.39048417657613754
RMSE train: 0.582045	val: 0.856546	test: 0.742571
MAE train: 0.438366	val: 0.672159	test: 0.547050

Epoch: 94
Loss: 0.40689685195684433
RMSE train: 0.558287	val: 0.840213	test: 0.706236
MAE train: 0.423892	val: 0.651585	test: 0.524090

Epoch: 95
Loss: 0.3898455947637558
RMSE train: 0.584107	val: 0.910097	test: 0.706623
MAE train: 0.444215	val: 0.682771	test: 0.525067

Epoch: 96
Loss: 0.42195461690425873
RMSE train: 0.529669	val: 0.801823	test: 0.721129
MAE train: 0.396932	val: 0.613862	test: 0.530392

Epoch: 97
Loss: 0.3737375810742378
RMSE train: 0.550814	val: 0.819844	test: 0.729754
MAE train: 0.412952	val: 0.631442	test: 0.535985

Epoch: 98
Loss: 0.3962852954864502
RMSE train: 0.591196	val: 0.866848	test: 0.731291
MAE train: 0.442804	val: 0.672165	test: 0.541095

Epoch: 99
Loss: 0.40723633766174316
RMSE train: 0.555740	val: 0.835714	test: 0.717452
MAE train: 0.423870	val: 0.654974	test: 0.529609

Epoch: 100
Loss: 0.3781348690390587
RMSE train: 0.539518	val: 0.819580	test: 0.750234
MAE train: 0.424360	val: 0.639547	test: 0.566662

Epoch: 101
Loss: 0.3891471326351166
RMSE train: 0.534704	val: 0.869069	test: 0.750893
MAE train: 0.420418	val: 0.640621	test: 0.554540

Epoch: 102
Loss: 0.40468598157167435
RMSE train: 0.542135	val: 0.796895	test: 0.784819
MAE train: 0.411174	val: 0.605595	test: 0.584369

Epoch: 103
Loss: 0.4247128739953041
RMSE train: 0.491941	val: 0.844146	test: 0.716082
MAE train: 0.376698	val: 0.620243	test: 0.536722

Epoch: 104
Loss: 0.4049132242798805
RMSE train: 0.542412	val: 0.894258	test: 0.716115
MAE train: 0.416911	val: 0.669374	test: 0.528233

Epoch: 105
Loss: 0.3721092492341995
RMSE train: 0.537808	val: 0.821496	test: 0.721905
MAE train: 0.414146	val: 0.628321	test: 0.549491

Epoch: 106
Loss: 0.4160946011543274
RMSE train: 0.573088	val: 0.881942	test: 0.711415
MAE train: 0.440310	val: 0.667071	test: 0.536640

Epoch: 107
Loss: 0.3676699325442314
RMSE train: 0.560506	val: 0.817503	test: 0.735908
MAE train: 0.424126	val: 0.641622	test: 0.548223

Epoch: 108
Loss: 0.34248460084199905
RMSE train: 0.529952	val: 0.781980	test: 0.751086
MAE train: 0.406153	val: 0.612759	test: 0.557514

Epoch: 109
Loss: 0.34486210346221924
RMSE train: 0.565050	val: 0.893105	test: 0.759273
MAE train: 0.440874	val: 0.672907	test: 0.567283

Epoch: 110
Loss: 0.36916303634643555
RMSE train: 0.505970	val: 0.825743	test: 0.751429
MAE train: 0.394291	val: 0.635513	test: 0.566957

Epoch: 111
Loss: 0.34595654904842377
RMSE train: 0.486773	val: 0.838105	test: 0.745777
MAE train: 0.379581	val: 0.640985	test: 0.569826

Epoch: 112
Loss: 0.36030565202236176
RMSE train: 0.562179	val: 0.933634	test: 0.751463
MAE train: 0.434914	val: 0.720080	test: 0.559174

Epoch: 113
Loss: 0.3687342405319214
RMSE train: 0.515481	val: 0.877084	test: 0.733319
MAE train: 0.396575	val: 0.668101	test: 0.550514

Epoch: 114
Loss: 0.35543159395456314
RMSE train: 0.504901	val: 0.874866	test: 0.735604
MAE train: 0.392258	val: 0.648026	test: 0.550810

Epoch: 115
Loss: 0.3777904212474823
RMSE train: 0.531212	val: 0.866103	test: 0.745188
MAE train: 0.413800	val: 0.651716	test: 0.550758

Epoch: 116
Loss: 0.3687860891222954
RMSE train: 0.546917	val: 0.886488	test: 0.736561
MAE train: 0.419112	val: 0.670581	test: 0.536275

Epoch: 117
Loss: 0.32568423449993134
RMSE train: 0.476338	val: 0.817157	test: 0.727029
MAE train: 0.367557	val: 0.613516	test: 0.549844

Epoch: 118
Loss: 0.3473401442170143
RMSE train: 0.576280	val: 0.965298	test: 0.751140
MAE train: 0.450085	val: 0.714627	test: 0.560404

Epoch: 119
Loss: 0.37798865884542465
RMSE train: 0.519798	val: 0.871758	test: 0.740138
MAE train: 0.410113	val: 0.659582	test: 0.557384

Epoch: 120
Loss: 0.35295668989419937
RMSE train: 0.511015	val: 0.914155	test: 0.747746
MAE train: 0.410851	val: 0.682083	test: 0.576479

Epoch: 121
Loss: 0.3636256754398346
RMSE train: 0.525006	val: 0.944690	test: 0.756682
MAE train: 0.418841	val: 0.687791	test: 0.570018

Epoch: 122
Loss: 0.33963175117969513
RMSE train: 0.498096	val: 0.885769	test: 0.740879
MAE train: 0.389766	val: 0.656260	test: 0.551677

Epoch: 123
Loss: 0.3295825570821762
RMSE train: 0.488334	val: 0.883322	test: 0.711774
MAE train: 0.382771	val: 0.660564	test: 0.542309

Epoch: 124
Loss: 0.34851862490177155
RMSE train: 0.496266	val: 0.877473	test: 0.742873
MAE train: 0.392097	val: 0.667085	test: 0.561021

Epoch: 125
Loss: 0.33501099795103073
RMSE train: 0.520377	val: 0.900055	test: 0.780525
MAE train: 0.411093	val: 0.687249	test: 0.589397

Epoch: 126
Loss: 0.34862375259399414
RMSE train: 0.474350	val: 0.870050	test: 0.772705
MAE train: 0.370428	val: 0.649601	test: 0.581025

Epoch: 127
Loss: 0.3565705120563507
RMSE train: 0.478203	val: 0.851302	test: 0.748401
MAE train: 0.377324	val: 0.643606	test: 0.561972

Epoch: 128
Loss: 0.3108898624777794
RMSE train: 0.538865	val: 0.921664	test: 0.738276
MAE train: 0.428510	val: 0.702913	test: 0.550328

Epoch: 129
Loss: 0.2907995320856571
RMSE train: 0.510870	val: 0.815803	test: 0.742683
MAE train: 0.399544	val: 0.634052	test: 0.564810

Epoch: 130
Loss: 0.35588958114385605
RMSE train: 0.516224	val: 0.860347	test: 0.742640
MAE train: 0.405939	val: 0.660530	test: 0.560055

Epoch: 131
Loss: 0.3481459394097328
RMSE train: 0.520795	val: 0.824977	test: 0.722357
MAE train: 0.399875	val: 0.630378	test: 0.541165

Epoch: 132
Loss: 0.35510412603616714
RMSE train: 0.544517	val: 0.820606	test: 0.747305
MAE train: 0.417374	val: 0.638424	test: 0.565022

Epoch: 133
Loss: 0.3619992509484291
RMSE train: 0.539009	val: 0.916292	test: 0.717010
MAE train: 0.412729	val: 0.666363	test: 0.541503

Epoch: 134
Loss: 0.3443601280450821
RMSE train: 0.539482	val: 0.842052	test: 0.758592
MAE train: 0.405222	val: 0.633016	test: 0.552328

Epoch: 135
Loss: 0.3574291244149208
RMSE train: 0.558264	val: 0.957153	test: 0.725420
MAE train: 0.429733	val: 0.712590	test: 0.554245

Epoch: 136
Loss: 0.3435278981924057
RMSE train: 0.494507	val: 0.879949	test: 0.687201
MAE train: 0.381883	val: 0.658507	test: 0.521732

Epoch: 137
Loss: 0.3238539546728134
RMSE train: 0.500898	val: 0.824706	test: 0.714339
MAE train: 0.379676	val: 0.621017	test: 0.523642

Epoch: 138
Loss: 0.3299901969730854
RMSE train: 0.521247	val: 0.842506	test: 0.761789
MAE train: 0.396402	val: 0.636220	test: 0.539177

Epoch: 139
Loss: 0.3250029906630516
RMSE train: 0.494432	val: 0.854552	test: 0.748754
MAE train: 0.379598	val: 0.644672	test: 0.538246

Epoch: 140
Loss: 0.32577069103717804
RMSE train: 0.483615	val: 0.850718	test: 0.707292
MAE train: 0.381469	val: 0.651380	test: 0.523035

Epoch: 141
Loss: 0.33307863026857376
RMSE train: 0.503818	val: 0.931437	test: 0.708341
MAE train: 0.393135	val: 0.694103	test: 0.522658

Epoch: 142
Loss: 0.3024505600333214
RMSE train: 0.478483	val: 0.827499	test: 0.718012
MAE train: 0.368109	val: 0.639967	test: 0.521047

Epoch: 143
Loss: 0.34663204103708267
RMSE train: 0.529219	val: 0.809590	test: 0.745624
MAE train: 0.401774	val: 0.640776	test: 0.539998

Early stopping
Best (RMSE):	 train: 0.529952	val: 0.781980	test: 0.751086
Best (MAE):	 train: 0.406153	val: 0.612759	test: 0.557514


Epoch: 145
Loss: 0.47215617448091507
RMSE train: 0.611898	val: 0.795048	test: 0.831888
MAE train: 0.467859	val: 0.595298	test: 0.632117

Epoch: 146
Loss: 0.4987126365303993
RMSE train: 0.558913	val: 0.756777	test: 0.818667
MAE train: 0.435789	val: 0.565650	test: 0.614730

Epoch: 147
Loss: 0.7207968682050705
RMSE train: 0.600958	val: 0.746836	test: 0.876471
MAE train: 0.474572	val: 0.589148	test: 0.668581

Epoch: 148
Loss: 0.4835323393344879
RMSE train: 0.553464	val: 0.716748	test: 0.817821
MAE train: 0.429148	val: 0.544408	test: 0.610756

Epoch: 149
Loss: 0.5148514360189438
RMSE train: 0.644721	val: 0.816856	test: 0.849179
MAE train: 0.500383	val: 0.607959	test: 0.640690

Epoch: 150
Loss: 0.7735411003232002
RMSE train: 0.602626	val: 0.775946	test: 0.832738
MAE train: 0.468103	val: 0.579067	test: 0.622501

Epoch: 151
Loss: 0.4937250688672066
RMSE train: 0.560279	val: 0.718192	test: 0.816296
MAE train: 0.426258	val: 0.545361	test: 0.611370

Epoch: 152
Loss: 0.426461361348629
RMSE train: 0.569456	val: 0.744479	test: 0.805747
MAE train: 0.430367	val: 0.558550	test: 0.603148

Epoch: 153
Loss: 0.5565934777259827
RMSE train: 0.590282	val: 0.762211	test: 0.819549
MAE train: 0.446606	val: 0.576006	test: 0.605297

Epoch: 154
Loss: 0.5493065714836121
RMSE train: 0.582668	val: 0.733289	test: 0.828431
MAE train: 0.443353	val: 0.567396	test: 0.602279

Epoch: 155
Loss: 0.5645433142781258
RMSE train: 0.576539	val: 0.736597	test: 0.817335
MAE train: 0.442880	val: 0.568016	test: 0.594662

Epoch: 156
Loss: 0.5239214152097702
RMSE train: 0.659910	val: 0.818303	test: 0.845392
MAE train: 0.496914	val: 0.609233	test: 0.611717

Epoch: 157
Loss: 0.6227453872561455
RMSE train: 0.609572	val: 0.740533	test: 0.808665
MAE train: 0.454575	val: 0.553250	test: 0.576154

Early stopping
Best (RMSE):	 train: 0.578176	val: 0.673774	test: 0.819665
Best (MAE):	 train: 0.443020	val: 0.521600	test: 0.615046


Epoch: 84
Loss: 0.42059990018606186
RMSE train: 0.592487	val: 0.822244	test: 0.726870
MAE train: 0.447903	val: 0.635390	test: 0.552406

Epoch: 85
Loss: 0.46677081286907196
RMSE train: 0.566560	val: 0.863104	test: 0.733470
MAE train: 0.435293	val: 0.644937	test: 0.556157

Epoch: 86
Loss: 0.49094753712415695
RMSE train: 0.580823	val: 0.837750	test: 0.753932
MAE train: 0.449671	val: 0.644048	test: 0.567313

Epoch: 87
Loss: 0.4117862284183502
RMSE train: 0.527019	val: 0.847867	test: 0.726103
MAE train: 0.412139	val: 0.627467	test: 0.560065

Epoch: 88
Loss: 0.4605133682489395
RMSE train: 0.631188	val: 0.922017	test: 0.756150
MAE train: 0.485176	val: 0.697848	test: 0.565437

Epoch: 89
Loss: 0.44549689441919327
RMSE train: 0.597049	val: 0.806408	test: 0.788476
MAE train: 0.457300	val: 0.634150	test: 0.580358

Epoch: 90
Loss: 0.43240874260663986
RMSE train: 0.577508	val: 0.803518	test: 0.763043
MAE train: 0.444727	val: 0.637419	test: 0.575662

Epoch: 91
Loss: 0.47040270268917084
RMSE train: 0.663836	val: 0.921535	test: 0.754861
MAE train: 0.498820	val: 0.704926	test: 0.569937

Epoch: 92
Loss: 0.5262656062841415
RMSE train: 0.593730	val: 0.794280	test: 0.736993
MAE train: 0.448394	val: 0.609633	test: 0.548466

Epoch: 93
Loss: 0.43869275599718094
RMSE train: 0.603096	val: 0.824408	test: 0.772156
MAE train: 0.454136	val: 0.631924	test: 0.576445

Epoch: 94
Loss: 0.4506276771426201
RMSE train: 0.585463	val: 0.844986	test: 0.737624
MAE train: 0.442145	val: 0.644443	test: 0.551643

Epoch: 95
Loss: 0.45547735691070557
RMSE train: 0.566159	val: 0.843058	test: 0.727770
MAE train: 0.437637	val: 0.653395	test: 0.550674

Epoch: 96
Loss: 0.42449165135622025
RMSE train: 0.598939	val: 0.892047	test: 0.749151
MAE train: 0.456736	val: 0.696431	test: 0.563980

Epoch: 97
Loss: 0.4736965671181679
RMSE train: 0.556730	val: 0.841601	test: 0.744878
MAE train: 0.426626	val: 0.645368	test: 0.561564

Epoch: 98
Loss: 0.4397352486848831
RMSE train: 0.535489	val: 0.825646	test: 0.749354
MAE train: 0.412431	val: 0.620298	test: 0.570327

Epoch: 99
Loss: 0.3756752908229828
RMSE train: 0.583954	val: 0.900137	test: 0.737158
MAE train: 0.443396	val: 0.671171	test: 0.548029

Epoch: 100
Loss: 0.3565233126282692
RMSE train: 0.651292	val: 0.868541	test: 0.779300
MAE train: 0.491168	val: 0.682967	test: 0.574913

Epoch: 101
Loss: 0.429005391895771
RMSE train: 0.571450	val: 0.835232	test: 0.735635
MAE train: 0.432265	val: 0.641097	test: 0.548742

Epoch: 102
Loss: 0.37407388538122177
RMSE train: 0.611790	val: 0.847634	test: 0.746940
MAE train: 0.459592	val: 0.653503	test: 0.552486

Epoch: 103
Loss: 0.3844524174928665
RMSE train: 0.619924	val: 0.846518	test: 0.750751
MAE train: 0.465413	val: 0.644198	test: 0.549856

Epoch: 104
Loss: 0.36188968271017075
RMSE train: 0.577167	val: 0.838009	test: 0.728248
MAE train: 0.437011	val: 0.641437	test: 0.539868

Epoch: 105
Loss: 0.38203562051057816
RMSE train: 0.575297	val: 0.785843	test: 0.749274
MAE train: 0.434367	val: 0.617872	test: 0.552919

Epoch: 106
Loss: 0.37104862183332443
RMSE train: 0.699452	val: 0.891235	test: 0.823209
MAE train: 0.520066	val: 0.698917	test: 0.601060

Epoch: 107
Loss: 0.3998931869864464
RMSE train: 0.609852	val: 0.798056	test: 0.770877
MAE train: 0.461042	val: 0.626974	test: 0.568230

Epoch: 108
Loss: 0.3699883222579956
RMSE train: 0.577722	val: 0.832354	test: 0.736319
MAE train: 0.433845	val: 0.648471	test: 0.551144

Epoch: 109
Loss: 0.38777513056993484
RMSE train: 0.612072	val: 0.838835	test: 0.760934
MAE train: 0.465192	val: 0.657074	test: 0.559047

Epoch: 110
Loss: 0.3713985085487366
RMSE train: 0.538800	val: 0.807736	test: 0.738113
MAE train: 0.419001	val: 0.623486	test: 0.548049

Epoch: 111
Loss: 0.40708769112825394
RMSE train: 0.559974	val: 0.835846	test: 0.758498
MAE train: 0.434473	val: 0.656478	test: 0.557636

Epoch: 112
Loss: 0.40092915296554565
RMSE train: 0.573179	val: 0.788536	test: 0.765353
MAE train: 0.438868	val: 0.623565	test: 0.554550

Epoch: 113
Loss: 0.35812849551439285
RMSE train: 0.621555	val: 0.863546	test: 0.752820
MAE train: 0.462506	val: 0.667313	test: 0.551070

Epoch: 114
Loss: 0.36513087898492813
RMSE train: 0.601991	val: 0.832669	test: 0.747384
MAE train: 0.454851	val: 0.648336	test: 0.543515

Epoch: 115
Loss: 0.3821420893073082
RMSE train: 0.555736	val: 0.844097	test: 0.716486
MAE train: 0.422252	val: 0.643536	test: 0.527080

Epoch: 116
Loss: 0.4116363823413849
RMSE train: 0.488587	val: 0.779684	test: 0.717500
MAE train: 0.377090	val: 0.590763	test: 0.541514

Epoch: 117
Loss: 0.35929790884256363
RMSE train: 0.534686	val: 0.765531	test: 0.749389
MAE train: 0.412903	val: 0.598515	test: 0.556532

Epoch: 118
Loss: 0.3943452760577202
RMSE train: 0.563760	val: 0.830419	test: 0.755101
MAE train: 0.438074	val: 0.640010	test: 0.558816

Epoch: 119
Loss: 0.3831203058362007
RMSE train: 0.529015	val: 0.786441	test: 0.727307
MAE train: 0.409931	val: 0.605061	test: 0.535468

Epoch: 120
Loss: 0.35292720794677734
RMSE train: 0.549236	val: 0.831460	test: 0.701620
MAE train: 0.418705	val: 0.634574	test: 0.512213

Epoch: 121
Loss: 0.35987216234207153
RMSE train: 0.579948	val: 0.847452	test: 0.714669
MAE train: 0.433344	val: 0.654712	test: 0.515371

Epoch: 122
Loss: 0.34570250660181046
RMSE train: 0.569159	val: 0.811411	test: 0.719943
MAE train: 0.431092	val: 0.635890	test: 0.527557

Epoch: 123
Loss: 0.34707774966955185
RMSE train: 0.549706	val: 0.840927	test: 0.704609
MAE train: 0.418102	val: 0.638742	test: 0.524775

Epoch: 124
Loss: 0.3649243712425232
RMSE train: 0.504406	val: 0.790478	test: 0.724328
MAE train: 0.387814	val: 0.599190	test: 0.541725

Epoch: 125
Loss: 0.3351375013589859
RMSE train: 0.523714	val: 0.831582	test: 0.763386
MAE train: 0.405711	val: 0.631492	test: 0.564525

Epoch: 126
Loss: 0.3421657457947731
RMSE train: 0.500342	val: 0.808612	test: 0.744878
MAE train: 0.382641	val: 0.604729	test: 0.545634

Epoch: 127
Loss: 0.35724513977766037
RMSE train: 0.510892	val: 0.805736	test: 0.704369
MAE train: 0.391015	val: 0.604723	test: 0.524960

Epoch: 128
Loss: 0.3574120029807091
RMSE train: 0.544932	val: 0.828538	test: 0.714037
MAE train: 0.414261	val: 0.639188	test: 0.522494

Epoch: 129
Loss: 0.3259308636188507
RMSE train: 0.515105	val: 0.791703	test: 0.721466
MAE train: 0.398886	val: 0.611223	test: 0.531415

Epoch: 130
Loss: 0.3438626080751419
RMSE train: 0.496166	val: 0.835658	test: 0.716317
MAE train: 0.387537	val: 0.618053	test: 0.539709

Epoch: 131
Loss: 0.3478250205516815
RMSE train: 0.554494	val: 0.877616	test: 0.738912
MAE train: 0.423849	val: 0.658725	test: 0.557003

Epoch: 132
Loss: 0.33835267275571823
RMSE train: 0.516369	val: 0.774155	test: 0.729684
MAE train: 0.400069	val: 0.597664	test: 0.545214

Epoch: 133
Loss: 0.35582372546195984
RMSE train: 0.557213	val: 0.864245	test: 0.713957
MAE train: 0.423271	val: 0.651528	test: 0.526619

Epoch: 134
Loss: 0.3777773529291153
RMSE train: 0.566202	val: 0.829832	test: 0.712270
MAE train: 0.426370	val: 0.645305	test: 0.519487

Epoch: 135
Loss: 0.35207677632570267
RMSE train: 0.537113	val: 0.808131	test: 0.723280
MAE train: 0.407260	val: 0.630187	test: 0.532615

Epoch: 136
Loss: 0.2943127006292343
RMSE train: 0.510502	val: 0.799277	test: 0.731587
MAE train: 0.389752	val: 0.611864	test: 0.546718

Epoch: 137
Loss: 0.36287665367126465
RMSE train: 0.523272	val: 0.781623	test: 0.748802
MAE train: 0.402064	val: 0.604987	test: 0.568079

Epoch: 138
Loss: 0.28984610736370087
RMSE train: 0.582071	val: 0.894632	test: 0.738089
MAE train: 0.432961	val: 0.683766	test: 0.554161

Epoch: 139
Loss: 0.390905424952507
RMSE train: 0.560467	val: 0.799908	test: 0.738871
MAE train: 0.424589	val: 0.631434	test: 0.537864

Epoch: 140
Loss: 0.3706362769007683
RMSE train: 0.538677	val: 0.764452	test: 0.735122
MAE train: 0.405170	val: 0.603563	test: 0.549880

Epoch: 141
Loss: 0.36060331761837006
RMSE train: 0.587345	val: 0.933245	test: 0.721526
MAE train: 0.443850	val: 0.700144	test: 0.545091

Epoch: 142
Loss: 0.3259051591157913
RMSE train: 0.572029	val: 0.831103	test: 0.713822
MAE train: 0.435390	val: 0.653528	test: 0.537753

Epoch: 143
Loss: 0.33480361104011536
RMSE train: 0.567018	val: 0.859799	test: 0.698214
MAE train: 0.424955	val: 0.670242	test: 0.524279

Epoch: 144
Loss: 0.34473883360624313
MAE train: 0.419339	val: 0.567131	test: 0.637509

Epoch: 145
Loss: 0.35489751895268756
RMSE train: 0.530533	val: 0.752602	test: 0.877441
MAE train: 0.416227	val: 0.571537	test: 0.637586

Epoch: 146
Loss: 0.37580297390619916
RMSE train: 0.507464	val: 0.732669	test: 0.845218
MAE train: 0.396914	val: 0.554327	test: 0.619618

Epoch: 147
Loss: 0.3511468172073364
RMSE train: 0.568699	val: 0.779946	test: 0.875168
MAE train: 0.442465	val: 0.581357	test: 0.646374

Epoch: 148
Loss: 0.3374313811461131
RMSE train: 0.512605	val: 0.750752	test: 0.849370
MAE train: 0.394104	val: 0.550258	test: 0.628182

Epoch: 149
Loss: 0.3102913995583852
RMSE train: 0.489164	val: 0.737217	test: 0.839221
MAE train: 0.376861	val: 0.538672	test: 0.618656

Epoch: 150
Loss: 0.30043145020802814
RMSE train: 0.491730	val: 0.726065	test: 0.828111
MAE train: 0.385861	val: 0.534832	test: 0.605445

Epoch: 151
Loss: 0.36952124039332074
RMSE train: 0.538141	val: 0.747928	test: 0.850235
MAE train: 0.420621	val: 0.553735	test: 0.620159

Epoch: 152
Loss: 0.33319904406865436
RMSE train: 0.475830	val: 0.716392	test: 0.820445
MAE train: 0.371665	val: 0.531281	test: 0.599738

Epoch: 153
Loss: 0.2946913739045461
RMSE train: 0.519651	val: 0.741661	test: 0.830833
MAE train: 0.396675	val: 0.535193	test: 0.613329

Epoch: 154
Loss: 0.3175591727097829
RMSE train: 0.577106	val: 0.780838	test: 0.850741
MAE train: 0.433310	val: 0.563472	test: 0.634401

Epoch: 155
Loss: 0.3159263034661611
RMSE train: 0.559347	val: 0.780718	test: 0.850234
MAE train: 0.423316	val: 0.567794	test: 0.630683

Epoch: 156
Loss: 0.3152397374312083
RMSE train: 0.519630	val: 0.769030	test: 0.864268
MAE train: 0.400459	val: 0.564551	test: 0.635503

Epoch: 157
Loss: 0.3485889434814453
RMSE train: 0.451134	val: 0.735541	test: 0.841847
MAE train: 0.360231	val: 0.560941	test: 0.611355

Epoch: 158
Loss: 0.33718814452489215
RMSE train: 0.466006	val: 0.739877	test: 0.871786
MAE train: 0.372737	val: 0.565941	test: 0.623359

Epoch: 159
Loss: 0.31898583968480426
RMSE train: 0.441975	val: 0.734006	test: 0.849634
MAE train: 0.353993	val: 0.564027	test: 0.614351

Epoch: 160
Loss: 0.27596938610076904
RMSE train: 0.442877	val: 0.722578	test: 0.858739
MAE train: 0.348162	val: 0.536477	test: 0.612482

Epoch: 161
Loss: 0.2685523231824239
RMSE train: 0.499001	val: 0.743904	test: 0.889336
MAE train: 0.388638	val: 0.554476	test: 0.639509

Epoch: 162
Loss: 0.3042922218640645
RMSE train: 0.445186	val: 0.721090	test: 0.862191
MAE train: 0.350362	val: 0.551393	test: 0.627353

Epoch: 163
Loss: 0.31269074976444244
RMSE train: 0.458219	val: 0.719426	test: 0.858328
MAE train: 0.355659	val: 0.537036	test: 0.623570

Epoch: 164
Loss: 0.2862110535303752
RMSE train: 0.532065	val: 0.756029	test: 0.884962
MAE train: 0.410095	val: 0.559460	test: 0.641552

Epoch: 165
Loss: 0.32276779413223267
RMSE train: 0.498970	val: 0.743382	test: 0.872076
MAE train: 0.387242	val: 0.538775	test: 0.628031

Epoch: 166
Loss: 0.2774443030357361
RMSE train: 0.470786	val: 0.727375	test: 0.863085
MAE train: 0.369427	val: 0.526340	test: 0.623236

Epoch: 167
Loss: 0.2756871779759725
RMSE train: 0.498621	val: 0.739890	test: 0.871361
MAE train: 0.390769	val: 0.539382	test: 0.634941

Epoch: 168
Loss: 0.3575182457764943
RMSE train: 0.542095	val: 0.762879	test: 0.879485
MAE train: 0.425412	val: 0.570448	test: 0.646280

Epoch: 169
Loss: 0.32128674785296124
RMSE train: 0.507153	val: 0.751646	test: 0.873551
MAE train: 0.401272	val: 0.558158	test: 0.635299

Epoch: 170
Loss: 0.27940021455287933
RMSE train: 0.474293	val: 0.735921	test: 0.835475
MAE train: 0.376083	val: 0.541417	test: 0.611262

Epoch: 171
Loss: 0.29017295440038043
RMSE train: 0.520428	val: 0.753159	test: 0.844124
MAE train: 0.409989	val: 0.562497	test: 0.622652

Epoch: 172
Loss: 0.32932716608047485
RMSE train: 0.544729	val: 0.766371	test: 0.868810
MAE train: 0.428555	val: 0.578895	test: 0.639961

Epoch: 173
Loss: 0.28993510206540424
RMSE train: 0.469766	val: 0.731013	test: 0.837037
MAE train: 0.371440	val: 0.548541	test: 0.611813

Epoch: 174
Loss: 0.31092803676923114
RMSE train: 0.429554	val: 0.708300	test: 0.813786
MAE train: 0.341905	val: 0.536131	test: 0.592636

Epoch: 175
Loss: 0.2918468515078227
RMSE train: 0.469007	val: 0.715221	test: 0.828342
MAE train: 0.373161	val: 0.552877	test: 0.614268

Epoch: 176
Loss: 0.2862293521563212
RMSE train: 0.487307	val: 0.717586	test: 0.837540
MAE train: 0.384054	val: 0.548013	test: 0.619588

Epoch: 177
Loss: 0.2849673231442769
RMSE train: 0.449745	val: 0.704478	test: 0.832779
MAE train: 0.355522	val: 0.532771	test: 0.611134

Epoch: 178
Loss: 0.3542499939600627
RMSE train: 0.457219	val: 0.705078	test: 0.840599
MAE train: 0.356178	val: 0.523171	test: 0.612657

Epoch: 179
Loss: 0.3273494889338811
RMSE train: 0.460929	val: 0.719673	test: 0.822767
MAE train: 0.352451	val: 0.538273	test: 0.610814

Epoch: 180
Loss: 0.3058292766412099
RMSE train: 0.542374	val: 0.752554	test: 0.869288
MAE train: 0.414251	val: 0.553974	test: 0.637092

Epoch: 181
Loss: 0.29173627495765686
RMSE train: 0.506850	val: 0.737388	test: 0.886078
MAE train: 0.394852	val: 0.547712	test: 0.644754

Epoch: 182
Loss: 0.3443352182706197
RMSE train: 0.419823	val: 0.713265	test: 0.849492
MAE train: 0.341243	val: 0.555344	test: 0.626042

Epoch: 183
Loss: 0.2630588511625926
RMSE train: 0.423158	val: 0.719036	test: 0.845560
MAE train: 0.342864	val: 0.559733	test: 0.619491

Epoch: 184
Loss: 0.27950142820676166
RMSE train: 0.446837	val: 0.727638	test: 0.858005
MAE train: 0.356235	val: 0.555464	test: 0.622548

Epoch: 185
Loss: 0.3076777259508769
RMSE train: 0.450653	val: 0.730772	test: 0.855997
MAE train: 0.353619	val: 0.550049	test: 0.622138

Epoch: 186
Loss: 0.3287985523541768
RMSE train: 0.416806	val: 0.721316	test: 0.839256
MAE train: 0.323836	val: 0.540752	test: 0.609325

Epoch: 187
Loss: 0.35664180914560956
RMSE train: 0.451408	val: 0.730595	test: 0.853827
MAE train: 0.355115	val: 0.535917	test: 0.622262

Epoch: 188
Loss: 0.28406885266304016
RMSE train: 0.496995	val: 0.752311	test: 0.873448
MAE train: 0.391555	val: 0.555294	test: 0.633977

Epoch: 189
Loss: 0.25361313422520954
RMSE train: 0.466895	val: 0.726310	test: 0.846752
MAE train: 0.369392	val: 0.539179	test: 0.612729

Epoch: 190
Loss: 0.27655558784802753
RMSE train: 0.489031	val: 0.732889	test: 0.842132
MAE train: 0.379520	val: 0.541500	test: 0.615299

Epoch: 191
Loss: 0.26874499022960663
RMSE train: 0.531942	val: 0.753364	test: 0.866465
MAE train: 0.410172	val: 0.562033	test: 0.634382

Epoch: 192
Loss: 0.28925931453704834
RMSE train: 0.453284	val: 0.717423	test: 0.838937
MAE train: 0.352777	val: 0.534932	test: 0.610301

Epoch: 193
Loss: 0.2810901403427124
RMSE train: 0.397501	val: 0.706432	test: 0.826792
MAE train: 0.312641	val: 0.532223	test: 0.596985

Epoch: 194
Loss: 0.2353425125281016
RMSE train: 0.430109	val: 0.711780	test: 0.845934
MAE train: 0.337494	val: 0.532162	test: 0.607044

Epoch: 195
Loss: 0.2637167274951935
RMSE train: 0.488616	val: 0.738060	test: 0.846694
MAE train: 0.380425	val: 0.558583	test: 0.619005

Epoch: 196
Loss: 0.30580328901608783
RMSE train: 0.485087	val: 0.740941	test: 0.830434
MAE train: 0.371058	val: 0.556584	test: 0.610229

Epoch: 197
Loss: 0.24307568868001303
RMSE train: 0.445689	val: 0.714298	test: 0.823811
MAE train: 0.339563	val: 0.519927	test: 0.602641

Epoch: 198
Loss: 0.237465833624204
RMSE train: 0.465661	val: 0.728304	test: 0.839445
MAE train: 0.354617	val: 0.527498	test: 0.612910

Epoch: 199
Loss: 0.302807942032814
RMSE train: 0.515556	val: 0.755828	test: 0.839667
MAE train: 0.394741	val: 0.550705	test: 0.620097

Epoch: 200
Loss: 0.2626570612192154
RMSE train: 0.540752	val: 0.770089	test: 0.851002
MAE train: 0.414005	val: 0.566672	test: 0.632017

Epoch: 201
Loss: 0.29239559173583984
RMSE train: 0.525393	val: 0.762097	test: 0.864242
MAE train: 0.403752	val: 0.568232	test: 0.640129

Epoch: 202
Loss: 0.236860454082489
RMSE train: 0.479369	val: 0.738367	test: 0.864359
MAE train: 0.372845	val: 0.544491	test: 0.625860

Epoch: 203
Loss: 0.25264429052670795
RMSE train: 0.416745	val: 0.701940	test: 0.829192
MAE train: 0.329023	val: 0.528943	test: 0.592799

Epoch: 204
Loss: 0.282755305369695
RMSE train: 0.418974	val: 0.700919	test: 0.824316
MAE train: 0.331051	val: 0.531188	test: 0.591243All runs completed.


Epoch: 205
Loss: 0.26246319711208344
RMSE train: 0.477970	val: 0.725755	test: 0.854021
MAE train: 0.376842	val: 0.545619	test: 0.615477

Epoch: 206
Loss: 0.2746350169181824
RMSE train: 0.442121	val: 0.714884	test: 0.838034
MAE train: 0.347818	val: 0.539227	test: 0.601332

Epoch: 207
Loss: 0.2718026985724767
RMSE train: 0.439212	val: 0.723570	test: 0.837479
MAE train: 0.340970	val: 0.535182	test: 0.596679

Epoch: 208
Loss: 0.3358425001303355
RMSE train: 0.482434	val: 0.749493	test: 0.854725
MAE train: 0.372488	val: 0.553425	test: 0.609701

Epoch: 209
Loss: 0.2832562327384949
RMSE train: 0.477888	val: 0.737597	test: 0.866960
MAE train: 0.376208	val: 0.551945	test: 0.620641

Epoch: 210
Loss: 0.2478601485490799
RMSE train: 0.419007	val: 0.714041	test: 0.854087
MAE train: 0.338412	val: 0.553034	test: 0.613065

Epoch: 211
Loss: 0.24850431084632874
RMSE train: 0.426224	val: 0.710856	test: 0.865736
MAE train: 0.344407	val: 0.541960	test: 0.616061

Epoch: 212
Loss: 0.2744117279847463
RMSE train: 0.477331	val: 0.734516	test: 0.887064
MAE train: 0.379918	val: 0.557552	test: 0.637888

Epoch: 213
Loss: 0.3342503756284714
RMSE train: 0.456692	val: 0.734906	test: 0.864270
MAE train: 0.357869	val: 0.556651	test: 0.631692

Epoch: 214
Loss: 0.2791014115015666
RMSE train: 0.450335	val: 0.733079	test: 0.847612
MAE train: 0.352910	val: 0.548350	test: 0.621361

Epoch: 215
Loss: 0.28169673681259155
RMSE train: 0.440598	val: 0.731567	test: 0.846609
MAE train: 0.343439	val: 0.534033	test: 0.608872

Epoch: 216
Loss: 0.22703363498051962
RMSE train: 0.435025	val: 0.731133	test: 0.847172
MAE train: 0.343197	val: 0.539467	test: 0.603646

Epoch: 217
Loss: 0.25298459827899933
RMSE train: 0.420231	val: 0.720763	test: 0.821057
MAE train: 0.330481	val: 0.541873	test: 0.595511

Epoch: 218
Loss: 0.2356934448083242
RMSE train: 0.497740	val: 0.753373	test: 0.841593
MAE train: 0.386712	val: 0.553659	test: 0.612396

Epoch: 219
Loss: 0.2200993299484253
RMSE train: 0.517545	val: 0.769122	test: 0.853577
MAE train: 0.395923	val: 0.566866	test: 0.624000

Epoch: 220
Loss: 0.26377418140570325
RMSE train: 0.459644	val: 0.727462	test: 0.821254
MAE train: 0.350595	val: 0.530431	test: 0.596644

Epoch: 221
Loss: 0.2456212987502416
RMSE train: 0.442851	val: 0.706421	test: 0.813853
MAE train: 0.342525	val: 0.527614	test: 0.590958

Epoch: 222
Loss: 0.27735690275828045
RMSE train: 0.493992	val: 0.726859	test: 0.849345
MAE train: 0.380802	val: 0.544596	test: 0.611466

Epoch: 223
Loss: 0.24503521621227264
RMSE train: 0.434331	val: 0.707790	test: 0.835356
MAE train: 0.339433	val: 0.537532	test: 0.603307

Epoch: 224
Loss: 0.25535568098227185
RMSE train: 0.388409	val: 0.700989	test: 0.842764
MAE train: 0.304036	val: 0.531111	test: 0.599527

Epoch: 225
Loss: 0.2703799108664195
RMSE train: 0.438589	val: 0.718266	test: 0.869204
MAE train: 0.343320	val: 0.534699	test: 0.608847

Epoch: 226
Loss: 0.2298348049322764
RMSE train: 0.486327	val: 0.735981	test: 0.875597
MAE train: 0.381540	val: 0.562241	test: 0.633503

Epoch: 227
Loss: 0.27234259247779846
RMSE train: 0.458784	val: 0.724815	test: 0.859554
MAE train: 0.360167	val: 0.551358	test: 0.622028

Epoch: 228
Loss: 0.24761269489924112
RMSE train: 0.416043	val: 0.713047	test: 0.848387
MAE train: 0.322963	val: 0.525802	test: 0.608905

Epoch: 229
Loss: 0.23828696211179098
RMSE train: 0.421834	val: 0.720636	test: 0.853551
MAE train: 0.328864	val: 0.527462	test: 0.610840

Epoch: 230
Loss: 0.28065847357114154
RMSE train: 0.480685	val: 0.753348	test: 0.875340
MAE train: 0.377768	val: 0.564800	test: 0.635878

Epoch: 231
Loss: 0.27327755590279895
RMSE train: 0.467958	val: 0.744275	test: 0.844882
MAE train: 0.372860	val: 0.577320	test: 0.627591

Epoch: 232
Loss: 0.25143541395664215
RMSE train: 0.426982	val: 0.739252	test: 0.838821
MAE train: 0.341169	val: 0.577506	test: 0.617388

Epoch: 233
Loss: 0.2256940503915151
RMSE train: 0.411669	val: 0.739960	test: 0.860464
MAE train: 0.326259	val: 0.555876	test: 0.616377

Epoch: 234
Loss: 0.23106398185094199
RMSE train: 0.423728	val: 0.742301	test: 0.886467
MAE train: 0.342383	val: 0.562672	test: 0.632653

Epoch: 235
Loss: 0.28675607840220135
RMSE train: 0.419122	val: 0.728134	test: 0.867740
MAE train: 0.341443	val: 0.574130	test: 0.638101

Epoch: 236
Loss: 0.26722079515457153
RMSE train: 0.480263	val: 0.751616	test: 0.872662
MAE train: 0.380035	val: 0.595149	test: 0.647763

Epoch: 237
Loss: 0.2672825405995051
RMSE train: 0.478345	val: 0.762120	test: 0.893885
MAE train: 0.378338	val: 0.580204	test: 0.640883

Epoch: 238
Loss: 0.2450301001469294
RMSE train: 0.402580	val: 0.726649	test: 0.853935
MAE train: 0.315231	val: 0.546548	test: 0.601569

Epoch: 239
Loss: 0.23479020595550537
RMSE train: 0.385357	val: 0.719016	test: 0.827789
MAE train: 0.302325	val: 0.541110	test: 0.595849

Early stopping
Best (RMSE):	 train: 0.418974	val: 0.700919	test: 0.824316
Best (MAE):	 train: 0.331051	val: 0.531188	test: 0.591243
All runs completed.

RMSE train: 0.613992	val: 0.901430	test: 0.724891
MAE train: 0.445230	val: 0.696849	test: 0.533994

Epoch: 145
Loss: 0.32844748347997665
RMSE train: 0.542652	val: 0.765474	test: 0.730542
MAE train: 0.412588	val: 0.613134	test: 0.550223

Epoch: 146
Loss: 0.34012409299612045
RMSE train: 0.491526	val: 0.799355	test: 0.708327
MAE train: 0.377033	val: 0.612159	test: 0.519909

Epoch: 147
Loss: 0.3424271419644356
RMSE train: 0.477069	val: 0.799874	test: 0.731286
MAE train: 0.367875	val: 0.594310	test: 0.528669

Epoch: 148
Loss: 0.3026674836874008
RMSE train: 0.476635	val: 0.730869	test: 0.730549
MAE train: 0.368432	val: 0.562800	test: 0.548192

Epoch: 149
Loss: 0.279856164008379
RMSE train: 0.531444	val: 0.837996	test: 0.711091
MAE train: 0.405762	val: 0.638268	test: 0.530139

Epoch: 150
Loss: 0.3302418366074562
RMSE train: 0.506366	val: 0.816684	test: 0.699667
MAE train: 0.388425	val: 0.626478	test: 0.514745

Epoch: 151
Loss: 0.29440461844205856
RMSE train: 0.445705	val: 0.749647	test: 0.702283
MAE train: 0.342381	val: 0.575706	test: 0.523064

Epoch: 152
Loss: 0.3385157510638237
RMSE train: 0.604522	val: 0.988568	test: 0.757429
MAE train: 0.450128	val: 0.710477	test: 0.553236

Epoch: 153
Loss: 0.34008102864027023
RMSE train: 0.504028	val: 0.794898	test: 0.700903
MAE train: 0.382503	val: 0.609339	test: 0.516311

Epoch: 154
Loss: 0.3305013179779053
RMSE train: 0.552249	val: 0.759360	test: 0.731512
MAE train: 0.422253	val: 0.613947	test: 0.540830

Epoch: 155
Loss: 0.3443745896220207
RMSE train: 0.616599	val: 0.887594	test: 0.742700
MAE train: 0.453122	val: 0.684938	test: 0.550113

Epoch: 156
Loss: 0.3339274749159813
RMSE train: 0.487290	val: 0.812672	test: 0.688586
MAE train: 0.370315	val: 0.615990	test: 0.513525

Epoch: 157
Loss: 0.29453032463788986
RMSE train: 0.470299	val: 0.773037	test: 0.710332
MAE train: 0.365736	val: 0.584797	test: 0.532599

Epoch: 158
Loss: 0.27726082876324654
RMSE train: 0.528144	val: 0.830234	test: 0.746569
MAE train: 0.408127	val: 0.624795	test: 0.551993

Epoch: 159
Loss: 0.32738566026091576
RMSE train: 0.527508	val: 0.840797	test: 0.715510
MAE train: 0.402224	val: 0.637060	test: 0.528112

Epoch: 160
Loss: 0.2886829748749733
RMSE train: 0.502910	val: 0.766374	test: 0.699151
MAE train: 0.386703	val: 0.604496	test: 0.521540

Epoch: 161
Loss: 0.29145514965057373
RMSE train: 0.586017	val: 0.858149	test: 0.706357
MAE train: 0.441885	val: 0.674838	test: 0.515919

Epoch: 162
Loss: 0.3089723363518715
RMSE train: 0.547361	val: 0.911901	test: 0.690242
MAE train: 0.405130	val: 0.678124	test: 0.509556

Epoch: 163
Loss: 0.3090822249650955
RMSE train: 0.488645	val: 0.820226	test: 0.702856
MAE train: 0.370002	val: 0.617872	test: 0.514277

Epoch: 164
Loss: 0.2596057169139385
RMSE train: 0.554472	val: 0.859599	test: 0.753957
MAE train: 0.416955	val: 0.657851	test: 0.538859

Epoch: 165
Loss: 0.3024335354566574
RMSE train: 0.519282	val: 0.873766	test: 0.728245
MAE train: 0.397451	val: 0.658658	test: 0.543383

Epoch: 166
Loss: 0.30109528079628944
RMSE train: 0.480527	val: 0.803643	test: 0.693416
MAE train: 0.370521	val: 0.625539	test: 0.526964

Epoch: 167
Loss: 0.3219260349869728
RMSE train: 0.529141	val: 0.847908	test: 0.700934
MAE train: 0.397384	val: 0.648961	test: 0.525343

Epoch: 168
Loss: 0.31105831265449524
RMSE train: 0.533019	val: 0.849848	test: 0.707144
MAE train: 0.408086	val: 0.651936	test: 0.522606

Epoch: 169
Loss: 0.2927534654736519
RMSE train: 0.485623	val: 0.806673	test: 0.716156
MAE train: 0.374815	val: 0.617240	test: 0.535821

Epoch: 170
Loss: 0.2803485319018364
RMSE train: 0.468562	val: 0.857090	test: 0.709849
MAE train: 0.365469	val: 0.623048	test: 0.532470

Epoch: 171
Loss: 0.278580404818058
RMSE train: 0.465768	val: 0.803300	test: 0.735107
MAE train: 0.358508	val: 0.602242	test: 0.546317

Epoch: 172
Loss: 0.296750545501709
RMSE train: 0.566989	val: 0.815572	test: 0.747999
MAE train: 0.426076	val: 0.641048	test: 0.554486

Epoch: 173
Loss: 0.3152155727148056
RMSE train: 0.595119	val: 0.840075	test: 0.741781
MAE train: 0.445351	val: 0.659976	test: 0.560782

Epoch: 174
Loss: 0.2826197221875191
RMSE train: 0.503422	val: 0.816094	test: 0.689195
MAE train: 0.382214	val: 0.619770	test: 0.522595

Epoch: 175
Loss: 0.2851848192512989
RMSE train: 0.523163	val: 0.818695	test: 0.700271
MAE train: 0.400566	val: 0.625893	test: 0.530715

Epoch: 176
Loss: 0.31082823127508163
RMSE train: 0.530962	val: 0.875767	test: 0.715176
MAE train: 0.408874	val: 0.650774	test: 0.548392

Epoch: 177
Loss: 0.31927797943353653
RMSE train: 0.528652	val: 0.790885	test: 0.737333
MAE train: 0.406030	val: 0.620507	test: 0.554268

Epoch: 178
Loss: 0.28476959094405174
RMSE train: 0.549561	val: 0.844975	test: 0.724255
MAE train: 0.411471	val: 0.648613	test: 0.540143

Epoch: 179
Loss: 0.3086770847439766
RMSE train: 0.477332	val: 0.801650	test: 0.693710
MAE train: 0.371442	val: 0.601139	test: 0.523171

Epoch: 180
Loss: 0.2744578346610069
RMSE train: 0.560107	val: 0.878069	test: 0.734473
MAE train: 0.431014	val: 0.664210	test: 0.550276

Epoch: 181
Loss: 0.30041658878326416
RMSE train: 0.481313	val: 0.746656	test: 0.760696
MAE train: 0.376999	val: 0.573146	test: 0.576912

Epoch: 182
Loss: 0.2959289029240608
RMSE train: 0.487516	val: 0.798501	test: 0.775942
MAE train: 0.380526	val: 0.610135	test: 0.574207

Epoch: 183
Loss: 0.2862849608063698
RMSE train: 0.532052	val: 0.832522	test: 0.760755
MAE train: 0.421785	val: 0.644952	test: 0.566617

Early stopping
Best (RMSE):	 train: 0.476635	val: 0.730869	test: 0.730549
Best (MAE):	 train: 0.368432	val: 0.562800	test: 0.548192
All runs completed.
