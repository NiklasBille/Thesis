>>> Starting run for dataset: freesolv
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running SCAFF configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.6.yml on cuda:0
Running SCAFF configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.7.yml on cuda:1
Running SCAFF configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.8.yml on cuda:2
Starting process for seed 1: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.8.yml --runseed 1 --device cuda:2
Starting process for seed 2: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.8.yml --runseed 2 --device cuda:2
Starting process for seed 3: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.8.yml --runseed 3 --device cuda:2
Starting process for seed 1: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.7.yml --runseed 1 --device cuda:1
Starting process for seed 2: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.7.yml --runseed 2 --device cuda:1
Starting process for seed 3: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.7.yml --runseed 3 --device cuda:1
Starting process for seed 1: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.6.yml --runseed 1 --device cuda:0
Starting process for seed 2: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.6.yml --runseed 2 --device cuda:0
Starting process for seed 3: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.6.yml --runseed 3 --device cuda:0
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/freesolv/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/freesolv/scaff/train_prop=0.6/freesolv_scaff_3_20-05_14-43-15  ]
[ Using Seed :  3  ]
[ Using device :  cuda:0  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 16.991137981414795
RMSE train: 4.221897	val: 5.611857	test: 7.898349
MAE train: 3.410507	val: 4.774702	test: 6.228401

Epoch: 2
Loss: 16.517903327941895
RMSE train: 4.228627	val: 5.553527	test: 7.833685
MAE train: 3.426619	val: 4.739381	test: 6.147988

Epoch: 3
Loss: 15.0310959815979
RMSE train: 4.154389	val: 5.434640	test: 7.688924
MAE train: 3.369265	val: 4.633363	test: 6.014376

Epoch: 4
Loss: 13.623531341552734
RMSE train: 3.954444	val: 5.272304	test: 7.491776
MAE train: 3.203649	val: 4.480525	test: 5.843065

Epoch: 5
Loss: 12.901329040527344
RMSE train: 3.689373	val: 5.121895	test: 7.257697
MAE train: 2.986344	val: 4.340225	test: 5.642207

Epoch: 6
Loss: 11.635118007659912
RMSE train: 3.438283	val: 4.973257	test: 6.984568
MAE train: 2.806875	val: 4.210884	test: 5.434017

Epoch: 7
Loss: 10.831128597259521
RMSE train: 3.254358	val: 4.800430	test: 6.704480
MAE train: 2.712291	val: 4.054966	test: 5.223605

Epoch: 8
Loss: 9.53149700164795
RMSE train: 3.134124	val: 4.522733	test: 6.435377
MAE train: 2.669806	val: 3.778792	test: 5.014481

Epoch: 9
Loss: 9.387681007385254
RMSE train: 3.104850	val: 4.179633	test: 6.151413
MAE train: 2.694420	val: 3.428232	test: 4.778927

Epoch: 10
Loss: 8.033703804016113
RMSE train: 3.074107	val: 3.804467	test: 5.848392
MAE train: 2.665694	val: 3.068201	test: 4.517171

Epoch: 11
Loss: 8.220770597457886
RMSE train: 3.087804	val: 3.634615	test: 5.662177
MAE train: 2.665239	val: 2.923084	test: 4.346306

Epoch: 12
Loss: 7.037130832672119
RMSE train: 3.078141	val: 3.741073	test: 5.573828
MAE train: 2.663991	val: 3.066626	test: 4.249999

Epoch: 13
Loss: 8.102180004119873
RMSE train: 3.020742	val: 3.877980	test: 5.609967
MAE train: 2.637525	val: 3.242959	test: 4.254716

Epoch: 14
Loss: 6.973541498184204
RMSE train: 2.941225	val: 4.116572	test: 5.718109
MAE train: 2.583036	val: 3.559987	test: 4.310073

Epoch: 15
Loss: 6.125027418136597
RMSE train: 2.816522	val: 4.254317	test: 5.771502
MAE train: 2.456297	val: 3.749781	test: 4.311369

Epoch: 16
Loss: 6.094316482543945
RMSE train: 2.737984	val: 4.253497	test: 5.796333
MAE train: 2.376292	val: 3.772036	test: 4.328600

Epoch: 17
Loss: 5.938834190368652
RMSE train: 2.685166	val: 4.171806	test: 5.764468
MAE train: 2.326348	val: 3.698199	test: 4.297685

Epoch: 18
Loss: 5.482566833496094
RMSE train: 2.657796	val: 4.047817	test: 5.698555
MAE train: 2.311317	val: 3.584095	test: 4.223830

Epoch: 19
Loss: 5.34810996055603
RMSE train: 2.638834	val: 3.909519	test: 5.605511
MAE train: 2.300355	val: 3.449797	test: 4.151948

Epoch: 20
Loss: 5.10157585144043
RMSE train: 2.631168	val: 3.752403	test: 5.496394
MAE train: 2.303152	val: 3.293972	test: 4.062250

Epoch: 21
Loss: 5.457251787185669
RMSE train: 2.632357	val: 3.698147	test: 5.450512
MAE train: 2.305197	val: 3.233311	test: 4.040198

Epoch: 22
Loss: 4.675185203552246
RMSE train: 2.636669	val: 3.701796	test: 5.486604
MAE train: 2.311762	val: 3.231462	test: 4.073626Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/freesolv/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/freesolv/scaff/train_prop=0.6/freesolv_scaff_2_20-05_14-43-15  ]
[ Using Seed :  2  ]
[ Using device :  cuda:0  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 17.54345703125
RMSE train: 4.098897	val: 5.756585	test: 8.153176
MAE train: 3.279828	val: 4.935153	test: 6.365675

Epoch: 2
Loss: 16.38554573059082
RMSE train: 4.115285	val: 5.716964	test: 8.083643
MAE train: 3.324126	val: 4.920852	test: 6.313586

Epoch: 3
Loss: 15.183122158050537
RMSE train: 4.144191	val: 5.667304	test: 7.940846
MAE train: 3.366936	val: 4.895235	test: 6.147933

Epoch: 4
Loss: 15.669020652770996
RMSE train: 4.040910	val: 5.611507	test: 7.789960
MAE train: 3.267072	val: 4.859003	test: 5.973720

Epoch: 5
Loss: 13.717694759368896
RMSE train: 3.876775	val: 5.589051	test: 7.598836
MAE train: 3.140662	val: 4.863302	test: 5.770428

Epoch: 6
Loss: 11.529846668243408
RMSE train: 3.674129	val: 5.571012	test: 7.384194
MAE train: 3.027457	val: 4.881460	test: 5.583286

Epoch: 7
Loss: 10.852630615234375
RMSE train: 3.481993	val: 5.520513	test: 7.144660
MAE train: 2.902987	val: 4.862263	test: 5.401503

Epoch: 8
Loss: 11.228331089019775
RMSE train: 3.360863	val: 5.348246	test: 6.906211
MAE train: 2.833609	val: 4.676197	test: 5.259153

Epoch: 9
Loss: 8.750938177108765
RMSE train: 3.253523	val: 4.991473	test: 6.585654
MAE train: 2.762880	val: 4.229887	test: 5.029262

Epoch: 10
Loss: 9.32212209701538
RMSE train: 3.250835	val: 4.882273	test: 6.439739
MAE train: 2.779631	val: 4.129201	test: 5.006018

Epoch: 11
Loss: 9.214282989501953
RMSE train: 3.255850	val: 5.047033	test: 6.438038
MAE train: 2.818131	val: 4.404525	test: 5.078106

Epoch: 12
Loss: 8.842770338058472
RMSE train: 3.231490	val: 5.341044	test: 6.508724
MAE train: 2.821039	val: 4.842431	test: 5.177963

Epoch: 13
Loss: 7.508398056030273
RMSE train: 3.191092	val: 5.469761	test: 6.465801
MAE train: 2.792660	val: 5.028587	test: 5.081432

Epoch: 14
Loss: 7.3831892013549805
RMSE train: 3.141328	val: 5.437014	test: 6.339241
MAE train: 2.751987	val: 5.012248	test: 4.835036

Epoch: 15
Loss: 6.89377760887146
RMSE train: 3.060471	val: 5.184651	test: 6.168826
MAE train: 2.691914	val: 4.750996	test: 4.668580

Epoch: 16
Loss: 7.171304702758789
RMSE train: 2.989089	val: 4.712114	test: 5.916868
MAE train: 2.647173	val: 4.216297	test: 4.423480

Epoch: 17
Loss: 6.338488817214966
RMSE train: 2.942370	val: 4.197984	test: 5.681633
MAE train: 2.626171	val: 3.625267	test: 4.204461

Epoch: 18
Loss: 6.498776435852051
RMSE train: 2.931150	val: 3.796855	test: 5.499803
MAE train: 2.626678	val: 3.172710	test: 4.073788

Epoch: 19
Loss: 5.705745458602905
RMSE train: 2.921166	val: 3.649953	test: 5.328134
MAE train: 2.624365	val: 3.026520	test: 3.963111

Epoch: 20
Loss: 5.395397901535034
RMSE train: 2.897604	val: 3.708263	test: 5.264871
MAE train: 2.607325	val: 3.142105	test: 3.928368

Epoch: 21
Loss: 5.063041925430298
RMSE train: 2.837586	val: 3.821810	test: 5.179730
MAE train: 2.552461	val: 3.340691	test: 3.888549

Epoch: 22
Loss: 4.888573169708252
RMSE train: 2.776211	val: 3.964096	test: 5.172383
MAE train: 2.486782	val: 3.549371	test: 3.924332Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/freesolv/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/freesolv/scaff/train_prop=0.6/freesolv_scaff_1_20-05_14-43-15  ]
[ Using Seed :  1  ]
[ Using device :  cuda:0  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 19.135100841522217
RMSE train: 4.262042	val: 5.688457	test: 8.468054
MAE train: 3.427112	val: 4.850061	test: 6.804791

Epoch: 2
Loss: 16.010786533355713
RMSE train: 4.250176	val: 5.658327	test: 8.378226
MAE train: 3.427841	val: 4.845984	test: 6.702174

Epoch: 3
Loss: 15.105768203735352
RMSE train: 4.208140	val: 5.608831	test: 8.260998
MAE train: 3.403321	val: 4.826193	test: 6.559210

Epoch: 4
Loss: 15.309648513793945
RMSE train: 4.130593	val: 5.526771	test: 8.131865
MAE train: 3.345751	val: 4.771491	test: 6.417889

Epoch: 5
Loss: 13.427688121795654
RMSE train: 4.000483	val: 5.420765	test: 7.940514
MAE train: 3.259981	val: 4.697455	test: 6.215420

Epoch: 6
Loss: 11.622788906097412
RMSE train: 3.802009	val: 5.271281	test: 7.637067
MAE train: 3.142769	val: 4.580910	test: 5.933923

Epoch: 7
Loss: 10.481409549713135
RMSE train: 3.564188	val: 5.042490	test: 7.271076
MAE train: 2.991743	val: 4.385402	test: 5.637955

Epoch: 8
Loss: 9.759256839752197
RMSE train: 3.311677	val: 4.777611	test: 6.850332
MAE train: 2.810646	val: 4.157109	test: 5.305504

Epoch: 9
Loss: 9.226678371429443
RMSE train: 3.124157	val: 4.447560	test: 6.442006
MAE train: 2.671751	val: 3.827728	test: 4.972530

Epoch: 10
Loss: 8.641332864761353
RMSE train: 3.037153	val: 4.177020	test: 6.062915
MAE train: 2.594823	val: 3.560689	test: 4.646329

Epoch: 11
Loss: 8.387527465820312
RMSE train: 3.017377	val: 4.045482	test: 5.797509
MAE train: 2.591222	val: 3.441389	test: 4.466813

Epoch: 12
Loss: 7.8508687019348145
RMSE train: 3.032564	val: 4.148745	test: 5.701269
MAE train: 2.622858	val: 3.617656	test: 4.408808

Epoch: 13
Loss: 7.7896716594696045
RMSE train: 3.033338	val: 4.221040	test: 5.640035
MAE train: 2.629102	val: 3.722741	test: 4.372005

Epoch: 14
Loss: 7.652857065200806
RMSE train: 3.044198	val: 4.352271	test: 5.706699
MAE train: 2.646592	val: 3.875657	test: 4.457616

Epoch: 15
Loss: 7.3534674644470215
RMSE train: 3.034707	val: 4.464554	test: 5.796754
MAE train: 2.642681	val: 4.002170	test: 4.541561

Epoch: 16
Loss: 7.017451286315918
RMSE train: 3.001328	val: 4.452732	test: 5.804973
MAE train: 2.613643	val: 3.990818	test: 4.539779

Epoch: 17
Loss: 6.151090383529663
RMSE train: 2.904384	val: 4.330504	test: 5.772030
MAE train: 2.529670	val: 3.860391	test: 4.501216

Epoch: 18
Loss: 5.551103830337524
RMSE train: 2.810142	val: 4.128733	test: 5.684175
MAE train: 2.452100	val: 3.646497	test: 4.414831

Epoch: 19
Loss: 5.6703832149505615
RMSE train: 2.733989	val: 3.937482	test: 5.611852
MAE train: 2.382150	val: 3.440528	test: 4.345353

Epoch: 20
Loss: 5.1137261390686035
RMSE train: 2.680463	val: 3.764402	test: 5.523754
MAE train: 2.336186	val: 3.261150	test: 4.269665

Epoch: 21
Loss: 5.31031060218811
RMSE train: 2.640680	val: 3.555137	test: 5.396801
MAE train: 2.318301	val: 3.033613	test: 4.164227

Epoch: 22
Loss: 4.7093024253845215
RMSE train: 2.577546	val: 3.349613	test: 5.233330
MAE train: 2.262623	val: 2.824799	test: 4.019344Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/freesolv/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/freesolv/scaff/train_prop=0.7/freesolv_scaff_3_20-05_14-43-15  ]
[ Using Seed :  3  ]
[ Using device :  cuda:1  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 26.47105312347412
RMSE train: 5.101031	val: 5.606850	test: 6.664141
MAE train: 3.923483	val: 4.764700	test: 5.514101

Epoch: 2
Loss: 23.754758834838867
RMSE train: 5.133934	val: 5.570324	test: 6.692687
MAE train: 4.014681	val: 4.753621	test: 5.582173

Epoch: 3
Loss: 22.4971342086792
RMSE train: 5.047235	val: 5.508430	test: 6.480094
MAE train: 4.008820	val: 4.720236	test: 5.392918

Epoch: 4
Loss: 20.3792667388916
RMSE train: 4.850185	val: 5.391853	test: 6.256126
MAE train: 3.910273	val: 4.630306	test: 5.215226

Epoch: 5
Loss: 18.82701587677002
RMSE train: 4.533909	val: 5.204305	test: 5.911477
MAE train: 3.720199	val: 4.457149	test: 4.956061

Epoch: 6
Loss: 16.96169090270996
RMSE train: 4.215185	val: 5.151587	test: 5.546087
MAE train: 3.568705	val: 4.454453	test: 4.701526

Epoch: 7
Loss: 15.383252620697021
RMSE train: 3.979718	val: 5.111886	test: 5.315478
MAE train: 3.445630	val: 4.477539	test: 4.532621

Epoch: 8
Loss: 14.22676944732666
RMSE train: 3.858043	val: 5.108209	test: 5.098268
MAE train: 3.381770	val: 4.535197	test: 4.287196

Epoch: 9
Loss: 13.351038932800293
RMSE train: 3.799508	val: 5.117221	test: 4.944890
MAE train: 3.344275	val: 4.591057	test: 4.220727

Epoch: 10
Loss: 13.01685619354248
RMSE train: 3.728764	val: 5.133381	test: 4.778946
MAE train: 3.313930	val: 4.651201	test: 4.057430

Epoch: 11
Loss: 12.082990646362305
RMSE train: 3.588418	val: 5.149760	test: 4.662755
MAE train: 3.179326	val: 4.713075	test: 3.865200

Epoch: 12
Loss: 11.775407791137695
RMSE train: 3.504246	val: 5.205938	test: 4.670599
MAE train: 3.093746	val: 4.795343	test: 3.829524

Epoch: 13
Loss: 11.73660135269165
RMSE train: 3.512689	val: 5.240555	test: 4.807553
MAE train: 3.114137	val: 4.841936	test: 3.989660

Epoch: 14
Loss: 10.803136825561523
RMSE train: 3.497386	val: 5.234866	test: 4.941785
MAE train: 3.086833	val: 4.841438	test: 4.145613

Epoch: 15
Loss: 10.080187320709229
RMSE train: 3.508579	val: 5.240570	test: 5.038597
MAE train: 3.077026	val: 4.842939	test: 4.289342

Epoch: 16
Loss: 9.967813491821289
RMSE train: 3.482107	val: 5.171256	test: 4.946683
MAE train: 3.054385	val: 4.766761	test: 4.175727

Epoch: 17
Loss: 9.566659450531006
RMSE train: 3.478103	val: 5.115568	test: 4.821369
MAE train: 3.067101	val: 4.702291	test: 4.004592

Epoch: 18
Loss: 9.341928958892822
RMSE train: 3.501392	val: 5.058249	test: 4.684121
MAE train: 3.108629	val: 4.635230	test: 3.837750

Epoch: 19
Loss: 8.74100923538208
RMSE train: 3.531371	val: 5.006889	test: 4.601701
MAE train: 3.147941	val: 4.578879	test: 3.768020

Epoch: 20
Loss: 8.550244092941284
RMSE train: 3.514342	val: 4.951538	test: 4.552044
MAE train: 3.136466	val: 4.524583	test: 3.712504

Epoch: 21
Loss: 7.790039539337158
RMSE train: 3.464357	val: 4.912284	test: 4.546045
MAE train: 3.098115	val: 4.495512	test: 3.743362

Epoch: 22
Loss: 7.763719081878662
RMSE train: 3.389255	val: 4.813335	test: 4.563922
MAE train: 3.026542	val: 4.401104	test: 3.844681Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/freesolv/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/freesolv/scaff/train_prop=0.7/freesolv_scaff_1_20-05_14-43-15  ]
[ Using Seed :  1  ]
[ Using device :  cuda:1  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 26.577876091003418
RMSE train: 5.439609	val: 5.783220	test: 7.446270
MAE train: 4.200784	val: 4.946311	test: 6.370863

Epoch: 2
Loss: 24.53364849090576
RMSE train: 5.549743	val: 5.801591	test: 7.404795
MAE train: 4.355383	val: 4.988088	test: 6.344499

Epoch: 3
Loss: 23.422919273376465
RMSE train: 5.571663	val: 5.807798	test: 7.311551
MAE train: 4.425146	val: 5.025181	test: 6.277883

Epoch: 4
Loss: 22.767709732055664
RMSE train: 5.513581	val: 5.781153	test: 7.008233
MAE train: 4.430035	val: 5.030948	test: 5.975693

Epoch: 5
Loss: 21.41099262237549
RMSE train: 5.350394	val: 5.717015	test: 6.540936
MAE train: 4.365648	val: 4.999859	test: 5.530287

Epoch: 6
Loss: 19.89298152923584
RMSE train: 5.102258	val: 5.583101	test: 6.068719
MAE train: 4.241039	val: 4.900375	test: 5.166448

Epoch: 7
Loss: 17.412137031555176
RMSE train: 4.825191	val: 5.528106	test: 5.727637
MAE train: 4.119168	val: 4.888356	test: 4.925382

Epoch: 8
Loss: 15.660013675689697
RMSE train: 4.486495	val: 5.517856	test: 5.436099
MAE train: 3.928683	val: 4.931620	test: 4.659485

Epoch: 9
Loss: 13.976822853088379
RMSE train: 4.131837	val: 5.462931	test: 5.382339
MAE train: 3.637101	val: 4.935078	test: 4.698459

Epoch: 10
Loss: 13.007977962493896
RMSE train: 3.871215	val: 5.369052	test: 5.424217
MAE train: 3.428877	val: 4.900111	test: 4.773078

Epoch: 11
Loss: 13.328022480010986
RMSE train: 3.685086	val: 5.210205	test: 5.311732
MAE train: 3.278418	val: 4.783420	test: 4.707789

Epoch: 12
Loss: 12.419291019439697
RMSE train: 3.565676	val: 5.007124	test: 5.121573
MAE train: 3.176225	val: 4.611447	test: 4.528758

Epoch: 13
Loss: 10.97055196762085
RMSE train: 3.450508	val: 4.784066	test: 4.858988
MAE train: 3.072631	val: 4.411017	test: 4.300432

Epoch: 14
Loss: 10.843199729919434
RMSE train: 3.403738	val: 4.571007	test: 4.597292
MAE train: 3.027380	val: 4.197084	test: 4.034034

Epoch: 15
Loss: 10.882001399993896
RMSE train: 3.365388	val: 4.394125	test: 4.426016
MAE train: 2.991584	val: 3.999947	test: 3.829902

Epoch: 16
Loss: 9.945122718811035
RMSE train: 3.348034	val: 4.319443	test: 4.344028
MAE train: 2.977640	val: 3.912764	test: 3.686092

Epoch: 17
Loss: 9.768331050872803
RMSE train: 3.332161	val: 4.305253	test: 4.332305
MAE train: 2.951300	val: 3.892818	test: 3.652072

Epoch: 18
Loss: 9.33052921295166
RMSE train: 3.309772	val: 4.298874	test: 4.366789
MAE train: 2.916856	val: 3.884966	test: 3.689817

Epoch: 19
Loss: 8.794918775558472
RMSE train: 3.243017	val: 4.329431	test: 4.422476
MAE train: 2.830928	val: 3.925706	test: 3.782913

Epoch: 20
Loss: 8.188759326934814
RMSE train: 3.192562	val: 4.373186	test: 4.512866
MAE train: 2.770712	val: 3.986940	test: 3.935716

Epoch: 21
Loss: 8.420347213745117
RMSE train: 3.162681	val: 4.309531	test: 4.558063
MAE train: 2.746566	val: 3.932621	test: 4.013760

Epoch: 22
Loss: 7.618659734725952
RMSE train: 3.151242	val: 4.259946	test: 4.616271
MAE train: 2.746436	val: 3.894743	test: 4.080779Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/freesolv/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/freesolv/scaff/train_prop=0.7/freesolv_scaff_2_20-05_14-43-15  ]
[ Using Seed :  2  ]
[ Using device :  cuda:1  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 26.85029888153076
RMSE train: 5.027373	val: 5.752223	test: 6.873115
MAE train: 3.763925	val: 4.925947	test: 5.699603

Epoch: 2
Loss: 24.920552253723145
RMSE train: 5.014853	val: 5.752190	test: 6.827555
MAE train: 3.803892	val: 4.955835	test: 5.726992

Epoch: 3
Loss: 23.96853733062744
RMSE train: 4.921530	val: 5.739556	test: 6.643921
MAE train: 3.772634	val: 4.975936	test: 5.611623

Epoch: 4
Loss: 22.3564510345459
RMSE train: 4.703352	val: 5.715416	test: 6.334066
MAE train: 3.628634	val: 4.983927	test: 5.402896

Epoch: 5
Loss: 20.453408241271973
RMSE train: 4.418231	val: 5.685596	test: 5.945108
MAE train: 3.450109	val: 4.989086	test: 5.079791

Epoch: 6
Loss: 18.31240177154541
RMSE train: 4.121677	val: 5.689264	test: 5.695202
MAE train: 3.304892	val: 5.034253	test: 4.864692

Epoch: 7
Loss: 16.898080348968506
RMSE train: 3.836525	val: 5.712380	test: 5.575549
MAE train: 3.173434	val: 5.098584	test: 4.729886

Epoch: 8
Loss: 15.559637546539307
RMSE train: 3.632557	val: 5.737891	test: 5.522925
MAE train: 3.030667	val: 5.167078	test: 4.714157

Epoch: 9
Loss: 14.67004108428955
RMSE train: 3.524861	val: 5.741248	test: 5.474968
MAE train: 2.956180	val: 5.213845	test: 4.716669

Epoch: 10
Loss: 13.594833374023438
RMSE train: 3.494392	val: 5.741089	test: 5.537967
MAE train: 2.957054	val: 5.256129	test: 4.680354

Epoch: 11
Loss: 12.764352321624756
RMSE train: 3.497028	val: 5.673140	test: 5.526142
MAE train: 2.997864	val: 5.219594	test: 4.618930

Epoch: 12
Loss: 12.392458438873291
RMSE train: 3.499212	val: 5.572350	test: 5.301029
MAE train: 3.038447	val: 5.136457	test: 4.489582

Epoch: 13
Loss: 11.630799770355225
RMSE train: 3.492545	val: 5.516176	test: 5.090088
MAE train: 3.073119	val: 5.094408	test: 4.329458

Epoch: 14
Loss: 11.17141056060791
RMSE train: 3.515185	val: 5.459982	test: 5.004981
MAE train: 3.097380	val: 5.046859	test: 4.291600

Epoch: 15
Loss: 10.77381944656372
RMSE train: 3.559002	val: 5.387941	test: 4.990038
MAE train: 3.157806	val: 4.979407	test: 4.288169

Epoch: 16
Loss: 10.370204448699951
RMSE train: 3.582195	val: 5.321569	test: 4.972136
MAE train: 3.198100	val: 4.913286	test: 4.276115

Epoch: 17
Loss: 9.891893863677979
RMSE train: 3.578086	val: 5.235136	test: 4.905174
MAE train: 3.206266	val: 4.822190	test: 4.195708

Epoch: 18
Loss: 9.338467597961426
RMSE train: 3.525638	val: 5.189465	test: 4.801240
MAE train: 3.163285	val: 4.776891	test: 4.084974

Epoch: 19
Loss: 9.234067916870117
RMSE train: 3.454986	val: 5.173929	test: 4.656311
MAE train: 3.099202	val: 4.762633	test: 3.918046

Epoch: 20
Loss: 8.858376026153564
RMSE train: 3.434559	val: 5.194446	test: 4.565393
MAE train: 3.074603	val: 4.781678	test: 3.778457

Epoch: 21
Loss: 8.16366982460022
RMSE train: 3.415243	val: 5.220856	test: 4.516403
MAE train: 3.045287	val: 4.814265	test: 3.654589

Epoch: 22
Loss: 7.574469327926636
RMSE train: 3.379729	val: 5.184027	test: 4.490120
MAE train: 3.003558	val: 4.774117	test: 3.580552Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/freesolv/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/freesolv/scaff/train_prop=0.8/freesolv_scaff_3_20-05_14-43-15  ]
[ Using Seed :  3  ]
[ Using device :  cuda:2  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 21.00619125366211
RMSE train: 4.811788	val: 8.515324	test: 6.733483
MAE train: 3.933588	val: 6.494791	test: 5.826450

Epoch: 2
Loss: 19.751975059509277
RMSE train: 4.882767	val: 8.497584	test: 6.935977
MAE train: 4.044635	val: 6.505386	test: 6.027646

Epoch: 3
Loss: 18.752111434936523
RMSE train: 4.979359	val: 8.392481	test: 6.943462
MAE train: 4.192227	val: 6.422693	test: 6.038887

Epoch: 4
Loss: 17.77876853942871
RMSE train: 4.932559	val: 8.225839	test: 6.849658
MAE train: 4.189295	val: 6.330085	test: 5.955491

Epoch: 5
Loss: 16.61654758453369
RMSE train: 4.789548	val: 8.034197	test: 6.727618
MAE train: 4.114783	val: 6.236423	test: 5.849887

Epoch: 6
Loss: 15.196025371551514
RMSE train: 4.592185	val: 7.846186	test: 6.622303
MAE train: 3.981596	val: 6.173342	test: 5.764144

Epoch: 7
Loss: 14.169169425964355
RMSE train: 4.240843	val: 7.573804	test: 6.439117
MAE train: 3.705488	val: 6.015375	test: 5.597335

Epoch: 8
Loss: 12.997424125671387
RMSE train: 3.798493	val: 7.145354	test: 6.203750
MAE train: 3.354684	val: 5.740448	test: 5.368321

Epoch: 9
Loss: 12.306687831878662
RMSE train: 3.468912	val: 6.774878	test: 6.041731
MAE train: 3.080459	val: 5.500435	test: 5.212497

Epoch: 10
Loss: 11.457278728485107
RMSE train: 3.190945	val: 6.393821	test: 5.851226
MAE train: 2.801853	val: 5.178766	test: 4.987320

Epoch: 11
Loss: 10.68510103225708
RMSE train: 3.043943	val: 6.109783	test: 5.728725
MAE train: 2.632851	val: 4.907904	test: 4.859922

Epoch: 12
Loss: 10.34012508392334
RMSE train: 3.013887	val: 5.996146	test: 5.707413
MAE train: 2.600137	val: 4.845243	test: 4.860040

Epoch: 13
Loss: 9.771429061889648
RMSE train: 3.033612	val: 6.015565	test: 5.728527
MAE train: 2.625502	val: 4.944782	test: 4.893849

Epoch: 14
Loss: 9.252004623413086
RMSE train: 3.049620	val: 6.066496	test: 5.738039
MAE train: 2.653741	val: 5.080489	test: 4.931307

Epoch: 15
Loss: 8.861753225326538
RMSE train: 3.047370	val: 6.130854	test: 5.700854
MAE train: 2.666854	val: 5.192564	test: 4.925853

Epoch: 16
Loss: 8.270267963409424
RMSE train: 3.042649	val: 6.175431	test: 5.653388
MAE train: 2.683471	val: 5.245370	test: 4.896559

Epoch: 17
Loss: 7.956530332565308
RMSE train: 3.049980	val: 6.185407	test: 5.554171
MAE train: 2.716469	val: 5.243413	test: 4.821524

Epoch: 18
Loss: 7.544031858444214
RMSE train: 3.079697	val: 6.188433	test: 5.475690
MAE train: 2.771177	val: 5.227984	test: 4.747089

Epoch: 19
Loss: 7.243429183959961
RMSE train: 3.106265	val: 6.173199	test: 5.402522
MAE train: 2.810216	val: 5.163800	test: 4.656317

Epoch: 20
Loss: 6.941314697265625
RMSE train: 3.091966	val: 6.173637	test: 5.334489
MAE train: 2.795083	val: 5.120102	test: 4.572768

Epoch: 21
Loss: 6.569024562835693
RMSE train: 3.052306	val: 6.183275	test: 5.304615
MAE train: 2.752040	val: 5.103774	test: 4.537797

Epoch: 22
Loss: 6.121359586715698
RMSE train: 2.988914	val: 6.168489	test: 5.257101
MAE train: 2.686106	val: 5.081613	test: 4.498995Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/freesolv/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/freesolv/scaff/train_prop=0.8/freesolv_scaff_1_20-05_14-43-15  ]
[ Using Seed :  1  ]
[ Using device :  cuda:2  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 21.71657657623291
RMSE train: 4.815474	val: 8.976548	test: 7.304000
MAE train: 3.925866	val: 6.949950	test: 6.443216

Epoch: 2
Loss: 20.576452255249023
RMSE train: 4.868723	val: 8.989910	test: 7.375554
MAE train: 3.993275	val: 7.006843	test: 6.516317

Epoch: 3
Loss: 19.674617767333984
RMSE train: 4.883329	val: 8.983965	test: 7.409009
MAE train: 4.038403	val: 7.048169	test: 6.545150

Epoch: 4
Loss: 18.60774040222168
RMSE train: 4.898784	val: 9.035929	test: 7.488482
MAE train: 4.096793	val: 7.181799	test: 6.619526

Epoch: 5
Loss: 17.20754909515381
RMSE train: 4.877381	val: 9.087087	test: 7.558949
MAE train: 4.114331	val: 7.313857	test: 6.669939

Epoch: 6
Loss: 16.294082164764404
RMSE train: 4.740485	val: 9.103832	test: 7.598741
MAE train: 4.038147	val: 7.403278	test: 6.678484

Epoch: 7
Loss: 15.00107717514038
RMSE train: 4.556203	val: 9.049149	test: 7.535077
MAE train: 3.928064	val: 7.414995	test: 6.616159

Epoch: 8
Loss: 13.963127613067627
RMSE train: 4.348621	val: 8.807854	test: 7.313653
MAE train: 3.796779	val: 7.249426	test: 6.421215

Epoch: 9
Loss: 12.992877960205078
RMSE train: 4.120183	val: 8.419492	test: 7.018240
MAE train: 3.643719	val: 6.966303	test: 6.158363

Epoch: 10
Loss: 12.141418933868408
RMSE train: 3.875625	val: 7.986458	test: 6.746101
MAE train: 3.445710	val: 6.645136	test: 5.897979

Epoch: 11
Loss: 11.146225929260254
RMSE train: 3.645419	val: 7.522039	test: 6.448360
MAE train: 3.236622	val: 6.289547	test: 5.602407

Epoch: 12
Loss: 10.469887256622314
RMSE train: 3.451073	val: 7.021447	test: 6.136509
MAE train: 3.047681	val: 5.936236	test: 5.309401

Epoch: 13
Loss: 9.92671537399292
RMSE train: 3.329165	val: 6.617443	test: 5.847242
MAE train: 2.935571	val: 5.651407	test: 5.056893

Epoch: 14
Loss: 9.68485975265503
RMSE train: 3.252814	val: 6.340032	test: 5.595020
MAE train: 2.866356	val: 5.433378	test: 4.808167

Epoch: 15
Loss: 9.221409320831299
RMSE train: 3.172061	val: 6.131436	test: 5.381117
MAE train: 2.793074	val: 5.239212	test: 4.575532

Epoch: 16
Loss: 8.658214330673218
RMSE train: 3.134979	val: 6.057056	test: 5.261340
MAE train: 2.769218	val: 5.134528	test: 4.414582

Epoch: 17
Loss: 8.503103256225586
RMSE train: 3.102477	val: 6.037891	test: 5.186836
MAE train: 2.747047	val: 5.094953	test: 4.307884

Epoch: 18
Loss: 8.402132987976074
RMSE train: 3.055122	val: 6.085086	test: 5.145719
MAE train: 2.702278	val: 5.111650	test: 4.232115

Epoch: 19
Loss: 7.925175428390503
RMSE train: 3.012992	val: 6.152330	test: 5.111151
MAE train: 2.661187	val: 5.136981	test: 4.192900

Epoch: 20
Loss: 7.685174942016602
RMSE train: 2.961793	val: 6.221143	test: 5.077160
MAE train: 2.606275	val: 5.149440	test: 4.157230

Epoch: 21
Loss: 6.916447162628174
RMSE train: 2.913647	val: 6.260746	test: 5.061053
MAE train: 2.562374	val: 5.169605	test: 4.140078

Epoch: 22
Loss: 6.6828837394714355
RMSE train: 2.838916	val: 6.233641	test: 5.021329
MAE train: 2.492282	val: 5.139267	test: 4.117135Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/freesolv/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/freesolv/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/freesolv/scaff/train_prop=0.8/freesolv_scaff_2_20-05_14-43-15  ]
[ Using Seed :  2  ]
[ Using device :  cuda:2  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 21.84232521057129
RMSE train: 4.681015	val: 8.736232	test: 6.491121
MAE train: 3.804966	val: 6.653216	test: 5.502229

Epoch: 2
Loss: 20.80053997039795
RMSE train: 4.813828	val: 8.713378	test: 6.690716
MAE train: 3.975909	val: 6.610245	test: 5.755161

Epoch: 3
Loss: 19.69467544555664
RMSE train: 4.835560	val: 8.601523	test: 6.677787
MAE train: 4.017562	val: 6.482887	test: 5.746654

Epoch: 4
Loss: 18.30480670928955
RMSE train: 4.749226	val: 8.456896	test: 6.567228
MAE train: 3.950147	val: 6.348996	test: 5.632208

Epoch: 5
Loss: 17.377129554748535
RMSE train: 4.626488	val: 8.282282	test: 6.423451
MAE train: 3.881172	val: 6.207395	test: 5.481805

Epoch: 6
Loss: 16.093753814697266
RMSE train: 4.435436	val: 8.039830	test: 6.305232
MAE train: 3.760793	val: 6.052968	test: 5.368972

Epoch: 7
Loss: 14.528203964233398
RMSE train: 4.226292	val: 7.783578	test: 6.268117
MAE train: 3.643218	val: 5.932473	test: 5.360756

Epoch: 8
Loss: 13.69372034072876
RMSE train: 4.026348	val: 7.509046	test: 6.260691
MAE train: 3.540654	val: 5.795353	test: 5.384609

Epoch: 9
Loss: 12.551476955413818
RMSE train: 3.857896	val: 7.266071	test: 6.266133
MAE train: 3.434674	val: 5.698583	test: 5.417331

Epoch: 10
Loss: 12.256502151489258
RMSE train: 3.786676	val: 7.127664	test: 6.288690
MAE train: 3.397480	val: 5.728897	test: 5.464745

Epoch: 11
Loss: 11.486868858337402
RMSE train: 3.736406	val: 7.074345	test: 6.290650
MAE train: 3.390391	val: 5.824854	test: 5.481282

Epoch: 12
Loss: 10.594245910644531
RMSE train: 3.798277	val: 7.209400	test: 6.356006
MAE train: 3.486758	val: 6.049327	test: 5.567222

Epoch: 13
Loss: 10.32081413269043
RMSE train: 3.894130	val: 7.275700	test: 6.381743
MAE train: 3.567080	val: 6.104328	test: 5.603031

Epoch: 14
Loss: 9.828825950622559
RMSE train: 3.836527	val: 7.279624	test: 6.383996
MAE train: 3.513665	val: 6.111140	test: 5.607925

Epoch: 15
Loss: 9.541849136352539
RMSE train: 3.731684	val: 7.157652	test: 6.315235
MAE train: 3.421353	val: 5.993858	test: 5.532580

Epoch: 16
Loss: 8.806107997894287
RMSE train: 3.649491	val: 6.990892	test: 6.212545
MAE train: 3.337619	val: 5.825982	test: 5.439275

Epoch: 17
Loss: 8.759863376617432
RMSE train: 3.628717	val: 6.856181	test: 6.147632
MAE train: 3.312994	val: 5.730986	test: 5.388733

Epoch: 18
Loss: 8.118654727935791
RMSE train: 3.621644	val: 6.788981	test: 6.088081
MAE train: 3.304581	val: 5.724206	test: 5.340526

Epoch: 19
Loss: 8.057459831237793
RMSE train: 3.585266	val: 6.747408	test: 5.990742
MAE train: 3.264474	val: 5.733082	test: 5.252116

Epoch: 20
Loss: 7.444990873336792
RMSE train: 3.511702	val: 6.702619	test: 5.892517
MAE train: 3.185192	val: 5.710912	test: 5.155240

Epoch: 21
Loss: 7.26365852355957
RMSE train: 3.380353	val: 6.649531	test: 5.782387
MAE train: 3.051448	val: 5.658971	test: 5.047690

Epoch: 22
Loss: 6.827802896499634
RMSE train: 3.224776	val: 6.550936	test: 5.638739
MAE train: 2.898360	val: 5.546736	test: 4.909437

Epoch: 23
Loss: 4.3555920124053955
RMSE train: 2.626190	val: 3.668535	test: 5.505668
MAE train: 2.290654	val: 3.192419	test: 4.093314

Epoch: 24
Loss: 4.542383313179016
RMSE train: 2.574438	val: 3.568398	test: 5.453290
MAE train: 2.231079	val: 3.089616	test: 4.044309

Epoch: 25
Loss: 3.804090738296509
RMSE train: 2.453212	val: 3.422414	test: 5.344696
MAE train: 2.113531	val: 2.940296	test: 3.959529

Epoch: 26
Loss: 3.867253303527832
RMSE train: 2.285992	val: 3.260851	test: 5.222353
MAE train: 1.950459	val: 2.779637	test: 3.854083

Epoch: 27
Loss: 3.374886155128479
RMSE train: 2.170096	val: 3.119586	test: 5.133839
MAE train: 1.838189	val: 2.635901	test: 3.767204

Epoch: 28
Loss: 3.4431148767471313
RMSE train: 2.117626	val: 3.013832	test: 5.077269
MAE train: 1.798724	val: 2.524481	test: 3.727852

Epoch: 29
Loss: 3.4621015787124634
RMSE train: 2.122450	val: 2.871881	test: 5.024022
MAE train: 1.825066	val: 2.371098	test: 3.698762

Epoch: 30
Loss: 2.891553521156311
RMSE train: 2.094687	val: 2.691147	test: 4.957518
MAE train: 1.800179	val: 2.174977	test: 3.651292

Epoch: 31
Loss: 2.660576343536377
RMSE train: 2.057113	val: 2.588489	test: 4.913603
MAE train: 1.752207	val: 2.056394	test: 3.610575

Epoch: 32
Loss: 2.80495023727417
RMSE train: 2.017202	val: 2.537573	test: 4.875576
MAE train: 1.706951	val: 1.994819	test: 3.560584

Epoch: 33
Loss: 2.5028117895126343
RMSE train: 1.984912	val: 2.519929	test: 4.846035
MAE train: 1.675826	val: 1.971934	test: 3.518681

Epoch: 34
Loss: 2.9389606714248657
RMSE train: 1.984233	val: 2.577176	test: 4.872445
MAE train: 1.671595	val: 2.031956	test: 3.532775

Epoch: 35
Loss: 2.1989089250564575
RMSE train: 1.929635	val: 2.626434	test: 4.855524
MAE train: 1.610653	val: 2.093280	test: 3.529050

Epoch: 36
Loss: 1.967991054058075
RMSE train: 1.882314	val: 2.546584	test: 4.802615
MAE train: 1.562984	val: 2.022247	test: 3.509068

Epoch: 37
Loss: 2.293005406856537
RMSE train: 1.857144	val: 2.383668	test: 4.732671
MAE train: 1.548006	val: 1.856518	test: 3.467031

Epoch: 38
Loss: 1.962352216243744
RMSE train: 1.781247	val: 2.162553	test: 4.620952
MAE train: 1.488283	val: 1.622370	test: 3.387464

Epoch: 39
Loss: 1.641084909439087
RMSE train: 1.733067	val: 1.973019	test: 4.548076
MAE train: 1.459332	val: 1.418853	test: 3.358039

Epoch: 40
Loss: 1.6726285219192505
RMSE train: 1.676328	val: 1.863047	test: 4.492121
MAE train: 1.414831	val: 1.294913	test: 3.314158

Epoch: 41
Loss: 1.7311357855796814
RMSE train: 1.649435	val: 1.850040	test: 4.483567
MAE train: 1.394716	val: 1.262892	test: 3.283241

Epoch: 42
Loss: 1.744604766368866
RMSE train: 1.622641	val: 1.944768	test: 4.546074
MAE train: 1.359840	val: 1.345990	test: 3.332386

Epoch: 43
Loss: 1.6273878812789917
RMSE train: 1.598028	val: 2.076123	test: 4.650569
MAE train: 1.326129	val: 1.470378	test: 3.403674

Epoch: 44
Loss: 1.5763686895370483
RMSE train: 1.569617	val: 2.125003	test: 4.741807
MAE train: 1.279096	val: 1.512200	test: 3.473512

Epoch: 45
Loss: 1.327537178993225
RMSE train: 1.528616	val: 1.998717	test: 4.738025
MAE train: 1.237174	val: 1.400086	test: 3.488775

Epoch: 46
Loss: 1.3659828901290894
RMSE train: 1.468713	val: 1.809490	test: 4.643048
MAE train: 1.189800	val: 1.251848	test: 3.408257

Epoch: 47
Loss: 1.1456184387207031
RMSE train: 1.374376	val: 1.677825	test: 4.508571
MAE train: 1.111939	val: 1.170116	test: 3.290775

Epoch: 48
Loss: 1.2365281581878662
RMSE train: 1.278397	val: 1.632606	test: 4.435832
MAE train: 1.027929	val: 1.152047	test: 3.209726

Epoch: 49
Loss: 1.2498712539672852
RMSE train: 1.192809	val: 1.629241	test: 4.405135
MAE train: 0.954027	val: 1.143970	test: 3.145699

Epoch: 50
Loss: 1.3583040833473206
RMSE train: 1.154876	val: 1.631492	test: 4.386967
MAE train: 0.922641	val: 1.146382	test: 3.109587

Epoch: 51
Loss: 1.1791436076164246
RMSE train: 1.133744	val: 1.636627	test: 4.359794
MAE train: 0.903235	val: 1.143182	test: 3.075110

Epoch: 52
Loss: 1.2864009737968445
RMSE train: 1.147909	val: 1.663749	test: 4.369080
MAE train: 0.910536	val: 1.158603	test: 3.070551

Epoch: 53
Loss: 1.4477553963661194
RMSE train: 1.200097	val: 1.699052	test: 4.403115
MAE train: 0.948173	val: 1.181523	test: 3.093802

Epoch: 54
Loss: 1.1837072968482971
RMSE train: 1.263896	val: 1.694379	test: 4.403052
MAE train: 0.995162	val: 1.181050	test: 3.121156

Epoch: 55
Loss: 1.0929537415504456
RMSE train: 1.308888	val: 1.700252	test: 4.442667
MAE train: 1.008219	val: 1.196826	test: 3.182852

Epoch: 56
Loss: 1.40961354970932
RMSE train: 1.319147	val: 1.698803	test: 4.512776
MAE train: 0.999718	val: 1.218639	test: 3.270477

Epoch: 57
Loss: 1.1860955357551575
RMSE train: 1.266662	val: 1.677191	test: 4.515572
MAE train: 0.963312	val: 1.212538	test: 3.289091

Epoch: 58
Loss: 1.082534372806549
RMSE train: 1.185818	val: 1.647423	test: 4.478807
MAE train: 0.908364	val: 1.216294	test: 3.264874

Epoch: 59
Loss: 1.2027767896652222
RMSE train: 1.101807	val: 1.622042	test: 4.446694
MAE train: 0.855858	val: 1.207111	test: 3.219830

Epoch: 60
Loss: 1.1443706154823303
RMSE train: 1.008566	val: 1.619802	test: 4.339343
MAE train: 0.791115	val: 1.217096	test: 3.116701

Epoch: 61
Loss: 0.9864514768123627
RMSE train: 1.002391	val: 1.713423	test: 4.269287
MAE train: 0.775992	val: 1.336909	test: 3.045715

Epoch: 62
Loss: 1.1389372944831848
RMSE train: 1.030537	val: 1.734408	test: 4.274959
MAE train: 0.797185	val: 1.359083	test: 3.035766

Epoch: 63
Loss: 1.0343685150146484
RMSE train: 1.014539	val: 1.694657	test: 4.359957
MAE train: 0.793253	val: 1.318735	test: 3.086992

Epoch: 64
Loss: 0.9832381308078766
RMSE train: 1.052748	val: 1.694963	test: 4.457872
MAE train: 0.834412	val: 1.317607	test: 3.162979

Epoch: 65
Loss: 1.0612101256847382
RMSE train: 1.050285	val: 1.723800	test: 4.499505
MAE train: 0.837114	val: 1.339049	test: 3.188266

Epoch: 66
Loss: 0.9887359738349915
RMSE train: 1.007351	val: 1.834816	test: 4.479429
MAE train: 0.797464	val: 1.431067	test: 3.183656

Epoch: 67
Loss: 1.0069005489349365
RMSE train: 1.003738	val: 1.810234	test: 4.469996
MAE train: 0.788560	val: 1.413743	test: 3.193377

Epoch: 68
Loss: 1.0231016874313354
RMSE train: 0.939678	val: 1.747792	test: 4.421254
MAE train: 0.722008	val: 1.364635	test: 3.161930

Epoch: 69
Loss: 0.8978143334388733
RMSE train: 0.904990	val: 1.740142	test: 4.352915
MAE train: 0.682866	val: 1.367172	test: 3.118762

Epoch: 70
Loss: 1.0071003437042236
RMSE train: 0.888289	val: 1.745481	test: 4.287541
MAE train: 0.665403	val: 1.378086	test: 3.078466

Epoch: 71
Loss: 1.030874252319336
RMSE train: 0.877588	val: 1.773655	test: 4.219372
MAE train: 0.662075	val: 1.407267	test: 3.024369

Epoch: 72
Loss: 0.8929843604564667
RMSE train: 0.901557	val: 1.793964	test: 4.213711
MAE train: 0.685232	val: 1.427894	test: 3.011596

Epoch: 73
Loss: 1.041850596666336
RMSE train: 0.947509	val: 1.720369	test: 4.287959
MAE train: 0.726368	val: 1.359229	test: 3.053595

Epoch: 74
Loss: 0.8155567944049835
RMSE train: 0.998830	val: 1.660064	test: 4.343610
MAE train: 0.778433	val: 1.290092	test: 3.090084

Epoch: 75
Loss: 0.8341762125492096
RMSE train: 1.023223	val: 1.610628	test: 4.412794
MAE train: 0.795819	val: 1.228073	test: 3.134466

Epoch: 76
Loss: 0.8019796907901764
RMSE train: 1.028820	val: 1.601083	test: 4.459479
MAE train: 0.793336	val: 1.220064	test: 3.158043

Epoch: 77
Loss: 0.8497288823127747
RMSE train: 0.994814	val: 1.637852	test: 4.450829
MAE train: 0.756389	val: 1.269905	test: 3.141174

Epoch: 78
Loss: 0.9742212891578674
RMSE train: 0.989651	val: 1.729925	test: 4.425779
MAE train: 0.750450	val: 1.357196	test: 3.121044

Epoch: 79
Loss: 0.9971778392791748
RMSE train: 0.983853	val: 1.746189	test: 4.408137
MAE train: 0.756801	val: 1.364374	test: 3.115496

Epoch: 80
Loss: 0.8745824992656708
RMSE train: 1.018151	val: 1.747068	test: 4.410304
MAE train: 0.785859	val: 1.357859	test: 3.118157

Epoch: 81
Loss: 0.8060465157032013
RMSE train: 0.997392	val: 1.764588	test: 4.389910
MAE train: 0.764649	val: 1.370979	test: 3.105879

Epoch: 82
Loss: 0.8193947672843933
RMSE train: 0.959717	val: 1.719199	test: 4.416087
MAE train: 0.730896	val: 1.328405	test: 3.116072

Epoch: 83
Loss: 0.7277647852897644
RMSE train: 0.881825	val: 1.759342	test: 4.416964
MAE train: 0.664512	val: 1.374111	test: 3.107999

Epoch: 23
Loss: 4.135252952575684
RMSE train: 2.705135	val: 3.829404	test: 5.079580
MAE train: 2.419478	val: 3.446576	test: 3.894776

Epoch: 24
Loss: 4.670858144760132
RMSE train: 2.640887	val: 3.512039	test: 4.953950
MAE train: 2.368222	val: 3.120245	test: 3.834223

Epoch: 25
Loss: 4.098567008972168
RMSE train: 2.588323	val: 3.215091	test: 4.826507
MAE train: 2.328972	val: 2.804496	test: 3.757866

Epoch: 26
Loss: 3.9790456295013428
RMSE train: 2.524255	val: 2.898723	test: 4.637783
MAE train: 2.279484	val: 2.457692	test: 3.644970

Epoch: 27
Loss: 4.035181999206543
RMSE train: 2.470166	val: 2.672991	test: 4.479027
MAE train: 2.235544	val: 2.215151	test: 3.539930

Epoch: 28
Loss: 4.210767030715942
RMSE train: 2.459271	val: 2.672837	test: 4.490123
MAE train: 2.229889	val: 2.228884	test: 3.543539

Epoch: 29
Loss: 3.8311619758605957
RMSE train: 2.412218	val: 2.737230	test: 4.550011
MAE train: 2.179662	val: 2.312810	test: 3.566785

Epoch: 30
Loss: 3.456395149230957
RMSE train: 2.396207	val: 2.899877	test: 4.627335
MAE train: 2.158506	val: 2.499745	test: 3.600667

Epoch: 31
Loss: 3.26443874835968
RMSE train: 2.365651	val: 2.956113	test: 4.648766
MAE train: 2.128431	val: 2.564521	test: 3.615397

Epoch: 32
Loss: 2.88681161403656
RMSE train: 2.298058	val: 2.937852	test: 4.622659
MAE train: 2.060350	val: 2.544297	test: 3.592322

Epoch: 33
Loss: 2.481600761413574
RMSE train: 2.248806	val: 2.794766	test: 4.511996
MAE train: 2.019537	val: 2.383758	test: 3.497971

Epoch: 34
Loss: 3.0137126445770264
RMSE train: 2.189703	val: 2.696665	test: 4.466410
MAE train: 1.959786	val: 2.257952	test: 3.439892

Epoch: 35
Loss: 2.3578134179115295
RMSE train: 2.120953	val: 2.540674	test: 4.402338
MAE train: 1.886746	val: 2.063416	test: 3.373592

Epoch: 36
Loss: 2.5963481664657593
RMSE train: 2.094920	val: 2.451536	test: 4.405492
MAE train: 1.855124	val: 1.931501	test: 3.353399

Epoch: 37
Loss: 2.239376425743103
RMSE train: 2.009257	val: 2.472756	test: 4.449895
MAE train: 1.770400	val: 1.935523	test: 3.359329

Epoch: 38
Loss: 2.570636749267578
RMSE train: 1.905549	val: 2.597188	test: 4.516727
MAE train: 1.664336	val: 2.062517	test: 3.380856

Epoch: 39
Loss: 1.8254095315933228
RMSE train: 1.826663	val: 2.594762	test: 4.461752
MAE train: 1.585564	val: 2.067133	test: 3.311999

Epoch: 40
Loss: 1.7261309623718262
RMSE train: 1.772880	val: 2.431008	test: 4.312293
MAE train: 1.539971	val: 1.888607	test: 3.191977

Epoch: 41
Loss: 2.1010430455207825
RMSE train: 1.705636	val: 2.225246	test: 4.228571
MAE train: 1.476969	val: 1.639211	test: 3.150910

Epoch: 42
Loss: 1.967308521270752
RMSE train: 1.685928	val: 2.030883	test: 4.237429
MAE train: 1.458316	val: 1.409814	test: 3.188595

Epoch: 43
Loss: 1.6593610048294067
RMSE train: 1.645575	val: 1.850272	test: 4.128202
MAE train: 1.417808	val: 1.285530	test: 3.121088

Epoch: 44
Loss: 1.566821277141571
RMSE train: 1.657587	val: 1.814484	test: 4.132003
MAE train: 1.416117	val: 1.268768	test: 3.120282

Epoch: 45
Loss: 1.484587848186493
RMSE train: 1.607831	val: 1.832224	test: 4.144564
MAE train: 1.363884	val: 1.265782	test: 3.111641

Epoch: 46
Loss: 1.6247281432151794
RMSE train: 1.574564	val: 1.906447	test: 4.179773
MAE train: 1.328749	val: 1.319066	test: 3.103282

Epoch: 47
Loss: 1.7190030217170715
RMSE train: 1.530349	val: 1.948649	test: 4.165432
MAE train: 1.290525	val: 1.353456	test: 3.055568

Epoch: 48
Loss: 1.5455552339553833
RMSE train: 1.436968	val: 1.914444	test: 4.123932
MAE train: 1.208883	val: 1.321118	test: 2.996469

Epoch: 49
Loss: 1.45520681142807
RMSE train: 1.374342	val: 1.846156	test: 4.067409
MAE train: 1.158811	val: 1.263202	test: 2.937177

Epoch: 50
Loss: 1.4530749917030334
RMSE train: 1.296971	val: 1.783011	test: 4.004818
MAE train: 1.093749	val: 1.223765	test: 2.883054

Epoch: 51
Loss: 1.1668536067008972
RMSE train: 1.266052	val: 1.740878	test: 3.930525
MAE train: 1.069199	val: 1.219013	test: 2.851193

Epoch: 52
Loss: 1.1847160458564758
RMSE train: 1.269727	val: 1.780634	test: 3.863405
MAE train: 1.067763	val: 1.282401	test: 2.826872

Epoch: 53
Loss: 1.1628302037715912
RMSE train: 1.248061	val: 1.799031	test: 3.894420
MAE train: 1.040278	val: 1.318525	test: 2.862966

Epoch: 54
Loss: 1.056354582309723
RMSE train: 1.212647	val: 1.796979	test: 3.923908
MAE train: 0.999736	val: 1.333937	test: 2.885192

Epoch: 55
Loss: 1.346959412097931
RMSE train: 1.126762	val: 1.766969	test: 3.929912
MAE train: 0.929033	val: 1.299846	test: 2.884891

Epoch: 56
Loss: 1.1395955085754395
RMSE train: 1.062879	val: 1.708675	test: 4.047925
MAE train: 0.857601	val: 1.241376	test: 2.966537

Epoch: 57
Loss: 1.211955040693283
RMSE train: 1.004656	val: 1.678213	test: 4.067524
MAE train: 0.791829	val: 1.195400	test: 2.970309

Epoch: 58
Loss: 1.2501848936080933
RMSE train: 0.992664	val: 1.678476	test: 4.062972
MAE train: 0.771095	val: 1.184583	test: 2.953802

Epoch: 59
Loss: 1.2027502059936523
RMSE train: 1.045342	val: 1.685076	test: 4.055808
MAE train: 0.814147	val: 1.200921	test: 2.959293

Epoch: 60
Loss: 0.9612587988376617
RMSE train: 1.093409	val: 1.682783	test: 3.983621
MAE train: 0.855321	val: 1.239340	test: 2.925038

Epoch: 61
Loss: 1.157530277967453
RMSE train: 1.160726	val: 1.700432	test: 3.906206
MAE train: 0.901485	val: 1.292992	test: 2.882384

Epoch: 62
Loss: 0.9871584177017212
RMSE train: 1.177224	val: 1.691555	test: 3.861602
MAE train: 0.901456	val: 1.303031	test: 2.847862

Epoch: 63
Loss: 1.14732825756073
RMSE train: 1.136574	val: 1.644048	test: 3.929180
MAE train: 0.863210	val: 1.247823	test: 2.863019

Epoch: 64
Loss: 1.2459796071052551
RMSE train: 1.047305	val: 1.637568	test: 3.985788
MAE train: 0.800938	val: 1.247099	test: 2.890094

Epoch: 65
Loss: 1.1215225458145142
RMSE train: 0.929936	val: 1.639438	test: 3.923914
MAE train: 0.714707	val: 1.274875	test: 2.828388

Epoch: 66
Loss: 1.023006409406662
RMSE train: 0.903995	val: 1.657893	test: 3.891146
MAE train: 0.696510	val: 1.317121	test: 2.799354

Epoch: 67
Loss: 1.1868157386779785
RMSE train: 0.900254	val: 1.712958	test: 3.838645
MAE train: 0.704266	val: 1.387035	test: 2.770194

Epoch: 68
Loss: 1.0239413976669312
RMSE train: 0.905485	val: 1.696823	test: 3.811387
MAE train: 0.712025	val: 1.359206	test: 2.750528

Epoch: 69
Loss: 0.9110700190067291
RMSE train: 0.890988	val: 1.664495	test: 3.739206
MAE train: 0.704071	val: 1.307799	test: 2.696015

Epoch: 70
Loss: 1.0539775490760803
RMSE train: 0.892430	val: 1.620754	test: 3.701752
MAE train: 0.702292	val: 1.244495	test: 2.680764

Epoch: 71
Loss: 1.0314234793186188
RMSE train: 0.890320	val: 1.589604	test: 3.690541
MAE train: 0.695591	val: 1.195024	test: 2.693113

Epoch: 72
Loss: 1.1248042583465576
RMSE train: 0.862123	val: 1.573279	test: 3.687703
MAE train: 0.665767	val: 1.171352	test: 2.710760

Epoch: 73
Loss: 1.016879916191101
RMSE train: 0.839686	val: 1.575253	test: 3.634327
MAE train: 0.649989	val: 1.198903	test: 2.701955

Epoch: 74
Loss: 0.9936621785163879
RMSE train: 0.812859	val: 1.610612	test: 3.626944
MAE train: 0.627109	val: 1.256921	test: 2.718992

Epoch: 75
Loss: 0.950463593006134
RMSE train: 0.815832	val: 1.650331	test: 3.693313
MAE train: 0.622588	val: 1.306440	test: 2.771294

Epoch: 76
Loss: 0.8054127395153046
RMSE train: 0.901072	val: 1.666899	test: 3.859945
MAE train: 0.671726	val: 1.288162	test: 2.880141

Epoch: 77
Loss: 0.9390539228916168
RMSE train: 0.922863	val: 1.700685	test: 3.919438
MAE train: 0.681232	val: 1.266947	test: 2.916509

Epoch: 78
Loss: 1.1073583364486694
RMSE train: 0.921738	val: 1.706621	test: 3.934441
MAE train: 0.677191	val: 1.254707	test: 2.932763

Epoch: 79
Loss: 0.9447806477546692
RMSE train: 0.865345	val: 1.667408	test: 3.845213
MAE train: 0.642223	val: 1.230534	test: 2.882835

Epoch: 80
Loss: 0.8633129000663757
RMSE train: 0.771196	val: 1.643758	test: 3.680325
MAE train: 0.584269	val: 1.252346	test: 2.778601

Epoch: 81
Loss: 0.7725146114826202
RMSE train: 0.721169	val: 1.686926	test: 3.587665
MAE train: 0.544186	val: 1.333308	test: 2.734141

Epoch: 82
Loss: 0.9921761453151703
RMSE train: 0.680864	val: 1.772071	test: 3.500160
MAE train: 0.511628	val: 1.425420	test: 2.687067

Epoch: 83
Loss: 0.7904755473136902
RMSE train: 0.671073	val: 1.811037	test: 3.517720
MAE train: 0.499000	val: 1.470781	test: 2.711090

Epoch: 23
Loss: 4.284799098968506
RMSE train: 2.538579	val: 3.182522	test: 5.111117
MAE train: 2.228041	val: 2.656015	test: 3.919767

Epoch: 24
Loss: 4.712132453918457
RMSE train: 2.483433	val: 3.072031	test: 5.040991
MAE train: 2.173654	val: 2.550021	test: 3.868152

Epoch: 25
Loss: 4.544354200363159
RMSE train: 2.362929	val: 2.950312	test: 4.950063
MAE train: 2.054789	val: 2.431747	test: 3.781205

Epoch: 26
Loss: 3.8514238595962524
RMSE train: 2.261381	val: 2.863715	test: 4.873445
MAE train: 1.951530	val: 2.351274	test: 3.727389

Epoch: 27
Loss: 3.4661543369293213
RMSE train: 2.216606	val: 2.796060	test: 4.817705
MAE train: 1.905623	val: 2.275964	test: 3.700691

Epoch: 28
Loss: 3.363574504852295
RMSE train: 2.235089	val: 2.723236	test: 4.789382
MAE train: 1.927588	val: 2.186110	test: 3.709704

Epoch: 29
Loss: 3.2182228565216064
RMSE train: 2.246238	val: 2.594483	test: 4.715680
MAE train: 1.936662	val: 2.032106	test: 3.679139

Epoch: 30
Loss: 3.4047963619232178
RMSE train: 2.216366	val: 2.495473	test: 4.634195
MAE train: 1.906668	val: 1.912868	test: 3.622374

Epoch: 31
Loss: 2.9117331504821777
RMSE train: 2.125125	val: 2.441038	test: 4.625520
MAE train: 1.815402	val: 1.844348	test: 3.607105

Epoch: 32
Loss: 2.924027919769287
RMSE train: 2.040687	val: 2.427451	test: 4.632128
MAE train: 1.730932	val: 1.823221	test: 3.595082

Epoch: 33
Loss: 2.7470465898513794
RMSE train: 1.943894	val: 2.413670	test: 4.608637
MAE train: 1.635755	val: 1.810357	test: 3.546756

Epoch: 34
Loss: 2.50574791431427
RMSE train: 1.861697	val: 2.367945	test: 4.566654
MAE train: 1.562028	val: 1.770475	test: 3.485031

Epoch: 35
Loss: 2.580094575881958
RMSE train: 1.837311	val: 2.335123	test: 4.600971
MAE train: 1.544683	val: 1.729372	test: 3.485456

Epoch: 36
Loss: 2.270948052406311
RMSE train: 1.815870	val: 2.248268	test: 4.611898
MAE train: 1.523403	val: 1.638709	test: 3.468497

Epoch: 37
Loss: 2.244793713092804
RMSE train: 1.778105	val: 2.159802	test: 4.589871
MAE train: 1.473978	val: 1.543634	test: 3.441842

Epoch: 38
Loss: 2.255233645439148
RMSE train: 1.754336	val: 2.078165	test: 4.592780
MAE train: 1.452026	val: 1.448254	test: 3.460741

Epoch: 39
Loss: 1.9957559704780579
RMSE train: 1.666758	val: 1.895370	test: 4.482421
MAE train: 1.371648	val: 1.276554	test: 3.390531

Epoch: 40
Loss: 2.304518699645996
RMSE train: 1.605647	val: 1.785136	test: 4.404777
MAE train: 1.319639	val: 1.214807	test: 3.355287

Epoch: 41
Loss: 1.9106948375701904
RMSE train: 1.543280	val: 1.825186	test: 4.393530
MAE train: 1.260971	val: 1.212328	test: 3.373616

Epoch: 42
Loss: 1.7141826152801514
RMSE train: 1.500806	val: 1.925596	test: 4.408973
MAE train: 1.213084	val: 1.297043	test: 3.384455

Epoch: 43
Loss: 1.6494075655937195
RMSE train: 1.455850	val: 1.941521	test: 4.364522
MAE train: 1.166031	val: 1.312251	test: 3.304227

Epoch: 44
Loss: 1.7519919872283936
RMSE train: 1.410959	val: 1.929392	test: 4.335176
MAE train: 1.123484	val: 1.289448	test: 3.262174

Epoch: 45
Loss: 1.3918037414550781
RMSE train: 1.373470	val: 1.902624	test: 4.316169
MAE train: 1.083710	val: 1.252459	test: 3.225313

Epoch: 46
Loss: 1.5486998558044434
RMSE train: 1.339378	val: 1.876015	test: 4.318195
MAE train: 1.049669	val: 1.221880	test: 3.195383

Epoch: 47
Loss: 1.4060840010643005
RMSE train: 1.287169	val: 1.842452	test: 4.312092
MAE train: 1.008993	val: 1.205167	test: 3.159976

Epoch: 48
Loss: 1.4529067873954773
RMSE train: 1.250227	val: 1.798838	test: 4.301502
MAE train: 0.973452	val: 1.199795	test: 3.141883

Epoch: 49
Loss: 1.4466471076011658
RMSE train: 1.283048	val: 1.771687	test: 4.294541
MAE train: 0.987565	val: 1.217730	test: 3.137853

Epoch: 50
Loss: 1.2810860872268677
RMSE train: 1.280932	val: 1.741591	test: 4.256501
MAE train: 0.972900	val: 1.209070	test: 3.113786

Epoch: 51
Loss: 1.6915912330150604
RMSE train: 1.196914	val: 1.699819	test: 4.171358
MAE train: 0.905528	val: 1.168990	test: 3.058002

Epoch: 52
Loss: 1.2426337599754333
RMSE train: 1.086271	val: 1.659943	test: 4.073244
MAE train: 0.816240	val: 1.149833	test: 3.026240

Epoch: 53
Loss: 1.2686355113983154
RMSE train: 1.023451	val: 1.644294	test: 4.053082
MAE train: 0.761693	val: 1.136521	test: 3.048354

Epoch: 54
Loss: 1.3162466883659363
RMSE train: 0.965723	val: 1.627681	test: 4.060888
MAE train: 0.721853	val: 1.134059	test: 3.079165

Epoch: 55
Loss: 1.0631970167160034
RMSE train: 0.926128	val: 1.622781	test: 4.073496
MAE train: 0.705252	val: 1.140090	test: 3.090372

Epoch: 56
Loss: 1.3436970710754395
RMSE train: 0.943713	val: 1.626864	test: 4.086424
MAE train: 0.732621	val: 1.169517	test: 3.075671

Epoch: 57
Loss: 1.330155372619629
RMSE train: 1.018395	val: 1.635860	test: 4.114400
MAE train: 0.789258	val: 1.206046	test: 3.073585

Epoch: 58
Loss: 1.2329999208450317
RMSE train: 0.976458	val: 1.646420	test: 4.138104
MAE train: 0.736983	val: 1.234662	test: 3.087726

Epoch: 59
Loss: 1.1920486688613892
RMSE train: 0.913050	val: 1.651678	test: 4.163138
MAE train: 0.686068	val: 1.240035	test: 3.103279

Epoch: 60
Loss: 0.9506649971008301
RMSE train: 0.927557	val: 1.659329	test: 4.153859
MAE train: 0.696628	val: 1.244498	test: 3.091540

Epoch: 61
Loss: 1.3326963782310486
RMSE train: 0.961651	val: 1.673514	test: 4.124805
MAE train: 0.723872	val: 1.269234	test: 3.066137

Epoch: 62
Loss: 1.270791471004486
RMSE train: 0.904832	val: 1.668737	test: 4.004270
MAE train: 0.685016	val: 1.252664	test: 2.996599

Epoch: 63
Loss: 0.9441055655479431
RMSE train: 0.851101	val: 1.681897	test: 3.900941
MAE train: 0.644840	val: 1.254803	test: 2.942713

Epoch: 64
Loss: 1.1313983798027039
RMSE train: 0.787796	val: 1.668395	test: 3.800801
MAE train: 0.596601	val: 1.235133	test: 2.892073

Epoch: 65
Loss: 1.1084067821502686
RMSE train: 0.768405	val: 1.611921	test: 3.817673
MAE train: 0.582159	val: 1.175843	test: 2.900445

Epoch: 66
Loss: 1.0284196138381958
RMSE train: 0.758351	val: 1.608141	test: 3.861128
MAE train: 0.572922	val: 1.171738	test: 2.917062

Epoch: 67
Loss: 1.0046928524971008
RMSE train: 0.816148	val: 1.631356	test: 3.926555
MAE train: 0.611477	val: 1.221732	test: 2.930629

Epoch: 68
Loss: 0.9588236808776855
RMSE train: 0.829858	val: 1.671879	test: 3.976314
MAE train: 0.612099	val: 1.293650	test: 2.940959

Epoch: 69
Loss: 0.875540018081665
RMSE train: 0.812906	val: 1.676040	test: 3.999760
MAE train: 0.599375	val: 1.311780	test: 2.958950

Epoch: 70
Loss: 0.9375978410243988
RMSE train: 0.738416	val: 1.717182	test: 3.953009
MAE train: 0.554324	val: 1.354433	test: 2.936302

Epoch: 71
Loss: 0.9026225507259369
RMSE train: 0.682635	val: 1.738341	test: 3.913639
MAE train: 0.519652	val: 1.373589	test: 2.931393

Epoch: 72
Loss: 0.9616915881633759
RMSE train: 0.669696	val: 1.837787	test: 3.825396
MAE train: 0.512134	val: 1.449846	test: 2.887941

Epoch: 73
Loss: 0.9676951766014099
RMSE train: 0.670291	val: 1.890855	test: 3.795690
MAE train: 0.514269	val: 1.479089	test: 2.870858

Epoch: 74
Loss: 0.9014849960803986
RMSE train: 0.695426	val: 1.892491	test: 3.833881
MAE train: 0.529871	val: 1.482992	test: 2.876573

Epoch: 75
Loss: 0.807173103094101
RMSE train: 0.745091	val: 1.857977	test: 3.907804
MAE train: 0.559318	val: 1.478745	test: 2.908499

Epoch: 76
Loss: 0.9422150254249573
RMSE train: 0.781301	val: 1.773373	test: 3.979841
MAE train: 0.573750	val: 1.418194	test: 2.949604

Epoch: 77
Loss: 0.8165600001811981
RMSE train: 0.773328	val: 1.679730	test: 4.057821
MAE train: 0.557755	val: 1.304107	test: 3.004710

Epoch: 78
Loss: 0.8838402628898621
RMSE train: 0.757279	val: 1.630591	test: 4.100371
MAE train: 0.545233	val: 1.222686	test: 3.038997

Epoch: 79
Loss: 1.007360428571701
RMSE train: 0.763780	val: 1.609790	test: 4.072815
MAE train: 0.555501	val: 1.190218	test: 3.023355

Epoch: 80
Loss: 0.9961126446723938
RMSE train: 0.771216	val: 1.604795	test: 4.060715
MAE train: 0.568665	val: 1.180088	test: 3.019379

Epoch: 81
Loss: 0.8630451560020447
RMSE train: 0.745239	val: 1.627196	test: 4.013999
MAE train: 0.560577	val: 1.214800	test: 2.993485

Epoch: 82
Loss: 0.8567794263362885
RMSE train: 0.747602	val: 1.669107	test: 3.990388
MAE train: 0.563981	val: 1.254003	test: 2.982351

Epoch: 83
Loss: 0.8523175716400146
RMSE train: 0.721721	val: 1.728043	test: 3.974146
MAE train: 0.544114	val: 1.305132	test: 2.968515

Epoch: 23
Loss: 7.381638050079346
RMSE train: 3.121091	val: 4.263182	test: 4.625845
MAE train: 2.726373	val: 3.909445	test: 4.092998

Epoch: 24
Loss: 6.9650397300720215
RMSE train: 3.097552	val: 4.271241	test: 4.568037
MAE train: 2.707763	val: 3.922842	test: 4.012806

Epoch: 25
Loss: 7.013817071914673
RMSE train: 3.058384	val: 4.197931	test: 4.447992
MAE train: 2.668415	val: 3.846970	test: 3.840139

Epoch: 26
Loss: 6.860591888427734
RMSE train: 2.979421	val: 4.083413	test: 4.328560
MAE train: 2.588216	val: 3.721151	test: 3.654698

Epoch: 27
Loss: 6.273013114929199
RMSE train: 2.869128	val: 3.969357	test: 4.218295
MAE train: 2.481518	val: 3.593528	test: 3.485351

Epoch: 28
Loss: 6.217055797576904
RMSE train: 2.775668	val: 3.839459	test: 4.101501
MAE train: 2.392160	val: 3.450099	test: 3.349933

Epoch: 29
Loss: 5.684724569320679
RMSE train: 2.666147	val: 3.747082	test: 4.010162
MAE train: 2.285285	val: 3.349936	test: 3.215452

Epoch: 30
Loss: 5.456818342208862
RMSE train: 2.587501	val: 3.723904	test: 3.949249
MAE train: 2.199527	val: 3.318022	test: 3.120667

Epoch: 31
Loss: 5.230854749679565
RMSE train: 2.531193	val: 3.708479	test: 3.897523
MAE train: 2.132168	val: 3.294451	test: 3.063836

Epoch: 32
Loss: 5.066583871841431
RMSE train: 2.452370	val: 3.591297	test: 3.779795
MAE train: 2.054827	val: 3.178014	test: 2.938854

Epoch: 33
Loss: 4.976313829421997
RMSE train: 2.379493	val: 3.461699	test: 3.654349
MAE train: 1.987581	val: 3.051192	test: 2.811288

Epoch: 34
Loss: 4.732069253921509
RMSE train: 2.291738	val: 3.250067	test: 3.509676
MAE train: 1.908517	val: 2.839766	test: 2.676082

Epoch: 35
Loss: 4.495335578918457
RMSE train: 2.224726	val: 3.012878	test: 3.423499
MAE train: 1.848984	val: 2.589550	test: 2.630413

Epoch: 36
Loss: 4.2980523109436035
RMSE train: 2.136308	val: 2.831729	test: 3.404613
MAE train: 1.760636	val: 2.395500	test: 2.654652

Epoch: 37
Loss: 3.6838096380233765
RMSE train: 2.050771	val: 2.739402	test: 3.393259
MAE train: 1.684203	val: 2.296326	test: 2.643069

Epoch: 38
Loss: 3.7635496854782104
RMSE train: 2.003259	val: 2.685020	test: 3.373728
MAE train: 1.655785	val: 2.234558	test: 2.613565

Epoch: 39
Loss: 3.418607234954834
RMSE train: 1.958073	val: 2.592121	test: 3.318538
MAE train: 1.629343	val: 2.109517	test: 2.566258

Epoch: 40
Loss: 3.1790359020233154
RMSE train: 1.907138	val: 2.496349	test: 3.245793
MAE train: 1.585007	val: 1.981927	test: 2.490818

Epoch: 41
Loss: 3.1766971349716187
RMSE train: 1.870340	val: 2.485102	test: 3.210246
MAE train: 1.546746	val: 1.956288	test: 2.455159

Epoch: 42
Loss: 2.959685802459717
RMSE train: 1.827607	val: 2.465178	test: 3.171221
MAE train: 1.499992	val: 1.929853	test: 2.401767

Epoch: 43
Loss: 2.8553223609924316
RMSE train: 1.768161	val: 2.494371	test: 3.150444
MAE train: 1.421400	val: 1.969637	test: 2.353296

Epoch: 44
Loss: 2.589867949485779
RMSE train: 1.758147	val: 2.482101	test: 3.149666
MAE train: 1.406181	val: 1.958936	test: 2.362684

Epoch: 45
Loss: 2.5823278427124023
RMSE train: 1.765362	val: 2.349715	test: 3.108460
MAE train: 1.418058	val: 1.801391	test: 2.344756

Epoch: 46
Loss: 2.4648695588111877
RMSE train: 1.719806	val: 2.187412	test: 3.009566
MAE train: 1.383031	val: 1.609447	test: 2.279421

Epoch: 47
Loss: 2.1373844742774963
RMSE train: 1.626557	val: 2.123102	test: 2.934002
MAE train: 1.290676	val: 1.529108	test: 2.195103

Epoch: 48
Loss: 1.9301998019218445
RMSE train: 1.538247	val: 2.084599	test: 2.893299
MAE train: 1.204310	val: 1.492569	test: 2.148206

Epoch: 49
Loss: 2.1529638171195984
RMSE train: 1.473670	val: 2.019206	test: 2.872450
MAE train: 1.148374	val: 1.426220	test: 2.133865

Epoch: 50
Loss: 1.9981510639190674
RMSE train: 1.415741	val: 1.932748	test: 2.826199
MAE train: 1.104212	val: 1.335544	test: 2.082609

Epoch: 51
Loss: 2.310720384120941
RMSE train: 1.374158	val: 1.888597	test: 2.786056
MAE train: 1.073083	val: 1.294676	test: 2.043726

Epoch: 52
Loss: 1.7074233293533325
RMSE train: 1.317885	val: 1.827715	test: 2.749916
MAE train: 1.013391	val: 1.241831	test: 1.998714

Epoch: 53
Loss: 1.847125232219696
RMSE train: 1.258335	val: 1.737665	test: 2.692077
MAE train: 0.958201	val: 1.156999	test: 1.948173

Epoch: 54
Loss: 1.670517086982727
RMSE train: 1.253754	val: 1.722814	test: 2.650979
MAE train: 0.944383	val: 1.144518	test: 1.904319

Epoch: 55
Loss: 1.5857025384902954
RMSE train: 1.204778	val: 1.593675	test: 2.573959
MAE train: 0.919999	val: 1.028514	test: 1.885235

Epoch: 56
Loss: 1.6051547527313232
RMSE train: 1.138241	val: 1.614551	test: 2.585189
MAE train: 0.857886	val: 1.050111	test: 1.874282

Epoch: 57
Loss: 1.5374746322631836
RMSE train: 1.066704	val: 1.713650	test: 2.632232
MAE train: 0.782234	val: 1.142387	test: 1.879742

Epoch: 58
Loss: 1.7596776485443115
RMSE train: 1.045936	val: 1.761882	test: 2.660450
MAE train: 0.760216	val: 1.197511	test: 1.878213

Epoch: 59
Loss: 1.4340794086456299
RMSE train: 1.031793	val: 1.765437	test: 2.701926
MAE train: 0.747879	val: 1.206471	test: 1.913685

Epoch: 60
Loss: 1.8026947975158691
RMSE train: 1.072129	val: 1.694474	test: 2.670574
MAE train: 0.770784	val: 1.142121	test: 1.915489

Epoch: 61
Loss: 1.68596351146698
RMSE train: 1.110183	val: 1.625302	test: 2.599306
MAE train: 0.772020	val: 1.085176	test: 1.897846

Epoch: 62
Loss: 1.4214155673980713
RMSE train: 1.066476	val: 1.567192	test: 2.508042
MAE train: 0.732305	val: 1.038982	test: 1.814784

Epoch: 63
Loss: 1.5178342461585999
RMSE train: 1.013513	val: 1.559739	test: 2.467409
MAE train: 0.702337	val: 1.032788	test: 1.794669

Epoch: 64
Loss: 1.297167956829071
RMSE train: 0.940962	val: 1.515629	test: 2.452163
MAE train: 0.664184	val: 1.007658	test: 1.833661

Epoch: 65
Loss: 1.2728586792945862
RMSE train: 0.875136	val: 1.447696	test: 2.455486
MAE train: 0.630748	val: 1.008754	test: 1.879671

Epoch: 66
Loss: 1.4627422094345093
RMSE train: 0.853148	val: 1.423778	test: 2.490152
MAE train: 0.618705	val: 1.020463	test: 1.945219

Epoch: 67
Loss: 1.6876019835472107
RMSE train: 0.852937	val: 1.421122	test: 2.476652
MAE train: 0.614470	val: 0.997966	test: 1.924918

Epoch: 68
Loss: 1.5011627078056335
RMSE train: 0.867983	val: 1.422864	test: 2.414184
MAE train: 0.637229	val: 1.014255	test: 1.839372

Epoch: 69
Loss: 1.341700792312622
RMSE train: 0.956828	val: 1.418072	test: 2.408512
MAE train: 0.694598	val: 0.978683	test: 1.765879

Epoch: 70
Loss: 1.3284295201301575
RMSE train: 1.029853	val: 1.430656	test: 2.436267
MAE train: 0.732813	val: 0.947633	test: 1.757719

Epoch: 71
Loss: 1.4509259462356567
RMSE train: 1.028480	val: 1.431759	test: 2.447223
MAE train: 0.725288	val: 0.949059	test: 1.761721

Epoch: 72
Loss: 1.621224582195282
RMSE train: 0.909634	val: 1.391534	test: 2.420586
MAE train: 0.660514	val: 0.929987	test: 1.765142

Epoch: 73
Loss: 1.324605941772461
RMSE train: 0.825703	val: 1.406000	test: 2.430983
MAE train: 0.609072	val: 0.944730	test: 1.802465

Epoch: 74
Loss: 1.0710481107234955
RMSE train: 0.804181	val: 1.437614	test: 2.471971
MAE train: 0.594826	val: 0.956118	test: 1.842793

Epoch: 75
Loss: 1.022051215171814
RMSE train: 0.796259	val: 1.448042	test: 2.493822
MAE train: 0.581701	val: 0.966905	test: 1.852935

Epoch: 76
Loss: 1.113425076007843
RMSE train: 0.791363	val: 1.428978	test: 2.496125
MAE train: 0.575127	val: 0.954985	test: 1.859264

Epoch: 77
Loss: 1.315441370010376
RMSE train: 0.850938	val: 1.506927	test: 2.550561
MAE train: 0.615167	val: 1.002252	test: 1.892599

Epoch: 78
Loss: 1.1782094836235046
RMSE train: 0.889345	val: 1.577538	test: 2.600237
MAE train: 0.642235	val: 1.072976	test: 1.932100

Epoch: 79
Loss: 1.2219221591949463
RMSE train: 0.901498	val: 1.600764	test: 2.602299
MAE train: 0.650502	val: 1.106612	test: 1.929220

Epoch: 80
Loss: 0.9577777683734894
RMSE train: 0.938296	val: 1.543658	test: 2.568966
MAE train: 0.674290	val: 1.076816	test: 1.888794

Epoch: 81
Loss: 1.1496044397354126
RMSE train: 0.932895	val: 1.520414	test: 2.533381
MAE train: 0.666002	val: 1.059947	test: 1.848640

Epoch: 82
Loss: 1.0385538637638092
RMSE train: 0.902741	val: 1.517809	test: 2.503258
MAE train: 0.645189	val: 1.055186	test: 1.831549

Epoch: 83
Loss: 1.1075321435928345
RMSE train: 0.884394	val: 1.529913	test: 2.504400
MAE train: 0.643837	val: 1.047891	test: 1.880492

Epoch: 23
Loss: 6.976991653442383
RMSE train: 3.314742	val: 4.701437	test: 4.604679
MAE train: 2.952243	val: 4.296491	test: 3.949793

Epoch: 24
Loss: 6.8348424434661865
RMSE train: 3.247384	val: 4.558857	test: 4.582939
MAE train: 2.887638	val: 4.158809	test: 3.943271

Epoch: 25
Loss: 6.674261093139648
RMSE train: 3.179476	val: 4.412190	test: 4.484398
MAE train: 2.821788	val: 4.006409	test: 3.811629

Epoch: 26
Loss: 6.376443147659302
RMSE train: 3.122782	val: 4.297285	test: 4.338637
MAE train: 2.771030	val: 3.881656	test: 3.614966

Epoch: 27
Loss: 5.9094507694244385
RMSE train: 3.091873	val: 4.222062	test: 4.215720
MAE train: 2.741800	val: 3.791314	test: 3.477948

Epoch: 28
Loss: 5.70915675163269
RMSE train: 3.050770	val: 4.085399	test: 4.094046
MAE train: 2.707240	val: 3.634092	test: 3.349639

Epoch: 29
Loss: 5.6395103931427
RMSE train: 3.013106	val: 3.996364	test: 4.005575
MAE train: 2.679197	val: 3.534411	test: 3.251083

Epoch: 30
Loss: 5.3108062744140625
RMSE train: 2.942289	val: 3.931671	test: 3.934708
MAE train: 2.612228	val: 3.468496	test: 3.185899

Epoch: 31
Loss: 4.889001727104187
RMSE train: 2.859561	val: 3.880410	test: 3.890660
MAE train: 2.529084	val: 3.421305	test: 3.144784

Epoch: 32
Loss: 4.909008502960205
RMSE train: 2.794678	val: 3.847225	test: 3.891770
MAE train: 2.464507	val: 3.396487	test: 3.157932

Epoch: 33
Loss: 4.426594018936157
RMSE train: 2.760268	val: 3.776090	test: 3.918996
MAE train: 2.426058	val: 3.326563	test: 3.245440

Epoch: 34
Loss: 4.745894908905029
RMSE train: 2.709071	val: 3.645089	test: 3.871570
MAE train: 2.373935	val: 3.202254	test: 3.241073

Epoch: 35
Loss: 4.638568997383118
RMSE train: 2.688193	val: 3.486122	test: 3.788784
MAE train: 2.355564	val: 3.034672	test: 3.170968

Epoch: 36
Loss: 3.9713003635406494
RMSE train: 2.608090	val: 3.339395	test: 3.658650
MAE train: 2.288670	val: 2.874222	test: 3.012930

Epoch: 37
Loss: 3.477008581161499
RMSE train: 2.516367	val: 3.226050	test: 3.544711
MAE train: 2.191237	val: 2.740828	test: 2.829493

Epoch: 38
Loss: 3.498802661895752
RMSE train: 2.389080	val: 3.024617	test: 3.439167
MAE train: 2.052773	val: 2.521092	test: 2.695862

Epoch: 39
Loss: 3.2220538854599
RMSE train: 2.288695	val: 2.881526	test: 3.412304
MAE train: 1.936182	val: 2.364859	test: 2.672728

Epoch: 40
Loss: 3.1844005584716797
RMSE train: 2.147517	val: 2.715356	test: 3.266101
MAE train: 1.806701	val: 2.187431	test: 2.519070

Epoch: 41
Loss: 2.7548365592956543
RMSE train: 2.007793	val: 2.639420	test: 3.126241
MAE train: 1.675460	val: 2.100764	test: 2.383033

Epoch: 42
Loss: 2.697301149368286
RMSE train: 1.940827	val: 2.637393	test: 3.069737
MAE train: 1.599054	val: 2.093019	test: 2.326230

Epoch: 43
Loss: 2.7425448894500732
RMSE train: 1.892253	val: 2.617763	test: 3.090376
MAE train: 1.533352	val: 2.075696	test: 2.379098

Epoch: 44
Loss: 2.5582330226898193
RMSE train: 1.843855	val: 2.631040	test: 3.134130
MAE train: 1.468290	val: 2.092404	test: 2.446252

Epoch: 45
Loss: 2.3547136783599854
RMSE train: 1.784071	val: 2.641041	test: 3.139925
MAE train: 1.400850	val: 2.115330	test: 2.454962

Epoch: 46
Loss: 2.217450499534607
RMSE train: 1.710182	val: 2.554350	test: 3.077427
MAE train: 1.337152	val: 2.031876	test: 2.372891

Epoch: 47
Loss: 2.291476786136627
RMSE train: 1.681777	val: 2.431126	test: 3.003323
MAE train: 1.324475	val: 1.892014	test: 2.256548

Epoch: 48
Loss: 2.1052936911582947
RMSE train: 1.662258	val: 2.310410	test: 2.942787
MAE train: 1.317007	val: 1.749058	test: 2.176129

Epoch: 49
Loss: 1.8804043531417847
RMSE train: 1.636481	val: 2.156842	test: 2.838102
MAE train: 1.314278	val: 1.571343	test: 2.074881

Epoch: 50
Loss: 1.9984256029129028
RMSE train: 1.609041	val: 2.012925	test: 2.763850
MAE train: 1.299251	val: 1.417053	test: 2.034687

Epoch: 51
Loss: 1.7754970788955688
RMSE train: 1.541400	val: 1.908293	test: 2.704662
MAE train: 1.242279	val: 1.320261	test: 2.013684

Epoch: 52
Loss: 1.8148163557052612
RMSE train: 1.481406	val: 1.906447	test: 2.659816
MAE train: 1.190151	val: 1.325161	test: 1.992416

Epoch: 53
Loss: 1.6580893397331238
RMSE train: 1.403484	val: 1.872509	test: 2.627611
MAE train: 1.118062	val: 1.303525	test: 1.984103

Epoch: 54
Loss: 1.6376054883003235
RMSE train: 1.349412	val: 1.856709	test: 2.649831
MAE train: 1.064485	val: 1.295899	test: 2.017035

Epoch: 55
Loss: 1.6631629467010498
RMSE train: 1.299370	val: 1.881392	test: 2.693466
MAE train: 0.998811	val: 1.318813	test: 2.064502

Epoch: 56
Loss: 1.6282691955566406
RMSE train: 1.295676	val: 1.864888	test: 2.687143
MAE train: 0.986055	val: 1.299327	test: 2.023881

Epoch: 57
Loss: 1.595201313495636
RMSE train: 1.280417	val: 1.798816	test: 2.624121
MAE train: 0.960386	val: 1.237487	test: 1.924768

Epoch: 58
Loss: 1.4864162802696228
RMSE train: 1.208021	val: 1.695512	test: 2.504871
MAE train: 0.910906	val: 1.170182	test: 1.853509

Epoch: 59
Loss: 1.6039955615997314
RMSE train: 1.119060	val: 1.658889	test: 2.457703
MAE train: 0.837290	val: 1.152197	test: 1.852543

Epoch: 60
Loss: 1.360760748386383
RMSE train: 1.079056	val: 1.691783	test: 2.480583
MAE train: 0.799913	val: 1.160080	test: 1.887963

Epoch: 61
Loss: 1.5544467568397522
RMSE train: 1.039991	val: 1.667529	test: 2.471638
MAE train: 0.766195	val: 1.138091	test: 1.893712

Epoch: 62
Loss: 1.3842267394065857
RMSE train: 1.001480	val: 1.609425	test: 2.435418
MAE train: 0.732638	val: 1.117893	test: 1.847848

Epoch: 63
Loss: 1.5304147005081177
RMSE train: 1.041901	val: 1.588481	test: 2.426604
MAE train: 0.759893	val: 1.115183	test: 1.815338

Epoch: 64
Loss: 1.4447240233421326
RMSE train: 1.137405	val: 1.575467	test: 2.452065
MAE train: 0.815524	val: 1.120578	test: 1.840930

Epoch: 65
Loss: 1.3926750421524048
RMSE train: 1.180785	val: 1.571021	test: 2.439506
MAE train: 0.845234	val: 1.121437	test: 1.837328

Epoch: 66
Loss: 1.592219352722168
RMSE train: 1.122986	val: 1.591598	test: 2.440579
MAE train: 0.823967	val: 1.121244	test: 1.845036

Epoch: 67
Loss: 1.3387090563774109
RMSE train: 1.066477	val: 1.590598	test: 2.453033
MAE train: 0.802385	val: 1.116538	test: 1.878613

Epoch: 68
Loss: 1.3194634318351746
RMSE train: 1.002273	val: 1.568803	test: 2.475593
MAE train: 0.768384	val: 1.099067	test: 1.940161

Epoch: 69
Loss: 1.2751082181930542
RMSE train: 0.976176	val: 1.559769	test: 2.470631
MAE train: 0.750756	val: 1.098753	test: 1.944660

Epoch: 70
Loss: 1.2377843856811523
RMSE train: 0.983700	val: 1.544217	test: 2.435930
MAE train: 0.760763	val: 1.096334	test: 1.914050

Epoch: 71
Loss: 0.9957018494606018
RMSE train: 1.014506	val: 1.532834	test: 2.416764
MAE train: 0.796301	val: 1.101195	test: 1.895423

Epoch: 72
Loss: 1.236613690853119
RMSE train: 1.085426	val: 1.557872	test: 2.421021
MAE train: 0.855156	val: 1.132495	test: 1.877008

Epoch: 73
Loss: 1.2137037515640259
RMSE train: 1.085912	val: 1.597739	test: 2.437335
MAE train: 0.838975	val: 1.164951	test: 1.880348

Epoch: 74
Loss: 1.0093677937984467
RMSE train: 1.053484	val: 1.620537	test: 2.440838
MAE train: 0.777909	val: 1.183672	test: 1.877513

Epoch: 75
Loss: 1.0933093428611755
RMSE train: 1.009568	val: 1.610365	test: 2.447961
MAE train: 0.734767	val: 1.179758	test: 1.888687

Epoch: 76
Loss: 1.2356218099594116
RMSE train: 0.939066	val: 1.559397	test: 2.426835
MAE train: 0.690019	val: 1.150475	test: 1.884460

Epoch: 77
Loss: 1.0146876275539398
RMSE train: 0.891408	val: 1.516698	test: 2.413193
MAE train: 0.657315	val: 1.134639	test: 1.875856

Epoch: 78
Loss: 1.3626312017440796
RMSE train: 0.869945	val: 1.506742	test: 2.419638
MAE train: 0.638063	val: 1.153165	test: 1.899269

Epoch: 79
Loss: 1.1555418372154236
RMSE train: 0.886053	val: 1.496258	test: 2.408517
MAE train: 0.645474	val: 1.155924	test: 1.889673

Epoch: 80
Loss: 1.2127566933631897
RMSE train: 0.923139	val: 1.481365	test: 2.418910
MAE train: 0.648551	val: 1.128971	test: 1.869022

Epoch: 81
Loss: 1.0879147350788116
RMSE train: 0.963131	val: 1.500235	test: 2.426799
MAE train: 0.655278	val: 1.143156	test: 1.848224

Epoch: 82
Loss: 1.328778862953186
RMSE train: 0.900833	val: 1.491764	test: 2.387379
MAE train: 0.619385	val: 1.144151	test: 1.823984

Epoch: 83
Loss: 1.3391010165214539
RMSE train: 0.840928	val: 1.506426	test: 2.376638
MAE train: 0.593966	val: 1.154441	test: 1.835025

Epoch: 84
Loss: 0.7527945935726166
RMSE train: 0.678810	val: 1.847656	test: 3.581008
MAE train: 0.496799	val: 1.506931	test: 2.764925

Epoch: 85
Loss: 0.7719052731990814
RMSE train: 0.696076	val: 1.887839	test: 3.615065
MAE train: 0.506579	val: 1.545013	test: 2.788120

Epoch: 86
Loss: 0.869748055934906
RMSE train: 0.732098	val: 1.947435	test: 3.642102
MAE train: 0.526117	val: 1.591923	test: 2.804321

Epoch: 87
Loss: 0.9932548403739929
RMSE train: 0.751077	val: 2.007758	test: 3.634905
MAE train: 0.528055	val: 1.629610	test: 2.785401

Epoch: 88
Loss: 0.9293772578239441
RMSE train: 0.759740	val: 2.090473	test: 3.607201
MAE train: 0.527572	val: 1.691658	test: 2.756841

Epoch: 89
Loss: 0.7321646511554718
RMSE train: 0.706893	val: 2.011373	test: 3.569178
MAE train: 0.495788	val: 1.598526	test: 2.714371

Epoch: 90
Loss: 0.7617252469062805
RMSE train: 0.676087	val: 1.843417	test: 3.568260
MAE train: 0.481479	val: 1.432060	test: 2.699621

Epoch: 91
Loss: 0.7000328004360199
RMSE train: 0.656023	val: 1.821724	test: 3.529606
MAE train: 0.472662	val: 1.421112	test: 2.663309

Epoch: 92
Loss: 0.8963700830936432
RMSE train: 0.711939	val: 1.838146	test: 3.576442
MAE train: 0.527876	val: 1.457580	test: 2.695340

Epoch: 93
Loss: 0.7392539083957672
RMSE train: 0.806065	val: 1.907858	test: 3.572523
MAE train: 0.609452	val: 1.530388	test: 2.706546

Epoch: 94
Loss: 0.7949079275131226
RMSE train: 0.868084	val: 1.917959	test: 3.600334
MAE train: 0.654733	val: 1.548298	test: 2.731978

Epoch: 95
Loss: 0.9292636513710022
RMSE train: 0.874852	val: 1.869439	test: 3.686070
MAE train: 0.650187	val: 1.514476	test: 2.783675

Epoch: 96
Loss: 0.7494023740291595
RMSE train: 0.872859	val: 1.863102	test: 3.778464
MAE train: 0.633035	val: 1.512869	test: 2.825050

Epoch: 97
Loss: 0.8228472769260406
RMSE train: 0.846053	val: 1.960206	test: 3.724336
MAE train: 0.603646	val: 1.587695	test: 2.774286

Epoch: 98
Loss: 0.743209719657898
RMSE train: 0.828492	val: 2.038289	test: 3.671953
MAE train: 0.586753	val: 1.647660	test: 2.736691

Epoch: 99
Loss: 0.7961735129356384
RMSE train: 0.793690	val: 2.067120	test: 3.608578
MAE train: 0.566778	val: 1.666727	test: 2.694442

Epoch: 100
Loss: 0.6305616796016693
RMSE train: 0.774210	val: 2.083910	test: 3.531536
MAE train: 0.561536	val: 1.671904	test: 2.649728

Epoch: 101
Loss: 0.6966813206672668
RMSE train: 0.747691	val: 2.057947	test: 3.474676
MAE train: 0.555037	val: 1.640401	test: 2.614625

Epoch: 102
Loss: 0.7200763523578644
RMSE train: 0.709590	val: 1.930128	test: 3.493742
MAE train: 0.529006	val: 1.534667	test: 2.618249

Epoch: 103
Loss: 1.0577546954154968
RMSE train: 0.707778	val: 1.824423	test: 3.552828
MAE train: 0.521882	val: 1.449445	test: 2.648394

Epoch: 104
Loss: 0.7726821303367615
RMSE train: 0.777857	val: 1.780395	test: 3.616604
MAE train: 0.568181	val: 1.422664	test: 2.683343

Epoch: 105
Loss: 0.627426266670227
RMSE train: 0.811490	val: 1.799689	test: 3.615798
MAE train: 0.594121	val: 1.446250	test: 2.687881

Epoch: 106
Loss: 0.7575511038303375
RMSE train: 0.791299	val: 1.876253	test: 3.590234
MAE train: 0.574182	val: 1.520550	test: 2.675694

Epoch: 107
Loss: 0.9937258362770081
RMSE train: 0.731229	val: 2.051378	test: 3.505505
MAE train: 0.530357	val: 1.656942	test: 2.632383

Epoch: 108
Loss: 0.7553570568561554
RMSE train: 0.705035	val: 2.171741	test: 3.515086
MAE train: 0.511382	val: 1.755252	test: 2.665066

Epoch: 109
Loss: 0.7444666624069214
RMSE train: 0.749599	val: 2.147602	test: 3.579424
MAE train: 0.540310	val: 1.732327	test: 2.742123

Epoch: 110
Loss: 0.6908244490623474
RMSE train: 0.786775	val: 1.998238	test: 3.669339
MAE train: 0.575569	val: 1.594446	test: 2.826781

Epoch: 111
Loss: 0.7247608602046967
RMSE train: 0.824406	val: 1.831051	test: 3.771801
MAE train: 0.612911	val: 1.457781	test: 2.899676

Epoch: 112
Loss: 0.6140234768390656
RMSE train: 0.827052	val: 1.749314	test: 3.806650
MAE train: 0.619696	val: 1.380190	test: 2.916887

Epoch: 113
Loss: 0.7505153715610504
RMSE train: 0.793413	val: 1.742703	test: 3.741427
MAE train: 0.598746	val: 1.375264	test: 2.861867

Epoch: 114
Loss: 0.681037038564682
RMSE train: 0.721325	val: 1.776270	test: 3.631793
MAE train: 0.544416	val: 1.406823	test: 2.770401

Epoch: 115
Loss: 0.6181773841381073
RMSE train: 0.696567	val: 1.818924	test: 3.550534
MAE train: 0.524557	val: 1.451523	test: 2.710787

Epoch: 116
Loss: 0.9779723286628723
RMSE train: 0.724168	val: 1.776278	test: 3.579117
MAE train: 0.544338	val: 1.424826	test: 2.747604

Epoch: 117
Loss: 0.5843392014503479
RMSE train: 0.763542	val: 1.714016	test: 3.696420
MAE train: 0.568543	val: 1.381780	test: 2.841146

Epoch: 118
Loss: 0.6802597045898438
RMSE train: 0.795996	val: 1.680120	test: 3.754301
MAE train: 0.579830	val: 1.361527	test: 2.880885

Epoch: 119
Loss: 0.6516433954238892
RMSE train: 0.821764	val: 1.653296	test: 3.777166
MAE train: 0.585829	val: 1.346282	test: 2.890179

Epoch: 120
Loss: 0.7327847480773926
RMSE train: 0.815736	val: 1.673658	test: 3.724750
MAE train: 0.579992	val: 1.379267	test: 2.848057

Epoch: 121
Loss: 0.8468075692653656
RMSE train: 0.822376	val: 1.704275	test: 3.711614
MAE train: 0.584684	val: 1.416907	test: 2.823058

Early stopping
Best (RMSE):	 train: 0.862123	val: 1.573279	test: 3.687703
Best (MAE):	 train: 0.665767	val: 1.171352	test: 2.710760


Epoch: 23
Loss: 7.6113903522491455
RMSE train: 3.333579	val: 5.073530	test: 4.448419
MAE train: 2.961189	val: 4.648716	test: 3.525682

Epoch: 24
Loss: 7.338942050933838
RMSE train: 3.279384	val: 4.948437	test: 4.430851
MAE train: 2.908576	val: 4.509341	test: 3.523245

Epoch: 25
Loss: 6.723667621612549
RMSE train: 3.234679	val: 4.747083	test: 4.364183
MAE train: 2.864756	val: 4.278425	test: 3.491044

Epoch: 26
Loss: 6.718501329421997
RMSE train: 3.186217	val: 4.529960	test: 4.254708
MAE train: 2.823177	val: 4.042156	test: 3.450986

Epoch: 27
Loss: 6.633837699890137
RMSE train: 3.119748	val: 4.332781	test: 4.122386
MAE train: 2.765562	val: 3.831174	test: 3.332271

Epoch: 28
Loss: 6.025768280029297
RMSE train: 3.022228	val: 4.189191	test: 3.979705
MAE train: 2.685784	val: 3.679854	test: 3.150207

Epoch: 29
Loss: 5.628335952758789
RMSE train: 2.900391	val: 4.023769	test: 3.842914
MAE train: 2.576837	val: 3.513503	test: 2.950352

Epoch: 30
Loss: 5.723869562149048
RMSE train: 2.794221	val: 3.905950	test: 3.747538
MAE train: 2.473248	val: 3.411765	test: 2.856129

Epoch: 31
Loss: 5.166568279266357
RMSE train: 2.733924	val: 3.793085	test: 3.721939
MAE train: 2.415054	val: 3.317853	test: 2.845760

Epoch: 32
Loss: 5.21908712387085
RMSE train: 2.673066	val: 3.679347	test: 3.697574
MAE train: 2.357394	val: 3.223367	test: 2.848429

Epoch: 33
Loss: 4.896622896194458
RMSE train: 2.603373	val: 3.615126	test: 3.683601
MAE train: 2.288355	val: 3.177812	test: 2.859306

Epoch: 34
Loss: 4.523521184921265
RMSE train: 2.551849	val: 3.528970	test: 3.620630
MAE train: 2.242542	val: 3.096666	test: 2.790862

Epoch: 35
Loss: 4.154590010643005
RMSE train: 2.557281	val: 3.523641	test: 3.610943
MAE train: 2.237649	val: 3.099487	test: 2.744038

Epoch: 36
Loss: 3.9501588344573975
RMSE train: 2.539268	val: 3.465678	test: 3.574505
MAE train: 2.204046	val: 3.044704	test: 2.700293

Epoch: 37
Loss: 3.8154587745666504
RMSE train: 2.450636	val: 3.389784	test: 3.509995
MAE train: 2.097459	val: 2.970597	test: 2.631897

Epoch: 38
Loss: 3.5625826120376587
RMSE train: 2.345250	val: 3.231255	test: 3.399729
MAE train: 1.985647	val: 2.788857	test: 2.525304

Epoch: 39
Loss: 4.052931904792786
RMSE train: 2.245808	val: 3.039706	test: 3.253887
MAE train: 1.888255	val: 2.548943	test: 2.369133

Epoch: 40
Loss: 3.556573748588562
RMSE train: 2.084976	val: 2.820326	test: 3.109480
MAE train: 1.761329	val: 2.305695	test: 2.263685

Epoch: 41
Loss: 2.925618290901184
RMSE train: 1.992243	val: 2.661811	test: 3.064167
MAE train: 1.676766	val: 2.111520	test: 2.230214

Epoch: 42
Loss: 3.231847405433655
RMSE train: 1.933333	val: 2.653523	test: 3.071559
MAE train: 1.618020	val: 2.099551	test: 2.258684

Epoch: 43
Loss: 2.5906985998153687
RMSE train: 1.886056	val: 2.742463	test: 3.124729
MAE train: 1.570408	val: 2.217437	test: 2.325884

Epoch: 44
Loss: 2.4126545190811157
RMSE train: 1.877233	val: 2.867681	test: 3.206446
MAE train: 1.555593	val: 2.382278	test: 2.414490

Epoch: 45
Loss: 2.7353591918945312
RMSE train: 1.904976	val: 2.914654	test: 3.266422
MAE train: 1.587166	val: 2.448311	test: 2.480082

Epoch: 46
Loss: 2.533682703971863
RMSE train: 1.896112	val: 2.731558	test: 3.193418
MAE train: 1.592608	val: 2.255215	test: 2.401000

Epoch: 47
Loss: 2.2393059134483337
RMSE train: 1.855896	val: 2.534800	test: 3.090291
MAE train: 1.569741	val: 2.040241	test: 2.311617

Epoch: 48
Loss: 1.8939787149429321
RMSE train: 1.811018	val: 2.370886	test: 3.007853
MAE train: 1.514481	val: 1.847638	test: 2.223013

Epoch: 49
Loss: 1.7295323610305786
RMSE train: 1.771609	val: 2.285737	test: 2.978023
MAE train: 1.454126	val: 1.743373	test: 2.182659

Epoch: 50
Loss: 1.7624579668045044
RMSE train: 1.690050	val: 2.137693	test: 2.909150
MAE train: 1.356434	val: 1.579531	test: 2.126967

Epoch: 51
Loss: 1.6640785932540894
RMSE train: 1.587458	val: 2.043283	test: 2.832501
MAE train: 1.262376	val: 1.476573	test: 2.063038

Epoch: 52
Loss: 1.7936904430389404
RMSE train: 1.511446	val: 2.004141	test: 2.811051
MAE train: 1.189764	val: 1.429513	test: 2.059769

Epoch: 53
Loss: 2.3688429594039917
RMSE train: 1.440641	val: 1.986710	test: 2.812720
MAE train: 1.116781	val: 1.406898	test: 2.082676

Epoch: 54
Loss: 1.5762311816215515
RMSE train: 1.384338	val: 1.973836	test: 2.857707
MAE train: 1.054727	val: 1.398152	test: 2.141196

Epoch: 55
Loss: 1.8410941362380981
RMSE train: 1.359638	val: 1.964478	test: 2.903626
MAE train: 1.032806	val: 1.385661	test: 2.191542

Epoch: 56
Loss: 1.5542539954185486
RMSE train: 1.320164	val: 1.863458	test: 2.835731
MAE train: 1.011890	val: 1.271535	test: 2.108899

Epoch: 57
Loss: 1.3922085762023926
RMSE train: 1.295611	val: 1.747365	test: 2.725713
MAE train: 0.996276	val: 1.171000	test: 1.982713

Epoch: 58
Loss: 1.503831386566162
RMSE train: 1.314125	val: 1.671101	test: 2.685032
MAE train: 0.998251	val: 1.143766	test: 1.968402

Epoch: 59
Loss: 1.549130916595459
RMSE train: 1.226822	val: 1.625288	test: 2.667791
MAE train: 0.926359	val: 1.124344	test: 1.957036

Epoch: 60
Loss: 1.223709523677826
RMSE train: 1.112593	val: 1.611487	test: 2.715158
MAE train: 0.833837	val: 1.114670	test: 2.013003

Epoch: 61
Loss: 1.5166720151901245
RMSE train: 1.050631	val: 1.609691	test: 2.835243
MAE train: 0.779375	val: 1.106283	test: 2.119676

Epoch: 62
Loss: 1.1934819221496582
RMSE train: 1.023766	val: 1.559024	test: 2.830442
MAE train: 0.754344	val: 1.073236	test: 2.111334

Epoch: 63
Loss: 1.3328219652175903
RMSE train: 1.045732	val: 1.554020	test: 2.782147
MAE train: 0.773395	val: 1.068001	test: 2.062134

Epoch: 64
Loss: 1.2300028204917908
RMSE train: 1.078480	val: 1.568050	test: 2.711870
MAE train: 0.805000	val: 1.070567	test: 2.010104

Epoch: 65
Loss: 1.2677617073059082
RMSE train: 1.075103	val: 1.636604	test: 2.692882
MAE train: 0.792306	val: 1.105739	test: 1.974131

Epoch: 66
Loss: 1.1152238845825195
RMSE train: 1.060284	val: 1.611652	test: 2.619675
MAE train: 0.775019	val: 1.084188	test: 1.888700

Epoch: 67
Loss: 1.2608012557029724
RMSE train: 1.080104	val: 1.613064	test: 2.571086
MAE train: 0.780510	val: 1.089598	test: 1.837971

Epoch: 68
Loss: 1.3557729721069336
RMSE train: 1.060977	val: 1.575517	test: 2.550590
MAE train: 0.779988	val: 1.066998	test: 1.845259

Epoch: 69
Loss: 1.0739089250564575
RMSE train: 1.013670	val: 1.577010	test: 2.574017
MAE train: 0.747891	val: 1.058072	test: 1.905545

Epoch: 70
Loss: 1.2429940700531006
RMSE train: 1.007173	val: 1.610366	test: 2.626929
MAE train: 0.751289	val: 1.070769	test: 1.971325

Epoch: 71
Loss: 1.3154191374778748
RMSE train: 1.042747	val: 1.660371	test: 2.653267
MAE train: 0.775497	val: 1.113450	test: 1.993672

Epoch: 72
Loss: 1.12777179479599
RMSE train: 1.073128	val: 1.723503	test: 2.702517
MAE train: 0.807885	val: 1.172628	test: 2.027794

Epoch: 73
Loss: 1.053438127040863
RMSE train: 1.092161	val: 1.706981	test: 2.701200
MAE train: 0.828460	val: 1.157708	test: 2.037042

Epoch: 74
Loss: 1.172253131866455
RMSE train: 1.097562	val: 1.655246	test: 2.687274
MAE train: 0.834549	val: 1.111560	test: 2.029037

Epoch: 75
Loss: 1.1999477744102478
RMSE train: 1.129817	val: 1.582298	test: 2.668688
MAE train: 0.855939	val: 1.064551	test: 2.020616

Epoch: 76
Loss: 1.120163917541504
RMSE train: 1.128478	val: 1.586443	test: 2.695129
MAE train: 0.852910	val: 1.071465	test: 2.028656

Epoch: 77
Loss: 1.28107351064682
RMSE train: 1.128892	val: 1.637567	test: 2.738592
MAE train: 0.833823	val: 1.100571	test: 2.073230

Epoch: 78
Loss: 1.1442187428474426
RMSE train: 1.060952	val: 1.658425	test: 2.765430
MAE train: 0.777059	val: 1.117811	test: 2.088580

Epoch: 79
Loss: 1.3109822273254395
RMSE train: 0.949493	val: 1.531696	test: 2.701883
MAE train: 0.701672	val: 1.042102	test: 2.054303

Epoch: 80
Loss: 1.3428910970687866
RMSE train: 0.865160	val: 1.444014	test: 2.556781
MAE train: 0.647345	val: 1.021618	test: 1.957116

Epoch: 81
Loss: 1.0312145948410034
RMSE train: 0.870961	val: 1.434672	test: 2.463129
MAE train: 0.651777	val: 1.020418	test: 1.899325

Epoch: 82
Loss: 1.3340595364570618
RMSE train: 0.881367	val: 1.481196	test: 2.450219
MAE train: 0.657152	val: 1.016102	test: 1.880043

Epoch: 83
Loss: 1.3779123425483704
RMSE train: 0.874300	val: 1.528538	test: 2.490068
MAE train: 0.655496	val: 1.025733	test: 1.917927

Epoch: 23
Loss: 6.103999137878418
RMSE train: 2.921366	val: 6.156963	test: 5.232414
MAE train: 2.614752	val: 5.072820	test: 4.489975

Epoch: 24
Loss: 5.588663578033447
RMSE train: 2.850789	val: 6.139860	test: 5.192240
MAE train: 2.536512	val: 5.053889	test: 4.461927

Epoch: 25
Loss: 5.325965881347656
RMSE train: 2.718731	val: 6.046782	test: 5.099892
MAE train: 2.402263	val: 4.955965	test: 4.372805

Epoch: 26
Loss: 4.99708890914917
RMSE train: 2.628368	val: 5.964036	test: 5.031556
MAE train: 2.311129	val: 4.876862	test: 4.300853

Epoch: 27
Loss: 4.723263502120972
RMSE train: 2.552179	val: 5.876785	test: 4.983964
MAE train: 2.242723	val: 4.793915	test: 4.242971

Epoch: 28
Loss: 4.566166996955872
RMSE train: 2.454329	val: 5.783271	test: 4.918219
MAE train: 2.150238	val: 4.682594	test: 4.169839

Epoch: 29
Loss: 3.9680299758911133
RMSE train: 2.369911	val: 5.704134	test: 4.815638
MAE train: 2.068815	val: 4.621460	test: 4.064056

Epoch: 30
Loss: 3.953270673751831
RMSE train: 2.309518	val: 5.616373	test: 4.712120
MAE train: 2.014241	val: 4.541192	test: 3.954511

Epoch: 31
Loss: 3.686425805091858
RMSE train: 2.239586	val: 5.504827	test: 4.589289
MAE train: 1.950945	val: 4.431409	test: 3.830683

Epoch: 32
Loss: 3.554621458053589
RMSE train: 2.176004	val: 5.416645	test: 4.495189
MAE train: 1.884013	val: 4.339286	test: 3.752611

Epoch: 33
Loss: 3.2789214849472046
RMSE train: 2.106544	val: 5.330131	test: 4.421502
MAE train: 1.807301	val: 4.245357	test: 3.691317

Epoch: 34
Loss: 3.166399598121643
RMSE train: 2.018314	val: 5.203257	test: 4.325706
MAE train: 1.714286	val: 4.111376	test: 3.609228

Epoch: 35
Loss: 2.943615674972534
RMSE train: 1.991328	val: 5.171196	test: 4.345630
MAE train: 1.686317	val: 4.064776	test: 3.641656

Epoch: 36
Loss: 2.7941728830337524
RMSE train: 1.908125	val: 5.089928	test: 4.327242
MAE train: 1.607146	val: 3.965204	test: 3.624400

Epoch: 37
Loss: 2.6558170318603516
RMSE train: 1.808937	val: 4.974395	test: 4.275555
MAE train: 1.514595	val: 3.829338	test: 3.560709

Epoch: 38
Loss: 2.542606294155121
RMSE train: 1.642193	val: 4.755226	test: 4.085757
MAE train: 1.357032	val: 3.568425	test: 3.367256

Epoch: 39
Loss: 2.271757960319519
RMSE train: 1.446740	val: 4.557247	test: 3.895605
MAE train: 1.176382	val: 3.347497	test: 3.180746

Epoch: 40
Loss: 2.1625717878341675
RMSE train: 1.295795	val: 4.430065	test: 3.737309
MAE train: 1.034654	val: 3.188605	test: 2.999986

Epoch: 41
Loss: 2.0988181233406067
RMSE train: 1.252261	val: 4.416918	test: 3.698402
MAE train: 0.980139	val: 3.158236	test: 2.948283

Epoch: 42
Loss: 1.9951844215393066
RMSE train: 1.269339	val: 4.484129	test: 3.751928
MAE train: 0.988547	val: 3.219363	test: 3.011036

Epoch: 43
Loss: 1.9090039134025574
RMSE train: 1.293037	val: 4.543819	test: 3.833638
MAE train: 1.026510	val: 3.323608	test: 3.096457

Epoch: 44
Loss: 1.7469091415405273
RMSE train: 1.354928	val: 4.648561	test: 3.935089
MAE train: 1.081378	val: 3.445912	test: 3.194926

Epoch: 45
Loss: 1.8038138151168823
RMSE train: 1.366080	val: 4.681409	test: 3.944048
MAE train: 1.084531	val: 3.456241	test: 3.188171

Epoch: 46
Loss: 1.6765397191047668
RMSE train: 1.342418	val: 4.659332	test: 3.888018
MAE train: 1.053279	val: 3.400135	test: 3.106250

Epoch: 47
Loss: 1.7486132979393005
RMSE train: 1.246484	val: 4.529084	test: 3.762272
MAE train: 0.958907	val: 3.254635	test: 2.962734

Epoch: 48
Loss: 1.5873627066612244
RMSE train: 1.142470	val: 4.409265	test: 3.627472
MAE train: 0.859678	val: 3.137251	test: 2.817383

Epoch: 49
Loss: 1.4588657021522522
RMSE train: 1.069369	val: 4.346618	test: 3.545698
MAE train: 0.794651	val: 3.067912	test: 2.741542

Epoch: 50
Loss: 1.2211630940437317
RMSE train: 1.026397	val: 4.294112	test: 3.482427
MAE train: 0.763212	val: 3.032425	test: 2.688365

Epoch: 51
Loss: 1.4927127361297607
RMSE train: 1.000545	val: 4.265278	test: 3.429263
MAE train: 0.745339	val: 2.999440	test: 2.643236

Epoch: 52
Loss: 1.3344261050224304
RMSE train: 1.019480	val: 4.276059	test: 3.377382
MAE train: 0.759238	val: 3.024996	test: 2.590759

Epoch: 53
Loss: 1.3700388669967651
RMSE train: 0.972018	val: 4.212682	test: 3.339925
MAE train: 0.722884	val: 3.000560	test: 2.564353

Epoch: 54
Loss: 1.1831875443458557
RMSE train: 0.913229	val: 4.143600	test: 3.331714
MAE train: 0.675050	val: 2.965598	test: 2.578773

Epoch: 55
Loss: 1.2477184534072876
RMSE train: 0.892080	val: 4.103579	test: 3.350955
MAE train: 0.656752	val: 2.940241	test: 2.625330

Epoch: 56
Loss: 1.2376682758331299
RMSE train: 0.878782	val: 4.093542	test: 3.373255
MAE train: 0.649448	val: 2.921993	test: 2.662461

Epoch: 57
Loss: 1.1522709727287292
RMSE train: 0.870888	val: 4.130257	test: 3.399625
MAE train: 0.643983	val: 2.932589	test: 2.691565

Epoch: 58
Loss: 1.1996612548828125
RMSE train: 0.879873	val: 4.158881	test: 3.426672
MAE train: 0.650523	val: 2.943651	test: 2.717707

Epoch: 59
Loss: 1.1567353010177612
RMSE train: 0.860899	val: 4.146482	test: 3.405620
MAE train: 0.633159	val: 2.949317	test: 2.708358

Epoch: 60
Loss: 1.1229468286037445
RMSE train: 0.875724	val: 4.169841	test: 3.404665
MAE train: 0.645825	val: 2.974249	test: 2.699777

Epoch: 61
Loss: 1.0715619325637817
RMSE train: 0.902719	val: 4.212086	test: 3.374268
MAE train: 0.667162	val: 2.990407	test: 2.643336

Epoch: 62
Loss: 1.114248514175415
RMSE train: 0.935547	val: 4.292886	test: 3.353760
MAE train: 0.699268	val: 3.003232	test: 2.563187

Epoch: 63
Loss: 1.1513068079948425
RMSE train: 0.943277	val: 4.343141	test: 3.319383
MAE train: 0.701125	val: 3.029689	test: 2.491538

Epoch: 64
Loss: 1.0924170911312103
RMSE train: 0.918671	val: 4.355904	test: 3.275154
MAE train: 0.675599	val: 3.050306	test: 2.429305

Epoch: 65
Loss: 1.0393076539039612
RMSE train: 0.856782	val: 4.313905	test: 3.230537
MAE train: 0.617356	val: 3.038663	test: 2.404330

Epoch: 66
Loss: 1.1884605884552002
RMSE train: 0.793664	val: 4.266676	test: 3.215991
MAE train: 0.566866	val: 2.992112	test: 2.396572

Epoch: 67
Loss: 1.118403822183609
RMSE train: 0.746276	val: 4.233430	test: 3.229528
MAE train: 0.535054	val: 2.966475	test: 2.414764

Epoch: 68
Loss: 1.100851833820343
RMSE train: 0.782085	val: 4.274212	test: 3.271769
MAE train: 0.564385	val: 2.987140	test: 2.440733

Epoch: 69
Loss: 1.2647327184677124
RMSE train: 0.792546	val: 4.335216	test: 3.342885
MAE train: 0.575521	val: 3.067017	test: 2.541183

Epoch: 70
Loss: 0.9574883878231049
RMSE train: 0.818882	val: 4.396534	test: 3.449961
MAE train: 0.600502	val: 3.134015	test: 2.653465

Epoch: 71
Loss: 1.1085829436779022
RMSE train: 0.822447	val: 4.434594	test: 3.525964
MAE train: 0.605658	val: 3.175445	test: 2.720050

Epoch: 72
Loss: 0.9424595832824707
RMSE train: 0.816725	val: 4.429945	test: 3.524191
MAE train: 0.603363	val: 3.174558	test: 2.708451

Epoch: 73
Loss: 1.0024836659431458
RMSE train: 0.781348	val: 4.363019	test: 3.464009
MAE train: 0.579768	val: 3.121510	test: 2.657192

Epoch: 74
Loss: 0.9235391318798065
RMSE train: 0.795022	val: 4.320609	test: 3.395762
MAE train: 0.586720	val: 3.080516	test: 2.594972

Epoch: 75
Loss: 1.0674712657928467
RMSE train: 0.809222	val: 4.311023	test: 3.360176
MAE train: 0.592425	val: 3.069449	test: 2.568464

Epoch: 76
Loss: 0.9018145799636841
RMSE train: 0.777572	val: 4.301938	test: 3.343128
MAE train: 0.567108	val: 3.059876	test: 2.560034

Epoch: 77
Loss: 0.9890961647033691
RMSE train: 0.755165	val: 4.304378	test: 3.357681
MAE train: 0.549394	val: 3.042777	test: 2.580173

Epoch: 78
Loss: 1.0440218448638916
RMSE train: 0.761791	val: 4.312649	test: 3.367860
MAE train: 0.553682	val: 3.040417	test: 2.592438

Epoch: 79
Loss: 0.9022782444953918
RMSE train: 0.759961	val: 4.306017	test: 3.369965
MAE train: 0.548422	val: 3.017085	test: 2.590705

Epoch: 80
Loss: 1.0011675357818604
RMSE train: 0.784135	val: 4.335956	test: 3.384617
MAE train: 0.559900	val: 3.020516	test: 2.590609

Epoch: 81
Loss: 1.0878941416740417
RMSE train: 0.814830	val: 4.364865	test: 3.406254
MAE train: 0.587543	val: 3.019661	test: 2.597825

Epoch: 82
Loss: 0.8303406834602356
RMSE train: 0.795690	val: 4.342249	test: 3.394185
MAE train: 0.581131	val: 3.000050	test: 2.580491

Epoch: 83
Loss: 0.885212779045105
RMSE train: 0.743263	val: 4.296146	test: 3.367620
MAE train: 0.553655	val: 2.996494	test: 2.564853

Epoch: 23
Loss: 6.282152414321899
RMSE train: 2.793900	val: 6.216613	test: 5.018349
MAE train: 2.453796	val: 5.156869	test: 4.144987

Epoch: 24
Loss: 5.940712928771973
RMSE train: 2.739556	val: 6.202840	test: 5.012003
MAE train: 2.398135	val: 5.183860	test: 4.165198

Epoch: 25
Loss: 5.659047365188599
RMSE train: 2.629571	val: 6.053137	test: 4.916492
MAE train: 2.290726	val: 5.047932	test: 4.098397

Epoch: 26
Loss: 5.4644575119018555
RMSE train: 2.543174	val: 5.913130	test: 4.819831
MAE train: 2.203731	val: 4.911874	test: 4.019701

Epoch: 27
Loss: 5.1119911670684814
RMSE train: 2.462835	val: 5.741545	test: 4.694161
MAE train: 2.123672	val: 4.718262	test: 3.904099

Epoch: 28
Loss: 4.842456340789795
RMSE train: 2.364866	val: 5.607030	test: 4.590844
MAE train: 2.019897	val: 4.563440	test: 3.802945

Epoch: 29
Loss: 4.603210210800171
RMSE train: 2.300426	val: 5.549534	test: 4.510383
MAE train: 1.947644	val: 4.460796	test: 3.725661

Epoch: 30
Loss: 4.264642953872681
RMSE train: 2.240332	val: 5.510845	test: 4.415968
MAE train: 1.886798	val: 4.378638	test: 3.636438

Epoch: 31
Loss: 4.283216238021851
RMSE train: 2.151446	val: 5.416142	test: 4.306406
MAE train: 1.796147	val: 4.196492	test: 3.506942

Epoch: 32
Loss: 3.7559040784835815
RMSE train: 2.074762	val: 5.320360	test: 4.246019
MAE train: 1.736765	val: 4.079500	test: 3.441589

Epoch: 33
Loss: 3.6513336896896362
RMSE train: 2.029739	val: 5.288125	test: 4.237177
MAE train: 1.711685	val: 4.074427	test: 3.435097

Epoch: 34
Loss: 3.4400299787521362
RMSE train: 1.969478	val: 5.241514	test: 4.205255
MAE train: 1.669756	val: 4.030746	test: 3.399934

Epoch: 35
Loss: 3.263115167617798
RMSE train: 1.867813	val: 5.116117	test: 4.109822
MAE train: 1.574436	val: 3.867170	test: 3.295196

Epoch: 36
Loss: 3.131851315498352
RMSE train: 1.791167	val: 5.005274	test: 4.034273
MAE train: 1.497367	val: 3.746658	test: 3.213724

Epoch: 37
Loss: 2.9331010580062866
RMSE train: 1.727990	val: 4.970723	test: 3.985438
MAE train: 1.434841	val: 3.703159	test: 3.158691

Epoch: 38
Loss: 2.7021615505218506
RMSE train: 1.684170	val: 4.966617	test: 3.955817
MAE train: 1.391699	val: 3.676200	test: 3.109526

Epoch: 39
Loss: 2.5278674364089966
RMSE train: 1.653325	val: 4.961085	test: 3.948093
MAE train: 1.350477	val: 3.643473	test: 3.085872

Epoch: 40
Loss: 2.3555006980895996
RMSE train: 1.612691	val: 4.906419	test: 3.935055
MAE train: 1.303135	val: 3.576401	test: 3.079918

Epoch: 41
Loss: 2.109166979789734
RMSE train: 1.574516	val: 4.839329	test: 3.910055
MAE train: 1.263787	val: 3.535793	test: 3.065765

Epoch: 42
Loss: 2.1973633766174316
RMSE train: 1.539471	val: 4.795042	test: 3.876616
MAE train: 1.227911	val: 3.505913	test: 3.031178

Epoch: 43
Loss: 2.0808435678482056
RMSE train: 1.493598	val: 4.724211	test: 3.832529
MAE train: 1.190358	val: 3.443920	test: 2.996740

Epoch: 44
Loss: 1.816394031047821
RMSE train: 1.426017	val: 4.662788	test: 3.771502
MAE train: 1.131219	val: 3.380809	test: 2.943150

Epoch: 45
Loss: 1.9957613348960876
RMSE train: 1.368675	val: 4.616244	test: 3.710769
MAE train: 1.064243	val: 3.280933	test: 2.873367

Epoch: 46
Loss: 1.7359064221382141
RMSE train: 1.341758	val: 4.605468	test: 3.676134
MAE train: 1.028507	val: 3.253762	test: 2.841387

Epoch: 47
Loss: 1.9256962537765503
RMSE train: 1.336312	val: 4.625333	test: 3.662062
MAE train: 1.010189	val: 3.285682	test: 2.830381

Epoch: 48
Loss: 1.5613358616828918
RMSE train: 1.300182	val: 4.639755	test: 3.620686
MAE train: 0.972244	val: 3.247801	test: 2.768699

Epoch: 49
Loss: 1.3758766055107117
RMSE train: 1.228806	val: 4.578409	test: 3.554661
MAE train: 0.915598	val: 3.161588	test: 2.677122

Epoch: 50
Loss: 1.5002813935279846
RMSE train: 1.176512	val: 4.523358	test: 3.515544
MAE train: 0.870243	val: 3.110216	test: 2.631084

Epoch: 51
Loss: 1.471121370792389
RMSE train: 1.144752	val: 4.506749	test: 3.487841
MAE train: 0.847136	val: 3.117690	test: 2.600664

Epoch: 52
Loss: 1.3762848377227783
RMSE train: 1.123876	val: 4.474604	test: 3.469850
MAE train: 0.837008	val: 3.119323	test: 2.594686

Epoch: 53
Loss: 1.2994319200515747
RMSE train: 1.103885	val: 4.432970	test: 3.444558
MAE train: 0.823664	val: 3.137307	test: 2.581241

Epoch: 54
Loss: 1.4091538786888123
RMSE train: 1.043787	val: 4.364473	test: 3.380685
MAE train: 0.775723	val: 3.048367	test: 2.502160

Epoch: 55
Loss: 1.2035247683525085
RMSE train: 1.000546	val: 4.282528	test: 3.328432
MAE train: 0.741063	val: 2.966822	test: 2.441837

Epoch: 56
Loss: 1.4606711864471436
RMSE train: 0.974697	val: 4.219256	test: 3.311206
MAE train: 0.721850	val: 2.924429	test: 2.421235

Epoch: 57
Loss: 1.1792247891426086
RMSE train: 0.969823	val: 4.169087	test: 3.320847
MAE train: 0.720640	val: 2.915367	test: 2.447521

Epoch: 58
Loss: 1.1158949136734009
RMSE train: 0.981115	val: 4.182403	test: 3.336073
MAE train: 0.726212	val: 2.989736	test: 2.482330

Epoch: 59
Loss: 1.1668800711631775
RMSE train: 0.970713	val: 4.228669	test: 3.325393
MAE train: 0.718257	val: 3.015276	test: 2.458686

Epoch: 60
Loss: 1.132224977016449
RMSE train: 0.970066	val: 4.282896	test: 3.298310
MAE train: 0.722264	val: 3.000581	test: 2.404019

Epoch: 61
Loss: 1.166135549545288
RMSE train: 0.958644	val: 4.286129	test: 3.271703
MAE train: 0.713028	val: 2.957870	test: 2.357231

Epoch: 62
Loss: 1.148074209690094
RMSE train: 0.895681	val: 4.237676	test: 3.239300
MAE train: 0.663725	val: 2.896644	test: 2.336173

Epoch: 63
Loss: 1.1895835995674133
RMSE train: 0.871676	val: 4.246826	test: 3.221419
MAE train: 0.640619	val: 2.885692	test: 2.337756

Epoch: 64
Loss: 1.1779008507728577
RMSE train: 0.870005	val: 4.225892	test: 3.216576
MAE train: 0.636378	val: 2.883767	test: 2.346795

Epoch: 65
Loss: 1.203884482383728
RMSE train: 0.887436	val: 4.230300	test: 3.242672
MAE train: 0.647888	val: 2.929310	test: 2.382975

Epoch: 66
Loss: 1.0775776505470276
RMSE train: 0.945655	val: 4.264204	test: 3.298736
MAE train: 0.692447	val: 2.999032	test: 2.438178

Epoch: 67
Loss: 1.0735111832618713
RMSE train: 0.954595	val: 4.229505	test: 3.300142
MAE train: 0.711126	val: 2.952679	test: 2.440120

Epoch: 68
Loss: 1.0094938576221466
RMSE train: 0.963434	val: 4.191134	test: 3.280833
MAE train: 0.729320	val: 2.880245	test: 2.416051

Epoch: 69
Loss: 1.1585623025894165
RMSE train: 0.938179	val: 4.149291	test: 3.251584
MAE train: 0.708768	val: 2.840087	test: 2.377533

Epoch: 70
Loss: 1.1619600653648376
RMSE train: 0.922175	val: 4.131013	test: 3.247443
MAE train: 0.689197	val: 2.842936	test: 2.367517

Epoch: 71
Loss: 1.005663812160492
RMSE train: 0.895721	val: 4.085246	test: 3.238409
MAE train: 0.647931	val: 2.824264	test: 2.370876

Epoch: 72
Loss: 1.0537614226341248
RMSE train: 0.884188	val: 4.082630	test: 3.228813
MAE train: 0.632774	val: 2.794349	test: 2.358589

Epoch: 73
Loss: 1.093359351158142
RMSE train: 0.917039	val: 4.131585	test: 3.225827
MAE train: 0.665622	val: 2.776339	test: 2.339469

Epoch: 74
Loss: 1.0917375683784485
RMSE train: 0.936725	val: 4.126123	test: 3.209541
MAE train: 0.681616	val: 2.743754	test: 2.324248

Epoch: 75
Loss: 1.0971534252166748
RMSE train: 0.892272	val: 4.075169	test: 3.193414
MAE train: 0.640690	val: 2.719204	test: 2.323470

Epoch: 76
Loss: 1.2291355431079865
RMSE train: 0.849740	val: 4.131185	test: 3.210453
MAE train: 0.613282	val: 2.834160	test: 2.377557

Epoch: 77
Loss: 0.96878781914711
RMSE train: 0.848444	val: 4.208051	test: 3.229066
MAE train: 0.617030	val: 2.934164	test: 2.427993

Epoch: 78
Loss: 1.0639943480491638
RMSE train: 0.843060	val: 4.247303	test: 3.201227
MAE train: 0.613779	val: 2.950068	test: 2.393234

Epoch: 79
Loss: 1.056160569190979
RMSE train: 0.840570	val: 4.268517	test: 3.175504
MAE train: 0.611269	val: 2.939058	test: 2.346933

Epoch: 80
Loss: 0.9091504812240601
RMSE train: 0.812263	val: 4.266307	test: 3.173276
MAE train: 0.599931	val: 2.937377	test: 2.332261

Epoch: 81
Loss: 0.9182097017765045
RMSE train: 0.718635	val: 4.192023	test: 3.152333
MAE train: 0.525092	val: 2.884133	test: 2.322796

Epoch: 82
Loss: 0.9487998485565186
RMSE train: 0.649553	val: 4.114232	test: 3.134931
MAE train: 0.477167	val: 2.809535	test: 2.315396

Epoch: 83
Loss: 1.0010119676589966
RMSE train: 0.632622	val: 4.045620	test: 3.103352
MAE train: 0.461590	val: 2.729318	test: 2.294621

Epoch: 23
Loss: 6.573547124862671
RMSE train: 3.023688	val: 6.343153	test: 5.436235
MAE train: 2.706213	val: 5.313936	test: 4.712864

Epoch: 24
Loss: 6.043626070022583
RMSE train: 2.874332	val: 6.182518	test: 5.265308
MAE train: 2.565092	val: 5.112519	test: 4.540833

Epoch: 25
Loss: 5.743322134017944
RMSE train: 2.741964	val: 5.997077	test: 5.107994
MAE train: 2.443097	val: 4.888659	test: 4.398044

Epoch: 26
Loss: 5.526376008987427
RMSE train: 2.698788	val: 5.912266	test: 5.013379
MAE train: 2.404293	val: 4.813136	test: 4.310142

Epoch: 27
Loss: 5.150288820266724
RMSE train: 2.715629	val: 5.880762	test: 4.967537
MAE train: 2.425712	val: 4.812407	test: 4.277247

Epoch: 28
Loss: 5.133105754852295
RMSE train: 2.698494	val: 5.834560	test: 4.925594
MAE train: 2.407037	val: 4.770326	test: 4.246140

Epoch: 29
Loss: 4.635390996932983
RMSE train: 2.659169	val: 5.805600	test: 4.881296
MAE train: 2.361508	val: 4.721112	test: 4.203048

Epoch: 30
Loss: 4.423023223876953
RMSE train: 2.612023	val: 5.731450	test: 4.822563
MAE train: 2.315195	val: 4.631809	test: 4.145478

Epoch: 31
Loss: 4.157275438308716
RMSE train: 2.524632	val: 5.614201	test: 4.739541
MAE train: 2.235262	val: 4.490980	test: 4.061945

Epoch: 32
Loss: 4.065409064292908
RMSE train: 2.431637	val: 5.431501	test: 4.596583
MAE train: 2.155409	val: 4.283546	test: 3.921656

Epoch: 33
Loss: 3.4780633449554443
RMSE train: 2.354520	val: 5.270583	test: 4.479028
MAE train: 2.090262	val: 4.105420	test: 3.805450

Epoch: 34
Loss: 3.4474986791610718
RMSE train: 2.285795	val: 5.144181	test: 4.362199
MAE train: 2.030196	val: 3.953856	test: 3.683031

Epoch: 35
Loss: 3.2036339044570923
RMSE train: 2.271014	val: 5.059043	test: 4.268837
MAE train: 2.013134	val: 3.857369	test: 3.584161

Epoch: 36
Loss: 3.1530498266220093
RMSE train: 2.302153	val: 5.063987	test: 4.244541
MAE train: 2.034699	val: 3.850581	test: 3.549443

Epoch: 37
Loss: 2.705227851867676
RMSE train: 2.267454	val: 5.097113	test: 4.244693
MAE train: 1.986710	val: 3.830755	test: 3.528664

Epoch: 38
Loss: 2.7515532970428467
RMSE train: 2.236425	val: 5.109669	test: 4.240339
MAE train: 1.951533	val: 3.785949	test: 3.515956

Epoch: 39
Loss: 2.5765767097473145
RMSE train: 2.134901	val: 5.027066	test: 4.170869
MAE train: 1.851018	val: 3.678446	test: 3.440539

Epoch: 40
Loss: 2.3047523498535156
RMSE train: 2.002343	val: 4.893259	test: 4.049813
MAE train: 1.712514	val: 3.530752	test: 3.297544

Epoch: 41
Loss: 2.185442566871643
RMSE train: 1.874652	val: 4.749753	test: 3.925130
MAE train: 1.586363	val: 3.384914	test: 3.154752

Epoch: 42
Loss: 1.951127529144287
RMSE train: 1.698522	val: 4.557889	test: 3.783722
MAE train: 1.423746	val: 3.208722	test: 3.001387

Epoch: 43
Loss: 1.975189983844757
RMSE train: 1.545456	val: 4.459543	test: 3.684997
MAE train: 1.284042	val: 3.134946	test: 2.900083

Epoch: 44
Loss: 2.0812684297561646
RMSE train: 1.437597	val: 4.403954	test: 3.616856
MAE train: 1.175015	val: 3.085439	test: 2.836524

Epoch: 45
Loss: 1.799803376197815
RMSE train: 1.405972	val: 4.407505	test: 3.591419
MAE train: 1.137178	val: 3.080437	test: 2.813529

Epoch: 46
Loss: 1.5617374777793884
RMSE train: 1.395481	val: 4.411366	test: 3.573242
MAE train: 1.124213	val: 3.069100	test: 2.790181

Epoch: 47
Loss: 1.6437681913375854
RMSE train: 1.369088	val: 4.375867	test: 3.549666
MAE train: 1.094614	val: 3.051099	test: 2.764264

Epoch: 48
Loss: 1.6870551109313965
RMSE train: 1.342834	val: 4.332712	test: 3.522040
MAE train: 1.061644	val: 3.025170	test: 2.736050

Epoch: 49
Loss: 1.6767721772193909
RMSE train: 1.270102	val: 4.266840	test: 3.491344
MAE train: 0.998311	val: 2.970536	test: 2.714258

Epoch: 50
Loss: 1.318493366241455
RMSE train: 1.214569	val: 4.238985	test: 3.471303
MAE train: 0.948128	val: 2.961136	test: 2.697348

Epoch: 51
Loss: 1.5035605430603027
RMSE train: 1.211074	val: 4.236498	test: 3.471146
MAE train: 0.942369	val: 2.997506	test: 2.697831

Epoch: 52
Loss: 1.284553050994873
RMSE train: 1.170298	val: 4.193275	test: 3.443886
MAE train: 0.907355	val: 2.994278	test: 2.687788

Epoch: 53
Loss: 1.4334924817085266
RMSE train: 1.138559	val: 4.177853	test: 3.423783
MAE train: 0.872413	val: 2.997044	test: 2.677069

Epoch: 54
Loss: 1.2262566089630127
RMSE train: 1.119542	val: 4.174400	test: 3.404129
MAE train: 0.853676	val: 2.982620	test: 2.662193

Epoch: 55
Loss: 1.2992311716079712
RMSE train: 1.090147	val: 4.115932	test: 3.391118
MAE train: 0.828792	val: 2.941002	test: 2.652660

Epoch: 56
Loss: 1.085836946964264
RMSE train: 1.077076	val: 4.066524	test: 3.370869
MAE train: 0.821407	val: 2.926697	test: 2.646765

Epoch: 57
Loss: 1.2146498560905457
RMSE train: 1.004854	val: 4.002752	test: 3.353668
MAE train: 0.751842	val: 2.875288	test: 2.635875

Epoch: 58
Loss: 1.337286353111267
RMSE train: 0.960442	val: 3.978018	test: 3.361950
MAE train: 0.710075	val: 2.823100	test: 2.661998

Epoch: 59
Loss: 1.171433687210083
RMSE train: 0.914713	val: 3.950892	test: 3.358761
MAE train: 0.676662	val: 2.798461	test: 2.678482

Epoch: 60
Loss: 1.2166928052902222
RMSE train: 0.873618	val: 3.987244	test: 3.363437
MAE train: 0.637339	val: 2.836446	test: 2.669141

Epoch: 61
Loss: 1.2873891592025757
RMSE train: 0.842802	val: 3.995829	test: 3.359746
MAE train: 0.606781	val: 2.839897	test: 2.652908

Epoch: 62
Loss: 1.017171323299408
RMSE train: 0.809129	val: 3.961556	test: 3.337202
MAE train: 0.585805	val: 2.833363	test: 2.625438

Epoch: 63
Loss: 1.1954454183578491
RMSE train: 0.807027	val: 3.909972	test: 3.289567
MAE train: 0.586630	val: 2.803085	test: 2.572684

Epoch: 64
Loss: 1.0414127111434937
RMSE train: 0.826259	val: 3.863446	test: 3.262100
MAE train: 0.599337	val: 2.747317	test: 2.534474

Epoch: 65
Loss: 0.8550547361373901
RMSE train: 0.833681	val: 3.799383	test: 3.213717
MAE train: 0.610585	val: 2.695766	test: 2.484569

Epoch: 66
Loss: 1.0339863896369934
RMSE train: 0.835942	val: 3.797218	test: 3.211035
MAE train: 0.616620	val: 2.717853	test: 2.471140

Epoch: 67
Loss: 0.9615122377872467
RMSE train: 0.860590	val: 3.819399	test: 3.202437
MAE train: 0.639148	val: 2.747371	test: 2.449339

Epoch: 68
Loss: 1.0797703266143799
RMSE train: 0.863239	val: 3.841550	test: 3.215101
MAE train: 0.641459	val: 2.781808	test: 2.456474

Epoch: 69
Loss: 1.045574128627777
RMSE train: 0.868830	val: 3.862536	test: 3.183058
MAE train: 0.645576	val: 2.832059	test: 2.444170

Epoch: 70
Loss: 1.1427628993988037
RMSE train: 0.831136	val: 3.833292	test: 3.158537
MAE train: 0.621349	val: 2.837605	test: 2.448740

Epoch: 71
Loss: 0.9364867210388184
RMSE train: 0.784485	val: 3.771927	test: 3.145724
MAE train: 0.591486	val: 2.808190	test: 2.478602

Epoch: 72
Loss: 1.040340781211853
RMSE train: 0.812574	val: 3.839436	test: 3.198073
MAE train: 0.615028	val: 2.845390	test: 2.515436

Epoch: 73
Loss: 0.9709568321704865
RMSE train: 0.846414	val: 3.913557	test: 3.253895
MAE train: 0.641804	val: 2.883897	test: 2.531252

Epoch: 74
Loss: 0.9660137593746185
RMSE train: 0.901442	val: 3.952714	test: 3.289015
MAE train: 0.677753	val: 2.872159	test: 2.542073

Epoch: 75
Loss: 1.0512045621871948
RMSE train: 0.877110	val: 3.941890	test: 3.294608
MAE train: 0.655000	val: 2.828942	test: 2.536386

Epoch: 76
Loss: 0.9575252532958984
RMSE train: 0.858769	val: 3.921728	test: 3.273787
MAE train: 0.641627	val: 2.810984	test: 2.524695

Epoch: 77
Loss: 0.8877938389778137
RMSE train: 0.877472	val: 3.908138	test: 3.259990
MAE train: 0.651113	val: 2.826742	test: 2.537691

Epoch: 78
Loss: 0.8798110485076904
RMSE train: 0.851698	val: 3.908909	test: 3.246607
MAE train: 0.630567	val: 2.828255	test: 2.546643

Epoch: 79
Loss: 1.0137852430343628
RMSE train: 0.812443	val: 3.919161	test: 3.223448
MAE train: 0.603867	val: 2.815318	test: 2.525871

Epoch: 80
Loss: 0.9356042742729187
RMSE train: 0.843083	val: 3.967073	test: 3.227608
MAE train: 0.627164	val: 2.827811	test: 2.481687

Epoch: 81
Loss: 0.9085488617420197
RMSE train: 0.849780	val: 3.962402	test: 3.270007
MAE train: 0.635763	val: 2.804906	test: 2.490179

Epoch: 82
Loss: 0.8999169170856476
RMSE train: 0.803688	val: 3.915409	test: 3.290557
MAE train: 0.603823	val: 2.785059	test: 2.517617

Epoch: 83
Loss: 0.9475160539150238
RMSE train: 0.793501	val: 3.884288	test: 3.277033
MAE train: 0.597874	val: 2.815946	test: 2.549955

Epoch: 84
Loss: 0.7623824179172516
RMSE train: 0.839082	val: 1.808324	test: 4.437507
MAE train: 0.620391	val: 1.422551	test: 3.118206

Epoch: 85
Loss: 0.8201828002929688
RMSE train: 0.805729	val: 1.869818	test: 4.472614
MAE train: 0.582296	val: 1.484973	test: 3.147331

Epoch: 86
Loss: 0.781828910112381
RMSE train: 0.806203	val: 1.944426	test: 4.490216
MAE train: 0.574712	val: 1.564302	test: 3.159975

Epoch: 87
Loss: 0.8299088776111603
RMSE train: 0.816839	val: 2.026921	test: 4.483293
MAE train: 0.586927	val: 1.650991	test: 3.162017

Epoch: 88
Loss: 0.8293573260307312
RMSE train: 0.777782	val: 2.095000	test: 4.431492
MAE train: 0.576750	val: 1.707025	test: 3.135299

Epoch: 89
Loss: 0.8025793731212616
RMSE train: 0.733082	val: 2.090100	test: 4.391894
MAE train: 0.555804	val: 1.690588	test: 3.122284

Epoch: 90
Loss: 0.631188839673996
RMSE train: 0.741442	val: 2.123527	test: 4.369731
MAE train: 0.567722	val: 1.717237	test: 3.110229

Epoch: 91
Loss: 0.9273213744163513
RMSE train: 0.745224	val: 2.101032	test: 4.355134
MAE train: 0.569973	val: 1.697848	test: 3.107328

Epoch: 92
Loss: 0.9745542705059052
RMSE train: 0.772347	val: 2.022455	test: 4.372779
MAE train: 0.590293	val: 1.634105	test: 3.114817

Epoch: 93
Loss: 0.7826391458511353
RMSE train: 0.790123	val: 1.911104	test: 4.406929
MAE train: 0.608519	val: 1.525839	test: 3.150017

Epoch: 94
Loss: 0.6824946105480194
RMSE train: 0.797023	val: 1.797584	test: 4.395817
MAE train: 0.617525	val: 1.419344	test: 3.138897

Epoch: 95
Loss: 0.7087573111057281
RMSE train: 0.784132	val: 1.750421	test: 4.367776
MAE train: 0.608120	val: 1.375372	test: 3.108492

Epoch: 96
Loss: 0.718245655298233
RMSE train: 0.759288	val: 1.755725	test: 4.317278
MAE train: 0.586705	val: 1.377961	test: 3.061040

Epoch: 97
Loss: 0.7703475654125214
RMSE train: 0.720186	val: 1.777121	test: 4.264150
MAE train: 0.553570	val: 1.393033	test: 3.022535

Epoch: 98
Loss: 0.8207380175590515
RMSE train: 0.742411	val: 1.699367	test: 4.265537
MAE train: 0.573920	val: 1.323346	test: 3.025087

Epoch: 99
Loss: 0.6987467408180237
RMSE train: 0.802509	val: 1.629903	test: 4.262754
MAE train: 0.621839	val: 1.259621	test: 3.030999

Epoch: 100
Loss: 0.8454582989215851
RMSE train: 0.867718	val: 1.584781	test: 4.288209
MAE train: 0.669716	val: 1.225215	test: 3.051864

Epoch: 101
Loss: 0.7222994863986969
RMSE train: 0.939266	val: 1.535012	test: 4.324111
MAE train: 0.721098	val: 1.185730	test: 3.079340

Epoch: 102
Loss: 0.6876533031463623
RMSE train: 0.954821	val: 1.475128	test: 4.352307
MAE train: 0.724963	val: 1.131044	test: 3.101032

Epoch: 103
Loss: 0.944618433713913
RMSE train: 0.886990	val: 1.452354	test: 4.408980
MAE train: 0.668941	val: 1.102382	test: 3.142644

Epoch: 104
Loss: 0.6550994515419006
RMSE train: 0.814925	val: 1.474062	test: 4.403165
MAE train: 0.608603	val: 1.139560	test: 3.142687

Epoch: 105
Loss: 0.6683126389980316
RMSE train: 0.756377	val: 1.537384	test: 4.331948
MAE train: 0.561638	val: 1.230351	test: 3.091556

Epoch: 106
Loss: 0.6842632591724396
RMSE train: 0.739975	val: 1.667833	test: 4.266012
MAE train: 0.552364	val: 1.351628	test: 3.040726

Epoch: 107
Loss: 0.6956700384616852
RMSE train: 0.733828	val: 1.800398	test: 4.235861
MAE train: 0.551178	val: 1.475174	test: 3.014956

Epoch: 108
Loss: 0.6526289284229279
RMSE train: 0.759878	val: 1.811821	test: 4.249452
MAE train: 0.577472	val: 1.474057	test: 3.034717

Epoch: 109
Loss: 1.0021299123764038
RMSE train: 0.818138	val: 1.733456	test: 4.291037
MAE train: 0.633117	val: 1.371448	test: 3.086885

Epoch: 110
Loss: 0.6123725175857544
RMSE train: 0.836467	val: 1.663412	test: 4.311958
MAE train: 0.652819	val: 1.288324	test: 3.122937

Epoch: 111
Loss: 0.7604953646659851
RMSE train: 0.825991	val: 1.669273	test: 4.307453
MAE train: 0.646040	val: 1.289666	test: 3.124133

Epoch: 112
Loss: 0.6902864575386047
RMSE train: 0.848897	val: 1.685141	test: 4.324921
MAE train: 0.654240	val: 1.314605	test: 3.118994

Epoch: 113
Loss: 0.5875407755374908
RMSE train: 0.831640	val: 1.723850	test: 4.310643
MAE train: 0.623837	val: 1.362326	test: 3.085775

Epoch: 114
Loss: 0.5478470325469971
RMSE train: 0.850555	val: 1.667159	test: 4.342940
MAE train: 0.631686	val: 1.312544	test: 3.095481

Epoch: 115
Loss: 0.730594277381897
RMSE train: 0.830063	val: 1.615870	test: 4.341120
MAE train: 0.619342	val: 1.257240	test: 3.097956

Epoch: 116
Loss: 0.5890043079853058
RMSE train: 0.770365	val: 1.566609	test: 4.302820
MAE train: 0.581667	val: 1.202821	test: 3.090264

Epoch: 117
Loss: 0.6496946215629578
RMSE train: 0.748312	val: 1.538058	test: 4.289670
MAE train: 0.573559	val: 1.168922	test: 3.101939

Epoch: 118
Loss: 0.6506441235542297
RMSE train: 0.756919	val: 1.571608	test: 4.225359
MAE train: 0.591574	val: 1.199158	test: 3.078956

Epoch: 119
Loss: 0.6519675850868225
RMSE train: 0.778960	val: 1.654218	test: 4.158494
MAE train: 0.613728	val: 1.282133	test: 3.042029

Epoch: 120
Loss: 0.5599640011787415
RMSE train: 0.816674	val: 1.837719	test: 4.126539
MAE train: 0.639112	val: 1.470220	test: 3.020081

Epoch: 121
Loss: 0.5441234707832336
RMSE train: 0.815217	val: 1.981766	test: 4.110626
MAE train: 0.627850	val: 1.612653	test: 3.008325

Epoch: 122
Loss: 0.6055267751216888
RMSE train: 0.837587	val: 1.863142	test: 4.192186
MAE train: 0.634271	val: 1.507525	test: 3.055453

Epoch: 123
Loss: 0.7396963536739349
RMSE train: 0.811229	val: 1.726056	test: 4.252255
MAE train: 0.613375	val: 1.369859	test: 3.101185

Epoch: 124
Loss: 0.6480077803134918
RMSE train: 0.816708	val: 1.734339	test: 4.273667
MAE train: 0.612222	val: 1.372026	test: 3.126747

Epoch: 125
Loss: 0.747132271528244
RMSE train: 0.812274	val: 1.746247	test: 4.284286
MAE train: 0.602094	val: 1.377775	test: 3.133180

Epoch: 126
Loss: 0.6684504747390747
RMSE train: 0.770280	val: 1.765715	test: 4.269002
MAE train: 0.569599	val: 1.392889	test: 3.112246

Epoch: 127
Loss: 0.5387507677078247
RMSE train: 0.720477	val: 1.860669	test: 4.192931
MAE train: 0.536456	val: 1.476061	test: 3.053393

Epoch: 128
Loss: 0.872447669506073
RMSE train: 0.738289	val: 1.879712	test: 4.185357
MAE train: 0.551725	val: 1.497126	test: 3.050801

Epoch: 129
Loss: 0.5706622898578644
RMSE train: 0.724370	val: 1.902477	test: 4.155714
MAE train: 0.547090	val: 1.513071	test: 3.039965

Epoch: 130
Loss: 0.5124521404504776
RMSE train: 0.725519	val: 1.771734	test: 4.154295
MAE train: 0.550894	val: 1.389530	test: 3.040040

Epoch: 131
Loss: 0.5086254328489304
RMSE train: 0.750682	val: 1.657310	test: 4.155202
MAE train: 0.574305	val: 1.287781	test: 3.042309

Epoch: 132
Loss: 0.8340660333633423
RMSE train: 0.812472	val: 1.592452	test: 4.179730
MAE train: 0.622855	val: 1.228744	test: 3.055684

Epoch: 133
Loss: 0.4776116907596588
RMSE train: 0.815634	val: 1.585435	test: 4.160043
MAE train: 0.627308	val: 1.225042	test: 3.035163

Epoch: 134
Loss: 0.6856594681739807
RMSE train: 0.812036	val: 1.594414	test: 4.170917
MAE train: 0.622495	val: 1.244150	test: 3.032780

Epoch: 135
Loss: 0.7800674438476562
RMSE train: 0.811743	val: 1.607963	test: 4.195560
MAE train: 0.615106	val: 1.270261	test: 3.038458

Epoch: 136
Loss: 0.47625675797462463
RMSE train: 0.748078	val: 1.627375	test: 4.190204
MAE train: 0.565530	val: 1.295790	test: 3.031071

Epoch: 137
Loss: 0.7059754431247711
RMSE train: 0.706265	val: 1.642981	test: 4.217382
MAE train: 0.532502	val: 1.312751	test: 3.048176

Epoch: 138
Loss: 0.695818305015564
RMSE train: 0.735493	val: 1.655858	test: 4.276781
MAE train: 0.554009	val: 1.330891	test: 3.076969

Early stopping
Best (RMSE):	 train: 0.886990	val: 1.452354	test: 4.408980
Best (MAE):	 train: 0.668941	val: 1.102382	test: 3.142644


Epoch: 84
Loss: 0.8981111347675323
RMSE train: 0.707660	val: 1.746216	test: 4.002102
MAE train: 0.534924	val: 1.318403	test: 2.983011

Epoch: 85
Loss: 0.7638510763645172
RMSE train: 0.710783	val: 1.725567	test: 4.035597
MAE train: 0.528106	val: 1.305406	test: 2.995282

Epoch: 86
Loss: 1.0245494544506073
RMSE train: 0.750777	val: 1.673454	test: 4.086686
MAE train: 0.553339	val: 1.268314	test: 3.012064

Epoch: 87
Loss: 0.884583979845047
RMSE train: 0.756902	val: 1.645949	test: 4.116127
MAE train: 0.555050	val: 1.236171	test: 3.022021

Epoch: 88
Loss: 0.830108642578125
RMSE train: 0.742081	val: 1.636215	test: 4.095421
MAE train: 0.542953	val: 1.223868	test: 3.011602

Epoch: 89
Loss: 0.9220758080482483
RMSE train: 0.740167	val: 1.635901	test: 4.062657
MAE train: 0.543891	val: 1.235809	test: 2.998038

Epoch: 90
Loss: 0.7409164309501648
RMSE train: 0.778905	val: 1.635852	test: 4.068315
MAE train: 0.577721	val: 1.246260	test: 3.017142

Epoch: 91
Loss: 0.8331672251224518
RMSE train: 0.799632	val: 1.637183	test: 4.092353
MAE train: 0.601289	val: 1.256268	test: 3.055411

Epoch: 92
Loss: 0.7469541430473328
RMSE train: 0.824686	val: 1.612662	test: 4.125387
MAE train: 0.623724	val: 1.230976	test: 3.096132

Epoch: 93
Loss: 0.785974383354187
RMSE train: 0.796323	val: 1.628168	test: 4.098386
MAE train: 0.599918	val: 1.250256	test: 3.090478

Epoch: 94
Loss: 0.7407630085945129
RMSE train: 0.771533	val: 1.653167	test: 4.035301
MAE train: 0.581907	val: 1.277915	test: 3.042946

Epoch: 95
Loss: 0.7369260489940643
RMSE train: 0.739825	val: 1.692967	test: 3.975043
MAE train: 0.564002	val: 1.324357	test: 2.994724

Epoch: 96
Loss: 0.7703110575675964
RMSE train: 0.723274	val: 1.741932	test: 3.925343
MAE train: 0.556600	val: 1.370642	test: 2.958186

Epoch: 97
Loss: 0.7900503873825073
RMSE train: 0.690948	val: 1.727259	test: 3.889443
MAE train: 0.535895	val: 1.346132	test: 2.928571

Epoch: 98
Loss: 0.5779174566268921
RMSE train: 0.655467	val: 1.719350	test: 3.897823
MAE train: 0.506969	val: 1.327526	test: 2.937742

Epoch: 99
Loss: 0.8794876635074615
RMSE train: 0.642952	val: 1.745166	test: 3.884198
MAE train: 0.490843	val: 1.352979	test: 2.929671

Epoch: 100
Loss: 0.7879875898361206
RMSE train: 0.668735	val: 1.838413	test: 3.868635
MAE train: 0.513390	val: 1.436494	test: 2.912112

Epoch: 101
Loss: 0.7400231957435608
RMSE train: 0.713577	val: 1.884529	test: 3.862401
MAE train: 0.549030	val: 1.481794	test: 2.905972

Epoch: 102
Loss: 0.8303377032279968
RMSE train: 0.744247	val: 1.800014	test: 3.893375
MAE train: 0.573021	val: 1.421126	test: 2.929862

Epoch: 103
Loss: 0.6109351515769958
RMSE train: 0.785479	val: 1.657997	test: 3.940959
MAE train: 0.606762	val: 1.292937	test: 2.961370

Epoch: 104
Loss: 0.8016072511672974
RMSE train: 0.820524	val: 1.582583	test: 3.932395
MAE train: 0.632446	val: 1.205954	test: 2.960825

Epoch: 105
Loss: 0.7802608609199524
RMSE train: 0.852186	val: 1.547624	test: 3.954508
MAE train: 0.651951	val: 1.149338	test: 2.983841

Epoch: 106
Loss: 0.6965899765491486
RMSE train: 0.844639	val: 1.539290	test: 3.962209
MAE train: 0.636514	val: 1.149688	test: 2.999403

Epoch: 107
Loss: 0.6609825491905212
RMSE train: 0.839359	val: 1.536297	test: 3.939578
MAE train: 0.614986	val: 1.177986	test: 3.004240

Epoch: 108
Loss: 0.7900184988975525
RMSE train: 0.824275	val: 1.545526	test: 3.945343
MAE train: 0.601533	val: 1.203769	test: 3.015905

Epoch: 109
Loss: 0.95672407746315
RMSE train: 0.802209	val: 1.541479	test: 4.021843
MAE train: 0.599426	val: 1.183544	test: 3.065850

Epoch: 110
Loss: 0.7298629581928253
RMSE train: 0.756623	val: 1.556675	test: 4.042459
MAE train: 0.577503	val: 1.182728	test: 3.064294

Epoch: 111
Loss: 0.6997822821140289
RMSE train: 0.745805	val: 1.587806	test: 4.044382
MAE train: 0.569150	val: 1.215576	test: 3.043119

Epoch: 112
Loss: 0.7066850960254669
RMSE train: 0.746907	val: 1.661713	test: 4.025708
MAE train: 0.560450	val: 1.299403	test: 3.012928

Epoch: 113
Loss: 0.6776097118854523
RMSE train: 0.738390	val: 1.688610	test: 3.998044
MAE train: 0.549662	val: 1.337539	test: 2.991767

Epoch: 114
Loss: 0.692446380853653
RMSE train: 0.717447	val: 1.642121	test: 3.966832
MAE train: 0.536451	val: 1.286576	test: 2.972872

Epoch: 115
Loss: 0.7470497786998749
RMSE train: 0.718341	val: 1.569820	test: 4.003683
MAE train: 0.535026	val: 1.202643	test: 2.998112

Epoch: 116
Loss: 0.6378455460071564
RMSE train: 0.763403	val: 1.559695	test: 4.100218
MAE train: 0.551042	val: 1.181791	test: 3.054918

Epoch: 117
Loss: 0.7297027111053467
RMSE train: 0.779647	val: 1.557783	test: 4.070635
MAE train: 0.551513	val: 1.180386	test: 3.020077

Epoch: 118
Loss: 0.6875512897968292
RMSE train: 0.767569	val: 1.585664	test: 3.982160
MAE train: 0.551579	val: 1.234600	test: 2.951254

Epoch: 119
Loss: 0.9661604762077332
RMSE train: 0.757991	val: 1.675706	test: 3.898827
MAE train: 0.555759	val: 1.339719	test: 2.888547

Epoch: 120
Loss: 0.6137740612030029
RMSE train: 0.703354	val: 1.799818	test: 3.820740
MAE train: 0.525497	val: 1.460514	test: 2.832278

Epoch: 121
Loss: 0.8122169673442841
RMSE train: 0.667090	val: 1.816198	test: 3.805990
MAE train: 0.506482	val: 1.460221	test: 2.836612

Epoch: 122
Loss: 0.5920058786869049
RMSE train: 0.622255	val: 1.777165	test: 3.807742
MAE train: 0.467620	val: 1.405224	test: 2.849538

Epoch: 123
Loss: 0.7299431562423706
RMSE train: 0.613436	val: 1.722916	test: 3.830012
MAE train: 0.455171	val: 1.345347	test: 2.882886

Epoch: 124
Loss: 0.7404293715953827
RMSE train: 0.619719	val: 1.635967	test: 3.911481
MAE train: 0.462940	val: 1.263330	test: 2.942185

Epoch: 125
Loss: 0.5854109525680542
RMSE train: 0.596819	val: 1.582188	test: 3.951185
MAE train: 0.440613	val: 1.231021	test: 2.954358

Epoch: 126
Loss: 0.6093273758888245
RMSE train: 0.634201	val: 1.582197	test: 3.986334
MAE train: 0.470365	val: 1.244492	test: 2.961308

Epoch: 127
Loss: 0.6897771060466766
RMSE train: 0.685469	val: 1.589420	test: 4.049244
MAE train: 0.503895	val: 1.249966	test: 2.992053

Epoch: 128
Loss: 0.5642575621604919
RMSE train: 0.734840	val: 1.581091	test: 4.152575
MAE train: 0.526421	val: 1.228194	test: 3.054715

Epoch: 129
Loss: 0.6565442681312561
RMSE train: 0.750262	val: 1.590510	test: 4.176912
MAE train: 0.535632	val: 1.235414	test: 3.072925

Epoch: 130
Loss: 0.5622823536396027
RMSE train: 0.728165	val: 1.614824	test: 4.141633
MAE train: 0.526917	val: 1.258831	test: 3.054864

Epoch: 131
Loss: 0.52334825694561
RMSE train: 0.695751	val: 1.633864	test: 4.096210
MAE train: 0.512783	val: 1.270672	test: 3.033024

Epoch: 132
Loss: 0.735861748456955
RMSE train: 0.669309	val: 1.640240	test: 4.074283
MAE train: 0.499046	val: 1.268508	test: 3.027748

Epoch: 133
Loss: 0.6710265874862671
RMSE train: 0.648272	val: 1.650442	test: 4.046955
MAE train: 0.491584	val: 1.285202	test: 3.016146

Epoch: 134
Loss: 0.5658778548240662
RMSE train: 0.650246	val: 1.652868	test: 4.036674
MAE train: 0.502682	val: 1.302865	test: 3.008226

Epoch: 135
Loss: 0.5493350923061371
RMSE train: 0.679166	val: 1.647455	test: 4.033430
MAE train: 0.522422	val: 1.311869	test: 3.002424

Epoch: 136
Loss: 0.6681261658668518
RMSE train: 0.705402	val: 1.656893	test: 4.037545
MAE train: 0.537762	val: 1.324376	test: 2.997900

Epoch: 137
Loss: 0.6837292909622192
RMSE train: 0.727223	val: 1.649626	test: 4.053671
MAE train: 0.548625	val: 1.309790	test: 3.001670

Epoch: 138
Loss: 0.6097665131092072
RMSE train: 0.669621	val: 1.694620	test: 4.053092
MAE train: 0.506512	val: 1.339022	test: 2.990943

Epoch: 139
Loss: 0.7016714513301849
RMSE train: 0.659883	val: 1.765229	test: 4.101505
MAE train: 0.498654	val: 1.393100	test: 3.016902

Epoch: 140
Loss: 0.811278760433197
RMSE train: 0.670878	val: 1.738240	test: 4.147800
MAE train: 0.498612	val: 1.355472	test: 3.043091

Epoch: 141
Loss: 0.581887274980545
RMSE train: 0.682555	val: 1.707773	test: 4.195354
MAE train: 0.504735	val: 1.316862	test: 3.078113

Epoch: 142
Loss: 0.9272672832012177
RMSE train: 0.627618	val: 1.727963	test: 4.125717
MAE train: 0.467646	val: 1.344979	test: 3.034363

Early stopping
Best (RMSE):	 train: 0.839359	val: 1.536297	test: 3.939578
Best (MAE):	 train: 0.614986	val: 1.177986	test: 3.004240
All runs completed.


Epoch: 84
Loss: 1.2437918782234192
RMSE train: 0.830914	val: 1.532906	test: 2.556310
MAE train: 0.614047	val: 1.025200	test: 1.980326

Epoch: 85
Loss: 0.9031727015972137
RMSE train: 0.836178	val: 1.509917	test: 2.585559
MAE train: 0.611812	val: 1.024616	test: 2.021082

Epoch: 86
Loss: 1.1467103362083435
RMSE train: 0.855914	val: 1.508524	test: 2.603665
MAE train: 0.623427	val: 1.043564	test: 2.024568

Epoch: 87
Loss: 1.2387003302574158
RMSE train: 0.903252	val: 1.509146	test: 2.588122
MAE train: 0.658093	val: 1.070481	test: 2.009001

Epoch: 88
Loss: 0.990077406167984
RMSE train: 0.918580	val: 1.508711	test: 2.612956
MAE train: 0.661188	val: 1.063667	test: 2.029306

Epoch: 89
Loss: 0.9605951607227325
RMSE train: 0.882809	val: 1.494676	test: 2.577336
MAE train: 0.637124	val: 1.054901	test: 2.009498

Epoch: 90
Loss: 1.0163828134536743
RMSE train: 0.809355	val: 1.473743	test: 2.555832
MAE train: 0.590124	val: 1.039014	test: 1.984033

Epoch: 91
Loss: 0.9519529640674591
RMSE train: 0.784700	val: 1.457799	test: 2.555864
MAE train: 0.578176	val: 1.030535	test: 2.001094

Epoch: 92
Loss: 0.9771905243396759
RMSE train: 0.803516	val: 1.444376	test: 2.563621
MAE train: 0.590578	val: 1.051655	test: 2.008365

Epoch: 93
Loss: 1.0504328310489655
RMSE train: 0.837099	val: 1.441456	test: 2.574224
MAE train: 0.614295	val: 1.086903	test: 2.036635

Epoch: 94
Loss: 1.052228569984436
RMSE train: 0.857029	val: 1.451892	test: 2.599607
MAE train: 0.631394	val: 1.077693	test: 2.048401

Epoch: 95
Loss: 1.0012132227420807
RMSE train: 0.872199	val: 1.492268	test: 2.615450
MAE train: 0.646308	val: 1.050120	test: 2.034489

Epoch: 96
Loss: 1.1069984436035156
RMSE train: 0.883161	val: 1.595589	test: 2.624574
MAE train: 0.653544	val: 1.077157	test: 1.994534

Epoch: 97
Loss: 0.8828360736370087
RMSE train: 0.874340	val: 1.628215	test: 2.610868
MAE train: 0.641993	val: 1.094851	test: 1.964420

Epoch: 98
Loss: 0.8666007816791534
RMSE train: 0.823868	val: 1.589011	test: 2.566625
MAE train: 0.587646	val: 1.065387	test: 1.927021

Epoch: 99
Loss: 1.1631556749343872
RMSE train: 0.787320	val: 1.515966	test: 2.481288
MAE train: 0.554818	val: 1.030000	test: 1.890712

Epoch: 100
Loss: 0.9410596191883087
RMSE train: 0.721389	val: 1.466851	test: 2.474055
MAE train: 0.511770	val: 1.026643	test: 1.922030

Epoch: 101
Loss: 1.0236589908599854
RMSE train: 0.724632	val: 1.468762	test: 2.481036
MAE train: 0.513359	val: 1.042653	test: 1.939636

Epoch: 102
Loss: 0.8272921442985535
RMSE train: 0.737730	val: 1.460232	test: 2.508530
MAE train: 0.528671	val: 1.051265	test: 1.967464

Epoch: 103
Loss: 0.9477105736732483
RMSE train: 0.780018	val: 1.483380	test: 2.548008
MAE train: 0.563670	val: 1.055685	test: 1.982070

Epoch: 104
Loss: 0.9184163510799408
RMSE train: 0.813061	val: 1.511215	test: 2.565003
MAE train: 0.595810	val: 1.058096	test: 1.968637

Epoch: 105
Loss: 0.9646320641040802
RMSE train: 0.804861	val: 1.525608	test: 2.567850
MAE train: 0.600007	val: 1.056526	test: 1.953491

Epoch: 106
Loss: 0.9079956114292145
RMSE train: 0.798878	val: 1.518334	test: 2.580518
MAE train: 0.607002	val: 1.052507	test: 1.966598

Epoch: 107
Loss: 0.8311308920383453
RMSE train: 0.802400	val: 1.509619	test: 2.566128
MAE train: 0.613886	val: 1.050711	test: 1.984589

Epoch: 108
Loss: 0.8416343629360199
RMSE train: 0.810623	val: 1.583170	test: 2.577006
MAE train: 0.613531	val: 1.085565	test: 1.974986

Epoch: 109
Loss: 0.8864704072475433
RMSE train: 0.798022	val: 1.574596	test: 2.592407
MAE train: 0.594343	val: 1.083155	test: 1.989436

Epoch: 110
Loss: 1.1615228652954102
RMSE train: 0.760855	val: 1.496025	test: 2.571763
MAE train: 0.561163	val: 1.043911	test: 1.990318

Epoch: 111
Loss: 0.736484706401825
RMSE train: 0.781841	val: 1.485113	test: 2.590120
MAE train: 0.567903	val: 1.056882	test: 2.021755

Epoch: 112
Loss: 0.9740824103355408
RMSE train: 0.817623	val: 1.485723	test: 2.607948
MAE train: 0.600722	val: 1.064595	test: 2.036653

Epoch: 113
Loss: 0.9660338163375854
RMSE train: 0.896218	val: 1.544773	test: 2.631886
MAE train: 0.662141	val: 1.099208	test: 2.042601

Epoch: 114
Loss: 0.8374010622501373
RMSE train: 0.923692	val: 1.638877	test: 2.669068
MAE train: 0.682630	val: 1.165130	test: 2.018551

Epoch: 115
Loss: 0.8084578216075897
RMSE train: 0.912122	val: 1.726159	test: 2.676917
MAE train: 0.672081	val: 1.237625	test: 1.985510

Epoch: 116
Loss: 0.7542513310909271
RMSE train: 0.828556	val: 1.748401	test: 2.711065
MAE train: 0.612385	val: 1.254139	test: 2.003410

Epoch: 117
Loss: 0.9764447212219238
RMSE train: 0.766806	val: 1.714345	test: 2.749457
MAE train: 0.568560	val: 1.220452	test: 2.036865

Epoch: 118
Loss: 1.0408392548561096
RMSE train: 0.742513	val: 1.630694	test: 2.735243
MAE train: 0.546547	val: 1.146023	test: 2.065212

Epoch: 119
Loss: 0.8071593344211578
RMSE train: 0.741411	val: 1.643458	test: 2.728289
MAE train: 0.542220	val: 1.150895	test: 2.066465

Epoch: 120
Loss: 0.9070043861865997
RMSE train: 0.763583	val: 1.674073	test: 2.617037
MAE train: 0.554520	val: 1.184449	test: 1.984969

Epoch: 121
Loss: 0.7853230834007263
RMSE train: 0.823480	val: 1.681060	test: 2.523351
MAE train: 0.595985	val: 1.201477	test: 1.901893

Early stopping
Best (RMSE):	 train: 0.870961	val: 1.434672	test: 2.463129
Best (MAE):	 train: 0.651777	val: 1.020418	test: 1.899325


Epoch: 84
Loss: 0.9297118782997131
RMSE train: 0.690175	val: 4.275608	test: 3.371088
MAE train: 0.518976	val: 3.043261	test: 2.588320

Epoch: 85
Loss: 0.9133405089378357
RMSE train: 0.692644	val: 4.292655	test: 3.414464
MAE train: 0.525137	val: 3.097498	test: 2.645753

Epoch: 86
Loss: 0.8542472124099731
RMSE train: 0.720004	val: 4.335889	test: 3.483747
MAE train: 0.546119	val: 3.133779	test: 2.721604

Epoch: 87
Loss: 0.9106026291847229
RMSE train: 0.794249	val: 4.397921	test: 3.532025
MAE train: 0.599836	val: 3.165589	test: 2.769739

Epoch: 88
Loss: 0.7901723086833954
RMSE train: 0.849001	val: 4.431616	test: 3.550656
MAE train: 0.642146	val: 3.202361	test: 2.794978

Epoch: 89
Loss: 0.9252132773399353
RMSE train: 0.902478	val: 4.460514	test: 3.545323
MAE train: 0.670606	val: 3.249398	test: 2.783816

Epoch: 90
Loss: 0.8693954050540924
RMSE train: 0.937692	val: 4.476510	test: 3.503304
MAE train: 0.677936	val: 3.243850	test: 2.738932

Epoch: 91
Loss: 0.92799311876297
RMSE train: 0.886172	val: 4.445433	test: 3.414739
MAE train: 0.640341	val: 3.169897	test: 2.632275

Epoch: 92
Loss: 1.007144421339035
RMSE train: 0.837093	val: 4.389866	test: 3.342734
MAE train: 0.609113	val: 3.102218	test: 2.555270

Epoch: 93
Loss: 0.9253087639808655
RMSE train: 0.809406	val: 4.335325	test: 3.327864
MAE train: 0.597835	val: 3.048769	test: 2.533759

Epoch: 94
Loss: 0.8628712594509125
RMSE train: 0.823710	val: 4.316797	test: 3.340289
MAE train: 0.611364	val: 3.033818	test: 2.543569

Epoch: 95
Loss: 0.8384595513343811
RMSE train: 0.845731	val: 4.321468	test: 3.341756
MAE train: 0.625845	val: 3.041811	test: 2.543576

Epoch: 96
Loss: 0.763973742723465
RMSE train: 0.853912	val: 4.348311	test: 3.349739
MAE train: 0.628853	val: 3.046066	test: 2.549055

Epoch: 97
Loss: 0.7961597442626953
RMSE train: 0.851735	val: 4.362754	test: 3.332183
MAE train: 0.628522	val: 3.007532	test: 2.521075

Epoch: 98
Loss: 0.9359256029129028
RMSE train: 0.848908	val: 4.357447	test: 3.338851
MAE train: 0.630432	val: 3.010766	test: 2.530373

Epoch: 99
Loss: 0.8937749266624451
RMSE train: 0.840217	val: 4.331393	test: 3.375804
MAE train: 0.630068	val: 2.990377	test: 2.567241

Epoch: 100
Loss: 0.7704271674156189
RMSE train: 0.815001	val: 4.283805	test: 3.381145
MAE train: 0.610348	val: 2.970234	test: 2.577482

Epoch: 101
Loss: 0.7816563546657562
RMSE train: 0.788029	val: 4.251517	test: 3.353034
MAE train: 0.595028	val: 2.940448	test: 2.555705

Epoch: 102
Loss: 0.9183188378810883
RMSE train: 0.816428	val: 4.270347	test: 3.342105
MAE train: 0.615706	val: 2.980742	test: 2.551552

Epoch: 103
Loss: 0.8027351498603821
RMSE train: 0.829630	val: 4.272776	test: 3.357356
MAE train: 0.622528	val: 2.970655	test: 2.572005

Epoch: 104
Loss: 0.8287116587162018
RMSE train: 0.852030	val: 4.274713	test: 3.375413
MAE train: 0.635252	val: 2.939442	test: 2.581574

Epoch: 105
Loss: 0.8847948610782623
RMSE train: 0.809176	val: 4.237423	test: 3.382920
MAE train: 0.605478	val: 2.934269	test: 2.596688

Epoch: 106
Loss: 0.8696161210536957
RMSE train: 0.808167	val: 4.243564	test: 3.377765
MAE train: 0.603520	val: 2.973109	test: 2.602481

Epoch: 107
Loss: 0.8305318653583527
RMSE train: 0.840382	val: 4.270703	test: 3.352360
MAE train: 0.621759	val: 3.018853	test: 2.579330

Epoch: 108
Loss: 0.7088701128959656
RMSE train: 0.842721	val: 4.283734	test: 3.335515
MAE train: 0.622252	val: 3.071475	test: 2.566878

Epoch: 109
Loss: 0.8326385319232941
RMSE train: 0.820509	val: 4.279753	test: 3.325041
MAE train: 0.610131	val: 3.076228	test: 2.552253

Epoch: 110
Loss: 0.7609304189682007
RMSE train: 0.807757	val: 4.281196	test: 3.310119
MAE train: 0.602035	val: 3.046717	test: 2.529397

Epoch: 111
Loss: 0.7875985503196716
RMSE train: 0.744011	val: 4.250289	test: 3.269132
MAE train: 0.556244	val: 3.018925	test: 2.487804

Epoch: 112
Loss: 0.7658757269382477
RMSE train: 0.672464	val: 4.223766	test: 3.241448
MAE train: 0.508182	val: 2.994582	test: 2.457052

Epoch: 113
Loss: 0.7909926474094391
RMSE train: 0.661753	val: 4.235297	test: 3.213275
MAE train: 0.501971	val: 3.005930	test: 2.421239

Epoch: 114
Loss: 0.7331239581108093
RMSE train: 0.669882	val: 4.268130	test: 3.187201
MAE train: 0.499037	val: 3.029906	test: 2.374007

Epoch: 115
Loss: 0.7547368109226227
RMSE train: 0.695509	val: 4.329543	test: 3.201332
MAE train: 0.514192	val: 3.096091	test: 2.384187

Epoch: 116
Loss: 0.7310380935668945
RMSE train: 0.743700	val: 4.374437	test: 3.217641
MAE train: 0.544964	val: 3.152845	test: 2.397859

Epoch: 117
Loss: 0.7526926398277283
RMSE train: 0.734441	val: 4.391884	test: 3.191391
MAE train: 0.533721	val: 3.121704	test: 2.334759

Epoch: 118
Loss: 0.8045916855335236
RMSE train: 0.735758	val: 4.385177	test: 3.185586
MAE train: 0.536261	val: 3.077513	test: 2.314381

Epoch: 119
Loss: 0.7360912263393402
RMSE train: 0.696929	val: 4.323797	test: 3.175437
MAE train: 0.509315	val: 3.016745	test: 2.322503

Epoch: 120
Loss: 0.6482742130756378
RMSE train: 0.649604	val: 4.271057	test: 3.181732
MAE train: 0.481067	val: 2.991462	test: 2.356414

Epoch: 121
Loss: 0.7821215689182281
RMSE train: 0.681419	val: 4.246938	test: 3.206037
MAE train: 0.510724	val: 2.997333	test: 2.398324

Early stopping
Best (RMSE):	 train: 0.878782	val: 4.093542	test: 3.373255
Best (MAE):	 train: 0.649448	val: 2.921993	test: 2.662461


Epoch: 84
Loss: 0.9373525083065033
RMSE train: 0.656194	val: 4.047691	test: 3.098081
MAE train: 0.486858	val: 2.691650	test: 2.276798

Epoch: 85
Loss: 0.945001095533371
RMSE train: 0.724387	val: 4.108124	test: 3.109016
MAE train: 0.549783	val: 2.731536	test: 2.271581

Epoch: 86
Loss: 1.0334694385528564
RMSE train: 0.807628	val: 4.161067	test: 3.088972
MAE train: 0.617860	val: 2.737665	test: 2.232429

Epoch: 87
Loss: 0.9289306700229645
RMSE train: 0.819951	val: 4.152779	test: 3.050510
MAE train: 0.620247	val: 2.709546	test: 2.180501

Epoch: 88
Loss: 0.9523271322250366
RMSE train: 0.754949	val: 4.072344	test: 3.013042
MAE train: 0.560267	val: 2.638576	test: 2.148140

Epoch: 89
Loss: 0.9354084134101868
RMSE train: 0.742037	val: 4.038237	test: 3.010647
MAE train: 0.537943	val: 2.637917	test: 2.143828

Epoch: 90
Loss: 0.9536204636096954
RMSE train: 0.788461	val: 4.060841	test: 3.044860
MAE train: 0.564408	val: 2.680648	test: 2.179539

Epoch: 91
Loss: 0.9116475582122803
RMSE train: 0.789224	val: 4.120715	test: 3.068163
MAE train: 0.562267	val: 2.690826	test: 2.195403

Epoch: 92
Loss: 0.977188915014267
RMSE train: 0.805805	val: 4.180657	test: 3.110411
MAE train: 0.574020	val: 2.740843	test: 2.244856

Epoch: 93
Loss: 0.9752788245677948
RMSE train: 0.826820	val: 4.251348	test: 3.132499
MAE train: 0.598510	val: 2.787652	test: 2.268297

Epoch: 94
Loss: 0.9923163056373596
RMSE train: 0.826949	val: 4.297499	test: 3.123577
MAE train: 0.607242	val: 2.822529	test: 2.265684

Epoch: 95
Loss: 0.9205620884895325
RMSE train: 0.793560	val: 4.261423	test: 3.087160
MAE train: 0.575574	val: 2.800409	test: 2.246571

Epoch: 96
Loss: 0.9169090390205383
RMSE train: 0.774898	val: 4.220155	test: 3.085936
MAE train: 0.556024	val: 2.779714	test: 2.248269

Epoch: 97
Loss: 0.9590103328227997
RMSE train: 0.757789	val: 4.224998	test: 3.107724
MAE train: 0.548607	val: 2.807112	test: 2.260408

Epoch: 98
Loss: 1.0269920825958252
RMSE train: 0.781027	val: 4.255269	test: 3.139881
MAE train: 0.573152	val: 2.777978	test: 2.264727

Epoch: 99
Loss: 0.9074684977531433
RMSE train: 0.816998	val: 4.249087	test: 3.173044
MAE train: 0.596933	val: 2.750425	test: 2.285239

Epoch: 100
Loss: 0.8389968872070312
RMSE train: 0.835997	val: 4.229116	test: 3.174267
MAE train: 0.614070	val: 2.748831	test: 2.289389

Epoch: 101
Loss: 0.9311938285827637
RMSE train: 0.813362	val: 4.211282	test: 3.154104
MAE train: 0.604366	val: 2.815158	test: 2.287964

Epoch: 102
Loss: 0.7143593728542328
RMSE train: 0.811840	val: 4.189869	test: 3.110517
MAE train: 0.611250	val: 2.807366	test: 2.240790

Epoch: 103
Loss: 0.7731693983078003
RMSE train: 0.804892	val: 4.158321	test: 3.073608
MAE train: 0.611509	val: 2.735946	test: 2.193513

Epoch: 104
Loss: 0.8583767712116241
RMSE train: 0.831854	val: 4.149835	test: 3.084545
MAE train: 0.627712	val: 2.689170	test: 2.187418

Epoch: 105
Loss: 0.8975971341133118
RMSE train: 0.817635	val: 4.172685	test: 3.111381
MAE train: 0.610913	val: 2.734222	test: 2.219951

Epoch: 106
Loss: 0.7299093306064606
RMSE train: 0.805524	val: 4.224287	test: 3.133333
MAE train: 0.592369	val: 2.810235	test: 2.254047

Epoch: 107
Loss: 0.813488245010376
RMSE train: 0.819728	val: 4.306697	test: 3.152400
MAE train: 0.591596	val: 2.912678	test: 2.283674

Epoch: 108
Loss: 0.8405128717422485
RMSE train: 0.814666	val: 4.331886	test: 3.147025
MAE train: 0.581870	val: 2.920163	test: 2.273249

Epoch: 109
Loss: 0.7139645218849182
RMSE train: 0.821861	val: 4.344293	test: 3.130451
MAE train: 0.587409	val: 2.885637	test: 2.237943

Epoch: 110
Loss: 0.8223811984062195
RMSE train: 0.784699	val: 4.282227	test: 3.083073
MAE train: 0.565790	val: 2.808118	test: 2.191610

Epoch: 111
Loss: 0.8812527358531952
RMSE train: 0.747621	val: 4.232009	test: 3.060557
MAE train: 0.545088	val: 2.776225	test: 2.166457

Epoch: 112
Loss: 0.8306436836719513
RMSE train: 0.751388	val: 4.188136	test: 3.060301
MAE train: 0.551389	val: 2.765112	test: 2.160670

Epoch: 113
Loss: 0.8142062425613403
RMSE train: 0.752898	val: 4.160226	test: 3.052836
MAE train: 0.552337	val: 2.748940	test: 2.152657

Epoch: 114
Loss: 0.7873924374580383
RMSE train: 0.752129	val: 4.197476	test: 3.048990
MAE train: 0.553986	val: 2.763441	test: 2.142923

Epoch: 115
Loss: 0.7268970906734467
RMSE train: 0.775883	val: 4.232483	test: 3.053533
MAE train: 0.571834	val: 2.802327	test: 2.152214

Epoch: 116
Loss: 0.815386027097702
RMSE train: 0.730513	val: 4.189732	test: 3.041860
MAE train: 0.541342	val: 2.740892	test: 2.139008

Epoch: 117
Loss: 0.7102331221103668
RMSE train: 0.677738	val: 4.136513	test: 3.028766
MAE train: 0.506744	val: 2.630816	test: 2.117786

Epoch: 118
Loss: 0.7750124037265778
RMSE train: 0.680477	val: 4.102763	test: 3.003586
MAE train: 0.517137	val: 2.557370	test: 2.086575

Epoch: 119
Loss: 0.7329776287078857
RMSE train: 0.702319	val: 4.135261	test: 2.995614
MAE train: 0.533669	val: 2.573767	test: 2.072213

Epoch: 120
Loss: 0.7364183068275452
RMSE train: 0.715360	val: 4.168296	test: 2.990076
MAE train: 0.537251	val: 2.615876	test: 2.068192

Epoch: 121
Loss: 0.717377096414566
RMSE train: 0.730336	val: 4.204708	test: 3.002328
MAE train: 0.537318	val: 2.683404	test: 2.092111

Epoch: 122
Loss: 0.6525942981243134
RMSE train: 0.718487	val: 4.204935	test: 2.996130
MAE train: 0.523764	val: 2.704517	test: 2.091121

Epoch: 123
Loss: 0.8100428283214569
RMSE train: 0.709906	val: 4.212886	test: 2.995553
MAE train: 0.516116	val: 2.690712	test: 2.082442

Epoch: 124
Loss: 0.768334299325943
RMSE train: 0.695010	val: 4.208721	test: 2.980211
MAE train: 0.509676	val: 2.650519	test: 2.060229

Early stopping
Best (RMSE):	 train: 0.742037	val: 4.038237	test: 3.010647
Best (MAE):	 train: 0.537943	val: 2.637917	test: 2.143828


Epoch: 84
Loss: 1.0799949765205383
RMSE train: 0.880948	val: 1.548889	test: 2.554936
MAE train: 0.652881	val: 1.049153	test: 1.929511

Epoch: 85
Loss: 1.3600269556045532
RMSE train: 0.859913	val: 1.540139	test: 2.539823
MAE train: 0.638471	val: 1.037511	test: 1.896969

Epoch: 86
Loss: 1.0956178307533264
RMSE train: 0.847147	val: 1.510108	test: 2.522902
MAE train: 0.624130	val: 1.017097	test: 1.868160

Epoch: 87
Loss: 1.2130199074745178
RMSE train: 0.843423	val: 1.434670	test: 2.476648
MAE train: 0.621287	val: 0.989878	test: 1.853379

Epoch: 88
Loss: 1.1899203062057495
RMSE train: 0.789245	val: 1.387313	test: 2.417480
MAE train: 0.580184	val: 1.046370	test: 1.846455

Epoch: 89
Loss: 1.1453142166137695
RMSE train: 0.804482	val: 1.459875	test: 2.401105
MAE train: 0.589101	val: 1.167229	test: 1.841600

Epoch: 90
Loss: 0.9223892092704773
RMSE train: 0.801940	val: 1.429120	test: 2.370202
MAE train: 0.583752	val: 1.106684	test: 1.809130

Epoch: 91
Loss: 0.8593934774398804
RMSE train: 0.811889	val: 1.448821	test: 2.388795
MAE train: 0.590581	val: 1.035537	test: 1.791458

Epoch: 92
Loss: 1.1020635962486267
RMSE train: 0.861995	val: 1.570291	test: 2.451593
MAE train: 0.630951	val: 1.079636	test: 1.802750

Epoch: 93
Loss: 1.0033310651779175
RMSE train: 0.875680	val: 1.684135	test: 2.497387
MAE train: 0.646575	val: 1.164727	test: 1.827154

Epoch: 94
Loss: 0.9167066216468811
RMSE train: 0.827301	val: 1.609025	test: 2.486323
MAE train: 0.626688	val: 1.115111	test: 1.834116

Epoch: 95
Loss: 0.876528263092041
RMSE train: 0.795156	val: 1.485975	test: 2.415589
MAE train: 0.608174	val: 1.055401	test: 1.835844

Epoch: 96
Loss: 1.0476126074790955
RMSE train: 0.809930	val: 1.427673	test: 2.363070
MAE train: 0.599625	val: 1.069556	test: 1.829115

Epoch: 97
Loss: 1.0147798955440521
RMSE train: 0.810719	val: 1.417174	test: 2.351134
MAE train: 0.588094	val: 1.080622	test: 1.808778

Epoch: 98
Loss: 1.0080791115760803
RMSE train: 0.770641	val: 1.393208	test: 2.360792
MAE train: 0.560384	val: 1.017149	test: 1.798491

Epoch: 99
Loss: 0.9038025140762329
RMSE train: 0.771900	val: 1.470918	test: 2.420316
MAE train: 0.567778	val: 1.027419	test: 1.819144

Epoch: 100
Loss: 1.0705769658088684
RMSE train: 0.735763	val: 1.467106	test: 2.449125
MAE train: 0.541247	val: 1.027917	test: 1.874794

Epoch: 101
Loss: 1.0039487481117249
RMSE train: 0.703688	val: 1.417074	test: 2.411370
MAE train: 0.509112	val: 1.025362	test: 1.885059

Epoch: 102
Loss: 0.9435731470584869
RMSE train: 0.711202	val: 1.416992	test: 2.385592
MAE train: 0.509196	val: 1.090377	test: 1.894597

Epoch: 103
Loss: 1.1243594884872437
RMSE train: 0.807334	val: 1.423654	test: 2.361305
MAE train: 0.579867	val: 1.121802	test: 1.875768

Epoch: 104
Loss: 0.9486440122127533
RMSE train: 0.827765	val: 1.387282	test: 2.345361
MAE train: 0.582223	val: 1.072892	test: 1.857786

Epoch: 105
Loss: 1.0605076551437378
RMSE train: 0.787825	val: 1.369504	test: 2.343946
MAE train: 0.551545	val: 1.017502	test: 1.838535

Epoch: 106
Loss: 0.8729194402694702
RMSE train: 0.749344	val: 1.373889	test: 2.356539
MAE train: 0.533492	val: 0.999852	test: 1.822858

Epoch: 107
Loss: 0.9012333750724792
RMSE train: 0.698948	val: 1.360002	test: 2.375573
MAE train: 0.503435	val: 0.980612	test: 1.825568

Epoch: 108
Loss: 1.0525383949279785
RMSE train: 0.679713	val: 1.345086	test: 2.374430
MAE train: 0.486517	val: 0.989003	test: 1.831358

Epoch: 109
Loss: 0.8857695460319519
RMSE train: 0.688644	val: 1.349735	test: 2.375314
MAE train: 0.495149	val: 0.993413	test: 1.821828

Epoch: 110
Loss: 0.8300731778144836
RMSE train: 0.727651	val: 1.356647	test: 2.383345
MAE train: 0.520371	val: 0.995901	test: 1.815482

Epoch: 111
Loss: 0.8584782183170319
RMSE train: 0.812319	val: 1.397403	test: 2.391306
MAE train: 0.569958	val: 1.005978	test: 1.785301

Epoch: 112
Loss: 0.8174918293952942
RMSE train: 0.885733	val: 1.505059	test: 2.436603
MAE train: 0.623578	val: 1.064234	test: 1.778384

Epoch: 113
Loss: 0.8478063642978668
RMSE train: 0.915998	val: 1.544075	test: 2.459156
MAE train: 0.651559	val: 1.090168	test: 1.778865

Epoch: 114
Loss: 0.7366558909416199
RMSE train: 0.885204	val: 1.537512	test: 2.457621
MAE train: 0.641044	val: 1.085937	test: 1.795413

Epoch: 115
Loss: 1.0018028318881989
RMSE train: 0.809439	val: 1.503480	test: 2.435517
MAE train: 0.586995	val: 1.066293	test: 1.812547

Epoch: 116
Loss: 1.1559359431266785
RMSE train: 0.761528	val: 1.516041	test: 2.430713
MAE train: 0.552160	val: 1.074639	test: 1.798114

Epoch: 117
Loss: 0.7365090847015381
RMSE train: 0.738569	val: 1.549717	test: 2.431528
MAE train: 0.535501	val: 1.100154	test: 1.776841

Epoch: 118
Loss: 0.7848408222198486
RMSE train: 0.742071	val: 1.546894	test: 2.404453
MAE train: 0.539364	val: 1.104131	test: 1.753415

Epoch: 119
Loss: 0.7507036030292511
RMSE train: 0.834629	val: 1.515319	test: 2.366225
MAE train: 0.602613	val: 1.108393	test: 1.725778

Epoch: 120
Loss: 0.9189813137054443
RMSE train: 0.897477	val: 1.485267	test: 2.341346
MAE train: 0.629863	val: 1.128858	test: 1.740900

Epoch: 121
Loss: 1.0591736733913422
RMSE train: 0.851273	val: 1.466668	test: 2.320000
MAE train: 0.599010	val: 1.138747	test: 1.754831

Epoch: 122
Loss: 0.9046498835086823
RMSE train: 0.735832	val: 1.478685	test: 2.355796
MAE train: 0.526337	val: 1.125007	test: 1.787813

Epoch: 123
Loss: 0.7653075754642487
RMSE train: 0.704798	val: 1.579799	test: 2.448707
MAE train: 0.501678	val: 1.154085	test: 1.836895

Epoch: 124
Loss: 0.8273928463459015
RMSE train: 0.757681	val: 1.657483	test: 2.528305
MAE train: 0.534002	val: 1.181722	test: 1.903878

Epoch: 125
Loss: 0.8974075019359589
RMSE train: 0.743169	val: 1.562794	test: 2.491501
MAE train: 0.536359	val: 1.104542	test: 1.866356

Epoch: 126
Loss: 0.9234272241592407
RMSE train: 0.749839	val: 1.421150	test: 2.454547
MAE train: 0.543134	val: 1.017508	test: 1.829406

Epoch: 127
Loss: 0.8661082684993744
RMSE train: 0.780250	val: 1.392776	test: 2.445655
MAE train: 0.556965	val: 1.015873	test: 1.819513

Epoch: 128
Loss: 0.7650955319404602
RMSE train: 0.763700	val: 1.417666	test: 2.449236
MAE train: 0.552985	val: 1.030428	test: 1.816563

Epoch: 129
Loss: 0.8225550055503845
RMSE train: 0.726716	val: 1.456617	test: 2.442200
MAE train: 0.539016	val: 1.052851	test: 1.794673

Epoch: 130
Loss: 0.8411548733711243
RMSE train: 0.702058	val: 1.477358	test: 2.442086
MAE train: 0.520719	val: 1.066487	test: 1.787626

Epoch: 131
Loss: 0.8723510503768921
RMSE train: 0.713964	val: 1.478893	test: 2.437244
MAE train: 0.526696	val: 1.073962	test: 1.790269

Epoch: 132
Loss: 0.776453822851181
RMSE train: 0.711075	val: 1.466570	test: 2.446169
MAE train: 0.526053	val: 1.087733	test: 1.816854

Epoch: 133
Loss: 0.7880943715572357
RMSE train: 0.710603	val: 1.449883	test: 2.442857
MAE train: 0.526618	val: 1.085476	test: 1.819911

Epoch: 134
Loss: 0.8745352625846863
RMSE train: 0.721999	val: 1.433502	test: 2.431699
MAE train: 0.520880	val: 1.067313	test: 1.808189

Epoch: 135
Loss: 0.8905584216117859
RMSE train: 0.679436	val: 1.417148	test: 2.438758
MAE train: 0.482272	val: 1.040033	test: 1.807289

Epoch: 136
Loss: 0.8877259790897369
RMSE train: 0.675321	val: 1.430572	test: 2.447187
MAE train: 0.485802	val: 1.052592	test: 1.817763

Epoch: 137
Loss: 0.805247575044632
RMSE train: 0.670749	val: 1.450444	test: 2.474960
MAE train: 0.492323	val: 1.056657	test: 1.846615

Epoch: 138
Loss: 0.6918560266494751
RMSE train: 0.679810	val: 1.467882	test: 2.478119
MAE train: 0.505210	val: 1.060533	test: 1.844651

Epoch: 139
Loss: 0.7859064340591431
RMSE train: 0.675805	val: 1.473803	test: 2.472812
MAE train: 0.508678	val: 1.058972	test: 1.831481

Epoch: 140
Loss: 0.6106236279010773
RMSE train: 0.658878	val: 1.459944	test: 2.438111
MAE train: 0.500908	val: 1.057875	test: 1.808582

Epoch: 141
Loss: 0.7587361335754395
RMSE train: 0.613319	val: 1.426765	test: 2.422011
MAE train: 0.458109	val: 1.069001	test: 1.821581

Epoch: 142
Loss: 0.7032014429569244
RMSE train: 0.625074	val: 1.426596	test: 2.428641
MAE train: 0.457708	val: 1.115362	test: 1.868173

Epoch: 143
Loss: 0.7096813321113586
RMSE train: 0.704310	val: 1.418973	test: 2.449369
MAE train: 0.507754	val: 1.098318	test: 1.891663

Early stopping
Best (RMSE):	 train: 0.679713	val: 1.345086	test: 2.374430
Best (MAE):	 train: 0.486517	val: 0.989003	test: 1.831358


Epoch: 84
Loss: 0.8762989938259125
RMSE train: 0.822323	val: 1.522143	test: 2.437978
MAE train: 0.598888	val: 1.138336	test: 1.892866

Epoch: 85
Loss: 1.0383494198322296
RMSE train: 0.830878	val: 1.549163	test: 2.505371
MAE train: 0.616764	val: 1.134950	test: 1.934023

Epoch: 86
Loss: 1.1594369411468506
RMSE train: 0.859434	val: 1.561052	test: 2.538773
MAE train: 0.643208	val: 1.124858	test: 1.949945

Epoch: 87
Loss: 0.9113523066043854
RMSE train: 0.884273	val: 1.538758	test: 2.512063
MAE train: 0.660116	val: 1.104392	test: 1.926624

Epoch: 88
Loss: 0.865352213382721
RMSE train: 0.925719	val: 1.539896	test: 2.518684
MAE train: 0.683226	val: 1.106052	test: 1.915091

Epoch: 89
Loss: 0.9491780996322632
RMSE train: 0.965657	val: 1.524085	test: 2.525833
MAE train: 0.707567	val: 1.097465	test: 1.909006

Epoch: 90
Loss: 1.218048632144928
RMSE train: 1.001567	val: 1.534940	test: 2.538875
MAE train: 0.735379	val: 1.104590	test: 1.917781

Epoch: 91
Loss: 1.4517214894294739
RMSE train: 1.029987	val: 1.520586	test: 2.497176
MAE train: 0.739815	val: 1.105936	test: 1.863840

Epoch: 92
Loss: 1.0224509537220001
RMSE train: 1.052016	val: 1.519110	test: 2.453773
MAE train: 0.738004	val: 1.117378	test: 1.817644

Epoch: 93
Loss: 0.9528674185276031
RMSE train: 1.002852	val: 1.530248	test: 2.444893
MAE train: 0.697104	val: 1.125844	test: 1.822594

Epoch: 94
Loss: 1.0175884664058685
RMSE train: 0.913519	val: 1.516072	test: 2.422142
MAE train: 0.646993	val: 1.106721	test: 1.836308

Epoch: 95
Loss: 1.0178840160369873
RMSE train: 0.869240	val: 1.497216	test: 2.445024
MAE train: 0.635828	val: 1.084971	test: 1.864987

Epoch: 96
Loss: 1.1681543588638306
RMSE train: 0.852372	val: 1.496523	test: 2.521966
MAE train: 0.629576	val: 1.076728	test: 1.923746

Epoch: 97
Loss: 0.9256892502307892
RMSE train: 0.908218	val: 1.471160	test: 2.527491
MAE train: 0.665193	val: 1.086782	test: 1.941043

Epoch: 98
Loss: 1.0017524063587189
RMSE train: 0.959640	val: 1.472463	test: 2.518294
MAE train: 0.693572	val: 1.117133	test: 1.944998

Epoch: 99
Loss: 1.5600616931915283
RMSE train: 0.978549	val: 1.467815	test: 2.468438
MAE train: 0.685577	val: 1.128270	test: 1.908654

Epoch: 100
Loss: 0.9243639409542084
RMSE train: 0.965042	val: 1.484285	test: 2.491505
MAE train: 0.673184	val: 1.128244	test: 1.922309

Epoch: 101
Loss: 1.0118277072906494
RMSE train: 0.897175	val: 1.525173	test: 2.538684
MAE train: 0.641372	val: 1.130746	test: 1.945146

Epoch: 102
Loss: 0.8839148581027985
RMSE train: 0.850344	val: 1.579704	test: 2.618894
MAE train: 0.616899	val: 1.146476	test: 1.996068

Epoch: 103
Loss: 0.9312872290611267
RMSE train: 0.880181	val: 1.666738	test: 2.676389
MAE train: 0.633030	val: 1.214496	test: 2.065995

Epoch: 104
Loss: 1.0719001293182373
RMSE train: 0.959716	val: 1.724353	test: 2.675003
MAE train: 0.682251	val: 1.260189	test: 2.033863

Epoch: 105
Loss: 0.8823832273483276
RMSE train: 1.064311	val: 1.668406	test: 2.648382
MAE train: 0.757590	val: 1.223788	test: 1.979848

Epoch: 106
Loss: 1.4267259240150452
RMSE train: 1.108788	val: 1.610882	test: 2.614069
MAE train: 0.779868	val: 1.197983	test: 1.959887

Epoch: 107
Loss: 0.8942879438400269
RMSE train: 1.024867	val: 1.522014	test: 2.515763
MAE train: 0.734403	val: 1.151839	test: 1.882602

Epoch: 108
Loss: 1.0036565363407135
RMSE train: 0.953980	val: 1.512411	test: 2.500907
MAE train: 0.693131	val: 1.142634	test: 1.875869

Epoch: 109
Loss: 0.9564691483974457
RMSE train: 0.881257	val: 1.512857	test: 2.520239
MAE train: 0.659167	val: 1.122602	test: 1.909193

Epoch: 110
Loss: 0.9049384593963623
RMSE train: 0.868158	val: 1.514318	test: 2.538072
MAE train: 0.658769	val: 1.109405	test: 1.921402

Epoch: 111
Loss: 1.0465344488620758
RMSE train: 0.812432	val: 1.461450	test: 2.492760
MAE train: 0.615519	val: 1.077216	test: 1.943724

Epoch: 112
Loss: 1.0784231424331665
RMSE train: 0.793989	val: 1.449104	test: 2.446274
MAE train: 0.591766	val: 1.084713	test: 1.919129

Epoch: 113
Loss: 0.8645461797714233
RMSE train: 0.861149	val: 1.468500	test: 2.410284
MAE train: 0.614328	val: 1.116116	test: 1.877137

Epoch: 114
Loss: 0.88850137591362
RMSE train: 0.908382	val: 1.503559	test: 2.403584
MAE train: 0.639704	val: 1.143446	test: 1.858630

Epoch: 115
Loss: 0.8845248818397522
RMSE train: 0.938755	val: 1.567232	test: 2.437422
MAE train: 0.666061	val: 1.176069	test: 1.870806

Epoch: 116
Loss: 0.9756004214286804
RMSE train: 0.893696	val: 1.619269	test: 2.479241
MAE train: 0.652270	val: 1.200337	test: 1.909153

Epoch: 117
Loss: 0.8079971373081207
RMSE train: 0.870598	val: 1.651881	test: 2.535335
MAE train: 0.644419	val: 1.211081	test: 1.956246

Epoch: 118
Loss: 0.8817789256572723
RMSE train: 0.844397	val: 1.630027	test: 2.562488
MAE train: 0.619297	val: 1.181572	test: 1.985246

Epoch: 119
Loss: 1.080579400062561
RMSE train: 0.824943	val: 1.554012	test: 2.543129
MAE train: 0.605840	val: 1.125549	test: 1.972615

Epoch: 120
Loss: 0.8408418297767639
RMSE train: 0.835118	val: 1.537665	test: 2.547408
MAE train: 0.611455	val: 1.131964	test: 1.960503

Epoch: 121
Loss: 0.9044007658958435
RMSE train: 0.850291	val: 1.525070	test: 2.509938
MAE train: 0.607972	val: 1.142385	test: 1.935028

Epoch: 122
Loss: 0.6378918290138245
RMSE train: 0.859653	val: 1.523308	test: 2.450895
MAE train: 0.598798	val: 1.157750	test: 1.885075

Epoch: 123
Loss: 0.7795452773571014
RMSE train: 0.866801	val: 1.530492	test: 2.406185
MAE train: 0.603399	val: 1.171137	test: 1.846876

Epoch: 124
Loss: 0.8537895083427429
RMSE train: 0.855562	val: 1.548171	test: 2.368829
MAE train: 0.605587	val: 1.186759	test: 1.810772

Epoch: 125
Loss: 0.786877304315567
RMSE train: 0.800688	val: 1.567905	test: 2.397562
MAE train: 0.578620	val: 1.187412	test: 1.843703

Epoch: 126
Loss: 1.3553268313407898
RMSE train: 0.793193	val: 1.575757	test: 2.482348
MAE train: 0.583286	val: 1.183417	test: 1.930596

Epoch: 127
Loss: 0.6679984629154205
RMSE train: 0.768929	val: 1.545639	test: 2.539293
MAE train: 0.565442	val: 1.149557	test: 1.987299

Epoch: 128
Loss: 0.9805779457092285
RMSE train: 0.766442	val: 1.503574	test: 2.530882
MAE train: 0.567366	val: 1.110521	test: 1.995224

Epoch: 129
Loss: 0.7505461871623993
RMSE train: 0.806011	val: 1.494863	test: 2.492412
MAE train: 0.588584	val: 1.118426	test: 1.947150

Epoch: 130
Loss: 0.7120536863803864
RMSE train: 0.870073	val: 1.487890	test: 2.450256
MAE train: 0.612504	val: 1.129405	test: 1.892378

Epoch: 131
Loss: 0.7429482638835907
RMSE train: 0.920021	val: 1.510250	test: 2.435006
MAE train: 0.633098	val: 1.154094	test: 1.855163

Epoch: 132
Loss: 0.9908786416053772
RMSE train: 0.915401	val: 1.546160	test: 2.453826
MAE train: 0.640060	val: 1.181280	test: 1.869087

Epoch: 133
Loss: 0.8215137124061584
RMSE train: 0.854674	val: 1.584487	test: 2.516805
MAE train: 0.627948	val: 1.202405	test: 1.936566

Epoch: 134
Loss: 0.8269738554954529
RMSE train: 0.800545	val: 1.609518	test: 2.573040
MAE train: 0.603346	val: 1.210636	test: 1.986631

Epoch: 135
Loss: 0.6871972978115082
RMSE train: 0.794890	val: 1.588911	test: 2.578235
MAE train: 0.601214	val: 1.180092	test: 1.988868

Epoch: 136
Loss: 1.0241854786872864
RMSE train: 0.830291	val: 1.549502	test: 2.533987
MAE train: 0.627467	val: 1.141046	test: 1.955546

Epoch: 137
Loss: 0.7867758572101593
RMSE train: 0.832602	val: 1.526427	test: 2.522478
MAE train: 0.634933	val: 1.124196	test: 1.947711

Epoch: 138
Loss: 0.6716162264347076
RMSE train: 0.822704	val: 1.527507	test: 2.509419
MAE train: 0.615465	val: 1.134974	test: 1.938984

Epoch: 139
Loss: 0.771809458732605
RMSE train: 0.852735	val: 1.534895	test: 2.475139
MAE train: 0.615217	val: 1.152419	test: 1.895141

Epoch: 140
Loss: 0.7829376459121704
RMSE train: 0.832821	val: 1.521143	test: 2.413919
MAE train: 0.591765	val: 1.146693	test: 1.834857

Epoch: 141
Loss: 0.7038047015666962
RMSE train: 0.760464	val: 1.501099	test: 2.397042
MAE train: 0.553158	val: 1.119430	test: 1.838742

Epoch: 142
Loss: 0.8466951847076416
RMSE train: 0.728049	val: 1.506597	test: 2.445868
MAE train: 0.541568	val: 1.112758	test: 1.885119

Epoch: 143
Loss: 0.6735782325267792
RMSE train: 0.734787	val: 1.515358	test: 2.483556
MAE train: 0.552314	val: 1.108901	test: 1.915828

Epoch: 144
Loss: 0.7518912255764008
RMSE train: 0.761827	val: 1.546226	test: 2.504781
MAE train: 0.570052	val: 1.131484	test: 1.919287

Epoch: 145
Loss: 0.744397759437561
RMSE train: 0.766309	val: 1.575350	test: 2.489381
MAE train: 0.566403	val: 1.156469	test: 1.894596

Epoch: 146
Loss: 0.7430062592029572
RMSE train: 0.754776	val: 1.548794	test: 2.414988
MAE train: 0.552832	val: 1.139157	test: 1.835060

Epoch: 147
Loss: 0.8650337159633636
RMSE train: 0.773528	val: 1.528102	test: 2.381670
MAE train: 0.565565	val: 1.120960	test: 1.813870

Early stopping
Best (RMSE):	 train: 0.793989	val: 1.449104	test: 2.446274
Best (MAE):	 train: 0.591766	val: 1.084713	test: 1.919129
All runs completed.


Epoch: 84
Loss: 0.8917718231678009
RMSE train: 0.827394	val: 3.872226	test: 3.254237
MAE train: 0.623431	val: 2.839915	test: 2.561547

Epoch: 85
Loss: 0.9286053478717804
RMSE train: 0.844549	val: 3.922930	test: 3.212281
MAE train: 0.639346	val: 2.870518	test: 2.520855

Epoch: 86
Loss: 0.8533622026443481
RMSE train: 0.887404	val: 4.032654	test: 3.202332
MAE train: 0.662474	val: 2.919466	test: 2.484081

Epoch: 87
Loss: 0.8010091483592987
RMSE train: 0.882890	val: 4.060180	test: 3.169405
MAE train: 0.645065	val: 2.916722	test: 2.444422

Epoch: 88
Loss: 0.8173507750034332
RMSE train: 0.838631	val: 4.022847	test: 3.144377
MAE train: 0.604918	val: 2.895272	test: 2.415377

Epoch: 89
Loss: 0.9579707980155945
RMSE train: 0.805028	val: 3.963922	test: 3.129190
MAE train: 0.589247	val: 2.855416	test: 2.412703

Epoch: 90
Loss: 0.953993558883667
RMSE train: 0.774999	val: 3.882641	test: 3.124282
MAE train: 0.579235	val: 2.784577	test: 2.407368

Epoch: 91
Loss: 0.856018602848053
RMSE train: 0.789061	val: 3.853719	test: 3.124448
MAE train: 0.584249	val: 2.717095	test: 2.388207

Epoch: 92
Loss: 0.9617680907249451
RMSE train: 0.825201	val: 3.837307	test: 3.126914
MAE train: 0.611146	val: 2.676962	test: 2.388373

Epoch: 93
Loss: 0.8589887320995331
RMSE train: 0.859058	val: 3.844349	test: 3.115197
MAE train: 0.646080	val: 2.681412	test: 2.389185

Epoch: 94
Loss: 0.8230685591697693
RMSE train: 0.834878	val: 3.830241	test: 3.092045
MAE train: 0.640574	val: 2.667856	test: 2.362538

Epoch: 95
Loss: 0.8319245278835297
RMSE train: 0.787595	val: 3.802061	test: 3.041853
MAE train: 0.603267	val: 2.628348	test: 2.301839

Epoch: 96
Loss: 0.8081133365631104
RMSE train: 0.741495	val: 3.776959	test: 3.008418
MAE train: 0.565936	val: 2.591078	test: 2.260589

Epoch: 97
Loss: 0.8654418289661407
RMSE train: 0.668254	val: 3.737321	test: 2.964707
MAE train: 0.506656	val: 2.545442	test: 2.228760

Epoch: 98
Loss: 0.922514408826828
RMSE train: 0.600114	val: 3.733080	test: 2.927850
MAE train: 0.453903	val: 2.558720	test: 2.233391

Epoch: 99
Loss: 0.7357716262340546
RMSE train: 0.601821	val: 3.741319	test: 2.926352
MAE train: 0.454965	val: 2.572077	test: 2.232535

Epoch: 100
Loss: 0.9044665396213531
RMSE train: 0.641129	val: 3.791693	test: 2.941482
MAE train: 0.488825	val: 2.592678	test: 2.221169

Epoch: 101
Loss: 0.8757918775081635
RMSE train: 0.691671	val: 3.863406	test: 2.972777
MAE train: 0.539908	val: 2.618089	test: 2.194016

Epoch: 102
Loss: 0.8444811701774597
RMSE train: 0.774449	val: 3.940372	test: 3.017657
MAE train: 0.605133	val: 2.685641	test: 2.225030

Epoch: 103
Loss: 0.9307861030101776
RMSE train: 0.802815	val: 3.991649	test: 3.027370
MAE train: 0.625946	val: 2.747345	test: 2.262081

Epoch: 104
Loss: 0.8611259162425995
RMSE train: 0.767499	val: 3.991558	test: 3.028280
MAE train: 0.588070	val: 2.775739	test: 2.294407

Epoch: 105
Loss: 0.801550418138504
RMSE train: 0.730647	val: 3.960223	test: 3.044571
MAE train: 0.551483	val: 2.798861	test: 2.337671

Epoch: 106
Loss: 0.8390072882175446
RMSE train: 0.703264	val: 3.915652	test: 3.044099
MAE train: 0.521987	val: 2.791195	test: 2.355571

Epoch: 107
Loss: 0.7260855436325073
RMSE train: 0.700976	val: 3.893871	test: 3.024361
MAE train: 0.519336	val: 2.783457	test: 2.332104

Epoch: 108
Loss: 0.8833453059196472
RMSE train: 0.729403	val: 3.894958	test: 3.033384
MAE train: 0.541943	val: 2.774857	test: 2.331134

Epoch: 109
Loss: 0.8359117209911346
RMSE train: 0.799988	val: 3.946095	test: 3.025547
MAE train: 0.596464	val: 2.793167	test: 2.304332

Epoch: 110
Loss: 0.860256552696228
RMSE train: 0.815332	val: 3.958446	test: 3.041146
MAE train: 0.612771	val: 2.808370	test: 2.331426

Epoch: 111
Loss: 0.7698299288749695
RMSE train: 0.837286	val: 3.943839	test: 3.061999
MAE train: 0.627880	val: 2.805267	test: 2.369602

Epoch: 112
Loss: 0.8166865706443787
RMSE train: 0.804767	val: 3.914926	test: 3.043448
MAE train: 0.598841	val: 2.781669	test: 2.355800

Epoch: 113
Loss: 0.8306663036346436
RMSE train: 0.759132	val: 3.908188	test: 3.012010
MAE train: 0.560064	val: 2.753208	test: 2.313953

Epoch: 114
Loss: 0.8732117712497711
RMSE train: 0.750830	val: 3.907365	test: 2.993688
MAE train: 0.552930	val: 2.729628	test: 2.290467

Epoch: 115
Loss: 0.7956354320049286
RMSE train: 0.731882	val: 3.887355	test: 2.952821
MAE train: 0.542620	val: 2.693459	test: 2.259762

Epoch: 116
Loss: 0.8313593566417694
RMSE train: 0.723859	val: 3.896730	test: 2.921348
MAE train: 0.533565	val: 2.690694	test: 2.253899

Epoch: 117
Loss: 0.8127713799476624
RMSE train: 0.757629	val: 3.921421	test: 2.920696
MAE train: 0.551048	val: 2.712428	test: 2.249351

Epoch: 118
Loss: 0.8293370604515076
RMSE train: 0.755323	val: 3.881701	test: 2.904593
MAE train: 0.551373	val: 2.686450	test: 2.229083

Epoch: 119
Loss: 0.8218437135219574
RMSE train: 0.722593	val: 3.849497	test: 2.897503
MAE train: 0.542118	val: 2.648207	test: 2.204904

Epoch: 120
Loss: 0.7549315690994263
RMSE train: 0.691272	val: 3.818939	test: 2.928863
MAE train: 0.529324	val: 2.605115	test: 2.218442

Epoch: 121
Loss: 0.7139501869678497
RMSE train: 0.719098	val: 3.822838	test: 2.999833
MAE train: 0.555583	val: 2.588866	test: 2.249477

Epoch: 122
Loss: 0.8654504120349884
RMSE train: 0.732944	val: 3.833126	test: 3.065473
MAE train: 0.565687	val: 2.590015	test: 2.323084

Epoch: 123
Loss: 0.7577159404754639
RMSE train: 0.720725	val: 3.799487	test: 3.052211
MAE train: 0.551698	val: 2.577377	test: 2.352748

Epoch: 124
Loss: 0.7773743271827698
RMSE train: 0.694342	val: 3.745906	test: 2.996789
MAE train: 0.526772	val: 2.570388	test: 2.331888

Epoch: 125
Loss: 0.9561052918434143
RMSE train: 0.685107	val: 3.695895	test: 2.967079
MAE train: 0.514361	val: 2.552712	test: 2.298817

Epoch: 126
Loss: 0.7688136398792267
RMSE train: 0.698987	val: 3.700200	test: 2.954844
MAE train: 0.520971	val: 2.561111	test: 2.273161

Epoch: 127
Loss: 0.7690139412879944
RMSE train: 0.714434	val: 3.746294	test: 2.983460
MAE train: 0.531939	val: 2.594623	test: 2.304926

Epoch: 128
Loss: 0.849001556634903
RMSE train: 0.752364	val: 3.812605	test: 3.030455
MAE train: 0.570089	val: 2.629609	test: 2.351673

Epoch: 129
Loss: 0.7186625599861145
RMSE train: 0.766723	val: 3.832307	test: 3.083904
MAE train: 0.591817	val: 2.631046	test: 2.395455

Epoch: 130
Loss: 0.7745942771434784
RMSE train: 0.763231	val: 3.809246	test: 3.098087
MAE train: 0.588534	val: 2.621410	test: 2.402604

Epoch: 131
Loss: 0.7154523134231567
RMSE train: 0.749403	val: 3.799033	test: 3.089509
MAE train: 0.577332	val: 2.616232	test: 2.397797

Epoch: 132
Loss: 0.799759030342102
RMSE train: 0.742592	val: 3.794174	test: 3.050561
MAE train: 0.575131	val: 2.660181	test: 2.398881

Epoch: 133
Loss: 0.7278031408786774
RMSE train: 0.741085	val: 3.809770	test: 3.015606
MAE train: 0.571612	val: 2.694788	test: 2.381650

Epoch: 134
Loss: 0.6852856576442719
RMSE train: 0.736770	val: 3.824959	test: 3.007671
MAE train: 0.564779	val: 2.697388	test: 2.355522

Epoch: 135
Loss: 0.692944347858429
RMSE train: 0.798268	val: 3.868098	test: 3.043529
MAE train: 0.608450	val: 2.707770	test: 2.360956

Epoch: 136
Loss: 0.6574076116085052
RMSE train: 0.827025	val: 3.913636	test: 3.054128
MAE train: 0.635424	val: 2.734498	test: 2.360640

Epoch: 137
Loss: 0.6501483619213104
RMSE train: 0.795842	val: 3.899911	test: 3.055166
MAE train: 0.617253	val: 2.730793	test: 2.374883

Epoch: 138
Loss: 0.7610062658786774
RMSE train: 0.724946	val: 3.852490	test: 3.046617
MAE train: 0.561301	val: 2.701540	test: 2.383712

Epoch: 139
Loss: 0.5924021601676941
RMSE train: 0.682586	val: 3.835470	test: 3.052503
MAE train: 0.526971	val: 2.663668	test: 2.387237

Epoch: 140
Loss: 0.692484050989151
RMSE train: 0.701131	val: 3.849485	test: 3.039379
MAE train: 0.540027	val: 2.655450	test: 2.348674

Epoch: 141
Loss: 0.7370710372924805
RMSE train: 0.730193	val: 3.887465	test: 2.993359
MAE train: 0.555918	val: 2.673619	test: 2.291180

Epoch: 142
Loss: 0.5629687607288361
RMSE train: 0.762777	val: 3.949481	test: 2.950011
MAE train: 0.562177	val: 2.703475	test: 2.246002

Epoch: 143
Loss: 0.7428092956542969
RMSE train: 0.789405	val: 4.004140	test: 2.950333
MAE train: 0.578639	val: 2.733202	test: 2.231825

Epoch: 144
Loss: 0.7183832824230194
RMSE train: 0.749610	val: 3.978502	test: 2.965152
MAE train: 0.561910	val: 2.721455	test: 2.225365

Epoch: 145
Loss: 0.6951724290847778
RMSE train: 0.696675	val: 3.950919	test: 2.988902
MAE train: 0.532335	val: 2.704586	test: 2.230971

Epoch: 146
Loss: 0.7531589269638062
RMSE train: 0.651781	val: 3.946392	test: 2.974825
MAE train: 0.507059	val: 2.723372	test: 2.235853

Epoch: 147
Loss: 0.6684938967227936
RMSE train: 0.614159	val: 3.921558	test: 2.990111
MAE train: 0.480308	val: 2.713131	test: 2.258282

Epoch: 148
Loss: 0.5741881430149078
RMSE train: 0.630602	val: 3.911298	test: 2.990802
MAE train: 0.495102	val: 2.689862	test: 2.249974

Epoch: 149
Loss: 0.6014310717582703
RMSE train: 0.673114	val: 3.918762	test: 2.998602
MAE train: 0.530252	val: 2.683659	test: 2.258594

Epoch: 150
Loss: 0.5972038507461548
RMSE train: 0.706430	val: 3.944949	test: 3.008653
MAE train: 0.557233	val: 2.696259	test: 2.283761

Epoch: 151
Loss: 0.7044619023799896
RMSE train: 0.749157	val: 3.979843	test: 3.015752
MAE train: 0.583179	val: 2.727404	test: 2.305191

Epoch: 152
Loss: 0.7008716762065887
RMSE train: 0.789113	val: 3.998208	test: 3.023139
MAE train: 0.602972	val: 2.744479	test: 2.321680

Epoch: 153
Loss: 0.5918621718883514
RMSE train: 0.811236	val: 3.996323	test: 3.001049
MAE train: 0.606000	val: 2.760082	test: 2.296205

Epoch: 154
Loss: 0.6398774981498718
RMSE train: 0.756497	val: 3.942508	test: 2.974253
MAE train: 0.556595	val: 2.722204	test: 2.276767

Epoch: 155
Loss: 0.6246232092380524
RMSE train: 0.711712	val: 3.879437	test: 2.992189
MAE train: 0.532909	val: 2.666055	test: 2.294819

Epoch: 156
Loss: 0.5450491905212402
RMSE train: 0.718047	val: 3.870629	test: 3.035816
MAE train: 0.549203	val: 2.647373	test: 2.333491

Epoch: 157
Loss: 0.5718747675418854
RMSE train: 0.716277	val: 3.864358	test: 3.039939
MAE train: 0.552952	val: 2.647338	test: 2.342774

Epoch: 158
Loss: 0.6310555338859558
RMSE train: 0.728419	val: 3.879909	test: 3.011585
MAE train: 0.563432	val: 2.657389	test: 2.321080

Epoch: 159
Loss: 0.6103565394878387
RMSE train: 0.700646	val: 3.884292	test: 2.995327
MAE train: 0.540413	val: 2.636601	test: 2.297386

Epoch: 160
Loss: 0.6361745595932007
RMSE train: 0.675337	val: 3.887172	test: 2.975456
MAE train: 0.516649	val: 2.619673	test: 2.257790

Early stopping
Best (RMSE):	 train: 0.685107	val: 3.695895	test: 2.967079
Best (MAE):	 train: 0.514361	val: 2.552712	test: 2.298817
All runs completed.
