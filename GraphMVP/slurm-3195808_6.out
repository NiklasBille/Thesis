>>> Starting run for dataset: lipo
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running RANDOM configs_split_experiments/GraphCL/lipo/random/train_prop=0.6.yml on cuda:0
Running RANDOM configs_split_experiments/GraphCL/lipo/random/train_prop=0.7.yml on cuda:1
Running RANDOM configs_split_experiments/GraphCL/lipo/random/train_prop=0.8.yml on cuda:2
Starting process for seed 1: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/lipo/random/train_prop=0.6.yml --runseed 1 --device cuda:0
Starting process for seed 2: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/lipo/random/train_prop=0.6.yml --runseed 2 --device cuda:0
Starting process for seed 1: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/lipo/random/train_prop=0.7.yml --runseed 1 --device cuda:1
Starting process for seed 3: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/lipo/random/train_prop=0.6.yml --runseed 3 --device cuda:0
Starting process for seed 1: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/lipo/random/train_prop=0.8.yml --runseed 1 --device cuda:2
Starting process for seed 2: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/lipo/random/train_prop=0.7.yml --runseed 2 --device cuda:1
Starting process for seed 2: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/lipo/random/train_prop=0.8.yml --runseed 2 --device cuda:2
Starting process for seed 3: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/lipo/random/train_prop=0.7.yml --runseed 3 --device cuda:1
Starting process for seed 3: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/lipo/random/train_prop=0.8.yml --runseed 3 --device cuda:2
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/lipo/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/lipo/random/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/lipo/random/train_prop=0.6/lipophilicity_random_1_20-05_14-43-15  ]
[ Using Seed :  1  ]
[ Using device :  cuda:0  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.198956394195557
RMSE train: 2.528258	val: 2.566182	test: 2.571322
MAE train: 2.279379	val: 2.317169	test: 2.309764

Epoch: 2
Loss: 3.7777397632598877
RMSE train: 2.176392	val: 2.208227	test: 2.211374
MAE train: 1.931091	val: 1.967951	test: 1.951262

Epoch: 3
Loss: 2.6545289278030397
RMSE train: 1.884642	val: 1.931248	test: 1.905727
MAE train: 1.663818	val: 1.708757	test: 1.671265

Epoch: 4
Loss: 1.8376079082489014
RMSE train: 1.330900	val: 1.385979	test: 1.364231
MAE train: 1.133769	val: 1.173621	test: 1.158114

Epoch: 5
Loss: 1.2567065477371215
RMSE train: 1.164057	val: 1.213232	test: 1.193939
MAE train: 0.977902	val: 1.015709	test: 1.005791

Epoch: 6
Loss: 0.991954904794693
RMSE train: 0.912140	val: 0.962590	test: 0.970071
MAE train: 0.746812	val: 0.774578	test: 0.789559

Epoch: 7
Loss: 0.8902863979339599
RMSE train: 0.863290	val: 0.914538	test: 0.937385
MAE train: 0.695585	val: 0.722051	test: 0.750148

Epoch: 8
Loss: 0.8301971137523652
RMSE train: 0.810824	val: 0.861490	test: 0.889957
MAE train: 0.648497	val: 0.673652	test: 0.708571

Epoch: 9
Loss: 0.7864652037620544
RMSE train: 0.806915	val: 0.836580	test: 0.882750
MAE train: 0.637009	val: 0.652065	test: 0.697810

Epoch: 10
Loss: 0.7592356383800507
RMSE train: 0.796153	val: 0.838978	test: 0.878583
MAE train: 0.630169	val: 0.653096	test: 0.688878

Epoch: 11
Loss: 0.7274158298969269
RMSE train: 0.788192	val: 0.826273	test: 0.870229
MAE train: 0.618253	val: 0.639690	test: 0.681945

Epoch: 12
Loss: 0.6999590754508972
RMSE train: 0.769230	val: 0.815827	test: 0.853318
MAE train: 0.605484	val: 0.631488	test: 0.667701

Epoch: 13
Loss: 0.7159278452396393
RMSE train: 0.783071	val: 0.824364	test: 0.864945
MAE train: 0.617646	val: 0.637842	test: 0.679841

Epoch: 14
Loss: 0.6732850193977356
RMSE train: 0.746957	val: 0.810643	test: 0.835326
MAE train: 0.593798	val: 0.626519	test: 0.653815

Epoch: 15
Loss: 0.6688972294330597
RMSE train: 0.751860	val: 0.804993	test: 0.846723
MAE train: 0.595157	val: 0.617896	test: 0.663383

Epoch: 16
Loss: 0.6695107519626617
RMSE train: 0.764396	val: 0.814929	test: 0.857479
MAE train: 0.595454	val: 0.632948	test: 0.660684

Epoch: 17
Loss: 0.6720478117465973
RMSE train: 0.718208	val: 0.779753	test: 0.817798
MAE train: 0.567300	val: 0.595842	test: 0.637425

Epoch: 18
Loss: 0.6426470637321472
RMSE train: 0.717120	val: 0.775677	test: 0.823826
MAE train: 0.562640	val: 0.596359	test: 0.635231

Epoch: 19
Loss: 0.6082256853580474
RMSE train: 0.723779	val: 0.786731	test: 0.819207
MAE train: 0.567454	val: 0.596241	test: 0.632624

Epoch: 20
Loss: 0.6012244105339051
RMSE train: 0.697326	val: 0.773081	test: 0.800660
MAE train: 0.548758	val: 0.587100	test: 0.622090

Epoch: 21
Loss: 0.6165443897247315
RMSE train: 0.711829	val: 0.791910	test: 0.811253
MAE train: 0.562259	val: 0.604553	test: 0.629416

Epoch: 22
Loss: 0.6033586144447327
RMSE train: 0.702867	val: 0.794087	test: 0.809573
MAE train: 0.558933	val: 0.609691	test: 0.635073Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/lipo/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/lipo/random/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/lipo/random/train_prop=0.6/lipophilicity_random_3_20-05_14-43-15  ]
[ Using Seed :  3  ]
[ Using device :  cuda:0  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.4625664234161375
RMSE train: 2.709887	val: 2.749278	test: 2.732408
MAE train: 2.480494	val: 2.523171	test: 2.483899

Epoch: 2
Loss: 4.0085282802581785
RMSE train: 2.283679	val: 2.314249	test: 2.304676
MAE train: 2.046190	val: 2.084742	test: 2.059484

Epoch: 3
Loss: 2.890876317024231
RMSE train: 1.749006	val: 1.795120	test: 1.774965
MAE train: 1.533320	val: 1.577628	test: 1.547765

Epoch: 4
Loss: 2.062778949737549
RMSE train: 1.424784	val: 1.459744	test: 1.461729
MAE train: 1.220145	val: 1.244592	test: 1.243903

Epoch: 5
Loss: 1.3882454991340638
RMSE train: 1.024030	val: 1.062509	test: 1.080111
MAE train: 0.841794	val: 0.866783	test: 0.894747

Epoch: 6
Loss: 1.0527400076389313
RMSE train: 0.905004	val: 0.932555	test: 0.978635
MAE train: 0.727220	val: 0.743133	test: 0.790320

Epoch: 7
Loss: 0.8868855953216552
RMSE train: 0.875352	val: 0.895474	test: 0.943850
MAE train: 0.688799	val: 0.698010	test: 0.752715

Epoch: 8
Loss: 0.8045756101608277
RMSE train: 0.823679	val: 0.835944	test: 0.906571
MAE train: 0.647812	val: 0.647988	test: 0.714764

Epoch: 9
Loss: 0.787380862236023
RMSE train: 0.824540	val: 0.830453	test: 0.899672
MAE train: 0.646317	val: 0.639863	test: 0.709682

Epoch: 10
Loss: 0.7705463647842408
RMSE train: 0.779692	val: 0.804332	test: 0.858769
MAE train: 0.615379	val: 0.618213	test: 0.681869

Epoch: 11
Loss: 0.7727581858634949
RMSE train: 0.800243	val: 0.819250	test: 0.890157
MAE train: 0.629646	val: 0.636158	test: 0.701990

Epoch: 12
Loss: 0.7512837648391724
RMSE train: 0.767621	val: 0.802091	test: 0.857173
MAE train: 0.607918	val: 0.619555	test: 0.676161

Epoch: 13
Loss: 0.7095535337924957
RMSE train: 0.809407	val: 0.822084	test: 0.894236
MAE train: 0.635866	val: 0.636774	test: 0.700756

Epoch: 14
Loss: 0.6658334374427796
RMSE train: 0.744698	val: 0.774985	test: 0.841154
MAE train: 0.586258	val: 0.594122	test: 0.654425

Epoch: 15
Loss: 0.7007803380489349
RMSE train: 0.748060	val: 0.785831	test: 0.854229
MAE train: 0.593375	val: 0.609352	test: 0.670145

Epoch: 16
Loss: 0.654374611377716
RMSE train: 0.721817	val: 0.777465	test: 0.827069
MAE train: 0.572842	val: 0.591099	test: 0.651946

Epoch: 17
Loss: 0.660271418094635
RMSE train: 0.728705	val: 0.787982	test: 0.835159
MAE train: 0.581810	val: 0.607382	test: 0.660873

Epoch: 18
Loss: 0.638892388343811
RMSE train: 0.713592	val: 0.766902	test: 0.817751
MAE train: 0.564053	val: 0.592321	test: 0.642311

Epoch: 19
Loss: 0.6359213769435883
RMSE train: 0.722338	val: 0.770102	test: 0.822755
MAE train: 0.571452	val: 0.586636	test: 0.643461

Epoch: 20
Loss: 0.5834037125110626
RMSE train: 0.739255	val: 0.782832	test: 0.850817
MAE train: 0.578758	val: 0.602727	test: 0.657650

Epoch: 21
Loss: 0.6020999372005462
RMSE train: 0.691166	val: 0.751435	test: 0.804625
MAE train: 0.544750	val: 0.572880	test: 0.628930

Epoch: 22
Loss: 0.5958129644393921
RMSE train: 0.687630	val: 0.752393	test: 0.797378
MAE train: 0.544361	val: 0.576140	test: 0.617775Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/lipo/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/lipo/random/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/lipo/random/train_prop=0.6/lipophilicity_random_2_20-05_14-43-15  ]
[ Using Seed :  2  ]
[ Using device :  cuda:0  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.077248811721802
RMSE train: 2.345097	val: 2.380515	test: 2.384853
MAE train: 2.107499	val: 2.146112	test: 2.132396

Epoch: 2
Loss: 3.6744734764099123
RMSE train: 2.050937	val: 2.092078	test: 2.082805
MAE train: 1.817560	val: 1.858348	test: 1.832247

Epoch: 3
Loss: 2.6026658535003664
RMSE train: 1.729630	val: 1.781779	test: 1.768899
MAE train: 1.513884	val: 1.561521	test: 1.533014

Epoch: 4
Loss: 1.7772241473197936
RMSE train: 1.262314	val: 1.321458	test: 1.302438
MAE train: 1.072714	val: 1.108559	test: 1.097266

Epoch: 5
Loss: 1.265678083896637
RMSE train: 1.016606	val: 1.062762	test: 1.072094
MAE train: 0.841943	val: 0.872787	test: 0.883440

Epoch: 6
Loss: 0.9983556807041168
RMSE train: 0.879302	val: 0.912448	test: 0.945610
MAE train: 0.705136	val: 0.726120	test: 0.751558

Epoch: 7
Loss: 0.8722727954387665
RMSE train: 0.838270	val: 0.874242	test: 0.914316
MAE train: 0.665016	val: 0.684776	test: 0.723671

Epoch: 8
Loss: 0.8033272743225097
RMSE train: 0.820897	val: 0.850876	test: 0.902782
MAE train: 0.645157	val: 0.657538	test: 0.706295

Epoch: 9
Loss: 0.8017034590244293
RMSE train: 0.822844	val: 0.856078	test: 0.900760
MAE train: 0.646370	val: 0.665884	test: 0.710668

Epoch: 10
Loss: 0.761237895488739
RMSE train: 0.808660	val: 0.855364	test: 0.876412
MAE train: 0.639149	val: 0.660933	test: 0.695723

Epoch: 11
Loss: 0.7048611998558044
RMSE train: 0.811265	val: 0.851068	test: 0.881946
MAE train: 0.635008	val: 0.658539	test: 0.692655

Epoch: 12
Loss: 0.7182619094848632
RMSE train: 0.767142	val: 0.815304	test: 0.854319
MAE train: 0.606162	val: 0.628578	test: 0.674616

Epoch: 13
Loss: 0.7437432944774628
RMSE train: 0.746489	val: 0.790029	test: 0.835777
MAE train: 0.589328	val: 0.605703	test: 0.657006

Epoch: 14
Loss: 0.6828013122081756
RMSE train: 0.761122	val: 0.806795	test: 0.845454
MAE train: 0.591700	val: 0.620904	test: 0.660093

Epoch: 15
Loss: 0.6669139802455902
RMSE train: 0.758646	val: 0.813760	test: 0.842707
MAE train: 0.595903	val: 0.623169	test: 0.658610

Epoch: 16
Loss: 0.6721849083900452
RMSE train: 0.761250	val: 0.804526	test: 0.847756
MAE train: 0.596723	val: 0.621422	test: 0.659997

Epoch: 17
Loss: 0.6386153995990753
RMSE train: 0.745546	val: 0.796273	test: 0.842700
MAE train: 0.587758	val: 0.617569	test: 0.656180

Epoch: 18
Loss: 0.6228714883327484
RMSE train: 0.787650	val: 0.835459	test: 0.881640
MAE train: 0.619524	val: 0.647107	test: 0.682086

Epoch: 19
Loss: 0.6105522811412811
RMSE train: 0.732254	val: 0.790230	test: 0.833252
MAE train: 0.574075	val: 0.610002	test: 0.650852

Epoch: 20
Loss: 0.6086791217327118
RMSE train: 0.712245	val: 0.782841	test: 0.816561
MAE train: 0.560322	val: 0.596384	test: 0.637020

Epoch: 21
Loss: 0.6021851062774658
RMSE train: 0.728926	val: 0.789365	test: 0.833068
MAE train: 0.572827	val: 0.612231	test: 0.646474

Epoch: 22
Loss: 0.5883161664009094
RMSE train: 0.708679	val: 0.777540	test: 0.811532
MAE train: 0.559493	val: 0.598233	test: 0.632765Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/lipo/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/lipo/random/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/lipo/random/train_prop=0.7/lipophilicity_random_2_20-05_14-43-15  ]
[ Using Seed :  2  ]
[ Using device :  cuda:1  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 4.966437578201294
RMSE train: 2.165669	val: 2.257035	test: 2.188600
MAE train: 1.928038	val: 2.007026	test: 1.944709

Epoch: 2
Loss: 3.3824579318364463
RMSE train: 2.054699	val: 2.139951	test: 2.062306
MAE train: 1.830688	val: 1.895983	test: 1.829470

Epoch: 3
Loss: 2.227490246295929
RMSE train: 1.578199	val: 1.665545	test: 1.607008
MAE train: 1.368143	val: 1.436019	test: 1.379493

Epoch: 4
Loss: 1.4303297102451324
RMSE train: 1.044486	val: 1.114594	test: 1.072083
MAE train: 0.861110	val: 0.913909	test: 0.893351

Epoch: 5
Loss: 1.0350016554196675
RMSE train: 0.980274	val: 1.052775	test: 1.016804
MAE train: 0.799697	val: 0.861134	test: 0.837448

Epoch: 6
Loss: 0.8554171919822693
RMSE train: 0.846589	val: 0.900763	test: 0.892463
MAE train: 0.668624	val: 0.717070	test: 0.712142

Epoch: 7
Loss: 0.8253961702187856
RMSE train: 0.838573	val: 0.909565	test: 0.888596
MAE train: 0.651919	val: 0.712103	test: 0.693674

Epoch: 8
Loss: 0.7906011392672857
RMSE train: 0.855505	val: 0.915234	test: 0.906498
MAE train: 0.663269	val: 0.716458	test: 0.703682

Epoch: 9
Loss: 0.7529459496339163
RMSE train: 0.805424	val: 0.869769	test: 0.863756
MAE train: 0.629085	val: 0.680806	test: 0.679130

Epoch: 10
Loss: 0.749413733681043
RMSE train: 0.814477	val: 0.883983	test: 0.862460
MAE train: 0.639991	val: 0.697277	test: 0.676669

Epoch: 11
Loss: 0.7084507395823797
RMSE train: 0.768014	val: 0.844708	test: 0.819479
MAE train: 0.604203	val: 0.658334	test: 0.646302

Epoch: 12
Loss: 0.6842179099718729
RMSE train: 0.765604	val: 0.846560	test: 0.829495
MAE train: 0.603585	val: 0.662196	test: 0.654122

Epoch: 13
Loss: 0.6556098560492197
RMSE train: 0.748412	val: 0.825068	test: 0.809830
MAE train: 0.583593	val: 0.635763	test: 0.631379

Epoch: 14
Loss: 0.6849383215109507
RMSE train: 0.757489	val: 0.839268	test: 0.827932
MAE train: 0.596422	val: 0.655812	test: 0.650929

Epoch: 15
Loss: 0.6732192784547806
RMSE train: 0.766011	val: 0.833910	test: 0.819834
MAE train: 0.598191	val: 0.642386	test: 0.639072

Epoch: 16
Loss: 0.6498457541068395
RMSE train: 0.716179	val: 0.811405	test: 0.765700
MAE train: 0.562453	val: 0.624382	test: 0.603715

Epoch: 17
Loss: 0.6316878646612167
RMSE train: 0.738625	val: 0.832647	test: 0.803238
MAE train: 0.577501	val: 0.643915	test: 0.633037

Epoch: 18
Loss: 0.616496741771698
RMSE train: 0.730298	val: 0.822256	test: 0.798129
MAE train: 0.573789	val: 0.635122	test: 0.621798

Epoch: 19
Loss: 0.5836663444836935
RMSE train: 0.713750	val: 0.814974	test: 0.771988
MAE train: 0.559061	val: 0.621463	test: 0.605154

Epoch: 20
Loss: 0.5840869545936584
RMSE train: 0.735133	val: 0.824703	test: 0.808570
MAE train: 0.576748	val: 0.641643	test: 0.627556

Epoch: 21
Loss: 0.5801787773768107
RMSE train: 0.714851	val: 0.806981	test: 0.774395
MAE train: 0.560071	val: 0.620494	test: 0.600128

Epoch: 22
Loss: 0.5669044752915701
RMSE train: 0.694921	val: 0.799323	test: 0.760966
MAE train: 0.549482	val: 0.617877	test: 0.597260Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/lipo/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/lipo/random/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/lipo/random/train_prop=0.7/lipophilicity_random_1_20-05_14-43-15  ]
[ Using Seed :  1  ]
[ Using device :  cuda:1  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.098620931307475
RMSE train: 2.318348	val: 2.418746	test: 2.352670
MAE train: 2.070469	val: 2.164846	test: 2.095262

Epoch: 2
Loss: 3.4730326533317566
RMSE train: 1.961795	val: 2.042408	test: 1.981669
MAE train: 1.732361	val: 1.801739	test: 1.741619

Epoch: 3
Loss: 2.2574380735556283
RMSE train: 1.515532	val: 1.592468	test: 1.533353
MAE train: 1.306963	val: 1.372019	test: 1.315950

Epoch: 4
Loss: 1.4283553461233776
RMSE train: 1.135947	val: 1.210639	test: 1.156127
MAE train: 0.952458	val: 1.003351	test: 0.968852

Epoch: 5
Loss: 1.0033018340667088
RMSE train: 0.887274	val: 0.955940	test: 0.923237
MAE train: 0.712681	val: 0.778404	test: 0.747282

Epoch: 6
Loss: 0.8788794676462809
RMSE train: 0.865710	val: 0.910615	test: 0.909521
MAE train: 0.676008	val: 0.719553	test: 0.716557

Epoch: 7
Loss: 0.8424175928036371
RMSE train: 0.820859	val: 0.885838	test: 0.874650
MAE train: 0.638120	val: 0.698763	test: 0.684352

Epoch: 8
Loss: 0.8152701258659363
RMSE train: 0.839432	val: 0.881535	test: 0.883407
MAE train: 0.660000	val: 0.699814	test: 0.693849

Epoch: 9
Loss: 0.773376519481341
RMSE train: 0.786360	val: 0.859030	test: 0.848996
MAE train: 0.614321	val: 0.672916	test: 0.671622

Epoch: 10
Loss: 0.7542977084716161
RMSE train: 0.781362	val: 0.846716	test: 0.832707
MAE train: 0.612619	val: 0.664325	test: 0.655905

Epoch: 11
Loss: 0.7382192860047022
RMSE train: 0.780676	val: 0.844572	test: 0.833661
MAE train: 0.609518	val: 0.662009	test: 0.652389

Epoch: 12
Loss: 0.7300296823183695
RMSE train: 0.774200	val: 0.848871	test: 0.846147
MAE train: 0.606471	val: 0.663571	test: 0.663866

Epoch: 13
Loss: 0.6753343145052592
RMSE train: 0.762306	val: 0.843909	test: 0.837799
MAE train: 0.595387	val: 0.657705	test: 0.653991

Epoch: 14
Loss: 0.6653184741735458
RMSE train: 0.761385	val: 0.832890	test: 0.828943
MAE train: 0.593889	val: 0.645294	test: 0.647049

Epoch: 15
Loss: 0.6434954305489858
RMSE train: 0.733630	val: 0.818950	test: 0.809256
MAE train: 0.577478	val: 0.641643	test: 0.632784

Epoch: 16
Loss: 0.6799097061157227
RMSE train: 0.704686	val: 0.799856	test: 0.784681
MAE train: 0.552867	val: 0.625070	test: 0.614466

Epoch: 17
Loss: 0.6184794257084528
RMSE train: 0.720281	val: 0.812272	test: 0.795749
MAE train: 0.567231	val: 0.636626	test: 0.620473

Epoch: 18
Loss: 0.6458640744288763
RMSE train: 0.742977	val: 0.819831	test: 0.809874
MAE train: 0.579173	val: 0.632829	test: 0.629601

Epoch: 19
Loss: 0.6346367398897806
RMSE train: 0.692791	val: 0.789402	test: 0.783226
MAE train: 0.540590	val: 0.615627	test: 0.607935

Epoch: 20
Loss: 0.5871552924315134
RMSE train: 0.705778	val: 0.805762	test: 0.787003
MAE train: 0.550099	val: 0.622069	test: 0.606873

Epoch: 21
Loss: 0.5719413831830025
RMSE train: 0.704964	val: 0.797431	test: 0.777728
MAE train: 0.547776	val: 0.610162	test: 0.602017

Epoch: 22
Loss: 0.5736701413989067
RMSE train: 0.785692	val: 0.860999	test: 0.849923
MAE train: 0.617435	val: 0.670342	test: 0.659489Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/lipo/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/lipo/random/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/lipo/random/train_prop=0.7/lipophilicity_random_3_20-05_14-43-15  ]
[ Using Seed :  3  ]
[ Using device :  cuda:1  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.27771532535553
RMSE train: 2.398648	val: 2.495817	test: 2.411461
MAE train: 2.161388	val: 2.246425	test: 2.165842

Epoch: 2
Loss: 3.6526079575220742
RMSE train: 2.002851	val: 2.090835	test: 2.009228
MAE train: 1.774636	val: 1.849863	test: 1.770645

Epoch: 3
Loss: 2.396595388650894
RMSE train: 1.448777	val: 1.529108	test: 1.470298
MAE train: 1.242891	val: 1.306640	test: 1.249522

Epoch: 4
Loss: 1.575314352909724
RMSE train: 1.080109	val: 1.150131	test: 1.112156
MAE train: 0.898806	val: 0.955787	test: 0.924808

Epoch: 5
Loss: 1.0438645432392757
RMSE train: 0.920315	val: 0.980545	test: 0.968018
MAE train: 0.744730	val: 0.807229	test: 0.780634

Epoch: 6
Loss: 0.8852598269780477
RMSE train: 0.889587	val: 0.928645	test: 0.929680
MAE train: 0.691238	val: 0.730778	test: 0.733090

Epoch: 7
Loss: 0.8237962822119395
RMSE train: 0.871763	val: 0.928364	test: 0.946724
MAE train: 0.677302	val: 0.731265	test: 0.747613

Epoch: 8
Loss: 0.8327685197194418
RMSE train: 0.865958	val: 0.912208	test: 0.918140
MAE train: 0.668823	val: 0.709782	test: 0.721393

Epoch: 9
Loss: 0.773850217461586
RMSE train: 0.827821	val: 0.889033	test: 0.894967
MAE train: 0.640353	val: 0.690659	test: 0.703385

Epoch: 10
Loss: 0.7380707909663519
RMSE train: 0.796421	val: 0.861604	test: 0.864079
MAE train: 0.623736	val: 0.679558	test: 0.680128

Epoch: 11
Loss: 0.7132278581460317
RMSE train: 0.741023	val: 0.834957	test: 0.819604
MAE train: 0.578381	val: 0.654567	test: 0.650713

Epoch: 12
Loss: 0.7108535716931025
RMSE train: 0.783891	val: 0.861707	test: 0.851875
MAE train: 0.615564	val: 0.674595	test: 0.674703

Epoch: 13
Loss: 0.7092470626036326
RMSE train: 0.755625	val: 0.835250	test: 0.826584
MAE train: 0.591784	val: 0.653970	test: 0.651238

Epoch: 14
Loss: 0.6659128367900848
RMSE train: 0.743306	val: 0.828369	test: 0.827101
MAE train: 0.583986	val: 0.644528	test: 0.647058

Epoch: 15
Loss: 0.6592481682697932
RMSE train: 0.751264	val: 0.842611	test: 0.836028
MAE train: 0.586184	val: 0.654046	test: 0.648794

Epoch: 16
Loss: 0.6297826071580251
RMSE train: 0.709879	val: 0.825384	test: 0.799111
MAE train: 0.559431	val: 0.644181	test: 0.628026

Epoch: 17
Loss: 0.594396191338698
RMSE train: 0.725917	val: 0.820649	test: 0.801817
MAE train: 0.570235	val: 0.636821	test: 0.633431

Epoch: 18
Loss: 0.6264513283967972
RMSE train: 0.740285	val: 0.827521	test: 0.828852
MAE train: 0.576180	val: 0.637121	test: 0.645716

Epoch: 19
Loss: 0.6049601684014002
RMSE train: 0.717067	val: 0.809830	test: 0.811231
MAE train: 0.562076	val: 0.624450	test: 0.634676

Epoch: 20
Loss: 0.6014044334491094
RMSE train: 0.700347	val: 0.800771	test: 0.793454
MAE train: 0.547111	val: 0.615372	test: 0.617923

Epoch: 21
Loss: 0.6024465411901474
RMSE train: 0.709836	val: 0.808502	test: 0.803925
MAE train: 0.554789	val: 0.629648	test: 0.629721

Epoch: 22
Loss: 0.5638401284813881
RMSE train: 0.744950	val: 0.824831	test: 0.830810
MAE train: 0.578192	val: 0.637901	test: 0.648507Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/lipo/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/lipo/random/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/lipo/random/train_prop=0.8/lipophilicity_random_2_20-05_14-43-15  ]
[ Using Seed :  2  ]
[ Using device :  cuda:2  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 4.815456049782889
RMSE train: 2.178265	val: 2.212938	test: 2.199953
MAE train: 1.937688	val: 1.939618	test: 1.960141

Epoch: 2
Loss: 3.1321435826165334
RMSE train: 1.787411	val: 1.819502	test: 1.814158
MAE train: 1.559731	val: 1.558546	test: 1.587937

Epoch: 3
Loss: 1.9271632432937622
RMSE train: 1.352011	val: 1.392356	test: 1.388407
MAE train: 1.149836	val: 1.173809	test: 1.179963

Epoch: 4
Loss: 1.184994169643947
RMSE train: 0.984580	val: 1.049417	test: 1.023293
MAE train: 0.807245	val: 0.854085	test: 0.849437

Epoch: 5
Loss: 0.9247638838631767
RMSE train: 0.859085	val: 0.967677	test: 0.904230
MAE train: 0.673251	val: 0.758540	test: 0.712757

Epoch: 6
Loss: 0.8603258601256779
RMSE train: 0.903288	val: 1.003999	test: 0.942712
MAE train: 0.703164	val: 0.786376	test: 0.730533

Epoch: 7
Loss: 0.7883076327187675
RMSE train: 0.829978	val: 0.927480	test: 0.871364
MAE train: 0.644136	val: 0.723242	test: 0.682929

Epoch: 8
Loss: 0.7559148115771157
RMSE train: 0.820427	val: 0.927317	test: 0.864592
MAE train: 0.640845	val: 0.726466	test: 0.677827

Epoch: 9
Loss: 0.7550154158047268
RMSE train: 0.778968	val: 0.886390	test: 0.822414
MAE train: 0.615051	val: 0.704655	test: 0.652044

Epoch: 10
Loss: 0.7142809075968606
RMSE train: 0.816917	val: 0.928144	test: 0.853551
MAE train: 0.638397	val: 0.729818	test: 0.665614

Epoch: 11
Loss: 0.6869903015238898
RMSE train: 0.780067	val: 0.888680	test: 0.821866
MAE train: 0.611395	val: 0.695314	test: 0.640540

Epoch: 12
Loss: 0.6602697138275418
RMSE train: 0.753059	val: 0.858702	test: 0.797505
MAE train: 0.592124	val: 0.669416	test: 0.626341

Epoch: 13
Loss: 0.6756063146250588
RMSE train: 0.744407	val: 0.864710	test: 0.793981
MAE train: 0.582950	val: 0.680674	test: 0.613544

Epoch: 14
Loss: 0.6869304605892727
RMSE train: 0.796950	val: 0.918872	test: 0.844124
MAE train: 0.622728	val: 0.714038	test: 0.652513

Epoch: 15
Loss: 0.6645588151046208
RMSE train: 0.739828	val: 0.864017	test: 0.794235
MAE train: 0.577612	val: 0.668223	test: 0.615623

Epoch: 16
Loss: 0.6261657135827201
RMSE train: 0.732252	val: 0.868280	test: 0.779987
MAE train: 0.570265	val: 0.667191	test: 0.601358

Epoch: 17
Loss: 0.6341492022786822
RMSE train: 0.721136	val: 0.862010	test: 0.768361
MAE train: 0.564655	val: 0.675874	test: 0.592858

Epoch: 18
Loss: 0.616991468838283
RMSE train: 0.725062	val: 0.869256	test: 0.761089
MAE train: 0.567258	val: 0.676991	test: 0.593294

Epoch: 19
Loss: 0.5876407729727882
RMSE train: 0.720189	val: 0.857697	test: 0.760355
MAE train: 0.560046	val: 0.660916	test: 0.583713

Epoch: 20
Loss: 0.6106767228671482
RMSE train: 0.686579	val: 0.839485	test: 0.738225
MAE train: 0.534964	val: 0.644506	test: 0.570035

Epoch: 21
Loss: 0.6116630882024765
RMSE train: 0.709462	val: 0.873135	test: 0.760507
MAE train: 0.554103	val: 0.674712	test: 0.586296

Epoch: 22
Loss: 0.562952309846878
RMSE train: 0.689795	val: 0.840019	test: 0.731896
MAE train: 0.540343	val: 0.650586	test: 0.564353Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/lipo/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/lipo/random/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/lipo/random/train_prop=0.8/lipophilicity_random_3_20-05_14-43-15  ]
[ Using Seed :  3  ]
[ Using device :  cuda:2  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.179191759654453
RMSE train: 2.350407	val: 2.367653	test: 2.363398
MAE train: 2.110668	val: 2.099084	test: 2.123896

Epoch: 2
Loss: 3.290851363113948
RMSE train: 1.867740	val: 1.885420	test: 1.882949
MAE train: 1.643474	val: 1.637695	test: 1.651452

Epoch: 3
Loss: 2.0730138335909163
RMSE train: 1.327490	val: 1.362663	test: 1.333258
MAE train: 1.121769	val: 1.140765	test: 1.129683

Epoch: 4
Loss: 1.3039833860737937
RMSE train: 0.952839	val: 1.028991	test: 0.993602
MAE train: 0.771127	val: 0.829526	test: 0.818556

Epoch: 5
Loss: 0.914087542465755
RMSE train: 0.893800	val: 1.003669	test: 0.935234
MAE train: 0.705434	val: 0.796680	test: 0.738667

Epoch: 6
Loss: 0.8219164652483804
RMSE train: 0.875939	val: 0.996476	test: 0.933242
MAE train: 0.675994	val: 0.787726	test: 0.721945

Epoch: 7
Loss: 0.8248339763709477
RMSE train: 0.840802	val: 0.965437	test: 0.880239
MAE train: 0.650215	val: 0.760010	test: 0.680808

Epoch: 8
Loss: 0.7684272187096732
RMSE train: 0.877052	val: 1.002690	test: 0.922172
MAE train: 0.682306	val: 0.796715	test: 0.707660

Epoch: 9
Loss: 0.753822820527213
RMSE train: 0.803202	val: 0.939885	test: 0.838029
MAE train: 0.626940	val: 0.744420	test: 0.660051

Epoch: 10
Loss: 0.735637583902904
RMSE train: 0.858365	val: 0.980825	test: 0.906713
MAE train: 0.662125	val: 0.768362	test: 0.703021

Epoch: 11
Loss: 0.7351065746375492
RMSE train: 0.786542	val: 0.920130	test: 0.831979
MAE train: 0.615046	val: 0.729681	test: 0.649353

Epoch: 12
Loss: 0.7142089818205152
RMSE train: 0.794852	val: 0.930620	test: 0.849254
MAE train: 0.619216	val: 0.735797	test: 0.655488

Epoch: 13
Loss: 0.6848144616399493
RMSE train: 0.850433	val: 0.987229	test: 0.894334
MAE train: 0.662986	val: 0.778839	test: 0.691741

Epoch: 14
Loss: 0.6800432205200195
RMSE train: 0.732522	val: 0.880767	test: 0.790401
MAE train: 0.569796	val: 0.688114	test: 0.612494

Epoch: 15
Loss: 0.6586582405226571
RMSE train: 0.784714	val: 0.921943	test: 0.826671
MAE train: 0.611906	val: 0.714904	test: 0.628460

Epoch: 16
Loss: 0.6398558786937169
RMSE train: 0.801091	val: 0.940353	test: 0.852673
MAE train: 0.628780	val: 0.746590	test: 0.656002

Epoch: 17
Loss: 0.627786751304354
RMSE train: 0.718783	val: 0.868004	test: 0.768264
MAE train: 0.557737	val: 0.675575	test: 0.590099

Epoch: 18
Loss: 0.6312068062169212
RMSE train: 0.758370	val: 0.914717	test: 0.814879
MAE train: 0.589337	val: 0.722112	test: 0.626228

Epoch: 19
Loss: 0.6477774935109275
RMSE train: 0.737888	val: 0.900387	test: 0.784794
MAE train: 0.576722	val: 0.695319	test: 0.606835

Epoch: 20
Loss: 0.6089859860283988
RMSE train: 0.742053	val: 0.905093	test: 0.786084
MAE train: 0.579412	val: 0.699085	test: 0.602751

Epoch: 21
Loss: 0.6225198187998363
RMSE train: 0.739027	val: 0.906599	test: 0.799623
MAE train: 0.577411	val: 0.697933	test: 0.608309

Epoch: 22
Loss: 0.5967289890561785
RMSE train: 0.731274	val: 0.896154	test: 0.800150
MAE train: 0.574715	val: 0.696571	test: 0.615125Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/lipo/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/lipo/random/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/lipo/random/train_prop=0.8/lipophilicity_random_1_20-05_14-43-15  ]
[ Using Seed :  1  ]
[ Using device :  cuda:2  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.017238003867013
RMSE train: 2.210277	val: 2.239915	test: 2.253075
MAE train: 1.961381	val: 1.964161	test: 2.005699

Epoch: 2
Loss: 3.1715943472726003
RMSE train: 1.866030	val: 1.891000	test: 1.902332
MAE train: 1.639302	val: 1.638358	test: 1.665118

Epoch: 3
Loss: 1.957071568284716
RMSE train: 1.338152	val: 1.377921	test: 1.368154
MAE train: 1.135472	val: 1.162753	test: 1.166181

Epoch: 4
Loss: 1.1925776643412453
RMSE train: 0.919251	val: 0.992008	test: 0.952578
MAE train: 0.747328	val: 0.791842	test: 0.774027

Epoch: 5
Loss: 0.906166992017201
RMSE train: 0.861107	val: 0.956529	test: 0.904559
MAE train: 0.685811	val: 0.760880	test: 0.723869

Epoch: 6
Loss: 0.8918855616024562
RMSE train: 0.831898	val: 0.931496	test: 0.876432
MAE train: 0.652916	val: 0.731179	test: 0.694962

Epoch: 7
Loss: 0.8475205813135419
RMSE train: 0.822165	val: 0.914855	test: 0.840002
MAE train: 0.645629	val: 0.724635	test: 0.666564

Epoch: 8
Loss: 0.787261358329228
RMSE train: 0.797519	val: 0.898211	test: 0.846684
MAE train: 0.622028	val: 0.708925	test: 0.674951

Epoch: 9
Loss: 0.7729473624910627
RMSE train: 0.804753	val: 0.908358	test: 0.852658
MAE train: 0.631698	val: 0.718463	test: 0.667237

Epoch: 10
Loss: 0.7531080224684307
RMSE train: 0.765178	val: 0.870833	test: 0.820537
MAE train: 0.597158	val: 0.685389	test: 0.648939

Epoch: 11
Loss: 0.7245985184397016
RMSE train: 0.774681	val: 0.886725	test: 0.828890
MAE train: 0.603494	val: 0.699393	test: 0.648265

Epoch: 12
Loss: 0.7149913012981415
RMSE train: 0.767531	val: 0.860651	test: 0.817150
MAE train: 0.599758	val: 0.685154	test: 0.634757

Epoch: 13
Loss: 0.7212046980857849
RMSE train: 0.743604	val: 0.855940	test: 0.812276
MAE train: 0.581290	val: 0.673897	test: 0.634134

Epoch: 14
Loss: 0.7200715584414346
RMSE train: 0.770742	val: 0.887513	test: 0.826910
MAE train: 0.601485	val: 0.696014	test: 0.640587

Epoch: 15
Loss: 0.6772273736340659
RMSE train: 0.743721	val: 0.863086	test: 0.796937
MAE train: 0.582292	val: 0.679901	test: 0.627880

Epoch: 16
Loss: 0.6539371524538312
RMSE train: 0.706700	val: 0.826948	test: 0.769717
MAE train: 0.548236	val: 0.639536	test: 0.606280

Epoch: 17
Loss: 0.6491815660681043
RMSE train: 0.728727	val: 0.852076	test: 0.791699
MAE train: 0.567081	val: 0.667559	test: 0.618122

Epoch: 18
Loss: 0.6205003729888371
RMSE train: 0.732250	val: 0.848474	test: 0.801502
MAE train: 0.565640	val: 0.659181	test: 0.628846

Epoch: 19
Loss: 0.616575111235891
RMSE train: 0.702443	val: 0.818672	test: 0.758427
MAE train: 0.549126	val: 0.641455	test: 0.596629

Epoch: 20
Loss: 0.6164489792925971
RMSE train: 0.691836	val: 0.814282	test: 0.744210
MAE train: 0.538702	val: 0.629370	test: 0.586429

Epoch: 21
Loss: 0.5960776720728193
RMSE train: 0.715969	val: 0.835695	test: 0.780557
MAE train: 0.553382	val: 0.651203	test: 0.596382

Epoch: 22
Loss: 0.6163103793348584
RMSE train: 0.736274	val: 0.860172	test: 0.805908
MAE train: 0.571599	val: 0.667581	test: 0.619892

Epoch: 23
Loss: 0.569117859005928
RMSE train: 0.697088	val: 0.791162	test: 0.801915
MAE train: 0.548006	val: 0.602171	test: 0.617302

Epoch: 24
Loss: 0.5551881641149521
RMSE train: 0.671675	val: 0.760946	test: 0.790989
MAE train: 0.531380	val: 0.578820	test: 0.616385

Epoch: 25
Loss: 0.5458262890577317
RMSE train: 0.676965	val: 0.765069	test: 0.795084
MAE train: 0.536732	val: 0.585922	test: 0.615876

Epoch: 26
Loss: 0.5235886901617051
RMSE train: 0.654992	val: 0.764183	test: 0.771419
MAE train: 0.520438	val: 0.573482	test: 0.605269

Epoch: 27
Loss: 0.5575774341821671
RMSE train: 0.664056	val: 0.756842	test: 0.784905
MAE train: 0.523747	val: 0.579396	test: 0.612517

Epoch: 28
Loss: 0.5324202954769135
RMSE train: 0.667405	val: 0.760549	test: 0.777632
MAE train: 0.523490	val: 0.574551	test: 0.603049

Epoch: 29
Loss: 0.538786867260933
RMSE train: 0.653588	val: 0.756533	test: 0.775524
MAE train: 0.513942	val: 0.573363	test: 0.601269

Epoch: 30
Loss: 0.5162460774183273
RMSE train: 0.661828	val: 0.756495	test: 0.778961
MAE train: 0.524518	val: 0.577562	test: 0.603083

Epoch: 31
Loss: 0.5129768639802933
RMSE train: 0.641041	val: 0.749170	test: 0.768023
MAE train: 0.505104	val: 0.567148	test: 0.590893

Epoch: 32
Loss: 0.5063438415527344
RMSE train: 0.645255	val: 0.759742	test: 0.768499
MAE train: 0.508712	val: 0.573461	test: 0.594263

Epoch: 33
Loss: 0.5314751327037811
RMSE train: 0.627207	val: 0.745849	test: 0.767766
MAE train: 0.496121	val: 0.567376	test: 0.597184

Epoch: 34
Loss: 0.5018580406904221
RMSE train: 0.625242	val: 0.740039	test: 0.765171
MAE train: 0.493958	val: 0.564281	test: 0.592026

Epoch: 35
Loss: 0.4787777602672577
RMSE train: 0.618473	val: 0.739616	test: 0.753446
MAE train: 0.486769	val: 0.556814	test: 0.582819

Epoch: 36
Loss: 0.46389120519161225
RMSE train: 0.623870	val: 0.745232	test: 0.755406
MAE train: 0.492817	val: 0.561788	test: 0.583098

Epoch: 37
Loss: 0.4782493501901627
RMSE train: 0.631633	val: 0.743929	test: 0.770748
MAE train: 0.495090	val: 0.560741	test: 0.592010

Epoch: 38
Loss: 0.4716989606618881
RMSE train: 0.614785	val: 0.729263	test: 0.759858
MAE train: 0.482352	val: 0.546433	test: 0.584413

Epoch: 39
Loss: 0.4720395773649216
RMSE train: 0.635757	val: 0.754251	test: 0.773437
MAE train: 0.500656	val: 0.568095	test: 0.597303

Epoch: 40
Loss: 0.48062320947647097
RMSE train: 0.628990	val: 0.749556	test: 0.768421
MAE train: 0.501472	val: 0.566848	test: 0.596446

Epoch: 41
Loss: 0.48136200606822965
RMSE train: 0.630253	val: 0.736768	test: 0.774770
MAE train: 0.491783	val: 0.556049	test: 0.594799

Epoch: 42
Loss: 0.46002618670463563
RMSE train: 0.593073	val: 0.729962	test: 0.748595
MAE train: 0.468211	val: 0.545665	test: 0.568277

Epoch: 43
Loss: 0.44960345327854156
RMSE train: 0.589637	val: 0.713137	test: 0.744152
MAE train: 0.462862	val: 0.536778	test: 0.570566

Epoch: 44
Loss: 0.4405809700489044
RMSE train: 0.598654	val: 0.725390	test: 0.735298
MAE train: 0.475534	val: 0.546470	test: 0.568725

Epoch: 45
Loss: 0.4546208918094635
RMSE train: 0.596871	val: 0.736946	test: 0.755363
MAE train: 0.473807	val: 0.556638	test: 0.584297

Epoch: 46
Loss: 0.4406887710094452
RMSE train: 0.602012	val: 0.727406	test: 0.748875
MAE train: 0.475942	val: 0.549985	test: 0.577071

Epoch: 47
Loss: 0.4310302734375
RMSE train: 0.578634	val: 0.709619	test: 0.739706
MAE train: 0.453040	val: 0.530965	test: 0.564739

Epoch: 48
Loss: 0.42771452367305757
RMSE train: 0.570691	val: 0.715863	test: 0.737609
MAE train: 0.449614	val: 0.537739	test: 0.567493

Epoch: 49
Loss: 0.40294023156166076
RMSE train: 0.572504	val: 0.721425	test: 0.732377
MAE train: 0.452768	val: 0.543453	test: 0.561638

Epoch: 50
Loss: 0.4008121132850647
RMSE train: 0.576405	val: 0.728349	test: 0.739289
MAE train: 0.451303	val: 0.540903	test: 0.562986

Epoch: 51
Loss: 0.4193204939365387
RMSE train: 0.571472	val: 0.712382	test: 0.725634
MAE train: 0.451904	val: 0.534242	test: 0.552990

Epoch: 52
Loss: 0.41885925829410553
RMSE train: 0.570609	val: 0.713708	test: 0.729309
MAE train: 0.450445	val: 0.532144	test: 0.555805

Epoch: 53
Loss: 0.39746772944927217
RMSE train: 0.563455	val: 0.721110	test: 0.728502
MAE train: 0.444797	val: 0.534056	test: 0.555680

Epoch: 54
Loss: 0.400773948431015
RMSE train: 0.561330	val: 0.720431	test: 0.729516
MAE train: 0.446713	val: 0.538538	test: 0.561114

Epoch: 55
Loss: 0.4011390745639801
RMSE train: 0.551558	val: 0.709393	test: 0.731068
MAE train: 0.434940	val: 0.528817	test: 0.552062

Epoch: 56
Loss: 0.3837186187505722
RMSE train: 0.561070	val: 0.726270	test: 0.742732
MAE train: 0.444633	val: 0.541644	test: 0.567184

Epoch: 57
Loss: 0.37788960337638855
RMSE train: 0.561966	val: 0.718030	test: 0.733601
MAE train: 0.442996	val: 0.533983	test: 0.559234

Epoch: 58
Loss: 0.3873844236135483
RMSE train: 0.540832	val: 0.700596	test: 0.719867
MAE train: 0.428070	val: 0.526504	test: 0.545237

Epoch: 59
Loss: 0.3835799187421799
RMSE train: 0.563024	val: 0.715170	test: 0.740859
MAE train: 0.442667	val: 0.530056	test: 0.560984

Epoch: 60
Loss: 0.37999625205993653
RMSE train: 0.556946	val: 0.715868	test: 0.734687
MAE train: 0.439933	val: 0.535065	test: 0.561901

Epoch: 61
Loss: 0.3842656284570694
RMSE train: 0.541437	val: 0.703294	test: 0.721800
MAE train: 0.428221	val: 0.521771	test: 0.546435

Epoch: 62
Loss: 0.3582034230232239
RMSE train: 0.531484	val: 0.706147	test: 0.724542
MAE train: 0.420860	val: 0.525850	test: 0.550688

Epoch: 63
Loss: 0.36671947240829467
RMSE train: 0.526067	val: 0.691189	test: 0.710381
MAE train: 0.416534	val: 0.517148	test: 0.540356

Epoch: 64
Loss: 0.3609608352184296
RMSE train: 0.547113	val: 0.712045	test: 0.719896
MAE train: 0.430622	val: 0.529591	test: 0.550102

Epoch: 65
Loss: 0.3842235952615738
RMSE train: 0.553716	val: 0.718068	test: 0.735154
MAE train: 0.436095	val: 0.532274	test: 0.558241

Epoch: 66
Loss: 0.36616171002388
RMSE train: 0.517254	val: 0.697177	test: 0.716001
MAE train: 0.408588	val: 0.518888	test: 0.543105

Epoch: 67
Loss: 0.3384769558906555
RMSE train: 0.528339	val: 0.697848	test: 0.725083
MAE train: 0.416799	val: 0.524324	test: 0.554588

Epoch: 68
Loss: 0.35092118978500364
RMSE train: 0.535703	val: 0.712622	test: 0.730533
MAE train: 0.421954	val: 0.526951	test: 0.547194

Epoch: 69
Loss: 0.34869922399520875
RMSE train: 0.519340	val: 0.714116	test: 0.730263
MAE train: 0.410567	val: 0.528188	test: 0.545053

Epoch: 70
Loss: 0.3400705516338348
RMSE train: 0.504975	val: 0.702695	test: 0.722211
MAE train: 0.397152	val: 0.516798	test: 0.541116

Epoch: 71
Loss: 0.32702387273311617
RMSE train: 0.529532	val: 0.724861	test: 0.723038
MAE train: 0.417392	val: 0.533301	test: 0.547664

Epoch: 72
Loss: 0.3313017308712006
RMSE train: 0.514303	val: 0.716304	test: 0.732968
MAE train: 0.404433	val: 0.529546	test: 0.554773

Epoch: 73
Loss: 0.3511974334716797
RMSE train: 0.532737	val: 0.710869	test: 0.721945
MAE train: 0.418970	val: 0.523971	test: 0.548770

Epoch: 74
Loss: 0.34808180928230287
RMSE train: 0.505885	val: 0.695716	test: 0.700129
MAE train: 0.399687	val: 0.517176	test: 0.522436

Epoch: 75
Loss: 0.3303503721952438
RMSE train: 0.525033	val: 0.702539	test: 0.721862
MAE train: 0.409929	val: 0.517264	test: 0.539800

Epoch: 76
Loss: 0.3326416015625
RMSE train: 0.511619	val: 0.700187	test: 0.706770
MAE train: 0.402979	val: 0.517477	test: 0.533087

Epoch: 77
Loss: 0.33351579308509827
RMSE train: 0.508714	val: 0.709907	test: 0.715820
MAE train: 0.400292	val: 0.522105	test: 0.540701

Epoch: 78
Loss: 0.31961060762405397
RMSE train: 0.507135	val: 0.712466	test: 0.715611
MAE train: 0.397881	val: 0.521746	test: 0.539542

Epoch: 79
Loss: 0.33503150939941406
RMSE train: 0.512372	val: 0.709832	test: 0.715949
MAE train: 0.404864	val: 0.529158	test: 0.541543

Epoch: 80
Loss: 0.33465324342250824
RMSE train: 0.501127	val: 0.702730	test: 0.708235
MAE train: 0.394999	val: 0.517728	test: 0.531685

Epoch: 81
Loss: 0.3045938104391098
RMSE train: 0.503442	val: 0.709210	test: 0.714272
MAE train: 0.396700	val: 0.527002	test: 0.540362

Epoch: 82
Loss: 0.3216946989297867
RMSE train: 0.496909	val: 0.705281	test: 0.718577
MAE train: 0.392837	val: 0.527932	test: 0.544026

Epoch: 83
Loss: 0.3264714330434799
RMSE train: 0.494440	val: 0.692838	test: 0.698382

Epoch: 23
Loss: 0.5818893820047378
RMSE train: 0.684800	val: 0.743221	test: 0.802095
MAE train: 0.540673	val: 0.576034	test: 0.621793

Epoch: 24
Loss: 0.5664912939071656
RMSE train: 0.718221	val: 0.777561	test: 0.833358
MAE train: 0.566009	val: 0.599613	test: 0.645916

Epoch: 25
Loss: 0.5588898122310638
RMSE train: 0.669814	val: 0.741806	test: 0.804683
MAE train: 0.523206	val: 0.566131	test: 0.619323

Epoch: 26
Loss: 0.5627715200185776
RMSE train: 0.717375	val: 0.773974	test: 0.850046
MAE train: 0.559252	val: 0.599017	test: 0.645427

Epoch: 27
Loss: 0.5633693248033523
RMSE train: 0.675291	val: 0.745504	test: 0.804924
MAE train: 0.530472	val: 0.569686	test: 0.621455

Epoch: 28
Loss: 0.5353360563516617
RMSE train: 0.657860	val: 0.740414	test: 0.784655
MAE train: 0.519597	val: 0.558327	test: 0.605978

Epoch: 29
Loss: 0.525653064250946
RMSE train: 0.661351	val: 0.735955	test: 0.792363
MAE train: 0.522682	val: 0.557733	test: 0.613047

Epoch: 30
Loss: 0.5229780077934265
RMSE train: 0.668574	val: 0.739754	test: 0.795148
MAE train: 0.526543	val: 0.567092	test: 0.612014

Epoch: 31
Loss: 0.5312238991260528
RMSE train: 0.642754	val: 0.730370	test: 0.771744
MAE train: 0.510864	val: 0.557359	test: 0.594684

Epoch: 32
Loss: 0.519165426492691
RMSE train: 0.643398	val: 0.727618	test: 0.775801
MAE train: 0.507654	val: 0.558632	test: 0.595239

Epoch: 33
Loss: 0.5155008643865585
RMSE train: 0.658445	val: 0.736807	test: 0.789532
MAE train: 0.516624	val: 0.563517	test: 0.601148

Epoch: 34
Loss: 0.505869773030281
RMSE train: 0.671316	val: 0.749143	test: 0.801835
MAE train: 0.527883	val: 0.575226	test: 0.614221

Epoch: 35
Loss: 0.4910290002822876
RMSE train: 0.621425	val: 0.715205	test: 0.759835
MAE train: 0.489045	val: 0.540228	test: 0.579343

Epoch: 36
Loss: 0.48181642293930055
RMSE train: 0.619551	val: 0.722500	test: 0.757427
MAE train: 0.490255	val: 0.549317	test: 0.579655

Epoch: 37
Loss: 0.4734443873167038
RMSE train: 0.637506	val: 0.751155	test: 0.763892
MAE train: 0.505780	val: 0.567192	test: 0.593458

Epoch: 38
Loss: 0.47198783457279203
RMSE train: 0.607939	val: 0.723200	test: 0.750051
MAE train: 0.477981	val: 0.541020	test: 0.571276

Epoch: 39
Loss: 0.4748256981372833
RMSE train: 0.608925	val: 0.726560	test: 0.747632
MAE train: 0.480056	val: 0.548542	test: 0.576818

Epoch: 40
Loss: 0.4637264162302017
RMSE train: 0.615825	val: 0.733963	test: 0.758505
MAE train: 0.485676	val: 0.552462	test: 0.582016

Epoch: 41
Loss: 0.4690186381340027
RMSE train: 0.608212	val: 0.720952	test: 0.759820
MAE train: 0.479266	val: 0.546528	test: 0.580494

Epoch: 42
Loss: 0.45908632278442385
RMSE train: 0.609309	val: 0.721509	test: 0.755393
MAE train: 0.480515	val: 0.543638	test: 0.574236

Epoch: 43
Loss: 0.4320722222328186
RMSE train: 0.608330	val: 0.718412	test: 0.755905
MAE train: 0.478483	val: 0.543447	test: 0.573884

Epoch: 44
Loss: 0.445688933134079
RMSE train: 0.612748	val: 0.743353	test: 0.755313
MAE train: 0.481951	val: 0.557110	test: 0.577483

Epoch: 45
Loss: 0.4510639786720276
RMSE train: 0.610722	val: 0.741789	test: 0.762978
MAE train: 0.489571	val: 0.567213	test: 0.594562

Epoch: 46
Loss: 0.43096393048763276
RMSE train: 0.586035	val: 0.716857	test: 0.739024
MAE train: 0.460614	val: 0.534971	test: 0.565994

Epoch: 47
Loss: 0.4262712925672531
RMSE train: 0.601948	val: 0.724183	test: 0.747917
MAE train: 0.471374	val: 0.544471	test: 0.571429

Epoch: 48
Loss: 0.4400037437677383
RMSE train: 0.609262	val: 0.730906	test: 0.756962
MAE train: 0.478952	val: 0.548280	test: 0.575615

Epoch: 49
Loss: 0.42610821723937986
RMSE train: 0.573922	val: 0.704556	test: 0.737851
MAE train: 0.451411	val: 0.524645	test: 0.554499

Epoch: 50
Loss: 0.44402699172496796
RMSE train: 0.581879	val: 0.715944	test: 0.752524
MAE train: 0.458690	val: 0.543266	test: 0.572938

Epoch: 51
Loss: 0.4186826467514038
RMSE train: 0.584558	val: 0.715283	test: 0.736395
MAE train: 0.461233	val: 0.535083	test: 0.560000

Epoch: 52
Loss: 0.41230817437171935
RMSE train: 0.583645	val: 0.715238	test: 0.732434
MAE train: 0.456097	val: 0.536565	test: 0.558370

Epoch: 53
Loss: 0.41318051517009735
RMSE train: 0.577466	val: 0.715511	test: 0.738757
MAE train: 0.453646	val: 0.532995	test: 0.560018

Epoch: 54
Loss: 0.4208617389202118
RMSE train: 0.609549	val: 0.724695	test: 0.761250
MAE train: 0.478327	val: 0.546615	test: 0.581583

Epoch: 55
Loss: 0.41724168956279756
RMSE train: 0.555020	val: 0.700448	test: 0.730786
MAE train: 0.436016	val: 0.522281	test: 0.555562

Epoch: 56
Loss: 0.39771862626075744
RMSE train: 0.576674	val: 0.707084	test: 0.740990
MAE train: 0.451491	val: 0.528559	test: 0.559570

Epoch: 57
Loss: 0.39567847847938536
RMSE train: 0.574125	val: 0.708736	test: 0.745288
MAE train: 0.449852	val: 0.529365	test: 0.560724

Epoch: 58
Loss: 0.4004534393548965
RMSE train: 0.571762	val: 0.710674	test: 0.738024
MAE train: 0.447982	val: 0.529657	test: 0.564880

Epoch: 59
Loss: 0.373987489938736
RMSE train: 0.554256	val: 0.703322	test: 0.725603
MAE train: 0.436334	val: 0.525263	test: 0.555001

Epoch: 60
Loss: 0.38491809368133545
RMSE train: 0.537511	val: 0.700620	test: 0.722778
MAE train: 0.422264	val: 0.516144	test: 0.548471

Epoch: 61
Loss: 0.3881633162498474
RMSE train: 0.576935	val: 0.724584	test: 0.745594
MAE train: 0.451894	val: 0.538767	test: 0.567043

Epoch: 62
Loss: 0.3703127920627594
RMSE train: 0.551601	val: 0.705605	test: 0.728831
MAE train: 0.437127	val: 0.524888	test: 0.552004

Epoch: 63
Loss: 0.3693519741296768
RMSE train: 0.551254	val: 0.717665	test: 0.732310
MAE train: 0.431387	val: 0.533785	test: 0.552455

Epoch: 64
Loss: 0.37734701335430143
RMSE train: 0.538860	val: 0.707583	test: 0.715217
MAE train: 0.423043	val: 0.523214	test: 0.542823

Epoch: 65
Loss: 0.3539168506860733
RMSE train: 0.526104	val: 0.703993	test: 0.712749
MAE train: 0.413614	val: 0.517349	test: 0.540482

Epoch: 66
Loss: 0.36287412643432615
RMSE train: 0.548174	val: 0.702917	test: 0.731148
MAE train: 0.431011	val: 0.522208	test: 0.552302

Epoch: 67
Loss: 0.35224277675151827
RMSE train: 0.546950	val: 0.720079	test: 0.718114
MAE train: 0.430389	val: 0.526455	test: 0.550808

Epoch: 68
Loss: 0.37037959396839143
RMSE train: 0.538208	val: 0.699073	test: 0.729524
MAE train: 0.422118	val: 0.519360	test: 0.552538

Epoch: 69
Loss: 0.3557933896780014
RMSE train: 0.545598	val: 0.706632	test: 0.728685
MAE train: 0.427164	val: 0.526355	test: 0.555091

Epoch: 70
Loss: 0.34374810457229615
RMSE train: 0.533104	val: 0.705093	test: 0.722916
MAE train: 0.417419	val: 0.524256	test: 0.547538

Epoch: 71
Loss: 0.3533835351467133
RMSE train: 0.531888	val: 0.706330	test: 0.717663
MAE train: 0.418291	val: 0.521829	test: 0.540403

Epoch: 72
Loss: 0.35579811930656435
RMSE train: 0.523255	val: 0.698323	test: 0.719946
MAE train: 0.412426	val: 0.515242	test: 0.542221

Epoch: 73
Loss: 0.35054926574230194
RMSE train: 0.556188	val: 0.716187	test: 0.741161
MAE train: 0.433786	val: 0.533936	test: 0.559991

Epoch: 74
Loss: 0.3455213993787766
RMSE train: 0.523932	val: 0.703242	test: 0.709406
MAE train: 0.409793	val: 0.522349	test: 0.541980

Epoch: 75
Loss: 0.3455309092998505
RMSE train: 0.546278	val: 0.701709	test: 0.737797
MAE train: 0.428679	val: 0.525650	test: 0.556336

Epoch: 76
Loss: 0.3422212690114975
RMSE train: 0.530992	val: 0.706075	test: 0.720818
MAE train: 0.417447	val: 0.526642	test: 0.549510

Epoch: 77
Loss: 0.35977048277854917
RMSE train: 0.578225	val: 0.735372	test: 0.756113
MAE train: 0.451004	val: 0.545641	test: 0.567520

Epoch: 78
Loss: 0.33635232150554656
RMSE train: 0.546403	val: 0.717411	test: 0.739648
MAE train: 0.430313	val: 0.532342	test: 0.560716

Epoch: 79
Loss: 0.32212336659431456
RMSE train: 0.520497	val: 0.690428	test: 0.716856
MAE train: 0.405480	val: 0.512189	test: 0.542066

Epoch: 80
Loss: 0.3261582046747208
RMSE train: 0.511783	val: 0.694483	test: 0.720975
MAE train: 0.400882	val: 0.512836	test: 0.539458

Epoch: 81
Loss: 0.3188505083322525
RMSE train: 0.534895	val: 0.712127	test: 0.722981
MAE train: 0.419110	val: 0.528288	test: 0.545932

Epoch: 82
Loss: 0.3150187015533447
RMSE train: 0.505394	val: 0.697318	test: 0.705134
MAE train: 0.398612	val: 0.518598	test: 0.535827

Epoch: 83
Loss: 0.3304136604070663
RMSE train: 0.552420	val: 0.719595	test: 0.741601

Epoch: 23
Loss: 0.5814059138298034
RMSE train: 0.726152	val: 0.791779	test: 0.819073
MAE train: 0.570873	val: 0.611402	test: 0.632244

Epoch: 24
Loss: 0.5613805204629898
RMSE train: 0.680874	val: 0.756549	test: 0.792751
MAE train: 0.535892	val: 0.577527	test: 0.613219

Epoch: 25
Loss: 0.5379814177751541
RMSE train: 0.718824	val: 0.795587	test: 0.828555
MAE train: 0.560187	val: 0.606343	test: 0.633978

Epoch: 26
Loss: 0.5375191152095795
RMSE train: 0.681285	val: 0.760082	test: 0.809992
MAE train: 0.534659	val: 0.582382	test: 0.623552

Epoch: 27
Loss: 0.512827804684639
RMSE train: 0.688432	val: 0.769040	test: 0.805943
MAE train: 0.540992	val: 0.587125	test: 0.618472

Epoch: 28
Loss: 0.5255724221467972
RMSE train: 0.692947	val: 0.771100	test: 0.807408
MAE train: 0.543060	val: 0.594527	test: 0.616950

Epoch: 29
Loss: 0.5539526224136353
RMSE train: 0.702379	val: 0.778098	test: 0.816267
MAE train: 0.549690	val: 0.595750	test: 0.630029

Epoch: 30
Loss: 0.5177614629268646
RMSE train: 0.662263	val: 0.754540	test: 0.791847
MAE train: 0.519358	val: 0.576791	test: 0.608257

Epoch: 31
Loss: 0.516934585571289
RMSE train: 0.709943	val: 0.789740	test: 0.822036
MAE train: 0.557796	val: 0.611604	test: 0.633825

Epoch: 32
Loss: 0.49634782373905184
RMSE train: 0.639049	val: 0.752089	test: 0.774343
MAE train: 0.501140	val: 0.567982	test: 0.588872

Epoch: 33
Loss: 0.5085392445325851
RMSE train: 0.661169	val: 0.759822	test: 0.784741
MAE train: 0.525957	val: 0.581957	test: 0.607168

Epoch: 34
Loss: 0.4857709288597107
RMSE train: 0.633959	val: 0.731701	test: 0.763962
MAE train: 0.501453	val: 0.560009	test: 0.584451

Epoch: 35
Loss: 0.46985018253326416
RMSE train: 0.665135	val: 0.759446	test: 0.784498
MAE train: 0.523332	val: 0.575036	test: 0.604373

Epoch: 36
Loss: 0.4997529357671738
RMSE train: 0.636985	val: 0.738890	test: 0.776434
MAE train: 0.501280	val: 0.560248	test: 0.592897

Epoch: 37
Loss: 0.48754864633083345
RMSE train: 0.657403	val: 0.749517	test: 0.780892
MAE train: 0.517210	val: 0.568955	test: 0.598308

Epoch: 38
Loss: 0.47748028934001924
RMSE train: 0.638494	val: 0.739280	test: 0.761769
MAE train: 0.501271	val: 0.557227	test: 0.584710

Epoch: 39
Loss: 0.4678911745548248
RMSE train: 0.680337	val: 0.767651	test: 0.799848
MAE train: 0.535634	val: 0.587562	test: 0.612336

Epoch: 40
Loss: 0.4689886003732681
RMSE train: 0.629504	val: 0.735235	test: 0.762863
MAE train: 0.492921	val: 0.558354	test: 0.580170

Epoch: 41
Loss: 0.45139659345149996
RMSE train: 0.634518	val: 0.741825	test: 0.762892
MAE train: 0.496747	val: 0.562656	test: 0.583371

Epoch: 42
Loss: 0.44125186204910277
RMSE train: 0.589667	val: 0.714223	test: 0.731560
MAE train: 0.464519	val: 0.539193	test: 0.558414

Epoch: 43
Loss: 0.4370973229408264
RMSE train: 0.590709	val: 0.716692	test: 0.745140
MAE train: 0.466278	val: 0.542961	test: 0.567365

Epoch: 44
Loss: 0.42325312495231626
RMSE train: 0.578924	val: 0.716797	test: 0.734817
MAE train: 0.456400	val: 0.537854	test: 0.558186

Epoch: 45
Loss: 0.43422676622867584
RMSE train: 0.637757	val: 0.743752	test: 0.764633
MAE train: 0.501835	val: 0.566508	test: 0.587575

Epoch: 46
Loss: 0.4413131058216095
RMSE train: 0.597102	val: 0.724102	test: 0.744845
MAE train: 0.470540	val: 0.546183	test: 0.569091

Epoch: 47
Loss: 0.44623022377490995
RMSE train: 0.616566	val: 0.737921	test: 0.766126
MAE train: 0.486059	val: 0.560287	test: 0.584741

Epoch: 48
Loss: 0.42658995985984804
RMSE train: 0.584742	val: 0.714605	test: 0.736984
MAE train: 0.459197	val: 0.536934	test: 0.564037

Epoch: 49
Loss: 0.4115044683218002
RMSE train: 0.606824	val: 0.712926	test: 0.752690
MAE train: 0.477559	val: 0.547465	test: 0.576873

Epoch: 50
Loss: 0.41311771869659425
RMSE train: 0.642069	val: 0.753362	test: 0.768367
MAE train: 0.504661	val: 0.570777	test: 0.583862

Epoch: 51
Loss: 0.41093844175338745
RMSE train: 0.601743	val: 0.720608	test: 0.748029
MAE train: 0.472350	val: 0.549736	test: 0.571241

Epoch: 52
Loss: 0.39734323024749757
RMSE train: 0.602040	val: 0.730003	test: 0.738254
MAE train: 0.475218	val: 0.551524	test: 0.566059

Epoch: 53
Loss: 0.41055277585983274
RMSE train: 0.592826	val: 0.717939	test: 0.745802
MAE train: 0.466163	val: 0.544276	test: 0.570200

Epoch: 54
Loss: 0.39666927754879
RMSE train: 0.564877	val: 0.702607	test: 0.731231
MAE train: 0.444951	val: 0.530942	test: 0.556690

Epoch: 55
Loss: 0.4137644648551941
RMSE train: 0.603270	val: 0.725298	test: 0.747181
MAE train: 0.475535	val: 0.549060	test: 0.576098

Epoch: 56
Loss: 0.40781460106372835
RMSE train: 0.586105	val: 0.720347	test: 0.736007
MAE train: 0.461160	val: 0.542658	test: 0.562256

Epoch: 57
Loss: 0.3716181516647339
RMSE train: 0.593283	val: 0.724540	test: 0.750697
MAE train: 0.465489	val: 0.546513	test: 0.569933

Epoch: 58
Loss: 0.37822921872138976
RMSE train: 0.583471	val: 0.728558	test: 0.744692
MAE train: 0.457391	val: 0.543665	test: 0.565127

Epoch: 59
Loss: 0.396917262673378
RMSE train: 0.585607	val: 0.713777	test: 0.750378
MAE train: 0.460021	val: 0.539672	test: 0.570669

Epoch: 60
Loss: 0.38386876583099366
RMSE train: 0.561678	val: 0.711611	test: 0.741065
MAE train: 0.440096	val: 0.524261	test: 0.554528

Epoch: 61
Loss: 0.3670352786779404
RMSE train: 0.589665	val: 0.730375	test: 0.756740
MAE train: 0.463520	val: 0.546721	test: 0.569131

Epoch: 62
Loss: 0.36799864172935487
RMSE train: 0.560029	val: 0.702853	test: 0.728573
MAE train: 0.438170	val: 0.525243	test: 0.548991

Epoch: 63
Loss: 0.3794669985771179
RMSE train: 0.581419	val: 0.720306	test: 0.753486
MAE train: 0.453111	val: 0.540561	test: 0.567537

Epoch: 64
Loss: 0.361144557595253
RMSE train: 0.563865	val: 0.705202	test: 0.737975
MAE train: 0.444382	val: 0.531845	test: 0.562060

Epoch: 65
Loss: 0.3536199271678925
RMSE train: 0.550316	val: 0.707919	test: 0.732482
MAE train: 0.433105	val: 0.525511	test: 0.548172

Epoch: 66
Loss: 0.37151793837547303
RMSE train: 0.535040	val: 0.697534	test: 0.718075
MAE train: 0.420160	val: 0.518575	test: 0.540594

Epoch: 67
Loss: 0.3607909858226776
RMSE train: 0.553963	val: 0.700781	test: 0.740343
MAE train: 0.434803	val: 0.527586	test: 0.561725

Epoch: 68
Loss: 0.3449361115694046
RMSE train: 0.550145	val: 0.699675	test: 0.720996
MAE train: 0.433733	val: 0.523304	test: 0.545761

Epoch: 69
Loss: 0.3495291739702225
RMSE train: 0.534108	val: 0.697812	test: 0.728576
MAE train: 0.418974	val: 0.518067	test: 0.545969

Epoch: 70
Loss: 0.3345183163881302
RMSE train: 0.542281	val: 0.697839	test: 0.733184
MAE train: 0.427625	val: 0.522203	test: 0.546191

Epoch: 71
Loss: 0.3343183308839798
RMSE train: 0.561201	val: 0.714178	test: 0.741381
MAE train: 0.440785	val: 0.535403	test: 0.556523

Epoch: 72
Loss: 0.36563870310783386
RMSE train: 0.547317	val: 0.702546	test: 0.726606
MAE train: 0.430947	val: 0.526071	test: 0.549228

Epoch: 73
Loss: 0.34456827044487
RMSE train: 0.544938	val: 0.698377	test: 0.729127
MAE train: 0.427797	val: 0.523799	test: 0.553479

Epoch: 74
Loss: 0.3610781282186508
RMSE train: 0.543804	val: 0.693038	test: 0.725442
MAE train: 0.428611	val: 0.524162	test: 0.547802

Epoch: 75
Loss: 0.33054245710372926
RMSE train: 0.521704	val: 0.705414	test: 0.728927
MAE train: 0.407076	val: 0.521613	test: 0.545507

Epoch: 76
Loss: 0.3333309441804886
RMSE train: 0.539036	val: 0.707720	test: 0.716041
MAE train: 0.426324	val: 0.526498	test: 0.542774

Epoch: 77
Loss: 0.3283561378717422
RMSE train: 0.534192	val: 0.700462	test: 0.713556
MAE train: 0.420316	val: 0.522663	test: 0.542886

Epoch: 78
Loss: 0.3412298709154129
RMSE train: 0.525888	val: 0.693685	test: 0.725509
MAE train: 0.413798	val: 0.521434	test: 0.551710

Epoch: 79
Loss: 0.3306772470474243
RMSE train: 0.522411	val: 0.703738	test: 0.727195
MAE train: 0.411938	val: 0.520247	test: 0.547971

Epoch: 80
Loss: 0.3277126044034958
RMSE train: 0.517398	val: 0.691134	test: 0.715896
MAE train: 0.407215	val: 0.515748	test: 0.542307

Epoch: 81
Loss: 0.32479841113090513
RMSE train: 0.534374	val: 0.703942	test: 0.725331
MAE train: 0.421229	val: 0.526682	test: 0.548847

Epoch: 82
Loss: 0.3165459394454956
RMSE train: 0.529547	val: 0.698791	test: 0.718625
MAE train: 0.417351	val: 0.523551	test: 0.543839

Epoch: 83
Loss: 0.3122531771659851
RMSE train: 0.521485	val: 0.696817	test: 0.718122

Epoch: 23
Loss: 0.5749136830369631
RMSE train: 0.679269	val: 0.792962	test: 0.755124
MAE train: 0.533160	val: 0.610692	test: 0.586196

Epoch: 24
Loss: 0.5560999860366186
RMSE train: 0.685692	val: 0.791795	test: 0.765427
MAE train: 0.539012	val: 0.607958	test: 0.592505

Epoch: 25
Loss: 0.573073998093605
RMSE train: 0.704530	val: 0.798934	test: 0.766973
MAE train: 0.554504	val: 0.616094	test: 0.590422

Epoch: 26
Loss: 0.5425065010786057
RMSE train: 0.692993	val: 0.799493	test: 0.776052
MAE train: 0.541105	val: 0.609293	test: 0.598786

Epoch: 27
Loss: 0.5368411739667257
RMSE train: 0.698586	val: 0.804234	test: 0.781233
MAE train: 0.548156	val: 0.618399	test: 0.607383

Epoch: 28
Loss: 0.5372006446123123
RMSE train: 0.668555	val: 0.785406	test: 0.739400
MAE train: 0.527288	val: 0.597745	test: 0.575333

Epoch: 29
Loss: 0.5125842690467834
RMSE train: 0.646118	val: 0.775205	test: 0.733939
MAE train: 0.508574	val: 0.587157	test: 0.572962

Epoch: 30
Loss: 0.5228553613026937
RMSE train: 0.668400	val: 0.781359	test: 0.740421
MAE train: 0.530892	val: 0.600753	test: 0.582088

Epoch: 31
Loss: 0.513358953098456
RMSE train: 0.663134	val: 0.784464	test: 0.749924
MAE train: 0.523575	val: 0.604133	test: 0.582347

Epoch: 32
Loss: 0.5023340856035551
RMSE train: 0.662710	val: 0.776001	test: 0.745550
MAE train: 0.522103	val: 0.592468	test: 0.579997

Epoch: 33
Loss: 0.4810257777571678
RMSE train: 0.643654	val: 0.775323	test: 0.734328
MAE train: 0.508430	val: 0.589614	test: 0.572896

Epoch: 34
Loss: 0.4840407595038414
RMSE train: 0.637197	val: 0.764471	test: 0.731158
MAE train: 0.501305	val: 0.578334	test: 0.565282

Epoch: 35
Loss: 0.47103768090407055
RMSE train: 0.631720	val: 0.770044	test: 0.729096
MAE train: 0.497610	val: 0.582430	test: 0.559713

Epoch: 36
Loss: 0.4639010777076085
RMSE train: 0.641145	val: 0.781563	test: 0.729302
MAE train: 0.505464	val: 0.590360	test: 0.566184

Epoch: 37
Loss: 0.4880850339929263
RMSE train: 0.612540	val: 0.765010	test: 0.721130
MAE train: 0.479842	val: 0.576456	test: 0.554441

Epoch: 38
Loss: 0.460427887737751
RMSE train: 0.638023	val: 0.788054	test: 0.723049
MAE train: 0.504760	val: 0.594705	test: 0.567873

Epoch: 39
Loss: 0.48104044795036316
RMSE train: 0.636131	val: 0.769621	test: 0.736484
MAE train: 0.501178	val: 0.580472	test: 0.570675

Epoch: 40
Loss: 0.45704232652982074
RMSE train: 0.625556	val: 0.758993	test: 0.724752
MAE train: 0.493002	val: 0.571993	test: 0.562567

Epoch: 41
Loss: 0.45379270364840824
RMSE train: 0.621202	val: 0.769218	test: 0.720270
MAE train: 0.485473	val: 0.578261	test: 0.552659

Epoch: 42
Loss: 0.44846844176451367
RMSE train: 0.615464	val: 0.759572	test: 0.721958
MAE train: 0.482444	val: 0.568403	test: 0.561843

Epoch: 43
Loss: 0.4318535476922989
RMSE train: 0.619049	val: 0.750904	test: 0.726613
MAE train: 0.484998	val: 0.565409	test: 0.555652

Epoch: 44
Loss: 0.4431482603152593
RMSE train: 0.619239	val: 0.760457	test: 0.729285
MAE train: 0.488457	val: 0.577653	test: 0.563836

Epoch: 45
Loss: 0.43543769667545956
RMSE train: 0.591882	val: 0.746834	test: 0.697821
MAE train: 0.464012	val: 0.560767	test: 0.537455

Epoch: 46
Loss: 0.42934704820315045
RMSE train: 0.607679	val: 0.743966	test: 0.713369
MAE train: 0.475584	val: 0.560940	test: 0.550374

Epoch: 47
Loss: 0.4253503854076068
RMSE train: 0.625661	val: 0.751692	test: 0.728060
MAE train: 0.490476	val: 0.569329	test: 0.556043

Epoch: 48
Loss: 0.42281706631183624
RMSE train: 0.600692	val: 0.754323	test: 0.700949
MAE train: 0.471579	val: 0.564687	test: 0.541604

Epoch: 49
Loss: 0.4238985801736514
RMSE train: 0.604276	val: 0.759802	test: 0.711838
MAE train: 0.467494	val: 0.560008	test: 0.546055

Epoch: 50
Loss: 0.4091726119319598
RMSE train: 0.579645	val: 0.738576	test: 0.697011
MAE train: 0.452263	val: 0.543132	test: 0.534949

Epoch: 51
Loss: 0.4138815328478813
RMSE train: 0.597580	val: 0.748698	test: 0.706224
MAE train: 0.467859	val: 0.555920	test: 0.544235

Epoch: 52
Loss: 0.40133703003327054
RMSE train: 0.599203	val: 0.755500	test: 0.711431
MAE train: 0.466564	val: 0.560464	test: 0.542068

Epoch: 53
Loss: 0.406568964322408
RMSE train: 0.613206	val: 0.758539	test: 0.718135
MAE train: 0.480633	val: 0.565748	test: 0.549902

Epoch: 54
Loss: 0.40394704540570575
RMSE train: 0.582888	val: 0.746797	test: 0.705948
MAE train: 0.459470	val: 0.559054	test: 0.544281

Epoch: 55
Loss: 0.4090843324859937
RMSE train: 0.585362	val: 0.748312	test: 0.703234
MAE train: 0.458481	val: 0.556170	test: 0.538588

Epoch: 56
Loss: 0.40502669165531796
RMSE train: 0.583168	val: 0.737743	test: 0.700459
MAE train: 0.453747	val: 0.545284	test: 0.531040

Epoch: 57
Loss: 0.39427082737286884
RMSE train: 0.584454	val: 0.740370	test: 0.702630
MAE train: 0.458523	val: 0.548856	test: 0.541378

Epoch: 58
Loss: 0.3828198586901029
RMSE train: 0.577859	val: 0.750625	test: 0.698260
MAE train: 0.452188	val: 0.554259	test: 0.536347

Epoch: 59
Loss: 0.37244301040967304
RMSE train: 0.564245	val: 0.741984	test: 0.688601
MAE train: 0.442876	val: 0.548067	test: 0.530048

Epoch: 60
Loss: 0.40205831329027814
RMSE train: 0.580063	val: 0.733362	test: 0.691812
MAE train: 0.454095	val: 0.544758	test: 0.533177

Epoch: 61
Loss: 0.3722831979393959
RMSE train: 0.599778	val: 0.746698	test: 0.721780
MAE train: 0.466583	val: 0.552461	test: 0.554873

Epoch: 62
Loss: 0.3753105476498604
RMSE train: 0.560442	val: 0.741548	test: 0.692761
MAE train: 0.439620	val: 0.547588	test: 0.528605

Epoch: 63
Loss: 0.36450084547201794
RMSE train: 0.563697	val: 0.742656	test: 0.688399
MAE train: 0.441311	val: 0.547556	test: 0.523826

Epoch: 64
Loss: 0.36809538553158444
RMSE train: 0.575222	val: 0.740426	test: 0.714003
MAE train: 0.451301	val: 0.540813	test: 0.544501

Epoch: 65
Loss: 0.3555072198311488
RMSE train: 0.576712	val: 0.750962	test: 0.707808
MAE train: 0.449472	val: 0.545918	test: 0.540537

Epoch: 66
Loss: 0.36713170011838275
RMSE train: 0.564511	val: 0.741195	test: 0.699279
MAE train: 0.442156	val: 0.541357	test: 0.530325

Epoch: 67
Loss: 0.36702025681734085
RMSE train: 0.546445	val: 0.743970	test: 0.689021
MAE train: 0.429288	val: 0.557342	test: 0.534105

Epoch: 68
Loss: 0.3488362058997154
RMSE train: 0.566837	val: 0.731267	test: 0.696013
MAE train: 0.443782	val: 0.543014	test: 0.533026

Epoch: 69
Loss: 0.34437717994054157
RMSE train: 0.567198	val: 0.742568	test: 0.706051
MAE train: 0.441267	val: 0.539565	test: 0.536727

Epoch: 70
Loss: 0.3615044529239337
RMSE train: 0.566088	val: 0.729426	test: 0.699170
MAE train: 0.443352	val: 0.544907	test: 0.536407

Epoch: 71
Loss: 0.3385605489214261
RMSE train: 0.544158	val: 0.716375	test: 0.678728
MAE train: 0.424599	val: 0.530828	test: 0.514034

Epoch: 72
Loss: 0.35639676203330356
RMSE train: 0.545036	val: 0.725280	test: 0.680642
MAE train: 0.425803	val: 0.534437	test: 0.515278

Epoch: 73
Loss: 0.3473520403107007
RMSE train: 0.563613	val: 0.737087	test: 0.699263
MAE train: 0.438788	val: 0.544406	test: 0.529774

Epoch: 74
Loss: 0.34541696061690647
RMSE train: 0.548096	val: 0.731653	test: 0.690121
MAE train: 0.428873	val: 0.539406	test: 0.534112

Epoch: 75
Loss: 0.34041507293780643
RMSE train: 0.528628	val: 0.729419	test: 0.674143
MAE train: 0.415743	val: 0.535645	test: 0.517530

Epoch: 76
Loss: 0.34182511270046234
RMSE train: 0.524798	val: 0.732975	test: 0.677166
MAE train: 0.409509	val: 0.537216	test: 0.520407

Epoch: 77
Loss: 0.34935882687568665
RMSE train: 0.547651	val: 0.730546	test: 0.698839
MAE train: 0.427577	val: 0.534467	test: 0.530303

Epoch: 78
Loss: 0.3261238733927409
RMSE train: 0.511514	val: 0.719173	test: 0.675767
MAE train: 0.398051	val: 0.522339	test: 0.515357

Epoch: 79
Loss: 0.3413414830962817
RMSE train: 0.554399	val: 0.747446	test: 0.710531
MAE train: 0.433933	val: 0.546287	test: 0.537132

Epoch: 80
Loss: 0.32399295022090274
RMSE train: 0.537758	val: 0.751499	test: 0.693369
MAE train: 0.418536	val: 0.547542	test: 0.523691

Epoch: 81
Loss: 0.3323669706781705
RMSE train: 0.518615	val: 0.722313	test: 0.675600
MAE train: 0.404775	val: 0.526898	test: 0.511070

Epoch: 82
Loss: 0.3288245350122452
RMSE train: 0.521198	val: 0.730580	test: 0.671748
MAE train: 0.407391	val: 0.532420	test: 0.513272

Epoch: 83
Loss: 0.315888449549675
RMSE train: 0.528804	val: 0.730660	test: 0.689509

Epoch: 23
Loss: 0.5729024286071459
RMSE train: 0.670501	val: 0.778784	test: 0.759480
MAE train: 0.523688	val: 0.597021	test: 0.590431

Epoch: 24
Loss: 0.5547650257746378
RMSE train: 0.659675	val: 0.774101	test: 0.747175
MAE train: 0.515834	val: 0.593040	test: 0.581052

Epoch: 25
Loss: 0.5705604180693626
RMSE train: 0.697290	val: 0.788216	test: 0.770026
MAE train: 0.543859	val: 0.604563	test: 0.594644

Epoch: 26
Loss: 0.5570443893472353
RMSE train: 0.696993	val: 0.805931	test: 0.789324
MAE train: 0.542196	val: 0.616205	test: 0.609144

Epoch: 27
Loss: 0.5383112952113152
RMSE train: 0.707791	val: 0.802461	test: 0.775107
MAE train: 0.550792	val: 0.607816	test: 0.597811

Epoch: 28
Loss: 0.5448655560612679
RMSE train: 0.639328	val: 0.772149	test: 0.739979
MAE train: 0.501098	val: 0.590213	test: 0.576205

Epoch: 29
Loss: 0.536153977115949
RMSE train: 0.663082	val: 0.780484	test: 0.754807
MAE train: 0.519958	val: 0.599609	test: 0.589244

Epoch: 30
Loss: 0.5150783782203993
RMSE train: 0.649092	val: 0.767677	test: 0.751229
MAE train: 0.510950	val: 0.589544	test: 0.585274

Epoch: 31
Loss: 0.5050631985068321
RMSE train: 0.637921	val: 0.753823	test: 0.730981
MAE train: 0.498201	val: 0.576647	test: 0.570329

Epoch: 32
Loss: 0.4997153381506602
RMSE train: 0.646553	val: 0.765720	test: 0.747222
MAE train: 0.502902	val: 0.580324	test: 0.576314

Epoch: 33
Loss: 0.5059387385845184
RMSE train: 0.652316	val: 0.766781	test: 0.747854
MAE train: 0.510059	val: 0.580401	test: 0.573354

Epoch: 34
Loss: 0.4773931751648585
RMSE train: 0.638450	val: 0.750937	test: 0.733715
MAE train: 0.499087	val: 0.570927	test: 0.563887

Epoch: 35
Loss: 0.4889981870849927
RMSE train: 0.617384	val: 0.742760	test: 0.725166
MAE train: 0.482374	val: 0.561892	test: 0.562825

Epoch: 36
Loss: 0.47162936131159466
RMSE train: 0.625322	val: 0.762272	test: 0.729727
MAE train: 0.488891	val: 0.574864	test: 0.561586

Epoch: 37
Loss: 0.48360658437013626
RMSE train: 0.614432	val: 0.749364	test: 0.720093
MAE train: 0.479116	val: 0.564003	test: 0.553972

Epoch: 38
Loss: 0.4723595678806305
RMSE train: 0.624794	val: 0.763507	test: 0.732779
MAE train: 0.487842	val: 0.573542	test: 0.565921

Epoch: 39
Loss: 0.4485093156496684
RMSE train: 0.598454	val: 0.747010	test: 0.720751
MAE train: 0.466953	val: 0.565957	test: 0.558491

Epoch: 40
Loss: 0.4604843532045682
RMSE train: 0.613823	val: 0.755246	test: 0.738272
MAE train: 0.481817	val: 0.576139	test: 0.572051

Epoch: 41
Loss: 0.4538869261741638
RMSE train: 0.616985	val: 0.745852	test: 0.726735
MAE train: 0.480834	val: 0.560504	test: 0.557236

Epoch: 42
Loss: 0.46087557077407837
RMSE train: 0.605772	val: 0.745939	test: 0.732256
MAE train: 0.473548	val: 0.569603	test: 0.562603

Epoch: 43
Loss: 0.44970884670813877
RMSE train: 0.589539	val: 0.736659	test: 0.713660
MAE train: 0.459338	val: 0.556424	test: 0.550947

Epoch: 44
Loss: 0.44925340761741
RMSE train: 0.611376	val: 0.752732	test: 0.723486
MAE train: 0.478488	val: 0.566462	test: 0.553482

Epoch: 45
Loss: 0.44038698077201843
RMSE train: 0.620267	val: 0.764562	test: 0.745037
MAE train: 0.483790	val: 0.572887	test: 0.569734

Epoch: 46
Loss: 0.4251217419902484
RMSE train: 0.588184	val: 0.731520	test: 0.703390
MAE train: 0.459276	val: 0.539790	test: 0.535448

Epoch: 47
Loss: 0.40638820578654605
RMSE train: 0.571853	val: 0.729558	test: 0.698146
MAE train: 0.444499	val: 0.543532	test: 0.537331

Epoch: 48
Loss: 0.4183775360385577
RMSE train: 0.567170	val: 0.730617	test: 0.701582
MAE train: 0.443237	val: 0.550098	test: 0.540592

Epoch: 49
Loss: 0.4128035505612691
RMSE train: 0.579644	val: 0.733919	test: 0.706839
MAE train: 0.451845	val: 0.544813	test: 0.543303

Epoch: 50
Loss: 0.4160735259453456
RMSE train: 0.604405	val: 0.745393	test: 0.730657
MAE train: 0.473025	val: 0.554862	test: 0.558273

Epoch: 51
Loss: 0.3943595066666603
RMSE train: 0.583385	val: 0.737602	test: 0.716299
MAE train: 0.453632	val: 0.548880	test: 0.544834

Epoch: 52
Loss: 0.404173011581103
RMSE train: 0.572693	val: 0.739566	test: 0.709802
MAE train: 0.448866	val: 0.554258	test: 0.541601

Epoch: 53
Loss: 0.42934825271368027
RMSE train: 0.574542	val: 0.727140	test: 0.710891
MAE train: 0.450427	val: 0.538679	test: 0.543936

Epoch: 54
Loss: 0.41443730642398197
RMSE train: 0.565411	val: 0.728884	test: 0.710666
MAE train: 0.441355	val: 0.545987	test: 0.546073

Epoch: 55
Loss: 0.4090884601076444
RMSE train: 0.568253	val: 0.728060	test: 0.717340
MAE train: 0.445166	val: 0.544081	test: 0.546785

Epoch: 56
Loss: 0.40817125141620636
RMSE train: 0.557307	val: 0.724122	test: 0.697090
MAE train: 0.435671	val: 0.535760	test: 0.530077

Epoch: 57
Loss: 0.38931892315546673
RMSE train: 0.592051	val: 0.749549	test: 0.730879
MAE train: 0.464678	val: 0.559282	test: 0.550244

Epoch: 58
Loss: 0.3822650263706843
RMSE train: 0.570488	val: 0.734278	test: 0.712457
MAE train: 0.445987	val: 0.544310	test: 0.540814

Epoch: 59
Loss: 0.394546481470267
RMSE train: 0.563062	val: 0.726736	test: 0.696243
MAE train: 0.439102	val: 0.538435	test: 0.531925

Epoch: 60
Loss: 0.3874489317337672
RMSE train: 0.575196	val: 0.726144	test: 0.702779
MAE train: 0.450381	val: 0.536548	test: 0.534877

Epoch: 61
Loss: 0.39488623291254044
RMSE train: 0.575297	val: 0.742032	test: 0.721093
MAE train: 0.449832	val: 0.547302	test: 0.544648

Epoch: 62
Loss: 0.37251286456982297
RMSE train: 0.548525	val: 0.720476	test: 0.685980
MAE train: 0.426182	val: 0.532085	test: 0.515901

Epoch: 63
Loss: 0.3770945320526759
RMSE train: 0.545537	val: 0.714731	test: 0.695928
MAE train: 0.428261	val: 0.537108	test: 0.533719

Epoch: 64
Loss: 0.39025574674208957
RMSE train: 0.542115	val: 0.708357	test: 0.692182
MAE train: 0.424701	val: 0.535252	test: 0.532404

Epoch: 65
Loss: 0.37048429995775223
RMSE train: 0.537933	val: 0.718895	test: 0.686714
MAE train: 0.419730	val: 0.529350	test: 0.523004

Epoch: 66
Loss: 0.36319977045059204
RMSE train: 0.559947	val: 0.732991	test: 0.702775
MAE train: 0.439234	val: 0.541802	test: 0.532804

Epoch: 67
Loss: 0.36701401323080063
RMSE train: 0.528919	val: 0.726236	test: 0.694704
MAE train: 0.414809	val: 0.540280	test: 0.524809

Epoch: 68
Loss: 0.3444387863079707
RMSE train: 0.542236	val: 0.717735	test: 0.713363
MAE train: 0.424126	val: 0.531318	test: 0.537398

Epoch: 69
Loss: 0.3513667012254397
RMSE train: 0.528149	val: 0.714646	test: 0.706034
MAE train: 0.412022	val: 0.535407	test: 0.533218

Epoch: 70
Loss: 0.35481177270412445
RMSE train: 0.558958	val: 0.730030	test: 0.713916
MAE train: 0.437322	val: 0.534739	test: 0.536372

Epoch: 71
Loss: 0.35978662222623825
RMSE train: 0.576330	val: 0.738429	test: 0.727103
MAE train: 0.454263	val: 0.551523	test: 0.554172

Epoch: 72
Loss: 0.3598534017801285
RMSE train: 0.537644	val: 0.726095	test: 0.701500
MAE train: 0.421679	val: 0.547455	test: 0.529521

Epoch: 73
Loss: 0.3567569702863693
RMSE train: 0.544688	val: 0.732070	test: 0.703557
MAE train: 0.425827	val: 0.535328	test: 0.530145

Epoch: 74
Loss: 0.3352711647748947
RMSE train: 0.538043	val: 0.723172	test: 0.695982
MAE train: 0.419487	val: 0.525329	test: 0.523329

Epoch: 75
Loss: 0.349665400882562
RMSE train: 0.529965	val: 0.721590	test: 0.702110
MAE train: 0.418141	val: 0.536389	test: 0.540626

Epoch: 76
Loss: 0.34694475928942364
RMSE train: 0.521455	val: 0.713157	test: 0.694123
MAE train: 0.409051	val: 0.520312	test: 0.518938

Epoch: 77
Loss: 0.34464113662640256
RMSE train: 0.529864	val: 0.727778	test: 0.701320
MAE train: 0.412396	val: 0.531963	test: 0.523332

Epoch: 78
Loss: 0.333736814558506
RMSE train: 0.529800	val: 0.730417	test: 0.703084
MAE train: 0.411629	val: 0.533804	test: 0.528570

Epoch: 79
Loss: 0.3300500164429347
RMSE train: 0.520319	val: 0.719540	test: 0.695260
MAE train: 0.404980	val: 0.517695	test: 0.521966

Epoch: 80
Loss: 0.3276911328236262
RMSE train: 0.510192	val: 0.712959	test: 0.688415
MAE train: 0.398785	val: 0.530132	test: 0.518302

Epoch: 81
Loss: 0.3252487728993098
RMSE train: 0.530252	val: 0.726976	test: 0.704163
MAE train: 0.412415	val: 0.532674	test: 0.531810

Epoch: 82
Loss: 0.32558463762203854
RMSE train: 0.510937	val: 0.719236	test: 0.683357
MAE train: 0.397321	val: 0.517203	test: 0.510834

Epoch: 83
Loss: 0.32596247146526974
RMSE train: 0.506469	val: 0.712168	test: 0.687239

Epoch: 23
Loss: 0.5632397805651029
RMSE train: 0.678029	val: 0.780628	test: 0.773661
MAE train: 0.528227	val: 0.596711	test: 0.600607

Epoch: 24
Loss: 0.570450613896052
RMSE train: 0.718799	val: 0.817184	test: 0.815992
MAE train: 0.560934	val: 0.632101	test: 0.635104

Epoch: 25
Loss: 0.5628068372607231
RMSE train: 0.676618	val: 0.776644	test: 0.766387
MAE train: 0.530820	val: 0.597233	test: 0.599587

Epoch: 26
Loss: 0.5369953836003939
RMSE train: 0.661727	val: 0.762350	test: 0.755721
MAE train: 0.520568	val: 0.586219	test: 0.590910

Epoch: 27
Loss: 0.5245318040251732
RMSE train: 0.669547	val: 0.784964	test: 0.772068
MAE train: 0.521249	val: 0.599923	test: 0.599772

Epoch: 28
Loss: 0.5373255213101705
RMSE train: 0.667207	val: 0.772722	test: 0.758161
MAE train: 0.525868	val: 0.590449	test: 0.590730

Epoch: 29
Loss: 0.5203413516283035
RMSE train: 0.669432	val: 0.777361	test: 0.755855
MAE train: 0.524143	val: 0.596769	test: 0.589532

Epoch: 30
Loss: 0.5217329363028208
RMSE train: 0.657706	val: 0.770606	test: 0.754153
MAE train: 0.517092	val: 0.594542	test: 0.587341

Epoch: 31
Loss: 0.5092094962795576
RMSE train: 0.654200	val: 0.769273	test: 0.750994
MAE train: 0.512537	val: 0.590560	test: 0.584609

Epoch: 32
Loss: 0.5070125286777815
RMSE train: 0.630534	val: 0.759419	test: 0.733295
MAE train: 0.492994	val: 0.578828	test: 0.567828

Epoch: 33
Loss: 0.48989483962456387
RMSE train: 0.656963	val: 0.775983	test: 0.754747
MAE train: 0.512947	val: 0.587798	test: 0.583839

Epoch: 34
Loss: 0.48160985857248306
RMSE train: 0.639607	val: 0.749407	test: 0.753272
MAE train: 0.497605	val: 0.571736	test: 0.582957

Epoch: 35
Loss: 0.49438876410325366
RMSE train: 0.653056	val: 0.756380	test: 0.751553
MAE train: 0.511200	val: 0.581548	test: 0.582873

Epoch: 36
Loss: 0.4927312731742859
RMSE train: 0.645411	val: 0.755962	test: 0.750911
MAE train: 0.503444	val: 0.573582	test: 0.578853

Epoch: 37
Loss: 0.48705122371514636
RMSE train: 0.637018	val: 0.752749	test: 0.748027
MAE train: 0.499278	val: 0.580403	test: 0.579242

Epoch: 38
Loss: 0.4663556714852651
RMSE train: 0.603239	val: 0.730034	test: 0.712242
MAE train: 0.469320	val: 0.552866	test: 0.550763

Epoch: 39
Loss: 0.46589835236469906
RMSE train: 0.662788	val: 0.776149	test: 0.786436
MAE train: 0.517251	val: 0.592341	test: 0.599757

Epoch: 40
Loss: 0.4538206954797109
RMSE train: 0.652090	val: 0.753755	test: 0.767510
MAE train: 0.507538	val: 0.570130	test: 0.586999

Epoch: 41
Loss: 0.45649929841359455
RMSE train: 0.590615	val: 0.723830	test: 0.720765
MAE train: 0.457655	val: 0.543912	test: 0.553352

Epoch: 42
Loss: 0.4617298866311709
RMSE train: 0.604437	val: 0.726227	test: 0.727800
MAE train: 0.472433	val: 0.548690	test: 0.562605

Epoch: 43
Loss: 0.43852074940999347
RMSE train: 0.625426	val: 0.743975	test: 0.741179
MAE train: 0.487202	val: 0.565316	test: 0.566372

Epoch: 44
Loss: 0.4577482268214226
RMSE train: 0.610535	val: 0.731491	test: 0.732568
MAE train: 0.477561	val: 0.556125	test: 0.563912

Epoch: 45
Loss: 0.4455574154853821
RMSE train: 0.588473	val: 0.714411	test: 0.723828
MAE train: 0.461897	val: 0.548905	test: 0.560441

Epoch: 46
Loss: 0.44446005175511044
RMSE train: 0.617850	val: 0.741178	test: 0.750473
MAE train: 0.481566	val: 0.562537	test: 0.574089

Epoch: 47
Loss: 0.43178999175628024
RMSE train: 0.613001	val: 0.732014	test: 0.736386
MAE train: 0.478260	val: 0.550472	test: 0.563031

Epoch: 48
Loss: 0.41963231066862744
RMSE train: 0.587306	val: 0.729815	test: 0.726710
MAE train: 0.457900	val: 0.556059	test: 0.562432

Epoch: 49
Loss: 0.4218761846423149
RMSE train: 0.574462	val: 0.717164	test: 0.710873
MAE train: 0.444634	val: 0.534878	test: 0.543155

Epoch: 50
Loss: 0.4239218185345332
RMSE train: 0.598301	val: 0.723288	test: 0.737727
MAE train: 0.465201	val: 0.545210	test: 0.561612

Epoch: 51
Loss: 0.42945485810438794
RMSE train: 0.628454	val: 0.747087	test: 0.760737
MAE train: 0.489157	val: 0.560824	test: 0.579985

Epoch: 52
Loss: 0.4129127835234006
RMSE train: 0.573357	val: 0.717582	test: 0.719711
MAE train: 0.444893	val: 0.533224	test: 0.551202

Epoch: 53
Loss: 0.4147406443953514
RMSE train: 0.590797	val: 0.729253	test: 0.730914
MAE train: 0.457558	val: 0.540716	test: 0.553922

Epoch: 54
Loss: 0.40527240683635074
RMSE train: 0.589586	val: 0.721121	test: 0.723510
MAE train: 0.458089	val: 0.539797	test: 0.555927

Epoch: 55
Loss: 0.40862854073445004
RMSE train: 0.559424	val: 0.718393	test: 0.711655
MAE train: 0.433848	val: 0.536520	test: 0.546386

Epoch: 56
Loss: 0.3939673925439517
RMSE train: 0.570041	val: 0.714855	test: 0.712393
MAE train: 0.443359	val: 0.532995	test: 0.550159

Epoch: 57
Loss: 0.39337624112764996
RMSE train: 0.589948	val: 0.734264	test: 0.736506
MAE train: 0.456418	val: 0.544551	test: 0.561291

Epoch: 58
Loss: 0.4114243487517039
RMSE train: 0.622305	val: 0.755107	test: 0.765475
MAE train: 0.487986	val: 0.568033	test: 0.589184

Epoch: 59
Loss: 0.40332935005426407
RMSE train: 0.575740	val: 0.737732	test: 0.720080
MAE train: 0.447013	val: 0.539685	test: 0.553180

Epoch: 60
Loss: 0.38311997056007385
RMSE train: 0.568964	val: 0.725976	test: 0.729189
MAE train: 0.442862	val: 0.531325	test: 0.557143

Epoch: 61
Loss: 0.37787415087223053
RMSE train: 0.576468	val: 0.724844	test: 0.730091
MAE train: 0.451035	val: 0.534559	test: 0.553873

Epoch: 62
Loss: 0.3705384135246277
RMSE train: 0.579023	val: 0.733731	test: 0.726613
MAE train: 0.452384	val: 0.541561	test: 0.560908

Epoch: 63
Loss: 0.3833177189032237
RMSE train: 0.558970	val: 0.711380	test: 0.715314
MAE train: 0.434612	val: 0.526744	test: 0.544715

Epoch: 64
Loss: 0.3681821624437968
RMSE train: 0.596883	val: 0.741020	test: 0.755256
MAE train: 0.466064	val: 0.554907	test: 0.575890

Epoch: 65
Loss: 0.35379550109306973
RMSE train: 0.544004	val: 0.714411	test: 0.714539
MAE train: 0.421163	val: 0.523657	test: 0.542353

Epoch: 66
Loss: 0.35837167501449585
RMSE train: 0.563458	val: 0.712754	test: 0.721888
MAE train: 0.439159	val: 0.522746	test: 0.549984

Epoch: 67
Loss: 0.3525449534257253
RMSE train: 0.556494	val: 0.726383	test: 0.725340
MAE train: 0.431589	val: 0.531268	test: 0.556165

Epoch: 68
Loss: 0.3533784970641136
RMSE train: 0.553611	val: 0.706394	test: 0.718669
MAE train: 0.430822	val: 0.514486	test: 0.545408

Epoch: 69
Loss: 0.3565446635087331
RMSE train: 0.589075	val: 0.730076	test: 0.749566
MAE train: 0.460359	val: 0.546242	test: 0.573684

Epoch: 70
Loss: 0.3510546609759331
RMSE train: 0.557698	val: 0.720548	test: 0.720481
MAE train: 0.433066	val: 0.529944	test: 0.546190

Epoch: 71
Loss: 0.34562980631987256
RMSE train: 0.531214	val: 0.720195	test: 0.705365
MAE train: 0.411806	val: 0.522923	test: 0.532235

Epoch: 72
Loss: 0.3496499756971995
RMSE train: 0.537851	val: 0.716707	test: 0.714371
MAE train: 0.419139	val: 0.524789	test: 0.542745

Epoch: 73
Loss: 0.33627400050560635
RMSE train: 0.523537	val: 0.708711	test: 0.705055
MAE train: 0.410114	val: 0.521204	test: 0.535278

Epoch: 74
Loss: 0.35166505227486294
RMSE train: 0.528275	val: 0.705667	test: 0.710870
MAE train: 0.412607	val: 0.519769	test: 0.540175

Epoch: 75
Loss: 0.3460340251525243
RMSE train: 0.542283	val: 0.717932	test: 0.720281
MAE train: 0.423397	val: 0.531195	test: 0.545507

Epoch: 76
Loss: 0.32413480927546817
RMSE train: 0.521479	val: 0.703607	test: 0.710085
MAE train: 0.405225	val: 0.515428	test: 0.535410

Epoch: 77
Loss: 0.34067922582228977
RMSE train: 0.599197	val: 0.751296	test: 0.758623
MAE train: 0.467966	val: 0.556342	test: 0.579332

Epoch: 78
Loss: 0.3378369038303693
RMSE train: 0.521645	val: 0.693330	test: 0.700200
MAE train: 0.406336	val: 0.504424	test: 0.528219

Epoch: 79
Loss: 0.3301841119925181
RMSE train: 0.522557	val: 0.712775	test: 0.716923
MAE train: 0.407494	val: 0.521720	test: 0.542270

Epoch: 80
Loss: 0.32876181850830716
RMSE train: 0.520214	val: 0.707219	test: 0.718228
MAE train: 0.405699	val: 0.513448	test: 0.537802

Epoch: 81
Loss: 0.3317955980698268
RMSE train: 0.518115	val: 0.706068	test: 0.705285
MAE train: 0.407087	val: 0.518114	test: 0.539207

Epoch: 82
Loss: 0.3153757279117902
RMSE train: 0.521780	val: 0.706346	test: 0.708655
MAE train: 0.405362	val: 0.517142	test: 0.536807

Epoch: 83
Loss: 0.32007161527872086
RMSE train: 0.538675	val: 0.727033	test: 0.725127

Epoch: 23
Loss: 0.5800014521394458
RMSE train: 0.696515	val: 0.800489	test: 0.749892
MAE train: 0.540451	val: 0.618177	test: 0.572271

Epoch: 24
Loss: 0.5929810575076512
RMSE train: 0.682613	val: 0.794486	test: 0.736872
MAE train: 0.532247	val: 0.618696	test: 0.569198

Epoch: 25
Loss: 0.5604760944843292
RMSE train: 0.718842	val: 0.846138	test: 0.784979
MAE train: 0.556518	val: 0.650328	test: 0.600299

Epoch: 26
Loss: 0.5878898565258298
RMSE train: 0.680059	val: 0.804680	test: 0.747541
MAE train: 0.532125	val: 0.622963	test: 0.574522

Epoch: 27
Loss: 0.5488617249897548
RMSE train: 0.660597	val: 0.804527	test: 0.724934
MAE train: 0.517348	val: 0.617135	test: 0.561346

Epoch: 28
Loss: 0.5539440214633942
RMSE train: 0.685975	val: 0.820021	test: 0.740913
MAE train: 0.533421	val: 0.626734	test: 0.569213

Epoch: 29
Loss: 0.5274775943585804
RMSE train: 0.664041	val: 0.791678	test: 0.738752
MAE train: 0.515580	val: 0.612346	test: 0.566847

Epoch: 30
Loss: 0.5268280782869884
RMSE train: 0.668159	val: 0.800428	test: 0.739232
MAE train: 0.522014	val: 0.620871	test: 0.566955

Epoch: 31
Loss: 0.5274435835225242
RMSE train: 0.670067	val: 0.793437	test: 0.730855
MAE train: 0.527146	val: 0.618858	test: 0.577830

Epoch: 32
Loss: 0.5302897500140327
RMSE train: 0.649532	val: 0.779883	test: 0.705577
MAE train: 0.508874	val: 0.605511	test: 0.548109

Epoch: 33
Loss: 0.5401744565793446
RMSE train: 0.695188	val: 0.836820	test: 0.767932
MAE train: 0.543918	val: 0.648155	test: 0.592434

Epoch: 34
Loss: 0.5164684270109449
RMSE train: 0.656911	val: 0.796765	test: 0.724961
MAE train: 0.512008	val: 0.612444	test: 0.561098

Epoch: 35
Loss: 0.4980440842253821
RMSE train: 0.642709	val: 0.780344	test: 0.715295
MAE train: 0.500133	val: 0.601814	test: 0.549677

Epoch: 36
Loss: 0.5010178110429219
RMSE train: 0.631171	val: 0.775346	test: 0.701798
MAE train: 0.491405	val: 0.604873	test: 0.539144

Epoch: 37
Loss: 0.4869856578963144
RMSE train: 0.619414	val: 0.765283	test: 0.683016
MAE train: 0.481754	val: 0.587845	test: 0.528858

Epoch: 38
Loss: 0.4737357199192047
RMSE train: 0.641649	val: 0.780050	test: 0.705074
MAE train: 0.503280	val: 0.609155	test: 0.545055

Epoch: 39
Loss: 0.4942133107355663
RMSE train: 0.620967	val: 0.763317	test: 0.682952
MAE train: 0.483174	val: 0.585166	test: 0.529063

Epoch: 40
Loss: 0.4821994240794863
RMSE train: 0.629746	val: 0.780871	test: 0.703026
MAE train: 0.489932	val: 0.598297	test: 0.536610

Epoch: 41
Loss: 0.4867330214806965
RMSE train: 0.645122	val: 0.785262	test: 0.701779
MAE train: 0.505867	val: 0.604386	test: 0.540978

Epoch: 42
Loss: 0.47722562083176207
RMSE train: 0.622637	val: 0.771255	test: 0.686402
MAE train: 0.484595	val: 0.596228	test: 0.525994

Epoch: 43
Loss: 0.4613203299897058
RMSE train: 0.627175	val: 0.773243	test: 0.679798
MAE train: 0.492527	val: 0.591484	test: 0.528124

Epoch: 44
Loss: 0.47270203701087404
RMSE train: 0.625180	val: 0.771232	test: 0.682744
MAE train: 0.490828	val: 0.593178	test: 0.530405

Epoch: 45
Loss: 0.4495758478130613
RMSE train: 0.620313	val: 0.770659	test: 0.684112
MAE train: 0.485291	val: 0.599264	test: 0.532611

Epoch: 46
Loss: 0.44836372775690897
RMSE train: 0.604417	val: 0.754679	test: 0.678799
MAE train: 0.472421	val: 0.580180	test: 0.527291

Epoch: 47
Loss: 0.45573742261954714
RMSE train: 0.605539	val: 0.760246	test: 0.674549
MAE train: 0.469844	val: 0.589533	test: 0.511719

Epoch: 48
Loss: 0.45667041199547903
RMSE train: 0.599750	val: 0.759887	test: 0.672571
MAE train: 0.467175	val: 0.584580	test: 0.513832

Epoch: 49
Loss: 0.4508245587348938
RMSE train: 0.608410	val: 0.761504	test: 0.683250
MAE train: 0.477483	val: 0.592396	test: 0.521159

Epoch: 50
Loss: 0.4350740270955222
RMSE train: 0.602564	val: 0.751796	test: 0.669251
MAE train: 0.470660	val: 0.581779	test: 0.510858

Epoch: 51
Loss: 0.4365269903625761
RMSE train: 0.604142	val: 0.760792	test: 0.671087
MAE train: 0.473452	val: 0.587856	test: 0.523552

Epoch: 52
Loss: 0.43204456142016817
RMSE train: 0.609432	val: 0.759844	test: 0.689007
MAE train: 0.479593	val: 0.587906	test: 0.529849

Epoch: 53
Loss: 0.43128727802208494
RMSE train: 0.585767	val: 0.746336	test: 0.661000
MAE train: 0.455283	val: 0.571978	test: 0.506499

Epoch: 54
Loss: 0.4365965170519693
RMSE train: 0.580903	val: 0.749104	test: 0.652849
MAE train: 0.454429	val: 0.575829	test: 0.502997

Epoch: 55
Loss: 0.43983214029244017
RMSE train: 0.599206	val: 0.754274	test: 0.666091
MAE train: 0.469708	val: 0.579222	test: 0.509405

Epoch: 56
Loss: 0.4241659470966884
RMSE train: 0.580930	val: 0.756852	test: 0.666513
MAE train: 0.451931	val: 0.578943	test: 0.519365

Epoch: 57
Loss: 0.44472074082919527
RMSE train: 0.578570	val: 0.756235	test: 0.669262
MAE train: 0.451181	val: 0.573253	test: 0.512212

Epoch: 58
Loss: 0.4162322389228003
RMSE train: 0.606150	val: 0.771023	test: 0.688943
MAE train: 0.474310	val: 0.586816	test: 0.526776

Epoch: 59
Loss: 0.40808968245983124
RMSE train: 0.602154	val: 0.771433	test: 0.695651
MAE train: 0.467520	val: 0.589003	test: 0.527278

Epoch: 60
Loss: 0.3955687774079187
RMSE train: 0.562220	val: 0.740419	test: 0.648369
MAE train: 0.440059	val: 0.571484	test: 0.498470

Epoch: 61
Loss: 0.39060130289622713
RMSE train: 0.565782	val: 0.755140	test: 0.662288
MAE train: 0.440602	val: 0.577082	test: 0.507911

Epoch: 62
Loss: 0.38358188526971
RMSE train: 0.560674	val: 0.742757	test: 0.651418
MAE train: 0.436700	val: 0.563769	test: 0.505082

Epoch: 63
Loss: 0.3948803948504584
RMSE train: 0.559796	val: 0.744000	test: 0.655164
MAE train: 0.435877	val: 0.568864	test: 0.503734

Epoch: 64
Loss: 0.3822041175195149
RMSE train: 0.555303	val: 0.744299	test: 0.643107
MAE train: 0.433104	val: 0.567735	test: 0.494580

Epoch: 65
Loss: 0.37936907368046896
RMSE train: 0.560046	val: 0.752690	test: 0.648567
MAE train: 0.436815	val: 0.574011	test: 0.500535

Epoch: 66
Loss: 0.3922363030058997
RMSE train: 0.561160	val: 0.745994	test: 0.647670
MAE train: 0.439250	val: 0.568003	test: 0.492618

Epoch: 67
Loss: 0.3834956650223051
RMSE train: 0.561851	val: 0.758807	test: 0.653795
MAE train: 0.436791	val: 0.572852	test: 0.498681

Epoch: 68
Loss: 0.38345539569854736
RMSE train: 0.553513	val: 0.750122	test: 0.664156
MAE train: 0.430688	val: 0.572775	test: 0.510636

Epoch: 69
Loss: 0.37000203132629395
RMSE train: 0.559470	val: 0.748491	test: 0.646132
MAE train: 0.433028	val: 0.566093	test: 0.488041

Epoch: 70
Loss: 0.37425736657210756
RMSE train: 0.557752	val: 0.746174	test: 0.635647
MAE train: 0.434716	val: 0.565713	test: 0.486513

Epoch: 71
Loss: 0.3857886620930263
RMSE train: 0.551745	val: 0.757342	test: 0.643798
MAE train: 0.427714	val: 0.566671	test: 0.487283

Epoch: 72
Loss: 0.3656223884650639
RMSE train: 0.568933	val: 0.764456	test: 0.671314
MAE train: 0.446350	val: 0.584967	test: 0.511170

Epoch: 73
Loss: 0.3562962774719511
RMSE train: 0.552576	val: 0.753460	test: 0.659943
MAE train: 0.428132	val: 0.567986	test: 0.499119

Epoch: 74
Loss: 0.3502196690865925
RMSE train: 0.541847	val: 0.746131	test: 0.646765
MAE train: 0.420719	val: 0.559531	test: 0.491535

Epoch: 75
Loss: 0.36478438121931894
RMSE train: 0.539915	val: 0.748926	test: 0.642422
MAE train: 0.422490	val: 0.564964	test: 0.497007

Epoch: 76
Loss: 0.35721929371356964
RMSE train: 0.550966	val: 0.769208	test: 0.659342
MAE train: 0.433326	val: 0.579452	test: 0.511913

Epoch: 77
Loss: 0.35800702444144655
RMSE train: 0.535149	val: 0.756272	test: 0.637468
MAE train: 0.419378	val: 0.566160	test: 0.487655

Epoch: 78
Loss: 0.3517886293785913
RMSE train: 0.559216	val: 0.773854	test: 0.674925
MAE train: 0.433942	val: 0.581092	test: 0.513940

Epoch: 79
Loss: 0.3419390618801117
RMSE train: 0.530201	val: 0.742283	test: 0.635050
MAE train: 0.415246	val: 0.554434	test: 0.484870

Epoch: 80
Loss: 0.34969585282461985
RMSE train: 0.531121	val: 0.736894	test: 0.632446
MAE train: 0.415219	val: 0.553001	test: 0.485227

Epoch: 81
Loss: 0.35362652369907926
RMSE train: 0.553753	val: 0.752284	test: 0.640366
MAE train: 0.432609	val: 0.573121	test: 0.496844

Epoch: 82
Loss: 0.36308404164654867
RMSE train: 0.533179	val: 0.761247	test: 0.635824
MAE train: 0.415458	val: 0.569832	test: 0.485475

Epoch: 83
Loss: 0.33813130642686573
RMSE train: 0.555535	val: 0.752379	test: 0.652322

Epoch: 23
Loss: 0.6027856639453343
RMSE train: 0.683081	val: 0.832081	test: 0.728078
MAE train: 0.535496	val: 0.640715	test: 0.560267

Epoch: 24
Loss: 0.5607200286218098
RMSE train: 0.717361	val: 0.867740	test: 0.746369
MAE train: 0.561087	val: 0.665233	test: 0.575049

Epoch: 25
Loss: 0.5612434276512691
RMSE train: 0.727007	val: 0.879730	test: 0.770257
MAE train: 0.567864	val: 0.678909	test: 0.593645

Epoch: 26
Loss: 0.5456650576421193
RMSE train: 0.691146	val: 0.843115	test: 0.737814
MAE train: 0.539440	val: 0.655842	test: 0.560768

Epoch: 27
Loss: 0.5467712474720818
RMSE train: 0.662717	val: 0.811473	test: 0.710420
MAE train: 0.521124	val: 0.628796	test: 0.546062

Epoch: 28
Loss: 0.520450044955526
RMSE train: 0.663844	val: 0.798901	test: 0.714260
MAE train: 0.523172	val: 0.624929	test: 0.560544

Epoch: 29
Loss: 0.5226125355277743
RMSE train: 0.682989	val: 0.833014	test: 0.727256
MAE train: 0.534663	val: 0.637335	test: 0.557355

Epoch: 30
Loss: 0.5150945846523557
RMSE train: 0.668390	val: 0.830684	test: 0.700789
MAE train: 0.525207	val: 0.635109	test: 0.544372

Epoch: 31
Loss: 0.5259500316211155
RMSE train: 0.679730	val: 0.827722	test: 0.735865
MAE train: 0.532401	val: 0.649674	test: 0.561384

Epoch: 32
Loss: 0.5217018659625735
RMSE train: 0.644726	val: 0.792768	test: 0.697582
MAE train: 0.506966	val: 0.615080	test: 0.540756

Epoch: 33
Loss: 0.49330829083919525
RMSE train: 0.683494	val: 0.840897	test: 0.733808
MAE train: 0.535503	val: 0.654010	test: 0.559219

Epoch: 34
Loss: 0.5152193435600826
RMSE train: 0.639834	val: 0.796594	test: 0.698360
MAE train: 0.496948	val: 0.612167	test: 0.533394

Epoch: 35
Loss: 0.5243144716535296
RMSE train: 0.656290	val: 0.800103	test: 0.699752
MAE train: 0.517131	val: 0.619302	test: 0.542533

Epoch: 36
Loss: 0.49234865392957416
RMSE train: 0.630780	val: 0.803593	test: 0.694422
MAE train: 0.489809	val: 0.613033	test: 0.532467

Epoch: 37
Loss: 0.4605223217180797
RMSE train: 0.651447	val: 0.814793	test: 0.713985
MAE train: 0.509389	val: 0.620426	test: 0.537200

Epoch: 38
Loss: 0.48134471263204304
RMSE train: 0.639726	val: 0.808054	test: 0.710423
MAE train: 0.503761	val: 0.621300	test: 0.544385

Epoch: 39
Loss: 0.4790340598140444
RMSE train: 0.635486	val: 0.807892	test: 0.686469
MAE train: 0.495801	val: 0.614686	test: 0.524915

Epoch: 40
Loss: 0.4590540443147932
RMSE train: 0.608494	val: 0.804021	test: 0.677249
MAE train: 0.474574	val: 0.602311	test: 0.514638

Epoch: 41
Loss: 0.44684551869119915
RMSE train: 0.610566	val: 0.790810	test: 0.675658
MAE train: 0.475308	val: 0.597061	test: 0.516209

Epoch: 42
Loss: 0.44282595813274384
RMSE train: 0.634809	val: 0.807354	test: 0.694232
MAE train: 0.494631	val: 0.614679	test: 0.521488

Epoch: 43
Loss: 0.45602360580648693
RMSE train: 0.622455	val: 0.794175	test: 0.690568
MAE train: 0.486750	val: 0.607290	test: 0.526939

Epoch: 44
Loss: 0.456655244742121
RMSE train: 0.598869	val: 0.787709	test: 0.671100
MAE train: 0.466731	val: 0.596369	test: 0.511789

Epoch: 45
Loss: 0.4570657249007906
RMSE train: 0.644744	val: 0.829968	test: 0.707296
MAE train: 0.500091	val: 0.622873	test: 0.536772

Epoch: 46
Loss: 0.4345622531005314
RMSE train: 0.588889	val: 0.783708	test: 0.673491
MAE train: 0.457836	val: 0.593217	test: 0.507983

Epoch: 47
Loss: 0.4485056442873819
RMSE train: 0.606341	val: 0.785814	test: 0.677729
MAE train: 0.473520	val: 0.594329	test: 0.520751

Epoch: 48
Loss: 0.42542728568826405
RMSE train: 0.614871	val: 0.797855	test: 0.676685
MAE train: 0.477116	val: 0.607353	test: 0.515649

Epoch: 49
Loss: 0.4369507474558694
RMSE train: 0.608638	val: 0.778075	test: 0.683099
MAE train: 0.477556	val: 0.595937	test: 0.525180

Epoch: 50
Loss: 0.46274266498429434
RMSE train: 0.608511	val: 0.777491	test: 0.685151
MAE train: 0.475825	val: 0.601476	test: 0.530054

Epoch: 51
Loss: 0.4298038227217538
RMSE train: 0.611055	val: 0.784173	test: 0.704185
MAE train: 0.476538	val: 0.604491	test: 0.526293

Epoch: 52
Loss: 0.42860028786318644
RMSE train: 0.583235	val: 0.764600	test: 0.673464
MAE train: 0.456462	val: 0.586729	test: 0.512732

Epoch: 53
Loss: 0.40173907577991486
RMSE train: 0.599889	val: 0.785040	test: 0.699091
MAE train: 0.468793	val: 0.597176	test: 0.523406

Epoch: 54
Loss: 0.40922969579696655
RMSE train: 0.595612	val: 0.790667	test: 0.690373
MAE train: 0.466139	val: 0.596223	test: 0.515420

Epoch: 55
Loss: 0.40025850917611805
RMSE train: 0.609459	val: 0.814431	test: 0.692453
MAE train: 0.471828	val: 0.609505	test: 0.520046

Epoch: 56
Loss: 0.42574769258499146
RMSE train: 0.570011	val: 0.798489	test: 0.672512
MAE train: 0.443474	val: 0.594923	test: 0.504458

Epoch: 57
Loss: 0.40582951051848276
RMSE train: 0.576891	val: 0.799551	test: 0.666399
MAE train: 0.448631	val: 0.593899	test: 0.499017

Epoch: 58
Loss: 0.48949871744428364
RMSE train: 0.577741	val: 0.774931	test: 0.673345
MAE train: 0.451636	val: 0.593154	test: 0.506692

Epoch: 59
Loss: 0.4248699652297156
RMSE train: 0.596688	val: 0.802618	test: 0.674380
MAE train: 0.468905	val: 0.610122	test: 0.519046

Epoch: 60
Loss: 0.3962825004543577
RMSE train: 0.567400	val: 0.765245	test: 0.654442
MAE train: 0.441679	val: 0.580712	test: 0.495982

Epoch: 61
Loss: 0.41166958638599943
RMSE train: 0.588635	val: 0.780756	test: 0.686338
MAE train: 0.459945	val: 0.598042	test: 0.527379

Epoch: 62
Loss: 0.3984192673649107
RMSE train: 0.590923	val: 0.781441	test: 0.669651
MAE train: 0.461703	val: 0.592245	test: 0.498702

Epoch: 63
Loss: 0.4091767988034657
RMSE train: 0.580354	val: 0.754039	test: 0.669890
MAE train: 0.451936	val: 0.577366	test: 0.501415

Epoch: 64
Loss: 0.3855468141181128
RMSE train: 0.561770	val: 0.759628	test: 0.647950
MAE train: 0.438100	val: 0.569386	test: 0.488150

Epoch: 65
Loss: 0.38927776898656574
RMSE train: 0.569544	val: 0.767079	test: 0.656564
MAE train: 0.447082	val: 0.578349	test: 0.488809

Epoch: 66
Loss: 0.3848180856023516
RMSE train: 0.565472	val: 0.769175	test: 0.661767
MAE train: 0.444227	val: 0.576082	test: 0.500676

Epoch: 67
Loss: 0.3568661404507501
RMSE train: 0.567649	val: 0.778858	test: 0.682541
MAE train: 0.442310	val: 0.576791	test: 0.514826

Epoch: 68
Loss: 0.3765913439648492
RMSE train: 0.573342	val: 0.771849	test: 0.680722
MAE train: 0.451185	val: 0.582556	test: 0.511320

Epoch: 69
Loss: 0.3973695593220847
RMSE train: 0.565551	val: 0.777786	test: 0.671488
MAE train: 0.442743	val: 0.586309	test: 0.502961

Epoch: 70
Loss: 0.3648226814610617
RMSE train: 0.534127	val: 0.750785	test: 0.650831
MAE train: 0.414858	val: 0.565727	test: 0.487517

Epoch: 71
Loss: 0.3804901497704642
RMSE train: 0.552149	val: 0.773428	test: 0.664148
MAE train: 0.432451	val: 0.578581	test: 0.497673

Epoch: 72
Loss: 0.3469537837164743
RMSE train: 0.548108	val: 0.774139	test: 0.662588
MAE train: 0.431199	val: 0.572413	test: 0.495971

Epoch: 73
Loss: 0.3386344973530088
RMSE train: 0.546237	val: 0.768231	test: 0.659229
MAE train: 0.422916	val: 0.567929	test: 0.484969

Epoch: 74
Loss: 0.402804040483066
RMSE train: 0.563231	val: 0.788499	test: 0.674593
MAE train: 0.439790	val: 0.583851	test: 0.495385

Epoch: 75
Loss: 0.39756494973387035
RMSE train: 0.564653	val: 0.769225	test: 0.686303
MAE train: 0.440907	val: 0.577731	test: 0.506804

Epoch: 76
Loss: 0.38394914141723085
RMSE train: 0.580276	val: 0.789352	test: 0.674337
MAE train: 0.449559	val: 0.593398	test: 0.499675

Epoch: 77
Loss: 0.357505310858999
RMSE train: 0.530168	val: 0.763780	test: 0.647954
MAE train: 0.414135	val: 0.567223	test: 0.482239

Epoch: 78
Loss: 0.356380147593362
RMSE train: 0.591041	val: 0.791080	test: 0.712196
MAE train: 0.460298	val: 0.587970	test: 0.525112

Epoch: 79
Loss: 0.35686527618340086
RMSE train: 0.537634	val: 0.770829	test: 0.649137
MAE train: 0.421708	val: 0.571655	test: 0.486548

Epoch: 80
Loss: 0.3530642496688025
RMSE train: 0.539538	val: 0.784556	test: 0.662449
MAE train: 0.419392	val: 0.577570	test: 0.495858

Epoch: 81
Loss: 0.3478796205350331
RMSE train: 0.535698	val: 0.763208	test: 0.665960
MAE train: 0.419638	val: 0.568317	test: 0.493108

Epoch: 82
Loss: 0.3528603230203901
RMSE train: 0.543458	val: 0.777460	test: 0.668574
MAE train: 0.424099	val: 0.574979	test: 0.497170

Epoch: 83
Loss: 0.3315868909869875
RMSE train: 0.563901	val: 0.798489	test: 0.686193

Epoch: 23
Loss: 0.5748931297234127
RMSE train: 0.703828	val: 0.880705	test: 0.775280
MAE train: 0.547631	val: 0.676261	test: 0.594988

Epoch: 24
Loss: 0.5409558841160366
RMSE train: 0.677618	val: 0.858651	test: 0.747470
MAE train: 0.526161	val: 0.657441	test: 0.565110

Epoch: 25
Loss: 0.5447625411408288
RMSE train: 0.688811	val: 0.854946	test: 0.755079
MAE train: 0.538080	val: 0.660866	test: 0.574300

Epoch: 26
Loss: 0.5534060086522784
RMSE train: 0.713764	val: 0.877816	test: 0.773798
MAE train: 0.555442	val: 0.675968	test: 0.588149

Epoch: 27
Loss: 0.5247799030372075
RMSE train: 0.677878	val: 0.851659	test: 0.743545
MAE train: 0.529979	val: 0.655562	test: 0.564094

Epoch: 28
Loss: 0.5248026528528759
RMSE train: 0.733672	val: 0.890226	test: 0.782817
MAE train: 0.572889	val: 0.685935	test: 0.597214

Epoch: 29
Loss: 0.5156671021665845
RMSE train: 0.655747	val: 0.834453	test: 0.718449
MAE train: 0.509571	val: 0.633815	test: 0.547749

Epoch: 30
Loss: 0.5430895579712731
RMSE train: 0.665093	val: 0.843086	test: 0.730498
MAE train: 0.517484	val: 0.645215	test: 0.552738

Epoch: 31
Loss: 0.5062630304268428
RMSE train: 0.690997	val: 0.862403	test: 0.745913
MAE train: 0.538584	val: 0.654059	test: 0.567339

Epoch: 32
Loss: 0.507012277841568
RMSE train: 0.671035	val: 0.830030	test: 0.723012
MAE train: 0.526408	val: 0.638247	test: 0.547204

Epoch: 33
Loss: 0.5095118709972927
RMSE train: 0.666403	val: 0.838350	test: 0.731114
MAE train: 0.518655	val: 0.643631	test: 0.556100

Epoch: 34
Loss: 0.4811329756464277
RMSE train: 0.670765	val: 0.835160	test: 0.747413
MAE train: 0.521030	val: 0.640119	test: 0.564577

Epoch: 35
Loss: 0.5277116362537656
RMSE train: 0.639004	val: 0.825543	test: 0.711619
MAE train: 0.495745	val: 0.629161	test: 0.542947

Epoch: 36
Loss: 0.4944478188242231
RMSE train: 0.652905	val: 0.837720	test: 0.739553
MAE train: 0.507587	val: 0.643564	test: 0.556114

Epoch: 37
Loss: 0.5008066445589066
RMSE train: 0.642691	val: 0.806833	test: 0.731284
MAE train: 0.501002	val: 0.611135	test: 0.555984

Epoch: 38
Loss: 0.4707030143056597
RMSE train: 0.627560	val: 0.823636	test: 0.731037
MAE train: 0.488983	val: 0.622374	test: 0.547639

Epoch: 39
Loss: 0.48087755697114126
RMSE train: 0.622026	val: 0.807402	test: 0.705228
MAE train: 0.484065	val: 0.609128	test: 0.531107

Epoch: 40
Loss: 0.45885543738092693
RMSE train: 0.625854	val: 0.819138	test: 0.705769
MAE train: 0.487456	val: 0.612290	test: 0.530607

Epoch: 41
Loss: 0.45066164433956146
RMSE train: 0.619241	val: 0.799277	test: 0.698926
MAE train: 0.483296	val: 0.599081	test: 0.529319

Epoch: 42
Loss: 0.46954514937741415
RMSE train: 0.625689	val: 0.802003	test: 0.710058
MAE train: 0.488235	val: 0.613952	test: 0.538703

Epoch: 43
Loss: 0.48206723587853567
RMSE train: 0.653317	val: 0.836453	test: 0.733629
MAE train: 0.509400	val: 0.631567	test: 0.556285

Epoch: 44
Loss: 0.47934929387910025
RMSE train: 0.623840	val: 0.823431	test: 0.706813
MAE train: 0.488428	val: 0.620008	test: 0.524174

Epoch: 45
Loss: 0.4533503438745226
RMSE train: 0.618930	val: 0.801278	test: 0.721199
MAE train: 0.480000	val: 0.609556	test: 0.542786

Epoch: 46
Loss: 0.4570573227746146
RMSE train: 0.613126	val: 0.806233	test: 0.702582
MAE train: 0.477591	val: 0.611832	test: 0.529829

Epoch: 47
Loss: 0.4443138986825943
RMSE train: 0.622046	val: 0.818359	test: 0.718895
MAE train: 0.481117	val: 0.618498	test: 0.542218

Epoch: 48
Loss: 0.4312741053955896
RMSE train: 0.626289	val: 0.813673	test: 0.723164
MAE train: 0.486312	val: 0.608598	test: 0.533943

Epoch: 49
Loss: 0.44305865679468426
RMSE train: 0.621209	val: 0.807743	test: 0.720061
MAE train: 0.482086	val: 0.614222	test: 0.539649

Epoch: 50
Loss: 0.4162936019045966
RMSE train: 0.592542	val: 0.801742	test: 0.702597
MAE train: 0.462778	val: 0.599455	test: 0.532458

Epoch: 51
Loss: 0.4214438519307545
RMSE train: 0.590724	val: 0.809763	test: 0.697020
MAE train: 0.459175	val: 0.601145	test: 0.520654

Epoch: 52
Loss: 0.43146049763475147
RMSE train: 0.612166	val: 0.812554	test: 0.737778
MAE train: 0.474813	val: 0.606170	test: 0.544776

Epoch: 53
Loss: 0.4187102679695402
RMSE train: 0.621864	val: 0.826326	test: 0.735618
MAE train: 0.482213	val: 0.619507	test: 0.550278

Epoch: 54
Loss: 0.4068647814648492
RMSE train: 0.573206	val: 0.799972	test: 0.687805
MAE train: 0.448545	val: 0.589603	test: 0.515761

Epoch: 55
Loss: 0.4249299743345806
RMSE train: 0.593390	val: 0.795796	test: 0.719733
MAE train: 0.462690	val: 0.600673	test: 0.534803

Epoch: 56
Loss: 0.421724796295166
RMSE train: 0.589632	val: 0.797860	test: 0.702770
MAE train: 0.462054	val: 0.591871	test: 0.524617

Epoch: 57
Loss: 0.4028710722923279
RMSE train: 0.586763	val: 0.784786	test: 0.695719
MAE train: 0.457743	val: 0.586077	test: 0.526351

Epoch: 58
Loss: 0.41610307140009745
RMSE train: 0.593298	val: 0.783329	test: 0.703902
MAE train: 0.461030	val: 0.588774	test: 0.526362

Epoch: 59
Loss: 0.40678210982254576
RMSE train: 0.576746	val: 0.783637	test: 0.689074
MAE train: 0.447910	val: 0.583652	test: 0.511423

Epoch: 60
Loss: 0.39618956404072897
RMSE train: 0.571850	val: 0.787270	test: 0.697131
MAE train: 0.443466	val: 0.583157	test: 0.526215

Epoch: 61
Loss: 0.39817890524864197
RMSE train: 0.585593	val: 0.799015	test: 0.717160
MAE train: 0.455545	val: 0.593991	test: 0.530825

Epoch: 62
Loss: 0.38606731593608856
RMSE train: 0.555896	val: 0.754079	test: 0.672958
MAE train: 0.429177	val: 0.561972	test: 0.504787

Epoch: 63
Loss: 0.3921799510717392
RMSE train: 0.590065	val: 0.783720	test: 0.716827
MAE train: 0.457169	val: 0.588849	test: 0.533211

Epoch: 64
Loss: 0.3810171016624996
RMSE train: 0.557879	val: 0.772471	test: 0.686479
MAE train: 0.433156	val: 0.562682	test: 0.509549

Epoch: 65
Loss: 0.3745830144201006
RMSE train: 0.558316	val: 0.771420	test: 0.705635
MAE train: 0.434115	val: 0.570267	test: 0.522872

Epoch: 66
Loss: 0.3696365697043283
RMSE train: 0.552232	val: 0.754502	test: 0.682869
MAE train: 0.429325	val: 0.561432	test: 0.511044

Epoch: 67
Loss: 0.36982213599341257
RMSE train: 0.554062	val: 0.765511	test: 0.693684
MAE train: 0.433179	val: 0.571846	test: 0.525839

Epoch: 68
Loss: 0.3942367455789021
RMSE train: 0.557731	val: 0.770919	test: 0.694876
MAE train: 0.438707	val: 0.580929	test: 0.528434

Epoch: 69
Loss: 0.3872953121151243
RMSE train: 0.574761	val: 0.797668	test: 0.721819
MAE train: 0.448402	val: 0.600069	test: 0.549100

Epoch: 70
Loss: 0.3566591292619705
RMSE train: 0.539562	val: 0.765175	test: 0.689606
MAE train: 0.420011	val: 0.564551	test: 0.511267

Epoch: 71
Loss: 0.38349286147526335
RMSE train: 0.596804	val: 0.804470	test: 0.737263
MAE train: 0.465568	val: 0.598484	test: 0.555648

Epoch: 72
Loss: 0.3786221316882542
RMSE train: 0.578302	val: 0.809376	test: 0.699907
MAE train: 0.450210	val: 0.598072	test: 0.535270

Epoch: 73
Loss: 0.37596401146480013
RMSE train: 0.605287	val: 0.807510	test: 0.754387
MAE train: 0.470912	val: 0.600837	test: 0.560504

Epoch: 74
Loss: 0.38236949486391886
RMSE train: 0.554138	val: 0.783936	test: 0.685831
MAE train: 0.430033	val: 0.577066	test: 0.519937

Epoch: 75
Loss: 0.35857753668512615
RMSE train: 0.551813	val: 0.766736	test: 0.696727
MAE train: 0.430384	val: 0.569986	test: 0.514731

Epoch: 76
Loss: 0.3629309982061386
RMSE train: 0.570789	val: 0.777126	test: 0.705897
MAE train: 0.444729	val: 0.577545	test: 0.526988

Epoch: 77
Loss: 0.35806416613715036
RMSE train: 0.542859	val: 0.770966	test: 0.676352
MAE train: 0.420108	val: 0.565861	test: 0.498184

Epoch: 78
Loss: 0.35722564586571287
RMSE train: 0.561107	val: 0.776545	test: 0.679019
MAE train: 0.437990	val: 0.580661	test: 0.510531

Epoch: 79
Loss: 0.3628911077976227
RMSE train: 0.537926	val: 0.765862	test: 0.670071
MAE train: 0.416662	val: 0.566952	test: 0.499641

Epoch: 80
Loss: 0.34607734211853575
RMSE train: 0.532111	val: 0.761004	test: 0.690075
MAE train: 0.412132	val: 0.563648	test: 0.510503

Epoch: 81
Loss: 0.3258280796664102
RMSE train: 0.531879	val: 0.767951	test: 0.686142
MAE train: 0.411253	val: 0.562360	test: 0.504603

Epoch: 82
Loss: 0.3327112985508783
RMSE train: 0.534361	val: 0.775264	test: 0.696236
MAE train: 0.410550	val: 0.568078	test: 0.516630

Epoch: 83
Loss: 0.33707294932433535
RMSE train: 0.537729	val: 0.779436	test: 0.685204
MAE train: 0.390187	val: 0.520195	test: 0.528168

Epoch: 84
Loss: 0.31967158019542696
RMSE train: 0.485652	val: 0.698478	test: 0.700484
MAE train: 0.383212	val: 0.510267	test: 0.525514

Epoch: 85
Loss: 0.31410559415817263
RMSE train: 0.495149	val: 0.718044	test: 0.710381
MAE train: 0.389601	val: 0.524218	test: 0.535103

Epoch: 86
Loss: 0.31727634072303773
RMSE train: 0.515672	val: 0.712088	test: 0.706920
MAE train: 0.405025	val: 0.528261	test: 0.539620

Epoch: 87
Loss: 0.3014300107955933
RMSE train: 0.476286	val: 0.692974	test: 0.685751
MAE train: 0.373280	val: 0.502143	test: 0.516293

Epoch: 88
Loss: 0.3053040415048599
RMSE train: 0.481717	val: 0.680492	test: 0.694716
MAE train: 0.377432	val: 0.499707	test: 0.521255

Epoch: 89
Loss: 0.30561503767967224
RMSE train: 0.485258	val: 0.702684	test: 0.701918
MAE train: 0.381056	val: 0.515389	test: 0.523284

Epoch: 90
Loss: 0.3172642678022385
RMSE train: 0.483179	val: 0.709368	test: 0.706962
MAE train: 0.378871	val: 0.516908	test: 0.531838

Epoch: 91
Loss: 0.2876472532749176
RMSE train: 0.488976	val: 0.714576	test: 0.709547
MAE train: 0.387863	val: 0.529083	test: 0.537177

Epoch: 92
Loss: 0.3026472270488739
RMSE train: 0.473055	val: 0.697713	test: 0.698780
MAE train: 0.373442	val: 0.514207	test: 0.522448

Epoch: 93
Loss: 0.286784927546978
RMSE train: 0.477650	val: 0.712920	test: 0.700468
MAE train: 0.377906	val: 0.521625	test: 0.531320

Epoch: 94
Loss: 0.296970534324646
RMSE train: 0.508059	val: 0.717040	test: 0.724540
MAE train: 0.401131	val: 0.531868	test: 0.551076

Epoch: 95
Loss: 0.2927068531513214
RMSE train: 0.488063	val: 0.714263	test: 0.702773
MAE train: 0.384523	val: 0.524225	test: 0.535563

Epoch: 96
Loss: 0.2718590036034584
RMSE train: 0.460272	val: 0.690269	test: 0.698140
MAE train: 0.362611	val: 0.510022	test: 0.530490

Epoch: 97
Loss: 0.29155494272708893
RMSE train: 0.495206	val: 0.722105	test: 0.716031
MAE train: 0.387314	val: 0.527155	test: 0.544571

Epoch: 98
Loss: 0.2828541874885559
RMSE train: 0.462298	val: 0.695399	test: 0.687646
MAE train: 0.365355	val: 0.513560	test: 0.519320

Epoch: 99
Loss: 0.290179418027401
RMSE train: 0.449532	val: 0.686064	test: 0.682733
MAE train: 0.354287	val: 0.504473	test: 0.510214

Epoch: 100
Loss: 0.2876478582620621
RMSE train: 0.478871	val: 0.698469	test: 0.693997
MAE train: 0.376786	val: 0.516272	test: 0.522579

Epoch: 101
Loss: 0.27994583547115326
RMSE train: 0.457843	val: 0.693351	test: 0.685074
MAE train: 0.360904	val: 0.507925	test: 0.516036

Epoch: 102
Loss: 0.29273730218410493
RMSE train: 0.460790	val: 0.691735	test: 0.694907
MAE train: 0.363903	val: 0.515859	test: 0.526372

Epoch: 103
Loss: 0.26940152496099473
RMSE train: 0.480634	val: 0.708316	test: 0.708393
MAE train: 0.380196	val: 0.528752	test: 0.536788

Epoch: 104
Loss: 0.27641547918319703
RMSE train: 0.465413	val: 0.701787	test: 0.699857
MAE train: 0.368712	val: 0.509474	test: 0.521679

Epoch: 105
Loss: 0.27896286100149154
RMSE train: 0.471670	val: 0.691188	test: 0.700511
MAE train: 0.371125	val: 0.510614	test: 0.534916

Epoch: 106
Loss: 0.2637007415294647
RMSE train: 0.452824	val: 0.688158	test: 0.683422
MAE train: 0.358303	val: 0.501126	test: 0.512392

Epoch: 107
Loss: 0.267975215613842
RMSE train: 0.453804	val: 0.689910	test: 0.692457
MAE train: 0.359206	val: 0.506076	test: 0.517209

Epoch: 108
Loss: 0.25858963578939437
RMSE train: 0.447065	val: 0.694849	test: 0.684615
MAE train: 0.354088	val: 0.509599	test: 0.513565

Epoch: 109
Loss: 0.26276352256536484
RMSE train: 0.459383	val: 0.709216	test: 0.700527
MAE train: 0.361110	val: 0.521240	test: 0.527724

Epoch: 110
Loss: 0.2643547341227531
RMSE train: 0.449488	val: 0.689978	test: 0.688109
MAE train: 0.355621	val: 0.501558	test: 0.518378

Epoch: 111
Loss: 0.26392434239387513
RMSE train: 0.442447	val: 0.680435	test: 0.684339
MAE train: 0.348306	val: 0.502054	test: 0.519330

Epoch: 112
Loss: 0.2629048481583595
RMSE train: 0.500740	val: 0.723613	test: 0.719505
MAE train: 0.395124	val: 0.537059	test: 0.546795

Epoch: 113
Loss: 0.26321425586938857
RMSE train: 0.445136	val: 0.687813	test: 0.694403
MAE train: 0.351442	val: 0.500780	test: 0.516125

Epoch: 114
Loss: 0.2631065472960472
RMSE train: 0.447029	val: 0.691658	test: 0.692102
MAE train: 0.351507	val: 0.506474	test: 0.520215

Epoch: 115
Loss: 0.26097477227449417
RMSE train: 0.460406	val: 0.709411	test: 0.701943
MAE train: 0.360347	val: 0.517422	test: 0.531651

Epoch: 116
Loss: 0.2580643057823181
RMSE train: 0.446185	val: 0.681172	test: 0.685897
MAE train: 0.352613	val: 0.503854	test: 0.515055

Epoch: 117
Loss: 0.26240764260292054
RMSE train: 0.435506	val: 0.686243	test: 0.707540
MAE train: 0.342207	val: 0.497746	test: 0.525951

Epoch: 118
Loss: 0.25005638748407366
RMSE train: 0.450899	val: 0.691429	test: 0.693741
MAE train: 0.355984	val: 0.509687	test: 0.523843

Epoch: 119
Loss: 0.26235167384147645
RMSE train: 0.457937	val: 0.695716	test: 0.704326
MAE train: 0.359568	val: 0.510019	test: 0.532229

Epoch: 120
Loss: 0.252686420083046
RMSE train: 0.445354	val: 0.681850	test: 0.696338
MAE train: 0.350740	val: 0.498488	test: 0.524041

Epoch: 121
Loss: 0.2553673475980759
RMSE train: 0.451871	val: 0.684706	test: 0.697577
MAE train: 0.355876	val: 0.504252	test: 0.528659

Epoch: 122
Loss: 0.2526825338602066
RMSE train: 0.488213	val: 0.720383	test: 0.720402
MAE train: 0.383713	val: 0.533284	test: 0.548422

Epoch: 123
Loss: 0.2508839279413223
RMSE train: 0.428446	val: 0.686041	test: 0.692053
MAE train: 0.335633	val: 0.507090	test: 0.520697

Epoch: 124
Loss: 0.251743021607399
RMSE train: 0.437621	val: 0.693470	test: 0.701064
MAE train: 0.344574	val: 0.501532	test: 0.521820

Epoch: 125
Loss: 0.24262063652276994
RMSE train: 0.421640	val: 0.678994	test: 0.687205
MAE train: 0.330895	val: 0.493557	test: 0.511770

Epoch: 126
Loss: 0.23874011784791946
RMSE train: 0.447910	val: 0.694649	test: 0.695169
MAE train: 0.352529	val: 0.506037	test: 0.522583

Epoch: 127
Loss: 0.24124013185501098
RMSE train: 0.441661	val: 0.693977	test: 0.705504
MAE train: 0.345868	val: 0.505718	test: 0.528427

Epoch: 128
Loss: 0.23951784521341324
RMSE train: 0.437681	val: 0.690376	test: 0.700774
MAE train: 0.344865	val: 0.504808	test: 0.528318

Epoch: 129
Loss: 0.23926334381103515
RMSE train: 0.433013	val: 0.691463	test: 0.687779
MAE train: 0.341419	val: 0.501419	test: 0.515365

Epoch: 130
Loss: 0.23878653049468995
RMSE train: 0.422084	val: 0.685769	test: 0.682520
MAE train: 0.331774	val: 0.491196	test: 0.507899

Epoch: 131
Loss: 0.23203697949647903
RMSE train: 0.447045	val: 0.693395	test: 0.694041
MAE train: 0.352064	val: 0.508125	test: 0.525676

Epoch: 132
Loss: 0.23297253698110582
RMSE train: 0.419641	val: 0.687806	test: 0.687748
MAE train: 0.328309	val: 0.497504	test: 0.515906

Epoch: 133
Loss: 0.24326211810112
RMSE train: 0.423564	val: 0.683902	test: 0.682561
MAE train: 0.330954	val: 0.491509	test: 0.506771

Epoch: 134
Loss: 0.24034370183944703
RMSE train: 0.464060	val: 0.706837	test: 0.703818
MAE train: 0.366697	val: 0.517236	test: 0.540308

Epoch: 135
Loss: 0.2274918258190155
RMSE train: 0.429862	val: 0.693723	test: 0.691215
MAE train: 0.336253	val: 0.505779	test: 0.521849

Epoch: 136
Loss: 0.23325820565223693
RMSE train: 0.423863	val: 0.679164	test: 0.677092
MAE train: 0.333883	val: 0.492811	test: 0.508560

Epoch: 137
Loss: 0.231320583820343
RMSE train: 0.438544	val: 0.707785	test: 0.700654
MAE train: 0.344203	val: 0.513267	test: 0.535845

Epoch: 138
Loss: 0.24095218628644943
RMSE train: 0.422743	val: 0.686667	test: 0.683157
MAE train: 0.332787	val: 0.494889	test: 0.517260

Epoch: 139
Loss: 0.22911833822727204
RMSE train: 0.466518	val: 0.711106	test: 0.707178
MAE train: 0.366048	val: 0.521582	test: 0.543169

Epoch: 140
Loss: 0.22950949817895888
RMSE train: 0.419564	val: 0.693550	test: 0.688206
MAE train: 0.328745	val: 0.502382	test: 0.519200

Epoch: 141
Loss: 0.21853727847337723
RMSE train: 0.427952	val: 0.685095	test: 0.689144
MAE train: 0.335731	val: 0.501905	test: 0.521817

Epoch: 142
Loss: 0.23006685972213745
RMSE train: 0.440546	val: 0.708089	test: 0.690700
MAE train: 0.347351	val: 0.514255	test: 0.521598

Epoch: 143
Loss: 0.224239082634449
RMSE train: 0.446034	val: 0.704188	test: 0.697081
MAE train: 0.351262	val: 0.514791	test: 0.528420
MAE train: 0.430934	val: 0.532050	test: 0.561392

Epoch: 84
Loss: 0.3227304697036743
RMSE train: 0.496639	val: 0.689906	test: 0.702466
MAE train: 0.389966	val: 0.502838	test: 0.532014

Epoch: 85
Loss: 0.31324794590473176
RMSE train: 0.524061	val: 0.705256	test: 0.718382
MAE train: 0.412316	val: 0.522487	test: 0.545257

Epoch: 86
Loss: 0.321989306807518
RMSE train: 0.503939	val: 0.700950	test: 0.721502
MAE train: 0.395403	val: 0.520080	test: 0.549976

Epoch: 87
Loss: 0.3170830726623535
RMSE train: 0.490604	val: 0.689185	test: 0.706618
MAE train: 0.386380	val: 0.508366	test: 0.534283

Epoch: 88
Loss: 0.30957870483398436
RMSE train: 0.501799	val: 0.703264	test: 0.709753
MAE train: 0.396736	val: 0.522845	test: 0.541199

Epoch: 89
Loss: 0.3076673328876495
RMSE train: 0.511021	val: 0.697114	test: 0.725420
MAE train: 0.398278	val: 0.516359	test: 0.542910

Epoch: 90
Loss: 0.3035507082939148
RMSE train: 0.520364	val: 0.714885	test: 0.732621
MAE train: 0.406822	val: 0.525794	test: 0.548639

Epoch: 91
Loss: 0.29965422451496126
RMSE train: 0.498876	val: 0.700314	test: 0.720324
MAE train: 0.390599	val: 0.513754	test: 0.541170

Epoch: 92
Loss: 0.3101058483123779
RMSE train: 0.489429	val: 0.685235	test: 0.718473
MAE train: 0.385291	val: 0.505322	test: 0.538571

Epoch: 93
Loss: 0.29244006276130674
RMSE train: 0.486391	val: 0.698321	test: 0.711146
MAE train: 0.381848	val: 0.505504	test: 0.532735

Epoch: 94
Loss: 0.28870721310377123
RMSE train: 0.506147	val: 0.695278	test: 0.723296
MAE train: 0.398119	val: 0.515099	test: 0.543059

Epoch: 95
Loss: 0.289351561665535
RMSE train: 0.476406	val: 0.695995	test: 0.704499
MAE train: 0.372663	val: 0.508528	test: 0.527864

Epoch: 96
Loss: 0.3039390414953232
RMSE train: 0.501940	val: 0.701028	test: 0.725773
MAE train: 0.391198	val: 0.516897	test: 0.545232

Epoch: 97
Loss: 0.28784162253141404
RMSE train: 0.470320	val: 0.682046	test: 0.694272
MAE train: 0.368685	val: 0.496503	test: 0.520269

Epoch: 98
Loss: 0.29946279227733613
RMSE train: 0.500754	val: 0.697689	test: 0.719412
MAE train: 0.392724	val: 0.511557	test: 0.540445

Epoch: 99
Loss: 0.2881624922156334
RMSE train: 0.482717	val: 0.685565	test: 0.722537
MAE train: 0.376827	val: 0.499504	test: 0.538084

Epoch: 100
Loss: 0.2908562749624252
RMSE train: 0.479227	val: 0.688775	test: 0.710216
MAE train: 0.373927	val: 0.505017	test: 0.529889

Epoch: 101
Loss: 0.2853820264339447
RMSE train: 0.475601	val: 0.687086	test: 0.713745
MAE train: 0.370680	val: 0.502952	test: 0.535717

Epoch: 102
Loss: 0.2895726874470711
RMSE train: 0.469655	val: 0.689674	test: 0.709844
MAE train: 0.370415	val: 0.512126	test: 0.534592

Epoch: 103
Loss: 0.2910595923662186
RMSE train: 0.500448	val: 0.706242	test: 0.721704
MAE train: 0.390756	val: 0.518748	test: 0.541635

Epoch: 104
Loss: 0.283862279355526
RMSE train: 0.459366	val: 0.675491	test: 0.693779
MAE train: 0.358621	val: 0.492141	test: 0.519572

Epoch: 105
Loss: 0.2804593965411186
RMSE train: 0.467621	val: 0.677827	test: 0.694966
MAE train: 0.366293	val: 0.495615	test: 0.524494

Epoch: 106
Loss: 0.26701987236738206
RMSE train: 0.480474	val: 0.685915	test: 0.711538
MAE train: 0.373118	val: 0.499346	test: 0.539647

Epoch: 107
Loss: 0.27466240972280503
RMSE train: 0.466820	val: 0.691646	test: 0.698434
MAE train: 0.367157	val: 0.506643	test: 0.529446

Epoch: 108
Loss: 0.2893258973956108
RMSE train: 0.503104	val: 0.695300	test: 0.724821
MAE train: 0.394941	val: 0.510933	test: 0.544486

Epoch: 109
Loss: 0.2669079676270485
RMSE train: 0.491118	val: 0.691917	test: 0.718701
MAE train: 0.384514	val: 0.508192	test: 0.541530

Epoch: 110
Loss: 0.2668618828058243
RMSE train: 0.486922	val: 0.692315	test: 0.704854
MAE train: 0.379652	val: 0.504535	test: 0.533116

Epoch: 111
Loss: 0.27066372632980346
RMSE train: 0.466480	val: 0.683568	test: 0.710106
MAE train: 0.365455	val: 0.498922	test: 0.535784

Epoch: 112
Loss: 0.2744706615805626
RMSE train: 0.475683	val: 0.693734	test: 0.706492
MAE train: 0.374902	val: 0.507922	test: 0.529836

Epoch: 113
Loss: 0.2525968536734581
RMSE train: 0.456230	val: 0.688943	test: 0.699510
MAE train: 0.357599	val: 0.503158	test: 0.527566

Epoch: 114
Loss: 0.25986623466014863
RMSE train: 0.449303	val: 0.682414	test: 0.686058
MAE train: 0.350240	val: 0.501783	test: 0.519559

Epoch: 115
Loss: 0.25647207349538803
RMSE train: 0.459894	val: 0.686876	test: 0.700268
MAE train: 0.360137	val: 0.503667	test: 0.525477

Epoch: 116
Loss: 0.2734744131565094
RMSE train: 0.460462	val: 0.678376	test: 0.700927
MAE train: 0.359820	val: 0.497429	test: 0.530238

Epoch: 117
Loss: 0.2536912143230438
RMSE train: 0.448852	val: 0.678881	test: 0.697220
MAE train: 0.349201	val: 0.494027	test: 0.525284

Epoch: 118
Loss: 0.25576018542051315
RMSE train: 0.458234	val: 0.685999	test: 0.695506
MAE train: 0.358914	val: 0.503692	test: 0.528364

Epoch: 119
Loss: 0.2624961793422699
RMSE train: 0.481976	val: 0.683394	test: 0.710288
MAE train: 0.377504	val: 0.504971	test: 0.540559

Epoch: 120
Loss: 0.25573932826519014
RMSE train: 0.465664	val: 0.679491	test: 0.698381
MAE train: 0.363644	val: 0.496097	test: 0.528495

Epoch: 121
Loss: 0.25158375352621076
RMSE train: 0.461747	val: 0.687044	test: 0.710949
MAE train: 0.360948	val: 0.499965	test: 0.535464

Epoch: 122
Loss: 0.2579068496823311
RMSE train: 0.475204	val: 0.681194	test: 0.712306
MAE train: 0.369865	val: 0.499832	test: 0.536399

Epoch: 123
Loss: 0.2378174126148224
RMSE train: 0.452114	val: 0.685117	test: 0.707625
MAE train: 0.351855	val: 0.498756	test: 0.532932

Epoch: 124
Loss: 0.24915378391742707
RMSE train: 0.480203	val: 0.699645	test: 0.720277
MAE train: 0.373470	val: 0.515537	test: 0.543710

Epoch: 125
Loss: 0.2514976397156715
RMSE train: 0.460388	val: 0.679838	test: 0.692176
MAE train: 0.361337	val: 0.498087	test: 0.524792

Epoch: 126
Loss: 0.2521159738302231
RMSE train: 0.435285	val: 0.674871	test: 0.687434
MAE train: 0.338822	val: 0.495229	test: 0.518111

Epoch: 127
Loss: 0.24418229907751082
RMSE train: 0.459662	val: 0.693139	test: 0.701723
MAE train: 0.360469	val: 0.505685	test: 0.529111

Epoch: 128
Loss: 0.24800105392932892
RMSE train: 0.466103	val: 0.689168	test: 0.701379
MAE train: 0.363414	val: 0.501227	test: 0.527997

Epoch: 129
Loss: 0.2340680703520775
RMSE train: 0.442129	val: 0.674168	test: 0.678418
MAE train: 0.346353	val: 0.494462	test: 0.511794

Epoch: 130
Loss: 0.2456767037510872
RMSE train: 0.442562	val: 0.682914	test: 0.694119
MAE train: 0.347856	val: 0.503129	test: 0.520004

Epoch: 131
Loss: 0.2376147761940956
RMSE train: 0.437445	val: 0.675206	test: 0.680558
MAE train: 0.342508	val: 0.490407	test: 0.514441

Epoch: 132
Loss: 0.23401658684015275
RMSE train: 0.443160	val: 0.684500	test: 0.694165
MAE train: 0.348070	val: 0.499506	test: 0.523486

Epoch: 133
Loss: 0.2280656173825264
RMSE train: 0.437616	val: 0.683767	test: 0.698341
MAE train: 0.342435	val: 0.504064	test: 0.526897

Epoch: 134
Loss: 0.2321503698825836
RMSE train: 0.459592	val: 0.693148	test: 0.710160
MAE train: 0.359419	val: 0.504653	test: 0.529401

Epoch: 135
Loss: 0.23548634499311447
RMSE train: 0.440196	val: 0.684525	test: 0.688923
MAE train: 0.345739	val: 0.495144	test: 0.515586

Epoch: 136
Loss: 0.23237185627222062
RMSE train: 0.453568	val: 0.691455	test: 0.714201
MAE train: 0.354374	val: 0.502941	test: 0.531863

Epoch: 137
Loss: 0.2374296948313713
RMSE train: 0.459307	val: 0.690679	test: 0.697755
MAE train: 0.357392	val: 0.501725	test: 0.527606

Epoch: 138
Loss: 0.23490869998931885
RMSE train: 0.455494	val: 0.692781	test: 0.697939
MAE train: 0.356731	val: 0.502884	test: 0.526079

Epoch: 139
Loss: 0.24155227541923524
RMSE train: 0.432989	val: 0.681503	test: 0.683548
MAE train: 0.338058	val: 0.497926	test: 0.512406

Epoch: 140
Loss: 0.22191811352968216
RMSE train: 0.427418	val: 0.675985	test: 0.678628
MAE train: 0.334288	val: 0.496971	test: 0.516981

Epoch: 141
Loss: 0.22874021679162979
RMSE train: 0.441899	val: 0.679708	test: 0.689233
MAE train: 0.344287	val: 0.497273	test: 0.519855

Epoch: 142
Loss: 0.2341650515794754
RMSE train: 0.446447	val: 0.686380	test: 0.690295
MAE train: 0.349548	val: 0.500175	test: 0.519799

Epoch: 143
Loss: 0.23431204557418822
RMSE train: 0.424427	val: 0.683884	test: 0.683753
MAE train: 0.333854	val: 0.497579	test: 0.520245
MAE train: 0.409186	val: 0.518917	test: 0.539115

Epoch: 84
Loss: 0.3264423072338104
RMSE train: 0.502551	val: 0.695475	test: 0.720687
MAE train: 0.393985	val: 0.515189	test: 0.538302

Epoch: 85
Loss: 0.3129772335290909
RMSE train: 0.510633	val: 0.711568	test: 0.724064
MAE train: 0.401974	val: 0.524311	test: 0.538823

Epoch: 86
Loss: 0.29920199066400527
RMSE train: 0.512567	val: 0.687299	test: 0.704789
MAE train: 0.405433	val: 0.515859	test: 0.533488

Epoch: 87
Loss: 0.31314208805561067
RMSE train: 0.520237	val: 0.689230	test: 0.708610
MAE train: 0.409176	val: 0.518726	test: 0.538290

Epoch: 88
Loss: 0.30416593253612517
RMSE train: 0.527090	val: 0.703194	test: 0.730564
MAE train: 0.413548	val: 0.521422	test: 0.546992

Epoch: 89
Loss: 0.30478566586971284
RMSE train: 0.516059	val: 0.696135	test: 0.727950
MAE train: 0.402169	val: 0.517968	test: 0.550793

Epoch: 90
Loss: 0.31316635608673093
RMSE train: 0.513519	val: 0.683725	test: 0.711335
MAE train: 0.403682	val: 0.509163	test: 0.536350

Epoch: 91
Loss: 0.29665824174880984
RMSE train: 0.514638	val: 0.696056	test: 0.714019
MAE train: 0.404415	val: 0.516489	test: 0.534524

Epoch: 92
Loss: 0.2942074179649353
RMSE train: 0.485898	val: 0.690116	test: 0.708090
MAE train: 0.379903	val: 0.506089	test: 0.531239

Epoch: 93
Loss: 0.29861877858638763
RMSE train: 0.492512	val: 0.683529	test: 0.707066
MAE train: 0.386563	val: 0.506830	test: 0.530927

Epoch: 94
Loss: 0.29984558969736097
RMSE train: 0.500468	val: 0.691870	test: 0.715297
MAE train: 0.390829	val: 0.508302	test: 0.532700

Epoch: 95
Loss: 0.2991115152835846
RMSE train: 0.491715	val: 0.692551	test: 0.705410
MAE train: 0.385134	val: 0.507338	test: 0.529782

Epoch: 96
Loss: 0.29723250269889834
RMSE train: 0.493773	val: 0.697383	test: 0.707199
MAE train: 0.385464	val: 0.510613	test: 0.533544

Epoch: 97
Loss: 0.29618775844573975
RMSE train: 0.483313	val: 0.688630	test: 0.695343
MAE train: 0.379173	val: 0.506160	test: 0.529143

Epoch: 98
Loss: 0.29855637699365617
RMSE train: 0.481734	val: 0.681710	test: 0.701817
MAE train: 0.379864	val: 0.499345	test: 0.524571

Epoch: 99
Loss: 0.2910731196403503
RMSE train: 0.471664	val: 0.685547	test: 0.701166
MAE train: 0.369640	val: 0.499523	test: 0.520064

Epoch: 100
Loss: 0.2823032736778259
RMSE train: 0.481740	val: 0.689611	test: 0.720926
MAE train: 0.376068	val: 0.504333	test: 0.532186

Epoch: 101
Loss: 0.2918476790189743
RMSE train: 0.499711	val: 0.689192	test: 0.716468
MAE train: 0.391254	val: 0.505397	test: 0.534586

Epoch: 102
Loss: 0.2704123169183731
RMSE train: 0.478386	val: 0.694432	test: 0.719257
MAE train: 0.376675	val: 0.504001	test: 0.533005

Epoch: 103
Loss: 0.28141732066869735
RMSE train: 0.466750	val: 0.681796	test: 0.706892
MAE train: 0.365062	val: 0.498423	test: 0.525990

Epoch: 104
Loss: 0.2905184507369995
RMSE train: 0.500875	val: 0.702593	test: 0.720538
MAE train: 0.394561	val: 0.516383	test: 0.545350

Epoch: 105
Loss: 0.27413819283246993
RMSE train: 0.496921	val: 0.693652	test: 0.689616
MAE train: 0.391798	val: 0.512146	test: 0.519121

Epoch: 106
Loss: 0.2924642086029053
RMSE train: 0.471685	val: 0.687735	test: 0.696110
MAE train: 0.371677	val: 0.502970	test: 0.525732

Epoch: 107
Loss: 0.26510653346776963
RMSE train: 0.469224	val: 0.695369	test: 0.706283
MAE train: 0.369572	val: 0.507604	test: 0.532168

Epoch: 108
Loss: 0.26893165409564973
RMSE train: 0.474871	val: 0.686254	test: 0.696573
MAE train: 0.372205	val: 0.502551	test: 0.525336

Epoch: 109
Loss: 0.26935355365276337
RMSE train: 0.482455	val: 0.690907	test: 0.705517
MAE train: 0.377292	val: 0.505247	test: 0.531582

Epoch: 110
Loss: 0.27084091156721113
RMSE train: 0.473574	val: 0.694564	test: 0.693969
MAE train: 0.369538	val: 0.507571	test: 0.522788

Epoch: 111
Loss: 0.2611999899148941
RMSE train: 0.491510	val: 0.703894	test: 0.698806
MAE train: 0.384464	val: 0.515833	test: 0.530377

Epoch: 112
Loss: 0.280528125166893
RMSE train: 0.458897	val: 0.687326	test: 0.700367
MAE train: 0.359107	val: 0.507966	test: 0.525424

Epoch: 113
Loss: 0.2716898828744888
RMSE train: 0.469505	val: 0.698238	test: 0.704564
MAE train: 0.369317	val: 0.510309	test: 0.527985

Epoch: 114
Loss: 0.2542037695646286
RMSE train: 0.453298	val: 0.684002	test: 0.707183
MAE train: 0.353977	val: 0.497463	test: 0.528180

Epoch: 115
Loss: 0.25860222727060317
RMSE train: 0.474124	val: 0.692712	test: 0.706387
MAE train: 0.372889	val: 0.504862	test: 0.529229

Epoch: 116
Loss: 0.2543812543153763
RMSE train: 0.456463	val: 0.677075	test: 0.701052
MAE train: 0.356926	val: 0.498837	test: 0.531174

Epoch: 117
Loss: 0.256461401283741
RMSE train: 0.474294	val: 0.694345	test: 0.702810
MAE train: 0.371920	val: 0.510402	test: 0.529659

Epoch: 118
Loss: 0.25408277213573455
RMSE train: 0.460658	val: 0.686395	test: 0.693275
MAE train: 0.360292	val: 0.499428	test: 0.520846

Epoch: 119
Loss: 0.25064769983291624
RMSE train: 0.466359	val: 0.699420	test: 0.693490
MAE train: 0.366438	val: 0.505889	test: 0.524017

Epoch: 120
Loss: 0.2498587667942047
RMSE train: 0.456058	val: 0.682840	test: 0.697171
MAE train: 0.357541	val: 0.497827	test: 0.522603

Epoch: 121
Loss: 0.25041149407625196
RMSE train: 0.478597	val: 0.701694	test: 0.714586
MAE train: 0.375090	val: 0.515235	test: 0.537865

Epoch: 122
Loss: 0.24880173355340957
RMSE train: 0.439114	val: 0.684026	test: 0.680500
MAE train: 0.345262	val: 0.503458	test: 0.514365

Epoch: 123
Loss: 0.25005964636802674
RMSE train: 0.466710	val: 0.692313	test: 0.695046
MAE train: 0.366134	val: 0.508336	test: 0.527106

Epoch: 124
Loss: 0.2538710504770279
RMSE train: 0.471417	val: 0.681387	test: 0.694763
MAE train: 0.370734	val: 0.501886	test: 0.521711

Epoch: 125
Loss: 0.2580148845911026
RMSE train: 0.491994	val: 0.711461	test: 0.719796
MAE train: 0.385423	val: 0.520627	test: 0.544311

Epoch: 126
Loss: 0.25354265570640566
RMSE train: 0.446685	val: 0.682553	test: 0.691201
MAE train: 0.351740	val: 0.497981	test: 0.521447

Epoch: 127
Loss: 0.23756515979766846
RMSE train: 0.433809	val: 0.685445	test: 0.693102
MAE train: 0.339819	val: 0.496597	test: 0.520446

Epoch: 128
Loss: 0.24468374252319336
RMSE train: 0.492410	val: 0.706446	test: 0.713583
MAE train: 0.384158	val: 0.521176	test: 0.537046

Epoch: 129
Loss: 0.23502769619226455
RMSE train: 0.441841	val: 0.675513	test: 0.697174
MAE train: 0.345975	val: 0.494263	test: 0.520577

Epoch: 130
Loss: 0.23633107244968415
RMSE train: 0.455378	val: 0.689229	test: 0.701362
MAE train: 0.356439	val: 0.506234	test: 0.527060

Epoch: 131
Loss: 0.23594397008419038
RMSE train: 0.461676	val: 0.683235	test: 0.697114
MAE train: 0.360414	val: 0.502251	test: 0.527344

Epoch: 132
Loss: 0.23945851475000382
RMSE train: 0.448453	val: 0.679687	test: 0.691857
MAE train: 0.349810	val: 0.499468	test: 0.519217

Epoch: 133
Loss: 0.23679956793785095
RMSE train: 0.449436	val: 0.690389	test: 0.699972
MAE train: 0.352803	val: 0.505071	test: 0.522048

Epoch: 134
Loss: 0.23991647809743882
RMSE train: 0.450156	val: 0.686824	test: 0.701904
MAE train: 0.351536	val: 0.499970	test: 0.525119

Epoch: 135
Loss: 0.2345789134502411
RMSE train: 0.421755	val: 0.682159	test: 0.680914
MAE train: 0.330541	val: 0.490155	test: 0.504159

Epoch: 136
Loss: 0.22926740050315858
RMSE train: 0.442419	val: 0.679571	test: 0.692490
MAE train: 0.347468	val: 0.496082	test: 0.519704

Epoch: 137
Loss: 0.23043123185634612
RMSE train: 0.445520	val: 0.702174	test: 0.698483
MAE train: 0.349530	val: 0.506822	test: 0.522506

Epoch: 138
Loss: 0.23197565227746964
RMSE train: 0.435390	val: 0.682272	test: 0.688243
MAE train: 0.340354	val: 0.498072	test: 0.521835

Epoch: 139
Loss: 0.22831388860940932
RMSE train: 0.432087	val: 0.669575	test: 0.686869
MAE train: 0.339403	val: 0.492679	test: 0.519934

Epoch: 140
Loss: 0.23731818795204163
RMSE train: 0.448103	val: 0.684807	test: 0.687424
MAE train: 0.351440	val: 0.501600	test: 0.525152

Epoch: 141
Loss: 0.22776081562042236
RMSE train: 0.439391	val: 0.673117	test: 0.685534
MAE train: 0.343011	val: 0.494991	test: 0.523460

Epoch: 142
Loss: 0.24025990217924117
RMSE train: 0.446658	val: 0.683637	test: 0.684460
MAE train: 0.350052	val: 0.501262	test: 0.517268

Epoch: 143
Loss: 0.23550910055637359
RMSE train: 0.456555	val: 0.683007	test: 0.693218
MAE train: 0.359459	val: 0.502860	test: 0.526935
MAE train: 0.432980	val: 0.570449	test: 0.496424

Epoch: 84
Loss: 0.36311009739126476
RMSE train: 0.559699	val: 0.760856	test: 0.672217
MAE train: 0.436685	val: 0.572332	test: 0.504020

Epoch: 85
Loss: 0.368301585316658
RMSE train: 0.555360	val: 0.754184	test: 0.658481
MAE train: 0.436807	val: 0.581500	test: 0.510736

Epoch: 86
Loss: 0.3307794417653765
RMSE train: 0.537357	val: 0.749753	test: 0.641486
MAE train: 0.419787	val: 0.567260	test: 0.492036

Epoch: 87
Loss: 0.34041900507041384
RMSE train: 0.529353	val: 0.730265	test: 0.647233
MAE train: 0.412137	val: 0.551670	test: 0.486559

Epoch: 88
Loss: 0.34351767173835207
RMSE train: 0.520858	val: 0.757494	test: 0.645355
MAE train: 0.404405	val: 0.572741	test: 0.491714

Epoch: 89
Loss: 0.3390930997473853
RMSE train: 0.533478	val: 0.760306	test: 0.653174
MAE train: 0.416219	val: 0.568462	test: 0.497448

Epoch: 90
Loss: 0.33157328835555483
RMSE train: 0.538666	val: 0.764792	test: 0.665608
MAE train: 0.419848	val: 0.564068	test: 0.501879

Epoch: 91
Loss: 0.3242527203900473
RMSE train: 0.525574	val: 0.749735	test: 0.654279
MAE train: 0.408034	val: 0.555914	test: 0.495424

Epoch: 92
Loss: 0.32631729543209076
RMSE train: 0.519411	val: 0.748237	test: 0.648459
MAE train: 0.399048	val: 0.557858	test: 0.487138

Epoch: 93
Loss: 0.324250904577119
RMSE train: 0.532129	val: 0.754514	test: 0.683227
MAE train: 0.413574	val: 0.573235	test: 0.520843

Epoch: 94
Loss: 0.3279219014304025
RMSE train: 0.530807	val: 0.753709	test: 0.660491
MAE train: 0.412215	val: 0.565868	test: 0.495867

Epoch: 95
Loss: 0.32813907095364164
RMSE train: 0.526414	val: 0.763634	test: 0.658489
MAE train: 0.406079	val: 0.572198	test: 0.497864

Epoch: 96
Loss: 0.33400011488369535
RMSE train: 0.507943	val: 0.757266	test: 0.629034
MAE train: 0.397502	val: 0.561397	test: 0.476142

Epoch: 97
Loss: 0.32436724432877134
RMSE train: 0.521933	val: 0.757626	test: 0.639268
MAE train: 0.405588	val: 0.560218	test: 0.482966

Epoch: 98
Loss: 0.3028132362025125
RMSE train: 0.522723	val: 0.758603	test: 0.646122
MAE train: 0.407298	val: 0.567412	test: 0.486453

Epoch: 99
Loss: 0.3045688856925283
RMSE train: 0.498040	val: 0.753492	test: 0.631588
MAE train: 0.386964	val: 0.558928	test: 0.477599

Epoch: 100
Loss: 0.3126666673592159
RMSE train: 0.521046	val: 0.766518	test: 0.645433
MAE train: 0.409268	val: 0.581469	test: 0.493384

Epoch: 101
Loss: 0.3144532058920179
RMSE train: 0.496086	val: 0.755907	test: 0.633074
MAE train: 0.384579	val: 0.560283	test: 0.476787

Epoch: 102
Loss: 0.29864202652658733
RMSE train: 0.511899	val: 0.763487	test: 0.650472
MAE train: 0.396553	val: 0.565688	test: 0.489203

Epoch: 103
Loss: 0.30328139875616345
RMSE train: 0.513159	val: 0.764728	test: 0.656809
MAE train: 0.397439	val: 0.567852	test: 0.491410

Epoch: 104
Loss: 0.318832984992436
RMSE train: 0.511595	val: 0.762085	test: 0.629905
MAE train: 0.401203	val: 0.574137	test: 0.477237

Epoch: 105
Loss: 0.30584525423390524
RMSE train: 0.504253	val: 0.742592	test: 0.648590
MAE train: 0.389416	val: 0.554361	test: 0.484373

Epoch: 106
Loss: 0.3036741444042751
RMSE train: 0.500572	val: 0.742061	test: 0.634913
MAE train: 0.391726	val: 0.552661	test: 0.479638

Epoch: 107
Loss: 0.2903962273682867
RMSE train: 0.491183	val: 0.743056	test: 0.639116
MAE train: 0.383804	val: 0.559838	test: 0.488778

Epoch: 108
Loss: 0.32097395935228895
RMSE train: 0.504970	val: 0.757366	test: 0.643303
MAE train: 0.391889	val: 0.567355	test: 0.490873

Epoch: 109
Loss: 0.3337379353387015
RMSE train: 0.532706	val: 0.764289	test: 0.651351
MAE train: 0.414068	val: 0.575317	test: 0.497993

Epoch: 110
Loss: 0.3061328913484301
RMSE train: 0.496803	val: 0.748181	test: 0.622461
MAE train: 0.385729	val: 0.554878	test: 0.473203

Epoch: 111
Loss: 0.2964485947574888
RMSE train: 0.482298	val: 0.741690	test: 0.625572
MAE train: 0.374399	val: 0.554549	test: 0.472709

Epoch: 112
Loss: 0.2881230999316488
RMSE train: 0.480180	val: 0.736056	test: 0.627121
MAE train: 0.374189	val: 0.547676	test: 0.475882

Epoch: 113
Loss: 0.27931056916713715
RMSE train: 0.491648	val: 0.745802	test: 0.633821
MAE train: 0.384017	val: 0.557967	test: 0.477385

Epoch: 114
Loss: 0.2810471813593592
RMSE train: 0.513117	val: 0.757483	test: 0.672057
MAE train: 0.398564	val: 0.570689	test: 0.512764

Epoch: 115
Loss: 0.29812431974070414
RMSE train: 0.502326	val: 0.767065	test: 0.636468
MAE train: 0.391343	val: 0.564416	test: 0.488590

Epoch: 116
Loss: 0.3142113611102104
RMSE train: 0.492561	val: 0.740958	test: 0.635684
MAE train: 0.383807	val: 0.555158	test: 0.483052

Epoch: 117
Loss: 0.2959108373948506
RMSE train: 0.493986	val: 0.740276	test: 0.634261
MAE train: 0.386845	val: 0.559981	test: 0.483909

Epoch: 118
Loss: 0.2948827924472945
RMSE train: 0.496542	val: 0.751865	test: 0.626679
MAE train: 0.386125	val: 0.556885	test: 0.475889

Epoch: 119
Loss: 0.29834476113319397
RMSE train: 0.478410	val: 0.750468	test: 0.637189
MAE train: 0.371579	val: 0.563551	test: 0.481736

Epoch: 120
Loss: 0.26246233923094614
RMSE train: 0.483076	val: 0.748060	test: 0.631454
MAE train: 0.377372	val: 0.557466	test: 0.479229

Epoch: 121
Loss: 0.2644529789686203
RMSE train: 0.497660	val: 0.749271	test: 0.643093
MAE train: 0.388758	val: 0.555529	test: 0.491650

Epoch: 122
Loss: 0.2800860490117754
RMSE train: 0.479765	val: 0.730707	test: 0.633009
MAE train: 0.373783	val: 0.546697	test: 0.479415

Early stopping
Best (RMSE):	 train: 0.529353	val: 0.730265	test: 0.647233
Best (MAE):	 train: 0.412137	val: 0.551670	test: 0.486559


Epoch: 144
Loss: 0.21927620768547057
RMSE train: 0.421223	val: 0.700233	test: 0.681136
MAE train: 0.331207	val: 0.508709	test: 0.515065

Epoch: 145
Loss: 0.21724785268306732
RMSE train: 0.435717	val: 0.700947	test: 0.700811
MAE train: 0.341669	val: 0.507670	test: 0.527645

Epoch: 146
Loss: 0.21801228672266007
RMSE train: 0.414291	val: 0.697363	test: 0.688343
MAE train: 0.324269	val: 0.499146	test: 0.519101

Epoch: 147
Loss: 0.21788484305143357
RMSE train: 0.442106	val: 0.705058	test: 0.697771
MAE train: 0.347143	val: 0.513131	test: 0.527869

Epoch: 148
Loss: 0.2270699992775917
RMSE train: 0.430939	val: 0.700124	test: 0.701446
MAE train: 0.338977	val: 0.505724	test: 0.527241

Epoch: 149
Loss: 0.22111071944236754
RMSE train: 0.413043	val: 0.693757	test: 0.689762
MAE train: 0.324929	val: 0.498707	test: 0.510631

Epoch: 150
Loss: 0.2184096097946167
RMSE train: 0.406990	val: 0.680728	test: 0.687335
MAE train: 0.317225	val: 0.490949	test: 0.513046

Epoch: 151
Loss: 0.2144540250301361
RMSE train: 0.403500	val: 0.683915	test: 0.677954
MAE train: 0.317200	val: 0.493274	test: 0.509588

Epoch: 152
Loss: 0.21885578632354735
RMSE train: 0.404387	val: 0.684576	test: 0.681962
MAE train: 0.317253	val: 0.494469	test: 0.511581

Epoch: 153
Loss: 0.2108288288116455
RMSE train: 0.414918	val: 0.687110	test: 0.685917
MAE train: 0.325831	val: 0.491930	test: 0.511880

Epoch: 154
Loss: 0.21217579394578934
RMSE train: 0.423523	val: 0.693826	test: 0.694779
MAE train: 0.331007	val: 0.503749	test: 0.523236

Epoch: 155
Loss: 0.20777416229248047
RMSE train: 0.433327	val: 0.698655	test: 0.703277
MAE train: 0.339493	val: 0.508049	test: 0.533727

Epoch: 156
Loss: 0.20742996484041215
RMSE train: 0.419505	val: 0.698093	test: 0.699920
MAE train: 0.328558	val: 0.503643	test: 0.526803

Epoch: 157
Loss: 0.2023061141371727
RMSE train: 0.430755	val: 0.693771	test: 0.705314
MAE train: 0.338531	val: 0.499615	test: 0.526551

Epoch: 158
Loss: 0.20818571895360946
RMSE train: 0.436396	val: 0.710714	test: 0.710618
MAE train: 0.343917	val: 0.518158	test: 0.535719

Epoch: 159
Loss: 0.2128534272313118
RMSE train: 0.424924	val: 0.691105	test: 0.701342
MAE train: 0.335924	val: 0.504245	test: 0.524235

Epoch: 160
Loss: 0.2168133407831192
RMSE train: 0.394656	val: 0.680978	test: 0.690686
MAE train: 0.308809	val: 0.491911	test: 0.517020

Early stopping
Best (RMSE):	 train: 0.421640	val: 0.678994	test: 0.687205
Best (MAE):	 train: 0.330895	val: 0.493557	test: 0.511770

MAE train: 0.395936	val: 0.511551	test: 0.515276

Epoch: 84
Loss: 0.3309728552897771
RMSE train: 0.524038	val: 0.721649	test: 0.696214
MAE train: 0.409333	val: 0.521373	test: 0.527413

Epoch: 85
Loss: 0.3234953184922536
RMSE train: 0.520403	val: 0.707688	test: 0.687477
MAE train: 0.405296	val: 0.517308	test: 0.518391

Epoch: 86
Loss: 0.312399297952652
RMSE train: 0.520787	val: 0.729899	test: 0.700716
MAE train: 0.407251	val: 0.538775	test: 0.523328

Epoch: 87
Loss: 0.3120872477690379
RMSE train: 0.508111	val: 0.719344	test: 0.684831
MAE train: 0.395291	val: 0.522449	test: 0.509337

Epoch: 88
Loss: 0.30466582874457043
RMSE train: 0.496961	val: 0.712959	test: 0.684746
MAE train: 0.385844	val: 0.516669	test: 0.506936

Epoch: 89
Loss: 0.3142126239836216
RMSE train: 0.490840	val: 0.705912	test: 0.692118
MAE train: 0.381923	val: 0.510747	test: 0.513769

Epoch: 90
Loss: 0.31591859459877014
RMSE train: 0.506023	val: 0.713190	test: 0.690421
MAE train: 0.396916	val: 0.517304	test: 0.517531

Epoch: 91
Loss: 0.32262108474969864
RMSE train: 0.512867	val: 0.720547	test: 0.687682
MAE train: 0.399127	val: 0.520574	test: 0.515169

Epoch: 92
Loss: 0.30628175909320515
RMSE train: 0.492686	val: 0.703227	test: 0.682070
MAE train: 0.387964	val: 0.515003	test: 0.508211

Epoch: 93
Loss: 0.29689642662803334
RMSE train: 0.477381	val: 0.688012	test: 0.677313
MAE train: 0.372943	val: 0.502521	test: 0.506404

Epoch: 94
Loss: 0.29975273956855136
RMSE train: 0.503209	val: 0.696202	test: 0.685060
MAE train: 0.393522	val: 0.510801	test: 0.514122

Epoch: 95
Loss: 0.29725321754813194
RMSE train: 0.552765	val: 0.742937	test: 0.731615
MAE train: 0.432574	val: 0.547488	test: 0.548512

Epoch: 96
Loss: 0.3074650640288989
RMSE train: 0.494182	val: 0.710119	test: 0.691195
MAE train: 0.383356	val: 0.519774	test: 0.516299

Epoch: 97
Loss: 0.30387790501117706
RMSE train: 0.498942	val: 0.716930	test: 0.694303
MAE train: 0.391388	val: 0.524148	test: 0.517281

Epoch: 98
Loss: 0.29898039251565933
RMSE train: 0.537790	val: 0.743967	test: 0.723638
MAE train: 0.422295	val: 0.545301	test: 0.541436

Epoch: 99
Loss: 0.2990499871472518
RMSE train: 0.491313	val: 0.712756	test: 0.698447
MAE train: 0.385361	val: 0.530148	test: 0.521227

Epoch: 100
Loss: 0.2941248118877411
RMSE train: 0.502264	val: 0.724153	test: 0.704522
MAE train: 0.392070	val: 0.530239	test: 0.522762

Epoch: 101
Loss: 0.29984842240810394
RMSE train: 0.511083	val: 0.732506	test: 0.709670
MAE train: 0.399996	val: 0.534622	test: 0.533491

Epoch: 102
Loss: 0.29480892419815063
RMSE train: 0.488743	val: 0.708167	test: 0.693926
MAE train: 0.381705	val: 0.516839	test: 0.521909

Epoch: 103
Loss: 0.29165343567728996
RMSE train: 0.475339	val: 0.698936	test: 0.686825
MAE train: 0.372459	val: 0.515278	test: 0.513393

Epoch: 104
Loss: 0.28505245223641396
RMSE train: 0.495331	val: 0.713706	test: 0.700701
MAE train: 0.389007	val: 0.519163	test: 0.522841

Epoch: 105
Loss: 0.28146061301231384
RMSE train: 0.500509	val: 0.715458	test: 0.702012
MAE train: 0.393277	val: 0.519800	test: 0.523929

Epoch: 106
Loss: 0.27929256359736127
RMSE train: 0.479621	val: 0.703197	test: 0.696803
MAE train: 0.374852	val: 0.510138	test: 0.514693

Epoch: 107
Loss: 0.28592439492543537
RMSE train: 0.496385	val: 0.715429	test: 0.696981
MAE train: 0.389186	val: 0.519790	test: 0.516067

Epoch: 108
Loss: 0.2780694055060546
RMSE train: 0.477184	val: 0.703084	test: 0.694959
MAE train: 0.372522	val: 0.513126	test: 0.519132

Epoch: 109
Loss: 0.2742086860040824
RMSE train: 0.479709	val: 0.695067	test: 0.686975
MAE train: 0.376901	val: 0.503437	test: 0.517056

Epoch: 110
Loss: 0.28058793644110364
RMSE train: 0.462410	val: 0.686008	test: 0.681000
MAE train: 0.361861	val: 0.499245	test: 0.508551

Epoch: 111
Loss: 0.27360812698801357
RMSE train: 0.491165	val: 0.710211	test: 0.695876
MAE train: 0.385671	val: 0.519909	test: 0.518861

Epoch: 112
Loss: 0.26796992992361385
RMSE train: 0.487914	val: 0.717777	test: 0.699590
MAE train: 0.383064	val: 0.522031	test: 0.521588

Epoch: 113
Loss: 0.26641705880562466
RMSE train: 0.463820	val: 0.704486	test: 0.683249
MAE train: 0.361885	val: 0.511881	test: 0.513604

Epoch: 114
Loss: 0.26201259965697926
RMSE train: 0.486182	val: 0.711284	test: 0.699542
MAE train: 0.379431	val: 0.516486	test: 0.527922

Epoch: 115
Loss: 0.26667844131588936
RMSE train: 0.465253	val: 0.709814	test: 0.690736
MAE train: 0.365590	val: 0.519055	test: 0.514292

Epoch: 116
Loss: 0.26646284759044647
RMSE train: 0.475579	val: 0.706099	test: 0.682284
MAE train: 0.370570	val: 0.512227	test: 0.511835

Epoch: 117
Loss: 0.2785443303485711
RMSE train: 0.476573	val: 0.705529	test: 0.692720
MAE train: 0.371819	val: 0.524472	test: 0.515534

Epoch: 118
Loss: 0.26481647168596584
RMSE train: 0.461793	val: 0.691786	test: 0.676592
MAE train: 0.361661	val: 0.509791	test: 0.505944

Epoch: 119
Loss: 0.25767506410678226
RMSE train: 0.456114	val: 0.689796	test: 0.682250
MAE train: 0.357049	val: 0.510717	test: 0.514357

Epoch: 120
Loss: 0.26135193929076195
RMSE train: 0.472744	val: 0.705145	test: 0.688563
MAE train: 0.369484	val: 0.513280	test: 0.516260

Epoch: 121
Loss: 0.2523300275206566
RMSE train: 0.462869	val: 0.687098	test: 0.689181
MAE train: 0.361538	val: 0.505474	test: 0.511962

Epoch: 122
Loss: 0.26536277184883755
RMSE train: 0.458399	val: 0.699457	test: 0.685947
MAE train: 0.359364	val: 0.521610	test: 0.514228

Epoch: 123
Loss: 0.2604137683908145
RMSE train: 0.453902	val: 0.686604	test: 0.675039
MAE train: 0.355134	val: 0.503508	test: 0.499890

Epoch: 124
Loss: 0.26134102419018745
RMSE train: 0.491538	val: 0.711353	test: 0.704085
MAE train: 0.385101	val: 0.525281	test: 0.528274

Epoch: 125
Loss: 0.2547784236570199
RMSE train: 0.466323	val: 0.698131	test: 0.685472
MAE train: 0.362909	val: 0.507649	test: 0.516961

Epoch: 126
Loss: 0.2675642942388852
RMSE train: 0.466336	val: 0.690987	test: 0.685204
MAE train: 0.365775	val: 0.502399	test: 0.515848

Epoch: 127
Loss: 0.26190773770213127
RMSE train: 0.455865	val: 0.700752	test: 0.688665
MAE train: 0.356982	val: 0.508323	test: 0.516351

Epoch: 128
Loss: 0.26887256527940434
RMSE train: 0.531101	val: 0.738071	test: 0.747160
MAE train: 0.419101	val: 0.547347	test: 0.567418

Epoch: 129
Loss: 0.2506221594909827
RMSE train: 0.460061	val: 0.694195	test: 0.675871
MAE train: 0.360174	val: 0.510382	test: 0.505324

Epoch: 130
Loss: 0.24116195614139238
RMSE train: 0.450454	val: 0.691001	test: 0.685218
MAE train: 0.351068	val: 0.505876	test: 0.508265

Epoch: 131
Loss: 0.24902642518281937
RMSE train: 0.436749	val: 0.693506	test: 0.692927
MAE train: 0.340810	val: 0.513598	test: 0.515760

Epoch: 132
Loss: 0.24955644582708678
RMSE train: 0.435467	val: 0.691479	test: 0.676834
MAE train: 0.339963	val: 0.504882	test: 0.506056

Epoch: 133
Loss: 0.2403807913263639
RMSE train: 0.470300	val: 0.721495	test: 0.696683
MAE train: 0.367853	val: 0.526955	test: 0.521817

Epoch: 134
Loss: 0.2451799437403679
RMSE train: 0.452090	val: 0.709811	test: 0.694412
MAE train: 0.351512	val: 0.522711	test: 0.513996

Epoch: 135
Loss: 0.2492487629254659
RMSE train: 0.465102	val: 0.705522	test: 0.696656
MAE train: 0.363564	val: 0.517374	test: 0.519936

Epoch: 136
Loss: 0.24423784390091896
RMSE train: 0.446800	val: 0.700662	test: 0.681077
MAE train: 0.351171	val: 0.519185	test: 0.509310

Epoch: 137
Loss: 0.24185418461759886
RMSE train: 0.450896	val: 0.701089	test: 0.679634
MAE train: 0.350684	val: 0.506809	test: 0.511070

Epoch: 138
Loss: 0.23019778976837793
RMSE train: 0.456868	val: 0.707647	test: 0.684697
MAE train: 0.355921	val: 0.518817	test: 0.512053

Epoch: 139
Loss: 0.23457489783565202
RMSE train: 0.463082	val: 0.711391	test: 0.699415
MAE train: 0.363783	val: 0.521548	test: 0.528372

Epoch: 140
Loss: 0.24047734712560973
RMSE train: 0.443573	val: 0.696326	test: 0.675241
MAE train: 0.348562	val: 0.521559	test: 0.505187

Epoch: 141
Loss: 0.23251734177271524
RMSE train: 0.458743	val: 0.706138	test: 0.696078
MAE train: 0.361291	val: 0.519209	test: 0.521656

Epoch: 142
Loss: 0.22567139069239298
RMSE train: 0.454340	val: 0.719025	test: 0.684799
MAE train: 0.358255	val: 0.521354	test: 0.514569

Epoch: 143
Loss: 0.226273192713658
RMSE train: 0.430455	val: 0.687003	test: 0.668575
MAE train: 0.338347	val: 0.502913	test: 0.498824
MAE train: 0.417356	val: 0.536423	test: 0.549943

Epoch: 84
Loss: 0.31087302789092064
RMSE train: 0.509336	val: 0.693117	test: 0.699609
MAE train: 0.397088	val: 0.504121	test: 0.529458

Epoch: 85
Loss: 0.31712555636962253
RMSE train: 0.524480	val: 0.701465	test: 0.723330
MAE train: 0.407892	val: 0.514280	test: 0.552115

Epoch: 86
Loss: 0.31669330100218457
RMSE train: 0.534672	val: 0.704995	test: 0.715324
MAE train: 0.416836	val: 0.520157	test: 0.538667

Epoch: 87
Loss: 0.30064260959625244
RMSE train: 0.497429	val: 0.690935	test: 0.691831
MAE train: 0.386298	val: 0.501431	test: 0.527434

Epoch: 88
Loss: 0.3084417035182317
RMSE train: 0.523706	val: 0.701389	test: 0.701554
MAE train: 0.407168	val: 0.516957	test: 0.530967

Epoch: 89
Loss: 0.31625790645678836
RMSE train: 0.492291	val: 0.699374	test: 0.689199
MAE train: 0.384688	val: 0.519877	test: 0.528213

Epoch: 90
Loss: 0.3199974795182546
RMSE train: 0.516121	val: 0.699614	test: 0.698608
MAE train: 0.402919	val: 0.514525	test: 0.534729

Epoch: 91
Loss: 0.3135824998219808
RMSE train: 0.490243	val: 0.693480	test: 0.689669
MAE train: 0.380459	val: 0.505433	test: 0.520622

Epoch: 92
Loss: 0.3074096664786339
RMSE train: 0.507377	val: 0.704083	test: 0.711347
MAE train: 0.395959	val: 0.520639	test: 0.537085

Epoch: 93
Loss: 0.3144081731637319
RMSE train: 0.478991	val: 0.695207	test: 0.687656
MAE train: 0.371785	val: 0.506836	test: 0.514426

Epoch: 94
Loss: 0.3059658408164978
RMSE train: 0.510020	val: 0.701557	test: 0.708876
MAE train: 0.396686	val: 0.512397	test: 0.531119

Epoch: 95
Loss: 0.2945222432414691
RMSE train: 0.503624	val: 0.684851	test: 0.700722
MAE train: 0.393581	val: 0.507008	test: 0.524785

Epoch: 96
Loss: 0.30361073464155197
RMSE train: 0.489458	val: 0.694182	test: 0.683837
MAE train: 0.382243	val: 0.498262	test: 0.513905

Epoch: 97
Loss: 0.2894936924179395
RMSE train: 0.544290	val: 0.724399	test: 0.729852
MAE train: 0.429433	val: 0.532327	test: 0.552032

Epoch: 98
Loss: 0.29807494580745697
RMSE train: 0.498313	val: 0.700353	test: 0.697021
MAE train: 0.387526	val: 0.508784	test: 0.527260

Epoch: 99
Loss: 0.28423447782794636
RMSE train: 0.514545	val: 0.705363	test: 0.712332
MAE train: 0.403208	val: 0.515441	test: 0.535991

Epoch: 100
Loss: 0.2916650039454301
RMSE train: 0.478365	val: 0.703816	test: 0.684533
MAE train: 0.372320	val: 0.507520	test: 0.516162

Epoch: 101
Loss: 0.29090214769045514
RMSE train: 0.498722	val: 0.715612	test: 0.700170
MAE train: 0.390023	val: 0.513525	test: 0.521083

Epoch: 102
Loss: 0.29262369001905125
RMSE train: 0.524422	val: 0.706970	test: 0.724764
MAE train: 0.408193	val: 0.520020	test: 0.547772

Epoch: 103
Loss: 0.3093396946787834
RMSE train: 0.465895	val: 0.671760	test: 0.668037
MAE train: 0.364544	val: 0.487082	test: 0.500250

Epoch: 104
Loss: 0.2840975560247898
RMSE train: 0.483862	val: 0.703108	test: 0.685726
MAE train: 0.375340	val: 0.509468	test: 0.511482

Epoch: 105
Loss: 0.2838287154833476
RMSE train: 0.510248	val: 0.706206	test: 0.710819
MAE train: 0.399440	val: 0.516029	test: 0.536069

Epoch: 106
Loss: 0.28284308438499767
RMSE train: 0.487294	val: 0.687274	test: 0.697575
MAE train: 0.379284	val: 0.500945	test: 0.527786

Epoch: 107
Loss: 0.2862609960138798
RMSE train: 0.506447	val: 0.693095	test: 0.708877
MAE train: 0.393963	val: 0.506305	test: 0.533693

Epoch: 108
Loss: 0.2795911841094494
RMSE train: 0.489888	val: 0.694469	test: 0.700474
MAE train: 0.383601	val: 0.503519	test: 0.531148

Epoch: 109
Loss: 0.2817952632904053
RMSE train: 0.464643	val: 0.680197	test: 0.680739
MAE train: 0.364996	val: 0.499373	test: 0.509431

Epoch: 110
Loss: 0.26486242190003395
RMSE train: 0.506915	val: 0.710903	test: 0.715408
MAE train: 0.397230	val: 0.515963	test: 0.534638

Epoch: 111
Loss: 0.27701127529144287
RMSE train: 0.468878	val: 0.682653	test: 0.681448
MAE train: 0.366395	val: 0.495339	test: 0.508341

Epoch: 112
Loss: 0.2770335301756859
RMSE train: 0.468263	val: 0.690694	test: 0.693137
MAE train: 0.367243	val: 0.498147	test: 0.518292

Epoch: 113
Loss: 0.26796577125787735
RMSE train: 0.472498	val: 0.686136	test: 0.685409
MAE train: 0.370298	val: 0.498597	test: 0.511908

Epoch: 114
Loss: 0.2656375877559185
RMSE train: 0.458588	val: 0.679648	test: 0.668222
MAE train: 0.359093	val: 0.488673	test: 0.496397

Epoch: 115
Loss: 0.26555611565709114
RMSE train: 0.485707	val: 0.697487	test: 0.701393
MAE train: 0.379034	val: 0.508468	test: 0.525445

Epoch: 116
Loss: 0.26302634303768474
RMSE train: 0.476562	val: 0.687217	test: 0.680914
MAE train: 0.373370	val: 0.501047	test: 0.512170

Epoch: 117
Loss: 0.2568766189118226
RMSE train: 0.461185	val: 0.685251	test: 0.686648
MAE train: 0.360775	val: 0.495670	test: 0.517275

Epoch: 118
Loss: 0.26170676201581955
RMSE train: 0.499534	val: 0.706436	test: 0.714619
MAE train: 0.389865	val: 0.513393	test: 0.542523

Epoch: 119
Loss: 0.25362709661324817
RMSE train: 0.454108	val: 0.689562	test: 0.676525
MAE train: 0.354343	val: 0.495429	test: 0.503166

Epoch: 120
Loss: 0.24790060271819434
RMSE train: 0.460098	val: 0.699881	test: 0.684835
MAE train: 0.359383	val: 0.503370	test: 0.513615

Epoch: 121
Loss: 0.25747974837819737
RMSE train: 0.445989	val: 0.681361	test: 0.679083
MAE train: 0.348947	val: 0.496208	test: 0.510941

Epoch: 122
Loss: 0.2473945952951908
RMSE train: 0.442717	val: 0.681731	test: 0.662117
MAE train: 0.345170	val: 0.487612	test: 0.495107

Epoch: 123
Loss: 0.25308649117747944
RMSE train: 0.456372	val: 0.675513	test: 0.679794
MAE train: 0.356365	val: 0.492328	test: 0.511042

Epoch: 124
Loss: 0.25441118826468784
RMSE train: 0.464760	val: 0.695299	test: 0.691688
MAE train: 0.361469	val: 0.504757	test: 0.514880

Epoch: 125
Loss: 0.2553277400632699
RMSE train: 0.482186	val: 0.703183	test: 0.695209
MAE train: 0.377811	val: 0.517724	test: 0.522088

Epoch: 126
Loss: 0.24951286986470222
RMSE train: 0.507243	val: 0.733438	test: 0.733143
MAE train: 0.399125	val: 0.541379	test: 0.552169

Epoch: 127
Loss: 0.2442100209494432
RMSE train: 0.460362	val: 0.688079	test: 0.688085
MAE train: 0.359261	val: 0.499512	test: 0.514293

Epoch: 128
Loss: 0.2417401820421219
RMSE train: 0.453923	val: 0.676948	test: 0.682973
MAE train: 0.355419	val: 0.497684	test: 0.519590

Epoch: 129
Loss: 0.24664821848273277
RMSE train: 0.436412	val: 0.673379	test: 0.682839
MAE train: 0.341038	val: 0.491458	test: 0.511202

Epoch: 130
Loss: 0.24883038302262625
RMSE train: 0.454322	val: 0.681442	test: 0.692410
MAE train: 0.356115	val: 0.495442	test: 0.523812

Epoch: 131
Loss: 0.2554490293065707
RMSE train: 0.476804	val: 0.702728	test: 0.694708
MAE train: 0.375394	val: 0.513406	test: 0.526362

Epoch: 132
Loss: 0.24961320559183756
RMSE train: 0.460130	val: 0.696402	test: 0.694538
MAE train: 0.359834	val: 0.507591	test: 0.519695

Epoch: 133
Loss: 0.24759325260917345
RMSE train: 0.465583	val: 0.696062	test: 0.707061
MAE train: 0.366039	val: 0.509054	test: 0.529257

Epoch: 134
Loss: 0.23751558984319368
RMSE train: 0.440723	val: 0.678541	test: 0.682853
MAE train: 0.345866	val: 0.490741	test: 0.512163

Epoch: 135
Loss: 0.2372562400996685
RMSE train: 0.431624	val: 0.675933	test: 0.668051
MAE train: 0.338741	val: 0.487832	test: 0.497349

Epoch: 136
Loss: 0.2388606034219265
RMSE train: 0.453041	val: 0.669284	test: 0.682897
MAE train: 0.355004	val: 0.488159	test: 0.509027

Epoch: 137
Loss: 0.24236985171834627
RMSE train: 0.447586	val: 0.674725	test: 0.685081
MAE train: 0.349887	val: 0.502967	test: 0.517770

Epoch: 138
Loss: 0.24221043412884077
RMSE train: 0.450686	val: 0.680406	test: 0.701395
MAE train: 0.353753	val: 0.495129	test: 0.528008

Epoch: 139
Loss: 0.23595848058660826
RMSE train: 0.462261	val: 0.696911	test: 0.692552
MAE train: 0.359919	val: 0.503575	test: 0.520873

Epoch: 140
Loss: 0.22631432488560677
RMSE train: 0.451409	val: 0.688527	test: 0.678576
MAE train: 0.355542	val: 0.499329	test: 0.509136

Epoch: 141
Loss: 0.23396986971298853
RMSE train: 0.434888	val: 0.677188	test: 0.686643
MAE train: 0.338036	val: 0.497864	test: 0.510394

Epoch: 142
Loss: 0.23248262455066046
RMSE train: 0.442871	val: 0.691548	test: 0.684282
MAE train: 0.347134	val: 0.499273	test: 0.508554

Epoch: 143
Loss: 0.23428802813092867
RMSE train: 0.441868	val: 0.673947	test: 0.685397
MAE train: 0.346320	val: 0.493096	test: 0.510502

Epoch: 144
Loss: 0.23735830436150232
RMSE train: 0.449173	val: 0.710022	test: 0.698940
MAE train: 0.351741	val: 0.525764	test: 0.521070

Epoch: 145
Loss: 0.23402572547396025
RMSE train: 0.470566	val: 0.725272	test: 0.710766
MAE train: 0.367419	val: 0.532440	test: 0.533076

Early stopping
Best (RMSE):	 train: 0.462410	val: 0.686008	test: 0.681000
Best (MAE):	 train: 0.361861	val: 0.499245	test: 0.508551

MAE train: 0.413072	val: 0.529041	test: 0.526161

Epoch: 84
Loss: 0.33345189442237216
RMSE train: 0.509821	val: 0.730595	test: 0.669260
MAE train: 0.399041	val: 0.537564	test: 0.512485

Epoch: 85
Loss: 0.31848207861185074
RMSE train: 0.529866	val: 0.734675	test: 0.687924
MAE train: 0.414484	val: 0.535719	test: 0.525501

Epoch: 86
Loss: 0.31999297936757404
RMSE train: 0.516304	val: 0.718860	test: 0.677784
MAE train: 0.404677	val: 0.522806	test: 0.516386

Epoch: 87
Loss: 0.3155695895353953
RMSE train: 0.547132	val: 0.749782	test: 0.706453
MAE train: 0.428891	val: 0.548001	test: 0.540437

Epoch: 88
Loss: 0.3164620374639829
RMSE train: 0.510485	val: 0.722520	test: 0.671833
MAE train: 0.398808	val: 0.517857	test: 0.508568

Epoch: 89
Loss: 0.31683072199424106
RMSE train: 0.547716	val: 0.758024	test: 0.711702
MAE train: 0.427859	val: 0.555097	test: 0.538391

Epoch: 90
Loss: 0.3046267032623291
RMSE train: 0.531638	val: 0.753884	test: 0.704660
MAE train: 0.416303	val: 0.551722	test: 0.528708

Epoch: 91
Loss: 0.3042120796938737
RMSE train: 0.517166	val: 0.733846	test: 0.691542
MAE train: 0.404726	val: 0.536151	test: 0.527372

Epoch: 92
Loss: 0.2998652160167694
RMSE train: 0.523310	val: 0.740978	test: 0.688388
MAE train: 0.412000	val: 0.536064	test: 0.524011

Epoch: 93
Loss: 0.2971122736732165
RMSE train: 0.488040	val: 0.712638	test: 0.670812
MAE train: 0.383272	val: 0.517848	test: 0.512700

Epoch: 94
Loss: 0.2871566911538442
RMSE train: 0.497720	val: 0.717339	test: 0.672970
MAE train: 0.388251	val: 0.527537	test: 0.507204

Epoch: 95
Loss: 0.2885540574789047
RMSE train: 0.500569	val: 0.719224	test: 0.675756
MAE train: 0.388880	val: 0.514683	test: 0.511608

Epoch: 96
Loss: 0.30462439854939777
RMSE train: 0.515509	val: 0.727846	test: 0.698976
MAE train: 0.401016	val: 0.531524	test: 0.536938

Epoch: 97
Loss: 0.2905578116575877
RMSE train: 0.517512	val: 0.735090	test: 0.699122
MAE train: 0.402879	val: 0.536300	test: 0.527256

Epoch: 98
Loss: 0.3050328580041726
RMSE train: 0.498059	val: 0.713816	test: 0.667512
MAE train: 0.392672	val: 0.526057	test: 0.515644

Epoch: 99
Loss: 0.29150805870691937
RMSE train: 0.494653	val: 0.726572	test: 0.683583
MAE train: 0.386005	val: 0.521867	test: 0.522877

Epoch: 100
Loss: 0.28561319038271904
RMSE train: 0.500672	val: 0.702124	test: 0.675642
MAE train: 0.392221	val: 0.510456	test: 0.518965

Epoch: 101
Loss: 0.2971482214828332
RMSE train: 0.494971	val: 0.731734	test: 0.673164
MAE train: 0.387337	val: 0.527239	test: 0.519058

Epoch: 102
Loss: 0.2865108201901118
RMSE train: 0.495456	val: 0.717607	test: 0.678386
MAE train: 0.386405	val: 0.513013	test: 0.514422

Epoch: 103
Loss: 0.27323784058292705
RMSE train: 0.494370	val: 0.721092	test: 0.668836
MAE train: 0.386277	val: 0.521898	test: 0.507892

Epoch: 104
Loss: 0.2739199101924896
RMSE train: 0.488458	val: 0.735458	test: 0.675121
MAE train: 0.379218	val: 0.538989	test: 0.509265

Epoch: 105
Loss: 0.28552763039867085
RMSE train: 0.487536	val: 0.720021	test: 0.670945
MAE train: 0.381199	val: 0.516062	test: 0.508874

Epoch: 106
Loss: 0.2742086748282115
RMSE train: 0.506579	val: 0.719423	test: 0.685957
MAE train: 0.396542	val: 0.522900	test: 0.519847

Epoch: 107
Loss: 0.28425624097387
RMSE train: 0.475979	val: 0.712008	test: 0.664625
MAE train: 0.373608	val: 0.517452	test: 0.506046

Epoch: 108
Loss: 0.2649151086807251
RMSE train: 0.478741	val: 0.703974	test: 0.665279
MAE train: 0.372216	val: 0.502833	test: 0.506939

Epoch: 109
Loss: 0.2646174021065235
RMSE train: 0.469323	val: 0.701576	test: 0.662019
MAE train: 0.365151	val: 0.504066	test: 0.506527

Epoch: 110
Loss: 0.2788841078678767
RMSE train: 0.476443	val: 0.709600	test: 0.672460
MAE train: 0.371966	val: 0.509885	test: 0.510835

Epoch: 111
Loss: 0.28073910251259804
RMSE train: 0.480952	val: 0.706542	test: 0.674466
MAE train: 0.373278	val: 0.515700	test: 0.508160

Epoch: 112
Loss: 0.26980770751833916
RMSE train: 0.475670	val: 0.733934	test: 0.678471
MAE train: 0.370374	val: 0.539510	test: 0.516961

Epoch: 113
Loss: 0.2596760056912899
RMSE train: 0.481154	val: 0.716381	test: 0.682939
MAE train: 0.374812	val: 0.519905	test: 0.514495

Epoch: 114
Loss: 0.2707785281042258
RMSE train: 0.472995	val: 0.702456	test: 0.671746
MAE train: 0.370494	val: 0.515298	test: 0.512624

Epoch: 115
Loss: 0.2632407794396083
RMSE train: 0.472308	val: 0.703203	test: 0.677154
MAE train: 0.370224	val: 0.515505	test: 0.520669

Epoch: 116
Loss: 0.2701287344098091
RMSE train: 0.506881	val: 0.715143	test: 0.696127
MAE train: 0.398030	val: 0.525804	test: 0.528082

Epoch: 117
Loss: 0.2660800516605377
RMSE train: 0.479012	val: 0.704641	test: 0.681290
MAE train: 0.372250	val: 0.514252	test: 0.515833

Epoch: 118
Loss: 0.26775672535101575
RMSE train: 0.469730	val: 0.705598	test: 0.674937
MAE train: 0.366046	val: 0.516840	test: 0.512187

Epoch: 119
Loss: 0.24925034369031587
RMSE train: 0.475345	val: 0.717524	test: 0.679375
MAE train: 0.368182	val: 0.522942	test: 0.513515

Epoch: 120
Loss: 0.2522328483561675
RMSE train: 0.455588	val: 0.704837	test: 0.659316
MAE train: 0.355361	val: 0.514610	test: 0.504192

Epoch: 121
Loss: 0.2624128833413124
RMSE train: 0.490899	val: 0.704785	test: 0.695398
MAE train: 0.382747	val: 0.517284	test: 0.530290

Epoch: 122
Loss: 0.24983859434723854
RMSE train: 0.454725	val: 0.699448	test: 0.670378
MAE train: 0.355997	val: 0.511062	test: 0.509314

Epoch: 123
Loss: 0.25319188709060353
RMSE train: 0.451287	val: 0.696864	test: 0.666395
MAE train: 0.352182	val: 0.503200	test: 0.501615

Epoch: 124
Loss: 0.26067901651064557
RMSE train: 0.457015	val: 0.689649	test: 0.665535
MAE train: 0.357793	val: 0.513216	test: 0.509422

Epoch: 125
Loss: 0.24809615314006805
RMSE train: 0.451432	val: 0.705725	test: 0.658615
MAE train: 0.353612	val: 0.508904	test: 0.498278

Epoch: 126
Loss: 0.25236666947603226
RMSE train: 0.451484	val: 0.704498	test: 0.667602
MAE train: 0.351755	val: 0.505491	test: 0.503836

Epoch: 127
Loss: 0.24257913852731386
RMSE train: 0.445913	val: 0.698096	test: 0.659566
MAE train: 0.347692	val: 0.501173	test: 0.496450

Epoch: 128
Loss: 0.2393639696141084
RMSE train: 0.452560	val: 0.703702	test: 0.665667
MAE train: 0.353248	val: 0.515299	test: 0.503289

Epoch: 129
Loss: 0.24958508213361105
RMSE train: 0.443683	val: 0.689067	test: 0.663342
MAE train: 0.348551	val: 0.503510	test: 0.503020

Epoch: 130
Loss: 0.24326388786236444
RMSE train: 0.456300	val: 0.692211	test: 0.667187
MAE train: 0.357213	val: 0.503071	test: 0.506217

Epoch: 131
Loss: 0.24280740693211555
RMSE train: 0.445403	val: 0.694773	test: 0.661224
MAE train: 0.348023	val: 0.499659	test: 0.495805

Epoch: 132
Loss: 0.24354699378212294
RMSE train: 0.451850	val: 0.699531	test: 0.669726
MAE train: 0.352415	val: 0.514466	test: 0.505459

Epoch: 133
Loss: 0.24156278495987257
RMSE train: 0.453387	val: 0.699884	test: 0.662978
MAE train: 0.353900	val: 0.499352	test: 0.502884

Epoch: 134
Loss: 0.24036729956666628
RMSE train: 0.440050	val: 0.694308	test: 0.664870
MAE train: 0.343397	val: 0.500917	test: 0.497476

Epoch: 135
Loss: 0.23536212369799614
RMSE train: 0.439772	val: 0.688658	test: 0.665802
MAE train: 0.342596	val: 0.494867	test: 0.499969

Epoch: 136
Loss: 0.23844105750322342
RMSE train: 0.436987	val: 0.687316	test: 0.659875
MAE train: 0.342732	val: 0.492030	test: 0.502221

Epoch: 137
Loss: 0.24013481164971986
RMSE train: 0.453294	val: 0.691243	test: 0.662223
MAE train: 0.355189	val: 0.503120	test: 0.504346

Epoch: 138
Loss: 0.23804553846518198
RMSE train: 0.443792	val: 0.697389	test: 0.665563
MAE train: 0.346371	val: 0.512038	test: 0.501016

Epoch: 139
Loss: 0.23217648640275002
RMSE train: 0.436589	val: 0.693872	test: 0.663560
MAE train: 0.339357	val: 0.503239	test: 0.499581

Epoch: 140
Loss: 0.2243950553238392
RMSE train: 0.432611	val: 0.689197	test: 0.658822
MAE train: 0.336536	val: 0.496839	test: 0.496097

Epoch: 141
Loss: 0.22917268797755241
RMSE train: 0.452157	val: 0.697610	test: 0.676101
MAE train: 0.353041	val: 0.508094	test: 0.506694

Epoch: 142
Loss: 0.23938348640998205
RMSE train: 0.451029	val: 0.706610	test: 0.677123
MAE train: 0.351116	val: 0.511844	test: 0.511409

Epoch: 143
Loss: 0.22829947248101234
RMSE train: 0.429971	val: 0.694812	test: 0.661212
MAE train: 0.336033	val: 0.497177	test: 0.497357
MAE train: 0.438996	val: 0.586117	test: 0.510002

Epoch: 84
Loss: 0.3218576876180513
RMSE train: 0.512745	val: 0.756309	test: 0.652939
MAE train: 0.398947	val: 0.555321	test: 0.484223

Epoch: 85
Loss: 0.3398969556604113
RMSE train: 0.530155	val: 0.786392	test: 0.664840
MAE train: 0.414644	val: 0.577300	test: 0.495127

Epoch: 86
Loss: 0.3411562442779541
RMSE train: 0.534534	val: 0.758103	test: 0.665979
MAE train: 0.417485	val: 0.578971	test: 0.497887

Epoch: 87
Loss: 0.3378490848200662
RMSE train: 0.525687	val: 0.760727	test: 0.638165
MAE train: 0.410503	val: 0.575605	test: 0.476717

Epoch: 88
Loss: 0.34490328814302174
RMSE train: 0.561488	val: 0.783452	test: 0.680001
MAE train: 0.437231	val: 0.582607	test: 0.507244

Epoch: 89
Loss: 0.3266556922878538
RMSE train: 0.525858	val: 0.776425	test: 0.673006
MAE train: 0.409752	val: 0.569052	test: 0.501034

Epoch: 90
Loss: 0.3299356315817152
RMSE train: 0.537278	val: 0.774413	test: 0.658153
MAE train: 0.420462	val: 0.569830	test: 0.493082

Epoch: 91
Loss: 0.32508324299539837
RMSE train: 0.531977	val: 0.766415	test: 0.650976
MAE train: 0.416763	val: 0.571826	test: 0.483364

Epoch: 92
Loss: 0.32844342078481403
RMSE train: 0.524894	val: 0.767196	test: 0.655912
MAE train: 0.410666	val: 0.568962	test: 0.478908

Epoch: 93
Loss: 0.31863099123750416
RMSE train: 0.506073	val: 0.777948	test: 0.637551
MAE train: 0.393752	val: 0.565213	test: 0.474034

Epoch: 94
Loss: 0.32108614487307413
RMSE train: 0.514644	val: 0.749305	test: 0.648434
MAE train: 0.401323	val: 0.567209	test: 0.483509

Epoch: 95
Loss: 0.31978230816977365
RMSE train: 0.509537	val: 0.755935	test: 0.646663
MAE train: 0.396938	val: 0.566335	test: 0.475051

Epoch: 96
Loss: 0.29880007462842123
RMSE train: 0.515343	val: 0.776391	test: 0.648510
MAE train: 0.404255	val: 0.586071	test: 0.485650

Epoch: 97
Loss: 0.3520211683852332
RMSE train: 0.527195	val: 0.769444	test: 0.645968
MAE train: 0.412487	val: 0.574956	test: 0.484531

Epoch: 98
Loss: 0.3060170944247927
RMSE train: 0.498629	val: 0.753365	test: 0.622503
MAE train: 0.388703	val: 0.553396	test: 0.462971

Epoch: 99
Loss: 0.3177003285714558
RMSE train: 0.521797	val: 0.773458	test: 0.664989
MAE train: 0.407658	val: 0.570891	test: 0.493337

Epoch: 100
Loss: 0.3111852833202907
RMSE train: 0.529059	val: 0.769989	test: 0.669727
MAE train: 0.413976	val: 0.568650	test: 0.495820

Epoch: 101
Loss: 0.3213718256780079
RMSE train: 0.501186	val: 0.760537	test: 0.645755
MAE train: 0.391476	val: 0.562923	test: 0.483220

Epoch: 102
Loss: 0.30201141961983274
RMSE train: 0.509156	val: 0.767814	test: 0.637977
MAE train: 0.395381	val: 0.567664	test: 0.477829

Epoch: 103
Loss: 0.29982752352952957
RMSE train: 0.506197	val: 0.753507	test: 0.640565
MAE train: 0.396195	val: 0.552453	test: 0.472623

Epoch: 104
Loss: 0.30743294741426197
RMSE train: 0.496891	val: 0.758165	test: 0.654145
MAE train: 0.385348	val: 0.556395	test: 0.487152

Epoch: 105
Loss: 0.3046540298632213
RMSE train: 0.558534	val: 0.798819	test: 0.683272
MAE train: 0.437573	val: 0.587511	test: 0.513428

Epoch: 106
Loss: 0.29932734795979093
RMSE train: 0.495252	val: 0.761418	test: 0.631756
MAE train: 0.385728	val: 0.554773	test: 0.464353

Epoch: 107
Loss: 0.3101346514054707
RMSE train: 0.497905	val: 0.746473	test: 0.653160
MAE train: 0.388504	val: 0.551195	test: 0.485815

Epoch: 108
Loss: 0.3215532952121326
RMSE train: 0.499304	val: 0.757358	test: 0.642712
MAE train: 0.390176	val: 0.566845	test: 0.480261

Epoch: 109
Loss: 0.29000525176525116
RMSE train: 0.493817	val: 0.747994	test: 0.649362
MAE train: 0.383688	val: 0.561297	test: 0.474345

Epoch: 110
Loss: 0.29066036322287153
RMSE train: 0.487794	val: 0.768021	test: 0.649624
MAE train: 0.379032	val: 0.562645	test: 0.479985

Epoch: 111
Loss: 0.29247420600482393
RMSE train: 0.476550	val: 0.752564	test: 0.624650
MAE train: 0.370397	val: 0.551749	test: 0.462290

Epoch: 112
Loss: 0.281734610242503
RMSE train: 0.502628	val: 0.766312	test: 0.644552
MAE train: 0.392923	val: 0.563246	test: 0.480243

Epoch: 113
Loss: 0.286761503134455
RMSE train: 0.491056	val: 0.781563	test: 0.655912
MAE train: 0.378395	val: 0.569650	test: 0.484605

Epoch: 114
Loss: 0.28138095885515213
RMSE train: 0.497954	val: 0.770109	test: 0.656361
MAE train: 0.387895	val: 0.557091	test: 0.484382

Epoch: 115
Loss: 0.2854198930518968
RMSE train: 0.486952	val: 0.750052	test: 0.627984
MAE train: 0.379832	val: 0.555025	test: 0.469653

Epoch: 116
Loss: 0.28409142047166824
RMSE train: 0.484009	val: 0.744647	test: 0.617566
MAE train: 0.376407	val: 0.547041	test: 0.458951

Epoch: 117
Loss: 0.2831034819994654
RMSE train: 0.485167	val: 0.749506	test: 0.648254
MAE train: 0.377656	val: 0.554592	test: 0.484005

Epoch: 118
Loss: 0.29231369495391846
RMSE train: 0.499800	val: 0.763231	test: 0.650031
MAE train: 0.388341	val: 0.560870	test: 0.479087

Epoch: 119
Loss: 0.29691260520900997
RMSE train: 0.487940	val: 0.768540	test: 0.639601
MAE train: 0.380869	val: 0.569692	test: 0.476127

Epoch: 120
Loss: 0.2699723648173468
RMSE train: 0.494581	val: 0.755043	test: 0.659616
MAE train: 0.383907	val: 0.557940	test: 0.486037

Epoch: 121
Loss: 0.27859757947070257
RMSE train: 0.497275	val: 0.750763	test: 0.634541
MAE train: 0.388848	val: 0.551390	test: 0.469134

Epoch: 122
Loss: 0.2753472572990826
RMSE train: 0.487768	val: 0.766079	test: 0.632880
MAE train: 0.377474	val: 0.560457	test: 0.460891

Epoch: 123
Loss: 0.28320724623543875
RMSE train: 0.483700	val: 0.752826	test: 0.642680
MAE train: 0.377360	val: 0.550930	test: 0.479822

Epoch: 124
Loss: 0.28785877994128634
RMSE train: 0.505507	val: 0.763764	test: 0.633816
MAE train: 0.393599	val: 0.564284	test: 0.475320

Epoch: 125
Loss: 0.27332670773778645
RMSE train: 0.480496	val: 0.723134	test: 0.637311
MAE train: 0.376658	val: 0.544360	test: 0.473330

Epoch: 126
Loss: 0.2812073113662856
RMSE train: 0.480046	val: 0.717312	test: 0.625339
MAE train: 0.376210	val: 0.543311	test: 0.466599

Epoch: 127
Loss: 0.26742280061755863
RMSE train: 0.465989	val: 0.741752	test: 0.615332
MAE train: 0.363385	val: 0.546590	test: 0.456150

Epoch: 128
Loss: 0.2724403994424002
RMSE train: 0.483846	val: 0.726360	test: 0.639317
MAE train: 0.378764	val: 0.543382	test: 0.479562

Epoch: 129
Loss: 0.275526082941464
RMSE train: 0.486979	val: 0.722193	test: 0.633353
MAE train: 0.382378	val: 0.548496	test: 0.482130

Epoch: 130
Loss: 0.2815771645733288
RMSE train: 0.471046	val: 0.734203	test: 0.630905
MAE train: 0.367515	val: 0.554959	test: 0.472953

Epoch: 131
Loss: 0.26183303339140757
RMSE train: 0.475248	val: 0.722277	test: 0.624428
MAE train: 0.372945	val: 0.550248	test: 0.468487

Epoch: 132
Loss: 0.26801808391298565
RMSE train: 0.479170	val: 0.733090	test: 0.622170
MAE train: 0.374538	val: 0.553083	test: 0.468840

Epoch: 133
Loss: 0.2665768467954227
RMSE train: 0.465226	val: 0.727914	test: 0.634612
MAE train: 0.362322	val: 0.542038	test: 0.471766

Epoch: 134
Loss: 0.2632195193852697
RMSE train: 0.463090	val: 0.717267	test: 0.629053
MAE train: 0.361935	val: 0.535166	test: 0.467120

Epoch: 135
Loss: 0.25153534540108274
RMSE train: 0.455563	val: 0.725452	test: 0.620971
MAE train: 0.354602	val: 0.534395	test: 0.458821

Epoch: 136
Loss: 0.2657940291932651
RMSE train: 0.451164	val: 0.711763	test: 0.638022
MAE train: 0.350404	val: 0.531964	test: 0.471014

Epoch: 137
Loss: 0.26715171550001415
RMSE train: 0.472538	val: 0.717257	test: 0.626168
MAE train: 0.369756	val: 0.536749	test: 0.470034

Epoch: 138
Loss: 0.26192317796604975
RMSE train: 0.450194	val: 0.718072	test: 0.637673
MAE train: 0.348907	val: 0.532074	test: 0.477381

Epoch: 139
Loss: 0.2570725775190762
RMSE train: 0.469225	val: 0.740106	test: 0.625110
MAE train: 0.367556	val: 0.542514	test: 0.471459

Epoch: 140
Loss: 0.2508521250316075
RMSE train: 0.449045	val: 0.726561	test: 0.607298
MAE train: 0.349221	val: 0.536389	test: 0.446806

Epoch: 141
Loss: 0.25764424140964237
RMSE train: 0.474384	val: 0.770921	test: 0.641943
MAE train: 0.369424	val: 0.561267	test: 0.485687

Epoch: 142
Loss: 0.2737088224717549
RMSE train: 0.462193	val: 0.714829	test: 0.618534
MAE train: 0.359875	val: 0.535113	test: 0.465449

Epoch: 143
Loss: 0.2529715576342174
RMSE train: 0.482182	val: 0.752733	test: 0.636313
MAE train: 0.374977	val: 0.554205	test: 0.480420
MAE train: 0.419504	val: 0.570397	test: 0.504707

Epoch: 84
Loss: 0.3384184879916055
RMSE train: 0.540638	val: 0.777412	test: 0.693202
MAE train: 0.416556	val: 0.577515	test: 0.518460

Epoch: 85
Loss: 0.3383129451956068
RMSE train: 0.523499	val: 0.768932	test: 0.670804
MAE train: 0.405466	val: 0.563452	test: 0.493188

Epoch: 86
Loss: 0.31001042893954683
RMSE train: 0.510972	val: 0.751357	test: 0.654303
MAE train: 0.397285	val: 0.553678	test: 0.495880

Epoch: 87
Loss: 0.33238889276981354
RMSE train: 0.563425	val: 0.796706	test: 0.707563
MAE train: 0.438640	val: 0.594266	test: 0.521234

Epoch: 88
Loss: 0.3242544893707548
RMSE train: 0.511716	val: 0.764739	test: 0.677344
MAE train: 0.394402	val: 0.564460	test: 0.503241

Epoch: 89
Loss: 0.33741974192006247
RMSE train: 0.521110	val: 0.754793	test: 0.686569
MAE train: 0.403479	val: 0.552318	test: 0.507549

Epoch: 90
Loss: 0.33087227812835146
RMSE train: 0.539526	val: 0.777533	test: 0.686887
MAE train: 0.421213	val: 0.574302	test: 0.519448

Epoch: 91
Loss: 0.33659182276044575
RMSE train: 0.524434	val: 0.774769	test: 0.675306
MAE train: 0.403980	val: 0.574877	test: 0.504569

Epoch: 92
Loss: 0.3174209786312921
RMSE train: 0.503115	val: 0.767732	test: 0.666366
MAE train: 0.390864	val: 0.554679	test: 0.489626

Epoch: 93
Loss: 0.3207658390913691
RMSE train: 0.539600	val: 0.778979	test: 0.683850
MAE train: 0.418250	val: 0.580310	test: 0.505701

Epoch: 94
Loss: 0.3259369603225163
RMSE train: 0.518475	val: 0.765532	test: 0.669882
MAE train: 0.402943	val: 0.562810	test: 0.501259

Epoch: 95
Loss: 0.31124705501965116
RMSE train: 0.513678	val: 0.753822	test: 0.678593
MAE train: 0.397758	val: 0.561920	test: 0.506273

Epoch: 96
Loss: 0.3197763306753976
RMSE train: 0.519112	val: 0.761003	test: 0.684685
MAE train: 0.403252	val: 0.565519	test: 0.507034

Epoch: 97
Loss: 0.3146304317883083
RMSE train: 0.496563	val: 0.736317	test: 0.663166
MAE train: 0.384747	val: 0.544996	test: 0.492240

Epoch: 98
Loss: 0.29578305248703274
RMSE train: 0.507924	val: 0.744539	test: 0.652753
MAE train: 0.394104	val: 0.554736	test: 0.485738

Epoch: 99
Loss: 0.3077755996159145
RMSE train: 0.531462	val: 0.763347	test: 0.696847
MAE train: 0.409709	val: 0.565884	test: 0.521390

Epoch: 100
Loss: 0.30994317148412975
RMSE train: 0.522070	val: 0.759866	test: 0.675387
MAE train: 0.405840	val: 0.561472	test: 0.504153

Epoch: 101
Loss: 0.31597247081143515
RMSE train: 0.501676	val: 0.758643	test: 0.660338
MAE train: 0.388543	val: 0.557332	test: 0.482203

Epoch: 102
Loss: 0.33001184463500977
RMSE train: 0.497790	val: 0.739321	test: 0.662789
MAE train: 0.388163	val: 0.550845	test: 0.493014

Epoch: 103
Loss: 0.30582078226975035
RMSE train: 0.495992	val: 0.743645	test: 0.644373
MAE train: 0.386032	val: 0.551355	test: 0.481713

Epoch: 104
Loss: 0.29875415776457104
RMSE train: 0.516434	val: 0.767757	test: 0.679157
MAE train: 0.400800	val: 0.567497	test: 0.512276

Epoch: 105
Loss: 0.2922499914254461
RMSE train: 0.506717	val: 0.770915	test: 0.678716
MAE train: 0.393025	val: 0.571649	test: 0.511560

Epoch: 106
Loss: 0.31718975411994116
RMSE train: 0.532595	val: 0.772537	test: 0.685290
MAE train: 0.412878	val: 0.571944	test: 0.513432

Epoch: 107
Loss: 0.313503982765334
RMSE train: 0.508564	val: 0.752354	test: 0.676214
MAE train: 0.392954	val: 0.556963	test: 0.503694

Epoch: 108
Loss: 0.31408535901989254
RMSE train: 0.507577	val: 0.764659	test: 0.665304
MAE train: 0.395830	val: 0.565490	test: 0.499315

Epoch: 109
Loss: 0.29737416548388346
RMSE train: 0.510013	val: 0.763222	test: 0.682611
MAE train: 0.395782	val: 0.567870	test: 0.509541

Epoch: 110
Loss: 0.28156972357204985
RMSE train: 0.533779	val: 0.786593	test: 0.678091
MAE train: 0.415845	val: 0.579253	test: 0.514291

Epoch: 111
Loss: 0.2939233034849167
RMSE train: 0.500689	val: 0.762288	test: 0.661234
MAE train: 0.388610	val: 0.556125	test: 0.496422

Epoch: 112
Loss: 0.29208128580025267
RMSE train: 0.509410	val: 0.763335	test: 0.655116
MAE train: 0.398938	val: 0.565867	test: 0.493451

Epoch: 113
Loss: 0.2972476077931268
RMSE train: 0.494052	val: 0.749886	test: 0.650834
MAE train: 0.384190	val: 0.562189	test: 0.490772

Epoch: 114
Loss: 0.2988949886390141
RMSE train: 0.502310	val: 0.754509	test: 0.667597
MAE train: 0.392736	val: 0.563591	test: 0.499363

Epoch: 115
Loss: 0.30458831787109375
RMSE train: 0.498732	val: 0.758425	test: 0.652819
MAE train: 0.386382	val: 0.568288	test: 0.493395

Epoch: 116
Loss: 0.2808622771075794
RMSE train: 0.498222	val: 0.747352	test: 0.671268
MAE train: 0.386211	val: 0.555996	test: 0.498538

Epoch: 117
Loss: 0.28457173172916683
RMSE train: 0.503457	val: 0.758696	test: 0.678448
MAE train: 0.390831	val: 0.557265	test: 0.499226

Epoch: 118
Loss: 0.3077615999749729
RMSE train: 0.490387	val: 0.742896	test: 0.652863
MAE train: 0.381066	val: 0.559953	test: 0.491828

Epoch: 119
Loss: 0.28710065143448965
RMSE train: 0.503804	val: 0.752330	test: 0.675712
MAE train: 0.392452	val: 0.561027	test: 0.505015

Epoch: 120
Loss: 0.2911552478160177
RMSE train: 0.507302	val: 0.760498	test: 0.678231
MAE train: 0.392839	val: 0.567691	test: 0.504428

Epoch: 121
Loss: 0.2849956452846527
RMSE train: 0.492152	val: 0.751207	test: 0.654864
MAE train: 0.382680	val: 0.563106	test: 0.485931

Epoch: 122
Loss: 0.27587310544082094
RMSE train: 0.474431	val: 0.742235	test: 0.640244
MAE train: 0.372154	val: 0.556836	test: 0.483942

Epoch: 123
Loss: 0.2681740075349808
RMSE train: 0.496008	val: 0.762374	test: 0.680059
MAE train: 0.386846	val: 0.566767	test: 0.512012

Epoch: 124
Loss: 0.2894428404314177
RMSE train: 0.489771	val: 0.758073	test: 0.653001
MAE train: 0.383594	val: 0.566163	test: 0.489793

Epoch: 125
Loss: 0.27378470663513454
RMSE train: 0.508972	val: 0.765062	test: 0.688451
MAE train: 0.396672	val: 0.571619	test: 0.518462

Epoch: 126
Loss: 0.2632023649556296
RMSE train: 0.482071	val: 0.750973	test: 0.652826
MAE train: 0.375602	val: 0.558684	test: 0.484696

Epoch: 127
Loss: 0.2810102828911373
RMSE train: 0.477023	val: 0.746321	test: 0.654251
MAE train: 0.373718	val: 0.554842	test: 0.491137

Epoch: 128
Loss: 0.2769530032362257
RMSE train: 0.506688	val: 0.757415	test: 0.671063
MAE train: 0.391265	val: 0.570441	test: 0.503156

Epoch: 129
Loss: 0.27291377633810043
RMSE train: 0.464498	val: 0.727335	test: 0.627143
MAE train: 0.361666	val: 0.546881	test: 0.469414

Epoch: 130
Loss: 0.2666533461638859
RMSE train: 0.520424	val: 0.771853	test: 0.689615
MAE train: 0.404218	val: 0.578139	test: 0.521120

Epoch: 131
Loss: 0.2808980867266655
RMSE train: 0.468116	val: 0.738744	test: 0.653278
MAE train: 0.363373	val: 0.545361	test: 0.489813

Epoch: 132
Loss: 0.2640579715371132
RMSE train: 0.511412	val: 0.768030	test: 0.680035
MAE train: 0.396122	val: 0.569630	test: 0.499752

Epoch: 133
Loss: 0.28452606179884504
RMSE train: 0.473641	val: 0.771729	test: 0.651231
MAE train: 0.371269	val: 0.568877	test: 0.487165

Epoch: 134
Loss: 0.29604110441037584
RMSE train: 0.489832	val: 0.751497	test: 0.661569
MAE train: 0.383667	val: 0.568129	test: 0.490411

Epoch: 135
Loss: 0.27270803174802233
RMSE train: 0.506950	val: 0.758486	test: 0.679727
MAE train: 0.393755	val: 0.573372	test: 0.513867

Epoch: 136
Loss: 0.259492610182081
RMSE train: 0.475212	val: 0.736477	test: 0.638065
MAE train: 0.370925	val: 0.552124	test: 0.485304

Epoch: 137
Loss: 0.27466609541858944
RMSE train: 0.467587	val: 0.743235	test: 0.644864
MAE train: 0.362750	val: 0.549353	test: 0.484213

Epoch: 138
Loss: 0.25787496886083056
RMSE train: 0.495472	val: 0.762241	test: 0.675805
MAE train: 0.387099	val: 0.569498	test: 0.503049

Epoch: 139
Loss: 0.2729315108486584
RMSE train: 0.464624	val: 0.736098	test: 0.648790
MAE train: 0.361132	val: 0.555670	test: 0.485677

Epoch: 140
Loss: 0.2611901621733393
RMSE train: 0.488133	val: 0.754906	test: 0.683193
MAE train: 0.377609	val: 0.566993	test: 0.511384

Epoch: 141
Loss: 0.2739397957921028
RMSE train: 0.461629	val: 0.727486	test: 0.650054
MAE train: 0.361379	val: 0.545076	test: 0.481395

Epoch: 142
Loss: 0.26315657049417496
RMSE train: 0.465981	val: 0.734548	test: 0.653945
MAE train: 0.362964	val: 0.553710	test: 0.493000

Epoch: 143
Loss: 0.2605927980371884
RMSE train: 0.496572	val: 0.756068	test: 0.674941
MAE train: 0.387455	val: 0.569660	test: 0.504921

Epoch: 144
Loss: 0.2250160053372383
RMSE train: 0.431159	val: 0.668087	test: 0.683753
MAE train: 0.336863	val: 0.498669	test: 0.518182

Epoch: 145
Loss: 0.23870596587657927
RMSE train: 0.447459	val: 0.679036	test: 0.689519
MAE train: 0.351698	val: 0.499974	test: 0.523554

Epoch: 146
Loss: 0.22487439066171647
RMSE train: 0.448827	val: 0.690319	test: 0.694281
MAE train: 0.350500	val: 0.504933	test: 0.528083

Epoch: 147
Loss: 0.22499047517776488
RMSE train: 0.420672	val: 0.673361	test: 0.703351
MAE train: 0.328063	val: 0.486527	test: 0.520028

Epoch: 148
Loss: 0.2218003585934639
RMSE train: 0.431939	val: 0.689811	test: 0.692209
MAE train: 0.339750	val: 0.501780	test: 0.516122

Epoch: 149
Loss: 0.2173751711845398
RMSE train: 0.447240	val: 0.697760	test: 0.698721
MAE train: 0.352119	val: 0.511592	test: 0.526354

Epoch: 150
Loss: 0.2291798010468483
RMSE train: 0.421325	val: 0.669005	test: 0.680381
MAE train: 0.330277	val: 0.490523	test: 0.511668

Epoch: 151
Loss: 0.22619192749261857
RMSE train: 0.430602	val: 0.679407	test: 0.680739
MAE train: 0.339490	val: 0.495199	test: 0.510193

Epoch: 152
Loss: 0.21791809052228928
RMSE train: 0.441602	val: 0.695014	test: 0.703019
MAE train: 0.346182	val: 0.505158	test: 0.532646

Epoch: 153
Loss: 0.22327516376972198
RMSE train: 0.427262	val: 0.684927	test: 0.693742
MAE train: 0.334223	val: 0.496769	test: 0.517882

Epoch: 154
Loss: 0.22411234825849533
RMSE train: 0.425239	val: 0.681137	test: 0.697579
MAE train: 0.332431	val: 0.492710	test: 0.518157

Epoch: 155
Loss: 0.212662909924984
RMSE train: 0.426057	val: 0.688693	test: 0.695210
MAE train: 0.334753	val: 0.498816	test: 0.518547

Epoch: 156
Loss: 0.22180372178554536
RMSE train: 0.436857	val: 0.683357	test: 0.694461
MAE train: 0.344019	val: 0.500717	test: 0.522814

Epoch: 157
Loss: 0.22257161885499954
RMSE train: 0.407933	val: 0.681608	test: 0.683618
MAE train: 0.317980	val: 0.491536	test: 0.516954

Epoch: 158
Loss: 0.2138618513941765
RMSE train: 0.425899	val: 0.680220	test: 0.683443
MAE train: 0.334086	val: 0.495455	test: 0.517535

Epoch: 159
Loss: 0.21491767168045045
RMSE train: 0.467277	val: 0.714021	test: 0.715683
MAE train: 0.367357	val: 0.522072	test: 0.542813

Epoch: 160
Loss: 0.20446344763040541
RMSE train: 0.418831	val: 0.676037	test: 0.696919
MAE train: 0.329208	val: 0.494754	test: 0.522443

Epoch: 161
Loss: 0.21743052899837495
RMSE train: 0.418430	val: 0.688395	test: 0.693901
MAE train: 0.328515	val: 0.500033	test: 0.515505

Epoch: 162
Loss: 0.20535468310117722
RMSE train: 0.450124	val: 0.699546	test: 0.706341
MAE train: 0.352640	val: 0.504867	test: 0.529964

Epoch: 163
Loss: 0.2094743773341179
RMSE train: 0.424327	val: 0.671957	test: 0.679601
MAE train: 0.333753	val: 0.486553	test: 0.508995

Epoch: 164
Loss: 0.21454065144062043
RMSE train: 0.417284	val: 0.670466	test: 0.680685
MAE train: 0.325641	val: 0.485241	test: 0.512705

Epoch: 165
Loss: 0.20442687273025512
RMSE train: 0.429070	val: 0.684871	test: 0.693364
MAE train: 0.337943	val: 0.498375	test: 0.521826

Epoch: 166
Loss: 0.20639918595552445
RMSE train: 0.408913	val: 0.675593	test: 0.695861
MAE train: 0.318388	val: 0.487062	test: 0.518978

Epoch: 167
Loss: 0.20369454771280288
RMSE train: 0.419257	val: 0.675199	test: 0.692299
MAE train: 0.329475	val: 0.488269	test: 0.520922

Epoch: 168
Loss: 0.19370207637548448
RMSE train: 0.405550	val: 0.676454	test: 0.684457
MAE train: 0.316911	val: 0.485274	test: 0.515282

Epoch: 169
Loss: 0.2075878769159317
RMSE train: 0.419259	val: 0.685338	test: 0.691075
MAE train: 0.328542	val: 0.496843	test: 0.523771

Epoch: 170
Loss: 0.19889491498470308
RMSE train: 0.430741	val: 0.690824	test: 0.688710
MAE train: 0.338822	val: 0.498298	test: 0.520908

Epoch: 171
Loss: 0.21380440890789032
RMSE train: 0.448032	val: 0.694121	test: 0.699653
MAE train: 0.352789	val: 0.506773	test: 0.531218

Epoch: 172
Loss: 0.2150859996676445
RMSE train: 0.429843	val: 0.687826	test: 0.697462
MAE train: 0.336811	val: 0.500513	test: 0.525231

Epoch: 173
Loss: 0.20548576563596727
RMSE train: 0.398863	val: 0.686601	test: 0.690988
MAE train: 0.312644	val: 0.496097	test: 0.523530

Epoch: 174
Loss: 0.19684171974658965
RMSE train: 0.418979	val: 0.670363	test: 0.677599
MAE train: 0.327925	val: 0.489088	test: 0.511978

Epoch: 175
Loss: 0.1978926107287407
RMSE train: 0.428129	val: 0.686754	test: 0.693578
MAE train: 0.333352	val: 0.498566	test: 0.523793

Epoch: 176
Loss: 0.20010699480772018
RMSE train: 0.390355	val: 0.678933	test: 0.679574
MAE train: 0.304962	val: 0.488776	test: 0.505034

Epoch: 177
Loss: 0.20833771973848342
RMSE train: 0.439788	val: 0.693918	test: 0.702577
MAE train: 0.346055	val: 0.505317	test: 0.531089

Epoch: 178
Loss: 0.19233148247003556
RMSE train: 0.408598	val: 0.681868	test: 0.690154
MAE train: 0.321527	val: 0.495117	test: 0.516439

Epoch: 179
Loss: 0.19929669350385665
RMSE train: 0.406654	val: 0.687068	test: 0.694892
MAE train: 0.317980	val: 0.498162	test: 0.517707

Early stopping
Best (RMSE):	 train: 0.431159	val: 0.668087	test: 0.683753
Best (MAE):	 train: 0.336863	val: 0.498669	test: 0.518182


Epoch: 144
Loss: 0.22288489043712617
RMSE train: 0.420461	val: 0.676732	test: 0.692838
MAE train: 0.327576	val: 0.495833	test: 0.522662

Epoch: 145
Loss: 0.2429679110646248
RMSE train: 0.449845	val: 0.686636	test: 0.695971
MAE train: 0.353327	val: 0.501735	test: 0.524265

Epoch: 146
Loss: 0.24086877852678298
RMSE train: 0.439894	val: 0.679916	test: 0.699357
MAE train: 0.342305	val: 0.496376	test: 0.523138

Epoch: 147
Loss: 0.22438482642173768
RMSE train: 0.429460	val: 0.683179	test: 0.696902
MAE train: 0.337906	val: 0.496271	test: 0.517701

Epoch: 148
Loss: 0.22992873638868333
RMSE train: 0.434794	val: 0.687572	test: 0.692183
MAE train: 0.339158	val: 0.500855	test: 0.524222

Epoch: 149
Loss: 0.23581450879573823
RMSE train: 0.459838	val: 0.693312	test: 0.701316
MAE train: 0.359763	val: 0.506288	test: 0.526915

Epoch: 150
Loss: 0.23365651816129684
RMSE train: 0.423385	val: 0.679982	test: 0.686524
MAE train: 0.330789	val: 0.493789	test: 0.516293

Epoch: 151
Loss: 0.22423246651887893
RMSE train: 0.434666	val: 0.674110	test: 0.688966
MAE train: 0.341502	val: 0.486611	test: 0.513611

Epoch: 152
Loss: 0.22314362972974777
RMSE train: 0.444103	val: 0.686610	test: 0.706146
MAE train: 0.347435	val: 0.499358	test: 0.527472

Epoch: 153
Loss: 0.21870527267456055
RMSE train: 0.440554	val: 0.684631	test: 0.712854
MAE train: 0.344743	val: 0.498392	test: 0.529664

Epoch: 154
Loss: 0.21372989416122437
RMSE train: 0.451083	val: 0.687761	test: 0.703893
MAE train: 0.354494	val: 0.502185	test: 0.524821

Epoch: 155
Loss: 0.21596550643444062
RMSE train: 0.443145	val: 0.678565	test: 0.703490
MAE train: 0.346043	val: 0.495666	test: 0.524347

Epoch: 156
Loss: 0.21727874279022216
RMSE train: 0.416626	val: 0.670805	test: 0.682743
MAE train: 0.326591	val: 0.488844	test: 0.506824

Epoch: 157
Loss: 0.21920603662729263
RMSE train: 0.428105	val: 0.670780	test: 0.686414
MAE train: 0.334770	val: 0.489534	test: 0.519127

Epoch: 158
Loss: 0.21562402695417404
RMSE train: 0.445821	val: 0.683942	test: 0.697778
MAE train: 0.351092	val: 0.496664	test: 0.522422

Epoch: 159
Loss: 0.20863090455532074
RMSE train: 0.437760	val: 0.691232	test: 0.695533
MAE train: 0.343212	val: 0.502996	test: 0.523029

Epoch: 160
Loss: 0.21401723325252534
RMSE train: 0.412015	val: 0.676140	test: 0.689201
MAE train: 0.319014	val: 0.491416	test: 0.516658

Epoch: 161
Loss: 0.2162204399704933
RMSE train: 0.410363	val: 0.672349	test: 0.677013
MAE train: 0.321064	val: 0.490375	test: 0.506655

Epoch: 162
Loss: 0.21314399242401122
RMSE train: 0.447732	val: 0.697154	test: 0.699454
MAE train: 0.348236	val: 0.513147	test: 0.528971

Epoch: 163
Loss: 0.21424198150634766
RMSE train: 0.430438	val: 0.690462	test: 0.696822
MAE train: 0.336455	val: 0.504345	test: 0.518828

Epoch: 164
Loss: 0.2108326554298401
RMSE train: 0.418207	val: 0.676194	test: 0.684993
MAE train: 0.325230	val: 0.493870	test: 0.513039

Epoch: 165
Loss: 0.2110143080353737
RMSE train: 0.413276	val: 0.669502	test: 0.688303
MAE train: 0.322037	val: 0.486393	test: 0.515654

Epoch: 166
Loss: 0.21236180514097214
RMSE train: 0.420848	val: 0.674953	test: 0.687972
MAE train: 0.329062	val: 0.491921	test: 0.518660

Epoch: 167
Loss: 0.2133850410580635
RMSE train: 0.404814	val: 0.674437	test: 0.685499
MAE train: 0.315992	val: 0.491266	test: 0.514913

Epoch: 168
Loss: 0.2050633907318115
RMSE train: 0.409356	val: 0.673254	test: 0.681126
MAE train: 0.318860	val: 0.489992	test: 0.510592

Epoch: 169
Loss: 0.20202149003744124
RMSE train: 0.415564	val: 0.672941	test: 0.693204
MAE train: 0.322980	val: 0.495064	test: 0.524859

Epoch: 170
Loss: 0.20047223418951035
RMSE train: 0.431347	val: 0.674901	test: 0.694649
MAE train: 0.338761	val: 0.493275	test: 0.522042

Epoch: 171
Loss: 0.20068764835596084
RMSE train: 0.420990	val: 0.674097	test: 0.696282
MAE train: 0.329662	val: 0.490752	test: 0.522489

Epoch: 172
Loss: 0.20353273749351503
RMSE train: 0.429466	val: 0.684019	test: 0.699542
MAE train: 0.336903	val: 0.499659	test: 0.525703

Epoch: 173
Loss: 0.2025132730603218
RMSE train: 0.396053	val: 0.674790	test: 0.671278
MAE train: 0.308065	val: 0.490607	test: 0.502484

Epoch: 174
Loss: 0.2031085580587387
RMSE train: 0.407458	val: 0.669478	test: 0.684194
MAE train: 0.320038	val: 0.491878	test: 0.512732

Epoch: 175
Loss: 0.19943934231996535
RMSE train: 0.401785	val: 0.662081	test: 0.681764
MAE train: 0.314149	val: 0.487484	test: 0.511756

Epoch: 176
Loss: 0.19761633574962617
RMSE train: 0.408837	val: 0.668849	test: 0.684843
MAE train: 0.321032	val: 0.488467	test: 0.514803

Epoch: 177
Loss: 0.1957804962992668
RMSE train: 0.418416	val: 0.671965	test: 0.694698
MAE train: 0.328012	val: 0.492402	test: 0.520911

Epoch: 178
Loss: 0.20241260081529616
RMSE train: 0.396830	val: 0.673946	test: 0.682225
MAE train: 0.307736	val: 0.489509	test: 0.510556

Epoch: 179
Loss: 0.19601079523563386
RMSE train: 0.419520	val: 0.684701	test: 0.695181
MAE train: 0.323890	val: 0.496467	test: 0.523260

Epoch: 180
Loss: 0.2000591591000557
RMSE train: 0.389637	val: 0.660553	test: 0.675569
MAE train: 0.304141	val: 0.481462	test: 0.505072

Epoch: 181
Loss: 0.19012544304132462
RMSE train: 0.410174	val: 0.680096	test: 0.690634
MAE train: 0.319251	val: 0.492383	test: 0.521495

Epoch: 182
Loss: 0.20057262480258942
RMSE train: 0.413781	val: 0.669489	test: 0.694193
MAE train: 0.322489	val: 0.490795	test: 0.519882

Epoch: 183
Loss: 0.1945083275437355
RMSE train: 0.460880	val: 0.701034	test: 0.730158
MAE train: 0.364619	val: 0.518839	test: 0.554911

Epoch: 184
Loss: 0.1939086362719536
RMSE train: 0.413610	val: 0.671315	test: 0.694526
MAE train: 0.322584	val: 0.491826	test: 0.521375

Epoch: 185
Loss: 0.18817782700061797
RMSE train: 0.413128	val: 0.692131	test: 0.701934
MAE train: 0.324599	val: 0.503383	test: 0.525149

Epoch: 186
Loss: 0.19065944403409957
RMSE train: 0.401414	val: 0.669564	test: 0.672267
MAE train: 0.313644	val: 0.486520	test: 0.504947

Epoch: 187
Loss: 0.19698797017335892
RMSE train: 0.379007	val: 0.661032	test: 0.670374
MAE train: 0.293501	val: 0.477843	test: 0.504712

Epoch: 188
Loss: 0.19804054647684097
RMSE train: 0.398470	val: 0.671631	test: 0.675656
MAE train: 0.310438	val: 0.488238	test: 0.502577

Epoch: 189
Loss: 0.18999614119529723
RMSE train: 0.395286	val: 0.676290	test: 0.688338
MAE train: 0.309436	val: 0.490963	test: 0.514465

Epoch: 190
Loss: 0.18644339591264725
RMSE train: 0.424851	val: 0.681998	test: 0.702761
MAE train: 0.331956	val: 0.498237	test: 0.529016

Epoch: 191
Loss: 0.19105168879032136
RMSE train: 0.392379	val: 0.676881	test: 0.688858
MAE train: 0.305925	val: 0.486822	test: 0.510782

Epoch: 192
Loss: 0.1866438403725624
RMSE train: 0.400121	val: 0.676815	test: 0.684206
MAE train: 0.312594	val: 0.490321	test: 0.515927

Epoch: 193
Loss: 0.19271623194217682
RMSE train: 0.374627	val: 0.672890	test: 0.685635
MAE train: 0.289222	val: 0.486160	test: 0.513981

Epoch: 194
Loss: 0.19016631692647934
RMSE train: 0.394173	val: 0.664996	test: 0.678258
MAE train: 0.306189	val: 0.481118	test: 0.504572

Epoch: 195
Loss: 0.18419188112020493
RMSE train: 0.403000	val: 0.673240	test: 0.682850
MAE train: 0.312572	val: 0.488107	test: 0.513092

Epoch: 196
Loss: 0.17911753803491592
RMSE train: 0.408398	val: 0.678890	test: 0.698134
MAE train: 0.315722	val: 0.492595	test: 0.525270

Epoch: 197
Loss: 0.18027567714452744
RMSE train: 0.387661	val: 0.674256	test: 0.679168
MAE train: 0.302569	val: 0.486685	test: 0.506799

Epoch: 198
Loss: 0.18426122665405273
RMSE train: 0.420926	val: 0.685427	test: 0.701035
MAE train: 0.328826	val: 0.500197	test: 0.526410

Epoch: 199
Loss: 0.18481598794460297
RMSE train: 0.380405	val: 0.663987	test: 0.673744
MAE train: 0.295416	val: 0.478765	test: 0.504005

Epoch: 200
Loss: 0.19659381210803986
RMSE train: 0.407432	val: 0.677856	test: 0.687492
MAE train: 0.320354	val: 0.493863	test: 0.515113

Epoch: 201
Loss: 0.19312373399734498
RMSE train: 0.401267	val: 0.678822	test: 0.691097
MAE train: 0.312716	val: 0.494153	test: 0.518483

Epoch: 202
Loss: 0.1766776517033577
RMSE train: 0.390299	val: 0.677493	test: 0.692311
MAE train: 0.302539	val: 0.490791	test: 0.520798

Epoch: 203
Loss: 0.17806115001440048
RMSE train: 0.396659	val: 0.671430	test: 0.685455
MAE train: 0.308740	val: 0.486181	test: 0.511239

Epoch: 204
Loss: 0.17889806926250457
RMSE train: 0.405721	val: 0.677154	test: 0.693754
MAE train: 0.316794	val: 0.492582	test: 0.516775

Epoch: 205
Loss: 0.17492670863866805
RMSE train: 0.387875	val: 0.677886	test: 0.683944
MAE train: 0.302491	val: 0.491713	test: 0.507352

Epoch: 206
Loss: 0.18352024108171464
RMSE train: 0.384456	val: 0.674561	test: 0.678473
MAE train: 0.300228	val: 0.492608	test: 0.508289

Epoch: 207
Loss: 0.1767647609114647
RMSE train: 0.378980	val: 0.674689	test: 0.682187
MAE train: 0.294530	val: 0.493100	test: 0.509229

Epoch: 208
Loss: 0.17795799523591996
RMSE train: 0.361713	val: 0.671456	test: 0.671607
MAE train: 0.280067	val: 0.484606	test: 0.497683

Epoch: 209
Loss: 0.17708527147769929
RMSE train: 0.397390	val: 0.688684	test: 0.694931
MAE train: 0.311592	val: 0.502228	test: 0.518895

Epoch: 210
Loss: 0.17641308158636093
RMSE train: 0.376257	val: 0.676250	test: 0.690821
MAE train: 0.294164	val: 0.490384	test: 0.515259

Epoch: 211
Loss: 0.16759268939495087
RMSE train: 0.377311	val: 0.669930	test: 0.681584
MAE train: 0.294338	val: 0.490266	test: 0.508655

Epoch: 212
Loss: 0.17431564927101134
RMSE train: 0.379932	val: 0.674903	test: 0.687077
MAE train: 0.297637	val: 0.491065	test: 0.513383

Epoch: 213
Loss: 0.1728869378566742
RMSE train: 0.371497	val: 0.669436	test: 0.678867
MAE train: 0.291690	val: 0.487115	test: 0.503088

Epoch: 214
Loss: 0.17378316074609756
RMSE train: 0.389191	val: 0.680233	test: 0.689275
MAE train: 0.304239	val: 0.491956	test: 0.516269

Epoch: 215
Loss: 0.17735718339681625
RMSE train: 0.356078	val: 0.672021	test: 0.676092
MAE train: 0.275373	val: 0.483739	test: 0.502900

Early stopping
Best (RMSE):	 train: 0.389637	val: 0.660553	test: 0.675569
Best (MAE):	 train: 0.304141	val: 0.481462	test: 0.505072
All runs completed.


Epoch: 144
Loss: 0.23405264814694723
RMSE train: 0.424433	val: 0.677563	test: 0.677535
MAE train: 0.331788	val: 0.490150	test: 0.506935

Epoch: 145
Loss: 0.22227329139908156
RMSE train: 0.459779	val: 0.703715	test: 0.705894
MAE train: 0.361588	val: 0.511394	test: 0.532482

Epoch: 146
Loss: 0.2285123902062575
RMSE train: 0.450046	val: 0.679180	test: 0.696833
MAE train: 0.352649	val: 0.496510	test: 0.519320

Epoch: 147
Loss: 0.2348510983089606
RMSE train: 0.431586	val: 0.672743	test: 0.670995
MAE train: 0.338926	val: 0.490661	test: 0.508709

Epoch: 148
Loss: 0.21736931055784225
RMSE train: 0.422829	val: 0.674359	test: 0.681794
MAE train: 0.329819	val: 0.491707	test: 0.506955

Epoch: 149
Loss: 0.22885318721334139
RMSE train: 0.460559	val: 0.694927	test: 0.710987
MAE train: 0.358791	val: 0.506411	test: 0.530860

Epoch: 150
Loss: 0.22443918387095133
RMSE train: 0.435758	val: 0.670031	test: 0.674615
MAE train: 0.343004	val: 0.486969	test: 0.506088

Epoch: 151
Loss: 0.21976855769753456
RMSE train: 0.426303	val: 0.670986	test: 0.678543
MAE train: 0.332547	val: 0.487088	test: 0.506090

Epoch: 152
Loss: 0.2161276638507843
RMSE train: 0.424074	val: 0.669677	test: 0.691243
MAE train: 0.330662	val: 0.487231	test: 0.516217

Epoch: 153
Loss: 0.22613746797045073
RMSE train: 0.433580	val: 0.670248	test: 0.688892
MAE train: 0.339234	val: 0.491731	test: 0.519298

Epoch: 154
Loss: 0.22243639454245567
RMSE train: 0.459287	val: 0.687493	test: 0.703496
MAE train: 0.359681	val: 0.499424	test: 0.527997

Epoch: 155
Loss: 0.22279570624232292
RMSE train: 0.412095	val: 0.670693	test: 0.672271
MAE train: 0.321334	val: 0.482029	test: 0.499789

Epoch: 156
Loss: 0.2238505519926548
RMSE train: 0.416573	val: 0.665542	test: 0.680583
MAE train: 0.324150	val: 0.484607	test: 0.508339

Epoch: 157
Loss: 0.22978906457622847
RMSE train: 0.414404	val: 0.671091	test: 0.679087
MAE train: 0.325473	val: 0.482450	test: 0.505810

Epoch: 158
Loss: 0.22241844361027083
RMSE train: 0.452614	val: 0.682581	test: 0.696462
MAE train: 0.355162	val: 0.495446	test: 0.520478

Epoch: 159
Loss: 0.22844759126504263
RMSE train: 0.401941	val: 0.655545	test: 0.662094
MAE train: 0.315193	val: 0.473317	test: 0.497147

Epoch: 160
Loss: 0.2161117730041345
RMSE train: 0.433660	val: 0.676829	test: 0.681993
MAE train: 0.340276	val: 0.485846	test: 0.509046

Epoch: 161
Loss: 0.21402805174390474
RMSE train: 0.434573	val: 0.674175	test: 0.693180
MAE train: 0.337593	val: 0.490890	test: 0.520623

Epoch: 162
Loss: 0.21134667098522186
RMSE train: 0.396783	val: 0.665659	test: 0.660839
MAE train: 0.308115	val: 0.482045	test: 0.490718

Epoch: 163
Loss: 0.2039287512501081
RMSE train: 0.427054	val: 0.679158	test: 0.673412
MAE train: 0.331026	val: 0.496864	test: 0.500790

Epoch: 164
Loss: 0.21470532938838005
RMSE train: 0.438353	val: 0.683850	test: 0.698391
MAE train: 0.343325	val: 0.497956	test: 0.519731

Epoch: 165
Loss: 0.21211733793218931
RMSE train: 0.438946	val: 0.679653	test: 0.695618
MAE train: 0.342832	val: 0.501915	test: 0.517473

Epoch: 166
Loss: 0.2061338114241759
RMSE train: 0.400664	val: 0.672086	test: 0.665787
MAE train: 0.312923	val: 0.485791	test: 0.494378

Epoch: 167
Loss: 0.20328358809153238
RMSE train: 0.467009	val: 0.697834	test: 0.705501
MAE train: 0.367270	val: 0.518882	test: 0.525279

Epoch: 168
Loss: 0.21330941220124564
RMSE train: 0.388578	val: 0.665910	test: 0.667287
MAE train: 0.303138	val: 0.483292	test: 0.493530

Epoch: 169
Loss: 0.20727863783637682
RMSE train: 0.418054	val: 0.668164	test: 0.681862
MAE train: 0.327141	val: 0.486681	test: 0.507594

Epoch: 170
Loss: 0.2085891825457414
RMSE train: 0.411232	val: 0.678362	test: 0.678169
MAE train: 0.321774	val: 0.494013	test: 0.506311

Epoch: 171
Loss: 0.21370534102121988
RMSE train: 0.408024	val: 0.669932	test: 0.674388
MAE train: 0.319989	val: 0.484577	test: 0.497766

Epoch: 172
Loss: 0.20223520447810492
RMSE train: 0.410970	val: 0.670480	test: 0.684815
MAE train: 0.319654	val: 0.490447	test: 0.511791

Epoch: 173
Loss: 0.2012061526377996
RMSE train: 0.396983	val: 0.660672	test: 0.675740
MAE train: 0.309725	val: 0.481941	test: 0.502012

Epoch: 174
Loss: 0.19804989794890085
RMSE train: 0.396114	val: 0.664347	test: 0.672719
MAE train: 0.309974	val: 0.478829	test: 0.500927

Epoch: 175
Loss: 0.2033127099275589
RMSE train: 0.421180	val: 0.678964	test: 0.696695
MAE train: 0.328740	val: 0.497260	test: 0.518851

Epoch: 176
Loss: 0.2006091152628263
RMSE train: 0.403867	val: 0.680120	test: 0.669259
MAE train: 0.315215	val: 0.489075	test: 0.496411

Epoch: 177
Loss: 0.20744279523690543
RMSE train: 0.389289	val: 0.668219	test: 0.675002
MAE train: 0.303168	val: 0.489535	test: 0.501002

Epoch: 178
Loss: 0.19862403348088264
RMSE train: 0.428159	val: 0.683876	test: 0.688377
MAE train: 0.334696	val: 0.497491	test: 0.518661

Epoch: 179
Loss: 0.20865076780319214
RMSE train: 0.406833	val: 0.675553	test: 0.677043
MAE train: 0.319019	val: 0.493409	test: 0.506578

Epoch: 180
Loss: 0.19180304060379663
RMSE train: 0.415254	val: 0.676948	test: 0.694478
MAE train: 0.325724	val: 0.491508	test: 0.517809

Epoch: 181
Loss: 0.18772058437267938
RMSE train: 0.384893	val: 0.657058	test: 0.677930
MAE train: 0.301670	val: 0.474397	test: 0.501715

Epoch: 182
Loss: 0.2087892765800158
RMSE train: 0.408809	val: 0.677569	test: 0.684063
MAE train: 0.318403	val: 0.489648	test: 0.507571

Epoch: 183
Loss: 0.19438111782073975
RMSE train: 0.405227	val: 0.684686	test: 0.684329
MAE train: 0.318830	val: 0.495050	test: 0.507733

Epoch: 184
Loss: 0.20511507987976074
RMSE train: 0.394266	val: 0.665589	test: 0.667384
MAE train: 0.309857	val: 0.480703	test: 0.497526

Epoch: 185
Loss: 0.19831335420409837
RMSE train: 0.405755	val: 0.673942	test: 0.675011
MAE train: 0.316291	val: 0.484821	test: 0.504226

Epoch: 186
Loss: 0.18632075935602188
RMSE train: 0.409593	val: 0.691950	test: 0.695247
MAE train: 0.320130	val: 0.499618	test: 0.516674

Epoch: 187
Loss: 0.18808881069223085
RMSE train: 0.379298	val: 0.673473	test: 0.665918
MAE train: 0.297318	val: 0.484651	test: 0.494527

Epoch: 188
Loss: 0.1909522165854772
RMSE train: 0.422098	val: 0.679342	test: 0.685364
MAE train: 0.330484	val: 0.486285	test: 0.512910

Epoch: 189
Loss: 0.19153825441996256
RMSE train: 0.405080	val: 0.670924	test: 0.690942
MAE train: 0.316826	val: 0.485859	test: 0.514312

Epoch: 190
Loss: 0.19372968624035516
RMSE train: 0.386439	val: 0.667764	test: 0.672291
MAE train: 0.301593	val: 0.485693	test: 0.503400

Epoch: 191
Loss: 0.18841121594111124
RMSE train: 0.421806	val: 0.669342	test: 0.692518
MAE train: 0.331393	val: 0.493661	test: 0.519637

Epoch: 192
Loss: 0.18893781552712122
RMSE train: 0.380374	val: 0.660768	test: 0.651324
MAE train: 0.295931	val: 0.472225	test: 0.489464

Epoch: 193
Loss: 0.18958870197335878
RMSE train: 0.389631	val: 0.669810	test: 0.665633
MAE train: 0.303492	val: 0.480935	test: 0.498974

Epoch: 194
Loss: 0.19092382490634918
RMSE train: 0.405020	val: 0.671681	test: 0.684039
MAE train: 0.316066	val: 0.490199	test: 0.511510

Early stopping
Best (RMSE):	 train: 0.401941	val: 0.655545	test: 0.662094
Best (MAE):	 train: 0.315193	val: 0.473317	test: 0.497147


Epoch: 144
Loss: 0.23613087212045988
RMSE train: 0.441350	val: 0.681941	test: 0.666063
MAE train: 0.345915	val: 0.504352	test: 0.505118

Epoch: 145
Loss: 0.22999685009320578
RMSE train: 0.429188	val: 0.690896	test: 0.669495
MAE train: 0.334991	val: 0.499381	test: 0.505009

Epoch: 146
Loss: 0.22552045434713364
RMSE train: 0.459787	val: 0.703501	test: 0.689921
MAE train: 0.360161	val: 0.507998	test: 0.522636

Epoch: 147
Loss: 0.2283139502008756
RMSE train: 0.441400	val: 0.688384	test: 0.667037
MAE train: 0.347798	val: 0.506598	test: 0.513444

Epoch: 148
Loss: 0.21550641457239786
RMSE train: 0.435033	val: 0.706498	test: 0.670189
MAE train: 0.338929	val: 0.504311	test: 0.503632

Epoch: 149
Loss: 0.22255714734395346
RMSE train: 0.423482	val: 0.697770	test: 0.657144
MAE train: 0.331110	val: 0.495955	test: 0.495433

Epoch: 150
Loss: 0.2246658056974411
RMSE train: 0.439763	val: 0.700789	test: 0.669665
MAE train: 0.342867	val: 0.501847	test: 0.507818

Epoch: 151
Loss: 0.22764060273766518
RMSE train: 0.428099	val: 0.694559	test: 0.666479
MAE train: 0.331936	val: 0.506397	test: 0.504787

Epoch: 152
Loss: 0.20096850767731667
RMSE train: 0.428241	val: 0.693612	test: 0.660513
MAE train: 0.334896	val: 0.497910	test: 0.506525

Epoch: 153
Loss: 0.2159111239016056
RMSE train: 0.430104	val: 0.714930	test: 0.670531
MAE train: 0.335834	val: 0.509770	test: 0.504797

Epoch: 154
Loss: 0.22205782557527223
RMSE train: 0.426353	val: 0.690060	test: 0.660245
MAE train: 0.335321	val: 0.494869	test: 0.503467

Epoch: 155
Loss: 0.2251216396689415
RMSE train: 0.430436	val: 0.690505	test: 0.667922
MAE train: 0.336529	val: 0.510739	test: 0.507914

Epoch: 156
Loss: 0.20811318730314574
RMSE train: 0.422391	val: 0.697152	test: 0.671386
MAE train: 0.330209	val: 0.516350	test: 0.510207

Epoch: 157
Loss: 0.212489303201437
RMSE train: 0.437988	val: 0.694339	test: 0.676278
MAE train: 0.342011	val: 0.502443	test: 0.514421

Epoch: 158
Loss: 0.21398299063245454
RMSE train: 0.430638	val: 0.691991	test: 0.661745
MAE train: 0.335775	val: 0.499213	test: 0.498610

Epoch: 159
Loss: 0.21115906412402788
RMSE train: 0.416575	val: 0.689266	test: 0.657736
MAE train: 0.326022	val: 0.502761	test: 0.493077

Epoch: 160
Loss: 0.2166555499037107
RMSE train: 0.425714	val: 0.693773	test: 0.666061
MAE train: 0.332871	val: 0.510998	test: 0.503531

Epoch: 161
Loss: 0.20373023549715677
RMSE train: 0.440451	val: 0.691745	test: 0.673592
MAE train: 0.346620	val: 0.506495	test: 0.510005

Epoch: 162
Loss: 0.21942864482601485
RMSE train: 0.402491	val: 0.684212	test: 0.651492
MAE train: 0.315045	val: 0.495337	test: 0.489661

Epoch: 163
Loss: 0.20849920436739922
RMSE train: 0.466642	val: 0.718880	test: 0.690481
MAE train: 0.364867	val: 0.517832	test: 0.522434

Epoch: 164
Loss: 0.21113922074437141
RMSE train: 0.421458	val: 0.688868	test: 0.666399
MAE train: 0.330071	val: 0.498225	test: 0.502361

Epoch: 165
Loss: 0.20739335815111795
RMSE train: 0.408926	val: 0.683035	test: 0.665637
MAE train: 0.318550	val: 0.491662	test: 0.499263

Epoch: 166
Loss: 0.21854853505889574
RMSE train: 0.428064	val: 0.687625	test: 0.678439
MAE train: 0.335254	val: 0.508238	test: 0.508473

Epoch: 167
Loss: 0.2135885792473952
RMSE train: 0.410659	val: 0.691488	test: 0.672877
MAE train: 0.319670	val: 0.507104	test: 0.503209

Epoch: 168
Loss: 0.20864134902755418
RMSE train: 0.418106	val: 0.685670	test: 0.669836
MAE train: 0.326884	val: 0.503650	test: 0.504876

Epoch: 169
Loss: 0.2109502467016379
RMSE train: 0.436440	val: 0.696343	test: 0.674383
MAE train: 0.340228	val: 0.506715	test: 0.506616

Epoch: 170
Loss: 0.1998068206012249
RMSE train: 0.404568	val: 0.688567	test: 0.660642
MAE train: 0.314526	val: 0.494419	test: 0.489970

Epoch: 171
Loss: 0.2038715419669946
RMSE train: 0.412966	val: 0.685335	test: 0.669938
MAE train: 0.322888	val: 0.499904	test: 0.507943

Epoch: 172
Loss: 0.20083209251364073
RMSE train: 0.394055	val: 0.685968	test: 0.657401
MAE train: 0.307996	val: 0.495587	test: 0.492161

Epoch: 173
Loss: 0.19627320021390915
RMSE train: 0.425429	val: 0.698938	test: 0.664174
MAE train: 0.332292	val: 0.498066	test: 0.499135

Epoch: 174
Loss: 0.20023082196712494
RMSE train: 0.389111	val: 0.666711	test: 0.644428
MAE train: 0.304198	val: 0.477350	test: 0.484001

Epoch: 175
Loss: 0.1919813131292661
RMSE train: 0.428817	val: 0.705631	test: 0.661766
MAE train: 0.337731	val: 0.518186	test: 0.502851

Epoch: 176
Loss: 0.21153147518634796
RMSE train: 0.413137	val: 0.696164	test: 0.659023
MAE train: 0.321285	val: 0.502731	test: 0.494498

Epoch: 177
Loss: 0.1960448200503985
RMSE train: 0.405321	val: 0.695891	test: 0.657271
MAE train: 0.318385	val: 0.506868	test: 0.491129

Epoch: 178
Loss: 0.19363509739438692
RMSE train: 0.410531	val: 0.693377	test: 0.664517
MAE train: 0.321675	val: 0.501900	test: 0.497311

Epoch: 179
Loss: 0.19174742077787718
RMSE train: 0.414575	val: 0.685137	test: 0.664496
MAE train: 0.326323	val: 0.496349	test: 0.501518

Epoch: 180
Loss: 0.19430595263838768
RMSE train: 0.394153	val: 0.675852	test: 0.655511
MAE train: 0.309113	val: 0.484562	test: 0.489134

Epoch: 181
Loss: 0.18953784803549448
RMSE train: 0.405031	val: 0.679894	test: 0.657363
MAE train: 0.318169	val: 0.491595	test: 0.495574

Epoch: 182
Loss: 0.2032132955888907
RMSE train: 0.420227	val: 0.687900	test: 0.656036
MAE train: 0.329953	val: 0.499804	test: 0.493150

Epoch: 183
Loss: 0.19447369997700056
RMSE train: 0.398408	val: 0.686260	test: 0.654980
MAE train: 0.310053	val: 0.499641	test: 0.489868

Epoch: 184
Loss: 0.19538628309965134
RMSE train: 0.391045	val: 0.695700	test: 0.652003
MAE train: 0.305105	val: 0.496731	test: 0.487009

Epoch: 185
Loss: 0.19281122336784998
RMSE train: 0.385410	val: 0.673785	test: 0.646779
MAE train: 0.299873	val: 0.488020	test: 0.482363

Epoch: 186
Loss: 0.19471663484970728
RMSE train: 0.405249	val: 0.692215	test: 0.655861
MAE train: 0.315454	val: 0.501918	test: 0.489906

Epoch: 187
Loss: 0.19586308424671492
RMSE train: 0.413487	val: 0.691342	test: 0.663813
MAE train: 0.323912	val: 0.500458	test: 0.495045

Epoch: 188
Loss: 0.2039831317961216
RMSE train: 0.402768	val: 0.679926	test: 0.662410
MAE train: 0.317051	val: 0.492722	test: 0.503255

Epoch: 189
Loss: 0.19470898310343424
RMSE train: 0.403293	val: 0.685385	test: 0.663639
MAE train: 0.317109	val: 0.498491	test: 0.496893

Epoch: 190
Loss: 0.18330874542395273
RMSE train: 0.394274	val: 0.698793	test: 0.657532
MAE train: 0.309076	val: 0.505577	test: 0.493651

Epoch: 191
Loss: 0.18673405051231384
RMSE train: 0.422509	val: 0.699641	test: 0.682840
MAE train: 0.331278	val: 0.506760	test: 0.511710

Epoch: 192
Loss: 0.19126520678400993
RMSE train: 0.372129	val: 0.681961	test: 0.641993
MAE train: 0.291390	val: 0.485257	test: 0.480228

Epoch: 193
Loss: 0.18765156716108322
RMSE train: 0.400713	val: 0.689431	test: 0.663029
MAE train: 0.311658	val: 0.503470	test: 0.497289

Epoch: 194
Loss: 0.18968462447325388
RMSE train: 0.407855	val: 0.697439	test: 0.665002
MAE train: 0.321120	val: 0.504550	test: 0.494826

Epoch: 195
Loss: 0.19983982294797897
RMSE train: 0.397417	val: 0.683285	test: 0.665143
MAE train: 0.310436	val: 0.490685	test: 0.501523

Epoch: 196
Loss: 0.18999281153082848
RMSE train: 0.387297	val: 0.671651	test: 0.648633
MAE train: 0.304758	val: 0.487008	test: 0.488149

Epoch: 197
Loss: 0.18688505391279855
RMSE train: 0.384499	val: 0.686871	test: 0.659570
MAE train: 0.301313	val: 0.490378	test: 0.492370

Epoch: 198
Loss: 0.18628784020741782
RMSE train: 0.400728	val: 0.700875	test: 0.673803
MAE train: 0.311663	val: 0.506476	test: 0.504093

Epoch: 199
Loss: 0.1902115816871325
RMSE train: 0.385034	val: 0.687513	test: 0.659949
MAE train: 0.301078	val: 0.494387	test: 0.495815

Epoch: 200
Loss: 0.18714132780830064
RMSE train: 0.377060	val: 0.685543	test: 0.659677
MAE train: 0.294476	val: 0.494447	test: 0.492081

Epoch: 201
Loss: 0.1795709878206253
RMSE train: 0.387380	val: 0.691520	test: 0.662238
MAE train: 0.303845	val: 0.489597	test: 0.489666

Epoch: 202
Loss: 0.17763405044873556
RMSE train: 0.386041	val: 0.709094	test: 0.665727
MAE train: 0.303789	val: 0.504104	test: 0.495072

Epoch: 203
Loss: 0.1959325075149536
RMSE train: 0.401557	val: 0.699800	test: 0.661053
MAE train: 0.314236	val: 0.503746	test: 0.497474

Epoch: 204
Loss: 0.18851711973547935
RMSE train: 0.399282	val: 0.690112	test: 0.667079
MAE train: 0.310988	val: 0.497643	test: 0.497100

Epoch: 205
Loss: 0.17471961180369058
RMSE train: 0.378978	val: 0.692398	test: 0.660334
MAE train: 0.297308	val: 0.495427	test: 0.494748

Epoch: 206
Loss: 0.1778924080232779
RMSE train: 0.395408	val: 0.700017	test: 0.663947
MAE train: 0.309022	val: 0.499329	test: 0.498924

Epoch: 207
Loss: 0.18058985968430838
RMSE train: 0.391766	val: 0.698116	test: 0.667511
MAE train: 0.307359	val: 0.495077	test: 0.504714

Epoch: 208
Loss: 0.18143113081653914
RMSE train: 0.377057	val: 0.673170	test: 0.657062
MAE train: 0.297440	val: 0.490972	test: 0.494724

Epoch: 209
Loss: 0.18892193958163261
RMSE train: 0.402767	val: 0.692647	test: 0.669131
MAE train: 0.315666	val: 0.500008	test: 0.502386

Early stopping
Best (RMSE):	 train: 0.389111	val: 0.666711	test: 0.644428
Best (MAE):	 train: 0.304198	val: 0.477350	test: 0.484001
All runs completed.


Epoch: 144
Loss: 0.26090373524597715
RMSE train: 0.468071	val: 0.729728	test: 0.628861
MAE train: 0.365335	val: 0.540568	test: 0.466787

Epoch: 145
Loss: 0.27073576194899424
RMSE train: 0.458146	val: 0.738614	test: 0.627628
MAE train: 0.356097	val: 0.536764	test: 0.470113

Epoch: 146
Loss: 0.24440753566367285
RMSE train: 0.476938	val: 0.761339	test: 0.639482
MAE train: 0.370080	val: 0.550768	test: 0.480430

Epoch: 147
Loss: 0.24768315255641937
RMSE train: 0.439028	val: 0.748390	test: 0.609877
MAE train: 0.339152	val: 0.531200	test: 0.450812

Epoch: 148
Loss: 0.2714814149907657
RMSE train: 0.485253	val: 0.747514	test: 0.636687
MAE train: 0.378336	val: 0.552707	test: 0.475148

Epoch: 149
Loss: 0.24452234378882817
RMSE train: 0.464186	val: 0.755644	test: 0.634878
MAE train: 0.363480	val: 0.551752	test: 0.470774

Epoch: 150
Loss: 0.24721018437828338
RMSE train: 0.435365	val: 0.730874	test: 0.625086
MAE train: 0.338557	val: 0.540045	test: 0.458105

Epoch: 151
Loss: 0.2395468151995114
RMSE train: 0.460475	val: 0.742355	test: 0.636180
MAE train: 0.359598	val: 0.545659	test: 0.466905

Epoch: 152
Loss: 0.24853022609438216
RMSE train: 0.450682	val: 0.745165	test: 0.617835
MAE train: 0.348990	val: 0.546602	test: 0.459944

Epoch: 153
Loss: 0.2539621176464217
RMSE train: 0.458877	val: 0.749906	test: 0.622005
MAE train: 0.356709	val: 0.548824	test: 0.464229

Epoch: 154
Loss: 0.2606396494167192
RMSE train: 0.492132	val: 0.762841	test: 0.644042
MAE train: 0.386133	val: 0.562353	test: 0.484383

Epoch: 155
Loss: 0.2503092235752514
RMSE train: 0.457321	val: 0.727074	test: 0.632004
MAE train: 0.356874	val: 0.532545	test: 0.471151

Epoch: 156
Loss: 0.25908381385462625
RMSE train: 0.455081	val: 0.736324	test: 0.611967
MAE train: 0.355003	val: 0.537016	test: 0.451246

Epoch: 157
Loss: 0.26122052861111505
RMSE train: 0.451149	val: 0.741908	test: 0.609663
MAE train: 0.350312	val: 0.540429	test: 0.449619

Epoch: 158
Loss: 0.2502197889345033
RMSE train: 0.468439	val: 0.747641	test: 0.654504
MAE train: 0.363200	val: 0.554565	test: 0.476974

Epoch: 159
Loss: 0.24662930518388748
RMSE train: 0.446563	val: 0.710794	test: 0.618311
MAE train: 0.347757	val: 0.537743	test: 0.459938

Epoch: 160
Loss: 0.2492239283663886
RMSE train: 0.483718	val: 0.743950	test: 0.656895
MAE train: 0.379480	val: 0.554278	test: 0.490215

Epoch: 161
Loss: 0.24467400567872183
RMSE train: 0.445435	val: 0.720358	test: 0.610031
MAE train: 0.345776	val: 0.530693	test: 0.460997

Epoch: 162
Loss: 0.23406840009348734
RMSE train: 0.450203	val: 0.712947	test: 0.633032
MAE train: 0.352564	val: 0.538115	test: 0.470172

Epoch: 163
Loss: 0.23250769398042134
RMSE train: 0.450638	val: 0.731215	test: 0.625031
MAE train: 0.353292	val: 0.542047	test: 0.460074

Epoch: 164
Loss: 0.22892729831593378
RMSE train: 0.458252	val: 0.759025	test: 0.623418
MAE train: 0.356673	val: 0.547036	test: 0.467656

Epoch: 165
Loss: 0.2416778260043689
RMSE train: 0.467197	val: 0.720884	test: 0.625876
MAE train: 0.364786	val: 0.548671	test: 0.467366

Epoch: 166
Loss: 0.24094826302358083
RMSE train: 0.439086	val: 0.725713	test: 0.622214
MAE train: 0.340829	val: 0.536317	test: 0.456901

Epoch: 167
Loss: 0.22219777213675634
RMSE train: 0.426550	val: 0.732476	test: 0.617438
MAE train: 0.329510	val: 0.531038	test: 0.455124

Epoch: 168
Loss: 0.23515490868261882
RMSE train: 0.437384	val: 0.733757	test: 0.612954
MAE train: 0.340411	val: 0.532861	test: 0.450153

Epoch: 169
Loss: 0.2464529914515359
RMSE train: 0.465854	val: 0.722383	test: 0.649523
MAE train: 0.363929	val: 0.538291	test: 0.477805

Epoch: 170
Loss: 0.23093838138239725
RMSE train: 0.444161	val: 0.726319	test: 0.637141
MAE train: 0.343790	val: 0.536258	test: 0.471701

Epoch: 171
Loss: 0.22283462435007095
RMSE train: 0.446208	val: 0.733448	test: 0.625247
MAE train: 0.346831	val: 0.541846	test: 0.461158

Epoch: 172
Loss: 0.22174106325422013
RMSE train: 0.452483	val: 0.738750	test: 0.632602
MAE train: 0.352120	val: 0.548143	test: 0.476075

Epoch: 173
Loss: 0.2250572783606393
RMSE train: 0.450135	val: 0.726874	test: 0.615237
MAE train: 0.353402	val: 0.534957	test: 0.461290

Epoch: 174
Loss: 0.2277353278228215
RMSE train: 0.466847	val: 0.752513	test: 0.644609
MAE train: 0.364395	val: 0.557554	test: 0.478019

Epoch: 175
Loss: 0.2223782560655049
RMSE train: 0.449182	val: 0.733329	test: 0.633540
MAE train: 0.350951	val: 0.544673	test: 0.474223

Epoch: 176
Loss: 0.22595376734222686
RMSE train: 0.431738	val: 0.759213	test: 0.627869
MAE train: 0.336092	val: 0.548172	test: 0.463397

Epoch: 177
Loss: 0.22113015289817536
RMSE train: 0.426140	val: 0.723369	test: 0.634365
MAE train: 0.330614	val: 0.538088	test: 0.467876

Epoch: 178
Loss: 0.2295429610780307
RMSE train: 0.421662	val: 0.722527	test: 0.620658
MAE train: 0.329040	val: 0.530082	test: 0.456832

Epoch: 179
Loss: 0.21983191690274648
RMSE train: 0.448492	val: 0.740244	test: 0.618631
MAE train: 0.350762	val: 0.544060	test: 0.460418

Epoch: 180
Loss: 0.22623607303415025
RMSE train: 0.437703	val: 0.748958	test: 0.630663
MAE train: 0.339392	val: 0.554023	test: 0.467785

Epoch: 181
Loss: 0.22296717017889023
RMSE train: 0.440883	val: 0.737348	test: 0.621589
MAE train: 0.342139	val: 0.549703	test: 0.460359

Epoch: 182
Loss: 0.22900765708514623
RMSE train: 0.440736	val: 0.727610	test: 0.627592
MAE train: 0.343012	val: 0.537490	test: 0.462335

Epoch: 183
Loss: 0.22910921382052557
RMSE train: 0.421130	val: 0.722830	test: 0.614865
MAE train: 0.327377	val: 0.526611	test: 0.457115

Epoch: 184
Loss: 0.2203465114746775
RMSE train: 0.453475	val: 0.729293	test: 0.647226
MAE train: 0.352536	val: 0.550158	test: 0.483093

Epoch: 185
Loss: 0.2176575841648238
RMSE train: 0.425201	val: 0.707423	test: 0.631758
MAE train: 0.330373	val: 0.536193	test: 0.467707

Epoch: 186
Loss: 0.21450437711817877
RMSE train: 0.453717	val: 0.733695	test: 0.634537
MAE train: 0.350910	val: 0.549940	test: 0.472675

Epoch: 187
Loss: 0.22865678795746394
RMSE train: 0.424449	val: 0.722795	test: 0.614615
MAE train: 0.330020	val: 0.538761	test: 0.456537

Epoch: 188
Loss: 0.21903532849890844
RMSE train: 0.427498	val: 0.700411	test: 0.625562
MAE train: 0.332153	val: 0.533906	test: 0.466249

Epoch: 189
Loss: 0.22203513767038072
RMSE train: 0.456903	val: 0.726303	test: 0.637240
MAE train: 0.355599	val: 0.548703	test: 0.477508

Epoch: 190
Loss: 0.20446644936289107
RMSE train: 0.436021	val: 0.742311	test: 0.636283
MAE train: 0.336347	val: 0.551072	test: 0.474869

Epoch: 191
Loss: 0.2270032029066767
RMSE train: 0.446055	val: 0.740272	test: 0.638620
MAE train: 0.347578	val: 0.542382	test: 0.475998

Epoch: 192
Loss: 0.21636307878153666
RMSE train: 0.409562	val: 0.716226	test: 0.628343
MAE train: 0.317127	val: 0.530972	test: 0.463291

Epoch: 193
Loss: 0.21632540545293263
RMSE train: 0.449860	val: 0.756718	test: 0.629754
MAE train: 0.350762	val: 0.554402	test: 0.472865

Epoch: 194
Loss: 0.21335442470652716
RMSE train: 0.427779	val: 0.745132	test: 0.619896
MAE train: 0.333112	val: 0.548350	test: 0.459487

Epoch: 195
Loss: 0.21284972769873484
RMSE train: 0.398990	val: 0.705130	test: 0.624539
MAE train: 0.307635	val: 0.529179	test: 0.457116

Epoch: 196
Loss: 0.20329989173582622
RMSE train: 0.436702	val: 0.732380	test: 0.644470
MAE train: 0.339717	val: 0.544880	test: 0.480136

Epoch: 197
Loss: 0.21587610031877244
RMSE train: 0.415872	val: 0.738982	test: 0.637817
MAE train: 0.321443	val: 0.544531	test: 0.475381

Epoch: 198
Loss: 0.21951767802238464
RMSE train: 0.421762	val: 0.715592	test: 0.618116
MAE train: 0.328039	val: 0.529208	test: 0.459177

Epoch: 199
Loss: 0.21397456846066884
RMSE train: 0.429617	val: 0.719517	test: 0.630165
MAE train: 0.334033	val: 0.538288	test: 0.470088

Epoch: 200
Loss: 0.2081551690186773
RMSE train: 0.412859	val: 0.688348	test: 0.624136
MAE train: 0.320788	val: 0.517072	test: 0.463666

Epoch: 201
Loss: 0.20692341668265207
RMSE train: 0.411282	val: 0.697761	test: 0.599602
MAE train: 0.320551	val: 0.523093	test: 0.447925

Epoch: 202
Loss: 0.198192937033517
RMSE train: 0.422364	val: 0.715514	test: 0.615681
MAE train: 0.331234	val: 0.529480	test: 0.457329

Epoch: 203
Loss: 0.20080282539129257
RMSE train: 0.436376	val: 0.719896	test: 0.633050
MAE train: 0.339903	val: 0.533168	test: 0.471341

Epoch: 144
Loss: 0.26879632685865673
RMSE train: 0.449966	val: 0.726643	test: 0.633210
MAE train: 0.349264	val: 0.545592	test: 0.473675

Epoch: 145
Loss: 0.25771633216312956
RMSE train: 0.469420	val: 0.744049	test: 0.668106
MAE train: 0.364943	val: 0.551226	test: 0.489407

Epoch: 146
Loss: 0.2625978227172579
RMSE train: 0.484257	val: 0.749430	test: 0.658891
MAE train: 0.378494	val: 0.567501	test: 0.499379

Epoch: 147
Loss: 0.2857079782656261
RMSE train: 0.468776	val: 0.728311	test: 0.633398
MAE train: 0.364390	val: 0.553203	test: 0.481905

Epoch: 148
Loss: 0.26002513830150875
RMSE train: 0.470540	val: 0.737778	test: 0.645764
MAE train: 0.366097	val: 0.551324	test: 0.493747

Epoch: 149
Loss: 0.24662102226700103
RMSE train: 0.454004	val: 0.730282	test: 0.625550
MAE train: 0.353895	val: 0.549438	test: 0.477283

Epoch: 150
Loss: 0.26501050272158216
RMSE train: 0.476045	val: 0.734639	test: 0.649640
MAE train: 0.372045	val: 0.564566	test: 0.488260

Epoch: 151
Loss: 0.2616324286375727
RMSE train: 0.460089	val: 0.736841	test: 0.629984
MAE train: 0.359201	val: 0.551272	test: 0.476995

Epoch: 152
Loss: 0.25600748668823925
RMSE train: 0.485327	val: 0.758138	test: 0.667435
MAE train: 0.378631	val: 0.568294	test: 0.499543

Epoch: 153
Loss: 0.2408189113651003
RMSE train: 0.459412	val: 0.739869	test: 0.629450
MAE train: 0.359157	val: 0.557017	test: 0.474356

Epoch: 154
Loss: 0.23426952958106995
RMSE train: 0.444639	val: 0.739827	test: 0.649313
MAE train: 0.345741	val: 0.550255	test: 0.484279

Epoch: 155
Loss: 0.24268743608679091
RMSE train: 0.445812	val: 0.745016	test: 0.643377
MAE train: 0.347987	val: 0.544545	test: 0.478159

Epoch: 156
Loss: 0.2322388184922082
RMSE train: 0.433664	val: 0.727338	test: 0.619840
MAE train: 0.338628	val: 0.535513	test: 0.462900

Epoch: 157
Loss: 0.2312727730189051
RMSE train: 0.427201	val: 0.714143	test: 0.618084
MAE train: 0.332096	val: 0.529168	test: 0.463593

Epoch: 158
Loss: 0.24271183354513987
RMSE train: 0.466650	val: 0.740116	test: 0.659520
MAE train: 0.361809	val: 0.550446	test: 0.496846

Epoch: 159
Loss: 0.262244245835713
RMSE train: 0.451449	val: 0.743439	test: 0.636575
MAE train: 0.350564	val: 0.550510	test: 0.479458

Epoch: 160
Loss: 0.2438246054308755
RMSE train: 0.450861	val: 0.737728	test: 0.647717
MAE train: 0.351397	val: 0.555854	test: 0.481695

Epoch: 161
Loss: 0.23989267434392655
RMSE train: 0.440858	val: 0.742184	test: 0.659212
MAE train: 0.343165	val: 0.548145	test: 0.487304

Epoch: 162
Loss: 0.2354389684540885
RMSE train: 0.462359	val: 0.734725	test: 0.668095
MAE train: 0.359539	val: 0.550524	test: 0.505280

Epoch: 163
Loss: 0.23883735069206782
RMSE train: 0.454341	val: 0.719068	test: 0.651175
MAE train: 0.353986	val: 0.534531	test: 0.487467

Epoch: 164
Loss: 0.22830116216625487
RMSE train: 0.467850	val: 0.735603	test: 0.656644
MAE train: 0.367051	val: 0.558677	test: 0.492995

Epoch: 165
Loss: 0.2503246281828199
RMSE train: 0.436377	val: 0.715424	test: 0.623876
MAE train: 0.340143	val: 0.537876	test: 0.473210

Epoch: 166
Loss: 0.24153312934296473
RMSE train: 0.430845	val: 0.719363	test: 0.632160
MAE train: 0.336034	val: 0.543415	test: 0.470552

Epoch: 167
Loss: 0.22767195744173868
RMSE train: 0.424061	val: 0.694927	test: 0.628133
MAE train: 0.331265	val: 0.523945	test: 0.465287

Epoch: 168
Loss: 0.21833879394190653
RMSE train: 0.441943	val: 0.730742	test: 0.654462
MAE train: 0.343169	val: 0.549333	test: 0.490183

Epoch: 169
Loss: 0.22168809814112528
RMSE train: 0.460807	val: 0.738401	test: 0.650666
MAE train: 0.359217	val: 0.555957	test: 0.494767

Epoch: 170
Loss: 0.23020868854863302
RMSE train: 0.447680	val: 0.728163	test: 0.656467
MAE train: 0.348817	val: 0.547402	test: 0.493120

Epoch: 171
Loss: 0.22365514295441763
RMSE train: 0.428927	val: 0.706511	test: 0.629472
MAE train: 0.335527	val: 0.531326	test: 0.473810

Epoch: 172
Loss: 0.2302426387156759
RMSE train: 0.441599	val: 0.716549	test: 0.635134
MAE train: 0.344469	val: 0.534163	test: 0.479393

Epoch: 173
Loss: 0.24144486976521357
RMSE train: 0.435641	val: 0.715554	test: 0.664309
MAE train: 0.337317	val: 0.530176	test: 0.493844

Epoch: 174
Loss: 0.23387142483677184
RMSE train: 0.433395	val: 0.727340	test: 0.641535
MAE train: 0.336586	val: 0.539310	test: 0.486735

Epoch: 175
Loss: 0.23897784416164672
RMSE train: 0.474829	val: 0.748269	test: 0.699486
MAE train: 0.368914	val: 0.561137	test: 0.526837

Epoch: 176
Loss: 0.23214996180364064
RMSE train: 0.409962	val: 0.708822	test: 0.633602
MAE train: 0.319366	val: 0.525566	test: 0.474672

Epoch: 177
Loss: 0.23179166870457785
RMSE train: 0.459410	val: 0.736226	test: 0.649393
MAE train: 0.358635	val: 0.550927	test: 0.493397

Epoch: 178
Loss: 0.2385544329881668
RMSE train: 0.442248	val: 0.715808	test: 0.645176
MAE train: 0.344898	val: 0.538964	test: 0.485205

Epoch: 179
Loss: 0.22728327448878968
RMSE train: 0.450192	val: 0.736999	test: 0.655816
MAE train: 0.350925	val: 0.544975	test: 0.484574

Epoch: 180
Loss: 0.23722645959683827
RMSE train: 0.436489	val: 0.730495	test: 0.629569
MAE train: 0.341078	val: 0.549140	test: 0.469940

Epoch: 181
Loss: 0.21114752335207804
RMSE train: 0.416185	val: 0.733433	test: 0.635768
MAE train: 0.323120	val: 0.544394	test: 0.470764

Epoch: 182
Loss: 0.22261388280561992
RMSE train: 0.475048	val: 0.754286	test: 0.680477
MAE train: 0.371552	val: 0.570406	test: 0.513306

Epoch: 183
Loss: 0.2111162937113217
RMSE train: 0.413585	val: 0.723276	test: 0.624162
MAE train: 0.321425	val: 0.535083	test: 0.463894

Epoch: 184
Loss: 0.2211963896240507
RMSE train: 0.447408	val: 0.719066	test: 0.664215
MAE train: 0.350916	val: 0.548993	test: 0.492614

Epoch: 185
Loss: 0.23737935083253042
RMSE train: 0.441487	val: 0.727707	test: 0.656439
MAE train: 0.340771	val: 0.542944	test: 0.498153

Epoch: 186
Loss: 0.22232678639037268
RMSE train: 0.419471	val: 0.711886	test: 0.630692
MAE train: 0.328358	val: 0.530300	test: 0.471820

Epoch: 187
Loss: 0.21597130596637726
RMSE train: 0.435665	val: 0.723771	test: 0.639308
MAE train: 0.338916	val: 0.546973	test: 0.483753

Epoch: 188
Loss: 0.21874977861131942
RMSE train: 0.434106	val: 0.728325	test: 0.647417
MAE train: 0.337741	val: 0.540876	test: 0.487359

Epoch: 189
Loss: 0.215678408741951
RMSE train: 0.422855	val: 0.716413	test: 0.638186
MAE train: 0.329503	val: 0.535324	test: 0.476427

Epoch: 190
Loss: 0.22701075566666468
RMSE train: 0.471488	val: 0.753291	test: 0.676671
MAE train: 0.370384	val: 0.568327	test: 0.511395

Epoch: 191
Loss: 0.20799436633075988
RMSE train: 0.435656	val: 0.746613	test: 0.655528
MAE train: 0.339426	val: 0.547442	test: 0.496082

Epoch: 192
Loss: 0.21227696112224034
RMSE train: 0.448627	val: 0.748867	test: 0.672618
MAE train: 0.350097	val: 0.555256	test: 0.503039

Epoch: 193
Loss: 0.21526037688766206
RMSE train: 0.452202	val: 0.753833	test: 0.681073
MAE train: 0.353983	val: 0.560871	test: 0.511378

Epoch: 194
Loss: 0.2061826107757432
RMSE train: 0.439689	val: 0.737035	test: 0.645496
MAE train: 0.345277	val: 0.549785	test: 0.488570

Epoch: 195
Loss: 0.20508361501353128
RMSE train: 0.449198	val: 0.728828	test: 0.651160
MAE train: 0.351710	val: 0.550606	test: 0.494053

Epoch: 196
Loss: 0.2069131593619074
RMSE train: 0.419192	val: 0.737826	test: 0.646204
MAE train: 0.326809	val: 0.548725	test: 0.489564

Epoch: 197
Loss: 0.20574614192758287
RMSE train: 0.432339	val: 0.741721	test: 0.646760
MAE train: 0.338523	val: 0.550261	test: 0.484438

Epoch: 198
Loss: 0.21047151514462062
RMSE train: 0.422906	val: 0.746924	test: 0.643543
MAE train: 0.328593	val: 0.551986	test: 0.477730

Epoch: 199
Loss: 0.23968476269926345
RMSE train: 0.438602	val: 0.726271	test: 0.651757
MAE train: 0.343630	val: 0.546598	test: 0.485774

Epoch: 200
Loss: 0.21200033277273178
RMSE train: 0.430969	val: 0.746432	test: 0.648328
MAE train: 0.335981	val: 0.551713	test: 0.493453

Epoch: 201
Loss: 0.20856096063341414
RMSE train: 0.423076	val: 0.739804	test: 0.650996
MAE train: 0.330113	val: 0.547371	test: 0.491286

Epoch: 202
Loss: 0.22286703224693025
RMSE train: 0.474480	val: 0.770671	test: 0.690218
MAE train: 0.374027	val: 0.571874	test: 0.528909

Early stopping
Best (RMSE):	 train: 0.424061	val: 0.694927	test: 0.628133
Best (MAE):	 train: 0.331265	val: 0.523945	test: 0.465287


Epoch: 204
Loss: 0.20290079819304602
RMSE train: 0.436496	val: 0.712205	test: 0.634035
MAE train: 0.339039	val: 0.532282	test: 0.472398

Epoch: 205
Loss: 0.20430958164589746
RMSE train: 0.402180	val: 0.696981	test: 0.612123
MAE train: 0.312634	val: 0.520709	test: 0.458946

Epoch: 206
Loss: 0.20639091304370336
RMSE train: 0.414279	val: 0.718982	test: 0.618218
MAE train: 0.322913	val: 0.534122	test: 0.461361

Epoch: 207
Loss: 0.19396004932267324
RMSE train: 0.412953	val: 0.714400	test: 0.608332
MAE train: 0.321976	val: 0.537532	test: 0.454948

Epoch: 208
Loss: 0.21616275502102716
RMSE train: 0.470614	val: 0.743130	test: 0.653841
MAE train: 0.370573	val: 0.557971	test: 0.492314

Epoch: 209
Loss: 0.19785321823188237
RMSE train: 0.402821	val: 0.694687	test: 0.619226
MAE train: 0.312123	val: 0.523878	test: 0.455129

Epoch: 210
Loss: 0.19676013184445246
RMSE train: 0.406588	val: 0.705210	test: 0.617042
MAE train: 0.315345	val: 0.525301	test: 0.454155

Epoch: 211
Loss: 0.19059421441384725
RMSE train: 0.409644	val: 0.718197	test: 0.600801
MAE train: 0.319928	val: 0.537672	test: 0.451324

Epoch: 212
Loss: 0.194867875959192
RMSE train: 0.421712	val: 0.722532	test: 0.637877
MAE train: 0.329221	val: 0.539309	test: 0.479711

Epoch: 213
Loss: 0.20471328922680446
RMSE train: 0.405641	val: 0.721284	test: 0.617046
MAE train: 0.316552	val: 0.534251	test: 0.458184

Epoch: 214
Loss: 0.20466562466961996
RMSE train: 0.400850	val: 0.723625	test: 0.617106
MAE train: 0.312447	val: 0.535989	test: 0.456532

Epoch: 215
Loss: 0.18779355713299342
RMSE train: 0.388467	val: 0.719725	test: 0.607083
MAE train: 0.302460	val: 0.532740	test: 0.451210

Epoch: 216
Loss: 0.2022416176540511
RMSE train: 0.410569	val: 0.722853	test: 0.604733
MAE train: 0.321591	val: 0.542208	test: 0.453821

Epoch: 217
Loss: 0.18900317592280252
RMSE train: 0.413035	val: 0.735489	test: 0.627033
MAE train: 0.320028	val: 0.541230	test: 0.470057

Epoch: 218
Loss: 0.20302347838878632
RMSE train: 0.420815	val: 0.730864	test: 0.615290
MAE train: 0.327824	val: 0.540868	test: 0.457906

Epoch: 219
Loss: 0.19387883586542948
RMSE train: 0.405934	val: 0.735221	test: 0.637612
MAE train: 0.318175	val: 0.538851	test: 0.470898

Epoch: 220
Loss: 0.18924010970762797
RMSE train: 0.396978	val: 0.718814	test: 0.609275
MAE train: 0.308410	val: 0.529660	test: 0.449840

Epoch: 221
Loss: 0.19545371617589677
RMSE train: 0.419508	val: 0.742498	test: 0.625282
MAE train: 0.326166	val: 0.545197	test: 0.468389

Epoch: 222
Loss: 0.1994006559252739
RMSE train: 0.392663	val: 0.741748	test: 0.613514
MAE train: 0.305427	val: 0.532068	test: 0.452434

Epoch: 223
Loss: 0.20552256171192443
RMSE train: 0.428324	val: 0.739240	test: 0.623927
MAE train: 0.336389	val: 0.540303	test: 0.463019

Epoch: 224
Loss: 0.2230630453143801
RMSE train: 0.408405	val: 0.699633	test: 0.607374
MAE train: 0.320139	val: 0.525818	test: 0.451809

Epoch: 225
Loss: 0.2098909808056695
RMSE train: 0.414364	val: 0.704740	test: 0.618984
MAE train: 0.322659	val: 0.530729	test: 0.457659

Epoch: 226
Loss: 0.20564281514712743
RMSE train: 0.435945	val: 0.738925	test: 0.611434
MAE train: 0.340943	val: 0.542295	test: 0.458778

Epoch: 227
Loss: 0.2046035443033491
RMSE train: 0.395220	val: 0.731900	test: 0.617192
MAE train: 0.307413	val: 0.537723	test: 0.457483

Epoch: 228
Loss: 0.1943753319127219
RMSE train: 0.392530	val: 0.735423	test: 0.602874
MAE train: 0.305817	val: 0.531457	test: 0.448444

Epoch: 229
Loss: 0.19843172601291112
RMSE train: 0.411709	val: 0.722944	test: 0.618041
MAE train: 0.320376	val: 0.535339	test: 0.462552

Epoch: 230
Loss: 0.1951770101274763
RMSE train: 0.419502	val: 0.737451	test: 0.630241
MAE train: 0.326112	val: 0.538552	test: 0.473393

Epoch: 231
Loss: 0.1987653842994145
RMSE train: 0.437723	val: 0.751459	test: 0.629991
MAE train: 0.339743	val: 0.547790	test: 0.474325

Epoch: 232
Loss: 0.1868167594075203
RMSE train: 0.410893	val: 0.736010	test: 0.618545
MAE train: 0.320255	val: 0.533785	test: 0.458717

Epoch: 233
Loss: 0.19336911610194615
RMSE train: 0.398362	val: 0.716787	test: 0.613557
MAE train: 0.310683	val: 0.522724	test: 0.458166

Epoch: 234
Loss: 0.1959452075617654
RMSE train: 0.457407	val: 0.766208	test: 0.656374
MAE train: 0.361602	val: 0.565192	test: 0.500366

Epoch: 235
Loss: 0.19637955831629889
RMSE train: 0.415755	val: 0.738756	test: 0.632244
MAE train: 0.323825	val: 0.541183	test: 0.475480

Early stopping
Best (RMSE):	 train: 0.412859	val: 0.688348	test: 0.624136
Best (MAE):	 train: 0.320788	val: 0.517072	test: 0.463666
All runs completed.
