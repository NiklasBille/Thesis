>>> Starting run for dataset: clintox
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running RANDOM configs_static_noise_experiments/GraphMVP/clintox/noise=0.0.yml on cuda:0
Running RANDOM configs_static_noise_experiments/GraphMVP/clintox/noise=0.05.yml on cuda:1
Running RANDOM configs_static_noise_experiments/GraphMVP/clintox/noise=0.1.yml on cuda:2
Running RANDOM configs_static_noise_experiments/GraphMVP/clintox/noise=0.2.yml on cuda:3
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphMVP/clintox/noise=0.2.yml --runseed 4 --device cuda:3
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphMVP/clintox/noise=0.0.yml --runseed 4 --device cuda:0
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphMVP/clintox/noise=0.2.yml --runseed 5 --device cuda:3
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphMVP/clintox/noise=0.0.yml --runseed 5 --device cuda:0
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphMVP/clintox/noise=0.2.yml --runseed 6 --device cuda:3
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphMVP/clintox/noise=0.0.yml --runseed 6 --device cuda:0
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphMVP/clintox/noise=0.05.yml --runseed 4 --device cuda:1
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphMVP/clintox/noise=0.05.yml --runseed 5 --device cuda:1
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphMVP/clintox/noise=0.05.yml --runseed 6 --device cuda:1
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphMVP/clintox/noise=0.1.yml --runseed 4 --device cuda:2
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphMVP/clintox/noise=0.1.yml --runseed 5 --device cuda:2
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphMVP/clintox/noise=0.1.yml --runseed 6 --device cuda:2
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/clintox/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: clintox
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/static-noise/GraphMVP/clintox/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/clintox/noise=0.0/clintox_scaff_6_26-05_11-18-00  ]
[ Using Seed :  6  ]
[ Using device :  cuda:0  ]
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.6446387580705981
ROC train: 0.616088	val: 0.796176	test: 0.488860
PRC train: 0.541795	val: 0.555206	test: 0.500362

Epoch: 2
Loss: 0.5689779829377681
ROC train: 0.685660	val: 0.830691	test: 0.502329
PRC train: 0.560067	val: 0.576736	test: 0.502616

Epoch: 3
Loss: 0.5123495030192544
ROC train: 0.730010	val: 0.822250	test: 0.491500
PRC train: 0.575293	val: 0.613021	test: 0.503538

Epoch: 4
Loss: 0.46632260928076263
ROC train: 0.771802	val: 0.860385	test: 0.529850
PRC train: 0.599192	val: 0.645276	test: 0.510624

Epoch: 5
Loss: 0.4262648258245905
ROC train: 0.783287	val: 0.835474	test: 0.523736
PRC train: 0.595945	val: 0.596305	test: 0.508515

Epoch: 6
Loss: 0.3915750818313942
ROC train: 0.809760	val: 0.864244	test: 0.555041
PRC train: 0.606514	val: 0.573833	test: 0.513447

Epoch: 7
Loss: 0.355712650747174
ROC train: 0.827175	val: 0.870812	test: 0.574797
PRC train: 0.617627	val: 0.573747	test: 0.518825

Epoch: 8
Loss: 0.32296870239263764
ROC train: 0.840175	val: 0.860860	test: 0.561944
PRC train: 0.629841	val: 0.575991	test: 0.518324

Epoch: 9
Loss: 0.3051948198253721
ROC train: 0.836356	val: 0.831229	test: 0.543962
PRC train: 0.622917	val: 0.575940	test: 0.513555

Epoch: 10
Loss: 0.27622360634146836
ROC train: 0.849315	val: 0.857950	test: 0.590512
PRC train: 0.645248	val: 0.577928	test: 0.523213

Epoch: 11
Loss: 0.26470248582721906
ROC train: 0.861978	val: 0.850208	test: 0.598382
PRC train: 0.671403	val: 0.574150	test: 0.527858

Epoch: 12
Loss: 0.2477987400228808
ROC train: 0.872023	val: 0.839645	test: 0.597399
PRC train: 0.683521	val: 0.644534	test: 0.523817

Epoch: 13
Loss: 0.23784843408460493
ROC train: 0.884875	val: 0.853480	test: 0.639123
PRC train: 0.704973	val: 0.656218	test: 0.536985

Epoch: 14
Loss: 0.22582243982260808
ROC train: 0.870686	val: 0.849984	test: 0.656129
PRC train: 0.711739	val: 0.647648	test: 0.538616

Epoch: 15
Loss: 0.2182395422936981
ROC train: 0.889325	val: 0.841068	test: 0.669759
PRC train: 0.726979	val: 0.659713	test: 0.544277

Epoch: 16
Loss: 0.2150814134952581
ROC train: 0.901121	val: 0.831366	test: 0.684887
PRC train: 0.745275	val: 0.615481	test: 0.551984

Epoch: 17
Loss: 0.2113922692475132
ROC train: 0.900644	val: 0.824598	test: 0.678067
PRC train: 0.737282	val: 0.605883	test: 0.548905

Epoch: 18
Loss: 0.19859791526633408
ROC train: 0.907505	val: 0.783328	test: 0.681540
PRC train: 0.746554	val: 0.614008	test: 0.557218

Epoch: 19
Loss: 0.19781897075115423
ROC train: 0.916232	val: 0.816544	test: 0.694838
PRC train: 0.766820	val: 0.609907	test: 0.560168

Epoch: 20
Loss: 0.18929046730714308
ROC train: 0.915591	val: 0.808377	test: 0.723387
PRC train: 0.768357	val: 0.655669	test: 0.570072

Epoch: 21
Loss: 0.1930201308584681
ROC train: 0.922996	val: 0.810225	test: 0.714874
PRC train: 0.774247	val: 0.661201	test: 0.574000

Epoch: 22
Loss: 0.18172741761617472
ROC train: 0.926872	val: 0.805369	test: 0.715098
PRC train: 0.785812	val: 0.648695	test: 0.571194

Epoch: 23
Loss: 0.17753083448926793
ROC train: 0.925017	val: 0.784066	test: 0.702163
PRC train: 0.782221	val: 0.640484	test: 0.561747

Epoch: 24
Loss: 0.17723444092111812
ROC train: 0.937111	val: 0.805130	test: 0.726035
PRC train: 0.795826	val: 0.655377	test: 0.576848

Epoch: 25
Loss: 0.18420420668655074
ROC train: 0.935661	val: 0.817331	test: 0.691021
PRC train: 0.797862	val: 0.659463	test: 0.563278

Epoch: 26
Loss: 0.17163952012444375
ROC train: 0.941170	val: 0.832852	test: 0.713343
PRC train: 0.797386	val: 0.641316	test: 0.582727

Epoch: 27
Loss: 0.17013032422481555
ROC train: 0.928860	val: 0.820690	test: 0.717302
PRC train: 0.787808	val: 0.612829	test: 0.575636

Epoch: 28
Loss: 0.17491970798024567
ROC train: 0.942656	val: 0.796478	test: 0.734578
PRC train: 0.815491	val: 0.619729	test: 0.596908

Epoch: 29
Loss: 0.1694137102949774
ROC train: 0.945393	val: 0.811525	test: 0.706141
PRC train: 0.813018	val: 0.625869	test: 0.612736

Epoch: 30
Loss: 0.17677050565887545
ROC train: 0.948153	val: 0.847074	test: 0.733379
PRC train: 0.809241	val: 0.667401	test: 0.584034

Epoch: 31
Loss: 0.1710622401062586
ROC train: 0.949698	val: 0.825336	test: 0.734903
PRC train: 0.815864	val: 0.597380	test: 0.587626

Epoch: 32
Loss: 0.163647246225234
ROC train: 0.951840	val: 0.840994	test: 0.747633
PRC train: 0.816396	val: 0.609693	test: 0.593795

Epoch: 33
Loss: 0.16594892514969067Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/clintox/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: clintox
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/static-noise/GraphMVP/clintox/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/clintox/noise=0.0/clintox_scaff_5_26-05_11-18-00  ]
[ Using Seed :  5  ]
[ Using device :  cuda:0  ]
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.672335581774792
ROC train: 0.618424	val: 0.690848	test: 0.438601
PRC train: 0.542542	val: 0.535817	test: 0.499304

Epoch: 2
Loss: 0.5948137069341773
ROC train: 0.682565	val: 0.791780	test: 0.463990
PRC train: 0.558573	val: 0.559493	test: 0.493677

Epoch: 3
Loss: 0.533551389817631
ROC train: 0.706892	val: 0.843651	test: 0.459893
PRC train: 0.565537	val: 0.564381	test: 0.489949

Epoch: 4
Loss: 0.4788470383401669
ROC train: 0.737779	val: 0.862919	test: 0.453486
PRC train: 0.574701	val: 0.572028	test: 0.492093

Epoch: 5
Loss: 0.438486889915582
ROC train: 0.774766	val: 0.883023	test: 0.502418
PRC train: 0.586109	val: 0.584413	test: 0.503942

Epoch: 6
Loss: 0.4000563905160104
ROC train: 0.791496	val: 0.881263	test: 0.518533
PRC train: 0.591280	val: 0.577941	test: 0.501706

Epoch: 7
Loss: 0.3638693759108917
ROC train: 0.816877	val: 0.882798	test: 0.530799
PRC train: 0.604928	val: 0.579694	test: 0.505558

Epoch: 8
Loss: 0.3367827888450618
ROC train: 0.828987	val: 0.870949	test: 0.525779
PRC train: 0.613234	val: 0.574092	test: 0.504628

Epoch: 9
Loss: 0.3154481330901932
ROC train: 0.838208	val: 0.848174	test: 0.521644
PRC train: 0.617937	val: 0.571065	test: 0.507590

Epoch: 10
Loss: 0.28855359126046426
ROC train: 0.853283	val: 0.849035	test: 0.566964
PRC train: 0.642667	val: 0.577120	test: 0.518841

Epoch: 11
Loss: 0.2651431880731582
ROC train: 0.856872	val: 0.849147	test: 0.585290
PRC train: 0.655687	val: 0.576781	test: 0.521270

Epoch: 12
Loss: 0.25347701331501427
ROC train: 0.863177	val: 0.853030	test: 0.588737
PRC train: 0.665114	val: 0.579096	test: 0.524023

Epoch: 13
Loss: 0.2441600350640448
ROC train: 0.873881	val: 0.839919	test: 0.619505
PRC train: 0.690532	val: 0.598689	test: 0.533475

Epoch: 14
Loss: 0.23013271674985622
ROC train: 0.893706	val: 0.820328	test: 0.629579
PRC train: 0.716137	val: 0.636119	test: 0.536880

Epoch: 15
Loss: 0.21772628332767469
ROC train: 0.897471	val: 0.817144	test: 0.629235
PRC train: 0.728672	val: 0.595754	test: 0.534828

Epoch: 16
Loss: 0.21053894670414564
ROC train: 0.894554	val: 0.827957	test: 0.636093
PRC train: 0.731585	val: 0.580436	test: 0.535500

Epoch: 17
Loss: 0.20686440569858466
ROC train: 0.901633	val: 0.846775	test: 0.653302
PRC train: 0.738956	val: 0.610063	test: 0.542539

Epoch: 18
Loss: 0.20979621965066647
ROC train: 0.901241	val: 0.832740	test: 0.656880
PRC train: 0.754467	val: 0.634572	test: 0.538422

Epoch: 19
Loss: 0.1920930486490303
ROC train: 0.909525	val: 0.865330	test: 0.649873
PRC train: 0.751873	val: 0.678458	test: 0.538400

Epoch: 20
Loss: 0.19297370312913906
ROC train: 0.909235	val: 0.865555	test: 0.636549
PRC train: 0.754314	val: 0.669170	test: 0.533704

Epoch: 21
Loss: 0.19852072135362456
ROC train: 0.914223	val: 0.826921	test: 0.633770
PRC train: 0.773913	val: 0.641574	test: 0.537044

Epoch: 22
Loss: 0.19098979296692584
ROC train: 0.922591	val: 0.865892	test: 0.678840
PRC train: 0.776916	val: 0.674041	test: 0.553209

Epoch: 23
Loss: 0.1835333404873689
ROC train: 0.929086	val: 0.859886	test: 0.687285
PRC train: 0.778036	val: 0.600034	test: 0.552158

Epoch: 24
Loss: 0.18449504523459195
ROC train: 0.926184	val: 0.849460	test: 0.678803
PRC train: 0.783916	val: 0.617375	test: 0.545981

Epoch: 25
Loss: 0.17431610695998973
ROC train: 0.921078	val: 0.867090	test: 0.678130
PRC train: 0.773194	val: 0.664093	test: 0.554423

Epoch: 26
Loss: 0.17959831405462565
ROC train: 0.935894	val: 0.869975	test: 0.666674
PRC train: 0.795163	val: 0.720481	test: 0.542246

Epoch: 27
Loss: 0.17870595271072565
ROC train: 0.933535	val: 0.861485	test: 0.681339
PRC train: 0.808238	val: 0.622926	test: 0.545762

Epoch: 28
Loss: 0.1799422173673931
ROC train: 0.931013	val: 0.908223	test: 0.723999
PRC train: 0.792901	val: 0.637348	test: 0.582426

Epoch: 29
Loss: 0.17405073661306247
ROC train: 0.942770	val: 0.896510	test: 0.693142
PRC train: 0.802887	val: 0.707265	test: 0.550950

Epoch: 30
Loss: 0.16566529369025823
ROC train: 0.947978	val: 0.862346	test: 0.690382
PRC train: 0.798188	val: 0.629489	test: 0.551171

Epoch: 31
Loss: 0.17602152239293642
ROC train: 0.951162	val: 0.849485	test: 0.693811
PRC train: 0.800985	val: 0.613536	test: 0.551518

Epoch: 32
Loss: 0.16251457904575486
ROC train: 0.953700	val: 0.876156	test: 0.709925
PRC train: 0.818205	val: 0.614553	test: 0.555664

Epoch: 33
Loss: 0.16712737141200026Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/clintox/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: clintox
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/static-noise/GraphMVP/clintox/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/clintox/noise=0.0/clintox_scaff_4_26-05_11-18-00  ]
[ Using Seed :  4  ]
[ Using device :  cuda:0  ]
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.6582952508905806
ROC train: 0.625981	val: 0.730213	test: 0.472820
PRC train: 0.552056	val: 0.528614	test: 0.509223

Epoch: 2
Loss: 0.585230151332648
ROC train: 0.701368	val: 0.844252	test: 0.510076
PRC train: 0.575544	val: 0.556263	test: 0.504485

Epoch: 3
Loss: 0.5239664071423795
ROC train: 0.726823	val: 0.860297	test: 0.509949
PRC train: 0.576489	val: 0.566044	test: 0.502230

Epoch: 4
Loss: 0.4720147859608711
ROC train: 0.760499	val: 0.874695	test: 0.529888
PRC train: 0.585708	val: 0.574616	test: 0.507537

Epoch: 5
Loss: 0.432251374374669
ROC train: 0.781702	val: 0.871448	test: 0.528278
PRC train: 0.590545	val: 0.573489	test: 0.507685

Epoch: 6
Loss: 0.3945833685414055
ROC train: 0.804267	val: 0.876343	test: 0.552624
PRC train: 0.600559	val: 0.573497	test: 0.512580

Epoch: 7
Loss: 0.36678702132980473
ROC train: 0.807874	val: 0.861197	test: 0.567614
PRC train: 0.600031	val: 0.564317	test: 0.517595

Epoch: 8
Loss: 0.33720224223580314
ROC train: 0.832339	val: 0.858723	test: 0.588252
PRC train: 0.615486	val: 0.571293	test: 0.529552

Epoch: 9
Loss: 0.3100366473937535
ROC train: 0.847578	val: 0.852742	test: 0.594348
PRC train: 0.636802	val: 0.568963	test: 0.530073

Epoch: 10
Loss: 0.2821923465744183
ROC train: 0.860434	val: 0.820950	test: 0.605090
PRC train: 0.658589	val: 0.569108	test: 0.536565

Epoch: 11
Loss: 0.2683799385615056
ROC train: 0.861241	val: 0.862532	test: 0.619867
PRC train: 0.665346	val: 0.569659	test: 0.536000

Epoch: 12
Loss: 0.25243191140167853
ROC train: 0.868458	val: 0.859823	test: 0.613409
PRC train: 0.672936	val: 0.567003	test: 0.534323

Epoch: 13
Loss: 0.23867235990250957
ROC train: 0.882065	val: 0.853255	test: 0.621455
PRC train: 0.704822	val: 0.575370	test: 0.536372

Epoch: 14
Loss: 0.2284904702770978
ROC train: 0.892622	val: 0.848834	test: 0.632448
PRC train: 0.719479	val: 0.574021	test: 0.537683

Epoch: 15
Loss: 0.22259455141559145
ROC train: 0.897518	val: 0.824011	test: 0.650885
PRC train: 0.735573	val: 0.554425	test: 0.541247

Epoch: 16
Loss: 0.21460500725770007
ROC train: 0.900781	val: 0.818592	test: 0.653208
PRC train: 0.747182	val: 0.552194	test: 0.538637

Epoch: 17
Loss: 0.21083809415859506
ROC train: 0.903205	val: 0.859935	test: 0.635957
PRC train: 0.744577	val: 0.572769	test: 0.534063

Epoch: 18
Loss: 0.19686948201111829
ROC train: 0.907579	val: 0.852155	test: 0.627412
PRC train: 0.748672	val: 0.569579	test: 0.532496

Epoch: 19
Loss: 0.19433872455165474
ROC train: 0.917582	val: 0.819952	test: 0.666749
PRC train: 0.765527	val: 0.584678	test: 0.545431

Epoch: 20
Loss: 0.19323729221964797
ROC train: 0.919749	val: 0.821825	test: 0.661714
PRC train: 0.772090	val: 0.663116	test: 0.545458

Epoch: 21
Loss: 0.1936502970072232
ROC train: 0.924944	val: 0.827669	test: 0.664000
PRC train: 0.775781	val: 0.607611	test: 0.546450

Epoch: 22
Loss: 0.18498694884113498
ROC train: 0.929676	val: 0.791970	test: 0.686112
PRC train: 0.777183	val: 0.609380	test: 0.553602

Epoch: 23
Loss: 0.18690207958229366
ROC train: 0.935577	val: 0.791608	test: 0.707859
PRC train: 0.804977	val: 0.600431	test: 0.552399

Epoch: 24
Loss: 0.17502335854241013
ROC train: 0.936631	val: 0.777822	test: 0.696979
PRC train: 0.808871	val: 0.589727	test: 0.548642

Epoch: 25
Loss: 0.17681894529297598
ROC train: 0.936365	val: 0.819390	test: 0.674175
PRC train: 0.804997	val: 0.650603	test: 0.541738

Epoch: 26
Loss: 0.17215238422046814
ROC train: 0.940700	val: 0.852581	test: 0.689982
PRC train: 0.793374	val: 0.620493	test: 0.551902

Epoch: 27
Loss: 0.16321384934738056
ROC train: 0.939401	val: 0.855402	test: 0.698065
PRC train: 0.790356	val: 0.591296	test: 0.553083

Epoch: 28
Loss: 0.16808133319195304
ROC train: 0.942830	val: 0.867877	test: 0.704625
PRC train: 0.809394	val: 0.591641	test: 0.552681

Epoch: 29
Loss: 0.1649672450497644
ROC train: 0.946370	val: 0.845226	test: 0.705349
PRC train: 0.820857	val: 0.613740	test: 0.554786

Epoch: 30
Loss: 0.16143255165751358
ROC train: 0.939162	val: 0.823136	test: 0.693781
PRC train: 0.812315	val: 0.584685	test: 0.550982

Epoch: 31
Loss: 0.17063294192533798
ROC train: 0.950328	val: 0.839694	test: 0.726472
PRC train: 0.830465	val: 0.592039	test: 0.561960

Epoch: 32
Loss: 0.15343681230893366
ROC train: 0.951666	val: 0.866865	test: 0.751256
PRC train: 0.830948	val: 0.610968	test: 0.574753

Epoch: 33
Loss: 0.15739450899288293Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/clintox/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: clintox
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/static-noise/GraphMVP/clintox/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/clintox/noise=0.05/clintox_scaff_4_26-05_11-18-00  ]
[ Using Seed :  4  ]
[ Using device :  cuda:1  ]
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.6589161059286794
ROC train: 0.611318	val: 0.751034	test: 0.476253
PRC train: 0.544122	val: 0.544377	test: 0.501337

Epoch: 2
Loss: 0.5850733605947898
ROC train: 0.664450	val: 0.820813	test: 0.505224
PRC train: 0.560356	val: 0.554876	test: 0.500046

Epoch: 3
Loss: 0.5311743308347154
ROC train: 0.684876	val: 0.827581	test: 0.494545
PRC train: 0.573334	val: 0.558850	test: 0.496944

Epoch: 4
Loss: 0.4789528199955611
ROC train: 0.711614	val: 0.839607	test: 0.502478
PRC train: 0.579520	val: 0.561886	test: 0.499916

Epoch: 5
Loss: 0.4407524872963086
ROC train: 0.751283	val: 0.870000	test: 0.540093
PRC train: 0.593552	val: 0.573319	test: 0.509525

Epoch: 6
Loss: 0.40187970684230157
ROC train: 0.788074	val: 0.875756	test: 0.554746
PRC train: 0.605955	val: 0.577513	test: 0.515401

Epoch: 7
Loss: 0.36475977129336323
ROC train: 0.807094	val: 0.873521	test: 0.563278
PRC train: 0.614244	val: 0.583244	test: 0.516888

Epoch: 8
Loss: 0.3407057698554067
ROC train: 0.823811	val: 0.895386	test: 0.565108
PRC train: 0.624264	val: 0.585040	test: 0.518897

Epoch: 9
Loss: 0.3122301850628918
ROC train: 0.849010	val: 0.893650	test: 0.586108
PRC train: 0.641105	val: 0.583416	test: 0.527777

Epoch: 10
Loss: 0.2834184327971303
ROC train: 0.872063	val: 0.885933	test: 0.581117
PRC train: 0.670318	val: 0.582915	test: 0.526966

Epoch: 11
Loss: 0.2659610610745962
ROC train: 0.886174	val: 0.880763	test: 0.584165
PRC train: 0.702218	val: 0.577530	test: 0.527471

Epoch: 12
Loss: 0.2450983423748471
ROC train: 0.897441	val: 0.890241	test: 0.592446
PRC train: 0.740417	val: 0.585329	test: 0.527848

Epoch: 13
Loss: 0.23290740901931167
ROC train: 0.905717	val: 0.884422	test: 0.625194
PRC train: 0.757044	val: 0.582007	test: 0.535301

Epoch: 14
Loss: 0.2169245880677712
ROC train: 0.901869	val: 0.871174	test: 0.644599
PRC train: 0.776435	val: 0.575136	test: 0.541179

Epoch: 15
Loss: 0.20057530666696813
ROC train: 0.915626	val: 0.858263	test: 0.670757
PRC train: 0.807618	val: 0.575983	test: 0.548418

Epoch: 16
Loss: 0.20052814703974278
ROC train: 0.933575	val: 0.862058	test: 0.696579
PRC train: 0.839087	val: 0.578098	test: 0.560361

Epoch: 17
Loss: 0.1887059373949947
ROC train: 0.946231	val: 0.841979	test: 0.720851
PRC train: 0.848850	val: 0.578143	test: 0.566393

Epoch: 18
Loss: 0.1816446964061646
ROC train: 0.951075	val: 0.833626	test: 0.689022
PRC train: 0.844111	val: 0.576249	test: 0.554592

Epoch: 19
Loss: 0.17496224023569265
ROC train: 0.966147	val: 0.803208	test: 0.712196
PRC train: 0.896654	val: 0.562974	test: 0.572325

Epoch: 20
Loss: 0.17831249816142355
ROC train: 0.956914	val: 0.826857	test: 0.719902
PRC train: 0.855244	val: 0.566936	test: 0.559700

Epoch: 21
Loss: 0.16013702193548932
ROC train: 0.958430	val: 0.803882	test: 0.695316
PRC train: 0.883384	val: 0.545893	test: 0.549584

Epoch: 22
Loss: 0.15230173492520452
ROC train: 0.968316	val: 0.870000	test: 0.728945
PRC train: 0.897326	val: 0.569792	test: 0.561989

Epoch: 23
Loss: 0.15118598513094494
ROC train: 0.971151	val: 0.869575	test: 0.730208
PRC train: 0.912666	val: 0.575452	test: 0.562396

Epoch: 24
Loss: 0.14375983653570648
ROC train: 0.978236	val: 0.813560	test: 0.742863
PRC train: 0.930406	val: 0.558240	test: 0.569066

Epoch: 25
Loss: 0.14072865378103522
ROC train: 0.984263	val: 0.769156	test: 0.767258
PRC train: 0.942658	val: 0.545287	test: 0.576539

Epoch: 26
Loss: 0.11858723782001739
ROC train: 0.989641	val: 0.666576	test: 0.710508
PRC train: 0.951130	val: 0.525523	test: 0.548734

Epoch: 27
Loss: 0.1342013631932673
ROC train: 0.988136	val: 0.735716	test: 0.724086
PRC train: 0.951261	val: 0.537962	test: 0.556129

Epoch: 28
Loss: 0.11647214532296964
ROC train: 0.988465	val: 0.752573	test: 0.785284
PRC train: 0.955377	val: 0.555019	test: 0.591829

Epoch: 29
Loss: 0.10776909457226888
ROC train: 0.990259	val: 0.729398	test: 0.775864
PRC train: 0.960084	val: 0.542764	test: 0.573289

Epoch: 30
Loss: 0.1147949786119109
ROC train: 0.990375	val: 0.736827	test: 0.781949
PRC train: 0.959307	val: 0.555057	test: 0.604654

Epoch: 31
Loss: 0.10380729081774473
ROC train: 0.989203	val: 0.687092	test: 0.743188
PRC train: 0.963167	val: 0.533279	test: 0.594124

Epoch: 32
Loss: 0.10641731513060756
ROC train: 0.993046	val: 0.709206	test: 0.809668Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/clintox/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:3
  input_data_dir: 
  dataset: clintox
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/static-noise/GraphMVP/clintox/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/clintox/noise=0.2/clintox_scaff_4_26-05_11-18-00  ]
[ Using Seed :  4  ]
[ Using device :  cuda:3  ]
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.6611427949326589
ROC train: 0.607542	val: 0.735850	test: 0.460550
PRC train: 0.537008	val: 0.540862	test: 0.501632

Epoch: 2
Loss: 0.5949240547263173
ROC train: 0.642922	val: 0.760251	test: 0.477392
PRC train: 0.546653	val: 0.540306	test: 0.494676

Epoch: 3
Loss: 0.5398999088367546
ROC train: 0.667846	val: 0.754495	test: 0.481240
PRC train: 0.556549	val: 0.540447	test: 0.496670

Epoch: 4
Loss: 0.4898969903472367
ROC train: 0.699671	val: 0.737976	test: 0.482790
PRC train: 0.567214	val: 0.541400	test: 0.496703

Epoch: 5
Loss: 0.45032618965660076
ROC train: 0.736773	val: 0.739149	test: 0.512403
PRC train: 0.584047	val: 0.562514	test: 0.496719

Epoch: 6
Loss: 0.40717413142552134
ROC train: 0.774840	val: 0.756794	test: 0.540044
PRC train: 0.626790	val: 0.558867	test: 0.508584

Epoch: 7
Loss: 0.3703743609540183
ROC train: 0.804862	val: 0.782092	test: 0.542618
PRC train: 0.655023	val: 0.576410	test: 0.513200

Epoch: 8
Loss: 0.34679141534178537
ROC train: 0.822784	val: 0.786175	test: 0.543017
PRC train: 0.671355	val: 0.577227	test: 0.511611

Epoch: 9
Loss: 0.3155854150434528
ROC train: 0.849153	val: 0.801296	test: 0.541493
PRC train: 0.698275	val: 0.567707	test: 0.513994

Epoch: 10
Loss: 0.28350130621842407
ROC train: 0.876019	val: 0.804793	test: 0.552662
PRC train: 0.731211	val: 0.560202	test: 0.522146

Epoch: 11
Loss: 0.2665041366056064
ROC train: 0.894221	val: 0.794004	test: 0.558590
PRC train: 0.757116	val: 0.556867	test: 0.524045

Epoch: 12
Loss: 0.24041636158340984
ROC train: 0.913564	val: 0.775924	test: 0.554111
PRC train: 0.778159	val: 0.550127	test: 0.518724

Epoch: 13
Loss: 0.22487357005290404
ROC train: 0.929537	val: 0.805555	test: 0.568115
PRC train: 0.805686	val: 0.564904	test: 0.523138

Epoch: 14
Loss: 0.2145582454999731
ROC train: 0.931799	val: 0.821513	test: 0.585316
PRC train: 0.814803	val: 0.568827	test: 0.525362

Epoch: 15
Loss: 0.19801808431168183
ROC train: 0.943371	val: 0.814895	test: 0.606476
PRC train: 0.853603	val: 0.560134	test: 0.527202

Epoch: 16
Loss: 0.1910527567150202
ROC train: 0.959898	val: 0.824123	test: 0.615108
PRC train: 0.876679	val: 0.562932	test: 0.529866

Epoch: 17
Loss: 0.1781766862132384
ROC train: 0.966638	val: 0.813947	test: 0.611123
PRC train: 0.894304	val: 0.558141	test: 0.527212

Epoch: 18
Loss: 0.16442176384984006
ROC train: 0.955558	val: 0.826383	test: 0.592140
PRC train: 0.864902	val: 0.560704	test: 0.522486

Epoch: 19
Loss: 0.16733620244410569
ROC train: 0.978534	val: 0.834188	test: 0.583646
PRC train: 0.931203	val: 0.575593	test: 0.522011

Epoch: 20
Loss: 0.16060208616554195
ROC train: 0.978291	val: 0.836222	test: 0.588875
PRC train: 0.924252	val: 0.572432	test: 0.522016

Epoch: 21
Loss: 0.13948282140861357
ROC train: 0.981152	val: 0.828643	test: 0.613558
PRC train: 0.930966	val: 0.558452	test: 0.526491

Epoch: 22
Loss: 0.13347129022129034
ROC train: 0.985376	val: 0.838633	test: 0.619673
PRC train: 0.954240	val: 0.575823	test: 0.529063

Epoch: 23
Loss: 0.1217617347921224
ROC train: 0.985353	val: 0.821751	test: 0.587419
PRC train: 0.953427	val: 0.564321	test: 0.521091

Epoch: 24
Loss: 0.13659472253910912
ROC train: 0.988308	val: 0.818230	test: 0.582354
PRC train: 0.966806	val: 0.585952	test: 0.520070

Epoch: 25
Loss: 0.12256300753580958
ROC train: 0.991873	val: 0.794855	test: 0.569149
PRC train: 0.976301	val: 0.557948	test: 0.517681

Epoch: 26
Loss: 0.11349628398787459
ROC train: 0.994151	val: 0.779646	test: 0.559344
PRC train: 0.979612	val: 0.541877	test: 0.518030

Epoch: 27
Loss: 0.10668233523312302
ROC train: 0.993333	val: 0.828344	test: 0.588763
PRC train: 0.976281	val: 0.567276	test: 0.524999

Epoch: 28
Loss: 0.09773444417454753
ROC train: 0.991535	val: 0.840731	test: 0.609879
PRC train: 0.969770	val: 0.578874	test: 0.526913

Epoch: 29
Loss: 0.08913086630858208
ROC train: 0.991128	val: 0.718472	test: 0.562165
PRC train: 0.967783	val: 0.541394	test: 0.518123

Epoch: 30
Loss: 0.10150659497056265
ROC train: 0.992026	val: 0.722081	test: 0.576736
PRC train: 0.972689	val: 0.555715	test: 0.520841

Epoch: 31
Loss: 0.08485676316092827
ROC train: 0.993657	val: 0.846262	test: 0.605613
PRC train: 0.979400	val: 0.564851	test: 0.526809

Epoch: 32
Loss: 0.08488571065751435
ROC train: 0.994550	val: 0.806166	test: 0.585913Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/clintox/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:3
  input_data_dir: 
  dataset: clintox
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/static-noise/GraphMVP/clintox/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/clintox/noise=0.2/clintox_scaff_5_26-05_11-18-00  ]
[ Using Seed :  5  ]
[ Using device :  cuda:3  ]
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.6495997296154135
ROC train: 0.624732	val: 0.677228	test: 0.439976
PRC train: 0.537906	val: 0.523506	test: 0.494357

Epoch: 2
Loss: 0.5814446766532432
ROC train: 0.698726	val: 0.765934	test: 0.454484
PRC train: 0.570738	val: 0.539404	test: 0.493736

Epoch: 3
Loss: 0.5221839448450931
ROC train: 0.727362	val: 0.740200	test: 0.446326
PRC train: 0.586504	val: 0.537338	test: 0.491487

Epoch: 4
Loss: 0.47134893561453967
ROC train: 0.760410	val: 0.718423	test: 0.454028
PRC train: 0.611715	val: 0.541449	test: 0.492889

Epoch: 5
Loss: 0.4273277373921302
ROC train: 0.805620	val: 0.804733	test: 0.484116
PRC train: 0.660353	val: 0.550749	test: 0.501718

Epoch: 6
Loss: 0.3906630126562533
ROC train: 0.832808	val: 0.787826	test: 0.483705
PRC train: 0.693203	val: 0.539040	test: 0.506392

Epoch: 7
Loss: 0.3601941886846451
ROC train: 0.853759	val: 0.780383	test: 0.495598
PRC train: 0.721750	val: 0.535890	test: 0.507131

Epoch: 8
Loss: 0.3185136162098803
ROC train: 0.880240	val: 0.762715	test: 0.499643
PRC train: 0.752662	val: 0.532572	test: 0.509029

Epoch: 9
Loss: 0.29026589610556897
ROC train: 0.896673	val: 0.749154	test: 0.533724
PRC train: 0.769604	val: 0.530908	test: 0.512838

Epoch: 10
Loss: 0.2690675905137826
ROC train: 0.912691	val: 0.751051	test: 0.554686
PRC train: 0.797012	val: 0.531976	test: 0.519841

Epoch: 11
Loss: 0.24385318514745485
ROC train: 0.924691	val: 0.763150	test: 0.572825
PRC train: 0.822945	val: 0.536888	test: 0.524296

Epoch: 12
Loss: 0.2260023219777938
ROC train: 0.925692	val: 0.752049	test: 0.564955
PRC train: 0.827245	val: 0.536301	test: 0.520089

Epoch: 13
Loss: 0.2167217805649127
ROC train: 0.943104	val: 0.727514	test: 0.565223
PRC train: 0.865693	val: 0.528758	test: 0.527082

Epoch: 14
Loss: 0.19418748692020513
ROC train: 0.944936	val: 0.698382	test: 0.580039
PRC train: 0.869245	val: 0.525733	test: 0.544354

Epoch: 15
Loss: 0.1848699152201347
ROC train: 0.959223	val: 0.695248	test: 0.587490
PRC train: 0.901651	val: 0.525096	test: 0.532571

Epoch: 16
Loss: 0.17444288360090565
ROC train: 0.957557	val: 0.749878	test: 0.584666
PRC train: 0.899246	val: 0.532862	test: 0.526699

Epoch: 17
Loss: 0.1615364299151318
ROC train: 0.958943	val: 0.732796	test: 0.594673
PRC train: 0.904003	val: 0.532141	test: 0.531116

Epoch: 18
Loss: 0.1588878893149375
ROC train: 0.980728	val: 0.618577	test: 0.609113
PRC train: 0.938697	val: 0.518312	test: 0.551814

Epoch: 19
Loss: 0.14935171758388047
ROC train: 0.982112	val: 0.625395	test: 0.643553
PRC train: 0.931276	val: 0.519139	test: 0.561504

Epoch: 20
Loss: 0.14956287167332322
ROC train: 0.984114	val: 0.599647	test: 0.637288
PRC train: 0.934527	val: 0.516797	test: 0.556629

Epoch: 21
Loss: 0.14571577711862893
ROC train: 0.985838	val: 0.655426	test: 0.630849
PRC train: 0.952022	val: 0.523078	test: 0.557730

Epoch: 22
Loss: 0.12962397009342091
ROC train: 0.987313	val: 0.671110	test: 0.628025
PRC train: 0.948747	val: 0.532722	test: 0.551797

Epoch: 23
Loss: 0.1272614287965935
ROC train: 0.988954	val: 0.683497	test: 0.651822
PRC train: 0.957004	val: 0.536287	test: 0.561122

Epoch: 24
Loss: 0.11212028121442266
ROC train: 0.994171	val: 0.639318	test: 0.659592
PRC train: 0.975982	val: 0.521242	test: 0.571964

Epoch: 25
Loss: 0.10770720178032463
ROC train: 0.994582	val: 0.678689	test: 0.674970
PRC train: 0.975643	val: 0.524128	test: 0.578658

Epoch: 26
Loss: 0.1004211028269586
ROC train: 0.995071	val: 0.629141	test: 0.641453
PRC train: 0.978533	val: 0.519682	test: 0.554673

Epoch: 27
Loss: 0.10227102871686988
ROC train: 0.997172	val: 0.658473	test: 0.650672
PRC train: 0.987688	val: 0.522617	test: 0.562527

Epoch: 28
Loss: 0.08694706120374977
ROC train: 0.996084	val: 0.710843	test: 0.663951
PRC train: 0.981302	val: 0.528569	test: 0.571868

Epoch: 29
Loss: 0.09785829740895158
ROC train: 0.995472	val: 0.679188	test: 0.644651
PRC train: 0.981155	val: 0.526493	test: 0.564949

Epoch: 30
Loss: 0.08598469472638823
ROC train: 0.997544	val: 0.683272	test: 0.646313
PRC train: 0.988637	val: 0.531240	test: 0.563905

Epoch: 31
Loss: 0.08756843127887189
ROC train: 0.998545	val: 0.655113	test: 0.662991
PRC train: 0.991198	val: 0.532336	test: 0.591336

Epoch: 32
Loss: 0.08545633201575331
ROC train: 0.998068	val: 0.672983	test: 0.669924Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/clintox/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:3
  input_data_dir: 
  dataset: clintox
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/static-noise/GraphMVP/clintox/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/clintox/noise=0.2/clintox_scaff_6_26-05_11-18-00  ]
[ Using Seed :  6  ]
[ Using device :  cuda:3  ]
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.6778024735987918
ROC train: 0.596726	val: 0.748651	test: 0.442072
PRC train: 0.534126	val: 0.562816	test: 0.501055

Epoch: 2
Loss: 0.6097147610015895
ROC train: 0.639655	val: 0.807618	test: 0.441758
PRC train: 0.548470	val: 0.554091	test: 0.495301

Epoch: 3
Loss: 0.5491198848711221
ROC train: 0.692348	val: 0.758143	test: 0.436413
PRC train: 0.565834	val: 0.548296	test: 0.494106

Epoch: 4
Loss: 0.49811053520179494
ROC train: 0.732121	val: 0.774526	test: 0.417407
PRC train: 0.593895	val: 0.549512	test: 0.495466

Epoch: 5
Loss: 0.45211745451757224
ROC train: 0.767583	val: 0.798938	test: 0.423122
PRC train: 0.611900	val: 0.563120	test: 0.499154

Epoch: 6
Loss: 0.41179189227939006
ROC train: 0.803543	val: 0.824798	test: 0.443035
PRC train: 0.651154	val: 0.578546	test: 0.502788

Epoch: 7
Loss: 0.37648835805683045
ROC train: 0.826158	val: 0.801735	test: 0.425640
PRC train: 0.677126	val: 0.583394	test: 0.502703

Epoch: 8
Loss: 0.341858206431221
ROC train: 0.853114	val: 0.782780	test: 0.421531
PRC train: 0.699153	val: 0.578830	test: 0.494761

Epoch: 9
Loss: 0.31754481368805776
ROC train: 0.873125	val: 0.769420	test: 0.426878
PRC train: 0.717016	val: 0.560056	test: 0.495118

Epoch: 10
Loss: 0.29355329523889423
ROC train: 0.895056	val: 0.789973	test: 0.465051
PRC train: 0.751138	val: 0.598811	test: 0.503034

Epoch: 11
Loss: 0.26168723752462814
ROC train: 0.912149	val: 0.775214	test: 0.463766
PRC train: 0.788746	val: 0.589422	test: 0.501711

Epoch: 12
Loss: 0.24674332915667302
ROC train: 0.928465	val: 0.796991	test: 0.536044
PRC train: 0.811427	val: 0.609519	test: 0.511323

Epoch: 13
Loss: 0.22579038579659788
ROC train: 0.933207	val: 0.788013	test: 0.557354
PRC train: 0.822555	val: 0.633103	test: 0.514865

Epoch: 14
Loss: 0.21087495530632583
ROC train: 0.949794	val: 0.778848	test: 0.568321
PRC train: 0.848287	val: 0.632120	test: 0.516907

Epoch: 15
Loss: 0.20583054050785235
ROC train: 0.960914	val: 0.748068	test: 0.561220
PRC train: 0.870615	val: 0.629600	test: 0.518662

Epoch: 16
Loss: 0.18348783477248332
ROC train: 0.968011	val: 0.739901	test: 0.535405
PRC train: 0.896456	val: 0.581275	test: 0.514943

Epoch: 17
Loss: 0.17838307624209304
ROC train: 0.974852	val: 0.722570	test: 0.502927
PRC train: 0.915062	val: 0.576928	test: 0.508044

Epoch: 18
Loss: 0.16621627234156047
ROC train: 0.979188	val: 0.732634	test: 0.524326
PRC train: 0.929254	val: 0.605023	test: 0.511025

Epoch: 19
Loss: 0.15948623884140095
ROC train: 0.985497	val: 0.723044	test: 0.555266
PRC train: 0.946817	val: 0.633662	test: 0.523008

Epoch: 20
Loss: 0.15083474178957523
ROC train: 0.985510	val: 0.686020	test: 0.566173
PRC train: 0.945236	val: 0.635837	test: 0.528666

Epoch: 21
Loss: 0.13485537951815016
ROC train: 0.992010	val: 0.716051	test: 0.606346
PRC train: 0.969942	val: 0.640106	test: 0.548867

Epoch: 22
Loss: 0.14208165962012334
ROC train: 0.988806	val: 0.729187	test: 0.655868
PRC train: 0.964782	val: 0.650685	test: 0.555001

Epoch: 23
Loss: 0.12626184777107524
ROC train: 0.993364	val: 0.727514	test: 0.615326
PRC train: 0.971454	val: 0.643454	test: 0.579061

Epoch: 24
Loss: 0.1214351003996188
ROC train: 0.993841	val: 0.682224	test: 0.559352
PRC train: 0.972992	val: 0.537923	test: 0.537769

Epoch: 25
Loss: 0.11525260840639065
ROC train: 0.995041	val: 0.722320	test: 0.569520
PRC train: 0.982705	val: 0.619911	test: 0.565232

Epoch: 26
Loss: 0.11009205072029979
ROC train: 0.993524	val: 0.749266	test: 0.578376
PRC train: 0.979672	val: 0.685984	test: 0.524891

Epoch: 27
Loss: 0.119429179372982
ROC train: 0.995992	val: 0.747843	test: 0.598001
PRC train: 0.986190	val: 0.631988	test: 0.534518

Epoch: 28
Loss: 0.09865438334416479
ROC train: 0.998272	val: 0.739040	test: 0.562837
PRC train: 0.993086	val: 0.582500	test: 0.535960

Epoch: 29
Loss: 0.1079316552692168
ROC train: 0.995522	val: 0.772754	test: 0.589033
PRC train: 0.981837	val: 0.642103	test: 0.529165

Epoch: 30
Loss: 0.10534924461383091
ROC train: 0.997903	val: 0.708060	test: 0.549420
PRC train: 0.987891	val: 0.586247	test: 0.537946

Epoch: 31
Loss: 0.09002200194657721
ROC train: 0.998419	val: 0.683423	test: 0.539813
PRC train: 0.992735	val: 0.624993	test: 0.535601

Epoch: 32
Loss: 0.08983929150457112
ROC train: 0.999058	val: 0.708085	test: 0.558713Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/clintox/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: clintox
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/static-noise/GraphMVP/clintox/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/clintox/noise=0.05/clintox_scaff_6_26-05_11-18-00  ]
[ Using Seed :  6  ]
[ Using device :  cuda:1  ]
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.6725509324488306
ROC train: 0.644465	val: 0.810563	test: 0.474311
PRC train: 0.547797	val: 0.588394	test: 0.500284

Epoch: 2
Loss: 0.6022912405155514
ROC train: 0.680835	val: 0.848237	test: 0.507760
PRC train: 0.559134	val: 0.565941	test: 0.502191

Epoch: 3
Loss: 0.5415225620194979
ROC train: 0.721926	val: 0.823150	test: 0.508948
PRC train: 0.570878	val: 0.560221	test: 0.500153

Epoch: 4
Loss: 0.4883805556374538
ROC train: 0.757252	val: 0.818279	test: 0.514159
PRC train: 0.582308	val: 0.577791	test: 0.503123

Epoch: 5
Loss: 0.44798203560069744
ROC train: 0.791621	val: 0.829493	test: 0.537183
PRC train: 0.604105	val: 0.642861	test: 0.510699

Epoch: 6
Loss: 0.4020964460205104
ROC train: 0.819293	val: 0.855715	test: 0.543772
PRC train: 0.621493	val: 0.604425	test: 0.512870

Epoch: 7
Loss: 0.3736997805479577
ROC train: 0.833810	val: 0.816294	test: 0.532861
PRC train: 0.631401	val: 0.594663	test: 0.510174

Epoch: 8
Loss: 0.3400713044583676
ROC train: 0.863713	val: 0.850433	test: 0.559725
PRC train: 0.667238	val: 0.588192	test: 0.519447

Epoch: 9
Loss: 0.31805597895803117
ROC train: 0.876745	val: 0.850346	test: 0.571062
PRC train: 0.682217	val: 0.596008	test: 0.520865

Epoch: 10
Loss: 0.289893700599667
ROC train: 0.873702	val: 0.837210	test: 0.549976
PRC train: 0.678240	val: 0.568117	test: 0.514360

Epoch: 11
Loss: 0.2649887368095602
ROC train: 0.899110	val: 0.850820	test: 0.570987
PRC train: 0.723427	val: 0.574890	test: 0.522506

Epoch: 12
Loss: 0.2488564847861205
ROC train: 0.911316	val: 0.855104	test: 0.622515
PRC train: 0.751682	val: 0.582486	test: 0.531663

Epoch: 13
Loss: 0.23315144564958817
ROC train: 0.904294	val: 0.800337	test: 0.612882
PRC train: 0.743613	val: 0.592274	test: 0.524624

Epoch: 14
Loss: 0.21828965792534719
ROC train: 0.927760	val: 0.826084	test: 0.629810
PRC train: 0.782678	val: 0.563120	test: 0.533439

Epoch: 15
Loss: 0.21420308922181874
ROC train: 0.936496	val: 0.795554	test: 0.624439
PRC train: 0.814021	val: 0.650482	test: 0.531570

Epoch: 16
Loss: 0.20318786044523449
ROC train: 0.938397	val: 0.813760	test: 0.645525
PRC train: 0.829715	val: 0.668331	test: 0.535924

Epoch: 17
Loss: 0.20166170774689895
ROC train: 0.948853	val: 0.852932	test: 0.660765
PRC train: 0.841928	val: 0.669856	test: 0.540374

Epoch: 18
Loss: 0.1871362795044773
ROC train: 0.955542	val: 0.846413	test: 0.640766
PRC train: 0.848167	val: 0.622368	test: 0.546480

Epoch: 19
Loss: 0.18037433279882759
ROC train: 0.960129	val: 0.786301	test: 0.606749
PRC train: 0.868864	val: 0.656370	test: 0.526920

Epoch: 20
Loss: 0.16075677458425858
ROC train: 0.961562	val: 0.825385	test: 0.634969
PRC train: 0.875724	val: 0.631647	test: 0.534307

Epoch: 21
Loss: 0.164947179680308
ROC train: 0.967128	val: 0.792757	test: 0.633669
PRC train: 0.889103	val: 0.693012	test: 0.552751

Epoch: 22
Loss: 0.15249918036645296
ROC train: 0.966719	val: 0.852932	test: 0.662524
PRC train: 0.883060	val: 0.724772	test: 0.558075

Epoch: 23
Loss: 0.15506173155007558
ROC train: 0.969733	val: 0.839708	test: 0.654916
PRC train: 0.896789	val: 0.654730	test: 0.557250

Epoch: 24
Loss: 0.14441205330376355
ROC train: 0.974034	val: 0.754260	test: 0.588472
PRC train: 0.906880	val: 0.618653	test: 0.529298

Epoch: 25
Loss: 0.13980365497176334
ROC train: 0.975864	val: 0.751863	test: 0.628085
PRC train: 0.917488	val: 0.587098	test: 0.595299

Epoch: 26
Loss: 0.1361883861373269
ROC train: 0.976190	val: 0.816445	test: 0.656040
PRC train: 0.913797	val: 0.713627	test: 0.579736

Epoch: 27
Loss: 0.1495484104723542
ROC train: 0.978256	val: 0.801461	test: 0.631219
PRC train: 0.915218	val: 0.722159	test: 0.573183

Epoch: 28
Loss: 0.13011939329979594
ROC train: 0.986163	val: 0.836437	test: 0.642175
PRC train: 0.931469	val: 0.639655	test: 0.558005

Epoch: 29
Loss: 0.1286087847954855
ROC train: 0.984533	val: 0.821165	test: 0.630408
PRC train: 0.923327	val: 0.618637	test: 0.532921

Epoch: 30
Loss: 0.13232274190655663
ROC train: 0.987393	val: 0.809515	test: 0.629870
PRC train: 0.949474	val: 0.663119	test: 0.531600

Epoch: 31
Loss: 0.12142050096895171
ROC train: 0.989806	val: 0.732096	test: 0.625485
PRC train: 0.962053	val: 0.635219	test: 0.538954

Epoch: 32
Loss: 0.10627786451541317
ROC train: 0.991385	val: 0.782056	test: 0.632369Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/clintox/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: clintox
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/static-noise/GraphMVP/clintox/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/clintox/noise=0.05/clintox_scaff_5_26-05_11-18-00  ]
[ Using Seed :  5  ]
[ Using device :  cuda:1  ]
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.6460630425613638
ROC train: 0.597312	val: 0.665715	test: 0.447335
PRC train: 0.530371	val: 0.518260	test: 0.495748

Epoch: 2
Loss: 0.5777073721898185
ROC train: 0.662625	val: 0.781930	test: 0.480067
PRC train: 0.550419	val: 0.540630	test: 0.499220

Epoch: 3
Loss: 0.5191997773884826
ROC train: 0.685379	val: 0.787338	test: 0.478868
PRC train: 0.563889	val: 0.544798	test: 0.497286

Epoch: 4
Loss: 0.4716392498609596
ROC train: 0.720293	val: 0.806430	test: 0.496114
PRC train: 0.572313	val: 0.559213	test: 0.497430

Epoch: 5
Loss: 0.4301336381858912
ROC train: 0.771157	val: 0.881326	test: 0.552460
PRC train: 0.598147	val: 0.591892	test: 0.532580

Epoch: 6
Loss: 0.394250727823672
ROC train: 0.803095	val: 0.858013	test: 0.552572
PRC train: 0.616137	val: 0.569841	test: 0.519721

Epoch: 7
Loss: 0.3627028224471389
ROC train: 0.822058	val: 0.834638	test: 0.549113
PRC train: 0.629553	val: 0.559121	test: 0.511825

Epoch: 8
Loss: 0.33167745984219393
ROC train: 0.847054	val: 0.849871	test: 0.569863
PRC train: 0.638629	val: 0.569404	test: 0.519124

Epoch: 9
Loss: 0.2984804268688236
ROC train: 0.869204	val: 0.823899	test: 0.567390
PRC train: 0.684610	val: 0.569708	test: 0.526516

Epoch: 10
Loss: 0.27964842858583416
ROC train: 0.886605	val: 0.790434	test: 0.571674
PRC train: 0.723108	val: 0.555432	test: 0.524867

Epoch: 11
Loss: 0.2537169967601709
ROC train: 0.902409	val: 0.778721	test: 0.577290
PRC train: 0.754937	val: 0.552679	test: 0.527560

Epoch: 12
Loss: 0.24032873695176576
ROC train: 0.905196	val: 0.773963	test: 0.582880
PRC train: 0.774221	val: 0.553442	test: 0.521256

Epoch: 13
Loss: 0.2267545350391841
ROC train: 0.928551	val: 0.750451	test: 0.620039
PRC train: 0.798772	val: 0.551007	test: 0.558888

Epoch: 14
Loss: 0.21538072177195605
ROC train: 0.927946	val: 0.768618	test: 0.613723
PRC train: 0.798805	val: 0.555556	test: 0.533180

Epoch: 15
Loss: 0.2077718154736999
ROC train: 0.937648	val: 0.779832	test: 0.623703
PRC train: 0.808370	val: 0.558363	test: 0.532519

Epoch: 16
Loss: 0.1903483625439708
ROC train: 0.950768	val: 0.713802	test: 0.636157
PRC train: 0.843952	val: 0.534879	test: 0.543883

Epoch: 17
Loss: 0.18314032061792634
ROC train: 0.955489	val: 0.711803	test: 0.651923
PRC train: 0.844938	val: 0.553751	test: 0.554830

Epoch: 18
Loss: 0.18303233839731614
ROC train: 0.963044	val: 0.666000	test: 0.662879
PRC train: 0.858142	val: 0.550683	test: 0.566793

Epoch: 19
Loss: 0.16735454316760373
ROC train: 0.960677	val: 0.636144	test: 0.648793
PRC train: 0.874538	val: 0.533686	test: 0.560649

Epoch: 20
Loss: 0.15943659275071512
ROC train: 0.972392	val: 0.643074	test: 0.676232
PRC train: 0.890329	val: 0.577861	test: 0.582184

Epoch: 21
Loss: 0.14904025268282467
ROC train: 0.968348	val: 0.666938	test: 0.656719
PRC train: 0.895865	val: 0.527470	test: 0.545708

Epoch: 22
Loss: 0.15610152362924667
ROC train: 0.981741	val: 0.669247	test: 0.706675
PRC train: 0.920699	val: 0.544707	test: 0.568085

Epoch: 23
Loss: 0.14450599618913879
ROC train: 0.985452	val: 0.612020	test: 0.682441
PRC train: 0.928594	val: 0.527171	test: 0.563988

Epoch: 24
Loss: 0.14939257342236872
ROC train: 0.979421	val: 0.656885	test: 0.687674
PRC train: 0.918336	val: 0.568731	test: 0.566125

Epoch: 25
Loss: 0.13532459742968722
ROC train: 0.988625	val: 0.614417	test: 0.707699
PRC train: 0.944118	val: 0.550361	test: 0.571682

Epoch: 26
Loss: 0.12872035738254
ROC train: 0.987390	val: 0.722493	test: 0.686224
PRC train: 0.943916	val: 0.537172	test: 0.571397

Epoch: 27
Loss: 0.1321597341392439
ROC train: 0.987166	val: 0.694046	test: 0.704400
PRC train: 0.944511	val: 0.571334	test: 0.582405

Epoch: 28
Loss: 0.11668167668520553
ROC train: 0.990856	val: 0.707245	test: 0.710758
PRC train: 0.952651	val: 0.563790	test: 0.583607

Epoch: 29
Loss: 0.1151414566187168
ROC train: 0.994128	val: 0.705548	test: 0.693381
PRC train: 0.966892	val: 0.532141	test: 0.567938

Epoch: 30
Loss: 0.11205875994857024
ROC train: 0.992980	val: 0.816681	test: 0.690509
PRC train: 0.961685	val: 0.558173	test: 0.555336

Epoch: 31
Loss: 0.10612711478018108
ROC train: 0.995463	val: 0.847861	test: 0.677805
PRC train: 0.971295	val: 0.567384	test: 0.544280

Epoch: 32
Loss: 0.10918792317171037
ROC train: 0.996583	val: 0.843128	test: 0.693251Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/clintox/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: clintox
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/static-noise/GraphMVP/clintox/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/clintox/noise=0.1/clintox_scaff_5_26-05_11-18-00  ]
[ Using Seed :  5  ]
[ Using device :  cuda:2  ]
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.6483236615740654
ROC train: 0.567492	val: 0.636510	test: 0.436659
PRC train: 0.530545	val: 0.514531	test: 0.495384

Epoch: 2
Loss: 0.5807507971569041
ROC train: 0.629790	val: 0.764398	test: 0.466313
PRC train: 0.549976	val: 0.537292	test: 0.497003

Epoch: 3
Loss: 0.5238793472218732
ROC train: 0.658548	val: 0.749315	test: 0.461678
PRC train: 0.567592	val: 0.537042	test: 0.494120

Epoch: 4
Loss: 0.47552790085103547
ROC train: 0.691255	val: 0.765248	test: 0.463628
PRC train: 0.570654	val: 0.544784	test: 0.496580

Epoch: 5
Loss: 0.4352131697791834
ROC train: 0.749633	val: 0.857876	test: 0.509101
PRC train: 0.600977	val: 0.571982	test: 0.508474

Epoch: 6
Loss: 0.3991713391688133
ROC train: 0.787370	val: 0.857852	test: 0.523161
PRC train: 0.620662	val: 0.563478	test: 0.513856

Epoch: 7
Loss: 0.3660140482412128
ROC train: 0.814336	val: 0.839396	test: 0.535371
PRC train: 0.641275	val: 0.555280	test: 0.514572

Epoch: 8
Loss: 0.33208505653683607
ROC train: 0.834556	val: 0.813012	test: 0.550712
PRC train: 0.651489	val: 0.547713	test: 0.517097

Epoch: 9
Loss: 0.3017608695118085
ROC train: 0.859096	val: 0.790747	test: 0.568025
PRC train: 0.682630	val: 0.549403	test: 0.520961

Epoch: 10
Loss: 0.2814864795152669
ROC train: 0.880153	val: 0.782780	test: 0.577770
PRC train: 0.712959	val: 0.549298	test: 0.524646

Epoch: 11
Loss: 0.25528996378786156
ROC train: 0.899000	val: 0.758568	test: 0.585166
PRC train: 0.750207	val: 0.549575	test: 0.525659

Epoch: 12
Loss: 0.24244750344326169
ROC train: 0.908918	val: 0.752000	test: 0.596346
PRC train: 0.778273	val: 0.542076	test: 0.526944

Epoch: 13
Loss: 0.22801757382130744
ROC train: 0.920891	val: 0.737129	test: 0.624801
PRC train: 0.802360	val: 0.535568	test: 0.533596

Epoch: 14
Loss: 0.21155907568595475
ROC train: 0.924695	val: 0.719621	test: 0.645812
PRC train: 0.815597	val: 0.536141	test: 0.539927

Epoch: 15
Loss: 0.20392830464436104
ROC train: 0.942687	val: 0.738127	test: 0.657575
PRC train: 0.837795	val: 0.536868	test: 0.546429

Epoch: 16
Loss: 0.19175048527845473
ROC train: 0.954059	val: 0.748890	test: 0.673364
PRC train: 0.857854	val: 0.538858	test: 0.550808

Epoch: 17
Loss: 0.18285356960869023
ROC train: 0.954280	val: 0.774750	test: 0.689710
PRC train: 0.862982	val: 0.556249	test: 0.568833

Epoch: 18
Loss: 0.17471272867063814
ROC train: 0.977048	val: 0.728175	test: 0.727223
PRC train: 0.911674	val: 0.537974	test: 0.606468

Epoch: 19
Loss: 0.15623736152017026
ROC train: 0.976367	val: 0.675218	test: 0.727047
PRC train: 0.918253	val: 0.530718	test: 0.593818

Epoch: 20
Loss: 0.155994491522319
ROC train: 0.983647	val: 0.642839	test: 0.736154
PRC train: 0.932605	val: 0.537098	test: 0.592732

Epoch: 21
Loss: 0.13594676615994086
ROC train: 0.979312	val: 0.648409	test: 0.720365
PRC train: 0.926885	val: 0.522042	test: 0.575444

Epoch: 22
Loss: 0.14424568402965127
ROC train: 0.986262	val: 0.643675	test: 0.738265
PRC train: 0.948649	val: 0.534447	test: 0.581837

Epoch: 23
Loss: 0.14232462149240357
ROC train: 0.987259	val: 0.631014	test: 0.697363
PRC train: 0.946145	val: 0.521134	test: 0.567329

Epoch: 24
Loss: 0.13127219547194818
ROC train: 0.990226	val: 0.660532	test: 0.740775
PRC train: 0.956163	val: 0.541667	test: 0.588266

Epoch: 25
Loss: 0.12025188267736345
ROC train: 0.993421	val: 0.657373	test: 0.762387
PRC train: 0.972029	val: 0.531043	test: 0.589591

Epoch: 26
Loss: 0.11156470895769051
ROC train: 0.989388	val: 0.718659	test: 0.735867
PRC train: 0.963066	val: 0.538554	test: 0.578707

Epoch: 27
Loss: 0.10344088197766348
ROC train: 0.993141	val: 0.763224	test: 0.737028
PRC train: 0.970653	val: 0.567466	test: 0.589273

Epoch: 28
Loss: 0.1024103988228815
ROC train: 0.997053	val: 0.775200	test: 0.760493
PRC train: 0.984150	val: 0.552489	test: 0.605190

Epoch: 29
Loss: 0.0930858558318081
ROC train: 0.997681	val: 0.723642	test: 0.760307
PRC train: 0.988177	val: 0.545551	test: 0.600693

Epoch: 30
Loss: 0.0935796949850565
ROC train: 0.997691	val: 0.642726	test: 0.741850
PRC train: 0.989229	val: 0.529654	test: 0.581847

Epoch: 31
Loss: 0.0886664339512765
ROC train: 0.998153	val: 0.635396	test: 0.753180
PRC train: 0.989332	val: 0.536813	test: 0.588490

Epoch: 32
Loss: 0.08404002016696085
ROC train: 0.997602	val: 0.659109	test: 0.755327Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/clintox/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: clintox
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/static-noise/GraphMVP/clintox/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/clintox/noise=0.1/clintox_scaff_6_26-05_11-18-00  ]
[ Using Seed :  6  ]
[ Using device :  cuda:2  ]
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.6745099023071346
ROC train: 0.600645	val: 0.774800	test: 0.445602
PRC train: 0.537907	val: 0.562375	test: 0.496626

Epoch: 2
Loss: 0.60494356877276
ROC train: 0.655262	val: 0.813286	test: 0.463945
PRC train: 0.549527	val: 0.553262	test: 0.495437

Epoch: 3
Loss: 0.545693233675949
ROC train: 0.700028	val: 0.800498	test: 0.484026
PRC train: 0.562475	val: 0.551555	test: 0.496928

Epoch: 4
Loss: 0.49396898390919886
ROC train: 0.736705	val: 0.811736	test: 0.499124
PRC train: 0.575120	val: 0.561588	test: 0.502310

Epoch: 5
Loss: 0.45043958760943736
ROC train: 0.770760	val: 0.782355	test: 0.500368
PRC train: 0.590907	val: 0.577509	test: 0.503206

Epoch: 6
Loss: 0.40640191906876294
ROC train: 0.808871	val: 0.784165	test: 0.498082
PRC train: 0.611501	val: 0.555997	test: 0.501343

Epoch: 7
Loss: 0.37411040143560376
ROC train: 0.844779	val: 0.784028	test: 0.511899
PRC train: 0.638882	val: 0.579655	test: 0.502980

Epoch: 8
Loss: 0.34006447916876414
ROC train: 0.879084	val: 0.807467	test: 0.542842
PRC train: 0.671999	val: 0.623938	test: 0.512168

Epoch: 9
Loss: 0.313746731407658
ROC train: 0.892482	val: 0.766334	test: 0.548725
PRC train: 0.701629	val: 0.572999	test: 0.512029

Epoch: 10
Loss: 0.2864651254584912
ROC train: 0.894564	val: 0.718247	test: 0.531554
PRC train: 0.721489	val: 0.534363	test: 0.509495

Epoch: 11
Loss: 0.26413182967079213
ROC train: 0.923108	val: 0.753473	test: 0.566725
PRC train: 0.785379	val: 0.544064	test: 0.514317

Epoch: 12
Loss: 0.24367718294499402
ROC train: 0.939173	val: 0.750264	test: 0.590273
PRC train: 0.811008	val: 0.555131	test: 0.521379

Epoch: 13
Loss: 0.23126618465676846
ROC train: 0.942106	val: 0.718160	test: 0.607985
PRC train: 0.813835	val: 0.544973	test: 0.524416

Epoch: 14
Loss: 0.20905560692874342
ROC train: 0.951515	val: 0.681761	test: 0.599678
PRC train: 0.843111	val: 0.542456	test: 0.525310

Epoch: 15
Loss: 0.20384479659594046
ROC train: 0.959477	val: 0.660184	test: 0.616154
PRC train: 0.863752	val: 0.526209	test: 0.530349

Epoch: 16
Loss: 0.19074005855652526
ROC train: 0.966094	val: 0.702539	test: 0.652458
PRC train: 0.887029	val: 0.529802	test: 0.547440

Epoch: 17
Loss: 0.1847409095881478
ROC train: 0.973892	val: 0.614332	test: 0.635574
PRC train: 0.909435	val: 0.534940	test: 0.533021

Epoch: 18
Loss: 0.16720329461532607
ROC train: 0.975070	val: 0.687394	test: 0.636792
PRC train: 0.918701	val: 0.535246	test: 0.531267

Epoch: 19
Loss: 0.16146044310561064
ROC train: 0.979159	val: 0.566158	test: 0.586773
PRC train: 0.932804	val: 0.515690	test: 0.517827

Epoch: 20
Loss: 0.1475219638275653
ROC train: 0.979512	val: 0.534640	test: 0.561089
PRC train: 0.934928	val: 0.518977	test: 0.517483

Epoch: 21
Loss: 0.14842362082986776
ROC train: 0.987954	val: 0.628118	test: 0.649720
PRC train: 0.962700	val: 0.599098	test: 0.546623

Epoch: 22
Loss: 0.1277565442153124
ROC train: 0.984244	val: 0.649533	test: 0.664228
PRC train: 0.952364	val: 0.558594	test: 0.553793

Epoch: 23
Loss: 0.12361059262385883
ROC train: 0.986286	val: 0.638906	test: 0.682460
PRC train: 0.957281	val: 0.583954	test: 0.569331

Epoch: 24
Loss: 0.11807890999701307
ROC train: 0.991682	val: 0.564872	test: 0.664859
PRC train: 0.976913	val: 0.542090	test: 0.555174

Epoch: 25
Loss: 0.11156343166183488
ROC train: 0.991200	val: 0.485580	test: 0.648271
PRC train: 0.972935	val: 0.508655	test: 0.552722

Epoch: 26
Loss: 0.11423295845952293
ROC train: 0.994217	val: 0.597950	test: 0.682374
PRC train: 0.976664	val: 0.520005	test: 0.576313

Epoch: 27
Loss: 0.12088224013003532
ROC train: 0.992948	val: 0.716849	test: 0.681387
PRC train: 0.976643	val: 0.543455	test: 0.603633

Epoch: 28
Loss: 0.10275953647620786
ROC train: 0.994162	val: 0.712354	test: 0.669069
PRC train: 0.984376	val: 0.552497	test: 0.552160

Epoch: 29
Loss: 0.10247757108551832
ROC train: 0.993465	val: 0.688504	test: 0.656712
PRC train: 0.983765	val: 0.546479	test: 0.546444

Epoch: 30
Loss: 0.10566136403915924
ROC train: 0.993190	val: 0.683922	test: 0.647169
PRC train: 0.981860	val: 0.530326	test: 0.548364

Epoch: 31
Loss: 0.09244784914052345
ROC train: 0.993792	val: 0.601559	test: 0.615127
PRC train: 0.981770	val: 0.524132	test: 0.553872

Epoch: 32
Loss: 0.08598840874744958
ROC train: 0.994970	val: 0.674757	test: 0.647684Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphMVP/clintox/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: clintox
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/static-noise/GraphMVP/clintox/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphMVP/clintox/noise=0.1/clintox_scaff_4_26-05_11-18-00  ]
[ Using Seed :  4  ]
[ Using device :  cuda:2  ]
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.6582285991229224
ROC train: 0.590738	val: 0.732578	test: 0.471181
PRC train: 0.533341	val: 0.540625	test: 0.502019

Epoch: 2
Loss: 0.5905370672994199
ROC train: 0.661310	val: 0.805854	test: 0.504526
PRC train: 0.552872	val: 0.551009	test: 0.499483

Epoch: 3
Loss: 0.5346983675004681
ROC train: 0.692396	val: 0.796352	test: 0.501190
PRC train: 0.569522	val: 0.549249	test: 0.500571

Epoch: 4
Loss: 0.48391141527366577
ROC train: 0.729252	val: 0.819590	test: 0.514245
PRC train: 0.579902	val: 0.555772	test: 0.500252

Epoch: 5
Loss: 0.44424102084965106
ROC train: 0.756664	val: 0.855040	test: 0.558986
PRC train: 0.585688	val: 0.572990	test: 0.512761

Epoch: 6
Loss: 0.40433501456177723
ROC train: 0.782673	val: 0.872147	test: 0.584001
PRC train: 0.600186	val: 0.573463	test: 0.520904

Epoch: 7
Loss: 0.36559705220400407
ROC train: 0.803631	val: 0.865105	test: 0.578230
PRC train: 0.610792	val: 0.576322	test: 0.520095

Epoch: 8
Loss: 0.3413837871119793
ROC train: 0.825584	val: 0.868963	test: 0.582727
PRC train: 0.632145	val: 0.573715	test: 0.520357

Epoch: 9
Loss: 0.3086751111476535
ROC train: 0.851124	val: 0.862982	test: 0.593078
PRC train: 0.660348	val: 0.574506	test: 0.526448

Epoch: 10
Loss: 0.28608481947654973
ROC train: 0.875284	val: 0.851881	test: 0.610648
PRC train: 0.699088	val: 0.571204	test: 0.538239

Epoch: 11
Loss: 0.2636620791355224
ROC train: 0.893072	val: 0.843827	test: 0.624308
PRC train: 0.734551	val: 0.569604	test: 0.543516

Epoch: 12
Loss: 0.24322921052371607
ROC train: 0.908771	val: 0.849622	test: 0.634521
PRC train: 0.770386	val: 0.574536	test: 0.543858

Epoch: 13
Loss: 0.23069485787250493
ROC train: 0.906722	val: 0.868376	test: 0.637117
PRC train: 0.758216	val: 0.580151	test: 0.542252

Epoch: 14
Loss: 0.2160304051677727
ROC train: 0.918925	val: 0.869750	test: 0.652719
PRC train: 0.782074	val: 0.598146	test: 0.549208

Epoch: 15
Loss: 0.2062216482896469
ROC train: 0.931898	val: 0.831640	test: 0.672195
PRC train: 0.816039	val: 0.648140	test: 0.555728

Epoch: 16
Loss: 0.19685156676881532
ROC train: 0.941470	val: 0.857725	test: 0.663249
PRC train: 0.833032	val: 0.581469	test: 0.547227

Epoch: 17
Loss: 0.17898468289766728
ROC train: 0.954207	val: 0.861584	test: 0.665415
PRC train: 0.861233	val: 0.589342	test: 0.550021

Epoch: 18
Loss: 0.17828860831144544
ROC train: 0.952946	val: 0.850658	test: 0.633079
PRC train: 0.839996	val: 0.599476	test: 0.536167

Epoch: 19
Loss: 0.17534844682447284
ROC train: 0.962800	val: 0.847650	test: 0.653366
PRC train: 0.883092	val: 0.584228	test: 0.542125

Epoch: 20
Loss: 0.16836608658591135
ROC train: 0.963011	val: 0.831292	test: 0.665094
PRC train: 0.880175	val: 0.567528	test: 0.543922

Epoch: 21
Loss: 0.15153331104580864
ROC train: 0.972032	val: 0.851220	test: 0.671840
PRC train: 0.907383	val: 0.599149	test: 0.553213

Epoch: 22
Loss: 0.14137248517074552
ROC train: 0.980141	val: 0.866117	test: 0.700867
PRC train: 0.925386	val: 0.654894	test: 0.568427

Epoch: 23
Loss: 0.14448266992730244
ROC train: 0.983481	val: 0.831204	test: 0.659600
PRC train: 0.936300	val: 0.641484	test: 0.548184

Epoch: 24
Loss: 0.13560147266062092
ROC train: 0.981436	val: 0.828020	test: 0.672035
PRC train: 0.941790	val: 0.675428	test: 0.565053

Epoch: 25
Loss: 0.13836669553321743
ROC train: 0.979262	val: 0.815883	test: 0.643859
PRC train: 0.923135	val: 0.569176	test: 0.548715

Epoch: 26
Loss: 0.11877359053827571
ROC train: 0.989590	val: 0.809677	test: 0.694159
PRC train: 0.958070	val: 0.583920	test: 0.567167

Epoch: 27
Loss: 0.13341650183119494
ROC train: 0.990780	val: 0.823512	test: 0.706918
PRC train: 0.963042	val: 0.648330	test: 0.563375

Epoch: 28
Loss: 0.11042953545219188
ROC train: 0.991342	val: 0.843504	test: 0.699511
PRC train: 0.968464	val: 0.642407	test: 0.558409

Epoch: 29
Loss: 0.10843271193278248
ROC train: 0.989503	val: 0.849622	test: 0.712054
PRC train: 0.965804	val: 0.641433	test: 0.567979

Epoch: 30
Loss: 0.10894880771590439
ROC train: 0.992171	val: 0.850433	test: 0.679837
PRC train: 0.971483	val: 0.639372	test: 0.568792

Epoch: 31
Loss: 0.10596635184642425
ROC train: 0.990480	val: 0.864894	test: 0.662610
PRC train: 0.961804	val: 0.600140	test: 0.562540

Epoch: 32
Loss: 0.11487034490306976
ROC train: 0.994228	val: 0.843279	test: 0.693864
ROC train: 0.941471	val: 0.890916	test: 0.720182
PRC train: 0.802310	val: 0.634917	test: 0.556955

Epoch: 34
Loss: 0.15686850762975418
ROC train: 0.954259	val: 0.885135	test: 0.704374
PRC train: 0.820514	val: 0.634547	test: 0.556793

Epoch: 35
Loss: 0.15608578404539847
ROC train: 0.955740	val: 0.880465	test: 0.725684
PRC train: 0.830909	val: 0.684309	test: 0.560921

Epoch: 36
Loss: 0.1557500967388073
ROC train: 0.956253	val: 0.897434	test: 0.759238
PRC train: 0.837291	val: 0.711008	test: 0.586343

Epoch: 37
Loss: 0.16006383673349578
ROC train: 0.953155	val: 0.862546	test: 0.725027
PRC train: 0.835503	val: 0.650390	test: 0.574251

Epoch: 38
Loss: 0.16162635559521474
ROC train: 0.957016	val: 0.833439	test: 0.732318
PRC train: 0.835055	val: 0.563903	test: 0.575722

Epoch: 39
Loss: 0.15060299652238146
ROC train: 0.954374	val: 0.842579	test: 0.755466
PRC train: 0.827109	val: 0.571212	test: 0.595053

Epoch: 40
Loss: 0.16071562612390994
ROC train: 0.954038	val: 0.865755	test: 0.778288
PRC train: 0.828892	val: 0.619024	test: 0.582085

Epoch: 41
Loss: 0.14881872007219138
ROC train: 0.961415	val: 0.885722	test: 0.754991
PRC train: 0.828950	val: 0.657057	test: 0.592255

Epoch: 42
Loss: 0.1503112845360039
ROC train: 0.955267	val: 0.887981	test: 0.727201
PRC train: 0.824870	val: 0.689395	test: 0.568335

Epoch: 43
Loss: 0.14794058766495716
ROC train: 0.957972	val: 0.862908	test: 0.726622
PRC train: 0.821596	val: 0.670212	test: 0.577146

Epoch: 44
Loss: 0.14967917788295187
ROC train: 0.954778	val: 0.897146	test: 0.726009
PRC train: 0.818320	val: 0.617737	test: 0.560375

Epoch: 45
Loss: 0.153210118248818
ROC train: 0.946248	val: 0.897058	test: 0.748357
PRC train: 0.808068	val: 0.630779	test: 0.569074

Epoch: 46
Loss: 0.1476908147587747
ROC train: 0.968166	val: 0.879428	test: 0.766627
PRC train: 0.851534	val: 0.642424	test: 0.582938

Epoch: 47
Loss: 0.14140775770247982
ROC train: 0.965416	val: 0.863882	test: 0.756426
PRC train: 0.845830	val: 0.626507	test: 0.582652

Epoch: 48
Loss: 0.1412085557528014
ROC train: 0.967351	val: 0.859661	test: 0.753527
PRC train: 0.849824	val: 0.588960	test: 0.596097

Epoch: 49
Loss: 0.1491386748668225
ROC train: 0.970185	val: 0.849347	test: 0.802004
PRC train: 0.861241	val: 0.609483	test: 0.652220

Epoch: 50
Loss: 0.14644154545621194
ROC train: 0.967372	val: 0.871985	test: 0.824707
PRC train: 0.857546	val: 0.614251	test: 0.639393

Epoch: 51
Loss: 0.14704300229517425
ROC train: 0.967207	val: 0.865892	test: 0.802990
PRC train: 0.859546	val: 0.609540	test: 0.606607

Epoch: 52
Loss: 0.1484524183029607
ROC train: 0.967232	val: 0.834750	test: 0.802665
PRC train: 0.858526	val: 0.593313	test: 0.625092

Epoch: 53
Loss: 0.1371694352773132
ROC train: 0.967682	val: 0.856439	test: 0.819123
PRC train: 0.859768	val: 0.628875	test: 0.662661

Epoch: 54
Loss: 0.14207418493003912
ROC train: 0.970886	val: 0.890779	test: 0.813837
PRC train: 0.861094	val: 0.671305	test: 0.644218

Epoch: 55
Loss: 0.13842168074258296
ROC train: 0.970376	val: 0.889805	test: 0.792852
PRC train: 0.862130	val: 0.638816	test: 0.640329

Epoch: 56
Loss: 0.14409885661240945
ROC train: 0.970751	val: 0.900144	test: 0.796300
PRC train: 0.862375	val: 0.628167	test: 0.654535

Epoch: 57
Loss: 0.13957941557852846
ROC train: 0.973877	val: 0.905401	test: 0.823609
PRC train: 0.875554	val: 0.661240	test: 0.673030

Epoch: 58
Loss: 0.13867304262806898
ROC train: 0.975252	val: 0.890754	test: 0.795202
PRC train: 0.876363	val: 0.672895	test: 0.660481

Epoch: 59
Loss: 0.15649680058495816
ROC train: 0.971424	val: 0.882724	test: 0.785045
PRC train: 0.859035	val: 0.655087	test: 0.647281

Epoch: 60
Loss: 0.12792908365392403
ROC train: 0.974599	val: 0.870313	test: 0.798623
PRC train: 0.872574	val: 0.645581	test: 0.647495

Epoch: 61
Loss: 0.14046279112458243
ROC train: 0.975561	val: 0.869726	test: 0.803909
PRC train: 0.874984	val: 0.642260	test: 0.664745

Epoch: 62
Loss: 0.13902014490708509
ROC train: 0.977938	val: 0.876856	test: 0.822159
PRC train: 0.881942	val: 0.660668	test: 0.672890

Epoch: 63
Loss: 0.13199875755029294
ROC train: 0.978832	val: 0.888705	test: 0.841553
PRC train: 0.885586	val: 0.649470	test: 0.681970

Epoch: 64
Loss: 0.1291225951599357
ROC train: 0.977524	val: 0.855241	test: 0.820766
PRC train: 0.881700	val: 0.615514	test: 0.655619

Epoch: 65
Loss: 0.13041161347889538
ROC train: 0.974877	val: 0.839083	test: 0.786607
PRC train: 0.879058	val: 0.609684	test: 0.629581

Epoch: 66
Loss: 0.1380259464743923
ROC train: 0.972615	val: 0.861334	test: 0.791496
PRC train: 0.873421	val: 0.606044	test: 0.633291

Epoch: 67
Loss: 0.12687444131615427
ROC train: 0.977605	val: 0.848448	test: 0.815750
PRC train: 0.884452	val: 0.648750	test: 0.646000

Epoch: 68
Loss: 0.12039519856885457
ROC train: 0.981121	val: 0.852443	test: 0.820359
PRC train: 0.895501	val: 0.700488	test: 0.652160

Epoch: 69
Loss: 0.12475143712887433
ROC train: 0.979682	val: 0.859823	test: 0.802108
PRC train: 0.891275	val: 0.694633	test: 0.651591

Epoch: 70
Loss: 0.133433032597733
ROC train: 0.974709	val: 0.836873	test: 0.770878
PRC train: 0.867740	val: 0.727962	test: 0.622537

Epoch: 71
Loss: 0.1281491815572505
ROC train: 0.975226	val: 0.808078	test: 0.788153
PRC train: 0.869143	val: 0.658246	test: 0.620838

Epoch: 72
Loss: 0.12931589770090327
ROC train: 0.980958	val: 0.836535	test: 0.798410
PRC train: 0.892262	val: 0.617325	test: 0.651294

Epoch: 73
Loss: 0.12029499818299623
ROC train: 0.980390	val: 0.827645	test: 0.768943
PRC train: 0.892283	val: 0.607799	test: 0.638324

Epoch: 74
Loss: 0.12792745023229368
ROC train: 0.979979	val: 0.857676	test: 0.813195
PRC train: 0.890938	val: 0.667199	test: 0.658517

Epoch: 75
Loss: 0.122971201851826
ROC train: 0.978043	val: 0.868777	test: 0.826067
PRC train: 0.884647	val: 0.630647	test: 0.659615

Epoch: 76
Loss: 0.13655369338666065
ROC train: 0.979530	val: 0.855666	test: 0.826055
PRC train: 0.893872	val: 0.603834	test: 0.654083

Epoch: 77
Loss: 0.12688514875182855
ROC train: 0.980852	val: 0.874034	test: 0.834187
PRC train: 0.897139	val: 0.636091	test: 0.654205

Epoch: 78
Loss: 0.12281104627736443
ROC train: 0.982120	val: 0.860361	test: 0.808316
PRC train: 0.897248	val: 0.639292	test: 0.639610

Epoch: 79
Loss: 0.11393034052063115
ROC train: 0.982495	val: 0.846438	test: 0.781434
PRC train: 0.902084	val: 0.634939	test: 0.627394

Epoch: 80
Loss: 0.11046333266690682
ROC train: 0.981280	val: 0.873609	test: 0.816224
PRC train: 0.897479	val: 0.622642	test: 0.644649

Epoch: 81
Loss: 0.11431879918891397
ROC train: 0.981995	val: 0.844589	test: 0.812989
PRC train: 0.898620	val: 0.589844	test: 0.644439

Epoch: 82
Loss: 0.11847084721766947
ROC train: 0.981323	val: 0.808914	test: 0.810053
PRC train: 0.898371	val: 0.556205	test: 0.639949

Epoch: 83
Loss: 0.12042604379838559
ROC train: 0.982157	val: 0.848972	test: 0.809952
PRC train: 0.904894	val: 0.615565	test: 0.645974

Epoch: 84
Loss: 0.11695413864852366
ROC train: 0.981095	val: 0.882050	test: 0.826616
PRC train: 0.901315	val: 0.648655	test: 0.654557

Epoch: 85
Loss: 0.1163773112424662
ROC train: 0.978897	val: 0.839332	test: 0.806146
PRC train: 0.893966	val: 0.632366	test: 0.639914

Epoch: 86
Loss: 0.12031570367701724
ROC train: 0.981057	val: 0.848610	test: 0.813673
PRC train: 0.899902	val: 0.628330	test: 0.651923

Epoch: 87
Loss: 0.11712556597673651
ROC train: 0.983548	val: 0.821600	test: 0.820669
PRC train: 0.907343	val: 0.612670	test: 0.651759

Epoch: 88
Loss: 0.11612544369847747
ROC train: 0.982885	val: 0.812348	test: 0.801395
PRC train: 0.907688	val: 0.629541	test: 0.637871

Epoch: 89
Loss: 0.11504609355805222
ROC train: 0.984466	val: 0.837934	test: 0.823706
PRC train: 0.910829	val: 0.599538	test: 0.649682

Epoch: 90
Loss: 0.11172189886014654
ROC train: 0.983071	val: 0.828231	test: 0.857409
PRC train: 0.902667	val: 0.592298	test: 0.674250

Epoch: 91
Loss: 0.1212737956722938
ROC train: 0.983588	val: 0.790409	test: 0.812818
PRC train: 0.909100	val: 0.577862	test: 0.645378

Epoch: 92
Loss: 0.11733452550876358
ROC train: 0.981026	val: 0.831253	test: 0.833500
PRC train: 0.900475	val: 0.588227	test: 0.658599

Epoch: 93
Loss: 0.11739513870386127
ROC train: 0.981719	val: 0.844140	test: 0.839446
PRC train: 0.903773	val: 0.616962	test: 0.652320

Epoch: 94
Loss: 0.10842441037771973
ROC train: 0.954410	val: 0.886520	test: 0.755029
PRC train: 0.833079	val: 0.620983	test: 0.575822

Epoch: 34
Loss: 0.1597357130031131
ROC train: 0.956664	val: 0.832926	test: 0.772379
PRC train: 0.838226	val: 0.566080	test: 0.576477

Epoch: 35
Loss: 0.16224322947086683
ROC train: 0.959928	val: 0.843416	test: 0.765009
PRC train: 0.842110	val: 0.573111	test: 0.567625

Epoch: 36
Loss: 0.16338821720989172
ROC train: 0.955161	val: 0.843602	test: 0.753505
PRC train: 0.835178	val: 0.626174	test: 0.578956

Epoch: 37
Loss: 0.15565723439113852
ROC train: 0.959493	val: 0.848610	test: 0.760822
PRC train: 0.838388	val: 0.609654	test: 0.586516

Epoch: 38
Loss: 0.1497268648412504
ROC train: 0.962140	val: 0.841255	test: 0.722748
PRC train: 0.840857	val: 0.624856	test: 0.562071

Epoch: 39
Loss: 0.14969132617659994
ROC train: 0.955426	val: 0.809551	test: 0.717153
PRC train: 0.831875	val: 0.636646	test: 0.561981

Epoch: 40
Loss: 0.15310814629339012
ROC train: 0.960513	val: 0.819253	test: 0.727858
PRC train: 0.834586	val: 0.596621	test: 0.563920

Epoch: 41
Loss: 0.14845567729340167
ROC train: 0.962829	val: 0.852605	test: 0.734078
PRC train: 0.836285	val: 0.609043	test: 0.583967

Epoch: 42
Loss: 0.13975906782377961
ROC train: 0.969210	val: 0.889854	test: 0.744764
PRC train: 0.859889	val: 0.649309	test: 0.577730

Epoch: 43
Loss: 0.14920080262678917
ROC train: 0.970532	val: 0.892676	test: 0.746651
PRC train: 0.862397	val: 0.641846	test: 0.562930

Epoch: 44
Loss: 0.14154482707200394
ROC train: 0.971311	val: 0.859960	test: 0.752261
PRC train: 0.870045	val: 0.626337	test: 0.567366

Epoch: 45
Loss: 0.1357612088076205
ROC train: 0.966271	val: 0.825322	test: 0.759679
PRC train: 0.856055	val: 0.603695	test: 0.589575

Epoch: 46
Loss: 0.14994523862328724
ROC train: 0.968620	val: 0.832789	test: 0.751701
PRC train: 0.857744	val: 0.624302	test: 0.597047

Epoch: 47
Loss: 0.14319215976063776
ROC train: 0.971331	val: 0.805731	test: 0.768427
PRC train: 0.867259	val: 0.610378	test: 0.600240

Epoch: 48
Loss: 0.12949697990365677
ROC train: 0.971215	val: 0.766296	test: 0.752840
PRC train: 0.867159	val: 0.631012	test: 0.586198

Epoch: 49
Loss: 0.13432133969860127
ROC train: 0.967386	val: 0.751312	test: 0.725994
PRC train: 0.858192	val: 0.560175	test: 0.588577

Epoch: 50
Loss: 0.13120567471846517
ROC train: 0.969969	val: 0.832027	test: 0.765394
PRC train: 0.863399	val: 0.590047	test: 0.574861

Epoch: 51
Loss: 0.14702379511723457
ROC train: 0.972640	val: 0.855353	test: 0.775737
PRC train: 0.863702	val: 0.610204	test: 0.582761

Epoch: 52
Loss: 0.13364667804468308
ROC train: 0.975399	val: 0.856615	test: 0.769197
PRC train: 0.873056	val: 0.599566	test: 0.595291

Epoch: 53
Loss: 0.13326345625928157
ROC train: 0.973733	val: 0.831728	test: 0.746718
PRC train: 0.870681	val: 0.614743	test: 0.567444

Epoch: 54
Loss: 0.1204518299578841
ROC train: 0.976919	val: 0.791158	test: 0.748944
PRC train: 0.884922	val: 0.621978	test: 0.573567

Epoch: 55
Loss: 0.13674477445774125
ROC train: 0.975016	val: 0.795178	test: 0.765846
PRC train: 0.875728	val: 0.628186	test: 0.586018

Epoch: 56
Loss: 0.13399178615986623
ROC train: 0.974913	val: 0.829855	test: 0.740338
PRC train: 0.873190	val: 0.592448	test: 0.581029

Epoch: 57
Loss: 0.13077864360183603
ROC train: 0.977227	val: 0.830442	test: 0.720963
PRC train: 0.880885	val: 0.607006	test: 0.591660

Epoch: 58
Loss: 0.1290467859587036
ROC train: 0.976790	val: 0.784253	test: 0.730869
PRC train: 0.879373	val: 0.616271	test: 0.599324

Epoch: 59
Loss: 0.14138038723132762
ROC train: 0.979017	val: 0.799462	test: 0.753628
PRC train: 0.885753	val: 0.602382	test: 0.602266

Epoch: 60
Loss: 0.13161097367522231
ROC train: 0.979074	val: 0.826334	test: 0.762933
PRC train: 0.887061	val: 0.583903	test: 0.609258

Epoch: 61
Loss: 0.13507300321646692
ROC train: 0.977041	val: 0.813947	test: 0.763583
PRC train: 0.876687	val: 0.591446	test: 0.593993

Epoch: 62
Loss: 0.13085831237732073
ROC train: 0.971879	val: 0.798875	test: 0.789894
PRC train: 0.871365	val: 0.599496	test: 0.601956

Epoch: 63
Loss: 0.14102251306630795
ROC train: 0.978432	val: 0.825610	test: 0.786943
PRC train: 0.888088	val: 0.587755	test: 0.600796

Epoch: 64
Loss: 0.12998522373418392
ROC train: 0.978085	val: 0.808303	test: 0.779872
PRC train: 0.886381	val: 0.582111	test: 0.607167

Epoch: 65
Loss: 0.13464610176589123
ROC train: 0.978990	val: 0.822451	test: 0.780660
PRC train: 0.889779	val: 0.571071	test: 0.610346

Epoch: 66
Loss: 0.1305528282690201
ROC train: 0.979644	val: 0.847436	test: 0.792811
PRC train: 0.889939	val: 0.644733	test: 0.642168

Epoch: 67
Loss: 0.1169795759128375
ROC train: 0.973663	val: 0.826970	test: 0.745448
PRC train: 0.880939	val: 0.615188	test: 0.610609

Epoch: 68
Loss: 0.11314326199042353
ROC train: 0.968734	val: 0.755708	test: 0.692294
PRC train: 0.869203	val: 0.556514	test: 0.596372

Epoch: 69
Loss: 0.11602674746994097
ROC train: 0.974814	val: 0.763586	test: 0.719868
PRC train: 0.882352	val: 0.606167	test: 0.584843

Epoch: 70
Loss: 0.13086394669894771
ROC train: 0.977681	val: 0.799100	test: 0.711617
PRC train: 0.888264	val: 0.593580	test: 0.581682

Epoch: 71
Loss: 0.12077829394075808
ROC train: 0.980843	val: 0.843254	test: 0.720698
PRC train: 0.894640	val: 0.592462	test: 0.578292

Epoch: 72
Loss: 0.12480540609448894
ROC train: 0.981928	val: 0.863994	test: 0.736524
PRC train: 0.896547	val: 0.623520	test: 0.587990

Epoch: 73
Loss: 0.1267000805600719
ROC train: 0.981988	val: 0.846912	test: 0.737536
PRC train: 0.899681	val: 0.604564	test: 0.581380

Epoch: 74
Loss: 0.12211273428265783
ROC train: 0.981691	val: 0.844677	test: 0.756138
PRC train: 0.894902	val: 0.582799	test: 0.608339

Epoch: 75
Loss: 0.11918211051510916
ROC train: 0.981596	val: 0.853231	test: 0.774139
PRC train: 0.896739	val: 0.589905	test: 0.614147

Epoch: 76
Loss: 0.12700998468674482
ROC train: 0.981871	val: 0.817281	test: 0.755387
PRC train: 0.896037	val: 0.573180	test: 0.613227

Epoch: 77
Loss: 0.11718868300079036
ROC train: 0.981859	val: 0.852282	test: 0.762895
PRC train: 0.897254	val: 0.620470	test: 0.616044

Epoch: 78
Loss: 0.12379125578530359
ROC train: 0.981942	val: 0.848785	test: 0.754364
PRC train: 0.898604	val: 0.673200	test: 0.615515

Epoch: 79
Loss: 0.12507104276535738
ROC train: 0.981244	val: 0.836735	test: 0.746919
PRC train: 0.899732	val: 0.642023	test: 0.612535

Epoch: 80
Loss: 0.11382620959808094
ROC train: 0.982061	val: 0.826784	test: 0.754390
PRC train: 0.905034	val: 0.607227	test: 0.614720

Epoch: 81
Loss: 0.11947252109082411
ROC train: 0.981586	val: 0.853543	test: 0.785057
PRC train: 0.902410	val: 0.648871	test: 0.627322

Epoch: 82
Loss: 0.10919967417011858
ROC train: 0.979158	val: 0.890079	test: 0.798635
PRC train: 0.893686	val: 0.655487	test: 0.634108

Epoch: 83
Loss: 0.11150921682138966
ROC train: 0.981398	val: 0.848736	test: 0.777986
PRC train: 0.896586	val: 0.597105	test: 0.599895

Epoch: 84
Loss: 0.12767582156858331
ROC train: 0.982157	val: 0.777636	test: 0.763209
PRC train: 0.901855	val: 0.570784	test: 0.616992

Epoch: 85
Loss: 0.1141471141971397
ROC train: 0.981623	val: 0.821414	test: 0.782222
PRC train: 0.901783	val: 0.606447	test: 0.616548

Epoch: 86
Loss: 0.12465432736141184
ROC train: 0.983101	val: 0.839557	test: 0.777250
PRC train: 0.903953	val: 0.613886	test: 0.618109

Epoch: 87
Loss: 0.11311655983257438
ROC train: 0.982649	val: 0.847812	test: 0.757356
PRC train: 0.898995	val: 0.581826	test: 0.605758

Epoch: 88
Loss: 0.11157703623964381
ROC train: 0.981802	val: 0.847499	test: 0.764483
PRC train: 0.891895	val: 0.581769	test: 0.587551

Epoch: 89
Loss: 0.11554527216594676
ROC train: 0.983230	val: 0.872997	test: 0.748720
PRC train: 0.901766	val: 0.606617	test: 0.602812

Epoch: 90
Loss: 0.11156901937754311
ROC train: 0.981778	val: 0.811912	test: 0.738765
PRC train: 0.899417	val: 0.577222	test: 0.603729

Epoch: 91
Loss: 0.10855929785963056
ROC train: 0.983238	val: 0.775112	test: 0.771979
PRC train: 0.907199	val: 0.572282	test: 0.619869

Epoch: 92
Loss: 0.1121946619649707
ROC train: 0.980398	val: 0.769044	test: 0.777149
PRC train: 0.899627	val: 0.565659	test: 0.635024

Epoch: 93
Loss: 0.10801525123285874
ROC train: 0.985271	val: 0.839420	test: 0.803995
PRC train: 0.915725	val: 0.598317	test: 0.630970

Epoch: 94
Loss: 0.13390453760335455
ROC train: 0.949973	val: 0.869251	test: 0.753378
PRC train: 0.812039	val: 0.698134	test: 0.587367

Epoch: 34
Loss: 0.15786689824619538
ROC train: 0.958396	val: 0.833552	test: 0.714867
PRC train: 0.838147	val: 0.639767	test: 0.579449

Epoch: 35
Loss: 0.17483971439425902
ROC train: 0.961847	val: 0.801760	test: 0.750569
PRC train: 0.840580	val: 0.635623	test: 0.588458

Epoch: 36
Loss: 0.1580033803655085
ROC train: 0.962347	val: 0.802846	test: 0.756971
PRC train: 0.836420	val: 0.606705	test: 0.600596

Epoch: 37
Loss: 0.16651260823699127
ROC train: 0.960911	val: 0.792982	test: 0.742373
PRC train: 0.830029	val: 0.593794	test: 0.590600

Epoch: 38
Loss: 0.15395752993496656
ROC train: 0.958685	val: 0.796590	test: 0.763986
PRC train: 0.829687	val: 0.603368	test: 0.626803

Epoch: 39
Loss: 0.15704648800977777
ROC train: 0.965824	val: 0.804420	test: 0.761099
PRC train: 0.846011	val: 0.597048	test: 0.623016

Epoch: 40
Loss: 0.15483831990230573
ROC train: 0.965829	val: 0.814122	test: 0.773552
PRC train: 0.852096	val: 0.615366	test: 0.635036

Epoch: 41
Loss: 0.1537197004600018
ROC train: 0.969087	val: 0.825585	test: 0.773701
PRC train: 0.860782	val: 0.631770	test: 0.628764

Epoch: 42
Loss: 0.13897215531161705
ROC train: 0.969142	val: 0.823287	test: 0.756713
PRC train: 0.859313	val: 0.623499	test: 0.630312

Epoch: 43
Loss: 0.1507515756175148
ROC train: 0.970469	val: 0.842692	test: 0.729737
PRC train: 0.863513	val: 0.647223	test: 0.618121

Epoch: 44
Loss: 0.14770605300014267
ROC train: 0.969596	val: 0.831591	test: 0.736427
PRC train: 0.851096	val: 0.671352	test: 0.611812

Epoch: 45
Loss: 0.14836621519736778
ROC train: 0.966305	val: 0.865080	test: 0.786495
PRC train: 0.851787	val: 0.726205	test: 0.643300

Epoch: 46
Loss: 0.14772825058231348
ROC train: 0.969207	val: 0.863320	test: 0.789087
PRC train: 0.858679	val: 0.693820	test: 0.645439

Epoch: 47
Loss: 0.1357523566619641
ROC train: 0.970549	val: 0.811550	test: 0.776451
PRC train: 0.864288	val: 0.626036	test: 0.629770

Epoch: 48
Loss: 0.13976677335660814
ROC train: 0.973197	val: 0.827957	test: 0.784726
PRC train: 0.875759	val: 0.662067	test: 0.631592

Epoch: 49
Loss: 0.14042190582336067
ROC train: 0.968286	val: 0.851382	test: 0.764882
PRC train: 0.863592	val: 0.657986	test: 0.621253

Epoch: 50
Loss: 0.14256557850198506
ROC train: 0.975119	val: 0.825135	test: 0.761509
PRC train: 0.874750	val: 0.630396	test: 0.614979

Epoch: 51
Loss: 0.13125027351139204
ROC train: 0.974911	val: 0.826221	test: 0.788254
PRC train: 0.870728	val: 0.621354	test: 0.622355

Epoch: 52
Loss: 0.12976936063082367
ROC train: 0.976231	val: 0.820216	test: 0.775225
PRC train: 0.869754	val: 0.608629	test: 0.616045

Epoch: 53
Loss: 0.12708281266489183
ROC train: 0.975676	val: 0.846575	test: 0.778393
PRC train: 0.872716	val: 0.606664	test: 0.625988

Epoch: 54
Loss: 0.1311429956118047
ROC train: 0.974555	val: 0.829317	test: 0.730693
PRC train: 0.873368	val: 0.602269	test: 0.607462

Epoch: 55
Loss: 0.14521425117922332
ROC train: 0.976460	val: 0.814147	test: 0.743147
PRC train: 0.882410	val: 0.650518	test: 0.618901

Epoch: 56
Loss: 0.13390188009184434
ROC train: 0.972055	val: 0.863882	test: 0.771404
PRC train: 0.864231	val: 0.739424	test: 0.622256

Epoch: 57
Loss: 0.13602858377865182
ROC train: 0.970805	val: 0.881389	test: 0.774975
PRC train: 0.866566	val: 0.686787	test: 0.628731

Epoch: 58
Loss: 0.13462470645083444
ROC train: 0.968342	val: 0.810401	test: 0.740622
PRC train: 0.854567	val: 0.667837	test: 0.606943

Epoch: 59
Loss: 0.12427379754674131
ROC train: 0.977914	val: 0.842042	test: 0.787242
PRC train: 0.875318	val: 0.651901	test: 0.628989

Epoch: 60
Loss: 0.13475543698847828
ROC train: 0.974924	val: 0.810787	test: 0.773264
PRC train: 0.872618	val: 0.610931	test: 0.613806

Epoch: 61
Loss: 0.12169752527438518
ROC train: 0.975462	val: 0.839919	test: 0.787029
PRC train: 0.877616	val: 0.680580	test: 0.625215

Epoch: 62
Loss: 0.1347714113554781
ROC train: 0.968067	val: 0.849822	test: 0.738149
PRC train: 0.858955	val: 0.651644	test: 0.605991

Epoch: 63
Loss: 0.11765620495680196
ROC train: 0.975318	val: 0.852668	test: 0.759448
PRC train: 0.883999	val: 0.677689	test: 0.610593

Epoch: 64
Loss: 0.14236989476753742
ROC train: 0.980308	val: 0.849210	test: 0.792400
PRC train: 0.895602	val: 0.692594	test: 0.627470

Epoch: 65
Loss: 0.12266797681346926
ROC train: 0.980525	val: 0.849284	test: 0.782494
PRC train: 0.896423	val: 0.690858	test: 0.618018

Epoch: 66
Loss: 0.1293746512169581
ROC train: 0.979076	val: 0.832339	test: 0.756818
PRC train: 0.886503	val: 0.685439	test: 0.606294

Epoch: 67
Loss: 0.12401712128744483
ROC train: 0.980429	val: 0.848111	test: 0.782808
PRC train: 0.892078	val: 0.653638	test: 0.623951

Epoch: 68
Loss: 0.13040882462826797
ROC train: 0.978429	val: 0.857040	test: 0.765245
PRC train: 0.885460	val: 0.650101	test: 0.634433

Epoch: 69
Loss: 0.12830385099874647
ROC train: 0.979587	val: 0.858326	test: 0.778998
PRC train: 0.888389	val: 0.660130	test: 0.633947

Epoch: 70
Loss: 0.1344869568700101
ROC train: 0.981470	val: 0.854404	test: 0.805575
PRC train: 0.901144	val: 0.669019	test: 0.637277

Epoch: 71
Loss: 0.12932642418998724
ROC train: 0.980259	val: 0.861534	test: 0.803419
PRC train: 0.896330	val: 0.656973	test: 0.638779

Epoch: 72
Loss: 0.12133500592324875
ROC train: 0.980431	val: 0.857339	test: 0.786200
PRC train: 0.892025	val: 0.635378	test: 0.633360

Epoch: 73
Loss: 0.1263925116446515
ROC train: 0.978845	val: 0.820876	test: 0.766257
PRC train: 0.885374	val: 0.605994	test: 0.629321

Epoch: 74
Loss: 0.12741538847859832
ROC train: 0.981012	val: 0.820789	test: 0.767243
PRC train: 0.891638	val: 0.634630	test: 0.622790

Epoch: 75
Loss: 0.1195567264906168
ROC train: 0.982162	val: 0.815345	test: 0.769921
PRC train: 0.896149	val: 0.631874	test: 0.617345

Epoch: 76
Loss: 0.11226710677852306
ROC train: 0.982964	val: 0.823062	test: 0.788478
PRC train: 0.904036	val: 0.656616	test: 0.624262

Epoch: 77
Loss: 0.11301652845321186
ROC train: 0.982147	val: 0.829967	test: 0.797447
PRC train: 0.900008	val: 0.670789	test: 0.633036

Epoch: 78
Loss: 0.12029162329451708
ROC train: 0.981942	val: 0.797427	test: 0.778184
PRC train: 0.895881	val: 0.616668	test: 0.625861

Epoch: 79
Loss: 0.10597860559455523
ROC train: 0.978176	val: 0.757131	test: 0.751484
PRC train: 0.884630	val: 0.604630	test: 0.610981

Epoch: 80
Loss: 0.10912867418280356
ROC train: 0.979197	val: 0.787886	test: 0.757061
PRC train: 0.889280	val: 0.628568	test: 0.612369

Epoch: 81
Loss: 0.113407091983205
ROC train: 0.979742	val: 0.808078	test: 0.765137
PRC train: 0.888218	val: 0.663353	test: 0.613657

Epoch: 82
Loss: 0.11141729430500764
ROC train: 0.984022	val: 0.842716	test: 0.803793
PRC train: 0.907665	val: 0.648241	test: 0.638744

Epoch: 83
Loss: 0.10920298322534439
ROC train: 0.984140	val: 0.851182	test: 0.829219
PRC train: 0.910070	val: 0.640337	test: 0.648368

Epoch: 84
Loss: 0.11281931441176662
ROC train: 0.983424	val: 0.833850	test: 0.805444
PRC train: 0.906026	val: 0.632226	test: 0.628605

Epoch: 85
Loss: 0.11088792883612741
ROC train: 0.983802	val: 0.839445	test: 0.816837
PRC train: 0.907287	val: 0.651574	test: 0.634264

Epoch: 86
Loss: 0.11796139583178891
ROC train: 0.983105	val: 0.874171	test: 0.831565
PRC train: 0.904554	val: 0.681981	test: 0.646774

Epoch: 87
Loss: 0.11034444267165602
ROC train: 0.981382	val: 0.846350	test: 0.798287
PRC train: 0.902970	val: 0.672148	test: 0.636888

Epoch: 88
Loss: 0.10456932534099954
ROC train: 0.982973	val: 0.837621	test: 0.791522
PRC train: 0.909833	val: 0.688635	test: 0.628780

Epoch: 89
Loss: 0.10722777559586316
ROC train: 0.983820	val: 0.838682	test: 0.787305
PRC train: 0.910705	val: 0.656760	test: 0.625192

Epoch: 90
Loss: 0.11231300174915608
ROC train: 0.983269	val: 0.854292	test: 0.789416
PRC train: 0.907209	val: 0.633912	test: 0.627271

Epoch: 91
Loss: 0.11263766985938439
ROC train: 0.985037	val: 0.864556	test: 0.797817
PRC train: 0.913554	val: 0.643699	test: 0.636676

Epoch: 92
Loss: 0.10751296896278018
ROC train: 0.984175	val: 0.878142	test: 0.811719
PRC train: 0.910641	val: 0.691322	test: 0.652627

Epoch: 93
Loss: 0.10926077161020147
ROC train: 0.985330	val: 0.868689	test: 0.825006
PRC train: 0.914899	val: 0.655530	test: 0.649010

Epoch: 94
Loss: 0.09958144138084776
PRC train: 0.977324	val: 0.554609	test: 0.523311

Epoch: 33
Loss: 0.08422240294897079
ROC train: 0.995271	val: 0.813047	test: 0.575993
PRC train: 0.983104	val: 0.551963	test: 0.521021

Epoch: 34
Loss: 0.07612020937190497
ROC train: 0.996813	val: 0.831928	test: 0.590295
PRC train: 0.990187	val: 0.573400	test: 0.522436

Epoch: 35
Loss: 0.06670545009795151
ROC train: 0.997483	val: 0.789710	test: 0.604997
PRC train: 0.991145	val: 0.620996	test: 0.529774

Epoch: 36
Loss: 0.06482646065785516
ROC train: 0.998281	val: 0.786038	test: 0.586560
PRC train: 0.992102	val: 0.545231	test: 0.524214

Epoch: 37
Loss: 0.06539422430192281
ROC train: 0.998575	val: 0.750113	test: 0.555643
PRC train: 0.992451	val: 0.535691	test: 0.518098

Epoch: 38
Loss: 0.06839245273591091
ROC train: 0.998516	val: 0.791495	test: 0.568959
PRC train: 0.992300	val: 0.548151	test: 0.521295

Epoch: 39
Loss: 0.06270072842199695
ROC train: 0.998143	val: 0.786913	test: 0.596604
PRC train: 0.993330	val: 0.580654	test: 0.525108

Epoch: 40
Loss: 0.06473174629966023
ROC train: 0.997803	val: 0.789622	test: 0.595293
PRC train: 0.993015	val: 0.557803	test: 0.525043

Epoch: 41
Loss: 0.052530992091910755
ROC train: 0.997729	val: 0.804107	test: 0.570184
PRC train: 0.993037	val: 0.577995	test: 0.520427

Epoch: 42
Loss: 0.06481847577552353
ROC train: 0.997697	val: 0.763737	test: 0.566113
PRC train: 0.992655	val: 0.556365	test: 0.520043

Epoch: 43
Loss: 0.053919761509930955
ROC train: 0.998444	val: 0.786600	test: 0.557567
PRC train: 0.993494	val: 0.554404	test: 0.521106

Epoch: 44
Loss: 0.05563598654571265
ROC train: 0.999143	val: 0.823199	test: 0.565810
PRC train: 0.994136	val: 0.557991	test: 0.520969

Epoch: 45
Loss: 0.048663797669450164
ROC train: 0.999543	val: 0.832227	test: 0.560753
PRC train: 0.996785	val: 0.558006	test: 0.519058

Epoch: 46
Loss: 0.05886561709039249
ROC train: 0.999639	val: 0.851793	test: 0.545289
PRC train: 0.997539	val: 0.563829	test: 0.516086

Epoch: 47
Loss: 0.0539661224571112
ROC train: 0.998632	val: 0.829679	test: 0.505728
PRC train: 0.993819	val: 0.558747	test: 0.511444

Epoch: 48
Loss: 0.04660270987464446
ROC train: 0.998638	val: 0.833938	test: 0.519280
PRC train: 0.993309	val: 0.569433	test: 0.513601

Epoch: 49
Loss: 0.05778267450688877
ROC train: 0.999079	val: 0.819566	test: 0.580341
PRC train: 0.995189	val: 0.567289	test: 0.522660

Epoch: 50
Loss: 0.04765515839037738
ROC train: 0.998891	val: 0.816558	test: 0.603974
PRC train: 0.995755	val: 0.560361	test: 0.525752

Epoch: 51
Loss: 0.047785845164279805
ROC train: 0.998693	val: 0.751638	test: 0.587232
PRC train: 0.995027	val: 0.553947	test: 0.525507

Epoch: 52
Loss: 0.05004708316194491
ROC train: 0.998796	val: 0.778834	test: 0.587146
PRC train: 0.994215	val: 0.547165	test: 0.524456

Epoch: 53
Loss: 0.042927881990786974
ROC train: 0.999576	val: 0.826133	test: 0.557391
PRC train: 0.996572	val: 0.555661	test: 0.519925

Epoch: 54
Loss: 0.04182952879740929
ROC train: 0.999765	val: 0.842653	test: 0.569990
PRC train: 0.998155	val: 0.566404	test: 0.521410

Epoch: 55
Loss: 0.04797302632704399
ROC train: 0.999746	val: 0.826706	test: 0.550372
PRC train: 0.998715	val: 0.574917	test: 0.519357

Epoch: 56
Loss: 0.04374347559584533
ROC train: 0.999636	val: 0.820139	test: 0.538154
PRC train: 0.998362	val: 0.562783	test: 0.517980

Epoch: 57
Loss: 0.0476305209736356
ROC train: 0.999889	val: 0.850708	test: 0.551381
PRC train: 0.999253	val: 0.568230	test: 0.522355

Epoch: 58
Loss: 0.04098036717143651
ROC train: 0.999827	val: 0.848922	test: 0.578552
PRC train: 0.998789	val: 0.561087	test: 0.525805

Epoch: 59
Loss: 0.03649317172935166
ROC train: 0.999942	val: 0.826770	test: 0.599152
PRC train: 0.999454	val: 0.556768	test: 0.527321

Epoch: 60
Loss: 0.03125416452954185
ROC train: 0.999940	val: 0.815106	test: 0.600664
PRC train: 0.999533	val: 0.561509	test: 0.527315

Epoch: 61
Loss: 0.028018323628803167
ROC train: 0.999897	val: 0.811111	test: 0.573419
PRC train: 0.999107	val: 0.556438	test: 0.523202

Epoch: 62
Loss: 0.037901387834302594
ROC train: 0.999986	val: 0.812084	test: 0.574700
PRC train: 0.999842	val: 0.564657	test: 0.524300

Epoch: 63
Loss: 0.027965701677258908
ROC train: 0.999870	val: 0.817654	test: 0.564806
PRC train: 0.998857	val: 0.562865	test: 0.523110

Epoch: 64
Loss: 0.0317658029386812
ROC train: 0.999801	val: 0.821165	test: 0.548296
PRC train: 0.998522	val: 0.569184	test: 0.522322

Epoch: 65
Loss: 0.029041083895437646
ROC train: 0.999933	val: 0.836823	test: 0.561724
PRC train: 0.999337	val: 0.576336	test: 0.523993

Epoch: 66
Loss: 0.02904841687818588
ROC train: 0.999994	val: 0.827008	test: 0.576777
PRC train: 1.000000	val: 0.561876	test: 0.527235

Epoch: 67
Loss: 0.027371495302586597
ROC train: 0.999994	val: 0.796165	test: 0.557216
PRC train: 1.000000	val: 0.546734	test: 0.522329

Epoch: 68
Loss: 0.028098853035052485
ROC train: 0.999983	val: 0.791720	test: 0.552856
PRC train: 0.999999	val: 0.545856	test: 0.526557

Epoch: 69
Loss: 0.02617747402600539
ROC train: 0.999974	val: 0.823424	test: 0.565011
PRC train: 0.999841	val: 0.566921	test: 0.525180

Epoch: 70
Loss: 0.026018496579143992
ROC train: 0.999986	val: 0.828094	test: 0.566217
PRC train: 0.999842	val: 0.573017	test: 0.524234

Epoch: 71
Loss: 0.028225244513344894
ROC train: 1.000000	val: 0.786277	test: 0.532663
PRC train: 1.000000	val: 0.551102	test: 0.520526

Epoch: 72
Loss: 0.03164650895159836
ROC train: 0.999990	val: 0.810127	test: 0.518529
PRC train: 0.999894	val: 0.551920	test: 0.522662

Epoch: 73
Loss: 0.02834901730480125
ROC train: 1.000000	val: 0.817355	test: 0.530471
PRC train: 1.000000	val: 0.555025	test: 0.524747

Epoch: 74
Loss: 0.02560259233809954
ROC train: 1.000000	val: 0.822075	test: 0.574428
PRC train: 1.000000	val: 0.555142	test: 0.525965

Epoch: 75
Loss: 0.030263390366365734
ROC train: 1.000000	val: 0.828456	test: 0.581798
PRC train: 1.000000	val: 0.555811	test: 0.525740

Epoch: 76
Loss: 0.023892034507118555
ROC train: 0.999986	val: 0.810650	test: 0.530519
PRC train: 0.999842	val: 0.553106	test: 0.524054

Epoch: 77
Loss: 0.023720509885640197
ROC train: 0.999983	val: 0.809389	test: 0.548210
PRC train: 0.999999	val: 0.554262	test: 0.525937

Epoch: 78
Loss: 0.020282263835660324
ROC train: 0.999989	val: 0.829430	test: 0.585372
PRC train: 0.999999	val: 0.560599	test: 0.529537

Epoch: 79
Loss: 0.019877617837961138
ROC train: 1.000000	val: 0.817292	test: 0.585847
PRC train: 1.000000	val: 0.559452	test: 0.529740

Epoch: 80
Loss: 0.027430743604090375
ROC train: 0.999994	val: 0.819653	test: 0.561287
PRC train: 1.000000	val: 0.555382	test: 0.527042

Epoch: 81
Loss: 0.02381391802880025
ROC train: 0.999994	val: 0.811462	test: 0.533493
PRC train: 1.000000	val: 0.553239	test: 0.524809

Epoch: 82
Loss: 0.02547036044948848
ROC train: 0.999990	val: 0.805692	test: 0.556540
PRC train: 0.999894	val: 0.556489	test: 0.531764

Epoch: 83
Loss: 0.01828307713611385
ROC train: 0.999986	val: 0.806402	test: 0.608781
PRC train: 0.999842	val: 0.568707	test: 0.532561

Epoch: 84
Loss: 0.020590154325500314
ROC train: 0.999983	val: 0.787609	test: 0.593280
PRC train: 0.999999	val: 0.565798	test: 0.538334

Epoch: 85
Loss: 0.021111444354555406
ROC train: 0.999953	val: 0.682857	test: 0.577707
PRC train: 0.999724	val: 0.529548	test: 0.546479

Epoch: 86
Loss: 0.022764874426616222
ROC train: 1.000000	val: 0.728210	test: 0.590830
PRC train: 1.000000	val: 0.539669	test: 0.531271

Epoch: 87
Loss: 0.02954752601060496
ROC train: 0.999989	val: 0.789232	test: 0.581723
PRC train: 0.999999	val: 0.560793	test: 0.528289

Epoch: 88
Loss: 0.01667014372157608
ROC train: 0.999937	val: 0.778718	test: 0.570517
PRC train: 0.999675	val: 0.560482	test: 0.523716

Epoch: 89
Loss: 0.021872529597812523
ROC train: 0.999979	val: 0.787971	test: 0.576064
PRC train: 0.999893	val: 0.561127	test: 0.525398

Epoch: 90
Loss: 0.020073338203899494
ROC train: 1.000000	val: 0.820226	test: 0.584158
PRC train: 1.000000	val: 0.567391	test: 0.529388

Epoch: 91
Loss: 0.02302710099094931
ROC train: 1.000000	val: 0.803594	test: 0.593754
PRC train: 1.000000	val: 0.549547	test: 0.535895

Epoch: 92
Loss: 0.01832444838927531
ROC train: 0.999995	val: 0.812147	test: 0.628067
PRC train: 0.999946	val: 0.551638	test: 0.534168

Epoch: 93
Loss: 0.023289501822660062
PRC train: 0.988404	val: 0.542067	test: 0.587995

Epoch: 33
Loss: 0.08314354274937114
ROC train: 0.999037	val: 0.675604	test: 0.680592
PRC train: 0.995100	val: 0.535747	test: 0.567660

Epoch: 34
Loss: 0.08210017438610065
ROC train: 0.999482	val: 0.650306	test: 0.681178
PRC train: 0.997119	val: 0.524199	test: 0.566523

Epoch: 35
Loss: 0.07474085865374311
ROC train: 0.997749	val: 0.700041	test: 0.650623
PRC train: 0.989133	val: 0.532710	test: 0.557607

Epoch: 36
Loss: 0.07320019331582525
ROC train: 0.999176	val: 0.662918	test: 0.636508
PRC train: 0.994820	val: 0.531563	test: 0.557972

Epoch: 37
Loss: 0.06359618361550298
ROC train: 0.999630	val: 0.689341	test: 0.679830
PRC train: 0.996743	val: 0.529098	test: 0.574349

Epoch: 38
Loss: 0.06581108517407173
ROC train: 0.998883	val: 0.705136	test: 0.700990
PRC train: 0.993266	val: 0.532179	test: 0.566361

Epoch: 39
Loss: 0.054643098768421924
ROC train: 0.999519	val: 0.666464	test: 0.674033
PRC train: 0.995694	val: 0.559791	test: 0.564146

Epoch: 40
Loss: 0.06056984246973414
ROC train: 0.999702	val: 0.701640	test: 0.687274
PRC train: 0.997036	val: 0.538169	test: 0.571393

Epoch: 41
Loss: 0.053551847582863285
ROC train: 0.999783	val: 0.734244	test: 0.690759
PRC train: 0.997778	val: 0.535534	test: 0.580839

Epoch: 42
Loss: 0.05145315854853334
ROC train: 0.999676	val: 0.736117	test: 0.697692
PRC train: 0.997636	val: 0.540147	test: 0.594611

Epoch: 43
Loss: 0.045858609829603506
ROC train: 0.999751	val: 0.783754	test: 0.700153
PRC train: 0.998360	val: 0.555531	test: 0.598568

Epoch: 44
Loss: 0.04631861057750851
ROC train: 0.999938	val: 0.768857	test: 0.698054
PRC train: 0.999622	val: 0.545115	test: 0.576672

Epoch: 45
Loss: 0.04184774604739218
ROC train: 0.999942	val: 0.718810	test: 0.699616
PRC train: 0.999425	val: 0.535199	test: 0.574563

Epoch: 46
Loss: 0.04334308641343506
ROC train: 0.999951	val: 0.647172	test: 0.701446
PRC train: 0.999526	val: 0.521929	test: 0.574777

Epoch: 47
Loss: 0.04476735230184092
ROC train: 0.999952	val: 0.658610	test: 0.693157
PRC train: 0.999471	val: 0.526439	test: 0.578282

Epoch: 48
Loss: 0.03820834441041228
ROC train: 0.999942	val: 0.703464	test: 0.671560
PRC train: 0.999434	val: 0.548908	test: 0.580668

Epoch: 49
Loss: 0.03570821158133779
ROC train: 0.999966	val: 0.711293	test: 0.661079
PRC train: 0.999693	val: 0.530751	test: 0.579886

Epoch: 50
Loss: 0.040849781239179206
ROC train: 0.999966	val: 0.680450	test: 0.665807
PRC train: 0.999628	val: 0.537650	test: 0.589333

Epoch: 51
Loss: 0.037485854562575874
ROC train: 0.999870	val: 0.654477	test: 0.650698
PRC train: 0.998568	val: 0.545602	test: 0.583291

Epoch: 52
Loss: 0.04436246626522121
ROC train: 0.999942	val: 0.706198	test: 0.666076
PRC train: 0.999385	val: 0.545215	test: 0.601927

Epoch: 53
Loss: 0.040012063679974806
ROC train: 0.999904	val: 0.727725	test: 0.693034
PRC train: 0.998968	val: 0.549480	test: 0.580853

Epoch: 54
Loss: 0.0373771236558969
ROC train: 0.999966	val: 0.688553	test: 0.707149
PRC train: 0.999625	val: 0.535171	test: 0.577903

Epoch: 55
Loss: 0.034894787358782785
ROC train: 0.999985	val: 0.678489	test: 0.696306
PRC train: 0.999892	val: 0.524449	test: 0.573853

Epoch: 56
Loss: 0.05499798031894719
ROC train: 0.999989	val: 0.703014	test: 0.696818
PRC train: 0.999999	val: 0.526648	test: 0.579116

Epoch: 57
Loss: 0.03215108499700825
ROC train: 0.999981	val: 0.707210	test: 0.675082
PRC train: 0.999788	val: 0.531432	test: 0.563633

Epoch: 58
Loss: 0.03408888964630382
ROC train: 0.999891	val: 0.708159	test: 0.676569
PRC train: 0.999390	val: 0.539787	test: 0.566142

Epoch: 59
Loss: 0.03372317363416186
ROC train: 0.999975	val: 0.681149	test: 0.682803
PRC train: 0.999788	val: 0.538929	test: 0.563120

Epoch: 60
Loss: 0.026884265406956147
ROC train: 1.000000	val: 0.701914	test: 0.677906
PRC train: 1.000000	val: 0.538017	test: 0.564594

Epoch: 61
Loss: 0.029984893003791157
ROC train: 0.999913	val: 0.703474	test: 0.673984
PRC train: 0.999787	val: 0.539051	test: 0.570045

Epoch: 62
Loss: 0.029615735410847978
ROC train: 0.999977	val: 0.687155	test: 0.681865
PRC train: 0.999998	val: 0.531394	test: 0.581274

Epoch: 63
Loss: 0.02643171366183299
ROC train: 1.000000	val: 0.666576	test: 0.669124
PRC train: 1.000000	val: 0.533012	test: 0.572296

Epoch: 64
Loss: 0.0232011663452202
ROC train: 0.999986	val: 0.675330	test: 0.664414
PRC train: 0.999842	val: 0.539713	test: 0.561402

Epoch: 65
Loss: 0.024683906175357456
ROC train: 0.999981	val: 0.736841	test: 0.688111
PRC train: 0.999786	val: 0.542776	test: 0.574872

Epoch: 66
Loss: 0.02302623879685627
ROC train: 0.999990	val: 0.739163	test: 0.703526
PRC train: 0.999894	val: 0.536821	test: 0.583573

Epoch: 67
Loss: 0.02985377368923801
ROC train: 1.000000	val: 0.709469	test: 0.715319
PRC train: 1.000000	val: 0.529537	test: 0.591550

Epoch: 68
Loss: 0.020873052613777844
ROC train: 1.000000	val: 0.694534	test: 0.707030
PRC train: 1.000000	val: 0.528354	test: 0.589659

Epoch: 69
Loss: 0.025469292524787624
ROC train: 1.000000	val: 0.726239	test: 0.711284
PRC train: 1.000000	val: 0.536862	test: 0.601166

Epoch: 70
Loss: 0.020706750626106763
ROC train: 1.000000	val: 0.699205	test: 0.701352
PRC train: 1.000000	val: 0.530610	test: 0.573876

Epoch: 71
Loss: 0.026668990043348096
ROC train: 1.000000	val: 0.664229	test: 0.679729
PRC train: 1.000000	val: 0.524643	test: 0.574500

Epoch: 72
Loss: 0.02063694539773977
ROC train: 1.000000	val: 0.656287	test: 0.681615
PRC train: 1.000000	val: 0.523725	test: 0.579520

Epoch: 73
Loss: 0.02026863502642083
ROC train: 1.000000	val: 0.719059	test: 0.689710
PRC train: 1.000000	val: 0.534765	test: 0.573928

Epoch: 74
Loss: 0.021300199004920427
ROC train: 1.000000	val: 0.722644	test: 0.670185
PRC train: 1.000000	val: 0.535534	test: 0.567770

Epoch: 75
Loss: 0.021248941243954312
ROC train: 0.999994	val: 0.668987	test: 0.661478
PRC train: 1.000000	val: 0.532470	test: 0.567424

Epoch: 76
Loss: 0.015184191361902886
ROC train: 1.000000	val: 0.608787	test: 0.656794
PRC train: 1.000000	val: 0.526964	test: 0.565822

Epoch: 77
Loss: 0.018118932388099994
ROC train: 1.000000	val: 0.635884	test: 0.679367
PRC train: 1.000000	val: 0.532100	test: 0.565323

Epoch: 78
Loss: 0.018619470144387897
ROC train: 1.000000	val: 0.639992	test: 0.672983
PRC train: 1.000000	val: 0.536829	test: 0.564921

Epoch: 79
Loss: 0.01563150568767319
ROC train: 1.000000	val: 0.674357	test: 0.664276
PRC train: 1.000000	val: 0.532033	test: 0.556921

Epoch: 80
Loss: 0.01682946480128846
ROC train: 1.000000	val: 0.729686	test: 0.658961
PRC train: 1.000000	val: 0.537849	test: 0.569031

Epoch: 81
Loss: 0.01558802787816182
ROC train: 1.000000	val: 0.733882	test: 0.671821
PRC train: 1.000000	val: 0.537415	test: 0.568873

Epoch: 82
Loss: 0.014209943054925506
ROC train: 1.000000	val: 0.740674	test: 0.687823
PRC train: 1.000000	val: 0.541806	test: 0.577258

Epoch: 83
Loss: 0.017312539528131048
ROC train: 1.000000	val: 0.700642	test: 0.701801
PRC train: 1.000000	val: 0.530103	test: 0.569869

Epoch: 84
Loss: 0.017978311583462293
ROC train: 1.000000	val: 0.649582	test: 0.683438
PRC train: 1.000000	val: 0.524433	test: 0.562748

Epoch: 85
Loss: 0.020648468467191965
ROC train: 1.000000	val: 0.695546	test: 0.715405
PRC train: 1.000000	val: 0.528783	test: 0.570963

Epoch: 86
Loss: 0.015784790747342882
ROC train: 0.999421	val: 0.642027	test: 0.717541
PRC train: 0.998466	val: 0.521390	test: 0.585475

Epoch: 87
Loss: 0.04047888516151623
ROC train: 0.991968	val: 0.672259	test: 0.673009
PRC train: 0.994789	val: 0.524671	test: 0.581120

Epoch: 88
Loss: 0.0237992707985529
ROC train: 0.999990	val: 0.588908	test: 0.597769
PRC train: 0.999894	val: 0.517350	test: 0.546552

Epoch: 89
Loss: 0.025792989423495415
ROC train: 0.997103	val: 0.591006	test: 0.624462
PRC train: 0.995283	val: 0.519468	test: 0.571797

Epoch: 90
Loss: 0.028535376003138758
ROC train: 0.999823	val: 0.635972	test: 0.670958
PRC train: 0.998995	val: 0.520387	test: 0.564655

Epoch: 91
Loss: 0.03154106942742356
ROC train: 0.999995	val: 0.660321	test: 0.647198
PRC train: 0.999946	val: 0.533419	test: 0.553313

Epoch: 92
Loss: 0.02828579230419906
ROC train: 1.000000	val: 0.632300	test: 0.663126
PRC train: 1.000000	val: 0.556369	test: 0.560323

Epoch: 93
Loss: 0.026039865777089964
PRC train: 0.995234	val: 0.592499	test: 0.564185

Epoch: 33
Loss: 0.07905305515820131
ROC train: 0.998670	val: 0.754024	test: 0.584035
PRC train: 0.992866	val: 0.596928	test: 0.543364

Epoch: 34
Loss: 0.08890042684736113
ROC train: 0.999403	val: 0.714990	test: 0.527385
PRC train: 0.996917	val: 0.608658	test: 0.522552

Epoch: 35
Loss: 0.07123220437779793
ROC train: 0.999657	val: 0.748655	test: 0.564667
PRC train: 0.997618	val: 0.656797	test: 0.563590

Epoch: 36
Loss: 0.06780751265164504
ROC train: 0.998992	val: 0.788413	test: 0.587546
PRC train: 0.995178	val: 0.640551	test: 0.530561

Epoch: 37
Loss: 0.07205090765016078
ROC train: 0.998886	val: 0.787601	test: 0.580826
PRC train: 0.994897	val: 0.638095	test: 0.525902

Epoch: 38
Loss: 0.06614712401435417
ROC train: 0.999414	val: 0.802336	test: 0.602125
PRC train: 0.997724	val: 0.605541	test: 0.537363

Epoch: 39
Loss: 0.06393707757005725
ROC train: 0.999336	val: 0.789562	test: 0.619188
PRC train: 0.998223	val: 0.577974	test: 0.554442

Epoch: 40
Loss: 0.057981887119004275
ROC train: 0.999318	val: 0.789200	test: 0.613447
PRC train: 0.999357	val: 0.631251	test: 0.550889

Epoch: 41
Loss: 0.056564851047579065
ROC train: 0.999435	val: 0.809916	test: 0.618325
PRC train: 0.997689	val: 0.664273	test: 0.535734

Epoch: 42
Loss: 0.05634948370392865
ROC train: 0.998374	val: 0.794345	test: 0.579276
PRC train: 0.993757	val: 0.716949	test: 0.524726

Epoch: 43
Loss: 0.05175428871419994
ROC train: 0.999863	val: 0.764675	test: 0.570681
PRC train: 0.998805	val: 0.686913	test: 0.566754

Epoch: 44
Loss: 0.055015848611059945
ROC train: 0.999202	val: 0.728751	test: 0.554130
PRC train: 0.996763	val: 0.615363	test: 0.564462

Epoch: 45
Loss: 0.04869366866135275
ROC train: 0.998515	val: 0.713704	test: 0.555628
PRC train: 0.995931	val: 0.615511	test: 0.563525

Epoch: 46
Loss: 0.04860219785645908
ROC train: 0.999862	val: 0.735143	test: 0.586097
PRC train: 0.999128	val: 0.558536	test: 0.536206

Epoch: 47
Loss: 0.04602073290881999
ROC train: 0.999657	val: 0.802385	test: 0.610570
PRC train: 0.997836	val: 0.593525	test: 0.536608

Epoch: 48
Loss: 0.04672337919380358
ROC train: 0.999924	val: 0.783493	test: 0.574506
PRC train: 0.999788	val: 0.611515	test: 0.540605

Epoch: 49
Loss: 0.04319622540927066
ROC train: 0.999938	val: 0.736918	test: 0.517285
PRC train: 0.999630	val: 0.558156	test: 0.517670

Epoch: 50
Loss: 0.04532453942950052
ROC train: 0.999847	val: 0.803123	test: 0.559752
PRC train: 0.998977	val: 0.608010	test: 0.529263

Epoch: 51
Loss: 0.040732779059101505
ROC train: 0.999890	val: 0.819843	test: 0.541281
PRC train: 0.999780	val: 0.623377	test: 0.525323

Epoch: 52
Loss: 0.04025310619158916
ROC train: 0.999922	val: 0.784281	test: 0.511432
PRC train: 0.999889	val: 0.602531	test: 0.520180

Epoch: 53
Loss: 0.042580459200551556
ROC train: 0.999951	val: 0.802649	test: 0.528921
PRC train: 0.999891	val: 0.603937	test: 0.531103

Epoch: 54
Loss: 0.03834788361238009
ROC train: 0.999960	val: 0.832441	test: 0.562344
PRC train: 0.999687	val: 0.655718	test: 0.562944

Epoch: 55
Loss: 0.03712921741466345
ROC train: 0.999837	val: 0.819956	test: 0.556166
PRC train: 0.998503	val: 0.648919	test: 0.536915

Epoch: 56
Loss: 0.04253600381191353
ROC train: 0.999990	val: 0.790237	test: 0.551725
PRC train: 0.999894	val: 0.633301	test: 0.539085

Epoch: 57
Loss: 0.03584090538335231
ROC train: 0.999921	val: 0.725504	test: 0.533919
PRC train: 0.999625	val: 0.657972	test: 0.556651

Epoch: 58
Loss: 0.03978776536040908
ROC train: 0.999759	val: 0.809828	test: 0.572157
PRC train: 0.999190	val: 0.596068	test: 0.543158

Epoch: 59
Loss: 0.04157410553987563
ROC train: 0.999863	val: 0.734957	test: 0.541602
PRC train: 0.999670	val: 0.545105	test: 0.562173

Epoch: 60
Loss: 0.03623093130332454
ROC train: 0.999945	val: 0.785841	test: 0.560757
PRC train: 0.999890	val: 0.585926	test: 0.562722

Epoch: 61
Loss: 0.032663824793954896
ROC train: 0.999920	val: 0.784080	test: 0.566733
PRC train: 0.999674	val: 0.627004	test: 0.537895

Epoch: 62
Loss: 0.028565558008329106
ROC train: 0.999971	val: 0.770632	test: 0.558713
PRC train: 0.999694	val: 0.581639	test: 0.525676

Epoch: 63
Loss: 0.031893611972563504
ROC train: 0.999938	val: 0.790761	test: 0.571294
PRC train: 0.999303	val: 0.580697	test: 0.524960

Epoch: 64
Loss: 0.022783593625596794
ROC train: 0.999986	val: 0.784917	test: 0.557466
PRC train: 0.999839	val: 0.575220	test: 0.520106

Epoch: 65
Loss: 0.028868357948773016
ROC train: 1.000000	val: 0.763614	test: 0.522851
PRC train: 1.000000	val: 0.577480	test: 0.517998

Epoch: 66
Loss: 0.025375150665021905
ROC train: 0.999989	val: 0.782095	test: 0.534150
PRC train: 0.999999	val: 0.615926	test: 0.536905

Epoch: 67
Loss: 0.025490458109495982
ROC train: 1.000000	val: 0.815809	test: 0.564174
PRC train: 1.000000	val: 0.647165	test: 0.542053

Epoch: 68
Loss: 0.025263105938718822
ROC train: 0.999983	val: 0.834651	test: 0.590482
PRC train: 0.999999	val: 0.620929	test: 0.533087

Epoch: 69
Loss: 0.027704065303937014
ROC train: 1.000000	val: 0.879467	test: 0.613110
PRC train: 1.000000	val: 0.639949	test: 0.535588

Epoch: 70
Loss: 0.028284791836160072
ROC train: 0.999990	val: 0.875158	test: 0.589571
PRC train: 0.999894	val: 0.674167	test: 0.524863

Epoch: 71
Loss: 0.02264396781040835
ROC train: 1.000000	val: 0.800326	test: 0.547466
PRC train: 1.000000	val: 0.653314	test: 0.528975

Epoch: 72
Loss: 0.022346137524251918
ROC train: 1.000000	val: 0.780921	test: 0.541314
PRC train: 1.000000	val: 0.701386	test: 0.537698

Epoch: 73
Loss: 0.024146991040764385
ROC train: 1.000000	val: 0.713454	test: 0.517767
PRC train: 1.000000	val: 0.620643	test: 0.525645

Epoch: 74
Loss: 0.023537034725015865
ROC train: 1.000000	val: 0.708896	test: 0.527710
PRC train: 1.000000	val: 0.590516	test: 0.518572

Epoch: 75
Loss: 0.023198712009716384
ROC train: 1.000000	val: 0.772579	test: 0.568246
PRC train: 1.000000	val: 0.654062	test: 0.527231

Epoch: 76
Loss: 0.020789818079329375
ROC train: 1.000000	val: 0.785254	test: 0.584061
PRC train: 1.000000	val: 0.626221	test: 0.534479

Epoch: 77
Loss: 0.016071880870271883
ROC train: 0.999994	val: 0.805569	test: 0.599701
PRC train: 1.000000	val: 0.638142	test: 0.535446

Epoch: 78
Loss: 0.023884507512802966
ROC train: 0.999972	val: 0.848174	test: 0.597265
PRC train: 0.999998	val: 0.616040	test: 0.539382

Epoch: 79
Loss: 0.023339137181049664
ROC train: 0.999955	val: 0.807305	test: 0.587184
PRC train: 0.999943	val: 0.637047	test: 0.526402

Epoch: 80
Loss: 0.0176656553465535
ROC train: 1.000000	val: 0.803622	test: 0.598326
PRC train: 1.000000	val: 0.591600	test: 0.529443

Epoch: 81
Loss: 0.022630496581258996
ROC train: 1.000000	val: 0.828843	test: 0.611781
PRC train: 1.000000	val: 0.620023	test: 0.538276

Epoch: 82
Loss: 0.02144028376210373
ROC train: 1.000000	val: 0.808479	test: 0.566535
PRC train: 1.000000	val: 0.570205	test: 0.527623

Epoch: 83
Loss: 0.018322557472558905
ROC train: 1.000000	val: 0.778061	test: 0.580576
PRC train: 1.000000	val: 0.563780	test: 0.535776

Epoch: 84
Loss: 0.01475132332175581
ROC train: 1.000000	val: 0.783455	test: 0.593743
PRC train: 1.000000	val: 0.569548	test: 0.542394

Epoch: 85
Loss: 0.025650586621030824
ROC train: 0.999995	val: 0.745882	test: 0.578290
PRC train: 0.999946	val: 0.559862	test: 0.528250

Epoch: 86
Loss: 0.01791474877024516
ROC train: 1.000000	val: 0.783416	test: 0.577416
PRC train: 1.000000	val: 0.583543	test: 0.524830

Epoch: 87
Loss: 0.01893173330837358
ROC train: 1.000000	val: 0.796541	test: 0.573793
PRC train: 1.000000	val: 0.585343	test: 0.522024

Epoch: 88
Loss: 0.021075753255109058
ROC train: 0.999995	val: 0.782320	test: 0.555131
PRC train: 0.999946	val: 0.587781	test: 0.523077

Epoch: 89
Loss: 0.01809526263443216
ROC train: 0.999983	val: 0.802835	test: 0.555968
PRC train: 0.999999	val: 0.599326	test: 0.524451

Epoch: 90
Loss: 0.023190676768947437
ROC train: 0.999984	val: 0.797378	test: 0.562815
PRC train: 0.999945	val: 0.660569	test: 0.520333

Epoch: 91
Loss: 0.01853804276485202
ROC train: 1.000000	val: 0.779909	test: 0.589933
PRC train: 1.000000	val: 0.624337	test: 0.528349

Epoch: 92
Loss: 0.01806576156637384
ROC train: 1.000000	val: 0.742860	test: 0.598539
PRC train: 1.000000	val: 0.548571	test: 0.528569

Epoch: 93
Loss: 0.014205787684283178
PRC train: 0.968936	val: 0.540989	test: 0.579493

Epoch: 33
Loss: 0.10293513029860304
ROC train: 0.991027	val: 0.800372	test: 0.798208
PRC train: 0.963049	val: 0.575608	test: 0.598974

Epoch: 34
Loss: 0.09662577649630365
ROC train: 0.989567	val: 0.809663	test: 0.755480
PRC train: 0.950709	val: 0.553225	test: 0.576364

Epoch: 35
Loss: 0.09553672789948922
ROC train: 0.995660	val: 0.722067	test: 0.756089
PRC train: 0.980661	val: 0.540313	test: 0.564498

Epoch: 36
Loss: 0.093363204375111
ROC train: 0.996305	val: 0.661569	test: 0.738575
PRC train: 0.984244	val: 0.530817	test: 0.558704

Epoch: 37
Loss: 0.08272283620840372
ROC train: 0.997730	val: 0.765161	test: 0.785408
PRC train: 0.987343	val: 0.559745	test: 0.579561

Epoch: 38
Loss: 0.08179376929358749
ROC train: 0.998051	val: 0.782018	test: 0.816769
PRC train: 0.989779	val: 0.582623	test: 0.606309

Epoch: 39
Loss: 0.07836364805594749
ROC train: 0.996178	val: 0.707146	test: 0.836525
PRC train: 0.984336	val: 0.553568	test: 0.619420

Epoch: 40
Loss: 0.09354948497073931
ROC train: 0.997599	val: 0.729647	test: 0.781822
PRC train: 0.983251	val: 0.541375	test: 0.582791

Epoch: 41
Loss: 0.07983130507395964
ROC train: 0.997259	val: 0.681747	test: 0.766657
PRC train: 0.981262	val: 0.534705	test: 0.565156

Epoch: 42
Loss: 0.08647648528418099
ROC train: 0.997129	val: 0.700378	test: 0.804196
PRC train: 0.988402	val: 0.539904	test: 0.573817

Epoch: 43
Loss: 0.07951229106087561
ROC train: 0.997327	val: 0.797539	test: 0.742919
PRC train: 0.986158	val: 0.552951	test: 0.561096

Epoch: 44
Loss: 0.08386572975002556
ROC train: 0.996981	val: 0.801633	test: 0.733442
PRC train: 0.983232	val: 0.556660	test: 0.558963

Epoch: 45
Loss: 0.07745003837982418
ROC train: 0.997309	val: 0.754857	test: 0.780399
PRC train: 0.986803	val: 0.556510	test: 0.604546

Epoch: 46
Loss: 0.06195908359982523
ROC train: 0.998160	val: 0.674831	test: 0.809616
PRC train: 0.991367	val: 0.535173	test: 0.628116

Epoch: 47
Loss: 0.062380488518344435
ROC train: 0.998707	val: 0.777973	test: 0.803307
PRC train: 0.994615	val: 0.559643	test: 0.614285

Epoch: 48
Loss: 0.06869945962700288
ROC train: 0.998819	val: 0.811062	test: 0.791717
PRC train: 0.992884	val: 0.584057	test: 0.587757

Epoch: 49
Loss: 0.05683085111006083
ROC train: 0.997928	val: 0.693385	test: 0.765121
PRC train: 0.990688	val: 0.562915	test: 0.590781

Epoch: 50
Loss: 0.0699538186752323
ROC train: 0.999441	val: 0.718247	test: 0.734828
PRC train: 0.996185	val: 0.533325	test: 0.574428

Epoch: 51
Loss: 0.05102264147405228
ROC train: 0.998778	val: 0.793867	test: 0.772805
PRC train: 0.992205	val: 0.549692	test: 0.564386

Epoch: 52
Loss: 0.06756485780574037
ROC train: 0.998115	val: 0.808465	test: 0.785710
PRC train: 0.989662	val: 0.556895	test: 0.572667

Epoch: 53
Loss: 0.05507701382480359
ROC train: 0.998674	val: 0.723867	test: 0.732968
PRC train: 0.993320	val: 0.535400	test: 0.554899

Epoch: 54
Loss: 0.06284570730389602
ROC train: 0.999525	val: 0.685219	test: 0.758902
PRC train: 0.996739	val: 0.531855	test: 0.566113

Epoch: 55
Loss: 0.059341299199406096
ROC train: 0.998747	val: 0.707807	test: 0.761827
PRC train: 0.994283	val: 0.541227	test: 0.575614

Epoch: 56
Loss: 0.06675348379087284
ROC train: 0.999500	val: 0.709944	test: 0.779095
PRC train: 0.996474	val: 0.543286	test: 0.567567

Epoch: 57
Loss: 0.05933983179511408
ROC train: 0.999757	val: 0.766896	test: 0.801488
PRC train: 0.998742	val: 0.547156	test: 0.579657

Epoch: 58
Loss: 0.04799881225898279
ROC train: 0.998898	val: 0.754109	test: 0.785871
PRC train: 0.992648	val: 0.544308	test: 0.577020

Epoch: 59
Loss: 0.053862286187603234
ROC train: 0.999083	val: 0.721544	test: 0.764759
PRC train: 0.996103	val: 0.532113	test: 0.570908

Epoch: 60
Loss: 0.04490267066942365
ROC train: 0.999196	val: 0.716923	test: 0.813766
PRC train: 0.995426	val: 0.544144	test: 0.585469

Epoch: 61
Loss: 0.04234414759765083
ROC train: 0.999898	val: 0.686979	test: 0.788332
PRC train: 0.999319	val: 0.536656	test: 0.579973

Epoch: 62
Loss: 0.056133721315998565
ROC train: 0.999809	val: 0.776198	test: 0.744062
PRC train: 0.999202	val: 0.574516	test: 0.559185

Epoch: 63
Loss: 0.04273504998960574
ROC train: 0.998930	val: 0.686867	test: 0.787533
PRC train: 0.994338	val: 0.564876	test: 0.571164

Epoch: 64
Loss: 0.046483953570811316
ROC train: 0.999680	val: 0.723891	test: 0.800050
PRC train: 0.999132	val: 0.544764	test: 0.576603

Epoch: 65
Loss: 0.04581009662809989
ROC train: 0.999478	val: 0.785265	test: 0.797316
PRC train: 0.997272	val: 0.553208	test: 0.591789

Epoch: 66
Loss: 0.03836843143912588
ROC train: 0.999713	val: 0.787524	test: 0.787858
PRC train: 0.998489	val: 0.556140	test: 0.585546

Epoch: 67
Loss: 0.03839463892102225
ROC train: 0.999903	val: 0.722668	test: 0.790992
PRC train: 0.999993	val: 0.545416	test: 0.590458

Epoch: 68
Loss: 0.04187311490974592
ROC train: 0.999931	val: 0.761590	test: 0.789700
PRC train: 0.999739	val: 0.557565	test: 0.602356

Epoch: 69
Loss: 0.045469350159295936
ROC train: 0.999936	val: 0.728723	test: 0.816777
PRC train: 0.999739	val: 0.573696	test: 0.615935

Epoch: 70
Loss: 0.03637823603146439
ROC train: 1.000000	val: 0.646497	test: 0.810755
PRC train: 1.000000	val: 0.541118	test: 0.589898

Epoch: 71
Loss: 0.0331959851455403
ROC train: 0.999752	val: 0.717309	test: 0.805070
PRC train: 0.997710	val: 0.552157	test: 0.582888

Epoch: 72
Loss: 0.04432699335825123
ROC train: 0.999868	val: 0.760315	test: 0.825958
PRC train: 0.999145	val: 0.558417	test: 0.618151

Epoch: 73
Loss: 0.040056539086089415
ROC train: 0.999914	val: 0.746768	test: 0.806714
PRC train: 0.999094	val: 0.546162	test: 0.607954

Epoch: 74
Loss: 0.033070080148656186
ROC train: 0.999881	val: 0.735941	test: 0.802586
PRC train: 0.999292	val: 0.541116	test: 0.610162

Epoch: 75
Loss: 0.042963089697153775
ROC train: 0.999927	val: 0.731833	test: 0.833216
PRC train: 0.999941	val: 0.547159	test: 0.615821

Epoch: 76
Loss: 0.029986669997569198
ROC train: 0.999928	val: 0.784790	test: 0.818824
PRC train: 0.999889	val: 0.551739	test: 0.593163

Epoch: 77
Loss: 0.02815197346311543
ROC train: 0.999915	val: 0.815433	test: 0.793371
PRC train: 0.999994	val: 0.553252	test: 0.582265

Epoch: 78
Loss: 0.039383491199031596
ROC train: 0.999876	val: 0.772340	test: 0.804222
PRC train: 0.999938	val: 0.574402	test: 0.595548

Epoch: 79
Loss: 0.03401622808283637
ROC train: 0.999841	val: 0.755195	test: 0.806751
PRC train: 0.999660	val: 0.553140	test: 0.586135

Epoch: 80
Loss: 0.04179513449918583
ROC train: 0.999507	val: 0.769118	test: 0.782449
PRC train: 0.997008	val: 0.543744	test: 0.582700

Epoch: 81
Loss: 0.029243303594465107
ROC train: 0.999995	val: 0.762813	test: 0.815727
PRC train: 0.999946	val: 0.538209	test: 0.609069

Epoch: 82
Loss: 0.03125585999609535
ROC train: 0.999957	val: 0.789373	test: 0.778300
PRC train: 0.999840	val: 0.543511	test: 0.586294

Epoch: 83
Loss: 0.027938858439484925
ROC train: 0.999989	val: 0.771953	test: 0.768039
PRC train: 0.999999	val: 0.539858	test: 0.582979

Epoch: 84
Loss: 0.026986872992750634
ROC train: 0.999955	val: 0.748553	test: 0.798377
PRC train: 0.999997	val: 0.537429	test: 0.598717

Epoch: 85
Loss: 0.02661033480313782
ROC train: 0.999869	val: 0.733256	test: 0.806109
PRC train: 0.999137	val: 0.533760	test: 0.599653

Epoch: 86
Loss: 0.03183387517712386
ROC train: 0.999943	val: 0.705773	test: 0.827770
PRC train: 0.999996	val: 0.534909	test: 0.612999

Epoch: 87
Loss: 0.024518414492967693
ROC train: 1.000000	val: 0.692486	test: 0.842897
PRC train: 1.000000	val: 0.541456	test: 0.631383

Epoch: 88
Loss: 0.023756981279301335
ROC train: 0.999994	val: 0.692936	test: 0.842035
PRC train: 1.000000	val: 0.540251	test: 0.624734

Epoch: 89
Loss: 0.03347571758660167
ROC train: 0.999983	val: 0.684720	test: 0.818125
PRC train: 0.999999	val: 0.531511	test: 0.597896

Epoch: 90
Loss: 0.026646119695818038
ROC train: 0.999983	val: 0.667350	test: 0.823549
PRC train: 0.999999	val: 0.534965	test: 0.611669

Epoch: 91
Loss: 0.02611689704719573
ROC train: 0.999984	val: 0.675404	test: 0.838389
PRC train: 0.999945	val: 0.535167	test: 0.614167

Epoch: 92
Loss: 0.025134737662561035
ROC train: 0.999952	val: 0.713377	test: 0.798952
PRC train: 0.999837	val: 0.534570	test: 0.590200

Epoch: 93
Loss: 0.032575394346479436
PRC train: 0.984453	val: 0.525075	test: 0.553993

Epoch: 33
Loss: 0.08387785881306181
ROC train: 0.995725	val: 0.619565	test: 0.642100
PRC train: 0.986411	val: 0.523043	test: 0.549023

Epoch: 34
Loss: 0.0835026109627641
ROC train: 0.996291	val: 0.525138	test: 0.594516
PRC train: 0.986826	val: 0.545801	test: 0.542743

Epoch: 35
Loss: 0.07122027385387224
ROC train: 0.995322	val: 0.566657	test: 0.669207
PRC train: 0.988274	val: 0.513693	test: 0.575681

Epoch: 36
Loss: 0.07415742395442046
ROC train: 0.996098	val: 0.527299	test: 0.610630
PRC train: 0.991768	val: 0.514208	test: 0.552477

Epoch: 37
Loss: 0.06447860945398282
ROC train: 0.997670	val: 0.471256	test: 0.601736
PRC train: 0.994343	val: 0.509901	test: 0.538995

Epoch: 38
Loss: 0.0713112773014129
ROC train: 0.997652	val: 0.558641	test: 0.632766
PRC train: 0.991787	val: 0.512406	test: 0.557957

Epoch: 39
Loss: 0.06596746581762568
ROC train: 0.997491	val: 0.580555	test: 0.627981
PRC train: 0.992761	val: 0.516467	test: 0.554486

Epoch: 40
Loss: 0.057691811890329014
ROC train: 0.997563	val: 0.559864	test: 0.616569
PRC train: 0.993404	val: 0.521481	test: 0.545313

Epoch: 41
Loss: 0.06045592922923483
ROC train: 0.998507	val: 0.589172	test: 0.649029
PRC train: 0.994446	val: 0.530945	test: 0.556808

Epoch: 42
Loss: 0.0628050952676757
ROC train: 0.998707	val: 0.592218	test: 0.666518
PRC train: 0.992712	val: 0.528959	test: 0.563523

Epoch: 43
Loss: 0.06194579934476339
ROC train: 0.998875	val: 0.620464	test: 0.684981
PRC train: 0.995768	val: 0.520862	test: 0.581930

Epoch: 44
Loss: 0.05940015077783176
ROC train: 0.998037	val: 0.687506	test: 0.648368
PRC train: 0.994370	val: 0.541712	test: 0.556886

Epoch: 45
Loss: 0.05273370167246292
ROC train: 0.998376	val: 0.617579	test: 0.641073
PRC train: 0.995777	val: 0.526777	test: 0.553016

Epoch: 46
Loss: 0.05179576590978265
ROC train: 0.998542	val: 0.595465	test: 0.636725
PRC train: 0.995636	val: 0.518452	test: 0.556806

Epoch: 47
Loss: 0.050860059425360336
ROC train: 0.998682	val: 0.519607	test: 0.595252
PRC train: 0.996023	val: 0.511052	test: 0.555346

Epoch: 48
Loss: 0.05243215206758094
ROC train: 0.998627	val: 0.522717	test: 0.636213
PRC train: 0.995661	val: 0.511538	test: 0.556013

Epoch: 49
Loss: 0.05006632330948648
ROC train: 0.998030	val: 0.512027	test: 0.639661
PRC train: 0.994343	val: 0.510171	test: 0.557047

Epoch: 50
Loss: 0.059867918230625115
ROC train: 0.998007	val: 0.622900	test: 0.666906
PRC train: 0.994289	val: 0.518779	test: 0.568816

Epoch: 51
Loss: 0.051440767934071964
ROC train: 0.998280	val: 0.586374	test: 0.619550
PRC train: 0.995729	val: 0.523113	test: 0.556134

Epoch: 52
Loss: 0.04637646901186916
ROC train: 0.993462	val: 0.531593	test: 0.631731
PRC train: 0.974981	val: 0.511157	test: 0.540336

Epoch: 53
Loss: 0.04866493204367146
ROC train: 0.995932	val: 0.568653	test: 0.679128
PRC train: 0.981378	val: 0.539498	test: 0.560535

Epoch: 54
Loss: 0.04452439172053964
ROC train: 0.999022	val: 0.525886	test: 0.638212
PRC train: 0.996592	val: 0.526762	test: 0.572990

Epoch: 55
Loss: 0.04411489808092984
ROC train: 0.999306	val: 0.556480	test: 0.607657
PRC train: 0.996645	val: 0.550230	test: 0.542600

Epoch: 56
Loss: 0.04405063372668036
ROC train: 0.998878	val: 0.687341	test: 0.644214
PRC train: 0.995797	val: 0.532985	test: 0.560112

Epoch: 57
Loss: 0.04598182313533548
ROC train: 0.999128	val: 0.599760	test: 0.675337
PRC train: 0.996409	val: 0.518824	test: 0.561189

Epoch: 58
Loss: 0.039612610916510214
ROC train: 0.999074	val: 0.640628	test: 0.690790
PRC train: 0.995913	val: 0.522636	test: 0.573303

Epoch: 59
Loss: 0.0471127299899475
ROC train: 0.998860	val: 0.673432	test: 0.648880
PRC train: 0.995767	val: 0.527556	test: 0.571207

Epoch: 60
Loss: 0.0421118288421483
ROC train: 0.998750	val: 0.521754	test: 0.563528
PRC train: 0.995815	val: 0.549445	test: 0.564289

Epoch: 61
Loss: 0.0392424569829666
ROC train: 0.999464	val: 0.540371	test: 0.598039
PRC train: 0.997531	val: 0.517370	test: 0.576620

Epoch: 62
Loss: 0.029897611971700117
ROC train: 0.999625	val: 0.566819	test: 0.635320
PRC train: 0.997209	val: 0.517226	test: 0.573945

Epoch: 63
Loss: 0.042966140157272605
ROC train: 0.999900	val: 0.524038	test: 0.650878
PRC train: 0.999188	val: 0.517515	test: 0.586951

Epoch: 64
Loss: 0.03171913661433044
ROC train: 0.999734	val: 0.483605	test: 0.644808
PRC train: 0.999058	val: 0.509475	test: 0.607617

Epoch: 65
Loss: 0.03306455492497768
ROC train: 0.999557	val: 0.550436	test: 0.657941
PRC train: 0.997778	val: 0.513502	test: 0.622043

Epoch: 66
Loss: 0.04009386123079116
ROC train: 0.999522	val: 0.603456	test: 0.668467
PRC train: 0.997579	val: 0.518765	test: 0.585982

Epoch: 67
Loss: 0.04419259879108062
ROC train: 0.999712	val: 0.570926	test: 0.643822
PRC train: 0.999281	val: 0.528136	test: 0.560667

Epoch: 68
Loss: 0.038859244145766775
ROC train: 0.999572	val: 0.577832	test: 0.613428
PRC train: 0.998139	val: 0.642272	test: 0.586973

Epoch: 69
Loss: 0.03678126845106754
ROC train: 0.999938	val: 0.611085	test: 0.629142
PRC train: 0.999379	val: 0.576142	test: 0.599026

Epoch: 70
Loss: 0.04054093371718921
ROC train: 0.999981	val: 0.579181	test: 0.631791
PRC train: 0.999792	val: 0.535704	test: 0.575888

Epoch: 71
Loss: 0.03303319639768354
ROC train: 0.999945	val: 0.629179	test: 0.635963
PRC train: 0.999598	val: 0.529036	test: 0.564536

Epoch: 72
Loss: 0.03567357158909207
ROC train: 0.999962	val: 0.614343	test: 0.657975
PRC train: 0.999599	val: 0.525164	test: 0.575693

Epoch: 73
Loss: 0.03309738647566374
ROC train: 1.000000	val: 0.521142	test: 0.664896
PRC train: 1.000000	val: 0.516112	test: 0.573672

Epoch: 74
Loss: 0.021941733629779907
ROC train: 0.999976	val: 0.540385	test: 0.658587
PRC train: 0.999735	val: 0.519139	test: 0.575285

Epoch: 75
Loss: 0.026849379706646758
ROC train: 1.000000	val: 0.581655	test: 0.668105
PRC train: 1.000000	val: 0.518297	test: 0.588160

Epoch: 76
Loss: 0.02968872909573893
ROC train: 0.999961	val: 0.570452	test: 0.669181
PRC train: 0.999634	val: 0.522140	test: 0.616123

Epoch: 77
Loss: 0.021684823833719162
ROC train: 0.999977	val: 0.573000	test: 0.673364
PRC train: 0.999998	val: 0.516028	test: 0.621954

Epoch: 78
Loss: 0.02890882271063695
ROC train: 1.000000	val: 0.557155	test: 0.654968
PRC train: 1.000000	val: 0.514526	test: 0.601478

Epoch: 79
Loss: 0.03227022501368555
ROC train: 1.000000	val: 0.614596	test: 0.666305
PRC train: 1.000000	val: 0.519994	test: 0.579991

Epoch: 80
Loss: 0.020807550899634205
ROC train: 1.000000	val: 0.600634	test: 0.652671
PRC train: 1.000000	val: 0.523412	test: 0.573764

Epoch: 81
Loss: 0.024313247594887663
ROC train: 0.999995	val: 0.634461	test: 0.659492
PRC train: 0.999946	val: 0.522170	test: 0.589742

Epoch: 82
Loss: 0.02147674988987508
ROC train: 0.999990	val: 0.638432	test: 0.657437
PRC train: 0.999892	val: 0.523737	test: 0.601753

Epoch: 83
Loss: 0.025406733149302262
ROC train: 0.999995	val: 0.550274	test: 0.617925
PRC train: 0.999946	val: 0.540992	test: 0.564054

Epoch: 84
Loss: 0.021258746484434377
ROC train: 1.000000	val: 0.553458	test: 0.606133
PRC train: 1.000000	val: 0.525638	test: 0.588396

Epoch: 85
Loss: 0.025581601640395534
ROC train: 0.999994	val: 0.572114	test: 0.629718
PRC train: 1.000000	val: 0.516449	test: 0.598624

Epoch: 86
Loss: 0.023000130594934935
ROC train: 1.000000	val: 0.489414	test: 0.650441
PRC train: 1.000000	val: 0.509379	test: 0.597068

Epoch: 87
Loss: 0.022834343704215786
ROC train: 1.000000	val: 0.461143	test: 0.643983
PRC train: 1.000000	val: 0.507153	test: 0.598322

Epoch: 88
Loss: 0.025659618991812117
ROC train: 1.000000	val: 0.470459	test: 0.663533
PRC train: 1.000000	val: 0.507643	test: 0.609829

Epoch: 89
Loss: 0.019003171037519712
ROC train: 0.999966	val: 0.595641	test: 0.672341
PRC train: 0.999998	val: 0.516313	test: 0.573801

Epoch: 90
Loss: 0.027945524152520596
ROC train: 1.000000	val: 0.610650	test: 0.674126
PRC train: 1.000000	val: 0.518077	test: 0.568701

Epoch: 91
Loss: 0.01930209539957627
ROC train: 1.000000	val: 0.616993	test: 0.642272
PRC train: 1.000000	val: 0.521170	test: 0.556049

Epoch: 92
Loss: 0.03171944468331467
ROC train: 1.000000	val: 0.624421	test: 0.637450
PRC train: 1.000000	val: 0.523085	test: 0.547967

Epoch: 93
Loss: 0.019071920672873856
PRC train: 0.991200	val: 0.543324	test: 0.594104

Epoch: 33
Loss: 0.08333300467053871
ROC train: 0.997467	val: 0.584287	test: 0.764759
PRC train: 0.991624	val: 0.521336	test: 0.587131

Epoch: 34
Loss: 0.08437144268385752
ROC train: 0.997862	val: 0.600895	test: 0.749396
PRC train: 0.992506	val: 0.538145	test: 0.579645

Epoch: 35
Loss: 0.06793567612271849
ROC train: 0.997090	val: 0.628178	test: 0.723419
PRC train: 0.987234	val: 0.567803	test: 0.566744

Epoch: 36
Loss: 0.08152130681247549
ROC train: 0.998695	val: 0.640203	test: 0.727922
PRC train: 0.993146	val: 0.530756	test: 0.572269

Epoch: 37
Loss: 0.06849349211841273
ROC train: 0.998169	val: 0.666200	test: 0.731395
PRC train: 0.989237	val: 0.597116	test: 0.574261

Epoch: 38
Loss: 0.07600473093902715
ROC train: 0.999477	val: 0.703225	test: 0.756153
PRC train: 0.996786	val: 0.561818	test: 0.584588

Epoch: 39
Loss: 0.07065009387373297
ROC train: 0.999282	val: 0.718859	test: 0.730906
PRC train: 0.994441	val: 0.585543	test: 0.572851

Epoch: 40
Loss: 0.06788691669995017
ROC train: 0.998989	val: 0.694359	test: 0.740913
PRC train: 0.993433	val: 0.622549	test: 0.583621

Epoch: 41
Loss: 0.06921806578629024
ROC train: 0.999146	val: 0.601956	test: 0.721751
PRC train: 0.998419	val: 0.524141	test: 0.577966

Epoch: 42
Loss: 0.06373267689743908
ROC train: 0.999441	val: 0.644174	test: 0.745067
PRC train: 0.997170	val: 0.564528	test: 0.579000

Epoch: 43
Loss: 0.05359022748464624
ROC train: 0.999517	val: 0.750001	test: 0.749807
PRC train: 0.996800	val: 0.650255	test: 0.578827

Epoch: 44
Loss: 0.04806565147240381
ROC train: 0.999563	val: 0.736640	test: 0.736752
PRC train: 0.997971	val: 0.559573	test: 0.578862

Epoch: 45
Loss: 0.059483529309780556
ROC train: 0.999129	val: 0.679399	test: 0.727077
PRC train: 0.996837	val: 0.540364	test: 0.567572

Epoch: 46
Loss: 0.053708281471495
ROC train: 0.998385	val: 0.685155	test: 0.747995
PRC train: 0.991810	val: 0.580164	test: 0.592864

Epoch: 47
Loss: 0.061985979687414185
ROC train: 0.998732	val: 0.663828	test: 0.756089
PRC train: 0.993061	val: 0.539078	test: 0.580980

Epoch: 48
Loss: 0.044789410358410595
ROC train: 0.999329	val: 0.698505	test: 0.733816
PRC train: 0.997723	val: 0.532212	test: 0.576265

Epoch: 49
Loss: 0.04909066412779931
ROC train: 0.999758	val: 0.662405	test: 0.739363
PRC train: 0.999507	val: 0.539062	test: 0.581188

Epoch: 50
Loss: 0.03995387371578121
ROC train: 0.999941	val: 0.617977	test: 0.728071
PRC train: 0.999779	val: 0.537079	test: 0.567954

Epoch: 51
Loss: 0.03930718700099876
ROC train: 0.999955	val: 0.635196	test: 0.735041
PRC train: 0.999639	val: 0.557990	test: 0.567961

Epoch: 52
Loss: 0.04446170547876163
ROC train: 0.999983	val: 0.654263	test: 0.752455
PRC train: 0.999999	val: 0.552499	test: 0.574657

Epoch: 53
Loss: 0.04644603947753778
ROC train: 0.999966	val: 0.668137	test: 0.743442
PRC train: 0.999628	val: 0.544154	test: 0.576234

Epoch: 54
Loss: 0.03750954191478521
ROC train: 0.999565	val: 0.697968	test: 0.755077
PRC train: 0.997011	val: 0.563730	test: 0.582587

Epoch: 55
Loss: 0.04018029957032675
ROC train: 0.999985	val: 0.701152	test: 0.740087
PRC train: 0.999893	val: 0.578925	test: 0.579219

Epoch: 56
Loss: 0.041664332989325016
ROC train: 0.999836	val: 0.684569	test: 0.723861
PRC train: 0.999935	val: 0.560962	test: 0.568667

Epoch: 57
Loss: 0.033328332588096435
ROC train: 0.999837	val: 0.621747	test: 0.709021
PRC train: 0.999265	val: 0.542777	test: 0.572583

Epoch: 58
Loss: 0.03585506102704902
ROC train: 0.999836	val: 0.626193	test: 0.715330
PRC train: 0.999624	val: 0.542754	test: 0.583384

Epoch: 59
Loss: 0.03709625903050872
ROC train: 0.999989	val: 0.688378	test: 0.744592
PRC train: 0.999999	val: 0.551480	test: 0.583252

Epoch: 60
Loss: 0.0345657936252604
ROC train: 0.999986	val: 0.745468	test: 0.763642
PRC train: 0.999837	val: 0.587539	test: 0.585982

Epoch: 61
Loss: 0.04646991280446887
ROC train: 0.999737	val: 0.761077	test: 0.721213
PRC train: 0.999767	val: 0.555020	test: 0.565012

Epoch: 62
Loss: 0.0373642766171767
ROC train: 0.999841	val: 0.691175	test: 0.684712
PRC train: 0.999989	val: 0.531971	test: 0.568269

Epoch: 63
Loss: 0.031129373536206983
ROC train: 0.999949	val: 0.659720	test: 0.693968
PRC train: 0.999996	val: 0.536358	test: 0.569103

Epoch: 64
Loss: 0.03181887902290343
ROC train: 0.999994	val: 0.742621	test: 0.735329
PRC train: 1.000000	val: 0.556976	test: 0.572216

Epoch: 65
Loss: 0.029926698581607437
ROC train: 1.000000	val: 0.765572	test: 0.744835
PRC train: 1.000000	val: 0.572505	test: 0.573351

Epoch: 66
Loss: 0.031091658920127618
ROC train: 0.999990	val: 0.733643	test: 0.734373
PRC train: 0.999892	val: 0.550546	test: 0.566360

Epoch: 67
Loss: 0.03860437679013072
ROC train: 0.999989	val: 0.720830	test: 0.723999
PRC train: 0.999999	val: 0.555672	test: 0.563604

Epoch: 68
Loss: 0.034048279647816634
ROC train: 0.999932	val: 0.751649	test: 0.712868
PRC train: 0.999995	val: 0.560168	test: 0.553435

Epoch: 69
Loss: 0.024381966327162306
ROC train: 0.999447	val: 0.781730	test: 0.686699
PRC train: 0.995851	val: 0.625914	test: 0.550773

Epoch: 70
Loss: 0.033883618079916036
ROC train: 1.000000	val: 0.745355	test: 0.683289
PRC train: 1.000000	val: 0.619902	test: 0.549188

Epoch: 71
Loss: 0.029967842620108688
ROC train: 0.999551	val: 0.703474	test: 0.715229
PRC train: 0.996164	val: 0.547836	test: 0.557044

Epoch: 72
Loss: 0.02608352989904671
ROC train: 0.999902	val: 0.675703	test: 0.730013
PRC train: 0.999054	val: 0.572493	test: 0.566252

Epoch: 73
Loss: 0.029875796890382732
ROC train: 0.999972	val: 0.659207	test: 0.710321
PRC train: 0.999945	val: 0.558532	test: 0.562721

Epoch: 74
Loss: 0.023751314449187523
ROC train: 0.999989	val: 0.671433	test: 0.693019
PRC train: 0.999999	val: 0.552792	test: 0.561407

Epoch: 75
Loss: 0.02294973979917394
ROC train: 1.000000	val: 0.683370	test: 0.702563
PRC train: 1.000000	val: 0.549595	test: 0.562248

Epoch: 76
Loss: 0.024804961591545077
ROC train: 1.000000	val: 0.706658	test: 0.733304
PRC train: 1.000000	val: 0.580036	test: 0.580117

Epoch: 77
Loss: 0.02354331658148286
ROC train: 1.000000	val: 0.766608	test: 0.731168
PRC train: 1.000000	val: 0.593034	test: 0.565301

Epoch: 78
Loss: 0.024144268222638383
ROC train: 0.999977	val: 0.761351	test: 0.731018
PRC train: 0.999998	val: 0.575596	test: 0.563129

Epoch: 79
Loss: 0.018728389608985086
ROC train: 0.999989	val: 0.694946	test: 0.725610
PRC train: 0.999999	val: 0.552755	test: 0.563054

Epoch: 80
Loss: 0.017333087889315903
ROC train: 1.000000	val: 0.674592	test: 0.698308
PRC train: 1.000000	val: 0.544572	test: 0.556765

Epoch: 81
Loss: 0.020025072595697115
ROC train: 1.000000	val: 0.677077	test: 0.712293
PRC train: 1.000000	val: 0.619636	test: 0.562336

Epoch: 82
Loss: 0.016709001239437953
ROC train: 1.000000	val: 0.673130	test: 0.719801
PRC train: 1.000000	val: 0.565396	test: 0.564249

Epoch: 83
Loss: 0.029378924897567483
ROC train: 1.000000	val: 0.652640	test: 0.725822
PRC train: 1.000000	val: 0.564786	test: 0.565223

Epoch: 84
Loss: 0.018589106960455154
ROC train: 1.000000	val: 0.644023	test: 0.726409
PRC train: 1.000000	val: 0.532885	test: 0.568657

Epoch: 85
Loss: 0.023439542268159218
ROC train: 0.999990	val: 0.692584	test: 0.711244
PRC train: 0.999946	val: 0.569862	test: 0.566802

Epoch: 86
Loss: 0.02108559059769008
ROC train: 1.000000	val: 0.708306	test: 0.697027
PRC train: 1.000000	val: 0.548565	test: 0.561088

Epoch: 87
Loss: 0.012262387927929146
ROC train: 1.000000	val: 0.706096	test: 0.719050
PRC train: 1.000000	val: 0.549160	test: 0.564486

Epoch: 88
Loss: 0.015995517022706063
ROC train: 1.000000	val: 0.691811	test: 0.719125
PRC train: 1.000000	val: 0.548635	test: 0.565061

Epoch: 89
Loss: 0.014848779351818064
ROC train: 1.000000	val: 0.766383	test: 0.715834
PRC train: 1.000000	val: 0.569977	test: 0.561510

Epoch: 90
Loss: 0.01690357563278598
ROC train: 1.000000	val: 0.745243	test: 0.689769
PRC train: 1.000000	val: 0.557174	test: 0.548676

Epoch: 91
Loss: 0.014851285250623302
ROC train: 1.000000	val: 0.681459	test: 0.700486
PRC train: 1.000000	val: 0.588919	test: 0.554186

Epoch: 92
Loss: 0.017523745277528507
ROC train: 1.000000	val: 0.639103	test: 0.706720
PRC train: 1.000000	val: 0.546553	test: 0.568973

Epoch: 93
Loss: 0.022652871603674535
PRC train: 0.965919	val: 0.628904	test: 0.540434

Epoch: 33
Loss: 0.10509947739430839
ROC train: 0.990641	val: 0.745682	test: 0.577730
PRC train: 0.957136	val: 0.644264	test: 0.542034

Epoch: 34
Loss: 0.11062951486143294
ROC train: 0.992913	val: 0.901230	test: 0.669244
PRC train: 0.970205	val: 0.616026	test: 0.561793

Epoch: 35
Loss: 0.10732296523814225
ROC train: 0.989564	val: 0.900193	test: 0.685646
PRC train: 0.958762	val: 0.607167	test: 0.559002

Epoch: 36
Loss: 0.09970434240102286
ROC train: 0.993652	val: 0.780408	test: 0.622837
PRC train: 0.972381	val: 0.628660	test: 0.549264

Epoch: 37
Loss: 0.09808163534114152
ROC train: 0.995259	val: 0.844589	test: 0.606211
PRC train: 0.976095	val: 0.679240	test: 0.541257

Epoch: 38
Loss: 0.10274761855316517
ROC train: 0.994633	val: 0.843279	test: 0.603537
PRC train: 0.971748	val: 0.672972	test: 0.572388

Epoch: 39
Loss: 0.08913859462773041
ROC train: 0.994384	val: 0.919960	test: 0.675389
PRC train: 0.971625	val: 0.644595	test: 0.558302

Epoch: 40
Loss: 0.09391290340983369
ROC train: 0.997668	val: 0.824911	test: 0.640651
PRC train: 0.990000	val: 0.629871	test: 0.545639

Epoch: 41
Loss: 0.09266026554373999
ROC train: 0.997649	val: 0.885272	test: 0.636179
PRC train: 0.989381	val: 0.670737	test: 0.548151

Epoch: 42
Loss: 0.08778161907077905
ROC train: 0.997077	val: 0.790996	test: 0.570196
PRC train: 0.984430	val: 0.624665	test: 0.534305

Epoch: 43
Loss: 0.08794292848112992
ROC train: 0.996611	val: 0.864918	test: 0.604373
PRC train: 0.985203	val: 0.666613	test: 0.538204

Epoch: 44
Loss: 0.08364790112405165
ROC train: 0.996654	val: 0.856228	test: 0.612344
PRC train: 0.989340	val: 0.640032	test: 0.555882

Epoch: 45
Loss: 0.06943652056509197
ROC train: 0.998107	val: 0.841430	test: 0.608097
PRC train: 0.994338	val: 0.662013	test: 0.551271

Epoch: 46
Loss: 0.08247449627293327
ROC train: 0.998682	val: 0.838970	test: 0.612483
PRC train: 0.993533	val: 0.675118	test: 0.543416

Epoch: 47
Loss: 0.06999417868382855
ROC train: 0.995362	val: 0.677902	test: 0.557518
PRC train: 0.975776	val: 0.607576	test: 0.561343

Epoch: 48
Loss: 0.06329050742823622
ROC train: 0.998384	val: 0.869163	test: 0.604213
PRC train: 0.991995	val: 0.682753	test: 0.537304

Epoch: 49
Loss: 0.06431118737165469
ROC train: 0.997505	val: 0.881125	test: 0.636355
PRC train: 0.992373	val: 0.691159	test: 0.556188

Epoch: 50
Loss: 0.08350833980534489
ROC train: 0.993720	val: 0.928376	test: 0.684047
PRC train: 0.978652	val: 0.726393	test: 0.571096

Epoch: 51
Loss: 0.07831802885746278
ROC train: 0.998791	val: 0.876880	test: 0.686740
PRC train: 0.994647	val: 0.650528	test: 0.572587

Epoch: 52
Loss: 0.06526883721402317
ROC train: 0.998519	val: 0.853842	test: 0.684025
PRC train: 0.991422	val: 0.657044	test: 0.559666

Epoch: 53
Loss: 0.06715594785427187
ROC train: 0.998230	val: 0.770031	test: 0.630860
PRC train: 0.989673	val: 0.573578	test: 0.559630

Epoch: 54
Loss: 0.05606513617523061
ROC train: 0.998007	val: 0.703313	test: 0.547575
PRC train: 0.987047	val: 0.586146	test: 0.527966

Epoch: 55
Loss: 0.05327271195100155
ROC train: 0.997813	val: 0.774775	test: 0.546137
PRC train: 0.985579	val: 0.647044	test: 0.521692

Epoch: 56
Loss: 0.05919763410238097
ROC train: 0.996137	val: 0.917837	test: 0.634091
PRC train: 0.981106	val: 0.694858	test: 0.541570

Epoch: 57
Loss: 0.06996881667619528
ROC train: 0.998931	val: 0.854541	test: 0.611855
PRC train: 0.994833	val: 0.648921	test: 0.546569

Epoch: 58
Loss: 0.054815940471816815
ROC train: 0.999103	val: 0.853055	test: 0.601613
PRC train: 0.994844	val: 0.648186	test: 0.537931

Epoch: 59
Loss: 0.051870898775933774
ROC train: 0.999193	val: 0.826021	test: 0.585413
PRC train: 0.995544	val: 0.630252	test: 0.543770

Epoch: 60
Loss: 0.05162764524601636
ROC train: 0.998936	val: 0.899806	test: 0.646922
PRC train: 0.994700	val: 0.730284	test: 0.546960

Epoch: 61
Loss: 0.054980718764112424
ROC train: 0.999385	val: 0.870899	test: 0.629471
PRC train: 0.996414	val: 0.670728	test: 0.542066

Epoch: 62
Loss: 0.04400676907007712
ROC train: 0.999332	val: 0.840418	test: 0.628134
PRC train: 0.996496	val: 0.671231	test: 0.542570

Epoch: 63
Loss: 0.04425665317304956
ROC train: 0.999624	val: 0.836785	test: 0.626961
PRC train: 0.997627	val: 0.617954	test: 0.542204

Epoch: 64
Loss: 0.04461923650258697
ROC train: 0.999478	val: 0.811824	test: 0.635780
PRC train: 0.996922	val: 0.622679	test: 0.551364

Epoch: 65
Loss: 0.0379393745970353
ROC train: 0.999679	val: 0.879453	test: 0.630494
PRC train: 0.997808	val: 0.671859	test: 0.576472

Epoch: 66
Loss: 0.057190760420168865
ROC train: 0.999302	val: 0.843328	test: 0.595368
PRC train: 0.995011	val: 0.625952	test: 0.537900

Epoch: 67
Loss: 0.04635835731998657
ROC train: 0.999333	val: 0.831053	test: 0.605572
PRC train: 0.996035	val: 0.609605	test: 0.536155

Epoch: 68
Loss: 0.041551678204595384
ROC train: 0.999681	val: 0.763400	test: 0.578746
PRC train: 0.998032	val: 0.620758	test: 0.542707

Epoch: 69
Loss: 0.03975290700751095
ROC train: 0.998818	val: 0.883311	test: 0.657370
PRC train: 0.993526	val: 0.683190	test: 0.541683

Epoch: 70
Loss: 0.03965720986034994
ROC train: 0.998843	val: 0.810538	test: 0.649795
PRC train: 0.994954	val: 0.636826	test: 0.543053

Epoch: 71
Loss: 0.04582035630020227
ROC train: 0.999580	val: 0.807041	test: 0.581790
PRC train: 0.997139	val: 0.634636	test: 0.541556

Epoch: 72
Loss: 0.05179382756723465
ROC train: 0.999629	val: 0.824847	test: 0.573176
PRC train: 0.996620	val: 0.678453	test: 0.540955

Epoch: 73
Loss: 0.03737951710865764
ROC train: 0.998914	val: 0.713514	test: 0.537142
PRC train: 0.991433	val: 0.609613	test: 0.527732

Epoch: 74
Loss: 0.0412271236750605
ROC train: 0.999667	val: 0.765821	test: 0.590908
PRC train: 0.996957	val: 0.622668	test: 0.536299

Epoch: 75
Loss: 0.04150331749513301
ROC train: 0.999746	val: 0.883786	test: 0.615243
PRC train: 0.997806	val: 0.670796	test: 0.575331

Epoch: 76
Loss: 0.04296024241757044
ROC train: 0.999131	val: 0.928464	test: 0.610776
PRC train: 0.995615	val: 0.735514	test: 0.577089

Epoch: 77
Loss: 0.036128798946379485
ROC train: 0.999011	val: 0.900530	test: 0.596666
PRC train: 0.996269	val: 0.701623	test: 0.571125

Epoch: 78
Loss: 0.05358967256339806
ROC train: 0.999695	val: 0.854517	test: 0.601676
PRC train: 0.998812	val: 0.638386	test: 0.570707

Epoch: 79
Loss: 0.04482323823020167
ROC train: 0.999910	val: 0.876856	test: 0.623386
PRC train: 0.999323	val: 0.646982	test: 0.552097

Epoch: 80
Loss: 0.03977985050969651
ROC train: 0.999952	val: 0.900892	test: 0.623173
PRC train: 0.999484	val: 0.668839	test: 0.552605

Epoch: 81
Loss: 0.04066207406389198
ROC train: 0.999689	val: 0.902066	test: 0.630181
PRC train: 0.997603	val: 0.635530	test: 0.545604

Epoch: 82
Loss: 0.03511079652599017
ROC train: 0.999857	val: 0.839107	test: 0.568821
PRC train: 0.998819	val: 0.639009	test: 0.541612

Epoch: 83
Loss: 0.034424122280783856
ROC train: 0.999972	val: 0.875369	test: 0.569135
PRC train: 0.999998	val: 0.671945	test: 0.535497

Epoch: 84
Loss: 0.02655255147211506
ROC train: 0.999950	val: 0.892813	test: 0.594557
PRC train: 0.999943	val: 0.678888	test: 0.548057

Epoch: 85
Loss: 0.05166597460685577
ROC train: 0.998428	val: 0.768207	test: 0.591046
PRC train: 0.990871	val: 0.618983	test: 0.572059

Epoch: 86
Loss: 0.033840044386681
ROC train: 0.999795	val: 0.841817	test: 0.607474
PRC train: 0.998129	val: 0.650630	test: 0.580338

Epoch: 87
Loss: 0.03212969860200291
ROC train: 0.999978	val: 0.868513	test: 0.606200
PRC train: 0.999945	val: 0.668419	test: 0.581790

Epoch: 88
Loss: 0.03148853710797389
ROC train: 0.999783	val: 0.893376	test: 0.634868
PRC train: 0.999067	val: 0.629095	test: 0.587246

Epoch: 89
Loss: 0.041020869009654265
ROC train: 0.999972	val: 0.911406	test: 0.661837
PRC train: 0.999998	val: 0.658452	test: 0.603664

Epoch: 90
Loss: 0.03138566530867006
ROC train: 0.999695	val: 0.911543	test: 0.656552
PRC train: 0.997481	val: 0.696981	test: 0.597589

Epoch: 91
Loss: 0.03065238914590767
ROC train: 0.999912	val: 0.860547	test: 0.599503
PRC train: 0.999468	val: 0.663846	test: 0.576130

Epoch: 92
Loss: 0.033043410943709876
ROC train: 0.999783	val: 0.838183	test: 0.574644
PRC train: 0.998420	val: 0.652207	test: 0.531752

Epoch: 93
Loss: 0.030225889882002466

PRC train: 0.977716	val: 0.595012	test: 0.553123

Epoch: 33
Loss: 0.10675020163331099
ROC train: 0.996396	val: 0.789696	test: 0.680472
PRC train: 0.981723	val: 0.562463	test: 0.551487

Epoch: 34
Loss: 0.09623422897585207
ROC train: 0.998413	val: 0.714575	test: 0.700639
PRC train: 0.988325	val: 0.533692	test: 0.565060

Epoch: 35
Loss: 0.09211570976832904
ROC train: 0.997314	val: 0.747727	test: 0.694719
PRC train: 0.985676	val: 0.548055	test: 0.569967

Epoch: 36
Loss: 0.08875228039480104
ROC train: 0.997486	val: 0.783290	test: 0.683587
PRC train: 0.985010	val: 0.566186	test: 0.565571

Epoch: 37
Loss: 0.08285978406233947
ROC train: 0.998075	val: 0.798587	test: 0.685474
PRC train: 0.987829	val: 0.567679	test: 0.574274

Epoch: 38
Loss: 0.08335419877231352
ROC train: 0.997883	val: 0.730796	test: 0.676954
PRC train: 0.989050	val: 0.550240	test: 0.568548

Epoch: 39
Loss: 0.08948386652997345
ROC train: 0.997845	val: 0.636956	test: 0.723929
PRC train: 0.989017	val: 0.529080	test: 0.557896

Epoch: 40
Loss: 0.08160034647128848
ROC train: 0.997180	val: 0.659232	test: 0.746546
PRC train: 0.985624	val: 0.553995	test: 0.570351

Epoch: 41
Loss: 0.08077457039572238
ROC train: 0.998933	val: 0.651441	test: 0.708909
PRC train: 0.991916	val: 0.527917	test: 0.558432

Epoch: 42
Loss: 0.08148366994867276
ROC train: 0.998767	val: 0.721393	test: 0.702514
PRC train: 0.990062	val: 0.539598	test: 0.566412

Epoch: 43
Loss: 0.06903187545475878
ROC train: 0.994925	val: 0.704061	test: 0.635413
PRC train: 0.978455	val: 0.534756	test: 0.536727

Epoch: 44
Loss: 0.07729091880574254
ROC train: 0.998196	val: 0.685967	test: 0.685567
PRC train: 0.990385	val: 0.535320	test: 0.591486

Epoch: 45
Loss: 0.0706219894993642
ROC train: 0.999216	val: 0.749189	test: 0.698951
PRC train: 0.994981	val: 0.552937	test: 0.573830

Epoch: 46
Loss: 0.06384457933253106
ROC train: 0.999283	val: 0.833675	test: 0.718019
PRC train: 0.994736	val: 0.581274	test: 0.610376

Epoch: 47
Loss: 0.05887798075978161
ROC train: 0.998614	val: 0.778658	test: 0.672068
PRC train: 0.994082	val: 0.549352	test: 0.580412

Epoch: 48
Loss: 0.06039941996511444
ROC train: 0.999702	val: 0.802171	test: 0.709918
PRC train: 0.999462	val: 0.554727	test: 0.594900

Epoch: 49
Loss: 0.060535228229384605
ROC train: 0.999874	val: 0.799623	test: 0.709533
PRC train: 0.999363	val: 0.554881	test: 0.601172

Epoch: 50
Loss: 0.061396668528766116
ROC train: 0.999741	val: 0.787935	test: 0.711068
PRC train: 0.998058	val: 0.552737	test: 0.597821

Epoch: 51
Loss: 0.0545858797683873
ROC train: 0.999706	val: 0.624931	test: 0.729255
PRC train: 0.998877	val: 0.535062	test: 0.605026

Epoch: 52
Loss: 0.055219906338592896
ROC train: 0.999751	val: 0.638829	test: 0.742646
PRC train: 0.998922	val: 0.539244	test: 0.608525

Epoch: 53
Loss: 0.054334730736496875
ROC train: 0.999852	val: 0.695308	test: 0.744719
PRC train: 0.999036	val: 0.540921	test: 0.627313

Epoch: 54
Loss: 0.05197576115674919
ROC train: 0.999890	val: 0.708369	test: 0.745926
PRC train: 0.999451	val: 0.540383	test: 0.638192

Epoch: 55
Loss: 0.04837343804278807
ROC train: 0.999818	val: 0.697855	test: 0.729692
PRC train: 0.998712	val: 0.568234	test: 0.616129

Epoch: 56
Loss: 0.060022952358704786
ROC train: 0.999811	val: 0.614979	test: 0.682799
PRC train: 0.999051	val: 0.526339	test: 0.584269

Epoch: 57
Loss: 0.03967109179013868
ROC train: 0.999754	val: 0.649867	test: 0.674742
PRC train: 0.998278	val: 0.538688	test: 0.585837

Epoch: 58
Loss: 0.04457959039227479
ROC train: 0.999905	val: 0.735966	test: 0.707594
PRC train: 0.999886	val: 0.553900	test: 0.597012

Epoch: 59
Loss: 0.04472050932604242
ROC train: 0.999983	val: 0.712477	test: 0.708356
PRC train: 0.999999	val: 0.550897	test: 0.591970

Epoch: 60
Loss: 0.03434166268238226
ROC train: 0.999963	val: 0.706184	test: 0.725908
PRC train: 0.999838	val: 0.557922	test: 0.603967

Epoch: 61
Loss: 0.039692574108547804
ROC train: 0.999559	val: 0.789946	test: 0.742848
PRC train: 0.998150	val: 0.577601	test: 0.616077

Epoch: 62
Loss: 0.039491258789781075
ROC train: 0.999770	val: 0.838907	test: 0.729356
PRC train: 0.999441	val: 0.576582	test: 0.607251

Epoch: 63
Loss: 0.04248110372087767
ROC train: 0.999837	val: 0.840756	test: 0.695522
PRC train: 0.999572	val: 0.577091	test: 0.564442

Epoch: 64
Loss: 0.03436324309231927
ROC train: 0.999751	val: 0.782267	test: 0.683972
PRC train: 0.999301	val: 0.548100	test: 0.587286

Epoch: 65
Loss: 0.052059838293585216
ROC train: 0.999964	val: 0.744930	test: 0.727027
PRC train: 0.999790	val: 0.567971	test: 0.582585

Epoch: 66
Loss: 0.04199072473442708
ROC train: 0.999994	val: 0.797525	test: 0.685048
PRC train: 1.000000	val: 0.561718	test: 0.567556

Epoch: 67
Loss: 0.035351033443336205
ROC train: 0.999726	val: 0.727887	test: 0.643848
PRC train: 0.999408	val: 0.535211	test: 0.576865

Epoch: 68
Loss: 0.04437339526486151
ROC train: 0.999812	val: 0.696868	test: 0.676629
PRC train: 0.998630	val: 0.534521	test: 0.558004

Epoch: 69
Loss: 0.036175661137439495
ROC train: 0.999861	val: 0.754794	test: 0.696766
PRC train: 0.998615	val: 0.559373	test: 0.555949

Epoch: 70
Loss: 0.04291787358057249
ROC train: 1.000000	val: 0.754471	test: 0.685496
PRC train: 1.000000	val: 0.544365	test: 0.554333

Epoch: 71
Loss: 0.040940386523363306
ROC train: 0.999798	val: 0.694521	test: 0.730211
PRC train: 0.998496	val: 0.571633	test: 0.576091

Epoch: 72
Loss: 0.036435929181339394
ROC train: 0.999977	val: 0.811986	test: 0.714239
PRC train: 0.999998	val: 0.568582	test: 0.569460

Epoch: 73
Loss: 0.03165670314630688
ROC train: 0.999857	val: 0.801447	test: 0.646698
PRC train: 0.999352	val: 0.556177	test: 0.551885

Epoch: 74
Loss: 0.02693694527537551
ROC train: 0.999956	val: 0.799398	test: 0.677253
PRC train: 0.999572	val: 0.563973	test: 0.547253

Epoch: 75
Loss: 0.0338907573613453
ROC train: 1.000000	val: 0.756456	test: 0.652518
PRC train: 1.000000	val: 0.544312	test: 0.579347

Epoch: 76
Loss: 0.034225084583281495
ROC train: 0.999986	val: 0.701039	test: 0.674029
PRC train: 0.999837	val: 0.545942	test: 0.596533

Epoch: 77
Loss: 0.03309322668394194
ROC train: 0.999948	val: 0.678524	test: 0.713728
PRC train: 0.999740	val: 0.574698	test: 0.615087

Epoch: 78
Loss: 0.032523425633904604
ROC train: 0.999866	val: 0.760564	test: 0.681149
PRC train: 0.999885	val: 0.552455	test: 0.595856

Epoch: 79
Loss: 0.03885132752757891
ROC train: 0.999994	val: 0.812397	test: 0.676154
PRC train: 1.000000	val: 0.576428	test: 0.596992

Epoch: 80
Loss: 0.030537217584726724
ROC train: 0.999984	val: 0.775798	test: 0.703956
PRC train: 0.999945	val: 0.577705	test: 0.621502

Epoch: 81
Loss: 0.028291145447976746
ROC train: 0.999949	val: 0.689039	test: 0.684025
PRC train: 0.999996	val: 0.545239	test: 0.612367

Epoch: 82
Loss: 0.036039180272405304
ROC train: 1.000000	val: 0.755894	test: 0.715629
PRC train: 1.000000	val: 0.583030	test: 0.610860

Epoch: 83
Loss: 0.023932630180475895
ROC train: 0.999984	val: 0.833900	test: 0.693766
PRC train: 0.999945	val: 0.587474	test: 0.593990

Epoch: 84
Loss: 0.02813490257024181
ROC train: 0.999989	val: 0.802445	test: 0.687607
PRC train: 0.999999	val: 0.557765	test: 0.597234

Epoch: 85
Loss: 0.02447267826219235
ROC train: 1.000000	val: 0.741859	test: 0.703648
PRC train: 1.000000	val: 0.557943	test: 0.615169

Epoch: 86
Loss: 0.022933524036167312
ROC train: 0.999960	val: 0.723290	test: 0.729169
PRC train: 0.999687	val: 0.571252	test: 0.624649

Epoch: 87
Loss: 0.023494033472228427
ROC train: 1.000000	val: 0.763161	test: 0.721124
PRC train: 1.000000	val: 0.549921	test: 0.613600

Epoch: 88
Loss: 0.019314100220753878
ROC train: 1.000000	val: 0.760677	test: 0.708106
PRC train: 1.000000	val: 0.542270	test: 0.606377

Epoch: 89
Loss: 0.02690954725971818
ROC train: 1.000000	val: 0.761625	test: 0.716600
PRC train: 1.000000	val: 0.546688	test: 0.612554

Epoch: 90
Loss: 0.018271417937762723
ROC train: 0.999945	val: 0.754970	test: 0.655767
PRC train: 0.999553	val: 0.548139	test: 0.602587

Epoch: 91
Loss: 0.029705262472029814
ROC train: 1.000000	val: 0.789471	test: 0.654893
PRC train: 1.000000	val: 0.560477	test: 0.596380

Epoch: 92
Loss: 0.028666682047690007
ROC train: 1.000000	val: 0.774013	test: 0.714153
PRC train: 1.000000	val: 0.552952	test: 0.605242

Epoch: 93
Loss: 0.0364630174985453
PRC train: 0.972743	val: 0.571605	test: 0.584217

Epoch: 33
Loss: 0.09954289357474505
ROC train: 0.992007	val: 0.834975	test: 0.685945
PRC train: 0.965345	val: 0.593876	test: 0.571352

Epoch: 34
Loss: 0.09562857656024393
ROC train: 0.992549	val: 0.807916	test: 0.661079
PRC train: 0.971466	val: 0.623913	test: 0.584227

Epoch: 35
Loss: 0.086332168652215
ROC train: 0.992388	val: 0.827346	test: 0.675751
PRC train: 0.968902	val: 0.574806	test: 0.568128

Epoch: 36
Loss: 0.09685840069955236
ROC train: 0.995814	val: 0.829156	test: 0.672528
PRC train: 0.980203	val: 0.588263	test: 0.563715

Epoch: 37
Loss: 0.08447102859119711
ROC train: 0.996776	val: 0.841367	test: 0.710452
PRC train: 0.982086	val: 0.644769	test: 0.572310

Epoch: 38
Loss: 0.08069330887064485
ROC train: 0.997736	val: 0.832114	test: 0.719073
PRC train: 0.985851	val: 0.649130	test: 0.579084

Epoch: 39
Loss: 0.07757068919988723
ROC train: 0.996829	val: 0.833151	test: 0.711053
PRC train: 0.983510	val: 0.634243	test: 0.603867

Epoch: 40
Loss: 0.08664196592515327
ROC train: 0.993967	val: 0.774402	test: 0.711270
PRC train: 0.972087	val: 0.557609	test: 0.578110

Epoch: 41
Loss: 0.07511476980937287
ROC train: 0.996186	val: 0.844951	test: 0.705955
PRC train: 0.983193	val: 0.568238	test: 0.567367

Epoch: 42
Loss: 0.08020778969342271
ROC train: 0.997553	val: 0.858013	test: 0.731426
PRC train: 0.988364	val: 0.660516	test: 0.587653

Epoch: 43
Loss: 0.06789260047864634
ROC train: 0.998627	val: 0.836310	test: 0.687973
PRC train: 0.992687	val: 0.574642	test: 0.561171

Epoch: 44
Loss: 0.07023996605569514
ROC train: 0.998639	val: 0.791070	test: 0.704550
PRC train: 0.993754	val: 0.591414	test: 0.565731

Epoch: 45
Loss: 0.07385722026574577
ROC train: 0.998863	val: 0.863544	test: 0.734074
PRC train: 0.992975	val: 0.689510	test: 0.572535

Epoch: 46
Loss: 0.0637662446801192
ROC train: 0.998761	val: 0.843166	test: 0.687254
PRC train: 0.993100	val: 0.630434	test: 0.556773

Epoch: 47
Loss: 0.06063109154218965
ROC train: 0.999333	val: 0.844340	test: 0.697210
PRC train: 0.996531	val: 0.631789	test: 0.560687

Epoch: 48
Loss: 0.06196392933461785
ROC train: 0.998889	val: 0.818417	test: 0.727261
PRC train: 0.995648	val: 0.624542	test: 0.569155

Epoch: 49
Loss: 0.05077391033412172
ROC train: 0.997612	val: 0.834687	test: 0.751279
PRC train: 0.989195	val: 0.664148	test: 0.580190

Epoch: 50
Loss: 0.06241098439019448
ROC train: 0.998412	val: 0.853656	test: 0.768708
PRC train: 0.991995	val: 0.690990	test: 0.574399

Epoch: 51
Loss: 0.06336602576892991
ROC train: 0.997400	val: 0.810201	test: 0.756971
PRC train: 0.986147	val: 0.686058	test: 0.572359

Epoch: 52
Loss: 0.06621954010874469
ROC train: 0.999011	val: 0.816744	test: 0.703631
PRC train: 0.993365	val: 0.629551	test: 0.563649

Epoch: 53
Loss: 0.061845065062893854
ROC train: 0.998217	val: 0.864307	test: 0.716974
PRC train: 0.991185	val: 0.664317	test: 0.574143

Epoch: 54
Loss: 0.049186212009270126
ROC train: 0.999343	val: 0.864219	test: 0.724444
PRC train: 0.996364	val: 0.746224	test: 0.576776

Epoch: 55
Loss: 0.04192567756627931
ROC train: 0.999814	val: 0.879741	test: 0.722244
PRC train: 0.998627	val: 0.699893	test: 0.574587

Epoch: 56
Loss: 0.04506553927344945
ROC train: 0.999786	val: 0.825248	test: 0.699078
PRC train: 0.998427	val: 0.627440	test: 0.568511

Epoch: 57
Loss: 0.0454470385309184
ROC train: 0.999884	val: 0.831591	test: 0.703493
PRC train: 0.998939	val: 0.630976	test: 0.565980

Epoch: 58
Loss: 0.0408288940116046
ROC train: 0.999970	val: 0.856727	test: 0.726297
PRC train: 0.999742	val: 0.639653	test: 0.573449

Epoch: 59
Loss: 0.03788384950492389
ROC train: 0.999955	val: 0.884798	test: 0.740532
PRC train: 0.999997	val: 0.664184	test: 0.579245

Epoch: 60
Loss: 0.036630029735226166
ROC train: 0.999953	val: 0.881052	test: 0.740170
PRC train: 0.999741	val: 0.671664	test: 0.582995

Epoch: 61
Loss: 0.04167461974408229
ROC train: 0.999887	val: 0.875819	test: 0.748589
PRC train: 0.999069	val: 0.670501	test: 0.579204

Epoch: 62
Loss: 0.04274568043882104
ROC train: 0.999901	val: 0.888182	test: 0.774800
PRC train: 0.999119	val: 0.705779	test: 0.595104

Epoch: 63
Loss: 0.03762944983707303
ROC train: 0.999756	val: 0.872997	test: 0.761682
PRC train: 0.998691	val: 0.713136	test: 0.623078

Epoch: 64
Loss: 0.040248486500828365
ROC train: 0.999742	val: 0.863857	test: 0.725655
PRC train: 0.998964	val: 0.663862	test: 0.598337

Epoch: 65
Loss: 0.038006446975503004
ROC train: 0.999887	val: 0.865618	test: 0.717747
PRC train: 0.999009	val: 0.642572	test: 0.573885

Epoch: 66
Loss: 0.04073796963827741
ROC train: 0.999855	val: 0.867403	test: 0.728703
PRC train: 0.999026	val: 0.640507	test: 0.587113

Epoch: 67
Loss: 0.029185602226995132
ROC train: 0.999776	val: 0.854692	test: 0.718360
PRC train: 0.998182	val: 0.634917	test: 0.590593

Epoch: 68
Loss: 0.037485849120355574
ROC train: 0.999949	val: 0.852981	test: 0.714949
PRC train: 0.999632	val: 0.634727	test: 0.579697

Epoch: 69
Loss: 0.03544567315922936
ROC train: 0.999910	val: 0.838833	test: 0.725729
PRC train: 0.999940	val: 0.632026	test: 0.572681

Epoch: 70
Loss: 0.0342875745951141
ROC train: 0.999989	val: 0.828407	test: 0.702694
PRC train: 0.999999	val: 0.628507	test: 0.566523

Epoch: 71
Loss: 0.028313266792335524
ROC train: 0.999901	val: 0.836774	test: 0.684481
PRC train: 0.999505	val: 0.643175	test: 0.568761

Epoch: 72
Loss: 0.04557272416348075
ROC train: 0.999405	val: 0.845440	test: 0.665393
PRC train: 0.997350	val: 0.655906	test: 0.554773

Epoch: 73
Loss: 0.045650921380516574
ROC train: 0.999981	val: 0.876494	test: 0.704782
PRC train: 0.999786	val: 0.653235	test: 0.569333

Epoch: 74
Loss: 0.03498673854996675
ROC train: 0.999932	val: 0.884935	test: 0.728381
PRC train: 0.999637	val: 0.668324	test: 0.572620

Epoch: 75
Loss: 0.03656975168219497
ROC train: 0.999900	val: 0.861011	test: 0.707650
PRC train: 0.999220	val: 0.640676	test: 0.555172

Epoch: 76
Loss: 0.035436747735545164
ROC train: 0.999978	val: 0.851944	test: 0.689856
PRC train: 0.999945	val: 0.640185	test: 0.552142

Epoch: 77
Loss: 0.03565534463227922
ROC train: 0.999846	val: 0.860923	test: 0.682247
PRC train: 0.999329	val: 0.652999	test: 0.550473

Epoch: 78
Loss: 0.038980718183654714
ROC train: 0.999671	val: 0.863133	test: 0.689654
PRC train: 0.997828	val: 0.654407	test: 0.550357

Epoch: 79
Loss: 0.04279678429114027
ROC train: 0.999955	val: 0.871124	test: 0.714807
PRC train: 0.999611	val: 0.649014	test: 0.564360

Epoch: 80
Loss: 0.02833797464346252
ROC train: 0.999823	val: 0.858987	test: 0.714389
PRC train: 0.998808	val: 0.638459	test: 0.563009

Epoch: 81
Loss: 0.03305904062392993
ROC train: 0.999975	val: 0.865031	test: 0.722902
PRC train: 0.999781	val: 0.632381	test: 0.570367

Epoch: 82
Loss: 0.031217103783965507
ROC train: 0.999986	val: 0.865481	test: 0.710060
PRC train: 0.999837	val: 0.632446	test: 0.585640

Epoch: 83
Loss: 0.025440877947070922
ROC train: 1.000000	val: 0.879878	test: 0.722020
PRC train: 1.000000	val: 0.637343	test: 0.578279

Epoch: 84
Loss: 0.022061774354219597
ROC train: 0.999946	val: 0.861872	test: 0.733151
PRC train: 0.999834	val: 0.664857	test: 0.589749

Epoch: 85
Loss: 0.031071551458862767
ROC train: 0.999980	val: 0.855729	test: 0.721083
PRC train: 0.999842	val: 0.638950	test: 0.598977

Epoch: 86
Loss: 0.02296159929793337
ROC train: 0.999994	val: 0.847337	test: 0.727616
PRC train: 1.000000	val: 0.638888	test: 0.577365

Epoch: 87
Loss: 0.021793364199467445
ROC train: 0.999966	val: 0.869750	test: 0.743345
PRC train: 0.999998	val: 0.651947	test: 0.579310

Epoch: 88
Loss: 0.02354518970110844
ROC train: 1.000000	val: 0.834051	test: 0.754637
PRC train: 1.000000	val: 0.627158	test: 0.595750

Epoch: 89
Loss: 0.019528937199791475
ROC train: 1.000000	val: 0.805119	test: 0.749814
PRC train: 1.000000	val: 0.620930	test: 0.597788

Epoch: 90
Loss: 0.025036844507179218
ROC train: 1.000000	val: 0.853817	test: 0.758372
PRC train: 1.000000	val: 0.635979	test: 0.596662

Epoch: 91
Loss: 0.024276837247258355
ROC train: 1.000000	val: 0.851744	test: 0.736125
PRC train: 1.000000	val: 0.636153	test: 0.595945

Epoch: 92
Loss: 0.024691957760747778
ROC train: 0.999994	val: 0.852282	test: 0.731515
PRC train: 1.000000	val: 0.638212	test: 0.603488

Epoch: 93
Loss: 0.022016363200667896
ROC train: 0.981513	val: 0.811487	test: 0.790678
PRC train: 0.905206	val: 0.602090	test: 0.622652

Epoch: 95
Loss: 0.11241240971075292
ROC train: 0.984471	val: 0.841680	test: 0.811940
PRC train: 0.910687	val: 0.638308	test: 0.647930

Epoch: 96
Loss: 0.11846326449679198
ROC train: 0.984344	val: 0.853280	test: 0.831550
PRC train: 0.910199	val: 0.615975	test: 0.657970

Epoch: 97
Loss: 0.11161526592808668
ROC train: 0.984620	val: 0.830329	test: 0.828065
PRC train: 0.912053	val: 0.610540	test: 0.649684

Epoch: 98
Loss: 0.11494100468898263
ROC train: 0.984347	val: 0.795016	test: 0.796199
PRC train: 0.918827	val: 0.592102	test: 0.633104

Epoch: 99
Loss: 0.10916967257065047
ROC train: 0.983703	val: 0.799149	test: 0.789453
PRC train: 0.909768	val: 0.604335	test: 0.639548

Epoch: 100
Loss: 0.12044827027080793
ROC train: 0.982832	val: 0.803907	test: 0.803042
PRC train: 0.908282	val: 0.580923	test: 0.630155

Epoch: 101
Loss: 0.11627211548096503
ROC train: 0.983890	val: 0.810900	test: 0.802605
PRC train: 0.911885	val: 0.601634	test: 0.645485

Epoch: 102
Loss: 0.10888861836847094
ROC train: 0.984201	val: 0.823062	test: 0.811137
PRC train: 0.915785	val: 0.596637	test: 0.647553

Epoch: 103
Loss: 0.11249096115946729
ROC train: 0.984649	val: 0.821101	test: 0.835196
PRC train: 0.912673	val: 0.616839	test: 0.669962

Epoch: 104
Loss: 0.10427884283374408
ROC train: 0.985544	val: 0.794791	test: 0.821868
PRC train: 0.920569	val: 0.594382	test: 0.660634

Epoch: 105
Loss: 0.10750516277341056
ROC train: 0.983067	val: 0.775998	test: 0.811402
PRC train: 0.913141	val: 0.557593	test: 0.645076

Epoch: 106
Loss: 0.12174752189021032
ROC train: 0.985521	val: 0.807252	test: 0.825730
PRC train: 0.914939	val: 0.609792	test: 0.655505

Epoch: 107
Loss: 0.10601535300791584
ROC train: 0.985110	val: 0.798312	test: 0.796124
PRC train: 0.918743	val: 0.588763	test: 0.643173

Epoch: 108
Loss: 0.10015929233640535
ROC train: 0.982815	val: 0.804132	test: 0.798324
PRC train: 0.910231	val: 0.582403	test: 0.638147

Epoch: 109
Loss: 0.11365108938721265
ROC train: 0.984882	val: 0.793818	test: 0.813781
PRC train: 0.916826	val: 0.579082	test: 0.637263

Epoch: 110
Loss: 0.10707398375417077
ROC train: 0.986454	val: 0.805555	test: 0.823105
PRC train: 0.921472	val: 0.587538	test: 0.640311

Epoch: 111
Loss: 0.105377834316117
ROC train: 0.985403	val: 0.826246	test: 0.815847
PRC train: 0.916445	val: 0.592346	test: 0.646314

Epoch: 112
Loss: 0.09713699425483976
ROC train: 0.982675	val: 0.842741	test: 0.827938
PRC train: 0.899546	val: 0.648322	test: 0.661969

Epoch: 113
Loss: 0.10324184330713967
ROC train: 0.983817	val: 0.825185	test: 0.818895
PRC train: 0.909981	val: 0.604984	test: 0.665044

Epoch: 114
Loss: 0.11043258775863514
ROC train: 0.985409	val: 0.800885	test: 0.807678
PRC train: 0.919491	val: 0.596019	test: 0.648498

Epoch: 115
Loss: 0.09719135598368711
ROC train: 0.985700	val: 0.771978	test: 0.803468
PRC train: 0.919057	val: 0.579931	test: 0.644647

Epoch: 116
Loss: 0.10724268503232275
ROC train: 0.985344	val: 0.788361	test: 0.808141
PRC train: 0.918367	val: 0.584475	test: 0.644223

Epoch: 117
Loss: 0.10109292004251971
ROC train: 0.986202	val: 0.802846	test: 0.804144
PRC train: 0.920727	val: 0.586323	test: 0.648153

Epoch: 118
Loss: 0.1146611096839456
ROC train: 0.986222	val: 0.779245	test: 0.825129
PRC train: 0.920066	val: 0.594436	test: 0.654650

Epoch: 119
Loss: 0.10960968431803134
ROC train: 0.985053	val: 0.769968	test: 0.827915
PRC train: 0.919142	val: 0.577562	test: 0.646244

Epoch: 120
Loss: 0.12237566760101706
ROC train: 0.984484	val: 0.840032	test: 0.859557
PRC train: 0.911832	val: 0.623090	test: 0.661238

Early stopping
Best (ROC):	 train: 0.931013	val: 0.908223	test: 0.723999
Best (PRC):	 train: 0.792901	val: 0.637348	test: 0.582426

ROC train: 0.999958	val: 0.595613	test: 0.668198
PRC train: 0.999786	val: 0.533001	test: 0.569263

Epoch: 94
Loss: 0.018823557239789936
ROC train: 1.000000	val: 0.596175	test: 0.682302
PRC train: 1.000000	val: 0.518983	test: 0.571341

Epoch: 95
Loss: 0.021017889558500896
ROC train: 0.999687	val: 0.603442	test: 0.675557
PRC train: 0.999091	val: 0.518216	test: 0.558745

Epoch: 96
Loss: 0.020354540185915614
ROC train: 0.999994	val: 0.659759	test: 0.678855
PRC train: 1.000000	val: 0.522618	test: 0.567671

Epoch: 97
Loss: 0.022091329449342792
ROC train: 1.000000	val: 0.680699	test: 0.694207
PRC train: 1.000000	val: 0.525370	test: 0.579879

Epoch: 98
Loss: 0.01746717661170264
ROC train: 0.999990	val: 0.706261	test: 0.669935
PRC train: 0.999892	val: 0.536123	test: 0.574981

Epoch: 99
Loss: 0.017591393243847862
ROC train: 1.000000	val: 0.705312	test: 0.651796
PRC train: 1.000000	val: 0.534369	test: 0.558189

Epoch: 100
Loss: 0.014424163026358353
ROC train: 1.000000	val: 0.647758	test: 0.650123
PRC train: 1.000000	val: 0.527343	test: 0.551829

Epoch: 101
Loss: 0.013099683709106805
ROC train: 1.000000	val: 0.660933	test: 0.660130
PRC train: 1.000000	val: 0.523449	test: 0.558714

Epoch: 102
Loss: 0.014165120177991259
ROC train: 1.000000	val: 0.676279	test: 0.657769
PRC train: 1.000000	val: 0.526588	test: 0.561484

Epoch: 103
Loss: 0.014169327652445662
ROC train: 1.000000	val: 0.648047	test: 0.656869
PRC train: 1.000000	val: 0.521554	test: 0.564983

Epoch: 104
Loss: 0.011700610573018274
ROC train: 1.000000	val: 0.665642	test: 0.663503
PRC train: 1.000000	val: 0.527538	test: 0.567805

Epoch: 105
Loss: 0.012505499429153766
ROC train: 1.000000	val: 0.698495	test: 0.677506
PRC train: 1.000000	val: 0.536705	test: 0.573545

Epoch: 106
Loss: 0.010214623133385555
ROC train: 1.000000	val: 0.690490	test: 0.685739
PRC train: 1.000000	val: 0.537340	test: 0.573753

Epoch: 107
Loss: 0.012563329917013755
ROC train: 1.000000	val: 0.685120	test: 0.677981
PRC train: 1.000000	val: 0.527962	test: 0.576695

Epoch: 108
Loss: 0.009213727743396927
ROC train: 1.000000	val: 0.680787	test: 0.659693
PRC train: 1.000000	val: 0.526036	test: 0.573352

Epoch: 109
Loss: 0.007390947666374428
ROC train: 1.000000	val: 0.691414	test: 0.658131
PRC train: 1.000000	val: 0.530736	test: 0.574854

Epoch: 110
Loss: 0.009861537238910117
ROC train: 1.000000	val: 0.689453	test: 0.650399
PRC train: 1.000000	val: 0.547670	test: 0.577112

Epoch: 111
Loss: 0.009583244966843586
ROC train: 1.000000	val: 0.690289	test: 0.669587
PRC train: 1.000000	val: 0.548357	test: 0.583926

Epoch: 112
Loss: 0.011692506058189247
ROC train: 1.000000	val: 0.677340	test: 0.684241
PRC train: 1.000000	val: 0.540801	test: 0.580555

Epoch: 113
Loss: 0.009146339888965016
ROC train: 1.000000	val: 0.642638	test: 0.676165
PRC train: 1.000000	val: 0.544501	test: 0.572185

Epoch: 114
Loss: 0.007873508389901461
ROC train: 1.000000	val: 0.653128	test: 0.692347
PRC train: 1.000000	val: 0.538469	test: 0.562951

Epoch: 115
Loss: 0.00943390245722602
ROC train: 1.000000	val: 0.670997	test: 0.697019
PRC train: 1.000000	val: 0.540525	test: 0.562355

Epoch: 116
Loss: 0.011873017819840386
ROC train: 1.000000	val: 0.683859	test: 0.693721
PRC train: 1.000000	val: 0.541747	test: 0.568620

Epoch: 117
Loss: 0.007746365302975351
ROC train: 1.000000	val: 0.642540	test: 0.676057
PRC train: 1.000000	val: 0.536768	test: 0.562033

Epoch: 118
Loss: 0.010527117691764131
ROC train: 1.000000	val: 0.616529	test: 0.675519
PRC train: 1.000000	val: 0.534685	test: 0.561502

Epoch: 119
Loss: 0.00651591129216286
ROC train: 1.000000	val: 0.664679	test: 0.681253
PRC train: 1.000000	val: 0.538993	test: 0.562228

Epoch: 120
Loss: 0.008301630108775155
ROC train: 1.000000	val: 0.722106	test: 0.682990
PRC train: 1.000000	val: 0.542727	test: 0.567026

Early stopping
Best (ROC):	 train: 0.805620	val: 0.804733	test: 0.484116
Best (PRC):	 train: 0.660353	val: 0.550749	test: 0.501718

ROC train: 1.000000	val: 0.770955	test: 0.598188
PRC train: 1.000000	val: 0.572625	test: 0.527184

Epoch: 94
Loss: 0.013435935095763098
ROC train: 1.000000	val: 0.808978	test: 0.601374
PRC train: 1.000000	val: 0.582377	test: 0.530085

Epoch: 95
Loss: 0.011714355630151241
ROC train: 1.000000	val: 0.813648	test: 0.606458
PRC train: 1.000000	val: 0.636870	test: 0.532725

Epoch: 96
Loss: 0.015119951132215
ROC train: 1.000000	val: 0.793495	test: 0.600537
PRC train: 1.000000	val: 0.595080	test: 0.529291

Epoch: 97
Loss: 0.012498567039212878
ROC train: 0.999957	val: 0.776325	test: 0.573819
PRC train: 0.999545	val: 0.635338	test: 0.521476

Epoch: 98
Loss: 0.01039378188857593
ROC train: 0.999952	val: 0.786639	test: 0.576318
PRC train: 0.999499	val: 0.611028	test: 0.521320

Epoch: 99
Loss: 0.016612769737213623
ROC train: 1.000000	val: 0.793382	test: 0.584961
PRC train: 1.000000	val: 0.606574	test: 0.526071

Epoch: 100
Loss: 0.01481319971012221
ROC train: 1.000000	val: 0.790335	test: 0.591132
PRC train: 1.000000	val: 0.603253	test: 0.551081

Epoch: 101
Loss: 0.009503118214401899
ROC train: 1.000000	val: 0.785191	test: 0.598203
PRC train: 1.000000	val: 0.631977	test: 0.556043

Epoch: 102
Loss: 0.018619151917616695
ROC train: 1.000000	val: 0.760529	test: 0.600739
PRC train: 1.000000	val: 0.631729	test: 0.555236

Epoch: 103
Loss: 0.009131592213288579
ROC train: 1.000000	val: 0.741686	test: 0.611538
PRC train: 1.000000	val: 0.569324	test: 0.559824

Epoch: 104
Loss: 0.013691257014051486
ROC train: 1.000000	val: 0.808405	test: 0.627308
PRC train: 1.000000	val: 0.632284	test: 0.559229

Epoch: 105
Loss: 0.016710455383539373
ROC train: 1.000000	val: 0.813799	test: 0.627682
PRC train: 1.000000	val: 0.592681	test: 0.556024

Epoch: 106
Loss: 0.0102059687245744
ROC train: 1.000000	val: 0.800825	test: 0.618500
PRC train: 1.000000	val: 0.637989	test: 0.560722

Epoch: 107
Loss: 0.008773012116582685
ROC train: 1.000000	val: 0.753824	test: 0.609356
PRC train: 1.000000	val: 0.567588	test: 0.558966

Epoch: 108
Loss: 0.0107766381365727
ROC train: 1.000000	val: 0.802498	test: 0.628806
PRC train: 1.000000	val: 0.638877	test: 0.570423

Epoch: 109
Loss: 0.014738833630819204
ROC train: 1.000000	val: 0.823551	test: 0.628556
PRC train: 1.000000	val: 0.591426	test: 0.585736

Epoch: 110
Loss: 0.021110901371565884
ROC train: 1.000000	val: 0.806693	test: 0.579452
PRC train: 1.000000	val: 0.600368	test: 0.535103

Epoch: 111
Loss: 0.013817649149338765
ROC train: 1.000000	val: 0.813525	test: 0.570640
PRC train: 1.000000	val: 0.563610	test: 0.527601

Epoch: 112
Loss: 0.013827530575475883
ROC train: 1.000000	val: 0.849836	test: 0.603563
PRC train: 1.000000	val: 0.620582	test: 0.545629

Epoch: 113
Loss: 0.009524027390090884
ROC train: 1.000000	val: 0.868791	test: 0.634017
PRC train: 1.000000	val: 0.662710	test: 0.558002

Epoch: 114
Loss: 0.015332086366268987
ROC train: 1.000000	val: 0.824250	test: 0.613174
PRC train: 1.000000	val: 0.632485	test: 0.544520

Epoch: 115
Loss: 0.0145864405992917
ROC train: 1.000000	val: 0.802698	test: 0.601613
PRC train: 1.000000	val: 0.606473	test: 0.541942

Epoch: 116
Loss: 0.008941887804610176
ROC train: 1.000000	val: 0.823027	test: 0.611743
PRC train: 1.000000	val: 0.647435	test: 0.540088

Epoch: 117
Loss: 0.014016966870549675
ROC train: 1.000000	val: 0.780696	test: 0.593556
PRC train: 1.000000	val: 0.637084	test: 0.551363

Epoch: 118
Loss: 0.01284031716738111
ROC train: 1.000000	val: 0.715215	test: 0.578891
PRC train: 1.000000	val: 0.612192	test: 0.546609

Epoch: 119
Loss: 0.011180970933308547
ROC train: 1.000000	val: 0.803808	test: 0.607347
PRC train: 1.000000	val: 0.748619	test: 0.536995

Epoch: 120
Loss: 0.01913317774132688
ROC train: 0.999947	val: 0.801261	test: 0.598091
PRC train: 0.999465	val: 0.630540	test: 0.532669

Early stopping
Best (ROC):	 train: 1.000000	val: 0.879467	test: 0.613110
Best (PRC):	 train: 1.000000	val: 0.639949	test: 0.535588

ROC train: 0.983788	val: 0.837821	test: 0.807061
PRC train: 0.912712	val: 0.643270	test: 0.634698

Epoch: 95
Loss: 0.10996864313920718
ROC train: 0.982230	val: 0.872010	test: 0.818659
PRC train: 0.904020	val: 0.626086	test: 0.644425

Epoch: 96
Loss: 0.11766699491441528
ROC train: 0.984541	val: 0.855828	test: 0.817035
PRC train: 0.908969	val: 0.627272	test: 0.632917

Epoch: 97
Loss: 0.11197662410512499
ROC train: 0.984302	val: 0.869676	test: 0.809303
PRC train: 0.909008	val: 0.651598	test: 0.636528

Epoch: 98
Loss: 0.11693245191725879
ROC train: 0.984886	val: 0.891590	test: 0.800872
PRC train: 0.911027	val: 0.646626	test: 0.638539

Epoch: 99
Loss: 0.11562055642091487
ROC train: 0.984081	val: 0.887395	test: 0.780511
PRC train: 0.907568	val: 0.629342	test: 0.631842

Epoch: 100
Loss: 0.112158893334656
ROC train: 0.982638	val: 0.864581	test: 0.794007
PRC train: 0.904960	val: 0.629462	test: 0.636071

Epoch: 101
Loss: 0.10446555557822097
ROC train: 0.985765	val: 0.869163	test: 0.827727
PRC train: 0.919769	val: 0.622347	test: 0.661566

Epoch: 102
Loss: 0.12509564875693419
ROC train: 0.985934	val: 0.850620	test: 0.798691
PRC train: 0.920528	val: 0.629358	test: 0.645014

Epoch: 103
Loss: 0.10690338389964318
ROC train: 0.984188	val: 0.823948	test: 0.763620
PRC train: 0.913385	val: 0.598491	test: 0.626676

Epoch: 104
Loss: 0.1113590558835467
ROC train: 0.983938	val: 0.845563	test: 0.780533
PRC train: 0.909735	val: 0.644030	test: 0.632058

Epoch: 105
Loss: 0.11102631682889244
ROC train: 0.985165	val: 0.869838	test: 0.796259
PRC train: 0.914472	val: 0.673545	test: 0.629261

Epoch: 106
Loss: 0.11133566964008904
ROC train: 0.983985	val: 0.867266	test: 0.803453
PRC train: 0.909366	val: 0.700787	test: 0.629181

Epoch: 107
Loss: 0.10034284740506025
ROC train: 0.985743	val: 0.841680	test: 0.762496
PRC train: 0.918181	val: 0.649134	test: 0.620284

Epoch: 108
Loss: 0.10678971510374627
ROC train: 0.983423	val: 0.805418	test: 0.721886
PRC train: 0.915480	val: 0.597256	test: 0.615110

Epoch: 109
Loss: 0.09991158167689848
ROC train: 0.985313	val: 0.850008	test: 0.746639
PRC train: 0.917736	val: 0.617034	test: 0.624186

Epoch: 110
Loss: 0.10218000730722762
ROC train: 0.984951	val: 0.875394	test: 0.794089
PRC train: 0.917242	val: 0.622423	test: 0.640467

Epoch: 111
Loss: 0.10959104092385172
ROC train: 0.984752	val: 0.888480	test: 0.810576
PRC train: 0.918580	val: 0.650323	test: 0.648098

Epoch: 112
Loss: 0.10379889213979451
ROC train: 0.986111	val: 0.867266	test: 0.773126
PRC train: 0.924791	val: 0.637354	test: 0.628034

Epoch: 113
Loss: 0.09725076052920249
ROC train: 0.985744	val: 0.873809	test: 0.755675
PRC train: 0.919466	val: 0.650832	test: 0.622399

Epoch: 114
Loss: 0.10506039602988256
ROC train: 0.986009	val: 0.866879	test: 0.803267
PRC train: 0.918856	val: 0.647496	test: 0.648920

Epoch: 115
Loss: 0.09415148336432022
ROC train: 0.987205	val: 0.851333	test: 0.809112
PRC train: 0.926500	val: 0.629221	test: 0.642893

Epoch: 116
Loss: 0.10668330242189472
ROC train: 0.985212	val: 0.842379	test: 0.806852
PRC train: 0.924006	val: 0.627197	test: 0.634142

Epoch: 117
Loss: 0.10108218969261498
ROC train: 0.985375	val: 0.863070	test: 0.816116
PRC train: 0.920506	val: 0.604605	test: 0.638275

Epoch: 118
Loss: 0.10084535072730691
ROC train: 0.986178	val: 0.837210	test: 0.788953
PRC train: 0.924033	val: 0.610922	test: 0.622237

Epoch: 119
Loss: 0.08977591511199581
ROC train: 0.986915	val: 0.821302	test: 0.812649
PRC train: 0.925517	val: 0.619662	test: 0.632401

Epoch: 120
Loss: 0.09909177953581509
ROC train: 0.987525	val: 0.815321	test: 0.822668
PRC train: 0.925837	val: 0.620862	test: 0.639601

Epoch: 121
Loss: 0.1006842285270561
ROC train: 0.987294	val: 0.788947	test: 0.819907
PRC train: 0.925664	val: 0.597376	test: 0.630146

Epoch: 122
Loss: 0.09540984491155471
ROC train: 0.986577	val: 0.798987	test: 0.802082
PRC train: 0.926431	val: 0.613274	test: 0.631850

Epoch: 123
Loss: 0.1015441709089123
ROC train: 0.987106	val: 0.809863	test: 0.796024
PRC train: 0.928333	val: 0.624063	test: 0.630281

Epoch: 124
Loss: 0.10530519779923206
ROC train: 0.986598	val: 0.843915	test: 0.795635
PRC train: 0.922552	val: 0.620203	test: 0.638723

Epoch: 125
Loss: 0.10268472191650899
ROC train: 0.986755	val: 0.847162	test: 0.830269
PRC train: 0.923630	val: 0.605438	test: 0.655046

Epoch: 126
Loss: 0.09511699000854587
ROC train: 0.986489	val: 0.843978	test: 0.824292
PRC train: 0.921336	val: 0.606824	test: 0.650813

Epoch: 127
Loss: 0.10223841926188153
ROC train: 0.986934	val: 0.844090	test: 0.847459
PRC train: 0.926159	val: 0.612430	test: 0.656043

Epoch: 128
Loss: 0.09790647872588513
ROC train: 0.988205	val: 0.836985	test: 0.832656
PRC train: 0.933438	val: 0.619345	test: 0.649205

Epoch: 129
Loss: 0.09527337771865546
ROC train: 0.988055	val: 0.843915	test: 0.821181
PRC train: 0.931387	val: 0.610605	test: 0.646560

Epoch: 130
Loss: 0.09105794049568117
ROC train: 0.986902	val: 0.827757	test: 0.808238
PRC train: 0.928315	val: 0.607702	test: 0.633186

Epoch: 131
Loss: 0.10608767633037708
ROC train: 0.987968	val: 0.844365	test: 0.800050
PRC train: 0.934304	val: 0.632183	test: 0.633474

Epoch: 132
Loss: 0.09627988119690155
ROC train: 0.987730	val: 0.860610	test: 0.826489
PRC train: 0.930699	val: 0.671961	test: 0.650877

Epoch: 133
Loss: 0.08793520457283545
ROC train: 0.987196	val: 0.882587	test: 0.815447
PRC train: 0.925153	val: 0.676944	test: 0.643204

Early stopping
Best (ROC):	 train: 0.984886	val: 0.891590	test: 0.800872
Best (PRC):	 train: 0.911027	val: 0.646626	test: 0.638539

ROC train: 0.999974	val: 0.728611	test: 0.802549
PRC train: 0.999839	val: 0.537503	test: 0.590539

Epoch: 94
Loss: 0.033871974595553404
ROC train: 0.999923	val: 0.705298	test: 0.850656
PRC train: 0.999551	val: 0.543766	test: 0.618625

Epoch: 95
Loss: 0.024074987127021125
ROC train: 0.999931	val: 0.695821	test: 0.858388
PRC train: 0.999739	val: 0.536823	test: 0.628576

Epoch: 96
Loss: 0.023613312026128066
ROC train: 0.999971	val: 0.687605	test: 0.837339
PRC train: 0.999694	val: 0.529544	test: 0.631949

Epoch: 97
Loss: 0.020648469945248774
ROC train: 0.999994	val: 0.719421	test: 0.793767
PRC train: 1.000000	val: 0.532319	test: 0.600664

Epoch: 98
Loss: 0.023858089476848154
ROC train: 0.999957	val: 0.757018	test: 0.789188
PRC train: 0.999840	val: 0.540490	test: 0.595916

Epoch: 99
Loss: 0.024043466103254495
ROC train: 0.999989	val: 0.696970	test: 0.788609
PRC train: 0.999999	val: 0.528431	test: 0.590299

Epoch: 100
Loss: 0.01898664367515655
ROC train: 0.999956	val: 0.703762	test: 0.774194
PRC train: 0.999891	val: 0.530382	test: 0.577470

Epoch: 101
Loss: 0.02218139209519747
ROC train: 0.999994	val: 0.651042	test: 0.764236
PRC train: 1.000000	val: 0.528375	test: 0.572243

Epoch: 102
Loss: 0.0279500041593637
ROC train: 0.999966	val: 0.625869	test: 0.795302
PRC train: 0.999998	val: 0.531996	test: 0.615676

Epoch: 103
Loss: 0.019468917014828214
ROC train: 0.999989	val: 0.721832	test: 0.786483
PRC train: 0.999999	val: 0.533623	test: 0.612864

Epoch: 104
Loss: 0.01964206108660841
ROC train: 0.999900	val: 0.851881	test: 0.772850
PRC train: 0.999224	val: 0.565878	test: 0.588190

Epoch: 105
Loss: 0.023577128652695827
ROC train: 0.999960	val: 0.835137	test: 0.774261
PRC train: 0.999677	val: 0.568306	test: 0.580784

Epoch: 106
Loss: 0.017553105732944527
ROC train: 0.999995	val: 0.792982	test: 0.799587
PRC train: 0.999946	val: 0.589021	test: 0.593383

Epoch: 107
Loss: 0.02303833270213501
ROC train: 0.999994	val: 0.759991	test: 0.811241
PRC train: 1.000000	val: 0.615282	test: 0.623839

Epoch: 108
Loss: 0.018660279398055014
ROC train: 1.000000	val: 0.751301	test: 0.807945
PRC train: 1.000000	val: 0.609910	test: 0.602739

Epoch: 109
Loss: 0.019584941802441184
ROC train: 1.000000	val: 0.802234	test: 0.804398
PRC train: 1.000000	val: 0.577420	test: 0.603106

Epoch: 110
Loss: 0.017620475115402028
ROC train: 1.000000	val: 0.824710	test: 0.811741
PRC train: 1.000000	val: 0.582231	test: 0.605784

Epoch: 111
Loss: 0.019596018436802105
ROC train: 1.000000	val: 0.763850	test: 0.806919
PRC train: 1.000000	val: 0.570272	test: 0.601308

Epoch: 112
Loss: 0.018125832562700013
ROC train: 1.000000	val: 0.728150	test: 0.806437
PRC train: 1.000000	val: 0.551517	test: 0.600318

Epoch: 113
Loss: 0.014293495514704843
ROC train: 1.000000	val: 0.698505	test: 0.835718
PRC train: 1.000000	val: 0.563327	test: 0.620323

Epoch: 114
Loss: 0.015644824516837474
ROC train: 0.999994	val: 0.694872	test: 0.839411
PRC train: 1.000000	val: 0.562186	test: 0.612944

Epoch: 115
Loss: 0.012924411173559937
ROC train: 1.000000	val: 0.683184	test: 0.804846
PRC train: 1.000000	val: 0.547540	test: 0.620340

Epoch: 116
Loss: 0.013400216135324166
ROC train: 1.000000	val: 0.689615	test: 0.780963
PRC train: 1.000000	val: 0.541608	test: 0.612615

Epoch: 117
Loss: 0.01412514380794822
ROC train: 1.000000	val: 0.709856	test: 0.805503
PRC train: 1.000000	val: 0.564265	test: 0.623768

Epoch: 118
Loss: 0.017312921576155757
ROC train: 0.999977	val: 0.701953	test: 0.814539
PRC train: 0.999998	val: 0.603955	test: 0.621683

Epoch: 119
Loss: 0.019741181372167228
ROC train: 1.000000	val: 0.727925	test: 0.779663
PRC train: 1.000000	val: 0.607141	test: 0.617938

Epoch: 120
Loss: 0.02098356185021878
ROC train: 1.000000	val: 0.716399	test: 0.795616
PRC train: 1.000000	val: 0.608730	test: 0.599113

Early stopping
Best (ROC):	 train: 0.823811	val: 0.895386	test: 0.565108
Best (PRC):	 train: 0.624264	val: 0.585040	test: 0.518897

ROC train: 0.999972	val: 0.579156	test: 0.623360
PRC train: 0.999998	val: 0.518648	test: 0.555695

Epoch: 94
Loss: 0.021066769484819708
ROC train: 0.999981	val: 0.521142	test: 0.622124
PRC train: 0.999782	val: 0.511371	test: 0.552376

Epoch: 95
Loss: 0.02164639618411747
ROC train: 0.999977	val: 0.485341	test: 0.591580
PRC train: 0.999998	val: 0.524171	test: 0.568301

Epoch: 96
Loss: 0.022477270861416744
ROC train: 1.000000	val: 0.548812	test: 0.637113
PRC train: 1.000000	val: 0.521222	test: 0.585032

Epoch: 97
Loss: 0.013502961545677612
ROC train: 1.000000	val: 0.630989	test: 0.645970
PRC train: 1.000000	val: 0.526037	test: 0.574345

Epoch: 98
Loss: 0.01514317599604904
ROC train: 1.000000	val: 0.634074	test: 0.653889
PRC train: 1.000000	val: 0.526842	test: 0.571565

Epoch: 99
Loss: 0.01676303853229444
ROC train: 1.000000	val: 0.627781	test: 0.662521
PRC train: 1.000000	val: 0.528773	test: 0.579805

Epoch: 100
Loss: 0.01728866809676916
ROC train: 1.000000	val: 0.581968	test: 0.639048
PRC train: 1.000000	val: 0.527177	test: 0.602199

Epoch: 101
Loss: 0.01620311514227123
ROC train: 1.000000	val: 0.615682	test: 0.635582
PRC train: 1.000000	val: 0.524725	test: 0.607784

Epoch: 102
Loss: 0.018305090281835013
ROC train: 1.000000	val: 0.573239	test: 0.632179
PRC train: 1.000000	val: 0.526763	test: 0.604431

Epoch: 103
Loss: 0.018644937356353548
ROC train: 1.000000	val: 0.484768	test: 0.602424
PRC train: 1.000000	val: 0.517513	test: 0.591182

Epoch: 104
Loss: 0.01682337170356587
ROC train: 1.000000	val: 0.538150	test: 0.622561
PRC train: 1.000000	val: 0.518738	test: 0.600910

Epoch: 105
Loss: 0.015141294043645549
ROC train: 1.000000	val: 0.538874	test: 0.673178
PRC train: 1.000000	val: 0.517270	test: 0.601158

Epoch: 106
Loss: 0.01390799026323456
ROC train: 1.000000	val: 0.568182	test: 0.669080
PRC train: 1.000000	val: 0.517724	test: 0.600416

Epoch: 107
Loss: 0.012621861820731679
ROC train: 1.000000	val: 0.610038	test: 0.667918
PRC train: 1.000000	val: 0.520352	test: 0.601456

Epoch: 108
Loss: 0.018311230942584474
ROC train: 1.000000	val: 0.630841	test: 0.665995
PRC train: 1.000000	val: 0.521800	test: 0.587951

Epoch: 109
Loss: 0.018897210080813864
ROC train: 1.000000	val: 0.566720	test: 0.651128
PRC train: 1.000000	val: 0.518172	test: 0.595113

Epoch: 110
Loss: 0.011646268018985372
ROC train: 1.000000	val: 0.525201	test: 0.604471
PRC train: 1.000000	val: 0.513162	test: 0.572109

Epoch: 111
Loss: 0.010043648485979105
ROC train: 1.000000	val: 0.502212	test: 0.618000
PRC train: 1.000000	val: 0.513357	test: 0.607464

Epoch: 112
Loss: 0.013406819394226038
ROC train: 1.000000	val: 0.516423	test: 0.644446
PRC train: 1.000000	val: 0.510802	test: 0.611622

Epoch: 113
Loss: 0.015258090185883418
ROC train: 1.000000	val: 0.537775	test: 0.683831
PRC train: 1.000000	val: 0.510815	test: 0.613220

Epoch: 114
Loss: 0.015191823261731057
ROC train: 1.000000	val: 0.510892	test: 0.711487
PRC train: 1.000000	val: 0.508769	test: 0.649573

Epoch: 115
Loss: 0.014004657406528509
ROC train: 1.000000	val: 0.559653	test: 0.717546
PRC train: 1.000000	val: 0.515403	test: 0.650948

Epoch: 116
Loss: 0.009355919958856788
ROC train: 1.000000	val: 0.597089	test: 0.715342
PRC train: 1.000000	val: 0.533872	test: 0.627252

Epoch: 117
Loss: 0.01150984585962347
ROC train: 1.000000	val: 0.594292	test: 0.696385
PRC train: 1.000000	val: 0.525620	test: 0.623174

Epoch: 118
Loss: 0.00967266573796405
ROC train: 1.000000	val: 0.529847	test: 0.689490
PRC train: 1.000000	val: 0.517600	test: 0.632004

Epoch: 119
Loss: 0.011107279516952617
ROC train: 1.000000	val: 0.523254	test: 0.701782
PRC train: 1.000000	val: 0.517891	test: 0.638841

Epoch: 120
Loss: 0.011651062409974213
ROC train: 1.000000	val: 0.523753	test: 0.678672
PRC train: 1.000000	val: 0.513694	test: 0.629669

Early stopping
Best (ROC):	 train: 0.655262	val: 0.813286	test: 0.463945
Best (PRC):	 train: 0.549527	val: 0.553262	test: 0.495437

ROC train: 0.999990	val: 0.633122	test: 0.714665
PRC train: 0.999894	val: 0.535783	test: 0.569050

Epoch: 94
Loss: 0.015460381998725561
ROC train: 0.999994	val: 0.697992	test: 0.742773
PRC train: 1.000000	val: 0.573456	test: 0.573318

Epoch: 95
Loss: 0.019708838037774356
ROC train: 0.999977	val: 0.690462	test: 0.710769
PRC train: 0.999998	val: 0.607435	test: 0.559545

Epoch: 96
Loss: 0.023961625218257298
ROC train: 1.000000	val: 0.673106	test: 0.666659
PRC train: 1.000000	val: 0.540343	test: 0.557683

Epoch: 97
Loss: 0.021526963086668717
ROC train: 0.999983	val: 0.708369	test: 0.675015
PRC train: 0.999999	val: 0.538098	test: 0.559331

Epoch: 98
Loss: 0.021593343132424857
ROC train: 0.999983	val: 0.727261	test: 0.718363
PRC train: 0.999999	val: 0.564051	test: 0.566804

Epoch: 99
Loss: 0.03289893597769228
ROC train: 0.999990	val: 0.729247	test: 0.735702
PRC train: 0.999894	val: 0.632704	test: 0.593506

Epoch: 100
Loss: 0.011786182256561816
ROC train: 0.999990	val: 0.736015	test: 0.737338
PRC train: 0.999894	val: 0.635899	test: 0.600704

Epoch: 101
Loss: 0.015602989836039593
ROC train: 1.000000	val: 0.711328	test: 0.727858
PRC train: 1.000000	val: 0.606734	test: 0.580759

Epoch: 102
Loss: 0.01728038141946613
ROC train: 1.000000	val: 0.669247	test: 0.705360
PRC train: 1.000000	val: 0.564294	test: 0.578624

Epoch: 103
Loss: 0.014684252804406847
ROC train: 0.999994	val: 0.612070	test: 0.689758
PRC train: 1.000000	val: 0.533658	test: 0.575074

Epoch: 104
Loss: 0.012786130659040528
ROC train: 1.000000	val: 0.638741	test: 0.714942
PRC train: 1.000000	val: 0.556016	test: 0.574923

Epoch: 105
Loss: 0.01539505400336721
ROC train: 1.000000	val: 0.659207	test: 0.715916
PRC train: 1.000000	val: 0.627446	test: 0.584786

Epoch: 106
Loss: 0.008442055487293496
ROC train: 1.000000	val: 0.686529	test: 0.708902
PRC train: 1.000000	val: 0.633541	test: 0.580840

Epoch: 107
Loss: 0.01364364867337177
ROC train: 1.000000	val: 0.645710	test: 0.689833
PRC train: 1.000000	val: 0.572782	test: 0.569994

Epoch: 108
Loss: 0.018217406705959217
ROC train: 0.999973	val: 0.691948	test: 0.711606
PRC train: 0.999891	val: 0.650943	test: 0.567903

Epoch: 109
Loss: 0.014553223205594198
ROC train: 0.999977	val: 0.700501	test: 0.714441
PRC train: 0.999998	val: 0.583288	test: 0.576642

Epoch: 110
Loss: 0.025245062333055717
ROC train: 1.000000	val: 0.689826	test: 0.712835
PRC train: 1.000000	val: 0.610051	test: 0.590592

Epoch: 111
Loss: 0.010662176800294949
ROC train: 1.000000	val: 0.690437	test: 0.703385
PRC train: 1.000000	val: 0.663722	test: 0.577669

Epoch: 112
Loss: 0.011987495535473808
ROC train: 1.000000	val: 0.678475	test: 0.686296
PRC train: 1.000000	val: 0.601861	test: 0.586779

Epoch: 113
Loss: 0.01299760832157829
ROC train: 1.000000	val: 0.670396	test: 0.686296
PRC train: 1.000000	val: 0.586314	test: 0.582158

Epoch: 114
Loss: 0.01199895704652334
ROC train: 0.999990	val: 0.661868	test: 0.687233
PRC train: 0.999894	val: 0.558848	test: 0.577122

Epoch: 115
Loss: 0.013380266940977454
ROC train: 1.000000	val: 0.677663	test: 0.712656
PRC train: 1.000000	val: 0.572991	test: 0.574655

Epoch: 116
Loss: 0.012118884605571225
ROC train: 1.000000	val: 0.684618	test: 0.729132
PRC train: 1.000000	val: 0.573059	test: 0.572318

Epoch: 117
Loss: 0.012222479264123698
ROC train: 1.000000	val: 0.659482	test: 0.725497
PRC train: 1.000000	val: 0.555728	test: 0.576630

Epoch: 118
Loss: 0.009598225611064758
ROC train: 1.000000	val: 0.625543	test: 0.713343
PRC train: 1.000000	val: 0.540064	test: 0.573680

Epoch: 119
Loss: 0.008982010488379141
ROC train: 1.000000	val: 0.641113	test: 0.704460
PRC train: 1.000000	val: 0.547831	test: 0.573464

Epoch: 120
Loss: 0.012111774793613233
ROC train: 1.000000	val: 0.667399	test: 0.728045
PRC train: 1.000000	val: 0.552716	test: 0.583675

Early stopping
Best (ROC):	 train: 0.749633	val: 0.857876	test: 0.509101
Best (PRC):	 train: 0.600977	val: 0.571982	test: 0.508474
ROC train: 0.999257	val: 0.841567	test: 0.581442
PRC train: 0.998118	val: 0.595885	test: 0.548973

Epoch: 94
Loss: 0.042432291686987166
ROC train: 0.996519	val: 0.820353	test: 0.606626
PRC train: 0.988538	val: 0.562642	test: 0.553870

Epoch: 95
Loss: 0.05555161029526321
ROC train: 0.999303	val: 0.780057	test: 0.543600
PRC train: 0.994394	val: 0.564221	test: 0.534116

Epoch: 96
Loss: 0.04150988974826679
ROC train: 0.999425	val: 0.739374	test: 0.494246
PRC train: 0.995180	val: 0.545986	test: 0.524879

Epoch: 97
Loss: 0.04294718828491399
ROC train: 0.999051	val: 0.868039	test: 0.549700
PRC train: 0.995157	val: 0.581365	test: 0.527608

Epoch: 98
Loss: 0.03707007743288154
ROC train: 0.993858	val: 0.879614	test: 0.656669
PRC train: 0.977467	val: 0.701098	test: 0.592492

Epoch: 99
Loss: 0.052196760281572586
ROC train: 0.999926	val: 0.900892	test: 0.606189
PRC train: 0.999685	val: 0.646312	test: 0.578543

Epoch: 100
Loss: 0.05623485611080722
ROC train: 0.999859	val: 0.864743	test: 0.587538
PRC train: 0.998614	val: 0.590063	test: 0.571391

Epoch: 101
Loss: 0.04345402027674093
ROC train: 0.999798	val: 0.790458	test: 0.623591
PRC train: 0.998094	val: 0.563927	test: 0.590589

Epoch: 102
Loss: 0.039819677732809325
ROC train: 0.998795	val: 0.694060	test: 0.609020
PRC train: 0.992245	val: 0.604700	test: 0.575371

Epoch: 103
Loss: 0.03185577199158461
ROC train: 0.999957	val: 0.811761	test: 0.610549
PRC train: 0.999837	val: 0.640783	test: 0.573368

Epoch: 104
Loss: 0.03747120692941057
ROC train: 0.999967	val: 0.880089	test: 0.630061
PRC train: 0.999944	val: 0.645850	test: 0.590245

Epoch: 105
Loss: 0.03156001250800712
ROC train: 0.999990	val: 0.845675	test: 0.632123
PRC train: 0.999946	val: 0.651101	test: 0.579052

Epoch: 106
Loss: 0.029737943092991874
ROC train: 0.999969	val: 0.815932	test: 0.636444
PRC train: 0.999838	val: 0.590571	test: 0.581422

Epoch: 107
Loss: 0.02627391822359948
ROC train: 0.999984	val: 0.830305	test: 0.631484
PRC train: 0.999945	val: 0.592885	test: 0.578847

Epoch: 108
Loss: 0.034210187805009104
ROC train: 0.999972	val: 0.885072	test: 0.666585
PRC train: 0.999945	val: 0.617870	test: 0.598250

Epoch: 109
Loss: 0.03452283402153694
ROC train: 1.000000	val: 0.900393	test: 0.668770
PRC train: 1.000000	val: 0.635578	test: 0.614114

Epoch: 110
Loss: 0.024638502431022762
ROC train: 1.000000	val: 0.879790	test: 0.659320
PRC train: 1.000000	val: 0.632829	test: 0.598239

Epoch: 111
Loss: 0.016652247456426836
ROC train: 1.000000	val: 0.854067	test: 0.632556
PRC train: 1.000000	val: 0.623422	test: 0.581262

Epoch: 112
Loss: 0.01519403707811034
ROC train: 1.000000	val: 0.817468	test: 0.590635
PRC train: 1.000000	val: 0.654051	test: 0.570808

Epoch: 113
Loss: 0.018799113716497107
ROC train: 1.000000	val: 0.847661	test: 0.614294
PRC train: 1.000000	val: 0.668681	test: 0.577025

Epoch: 114
Loss: 0.019488544688117143
ROC train: 1.000000	val: 0.840643	test: 0.628410
PRC train: 1.000000	val: 0.680774	test: 0.582925

Epoch: 115
Loss: 0.021118873182289232
ROC train: 1.000000	val: 0.847661	test: 0.640090
PRC train: 1.000000	val: 0.653614	test: 0.589564

Epoch: 116
Loss: 0.019306315496712752
ROC train: 1.000000	val: 0.838408	test: 0.641790
PRC train: 1.000000	val: 0.660989	test: 0.596915

Epoch: 117
Loss: 0.015114473753273391
ROC train: 1.000000	val: 0.790596	test: 0.623845
PRC train: 1.000000	val: 0.600736	test: 0.581541

Epoch: 118
Loss: 0.02263152345940698
ROC train: 1.000000	val: 0.775074	test: 0.595644
PRC train: 1.000000	val: 0.593519	test: 0.568775

Epoch: 119
Loss: 0.02186007087573381
ROC train: 1.000000	val: 0.872934	test: 0.604799
PRC train: 1.000000	val: 0.613472	test: 0.571847

Epoch: 120
Loss: 0.025491485492727766
ROC train: 1.000000	val: 0.873159	test: 0.594856
PRC train: 1.000000	val: 0.617596	test: 0.570437

Early stopping
Best (ROC):	 train: 0.999131	val: 0.928464	test: 0.610776
Best (PRC):	 train: 0.995615	val: 0.735514	test: 0.577089

ROC train: 1.000000	val: 0.757542	test: 0.746008
PRC train: 1.000000	val: 0.581251	test: 0.621984

Epoch: 94
Loss: 0.029500849028546616
ROC train: 0.999886	val: 0.741497	test: 0.724934
PRC train: 0.999672	val: 0.597870	test: 0.627113

Epoch: 95
Loss: 0.024475625303923568
ROC train: 0.999522	val: 0.687791	test: 0.708950
PRC train: 0.997635	val: 0.546739	test: 0.595978

Epoch: 96
Loss: 0.03347342367135827
ROC train: 0.998275	val: 0.719945	test: 0.702824
PRC train: 0.996233	val: 0.539524	test: 0.599979

Epoch: 97
Loss: 0.029824327678791916
ROC train: 0.999995	val: 0.710316	test: 0.714930
PRC train: 0.999946	val: 0.552643	test: 0.614489

Epoch: 98
Loss: 0.0191863902123846
ROC train: 1.000000	val: 0.686080	test: 0.675194
PRC train: 1.000000	val: 0.531736	test: 0.599504

Epoch: 99
Loss: 0.029632150371537137
ROC train: 1.000000	val: 0.767606	test: 0.695230
PRC train: 1.000000	val: 0.566424	test: 0.600116

Epoch: 100
Loss: 0.019202838381759107
ROC train: 0.999990	val: 0.790820	test: 0.688458
PRC train: 0.999946	val: 0.584260	test: 0.598340

Epoch: 101
Loss: 0.021893987427125693
ROC train: 1.000000	val: 0.753009	test: 0.717627
PRC train: 1.000000	val: 0.632338	test: 0.611135

Epoch: 102
Loss: 0.028214643488823788
ROC train: 0.999931	val: 0.740211	test: 0.651046
PRC train: 0.999330	val: 0.543382	test: 0.585926

Epoch: 103
Loss: 0.018968732591399966
ROC train: 0.999920	val: 0.731471	test: 0.607223
PRC train: 0.999670	val: 0.536185	test: 0.574783

Epoch: 104
Loss: 0.017450177444237013
ROC train: 1.000000	val: 0.733667	test: 0.674866
PRC train: 1.000000	val: 0.571701	test: 0.600369

Epoch: 105
Loss: 0.02913390522415729
ROC train: 0.999970	val: 0.709842	test: 0.681036
PRC train: 0.999733	val: 0.578017	test: 0.603020

Epoch: 106
Loss: 0.018128495998261572
ROC train: 0.999983	val: 0.736914	test: 0.651748
PRC train: 0.999999	val: 0.544211	test: 0.592561

Epoch: 107
Loss: 0.029157981233309482
ROC train: 1.000000	val: 0.789222	test: 0.698887
PRC train: 1.000000	val: 0.573054	test: 0.622535

Epoch: 108
Loss: 0.015565842322562395
ROC train: 0.999995	val: 0.838457	test: 0.730917
PRC train: 0.999946	val: 0.651711	test: 0.612933

Epoch: 109
Loss: 0.031505562776257434
ROC train: 0.999995	val: 0.808352	test: 0.691506
PRC train: 0.999946	val: 0.587806	test: 0.599641

Epoch: 110
Loss: 0.024716831814638417
ROC train: 0.999995	val: 0.756906	test: 0.639933
PRC train: 0.999946	val: 0.545878	test: 0.581432

Epoch: 111
Loss: 0.024479416850185457
ROC train: 0.999986	val: 0.727423	test: 0.695365
PRC train: 0.999837	val: 0.572249	test: 0.608976

Epoch: 112
Loss: 0.021834880605296886
ROC train: 1.000000	val: 0.713950	test: 0.692340
PRC train: 1.000000	val: 0.551713	test: 0.607171

Epoch: 113
Loss: 0.0188182092590921
ROC train: 1.000000	val: 0.721006	test: 0.688642
PRC train: 1.000000	val: 0.555085	test: 0.613650

Epoch: 114
Loss: 0.021435449870253916
ROC train: 0.999955	val: 0.707133	test: 0.740409
PRC train: 0.999997	val: 0.573815	test: 0.631811

Epoch: 115
Loss: 0.02332007600620249
ROC train: 1.000000	val: 0.703000	test: 0.737820
PRC train: 1.000000	val: 0.628171	test: 0.629179

Epoch: 116
Loss: 0.01677076491801115
ROC train: 1.000000	val: 0.723554	test: 0.690909
PRC train: 1.000000	val: 0.592246	test: 0.608677

Epoch: 117
Loss: 0.012360418134509285
ROC train: 1.000000	val: 0.758554	test: 0.671907
PRC train: 1.000000	val: 0.554102	test: 0.599260

Epoch: 118
Loss: 0.021731066549438727
ROC train: 1.000000	val: 0.807389	test: 0.676842
PRC train: 1.000000	val: 0.575461	test: 0.610214

Epoch: 119
Loss: 0.01439499687468406
ROC train: 1.000000	val: 0.801296	test: 0.646948
PRC train: 1.000000	val: 0.558465	test: 0.605033

Epoch: 120
Loss: 0.017117009927478093
ROC train: 1.000000	val: 0.765821	test: 0.622340
PRC train: 1.000000	val: 0.541621	test: 0.597986

Early stopping
Best (ROC):	 train: 0.771157	val: 0.881326	test: 0.552460
Best (PRC):	 train: 0.598147	val: 0.591892	test: 0.532580
All runs completed.

ROC train: 0.984426	val: 0.863857	test: 0.810053
PRC train: 0.914245	val: 0.641261	test: 0.628524

Epoch: 95
Loss: 0.11639243247470928
ROC train: 0.985138	val: 0.815907	test: 0.792789
PRC train: 0.918983	val: 0.624361	test: 0.620398

Epoch: 96
Loss: 0.1126301640798828
ROC train: 0.983708	val: 0.790297	test: 0.782233
PRC train: 0.908183	val: 0.621240	test: 0.620424

Epoch: 97
Loss: 0.1111933242996161
ROC train: 0.984503	val: 0.798825	test: 0.800734
PRC train: 0.913212	val: 0.609095	test: 0.626024

Epoch: 98
Loss: 0.10086378311003671
ROC train: 0.984951	val: 0.814846	test: 0.790809
PRC train: 0.917535	val: 0.626354	test: 0.622830

Epoch: 99
Loss: 0.10918693714891581
ROC train: 0.984609	val: 0.830754	test: 0.793965
PRC train: 0.914987	val: 0.629715	test: 0.624983

Epoch: 100
Loss: 0.11171030379954827
ROC train: 0.985542	val: 0.836373	test: 0.793715
PRC train: 0.914452	val: 0.628708	test: 0.628423

Epoch: 101
Loss: 0.10416828899370403
ROC train: 0.985551	val: 0.829693	test: 0.771352
PRC train: 0.915991	val: 0.594093	test: 0.622892

Epoch: 102
Loss: 0.10788076578403316
ROC train: 0.985878	val: 0.861260	test: 0.804267
PRC train: 0.916121	val: 0.599123	test: 0.628004

Epoch: 103
Loss: 0.10058139191864186
ROC train: 0.985048	val: 0.865481	test: 0.805380
PRC train: 0.918120	val: 0.607734	test: 0.621671

Epoch: 104
Loss: 0.1052914949168516
ROC train: 0.986288	val: 0.881276	test: 0.809291
PRC train: 0.923442	val: 0.613580	test: 0.628910

Epoch: 105
Loss: 0.10727411343698876
ROC train: 0.985433	val: 0.887707	test: 0.799722
PRC train: 0.920618	val: 0.621800	test: 0.618012

Epoch: 106
Loss: 0.10142008294021214
ROC train: 0.986270	val: 0.893326	test: 0.804144
PRC train: 0.922987	val: 0.637351	test: 0.622741

Epoch: 107
Loss: 0.11406690291525332
ROC train: 0.986418	val: 0.883961	test: 0.805918
PRC train: 0.922675	val: 0.657515	test: 0.623452

Epoch: 108
Loss: 0.10659174057945735
ROC train: 0.986923	val: 0.866855	test: 0.776551
PRC train: 0.928453	val: 0.634279	test: 0.618926

Epoch: 109
Loss: 0.10573064703681334
ROC train: 0.985302	val: 0.858962	test: 0.790604
PRC train: 0.918033	val: 0.603043	test: 0.628619

Epoch: 110
Loss: 0.10215743721012618
ROC train: 0.985001	val: 0.864620	test: 0.805343
PRC train: 0.917889	val: 0.587225	test: 0.634927

Epoch: 111
Loss: 0.10551804872865322
ROC train: 0.986340	val: 0.836437	test: 0.795176
PRC train: 0.923410	val: 0.609100	test: 0.633187

Epoch: 112
Loss: 0.09601776326941203
ROC train: 0.985702	val: 0.807892	test: 0.775700
PRC train: 0.919772	val: 0.594876	test: 0.631330

Epoch: 113
Loss: 0.10257952824013354
ROC train: 0.984088	val: 0.818269	test: 0.808589
PRC train: 0.910641	val: 0.589078	test: 0.633969

Epoch: 114
Loss: 0.09257764197439908
ROC train: 0.986364	val: 0.835151	test: 0.797484
PRC train: 0.920554	val: 0.646431	test: 0.634699

Epoch: 115
Loss: 0.11032532815083496
ROC train: 0.984719	val: 0.849372	test: 0.787328
PRC train: 0.912629	val: 0.675678	test: 0.631395

Epoch: 116
Loss: 0.11646235751633527
ROC train: 0.986165	val: 0.818842	test: 0.762458
PRC train: 0.923427	val: 0.670971	test: 0.621598

Epoch: 117
Loss: 0.10373147005514585
ROC train: 0.987175	val: 0.832740	test: 0.775375
PRC train: 0.928486	val: 0.633519	test: 0.625534

Epoch: 118
Loss: 0.09497644700767936
ROC train: 0.986458	val: 0.823962	test: 0.805904
PRC train: 0.919796	val: 0.673030	test: 0.635327

Epoch: 119
Loss: 0.10107031329269141
ROC train: 0.986159	val: 0.785602	test: 0.792636
PRC train: 0.917887	val: 0.622935	test: 0.628766

Epoch: 120
Loss: 0.1032862204138995
ROC train: 0.987131	val: 0.793593	test: 0.793323
PRC train: 0.926840	val: 0.637955	test: 0.636130

Epoch: 121
Loss: 0.09757134022337535
ROC train: 0.986080	val: 0.793143	test: 0.782558
PRC train: 0.922103	val: 0.636579	test: 0.621273

Epoch: 122
Loss: 0.09452624607856908
ROC train: 0.984086	val: 0.781431	test: 0.757606
PRC train: 0.910731	val: 0.583455	test: 0.612918

Epoch: 123
Loss: 0.10296685764278704
ROC train: 0.987213	val: 0.829693	test: 0.779536
PRC train: 0.929347	val: 0.592391	test: 0.621033

Epoch: 124
Loss: 0.0999525023493387
ROC train: 0.984805	val: 0.875932	test: 0.783383
PRC train: 0.915530	val: 0.618925	test: 0.617243

Epoch: 125
Loss: 0.1049506043789092
ROC train: 0.986070	val: 0.846526	test: 0.750703
PRC train: 0.920003	val: 0.607992	test: 0.607778

Epoch: 126
Loss: 0.10685159363964133
ROC train: 0.986441	val: 0.810851	test: 0.737637
PRC train: 0.921924	val: 0.594473	test: 0.608793

Epoch: 127
Loss: 0.10329271313366459
ROC train: 0.985557	val: 0.784903	test: 0.745377
PRC train: 0.918209	val: 0.606808	test: 0.610351

Epoch: 128
Loss: 0.10061155731442946
ROC train: 0.986331	val: 0.789822	test: 0.734429
PRC train: 0.925104	val: 0.601241	test: 0.611663

Epoch: 129
Loss: 0.09547808144170293
ROC train: 0.987741	val: 0.827507	test: 0.758947
PRC train: 0.933301	val: 0.624563	test: 0.611785

Epoch: 130
Loss: 0.11405032719382677
ROC train: 0.987404	val: 0.825859	test: 0.785154
PRC train: 0.927566	val: 0.647862	test: 0.618511

Epoch: 131
Loss: 0.09852416362930294
ROC train: 0.988226	val: 0.802909	test: 0.768510
PRC train: 0.934077	val: 0.587921	test: 0.616437

Epoch: 132
Loss: 0.0874993819415022
ROC train: 0.987493	val: 0.790385	test: 0.766268
PRC train: 0.933483	val: 0.584178	test: 0.619729

Epoch: 133
Loss: 0.09698040650498148
ROC train: 0.987532	val: 0.813560	test: 0.746419
PRC train: 0.934362	val: 0.607087	test: 0.607140

Epoch: 134
Loss: 0.09595218362349293
ROC train: 0.987983	val: 0.840706	test: 0.764632
PRC train: 0.937369	val: 0.641778	test: 0.614382

Epoch: 135
Loss: 0.09716041829858091
ROC train: 0.987684	val: 0.867853	test: 0.770702
PRC train: 0.933279	val: 0.672840	test: 0.624622

Epoch: 136
Loss: 0.09949610765266417
ROC train: 0.987399	val: 0.862708	test: 0.755014
PRC train: 0.929870	val: 0.625388	test: 0.620421

Epoch: 137
Loss: 0.1044057872126378
ROC train: 0.987611	val: 0.826334	test: 0.756601
PRC train: 0.931568	val: 0.609711	test: 0.624019

Epoch: 138
Loss: 0.09606539856571636
ROC train: 0.987762	val: 0.833126	test: 0.777160
PRC train: 0.932889	val: 0.619553	test: 0.628880

Epoch: 139
Loss: 0.09162943442254735
ROC train: 0.987205	val: 0.841205	test: 0.782368
PRC train: 0.927333	val: 0.669222	test: 0.621376

Epoch: 140
Loss: 0.10374647991928795
ROC train: 0.986187	val: 0.863632	test: 0.799894
PRC train: 0.918070	val: 0.683272	test: 0.630716

Epoch: 141
Loss: 0.09693266475894227
ROC train: 0.987816	val: 0.841269	test: 0.769940
PRC train: 0.931287	val: 0.646105	test: 0.621282

Early stopping
Best (ROC):	 train: 0.986270	val: 0.893326	test: 0.804144
Best (PRC):	 train: 0.922987	val: 0.637351	test: 0.622741
All runs completed.

ROC train: 0.999990	val: 0.861172	test: 0.723783
PRC train: 0.999892	val: 0.651740	test: 0.628499

Epoch: 94
Loss: 0.019067628053151032
ROC train: 1.000000	val: 0.908648	test: 0.711322
PRC train: 1.000000	val: 0.713529	test: 0.593641

Epoch: 95
Loss: 0.02250908579276175
ROC train: 1.000000	val: 0.925617	test: 0.728254
PRC train: 1.000000	val: 0.682406	test: 0.611383

Epoch: 96
Loss: 0.016924795504726343
ROC train: 0.999983	val: 0.901679	test: 0.767635
PRC train: 0.999999	val: 0.669095	test: 0.606896

Epoch: 97
Loss: 0.021932515171146756
ROC train: 1.000000	val: 0.885160	test: 0.748227
PRC train: 1.000000	val: 0.647254	test: 0.589445

Epoch: 98
Loss: 0.021199113494084747
ROC train: 0.999994	val: 0.860835	test: 0.737772
PRC train: 1.000000	val: 0.634838	test: 0.578027

Epoch: 99
Loss: 0.021667679028489574
ROC train: 0.999977	val: 0.835924	test: 0.745631
PRC train: 0.999998	val: 0.667905	test: 0.603636

Epoch: 100
Loss: 0.018244725993807043
ROC train: 1.000000	val: 0.823062	test: 0.731990
PRC train: 1.000000	val: 0.650836	test: 0.602425

Epoch: 101
Loss: 0.014553880441649078
ROC train: 0.999994	val: 0.830417	test: 0.734571
PRC train: 1.000000	val: 0.679463	test: 0.605390

Epoch: 102
Loss: 0.020017933200786434
ROC train: 0.999977	val: 0.836623	test: 0.749504
PRC train: 0.999998	val: 0.699296	test: 0.613273

Epoch: 103
Loss: 0.018096807176328316
ROC train: 1.000000	val: 0.830168	test: 0.699373
PRC train: 1.000000	val: 0.626603	test: 0.605049

Epoch: 104
Loss: 0.017322309729263706
ROC train: 1.000000	val: 0.794517	test: 0.675045
PRC train: 1.000000	val: 0.624735	test: 0.578611

Epoch: 105
Loss: 0.021500204676909957
ROC train: 1.000000	val: 0.858038	test: 0.709204
PRC train: 1.000000	val: 0.633964	test: 0.572582

Epoch: 106
Loss: 0.018604795497867102
ROC train: 1.000000	val: 0.869139	test: 0.741485
PRC train: 1.000000	val: 0.647692	test: 0.584352

Epoch: 107
Loss: 0.026225985336643463
ROC train: 1.000000	val: 0.897571	test: 0.748642
PRC train: 1.000000	val: 0.667479	test: 0.618438

Epoch: 108
Loss: 0.021589719782785483
ROC train: 0.999974	val: 0.854292	test: 0.759634
PRC train: 0.999836	val: 0.658138	test: 0.621576

Epoch: 109
Loss: 0.026611702432411503
ROC train: 0.999804	val: 0.836261	test: 0.733756
PRC train: 0.998578	val: 0.638617	test: 0.604832

Epoch: 110
Loss: 0.020025841150577255
ROC train: 0.998927	val: 0.838496	test: 0.737917
PRC train: 0.995020	val: 0.592558	test: 0.608571

Epoch: 111
Loss: 0.01987230302233591
ROC train: 0.999986	val: 0.856727	test: 0.770964
PRC train: 0.999839	val: 0.640458	test: 0.614449

Epoch: 112
Loss: 0.02364022822170052
ROC train: 0.999995	val: 0.864219	test: 0.761581
PRC train: 0.999946	val: 0.649091	test: 0.612177

Epoch: 113
Loss: 0.018629784874989938
ROC train: 0.999995	val: 0.842193	test: 0.683524
PRC train: 0.999946	val: 0.591788	test: 0.591480

Epoch: 114
Loss: 0.020002190347533626
ROC train: 1.000000	val: 0.829043	test: 0.705869
PRC train: 1.000000	val: 0.614164	test: 0.580410

Epoch: 115
Loss: 0.014826657851894418
ROC train: 1.000000	val: 0.809839	test: 0.683846
PRC train: 1.000000	val: 0.578032	test: 0.562481

Epoch: 116
Loss: 0.01803412702901837
ROC train: 0.999957	val: 0.759092	test: 0.640811
PRC train: 0.999554	val: 0.570745	test: 0.592797

Epoch: 117
Loss: 0.016068346247143428
ROC train: 0.999986	val: 0.817169	test: 0.653227
PRC train: 0.999842	val: 0.594124	test: 0.595461

Epoch: 118
Loss: 0.01553928706900603
ROC train: 1.000000	val: 0.874396	test: 0.723682
PRC train: 1.000000	val: 0.675592	test: 0.619943

Epoch: 119
Loss: 0.016903801497159403
ROC train: 1.000000	val: 0.881751	test: 0.752982
PRC train: 1.000000	val: 0.720599	test: 0.634882

Epoch: 120
Loss: 0.012317467289539391
ROC train: 1.000000	val: 0.874146	test: 0.730040
PRC train: 1.000000	val: 0.655190	test: 0.623133

Epoch: 121
Loss: 0.010651359219675288
ROC train: 1.000000	val: 0.865031	test: 0.727679
PRC train: 1.000000	val: 0.647270	test: 0.621118

Epoch: 122
Loss: 0.011044154509866134
ROC train: 1.000000	val: 0.839557	test: 0.713489
PRC train: 1.000000	val: 0.639478	test: 0.617207

Epoch: 123
Loss: 0.0110199871437295
ROC train: 1.000000	val: 0.846575	test: 0.709380
PRC train: 1.000000	val: 0.637160	test: 0.612594

Epoch: 124
Loss: 0.009516723624101425
ROC train: 1.000000	val: 0.846188	test: 0.703390
PRC train: 1.000000	val: 0.645811	test: 0.610387

Epoch: 125
Loss: 0.015011114499662182
ROC train: 1.000000	val: 0.846975	test: 0.715300
PRC train: 1.000000	val: 0.646742	test: 0.612299

Epoch: 126
Loss: 0.012832104054781898
ROC train: 1.000000	val: 0.838784	test: 0.734825
PRC train: 1.000000	val: 0.692302	test: 0.626648

Epoch: 127
Loss: 0.009508539195247087
ROC train: 1.000000	val: 0.833214	test: 0.725449
PRC train: 1.000000	val: 0.636997	test: 0.626276

Epoch: 128
Loss: 0.013894049015353783
ROC train: 1.000000	val: 0.837097	test: 0.718871
PRC train: 1.000000	val: 0.634688	test: 0.605851

Epoch: 129
Loss: 0.012217612415676673
ROC train: 1.000000	val: 0.840594	test: 0.725356
PRC train: 1.000000	val: 0.643692	test: 0.601495

Epoch: 130
Loss: 0.012551750141511591
ROC train: 1.000000	val: 0.836936	test: 0.707680
PRC train: 1.000000	val: 0.642791	test: 0.598779

Early stopping
Best (ROC):	 train: 1.000000	val: 0.925617	test: 0.728254
Best (PRC):	 train: 1.000000	val: 0.682406	test: 0.611383
All runs completed.

ROC train: 1.000000	val: 0.812123	test: 0.588331
PRC train: 1.000000	val: 0.552782	test: 0.528214

Epoch: 94
Loss: 0.017075484231457973
ROC train: 1.000000	val: 0.805892	test: 0.573214
PRC train: 1.000000	val: 0.552405	test: 0.526883

Epoch: 95
Loss: 0.026905657610683847
ROC train: 0.999995	val: 0.841279	test: 0.610104
PRC train: 0.999946	val: 0.563021	test: 0.544204

Epoch: 96
Loss: 0.020490624514221255
ROC train: 1.000000	val: 0.828305	test: 0.629090
PRC train: 1.000000	val: 0.563397	test: 0.541647

Epoch: 97
Loss: 0.024840805066258388
ROC train: 1.000000	val: 0.808739	test: 0.574801
PRC train: 1.000000	val: 0.549948	test: 0.535525

Epoch: 98
Loss: 0.02415884052097169
ROC train: 0.999990	val: 0.775973	test: 0.554003
PRC train: 0.999946	val: 0.542188	test: 0.536266

Epoch: 99
Loss: 0.014883031661030336
ROC train: 0.999989	val: 0.772452	test: 0.548595
PRC train: 0.999999	val: 0.544501	test: 0.545523

Epoch: 100
Loss: 0.02425024384291291
ROC train: 1.000000	val: 0.792131	test: 0.555139
PRC train: 1.000000	val: 0.549629	test: 0.564520

Epoch: 101
Loss: 0.022059330513797142
ROC train: 0.999994	val: 0.815029	test: 0.543911
PRC train: 1.000000	val: 0.570616	test: 0.530663

Epoch: 102
Loss: 0.018415793920475265
ROC train: 0.999944	val: 0.800969	test: 0.539850
PRC train: 0.999943	val: 0.560022	test: 0.524912

Epoch: 103
Loss: 0.01901714429755572
ROC train: 1.000000	val: 0.803556	test: 0.531745
PRC train: 1.000000	val: 0.552076	test: 0.530628

Epoch: 104
Loss: 0.013215956684672878
ROC train: 1.000000	val: 0.801335	test: 0.554666
PRC train: 1.000000	val: 0.545465	test: 0.542188

Epoch: 105
Loss: 0.01985647843769788
ROC train: 1.000000	val: 0.811736	test: 0.572067
PRC train: 1.000000	val: 0.547693	test: 0.539134

Epoch: 106
Loss: 0.014763841866132308
ROC train: 1.000000	val: 0.800597	test: 0.605494
PRC train: 1.000000	val: 0.552265	test: 0.536420

Epoch: 107
Loss: 0.016616201597716106
ROC train: 0.999971	val: 0.795213	test: 0.611516
PRC train: 0.999694	val: 0.558613	test: 0.534375

Epoch: 108
Loss: 0.018379149931138525
ROC train: 1.000000	val: 0.838844	test: 0.579112
PRC train: 1.000000	val: 0.563163	test: 0.531040

Epoch: 109
Loss: 0.013043535023645376
ROC train: 1.000000	val: 0.855265	test: 0.571705
PRC train: 1.000000	val: 0.562055	test: 0.531412

Epoch: 110
Loss: 0.012984943770938886
ROC train: 1.000000	val: 0.838496	test: 0.584121
PRC train: 1.000000	val: 0.558083	test: 0.534649

Epoch: 111
Loss: 0.013550790112342514
ROC train: 1.000000	val: 0.822676	test: 0.579362
PRC train: 1.000000	val: 0.550927	test: 0.532641

Epoch: 112
Loss: 0.011202437532023258
ROC train: 1.000000	val: 0.820465	test: 0.572179
PRC train: 1.000000	val: 0.550764	test: 0.566891

Epoch: 113
Loss: 0.010432827115087553
ROC train: 1.000000	val: 0.830716	test: 0.571167
PRC train: 1.000000	val: 0.559055	test: 0.529506

Epoch: 114
Loss: 0.009884679218261572
ROC train: 1.000000	val: 0.820676	test: 0.569105
PRC train: 1.000000	val: 0.554885	test: 0.529550

Epoch: 115
Loss: 0.010154224954329698
ROC train: 1.000000	val: 0.820202	test: 0.569755
PRC train: 1.000000	val: 0.552063	test: 0.533543

Epoch: 116
Loss: 0.008668043711088555
ROC train: 1.000000	val: 0.815008	test: 0.587980
PRC train: 1.000000	val: 0.548148	test: 0.543728

Epoch: 117
Loss: 0.009267005618414526
ROC train: 1.000000	val: 0.802958	test: 0.588417
PRC train: 1.000000	val: 0.547377	test: 0.532207

Epoch: 118
Loss: 0.009431899578453145
ROC train: 1.000000	val: 0.799711	test: 0.574876
PRC train: 1.000000	val: 0.553122	test: 0.528496

Epoch: 119
Loss: 0.011307153672560728
ROC train: 1.000000	val: 0.814607	test: 0.563484
PRC train: 1.000000	val: 0.564017	test: 0.526507

Epoch: 120
Loss: 0.009300399943851153
ROC train: 1.000000	val: 0.801957	test: 0.564981
PRC train: 1.000000	val: 0.584813	test: 0.526522

Epoch: 121
Loss: 0.007712618805120254
ROC train: 1.000000	val: 0.797487	test: 0.546081
PRC train: 1.000000	val: 0.582358	test: 0.526908

Epoch: 122
Loss: 0.008836746066570986
ROC train: 1.000000	val: 0.799623	test: 0.532141
PRC train: 1.000000	val: 0.579310	test: 0.537587

Epoch: 123
Loss: 0.01114180934278619
ROC train: 1.000000	val: 0.809326	test: 0.535857
PRC train: 1.000000	val: 0.581668	test: 0.539011

Epoch: 124
Loss: 0.006231668503110287
ROC train: 1.000000	val: 0.808377	test: 0.539025
PRC train: 1.000000	val: 0.553508	test: 0.532866

Epoch: 125
Loss: 0.011475425031970972
ROC train: 1.000000	val: 0.802034	test: 0.551542
PRC train: 1.000000	val: 0.549142	test: 0.542252

Epoch: 126
Loss: 0.00740074678155155
ROC train: 1.000000	val: 0.793618	test: 0.558512
PRC train: 1.000000	val: 0.546190	test: 0.567485

Epoch: 127
Loss: 0.007742448509474062
ROC train: 1.000000	val: 0.790958	test: 0.559804
PRC train: 1.000000	val: 0.545302	test: 0.564996

Epoch: 128
Loss: 0.008576458460031937
ROC train: 1.000000	val: 0.783965	test: 0.552752
PRC train: 1.000000	val: 0.544273	test: 0.564099

Epoch: 129
Loss: 0.008425669193847558
ROC train: 1.000000	val: 0.770878	test: 0.533071
PRC train: 1.000000	val: 0.541011	test: 0.537502

Epoch: 130
Loss: 0.008966433718037573
ROC train: 1.000000	val: 0.780830	test: 0.529354
PRC train: 1.000000	val: 0.544858	test: 0.529332

Epoch: 131
Loss: 0.014125072067298882
ROC train: 1.000000	val: 0.785300	test: 0.524196
PRC train: 1.000000	val: 0.547479	test: 0.529127

Epoch: 132
Loss: 0.007900567473852689
ROC train: 1.000000	val: 0.787222	test: 0.523721
PRC train: 1.000000	val: 0.558384	test: 0.531954

Epoch: 133
Loss: 0.011010844538251841
ROC train: 1.000000	val: 0.792029	test: 0.550885
PRC train: 1.000000	val: 0.569453	test: 0.535317

Epoch: 134
Loss: 0.009048584715312655
ROC train: 1.000000	val: 0.790993	test: 0.580636
PRC train: 1.000000	val: 0.567328	test: 0.533610

Epoch: 135
Loss: 0.007705951275227087
ROC train: 1.000000	val: 0.796201	test: 0.583247
PRC train: 1.000000	val: 0.555264	test: 0.534604

Epoch: 136
Loss: 0.009685585079045678
ROC train: 1.000000	val: 0.790033	test: 0.605946
PRC train: 1.000000	val: 0.547480	test: 0.541248

Epoch: 137
Loss: 0.005602093195543561
ROC train: 1.000000	val: 0.778795	test: 0.624234
PRC train: 1.000000	val: 0.543953	test: 0.547326

Epoch: 138
Loss: 0.012546971450369853
ROC train: 1.000000	val: 0.780869	test: 0.605057
PRC train: 1.000000	val: 0.542190	test: 0.535718

Epoch: 139
Loss: 0.009229103948420872
ROC train: 1.000000	val: 0.789671	test: 0.585507
PRC train: 1.000000	val: 0.550568	test: 0.533808

Epoch: 140
Loss: 0.011298952811257767
ROC train: 1.000000	val: 0.788522	test: 0.572852
PRC train: 1.000000	val: 0.551615	test: 0.529690

Epoch: 141
Loss: 0.009848810197141117
ROC train: 1.000000	val: 0.762198	test: 0.574338
PRC train: 1.000000	val: 0.548498	test: 0.530348

Epoch: 142
Loss: 0.018037957444124696
ROC train: 1.000000	val: 0.713412	test: 0.564970
PRC train: 1.000000	val: 0.537057	test: 0.566555

Epoch: 143
Loss: 0.009844686992959893
ROC train: 1.000000	val: 0.686428	test: 0.538689
PRC train: 1.000000	val: 0.532591	test: 0.564819

Epoch: 144
Loss: 0.030167857776054973
ROC train: 1.000000	val: 0.718532	test: 0.529268
PRC train: 1.000000	val: 0.540332	test: 0.562627

Early stopping
Best (ROC):	 train: 1.000000	val: 0.855265	test: 0.571705
Best (PRC):	 train: 1.000000	val: 0.562055	test: 0.531412
All runs completed.
