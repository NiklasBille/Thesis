>>> Starting run for dataset: tox21
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running SCAFF configs_split_experiments/GraphMVP/tox21/scaff/train_prop=0.6.yml on cuda:0
Running SCAFF configs_split_experiments/GraphMVP/tox21/scaff/train_prop=0.7.yml on cuda:1
Running SCAFF configs_split_experiments/GraphMVP/tox21/scaff/train_prop=0.8.yml on cuda:2
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphMVP/tox21/scaff/train_prop=0.8.yml --runseed 4 --device cuda:2
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphMVP/tox21/scaff/train_prop=0.8.yml --runseed 5 --device cuda:2
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphMVP/tox21/scaff/train_prop=0.8.yml --runseed 6 --device cuda:2
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphMVP/tox21/scaff/train_prop=0.6.yml --runseed 4 --device cuda:0
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphMVP/tox21/scaff/train_prop=0.6.yml --runseed 5 --device cuda:0
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphMVP/tox21/scaff/train_prop=0.6.yml --runseed 6 --device cuda:0
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphMVP/tox21/scaff/train_prop=0.7.yml --runseed 4 --device cuda:1
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphMVP/tox21/scaff/train_prop=0.7.yml --runseed 5 --device cuda:1
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphMVP/tox21/scaff/train_prop=0.7.yml --runseed 6 --device cuda:1
[11:13:31] WARNING: not removing hydrogen atom without neighbors
[11:13:31] WARNING: not removing hydrogen atom without neighbors
[11:13:31] WARNING: not removing hydrogen atom without neighbors
[11:13:31] WARNING: not removing hydrogen atom without neighbors
[11:13:31] WARNING: not removing hydrogen atom without neighbors
[11:13:31] WARNING: not removing hydrogen atom without neighbors
[11:13:31] WARNING: not removing hydrogen atom without neighbors
[11:13:31] WARNING: not removing hydrogen atom without neighbors
[11:13:31] WARNING: not removing hydrogen atom without neighbors
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/tox21/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/split/GraphMVP/tox21/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/tox21/scaff/train_prop=0.6/tox21_scaff_4_26-05_11-13-31  ]
[ Using Seed :  4  ]
[ Using device :  cuda:0  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.590726353971653
ROC train: 0.681122	val: 0.589558	test: 0.560929
PRC train: 0.205162	val: 0.157873	test: 0.140809

Epoch: 2
Loss: 0.4047098811204396
ROC train: 0.745418	val: 0.629278	test: 0.610572
PRC train: 0.241252	val: 0.179383	test: 0.166754

Epoch: 3
Loss: 0.2919831953640022
ROC train: 0.796089	val: 0.702507	test: 0.672524
PRC train: 0.325563	val: 0.228162	test: 0.211212

Epoch: 4
Loss: 0.22362326861598733
ROC train: 0.808353	val: 0.697639	test: 0.668879
PRC train: 0.334656	val: 0.238625	test: 0.214540

Epoch: 5
Loss: 0.1927107496670585
ROC train: 0.821290	val: 0.694431	test: 0.671167
PRC train: 0.348647	val: 0.238709	test: 0.215706

Epoch: 6
Loss: 0.1790575791946336
ROC train: 0.841286	val: 0.720393	test: 0.688961
PRC train: 0.377525	val: 0.248226	test: 0.220274

Epoch: 7
Loss: 0.17341019378368183
ROC train: 0.850362	val: 0.713361	test: 0.683671
PRC train: 0.412341	val: 0.268004	test: 0.237017

Epoch: 8
Loss: 0.17027031435088352
ROC train: 0.858648	val: 0.734075	test: 0.704177
PRC train: 0.423696	val: 0.276308	test: 0.245742

Epoch: 9
Loss: 0.16687989956398483
ROC train: 0.862157	val: 0.730055	test: 0.694563
PRC train: 0.434017	val: 0.286673	test: 0.246650

Epoch: 10
Loss: 0.16500497718305704
ROC train: 0.866722	val: 0.727664	test: 0.694095
PRC train: 0.447045	val: 0.294903	test: 0.249584

Epoch: 11
Loss: 0.16382102144555585
ROC train: 0.869863	val: 0.722078	test: 0.681274
PRC train: 0.463634	val: 0.283247	test: 0.238560

Epoch: 12
Loss: 0.16020534651879972
ROC train: 0.871092	val: 0.732764	test: 0.706164
PRC train: 0.471356	val: 0.286941	test: 0.255723

Epoch: 13
Loss: 0.1581296098538779
ROC train: 0.876911	val: 0.725661	test: 0.689344
PRC train: 0.494868	val: 0.290210	test: 0.248260

Epoch: 14
Loss: 0.15579388902557564
ROC train: 0.881903	val: 0.718448	test: 0.692243
PRC train: 0.506450	val: 0.289720	test: 0.250797

Epoch: 15
Loss: 0.15589810589277422
ROC train: 0.885622	val: 0.747723	test: 0.713686
PRC train: 0.528280	val: 0.307440	test: 0.263239

Epoch: 16
Loss: 0.15389754829555433
ROC train: 0.885794	val: 0.742586	test: 0.711965
PRC train: 0.527586	val: 0.303117	test: 0.257843

Epoch: 17
Loss: 0.1510516120726078
ROC train: 0.890394	val: 0.742867	test: 0.713219
PRC train: 0.533166	val: 0.311393	test: 0.260808

Epoch: 18
Loss: 0.15097190142568584
ROC train: 0.892875	val: 0.730842	test: 0.703430
PRC train: 0.548291	val: 0.295358	test: 0.252605

Epoch: 19
Loss: 0.14875973264802314
ROC train: 0.886876	val: 0.732356	test: 0.703386
PRC train: 0.532491	val: 0.297571	test: 0.258469

Epoch: 20
Loss: 0.15037746295785692
ROC train: 0.894155	val: 0.735013	test: 0.702991
PRC train: 0.547196	val: 0.304146	test: 0.258965

Epoch: 21
Loss: 0.14838411921486278
ROC train: 0.899190	val: 0.743725	test: 0.713658
PRC train: 0.567189	val: 0.310629	test: 0.265551

Epoch: 22
Loss: 0.1459964422206791
ROC train: 0.900850	val: 0.737770	test: 0.715630
PRC train: 0.570734	val: 0.309749	test: 0.269266

Epoch: 23
Loss: 0.14287867875119903
ROC train: 0.899125	val: 0.746151	test: 0.716000
PRC train: 0.557645	val: 0.308010	test: 0.268450

Epoch: 24
Loss: 0.1443929916297282
ROC train: 0.903815	val: 0.745274	test: 0.718517
PRC train: 0.575495	val: 0.320087	test: 0.281316

Epoch: 25
Loss: 0.1416839598889847
ROC train: 0.905707	val: 0.741578	test: 0.716460
PRC train: 0.571519	val: 0.304122	test: 0.260630

Epoch: 26
Loss: 0.1395356813011012
ROC train: 0.898759	val: 0.727619	test: 0.710836
PRC train: 0.564946	val: 0.290061	test: 0.268371

Epoch: 27
Loss: 0.1402144920088161
ROC train: 0.908845	val: 0.750022	test: 0.727305
PRC train: 0.587882	val: 0.325086	test: 0.284095

Epoch: 28
Loss: 0.14026301560706267
ROC train: 0.912188	val: 0.748828	test: 0.719600
PRC train: 0.598899	val: 0.319380	test: 0.270149

Epoch: 29
Loss: 0.13875583683425058
ROC train: 0.914811	val: 0.755457	test: 0.725196
PRC train: 0.607982	val: 0.328223	test: 0.282355

Epoch: 30
Loss: 0.13961082768064284
ROC train: 0.914465	val: 0.740797	test: 0.714971
PRC train: 0.604492	val: 0.322200	test: 0.275215

Epoch: 31
Loss: 0.1355611197561008
ROC train: 0.917224	val: 0.750216	test: 0.721541
PRC train: 0.612506	val: 0.314104	test: 0.267248

Epoch: 32
Loss: 0.13641901594670555
ROC train: 0.918448	val: 0.747584	test: 0.717310
PRC train: 0.614153	val: 0.327857	test: 0.279171

Epoch: 33
Loss: 0.13456285539199847Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/tox21/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/split/GraphMVP/tox21/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/tox21/scaff/train_prop=0.6/tox21_scaff_5_26-05_11-13-31  ]
[ Using Seed :  5  ]
[ Using device :  cuda:0  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5796035210893212
ROC train: 0.690439	val: 0.583059	test: 0.554968
PRC train: 0.211723	val: 0.166433	test: 0.141507

Epoch: 2
Loss: 0.3997578019145976
ROC train: 0.760038	val: 0.691919	test: 0.652244
PRC train: 0.277106	val: 0.206145	test: 0.191430

Epoch: 3
Loss: 0.28639563366287135
ROC train: 0.785508	val: 0.688285	test: 0.655024
PRC train: 0.299693	val: 0.221874	test: 0.199171

Epoch: 4
Loss: 0.22023405117468015
ROC train: 0.815023	val: 0.710003	test: 0.671770
PRC train: 0.344205	val: 0.237942	test: 0.209173

Epoch: 5
Loss: 0.19084914139263187
ROC train: 0.831373	val: 0.718694	test: 0.693795
PRC train: 0.369158	val: 0.261570	test: 0.233315

Epoch: 6
Loss: 0.17832211270986453
ROC train: 0.844680	val: 0.723061	test: 0.690957
PRC train: 0.393430	val: 0.262580	test: 0.228388

Epoch: 7
Loss: 0.1740157390342614
ROC train: 0.852254	val: 0.725951	test: 0.694389
PRC train: 0.397777	val: 0.264398	test: 0.238819

Epoch: 8
Loss: 0.17068921337022142
ROC train: 0.850135	val: 0.703239	test: 0.681840
PRC train: 0.422868	val: 0.268111	test: 0.237022

Epoch: 9
Loss: 0.16901054515922112
ROC train: 0.859617	val: 0.741166	test: 0.714526
PRC train: 0.432812	val: 0.284058	test: 0.253318

Epoch: 10
Loss: 0.1635817696080009
ROC train: 0.865480	val: 0.732974	test: 0.709312
PRC train: 0.458308	val: 0.283421	test: 0.252931

Epoch: 11
Loss: 0.16187397538848153
ROC train: 0.874378	val: 0.740862	test: 0.708178
PRC train: 0.471542	val: 0.285895	test: 0.251386

Epoch: 12
Loss: 0.1609755248891216
ROC train: 0.866715	val: 0.742758	test: 0.714207
PRC train: 0.464509	val: 0.282923	test: 0.245813

Epoch: 13
Loss: 0.15775668037934062
ROC train: 0.878826	val: 0.736501	test: 0.711796
PRC train: 0.497852	val: 0.283718	test: 0.254411

Epoch: 14
Loss: 0.15392801859332533
ROC train: 0.883442	val: 0.734645	test: 0.707164
PRC train: 0.518416	val: 0.296923	test: 0.254919

Epoch: 15
Loss: 0.15415849097012047
ROC train: 0.884853	val: 0.732667	test: 0.718460
PRC train: 0.516100	val: 0.274793	test: 0.255527

Epoch: 16
Loss: 0.15285009951562648
ROC train: 0.887499	val: 0.734745	test: 0.711203
PRC train: 0.535574	val: 0.297935	test: 0.260197

Epoch: 17
Loss: 0.1502123131149073
ROC train: 0.891243	val: 0.745189	test: 0.714630
PRC train: 0.535532	val: 0.312218	test: 0.272559

Epoch: 18
Loss: 0.1493597599725128
ROC train: 0.894955	val: 0.742245	test: 0.711447
PRC train: 0.540448	val: 0.298836	test: 0.249919

Epoch: 19
Loss: 0.14977016866257373
ROC train: 0.891099	val: 0.745761	test: 0.712945
PRC train: 0.529051	val: 0.307433	test: 0.256683

Epoch: 20
Loss: 0.1478726593309585
ROC train: 0.896382	val: 0.740710	test: 0.716347
PRC train: 0.555091	val: 0.299197	test: 0.257207

Epoch: 21
Loss: 0.147062382011877
ROC train: 0.895162	val: 0.747418	test: 0.725761
PRC train: 0.557737	val: 0.302377	test: 0.267618

Epoch: 22
Loss: 0.14453824426097042
ROC train: 0.906489	val: 0.728746	test: 0.710284
PRC train: 0.582294	val: 0.290859	test: 0.254842

Epoch: 23
Loss: 0.144714572991108
ROC train: 0.906910	val: 0.733839	test: 0.715659
PRC train: 0.579194	val: 0.291156	test: 0.254462

Epoch: 24
Loss: 0.14221279085309646
ROC train: 0.908636	val: 0.748136	test: 0.723018
PRC train: 0.586567	val: 0.316010	test: 0.273802

Epoch: 25
Loss: 0.14105710177989128
ROC train: 0.910589	val: 0.740615	test: 0.717016
PRC train: 0.590773	val: 0.293585	test: 0.258058

Epoch: 26
Loss: 0.14104313382443956
ROC train: 0.910426	val: 0.740589	test: 0.716498
PRC train: 0.590244	val: 0.318849	test: 0.276555

Epoch: 27
Loss: 0.1392627137323675
ROC train: 0.910699	val: 0.732548	test: 0.700155
PRC train: 0.588257	val: 0.293996	test: 0.255581

Epoch: 28
Loss: 0.14152941715108344
ROC train: 0.911112	val: 0.760487	test: 0.730347
PRC train: 0.599006	val: 0.314340	test: 0.274848

Epoch: 29
Loss: 0.13853156635953182
ROC train: 0.917543	val: 0.737720	test: 0.714086
PRC train: 0.611489	val: 0.289783	test: 0.256702

Epoch: 30
Loss: 0.13601609004284365
ROC train: 0.922203	val: 0.736412	test: 0.713371
PRC train: 0.629541	val: 0.310466	test: 0.267173

Epoch: 31
Loss: 0.13681557919119627
ROC train: 0.919216	val: 0.741955	test: 0.718208
PRC train: 0.617102	val: 0.309602	test: 0.267255

Epoch: 32
Loss: 0.13646028578485436
ROC train: 0.921536	val: 0.744561	test: 0.718842
PRC train: 0.628617	val: 0.315905	test: 0.270840

Epoch: 33
Loss: 0.13753652625194776Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/tox21/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/split/GraphMVP/tox21/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/tox21/scaff/train_prop=0.6/tox21_scaff_6_26-05_11-13-31  ]
[ Using Seed :  6  ]
[ Using device :  cuda:0  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5831931829799588
ROC train: 0.684914	val: 0.609879	test: 0.572048
PRC train: 0.207024	val: 0.176048	test: 0.154321

Epoch: 2
Loss: 0.4016917555037029
ROC train: 0.763390	val: 0.674046	test: 0.651325
PRC train: 0.265621	val: 0.201975	test: 0.192133

Epoch: 3
Loss: 0.28773679570450844
ROC train: 0.786936	val: 0.664619	test: 0.646865
PRC train: 0.296055	val: 0.187357	test: 0.176747

Epoch: 4
Loss: 0.22199784625442956
ROC train: 0.813651	val: 0.700380	test: 0.671254
PRC train: 0.327736	val: 0.232760	test: 0.210697

Epoch: 5
Loss: 0.1926203659806315
ROC train: 0.825661	val: 0.699276	test: 0.669158
PRC train: 0.333599	val: 0.236225	test: 0.210684

Epoch: 6
Loss: 0.1778372732061669
ROC train: 0.843209	val: 0.725143	test: 0.691337
PRC train: 0.370496	val: 0.270869	test: 0.236646

Epoch: 7
Loss: 0.17307025440349888
ROC train: 0.853718	val: 0.725643	test: 0.706101
PRC train: 0.408312	val: 0.288276	test: 0.252507

Epoch: 8
Loss: 0.16903332398848533
ROC train: 0.859256	val: 0.720072	test: 0.700897
PRC train: 0.429476	val: 0.280827	test: 0.248852

Epoch: 9
Loss: 0.165609885024674
ROC train: 0.863666	val: 0.730709	test: 0.699871
PRC train: 0.447529	val: 0.285166	test: 0.245162

Epoch: 10
Loss: 0.1631566769461473
ROC train: 0.859643	val: 0.727890	test: 0.707338
PRC train: 0.438240	val: 0.292081	test: 0.261566

Epoch: 11
Loss: 0.16047636541140675
ROC train: 0.866873	val: 0.729746	test: 0.697043
PRC train: 0.447036	val: 0.280269	test: 0.247878

Epoch: 12
Loss: 0.16089679421076722
ROC train: 0.874079	val: 0.722024	test: 0.700667
PRC train: 0.481558	val: 0.272603	test: 0.247832

Epoch: 13
Loss: 0.1564399735053236
ROC train: 0.878359	val: 0.736915	test: 0.716883
PRC train: 0.496692	val: 0.294597	test: 0.266934

Epoch: 14
Loss: 0.15723783383384454
ROC train: 0.885208	val: 0.737035	test: 0.704015
PRC train: 0.514404	val: 0.290611	test: 0.254094

Epoch: 15
Loss: 0.15546233772189452
ROC train: 0.885527	val: 0.742743	test: 0.705238
PRC train: 0.518603	val: 0.295006	test: 0.257307

Epoch: 16
Loss: 0.15203198019440517
ROC train: 0.886287	val: 0.726178	test: 0.694668
PRC train: 0.516456	val: 0.276930	test: 0.251315

Epoch: 17
Loss: 0.15108677095129294
ROC train: 0.891743	val: 0.746686	test: 0.714632
PRC train: 0.535791	val: 0.308929	test: 0.271123

Epoch: 18
Loss: 0.14872509095622047
ROC train: 0.893872	val: 0.733634	test: 0.701749
PRC train: 0.538156	val: 0.305693	test: 0.260682

Epoch: 19
Loss: 0.1489347235826932
ROC train: 0.895446	val: 0.737405	test: 0.706763
PRC train: 0.540453	val: 0.295411	test: 0.261366

Epoch: 20
Loss: 0.14694330091060478
ROC train: 0.897168	val: 0.733778	test: 0.710940
PRC train: 0.556175	val: 0.299503	test: 0.266052

Epoch: 21
Loss: 0.14332770596857589
ROC train: 0.901648	val: 0.739083	test: 0.705591
PRC train: 0.560261	val: 0.294336	test: 0.254911

Epoch: 22
Loss: 0.1451075754476422
ROC train: 0.903001	val: 0.744942	test: 0.709905
PRC train: 0.562106	val: 0.307632	test: 0.261204

Epoch: 23
Loss: 0.14393590631341183
ROC train: 0.906917	val: 0.719493	test: 0.701461
PRC train: 0.583552	val: 0.289868	test: 0.256942

Epoch: 24
Loss: 0.14319216841888707
ROC train: 0.906811	val: 0.732951	test: 0.701827
PRC train: 0.567559	val: 0.310657	test: 0.260448

Epoch: 25
Loss: 0.14398041433864026
ROC train: 0.907576	val: 0.737199	test: 0.720756
PRC train: 0.582230	val: 0.313444	test: 0.269739

Epoch: 26
Loss: 0.14112562480372212
ROC train: 0.912097	val: 0.738571	test: 0.707923
PRC train: 0.588770	val: 0.298921	test: 0.262325

Epoch: 27
Loss: 0.13980513164875533
ROC train: 0.912393	val: 0.735908	test: 0.721955
PRC train: 0.593322	val: 0.312479	test: 0.269692

Epoch: 28
Loss: 0.14094062081436468
ROC train: 0.916349	val: 0.733605	test: 0.705199
PRC train: 0.599925	val: 0.296225	test: 0.260073

Epoch: 29
Loss: 0.1397247921439373
ROC train: 0.916050	val: 0.736285	test: 0.705117
PRC train: 0.598864	val: 0.296367	test: 0.260869

Epoch: 30
Loss: 0.1388344550452276
ROC train: 0.917102	val: 0.737981	test: 0.697780
PRC train: 0.599984	val: 0.290753	test: 0.248521

Epoch: 31
Loss: 0.13882864336621498
ROC train: 0.919098	val: 0.744023	test: 0.715639
PRC train: 0.613207	val: 0.301943	test: 0.263209

Epoch: 32
Loss: 0.13982680508319764
ROC train: 0.922620	val: 0.736078	test: 0.706524
PRC train: 0.624754	val: 0.293128	test: 0.263387

Epoch: 33
Loss: 0.13771515713495375Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/tox21/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/split/GraphMVP/tox21/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/tox21/scaff/train_prop=0.7/tox21_scaff_6_26-05_11-13-31  ]
[ Using Seed :  6  ]
[ Using device :  cuda:1  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5661924046150122
ROC train: 0.719065	val: 0.657266	test: 0.626191
PRC train: 0.219271	val: 0.179006	test: 0.175771

Epoch: 2
Loss: 0.3763636200023388
ROC train: 0.763204	val: 0.669717	test: 0.649340
PRC train: 0.255171	val: 0.215771	test: 0.214504

Epoch: 3
Loss: 0.2641889280892118
ROC train: 0.788347	val: 0.690160	test: 0.679810
PRC train: 0.300951	val: 0.244888	test: 0.238197

Epoch: 4
Loss: 0.21277891327524026
ROC train: 0.816015	val: 0.709655	test: 0.706327
PRC train: 0.346850	val: 0.265482	test: 0.263086

Epoch: 5
Loss: 0.1935716615828838
ROC train: 0.825182	val: 0.710516	test: 0.702570
PRC train: 0.357324	val: 0.271814	test: 0.262605

Epoch: 6
Loss: 0.18640119502880148
ROC train: 0.838407	val: 0.699239	test: 0.699539
PRC train: 0.372013	val: 0.272052	test: 0.262099

Epoch: 7
Loss: 0.18265166297811658
ROC train: 0.844403	val: 0.734104	test: 0.727730
PRC train: 0.389531	val: 0.293869	test: 0.281945

Epoch: 8
Loss: 0.18046399406106262
ROC train: 0.854991	val: 0.727341	test: 0.712332
PRC train: 0.415783	val: 0.292630	test: 0.275207

Epoch: 9
Loss: 0.17990767491511025
ROC train: 0.850368	val: 0.740782	test: 0.732809
PRC train: 0.395297	val: 0.304992	test: 0.290309

Epoch: 10
Loss: 0.17558510290720505
ROC train: 0.856477	val: 0.722451	test: 0.720116
PRC train: 0.423869	val: 0.314268	test: 0.278072

Epoch: 11
Loss: 0.17179948538381845
ROC train: 0.869335	val: 0.725304	test: 0.718219
PRC train: 0.465163	val: 0.299177	test: 0.283946

Epoch: 12
Loss: 0.16933721137558336
ROC train: 0.874469	val: 0.724118	test: 0.722530
PRC train: 0.485219	val: 0.313845	test: 0.301774

Epoch: 13
Loss: 0.16945634026344722
ROC train: 0.878777	val: 0.731363	test: 0.723821
PRC train: 0.507082	val: 0.335071	test: 0.296901

Epoch: 14
Loss: 0.16589693941665687
ROC train: 0.878371	val: 0.717561	test: 0.710842
PRC train: 0.499018	val: 0.298746	test: 0.283373

Epoch: 15
Loss: 0.1652054697853271
ROC train: 0.882875	val: 0.726550	test: 0.724237
PRC train: 0.517298	val: 0.320370	test: 0.300280

Epoch: 16
Loss: 0.16604671857488548
ROC train: 0.887692	val: 0.740696	test: 0.725279
PRC train: 0.525959	val: 0.317765	test: 0.295624

Epoch: 17
Loss: 0.16283386166923508
ROC train: 0.884704	val: 0.749849	test: 0.725327
PRC train: 0.525516	val: 0.328902	test: 0.304622

Epoch: 18
Loss: 0.1608010821880645
ROC train: 0.888953	val: 0.752374	test: 0.738685
PRC train: 0.542707	val: 0.326670	test: 0.308054

Epoch: 19
Loss: 0.15990602347842575
ROC train: 0.890980	val: 0.738520	test: 0.726313
PRC train: 0.537417	val: 0.325926	test: 0.304505

Epoch: 20
Loss: 0.1580539432754247
ROC train: 0.893989	val: 0.742151	test: 0.724127
PRC train: 0.543112	val: 0.318521	test: 0.295621

Epoch: 21
Loss: 0.15721340231205072
ROC train: 0.890732	val: 0.744628	test: 0.737332
PRC train: 0.537226	val: 0.346904	test: 0.303385

Epoch: 22
Loss: 0.15517503957141884
ROC train: 0.897801	val: 0.737134	test: 0.726311
PRC train: 0.557426	val: 0.324062	test: 0.298330

Epoch: 23
Loss: 0.15426643910496404
ROC train: 0.901270	val: 0.739382	test: 0.732252
PRC train: 0.579251	val: 0.341003	test: 0.309720

Epoch: 24
Loss: 0.15381146422150882
ROC train: 0.905101	val: 0.757503	test: 0.740591
PRC train: 0.577915	val: 0.342758	test: 0.312638

Epoch: 25
Loss: 0.15084557244041163
ROC train: 0.908057	val: 0.736023	test: 0.715912
PRC train: 0.589311	val: 0.326694	test: 0.303390

Epoch: 26
Loss: 0.15192107398697047
ROC train: 0.908551	val: 0.749648	test: 0.736601
PRC train: 0.584295	val: 0.322581	test: 0.300102

Epoch: 27
Loss: 0.15155420616462717
ROC train: 0.910936	val: 0.736372	test: 0.735088
PRC train: 0.604472	val: 0.344570	test: 0.308167

Epoch: 28
Loss: 0.15048584687521172
ROC train: 0.911419	val: 0.740337	test: 0.735791
PRC train: 0.595846	val: 0.332721	test: 0.315993

Epoch: 29
Loss: 0.14727049049685015
ROC train: 0.912960	val: 0.746982	test: 0.728397
PRC train: 0.606764	val: 0.350549	test: 0.307623

Epoch: 30
Loss: 0.14701777364206348
ROC train: 0.914311	val: 0.745077	test: 0.729111
PRC train: 0.612932	val: 0.347300	test: 0.307434

Epoch: 31
Loss: 0.1467189013120614
ROC train: 0.914861	val: 0.745281	test: 0.732285
PRC train: 0.605742	val: 0.332442	test: 0.318726

Epoch: 32
Loss: 0.14782695389969022
ROC train: 0.917750	val: 0.734908	test: 0.731102
PRC train: 0.617054	val: 0.326745	test: 0.315698

Epoch: 33
Loss: 0.14511479825755652Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/tox21/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/split/GraphMVP/tox21/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/tox21/scaff/train_prop=0.7/tox21_scaff_5_26-05_11-13-31  ]
[ Using Seed :  5  ]
[ Using device :  cuda:1  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5613179352872272
ROC train: 0.712215	val: 0.612718	test: 0.583631
PRC train: 0.206189	val: 0.157732	test: 0.158555

Epoch: 2
Loss: 0.37185690052348774
ROC train: 0.742838	val: 0.625754	test: 0.622088
PRC train: 0.237325	val: 0.175980	test: 0.179335

Epoch: 3
Loss: 0.26235432545247744
ROC train: 0.788838	val: 0.672553	test: 0.666329
PRC train: 0.301524	val: 0.227769	test: 0.225066

Epoch: 4
Loss: 0.21263256596099714
ROC train: 0.811628	val: 0.724029	test: 0.713535
PRC train: 0.351155	val: 0.273777	test: 0.265356

Epoch: 5
Loss: 0.19285506496417854
ROC train: 0.829139	val: 0.733022	test: 0.714157
PRC train: 0.364229	val: 0.289955	test: 0.276674

Epoch: 6
Loss: 0.1858019523957264
ROC train: 0.839740	val: 0.729691	test: 0.718331
PRC train: 0.386544	val: 0.279658	test: 0.278976

Epoch: 7
Loss: 0.182543511177951
ROC train: 0.841069	val: 0.710448	test: 0.700192
PRC train: 0.382291	val: 0.288899	test: 0.267382

Epoch: 8
Loss: 0.1794483325119191
ROC train: 0.851797	val: 0.740480	test: 0.729023
PRC train: 0.402265	val: 0.307516	test: 0.298160

Epoch: 9
Loss: 0.1777361093396623
ROC train: 0.861546	val: 0.721067	test: 0.711016
PRC train: 0.434887	val: 0.275441	test: 0.276244

Epoch: 10
Loss: 0.17423899491003597
ROC train: 0.861552	val: 0.731205	test: 0.723929
PRC train: 0.424062	val: 0.306844	test: 0.280097

Epoch: 11
Loss: 0.1723897174241908
ROC train: 0.870168	val: 0.722248	test: 0.714521
PRC train: 0.465148	val: 0.300422	test: 0.281403

Epoch: 12
Loss: 0.16955391005238307
ROC train: 0.865396	val: 0.702648	test: 0.693666
PRC train: 0.451977	val: 0.282328	test: 0.270672

Epoch: 13
Loss: 0.1702707323700935
ROC train: 0.875270	val: 0.727652	test: 0.721200
PRC train: 0.491730	val: 0.302898	test: 0.291580

Epoch: 14
Loss: 0.1670833424356174
ROC train: 0.880130	val: 0.737913	test: 0.723912
PRC train: 0.499781	val: 0.318708	test: 0.304382

Epoch: 15
Loss: 0.16531344412187907
ROC train: 0.882359	val: 0.744918	test: 0.731478
PRC train: 0.515419	val: 0.316996	test: 0.301493

Epoch: 16
Loss: 0.16207929542735747
ROC train: 0.884838	val: 0.738307	test: 0.718551
PRC train: 0.526245	val: 0.323617	test: 0.296092

Epoch: 17
Loss: 0.16128174868566217
ROC train: 0.883619	val: 0.731378	test: 0.722106
PRC train: 0.537484	val: 0.337000	test: 0.300511

Epoch: 18
Loss: 0.1617775702965097
ROC train: 0.891006	val: 0.727071	test: 0.707328
PRC train: 0.530050	val: 0.313473	test: 0.286508

Epoch: 19
Loss: 0.1587520357559342
ROC train: 0.892726	val: 0.722742	test: 0.714101
PRC train: 0.553160	val: 0.323786	test: 0.292458

Epoch: 20
Loss: 0.15827209860685376
ROC train: 0.892779	val: 0.733343	test: 0.725346
PRC train: 0.552024	val: 0.333045	test: 0.308595

Epoch: 21
Loss: 0.1568362055525197
ROC train: 0.897836	val: 0.732411	test: 0.727067
PRC train: 0.564449	val: 0.336331	test: 0.309935

Epoch: 22
Loss: 0.1563166689860459
ROC train: 0.901680	val: 0.730325	test: 0.721438
PRC train: 0.574432	val: 0.329776	test: 0.299345

Epoch: 23
Loss: 0.15262669473001655
ROC train: 0.903289	val: 0.738564	test: 0.723976
PRC train: 0.577688	val: 0.333741	test: 0.306889

Epoch: 24
Loss: 0.1540071710874228
ROC train: 0.905828	val: 0.738002	test: 0.725091
PRC train: 0.585755	val: 0.330437	test: 0.307192

Epoch: 25
Loss: 0.15383494797426225
ROC train: 0.903718	val: 0.742605	test: 0.734875
PRC train: 0.579662	val: 0.327506	test: 0.314189

Epoch: 26
Loss: 0.14966010491580553
ROC train: 0.909300	val: 0.741138	test: 0.724096
PRC train: 0.595447	val: 0.342781	test: 0.312597

Epoch: 27
Loss: 0.15004853246589797
ROC train: 0.909308	val: 0.735929	test: 0.719332
PRC train: 0.606385	val: 0.342808	test: 0.305394

Epoch: 28
Loss: 0.14998741161945559
ROC train: 0.911413	val: 0.746701	test: 0.732781
PRC train: 0.610252	val: 0.348515	test: 0.318665

Epoch: 29
Loss: 0.14960120031049698
ROC train: 0.912899	val: 0.751750	test: 0.726058
PRC train: 0.602568	val: 0.329179	test: 0.296653

Epoch: 30
Loss: 0.14800691412884673
ROC train: 0.915028	val: 0.747823	test: 0.728734
PRC train: 0.615557	val: 0.337673	test: 0.318236

Epoch: 31
Loss: 0.14799639266282164
ROC train: 0.916928	val: 0.741174	test: 0.729624
PRC train: 0.623501	val: 0.344255	test: 0.316359

Epoch: 32
Loss: 0.14716609418219104
ROC train: 0.916755	val: 0.724515	test: 0.714943
PRC train: 0.612431	val: 0.317510	test: 0.299233

Epoch: 33
Loss: 0.14753369509698877Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/tox21/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/split/GraphMVP/tox21/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/tox21/scaff/train_prop=0.7/tox21_scaff_4_26-05_11-13-31  ]
[ Using Seed :  4  ]
[ Using device :  cuda:1  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5743634040005526
ROC train: 0.677610	val: 0.560120	test: 0.544903
PRC train: 0.175152	val: 0.132739	test: 0.133560

Epoch: 2
Loss: 0.3820398159860013
ROC train: 0.759859	val: 0.653628	test: 0.637250
PRC train: 0.244622	val: 0.190035	test: 0.192539

Epoch: 3
Loss: 0.27063660819302665
ROC train: 0.777946	val: 0.670905	test: 0.673828
PRC train: 0.302989	val: 0.235405	test: 0.235573

Epoch: 4
Loss: 0.215122017530909
ROC train: 0.811778	val: 0.714730	test: 0.704247
PRC train: 0.344678	val: 0.271348	test: 0.263271

Epoch: 5
Loss: 0.1953936973736025
ROC train: 0.821921	val: 0.735216	test: 0.717973
PRC train: 0.364427	val: 0.301900	test: 0.281080

Epoch: 6
Loss: 0.1858544722414811
ROC train: 0.841202	val: 0.728063	test: 0.714923
PRC train: 0.377484	val: 0.262831	test: 0.269359

Epoch: 7
Loss: 0.18375461181643055
ROC train: 0.846483	val: 0.736874	test: 0.723150
PRC train: 0.404914	val: 0.317576	test: 0.289773

Epoch: 8
Loss: 0.18019233089136794
ROC train: 0.844443	val: 0.717002	test: 0.703861
PRC train: 0.403573	val: 0.306628	test: 0.273209

Epoch: 9
Loss: 0.17685014310330385
ROC train: 0.855227	val: 0.745109	test: 0.738313
PRC train: 0.421515	val: 0.328915	test: 0.309067

Epoch: 10
Loss: 0.17396816167094267
ROC train: 0.867950	val: 0.728372	test: 0.718127
PRC train: 0.449014	val: 0.307468	test: 0.290042

Epoch: 11
Loss: 0.17201813058314644
ROC train: 0.871710	val: 0.723715	test: 0.723942
PRC train: 0.472957	val: 0.310937	test: 0.296364

Epoch: 12
Loss: 0.17094457186407455
ROC train: 0.866590	val: 0.716633	test: 0.721489
PRC train: 0.462554	val: 0.314253	test: 0.293826

Epoch: 13
Loss: 0.1702481334400052
ROC train: 0.875260	val: 0.727719	test: 0.723330
PRC train: 0.482167	val: 0.300591	test: 0.295349

Epoch: 14
Loss: 0.1695985985822772
ROC train: 0.879644	val: 0.731085	test: 0.734581
PRC train: 0.501010	val: 0.317979	test: 0.299081

Epoch: 15
Loss: 0.16490602599293255
ROC train: 0.882670	val: 0.740572	test: 0.726642
PRC train: 0.508394	val: 0.314697	test: 0.301791

Epoch: 16
Loss: 0.1632105584261812
ROC train: 0.885330	val: 0.734367	test: 0.709626
PRC train: 0.511593	val: 0.309432	test: 0.295013

Epoch: 17
Loss: 0.1635063827975367
ROC train: 0.887314	val: 0.732270	test: 0.723713
PRC train: 0.523828	val: 0.295625	test: 0.295759

Epoch: 18
Loss: 0.1623342044580574
ROC train: 0.887641	val: 0.745049	test: 0.733955
PRC train: 0.532205	val: 0.323122	test: 0.310618

Epoch: 19
Loss: 0.15951315750539444
ROC train: 0.892670	val: 0.735660	test: 0.719662
PRC train: 0.534498	val: 0.329030	test: 0.300046

Epoch: 20
Loss: 0.15998065215709367
ROC train: 0.888567	val: 0.746502	test: 0.738950
PRC train: 0.544401	val: 0.334223	test: 0.313446

Epoch: 21
Loss: 0.15696890287903212
ROC train: 0.898337	val: 0.739950	test: 0.738467
PRC train: 0.559990	val: 0.332086	test: 0.322965

Epoch: 22
Loss: 0.15713786327246068
ROC train: 0.901497	val: 0.736194	test: 0.727791
PRC train: 0.564726	val: 0.339641	test: 0.309381

Epoch: 23
Loss: 0.1552977524499257
ROC train: 0.902934	val: 0.743354	test: 0.735456
PRC train: 0.576019	val: 0.326802	test: 0.318654

Epoch: 24
Loss: 0.1539669484527035
ROC train: 0.905521	val: 0.744326	test: 0.734221
PRC train: 0.581081	val: 0.334636	test: 0.320757

Epoch: 25
Loss: 0.15346605021921003
ROC train: 0.906770	val: 0.748227	test: 0.732181
PRC train: 0.586094	val: 0.334647	test: 0.316602

Epoch: 26
Loss: 0.152082719989958
ROC train: 0.909191	val: 0.747104	test: 0.732345
PRC train: 0.595130	val: 0.344536	test: 0.318684

Epoch: 27
Loss: 0.15213239612836987
ROC train: 0.909006	val: 0.750430	test: 0.739142
PRC train: 0.600075	val: 0.348790	test: 0.329185

Epoch: 28
Loss: 0.15191597468621296
ROC train: 0.913147	val: 0.737865	test: 0.724233
PRC train: 0.607584	val: 0.338439	test: 0.316464

Epoch: 29
Loss: 0.1490369214228964
ROC train: 0.912399	val: 0.745539	test: 0.733357
PRC train: 0.604354	val: 0.337064	test: 0.322677

Epoch: 30
Loss: 0.14889945255351902
ROC train: 0.914541	val: 0.740140	test: 0.728636
PRC train: 0.616470	val: 0.336616	test: 0.325863

Epoch: 31
Loss: 0.14573815382849611
ROC train: 0.917779	val: 0.745143	test: 0.738734
PRC train: 0.627867	val: 0.347691	test: 0.337834

Epoch: 32
Loss: 0.1462429335836223
ROC train: 0.917479	val: 0.754199	test: 0.735109
PRC train: 0.621483	val: 0.340095	test: 0.325622

Epoch: 33
Loss: 0.14590690868540993Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/tox21/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/split/GraphMVP/tox21/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/tox21/scaff/train_prop=0.8/tox21_scaff_5_26-05_11-13-31  ]
[ Using Seed :  5  ]
[ Using device :  cuda:2  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5431118950207625
ROC train: 0.724967	val: 0.661954	test: 0.616425
PRC train: 0.210444	val: 0.202534	test: 0.187301

Epoch: 2
Loss: 0.3512267229368369
ROC train: 0.740452	val: 0.662018	test: 0.632334
PRC train: 0.245823	val: 0.214743	test: 0.216950

Epoch: 3
Loss: 0.24748752200389962
ROC train: 0.786682	val: 0.724587	test: 0.669927
PRC train: 0.303357	val: 0.267798	test: 0.262154

Epoch: 4
Loss: 0.2106285414720096
ROC train: 0.806485	val: 0.743000	test: 0.693245
PRC train: 0.326312	val: 0.278610	test: 0.274810

Epoch: 5
Loss: 0.19855380812138054
ROC train: 0.821275	val: 0.751881	test: 0.710867
PRC train: 0.375887	val: 0.329747	test: 0.317680

Epoch: 6
Loss: 0.1934979275957903
ROC train: 0.824757	val: 0.752110	test: 0.707716
PRC train: 0.357458	val: 0.304881	test: 0.295211

Epoch: 7
Loss: 0.19152752517271454
ROC train: 0.835266	val: 0.755512	test: 0.720225
PRC train: 0.383399	val: 0.328023	test: 0.325158

Epoch: 8
Loss: 0.1870379112376065
ROC train: 0.842723	val: 0.752147	test: 0.726130
PRC train: 0.402547	val: 0.339644	test: 0.332522

Epoch: 9
Loss: 0.18687048529276556
ROC train: 0.841943	val: 0.743200	test: 0.711662
PRC train: 0.393282	val: 0.306121	test: 0.312689

Epoch: 10
Loss: 0.18373983555083462
ROC train: 0.856635	val: 0.754508	test: 0.716242
PRC train: 0.441504	val: 0.331677	test: 0.323548

Epoch: 11
Loss: 0.17973118369291416
ROC train: 0.858834	val: 0.760478	test: 0.720689
PRC train: 0.443287	val: 0.336497	test: 0.334333

Epoch: 12
Loss: 0.18027444684790495
ROC train: 0.862093	val: 0.764674	test: 0.722746
PRC train: 0.472996	val: 0.347785	test: 0.340371

Epoch: 13
Loss: 0.1786930675495256
ROC train: 0.864466	val: 0.761031	test: 0.724610
PRC train: 0.467035	val: 0.328056	test: 0.324188

Epoch: 14
Loss: 0.17659800262269715
ROC train: 0.866543	val: 0.741501	test: 0.707339
PRC train: 0.478201	val: 0.316455	test: 0.321617

Epoch: 15
Loss: 0.17449195040849255
ROC train: 0.874446	val: 0.754332	test: 0.724670
PRC train: 0.506224	val: 0.342652	test: 0.329286

Epoch: 16
Loss: 0.1727608313575305
ROC train: 0.865553	val: 0.782142	test: 0.731642
PRC train: 0.477081	val: 0.359191	test: 0.342021

Epoch: 17
Loss: 0.17132443680713866
ROC train: 0.879297	val: 0.766262	test: 0.721242
PRC train: 0.511036	val: 0.348618	test: 0.326348

Epoch: 18
Loss: 0.17066199566118317
ROC train: 0.884794	val: 0.765589	test: 0.723786
PRC train: 0.535507	val: 0.342142	test: 0.328572

Epoch: 19
Loss: 0.1690534735350121
ROC train: 0.884988	val: 0.772700	test: 0.720044
PRC train: 0.532030	val: 0.348888	test: 0.328879

Epoch: 20
Loss: 0.16647280776780968
ROC train: 0.888345	val: 0.771529	test: 0.722108
PRC train: 0.545292	val: 0.344648	test: 0.339805

Epoch: 21
Loss: 0.16732165583713854
ROC train: 0.886676	val: 0.780233	test: 0.731298
PRC train: 0.535577	val: 0.361924	test: 0.337387

Epoch: 22
Loss: 0.1662428505420644
ROC train: 0.891866	val: 0.764434	test: 0.715901
PRC train: 0.550248	val: 0.326957	test: 0.310351

Epoch: 23
Loss: 0.16541858829233924
ROC train: 0.892830	val: 0.777917	test: 0.739020
PRC train: 0.551997	val: 0.355164	test: 0.347035

Epoch: 24
Loss: 0.16521357661386865
ROC train: 0.894236	val: 0.783190	test: 0.729106
PRC train: 0.556637	val: 0.373735	test: 0.332382

Epoch: 25
Loss: 0.16205028309798725
ROC train: 0.899886	val: 0.777465	test: 0.730715
PRC train: 0.570128	val: 0.366149	test: 0.340002

Epoch: 26
Loss: 0.16172794278378624
ROC train: 0.900262	val: 0.783079	test: 0.734432
PRC train: 0.579867	val: 0.377721	test: 0.354852

Epoch: 27
Loss: 0.1595642105344653
ROC train: 0.904794	val: 0.775859	test: 0.725169
PRC train: 0.588643	val: 0.367022	test: 0.336541

Epoch: 28
Loss: 0.15910928798059149
ROC train: 0.902636	val: 0.767042	test: 0.740322
PRC train: 0.579137	val: 0.359989	test: 0.350768

Epoch: 29
Loss: 0.159061531431666
ROC train: 0.903885	val: 0.780507	test: 0.743458
PRC train: 0.594677	val: 0.365380	test: 0.350753

Epoch: 30
Loss: 0.15683806451623816
ROC train: 0.907955	val: 0.768397	test: 0.721461
PRC train: 0.595066	val: 0.366810	test: 0.335908

Epoch: 31
Loss: 0.15700874824881444
ROC train: 0.910731	val: 0.795266	test: 0.739137
PRC train: 0.601825	val: 0.372402	test: 0.350297

Epoch: 32
Loss: 0.15497761230165175
ROC train: 0.913744	val: 0.788359	test: 0.737520
PRC train: 0.611285	val: 0.386246	test: 0.353196

Epoch: 33
Loss: 0.1558803507024996Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/tox21/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/split/GraphMVP/tox21/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/tox21/scaff/train_prop=0.8/tox21_scaff_6_26-05_11-13-31  ]
[ Using Seed :  6  ]
[ Using device :  cuda:2  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5489945754558119
ROC train: 0.702801	val: 0.618873	test: 0.577651
PRC train: 0.179398	val: 0.164498	test: 0.163385

Epoch: 2
Loss: 0.35385721571442175
ROC train: 0.750759	val: 0.675429	test: 0.632180
PRC train: 0.246345	val: 0.222046	test: 0.217921

Epoch: 3
Loss: 0.24959787023353003
ROC train: 0.783684	val: 0.728691	test: 0.674281
PRC train: 0.309693	val: 0.276218	test: 0.275400

Epoch: 4
Loss: 0.2131851403644357
ROC train: 0.805435	val: 0.735064	test: 0.698769
PRC train: 0.330205	val: 0.283798	test: 0.281668

Epoch: 5
Loss: 0.19869749919170115
ROC train: 0.823396	val: 0.741173	test: 0.704697
PRC train: 0.350036	val: 0.304361	test: 0.299124

Epoch: 6
Loss: 0.19470010368757273
ROC train: 0.822657	val: 0.749814	test: 0.722521
PRC train: 0.367529	val: 0.329743	test: 0.314840

Epoch: 7
Loss: 0.19143516216694473
ROC train: 0.834707	val: 0.754118	test: 0.717821
PRC train: 0.385850	val: 0.331964	test: 0.323712

Epoch: 8
Loss: 0.18958059241775593
ROC train: 0.840146	val: 0.754261	test: 0.729383
PRC train: 0.398685	val: 0.345808	test: 0.327617

Epoch: 9
Loss: 0.18482727671485802
ROC train: 0.855221	val: 0.753928	test: 0.732987
PRC train: 0.428602	val: 0.334672	test: 0.337718

Epoch: 10
Loss: 0.18296551760139035
ROC train: 0.856710	val: 0.749399	test: 0.713456
PRC train: 0.442354	val: 0.336388	test: 0.322073

Epoch: 11
Loss: 0.1836350400445616
ROC train: 0.858235	val: 0.743250	test: 0.723920
PRC train: 0.447712	val: 0.322118	test: 0.330917

Epoch: 12
Loss: 0.1794316286209118
ROC train: 0.860426	val: 0.751623	test: 0.728301
PRC train: 0.446699	val: 0.320715	test: 0.324083

Epoch: 13
Loss: 0.17675385380927344
ROC train: 0.862990	val: 0.755390	test: 0.724581
PRC train: 0.480519	val: 0.340168	test: 0.337664

Epoch: 14
Loss: 0.17835418234205208
ROC train: 0.869637	val: 0.757098	test: 0.713045
PRC train: 0.488510	val: 0.323862	test: 0.327113

Epoch: 15
Loss: 0.17495798699544107
ROC train: 0.874112	val: 0.759254	test: 0.723709
PRC train: 0.499887	val: 0.333082	test: 0.321045

Epoch: 16
Loss: 0.17253390076256536
ROC train: 0.876885	val: 0.758635	test: 0.728691
PRC train: 0.506998	val: 0.330682	test: 0.344870

Epoch: 17
Loss: 0.17187885564186486
ROC train: 0.881421	val: 0.766236	test: 0.724355
PRC train: 0.522932	val: 0.347089	test: 0.341847

Epoch: 18
Loss: 0.17002540999715965
ROC train: 0.885916	val: 0.769002	test: 0.731858
PRC train: 0.530405	val: 0.343214	test: 0.347454

Epoch: 19
Loss: 0.16913058296245287
ROC train: 0.883615	val: 0.763765	test: 0.735991
PRC train: 0.525997	val: 0.339572	test: 0.355499

Epoch: 20
Loss: 0.16811246374014083
ROC train: 0.891482	val: 0.765905	test: 0.733808
PRC train: 0.550823	val: 0.356055	test: 0.349384

Epoch: 21
Loss: 0.16847224854210832
ROC train: 0.890130	val: 0.768676	test: 0.733346
PRC train: 0.540661	val: 0.347063	test: 0.353284

Epoch: 22
Loss: 0.16560028647068667
ROC train: 0.893844	val: 0.761307	test: 0.736972
PRC train: 0.554144	val: 0.348188	test: 0.349254

Epoch: 23
Loss: 0.16474169621221782
ROC train: 0.896363	val: 0.761963	test: 0.730891
PRC train: 0.561853	val: 0.344405	test: 0.346298

Epoch: 24
Loss: 0.16379002653694985
ROC train: 0.892524	val: 0.772922	test: 0.733038
PRC train: 0.550431	val: 0.355971	test: 0.345040

Epoch: 25
Loss: 0.1624077179440364
ROC train: 0.899727	val: 0.766220	test: 0.732981
PRC train: 0.566133	val: 0.343073	test: 0.352173

Epoch: 26
Loss: 0.16071935354545072
ROC train: 0.897185	val: 0.762099	test: 0.732457
PRC train: 0.568070	val: 0.360819	test: 0.357015

Epoch: 27
Loss: 0.16227578541957954
ROC train: 0.902792	val: 0.779028	test: 0.738738
PRC train: 0.583143	val: 0.371832	test: 0.361923

Epoch: 28
Loss: 0.16056344092992744
ROC train: 0.905382	val: 0.766312	test: 0.734276
PRC train: 0.585765	val: 0.347513	test: 0.350404

Epoch: 29
Loss: 0.15968586345154173
ROC train: 0.903088	val: 0.766817	test: 0.723967
PRC train: 0.573594	val: 0.339485	test: 0.339804

Epoch: 30
Loss: 0.157182372909307
ROC train: 0.906989	val: 0.779037	test: 0.734399
PRC train: 0.595609	val: 0.367684	test: 0.353197

Epoch: 31
Loss: 0.15896047504080682
ROC train: 0.909459	val: 0.774231	test: 0.739291
PRC train: 0.598641	val: 0.361679	test: 0.360990

Epoch: 32
Loss: 0.15582691954359296
ROC train: 0.912634	val: 0.775905	test: 0.736496
PRC train: 0.610376	val: 0.368012	test: 0.356074

Epoch: 33
Loss: 0.15427043823574915Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/tox21/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/split/GraphMVP/tox21/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/tox21/scaff/train_prop=0.8/tox21_scaff_4_26-05_11-13-31  ]
[ Using Seed :  4  ]
[ Using device :  cuda:2  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.558589404331482
ROC train: 0.694975	val: 0.610136	test: 0.568402
PRC train: 0.161532	val: 0.145650	test: 0.144705

Epoch: 2
Loss: 0.35546434296747037
ROC train: 0.757997	val: 0.690858	test: 0.657176
PRC train: 0.257279	val: 0.240047	test: 0.235595

Epoch: 3
Loss: 0.25203665533770225
ROC train: 0.795082	val: 0.734349	test: 0.692346
PRC train: 0.325797	val: 0.290423	test: 0.280006

Epoch: 4
Loss: 0.2112080552026208
ROC train: 0.801855	val: 0.745191	test: 0.704393
PRC train: 0.320708	val: 0.298529	test: 0.283101

Epoch: 5
Loss: 0.19900040005538386
ROC train: 0.818324	val: 0.735230	test: 0.705008
PRC train: 0.358331	val: 0.295208	test: 0.311821

Epoch: 6
Loss: 0.19325836324275486
ROC train: 0.835445	val: 0.754311	test: 0.724749
PRC train: 0.387574	val: 0.315865	test: 0.315225

Epoch: 7
Loss: 0.19088360091220655
ROC train: 0.840928	val: 0.759983	test: 0.734757
PRC train: 0.403304	val: 0.342878	test: 0.332024

Epoch: 8
Loss: 0.1866012797398844
ROC train: 0.839595	val: 0.757210	test: 0.731282
PRC train: 0.391326	val: 0.349033	test: 0.332835

Epoch: 9
Loss: 0.18603756395941148
ROC train: 0.844214	val: 0.741497	test: 0.703553
PRC train: 0.413203	val: 0.306039	test: 0.316352

Epoch: 10
Loss: 0.18317330304962115
ROC train: 0.859079	val: 0.771826	test: 0.734649
PRC train: 0.436482	val: 0.350218	test: 0.344856

Epoch: 11
Loss: 0.17960288502796387
ROC train: 0.866799	val: 0.760936	test: 0.722564
PRC train: 0.462371	val: 0.338186	test: 0.344500

Epoch: 12
Loss: 0.17817232314621204
ROC train: 0.869153	val: 0.771852	test: 0.732889
PRC train: 0.477703	val: 0.346330	test: 0.347635

Epoch: 13
Loss: 0.17577434641349185
ROC train: 0.867933	val: 0.769265	test: 0.730332
PRC train: 0.481332	val: 0.348567	test: 0.342661

Epoch: 14
Loss: 0.17535649130002032
ROC train: 0.866432	val: 0.767602	test: 0.724417
PRC train: 0.473051	val: 0.346938	test: 0.343541

Epoch: 15
Loss: 0.17302766217796123
ROC train: 0.870631	val: 0.761051	test: 0.720811
PRC train: 0.493914	val: 0.314390	test: 0.326309

Epoch: 16
Loss: 0.17168787890196838
ROC train: 0.877163	val: 0.772252	test: 0.721967
PRC train: 0.509116	val: 0.347621	test: 0.351832

Epoch: 17
Loss: 0.17240061653491331
ROC train: 0.881285	val: 0.771925	test: 0.727587
PRC train: 0.521579	val: 0.350944	test: 0.352672

Epoch: 18
Loss: 0.16745261819471494
ROC train: 0.884975	val: 0.779049	test: 0.730277
PRC train: 0.533973	val: 0.361171	test: 0.362601

Epoch: 19
Loss: 0.16780773689888073
ROC train: 0.883991	val: 0.770447	test: 0.735357
PRC train: 0.530157	val: 0.342085	test: 0.348068

Epoch: 20
Loss: 0.16685104511947327
ROC train: 0.887934	val: 0.767667	test: 0.722913
PRC train: 0.532480	val: 0.341675	test: 0.336754

Epoch: 21
Loss: 0.16543987902474555
ROC train: 0.890196	val: 0.774068	test: 0.749825
PRC train: 0.551730	val: 0.359628	test: 0.358209

Epoch: 22
Loss: 0.16398750249551342
ROC train: 0.893293	val: 0.780243	test: 0.732503
PRC train: 0.555364	val: 0.351580	test: 0.345678

Epoch: 23
Loss: 0.16325017447221396
ROC train: 0.893690	val: 0.769156	test: 0.728774
PRC train: 0.551803	val: 0.357458	test: 0.331217

Epoch: 24
Loss: 0.16155066722766787
ROC train: 0.898604	val: 0.769801	test: 0.736446
PRC train: 0.568388	val: 0.357279	test: 0.353290

Epoch: 25
Loss: 0.1612735862701803
ROC train: 0.898416	val: 0.776448	test: 0.747174
PRC train: 0.578235	val: 0.360645	test: 0.358442

Epoch: 26
Loss: 0.16205035717319194
ROC train: 0.899615	val: 0.773378	test: 0.737151
PRC train: 0.566831	val: 0.358951	test: 0.344785

Epoch: 27
Loss: 0.15975451908908822
ROC train: 0.901945	val: 0.782743	test: 0.744355
PRC train: 0.577179	val: 0.376706	test: 0.355272

Epoch: 28
Loss: 0.15919337345350904
ROC train: 0.902590	val: 0.782826	test: 0.736356
PRC train: 0.585477	val: 0.361631	test: 0.349386

Epoch: 29
Loss: 0.15848394393312687
ROC train: 0.906162	val: 0.769335	test: 0.741906
PRC train: 0.588176	val: 0.357386	test: 0.345031

Epoch: 30
Loss: 0.1559330337231842
ROC train: 0.910520	val: 0.771168	test: 0.730020
PRC train: 0.602843	val: 0.362541	test: 0.341120

Epoch: 31
Loss: 0.1552035802864217
ROC train: 0.911395	val: 0.769497	test: 0.737822
PRC train: 0.604114	val: 0.358895	test: 0.338215

Epoch: 32
Loss: 0.15632581229270642
ROC train: 0.910410	val: 0.783514	test: 0.746010
PRC train: 0.607983	val: 0.374806	test: 0.360292

Epoch: 33
Loss: 0.1552780446927787
ROC train: 0.924034	val: 0.746369	test: 0.718978
PRC train: 0.636938	val: 0.309338	test: 0.274505

Epoch: 34
Loss: 0.13524405584577748
ROC train: 0.925263	val: 0.749507	test: 0.724097
PRC train: 0.637850	val: 0.308009	test: 0.275097

Epoch: 35
Loss: 0.1336964888571043
ROC train: 0.925778	val: 0.738292	test: 0.716316
PRC train: 0.637738	val: 0.312160	test: 0.271723

Epoch: 36
Loss: 0.13189900579690625
ROC train: 0.927481	val: 0.732027	test: 0.707245
PRC train: 0.642790	val: 0.307002	test: 0.270984

Epoch: 37
Loss: 0.13104892752943073
ROC train: 0.928019	val: 0.741101	test: 0.715679
PRC train: 0.647362	val: 0.314164	test: 0.276373

Epoch: 38
Loss: 0.13212462161673721
ROC train: 0.928978	val: 0.723142	test: 0.698365
PRC train: 0.650595	val: 0.280487	test: 0.258130

Epoch: 39
Loss: 0.13076323706812812
ROC train: 0.931124	val: 0.741319	test: 0.721275
PRC train: 0.655887	val: 0.284253	test: 0.258582

Epoch: 40
Loss: 0.12965868203815764
ROC train: 0.932246	val: 0.737356	test: 0.700468
PRC train: 0.655758	val: 0.290582	test: 0.261038

Epoch: 41
Loss: 0.1287997540577013
ROC train: 0.932916	val: 0.738128	test: 0.719102
PRC train: 0.664128	val: 0.302700	test: 0.273750

Epoch: 42
Loss: 0.13127185467737992
ROC train: 0.936558	val: 0.739903	test: 0.710975
PRC train: 0.675311	val: 0.313714	test: 0.268912

Epoch: 43
Loss: 0.1303531395349838
ROC train: 0.935250	val: 0.741101	test: 0.721024
PRC train: 0.672979	val: 0.311754	test: 0.275322

Epoch: 44
Loss: 0.12752847113693236
ROC train: 0.938514	val: 0.751106	test: 0.718434
PRC train: 0.685635	val: 0.315497	test: 0.279155

Epoch: 45
Loss: 0.12726471362190947
ROC train: 0.936618	val: 0.732355	test: 0.707882
PRC train: 0.680113	val: 0.306513	test: 0.274354

Epoch: 46
Loss: 0.12691865454814094
ROC train: 0.938771	val: 0.752494	test: 0.724582
PRC train: 0.687651	val: 0.314637	test: 0.286001

Epoch: 47
Loss: 0.12476076087359377
ROC train: 0.941564	val: 0.725153	test: 0.697296
PRC train: 0.690853	val: 0.288645	test: 0.256754

Epoch: 48
Loss: 0.1248774711685729
ROC train: 0.938231	val: 0.750791	test: 0.727870
PRC train: 0.680727	val: 0.303441	test: 0.270881

Epoch: 49
Loss: 0.12210247153086345
ROC train: 0.940755	val: 0.734647	test: 0.719671
PRC train: 0.688324	val: 0.308304	test: 0.280962

Epoch: 50
Loss: 0.1257473878110776
ROC train: 0.941219	val: 0.743192	test: 0.716771
PRC train: 0.695004	val: 0.294739	test: 0.266512

Epoch: 51
Loss: 0.1235610317972241
ROC train: 0.944448	val: 0.736138	test: 0.703121
PRC train: 0.699800	val: 0.310023	test: 0.268458

Epoch: 52
Loss: 0.122387597897364
ROC train: 0.945150	val: 0.735027	test: 0.705277
PRC train: 0.701902	val: 0.293400	test: 0.266616

Epoch: 53
Loss: 0.12190334208617985
ROC train: 0.946770	val: 0.735992	test: 0.706243
PRC train: 0.708730	val: 0.287459	test: 0.262109

Epoch: 54
Loss: 0.12021344863345884
ROC train: 0.946690	val: 0.732426	test: 0.703933
PRC train: 0.713647	val: 0.285163	test: 0.256877

Epoch: 55
Loss: 0.11922799637952716
ROC train: 0.949735	val: 0.738756	test: 0.714217
PRC train: 0.724375	val: 0.298215	test: 0.262879

Epoch: 56
Loss: 0.11856104313800084
ROC train: 0.949789	val: 0.738204	test: 0.717731
PRC train: 0.721237	val: 0.280207	test: 0.258868

Epoch: 57
Loss: 0.11904990685362471
ROC train: 0.948769	val: 0.719527	test: 0.700725
PRC train: 0.714902	val: 0.304511	test: 0.265113

Epoch: 58
Loss: 0.11769961744273054
ROC train: 0.951200	val: 0.737537	test: 0.710325
PRC train: 0.730708	val: 0.297125	test: 0.266420

Epoch: 59
Loss: 0.11556138791420588
ROC train: 0.952793	val: 0.725325	test: 0.691276
PRC train: 0.727272	val: 0.273368	test: 0.238395

Epoch: 60
Loss: 0.11640097251195965
ROC train: 0.953135	val: 0.738282	test: 0.712926
PRC train: 0.736018	val: 0.296514	test: 0.269054

Epoch: 61
Loss: 0.11556623481509458
ROC train: 0.951385	val: 0.738776	test: 0.712366
PRC train: 0.729608	val: 0.297809	test: 0.267075

Epoch: 62
Loss: 0.11419596711773224
ROC train: 0.953932	val: 0.723788	test: 0.701506
PRC train: 0.737660	val: 0.285972	test: 0.260205

Epoch: 63
Loss: 0.116028320290997
ROC train: 0.955982	val: 0.732793	test: 0.708604
PRC train: 0.746765	val: 0.278108	test: 0.258919

Epoch: 64
Loss: 0.11590939330250037
ROC train: 0.953852	val: 0.721704	test: 0.698021
PRC train: 0.741522	val: 0.291267	test: 0.268987

Epoch: 65
Loss: 0.11398186182205115
ROC train: 0.955549	val: 0.734749	test: 0.709978
PRC train: 0.743817	val: 0.299410	test: 0.276889

Epoch: 66
Loss: 0.11387210707792009
ROC train: 0.957140	val: 0.735737	test: 0.710741
PRC train: 0.750720	val: 0.293581	test: 0.267364

Epoch: 67
Loss: 0.11189776913941867
ROC train: 0.956578	val: 0.731486	test: 0.710769
PRC train: 0.748375	val: 0.299881	test: 0.267312

Epoch: 68
Loss: 0.11373480263657262
ROC train: 0.960120	val: 0.728044	test: 0.703351
PRC train: 0.759168	val: 0.306638	test: 0.273198

Epoch: 69
Loss: 0.11327355784239976
ROC train: 0.958277	val: 0.729788	test: 0.709826
PRC train: 0.756621	val: 0.301160	test: 0.263730

Epoch: 70
Loss: 0.11245023094307582
ROC train: 0.955982	val: 0.723685	test: 0.712663
PRC train: 0.742375	val: 0.286120	test: 0.258620

Epoch: 71
Loss: 0.11260063673315297
ROC train: 0.958767	val: 0.738914	test: 0.721513
PRC train: 0.760589	val: 0.298261	test: 0.277677

Epoch: 72
Loss: 0.11089536726822402
ROC train: 0.958709	val: 0.725934	test: 0.706796
PRC train: 0.756176	val: 0.292343	test: 0.268185

Epoch: 73
Loss: 0.11312851615002843
ROC train: 0.961614	val: 0.732828	test: 0.709879
PRC train: 0.769649	val: 0.290281	test: 0.259196

Epoch: 74
Loss: 0.11161295893385295
ROC train: 0.962798	val: 0.728653	test: 0.709371
PRC train: 0.771920	val: 0.291691	test: 0.262094

Epoch: 75
Loss: 0.10919216110826288
ROC train: 0.963641	val: 0.727358	test: 0.712474
PRC train: 0.775254	val: 0.286311	test: 0.258823

Epoch: 76
Loss: 0.1101791027328523
ROC train: 0.962249	val: 0.738004	test: 0.718143
PRC train: 0.776581	val: 0.289543	test: 0.263325

Epoch: 77
Loss: 0.10985236785381967
ROC train: 0.964556	val: 0.721991	test: 0.700246
PRC train: 0.781444	val: 0.290068	test: 0.256067

Epoch: 78
Loss: 0.10894705536708937
ROC train: 0.963425	val: 0.733438	test: 0.712393
PRC train: 0.782700	val: 0.294772	test: 0.268496

Epoch: 79
Loss: 0.10642984477401028
ROC train: 0.966322	val: 0.727364	test: 0.712747
PRC train: 0.787830	val: 0.286567	test: 0.261597

Epoch: 80
Loss: 0.10517511197293192
ROC train: 0.962357	val: 0.711058	test: 0.689584
PRC train: 0.761316	val: 0.264500	test: 0.248668

Epoch: 81
Loss: 0.10603374048160064
ROC train: 0.966156	val: 0.728746	test: 0.712210
PRC train: 0.792257	val: 0.296017	test: 0.266655

Epoch: 82
Loss: 0.10473466652712726
ROC train: 0.966403	val: 0.724822	test: 0.707887
PRC train: 0.793352	val: 0.282699	test: 0.252587

Epoch: 83
Loss: 0.10448709877777607
ROC train: 0.968163	val: 0.726950	test: 0.709753
PRC train: 0.800483	val: 0.277842	test: 0.254411

Epoch: 84
Loss: 0.10386707798482553
ROC train: 0.966969	val: 0.725825	test: 0.710945
PRC train: 0.794162	val: 0.287982	test: 0.262804

Epoch: 85
Loss: 0.10341627157923766
ROC train: 0.968400	val: 0.722549	test: 0.711127
PRC train: 0.799416	val: 0.280422	test: 0.258249

Epoch: 86
Loss: 0.1055732637811187
ROC train: 0.967757	val: 0.727716	test: 0.715361
PRC train: 0.797886	val: 0.290376	test: 0.267295

Epoch: 87
Loss: 0.10356864709296619
ROC train: 0.967757	val: 0.728733	test: 0.713851
PRC train: 0.792487	val: 0.288777	test: 0.265043

Epoch: 88
Loss: 0.10193615167459724
ROC train: 0.968913	val: 0.731472	test: 0.711478
PRC train: 0.799587	val: 0.296529	test: 0.265374

Epoch: 89
Loss: 0.10338194028069989
ROC train: 0.969179	val: 0.716676	test: 0.696995
PRC train: 0.807395	val: 0.269434	test: 0.243600

Epoch: 90
Loss: 0.1017641336532971
ROC train: 0.971268	val: 0.728136	test: 0.701415
PRC train: 0.811587	val: 0.272536	test: 0.246216

Epoch: 91
Loss: 0.10377958528786667
ROC train: 0.971732	val: 0.736274	test: 0.714443
PRC train: 0.816074	val: 0.311960	test: 0.277909

Epoch: 92
Loss: 0.09947486652811262
ROC train: 0.971910	val: 0.722088	test: 0.706653
PRC train: 0.820917	val: 0.284610	test: 0.256255

Epoch: 93
Loss: 0.09985817047335423
ROC train: 0.971949	val: 0.714622	test: 0.690456
PRC train: 0.812369	val: 0.269968	test: 0.242116

Epoch: 94
Loss: 0.10202148990010824
ROC train: 0.918189	val: 0.741329	test: 0.726283
PRC train: 0.622760	val: 0.303638	test: 0.274216

Epoch: 34
Loss: 0.13648540401315276
ROC train: 0.920788	val: 0.752452	test: 0.723007
PRC train: 0.629912	val: 0.322503	test: 0.283123

Epoch: 35
Loss: 0.13400551634696645
ROC train: 0.925120	val: 0.739855	test: 0.720144
PRC train: 0.640853	val: 0.317801	test: 0.284967

Epoch: 36
Loss: 0.13257197527226455
ROC train: 0.922583	val: 0.735894	test: 0.718456
PRC train: 0.636666	val: 0.319480	test: 0.287525

Epoch: 37
Loss: 0.13356815031573838
ROC train: 0.925960	val: 0.744082	test: 0.724047
PRC train: 0.647870	val: 0.315991	test: 0.283383

Epoch: 38
Loss: 0.13147626527017783
ROC train: 0.928762	val: 0.747142	test: 0.724352
PRC train: 0.653653	val: 0.319960	test: 0.285911

Epoch: 39
Loss: 0.1329207277176656
ROC train: 0.928352	val: 0.743490	test: 0.714584
PRC train: 0.653782	val: 0.327238	test: 0.284663

Epoch: 40
Loss: 0.13203536692774173
ROC train: 0.929995	val: 0.744294	test: 0.718163
PRC train: 0.655981	val: 0.326253	test: 0.281020

Epoch: 41
Loss: 0.1311065895164962
ROC train: 0.927265	val: 0.741495	test: 0.717012
PRC train: 0.650297	val: 0.313846	test: 0.273381

Epoch: 42
Loss: 0.1275621593672502
ROC train: 0.932556	val: 0.743337	test: 0.718340
PRC train: 0.662903	val: 0.320502	test: 0.285311

Epoch: 43
Loss: 0.1293051469123391
ROC train: 0.934021	val: 0.741393	test: 0.726174
PRC train: 0.663821	val: 0.325732	test: 0.297101

Epoch: 44
Loss: 0.12759226464830753
ROC train: 0.934945	val: 0.745869	test: 0.727034
PRC train: 0.668046	val: 0.327940	test: 0.289544

Epoch: 45
Loss: 0.12798913201757492
ROC train: 0.934302	val: 0.738880	test: 0.719438
PRC train: 0.670597	val: 0.321699	test: 0.291482

Epoch: 46
Loss: 0.128485978708942
ROC train: 0.937091	val: 0.737979	test: 0.725787
PRC train: 0.673717	val: 0.318335	test: 0.284703

Epoch: 47
Loss: 0.12693820065956246
ROC train: 0.937592	val: 0.743780	test: 0.728762
PRC train: 0.679843	val: 0.315451	test: 0.288323

Epoch: 48
Loss: 0.1246554102808945
ROC train: 0.939221	val: 0.736222	test: 0.714280
PRC train: 0.687645	val: 0.317113	test: 0.285668

Epoch: 49
Loss: 0.1257552607479963
ROC train: 0.939762	val: 0.737875	test: 0.719706
PRC train: 0.691628	val: 0.318059	test: 0.288348

Epoch: 50
Loss: 0.12377448068974521
ROC train: 0.937332	val: 0.748536	test: 0.735035
PRC train: 0.677215	val: 0.323573	test: 0.294069

Epoch: 51
Loss: 0.12459287443692064
ROC train: 0.940801	val: 0.739007	test: 0.719688
PRC train: 0.691126	val: 0.327356	test: 0.289885

Epoch: 52
Loss: 0.12252130266888205
ROC train: 0.940885	val: 0.737246	test: 0.726918
PRC train: 0.691277	val: 0.314275	test: 0.295832

Epoch: 53
Loss: 0.12294373185471369
ROC train: 0.945347	val: 0.736838	test: 0.715284
PRC train: 0.705547	val: 0.324949	test: 0.294749

Epoch: 54
Loss: 0.12436857288248761
ROC train: 0.945600	val: 0.737459	test: 0.719571
PRC train: 0.710453	val: 0.328497	test: 0.288236

Epoch: 55
Loss: 0.12194790495947781
ROC train: 0.944611	val: 0.735018	test: 0.717925
PRC train: 0.706963	val: 0.320756	test: 0.284777

Epoch: 56
Loss: 0.1213813324361861
ROC train: 0.947426	val: 0.739363	test: 0.724264
PRC train: 0.713427	val: 0.331178	test: 0.297804

Epoch: 57
Loss: 0.11923584249003896
ROC train: 0.946325	val: 0.739574	test: 0.715466
PRC train: 0.713569	val: 0.322718	test: 0.287819

Epoch: 58
Loss: 0.11743958684221704
ROC train: 0.948122	val: 0.739003	test: 0.719504
PRC train: 0.720448	val: 0.320752	test: 0.289352

Epoch: 59
Loss: 0.11681099101250512
ROC train: 0.948775	val: 0.727184	test: 0.705497
PRC train: 0.716554	val: 0.314314	test: 0.285662

Epoch: 60
Loss: 0.11826098845228974
ROC train: 0.948976	val: 0.746903	test: 0.722850
PRC train: 0.717409	val: 0.319805	test: 0.278113

Epoch: 61
Loss: 0.11828775572187529
ROC train: 0.949988	val: 0.743207	test: 0.724381
PRC train: 0.724340	val: 0.324028	test: 0.294382

Epoch: 62
Loss: 0.12020928215427318
ROC train: 0.950352	val: 0.724784	test: 0.703162
PRC train: 0.716811	val: 0.303461	test: 0.277038

Epoch: 63
Loss: 0.11778590485122054
ROC train: 0.953193	val: 0.736837	test: 0.715374
PRC train: 0.728340	val: 0.321198	test: 0.287195

Epoch: 64
Loss: 0.11484718140291021
ROC train: 0.954012	val: 0.724653	test: 0.710436
PRC train: 0.734617	val: 0.308021	test: 0.278418

Epoch: 65
Loss: 0.11653142275525202
ROC train: 0.955359	val: 0.729158	test: 0.715977
PRC train: 0.738079	val: 0.308412	test: 0.277456

Epoch: 66
Loss: 0.11498711310712557
ROC train: 0.952985	val: 0.728194	test: 0.705664
PRC train: 0.731303	val: 0.298442	test: 0.264572

Epoch: 67
Loss: 0.11336493353156611
ROC train: 0.949991	val: 0.736735	test: 0.722186
PRC train: 0.717255	val: 0.317146	test: 0.287102

Epoch: 68
Loss: 0.11473134748037335
ROC train: 0.955718	val: 0.733412	test: 0.719919
PRC train: 0.741947	val: 0.306223	test: 0.281034

Epoch: 69
Loss: 0.11419586934229915
ROC train: 0.957793	val: 0.718696	test: 0.698573
PRC train: 0.749796	val: 0.286205	test: 0.259018

Epoch: 70
Loss: 0.11371278040206906
ROC train: 0.955815	val: 0.724297	test: 0.714817
PRC train: 0.741046	val: 0.307541	test: 0.278718

Epoch: 71
Loss: 0.11310046735751894
ROC train: 0.959058	val: 0.734014	test: 0.720456
PRC train: 0.759450	val: 0.307336	test: 0.286220

Epoch: 72
Loss: 0.11212529036692975
ROC train: 0.958936	val: 0.727900	test: 0.709646
PRC train: 0.753715	val: 0.315228	test: 0.277232

Epoch: 73
Loss: 0.11186932771023803
ROC train: 0.960060	val: 0.728720	test: 0.709846
PRC train: 0.761572	val: 0.301541	test: 0.272581

Epoch: 74
Loss: 0.11009560256794829
ROC train: 0.960514	val: 0.732367	test: 0.717264
PRC train: 0.765912	val: 0.308114	test: 0.286612

Epoch: 75
Loss: 0.11008125706921552
ROC train: 0.961720	val: 0.723899	test: 0.708130
PRC train: 0.768398	val: 0.311040	test: 0.282056

Epoch: 76
Loss: 0.10981577518491278
ROC train: 0.962283	val: 0.732873	test: 0.716170
PRC train: 0.770598	val: 0.305501	test: 0.283064

Epoch: 77
Loss: 0.10922052017027058
ROC train: 0.961497	val: 0.726919	test: 0.704704
PRC train: 0.767312	val: 0.288496	test: 0.266646

Epoch: 78
Loss: 0.10883574022754017
ROC train: 0.965670	val: 0.726435	test: 0.708751
PRC train: 0.782197	val: 0.297893	test: 0.273654

Epoch: 79
Loss: 0.11066492536180549
ROC train: 0.963610	val: 0.735865	test: 0.718906
PRC train: 0.777862	val: 0.313098	test: 0.284726

Epoch: 80
Loss: 0.11079605191881337
ROC train: 0.963742	val: 0.721563	test: 0.706087
PRC train: 0.771994	val: 0.307148	test: 0.282861

Epoch: 81
Loss: 0.11138360423192727
ROC train: 0.964258	val: 0.726183	test: 0.703975
PRC train: 0.775315	val: 0.299883	test: 0.279887

Epoch: 82
Loss: 0.1081055236277735
ROC train: 0.964440	val: 0.730946	test: 0.709165
PRC train: 0.781137	val: 0.316511	test: 0.278394

Epoch: 83
Loss: 0.107287169839611
ROC train: 0.966505	val: 0.710641	test: 0.704535
PRC train: 0.783243	val: 0.282920	test: 0.257256

Epoch: 84
Loss: 0.10871019010998016
ROC train: 0.966589	val: 0.724028	test: 0.709307
PRC train: 0.790521	val: 0.292972	test: 0.271998

Epoch: 85
Loss: 0.10486487972038004
ROC train: 0.965229	val: 0.734182	test: 0.714121
PRC train: 0.781385	val: 0.301868	test: 0.267976

Epoch: 86
Loss: 0.1046437134335736
ROC train: 0.968451	val: 0.727404	test: 0.709962
PRC train: 0.796589	val: 0.303138	test: 0.278073

Epoch: 87
Loss: 0.1030590964753105
ROC train: 0.969193	val: 0.730821	test: 0.715200
PRC train: 0.802138	val: 0.294692	test: 0.272831

Epoch: 88
Loss: 0.10453987441129693
ROC train: 0.969324	val: 0.733285	test: 0.712526
PRC train: 0.803244	val: 0.306469	test: 0.284995

Epoch: 89
Loss: 0.10581725078455625
ROC train: 0.970926	val: 0.717874	test: 0.705996
PRC train: 0.805872	val: 0.281123	test: 0.269133

Epoch: 90
Loss: 0.10419072902629735
ROC train: 0.970620	val: 0.730284	test: 0.711762
PRC train: 0.808687	val: 0.304272	test: 0.277049

Epoch: 91
Loss: 0.10372180770988332
ROC train: 0.971674	val: 0.720941	test: 0.708942
PRC train: 0.815102	val: 0.300143	test: 0.278316

Epoch: 92
Loss: 0.10179128470739461
ROC train: 0.972194	val: 0.723503	test: 0.707287
PRC train: 0.817730	val: 0.292897	test: 0.272171

Epoch: 93
Loss: 0.10310812145723731
ROC train: 0.970558	val: 0.715803	test: 0.707004
PRC train: 0.807653	val: 0.283206	test: 0.273205

Epoch: 94
Loss: 0.10319648003692815
ROC train: 0.923835	val: 0.724492	test: 0.703629
PRC train: 0.627643	val: 0.303725	test: 0.266931

Epoch: 34
Loss: 0.133296465130462
ROC train: 0.922948	val: 0.729269	test: 0.700623
PRC train: 0.623261	val: 0.280086	test: 0.262461

Epoch: 35
Loss: 0.13290835352985636
ROC train: 0.926960	val: 0.743575	test: 0.714413
PRC train: 0.639615	val: 0.301954	test: 0.266776

Epoch: 36
Loss: 0.13277708032360364
ROC train: 0.927695	val: 0.743468	test: 0.713332
PRC train: 0.648096	val: 0.319625	test: 0.272174

Epoch: 37
Loss: 0.13110629962447784
ROC train: 0.929424	val: 0.736153	test: 0.709542
PRC train: 0.647807	val: 0.301811	test: 0.263958

Epoch: 38
Loss: 0.1321587721537314
ROC train: 0.928963	val: 0.729165	test: 0.701974
PRC train: 0.648692	val: 0.294704	test: 0.260667

Epoch: 39
Loss: 0.1296142412335935
ROC train: 0.932489	val: 0.734278	test: 0.711219
PRC train: 0.659433	val: 0.312950	test: 0.273858

Epoch: 40
Loss: 0.12978976193986372
ROC train: 0.932780	val: 0.729427	test: 0.706295
PRC train: 0.657444	val: 0.303039	test: 0.271395

Epoch: 41
Loss: 0.13103602144798984
ROC train: 0.935587	val: 0.728210	test: 0.702461
PRC train: 0.672702	val: 0.303818	test: 0.261166

Epoch: 42
Loss: 0.1291207026743276
ROC train: 0.935659	val: 0.736317	test: 0.707839
PRC train: 0.669268	val: 0.299390	test: 0.266431

Epoch: 43
Loss: 0.1275024394220206
ROC train: 0.936274	val: 0.725055	test: 0.692558
PRC train: 0.671924	val: 0.296846	test: 0.257222

Epoch: 44
Loss: 0.12616022586255426
ROC train: 0.935606	val: 0.735121	test: 0.717958
PRC train: 0.671355	val: 0.309903	test: 0.274585

Epoch: 45
Loss: 0.12566042179342118
ROC train: 0.938429	val: 0.732227	test: 0.716951
PRC train: 0.679360	val: 0.300338	test: 0.274760

Epoch: 46
Loss: 0.12605169607413905
ROC train: 0.939922	val: 0.732534	test: 0.707577
PRC train: 0.681886	val: 0.302898	test: 0.264019

Epoch: 47
Loss: 0.12506243815811385
ROC train: 0.939979	val: 0.727562	test: 0.706358
PRC train: 0.678923	val: 0.296220	test: 0.263859

Epoch: 48
Loss: 0.12303570962871448
ROC train: 0.942050	val: 0.733432	test: 0.716503
PRC train: 0.686984	val: 0.310896	test: 0.277424

Epoch: 49
Loss: 0.12353383643435001
ROC train: 0.943435	val: 0.729611	test: 0.710792
PRC train: 0.697805	val: 0.308280	test: 0.279450

Epoch: 50
Loss: 0.12360727986002545
ROC train: 0.943899	val: 0.718805	test: 0.700879
PRC train: 0.696394	val: 0.285101	test: 0.254468

Epoch: 51
Loss: 0.12438088496240163
ROC train: 0.944561	val: 0.729457	test: 0.710165
PRC train: 0.699989	val: 0.298068	test: 0.272748

Epoch: 52
Loss: 0.12260772893999282
ROC train: 0.946522	val: 0.734150	test: 0.715026
PRC train: 0.710365	val: 0.304696	test: 0.266415

Epoch: 53
Loss: 0.12214951589160848
ROC train: 0.947502	val: 0.714198	test: 0.698906
PRC train: 0.703820	val: 0.274567	test: 0.247954

Epoch: 54
Loss: 0.12011311297060762
ROC train: 0.944824	val: 0.734278	test: 0.718390
PRC train: 0.705051	val: 0.307982	test: 0.281960

Epoch: 55
Loss: 0.12120002767034403
ROC train: 0.947333	val: 0.729323	test: 0.712088
PRC train: 0.707126	val: 0.287611	test: 0.263997

Epoch: 56
Loss: 0.11979675134598698
ROC train: 0.947183	val: 0.736307	test: 0.710973
PRC train: 0.711120	val: 0.313239	test: 0.271048

Epoch: 57
Loss: 0.120006542883748
ROC train: 0.948931	val: 0.725889	test: 0.695189
PRC train: 0.715703	val: 0.280069	test: 0.256187

Epoch: 58
Loss: 0.1168711302962943
ROC train: 0.952238	val: 0.723107	test: 0.702722
PRC train: 0.728116	val: 0.293044	test: 0.259392

Epoch: 59
Loss: 0.11644390766688202
ROC train: 0.949063	val: 0.719687	test: 0.707621
PRC train: 0.721036	val: 0.289428	test: 0.271187

Epoch: 60
Loss: 0.11572055220703009
ROC train: 0.951685	val: 0.729688	test: 0.704626
PRC train: 0.727785	val: 0.286549	test: 0.262049

Epoch: 61
Loss: 0.11865282500682407
ROC train: 0.951867	val: 0.737546	test: 0.714731
PRC train: 0.725367	val: 0.301750	test: 0.270450

Epoch: 62
Loss: 0.1172047649553698
ROC train: 0.952910	val: 0.739976	test: 0.719246
PRC train: 0.737261	val: 0.307606	test: 0.276532

Epoch: 63
Loss: 0.11376973910852749
ROC train: 0.951139	val: 0.730399	test: 0.707431
PRC train: 0.731206	val: 0.286281	test: 0.259017

Epoch: 64
Loss: 0.11485218362645802
ROC train: 0.955777	val: 0.727375	test: 0.707153
PRC train: 0.741354	val: 0.283652	test: 0.260867

Epoch: 65
Loss: 0.11476358148973966
ROC train: 0.955608	val: 0.729550	test: 0.710407
PRC train: 0.741188	val: 0.298230	test: 0.263346

Epoch: 66
Loss: 0.1191653557631888
ROC train: 0.956205	val: 0.719232	test: 0.697318
PRC train: 0.747084	val: 0.286606	test: 0.251786

Epoch: 67
Loss: 0.1143783145168966
ROC train: 0.956008	val: 0.723866	test: 0.705566
PRC train: 0.747120	val: 0.293042	test: 0.263828

Epoch: 68
Loss: 0.11453928944562962
ROC train: 0.956051	val: 0.726519	test: 0.708480
PRC train: 0.749589	val: 0.276204	test: 0.260560

Epoch: 69
Loss: 0.11252876127259066
ROC train: 0.958773	val: 0.724919	test: 0.707312
PRC train: 0.757479	val: 0.286935	test: 0.257422

Epoch: 70
Loss: 0.1115259942449617
ROC train: 0.959019	val: 0.726987	test: 0.699074
PRC train: 0.757629	val: 0.288086	test: 0.257152

Epoch: 71
Loss: 0.11149716695062442
ROC train: 0.960880	val: 0.731070	test: 0.706157
PRC train: 0.766381	val: 0.298271	test: 0.264045

Epoch: 72
Loss: 0.1132747782477297
ROC train: 0.957894	val: 0.718281	test: 0.713460
PRC train: 0.749884	val: 0.275061	test: 0.269422

Epoch: 73
Loss: 0.11151487424679019
ROC train: 0.958286	val: 0.716030	test: 0.697986
PRC train: 0.757419	val: 0.267706	test: 0.247621

Epoch: 74
Loss: 0.11254190765610451
ROC train: 0.959523	val: 0.730599	test: 0.718473
PRC train: 0.767030	val: 0.299401	test: 0.278880

Epoch: 75
Loss: 0.10779630550246487
ROC train: 0.963395	val: 0.717163	test: 0.702421
PRC train: 0.779470	val: 0.270117	test: 0.248687

Epoch: 76
Loss: 0.10980474544222642
ROC train: 0.963467	val: 0.728821	test: 0.711190
PRC train: 0.774845	val: 0.283888	test: 0.264088

Epoch: 77
Loss: 0.10904459707095608
ROC train: 0.964203	val: 0.717107	test: 0.701298
PRC train: 0.781557	val: 0.281492	test: 0.260956

Epoch: 78
Loss: 0.10690344353757682
ROC train: 0.963987	val: 0.718063	test: 0.703980
PRC train: 0.782182	val: 0.281498	test: 0.254002

Epoch: 79
Loss: 0.11166226956685917
ROC train: 0.964804	val: 0.726915	test: 0.703968
PRC train: 0.778043	val: 0.281367	test: 0.258174

Epoch: 80
Loss: 0.10668473224167499
ROC train: 0.965393	val: 0.721668	test: 0.701871
PRC train: 0.786103	val: 0.270067	test: 0.245957

Epoch: 81
Loss: 0.1054910687416366
ROC train: 0.965402	val: 0.724703	test: 0.711575
PRC train: 0.785046	val: 0.287570	test: 0.258596

Epoch: 82
Loss: 0.10658978870955939
ROC train: 0.967070	val: 0.720968	test: 0.708014
PRC train: 0.793012	val: 0.300732	test: 0.272827

Epoch: 83
Loss: 0.1056139240719559
ROC train: 0.967421	val: 0.729481	test: 0.714408
PRC train: 0.796026	val: 0.287367	test: 0.267198

Epoch: 84
Loss: 0.10616358903630636
ROC train: 0.969020	val: 0.722407	test: 0.707551
PRC train: 0.799835	val: 0.297935	test: 0.270603

Epoch: 85
Loss: 0.10490645652129071
ROC train: 0.969016	val: 0.729615	test: 0.712558
PRC train: 0.801725	val: 0.290554	test: 0.268210

Epoch: 86
Loss: 0.10438613651292117
ROC train: 0.967241	val: 0.718705	test: 0.694610
PRC train: 0.790093	val: 0.284719	test: 0.255900

Epoch: 87
Loss: 0.10638109873038055
ROC train: 0.969134	val: 0.724342	test: 0.698819
PRC train: 0.796155	val: 0.282422	test: 0.257217

Epoch: 88
Loss: 0.10357280822222545
ROC train: 0.970410	val: 0.726615	test: 0.706294
PRC train: 0.806172	val: 0.277481	test: 0.258843

Epoch: 89
Loss: 0.10211629085933521
ROC train: 0.970970	val: 0.720769	test: 0.704134
PRC train: 0.808211	val: 0.289535	test: 0.259845

Epoch: 90
Loss: 0.10299316035629023
ROC train: 0.970267	val: 0.726128	test: 0.707792
PRC train: 0.801703	val: 0.292421	test: 0.267594

Epoch: 91
Loss: 0.10279096490644309
ROC train: 0.971832	val: 0.728930	test: 0.713226
PRC train: 0.812745	val: 0.295742	test: 0.265420

Epoch: 92
Loss: 0.10253643056117034
ROC train: 0.972953	val: 0.722916	test: 0.707345
PRC train: 0.817408	val: 0.282132	test: 0.263399

Epoch: 93
Loss: 0.10290841243329789
ROC train: 0.972804	val: 0.721054	test: 0.710563
PRC train: 0.820216	val: 0.293895	test: 0.269412

Epoch: 94
Loss: 0.10134474114104211
ROC train: 0.918281	val: 0.754215	test: 0.733675
PRC train: 0.618646	val: 0.339494	test: 0.317668

Epoch: 34
Loss: 0.14437161801701565
ROC train: 0.920023	val: 0.751381	test: 0.740342
PRC train: 0.634998	val: 0.345517	test: 0.323195

Epoch: 35
Loss: 0.14558032932269843
ROC train: 0.923093	val: 0.745105	test: 0.732946
PRC train: 0.643416	val: 0.344707	test: 0.320767

Epoch: 36
Loss: 0.14233373611882308
ROC train: 0.922899	val: 0.745597	test: 0.735027
PRC train: 0.632632	val: 0.324809	test: 0.315235

Epoch: 37
Loss: 0.14314279811794087
ROC train: 0.925205	val: 0.751911	test: 0.732799
PRC train: 0.643185	val: 0.338901	test: 0.316297

Epoch: 38
Loss: 0.14291220136587335
ROC train: 0.928929	val: 0.743272	test: 0.732611
PRC train: 0.663024	val: 0.340535	test: 0.319193

Epoch: 39
Loss: 0.14058538312695606
ROC train: 0.927284	val: 0.733320	test: 0.733238
PRC train: 0.638021	val: 0.329747	test: 0.326249

Epoch: 40
Loss: 0.14174653211828162
ROC train: 0.928710	val: 0.733336	test: 0.731747
PRC train: 0.659454	val: 0.329271	test: 0.309061

Epoch: 41
Loss: 0.141259458722034
ROC train: 0.928737	val: 0.744566	test: 0.732863
PRC train: 0.658316	val: 0.343670	test: 0.309047

Epoch: 42
Loss: 0.14045360953762767
ROC train: 0.931480	val: 0.738542	test: 0.732174
PRC train: 0.663730	val: 0.328839	test: 0.315425

Epoch: 43
Loss: 0.1395537418462347
ROC train: 0.930169	val: 0.748709	test: 0.734467
PRC train: 0.658253	val: 0.341877	test: 0.322006

Epoch: 44
Loss: 0.1358061366245715
ROC train: 0.932541	val: 0.742505	test: 0.733740
PRC train: 0.676574	val: 0.340846	test: 0.321881

Epoch: 45
Loss: 0.13760374281271004
ROC train: 0.933585	val: 0.736584	test: 0.729415
PRC train: 0.663976	val: 0.328842	test: 0.318601

Epoch: 46
Loss: 0.13447603200810795
ROC train: 0.936778	val: 0.738072	test: 0.724651
PRC train: 0.689863	val: 0.335894	test: 0.308356

Epoch: 47
Loss: 0.13569073566675757
ROC train: 0.936415	val: 0.739775	test: 0.730237
PRC train: 0.686405	val: 0.340704	test: 0.320925

Epoch: 48
Loss: 0.13472251117500297
ROC train: 0.939662	val: 0.743808	test: 0.726025
PRC train: 0.696242	val: 0.341278	test: 0.320753

Epoch: 49
Loss: 0.13217235287923418
ROC train: 0.938399	val: 0.738327	test: 0.728019
PRC train: 0.692317	val: 0.341054	test: 0.320255

Epoch: 50
Loss: 0.1324812440471783
ROC train: 0.939407	val: 0.742244	test: 0.733865
PRC train: 0.692963	val: 0.337518	test: 0.320546

Epoch: 51
Loss: 0.13220716167568683
ROC train: 0.940093	val: 0.740339	test: 0.731496
PRC train: 0.690958	val: 0.327563	test: 0.319928

Epoch: 52
Loss: 0.1317493810882887
ROC train: 0.939627	val: 0.738127	test: 0.730830
PRC train: 0.696749	val: 0.339314	test: 0.328144

Epoch: 53
Loss: 0.1318506575375493
ROC train: 0.942764	val: 0.730878	test: 0.723460
PRC train: 0.701321	val: 0.310472	test: 0.310051

Epoch: 54
Loss: 0.13190847625963084
ROC train: 0.941858	val: 0.725683	test: 0.716317
PRC train: 0.695661	val: 0.319444	test: 0.308636

Epoch: 55
Loss: 0.13091924294017898
ROC train: 0.944827	val: 0.736591	test: 0.724758
PRC train: 0.716507	val: 0.335294	test: 0.319362

Epoch: 56
Loss: 0.1289827555811172
ROC train: 0.947206	val: 0.737182	test: 0.725836
PRC train: 0.721657	val: 0.325788	test: 0.309199

Epoch: 57
Loss: 0.12620409182589476
ROC train: 0.947284	val: 0.740273	test: 0.723419
PRC train: 0.727615	val: 0.334775	test: 0.307237

Epoch: 58
Loss: 0.12702814434484933
ROC train: 0.949865	val: 0.740001	test: 0.724615
PRC train: 0.735233	val: 0.331010	test: 0.318333

Epoch: 59
Loss: 0.1267824729449463
ROC train: 0.950400	val: 0.738588	test: 0.734771
PRC train: 0.730114	val: 0.335595	test: 0.322168

Epoch: 60
Loss: 0.12565249770950335
ROC train: 0.949852	val: 0.730789	test: 0.721328
PRC train: 0.730759	val: 0.325703	test: 0.313185

Epoch: 61
Loss: 0.1274847366868542
ROC train: 0.951486	val: 0.736926	test: 0.733628
PRC train: 0.736876	val: 0.331662	test: 0.328318

Epoch: 62
Loss: 0.12591221082801365
ROC train: 0.951660	val: 0.748900	test: 0.732916
PRC train: 0.741689	val: 0.343915	test: 0.317586

Epoch: 63
Loss: 0.12439685537522176
ROC train: 0.953230	val: 0.736348	test: 0.725183
PRC train: 0.744421	val: 0.325000	test: 0.309057

Epoch: 64
Loss: 0.1236254825544068
ROC train: 0.954488	val: 0.736973	test: 0.730868
PRC train: 0.750582	val: 0.320953	test: 0.310822

Epoch: 65
Loss: 0.12285102566644265
ROC train: 0.954741	val: 0.741867	test: 0.734580
PRC train: 0.754716	val: 0.321515	test: 0.322170

Epoch: 66
Loss: 0.12324107618142173
ROC train: 0.953972	val: 0.744875	test: 0.734592
PRC train: 0.752615	val: 0.341957	test: 0.324823

Epoch: 67
Loss: 0.12087186035346585
ROC train: 0.954886	val: 0.740794	test: 0.731210
PRC train: 0.756340	val: 0.323849	test: 0.320513

Epoch: 68
Loss: 0.12102787432837339
ROC train: 0.954780	val: 0.740097	test: 0.730368
PRC train: 0.751223	val: 0.335802	test: 0.320562

Epoch: 69
Loss: 0.12128760012629995
ROC train: 0.958512	val: 0.738464	test: 0.734110
PRC train: 0.767498	val: 0.326275	test: 0.322346

Epoch: 70
Loss: 0.11930392884386437
ROC train: 0.958649	val: 0.741353	test: 0.729339
PRC train: 0.767907	val: 0.323650	test: 0.319558

Epoch: 71
Loss: 0.11800112341363055
ROC train: 0.958495	val: 0.722620	test: 0.720183
PRC train: 0.770367	val: 0.327159	test: 0.314854

Epoch: 72
Loss: 0.12012353538834303
ROC train: 0.959285	val: 0.741486	test: 0.730636
PRC train: 0.765653	val: 0.326816	test: 0.325310

Epoch: 73
Loss: 0.11589286172138942
ROC train: 0.961436	val: 0.726108	test: 0.727938
PRC train: 0.770206	val: 0.319634	test: 0.320319

Epoch: 74
Loss: 0.11655378112486091
ROC train: 0.963210	val: 0.730052	test: 0.724337
PRC train: 0.785501	val: 0.319936	test: 0.314675

Epoch: 75
Loss: 0.11718016590774151
ROC train: 0.962678	val: 0.738131	test: 0.736294
PRC train: 0.784309	val: 0.329182	test: 0.326774

Epoch: 76
Loss: 0.11632765761559907
ROC train: 0.963624	val: 0.738790	test: 0.738158
PRC train: 0.785911	val: 0.334120	test: 0.333259

Epoch: 77
Loss: 0.11708015899934145
ROC train: 0.960276	val: 0.740915	test: 0.732459
PRC train: 0.763617	val: 0.318262	test: 0.312642

Epoch: 78
Loss: 0.11653218604159886
ROC train: 0.964466	val: 0.738069	test: 0.731891
PRC train: 0.793608	val: 0.322155	test: 0.313947

Epoch: 79
Loss: 0.11487106576123428
ROC train: 0.962305	val: 0.727201	test: 0.721494
PRC train: 0.781856	val: 0.309684	test: 0.300858

Epoch: 80
Loss: 0.11628853173321682
ROC train: 0.962389	val: 0.735323	test: 0.728170
PRC train: 0.786825	val: 0.316920	test: 0.322996

Epoch: 81
Loss: 0.11617767388862009
ROC train: 0.962687	val: 0.733986	test: 0.724772
PRC train: 0.783573	val: 0.318605	test: 0.305988

Epoch: 82
Loss: 0.11507102685016282
ROC train: 0.966250	val: 0.731642	test: 0.728351
PRC train: 0.798754	val: 0.319998	test: 0.320003

Epoch: 83
Loss: 0.11281251364453992
ROC train: 0.967359	val: 0.733725	test: 0.729165
PRC train: 0.805704	val: 0.318479	test: 0.322093

Epoch: 84
Loss: 0.1093089002765486
ROC train: 0.968275	val: 0.732902	test: 0.722378
PRC train: 0.808042	val: 0.313462	test: 0.315185

Epoch: 85
Loss: 0.1114942548402681
ROC train: 0.968487	val: 0.734168	test: 0.731119
PRC train: 0.806401	val: 0.314430	test: 0.323806

Epoch: 86
Loss: 0.11023935590221448
ROC train: 0.969636	val: 0.732272	test: 0.722868
PRC train: 0.821283	val: 0.320985	test: 0.319854

Epoch: 87
Loss: 0.10935287268886697
ROC train: 0.967794	val: 0.733971	test: 0.721566
PRC train: 0.811130	val: 0.318241	test: 0.309252

Epoch: 88
Loss: 0.10899506513229264
ROC train: 0.969154	val: 0.725449	test: 0.721340
PRC train: 0.809833	val: 0.308287	test: 0.321105

Epoch: 89
Loss: 0.10816129856093223
ROC train: 0.970355	val: 0.734471	test: 0.730057
PRC train: 0.816498	val: 0.320231	test: 0.324082

Epoch: 90
Loss: 0.10798448624735157
ROC train: 0.971308	val: 0.730580	test: 0.719301
PRC train: 0.823281	val: 0.319701	test: 0.322481

Epoch: 91
Loss: 0.10861222101724032
ROC train: 0.971499	val: 0.730319	test: 0.718873
PRC train: 0.817826	val: 0.307230	test: 0.312709

Epoch: 92
Loss: 0.108027860898751
ROC train: 0.972015	val: 0.731418	test: 0.728363
PRC train: 0.826968	val: 0.314421	test: 0.321121

Epoch: 93
Loss: 0.10686584682829885
ROC train: 0.973147	val: 0.734568	test: 0.725696
PRC train: 0.833024	val: 0.322895	test: 0.309560

Epoch: 94
Loss: 0.10683163561744911
ROC train: 0.916200	val: 0.739906	test: 0.725409
PRC train: 0.617160	val: 0.329915	test: 0.307638

Epoch: 34
Loss: 0.14587592570183963
ROC train: 0.922400	val: 0.746657	test: 0.732639
PRC train: 0.640923	val: 0.343462	test: 0.319661

Epoch: 35
Loss: 0.14418384308081286
ROC train: 0.922166	val: 0.739478	test: 0.733334
PRC train: 0.640848	val: 0.331804	test: 0.321626

Epoch: 36
Loss: 0.14384764900519537
ROC train: 0.925130	val: 0.738577	test: 0.720140
PRC train: 0.643399	val: 0.340618	test: 0.304703

Epoch: 37
Loss: 0.14347731163227292
ROC train: 0.925103	val: 0.742780	test: 0.726552
PRC train: 0.641238	val: 0.335612	test: 0.313803

Epoch: 38
Loss: 0.1413568653112616
ROC train: 0.926607	val: 0.755079	test: 0.732316
PRC train: 0.653569	val: 0.354326	test: 0.319962

Epoch: 39
Loss: 0.14079231089234132
ROC train: 0.925240	val: 0.739338	test: 0.728303
PRC train: 0.637262	val: 0.298780	test: 0.292912

Epoch: 40
Loss: 0.1413324439010028
ROC train: 0.927987	val: 0.733560	test: 0.718451
PRC train: 0.646382	val: 0.318705	test: 0.302440

Epoch: 41
Loss: 0.138979770062271
ROC train: 0.933395	val: 0.731067	test: 0.719873
PRC train: 0.667708	val: 0.326505	test: 0.310109

Epoch: 42
Loss: 0.13726743594260613
ROC train: 0.931522	val: 0.747658	test: 0.729614
PRC train: 0.669928	val: 0.332985	test: 0.327140

Epoch: 43
Loss: 0.13843614429653853
ROC train: 0.931635	val: 0.749023	test: 0.733998
PRC train: 0.671452	val: 0.348921	test: 0.324960

Epoch: 44
Loss: 0.13714764261394252
ROC train: 0.935067	val: 0.745550	test: 0.729532
PRC train: 0.675214	val: 0.356455	test: 0.320211

Epoch: 45
Loss: 0.1371427691204582
ROC train: 0.934079	val: 0.731960	test: 0.722529
PRC train: 0.677624	val: 0.337522	test: 0.314460

Epoch: 46
Loss: 0.1354907893991595
ROC train: 0.933705	val: 0.741213	test: 0.722707
PRC train: 0.665306	val: 0.319436	test: 0.315404

Epoch: 47
Loss: 0.13351320568950822
ROC train: 0.938551	val: 0.746364	test: 0.728364
PRC train: 0.692996	val: 0.337252	test: 0.318338

Epoch: 48
Loss: 0.13479135220986393
ROC train: 0.941529	val: 0.733534	test: 0.724817
PRC train: 0.702897	val: 0.333744	test: 0.316260

Epoch: 49
Loss: 0.13346341184254334
ROC train: 0.940890	val: 0.743843	test: 0.726620
PRC train: 0.700643	val: 0.331519	test: 0.326080

Epoch: 50
Loss: 0.13177751587376926
ROC train: 0.942337	val: 0.734175	test: 0.723320
PRC train: 0.704383	val: 0.339657	test: 0.322336

Epoch: 51
Loss: 0.13118215773068087
ROC train: 0.943573	val: 0.730547	test: 0.724053
PRC train: 0.709817	val: 0.335878	test: 0.316163

Epoch: 52
Loss: 0.13137847512914969
ROC train: 0.943474	val: 0.733776	test: 0.717430
PRC train: 0.709735	val: 0.333239	test: 0.304642

Epoch: 53
Loss: 0.12952823780233527
ROC train: 0.944970	val: 0.752314	test: 0.738603
PRC train: 0.712592	val: 0.346992	test: 0.335159

Epoch: 54
Loss: 0.12953348283417696
ROC train: 0.946781	val: 0.747398	test: 0.723935
PRC train: 0.722337	val: 0.337748	test: 0.315588

Epoch: 55
Loss: 0.12944827088995772
ROC train: 0.947023	val: 0.743035	test: 0.725698
PRC train: 0.723662	val: 0.330899	test: 0.311282

Epoch: 56
Loss: 0.13000919539477526
ROC train: 0.947859	val: 0.742596	test: 0.718912
PRC train: 0.723576	val: 0.326047	test: 0.308847

Epoch: 57
Loss: 0.12672289012600746
ROC train: 0.949735	val: 0.738592	test: 0.729181
PRC train: 0.735715	val: 0.329901	test: 0.318892

Epoch: 58
Loss: 0.12892526198632825
ROC train: 0.948208	val: 0.746343	test: 0.726809
PRC train: 0.718808	val: 0.343666	test: 0.324698

Epoch: 59
Loss: 0.12466858251055557
ROC train: 0.951092	val: 0.746221	test: 0.728597
PRC train: 0.736286	val: 0.337944	test: 0.318780

Epoch: 60
Loss: 0.12591450846261065
ROC train: 0.948664	val: 0.735984	test: 0.729211
PRC train: 0.727488	val: 0.338296	test: 0.321144

Epoch: 61
Loss: 0.12802833520400186
ROC train: 0.952925	val: 0.743128	test: 0.729887
PRC train: 0.739086	val: 0.344078	test: 0.322437

Epoch: 62
Loss: 0.12649021314489198
ROC train: 0.950235	val: 0.734800	test: 0.722498
PRC train: 0.725317	val: 0.324992	test: 0.309227

Epoch: 63
Loss: 0.12392830691276302
ROC train: 0.954078	val: 0.739215	test: 0.735876
PRC train: 0.746821	val: 0.338990	test: 0.324437

Epoch: 64
Loss: 0.12269222928134997
ROC train: 0.955922	val: 0.746818	test: 0.733676
PRC train: 0.755847	val: 0.338923	test: 0.330892

Epoch: 65
Loss: 0.12155507577175405
ROC train: 0.956052	val: 0.739718	test: 0.731419
PRC train: 0.757520	val: 0.341022	test: 0.326159

Epoch: 66
Loss: 0.12255958163210473
ROC train: 0.955682	val: 0.725130	test: 0.712595
PRC train: 0.752473	val: 0.325587	test: 0.303267

Epoch: 67
Loss: 0.12216939081809369
ROC train: 0.957246	val: 0.742595	test: 0.732532
PRC train: 0.762545	val: 0.328585	test: 0.322751

Epoch: 68
Loss: 0.12104865098337513
ROC train: 0.957155	val: 0.754064	test: 0.732921
PRC train: 0.759488	val: 0.343100	test: 0.325701

Epoch: 69
Loss: 0.11970577707820537
ROC train: 0.960108	val: 0.729600	test: 0.729575
PRC train: 0.774996	val: 0.336713	test: 0.327569

Epoch: 70
Loss: 0.11818427028460916
ROC train: 0.959237	val: 0.740501	test: 0.733556
PRC train: 0.771128	val: 0.307160	test: 0.308579

Epoch: 71
Loss: 0.11894614806704906
ROC train: 0.960412	val: 0.743335	test: 0.726413
PRC train: 0.774459	val: 0.331647	test: 0.324320

Epoch: 72
Loss: 0.11962107474096834
ROC train: 0.961654	val: 0.738354	test: 0.723752
PRC train: 0.782525	val: 0.319557	test: 0.308186

Epoch: 73
Loss: 0.1185755314043444
ROC train: 0.962841	val: 0.741723	test: 0.727806
PRC train: 0.789511	val: 0.334628	test: 0.322963

Epoch: 74
Loss: 0.11927559438164045
ROC train: 0.960755	val: 0.720178	test: 0.713364
PRC train: 0.772626	val: 0.305182	test: 0.296155

Epoch: 75
Loss: 0.11873675468349405
ROC train: 0.961756	val: 0.748559	test: 0.732575
PRC train: 0.780953	val: 0.322628	test: 0.328308

Epoch: 76
Loss: 0.11687524087365934
ROC train: 0.962904	val: 0.733189	test: 0.724442
PRC train: 0.791460	val: 0.323533	test: 0.316098

Epoch: 77
Loss: 0.11645406109628982
ROC train: 0.963946	val: 0.738346	test: 0.728530
PRC train: 0.786406	val: 0.321777	test: 0.334848

Epoch: 78
Loss: 0.11455366826577346
ROC train: 0.965365	val: 0.740911	test: 0.733359
PRC train: 0.796727	val: 0.338412	test: 0.330692

Epoch: 79
Loss: 0.11529277847559571
ROC train: 0.967025	val: 0.724597	test: 0.719529
PRC train: 0.803977	val: 0.313909	test: 0.312936

Epoch: 80
Loss: 0.11309638798876716
ROC train: 0.966247	val: 0.742619	test: 0.727013
PRC train: 0.797903	val: 0.333772	test: 0.335085

Epoch: 81
Loss: 0.11146726099451533
ROC train: 0.966293	val: 0.736274	test: 0.722562
PRC train: 0.804441	val: 0.324246	test: 0.317710

Epoch: 82
Loss: 0.11267726325651375
ROC train: 0.967963	val: 0.744780	test: 0.724864
PRC train: 0.806185	val: 0.330986	test: 0.330801

Epoch: 83
Loss: 0.11184234093765646
ROC train: 0.969603	val: 0.739798	test: 0.721642
PRC train: 0.814522	val: 0.328257	test: 0.316986

Epoch: 84
Loss: 0.11175622047485234
ROC train: 0.967895	val: 0.739715	test: 0.721085
PRC train: 0.806626	val: 0.319384	test: 0.307351

Epoch: 85
Loss: 0.11105376050819445
ROC train: 0.967893	val: 0.730569	test: 0.719684
PRC train: 0.809211	val: 0.331963	test: 0.331383

Epoch: 86
Loss: 0.11023822813962957
ROC train: 0.969427	val: 0.743574	test: 0.717715
PRC train: 0.813186	val: 0.322071	test: 0.307406

Epoch: 87
Loss: 0.11042782657985227
ROC train: 0.970361	val: 0.733050	test: 0.716005
PRC train: 0.820671	val: 0.320208	test: 0.303646

Epoch: 88
Loss: 0.1092091610420356
ROC train: 0.970765	val: 0.720459	test: 0.708034
PRC train: 0.825794	val: 0.316187	test: 0.309565

Epoch: 89
Loss: 0.11120248335870137
ROC train: 0.969100	val: 0.738881	test: 0.725276
PRC train: 0.808852	val: 0.321725	test: 0.320143

Epoch: 90
Loss: 0.10888148351093782
ROC train: 0.970738	val: 0.736846	test: 0.724183
PRC train: 0.819777	val: 0.331030	test: 0.321011

Epoch: 91
Loss: 0.10833915424806889
ROC train: 0.972140	val: 0.735473	test: 0.719104
PRC train: 0.828798	val: 0.338643	test: 0.327152

Epoch: 92
Loss: 0.10760200409288112
ROC train: 0.971903	val: 0.729653	test: 0.720374
PRC train: 0.826617	val: 0.312691	test: 0.311703

Epoch: 93
Loss: 0.10778230340611712
ROC train: 0.971409	val: 0.732512	test: 0.718010
PRC train: 0.817900	val: 0.291085	test: 0.298523

Epoch: 94
Loss: 0.10613479674343095
ROC train: 0.921442	val: 0.734835	test: 0.727097
PRC train: 0.633415	val: 0.345132	test: 0.324627

Epoch: 34
Loss: 0.14493778880051902
ROC train: 0.922684	val: 0.738130	test: 0.730832
PRC train: 0.640362	val: 0.337578	test: 0.329623

Epoch: 35
Loss: 0.1434700477324059
ROC train: 0.924350	val: 0.747182	test: 0.736084
PRC train: 0.637162	val: 0.338269	test: 0.322277

Epoch: 36
Loss: 0.14318683288184803
ROC train: 0.921670	val: 0.744112	test: 0.730549
PRC train: 0.640393	val: 0.323917	test: 0.319879

Epoch: 37
Loss: 0.1441053537203262
ROC train: 0.924211	val: 0.733097	test: 0.728621
PRC train: 0.643031	val: 0.336160	test: 0.324891

Epoch: 38
Loss: 0.14145417999339321
ROC train: 0.925716	val: 0.736113	test: 0.728437
PRC train: 0.644299	val: 0.326063	test: 0.322239

Epoch: 39
Loss: 0.14043503036078445
ROC train: 0.928992	val: 0.743225	test: 0.729575
PRC train: 0.661018	val: 0.346439	test: 0.315298

Epoch: 40
Loss: 0.13876795464372105
ROC train: 0.930202	val: 0.746901	test: 0.727763
PRC train: 0.658206	val: 0.329147	test: 0.317908

Epoch: 41
Loss: 0.1402640311734897
ROC train: 0.932435	val: 0.734551	test: 0.717034
PRC train: 0.664741	val: 0.332529	test: 0.315924

Epoch: 42
Loss: 0.1395559048191085
ROC train: 0.927963	val: 0.743309	test: 0.727247
PRC train: 0.656761	val: 0.338314	test: 0.327818

Epoch: 43
Loss: 0.13762971870767818
ROC train: 0.929093	val: 0.738783	test: 0.736441
PRC train: 0.650758	val: 0.304780	test: 0.321182

Epoch: 44
Loss: 0.13671314460529763
ROC train: 0.930248	val: 0.739436	test: 0.738354
PRC train: 0.667293	val: 0.344767	test: 0.337334

Epoch: 45
Loss: 0.13593242173967077
ROC train: 0.935024	val: 0.742175	test: 0.728953
PRC train: 0.677984	val: 0.352332	test: 0.329880

Epoch: 46
Loss: 0.13819311219572303
ROC train: 0.937690	val: 0.738188	test: 0.725769
PRC train: 0.691351	val: 0.339045	test: 0.326940

Epoch: 47
Loss: 0.13530308757039386
ROC train: 0.937856	val: 0.739815	test: 0.730216
PRC train: 0.689696	val: 0.336371	test: 0.333945

Epoch: 48
Loss: 0.13359280746411611
ROC train: 0.938446	val: 0.737041	test: 0.722866
PRC train: 0.690730	val: 0.339764	test: 0.314537

Epoch: 49
Loss: 0.13416511589775196
ROC train: 0.939396	val: 0.736908	test: 0.727947
PRC train: 0.697118	val: 0.337509	test: 0.324628

Epoch: 50
Loss: 0.134360719769282
ROC train: 0.939899	val: 0.737245	test: 0.726422
PRC train: 0.695545	val: 0.344595	test: 0.328137

Epoch: 51
Loss: 0.1332779239127931
ROC train: 0.941252	val: 0.745422	test: 0.727453
PRC train: 0.697985	val: 0.347625	test: 0.324494

Epoch: 52
Loss: 0.13168193404351136
ROC train: 0.943310	val: 0.726744	test: 0.723514
PRC train: 0.707167	val: 0.328472	test: 0.319399

Epoch: 53
Loss: 0.1317673906713401
ROC train: 0.945562	val: 0.731495	test: 0.727488
PRC train: 0.716741	val: 0.334672	test: 0.321596

Epoch: 54
Loss: 0.1300120510326373
ROC train: 0.942778	val: 0.732109	test: 0.722465
PRC train: 0.704620	val: 0.334060	test: 0.322353

Epoch: 55
Loss: 0.13041079459858643
ROC train: 0.944167	val: 0.720205	test: 0.714814
PRC train: 0.707501	val: 0.332851	test: 0.319018

Epoch: 56
Loss: 0.12994391593101112
ROC train: 0.947049	val: 0.726221	test: 0.719380
PRC train: 0.718256	val: 0.328312	test: 0.320881

Epoch: 57
Loss: 0.12919669655974986
ROC train: 0.947775	val: 0.730262	test: 0.720659
PRC train: 0.726594	val: 0.325824	test: 0.320359

Epoch: 58
Loss: 0.12988025875307302
ROC train: 0.948257	val: 0.726930	test: 0.716734
PRC train: 0.724775	val: 0.329269	test: 0.321046

Epoch: 59
Loss: 0.12739829416276782
ROC train: 0.948149	val: 0.733142	test: 0.724893
PRC train: 0.722838	val: 0.338235	test: 0.331733

Epoch: 60
Loss: 0.12711792955464749
ROC train: 0.949588	val: 0.729560	test: 0.720429
PRC train: 0.726942	val: 0.325707	test: 0.318894

Epoch: 61
Loss: 0.12603722219134472
ROC train: 0.952139	val: 0.729893	test: 0.718138
PRC train: 0.733615	val: 0.335901	test: 0.327914

Epoch: 62
Loss: 0.12414156383931683
ROC train: 0.952331	val: 0.731074	test: 0.725492
PRC train: 0.737947	val: 0.321976	test: 0.321355

Epoch: 63
Loss: 0.12219717285710402
ROC train: 0.951623	val: 0.734825	test: 0.723477
PRC train: 0.730139	val: 0.327814	test: 0.322220

Epoch: 64
Loss: 0.12557232040381872
ROC train: 0.952996	val: 0.728067	test: 0.721434
PRC train: 0.746179	val: 0.334493	test: 0.331983

Epoch: 65
Loss: 0.12611453758578797
ROC train: 0.951591	val: 0.722053	test: 0.718109
PRC train: 0.738029	val: 0.318295	test: 0.311027

Epoch: 66
Loss: 0.12317577900338086
ROC train: 0.952291	val: 0.727698	test: 0.727815
PRC train: 0.741032	val: 0.329609	test: 0.331196

Epoch: 67
Loss: 0.12184512899485565
ROC train: 0.955615	val: 0.739925	test: 0.726121
PRC train: 0.749416	val: 0.358292	test: 0.330877

Epoch: 68
Loss: 0.12096032015135266
ROC train: 0.956383	val: 0.731876	test: 0.725704
PRC train: 0.757753	val: 0.334185	test: 0.329497

Epoch: 69
Loss: 0.12188715497755548
ROC train: 0.956432	val: 0.738424	test: 0.724683
PRC train: 0.754847	val: 0.338161	test: 0.333730

Epoch: 70
Loss: 0.12256825788406672
ROC train: 0.958023	val: 0.732496	test: 0.712657
PRC train: 0.762217	val: 0.335635	test: 0.312749

Epoch: 71
Loss: 0.12055954665390364
ROC train: 0.959071	val: 0.735252	test: 0.722324
PRC train: 0.766093	val: 0.323095	test: 0.317573

Epoch: 72
Loss: 0.12076636705597082
ROC train: 0.960310	val: 0.729052	test: 0.719322
PRC train: 0.777179	val: 0.334782	test: 0.323399

Epoch: 73
Loss: 0.11898814274338788
ROC train: 0.960452	val: 0.740345	test: 0.718833
PRC train: 0.770964	val: 0.339584	test: 0.318912

Epoch: 74
Loss: 0.11910104766153375
ROC train: 0.960629	val: 0.725640	test: 0.723328
PRC train: 0.774600	val: 0.324057	test: 0.321146

Epoch: 75
Loss: 0.11637969791236187
ROC train: 0.960857	val: 0.728282	test: 0.711555
PRC train: 0.778077	val: 0.335292	test: 0.322026

Epoch: 76
Loss: 0.1185401494520219
ROC train: 0.956795	val: 0.724449	test: 0.724499
PRC train: 0.755212	val: 0.309438	test: 0.310946

Epoch: 77
Loss: 0.11863319277535771
ROC train: 0.962028	val: 0.732918	test: 0.714448
PRC train: 0.783120	val: 0.336925	test: 0.314859

Epoch: 78
Loss: 0.11677421285139572
ROC train: 0.962499	val: 0.740666	test: 0.724497
PRC train: 0.783636	val: 0.333138	test: 0.319883

Epoch: 79
Loss: 0.11513046790692019
ROC train: 0.964409	val: 0.733540	test: 0.720454
PRC train: 0.790475	val: 0.318709	test: 0.315365

Epoch: 80
Loss: 0.11513552862018385
ROC train: 0.964928	val: 0.733082	test: 0.720054
PRC train: 0.797545	val: 0.327531	test: 0.321764

Epoch: 81
Loss: 0.11391738907964023
ROC train: 0.965908	val: 0.722467	test: 0.713421
PRC train: 0.799894	val: 0.310816	test: 0.308615

Epoch: 82
Loss: 0.11345995408615836
ROC train: 0.965967	val: 0.725389	test: 0.711376
PRC train: 0.798132	val: 0.313812	test: 0.307066

Epoch: 83
Loss: 0.11193481908771769
ROC train: 0.962949	val: 0.727523	test: 0.718425
PRC train: 0.785964	val: 0.335198	test: 0.325191

Epoch: 84
Loss: 0.1130004282223846
ROC train: 0.967920	val: 0.716287	test: 0.705848
PRC train: 0.804673	val: 0.320505	test: 0.309664

Epoch: 85
Loss: 0.11074112525287462
ROC train: 0.967870	val: 0.726563	test: 0.718057
PRC train: 0.807449	val: 0.321705	test: 0.318559

Epoch: 86
Loss: 0.11189013325966592
ROC train: 0.968209	val: 0.716661	test: 0.710464
PRC train: 0.809802	val: 0.317678	test: 0.318490

Epoch: 87
Loss: 0.11353034486393945
ROC train: 0.968961	val: 0.728661	test: 0.719830
PRC train: 0.812482	val: 0.330685	test: 0.326878

Epoch: 88
Loss: 0.11032718416288091
ROC train: 0.970202	val: 0.718281	test: 0.711081
PRC train: 0.814877	val: 0.301821	test: 0.300671

Epoch: 89
Loss: 0.11011372647983662
ROC train: 0.969011	val: 0.721803	test: 0.710092
PRC train: 0.808698	val: 0.321973	test: 0.315468

Epoch: 90
Loss: 0.10757611065507139
ROC train: 0.971157	val: 0.717600	test: 0.706392
PRC train: 0.818084	val: 0.313761	test: 0.305220

Epoch: 91
Loss: 0.10910966103994868
ROC train: 0.971562	val: 0.733887	test: 0.720530
PRC train: 0.822304	val: 0.331234	test: 0.324423

Epoch: 92
Loss: 0.11000622688619686
ROC train: 0.971709	val: 0.733352	test: 0.714398
PRC train: 0.824680	val: 0.327062	test: 0.314527

Epoch: 93
Loss: 0.10694247589782419
ROC train: 0.971158	val: 0.722749	test: 0.708149
PRC train: 0.823672	val: 0.316960	test: 0.312400

Epoch: 94
Loss: 0.10674829733797013
ROC train: 0.912719	val: 0.772235	test: 0.729654
PRC train: 0.609297	val: 0.365403	test: 0.351167

Epoch: 34
Loss: 0.15439671453809248
ROC train: 0.913895	val: 0.777181	test: 0.729681
PRC train: 0.609828	val: 0.363531	test: 0.355542

Epoch: 35
Loss: 0.15415326861691891
ROC train: 0.915130	val: 0.780196	test: 0.740223
PRC train: 0.616177	val: 0.380580	test: 0.363069

Epoch: 36
Loss: 0.15286804027540896
ROC train: 0.917723	val: 0.773097	test: 0.732337
PRC train: 0.626468	val: 0.369517	test: 0.345307

Epoch: 37
Loss: 0.15076963086303719
ROC train: 0.919599	val: 0.769082	test: 0.735790
PRC train: 0.624550	val: 0.358544	test: 0.353466

Epoch: 38
Loss: 0.15225219734316045
ROC train: 0.917845	val: 0.772632	test: 0.737325
PRC train: 0.630257	val: 0.369465	test: 0.363297

Epoch: 39
Loss: 0.15014224166731358
ROC train: 0.905401	val: 0.749321	test: 0.716302
PRC train: 0.560282	val: 0.307470	test: 0.296723

Epoch: 40
Loss: 0.15052049190635647
ROC train: 0.923382	val: 0.760149	test: 0.730347
PRC train: 0.640265	val: 0.350205	test: 0.353860

Epoch: 41
Loss: 0.1494705249716857
ROC train: 0.926061	val: 0.775622	test: 0.737614
PRC train: 0.647819	val: 0.371601	test: 0.356894

Epoch: 42
Loss: 0.14820401087034746
ROC train: 0.924287	val: 0.771688	test: 0.737815
PRC train: 0.645298	val: 0.380829	test: 0.369497

Epoch: 43
Loss: 0.14814745503375856
ROC train: 0.927203	val: 0.775539	test: 0.739646
PRC train: 0.654415	val: 0.371325	test: 0.366445

Epoch: 44
Loss: 0.14583258080425746
ROC train: 0.929336	val: 0.771902	test: 0.744012
PRC train: 0.662228	val: 0.368777	test: 0.367608

Epoch: 45
Loss: 0.14664625360444453
ROC train: 0.927015	val: 0.765808	test: 0.736802
PRC train: 0.649839	val: 0.354240	test: 0.361625

Epoch: 46
Loss: 0.146562325369361
ROC train: 0.928644	val: 0.767556	test: 0.736946
PRC train: 0.657817	val: 0.358913	test: 0.348767

Epoch: 47
Loss: 0.14518134213701792
ROC train: 0.929604	val: 0.770149	test: 0.724019
PRC train: 0.660489	val: 0.362734	test: 0.346546

Epoch: 48
Loss: 0.1429411163925732
ROC train: 0.933331	val: 0.779039	test: 0.743715
PRC train: 0.674920	val: 0.384061	test: 0.376664

Epoch: 49
Loss: 0.14038055331652963
ROC train: 0.935002	val: 0.771879	test: 0.736833
PRC train: 0.679687	val: 0.370950	test: 0.364321

Epoch: 50
Loss: 0.14097754902416568
ROC train: 0.935575	val: 0.772295	test: 0.739195
PRC train: 0.690076	val: 0.376807	test: 0.364963

Epoch: 51
Loss: 0.1414155141231931
ROC train: 0.937130	val: 0.764489	test: 0.750111
PRC train: 0.686388	val: 0.373300	test: 0.367813

Epoch: 52
Loss: 0.14101519687300873
ROC train: 0.938892	val: 0.779440	test: 0.744235
PRC train: 0.695869	val: 0.396495	test: 0.373881

Epoch: 53
Loss: 0.14015701226702082
ROC train: 0.939707	val: 0.764302	test: 0.727104
PRC train: 0.693863	val: 0.373508	test: 0.349208

Epoch: 54
Loss: 0.14031661069695128
ROC train: 0.940161	val: 0.773458	test: 0.748845
PRC train: 0.704363	val: 0.389300	test: 0.364313

Epoch: 55
Loss: 0.1387675463071033
ROC train: 0.940732	val: 0.766256	test: 0.739856
PRC train: 0.693060	val: 0.367883	test: 0.367341

Epoch: 56
Loss: 0.13683779991366585
ROC train: 0.940442	val: 0.771881	test: 0.737731
PRC train: 0.699140	val: 0.389978	test: 0.355978

Epoch: 57
Loss: 0.13500734654670413
ROC train: 0.942718	val: 0.770593	test: 0.754317
PRC train: 0.714320	val: 0.390537	test: 0.379767

Epoch: 58
Loss: 0.13735944081025944
ROC train: 0.941616	val: 0.762575	test: 0.736955
PRC train: 0.705597	val: 0.361964	test: 0.363071

Epoch: 59
Loss: 0.13666156373946178
ROC train: 0.945014	val: 0.775318	test: 0.735876
PRC train: 0.716869	val: 0.372919	test: 0.358204

Epoch: 60
Loss: 0.13528784110722852
ROC train: 0.945082	val: 0.766604	test: 0.735939
PRC train: 0.709404	val: 0.368138	test: 0.362220

Epoch: 61
Loss: 0.13448721539209305
ROC train: 0.944984	val: 0.763166	test: 0.733892
PRC train: 0.709119	val: 0.357633	test: 0.352648

Epoch: 62
Loss: 0.1331459091262445
ROC train: 0.946672	val: 0.782186	test: 0.741097
PRC train: 0.722128	val: 0.397208	test: 0.364565

Epoch: 63
Loss: 0.13457036198720643
ROC train: 0.949843	val: 0.779785	test: 0.751600
PRC train: 0.739179	val: 0.381514	test: 0.378405

Epoch: 64
Loss: 0.13469378870733326
ROC train: 0.942004	val: 0.766206	test: 0.740220
PRC train: 0.693891	val: 0.358921	test: 0.358390

Epoch: 65
Loss: 0.13264835330448915
ROC train: 0.950758	val: 0.774111	test: 0.746034
PRC train: 0.734423	val: 0.376836	test: 0.379324

Epoch: 66
Loss: 0.13179381819441255
ROC train: 0.952124	val: 0.774205	test: 0.735977
PRC train: 0.741930	val: 0.367579	test: 0.363105

Epoch: 67
Loss: 0.13033075766439128
ROC train: 0.954079	val: 0.771756	test: 0.736469
PRC train: 0.751892	val: 0.379093	test: 0.365636

Epoch: 68
Loss: 0.12925936778600328
ROC train: 0.951836	val: 0.763838	test: 0.744779
PRC train: 0.737167	val: 0.349724	test: 0.357049

Epoch: 69
Loss: 0.1307873785739172
ROC train: 0.946544	val: 0.771800	test: 0.730136
PRC train: 0.720160	val: 0.378809	test: 0.342079

Epoch: 70
Loss: 0.1296182084958067
ROC train: 0.954823	val: 0.776158	test: 0.739527
PRC train: 0.754355	val: 0.386366	test: 0.361577

Epoch: 71
Loss: 0.12733336950645274
ROC train: 0.955959	val: 0.775489	test: 0.749833
PRC train: 0.763158	val: 0.378689	test: 0.369835

Epoch: 72
Loss: 0.12711556994022927
ROC train: 0.957621	val: 0.782020	test: 0.740484
PRC train: 0.758104	val: 0.373193	test: 0.359611

Epoch: 73
Loss: 0.12727200003475045
ROC train: 0.954627	val: 0.767270	test: 0.745221
PRC train: 0.748704	val: 0.378369	test: 0.363936

Epoch: 74
Loss: 0.12655358185288765
ROC train: 0.957493	val: 0.768439	test: 0.737835
PRC train: 0.765774	val: 0.375641	test: 0.362107

Epoch: 75
Loss: 0.12589947522645042
ROC train: 0.954054	val: 0.766479	test: 0.748477
PRC train: 0.745383	val: 0.356350	test: 0.367991

Epoch: 76
Loss: 0.12484602527710319
ROC train: 0.961232	val: 0.769341	test: 0.740683
PRC train: 0.784569	val: 0.372810	test: 0.366904

Epoch: 77
Loss: 0.12303352094724096
ROC train: 0.960201	val: 0.774032	test: 0.745254
PRC train: 0.777077	val: 0.370281	test: 0.366729

Epoch: 78
Loss: 0.12275496999724046
ROC train: 0.963414	val: 0.771024	test: 0.740990
PRC train: 0.792742	val: 0.374885	test: 0.371289

Epoch: 79
Loss: 0.12270423781916609
ROC train: 0.962218	val: 0.783021	test: 0.748646
PRC train: 0.785570	val: 0.408913	test: 0.376265

Epoch: 80
Loss: 0.12266633772667954
ROC train: 0.961203	val: 0.776663	test: 0.742547
PRC train: 0.777904	val: 0.370565	test: 0.358088

Epoch: 81
Loss: 0.12078919687792741
ROC train: 0.962577	val: 0.765561	test: 0.745614
PRC train: 0.785395	val: 0.368275	test: 0.362997

Epoch: 82
Loss: 0.12237133597545524
ROC train: 0.963646	val: 0.771778	test: 0.746211
PRC train: 0.786377	val: 0.370268	test: 0.367028

Epoch: 83
Loss: 0.12216687387063897
ROC train: 0.963520	val: 0.784437	test: 0.750110
PRC train: 0.790544	val: 0.375006	test: 0.375792

Epoch: 84
Loss: 0.12195927707334081
ROC train: 0.965656	val: 0.779474	test: 0.747105
PRC train: 0.797366	val: 0.386035	test: 0.368869

Epoch: 85
Loss: 0.11990413826734865
ROC train: 0.967013	val: 0.773189	test: 0.745484
PRC train: 0.804595	val: 0.373632	test: 0.373529

Epoch: 86
Loss: 0.11872555548479279
ROC train: 0.966225	val: 0.777622	test: 0.746944
PRC train: 0.802183	val: 0.377420	test: 0.371141

Epoch: 87
Loss: 0.11681819370367842
ROC train: 0.969646	val: 0.771619	test: 0.739599
PRC train: 0.809651	val: 0.374801	test: 0.358057

Epoch: 88
Loss: 0.11881685734934826
ROC train: 0.967079	val: 0.771528	test: 0.736430
PRC train: 0.807595	val: 0.363328	test: 0.353683

Epoch: 89
Loss: 0.11759795695224037
ROC train: 0.969757	val: 0.773629	test: 0.740061
PRC train: 0.816069	val: 0.379122	test: 0.360076

Epoch: 90
Loss: 0.1163768343425053
ROC train: 0.969582	val: 0.772799	test: 0.744740
PRC train: 0.820573	val: 0.380995	test: 0.363787

Epoch: 91
Loss: 0.1163299160557051
ROC train: 0.971105	val: 0.764593	test: 0.732935
PRC train: 0.824738	val: 0.379870	test: 0.362597

Epoch: 92
Loss: 0.11409370932802868
ROC train: 0.971246	val: 0.774064	test: 0.742990
PRC train: 0.819959	val: 0.368745	test: 0.357503

Epoch: 93
Loss: 0.11457669247411538
ROC train: 0.971003	val: 0.784542	test: 0.742739
PRC train: 0.826434	val: 0.372731	test: 0.357455

Epoch: 94
Loss: 0.11521196034554736
ROC train: 0.911086	val: 0.781255	test: 0.728508
PRC train: 0.606166	val: 0.383616	test: 0.342192

Epoch: 34
Loss: 0.15441132785952075
ROC train: 0.915283	val: 0.786692	test: 0.735607
PRC train: 0.616677	val: 0.381924	test: 0.347644

Epoch: 35
Loss: 0.15464890557182712
ROC train: 0.913508	val: 0.787312	test: 0.734649
PRC train: 0.614171	val: 0.376768	test: 0.346441

Epoch: 36
Loss: 0.15344809264271994
ROC train: 0.918437	val: 0.784283	test: 0.740285
PRC train: 0.619913	val: 0.383810	test: 0.350448

Epoch: 37
Loss: 0.1495954991334722
ROC train: 0.918953	val: 0.783481	test: 0.734491
PRC train: 0.631664	val: 0.387553	test: 0.344920

Epoch: 38
Loss: 0.15193393055371357
ROC train: 0.923163	val: 0.782151	test: 0.746913
PRC train: 0.643451	val: 0.388859	test: 0.353416

Epoch: 39
Loss: 0.150944536436012
ROC train: 0.919414	val: 0.777681	test: 0.750863
PRC train: 0.636011	val: 0.394048	test: 0.358583

Epoch: 40
Loss: 0.14828216677068265
ROC train: 0.923745	val: 0.784812	test: 0.739304
PRC train: 0.643676	val: 0.375280	test: 0.353270

Epoch: 41
Loss: 0.14827662702902125
ROC train: 0.923982	val: 0.785595	test: 0.746388
PRC train: 0.641006	val: 0.378370	test: 0.352201

Epoch: 42
Loss: 0.14648381219992979
ROC train: 0.926509	val: 0.776437	test: 0.740278
PRC train: 0.650335	val: 0.386041	test: 0.350313

Epoch: 43
Loss: 0.1471624851265819
ROC train: 0.928418	val: 0.777028	test: 0.732131
PRC train: 0.654353	val: 0.380065	test: 0.352215

Epoch: 44
Loss: 0.14541558208299357
ROC train: 0.929854	val: 0.780032	test: 0.736904
PRC train: 0.667141	val: 0.388504	test: 0.349025

Epoch: 45
Loss: 0.14389922852757953
ROC train: 0.931089	val: 0.780732	test: 0.740933
PRC train: 0.660767	val: 0.376090	test: 0.347317

Epoch: 46
Loss: 0.14431837070739562
ROC train: 0.931747	val: 0.769604	test: 0.740107
PRC train: 0.667394	val: 0.380027	test: 0.348134

Epoch: 47
Loss: 0.14294315138219774
ROC train: 0.934817	val: 0.762834	test: 0.739035
PRC train: 0.681034	val: 0.384456	test: 0.363354

Epoch: 48
Loss: 0.14185062499107365
ROC train: 0.935551	val: 0.766937	test: 0.736694
PRC train: 0.673958	val: 0.375813	test: 0.355152

Epoch: 49
Loss: 0.14271902379983956
ROC train: 0.935606	val: 0.773793	test: 0.748814
PRC train: 0.676915	val: 0.377573	test: 0.355711

Epoch: 50
Loss: 0.14268804712054178
ROC train: 0.934321	val: 0.785468	test: 0.746166
PRC train: 0.679638	val: 0.399131	test: 0.364579

Epoch: 51
Loss: 0.1421447935060416
ROC train: 0.936931	val: 0.782338	test: 0.748346
PRC train: 0.679545	val: 0.384195	test: 0.361496

Epoch: 52
Loss: 0.14244267392176713
ROC train: 0.939829	val: 0.778537	test: 0.745075
PRC train: 0.694722	val: 0.388478	test: 0.361792

Epoch: 53
Loss: 0.13849044940009195
ROC train: 0.936973	val: 0.761863	test: 0.741508
PRC train: 0.675362	val: 0.345278	test: 0.337112

Epoch: 54
Loss: 0.13949481723871612
ROC train: 0.939856	val: 0.773222	test: 0.738897
PRC train: 0.692228	val: 0.378559	test: 0.355358

Epoch: 55
Loss: 0.13933106579011764
ROC train: 0.941001	val: 0.788498	test: 0.753510
PRC train: 0.695427	val: 0.414244	test: 0.365888

Epoch: 56
Loss: 0.13969263500220092
ROC train: 0.944664	val: 0.770977	test: 0.745340
PRC train: 0.718580	val: 0.396157	test: 0.364662

Epoch: 57
Loss: 0.13693013545494748
ROC train: 0.945517	val: 0.790509	test: 0.740576
PRC train: 0.711915	val: 0.406638	test: 0.352620

Epoch: 58
Loss: 0.13880450170909817
ROC train: 0.945297	val: 0.780805	test: 0.747689
PRC train: 0.716169	val: 0.407978	test: 0.367446

Epoch: 59
Loss: 0.13546260816733685
ROC train: 0.947435	val: 0.759033	test: 0.737960
PRC train: 0.726149	val: 0.373974	test: 0.351062

Epoch: 60
Loss: 0.13597535146146858
ROC train: 0.947837	val: 0.779699	test: 0.744024
PRC train: 0.726073	val: 0.393306	test: 0.372930

Epoch: 61
Loss: 0.13390193238499987
ROC train: 0.945731	val: 0.780984	test: 0.743854
PRC train: 0.714505	val: 0.383625	test: 0.356110

Epoch: 62
Loss: 0.13306155367758885
ROC train: 0.948172	val: 0.773254	test: 0.754080
PRC train: 0.724796	val: 0.384962	test: 0.366874

Epoch: 63
Loss: 0.1332883547014868
ROC train: 0.948018	val: 0.765480	test: 0.739119
PRC train: 0.723520	val: 0.381975	test: 0.346545

Epoch: 64
Loss: 0.13174412572622785
ROC train: 0.949451	val: 0.779647	test: 0.752018
PRC train: 0.730346	val: 0.398586	test: 0.362461

Epoch: 65
Loss: 0.13260855111890024
ROC train: 0.950428	val: 0.776388	test: 0.750432
PRC train: 0.733702	val: 0.376308	test: 0.358476

Epoch: 66
Loss: 0.12982527569351116
ROC train: 0.950013	val: 0.774047	test: 0.749816
PRC train: 0.730339	val: 0.377316	test: 0.350292

Epoch: 67
Loss: 0.1312880338084205
ROC train: 0.955152	val: 0.775487	test: 0.743382
PRC train: 0.750817	val: 0.389046	test: 0.353632

Epoch: 68
Loss: 0.12928306534362036
ROC train: 0.954059	val: 0.779815	test: 0.747008
PRC train: 0.751030	val: 0.392036	test: 0.359690

Epoch: 69
Loss: 0.1297836075252981
ROC train: 0.955707	val: 0.769203	test: 0.739431
PRC train: 0.751682	val: 0.383806	test: 0.356193

Epoch: 70
Loss: 0.12784635022246998
ROC train: 0.953642	val: 0.772638	test: 0.745629
PRC train: 0.750831	val: 0.374846	test: 0.361336

Epoch: 71
Loss: 0.1264835716414863
ROC train: 0.957189	val: 0.767713	test: 0.744370
PRC train: 0.764243	val: 0.372958	test: 0.355158

Epoch: 72
Loss: 0.1263613408467391
ROC train: 0.959033	val: 0.768001	test: 0.746601
PRC train: 0.771824	val: 0.385150	test: 0.364628

Epoch: 73
Loss: 0.12664334408796393
ROC train: 0.957621	val: 0.780639	test: 0.745894
PRC train: 0.765114	val: 0.389099	test: 0.359832

Epoch: 74
Loss: 0.1258429438996636
ROC train: 0.958918	val: 0.768573	test: 0.742054
PRC train: 0.769081	val: 0.371270	test: 0.356377

Epoch: 75
Loss: 0.1261359767114557
ROC train: 0.957810	val: 0.777224	test: 0.740817
PRC train: 0.767202	val: 0.391470	test: 0.360749

Epoch: 76
Loss: 0.12487033726582648
ROC train: 0.956421	val: 0.766729	test: 0.739998
PRC train: 0.759539	val: 0.383572	test: 0.360041

Epoch: 77
Loss: 0.12362663871516626
ROC train: 0.962246	val: 0.768039	test: 0.744847
PRC train: 0.782576	val: 0.383477	test: 0.356924

Epoch: 78
Loss: 0.1220359153361441
ROC train: 0.960411	val: 0.770245	test: 0.746656
PRC train: 0.768884	val: 0.386320	test: 0.358098

Epoch: 79
Loss: 0.12283792667720757
ROC train: 0.962992	val: 0.769634	test: 0.744693
PRC train: 0.790153	val: 0.390184	test: 0.364515

Epoch: 80
Loss: 0.12224585371437031
ROC train: 0.963038	val: 0.768178	test: 0.744048
PRC train: 0.785147	val: 0.384470	test: 0.362794

Epoch: 81
Loss: 0.1217677557310607
ROC train: 0.963351	val: 0.771126	test: 0.742840
PRC train: 0.790306	val: 0.389220	test: 0.359894

Epoch: 82
Loss: 0.12095350541464772
ROC train: 0.965634	val: 0.776074	test: 0.745585
PRC train: 0.797468	val: 0.403133	test: 0.369422

Epoch: 83
Loss: 0.11928249809974016
ROC train: 0.962588	val: 0.771278	test: 0.752898
PRC train: 0.783234	val: 0.382367	test: 0.367074

Epoch: 84
Loss: 0.11971374979467544
ROC train: 0.965307	val: 0.775347	test: 0.746431
PRC train: 0.798702	val: 0.401369	test: 0.356550

Epoch: 85
Loss: 0.11940871949337549
ROC train: 0.967631	val: 0.777428	test: 0.748948
PRC train: 0.804028	val: 0.403057	test: 0.363760

Epoch: 86
Loss: 0.12009313521741698
ROC train: 0.968211	val: 0.776806	test: 0.741931
PRC train: 0.806065	val: 0.393047	test: 0.366463

Epoch: 87
Loss: 0.1186703466747832
ROC train: 0.967206	val: 0.768934	test: 0.738871
PRC train: 0.804440	val: 0.396774	test: 0.370682

Epoch: 88
Loss: 0.11880560547333802
ROC train: 0.968912	val: 0.764561	test: 0.740030
PRC train: 0.818139	val: 0.388066	test: 0.353287

Epoch: 89
Loss: 0.11733447241423665
ROC train: 0.968318	val: 0.769367	test: 0.730365
PRC train: 0.811294	val: 0.381574	test: 0.337977

Epoch: 90
Loss: 0.11549165036680152
ROC train: 0.969153	val: 0.758762	test: 0.744355
PRC train: 0.813407	val: 0.374414	test: 0.351611

Epoch: 91
Loss: 0.11643241846950074
ROC train: 0.970928	val: 0.770070	test: 0.744670
PRC train: 0.824110	val: 0.389541	test: 0.351623

Epoch: 92
Loss: 0.11399918109678671
ROC train: 0.971203	val: 0.768189	test: 0.743512
PRC train: 0.823511	val: 0.374206	test: 0.355470

Epoch: 93
Loss: 0.11348836399683616
ROC train: 0.971796	val: 0.772234	test: 0.743222
PRC train: 0.828815	val: 0.395060	test: 0.355587

Epoch: 94
Loss: 0.11112883830151281
ROC train: 0.914108	val: 0.780445	test: 0.732919
PRC train: 0.606966	val: 0.376244	test: 0.346699

Epoch: 34
Loss: 0.15459498937069252
ROC train: 0.915279	val: 0.773844	test: 0.750797
PRC train: 0.617155	val: 0.366277	test: 0.349694

Epoch: 35
Loss: 0.1519052225419986
ROC train: 0.915875	val: 0.779596	test: 0.735585
PRC train: 0.614924	val: 0.369215	test: 0.342317

Epoch: 36
Loss: 0.1526603858417397
ROC train: 0.916881	val: 0.777106	test: 0.731806
PRC train: 0.622426	val: 0.366837	test: 0.343809

Epoch: 37
Loss: 0.15131669148095628
ROC train: 0.918795	val: 0.774496	test: 0.738123
PRC train: 0.627940	val: 0.368638	test: 0.354921

Epoch: 38
Loss: 0.14992410691367342
ROC train: 0.920924	val: 0.774676	test: 0.747262
PRC train: 0.634963	val: 0.375380	test: 0.351817

Epoch: 39
Loss: 0.1496850120205783
ROC train: 0.922041	val: 0.772775	test: 0.745926
PRC train: 0.635956	val: 0.366212	test: 0.345282

Epoch: 40
Loss: 0.14949660933748352
ROC train: 0.924766	val: 0.769274	test: 0.741257
PRC train: 0.652602	val: 0.376264	test: 0.357454

Epoch: 41
Loss: 0.14962260056626583
ROC train: 0.923092	val: 0.770196	test: 0.740849
PRC train: 0.641798	val: 0.364679	test: 0.354907

Epoch: 42
Loss: 0.1489248736689167
ROC train: 0.927826	val: 0.774527	test: 0.741501
PRC train: 0.655503	val: 0.372571	test: 0.355962

Epoch: 43
Loss: 0.147298159321656
ROC train: 0.928019	val: 0.774983	test: 0.745494
PRC train: 0.655055	val: 0.373029	test: 0.361919

Epoch: 44
Loss: 0.14642487280523414
ROC train: 0.929081	val: 0.772903	test: 0.743982
PRC train: 0.657216	val: 0.371513	test: 0.354731

Epoch: 45
Loss: 0.14430151550721246
ROC train: 0.929056	val: 0.765816	test: 0.742171
PRC train: 0.661117	val: 0.363675	test: 0.341566

Epoch: 46
Loss: 0.1452401504426753
ROC train: 0.929654	val: 0.768292	test: 0.740784
PRC train: 0.670157	val: 0.383842	test: 0.359469

Epoch: 47
Loss: 0.1446587522916737
ROC train: 0.930721	val: 0.766896	test: 0.729015
PRC train: 0.658423	val: 0.368619	test: 0.342219

Epoch: 48
Loss: 0.141989407954684
ROC train: 0.935029	val: 0.761878	test: 0.731602
PRC train: 0.678984	val: 0.365660	test: 0.344624

Epoch: 49
Loss: 0.1428665045743324
ROC train: 0.935428	val: 0.766928	test: 0.738919
PRC train: 0.677756	val: 0.365214	test: 0.333568

Epoch: 50
Loss: 0.14211787798866501
ROC train: 0.936073	val: 0.777286	test: 0.734274
PRC train: 0.681215	val: 0.363151	test: 0.340551

Epoch: 51
Loss: 0.14173678822170693
ROC train: 0.938591	val: 0.774267	test: 0.746689
PRC train: 0.693724	val: 0.390058	test: 0.358890

Epoch: 52
Loss: 0.14019248360958714
ROC train: 0.934223	val: 0.758723	test: 0.741091
PRC train: 0.663722	val: 0.356295	test: 0.347443

Epoch: 53
Loss: 0.1384140436640982
ROC train: 0.939462	val: 0.772203	test: 0.743734
PRC train: 0.693862	val: 0.369236	test: 0.360397

Epoch: 54
Loss: 0.14044859795768797
ROC train: 0.940682	val: 0.768392	test: 0.735989
PRC train: 0.697765	val: 0.374136	test: 0.347468

Epoch: 55
Loss: 0.1393642460410633
ROC train: 0.938549	val: 0.770600	test: 0.739357
PRC train: 0.683230	val: 0.360870	test: 0.340241

Epoch: 56
Loss: 0.13910694519485844
ROC train: 0.942746	val: 0.777176	test: 0.740757
PRC train: 0.701525	val: 0.380964	test: 0.357197

Epoch: 57
Loss: 0.13619157516323932
ROC train: 0.941800	val: 0.768228	test: 0.732517
PRC train: 0.695342	val: 0.374168	test: 0.339728

Epoch: 58
Loss: 0.13660426854528213
ROC train: 0.945279	val: 0.775240	test: 0.739200
PRC train: 0.713112	val: 0.375761	test: 0.353271

Epoch: 59
Loss: 0.13781358082729528
ROC train: 0.945784	val: 0.772273	test: 0.734782
PRC train: 0.718650	val: 0.385751	test: 0.351502

Epoch: 60
Loss: 0.13563389719720323
ROC train: 0.947627	val: 0.770427	test: 0.741556
PRC train: 0.722009	val: 0.373331	test: 0.367445

Epoch: 61
Loss: 0.1349137043375006
ROC train: 0.948050	val: 0.768079	test: 0.737126
PRC train: 0.722662	val: 0.367113	test: 0.357758

Epoch: 62
Loss: 0.13372635941596026
ROC train: 0.948289	val: 0.775177	test: 0.738945
PRC train: 0.720969	val: 0.382975	test: 0.352531

Epoch: 63
Loss: 0.13344843816213026
ROC train: 0.949737	val: 0.777663	test: 0.741219
PRC train: 0.730751	val: 0.376195	test: 0.347876

Epoch: 64
Loss: 0.13269892813990844
ROC train: 0.950508	val: 0.775211	test: 0.741837
PRC train: 0.730282	val: 0.370818	test: 0.349341

Epoch: 65
Loss: 0.13266815720811997
ROC train: 0.951295	val: 0.772831	test: 0.750178
PRC train: 0.733604	val: 0.374248	test: 0.355345

Epoch: 66
Loss: 0.13335903310434116
ROC train: 0.952591	val: 0.776287	test: 0.743731
PRC train: 0.743775	val: 0.381446	test: 0.355304

Epoch: 67
Loss: 0.1310909576536069
ROC train: 0.950318	val: 0.771626	test: 0.742597
PRC train: 0.737296	val: 0.362805	test: 0.355663

Epoch: 68
Loss: 0.13269347864056352
ROC train: 0.953380	val: 0.767157	test: 0.737805
PRC train: 0.741525	val: 0.359728	test: 0.343328

Epoch: 69
Loss: 0.12968223573057502
ROC train: 0.952918	val: 0.777642	test: 0.747027
PRC train: 0.743011	val: 0.372034	test: 0.355545

Epoch: 70
Loss: 0.1295867731335274
ROC train: 0.954065	val: 0.773192	test: 0.743016
PRC train: 0.743956	val: 0.366240	test: 0.336847

Epoch: 71
Loss: 0.128203120315175
ROC train: 0.956201	val: 0.768814	test: 0.741754
PRC train: 0.757598	val: 0.362539	test: 0.341490

Epoch: 72
Loss: 0.12669417358252008
ROC train: 0.956826	val: 0.778448	test: 0.741376
PRC train: 0.765579	val: 0.373011	test: 0.337439

Epoch: 73
Loss: 0.12690745753414764
ROC train: 0.958488	val: 0.777624	test: 0.747820
PRC train: 0.763991	val: 0.369131	test: 0.360509

Epoch: 74
Loss: 0.12679293608854647
ROC train: 0.957627	val: 0.776737	test: 0.742870
PRC train: 0.761894	val: 0.377177	test: 0.351723

Epoch: 75
Loss: 0.12615494221556223
ROC train: 0.960089	val: 0.778657	test: 0.743799
PRC train: 0.777990	val: 0.369571	test: 0.338190

Epoch: 76
Loss: 0.12430726432236724
ROC train: 0.959074	val: 0.763824	test: 0.743496
PRC train: 0.772028	val: 0.368184	test: 0.359681

Epoch: 77
Loss: 0.12520769667768472
ROC train: 0.961714	val: 0.771222	test: 0.742983
PRC train: 0.779636	val: 0.380491	test: 0.358237

Epoch: 78
Loss: 0.12418939703406286
ROC train: 0.961175	val: 0.753656	test: 0.738230
PRC train: 0.775967	val: 0.354267	test: 0.338818

Epoch: 79
Loss: 0.12259551631099941
ROC train: 0.963240	val: 0.773026	test: 0.755180
PRC train: 0.787907	val: 0.375934	test: 0.358264

Epoch: 80
Loss: 0.12297411217569869
ROC train: 0.961768	val: 0.776295	test: 0.747470
PRC train: 0.780416	val: 0.376117	test: 0.366216

Epoch: 81
Loss: 0.12282642783198217
ROC train: 0.963631	val: 0.778389	test: 0.745370
PRC train: 0.790826	val: 0.367744	test: 0.344148

Epoch: 82
Loss: 0.12158110341332629
ROC train: 0.963612	val: 0.777821	test: 0.741244
PRC train: 0.790051	val: 0.376280	test: 0.353839

Epoch: 83
Loss: 0.12128977945169832
ROC train: 0.966181	val: 0.779147	test: 0.733948
PRC train: 0.801636	val: 0.377851	test: 0.346313

Epoch: 84
Loss: 0.12070061624843482
ROC train: 0.962522	val: 0.775123	test: 0.735455
PRC train: 0.778916	val: 0.364405	test: 0.340709

Epoch: 85
Loss: 0.1213103314260192
ROC train: 0.965337	val: 0.776256	test: 0.739696
PRC train: 0.797056	val: 0.378006	test: 0.338468

Epoch: 86
Loss: 0.12033868174266157
ROC train: 0.967175	val: 0.775231	test: 0.739923
PRC train: 0.803740	val: 0.366261	test: 0.335668

Epoch: 87
Loss: 0.11871103886118288
ROC train: 0.966403	val: 0.776482	test: 0.744292
PRC train: 0.798996	val: 0.384812	test: 0.355309

Epoch: 88
Loss: 0.11731845874128037
ROC train: 0.964629	val: 0.775593	test: 0.739761
PRC train: 0.792953	val: 0.367491	test: 0.329223

Epoch: 89
Loss: 0.1184328463267018
ROC train: 0.967169	val: 0.780755	test: 0.743474
PRC train: 0.803901	val: 0.375403	test: 0.339444

Epoch: 90
Loss: 0.11834622445895018
ROC train: 0.968118	val: 0.776053	test: 0.745878
PRC train: 0.813413	val: 0.367300	test: 0.342517

Epoch: 91
Loss: 0.11777156570740148
ROC train: 0.970242	val: 0.769333	test: 0.742975
PRC train: 0.821251	val: 0.359807	test: 0.342103

Epoch: 92
Loss: 0.11647166033817408
ROC train: 0.968641	val: 0.773634	test: 0.730408
PRC train: 0.811574	val: 0.354279	test: 0.330047

Epoch: 93
Loss: 0.11592226623697595
ROC train: 0.971317	val: 0.773779	test: 0.735155
PRC train: 0.827038	val: 0.363370	test: 0.334996

Epoch: 94
Loss: 0.11468533812691988
ROC train: 0.973237	val: 0.736609	test: 0.719689
PRC train: 0.821212	val: 0.293716	test: 0.262829

Epoch: 95
Loss: 0.10003189407814128
ROC train: 0.972347	val: 0.720657	test: 0.701298
PRC train: 0.821006	val: 0.273357	test: 0.247538

Epoch: 96
Loss: 0.09830385157905526
ROC train: 0.974230	val: 0.726904	test: 0.708951
PRC train: 0.827259	val: 0.286008	test: 0.261606

Epoch: 97
Loss: 0.09829675045833365
ROC train: 0.973377	val: 0.726416	test: 0.702258
PRC train: 0.821042	val: 0.282974	test: 0.253530

Epoch: 98
Loss: 0.1004660575522502
ROC train: 0.974543	val: 0.722312	test: 0.709231
PRC train: 0.829083	val: 0.261836	test: 0.242421

Epoch: 99
Loss: 0.1005413100977786
ROC train: 0.974105	val: 0.715007	test: 0.699023
PRC train: 0.825047	val: 0.272129	test: 0.249773

Epoch: 100
Loss: 0.09809802635487502
ROC train: 0.975554	val: 0.729292	test: 0.716484
PRC train: 0.835708	val: 0.282069	test: 0.263405

Epoch: 101
Loss: 0.09618820473374468
ROC train: 0.975733	val: 0.725453	test: 0.705854
PRC train: 0.832785	val: 0.283746	test: 0.251732

Epoch: 102
Loss: 0.095904405445166
ROC train: 0.975201	val: 0.723105	test: 0.709081
PRC train: 0.834999	val: 0.267803	test: 0.250270

Epoch: 103
Loss: 0.09618591472603595
ROC train: 0.976731	val: 0.713896	test: 0.700950
PRC train: 0.836090	val: 0.272923	test: 0.254260

Epoch: 104
Loss: 0.09568140629237906
ROC train: 0.976553	val: 0.717637	test: 0.699958
PRC train: 0.837373	val: 0.274648	test: 0.251863

Epoch: 105
Loss: 0.09461438711500501
ROC train: 0.977154	val: 0.731074	test: 0.707640
PRC train: 0.839283	val: 0.278295	test: 0.248816

Epoch: 106
Loss: 0.09460243185274488
ROC train: 0.976694	val: 0.720935	test: 0.693797
PRC train: 0.833503	val: 0.290529	test: 0.256829

Epoch: 107
Loss: 0.09270128068720482
ROC train: 0.977335	val: 0.731999	test: 0.710664
PRC train: 0.840207	val: 0.278511	test: 0.255892

Epoch: 108
Loss: 0.09377356620241949
ROC train: 0.978102	val: 0.725761	test: 0.707004
PRC train: 0.843708	val: 0.302956	test: 0.270873

Epoch: 109
Loss: 0.09215842213259433
ROC train: 0.977184	val: 0.727579	test: 0.705629
PRC train: 0.840305	val: 0.291447	test: 0.262242

Epoch: 110
Loss: 0.09289088188358653
ROC train: 0.977018	val: 0.720365	test: 0.700213
PRC train: 0.835574	val: 0.276297	test: 0.248872

Epoch: 111
Loss: 0.09280158557404357
ROC train: 0.979734	val: 0.731190	test: 0.716248
PRC train: 0.854619	val: 0.285330	test: 0.264909

Epoch: 112
Loss: 0.09339691281271383
ROC train: 0.979345	val: 0.718435	test: 0.699727
PRC train: 0.851102	val: 0.278113	test: 0.249448

Epoch: 113
Loss: 0.09014803095185688
ROC train: 0.980097	val: 0.730322	test: 0.709559
PRC train: 0.857971	val: 0.287980	test: 0.261764

Epoch: 114
Loss: 0.0917384643707186
ROC train: 0.981584	val: 0.726702	test: 0.708494
PRC train: 0.865712	val: 0.277389	test: 0.250585

Epoch: 115
Loss: 0.09102282783738355
ROC train: 0.980359	val: 0.735376	test: 0.718360
PRC train: 0.860197	val: 0.293610	test: 0.263141

Epoch: 116
Loss: 0.09288087705613793
ROC train: 0.982155	val: 0.717626	test: 0.700383
PRC train: 0.866523	val: 0.263197	test: 0.239812

Epoch: 117
Loss: 0.09252641617939095
ROC train: 0.982135	val: 0.728650	test: 0.710080
PRC train: 0.866795	val: 0.291743	test: 0.257920

Epoch: 118
Loss: 0.08981204978319471
ROC train: 0.983000	val: 0.716266	test: 0.701040
PRC train: 0.868671	val: 0.258666	test: 0.237950

Epoch: 119
Loss: 0.09060661402658338
ROC train: 0.980667	val: 0.722867	test: 0.701996
PRC train: 0.858615	val: 0.257433	test: 0.236945

Epoch: 120
Loss: 0.08904933859431692
ROC train: 0.982988	val: 0.719678	test: 0.707654
PRC train: 0.870755	val: 0.277651	test: 0.252450

Early stopping
Best (ROC):	 train: 0.911112	val: 0.760487	test: 0.730347
Best (PRC):	 train: 0.599006	val: 0.314340	test: 0.274848

ROC train: 0.972086	val: 0.728665	test: 0.713242
PRC train: 0.820108	val: 0.299950	test: 0.283007

Epoch: 95
Loss: 0.10007407365262244
ROC train: 0.971059	val: 0.725521	test: 0.708580
PRC train: 0.808613	val: 0.295567	test: 0.282147

Epoch: 96
Loss: 0.10173140284084367
ROC train: 0.973728	val: 0.731275	test: 0.707334
PRC train: 0.824574	val: 0.284156	test: 0.268004

Epoch: 97
Loss: 0.09889939879860961
ROC train: 0.974908	val: 0.716521	test: 0.694788
PRC train: 0.821736	val: 0.279233	test: 0.262126

Epoch: 98
Loss: 0.09959070410175248
ROC train: 0.974859	val: 0.726638	test: 0.705134
PRC train: 0.833194	val: 0.285216	test: 0.264743

Epoch: 99
Loss: 0.09801997832529206
ROC train: 0.974031	val: 0.725659	test: 0.704532
PRC train: 0.826471	val: 0.294025	test: 0.275503

Epoch: 100
Loss: 0.09856587695629612
ROC train: 0.974565	val: 0.734420	test: 0.709863
PRC train: 0.829766	val: 0.293356	test: 0.271100

Epoch: 101
Loss: 0.09615205421015781
ROC train: 0.976588	val: 0.725506	test: 0.706270
PRC train: 0.839574	val: 0.280484	test: 0.263866

Epoch: 102
Loss: 0.09672359347118334
ROC train: 0.976687	val: 0.724184	test: 0.703514
PRC train: 0.834520	val: 0.293910	test: 0.268563

Epoch: 103
Loss: 0.09587809124759543
ROC train: 0.976384	val: 0.723617	test: 0.702362
PRC train: 0.834334	val: 0.280818	test: 0.264763

Epoch: 104
Loss: 0.09436896690775842
ROC train: 0.976479	val: 0.724288	test: 0.710687
PRC train: 0.835618	val: 0.316042	test: 0.285457

Epoch: 105
Loss: 0.09943464106753148
ROC train: 0.975237	val: 0.725808	test: 0.707386
PRC train: 0.827986	val: 0.300964	test: 0.285675

Epoch: 106
Loss: 0.09915140839412144
ROC train: 0.976720	val: 0.715077	test: 0.703833
PRC train: 0.837666	val: 0.285388	test: 0.265403

Epoch: 107
Loss: 0.09607056296175187
ROC train: 0.975593	val: 0.722262	test: 0.708634
PRC train: 0.830780	val: 0.296967	test: 0.271729

Epoch: 108
Loss: 0.09515590878951234
ROC train: 0.978388	val: 0.722980	test: 0.709359
PRC train: 0.845680	val: 0.308950	test: 0.290309

Epoch: 109
Loss: 0.0951677512050348
ROC train: 0.978560	val: 0.711441	test: 0.695521
PRC train: 0.848222	val: 0.273780	test: 0.260691

Epoch: 110
Loss: 0.09218500961047074
ROC train: 0.979381	val: 0.716212	test: 0.704550
PRC train: 0.855167	val: 0.285222	test: 0.280268

Epoch: 111
Loss: 0.0925612921909393
ROC train: 0.978599	val: 0.725626	test: 0.707599
PRC train: 0.851016	val: 0.301872	test: 0.273945

Epoch: 112
Loss: 0.09255136185533917
ROC train: 0.979824	val: 0.731237	test: 0.708951
PRC train: 0.856796	val: 0.303226	test: 0.285574

Epoch: 113
Loss: 0.09277316723413974
ROC train: 0.979995	val: 0.722746	test: 0.702519
PRC train: 0.857873	val: 0.282727	test: 0.266417

Epoch: 114
Loss: 0.09183250916492035
ROC train: 0.980770	val: 0.725450	test: 0.705673
PRC train: 0.862374	val: 0.297328	test: 0.279167

Epoch: 115
Loss: 0.0915346892205985
ROC train: 0.979855	val: 0.724365	test: 0.711447
PRC train: 0.857586	val: 0.289803	test: 0.269033

Epoch: 116
Loss: 0.09135624869939282
ROC train: 0.981363	val: 0.728611	test: 0.715092
PRC train: 0.862780	val: 0.301570	test: 0.285608

Epoch: 117
Loss: 0.09039994587043028
ROC train: 0.981490	val: 0.723489	test: 0.711121
PRC train: 0.862023	val: 0.294438	test: 0.275476

Epoch: 118
Loss: 0.08898245883928672
ROC train: 0.981171	val: 0.725326	test: 0.702852
PRC train: 0.861176	val: 0.288366	test: 0.269122

Epoch: 119
Loss: 0.09197914435546768
ROC train: 0.982094	val: 0.723440	test: 0.706774
PRC train: 0.869070	val: 0.293652	test: 0.279332

Epoch: 120
Loss: 0.08950129724315448
ROC train: 0.983157	val: 0.724254	test: 0.701048
PRC train: 0.874290	val: 0.299088	test: 0.272625

Early stopping
Best (ROC):	 train: 0.914811	val: 0.755457	test: 0.725196
Best (PRC):	 train: 0.607982	val: 0.328223	test: 0.282355

ROC train: 0.970661	val: 0.712219	test: 0.700797
PRC train: 0.811195	val: 0.272908	test: 0.254321

Epoch: 95
Loss: 0.09844393659046578
ROC train: 0.973696	val: 0.725260	test: 0.710807
PRC train: 0.825109	val: 0.280609	test: 0.258550

Epoch: 96
Loss: 0.09876193387560292
ROC train: 0.973569	val: 0.724730	test: 0.712156
PRC train: 0.819704	val: 0.299845	test: 0.276132

Epoch: 97
Loss: 0.10033551244690322
ROC train: 0.972479	val: 0.720850	test: 0.699665
PRC train: 0.814448	val: 0.287181	test: 0.261916

Epoch: 98
Loss: 0.10084813970880524
ROC train: 0.972895	val: 0.723110	test: 0.713475
PRC train: 0.813192	val: 0.298615	test: 0.278599

Epoch: 99
Loss: 0.09718645050515272
ROC train: 0.974313	val: 0.726249	test: 0.713704
PRC train: 0.825489	val: 0.301366	test: 0.269351

Epoch: 100
Loss: 0.09706954785354549
ROC train: 0.975751	val: 0.717155	test: 0.708463
PRC train: 0.830375	val: 0.285212	test: 0.268234

Epoch: 101
Loss: 0.09678022668441817
ROC train: 0.976740	val: 0.722636	test: 0.709020
PRC train: 0.838712	val: 0.273251	test: 0.257615

Epoch: 102
Loss: 0.0979869839585653
ROC train: 0.976867	val: 0.721293	test: 0.714304
PRC train: 0.839847	val: 0.273582	test: 0.258378

Epoch: 103
Loss: 0.09712344103521188
ROC train: 0.977498	val: 0.719436	test: 0.707919
PRC train: 0.838389	val: 0.285067	test: 0.274568

Epoch: 104
Loss: 0.09725924166030012
ROC train: 0.977564	val: 0.725871	test: 0.708913
PRC train: 0.841750	val: 0.282332	test: 0.263958

Epoch: 105
Loss: 0.09400568154907162
ROC train: 0.977383	val: 0.722421	test: 0.711037
PRC train: 0.843570	val: 0.286583	test: 0.270425

Epoch: 106
Loss: 0.09530762505713233
ROC train: 0.978113	val: 0.717344	test: 0.706492
PRC train: 0.841104	val: 0.280931	test: 0.265289

Epoch: 107
Loss: 0.09479501292228791
ROC train: 0.978342	val: 0.702543	test: 0.697758
PRC train: 0.843607	val: 0.272745	test: 0.255225

Epoch: 108
Loss: 0.09637629764589571
ROC train: 0.978745	val: 0.722893	test: 0.709472
PRC train: 0.849117	val: 0.275293	test: 0.263369

Epoch: 109
Loss: 0.09397075467601952
ROC train: 0.979070	val: 0.718425	test: 0.703335
PRC train: 0.853417	val: 0.281443	test: 0.260547

Epoch: 110
Loss: 0.09338740243504431
ROC train: 0.979052	val: 0.721805	test: 0.711758
PRC train: 0.851985	val: 0.269144	test: 0.252617

Epoch: 111
Loss: 0.09256331660053707
ROC train: 0.979575	val: 0.710670	test: 0.703525
PRC train: 0.851191	val: 0.276447	test: 0.261424

Epoch: 112
Loss: 0.09384864454292707
ROC train: 0.980507	val: 0.713418	test: 0.702174
PRC train: 0.856607	val: 0.276516	test: 0.251629

Epoch: 113
Loss: 0.09206832891683785
ROC train: 0.980264	val: 0.716834	test: 0.711692
PRC train: 0.851366	val: 0.280806	test: 0.267849

Epoch: 114
Loss: 0.09252026713737778
ROC train: 0.981241	val: 0.710948	test: 0.699107
PRC train: 0.859660	val: 0.269651	test: 0.242875

Epoch: 115
Loss: 0.09054637848652805
ROC train: 0.981117	val: 0.719812	test: 0.710950
PRC train: 0.860330	val: 0.277761	test: 0.262257

Epoch: 116
Loss: 0.09119268094310715
ROC train: 0.980879	val: 0.702336	test: 0.695188
PRC train: 0.857087	val: 0.267030	test: 0.245485

Epoch: 117
Loss: 0.08966833526467329
ROC train: 0.979415	val: 0.712399	test: 0.704082
PRC train: 0.847231	val: 0.269378	test: 0.262260

Epoch: 118
Loss: 0.0921360615449202
ROC train: 0.982928	val: 0.704309	test: 0.697368
PRC train: 0.871666	val: 0.262784	test: 0.248847

Epoch: 119
Loss: 0.0884704471826313
ROC train: 0.982639	val: 0.716389	test: 0.714380
PRC train: 0.869156	val: 0.268997	test: 0.262382

Epoch: 120
Loss: 0.0894441205020287
ROC train: 0.982869	val: 0.722905	test: 0.715279
PRC train: 0.870888	val: 0.283912	test: 0.264835

Early stopping
Best (ROC):	 train: 0.891743	val: 0.746686	test: 0.714632
Best (PRC):	 train: 0.535791	val: 0.308929	test: 0.271123
All runs completed.

ROC train: 0.973122	val: 0.731424	test: 0.718841
PRC train: 0.831296	val: 0.308045	test: 0.312001

Epoch: 95
Loss: 0.10609190209154241
ROC train: 0.973460	val: 0.732056	test: 0.720611
PRC train: 0.835064	val: 0.309354	test: 0.309600

Epoch: 96
Loss: 0.10342055838527275
ROC train: 0.973906	val: 0.736311	test: 0.718665
PRC train: 0.836064	val: 0.310358	test: 0.302278

Epoch: 97
Loss: 0.10596092214256864
ROC train: 0.976098	val: 0.735109	test: 0.724866
PRC train: 0.846920	val: 0.325612	test: 0.316351

Epoch: 98
Loss: 0.10295197974465711
ROC train: 0.976114	val: 0.732210	test: 0.717098
PRC train: 0.848966	val: 0.306422	test: 0.297799

Epoch: 99
Loss: 0.1061981566058137
ROC train: 0.974823	val: 0.733494	test: 0.727605
PRC train: 0.842061	val: 0.321557	test: 0.313579

Epoch: 100
Loss: 0.1025536155239657
ROC train: 0.974984	val: 0.732968	test: 0.723396
PRC train: 0.841830	val: 0.311452	test: 0.308713

Epoch: 101
Loss: 0.10295604749499865
ROC train: 0.975215	val: 0.728064	test: 0.723948
PRC train: 0.842713	val: 0.301839	test: 0.294752

Epoch: 102
Loss: 0.10460818273410032
ROC train: 0.976233	val: 0.740193	test: 0.731665
PRC train: 0.849986	val: 0.327374	test: 0.319269

Epoch: 103
Loss: 0.10163820898377444
ROC train: 0.978410	val: 0.733722	test: 0.719665
PRC train: 0.858191	val: 0.318303	test: 0.308890

Epoch: 104
Loss: 0.10070881092922625
ROC train: 0.977557	val: 0.728728	test: 0.718992
PRC train: 0.851283	val: 0.315994	test: 0.300695

Epoch: 105
Loss: 0.10156072672334372
ROC train: 0.977113	val: 0.727190	test: 0.723771
PRC train: 0.857715	val: 0.306719	test: 0.311207

Epoch: 106
Loss: 0.10069835848148037
ROC train: 0.976887	val: 0.738290	test: 0.733986
PRC train: 0.849769	val: 0.324892	test: 0.323943

Epoch: 107
Loss: 0.10068196482173625
ROC train: 0.977187	val: 0.733618	test: 0.719841
PRC train: 0.851931	val: 0.313160	test: 0.306674

Epoch: 108
Loss: 0.10115668606215493
ROC train: 0.979625	val: 0.731120	test: 0.723977
PRC train: 0.864643	val: 0.327721	test: 0.319770

Epoch: 109
Loss: 0.09977135903162711
ROC train: 0.979583	val: 0.728699	test: 0.724411
PRC train: 0.866154	val: 0.294971	test: 0.297632

Epoch: 110
Loss: 0.09702749436979614
ROC train: 0.981123	val: 0.730054	test: 0.726624
PRC train: 0.870451	val: 0.317530	test: 0.316724

Epoch: 111
Loss: 0.09810920263698125
ROC train: 0.980454	val: 0.736084	test: 0.733484
PRC train: 0.866638	val: 0.329615	test: 0.317711

Epoch: 112
Loss: 0.09573083863443607
ROC train: 0.980533	val: 0.724077	test: 0.716190
PRC train: 0.868861	val: 0.310264	test: 0.305099

Epoch: 113
Loss: 0.097011951722784
ROC train: 0.980004	val: 0.726370	test: 0.716697
PRC train: 0.863658	val: 0.310660	test: 0.308798

Epoch: 114
Loss: 0.097322671075622
ROC train: 0.980894	val: 0.729214	test: 0.722583
PRC train: 0.870090	val: 0.304614	test: 0.302184

Epoch: 115
Loss: 0.09691492703885772
ROC train: 0.981488	val: 0.721960	test: 0.713435
PRC train: 0.876847	val: 0.309199	test: 0.302461

Epoch: 116
Loss: 0.09673003422852605
ROC train: 0.980954	val: 0.728841	test: 0.719991
PRC train: 0.875576	val: 0.312817	test: 0.298522

Epoch: 117
Loss: 0.09504035574683102
ROC train: 0.982302	val: 0.722423	test: 0.717368
PRC train: 0.876360	val: 0.310854	test: 0.295387

Epoch: 118
Loss: 0.09533787575026716
ROC train: 0.982924	val: 0.725017	test: 0.715186
PRC train: 0.881852	val: 0.293599	test: 0.295518

Epoch: 119
Loss: 0.09546918293358927
ROC train: 0.983656	val: 0.730802	test: 0.717895
PRC train: 0.887461	val: 0.301362	test: 0.308725

Epoch: 120
Loss: 0.09429232279853482
ROC train: 0.982459	val: 0.728174	test: 0.720441
PRC train: 0.883514	val: 0.322194	test: 0.307609

Early stopping
Best (ROC):	 train: 0.905101	val: 0.757503	test: 0.740591
Best (PRC):	 train: 0.577915	val: 0.342758	test: 0.312638

ROC train: 0.974029	val: 0.734898	test: 0.721320
PRC train: 0.833759	val: 0.329812	test: 0.327435

Epoch: 95
Loss: 0.1072434166098636
ROC train: 0.974302	val: 0.736566	test: 0.724088
PRC train: 0.838879	val: 0.320915	test: 0.318448

Epoch: 96
Loss: 0.10454968422961532
ROC train: 0.973001	val: 0.737389	test: 0.719795
PRC train: 0.826876	val: 0.325570	test: 0.327110

Epoch: 97
Loss: 0.10529230160647139
ROC train: 0.974800	val: 0.730835	test: 0.727011
PRC train: 0.840274	val: 0.315307	test: 0.318364

Epoch: 98
Loss: 0.10376579524856848
ROC train: 0.975658	val: 0.738199	test: 0.724793
PRC train: 0.845793	val: 0.338304	test: 0.320578

Epoch: 99
Loss: 0.10409966086130616
ROC train: 0.975650	val: 0.741247	test: 0.725342
PRC train: 0.841566	val: 0.331617	test: 0.325820

Epoch: 100
Loss: 0.10388823897645151
ROC train: 0.976866	val: 0.733035	test: 0.720313
PRC train: 0.848995	val: 0.325193	test: 0.317421

Epoch: 101
Loss: 0.10325290097610106
ROC train: 0.977512	val: 0.740798	test: 0.719518
PRC train: 0.851037	val: 0.337448	test: 0.316091

Epoch: 102
Loss: 0.10191602834696964
ROC train: 0.976669	val: 0.735902	test: 0.723754
PRC train: 0.846440	val: 0.325509	test: 0.333457

Epoch: 103
Loss: 0.10134370035161727
ROC train: 0.977190	val: 0.734742	test: 0.718979
PRC train: 0.848576	val: 0.314876	test: 0.315218

Epoch: 104
Loss: 0.10074514023430449
ROC train: 0.977228	val: 0.733667	test: 0.722189
PRC train: 0.850534	val: 0.314667	test: 0.313784

Epoch: 105
Loss: 0.09926140185967505
ROC train: 0.978612	val: 0.726165	test: 0.707983
PRC train: 0.856436	val: 0.326814	test: 0.315637

Epoch: 106
Loss: 0.09817081731598697
ROC train: 0.979141	val: 0.728030	test: 0.716170
PRC train: 0.862187	val: 0.326628	test: 0.314690

Epoch: 107
Loss: 0.09862856583726835
ROC train: 0.979149	val: 0.743857	test: 0.717954
PRC train: 0.859363	val: 0.332971	test: 0.312797

Epoch: 108
Loss: 0.09964482413995522
ROC train: 0.978483	val: 0.737508	test: 0.720346
PRC train: 0.858662	val: 0.316304	test: 0.308252

Epoch: 109
Loss: 0.09765926800270458
ROC train: 0.979946	val: 0.731362	test: 0.711033
PRC train: 0.866568	val: 0.327744	test: 0.305735

Epoch: 110
Loss: 0.09970516388567843
ROC train: 0.981202	val: 0.732911	test: 0.721815
PRC train: 0.872065	val: 0.337769	test: 0.323279

Epoch: 111
Loss: 0.09729305518984499
ROC train: 0.980506	val: 0.733241	test: 0.719697
PRC train: 0.871270	val: 0.307976	test: 0.314456

Epoch: 112
Loss: 0.09693161209150981
ROC train: 0.981131	val: 0.730339	test: 0.716353
PRC train: 0.868990	val: 0.312474	test: 0.312623

Epoch: 113
Loss: 0.09638918753452952
ROC train: 0.980838	val: 0.731184	test: 0.713865
PRC train: 0.865142	val: 0.333802	test: 0.317238

Epoch: 114
Loss: 0.09697673559144349
ROC train: 0.981851	val: 0.732677	test: 0.722418
PRC train: 0.873610	val: 0.332813	test: 0.328009

Epoch: 115
Loss: 0.09571659117802876
ROC train: 0.981602	val: 0.730775	test: 0.713582
PRC train: 0.874122	val: 0.324965	test: 0.304910

Epoch: 116
Loss: 0.09488560807095602
ROC train: 0.982908	val: 0.737265	test: 0.718316
PRC train: 0.880409	val: 0.334085	test: 0.319220

Epoch: 117
Loss: 0.09516456113498001
ROC train: 0.981065	val: 0.727173	test: 0.704338
PRC train: 0.868662	val: 0.333148	test: 0.315191

Epoch: 118
Loss: 0.09341630792683916
ROC train: 0.982942	val: 0.732651	test: 0.716540
PRC train: 0.881230	val: 0.325279	test: 0.310007

Epoch: 119
Loss: 0.09688325408125048
ROC train: 0.983053	val: 0.733762	test: 0.718350
PRC train: 0.876758	val: 0.324277	test: 0.312407

Epoch: 120
Loss: 0.09208678897629796
ROC train: 0.983797	val: 0.733436	test: 0.720012
PRC train: 0.886144	val: 0.324956	test: 0.312797

Early stopping
Best (ROC):	 train: 0.926607	val: 0.755079	test: 0.732316
Best (PRC):	 train: 0.653569	val: 0.354326	test: 0.319962

ROC train: 0.972225	val: 0.730046	test: 0.715998
PRC train: 0.830298	val: 0.334070	test: 0.324920

Epoch: 95
Loss: 0.10615009481713633
ROC train: 0.973617	val: 0.738253	test: 0.712808
PRC train: 0.832672	val: 0.331997	test: 0.313044

Epoch: 96
Loss: 0.10657147067474103
ROC train: 0.973822	val: 0.729179	test: 0.714367
PRC train: 0.836707	val: 0.336444	test: 0.315247

Epoch: 97
Loss: 0.10553002635709663
ROC train: 0.973383	val: 0.730444	test: 0.714296
PRC train: 0.835146	val: 0.333732	test: 0.319250

Epoch: 98
Loss: 0.10419017586167013
ROC train: 0.974356	val: 0.722431	test: 0.713254
PRC train: 0.836076	val: 0.308871	test: 0.309025

Epoch: 99
Loss: 0.10428120025046303
ROC train: 0.975187	val: 0.733289	test: 0.715462
PRC train: 0.838728	val: 0.322801	test: 0.315985

Epoch: 100
Loss: 0.1057913945638511
ROC train: 0.975178	val: 0.736555	test: 0.710098
PRC train: 0.835806	val: 0.338889	test: 0.316471

Epoch: 101
Loss: 0.1032706963214396
ROC train: 0.975541	val: 0.729792	test: 0.715169
PRC train: 0.840343	val: 0.307683	test: 0.311746

Epoch: 102
Loss: 0.10337813524230403
ROC train: 0.976237	val: 0.724630	test: 0.709001
PRC train: 0.840352	val: 0.315767	test: 0.317591

Epoch: 103
Loss: 0.1053420615057595
ROC train: 0.976734	val: 0.725970	test: 0.714056
PRC train: 0.848962	val: 0.312246	test: 0.311526

Epoch: 104
Loss: 0.10180636009409773
ROC train: 0.977597	val: 0.732292	test: 0.715200
PRC train: 0.854610	val: 0.322502	test: 0.319808

Epoch: 105
Loss: 0.10202769454021031
ROC train: 0.976133	val: 0.726030	test: 0.709784
PRC train: 0.848890	val: 0.304630	test: 0.309441

Epoch: 106
Loss: 0.10183223229005424
ROC train: 0.977227	val: 0.730692	test: 0.711754
PRC train: 0.848556	val: 0.297934	test: 0.304130

Epoch: 107
Loss: 0.10027279845548255
ROC train: 0.978141	val: 0.715180	test: 0.704757
PRC train: 0.852257	val: 0.299889	test: 0.309106

Epoch: 108
Loss: 0.10018618053963689
ROC train: 0.978764	val: 0.728931	test: 0.713394
PRC train: 0.858150	val: 0.326620	test: 0.322833

Epoch: 109
Loss: 0.0979931275710216
ROC train: 0.978526	val: 0.727992	test: 0.716742
PRC train: 0.857411	val: 0.289699	test: 0.303790

Epoch: 110
Loss: 0.09939452594526753
ROC train: 0.979722	val: 0.718401	test: 0.709257
PRC train: 0.865873	val: 0.306143	test: 0.312988

Epoch: 111
Loss: 0.09983449969626013
ROC train: 0.979539	val: 0.713345	test: 0.708163
PRC train: 0.862629	val: 0.288182	test: 0.298566

Epoch: 112
Loss: 0.0990289141394119
ROC train: 0.979323	val: 0.722365	test: 0.711343
PRC train: 0.865247	val: 0.309718	test: 0.316228

Epoch: 113
Loss: 0.09876350720839598
ROC train: 0.978125	val: 0.713145	test: 0.708293
PRC train: 0.851302	val: 0.305830	test: 0.310276

Epoch: 114
Loss: 0.0998127246719494
ROC train: 0.980393	val: 0.720720	test: 0.711207
PRC train: 0.869602	val: 0.307888	test: 0.312752

Epoch: 115
Loss: 0.09687923935435448
ROC train: 0.980229	val: 0.716654	test: 0.708154
PRC train: 0.866043	val: 0.308717	test: 0.300286

Epoch: 116
Loss: 0.09498200012212216
ROC train: 0.981417	val: 0.714224	test: 0.705518
PRC train: 0.873075	val: 0.297890	test: 0.306899

Epoch: 117
Loss: 0.09615794044425675
ROC train: 0.981276	val: 0.725458	test: 0.709341
PRC train: 0.874233	val: 0.308175	test: 0.297913

Epoch: 118
Loss: 0.09678288438362541
ROC train: 0.980518	val: 0.724720	test: 0.710335
PRC train: 0.866454	val: 0.314470	test: 0.310485

Epoch: 119
Loss: 0.09698724013885845
ROC train: 0.981969	val: 0.719105	test: 0.708945
PRC train: 0.878033	val: 0.313771	test: 0.309894

Epoch: 120
Loss: 0.09706262517255072
ROC train: 0.982043	val: 0.709533	test: 0.699918
PRC train: 0.876024	val: 0.287554	test: 0.297957

Early stopping
Best (ROC):	 train: 0.917479	val: 0.754199	test: 0.735109
Best (PRC):	 train: 0.621483	val: 0.340095	test: 0.325622
All runs completed.

ROC train: 0.971602	val: 0.769166	test: 0.740178
PRC train: 0.831412	val: 0.389652	test: 0.354657

Epoch: 95
Loss: 0.11468591042523801
ROC train: 0.972408	val: 0.758777	test: 0.739530
PRC train: 0.830755	val: 0.368061	test: 0.344509

Epoch: 96
Loss: 0.11206963347359246
ROC train: 0.973780	val: 0.767083	test: 0.747170
PRC train: 0.837895	val: 0.378578	test: 0.366376

Epoch: 97
Loss: 0.11160418976563662
ROC train: 0.974759	val: 0.762602	test: 0.730072
PRC train: 0.839781	val: 0.379834	test: 0.340638

Epoch: 98
Loss: 0.11187709956876074
ROC train: 0.974453	val: 0.764066	test: 0.746352
PRC train: 0.840631	val: 0.396187	test: 0.366924

Epoch: 99
Loss: 0.11154933949175753
ROC train: 0.974844	val: 0.765304	test: 0.742496
PRC train: 0.844081	val: 0.388282	test: 0.345395

Epoch: 100
Loss: 0.1094699112877211
ROC train: 0.975753	val: 0.755244	test: 0.739556
PRC train: 0.847674	val: 0.379447	test: 0.345562

Epoch: 101
Loss: 0.10918117747511237
ROC train: 0.975884	val: 0.770540	test: 0.746723
PRC train: 0.846580	val: 0.381509	test: 0.356165

Epoch: 102
Loss: 0.10682972273028231
ROC train: 0.976595	val: 0.758764	test: 0.731126
PRC train: 0.848978	val: 0.383401	test: 0.342776

Epoch: 103
Loss: 0.10663054329642412
ROC train: 0.976650	val: 0.767026	test: 0.734491
PRC train: 0.857657	val: 0.391030	test: 0.353363

Epoch: 104
Loss: 0.10978537707891636
ROC train: 0.976791	val: 0.764360	test: 0.735955
PRC train: 0.853960	val: 0.393783	test: 0.361566

Epoch: 105
Loss: 0.10903159608577086
ROC train: 0.978036	val: 0.763112	test: 0.739097
PRC train: 0.865717	val: 0.384767	test: 0.345610

Epoch: 106
Loss: 0.10756716672807265
ROC train: 0.974544	val: 0.759864	test: 0.738898
PRC train: 0.838457	val: 0.380662	test: 0.348250

Epoch: 107
Loss: 0.10721901389041935
ROC train: 0.977914	val: 0.769411	test: 0.737911
PRC train: 0.855430	val: 0.387028	test: 0.345643

Epoch: 108
Loss: 0.1070058899793442
ROC train: 0.977856	val: 0.770399	test: 0.739967
PRC train: 0.856661	val: 0.392592	test: 0.341008

Epoch: 109
Loss: 0.10433403680281735
ROC train: 0.977492	val: 0.768730	test: 0.741041
PRC train: 0.856490	val: 0.400263	test: 0.346151

Epoch: 110
Loss: 0.10395152784935298
ROC train: 0.979847	val: 0.749963	test: 0.733360
PRC train: 0.871299	val: 0.375558	test: 0.336410

Epoch: 111
Loss: 0.10524274265593975
ROC train: 0.980295	val: 0.762748	test: 0.735589
PRC train: 0.870558	val: 0.397828	test: 0.342925

Epoch: 112
Loss: 0.10355823263971554
ROC train: 0.979276	val: 0.756740	test: 0.742229
PRC train: 0.865808	val: 0.394747	test: 0.357130

Epoch: 113
Loss: 0.10505403864673266
ROC train: 0.978992	val: 0.755572	test: 0.742386
PRC train: 0.862769	val: 0.391693	test: 0.357592

Epoch: 114
Loss: 0.10314117753275967
ROC train: 0.980843	val: 0.763098	test: 0.729004
PRC train: 0.876412	val: 0.382793	test: 0.342460

Epoch: 115
Loss: 0.10151192378238909
ROC train: 0.981309	val: 0.769344	test: 0.738790
PRC train: 0.876875	val: 0.406456	test: 0.359523

Epoch: 116
Loss: 0.09999590354197446
ROC train: 0.981273	val: 0.763318	test: 0.747260
PRC train: 0.877625	val: 0.394282	test: 0.357638

Epoch: 117
Loss: 0.09884400352506782
ROC train: 0.981729	val: 0.760935	test: 0.739675
PRC train: 0.875003	val: 0.392540	test: 0.339680

Epoch: 118
Loss: 0.10104056803094111
ROC train: 0.982346	val: 0.771190	test: 0.744904
PRC train: 0.882125	val: 0.388745	test: 0.337395

Epoch: 119
Loss: 0.10013936961899786
ROC train: 0.982305	val: 0.764608	test: 0.741458
PRC train: 0.886708	val: 0.391850	test: 0.342152

Epoch: 120
Loss: 0.10057883862646648
ROC train: 0.982038	val: 0.763846	test: 0.749337
PRC train: 0.880454	val: 0.370401	test: 0.329373

Early stopping
Best (ROC):	 train: 0.910731	val: 0.795266	test: 0.739137
Best (PRC):	 train: 0.601825	val: 0.372402	test: 0.350297

ROC train: 0.970078	val: 0.774551	test: 0.743956
PRC train: 0.820508	val: 0.359470	test: 0.344395

Epoch: 95
Loss: 0.11217446139670939
ROC train: 0.970338	val: 0.771568	test: 0.742548
PRC train: 0.820089	val: 0.362796	test: 0.331400

Epoch: 96
Loss: 0.11402345353619393
ROC train: 0.972929	val: 0.780448	test: 0.744539
PRC train: 0.830798	val: 0.372368	test: 0.338462

Epoch: 97
Loss: 0.11268266781869868
ROC train: 0.973238	val: 0.773929	test: 0.742199
PRC train: 0.834810	val: 0.369934	test: 0.337966

Epoch: 98
Loss: 0.1136893137155294
ROC train: 0.968796	val: 0.775776	test: 0.744726
PRC train: 0.805977	val: 0.365517	test: 0.335107

Epoch: 99
Loss: 0.11155086194875288
ROC train: 0.973602	val: 0.763486	test: 0.743288
PRC train: 0.833441	val: 0.372278	test: 0.342424

Epoch: 100
Loss: 0.11043572324171379
ROC train: 0.973989	val: 0.763890	test: 0.748263
PRC train: 0.837876	val: 0.358335	test: 0.340542

Epoch: 101
Loss: 0.10909160255091466
ROC train: 0.975346	val: 0.776026	test: 0.752808
PRC train: 0.844812	val: 0.376223	test: 0.354421

Epoch: 102
Loss: 0.10879457476645606
ROC train: 0.975678	val: 0.775018	test: 0.744637
PRC train: 0.845797	val: 0.371972	test: 0.342375

Epoch: 103
Loss: 0.1066898455965809
ROC train: 0.976429	val: 0.766350	test: 0.740704
PRC train: 0.846606	val: 0.369452	test: 0.329948

Epoch: 104
Loss: 0.10922248781361661
ROC train: 0.975237	val: 0.763692	test: 0.744036
PRC train: 0.842917	val: 0.367565	test: 0.335534

Epoch: 105
Loss: 0.10739627924725897
ROC train: 0.976396	val: 0.756856	test: 0.725094
PRC train: 0.847605	val: 0.334809	test: 0.300150

Epoch: 106
Loss: 0.11042084291203894
ROC train: 0.975692	val: 0.771421	test: 0.732379
PRC train: 0.844882	val: 0.349799	test: 0.307221

Epoch: 107
Loss: 0.1074866644657657
ROC train: 0.972682	val: 0.763105	test: 0.745353
PRC train: 0.832064	val: 0.351224	test: 0.336923

Epoch: 108
Loss: 0.10752806943861361
ROC train: 0.977850	val: 0.763013	test: 0.734423
PRC train: 0.858552	val: 0.349132	test: 0.333363

Epoch: 109
Loss: 0.1063677027196948
ROC train: 0.978468	val: 0.768128	test: 0.742870
PRC train: 0.866683	val: 0.349645	test: 0.335109

Epoch: 110
Loss: 0.10528697795960848
ROC train: 0.978425	val: 0.762255	test: 0.734943
PRC train: 0.857616	val: 0.350199	test: 0.328035

Epoch: 111
Loss: 0.10504645027437876
ROC train: 0.979395	val: 0.762918	test: 0.730248
PRC train: 0.862493	val: 0.354618	test: 0.331940

Epoch: 112
Loss: 0.10578724202217876
ROC train: 0.978394	val: 0.764980	test: 0.738526
PRC train: 0.856656	val: 0.354087	test: 0.339664

Epoch: 113
Loss: 0.10444250471485289
ROC train: 0.979577	val: 0.771252	test: 0.730747
PRC train: 0.866353	val: 0.360394	test: 0.329254

Epoch: 114
Loss: 0.10494178539369811
ROC train: 0.978282	val: 0.760479	test: 0.739287
PRC train: 0.861972	val: 0.363997	test: 0.338609

Epoch: 115
Loss: 0.1045352572419459
ROC train: 0.979315	val: 0.753962	test: 0.751131
PRC train: 0.862359	val: 0.350913	test: 0.344337

Epoch: 116
Loss: 0.10367514298955577
ROC train: 0.980473	val: 0.765410	test: 0.748068
PRC train: 0.872147	val: 0.352227	test: 0.338347

Epoch: 117
Loss: 0.10263395440785077
ROC train: 0.981894	val: 0.765062	test: 0.738018
PRC train: 0.877479	val: 0.359556	test: 0.341256

Epoch: 118
Loss: 0.10211630205094689
ROC train: 0.980752	val: 0.767332	test: 0.744376
PRC train: 0.871143	val: 0.361572	test: 0.342987

Epoch: 119
Loss: 0.1000342560466424
ROC train: 0.983103	val: 0.769740	test: 0.737246
PRC train: 0.884619	val: 0.377056	test: 0.348130

Epoch: 120
Loss: 0.10280316071795627
ROC train: 0.981711	val: 0.767091	test: 0.733913
PRC train: 0.877031	val: 0.368335	test: 0.345581

Early stopping
Best (ROC):	 train: 0.910410	val: 0.783514	test: 0.746010
Best (PRC):	 train: 0.607983	val: 0.374806	test: 0.360292

ROC train: 0.972238	val: 0.779414	test: 0.750685
PRC train: 0.828030	val: 0.367575	test: 0.362096

Epoch: 95
Loss: 0.11418055534727416
ROC train: 0.974048	val: 0.769573	test: 0.742958
PRC train: 0.839747	val: 0.377736	test: 0.365081

Epoch: 96
Loss: 0.11333492225949321
ROC train: 0.973482	val: 0.777945	test: 0.739893
PRC train: 0.833903	val: 0.379544	test: 0.358802

Epoch: 97
Loss: 0.11163485494287519
ROC train: 0.972112	val: 0.773537	test: 0.747875
PRC train: 0.831483	val: 0.363643	test: 0.371806

Epoch: 98
Loss: 0.11146771534997106
ROC train: 0.973749	val: 0.779091	test: 0.730814
PRC train: 0.833311	val: 0.375153	test: 0.348630

Epoch: 99
Loss: 0.11351397602653542
ROC train: 0.973039	val: 0.775044	test: 0.745632
PRC train: 0.835766	val: 0.384404	test: 0.361552

Epoch: 100
Loss: 0.11127010552038291
ROC train: 0.973643	val: 0.772754	test: 0.719987
PRC train: 0.835868	val: 0.377042	test: 0.338332

Epoch: 101
Loss: 0.10993721109707742
ROC train: 0.975807	val: 0.765687	test: 0.734310
PRC train: 0.846297	val: 0.364810	test: 0.342811

Epoch: 102
Loss: 0.11033725085277023
ROC train: 0.975721	val: 0.778332	test: 0.738567
PRC train: 0.846177	val: 0.370888	test: 0.364983

Epoch: 103
Loss: 0.10911021658963092
ROC train: 0.976116	val: 0.769399	test: 0.732241
PRC train: 0.850548	val: 0.361262	test: 0.341807

Epoch: 104
Loss: 0.11043546963450654
ROC train: 0.969021	val: 0.754382	test: 0.729227
PRC train: 0.807585	val: 0.348441	test: 0.344107

Epoch: 105
Loss: 0.108633581399456
ROC train: 0.976794	val: 0.767674	test: 0.730730
PRC train: 0.856505	val: 0.373688	test: 0.347215

Epoch: 106
Loss: 0.10793798137465098
ROC train: 0.977689	val: 0.773919	test: 0.742939
PRC train: 0.857766	val: 0.374743	test: 0.353101

Epoch: 107
Loss: 0.10519865138899771
ROC train: 0.978279	val: 0.770811	test: 0.739604
PRC train: 0.864074	val: 0.379763	test: 0.374172

Epoch: 108
Loss: 0.10600658271244187
ROC train: 0.977719	val: 0.763721	test: 0.740552
PRC train: 0.863537	val: 0.365189	test: 0.355423

Epoch: 109
Loss: 0.10651519673456482
ROC train: 0.979050	val: 0.778211	test: 0.745259
PRC train: 0.865006	val: 0.385366	test: 0.370776

Epoch: 110
Loss: 0.1066799005248095
ROC train: 0.977280	val: 0.764950	test: 0.735448
PRC train: 0.857215	val: 0.348209	test: 0.344267

Epoch: 111
Loss: 0.10564452641107923
ROC train: 0.978884	val: 0.780019	test: 0.736125
PRC train: 0.870276	val: 0.371206	test: 0.357499

Epoch: 112
Loss: 0.10485297358975453
ROC train: 0.981038	val: 0.768793	test: 0.745881
PRC train: 0.875910	val: 0.371662	test: 0.357316

Epoch: 113
Loss: 0.10349739506611565
ROC train: 0.980587	val: 0.777456	test: 0.749417
PRC train: 0.873912	val: 0.378363	test: 0.376889

Epoch: 114
Loss: 0.10489352236489129
ROC train: 0.980494	val: 0.773450	test: 0.734157
PRC train: 0.872249	val: 0.376422	test: 0.346572

Epoch: 115
Loss: 0.10286516583596436
ROC train: 0.979530	val: 0.774428	test: 0.739645
PRC train: 0.870190	val: 0.370202	test: 0.359692

Epoch: 116
Loss: 0.10277244831757021
ROC train: 0.981175	val: 0.781164	test: 0.743344
PRC train: 0.880142	val: 0.385181	test: 0.362748

Epoch: 117
Loss: 0.10107168223855582
ROC train: 0.981803	val: 0.770397	test: 0.744717
PRC train: 0.880956	val: 0.367343	test: 0.359549

Epoch: 118
Loss: 0.10225079571344411
ROC train: 0.981653	val: 0.773677	test: 0.738408
PRC train: 0.880061	val: 0.364989	test: 0.348527

Epoch: 119
Loss: 0.10188741013406837
ROC train: 0.981341	val: 0.771044	test: 0.751933
PRC train: 0.878465	val: 0.380795	test: 0.358283

Epoch: 120
Loss: 0.10214957428498085
ROC train: 0.981316	val: 0.770343	test: 0.731352
PRC train: 0.876685	val: 0.373437	test: 0.336419

Epoch: 121
Loss: 0.1012108709186277
ROC train: 0.982322	val: 0.770662	test: 0.742082
PRC train: 0.885560	val: 0.379533	test: 0.363339

Epoch: 122
Loss: 0.10040151083408592
ROC train: 0.982758	val: 0.776578	test: 0.740145
PRC train: 0.882425	val: 0.369341	test: 0.350726

Epoch: 123
Loss: 0.09900435976820185
ROC train: 0.981900	val: 0.770469	test: 0.741580
PRC train: 0.882789	val: 0.374240	test: 0.354492

Epoch: 124
Loss: 0.10043502660548333
ROC train: 0.983333	val: 0.770100	test: 0.730876
PRC train: 0.887519	val: 0.373520	test: 0.347971

Epoch: 125
Loss: 0.09902711809291903
ROC train: 0.983808	val: 0.774700	test: 0.737298
PRC train: 0.889677	val: 0.367932	test: 0.351987

Epoch: 126
Loss: 0.0984584979302367
ROC train: 0.984802	val: 0.771342	test: 0.739227
PRC train: 0.898150	val: 0.369411	test: 0.345126

Epoch: 127
Loss: 0.09653747172893133
ROC train: 0.983295	val: 0.774396	test: 0.738188
PRC train: 0.890799	val: 0.356594	test: 0.344360

Epoch: 128
Loss: 0.09649881129242091
ROC train: 0.984989	val: 0.773994	test: 0.732244
PRC train: 0.898193	val: 0.385918	test: 0.353806

Early stopping
Best (ROC):	 train: 0.971003	val: 0.784542	test: 0.742739
Best (PRC):	 train: 0.826434	val: 0.372731	test: 0.357455
All runs completed.
