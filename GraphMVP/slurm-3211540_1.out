>>> Starting run for dataset: freesolv
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running RANDOM configs_split_experiments/GraphMVP/freesolv/random/train_prop=0.6.yml on cuda:0
Running RANDOM configs_split_experiments/GraphMVP/freesolv/random/train_prop=0.7.yml on cuda:1
Running RANDOM configs_split_experiments/GraphMVP/freesolv/random/train_prop=0.8.yml on cuda:2
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/freesolv/random/train_prop=0.8.yml --runseed 4 --device cuda:2
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/freesolv/random/train_prop=0.8.yml --runseed 5 --device cuda:2
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/freesolv/random/train_prop=0.8.yml --runseed 6 --device cuda:2
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/freesolv/random/train_prop=0.7.yml --runseed 4 --device cuda:1
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/freesolv/random/train_prop=0.7.yml --runseed 5 --device cuda:1
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/freesolv/random/train_prop=0.6.yml --runseed 4 --device cuda:0
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/freesolv/random/train_prop=0.7.yml --runseed 6 --device cuda:1
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/freesolv/random/train_prop=0.6.yml --runseed 5 --device cuda:0
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/freesolv/random/train_prop=0.6.yml --runseed 6 --device cuda:0
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/freesolv/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/freesolv/random/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/freesolv/random/train_prop=0.6/freesolv_random_6_26-05_11-05-10  ]
[ Using Seed :  6  ]
[ Using device :  cuda:0  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 31.38041877746582
RMSE train: 5.419871	val: 5.519873	test: 5.318568
MAE train: 4.258120	val: 4.232114	test: 4.215514

Epoch: 2
Loss: 28.489768981933594
RMSE train: 5.242590	val: 5.344500	test: 5.144342
MAE train: 4.107927	val: 4.086229	test: 4.062995

Epoch: 3
Loss: 27.21208667755127
RMSE train: 5.075803	val: 5.185488	test: 4.991778
MAE train: 3.983546	val: 3.966619	test: 3.945420

Epoch: 4
Loss: 24.03274440765381
RMSE train: 4.917901	val: 5.037711	test: 4.867521
MAE train: 3.881075	val: 3.871265	test: 3.871369

Epoch: 5
Loss: 22.623849868774414
RMSE train: 4.764942	val: 4.883656	test: 4.770844
MAE train: 3.797292	val: 3.821872	test: 3.812263

Epoch: 6
Loss: 20.918173789978027
RMSE train: 4.579086	val: 4.686641	test: 4.645396
MAE train: 3.699887	val: 3.750453	test: 3.728573

Epoch: 7
Loss: 18.363860607147217
RMSE train: 4.327634	val: 4.382168	test: 4.453346
MAE train: 3.556948	val: 3.595944	test: 3.616124

Epoch: 8
Loss: 18.806013107299805
RMSE train: 4.057516	val: 4.007542	test: 4.206947
MAE train: 3.392056	val: 3.390957	test: 3.469575

Epoch: 9
Loss: 17.08230447769165
RMSE train: 3.801679	val: 3.602638	test: 3.935170
MAE train: 3.242731	val: 3.134414	test: 3.317375

Epoch: 10
Loss: 14.689143180847168
RMSE train: 3.693434	val: 3.412539	test: 3.800685
MAE train: 3.184839	val: 2.987519	test: 3.243284

Epoch: 11
Loss: 14.514894485473633
RMSE train: 3.659913	val: 3.360816	test: 3.761436
MAE train: 3.176523	val: 2.934362	test: 3.224543

Epoch: 12
Loss: 14.086364269256592
RMSE train: 3.597361	val: 3.304369	test: 3.744298
MAE train: 3.117087	val: 2.870913	test: 3.214924

Epoch: 13
Loss: 14.997931957244873
RMSE train: 3.589372	val: 3.356448	test: 3.800344
MAE train: 3.101622	val: 2.901399	test: 3.260075

Epoch: 14
Loss: 13.24376916885376
RMSE train: 3.596150	val: 3.439581	test: 3.843194
MAE train: 3.117091	val: 2.973181	test: 3.281747

Epoch: 15
Loss: 11.8695387840271
RMSE train: 3.597439	val: 3.509181	test: 3.836749
MAE train: 3.153997	val: 3.065550	test: 3.280172

Epoch: 16
Loss: 12.098965644836426
RMSE train: 3.594824	val: 3.523419	test: 3.794022
MAE train: 3.202000	val: 3.115936	test: 3.271324

Epoch: 17
Loss: 13.125303745269775
RMSE train: 3.599990	val: 3.514311	test: 3.782186
MAE train: 3.242769	val: 3.137793	test: 3.302933

Epoch: 18
Loss: 11.086932182312012
RMSE train: 3.571427	val: 3.487209	test: 3.765458
MAE train: 3.228857	val: 3.128844	test: 3.317521

Epoch: 19
Loss: 10.790023803710938
RMSE train: 3.580162	val: 3.496005	test: 3.780403
MAE train: 3.229577	val: 3.138700	test: 3.335851

Epoch: 20
Loss: 10.222341537475586
RMSE train: 3.537580	val: 3.455904	test: 3.734668
MAE train: 3.177829	val: 3.098173	test: 3.293791

Epoch: 21
Loss: 9.804360389709473
RMSE train: 3.447030	val: 3.353778	test: 3.634290
MAE train: 3.091378	val: 3.005333	test: 3.210371

Epoch: 22
Loss: 9.451221466064453
RMSE train: 3.357294	val: 3.237632	test: 3.537009
MAE train: 3.018184	val: 2.905984	test: 3.134168Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/freesolv/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/freesolv/random/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/freesolv/random/train_prop=0.6/freesolv_random_4_26-05_11-05-10  ]
[ Using Seed :  4  ]
[ Using device :  cuda:0  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 30.422832489013672
RMSE train: 5.426613	val: 5.464270	test: 5.377981
MAE train: 4.308131	val: 4.278633	test: 4.326419

Epoch: 2
Loss: 28.776872634887695
RMSE train: 5.256468	val: 5.318944	test: 5.213229
MAE train: 4.172634	val: 4.152384	test: 4.175180

Epoch: 3
Loss: 23.824830055236816
RMSE train: 5.066564	val: 5.153217	test: 5.042820
MAE train: 4.027696	val: 4.010175	test: 4.021373

Epoch: 4
Loss: 23.714346885681152
RMSE train: 4.856410	val: 4.960474	test: 4.868181
MAE train: 3.873553	val: 3.854623	test: 3.867088

Epoch: 5
Loss: 21.48446273803711
RMSE train: 4.632602	val: 4.734621	test: 4.700128
MAE train: 3.727158	val: 3.731093	test: 3.749894

Epoch: 6
Loss: 19.348520278930664
RMSE train: 4.413582	val: 4.459426	test: 4.554956
MAE train: 3.612023	val: 3.620614	test: 3.694429

Epoch: 7
Loss: 18.165912628173828
RMSE train: 4.193475	val: 4.127748	test: 4.396418
MAE train: 3.482025	val: 3.470242	test: 3.630676

Epoch: 8
Loss: 17.470162391662598
RMSE train: 3.972183	val: 3.782051	test: 4.169972
MAE train: 3.357741	val: 3.278249	test: 3.514655

Epoch: 9
Loss: 15.864796161651611
RMSE train: 3.839444	val: 3.565225	test: 3.982245
MAE train: 3.300713	val: 3.131345	test: 3.405106

Epoch: 10
Loss: 15.112760543823242
RMSE train: 3.783025	val: 3.459445	test: 3.851984
MAE train: 3.300375	val: 3.041846	test: 3.357677

Epoch: 11
Loss: 15.410066604614258
RMSE train: 3.784120	val: 3.459880	test: 3.842220
MAE train: 3.342358	val: 3.064646	test: 3.390236

Epoch: 12
Loss: 14.29528522491455
RMSE train: 3.765566	val: 3.459212	test: 3.851757
MAE train: 3.347440	val: 3.077781	test: 3.430688

Epoch: 13
Loss: 13.62661600112915
RMSE train: 3.723393	val: 3.444313	test: 3.846005
MAE train: 3.323689	val: 3.072445	test: 3.445828

Epoch: 14
Loss: 13.332930088043213
RMSE train: 3.679579	val: 3.478690	test: 3.825005
MAE train: 3.301485	val: 3.114538	test: 3.427774

Epoch: 15
Loss: 12.545594215393066
RMSE train: 3.667609	val: 3.526917	test: 3.830349
MAE train: 3.286055	val: 3.157524	test: 3.408851

Epoch: 16
Loss: 11.67045783996582
RMSE train: 3.600451	val: 3.448664	test: 3.781291
MAE train: 3.199424	val: 3.082575	test: 3.332297

Epoch: 17
Loss: 11.771305084228516
RMSE train: 3.529399	val: 3.355201	test: 3.720042
MAE train: 3.117984	val: 2.994698	test: 3.258504

Epoch: 18
Loss: 10.655505657196045
RMSE train: 3.420374	val: 3.226371	test: 3.611826
MAE train: 3.011178	val: 2.876248	test: 3.151551

Epoch: 19
Loss: 10.255755424499512
RMSE train: 3.330261	val: 3.158294	test: 3.509778
MAE train: 2.942245	val: 2.822019	test: 3.065810

Epoch: 20
Loss: 9.691178321838379
RMSE train: 3.221739	val: 3.084989	test: 3.382956
MAE train: 2.870115	val: 2.768351	test: 2.972285

Epoch: 21
Loss: 9.868766784667969
RMSE train: 3.193568	val: 3.104219	test: 3.344147
MAE train: 2.866943	val: 2.790047	test: 2.957641

Epoch: 22
Loss: 9.300775527954102
RMSE train: 3.170608	val: 3.093040	test: 3.310844
MAE train: 2.862778	val: 2.790503	test: 2.946467Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/freesolv/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/freesolv/random/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/freesolv/random/train_prop=0.6/freesolv_random_5_26-05_11-05-10  ]
[ Using Seed :  5  ]
[ Using device :  cuda:0  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 29.59422779083252
RMSE train: 5.499407	val: 5.516536	test: 5.433216
MAE train: 4.348929	val: 4.277060	test: 4.326735

Epoch: 2
Loss: 28.34329605102539
RMSE train: 5.357218	val: 5.360725	test: 5.300185
MAE train: 4.250012	val: 4.180156	test: 4.224348

Epoch: 3
Loss: 27.85537338256836
RMSE train: 5.224773	val: 5.216915	test: 5.194324
MAE train: 4.169078	val: 4.100103	test: 4.148321

Epoch: 4
Loss: 25.295743942260742
RMSE train: 5.120570	val: 5.099790	test: 5.139571
MAE train: 4.124670	val: 4.066828	test: 4.123449

Epoch: 5
Loss: 24.112407684326172
RMSE train: 5.034899	val: 5.012288	test: 5.114962
MAE train: 4.100668	val: 4.060718	test: 4.126939

Epoch: 6
Loss: 21.505508422851562
RMSE train: 4.903339	val: 4.878200	test: 5.059861
MAE train: 4.044971	val: 4.029270	test: 4.100572

Epoch: 7
Loss: 21.039341926574707
RMSE train: 4.727104	val: 4.670101	test: 4.959395
MAE train: 3.951478	val: 3.967874	test: 4.053812

Epoch: 8
Loss: 18.04042625427246
RMSE train: 4.530898	val: 4.384727	test: 4.785918
MAE train: 3.878923	val: 3.827208	test: 4.000741

Epoch: 9
Loss: 17.400511741638184
RMSE train: 4.343240	val: 4.088688	test: 4.541721
MAE train: 3.799652	val: 3.621788	test: 3.904862

Epoch: 10
Loss: 15.350633144378662
RMSE train: 4.203999	val: 3.914595	test: 4.320806
MAE train: 3.726497	val: 3.472555	test: 3.775943

Epoch: 11
Loss: 15.99096155166626
RMSE train: 4.130075	val: 3.846002	test: 4.192979
MAE train: 3.688380	val: 3.408855	test: 3.702833

Epoch: 12
Loss: 15.150773048400879
RMSE train: 4.088694	val: 3.862201	test: 4.145581
MAE train: 3.663869	val: 3.432316	test: 3.671740

Epoch: 13
Loss: 13.945610046386719
RMSE train: 4.070626	val: 3.882436	test: 4.134425
MAE train: 3.659866	val: 3.475036	test: 3.663309

Epoch: 14
Loss: 14.601300716400146
RMSE train: 4.004759	val: 3.797723	test: 4.099305
MAE train: 3.615268	val: 3.430454	test: 3.630569

Epoch: 15
Loss: 12.526318073272705
RMSE train: 3.952957	val: 3.758231	test: 4.087261
MAE train: 3.555089	val: 3.389099	test: 3.590686

Epoch: 16
Loss: 12.10056447982788
RMSE train: 3.923596	val: 3.759580	test: 4.089281
MAE train: 3.509749	val: 3.384081	test: 3.563061

Epoch: 17
Loss: 11.320309162139893
RMSE train: 3.852252	val: 3.719531	test: 4.033234
MAE train: 3.443026	val: 3.337582	test: 3.501683

Epoch: 18
Loss: 10.72469425201416
RMSE train: 3.709347	val: 3.602166	test: 3.894304
MAE train: 3.318594	val: 3.223971	test: 3.381923

Epoch: 19
Loss: 10.710988998413086
RMSE train: 3.534209	val: 3.429046	test: 3.720289
MAE train: 3.167186	val: 3.066478	test: 3.243698

Epoch: 20
Loss: 10.442039012908936
RMSE train: 3.371025	val: 3.245298	test: 3.550146
MAE train: 3.030128	val: 2.912430	test: 3.117920

Epoch: 21
Loss: 11.101334571838379
RMSE train: 3.303833	val: 3.159875	test: 3.466806
MAE train: 2.987301	val: 2.859089	test: 3.079031

Epoch: 22
Loss: 9.720348358154297
RMSE train: 3.242826	val: 3.045824	test: 3.392295
MAE train: 2.945162	val: 2.770949	test: 3.032297Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/freesolv/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/freesolv/random/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/freesolv/random/train_prop=0.8/freesolv_random_6_26-05_11-05-10  ]
[ Using Seed :  6  ]
[ Using device :  cuda:2  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 29.61820888519287
RMSE train: 5.425037	val: 5.495120	test: 5.144177
MAE train: 4.233220	val: 4.635221	test: 3.802174

Epoch: 2
Loss: 28.01936626434326
RMSE train: 5.264025	val: 5.333539	test: 4.992846
MAE train: 4.102076	val: 4.475443	test: 3.691987

Epoch: 3
Loss: 26.41668128967285
RMSE train: 5.127884	val: 5.212637	test: 4.875079
MAE train: 4.004998	val: 4.351833	test: 3.636785

Epoch: 4
Loss: 25.03196430206299
RMSE train: 5.013062	val: 5.135807	test: 4.799817
MAE train: 3.943269	val: 4.267184	test: 3.643077

Epoch: 5
Loss: 22.909812927246094
RMSE train: 4.875065	val: 5.067233	test: 4.727440
MAE train: 3.878436	val: 4.178436	test: 3.651093

Epoch: 6
Loss: 21.001883506774902
RMSE train: 4.697496	val: 4.977822	test: 4.652846
MAE train: 3.807455	val: 4.090792	test: 3.670278

Epoch: 7
Loss: 19.014973640441895
RMSE train: 4.466679	val: 4.831152	test: 4.509613
MAE train: 3.704143	val: 3.988101	test: 3.670676

Epoch: 8
Loss: 17.28286647796631
RMSE train: 4.184999	val: 4.605837	test: 4.265393
MAE train: 3.525465	val: 3.823339	test: 3.571827

Epoch: 9
Loss: 15.740184307098389
RMSE train: 3.903405	val: 4.347398	test: 3.935490
MAE train: 3.326471	val: 3.616732	test: 3.393436

Epoch: 10
Loss: 14.898179054260254
RMSE train: 3.743491	val: 4.176110	test: 3.719687
MAE train: 3.231547	val: 3.501495	test: 3.275524

Epoch: 11
Loss: 14.158162117004395
RMSE train: 3.688220	val: 4.120496	test: 3.625762
MAE train: 3.179541	val: 3.451696	test: 3.204150

Epoch: 12
Loss: 13.502789497375488
RMSE train: 3.640503	val: 4.120172	test: 3.570496
MAE train: 3.115834	val: 3.420036	test: 3.145272

Epoch: 13
Loss: 13.401731967926025
RMSE train: 3.536606	val: 4.065602	test: 3.491839
MAE train: 3.033827	val: 3.355192	test: 3.095250

Epoch: 14
Loss: 12.748482704162598
RMSE train: 3.422923	val: 3.981869	test: 3.391742
MAE train: 2.948862	val: 3.296865	test: 3.014630

Epoch: 15
Loss: 12.148969650268555
RMSE train: 3.467405	val: 4.025595	test: 3.449473
MAE train: 3.005135	val: 3.333306	test: 3.058534

Epoch: 16
Loss: 11.94351053237915
RMSE train: 3.612725	val: 4.166253	test: 3.599102
MAE train: 3.160473	val: 3.457143	test: 3.140038

Epoch: 17
Loss: 11.061439514160156
RMSE train: 3.630474	val: 4.162285	test: 3.621812
MAE train: 3.193217	val: 3.481721	test: 3.170727

Epoch: 18
Loss: 11.052628993988037
RMSE train: 3.557170	val: 4.068266	test: 3.579391
MAE train: 3.165425	val: 3.461478	test: 3.192633

Epoch: 19
Loss: 10.741682291030884
RMSE train: 3.473932	val: 3.970994	test: 3.514017
MAE train: 3.113211	val: 3.435152	test: 3.165649

Epoch: 20
Loss: 10.049704551696777
RMSE train: 3.429287	val: 3.924568	test: 3.462526
MAE train: 3.074616	val: 3.417717	test: 3.121240

Epoch: 21
Loss: 9.418673515319824
RMSE train: 3.414401	val: 3.909281	test: 3.437047
MAE train: 3.056120	val: 3.399894	test: 3.093485

Epoch: 22
Loss: 9.01304817199707
RMSE train: 3.381460	val: 3.865226	test: 3.389906
MAE train: 3.017819	val: 3.350539	test: 3.038095Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/freesolv/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/freesolv/random/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/freesolv/random/train_prop=0.8/freesolv_random_5_26-05_11-05-10  ]
[ Using Seed :  5  ]
[ Using device :  cuda:2  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 30.448320388793945
RMSE train: 5.521837	val: 5.641645	test: 5.300904
MAE train: 4.346746	val: 4.801698	test: 3.933050

Epoch: 2
Loss: 28.590373992919922
RMSE train: 5.395838	val: 5.521151	test: 5.204260
MAE train: 4.270477	val: 4.686547	test: 3.889076

Epoch: 3
Loss: 26.816272735595703
RMSE train: 5.258092	val: 5.416576	test: 5.125659
MAE train: 4.198844	val: 4.583011	test: 3.869342

Epoch: 4
Loss: 25.08441162109375
RMSE train: 5.117963	val: 5.331069	test: 5.077732
MAE train: 4.136025	val: 4.487677	test: 3.889934

Epoch: 5
Loss: 23.31664276123047
RMSE train: 4.982773	val: 5.250036	test: 5.053094
MAE train: 4.073022	val: 4.386089	test: 3.940446

Epoch: 6
Loss: 21.192240715026855
RMSE train: 4.821567	val: 5.142307	test: 4.988153
MAE train: 3.990791	val: 4.242023	test: 3.977127

Epoch: 7
Loss: 19.243717670440674
RMSE train: 4.600603	val: 4.981699	test: 4.830191
MAE train: 3.878716	val: 4.046641	test: 3.962318

Epoch: 8
Loss: 17.47354793548584
RMSE train: 4.347758	val: 4.757789	test: 4.572227
MAE train: 3.749327	val: 3.928982	test: 3.889390

Epoch: 9
Loss: 16.67252540588379
RMSE train: 4.113657	val: 4.490514	test: 4.262140
MAE train: 3.599384	val: 3.761381	test: 3.770235

Epoch: 10
Loss: 15.79123592376709
RMSE train: 3.972871	val: 4.292469	test: 4.066973
MAE train: 3.504290	val: 3.634603	test: 3.726030

Epoch: 11
Loss: 15.181011199951172
RMSE train: 3.841080	val: 4.124558	test: 3.936696
MAE train: 3.381937	val: 3.496348	test: 3.629089

Epoch: 12
Loss: 14.385504245758057
RMSE train: 3.835707	val: 4.152377	test: 3.907625
MAE train: 3.363711	val: 3.491216	test: 3.565790

Epoch: 13
Loss: 13.838431358337402
RMSE train: 3.868204	val: 4.279683	test: 3.918935
MAE train: 3.368173	val: 3.558025	test: 3.495404

Epoch: 14
Loss: 13.098886966705322
RMSE train: 3.851729	val: 4.326844	test: 3.884129
MAE train: 3.336755	val: 3.553347	test: 3.386239

Epoch: 15
Loss: 12.654282569885254
RMSE train: 3.696986	val: 4.189148	test: 3.744846
MAE train: 3.226115	val: 3.432816	test: 3.276689

Epoch: 16
Loss: 12.00662899017334
RMSE train: 3.434292	val: 3.894830	test: 3.499298
MAE train: 3.020622	val: 3.219138	test: 3.119430

Epoch: 17
Loss: 11.60952615737915
RMSE train: 3.280846	val: 3.682197	test: 3.343994
MAE train: 2.899882	val: 3.100076	test: 2.988117

Epoch: 18
Loss: 11.142798900604248
RMSE train: 3.250399	val: 3.631704	test: 3.305453
MAE train: 2.905888	val: 3.098904	test: 2.965403

Epoch: 19
Loss: 10.501092433929443
RMSE train: 3.271559	val: 3.678909	test: 3.305994
MAE train: 2.954869	val: 3.154873	test: 2.964747

Epoch: 20
Loss: 10.164125919342041
RMSE train: 3.278164	val: 3.726333	test: 3.291811
MAE train: 2.951209	val: 3.186457	test: 2.925860

Epoch: 21
Loss: 10.186119556427002
RMSE train: 3.289774	val: 3.770074	test: 3.293561
MAE train: 2.955827	val: 3.222014	test: 2.918897

Epoch: 22
Loss: 9.350884437561035
RMSE train: 3.278578	val: 3.779015	test: 3.291877
MAE train: 2.943748	val: 3.228321	test: 2.925804Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/freesolv/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/freesolv/random/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/freesolv/random/train_prop=0.7/freesolv_random_4_26-05_11-05-10  ]
[ Using Seed :  4  ]
[ Using device :  cuda:1  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 28.109039306640625
RMSE train: 5.316181	val: 5.953959	test: 5.369454
MAE train: 4.208443	val: 4.729866	test: 4.320440

Epoch: 2
Loss: 26.357086181640625
RMSE train: 5.165710	val: 5.790194	test: 5.217569
MAE train: 4.094532	val: 4.577346	test: 4.197839

Epoch: 3
Loss: 24.224316596984863
RMSE train: 4.996030	val: 5.608204	test: 5.055138
MAE train: 3.975549	val: 4.409576	test: 4.075957

Epoch: 4
Loss: 22.67241382598877
RMSE train: 4.826816	val: 5.410801	test: 4.913326
MAE train: 3.868548	val: 4.246285	test: 3.985749

Epoch: 5
Loss: 20.89576244354248
RMSE train: 4.636370	val: 5.183335	test: 4.752226
MAE train: 3.757670	val: 4.106237	test: 3.895618

Epoch: 6
Loss: 19.442559242248535
RMSE train: 4.453948	val: 4.941002	test: 4.612520
MAE train: 3.677012	val: 4.001093	test: 3.857707

Epoch: 7
Loss: 17.335253715515137
RMSE train: 4.272170	val: 4.671893	test: 4.450528
MAE train: 3.602197	val: 3.914761	test: 3.815939

Epoch: 8
Loss: 16.10743474960327
RMSE train: 4.099577	val: 4.391552	test: 4.248863
MAE train: 3.542881	val: 3.794701	test: 3.732538

Epoch: 9
Loss: 14.961558818817139
RMSE train: 3.923362	val: 4.101085	test: 4.004703
MAE train: 3.447170	val: 3.584611	test: 3.570298

Epoch: 10
Loss: 13.893120765686035
RMSE train: 3.779617	val: 3.923810	test: 3.786087
MAE train: 3.355567	val: 3.407151	test: 3.425272

Epoch: 11
Loss: 13.488630294799805
RMSE train: 3.727012	val: 3.882042	test: 3.681941
MAE train: 3.319221	val: 3.379417	test: 3.348692

Epoch: 12
Loss: 12.868814468383789
RMSE train: 3.633661	val: 3.809101	test: 3.571387
MAE train: 3.245553	val: 3.311324	test: 3.233679

Epoch: 13
Loss: 12.111193656921387
RMSE train: 3.530120	val: 3.729890	test: 3.467441
MAE train: 3.157844	val: 3.233442	test: 3.114822

Epoch: 14
Loss: 11.774165153503418
RMSE train: 3.460845	val: 3.642542	test: 3.416587
MAE train: 3.074799	val: 3.132632	test: 3.050165

Epoch: 15
Loss: 11.270859241485596
RMSE train: 3.514095	val: 3.692289	test: 3.489360
MAE train: 3.103326	val: 3.163656	test: 3.103161

Epoch: 16
Loss: 11.022959232330322
RMSE train: 3.540505	val: 3.761803	test: 3.546208
MAE train: 3.132027	val: 3.261882	test: 3.139538

Epoch: 17
Loss: 10.32428503036499
RMSE train: 3.468537	val: 3.767047	test: 3.488761
MAE train: 3.070197	val: 3.296145	test: 3.069952

Epoch: 18
Loss: 10.288900375366211
RMSE train: 3.406971	val: 3.741424	test: 3.425435
MAE train: 3.035051	val: 3.307611	test: 3.030568

Epoch: 19
Loss: 10.492891311645508
RMSE train: 3.343558	val: 3.659853	test: 3.352503
MAE train: 2.998504	val: 3.259492	test: 2.994613

Epoch: 20
Loss: 9.227445363998413
RMSE train: 3.252010	val: 3.509697	test: 3.252493
MAE train: 2.927741	val: 3.140262	test: 2.926586

Epoch: 21
Loss: 8.766288757324219
RMSE train: 3.178268	val: 3.380530	test: 3.178421
MAE train: 2.868790	val: 3.033785	test: 2.873497

Epoch: 22
Loss: 8.668865203857422
RMSE train: 3.121113	val: 3.315692	test: 3.130055
MAE train: 2.822514	val: 2.972574	test: 2.832234Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/freesolv/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/freesolv/random/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/freesolv/random/train_prop=0.7/freesolv_random_6_26-05_11-05-10  ]
[ Using Seed :  6  ]
[ Using device :  cuda:1  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 28.342415809631348
RMSE train: 5.309966	val: 5.974355	test: 5.302956
MAE train: 4.164354	val: 4.638936	test: 4.189224

Epoch: 2
Loss: 26.533150672912598
RMSE train: 5.139287	val: 5.799602	test: 5.123743
MAE train: 4.021081	val: 4.474381	test: 4.049319

Epoch: 3
Loss: 25.181884765625
RMSE train: 4.997409	val: 5.651448	test: 4.977304
MAE train: 3.919103	val: 4.341811	test: 3.955913

Epoch: 4
Loss: 23.07003879547119
RMSE train: 4.861044	val: 5.505482	test: 4.850316
MAE train: 3.835085	val: 4.227950	test: 3.883651

Epoch: 5
Loss: 21.688063621520996
RMSE train: 4.690534	val: 5.330990	test: 4.712706
MAE train: 3.730979	val: 4.137171	test: 3.790080

Epoch: 6
Loss: 19.6437931060791
RMSE train: 4.470780	val: 5.075987	test: 4.547537
MAE train: 3.601608	val: 4.031756	test: 3.680951

Epoch: 7
Loss: 17.8987979888916
RMSE train: 4.197572	val: 4.701653	test: 4.327996
MAE train: 3.457444	val: 3.846831	test: 3.563996

Epoch: 8
Loss: 16.601388454437256
RMSE train: 3.881969	val: 4.230244	test: 4.003468
MAE train: 3.275651	val: 3.549762	test: 3.372377

Epoch: 9
Loss: 15.236740112304688
RMSE train: 3.640563	val: 3.848450	test: 3.694832
MAE train: 3.113476	val: 3.255835	test: 3.168837

Epoch: 10
Loss: 14.617863655090332
RMSE train: 3.515282	val: 3.666322	test: 3.503363
MAE train: 3.023930	val: 3.100303	test: 3.045175

Epoch: 11
Loss: 13.74255084991455
RMSE train: 3.419182	val: 3.573438	test: 3.388331
MAE train: 2.920356	val: 2.955754	test: 2.936253

Epoch: 12
Loss: 12.935102462768555
RMSE train: 3.290681	val: 3.450979	test: 3.260214
MAE train: 2.783656	val: 2.797556	test: 2.810424

Epoch: 13
Loss: 12.377003192901611
RMSE train: 3.167795	val: 3.326555	test: 3.137592
MAE train: 2.672523	val: 2.684538	test: 2.687247

Epoch: 14
Loss: 12.127579689025879
RMSE train: 3.111225	val: 3.248073	test: 3.059651
MAE train: 2.655654	val: 2.659700	test: 2.653169

Epoch: 15
Loss: 11.422497272491455
RMSE train: 3.200643	val: 3.399023	test: 3.164126
MAE train: 2.797188	val: 2.842573	test: 2.759692

Epoch: 16
Loss: 10.855267524719238
RMSE train: 3.404399	val: 3.716989	test: 3.410331
MAE train: 3.004428	val: 3.172940	test: 2.928477

Epoch: 17
Loss: 10.723303318023682
RMSE train: 3.481637	val: 3.858147	test: 3.505650
MAE train: 3.067691	val: 3.313665	test: 3.007075

Epoch: 18
Loss: 10.511284828186035
RMSE train: 3.425929	val: 3.800352	test: 3.454712
MAE train: 3.036522	val: 3.292001	test: 2.991597

Epoch: 19
Loss: 9.830202579498291
RMSE train: 3.277414	val: 3.612500	test: 3.292845
MAE train: 2.900323	val: 3.127864	test: 2.867213

Epoch: 20
Loss: 9.241971015930176
RMSE train: 3.152091	val: 3.426818	test: 3.158957
MAE train: 2.774952	val: 2.959001	test: 2.752222

Epoch: 21
Loss: 9.231739044189453
RMSE train: 3.087036	val: 3.307586	test: 3.101639
MAE train: 2.707111	val: 2.841301	test: 2.701809

Epoch: 22
Loss: 8.804261207580566
RMSE train: 3.064954	val: 3.284204	test: 3.094876
MAE train: 2.687736	val: 2.792742	test: 2.686297Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/freesolv/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/freesolv/random/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/freesolv/random/train_prop=0.7/freesolv_random_5_26-05_11-05-10  ]
[ Using Seed :  5  ]
[ Using device :  cuda:1  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 29.517797470092773
RMSE train: 5.416478	val: 6.025328	test: 5.479968
MAE train: 4.277753	val: 4.750996	test: 4.346337

Epoch: 2
Loss: 27.283848762512207
RMSE train: 5.288406	val: 5.872044	test: 5.365658
MAE train: 4.194120	val: 4.633156	test: 4.275958

Epoch: 3
Loss: 26.222366333007812
RMSE train: 5.154111	val: 5.698926	test: 5.266464
MAE train: 4.115458	val: 4.504808	test: 4.217905

Epoch: 4
Loss: 24.488466262817383
RMSE train: 5.034278	val: 5.537120	test: 5.196765
MAE train: 4.056366	val: 4.398952	test: 4.194310

Epoch: 5
Loss: 22.035497665405273
RMSE train: 4.906935	val: 5.362276	test: 5.129868
MAE train: 3.994903	val: 4.284533	test: 4.178069

Epoch: 6
Loss: 20.2415771484375
RMSE train: 4.771270	val: 5.206040	test: 5.042990
MAE train: 3.928597	val: 4.180755	test: 4.139571

Epoch: 7
Loss: 18.836709022521973
RMSE train: 4.602786	val: 5.000387	test: 4.911757
MAE train: 3.851575	val: 4.114642	test: 4.074014

Epoch: 8
Loss: 17.455560207366943
RMSE train: 4.371884	val: 4.701848	test: 4.667091
MAE train: 3.763171	val: 3.986186	test: 3.962848

Epoch: 9
Loss: 16.331125259399414
RMSE train: 4.123372	val: 4.355731	test: 4.344270
MAE train: 3.625652	val: 3.775365	test: 3.795704

Epoch: 10
Loss: 15.19070291519165
RMSE train: 3.932124	val: 4.089226	test: 4.057996
MAE train: 3.490893	val: 3.570508	test: 3.615970

Epoch: 11
Loss: 14.31775951385498
RMSE train: 3.834535	val: 3.973788	test: 3.898651
MAE train: 3.408175	val: 3.442683	test: 3.519411

Epoch: 12
Loss: 13.800215244293213
RMSE train: 3.807412	val: 3.982626	test: 3.857475
MAE train: 3.370175	val: 3.406179	test: 3.454577

Epoch: 13
Loss: 13.40717601776123
RMSE train: 3.720448	val: 3.936911	test: 3.770151
MAE train: 3.278709	val: 3.354064	test: 3.347048

Epoch: 14
Loss: 12.259926319122314
RMSE train: 3.614179	val: 3.871133	test: 3.669119
MAE train: 3.174211	val: 3.317792	test: 3.219912

Epoch: 15
Loss: 12.12042236328125
RMSE train: 3.618475	val: 3.927900	test: 3.675190
MAE train: 3.185685	val: 3.382741	test: 3.185455

Epoch: 16
Loss: 11.65932035446167
RMSE train: 3.604340	val: 3.944489	test: 3.659723
MAE train: 3.187920	val: 3.419277	test: 3.160398

Epoch: 17
Loss: 10.831787586212158
RMSE train: 3.579603	val: 3.938820	test: 3.657872
MAE train: 3.157486	val: 3.414112	test: 3.134999

Epoch: 18
Loss: 10.773591995239258
RMSE train: 3.529369	val: 3.878235	test: 3.627849
MAE train: 3.116383	val: 3.367788	test: 3.103867

Epoch: 19
Loss: 10.791490077972412
RMSE train: 3.412536	val: 3.749183	test: 3.528168
MAE train: 3.022426	val: 3.260219	test: 3.033171

Epoch: 20
Loss: 9.45311164855957
RMSE train: 3.287524	val: 3.592767	test: 3.397790
MAE train: 2.925003	val: 3.129277	test: 2.953423

Epoch: 21
Loss: 9.307404041290283
RMSE train: 3.172522	val: 3.460728	test: 3.249779
MAE train: 2.838287	val: 3.015798	test: 2.869951

Epoch: 22
Loss: 9.045018672943115
RMSE train: 3.106876	val: 3.377889	test: 3.171056
MAE train: 2.782463	val: 2.942708	test: 2.816120Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/freesolv/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/freesolv/random/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/freesolv/random/train_prop=0.8/freesolv_random_4_26-05_11-05-10  ]
[ Using Seed :  4  ]
[ Using device :  cuda:2  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 29.19430923461914
RMSE train: 5.469900	val: 5.608343	test: 5.256063
MAE train: 4.339553	val: 4.773234	test: 3.997598

Epoch: 2
Loss: 27.399499893188477
RMSE train: 5.323447	val: 5.453652	test: 5.120753
MAE train: 4.228841	val: 4.609705	test: 3.903807

Epoch: 3
Loss: 25.94700813293457
RMSE train: 5.153396	val: 5.289683	test: 4.978735
MAE train: 4.106283	val: 4.436784	test: 3.814720

Epoch: 4
Loss: 23.97909450531006
RMSE train: 4.977464	val: 5.141757	test: 4.860111
MAE train: 3.994907	val: 4.293778	test: 3.768323

Epoch: 5
Loss: 21.81400489807129
RMSE train: 4.785250	val: 5.008038	test: 4.744635
MAE train: 3.893816	val: 4.189882	test: 3.768132

Epoch: 6
Loss: 20.58042621612549
RMSE train: 4.580294	val: 4.895093	test: 4.629831
MAE train: 3.800674	val: 4.091715	test: 3.813685

Epoch: 7
Loss: 18.696016788482666
RMSE train: 4.370684	val: 4.783018	test: 4.489530
MAE train: 3.704831	val: 3.992630	test: 3.842183

Epoch: 8
Loss: 16.94524383544922
RMSE train: 4.179443	val: 4.637563	test: 4.317065
MAE train: 3.615861	val: 3.918270	test: 3.824333

Epoch: 9
Loss: 15.831063270568848
RMSE train: 4.011719	val: 4.452041	test: 4.104623
MAE train: 3.522580	val: 3.800147	test: 3.706924

Epoch: 10
Loss: 14.904639720916748
RMSE train: 3.853646	val: 4.263440	test: 3.875632
MAE train: 3.413109	val: 3.646590	test: 3.579000

Epoch: 11
Loss: 14.405827522277832
RMSE train: 3.737667	val: 4.150555	test: 3.711523
MAE train: 3.310888	val: 3.559044	test: 3.463405

Epoch: 12
Loss: 13.852327346801758
RMSE train: 3.655858	val: 4.097536	test: 3.597942
MAE train: 3.232110	val: 3.505280	test: 3.353741

Epoch: 13
Loss: 13.197844982147217
RMSE train: 3.638961	val: 4.117773	test: 3.570087
MAE train: 3.219502	val: 3.538664	test: 3.304896

Epoch: 14
Loss: 12.507434844970703
RMSE train: 3.666320	val: 4.201708	test: 3.578092
MAE train: 3.271027	val: 3.639298	test: 3.279250

Epoch: 15
Loss: 12.064096450805664
RMSE train: 3.691001	val: 4.267126	test: 3.595539
MAE train: 3.312347	val: 3.696316	test: 3.240381

Epoch: 16
Loss: 11.437619686126709
RMSE train: 3.696082	val: 4.276793	test: 3.599347
MAE train: 3.315467	val: 3.681569	test: 3.227993

Epoch: 17
Loss: 11.206588745117188
RMSE train: 3.640968	val: 4.224552	test: 3.570463
MAE train: 3.265638	val: 3.632614	test: 3.211902

Epoch: 18
Loss: 10.980165958404541
RMSE train: 3.555081	val: 4.135450	test: 3.494589
MAE train: 3.192727	val: 3.577264	test: 3.163824

Epoch: 19
Loss: 10.084232330322266
RMSE train: 3.478679	val: 4.034634	test: 3.413106
MAE train: 3.128748	val: 3.509657	test: 3.095437

Epoch: 20
Loss: 9.817610263824463
RMSE train: 3.406881	val: 3.940082	test: 3.336874
MAE train: 3.069600	val: 3.435254	test: 3.026373

Epoch: 21
Loss: 9.318967819213867
RMSE train: 3.366079	val: 3.869120	test: 3.290574
MAE train: 3.043708	val: 3.384493	test: 2.982318

Epoch: 22
Loss: 8.941746234893799
RMSE train: 3.334003	val: 3.809525	test: 3.275852
MAE train: 3.026568	val: 3.342836	test: 2.975133

Epoch: 23
Loss: 9.201574802398682
RMSE train: 3.295041	val: 3.162062	test: 3.470532
MAE train: 2.971028	val: 2.841909	test: 3.076398

Epoch: 24
Loss: 9.280941009521484
RMSE train: 3.280655	val: 3.150497	test: 3.457784
MAE train: 2.958524	val: 2.825161	test: 3.056884

Epoch: 25
Loss: 9.175732851028442
RMSE train: 3.292554	val: 3.187781	test: 3.472056
MAE train: 2.962439	val: 2.849042	test: 3.055943

Epoch: 26
Loss: 7.765655040740967
RMSE train: 3.239158	val: 3.175213	test: 3.419535
MAE train: 2.904367	val: 2.823195	test: 2.990784

Epoch: 27
Loss: 7.8222198486328125
RMSE train: 3.172648	val: 3.120135	test: 3.344902
MAE train: 2.845956	val: 2.776361	test: 2.925966

Epoch: 28
Loss: 6.795420169830322
RMSE train: 3.064788	val: 3.009482	test: 3.217961
MAE train: 2.760629	val: 2.687603	test: 2.834361

Epoch: 29
Loss: 7.15892767906189
RMSE train: 2.939943	val: 2.883742	test: 3.090827
MAE train: 2.657348	val: 2.580045	test: 2.732757

Epoch: 30
Loss: 6.5783679485321045
RMSE train: 2.858440	val: 2.819338	test: 3.024024
MAE train: 2.588872	val: 2.538225	test: 2.681684

Epoch: 31
Loss: 6.14026141166687
RMSE train: 2.754779	val: 2.732377	test: 2.936346
MAE train: 2.493483	val: 2.458802	test: 2.599415

Epoch: 32
Loss: 5.789667844772339
RMSE train: 2.679505	val: 2.649391	test: 2.872607
MAE train: 2.425273	val: 2.379155	test: 2.544597

Epoch: 33
Loss: 5.3053038120269775
RMSE train: 2.601586	val: 2.559120	test: 2.808032
MAE train: 2.351829	val: 2.288288	test: 2.481028

Epoch: 34
Loss: 5.299965858459473
RMSE train: 2.565382	val: 2.522533	test: 2.787569
MAE train: 2.300404	val: 2.234947	test: 2.435054

Epoch: 35
Loss: 5.38507342338562
RMSE train: 2.535153	val: 2.495211	test: 2.778048
MAE train: 2.248949	val: 2.190304	test: 2.384742

Epoch: 36
Loss: 4.9214630126953125
RMSE train: 2.478197	val: 2.435713	test: 2.720979
MAE train: 2.183435	val: 2.123338	test: 2.319068

Epoch: 37
Loss: 3.951499104499817
RMSE train: 2.421232	val: 2.351195	test: 2.647109
MAE train: 2.143274	val: 2.059028	test: 2.272361

Epoch: 38
Loss: 4.012519359588623
RMSE train: 2.373407	val: 2.256426	test: 2.562763
MAE train: 2.126751	val: 1.994827	test: 2.235839

Epoch: 39
Loss: 3.8530479669570923
RMSE train: 2.336763	val: 2.201294	test: 2.495988
MAE train: 2.108605	val: 1.953815	test: 2.199290

Epoch: 40
Loss: 3.408095598220825
RMSE train: 2.263790	val: 2.148076	test: 2.426813
MAE train: 2.039711	val: 1.903578	test: 2.129132

Epoch: 41
Loss: 3.5903366804122925
RMSE train: 2.184746	val: 2.118922	test: 2.374466
MAE train: 1.952180	val: 1.856295	test: 2.048814

Epoch: 42
Loss: 2.8840545415878296
RMSE train: 2.094289	val: 2.062249	test: 2.299459
MAE train: 1.852188	val: 1.776720	test: 1.951779

Epoch: 43
Loss: 2.9393362998962402
RMSE train: 2.021458	val: 2.001401	test: 2.230190
MAE train: 1.776448	val: 1.704702	test: 1.875254

Epoch: 44
Loss: 2.6222327947616577
RMSE train: 1.937845	val: 1.907553	test: 2.147986
MAE train: 1.688205	val: 1.599648	test: 1.785023

Epoch: 45
Loss: 2.490905284881592
RMSE train: 1.862135	val: 1.827121	test: 2.079508
MAE train: 1.612582	val: 1.523966	test: 1.715142

Epoch: 46
Loss: 3.1786025762557983
RMSE train: 1.787652	val: 1.780272	test: 2.016718
MAE train: 1.551646	val: 1.487478	test: 1.660074

Epoch: 47
Loss: 2.4244930148124695
RMSE train: 1.580083	val: 1.636350	test: 1.833334
MAE train: 1.361423	val: 1.359088	test: 1.490204

Epoch: 48
Loss: 1.9514706134796143
RMSE train: 1.467006	val: 1.554790	test: 1.729732
MAE train: 1.250924	val: 1.284436	test: 1.384616

Epoch: 49
Loss: 1.9933029413223267
RMSE train: 1.377442	val: 1.468348	test: 1.653255
MAE train: 1.176273	val: 1.208069	test: 1.319824

Epoch: 50
Loss: 2.4594539403915405
RMSE train: 1.350072	val: 1.432516	test: 1.633243
MAE train: 1.142522	val: 1.159691	test: 1.302901

Epoch: 51
Loss: 1.7791545987129211
RMSE train: 1.304620	val: 1.401956	test: 1.607699
MAE train: 1.087124	val: 1.127565	test: 1.270938

Epoch: 52
Loss: 2.075441360473633
RMSE train: 1.322892	val: 1.425140	test: 1.644853
MAE train: 1.084388	val: 1.146719	test: 1.274152

Epoch: 53
Loss: 1.6186309456825256
RMSE train: 1.244727	val: 1.386404	test: 1.594536
MAE train: 0.997294	val: 1.089373	test: 1.192630

Epoch: 54
Loss: 1.4727331399917603
RMSE train: 1.151662	val: 1.314032	test: 1.517206
MAE train: 0.928744	val: 1.012816	test: 1.120762

Epoch: 55
Loss: 1.3018880486488342
RMSE train: 1.111715	val: 1.274795	test: 1.486778
MAE train: 0.906334	val: 0.977639	test: 1.097363

Epoch: 56
Loss: 1.552946150302887
RMSE train: 1.017955	val: 1.206705	test: 1.417989
MAE train: 0.816744	val: 0.912252	test: 1.027751

Epoch: 57
Loss: 1.665776014328003
RMSE train: 0.974376	val: 1.194304	test: 1.388099
MAE train: 0.754087	val: 0.875947	test: 0.977069

Epoch: 58
Loss: 1.1017305254936218
RMSE train: 0.951007	val: 1.190557	test: 1.350801
MAE train: 0.726587	val: 0.869825	test: 0.945329

Epoch: 59
Loss: 1.2342746257781982
RMSE train: 0.916060	val: 1.132913	test: 1.311097
MAE train: 0.711981	val: 0.854716	test: 0.939236

Epoch: 60
Loss: 1.1215267777442932
RMSE train: 0.949432	val: 1.115567	test: 1.342966
MAE train: 0.756353	val: 0.856346	test: 0.999482

Epoch: 61
Loss: 1.193945586681366
RMSE train: 0.983536	val: 1.110188	test: 1.368568
MAE train: 0.787973	val: 0.869500	test: 1.026109

Epoch: 62
Loss: 1.3414050936698914
RMSE train: 1.001977	val: 1.101997	test: 1.402565
MAE train: 0.789066	val: 0.868501	test: 1.022785

Epoch: 63
Loss: 1.841978371143341
RMSE train: 0.993092	val: 1.145781	test: 1.401524
MAE train: 0.768826	val: 0.868764	test: 0.990480

Epoch: 64
Loss: 0.9975082874298096
RMSE train: 1.016614	val: 1.237951	test: 1.403913
MAE train: 0.771151	val: 0.884226	test: 0.970178

Epoch: 65
Loss: 1.2845803499221802
RMSE train: 0.985584	val: 1.232783	test: 1.361340
MAE train: 0.750282	val: 0.885672	test: 0.952182

Epoch: 66
Loss: 1.3099621534347534
RMSE train: 0.899222	val: 1.161409	test: 1.302492
MAE train: 0.709863	val: 0.876256	test: 0.936722

Epoch: 67
Loss: 1.101975679397583
RMSE train: 0.914143	val: 1.170727	test: 1.316599
MAE train: 0.731739	val: 0.916028	test: 0.977351

Epoch: 68
Loss: 1.0225498378276825
RMSE train: 0.927859	val: 1.182171	test: 1.332455
MAE train: 0.746631	val: 0.925466	test: 0.988831

Epoch: 69
Loss: 1.0262903571128845
RMSE train: 0.911035	val: 1.169757	test: 1.332824
MAE train: 0.706887	val: 0.898982	test: 0.968558

Epoch: 70
Loss: 0.9566965103149414
RMSE train: 0.866289	val: 1.137064	test: 1.303599
MAE train: 0.659307	val: 0.840131	test: 0.914331

Epoch: 71
Loss: 1.0896291136741638
RMSE train: 0.792781	val: 1.077056	test: 1.245530
MAE train: 0.606942	val: 0.778864	test: 0.856868

Epoch: 72
Loss: 0.9623679518699646
RMSE train: 0.752580	val: 1.041187	test: 1.221863
MAE train: 0.590103	val: 0.762367	test: 0.849295

Epoch: 73
Loss: 0.9635654091835022
RMSE train: 0.749971	val: 1.040695	test: 1.221670
MAE train: 0.602660	val: 0.776188	test: 0.873511

Epoch: 74
Loss: 0.8069237172603607
RMSE train: 0.769833	val: 1.054973	test: 1.238519
MAE train: 0.633562	val: 0.805064	test: 0.901873

Epoch: 75
Loss: 0.9110480546951294
RMSE train: 0.798982	val: 1.070927	test: 1.275759
MAE train: 0.650092	val: 0.810393	test: 0.910812

Epoch: 76
Loss: 0.8235625922679901
RMSE train: 0.871751	val: 1.119358	test: 1.355355
MAE train: 0.686483	val: 0.839362	test: 0.942751

Epoch: 77
Loss: 0.9582355916500092
RMSE train: 0.909909	val: 1.128011	test: 1.403336
MAE train: 0.702743	val: 0.839435	test: 0.964094

Epoch: 78
Loss: 0.9913964867591858
RMSE train: 0.929646	val: 1.138732	test: 1.414266
MAE train: 0.701924	val: 0.830094	test: 0.959977

Epoch: 79
Loss: 0.7691465318202972
RMSE train: 0.913041	val: 1.149428	test: 1.384681
MAE train: 0.679201	val: 0.828686	test: 0.937311

Epoch: 80
Loss: 0.90312260389328
RMSE train: 0.866470	val: 1.118038	test: 1.335445
MAE train: 0.654771	val: 0.814536	test: 0.924884

Epoch: 81
Loss: 0.9190132021903992
RMSE train: 0.841657	val: 1.077932	test: 1.314667
MAE train: 0.650092	val: 0.818192	test: 0.944157

Epoch: 82
Loss: 0.924788236618042
RMSE train: 0.815334	val: 1.056804	test: 1.287969
MAE train: 0.642099	val: 0.822035	test: 0.947565

Epoch: 83
Loss: 1.0405510067939758
RMSE train: 0.791091	val: 1.058407	test: 1.270745
MAE train: 0.630299	val: 0.816731	test: 0.927684

Epoch: 23
Loss: 9.243854999542236
RMSE train: 3.207545	val: 3.102818	test: 3.355098
MAE train: 2.906504	val: 2.818444	test: 2.986647

Epoch: 24
Loss: 8.915840148925781
RMSE train: 3.259326	val: 3.156603	test: 3.422482
MAE train: 2.944656	val: 2.864106	test: 3.033735

Epoch: 25
Loss: 9.414763450622559
RMSE train: 3.286027	val: 3.190197	test: 3.460355
MAE train: 2.964303	val: 2.879174	test: 3.044760

Epoch: 26
Loss: 8.343774795532227
RMSE train: 3.251688	val: 3.111113	test: 3.428674
MAE train: 2.920805	val: 2.793240	test: 2.994093

Epoch: 27
Loss: 7.334010601043701
RMSE train: 3.153837	val: 2.976819	test: 3.325192
MAE train: 2.823744	val: 2.660596	test: 2.897665

Epoch: 28
Loss: 6.936952352523804
RMSE train: 3.034946	val: 2.848293	test: 3.196823
MAE train: 2.712510	val: 2.541940	test: 2.783439

Epoch: 29
Loss: 6.911807537078857
RMSE train: 2.928307	val: 2.763609	test: 3.079619
MAE train: 2.616868	val: 2.458683	test: 2.681933

Epoch: 30
Loss: 6.262317657470703
RMSE train: 2.806248	val: 2.642167	test: 2.945615
MAE train: 2.507663	val: 2.339254	test: 2.563725

Epoch: 31
Loss: 6.074079275131226
RMSE train: 2.674026	val: 2.543898	test: 2.805138
MAE train: 2.386929	val: 2.231918	test: 2.431069

Epoch: 32
Loss: 5.1870715618133545
RMSE train: 2.508951	val: 2.390625	test: 2.652511
MAE train: 2.224111	val: 2.084430	test: 2.271168

Epoch: 33
Loss: 7.066407918930054
RMSE train: 2.422782	val: 2.291739	test: 2.582007
MAE train: 2.129309	val: 1.992323	test: 2.183705

Epoch: 34
Loss: 5.38744854927063
RMSE train: 2.365075	val: 2.194615	test: 2.538672
MAE train: 2.064790	val: 1.913998	test: 2.131045

Epoch: 35
Loss: 4.973465442657471
RMSE train: 2.370478	val: 2.159638	test: 2.551639
MAE train: 2.061988	val: 1.887143	test: 2.135415

Epoch: 36
Loss: 4.592227935791016
RMSE train: 2.338020	val: 2.119442	test: 2.528847
MAE train: 2.024573	val: 1.849698	test: 2.116975

Epoch: 37
Loss: 4.426635265350342
RMSE train: 2.315308	val: 2.136954	test: 2.519490
MAE train: 2.012356	val: 1.858715	test: 2.112506

Epoch: 38
Loss: 3.9388445615768433
RMSE train: 2.230027	val: 2.083367	test: 2.435144
MAE train: 1.943251	val: 1.795789	test: 2.030183

Epoch: 39
Loss: 3.577462911605835
RMSE train: 2.133545	val: 2.039393	test: 2.330329
MAE train: 1.853185	val: 1.730775	test: 1.923471

Epoch: 40
Loss: 3.294676899909973
RMSE train: 2.067421	val: 2.047225	test: 2.258774
MAE train: 1.788990	val: 1.710016	test: 1.848309

Epoch: 41
Loss: 3.2605485916137695
RMSE train: 1.998711	val: 2.023432	test: 2.191102
MAE train: 1.710521	val: 1.664435	test: 1.763333

Epoch: 42
Loss: 2.978602409362793
RMSE train: 1.830005	val: 1.801431	test: 2.038987
MAE train: 1.574610	val: 1.509080	test: 1.641523

Epoch: 43
Loss: 2.694678544998169
RMSE train: 1.667445	val: 1.572692	test: 1.877522
MAE train: 1.433110	val: 1.330192	test: 1.520887

Epoch: 44
Loss: 2.557815194129944
RMSE train: 1.581429	val: 1.432758	test: 1.785145
MAE train: 1.347375	val: 1.207511	test: 1.446568

Epoch: 45
Loss: 2.384464204311371
RMSE train: 1.531970	val: 1.369002	test: 1.718926
MAE train: 1.304191	val: 1.157277	test: 1.409866

Epoch: 46
Loss: 2.2161266803741455
RMSE train: 1.501500	val: 1.384743	test: 1.710141
MAE train: 1.273492	val: 1.151543	test: 1.377700

Epoch: 47
Loss: 2.4090607166290283
RMSE train: 1.434317	val: 1.394069	test: 1.688095
MAE train: 1.184302	val: 1.122025	test: 1.298140

Epoch: 48
Loss: 2.007595658302307
RMSE train: 1.367883	val: 1.358123	test: 1.655175
MAE train: 1.123078	val: 1.072343	test: 1.237576

Epoch: 49
Loss: 1.7547969222068787
RMSE train: 1.302408	val: 1.284028	test: 1.612871
MAE train: 1.064446	val: 1.002262	test: 1.185658

Epoch: 50
Loss: 2.2870973348617554
RMSE train: 1.292251	val: 1.260544	test: 1.576650
MAE train: 1.065365	val: 0.978942	test: 1.181345

Epoch: 51
Loss: 1.8598885536193848
RMSE train: 1.297264	val: 1.262801	test: 1.567080
MAE train: 1.087456	val: 0.997277	test: 1.202847

Epoch: 52
Loss: 1.5257497429847717
RMSE train: 1.279652	val: 1.269566	test: 1.593308
MAE train: 1.060291	val: 1.016244	test: 1.214054

Epoch: 53
Loss: 1.5605484247207642
RMSE train: 1.249529	val: 1.275781	test: 1.610515
MAE train: 1.012529	val: 1.011519	test: 1.182194

Epoch: 54
Loss: 1.6404268145561218
RMSE train: 1.177002	val: 1.224179	test: 1.553156
MAE train: 0.941374	val: 0.968378	test: 1.116156

Epoch: 55
Loss: 1.4272198677062988
RMSE train: 1.118145	val: 1.161895	test: 1.480435
MAE train: 0.901021	val: 0.926968	test: 1.081290

Epoch: 56
Loss: 1.3453842997550964
RMSE train: 1.074421	val: 1.090953	test: 1.410689
MAE train: 0.871168	val: 0.869574	test: 1.055777

Epoch: 57
Loss: 1.442952811717987
RMSE train: 1.024797	val: 1.033122	test: 1.348179
MAE train: 0.829276	val: 0.815552	test: 1.025674

Epoch: 58
Loss: 1.1742926239967346
RMSE train: 0.974800	val: 1.002158	test: 1.299284
MAE train: 0.785476	val: 0.787068	test: 0.981459

Epoch: 59
Loss: 1.1782471537590027
RMSE train: 0.977081	val: 1.012157	test: 1.312514
MAE train: 0.786856	val: 0.805371	test: 0.972632

Epoch: 60
Loss: 1.0559104084968567
RMSE train: 1.011751	val: 1.033340	test: 1.347532
MAE train: 0.819317	val: 0.832726	test: 0.997278

Epoch: 61
Loss: 1.3061875104904175
RMSE train: 1.043709	val: 1.049944	test: 1.383933
MAE train: 0.848052	val: 0.850721	test: 1.021303

Epoch: 62
Loss: 1.0799006223678589
RMSE train: 1.036858	val: 1.056863	test: 1.383771
MAE train: 0.838984	val: 0.856219	test: 1.004642

Epoch: 63
Loss: 1.2490635514259338
RMSE train: 0.948753	val: 1.008943	test: 1.332207
MAE train: 0.743974	val: 0.793616	test: 0.927565

Epoch: 64
Loss: 1.1811299920082092
RMSE train: 0.846538	val: 0.965822	test: 1.256939
MAE train: 0.631785	val: 0.731798	test: 0.839787

Epoch: 65
Loss: 1.1532330513000488
RMSE train: 0.791766	val: 0.947884	test: 1.195896
MAE train: 0.587740	val: 0.703649	test: 0.806612

Epoch: 66
Loss: 1.1596118807792664
RMSE train: 0.841223	val: 1.003844	test: 1.215770
MAE train: 0.633637	val: 0.742600	test: 0.828645

Epoch: 67
Loss: 1.3142118453979492
RMSE train: 0.947136	val: 1.092863	test: 1.296159
MAE train: 0.734501	val: 0.816946	test: 0.911987

Epoch: 68
Loss: 0.9111844599246979
RMSE train: 1.006095	val: 1.120410	test: 1.345966
MAE train: 0.808991	val: 0.862926	test: 0.993783

Epoch: 69
Loss: 1.0794470310211182
RMSE train: 1.020753	val: 1.118193	test: 1.375643
MAE train: 0.840878	val: 0.888076	test: 1.051243

Epoch: 70
Loss: 0.9410712420940399
RMSE train: 0.963164	val: 1.087213	test: 1.339011
MAE train: 0.787979	val: 0.855162	test: 1.012921

Epoch: 71
Loss: 1.0645110607147217
RMSE train: 0.873214	val: 1.030919	test: 1.278422
MAE train: 0.693622	val: 0.789596	test: 0.924918

Epoch: 72
Loss: 0.8868339359760284
RMSE train: 0.775958	val: 0.965896	test: 1.199294
MAE train: 0.586598	val: 0.718990	test: 0.822876

Epoch: 73
Loss: 1.0057828426361084
RMSE train: 0.732518	val: 0.925193	test: 1.134317
MAE train: 0.540170	val: 0.677770	test: 0.766120

Epoch: 74
Loss: 1.1117433905601501
RMSE train: 0.754579	val: 0.920680	test: 1.114013
MAE train: 0.559535	val: 0.678760	test: 0.766937

Epoch: 75
Loss: 1.038360059261322
RMSE train: 0.810128	val: 0.942660	test: 1.138421
MAE train: 0.607473	val: 0.701711	test: 0.790155

Epoch: 76
Loss: 0.9907725751399994
RMSE train: 0.860372	val: 0.951420	test: 1.170506
MAE train: 0.657568	val: 0.715938	test: 0.813684

Epoch: 77
Loss: 1.2210325598716736
RMSE train: 0.867232	val: 0.963848	test: 1.201920
MAE train: 0.682387	val: 0.738759	test: 0.857215

Epoch: 78
Loss: 0.992711752653122
RMSE train: 0.872655	val: 0.985021	test: 1.228455
MAE train: 0.693547	val: 0.760134	test: 0.887122

Epoch: 79
Loss: 0.921969085931778
RMSE train: 0.881706	val: 0.998553	test: 1.249366
MAE train: 0.699128	val: 0.766882	test: 0.905125

Epoch: 80
Loss: 0.88203164935112
RMSE train: 0.856115	val: 0.995562	test: 1.234043
MAE train: 0.683030	val: 0.760204	test: 0.901646

Epoch: 81
Loss: 0.9431589543819427
RMSE train: 0.858142	val: 1.026377	test: 1.242689
MAE train: 0.676159	val: 0.774897	test: 0.902701

Epoch: 82
Loss: 0.9579764306545258
RMSE train: 0.882991	val: 1.077126	test: 1.257202
MAE train: 0.695754	val: 0.795518	test: 0.899847

Epoch: 83
Loss: 0.9818613529205322
RMSE train: 0.926581	val: 1.116306	test: 1.300117
MAE train: 0.728256	val: 0.810020	test: 0.906025

Epoch: 23
Loss: 9.6816987991333
RMSE train: 3.213308	val: 2.984851	test: 3.363802
MAE train: 2.925340	val: 2.721576	test: 3.009032

Epoch: 24
Loss: 9.196008682250977
RMSE train: 3.212232	val: 3.011140	test: 3.386461
MAE train: 2.929108	val: 2.740103	test: 3.007782

Epoch: 25
Loss: 8.922248840332031
RMSE train: 3.200306	val: 3.017194	test: 3.390040
MAE train: 2.911750	val: 2.731503	test: 2.983059

Epoch: 26
Loss: 8.510178089141846
RMSE train: 3.189738	val: 3.030102	test: 3.391413
MAE train: 2.889772	val: 2.724001	test: 2.956652

Epoch: 27
Loss: 7.948906421661377
RMSE train: 3.160525	val: 3.033414	test: 3.368660
MAE train: 2.847635	val: 2.698692	test: 2.911261

Epoch: 28
Loss: 7.792422294616699
RMSE train: 3.101272	val: 2.983240	test: 3.322287
MAE train: 2.771282	val: 2.630495	test: 2.844174

Epoch: 29
Loss: 7.257969856262207
RMSE train: 3.009823	val: 2.849538	test: 3.241589
MAE train: 2.671941	val: 2.507402	test: 2.770037

Epoch: 30
Loss: 7.259956121444702
RMSE train: 2.849375	val: 2.649433	test: 3.088230
MAE train: 2.510460	val: 2.337319	test: 2.635824

Epoch: 31
Loss: 6.629403829574585
RMSE train: 2.729225	val: 2.521458	test: 2.965268
MAE train: 2.407235	val: 2.237703	test: 2.542141

Epoch: 32
Loss: 5.740117788314819
RMSE train: 2.625890	val: 2.435073	test: 2.852181
MAE train: 2.332228	val: 2.181499	test: 2.463299

Epoch: 33
Loss: 5.710661888122559
RMSE train: 2.568328	val: 2.404794	test: 2.785563
MAE train: 2.297015	val: 2.171369	test: 2.420651

Epoch: 34
Loss: 5.396999835968018
RMSE train: 2.477406	val: 2.347869	test: 2.683907
MAE train: 2.218320	val: 2.122775	test: 2.353008

Epoch: 35
Loss: 5.2041707038879395
RMSE train: 2.394569	val: 2.275794	test: 2.587996
MAE train: 2.143292	val: 2.052720	test: 2.271789

Epoch: 36
Loss: 4.548672795295715
RMSE train: 2.286745	val: 2.184017	test: 2.491216
MAE train: 2.035563	val: 1.950734	test: 2.162051

Epoch: 37
Loss: 4.261311054229736
RMSE train: 2.239887	val: 2.153887	test: 2.438836
MAE train: 1.993787	val: 1.902607	test: 2.103361

Epoch: 38
Loss: 4.179088830947876
RMSE train: 2.265806	val: 2.165245	test: 2.470291
MAE train: 2.017834	val: 1.918325	test: 2.128174

Epoch: 39
Loss: 4.083214163780212
RMSE train: 2.306608	val: 2.214411	test: 2.516681
MAE train: 2.054516	val: 1.965002	test: 2.168158

Epoch: 40
Loss: 4.4960925579071045
RMSE train: 2.324524	val: 2.265816	test: 2.537403
MAE train: 2.059055	val: 1.994126	test: 2.176960

Epoch: 41
Loss: 3.314916729927063
RMSE train: 2.196648	val: 2.162325	test: 2.398825
MAE train: 1.937143	val: 1.888187	test: 2.053243

Epoch: 42
Loss: 3.3270405530929565
RMSE train: 2.065813	val: 2.043386	test: 2.268350
MAE train: 1.822933	val: 1.780812	test: 1.941372

Epoch: 43
Loss: 3.0141518115997314
RMSE train: 1.895559	val: 1.861504	test: 2.111258
MAE train: 1.663326	val: 1.619577	test: 1.794678

Epoch: 44
Loss: 2.892857789993286
RMSE train: 1.761797	val: 1.724282	test: 2.002963
MAE train: 1.530955	val: 1.483614	test: 1.667679

Epoch: 45
Loss: 2.822966456413269
RMSE train: 1.663581	val: 1.644155	test: 1.927655
MAE train: 1.426406	val: 1.402367	test: 1.576071

Epoch: 46
Loss: 2.8370524644851685
RMSE train: 1.653228	val: 1.623994	test: 1.927857
MAE train: 1.411771	val: 1.387419	test: 1.570090

Epoch: 47
Loss: 2.438058614730835
RMSE train: 1.649700	val: 1.609674	test: 1.924509
MAE train: 1.410973	val: 1.385378	test: 1.571781

Epoch: 48
Loss: 2.6323935985565186
RMSE train: 1.641489	val: 1.595539	test: 1.912666
MAE train: 1.403689	val: 1.382681	test: 1.562868

Epoch: 49
Loss: 2.0609244108200073
RMSE train: 1.593392	val: 1.559246	test: 1.876253
MAE train: 1.364242	val: 1.346726	test: 1.512790

Epoch: 50
Loss: 2.251781702041626
RMSE train: 1.549806	val: 1.552417	test: 1.840822
MAE train: 1.338722	val: 1.332771	test: 1.474100

Epoch: 51
Loss: 1.71511971950531
RMSE train: 1.401918	val: 1.455270	test: 1.702933
MAE train: 1.202443	val: 1.212269	test: 1.344280

Epoch: 52
Loss: 1.8756386637687683
RMSE train: 1.277810	val: 1.342925	test: 1.573526
MAE train: 1.085617	val: 1.095640	test: 1.225198

Epoch: 53
Loss: 1.682712972164154
RMSE train: 1.203638	val: 1.298894	test: 1.524352
MAE train: 1.010550	val: 1.026808	test: 1.158362

Epoch: 54
Loss: 1.7641710042953491
RMSE train: 1.189352	val: 1.272173	test: 1.553820
MAE train: 0.981584	val: 0.989907	test: 1.146519

Epoch: 55
Loss: 1.2821316719055176
RMSE train: 1.206962	val: 1.245647	test: 1.593205
MAE train: 0.988961	val: 0.982893	test: 1.162141

Epoch: 56
Loss: 1.9845234751701355
RMSE train: 1.230141	val: 1.228360	test: 1.617449
MAE train: 1.016334	val: 0.995648	test: 1.199671

Epoch: 57
Loss: 1.661421000957489
RMSE train: 1.230187	val: 1.229634	test: 1.592352
MAE train: 1.040341	val: 1.019955	test: 1.237223

Epoch: 58
Loss: 1.2804411053657532
RMSE train: 1.181194	val: 1.181956	test: 1.507862
MAE train: 0.991413	val: 0.970985	test: 1.190948

Epoch: 59
Loss: 1.3719605505466461
RMSE train: 1.135994	val: 1.120561	test: 1.445866
MAE train: 0.914109	val: 0.905475	test: 1.115714

Epoch: 60
Loss: 1.386232852935791
RMSE train: 1.117273	val: 1.117165	test: 1.433655
MAE train: 0.901929	val: 0.901733	test: 1.106255

Epoch: 61
Loss: 1.15187269449234
RMSE train: 1.064130	val: 1.116648	test: 1.425911
MAE train: 0.876568	val: 0.905363	test: 1.101016

Epoch: 62
Loss: 1.1531500816345215
RMSE train: 0.993084	val: 1.096042	test: 1.430959
MAE train: 0.828230	val: 0.875447	test: 1.069995

Epoch: 63
Loss: 1.082076370716095
RMSE train: 0.960331	val: 1.074533	test: 1.453318
MAE train: 0.776701	val: 0.826610	test: 1.012303

Epoch: 64
Loss: 1.1226367354393005
RMSE train: 0.910122	val: 1.033246	test: 1.450611
MAE train: 0.686428	val: 0.756575	test: 0.932100

Epoch: 65
Loss: 1.3230164051055908
RMSE train: 0.849302	val: 1.008915	test: 1.424907
MAE train: 0.634158	val: 0.737323	test: 0.900971

Epoch: 66
Loss: 1.1031613647937775
RMSE train: 0.825037	val: 0.997902	test: 1.389097
MAE train: 0.636729	val: 0.740508	test: 0.905572

Epoch: 67
Loss: 1.2802549004554749
RMSE train: 0.862335	val: 1.044463	test: 1.382715
MAE train: 0.697578	val: 0.797354	test: 0.957759

Epoch: 68
Loss: 1.174239158630371
RMSE train: 0.949621	val: 1.110240	test: 1.398448
MAE train: 0.799072	val: 0.903196	test: 1.038704

Epoch: 69
Loss: 1.0349372923374176
RMSE train: 1.003422	val: 1.137279	test: 1.415423
MAE train: 0.841796	val: 0.932101	test: 1.074180

Epoch: 70
Loss: 1.02662992477417
RMSE train: 0.954559	val: 1.095035	test: 1.375741
MAE train: 0.775375	val: 0.877075	test: 1.021016

Epoch: 71
Loss: 0.8758425414562225
RMSE train: 0.894144	val: 1.073364	test: 1.360635
MAE train: 0.717347	val: 0.837259	test: 0.968497

Epoch: 72
Loss: 1.0718799233436584
RMSE train: 0.884782	val: 1.064793	test: 1.379223
MAE train: 0.703484	val: 0.818849	test: 0.948525

Epoch: 73
Loss: 0.9945128560066223
RMSE train: 0.906074	val: 1.051908	test: 1.413573
MAE train: 0.714624	val: 0.813963	test: 0.959674

Epoch: 74
Loss: 1.0582054257392883
RMSE train: 0.928616	val: 1.054281	test: 1.451977
MAE train: 0.724529	val: 0.812755	test: 0.973667

Epoch: 75
Loss: 1.022600919008255
RMSE train: 0.913708	val: 1.037412	test: 1.433037
MAE train: 0.719163	val: 0.812050	test: 0.969325

Epoch: 76
Loss: 0.8843617737293243
RMSE train: 0.866648	val: 0.994360	test: 1.386240
MAE train: 0.687766	val: 0.770615	test: 0.938408

Epoch: 77
Loss: 0.8623502850532532
RMSE train: 0.810543	val: 0.959455	test: 1.352527
MAE train: 0.643296	val: 0.738409	test: 0.897275

Epoch: 78
Loss: 0.9010254144668579
RMSE train: 0.778961	val: 0.934701	test: 1.327410
MAE train: 0.611400	val: 0.719671	test: 0.868170

Epoch: 79
Loss: 1.048477679491043
RMSE train: 0.779233	val: 0.941979	test: 1.334704
MAE train: 0.603226	val: 0.715159	test: 0.860550

Epoch: 80
Loss: 1.1690516471862793
RMSE train: 0.766921	val: 0.955253	test: 1.324285
MAE train: 0.585439	val: 0.700780	test: 0.845855

Epoch: 81
Loss: 0.99022376537323
RMSE train: 0.748372	val: 0.995182	test: 1.310809
MAE train: 0.566099	val: 0.712106	test: 0.831613

Epoch: 82
Loss: 0.9106044173240662
RMSE train: 0.726935	val: 0.996924	test: 1.269615
MAE train: 0.560293	val: 0.716760	test: 0.828194

Epoch: 83
Loss: 1.0885302424430847
RMSE train: 0.771891	val: 1.030559	test: 1.261716
MAE train: 0.602942	val: 0.752824	test: 0.860928

Epoch: 23
Loss: 8.117016792297363
RMSE train: 3.066273	val: 3.261557	test: 3.085568
MAE train: 2.769007	val: 2.907566	test: 2.778634

Epoch: 24
Loss: 7.660351514816284
RMSE train: 2.980124	val: 3.217723	test: 2.999410
MAE train: 2.677648	val: 2.842582	test: 2.687579

Epoch: 25
Loss: 7.406495571136475
RMSE train: 2.884213	val: 3.119682	test: 2.903186
MAE train: 2.583558	val: 2.737878	test: 2.599217

Epoch: 26
Loss: 7.1233885288238525
RMSE train: 2.793323	val: 3.008397	test: 2.800849
MAE train: 2.499277	val: 2.629480	test: 2.514783

Epoch: 27
Loss: 6.817229747772217
RMSE train: 2.693496	val: 2.864993	test: 2.694259
MAE train: 2.402857	val: 2.495047	test: 2.416505

Epoch: 28
Loss: 6.429502964019775
RMSE train: 2.579464	val: 2.772043	test: 2.577252
MAE train: 2.287088	val: 2.389120	test: 2.297486

Epoch: 29
Loss: 6.075402021408081
RMSE train: 2.528361	val: 2.766445	test: 2.524611
MAE train: 2.238410	val: 2.372113	test: 2.239129

Epoch: 30
Loss: 5.718337535858154
RMSE train: 2.510659	val: 2.804386	test: 2.503062
MAE train: 2.217146	val: 2.388193	test: 2.206728

Epoch: 31
Loss: 5.141676902770996
RMSE train: 2.483022	val: 2.785389	test: 2.480427
MAE train: 2.185126	val: 2.362776	test: 2.171932

Epoch: 32
Loss: 5.221813678741455
RMSE train: 2.474160	val: 2.767541	test: 2.482934
MAE train: 2.182546	val: 2.357438	test: 2.178416

Epoch: 33
Loss: 4.806214332580566
RMSE train: 2.436319	val: 2.686642	test: 2.466071
MAE train: 2.151199	val: 2.301501	test: 2.169353

Epoch: 34
Loss: 4.519922256469727
RMSE train: 2.376907	val: 2.603215	test: 2.417887
MAE train: 2.103997	val: 2.233118	test: 2.128839

Epoch: 35
Loss: 4.151772737503052
RMSE train: 2.311813	val: 2.515009	test: 2.352087
MAE train: 2.040631	val: 2.148501	test: 2.072731

Epoch: 36
Loss: 3.8827924728393555
RMSE train: 2.193823	val: 2.378381	test: 2.234376
MAE train: 1.931052	val: 2.023226	test: 1.961367

Epoch: 37
Loss: 3.890872359275818
RMSE train: 2.075995	val: 2.276822	test: 2.128301
MAE train: 1.829771	val: 1.929016	test: 1.845511

Epoch: 38
Loss: 3.3209166526794434
RMSE train: 1.985986	val: 2.232066	test: 2.054603
MAE train: 1.742886	val: 1.880869	test: 1.763894

Epoch: 39
Loss: 3.2050364017486572
RMSE train: 1.926326	val: 2.182665	test: 2.009735
MAE train: 1.672757	val: 1.837351	test: 1.711764

Epoch: 40
Loss: 2.9463306665420532
RMSE train: 1.879328	val: 2.160510	test: 1.958728
MAE train: 1.615747	val: 1.805263	test: 1.669097

Epoch: 41
Loss: 3.329984426498413
RMSE train: 1.841476	val: 2.164399	test: 1.911292
MAE train: 1.569494	val: 1.788700	test: 1.644376

Epoch: 42
Loss: 3.053339123725891
RMSE train: 1.777318	val: 2.089122	test: 1.825640
MAE train: 1.517529	val: 1.716487	test: 1.588695

Epoch: 43
Loss: 2.3935540914535522
RMSE train: 1.720514	val: 2.014217	test: 1.766029
MAE train: 1.475602	val: 1.651528	test: 1.544189

Epoch: 44
Loss: 2.336551308631897
RMSE train: 1.651150	val: 1.913334	test: 1.699722
MAE train: 1.422445	val: 1.571878	test: 1.484235

Epoch: 45
Loss: 2.111326277256012
RMSE train: 1.617244	val: 1.821233	test: 1.697511
MAE train: 1.400700	val: 1.496463	test: 1.464532

Epoch: 46
Loss: 2.0229706168174744
RMSE train: 1.581853	val: 1.767104	test: 1.696673
MAE train: 1.343664	val: 1.427648	test: 1.406545

Epoch: 47
Loss: 1.8856000900268555
RMSE train: 1.547647	val: 1.769778	test: 1.675802
MAE train: 1.286688	val: 1.404794	test: 1.354051

Epoch: 48
Loss: 1.6947423219680786
RMSE train: 1.539196	val: 1.808654	test: 1.681499
MAE train: 1.271313	val: 1.422856	test: 1.348098

Epoch: 49
Loss: 1.6133800148963928
RMSE train: 1.457569	val: 1.693834	test: 1.610594
MAE train: 1.214823	val: 1.335790	test: 1.320806

Epoch: 50
Loss: 1.5067092180252075
RMSE train: 1.333767	val: 1.512723	test: 1.491824
MAE train: 1.120737	val: 1.188581	test: 1.248688

Epoch: 51
Loss: 1.4039329290390015
RMSE train: 1.234747	val: 1.404289	test: 1.428314
MAE train: 1.026700	val: 1.095033	test: 1.179617

Epoch: 52
Loss: 1.4597219228744507
RMSE train: 1.185651	val: 1.417201	test: 1.406580
MAE train: 0.989087	val: 1.080152	test: 1.153176

Epoch: 53
Loss: 1.4446841478347778
RMSE train: 1.113630	val: 1.404831	test: 1.361183
MAE train: 0.909008	val: 1.038327	test: 1.065695

Epoch: 54
Loss: 1.3662870526313782
RMSE train: 1.085720	val: 1.377759	test: 1.327038
MAE train: 0.887245	val: 0.991626	test: 1.029022

Epoch: 55
Loss: 1.4401033520698547
RMSE train: 1.102639	val: 1.393835	test: 1.328664
MAE train: 0.896755	val: 1.013313	test: 1.016222

Epoch: 56
Loss: 1.1380039751529694
RMSE train: 1.097800	val: 1.366183	test: 1.305887
MAE train: 0.890874	val: 1.006053	test: 1.000645

Epoch: 57
Loss: 1.1809386610984802
RMSE train: 1.047455	val: 1.301916	test: 1.247061
MAE train: 0.835722	val: 0.952186	test: 0.951556

Epoch: 58
Loss: 1.1462481021881104
RMSE train: 0.956384	val: 1.196763	test: 1.155416
MAE train: 0.755999	val: 0.856447	test: 0.889370

Epoch: 59
Loss: 0.9745810925960541
RMSE train: 0.871850	val: 1.079720	test: 1.063378
MAE train: 0.697324	val: 0.771361	test: 0.830694

Epoch: 60
Loss: 1.2775967121124268
RMSE train: 0.832530	val: 1.011311	test: 1.044301
MAE train: 0.665025	val: 0.737520	test: 0.805995

Epoch: 61
Loss: 1.0596441626548767
RMSE train: 0.830288	val: 1.009541	test: 1.066598
MAE train: 0.661704	val: 0.730811	test: 0.810392

Epoch: 62
Loss: 1.0063876807689667
RMSE train: 0.875532	val: 1.103630	test: 1.110386
MAE train: 0.709829	val: 0.770463	test: 0.849861

Epoch: 63
Loss: 0.9301723539829254
RMSE train: 0.958386	val: 1.235350	test: 1.195564
MAE train: 0.764365	val: 0.845260	test: 0.896662

Epoch: 64
Loss: 0.9994332492351532
RMSE train: 0.984392	val: 1.288121	test: 1.228137
MAE train: 0.780686	val: 0.880749	test: 0.918462

Epoch: 65
Loss: 1.0051013231277466
RMSE train: 0.907038	val: 1.238733	test: 1.189861
MAE train: 0.703603	val: 0.826691	test: 0.872427

Epoch: 66
Loss: 1.0302405655384064
RMSE train: 0.781737	val: 1.146682	test: 1.121081
MAE train: 0.593248	val: 0.744579	test: 0.797341

Epoch: 67
Loss: 0.9269278049468994
RMSE train: 0.744375	val: 1.122430	test: 1.094229
MAE train: 0.567155	val: 0.742681	test: 0.792292

Epoch: 68
Loss: 0.9000942409038544
RMSE train: 0.751931	val: 1.113847	test: 1.058155
MAE train: 0.586361	val: 0.763217	test: 0.798960

Epoch: 69
Loss: 1.0041500627994537
RMSE train: 0.814921	val: 1.180558	test: 1.092106
MAE train: 0.647097	val: 0.811204	test: 0.848767

Epoch: 70
Loss: 1.0348190665245056
RMSE train: 0.896910	val: 1.284873	test: 1.163152
MAE train: 0.704365	val: 0.846726	test: 0.894186

Epoch: 71
Loss: 0.9054609835147858
RMSE train: 0.964990	val: 1.390553	test: 1.212104
MAE train: 0.744771	val: 0.892676	test: 0.916241

Epoch: 72
Loss: 0.9166213870048523
RMSE train: 0.967254	val: 1.419179	test: 1.200689
MAE train: 0.751462	val: 0.911896	test: 0.912631

Epoch: 73
Loss: 0.8971318602561951
RMSE train: 0.972806	val: 1.404191	test: 1.159743
MAE train: 0.774010	val: 0.921167	test: 0.915725

Epoch: 74
Loss: 0.9601115882396698
RMSE train: 0.977628	val: 1.402307	test: 1.160986
MAE train: 0.788425	val: 0.946523	test: 0.932244

Epoch: 75
Loss: 0.9441176950931549
RMSE train: 0.964336	val: 1.381793	test: 1.155667
MAE train: 0.774117	val: 0.938472	test: 0.922043

Epoch: 76
Loss: 0.8578614592552185
RMSE train: 0.963683	val: 1.358995	test: 1.174489
MAE train: 0.775967	val: 0.931078	test: 0.938679

Epoch: 77
Loss: 0.8054631352424622
RMSE train: 1.026247	val: 1.364624	test: 1.237091
MAE train: 0.834690	val: 0.971790	test: 0.999398

Epoch: 78
Loss: 1.2269722819328308
RMSE train: 1.037514	val: 1.338007	test: 1.257397
MAE train: 0.841292	val: 0.959479	test: 1.014404

Epoch: 79
Loss: 0.8393421471118927
RMSE train: 1.028049	val: 1.359580	test: 1.269002
MAE train: 0.812097	val: 0.955081	test: 0.996647

Epoch: 80
Loss: 0.8527461588382721
RMSE train: 1.042211	val: 1.421719	test: 1.278152
MAE train: 0.802643	val: 0.969936	test: 0.978152

Epoch: 81
Loss: 0.7046041488647461
RMSE train: 1.036611	val: 1.416179	test: 1.269190
MAE train: 0.804581	val: 0.956328	test: 0.974063

Epoch: 82
Loss: 0.7531443536281586
RMSE train: 0.955867	val: 1.294405	test: 1.196567
MAE train: 0.736581	val: 0.862413	test: 0.900325

Epoch: 83
Loss: 0.8616430759429932
RMSE train: 0.840529	val: 1.174201	test: 1.102664
MAE train: 0.650335	val: 0.778729	test: 0.824743

Epoch: 23
Loss: 8.714662075042725
RMSE train: 3.301158	val: 3.773319	test: 3.299361
MAE train: 2.929931	val: 3.257579	test: 2.937966

Epoch: 24
Loss: 8.420614004135132
RMSE train: 3.210359	val: 3.661541	test: 3.214340
MAE train: 2.845905	val: 3.157673	test: 2.854450

Epoch: 25
Loss: 7.867641448974609
RMSE train: 3.124943	val: 3.553305	test: 3.145352
MAE train: 2.779637	val: 3.084537	test: 2.800394

Epoch: 26
Loss: 7.753604173660278
RMSE train: 3.029561	val: 3.453601	test: 3.029026
MAE train: 2.700405	val: 3.011773	test: 2.706669

Epoch: 27
Loss: 7.360989332199097
RMSE train: 2.945808	val: 3.355804	test: 2.911564
MAE train: 2.628637	val: 2.922366	test: 2.597730

Epoch: 28
Loss: 6.726428747177124
RMSE train: 2.861170	val: 3.254573	test: 2.808599
MAE train: 2.558341	val: 2.829217	test: 2.502407

Epoch: 29
Loss: 6.621251344680786
RMSE train: 2.804389	val: 3.196631	test: 2.742975
MAE train: 2.506223	val: 2.771564	test: 2.439403

Epoch: 30
Loss: 6.392430543899536
RMSE train: 2.751088	val: 3.135436	test: 2.748417
MAE train: 2.455771	val: 2.709276	test: 2.427401

Epoch: 31
Loss: 5.88398551940918
RMSE train: 2.676092	val: 3.048051	test: 2.735397
MAE train: 2.380932	val: 2.627316	test: 2.395436

Epoch: 32
Loss: 5.579543113708496
RMSE train: 2.631656	val: 3.018766	test: 2.698458
MAE train: 2.330631	val: 2.586815	test: 2.354867

Epoch: 33
Loss: 5.195390224456787
RMSE train: 2.566507	val: 2.971778	test: 2.627737
MAE train: 2.269299	val: 2.533463	test: 2.288393

Epoch: 34
Loss: 4.92560887336731
RMSE train: 2.503189	val: 2.928176	test: 2.556525
MAE train: 2.208081	val: 2.480751	test: 2.214134

Epoch: 35
Loss: 4.753016233444214
RMSE train: 2.431860	val: 2.856173	test: 2.479022
MAE train: 2.146009	val: 2.413821	test: 2.136343

Epoch: 36
Loss: 4.579792022705078
RMSE train: 2.282631	val: 2.685581	test: 2.346936
MAE train: 2.023023	val: 2.278860	test: 2.020902

Epoch: 37
Loss: 4.155463933944702
RMSE train: 2.186088	val: 2.589357	test: 2.257237
MAE train: 1.929445	val: 2.196407	test: 1.920524

Epoch: 38
Loss: 3.751519799232483
RMSE train: 2.108306	val: 2.511660	test: 2.168074
MAE train: 1.854408	val: 2.120080	test: 1.837915

Epoch: 39
Loss: 3.651853322982788
RMSE train: 2.075238	val: 2.506028	test: 2.103125
MAE train: 1.809304	val: 2.089275	test: 1.783528

Epoch: 40
Loss: 3.365225672721863
RMSE train: 2.056380	val: 2.512629	test: 2.052557
MAE train: 1.792367	val: 2.078127	test: 1.762266

Epoch: 41
Loss: 3.1408448219299316
RMSE train: 1.961746	val: 2.391941	test: 1.957105
MAE train: 1.723532	val: 1.982596	test: 1.712125

Epoch: 42
Loss: 3.0261974334716797
RMSE train: 1.912235	val: 2.322040	test: 1.947647
MAE train: 1.687704	val: 1.937971	test: 1.696505

Epoch: 43
Loss: 2.677122473716736
RMSE train: 1.831064	val: 2.222419	test: 1.914727
MAE train: 1.618862	val: 1.862046	test: 1.658674

Epoch: 44
Loss: 2.569432258605957
RMSE train: 1.794687	val: 2.233816	test: 1.894661
MAE train: 1.564270	val: 1.846510	test: 1.616124

Epoch: 45
Loss: 2.3585339784622192
RMSE train: 1.765403	val: 2.235832	test: 1.872327
MAE train: 1.517641	val: 1.814628	test: 1.575793

Epoch: 46
Loss: 2.222533404827118
RMSE train: 1.654362	val: 2.106382	test: 1.743560
MAE train: 1.422756	val: 1.697560	test: 1.477463

Epoch: 47
Loss: 2.102842390537262
RMSE train: 1.524797	val: 1.945521	test: 1.604559
MAE train: 1.304601	val: 1.547173	test: 1.354769

Epoch: 48
Loss: 1.876059353351593
RMSE train: 1.472053	val: 1.880847	test: 1.537955
MAE train: 1.253130	val: 1.486685	test: 1.294115

Epoch: 49
Loss: 1.9470234513282776
RMSE train: 1.429872	val: 1.828336	test: 1.515237
MAE train: 1.193381	val: 1.421283	test: 1.233577

Epoch: 50
Loss: 1.6812742948532104
RMSE train: 1.402580	val: 1.812811	test: 1.545494
MAE train: 1.154840	val: 1.382900	test: 1.212586

Epoch: 51
Loss: 1.5336745977401733
RMSE train: 1.413876	val: 1.808471	test: 1.644809
MAE train: 1.143165	val: 1.360897	test: 1.232634

Epoch: 52
Loss: 1.6990700960159302
RMSE train: 1.337228	val: 1.725343	test: 1.614632
MAE train: 1.086363	val: 1.294375	test: 1.205137

Epoch: 53
Loss: 1.6697848439216614
RMSE train: 1.287867	val: 1.670526	test: 1.552984
MAE train: 1.081910	val: 1.276196	test: 1.211803

Epoch: 54
Loss: 1.3513336777687073
RMSE train: 1.266667	val: 1.710978	test: 1.512769
MAE train: 1.060892	val: 1.310269	test: 1.190616

Epoch: 55
Loss: 1.289544939994812
RMSE train: 1.224627	val: 1.764382	test: 1.424346
MAE train: 0.988082	val: 1.306285	test: 1.106626

Epoch: 56
Loss: 1.3290222883224487
RMSE train: 1.146751	val: 1.712244	test: 1.309428
MAE train: 0.901730	val: 1.235131	test: 1.007706

Epoch: 57
Loss: 1.2449703216552734
RMSE train: 1.016354	val: 1.529245	test: 1.209284
MAE train: 0.811981	val: 1.075717	test: 0.929154

Epoch: 58
Loss: 1.2259801626205444
RMSE train: 0.952731	val: 1.392349	test: 1.191046
MAE train: 0.767471	val: 0.991580	test: 0.920157

Epoch: 59
Loss: 1.1359316110610962
RMSE train: 0.919462	val: 1.355540	test: 1.223368
MAE train: 0.729492	val: 0.932416	test: 0.903559

Epoch: 60
Loss: 1.0331038236618042
RMSE train: 0.911782	val: 1.409807	test: 1.185219
MAE train: 0.702696	val: 0.929384	test: 0.845535

Epoch: 61
Loss: 0.9748360514640808
RMSE train: 0.938609	val: 1.470518	test: 1.167967
MAE train: 0.710336	val: 0.962699	test: 0.839986

Epoch: 62
Loss: 1.009224772453308
RMSE train: 0.936445	val: 1.460271	test: 1.155666
MAE train: 0.746597	val: 0.988917	test: 0.887083

Epoch: 63
Loss: 0.9753633439540863
RMSE train: 0.931762	val: 1.395604	test: 1.175706
MAE train: 0.770933	val: 0.968483	test: 0.934972

Epoch: 64
Loss: 0.9288998246192932
RMSE train: 0.917323	val: 1.369794	test: 1.177891
MAE train: 0.749904	val: 0.955664	test: 0.921507

Epoch: 65
Loss: 1.0284734964370728
RMSE train: 0.932930	val: 1.406172	test: 1.223521
MAE train: 0.722700	val: 0.966354	test: 0.898458

Epoch: 66
Loss: 0.893814742565155
RMSE train: 0.923768	val: 1.400134	test: 1.229779
MAE train: 0.682892	val: 0.940956	test: 0.838081

Epoch: 67
Loss: 0.9644019901752472
RMSE train: 0.847078	val: 1.285443	test: 1.181295
MAE train: 0.632234	val: 0.880869	test: 0.814640

Epoch: 68
Loss: 0.875542551279068
RMSE train: 0.799457	val: 1.191619	test: 1.131191
MAE train: 0.625562	val: 0.840651	test: 0.854902

Epoch: 69
Loss: 0.9380095303058624
RMSE train: 0.755695	val: 1.137176	test: 1.077583
MAE train: 0.596066	val: 0.812162	test: 0.830031

Epoch: 70
Loss: 1.0388349294662476
RMSE train: 0.692150	val: 1.134190	test: 0.991012
MAE train: 0.539942	val: 0.795125	test: 0.739123

Epoch: 71
Loss: 1.0219641327857971
RMSE train: 0.689006	val: 1.195034	test: 0.971962
MAE train: 0.521232	val: 0.803793	test: 0.689039

Epoch: 72
Loss: 0.8105322420597076
RMSE train: 0.676886	val: 1.191838	test: 0.974837
MAE train: 0.507358	val: 0.783807	test: 0.676368

Epoch: 73
Loss: 0.9418789148330688
RMSE train: 0.661946	val: 1.103978	test: 1.036139
MAE train: 0.500326	val: 0.735117	test: 0.712728

Epoch: 74
Loss: 0.7736911177635193
RMSE train: 0.707551	val: 1.087067	test: 1.093028
MAE train: 0.543126	val: 0.734491	test: 0.793924

Epoch: 75
Loss: 0.8720801472663879
RMSE train: 0.769270	val: 1.150869	test: 1.114491
MAE train: 0.594915	val: 0.778109	test: 0.835227

Epoch: 76
Loss: 0.9066441655158997
RMSE train: 0.794586	val: 1.209843	test: 1.093595
MAE train: 0.619128	val: 0.829859	test: 0.832152

Epoch: 77
Loss: 0.7629320323467255
RMSE train: 0.794946	val: 1.267131	test: 1.079307
MAE train: 0.614008	val: 0.859216	test: 0.803203

Epoch: 78
Loss: 0.8252370357513428
RMSE train: 0.791171	val: 1.304702	test: 1.064979
MAE train: 0.606876	val: 0.865624	test: 0.780191

Epoch: 79
Loss: 0.885177731513977
RMSE train: 0.834949	val: 1.374388	test: 1.078173
MAE train: 0.632893	val: 0.891145	test: 0.784795

Epoch: 80
Loss: 0.8050607144832611
RMSE train: 0.873785	val: 1.451017	test: 1.079034
MAE train: 0.667320	val: 0.929098	test: 0.784373

Epoch: 81
Loss: 0.8328089714050293
RMSE train: 0.846407	val: 1.448845	test: 1.058451
MAE train: 0.653275	val: 0.934555	test: 0.788050

Epoch: 82
Loss: 0.8586061894893646
RMSE train: 0.772733	val: 1.361473	test: 1.028526
MAE train: 0.607698	val: 0.861578	test: 0.777379

Epoch: 83
Loss: 0.7596481144428253
RMSE train: 0.715108	val: 1.294165	test: 0.997864
MAE train: 0.558743	val: 0.809558	test: 0.742875

Epoch: 23
Loss: 8.547327041625977
RMSE train: 3.027377	val: 3.270807	test: 3.057542
MAE train: 2.651163	val: 2.764826	test: 2.653850

Epoch: 24
Loss: 8.196054458618164
RMSE train: 2.993522	val: 3.233403	test: 3.013402
MAE train: 2.630403	val: 2.744127	test: 2.628420

Epoch: 25
Loss: 7.982824325561523
RMSE train: 2.995445	val: 3.233374	test: 2.986852
MAE train: 2.644976	val: 2.766497	test: 2.624548

Epoch: 26
Loss: 7.275007009506226
RMSE train: 2.996824	val: 3.240272	test: 2.968315
MAE train: 2.657542	val: 2.797538	test: 2.622226

Epoch: 27
Loss: 7.363689661026001
RMSE train: 2.967265	val: 3.229621	test: 2.952091
MAE train: 2.643461	val: 2.800245	test: 2.610762

Epoch: 28
Loss: 6.73037576675415
RMSE train: 2.892094	val: 3.183486	test: 2.913590
MAE train: 2.573660	val: 2.750087	test: 2.558494

Epoch: 29
Loss: 6.440775632858276
RMSE train: 2.784530	val: 3.097896	test: 2.833704
MAE train: 2.479652	val: 2.660130	test: 2.473186

Epoch: 30
Loss: 5.927791595458984
RMSE train: 2.603091	val: 2.921014	test: 2.655675
MAE train: 2.322624	val: 2.494610	test: 2.320291

Epoch: 31
Loss: 5.770153999328613
RMSE train: 2.458430	val: 2.763926	test: 2.511898
MAE train: 2.205768	val: 2.355278	test: 2.199048

Epoch: 32
Loss: 5.436311721801758
RMSE train: 2.377279	val: 2.678224	test: 2.429616
MAE train: 2.130426	val: 2.268340	test: 2.128394

Epoch: 33
Loss: 5.005762100219727
RMSE train: 2.325675	val: 2.644327	test: 2.378846
MAE train: 2.053203	val: 2.198448	test: 2.069125

Epoch: 34
Loss: 4.498950004577637
RMSE train: 2.278680	val: 2.574224	test: 2.322137
MAE train: 1.991233	val: 2.123407	test: 2.022236

Epoch: 35
Loss: 4.27608323097229
RMSE train: 2.248251	val: 2.507138	test: 2.295809
MAE train: 1.972707	val: 2.080126	test: 2.012037

Epoch: 36
Loss: 4.043819904327393
RMSE train: 2.237285	val: 2.496133	test: 2.312204
MAE train: 1.977462	val: 2.084135	test: 2.024561

Epoch: 37
Loss: 3.9233680963516235
RMSE train: 2.249645	val: 2.542298	test: 2.352655
MAE train: 1.990631	val: 2.116703	test: 2.040278

Epoch: 38
Loss: 3.5806745290756226
RMSE train: 2.245195	val: 2.556986	test: 2.382116
MAE train: 1.971275	val: 2.112835	test: 2.029243

Epoch: 39
Loss: 3.395976185798645
RMSE train: 2.179517	val: 2.482469	test: 2.326646
MAE train: 1.911924	val: 2.050713	test: 1.981871

Epoch: 40
Loss: 3.221753716468811
RMSE train: 2.084646	val: 2.404324	test: 2.228662
MAE train: 1.824545	val: 1.964040	test: 1.893522

Epoch: 41
Loss: 3.25178599357605
RMSE train: 1.988284	val: 2.308718	test: 2.143172
MAE train: 1.727142	val: 1.856472	test: 1.799613

Epoch: 42
Loss: 2.738248109817505
RMSE train: 1.851568	val: 2.158064	test: 1.996905
MAE train: 1.613589	val: 1.731943	test: 1.690697

Epoch: 43
Loss: 2.482635974884033
RMSE train: 1.729474	val: 2.042112	test: 1.848554
MAE train: 1.516245	val: 1.636521	test: 1.593306

Epoch: 44
Loss: 2.2626171112060547
RMSE train: 1.618587	val: 1.942927	test: 1.723280
MAE train: 1.418133	val: 1.542853	test: 1.494245

Epoch: 45
Loss: 2.1501680612564087
RMSE train: 1.524756	val: 1.882129	test: 1.625125
MAE train: 1.328702	val: 1.481364	test: 1.416871

Epoch: 46
Loss: 2.1324105262756348
RMSE train: 1.486427	val: 1.856202	test: 1.624680
MAE train: 1.278498	val: 1.457056	test: 1.392725

Epoch: 47
Loss: 1.849452555179596
RMSE train: 1.527180	val: 1.872908	test: 1.695224
MAE train: 1.311019	val: 1.482475	test: 1.437555

Epoch: 48
Loss: 1.8306146264076233
RMSE train: 1.567451	val: 1.867492	test: 1.756665
MAE train: 1.341904	val: 1.496530	test: 1.471364

Epoch: 49
Loss: 1.596489131450653
RMSE train: 1.558832	val: 1.848717	test: 1.743077
MAE train: 1.334907	val: 1.477850	test: 1.458020

Epoch: 50
Loss: 1.6257291436195374
RMSE train: 1.496144	val: 1.800039	test: 1.665255
MAE train: 1.282229	val: 1.416774	test: 1.392532

Epoch: 51
Loss: 1.5670979022979736
RMSE train: 1.395696	val: 1.750703	test: 1.553306
MAE train: 1.192745	val: 1.347693	test: 1.294212

Epoch: 52
Loss: 1.374643862247467
RMSE train: 1.248862	val: 1.632182	test: 1.423956
MAE train: 1.058603	val: 1.227840	test: 1.172440

Epoch: 53
Loss: 1.2685200572013855
RMSE train: 1.115300	val: 1.523836	test: 1.318275
MAE train: 0.934282	val: 1.116784	test: 1.063559

Epoch: 54
Loss: 1.2731829285621643
RMSE train: 1.053838	val: 1.454378	test: 1.281305
MAE train: 0.871475	val: 1.044713	test: 1.010793

Epoch: 55
Loss: 1.2474433779716492
RMSE train: 1.016630	val: 1.424674	test: 1.273928
MAE train: 0.815946	val: 0.995971	test: 0.962831

Epoch: 56
Loss: 1.2064990401268005
RMSE train: 1.009693	val: 1.392916	test: 1.252357
MAE train: 0.810403	val: 0.961539	test: 0.936220

Epoch: 57
Loss: 1.0683004260063171
RMSE train: 0.973427	val: 1.339540	test: 1.207452
MAE train: 0.785810	val: 0.921707	test: 0.911862

Epoch: 58
Loss: 1.1627024710178375
RMSE train: 0.964225	val: 1.343637	test: 1.205937
MAE train: 0.783016	val: 0.933539	test: 0.935481

Epoch: 59
Loss: 1.1735784411430359
RMSE train: 0.993146	val: 1.398735	test: 1.240359
MAE train: 0.801067	val: 0.978069	test: 0.968736

Epoch: 60
Loss: 0.9261204302310944
RMSE train: 0.980616	val: 1.418721	test: 1.223854
MAE train: 0.775375	val: 0.975094	test: 0.955782

Epoch: 61
Loss: 0.9523655772209167
RMSE train: 0.940502	val: 1.399218	test: 1.157418
MAE train: 0.759453	val: 0.942619	test: 0.921906

Epoch: 62
Loss: 1.047821819782257
RMSE train: 0.916272	val: 1.352556	test: 1.128783
MAE train: 0.747923	val: 0.910223	test: 0.902311

Epoch: 63
Loss: 1.1206556558609009
RMSE train: 0.914376	val: 1.270311	test: 1.149274
MAE train: 0.757065	val: 0.899082	test: 0.940260

Epoch: 64
Loss: 0.9833275973796844
RMSE train: 0.961872	val: 1.273197	test: 1.232386
MAE train: 0.781663	val: 0.903743	test: 0.994230

Epoch: 65
Loss: 0.9993780255317688
RMSE train: 1.001575	val: 1.316295	test: 1.312338
MAE train: 0.778470	val: 0.921588	test: 1.005693

Epoch: 66
Loss: 0.8371475040912628
RMSE train: 0.964402	val: 1.375560	test: 1.281820
MAE train: 0.721628	val: 0.921171	test: 0.969581

Epoch: 67
Loss: 0.869470089673996
RMSE train: 0.937408	val: 1.405479	test: 1.240865
MAE train: 0.694086	val: 0.919515	test: 0.934255

Epoch: 68
Loss: 0.9162830710411072
RMSE train: 0.882296	val: 1.361679	test: 1.173908
MAE train: 0.669015	val: 0.893201	test: 0.902813

Epoch: 69
Loss: 0.8951495289802551
RMSE train: 0.799896	val: 1.267846	test: 1.108233
MAE train: 0.638320	val: 0.868939	test: 0.891660

Epoch: 70
Loss: 1.0375381708145142
RMSE train: 0.781905	val: 1.229030	test: 1.090116
MAE train: 0.639000	val: 0.863978	test: 0.894900

Epoch: 71
Loss: 0.8274506330490112
RMSE train: 0.780280	val: 1.242896	test: 1.086153
MAE train: 0.629588	val: 0.852915	test: 0.881820

Epoch: 72
Loss: 0.7919782996177673
RMSE train: 0.815085	val: 1.280308	test: 1.118023
MAE train: 0.648048	val: 0.875964	test: 0.884787

Epoch: 73
Loss: 0.825407475233078
RMSE train: 0.825255	val: 1.328129	test: 1.130497
MAE train: 0.632052	val: 0.876422	test: 0.853241

Epoch: 74
Loss: 0.9555296003818512
RMSE train: 0.805773	val: 1.315192	test: 1.118588
MAE train: 0.614389	val: 0.854701	test: 0.838654

Epoch: 75
Loss: 0.7495511472225189
RMSE train: 0.762374	val: 1.296765	test: 1.087832
MAE train: 0.590625	val: 0.856582	test: 0.838406

Epoch: 76
Loss: 0.9290816783905029
RMSE train: 0.760831	val: 1.302011	test: 1.085061
MAE train: 0.600245	val: 0.859719	test: 0.855897

Epoch: 77
Loss: 0.9660055339336395
RMSE train: 0.753325	val: 1.313217	test: 1.087624
MAE train: 0.592384	val: 0.856428	test: 0.852245

Epoch: 78
Loss: 0.8020377457141876
RMSE train: 0.759758	val: 1.325184	test: 1.090987
MAE train: 0.591981	val: 0.857835	test: 0.835897

Epoch: 79
Loss: 0.8539451360702515
RMSE train: 0.723489	val: 1.255681	test: 1.098643
MAE train: 0.556014	val: 0.807629	test: 0.829508

Epoch: 80
Loss: 0.7409105896949768
RMSE train: 0.686563	val: 1.238435	test: 1.096797
MAE train: 0.521119	val: 0.800986	test: 0.827960

Epoch: 81
Loss: 0.8606546819210052
RMSE train: 0.631315	val: 1.204492	test: 1.049384
MAE train: 0.483942	val: 0.783924	test: 0.800347

Epoch: 82
Loss: 0.8632645308971405
RMSE train: 0.601307	val: 1.172956	test: 0.992358
MAE train: 0.473890	val: 0.771927	test: 0.762658

Epoch: 83
Loss: 0.7737669050693512
RMSE train: 0.649071	val: 1.158573	test: 0.997221
MAE train: 0.512858	val: 0.788754	test: 0.758225

Epoch: 23
Loss: 8.82601022720337
RMSE train: 3.063617	val: 3.341449	test: 3.139333
MAE train: 2.751948	val: 2.913582	test: 2.781525

Epoch: 24
Loss: 8.265782356262207
RMSE train: 3.043446	val: 3.359612	test: 3.135384
MAE train: 2.735626	val: 2.926407	test: 2.749574

Epoch: 25
Loss: 7.990718841552734
RMSE train: 3.012908	val: 3.347170	test: 3.116873
MAE train: 2.714830	val: 2.920945	test: 2.719298

Epoch: 26
Loss: 7.737575054168701
RMSE train: 2.944175	val: 3.264306	test: 3.081946
MAE train: 2.645111	val: 2.845559	test: 2.660660

Epoch: 27
Loss: 6.96093487739563
RMSE train: 2.853306	val: 3.140573	test: 3.014849
MAE train: 2.551854	val: 2.730259	test: 2.580644

Epoch: 28
Loss: 6.593951940536499
RMSE train: 2.764372	val: 3.015080	test: 2.940884
MAE train: 2.457778	val: 2.613459	test: 2.496348

Epoch: 29
Loss: 6.317319631576538
RMSE train: 2.658676	val: 2.903969	test: 2.847033
MAE train: 2.338695	val: 2.483792	test: 2.382774

Epoch: 30
Loss: 6.324233770370483
RMSE train: 2.574648	val: 2.860217	test: 2.764537
MAE train: 2.263027	val: 2.416176	test: 2.300366

Epoch: 31
Loss: 5.942577838897705
RMSE train: 2.532015	val: 2.882587	test: 2.704063
MAE train: 2.232917	val: 2.422288	test: 2.263137

Epoch: 32
Loss: 5.525317668914795
RMSE train: 2.502331	val: 2.885040	test: 2.653125
MAE train: 2.214465	val: 2.428714	test: 2.238583

Epoch: 33
Loss: 5.236185312271118
RMSE train: 2.419747	val: 2.799947	test: 2.550642
MAE train: 2.148386	val: 2.365975	test: 2.179002

Epoch: 34
Loss: 5.028710603713989
RMSE train: 2.365452	val: 2.758468	test: 2.490399
MAE train: 2.103257	val: 2.327623	test: 2.134520

Epoch: 35
Loss: 4.581698656082153
RMSE train: 2.296034	val: 2.662594	test: 2.442052
MAE train: 2.041324	val: 2.250459	test: 2.095083

Epoch: 36
Loss: 4.193938732147217
RMSE train: 2.254216	val: 2.592662	test: 2.428445
MAE train: 1.995457	val: 2.192705	test: 2.067135

Epoch: 37
Loss: 4.2541327476501465
RMSE train: 2.204551	val: 2.540336	test: 2.405502
MAE train: 1.931618	val: 2.129725	test: 2.018237

Epoch: 38
Loss: 3.8181415796279907
RMSE train: 2.150274	val: 2.485154	test: 2.351583
MAE train: 1.879967	val: 2.065907	test: 1.959883

Epoch: 39
Loss: 3.417905807495117
RMSE train: 2.083819	val: 2.404726	test: 2.263774
MAE train: 1.826086	val: 1.996103	test: 1.889477

Epoch: 40
Loss: 3.343467652797699
RMSE train: 2.020834	val: 2.341954	test: 2.172808
MAE train: 1.780368	val: 1.941366	test: 1.827654

Epoch: 41
Loss: 3.1054186820983887
RMSE train: 1.960298	val: 2.277285	test: 2.094794
MAE train: 1.725655	val: 1.871542	test: 1.761877

Epoch: 42
Loss: 2.8772547245025635
RMSE train: 1.913693	val: 2.264790	test: 2.049721
MAE train: 1.676168	val: 1.843894	test: 1.728515

Epoch: 43
Loss: 2.6228015422821045
RMSE train: 1.828873	val: 2.208934	test: 2.001169
MAE train: 1.562269	val: 1.766580	test: 1.659219

Epoch: 44
Loss: 2.404702067375183
RMSE train: 1.711243	val: 2.097952	test: 1.902279
MAE train: 1.431003	val: 1.651606	test: 1.564165

Epoch: 45
Loss: 2.223181962966919
RMSE train: 1.590102	val: 1.935752	test: 1.783499
MAE train: 1.331997	val: 1.528323	test: 1.465574

Epoch: 46
Loss: 2.0586437582969666
RMSE train: 1.533098	val: 1.833831	test: 1.714672
MAE train: 1.298355	val: 1.467047	test: 1.426043

Epoch: 47
Loss: 2.142244338989258
RMSE train: 1.550503	val: 1.863531	test: 1.752887
MAE train: 1.313195	val: 1.490060	test: 1.450585

Epoch: 48
Loss: 2.222201347351074
RMSE train: 1.621501	val: 1.969583	test: 1.860447
MAE train: 1.376586	val: 1.572419	test: 1.518531

Epoch: 49
Loss: 1.8625209331512451
RMSE train: 1.673252	val: 2.055604	test: 1.913356
MAE train: 1.416288	val: 1.622898	test: 1.554296

Epoch: 50
Loss: 1.711753249168396
RMSE train: 1.603978	val: 2.016558	test: 1.823815
MAE train: 1.364792	val: 1.568660	test: 1.484299

Epoch: 51
Loss: 1.7959038019180298
RMSE train: 1.501541	val: 1.914935	test: 1.712874
MAE train: 1.277391	val: 1.460409	test: 1.374896

Epoch: 52
Loss: 1.4799355268478394
RMSE train: 1.412095	val: 1.857553	test: 1.622636
MAE train: 1.197872	val: 1.406417	test: 1.309040

Epoch: 53
Loss: 1.447344183921814
RMSE train: 1.278748	val: 1.758307	test: 1.485232
MAE train: 1.065594	val: 1.295277	test: 1.194985

Epoch: 54
Loss: 1.2795860767364502
RMSE train: 1.159248	val: 1.650019	test: 1.390940
MAE train: 0.938100	val: 1.174822	test: 1.098862

Epoch: 55
Loss: 1.1634793281555176
RMSE train: 1.056288	val: 1.547867	test: 1.374480
MAE train: 0.842453	val: 1.081078	test: 1.029353

Epoch: 56
Loss: 1.1564258337020874
RMSE train: 1.017304	val: 1.505756	test: 1.398457
MAE train: 0.812034	val: 1.038714	test: 0.998867

Epoch: 57
Loss: 1.3387765884399414
RMSE train: 0.980500	val: 1.484866	test: 1.379223
MAE train: 0.787645	val: 1.005298	test: 0.961383

Epoch: 58
Loss: 0.9518189430236816
RMSE train: 0.984086	val: 1.522974	test: 1.391272
MAE train: 0.791353	val: 1.019114	test: 0.970921

Epoch: 59
Loss: 1.0501799583435059
RMSE train: 0.962542	val: 1.486723	test: 1.407480
MAE train: 0.782044	val: 1.003912	test: 0.977644

Epoch: 60
Loss: 0.9031409025192261
RMSE train: 0.969892	val: 1.432399	test: 1.426241
MAE train: 0.805513	val: 1.015908	test: 0.992903

Epoch: 61
Loss: 1.1034915447235107
RMSE train: 0.917125	val: 1.386174	test: 1.392642
MAE train: 0.742867	val: 0.954961	test: 0.949174

Epoch: 62
Loss: 1.0396782755851746
RMSE train: 0.869831	val: 1.360765	test: 1.344840
MAE train: 0.689435	val: 0.917637	test: 0.914147

Epoch: 63
Loss: 1.1130402088165283
RMSE train: 0.817264	val: 1.313760	test: 1.275328
MAE train: 0.654059	val: 0.881498	test: 0.871012

Epoch: 64
Loss: 0.9469398260116577
RMSE train: 0.812576	val: 1.283049	test: 1.243281
MAE train: 0.659313	val: 0.888456	test: 0.860206

Epoch: 65
Loss: 1.032438039779663
RMSE train: 0.863725	val: 1.314183	test: 1.299265
MAE train: 0.709371	val: 0.921284	test: 0.895175

Epoch: 66
Loss: 0.9003564417362213
RMSE train: 0.962285	val: 1.406835	test: 1.403215
MAE train: 0.790418	val: 0.987439	test: 0.969575

Epoch: 67
Loss: 0.9136450588703156
RMSE train: 1.025959	val: 1.532762	test: 1.450570
MAE train: 0.822226	val: 1.051230	test: 0.994833

Epoch: 68
Loss: 1.0780242681503296
RMSE train: 0.961589	val: 1.496021	test: 1.395256
MAE train: 0.753899	val: 0.982550	test: 0.920145

Epoch: 69
Loss: 0.9130124449729919
RMSE train: 0.864671	val: 1.391535	test: 1.317287
MAE train: 0.666750	val: 0.899991	test: 0.838358

Epoch: 70
Loss: 0.9544251561164856
RMSE train: 0.819235	val: 1.316749	test: 1.252177
MAE train: 0.650610	val: 0.868113	test: 0.818495

Epoch: 71
Loss: 0.8679119348526001
RMSE train: 0.784086	val: 1.268126	test: 1.192436
MAE train: 0.618869	val: 0.842920	test: 0.792186

Epoch: 72
Loss: 1.0066032409667969
RMSE train: 0.744912	val: 1.226014	test: 1.145002
MAE train: 0.589362	val: 0.815372	test: 0.772491

Epoch: 73
Loss: 0.8742695748806
RMSE train: 0.710205	val: 1.226171	test: 1.090925
MAE train: 0.561872	val: 0.789291	test: 0.739479

Epoch: 74
Loss: 0.7001274526119232
RMSE train: 0.681588	val: 1.255130	test: 1.104238
MAE train: 0.542461	val: 0.792173	test: 0.742494

Epoch: 75
Loss: 1.0231480598449707
RMSE train: 0.726344	val: 1.316384	test: 1.155501
MAE train: 0.581967	val: 0.840298	test: 0.792019

Epoch: 76
Loss: 1.0829916894435883
RMSE train: 0.817925	val: 1.391320	test: 1.228944
MAE train: 0.668844	val: 0.917466	test: 0.882846

Epoch: 77
Loss: 0.7977342009544373
RMSE train: 0.913182	val: 1.455611	test: 1.291772
MAE train: 0.756307	val: 1.004651	test: 0.965012

Epoch: 78
Loss: 0.8747216761112213
RMSE train: 0.910962	val: 1.411951	test: 1.304773
MAE train: 0.736268	val: 0.976378	test: 0.959205

Epoch: 79
Loss: 0.8187745809555054
RMSE train: 0.852898	val: 1.321452	test: 1.268029
MAE train: 0.681696	val: 0.903301	test: 0.908712

Epoch: 80
Loss: 0.7811063230037689
RMSE train: 0.778167	val: 1.246709	test: 1.239008
MAE train: 0.602970	val: 0.818918	test: 0.838203

Epoch: 81
Loss: 0.7160008251667023
RMSE train: 0.728942	val: 1.171796	test: 1.214280
MAE train: 0.550233	val: 0.752889	test: 0.799666

Epoch: 82
Loss: 0.9285682439804077
RMSE train: 0.723073	val: 1.141989	test: 1.206110
MAE train: 0.557795	val: 0.749891	test: 0.791322

Epoch: 83
Loss: 0.8801223337650299
RMSE train: 0.737637	val: 1.154383	test: 1.220748
MAE train: 0.584494	val: 0.779659	test: 0.803749

Epoch: 23
Loss: 9.021636486053467
RMSE train: 3.219390	val: 3.720324	test: 3.249204
MAE train: 2.882079	val: 3.174322	test: 2.898489

Epoch: 24
Loss: 8.674954175949097
RMSE train: 3.157350	val: 3.643825	test: 3.207878
MAE train: 2.825030	val: 3.110340	test: 2.851321

Epoch: 25
Loss: 8.215775728225708
RMSE train: 3.058304	val: 3.522334	test: 3.111826
MAE train: 2.743145	val: 2.997654	test: 2.760028

Epoch: 26
Loss: 7.85542106628418
RMSE train: 2.973233	val: 3.418169	test: 3.022625
MAE train: 2.674074	val: 2.904019	test: 2.659626

Epoch: 27
Loss: 7.527616024017334
RMSE train: 2.893802	val: 3.328946	test: 2.938291
MAE train: 2.600463	val: 2.824045	test: 2.561957

Epoch: 28
Loss: 7.084494113922119
RMSE train: 2.801089	val: 3.247552	test: 2.845160
MAE train: 2.508101	val: 2.740256	test: 2.460806

Epoch: 29
Loss: 6.9761903285980225
RMSE train: 2.702459	val: 3.174542	test: 2.753443
MAE train: 2.409932	val: 2.670047	test: 2.365731

Epoch: 30
Loss: 6.6427388191223145
RMSE train: 2.630722	val: 3.135452	test: 2.684115
MAE train: 2.327528	val: 2.624946	test: 2.293115

Epoch: 31
Loss: 6.111114501953125
RMSE train: 2.571343	val: 3.091845	test: 2.626944
MAE train: 2.262103	val: 2.575838	test: 2.249232

Epoch: 32
Loss: 5.736871004104614
RMSE train: 2.521768	val: 3.052434	test: 2.565634
MAE train: 2.217548	val: 2.537080	test: 2.221621

Epoch: 33
Loss: 5.390959978103638
RMSE train: 2.438899	val: 2.972063	test: 2.445893
MAE train: 2.155602	val: 2.469697	test: 2.157487

Epoch: 34
Loss: 5.052057981491089
RMSE train: 2.357830	val: 2.879717	test: 2.335043
MAE train: 2.094362	val: 2.392022	test: 2.075842

Epoch: 35
Loss: 5.046445846557617
RMSE train: 2.307107	val: 2.819503	test: 2.287392
MAE train: 2.047258	val: 2.331059	test: 2.017132

Epoch: 36
Loss: 4.664216756820679
RMSE train: 2.263824	val: 2.758712	test: 2.266274
MAE train: 2.010077	val: 2.282153	test: 1.986466

Epoch: 37
Loss: 4.432188510894775
RMSE train: 2.247344	val: 2.723926	test: 2.291218
MAE train: 1.997835	val: 2.263519	test: 1.984550

Epoch: 38
Loss: 3.9795418977737427
RMSE train: 2.214189	val: 2.694621	test: 2.297102
MAE train: 1.958116	val: 2.231884	test: 1.961133

Epoch: 39
Loss: 3.817228317260742
RMSE train: 2.138168	val: 2.618770	test: 2.227118
MAE train: 1.879847	val: 2.157150	test: 1.896831

Epoch: 40
Loss: 3.4544440507888794
RMSE train: 2.041143	val: 2.522588	test: 2.124323
MAE train: 1.788234	val: 2.062183	test: 1.807394

Epoch: 41
Loss: 3.398685097694397
RMSE train: 1.953091	val: 2.410806	test: 2.072687
MAE train: 1.713995	val: 1.961434	test: 1.734301

Epoch: 42
Loss: 3.03615140914917
RMSE train: 1.903545	val: 2.354727	test: 2.017840
MAE train: 1.665905	val: 1.905053	test: 1.670763

Epoch: 43
Loss: 2.9792098999023438
RMSE train: 1.835449	val: 2.306615	test: 1.940145
MAE train: 1.599555	val: 1.857348	test: 1.605551

Epoch: 44
Loss: 2.505928933620453
RMSE train: 1.768528	val: 2.232105	test: 1.914391
MAE train: 1.537345	val: 1.784101	test: 1.564744

Epoch: 45
Loss: 2.4306679368019104
RMSE train: 1.706758	val: 2.172660	test: 1.844615
MAE train: 1.478162	val: 1.733381	test: 1.523715

Epoch: 46
Loss: 2.1049283742904663
RMSE train: 1.651471	val: 2.111290	test: 1.782636
MAE train: 1.429600	val: 1.671307	test: 1.483669

Epoch: 47
Loss: 2.0977683067321777
RMSE train: 1.599226	val: 2.081099	test: 1.694905
MAE train: 1.371900	val: 1.617553	test: 1.423125

Epoch: 48
Loss: 2.0060354471206665
RMSE train: 1.542554	val: 2.037818	test: 1.631704
MAE train: 1.318464	val: 1.555183	test: 1.358700

Epoch: 49
Loss: 1.6776959896087646
RMSE train: 1.500852	val: 1.990450	test: 1.622563
MAE train: 1.277376	val: 1.502776	test: 1.317643

Epoch: 50
Loss: 1.6559730172157288
RMSE train: 1.419152	val: 1.915468	test: 1.607572
MAE train: 1.191374	val: 1.424890	test: 1.260579

Epoch: 51
Loss: 1.4956183433532715
RMSE train: 1.345198	val: 1.855865	test: 1.583308
MAE train: 1.120059	val: 1.363390	test: 1.222828

Epoch: 52
Loss: 1.445725917816162
RMSE train: 1.270340	val: 1.793957	test: 1.531209
MAE train: 1.031964	val: 1.297610	test: 1.153209

Epoch: 53
Loss: 1.4152488708496094
RMSE train: 1.195279	val: 1.700405	test: 1.483397
MAE train: 0.969317	val: 1.225813	test: 1.099592

Epoch: 54
Loss: 1.2752636075019836
RMSE train: 1.127910	val: 1.634155	test: 1.434185
MAE train: 0.910244	val: 1.161472	test: 1.045777

Epoch: 55
Loss: 1.2613444328308105
RMSE train: 1.093308	val: 1.611410	test: 1.404146
MAE train: 0.883391	val: 1.136172	test: 1.024971

Epoch: 56
Loss: 1.443480759859085
RMSE train: 1.138368	val: 1.663204	test: 1.428734
MAE train: 0.925069	val: 1.163697	test: 1.043301

Epoch: 57
Loss: 1.214410662651062
RMSE train: 1.132924	val: 1.651256	test: 1.384152
MAE train: 0.931826	val: 1.160018	test: 1.042718

Epoch: 58
Loss: 1.1537258923053741
RMSE train: 1.053343	val: 1.576962	test: 1.314426
MAE train: 0.865086	val: 1.112798	test: 0.994738

Epoch: 59
Loss: 1.1765846610069275
RMSE train: 0.971697	val: 1.497408	test: 1.281766
MAE train: 0.786447	val: 1.058134	test: 0.956767

Epoch: 60
Loss: 1.1263519823551178
RMSE train: 0.905662	val: 1.396335	test: 1.311709
MAE train: 0.727373	val: 0.984761	test: 0.940057

Epoch: 61
Loss: 1.0787624716758728
RMSE train: 0.867677	val: 1.347529	test: 1.317701
MAE train: 0.694870	val: 0.934512	test: 0.913789

Epoch: 62
Loss: 1.0233813524246216
RMSE train: 0.879068	val: 1.392360	test: 1.302501
MAE train: 0.702301	val: 0.949151	test: 0.903779

Epoch: 63
Loss: 0.9390078783035278
RMSE train: 0.911467	val: 1.439646	test: 1.315487
MAE train: 0.733237	val: 0.977296	test: 0.925063

Epoch: 64
Loss: 1.1278316378593445
RMSE train: 0.916067	val: 1.422152	test: 1.332629
MAE train: 0.742519	val: 0.982808	test: 0.920681

Epoch: 65
Loss: 0.9350093603134155
RMSE train: 0.918632	val: 1.417192	test: 1.368029
MAE train: 0.732748	val: 0.992461	test: 0.926670

Epoch: 66
Loss: 0.9385952055454254
RMSE train: 0.893905	val: 1.432466	test: 1.360552
MAE train: 0.695980	val: 0.977945	test: 0.894645

Epoch: 67
Loss: 0.8933461308479309
RMSE train: 0.857577	val: 1.396425	test: 1.338481
MAE train: 0.672986	val: 0.942315	test: 0.878272

Epoch: 68
Loss: 0.8823295831680298
RMSE train: 0.812518	val: 1.316567	test: 1.321854
MAE train: 0.650783	val: 0.889222	test: 0.876926

Epoch: 69
Loss: 0.9129771888256073
RMSE train: 0.835177	val: 1.349422	test: 1.283801
MAE train: 0.684294	val: 0.907364	test: 0.910747

Epoch: 70
Loss: 0.9736856520175934
RMSE train: 0.910917	val: 1.429505	test: 1.344886
MAE train: 0.735674	val: 0.953436	test: 0.951688

Epoch: 71
Loss: 0.9747766256332397
RMSE train: 0.944112	val: 1.471454	test: 1.387235
MAE train: 0.763652	val: 1.002521	test: 0.975283

Epoch: 72
Loss: 1.0715427696704865
RMSE train: 0.914571	val: 1.448594	test: 1.389869
MAE train: 0.723296	val: 0.995465	test: 0.948816

Epoch: 73
Loss: 1.1281014680862427
RMSE train: 0.858266	val: 1.408046	test: 1.351092
MAE train: 0.655433	val: 0.945849	test: 0.887806

Epoch: 74
Loss: 0.9204858541488647
RMSE train: 0.776648	val: 1.354880	test: 1.266987
MAE train: 0.579765	val: 0.873245	test: 0.791915

Epoch: 75
Loss: 0.8832318782806396
RMSE train: 0.713556	val: 1.352027	test: 1.151833
MAE train: 0.542647	val: 0.840313	test: 0.719017

Epoch: 76
Loss: 0.9169678092002869
RMSE train: 0.719053	val: 1.357363	test: 1.121864
MAE train: 0.554601	val: 0.836903	test: 0.722076

Epoch: 77
Loss: 0.8826180696487427
RMSE train: 0.689101	val: 1.288938	test: 1.164698
MAE train: 0.531405	val: 0.801452	test: 0.723898

Epoch: 78
Loss: 0.8866540193557739
RMSE train: 0.705538	val: 1.232009	test: 1.260805
MAE train: 0.537511	val: 0.805744	test: 0.757442

Epoch: 79
Loss: 0.9262613654136658
RMSE train: 0.704302	val: 1.244847	test: 1.296021
MAE train: 0.529127	val: 0.833337	test: 0.769088

Epoch: 80
Loss: 0.9245015978813171
RMSE train: 0.717326	val: 1.329482	test: 1.274026
MAE train: 0.548800	val: 0.867611	test: 0.783167

Epoch: 81
Loss: 0.8377008140087128
RMSE train: 0.787307	val: 1.422432	test: 1.312832
MAE train: 0.591169	val: 0.905282	test: 0.840925

Epoch: 82
Loss: 0.8111594617366791
RMSE train: 0.802264	val: 1.440995	test: 1.376345
MAE train: 0.621406	val: 0.932060	test: 0.879660

Epoch: 83
Loss: 0.7762008607387543
RMSE train: 0.793370	val: 1.422023	test: 1.400415
MAE train: 0.637514	val: 0.949186	test: 0.904938

Epoch: 23
Loss: 8.477762222290039
RMSE train: 3.269319	val: 3.721456	test: 3.228797
MAE train: 2.982663	val: 3.270031	test: 2.931998

Epoch: 24
Loss: 8.246252536773682
RMSE train: 3.200500	val: 3.638198	test: 3.172374
MAE train: 2.926788	val: 3.200447	test: 2.868352

Epoch: 25
Loss: 7.679645776748657
RMSE train: 3.108125	val: 3.539507	test: 3.077593
MAE train: 2.836855	val: 3.105690	test: 2.768109

Epoch: 26
Loss: 7.506669282913208
RMSE train: 2.967020	val: 3.403073	test: 2.927261
MAE train: 2.689545	val: 2.957287	test: 2.612141

Epoch: 27
Loss: 7.038567304611206
RMSE train: 2.838692	val: 3.289489	test: 2.768200
MAE train: 2.555183	val: 2.841791	test: 2.457265

Epoch: 28
Loss: 6.698082447052002
RMSE train: 2.730343	val: 3.190415	test: 2.653514
MAE train: 2.437841	val: 2.734617	test: 2.344762

Epoch: 29
Loss: 6.264106750488281
RMSE train: 2.640781	val: 3.095400	test: 2.561193
MAE train: 2.337742	val: 2.622705	test: 2.259333

Epoch: 30
Loss: 5.918837070465088
RMSE train: 2.554125	val: 2.992172	test: 2.486416
MAE train: 2.256047	val: 2.500524	test: 2.204382

Epoch: 31
Loss: 5.605872392654419
RMSE train: 2.455278	val: 2.878682	test: 2.410874
MAE train: 2.172333	val: 2.386823	test: 2.148371

Epoch: 32
Loss: 5.392972350120544
RMSE train: 2.332880	val: 2.743903	test: 2.312993
MAE train: 2.061957	val: 2.274512	test: 2.045265

Epoch: 33
Loss: 4.870384454727173
RMSE train: 2.247243	val: 2.654994	test: 2.288090
MAE train: 1.973845	val: 2.197028	test: 1.981494

Epoch: 34
Loss: 4.843746662139893
RMSE train: 2.197965	val: 2.618680	test: 2.282556
MAE train: 1.909455	val: 2.161639	test: 1.925165

Epoch: 35
Loss: 4.768408298492432
RMSE train: 2.182624	val: 2.626636	test: 2.268601
MAE train: 1.884185	val: 2.161422	test: 1.897343

Epoch: 36
Loss: 4.208856105804443
RMSE train: 2.120685	val: 2.568249	test: 2.174434
MAE train: 1.829776	val: 2.118234	test: 1.813032

Epoch: 37
Loss: 3.9364078044891357
RMSE train: 2.032471	val: 2.454590	test: 2.062982
MAE train: 1.762335	val: 2.023740	test: 1.723949

Epoch: 38
Loss: 3.642942428588867
RMSE train: 1.988316	val: 2.358615	test: 2.050196
MAE train: 1.737778	val: 1.958332	test: 1.710011

Epoch: 39
Loss: 3.500871419906616
RMSE train: 1.970498	val: 2.324661	test: 2.056731
MAE train: 1.726669	val: 1.928007	test: 1.713094

Epoch: 40
Loss: 2.999325394630432
RMSE train: 1.987363	val: 2.355153	test: 2.098041
MAE train: 1.735041	val: 1.915107	test: 1.741734

Epoch: 41
Loss: 3.1599607467651367
RMSE train: 1.965064	val: 2.369006	test: 2.090114
MAE train: 1.700648	val: 1.898964	test: 1.734003

Epoch: 42
Loss: 2.696577310562134
RMSE train: 1.934459	val: 2.357587	test: 2.062359
MAE train: 1.674439	val: 1.885785	test: 1.723188

Epoch: 43
Loss: 2.531066656112671
RMSE train: 1.859728	val: 2.294643	test: 2.006628
MAE train: 1.615158	val: 1.850251	test: 1.671950

Epoch: 44
Loss: 2.49429452419281
RMSE train: 1.750382	val: 2.198472	test: 1.914647
MAE train: 1.517879	val: 1.776253	test: 1.586808

Epoch: 45
Loss: 2.193273186683655
RMSE train: 1.625001	val: 2.069277	test: 1.855538
MAE train: 1.389146	val: 1.647251	test: 1.471162

Epoch: 46
Loss: 2.0746023654937744
RMSE train: 1.536514	val: 1.981018	test: 1.816007
MAE train: 1.285286	val: 1.540924	test: 1.382933

Epoch: 47
Loss: 2.03071391582489
RMSE train: 1.501479	val: 1.942339	test: 1.779949
MAE train: 1.250314	val: 1.496982	test: 1.355812

Epoch: 48
Loss: 1.9123119115829468
RMSE train: 1.493979	val: 1.922165	test: 1.730650
MAE train: 1.258547	val: 1.487625	test: 1.347602

Epoch: 49
Loss: 1.8347037434577942
RMSE train: 1.524404	val: 1.944589	test: 1.741020
MAE train: 1.295299	val: 1.511540	test: 1.393451

Epoch: 50
Loss: 1.5263020396232605
RMSE train: 1.490087	val: 1.929340	test: 1.710485
MAE train: 1.249314	val: 1.474045	test: 1.375873

Epoch: 51
Loss: 1.5102150440216064
RMSE train: 1.409573	val: 1.855661	test: 1.674679
MAE train: 1.164482	val: 1.395186	test: 1.316629

Epoch: 52
Loss: 1.5939052104949951
RMSE train: 1.273013	val: 1.724031	test: 1.565476
MAE train: 1.052541	val: 1.274455	test: 1.213776

Epoch: 53
Loss: 1.241895616054535
RMSE train: 1.128068	val: 1.580954	test: 1.420492
MAE train: 0.940984	val: 1.155067	test: 1.108759

Epoch: 54
Loss: 1.1785635650157928
RMSE train: 1.085880	val: 1.515340	test: 1.386208
MAE train: 0.908342	val: 1.120797	test: 1.069531

Epoch: 55
Loss: 1.7997124195098877
RMSE train: 1.097473	val: 1.555500	test: 1.414035
MAE train: 0.915674	val: 1.118472	test: 1.081392

Epoch: 56
Loss: 1.1652398705482483
RMSE train: 1.125635	val: 1.614458	test: 1.419207
MAE train: 0.904021	val: 1.122500	test: 1.053209

Epoch: 57
Loss: 1.1336933970451355
RMSE train: 1.139473	val: 1.606379	test: 1.452321
MAE train: 0.909135	val: 1.093020	test: 1.052291

Epoch: 58
Loss: 1.0808237195014954
RMSE train: 1.105719	val: 1.541806	test: 1.446213
MAE train: 0.893230	val: 1.066831	test: 1.045373

Epoch: 59
Loss: 1.1222922205924988
RMSE train: 1.044454	val: 1.463791	test: 1.392227
MAE train: 0.842088	val: 1.027552	test: 0.992002

Epoch: 60
Loss: 1.045561522245407
RMSE train: 0.992415	val: 1.421480	test: 1.361211
MAE train: 0.812709	val: 1.004090	test: 0.959567

Epoch: 61
Loss: 1.0179550647735596
RMSE train: 0.946233	val: 1.406491	test: 1.343734
MAE train: 0.774052	val: 0.969310	test: 0.935359

Epoch: 62
Loss: 1.0080081820487976
RMSE train: 0.934133	val: 1.427055	test: 1.331303
MAE train: 0.740576	val: 0.975311	test: 0.895643

Epoch: 63
Loss: 1.0021634101867676
RMSE train: 0.911491	val: 1.419314	test: 1.330901
MAE train: 0.720395	val: 0.989475	test: 0.896402

Epoch: 64
Loss: 0.8888169527053833
RMSE train: 0.890123	val: 1.395717	test: 1.347439
MAE train: 0.713458	val: 0.999390	test: 0.917965

Epoch: 65
Loss: 1.0942684412002563
RMSE train: 0.885056	val: 1.383618	test: 1.359704
MAE train: 0.682218	val: 0.981905	test: 0.889550

Epoch: 66
Loss: 0.9144390821456909
RMSE train: 0.840182	val: 1.336560	test: 1.326516
MAE train: 0.636594	val: 0.929481	test: 0.848111

Epoch: 67
Loss: 1.0099689662456512
RMSE train: 0.775771	val: 1.322328	test: 1.318259
MAE train: 0.585708	val: 0.882918	test: 0.810840

Epoch: 68
Loss: 1.0523894131183624
RMSE train: 0.725212	val: 1.298836	test: 1.275015
MAE train: 0.541671	val: 0.816072	test: 0.749565

Epoch: 69
Loss: 0.9254079163074493
RMSE train: 0.716869	val: 1.254668	test: 1.217339
MAE train: 0.535339	val: 0.773448	test: 0.722106

Epoch: 70
Loss: 1.0301813185214996
RMSE train: 0.718411	val: 1.210689	test: 1.091891
MAE train: 0.570737	val: 0.760322	test: 0.742068

Epoch: 71
Loss: 0.9730704128742218
RMSE train: 0.774869	val: 1.252907	test: 1.145967
MAE train: 0.634454	val: 0.827614	test: 0.812405

Epoch: 72
Loss: 1.12098428606987
RMSE train: 0.854582	val: 1.297599	test: 1.313620
MAE train: 0.694708	val: 0.907831	test: 0.892444

Epoch: 73
Loss: 0.8152884840965271
RMSE train: 0.867888	val: 1.279670	test: 1.459127
MAE train: 0.650772	val: 0.883112	test: 0.884891

Epoch: 74
Loss: 1.0282282829284668
RMSE train: 0.857773	val: 1.283863	test: 1.497690
MAE train: 0.603692	val: 0.859833	test: 0.842322

Epoch: 75
Loss: 0.9800737500190735
RMSE train: 0.845692	val: 1.301047	test: 1.485265
MAE train: 0.575069	val: 0.842513	test: 0.798461

Epoch: 76
Loss: 1.0852358937263489
RMSE train: 0.775848	val: 1.271584	test: 1.374424
MAE train: 0.548983	val: 0.800499	test: 0.755366

Epoch: 77
Loss: 1.053111582994461
RMSE train: 0.751049	val: 1.258190	test: 1.296856
MAE train: 0.567219	val: 0.803694	test: 0.779427

Epoch: 78
Loss: 0.8973080515861511
RMSE train: 0.749242	val: 1.252430	test: 1.253678
MAE train: 0.591932	val: 0.847057	test: 0.821698

Epoch: 79
Loss: 0.9668540358543396
RMSE train: 0.760190	val: 1.276148	test: 1.208447
MAE train: 0.614944	val: 0.889770	test: 0.841001

Epoch: 80
Loss: 0.8159310221672058
RMSE train: 0.797411	val: 1.322679	test: 1.209881
MAE train: 0.641113	val: 0.921754	test: 0.841618

Epoch: 81
Loss: 0.8895609080791473
RMSE train: 0.842284	val: 1.353372	test: 1.260774
MAE train: 0.672042	val: 0.936211	test: 0.848161

Epoch: 82
Loss: 0.8511091470718384
RMSE train: 0.825742	val: 1.355786	test: 1.215415
MAE train: 0.654122	val: 0.911992	test: 0.823681

Epoch: 83
Loss: 0.8311668932437897
RMSE train: 0.792939	val: 1.290161	test: 1.214518
MAE train: 0.617126	val: 0.859894	test: 0.814699

Epoch: 84
Loss: 0.9051235616207123
RMSE train: 0.766980	val: 1.047329	test: 1.255504
MAE train: 0.599269	val: 0.794796	test: 0.892954

Epoch: 85
Loss: 1.0154965817928314
RMSE train: 0.772401	val: 1.050998	test: 1.253610
MAE train: 0.590266	val: 0.773756	test: 0.866840

Epoch: 86
Loss: 0.958042562007904
RMSE train: 0.745680	val: 1.067655	test: 1.235921
MAE train: 0.565538	val: 0.762149	test: 0.824753

Epoch: 87
Loss: 0.7863370776176453
RMSE train: 0.725868	val: 1.085832	test: 1.221062
MAE train: 0.549035	val: 0.761155	test: 0.803334

Epoch: 88
Loss: 0.8490899205207825
RMSE train: 0.720103	val: 1.074920	test: 1.210171
MAE train: 0.543588	val: 0.757932	test: 0.804541

Epoch: 89
Loss: 0.673069953918457
RMSE train: 0.685769	val: 1.054288	test: 1.186034
MAE train: 0.525653	val: 0.746404	test: 0.809579

Epoch: 90
Loss: 0.7954754829406738
RMSE train: 0.688058	val: 1.060156	test: 1.184570
MAE train: 0.541287	val: 0.756938	test: 0.833687

Epoch: 91
Loss: 0.7115474343299866
RMSE train: 0.678173	val: 1.055361	test: 1.180276
MAE train: 0.542820	val: 0.748984	test: 0.826111

Epoch: 92
Loss: 1.1700206398963928
RMSE train: 0.681801	val: 1.046897	test: 1.199901
MAE train: 0.526128	val: 0.737687	test: 0.825362

Epoch: 93
Loss: 0.9223317503929138
RMSE train: 0.690860	val: 1.047876	test: 1.221933
MAE train: 0.529889	val: 0.744319	test: 0.838113

Epoch: 94
Loss: 0.7641388773918152
RMSE train: 0.712683	val: 1.060516	test: 1.246187
MAE train: 0.551440	val: 0.774412	test: 0.858685

Epoch: 95
Loss: 0.7712511122226715
RMSE train: 0.702891	val: 1.057610	test: 1.249769
MAE train: 0.541243	val: 0.772349	test: 0.858140

Epoch: 96
Loss: 0.7599663138389587
RMSE train: 0.692284	val: 1.056259	test: 1.240062
MAE train: 0.537306	val: 0.771756	test: 0.861845

Epoch: 97
Loss: 0.7941187620162964
RMSE train: 0.679024	val: 1.055124	test: 1.227323
MAE train: 0.527862	val: 0.764148	test: 0.857999

Epoch: 98
Loss: 0.9860446155071259
RMSE train: 0.662594	val: 1.057824	test: 1.219024
MAE train: 0.514297	val: 0.760343	test: 0.843230

Epoch: 99
Loss: 0.7203736305236816
RMSE train: 0.715311	val: 1.080233	test: 1.260227
MAE train: 0.531589	val: 0.764605	test: 0.843434

Epoch: 100
Loss: 0.7560241222381592
RMSE train: 0.785998	val: 1.112173	test: 1.308005
MAE train: 0.576896	val: 0.780816	test: 0.873543

Epoch: 101
Loss: 0.8996831178665161
RMSE train: 0.768782	val: 1.091152	test: 1.287672
MAE train: 0.574101	val: 0.775178	test: 0.874407

Epoch: 102
Loss: 0.6916418969631195
RMSE train: 0.716897	val: 1.069994	test: 1.232929
MAE train: 0.558168	val: 0.775924	test: 0.875778

Epoch: 103
Loss: 0.7902767360210419
RMSE train: 0.642178	val: 1.054622	test: 1.175595
MAE train: 0.506517	val: 0.760930	test: 0.836319

Epoch: 104
Loss: 0.793757975101471
RMSE train: 0.585364	val: 1.046104	test: 1.148506
MAE train: 0.456169	val: 0.741144	test: 0.798639

Epoch: 105
Loss: 0.9151861369609833
RMSE train: 0.611850	val: 1.073336	test: 1.161343
MAE train: 0.460544	val: 0.745504	test: 0.778999

Epoch: 106
Loss: 1.0968838334083557
RMSE train: 0.702893	val: 1.133569	test: 1.202300
MAE train: 0.523351	val: 0.770348	test: 0.796101

Epoch: 107
Loss: 0.7683406472206116
RMSE train: 0.741200	val: 1.114742	test: 1.218014
MAE train: 0.573667	val: 0.778938	test: 0.832010

Epoch: 108
Loss: 0.9826694130897522
RMSE train: 0.743633	val: 1.052051	test: 1.217717
MAE train: 0.589005	val: 0.766986	test: 0.855894

Epoch: 109
Loss: 0.9677742719650269
RMSE train: 0.787292	val: 1.044783	test: 1.236769
MAE train: 0.614264	val: 0.785776	test: 0.896689

Epoch: 110
Loss: 0.7804627120494843
RMSE train: 0.758172	val: 1.049699	test: 1.222032
MAE train: 0.588582	val: 0.789735	test: 0.875784

Epoch: 111
Loss: 0.7982814013957977
RMSE train: 0.701019	val: 1.049108	test: 1.209111
MAE train: 0.531261	val: 0.760394	test: 0.823535

Epoch: 112
Loss: 0.8427531123161316
RMSE train: 0.673725	val: 1.076715	test: 1.205698
MAE train: 0.497000	val: 0.752107	test: 0.813020

Epoch: 113
Loss: 0.7089123725891113
RMSE train: 0.686304	val: 1.060796	test: 1.216081
MAE train: 0.510944	val: 0.747741	test: 0.828957

Epoch: 114
Loss: 0.7844590246677399
RMSE train: 0.661013	val: 1.030639	test: 1.205493
MAE train: 0.498872	val: 0.737574	test: 0.836051

Epoch: 115
Loss: 0.9842339158058167
RMSE train: 0.705546	val: 1.042112	test: 1.232043
MAE train: 0.540826	val: 0.758858	test: 0.861748

Epoch: 116
Loss: 0.6796789765357971
RMSE train: 0.707531	val: 1.059422	test: 1.234936
MAE train: 0.545961	val: 0.770534	test: 0.866821

Epoch: 117
Loss: 0.7000342905521393
RMSE train: 0.689442	val: 1.066114	test: 1.221680
MAE train: 0.532026	val: 0.770821	test: 0.849654

Epoch: 118
Loss: 0.6898223459720612
RMSE train: 0.649658	val: 1.056305	test: 1.194748
MAE train: 0.484744	val: 0.751290	test: 0.811123

Epoch: 119
Loss: 0.6820038557052612
RMSE train: 0.605354	val: 1.054547	test: 1.177312
MAE train: 0.451236	val: 0.741111	test: 0.782397

Epoch: 120
Loss: 0.5660115480422974
RMSE train: 0.568754	val: 1.058101	test: 1.171791
MAE train: 0.432477	val: 0.744122	test: 0.781297

Epoch: 121
Loss: 0.6770257949829102
RMSE train: 0.562662	val: 1.054689	test: 1.169156
MAE train: 0.430927	val: 0.740595	test: 0.788338

Epoch: 122
Loss: 0.5434538125991821
RMSE train: 0.612542	val: 1.064645	test: 1.190280
MAE train: 0.471575	val: 0.750751	test: 0.814731

Epoch: 123
Loss: 0.6536222994327545
RMSE train: 0.666230	val: 1.073886	test: 1.218493
MAE train: 0.506000	val: 0.753392	test: 0.833310

Epoch: 124
Loss: 0.6279437243938446
RMSE train: 0.714313	val: 1.075772	test: 1.248216
MAE train: 0.533834	val: 0.753937	test: 0.848168

Epoch: 125
Loss: 0.6622629165649414
RMSE train: 0.713754	val: 1.043173	test: 1.244154
MAE train: 0.535972	val: 0.746085	test: 0.857282

Epoch: 126
Loss: 0.70074263215065
RMSE train: 0.713225	val: 1.026122	test: 1.232875
MAE train: 0.548849	val: 0.750974	test: 0.875406

Epoch: 127
Loss: 1.001548320055008
RMSE train: 0.630258	val: 1.016135	test: 1.187910
MAE train: 0.498756	val: 0.746186	test: 0.841763

Epoch: 128
Loss: 0.8200135827064514
RMSE train: 0.642724	val: 1.050802	test: 1.201538
MAE train: 0.504578	val: 0.761324	test: 0.838539

Epoch: 129
Loss: 0.5997721552848816
RMSE train: 0.680055	val: 1.076197	test: 1.219439
MAE train: 0.524074	val: 0.773986	test: 0.841729

Epoch: 130
Loss: 0.6358093917369843
RMSE train: 0.720605	val: 1.080224	test: 1.241143
MAE train: 0.551009	val: 0.777678	test: 0.861739

Epoch: 131
Loss: 0.7451586127281189
RMSE train: 0.780879	val: 1.081004	test: 1.283746
MAE train: 0.603349	val: 0.793130	test: 0.903594

Epoch: 132
Loss: 0.6738407015800476
RMSE train: 0.822474	val: 1.073395	test: 1.322622
MAE train: 0.634201	val: 0.795473	test: 0.936309

Epoch: 133
Loss: 0.6319757103919983
RMSE train: 0.836502	val: 1.072984	test: 1.338694
MAE train: 0.644468	val: 0.801680	test: 0.960352

Epoch: 134
Loss: 0.7053993940353394
RMSE train: 0.791435	val: 1.045213	test: 1.309359
MAE train: 0.616091	val: 0.782362	test: 0.936910

Epoch: 135
Loss: 0.626461535692215
RMSE train: 0.707510	val: 1.017839	test: 1.248908
MAE train: 0.559884	val: 0.755579	test: 0.886687

Epoch: 136
Loss: 0.6985694766044617
RMSE train: 0.645643	val: 1.008941	test: 1.202495
MAE train: 0.502797	val: 0.724325	test: 0.824949

Epoch: 137
Loss: 0.611294150352478
RMSE train: 0.576588	val: 0.991596	test: 1.154055
MAE train: 0.437545	val: 0.687038	test: 0.763423

Epoch: 138
Loss: 0.7758564949035645
RMSE train: 0.559852	val: 1.005727	test: 1.132225
MAE train: 0.416854	val: 0.685648	test: 0.737090

Epoch: 139
Loss: 0.7221017181873322
RMSE train: 0.527529	val: 0.983435	test: 1.103975
MAE train: 0.398257	val: 0.678232	test: 0.717240

Epoch: 140
Loss: 0.6448855400085449
RMSE train: 0.543467	val: 0.972216	test: 1.099031
MAE train: 0.420627	val: 0.690541	test: 0.740946

Epoch: 141
Loss: 0.6224835216999054
RMSE train: 0.551858	val: 0.966059	test: 1.104364
MAE train: 0.431998	val: 0.700070	test: 0.752622

Epoch: 142
Loss: 0.6085445731878281
RMSE train: 0.563457	val: 0.972374	test: 1.121656
MAE train: 0.436784	val: 0.704126	test: 0.766483

Epoch: 143
Loss: 0.6239831745624542
RMSE train: 0.582512	val: 0.998173	test: 1.143141
MAE train: 0.454239	val: 0.715537	test: 0.786536

Epoch: 144
Loss: 0.575037807226181
RMSE train: 0.641438	val: 1.030279	test: 1.186137

Epoch: 84
Loss: 1.074549674987793
RMSE train: 0.939594	val: 1.107201	test: 1.318689
MAE train: 0.749276	val: 0.814975	test: 0.931862

Epoch: 85
Loss: 0.8754028081893921
RMSE train: 0.889287	val: 1.003304	test: 1.291823
MAE train: 0.705073	val: 0.760103	test: 0.920545

Epoch: 86
Loss: 0.8890566229820251
RMSE train: 0.872511	val: 0.943758	test: 1.286555
MAE train: 0.679534	val: 0.718429	test: 0.922408

Epoch: 87
Loss: 0.9950918555259705
RMSE train: 0.877416	val: 0.939848	test: 1.295067
MAE train: 0.674421	val: 0.706946	test: 0.921849

Epoch: 88
Loss: 0.9594473838806152
RMSE train: 0.808390	val: 0.923465	test: 1.250098
MAE train: 0.626278	val: 0.693094	test: 0.895264

Epoch: 89
Loss: 0.9001086354255676
RMSE train: 0.750793	val: 0.897492	test: 1.215731
MAE train: 0.568369	val: 0.674487	test: 0.848928

Epoch: 90
Loss: 0.7896555960178375
RMSE train: 0.708246	val: 0.876142	test: 1.178229
MAE train: 0.533859	val: 0.670052	test: 0.817256

Epoch: 91
Loss: 0.7382624447345734
RMSE train: 0.721523	val: 0.887459	test: 1.200423
MAE train: 0.555401	val: 0.684875	test: 0.831049

Epoch: 92
Loss: 1.10589599609375
RMSE train: 0.761152	val: 0.916635	test: 1.220746
MAE train: 0.600674	val: 0.699930	test: 0.849614

Epoch: 93
Loss: 0.797503650188446
RMSE train: 0.830907	val: 0.962677	test: 1.279685
MAE train: 0.665676	val: 0.734189	test: 0.883960

Epoch: 94
Loss: 1.0006969571113586
RMSE train: 0.820041	val: 0.945758	test: 1.297525
MAE train: 0.646861	val: 0.730768	test: 0.879767

Epoch: 95
Loss: 0.8329082727432251
RMSE train: 0.770869	val: 0.889180	test: 1.284736
MAE train: 0.600957	val: 0.698049	test: 0.860043

Epoch: 96
Loss: 1.0014754235744476
RMSE train: 0.824032	val: 0.903192	test: 1.321368
MAE train: 0.642005	val: 0.707343	test: 0.887622

Epoch: 97
Loss: 0.7979164123535156
RMSE train: 0.884538	val: 0.933544	test: 1.358783
MAE train: 0.689027	val: 0.731965	test: 0.926740

Epoch: 98
Loss: 0.823612779378891
RMSE train: 0.926194	val: 0.954477	test: 1.377632
MAE train: 0.723993	val: 0.747740	test: 0.956007

Epoch: 99
Loss: 0.8567636013031006
RMSE train: 0.920110	val: 0.945195	test: 1.363187
MAE train: 0.715781	val: 0.735176	test: 0.942377

Epoch: 100
Loss: 0.8079509437084198
RMSE train: 0.900995	val: 0.949657	test: 1.331664
MAE train: 0.716308	val: 0.729285	test: 0.919582

Epoch: 101
Loss: 0.9774677455425262
RMSE train: 0.889581	val: 0.979340	test: 1.302258
MAE train: 0.716530	val: 0.744462	test: 0.899009

Epoch: 102
Loss: 0.8215073347091675
RMSE train: 0.834882	val: 0.960883	test: 1.248237
MAE train: 0.679789	val: 0.745316	test: 0.870246

Epoch: 103
Loss: 0.8769044876098633
RMSE train: 0.806270	val: 0.937925	test: 1.199378
MAE train: 0.666304	val: 0.733627	test: 0.861562

Epoch: 104
Loss: 0.691331297159195
RMSE train: 0.770652	val: 0.920508	test: 1.169185
MAE train: 0.617533	val: 0.704235	test: 0.840194

Epoch: 105
Loss: 0.6823429465293884
RMSE train: 0.747122	val: 0.912091	test: 1.167544
MAE train: 0.577576	val: 0.691472	test: 0.832229

Epoch: 106
Loss: 0.7112798690795898
RMSE train: 0.713951	val: 0.894466	test: 1.170425
MAE train: 0.542131	val: 0.671391	test: 0.822330

Epoch: 107
Loss: 0.7075071930885315
RMSE train: 0.684258	val: 0.886165	test: 1.185345
MAE train: 0.512006	val: 0.656396	test: 0.821180

Epoch: 108
Loss: 0.9524426460266113
RMSE train: 0.667101	val: 0.879754	test: 1.196332
MAE train: 0.503629	val: 0.651890	test: 0.820581

Epoch: 109
Loss: 0.9218718409538269
RMSE train: 0.680817	val: 0.870303	test: 1.217758
MAE train: 0.497386	val: 0.639239	test: 0.809359

Epoch: 110
Loss: 0.8781405091285706
RMSE train: 0.687213	val: 0.888679	test: 1.234543
MAE train: 0.482849	val: 0.633187	test: 0.796952

Epoch: 111
Loss: 0.8199287950992584
RMSE train: 0.733042	val: 0.926343	test: 1.259889
MAE train: 0.522972	val: 0.655956	test: 0.819075

Epoch: 112
Loss: 0.8331574499607086
RMSE train: 0.722325	val: 0.919341	test: 1.230736
MAE train: 0.534363	val: 0.661993	test: 0.824666

Epoch: 113
Loss: 0.7545204758644104
RMSE train: 0.706663	val: 0.912959	test: 1.192250
MAE train: 0.545834	val: 0.674405	test: 0.833098

Epoch: 114
Loss: 0.6394765675067902
RMSE train: 0.683257	val: 0.902010	test: 1.153528
MAE train: 0.529635	val: 0.669870	test: 0.809270

Epoch: 115
Loss: 0.6355724036693573
RMSE train: 0.648106	val: 0.892245	test: 1.112325
MAE train: 0.506850	val: 0.661285	test: 0.765501

Epoch: 116
Loss: 0.6772588491439819
RMSE train: 0.607838	val: 0.873468	test: 1.071362
MAE train: 0.473958	val: 0.637179	test: 0.720389

Epoch: 117
Loss: 0.7668934166431427
RMSE train: 0.611440	val: 0.863670	test: 1.045690
MAE train: 0.463842	val: 0.633112	test: 0.698788

Epoch: 118
Loss: 0.9504528045654297
RMSE train: 0.616251	val: 0.866661	test: 1.085753
MAE train: 0.475545	val: 0.632663	test: 0.725834

Epoch: 119
Loss: 0.8543253540992737
RMSE train: 0.681033	val: 0.872340	test: 1.150591
MAE train: 0.526705	val: 0.634541	test: 0.792141

Epoch: 120
Loss: 0.7495278120040894
RMSE train: 0.704825	val: 0.873343	test: 1.179217
MAE train: 0.542344	val: 0.635340	test: 0.809224

Epoch: 121
Loss: 0.8940297067165375
RMSE train: 0.786392	val: 0.929873	test: 1.251568
MAE train: 0.605712	val: 0.680297	test: 0.850272

Epoch: 122
Loss: 0.6322254836559296
RMSE train: 0.828761	val: 0.976426	test: 1.301149
MAE train: 0.645823	val: 0.739311	test: 0.897409

Epoch: 123
Loss: 0.6274559497833252
RMSE train: 0.813768	val: 0.970742	test: 1.298322
MAE train: 0.640853	val: 0.753025	test: 0.908694

Epoch: 124
Loss: 0.7513184547424316
RMSE train: 0.769600	val: 0.925838	test: 1.264768
MAE train: 0.628461	val: 0.712206	test: 0.891518

Epoch: 125
Loss: 0.8162701427936554
RMSE train: 0.723272	val: 0.896381	test: 1.220762
MAE train: 0.589362	val: 0.676801	test: 0.858114

Epoch: 126
Loss: 0.5905906558036804
RMSE train: 0.659727	val: 0.866571	test: 1.164333
MAE train: 0.521398	val: 0.628329	test: 0.801311

Epoch: 127
Loss: 0.8425982594490051
RMSE train: 0.642213	val: 0.845570	test: 1.138041
MAE train: 0.495761	val: 0.601050	test: 0.764979

Epoch: 128
Loss: 0.6516571938991547
RMSE train: 0.679048	val: 0.866091	test: 1.167306
MAE train: 0.518762	val: 0.617541	test: 0.779047

Epoch: 129
Loss: 0.5841353237628937
RMSE train: 0.673883	val: 0.870177	test: 1.165163
MAE train: 0.516400	val: 0.629419	test: 0.782484

Epoch: 130
Loss: 0.7292484641075134
RMSE train: 0.667392	val: 0.876411	test: 1.163707
MAE train: 0.518483	val: 0.646907	test: 0.802110

Epoch: 131
Loss: 0.6947174072265625
RMSE train: 0.699503	val: 0.893540	test: 1.193535
MAE train: 0.542565	val: 0.673895	test: 0.828892

Epoch: 132
Loss: 1.0723636150360107
RMSE train: 0.750199	val: 0.915568	test: 1.232567
MAE train: 0.582277	val: 0.694187	test: 0.853458

Epoch: 133
Loss: 0.579412579536438
RMSE train: 0.776319	val: 0.919812	test: 1.246966
MAE train: 0.619650	val: 0.698422	test: 0.877376

Epoch: 134
Loss: 0.6278952658176422
RMSE train: 0.779489	val: 0.906699	test: 1.250717
MAE train: 0.634800	val: 0.697262	test: 0.888089

Epoch: 135
Loss: 0.6694422960281372
RMSE train: 0.697612	val: 0.850305	test: 1.189103
MAE train: 0.552987	val: 0.638785	test: 0.829665

Epoch: 136
Loss: 0.690201997756958
RMSE train: 0.626437	val: 0.811108	test: 1.123822
MAE train: 0.485053	val: 0.602796	test: 0.780777

Epoch: 137
Loss: 0.8046826422214508
RMSE train: 0.585661	val: 0.829197	test: 1.084601
MAE train: 0.453096	val: 0.607097	test: 0.742512

Epoch: 138
Loss: 0.7000635862350464
RMSE train: 0.582831	val: 0.847189	test: 1.091317
MAE train: 0.457730	val: 0.618347	test: 0.740961

Epoch: 139
Loss: 0.5644339621067047
RMSE train: 0.576915	val: 0.845082	test: 1.101798
MAE train: 0.449200	val: 0.615648	test: 0.745929

Epoch: 140
Loss: 0.890563577413559
RMSE train: 0.568660	val: 0.834535	test: 1.106378
MAE train: 0.436737	val: 0.608423	test: 0.747616

Epoch: 141
Loss: 0.7270777225494385
RMSE train: 0.600922	val: 0.834214	test: 1.141349
MAE train: 0.455698	val: 0.610516	test: 0.772626

Epoch: 142
Loss: 0.7035430073738098
RMSE train: 0.578978	val: 0.805699	test: 1.131034
MAE train: 0.439553	val: 0.595981	test: 0.758865

Epoch: 143
Loss: 0.8125452399253845
RMSE train: 0.564342	val: 0.785317	test: 1.110279
MAE train: 0.441610	val: 0.577007	test: 0.743064

Epoch: 144
Loss: 0.6708855330944061
RMSE train: 0.574910	val: 0.799926	test: 1.120126

Epoch: 84
Loss: 1.166293442249298
RMSE train: 0.771300	val: 1.014767	test: 1.245405
MAE train: 0.596664	val: 0.743831	test: 0.865047

Epoch: 85
Loss: 0.9665370285511017
RMSE train: 0.808651	val: 1.027717	test: 1.288348
MAE train: 0.630641	val: 0.764750	test: 0.899977

Epoch: 86
Loss: 1.011281132698059
RMSE train: 0.847138	val: 1.066387	test: 1.345468
MAE train: 0.669899	val: 0.798563	test: 0.929393

Epoch: 87
Loss: 1.0671956241130829
RMSE train: 0.882747	val: 1.108285	test: 1.369621
MAE train: 0.712929	val: 0.827667	test: 0.954101

Epoch: 88
Loss: 0.8211791217327118
RMSE train: 0.888940	val: 1.086655	test: 1.374232
MAE train: 0.728118	val: 0.829242	test: 0.967417

Epoch: 89
Loss: 0.8774022161960602
RMSE train: 0.852201	val: 1.058754	test: 1.353105
MAE train: 0.707362	val: 0.834772	test: 0.952723

Epoch: 90
Loss: 0.8970562219619751
RMSE train: 0.819474	val: 1.036878	test: 1.344052
MAE train: 0.683329	val: 0.817650	test: 0.930783

Epoch: 91
Loss: 0.8238089680671692
RMSE train: 0.781128	val: 0.997264	test: 1.332202
MAE train: 0.642856	val: 0.779859	test: 0.910462

Epoch: 92
Loss: 0.814076155424118
RMSE train: 0.751971	val: 0.969664	test: 1.329112
MAE train: 0.600732	val: 0.749552	test: 0.895996

Epoch: 93
Loss: 1.0238879323005676
RMSE train: 0.745360	val: 0.957201	test: 1.345482
MAE train: 0.582681	val: 0.739700	test: 0.896061

Epoch: 94
Loss: 0.9094187617301941
RMSE train: 0.737343	val: 0.942129	test: 1.356547
MAE train: 0.573461	val: 0.725061	test: 0.897151

Epoch: 95
Loss: 0.7231785953044891
RMSE train: 0.755946	val: 0.948526	test: 1.387730
MAE train: 0.588336	val: 0.732763	test: 0.924106

Epoch: 96
Loss: 0.6879515051841736
RMSE train: 0.767536	val: 0.962355	test: 1.405102
MAE train: 0.616660	val: 0.752171	test: 0.952976

Epoch: 97
Loss: 0.8037281036376953
RMSE train: 0.751076	val: 0.958686	test: 1.421169
MAE train: 0.611759	val: 0.754536	test: 0.953178

Epoch: 98
Loss: 0.7893227338790894
RMSE train: 0.723342	val: 0.964378	test: 1.417095
MAE train: 0.574685	val: 0.724087	test: 0.922409

Epoch: 99
Loss: 0.7430188357830048
RMSE train: 0.707760	val: 0.962963	test: 1.386294
MAE train: 0.552927	val: 0.713170	test: 0.906196

Epoch: 100
Loss: 0.8266023993492126
RMSE train: 0.709039	val: 0.963937	test: 1.382269
MAE train: 0.549837	val: 0.712884	test: 0.888794

Epoch: 101
Loss: 0.7606533765792847
RMSE train: 0.681339	val: 0.933639	test: 1.355261
MAE train: 0.532295	val: 0.694907	test: 0.869359

Epoch: 102
Loss: 0.8120084404945374
RMSE train: 0.648555	val: 0.905769	test: 1.325459
MAE train: 0.509971	val: 0.672679	test: 0.840589

Epoch: 103
Loss: 0.750412255525589
RMSE train: 0.637792	val: 0.887100	test: 1.306783
MAE train: 0.503922	val: 0.667068	test: 0.835863

Epoch: 104
Loss: 0.7158875167369843
RMSE train: 0.649133	val: 0.887132	test: 1.318146
MAE train: 0.490519	val: 0.657509	test: 0.833408

Epoch: 105
Loss: 0.7278775274753571
RMSE train: 0.631630	val: 0.875534	test: 1.309946
MAE train: 0.470389	val: 0.639300	test: 0.820781

Epoch: 106
Loss: 0.7176003456115723
RMSE train: 0.620937	val: 0.885426	test: 1.315982
MAE train: 0.464761	val: 0.633706	test: 0.822133

Epoch: 107
Loss: 0.6851722598075867
RMSE train: 0.664066	val: 0.963248	test: 1.343032
MAE train: 0.502929	val: 0.674857	test: 0.848009

Epoch: 108
Loss: 0.8643966615200043
RMSE train: 0.693041	val: 1.002883	test: 1.356943
MAE train: 0.546453	val: 0.715001	test: 0.884806

Epoch: 109
Loss: 0.7825068831443787
RMSE train: 0.651049	val: 0.965247	test: 1.324538
MAE train: 0.507109	val: 0.694230	test: 0.866487

Epoch: 110
Loss: 0.7110492885112762
RMSE train: 0.620031	val: 0.919940	test: 1.299591
MAE train: 0.483487	val: 0.669919	test: 0.846304

Epoch: 111
Loss: 0.7876823842525482
RMSE train: 0.603122	val: 0.900795	test: 1.287079
MAE train: 0.471225	val: 0.664764	test: 0.829326

Epoch: 112
Loss: 0.7939403653144836
RMSE train: 0.611761	val: 0.894901	test: 1.292742
MAE train: 0.473187	val: 0.671099	test: 0.827820

Epoch: 113
Loss: 0.8587343394756317
RMSE train: 0.612095	val: 0.889535	test: 1.284279
MAE train: 0.466168	val: 0.666515	test: 0.821428

Epoch: 114
Loss: 0.8332234919071198
RMSE train: 0.607987	val: 0.891375	test: 1.286263
MAE train: 0.467555	val: 0.665056	test: 0.817178

Epoch: 115
Loss: 1.1424659490585327
RMSE train: 0.652317	val: 0.920004	test: 1.308427
MAE train: 0.510401	val: 0.693832	test: 0.846612

Epoch: 116
Loss: 0.7861309349536896
RMSE train: 0.683316	val: 0.951159	test: 1.329778
MAE train: 0.544016	val: 0.707182	test: 0.882410

Epoch: 117
Loss: 0.9294102191925049
RMSE train: 0.753776	val: 0.994852	test: 1.381842
MAE train: 0.597917	val: 0.736645	test: 0.940020

Epoch: 118
Loss: 0.70295649766922
RMSE train: 0.822708	val: 1.000893	test: 1.420321
MAE train: 0.644395	val: 0.748593	test: 0.974126

Epoch: 119
Loss: 0.6023977100849152
RMSE train: 0.832541	val: 0.954747	test: 1.408933
MAE train: 0.631273	val: 0.722763	test: 0.964286

Epoch: 120
Loss: 0.7043079733848572
RMSE train: 0.747168	val: 0.891496	test: 1.354567
MAE train: 0.566782	val: 0.679851	test: 0.915776

Epoch: 121
Loss: 0.9203565120697021
RMSE train: 0.664254	val: 0.863348	test: 1.313608
MAE train: 0.516691	val: 0.656248	test: 0.856614

Epoch: 122
Loss: 1.1379969418048859
RMSE train: 0.632623	val: 0.871152	test: 1.294313
MAE train: 0.495712	val: 0.649803	test: 0.834309

Epoch: 123
Loss: 0.760448694229126
RMSE train: 0.638553	val: 0.898828	test: 1.288769
MAE train: 0.495382	val: 0.650714	test: 0.828517

Epoch: 124
Loss: 0.6247605383396149
RMSE train: 0.618518	val: 0.916714	test: 1.315414
MAE train: 0.481074	val: 0.648323	test: 0.817195

Epoch: 125
Loss: 0.7986951172351837
RMSE train: 0.610890	val: 0.931477	test: 1.369771
MAE train: 0.470316	val: 0.656960	test: 0.815687

Epoch: 126
Loss: 0.6870492994785309
RMSE train: 0.740963	val: 0.985755	test: 1.455269
MAE train: 0.538000	val: 0.710447	test: 0.864485

Epoch: 127
Loss: 0.8442965149879456
RMSE train: 0.820856	val: 1.042705	test: 1.501791
MAE train: 0.581043	val: 0.738493	test: 0.897915

Epoch: 128
Loss: 0.8109379708766937
RMSE train: 0.809106	val: 1.055272	test: 1.497913
MAE train: 0.590085	val: 0.743601	test: 0.921569

Epoch: 129
Loss: 0.7975955903530121
RMSE train: 0.728362	val: 1.014962	test: 1.433827
MAE train: 0.544771	val: 0.718831	test: 0.925329

Epoch: 130
Loss: 0.9217545688152313
RMSE train: 0.630110	val: 0.939120	test: 1.341850
MAE train: 0.478209	val: 0.684970	test: 0.887864

Epoch: 131
Loss: 0.7255890667438507
RMSE train: 0.597233	val: 0.882978	test: 1.292742
MAE train: 0.448807	val: 0.647095	test: 0.859319

Epoch: 132
Loss: 0.798734039068222
RMSE train: 0.616939	val: 0.863950	test: 1.307594
MAE train: 0.463086	val: 0.637151	test: 0.860474

Epoch: 133
Loss: 0.751344621181488
RMSE train: 0.661321	val: 0.888415	test: 1.341222
MAE train: 0.508887	val: 0.669699	test: 0.890088

Epoch: 134
Loss: 0.7563551068305969
RMSE train: 0.686927	val: 0.912749	test: 1.356229
MAE train: 0.532491	val: 0.687678	test: 0.904118

Epoch: 135
Loss: 0.6432755887508392
RMSE train: 0.707217	val: 0.927661	test: 1.366445
MAE train: 0.557415	val: 0.701684	test: 0.908322

Epoch: 136
Loss: 0.6009634435176849
RMSE train: 0.720990	val: 0.946877	test: 1.374964
MAE train: 0.563327	val: 0.710589	test: 0.903612

Epoch: 137
Loss: 0.584022730588913
RMSE train: 0.710728	val: 0.950594	test: 1.369560
MAE train: 0.542599	val: 0.697320	test: 0.874640

Epoch: 138
Loss: 0.6478741466999054
RMSE train: 0.710739	val: 0.963041	test: 1.368189
MAE train: 0.538559	val: 0.697078	test: 0.875602

Epoch: 139
Loss: 0.6267377138137817
RMSE train: 0.703792	val: 0.966658	test: 1.367474
MAE train: 0.546249	val: 0.701701	test: 0.876028

Epoch: 140
Loss: 0.5859088599681854
RMSE train: 0.680393	val: 0.959444	test: 1.353420
MAE train: 0.552724	val: 0.707190	test: 0.879157

Epoch: 141
Loss: 0.7107547223567963
RMSE train: 0.656885	val: 0.922897	test: 1.328382
MAE train: 0.533177	val: 0.693535	test: 0.866886

Epoch: 142
Loss: 0.5097027122974396
RMSE train: 0.637254	val: 0.900163	test: 1.290697
MAE train: 0.512492	val: 0.677869	test: 0.856482

Epoch: 143
Loss: 0.7447174191474915
RMSE train: 0.678822	val: 0.898584	test: 1.280884
MAE train: 0.533879	val: 0.678944	test: 0.868243

Epoch: 144
Loss: 0.580314040184021
RMSE train: 0.676180	val: 0.897300	test: 1.279181

Epoch: 84
Loss: 0.9292997717857361
RMSE train: 0.772539	val: 1.162981	test: 1.050793
MAE train: 0.606220	val: 0.755876	test: 0.781394

Epoch: 85
Loss: 0.7167993783950806
RMSE train: 0.775862	val: 1.221478	test: 1.035527
MAE train: 0.609562	val: 0.776518	test: 0.779979

Epoch: 86
Loss: 0.8224812150001526
RMSE train: 0.825345	val: 1.263013	test: 1.088998
MAE train: 0.644489	val: 0.813719	test: 0.817837

Epoch: 87
Loss: 1.0632545948028564
RMSE train: 0.786698	val: 1.203920	test: 1.079993
MAE train: 0.622273	val: 0.781642	test: 0.816097

Epoch: 88
Loss: 0.916033923625946
RMSE train: 0.767424	val: 1.137525	test: 1.064526
MAE train: 0.599330	val: 0.758035	test: 0.813008

Epoch: 89
Loss: 0.7937822937965393
RMSE train: 0.823656	val: 1.121732	test: 1.119277
MAE train: 0.617027	val: 0.758577	test: 0.825886

Epoch: 90
Loss: 0.882124125957489
RMSE train: 0.836829	val: 1.127927	test: 1.157888
MAE train: 0.617074	val: 0.759600	test: 0.832813

Epoch: 91
Loss: 0.8073600828647614
RMSE train: 0.799719	val: 1.166700	test: 1.139551
MAE train: 0.584668	val: 0.761526	test: 0.834186

Epoch: 92
Loss: 0.7129186391830444
RMSE train: 0.752292	val: 1.218573	test: 1.083289
MAE train: 0.568660	val: 0.787190	test: 0.831480

Epoch: 93
Loss: 0.7644931972026825
RMSE train: 0.691161	val: 1.158652	test: 1.023518
MAE train: 0.546662	val: 0.759621	test: 0.789434

Epoch: 94
Loss: 0.76246178150177
RMSE train: 0.717948	val: 1.129801	test: 1.062062
MAE train: 0.574912	val: 0.756485	test: 0.812050

Epoch: 95
Loss: 0.8379644453525543
RMSE train: 0.729185	val: 1.157277	test: 1.087531
MAE train: 0.577791	val: 0.774535	test: 0.841931

Epoch: 96
Loss: 0.7637079656124115
RMSE train: 0.768139	val: 1.268144	test: 1.129756
MAE train: 0.599075	val: 0.826571	test: 0.866151

Epoch: 97
Loss: 0.8596546351909637
RMSE train: 0.763089	val: 1.276354	test: 1.122044
MAE train: 0.581418	val: 0.813315	test: 0.840908

Epoch: 98
Loss: 0.7705795466899872
RMSE train: 0.695255	val: 1.176795	test: 1.058197
MAE train: 0.528749	val: 0.748659	test: 0.775470

Epoch: 99
Loss: 0.7896162271499634
RMSE train: 0.665241	val: 1.098318	test: 1.017303
MAE train: 0.505402	val: 0.720539	test: 0.747307

Epoch: 100
Loss: 0.780104011297226
RMSE train: 0.644731	val: 1.054736	test: 0.995042
MAE train: 0.488016	val: 0.693275	test: 0.710363

Epoch: 101
Loss: 0.6767283380031586
RMSE train: 0.631910	val: 1.055488	test: 0.964636
MAE train: 0.485039	val: 0.691015	test: 0.692295

Epoch: 102
Loss: 0.7301836907863617
RMSE train: 0.625506	val: 1.079062	test: 0.948613
MAE train: 0.484387	val: 0.706561	test: 0.703792

Epoch: 103
Loss: 0.7496933937072754
RMSE train: 0.687201	val: 1.168362	test: 0.985482
MAE train: 0.532750	val: 0.736048	test: 0.751159

Epoch: 104
Loss: 0.7418926358222961
RMSE train: 0.744344	val: 1.220846	test: 1.025257
MAE train: 0.580605	val: 0.765989	test: 0.785461

Epoch: 105
Loss: 0.7737064063549042
RMSE train: 0.765552	val: 1.201985	test: 1.039590
MAE train: 0.602764	val: 0.777369	test: 0.812834

Epoch: 106
Loss: 0.8387936353683472
RMSE train: 0.776258	val: 1.158251	test: 1.066767
MAE train: 0.606464	val: 0.783532	test: 0.841866

Epoch: 107
Loss: 0.7810122668743134
RMSE train: 0.782039	val: 1.156039	test: 1.113468
MAE train: 0.618550	val: 0.790668	test: 0.873212

Epoch: 108
Loss: 0.7244596481323242
RMSE train: 0.795004	val: 1.179567	test: 1.156658
MAE train: 0.631529	val: 0.802000	test: 0.888101

Epoch: 109
Loss: 0.7520596981048584
RMSE train: 0.776139	val: 1.182039	test: 1.140727
MAE train: 0.609136	val: 0.798391	test: 0.854612

Epoch: 110
Loss: 0.7221873104572296
RMSE train: 0.800956	val: 1.228133	test: 1.145683
MAE train: 0.626776	val: 0.826297	test: 0.858056

Epoch: 111
Loss: 0.6459088921546936
RMSE train: 0.801022	val: 1.217223	test: 1.131014
MAE train: 0.628146	val: 0.822641	test: 0.849849

Epoch: 112
Loss: 0.5602092742919922
RMSE train: 0.774275	val: 1.195262	test: 1.106478
MAE train: 0.610832	val: 0.809980	test: 0.836134

Epoch: 113
Loss: 0.5908951759338379
RMSE train: 0.766508	val: 1.190925	test: 1.095359
MAE train: 0.608679	val: 0.801170	test: 0.844525

Epoch: 114
Loss: 0.6157722175121307
RMSE train: 0.760633	val: 1.207103	test: 1.080472
MAE train: 0.591494	val: 0.793475	test: 0.838421

Epoch: 115
Loss: 0.7200640141963959
RMSE train: 0.720286	val: 1.179487	test: 1.037103
MAE train: 0.553279	val: 0.764246	test: 0.797977

Epoch: 116
Loss: 0.7090362310409546
RMSE train: 0.651516	val: 1.130812	test: 0.985934
MAE train: 0.498019	val: 0.727379	test: 0.741605

Epoch: 117
Loss: 0.5815572440624237
RMSE train: 0.624773	val: 1.119105	test: 0.969953
MAE train: 0.469129	val: 0.710153	test: 0.709802

Epoch: 118
Loss: 0.5774025022983551
RMSE train: 0.621290	val: 1.124211	test: 0.964732
MAE train: 0.460771	val: 0.700435	test: 0.702155

Epoch: 119
Loss: 0.6182166039943695
RMSE train: 0.642526	val: 1.146542	test: 0.972673
MAE train: 0.500620	val: 0.730350	test: 0.729875

Epoch: 120
Loss: 0.7556828260421753
RMSE train: 0.627172	val: 1.150600	test: 0.967125
MAE train: 0.505527	val: 0.737741	test: 0.742019

Epoch: 121
Loss: 0.8094459772109985
RMSE train: 0.592979	val: 1.153205	test: 0.958007
MAE train: 0.460313	val: 0.703105	test: 0.719695

Early stopping
Best (RMSE):	 train: 0.830288	val: 1.009541	test: 1.066598
Best (MAE):	 train: 0.661704	val: 0.730811	test: 0.810392

MAE train: 0.525787	val: 0.669701	test: 0.868179

Epoch: 145
Loss: 0.6250089108943939
RMSE train: 0.653268	val: 0.895860	test: 1.291195
MAE train: 0.511156	val: 0.659432	test: 0.861600

Epoch: 146
Loss: 0.6921317279338837
RMSE train: 0.657761	val: 0.919420	test: 1.322308
MAE train: 0.513470	val: 0.665021	test: 0.859191

Epoch: 147
Loss: 0.7507513761520386
RMSE train: 0.693277	val: 0.960165	test: 1.360067
MAE train: 0.537119	val: 0.683884	test: 0.870908

Epoch: 148
Loss: 0.672405332326889
RMSE train: 0.768193	val: 0.999718	test: 1.408232
MAE train: 0.580200	val: 0.711536	test: 0.896458

Epoch: 149
Loss: 0.7471035718917847
RMSE train: 0.839585	val: 1.005650	test: 1.443612
MAE train: 0.637718	val: 0.734978	test: 0.940692

Epoch: 150
Loss: 0.5676130950450897
RMSE train: 0.839352	val: 0.972887	test: 1.428793
MAE train: 0.657098	val: 0.735589	test: 0.972754

Epoch: 151
Loss: 0.6437731087207794
RMSE train: 0.751675	val: 0.923016	test: 1.366624
MAE train: 0.594856	val: 0.710247	test: 0.943853

Epoch: 152
Loss: 0.5610509514808655
RMSE train: 0.658827	val: 0.888749	test: 1.319924
MAE train: 0.508808	val: 0.678019	test: 0.892055

Epoch: 153
Loss: 0.7639256715774536
RMSE train: 0.590007	val: 0.882794	test: 1.299297
MAE train: 0.436844	val: 0.653015	test: 0.838472

Epoch: 154
Loss: 0.5601800978183746
RMSE train: 0.590057	val: 0.909986	test: 1.310123
MAE train: 0.437629	val: 0.656945	test: 0.840105

Epoch: 155
Loss: 0.7760283648967743
RMSE train: 0.604089	val: 0.917366	test: 1.308577
MAE train: 0.455656	val: 0.666544	test: 0.849270

Epoch: 156
Loss: 0.621395468711853
RMSE train: 0.625927	val: 0.920994	test: 1.303770
MAE train: 0.475956	val: 0.678561	test: 0.858670

Early stopping
Best (RMSE):	 train: 0.664254	val: 0.863348	test: 1.313608
Best (MAE):	 train: 0.516691	val: 0.656248	test: 0.856614


Epoch: 84
Loss: 1.1467929482460022
RMSE train: 0.823925	val: 1.235958	test: 1.300819
MAE train: 0.654682	val: 0.840050	test: 0.868002

Epoch: 85
Loss: 0.8531641364097595
RMSE train: 0.815579	val: 1.300334	test: 1.281016
MAE train: 0.654714	val: 0.868553	test: 0.874371

Epoch: 86
Loss: 0.8623058199882507
RMSE train: 0.789561	val: 1.349109	test: 1.226125
MAE train: 0.623702	val: 0.874011	test: 0.838366

Epoch: 87
Loss: 0.7428889870643616
RMSE train: 0.776283	val: 1.329356	test: 1.215694
MAE train: 0.623257	val: 0.868788	test: 0.833213

Epoch: 88
Loss: 0.7532758414745331
RMSE train: 0.795963	val: 1.324888	test: 1.232563
MAE train: 0.634406	val: 0.863244	test: 0.834916

Epoch: 89
Loss: 0.9633999466896057
RMSE train: 0.844576	val: 1.380817	test: 1.250138
MAE train: 0.670851	val: 0.901385	test: 0.863743

Epoch: 90
Loss: 0.657058835029602
RMSE train: 0.865358	val: 1.416284	test: 1.253156
MAE train: 0.678164	val: 0.932387	test: 0.876040

Epoch: 91
Loss: 0.7545966506004333
RMSE train: 0.842890	val: 1.397418	test: 1.237237
MAE train: 0.660112	val: 0.922538	test: 0.870716

Epoch: 92
Loss: 0.7583352625370026
RMSE train: 0.786791	val: 1.350248	test: 1.203246
MAE train: 0.630782	val: 0.883670	test: 0.854663

Epoch: 93
Loss: 0.8875508904457092
RMSE train: 0.741377	val: 1.344384	test: 1.176963
MAE train: 0.609253	val: 0.864358	test: 0.826026

Epoch: 94
Loss: 0.7664519250392914
RMSE train: 0.728368	val: 1.374965	test: 1.164201
MAE train: 0.592691	val: 0.871712	test: 0.810838

Epoch: 95
Loss: 0.7401585280895233
RMSE train: 0.702613	val: 1.381805	test: 1.140835
MAE train: 0.536757	val: 0.846288	test: 0.783223

Epoch: 96
Loss: 0.6948701739311218
RMSE train: 0.669392	val: 1.322616	test: 1.125983
MAE train: 0.509631	val: 0.815808	test: 0.779478

Epoch: 97
Loss: 0.816395491361618
RMSE train: 0.674514	val: 1.256454	test: 1.150051
MAE train: 0.518214	val: 0.802917	test: 0.782662

Epoch: 98
Loss: 0.6993933916091919
RMSE train: 0.699727	val: 1.243340	test: 1.168425
MAE train: 0.539397	val: 0.808701	test: 0.806245

Epoch: 99
Loss: 0.7153435051441193
RMSE train: 0.736281	val: 1.307967	test: 1.179934
MAE train: 0.571867	val: 0.860689	test: 0.840029

Epoch: 100
Loss: 0.6019606590270996
RMSE train: 0.777450	val: 1.380414	test: 1.207409
MAE train: 0.607788	val: 0.899148	test: 0.861468

Epoch: 101
Loss: 1.061039686203003
RMSE train: 0.789731	val: 1.400160	test: 1.229978
MAE train: 0.620647	val: 0.903124	test: 0.862157

Epoch: 102
Loss: 0.6009906530380249
RMSE train: 0.777120	val: 1.373510	test: 1.237649
MAE train: 0.613930	val: 0.877535	test: 0.841915

Epoch: 103
Loss: 0.6158018112182617
RMSE train: 0.747559	val: 1.369122	test: 1.210623
MAE train: 0.599898	val: 0.869748	test: 0.835423

Epoch: 104
Loss: 0.6985915005207062
RMSE train: 0.725796	val: 1.348767	test: 1.194887
MAE train: 0.576263	val: 0.846010	test: 0.815455

Epoch: 105
Loss: 0.7629432678222656
RMSE train: 0.754805	val: 1.345513	test: 1.213517
MAE train: 0.597052	val: 0.860466	test: 0.838937

Epoch: 106
Loss: 0.6862190663814545
RMSE train: 0.745626	val: 1.324316	test: 1.200988
MAE train: 0.582821	val: 0.844652	test: 0.835986

Epoch: 107
Loss: 0.7888466119766235
RMSE train: 0.686120	val: 1.260560	test: 1.163628
MAE train: 0.536027	val: 0.790967	test: 0.793692

Epoch: 108
Loss: 0.6726762354373932
RMSE train: 0.662043	val: 1.249913	test: 1.154049
MAE train: 0.503106	val: 0.762410	test: 0.772195

Epoch: 109
Loss: 0.8313109278678894
RMSE train: 0.710919	val: 1.298833	test: 1.188833
MAE train: 0.529677	val: 0.785753	test: 0.805971

Epoch: 110
Loss: 0.6784302890300751
RMSE train: 0.752243	val: 1.310765	test: 1.224171
MAE train: 0.574396	val: 0.814943	test: 0.830714

Epoch: 111
Loss: 0.6251094937324524
RMSE train: 0.779273	val: 1.288887	test: 1.239743
MAE train: 0.607712	val: 0.832902	test: 0.846383

Epoch: 112
Loss: 0.7077423334121704
RMSE train: 0.744489	val: 1.234680	test: 1.216496
MAE train: 0.565100	val: 0.791077	test: 0.805435

Epoch: 113
Loss: 0.8714375197887421
RMSE train: 0.689514	val: 1.219460	test: 1.171613
MAE train: 0.531444	val: 0.782317	test: 0.782019

Epoch: 114
Loss: 0.6294729113578796
RMSE train: 0.608978	val: 1.202830	test: 1.083515
MAE train: 0.476688	val: 0.767715	test: 0.761901

Epoch: 115
Loss: 0.5893873572349548
RMSE train: 0.588559	val: 1.183817	test: 1.026558
MAE train: 0.456910	val: 0.756492	test: 0.741296

Epoch: 116
Loss: 0.6893892586231232
RMSE train: 0.603986	val: 1.178194	test: 0.993344
MAE train: 0.455969	val: 0.755773	test: 0.724383

Epoch: 117
Loss: 0.8779615759849548
RMSE train: 0.625132	val: 1.203910	test: 1.042980
MAE train: 0.487678	val: 0.773232	test: 0.747044

Epoch: 118
Loss: 0.756386935710907
RMSE train: 0.699314	val: 1.248884	test: 1.118386
MAE train: 0.544936	val: 0.801782	test: 0.787726

Epoch: 119
Loss: 0.6701638698577881
RMSE train: 0.784508	val: 1.321157	test: 1.195753
MAE train: 0.606212	val: 0.845557	test: 0.843258

Epoch: 120
Loss: 0.6748485565185547
RMSE train: 0.766992	val: 1.314514	test: 1.182600
MAE train: 0.594092	val: 0.841654	test: 0.850374

Epoch: 121
Loss: 0.6791449785232544
RMSE train: 0.746124	val: 1.270509	test: 1.145927
MAE train: 0.602822	val: 0.846905	test: 0.856203

Early stopping
Best (RMSE):	 train: 0.723073	val: 1.141989	test: 1.206110
Best (MAE):	 train: 0.557795	val: 0.749891	test: 0.791322


Epoch: 84
Loss: 0.8516744077205658
RMSE train: 0.687270	val: 1.274255	test: 0.959288
MAE train: 0.532198	val: 0.822970	test: 0.707863

Epoch: 85
Loss: 0.8085171580314636
RMSE train: 0.665826	val: 1.284245	test: 0.909097
MAE train: 0.512882	val: 0.854363	test: 0.675199

Epoch: 86
Loss: 0.6797570586204529
RMSE train: 0.659153	val: 1.282509	test: 0.887920
MAE train: 0.498030	val: 0.852676	test: 0.649562

Epoch: 87
Loss: 0.7631124556064606
RMSE train: 0.655658	val: 1.287846	test: 0.903289
MAE train: 0.499293	val: 0.853143	test: 0.675251

Epoch: 88
Loss: 0.7659256458282471
RMSE train: 0.654917	val: 1.262322	test: 0.941141
MAE train: 0.512665	val: 0.847260	test: 0.713869

Epoch: 89
Loss: 0.7428886890411377
RMSE train: 0.704147	val: 1.292941	test: 0.988587
MAE train: 0.546910	val: 0.848571	test: 0.746847

Epoch: 90
Loss: 0.734129935503006
RMSE train: 0.746186	val: 1.319294	test: 1.030419
MAE train: 0.578949	val: 0.874885	test: 0.755855

Epoch: 91
Loss: 0.9022282063961029
RMSE train: 0.750914	val: 1.306072	test: 1.040732
MAE train: 0.584685	val: 0.868746	test: 0.754198

Epoch: 92
Loss: 0.7341169118881226
RMSE train: 0.735504	val: 1.259950	test: 1.047723
MAE train: 0.570063	val: 0.811763	test: 0.762100

Epoch: 93
Loss: 0.7651202082633972
RMSE train: 0.684159	val: 1.236309	test: 1.006988
MAE train: 0.528379	val: 0.767907	test: 0.738597

Epoch: 94
Loss: 0.8483546078205109
RMSE train: 0.611588	val: 1.229348	test: 0.935505
MAE train: 0.476993	val: 0.752414	test: 0.685436

Epoch: 95
Loss: 0.7558662593364716
RMSE train: 0.605858	val: 1.218746	test: 0.919693
MAE train: 0.474852	val: 0.763871	test: 0.675213

Epoch: 96
Loss: 0.6147192418575287
RMSE train: 0.618563	val: 1.209217	test: 0.915100
MAE train: 0.486438	val: 0.786354	test: 0.681327

Epoch: 97
Loss: 0.8814805150032043
RMSE train: 0.603627	val: 1.216199	test: 0.895598
MAE train: 0.473802	val: 0.786073	test: 0.664418

Epoch: 98
Loss: 0.6164482235908508
RMSE train: 0.596128	val: 1.228492	test: 0.904551
MAE train: 0.463185	val: 0.775337	test: 0.657511

Epoch: 99
Loss: 0.6257712543010712
RMSE train: 0.614826	val: 1.240170	test: 0.927367
MAE train: 0.467629	val: 0.767027	test: 0.661498

Epoch: 100
Loss: 0.716064840555191
RMSE train: 0.615456	val: 1.255465	test: 0.933257
MAE train: 0.461838	val: 0.771011	test: 0.658460

Epoch: 101
Loss: 0.7724584341049194
RMSE train: 0.610116	val: 1.291110	test: 0.914580
MAE train: 0.455764	val: 0.783868	test: 0.642902

Epoch: 102
Loss: 0.6752639710903168
RMSE train: 0.605557	val: 1.308580	test: 0.904527
MAE train: 0.457404	val: 0.788134	test: 0.661940

Epoch: 103
Loss: 0.8451101779937744
RMSE train: 0.648335	val: 1.337188	test: 0.926821
MAE train: 0.489250	val: 0.814045	test: 0.686900

Epoch: 104
Loss: 0.6740454733371735
RMSE train: 0.685898	val: 1.336381	test: 0.946413
MAE train: 0.519517	val: 0.834978	test: 0.702002

Epoch: 105
Loss: 0.7216478288173676
RMSE train: 0.643705	val: 1.309989	test: 0.913815
MAE train: 0.491144	val: 0.825619	test: 0.680313

Epoch: 106
Loss: 0.7539821863174438
RMSE train: 0.632448	val: 1.298210	test: 0.903828
MAE train: 0.485930	val: 0.822973	test: 0.670233

Epoch: 107
Loss: 0.638978898525238
RMSE train: 0.599976	val: 1.271249	test: 0.886601
MAE train: 0.466480	val: 0.792590	test: 0.650425

Epoch: 108
Loss: 0.6873914897441864
RMSE train: 0.610031	val: 1.266545	test: 0.912444
MAE train: 0.471940	val: 0.781693	test: 0.658766

Epoch: 109
Loss: 0.6376046538352966
RMSE train: 0.641808	val: 1.311467	test: 0.919990
MAE train: 0.497127	val: 0.801713	test: 0.676146

Epoch: 110
Loss: 0.6701378226280212
RMSE train: 0.737427	val: 1.380840	test: 0.974612
MAE train: 0.558240	val: 0.865128	test: 0.710452

Epoch: 111
Loss: 0.5900098383426666
RMSE train: 0.726044	val: 1.338027	test: 0.977403
MAE train: 0.552530	val: 0.851410	test: 0.719835

Epoch: 112
Loss: 0.7328687608242035
RMSE train: 0.649519	val: 1.269812	test: 0.942641
MAE train: 0.502969	val: 0.800903	test: 0.700132

Epoch: 113
Loss: 0.5588712394237518
RMSE train: 0.580615	val: 1.275742	test: 0.881476
MAE train: 0.458391	val: 0.791370	test: 0.663973

Epoch: 114
Loss: 0.6399136483669281
RMSE train: 0.558928	val: 1.269610	test: 0.867204
MAE train: 0.442025	val: 0.790190	test: 0.633888

Epoch: 115
Loss: 0.5182503759860992
RMSE train: 0.584957	val: 1.298789	test: 0.863032
MAE train: 0.449167	val: 0.809818	test: 0.620068

Epoch: 116
Loss: 0.6339617669582367
RMSE train: 0.598797	val: 1.295079	test: 0.875543
MAE train: 0.459496	val: 0.794911	test: 0.632193

Epoch: 117
Loss: 0.7821918427944183
RMSE train: 0.680315	val: 1.338779	test: 0.953381
MAE train: 0.510836	val: 0.818712	test: 0.678709

Epoch: 118
Loss: 0.6498507857322693
RMSE train: 0.730218	val: 1.361871	test: 1.030913
MAE train: 0.537773	val: 0.829972	test: 0.724395

Epoch: 119
Loss: 0.7047808170318604
RMSE train: 0.755733	val: 1.366539	test: 1.085035
MAE train: 0.563820	val: 0.836076	test: 0.784029

Epoch: 120
Loss: 0.6950499713420868
RMSE train: 0.746773	val: 1.382773	test: 1.074258
MAE train: 0.573678	val: 0.865674	test: 0.805660

Epoch: 121
Loss: 0.6052093803882599
RMSE train: 0.710792	val: 1.396886	test: 1.009683
MAE train: 0.560177	val: 0.884342	test: 0.779207

Early stopping
Best (RMSE):	 train: 0.707551	val: 1.087067	test: 1.093028
Best (MAE):	 train: 0.543126	val: 0.734491	test: 0.793924


Epoch: 84
Loss: 0.6929469406604767
RMSE train: 0.757079	val: 1.229815	test: 1.198449
MAE train: 0.577347	val: 0.813357	test: 0.790834

Epoch: 85
Loss: 0.9522086977958679
RMSE train: 0.763028	val: 1.245503	test: 1.183124
MAE train: 0.584891	val: 0.824983	test: 0.791989

Epoch: 86
Loss: 0.7396791577339172
RMSE train: 0.795871	val: 1.263204	test: 1.234199
MAE train: 0.603394	val: 0.836155	test: 0.807796

Epoch: 87
Loss: 0.7797899544239044
RMSE train: 0.826961	val: 1.296418	test: 1.250317
MAE train: 0.639165	val: 0.875600	test: 0.841692

Epoch: 88
Loss: 0.7489287853240967
RMSE train: 0.805965	val: 1.288271	test: 1.210831
MAE train: 0.657363	val: 0.906181	test: 0.857696

Epoch: 89
Loss: 0.8369113802909851
RMSE train: 0.787688	val: 1.275689	test: 1.140578
MAE train: 0.654292	val: 0.913558	test: 0.836172

Epoch: 90
Loss: 0.7305164337158203
RMSE train: 0.734024	val: 1.264334	test: 1.031532
MAE train: 0.599039	val: 0.884331	test: 0.768173

Epoch: 91
Loss: 0.7552268207073212
RMSE train: 0.703151	val: 1.256887	test: 0.979601
MAE train: 0.564490	val: 0.869349	test: 0.725930

Epoch: 92
Loss: 0.8032859265804291
RMSE train: 0.687635	val: 1.235449	test: 1.019626
MAE train: 0.541578	val: 0.854405	test: 0.718495

Epoch: 93
Loss: 0.6952070593833923
RMSE train: 0.700139	val: 1.242190	test: 1.120388
MAE train: 0.540598	val: 0.855874	test: 0.742221

Epoch: 94
Loss: 0.8045942485332489
RMSE train: 0.760135	val: 1.284863	test: 1.221144
MAE train: 0.585647	val: 0.885359	test: 0.789310

Epoch: 95
Loss: 0.8038777112960815
RMSE train: 0.778795	val: 1.328826	test: 1.222856
MAE train: 0.600477	val: 0.902538	test: 0.789829

Epoch: 96
Loss: 0.7767379879951477
RMSE train: 0.792968	val: 1.395066	test: 1.198899
MAE train: 0.627372	val: 0.937967	test: 0.802964

Epoch: 97
Loss: 0.7720015048980713
RMSE train: 0.797018	val: 1.433398	test: 1.147979
MAE train: 0.646520	val: 0.972025	test: 0.818666

Epoch: 98
Loss: 0.8404450118541718
RMSE train: 0.709980	val: 1.355502	test: 1.095939
MAE train: 0.581411	val: 0.928315	test: 0.784750

Epoch: 99
Loss: 0.7602781355381012
RMSE train: 0.635305	val: 1.278554	test: 1.094510
MAE train: 0.505197	val: 0.876533	test: 0.753838

Epoch: 100
Loss: 0.7804258763790131
RMSE train: 0.615683	val: 1.270671	test: 1.085306
MAE train: 0.486286	val: 0.866194	test: 0.731646

Epoch: 101
Loss: 0.7124256491661072
RMSE train: 0.617797	val: 1.306030	test: 1.095832
MAE train: 0.485819	val: 0.871809	test: 0.705067

Epoch: 102
Loss: 0.7025832235813141
RMSE train: 0.616047	val: 1.325277	test: 1.112760
MAE train: 0.473495	val: 0.860858	test: 0.674383

Epoch: 103
Loss: 0.6799671053886414
RMSE train: 0.632065	val: 1.337360	test: 1.129669
MAE train: 0.477561	val: 0.855848	test: 0.669015

Epoch: 104
Loss: 0.7565646469593048
RMSE train: 0.627110	val: 1.323910	test: 1.137137
MAE train: 0.473975	val: 0.852795	test: 0.678487

Epoch: 105
Loss: 1.050445020198822
RMSE train: 0.610809	val: 1.299311	test: 1.118035
MAE train: 0.465043	val: 0.826031	test: 0.681582

Epoch: 106
Loss: 0.6168463826179504
RMSE train: 0.624009	val: 1.241998	test: 1.140708
MAE train: 0.477910	val: 0.811074	test: 0.723487

Epoch: 107
Loss: 0.624044269323349
RMSE train: 0.663290	val: 1.253801	test: 1.166246
MAE train: 0.512431	val: 0.822775	test: 0.752410

Epoch: 108
Loss: 0.6698189675807953
RMSE train: 0.701339	val: 1.290854	test: 1.185482
MAE train: 0.546624	val: 0.862150	test: 0.790664

Epoch: 109
Loss: 0.5561446845531464
RMSE train: 0.700674	val: 1.304354	test: 1.182565
MAE train: 0.538231	val: 0.858503	test: 0.777877

Epoch: 110
Loss: 0.6798120141029358
RMSE train: 0.687446	val: 1.321465	test: 1.128459
MAE train: 0.522285	val: 0.863827	test: 0.737676

Epoch: 111
Loss: 0.7116251587867737
RMSE train: 0.649000	val: 1.280152	test: 1.054672
MAE train: 0.499036	val: 0.858117	test: 0.693605

Epoch: 112
Loss: 0.6259937584400177
RMSE train: 0.637515	val: 1.252925	test: 1.072518
MAE train: 0.497850	val: 0.850872	test: 0.673827

Epoch: 113
Loss: 0.6735172867774963
RMSE train: 0.641217	val: 1.248810	test: 1.086187
MAE train: 0.504956	val: 0.847554	test: 0.687129

Epoch: 114
Loss: 0.6874725818634033
RMSE train: 0.657901	val: 1.260520	test: 1.138357
MAE train: 0.518945	val: 0.858795	test: 0.713558

Epoch: 115
Loss: 0.5639587044715881
RMSE train: 0.682910	val: 1.309268	test: 1.171558
MAE train: 0.536310	val: 0.888786	test: 0.746885

Epoch: 116
Loss: 0.644322007894516
RMSE train: 0.711468	val: 1.299299	test: 1.214163
MAE train: 0.556747	val: 0.907640	test: 0.783197

Epoch: 117
Loss: 0.6132409125566483
RMSE train: 0.722779	val: 1.324570	test: 1.175257
MAE train: 0.572102	val: 0.942218	test: 0.794222

Epoch: 118
Loss: 0.6102372705936432
RMSE train: 0.679605	val: 1.302153	test: 1.103015
MAE train: 0.534855	val: 0.926647	test: 0.758351

Epoch: 119
Loss: 0.7750485837459564
RMSE train: 0.649938	val: 1.307856	test: 1.040113
MAE train: 0.504071	val: 0.921606	test: 0.719055

Epoch: 120
Loss: 0.5221185237169266
RMSE train: 0.638840	val: 1.304595	test: 1.028283
MAE train: 0.498061	val: 0.906954	test: 0.706017

Epoch: 121
Loss: 0.6438201665878296
RMSE train: 0.618221	val: 1.275882	test: 1.032828
MAE train: 0.493273	val: 0.877836	test: 0.705511

Early stopping
Best (RMSE):	 train: 0.718411	val: 1.210689	test: 1.091891
Best (MAE):	 train: 0.570737	val: 0.760322	test: 0.742068


Epoch: 84
Loss: 0.8747597336769104
RMSE train: 0.642684	val: 1.163132	test: 0.974458
MAE train: 0.495808	val: 0.775635	test: 0.734343

Epoch: 85
Loss: 0.8590642213821411
RMSE train: 0.662361	val: 1.227543	test: 0.967784
MAE train: 0.513301	val: 0.793065	test: 0.748783

Epoch: 86
Loss: 0.8100482225418091
RMSE train: 0.708261	val: 1.249701	test: 1.020783
MAE train: 0.564672	val: 0.818757	test: 0.814918

Epoch: 87
Loss: 0.739098995923996
RMSE train: 0.736794	val: 1.262524	test: 1.067215
MAE train: 0.587498	val: 0.833100	test: 0.852494

Epoch: 88
Loss: 0.8672827780246735
RMSE train: 0.781077	val: 1.294382	test: 1.119030
MAE train: 0.602257	val: 0.858501	test: 0.871044

Epoch: 89
Loss: 0.9604001045227051
RMSE train: 0.786187	val: 1.261871	test: 1.132744
MAE train: 0.599432	val: 0.839172	test: 0.858614

Epoch: 90
Loss: 0.7902356088161469
RMSE train: 0.796683	val: 1.260427	test: 1.133728
MAE train: 0.595330	val: 0.832045	test: 0.831571

Epoch: 91
Loss: 0.9110344648361206
RMSE train: 0.787316	val: 1.284015	test: 1.106364
MAE train: 0.592454	val: 0.841354	test: 0.803173

Epoch: 92
Loss: 0.8797734677791595
RMSE train: 0.765292	val: 1.322157	test: 1.063798
MAE train: 0.599162	val: 0.859417	test: 0.792598

Epoch: 93
Loss: 0.9264524281024933
RMSE train: 0.767389	val: 1.347480	test: 1.056499
MAE train: 0.598423	val: 0.883385	test: 0.805023

Epoch: 94
Loss: 1.007392406463623
RMSE train: 0.737636	val: 1.280744	test: 1.046308
MAE train: 0.581806	val: 0.852501	test: 0.828529

Epoch: 95
Loss: 0.6278981864452362
RMSE train: 0.742138	val: 1.212207	test: 1.066531
MAE train: 0.580420	val: 0.823772	test: 0.848553

Epoch: 96
Loss: 0.7280402481555939
RMSE train: 0.738946	val: 1.203890	test: 1.067934
MAE train: 0.568648	val: 0.812978	test: 0.849200

Epoch: 97
Loss: 1.1246770322322845
RMSE train: 0.711839	val: 1.243084	test: 1.035862
MAE train: 0.545315	val: 0.813803	test: 0.822445

Epoch: 98
Loss: 0.6649524867534637
RMSE train: 0.733048	val: 1.350361	test: 1.030192
MAE train: 0.568360	val: 0.859207	test: 0.806791

Epoch: 99
Loss: 0.747162938117981
RMSE train: 0.747191	val: 1.395265	test: 1.031474
MAE train: 0.585103	val: 0.895018	test: 0.807903

Epoch: 100
Loss: 0.6855350732803345
RMSE train: 0.762895	val: 1.399953	test: 1.052200
MAE train: 0.600704	val: 0.913349	test: 0.825398

Epoch: 101
Loss: 0.7571051716804504
RMSE train: 0.751258	val: 1.356002	test: 1.073478
MAE train: 0.602866	val: 0.895734	test: 0.851523

Epoch: 102
Loss: 0.7401272356510162
RMSE train: 0.767954	val: 1.327133	test: 1.116239
MAE train: 0.606515	val: 0.879673	test: 0.882590

Epoch: 103
Loss: 0.761732667684555
RMSE train: 0.781208	val: 1.309608	test: 1.144033
MAE train: 0.605722	val: 0.881831	test: 0.910213

Epoch: 104
Loss: 0.7207591831684113
RMSE train: 0.758732	val: 1.267775	test: 1.130502
MAE train: 0.586090	val: 0.862014	test: 0.896315

Epoch: 105
Loss: 0.6104088127613068
RMSE train: 0.705537	val: 1.221917	test: 1.076076
MAE train: 0.555652	val: 0.818880	test: 0.845248

Epoch: 106
Loss: 0.5845034420490265
RMSE train: 0.668978	val: 1.203569	test: 1.044922
MAE train: 0.527955	val: 0.795051	test: 0.804427

Epoch: 107
Loss: 0.6049308776855469
RMSE train: 0.672022	val: 1.203810	test: 1.054166
MAE train: 0.531688	val: 0.794556	test: 0.804199

Epoch: 108
Loss: 0.6783933341503143
RMSE train: 0.672018	val: 1.206543	test: 1.057690
MAE train: 0.528215	val: 0.800766	test: 0.817377

Epoch: 109
Loss: 0.7278266847133636
RMSE train: 0.675426	val: 1.220770	test: 1.059697
MAE train: 0.518794	val: 0.800885	test: 0.822989

Epoch: 110
Loss: 0.641106903553009
RMSE train: 0.679588	val: 1.235975	test: 1.055513
MAE train: 0.517075	val: 0.791639	test: 0.815412

Epoch: 111
Loss: 0.7679237425327301
RMSE train: 0.630557	val: 1.171604	test: 1.025288
MAE train: 0.481723	val: 0.759549	test: 0.790081

Epoch: 112
Loss: 0.6523913741111755
RMSE train: 0.631628	val: 1.138156	test: 1.032212
MAE train: 0.480901	val: 0.757111	test: 0.783871

Epoch: 113
Loss: 0.5960389077663422
RMSE train: 0.621649	val: 1.125097	test: 1.032025
MAE train: 0.468989	val: 0.751218	test: 0.786163

Epoch: 114
Loss: 0.7252521514892578
RMSE train: 0.653232	val: 1.172272	test: 1.048165
MAE train: 0.494985	val: 0.767047	test: 0.795708

Epoch: 115
Loss: 0.6278151869773865
RMSE train: 0.607885	val: 1.165923	test: 1.018194
MAE train: 0.466935	val: 0.761210	test: 0.780792

Epoch: 116
Loss: 0.597982257604599
RMSE train: 0.565287	val: 1.159440	test: 0.990393
MAE train: 0.442969	val: 0.758384	test: 0.756745

Epoch: 117
Loss: 0.6380459070205688
RMSE train: 0.543305	val: 1.156860	test: 0.983126
MAE train: 0.435190	val: 0.760245	test: 0.748747

Epoch: 118
Loss: 0.7243334054946899
RMSE train: 0.510050	val: 1.148542	test: 0.971635
MAE train: 0.394944	val: 0.728293	test: 0.722651

Epoch: 119
Loss: 0.9955624043941498
RMSE train: 0.570524	val: 1.198213	test: 0.994665
MAE train: 0.422585	val: 0.722888	test: 0.736035

Epoch: 120
Loss: 0.6127626299858093
RMSE train: 0.607238	val: 1.205131	test: 1.017254
MAE train: 0.449844	val: 0.733253	test: 0.760997

Epoch: 121
Loss: 0.6667642593383789
RMSE train: 0.621694	val: 1.177033	test: 1.032165
MAE train: 0.465982	val: 0.743571	test: 0.774946

Epoch: 122
Loss: 0.620652437210083
RMSE train: 0.632508	val: 1.141382	test: 1.034876
MAE train: 0.485732	val: 0.750532	test: 0.775708

Epoch: 123
Loss: 0.6851739883422852
RMSE train: 0.601565	val: 1.130436	test: 1.000285
MAE train: 0.472930	val: 0.747963	test: 0.747523

Epoch: 124
Loss: 0.613500326871872
RMSE train: 0.572894	val: 1.157207	test: 0.966581
MAE train: 0.455113	val: 0.748679	test: 0.730199

Epoch: 125
Loss: 0.6387640535831451
RMSE train: 0.617714	val: 1.241816	test: 0.967509
MAE train: 0.477935	val: 0.785619	test: 0.746126

Epoch: 126
Loss: 0.6882712543010712
RMSE train: 0.692591	val: 1.303208	test: 1.003525
MAE train: 0.521539	val: 0.820865	test: 0.774758

Epoch: 127
Loss: 0.7070733308792114
RMSE train: 0.713042	val: 1.261272	test: 1.039841
MAE train: 0.542820	val: 0.832746	test: 0.814221

Epoch: 128
Loss: 0.5890132188796997
RMSE train: 0.746098	val: 1.217774	test: 1.080935
MAE train: 0.585479	val: 0.834630	test: 0.862530

Epoch: 129
Loss: 0.5577951669692993
RMSE train: 0.785617	val: 1.217925	test: 1.123348
MAE train: 0.610665	val: 0.842891	test: 0.887326

Epoch: 130
Loss: 0.5934949517250061
RMSE train: 0.756864	val: 1.211913	test: 1.109809
MAE train: 0.578274	val: 0.826111	test: 0.867408

Epoch: 131
Loss: 0.6002226173877716
RMSE train: 0.699726	val: 1.237142	test: 1.059072
MAE train: 0.521270	val: 0.806846	test: 0.821837

Epoch: 132
Loss: 0.5941648781299591
RMSE train: 0.671399	val: 1.264834	test: 1.019905
MAE train: 0.502132	val: 0.791191	test: 0.781098

Epoch: 133
Loss: 0.6713516116142273
RMSE train: 0.659892	val: 1.263400	test: 1.007073
MAE train: 0.506727	val: 0.800836	test: 0.782177

Epoch: 134
Loss: 0.7101781368255615
RMSE train: 0.723699	val: 1.269023	test: 1.073012
MAE train: 0.563886	val: 0.824203	test: 0.834781

Epoch: 135
Loss: 0.5272126942873001
RMSE train: 0.735714	val: 1.261325	test: 1.093742
MAE train: 0.576329	val: 0.818473	test: 0.848317

Epoch: 136
Loss: 0.6142830848693848
RMSE train: 0.730493	val: 1.281656	test: 1.087327
MAE train: 0.564579	val: 0.831022	test: 0.849800

Epoch: 137
Loss: 0.7054212391376495
RMSE train: 0.722530	val: 1.300852	test: 1.071914
MAE train: 0.544166	val: 0.840228	test: 0.837260

Epoch: 138
Loss: 0.9614072144031525
RMSE train: 0.652165	val: 1.260378	test: 1.007686
MAE train: 0.501412	val: 0.804821	test: 0.792489

Epoch: 139
Loss: 0.5536330342292786
RMSE train: 0.583257	val: 1.210895	test: 0.960411
MAE train: 0.456779	val: 0.767562	test: 0.748416

Epoch: 140
Loss: 0.5426062047481537
RMSE train: 0.585066	val: 1.174216	test: 0.960838
MAE train: 0.455711	val: 0.744469	test: 0.737230

Epoch: 141
Loss: 0.5489209890365601
RMSE train: 0.591410	val: 1.172396	test: 0.961870
MAE train: 0.449342	val: 0.741899	test: 0.733963

Epoch: 142
Loss: 0.5720541477203369
RMSE train: 0.586651	val: 1.178298	test: 0.952406
MAE train: 0.442067	val: 0.737608	test: 0.722657

Epoch: 143
Loss: 0.6234856843948364
RMSE train: 0.573116	val: 1.167762	test: 0.951850
MAE train: 0.438525	val: 0.745966	test: 0.728957

Epoch: 144
Loss: 0.45900292694568634
RMSE train: 0.620731	val: 1.162887	test: 0.995799
MAE train: 0.487678	val: 0.772098	test: 0.779956

Epoch: 145
Loss: 0.4497222304344177
RMSE train: 0.673941	val: 1.174493	test: 1.041971
MAE train: 0.537258	val: 0.795230	test: 0.821740

Epoch: 146
Loss: 0.519481748342514
RMSE train: 0.715337	val: 1.227409	test: 1.066653
MAE train: 0.563205	val: 0.810424	test: 0.841913

Epoch: 147
Loss: 0.6851513385772705
RMSE train: 0.743190	val: 1.282653	test: 1.074407
MAE train: 0.569305	val: 0.813107	test: 0.835047

Epoch: 148
Loss: 0.5561591386795044
RMSE train: 0.703408	val: 1.269157	test: 1.041710
MAE train: 0.536331	val: 0.787216	test: 0.797391

Early stopping
Best (RMSE):	 train: 0.621649	val: 1.125097	test: 1.032025
Best (MAE):	 train: 0.468989	val: 0.751218	test: 0.786163


Epoch: 84
Loss: 0.8615520000457764
RMSE train: 0.791199	val: 1.403504	test: 1.420649
MAE train: 0.635624	val: 0.934305	test: 0.913163

Epoch: 85
Loss: 0.828799307346344
RMSE train: 0.737978	val: 1.376087	test: 1.347630
MAE train: 0.586696	val: 0.880434	test: 0.853651

Epoch: 86
Loss: 0.7986367344856262
RMSE train: 0.712980	val: 1.384528	test: 1.263146
MAE train: 0.566438	val: 0.868193	test: 0.806260

Epoch: 87
Loss: 0.9599506556987762
RMSE train: 0.724539	val: 1.374829	test: 1.235037
MAE train: 0.582145	val: 0.872843	test: 0.810962

Epoch: 88
Loss: 0.6678425669670105
RMSE train: 0.776205	val: 1.402935	test: 1.253060
MAE train: 0.628329	val: 0.919752	test: 0.864227

Epoch: 89
Loss: 0.8384245932102203
RMSE train: 0.787516	val: 1.402692	test: 1.258906
MAE train: 0.643517	val: 0.932412	test: 0.888679

Epoch: 90
Loss: 0.7793707847595215
RMSE train: 0.797953	val: 1.406054	test: 1.312376
MAE train: 0.652821	val: 0.932764	test: 0.909678

Epoch: 91
Loss: 0.7320165932178497
RMSE train: 0.795311	val: 1.410550	test: 1.363236
MAE train: 0.635558	val: 0.921199	test: 0.890044

Epoch: 92
Loss: 0.6636435687541962
RMSE train: 0.839704	val: 1.454119	test: 1.432520
MAE train: 0.642955	val: 0.945581	test: 0.884973

Epoch: 93
Loss: 0.6579251885414124
RMSE train: 0.874635	val: 1.482996	test: 1.481049
MAE train: 0.650034	val: 0.956464	test: 0.892977

Epoch: 94
Loss: 0.8666852712631226
RMSE train: 0.885812	val: 1.476957	test: 1.509148
MAE train: 0.662095	val: 0.960282	test: 0.917274

Epoch: 95
Loss: 0.7985206842422485
RMSE train: 0.847796	val: 1.448767	test: 1.472727
MAE train: 0.662906	val: 0.957991	test: 0.942563

Epoch: 96
Loss: 0.7383479475975037
RMSE train: 0.780440	val: 1.367592	test: 1.433373
MAE train: 0.631180	val: 0.919233	test: 0.923637

Epoch: 97
Loss: 0.8081657290458679
RMSE train: 0.701902	val: 1.313680	test: 1.365141
MAE train: 0.562757	val: 0.865988	test: 0.857683

Epoch: 98
Loss: 0.6629994809627533
RMSE train: 0.676376	val: 1.301742	test: 1.330535
MAE train: 0.517960	val: 0.824576	test: 0.795132

Epoch: 99
Loss: 0.7898347675800323
RMSE train: 0.732139	val: 1.315703	test: 1.373078
MAE train: 0.545679	val: 0.827792	test: 0.796936

Epoch: 100
Loss: 0.7729485332965851
RMSE train: 0.722994	val: 1.297554	test: 1.363636
MAE train: 0.539426	val: 0.818469	test: 0.789252

Epoch: 101
Loss: 0.9642998576164246
RMSE train: 0.695462	val: 1.293710	test: 1.350284
MAE train: 0.530843	val: 0.819813	test: 0.794175

Epoch: 102
Loss: 0.8524812459945679
RMSE train: 0.679043	val: 1.297869	test: 1.323138
MAE train: 0.543502	val: 0.829970	test: 0.809787

Epoch: 103
Loss: 0.6402647197246552
RMSE train: 0.696884	val: 1.319220	test: 1.301561
MAE train: 0.567285	val: 0.849811	test: 0.826776

Epoch: 104
Loss: 0.8512747585773468
RMSE train: 0.664216	val: 1.325497	test: 1.260546
MAE train: 0.535327	val: 0.828052	test: 0.794068

Epoch: 105
Loss: 0.7327215373516083
RMSE train: 0.662255	val: 1.365868	test: 1.245911
MAE train: 0.498812	val: 0.808041	test: 0.761112

Epoch: 106
Loss: 0.7859160602092743
RMSE train: 0.730845	val: 1.396823	test: 1.291481
MAE train: 0.538628	val: 0.835289	test: 0.793868

Epoch: 107
Loss: 0.7844809591770172
RMSE train: 0.781794	val: 1.420800	test: 1.335374
MAE train: 0.584590	val: 0.872521	test: 0.833089

Epoch: 108
Loss: 0.6483598351478577
RMSE train: 0.791406	val: 1.399532	test: 1.355925
MAE train: 0.609619	val: 0.883731	test: 0.852818

Epoch: 109
Loss: 0.7028930485248566
RMSE train: 0.758427	val: 1.357283	test: 1.341342
MAE train: 0.602208	val: 0.865867	test: 0.869164

Epoch: 110
Loss: 0.7749747037887573
RMSE train: 0.707087	val: 1.321006	test: 1.275088
MAE train: 0.552903	val: 0.829448	test: 0.845015

Epoch: 111
Loss: 0.7276595234870911
RMSE train: 0.647394	val: 1.269011	test: 1.213803
MAE train: 0.508090	val: 0.801848	test: 0.798526

Epoch: 112
Loss: 0.8093892931938171
RMSE train: 0.590494	val: 1.206502	test: 1.182392
MAE train: 0.460360	val: 0.758227	test: 0.742617

Epoch: 113
Loss: 0.8722991943359375
RMSE train: 0.627899	val: 1.232649	test: 1.183301
MAE train: 0.484485	val: 0.791543	test: 0.759648

Epoch: 114
Loss: 0.6146706640720367
RMSE train: 0.688456	val: 1.325831	test: 1.169628
MAE train: 0.542439	val: 0.844752	test: 0.788291

Epoch: 115
Loss: 0.7414220869541168
RMSE train: 0.790806	val: 1.454759	test: 1.155429
MAE train: 0.629337	val: 0.942284	test: 0.845458

Epoch: 116
Loss: 0.5760177075862885
RMSE train: 0.781043	val: 1.441462	test: 1.124110
MAE train: 0.630517	val: 0.916800	test: 0.837379

Epoch: 117
Loss: 0.6767413914203644
RMSE train: 0.695131	val: 1.360165	test: 1.102279
MAE train: 0.562569	val: 0.835711	test: 0.777661

Epoch: 118
Loss: 0.6143566966056824
RMSE train: 0.661291	val: 1.286671	test: 1.221007
MAE train: 0.516252	val: 0.775943	test: 0.750833

Epoch: 119
Loss: 0.6736598312854767
RMSE train: 0.683056	val: 1.272006	test: 1.324593
MAE train: 0.516343	val: 0.787920	test: 0.767722

Epoch: 120
Loss: 0.7610232532024384
RMSE train: 0.731936	val: 1.335631	test: 1.361781
MAE train: 0.567719	val: 0.873818	test: 0.820024

Epoch: 121
Loss: 0.5658749341964722
RMSE train: 0.803591	val: 1.409826	test: 1.400871
MAE train: 0.628277	val: 0.949558	test: 0.883711

Epoch: 122
Loss: 0.7296063601970673
RMSE train: 0.851378	val: 1.456166	test: 1.419712
MAE train: 0.673127	val: 0.998169	test: 0.942655

Epoch: 123
Loss: 0.7072261273860931
RMSE train: 0.835757	val: 1.455916	test: 1.398201
MAE train: 0.677950	val: 0.999477	test: 0.946992

Epoch: 124
Loss: 0.6740598082542419
RMSE train: 0.759435	val: 1.399038	test: 1.350639
MAE train: 0.613773	val: 0.943796	test: 0.876091

Epoch: 125
Loss: 0.5895012319087982
RMSE train: 0.699232	val: 1.353876	test: 1.319758
MAE train: 0.558527	val: 0.889309	test: 0.821211

Epoch: 126
Loss: 0.6385138928890228
RMSE train: 0.643683	val: 1.318105	test: 1.311143
MAE train: 0.499209	val: 0.842657	test: 0.769490

Epoch: 127
Loss: 0.7692653834819794
RMSE train: 0.642451	val: 1.320436	test: 1.337606
MAE train: 0.491445	val: 0.846191	test: 0.763555

Epoch: 128
Loss: 0.6076925992965698
RMSE train: 0.651147	val: 1.341295	test: 1.347951
MAE train: 0.501590	val: 0.867901	test: 0.794285

Epoch: 129
Loss: 0.6951811909675598
RMSE train: 0.671705	val: 1.366254	test: 1.356426
MAE train: 0.518269	val: 0.889408	test: 0.831763

Epoch: 130
Loss: 0.6456391215324402
RMSE train: 0.651788	val: 1.352440	test: 1.327456
MAE train: 0.511768	val: 0.878804	test: 0.819612

Epoch: 131
Loss: 0.6387056410312653
RMSE train: 0.643363	val: 1.336916	test: 1.330047
MAE train: 0.508303	val: 0.862691	test: 0.816077

Epoch: 132
Loss: 0.8981757462024689
RMSE train: 0.668962	val: 1.363693	test: 1.324803
MAE train: 0.519241	val: 0.866225	test: 0.812449

Epoch: 133
Loss: 0.6425758302211761
RMSE train: 0.721829	val: 1.383645	test: 1.338753
MAE train: 0.545297	val: 0.874269	test: 0.807608

Epoch: 134
Loss: 0.6514985263347626
RMSE train: 0.689127	val: 1.361994	test: 1.324994
MAE train: 0.521700	val: 0.866575	test: 0.790693

Epoch: 135
Loss: 0.7701747119426727
RMSE train: 0.643689	val: 1.328083	test: 1.335380
MAE train: 0.495014	val: 0.863014	test: 0.774058

Epoch: 136
Loss: 0.6113212406635284
RMSE train: 0.631762	val: 1.308115	test: 1.365230
MAE train: 0.497526	val: 0.868309	test: 0.792490

Epoch: 137
Loss: 0.6344693303108215
RMSE train: 0.630853	val: 1.299815	test: 1.361937
MAE train: 0.504025	val: 0.860016	test: 0.803149

Epoch: 138
Loss: 0.7510192394256592
RMSE train: 0.595328	val: 1.281097	test: 1.339522
MAE train: 0.465347	val: 0.815009	test: 0.757462

Epoch: 139
Loss: 0.5607382357120514
RMSE train: 0.591390	val: 1.297110	test: 1.296568
MAE train: 0.461585	val: 0.816116	test: 0.758121

Epoch: 140
Loss: 0.5688459277153015
RMSE train: 0.627014	val: 1.331580	test: 1.285394
MAE train: 0.487692	val: 0.845339	test: 0.782893

Epoch: 141
Loss: 0.5658625066280365
RMSE train: 0.674866	val: 1.353838	test: 1.350985
MAE train: 0.523654	val: 0.881139	test: 0.824918

Epoch: 142
Loss: 0.6133629977703094
RMSE train: 0.734127	val: 1.384186	test: 1.442973
MAE train: 0.574045	val: 0.919310	test: 0.876027

Epoch: 143
Loss: 0.5630970001220703
RMSE train: 0.756221	val: 1.379377	test: 1.498982
MAE train: 0.595567	val: 0.915539	test: 0.892371

Epoch: 144
Loss: 0.6332940757274628
RMSE train: 0.691883	val: 1.342578	test: 1.443074
MAE train: 0.545011	val: 0.882967	test: 0.854682

Epoch: 145
Loss: 0.6680008471012115
RMSE train: 0.638065	val: 1.304869	test: 1.375474
MAE train: 0.509574	val: 0.855756	test: 0.814231

Epoch: 146
Loss: 0.5517032742500305
RMSE train: 0.610644	val: 1.309347	test: 1.301557
MAE train: 0.484877	val: 0.846932	test: 0.768677

Epoch: 147
Loss: 0.6473096907138824
RMSE train: 0.621694	val: 1.324872	test: 1.294782
MAE train: 0.490763	val: 0.856450	test: 0.774981

Early stopping
Best (RMSE):	 train: 0.590494	val: 1.206502	test: 1.182392
Best (MAE):	 train: 0.460360	val: 0.758227	test: 0.742617
All runs completed.

MAE train: 0.442716	val: 0.568244	test: 0.738730

Epoch: 145
Loss: 0.7009234130382538
RMSE train: 0.591173	val: 0.829563	test: 1.139763
MAE train: 0.449630	val: 0.582930	test: 0.749413

Epoch: 146
Loss: 0.7503939867019653
RMSE train: 0.602340	val: 0.831705	test: 1.174092
MAE train: 0.451477	val: 0.589445	test: 0.751428

Epoch: 147
Loss: 0.6384623348712921
RMSE train: 0.685503	val: 0.870759	test: 1.249566
MAE train: 0.498605	val: 0.622608	test: 0.783471

Epoch: 148
Loss: 0.7971990406513214
RMSE train: 0.704201	val: 0.893750	test: 1.275583
MAE train: 0.514710	val: 0.639948	test: 0.807420

Epoch: 149
Loss: 0.6803922057151794
RMSE train: 0.671372	val: 0.896766	test: 1.247765
MAE train: 0.532259	val: 0.648500	test: 0.822023

Epoch: 150
Loss: 0.5903583467006683
RMSE train: 0.649898	val: 0.886663	test: 1.208300
MAE train: 0.531976	val: 0.640753	test: 0.824854

Epoch: 151
Loss: 0.6461805403232574
RMSE train: 0.652908	val: 0.884060	test: 1.166867
MAE train: 0.533888	val: 0.630347	test: 0.825005

Epoch: 152
Loss: 0.6597415506839752
RMSE train: 0.697219	val: 0.883921	test: 1.164504
MAE train: 0.551589	val: 0.628971	test: 0.818507

Epoch: 153
Loss: 0.5833326578140259
RMSE train: 0.729739	val: 0.868690	test: 1.172748
MAE train: 0.558469	val: 0.628023	test: 0.821096

Epoch: 154
Loss: 0.6823213696479797
RMSE train: 0.709328	val: 0.852114	test: 1.187906
MAE train: 0.543377	val: 0.613883	test: 0.819264

Epoch: 155
Loss: 0.6417774558067322
RMSE train: 0.684063	val: 0.863473	test: 1.207429
MAE train: 0.528152	val: 0.619814	test: 0.804611

Epoch: 156
Loss: 0.6050958633422852
RMSE train: 0.690115	val: 0.885257	test: 1.242986
MAE train: 0.525632	val: 0.635231	test: 0.803345

Epoch: 157
Loss: 0.6979028880596161
RMSE train: 0.704443	val: 0.896538	test: 1.261130
MAE train: 0.536306	val: 0.645344	test: 0.820794

Epoch: 158
Loss: 0.5894290208816528
RMSE train: 0.688779	val: 0.870144	test: 1.252360
MAE train: 0.534335	val: 0.636517	test: 0.825237

Epoch: 159
Loss: 0.5756128430366516
RMSE train: 0.658229	val: 0.830206	test: 1.233180
MAE train: 0.516057	val: 0.620883	test: 0.822569

Epoch: 160
Loss: 0.6002261638641357
RMSE train: 0.629427	val: 0.791557	test: 1.212112
MAE train: 0.489400	val: 0.600514	test: 0.801489

Epoch: 161
Loss: 0.5757494866847992
RMSE train: 0.615859	val: 0.782494	test: 1.188636
MAE train: 0.480358	val: 0.594112	test: 0.793999

Epoch: 162
Loss: 0.701548159122467
RMSE train: 0.609578	val: 0.794667	test: 1.148270
MAE train: 0.488036	val: 0.606021	test: 0.785465

Epoch: 163
Loss: 0.5672082901000977
RMSE train: 0.641786	val: 0.830208	test: 1.135954
MAE train: 0.516596	val: 0.635019	test: 0.803759

Epoch: 164
Loss: 0.7923392653465271
RMSE train: 0.721702	val: 0.892120	test: 1.169529
MAE train: 0.582370	val: 0.687267	test: 0.833410

Epoch: 165
Loss: 0.6872157454490662
RMSE train: 0.767905	val: 0.930164	test: 1.185540
MAE train: 0.620655	val: 0.717210	test: 0.844255

Epoch: 166
Loss: 0.6286345422267914
RMSE train: 0.729637	val: 0.905332	test: 1.180416
MAE train: 0.587156	val: 0.682572	test: 0.827493

Epoch: 167
Loss: 0.609422504901886
RMSE train: 0.688652	val: 0.866200	test: 1.166536
MAE train: 0.551465	val: 0.634918	test: 0.797820

Epoch: 168
Loss: 0.5458121001720428
RMSE train: 0.621252	val: 0.834062	test: 1.159816
MAE train: 0.482812	val: 0.584253	test: 0.760689

Epoch: 169
Loss: 0.6197810024023056
RMSE train: 0.558194	val: 0.814095	test: 1.154389
MAE train: 0.426838	val: 0.562481	test: 0.744269

Epoch: 170
Loss: 0.6428532898426056
RMSE train: 0.568882	val: 0.816650	test: 1.178565
MAE train: 0.439185	val: 0.588999	test: 0.771713

Epoch: 171
Loss: 0.546209990978241
RMSE train: 0.639704	val: 0.862074	test: 1.196660
MAE train: 0.506339	val: 0.635823	test: 0.823192

Epoch: 172
Loss: 0.5529360473155975
RMSE train: 0.684434	val: 0.882437	test: 1.209335
MAE train: 0.544285	val: 0.660411	test: 0.860372

Epoch: 173
Loss: 0.561159610748291
RMSE train: 0.676159	val: 0.872026	test: 1.219088
MAE train: 0.528767	val: 0.645487	test: 0.859038

Epoch: 174
Loss: 0.4985479712486267
RMSE train: 0.636314	val: 0.877848	test: 1.210705
MAE train: 0.490544	val: 0.624488	test: 0.836397

Epoch: 175
Loss: 0.6825480163097382
RMSE train: 0.628471	val: 0.899187	test: 1.202823
MAE train: 0.478855	val: 0.613804	test: 0.807885

Epoch: 176
Loss: 0.5759216248989105
RMSE train: 0.617931	val: 0.894070	test: 1.182388
MAE train: 0.468095	val: 0.614105	test: 0.788301

Epoch: 177
Loss: 0.4819944500923157
RMSE train: 0.614290	val: 0.876162	test: 1.166735
MAE train: 0.486602	val: 0.623023	test: 0.796888

Epoch: 178
Loss: 0.5429355502128601
RMSE train: 0.655768	val: 0.871653	test: 1.176373
MAE train: 0.528949	val: 0.641810	test: 0.814619

Epoch: 179
Loss: 0.5940783619880676
RMSE train: 0.695783	val: 0.864653	test: 1.196205
MAE train: 0.551105	val: 0.642955	test: 0.823924

Epoch: 180
Loss: 0.5130369961261749
RMSE train: 0.698758	val: 0.834814	test: 1.220775
MAE train: 0.550070	val: 0.623103	test: 0.825288

Epoch: 181
Loss: 0.5296662300825119
RMSE train: 0.631692	val: 0.787582	test: 1.213462
MAE train: 0.479556	val: 0.572182	test: 0.778369

Epoch: 182
Loss: 0.6166031062602997
RMSE train: 0.610459	val: 0.806450	test: 1.243242
MAE train: 0.444422	val: 0.573210	test: 0.765917

Epoch: 183
Loss: 0.7959851920604706
RMSE train: 0.594511	val: 0.816543	test: 1.254528
MAE train: 0.420950	val: 0.577867	test: 0.766405

Epoch: 184
Loss: 0.6529054641723633
RMSE train: 0.576077	val: 0.800408	test: 1.249818
MAE train: 0.412498	val: 0.580363	test: 0.760400

Epoch: 185
Loss: 0.5808985233306885
RMSE train: 0.558559	val: 0.797736	test: 1.218340
MAE train: 0.416685	val: 0.580555	test: 0.769923

Epoch: 186
Loss: 0.528139591217041
RMSE train: 0.604510	val: 0.819665	test: 1.192066
MAE train: 0.472103	val: 0.596829	test: 0.813827

Epoch: 187
Loss: 0.46230843663215637
RMSE train: 0.685919	val: 0.862932	test: 1.172951
MAE train: 0.539731	val: 0.631901	test: 0.845440

Epoch: 188
Loss: 0.4985829293727875
RMSE train: 0.685743	val: 0.883767	test: 1.155872
MAE train: 0.533264	val: 0.635466	test: 0.838352

Epoch: 189
Loss: 0.7088245749473572
RMSE train: 0.641787	val: 0.895675	test: 1.133942
MAE train: 0.487254	val: 0.645098	test: 0.798652

Epoch: 190
Loss: 0.7182082533836365
RMSE train: 0.644589	val: 0.908438	test: 1.157091
MAE train: 0.502990	val: 0.659770	test: 0.795356

Epoch: 191
Loss: 0.5419672429561615
RMSE train: 0.647899	val: 0.889540	test: 1.184069
MAE train: 0.506593	val: 0.645220	test: 0.803977

Epoch: 192
Loss: 0.7424913048744202
RMSE train: 0.693604	val: 0.889458	test: 1.206124
MAE train: 0.549989	val: 0.632320	test: 0.815115

Epoch: 193
Loss: 0.6554197520017624
RMSE train: 0.733103	val: 0.907496	test: 1.226899
MAE train: 0.593716	val: 0.653681	test: 0.839474

Epoch: 194
Loss: 0.48620519042015076
RMSE train: 0.723257	val: 0.894301	test: 1.208740
MAE train: 0.582950	val: 0.663199	test: 0.840317

Epoch: 195
Loss: 0.8135167956352234
RMSE train: 0.693834	val: 0.859143	test: 1.152608
MAE train: 0.530319	val: 0.628656	test: 0.802658

Epoch: 196
Loss: 0.7409440875053406
RMSE train: 0.559815	val: 0.823142	test: 1.098942
MAE train: 0.434228	val: 0.582491	test: 0.758649

Early stopping
Best (RMSE):	 train: 0.615859	val: 0.782494	test: 1.188636
Best (MAE):	 train: 0.480358	val: 0.594112	test: 0.793999
All runs completed.

MAE train: 0.490199	val: 0.728154	test: 0.817220

Epoch: 145
Loss: 0.4966139644384384
RMSE train: 0.648457	val: 1.024753	test: 1.195794
MAE train: 0.489922	val: 0.727981	test: 0.822234

Epoch: 146
Loss: 0.5194184482097626
RMSE train: 0.652377	val: 1.014188	test: 1.203933
MAE train: 0.488617	val: 0.724114	test: 0.825533

Epoch: 147
Loss: 0.5857653021812439
RMSE train: 0.707878	val: 1.025146	test: 1.241013
MAE train: 0.531050	val: 0.741177	test: 0.847195

Epoch: 148
Loss: 0.8397940695285797
RMSE train: 0.724089	val: 1.022467	test: 1.260973
MAE train: 0.549099	val: 0.749967	test: 0.862019

Epoch: 149
Loss: 0.7391086220741272
RMSE train: 0.751925	val: 1.041649	test: 1.281281
MAE train: 0.593344	val: 0.768225	test: 0.885104

Epoch: 150
Loss: 0.497677206993103
RMSE train: 0.781489	val: 1.073003	test: 1.296673
MAE train: 0.636803	val: 0.793599	test: 0.913923

Epoch: 151
Loss: 0.7410642206668854
RMSE train: 0.776106	val: 1.081220	test: 1.296000
MAE train: 0.636021	val: 0.802188	test: 0.929140

Epoch: 152
Loss: 0.6037207543849945
RMSE train: 0.762689	val: 1.090412	test: 1.284094
MAE train: 0.619968	val: 0.802804	test: 0.918528

Epoch: 153
Loss: 0.6339939534664154
RMSE train: 0.745714	val: 1.109032	test: 1.272546
MAE train: 0.580661	val: 0.791445	test: 0.893401

Epoch: 154
Loss: 0.7299912869930267
RMSE train: 0.705858	val: 1.126139	test: 1.249382
MAE train: 0.527150	val: 0.776678	test: 0.859048

Epoch: 155
Loss: 0.6956607103347778
RMSE train: 0.676006	val: 1.105867	test: 1.223377
MAE train: 0.501544	val: 0.759377	test: 0.828743

Epoch: 156
Loss: 0.5841598212718964
RMSE train: 0.681670	val: 1.079392	test: 1.227478
MAE train: 0.503309	val: 0.753175	test: 0.822926

Epoch: 157
Loss: 0.6148965060710907
RMSE train: 0.677564	val: 1.048641	test: 1.226826
MAE train: 0.503353	val: 0.745956	test: 0.824765

Epoch: 158
Loss: 0.5394644737243652
RMSE train: 0.637992	val: 1.020616	test: 1.199484
MAE train: 0.488694	val: 0.740336	test: 0.815209

Epoch: 159
Loss: 0.6200262010097504
RMSE train: 0.601128	val: 1.000847	test: 1.178425
MAE train: 0.452489	val: 0.713506	test: 0.776804

Epoch: 160
Loss: 0.7395884096622467
RMSE train: 0.596358	val: 0.983329	test: 1.181508
MAE train: 0.449094	val: 0.696158	test: 0.769394

Epoch: 161
Loss: 0.7364340126514435
RMSE train: 0.617633	val: 0.972817	test: 1.201656
MAE train: 0.472632	val: 0.690284	test: 0.795058

Epoch: 162
Loss: 0.7768730819225311
RMSE train: 0.711904	val: 0.976869	test: 1.254186
MAE train: 0.524402	val: 0.695102	test: 0.834166

Epoch: 163
Loss: 0.5930902361869812
RMSE train: 0.707201	val: 0.976456	test: 1.250477
MAE train: 0.528049	val: 0.702228	test: 0.834624

Epoch: 164
Loss: 0.6586979627609253
RMSE train: 0.635784	val: 0.967496	test: 1.213155
MAE train: 0.495639	val: 0.702323	test: 0.811304

Epoch: 165
Loss: 0.5927375257015228
RMSE train: 0.579316	val: 0.978676	test: 1.185538
MAE train: 0.458580	val: 0.710173	test: 0.813862

Epoch: 166
Loss: 0.6210847795009613
RMSE train: 0.541710	val: 0.976952	test: 1.178261
MAE train: 0.434487	val: 0.698366	test: 0.813571

Epoch: 167
Loss: 0.5781107544898987
RMSE train: 0.528986	val: 0.960583	test: 1.177044
MAE train: 0.415952	val: 0.674662	test: 0.807659

Epoch: 168
Loss: 0.5304418504238129
RMSE train: 0.580767	val: 0.962706	test: 1.210288
MAE train: 0.434421	val: 0.672901	test: 0.816986

Epoch: 169
Loss: 0.8294402360916138
RMSE train: 0.642659	val: 0.977302	test: 1.244088
MAE train: 0.457346	val: 0.673893	test: 0.820528

Epoch: 170
Loss: 0.6854500472545624
RMSE train: 0.617318	val: 0.948603	test: 1.222579
MAE train: 0.445652	val: 0.665189	test: 0.803435

Epoch: 171
Loss: 0.659398153424263
RMSE train: 0.649277	val: 0.941646	test: 1.230787
MAE train: 0.487070	val: 0.680315	test: 0.817176

Epoch: 172
Loss: 0.7594598531723022
RMSE train: 0.673110	val: 0.944144	test: 1.238028
MAE train: 0.530047	val: 0.699131	test: 0.848912

Epoch: 173
Loss: 0.9224505126476288
RMSE train: 0.593142	val: 0.926722	test: 1.183945
MAE train: 0.467877	val: 0.669923	test: 0.801568

Epoch: 174
Loss: 0.542618989944458
RMSE train: 0.530817	val: 0.917722	test: 1.146720
MAE train: 0.410763	val: 0.628889	test: 0.740685

Epoch: 175
Loss: 0.6800921559333801
RMSE train: 0.557503	val: 0.946303	test: 1.153387
MAE train: 0.415102	val: 0.633535	test: 0.738268

Epoch: 176
Loss: 0.3801250010728836
RMSE train: 0.575281	val: 0.978102	test: 1.157994
MAE train: 0.418792	val: 0.651557	test: 0.734296

Epoch: 177
Loss: 0.5009117126464844
RMSE train: 0.578602	val: 1.001691	test: 1.158462
MAE train: 0.421885	val: 0.666796	test: 0.732971

Epoch: 178
Loss: 0.5623621344566345
RMSE train: 0.574376	val: 1.004095	test: 1.156755
MAE train: 0.428465	val: 0.682821	test: 0.743448

Epoch: 179
Loss: 0.5409994125366211
RMSE train: 0.579491	val: 1.002198	test: 1.159308
MAE train: 0.445391	val: 0.694385	test: 0.777554

Epoch: 180
Loss: 0.5092548429965973
RMSE train: 0.597641	val: 0.991290	test: 1.172204
MAE train: 0.462819	val: 0.710531	test: 0.820995

Epoch: 181
Loss: 0.5233798921108246
RMSE train: 0.612377	val: 0.975500	test: 1.189643
MAE train: 0.458230	val: 0.700258	test: 0.835518

Epoch: 182
Loss: 0.586495578289032
RMSE train: 0.589778	val: 0.962238	test: 1.185548
MAE train: 0.433706	val: 0.675143	test: 0.819565

Epoch: 183
Loss: 0.5773197412490845
RMSE train: 0.510797	val: 0.941253	test: 1.149350
MAE train: 0.380626	val: 0.648003	test: 0.771610

Epoch: 184
Loss: 0.47269487380981445
RMSE train: 0.488169	val: 0.946037	test: 1.138095
MAE train: 0.365848	val: 0.647357	test: 0.744596

Epoch: 185
Loss: 0.5208742916584015
RMSE train: 0.529233	val: 0.956418	test: 1.156214
MAE train: 0.397542	val: 0.660643	test: 0.746987

Epoch: 186
Loss: 0.5125864297151566
RMSE train: 0.602085	val: 0.980984	test: 1.185959
MAE train: 0.452837	val: 0.686130	test: 0.774823

Epoch: 187
Loss: 0.566071629524231
RMSE train: 0.606485	val: 0.983706	test: 1.187368
MAE train: 0.458055	val: 0.690146	test: 0.786980

Epoch: 188
Loss: 0.4969712793827057
RMSE train: 0.578572	val: 0.989348	test: 1.171641
MAE train: 0.445842	val: 0.688200	test: 0.793069

Epoch: 189
Loss: 0.5038201808929443
RMSE train: 0.528271	val: 0.996016	test: 1.147264
MAE train: 0.409419	val: 0.679175	test: 0.776595

Epoch: 190
Loss: 0.41540132462978363
RMSE train: 0.497520	val: 1.001284	test: 1.136940
MAE train: 0.382593	val: 0.666832	test: 0.773543

Epoch: 191
Loss: 0.45498640835285187
RMSE train: 0.517305	val: 1.015062	test: 1.149413
MAE train: 0.394704	val: 0.674943	test: 0.794572

Epoch: 192
Loss: 0.47888365387916565
RMSE train: 0.533415	val: 1.005612	test: 1.155187
MAE train: 0.416412	val: 0.683206	test: 0.802884

Epoch: 193
Loss: 0.5336077213287354
RMSE train: 0.542536	val: 0.982180	test: 1.163622
MAE train: 0.422679	val: 0.679733	test: 0.809247

Epoch: 194
Loss: 0.8354231119155884
RMSE train: 0.558326	val: 0.982172	test: 1.178224
MAE train: 0.429077	val: 0.685207	test: 0.823853

Epoch: 195
Loss: 0.5372965782880783
RMSE train: 0.605603	val: 0.978683	test: 1.200707
MAE train: 0.460586	val: 0.678849	test: 0.828302

Epoch: 196
Loss: 0.43500618636608124
RMSE train: 0.615405	val: 0.983876	test: 1.199113
MAE train: 0.470342	val: 0.679200	test: 0.819418

Epoch: 197
Loss: 0.5365826189517975
RMSE train: 0.647205	val: 1.014411	test: 1.216273
MAE train: 0.489461	val: 0.695413	test: 0.825699

Epoch: 198
Loss: 0.5116593539714813
RMSE train: 0.620917	val: 1.012767	test: 1.202388
MAE train: 0.464323	val: 0.690981	test: 0.813122

Epoch: 199
Loss: 0.5215740203857422
RMSE train: 0.621818	val: 0.999393	test: 1.204040
MAE train: 0.477267	val: 0.701580	test: 0.829911

Epoch: 200
Loss: 0.6279098391532898
RMSE train: 0.636745	val: 0.986017	test: 1.216483
MAE train: 0.487433	val: 0.701853	test: 0.844050

Epoch: 201
Loss: 0.48953601717948914
RMSE train: 0.632639	val: 0.989093	test: 1.236007
MAE train: 0.461810	val: 0.688694	test: 0.820256

Epoch: 202
Loss: 0.5485937297344208
RMSE train: 0.593288	val: 0.991066	test: 1.231138
MAE train: 0.431852	val: 0.676562	test: 0.791878

Epoch: 203
Loss: 0.4761107414960861
RMSE train: 0.539729	val: 0.982615	test: 1.200614
MAE train: 0.401863	val: 0.662113	test: 0.765423

Epoch: 204
Loss: 0.7416591644287109
RMSE train: 0.524912	val: 0.978483	test: 1.177373
MAE train: 0.404373	val: 0.660025	test: 0.759578

Epoch: 205
Loss: 0.40561801195144653
RMSE train: 0.516435	val: 0.976328	test: 1.161552
MAE train: 0.402131	val: 0.661672	test: 0.762482

Epoch: 206
Loss: 0.5812332630157471
RMSE train: 0.553581	val: 0.979468	test: 1.177918
MAE train: 0.431834	val: 0.678048	test: 0.783457

Epoch: 207
Loss: 0.5045825988054276
RMSE train: 0.598843	val: 0.989351	test: 1.198515
MAE train: 0.465885	val: 0.692336	test: 0.798400

Epoch: 208
Loss: 0.4654280096292496
RMSE train: 0.628168	val: 0.997287	test: 1.211145
MAE train: 0.478502	val: 0.696346	test: 0.805082

Epoch: 209
Loss: 0.5809429585933685
RMSE train: 0.608002	val: 1.010761	test: 1.200123
MAE train: 0.458603	val: 0.695723	test: 0.798835

Early stopping
Best (RMSE):	 train: 0.530817	val: 0.917722	test: 1.146720
Best (MAE):	 train: 0.410763	val: 0.628889	test: 0.740685
All runs completed.
