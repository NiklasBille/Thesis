>>> Starting run for dataset: bbbp
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running SCAFF configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.6.yml on cuda:0
Running SCAFF configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.7.yml on cuda:1
Running SCAFF configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.8.yml on cuda:2
Starting process for seed 1: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.8.yml --runseed 1 --device cuda:2
Starting process for seed 2: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.8.yml --runseed 2 --device cuda:2
Starting process for seed 1: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.7.yml --runseed 1 --device cuda:1
Starting process for seed 3: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.8.yml --runseed 3 --device cuda:2
Starting process for seed 1: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.6.yml --runseed 1 --device cuda:0
Starting process for seed 2: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.7.yml --runseed 2 --device cuda:1
Starting process for seed 3: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.7.yml --runseed 3 --device cuda:1
Starting process for seed 2: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.6.yml --runseed 2 --device cuda:0
Starting process for seed 3: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.6.yml --runseed 3 --device cuda:0
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] [14:43:16] WARNING: not removing hydrogen atom without neighborsWARNING: not removing hydrogen atom without neighbors[14:43:16] 

WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] [14:43:16] WARNING: not removing hydrogen atom without neighborsWARNING: not removing hydrogen atom without neighbors

[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] [14:43:16] WARNING: not removing hydrogen atom without neighborsWARNING: not removing hydrogen atom without neighbors

[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:16] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] [14:43:17] WARNING: not removing hydrogen atom without neighborsWARNING: not removing hydrogen atom without neighbors

[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] [14:43:17] WARNING: not removing hydrogen atom without neighborsWARNING: not removing hydrogen atom without neighbors

[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] [14:43:17] WARNING: not removing hydrogen atom without neighborsWARNING: not removing hydrogen atom without neighbors

[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] [14:43:17] WARNING: not removing hydrogen atom without neighborsWARNING: not removing hydrogen atom without neighbors

[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] [14:43:17] WARNING: not removing hydrogen atom without neighborsWARNING: not removing hydrogen atom without neighbors

[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] [14:43:17] WARNING: not removing hydrogen atom without neighborsWARNING: not removing hydrogen atom without neighbors

[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
[14:43:17] WARNING: not removing hydrogen atom without neighbors
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/bbbp/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/bbbp/scaff/train_prop=0.6/bbbp_scaff_3_20-05_14-43-16  ]
[ Using Seed :  3  ]
[ Using device :  cuda:0  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=60Traceback (most recent call last):
  File "molecule_finetune.py", line 263, in <module>
    val_result, val_target, val_pred, val_loss = eval(model, device, val_loader, compute_loss=True)
  File "molecule_finetune.py", line 107, in eval
    return {'ROC': sum(roc_list) / len(roc_list), 'PRC': sum(prc_list) / len(prc_list)}, y_true, y_scores, total_loss/len(loader)
ZeroDivisionError: division by zero
0, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6523952375501072
0 is invalid
0
Some target is missing!
Missing ratio: 1.000000
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/bbbp/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/bbbp/scaff/train_prop=0.6/bbbp_scaff_2_20-05_14-43-16  ]
[ Using Seed :  2  ]
[ Using device :  cuda:0  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6502621871335991
0 is invalid
0
Some target is missing!
Missing ratio: 1.000000
Traceback (most recent call last):
  File "molecule_finetune.py", line 263, in <module>
    val_result, val_target, val_pred, val_loss = eval(model, device, val_loader, compute_loss=True)
  File "molecule_finetune.py", line 107, in eval
    return {'ROC': sum(roc_list) / len(roc_list), 'PRC': sum(prc_list) / len(prc_list)}, y_true, y_scores, total_loss/len(loader)
ZeroDivisionError: division by zero
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/bbbp/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/bbbp/scaff/train_prop=0.6/bbbp_scaff_1_20-05_14-43-16  ]
[ Using Seed :  1  ]
[ Using device :  cuda:0  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6549766215023366
0 is invalid
0
Some target is missing!
Missing ratio: 1.000000
Traceback (most recent call last):
  File "molecule_finetune.py", line 263, in <module>
    val_result, val_target, val_pred, val_loss = eval(model, device, val_loader, compute_loss=True)
  File "molecule_finetune.py", line 107, in eval
    return {'ROC': sum(roc_list) / len(roc_list), 'PRC': sum(prc_list) / len(prc_list)}, y_true, y_scores, total_loss/len(loader)
ZeroDivisionError: division by zero
All runs completed.
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/bbbp/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/bbbp/scaff/train_prop=0.7/bbbp_scaff_3_20-05_14-43-16  ]
[ Using Seed :  3  ]
[ Using device :  cuda:1  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.639358891790358
ROC train: 0.789276	val: 0.878344	test: 0.732751
PRC train: 0.936823	val: 0.982660	test: 0.639824

Epoch: 2
Loss: 0.5235123019421617
ROC train: 0.820355	val: 0.874237	test: 0.722670
PRC train: 0.941431	val: 0.980344	test: 0.610193

Epoch: 3
Loss: 0.4452596975301982
ROC train: 0.848598	val: 0.915751	test: 0.745833
PRC train: 0.955459	val: 0.988240	test: 0.641199

Epoch: 4
Loss: 0.3930796260706327
ROC train: 0.866734	val: 0.933955	test: 0.754480
PRC train: 0.960804	val: 0.991138	test: 0.653112

Epoch: 5
Loss: 0.3623138996971866
ROC train: 0.889402	val: 0.944389	test: 0.748970
PRC train: 0.967368	val: 0.992930	test: 0.648442

Epoch: 6
Loss: 0.3402837223026087
ROC train: 0.897096	val: 0.944167	test: 0.759588
PRC train: 0.969703	val: 0.992587	test: 0.647692

Epoch: 7
Loss: 0.2974289022268452
ROC train: 0.920292	val: 0.944500	test: 0.768324
PRC train: 0.976926	val: 0.992623	test: 0.657987

Epoch: 8
Loss: 0.2834262131964411
ROC train: 0.935800	val: 0.931291	test: 0.780466
PRC train: 0.981649	val: 0.989447	test: 0.677241

Epoch: 9
Loss: 0.27733788696208284
ROC train: 0.942077	val: 0.918415	test: 0.779301
PRC train: 0.983416	val: 0.984764	test: 0.679702

Epoch: 10
Loss: 0.26410022073878797
ROC train: 0.945312	val: 0.912754	test: 0.765188
PRC train: 0.984256	val: 0.978321	test: 0.668145

Epoch: 11
Loss: 0.26409374201945895
ROC train: 0.950384	val: 0.898546	test: 0.776075
PRC train: 0.986249	val: 0.970007	test: 0.682727

Epoch: 12
Loss: 0.23980090928960027
ROC train: 0.955298	val: 0.889555	test: 0.771281
PRC train: 0.987827	val: 0.967193	test: 0.666205

Epoch: 13
Loss: 0.22988754514684906
ROC train: 0.955703	val: 0.887779	test: 0.779301
PRC train: 0.987764	val: 0.965120	test: 0.686679

Epoch: 14
Loss: 0.22603810585117765
ROC train: 0.961051	val: 0.890221	test: 0.775000
PRC train: 0.989333	val: 0.967175	test: 0.665958

Epoch: 15
Loss: 0.21753128238328698
ROC train: 0.960505	val: 0.900211	test: 0.761828
PRC train: 0.988742	val: 0.976733	test: 0.642985

Epoch: 16
Loss: 0.22189695685581245
ROC train: 0.964719	val: 0.902320	test: 0.772536
PRC train: 0.989645	val: 0.978567	test: 0.657492

Epoch: 17
Loss: 0.21159750966285032
ROC train: 0.966663	val: 0.898324	test: 0.771864
PRC train: 0.990547	val: 0.968562	test: 0.675611

Epoch: 18
Loss: 0.21199667554356774
ROC train: 0.965914	val: 0.892885	test: 0.777733
PRC train: 0.990904	val: 0.964781	test: 0.709589

Epoch: 19
Loss: 0.21072877206707138
ROC train: 0.966517	val: 0.888889	test: 0.783647
PRC train: 0.991019	val: 0.968744	test: 0.725468

Epoch: 20
Loss: 0.20181108636522618
ROC train: 0.971897	val: 0.882673	test: 0.775403
PRC train: 0.992677	val: 0.964692	test: 0.697787

Epoch: 21
Loss: 0.19617445551724155
ROC train: 0.974796	val: 0.889111	test: 0.768324
PRC train: 0.993530	val: 0.970210	test: 0.684545

Epoch: 22
Loss: 0.19795627639443422
ROC train: 0.975437	val: 0.894439	test: 0.785349
PRC train: 0.993617	val: 0.971935	test: 0.707594

Epoch: 23
Loss: 0.1837374619154635
ROC train: 0.976043	val: 0.884005	test: 0.770968
PRC train: 0.993827	val: 0.968275	test: 0.685288

Epoch: 24
Loss: 0.19453502475190818
ROC train: 0.977625	val: 0.894106	test: 0.771998
PRC train: 0.994278	val: 0.975326	test: 0.651206

Epoch: 25
Loss: 0.18319102305613752
ROC train: 0.978968	val: 0.895549	test: 0.779973
PRC train: 0.994759	val: 0.976921	test: 0.680538

Epoch: 26
Loss: 0.1799073053902995
ROC train: 0.977714	val: 0.898546	test: 0.771326
PRC train: 0.994621	val: 0.978363	test: 0.676618

Epoch: 27
Loss: 0.17402830318078052
ROC train: 0.981712	val: 0.897547	test: 0.772177
PRC train: 0.995616	val: 0.979415	test: 0.671185

Epoch: 28
Loss: 0.17437172718245983
ROC train: 0.979199	val: 0.901654	test: 0.771192
PRC train: 0.994797	val: 0.983542	test: 0.668026

Epoch: 29
Loss: 0.18979050804184705
ROC train: 0.981043	val: 0.906649	test: 0.780376
PRC train: 0.995286	val: 0.985318	test: 0.678311

Epoch: 30
Loss: 0.18116484054436868
ROC train: 0.981799	val: 0.900655	test: 0.780108
PRC train: 0.995394	val: 0.983979	test: 0.686513

Epoch: 31
Loss: 0.16659319952302074
ROC train: 0.982200	val: 0.915751	test: 0.777285
PRC train: 0.995379	val: 0.986052	test: 0.693903

Epoch: 32
Loss: 0.1627815889261245
ROC train: 0.983516	val: 0.911977	test: 0.786514
PRC train: 0.995775	val: 0.985536	test: 0.706376

Epoch: 33
Loss: 0.1597451816545443Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/bbbp/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/bbbp/scaff/train_prop=0.7/bbbp_scaff_1_20-05_14-43-16  ]
[ Using Seed :  1  ]
[ Using device :  cuda:1  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6379382061600156
ROC train: 0.803078	val: 0.896326	test: 0.740591
PRC train: 0.937834	val: 0.985367	test: 0.643464

Epoch: 2
Loss: 0.5198762445767281
ROC train: 0.830423	val: 0.892663	test: 0.737455
PRC train: 0.948652	val: 0.984061	test: 0.638501

Epoch: 3
Loss: 0.43710563578724604
ROC train: 0.861021	val: 0.900766	test: 0.768369
PRC train: 0.958136	val: 0.985816	test: 0.685773

Epoch: 4
Loss: 0.39398319182562047
ROC train: 0.885738	val: 0.923521	test: 0.774597
PRC train: 0.965074	val: 0.989479	test: 0.687265

Epoch: 5
Loss: 0.3472560013960751
ROC train: 0.900227	val: 0.946276	test: 0.763844
PRC train: 0.969327	val: 0.993024	test: 0.665283

Epoch: 6
Loss: 0.3072153063148061
ROC train: 0.907150	val: 0.943279	test: 0.771461
PRC train: 0.971868	val: 0.992288	test: 0.668291

Epoch: 7
Loss: 0.3049783135867163
ROC train: 0.920938	val: 0.949939	test: 0.767473
PRC train: 0.976395	val: 0.993063	test: 0.662738

Epoch: 8
Loss: 0.2849851554994356
ROC train: 0.928355	val: 0.937618	test: 0.771685
PRC train: 0.979041	val: 0.990078	test: 0.675077

Epoch: 9
Loss: 0.26583130074064615
ROC train: 0.930514	val: 0.935287	test: 0.772446
PRC train: 0.979572	val: 0.989099	test: 0.674830

Epoch: 10
Loss: 0.2499794659427225
ROC train: 0.934744	val: 0.925075	test: 0.761514
PRC train: 0.980829	val: 0.986057	test: 0.656397

Epoch: 11
Loss: 0.2584525119107579
ROC train: 0.939582	val: 0.926962	test: 0.761604
PRC train: 0.982349	val: 0.987087	test: 0.660266

Epoch: 12
Loss: 0.24709354063031627
ROC train: 0.942125	val: 0.931846	test: 0.771819
PRC train: 0.983632	val: 0.989148	test: 0.683276

Epoch: 13
Loss: 0.24151206253205315
ROC train: 0.951285	val: 0.926407	test: 0.765547
PRC train: 0.986724	val: 0.986741	test: 0.674691

Epoch: 14
Loss: 0.23696751340878133
ROC train: 0.951462	val: 0.923965	test: 0.765502
PRC train: 0.986711	val: 0.983578	test: 0.660539

Epoch: 15
Loss: 0.22456037456449127
ROC train: 0.948286	val: 0.914974	test: 0.765950
PRC train: 0.985537	val: 0.977436	test: 0.656908

Epoch: 16
Loss: 0.21223126208699128
ROC train: 0.960039	val: 0.917638	test: 0.765771
PRC train: 0.989325	val: 0.980317	test: 0.672032

Epoch: 17
Loss: 0.21156898704924895
ROC train: 0.959269	val: 0.931291	test: 0.778226
PRC train: 0.989357	val: 0.987045	test: 0.695397

Epoch: 18
Loss: 0.21324949719865807
ROC train: 0.962178	val: 0.934510	test: 0.776434
PRC train: 0.989999	val: 0.988833	test: 0.683655

Epoch: 19
Loss: 0.20290812002829414
ROC train: 0.962183	val: 0.935953	test: 0.777330
PRC train: 0.989583	val: 0.990850	test: 0.688066

Epoch: 20
Loss: 0.19990120169857595
ROC train: 0.967271	val: 0.923854	test: 0.775806
PRC train: 0.991132	val: 0.987934	test: 0.679757

Epoch: 21
Loss: 0.2032914303459625
ROC train: 0.967639	val: 0.937174	test: 0.767518
PRC train: 0.991720	val: 0.990832	test: 0.644117

Epoch: 22
Loss: 0.18335746541006195
ROC train: 0.963485	val: 0.939949	test: 0.768996
PRC train: 0.990627	val: 0.990953	test: 0.621310

Epoch: 23
Loss: 0.19901137912640918
ROC train: 0.971093	val: 0.926629	test: 0.768145
PRC train: 0.992808	val: 0.988859	test: 0.668766

Epoch: 24
Loss: 0.2125803782989132
ROC train: 0.970722	val: 0.927628	test: 0.777733
PRC train: 0.992500	val: 0.988853	test: 0.664028

Epoch: 25
Loss: 0.18795100307589027
ROC train: 0.973552	val: 0.917527	test: 0.771864
PRC train: 0.993363	val: 0.986728	test: 0.639341

Epoch: 26
Loss: 0.19126408419673221
ROC train: 0.978426	val: 0.922522	test: 0.770923
PRC train: 0.994645	val: 0.988348	test: 0.653063

Epoch: 27
Loss: 0.18745456282094333
ROC train: 0.976893	val: 0.939172	test: 0.765815
PRC train: 0.994254	val: 0.991713	test: 0.655019

Epoch: 28
Loss: 0.19274921641829848
ROC train: 0.980021	val: 0.951160	test: 0.774328
PRC train: 0.995086	val: 0.993861	test: 0.676872

Epoch: 29
Loss: 0.17581824344207955
ROC train: 0.978227	val: 0.943612	test: 0.771505
PRC train: 0.994670	val: 0.992816	test: 0.672563

Epoch: 30
Loss: 0.17368944225698266
ROC train: 0.979467	val: 0.935953	test: 0.781048
PRC train: 0.995036	val: 0.991283	test: 0.670453

Epoch: 31
Loss: 0.16976101968112087
ROC train: 0.979947	val: 0.909424	test: 0.767428
PRC train: 0.995147	val: 0.985099	test: 0.639077

Epoch: 32
Loss: 0.16749063156136354
ROC train: 0.982071	val: 0.899989	test: 0.776837
PRC train: 0.995625	val: 0.983127	test: 0.645915

Epoch: 33
Loss: 0.1616465434920114Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/bbbp/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/bbbp/scaff/train_prop=0.7/bbbp_scaff_2_20-05_14-43-16  ]
[ Using Seed :  2  ]
[ Using device :  cuda:1  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6365243659674092
ROC train: 0.795200	val: 0.788323	test: 0.730556
PRC train: 0.934486	val: 0.938519	test: 0.628029

Epoch: 2
Loss: 0.5129792306597698
ROC train: 0.837008	val: 0.810079	test: 0.737186
PRC train: 0.952753	val: 0.958069	test: 0.654378

Epoch: 3
Loss: 0.44439765496412376
ROC train: 0.859996	val: 0.855145	test: 0.753315
PRC train: 0.958914	val: 0.969124	test: 0.664874

Epoch: 4
Loss: 0.4052953026383144
ROC train: 0.875735	val: 0.881452	test: 0.759633
PRC train: 0.963262	val: 0.976191	test: 0.660833

Epoch: 5
Loss: 0.34867055559459087
ROC train: 0.894038	val: 0.880453	test: 0.741532
PRC train: 0.969819	val: 0.979489	test: 0.627558

Epoch: 6
Loss: 0.32443338280342265
ROC train: 0.908149	val: 0.894217	test: 0.761828
PRC train: 0.973624	val: 0.978018	test: 0.652443

Epoch: 7
Loss: 0.30253227537305855
ROC train: 0.920506	val: 0.884449	test: 0.770744
PRC train: 0.977561	val: 0.967793	test: 0.672115

Epoch: 8
Loss: 0.2856933526462995
ROC train: 0.933309	val: 0.868576	test: 0.765726
PRC train: 0.981575	val: 0.959605	test: 0.673037

Epoch: 9
Loss: 0.27177088591558596
ROC train: 0.936570	val: 0.869908	test: 0.768593
PRC train: 0.982490	val: 0.964626	test: 0.675440

Epoch: 10
Loss: 0.26368167696131256
ROC train: 0.942893	val: 0.874459	test: 0.771147
PRC train: 0.984641	val: 0.962240	test: 0.665836

Epoch: 11
Loss: 0.25904413873075055
ROC train: 0.952737	val: 0.866023	test: 0.768638
PRC train: 0.987567	val: 0.954296	test: 0.670563

Epoch: 12
Loss: 0.24407006782297827
ROC train: 0.953288	val: 0.870130	test: 0.764740
PRC train: 0.987568	val: 0.962068	test: 0.649025

Epoch: 13
Loss: 0.24222831497358752
ROC train: 0.956099	val: 0.883561	test: 0.765771
PRC train: 0.988582	val: 0.975862	test: 0.625418

Epoch: 14
Loss: 0.23750484013961215
ROC train: 0.958394	val: 0.891220	test: 0.778226
PRC train: 0.988994	val: 0.981820	test: 0.689824

Epoch: 15
Loss: 0.22641759778251294
ROC train: 0.952441	val: 0.913975	test: 0.778584
PRC train: 0.987408	val: 0.987710	test: 0.679767

Epoch: 16
Loss: 0.2232837302321107
ROC train: 0.963272	val: 0.889777	test: 0.774910
PRC train: 0.990078	val: 0.978606	test: 0.672821

Epoch: 17
Loss: 0.20584672906456994
ROC train: 0.962989	val: 0.906094	test: 0.772715
PRC train: 0.990155	val: 0.982058	test: 0.667657

Epoch: 18
Loss: 0.21149373204490327
ROC train: 0.960065	val: 0.911422	test: 0.770430
PRC train: 0.989174	val: 0.985564	test: 0.660634

Epoch: 19
Loss: 0.1924349239235598
ROC train: 0.967843	val: 0.904429	test: 0.776747
PRC train: 0.991607	val: 0.980562	test: 0.659616

Epoch: 20
Loss: 0.20481891989810297
ROC train: 0.971530	val: 0.910201	test: 0.786425
PRC train: 0.992768	val: 0.983400	test: 0.685826

Epoch: 21
Loss: 0.20928201931206006
ROC train: 0.969448	val: 0.917083	test: 0.785708
PRC train: 0.991776	val: 0.986407	test: 0.704815

Epoch: 22
Loss: 0.19814095865003636
ROC train: 0.968944	val: 0.917083	test: 0.778943
PRC train: 0.991985	val: 0.985202	test: 0.691651

Epoch: 23
Loss: 0.1954298702030407
ROC train: 0.975994	val: 0.904651	test: 0.769713
PRC train: 0.993947	val: 0.983479	test: 0.689222

Epoch: 24
Loss: 0.18112249187403784
ROC train: 0.975232	val: 0.931513	test: 0.782841
PRC train: 0.993597	val: 0.990161	test: 0.713012

Epoch: 25
Loss: 0.17701402976941416
ROC train: 0.976149	val: 0.921634	test: 0.769892
PRC train: 0.994052	val: 0.987804	test: 0.687249

Epoch: 26
Loss: 0.16205869976443177
ROC train: 0.977501	val: 0.935065	test: 0.778763
PRC train: 0.994436	val: 0.989537	test: 0.685380

Epoch: 27
Loss: 0.17428990424892032
ROC train: 0.978732	val: 0.914308	test: 0.757348
PRC train: 0.994836	val: 0.986885	test: 0.622184

Epoch: 28
Loss: 0.18920391769811604
ROC train: 0.977579	val: 0.940837	test: 0.779570
PRC train: 0.994163	val: 0.992121	test: 0.662754

Epoch: 29
Loss: 0.17509516958949392
ROC train: 0.981868	val: 0.931624	test: 0.782706
PRC train: 0.995378	val: 0.990499	test: 0.712295

Epoch: 30
Loss: 0.17432345277388406
ROC train: 0.981242	val: 0.926296	test: 0.772894
PRC train: 0.995425	val: 0.989407	test: 0.691042

Epoch: 31
Loss: 0.1783442606358114
ROC train: 0.980112	val: 0.908092	test: 0.775000
PRC train: 0.995143	val: 0.984893	test: 0.695827

Epoch: 32
Loss: 0.16593834305430474
ROC train: 0.982186	val: 0.909313	test: 0.779525
PRC train: 0.995557	val: 0.985386	test: 0.706682

Epoch: 33
Loss: 0.15877367572610326Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/bbbp/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/bbbp/scaff/train_prop=0.8/bbbp_scaff_2_20-05_14-43-16  ]
[ Using Seed :  2  ]
[ Using device :  cuda:2  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6389954028657652
ROC train: 0.801271	val: 0.860484	test: 0.602141
PRC train: 0.947464	val: 0.745508	test: 0.648713

Epoch: 2
Loss: 0.4965948900932072
ROC train: 0.829023	val: 0.881461	test: 0.622975
PRC train: 0.956374	val: 0.778619	test: 0.672095

Epoch: 3
Loss: 0.4202898156873724
ROC train: 0.854609	val: 0.882867	test: 0.646701
PRC train: 0.963371	val: 0.732666	test: 0.687897

Epoch: 4
Loss: 0.36826459654338833
ROC train: 0.868954	val: 0.891699	test: 0.645448
PRC train: 0.967011	val: 0.757091	test: 0.674955

Epoch: 5
Loss: 0.33574738608386884
ROC train: 0.888570	val: 0.901736	test: 0.641300
PRC train: 0.971276	val: 0.777430	test: 0.666409

Epoch: 6
Loss: 0.3011946783090019
ROC train: 0.919215	val: 0.901235	test: 0.653067
PRC train: 0.980520	val: 0.768247	test: 0.682565

Epoch: 7
Loss: 0.2910132070027376
ROC train: 0.930235	val: 0.900833	test: 0.664159
PRC train: 0.983434	val: 0.765783	test: 0.703560

Epoch: 8
Loss: 0.2857507223398757
ROC train: 0.933897	val: 0.901235	test: 0.664352
PRC train: 0.984026	val: 0.778097	test: 0.704661

Epoch: 9
Loss: 0.25012268203842275
ROC train: 0.933298	val: 0.899428	test: 0.681809
PRC train: 0.983882	val: 0.775129	test: 0.728646

Epoch: 10
Loss: 0.24566399504107692
ROC train: 0.943093	val: 0.899829	test: 0.673708
PRC train: 0.985933	val: 0.773179	test: 0.729545

Epoch: 11
Loss: 0.2226017636572539
ROC train: 0.944259	val: 0.871926	test: 0.650270
PRC train: 0.986978	val: 0.748747	test: 0.700553

Epoch: 12
Loss: 0.23919009236671837
ROC train: 0.949164	val: 0.901937	test: 0.681327
PRC train: 0.987647	val: 0.808769	test: 0.725545

Epoch: 13
Loss: 0.2205653929426339
ROC train: 0.953306	val: 0.904246	test: 0.695216
PRC train: 0.988816	val: 0.801886	test: 0.739644

Epoch: 14
Loss: 0.2291391684767131
ROC train: 0.958688	val: 0.890896	test: 0.690394
PRC train: 0.990190	val: 0.790180	test: 0.721732

Epoch: 15
Loss: 0.2141567487767894
ROC train: 0.956263	val: 0.889090	test: 0.701292
PRC train: 0.989926	val: 0.777714	test: 0.698731

Epoch: 16
Loss: 0.20704112013109727
ROC train: 0.963758	val: 0.882666	test: 0.692612
PRC train: 0.991870	val: 0.800019	test: 0.703362

Epoch: 17
Loss: 0.2020756777403505
ROC train: 0.959279	val: 0.895112	test: 0.679495
PRC train: 0.989809	val: 0.802755	test: 0.665258

Epoch: 18
Loss: 0.20154385099328423
ROC train: 0.966330	val: 0.884774	test: 0.700135
PRC train: 0.992281	val: 0.783956	test: 0.712396

Epoch: 19
Loss: 0.2127519970963317
ROC train: 0.968866	val: 0.874435	test: 0.709877
PRC train: 0.992696	val: 0.755058	test: 0.714710

Epoch: 20
Loss: 0.19371991106689265
ROC train: 0.968114	val: 0.873030	test: 0.700714
PRC train: 0.992438	val: 0.772797	test: 0.697789

Epoch: 21
Loss: 0.19331239278593942
ROC train: 0.969629	val: 0.881963	test: 0.689333
PRC train: 0.992829	val: 0.792400	test: 0.694684

Epoch: 22
Loss: 0.1860517104101987
ROC train: 0.971011	val: 0.871123	test: 0.691551
PRC train: 0.992832	val: 0.779157	test: 0.694155

Epoch: 23
Loss: 0.1781490232289806
ROC train: 0.974279	val: 0.879655	test: 0.704379
PRC train: 0.993101	val: 0.800770	test: 0.730257

Epoch: 24
Loss: 0.18907971365441073
ROC train: 0.974297	val: 0.876744	test: 0.694734
PRC train: 0.993656	val: 0.805931	test: 0.717649

Epoch: 25
Loss: 0.18166695565650087
ROC train: 0.976842	val: 0.881763	test: 0.697724
PRC train: 0.994184	val: 0.829955	test: 0.748347

Epoch: 26
Loss: 0.1811055305353129
ROC train: 0.973921	val: 0.895313	test: 0.714313
PRC train: 0.992766	val: 0.847407	test: 0.768270

Epoch: 27
Loss: 0.16731107463674344
ROC train: 0.977076	val: 0.897521	test: 0.705922
PRC train: 0.993979	val: 0.854201	test: 0.755117

Epoch: 28
Loss: 0.18256665831728439
ROC train: 0.976051	val: 0.895915	test: 0.718654
PRC train: 0.993214	val: 0.839930	test: 0.772184

Epoch: 29
Loss: 0.16987983232016557
ROC train: 0.978623	val: 0.874134	test: 0.702160
PRC train: 0.994627	val: 0.785344	test: 0.716049

Epoch: 30
Loss: 0.16587307149309533
ROC train: 0.981154	val: 0.883368	test: 0.698592
PRC train: 0.995279	val: 0.803460	test: 0.726856

Epoch: 31
Loss: 0.15948681534717052
ROC train: 0.981419	val: 0.878551	test: 0.689525
PRC train: 0.995649	val: 0.790258	test: 0.688454

Epoch: 32
Loss: 0.15175145529935388
ROC train: 0.980973	val: 0.887684	test: 0.706887
PRC train: 0.995583	val: 0.804182	test: 0.721894

Epoch: 33
Loss: 0.16964783573552605Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/bbbp/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/bbbp/scaff/train_prop=0.8/bbbp_scaff_3_20-05_14-43-16  ]
[ Using Seed :  3  ]
[ Using device :  cuda:2  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6245275546352234
ROC train: 0.800048	val: 0.887283	test: 0.629051
PRC train: 0.949265	val: 0.810420	test: 0.688316

Epoch: 2
Loss: 0.5127823713019138
ROC train: 0.833814	val: 0.885777	test: 0.622203
PRC train: 0.955704	val: 0.816765	test: 0.661567

Epoch: 3
Loss: 0.41677285757749705
ROC train: 0.840704	val: 0.906354	test: 0.617477
PRC train: 0.957741	val: 0.829976	test: 0.657255

Epoch: 4
Loss: 0.3519801193278638
ROC train: 0.852268	val: 0.901736	test: 0.614005
PRC train: 0.963348	val: 0.807611	test: 0.670448

Epoch: 5
Loss: 0.3220425366135299
ROC train: 0.882159	val: 0.913279	test: 0.634838
PRC train: 0.970516	val: 0.836754	test: 0.686622

Epoch: 6
Loss: 0.30825602571987554
ROC train: 0.893201	val: 0.908461	test: 0.647184
PRC train: 0.973361	val: 0.831603	test: 0.705097

Epoch: 7
Loss: 0.31478461881803527
ROC train: 0.908252	val: 0.911272	test: 0.656154
PRC train: 0.976965	val: 0.830505	test: 0.709137

Epoch: 8
Loss: 0.2693858921595889
ROC train: 0.924771	val: 0.899227	test: 0.672164
PRC train: 0.981198	val: 0.779014	test: 0.729847

Epoch: 9
Loss: 0.27211972993334715
ROC train: 0.924921	val: 0.907558	test: 0.680459
PRC train: 0.981916	val: 0.819745	test: 0.731503

Epoch: 10
Loss: 0.2545155671363752
ROC train: 0.936132	val: 0.905049	test: 0.688947
PRC train: 0.984666	val: 0.817356	test: 0.753814

Epoch: 11
Loss: 0.25459872499674224
ROC train: 0.946411	val: 0.902740	test: 0.695216
PRC train: 0.987164	val: 0.831235	test: 0.764443

Epoch: 12
Loss: 0.23176889117425595
ROC train: 0.949989	val: 0.893205	test: 0.710455
PRC train: 0.987874	val: 0.794476	test: 0.770032

Epoch: 13
Loss: 0.22216777751695127
ROC train: 0.956709	val: 0.891900	test: 0.705536
PRC train: 0.989084	val: 0.808385	test: 0.762081

Epoch: 14
Loss: 0.2121145696901127
ROC train: 0.955678	val: 0.881361	test: 0.709008
PRC train: 0.989558	val: 0.801637	test: 0.776895

Epoch: 15
Loss: 0.2076087683230609
ROC train: 0.957374	val: 0.889090	test: 0.710359
PRC train: 0.989876	val: 0.822727	test: 0.783256

Epoch: 16
Loss: 0.21929098460865362
ROC train: 0.962773	val: 0.895614	test: 0.706308
PRC train: 0.990946	val: 0.840275	test: 0.772850

Epoch: 17
Loss: 0.20775673191248695
ROC train: 0.964128	val: 0.884573	test: 0.707755
PRC train: 0.991339	val: 0.812570	test: 0.766708

Epoch: 18
Loss: 0.19753027972632434
ROC train: 0.963759	val: 0.886078	test: 0.709394
PRC train: 0.991289	val: 0.815448	test: 0.774050

Epoch: 19
Loss: 0.18999860618961587
ROC train: 0.964356	val: 0.863194	test: 0.703414
PRC train: 0.991496	val: 0.801771	test: 0.770568

Epoch: 20
Loss: 0.1953459483863023
ROC train: 0.969001	val: 0.881160	test: 0.702353
PRC train: 0.992578	val: 0.820125	test: 0.755379

Epoch: 21
Loss: 0.19013988383650487
ROC train: 0.963150	val: 0.892502	test: 0.709105
PRC train: 0.991678	val: 0.834444	test: 0.753089

Epoch: 22
Loss: 0.1795120108491633
ROC train: 0.973515	val: 0.896417	test: 0.727238
PRC train: 0.994172	val: 0.849468	test: 0.795175

Epoch: 23
Loss: 0.19324553886817317
ROC train: 0.973327	val: 0.887484	test: 0.719425
PRC train: 0.994299	val: 0.827749	test: 0.784124

Epoch: 24
Loss: 0.18418615129252344
ROC train: 0.974596	val: 0.894108	test: 0.692901
PRC train: 0.994431	val: 0.834682	test: 0.731950

Epoch: 25
Loss: 0.18208972260247727
ROC train: 0.978333	val: 0.893506	test: 0.709298
PRC train: 0.995353	val: 0.851700	test: 0.764670

Epoch: 26
Loss: 0.17201752832848788
ROC train: 0.974655	val: 0.884071	test: 0.720390
PRC train: 0.994601	val: 0.835029	test: 0.780669

Epoch: 27
Loss: 0.17650432575026867
ROC train: 0.979441	val: 0.887082	test: 0.725598
PRC train: 0.995708	val: 0.806051	test: 0.779958

Epoch: 28
Loss: 0.1699221429262993
ROC train: 0.980381	val: 0.887283	test: 0.715953
PRC train: 0.995843	val: 0.815260	test: 0.751105

Epoch: 29
Loss: 0.16631797683373323
ROC train: 0.977474	val: 0.888688	test: 0.704379
PRC train: 0.995188	val: 0.828229	test: 0.732023

Epoch: 30
Loss: 0.15172296380619246
ROC train: 0.980383	val: 0.888287	test: 0.714506
PRC train: 0.995829	val: 0.828698	test: 0.741450

Epoch: 31
Loss: 0.14152520264887944
ROC train: 0.982586	val: 0.898725	test: 0.720486
PRC train: 0.996333	val: 0.828915	test: 0.742638

Epoch: 32
Loss: 0.1451990418627451
ROC train: 0.985010	val: 0.890495	test: 0.724055
PRC train: 0.996887	val: 0.811727	test: 0.749524

Epoch: 33
Loss: 0.1528963391575311Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/bbbp/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/bbbp/scaff/train_prop=0.8/bbbp_scaff_1_20-05_14-43-16  ]
[ Using Seed :  1  ]
[ Using device :  cuda:2  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6228693989748939
ROC train: 0.794621	val: 0.880157	test: 0.631752
PRC train: 0.940329	val: 0.809528	test: 0.668984

Epoch: 2
Loss: 0.503621732229231
ROC train: 0.832124	val: 0.907558	test: 0.632137
PRC train: 0.954710	val: 0.868285	test: 0.686122

Epoch: 3
Loss: 0.41107217866036055
ROC train: 0.867845	val: 0.917595	test: 0.670525
PRC train: 0.965717	val: 0.859226	test: 0.727507

Epoch: 4
Loss: 0.36273751199611687
ROC train: 0.884712	val: 0.918298	test: 0.683931
PRC train: 0.969415	val: 0.814799	test: 0.742999

Epoch: 5
Loss: 0.32990503872633864
ROC train: 0.906980	val: 0.915086	test: 0.673900
PRC train: 0.975728	val: 0.822487	test: 0.721667

Epoch: 6
Loss: 0.30023084600666367
ROC train: 0.916773	val: 0.913279	test: 0.658275
PRC train: 0.978100	val: 0.861984	test: 0.692508

Epoch: 7
Loss: 0.27430484357578117
ROC train: 0.909110	val: 0.919101	test: 0.660301
PRC train: 0.976120	val: 0.853440	test: 0.682818

Epoch: 8
Loss: 0.25673119659054694
ROC train: 0.932656	val: 0.918298	test: 0.671296
PRC train: 0.983278	val: 0.839714	test: 0.703115

Epoch: 9
Loss: 0.25836700568764115
ROC train: 0.936192	val: 0.915889	test: 0.679109
PRC train: 0.984015	val: 0.844201	test: 0.718777

Epoch: 10
Loss: 0.22615554742881194
ROC train: 0.935791	val: 0.914684	test: 0.688850
PRC train: 0.983943	val: 0.857847	test: 0.727383

Epoch: 11
Loss: 0.22726028242351917
ROC train: 0.947000	val: 0.911874	test: 0.678241
PRC train: 0.986637	val: 0.860552	test: 0.719364

Epoch: 12
Loss: 0.2546110902685298
ROC train: 0.947890	val: 0.910168	test: 0.692419
PRC train: 0.987131	val: 0.840344	test: 0.730905

Epoch: 13
Loss: 0.21904862170291947
ROC train: 0.954329	val: 0.902439	test: 0.706019
PRC train: 0.988996	val: 0.823606	test: 0.753521

Epoch: 14
Loss: 0.21325563130978095
ROC train: 0.956212	val: 0.909465	test: 0.697724
PRC train: 0.989584	val: 0.847756	test: 0.732731

Epoch: 15
Loss: 0.2065571535642098
ROC train: 0.953260	val: 0.902740	test: 0.693673
PRC train: 0.989099	val: 0.829034	test: 0.724465

Epoch: 16
Loss: 0.19993776538586358
ROC train: 0.962171	val: 0.902841	test: 0.700810
PRC train: 0.990742	val: 0.832535	test: 0.745878

Epoch: 17
Loss: 0.19767665942650342
ROC train: 0.964585	val: 0.912275	test: 0.690201
PRC train: 0.991036	val: 0.858832	test: 0.731059

Epoch: 18
Loss: 0.18916803611024788
ROC train: 0.965336	val: 0.904547	test: 0.710166
PRC train: 0.991491	val: 0.836552	test: 0.754774

Epoch: 19
Loss: 0.1846149820049165
ROC train: 0.967574	val: 0.892201	test: 0.715856
PRC train: 0.992274	val: 0.813054	test: 0.752536

Epoch: 20
Loss: 0.2083911003596719
ROC train: 0.969621	val: 0.904346	test: 0.712384
PRC train: 0.992939	val: 0.806598	test: 0.731096

Epoch: 21
Loss: 0.19005369114070542
ROC train: 0.973496	val: 0.899328	test: 0.709298
PRC train: 0.994089	val: 0.794868	test: 0.724147

Epoch: 22
Loss: 0.17797712790738304
ROC train: 0.970831	val: 0.884272	test: 0.709587
PRC train: 0.993286	val: 0.774669	test: 0.740794

Epoch: 23
Loss: 0.17211993920732147
ROC train: 0.973912	val: 0.900130	test: 0.704090
PRC train: 0.994075	val: 0.806132	test: 0.717066

Epoch: 24
Loss: 0.16346442498484226
ROC train: 0.976225	val: 0.902138	test: 0.715085
PRC train: 0.994531	val: 0.837831	test: 0.768786

Epoch: 25
Loss: 0.16485261032359985
ROC train: 0.975634	val: 0.892803	test: 0.704765
PRC train: 0.994355	val: 0.805283	test: 0.728439

Epoch: 26
Loss: 0.1620568546074914
ROC train: 0.978155	val: 0.901134	test: 0.683546
PRC train: 0.995060	val: 0.827002	test: 0.693909

Epoch: 27
Loss: 0.15319935974548574
ROC train: 0.979668	val: 0.904647	test: 0.704282
PRC train: 0.995319	val: 0.832600	test: 0.724314

Epoch: 28
Loss: 0.165969077406084
ROC train: 0.980103	val: 0.900130	test: 0.691551
PRC train: 0.995477	val: 0.836392	test: 0.706722

Epoch: 29
Loss: 0.16364423702452482
ROC train: 0.974464	val: 0.897320	test: 0.681327
PRC train: 0.994400	val: 0.835000	test: 0.701845

Epoch: 30
Loss: 0.1645440244675787
ROC train: 0.981790	val: 0.911372	test: 0.696277
PRC train: 0.995900	val: 0.853538	test: 0.721465

Epoch: 31
Loss: 0.15196360439567644
ROC train: 0.982692	val: 0.904848	test: 0.696759
PRC train: 0.996130	val: 0.857995	test: 0.723361

Epoch: 32
Loss: 0.1500777218409822
ROC train: 0.984283	val: 0.902841	test: 0.687886
PRC train: 0.996628	val: 0.834224	test: 0.707840

Epoch: 33
Loss: 0.15360193024756483
ROC train: 0.985790	val: 0.913642	test: 0.785305
PRC train: 0.996527	val: 0.987010	test: 0.687391

Epoch: 34
Loss: 0.15862628620432687
ROC train: 0.986586	val: 0.906538	test: 0.780287
PRC train: 0.996737	val: 0.982212	test: 0.660592

Epoch: 35
Loss: 0.15461390570800385
ROC train: 0.983922	val: 0.891886	test: 0.765995
PRC train: 0.996069	val: 0.978927	test: 0.639021

Epoch: 36
Loss: 0.15568228743846863
ROC train: 0.986857	val: 0.898768	test: 0.783020
PRC train: 0.996736	val: 0.981136	test: 0.701155

Epoch: 37
Loss: 0.16457018382733798
ROC train: 0.987778	val: 0.880453	test: 0.781944
PRC train: 0.997052	val: 0.978707	test: 0.685790

Epoch: 38
Loss: 0.158194472389102
ROC train: 0.984169	val: 0.877012	test: 0.776658
PRC train: 0.996258	val: 0.976243	test: 0.669371

Epoch: 39
Loss: 0.15994552598011444
ROC train: 0.987453	val: 0.866578	test: 0.755242
PRC train: 0.997087	val: 0.972659	test: 0.637218

Epoch: 40
Loss: 0.1474274561663426
ROC train: 0.986941	val: 0.887446	test: 0.775000
PRC train: 0.997000	val: 0.976282	test: 0.687229

Epoch: 41
Loss: 0.1537617293695183
ROC train: 0.991009	val: 0.872683	test: 0.778405
PRC train: 0.997946	val: 0.975617	test: 0.690480

Epoch: 42
Loss: 0.14876130522437825
ROC train: 0.988651	val: 0.844711	test: 0.766756
PRC train: 0.997386	val: 0.973411	test: 0.689399

Epoch: 43
Loss: 0.1444958383401776
ROC train: 0.986626	val: 0.877345	test: 0.777509
PRC train: 0.996894	val: 0.976442	test: 0.714713

Epoch: 44
Loss: 0.1505399968268845
ROC train: 0.990114	val: 0.863803	test: 0.778808
PRC train: 0.997723	val: 0.971800	test: 0.732762

Epoch: 45
Loss: 0.14704993061161323
ROC train: 0.990810	val: 0.823399	test: 0.767608
PRC train: 0.997898	val: 0.961083	test: 0.699487

Epoch: 46
Loss: 0.135384521296498
ROC train: 0.991121	val: 0.850150	test: 0.768862
PRC train: 0.997954	val: 0.968585	test: 0.668495

Epoch: 47
Loss: 0.14225250085220031
ROC train: 0.991406	val: 0.899656	test: 0.790323
PRC train: 0.998032	val: 0.980409	test: 0.721848

Epoch: 48
Loss: 0.12897583684401345
ROC train: 0.992349	val: 0.900877	test: 0.791398
PRC train: 0.998241	val: 0.981385	test: 0.732038

Epoch: 49
Loss: 0.1192103699219549
ROC train: 0.993298	val: 0.887224	test: 0.774373
PRC train: 0.998486	val: 0.977912	test: 0.695723

Epoch: 50
Loss: 0.13230307666737412
ROC train: 0.992578	val: 0.897991	test: 0.772849
PRC train: 0.998332	val: 0.979572	test: 0.687787

Epoch: 51
Loss: 0.12311996459549983
ROC train: 0.994300	val: 0.908647	test: 0.782303
PRC train: 0.998712	val: 0.980767	test: 0.712574

Epoch: 52
Loss: 0.12968393676495535
ROC train: 0.994190	val: 0.910312	test: 0.775269
PRC train: 0.998696	val: 0.982954	test: 0.698370

Epoch: 53
Loss: 0.13366280323924476
ROC train: 0.994371	val: 0.896770	test: 0.776523
PRC train: 0.998729	val: 0.979962	test: 0.701622

Epoch: 54
Loss: 0.14060999121597442
ROC train: 0.994349	val: 0.885004	test: 0.759274
PRC train: 0.998726	val: 0.975480	test: 0.667127

Epoch: 55
Loss: 0.12006665895548535
ROC train: 0.995252	val: 0.874903	test: 0.762814
PRC train: 0.998916	val: 0.975347	test: 0.657375

Epoch: 56
Loss: 0.12704076753409818
ROC train: 0.995534	val: 0.888889	test: 0.773253
PRC train: 0.998988	val: 0.976446	test: 0.673873

Epoch: 57
Loss: 0.11149126225129553
ROC train: 0.994920	val: 0.858697	test: 0.777016
PRC train: 0.998856	val: 0.969561	test: 0.686894

Epoch: 58
Loss: 0.11779028518432448
ROC train: 0.995218	val: 0.887335	test: 0.780690
PRC train: 0.998920	val: 0.974576	test: 0.699349

Epoch: 59
Loss: 0.11670566626259204
ROC train: 0.995607	val: 0.897658	test: 0.764023
PRC train: 0.999011	val: 0.977752	test: 0.674360

Epoch: 60
Loss: 0.12097235083196645
ROC train: 0.996055	val: 0.902764	test: 0.764023
PRC train: 0.999127	val: 0.980632	test: 0.653997

Epoch: 61
Loss: 0.12763930986583014
ROC train: 0.994158	val: 0.905317	test: 0.765905
PRC train: 0.998706	val: 0.983793	test: 0.666691

Epoch: 62
Loss: 0.11909349562230491
ROC train: 0.995599	val: 0.918859	test: 0.780735
PRC train: 0.999011	val: 0.986073	test: 0.697709

Epoch: 63
Loss: 0.10223580928805198
ROC train: 0.995409	val: 0.901432	test: 0.776165
PRC train: 0.998969	val: 0.981727	test: 0.684833

Epoch: 64
Loss: 0.11644314962352005
ROC train: 0.995042	val: 0.893329	test: 0.770520
PRC train: 0.998886	val: 0.979395	test: 0.675401

Epoch: 65
Loss: 0.11196570248536612
ROC train: 0.995346	val: 0.896770	test: 0.787903
PRC train: 0.998969	val: 0.979159	test: 0.714389

Epoch: 66
Loss: 0.10983592333224583
ROC train: 0.995490	val: 0.883894	test: 0.781407
PRC train: 0.998994	val: 0.973737	test: 0.706558

Epoch: 67
Loss: 0.11603786543903634
ROC train: 0.994753	val: 0.886780	test: 0.770161
PRC train: 0.998832	val: 0.972374	test: 0.672437

Epoch: 68
Loss: 0.11129269190596747
ROC train: 0.995867	val: 0.886780	test: 0.754749
PRC train: 0.999077	val: 0.974208	test: 0.646990

Epoch: 69
Loss: 0.10421316514170058
ROC train: 0.996361	val: 0.896104	test: 0.760036
PRC train: 0.999190	val: 0.980272	test: 0.664238

Epoch: 70
Loss: 0.10666505573027008
ROC train: 0.997171	val: 0.898657	test: 0.780690
PRC train: 0.999364	val: 0.981731	test: 0.707494

Epoch: 71
Loss: 0.10317855817449954
ROC train: 0.997283	val: 0.902431	test: 0.778674
PRC train: 0.999392	val: 0.983991	test: 0.691083

Epoch: 72
Loss: 0.10210306702576084
ROC train: 0.996846	val: 0.896992	test: 0.768772
PRC train: 0.999300	val: 0.981317	test: 0.646613

Epoch: 73
Loss: 0.09950520510833853
ROC train: 0.996434	val: 0.894439	test: 0.775179
PRC train: 0.999205	val: 0.978966	test: 0.670859

Epoch: 74
Loss: 0.09834846246817892
ROC train: 0.997180	val: 0.900100	test: 0.760797
PRC train: 0.999368	val: 0.979857	test: 0.651387

Epoch: 75
Loss: 0.10165325038621109
ROC train: 0.995725	val: 0.895216	test: 0.766487
PRC train: 0.999013	val: 0.979561	test: 0.636706

Epoch: 76
Loss: 0.08761982355712587
ROC train: 0.996848	val: 0.881008	test: 0.764427
PRC train: 0.999289	val: 0.977807	test: 0.646533

Epoch: 77
Loss: 0.09627914372291951
ROC train: 0.997072	val: 0.855811	test: 0.753898
PRC train: 0.999339	val: 0.972053	test: 0.634128

Epoch: 78
Loss: 0.09776166465334009
ROC train: 0.997304	val: 0.886891	test: 0.766353
PRC train: 0.999393	val: 0.976798	test: 0.636916

Epoch: 79
Loss: 0.09452052311417387
ROC train: 0.997596	val: 0.893218	test: 0.774104
PRC train: 0.999460	val: 0.979326	test: 0.654987

Epoch: 80
Loss: 0.10568899808780896
ROC train: 0.998046	val: 0.894883	test: 0.772357
PRC train: 0.999563	val: 0.981999	test: 0.660770

Epoch: 81
Loss: 0.09763410046610876
ROC train: 0.997536	val: 0.892996	test: 0.773701
PRC train: 0.999447	val: 0.979625	test: 0.679267

Epoch: 82
Loss: 0.09270787594133188
ROC train: 0.996386	val: 0.884116	test: 0.754301
PRC train: 0.999193	val: 0.980401	test: 0.630833

Epoch: 83
Loss: 0.10529780494922904
ROC train: 0.997627	val: 0.902320	test: 0.759991
PRC train: 0.999474	val: 0.983647	test: 0.646476

Epoch: 84
Loss: 0.09371911268378748
ROC train: 0.998125	val: 0.902764	test: 0.780287
PRC train: 0.999583	val: 0.983510	test: 0.672188

Epoch: 85
Loss: 0.09611035720694261
ROC train: 0.997897	val: 0.903319	test: 0.779435
PRC train: 0.999529	val: 0.984979	test: 0.672521

Epoch: 86
Loss: 0.09783705886177142
ROC train: 0.997891	val: 0.900766	test: 0.764516
PRC train: 0.999528	val: 0.984081	test: 0.658400

Epoch: 87
Loss: 0.09366626844100838
ROC train: 0.998319	val: 0.896437	test: 0.753136
PRC train: 0.999630	val: 0.979303	test: 0.622951

Epoch: 88
Loss: 0.09511460603961537
ROC train: 0.998184	val: 0.893551	test: 0.760618
PRC train: 0.999599	val: 0.976043	test: 0.646004

Epoch: 89
Loss: 0.08644085387416568
ROC train: 0.998126	val: 0.890110	test: 0.765278
PRC train: 0.999584	val: 0.975720	test: 0.655468

Epoch: 90
Loss: 0.08313401027884855
ROC train: 0.998393	val: 0.877789	test: 0.763710
PRC train: 0.999642	val: 0.974645	test: 0.656436

Epoch: 91
Loss: 0.08951226189951607
ROC train: 0.998599	val: 0.886891	test: 0.755914
PRC train: 0.999690	val: 0.977523	test: 0.644522

Epoch: 92
Loss: 0.08548946848980048
ROC train: 0.998418	val: 0.892330	test: 0.758602
PRC train: 0.999647	val: 0.977546	test: 0.648519

Epoch: 93
Loss: 0.0832998217269541
ROC train: 0.997227	val: 0.896548	test: 0.755556
PRC train: 0.999375	val: 0.979590	test: 0.651821

Epoch: 94
Loss: 0.08836595722661567
ROC train: 0.982664	val: 0.914197	test: 0.790233
PRC train: 0.995723	val: 0.986033	test: 0.675073

Epoch: 34
Loss: 0.1598970727474185
ROC train: 0.983094	val: 0.916528	test: 0.783692
PRC train: 0.995856	val: 0.987450	test: 0.687171

Epoch: 35
Loss: 0.1623255413129309
ROC train: 0.982610	val: 0.922855	test: 0.781004
PRC train: 0.995865	val: 0.988427	test: 0.661159

Epoch: 36
Loss: 0.172617673965751
ROC train: 0.984844	val: 0.936175	test: 0.789695
PRC train: 0.996347	val: 0.990774	test: 0.677231

Epoch: 37
Loss: 0.1510396673241349
ROC train: 0.986365	val: 0.925075	test: 0.786962
PRC train: 0.996680	val: 0.988943	test: 0.697853

Epoch: 38
Loss: 0.14933862110857535
ROC train: 0.987488	val: 0.905761	test: 0.783871
PRC train: 0.996979	val: 0.984343	test: 0.665721

Epoch: 39
Loss: 0.15788231196647243
ROC train: 0.987453	val: 0.891220	test: 0.777151
PRC train: 0.996962	val: 0.980147	test: 0.651089

Epoch: 40
Loss: 0.13697718202102785
ROC train: 0.987781	val: 0.899989	test: 0.765726
PRC train: 0.996967	val: 0.980315	test: 0.635862

Epoch: 41
Loss: 0.15289800596743167
ROC train: 0.987605	val: 0.906205	test: 0.764875
PRC train: 0.996914	val: 0.982723	test: 0.627134

Epoch: 42
Loss: 0.14422987366406007
ROC train: 0.988735	val: 0.895660	test: 0.764247
PRC train: 0.997318	val: 0.983719	test: 0.639105

Epoch: 43
Loss: 0.1523252093296397
ROC train: 0.990780	val: 0.900988	test: 0.779211
PRC train: 0.997843	val: 0.983920	test: 0.666981

Epoch: 44
Loss: 0.14236667868986622
ROC train: 0.991301	val: 0.913975	test: 0.789427
PRC train: 0.997978	val: 0.985595	test: 0.693359

Epoch: 45
Loss: 0.13617534729915187
ROC train: 0.990019	val: 0.922855	test: 0.782079
PRC train: 0.997682	val: 0.988737	test: 0.661942

Epoch: 46
Loss: 0.13026284022766405
ROC train: 0.991177	val: 0.908203	test: 0.776613
PRC train: 0.997960	val: 0.986440	test: 0.644403

Epoch: 47
Loss: 0.1329993035304863
ROC train: 0.991382	val: 0.892219	test: 0.776210
PRC train: 0.997997	val: 0.982159	test: 0.646511

Epoch: 48
Loss: 0.14683597607271906
ROC train: 0.992609	val: 0.886336	test: 0.760484
PRC train: 0.998308	val: 0.979456	test: 0.630416

Epoch: 49
Loss: 0.13732601457500856
ROC train: 0.993382	val: 0.881008	test: 0.771371
PRC train: 0.998485	val: 0.972534	test: 0.646311

Epoch: 50
Loss: 0.12975450899164484
ROC train: 0.991836	val: 0.889666	test: 0.777778
PRC train: 0.998126	val: 0.976594	test: 0.655810

Epoch: 51
Loss: 0.135689218206487
ROC train: 0.992657	val: 0.889222	test: 0.765636
PRC train: 0.998308	val: 0.979029	test: 0.654426

Epoch: 52
Loss: 0.1337042792038813
ROC train: 0.994143	val: 0.880564	test: 0.760215
PRC train: 0.998657	val: 0.976169	test: 0.634683

Epoch: 53
Loss: 0.12800692047560525
ROC train: 0.992662	val: 0.851482	test: 0.747939
PRC train: 0.998333	val: 0.969652	test: 0.602427

Epoch: 54
Loss: 0.12626527296489318
ROC train: 0.991113	val: 0.867910	test: 0.761604
PRC train: 0.997910	val: 0.970477	test: 0.634762

Epoch: 55
Loss: 0.13199551227915848
ROC train: 0.992573	val: 0.867799	test: 0.760618
PRC train: 0.998253	val: 0.969243	test: 0.631595

Epoch: 56
Loss: 0.1309865134793394
ROC train: 0.993125	val: 0.877345	test: 0.770296
PRC train: 0.998348	val: 0.970095	test: 0.663330

Epoch: 57
Loss: 0.141971805973139
ROC train: 0.992683	val: 0.873127	test: 0.764471
PRC train: 0.998284	val: 0.971167	test: 0.638950

Epoch: 58
Loss: 0.11764296030778944
ROC train: 0.994699	val: 0.878122	test: 0.765323
PRC train: 0.998757	val: 0.970843	test: 0.648537

Epoch: 59
Loss: 0.12375830655770326
ROC train: 0.994949	val: 0.855811	test: 0.749373
PRC train: 0.998845	val: 0.961109	test: 0.622196

Epoch: 60
Loss: 0.11957359287353157
ROC train: 0.993087	val: 0.873127	test: 0.758961
PRC train: 0.998447	val: 0.958913	test: 0.623729

Epoch: 61
Loss: 0.1385243552492227
ROC train: 0.994847	val: 0.872239	test: 0.754884
PRC train: 0.998825	val: 0.957631	test: 0.636373

Epoch: 62
Loss: 0.11382816112611378
ROC train: 0.995643	val: 0.887779	test: 0.759946
PRC train: 0.999006	val: 0.970039	test: 0.637156

Epoch: 63
Loss: 0.11951095673528378
ROC train: 0.995913	val: 0.892330	test: 0.759140
PRC train: 0.999061	val: 0.975520	test: 0.629924

Epoch: 64
Loss: 0.11716258328416186
ROC train: 0.996179	val: 0.880120	test: 0.755421
PRC train: 0.999131	val: 0.972420	test: 0.616851

Epoch: 65
Loss: 0.11145083722896315
ROC train: 0.996376	val: 0.871684	test: 0.751389
PRC train: 0.999184	val: 0.972658	test: 0.614597

Epoch: 66
Loss: 0.12115831760510715
ROC train: 0.996012	val: 0.895882	test: 0.769086
PRC train: 0.999093	val: 0.974647	test: 0.653899

Epoch: 67
Loss: 0.11129053202420314
ROC train: 0.995147	val: 0.889888	test: 0.774149
PRC train: 0.998902	val: 0.977647	test: 0.659546

Epoch: 68
Loss: 0.11550980397334043
ROC train: 0.996221	val: 0.877900	test: 0.772760
PRC train: 0.999154	val: 0.977262	test: 0.665875

Epoch: 69
Loss: 0.10459356280218875
ROC train: 0.996244	val: 0.876679	test: 0.763978
PRC train: 0.999157	val: 0.972029	test: 0.627315

Epoch: 70
Loss: 0.1112844560228019
ROC train: 0.996286	val: 0.860251	test: 0.751568
PRC train: 0.999167	val: 0.961109	test: 0.617805

Epoch: 71
Loss: 0.12207525849659244
ROC train: 0.995282	val: 0.849706	test: 0.739695
PRC train: 0.998929	val: 0.953732	test: 0.605125

Epoch: 72
Loss: 0.10936953127776344
ROC train: 0.994600	val: 0.820957	test: 0.743369
PRC train: 0.998781	val: 0.952858	test: 0.594080

Epoch: 73
Loss: 0.10509327886526426
ROC train: 0.996220	val: 0.861583	test: 0.757572
PRC train: 0.999155	val: 0.961836	test: 0.614871

Epoch: 74
Loss: 0.10554946684806453
ROC train: 0.997296	val: 0.880231	test: 0.754480
PRC train: 0.999394	val: 0.963204	test: 0.627665

Epoch: 75
Loss: 0.10169674789155346
ROC train: 0.997714	val: 0.879121	test: 0.756586
PRC train: 0.999486	val: 0.962565	test: 0.627026

Epoch: 76
Loss: 0.10553688202549429
ROC train: 0.996660	val: 0.854590	test: 0.755063
PRC train: 0.999253	val: 0.958357	test: 0.626841

Epoch: 77
Loss: 0.09898407390297541
ROC train: 0.996457	val: 0.859030	test: 0.755914
PRC train: 0.999211	val: 0.959411	test: 0.635830

Epoch: 78
Loss: 0.09932674777896995
ROC train: 0.997317	val: 0.862582	test: 0.746371
PRC train: 0.999403	val: 0.961982	test: 0.626952

Epoch: 79
Loss: 0.09848302344130494
ROC train: 0.996782	val: 0.856477	test: 0.740188
PRC train: 0.999286	val: 0.963271	test: 0.603464

Epoch: 80
Loss: 0.10028322612607837
ROC train: 0.997818	val: 0.874792	test: 0.752240
PRC train: 0.999508	val: 0.964369	test: 0.612477

Epoch: 81
Loss: 0.08728902303295615
ROC train: 0.997372	val: 0.863692	test: 0.740860
PRC train: 0.999410	val: 0.961175	test: 0.599974

Epoch: 82
Loss: 0.09741705217687273
ROC train: 0.997446	val: 0.873571	test: 0.751792
PRC train: 0.999420	val: 0.964243	test: 0.616425

Epoch: 83
Loss: 0.10448145316009043
ROC train: 0.997538	val: 0.872905	test: 0.759319
PRC train: 0.999444	val: 0.966792	test: 0.627165

Epoch: 84
Loss: 0.08807737973478515
ROC train: 0.997213	val: 0.843046	test: 0.747581
PRC train: 0.999363	val: 0.962291	test: 0.618942

Epoch: 85
Loss: 0.10298428212184031
ROC train: 0.998304	val: 0.842380	test: 0.749014
PRC train: 0.999622	val: 0.957960	test: 0.633489

Epoch: 86
Loss: 0.08369768984140373
ROC train: 0.998327	val: 0.842380	test: 0.742966
PRC train: 0.999626	val: 0.953513	test: 0.617339

Epoch: 87
Loss: 0.08786552328900943
ROC train: 0.998291	val: 0.858697	test: 0.748387
PRC train: 0.999620	val: 0.957879	test: 0.623206

Epoch: 88
Loss: 0.0883138300034424
ROC train: 0.998561	val: 0.870685	test: 0.758781
PRC train: 0.999678	val: 0.960188	test: 0.633895

Epoch: 89
Loss: 0.08350620147782416
ROC train: 0.998013	val: 0.858253	test: 0.745609
PRC train: 0.999557	val: 0.959067	test: 0.600690

Epoch: 90
Loss: 0.09200377316269769
ROC train: 0.998477	val: 0.866023	test: 0.742025
PRC train: 0.999660	val: 0.963610	test: 0.607524

Epoch: 91
Loss: 0.08532644229388109
ROC train: 0.998838	val: 0.872461	test: 0.741756
PRC train: 0.999740	val: 0.968238	test: 0.604631

Epoch: 92
Loss: 0.08964747016974445
ROC train: 0.998372	val: 0.856699	test: 0.740547
PRC train: 0.999637	val: 0.965687	test: 0.602102

Epoch: 93
Loss: 0.07877060888182268
ROC train: 0.998739	val: 0.842491	test: 0.737366
PRC train: 0.999719	val: 0.964853	test: 0.595943

Epoch: 94
Loss: 0.08924444652205865
ROC train: 0.977207	val: 0.899101	test: 0.757527
PRC train: 0.994427	val: 0.985617	test: 0.686606

Epoch: 34
Loss: 0.16983504565169563
ROC train: 0.981784	val: 0.919525	test: 0.775538
PRC train: 0.995629	val: 0.988147	test: 0.705043

Epoch: 35
Loss: 0.1568673744480061
ROC train: 0.986885	val: 0.929293	test: 0.765591
PRC train: 0.996939	val: 0.989716	test: 0.674388

Epoch: 36
Loss: 0.17059382501876028
ROC train: 0.983404	val: 0.913309	test: 0.767294
PRC train: 0.996038	val: 0.985886	test: 0.689656

Epoch: 37
Loss: 0.15341854273470032
ROC train: 0.983399	val: 0.913420	test: 0.772357
PRC train: 0.995956	val: 0.984841	test: 0.650510

Epoch: 38
Loss: 0.146435540333775
ROC train: 0.987521	val: 0.923188	test: 0.776927
PRC train: 0.997010	val: 0.986801	test: 0.674570

Epoch: 39
Loss: 0.15207447596429116
ROC train: 0.985284	val: 0.935620	test: 0.777240
PRC train: 0.996382	val: 0.989954	test: 0.691551

Epoch: 40
Loss: 0.1627396995552786
ROC train: 0.988038	val: 0.927184	test: 0.778495
PRC train: 0.997131	val: 0.986524	test: 0.692806

Epoch: 41
Loss: 0.1676216449501524
ROC train: 0.986901	val: 0.916528	test: 0.760842
PRC train: 0.996870	val: 0.986577	test: 0.644816

Epoch: 42
Loss: 0.1500787590347418
ROC train: 0.986878	val: 0.904651	test: 0.772312
PRC train: 0.996833	val: 0.984477	test: 0.672560

Epoch: 43
Loss: 0.14539415168845055
ROC train: 0.989211	val: 0.887668	test: 0.771685
PRC train: 0.997465	val: 0.979012	test: 0.682847

Epoch: 44
Loss: 0.14740510557137584
ROC train: 0.989935	val: 0.889111	test: 0.766711
PRC train: 0.997684	val: 0.979436	test: 0.664700

Epoch: 45
Loss: 0.1466717094960536
ROC train: 0.990230	val: 0.895660	test: 0.768952
PRC train: 0.997734	val: 0.979916	test: 0.667410

Epoch: 46
Loss: 0.1304330680328226
ROC train: 0.991329	val: 0.909313	test: 0.766577
PRC train: 0.997977	val: 0.984690	test: 0.662887

Epoch: 47
Loss: 0.13672301529086897
ROC train: 0.991326	val: 0.918970	test: 0.760663
PRC train: 0.998003	val: 0.988577	test: 0.645422

Epoch: 48
Loss: 0.13835603640299127
ROC train: 0.991237	val: 0.921856	test: 0.768817
PRC train: 0.997972	val: 0.988367	test: 0.653448

Epoch: 49
Loss: 0.14405713930066752
ROC train: 0.991451	val: 0.906427	test: 0.757482
PRC train: 0.998035	val: 0.985286	test: 0.630323

Epoch: 50
Loss: 0.1312515580462089
ROC train: 0.992115	val: 0.900655	test: 0.748073
PRC train: 0.998207	val: 0.983329	test: 0.630468

Epoch: 51
Loss: 0.13922635948267534
ROC train: 0.990531	val: 0.903985	test: 0.767876
PRC train: 0.997831	val: 0.983736	test: 0.657642

Epoch: 52
Loss: 0.13668553407177422
ROC train: 0.992107	val: 0.893440	test: 0.767518
PRC train: 0.998202	val: 0.978932	test: 0.642183

Epoch: 53
Loss: 0.1340738414724499
ROC train: 0.993582	val: 0.896215	test: 0.776523
PRC train: 0.998559	val: 0.979477	test: 0.694272

Epoch: 54
Loss: 0.12349995122884716
ROC train: 0.992159	val: 0.889444	test: 0.759543
PRC train: 0.998206	val: 0.979668	test: 0.661236

Epoch: 55
Loss: 0.1378155647092464
ROC train: 0.993738	val: 0.891442	test: 0.761111
PRC train: 0.998590	val: 0.979149	test: 0.647421

Epoch: 56
Loss: 0.11679597881013155
ROC train: 0.994892	val: 0.892552	test: 0.777016
PRC train: 0.998851	val: 0.977317	test: 0.682158

Epoch: 57
Loss: 0.13117948948469918
ROC train: 0.994671	val: 0.895771	test: 0.773477
PRC train: 0.998789	val: 0.978150	test: 0.683652

Epoch: 58
Loss: 0.11348027261925403
ROC train: 0.993308	val: 0.885781	test: 0.757930
PRC train: 0.998488	val: 0.975718	test: 0.662650

Epoch: 59
Loss: 0.11793798839449475
ROC train: 0.992906	val: 0.906649	test: 0.766532
PRC train: 0.998396	val: 0.984205	test: 0.671273

Epoch: 60
Loss: 0.12222148842653786
ROC train: 0.995627	val: 0.894550	test: 0.759901
PRC train: 0.999025	val: 0.980628	test: 0.663246

Epoch: 61
Loss: 0.1275402783576906
ROC train: 0.995412	val: 0.882451	test: 0.750179
PRC train: 0.998983	val: 0.979234	test: 0.644245

Epoch: 62
Loss: 0.11894697941424259
ROC train: 0.995051	val: 0.886003	test: 0.758199
PRC train: 0.998890	val: 0.978605	test: 0.671482

Epoch: 63
Loss: 0.11446921123551403
ROC train: 0.995485	val: 0.875347	test: 0.760125
PRC train: 0.998974	val: 0.977655	test: 0.664714

Epoch: 64
Loss: 0.12180948055685288
ROC train: 0.996162	val: 0.888778	test: 0.771819
PRC train: 0.999139	val: 0.978516	test: 0.684624

Epoch: 65
Loss: 0.1186098127784621
ROC train: 0.994925	val: 0.894106	test: 0.759946
PRC train: 0.998860	val: 0.981640	test: 0.641944

Epoch: 66
Loss: 0.12130528434053263
ROC train: 0.995396	val: 0.898102	test: 0.767518
PRC train: 0.998965	val: 0.983422	test: 0.676036

Epoch: 67
Loss: 0.12046971986388612
ROC train: 0.996495	val: 0.893440	test: 0.770878
PRC train: 0.999216	val: 0.976403	test: 0.693946

Epoch: 68
Loss: 0.1319860091625781
ROC train: 0.992100	val: 0.886780	test: 0.754659
PRC train: 0.998204	val: 0.977889	test: 0.671239

Epoch: 69
Loss: 0.12237940115559716
ROC train: 0.994509	val: 0.862915	test: 0.739785
PRC train: 0.998756	val: 0.975990	test: 0.635367

Epoch: 70
Loss: 0.10382591047615268
ROC train: 0.994669	val: 0.907537	test: 0.759229
PRC train: 0.998800	val: 0.983774	test: 0.647912

Epoch: 71
Loss: 0.12837422763447967
ROC train: 0.994491	val: 0.886003	test: 0.750806
PRC train: 0.998772	val: 0.977254	test: 0.628735

Epoch: 72
Loss: 0.11102354945158244
ROC train: 0.995610	val: 0.864025	test: 0.734812
PRC train: 0.999023	val: 0.973032	test: 0.626762

Epoch: 73
Loss: 0.10107006891451592
ROC train: 0.996360	val: 0.875347	test: 0.755018
PRC train: 0.999188	val: 0.975213	test: 0.650549

Epoch: 74
Loss: 0.10439367632526819
ROC train: 0.997110	val: 0.882340	test: 0.761380
PRC train: 0.999353	val: 0.975246	test: 0.655946

Epoch: 75
Loss: 0.10819106582512938
ROC train: 0.996945	val: 0.887779	test: 0.756004
PRC train: 0.999311	val: 0.974237	test: 0.660583

Epoch: 76
Loss: 0.10042581969172278
ROC train: 0.996879	val: 0.893995	test: 0.748746
PRC train: 0.999295	val: 0.977573	test: 0.662294

Epoch: 77
Loss: 0.10160055341182483
ROC train: 0.996332	val: 0.900544	test: 0.751568
PRC train: 0.999172	val: 0.979437	test: 0.643852

Epoch: 78
Loss: 0.10700435632063741
ROC train: 0.996309	val: 0.905872	test: 0.748208
PRC train: 0.999177	val: 0.982837	test: 0.635114

Epoch: 79
Loss: 0.1037629768525389
ROC train: 0.997011	val: 0.904873	test: 0.748342
PRC train: 0.999334	val: 0.983997	test: 0.642540

Epoch: 80
Loss: 0.09776173329694017
ROC train: 0.997543	val: 0.895771	test: 0.748387
PRC train: 0.999453	val: 0.980787	test: 0.644323

Epoch: 81
Loss: 0.09257160329802831
ROC train: 0.997583	val: 0.886891	test: 0.737321
PRC train: 0.999463	val: 0.979160	test: 0.621825

Epoch: 82
Loss: 0.08814402279131839
ROC train: 0.997767	val: 0.903097	test: 0.742204
PRC train: 0.999504	val: 0.982983	test: 0.626855

Epoch: 83
Loss: 0.09539023802611002
ROC train: 0.998349	val: 0.904207	test: 0.743190
PRC train: 0.999630	val: 0.983042	test: 0.622631

Epoch: 84
Loss: 0.09378388790785241
ROC train: 0.997241	val: 0.896215	test: 0.731272
PRC train: 0.999385	val: 0.984272	test: 0.623662

Epoch: 85
Loss: 0.08936346121090517
ROC train: 0.997431	val: 0.891775	test: 0.736022
PRC train: 0.999425	val: 0.982535	test: 0.629579

Epoch: 86
Loss: 0.08208304662034303
ROC train: 0.997879	val: 0.901765	test: 0.746685
PRC train: 0.999525	val: 0.982515	test: 0.639685

Epoch: 87
Loss: 0.08741948777077935
ROC train: 0.998008	val: 0.892108	test: 0.739964
PRC train: 0.999557	val: 0.976996	test: 0.619440

Epoch: 88
Loss: 0.08320924981181954
ROC train: 0.998352	val: 0.872017	test: 0.739516
PRC train: 0.999633	val: 0.972849	test: 0.617287

Epoch: 89
Loss: 0.09464513210824164
ROC train: 0.998260	val: 0.871129	test: 0.754839
PRC train: 0.999608	val: 0.971977	test: 0.636545

Epoch: 90
Loss: 0.07877815167279914
ROC train: 0.997681	val: 0.889111	test: 0.750538
PRC train: 0.999478	val: 0.974895	test: 0.620068

Epoch: 91
Loss: 0.08848629367149302
ROC train: 0.998079	val: 0.902986	test: 0.741801
PRC train: 0.999572	val: 0.977971	test: 0.615096

Epoch: 92
Loss: 0.0874695271974855
ROC train: 0.997601	val: 0.908425	test: 0.750941
PRC train: 0.999467	val: 0.978037	test: 0.660490

Epoch: 93
Loss: 0.08407443061296797
ROC train: 0.997881	val: 0.880786	test: 0.730332
PRC train: 0.999528	val: 0.975842	test: 0.616396

Epoch: 94
Loss: 0.0827453703923168
ROC train: 0.984206	val: 0.889391	test: 0.716821
PRC train: 0.996783	val: 0.821714	test: 0.745613

Epoch: 34
Loss: 0.15769654348739073
ROC train: 0.984941	val: 0.902740	test: 0.714024
PRC train: 0.996973	val: 0.849529	test: 0.763107

Epoch: 35
Loss: 0.15249615655802498
ROC train: 0.986935	val: 0.890093	test: 0.728684
PRC train: 0.997367	val: 0.828977	test: 0.779279

Epoch: 36
Loss: 0.14541884082473813
ROC train: 0.985705	val: 0.896216	test: 0.732832
PRC train: 0.997100	val: 0.808151	test: 0.757767

Epoch: 37
Loss: 0.15249153261932138
ROC train: 0.985714	val: 0.873632	test: 0.680266
PRC train: 0.997075	val: 0.801246	test: 0.704080

Epoch: 38
Loss: 0.15079056132758
ROC train: 0.987311	val: 0.890093	test: 0.708237
PRC train: 0.997469	val: 0.817408	test: 0.747640

Epoch: 39
Loss: 0.14271391617946816
ROC train: 0.985979	val: 0.888186	test: 0.716532
PRC train: 0.997204	val: 0.808565	test: 0.763370

Epoch: 40
Loss: 0.1423924562830082
ROC train: 0.985307	val: 0.873030	test: 0.720004
PRC train: 0.997066	val: 0.786121	test: 0.757792

Epoch: 41
Loss: 0.15295233007078382
ROC train: 0.987327	val: 0.880357	test: 0.711034
PRC train: 0.997429	val: 0.790021	test: 0.725693

Epoch: 42
Loss: 0.12818237190598802
ROC train: 0.985032	val: 0.877948	test: 0.709684
PRC train: 0.997012	val: 0.786422	test: 0.720921

Epoch: 43
Loss: 0.134722307306354
ROC train: 0.986454	val: 0.886380	test: 0.717014
PRC train: 0.997211	val: 0.795063	test: 0.742122

Epoch: 44
Loss: 0.1492646512092593
ROC train: 0.988687	val: 0.886681	test: 0.723669
PRC train: 0.997743	val: 0.794124	test: 0.750992

Epoch: 45
Loss: 0.13889779033499097
ROC train: 0.989363	val: 0.884573	test: 0.726466
PRC train: 0.997845	val: 0.800497	test: 0.741977

Epoch: 46
Loss: 0.1319030384673903
ROC train: 0.990968	val: 0.878751	test: 0.712095
PRC train: 0.998209	val: 0.790062	test: 0.733275

Epoch: 47
Loss: 0.135744914019701
ROC train: 0.990066	val: 0.880658	test: 0.724248
PRC train: 0.997988	val: 0.805403	test: 0.763313

Epoch: 48
Loss: 0.12964339186209678
ROC train: 0.987808	val: 0.895714	test: 0.725791
PRC train: 0.997515	val: 0.822930	test: 0.764928

Epoch: 49
Loss: 0.11124638217296225
ROC train: 0.990933	val: 0.875640	test: 0.705343
PRC train: 0.998154	val: 0.794187	test: 0.745683

Epoch: 50
Loss: 0.14066192579905087
ROC train: 0.991223	val: 0.888989	test: 0.726273
PRC train: 0.998245	val: 0.793431	test: 0.750305

Epoch: 51
Loss: 0.12106555405254768
ROC train: 0.987078	val: 0.897621	test: 0.714024
PRC train: 0.997287	val: 0.807941	test: 0.719410

Epoch: 52
Loss: 0.1316143820416195
ROC train: 0.990857	val: 0.887785	test: 0.692033
PRC train: 0.998188	val: 0.822347	test: 0.718598

Epoch: 53
Loss: 0.10949492734068711
ROC train: 0.987070	val: 0.883870	test: 0.703029
PRC train: 0.997479	val: 0.822307	test: 0.734966

Epoch: 54
Loss: 0.12978126509810992
ROC train: 0.993816	val: 0.883368	test: 0.713831
PRC train: 0.998788	val: 0.797317	test: 0.740642

Epoch: 55
Loss: 0.12169007071749409
ROC train: 0.993361	val: 0.884171	test: 0.714410
PRC train: 0.998686	val: 0.788496	test: 0.728010

Epoch: 56
Loss: 0.11837909218860489
ROC train: 0.994119	val: 0.883971	test: 0.715856
PRC train: 0.998828	val: 0.786815	test: 0.742195

Epoch: 57
Loss: 0.11466174497552958
ROC train: 0.994440	val: 0.876443	test: 0.717014
PRC train: 0.998923	val: 0.780170	test: 0.743938

Epoch: 58
Loss: 0.10882063119706832
ROC train: 0.993998	val: 0.866406	test: 0.710262
PRC train: 0.998843	val: 0.762110	test: 0.729196

Epoch: 59
Loss: 0.09837987273938856
ROC train: 0.995047	val: 0.882164	test: 0.711034
PRC train: 0.999042	val: 0.788084	test: 0.726844

Epoch: 60
Loss: 0.1103094670591547
ROC train: 0.993168	val: 0.885075	test: 0.687114
PRC train: 0.998641	val: 0.779932	test: 0.692557

Epoch: 61
Loss: 0.10978747803761248
ROC train: 0.993041	val: 0.868112	test: 0.694637
PRC train: 0.998635	val: 0.743442	test: 0.700506

Epoch: 62
Loss: 0.10647342209908625
ROC train: 0.995514	val: 0.883870	test: 0.706404
PRC train: 0.999139	val: 0.776962	test: 0.729140

Epoch: 63
Loss: 0.11711981061350987
ROC train: 0.994189	val: 0.891398	test: 0.695312
PRC train: 0.998886	val: 0.802772	test: 0.731019

Epoch: 64
Loss: 0.1014678284515427
ROC train: 0.994744	val: 0.882264	test: 0.720968
PRC train: 0.998993	val: 0.778366	test: 0.748065

Epoch: 65
Loss: 0.12340171329340723
ROC train: 0.994131	val: 0.894008	test: 0.717303
PRC train: 0.998867	val: 0.812547	test: 0.753845

Epoch: 66
Loss: 0.10881005080047428
ROC train: 0.995479	val: 0.875640	test: 0.705440
PRC train: 0.999132	val: 0.775191	test: 0.735075

Epoch: 67
Loss: 0.0967294099690197
ROC train: 0.993942	val: 0.876342	test: 0.729167
PRC train: 0.998826	val: 0.780924	test: 0.772068

Epoch: 68
Loss: 0.10209039829057752
ROC train: 0.995154	val: 0.884071	test: 0.725309
PRC train: 0.999055	val: 0.778820	test: 0.729451

Epoch: 69
Loss: 0.09621760545701988
ROC train: 0.995417	val: 0.879956	test: 0.715664
PRC train: 0.999097	val: 0.788399	test: 0.715057

Epoch: 70
Loss: 0.09888062514028671
ROC train: 0.995506	val: 0.860885	test: 0.721933
PRC train: 0.999130	val: 0.747683	test: 0.735463

Epoch: 71
Loss: 0.10481180216876988
ROC train: 0.995929	val: 0.866807	test: 0.726466
PRC train: 0.999220	val: 0.779674	test: 0.773673

Epoch: 72
Loss: 0.09001275606994771
ROC train: 0.996541	val: 0.878250	test: 0.719811
PRC train: 0.999330	val: 0.781143	test: 0.736405

Epoch: 73
Loss: 0.0964880854874474
ROC train: 0.996635	val: 0.877948	test: 0.722029
PRC train: 0.999353	val: 0.770669	test: 0.736865

Epoch: 74
Loss: 0.08394465070504986
ROC train: 0.996896	val: 0.874435	test: 0.708237
PRC train: 0.999408	val: 0.761117	test: 0.724588

Epoch: 75
Loss: 0.08264287022398718
ROC train: 0.996763	val: 0.877346	test: 0.709105
PRC train: 0.999387	val: 0.764782	test: 0.746805

Epoch: 76
Loss: 0.09997561601742969
ROC train: 0.997144	val: 0.877948	test: 0.692612
PRC train: 0.999455	val: 0.774711	test: 0.720914

Epoch: 77
Loss: 0.09525530453242115
ROC train: 0.996694	val: 0.873131	test: 0.697531
PRC train: 0.999361	val: 0.768048	test: 0.719239

Epoch: 78
Loss: 0.08134239742008408
ROC train: 0.996868	val: 0.879755	test: 0.713252
PRC train: 0.999403	val: 0.771660	test: 0.729695

Epoch: 79
Loss: 0.09136375446004046
ROC train: 0.997639	val: 0.875339	test: 0.709973
PRC train: 0.999551	val: 0.777431	test: 0.721034

Epoch: 80
Loss: 0.08481179443163124
ROC train: 0.997148	val: 0.889090	test: 0.691840
PRC train: 0.999458	val: 0.779018	test: 0.706651

Epoch: 81
Loss: 0.0793712530697249
ROC train: 0.997798	val: 0.881863	test: 0.704282
PRC train: 0.999583	val: 0.790813	test: 0.737558

Epoch: 82
Loss: 0.0971803504375134
ROC train: 0.996785	val: 0.887082	test: 0.713349
PRC train: 0.999386	val: 0.792662	test: 0.752321

Epoch: 83
Loss: 0.10667040925858488
ROC train: 0.992937	val: 0.837097	test: 0.680363
PRC train: 0.998142	val: 0.727801	test: 0.705668

Epoch: 84
Loss: 0.10487865734753651
ROC train: 0.996021	val: 0.867409	test: 0.701003
PRC train: 0.999234	val: 0.763779	test: 0.714116

Epoch: 85
Loss: 0.09721951815431251
ROC train: 0.997743	val: 0.870421	test: 0.707272
PRC train: 0.999568	val: 0.771821	test: 0.714921

Epoch: 86
Loss: 0.09098656568591412
ROC train: 0.997446	val: 0.868915	test: 0.692226
PRC train: 0.999515	val: 0.773337	test: 0.706631

Epoch: 87
Loss: 0.08678092749152969
ROC train: 0.997500	val: 0.872528	test: 0.702450
PRC train: 0.999522	val: 0.787232	test: 0.709306

Epoch: 88
Loss: 0.08790316211792888
ROC train: 0.997956	val: 0.864499	test: 0.701485
PRC train: 0.999613	val: 0.766236	test: 0.721663

Epoch: 89
Loss: 0.08717429043259392
ROC train: 0.998191	val: 0.867711	test: 0.697917
PRC train: 0.999657	val: 0.767976	test: 0.710868

Epoch: 90
Loss: 0.07531664288325586
ROC train: 0.997864	val: 0.865603	test: 0.691454
PRC train: 0.999597	val: 0.750325	test: 0.725764

Epoch: 91
Loss: 0.08753975098312188
ROC train: 0.998193	val: 0.866004	test: 0.696952
PRC train: 0.999657	val: 0.749451	test: 0.733164

Epoch: 92
Loss: 0.08090580856362932
ROC train: 0.997607	val: 0.865502	test: 0.705440
PRC train: 0.999545	val: 0.747769	test: 0.729884

Epoch: 93
Loss: 0.08252605112348853
ROC train: 0.998256	val: 0.872227	test: 0.712095
PRC train: 0.999667	val: 0.742992	test: 0.728637

Epoch: 94
Loss: 0.08516875520783498
ROC train: 0.982370	val: 0.884171	test: 0.702836
PRC train: 0.996046	val: 0.811652	test: 0.726603

Epoch: 34
Loss: 0.16221729956505498
ROC train: 0.984317	val: 0.877145	test: 0.702064
PRC train: 0.996650	val: 0.807527	test: 0.722852

Epoch: 35
Loss: 0.151642392574353
ROC train: 0.984154	val: 0.878551	test: 0.692033
PRC train: 0.996378	val: 0.809038	test: 0.698267

Epoch: 36
Loss: 0.15019766546894103
ROC train: 0.984742	val: 0.880658	test: 0.692612
PRC train: 0.996581	val: 0.805845	test: 0.711337

Epoch: 37
Loss: 0.16059617233993032
ROC train: 0.986659	val: 0.883168	test: 0.701678
PRC train: 0.997226	val: 0.801486	test: 0.733159

Epoch: 38
Loss: 0.15788257247880538
ROC train: 0.986687	val: 0.868614	test: 0.694444
PRC train: 0.997262	val: 0.797619	test: 0.729909

Epoch: 39
Loss: 0.14372910851889237
ROC train: 0.982652	val: 0.880157	test: 0.674769
PRC train: 0.995143	val: 0.799262	test: 0.663723

Epoch: 40
Loss: 0.16352161382646255
ROC train: 0.983993	val: 0.881863	test: 0.693383
PRC train: 0.996448	val: 0.812497	test: 0.726904

Epoch: 41
Loss: 0.14117504192063454
ROC train: 0.986980	val: 0.874636	test: 0.684799
PRC train: 0.997231	val: 0.807255	test: 0.709958

Epoch: 42
Loss: 0.13503891332876475
ROC train: 0.988801	val: 0.875138	test: 0.678241
PRC train: 0.997697	val: 0.807574	test: 0.710711

Epoch: 43
Loss: 0.13415955167502688
ROC train: 0.989637	val: 0.882365	test: 0.693480
PRC train: 0.997877	val: 0.805002	test: 0.734047

Epoch: 44
Loss: 0.1323910657161633
ROC train: 0.989285	val: 0.879655	test: 0.687596
PRC train: 0.997861	val: 0.793424	test: 0.717697

Epoch: 45
Loss: 0.1315456840688054
ROC train: 0.991264	val: 0.880157	test: 0.682581
PRC train: 0.998286	val: 0.789311	test: 0.714000

Epoch: 46
Loss: 0.15133474998079205
ROC train: 0.990403	val: 0.868212	test: 0.684703
PRC train: 0.998088	val: 0.790530	test: 0.723630

Epoch: 47
Loss: 0.13473664507701796
ROC train: 0.989588	val: 0.861588	test: 0.686053
PRC train: 0.997925	val: 0.791341	test: 0.717881

Epoch: 48
Loss: 0.1299174813222205
ROC train: 0.992247	val: 0.869517	test: 0.663484
PRC train: 0.998477	val: 0.775656	test: 0.672903

Epoch: 49
Loss: 0.12870023812872552
ROC train: 0.992260	val: 0.857071	test: 0.650174
PRC train: 0.998478	val: 0.742843	test: 0.653828

Epoch: 50
Loss: 0.12215476218444651
ROC train: 0.991786	val: 0.856369	test: 0.679784
PRC train: 0.998369	val: 0.748271	test: 0.703482

Epoch: 51
Loss: 0.12360158073397895
ROC train: 0.993541	val: 0.866004	test: 0.683642
PRC train: 0.998688	val: 0.768048	test: 0.704874

Epoch: 52
Loss: 0.13068576898511167
ROC train: 0.993729	val: 0.870922	test: 0.675058
PRC train: 0.998735	val: 0.785914	test: 0.695528

Epoch: 53
Loss: 0.11296071167455816
ROC train: 0.992312	val: 0.880759	test: 0.659626
PRC train: 0.998456	val: 0.779215	test: 0.663979

Epoch: 54
Loss: 0.10995220129256089
ROC train: 0.992566	val: 0.886982	test: 0.663002
PRC train: 0.998551	val: 0.781061	test: 0.663070

Epoch: 55
Loss: 0.12835242248822526
ROC train: 0.993879	val: 0.880257	test: 0.671296
PRC train: 0.998821	val: 0.778493	test: 0.670927

Epoch: 56
Loss: 0.12893666795838715
ROC train: 0.993089	val: 0.869015	test: 0.665509
PRC train: 0.998657	val: 0.783219	test: 0.696195

Epoch: 57
Loss: 0.1161100794523855
ROC train: 0.994919	val: 0.879454	test: 0.653646
PRC train: 0.999022	val: 0.777437	test: 0.667924

Epoch: 58
Loss: 0.12462915864916269
ROC train: 0.994882	val: 0.882264	test: 0.650463
PRC train: 0.999000	val: 0.782631	test: 0.651433

Epoch: 59
Loss: 0.10482359737350698
ROC train: 0.995782	val: 0.877346	test: 0.665509
PRC train: 0.999180	val: 0.786793	test: 0.685480

Epoch: 60
Loss: 0.11280379361675734
ROC train: 0.995610	val: 0.881361	test: 0.681713
PRC train: 0.999149	val: 0.773859	test: 0.704888

Epoch: 61
Loss: 0.10748092919581108
ROC train: 0.996074	val: 0.863595	test: 0.661941
PRC train: 0.999238	val: 0.751410	test: 0.665903

Epoch: 62
Loss: 0.10778022350190328
ROC train: 0.996656	val: 0.866205	test: 0.658468
PRC train: 0.999358	val: 0.747349	test: 0.685659

Epoch: 63
Loss: 0.11936357082987772
ROC train: 0.995687	val: 0.870119	test: 0.662423
PRC train: 0.999171	val: 0.774352	test: 0.683498

Epoch: 64
Loss: 0.11453186776569957
ROC train: 0.996175	val: 0.872829	test: 0.661651
PRC train: 0.999271	val: 0.786396	test: 0.673999

Epoch: 65
Loss: 0.09205695172761116
ROC train: 0.995803	val: 0.869316	test: 0.667052
PRC train: 0.999201	val: 0.805214	test: 0.691659

Epoch: 66
Loss: 0.10790479265229143
ROC train: 0.996335	val: 0.881562	test: 0.671779
PRC train: 0.999305	val: 0.816566	test: 0.707107

Epoch: 67
Loss: 0.11024190016052245
ROC train: 0.996311	val: 0.873632	test: 0.671393
PRC train: 0.999300	val: 0.789580	test: 0.695972

Epoch: 68
Loss: 0.10808284594637119
ROC train: 0.996621	val: 0.873532	test: 0.676505
PRC train: 0.999353	val: 0.781612	test: 0.706386

Epoch: 69
Loss: 0.0924393731168431
ROC train: 0.996775	val: 0.860885	test: 0.668981
PRC train: 0.999382	val: 0.765545	test: 0.701321

Epoch: 70
Loss: 0.11096554288052829
ROC train: 0.994960	val: 0.870019	test: 0.658179
PRC train: 0.998926	val: 0.763475	test: 0.684178

Epoch: 71
Loss: 0.11256850590847851
ROC train: 0.996190	val: 0.867209	test: 0.660687
PRC train: 0.999267	val: 0.772805	test: 0.690782

Epoch: 72
Loss: 0.0897993762604702
ROC train: 0.997291	val: 0.866305	test: 0.668596
PRC train: 0.999487	val: 0.788064	test: 0.697543

Epoch: 73
Loss: 0.09817632788098075
ROC train: 0.997593	val: 0.872829	test: 0.671103
PRC train: 0.999540	val: 0.778418	test: 0.673319

Epoch: 74
Loss: 0.0941970924476243
ROC train: 0.997172	val: 0.871525	test: 0.662326
PRC train: 0.999459	val: 0.774013	test: 0.647066

Epoch: 75
Loss: 0.07959398649782547
ROC train: 0.997463	val: 0.862290	test: 0.653453
PRC train: 0.999512	val: 0.763973	test: 0.640976

Epoch: 76
Loss: 0.08173292840865973
ROC train: 0.997878	val: 0.878049	test: 0.664255
PRC train: 0.999598	val: 0.782275	test: 0.680290

Epoch: 77
Loss: 0.09108163318645633
ROC train: 0.997909	val: 0.868413	test: 0.661555
PRC train: 0.999601	val: 0.768780	test: 0.679739

Epoch: 78
Loss: 0.0966580650224959
ROC train: 0.998199	val: 0.864900	test: 0.648148
PRC train: 0.999656	val: 0.761324	test: 0.665436

Epoch: 79
Loss: 0.09365402042538182
ROC train: 0.997704	val: 0.863696	test: 0.658275
PRC train: 0.999560	val: 0.762563	test: 0.675519

Epoch: 80
Loss: 0.09464240585102311
ROC train: 0.996199	val: 0.866506	test: 0.652006
PRC train: 0.999274	val: 0.777379	test: 0.659896

Epoch: 81
Loss: 0.09933799300035506
ROC train: 0.997953	val: 0.860484	test: 0.661169
PRC train: 0.999608	val: 0.746568	test: 0.681396

Epoch: 82
Loss: 0.0965733057151287
ROC train: 0.998031	val: 0.879956	test: 0.671489
PRC train: 0.999626	val: 0.777279	test: 0.690234

Epoch: 83
Loss: 0.08895146731225549
ROC train: 0.998478	val: 0.872629	test: 0.650752
PRC train: 0.999709	val: 0.768950	test: 0.675402

Epoch: 84
Loss: 0.10292784898727438
ROC train: 0.998363	val: 0.883971	test: 0.662423
PRC train: 0.999690	val: 0.778308	test: 0.680078

Epoch: 85
Loss: 0.08982180404663323
ROC train: 0.998097	val: 0.875640	test: 0.668789
PRC train: 0.999637	val: 0.769860	test: 0.677710

Epoch: 86
Loss: 0.0911702391765075
ROC train: 0.997726	val: 0.877447	test: 0.677566
PRC train: 0.999564	val: 0.779515	test: 0.673024

Epoch: 87
Loss: 0.08218030110288621
ROC train: 0.997946	val: 0.855264	test: 0.647280
PRC train: 0.999606	val: 0.750126	test: 0.656376

Epoch: 88
Loss: 0.08300348167555602
ROC train: 0.997753	val: 0.868112	test: 0.638696
PRC train: 0.999575	val: 0.771375	test: 0.663676

Epoch: 89
Loss: 0.08002022338269645
ROC train: 0.998088	val: 0.873532	test: 0.645737
PRC train: 0.999635	val: 0.796900	test: 0.665859

Epoch: 90
Loss: 0.07648432651190196
ROC train: 0.998085	val: 0.872629	test: 0.660783
PRC train: 0.999632	val: 0.789194	test: 0.682093

Epoch: 91
Loss: 0.0836108956817596
ROC train: 0.998465	val: 0.887283	test: 0.651427
PRC train: 0.999711	val: 0.802310	test: 0.662158

Epoch: 92
Loss: 0.09433288916627038
ROC train: 0.998731	val: 0.896919	test: 0.664062
PRC train: 0.999760	val: 0.814451	test: 0.678713

Epoch: 93
Loss: 0.07713702500831024
ROC train: 0.997935	val: 0.870421	test: 0.645737
PRC train: 0.999612	val: 0.764305	test: 0.649744

Epoch: 94
Loss: 0.07312450783486031
ROC train: 0.983998	val: 0.897420	test: 0.690394
PRC train: 0.996547	val: 0.814716	test: 0.708262

Epoch: 34
Loss: 0.15880531146612606
ROC train: 0.985082	val: 0.887383	test: 0.696277
PRC train: 0.996732	val: 0.778914	test: 0.717474

Epoch: 35
Loss: 0.15055444492494804
ROC train: 0.982994	val: 0.896316	test: 0.709008
PRC train: 0.996086	val: 0.809261	test: 0.729545

Epoch: 36
Loss: 0.17178037328554888
ROC train: 0.985367	val: 0.895012	test: 0.685378
PRC train: 0.996935	val: 0.814650	test: 0.695687

Epoch: 37
Loss: 0.15210957455659385
ROC train: 0.982604	val: 0.899829	test: 0.691647
PRC train: 0.996158	val: 0.822898	test: 0.717733

Epoch: 38
Loss: 0.15897947923524322
ROC train: 0.983218	val: 0.897420	test: 0.702257
PRC train: 0.996392	val: 0.819082	test: 0.728049

Epoch: 39
Loss: 0.13720036024402008
ROC train: 0.987757	val: 0.888287	test: 0.701196
PRC train: 0.997532	val: 0.799024	test: 0.738752

Epoch: 40
Loss: 0.14514915126109137
ROC train: 0.987624	val: 0.895313	test: 0.706019
PRC train: 0.997542	val: 0.802008	test: 0.737063

Epoch: 41
Loss: 0.13301222681817942
ROC train: 0.989108	val: 0.892803	test: 0.685185
PRC train: 0.997865	val: 0.794706	test: 0.693086

Epoch: 42
Loss: 0.13754579104927256
ROC train: 0.989292	val: 0.894309	test: 0.683642
PRC train: 0.997893	val: 0.793510	test: 0.698733

Epoch: 43
Loss: 0.1274858790770275
ROC train: 0.989933	val: 0.899428	test: 0.678434
PRC train: 0.998005	val: 0.830322	test: 0.689441

Epoch: 44
Loss: 0.13365758461181315
ROC train: 0.991700	val: 0.900231	test: 0.685089
PRC train: 0.998340	val: 0.834821	test: 0.707986

Epoch: 45
Loss: 0.1265158014594113
ROC train: 0.989760	val: 0.887885	test: 0.696181
PRC train: 0.997954	val: 0.762031	test: 0.703034

Epoch: 46
Loss: 0.13319774685543279
ROC train: 0.991751	val: 0.893406	test: 0.687886
PRC train: 0.998377	val: 0.806664	test: 0.698056

Epoch: 47
Loss: 0.13490210862102828
ROC train: 0.990124	val: 0.902339	test: 0.696277
PRC train: 0.998017	val: 0.846440	test: 0.717334

Epoch: 48
Loss: 0.12664612270223582
ROC train: 0.992096	val: 0.900532	test: 0.706404
PRC train: 0.998437	val: 0.834253	test: 0.744203

Epoch: 49
Loss: 0.11914055823004735
ROC train: 0.994176	val: 0.890696	test: 0.686535
PRC train: 0.998873	val: 0.810619	test: 0.714663

Epoch: 50
Loss: 0.10904491806497026
ROC train: 0.992043	val: 0.897220	test: 0.698495
PRC train: 0.998428	val: 0.814450	test: 0.718443

Epoch: 51
Loss: 0.129748386595567
ROC train: 0.993327	val: 0.891298	test: 0.687982
PRC train: 0.998690	val: 0.804654	test: 0.694025

Epoch: 52
Loss: 0.11556211451475415
ROC train: 0.993950	val: 0.905450	test: 0.702160
PRC train: 0.998811	val: 0.844832	test: 0.721085

Epoch: 53
Loss: 0.1153585958780728
ROC train: 0.994814	val: 0.902238	test: 0.706019
PRC train: 0.998988	val: 0.829385	test: 0.738683

Epoch: 54
Loss: 0.11912310714944876
ROC train: 0.994587	val: 0.880759	test: 0.687404
PRC train: 0.998956	val: 0.774802	test: 0.691305

Epoch: 55
Loss: 0.11754204526693825
ROC train: 0.995154	val: 0.880558	test: 0.693383
PRC train: 0.999077	val: 0.785478	test: 0.691991

Epoch: 56
Loss: 0.11959908063606688
ROC train: 0.994786	val: 0.879454	test: 0.691069
PRC train: 0.999003	val: 0.783481	test: 0.689011

Epoch: 57
Loss: 0.127689360381724
ROC train: 0.992825	val: 0.885777	test: 0.663870
PRC train: 0.998593	val: 0.771989	test: 0.658500

Epoch: 58
Loss: 0.11414812522882002
ROC train: 0.993377	val: 0.880458	test: 0.689525
PRC train: 0.998703	val: 0.748791	test: 0.720605

Epoch: 59
Loss: 0.11839837566132538
ROC train: 0.989415	val: 0.878852	test: 0.711034
PRC train: 0.997880	val: 0.765446	test: 0.741135

Epoch: 60
Loss: 0.10839775492493874
ROC train: 0.990131	val: 0.882666	test: 0.689043
PRC train: 0.998048	val: 0.784182	test: 0.699040

Epoch: 61
Loss: 0.11233487477925588
ROC train: 0.995514	val: 0.898123	test: 0.669464
PRC train: 0.999132	val: 0.845251	test: 0.679219

Epoch: 62
Loss: 0.11532581955762385
ROC train: 0.995208	val: 0.895614	test: 0.675251
PRC train: 0.999071	val: 0.839459	test: 0.697417

Epoch: 63
Loss: 0.10849771590248948
ROC train: 0.995429	val: 0.896919	test: 0.681713
PRC train: 0.999114	val: 0.831921	test: 0.716555

Epoch: 64
Loss: 0.12490250099855951
ROC train: 0.994375	val: 0.889993	test: 0.686921
PRC train: 0.998922	val: 0.818822	test: 0.727675

Epoch: 65
Loss: 0.10865987297042591
ROC train: 0.994688	val: 0.883368	test: 0.675154
PRC train: 0.998964	val: 0.798691	test: 0.692556

Epoch: 66
Loss: 0.11339727619482244
ROC train: 0.995279	val: 0.882064	test: 0.681038
PRC train: 0.999087	val: 0.774749	test: 0.701975

Epoch: 67
Loss: 0.11904961815789274
ROC train: 0.994824	val: 0.883469	test: 0.704572
PRC train: 0.999006	val: 0.775064	test: 0.748623

Epoch: 68
Loss: 0.09963700997678455
ROC train: 0.995629	val: 0.876744	test: 0.706597
PRC train: 0.999168	val: 0.752861	test: 0.767082

Epoch: 69
Loss: 0.11119245262799535
ROC train: 0.996364	val: 0.893305	test: 0.694637
PRC train: 0.999301	val: 0.787085	test: 0.758963

Epoch: 70
Loss: 0.09817994449288185
ROC train: 0.996455	val: 0.879855	test: 0.683738
PRC train: 0.999322	val: 0.774986	test: 0.711405

Epoch: 71
Loss: 0.08918095751719272
ROC train: 0.996666	val: 0.884573	test: 0.687693
PRC train: 0.999362	val: 0.804002	test: 0.710232

Epoch: 72
Loss: 0.09390917284593796
ROC train: 0.997334	val: 0.890896	test: 0.688947
PRC train: 0.999491	val: 0.783132	test: 0.714101

Epoch: 73
Loss: 0.09540721644742389
ROC train: 0.996878	val: 0.886781	test: 0.684414
PRC train: 0.999404	val: 0.763556	test: 0.710928

Epoch: 74
Loss: 0.0896259250550249
ROC train: 0.996506	val: 0.886881	test: 0.673515
PRC train: 0.999332	val: 0.800097	test: 0.693146

Epoch: 75
Loss: 0.10081249950863443
ROC train: 0.997425	val: 0.883268	test: 0.687018
PRC train: 0.999512	val: 0.786810	test: 0.707735

Epoch: 76
Loss: 0.08698977327284689
ROC train: 0.996381	val: 0.888989	test: 0.697917
PRC train: 0.999302	val: 0.792556	test: 0.723793

Epoch: 77
Loss: 0.09460033939771706
ROC train: 0.996071	val: 0.887383	test: 0.703607
PRC train: 0.999244	val: 0.773961	test: 0.731736

Epoch: 78
Loss: 0.09993408651867929
ROC train: 0.997707	val: 0.886982	test: 0.681327
PRC train: 0.999561	val: 0.771958	test: 0.729707

Epoch: 79
Loss: 0.09177295927798092
ROC train: 0.997883	val: 0.893907	test: 0.694252
PRC train: 0.999593	val: 0.811404	test: 0.747968

Epoch: 80
Loss: 0.08640077099842473
ROC train: 0.997538	val: 0.891097	test: 0.706597
PRC train: 0.999529	val: 0.781512	test: 0.757899

Epoch: 81
Loss: 0.09163957870355524
ROC train: 0.998450	val: 0.887986	test: 0.682967
PRC train: 0.999703	val: 0.784510	test: 0.704285

Epoch: 82
Loss: 0.07296093157162895
ROC train: 0.996997	val: 0.882365	test: 0.688368
PRC train: 0.999426	val: 0.792123	test: 0.692020

Epoch: 83
Loss: 0.07899074078039622
ROC train: 0.997676	val: 0.884673	test: 0.677083
PRC train: 0.999555	val: 0.800540	test: 0.688429

Epoch: 84
Loss: 0.09049219131367865
ROC train: 0.997687	val: 0.876945	test: 0.658275
PRC train: 0.999558	val: 0.768022	test: 0.662716

Epoch: 85
Loss: 0.09092506004390219
ROC train: 0.997627	val: 0.891900	test: 0.692226
PRC train: 0.999549	val: 0.790851	test: 0.737359

Epoch: 86
Loss: 0.07971604752870652
ROC train: 0.998056	val: 0.880357	test: 0.707851
PRC train: 0.999629	val: 0.789602	test: 0.762819

Epoch: 87
Loss: 0.08112754200921132
ROC train: 0.998314	val: 0.872829	test: 0.696952
PRC train: 0.999678	val: 0.780823	test: 0.717959

Epoch: 88
Loss: 0.09299045415655985
ROC train: 0.998876	val: 0.870421	test: 0.700424
PRC train: 0.999784	val: 0.751407	test: 0.705560

Epoch: 89
Loss: 0.08901466801937921
ROC train: 0.998287	val: 0.867711	test: 0.714796
PRC train: 0.999676	val: 0.732810	test: 0.723614

Epoch: 90
Loss: 0.08258911250176858
ROC train: 0.998290	val: 0.881863	test: 0.689815
PRC train: 0.999674	val: 0.801256	test: 0.717036

Epoch: 91
Loss: 0.07058047349340393
ROC train: 0.998206	val: 0.890696	test: 0.684221
PRC train: 0.999658	val: 0.821599	test: 0.700008

Epoch: 92
Loss: 0.07998998988485198
ROC train: 0.998106	val: 0.867610	test: 0.689043
PRC train: 0.999641	val: 0.776133	test: 0.685075

Epoch: 93
Loss: 0.09248591311518566
ROC train: 0.997759	val: 0.874737	test: 0.715664
PRC train: 0.999573	val: 0.781279	test: 0.743762

Epoch: 94
Loss: 0.08167174737654706
ROC train: 0.997762	val: 0.877900	test: 0.748656
PRC train: 0.999498	val: 0.974317	test: 0.642964

Epoch: 95
Loss: 0.0895557004570557
ROC train: 0.998006	val: 0.842713	test: 0.745968
PRC train: 0.999557	val: 0.965556	test: 0.634210

Epoch: 96
Loss: 0.09954011018464846
ROC train: 0.998433	val: 0.853702	test: 0.759588
PRC train: 0.999651	val: 0.968354	test: 0.665849

Epoch: 97
Loss: 0.07642976239586721
ROC train: 0.998570	val: 0.883894	test: 0.762769
PRC train: 0.999679	val: 0.974450	test: 0.667262

Epoch: 98
Loss: 0.08073228066334354
ROC train: 0.998761	val: 0.893773	test: 0.762858
PRC train: 0.999721	val: 0.976989	test: 0.664314

Epoch: 99
Loss: 0.08419412729982306
ROC train: 0.998850	val: 0.893107	test: 0.755959
PRC train: 0.999744	val: 0.975685	test: 0.641225

Epoch: 100
Loss: 0.07759124932333998
ROC train: 0.998505	val: 0.856921	test: 0.731989
PRC train: 0.999670	val: 0.968594	test: 0.626883

Epoch: 101
Loss: 0.07215850823087282
ROC train: 0.998789	val: 0.856033	test: 0.742876
PRC train: 0.999730	val: 0.968421	test: 0.648432

Epoch: 102
Loss: 0.06896496822448754
ROC train: 0.998975	val: 0.883783	test: 0.759722
PRC train: 0.999771	val: 0.975129	test: 0.674356

Epoch: 103
Loss: 0.07913393582441201
ROC train: 0.998563	val: 0.875902	test: 0.758916
PRC train: 0.999685	val: 0.974155	test: 0.663486

Epoch: 104
Loss: 0.08805772313484556
ROC train: 0.999199	val: 0.872128	test: 0.749731
PRC train: 0.999822	val: 0.969993	test: 0.633887

Epoch: 105
Loss: 0.0775367425729925
ROC train: 0.999204	val: 0.881119	test: 0.751703
PRC train: 0.999822	val: 0.968787	test: 0.635857

Epoch: 106
Loss: 0.06712911145427572
ROC train: 0.999291	val: 0.895216	test: 0.760842
PRC train: 0.999841	val: 0.973696	test: 0.657188

Epoch: 107
Loss: 0.07542270650186046
ROC train: 0.999230	val: 0.886669	test: 0.765323
PRC train: 0.999827	val: 0.972855	test: 0.655056

Epoch: 108
Loss: 0.07822446959836504
ROC train: 0.999255	val: 0.883228	test: 0.761738
PRC train: 0.999833	val: 0.973156	test: 0.646296

Epoch: 109
Loss: 0.07626246581729446
ROC train: 0.999211	val: 0.873238	test: 0.756855
PRC train: 0.999825	val: 0.968490	test: 0.661699

Epoch: 110
Loss: 0.07150934948534647
ROC train: 0.998584	val: 0.856588	test: 0.757482
PRC train: 0.999684	val: 0.964779	test: 0.650444

Epoch: 111
Loss: 0.08362668716653299
ROC train: 0.998599	val: 0.852370	test: 0.754570
PRC train: 0.999688	val: 0.963998	test: 0.646154

Epoch: 112
Loss: 0.07400264316122872
ROC train: 0.998734	val: 0.868243	test: 0.758647
PRC train: 0.999715	val: 0.968183	test: 0.662402

Epoch: 113
Loss: 0.07607871243943573
ROC train: 0.998917	val: 0.880786	test: 0.763844
PRC train: 0.999760	val: 0.968831	test: 0.671708

Epoch: 114
Loss: 0.06900259669086693
ROC train: 0.998942	val: 0.872350	test: 0.761156
PRC train: 0.999764	val: 0.965695	test: 0.667561

Epoch: 115
Loss: 0.06681962931603169
ROC train: 0.999247	val: 0.870130	test: 0.769758
PRC train: 0.999830	val: 0.964838	test: 0.685804

Epoch: 116
Loss: 0.08210235480943902
ROC train: 0.999125	val: 0.864358	test: 0.767070
PRC train: 0.999804	val: 0.965223	test: 0.684951

Epoch: 117
Loss: 0.06380285320947959
ROC train: 0.999158	val: 0.847153	test: 0.747133
PRC train: 0.999811	val: 0.962326	test: 0.639545

Epoch: 118
Loss: 0.06323117763446283
ROC train: 0.998863	val: 0.831613	test: 0.745878
PRC train: 0.999747	val: 0.959917	test: 0.644898

Epoch: 119
Loss: 0.06981910491328824
ROC train: 0.999258	val: 0.844600	test: 0.752375
PRC train: 0.999833	val: 0.960052	test: 0.654744

Epoch: 120
Loss: 0.07894485461521367
ROC train: 0.999288	val: 0.871573	test: 0.760170
PRC train: 0.999840	val: 0.966501	test: 0.661780

Early stopping
Best (ROC):	 train: 0.920292	val: 0.944500	test: 0.768324
Best (PRC):	 train: 0.976926	val: 0.992623	test: 0.657987

ROC train: 0.998795	val: 0.846043	test: 0.735842
PRC train: 0.999732	val: 0.967382	test: 0.593290

Epoch: 95
Loss: 0.06949575742336353
ROC train: 0.998950	val: 0.875236	test: 0.755511
PRC train: 0.999764	val: 0.975110	test: 0.605158

Epoch: 96
Loss: 0.0885783893513228
ROC train: 0.998634	val: 0.849595	test: 0.745072
PRC train: 0.999694	val: 0.972324	test: 0.592535

Epoch: 97
Loss: 0.08568248134129086
ROC train: 0.998842	val: 0.864469	test: 0.742608
PRC train: 0.999740	val: 0.975055	test: 0.591814

Epoch: 98
Loss: 0.08956762884091628
ROC train: 0.999015	val: 0.870796	test: 0.739023
PRC train: 0.999779	val: 0.974999	test: 0.610397

Epoch: 99
Loss: 0.08794703125517182
ROC train: 0.998962	val: 0.861805	test: 0.736514
PRC train: 0.999769	val: 0.964652	test: 0.610270

Epoch: 100
Loss: 0.085947896987297
ROC train: 0.999010	val: 0.846154	test: 0.731496
PRC train: 0.999780	val: 0.958476	test: 0.604097

Epoch: 101
Loss: 0.07657401921472994
ROC train: 0.998861	val: 0.848818	test: 0.737903
PRC train: 0.999744	val: 0.962264	test: 0.603645

Epoch: 102
Loss: 0.08207361736206119
ROC train: 0.999005	val: 0.850705	test: 0.742921
PRC train: 0.999777	val: 0.960507	test: 0.605437

Epoch: 103
Loss: 0.08309281108579529
ROC train: 0.998837	val: 0.853369	test: 0.739651
PRC train: 0.999741	val: 0.965623	test: 0.598450

Epoch: 104
Loss: 0.077483168870882
ROC train: 0.998842	val: 0.862804	test: 0.744624
PRC train: 0.999742	val: 0.968353	test: 0.605170

Epoch: 105
Loss: 0.08027111896737534
ROC train: 0.998532	val: 0.857365	test: 0.753405
PRC train: 0.999669	val: 0.965465	test: 0.610210

Epoch: 106
Loss: 0.0792362256683383
ROC train: 0.998636	val: 0.856033	test: 0.747760
PRC train: 0.999694	val: 0.963558	test: 0.599761

Epoch: 107
Loss: 0.0652547211763931
ROC train: 0.999189	val: 0.868132	test: 0.750045
PRC train: 0.999817	val: 0.964793	test: 0.611617

Epoch: 108
Loss: 0.07597076243714913
ROC train: 0.999300	val: 0.868909	test: 0.758199
PRC train: 0.999843	val: 0.966199	test: 0.627175

Epoch: 109
Loss: 0.0664604377245509
ROC train: 0.999132	val: 0.854368	test: 0.758781
PRC train: 0.999804	val: 0.965929	test: 0.625293

Epoch: 110
Loss: 0.07048862444963545
ROC train: 0.998593	val: 0.833944	test: 0.744041
PRC train: 0.999686	val: 0.966146	test: 0.611244

Epoch: 111
Loss: 0.07293216140023616
ROC train: 0.999029	val: 0.856588	test: 0.746371
PRC train: 0.999783	val: 0.967269	test: 0.607743

Epoch: 112
Loss: 0.06565628884951612
ROC train: 0.999204	val: 0.855478	test: 0.746192
PRC train: 0.999820	val: 0.962511	test: 0.613838

Epoch: 113
Loss: 0.07276210376535037
ROC train: 0.999015	val: 0.842380	test: 0.747177
PRC train: 0.999779	val: 0.961200	test: 0.616673

Epoch: 114
Loss: 0.07421822748895825
ROC train: 0.999250	val: 0.848485	test: 0.748566
PRC train: 0.999831	val: 0.962228	test: 0.623117

Epoch: 115
Loss: 0.0658425263572981
ROC train: 0.999044	val: 0.816184	test: 0.731900
PRC train: 0.999785	val: 0.953142	test: 0.612906

Epoch: 116
Loss: 0.07280745079645796
ROC train: 0.998998	val: 0.821179	test: 0.729167
PRC train: 0.999776	val: 0.950676	test: 0.602460

Epoch: 117
Loss: 0.07743067227546252
ROC train: 0.997546	val: 0.798868	test: 0.713396
PRC train: 0.999454	val: 0.945854	test: 0.575970

Epoch: 118
Loss: 0.06956785966984687
ROC train: 0.999084	val: 0.810190	test: 0.719713
PRC train: 0.999796	val: 0.947936	test: 0.583123

Epoch: 119
Loss: 0.07079612544379078
ROC train: 0.999153	val: 0.843379	test: 0.725538
PRC train: 0.999811	val: 0.963389	test: 0.592369

Epoch: 120
Loss: 0.0761792909388841
ROC train: 0.998716	val: 0.855478	test: 0.721864
PRC train: 0.999714	val: 0.974170	test: 0.580182

Early stopping
Best (ROC):	 train: 0.980021	val: 0.951160	test: 0.774328
Best (PRC):	 train: 0.995086	val: 0.993861	test: 0.676872

ROC train: 0.998428	val: 0.896548	test: 0.734722
PRC train: 0.999650	val: 0.977957	test: 0.617413

Epoch: 95
Loss: 0.07719247965117787
ROC train: 0.998710	val: 0.895105	test: 0.738799
PRC train: 0.999711	val: 0.978335	test: 0.634272

Epoch: 96
Loss: 0.08894349640478726
ROC train: 0.998566	val: 0.879232	test: 0.734229
PRC train: 0.999680	val: 0.976001	test: 0.618057

Epoch: 97
Loss: 0.09598284969221886
ROC train: 0.997177	val: 0.872128	test: 0.733289
PRC train: 0.999376	val: 0.973919	test: 0.612932

Epoch: 98
Loss: 0.11115663002452802
ROC train: 0.998158	val: 0.821956	test: 0.723880
PRC train: 0.999591	val: 0.965166	test: 0.612196

Epoch: 99
Loss: 0.11012391221299857
ROC train: 0.998433	val: 0.836719	test: 0.740412
PRC train: 0.999650	val: 0.965584	test: 0.639212

Epoch: 100
Loss: 0.08568056148532442
ROC train: 0.997599	val: 0.847819	test: 0.752375
PRC train: 0.999466	val: 0.968149	test: 0.637994

Epoch: 101
Loss: 0.08839781641462398
ROC train: 0.998697	val: 0.862027	test: 0.749462
PRC train: 0.999708	val: 0.967494	test: 0.658015

Epoch: 102
Loss: 0.09858211414161373
ROC train: 0.998604	val: 0.868687	test: 0.737142
PRC train: 0.999689	val: 0.971729	test: 0.639570

Epoch: 103
Loss: 0.08375588738341892
ROC train: 0.998576	val: 0.860029	test: 0.734857
PRC train: 0.999681	val: 0.969417	test: 0.608969

Epoch: 104
Loss: 0.07314500196075392
ROC train: 0.998499	val: 0.866689	test: 0.735125
PRC train: 0.999664	val: 0.970529	test: 0.605608

Epoch: 105
Loss: 0.0756758159440686
ROC train: 0.998963	val: 0.885115	test: 0.750717
PRC train: 0.999769	val: 0.974546	test: 0.641159

Epoch: 106
Loss: 0.07691192900620415
ROC train: 0.998795	val: 0.902209	test: 0.743907
PRC train: 0.999731	val: 0.977796	test: 0.652829

Epoch: 107
Loss: 0.06758097566263288
ROC train: 0.998695	val: 0.898546	test: 0.744534
PRC train: 0.999709	val: 0.972841	test: 0.645378

Epoch: 108
Loss: 0.07324046689390822
ROC train: 0.998907	val: 0.896437	test: 0.746192
PRC train: 0.999756	val: 0.966806	test: 0.650187

Epoch: 109
Loss: 0.0853770841611393
ROC train: 0.999183	val: 0.893551	test: 0.741129
PRC train: 0.999817	val: 0.963547	test: 0.642805

Epoch: 110
Loss: 0.07925111595720423
ROC train: 0.999001	val: 0.880009	test: 0.728539
PRC train: 0.999778	val: 0.961248	test: 0.622244

Epoch: 111
Loss: 0.07901858194221269
ROC train: 0.999113	val: 0.892108	test: 0.727688
PRC train: 0.999802	val: 0.966904	test: 0.602371

Epoch: 112
Loss: 0.07686260982371855
ROC train: 0.999044	val: 0.901210	test: 0.734095
PRC train: 0.999786	val: 0.973526	test: 0.608421

Epoch: 113
Loss: 0.07500250180989006
ROC train: 0.998876	val: 0.900988	test: 0.739830
PRC train: 0.999747	val: 0.977745	test: 0.610900

Epoch: 114
Loss: 0.07990069056956009
ROC train: 0.998959	val: 0.903874	test: 0.754928
PRC train: 0.999767	val: 0.976542	test: 0.645136

Epoch: 115
Loss: 0.07991615066336362
ROC train: 0.998494	val: 0.896659	test: 0.743414
PRC train: 0.999665	val: 0.976354	test: 0.618532

Epoch: 116
Loss: 0.07255768641359266
ROC train: 0.998662	val: 0.889555	test: 0.734901
PRC train: 0.999702	val: 0.969147	test: 0.617070

Epoch: 117
Loss: 0.06346009434261977
ROC train: 0.998825	val: 0.896215	test: 0.731496
PRC train: 0.999738	val: 0.968749	test: 0.623850

Epoch: 118
Loss: 0.0733107237979396
ROC train: 0.998968	val: 0.902431	test: 0.726613
PRC train: 0.999769	val: 0.977830	test: 0.603463

Epoch: 119
Loss: 0.06651945737546443
ROC train: 0.998718	val: 0.897991	test: 0.728584
PRC train: 0.999713	val: 0.980115	test: 0.597517

Epoch: 120
Loss: 0.06557927556680418
ROC train: 0.999209	val: 0.899545	test: 0.738038
PRC train: 0.999822	val: 0.980705	test: 0.606570

Early stopping
Best (ROC):	 train: 0.977579	val: 0.940837	test: 0.779570
Best (PRC):	 train: 0.994163	val: 0.992121	test: 0.662754
All runs completed.

ROC train: 0.998219	val: 0.875339	test: 0.710069
PRC train: 0.999658	val: 0.777297	test: 0.724173

Epoch: 95
Loss: 0.0887586929340572
ROC train: 0.997513	val: 0.873934	test: 0.709491
PRC train: 0.999519	val: 0.789982	test: 0.719880

Epoch: 96
Loss: 0.08554329843044238
ROC train: 0.998356	val: 0.861488	test: 0.695120
PRC train: 0.999688	val: 0.751351	test: 0.726099

Epoch: 97
Loss: 0.08016341265636619
ROC train: 0.997962	val: 0.881963	test: 0.703993
PRC train: 0.999612	val: 0.752963	test: 0.735706

Epoch: 98
Loss: 0.07955101066491162
ROC train: 0.998303	val: 0.883469	test: 0.703029
PRC train: 0.999676	val: 0.767195	test: 0.723661

Epoch: 99
Loss: 0.06279949430642977
ROC train: 0.997730	val: 0.877346	test: 0.691165
PRC train: 0.999564	val: 0.776505	test: 0.703954

Epoch: 100
Loss: 0.08010983885575915
ROC train: 0.998311	val: 0.886380	test: 0.699460
PRC train: 0.999680	val: 0.771663	test: 0.722548

Epoch: 101
Loss: 0.07253852020988265
ROC train: 0.998609	val: 0.882465	test: 0.686246
PRC train: 0.999737	val: 0.773538	test: 0.711906

Epoch: 102
Loss: 0.07967881167730392
ROC train: 0.998663	val: 0.885376	test: 0.686246
PRC train: 0.999747	val: 0.777924	test: 0.705530

Epoch: 103
Loss: 0.07485800748010348
ROC train: 0.998623	val: 0.880759	test: 0.673997
PRC train: 0.999740	val: 0.780356	test: 0.676444

Epoch: 104
Loss: 0.08084615118131164
ROC train: 0.998618	val: 0.885476	test: 0.682002
PRC train: 0.999738	val: 0.774822	test: 0.681810

Epoch: 105
Loss: 0.06938613489295369
ROC train: 0.998388	val: 0.894309	test: 0.692130
PRC train: 0.999695	val: 0.792930	test: 0.718001

Epoch: 106
Loss: 0.07050386770400234
ROC train: 0.998460	val: 0.874134	test: 0.688079
PRC train: 0.999707	val: 0.795124	test: 0.735242

Epoch: 107
Loss: 0.06564548937631846
ROC train: 0.998545	val: 0.874737	test: 0.674865
PRC train: 0.999723	val: 0.796967	test: 0.722871

Epoch: 108
Loss: 0.06816701729014792
ROC train: 0.998357	val: 0.874937	test: 0.676408
PRC train: 0.999688	val: 0.776931	test: 0.706060

Epoch: 109
Loss: 0.07470685582711467
ROC train: 0.998447	val: 0.876142	test: 0.692805
PRC train: 0.999704	val: 0.789475	test: 0.726835

Epoch: 110
Loss: 0.06856333609863915
ROC train: 0.998428	val: 0.882565	test: 0.692515
PRC train: 0.999701	val: 0.797761	test: 0.728708

Epoch: 111
Loss: 0.07639282774227708
ROC train: 0.999017	val: 0.884473	test: 0.692033
PRC train: 0.999812	val: 0.787652	test: 0.709974

Epoch: 112
Loss: 0.08018588354455063
ROC train: 0.998854	val: 0.881261	test: 0.689140
PRC train: 0.999782	val: 0.795049	test: 0.706049

Epoch: 113
Loss: 0.06781164634598015
ROC train: 0.998238	val: 0.878751	test: 0.697627
PRC train: 0.999654	val: 0.780950	test: 0.727005

Epoch: 114
Loss: 0.07665620372736334
ROC train: 0.998731	val: 0.866606	test: 0.699749
PRC train: 0.999758	val: 0.762285	test: 0.724986

Epoch: 115
Loss: 0.06070787164642035
ROC train: 0.998829	val: 0.856469	test: 0.666667
PRC train: 0.999778	val: 0.754662	test: 0.677916

Epoch: 116
Loss: 0.0649250731056679
ROC train: 0.999129	val: 0.867510	test: 0.668113
PRC train: 0.999834	val: 0.777460	test: 0.678949

Epoch: 117
Loss: 0.06365675864686662
ROC train: 0.998701	val: 0.855064	test: 0.649788
PRC train: 0.999753	val: 0.756618	test: 0.662805

Epoch: 118
Loss: 0.07726720856302353
ROC train: 0.999085	val: 0.868112	test: 0.682774
PRC train: 0.999825	val: 0.767336	test: 0.716118

Epoch: 119
Loss: 0.052211259467574336
ROC train: 0.998977	val: 0.883268	test: 0.698013
PRC train: 0.999805	val: 0.802994	test: 0.737755

Epoch: 120
Loss: 0.06686012919523951
ROC train: 0.998729	val: 0.873833	test: 0.691454
PRC train: 0.999757	val: 0.780881	test: 0.708629

Early stopping
Best (ROC):	 train: 0.882159	val: 0.913279	test: 0.634838
Best (PRC):	 train: 0.970516	val: 0.836754	test: 0.686622

ROC train: 0.998255	val: 0.872428	test: 0.651427
PRC train: 0.999668	val: 0.758831	test: 0.653691

Epoch: 95
Loss: 0.08456943805138903
ROC train: 0.998624	val: 0.873231	test: 0.656732
PRC train: 0.999739	val: 0.757464	test: 0.661823

Epoch: 96
Loss: 0.07908451306235997
ROC train: 0.998735	val: 0.866606	test: 0.658758
PRC train: 0.999760	val: 0.754292	test: 0.662579

Epoch: 97
Loss: 0.06494144110733481
ROC train: 0.998547	val: 0.871424	test: 0.658083
PRC train: 0.999724	val: 0.769511	test: 0.678442

Epoch: 98
Loss: 0.08168715720889526
ROC train: 0.998276	val: 0.868714	test: 0.660012
PRC train: 0.999670	val: 0.765301	test: 0.658780

Epoch: 99
Loss: 0.07124357213691727
ROC train: 0.997625	val: 0.873030	test: 0.651524
PRC train: 0.999515	val: 0.774766	test: 0.637549

Epoch: 100
Loss: 0.08196023596051523
ROC train: 0.998837	val: 0.871625	test: 0.639178
PRC train: 0.999778	val: 0.755129	test: 0.639038

Epoch: 101
Loss: 0.0702821962830231
ROC train: 0.998761	val: 0.871625	test: 0.634838
PRC train: 0.999765	val: 0.761717	test: 0.634424

Epoch: 102
Loss: 0.07814199844571248
ROC train: 0.998837	val: 0.874737	test: 0.653453
PRC train: 0.999778	val: 0.773833	test: 0.649007

Epoch: 103
Loss: 0.08586380943980425
ROC train: 0.998498	val: 0.857071	test: 0.630980
PRC train: 0.999711	val: 0.756970	test: 0.634528

Epoch: 104
Loss: 0.08098311563809861
ROC train: 0.999078	val: 0.862692	test: 0.636285
PRC train: 0.999826	val: 0.768696	test: 0.628006

Epoch: 105
Loss: 0.0720124716579812
ROC train: 0.998795	val: 0.867309	test: 0.652392
PRC train: 0.999771	val: 0.765617	test: 0.632005

Epoch: 106
Loss: 0.07978573493413986
ROC train: 0.998534	val: 0.871926	test: 0.647859
PRC train: 0.999720	val: 0.772907	test: 0.624590

Epoch: 107
Loss: 0.07060814446473444
ROC train: 0.998735	val: 0.878350	test: 0.642650
PRC train: 0.999758	val: 0.767148	test: 0.632679

Epoch: 108
Loss: 0.07270578058089643
ROC train: 0.998529	val: 0.855867	test: 0.645930
PRC train: 0.999719	val: 0.743630	test: 0.641118

Epoch: 109
Loss: 0.06932117931924438
ROC train: 0.998991	val: 0.859882	test: 0.644001
PRC train: 0.999806	val: 0.753327	test: 0.636621

Epoch: 110
Loss: 0.08146088847846056
ROC train: 0.999052	val: 0.863595	test: 0.637539
PRC train: 0.999819	val: 0.760816	test: 0.643217

Epoch: 111
Loss: 0.07086547083203305
ROC train: 0.999143	val: 0.865201	test: 0.630691
PRC train: 0.999836	val: 0.768356	test: 0.643079

Epoch: 112
Loss: 0.06602659326525837
ROC train: 0.999129	val: 0.874536	test: 0.649113
PRC train: 0.999835	val: 0.783089	test: 0.658261

Epoch: 113
Loss: 0.06771489616356809
ROC train: 0.999214	val: 0.859079	test: 0.652199
PRC train: 0.999851	val: 0.757802	test: 0.653682

Epoch: 114
Loss: 0.07623255067613435
ROC train: 0.999293	val: 0.872629	test: 0.647955
PRC train: 0.999865	val: 0.777646	test: 0.652828

Epoch: 115
Loss: 0.06322150298572789
ROC train: 0.999265	val: 0.856770	test: 0.638503
PRC train: 0.999859	val: 0.756254	test: 0.644972

Epoch: 116
Loss: 0.06341466581959583
ROC train: 0.999308	val: 0.854160	test: 0.628954
PRC train: 0.999867	val: 0.759659	test: 0.646651

Epoch: 117
Loss: 0.06970315097875768
ROC train: 0.999168	val: 0.845729	test: 0.639178
PRC train: 0.999841	val: 0.736549	test: 0.634387

Epoch: 118
Loss: 0.059719136685516865
ROC train: 0.999085	val: 0.857071	test: 0.646798
PRC train: 0.999826	val: 0.762294	test: 0.641161

Epoch: 119
Loss: 0.06107517649275031
ROC train: 0.999337	val: 0.849142	test: 0.610629
PRC train: 0.999874	val: 0.729012	test: 0.613103

Epoch: 120
Loss: 0.06614236244714221
ROC train: 0.998935	val: 0.846030	test: 0.603299
PRC train: 0.999797	val: 0.728414	test: 0.593956

Early stopping
Best (ROC):	 train: 0.953306	val: 0.904246	test: 0.695216
Best (PRC):	 train: 0.988816	val: 0.801886	test: 0.739644

ROC train: 0.998270	val: 0.873733	test: 0.702064
PRC train: 0.999670	val: 0.774597	test: 0.730145

Epoch: 95
Loss: 0.08245446621773223
ROC train: 0.998674	val: 0.870722	test: 0.694927
PRC train: 0.999746	val: 0.789533	test: 0.708702

Epoch: 96
Loss: 0.08501706804580775
ROC train: 0.998105	val: 0.866305	test: 0.688465
PRC train: 0.999636	val: 0.767882	test: 0.691848

Epoch: 97
Loss: 0.09014863891095626
ROC train: 0.997939	val: 0.858276	test: 0.695312
PRC train: 0.999609	val: 0.728504	test: 0.696748

Epoch: 98
Loss: 0.0719178330773402
ROC train: 0.999015	val: 0.875339	test: 0.681231
PRC train: 0.999813	val: 0.775346	test: 0.679558

Epoch: 99
Loss: 0.06157071624049778
ROC train: 0.999004	val: 0.875238	test: 0.671489
PRC train: 0.999810	val: 0.787495	test: 0.663651

Epoch: 100
Loss: 0.07403307341174438
ROC train: 0.998552	val: 0.880458	test: 0.676890
PRC train: 0.999723	val: 0.790872	test: 0.670990

Epoch: 101
Loss: 0.08272315060229858
ROC train: 0.998468	val: 0.870320	test: 0.685185
PRC train: 0.999707	val: 0.763716	test: 0.683974

Epoch: 102
Loss: 0.07536654822552578
ROC train: 0.998851	val: 0.873532	test: 0.667728
PRC train: 0.999780	val: 0.782135	test: 0.667264

Epoch: 103
Loss: 0.07759981311282409
ROC train: 0.998032	val: 0.880157	test: 0.695120
PRC train: 0.999623	val: 0.804180	test: 0.697157

Epoch: 104
Loss: 0.07568185462829922
ROC train: 0.998499	val: 0.891699	test: 0.707465
PRC train: 0.999714	val: 0.811352	test: 0.724563

Epoch: 105
Loss: 0.0731204734672986
ROC train: 0.998914	val: 0.892302	test: 0.691744
PRC train: 0.999791	val: 0.820888	test: 0.722476

Epoch: 106
Loss: 0.08804970141951805
ROC train: 0.998899	val: 0.885878	test: 0.683642
PRC train: 0.999790	val: 0.793561	test: 0.694122

Epoch: 107
Loss: 0.08449086055091283
ROC train: 0.998729	val: 0.888889	test: 0.692998
PRC train: 0.999757	val: 0.798450	test: 0.696268

Epoch: 108
Loss: 0.08012179609850525
ROC train: 0.998436	val: 0.877246	test: 0.689718
PRC train: 0.999704	val: 0.779930	test: 0.688588

Epoch: 109
Loss: 0.06931483662986648
ROC train: 0.998886	val: 0.886480	test: 0.687693
PRC train: 0.999788	val: 0.801286	test: 0.692469

Epoch: 110
Loss: 0.07645277024371311
ROC train: 0.999097	val: 0.891097	test: 0.677566
PRC train: 0.999829	val: 0.811260	test: 0.703192

Epoch: 111
Loss: 0.06868080092384479
ROC train: 0.999017	val: 0.879655	test: 0.681424
PRC train: 0.999817	val: 0.781284	test: 0.706889

Epoch: 112
Loss: 0.07781236795390688
ROC train: 0.998857	val: 0.872026	test: 0.679880
PRC train: 0.999785	val: 0.747341	test: 0.702853

Epoch: 113
Loss: 0.07188285115085703
ROC train: 0.999207	val: 0.891599	test: 0.689236
PRC train: 0.999850	val: 0.801550	test: 0.713165

Epoch: 114
Loss: 0.06349745986369999
ROC train: 0.998973	val: 0.897722	test: 0.703993
PRC train: 0.999804	val: 0.813825	test: 0.735792

Epoch: 115
Loss: 0.06108884732561975
ROC train: 0.999129	val: 0.891398	test: 0.703318
PRC train: 0.999833	val: 0.806555	test: 0.733734

Epoch: 116
Loss: 0.06275097040651026
ROC train: 0.999352	val: 0.894710	test: 0.686632
PRC train: 0.999875	val: 0.816414	test: 0.705710

Epoch: 117
Loss: 0.05163487087479448
ROC train: 0.999059	val: 0.893004	test: 0.679495
PRC train: 0.999822	val: 0.813784	test: 0.688760

Epoch: 118
Loss: 0.062381449888229
ROC train: 0.998937	val: 0.882365	test: 0.676505
PRC train: 0.999798	val: 0.804187	test: 0.689826

Epoch: 119
Loss: 0.06228129749378632
ROC train: 0.998917	val: 0.879655	test: 0.689718
PRC train: 0.999797	val: 0.802947	test: 0.712591

Epoch: 120
Loss: 0.06599636675581552
ROC train: 0.999346	val: 0.887183	test: 0.691262
PRC train: 0.999877	val: 0.809084	test: 0.708770

Early stopping
Best (ROC):	 train: 0.909110	val: 0.919101	test: 0.660301
Best (PRC):	 train: 0.976120	val: 0.853440	test: 0.682818
All runs completed.
