>>> Starting run for dataset: tox21
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running RANDOM configs_static_noise_experiments/GraphCL/tox21/noise=0.0.yml on cuda:0
Running RANDOM configs_static_noise_experiments/GraphCL/tox21/noise=0.05.yml on cuda:1
Running RANDOM configs_static_noise_experiments/GraphCL/tox21/noise=0.1.yml on cuda:2
Running RANDOM configs_static_noise_experiments/GraphCL/tox21/noise=0.2.yml on cuda:3
Starting process for seed 1: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.2.yml --runseed 1 --device cuda:3
Starting process for seed 2: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.2.yml --runseed 2 --device cuda:3
Starting process for seed 3: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.2.yml --runseed 3 --device cuda:3
Starting process for seed 1: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.1.yml --runseed 1 --device cuda:2
Starting process for seed 1: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.0.yml --runseed 1 --device cuda:0
Starting process for seed 2: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.1.yml --runseed 2 --device cuda:2
Starting process for seed 2: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.0.yml --runseed 2 --device cuda:0
Starting process for seed 3: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.1.yml --runseed 3 --device cuda:2
Starting process for seed 3: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.0.yml --runseed 3 --device cuda:0
Starting process for seed 1: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.05.yml --runseed 1 --device cuda:1
Starting process for seed 2: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.05.yml --runseed 2 --device cuda:1
Starting process for seed 3: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.05.yml --runseed 3 --device cuda:1
[14:43:33] WARNING: not removing hydrogen atom without neighbors
[14:43:33] WARNING: not removing hydrogen atom without neighbors
[14:43:33] WARNING: not removing hydrogen atom without neighbors
[14:43:33] WARNING: not removing hydrogen atom without neighbors
[14:43:33] WARNING: not removing hydrogen atom without neighbors
[14:43:34] WARNING: not removing hydrogen atom without neighbors
[14:43:34] WARNING: not removing hydrogen atom without neighbors
[14:43:34] WARNING: not removing hydrogen atom without neighbors
[14:43:34] WARNING: not removing hydrogen atom without neighbors
[14:43:35] WARNING: not removing hydrogen atom without neighbors
[14:43:35] WARNING: not removing hydrogen atom without neighbors
[14:43:35] WARNING: not removing hydrogen atom without neighbors
[14:43:36] WARNING: not removing hydrogen atom without neighbors
[14:43:36] WARNING: not removing hydrogen atom without neighbors
[14:43:36] WARNING: not removing hydrogen atom without neighbors
[14:43:36] WARNING: not removing hydrogen atom without neighbors
[14:43:36] WARNING: not removing hydrogen atom without neighbors
[14:43:36] WARNING: not removing hydrogen atom without neighbors
[14:43:37] WARNING: not removing hydrogen atom without neighbors
[14:43:37] WARNING: not removing hydrogen atom without neighbors
[14:43:37] WARNING: not removing hydrogen atom without neighbors
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/tox21/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/tox21/noise=0.0/tox21_scaff_3_20-05_14-43-32  ]
[ Using Seed :  3  ]
[ Using device :  cuda:0  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5305622776995732
ROC train: 0.687801	val: 0.566021	test: 0.564784
PRC train: 0.200267	val: 0.179115	test: 0.172086

Epoch: 2
Loss: 0.3262622112313599
ROC train: 0.756492	val: 0.717914	test: 0.653178
PRC train: 0.275931	val: 0.282548	test: 0.238742

Epoch: 3
Loss: 0.2364828281901236
ROC train: 0.788712	val: 0.747235	test: 0.692766
PRC train: 0.308631	val: 0.280210	test: 0.274143

Epoch: 4
Loss: 0.20931440544998542
ROC train: 0.790306	val: 0.717605	test: 0.688066
PRC train: 0.318174	val: 0.252249	test: 0.285328

Epoch: 5
Loss: 0.20289475130500448
ROC train: 0.820010	val: 0.759199	test: 0.717820
PRC train: 0.366018	val: 0.305473	test: 0.319972

Epoch: 6
Loss: 0.1958337385377621
ROC train: 0.835843	val: 0.771518	test: 0.725538
PRC train: 0.390611	val: 0.335051	test: 0.340200

Epoch: 7
Loss: 0.19062171573995637
ROC train: 0.843109	val: 0.757969	test: 0.728275
PRC train: 0.414692	val: 0.324684	test: 0.342140

Epoch: 8
Loss: 0.18624149144703017
ROC train: 0.847781	val: 0.770247	test: 0.719379
PRC train: 0.433483	val: 0.333311	test: 0.352604

Epoch: 9
Loss: 0.1850741656445013
ROC train: 0.856437	val: 0.771490	test: 0.733206
PRC train: 0.461033	val: 0.336308	test: 0.346814

Epoch: 10
Loss: 0.18247320146833812
ROC train: 0.858119	val: 0.779618	test: 0.732626
PRC train: 0.465746	val: 0.347597	test: 0.336522

Epoch: 11
Loss: 0.17856531881705134
ROC train: 0.868174	val: 0.765489	test: 0.728608
PRC train: 0.491071	val: 0.334797	test: 0.349955

Epoch: 12
Loss: 0.17640894094441661
ROC train: 0.868226	val: 0.758776	test: 0.738365
PRC train: 0.494888	val: 0.337632	test: 0.360339

Epoch: 13
Loss: 0.17604577066865748
ROC train: 0.864295	val: 0.775868	test: 0.736656
PRC train: 0.486573	val: 0.353607	test: 0.371920

Epoch: 14
Loss: 0.17487124513440125
ROC train: 0.874459	val: 0.771117	test: 0.729970
PRC train: 0.522183	val: 0.344502	test: 0.356269

Epoch: 15
Loss: 0.1715751162648037
ROC train: 0.876286	val: 0.763728	test: 0.745869
PRC train: 0.518814	val: 0.359743	test: 0.375707

Epoch: 16
Loss: 0.1697940983713486
ROC train: 0.882757	val: 0.764906	test: 0.750490
PRC train: 0.538213	val: 0.352169	test: 0.377743

Epoch: 17
Loss: 0.16876229507042617
ROC train: 0.882342	val: 0.768991	test: 0.744826
PRC train: 0.543863	val: 0.347908	test: 0.354535

Epoch: 18
Loss: 0.16944497825358243
ROC train: 0.884000	val: 0.772280	test: 0.750698
PRC train: 0.545860	val: 0.356347	test: 0.374531

Epoch: 19
Loss: 0.16726743566139632
ROC train: 0.885338	val: 0.777306	test: 0.751880
PRC train: 0.553299	val: 0.355940	test: 0.368427

Epoch: 20
Loss: 0.1652361615474024
ROC train: 0.890703	val: 0.775486	test: 0.744157
PRC train: 0.562590	val: 0.353763	test: 0.370705

Epoch: 21
Loss: 0.16386353092809253
ROC train: 0.895591	val: 0.778997	test: 0.749521
PRC train: 0.576976	val: 0.365303	test: 0.368909

Epoch: 22
Loss: 0.1628000250172555
ROC train: 0.896296	val: 0.780816	test: 0.746942
PRC train: 0.580540	val: 0.349242	test: 0.373429

Epoch: 23
Loss: 0.16284506586010772
ROC train: 0.900996	val: 0.779311	test: 0.750248
PRC train: 0.590739	val: 0.359398	test: 0.369431

Epoch: 24
Loss: 0.16029628112220562
ROC train: 0.901972	val: 0.774417	test: 0.752515
PRC train: 0.592631	val: 0.355969	test: 0.355601

Epoch: 25
Loss: 0.15939376331692381
ROC train: 0.892509	val: 0.763308	test: 0.739250
PRC train: 0.552331	val: 0.327803	test: 0.347782

Epoch: 26
Loss: 0.15770535136500416
ROC train: 0.905614	val: 0.782374	test: 0.756265
PRC train: 0.605568	val: 0.350199	test: 0.350206

Epoch: 27
Loss: 0.15688987895633844
ROC train: 0.908317	val: 0.778162	test: 0.750695
PRC train: 0.603956	val: 0.357117	test: 0.360471

Epoch: 28
Loss: 0.15760418770155732
ROC train: 0.907085	val: 0.773321	test: 0.759693
PRC train: 0.599522	val: 0.335988	test: 0.365475

Epoch: 29
Loss: 0.15579337715291774
ROC train: 0.911594	val: 0.773873	test: 0.753065
PRC train: 0.617661	val: 0.338396	test: 0.356564

Epoch: 30
Loss: 0.15493111741449156
ROC train: 0.912324	val: 0.770052	test: 0.748169
PRC train: 0.619131	val: 0.350775	test: 0.344980

Epoch: 31
Loss: 0.15302936403095135
ROC train: 0.915126	val: 0.771795	test: 0.748489
PRC train: 0.626184	val: 0.343325	test: 0.352847

Epoch: 32
Loss: 0.153358576637929
ROC train: 0.916912	val: 0.776734	test: 0.753388
PRC train: 0.631648	val: 0.355665	test: 0.375642

Epoch: 33
Loss: 0.15244777908090942Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/tox21/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/tox21/noise=0.0/tox21_scaff_2_20-05_14-43-32  ]
[ Using Seed :  2  ]
[ Using device :  cuda:0  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5324206886537349
ROC train: 0.696608	val: 0.629900	test: 0.589304
PRC train: 0.216076	val: 0.207101	test: 0.190031

Epoch: 2
Loss: 0.322989689654865
ROC train: 0.752088	val: 0.700147	test: 0.648871
PRC train: 0.269686	val: 0.263628	test: 0.233678

Epoch: 3
Loss: 0.23826698705283517
ROC train: 0.790777	val: 0.733580	test: 0.676790
PRC train: 0.330220	val: 0.295364	test: 0.296372

Epoch: 4
Loss: 0.20926719588660084
ROC train: 0.800972	val: 0.753532	test: 0.701435
PRC train: 0.328697	val: 0.290540	test: 0.282473

Epoch: 5
Loss: 0.19892639237350654
ROC train: 0.826788	val: 0.760981	test: 0.718976
PRC train: 0.389505	val: 0.317300	test: 0.327662

Epoch: 6
Loss: 0.19399439941914035
ROC train: 0.833454	val: 0.740920	test: 0.694394
PRC train: 0.387905	val: 0.294881	test: 0.310970

Epoch: 7
Loss: 0.18879265192874078
ROC train: 0.845399	val: 0.769615	test: 0.723640
PRC train: 0.423108	val: 0.340663	test: 0.334182

Epoch: 8
Loss: 0.1862451256857641
ROC train: 0.855036	val: 0.772682	test: 0.723847
PRC train: 0.441241	val: 0.331654	test: 0.333863

Epoch: 9
Loss: 0.18168009099874866
ROC train: 0.854654	val: 0.774828	test: 0.726353
PRC train: 0.449765	val: 0.339025	test: 0.338173

Epoch: 10
Loss: 0.18121295579955035
ROC train: 0.863024	val: 0.771542	test: 0.711505
PRC train: 0.461799	val: 0.359895	test: 0.346183

Epoch: 11
Loss: 0.18062808557770216
ROC train: 0.871649	val: 0.774894	test: 0.725276
PRC train: 0.497001	val: 0.332365	test: 0.347340

Epoch: 12
Loss: 0.17565999746531663
ROC train: 0.874770	val: 0.772747	test: 0.735673
PRC train: 0.507032	val: 0.344719	test: 0.367624

Epoch: 13
Loss: 0.17572615511214684
ROC train: 0.875705	val: 0.781764	test: 0.732157
PRC train: 0.505507	val: 0.339986	test: 0.344743

Epoch: 14
Loss: 0.17507603307610228
ROC train: 0.880185	val: 0.773224	test: 0.734314
PRC train: 0.523158	val: 0.347640	test: 0.362570

Epoch: 15
Loss: 0.17060655333107544
ROC train: 0.879904	val: 0.769952	test: 0.722788
PRC train: 0.517957	val: 0.322959	test: 0.317765

Epoch: 16
Loss: 0.17122051247898562
ROC train: 0.885764	val: 0.774950	test: 0.724971
PRC train: 0.541524	val: 0.355512	test: 0.351499

Epoch: 17
Loss: 0.16853196174184082
ROC train: 0.885067	val: 0.780870	test: 0.733881
PRC train: 0.533776	val: 0.362555	test: 0.358721

Epoch: 18
Loss: 0.1666348734721528
ROC train: 0.890846	val: 0.777979	test: 0.734931
PRC train: 0.554649	val: 0.350759	test: 0.353686

Epoch: 19
Loss: 0.1665111895904534
ROC train: 0.893137	val: 0.785594	test: 0.735413
PRC train: 0.563992	val: 0.353410	test: 0.364175

Epoch: 20
Loss: 0.16480715580432195
ROC train: 0.896470	val: 0.773357	test: 0.727353
PRC train: 0.566170	val: 0.330962	test: 0.336650

Epoch: 21
Loss: 0.1637015130010766
ROC train: 0.900404	val: 0.772603	test: 0.729816
PRC train: 0.584386	val: 0.350355	test: 0.359128

Epoch: 22
Loss: 0.1623036288692587
ROC train: 0.899180	val: 0.782832	test: 0.735852
PRC train: 0.578973	val: 0.359757	test: 0.359212

Epoch: 23
Loss: 0.15956481507903936
ROC train: 0.898463	val: 0.775776	test: 0.736365
PRC train: 0.573874	val: 0.358059	test: 0.352403

Epoch: 24
Loss: 0.1588113863290318
ROC train: 0.903854	val: 0.775636	test: 0.730902
PRC train: 0.592418	val: 0.350473	test: 0.349238

Epoch: 25
Loss: 0.1589250138174064
ROC train: 0.904036	val: 0.780543	test: 0.733899
PRC train: 0.595255	val: 0.359328	test: 0.357407

Epoch: 26
Loss: 0.15718741981867435
ROC train: 0.903217	val: 0.782980	test: 0.729454
PRC train: 0.593278	val: 0.353683	test: 0.351187

Epoch: 27
Loss: 0.15613535650681235
ROC train: 0.909391	val: 0.779445	test: 0.742956
PRC train: 0.617468	val: 0.363217	test: 0.370919

Epoch: 28
Loss: 0.1548669359755233
ROC train: 0.913532	val: 0.777593	test: 0.740231
PRC train: 0.619524	val: 0.359019	test: 0.374373

Epoch: 29
Loss: 0.15600820506172056
ROC train: 0.911064	val: 0.777203	test: 0.736853
PRC train: 0.615041	val: 0.364350	test: 0.369532

Epoch: 30
Loss: 0.1540203460651471
ROC train: 0.913841	val: 0.780032	test: 0.737234
PRC train: 0.622403	val: 0.367208	test: 0.362453

Epoch: 31
Loss: 0.15336130381523716
ROC train: 0.914697	val: 0.780873	test: 0.736031
PRC train: 0.632306	val: 0.382400	test: 0.372485

Epoch: 32
Loss: 0.15173644191988395
ROC train: 0.908921	val: 0.767013	test: 0.731367
PRC train: 0.607787	val: 0.354060	test: 0.348067

Epoch: 33
Loss: 0.1507917130375561Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/tox21/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/tox21/noise=0.0/tox21_scaff_1_20-05_14-43-32  ]
[ Using Seed :  1  ]
[ Using device :  cuda:0  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5216657896503294
ROC train: 0.688508	val: 0.587899	test: 0.579676
PRC train: 0.200783	val: 0.179134	test: 0.176050

Epoch: 2
Loss: 0.31279427774576296
ROC train: 0.756524	val: 0.706024	test: 0.659989
PRC train: 0.270942	val: 0.262799	test: 0.242081

Epoch: 3
Loss: 0.23277617888543817
ROC train: 0.792749	val: 0.736464	test: 0.674651
PRC train: 0.316888	val: 0.281485	test: 0.277328

Epoch: 4
Loss: 0.20706250619164146
ROC train: 0.811704	val: 0.759954	test: 0.706772
PRC train: 0.354569	val: 0.317902	test: 0.295781

Epoch: 5
Loss: 0.19798975377958983
ROC train: 0.835224	val: 0.769609	test: 0.713495
PRC train: 0.376118	val: 0.328922	test: 0.308923

Epoch: 6
Loss: 0.1924385920310087
ROC train: 0.838122	val: 0.761577	test: 0.726575
PRC train: 0.392124	val: 0.323242	test: 0.319644

Epoch: 7
Loss: 0.18929245959468538
ROC train: 0.846358	val: 0.771841	test: 0.712790
PRC train: 0.418099	val: 0.335646	test: 0.315808

Epoch: 8
Loss: 0.18674470794476597
ROC train: 0.857263	val: 0.771969	test: 0.721223
PRC train: 0.453631	val: 0.338202	test: 0.337221

Epoch: 9
Loss: 0.18440267369102212
ROC train: 0.855993	val: 0.776710	test: 0.739915
PRC train: 0.448394	val: 0.332343	test: 0.334827

Epoch: 10
Loss: 0.18363947399407451
ROC train: 0.867321	val: 0.768905	test: 0.721370
PRC train: 0.472765	val: 0.322419	test: 0.323293

Epoch: 11
Loss: 0.17835667604997144
ROC train: 0.869321	val: 0.767924	test: 0.728273
PRC train: 0.485558	val: 0.330410	test: 0.346871

Epoch: 12
Loss: 0.17752120026670073
ROC train: 0.869353	val: 0.772342	test: 0.725148
PRC train: 0.491561	val: 0.348417	test: 0.334080

Epoch: 13
Loss: 0.17610618629565594
ROC train: 0.877445	val: 0.771151	test: 0.742769
PRC train: 0.517494	val: 0.352916	test: 0.353625

Epoch: 14
Loss: 0.1714713527758207
ROC train: 0.879405	val: 0.760720	test: 0.722955
PRC train: 0.510830	val: 0.328739	test: 0.335411

Epoch: 15
Loss: 0.1724887906432092
ROC train: 0.879138	val: 0.760963	test: 0.729079
PRC train: 0.509893	val: 0.331993	test: 0.346020

Epoch: 16
Loss: 0.1712723442337111
ROC train: 0.878223	val: 0.772872	test: 0.738731
PRC train: 0.517144	val: 0.341907	test: 0.337943

Epoch: 17
Loss: 0.17277669440967533
ROC train: 0.885711	val: 0.768702	test: 0.736593
PRC train: 0.543955	val: 0.353776	test: 0.352350

Epoch: 18
Loss: 0.1677264116643865
ROC train: 0.890702	val: 0.769590	test: 0.742873
PRC train: 0.552850	val: 0.351150	test: 0.358265

Epoch: 19
Loss: 0.16660457879809365
ROC train: 0.891473	val: 0.772512	test: 0.739252
PRC train: 0.563001	val: 0.352382	test: 0.350697

Epoch: 20
Loss: 0.16485073195931602
ROC train: 0.894089	val: 0.767605	test: 0.731533
PRC train: 0.560992	val: 0.344591	test: 0.342052

Epoch: 21
Loss: 0.16432714364322756
ROC train: 0.894942	val: 0.776198	test: 0.743670
PRC train: 0.571837	val: 0.362026	test: 0.354577

Epoch: 22
Loss: 0.1625081308314854
ROC train: 0.897266	val: 0.761980	test: 0.740396
PRC train: 0.574319	val: 0.347506	test: 0.351632

Epoch: 23
Loss: 0.16075910835710633
ROC train: 0.900131	val: 0.780250	test: 0.744098
PRC train: 0.587344	val: 0.361220	test: 0.358867

Epoch: 24
Loss: 0.15974040363189945
ROC train: 0.901860	val: 0.764426	test: 0.735104
PRC train: 0.586707	val: 0.351000	test: 0.351459

Epoch: 25
Loss: 0.15952171905099116
ROC train: 0.905231	val: 0.778852	test: 0.742223
PRC train: 0.596631	val: 0.367854	test: 0.367362

Epoch: 26
Loss: 0.15854711474842323
ROC train: 0.902265	val: 0.774942	test: 0.736066
PRC train: 0.586965	val: 0.377979	test: 0.363773

Epoch: 27
Loss: 0.1579255230269632
ROC train: 0.908249	val: 0.774727	test: 0.749476
PRC train: 0.600395	val: 0.358336	test: 0.373936

Epoch: 28
Loss: 0.1563325710053457
ROC train: 0.911419	val: 0.771066	test: 0.745038
PRC train: 0.612647	val: 0.365196	test: 0.356668

Epoch: 29
Loss: 0.15508395607685657
ROC train: 0.913192	val: 0.773532	test: 0.748974
PRC train: 0.620946	val: 0.375561	test: 0.352311

Epoch: 30
Loss: 0.15422016440436936
ROC train: 0.912862	val: 0.768656	test: 0.734178
PRC train: 0.616076	val: 0.358944	test: 0.350631

Epoch: 31
Loss: 0.15385633067504143
ROC train: 0.914335	val: 0.778136	test: 0.743550
PRC train: 0.617718	val: 0.363729	test: 0.351729

Epoch: 32
Loss: 0.1520195027250159
ROC train: 0.917760	val: 0.773045	test: 0.744946
PRC train: 0.638668	val: 0.363261	test: 0.358655

Epoch: 33
Loss: 0.152522673776606Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/tox21/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/tox21/noise=0.05/tox21_scaff_3_20-05_14-43-32  ]
[ Using Seed :  3  ]
[ Using device :  cuda:1  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.531409310477262
ROC train: 0.692295	val: 0.641308	test: 0.608390
PRC train: 0.203162	val: 0.212253	test: 0.189373

Epoch: 2
Loss: 0.3228969113708462
ROC train: 0.753666	val: 0.727365	test: 0.665743
PRC train: 0.275796	val: 0.298874	test: 0.261034

Epoch: 3
Loss: 0.23829700229057077
ROC train: 0.779211	val: 0.717016	test: 0.673487
PRC train: 0.295579	val: 0.275906	test: 0.268851

Epoch: 4
Loss: 0.21288462469859998
ROC train: 0.799347	val: 0.741200	test: 0.687867
PRC train: 0.332776	val: 0.299462	test: 0.303591

Epoch: 5
Loss: 0.2041218341553002
ROC train: 0.811944	val: 0.745688	test: 0.679081
PRC train: 0.353424	val: 0.312492	test: 0.292958

Epoch: 6
Loss: 0.19636129310702752
ROC train: 0.815741	val: 0.751065	test: 0.699063
PRC train: 0.363245	val: 0.307389	test: 0.288642

Epoch: 7
Loss: 0.1963796358276972
ROC train: 0.830700	val: 0.734921	test: 0.693091
PRC train: 0.393752	val: 0.306313	test: 0.297396

Epoch: 8
Loss: 0.1917282772971951
ROC train: 0.837904	val: 0.747667	test: 0.705606
PRC train: 0.407890	val: 0.307917	test: 0.300074

Epoch: 9
Loss: 0.18732457119038382
ROC train: 0.850013	val: 0.739842	test: 0.722864
PRC train: 0.434183	val: 0.310111	test: 0.319084

Epoch: 10
Loss: 0.18704973178846285
ROC train: 0.853554	val: 0.747862	test: 0.708157
PRC train: 0.439987	val: 0.326321	test: 0.324956

Epoch: 11
Loss: 0.18558618040956287
ROC train: 0.856642	val: 0.761996	test: 0.729084
PRC train: 0.444663	val: 0.325873	test: 0.328797

Epoch: 12
Loss: 0.18235749134207752
ROC train: 0.864089	val: 0.761770	test: 0.725833
PRC train: 0.470606	val: 0.332663	test: 0.337300

Epoch: 13
Loss: 0.18176348254509425
ROC train: 0.864396	val: 0.759139	test: 0.720725
PRC train: 0.471742	val: 0.333140	test: 0.332034

Epoch: 14
Loss: 0.17727467248089265
ROC train: 0.872317	val: 0.755485	test: 0.727908
PRC train: 0.492613	val: 0.338158	test: 0.329866

Epoch: 15
Loss: 0.17622694203579808
ROC train: 0.879410	val: 0.760048	test: 0.731706
PRC train: 0.507458	val: 0.342190	test: 0.342484

Epoch: 16
Loss: 0.17597164528396142
ROC train: 0.883564	val: 0.749266	test: 0.723773
PRC train: 0.522748	val: 0.344755	test: 0.334759

Epoch: 17
Loss: 0.17308330012537365
ROC train: 0.885300	val: 0.752135	test: 0.717937
PRC train: 0.528955	val: 0.345933	test: 0.335788

Epoch: 18
Loss: 0.17518474645299187
ROC train: 0.888666	val: 0.754380	test: 0.731436
PRC train: 0.538807	val: 0.338065	test: 0.336862

Epoch: 19
Loss: 0.16948570623507492
ROC train: 0.891062	val: 0.747928	test: 0.726442
PRC train: 0.544334	val: 0.323274	test: 0.345776

Epoch: 20
Loss: 0.16926332501359004
ROC train: 0.896879	val: 0.755429	test: 0.721864
PRC train: 0.560243	val: 0.330265	test: 0.341052

Epoch: 21
Loss: 0.1668263698032774
ROC train: 0.900924	val: 0.757603	test: 0.721868
PRC train: 0.565638	val: 0.332329	test: 0.351912

Epoch: 22
Loss: 0.16454269918946404
ROC train: 0.903785	val: 0.755928	test: 0.725638
PRC train: 0.580166	val: 0.337049	test: 0.354296

Epoch: 23
Loss: 0.1641579782470237
ROC train: 0.904008	val: 0.753536	test: 0.721822
PRC train: 0.579750	val: 0.346579	test: 0.344067

Epoch: 24
Loss: 0.16308788263865634
ROC train: 0.910093	val: 0.760525	test: 0.737933
PRC train: 0.595517	val: 0.352259	test: 0.350624

Epoch: 25
Loss: 0.1618076661962102
ROC train: 0.907730	val: 0.750883	test: 0.730930
PRC train: 0.595260	val: 0.357040	test: 0.342369

Epoch: 26
Loss: 0.1599856380839458
ROC train: 0.910928	val: 0.761094	test: 0.719051
PRC train: 0.602838	val: 0.355235	test: 0.335776

Epoch: 27
Loss: 0.15852207021865433
ROC train: 0.915557	val: 0.754400	test: 0.730881
PRC train: 0.608925	val: 0.343981	test: 0.347028

Epoch: 28
Loss: 0.15783373598105224
ROC train: 0.916119	val: 0.750100	test: 0.735165
PRC train: 0.612796	val: 0.331386	test: 0.338896

Epoch: 29
Loss: 0.15613501793160878
ROC train: 0.919272	val: 0.752000	test: 0.726616
PRC train: 0.615298	val: 0.335552	test: 0.340976

Epoch: 30
Loss: 0.15440331680021563
ROC train: 0.923447	val: 0.756719	test: 0.731454
PRC train: 0.634705	val: 0.355271	test: 0.343944

Epoch: 31
Loss: 0.15260455028662556
ROC train: 0.919210	val: 0.761241	test: 0.730990
PRC train: 0.617929	val: 0.326491	test: 0.331666

Epoch: 32
Loss: 0.1522664432177474
ROC train: 0.927418	val: 0.759199	test: 0.727083Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/tox21/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/tox21/noise=0.05/tox21_scaff_1_20-05_14-43-32  ]
[ Using Seed :  1  ]
[ Using device :  cuda:1  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5427900398243137
ROC train: 0.700479	val: 0.650479	test: 0.622432
PRC train: 0.205602	val: 0.201404	test: 0.185925

Epoch: 2
Loss: 0.32751714981013735
ROC train: 0.734864	val: 0.697839	test: 0.637206
PRC train: 0.241953	val: 0.266767	test: 0.240883

Epoch: 3
Loss: 0.24100740314478653
ROC train: 0.768106	val: 0.738951	test: 0.663650
PRC train: 0.292166	val: 0.304896	test: 0.287119

Epoch: 4
Loss: 0.21428715668364692
ROC train: 0.788439	val: 0.749555	test: 0.687913
PRC train: 0.317766	val: 0.310474	test: 0.286082

Epoch: 5
Loss: 0.20580046773139515
ROC train: 0.806939	val: 0.751209	test: 0.692372
PRC train: 0.345610	val: 0.332724	test: 0.294112

Epoch: 6
Loss: 0.19846750663078427
ROC train: 0.813157	val: 0.736293	test: 0.685203
PRC train: 0.348024	val: 0.286653	test: 0.304308

Epoch: 7
Loss: 0.19544759893313438
ROC train: 0.830227	val: 0.767823	test: 0.700735
PRC train: 0.387775	val: 0.327083	test: 0.302784

Epoch: 8
Loss: 0.19263572072607857
ROC train: 0.841035	val: 0.762987	test: 0.714962
PRC train: 0.401846	val: 0.332031	test: 0.307236

Epoch: 9
Loss: 0.19079476263793868
ROC train: 0.846823	val: 0.767009	test: 0.706942
PRC train: 0.419759	val: 0.316862	test: 0.316793

Epoch: 10
Loss: 0.18707879480121725
ROC train: 0.845707	val: 0.766096	test: 0.713122
PRC train: 0.419963	val: 0.328288	test: 0.311611

Epoch: 11
Loss: 0.18620794742360705
ROC train: 0.858209	val: 0.759874	test: 0.713682
PRC train: 0.447046	val: 0.323027	test: 0.316965

Epoch: 12
Loss: 0.1836360107189239
ROC train: 0.857142	val: 0.765416	test: 0.717317
PRC train: 0.453254	val: 0.324997	test: 0.320994

Epoch: 13
Loss: 0.1796394800945914
ROC train: 0.870408	val: 0.767674	test: 0.718541
PRC train: 0.484906	val: 0.337370	test: 0.332999

Epoch: 14
Loss: 0.17838103058237642
ROC train: 0.872249	val: 0.777743	test: 0.718810
PRC train: 0.483873	val: 0.343195	test: 0.325848

Epoch: 15
Loss: 0.17717087542173107
ROC train: 0.870820	val: 0.772900	test: 0.726692
PRC train: 0.493373	val: 0.353005	test: 0.336613

Epoch: 16
Loss: 0.17522819325130004
ROC train: 0.871979	val: 0.772569	test: 0.721702
PRC train: 0.494147	val: 0.337652	test: 0.322535

Epoch: 17
Loss: 0.17383453135781834
ROC train: 0.879551	val: 0.767133	test: 0.719275
PRC train: 0.507824	val: 0.349289	test: 0.340521

Epoch: 18
Loss: 0.17445754628621213
ROC train: 0.885236	val: 0.766087	test: 0.728273
PRC train: 0.532318	val: 0.331138	test: 0.340994

Epoch: 19
Loss: 0.1705554864332537
ROC train: 0.888326	val: 0.765053	test: 0.722563
PRC train: 0.533966	val: 0.330930	test: 0.324835

Epoch: 20
Loss: 0.1694687717501582
ROC train: 0.883832	val: 0.769559	test: 0.738373
PRC train: 0.531244	val: 0.354195	test: 0.340084

Epoch: 21
Loss: 0.16723100618538914
ROC train: 0.890796	val: 0.772145	test: 0.728223
PRC train: 0.550949	val: 0.349235	test: 0.336931

Epoch: 22
Loss: 0.16650361716884865
ROC train: 0.894975	val: 0.761831	test: 0.733117
PRC train: 0.542417	val: 0.332302	test: 0.321529

Epoch: 23
Loss: 0.16629313256858314
ROC train: 0.891168	val: 0.771520	test: 0.732980
PRC train: 0.550600	val: 0.356572	test: 0.337690

Epoch: 24
Loss: 0.16594902787171562
ROC train: 0.897735	val: 0.771492	test: 0.730690
PRC train: 0.566436	val: 0.356134	test: 0.339912

Epoch: 25
Loss: 0.1630618833487429
ROC train: 0.905952	val: 0.770406	test: 0.726821
PRC train: 0.590754	val: 0.337722	test: 0.324509

Epoch: 26
Loss: 0.16154583084121346
ROC train: 0.906724	val: 0.770954	test: 0.726192
PRC train: 0.592229	val: 0.336452	test: 0.323594

Epoch: 27
Loss: 0.15993592355382943
ROC train: 0.909206	val: 0.763794	test: 0.723199
PRC train: 0.601564	val: 0.327379	test: 0.321904

Epoch: 28
Loss: 0.15905074163570332
ROC train: 0.911061	val: 0.775641	test: 0.733962
PRC train: 0.615064	val: 0.363552	test: 0.341704

Epoch: 29
Loss: 0.15786730252728448
ROC train: 0.916560	val: 0.778455	test: 0.729559
PRC train: 0.620321	val: 0.359671	test: 0.340084

Epoch: 30
Loss: 0.15706299482208722
ROC train: 0.913259	val: 0.775850	test: 0.730913
PRC train: 0.620655	val: 0.338877	test: 0.333864

Epoch: 31
Loss: 0.15647024194257037
ROC train: 0.918690	val: 0.779069	test: 0.735304
PRC train: 0.635693	val: 0.345964	test: 0.346144

Epoch: 32
Loss: 0.15429068772191182
ROC train: 0.920266	val: 0.777624	test: 0.734547Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/tox21/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/tox21/noise=0.05/tox21_scaff_2_20-05_14-43-32  ]
[ Using Seed :  2  ]
[ Using device :  cuda:1  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.552070292317252
ROC train: 0.679689	val: 0.616779	test: 0.584605
PRC train: 0.195622	val: 0.188892	test: 0.174862

Epoch: 2
Loss: 0.33219455384755664
ROC train: 0.736273	val: 0.686020	test: 0.647200
PRC train: 0.251205	val: 0.247665	test: 0.219540

Epoch: 3
Loss: 0.24442807670196082
ROC train: 0.770510	val: 0.730448	test: 0.680104
PRC train: 0.304784	val: 0.281381	test: 0.268724

Epoch: 4
Loss: 0.2143939986553544
ROC train: 0.787749	val: 0.742220	test: 0.689459
PRC train: 0.333716	val: 0.293079	test: 0.284672

Epoch: 5
Loss: 0.2059509652473643
ROC train: 0.807919	val: 0.749624	test: 0.691640
PRC train: 0.353167	val: 0.307787	test: 0.295286

Epoch: 6
Loss: 0.1998399944160525
ROC train: 0.819561	val: 0.760912	test: 0.707129
PRC train: 0.371050	val: 0.335995	test: 0.308234

Epoch: 7
Loss: 0.195053388315384
ROC train: 0.832850	val: 0.757527	test: 0.730393
PRC train: 0.396771	val: 0.332558	test: 0.324606

Epoch: 8
Loss: 0.1928379254322079
ROC train: 0.839111	val: 0.771316	test: 0.730307
PRC train: 0.417823	val: 0.314592	test: 0.313915

Epoch: 9
Loss: 0.1899749963254406
ROC train: 0.845068	val: 0.765413	test: 0.726514
PRC train: 0.428379	val: 0.342216	test: 0.326684

Epoch: 10
Loss: 0.18746270100059545
ROC train: 0.849873	val: 0.758639	test: 0.712177
PRC train: 0.434989	val: 0.312678	test: 0.323106

Epoch: 11
Loss: 0.1852889310703443
ROC train: 0.858680	val: 0.764190	test: 0.726006
PRC train: 0.456902	val: 0.333868	test: 0.337151

Epoch: 12
Loss: 0.1822734239024585
ROC train: 0.863102	val: 0.768276	test: 0.726900
PRC train: 0.471770	val: 0.339618	test: 0.351806

Epoch: 13
Loss: 0.18466680876481384
ROC train: 0.866076	val: 0.768383	test: 0.737559
PRC train: 0.478535	val: 0.312486	test: 0.318013

Epoch: 14
Loss: 0.17882219387952056
ROC train: 0.868930	val: 0.767424	test: 0.745461
PRC train: 0.486395	val: 0.352559	test: 0.354798

Epoch: 15
Loss: 0.17819339282687413
ROC train: 0.873329	val: 0.769112	test: 0.733231
PRC train: 0.508063	val: 0.357928	test: 0.353574

Epoch: 16
Loss: 0.17372441168638864
ROC train: 0.882291	val: 0.762898	test: 0.739398
PRC train: 0.522640	val: 0.342234	test: 0.356557

Epoch: 17
Loss: 0.1722233318768538
ROC train: 0.884552	val: 0.766759	test: 0.728509
PRC train: 0.540414	val: 0.340458	test: 0.352849

Epoch: 18
Loss: 0.17135615157945885
ROC train: 0.886603	val: 0.763679	test: 0.737812
PRC train: 0.538617	val: 0.328033	test: 0.362081

Epoch: 19
Loss: 0.16887851411748955
ROC train: 0.892308	val: 0.777444	test: 0.742750
PRC train: 0.563270	val: 0.354805	test: 0.366444

Epoch: 20
Loss: 0.16972148363946724
ROC train: 0.893514	val: 0.755497	test: 0.726277
PRC train: 0.556730	val: 0.339608	test: 0.363061

Epoch: 21
Loss: 0.16916407690595572
ROC train: 0.896735	val: 0.773886	test: 0.734738
PRC train: 0.571358	val: 0.354257	test: 0.365883

Epoch: 22
Loss: 0.16698338279526115
ROC train: 0.901565	val: 0.763453	test: 0.734012
PRC train: 0.583483	val: 0.356161	test: 0.358742

Epoch: 23
Loss: 0.1647066918214096
ROC train: 0.899825	val: 0.763967	test: 0.719267
PRC train: 0.571116	val: 0.349935	test: 0.345715

Epoch: 24
Loss: 0.1651688212394347
ROC train: 0.902613	val: 0.769241	test: 0.733830
PRC train: 0.582465	val: 0.351260	test: 0.355149

Epoch: 25
Loss: 0.16240870198585566
ROC train: 0.907818	val: 0.763142	test: 0.715273
PRC train: 0.601321	val: 0.339761	test: 0.350205

Epoch: 26
Loss: 0.16144608822177964
ROC train: 0.907153	val: 0.765138	test: 0.739664
PRC train: 0.603212	val: 0.334316	test: 0.346245

Epoch: 27
Loss: 0.16135749633597862
ROC train: 0.912313	val: 0.770027	test: 0.733227
PRC train: 0.614846	val: 0.355171	test: 0.367414

Epoch: 28
Loss: 0.1579087978959794
ROC train: 0.915337	val: 0.771247	test: 0.726097
PRC train: 0.619265	val: 0.350634	test: 0.360275

Epoch: 29
Loss: 0.15831074939444711
ROC train: 0.916870	val: 0.770395	test: 0.747724
PRC train: 0.626182	val: 0.350252	test: 0.374362

Epoch: 30
Loss: 0.15537970224860564
ROC train: 0.916947	val: 0.768547	test: 0.739439
PRC train: 0.628204	val: 0.332411	test: 0.356913

Epoch: 31
Loss: 0.15460389893758963
ROC train: 0.921805	val: 0.778215	test: 0.734065
PRC train: 0.641438	val: 0.352210	test: 0.375120

Epoch: 32
Loss: 0.15380832602561992
ROC train: 0.923718	val: 0.763505	test: 0.733387Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:3
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/tox21/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/tox21/noise=0.2/tox21_scaff_3_20-05_14-43-32  ]
[ Using Seed :  3  ]
[ Using device :  cuda:3  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5402378284361854
ROC train: 0.668122	val: 0.645055	test: 0.623854
PRC train: 0.166974	val: 0.195495	test: 0.172847

Epoch: 2
Loss: 0.3278538489149742
ROC train: 0.715834	val: 0.687065	test: 0.670737
PRC train: 0.211799	val: 0.230847	test: 0.216402

Epoch: 3
Loss: 0.24480540385151994
ROC train: 0.735382	val: 0.708189	test: 0.667846
PRC train: 0.234238	val: 0.249635	test: 0.226368

Epoch: 4
Loss: 0.2209701430094705
ROC train: 0.751150	val: 0.705012	test: 0.676286
PRC train: 0.267486	val: 0.268060	test: 0.234172

Epoch: 5
Loss: 0.21539512586419626
ROC train: 0.767054	val: 0.714793	test: 0.674218
PRC train: 0.293586	val: 0.269245	test: 0.239862

Epoch: 6
Loss: 0.20943326296261933
ROC train: 0.773718	val: 0.728224	test: 0.678074
PRC train: 0.295843	val: 0.278046	test: 0.248518

Epoch: 7
Loss: 0.20991593241785314
ROC train: 0.796507	val: 0.723488	test: 0.689665
PRC train: 0.330821	val: 0.292114	test: 0.267382

Epoch: 8
Loss: 0.204958606082936
ROC train: 0.798044	val: 0.747960	test: 0.696064
PRC train: 0.333303	val: 0.293779	test: 0.268271

Epoch: 9
Loss: 0.20374129612878084
ROC train: 0.809999	val: 0.719632	test: 0.683331
PRC train: 0.347457	val: 0.277629	test: 0.270182

Epoch: 10
Loss: 0.19991845592413565
ROC train: 0.814647	val: 0.738810	test: 0.693416
PRC train: 0.347809	val: 0.299978	test: 0.278402

Epoch: 11
Loss: 0.20094565361786582
ROC train: 0.822140	val: 0.732161	test: 0.692721
PRC train: 0.367073	val: 0.305915	test: 0.273071

Epoch: 12
Loss: 0.19811883479555387
ROC train: 0.821313	val: 0.743909	test: 0.724650
PRC train: 0.375700	val: 0.309585	test: 0.268444

Epoch: 13
Loss: 0.1963583110550741
ROC train: 0.822006	val: 0.744029	test: 0.712344
PRC train: 0.372392	val: 0.318252	test: 0.267121

Epoch: 14
Loss: 0.19285784392608135
ROC train: 0.840458	val: 0.728329	test: 0.700185
PRC train: 0.415567	val: 0.308103	test: 0.257909

Epoch: 15
Loss: 0.19113396929149154
ROC train: 0.850751	val: 0.712030	test: 0.698537
PRC train: 0.429561	val: 0.290213	test: 0.267963

Epoch: 16
Loss: 0.1895465660498419
ROC train: 0.860951	val: 0.713801	test: 0.679079
PRC train: 0.442096	val: 0.311981	test: 0.258181

Epoch: 17
Loss: 0.1885998145416166
ROC train: 0.865714	val: 0.720632	test: 0.686011
PRC train: 0.461124	val: 0.319581	test: 0.283808

Epoch: 18
Loss: 0.18806332025536515
ROC train: 0.863432	val: 0.717609	test: 0.681640
PRC train: 0.453723	val: 0.306185	test: 0.247434

Epoch: 19
Loss: 0.1844107411588422
ROC train: 0.869109	val: 0.716055	test: 0.684744
PRC train: 0.464478	val: 0.305324	test: 0.266162

Epoch: 20
Loss: 0.1820716337698953
ROC train: 0.877975	val: 0.689018	test: 0.650965
PRC train: 0.483297	val: 0.275889	test: 0.252708

Epoch: 21
Loss: 0.18041027538645094
ROC train: 0.882546	val: 0.710274	test: 0.675490
PRC train: 0.510612	val: 0.277094	test: 0.253008

Epoch: 22
Loss: 0.17826507559983976
ROC train: 0.887169	val: 0.716543	test: 0.678964
PRC train: 0.517912	val: 0.310934	test: 0.266347

Epoch: 23
Loss: 0.17682800238538998
ROC train: 0.884817	val: 0.721292	test: 0.682929
PRC train: 0.513993	val: 0.301481	test: 0.245125

Epoch: 24
Loss: 0.17641902859787187
ROC train: 0.892535	val: 0.691294	test: 0.671713
PRC train: 0.533613	val: 0.287708	test: 0.256197

Epoch: 25
Loss: 0.1737629149692492
ROC train: 0.891811	val: 0.719934	test: 0.696606
PRC train: 0.534335	val: 0.317837	test: 0.276446

Epoch: 26
Loss: 0.17289719130305756
ROC train: 0.898034	val: 0.683249	test: 0.646804
PRC train: 0.554522	val: 0.287747	test: 0.250112

Epoch: 27
Loss: 0.17050484954716832
ROC train: 0.897521	val: 0.718027	test: 0.678541
PRC train: 0.542214	val: 0.296393	test: 0.257563

Epoch: 28
Loss: 0.1666886491099594
ROC train: 0.910956	val: 0.678980	test: 0.646472
PRC train: 0.584296	val: 0.266697	test: 0.236106

Epoch: 29
Loss: 0.16833233643902845
ROC train: 0.911337	val: 0.712538	test: 0.664820
PRC train: 0.592115	val: 0.286670	test: 0.235561

Epoch: 30
Loss: 0.165702329915514
ROC train: 0.910495	val: 0.694688	test: 0.664701
PRC train: 0.582670	val: 0.280863	test: 0.250438

Epoch: 31
Loss: 0.16448183516724638
ROC train: 0.915384	val: 0.686368	test: 0.667472
PRC train: 0.605871	val: 0.266310	test: 0.256943

Epoch: 32
Loss: 0.1637357534022778
ROC train: 0.916809	val: 0.698708	test: 0.656501Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:3
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/tox21/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/tox21/noise=0.2/tox21_scaff_2_20-05_14-43-32  ]
[ Using Seed :  2  ]
[ Using device :  cuda:3  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5555567019279928
ROC train: 0.678249	val: 0.655816	test: 0.614698
PRC train: 0.178443	val: 0.200536	test: 0.186313

Epoch: 2
Loss: 0.33362155985679004
ROC train: 0.718676	val: 0.697821	test: 0.647116
PRC train: 0.219683	val: 0.230881	test: 0.205327

Epoch: 3
Loss: 0.2481934053651759
ROC train: 0.739504	val: 0.710618	test: 0.657143
PRC train: 0.247536	val: 0.276779	test: 0.222919

Epoch: 4
Loss: 0.22146551077910878
ROC train: 0.756087	val: 0.721196	test: 0.662555
PRC train: 0.268217	val: 0.276840	test: 0.249305

Epoch: 5
Loss: 0.21475053945725758
ROC train: 0.768750	val: 0.730258	test: 0.663067
PRC train: 0.285605	val: 0.275149	test: 0.247565

Epoch: 6
Loss: 0.20970262318513888
ROC train: 0.779667	val: 0.742948	test: 0.694017
PRC train: 0.304197	val: 0.310642	test: 0.273293

Epoch: 7
Loss: 0.2067151948989412
ROC train: 0.797014	val: 0.731143	test: 0.676930
PRC train: 0.325989	val: 0.307727	test: 0.272852

Epoch: 8
Loss: 0.20392451621701477
ROC train: 0.802280	val: 0.747618	test: 0.690445
PRC train: 0.336815	val: 0.329964	test: 0.286705

Epoch: 9
Loss: 0.201105822566183
ROC train: 0.811613	val: 0.729870	test: 0.684778
PRC train: 0.344186	val: 0.317406	test: 0.299195

Epoch: 10
Loss: 0.20099005470490894
ROC train: 0.826996	val: 0.736726	test: 0.697697
PRC train: 0.371161	val: 0.329034	test: 0.296945

Epoch: 11
Loss: 0.19729962029777887
ROC train: 0.830211	val: 0.739147	test: 0.698080
PRC train: 0.374510	val: 0.320246	test: 0.287420

Epoch: 12
Loss: 0.1955185396183702
ROC train: 0.836134	val: 0.739923	test: 0.707373
PRC train: 0.393378	val: 0.322694	test: 0.293452

Epoch: 13
Loss: 0.19611130331053403
ROC train: 0.845537	val: 0.732418	test: 0.690435
PRC train: 0.408095	val: 0.307467	test: 0.287653

Epoch: 14
Loss: 0.19096481977090057
ROC train: 0.847770	val: 0.744786	test: 0.713297
PRC train: 0.414716	val: 0.331583	test: 0.303288

Epoch: 15
Loss: 0.18962439306663176
ROC train: 0.853215	val: 0.746682	test: 0.697784
PRC train: 0.438939	val: 0.333064	test: 0.290301

Epoch: 16
Loss: 0.1881572093693919
ROC train: 0.859534	val: 0.734871	test: 0.694708
PRC train: 0.457113	val: 0.326857	test: 0.294556

Epoch: 17
Loss: 0.18594536585080224
ROC train: 0.867709	val: 0.743395	test: 0.692549
PRC train: 0.461390	val: 0.312102	test: 0.289802

Epoch: 18
Loss: 0.18365010131119316
ROC train: 0.872433	val: 0.733593	test: 0.682093
PRC train: 0.473588	val: 0.295438	test: 0.277662

Epoch: 19
Loss: 0.18120663261539494
ROC train: 0.874214	val: 0.730853	test: 0.692497
PRC train: 0.483963	val: 0.316809	test: 0.285331

Epoch: 20
Loss: 0.18176106577726978
ROC train: 0.879364	val: 0.732416	test: 0.692060
PRC train: 0.492058	val: 0.312682	test: 0.286368

Epoch: 21
Loss: 0.1796035665021146
ROC train: 0.884042	val: 0.746060	test: 0.702048
PRC train: 0.503003	val: 0.319230	test: 0.292918

Epoch: 22
Loss: 0.17832366504118713
ROC train: 0.888583	val: 0.733402	test: 0.694084
PRC train: 0.519851	val: 0.319979	test: 0.291051

Epoch: 23
Loss: 0.17574098686889947
ROC train: 0.894344	val: 0.727574	test: 0.690200
PRC train: 0.538450	val: 0.314969	test: 0.284328

Epoch: 24
Loss: 0.1754197330672854
ROC train: 0.894131	val: 0.744153	test: 0.709215
PRC train: 0.531600	val: 0.337553	test: 0.300828

Epoch: 25
Loss: 0.17465413605129537
ROC train: 0.901571	val: 0.744207	test: 0.690876
PRC train: 0.557344	val: 0.322564	test: 0.290297

Epoch: 26
Loss: 0.17100378332073768
ROC train: 0.903308	val: 0.737655	test: 0.698819
PRC train: 0.561563	val: 0.322767	test: 0.286122

Epoch: 27
Loss: 0.16911956195689207
ROC train: 0.904660	val: 0.754683	test: 0.710889
PRC train: 0.558577	val: 0.337369	test: 0.305464

Epoch: 28
Loss: 0.16867136484687886
ROC train: 0.910611	val: 0.726586	test: 0.693450
PRC train: 0.591567	val: 0.314612	test: 0.290865

Epoch: 29
Loss: 0.1659945891905052
ROC train: 0.913692	val: 0.735328	test: 0.704921
PRC train: 0.598246	val: 0.321110	test: 0.306971

Epoch: 30
Loss: 0.16482980193206373
ROC train: 0.914724	val: 0.726676	test: 0.704506
PRC train: 0.601287	val: 0.315325	test: 0.298908

Epoch: 31
Loss: 0.1646684572888196
ROC train: 0.919650	val: 0.747138	test: 0.712613
PRC train: 0.618725	val: 0.335698	test: 0.306709

Epoch: 32
Loss: 0.16283135584814523
ROC train: 0.918667	val: 0.742172	test: 0.701856Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/tox21/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/tox21/noise=0.1/tox21_scaff_3_20-05_14-43-32  ]
[ Using Seed :  3  ]
[ Using device :  cuda:2  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5341051514469924
ROC train: 0.679344	val: 0.635206	test: 0.617104
PRC train: 0.187806	val: 0.194020	test: 0.174757

Epoch: 2
Loss: 0.3267085224217648
ROC train: 0.725984	val: 0.690381	test: 0.639053
PRC train: 0.235656	val: 0.256082	test: 0.224421

Epoch: 3
Loss: 0.24260196902120465
ROC train: 0.765151	val: 0.705040	test: 0.642723
PRC train: 0.269224	val: 0.259042	test: 0.244581

Epoch: 4
Loss: 0.21632130467201385
ROC train: 0.789192	val: 0.746320	test: 0.679939
PRC train: 0.311586	val: 0.296305	test: 0.285376

Epoch: 5
Loss: 0.20851845019346293
ROC train: 0.799841	val: 0.746951	test: 0.696341
PRC train: 0.329788	val: 0.311139	test: 0.277312

Epoch: 6
Loss: 0.20184193120106222
ROC train: 0.807534	val: 0.751995	test: 0.702659
PRC train: 0.330881	val: 0.302062	test: 0.279855

Epoch: 7
Loss: 0.2010934034472186
ROC train: 0.826712	val: 0.738977	test: 0.696476
PRC train: 0.369180	val: 0.313684	test: 0.309973

Epoch: 8
Loss: 0.19642381659464248
ROC train: 0.827394	val: 0.750833	test: 0.708603
PRC train: 0.379975	val: 0.321184	test: 0.308779

Epoch: 9
Loss: 0.19374082381079383
ROC train: 0.836084	val: 0.729358	test: 0.697579
PRC train: 0.390817	val: 0.308092	test: 0.306154

Epoch: 10
Loss: 0.19212091717342697
ROC train: 0.842516	val: 0.746240	test: 0.710877
PRC train: 0.408580	val: 0.319840	test: 0.313474

Epoch: 11
Loss: 0.18981099017795441
ROC train: 0.848481	val: 0.757139	test: 0.709563
PRC train: 0.417951	val: 0.326358	test: 0.300685

Epoch: 12
Loss: 0.1885973116357243
ROC train: 0.847118	val: 0.767024	test: 0.733479
PRC train: 0.419523	val: 0.342073	test: 0.320125

Epoch: 13
Loss: 0.1860363270982229
ROC train: 0.857384	val: 0.760754	test: 0.724493
PRC train: 0.444378	val: 0.346250	test: 0.316662

Epoch: 14
Loss: 0.18214053122101975
ROC train: 0.867196	val: 0.754129	test: 0.713254
PRC train: 0.467002	val: 0.342872	test: 0.313098

Epoch: 15
Loss: 0.1819333204668054
ROC train: 0.866758	val: 0.741773	test: 0.717642
PRC train: 0.464012	val: 0.312745	test: 0.319600

Epoch: 16
Loss: 0.18049589165538024
ROC train: 0.873952	val: 0.755281	test: 0.713480
PRC train: 0.470732	val: 0.341496	test: 0.308244

Epoch: 17
Loss: 0.17887229382832687
ROC train: 0.878539	val: 0.751895	test: 0.709623
PRC train: 0.499671	val: 0.332362	test: 0.318608

Epoch: 18
Loss: 0.1783957581885773
ROC train: 0.881337	val: 0.744326	test: 0.718850
PRC train: 0.502523	val: 0.338904	test: 0.309159

Epoch: 19
Loss: 0.17549336654054412
ROC train: 0.886713	val: 0.760728	test: 0.718062
PRC train: 0.507122	val: 0.327683	test: 0.325293

Epoch: 20
Loss: 0.17318500080468924
ROC train: 0.891293	val: 0.754734	test: 0.717528
PRC train: 0.532238	val: 0.336740	test: 0.315905

Epoch: 21
Loss: 0.17189478745761416
ROC train: 0.894539	val: 0.753680	test: 0.716774
PRC train: 0.541230	val: 0.340385	test: 0.326265

Epoch: 22
Loss: 0.16884713307140345
ROC train: 0.897851	val: 0.749269	test: 0.716203
PRC train: 0.554645	val: 0.341903	test: 0.327053

Epoch: 23
Loss: 0.16773206599967616
ROC train: 0.899812	val: 0.755649	test: 0.720300
PRC train: 0.556819	val: 0.343395	test: 0.324787

Epoch: 24
Loss: 0.1684188882157504
ROC train: 0.901950	val: 0.744124	test: 0.723766
PRC train: 0.562293	val: 0.343167	test: 0.330492

Epoch: 25
Loss: 0.16630091414328288
ROC train: 0.907124	val: 0.747900	test: 0.721243
PRC train: 0.580347	val: 0.339279	test: 0.332728

Epoch: 26
Loss: 0.16362193020230914
ROC train: 0.908515	val: 0.749214	test: 0.704164
PRC train: 0.589669	val: 0.341554	test: 0.315390

Epoch: 27
Loss: 0.16154848768493135
ROC train: 0.912245	val: 0.756892	test: 0.722210
PRC train: 0.597695	val: 0.345033	test: 0.329373

Epoch: 28
Loss: 0.15930076911970203
ROC train: 0.908660	val: 0.744366	test: 0.702871
PRC train: 0.592382	val: 0.351687	test: 0.325823

Epoch: 29
Loss: 0.16244426770615608
ROC train: 0.916632	val: 0.764977	test: 0.717243
PRC train: 0.613534	val: 0.344985	test: 0.325884

Epoch: 30
Loss: 0.15738129247372046
ROC train: 0.919805	val: 0.751339	test: 0.712390
PRC train: 0.623793	val: 0.332832	test: 0.326999

Epoch: 31
Loss: 0.15446070954875865
ROC train: 0.917310	val: 0.761387	test: 0.721644
PRC train: 0.603616	val: 0.337404	test: 0.336562

Epoch: 32
Loss: 0.15575006363390906
ROC train: 0.924698	val: 0.750777	test: 0.707071Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/tox21/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/tox21/noise=0.1/tox21_scaff_2_20-05_14-43-32  ]
[ Using Seed :  2  ]
[ Using device :  cuda:2  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5550881925038545
ROC train: 0.687947	val: 0.629856	test: 0.608073
PRC train: 0.194743	val: 0.198159	test: 0.181496

Epoch: 2
Loss: 0.3333865517110435
ROC train: 0.730300	val: 0.683894	test: 0.636434
PRC train: 0.232055	val: 0.223644	test: 0.213777

Epoch: 3
Loss: 0.24747784336291687
ROC train: 0.756638	val: 0.710697	test: 0.678507
PRC train: 0.269547	val: 0.253896	test: 0.227337

Epoch: 4
Loss: 0.2175308023202769
ROC train: 0.777790	val: 0.736891	test: 0.697229
PRC train: 0.302540	val: 0.287383	test: 0.258790

Epoch: 5
Loss: 0.21015673764939255
ROC train: 0.789736	val: 0.750306	test: 0.706800
PRC train: 0.319375	val: 0.311923	test: 0.304506

Epoch: 6
Loss: 0.20559799299079903
ROC train: 0.801024	val: 0.760244	test: 0.705606
PRC train: 0.335871	val: 0.328980	test: 0.304582

Epoch: 7
Loss: 0.2010922102495613
ROC train: 0.819331	val: 0.746316	test: 0.710081
PRC train: 0.355944	val: 0.302487	test: 0.302323

Epoch: 8
Loss: 0.1968631284812525
ROC train: 0.818092	val: 0.757598	test: 0.712769
PRC train: 0.368459	val: 0.339189	test: 0.300842

Epoch: 9
Loss: 0.19487527249235476
ROC train: 0.832892	val: 0.753974	test: 0.714360
PRC train: 0.388147	val: 0.331767	test: 0.304395

Epoch: 10
Loss: 0.19376039904021694
ROC train: 0.844443	val: 0.743748	test: 0.706641
PRC train: 0.418209	val: 0.312572	test: 0.311704

Epoch: 11
Loss: 0.18893849238588797
ROC train: 0.848813	val: 0.747447	test: 0.717470
PRC train: 0.417348	val: 0.321829	test: 0.317164

Epoch: 12
Loss: 0.18749636875904277
ROC train: 0.845811	val: 0.758304	test: 0.722530
PRC train: 0.414136	val: 0.340264	test: 0.326075

Epoch: 13
Loss: 0.18769537866114128
ROC train: 0.860105	val: 0.749070	test: 0.721482
PRC train: 0.460410	val: 0.324587	test: 0.323449

Epoch: 14
Loss: 0.18331709178768968
ROC train: 0.865017	val: 0.755448	test: 0.726502
PRC train: 0.458593	val: 0.357048	test: 0.322619

Epoch: 15
Loss: 0.18223133497446017
ROC train: 0.866937	val: 0.754315	test: 0.726449
PRC train: 0.475533	val: 0.341286	test: 0.319371

Epoch: 16
Loss: 0.18012697464845057
ROC train: 0.871023	val: 0.741956	test: 0.720084
PRC train: 0.487141	val: 0.330196	test: 0.338828

Epoch: 17
Loss: 0.178318541575567
ROC train: 0.876057	val: 0.752855	test: 0.715469
PRC train: 0.504028	val: 0.336837	test: 0.332130

Epoch: 18
Loss: 0.17492660271491997
ROC train: 0.881762	val: 0.758209	test: 0.724637
PRC train: 0.523685	val: 0.352241	test: 0.344030

Epoch: 19
Loss: 0.17390548190511285
ROC train: 0.888861	val: 0.748931	test: 0.719813
PRC train: 0.543937	val: 0.336832	test: 0.331940

Epoch: 20
Loss: 0.17379352436459325
ROC train: 0.886461	val: 0.744921	test: 0.716793
PRC train: 0.529604	val: 0.337357	test: 0.347048

Epoch: 21
Loss: 0.173004211890262
ROC train: 0.885508	val: 0.761658	test: 0.724155
PRC train: 0.523355	val: 0.345013	test: 0.332238

Epoch: 22
Loss: 0.17103058686923758
ROC train: 0.895903	val: 0.748659	test: 0.728403
PRC train: 0.563431	val: 0.351770	test: 0.348997

Epoch: 23
Loss: 0.16831608583894322
ROC train: 0.895146	val: 0.762590	test: 0.719820
PRC train: 0.565495	val: 0.350742	test: 0.334584

Epoch: 24
Loss: 0.16837514911257923
ROC train: 0.895824	val: 0.762443	test: 0.734918
PRC train: 0.562559	val: 0.361409	test: 0.332695

Epoch: 25
Loss: 0.1674693822015853
ROC train: 0.901364	val: 0.762628	test: 0.724472
PRC train: 0.585189	val: 0.364849	test: 0.333567

Epoch: 26
Loss: 0.16500361262739965
ROC train: 0.906283	val: 0.758548	test: 0.723592
PRC train: 0.602975	val: 0.347261	test: 0.338842

Epoch: 27
Loss: 0.16368820128357617
ROC train: 0.909368	val: 0.760095	test: 0.722532
PRC train: 0.611047	val: 0.352256	test: 0.352782

Epoch: 28
Loss: 0.16161199290032877
ROC train: 0.911472	val: 0.756859	test: 0.724454
PRC train: 0.596008	val: 0.346414	test: 0.352170

Epoch: 29
Loss: 0.1614164648139667
ROC train: 0.915629	val: 0.748712	test: 0.735965
PRC train: 0.629289	val: 0.343713	test: 0.356111

Epoch: 30
Loss: 0.16004323494584216
ROC train: 0.909927	val: 0.740550	test: 0.730689
PRC train: 0.604746	val: 0.331400	test: 0.331243

Epoch: 31
Loss: 0.15710995977328268
ROC train: 0.919678	val: 0.760156	test: 0.728199
PRC train: 0.636661	val: 0.363877	test: 0.349912

Epoch: 32
Loss: 0.15658587989413397
ROC train: 0.918797	val: 0.752133	test: 0.734424Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/tox21/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/tox21/noise=0.1/tox21_scaff_1_20-05_14-43-32  ]
[ Using Seed :  1  ]
[ Using device :  cuda:2  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5454394881069448
ROC train: 0.689962	val: 0.648079	test: 0.620391
PRC train: 0.186434	val: 0.201383	test: 0.191513

Epoch: 2
Loss: 0.33078583558461744
ROC train: 0.722235	val: 0.687709	test: 0.637611
PRC train: 0.233095	val: 0.239694	test: 0.229625

Epoch: 3
Loss: 0.24347647064064096
ROC train: 0.753208	val: 0.728476	test: 0.662654
PRC train: 0.281022	val: 0.303001	test: 0.275272

Epoch: 4
Loss: 0.21851082540040867
ROC train: 0.769203	val: 0.736429	test: 0.680574
PRC train: 0.301595	val: 0.284817	test: 0.271845

Epoch: 5
Loss: 0.2106135300757352
ROC train: 0.781881	val: 0.740960	test: 0.677648
PRC train: 0.319845	val: 0.301084	test: 0.266407

Epoch: 6
Loss: 0.20426717332107047
ROC train: 0.799291	val: 0.735280	test: 0.683636
PRC train: 0.328984	val: 0.282447	test: 0.285946

Epoch: 7
Loss: 0.20091414853237333
ROC train: 0.812216	val: 0.762370	test: 0.704112
PRC train: 0.350042	val: 0.320376	test: 0.300961

Epoch: 8
Loss: 0.1987285900225628
ROC train: 0.817251	val: 0.749362	test: 0.702311
PRC train: 0.374194	val: 0.326190	test: 0.298112

Epoch: 9
Loss: 0.19758976003767142
ROC train: 0.830303	val: 0.752695	test: 0.706211
PRC train: 0.382079	val: 0.300614	test: 0.303794

Epoch: 10
Loss: 0.19130784134609616
ROC train: 0.833392	val: 0.757839	test: 0.720370
PRC train: 0.400508	val: 0.328748	test: 0.327004

Epoch: 11
Loss: 0.18987214552869516
ROC train: 0.846290	val: 0.754661	test: 0.710054
PRC train: 0.420051	val: 0.321158	test: 0.313950

Epoch: 12
Loss: 0.188610159372017
ROC train: 0.846248	val: 0.758271	test: 0.722304
PRC train: 0.427304	val: 0.327776	test: 0.328646

Epoch: 13
Loss: 0.18524148696354037
ROC train: 0.860123	val: 0.763980	test: 0.724786
PRC train: 0.461000	val: 0.336886	test: 0.333745

Epoch: 14
Loss: 0.1840113652386356
ROC train: 0.865973	val: 0.771914	test: 0.722859
PRC train: 0.467445	val: 0.341432	test: 0.335040

Epoch: 15
Loss: 0.18029665894693037
ROC train: 0.869374	val: 0.762150	test: 0.721766
PRC train: 0.476611	val: 0.337288	test: 0.333059

Epoch: 16
Loss: 0.18171475600453024
ROC train: 0.868272	val: 0.772348	test: 0.725695
PRC train: 0.481709	val: 0.351127	test: 0.329418

Epoch: 17
Loss: 0.17773842470378345
ROC train: 0.872166	val: 0.770138	test: 0.727338
PRC train: 0.488322	val: 0.360672	test: 0.350218

Epoch: 18
Loss: 0.1788652613220435
ROC train: 0.882971	val: 0.770177	test: 0.722094
PRC train: 0.515978	val: 0.336723	test: 0.339438

Epoch: 19
Loss: 0.1753342876146118
ROC train: 0.886654	val: 0.767322	test: 0.733913
PRC train: 0.518928	val: 0.332027	test: 0.333469

Epoch: 20
Loss: 0.17464036320913626
ROC train: 0.882245	val: 0.761738	test: 0.723222
PRC train: 0.514803	val: 0.349232	test: 0.332690

Epoch: 21
Loss: 0.1718236865449395
ROC train: 0.886322	val: 0.772329	test: 0.733900
PRC train: 0.534151	val: 0.344774	test: 0.345135

Epoch: 22
Loss: 0.17038666971696487
ROC train: 0.893815	val: 0.772380	test: 0.742419
PRC train: 0.546466	val: 0.339935	test: 0.349647

Epoch: 23
Loss: 0.17004491168229793
ROC train: 0.889071	val: 0.782975	test: 0.737177
PRC train: 0.533165	val: 0.365670	test: 0.358908

Epoch: 24
Loss: 0.16902040281446074
ROC train: 0.901597	val: 0.773063	test: 0.735949
PRC train: 0.574542	val: 0.345971	test: 0.351483

Epoch: 25
Loss: 0.1655352962119533
ROC train: 0.906984	val: 0.766748	test: 0.725484
PRC train: 0.585504	val: 0.335316	test: 0.340141

Epoch: 26
Loss: 0.16484970165255913
ROC train: 0.906599	val: 0.772969	test: 0.728218
PRC train: 0.584268	val: 0.335311	test: 0.336537

Epoch: 27
Loss: 0.16268288589941948
ROC train: 0.911723	val: 0.766620	test: 0.723519
PRC train: 0.600495	val: 0.316287	test: 0.330673

Epoch: 28
Loss: 0.1610239831212225
ROC train: 0.911335	val: 0.766674	test: 0.736013
PRC train: 0.605805	val: 0.345360	test: 0.356738

Epoch: 29
Loss: 0.1596511459201637
ROC train: 0.916867	val: 0.774314	test: 0.727591
PRC train: 0.621013	val: 0.333621	test: 0.337528

Epoch: 30
Loss: 0.15772057187174274
ROC train: 0.917703	val: 0.768620	test: 0.727460
PRC train: 0.627896	val: 0.334252	test: 0.338985

Epoch: 31
Loss: 0.15889861556930593
ROC train: 0.917239	val: 0.770341	test: 0.744792
PRC train: 0.623132	val: 0.334677	test: 0.355439

Epoch: 32
Loss: 0.1576686416481535
ROC train: 0.920497	val: 0.764498	test: 0.731436Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/tox21/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:3
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/tox21/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/tox21/noise=0.2/tox21_scaff_1_20-05_14-43-32  ]
[ Using Seed :  1  ]
[ Using device :  cuda:3  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5508795074004194
ROC train: 0.675627	val: 0.635698	test: 0.613311
PRC train: 0.166331	val: 0.175722	test: 0.164893

Epoch: 2
Loss: 0.331809415379075
ROC train: 0.718808	val: 0.664976	test: 0.645238
PRC train: 0.217444	val: 0.204506	test: 0.195337

Epoch: 3
Loss: 0.24690448840179102
ROC train: 0.740760	val: 0.714537	test: 0.667420
PRC train: 0.253879	val: 0.262373	test: 0.238270

Epoch: 4
Loss: 0.22289604258011653
ROC train: 0.756177	val: 0.721565	test: 0.677268
PRC train: 0.269074	val: 0.259361	test: 0.235743

Epoch: 5
Loss: 0.21639839420305784
ROC train: 0.773450	val: 0.729489	test: 0.685370
PRC train: 0.292209	val: 0.264299	test: 0.244375

Epoch: 6
Loss: 0.20923068625353536
ROC train: 0.788998	val: 0.741365	test: 0.692807
PRC train: 0.306060	val: 0.264847	test: 0.250782

Epoch: 7
Loss: 0.2066357149846379
ROC train: 0.803479	val: 0.752514	test: 0.702841
PRC train: 0.338941	val: 0.290770	test: 0.250110

Epoch: 8
Loss: 0.20376445780011138
ROC train: 0.812883	val: 0.750847	test: 0.708527
PRC train: 0.347186	val: 0.271002	test: 0.264765

Epoch: 9
Loss: 0.20197403137618072
ROC train: 0.820936	val: 0.744483	test: 0.705604
PRC train: 0.352394	val: 0.272615	test: 0.248121

Epoch: 10
Loss: 0.19965338918871467
ROC train: 0.815259	val: 0.747815	test: 0.723646
PRC train: 0.369536	val: 0.330130	test: 0.294249

Epoch: 11
Loss: 0.19554489686108498
ROC train: 0.831713	val: 0.748862	test: 0.714657
PRC train: 0.385254	val: 0.278365	test: 0.268370

Epoch: 12
Loss: 0.19444754324500557
ROC train: 0.837028	val: 0.749209	test: 0.729024
PRC train: 0.405953	val: 0.313551	test: 0.288734

Epoch: 13
Loss: 0.19314770663694755
ROC train: 0.847219	val: 0.756124	test: 0.736392
PRC train: 0.427554	val: 0.327200	test: 0.292512

Epoch: 14
Loss: 0.1904040985160438
ROC train: 0.851214	val: 0.760235	test: 0.723433
PRC train: 0.425353	val: 0.300825	test: 0.278540

Epoch: 15
Loss: 0.18805104572020606
ROC train: 0.856134	val: 0.749526	test: 0.729803
PRC train: 0.441846	val: 0.318626	test: 0.292451

Epoch: 16
Loss: 0.1886572581692424
ROC train: 0.863546	val: 0.750062	test: 0.735763
PRC train: 0.457118	val: 0.307491	test: 0.287402

Epoch: 17
Loss: 0.1830733782176863
ROC train: 0.867311	val: 0.743213	test: 0.728677
PRC train: 0.466792	val: 0.304612	test: 0.297369

Epoch: 18
Loss: 0.18648155485255713
ROC train: 0.872078	val: 0.752442	test: 0.731363
PRC train: 0.473794	val: 0.320913	test: 0.298866

Epoch: 19
Loss: 0.18144023364968725
ROC train: 0.876382	val: 0.737900	test: 0.713666
PRC train: 0.493212	val: 0.276531	test: 0.259669

Epoch: 20
Loss: 0.18036822654594759
ROC train: 0.875009	val: 0.747233	test: 0.732316
PRC train: 0.494143	val: 0.321947	test: 0.297213

Epoch: 21
Loss: 0.17864648225824728
ROC train: 0.885239	val: 0.756036	test: 0.738989
PRC train: 0.519607	val: 0.334112	test: 0.311011

Epoch: 22
Loss: 0.17713076516686102
ROC train: 0.887015	val: 0.752648	test: 0.746297
PRC train: 0.514729	val: 0.330206	test: 0.316443

Epoch: 23
Loss: 0.17457386528165444
ROC train: 0.891983	val: 0.755379	test: 0.738072
PRC train: 0.537394	val: 0.341382	test: 0.304000

Epoch: 24
Loss: 0.17259793935880618
ROC train: 0.891950	val: 0.756975	test: 0.738061
PRC train: 0.548130	val: 0.363104	test: 0.323435

Epoch: 25
Loss: 0.1712579387171702
ROC train: 0.892744	val: 0.725139	test: 0.706320
PRC train: 0.539450	val: 0.261358	test: 0.250079

Epoch: 26
Loss: 0.1702992148729768
ROC train: 0.902225	val: 0.749592	test: 0.735033
PRC train: 0.574106	val: 0.302874	test: 0.299193

Epoch: 27
Loss: 0.16723689227312982
ROC train: 0.905136	val: 0.743945	test: 0.729636
PRC train: 0.574591	val: 0.314932	test: 0.304660

Epoch: 28
Loss: 0.1663130151039404
ROC train: 0.914107	val: 0.763802	test: 0.731962
PRC train: 0.599917	val: 0.352876	test: 0.322826

Epoch: 29
Loss: 0.1645836265854091
ROC train: 0.916659	val: 0.756633	test: 0.726148
PRC train: 0.607259	val: 0.320343	test: 0.297709

Epoch: 30
Loss: 0.16356680362603662
ROC train: 0.912751	val: 0.732476	test: 0.724480
PRC train: 0.596302	val: 0.288305	test: 0.277433

Epoch: 31
Loss: 0.16246429099077017
ROC train: 0.921292	val: 0.754842	test: 0.733170
PRC train: 0.622421	val: 0.364721	test: 0.323453

Epoch: 32
Loss: 0.16041885123042174
ROC train: 0.920727	val: 0.736367	test: 0.723510
ROC train: 0.917717	val: 0.778922	test: 0.751334
PRC train: 0.638391	val: 0.354835	test: 0.366490

Epoch: 34
Loss: 0.15116516389399912
ROC train: 0.921003	val: 0.781719	test: 0.747609
PRC train: 0.644328	val: 0.361920	test: 0.361213

Epoch: 35
Loss: 0.1489455931303491
ROC train: 0.922055	val: 0.778087	test: 0.754674
PRC train: 0.653293	val: 0.359280	test: 0.361745

Epoch: 36
Loss: 0.14852343504029986
ROC train: 0.922207	val: 0.779187	test: 0.755534
PRC train: 0.648586	val: 0.354356	test: 0.355494

Epoch: 37
Loss: 0.14919524726185981
ROC train: 0.923621	val: 0.769613	test: 0.755170
PRC train: 0.640876	val: 0.347140	test: 0.363086

Epoch: 38
Loss: 0.1459117950652985
ROC train: 0.927895	val: 0.765414	test: 0.745966
PRC train: 0.658917	val: 0.351610	test: 0.334713

Epoch: 39
Loss: 0.14674949296868456
ROC train: 0.928399	val: 0.779421	test: 0.758098
PRC train: 0.673850	val: 0.374026	test: 0.372687

Epoch: 40
Loss: 0.14505090438468474
ROC train: 0.927928	val: 0.792712	test: 0.754822
PRC train: 0.663884	val: 0.377515	test: 0.362197

Epoch: 41
Loss: 0.1437702908745711
ROC train: 0.931964	val: 0.779542	test: 0.746674
PRC train: 0.679182	val: 0.350839	test: 0.351402

Epoch: 42
Loss: 0.14436363373283082
ROC train: 0.929972	val: 0.778412	test: 0.747789
PRC train: 0.670355	val: 0.347992	test: 0.351910

Epoch: 43
Loss: 0.14143395166685488
ROC train: 0.933234	val: 0.789888	test: 0.753599
PRC train: 0.686419	val: 0.367207	test: 0.369173

Epoch: 44
Loss: 0.14187586487084908
ROC train: 0.933658	val: 0.786849	test: 0.748201
PRC train: 0.692499	val: 0.373077	test: 0.355334

Epoch: 45
Loss: 0.1407365530625211
ROC train: 0.936881	val: 0.783059	test: 0.746863
PRC train: 0.690262	val: 0.355078	test: 0.356192

Epoch: 46
Loss: 0.13884187038570356
ROC train: 0.935555	val: 0.778873	test: 0.758630
PRC train: 0.686791	val: 0.362388	test: 0.352842

Epoch: 47
Loss: 0.14095489492166702
ROC train: 0.937726	val: 0.786997	test: 0.755057
PRC train: 0.689286	val: 0.372382	test: 0.360850

Epoch: 48
Loss: 0.14088649851040397
ROC train: 0.940566	val: 0.781995	test: 0.748716
PRC train: 0.704034	val: 0.375229	test: 0.348371

Epoch: 49
Loss: 0.139198293817066
ROC train: 0.943958	val: 0.778111	test: 0.748653
PRC train: 0.714559	val: 0.371311	test: 0.359282

Epoch: 50
Loss: 0.1385688833375716
ROC train: 0.942621	val: 0.771713	test: 0.753523
PRC train: 0.706058	val: 0.350407	test: 0.365962

Epoch: 51
Loss: 0.13711865413034663
ROC train: 0.940587	val: 0.774970	test: 0.756979
PRC train: 0.701191	val: 0.351083	test: 0.367076

Epoch: 52
Loss: 0.13772432294700213
ROC train: 0.945202	val: 0.778000	test: 0.753610
PRC train: 0.718286	val: 0.360922	test: 0.347177

Epoch: 53
Loss: 0.13451972205607762
ROC train: 0.943472	val: 0.759348	test: 0.747277
PRC train: 0.711765	val: 0.366545	test: 0.356712

Epoch: 54
Loss: 0.13428144125649105
ROC train: 0.945716	val: 0.774076	test: 0.749531
PRC train: 0.726446	val: 0.358156	test: 0.346722

Epoch: 55
Loss: 0.13633969168355273
ROC train: 0.948585	val: 0.770326	test: 0.751869
PRC train: 0.734847	val: 0.356362	test: 0.360750

Epoch: 56
Loss: 0.1344194682986199
ROC train: 0.948428	val: 0.756299	test: 0.752098
PRC train: 0.734573	val: 0.342690	test: 0.350988

Epoch: 57
Loss: 0.1333268695479561
ROC train: 0.951746	val: 0.773487	test: 0.755602
PRC train: 0.743945	val: 0.370136	test: 0.365218

Epoch: 58
Loss: 0.1325807039820272
ROC train: 0.951796	val: 0.776970	test: 0.758622
PRC train: 0.747300	val: 0.374772	test: 0.370266

Epoch: 59
Loss: 0.13146706713429193
ROC train: 0.950336	val: 0.771953	test: 0.753583
PRC train: 0.746550	val: 0.375483	test: 0.354381

Epoch: 60
Loss: 0.13017437170487223
ROC train: 0.952909	val: 0.773427	test: 0.759724
PRC train: 0.746469	val: 0.361931	test: 0.354080

Epoch: 61
Loss: 0.12870142993180886
ROC train: 0.953834	val: 0.772846	test: 0.749839
PRC train: 0.750165	val: 0.367634	test: 0.344989

Epoch: 62
Loss: 0.12994220543785867
ROC train: 0.955404	val: 0.769558	test: 0.753208
PRC train: 0.765471	val: 0.367407	test: 0.337152

Epoch: 63
Loss: 0.12841373297101766
ROC train: 0.956557	val: 0.779890	test: 0.756078
PRC train: 0.766695	val: 0.367233	test: 0.359793

Epoch: 64
Loss: 0.1278348225341848
ROC train: 0.957257	val: 0.787720	test: 0.750123
PRC train: 0.767364	val: 0.381114	test: 0.358766

Epoch: 65
Loss: 0.12684940583662013
ROC train: 0.959299	val: 0.778501	test: 0.758795
PRC train: 0.778693	val: 0.370595	test: 0.359917

Epoch: 66
Loss: 0.12661037847789752
ROC train: 0.957523	val: 0.767980	test: 0.740278
PRC train: 0.765682	val: 0.345921	test: 0.343932

Epoch: 67
Loss: 0.12414537607548486
ROC train: 0.961274	val: 0.777415	test: 0.754659
PRC train: 0.788444	val: 0.371415	test: 0.363571

Epoch: 68
Loss: 0.12629606905690433
ROC train: 0.960796	val: 0.768357	test: 0.748101
PRC train: 0.779190	val: 0.361505	test: 0.352099

Epoch: 69
Loss: 0.1241290550563533
ROC train: 0.962210	val: 0.766534	test: 0.749698
PRC train: 0.791310	val: 0.367962	test: 0.357062

Epoch: 70
Loss: 0.12399558922281333
ROC train: 0.957452	val: 0.762891	test: 0.755378
PRC train: 0.774090	val: 0.342023	test: 0.330316

Epoch: 71
Loss: 0.12261124525672136
ROC train: 0.961663	val: 0.771355	test: 0.747461
PRC train: 0.786874	val: 0.355216	test: 0.336339

Epoch: 72
Loss: 0.12202654649833163
ROC train: 0.963628	val: 0.772850	test: 0.749073
PRC train: 0.799666	val: 0.364934	test: 0.353834

Epoch: 73
Loss: 0.12205770516217024
ROC train: 0.963720	val: 0.783973	test: 0.759608
PRC train: 0.803995	val: 0.365047	test: 0.357702

Epoch: 74
Loss: 0.11951845239838586
ROC train: 0.966161	val: 0.776302	test: 0.749633
PRC train: 0.805558	val: 0.366904	test: 0.349685

Epoch: 75
Loss: 0.12047833828138482
ROC train: 0.964983	val: 0.778577	test: 0.757440
PRC train: 0.804825	val: 0.365619	test: 0.347827

Epoch: 76
Loss: 0.11958782042917505
ROC train: 0.965235	val: 0.787732	test: 0.749138
PRC train: 0.804027	val: 0.371178	test: 0.358933

Epoch: 77
Loss: 0.11900594466045199
ROC train: 0.965698	val: 0.771753	test: 0.752185
PRC train: 0.797282	val: 0.348943	test: 0.351665

Epoch: 78
Loss: 0.1178694715461503
ROC train: 0.967126	val: 0.779575	test: 0.745711
PRC train: 0.814615	val: 0.365658	test: 0.348302

Epoch: 79
Loss: 0.1196717753998786
ROC train: 0.969099	val: 0.780977	test: 0.749677
PRC train: 0.821151	val: 0.352229	test: 0.347996

Epoch: 80
Loss: 0.11605004195671192
ROC train: 0.969799	val: 0.775764	test: 0.740357
PRC train: 0.825413	val: 0.362842	test: 0.349456

Epoch: 81
Loss: 0.11716099185881527
ROC train: 0.967814	val: 0.768910	test: 0.740136
PRC train: 0.808561	val: 0.356220	test: 0.348017

Epoch: 82
Loss: 0.11560604296486238
ROC train: 0.970011	val: 0.777206	test: 0.747048
PRC train: 0.824798	val: 0.361679	test: 0.346541

Epoch: 83
Loss: 0.11433453587891997
ROC train: 0.970474	val: 0.776083	test: 0.744434
PRC train: 0.827790	val: 0.365022	test: 0.349729

Epoch: 84
Loss: 0.11502437874803637
ROC train: 0.970989	val: 0.771095	test: 0.739874
PRC train: 0.830230	val: 0.354578	test: 0.351945

Epoch: 85
Loss: 0.1128857581667087
ROC train: 0.970924	val: 0.775558	test: 0.745493
PRC train: 0.827858	val: 0.366840	test: 0.350055

Epoch: 86
Loss: 0.11306101826208907
ROC train: 0.972306	val: 0.776159	test: 0.746118
PRC train: 0.837824	val: 0.347533	test: 0.334131

Epoch: 87
Loss: 0.1127264229525743
ROC train: 0.972778	val: 0.777039	test: 0.747832
PRC train: 0.839392	val: 0.372801	test: 0.352804

Epoch: 88
Loss: 0.11263654166970856
ROC train: 0.973952	val: 0.763704	test: 0.742051
PRC train: 0.844058	val: 0.355998	test: 0.336684

Epoch: 89
Loss: 0.11178932532881404
ROC train: 0.973803	val: 0.768618	test: 0.751412
PRC train: 0.843907	val: 0.352469	test: 0.340156

Epoch: 90
Loss: 0.11065033264375095
ROC train: 0.973775	val: 0.787502	test: 0.753883
PRC train: 0.844765	val: 0.364296	test: 0.354558

Epoch: 91
Loss: 0.11142990642148157
ROC train: 0.974759	val: 0.782222	test: 0.758356
PRC train: 0.847326	val: 0.361033	test: 0.360824

Epoch: 92
Loss: 0.1088153821286933
ROC train: 0.975144	val: 0.783338	test: 0.752412
PRC train: 0.846086	val: 0.374428	test: 0.356854

Epoch: 93
Loss: 0.10935332937765863
ROC train: 0.975973	val: 0.770250	test: 0.734746
PRC train: 0.858162	val: 0.360372	test: 0.335439

Epoch: 94
Loss: 0.11067817660439287
ROC train: 0.919151	val: 0.781115	test: 0.736488
PRC train: 0.640477	val: 0.357542	test: 0.369879

Epoch: 34
Loss: 0.1508598338770143
ROC train: 0.921177	val: 0.763671	test: 0.737268
PRC train: 0.639744	val: 0.337474	test: 0.355364

Epoch: 35
Loss: 0.14933484755035248
ROC train: 0.920802	val: 0.785625	test: 0.741379
PRC train: 0.643161	val: 0.362794	test: 0.357497

Epoch: 36
Loss: 0.14917423431966811
ROC train: 0.922542	val: 0.781933	test: 0.730950
PRC train: 0.644250	val: 0.351523	test: 0.354234

Epoch: 37
Loss: 0.14830091504796855
ROC train: 0.925923	val: 0.779319	test: 0.747962
PRC train: 0.661447	val: 0.370007	test: 0.372081

Epoch: 38
Loss: 0.14637893878921152
ROC train: 0.926778	val: 0.774138	test: 0.732872
PRC train: 0.650159	val: 0.346118	test: 0.352271

Epoch: 39
Loss: 0.14588740480368687
ROC train: 0.930333	val: 0.779274	test: 0.742829
PRC train: 0.668645	val: 0.368312	test: 0.373958

Epoch: 40
Loss: 0.14318449112345433
ROC train: 0.932484	val: 0.771835	test: 0.735726
PRC train: 0.673349	val: 0.360933	test: 0.371571

Epoch: 41
Loss: 0.1439908001414981
ROC train: 0.932260	val: 0.774018	test: 0.722695
PRC train: 0.678155	val: 0.362679	test: 0.365045

Epoch: 42
Loss: 0.14335986586685168
ROC train: 0.931841	val: 0.773353	test: 0.733767
PRC train: 0.677512	val: 0.356395	test: 0.368442

Epoch: 43
Loss: 0.1439981599177114
ROC train: 0.933167	val: 0.767392	test: 0.736541
PRC train: 0.678364	val: 0.362906	test: 0.359733

Epoch: 44
Loss: 0.14122290053851322
ROC train: 0.932023	val: 0.777631	test: 0.737935
PRC train: 0.666904	val: 0.358742	test: 0.346789

Epoch: 45
Loss: 0.14069949363613996
ROC train: 0.937620	val: 0.772591	test: 0.729327
PRC train: 0.695808	val: 0.355375	test: 0.352209

Epoch: 46
Loss: 0.13896295408100343
ROC train: 0.937671	val: 0.772459	test: 0.735893
PRC train: 0.694000	val: 0.356057	test: 0.355391

Epoch: 47
Loss: 0.13948983933945056
ROC train: 0.940147	val: 0.771157	test: 0.742030
PRC train: 0.701915	val: 0.353393	test: 0.350441

Epoch: 48
Loss: 0.13987760006623814
ROC train: 0.941798	val: 0.777026	test: 0.742813
PRC train: 0.709295	val: 0.363131	test: 0.368022

Epoch: 49
Loss: 0.1376917011863786
ROC train: 0.943171	val: 0.780555	test: 0.743354
PRC train: 0.713428	val: 0.358860	test: 0.384903

Epoch: 50
Loss: 0.13780763293761591
ROC train: 0.942768	val: 0.769134	test: 0.741739
PRC train: 0.712727	val: 0.348546	test: 0.349459

Epoch: 51
Loss: 0.1378086980646697
ROC train: 0.945122	val: 0.781559	test: 0.735766
PRC train: 0.726945	val: 0.360641	test: 0.364914

Epoch: 52
Loss: 0.13662328492336312
ROC train: 0.945635	val: 0.778298	test: 0.740319
PRC train: 0.723627	val: 0.368202	test: 0.376210

Epoch: 53
Loss: 0.13303920235016067
ROC train: 0.946554	val: 0.776515	test: 0.736716
PRC train: 0.730068	val: 0.364029	test: 0.361796

Epoch: 54
Loss: 0.133061681495757
ROC train: 0.946671	val: 0.768590	test: 0.753144
PRC train: 0.729572	val: 0.354631	test: 0.383184

Epoch: 55
Loss: 0.13275718476162815
ROC train: 0.948318	val: 0.767019	test: 0.729046
PRC train: 0.735365	val: 0.354299	test: 0.355341

Epoch: 56
Loss: 0.13438540530001547
ROC train: 0.948773	val: 0.781443	test: 0.744358
PRC train: 0.737222	val: 0.373224	test: 0.365726

Epoch: 57
Loss: 0.13054383304309886
ROC train: 0.949738	val: 0.762861	test: 0.743330
PRC train: 0.733224	val: 0.345872	test: 0.362500

Epoch: 58
Loss: 0.13106774527809242
ROC train: 0.952561	val: 0.767903	test: 0.739319
PRC train: 0.750187	val: 0.351417	test: 0.356534

Epoch: 59
Loss: 0.1289491671065795
ROC train: 0.954121	val: 0.774596	test: 0.745007
PRC train: 0.751482	val: 0.365768	test: 0.371632

Epoch: 60
Loss: 0.12809501496077869
ROC train: 0.955998	val: 0.773348	test: 0.740245
PRC train: 0.760147	val: 0.363205	test: 0.360724

Epoch: 61
Loss: 0.12960614210315452
ROC train: 0.956173	val: 0.763404	test: 0.737164
PRC train: 0.758519	val: 0.348022	test: 0.352153

Epoch: 62
Loss: 0.13049957842638094
ROC train: 0.955079	val: 0.774097	test: 0.741870
PRC train: 0.757553	val: 0.363820	test: 0.354349

Epoch: 63
Loss: 0.1286306447722578
ROC train: 0.957178	val: 0.765402	test: 0.739112
PRC train: 0.766624	val: 0.358200	test: 0.357575

Epoch: 64
Loss: 0.126308573002469
ROC train: 0.957655	val: 0.759470	test: 0.742476
PRC train: 0.763549	val: 0.360984	test: 0.369829

Epoch: 65
Loss: 0.12832817005135047
ROC train: 0.956100	val: 0.765147	test: 0.748470
PRC train: 0.763242	val: 0.366171	test: 0.371922

Epoch: 66
Loss: 0.12774567996945113
ROC train: 0.959392	val: 0.761567	test: 0.736503
PRC train: 0.775779	val: 0.359880	test: 0.355291

Epoch: 67
Loss: 0.1249046096229259
ROC train: 0.957444	val: 0.757283	test: 0.741007
PRC train: 0.767450	val: 0.347085	test: 0.351540

Epoch: 68
Loss: 0.12300009726941205
ROC train: 0.955013	val: 0.763371	test: 0.741238
PRC train: 0.759795	val: 0.366369	test: 0.372627

Epoch: 69
Loss: 0.1241361002969133
ROC train: 0.960639	val: 0.768210	test: 0.746044
PRC train: 0.783996	val: 0.372041	test: 0.376292

Epoch: 70
Loss: 0.12198331723607574
ROC train: 0.962761	val: 0.758902	test: 0.738022
PRC train: 0.785585	val: 0.357596	test: 0.354785

Epoch: 71
Loss: 0.1215422352864313
ROC train: 0.962960	val: 0.748625	test: 0.737982
PRC train: 0.791517	val: 0.361213	test: 0.374692

Epoch: 72
Loss: 0.12060584431918041
ROC train: 0.963119	val: 0.766138	test: 0.738399
PRC train: 0.790572	val: 0.372360	test: 0.378798

Epoch: 73
Loss: 0.1215328783437659
ROC train: 0.964589	val: 0.764500	test: 0.739320
PRC train: 0.794749	val: 0.353675	test: 0.369499

Epoch: 74
Loss: 0.11932740475908776
ROC train: 0.966335	val: 0.761566	test: 0.738223
PRC train: 0.807302	val: 0.365229	test: 0.372980

Epoch: 75
Loss: 0.11942931617186085
ROC train: 0.964334	val: 0.761934	test: 0.731268
PRC train: 0.797808	val: 0.362243	test: 0.361116

Epoch: 76
Loss: 0.1194294139827008
ROC train: 0.967537	val: 0.759130	test: 0.739841
PRC train: 0.813659	val: 0.366215	test: 0.365622

Epoch: 77
Loss: 0.1178346438140958
ROC train: 0.965245	val: 0.762921	test: 0.741968
PRC train: 0.803536	val: 0.350636	test: 0.380452

Epoch: 78
Loss: 0.11912924988516321
ROC train: 0.967001	val: 0.761360	test: 0.735065
PRC train: 0.815699	val: 0.363823	test: 0.373835

Epoch: 79
Loss: 0.11846662281655021
ROC train: 0.968029	val: 0.764942	test: 0.740884
PRC train: 0.814183	val: 0.366386	test: 0.364832

Epoch: 80
Loss: 0.11664288310120091
ROC train: 0.969504	val: 0.757258	test: 0.745096
PRC train: 0.821676	val: 0.364749	test: 0.373504

Epoch: 81
Loss: 0.11589064015030004
ROC train: 0.968570	val: 0.754962	test: 0.746480
PRC train: 0.823313	val: 0.361255	test: 0.377735

Epoch: 82
Loss: 0.11347044191102984
ROC train: 0.970912	val: 0.755051	test: 0.740417
PRC train: 0.828674	val: 0.356215	test: 0.370629

Epoch: 83
Loss: 0.11319133861855155
ROC train: 0.971814	val: 0.756061	test: 0.734970
PRC train: 0.831149	val: 0.347149	test: 0.359143

Epoch: 84
Loss: 0.11302301252115882
ROC train: 0.971935	val: 0.766722	test: 0.748007
PRC train: 0.832841	val: 0.358180	test: 0.384309

Epoch: 85
Loss: 0.11257513199287716
ROC train: 0.972245	val: 0.760256	test: 0.744637
PRC train: 0.837066	val: 0.356391	test: 0.373726

Epoch: 86
Loss: 0.11317717167329397
ROC train: 0.972181	val: 0.754241	test: 0.735299
PRC train: 0.832690	val: 0.354535	test: 0.379520

Epoch: 87
Loss: 0.11242795426289628
ROC train: 0.973770	val: 0.755075	test: 0.743962
PRC train: 0.842296	val: 0.352503	test: 0.362470

Epoch: 88
Loss: 0.11363675109945
ROC train: 0.972531	val: 0.743014	test: 0.736506
PRC train: 0.833135	val: 0.340272	test: 0.351090

Epoch: 89
Loss: 0.1111400223988891
ROC train: 0.974052	val: 0.762336	test: 0.744084
PRC train: 0.843664	val: 0.360213	test: 0.375595

Epoch: 90
Loss: 0.11078626529793163
ROC train: 0.974799	val: 0.760466	test: 0.745078
PRC train: 0.849299	val: 0.353854	test: 0.383281

Epoch: 91
Loss: 0.1108053801531954
ROC train: 0.975659	val: 0.756656	test: 0.740183
PRC train: 0.851958	val: 0.357449	test: 0.376013

Epoch: 92
Loss: 0.10949185428621687
ROC train: 0.975803	val: 0.762370	test: 0.744442
PRC train: 0.851984	val: 0.355860	test: 0.369942

Epoch: 93
Loss: 0.1072319867160001
ROC train: 0.976425	val: 0.759362	test: 0.737372
PRC train: 0.854763	val: 0.361814	test: 0.364934

Epoch: 94
Loss: 0.10698978803465053
PRC train: 0.642529	val: 0.364731	test: 0.351984

Epoch: 33
Loss: 0.15235618258688055
ROC train: 0.923765	val: 0.767236	test: 0.727798
PRC train: 0.638151	val: 0.336234	test: 0.333228

Epoch: 34
Loss: 0.15360914090125205
ROC train: 0.925941	val: 0.775348	test: 0.717652
PRC train: 0.656610	val: 0.351972	test: 0.330631

Epoch: 35
Loss: 0.15212264782097235
ROC train: 0.928527	val: 0.775566	test: 0.722547
PRC train: 0.661596	val: 0.342104	test: 0.352366

Epoch: 36
Loss: 0.14925457182291293
ROC train: 0.930238	val: 0.775973	test: 0.716027
PRC train: 0.667731	val: 0.355201	test: 0.337473

Epoch: 37
Loss: 0.1479571055280672
ROC train: 0.934429	val: 0.778939	test: 0.726873
PRC train: 0.680396	val: 0.349099	test: 0.335272

Epoch: 38
Loss: 0.1480041499965618
ROC train: 0.931767	val: 0.769744	test: 0.717955
PRC train: 0.665689	val: 0.332824	test: 0.320868

Epoch: 39
Loss: 0.14501385017485663
ROC train: 0.935043	val: 0.768036	test: 0.720449
PRC train: 0.688205	val: 0.354720	test: 0.332825

Epoch: 40
Loss: 0.14569614859292096
ROC train: 0.938021	val: 0.773283	test: 0.724669
PRC train: 0.694398	val: 0.360189	test: 0.336879

Epoch: 41
Loss: 0.14303077461663422
ROC train: 0.937608	val: 0.784667	test: 0.720043
PRC train: 0.698857	val: 0.364164	test: 0.336975

Epoch: 42
Loss: 0.14303193510302095
ROC train: 0.937905	val: 0.756203	test: 0.712947
PRC train: 0.682890	val: 0.314842	test: 0.292709

Epoch: 43
Loss: 0.1420086070391877
ROC train: 0.943991	val: 0.775381	test: 0.717922
PRC train: 0.710888	val: 0.361168	test: 0.333957

Epoch: 44
Loss: 0.1423523235861321
ROC train: 0.941433	val: 0.773472	test: 0.722070
PRC train: 0.705866	val: 0.354365	test: 0.330478

Epoch: 45
Loss: 0.14095530347041352
ROC train: 0.944813	val: 0.776210	test: 0.725021
PRC train: 0.723656	val: 0.370393	test: 0.351843

Epoch: 46
Loss: 0.13777718999735328
ROC train: 0.948218	val: 0.772015	test: 0.717206
PRC train: 0.726470	val: 0.374083	test: 0.334203

Epoch: 47
Loss: 0.13744374824390249
ROC train: 0.947681	val: 0.773099	test: 0.717868
PRC train: 0.733029	val: 0.375187	test: 0.339155

Epoch: 48
Loss: 0.13685840818805958
ROC train: 0.949461	val: 0.777125	test: 0.720480
PRC train: 0.742057	val: 0.363184	test: 0.344565

Epoch: 49
Loss: 0.13685147708225887
ROC train: 0.951229	val: 0.780117	test: 0.731767
PRC train: 0.740946	val: 0.368577	test: 0.346939

Epoch: 50
Loss: 0.13523522468548124
ROC train: 0.951310	val: 0.771501	test: 0.730153
PRC train: 0.739984	val: 0.355783	test: 0.344403

Epoch: 51
Loss: 0.13255243783200024
ROC train: 0.955312	val: 0.773695	test: 0.730118
PRC train: 0.755011	val: 0.363172	test: 0.342574

Epoch: 52
Loss: 0.13286290284660693
ROC train: 0.953592	val: 0.772131	test: 0.724673
PRC train: 0.747448	val: 0.353155	test: 0.341521

Epoch: 53
Loss: 0.13188052077933352
ROC train: 0.957339	val: 0.770760	test: 0.729548
PRC train: 0.764439	val: 0.386186	test: 0.353342

Epoch: 54
Loss: 0.13088036627339938
ROC train: 0.958615	val: 0.777667	test: 0.733854
PRC train: 0.768597	val: 0.371572	test: 0.347014

Epoch: 55
Loss: 0.12823815221492793
ROC train: 0.957869	val: 0.758805	test: 0.718637
PRC train: 0.769456	val: 0.357823	test: 0.337014

Epoch: 56
Loss: 0.13098813987461524
ROC train: 0.959498	val: 0.771481	test: 0.734629
PRC train: 0.767980	val: 0.366053	test: 0.338086

Epoch: 57
Loss: 0.12800091511008319
ROC train: 0.959908	val: 0.770992	test: 0.723695
PRC train: 0.768806	val: 0.367465	test: 0.329861

Epoch: 58
Loss: 0.12812519155863872
ROC train: 0.961299	val: 0.779119	test: 0.736074
PRC train: 0.784982	val: 0.375017	test: 0.344063

Epoch: 59
Loss: 0.12579516945910765
ROC train: 0.962715	val: 0.777200	test: 0.726619
PRC train: 0.790715	val: 0.377400	test: 0.349041

Epoch: 60
Loss: 0.1253657527812957
ROC train: 0.964279	val: 0.772400	test: 0.729560
PRC train: 0.789039	val: 0.352796	test: 0.331911

Epoch: 61
Loss: 0.12493321189565508
ROC train: 0.966337	val: 0.765574	test: 0.726049
PRC train: 0.803892	val: 0.369961	test: 0.334828

Epoch: 62
Loss: 0.12147252186866217
ROC train: 0.969615	val: 0.767664	test: 0.718225
PRC train: 0.813457	val: 0.374726	test: 0.339925

Epoch: 63
Loss: 0.12224566601982063
ROC train: 0.967526	val: 0.767833	test: 0.723998
PRC train: 0.812687	val: 0.354218	test: 0.327582

Epoch: 64
Loss: 0.1211457108428034
ROC train: 0.970637	val: 0.771625	test: 0.730984
PRC train: 0.823325	val: 0.356730	test: 0.348649

Epoch: 65
Loss: 0.12042435903628795
ROC train: 0.970027	val: 0.772225	test: 0.731825
PRC train: 0.821184	val: 0.382825	test: 0.345644

Epoch: 66
Loss: 0.11787994017138514
ROC train: 0.970558	val: 0.765811	test: 0.728644
PRC train: 0.825557	val: 0.376127	test: 0.338681

Epoch: 67
Loss: 0.11831118999793266
ROC train: 0.970718	val: 0.776053	test: 0.720342
PRC train: 0.829678	val: 0.369674	test: 0.325361

Epoch: 68
Loss: 0.11596276714938325
ROC train: 0.973366	val: 0.773404	test: 0.726867
PRC train: 0.833746	val: 0.390497	test: 0.340875

Epoch: 69
Loss: 0.11623872835030476
ROC train: 0.973909	val: 0.765764	test: 0.728864
PRC train: 0.841471	val: 0.377656	test: 0.340367

Epoch: 70
Loss: 0.11509961912274962
ROC train: 0.974027	val: 0.755041	test: 0.725548
PRC train: 0.839493	val: 0.359092	test: 0.332803

Epoch: 71
Loss: 0.11570031667481141
ROC train: 0.975065	val: 0.767210	test: 0.728218
PRC train: 0.843420	val: 0.373458	test: 0.342076

Epoch: 72
Loss: 0.11378188243952915
ROC train: 0.975204	val: 0.770021	test: 0.726562
PRC train: 0.848548	val: 0.369246	test: 0.325890

Epoch: 73
Loss: 0.11048784848816971
ROC train: 0.978251	val: 0.760485	test: 0.711590
PRC train: 0.864106	val: 0.374929	test: 0.335320

Epoch: 74
Loss: 0.10941111229082731
ROC train: 0.978190	val: 0.760898	test: 0.719832
PRC train: 0.861087	val: 0.396170	test: 0.347725

Epoch: 75
Loss: 0.10996309579181654
ROC train: 0.976381	val: 0.755191	test: 0.717203
PRC train: 0.846407	val: 0.363697	test: 0.337468

Epoch: 76
Loss: 0.1082733194594165
ROC train: 0.980216	val: 0.758794	test: 0.712658
PRC train: 0.872024	val: 0.384288	test: 0.345147

Epoch: 77
Loss: 0.10985358267773593
ROC train: 0.979635	val: 0.758952	test: 0.728360
PRC train: 0.867392	val: 0.372578	test: 0.344036

Epoch: 78
Loss: 0.10902652009173018
ROC train: 0.980539	val: 0.756330	test: 0.713115
PRC train: 0.874826	val: 0.365602	test: 0.338402

Epoch: 79
Loss: 0.10851835777183087
ROC train: 0.982313	val: 0.762224	test: 0.722526
PRC train: 0.881579	val: 0.373634	test: 0.341835

Epoch: 80
Loss: 0.10487278377480563
ROC train: 0.981977	val: 0.768694	test: 0.723982
PRC train: 0.881189	val: 0.378593	test: 0.331288

Epoch: 81
Loss: 0.10522829364491643
ROC train: 0.982640	val: 0.757903	test: 0.717947
PRC train: 0.884667	val: 0.366075	test: 0.342968

Epoch: 82
Loss: 0.10559880973071442
ROC train: 0.984206	val: 0.750849	test: 0.714962
PRC train: 0.892187	val: 0.358924	test: 0.328192

Epoch: 83
Loss: 0.10368174639729448
ROC train: 0.983847	val: 0.755904	test: 0.712474
PRC train: 0.890887	val: 0.352650	test: 0.315513

Epoch: 84
Loss: 0.10206673060012929
ROC train: 0.984373	val: 0.753091	test: 0.721255
PRC train: 0.886957	val: 0.352968	test: 0.326582

Epoch: 85
Loss: 0.10114185012272844
ROC train: 0.985022	val: 0.762930	test: 0.722784
PRC train: 0.891581	val: 0.375071	test: 0.326813

Epoch: 86
Loss: 0.09889399313077198
ROC train: 0.986078	val: 0.745228	test: 0.717509
PRC train: 0.899495	val: 0.357315	test: 0.321265

Epoch: 87
Loss: 0.09907807573353371
ROC train: 0.987363	val: 0.762292	test: 0.723163
PRC train: 0.907959	val: 0.379230	test: 0.342354

Epoch: 88
Loss: 0.09810372078797253
ROC train: 0.987088	val: 0.764234	test: 0.728747
PRC train: 0.904191	val: 0.377110	test: 0.340301

Epoch: 89
Loss: 0.09835270738777047
ROC train: 0.987359	val: 0.761748	test: 0.719534
PRC train: 0.909820	val: 0.383806	test: 0.330407

Epoch: 90
Loss: 0.09680583192488106
ROC train: 0.988686	val: 0.757096	test: 0.725106
PRC train: 0.917798	val: 0.370903	test: 0.324008

Epoch: 91
Loss: 0.09459181527808581
ROC train: 0.988561	val: 0.756991	test: 0.723794
PRC train: 0.919023	val: 0.356852	test: 0.316177

Epoch: 92
Loss: 0.09461983608243768
ROC train: 0.988557	val: 0.760385	test: 0.726654
PRC train: 0.914463	val: 0.379404	test: 0.332299

Epoch: 93
Loss: 0.09370165363246252
ROC train: 0.989153	val: 0.763922	test: 0.726527
ROC train: 0.919425	val: 0.771295	test: 0.749682
PRC train: 0.635022	val: 0.360101	test: 0.367125

Epoch: 34
Loss: 0.1508409428596493
ROC train: 0.917274	val: 0.764680	test: 0.737736
PRC train: 0.632770	val: 0.359664	test: 0.358655

Epoch: 35
Loss: 0.15080231377953135
ROC train: 0.921093	val: 0.765778	test: 0.751531
PRC train: 0.635488	val: 0.355782	test: 0.364422

Epoch: 36
Loss: 0.14985523250768723
ROC train: 0.925227	val: 0.766475	test: 0.733145
PRC train: 0.654212	val: 0.347634	test: 0.364101

Epoch: 37
Loss: 0.14702473251276157
ROC train: 0.923199	val: 0.778433	test: 0.740017
PRC train: 0.655251	val: 0.362119	test: 0.368464

Epoch: 38
Loss: 0.14851144774404174
ROC train: 0.927420	val: 0.781583	test: 0.746508
PRC train: 0.659802	val: 0.363100	test: 0.362850

Epoch: 39
Loss: 0.14643390695367173
ROC train: 0.929762	val: 0.772070	test: 0.738626
PRC train: 0.662795	val: 0.344724	test: 0.354641

Epoch: 40
Loss: 0.14594681476690632
ROC train: 0.931509	val: 0.768048	test: 0.734942
PRC train: 0.671029	val: 0.367301	test: 0.366982

Epoch: 41
Loss: 0.14467416185243281
ROC train: 0.933135	val: 0.773356	test: 0.728181
PRC train: 0.681206	val: 0.364334	test: 0.359157

Epoch: 42
Loss: 0.14369036363698853
ROC train: 0.931705	val: 0.767314	test: 0.739125
PRC train: 0.670199	val: 0.347154	test: 0.359550

Epoch: 43
Loss: 0.14259632658083304
ROC train: 0.935373	val: 0.767822	test: 0.737199
PRC train: 0.682221	val: 0.351828	test: 0.354097

Epoch: 44
Loss: 0.14297923473476198
ROC train: 0.933896	val: 0.775231	test: 0.742967
PRC train: 0.686641	val: 0.364200	test: 0.350953

Epoch: 45
Loss: 0.14185150973194827
ROC train: 0.937982	val: 0.764252	test: 0.736284
PRC train: 0.700028	val: 0.358122	test: 0.345055

Epoch: 46
Loss: 0.14045666778252713
ROC train: 0.939071	val: 0.777950	test: 0.730943
PRC train: 0.702951	val: 0.367717	test: 0.348862

Epoch: 47
Loss: 0.13975498096554564
ROC train: 0.940297	val: 0.770695	test: 0.741040
PRC train: 0.697914	val: 0.370620	test: 0.360371

Epoch: 48
Loss: 0.13693528991131923
ROC train: 0.941687	val: 0.760741	test: 0.746561
PRC train: 0.703863	val: 0.354336	test: 0.353803

Epoch: 49
Loss: 0.13999891741852977
ROC train: 0.941820	val: 0.752635	test: 0.730216
PRC train: 0.697539	val: 0.359592	test: 0.361995

Epoch: 50
Loss: 0.1364633593684575
ROC train: 0.943098	val: 0.755415	test: 0.727870
PRC train: 0.712574	val: 0.356009	test: 0.345942

Epoch: 51
Loss: 0.13713183006232696
ROC train: 0.945065	val: 0.766153	test: 0.733871
PRC train: 0.717936	val: 0.379349	test: 0.356311

Epoch: 52
Loss: 0.13709090342607225
ROC train: 0.945955	val: 0.766902	test: 0.737439
PRC train: 0.714745	val: 0.365548	test: 0.372808

Epoch: 53
Loss: 0.13577242918731686
ROC train: 0.947270	val: 0.775222	test: 0.743040
PRC train: 0.726235	val: 0.374891	test: 0.370026

Epoch: 54
Loss: 0.1354130454932826
ROC train: 0.948514	val: 0.770797	test: 0.738182
PRC train: 0.731963	val: 0.368718	test: 0.366627

Epoch: 55
Loss: 0.1337657726455495
ROC train: 0.949937	val: 0.755876	test: 0.732177
PRC train: 0.730253	val: 0.348058	test: 0.351025

Epoch: 56
Loss: 0.13382007672160331
ROC train: 0.949962	val: 0.769247	test: 0.738064
PRC train: 0.737287	val: 0.375879	test: 0.379357

Epoch: 57
Loss: 0.1357939608315489
ROC train: 0.950024	val: 0.766454	test: 0.730415
PRC train: 0.737424	val: 0.383748	test: 0.362474

Epoch: 58
Loss: 0.13259939646324215
ROC train: 0.951063	val: 0.756834	test: 0.735627
PRC train: 0.731415	val: 0.349985	test: 0.361910

Epoch: 59
Loss: 0.1299373376487543
ROC train: 0.953015	val: 0.770979	test: 0.737596
PRC train: 0.752038	val: 0.378209	test: 0.370519

Epoch: 60
Loss: 0.12923459255150319
ROC train: 0.953438	val: 0.771642	test: 0.737636
PRC train: 0.753152	val: 0.362826	test: 0.357636

Epoch: 61
Loss: 0.12961503179869777
ROC train: 0.955048	val: 0.758163	test: 0.738841
PRC train: 0.756936	val: 0.335558	test: 0.338086

Epoch: 62
Loss: 0.12855859824345395
ROC train: 0.956936	val: 0.769637	test: 0.730465
PRC train: 0.762343	val: 0.364108	test: 0.343393

Epoch: 63
Loss: 0.12742295240840826
ROC train: 0.956060	val: 0.773789	test: 0.738932
PRC train: 0.760839	val: 0.372976	test: 0.365388

Epoch: 64
Loss: 0.12598459288395533
ROC train: 0.956863	val: 0.766772	test: 0.732259
PRC train: 0.768274	val: 0.366450	test: 0.364405

Epoch: 65
Loss: 0.12660102668969167
ROC train: 0.960224	val: 0.768150	test: 0.739049
PRC train: 0.780932	val: 0.380949	test: 0.379457

Epoch: 66
Loss: 0.12532592967461617
ROC train: 0.957312	val: 0.773851	test: 0.735571
PRC train: 0.773295	val: 0.365361	test: 0.352083

Epoch: 67
Loss: 0.12561825427365578
ROC train: 0.958967	val: 0.760701	test: 0.719649
PRC train: 0.772499	val: 0.358575	test: 0.352068

Epoch: 68
Loss: 0.12473317766413725
ROC train: 0.962184	val: 0.776808	test: 0.741329
PRC train: 0.790452	val: 0.363549	test: 0.379580

Epoch: 69
Loss: 0.12408113540069801
ROC train: 0.960034	val: 0.760642	test: 0.733479
PRC train: 0.774131	val: 0.363254	test: 0.369493

Epoch: 70
Loss: 0.12160709968304645
ROC train: 0.963024	val: 0.762850	test: 0.737308
PRC train: 0.792007	val: 0.362859	test: 0.371717

Epoch: 71
Loss: 0.1209222590853742
ROC train: 0.964933	val: 0.769353	test: 0.735472
PRC train: 0.798658	val: 0.362509	test: 0.356894

Epoch: 72
Loss: 0.12005585914148315
ROC train: 0.964865	val: 0.767294	test: 0.735869
PRC train: 0.799365	val: 0.362509	test: 0.354824

Epoch: 73
Loss: 0.1219075224810301
ROC train: 0.965258	val: 0.773468	test: 0.736579
PRC train: 0.803508	val: 0.370074	test: 0.364800

Epoch: 74
Loss: 0.11959079044317995
ROC train: 0.967092	val: 0.768634	test: 0.737718
PRC train: 0.809686	val: 0.354380	test: 0.363994

Epoch: 75
Loss: 0.11942458641749674
ROC train: 0.966625	val: 0.762500	test: 0.729904
PRC train: 0.809295	val: 0.352250	test: 0.361335

Epoch: 76
Loss: 0.11910893749289364
ROC train: 0.966172	val: 0.767790	test: 0.733483
PRC train: 0.805085	val: 0.363213	test: 0.372748

Epoch: 77
Loss: 0.11764178853714384
ROC train: 0.967508	val: 0.773267	test: 0.732945
PRC train: 0.814471	val: 0.382983	test: 0.356050

Epoch: 78
Loss: 0.1181052197695675
ROC train: 0.967223	val: 0.756063	test: 0.722290
PRC train: 0.807934	val: 0.355776	test: 0.351803

Epoch: 79
Loss: 0.11530750765809654
ROC train: 0.968306	val: 0.764801	test: 0.734402
PRC train: 0.816127	val: 0.368968	test: 0.371443

Epoch: 80
Loss: 0.11565889369653305
ROC train: 0.969343	val: 0.773457	test: 0.740322
PRC train: 0.822982	val: 0.377244	test: 0.359033

Epoch: 81
Loss: 0.11657132058374653
ROC train: 0.970697	val: 0.757546	test: 0.722454
PRC train: 0.823976	val: 0.357105	test: 0.346916

Epoch: 82
Loss: 0.11481306052582602
ROC train: 0.971341	val: 0.765204	test: 0.731926
PRC train: 0.829250	val: 0.364580	test: 0.360150

Epoch: 83
Loss: 0.11577190342191544
ROC train: 0.971124	val: 0.758595	test: 0.732509
PRC train: 0.829392	val: 0.368325	test: 0.354554

Epoch: 84
Loss: 0.11466324502103584
ROC train: 0.971074	val: 0.759853	test: 0.720806
PRC train: 0.823736	val: 0.371503	test: 0.356745

Epoch: 85
Loss: 0.11253089868771717
ROC train: 0.972858	val: 0.765249	test: 0.722199
PRC train: 0.839717	val: 0.373687	test: 0.351950

Epoch: 86
Loss: 0.11301692832918647
ROC train: 0.971879	val: 0.767728	test: 0.737620
PRC train: 0.832714	val: 0.361601	test: 0.357722

Epoch: 87
Loss: 0.1123884765742094
ROC train: 0.973324	val: 0.758344	test: 0.727873
PRC train: 0.839036	val: 0.341482	test: 0.333044

Epoch: 88
Loss: 0.1121677971324506
ROC train: 0.974448	val: 0.763649	test: 0.730669
PRC train: 0.844359	val: 0.359702	test: 0.351893

Epoch: 89
Loss: 0.1116131068868274
ROC train: 0.972755	val: 0.759523	test: 0.735463
PRC train: 0.834292	val: 0.349654	test: 0.339263

Epoch: 90
Loss: 0.10952787700986613
ROC train: 0.974026	val: 0.767798	test: 0.730905
PRC train: 0.843925	val: 0.371786	test: 0.358488

Epoch: 91
Loss: 0.10940989123094601
ROC train: 0.976075	val: 0.769235	test: 0.730323
PRC train: 0.852711	val: 0.368655	test: 0.354887

Epoch: 92
Loss: 0.10996033178371341
ROC train: 0.976341	val: 0.771548	test: 0.726979
PRC train: 0.853849	val: 0.372267	test: 0.360416

Epoch: 93
Loss: 0.1093527360181543
ROC train: 0.976759	val: 0.752678	test: 0.726095
PRC train: 0.854915	val: 0.360422	test: 0.352644

Epoch: 94
Loss: 0.10894079489671789
PRC train: 0.611035	val: 0.279571	test: 0.241733

Epoch: 33
Loss: 0.1620043008457751
ROC train: 0.922979	val: 0.681241	test: 0.644454
PRC train: 0.622405	val: 0.259294	test: 0.215031

Epoch: 34
Loss: 0.16018492823864017
ROC train: 0.924522	val: 0.708911	test: 0.667699
PRC train: 0.631053	val: 0.289748	test: 0.244892

Epoch: 35
Loss: 0.15773986400904824
ROC train: 0.927847	val: 0.711395	test: 0.675255
PRC train: 0.644295	val: 0.302615	test: 0.264411

Epoch: 36
Loss: 0.15822039972588905
ROC train: 0.927532	val: 0.705230	test: 0.667738
PRC train: 0.643420	val: 0.280863	test: 0.249122

Epoch: 37
Loss: 0.15559413078434958
ROC train: 0.932858	val: 0.703171	test: 0.650721
PRC train: 0.660106	val: 0.279753	test: 0.236486

Epoch: 38
Loss: 0.15264995041813742
ROC train: 0.932475	val: 0.705572	test: 0.668536
PRC train: 0.657214	val: 0.284823	test: 0.255502

Epoch: 39
Loss: 0.1532027974010297
ROC train: 0.937926	val: 0.684476	test: 0.647034
PRC train: 0.678713	val: 0.262946	test: 0.245424

Epoch: 40
Loss: 0.15372140978288984
ROC train: 0.938002	val: 0.671093	test: 0.628419
PRC train: 0.669618	val: 0.247068	test: 0.217001

Epoch: 41
Loss: 0.1509920947899578
ROC train: 0.936125	val: 0.683405	test: 0.658839
PRC train: 0.667275	val: 0.256305	test: 0.242186

Epoch: 42
Loss: 0.14957715750373327
ROC train: 0.942200	val: 0.690963	test: 0.647561
PRC train: 0.693361	val: 0.279658	test: 0.250164

Epoch: 43
Loss: 0.147839910584691
ROC train: 0.944180	val: 0.684118	test: 0.633979
PRC train: 0.698205	val: 0.278172	test: 0.233064

Epoch: 44
Loss: 0.14638548241443863
ROC train: 0.950862	val: 0.681744	test: 0.634309
PRC train: 0.723004	val: 0.258587	test: 0.222347

Epoch: 45
Loss: 0.14497728445345676
ROC train: 0.949382	val: 0.682716	test: 0.638529
PRC train: 0.712962	val: 0.266216	test: 0.241299

Epoch: 46
Loss: 0.1440223111241354
ROC train: 0.951904	val: 0.687873	test: 0.640098
PRC train: 0.728612	val: 0.273011	test: 0.224818

Epoch: 47
Loss: 0.14152713093948932
ROC train: 0.952111	val: 0.688530	test: 0.655202
PRC train: 0.723476	val: 0.274313	test: 0.238419

Epoch: 48
Loss: 0.1393368422202603
ROC train: 0.958290	val: 0.682402	test: 0.624845
PRC train: 0.749213	val: 0.263486	test: 0.226020

Epoch: 49
Loss: 0.13757981888015564
ROC train: 0.954893	val: 0.693226	test: 0.656199
PRC train: 0.736699	val: 0.264412	test: 0.239609

Epoch: 50
Loss: 0.13785603603848035
ROC train: 0.958692	val: 0.699499	test: 0.652419
PRC train: 0.753842	val: 0.274461	test: 0.240920

Epoch: 51
Loss: 0.1359246847552399
ROC train: 0.959545	val: 0.688524	test: 0.629397
PRC train: 0.761163	val: 0.281257	test: 0.226629

Epoch: 52
Loss: 0.1330761478780175
ROC train: 0.962212	val: 0.710790	test: 0.663000
PRC train: 0.771532	val: 0.297901	test: 0.255455

Epoch: 53
Loss: 0.13699833800001018
ROC train: 0.962174	val: 0.698568	test: 0.653977
PRC train: 0.763300	val: 0.286242	test: 0.252984

Epoch: 54
Loss: 0.13215649953652522
ROC train: 0.964154	val: 0.694536	test: 0.642156
PRC train: 0.773307	val: 0.251832	test: 0.209922

Epoch: 55
Loss: 0.12980454155534674
ROC train: 0.965390	val: 0.690843	test: 0.640646
PRC train: 0.781981	val: 0.267612	test: 0.237824

Epoch: 56
Loss: 0.13042216642125934
ROC train: 0.965299	val: 0.700956	test: 0.645749
PRC train: 0.782405	val: 0.269953	test: 0.217347

Epoch: 57
Loss: 0.12890724706971018
ROC train: 0.968254	val: 0.696142	test: 0.653238
PRC train: 0.797908	val: 0.278322	test: 0.245910

Epoch: 58
Loss: 0.12493718675980521
ROC train: 0.970603	val: 0.696147	test: 0.643980
PRC train: 0.813682	val: 0.271576	test: 0.236861

Epoch: 59
Loss: 0.12561505240398094
ROC train: 0.970753	val: 0.666997	test: 0.633063
PRC train: 0.813654	val: 0.243614	test: 0.230505

Epoch: 60
Loss: 0.12359385245303887
ROC train: 0.973221	val: 0.702384	test: 0.661574
PRC train: 0.823188	val: 0.275733	test: 0.241798

Epoch: 61
Loss: 0.12380463750583641
ROC train: 0.972272	val: 0.677802	test: 0.634288
PRC train: 0.819106	val: 0.262093	test: 0.227653

Epoch: 62
Loss: 0.12415167160722143
ROC train: 0.971681	val: 0.692022	test: 0.642592
PRC train: 0.816355	val: 0.285703	test: 0.251736

Epoch: 63
Loss: 0.11969747560717678
ROC train: 0.976500	val: 0.681890	test: 0.644529
PRC train: 0.844202	val: 0.266533	test: 0.240716

Epoch: 64
Loss: 0.12017198477030112
ROC train: 0.975212	val: 0.692599	test: 0.635516
PRC train: 0.829081	val: 0.280974	test: 0.232333

Epoch: 65
Loss: 0.11835547434956215
ROC train: 0.972564	val: 0.681323	test: 0.641478
PRC train: 0.809713	val: 0.246823	test: 0.203243

Epoch: 66
Loss: 0.11927009369360576
ROC train: 0.978565	val: 0.703200	test: 0.651114
PRC train: 0.843779	val: 0.283296	test: 0.233037

Epoch: 67
Loss: 0.11830166949696858
ROC train: 0.979413	val: 0.699364	test: 0.648981
PRC train: 0.852725	val: 0.289341	test: 0.240880

Epoch: 68
Loss: 0.1171686409871725
ROC train: 0.980907	val: 0.693391	test: 0.642780
PRC train: 0.866671	val: 0.262679	test: 0.224485

Epoch: 69
Loss: 0.11587189470689196
ROC train: 0.981667	val: 0.687185	test: 0.640069
PRC train: 0.861034	val: 0.270001	test: 0.240502

Epoch: 70
Loss: 0.11436315261510036
ROC train: 0.980223	val: 0.673819	test: 0.630705
PRC train: 0.853822	val: 0.262875	test: 0.228978

Epoch: 71
Loss: 0.11099987970055711
ROC train: 0.984726	val: 0.674363	test: 0.624320
PRC train: 0.880082	val: 0.251045	test: 0.212472

Epoch: 72
Loss: 0.10923222246353519
ROC train: 0.982825	val: 0.673027	test: 0.618686
PRC train: 0.867599	val: 0.257014	test: 0.223838

Epoch: 73
Loss: 0.10924317432866468
ROC train: 0.983420	val: 0.688499	test: 0.632436
PRC train: 0.878251	val: 0.262483	test: 0.232272

Epoch: 74
Loss: 0.1064371929704624
ROC train: 0.986133	val: 0.673937	test: 0.637432
PRC train: 0.888990	val: 0.259263	test: 0.217576

Epoch: 75
Loss: 0.10752156988975377
ROC train: 0.985827	val: 0.691486	test: 0.646405
PRC train: 0.889217	val: 0.270154	test: 0.235270

Epoch: 76
Loss: 0.10667643162234214
ROC train: 0.985890	val: 0.697742	test: 0.639415
PRC train: 0.893350	val: 0.275431	test: 0.250706

Epoch: 77
Loss: 0.10489808811388655
ROC train: 0.987386	val: 0.693059	test: 0.654043
PRC train: 0.898154	val: 0.277920	test: 0.242056

Epoch: 78
Loss: 0.10021883071667347
ROC train: 0.988869	val: 0.689224	test: 0.631167
PRC train: 0.902037	val: 0.262359	test: 0.224116

Epoch: 79
Loss: 0.10268304866082083
ROC train: 0.988497	val: 0.682668	test: 0.630634
PRC train: 0.899043	val: 0.265832	test: 0.231598

Epoch: 80
Loss: 0.09903621015140723
ROC train: 0.989996	val: 0.693236	test: 0.633981
PRC train: 0.918727	val: 0.253729	test: 0.214062

Epoch: 81
Loss: 0.09825391087133119
ROC train: 0.989892	val: 0.696685	test: 0.651508
PRC train: 0.915707	val: 0.263212	test: 0.225766

Epoch: 82
Loss: 0.09898564862582653
ROC train: 0.990742	val: 0.663429	test: 0.622221
PRC train: 0.919486	val: 0.240102	test: 0.224567

Epoch: 83
Loss: 0.09916248974064219
ROC train: 0.990977	val: 0.693989	test: 0.652104
PRC train: 0.916405	val: 0.278614	test: 0.233857

Epoch: 84
Loss: 0.09686358572195733
ROC train: 0.992280	val: 0.678930	test: 0.638839
PRC train: 0.932075	val: 0.251280	test: 0.220248

Epoch: 85
Loss: 0.09751474774164466
ROC train: 0.991122	val: 0.675145	test: 0.633573
PRC train: 0.924281	val: 0.252298	test: 0.215722

Epoch: 86
Loss: 0.09543308205944412
ROC train: 0.991175	val: 0.666581	test: 0.624357
PRC train: 0.920505	val: 0.236945	test: 0.216848

Epoch: 87
Loss: 0.09444472258943776
ROC train: 0.992988	val: 0.668590	test: 0.633840
PRC train: 0.935041	val: 0.252043	test: 0.229124

Epoch: 88
Loss: 0.0926075121432674
ROC train: 0.992512	val: 0.670214	test: 0.626586
PRC train: 0.931673	val: 0.248738	test: 0.219553

Epoch: 89
Loss: 0.09346016795648081
ROC train: 0.991276	val: 0.702762	test: 0.664273
PRC train: 0.926739	val: 0.270783	test: 0.229282

Epoch: 90
Loss: 0.09255483788140939
ROC train: 0.993666	val: 0.682721	test: 0.634729
PRC train: 0.941747	val: 0.265354	test: 0.235721

Epoch: 91
Loss: 0.0900915047592877
ROC train: 0.994221	val: 0.679291	test: 0.624907
PRC train: 0.944089	val: 0.237248	test: 0.205281

Epoch: 92
Loss: 0.08765208524084629
ROC train: 0.994047	val: 0.693425	test: 0.643992
PRC train: 0.944223	val: 0.268303	test: 0.233147

Epoch: 93
Loss: 0.08432405724677963
ROC train: 0.995302	val: 0.680699	test: 0.633937
PRC train: 0.647069	val: 0.355333	test: 0.331277

Epoch: 33
Loss: 0.14919051871269862
ROC train: 0.925487	val: 0.747480	test: 0.717479
PRC train: 0.638362	val: 0.336092	test: 0.329548

Epoch: 34
Loss: 0.15036843479050122
ROC train: 0.930790	val: 0.776634	test: 0.738901
PRC train: 0.660204	val: 0.369180	test: 0.352941

Epoch: 35
Loss: 0.1476142519065712
ROC train: 0.932483	val: 0.774948	test: 0.739842
PRC train: 0.664972	val: 0.367636	test: 0.341063

Epoch: 36
Loss: 0.1485779076534747
ROC train: 0.930797	val: 0.763279	test: 0.736092
PRC train: 0.657287	val: 0.353129	test: 0.361563

Epoch: 37
Loss: 0.14682961565305358
ROC train: 0.937348	val: 0.760485	test: 0.732455
PRC train: 0.677982	val: 0.355388	test: 0.341769

Epoch: 38
Loss: 0.14527310128619014
ROC train: 0.939099	val: 0.761131	test: 0.737553
PRC train: 0.689957	val: 0.354530	test: 0.351297

Epoch: 39
Loss: 0.14435584127393988
ROC train: 0.940138	val: 0.765470	test: 0.733916
PRC train: 0.689518	val: 0.350427	test: 0.345766

Epoch: 40
Loss: 0.14385110995425612
ROC train: 0.940368	val: 0.746770	test: 0.711733
PRC train: 0.684442	val: 0.343889	test: 0.326134

Epoch: 41
Loss: 0.1431125923005113
ROC train: 0.945368	val: 0.760469	test: 0.738123
PRC train: 0.712671	val: 0.355681	test: 0.325767

Epoch: 42
Loss: 0.14025609446369983
ROC train: 0.945240	val: 0.764263	test: 0.733673
PRC train: 0.712369	val: 0.358611	test: 0.345685

Epoch: 43
Loss: 0.14151451548245697
ROC train: 0.944784	val: 0.763703	test: 0.734312
PRC train: 0.700691	val: 0.362333	test: 0.332730

Epoch: 44
Loss: 0.13937766423764672
ROC train: 0.948947	val: 0.748516	test: 0.721838
PRC train: 0.730651	val: 0.352856	test: 0.326282

Epoch: 45
Loss: 0.13988451278193723
ROC train: 0.948147	val: 0.751012	test: 0.722783
PRC train: 0.717186	val: 0.345845	test: 0.340125

Epoch: 46
Loss: 0.13937118113981387
ROC train: 0.951892	val: 0.763084	test: 0.733445
PRC train: 0.738058	val: 0.359183	test: 0.341515

Epoch: 47
Loss: 0.13506670540272317
ROC train: 0.954327	val: 0.763117	test: 0.730935
PRC train: 0.745508	val: 0.365289	test: 0.343149

Epoch: 48
Loss: 0.13305178400844922
ROC train: 0.954257	val: 0.760923	test: 0.729474
PRC train: 0.746565	val: 0.354560	test: 0.342344

Epoch: 49
Loss: 0.13430200449182586
ROC train: 0.956460	val: 0.767282	test: 0.736088
PRC train: 0.757063	val: 0.363870	test: 0.352607

Epoch: 50
Loss: 0.13293313788659267
ROC train: 0.957589	val: 0.768454	test: 0.745858
PRC train: 0.759279	val: 0.365531	test: 0.360017

Epoch: 51
Loss: 0.13292582139355438
ROC train: 0.959713	val: 0.756035	test: 0.721249
PRC train: 0.765592	val: 0.349331	test: 0.326433

Epoch: 52
Loss: 0.1304761653387015
ROC train: 0.960628	val: 0.760157	test: 0.735350
PRC train: 0.775855	val: 0.352371	test: 0.343885

Epoch: 53
Loss: 0.12969446712771016
ROC train: 0.960921	val: 0.747208	test: 0.704366
PRC train: 0.767304	val: 0.355521	test: 0.332545

Epoch: 54
Loss: 0.12880313811237282
ROC train: 0.961740	val: 0.750732	test: 0.711446
PRC train: 0.776958	val: 0.352198	test: 0.326351

Epoch: 55
Loss: 0.127834272006566
ROC train: 0.963274	val: 0.761016	test: 0.723499
PRC train: 0.780501	val: 0.353738	test: 0.333614

Epoch: 56
Loss: 0.12893095315826567
ROC train: 0.963881	val: 0.755727	test: 0.728221
PRC train: 0.792454	val: 0.353799	test: 0.331281

Epoch: 57
Loss: 0.1254224164737825
ROC train: 0.964367	val: 0.764419	test: 0.727389
PRC train: 0.794935	val: 0.356995	test: 0.350437

Epoch: 58
Loss: 0.1245225853763311
ROC train: 0.965863	val: 0.762846	test: 0.721533
PRC train: 0.791753	val: 0.361794	test: 0.342179

Epoch: 59
Loss: 0.12381984622991961
ROC train: 0.967798	val: 0.746936	test: 0.721719
PRC train: 0.802450	val: 0.353723	test: 0.341559

Epoch: 60
Loss: 0.12255190792129653
ROC train: 0.966342	val: 0.750223	test: 0.713912
PRC train: 0.802222	val: 0.349499	test: 0.349303

Epoch: 61
Loss: 0.12172117922298392
ROC train: 0.967660	val: 0.751133	test: 0.709907
PRC train: 0.804653	val: 0.355145	test: 0.341372

Epoch: 62
Loss: 0.12062009625899021
ROC train: 0.970209	val: 0.761953	test: 0.717234
PRC train: 0.815588	val: 0.371317	test: 0.342201

Epoch: 63
Loss: 0.1204437876436726
ROC train: 0.971556	val: 0.760735	test: 0.717647
PRC train: 0.819203	val: 0.367148	test: 0.346114

Epoch: 64
Loss: 0.11895745306143829
ROC train: 0.969861	val: 0.763252	test: 0.725061
PRC train: 0.816044	val: 0.366835	test: 0.347080

Epoch: 65
Loss: 0.12003505896552187
ROC train: 0.971023	val: 0.757335	test: 0.717745
PRC train: 0.813071	val: 0.355692	test: 0.336485

Epoch: 66
Loss: 0.11934480677006594
ROC train: 0.971153	val: 0.763056	test: 0.717824
PRC train: 0.818562	val: 0.366222	test: 0.349314

Epoch: 67
Loss: 0.11728599254512254
ROC train: 0.967176	val: 0.747128	test: 0.702261
PRC train: 0.801180	val: 0.356371	test: 0.334756

Epoch: 68
Loss: 0.11510912246001795
ROC train: 0.976358	val: 0.760652	test: 0.715766
PRC train: 0.850518	val: 0.363768	test: 0.342112

Epoch: 69
Loss: 0.11428694271827398
ROC train: 0.975069	val: 0.744295	test: 0.713109
PRC train: 0.842672	val: 0.346378	test: 0.338048

Epoch: 70
Loss: 0.11309767102114618
ROC train: 0.978327	val: 0.756876	test: 0.711244
PRC train: 0.858018	val: 0.355337	test: 0.332003

Epoch: 71
Loss: 0.11438886474608172
ROC train: 0.978136	val: 0.748618	test: 0.706862
PRC train: 0.859958	val: 0.346297	test: 0.320269

Epoch: 72
Loss: 0.11048546298375968
ROC train: 0.977622	val: 0.755763	test: 0.711513
PRC train: 0.856094	val: 0.370503	test: 0.341086

Epoch: 73
Loss: 0.10948068655764295
ROC train: 0.979533	val: 0.759803	test: 0.715306
PRC train: 0.867638	val: 0.362943	test: 0.338040

Epoch: 74
Loss: 0.10642213701363695
ROC train: 0.980874	val: 0.757582	test: 0.713971
PRC train: 0.875362	val: 0.352847	test: 0.331876

Epoch: 75
Loss: 0.10972053480798735
ROC train: 0.980699	val: 0.756430	test: 0.717409
PRC train: 0.873205	val: 0.355972	test: 0.347901

Epoch: 76
Loss: 0.10753568989535482
ROC train: 0.981512	val: 0.748222	test: 0.707460
PRC train: 0.877706	val: 0.354944	test: 0.336413

Epoch: 77
Loss: 0.10585562790012841
ROC train: 0.981880	val: 0.752383	test: 0.713296
PRC train: 0.879880	val: 0.350604	test: 0.330230

Epoch: 78
Loss: 0.1047236412716025
ROC train: 0.981944	val: 0.754268	test: 0.709366
PRC train: 0.876791	val: 0.351141	test: 0.325829

Epoch: 79
Loss: 0.10473525404273594
ROC train: 0.983736	val: 0.757651	test: 0.710049
PRC train: 0.888904	val: 0.361914	test: 0.331250

Epoch: 80
Loss: 0.10303318437452189
ROC train: 0.982943	val: 0.757881	test: 0.712165
PRC train: 0.886624	val: 0.349379	test: 0.349646

Epoch: 81
Loss: 0.1028003244038862
ROC train: 0.985274	val: 0.748881	test: 0.708911
PRC train: 0.896083	val: 0.346577	test: 0.318206

Epoch: 82
Loss: 0.10378664758739424
ROC train: 0.984546	val: 0.743813	test: 0.708520
PRC train: 0.895970	val: 0.351034	test: 0.312376

Epoch: 83
Loss: 0.10042628236109977
ROC train: 0.984926	val: 0.744439	test: 0.708992
PRC train: 0.892403	val: 0.358092	test: 0.323953

Epoch: 84
Loss: 0.10032839125801296
ROC train: 0.985313	val: 0.750231	test: 0.697911
PRC train: 0.897661	val: 0.356995	test: 0.339986

Epoch: 85
Loss: 0.09748524553144562
ROC train: 0.986967	val: 0.747838	test: 0.712664
PRC train: 0.907476	val: 0.355007	test: 0.333123

Epoch: 86
Loss: 0.10137814252075597
ROC train: 0.986127	val: 0.758452	test: 0.718246
PRC train: 0.900291	val: 0.362873	test: 0.342044

Epoch: 87
Loss: 0.09853769842613808
ROC train: 0.985878	val: 0.736959	test: 0.697185
PRC train: 0.901597	val: 0.347000	test: 0.329581

Epoch: 88
Loss: 0.09834290889550151
ROC train: 0.986262	val: 0.750683	test: 0.706236
PRC train: 0.899290	val: 0.358379	test: 0.335086

Epoch: 89
Loss: 0.09632732659937057
ROC train: 0.987681	val: 0.755285	test: 0.713943
PRC train: 0.911007	val: 0.353939	test: 0.340462

Epoch: 90
Loss: 0.09621801433219761
ROC train: 0.987977	val: 0.752496	test: 0.719721
PRC train: 0.916879	val: 0.355675	test: 0.325236

Epoch: 91
Loss: 0.09377461068241885
ROC train: 0.988757	val: 0.751979	test: 0.715587
PRC train: 0.920364	val: 0.352209	test: 0.324433

Epoch: 92
Loss: 0.0945480179976458
ROC train: 0.989436	val: 0.746018	test: 0.714396
PRC train: 0.923544	val: 0.353345	test: 0.336380

Epoch: 93
Loss: 0.094827382991662
ROC train: 0.989564	val: 0.750953	test: 0.701781
PRC train: 0.615358	val: 0.336690	test: 0.292900

Epoch: 33
Loss: 0.15911883636805327
ROC train: 0.925226	val: 0.745481	test: 0.707078
PRC train: 0.635718	val: 0.338681	test: 0.314958

Epoch: 34
Loss: 0.15962946522119206
ROC train: 0.928160	val: 0.732346	test: 0.704131
PRC train: 0.643962	val: 0.322199	test: 0.298283

Epoch: 35
Loss: 0.15774238233247745
ROC train: 0.931062	val: 0.717339	test: 0.696620
PRC train: 0.657025	val: 0.308188	test: 0.295450

Epoch: 36
Loss: 0.1575390723963896
ROC train: 0.928154	val: 0.732809	test: 0.707931
PRC train: 0.644547	val: 0.323343	test: 0.312872

Epoch: 37
Loss: 0.15536472941115725
ROC train: 0.934292	val: 0.741041	test: 0.696782
PRC train: 0.659912	val: 0.333225	test: 0.297334

Epoch: 38
Loss: 0.15375204925099836
ROC train: 0.934925	val: 0.730852	test: 0.707502
PRC train: 0.666532	val: 0.320695	test: 0.295724

Epoch: 39
Loss: 0.1544944327784857
ROC train: 0.938452	val: 0.729105	test: 0.691092
PRC train: 0.683269	val: 0.323253	test: 0.303441

Epoch: 40
Loss: 0.14965784813785446
ROC train: 0.935981	val: 0.715864	test: 0.682284
PRC train: 0.664615	val: 0.299455	test: 0.296411

Epoch: 41
Loss: 0.15158952231580755
ROC train: 0.941993	val: 0.713824	test: 0.688691
PRC train: 0.698675	val: 0.311197	test: 0.295737

Epoch: 42
Loss: 0.1499268013745411
ROC train: 0.945164	val: 0.707248	test: 0.675641
PRC train: 0.706598	val: 0.297461	test: 0.284899

Epoch: 43
Loss: 0.14584496466957197
ROC train: 0.948649	val: 0.734837	test: 0.701102
PRC train: 0.714616	val: 0.319668	test: 0.292397

Epoch: 44
Loss: 0.14470347435776607
ROC train: 0.946991	val: 0.733858	test: 0.691909
PRC train: 0.712701	val: 0.330904	test: 0.299969

Epoch: 45
Loss: 0.1441833996623496
ROC train: 0.952137	val: 0.726774	test: 0.698617
PRC train: 0.730137	val: 0.333449	test: 0.292496

Epoch: 46
Loss: 0.14452866605183728
ROC train: 0.949385	val: 0.733102	test: 0.686696
PRC train: 0.722714	val: 0.339170	test: 0.285295

Epoch: 47
Loss: 0.14197289044298844
ROC train: 0.954307	val: 0.736293	test: 0.702945
PRC train: 0.739835	val: 0.334824	test: 0.298135

Epoch: 48
Loss: 0.14198109356317673
ROC train: 0.956142	val: 0.742374	test: 0.711147
PRC train: 0.749960	val: 0.346527	test: 0.311097

Epoch: 49
Loss: 0.13937216099259264
ROC train: 0.955872	val: 0.743481	test: 0.705293
PRC train: 0.751076	val: 0.344439	test: 0.316739

Epoch: 50
Loss: 0.13697190534082795
ROC train: 0.959653	val: 0.737175	test: 0.704403
PRC train: 0.764485	val: 0.337655	test: 0.319718

Epoch: 51
Loss: 0.13512080571073237
ROC train: 0.961851	val: 0.743949	test: 0.708884
PRC train: 0.781078	val: 0.344084	test: 0.331488

Epoch: 52
Loss: 0.13398093803257383
ROC train: 0.962699	val: 0.737613	test: 0.698167
PRC train: 0.782936	val: 0.340710	test: 0.307656

Epoch: 53
Loss: 0.1349814392727797
ROC train: 0.963930	val: 0.727909	test: 0.690977
PRC train: 0.783437	val: 0.328247	test: 0.312835

Epoch: 54
Loss: 0.13205352506280776
ROC train: 0.962864	val: 0.736910	test: 0.702425
PRC train: 0.775799	val: 0.342649	test: 0.320115

Epoch: 55
Loss: 0.1299846016514829
ROC train: 0.967661	val: 0.741715	test: 0.694099
PRC train: 0.798951	val: 0.338227	test: 0.308038

Epoch: 56
Loss: 0.13023838075366614
ROC train: 0.968224	val: 0.736887	test: 0.697335
PRC train: 0.804104	val: 0.344040	test: 0.308919

Epoch: 57
Loss: 0.12617387995141488
ROC train: 0.969693	val: 0.738617	test: 0.684880
PRC train: 0.802551	val: 0.327743	test: 0.302769

Epoch: 58
Loss: 0.12662804147429177
ROC train: 0.972570	val: 0.737464	test: 0.682246
PRC train: 0.816184	val: 0.331633	test: 0.289180

Epoch: 59
Loss: 0.12429885861328682
ROC train: 0.972897	val: 0.732563	test: 0.696224
PRC train: 0.817170	val: 0.328079	test: 0.314515

Epoch: 60
Loss: 0.12161550981008652
ROC train: 0.972048	val: 0.734421	test: 0.696183
PRC train: 0.822651	val: 0.347438	test: 0.303394

Epoch: 61
Loss: 0.12361952438895935
ROC train: 0.975039	val: 0.722160	test: 0.680573
PRC train: 0.830293	val: 0.314161	test: 0.279953

Epoch: 62
Loss: 0.12379920674599813
ROC train: 0.975740	val: 0.741641	test: 0.693018
PRC train: 0.837661	val: 0.341911	test: 0.314700

Epoch: 63
Loss: 0.12193489775201298
ROC train: 0.976788	val: 0.730263	test: 0.668260
PRC train: 0.841629	val: 0.331745	test: 0.295368

Epoch: 64
Loss: 0.11960790061690638
ROC train: 0.977399	val: 0.734351	test: 0.683656
PRC train: 0.847431	val: 0.348867	test: 0.311437

Epoch: 65
Loss: 0.11859818318848168
ROC train: 0.978389	val: 0.726430	test: 0.681963
PRC train: 0.849852	val: 0.326048	test: 0.288044

Epoch: 66
Loss: 0.1154236495977527
ROC train: 0.979703	val: 0.726172	test: 0.675252
PRC train: 0.853761	val: 0.339529	test: 0.299477

Epoch: 67
Loss: 0.11502341587085933
ROC train: 0.980831	val: 0.707984	test: 0.660759
PRC train: 0.867776	val: 0.308171	test: 0.275010

Epoch: 68
Loss: 0.11464694740725456
ROC train: 0.980468	val: 0.723208	test: 0.675856
PRC train: 0.862376	val: 0.340350	test: 0.300282

Epoch: 69
Loss: 0.11483835932435785
ROC train: 0.981820	val: 0.724509	test: 0.684410
PRC train: 0.864954	val: 0.338359	test: 0.300513

Epoch: 70
Loss: 0.11268335564303593
ROC train: 0.981046	val: 0.707385	test: 0.672181
PRC train: 0.867888	val: 0.295895	test: 0.267550

Epoch: 71
Loss: 0.11293908598279101
ROC train: 0.983595	val: 0.727837	test: 0.666994
PRC train: 0.879630	val: 0.320115	test: 0.270765

Epoch: 72
Loss: 0.10994907236652024
ROC train: 0.984917	val: 0.728145	test: 0.677672
PRC train: 0.888197	val: 0.335636	test: 0.283372

Epoch: 73
Loss: 0.1108586956601178
ROC train: 0.983611	val: 0.719534	test: 0.676236
PRC train: 0.876086	val: 0.321789	test: 0.292615

Epoch: 74
Loss: 0.10820178152075936
ROC train: 0.984674	val: 0.720477	test: 0.676892
PRC train: 0.883690	val: 0.298853	test: 0.279020

Epoch: 75
Loss: 0.10830089672365663
ROC train: 0.987385	val: 0.729306	test: 0.679313
PRC train: 0.896504	val: 0.322882	test: 0.287990

Epoch: 76
Loss: 0.10572572853846841
ROC train: 0.988044	val: 0.724583	test: 0.676676
PRC train: 0.899922	val: 0.325129	test: 0.294697

Epoch: 77
Loss: 0.102401211656168
ROC train: 0.988876	val: 0.719300	test: 0.661717
PRC train: 0.909374	val: 0.319905	test: 0.281817

Epoch: 78
Loss: 0.10279898665309238
ROC train: 0.989248	val: 0.724112	test: 0.679625
PRC train: 0.912977	val: 0.328796	test: 0.297096

Epoch: 79
Loss: 0.10230912722429561
ROC train: 0.990474	val: 0.718448	test: 0.679846
PRC train: 0.920653	val: 0.318463	test: 0.284643

Epoch: 80
Loss: 0.10159138106937927
ROC train: 0.990648	val: 0.725942	test: 0.677483
PRC train: 0.921103	val: 0.320320	test: 0.284228

Epoch: 81
Loss: 0.09898237278472644
ROC train: 0.991373	val: 0.717210	test: 0.682108
PRC train: 0.921215	val: 0.315261	test: 0.295232

Epoch: 82
Loss: 0.09825719554540271
ROC train: 0.991884	val: 0.732845	test: 0.691962
PRC train: 0.929555	val: 0.315728	test: 0.282822

Epoch: 83
Loss: 0.09692995898197589
ROC train: 0.992240	val: 0.721302	test: 0.679451
PRC train: 0.930422	val: 0.313598	test: 0.291111

Epoch: 84
Loss: 0.09695226317451669
ROC train: 0.991770	val: 0.722968	test: 0.681044
PRC train: 0.931566	val: 0.335459	test: 0.298643

Epoch: 85
Loss: 0.09612499828665473
ROC train: 0.992818	val: 0.724329	test: 0.680308
PRC train: 0.934518	val: 0.309587	test: 0.289251

Epoch: 86
Loss: 0.09376719549086193
ROC train: 0.992771	val: 0.717723	test: 0.673765
PRC train: 0.934548	val: 0.322978	test: 0.294084

Epoch: 87
Loss: 0.09376904682811275
ROC train: 0.992981	val: 0.714960	test: 0.675967
PRC train: 0.937398	val: 0.305719	test: 0.280233

Epoch: 88
Loss: 0.09314229812059305
ROC train: 0.993993	val: 0.719122	test: 0.673148
PRC train: 0.944012	val: 0.324328	test: 0.288913

Epoch: 89
Loss: 0.09090337216231628
ROC train: 0.994019	val: 0.711116	test: 0.666109
PRC train: 0.943372	val: 0.316189	test: 0.291058

Epoch: 90
Loss: 0.0897964636248667
ROC train: 0.993390	val: 0.719258	test: 0.679273
PRC train: 0.946021	val: 0.333246	test: 0.293470

Epoch: 91
Loss: 0.08807976859235536
ROC train: 0.995183	val: 0.707014	test: 0.670587
PRC train: 0.953564	val: 0.313732	test: 0.283063

Epoch: 92
Loss: 0.08915355727992987
ROC train: 0.994379	val: 0.719982	test: 0.669453
PRC train: 0.951450	val: 0.323190	test: 0.292225

Epoch: 93
Loss: 0.08948364786441601
ROC train: 0.994731	val: 0.723741	test: 0.683308
PRC train: 0.654440	val: 0.361872	test: 0.365465

Epoch: 33
Loss: 0.1509642332880267
ROC train: 0.925641	val: 0.757802	test: 0.724508
PRC train: 0.648812	val: 0.316941	test: 0.340891

Epoch: 34
Loss: 0.15064384305978742
ROC train: 0.929646	val: 0.756807	test: 0.721419
PRC train: 0.672069	val: 0.338495	test: 0.349857

Epoch: 35
Loss: 0.14994812243463132
ROC train: 0.931806	val: 0.767582	test: 0.733749
PRC train: 0.679793	val: 0.356247	test: 0.369271

Epoch: 36
Loss: 0.14952526810461797
ROC train: 0.929618	val: 0.770255	test: 0.734185
PRC train: 0.663376	val: 0.344263	test: 0.356583

Epoch: 37
Loss: 0.14918617799231473
ROC train: 0.932109	val: 0.763033	test: 0.734432
PRC train: 0.681323	val: 0.339189	test: 0.355445

Epoch: 38
Loss: 0.14703928017261783
ROC train: 0.933979	val: 0.759488	test: 0.738832
PRC train: 0.681544	val: 0.337084	test: 0.360146

Epoch: 39
Loss: 0.14633788957995683
ROC train: 0.936897	val: 0.746173	test: 0.729966
PRC train: 0.686676	val: 0.321590	test: 0.336834

Epoch: 40
Loss: 0.14571114053699782
ROC train: 0.940619	val: 0.761272	test: 0.728164
PRC train: 0.701636	val: 0.340844	test: 0.348921

Epoch: 41
Loss: 0.14368470950144185
ROC train: 0.940861	val: 0.755879	test: 0.736152
PRC train: 0.713028	val: 0.339988	test: 0.364594

Epoch: 42
Loss: 0.14271569891994482
ROC train: 0.942897	val: 0.755742	test: 0.731023
PRC train: 0.706835	val: 0.339696	test: 0.362600

Epoch: 43
Loss: 0.14116229596117336
ROC train: 0.944928	val: 0.762591	test: 0.742263
PRC train: 0.714823	val: 0.346983	test: 0.364640

Epoch: 44
Loss: 0.13937089730816532
ROC train: 0.946163	val: 0.748722	test: 0.732939
PRC train: 0.725833	val: 0.334626	test: 0.357821

Epoch: 45
Loss: 0.13755804878741512
ROC train: 0.950137	val: 0.748713	test: 0.736995
PRC train: 0.740420	val: 0.338421	test: 0.370129

Epoch: 46
Loss: 0.13823179647499081
ROC train: 0.950078	val: 0.762323	test: 0.740610
PRC train: 0.736486	val: 0.345629	test: 0.368390

Epoch: 47
Loss: 0.13659366943107526
ROC train: 0.952297	val: 0.753846	test: 0.737963
PRC train: 0.744107	val: 0.329349	test: 0.369729

Epoch: 48
Loss: 0.13842637747964207
ROC train: 0.951796	val: 0.757603	test: 0.735657
PRC train: 0.747046	val: 0.328494	test: 0.338529

Epoch: 49
Loss: 0.13528768148506962
ROC train: 0.951021	val: 0.757336	test: 0.736392
PRC train: 0.744876	val: 0.328036	test: 0.346727

Epoch: 50
Loss: 0.1351693830027173
ROC train: 0.954399	val: 0.754320	test: 0.733819
PRC train: 0.754023	val: 0.347255	test: 0.361039

Epoch: 51
Loss: 0.1336971053937753
ROC train: 0.956436	val: 0.762660	test: 0.723601
PRC train: 0.766645	val: 0.359767	test: 0.357384

Epoch: 52
Loss: 0.13020400646939817
ROC train: 0.958245	val: 0.745636	test: 0.716010
PRC train: 0.776251	val: 0.346613	test: 0.348315

Epoch: 53
Loss: 0.13233317653037963
ROC train: 0.959457	val: 0.759127	test: 0.727475
PRC train: 0.770770	val: 0.338093	test: 0.352251

Epoch: 54
Loss: 0.13010208161628037
ROC train: 0.960850	val: 0.755475	test: 0.733487
PRC train: 0.780350	val: 0.339833	test: 0.361241

Epoch: 55
Loss: 0.12728321847560453
ROC train: 0.961277	val: 0.757875	test: 0.736425
PRC train: 0.790260	val: 0.342557	test: 0.362303

Epoch: 56
Loss: 0.12706042785928431
ROC train: 0.963900	val: 0.751094	test: 0.731766
PRC train: 0.794445	val: 0.338946	test: 0.364733

Epoch: 57
Loss: 0.1251474825337247
ROC train: 0.963683	val: 0.755873	test: 0.730687
PRC train: 0.793488	val: 0.346185	test: 0.378076

Epoch: 58
Loss: 0.12440824790930377
ROC train: 0.966392	val: 0.761124	test: 0.737681
PRC train: 0.806465	val: 0.344860	test: 0.365712

Epoch: 59
Loss: 0.12481013712630709
ROC train: 0.965564	val: 0.743069	test: 0.718020
PRC train: 0.806726	val: 0.321863	test: 0.329306

Epoch: 60
Loss: 0.12059034662772657
ROC train: 0.966625	val: 0.754031	test: 0.742293
PRC train: 0.813723	val: 0.344358	test: 0.357179

Epoch: 61
Loss: 0.1225137702370781
ROC train: 0.968800	val: 0.762097	test: 0.726893
PRC train: 0.816974	val: 0.357787	test: 0.358748

Epoch: 62
Loss: 0.12048254838183677
ROC train: 0.967284	val: 0.768002	test: 0.732033
PRC train: 0.809481	val: 0.357534	test: 0.371039

Epoch: 63
Loss: 0.12058207759559655
ROC train: 0.969167	val: 0.754200	test: 0.730891
PRC train: 0.817870	val: 0.347682	test: 0.364951

Epoch: 64
Loss: 0.11973109592424386
ROC train: 0.967800	val: 0.755864	test: 0.727312
PRC train: 0.810283	val: 0.340411	test: 0.356116

Epoch: 65
Loss: 0.1176837161049332
ROC train: 0.973037	val: 0.749010	test: 0.732907
PRC train: 0.836588	val: 0.352012	test: 0.358486

Epoch: 66
Loss: 0.11712612262079597
ROC train: 0.973253	val: 0.763426	test: 0.727877
PRC train: 0.832757	val: 0.361740	test: 0.364494

Epoch: 67
Loss: 0.11499765676018192
ROC train: 0.974520	val: 0.767552	test: 0.740565
PRC train: 0.843835	val: 0.356537	test: 0.374431

Epoch: 68
Loss: 0.11615577407673269
ROC train: 0.974477	val: 0.753921	test: 0.736407
PRC train: 0.852537	val: 0.346061	test: 0.354416

Epoch: 69
Loss: 0.11342345606638514
ROC train: 0.976789	val: 0.760800	test: 0.731743
PRC train: 0.856060	val: 0.339815	test: 0.368111

Epoch: 70
Loss: 0.11283935557425305
ROC train: 0.972069	val: 0.760346	test: 0.727750
PRC train: 0.840942	val: 0.365339	test: 0.370921

Epoch: 71
Loss: 0.1131996952289653
ROC train: 0.977159	val: 0.752962	test: 0.727939
PRC train: 0.851834	val: 0.347201	test: 0.356956

Epoch: 72
Loss: 0.1128402116626256
ROC train: 0.977804	val: 0.762721	test: 0.717584
PRC train: 0.861802	val: 0.361525	test: 0.365484

Epoch: 73
Loss: 0.11159274029869991
ROC train: 0.980498	val: 0.751361	test: 0.728292
PRC train: 0.875585	val: 0.330864	test: 0.349049

Epoch: 74
Loss: 0.10942189618336565
ROC train: 0.980309	val: 0.738174	test: 0.722760
PRC train: 0.872859	val: 0.314865	test: 0.333901

Epoch: 75
Loss: 0.11011008365992488
ROC train: 0.980713	val: 0.754817	test: 0.723407
PRC train: 0.875834	val: 0.342061	test: 0.350079

Epoch: 76
Loss: 0.1087224129056535
ROC train: 0.981616	val: 0.753531	test: 0.726131
PRC train: 0.881604	val: 0.340546	test: 0.355519

Epoch: 77
Loss: 0.10716739531400128
ROC train: 0.982895	val: 0.751968	test: 0.729004
PRC train: 0.892635	val: 0.326682	test: 0.357171

Epoch: 78
Loss: 0.10533561216777208
ROC train: 0.982370	val: 0.748528	test: 0.736468
PRC train: 0.886353	val: 0.329908	test: 0.354935

Epoch: 79
Loss: 0.10338828903301925
ROC train: 0.984071	val: 0.745923	test: 0.723904
PRC train: 0.895818	val: 0.330310	test: 0.339981

Epoch: 80
Loss: 0.10338994556016319
ROC train: 0.984189	val: 0.756208	test: 0.726201
PRC train: 0.896329	val: 0.329859	test: 0.340225

Epoch: 81
Loss: 0.10168988387777335
ROC train: 0.980611	val: 0.751709	test: 0.725999
PRC train: 0.874883	val: 0.340157	test: 0.361223

Epoch: 82
Loss: 0.10252779108619464
ROC train: 0.984344	val: 0.743933	test: 0.725359
PRC train: 0.894997	val: 0.304735	test: 0.315711

Epoch: 83
Loss: 0.1005832355918237
ROC train: 0.986130	val: 0.761503	test: 0.732415
PRC train: 0.903416	val: 0.335221	test: 0.365826

Epoch: 84
Loss: 0.09887497204011214
ROC train: 0.985476	val: 0.763405	test: 0.735149
PRC train: 0.906187	val: 0.336552	test: 0.351626

Epoch: 85
Loss: 0.09795933860638378
ROC train: 0.985775	val: 0.746436	test: 0.718115
PRC train: 0.902914	val: 0.330924	test: 0.357412

Epoch: 86
Loss: 0.09752000013739115
ROC train: 0.987001	val: 0.750252	test: 0.726357
PRC train: 0.910104	val: 0.338959	test: 0.354520

Epoch: 87
Loss: 0.09533927499624616
ROC train: 0.987445	val: 0.750452	test: 0.723268
PRC train: 0.913268	val: 0.333002	test: 0.353337

Epoch: 88
Loss: 0.09660589924120368
ROC train: 0.988776	val: 0.755507	test: 0.726085
PRC train: 0.923823	val: 0.339640	test: 0.355653

Epoch: 89
Loss: 0.09611805981059547
ROC train: 0.988620	val: 0.759215	test: 0.724836
PRC train: 0.920160	val: 0.342217	test: 0.353069

Epoch: 90
Loss: 0.09394435340606959
ROC train: 0.989055	val: 0.759223	test: 0.728429
PRC train: 0.922844	val: 0.337595	test: 0.353656

Epoch: 91
Loss: 0.09499419204522691
ROC train: 0.989960	val: 0.754639	test: 0.734935
PRC train: 0.931519	val: 0.337761	test: 0.366146

Epoch: 92
Loss: 0.09486456743867686
ROC train: 0.987336	val: 0.752472	test: 0.726295
PRC train: 0.915124	val: 0.337667	test: 0.344992

Epoch: 93
Loss: 0.09232282368686327
ROC train: 0.990543	val: 0.750577	test: 0.724745
PRC train: 0.635816	val: 0.346527	test: 0.329414

Epoch: 33
Loss: 0.15301009614507735
ROC train: 0.926371	val: 0.760565	test: 0.706954
PRC train: 0.644346	val: 0.350225	test: 0.318163

Epoch: 34
Loss: 0.15327867735325004
ROC train: 0.931106	val: 0.758346	test: 0.714735
PRC train: 0.662824	val: 0.346524	test: 0.327547

Epoch: 35
Loss: 0.15268508124844904
ROC train: 0.931066	val: 0.759650	test: 0.715061
PRC train: 0.668045	val: 0.352855	test: 0.328592

Epoch: 36
Loss: 0.148054749505982
ROC train: 0.935842	val: 0.761786	test: 0.717522
PRC train: 0.679697	val: 0.355840	test: 0.338810

Epoch: 37
Loss: 0.14532534212012455
ROC train: 0.935932	val: 0.761125	test: 0.714425
PRC train: 0.678520	val: 0.360107	test: 0.329727

Epoch: 38
Loss: 0.14613313070599213
ROC train: 0.935075	val: 0.756715	test: 0.726161
PRC train: 0.679361	val: 0.358109	test: 0.335221

Epoch: 39
Loss: 0.14333997302290855
ROC train: 0.941200	val: 0.747712	test: 0.705854
PRC train: 0.695715	val: 0.354386	test: 0.326909

Epoch: 40
Loss: 0.1445826266511768
ROC train: 0.936849	val: 0.740417	test: 0.701982
PRC train: 0.668686	val: 0.337916	test: 0.319964

Epoch: 41
Loss: 0.14450250026840494
ROC train: 0.939607	val: 0.750662	test: 0.705793
PRC train: 0.687119	val: 0.341009	test: 0.317366

Epoch: 42
Loss: 0.14314978361065508
ROC train: 0.939394	val: 0.756229	test: 0.720633
PRC train: 0.689977	val: 0.354757	test: 0.327736

Epoch: 43
Loss: 0.1432775798967459
ROC train: 0.946754	val: 0.757493	test: 0.704264
PRC train: 0.712555	val: 0.361813	test: 0.321263

Epoch: 44
Loss: 0.14072913606967388
ROC train: 0.950163	val: 0.760527	test: 0.713525
PRC train: 0.724493	val: 0.362114	test: 0.331410

Epoch: 45
Loss: 0.14137334357732764
ROC train: 0.948977	val: 0.753028	test: 0.708052
PRC train: 0.714143	val: 0.355713	test: 0.328341

Epoch: 46
Loss: 0.13639218168122785
ROC train: 0.953164	val: 0.762792	test: 0.719499
PRC train: 0.742128	val: 0.368781	test: 0.336732

Epoch: 47
Loss: 0.13686021845242102
ROC train: 0.954962	val: 0.756080	test: 0.708400
PRC train: 0.740721	val: 0.365983	test: 0.330547

Epoch: 48
Loss: 0.13676795848858356
ROC train: 0.955889	val: 0.769028	test: 0.710197
PRC train: 0.748984	val: 0.373238	test: 0.326196

Epoch: 49
Loss: 0.1344437767829952
ROC train: 0.957853	val: 0.766442	test: 0.721712
PRC train: 0.759451	val: 0.369086	test: 0.335815

Epoch: 50
Loss: 0.13230014152876646
ROC train: 0.955864	val: 0.759125	test: 0.715509
PRC train: 0.750254	val: 0.364946	test: 0.340438

Epoch: 51
Loss: 0.13194824183364834
ROC train: 0.958074	val: 0.756131	test: 0.710970
PRC train: 0.753984	val: 0.352165	test: 0.322433

Epoch: 52
Loss: 0.13111575156150113
ROC train: 0.959800	val: 0.762421	test: 0.728011
PRC train: 0.771163	val: 0.378207	test: 0.340944

Epoch: 53
Loss: 0.1297826082801229
ROC train: 0.961644	val: 0.748700	test: 0.720254
PRC train: 0.767973	val: 0.353447	test: 0.331510

Epoch: 54
Loss: 0.127156582975742
ROC train: 0.965540	val: 0.750385	test: 0.712780
PRC train: 0.792733	val: 0.358684	test: 0.331520

Epoch: 55
Loss: 0.12757649551900993
ROC train: 0.963788	val: 0.758514	test: 0.702911
PRC train: 0.776584	val: 0.358062	test: 0.327636

Epoch: 56
Loss: 0.12476892086806338
ROC train: 0.967653	val: 0.757357	test: 0.708658
PRC train: 0.802034	val: 0.363289	test: 0.327055

Epoch: 57
Loss: 0.1253853310002443
ROC train: 0.964871	val: 0.760488	test: 0.718888
PRC train: 0.784178	val: 0.356156	test: 0.331137

Epoch: 58
Loss: 0.12353550069902342
ROC train: 0.970608	val: 0.749665	test: 0.702241
PRC train: 0.814303	val: 0.354398	test: 0.324593

Epoch: 59
Loss: 0.12241484282982416
ROC train: 0.967291	val: 0.749362	test: 0.712990
PRC train: 0.798101	val: 0.359738	test: 0.328671

Epoch: 60
Loss: 0.12030813708435167
ROC train: 0.971553	val: 0.752020	test: 0.712670
PRC train: 0.822459	val: 0.362467	test: 0.323232

Epoch: 61
Loss: 0.12108630152124485
ROC train: 0.968658	val: 0.754651	test: 0.737235
PRC train: 0.802876	val: 0.346654	test: 0.341087

Epoch: 62
Loss: 0.11938881290504101
ROC train: 0.972373	val: 0.756614	test: 0.711580
PRC train: 0.821413	val: 0.363766	test: 0.323628

Epoch: 63
Loss: 0.11749972748197841
ROC train: 0.975122	val: 0.755146	test: 0.707212
PRC train: 0.834326	val: 0.356436	test: 0.314549

Epoch: 64
Loss: 0.11691339789245689
ROC train: 0.972267	val: 0.753253	test: 0.723725
PRC train: 0.821193	val: 0.348865	test: 0.330792

Epoch: 65
Loss: 0.11585128318584077
ROC train: 0.974725	val: 0.750347	test: 0.710228
PRC train: 0.829771	val: 0.369052	test: 0.334844

Epoch: 66
Loss: 0.11372595096664702
ROC train: 0.975873	val: 0.757716	test: 0.710916
PRC train: 0.837230	val: 0.350782	test: 0.321758

Epoch: 67
Loss: 0.11319957193943936
ROC train: 0.978403	val: 0.749953	test: 0.708196
PRC train: 0.851736	val: 0.362468	test: 0.329058

Epoch: 68
Loss: 0.11202604969809903
ROC train: 0.977304	val: 0.742890	test: 0.704488
PRC train: 0.843397	val: 0.349081	test: 0.330425

Epoch: 69
Loss: 0.11234731611377301
ROC train: 0.977900	val: 0.753554	test: 0.705304
PRC train: 0.848207	val: 0.365734	test: 0.312195

Epoch: 70
Loss: 0.11272488696410221
ROC train: 0.981097	val: 0.740665	test: 0.698075
PRC train: 0.861352	val: 0.347862	test: 0.320585

Epoch: 71
Loss: 0.10962860311137236
ROC train: 0.981636	val: 0.741503	test: 0.704031
PRC train: 0.867722	val: 0.366063	test: 0.327351

Epoch: 72
Loss: 0.10673415691073818
ROC train: 0.982075	val: 0.748467	test: 0.707252
PRC train: 0.868523	val: 0.358695	test: 0.338507

Epoch: 73
Loss: 0.10747360600611122
ROC train: 0.981890	val: 0.754637	test: 0.713248
PRC train: 0.868747	val: 0.358565	test: 0.323296

Epoch: 74
Loss: 0.107788024911642
ROC train: 0.984738	val: 0.754441	test: 0.711254
PRC train: 0.883310	val: 0.355339	test: 0.315433

Epoch: 75
Loss: 0.10613142931670126
ROC train: 0.983689	val: 0.754700	test: 0.716140
PRC train: 0.879301	val: 0.349326	test: 0.322675

Epoch: 76
Loss: 0.1022593698513407
ROC train: 0.986216	val: 0.757889	test: 0.715407
PRC train: 0.889894	val: 0.370936	test: 0.327388

Epoch: 77
Loss: 0.10223809341579605
ROC train: 0.985316	val: 0.748424	test: 0.710220
PRC train: 0.887643	val: 0.351932	test: 0.316384

Epoch: 78
Loss: 0.10328500416546625
ROC train: 0.984495	val: 0.747487	test: 0.681430
PRC train: 0.885340	val: 0.340931	test: 0.288486

Epoch: 79
Loss: 0.10375978485682254
ROC train: 0.986048	val: 0.746749	test: 0.700808
PRC train: 0.893322	val: 0.344352	test: 0.308791

Epoch: 80
Loss: 0.0987365037270174
ROC train: 0.987129	val: 0.744800	test: 0.706737
PRC train: 0.898283	val: 0.335342	test: 0.305592

Epoch: 81
Loss: 0.09715219904188098
ROC train: 0.988156	val: 0.738167	test: 0.697949
PRC train: 0.908655	val: 0.337991	test: 0.314516

Epoch: 82
Loss: 0.09756859783471296
ROC train: 0.988462	val: 0.754736	test: 0.714436
PRC train: 0.909711	val: 0.345912	test: 0.314526

Epoch: 83
Loss: 0.09676428831709866
ROC train: 0.988447	val: 0.754564	test: 0.710088
PRC train: 0.906364	val: 0.363156	test: 0.334633

Epoch: 84
Loss: 0.09897812047728813
ROC train: 0.990576	val: 0.754341	test: 0.704557
PRC train: 0.922698	val: 0.346142	test: 0.325390

Epoch: 85
Loss: 0.09483747751028206
ROC train: 0.990319	val: 0.755255	test: 0.705203
PRC train: 0.921673	val: 0.355594	test: 0.319286

Epoch: 86
Loss: 0.09447333067213119
ROC train: 0.990749	val: 0.744243	test: 0.703285
PRC train: 0.922477	val: 0.344319	test: 0.319709

Epoch: 87
Loss: 0.09437147398200446
ROC train: 0.991173	val: 0.741428	test: 0.696823
PRC train: 0.930433	val: 0.356262	test: 0.318377

Epoch: 88
Loss: 0.09430046036166301
ROC train: 0.991358	val: 0.748109	test: 0.699590
PRC train: 0.925737	val: 0.352157	test: 0.326443

Epoch: 89
Loss: 0.09315817027498373
ROC train: 0.991369	val: 0.758619	test: 0.711853
PRC train: 0.930020	val: 0.364599	test: 0.329979

Epoch: 90
Loss: 0.09105049252838292
ROC train: 0.992328	val: 0.751885	test: 0.701393
PRC train: 0.936375	val: 0.347115	test: 0.312056

Epoch: 91
Loss: 0.089424120868755
ROC train: 0.991800	val: 0.744866	test: 0.712401
PRC train: 0.934069	val: 0.338236	test: 0.329156

Epoch: 92
Loss: 0.09080528583079735
ROC train: 0.992325	val: 0.747997	test: 0.698396
PRC train: 0.936594	val: 0.338408	test: 0.316312

Epoch: 93
Loss: 0.08944149810685292
ROC train: 0.992983	val: 0.756586	test: 0.701422
PRC train: 0.642769	val: 0.369022	test: 0.354910

Epoch: 33
Loss: 0.1528863881058437
ROC train: 0.923726	val: 0.757542	test: 0.724935
PRC train: 0.652603	val: 0.354693	test: 0.345462

Epoch: 34
Loss: 0.1532018374268601
ROC train: 0.926781	val: 0.752991	test: 0.723339
PRC train: 0.666425	val: 0.353223	test: 0.349349

Epoch: 35
Loss: 0.15254392266512262
ROC train: 0.931575	val: 0.762334	test: 0.722825
PRC train: 0.677135	val: 0.356075	test: 0.356261

Epoch: 36
Loss: 0.15028140385751385
ROC train: 0.929508	val: 0.745244	test: 0.724426
PRC train: 0.671806	val: 0.331734	test: 0.342533

Epoch: 37
Loss: 0.14955122901524626
ROC train: 0.934593	val: 0.752564	test: 0.731544
PRC train: 0.685428	val: 0.355377	test: 0.357122

Epoch: 38
Loss: 0.14669270219865946
ROC train: 0.934540	val: 0.745431	test: 0.726255
PRC train: 0.686913	val: 0.341171	test: 0.343534

Epoch: 39
Loss: 0.14687700059252584
ROC train: 0.936414	val: 0.743539	test: 0.721995
PRC train: 0.688579	val: 0.354159	test: 0.360783

Epoch: 40
Loss: 0.14632510307533964
ROC train: 0.938798	val: 0.751032	test: 0.719185
PRC train: 0.692949	val: 0.338935	test: 0.346831

Epoch: 41
Loss: 0.14566756416577242
ROC train: 0.942740	val: 0.751259	test: 0.736525
PRC train: 0.714754	val: 0.338980	test: 0.357210

Epoch: 42
Loss: 0.14466962601276542
ROC train: 0.945045	val: 0.740884	test: 0.723292
PRC train: 0.709792	val: 0.344386	test: 0.355035

Epoch: 43
Loss: 0.14448160764351087
ROC train: 0.942576	val: 0.760425	test: 0.743884
PRC train: 0.713878	val: 0.367114	test: 0.369582

Epoch: 44
Loss: 0.14020347116508053
ROC train: 0.946380	val: 0.743534	test: 0.721040
PRC train: 0.725872	val: 0.336348	test: 0.340224

Epoch: 45
Loss: 0.13964891374748142
ROC train: 0.946968	val: 0.754838	test: 0.738392
PRC train: 0.730138	val: 0.348095	test: 0.378224

Epoch: 46
Loss: 0.14074683794461615
ROC train: 0.948708	val: 0.752832	test: 0.735814
PRC train: 0.735036	val: 0.348757	test: 0.363796

Epoch: 47
Loss: 0.13714748693633158
ROC train: 0.951273	val: 0.754959	test: 0.735994
PRC train: 0.747313	val: 0.340502	test: 0.361168

Epoch: 48
Loss: 0.13606307723709593
ROC train: 0.953212	val: 0.760220	test: 0.734192
PRC train: 0.755831	val: 0.349870	test: 0.378097

Epoch: 49
Loss: 0.13475853098226084
ROC train: 0.954135	val: 0.762474	test: 0.731643
PRC train: 0.750972	val: 0.353383	test: 0.365974

Epoch: 50
Loss: 0.13536626766363413
ROC train: 0.956542	val: 0.751232	test: 0.730657
PRC train: 0.769568	val: 0.352807	test: 0.352707

Epoch: 51
Loss: 0.13616934852533627
ROC train: 0.957227	val: 0.739920	test: 0.718769
PRC train: 0.763888	val: 0.324970	test: 0.340482

Epoch: 52
Loss: 0.1328215197661315
ROC train: 0.958622	val: 0.745843	test: 0.728197
PRC train: 0.770608	val: 0.332804	test: 0.346801

Epoch: 53
Loss: 0.13210944654906853
ROC train: 0.962295	val: 0.753856	test: 0.720716
PRC train: 0.781111	val: 0.328964	test: 0.339412

Epoch: 54
Loss: 0.12959646183385087
ROC train: 0.963687	val: 0.756352	test: 0.730744
PRC train: 0.790959	val: 0.353349	test: 0.356881

Epoch: 55
Loss: 0.12716802659006943
ROC train: 0.963858	val: 0.743149	test: 0.714771
PRC train: 0.793889	val: 0.327305	test: 0.324405

Epoch: 56
Loss: 0.12899724431776877
ROC train: 0.965365	val: 0.760023	test: 0.742316
PRC train: 0.801549	val: 0.351875	test: 0.370806

Epoch: 57
Loss: 0.12489257208584917
ROC train: 0.962082	val: 0.758324	test: 0.723390
PRC train: 0.780111	val: 0.346787	test: 0.342226

Epoch: 58
Loss: 0.1249302347929622
ROC train: 0.967827	val: 0.743463	test: 0.732555
PRC train: 0.810942	val: 0.336043	test: 0.355665

Epoch: 59
Loss: 0.12415604973431306
ROC train: 0.967077	val: 0.743549	test: 0.730274
PRC train: 0.805172	val: 0.334889	test: 0.351905

Epoch: 60
Loss: 0.12305184209115957
ROC train: 0.968316	val: 0.738594	test: 0.725695
PRC train: 0.821446	val: 0.326531	test: 0.339254

Epoch: 61
Loss: 0.12102353500860444
ROC train: 0.973133	val: 0.735494	test: 0.715615
PRC train: 0.836189	val: 0.314326	test: 0.319619

Epoch: 62
Loss: 0.12096910002753805
ROC train: 0.972716	val: 0.739330	test: 0.726675
PRC train: 0.830208	val: 0.346104	test: 0.341949

Epoch: 63
Loss: 0.12006839612714469
ROC train: 0.974631	val: 0.738208	test: 0.720590
PRC train: 0.835945	val: 0.340270	test: 0.342144

Epoch: 64
Loss: 0.11920466523780608
ROC train: 0.973609	val: 0.718830	test: 0.707297
PRC train: 0.834483	val: 0.294664	test: 0.321030

Epoch: 65
Loss: 0.11531559868640592
ROC train: 0.973610	val: 0.746873	test: 0.727538
PRC train: 0.831945	val: 0.334813	test: 0.332251

Epoch: 66
Loss: 0.11759482792594003
ROC train: 0.974791	val: 0.754774	test: 0.723854
PRC train: 0.840848	val: 0.351509	test: 0.356995

Epoch: 67
Loss: 0.11582081521323338
ROC train: 0.977376	val: 0.747073	test: 0.724295
PRC train: 0.855700	val: 0.340437	test: 0.344341

Epoch: 68
Loss: 0.11519925828249898
ROC train: 0.977745	val: 0.740001	test: 0.728196
PRC train: 0.853643	val: 0.323398	test: 0.346449

Epoch: 69
Loss: 0.11301295875401847
ROC train: 0.979028	val: 0.749231	test: 0.719248
PRC train: 0.864761	val: 0.343553	test: 0.341884

Epoch: 70
Loss: 0.11182313763823429
ROC train: 0.977206	val: 0.732135	test: 0.728304
PRC train: 0.851630	val: 0.323904	test: 0.345648

Epoch: 71
Loss: 0.11083773053646945
ROC train: 0.979579	val: 0.751318	test: 0.718192
PRC train: 0.863672	val: 0.329962	test: 0.320499

Epoch: 72
Loss: 0.10894866828388838
ROC train: 0.981495	val: 0.742392	test: 0.709085
PRC train: 0.876570	val: 0.316331	test: 0.305007

Epoch: 73
Loss: 0.10900264394576084
ROC train: 0.982117	val: 0.745559	test: 0.719104
PRC train: 0.876765	val: 0.324392	test: 0.314001

Epoch: 74
Loss: 0.10602269558631208
ROC train: 0.982631	val: 0.743993	test: 0.716476
PRC train: 0.876664	val: 0.322086	test: 0.328338

Epoch: 75
Loss: 0.10957697457420298
ROC train: 0.980727	val: 0.732596	test: 0.728600
PRC train: 0.869001	val: 0.291761	test: 0.331388

Epoch: 76
Loss: 0.10849376874272458
ROC train: 0.983432	val: 0.743796	test: 0.721257
PRC train: 0.887692	val: 0.314390	test: 0.332877

Epoch: 77
Loss: 0.10479736774457299
ROC train: 0.986499	val: 0.748097	test: 0.719148
PRC train: 0.902355	val: 0.329556	test: 0.323296

Epoch: 78
Loss: 0.1029880812870987
ROC train: 0.984569	val: 0.754489	test: 0.717499
PRC train: 0.890083	val: 0.331501	test: 0.319781

Epoch: 79
Loss: 0.1036611696355232
ROC train: 0.987107	val: 0.738947	test: 0.717373
PRC train: 0.902871	val: 0.316694	test: 0.321281

Epoch: 80
Loss: 0.10140271645024043
ROC train: 0.986749	val: 0.728641	test: 0.712303
PRC train: 0.901303	val: 0.321611	test: 0.320824

Epoch: 81
Loss: 0.10251520333705737
ROC train: 0.986667	val: 0.740929	test: 0.725694
PRC train: 0.900338	val: 0.326374	test: 0.344432

Epoch: 82
Loss: 0.10183646775968386
ROC train: 0.987970	val: 0.744088	test: 0.721941
PRC train: 0.911697	val: 0.324054	test: 0.321400

Epoch: 83
Loss: 0.0975923302284634
ROC train: 0.988144	val: 0.741303	test: 0.721991
PRC train: 0.909756	val: 0.343744	test: 0.330405

Epoch: 84
Loss: 0.09845757527816539
ROC train: 0.989186	val: 0.748004	test: 0.720179
PRC train: 0.920653	val: 0.344842	test: 0.343363

Epoch: 85
Loss: 0.09661858265629363
ROC train: 0.988564	val: 0.734561	test: 0.720711
PRC train: 0.916144	val: 0.335463	test: 0.342758

Epoch: 86
Loss: 0.09456975279433276
ROC train: 0.990426	val: 0.726172	test: 0.704304
PRC train: 0.925007	val: 0.307318	test: 0.312306

Epoch: 87
Loss: 0.09555194689395818
ROC train: 0.990118	val: 0.725153	test: 0.721293
PRC train: 0.921804	val: 0.291044	test: 0.316531

Epoch: 88
Loss: 0.09191394588671319
ROC train: 0.990710	val: 0.739187	test: 0.713650
PRC train: 0.927018	val: 0.312743	test: 0.312428

Epoch: 89
Loss: 0.0957469661812145
ROC train: 0.991224	val: 0.743612	test: 0.724810
PRC train: 0.930201	val: 0.322301	test: 0.331237

Epoch: 90
Loss: 0.09207902546631541
ROC train: 0.991133	val: 0.743138	test: 0.722609
PRC train: 0.931866	val: 0.340760	test: 0.332566

Epoch: 91
Loss: 0.09183949580572005
ROC train: 0.993350	val: 0.737484	test: 0.716314
PRC train: 0.944329	val: 0.330589	test: 0.321325

Epoch: 92
Loss: 0.0912315751407779
ROC train: 0.993156	val: 0.732292	test: 0.714426
PRC train: 0.945464	val: 0.318564	test: 0.332344

Epoch: 93
Loss: 0.08758498556284733
ROC train: 0.993288	val: 0.749402	test: 0.718598
PRC train: 0.626010	val: 0.291255	test: 0.269572

Epoch: 33
Loss: 0.15913826516129131
ROC train: 0.923711	val: 0.730842	test: 0.713312
PRC train: 0.632041	val: 0.288179	test: 0.270645

Epoch: 34
Loss: 0.158205571645375
ROC train: 0.926822	val: 0.727921	test: 0.704428
PRC train: 0.628427	val: 0.270007	test: 0.248299

Epoch: 35
Loss: 0.15754283435203628
ROC train: 0.927496	val: 0.739833	test: 0.730253
PRC train: 0.643910	val: 0.345239	test: 0.324029

Epoch: 36
Loss: 0.156894188971132
ROC train: 0.930677	val: 0.742299	test: 0.726020
PRC train: 0.654091	val: 0.285968	test: 0.296602

Epoch: 37
Loss: 0.1561293100796218
ROC train: 0.934976	val: 0.756940	test: 0.740711
PRC train: 0.665319	val: 0.335602	test: 0.326353

Epoch: 38
Loss: 0.15339955372121725
ROC train: 0.935768	val: 0.738511	test: 0.731054
PRC train: 0.670634	val: 0.315360	test: 0.305058

Epoch: 39
Loss: 0.15037069786256863
ROC train: 0.936108	val: 0.749155	test: 0.723261
PRC train: 0.676638	val: 0.337586	test: 0.314467

Epoch: 40
Loss: 0.14908868759407087
ROC train: 0.939920	val: 0.735276	test: 0.716027
PRC train: 0.693409	val: 0.287304	test: 0.283629

Epoch: 41
Loss: 0.1467996656422883
ROC train: 0.940682	val: 0.744423	test: 0.724112
PRC train: 0.691302	val: 0.300381	test: 0.298330

Epoch: 42
Loss: 0.14837032033133016
ROC train: 0.944346	val: 0.736354	test: 0.714759
PRC train: 0.700789	val: 0.301707	test: 0.295202

Epoch: 43
Loss: 0.14590451554373285
ROC train: 0.947596	val: 0.727442	test: 0.711643
PRC train: 0.711548	val: 0.281458	test: 0.284879

Epoch: 44
Loss: 0.14589750260463683
ROC train: 0.947397	val: 0.736370	test: 0.729280
PRC train: 0.717349	val: 0.313163	test: 0.318145

Epoch: 45
Loss: 0.1420604653803706
ROC train: 0.949866	val: 0.728009	test: 0.722006
PRC train: 0.725721	val: 0.303176	test: 0.302698

Epoch: 46
Loss: 0.14086637427279658
ROC train: 0.952501	val: 0.684168	test: 0.674611
PRC train: 0.732110	val: 0.240897	test: 0.239209

Epoch: 47
Loss: 0.13941006864393024
ROC train: 0.954788	val: 0.733096	test: 0.710552
PRC train: 0.738952	val: 0.270190	test: 0.270414

Epoch: 48
Loss: 0.14009350271007692
ROC train: 0.952150	val: 0.723621	test: 0.710521
PRC train: 0.728543	val: 0.279773	test: 0.291174

Epoch: 49
Loss: 0.13841721171849788
ROC train: 0.957410	val: 0.710912	test: 0.706059
PRC train: 0.752135	val: 0.251156	test: 0.254731

Epoch: 50
Loss: 0.136704613894079
ROC train: 0.958919	val: 0.730636	test: 0.726905
PRC train: 0.761132	val: 0.305296	test: 0.313405

Epoch: 51
Loss: 0.1348031353743733
ROC train: 0.960935	val: 0.724852	test: 0.716905
PRC train: 0.772079	val: 0.282410	test: 0.284951

Epoch: 52
Loss: 0.13336851086022375
ROC train: 0.953552	val: 0.705386	test: 0.697665
PRC train: 0.731907	val: 0.236347	test: 0.246244

Epoch: 53
Loss: 0.13137856756337843
ROC train: 0.963634	val: 0.695081	test: 0.690337
PRC train: 0.781234	val: 0.239022	test: 0.253898

Epoch: 54
Loss: 0.13405645225116564
ROC train: 0.965104	val: 0.732233	test: 0.707046
PRC train: 0.785058	val: 0.299215	test: 0.299953

Epoch: 55
Loss: 0.12872924040630143
ROC train: 0.966472	val: 0.727328	test: 0.716789
PRC train: 0.794183	val: 0.288040	test: 0.296284

Epoch: 56
Loss: 0.12951314248123177
ROC train: 0.967514	val: 0.719912	test: 0.703217
PRC train: 0.797736	val: 0.277650	test: 0.277836

Epoch: 57
Loss: 0.12691809166176496
ROC train: 0.966165	val: 0.727439	test: 0.708307
PRC train: 0.786160	val: 0.309537	test: 0.305448

Epoch: 58
Loss: 0.12768436053113638
ROC train: 0.966548	val: 0.685301	test: 0.687005
PRC train: 0.789628	val: 0.243764	test: 0.241057

Epoch: 59
Loss: 0.12547992665869512
ROC train: 0.968922	val: 0.679416	test: 0.676947
PRC train: 0.798324	val: 0.224320	test: 0.233302

Epoch: 60
Loss: 0.12720807730092853
ROC train: 0.971524	val: 0.683475	test: 0.672561
PRC train: 0.806611	val: 0.224890	test: 0.227781

Epoch: 61
Loss: 0.12398971762337188
ROC train: 0.973429	val: 0.719175	test: 0.706214
PRC train: 0.820757	val: 0.262348	test: 0.270627

Epoch: 62
Loss: 0.12256545751128665
ROC train: 0.973917	val: 0.723956	test: 0.706853
PRC train: 0.827239	val: 0.300117	test: 0.298221

Epoch: 63
Loss: 0.12060006729879542
ROC train: 0.977465	val: 0.716152	test: 0.694615
PRC train: 0.838771	val: 0.286853	test: 0.275228

Epoch: 64
Loss: 0.11929718414026944
ROC train: 0.975981	val: 0.718500	test: 0.701390
PRC train: 0.838904	val: 0.278004	test: 0.279465

Epoch: 65
Loss: 0.11891235257457762
ROC train: 0.977530	val: 0.711896	test: 0.688729
PRC train: 0.841207	val: 0.255070	test: 0.243464

Epoch: 66
Loss: 0.11729825023453505
ROC train: 0.977655	val: 0.723172	test: 0.718805
PRC train: 0.847113	val: 0.268539	test: 0.273684

Epoch: 67
Loss: 0.1159149768494543
ROC train: 0.979349	val: 0.683799	test: 0.674524
PRC train: 0.850972	val: 0.243218	test: 0.230791

Epoch: 68
Loss: 0.11400138813547157
ROC train: 0.981311	val: 0.725603	test: 0.701525
PRC train: 0.861362	val: 0.298149	test: 0.286046

Epoch: 69
Loss: 0.11273994378358729
ROC train: 0.982047	val: 0.715612	test: 0.696859
PRC train: 0.867078	val: 0.260289	test: 0.274252

Epoch: 70
Loss: 0.11070639069010396
ROC train: 0.982549	val: 0.691092	test: 0.686689
PRC train: 0.872628	val: 0.233332	test: 0.245007

Epoch: 71
Loss: 0.11075137793129577
ROC train: 0.984652	val: 0.698451	test: 0.680133
PRC train: 0.884304	val: 0.239482	test: 0.235777

Epoch: 72
Loss: 0.10968256855364107
ROC train: 0.983798	val: 0.715909	test: 0.685596
PRC train: 0.874142	val: 0.270845	test: 0.249744

Epoch: 73
Loss: 0.10936096377072418
ROC train: 0.984967	val: 0.708963	test: 0.697271
PRC train: 0.880299	val: 0.259267	test: 0.253502

Epoch: 74
Loss: 0.10821564329373469
ROC train: 0.985107	val: 0.709006	test: 0.690106
PRC train: 0.885261	val: 0.271742	test: 0.259275

Epoch: 75
Loss: 0.10770326573188856
ROC train: 0.985383	val: 0.699044	test: 0.683263
PRC train: 0.892403	val: 0.250648	test: 0.227546

Epoch: 76
Loss: 0.10362399413376572
ROC train: 0.988066	val: 0.699469	test: 0.682147
PRC train: 0.903070	val: 0.246266	test: 0.241252

Epoch: 77
Loss: 0.10504575942818511
ROC train: 0.986294	val: 0.698151	test: 0.676401
PRC train: 0.894669	val: 0.248708	test: 0.235107

Epoch: 78
Loss: 0.10297578289987273
ROC train: 0.988397	val: 0.712305	test: 0.703623
PRC train: 0.904133	val: 0.305117	test: 0.302277

Epoch: 79
Loss: 0.10164490380571368
ROC train: 0.989396	val: 0.689659	test: 0.692654
PRC train: 0.911431	val: 0.241240	test: 0.253042

Epoch: 80
Loss: 0.10139939448892006
ROC train: 0.989392	val: 0.708627	test: 0.693018
PRC train: 0.912820	val: 0.272920	test: 0.274073

Epoch: 81
Loss: 0.10219461201952841
ROC train: 0.989711	val: 0.685679	test: 0.677731
PRC train: 0.913374	val: 0.230905	test: 0.236999

Epoch: 82
Loss: 0.0994469617320371
ROC train: 0.990226	val: 0.698175	test: 0.688307
PRC train: 0.915718	val: 0.287878	test: 0.274950

Epoch: 83
Loss: 0.0992218129723363
ROC train: 0.990911	val: 0.703418	test: 0.698879
PRC train: 0.923556	val: 0.254473	test: 0.273926

Epoch: 84
Loss: 0.09567487446363548
ROC train: 0.991926	val: 0.685660	test: 0.683783
PRC train: 0.928229	val: 0.233240	test: 0.246416

Epoch: 85
Loss: 0.09396327384199261
ROC train: 0.992139	val: 0.691450	test: 0.676813
PRC train: 0.932015	val: 0.252979	test: 0.248807

Epoch: 86
Loss: 0.09263637217241386
ROC train: 0.992833	val: 0.692373	test: 0.679498
PRC train: 0.934651	val: 0.275570	test: 0.280582

Epoch: 87
Loss: 0.09194678440608003
ROC train: 0.992966	val: 0.689007	test: 0.673370
PRC train: 0.935101	val: 0.245812	test: 0.247089

Epoch: 88
Loss: 0.09087128411080032
ROC train: 0.992659	val: 0.707644	test: 0.698968
PRC train: 0.936790	val: 0.278476	test: 0.290531

Epoch: 89
Loss: 0.09216717938787033
ROC train: 0.993279	val: 0.701692	test: 0.688972
PRC train: 0.939301	val: 0.256881	test: 0.257562

Epoch: 90
Loss: 0.09064643011047434
ROC train: 0.993168	val: 0.700229	test: 0.685742
PRC train: 0.939010	val: 0.248227	test: 0.247562

Epoch: 91
Loss: 0.0904029068463625
ROC train: 0.993809	val: 0.698865	test: 0.697219
PRC train: 0.945134	val: 0.250967	test: 0.256035

Epoch: 92
Loss: 0.08819486128934864
ROC train: 0.994506	val: 0.685011	test: 0.680379
PRC train: 0.946904	val: 0.249163	test: 0.257656

Epoch: 93
Loss: 0.0884495327297363
ROC train: 0.994801	val: 0.693225	test: 0.689084
PRC train: 0.635099	val: 0.330455	test: 0.337298

Epoch: 33
Loss: 0.15639812688514448
ROC train: 0.920846	val: 0.755358	test: 0.729583
PRC train: 0.626926	val: 0.325177	test: 0.341605

Epoch: 34
Loss: 0.1574754370688027
ROC train: 0.925638	val: 0.765985	test: 0.728095
PRC train: 0.655337	val: 0.318669	test: 0.330885

Epoch: 35
Loss: 0.1533341973471674
ROC train: 0.929310	val: 0.757772	test: 0.732174
PRC train: 0.661648	val: 0.329116	test: 0.337018

Epoch: 36
Loss: 0.15070853479704052
ROC train: 0.932974	val: 0.758239	test: 0.721257
PRC train: 0.672988	val: 0.323311	test: 0.324509

Epoch: 37
Loss: 0.14958762961701216
ROC train: 0.935415	val: 0.770543	test: 0.724845
PRC train: 0.667094	val: 0.321253	test: 0.333420

Epoch: 38
Loss: 0.14868042470732692
ROC train: 0.933964	val: 0.773981	test: 0.744332
PRC train: 0.672802	val: 0.349788	test: 0.363260

Epoch: 39
Loss: 0.14977097300986475
ROC train: 0.935868	val: 0.765283	test: 0.729201
PRC train: 0.686725	val: 0.344534	test: 0.351563

Epoch: 40
Loss: 0.14657531236255064
ROC train: 0.939937	val: 0.760505	test: 0.727359
PRC train: 0.691675	val: 0.318290	test: 0.347090

Epoch: 41
Loss: 0.14399282602893065
ROC train: 0.937104	val: 0.765583	test: 0.739895
PRC train: 0.688586	val: 0.354761	test: 0.360297

Epoch: 42
Loss: 0.1460817577125974
ROC train: 0.941248	val: 0.753243	test: 0.721028
PRC train: 0.697935	val: 0.313855	test: 0.340648

Epoch: 43
Loss: 0.14342735031428172
ROC train: 0.942964	val: 0.756126	test: 0.722356
PRC train: 0.700691	val: 0.321654	test: 0.337686

Epoch: 44
Loss: 0.14216510044386552
ROC train: 0.947375	val: 0.762020	test: 0.739281
PRC train: 0.723407	val: 0.330576	test: 0.350330

Epoch: 45
Loss: 0.13948325474617757
ROC train: 0.949432	val: 0.766500	test: 0.743272
PRC train: 0.733551	val: 0.340856	test: 0.353698

Epoch: 46
Loss: 0.13921639448305598
ROC train: 0.950629	val: 0.758037	test: 0.730214
PRC train: 0.728048	val: 0.319906	test: 0.334172

Epoch: 47
Loss: 0.13811791471990487
ROC train: 0.951924	val: 0.762267	test: 0.733192
PRC train: 0.739957	val: 0.328116	test: 0.348454

Epoch: 48
Loss: 0.13814253369126722
ROC train: 0.954488	val: 0.762823	test: 0.736086
PRC train: 0.751197	val: 0.324731	test: 0.349645

Epoch: 49
Loss: 0.13543707925332427
ROC train: 0.957849	val: 0.765006	test: 0.738676
PRC train: 0.756383	val: 0.326080	test: 0.349457

Epoch: 50
Loss: 0.13635255790256776
ROC train: 0.958168	val: 0.759051	test: 0.730359
PRC train: 0.760992	val: 0.332210	test: 0.349798

Epoch: 51
Loss: 0.13532626212839405
ROC train: 0.957323	val: 0.759207	test: 0.748603
PRC train: 0.758308	val: 0.337088	test: 0.356415

Epoch: 52
Loss: 0.13184406638477297
ROC train: 0.959961	val: 0.748617	test: 0.736401
PRC train: 0.769398	val: 0.318052	test: 0.344696

Epoch: 53
Loss: 0.12764956175467695
ROC train: 0.958849	val: 0.744328	test: 0.730736
PRC train: 0.769261	val: 0.319861	test: 0.348836

Epoch: 54
Loss: 0.13111490450595908
ROC train: 0.962232	val: 0.748569	test: 0.741157
PRC train: 0.774412	val: 0.315659	test: 0.342063

Epoch: 55
Loss: 0.12846930344511287
ROC train: 0.963424	val: 0.754466	test: 0.744294
PRC train: 0.791288	val: 0.316124	test: 0.348696

Epoch: 56
Loss: 0.12695478572405716
ROC train: 0.961278	val: 0.739342	test: 0.716549
PRC train: 0.773515	val: 0.294721	test: 0.322551

Epoch: 57
Loss: 0.12460536349392834
ROC train: 0.967108	val: 0.762035	test: 0.733374
PRC train: 0.810366	val: 0.349745	test: 0.353638

Epoch: 58
Loss: 0.12467997387171727
ROC train: 0.966838	val: 0.758152	test: 0.731631
PRC train: 0.805235	val: 0.337751	test: 0.346484

Epoch: 59
Loss: 0.12404969504803331
ROC train: 0.969784	val: 0.747664	test: 0.734479
PRC train: 0.813938	val: 0.330733	test: 0.357083

Epoch: 60
Loss: 0.12594275950919395
ROC train: 0.968797	val: 0.757991	test: 0.746490
PRC train: 0.811758	val: 0.340227	test: 0.353591

Epoch: 61
Loss: 0.12347063521141387
ROC train: 0.969640	val: 0.748661	test: 0.730606
PRC train: 0.808102	val: 0.302796	test: 0.339379

Epoch: 62
Loss: 0.12197695476106883
ROC train: 0.970645	val: 0.747111	test: 0.726621
PRC train: 0.820572	val: 0.346718	test: 0.342487

Epoch: 63
Loss: 0.12090036598966894
ROC train: 0.973956	val: 0.758649	test: 0.742205
PRC train: 0.838969	val: 0.336281	test: 0.349993

Epoch: 64
Loss: 0.11768776559580196
ROC train: 0.974302	val: 0.760395	test: 0.741149
PRC train: 0.835369	val: 0.334729	test: 0.353811

Epoch: 65
Loss: 0.11803302597694908
ROC train: 0.975780	val: 0.744350	test: 0.740474
PRC train: 0.839222	val: 0.315301	test: 0.338380

Epoch: 66
Loss: 0.11754734734720483
ROC train: 0.974273	val: 0.751207	test: 0.724709
PRC train: 0.838809	val: 0.325536	test: 0.339711

Epoch: 67
Loss: 0.11512296596263083
ROC train: 0.976411	val: 0.745333	test: 0.733217
PRC train: 0.849179	val: 0.321550	test: 0.341761

Epoch: 68
Loss: 0.11283287144392
ROC train: 0.977366	val: 0.746409	test: 0.718438
PRC train: 0.850319	val: 0.335300	test: 0.339991

Epoch: 69
Loss: 0.11203057510430325
ROC train: 0.979256	val: 0.747504	test: 0.726168
PRC train: 0.864315	val: 0.344473	test: 0.342356

Epoch: 70
Loss: 0.11203139174212903
ROC train: 0.978229	val: 0.732157	test: 0.725893
PRC train: 0.858689	val: 0.326629	test: 0.333830

Epoch: 71
Loss: 0.11253966816011898
ROC train: 0.979141	val: 0.735293	test: 0.725559
PRC train: 0.863673	val: 0.325110	test: 0.333603

Epoch: 72
Loss: 0.11051601185400468
ROC train: 0.980839	val: 0.745270	test: 0.727432
PRC train: 0.869099	val: 0.320203	test: 0.330171

Epoch: 73
Loss: 0.10654704208300593
ROC train: 0.980752	val: 0.750297	test: 0.729387
PRC train: 0.872829	val: 0.346660	test: 0.348086

Epoch: 74
Loss: 0.1035247063619012
ROC train: 0.983386	val: 0.746114	test: 0.729570
PRC train: 0.886026	val: 0.318203	test: 0.338246

Epoch: 75
Loss: 0.10369340439098586
ROC train: 0.983865	val: 0.753815	test: 0.727580
PRC train: 0.889269	val: 0.332781	test: 0.344792

Epoch: 76
Loss: 0.10427613677011943
ROC train: 0.984347	val: 0.741422	test: 0.730861
PRC train: 0.893645	val: 0.319327	test: 0.333867

Epoch: 77
Loss: 0.10401152238685907
ROC train: 0.983778	val: 0.726071	test: 0.718829
PRC train: 0.879596	val: 0.312334	test: 0.336124

Epoch: 78
Loss: 0.10302036197199853
ROC train: 0.984737	val: 0.736480	test: 0.727124
PRC train: 0.887139	val: 0.309687	test: 0.349766

Epoch: 79
Loss: 0.10175605872822176
ROC train: 0.984910	val: 0.740200	test: 0.733092
PRC train: 0.897633	val: 0.321400	test: 0.334609

Epoch: 80
Loss: 0.10145302674195475
ROC train: 0.986872	val: 0.736415	test: 0.723981
PRC train: 0.902061	val: 0.320805	test: 0.339464

Epoch: 81
Loss: 0.10061042002905764
ROC train: 0.986138	val: 0.741849	test: 0.732162
PRC train: 0.898002	val: 0.319019	test: 0.349071

Epoch: 82
Loss: 0.09933043147921623
ROC train: 0.987819	val: 0.736749	test: 0.729273
PRC train: 0.912310	val: 0.313641	test: 0.344296

Epoch: 83
Loss: 0.09672720194432476
ROC train: 0.988898	val: 0.730044	test: 0.723318
PRC train: 0.915388	val: 0.304554	test: 0.337077

Epoch: 84
Loss: 0.09454035408282127
ROC train: 0.989152	val: 0.728397	test: 0.717203
PRC train: 0.916100	val: 0.300467	test: 0.330462

Epoch: 85
Loss: 0.09643468169807401
ROC train: 0.988547	val: 0.748119	test: 0.728944
PRC train: 0.913984	val: 0.323012	test: 0.345207

Epoch: 86
Loss: 0.09654382153370132
ROC train: 0.990115	val: 0.727516	test: 0.709055
PRC train: 0.924035	val: 0.315844	test: 0.329375

Epoch: 87
Loss: 0.09744402596221091
ROC train: 0.990632	val: 0.728671	test: 0.731108
PRC train: 0.929365	val: 0.300857	test: 0.323663

Epoch: 88
Loss: 0.09335181822664604
ROC train: 0.989811	val: 0.759374	test: 0.741532
PRC train: 0.923106	val: 0.332542	test: 0.349966

Epoch: 89
Loss: 0.09253337335164057
ROC train: 0.991307	val: 0.756647	test: 0.736731
PRC train: 0.934309	val: 0.327440	test: 0.345259

Epoch: 90
Loss: 0.0931843354014287
ROC train: 0.990203	val: 0.749374	test: 0.730247
PRC train: 0.927054	val: 0.301465	test: 0.335113

Epoch: 91
Loss: 0.09438075316569507
ROC train: 0.990985	val: 0.743405	test: 0.720108
PRC train: 0.931699	val: 0.327136	test: 0.337135

Epoch: 92
Loss: 0.09203138376382886
ROC train: 0.992419	val: 0.737563	test: 0.721058
PRC train: 0.940036	val: 0.326863	test: 0.331119

Epoch: 93
Loss: 0.0905939710433273
ROC train: 0.992209	val: 0.738542	test: 0.723776
ROC train: 0.975355	val: 0.768407	test: 0.747921
PRC train: 0.852110	val: 0.351097	test: 0.352143

Epoch: 95
Loss: 0.10901376671039806
ROC train: 0.976240	val: 0.765676	test: 0.736811
PRC train: 0.853442	val: 0.347754	test: 0.337077

Epoch: 96
Loss: 0.10559029204390334
ROC train: 0.977851	val: 0.769622	test: 0.742869
PRC train: 0.863689	val: 0.347484	test: 0.343074

Epoch: 97
Loss: 0.10638951357189398
ROC train: 0.978526	val: 0.766992	test: 0.748325
PRC train: 0.862560	val: 0.365670	test: 0.344639

Epoch: 98
Loss: 0.10647964277223186
ROC train: 0.978237	val: 0.764256	test: 0.746329
PRC train: 0.866089	val: 0.355232	test: 0.342066

Epoch: 99
Loss: 0.10627772529773344
ROC train: 0.978930	val: 0.770845	test: 0.734460
PRC train: 0.864569	val: 0.355594	test: 0.341397

Epoch: 100
Loss: 0.10520796632518166
ROC train: 0.978043	val: 0.769706	test: 0.742572
PRC train: 0.864315	val: 0.343636	test: 0.321405

Epoch: 101
Loss: 0.1054893460629349
ROC train: 0.979400	val: 0.774432	test: 0.741756
PRC train: 0.869212	val: 0.364445	test: 0.332396

Epoch: 102
Loss: 0.10772347585481612
ROC train: 0.978813	val: 0.773696	test: 0.744605
PRC train: 0.867874	val: 0.341672	test: 0.345092

Epoch: 103
Loss: 0.10569296148164244
ROC train: 0.978741	val: 0.772762	test: 0.752405
PRC train: 0.870302	val: 0.354726	test: 0.358207

Epoch: 104
Loss: 0.10703056443889057
ROC train: 0.979011	val: 0.767250	test: 0.733548
PRC train: 0.862642	val: 0.338518	test: 0.345598

Epoch: 105
Loss: 0.10443424917112189
ROC train: 0.980713	val: 0.769554	test: 0.739143
PRC train: 0.874854	val: 0.359198	test: 0.361334

Epoch: 106
Loss: 0.10350603391826924
ROC train: 0.981060	val: 0.774655	test: 0.748576
PRC train: 0.876290	val: 0.356314	test: 0.339156

Epoch: 107
Loss: 0.10333023908961982
ROC train: 0.981264	val: 0.773238	test: 0.741222
PRC train: 0.881604	val: 0.352928	test: 0.343843

Epoch: 108
Loss: 0.10271896039473298
ROC train: 0.981771	val: 0.774914	test: 0.738489
PRC train: 0.883823	val: 0.357543	test: 0.347749

Epoch: 109
Loss: 0.09988195083503523
ROC train: 0.980915	val: 0.777111	test: 0.740614
PRC train: 0.876412	val: 0.369606	test: 0.351024

Epoch: 110
Loss: 0.09846036163557936
ROC train: 0.983025	val: 0.769113	test: 0.740273
PRC train: 0.888724	val: 0.365419	test: 0.351340

Epoch: 111
Loss: 0.09887451654090537
ROC train: 0.983405	val: 0.770229	test: 0.749436
PRC train: 0.891400	val: 0.351854	test: 0.343232

Epoch: 112
Loss: 0.09993830605032829
ROC train: 0.982695	val: 0.768451	test: 0.740374
PRC train: 0.889004	val: 0.354845	test: 0.323052

Epoch: 113
Loss: 0.09944361373204724
ROC train: 0.983610	val: 0.771182	test: 0.744845
PRC train: 0.892510	val: 0.356058	test: 0.353035

Epoch: 114
Loss: 0.09772511500023388
ROC train: 0.983587	val: 0.771223	test: 0.745149
PRC train: 0.891300	val: 0.362237	test: 0.363070

Epoch: 115
Loss: 0.09772716401244344
ROC train: 0.982030	val: 0.772935	test: 0.750177
PRC train: 0.882381	val: 0.369208	test: 0.351760

Epoch: 116
Loss: 0.09658271091750857
ROC train: 0.983116	val: 0.766408	test: 0.748409
PRC train: 0.890724	val: 0.347917	test: 0.346561

Epoch: 117
Loss: 0.09581019368961119
ROC train: 0.984916	val: 0.763429	test: 0.739562
PRC train: 0.898455	val: 0.339130	test: 0.334573

Epoch: 118
Loss: 0.09703660935616641
ROC train: 0.984665	val: 0.775346	test: 0.746892
PRC train: 0.897057	val: 0.359644	test: 0.342941

Epoch: 119
Loss: 0.09709224007439259
ROC train: 0.984787	val: 0.778786	test: 0.745053
PRC train: 0.899742	val: 0.375478	test: 0.359853

Epoch: 120
Loss: 0.09819874850207082
ROC train: 0.985231	val: 0.767541	test: 0.740162
PRC train: 0.900458	val: 0.352703	test: 0.327189

Early stopping
Best (ROC):	 train: 0.927928	val: 0.792712	test: 0.754822
Best (PRC):	 train: 0.663884	val: 0.377515	test: 0.362197

ROC train: 0.977765	val: 0.758460	test: 0.741612
PRC train: 0.860786	val: 0.362547	test: 0.370664

Epoch: 95
Loss: 0.10781199344422575
ROC train: 0.975932	val: 0.759620	test: 0.742486
PRC train: 0.850500	val: 0.355128	test: 0.377256

Epoch: 96
Loss: 0.10646787319658539
ROC train: 0.977510	val: 0.760291	test: 0.752980
PRC train: 0.864527	val: 0.356213	test: 0.380891

Epoch: 97
Loss: 0.10798698869785085
ROC train: 0.977305	val: 0.740185	test: 0.731059
PRC train: 0.858451	val: 0.345658	test: 0.343473

Epoch: 98
Loss: 0.10636576586253861
ROC train: 0.978407	val: 0.752940	test: 0.745767
PRC train: 0.865374	val: 0.351581	test: 0.360277

Epoch: 99
Loss: 0.10415796990137909
ROC train: 0.979908	val: 0.755746	test: 0.738917
PRC train: 0.871403	val: 0.364810	test: 0.366863

Epoch: 100
Loss: 0.10697941984152035
ROC train: 0.978684	val: 0.755068	test: 0.740999
PRC train: 0.864938	val: 0.357703	test: 0.370199

Epoch: 101
Loss: 0.10670439700491542
ROC train: 0.978678	val: 0.759020	test: 0.739753
PRC train: 0.867882	val: 0.352429	test: 0.348886

Epoch: 102
Loss: 0.10376865893347252
ROC train: 0.980540	val: 0.765978	test: 0.746611
PRC train: 0.877307	val: 0.355990	test: 0.369331

Epoch: 103
Loss: 0.10146857799047287
ROC train: 0.980050	val: 0.760634	test: 0.742619
PRC train: 0.872513	val: 0.359654	test: 0.379236

Epoch: 104
Loss: 0.10173589817008077
ROC train: 0.981226	val: 0.757221	test: 0.731392
PRC train: 0.877646	val: 0.352005	test: 0.352737

Epoch: 105
Loss: 0.10044143670639732
ROC train: 0.980562	val: 0.766812	test: 0.739012
PRC train: 0.877792	val: 0.366558	test: 0.378093

Epoch: 106
Loss: 0.1021222108212108
ROC train: 0.980859	val: 0.763814	test: 0.741637
PRC train: 0.878365	val: 0.359385	test: 0.369943

Epoch: 107
Loss: 0.1007494569576812
ROC train: 0.982177	val: 0.743152	test: 0.721746
PRC train: 0.885553	val: 0.345947	test: 0.356521

Epoch: 108
Loss: 0.10109154657423425
ROC train: 0.981979	val: 0.759921	test: 0.748184
PRC train: 0.886621	val: 0.347070	test: 0.367732

Epoch: 109
Loss: 0.09855431795084671
ROC train: 0.981742	val: 0.761289	test: 0.742276
PRC train: 0.884896	val: 0.365068	test: 0.358317

Epoch: 110
Loss: 0.10144751363141533
ROC train: 0.982287	val: 0.759000	test: 0.738480
PRC train: 0.886254	val: 0.353671	test: 0.363789

Epoch: 111
Loss: 0.09972655744901923
ROC train: 0.982154	val: 0.761019	test: 0.735441
PRC train: 0.884508	val: 0.362410	test: 0.363361

Epoch: 112
Loss: 0.0990891690559966
ROC train: 0.982758	val: 0.764041	test: 0.732295
PRC train: 0.889401	val: 0.357205	test: 0.351891

Epoch: 113
Loss: 0.09726779970974286
ROC train: 0.982860	val: 0.756412	test: 0.731960
PRC train: 0.888731	val: 0.363365	test: 0.366632

Epoch: 114
Loss: 0.09703724249696309
ROC train: 0.982977	val: 0.757445	test: 0.739093
PRC train: 0.892865	val: 0.347278	test: 0.358492

Epoch: 115
Loss: 0.09667182589554599
ROC train: 0.983878	val: 0.750725	test: 0.727988
PRC train: 0.893125	val: 0.332238	test: 0.321249

Epoch: 116
Loss: 0.09491830162375103
ROC train: 0.984387	val: 0.760440	test: 0.737942
PRC train: 0.896428	val: 0.352180	test: 0.343789

Epoch: 117
Loss: 0.09582963292664295
ROC train: 0.983568	val: 0.760663	test: 0.742959
PRC train: 0.889747	val: 0.352890	test: 0.367049

Epoch: 118
Loss: 0.09695800363702858
ROC train: 0.985339	val: 0.756639	test: 0.730284
PRC train: 0.901284	val: 0.355599	test: 0.355647

Epoch: 119
Loss: 0.09456251186503228
ROC train: 0.984481	val: 0.756410	test: 0.740715
PRC train: 0.901503	val: 0.340910	test: 0.351109

Epoch: 120
Loss: 0.09267546929905558
ROC train: 0.985827	val: 0.753111	test: 0.730922
PRC train: 0.903993	val: 0.351061	test: 0.348559

Early stopping
Best (ROC):	 train: 0.920802	val: 0.785625	test: 0.741379
Best (PRC):	 train: 0.643161	val: 0.362794	test: 0.357497

ROC train: 0.977125	val: 0.765941	test: 0.735946
PRC train: 0.856089	val: 0.364318	test: 0.357788

Epoch: 95
Loss: 0.10692502310986533
ROC train: 0.976722	val: 0.748660	test: 0.719649
PRC train: 0.854416	val: 0.343577	test: 0.344565

Epoch: 96
Loss: 0.10624856580546957
ROC train: 0.977354	val: 0.754403	test: 0.725522
PRC train: 0.858953	val: 0.343111	test: 0.334626

Epoch: 97
Loss: 0.10737834685685256
ROC train: 0.978884	val: 0.761342	test: 0.726896
PRC train: 0.865203	val: 0.376244	test: 0.354973

Epoch: 98
Loss: 0.10654491512094308
ROC train: 0.978194	val: 0.749619	test: 0.724356
PRC train: 0.863495	val: 0.340133	test: 0.343333

Epoch: 99
Loss: 0.10320091308620917
ROC train: 0.979419	val: 0.756334	test: 0.728671
PRC train: 0.870382	val: 0.350499	test: 0.349965

Epoch: 100
Loss: 0.10449548171568872
ROC train: 0.980342	val: 0.750188	test: 0.725261
PRC train: 0.876119	val: 0.352071	test: 0.352320

Epoch: 101
Loss: 0.10301530591542907
ROC train: 0.980792	val: 0.764606	test: 0.724583
PRC train: 0.876828	val: 0.365334	test: 0.343174

Epoch: 102
Loss: 0.10328734592632415
ROC train: 0.981048	val: 0.758561	test: 0.712768
PRC train: 0.876809	val: 0.356670	test: 0.327900

Epoch: 103
Loss: 0.1051102421682601
ROC train: 0.979990	val: 0.764057	test: 0.721541
PRC train: 0.870996	val: 0.370364	test: 0.357350

Epoch: 104
Loss: 0.10208349888217945
ROC train: 0.981169	val: 0.749867	test: 0.716298
PRC train: 0.874578	val: 0.343093	test: 0.333125

Epoch: 105
Loss: 0.10215051080951638
ROC train: 0.981421	val: 0.768069	test: 0.721323
PRC train: 0.880424	val: 0.347790	test: 0.338701

Epoch: 106
Loss: 0.09971400507085057
ROC train: 0.983046	val: 0.761382	test: 0.723540
PRC train: 0.886809	val: 0.356333	test: 0.345006

Epoch: 107
Loss: 0.10043150751095159
ROC train: 0.981782	val: 0.752834	test: 0.726574
PRC train: 0.879801	val: 0.349774	test: 0.341308

Epoch: 108
Loss: 0.10018972836610622
ROC train: 0.982919	val: 0.762455	test: 0.720594
PRC train: 0.889867	val: 0.363272	test: 0.336338

Epoch: 109
Loss: 0.10096474741816025
ROC train: 0.982588	val: 0.747843	test: 0.722573
PRC train: 0.885250	val: 0.345309	test: 0.334599

Epoch: 110
Loss: 0.0989710520202504
ROC train: 0.981688	val: 0.750958	test: 0.722150
PRC train: 0.878420	val: 0.350990	test: 0.342966

Epoch: 111
Loss: 0.09865989037528988
ROC train: 0.983805	val: 0.756121	test: 0.722728
PRC train: 0.892964	val: 0.350986	test: 0.342151

Epoch: 112
Loss: 0.09819341008280469
ROC train: 0.982655	val: 0.745350	test: 0.718353
PRC train: 0.883973	val: 0.330704	test: 0.328258

Epoch: 113
Loss: 0.09941758623847533
ROC train: 0.984195	val: 0.747950	test: 0.725985
PRC train: 0.895043	val: 0.345024	test: 0.334298

Epoch: 114
Loss: 0.09834893442414726
ROC train: 0.984040	val: 0.755916	test: 0.725729
PRC train: 0.892917	val: 0.357194	test: 0.339336

Epoch: 115
Loss: 0.09653686744947018
ROC train: 0.984630	val: 0.743911	test: 0.718449
PRC train: 0.894547	val: 0.336380	test: 0.338901

Epoch: 116
Loss: 0.09625127240118087
ROC train: 0.985323	val: 0.751204	test: 0.716092
PRC train: 0.899002	val: 0.353757	test: 0.343373

Epoch: 117
Loss: 0.09332139412506711
ROC train: 0.985664	val: 0.760980	test: 0.732470
PRC train: 0.902058	val: 0.349716	test: 0.345474

Epoch: 118
Loss: 0.09548390507721165
ROC train: 0.984823	val: 0.745190	test: 0.715716
PRC train: 0.891990	val: 0.347713	test: 0.323593

Epoch: 119
Loss: 0.09542926013940675
ROC train: 0.985512	val: 0.757156	test: 0.726689
PRC train: 0.901713	val: 0.363957	test: 0.338793

Epoch: 120
Loss: 0.09409720948100408
ROC train: 0.985892	val: 0.751747	test: 0.725518
PRC train: 0.904993	val: 0.361777	test: 0.344851

Early stopping
Best (ROC):	 train: 0.927420	val: 0.781583	test: 0.746508
Best (PRC):	 train: 0.659802	val: 0.363100	test: 0.362850

PRC train: 0.922449	val: 0.361605	test: 0.321946

Epoch: 94
Loss: 0.09353250013111303
ROC train: 0.989997	val: 0.758131	test: 0.717713
PRC train: 0.925721	val: 0.366650	test: 0.326056

Epoch: 95
Loss: 0.09469633006011373
ROC train: 0.990505	val: 0.763103	test: 0.717485
PRC train: 0.928215	val: 0.370793	test: 0.315591

Epoch: 96
Loss: 0.09164588935843623
ROC train: 0.989074	val: 0.766630	test: 0.724936
PRC train: 0.917518	val: 0.365317	test: 0.322674

Epoch: 97
Loss: 0.09044343942468702
ROC train: 0.991068	val: 0.758507	test: 0.735297
PRC train: 0.930582	val: 0.360134	test: 0.334001

Epoch: 98
Loss: 0.08850835586922305
ROC train: 0.991366	val: 0.760594	test: 0.726891
PRC train: 0.931538	val: 0.366257	test: 0.325812

Epoch: 99
Loss: 0.08721754203542659
ROC train: 0.992266	val: 0.753439	test: 0.715868
PRC train: 0.940422	val: 0.365822	test: 0.324981

Epoch: 100
Loss: 0.0896190236613923
ROC train: 0.992156	val: 0.757820	test: 0.728041
PRC train: 0.939191	val: 0.373150	test: 0.324902

Epoch: 101
Loss: 0.08651558913800869
ROC train: 0.992355	val: 0.749403	test: 0.713523
PRC train: 0.941511	val: 0.364451	test: 0.329368

Epoch: 102
Loss: 0.08673568769663927
ROC train: 0.989698	val: 0.752477	test: 0.721213
PRC train: 0.928544	val: 0.338455	test: 0.319345

Epoch: 103
Loss: 0.08834579440004972
ROC train: 0.992832	val: 0.752669	test: 0.723329
PRC train: 0.941496	val: 0.361691	test: 0.332549

Epoch: 104
Loss: 0.08535221483316809
ROC train: 0.992524	val: 0.769863	test: 0.705141
PRC train: 0.941895	val: 0.371783	test: 0.340390

Epoch: 105
Loss: 0.08471194620591323
ROC train: 0.993022	val: 0.753658	test: 0.716844
PRC train: 0.946235	val: 0.372385	test: 0.340334

Epoch: 106
Loss: 0.0859037079878819
ROC train: 0.993589	val: 0.757347	test: 0.711788
PRC train: 0.948094	val: 0.373184	test: 0.332214

Epoch: 107
Loss: 0.08325873665593975
ROC train: 0.994443	val: 0.749773	test: 0.724554
PRC train: 0.954864	val: 0.369467	test: 0.327206

Epoch: 108
Loss: 0.08176758532731972
ROC train: 0.994758	val: 0.752528	test: 0.724476
PRC train: 0.957109	val: 0.376151	test: 0.340374

Epoch: 109
Loss: 0.08190364920869436
ROC train: 0.994441	val: 0.742565	test: 0.724099
PRC train: 0.954000	val: 0.354931	test: 0.316692

Epoch: 110
Loss: 0.08119270020692167
ROC train: 0.994802	val: 0.749903	test: 0.723485
PRC train: 0.958551	val: 0.364133	test: 0.325825

Epoch: 111
Loss: 0.08197946436361518
ROC train: 0.994756	val: 0.746601	test: 0.712577
PRC train: 0.955470	val: 0.355455	test: 0.322357

Epoch: 112
Loss: 0.08077951564112991
ROC train: 0.995089	val: 0.767899	test: 0.725635
PRC train: 0.958725	val: 0.368246	test: 0.331311

Epoch: 113
Loss: 0.08147511483537975
ROC train: 0.994948	val: 0.759740	test: 0.732615
PRC train: 0.956861	val: 0.366184	test: 0.330776

Epoch: 114
Loss: 0.08120124339097787
ROC train: 0.994624	val: 0.751187	test: 0.726344
PRC train: 0.955934	val: 0.356188	test: 0.334622

Epoch: 115
Loss: 0.07836153364857842
ROC train: 0.995519	val: 0.752896	test: 0.723068
PRC train: 0.962137	val: 0.350574	test: 0.310310

Epoch: 116
Loss: 0.07807702490293249
ROC train: 0.996058	val: 0.753708	test: 0.720462
PRC train: 0.967226	val: 0.358744	test: 0.309345

Epoch: 117
Loss: 0.076498175583272
ROC train: 0.995878	val: 0.754856	test: 0.717980
PRC train: 0.966226	val: 0.354442	test: 0.315494

Epoch: 118
Loss: 0.07432508024981108
ROC train: 0.996456	val: 0.750551	test: 0.711582
PRC train: 0.970564	val: 0.356872	test: 0.315661

Epoch: 119
Loss: 0.07292565145441576
ROC train: 0.996513	val: 0.761891	test: 0.718127
PRC train: 0.969585	val: 0.374671	test: 0.315669

Epoch: 120
Loss: 0.07465668830230085
ROC train: 0.996607	val: 0.758468	test: 0.726429
PRC train: 0.970970	val: 0.360566	test: 0.314688

Early stopping
Best (ROC):	 train: 0.937608	val: 0.784667	test: 0.720043
Best (PRC):	 train: 0.698857	val: 0.364164	test: 0.336975
All runs completed.

PRC train: 0.952788	val: 0.247359	test: 0.224899

Epoch: 94
Loss: 0.08475891932675887
ROC train: 0.995365	val: 0.688283	test: 0.646736
PRC train: 0.954547	val: 0.255025	test: 0.223721

Epoch: 95
Loss: 0.08527501817559743
ROC train: 0.995409	val: 0.694430	test: 0.645086
PRC train: 0.955162	val: 0.253034	test: 0.221693

Epoch: 96
Loss: 0.08521099095149115
ROC train: 0.995489	val: 0.697605	test: 0.635198
PRC train: 0.955759	val: 0.256125	test: 0.228354

Epoch: 97
Loss: 0.08352433797004584
ROC train: 0.995799	val: 0.678229	test: 0.634537
PRC train: 0.958365	val: 0.240067	test: 0.225085

Epoch: 98
Loss: 0.08481426565253214
ROC train: 0.995491	val: 0.673965	test: 0.623374
PRC train: 0.951900	val: 0.252272	test: 0.229560

Epoch: 99
Loss: 0.08113551802705542
ROC train: 0.996505	val: 0.675562	test: 0.637910
PRC train: 0.964858	val: 0.246505	test: 0.227988

Epoch: 100
Loss: 0.0828579844906459
ROC train: 0.995377	val: 0.693243	test: 0.638458
PRC train: 0.958965	val: 0.276142	test: 0.235151

Epoch: 101
Loss: 0.08136381097782419
ROC train: 0.996661	val: 0.679881	test: 0.634853
PRC train: 0.965496	val: 0.254413	test: 0.236164

Epoch: 102
Loss: 0.07872946165463582
ROC train: 0.997226	val: 0.677301	test: 0.639708
PRC train: 0.970846	val: 0.248880	test: 0.225115

Epoch: 103
Loss: 0.07620436701328447
ROC train: 0.997311	val: 0.689826	test: 0.644669
PRC train: 0.971037	val: 0.248647	test: 0.229553

Epoch: 104
Loss: 0.07692328911967791
ROC train: 0.997240	val: 0.673519	test: 0.644815
PRC train: 0.971847	val: 0.243841	test: 0.222909

Epoch: 105
Loss: 0.07768261740826554
ROC train: 0.996527	val: 0.658004	test: 0.629706
PRC train: 0.962278	val: 0.226799	test: 0.222968

Epoch: 106
Loss: 0.07329334219785476
ROC train: 0.997704	val: 0.683200	test: 0.643733
PRC train: 0.974513	val: 0.264868	test: 0.237758

Epoch: 107
Loss: 0.07515305654154407
ROC train: 0.997940	val: 0.694862	test: 0.651599
PRC train: 0.977040	val: 0.263988	test: 0.243548

Epoch: 108
Loss: 0.07320202250391102
ROC train: 0.997971	val: 0.698057	test: 0.649988
PRC train: 0.977168	val: 0.263201	test: 0.232568

Epoch: 109
Loss: 0.0729470321031699
ROC train: 0.997676	val: 0.698265	test: 0.655872
PRC train: 0.972394	val: 0.260161	test: 0.232629

Epoch: 110
Loss: 0.07263405572894921
ROC train: 0.997562	val: 0.686168	test: 0.641775
PRC train: 0.975176	val: 0.263031	test: 0.246106

Epoch: 111
Loss: 0.06946970191189596
ROC train: 0.998106	val: 0.685841	test: 0.644107
PRC train: 0.979093	val: 0.249921	test: 0.219880

Epoch: 112
Loss: 0.07024875658861025
ROC train: 0.998280	val: 0.687795	test: 0.625353
PRC train: 0.980665	val: 0.245843	test: 0.219322

Epoch: 113
Loss: 0.06904294833467879
ROC train: 0.998574	val: 0.671327	test: 0.632303
PRC train: 0.982730	val: 0.246600	test: 0.223687

Epoch: 114
Loss: 0.07042763779015689
ROC train: 0.998529	val: 0.677466	test: 0.635607
PRC train: 0.982805	val: 0.254801	test: 0.232547

Epoch: 115
Loss: 0.06833992505393768
ROC train: 0.998574	val: 0.683396	test: 0.643501
PRC train: 0.983059	val: 0.260275	test: 0.244900

Epoch: 116
Loss: 0.06693804157091575
ROC train: 0.998941	val: 0.676810	test: 0.647160
PRC train: 0.986845	val: 0.253364	test: 0.239383

Epoch: 117
Loss: 0.06750313180920917
ROC train: 0.998848	val: 0.673493	test: 0.637152
PRC train: 0.986596	val: 0.253350	test: 0.232914

Epoch: 118
Loss: 0.06556221633169108
ROC train: 0.998904	val: 0.680952	test: 0.642291
PRC train: 0.986845	val: 0.252889	test: 0.227842

Epoch: 119
Loss: 0.06589688021706797
ROC train: 0.998890	val: 0.681471	test: 0.637011
PRC train: 0.987204	val: 0.244892	test: 0.223551

Epoch: 120
Loss: 0.06604774335220026
ROC train: 0.998916	val: 0.681661	test: 0.640045
PRC train: 0.987472	val: 0.254097	test: 0.226186

Early stopping
Best (ROC):	 train: 0.798044	val: 0.747960	test: 0.696064
Best (PRC):	 train: 0.333303	val: 0.293779	test: 0.268271

PRC train: 0.950862	val: 0.330611	test: 0.297656

Epoch: 94
Loss: 0.08815533495374442
ROC train: 0.995833	val: 0.710236	test: 0.669994
PRC train: 0.960789	val: 0.315324	test: 0.276687

Epoch: 95
Loss: 0.08506128460189136
ROC train: 0.995666	val: 0.728172	test: 0.677607
PRC train: 0.957937	val: 0.330846	test: 0.287583

Epoch: 96
Loss: 0.08555468549114525
ROC train: 0.996330	val: 0.710168	test: 0.682720
PRC train: 0.965101	val: 0.320978	test: 0.281522

Epoch: 97
Loss: 0.08341654570868261
ROC train: 0.996150	val: 0.719684	test: 0.681122
PRC train: 0.964001	val: 0.331209	test: 0.302876

Epoch: 98
Loss: 0.08213772922599033
ROC train: 0.996348	val: 0.712333	test: 0.680376
PRC train: 0.962732	val: 0.311696	test: 0.298924

Epoch: 99
Loss: 0.08456886098083231
ROC train: 0.996456	val: 0.708947	test: 0.688302
PRC train: 0.964230	val: 0.305813	test: 0.288869

Epoch: 100
Loss: 0.08162236283654072
ROC train: 0.996367	val: 0.712501	test: 0.676714
PRC train: 0.963966	val: 0.311653	test: 0.290462

Epoch: 101
Loss: 0.08149051939122375
ROC train: 0.996358	val: 0.722312	test: 0.687628
PRC train: 0.964298	val: 0.323077	test: 0.309650

Epoch: 102
Loss: 0.08016032941814653
ROC train: 0.996956	val: 0.718686	test: 0.677563
PRC train: 0.970634	val: 0.306661	test: 0.288822

Epoch: 103
Loss: 0.07816142960716224
ROC train: 0.997102	val: 0.717383	test: 0.680554
PRC train: 0.970973	val: 0.316545	test: 0.294583

Epoch: 104
Loss: 0.07968752006988736
ROC train: 0.996758	val: 0.716895	test: 0.668760
PRC train: 0.968490	val: 0.287951	test: 0.274969

Epoch: 105
Loss: 0.07754720426092968
ROC train: 0.997407	val: 0.725477	test: 0.685357
PRC train: 0.973533	val: 0.315998	test: 0.294704

Epoch: 106
Loss: 0.07705372193464803
ROC train: 0.997432	val: 0.712848	test: 0.668177
PRC train: 0.973450	val: 0.308494	test: 0.293847

Epoch: 107
Loss: 0.07491585708894681
ROC train: 0.997831	val: 0.716051	test: 0.676527
PRC train: 0.977225	val: 0.294479	test: 0.281704

Epoch: 108
Loss: 0.0758354909820144
ROC train: 0.998057	val: 0.712760	test: 0.670831
PRC train: 0.979266	val: 0.316191	test: 0.301541

Epoch: 109
Loss: 0.07384716988773987
ROC train: 0.997882	val: 0.720748	test: 0.676356
PRC train: 0.976826	val: 0.310663	test: 0.288930

Epoch: 110
Loss: 0.07325222842680876
ROC train: 0.998125	val: 0.703953	test: 0.663220
PRC train: 0.978399	val: 0.295752	test: 0.283687

Epoch: 111
Loss: 0.07064055143926903
ROC train: 0.998462	val: 0.713454	test: 0.673885
PRC train: 0.984072	val: 0.304547	test: 0.287663

Epoch: 112
Loss: 0.06952399874028746
ROC train: 0.998285	val: 0.718802	test: 0.678556
PRC train: 0.980833	val: 0.312424	test: 0.303553

Epoch: 113
Loss: 0.06921846059403806
ROC train: 0.998643	val: 0.713952	test: 0.684503
PRC train: 0.984989	val: 0.295971	test: 0.273228

Epoch: 114
Loss: 0.0715253671007521
ROC train: 0.998597	val: 0.715422	test: 0.684279
PRC train: 0.985079	val: 0.303556	test: 0.280854

Epoch: 115
Loss: 0.06914157561863928
ROC train: 0.998610	val: 0.713983	test: 0.689496
PRC train: 0.985027	val: 0.290921	test: 0.265293

Epoch: 116
Loss: 0.07048091373957806
ROC train: 0.998436	val: 0.712734	test: 0.689584
PRC train: 0.982387	val: 0.301081	test: 0.289594

Epoch: 117
Loss: 0.06866945520704335
ROC train: 0.998749	val: 0.711518	test: 0.682940
PRC train: 0.986442	val: 0.299336	test: 0.278416

Epoch: 118
Loss: 0.06831667093711485
ROC train: 0.999089	val: 0.716165	test: 0.679903
PRC train: 0.989399	val: 0.305414	test: 0.277784

Epoch: 119
Loss: 0.06444042065177148
ROC train: 0.999012	val: 0.718279	test: 0.681902
PRC train: 0.988940	val: 0.317985	test: 0.296238

Epoch: 120
Loss: 0.06514928704816662
ROC train: 0.999049	val: 0.708576	test: 0.674938
PRC train: 0.990119	val: 0.309941	test: 0.292443

Early stopping
Best (ROC):	 train: 0.904660	val: 0.754683	test: 0.710889
Best (PRC):	 train: 0.558577	val: 0.337369	test: 0.305464

PRC train: 0.933789	val: 0.327464	test: 0.349318

Epoch: 94
Loss: 0.09238767284644575
ROC train: 0.990697	val: 0.758093	test: 0.728851
PRC train: 0.934190	val: 0.341760	test: 0.370011

Epoch: 95
Loss: 0.09045033685571192
ROC train: 0.990617	val: 0.766311	test: 0.739599
PRC train: 0.934345	val: 0.345335	test: 0.367781

Epoch: 96
Loss: 0.09094772702148513
ROC train: 0.991643	val: 0.761430	test: 0.730826
PRC train: 0.939647	val: 0.345325	test: 0.365996

Epoch: 97
Loss: 0.09080516502610486
ROC train: 0.992147	val: 0.757120	test: 0.731744
PRC train: 0.942416	val: 0.342265	test: 0.364215

Epoch: 98
Loss: 0.08756938143348547
ROC train: 0.991697	val: 0.762044	test: 0.727842
PRC train: 0.942146	val: 0.339373	test: 0.368675

Epoch: 99
Loss: 0.0886403234079929
ROC train: 0.991823	val: 0.751229	test: 0.726487
PRC train: 0.939961	val: 0.330975	test: 0.344583

Epoch: 100
Loss: 0.08783629462735937
ROC train: 0.992109	val: 0.752607	test: 0.723334
PRC train: 0.942314	val: 0.333425	test: 0.345979

Epoch: 101
Loss: 0.0868192201862677
ROC train: 0.991842	val: 0.758595	test: 0.725467
PRC train: 0.939398	val: 0.334798	test: 0.362454

Epoch: 102
Loss: 0.08413549336372421
ROC train: 0.992462	val: 0.765432	test: 0.723958
PRC train: 0.945066	val: 0.345799	test: 0.361862

Epoch: 103
Loss: 0.08317150549681786
ROC train: 0.993572	val: 0.751996	test: 0.720540
PRC train: 0.952565	val: 0.334905	test: 0.332789

Epoch: 104
Loss: 0.08247611278292816
ROC train: 0.993016	val: 0.740499	test: 0.722523
PRC train: 0.948517	val: 0.318644	test: 0.331410

Epoch: 105
Loss: 0.08154299013134401
ROC train: 0.994465	val: 0.754841	test: 0.722820
PRC train: 0.958253	val: 0.313149	test: 0.339258

Epoch: 106
Loss: 0.08404391117632
ROC train: 0.994229	val: 0.757882	test: 0.719590
PRC train: 0.956405	val: 0.336649	test: 0.359209

Epoch: 107
Loss: 0.08296663770531115
ROC train: 0.994146	val: 0.759029	test: 0.725302
PRC train: 0.956567	val: 0.333280	test: 0.351374

Epoch: 108
Loss: 0.08013048816582544
ROC train: 0.994049	val: 0.765174	test: 0.723679
PRC train: 0.955940	val: 0.339059	test: 0.363794

Epoch: 109
Loss: 0.08087066351146233
ROC train: 0.994902	val: 0.759076	test: 0.716727
PRC train: 0.961304	val: 0.325268	test: 0.348455

Epoch: 110
Loss: 0.08114924034242206
ROC train: 0.995422	val: 0.756621	test: 0.721475
PRC train: 0.965011	val: 0.334794	test: 0.361910

Epoch: 111
Loss: 0.07818200393006138
ROC train: 0.995538	val: 0.751968	test: 0.720009
PRC train: 0.965331	val: 0.318088	test: 0.342479

Epoch: 112
Loss: 0.0777082007987492
ROC train: 0.995283	val: 0.760192	test: 0.717722
PRC train: 0.963330	val: 0.324120	test: 0.344126

Epoch: 113
Loss: 0.0756823415389143
ROC train: 0.994889	val: 0.746433	test: 0.727750
PRC train: 0.960778	val: 0.321512	test: 0.345003

Epoch: 114
Loss: 0.07815420328899267
ROC train: 0.995783	val: 0.758146	test: 0.721384
PRC train: 0.967431	val: 0.326575	test: 0.352297

Epoch: 115
Loss: 0.0776626576512375
ROC train: 0.995408	val: 0.751650	test: 0.728500
PRC train: 0.967018	val: 0.321350	test: 0.348616

Epoch: 116
Loss: 0.07770685959860947
ROC train: 0.996575	val: 0.751977	test: 0.725870
PRC train: 0.973535	val: 0.328808	test: 0.345903

Epoch: 117
Loss: 0.07816696714738987
ROC train: 0.996298	val: 0.756279	test: 0.719791
PRC train: 0.970143	val: 0.335683	test: 0.332518

Epoch: 118
Loss: 0.07567434303493792
ROC train: 0.996367	val: 0.753082	test: 0.718804
PRC train: 0.973080	val: 0.342297	test: 0.356755

Epoch: 119
Loss: 0.07331914894135703
ROC train: 0.996106	val: 0.763875	test: 0.720863
PRC train: 0.973096	val: 0.337633	test: 0.347532

Epoch: 120
Loss: 0.0755964990022888
ROC train: 0.996142	val: 0.759571	test: 0.716162
PRC train: 0.969138	val: 0.339531	test: 0.344941

Early stopping
Best (ROC):	 train: 0.921805	val: 0.778215	test: 0.734065
Best (PRC):	 train: 0.641438	val: 0.352210	test: 0.375120

PRC train: 0.923411	val: 0.342040	test: 0.322555

Epoch: 94
Loss: 0.09041723673532279
ROC train: 0.989978	val: 0.759579	test: 0.712910
PRC train: 0.924442	val: 0.341820	test: 0.325621

Epoch: 95
Loss: 0.0904113752463461
ROC train: 0.990922	val: 0.745305	test: 0.703606
PRC train: 0.931006	val: 0.344462	test: 0.332458

Epoch: 96
Loss: 0.0901818489800465
ROC train: 0.991555	val: 0.764153	test: 0.703855
PRC train: 0.937497	val: 0.362430	test: 0.342844

Epoch: 97
Loss: 0.08939247912739363
ROC train: 0.991567	val: 0.753093	test: 0.712814
PRC train: 0.938667	val: 0.360062	test: 0.343576

Epoch: 98
Loss: 0.08757404173166222
ROC train: 0.991472	val: 0.758175	test: 0.708180
PRC train: 0.938557	val: 0.363713	test: 0.335420

Epoch: 99
Loss: 0.08760055912036409
ROC train: 0.990472	val: 0.752502	test: 0.710040
PRC train: 0.927776	val: 0.357116	test: 0.335474

Epoch: 100
Loss: 0.08932446012536596
ROC train: 0.991164	val: 0.759393	test: 0.708673
PRC train: 0.936432	val: 0.351400	test: 0.337029

Epoch: 101
Loss: 0.08703347184862588
ROC train: 0.992348	val: 0.750262	test: 0.711760
PRC train: 0.939741	val: 0.364447	test: 0.340199

Epoch: 102
Loss: 0.08455765937880845
ROC train: 0.991955	val: 0.749512	test: 0.709023
PRC train: 0.937769	val: 0.371023	test: 0.335385

Epoch: 103
Loss: 0.08532382434588881
ROC train: 0.992064	val: 0.752832	test: 0.712549
PRC train: 0.940350	val: 0.365936	test: 0.337457

Epoch: 104
Loss: 0.08499697261367106
ROC train: 0.993105	val: 0.751930	test: 0.707795
PRC train: 0.949799	val: 0.367984	test: 0.338359

Epoch: 105
Loss: 0.08294424312126804
ROC train: 0.993283	val: 0.757502	test: 0.713410
PRC train: 0.949580	val: 0.381048	test: 0.338562

Epoch: 106
Loss: 0.0825220670702577
ROC train: 0.993382	val: 0.757870	test: 0.714863
PRC train: 0.950467	val: 0.370517	test: 0.350982

Epoch: 107
Loss: 0.0803161120070967
ROC train: 0.994545	val: 0.753784	test: 0.713223
PRC train: 0.958923	val: 0.360502	test: 0.338758

Epoch: 108
Loss: 0.08059451831543571
ROC train: 0.994838	val: 0.747514	test: 0.709093
PRC train: 0.961624	val: 0.338072	test: 0.335165

Epoch: 109
Loss: 0.07907138535350672
ROC train: 0.994212	val: 0.760156	test: 0.705086
PRC train: 0.953480	val: 0.355255	test: 0.319907

Epoch: 110
Loss: 0.08090952572512085
ROC train: 0.994697	val: 0.743716	test: 0.702288
PRC train: 0.960292	val: 0.355470	test: 0.314111

Epoch: 111
Loss: 0.07853655989123293
ROC train: 0.994484	val: 0.741285	test: 0.701317
PRC train: 0.957639	val: 0.352393	test: 0.310498

Epoch: 112
Loss: 0.07778067788006963
ROC train: 0.994658	val: 0.747822	test: 0.708910
PRC train: 0.957995	val: 0.341179	test: 0.325931

Epoch: 113
Loss: 0.07874667027094578
ROC train: 0.995877	val: 0.748050	test: 0.704791
PRC train: 0.967985	val: 0.363904	test: 0.323125

Epoch: 114
Loss: 0.07825318943148546
ROC train: 0.995643	val: 0.759438	test: 0.711489
PRC train: 0.966180	val: 0.370478	test: 0.348440

Epoch: 115
Loss: 0.07730942233192314
ROC train: 0.995431	val: 0.755570	test: 0.700405
PRC train: 0.965578	val: 0.361044	test: 0.327846

Epoch: 116
Loss: 0.07676140451568048
ROC train: 0.996061	val: 0.752685	test: 0.704100
PRC train: 0.969071	val: 0.362753	test: 0.316630

Epoch: 117
Loss: 0.07528487154092199
ROC train: 0.995534	val: 0.756038	test: 0.709828
PRC train: 0.965126	val: 0.365695	test: 0.339697

Epoch: 118
Loss: 0.07577125606035078
ROC train: 0.995981	val: 0.757440	test: 0.710000
PRC train: 0.967609	val: 0.364346	test: 0.343522

Epoch: 119
Loss: 0.07489513990336055
ROC train: 0.996609	val: 0.737245	test: 0.692843
PRC train: 0.971141	val: 0.337881	test: 0.305753

Epoch: 120
Loss: 0.07554160823209642
ROC train: 0.996519	val: 0.751974	test: 0.703692
PRC train: 0.973067	val: 0.349390	test: 0.319216

Early stopping
Best (ROC):	 train: 0.930790	val: 0.776634	test: 0.738901
Best (PRC):	 train: 0.660204	val: 0.369180	test: 0.352941

PRC train: 0.938899	val: 0.358890	test: 0.321213

Epoch: 94
Loss: 0.08711940187194277
ROC train: 0.994042	val: 0.755704	test: 0.709243
PRC train: 0.948669	val: 0.350829	test: 0.326131

Epoch: 95
Loss: 0.08657067461020677
ROC train: 0.993714	val: 0.744044	test: 0.719242
PRC train: 0.945110	val: 0.331540	test: 0.318188

Epoch: 96
Loss: 0.08592141653279885
ROC train: 0.994505	val: 0.751492	test: 0.709110
PRC train: 0.951741	val: 0.352374	test: 0.319933

Epoch: 97
Loss: 0.08239150229120192
ROC train: 0.994731	val: 0.744914	test: 0.721756
PRC train: 0.954705	val: 0.338974	test: 0.323322

Epoch: 98
Loss: 0.08226898469110357
ROC train: 0.994166	val: 0.752095	test: 0.712203
PRC train: 0.947254	val: 0.342714	test: 0.320328

Epoch: 99
Loss: 0.0834937830793233
ROC train: 0.994653	val: 0.745246	test: 0.701627
PRC train: 0.952473	val: 0.338897	test: 0.307501

Epoch: 100
Loss: 0.08717981525523838
ROC train: 0.993893	val: 0.749443	test: 0.716484
PRC train: 0.949416	val: 0.350971	test: 0.319457

Epoch: 101
Loss: 0.08494151597114726
ROC train: 0.994956	val: 0.747906	test: 0.710756
PRC train: 0.953397	val: 0.351689	test: 0.323392

Epoch: 102
Loss: 0.0804208438168339
ROC train: 0.995629	val: 0.749644	test: 0.705791
PRC train: 0.960768	val: 0.349163	test: 0.325286

Epoch: 103
Loss: 0.07830495313150868
ROC train: 0.996573	val: 0.752465	test: 0.711081
PRC train: 0.968231	val: 0.344160	test: 0.321303

Epoch: 104
Loss: 0.08015400909200586
ROC train: 0.996394	val: 0.751173	test: 0.713633
PRC train: 0.968107	val: 0.343122	test: 0.315249

Epoch: 105
Loss: 0.07938177879281255
ROC train: 0.996757	val: 0.753819	test: 0.709793
PRC train: 0.968368	val: 0.348071	test: 0.320728

Epoch: 106
Loss: 0.07592477969624588
ROC train: 0.996434	val: 0.749135	test: 0.707959
PRC train: 0.966388	val: 0.339994	test: 0.324902

Epoch: 107
Loss: 0.0758357385519781
ROC train: 0.997298	val: 0.748415	test: 0.702834
PRC train: 0.974567	val: 0.339250	test: 0.314676

Epoch: 108
Loss: 0.07564166052533947
ROC train: 0.996930	val: 0.734153	test: 0.692502
PRC train: 0.969223	val: 0.323659	test: 0.305975

Epoch: 109
Loss: 0.0745326677904386
ROC train: 0.996431	val: 0.757006	test: 0.711053
PRC train: 0.968659	val: 0.344375	test: 0.312695

Epoch: 110
Loss: 0.07421919300821114
ROC train: 0.997051	val: 0.740437	test: 0.696822
PRC train: 0.970392	val: 0.326143	test: 0.306871

Epoch: 111
Loss: 0.07328830713956194
ROC train: 0.997118	val: 0.755524	test: 0.712771
PRC train: 0.972069	val: 0.345127	test: 0.328200

Epoch: 112
Loss: 0.0711725322758972
ROC train: 0.997559	val: 0.747006	test: 0.693033
PRC train: 0.979225	val: 0.331613	test: 0.289812

Epoch: 113
Loss: 0.07120441202877488
ROC train: 0.997710	val: 0.744318	test: 0.706606
PRC train: 0.978336	val: 0.337379	test: 0.300168

Epoch: 114
Loss: 0.06976774072793691
ROC train: 0.997685	val: 0.741051	test: 0.703740
PRC train: 0.977969	val: 0.329112	test: 0.304424

Epoch: 115
Loss: 0.06982669162757253
ROC train: 0.998085	val: 0.751102	test: 0.699778
PRC train: 0.982154	val: 0.347414	test: 0.299267

Epoch: 116
Loss: 0.07014306194698326
ROC train: 0.997950	val: 0.742564	test: 0.698032
PRC train: 0.979139	val: 0.339096	test: 0.301486

Epoch: 117
Loss: 0.07243531863342287
ROC train: 0.998146	val: 0.742944	test: 0.701392
PRC train: 0.982010	val: 0.330588	test: 0.318149

Epoch: 118
Loss: 0.0690440817926337
ROC train: 0.998117	val: 0.755199	test: 0.712449
PRC train: 0.981491	val: 0.350027	test: 0.319761

Epoch: 119
Loss: 0.06790765000340798
ROC train: 0.998337	val: 0.751223	test: 0.702383
PRC train: 0.984529	val: 0.341135	test: 0.292662

Epoch: 120
Loss: 0.06735709828729053
ROC train: 0.998602	val: 0.751755	test: 0.702470
PRC train: 0.987163	val: 0.337097	test: 0.300355

Early stopping
Best (ROC):	 train: 0.955889	val: 0.769028	test: 0.710197
Best (PRC):	 train: 0.748984	val: 0.373238	test: 0.326196
All runs completed.

PRC train: 0.952027	val: 0.235202	test: 0.248678

Epoch: 94
Loss: 0.0868055402004493
ROC train: 0.994642	val: 0.672120	test: 0.663991
PRC train: 0.949968	val: 0.219775	test: 0.216899

Epoch: 95
Loss: 0.0844145488656018
ROC train: 0.995256	val: 0.708878	test: 0.693989
PRC train: 0.956833	val: 0.277878	test: 0.277553

Epoch: 96
Loss: 0.0838292985161024
ROC train: 0.994271	val: 0.713841	test: 0.697174
PRC train: 0.951650	val: 0.263697	test: 0.261782

Epoch: 97
Loss: 0.08224478134676903
ROC train: 0.995374	val: 0.678137	test: 0.672348
PRC train: 0.956176	val: 0.225550	test: 0.225958

Epoch: 98
Loss: 0.08060649405198995
ROC train: 0.995899	val: 0.686614	test: 0.677407
PRC train: 0.961622	val: 0.222756	test: 0.220391

Epoch: 99
Loss: 0.0803952674646162
ROC train: 0.996429	val: 0.686292	test: 0.677588
PRC train: 0.964481	val: 0.251388	test: 0.253917

Epoch: 100
Loss: 0.08145711286166074
ROC train: 0.995902	val: 0.655659	test: 0.655719
PRC train: 0.958413	val: 0.208222	test: 0.209525

Epoch: 101
Loss: 0.0795686010548908
ROC train: 0.996850	val: 0.694572	test: 0.684936
PRC train: 0.966363	val: 0.244004	test: 0.247153

Epoch: 102
Loss: 0.07831186181472066
ROC train: 0.996645	val: 0.702293	test: 0.691611
PRC train: 0.966285	val: 0.234809	test: 0.252076

Epoch: 103
Loss: 0.0804542065559874
ROC train: 0.996709	val: 0.689888	test: 0.690939
PRC train: 0.962784	val: 0.242999	test: 0.253453

Epoch: 104
Loss: 0.07867799318917684
ROC train: 0.996974	val: 0.702578	test: 0.696996
PRC train: 0.968247	val: 0.258737	test: 0.270814

Epoch: 105
Loss: 0.07497341537894035
ROC train: 0.997094	val: 0.692538	test: 0.689350
PRC train: 0.970989	val: 0.238783	test: 0.247534

Epoch: 106
Loss: 0.0761497792306703
ROC train: 0.997469	val: 0.678328	test: 0.679661
PRC train: 0.973526	val: 0.219725	test: 0.230373

Epoch: 107
Loss: 0.0750048574794855
ROC train: 0.997420	val: 0.690825	test: 0.692572
PRC train: 0.974073	val: 0.257589	test: 0.258064

Epoch: 108
Loss: 0.07387861526686569
ROC train: 0.997549	val: 0.687395	test: 0.679076
PRC train: 0.974962	val: 0.245855	test: 0.242304

Epoch: 109
Loss: 0.07620413587384904
ROC train: 0.997522	val: 0.685929	test: 0.687133
PRC train: 0.972699	val: 0.245738	test: 0.257754

Epoch: 110
Loss: 0.07494251472420077
ROC train: 0.997977	val: 0.694187	test: 0.689504
PRC train: 0.979330	val: 0.256544	test: 0.264360

Epoch: 111
Loss: 0.07408770271645392
ROC train: 0.998206	val: 0.687496	test: 0.692991
PRC train: 0.980376	val: 0.254138	test: 0.264828

Epoch: 112
Loss: 0.07124822629826846
ROC train: 0.998092	val: 0.687768	test: 0.688143
PRC train: 0.980807	val: 0.237749	test: 0.252034

Epoch: 113
Loss: 0.06889640436360352
ROC train: 0.998315	val: 0.690416	test: 0.678061
PRC train: 0.982749	val: 0.242137	test: 0.250610

Epoch: 114
Loss: 0.06636258656482072
ROC train: 0.998590	val: 0.677516	test: 0.672095
PRC train: 0.983350	val: 0.241407	test: 0.261828

Epoch: 115
Loss: 0.0679829801480189
ROC train: 0.998683	val: 0.694220	test: 0.679353
PRC train: 0.986185	val: 0.246516	test: 0.249755

Epoch: 116
Loss: 0.06950254706952769
ROC train: 0.998663	val: 0.699880	test: 0.688765
PRC train: 0.985832	val: 0.256458	test: 0.255342

Epoch: 117
Loss: 0.06946763853490384
ROC train: 0.998183	val: 0.688976	test: 0.679767
PRC train: 0.982775	val: 0.251816	test: 0.258643

Epoch: 118
Loss: 0.06538679795978505
ROC train: 0.998848	val: 0.682679	test: 0.689449
PRC train: 0.987153	val: 0.225346	test: 0.236503

Epoch: 119
Loss: 0.06605308291630782
ROC train: 0.998905	val: 0.699056	test: 0.686963
PRC train: 0.988058	val: 0.266818	test: 0.270726

Epoch: 120
Loss: 0.06548713413957598
ROC train: 0.998500	val: 0.678960	test: 0.681695
PRC train: 0.984134	val: 0.205252	test: 0.216138

Early stopping
Best (ROC):	 train: 0.914107	val: 0.763802	test: 0.731962
Best (PRC):	 train: 0.599917	val: 0.352876	test: 0.322826

PRC train: 0.943366	val: 0.340717	test: 0.344873

Epoch: 94
Loss: 0.08731581677382043
ROC train: 0.992525	val: 0.732996	test: 0.708746
PRC train: 0.939339	val: 0.314194	test: 0.321420

Epoch: 95
Loss: 0.08906471870347911
ROC train: 0.992261	val: 0.759029	test: 0.719770
PRC train: 0.939967	val: 0.347185	test: 0.338608

Epoch: 96
Loss: 0.09003593436593532
ROC train: 0.993802	val: 0.742149	test: 0.712587
PRC train: 0.944913	val: 0.316400	test: 0.330130

Epoch: 97
Loss: 0.08490838354071263
ROC train: 0.994449	val: 0.742784	test: 0.709988
PRC train: 0.955230	val: 0.333134	test: 0.334665

Epoch: 98
Loss: 0.08367967336709752
ROC train: 0.994471	val: 0.736904	test: 0.720673
PRC train: 0.955947	val: 0.324742	test: 0.333683

Epoch: 99
Loss: 0.08323440970707521
ROC train: 0.994323	val: 0.719482	test: 0.708688
PRC train: 0.951043	val: 0.294747	test: 0.303661

Epoch: 100
Loss: 0.08457036185396445
ROC train: 0.994097	val: 0.734786	test: 0.719580
PRC train: 0.950288	val: 0.309391	test: 0.308907

Epoch: 101
Loss: 0.0832547941513425
ROC train: 0.994860	val: 0.730241	test: 0.716717
PRC train: 0.958992	val: 0.320868	test: 0.340255

Epoch: 102
Loss: 0.0827802350525339
ROC train: 0.995123	val: 0.736142	test: 0.721164
PRC train: 0.960077	val: 0.334007	test: 0.338044

Epoch: 103
Loss: 0.08037993997893027
ROC train: 0.996031	val: 0.733954	test: 0.712041
PRC train: 0.966903	val: 0.315649	test: 0.319716

Epoch: 104
Loss: 0.07999445494360334
ROC train: 0.995739	val: 0.744046	test: 0.718692
PRC train: 0.963335	val: 0.325106	test: 0.336868

Epoch: 105
Loss: 0.07853435606027138
ROC train: 0.996475	val: 0.738596	test: 0.718594
PRC train: 0.969333	val: 0.294287	test: 0.294662

Epoch: 106
Loss: 0.07935465162892492
ROC train: 0.996848	val: 0.735616	test: 0.712180
PRC train: 0.972303	val: 0.326371	test: 0.321182

Epoch: 107
Loss: 0.07829572403021286
ROC train: 0.995913	val: 0.728600	test: 0.723191
PRC train: 0.965998	val: 0.301942	test: 0.297535

Epoch: 108
Loss: 0.07763355505934615
ROC train: 0.996608	val: 0.734462	test: 0.706781
PRC train: 0.971127	val: 0.311025	test: 0.298214

Epoch: 109
Loss: 0.08015570697335457
ROC train: 0.996617	val: 0.732445	test: 0.713045
PRC train: 0.970531	val: 0.302861	test: 0.305940

Epoch: 110
Loss: 0.07795133625819792
ROC train: 0.996659	val: 0.719791	test: 0.705829
PRC train: 0.969444	val: 0.296342	test: 0.312799

Epoch: 111
Loss: 0.07479226389259014
ROC train: 0.997265	val: 0.739997	test: 0.722095
PRC train: 0.976543	val: 0.310671	test: 0.333644

Epoch: 112
Loss: 0.07211939042879316
ROC train: 0.996626	val: 0.721073	test: 0.717748
PRC train: 0.971381	val: 0.293517	test: 0.313052

Epoch: 113
Loss: 0.07379738540471531
ROC train: 0.997207	val: 0.728980	test: 0.713497
PRC train: 0.975378	val: 0.304178	test: 0.314866

Epoch: 114
Loss: 0.0718608549118365
ROC train: 0.997458	val: 0.732172	test: 0.715821
PRC train: 0.978350	val: 0.310076	test: 0.325365

Epoch: 115
Loss: 0.07343646448039555
ROC train: 0.997019	val: 0.711368	test: 0.711107
PRC train: 0.973295	val: 0.284854	test: 0.294695

Epoch: 116
Loss: 0.07310493271538233
ROC train: 0.997732	val: 0.726315	test: 0.714162
PRC train: 0.979789	val: 0.312044	test: 0.317985

Epoch: 117
Loss: 0.072766340485432
ROC train: 0.997559	val: 0.745199	test: 0.712203
PRC train: 0.977971	val: 0.326646	test: 0.325388

Epoch: 118
Loss: 0.0702317279758116
ROC train: 0.998055	val: 0.723638	test: 0.712187
PRC train: 0.983002	val: 0.296707	test: 0.297947

Epoch: 119
Loss: 0.06654809737720022
ROC train: 0.997858	val: 0.737139	test: 0.712688
PRC train: 0.981153	val: 0.331954	test: 0.323708

Epoch: 120
Loss: 0.06918470748371924
ROC train: 0.998440	val: 0.733716	test: 0.712735
PRC train: 0.985762	val: 0.316207	test: 0.317226

Early stopping
Best (ROC):	 train: 0.901364	val: 0.762628	test: 0.724472
Best (PRC):	 train: 0.585189	val: 0.364849	test: 0.333567
All runs completed.

PRC train: 0.937618	val: 0.297024	test: 0.307103

Epoch: 94
Loss: 0.09014852612396361
ROC train: 0.993084	val: 0.744705	test: 0.717403
PRC train: 0.944352	val: 0.328373	test: 0.338506

Epoch: 95
Loss: 0.08893793566444641
ROC train: 0.992508	val: 0.740843	test: 0.729061
PRC train: 0.941936	val: 0.314020	test: 0.330602

Epoch: 96
Loss: 0.08418065294093559
ROC train: 0.993128	val: 0.743890	test: 0.728795
PRC train: 0.946942	val: 0.306462	test: 0.326920

Epoch: 97
Loss: 0.08396465342578381
ROC train: 0.994458	val: 0.738607	test: 0.728909
PRC train: 0.955006	val: 0.313713	test: 0.336005

Epoch: 98
Loss: 0.08398351515198599
ROC train: 0.993868	val: 0.743137	test: 0.729036
PRC train: 0.950413	val: 0.318922	test: 0.327852

Epoch: 99
Loss: 0.08496720203474219
ROC train: 0.994399	val: 0.748460	test: 0.724982
PRC train: 0.953797	val: 0.324450	test: 0.340230

Epoch: 100
Loss: 0.08354716267778367
ROC train: 0.994541	val: 0.736895	test: 0.724490
PRC train: 0.954516	val: 0.318896	test: 0.321953

Epoch: 101
Loss: 0.0821326658694775
ROC train: 0.994556	val: 0.742366	test: 0.719241
PRC train: 0.955214	val: 0.321692	test: 0.341123

Epoch: 102
Loss: 0.08062459365389385
ROC train: 0.994541	val: 0.742258	test: 0.736419
PRC train: 0.955467	val: 0.318954	test: 0.335216

Epoch: 103
Loss: 0.08233733277697347
ROC train: 0.995289	val: 0.733442	test: 0.732330
PRC train: 0.960575	val: 0.323144	test: 0.329840

Epoch: 104
Loss: 0.0786470507611055
ROC train: 0.996051	val: 0.732439	test: 0.732864
PRC train: 0.967154	val: 0.331191	test: 0.340124

Epoch: 105
Loss: 0.07811323583204427
ROC train: 0.995687	val: 0.731062	test: 0.721242
PRC train: 0.962958	val: 0.303919	test: 0.337890

Epoch: 106
Loss: 0.07961760523426278
ROC train: 0.995996	val: 0.737026	test: 0.730423
PRC train: 0.964103	val: 0.322760	test: 0.356156

Epoch: 107
Loss: 0.07644633692416065
ROC train: 0.995427	val: 0.715164	test: 0.718246
PRC train: 0.958611	val: 0.298736	test: 0.320724

Epoch: 108
Loss: 0.07873875077936261
ROC train: 0.995979	val: 0.739846	test: 0.729975
PRC train: 0.964935	val: 0.312800	test: 0.325448

Epoch: 109
Loss: 0.07905698732324913
ROC train: 0.996230	val: 0.723459	test: 0.723546
PRC train: 0.967017	val: 0.299901	test: 0.320835

Epoch: 110
Loss: 0.07539781474896536
ROC train: 0.996991	val: 0.731289	test: 0.725330
PRC train: 0.972404	val: 0.309868	test: 0.332747

Epoch: 111
Loss: 0.07551395674218175
ROC train: 0.996922	val: 0.731202	test: 0.733946
PRC train: 0.970815	val: 0.309398	test: 0.317123

Epoch: 112
Loss: 0.07429927477270644
ROC train: 0.996796	val: 0.722610	test: 0.724861
PRC train: 0.969387	val: 0.289957	test: 0.301738

Epoch: 113
Loss: 0.07149443564650089
ROC train: 0.997131	val: 0.733191	test: 0.736052
PRC train: 0.974490	val: 0.325081	test: 0.333019

Epoch: 114
Loss: 0.07352460768429717
ROC train: 0.997185	val: 0.727750	test: 0.726413
PRC train: 0.975375	val: 0.313811	test: 0.314659

Epoch: 115
Loss: 0.07299380087714172
ROC train: 0.997445	val: 0.732334	test: 0.725509
PRC train: 0.976287	val: 0.303944	test: 0.320768

Epoch: 116
Loss: 0.07174857434769717
ROC train: 0.997375	val: 0.729084	test: 0.723353
PRC train: 0.976465	val: 0.315019	test: 0.326639

Epoch: 117
Loss: 0.07170049380712899
ROC train: 0.997906	val: 0.726011	test: 0.722505
PRC train: 0.982391	val: 0.302880	test: 0.320143

Epoch: 118
Loss: 0.06960600252703207
ROC train: 0.997834	val: 0.741122	test: 0.725027
PRC train: 0.981105	val: 0.326628	test: 0.341246

Epoch: 119
Loss: 0.0695830719623794
ROC train: 0.997459	val: 0.725904	test: 0.728396
PRC train: 0.978539	val: 0.311577	test: 0.328387

Epoch: 120
Loss: 0.06616559550094343
ROC train: 0.997919	val: 0.733273	test: 0.726979
PRC train: 0.981221	val: 0.307514	test: 0.339479

Early stopping
Best (ROC):	 train: 0.889071	val: 0.782975	test: 0.737177
Best (PRC):	 train: 0.533165	val: 0.365670	test: 0.358908
All runs completed.
