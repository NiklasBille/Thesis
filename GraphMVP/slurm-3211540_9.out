>>> Starting run for dataset: tox21
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running RANDOM configs_split_experiments/GraphMVP/tox21/random/train_prop=0.6.yml on cuda:0
Running RANDOM configs_split_experiments/GraphMVP/tox21/random/train_prop=0.7.yml on cuda:1
Running RANDOM configs_split_experiments/GraphMVP/tox21/random/train_prop=0.8.yml on cuda:2
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphMVP/tox21/random/train_prop=0.8.yml --runseed 4 --device cuda:2
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphMVP/tox21/random/train_prop=0.7.yml --runseed 4 --device cuda:1
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphMVP/tox21/random/train_prop=0.8.yml --runseed 5 --device cuda:2
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphMVP/tox21/random/train_prop=0.7.yml --runseed 5 --device cuda:1
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphMVP/tox21/random/train_prop=0.8.yml --runseed 6 --device cuda:2
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphMVP/tox21/random/train_prop=0.7.yml --runseed 6 --device cuda:1
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphMVP/tox21/random/train_prop=0.6.yml --runseed 4 --device cuda:0
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphMVP/tox21/random/train_prop=0.6.yml --runseed 5 --device cuda:0
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphMVP/tox21/random/train_prop=0.6.yml --runseed 6 --device cuda:0
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/tox21/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/split/GraphMVP/tox21/random/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/tox21/random/train_prop=0.6/tox21_random_6_26-05_11-07-48  ]
[ Using Seed :  6  ]
[ Using device :  cuda:0  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
randomly split
Data(edge_attr=[26, 2], edge_index=[2, 26], id=[1], x=[13, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5795832172259143
ROC train: 0.701759	val: 0.685994	test: 0.656879
PRC train: 0.174008	val: 0.200737	test: 0.149063

Epoch: 2
Loss: 0.411772607683041
ROC train: 0.751295	val: 0.719569	test: 0.700903
PRC train: 0.267324	val: 0.260263	test: 0.204892

Epoch: 3
Loss: 0.3071843502161528
ROC train: 0.775398	val: 0.736187	test: 0.716998
PRC train: 0.303312	val: 0.286408	test: 0.223729

Epoch: 4
Loss: 0.24791351803318298
ROC train: 0.790626	val: 0.758401	test: 0.729723
PRC train: 0.320743	val: 0.299574	test: 0.244130

Epoch: 5
Loss: 0.2215838029359716
ROC train: 0.809547	val: 0.772050	test: 0.756308
PRC train: 0.355595	val: 0.318388	test: 0.273863

Epoch: 6
Loss: 0.21174708470250023
ROC train: 0.817685	val: 0.774897	test: 0.764473
PRC train: 0.378245	val: 0.332200	test: 0.284179

Epoch: 7
Loss: 0.20714325052459676
ROC train: 0.823388	val: 0.782632	test: 0.761210
PRC train: 0.376695	val: 0.336298	test: 0.281222

Epoch: 8
Loss: 0.20319177260847274
ROC train: 0.825239	val: 0.788584	test: 0.760126
PRC train: 0.378959	val: 0.338259	test: 0.278120

Epoch: 9
Loss: 0.20349751931267723
ROC train: 0.839013	val: 0.796883	test: 0.773876
PRC train: 0.403267	val: 0.358294	test: 0.298922

Epoch: 10
Loss: 0.1991647121386193
ROC train: 0.841255	val: 0.794897	test: 0.779456
PRC train: 0.418202	val: 0.347493	test: 0.309708

Epoch: 11
Loss: 0.19714398825531315
ROC train: 0.843416	val: 0.787917	test: 0.778576
PRC train: 0.426641	val: 0.344922	test: 0.305974

Epoch: 12
Loss: 0.1928417706772814
ROC train: 0.853303	val: 0.803984	test: 0.793158
PRC train: 0.444024	val: 0.364432	test: 0.324470

Epoch: 13
Loss: 0.19333027269323197
ROC train: 0.852663	val: 0.794928	test: 0.781485
PRC train: 0.438124	val: 0.353794	test: 0.313522

Epoch: 14
Loss: 0.19047631172320445
ROC train: 0.859328	val: 0.800216	test: 0.789097
PRC train: 0.456960	val: 0.368864	test: 0.328738

Epoch: 15
Loss: 0.18924683289349922
ROC train: 0.862964	val: 0.803410	test: 0.790011
PRC train: 0.464541	val: 0.368396	test: 0.326616

Epoch: 16
Loss: 0.18919384321901253
ROC train: 0.869420	val: 0.813616	test: 0.801229
PRC train: 0.489281	val: 0.376411	test: 0.344338

Epoch: 17
Loss: 0.1857572004375386
ROC train: 0.866963	val: 0.806085	test: 0.799889
PRC train: 0.482839	val: 0.368411	test: 0.348836

Epoch: 18
Loss: 0.186333826951297
ROC train: 0.874345	val: 0.818118	test: 0.802360
PRC train: 0.505562	val: 0.380556	test: 0.353034

Epoch: 19
Loss: 0.18511331395417388
ROC train: 0.878953	val: 0.813519	test: 0.803748
PRC train: 0.515225	val: 0.389453	test: 0.362413

Epoch: 20
Loss: 0.18074137762517362
ROC train: 0.880401	val: 0.822866	test: 0.809510
PRC train: 0.524286	val: 0.380886	test: 0.370290

Epoch: 21
Loss: 0.1806345680489244
ROC train: 0.882203	val: 0.825349	test: 0.808060
PRC train: 0.529671	val: 0.401361	test: 0.372349

Epoch: 22
Loss: 0.17838872514610127
ROC train: 0.881961	val: 0.815349	test: 0.811517
PRC train: 0.524207	val: 0.379883	test: 0.360656

Epoch: 23
Loss: 0.1782982675891041
ROC train: 0.882525	val: 0.815862	test: 0.810321
PRC train: 0.531917	val: 0.386854	test: 0.377680

Epoch: 24
Loss: 0.17624689227480994
ROC train: 0.888382	val: 0.815527	test: 0.807569
PRC train: 0.548936	val: 0.398829	test: 0.380054

Epoch: 25
Loss: 0.17519298936867014
ROC train: 0.890331	val: 0.823496	test: 0.819924
PRC train: 0.557092	val: 0.389088	test: 0.390561

Epoch: 26
Loss: 0.17424079156116723
ROC train: 0.889336	val: 0.812121	test: 0.809033
PRC train: 0.552477	val: 0.385948	test: 0.389364

Epoch: 27
Loss: 0.1734893887039383
ROC train: 0.893421	val: 0.822102	test: 0.817926
PRC train: 0.557924	val: 0.392878	test: 0.392066

Epoch: 28
Loss: 0.1746119129425688
ROC train: 0.890171	val: 0.823608	test: 0.814807
PRC train: 0.551398	val: 0.389928	test: 0.396777

Epoch: 29
Loss: 0.17147356096226526
ROC train: 0.901032	val: 0.823830	test: 0.819881
PRC train: 0.581478	val: 0.400500	test: 0.402316

Epoch: 30
Loss: 0.16861004464633217
ROC train: 0.899387	val: 0.826337	test: 0.818468
PRC train: 0.573999	val: 0.398705	test: 0.407200

Epoch: 31
Loss: 0.17065236147167276
ROC train: 0.904606	val: 0.826788	test: 0.820866
PRC train: 0.597347	val: 0.410782	test: 0.412032

Epoch: 32
Loss: 0.16911465726168198
ROC train: 0.902332	val: 0.824965	test: 0.814358
PRC train: 0.581163	val: 0.409674	test: 0.402652

Epoch: 33
Loss: 0.1679345621939831Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/tox21/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/split/GraphMVP/tox21/random/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/tox21/random/train_prop=0.6/tox21_random_5_26-05_11-07-48  ]
[ Using Seed :  5  ]
[ Using device :  cuda:0  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
randomly split
Data(edge_attr=[26, 2], edge_index=[2, 26], id=[1], x=[13, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5750413960874322
ROC train: 0.697089	val: 0.678686	test: 0.651323
PRC train: 0.159478	val: 0.173997	test: 0.139855

Epoch: 2
Loss: 0.4053483662418557
ROC train: 0.728317	val: 0.719497	test: 0.667976
PRC train: 0.212748	val: 0.225508	test: 0.182687

Epoch: 3
Loss: 0.3054460985466583
ROC train: 0.764180	val: 0.732674	test: 0.705516
PRC train: 0.287599	val: 0.271567	test: 0.219990

Epoch: 4
Loss: 0.24970193534181045
ROC train: 0.794419	val: 0.757014	test: 0.738018
PRC train: 0.343670	val: 0.315914	test: 0.257426

Epoch: 5
Loss: 0.22241425343713664
ROC train: 0.809156	val: 0.768199	test: 0.753033
PRC train: 0.372555	val: 0.330142	test: 0.280924

Epoch: 6
Loss: 0.21216252977733774
ROC train: 0.812829	val: 0.765144	test: 0.749485
PRC train: 0.355889	val: 0.297332	test: 0.267704

Epoch: 7
Loss: 0.20759124024052059
ROC train: 0.829186	val: 0.795165	test: 0.770490
PRC train: 0.398940	val: 0.348082	test: 0.304322

Epoch: 8
Loss: 0.2022974041454866
ROC train: 0.827980	val: 0.786402	test: 0.768809
PRC train: 0.401924	val: 0.346674	test: 0.306994

Epoch: 9
Loss: 0.20357703054859563
ROC train: 0.833236	val: 0.796030	test: 0.767232
PRC train: 0.384126	val: 0.335060	test: 0.286088

Epoch: 10
Loss: 0.19957475338440694
ROC train: 0.844688	val: 0.797525	test: 0.773734
PRC train: 0.413808	val: 0.359366	test: 0.309634

Epoch: 11
Loss: 0.19603371675193304
ROC train: 0.848679	val: 0.805688	test: 0.781174
PRC train: 0.424799	val: 0.357178	test: 0.317044

Epoch: 12
Loss: 0.19253751512864303
ROC train: 0.856141	val: 0.800950	test: 0.792054
PRC train: 0.450571	val: 0.360108	test: 0.334632

Epoch: 13
Loss: 0.1917099120941732
ROC train: 0.862365	val: 0.807887	test: 0.792587
PRC train: 0.468560	val: 0.380478	test: 0.342035

Epoch: 14
Loss: 0.19037005821200378
ROC train: 0.866695	val: 0.807179	test: 0.798207
PRC train: 0.485822	val: 0.377586	test: 0.357731

Epoch: 15
Loss: 0.18899265669927306
ROC train: 0.864506	val: 0.811053	test: 0.797135
PRC train: 0.477182	val: 0.373703	test: 0.347659

Epoch: 16
Loss: 0.18948477344230572
ROC train: 0.869770	val: 0.807323	test: 0.800237
PRC train: 0.499388	val: 0.370642	test: 0.358872

Epoch: 17
Loss: 0.1850261881962684
ROC train: 0.873184	val: 0.809502	test: 0.800077
PRC train: 0.500094	val: 0.380093	test: 0.352940

Epoch: 18
Loss: 0.18353101137394723
ROC train: 0.875845	val: 0.805994	test: 0.799256
PRC train: 0.506913	val: 0.379860	test: 0.360254

Epoch: 19
Loss: 0.18422161743787946
ROC train: 0.878404	val: 0.809532	test: 0.806913
PRC train: 0.525914	val: 0.377643	test: 0.383907

Epoch: 20
Loss: 0.18170242456904084
ROC train: 0.876234	val: 0.808538	test: 0.803054
PRC train: 0.498254	val: 0.362196	test: 0.352503

Epoch: 21
Loss: 0.18209781751014933
ROC train: 0.881588	val: 0.812246	test: 0.806987
PRC train: 0.522366	val: 0.378453	test: 0.362817

Epoch: 22
Loss: 0.17808848321980958
ROC train: 0.886711	val: 0.814603	test: 0.801131
PRC train: 0.541912	val: 0.388835	test: 0.377291

Epoch: 23
Loss: 0.17700117436433377
ROC train: 0.888655	val: 0.810643	test: 0.805680
PRC train: 0.553794	val: 0.396234	test: 0.386971

Epoch: 24
Loss: 0.17612015226047953
ROC train: 0.893552	val: 0.825277	test: 0.812458
PRC train: 0.560389	val: 0.399904	test: 0.394671

Epoch: 25
Loss: 0.17347599597480845
ROC train: 0.891531	val: 0.820206	test: 0.808669
PRC train: 0.560018	val: 0.399740	test: 0.398753

Epoch: 26
Loss: 0.17316666972663647
ROC train: 0.893885	val: 0.822708	test: 0.812003
PRC train: 0.566043	val: 0.393855	test: 0.403328

Epoch: 27
Loss: 0.17213490237993767
ROC train: 0.890708	val: 0.821581	test: 0.793732
PRC train: 0.543118	val: 0.390409	test: 0.378370

Epoch: 28
Loss: 0.17583643217130687
ROC train: 0.899412	val: 0.826968	test: 0.809265
PRC train: 0.582726	val: 0.410395	test: 0.411299

Epoch: 29
Loss: 0.17211564037677807
ROC train: 0.901651	val: 0.821763	test: 0.809253
PRC train: 0.586550	val: 0.392424	test: 0.411547

Epoch: 30
Loss: 0.169561108046246
ROC train: 0.901557	val: 0.819178	test: 0.806906
PRC train: 0.588951	val: 0.411285	test: 0.411152

Epoch: 31
Loss: 0.16745743616115186
ROC train: 0.895973	val: 0.815890	test: 0.810475
PRC train: 0.575065	val: 0.387161	test: 0.405687

Epoch: 32
Loss: 0.16755795199466547
ROC train: 0.900012	val: 0.822597	test: 0.808734
PRC train: 0.580228	val: 0.405177	test: 0.400006

Epoch: 33
Loss: 0.17190656331185913Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/tox21/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/split/GraphMVP/tox21/random/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/tox21/random/train_prop=0.6/tox21_random_4_26-05_11-07-48  ]
[ Using Seed :  4  ]
[ Using device :  cuda:0  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
randomly split
Data(edge_attr=[26, 2], edge_index=[2, 26], id=[1], x=[13, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5874517911653936
ROC train: 0.693065	val: 0.669361	test: 0.643499
PRC train: 0.159833	val: 0.176236	test: 0.136741

Epoch: 2
Loss: 0.41226042190336737
ROC train: 0.747175	val: 0.712060	test: 0.695442
PRC train: 0.251112	val: 0.249600	test: 0.196126

Epoch: 3
Loss: 0.30903729987125855
ROC train: 0.778470	val: 0.745063	test: 0.718269
PRC train: 0.300925	val: 0.292571	test: 0.227870

Epoch: 4
Loss: 0.24896820658180785
ROC train: 0.786777	val: 0.766099	test: 0.723497
PRC train: 0.313870	val: 0.299004	test: 0.234053

Epoch: 5
Loss: 0.22262431589214318
ROC train: 0.796756	val: 0.765716	test: 0.726870
PRC train: 0.336607	val: 0.317594	test: 0.246662

Epoch: 6
Loss: 0.2105373767735897
ROC train: 0.810016	val: 0.771352	test: 0.750825
PRC train: 0.371502	val: 0.326573	test: 0.275401

Epoch: 7
Loss: 0.20528464896255758
ROC train: 0.822211	val: 0.784062	test: 0.762328
PRC train: 0.390087	val: 0.343562	test: 0.302722

Epoch: 8
Loss: 0.20392769152280724
ROC train: 0.834082	val: 0.797989	test: 0.762092
PRC train: 0.399711	val: 0.348379	test: 0.299819

Epoch: 9
Loss: 0.19992311467857224
ROC train: 0.838121	val: 0.794852	test: 0.769096
PRC train: 0.409509	val: 0.353729	test: 0.300627

Epoch: 10
Loss: 0.19900945512399196
ROC train: 0.840775	val: 0.799182	test: 0.767743
PRC train: 0.417565	val: 0.354604	test: 0.307874

Epoch: 11
Loss: 0.196130708681903
ROC train: 0.850716	val: 0.809137	test: 0.773159
PRC train: 0.440096	val: 0.373325	test: 0.322389

Epoch: 12
Loss: 0.19435925992794156
ROC train: 0.852477	val: 0.800154	test: 0.776419
PRC train: 0.438525	val: 0.365027	test: 0.311231

Epoch: 13
Loss: 0.19215968687409976
ROC train: 0.858151	val: 0.803983	test: 0.782497
PRC train: 0.443973	val: 0.356848	test: 0.338340

Epoch: 14
Loss: 0.18862442436179
ROC train: 0.863678	val: 0.819101	test: 0.782787
PRC train: 0.468646	val: 0.384558	test: 0.334468

Epoch: 15
Loss: 0.1901658864634444
ROC train: 0.865549	val: 0.806266	test: 0.786082
PRC train: 0.466427	val: 0.377582	test: 0.333170

Epoch: 16
Loss: 0.18763659943967947
ROC train: 0.868095	val: 0.814523	test: 0.782389
PRC train: 0.468079	val: 0.380618	test: 0.328413

Epoch: 17
Loss: 0.1840440119276175
ROC train: 0.875784	val: 0.808481	test: 0.797005
PRC train: 0.493996	val: 0.384667	test: 0.349053

Epoch: 18
Loss: 0.1859522091396124
ROC train: 0.876859	val: 0.805776	test: 0.795596
PRC train: 0.506433	val: 0.377892	test: 0.358366

Epoch: 19
Loss: 0.1841306906607599
ROC train: 0.874095	val: 0.810938	test: 0.797023
PRC train: 0.493222	val: 0.372613	test: 0.344892

Epoch: 20
Loss: 0.1818760631384422
ROC train: 0.880262	val: 0.813448	test: 0.799639
PRC train: 0.514316	val: 0.386604	test: 0.352850

Epoch: 21
Loss: 0.17918411562640954
ROC train: 0.885426	val: 0.822964	test: 0.807150
PRC train: 0.541965	val: 0.403887	test: 0.385824

Epoch: 22
Loss: 0.17829686027051428
ROC train: 0.886866	val: 0.816922	test: 0.801773
PRC train: 0.536299	val: 0.403576	test: 0.372424

Epoch: 23
Loss: 0.175218916251839
ROC train: 0.888937	val: 0.810369	test: 0.800227
PRC train: 0.536005	val: 0.390102	test: 0.370653

Epoch: 24
Loss: 0.1765291896123307
ROC train: 0.887088	val: 0.815926	test: 0.807931
PRC train: 0.548628	val: 0.396784	test: 0.386296

Epoch: 25
Loss: 0.17595761088527145
ROC train: 0.895581	val: 0.821076	test: 0.811602
PRC train: 0.563631	val: 0.395985	test: 0.378865

Epoch: 26
Loss: 0.17569649647935162
ROC train: 0.891522	val: 0.819735	test: 0.796508
PRC train: 0.551007	val: 0.403851	test: 0.382034

Epoch: 27
Loss: 0.17100564497546367
ROC train: 0.893526	val: 0.817852	test: 0.809785
PRC train: 0.557534	val: 0.395361	test: 0.373818

Epoch: 28
Loss: 0.17052309723780357
ROC train: 0.899685	val: 0.823405	test: 0.812226
PRC train: 0.581955	val: 0.402876	test: 0.397296

Epoch: 29
Loss: 0.17203991804780178
ROC train: 0.896526	val: 0.819282	test: 0.802602
PRC train: 0.564526	val: 0.404701	test: 0.391266

Epoch: 30
Loss: 0.17055753448218472
ROC train: 0.900309	val: 0.829903	test: 0.813628
PRC train: 0.573534	val: 0.407179	test: 0.396507

Epoch: 31
Loss: 0.169230772804268
ROC train: 0.901606	val: 0.824917	test: 0.809938
PRC train: 0.584426	val: 0.408547	test: 0.395989

Epoch: 32
Loss: 0.16632556117794445
ROC train: 0.905943	val: 0.821132	test: 0.808940
PRC train: 0.600859	val: 0.407434	test: 0.405974

Epoch: 33
Loss: 0.16695752799013827Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/tox21/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/split/GraphMVP/tox21/random/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/tox21/random/train_prop=0.7/tox21_random_4_26-05_11-07-48  ]
[ Using Seed :  4  ]
[ Using device :  cuda:1  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
randomly split
Data(edge_attr=[26, 2], edge_index=[2, 26], id=[1], x=[13, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.572120274308864
ROC train: 0.700301	val: 0.701642	test: 0.649259
PRC train: 0.162270	val: 0.183405	test: 0.141313

Epoch: 2
Loss: 0.38693735034759363
ROC train: 0.750350	val: 0.736563	test: 0.700886
PRC train: 0.259353	val: 0.277746	test: 0.201666

Epoch: 3
Loss: 0.28074874640493025
ROC train: 0.779842	val: 0.757545	test: 0.726905
PRC train: 0.303979	val: 0.307002	test: 0.233947

Epoch: 4
Loss: 0.2296299978447367
ROC train: 0.797775	val: 0.769158	test: 0.735982
PRC train: 0.331410	val: 0.324730	test: 0.249443

Epoch: 5
Loss: 0.21356395817195076
ROC train: 0.804516	val: 0.775851	test: 0.751502
PRC train: 0.354945	val: 0.343144	test: 0.278940

Epoch: 6
Loss: 0.2054383168541898
ROC train: 0.821465	val: 0.788366	test: 0.766449
PRC train: 0.373186	val: 0.351289	test: 0.282334

Epoch: 7
Loss: 0.20139471067205886
ROC train: 0.830726	val: 0.801806	test: 0.777917
PRC train: 0.384363	val: 0.363084	test: 0.299840

Epoch: 8
Loss: 0.19881443538584387
ROC train: 0.833774	val: 0.794463	test: 0.783187
PRC train: 0.399665	val: 0.354695	test: 0.314499

Epoch: 9
Loss: 0.19595769876491895
ROC train: 0.842610	val: 0.805276	test: 0.785464
PRC train: 0.413651	val: 0.368985	test: 0.319947

Epoch: 10
Loss: 0.19572078286511393
ROC train: 0.845971	val: 0.803692	test: 0.784385
PRC train: 0.418663	val: 0.362574	test: 0.316041

Epoch: 11
Loss: 0.19400006962650937
ROC train: 0.851700	val: 0.804260	test: 0.793486
PRC train: 0.432157	val: 0.371829	test: 0.322808

Epoch: 12
Loss: 0.19035273431723843
ROC train: 0.854752	val: 0.810003	test: 0.797971
PRC train: 0.441985	val: 0.370648	test: 0.332339

Epoch: 13
Loss: 0.18885691110765943
ROC train: 0.863445	val: 0.815749	test: 0.795948
PRC train: 0.461002	val: 0.386165	test: 0.341178

Epoch: 14
Loss: 0.18559328038131329
ROC train: 0.858066	val: 0.816803	test: 0.793750
PRC train: 0.456157	val: 0.374796	test: 0.334807

Epoch: 15
Loss: 0.1844439049093613
ROC train: 0.870612	val: 0.821244	test: 0.804716
PRC train: 0.487571	val: 0.397031	test: 0.359486

Epoch: 16
Loss: 0.18323747906811533
ROC train: 0.861994	val: 0.804542	test: 0.803129
PRC train: 0.468088	val: 0.369266	test: 0.343238

Epoch: 17
Loss: 0.182553158999761
ROC train: 0.875526	val: 0.820487	test: 0.810722
PRC train: 0.505955	val: 0.395388	test: 0.369920

Epoch: 18
Loss: 0.18203617528137075
ROC train: 0.876841	val: 0.826239	test: 0.818031
PRC train: 0.515635	val: 0.395306	test: 0.380111

Epoch: 19
Loss: 0.17993572885585352
ROC train: 0.872063	val: 0.819109	test: 0.805595
PRC train: 0.493228	val: 0.394053	test: 0.359831

Epoch: 20
Loss: 0.1777305602122831
ROC train: 0.880869	val: 0.820793	test: 0.809701
PRC train: 0.525130	val: 0.405911	test: 0.376943

Epoch: 21
Loss: 0.1768654664229998
ROC train: 0.884006	val: 0.823674	test: 0.806878
PRC train: 0.533415	val: 0.409141	test: 0.387632

Epoch: 22
Loss: 0.17683235416149612
ROC train: 0.885918	val: 0.821629	test: 0.814039
PRC train: 0.534504	val: 0.407381	test: 0.391823

Epoch: 23
Loss: 0.17378153850778447
ROC train: 0.885912	val: 0.826028	test: 0.818465
PRC train: 0.531808	val: 0.405914	test: 0.393917

Epoch: 24
Loss: 0.1736517467957752
ROC train: 0.887808	val: 0.830654	test: 0.820608
PRC train: 0.543149	val: 0.408713	test: 0.398467

Epoch: 25
Loss: 0.1720252092744148
ROC train: 0.893453	val: 0.826246	test: 0.823294
PRC train: 0.560883	val: 0.412862	test: 0.413427

Epoch: 26
Loss: 0.17269575237004983
ROC train: 0.892112	val: 0.833528	test: 0.820417
PRC train: 0.549683	val: 0.421210	test: 0.387340

Epoch: 27
Loss: 0.17016629936535654
ROC train: 0.897006	val: 0.821804	test: 0.818036
PRC train: 0.567990	val: 0.411403	test: 0.396435

Epoch: 28
Loss: 0.16884595818869413
ROC train: 0.899215	val: 0.825347	test: 0.820238
PRC train: 0.580536	val: 0.420524	test: 0.415321

Epoch: 29
Loss: 0.1673186954840085
ROC train: 0.899191	val: 0.833846	test: 0.821076
PRC train: 0.574116	val: 0.423367	test: 0.411859

Epoch: 30
Loss: 0.16584847638742872
ROC train: 0.903380	val: 0.831365	test: 0.825725
PRC train: 0.593828	val: 0.421509	test: 0.414036

Epoch: 31
Loss: 0.1665265040354002
ROC train: 0.893955	val: 0.817979	test: 0.820079
PRC train: 0.568661	val: 0.404512	test: 0.404359

Epoch: 32
Loss: 0.16545464288749787
ROC train: 0.904680	val: 0.826410	test: 0.825521
PRC train: 0.592602	val: 0.405335	test: 0.421023

Epoch: 33
Loss: 0.16409465537680498Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/tox21/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/split/GraphMVP/tox21/random/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/tox21/random/train_prop=0.7/tox21_random_6_26-05_11-07-48  ]
[ Using Seed :  6  ]
[ Using device :  cuda:1  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
randomly split
Data(edge_attr=[26, 2], edge_index=[2, 26], id=[1], x=[13, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5616410162836988
ROC train: 0.704734	val: 0.700308	test: 0.650796
PRC train: 0.174415	val: 0.195474	test: 0.152565

Epoch: 2
Loss: 0.38114828335773987
ROC train: 0.745521	val: 0.730614	test: 0.687062
PRC train: 0.244994	val: 0.261860	test: 0.195412

Epoch: 3
Loss: 0.27663662022687485
ROC train: 0.778366	val: 0.749341	test: 0.727274
PRC train: 0.306611	val: 0.310383	test: 0.234385

Epoch: 4
Loss: 0.2280686317465551
ROC train: 0.798732	val: 0.764935	test: 0.741013
PRC train: 0.339884	val: 0.319399	test: 0.265069

Epoch: 5
Loss: 0.21199313378843077
ROC train: 0.810303	val: 0.769836	test: 0.764915
PRC train: 0.364007	val: 0.351820	test: 0.289137

Epoch: 6
Loss: 0.20641018712216005
ROC train: 0.814447	val: 0.777444	test: 0.755643
PRC train: 0.361938	val: 0.325012	test: 0.290336

Epoch: 7
Loss: 0.20198010448099082
ROC train: 0.818988	val: 0.792367	test: 0.777122
PRC train: 0.333558	val: 0.303305	test: 0.277245

Epoch: 8
Loss: 0.20018609288262929
ROC train: 0.832343	val: 0.789995	test: 0.779739
PRC train: 0.400295	val: 0.353183	test: 0.314374

Epoch: 9
Loss: 0.1960422179238923
ROC train: 0.841125	val: 0.797519	test: 0.790782
PRC train: 0.412128	val: 0.374587	test: 0.317523

Epoch: 10
Loss: 0.19471931185962113
ROC train: 0.839647	val: 0.790897	test: 0.787026
PRC train: 0.404746	val: 0.360069	test: 0.308503

Epoch: 11
Loss: 0.19458516762876582
ROC train: 0.845155	val: 0.799579	test: 0.787462
PRC train: 0.417626	val: 0.356915	test: 0.321322

Epoch: 12
Loss: 0.19305658833809405
ROC train: 0.847597	val: 0.806686	test: 0.787086
PRC train: 0.431022	val: 0.355333	test: 0.328927

Epoch: 13
Loss: 0.1902827807400408
ROC train: 0.852013	val: 0.801853	test: 0.795889
PRC train: 0.437905	val: 0.367406	test: 0.321977

Epoch: 14
Loss: 0.1884949754728963
ROC train: 0.861593	val: 0.808378	test: 0.797811
PRC train: 0.464101	val: 0.378616	test: 0.342003

Epoch: 15
Loss: 0.18613049374106413
ROC train: 0.861160	val: 0.807907	test: 0.796031
PRC train: 0.453211	val: 0.373359	test: 0.321869

Epoch: 16
Loss: 0.18676009132947763
ROC train: 0.865042	val: 0.810980	test: 0.804379
PRC train: 0.473596	val: 0.371718	test: 0.342136

Epoch: 17
Loss: 0.18393402647258103
ROC train: 0.866915	val: 0.816150	test: 0.810427
PRC train: 0.479508	val: 0.396099	test: 0.352027

Epoch: 18
Loss: 0.18392282625023004
ROC train: 0.868835	val: 0.816461	test: 0.808918
PRC train: 0.485260	val: 0.394145	test: 0.366697

Epoch: 19
Loss: 0.18103677942046106
ROC train: 0.876720	val: 0.817224	test: 0.818970
PRC train: 0.501249	val: 0.381777	test: 0.367710

Epoch: 20
Loss: 0.17927793295810612
ROC train: 0.879983	val: 0.826995	test: 0.814905
PRC train: 0.523268	val: 0.409259	test: 0.375552

Epoch: 21
Loss: 0.18152584081395928
ROC train: 0.879511	val: 0.818262	test: 0.814855
PRC train: 0.512282	val: 0.394934	test: 0.360827

Epoch: 22
Loss: 0.177482934466631
ROC train: 0.883776	val: 0.819139	test: 0.812958
PRC train: 0.532636	val: 0.409093	test: 0.390561

Epoch: 23
Loss: 0.1758878564091546
ROC train: 0.883768	val: 0.821725	test: 0.818617
PRC train: 0.530119	val: 0.406818	test: 0.370979

Epoch: 24
Loss: 0.1766092368094846
ROC train: 0.886880	val: 0.832740	test: 0.816026
PRC train: 0.539458	val: 0.407894	test: 0.397811

Epoch: 25
Loss: 0.17340922828556404
ROC train: 0.887224	val: 0.823938	test: 0.814218
PRC train: 0.536480	val: 0.408726	test: 0.386550

Epoch: 26
Loss: 0.172688388563457
ROC train: 0.892341	val: 0.818306	test: 0.818111
PRC train: 0.557990	val: 0.408471	test: 0.398191

Epoch: 27
Loss: 0.17473733597973035
ROC train: 0.892423	val: 0.824791	test: 0.826461
PRC train: 0.546944	val: 0.414583	test: 0.392721

Epoch: 28
Loss: 0.17003859381687395
ROC train: 0.895270	val: 0.828471	test: 0.828214
PRC train: 0.567574	val: 0.409737	test: 0.410705

Epoch: 29
Loss: 0.16935767425012932
ROC train: 0.898593	val: 0.831343	test: 0.827046
PRC train: 0.574453	val: 0.415876	test: 0.412621

Epoch: 30
Loss: 0.16935979127985
ROC train: 0.896255	val: 0.820679	test: 0.821972
PRC train: 0.566063	val: 0.411502	test: 0.399987

Epoch: 31
Loss: 0.168825929449022
ROC train: 0.898259	val: 0.816934	test: 0.820682
PRC train: 0.569330	val: 0.406799	test: 0.412799

Epoch: 32
Loss: 0.1698444184259296
ROC train: 0.900157	val: 0.819914	test: 0.826341
PRC train: 0.578868	val: 0.413855	test: 0.406034

Epoch: 33
Loss: 0.16816440513394734Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/tox21/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/split/GraphMVP/tox21/random/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/tox21/random/train_prop=0.7/tox21_random_5_26-05_11-07-48  ]
[ Using Seed :  5  ]
[ Using device :  cuda:1  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
randomly split
Data(edge_attr=[26, 2], edge_index=[2, 26], id=[1], x=[13, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.56169600008596
ROC train: 0.698730	val: 0.694000	test: 0.643959
PRC train: 0.157737	val: 0.171446	test: 0.141554

Epoch: 2
Loss: 0.3817125231972191
ROC train: 0.752580	val: 0.739708	test: 0.697642
PRC train: 0.250025	val: 0.264722	test: 0.203579

Epoch: 3
Loss: 0.2759089683855145
ROC train: 0.776209	val: 0.754754	test: 0.715554
PRC train: 0.317173	val: 0.299555	test: 0.236657

Epoch: 4
Loss: 0.23022640581667025
ROC train: 0.799794	val: 0.767103	test: 0.750525
PRC train: 0.344917	val: 0.322200	test: 0.274544

Epoch: 5
Loss: 0.21125839853884038
ROC train: 0.804287	val: 0.772882	test: 0.760647
PRC train: 0.361465	val: 0.336923	test: 0.293975

Epoch: 6
Loss: 0.20424681990280458
ROC train: 0.812506	val: 0.769349	test: 0.771532
PRC train: 0.367072	val: 0.342725	test: 0.289366

Epoch: 7
Loss: 0.20203835397991493
ROC train: 0.825495	val: 0.788066	test: 0.777979
PRC train: 0.385380	val: 0.350947	test: 0.309544

Epoch: 8
Loss: 0.19941360920744208
ROC train: 0.829713	val: 0.792502	test: 0.763568
PRC train: 0.386513	val: 0.347264	test: 0.297856

Epoch: 9
Loss: 0.19738027015619497
ROC train: 0.839513	val: 0.793488	test: 0.784378
PRC train: 0.413362	val: 0.360980	test: 0.327443

Epoch: 10
Loss: 0.19453345234978833
ROC train: 0.845860	val: 0.801737	test: 0.786046
PRC train: 0.423989	val: 0.368521	test: 0.324112

Epoch: 11
Loss: 0.19082746680626092
ROC train: 0.844812	val: 0.797712	test: 0.786837
PRC train: 0.430539	val: 0.366399	test: 0.331807

Epoch: 12
Loss: 0.19225872893841062
ROC train: 0.853182	val: 0.798529	test: 0.792175
PRC train: 0.439210	val: 0.369576	test: 0.327555

Epoch: 13
Loss: 0.1887037879831743
ROC train: 0.853160	val: 0.797745	test: 0.797785
PRC train: 0.453257	val: 0.370984	test: 0.342505

Epoch: 14
Loss: 0.18831036283406333
ROC train: 0.862785	val: 0.812439	test: 0.806176
PRC train: 0.472167	val: 0.386662	test: 0.354055

Epoch: 15
Loss: 0.1855598938810654
ROC train: 0.862339	val: 0.807388	test: 0.797745
PRC train: 0.466121	val: 0.378518	test: 0.350616

Epoch: 16
Loss: 0.1843387900559104
ROC train: 0.867553	val: 0.815078	test: 0.799926
PRC train: 0.484594	val: 0.388425	test: 0.354624

Epoch: 17
Loss: 0.18142697543433572
ROC train: 0.869264	val: 0.815300	test: 0.795204
PRC train: 0.488097	val: 0.385347	test: 0.344739

Epoch: 18
Loss: 0.18412075008755766
ROC train: 0.872983	val: 0.815013	test: 0.802579
PRC train: 0.492759	val: 0.387713	test: 0.358414

Epoch: 19
Loss: 0.18127002785543847
ROC train: 0.875331	val: 0.814679	test: 0.809306
PRC train: 0.507371	val: 0.403084	test: 0.371190

Epoch: 20
Loss: 0.18173167750892147
ROC train: 0.875910	val: 0.808141	test: 0.800469
PRC train: 0.501389	val: 0.406502	test: 0.359281

Epoch: 21
Loss: 0.17953370087412096
ROC train: 0.877787	val: 0.813319	test: 0.809372
PRC train: 0.516122	val: 0.405225	test: 0.370758

Epoch: 22
Loss: 0.1759424179625214
ROC train: 0.880320	val: 0.828177	test: 0.802681
PRC train: 0.507377	val: 0.401899	test: 0.369051

Epoch: 23
Loss: 0.17758644648064822
ROC train: 0.886466	val: 0.820411	test: 0.811972
PRC train: 0.531608	val: 0.411967	test: 0.391255

Epoch: 24
Loss: 0.17608644421459055
ROC train: 0.883129	val: 0.817057	test: 0.804701
PRC train: 0.530267	val: 0.396414	test: 0.374730

Epoch: 25
Loss: 0.17429587885914247
ROC train: 0.888712	val: 0.821596	test: 0.821206
PRC train: 0.539118	val: 0.405684	test: 0.390603

Epoch: 26
Loss: 0.17311628687594777
ROC train: 0.893063	val: 0.833662	test: 0.821241
PRC train: 0.563345	val: 0.426232	test: 0.408293

Epoch: 27
Loss: 0.17166446693282172
ROC train: 0.894197	val: 0.821281	test: 0.823161
PRC train: 0.561798	val: 0.415460	test: 0.388525

Epoch: 28
Loss: 0.16875642735740684
ROC train: 0.897225	val: 0.826592	test: 0.822243
PRC train: 0.574150	val: 0.429079	test: 0.405232

Epoch: 29
Loss: 0.16773331440925876
ROC train: 0.900880	val: 0.828868	test: 0.829341
PRC train: 0.583360	val: 0.419076	test: 0.410616

Epoch: 30
Loss: 0.16650935928981145
ROC train: 0.902281	val: 0.832204	test: 0.825014
PRC train: 0.582928	val: 0.429248	test: 0.410991

Epoch: 31
Loss: 0.16667954665027915
ROC train: 0.901095	val: 0.825300	test: 0.827286
PRC train: 0.581420	val: 0.421887	test: 0.410715

Epoch: 32
Loss: 0.16623130873167416
ROC train: 0.906521	val: 0.829085	test: 0.820400
PRC train: 0.599684	val: 0.438715	test: 0.413683

Epoch: 33
Loss: 0.16306536994859153Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/tox21/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/split/GraphMVP/tox21/random/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/tox21/random/train_prop=0.8/tox21_random_5_26-05_11-07-48  ]
[ Using Seed :  5  ]
[ Using device :  cuda:2  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
randomly split
Data(edge_attr=[26, 2], edge_index=[2, 26], id=[1], x=[13, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5443329065124574
ROC train: 0.714345	val: 0.686886	test: 0.657751
PRC train: 0.179381	val: 0.184254	test: 0.154567

Epoch: 2
Loss: 0.35690997199556795
ROC train: 0.758850	val: 0.724897	test: 0.698110
PRC train: 0.263084	val: 0.247688	test: 0.210819

Epoch: 3
Loss: 0.2576864419588974
ROC train: 0.782203	val: 0.748143	test: 0.723877
PRC train: 0.330848	val: 0.293030	test: 0.233779

Epoch: 4
Loss: 0.22105102146590805
ROC train: 0.804126	val: 0.764838	test: 0.755979
PRC train: 0.361637	val: 0.319989	test: 0.264320

Epoch: 5
Loss: 0.2102631985143634
ROC train: 0.810744	val: 0.756734	test: 0.749995
PRC train: 0.360676	val: 0.324341	test: 0.255439

Epoch: 6
Loss: 0.2048439599943568
ROC train: 0.809444	val: 0.772364	test: 0.755994
PRC train: 0.354693	val: 0.324741	test: 0.256969

Epoch: 7
Loss: 0.2029048271558473
ROC train: 0.827372	val: 0.783039	test: 0.773384
PRC train: 0.383743	val: 0.338699	test: 0.282001

Epoch: 8
Loss: 0.2004532010269078
ROC train: 0.830737	val: 0.782705	test: 0.774048
PRC train: 0.384403	val: 0.337184	test: 0.284888

Epoch: 9
Loss: 0.1969983906270351
ROC train: 0.839857	val: 0.799732	test: 0.789332
PRC train: 0.408077	val: 0.356274	test: 0.303321

Epoch: 10
Loss: 0.1962593825854363
ROC train: 0.845293	val: 0.799866	test: 0.794888
PRC train: 0.423673	val: 0.363903	test: 0.313170

Epoch: 11
Loss: 0.19445285010502697
ROC train: 0.850951	val: 0.808146	test: 0.799680
PRC train: 0.446956	val: 0.375620	test: 0.330588

Epoch: 12
Loss: 0.19194080140346206
ROC train: 0.853733	val: 0.811279	test: 0.805301
PRC train: 0.440863	val: 0.376322	test: 0.322895

Epoch: 13
Loss: 0.1899765635814582
ROC train: 0.853472	val: 0.804898	test: 0.787710
PRC train: 0.441945	val: 0.382691	test: 0.310389

Epoch: 14
Loss: 0.18915271038174555
ROC train: 0.859612	val: 0.820215	test: 0.803572
PRC train: 0.461762	val: 0.400862	test: 0.343864

Epoch: 15
Loss: 0.1879302706191215
ROC train: 0.865427	val: 0.814210	test: 0.797656
PRC train: 0.469763	val: 0.396061	test: 0.326321

Epoch: 16
Loss: 0.18756343636397296
ROC train: 0.870027	val: 0.821444	test: 0.809580
PRC train: 0.483617	val: 0.411471	test: 0.350369

Epoch: 17
Loss: 0.1842896369733854
ROC train: 0.869269	val: 0.823123	test: 0.804282
PRC train: 0.479274	val: 0.398347	test: 0.331825

Epoch: 18
Loss: 0.18491850174681917
ROC train: 0.872460	val: 0.824052	test: 0.810673
PRC train: 0.497116	val: 0.417782	test: 0.364760

Epoch: 19
Loss: 0.18176429567914096
ROC train: 0.874432	val: 0.820109	test: 0.809262
PRC train: 0.499966	val: 0.427903	test: 0.362261

Epoch: 20
Loss: 0.18090685542148446
ROC train: 0.879542	val: 0.823991	test: 0.815453
PRC train: 0.507434	val: 0.415623	test: 0.359566

Epoch: 21
Loss: 0.1808774919505806
ROC train: 0.870641	val: 0.814569	test: 0.807967
PRC train: 0.494621	val: 0.407468	test: 0.351627

Epoch: 22
Loss: 0.17710128376587753
ROC train: 0.883286	val: 0.824317	test: 0.820260
PRC train: 0.527297	val: 0.438139	test: 0.362643

Epoch: 23
Loss: 0.1755184063634696
ROC train: 0.888626	val: 0.827068	test: 0.829701
PRC train: 0.539696	val: 0.443470	test: 0.378237

Epoch: 24
Loss: 0.17664108849875604
ROC train: 0.888209	val: 0.831018	test: 0.827169
PRC train: 0.545755	val: 0.451367	test: 0.380481

Epoch: 25
Loss: 0.1748146351235211
ROC train: 0.893986	val: 0.837357	test: 0.822974
PRC train: 0.557534	val: 0.462754	test: 0.380174

Epoch: 26
Loss: 0.17195445603213855
ROC train: 0.895167	val: 0.834630	test: 0.830604
PRC train: 0.554374	val: 0.453132	test: 0.389177

Epoch: 27
Loss: 0.17189687947815774
ROC train: 0.898425	val: 0.834880	test: 0.823987
PRC train: 0.567953	val: 0.469293	test: 0.383115

Epoch: 28
Loss: 0.17025196843885354
ROC train: 0.898676	val: 0.832721	test: 0.833943
PRC train: 0.576076	val: 0.459863	test: 0.397413

Epoch: 29
Loss: 0.17077381402614356
ROC train: 0.898004	val: 0.831500	test: 0.827088
PRC train: 0.568759	val: 0.477964	test: 0.406276

Epoch: 30
Loss: 0.17146844904606418
ROC train: 0.894815	val: 0.829112	test: 0.819649
PRC train: 0.558124	val: 0.455789	test: 0.396952

Epoch: 31
Loss: 0.16989183842887662
ROC train: 0.902973	val: 0.833189	test: 0.834117
PRC train: 0.586750	val: 0.483051	test: 0.401022

Epoch: 32
Loss: 0.1680586264744563
ROC train: 0.902926	val: 0.833777	test: 0.832707
PRC train: 0.588513	val: 0.485548	test: 0.405301

Epoch: 33
Loss: 0.16753662318685614Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/tox21/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/split/GraphMVP/tox21/random/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/tox21/random/train_prop=0.8/tox21_random_6_26-05_11-07-48  ]
[ Using Seed :  6  ]
[ Using device :  cuda:2  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
randomly split
Data(edge_attr=[26, 2], edge_index=[2, 26], id=[1], x=[13, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5495136275125716
ROC train: 0.689296	val: 0.664180	test: 0.624160
PRC train: 0.152993	val: 0.155169	test: 0.131626

Epoch: 2
Loss: 0.3558794705282441
ROC train: 0.744375	val: 0.709693	test: 0.665272
PRC train: 0.249465	val: 0.235058	test: 0.163925

Epoch: 3
Loss: 0.25865530582905905
ROC train: 0.781512	val: 0.743970	test: 0.728096
PRC train: 0.316289	val: 0.288419	test: 0.227523

Epoch: 4
Loss: 0.2209439653657835
ROC train: 0.803525	val: 0.760038	test: 0.752248
PRC train: 0.344717	val: 0.299880	test: 0.243843

Epoch: 5
Loss: 0.2099137536955508
ROC train: 0.813313	val: 0.767545	test: 0.760976
PRC train: 0.359149	val: 0.315656	test: 0.266211

Epoch: 6
Loss: 0.2049521492013867
ROC train: 0.824422	val: 0.783064	test: 0.770866
PRC train: 0.379714	val: 0.327986	test: 0.285244

Epoch: 7
Loss: 0.19990966338482752
ROC train: 0.830643	val: 0.778358	test: 0.783867
PRC train: 0.384506	val: 0.340502	test: 0.291285

Epoch: 8
Loss: 0.2002820291639962
ROC train: 0.837733	val: 0.787486	test: 0.793866
PRC train: 0.405478	val: 0.350068	test: 0.304828

Epoch: 9
Loss: 0.1974946270152199
ROC train: 0.844962	val: 0.787317	test: 0.791318
PRC train: 0.415231	val: 0.346297	test: 0.307338

Epoch: 10
Loss: 0.195136358737441
ROC train: 0.849771	val: 0.802565	test: 0.796538
PRC train: 0.433246	val: 0.363250	test: 0.311864

Epoch: 11
Loss: 0.19191183358444172
ROC train: 0.848471	val: 0.795167	test: 0.795900
PRC train: 0.428707	val: 0.371279	test: 0.311937

Epoch: 12
Loss: 0.19092560073120038
ROC train: 0.856112	val: 0.794709	test: 0.795454
PRC train: 0.438569	val: 0.363545	test: 0.304621

Epoch: 13
Loss: 0.1893567353726533
ROC train: 0.859956	val: 0.797961	test: 0.808555
PRC train: 0.455829	val: 0.375909	test: 0.339718

Epoch: 14
Loss: 0.18802518097139306
ROC train: 0.865070	val: 0.801672	test: 0.802385
PRC train: 0.465619	val: 0.381758	test: 0.325824

Epoch: 15
Loss: 0.1882832561136568
ROC train: 0.868191	val: 0.804000	test: 0.803957
PRC train: 0.472002	val: 0.386427	test: 0.326001

Epoch: 16
Loss: 0.18448769756356348
ROC train: 0.872119	val: 0.812809	test: 0.806013
PRC train: 0.490631	val: 0.400271	test: 0.347432

Epoch: 17
Loss: 0.18423440023272722
ROC train: 0.869207	val: 0.801029	test: 0.811408
PRC train: 0.472043	val: 0.379165	test: 0.353546

Epoch: 18
Loss: 0.18407253892521613
ROC train: 0.878347	val: 0.815266	test: 0.822586
PRC train: 0.505597	val: 0.394389	test: 0.351935

Epoch: 19
Loss: 0.1815520183794529
ROC train: 0.876520	val: 0.811574	test: 0.798738
PRC train: 0.502829	val: 0.403019	test: 0.356534

Epoch: 20
Loss: 0.18058296799920664
ROC train: 0.878743	val: 0.820133	test: 0.818813
PRC train: 0.504234	val: 0.411901	test: 0.361617

Epoch: 21
Loss: 0.17909616976788206
ROC train: 0.881441	val: 0.820411	test: 0.823618
PRC train: 0.520091	val: 0.414075	test: 0.379411

Epoch: 22
Loss: 0.17802131031214766
ROC train: 0.886815	val: 0.820607	test: 0.818149
PRC train: 0.534054	val: 0.436250	test: 0.377415

Epoch: 23
Loss: 0.17459565381361558
ROC train: 0.893054	val: 0.826713	test: 0.826172
PRC train: 0.553613	val: 0.441066	test: 0.398625

Epoch: 24
Loss: 0.17459034687953992
ROC train: 0.892727	val: 0.822539	test: 0.823251
PRC train: 0.551348	val: 0.447492	test: 0.395135

Epoch: 25
Loss: 0.17388479498186907
ROC train: 0.896110	val: 0.827220	test: 0.819822
PRC train: 0.564865	val: 0.461216	test: 0.412923

Epoch: 26
Loss: 0.17247635292911506
ROC train: 0.894821	val: 0.829582	test: 0.814388
PRC train: 0.563384	val: 0.449037	test: 0.389626

Epoch: 27
Loss: 0.1708697077898288
ROC train: 0.896439	val: 0.820202	test: 0.824983
PRC train: 0.560317	val: 0.439026	test: 0.386827

Epoch: 28
Loss: 0.170748541987219
ROC train: 0.902315	val: 0.826769	test: 0.825276
PRC train: 0.580027	val: 0.456533	test: 0.400561

Epoch: 29
Loss: 0.17160335236710048
ROC train: 0.901898	val: 0.831870	test: 0.835673
PRC train: 0.577668	val: 0.461102	test: 0.422031

Epoch: 30
Loss: 0.16965778322198816
ROC train: 0.900332	val: 0.821513	test: 0.818321
PRC train: 0.568412	val: 0.456725	test: 0.394493

Epoch: 31
Loss: 0.16627207105675396
ROC train: 0.906189	val: 0.827234	test: 0.830298
PRC train: 0.590135	val: 0.476962	test: 0.421983

Epoch: 32
Loss: 0.16716905698367632
ROC train: 0.904644	val: 0.823418	test: 0.824600
PRC train: 0.587744	val: 0.454237	test: 0.394524

Epoch: 33
Loss: 0.16502856864944174Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/tox21/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: tox21
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/split/GraphMVP/tox21/random/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/tox21/random/train_prop=0.8/tox21_random_4_26-05_11-07-48  ]
[ Using Seed :  4  ]
[ Using device :  cuda:2  ]
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
randomly split
Data(edge_attr=[26, 2], edge_index=[2, 26], id=[1], x=[13, 2], y=[12])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5571751266837425
ROC train: 0.699143	val: 0.672020	test: 0.635419
PRC train: 0.161498	val: 0.160745	test: 0.137291

Epoch: 2
Loss: 0.3607635183145977
ROC train: 0.763104	val: 0.730144	test: 0.692610
PRC train: 0.264724	val: 0.248421	test: 0.195244

Epoch: 3
Loss: 0.25971426176102674
ROC train: 0.788011	val: 0.748962	test: 0.725262
PRC train: 0.310778	val: 0.272865	test: 0.217056

Epoch: 4
Loss: 0.2204821310876022
ROC train: 0.803748	val: 0.761941	test: 0.753603
PRC train: 0.355245	val: 0.307447	test: 0.257284

Epoch: 5
Loss: 0.20855204801807248
ROC train: 0.818523	val: 0.761723	test: 0.761736
PRC train: 0.377658	val: 0.319684	test: 0.281698

Epoch: 6
Loss: 0.20317557318913074
ROC train: 0.821022	val: 0.773099	test: 0.756040
PRC train: 0.381745	val: 0.325290	test: 0.275894

Epoch: 7
Loss: 0.20074326140939966
ROC train: 0.827892	val: 0.773148	test: 0.772574
PRC train: 0.393148	val: 0.334339	test: 0.284063

Epoch: 8
Loss: 0.1976993319947053
ROC train: 0.836821	val: 0.781337	test: 0.778303
PRC train: 0.402929	val: 0.343261	test: 0.288761

Epoch: 9
Loss: 0.19735634171498945
ROC train: 0.845424	val: 0.786771	test: 0.787919
PRC train: 0.420414	val: 0.351931	test: 0.312597

Epoch: 10
Loss: 0.1954207764839639
ROC train: 0.846123	val: 0.793257	test: 0.789637
PRC train: 0.423740	val: 0.346124	test: 0.303666

Epoch: 11
Loss: 0.19457392882576122
ROC train: 0.854197	val: 0.793284	test: 0.783496
PRC train: 0.442928	val: 0.368200	test: 0.311754

Epoch: 12
Loss: 0.1910605970204515
ROC train: 0.850918	val: 0.790220	test: 0.791120
PRC train: 0.427833	val: 0.359989	test: 0.309987

Epoch: 13
Loss: 0.18795409247805472
ROC train: 0.862136	val: 0.798200	test: 0.798614
PRC train: 0.454936	val: 0.363105	test: 0.335077

Epoch: 14
Loss: 0.18768627835550053
ROC train: 0.866354	val: 0.799095	test: 0.792777
PRC train: 0.468183	val: 0.374672	test: 0.337970

Epoch: 15
Loss: 0.18711800106341495
ROC train: 0.869743	val: 0.807473	test: 0.803291
PRC train: 0.473728	val: 0.397450	test: 0.348144

Epoch: 16
Loss: 0.18597153866101476
ROC train: 0.870051	val: 0.801965	test: 0.808629
PRC train: 0.478716	val: 0.397704	test: 0.350990

Epoch: 17
Loss: 0.18342910256093248
ROC train: 0.872012	val: 0.805152	test: 0.809531
PRC train: 0.483144	val: 0.392900	test: 0.354213

Epoch: 18
Loss: 0.18375904950881256
ROC train: 0.878266	val: 0.813359	test: 0.813462
PRC train: 0.497999	val: 0.404179	test: 0.359418

Epoch: 19
Loss: 0.18071766854255153
ROC train: 0.875771	val: 0.811607	test: 0.807328
PRC train: 0.498249	val: 0.406191	test: 0.360389

Epoch: 20
Loss: 0.17912916935121742
ROC train: 0.878221	val: 0.811818	test: 0.814733
PRC train: 0.512233	val: 0.417549	test: 0.376469

Epoch: 21
Loss: 0.17803517588950415
ROC train: 0.884967	val: 0.822505	test: 0.813978
PRC train: 0.525063	val: 0.426997	test: 0.376020

Epoch: 22
Loss: 0.17853495155532298
ROC train: 0.889214	val: 0.829724	test: 0.826544
PRC train: 0.542436	val: 0.458148	test: 0.400234

Epoch: 23
Loss: 0.1758725367697454
ROC train: 0.888980	val: 0.827906	test: 0.818826
PRC train: 0.527374	val: 0.420777	test: 0.382467

Epoch: 24
Loss: 0.17372731295517838
ROC train: 0.889479	val: 0.827141	test: 0.827964
PRC train: 0.543860	val: 0.449864	test: 0.395123

Epoch: 25
Loss: 0.1750680734226486
ROC train: 0.891547	val: 0.821143	test: 0.819190
PRC train: 0.544433	val: 0.449326	test: 0.391747

Epoch: 26
Loss: 0.17353803721191557
ROC train: 0.893093	val: 0.833006	test: 0.826182
PRC train: 0.546944	val: 0.447959	test: 0.388238

Epoch: 27
Loss: 0.1729838173764795
ROC train: 0.894704	val: 0.830320	test: 0.832820
PRC train: 0.550764	val: 0.465854	test: 0.392610

Epoch: 28
Loss: 0.171242843995776
ROC train: 0.897251	val: 0.833458	test: 0.824277
PRC train: 0.560368	val: 0.457724	test: 0.400664

Epoch: 29
Loss: 0.16874487365561763
ROC train: 0.902210	val: 0.836031	test: 0.828341
PRC train: 0.577879	val: 0.471565	test: 0.409481

Epoch: 30
Loss: 0.16741162275415927
ROC train: 0.905470	val: 0.835513	test: 0.826118
PRC train: 0.585492	val: 0.473150	test: 0.417501

Epoch: 31
Loss: 0.16689264440782695
ROC train: 0.904507	val: 0.828149	test: 0.831192
PRC train: 0.586133	val: 0.478254	test: 0.433201

Epoch: 32
Loss: 0.16704886057576296
ROC train: 0.909210	val: 0.837197	test: 0.833224
PRC train: 0.599335	val: 0.482187	test: 0.428183

Epoch: 33
Loss: 0.16382689142674536
ROC train: 0.907658	val: 0.822102	test: 0.811587
PRC train: 0.599963	val: 0.411112	test: 0.405429

Epoch: 34
Loss: 0.16708098837612934
ROC train: 0.910972	val: 0.820738	test: 0.812573
PRC train: 0.611607	val: 0.409481	test: 0.413093

Epoch: 35
Loss: 0.16493157566693178
ROC train: 0.910401	val: 0.825158	test: 0.814717
PRC train: 0.618259	val: 0.408352	test: 0.429800

Epoch: 36
Loss: 0.16442379803767437
ROC train: 0.907989	val: 0.828562	test: 0.808994
PRC train: 0.601738	val: 0.418816	test: 0.407243

Epoch: 37
Loss: 0.16286114643349955
ROC train: 0.911872	val: 0.821635	test: 0.813424
PRC train: 0.617825	val: 0.406233	test: 0.413066

Epoch: 38
Loss: 0.16301338710205668
ROC train: 0.916199	val: 0.829119	test: 0.813428
PRC train: 0.625074	val: 0.415418	test: 0.429727

Epoch: 39
Loss: 0.16195792446857263
ROC train: 0.919677	val: 0.821483	test: 0.812447
PRC train: 0.637185	val: 0.416050	test: 0.429566

Epoch: 40
Loss: 0.15976968482438508
ROC train: 0.918322	val: 0.827444	test: 0.815927
PRC train: 0.636324	val: 0.409345	test: 0.444432

Epoch: 41
Loss: 0.16411038334428774
ROC train: 0.920462	val: 0.827735	test: 0.819004
PRC train: 0.643011	val: 0.405435	test: 0.439320

Epoch: 42
Loss: 0.1607266435402634
ROC train: 0.921923	val: 0.828678	test: 0.816406
PRC train: 0.642355	val: 0.409659	test: 0.417077

Epoch: 43
Loss: 0.16046538477121347
ROC train: 0.921503	val: 0.826591	test: 0.818115
PRC train: 0.633894	val: 0.410789	test: 0.440327

Epoch: 44
Loss: 0.15604465027992212
ROC train: 0.928725	val: 0.828509	test: 0.817472
PRC train: 0.663994	val: 0.421731	test: 0.442047

Epoch: 45
Loss: 0.15605665843701946
ROC train: 0.925688	val: 0.826403	test: 0.814901
PRC train: 0.654300	val: 0.424108	test: 0.436254

Epoch: 46
Loss: 0.1564149847864769
ROC train: 0.927057	val: 0.829544	test: 0.820919
PRC train: 0.661360	val: 0.413510	test: 0.434339

Epoch: 47
Loss: 0.15746645005365875
ROC train: 0.926784	val: 0.821169	test: 0.815704
PRC train: 0.670215	val: 0.413286	test: 0.439950

Epoch: 48
Loss: 0.15297647030281772
ROC train: 0.929931	val: 0.831564	test: 0.819219
PRC train: 0.663409	val: 0.417743	test: 0.432511

Epoch: 49
Loss: 0.15485150770002493
ROC train: 0.929512	val: 0.825164	test: 0.819201
PRC train: 0.663939	val: 0.401900	test: 0.417485

Epoch: 50
Loss: 0.15398151521246367
ROC train: 0.930647	val: 0.822460	test: 0.814885
PRC train: 0.669350	val: 0.407769	test: 0.434032

Epoch: 51
Loss: 0.15421104370614572
ROC train: 0.934184	val: 0.827977	test: 0.817065
PRC train: 0.677303	val: 0.406025	test: 0.418414

Epoch: 52
Loss: 0.15274288924300275
ROC train: 0.934056	val: 0.830161	test: 0.815354
PRC train: 0.673889	val: 0.417022	test: 0.430006

Epoch: 53
Loss: 0.15179039677960882
ROC train: 0.936843	val: 0.828746	test: 0.817615
PRC train: 0.690881	val: 0.424813	test: 0.437729

Epoch: 54
Loss: 0.1498986410357411
ROC train: 0.938746	val: 0.830433	test: 0.817716
PRC train: 0.700317	val: 0.420890	test: 0.437710

Epoch: 55
Loss: 0.14808930018987762
ROC train: 0.939748	val: 0.829211	test: 0.817020
PRC train: 0.701354	val: 0.423894	test: 0.439363

Epoch: 56
Loss: 0.14850707633797167
ROC train: 0.940228	val: 0.828547	test: 0.814989
PRC train: 0.694686	val: 0.426823	test: 0.441925

Epoch: 57
Loss: 0.1460311619551717
ROC train: 0.942236	val: 0.830384	test: 0.817156
PRC train: 0.708999	val: 0.424434	test: 0.450879

Epoch: 58
Loss: 0.14598658857096228
ROC train: 0.943899	val: 0.832597	test: 0.817654
PRC train: 0.711068	val: 0.428971	test: 0.437122

Epoch: 59
Loss: 0.14570565911039404
ROC train: 0.943311	val: 0.825842	test: 0.820410
PRC train: 0.719349	val: 0.415404	test: 0.445559

Epoch: 60
Loss: 0.1441376588275142
ROC train: 0.944357	val: 0.837718	test: 0.825642
PRC train: 0.719495	val: 0.434776	test: 0.451242

Epoch: 61
Loss: 0.14405155348722226
ROC train: 0.944168	val: 0.833629	test: 0.819940
PRC train: 0.714731	val: 0.432161	test: 0.446991

Epoch: 62
Loss: 0.14492843247357157
ROC train: 0.947175	val: 0.828506	test: 0.816219
PRC train: 0.722384	val: 0.417050	test: 0.433753

Epoch: 63
Loss: 0.14536443806501811
ROC train: 0.947188	val: 0.833161	test: 0.824081
PRC train: 0.724530	val: 0.425091	test: 0.452013

Epoch: 64
Loss: 0.14096241815296137
ROC train: 0.949038	val: 0.827354	test: 0.817191
PRC train: 0.733382	val: 0.416089	test: 0.433502

Epoch: 65
Loss: 0.14120202801045215
ROC train: 0.950383	val: 0.833299	test: 0.819272
PRC train: 0.733416	val: 0.415508	test: 0.438856

Epoch: 66
Loss: 0.1407828911966151
ROC train: 0.947506	val: 0.830453	test: 0.817127
PRC train: 0.731817	val: 0.422958	test: 0.445273

Epoch: 67
Loss: 0.14048651837332407
ROC train: 0.952911	val: 0.829939	test: 0.819445
PRC train: 0.747271	val: 0.426140	test: 0.433317

Epoch: 68
Loss: 0.1388684827414582
ROC train: 0.952491	val: 0.830014	test: 0.821031
PRC train: 0.748988	val: 0.425708	test: 0.449830

Epoch: 69
Loss: 0.13772724237769418
ROC train: 0.953956	val: 0.827904	test: 0.816371
PRC train: 0.749482	val: 0.427802	test: 0.437553

Epoch: 70
Loss: 0.13673448606526215
ROC train: 0.954737	val: 0.827193	test: 0.818316
PRC train: 0.758777	val: 0.418265	test: 0.442119

Epoch: 71
Loss: 0.1359899462645404
ROC train: 0.956056	val: 0.829464	test: 0.819161
PRC train: 0.758751	val: 0.422630	test: 0.453256

Epoch: 72
Loss: 0.13610221570802078
ROC train: 0.956784	val: 0.829083	test: 0.818777
PRC train: 0.760785	val: 0.420491	test: 0.444604

Epoch: 73
Loss: 0.13636136065821086
ROC train: 0.955567	val: 0.828199	test: 0.811709
PRC train: 0.755637	val: 0.423191	test: 0.426707

Epoch: 74
Loss: 0.13332257937641476
ROC train: 0.955723	val: 0.827458	test: 0.819236
PRC train: 0.756880	val: 0.417886	test: 0.432466

Epoch: 75
Loss: 0.13430084458790803
ROC train: 0.958909	val: 0.827170	test: 0.814920
PRC train: 0.772435	val: 0.428805	test: 0.445613

Epoch: 76
Loss: 0.13220578139011363
ROC train: 0.958820	val: 0.826467	test: 0.817066
PRC train: 0.777520	val: 0.425998	test: 0.438985

Epoch: 77
Loss: 0.13318101778034236
ROC train: 0.960288	val: 0.831582	test: 0.819562
PRC train: 0.784933	val: 0.435631	test: 0.452702

Epoch: 78
Loss: 0.13138807436929348
ROC train: 0.961494	val: 0.829702	test: 0.816807
PRC train: 0.789801	val: 0.425389	test: 0.430022

Epoch: 79
Loss: 0.13069104404729606
ROC train: 0.961667	val: 0.826790	test: 0.821431
PRC train: 0.788462	val: 0.443435	test: 0.447408

Epoch: 80
Loss: 0.13128534416387205
ROC train: 0.961746	val: 0.828824	test: 0.810501
PRC train: 0.784485	val: 0.427631	test: 0.434428

Epoch: 81
Loss: 0.12945867731895594
ROC train: 0.962703	val: 0.825211	test: 0.813117
PRC train: 0.792467	val: 0.445659	test: 0.440966

Epoch: 82
Loss: 0.12875338083461613
ROC train: 0.965146	val: 0.827575	test: 0.814610
PRC train: 0.798765	val: 0.433273	test: 0.436318

Epoch: 83
Loss: 0.12492070548425704
ROC train: 0.963321	val: 0.823598	test: 0.813736
PRC train: 0.800282	val: 0.429775	test: 0.442951

Epoch: 84
Loss: 0.1278839672482038
ROC train: 0.965371	val: 0.824604	test: 0.813375
PRC train: 0.799621	val: 0.423993	test: 0.422855

Epoch: 85
Loss: 0.12805768821398886
ROC train: 0.966002	val: 0.826683	test: 0.815091
PRC train: 0.805372	val: 0.431704	test: 0.443624

Epoch: 86
Loss: 0.12588772070374468
ROC train: 0.966689	val: 0.822865	test: 0.810358
PRC train: 0.806473	val: 0.434907	test: 0.433473

Epoch: 87
Loss: 0.12435233644159235
ROC train: 0.967874	val: 0.823514	test: 0.814949
PRC train: 0.811566	val: 0.423418	test: 0.435111

Epoch: 88
Loss: 0.12548845899896244
ROC train: 0.968338	val: 0.827097	test: 0.817340
PRC train: 0.813839	val: 0.432090	test: 0.435683

Epoch: 89
Loss: 0.1244984959563488
ROC train: 0.967962	val: 0.825682	test: 0.811032
PRC train: 0.804992	val: 0.430353	test: 0.436116

Epoch: 90
Loss: 0.12229581888375037
ROC train: 0.970411	val: 0.831201	test: 0.812958
PRC train: 0.823463	val: 0.436204	test: 0.435543

Epoch: 91
Loss: 0.12327622432449026
ROC train: 0.971581	val: 0.818691	test: 0.809500
PRC train: 0.831929	val: 0.425854	test: 0.435994

Epoch: 92
Loss: 0.12205202516046555
ROC train: 0.968251	val: 0.830595	test: 0.813355
PRC train: 0.812703	val: 0.427682	test: 0.428207

Epoch: 93
Loss: 0.12261744706805
ROC train: 0.969781	val: 0.834224	test: 0.819156
PRC train: 0.824465	val: 0.430468	test: 0.428746

Epoch: 94
Loss: 0.12094612488483562
ROC train: 0.905759	val: 0.821933	test: 0.815429
PRC train: 0.595058	val: 0.406275	test: 0.401721

Epoch: 34
Loss: 0.16710841777794463
ROC train: 0.905294	val: 0.823268	test: 0.821271
PRC train: 0.594593	val: 0.407300	test: 0.418598

Epoch: 35
Loss: 0.16788980004359874
ROC train: 0.905276	val: 0.829776	test: 0.815669
PRC train: 0.594825	val: 0.407871	test: 0.413622

Epoch: 36
Loss: 0.16612160103520904
ROC train: 0.912436	val: 0.831516	test: 0.824676
PRC train: 0.623234	val: 0.410954	test: 0.418064

Epoch: 37
Loss: 0.1630488544416234
ROC train: 0.914323	val: 0.828451	test: 0.821796
PRC train: 0.629164	val: 0.422714	test: 0.428095

Epoch: 38
Loss: 0.1633616396650838
ROC train: 0.915634	val: 0.830350	test: 0.820794
PRC train: 0.630933	val: 0.421765	test: 0.424523

Epoch: 39
Loss: 0.1616378847922763
ROC train: 0.916347	val: 0.827740	test: 0.825027
PRC train: 0.636741	val: 0.425901	test: 0.432122

Epoch: 40
Loss: 0.16037897796314868
ROC train: 0.918965	val: 0.828932	test: 0.818332
PRC train: 0.632661	val: 0.426788	test: 0.419732

Epoch: 41
Loss: 0.1612926403773253
ROC train: 0.916224	val: 0.829273	test: 0.816733
PRC train: 0.628704	val: 0.423233	test: 0.418341

Epoch: 42
Loss: 0.16146078964766147
ROC train: 0.918854	val: 0.826767	test: 0.819503
PRC train: 0.638778	val: 0.411447	test: 0.426499

Epoch: 43
Loss: 0.15886450421570814
ROC train: 0.920895	val: 0.830176	test: 0.823168
PRC train: 0.643708	val: 0.409333	test: 0.421307

Epoch: 44
Loss: 0.16093810150776325
ROC train: 0.922241	val: 0.832173	test: 0.827994
PRC train: 0.646192	val: 0.424213	test: 0.431130

Epoch: 45
Loss: 0.15926692752582167
ROC train: 0.924230	val: 0.831175	test: 0.823803
PRC train: 0.648547	val: 0.421259	test: 0.422796

Epoch: 46
Loss: 0.15753045661607362
ROC train: 0.924227	val: 0.828667	test: 0.823762
PRC train: 0.654161	val: 0.423216	test: 0.434404

Epoch: 47
Loss: 0.15494974872587497
ROC train: 0.923925	val: 0.829568	test: 0.820946
PRC train: 0.649364	val: 0.418332	test: 0.425722

Epoch: 48
Loss: 0.15470027330981795
ROC train: 0.927365	val: 0.832202	test: 0.825968
PRC train: 0.668489	val: 0.420736	test: 0.433326

Epoch: 49
Loss: 0.15647879085594832
ROC train: 0.930795	val: 0.832311	test: 0.827344
PRC train: 0.671736	val: 0.432602	test: 0.437469

Epoch: 50
Loss: 0.15562752988349918
ROC train: 0.927449	val: 0.835016	test: 0.820714
PRC train: 0.665542	val: 0.429525	test: 0.430983

Epoch: 51
Loss: 0.1537544571229619
ROC train: 0.931231	val: 0.831255	test: 0.823660
PRC train: 0.675859	val: 0.429626	test: 0.431746

Epoch: 52
Loss: 0.1523722707362828
ROC train: 0.931097	val: 0.836861	test: 0.825910
PRC train: 0.669269	val: 0.431324	test: 0.431534

Epoch: 53
Loss: 0.1512234866510576
ROC train: 0.933638	val: 0.829741	test: 0.821770
PRC train: 0.679539	val: 0.433190	test: 0.431015

Epoch: 54
Loss: 0.15204875732941692
ROC train: 0.932657	val: 0.837837	test: 0.825027
PRC train: 0.684294	val: 0.439315	test: 0.444545

Epoch: 55
Loss: 0.15313730028720132
ROC train: 0.934409	val: 0.834878	test: 0.829664
PRC train: 0.674376	val: 0.429876	test: 0.432902

Epoch: 56
Loss: 0.14950726125551184
ROC train: 0.939496	val: 0.837549	test: 0.827037
PRC train: 0.698104	val: 0.431683	test: 0.438803

Epoch: 57
Loss: 0.15159423389951002
ROC train: 0.939916	val: 0.835753	test: 0.825495
PRC train: 0.703264	val: 0.435208	test: 0.443624

Epoch: 58
Loss: 0.14661499888818041
ROC train: 0.940810	val: 0.835337	test: 0.831799
PRC train: 0.705242	val: 0.429731	test: 0.443668

Epoch: 59
Loss: 0.14486068569145846
ROC train: 0.941059	val: 0.836632	test: 0.827231
PRC train: 0.701808	val: 0.443100	test: 0.438870

Epoch: 60
Loss: 0.14463294177523398
ROC train: 0.941217	val: 0.841561	test: 0.834142
PRC train: 0.709739	val: 0.439475	test: 0.444894

Epoch: 61
Loss: 0.14593808979586828
ROC train: 0.944411	val: 0.831971	test: 0.827567
PRC train: 0.720393	val: 0.428228	test: 0.435737

Epoch: 62
Loss: 0.14429326900605444
ROC train: 0.941980	val: 0.834433	test: 0.825463
PRC train: 0.707799	val: 0.442622	test: 0.437353

Epoch: 63
Loss: 0.14228108159930616
ROC train: 0.946038	val: 0.837880	test: 0.829471
PRC train: 0.728148	val: 0.438413	test: 0.440099

Epoch: 64
Loss: 0.1434531722709784
ROC train: 0.945806	val: 0.835062	test: 0.830827
PRC train: 0.726711	val: 0.437784	test: 0.440947

Epoch: 65
Loss: 0.14434626971180103
ROC train: 0.947368	val: 0.834529	test: 0.829114
PRC train: 0.728504	val: 0.431486	test: 0.444938

Epoch: 66
Loss: 0.14213653224342576
ROC train: 0.947463	val: 0.834600	test: 0.830132
PRC train: 0.728917	val: 0.432610	test: 0.445049

Epoch: 67
Loss: 0.1400768732799895
ROC train: 0.950936	val: 0.836879	test: 0.826317
PRC train: 0.744479	val: 0.442962	test: 0.447279

Epoch: 68
Loss: 0.1414770994836014
ROC train: 0.951291	val: 0.832320	test: 0.824690
PRC train: 0.747811	val: 0.437136	test: 0.443481

Epoch: 69
Loss: 0.14214665831004958
ROC train: 0.952518	val: 0.834217	test: 0.829128
PRC train: 0.755155	val: 0.444489	test: 0.447883

Epoch: 70
Loss: 0.14041072610015262
ROC train: 0.953946	val: 0.833001	test: 0.825122
PRC train: 0.759575	val: 0.436192	test: 0.435741

Epoch: 71
Loss: 0.1378985834567954
ROC train: 0.953803	val: 0.827796	test: 0.824788
PRC train: 0.759047	val: 0.436661	test: 0.438224

Epoch: 72
Loss: 0.13680468929390108
ROC train: 0.955102	val: 0.835331	test: 0.828829
PRC train: 0.752198	val: 0.431567	test: 0.441845

Epoch: 73
Loss: 0.13530311778560472
ROC train: 0.956533	val: 0.827626	test: 0.819573
PRC train: 0.766205	val: 0.435116	test: 0.432164

Epoch: 74
Loss: 0.13420597657342156
ROC train: 0.956920	val: 0.835251	test: 0.829860
PRC train: 0.767147	val: 0.436905	test: 0.442945

Epoch: 75
Loss: 0.134449561020528
ROC train: 0.957699	val: 0.834371	test: 0.826238
PRC train: 0.769520	val: 0.442715	test: 0.437200

Epoch: 76
Loss: 0.1331046151374017
ROC train: 0.960443	val: 0.832343	test: 0.823638
PRC train: 0.782209	val: 0.439167	test: 0.440467

Epoch: 77
Loss: 0.13492435474537254
ROC train: 0.960286	val: 0.831042	test: 0.820085
PRC train: 0.778741	val: 0.443806	test: 0.450235

Epoch: 78
Loss: 0.13283723184218402
ROC train: 0.960503	val: 0.830661	test: 0.825175
PRC train: 0.784482	val: 0.432633	test: 0.436733

Epoch: 79
Loss: 0.13000877449285375
ROC train: 0.960467	val: 0.831814	test: 0.822039
PRC train: 0.781315	val: 0.432831	test: 0.430895

Epoch: 80
Loss: 0.13062721550566
ROC train: 0.962311	val: 0.832701	test: 0.830780
PRC train: 0.793225	val: 0.439034	test: 0.451702

Epoch: 81
Loss: 0.13028308493910928
ROC train: 0.962650	val: 0.831831	test: 0.822467
PRC train: 0.788350	val: 0.446422	test: 0.438247

Epoch: 82
Loss: 0.1296826795445797
ROC train: 0.963373	val: 0.833832	test: 0.824859
PRC train: 0.793419	val: 0.448383	test: 0.449024

Epoch: 83
Loss: 0.12925172751624545
ROC train: 0.965032	val: 0.831544	test: 0.827028
PRC train: 0.798921	val: 0.445158	test: 0.441781

Epoch: 84
Loss: 0.1281295802636799
ROC train: 0.961125	val: 0.826123	test: 0.815751
PRC train: 0.780937	val: 0.430064	test: 0.420348

Epoch: 85
Loss: 0.12627999640008417
ROC train: 0.966840	val: 0.830355	test: 0.820662
PRC train: 0.807674	val: 0.445953	test: 0.433712

Epoch: 86
Loss: 0.12731612212449697
ROC train: 0.964390	val: 0.831531	test: 0.822161
PRC train: 0.792774	val: 0.444072	test: 0.417767

Epoch: 87
Loss: 0.12482553625761965
ROC train: 0.966555	val: 0.832536	test: 0.823626
PRC train: 0.804988	val: 0.447703	test: 0.435021

Epoch: 88
Loss: 0.12512848366009754
ROC train: 0.968912	val: 0.833180	test: 0.824765
PRC train: 0.819246	val: 0.434702	test: 0.441004

Epoch: 89
Loss: 0.12112438080435323
ROC train: 0.969769	val: 0.829458	test: 0.822878
PRC train: 0.825416	val: 0.443837	test: 0.446851

Epoch: 90
Loss: 0.12548026458450892
ROC train: 0.970611	val: 0.831733	test: 0.827565
PRC train: 0.829201	val: 0.447127	test: 0.440283

Epoch: 91
Loss: 0.12270348070975402
ROC train: 0.968859	val: 0.825563	test: 0.816701
PRC train: 0.815154	val: 0.435327	test: 0.430592

Epoch: 92
Loss: 0.12492037338375338
ROC train: 0.971337	val: 0.830532	test: 0.822859
PRC train: 0.822984	val: 0.435435	test: 0.439911

Epoch: 93
Loss: 0.12456186565883504
ROC train: 0.970505	val: 0.824376	test: 0.816787
PRC train: 0.819858	val: 0.431697	test: 0.418699

Epoch: 94
Loss: 0.12544674774535802
ROC train: 0.906690	val: 0.828747	test: 0.811709
PRC train: 0.601033	val: 0.417577	test: 0.407423

Epoch: 34
Loss: 0.16612698531417122
ROC train: 0.910382	val: 0.825032	test: 0.814710
PRC train: 0.615432	val: 0.414967	test: 0.401750

Epoch: 35
Loss: 0.1639072794513227
ROC train: 0.911108	val: 0.821821	test: 0.810915
PRC train: 0.611657	val: 0.411127	test: 0.411713

Epoch: 36
Loss: 0.1644012282595353
ROC train: 0.910541	val: 0.834326	test: 0.809503
PRC train: 0.615058	val: 0.425493	test: 0.406506

Epoch: 37
Loss: 0.161589571618118
ROC train: 0.912313	val: 0.825008	test: 0.816505
PRC train: 0.616779	val: 0.413921	test: 0.411035

Epoch: 38
Loss: 0.1605518822627504
ROC train: 0.918348	val: 0.828308	test: 0.812376
PRC train: 0.641822	val: 0.415579	test: 0.417954

Epoch: 39
Loss: 0.15963901655816215
ROC train: 0.916921	val: 0.832228	test: 0.819493
PRC train: 0.635418	val: 0.418558	test: 0.413700

Epoch: 40
Loss: 0.16173705073529826
ROC train: 0.919512	val: 0.829416	test: 0.821963
PRC train: 0.641895	val: 0.409112	test: 0.415321

Epoch: 41
Loss: 0.16168070272641435
ROC train: 0.918837	val: 0.828151	test: 0.818209
PRC train: 0.645713	val: 0.422578	test: 0.422811

Epoch: 42
Loss: 0.15962793207558632
ROC train: 0.920136	val: 0.825238	test: 0.813796
PRC train: 0.644195	val: 0.416219	test: 0.408821

Epoch: 43
Loss: 0.15907904687341667
ROC train: 0.920885	val: 0.824005	test: 0.816015
PRC train: 0.649380	val: 0.417675	test: 0.414752

Epoch: 44
Loss: 0.15887278835972546
ROC train: 0.924028	val: 0.831084	test: 0.816429
PRC train: 0.650691	val: 0.422676	test: 0.420164

Epoch: 45
Loss: 0.15677433729395868
ROC train: 0.924416	val: 0.825670	test: 0.821485
PRC train: 0.656907	val: 0.407629	test: 0.422594

Epoch: 46
Loss: 0.15510662370111333
ROC train: 0.928148	val: 0.832598	test: 0.824740
PRC train: 0.668857	val: 0.424870	test: 0.436473

Epoch: 47
Loss: 0.15569040966168726
ROC train: 0.928657	val: 0.833326	test: 0.824572
PRC train: 0.670841	val: 0.423307	test: 0.427778

Epoch: 48
Loss: 0.15520842756894349
ROC train: 0.930208	val: 0.830443	test: 0.826853
PRC train: 0.670250	val: 0.412374	test: 0.436018

Epoch: 49
Loss: 0.1556807246997521
ROC train: 0.930323	val: 0.832099	test: 0.821354
PRC train: 0.672399	val: 0.423599	test: 0.427886

Epoch: 50
Loss: 0.15230551838679215
ROC train: 0.929596	val: 0.836160	test: 0.822177
PRC train: 0.669872	val: 0.423153	test: 0.431885

Epoch: 51
Loss: 0.15019863933118774
ROC train: 0.931880	val: 0.826288	test: 0.818201
PRC train: 0.685500	val: 0.412432	test: 0.420509

Epoch: 52
Loss: 0.15032693566284763
ROC train: 0.933891	val: 0.831687	test: 0.820807
PRC train: 0.686821	val: 0.422410	test: 0.424017

Epoch: 53
Loss: 0.14998247475068002
ROC train: 0.933081	val: 0.828580	test: 0.817094
PRC train: 0.681548	val: 0.411895	test: 0.425287

Epoch: 54
Loss: 0.1497551187089703
ROC train: 0.935244	val: 0.833997	test: 0.819873
PRC train: 0.689164	val: 0.426290	test: 0.432856

Epoch: 55
Loss: 0.1481596477473597
ROC train: 0.937987	val: 0.838041	test: 0.818760
PRC train: 0.704529	val: 0.432267	test: 0.430874

Epoch: 56
Loss: 0.14840317024840785
ROC train: 0.940723	val: 0.832441	test: 0.817141
PRC train: 0.713518	val: 0.431933	test: 0.442738

Epoch: 57
Loss: 0.14677617206447718
ROC train: 0.941297	val: 0.837222	test: 0.816101
PRC train: 0.705917	val: 0.434565	test: 0.426208

Epoch: 58
Loss: 0.14590010802298187
ROC train: 0.938769	val: 0.834099	test: 0.822757
PRC train: 0.701895	val: 0.418785	test: 0.443101

Epoch: 59
Loss: 0.14630223665564263
ROC train: 0.943014	val: 0.835076	test: 0.822562
PRC train: 0.718307	val: 0.420238	test: 0.435111

Epoch: 60
Loss: 0.1455719330469624
ROC train: 0.943883	val: 0.829908	test: 0.817841
PRC train: 0.721076	val: 0.420766	test: 0.425546

Epoch: 61
Loss: 0.14863736246507822
ROC train: 0.942979	val: 0.832272	test: 0.820126
PRC train: 0.712138	val: 0.431446	test: 0.436089

Epoch: 62
Loss: 0.14597478186581317
ROC train: 0.941286	val: 0.834617	test: 0.822809
PRC train: 0.709184	val: 0.412284	test: 0.444822

Epoch: 63
Loss: 0.1459011324401858
ROC train: 0.948008	val: 0.831517	test: 0.818066
PRC train: 0.732231	val: 0.420655	test: 0.423346

Epoch: 64
Loss: 0.14367693036676962
ROC train: 0.947306	val: 0.830280	test: 0.816753
PRC train: 0.733657	val: 0.431963	test: 0.432363

Epoch: 65
Loss: 0.14164715858196936
ROC train: 0.948176	val: 0.832665	test: 0.815665
PRC train: 0.735328	val: 0.437683	test: 0.430955

Epoch: 66
Loss: 0.14036080382677896
ROC train: 0.949999	val: 0.834313	test: 0.823514
PRC train: 0.742451	val: 0.432008	test: 0.442390

Epoch: 67
Loss: 0.13709217914248747
ROC train: 0.950496	val: 0.825979	test: 0.817311
PRC train: 0.733947	val: 0.424900	test: 0.424797

Epoch: 68
Loss: 0.13916280982733284
ROC train: 0.951001	val: 0.835218	test: 0.820708
PRC train: 0.744592	val: 0.439196	test: 0.430752

Epoch: 69
Loss: 0.1386099602966572
ROC train: 0.951761	val: 0.830629	test: 0.820778
PRC train: 0.746427	val: 0.421951	test: 0.417340

Epoch: 70
Loss: 0.13599437490353514
ROC train: 0.953035	val: 0.830881	test: 0.823924
PRC train: 0.756125	val: 0.423023	test: 0.441746

Epoch: 71
Loss: 0.13638758716751137
ROC train: 0.954581	val: 0.837750	test: 0.825750
PRC train: 0.756505	val: 0.438606	test: 0.434816

Epoch: 72
Loss: 0.13607376571327207
ROC train: 0.954627	val: 0.832750	test: 0.824674
PRC train: 0.762043	val: 0.427819	test: 0.428962

Epoch: 73
Loss: 0.13654416315982867
ROC train: 0.956764	val: 0.830765	test: 0.817716
PRC train: 0.761914	val: 0.427661	test: 0.429005

Epoch: 74
Loss: 0.1360030539275789
ROC train: 0.956108	val: 0.830011	test: 0.824549
PRC train: 0.755137	val: 0.418295	test: 0.434116

Epoch: 75
Loss: 0.1332637542392749
ROC train: 0.957549	val: 0.833766	test: 0.820555
PRC train: 0.767572	val: 0.434541	test: 0.420902

Epoch: 76
Loss: 0.13559484698662613
ROC train: 0.958256	val: 0.827829	test: 0.816907
PRC train: 0.775255	val: 0.412866	test: 0.437005

Epoch: 77
Loss: 0.13425826931039148
ROC train: 0.956562	val: 0.831210	test: 0.816930
PRC train: 0.765091	val: 0.410112	test: 0.406862

Epoch: 78
Loss: 0.1314329398827554
ROC train: 0.960022	val: 0.831169	test: 0.823036
PRC train: 0.778052	val: 0.432370	test: 0.426627

Epoch: 79
Loss: 0.13223712244413277
ROC train: 0.958068	val: 0.836030	test: 0.820074
PRC train: 0.765755	val: 0.444843	test: 0.434829

Epoch: 80
Loss: 0.13189631346569533
ROC train: 0.963194	val: 0.831419	test: 0.818111
PRC train: 0.791564	val: 0.431188	test: 0.432468

Epoch: 81
Loss: 0.1287250579851198
ROC train: 0.962887	val: 0.829838	test: 0.821792
PRC train: 0.792543	val: 0.427436	test: 0.435057

Epoch: 82
Loss: 0.127336851181862
ROC train: 0.962842	val: 0.835206	test: 0.822545
PRC train: 0.790242	val: 0.433914	test: 0.431802

Epoch: 83
Loss: 0.12725408451878853
ROC train: 0.963910	val: 0.827890	test: 0.817098
PRC train: 0.793551	val: 0.436076	test: 0.433502

Epoch: 84
Loss: 0.1273253342106903
ROC train: 0.965106	val: 0.831875	test: 0.819977
PRC train: 0.799975	val: 0.422261	test: 0.430411

Epoch: 85
Loss: 0.12709478038282274
ROC train: 0.965443	val: 0.828865	test: 0.814645
PRC train: 0.797908	val: 0.431042	test: 0.429898

Epoch: 86
Loss: 0.12647898528250806
ROC train: 0.965199	val: 0.831076	test: 0.815691
PRC train: 0.798587	val: 0.431725	test: 0.426373

Epoch: 87
Loss: 0.12493003595526657
ROC train: 0.965961	val: 0.830733	test: 0.815809
PRC train: 0.798377	val: 0.433790	test: 0.423234

Epoch: 88
Loss: 0.1266492179888561
ROC train: 0.969884	val: 0.825754	test: 0.812639
PRC train: 0.822580	val: 0.428952	test: 0.430053

Epoch: 89
Loss: 0.12605454657629503
ROC train: 0.966824	val: 0.833588	test: 0.821117
PRC train: 0.804521	val: 0.430761	test: 0.434854

Epoch: 90
Loss: 0.12462114491903702
ROC train: 0.968501	val: 0.835853	test: 0.821654
PRC train: 0.816836	val: 0.437762	test: 0.434268

Epoch: 91
Loss: 0.1217695321051002
ROC train: 0.971024	val: 0.830418	test: 0.821926
PRC train: 0.823592	val: 0.436334	test: 0.430089

Epoch: 92
Loss: 0.1233703934679375
ROC train: 0.972003	val: 0.832294	test: 0.817878
PRC train: 0.832035	val: 0.439348	test: 0.439521

Epoch: 93
Loss: 0.12129992816422508
ROC train: 0.972100	val: 0.833403	test: 0.819852
PRC train: 0.829553	val: 0.429759	test: 0.438570

Epoch: 94
Loss: 0.12348709454880072
ROC train: 0.906063	val: 0.831598	test: 0.827004
PRC train: 0.593452	val: 0.415991	test: 0.421180

Epoch: 34
Loss: 0.16286525513612388
ROC train: 0.905624	val: 0.830615	test: 0.818950
PRC train: 0.595130	val: 0.419681	test: 0.419378

Epoch: 35
Loss: 0.16351397293552614
ROC train: 0.908739	val: 0.828552	test: 0.828307
PRC train: 0.607478	val: 0.413590	test: 0.438227

Epoch: 36
Loss: 0.16245663358048287
ROC train: 0.911722	val: 0.833889	test: 0.831082
PRC train: 0.610191	val: 0.420740	test: 0.430460

Epoch: 37
Loss: 0.16278160438791783
ROC train: 0.913102	val: 0.835183	test: 0.826953
PRC train: 0.618752	val: 0.433175	test: 0.436867

Epoch: 38
Loss: 0.16114202246426063
ROC train: 0.913438	val: 0.830978	test: 0.826578
PRC train: 0.622512	val: 0.425418	test: 0.445959

Epoch: 39
Loss: 0.15916746096237464
ROC train: 0.916667	val: 0.831343	test: 0.829023
PRC train: 0.628507	val: 0.433505	test: 0.442690

Epoch: 40
Loss: 0.1571071133188828
ROC train: 0.915881	val: 0.831603	test: 0.830159
PRC train: 0.628993	val: 0.425641	test: 0.438440

Epoch: 41
Loss: 0.15846230500361222
ROC train: 0.919536	val: 0.831174	test: 0.830005
PRC train: 0.635722	val: 0.436576	test: 0.443401

Epoch: 42
Loss: 0.15657650695245895
ROC train: 0.921697	val: 0.838865	test: 0.826971
PRC train: 0.640330	val: 0.438391	test: 0.437524

Epoch: 43
Loss: 0.1556900278721027
ROC train: 0.922491	val: 0.836222	test: 0.825962
PRC train: 0.642481	val: 0.429637	test: 0.432488

Epoch: 44
Loss: 0.15323189064760892
ROC train: 0.921833	val: 0.841657	test: 0.828768
PRC train: 0.648589	val: 0.440620	test: 0.443014

Epoch: 45
Loss: 0.15823105408467997
ROC train: 0.924353	val: 0.835555	test: 0.827363
PRC train: 0.654564	val: 0.443516	test: 0.440932

Epoch: 46
Loss: 0.15351493560856613
ROC train: 0.924006	val: 0.840183	test: 0.829818
PRC train: 0.648838	val: 0.422372	test: 0.455520

Epoch: 47
Loss: 0.1534621052470739
ROC train: 0.927699	val: 0.835071	test: 0.826747
PRC train: 0.663251	val: 0.433891	test: 0.437641

Epoch: 48
Loss: 0.15185378826026638
ROC train: 0.928658	val: 0.831575	test: 0.825553
PRC train: 0.664610	val: 0.435110	test: 0.448384

Epoch: 49
Loss: 0.1529680141664531
ROC train: 0.928023	val: 0.828044	test: 0.825822
PRC train: 0.662962	val: 0.423907	test: 0.444766

Epoch: 50
Loss: 0.15041456817988752
ROC train: 0.929657	val: 0.833988	test: 0.829020
PRC train: 0.667500	val: 0.432339	test: 0.443748

Epoch: 51
Loss: 0.1508589312492488
ROC train: 0.932111	val: 0.837443	test: 0.830346
PRC train: 0.674568	val: 0.425798	test: 0.434706

Epoch: 52
Loss: 0.15088459413733357
ROC train: 0.933788	val: 0.839616	test: 0.830844
PRC train: 0.688202	val: 0.431565	test: 0.454413

Epoch: 53
Loss: 0.14940650034597008
ROC train: 0.936384	val: 0.842001	test: 0.832385
PRC train: 0.693558	val: 0.431108	test: 0.459400

Epoch: 54
Loss: 0.14599662646656741
ROC train: 0.936791	val: 0.844360	test: 0.832550
PRC train: 0.694275	val: 0.439162	test: 0.461823

Epoch: 55
Loss: 0.14769784933506452
ROC train: 0.939652	val: 0.842711	test: 0.833367
PRC train: 0.700815	val: 0.437476	test: 0.455535

Epoch: 56
Loss: 0.14492190047448006
ROC train: 0.939415	val: 0.831047	test: 0.823183
PRC train: 0.697510	val: 0.422838	test: 0.440806

Epoch: 57
Loss: 0.1466788748770387
ROC train: 0.935252	val: 0.837338	test: 0.820118
PRC train: 0.682583	val: 0.450826	test: 0.427172

Epoch: 58
Loss: 0.14512921637651582
ROC train: 0.941303	val: 0.836053	test: 0.824937
PRC train: 0.708990	val: 0.434563	test: 0.447982

Epoch: 59
Loss: 0.1444779742999203
ROC train: 0.941133	val: 0.836501	test: 0.822422
PRC train: 0.699028	val: 0.440122	test: 0.436398

Epoch: 60
Loss: 0.14283951234262093
ROC train: 0.943066	val: 0.838835	test: 0.830187
PRC train: 0.711976	val: 0.444332	test: 0.456659

Epoch: 61
Loss: 0.14371475533967298
ROC train: 0.945351	val: 0.843375	test: 0.833048
PRC train: 0.724366	val: 0.447649	test: 0.451728

Epoch: 62
Loss: 0.14090204113519791
ROC train: 0.943566	val: 0.833152	test: 0.825255
PRC train: 0.712136	val: 0.434330	test: 0.441168

Epoch: 63
Loss: 0.1410513422419176
ROC train: 0.945279	val: 0.843779	test: 0.826406
PRC train: 0.721022	val: 0.437694	test: 0.446999

Epoch: 64
Loss: 0.1424786812267986
ROC train: 0.946054	val: 0.835038	test: 0.831088
PRC train: 0.723560	val: 0.431898	test: 0.436729

Epoch: 65
Loss: 0.1398863978551056
ROC train: 0.949168	val: 0.845274	test: 0.833164
PRC train: 0.733628	val: 0.469327	test: 0.447544

Epoch: 66
Loss: 0.13927660209966486
ROC train: 0.944684	val: 0.834707	test: 0.828838
PRC train: 0.716977	val: 0.428648	test: 0.449297

Epoch: 67
Loss: 0.13962827840097972
ROC train: 0.951627	val: 0.842104	test: 0.828438
PRC train: 0.748055	val: 0.456788	test: 0.456269

Epoch: 68
Loss: 0.13655983291854246
ROC train: 0.952669	val: 0.832985	test: 0.824908
PRC train: 0.748619	val: 0.445543	test: 0.449404

Epoch: 69
Loss: 0.13662433012195016
ROC train: 0.951324	val: 0.842349	test: 0.824874
PRC train: 0.742166	val: 0.445690	test: 0.443878

Epoch: 70
Loss: 0.1363274378358444
ROC train: 0.950619	val: 0.832608	test: 0.822430
PRC train: 0.738554	val: 0.443725	test: 0.452165

Epoch: 71
Loss: 0.13629756553405015
ROC train: 0.954220	val: 0.832866	test: 0.825931
PRC train: 0.757054	val: 0.442618	test: 0.450875

Epoch: 72
Loss: 0.1330581805201134
ROC train: 0.955497	val: 0.834517	test: 0.827351
PRC train: 0.765094	val: 0.450833	test: 0.453594

Epoch: 73
Loss: 0.13479639222010348
ROC train: 0.957333	val: 0.833528	test: 0.826417
PRC train: 0.762485	val: 0.450612	test: 0.449962

Epoch: 74
Loss: 0.13215535249998048
ROC train: 0.956187	val: 0.835146	test: 0.826413
PRC train: 0.763696	val: 0.443099	test: 0.454458

Epoch: 75
Loss: 0.13295755923655572
ROC train: 0.957896	val: 0.831765	test: 0.821683
PRC train: 0.767642	val: 0.450630	test: 0.444675

Epoch: 76
Loss: 0.13214369196761325
ROC train: 0.956591	val: 0.831210	test: 0.822068
PRC train: 0.760973	val: 0.438753	test: 0.439542

Epoch: 77
Loss: 0.13003955735456418
ROC train: 0.960180	val: 0.830611	test: 0.827443
PRC train: 0.777051	val: 0.448431	test: 0.459882

Epoch: 78
Loss: 0.129308000920426
ROC train: 0.960750	val: 0.838350	test: 0.826816
PRC train: 0.778894	val: 0.446384	test: 0.455715

Epoch: 79
Loss: 0.13018025409101286
ROC train: 0.961534	val: 0.832325	test: 0.822527
PRC train: 0.787295	val: 0.457677	test: 0.459941

Epoch: 80
Loss: 0.1300683703520393
ROC train: 0.962597	val: 0.823899	test: 0.825522
PRC train: 0.790626	val: 0.438010	test: 0.456348

Epoch: 81
Loss: 0.12797498963937004
ROC train: 0.962431	val: 0.828769	test: 0.819623
PRC train: 0.784626	val: 0.434715	test: 0.456709

Epoch: 82
Loss: 0.12685113645844814
ROC train: 0.963811	val: 0.833825	test: 0.827039
PRC train: 0.792413	val: 0.438246	test: 0.464554

Epoch: 83
Loss: 0.12606306074968213
ROC train: 0.964118	val: 0.831076	test: 0.819395
PRC train: 0.791979	val: 0.437131	test: 0.448978

Epoch: 84
Loss: 0.1271579328149415
ROC train: 0.962125	val: 0.837379	test: 0.824009
PRC train: 0.780206	val: 0.446522	test: 0.444886

Epoch: 85
Loss: 0.12880267401069723
ROC train: 0.965630	val: 0.831938	test: 0.818294
PRC train: 0.801643	val: 0.452440	test: 0.453984

Epoch: 86
Loss: 0.1249027765620144
ROC train: 0.964946	val: 0.831462	test: 0.824746
PRC train: 0.797732	val: 0.448277	test: 0.455568

Epoch: 87
Loss: 0.12425226896922399
ROC train: 0.967007	val: 0.828407	test: 0.818611
PRC train: 0.803372	val: 0.441888	test: 0.450768

Epoch: 88
Loss: 0.1242078812831664
ROC train: 0.968872	val: 0.831768	test: 0.819852
PRC train: 0.815877	val: 0.441999	test: 0.460142

Epoch: 89
Loss: 0.1207248062896788
ROC train: 0.968564	val: 0.827043	test: 0.821930
PRC train: 0.816855	val: 0.441383	test: 0.461535

Epoch: 90
Loss: 0.12263879827584473
ROC train: 0.968562	val: 0.829319	test: 0.819289
PRC train: 0.813617	val: 0.446753	test: 0.459004

Epoch: 91
Loss: 0.12231551102275681
ROC train: 0.969773	val: 0.826944	test: 0.820306
PRC train: 0.823822	val: 0.439039	test: 0.453706

Epoch: 92
Loss: 0.12042732231371488
ROC train: 0.970598	val: 0.829572	test: 0.823205
PRC train: 0.822004	val: 0.445629	test: 0.465177

Epoch: 93
Loss: 0.12033338032980528
ROC train: 0.968900	val: 0.825917	test: 0.820156
PRC train: 0.816126	val: 0.444256	test: 0.451341

Epoch: 94
Loss: 0.11905456668197055
ROC train: 0.903795	val: 0.833658	test: 0.827109
PRC train: 0.582108	val: 0.420740	test: 0.416280

Epoch: 34
Loss: 0.16624632430905203
ROC train: 0.902189	val: 0.830271	test: 0.823494
PRC train: 0.574457	val: 0.414271	test: 0.419216

Epoch: 35
Loss: 0.16501529883287666
ROC train: 0.906711	val: 0.829616	test: 0.825670
PRC train: 0.595943	val: 0.424017	test: 0.429992

Epoch: 36
Loss: 0.16576555751291966
ROC train: 0.908479	val: 0.829388	test: 0.828195
PRC train: 0.594269	val: 0.412711	test: 0.426016

Epoch: 37
Loss: 0.1637449127225995
ROC train: 0.910582	val: 0.832206	test: 0.829260
PRC train: 0.606400	val: 0.441958	test: 0.425864

Epoch: 38
Loss: 0.16286038229918562
ROC train: 0.912780	val: 0.833844	test: 0.822199
PRC train: 0.610774	val: 0.429637	test: 0.431954

Epoch: 39
Loss: 0.16253294965875023
ROC train: 0.910137	val: 0.826355	test: 0.830677
PRC train: 0.606185	val: 0.424523	test: 0.432994

Epoch: 40
Loss: 0.16049407836150187
ROC train: 0.915315	val: 0.833113	test: 0.826484
PRC train: 0.623003	val: 0.436006	test: 0.432638

Epoch: 41
Loss: 0.1601305216782987
ROC train: 0.914212	val: 0.830716	test: 0.824394
PRC train: 0.621047	val: 0.430032	test: 0.433676

Epoch: 42
Loss: 0.15887499324249477
ROC train: 0.915248	val: 0.833768	test: 0.823276
PRC train: 0.619864	val: 0.432521	test: 0.433866

Epoch: 43
Loss: 0.158328477186776
ROC train: 0.916977	val: 0.829801	test: 0.827527
PRC train: 0.622011	val: 0.438637	test: 0.427163

Epoch: 44
Loss: 0.15840603627865948
ROC train: 0.919983	val: 0.840697	test: 0.826935
PRC train: 0.631641	val: 0.426532	test: 0.424671

Epoch: 45
Loss: 0.15683482941719673
ROC train: 0.918927	val: 0.841329	test: 0.824626
PRC train: 0.636638	val: 0.433085	test: 0.426154

Epoch: 46
Loss: 0.15712376802921282
ROC train: 0.924545	val: 0.831029	test: 0.825959
PRC train: 0.652284	val: 0.439780	test: 0.441645

Epoch: 47
Loss: 0.15593050970750655
ROC train: 0.925046	val: 0.837485	test: 0.830995
PRC train: 0.650282	val: 0.431740	test: 0.432438

Epoch: 48
Loss: 0.15309826751316927
ROC train: 0.927146	val: 0.836502	test: 0.827151
PRC train: 0.655787	val: 0.446426	test: 0.445928

Epoch: 49
Loss: 0.15414153306744907
ROC train: 0.925737	val: 0.836588	test: 0.825562
PRC train: 0.649330	val: 0.429955	test: 0.431396

Epoch: 50
Loss: 0.15437419326781543
ROC train: 0.922775	val: 0.827397	test: 0.827749
PRC train: 0.647381	val: 0.430189	test: 0.445360

Epoch: 51
Loss: 0.152153967841834
ROC train: 0.929404	val: 0.834249	test: 0.818664
PRC train: 0.663344	val: 0.438256	test: 0.427150

Epoch: 52
Loss: 0.1500870609436236
ROC train: 0.931568	val: 0.835299	test: 0.830535
PRC train: 0.674022	val: 0.447701	test: 0.452731

Epoch: 53
Loss: 0.14997733411583578
ROC train: 0.932803	val: 0.830978	test: 0.825322
PRC train: 0.677819	val: 0.440492	test: 0.444204

Epoch: 54
Loss: 0.1496005940489823
ROC train: 0.934179	val: 0.839983	test: 0.829569
PRC train: 0.683830	val: 0.439377	test: 0.448682

Epoch: 55
Loss: 0.14761208128789677
ROC train: 0.934120	val: 0.837992	test: 0.824064
PRC train: 0.672605	val: 0.438989	test: 0.450199

Epoch: 56
Loss: 0.1486698465206445
ROC train: 0.936556	val: 0.839921	test: 0.834948
PRC train: 0.690877	val: 0.445702	test: 0.457591

Epoch: 57
Loss: 0.14715160553544246
ROC train: 0.935031	val: 0.838514	test: 0.828225
PRC train: 0.679623	val: 0.442070	test: 0.434774

Epoch: 58
Loss: 0.14752060037322787
ROC train: 0.937649	val: 0.839789	test: 0.828412
PRC train: 0.690788	val: 0.435248	test: 0.448726

Epoch: 59
Loss: 0.14600783256637337
ROC train: 0.938174	val: 0.830764	test: 0.833778
PRC train: 0.694239	val: 0.434008	test: 0.449134

Epoch: 60
Loss: 0.14389545299977416
ROC train: 0.941808	val: 0.833434	test: 0.822375
PRC train: 0.710759	val: 0.442028	test: 0.439428

Epoch: 61
Loss: 0.14443508952993142
ROC train: 0.942232	val: 0.837550	test: 0.831114
PRC train: 0.711275	val: 0.452015	test: 0.449615

Epoch: 62
Loss: 0.1436473675352944
ROC train: 0.942171	val: 0.836096	test: 0.824558
PRC train: 0.709386	val: 0.449465	test: 0.444603

Epoch: 63
Loss: 0.14713827751620617
ROC train: 0.942848	val: 0.832484	test: 0.821315
PRC train: 0.707535	val: 0.445489	test: 0.440098

Epoch: 64
Loss: 0.14424550481861617
ROC train: 0.944151	val: 0.829737	test: 0.823316
PRC train: 0.720134	val: 0.443340	test: 0.434824

Epoch: 65
Loss: 0.14331921092952193
ROC train: 0.944896	val: 0.839630	test: 0.826320
PRC train: 0.716152	val: 0.448874	test: 0.451663

Epoch: 66
Loss: 0.1424267714483115
ROC train: 0.945200	val: 0.831322	test: 0.827601
PRC train: 0.717422	val: 0.441330	test: 0.445342

Epoch: 67
Loss: 0.14103377288455607
ROC train: 0.940679	val: 0.838641	test: 0.817339
PRC train: 0.695490	val: 0.448295	test: 0.437864

Epoch: 68
Loss: 0.1415044767273168
ROC train: 0.948871	val: 0.827549	test: 0.823047
PRC train: 0.737940	val: 0.448410	test: 0.453167

Epoch: 69
Loss: 0.13893646555025715
ROC train: 0.950591	val: 0.834046	test: 0.828313
PRC train: 0.746408	val: 0.446695	test: 0.452330

Epoch: 70
Loss: 0.13771076549001726
ROC train: 0.949223	val: 0.828395	test: 0.822789
PRC train: 0.739102	val: 0.453025	test: 0.441241

Epoch: 71
Loss: 0.13649479599632067
ROC train: 0.950422	val: 0.832804	test: 0.828374
PRC train: 0.742855	val: 0.444822	test: 0.442245

Epoch: 72
Loss: 0.1361140797434666
ROC train: 0.953211	val: 0.839490	test: 0.824745
PRC train: 0.753703	val: 0.469204	test: 0.444637

Epoch: 73
Loss: 0.13404029256406777
ROC train: 0.954640	val: 0.838444	test: 0.824720
PRC train: 0.759658	val: 0.461578	test: 0.445691

Epoch: 74
Loss: 0.13615092229450634
ROC train: 0.954940	val: 0.837050	test: 0.828757
PRC train: 0.760411	val: 0.445190	test: 0.449387

Epoch: 75
Loss: 0.1338911351383331
ROC train: 0.950600	val: 0.839121	test: 0.822344
PRC train: 0.733808	val: 0.453954	test: 0.446258

Epoch: 76
Loss: 0.1351656022384954
ROC train: 0.949615	val: 0.825037	test: 0.822442
PRC train: 0.728602	val: 0.422182	test: 0.417687

Epoch: 77
Loss: 0.13370472445299184
ROC train: 0.956521	val: 0.841432	test: 0.828669
PRC train: 0.764193	val: 0.454427	test: 0.454869

Epoch: 78
Loss: 0.13260425085294866
ROC train: 0.959425	val: 0.837722	test: 0.827437
PRC train: 0.777061	val: 0.451214	test: 0.455829

Epoch: 79
Loss: 0.13198655574559237
ROC train: 0.956505	val: 0.838623	test: 0.824305
PRC train: 0.759742	val: 0.461112	test: 0.458388

Epoch: 80
Loss: 0.13157593901258943
ROC train: 0.957718	val: 0.831863	test: 0.826464
PRC train: 0.763439	val: 0.432353	test: 0.445566

Epoch: 81
Loss: 0.13053720425118562
ROC train: 0.954928	val: 0.832337	test: 0.819921
PRC train: 0.748234	val: 0.449832	test: 0.442966

Epoch: 82
Loss: 0.12991210500724346
ROC train: 0.961915	val: 0.836353	test: 0.828055
PRC train: 0.787638	val: 0.461535	test: 0.438133

Epoch: 83
Loss: 0.12913204696680558
ROC train: 0.961815	val: 0.829806	test: 0.827501
PRC train: 0.782056	val: 0.445369	test: 0.451524

Epoch: 84
Loss: 0.12906646349871254
ROC train: 0.957257	val: 0.836787	test: 0.823759
PRC train: 0.760677	val: 0.441990	test: 0.451701

Epoch: 85
Loss: 0.13077628295521684
ROC train: 0.959088	val: 0.832208	test: 0.823687
PRC train: 0.781805	val: 0.439656	test: 0.442697

Epoch: 86
Loss: 0.12949790916907958
ROC train: 0.964680	val: 0.831402	test: 0.819202
PRC train: 0.803157	val: 0.454080	test: 0.444529

Epoch: 87
Loss: 0.12615531604069563
ROC train: 0.964683	val: 0.836689	test: 0.826983
PRC train: 0.802107	val: 0.448402	test: 0.454141

Epoch: 88
Loss: 0.12506156053562428
ROC train: 0.965215	val: 0.835222	test: 0.823794
PRC train: 0.801715	val: 0.464736	test: 0.453335

Epoch: 89
Loss: 0.12562663494019147
ROC train: 0.965205	val: 0.833044	test: 0.815197
PRC train: 0.798484	val: 0.455076	test: 0.456486

Epoch: 90
Loss: 0.12598309573061697
ROC train: 0.967881	val: 0.831744	test: 0.819972
PRC train: 0.814708	val: 0.450412	test: 0.454220

Epoch: 91
Loss: 0.12364488030291793
ROC train: 0.968163	val: 0.834203	test: 0.820626
PRC train: 0.814186	val: 0.461486	test: 0.451746

Epoch: 92
Loss: 0.12560758811130301
ROC train: 0.968475	val: 0.836736	test: 0.819258
PRC train: 0.813874	val: 0.447989	test: 0.452150

Epoch: 93
Loss: 0.12341823518648525
ROC train: 0.970525	val: 0.830182	test: 0.816112
PRC train: 0.820355	val: 0.470164	test: 0.443755

Epoch: 94
Loss: 0.1216488871199525
ROC train: 0.909560	val: 0.830081	test: 0.826116
PRC train: 0.607127	val: 0.438332	test: 0.424170

Epoch: 34
Loss: 0.16430810149811836
ROC train: 0.906843	val: 0.822077	test: 0.826662
PRC train: 0.605331	val: 0.424536	test: 0.414145

Epoch: 35
Loss: 0.16444332438014908
ROC train: 0.910422	val: 0.827359	test: 0.825303
PRC train: 0.605640	val: 0.430821	test: 0.406959

Epoch: 36
Loss: 0.16193022633719226
ROC train: 0.912968	val: 0.827340	test: 0.825638
PRC train: 0.617677	val: 0.431640	test: 0.424484

Epoch: 37
Loss: 0.16014934377002024
ROC train: 0.911283	val: 0.837785	test: 0.825505
PRC train: 0.609615	val: 0.438628	test: 0.421795

Epoch: 38
Loss: 0.1596514810758763
ROC train: 0.917527	val: 0.830786	test: 0.832863
PRC train: 0.630726	val: 0.432322	test: 0.433914

Epoch: 39
Loss: 0.1617906119039427
ROC train: 0.914925	val: 0.834316	test: 0.821127
PRC train: 0.614951	val: 0.426741	test: 0.400768

Epoch: 40
Loss: 0.15872824502039645
ROC train: 0.920041	val: 0.834606	test: 0.831704
PRC train: 0.636249	val: 0.441292	test: 0.430642

Epoch: 41
Loss: 0.15563981635628993
ROC train: 0.920810	val: 0.831771	test: 0.832740
PRC train: 0.640072	val: 0.434443	test: 0.420060

Epoch: 42
Loss: 0.15660356273623005
ROC train: 0.922142	val: 0.838847	test: 0.827739
PRC train: 0.647076	val: 0.430717	test: 0.439635

Epoch: 43
Loss: 0.15507677791513502
ROC train: 0.923838	val: 0.838024	test: 0.834719
PRC train: 0.650163	val: 0.436333	test: 0.438594

Epoch: 44
Loss: 0.15629177184711868
ROC train: 0.925064	val: 0.823807	test: 0.829078
PRC train: 0.656262	val: 0.441695	test: 0.436198

Epoch: 45
Loss: 0.15437156139781513
ROC train: 0.923685	val: 0.831051	test: 0.831339
PRC train: 0.637367	val: 0.430313	test: 0.434719

Epoch: 46
Loss: 0.1547002441186838
ROC train: 0.923022	val: 0.834388	test: 0.828668
PRC train: 0.640808	val: 0.427687	test: 0.448816

Epoch: 47
Loss: 0.1560173902273703
ROC train: 0.928120	val: 0.831306	test: 0.831309
PRC train: 0.671460	val: 0.448776	test: 0.461048

Epoch: 48
Loss: 0.15377501052332063
ROC train: 0.930006	val: 0.833090	test: 0.833009
PRC train: 0.670075	val: 0.445331	test: 0.451350

Epoch: 49
Loss: 0.15342037572721606
ROC train: 0.930256	val: 0.832448	test: 0.825529
PRC train: 0.663890	val: 0.452909	test: 0.436163

Epoch: 50
Loss: 0.15063639981740118
ROC train: 0.931708	val: 0.836267	test: 0.829959
PRC train: 0.676209	val: 0.449634	test: 0.436957

Epoch: 51
Loss: 0.1512086551535411
ROC train: 0.932370	val: 0.838637	test: 0.832128
PRC train: 0.676411	val: 0.447970	test: 0.457677

Epoch: 52
Loss: 0.1495525846520414
ROC train: 0.934714	val: 0.841893	test: 0.825653
PRC train: 0.683111	val: 0.458334	test: 0.431104

Epoch: 53
Loss: 0.14814135030410874
ROC train: 0.935657	val: 0.836261	test: 0.837052
PRC train: 0.691833	val: 0.447444	test: 0.430005

Epoch: 54
Loss: 0.1443607280587442
ROC train: 0.938343	val: 0.834542	test: 0.835373
PRC train: 0.700064	val: 0.452101	test: 0.448566

Epoch: 55
Loss: 0.1467596969390623
ROC train: 0.938749	val: 0.835078	test: 0.835590
PRC train: 0.696356	val: 0.438968	test: 0.450528

Epoch: 56
Loss: 0.14486699847999762
ROC train: 0.936616	val: 0.832697	test: 0.829232
PRC train: 0.682844	val: 0.441446	test: 0.453838

Epoch: 57
Loss: 0.14466142025075351
ROC train: 0.940292	val: 0.830346	test: 0.828694
PRC train: 0.704586	val: 0.449328	test: 0.433244

Epoch: 58
Loss: 0.14740088729929723
ROC train: 0.938336	val: 0.822122	test: 0.827386
PRC train: 0.699519	val: 0.424297	test: 0.427269

Epoch: 59
Loss: 0.14414192441547316
ROC train: 0.943033	val: 0.836190	test: 0.829922
PRC train: 0.710515	val: 0.453199	test: 0.452902

Epoch: 60
Loss: 0.14312883324520587
ROC train: 0.945674	val: 0.839926	test: 0.830911
PRC train: 0.716395	val: 0.451175	test: 0.420851

Epoch: 61
Loss: 0.14376803043715677
ROC train: 0.940379	val: 0.825944	test: 0.827827
PRC train: 0.702624	val: 0.422511	test: 0.427252

Epoch: 62
Loss: 0.14081607860266313
ROC train: 0.946768	val: 0.832329	test: 0.833881
PRC train: 0.724669	val: 0.435836	test: 0.458143

Epoch: 63
Loss: 0.13983979912753328
ROC train: 0.948117	val: 0.835618	test: 0.833579
PRC train: 0.727632	val: 0.457120	test: 0.451795

Epoch: 64
Loss: 0.1409883935337591
ROC train: 0.943622	val: 0.831815	test: 0.829518
PRC train: 0.713536	val: 0.442191	test: 0.454910

Epoch: 65
Loss: 0.14243922444983476
ROC train: 0.947952	val: 0.830883	test: 0.830593
PRC train: 0.732751	val: 0.432244	test: 0.438632

Epoch: 66
Loss: 0.13933610573250868
ROC train: 0.947715	val: 0.838227	test: 0.832682
PRC train: 0.720001	val: 0.431881	test: 0.437669

Epoch: 67
Loss: 0.13941177725154136
ROC train: 0.949152	val: 0.836933	test: 0.825177
PRC train: 0.730651	val: 0.441715	test: 0.443242

Epoch: 68
Loss: 0.1381253174879102
ROC train: 0.954354	val: 0.843228	test: 0.825710
PRC train: 0.754898	val: 0.453043	test: 0.443825

Epoch: 69
Loss: 0.13767308374728157
ROC train: 0.954050	val: 0.831287	test: 0.831573
PRC train: 0.755628	val: 0.445767	test: 0.453993

Epoch: 70
Loss: 0.1373856887839714
ROC train: 0.954912	val: 0.833933	test: 0.824787
PRC train: 0.752119	val: 0.436793	test: 0.425570

Epoch: 71
Loss: 0.13534235997604754
ROC train: 0.954689	val: 0.837297	test: 0.828778
PRC train: 0.752372	val: 0.458696	test: 0.449567

Epoch: 72
Loss: 0.13382868773615877
ROC train: 0.957671	val: 0.836916	test: 0.827387
PRC train: 0.763356	val: 0.453720	test: 0.457543

Epoch: 73
Loss: 0.13539658919489989
ROC train: 0.959205	val: 0.835089	test: 0.829870
PRC train: 0.771299	val: 0.440312	test: 0.454103

Epoch: 74
Loss: 0.1345656890628987
ROC train: 0.957840	val: 0.830079	test: 0.829233
PRC train: 0.766831	val: 0.435785	test: 0.453678

Epoch: 75
Loss: 0.13301978168300368
ROC train: 0.959496	val: 0.833336	test: 0.830936
PRC train: 0.774006	val: 0.443029	test: 0.452398

Epoch: 76
Loss: 0.13304431089146906
ROC train: 0.961004	val: 0.829388	test: 0.825783
PRC train: 0.779364	val: 0.434849	test: 0.455819

Epoch: 77
Loss: 0.13036571693073007
ROC train: 0.961045	val: 0.837092	test: 0.827832
PRC train: 0.777075	val: 0.447096	test: 0.454935

Epoch: 78
Loss: 0.1296837941587212
ROC train: 0.960423	val: 0.827505	test: 0.833538
PRC train: 0.781552	val: 0.431270	test: 0.450926

Epoch: 79
Loss: 0.12875811235065948
ROC train: 0.962393	val: 0.832613	test: 0.827445
PRC train: 0.785213	val: 0.452141	test: 0.452520

Epoch: 80
Loss: 0.13007524565577466
ROC train: 0.963574	val: 0.831113	test: 0.832596
PRC train: 0.789789	val: 0.442626	test: 0.468461

Epoch: 81
Loss: 0.1274224986876012
ROC train: 0.963643	val: 0.834663	test: 0.834712
PRC train: 0.792492	val: 0.443184	test: 0.448364

Epoch: 82
Loss: 0.1265500007155387
ROC train: 0.964033	val: 0.825741	test: 0.824900
PRC train: 0.791742	val: 0.440851	test: 0.452696

Epoch: 83
Loss: 0.1280171159251851
ROC train: 0.966212	val: 0.825846	test: 0.826075
PRC train: 0.801554	val: 0.443987	test: 0.448062

Epoch: 84
Loss: 0.12960002638732948
ROC train: 0.963976	val: 0.833534	test: 0.829650
PRC train: 0.792691	val: 0.449312	test: 0.458889

Epoch: 85
Loss: 0.12567111028417147
ROC train: 0.966401	val: 0.825534	test: 0.827009
PRC train: 0.806317	val: 0.437658	test: 0.451520

Epoch: 86
Loss: 0.12314195864723566
ROC train: 0.967247	val: 0.836707	test: 0.831660
PRC train: 0.808483	val: 0.474993	test: 0.457933

Epoch: 87
Loss: 0.12315237973783888
ROC train: 0.968858	val: 0.830278	test: 0.829688
PRC train: 0.816176	val: 0.429284	test: 0.455787

Epoch: 88
Loss: 0.12601352344078737
ROC train: 0.968736	val: 0.820382	test: 0.820116
PRC train: 0.809764	val: 0.431144	test: 0.431739

Epoch: 89
Loss: 0.1201368004495489
ROC train: 0.969265	val: 0.839719	test: 0.824332
PRC train: 0.817319	val: 0.452256	test: 0.446484

Epoch: 90
Loss: 0.1207882194523525
ROC train: 0.969540	val: 0.838516	test: 0.825022
PRC train: 0.812602	val: 0.455219	test: 0.449758

Epoch: 91
Loss: 0.12084026629843478
ROC train: 0.971496	val: 0.829147	test: 0.826996
PRC train: 0.826801	val: 0.453704	test: 0.442611

Epoch: 92
Loss: 0.12196797622977003
ROC train: 0.972042	val: 0.831577	test: 0.822855
PRC train: 0.831935	val: 0.445990	test: 0.444597

Epoch: 93
Loss: 0.12015263516620914
ROC train: 0.972038	val: 0.829577	test: 0.830124
PRC train: 0.829258	val: 0.445961	test: 0.458817

Epoch: 94
Loss: 0.1220633209211086
ROC train: 0.910644	val: 0.831977	test: 0.829017
PRC train: 0.607176	val: 0.481251	test: 0.401275

Epoch: 34
Loss: 0.16431630978251552
ROC train: 0.909027	val: 0.826414	test: 0.827686
PRC train: 0.593414	val: 0.477049	test: 0.423861

Epoch: 35
Loss: 0.16541174853061172
ROC train: 0.906353	val: 0.829249	test: 0.828324
PRC train: 0.584379	val: 0.466944	test: 0.387906

Epoch: 36
Loss: 0.164743627958071
ROC train: 0.914625	val: 0.836076	test: 0.830319
PRC train: 0.615663	val: 0.489719	test: 0.419609

Epoch: 37
Loss: 0.16056187066514305
ROC train: 0.915732	val: 0.834359	test: 0.832036
PRC train: 0.624622	val: 0.499741	test: 0.433822

Epoch: 38
Loss: 0.15896639946119032
ROC train: 0.918533	val: 0.831195	test: 0.827670
PRC train: 0.631358	val: 0.500195	test: 0.432686

Epoch: 39
Loss: 0.15905332499805463
ROC train: 0.915833	val: 0.834508	test: 0.829022
PRC train: 0.618694	val: 0.483474	test: 0.427442

Epoch: 40
Loss: 0.1617516838854525
ROC train: 0.916587	val: 0.829996	test: 0.832045
PRC train: 0.619892	val: 0.491492	test: 0.416486

Epoch: 41
Loss: 0.15905863760644054
ROC train: 0.918867	val: 0.829768	test: 0.834630
PRC train: 0.627244	val: 0.478581	test: 0.431475

Epoch: 42
Loss: 0.15780641274463186
ROC train: 0.921128	val: 0.834536	test: 0.832694
PRC train: 0.645820	val: 0.499806	test: 0.430425

Epoch: 43
Loss: 0.15704078579602787
ROC train: 0.921538	val: 0.829067	test: 0.824159
PRC train: 0.635438	val: 0.485808	test: 0.407241

Epoch: 44
Loss: 0.157410425838835
ROC train: 0.920598	val: 0.835133	test: 0.837812
PRC train: 0.635708	val: 0.496602	test: 0.438504

Epoch: 45
Loss: 0.15619018660220083
ROC train: 0.927963	val: 0.842813	test: 0.834329
PRC train: 0.655557	val: 0.505118	test: 0.443306

Epoch: 46
Loss: 0.15547422694418614
ROC train: 0.928866	val: 0.843597	test: 0.832242
PRC train: 0.664453	val: 0.513125	test: 0.427910

Epoch: 47
Loss: 0.15374498062221117
ROC train: 0.930124	val: 0.841644	test: 0.829254
PRC train: 0.666868	val: 0.493986	test: 0.418737

Epoch: 48
Loss: 0.15266616686981233
ROC train: 0.927773	val: 0.835665	test: 0.832931
PRC train: 0.666555	val: 0.507791	test: 0.437918

Epoch: 49
Loss: 0.1535296550879915
ROC train: 0.933262	val: 0.840920	test: 0.833774
PRC train: 0.676586	val: 0.493460	test: 0.439681

Epoch: 50
Loss: 0.15216498039698972
ROC train: 0.933530	val: 0.837109	test: 0.835093
PRC train: 0.674856	val: 0.493010	test: 0.416540

Epoch: 51
Loss: 0.15073705621654107
ROC train: 0.934034	val: 0.837249	test: 0.833323
PRC train: 0.683011	val: 0.499201	test: 0.432620

Epoch: 52
Loss: 0.14998142365453987
ROC train: 0.935617	val: 0.842256	test: 0.837198
PRC train: 0.687481	val: 0.501642	test: 0.439993

Epoch: 53
Loss: 0.14985103960118254
ROC train: 0.936639	val: 0.836537	test: 0.837300
PRC train: 0.684240	val: 0.493004	test: 0.436713

Epoch: 54
Loss: 0.14807243277343451
ROC train: 0.938730	val: 0.842355	test: 0.836164
PRC train: 0.701208	val: 0.504177	test: 0.446607

Epoch: 55
Loss: 0.14751653428230088
ROC train: 0.940332	val: 0.838419	test: 0.835875
PRC train: 0.707744	val: 0.494171	test: 0.425601

Epoch: 56
Loss: 0.1463873507352299
ROC train: 0.938668	val: 0.837750	test: 0.836202
PRC train: 0.698703	val: 0.498955	test: 0.442085

Epoch: 57
Loss: 0.14729815708108807
ROC train: 0.940728	val: 0.836530	test: 0.835838
PRC train: 0.702391	val: 0.487742	test: 0.425891

Epoch: 58
Loss: 0.14427909424662277
ROC train: 0.939219	val: 0.838411	test: 0.829671
PRC train: 0.699696	val: 0.511021	test: 0.435800

Epoch: 59
Loss: 0.1471786311698203
ROC train: 0.940541	val: 0.832904	test: 0.835628
PRC train: 0.703854	val: 0.511340	test: 0.443917

Epoch: 60
Loss: 0.14760473293846366
ROC train: 0.941375	val: 0.840689	test: 0.838436
PRC train: 0.708499	val: 0.494630	test: 0.438685

Epoch: 61
Loss: 0.1439141141756866
ROC train: 0.944725	val: 0.841812	test: 0.831609
PRC train: 0.724151	val: 0.506554	test: 0.424550

Epoch: 62
Loss: 0.14241185057063246
ROC train: 0.944541	val: 0.839307	test: 0.839338
PRC train: 0.722994	val: 0.516360	test: 0.449687

Epoch: 63
Loss: 0.14011119635874758
ROC train: 0.946140	val: 0.841605	test: 0.833683
PRC train: 0.724098	val: 0.507429	test: 0.438949

Epoch: 64
Loss: 0.14186998537843348
ROC train: 0.948118	val: 0.835072	test: 0.833312
PRC train: 0.731855	val: 0.507397	test: 0.421854

Epoch: 65
Loss: 0.13970327711163674
ROC train: 0.947853	val: 0.834157	test: 0.833541
PRC train: 0.724809	val: 0.481447	test: 0.442146

Epoch: 66
Loss: 0.14074899168569877
ROC train: 0.950599	val: 0.842793	test: 0.826759
PRC train: 0.743738	val: 0.502310	test: 0.437083

Epoch: 67
Loss: 0.13776122928596163
ROC train: 0.951004	val: 0.839871	test: 0.829582
PRC train: 0.744219	val: 0.518479	test: 0.450955

Epoch: 68
Loss: 0.13837117485402428
ROC train: 0.952300	val: 0.837326	test: 0.831417
PRC train: 0.749238	val: 0.502985	test: 0.426707

Epoch: 69
Loss: 0.13750707490099623
ROC train: 0.950357	val: 0.842657	test: 0.829732
PRC train: 0.740492	val: 0.501600	test: 0.427131

Epoch: 70
Loss: 0.13707335320602365
ROC train: 0.952361	val: 0.840285	test: 0.831428
PRC train: 0.750367	val: 0.490480	test: 0.444048

Epoch: 71
Loss: 0.13628330644190748
ROC train: 0.954310	val: 0.836451	test: 0.832572
PRC train: 0.762048	val: 0.498244	test: 0.433681

Epoch: 72
Loss: 0.13482873431409761
ROC train: 0.955522	val: 0.839366	test: 0.833477
PRC train: 0.766407	val: 0.491964	test: 0.434636

Epoch: 73
Loss: 0.13460422211970977
ROC train: 0.956564	val: 0.839601	test: 0.834545
PRC train: 0.767686	val: 0.501067	test: 0.436997

Epoch: 74
Loss: 0.1356127478329562
ROC train: 0.956460	val: 0.834771	test: 0.826885
PRC train: 0.773794	val: 0.508123	test: 0.429650

Epoch: 75
Loss: 0.13372422381985394
ROC train: 0.959082	val: 0.843557	test: 0.827404
PRC train: 0.777976	val: 0.503525	test: 0.427460

Epoch: 76
Loss: 0.13244744681360038
ROC train: 0.959010	val: 0.843775	test: 0.836171
PRC train: 0.780280	val: 0.499399	test: 0.444881

Epoch: 77
Loss: 0.13183277055787088
ROC train: 0.957519	val: 0.845026	test: 0.829642
PRC train: 0.775909	val: 0.504298	test: 0.442577

Epoch: 78
Loss: 0.13435197893993536
ROC train: 0.956862	val: 0.842949	test: 0.827789
PRC train: 0.761458	val: 0.507998	test: 0.434517

Epoch: 79
Loss: 0.1326871386452366
ROC train: 0.960441	val: 0.841409	test: 0.832892
PRC train: 0.786147	val: 0.507930	test: 0.441477

Epoch: 80
Loss: 0.1328482265849742
ROC train: 0.961738	val: 0.842365	test: 0.831592
PRC train: 0.787338	val: 0.494692	test: 0.437638

Epoch: 81
Loss: 0.1293889833241337
ROC train: 0.962910	val: 0.841297	test: 0.832096
PRC train: 0.795035	val: 0.498665	test: 0.438873

Epoch: 82
Loss: 0.1307543794933143
ROC train: 0.961587	val: 0.832915	test: 0.828059
PRC train: 0.783821	val: 0.485968	test: 0.418650

Epoch: 83
Loss: 0.1289871237329275
ROC train: 0.963711	val: 0.836760	test: 0.833166
PRC train: 0.797477	val: 0.501509	test: 0.438868

Epoch: 84
Loss: 0.12878561664685087
ROC train: 0.963501	val: 0.840516	test: 0.828702
PRC train: 0.796254	val: 0.500427	test: 0.436712

Epoch: 85
Loss: 0.12774577632344944
ROC train: 0.964634	val: 0.834234	test: 0.833425
PRC train: 0.806708	val: 0.500112	test: 0.450096

Epoch: 86
Loss: 0.1262801080539407
ROC train: 0.965808	val: 0.839348	test: 0.828475
PRC train: 0.811371	val: 0.507658	test: 0.432487

Epoch: 87
Loss: 0.12702628719708195
ROC train: 0.966587	val: 0.846362	test: 0.829927
PRC train: 0.812307	val: 0.503709	test: 0.447372

Epoch: 88
Loss: 0.12708441782239224
ROC train: 0.963827	val: 0.836421	test: 0.826407
PRC train: 0.796461	val: 0.508138	test: 0.434055

Epoch: 89
Loss: 0.12397322407360628
ROC train: 0.965615	val: 0.832187	test: 0.826638
PRC train: 0.807742	val: 0.483150	test: 0.421186

Epoch: 90
Loss: 0.1232752862439638
ROC train: 0.967755	val: 0.834616	test: 0.826437
PRC train: 0.815575	val: 0.503783	test: 0.441956

Epoch: 91
Loss: 0.12243470069652769
ROC train: 0.970096	val: 0.841802	test: 0.833596
PRC train: 0.827014	val: 0.503367	test: 0.439060

Epoch: 92
Loss: 0.1219081162096554
ROC train: 0.970132	val: 0.840768	test: 0.826786
PRC train: 0.827738	val: 0.511368	test: 0.451941

Epoch: 93
Loss: 0.12118213470060021
ROC train: 0.970241	val: 0.837130	test: 0.823794
PRC train: 0.825318	val: 0.491546	test: 0.422804

Epoch: 94
Loss: 0.12026968609408346
ROC train: 0.909174	val: 0.828114	test: 0.829433
PRC train: 0.598264	val: 0.471595	test: 0.434419

Epoch: 34
Loss: 0.1659922293336792
ROC train: 0.901586	val: 0.821660	test: 0.818983
PRC train: 0.577286	val: 0.461000	test: 0.384953

Epoch: 35
Loss: 0.16662045396438735
ROC train: 0.908113	val: 0.827303	test: 0.831865
PRC train: 0.595575	val: 0.478302	test: 0.430160

Epoch: 36
Loss: 0.16336198919353662
ROC train: 0.912579	val: 0.828166	test: 0.834378
PRC train: 0.609051	val: 0.471229	test: 0.425658

Epoch: 37
Loss: 0.1624331822844442
ROC train: 0.913951	val: 0.830479	test: 0.831617
PRC train: 0.607144	val: 0.479844	test: 0.430127

Epoch: 38
Loss: 0.1641002523323208
ROC train: 0.910537	val: 0.826834	test: 0.826545
PRC train: 0.607188	val: 0.481252	test: 0.413856

Epoch: 39
Loss: 0.16087039390413083
ROC train: 0.913292	val: 0.819125	test: 0.818757
PRC train: 0.610891	val: 0.469614	test: 0.412316

Epoch: 40
Loss: 0.1608859201585986
ROC train: 0.909091	val: 0.817139	test: 0.815995
PRC train: 0.591865	val: 0.459883	test: 0.377943

Epoch: 41
Loss: 0.16069857334397153
ROC train: 0.917223	val: 0.829091	test: 0.828100
PRC train: 0.625806	val: 0.482461	test: 0.422890

Epoch: 42
Loss: 0.15776115358860218
ROC train: 0.919158	val: 0.831776	test: 0.821674
PRC train: 0.625339	val: 0.482202	test: 0.420730

Epoch: 43
Loss: 0.1569215117850087
ROC train: 0.924593	val: 0.830134	test: 0.832939
PRC train: 0.650526	val: 0.496522	test: 0.434915

Epoch: 44
Loss: 0.15609205748164703
ROC train: 0.923896	val: 0.837977	test: 0.830239
PRC train: 0.645569	val: 0.489550	test: 0.422674

Epoch: 45
Loss: 0.15710468810785258
ROC train: 0.924750	val: 0.837581	test: 0.825538
PRC train: 0.649210	val: 0.500157	test: 0.427707

Epoch: 46
Loss: 0.15572906277177806
ROC train: 0.927025	val: 0.834234	test: 0.833361
PRC train: 0.658000	val: 0.497445	test: 0.440765

Epoch: 47
Loss: 0.15474928034960592
ROC train: 0.928506	val: 0.834399	test: 0.825714
PRC train: 0.655627	val: 0.496054	test: 0.429225

Epoch: 48
Loss: 0.15188611183610598
ROC train: 0.926065	val: 0.839646	test: 0.828563
PRC train: 0.652359	val: 0.502140	test: 0.435557

Epoch: 49
Loss: 0.1542451348286
ROC train: 0.932138	val: 0.833198	test: 0.825936
PRC train: 0.671943	val: 0.497259	test: 0.434799

Epoch: 50
Loss: 0.15095001138207664
ROC train: 0.930901	val: 0.833642	test: 0.826010
PRC train: 0.663042	val: 0.484889	test: 0.425406

Epoch: 51
Loss: 0.1496139127108226
ROC train: 0.926484	val: 0.839052	test: 0.822927
PRC train: 0.651400	val: 0.481550	test: 0.421200

Epoch: 52
Loss: 0.15008689056199478
ROC train: 0.933775	val: 0.827757	test: 0.829032
PRC train: 0.679732	val: 0.475931	test: 0.441695

Epoch: 53
Loss: 0.1511767458474643
ROC train: 0.935299	val: 0.834412	test: 0.825958
PRC train: 0.690450	val: 0.500176	test: 0.437778

Epoch: 54
Loss: 0.1492242488871563
ROC train: 0.935345	val: 0.832053	test: 0.824912
PRC train: 0.678835	val: 0.496659	test: 0.434711

Epoch: 55
Loss: 0.14876165150805584
ROC train: 0.938124	val: 0.837825	test: 0.826854
PRC train: 0.698232	val: 0.494688	test: 0.444726

Epoch: 56
Loss: 0.14616660369135795
ROC train: 0.939077	val: 0.836727	test: 0.829598
PRC train: 0.694012	val: 0.497908	test: 0.434158

Epoch: 57
Loss: 0.1463686835095716
ROC train: 0.940826	val: 0.841068	test: 0.831526
PRC train: 0.712076	val: 0.495442	test: 0.446459

Epoch: 58
Loss: 0.14501648000467957
ROC train: 0.939908	val: 0.841970	test: 0.824886
PRC train: 0.700773	val: 0.501508	test: 0.438872

Epoch: 59
Loss: 0.1460064605613996
ROC train: 0.943433	val: 0.842217	test: 0.831609
PRC train: 0.714330	val: 0.498100	test: 0.444314

Epoch: 60
Loss: 0.14324813395662783
ROC train: 0.944256	val: 0.844173	test: 0.830376
PRC train: 0.725427	val: 0.506871	test: 0.435194

Epoch: 61
Loss: 0.14317685263025173
ROC train: 0.945050	val: 0.837403	test: 0.829669
PRC train: 0.721472	val: 0.497157	test: 0.436810

Epoch: 62
Loss: 0.14258378385592765
ROC train: 0.943963	val: 0.840067	test: 0.834589
PRC train: 0.713976	val: 0.513627	test: 0.426937

Epoch: 63
Loss: 0.1439310691010537
ROC train: 0.942300	val: 0.838064	test: 0.824832
PRC train: 0.710200	val: 0.503086	test: 0.407425

Epoch: 64
Loss: 0.14280371144399887
ROC train: 0.947317	val: 0.839520	test: 0.826038
PRC train: 0.729982	val: 0.503535	test: 0.413912

Epoch: 65
Loss: 0.13894724064130878
ROC train: 0.940790	val: 0.829012	test: 0.824748
PRC train: 0.702019	val: 0.483575	test: 0.418281

Epoch: 66
Loss: 0.14045577317898736
ROC train: 0.949373	val: 0.837890	test: 0.828888
PRC train: 0.736630	val: 0.507933	test: 0.444040

Epoch: 67
Loss: 0.1379595035850644
ROC train: 0.949533	val: 0.842986	test: 0.829758
PRC train: 0.738881	val: 0.491054	test: 0.436107

Epoch: 68
Loss: 0.13967960074066746
ROC train: 0.949938	val: 0.835312	test: 0.828169
PRC train: 0.732017	val: 0.496365	test: 0.444994

Epoch: 69
Loss: 0.1385577114085871
ROC train: 0.951921	val: 0.839458	test: 0.826675
PRC train: 0.749509	val: 0.515966	test: 0.439532

Epoch: 70
Loss: 0.13563189714034704
ROC train: 0.952620	val: 0.836605	test: 0.819644
PRC train: 0.749166	val: 0.505794	test: 0.436078

Epoch: 71
Loss: 0.13584032387495795
ROC train: 0.950365	val: 0.837546	test: 0.825262
PRC train: 0.736421	val: 0.485553	test: 0.453769

Epoch: 72
Loss: 0.1349473574151405
ROC train: 0.954818	val: 0.832437	test: 0.824580
PRC train: 0.759628	val: 0.493122	test: 0.435362

Epoch: 73
Loss: 0.1349326438447689
ROC train: 0.953676	val: 0.837697	test: 0.830254
PRC train: 0.752104	val: 0.487840	test: 0.447707

Epoch: 74
Loss: 0.13490007020109643
ROC train: 0.954559	val: 0.830895	test: 0.823845
PRC train: 0.749817	val: 0.491900	test: 0.443634

Epoch: 75
Loss: 0.13284109700891183
ROC train: 0.958005	val: 0.837550	test: 0.825400
PRC train: 0.774928	val: 0.495593	test: 0.445681

Epoch: 76
Loss: 0.13198214314071785
ROC train: 0.959202	val: 0.838946	test: 0.832068
PRC train: 0.781731	val: 0.500466	test: 0.453247

Epoch: 77
Loss: 0.13031643657833103
ROC train: 0.959162	val: 0.838496	test: 0.826203
PRC train: 0.775712	val: 0.500825	test: 0.436101

Epoch: 78
Loss: 0.1319918880929859
ROC train: 0.961217	val: 0.835037	test: 0.834145
PRC train: 0.782521	val: 0.504066	test: 0.450790

Epoch: 79
Loss: 0.12979448917281017
ROC train: 0.960010	val: 0.838271	test: 0.824210
PRC train: 0.783483	val: 0.504043	test: 0.434708

Epoch: 80
Loss: 0.1277642292354728
ROC train: 0.961547	val: 0.832810	test: 0.823748
PRC train: 0.783843	val: 0.494754	test: 0.426982

Epoch: 81
Loss: 0.130304806707033
ROC train: 0.962981	val: 0.832212	test: 0.830795
PRC train: 0.791053	val: 0.487071	test: 0.445463

Epoch: 82
Loss: 0.12856700457807174
ROC train: 0.963034	val: 0.834103	test: 0.826626
PRC train: 0.792911	val: 0.493881	test: 0.441187

Epoch: 83
Loss: 0.1284305265014386
ROC train: 0.962956	val: 0.835626	test: 0.825274
PRC train: 0.787477	val: 0.499242	test: 0.433728

Epoch: 84
Loss: 0.12693531542369954
ROC train: 0.964908	val: 0.832949	test: 0.822447
PRC train: 0.801436	val: 0.502087	test: 0.434589

Epoch: 85
Loss: 0.12649403695757552
ROC train: 0.966167	val: 0.840665	test: 0.830294
PRC train: 0.804593	val: 0.501612	test: 0.450697

Epoch: 86
Loss: 0.1266189014909583
ROC train: 0.965217	val: 0.836782	test: 0.822850
PRC train: 0.801915	val: 0.490037	test: 0.436186

Epoch: 87
Loss: 0.12459085688856313
ROC train: 0.966280	val: 0.842542	test: 0.826476
PRC train: 0.804229	val: 0.501816	test: 0.434192

Epoch: 88
Loss: 0.12455108670256672
ROC train: 0.966663	val: 0.841126	test: 0.830604
PRC train: 0.806067	val: 0.504507	test: 0.448148

Epoch: 89
Loss: 0.12424095794190287
ROC train: 0.966799	val: 0.831914	test: 0.827072
PRC train: 0.811683	val: 0.485716	test: 0.445100

Epoch: 90
Loss: 0.1243983287905708
ROC train: 0.967126	val: 0.838955	test: 0.827476
PRC train: 0.812509	val: 0.504152	test: 0.450506

Epoch: 91
Loss: 0.12304609361934218
ROC train: 0.968482	val: 0.833994	test: 0.823300
PRC train: 0.819082	val: 0.481254	test: 0.450509

Epoch: 92
Loss: 0.12244078368406351
ROC train: 0.970855	val: 0.842486	test: 0.824979
PRC train: 0.826463	val: 0.502783	test: 0.458960

Epoch: 93
Loss: 0.11869578616115803
ROC train: 0.969587	val: 0.835615	test: 0.831576
PRC train: 0.825630	val: 0.499962	test: 0.455545

Epoch: 94
Loss: 0.12165309414180114
ROC train: 0.904571	val: 0.834760	test: 0.836485
PRC train: 0.588873	val: 0.482991	test: 0.401931

Epoch: 34
Loss: 0.16601335219088262
ROC train: 0.906390	val: 0.827936	test: 0.829443
PRC train: 0.592921	val: 0.478502	test: 0.398695

Epoch: 35
Loss: 0.1656488068205296
ROC train: 0.903631	val: 0.835029	test: 0.833131
PRC train: 0.579197	val: 0.482704	test: 0.420851

Epoch: 36
Loss: 0.16420331553587386
ROC train: 0.913231	val: 0.839457	test: 0.837870
PRC train: 0.616376	val: 0.491468	test: 0.425581

Epoch: 37
Loss: 0.1639568588940482
ROC train: 0.914704	val: 0.838036	test: 0.839083
PRC train: 0.619723	val: 0.494200	test: 0.419715

Epoch: 38
Loss: 0.16275883639454647
ROC train: 0.913901	val: 0.840562	test: 0.839311
PRC train: 0.613744	val: 0.501757	test: 0.425209

Epoch: 39
Loss: 0.1627213345535509
ROC train: 0.916101	val: 0.832203	test: 0.833225
PRC train: 0.622766	val: 0.485646	test: 0.412060

Epoch: 40
Loss: 0.16161053942057435
ROC train: 0.918149	val: 0.833755	test: 0.831030
PRC train: 0.629808	val: 0.498780	test: 0.416839

Epoch: 41
Loss: 0.1593362943397976
ROC train: 0.920170	val: 0.841743	test: 0.838330
PRC train: 0.639366	val: 0.508404	test: 0.434179

Epoch: 42
Loss: 0.15938549414016434
ROC train: 0.916106	val: 0.836169	test: 0.833517
PRC train: 0.611746	val: 0.485117	test: 0.421218

Epoch: 43
Loss: 0.15848992730953693
ROC train: 0.921261	val: 0.831456	test: 0.832008
PRC train: 0.645668	val: 0.497569	test: 0.429624

Epoch: 44
Loss: 0.15741864000637237
ROC train: 0.924406	val: 0.836486	test: 0.833595
PRC train: 0.655487	val: 0.496842	test: 0.418365

Epoch: 45
Loss: 0.15632217398033257
ROC train: 0.923906	val: 0.839271	test: 0.835614
PRC train: 0.648406	val: 0.492197	test: 0.440036

Epoch: 46
Loss: 0.15711074069126685
ROC train: 0.925860	val: 0.833782	test: 0.833126
PRC train: 0.659108	val: 0.479089	test: 0.413675

Epoch: 47
Loss: 0.155255065734747
ROC train: 0.926139	val: 0.842346	test: 0.839210
PRC train: 0.657549	val: 0.499334	test: 0.432708

Epoch: 48
Loss: 0.15530622862862795
ROC train: 0.927537	val: 0.839541	test: 0.840542
PRC train: 0.656408	val: 0.489747	test: 0.432124

Epoch: 49
Loss: 0.15545884191854803
ROC train: 0.928017	val: 0.840496	test: 0.833995
PRC train: 0.662259	val: 0.489039	test: 0.424765

Epoch: 50
Loss: 0.15177560969865528
ROC train: 0.930686	val: 0.837489	test: 0.833904
PRC train: 0.672642	val: 0.522099	test: 0.445963

Epoch: 51
Loss: 0.14885122307109763
ROC train: 0.931851	val: 0.839223	test: 0.836433
PRC train: 0.677024	val: 0.497588	test: 0.434815

Epoch: 52
Loss: 0.15040393960275616
ROC train: 0.934318	val: 0.837511	test: 0.838538
PRC train: 0.684294	val: 0.509833	test: 0.432863

Epoch: 53
Loss: 0.15005779231826544
ROC train: 0.935734	val: 0.841896	test: 0.836015
PRC train: 0.692293	val: 0.509155	test: 0.424336

Epoch: 54
Loss: 0.14826563552225613
ROC train: 0.936715	val: 0.832805	test: 0.831636
PRC train: 0.694352	val: 0.509972	test: 0.425517

Epoch: 55
Loss: 0.14952699281732995
ROC train: 0.935565	val: 0.834390	test: 0.832135
PRC train: 0.688302	val: 0.514002	test: 0.438833

Epoch: 56
Loss: 0.14672517633568663
ROC train: 0.939033	val: 0.842907	test: 0.832133
PRC train: 0.703217	val: 0.506806	test: 0.427341

Epoch: 57
Loss: 0.1459985586946316
ROC train: 0.937253	val: 0.838652	test: 0.832008
PRC train: 0.692591	val: 0.484351	test: 0.427655

Epoch: 58
Loss: 0.14512440170521812
ROC train: 0.940611	val: 0.839946	test: 0.836412
PRC train: 0.705978	val: 0.507735	test: 0.420627

Epoch: 59
Loss: 0.14696983165038782
ROC train: 0.940658	val: 0.838465	test: 0.835432
PRC train: 0.712698	val: 0.494261	test: 0.436522

Epoch: 60
Loss: 0.14391738160390125
ROC train: 0.939459	val: 0.834186	test: 0.831432
PRC train: 0.703104	val: 0.495115	test: 0.422186

Epoch: 61
Loss: 0.14352278146125044
ROC train: 0.943037	val: 0.838858	test: 0.829861
PRC train: 0.717787	val: 0.509569	test: 0.447586

Epoch: 62
Loss: 0.14221505697982317
ROC train: 0.944732	val: 0.840306	test: 0.834580
PRC train: 0.723183	val: 0.507118	test: 0.438371

Epoch: 63
Loss: 0.14237758205304626
ROC train: 0.946783	val: 0.837632	test: 0.831232
PRC train: 0.729645	val: 0.507959	test: 0.448875

Epoch: 64
Loss: 0.14125452802892358
ROC train: 0.946982	val: 0.838312	test: 0.835716
PRC train: 0.726518	val: 0.500771	test: 0.434702

Epoch: 65
Loss: 0.14345336042855186
ROC train: 0.943083	val: 0.836732	test: 0.828651
PRC train: 0.715523	val: 0.501970	test: 0.425297

Epoch: 66
Loss: 0.14113028737248
ROC train: 0.948991	val: 0.841099	test: 0.835104
PRC train: 0.745086	val: 0.510639	test: 0.439048

Epoch: 67
Loss: 0.13955162009077848
ROC train: 0.947561	val: 0.839679	test: 0.836173
PRC train: 0.728186	val: 0.501641	test: 0.454033

Epoch: 68
Loss: 0.13959920302134438
ROC train: 0.951165	val: 0.841461	test: 0.833221
PRC train: 0.749487	val: 0.508357	test: 0.445415

Epoch: 69
Loss: 0.13791211038810133
ROC train: 0.948167	val: 0.841036	test: 0.836675
PRC train: 0.735464	val: 0.500849	test: 0.422919

Epoch: 70
Loss: 0.13923364754543552
ROC train: 0.952037	val: 0.844568	test: 0.835614
PRC train: 0.759122	val: 0.522282	test: 0.452583

Epoch: 71
Loss: 0.13828809475320683
ROC train: 0.954239	val: 0.840155	test: 0.836680
PRC train: 0.763173	val: 0.503974	test: 0.444079

Epoch: 72
Loss: 0.13600098600398763
ROC train: 0.953822	val: 0.839644	test: 0.830853
PRC train: 0.764960	val: 0.513927	test: 0.446010

Epoch: 73
Loss: 0.13468034431868858
ROC train: 0.954504	val: 0.834217	test: 0.824249
PRC train: 0.755818	val: 0.502259	test: 0.423569

Epoch: 74
Loss: 0.13385525083964822
ROC train: 0.956767	val: 0.837884	test: 0.828663
PRC train: 0.770395	val: 0.499190	test: 0.422146

Epoch: 75
Loss: 0.13376271851168792
ROC train: 0.956173	val: 0.841391	test: 0.834726
PRC train: 0.771160	val: 0.495204	test: 0.449661

Epoch: 76
Loss: 0.1326478349003989
ROC train: 0.957499	val: 0.836468	test: 0.826958
PRC train: 0.775361	val: 0.513562	test: 0.421627

Epoch: 77
Loss: 0.13379692319518222
ROC train: 0.959436	val: 0.831975	test: 0.826813
PRC train: 0.781487	val: 0.498298	test: 0.427809

Epoch: 78
Loss: 0.13014436986054453
ROC train: 0.959184	val: 0.834390	test: 0.827263
PRC train: 0.778899	val: 0.495017	test: 0.423121

Epoch: 79
Loss: 0.13052364648210962
ROC train: 0.958937	val: 0.841006	test: 0.834311
PRC train: 0.775340	val: 0.512972	test: 0.454029

Epoch: 80
Loss: 0.13099629596569126
ROC train: 0.961299	val: 0.834104	test: 0.834349
PRC train: 0.788200	val: 0.500895	test: 0.456854

Epoch: 81
Loss: 0.12954690113533243
ROC train: 0.961726	val: 0.843426	test: 0.834637
PRC train: 0.791177	val: 0.511201	test: 0.455059

Epoch: 82
Loss: 0.1290840179241287
ROC train: 0.963052	val: 0.838396	test: 0.832634
PRC train: 0.804828	val: 0.505426	test: 0.448691

Epoch: 83
Loss: 0.12935717535556865
ROC train: 0.958351	val: 0.841489	test: 0.822710
PRC train: 0.773610	val: 0.508630	test: 0.418741

Epoch: 84
Loss: 0.127562452413046
ROC train: 0.965364	val: 0.835813	test: 0.830755
PRC train: 0.807386	val: 0.505659	test: 0.428259

Epoch: 85
Loss: 0.1274454711533646
ROC train: 0.963607	val: 0.837836	test: 0.833313
PRC train: 0.803932	val: 0.498634	test: 0.437213

Epoch: 86
Loss: 0.1253305816213591
ROC train: 0.965628	val: 0.836008	test: 0.830334
PRC train: 0.806284	val: 0.502773	test: 0.424977

Epoch: 87
Loss: 0.12357421136992855
ROC train: 0.966566	val: 0.840596	test: 0.834470
PRC train: 0.813358	val: 0.509610	test: 0.457234

Epoch: 88
Loss: 0.12317815082326461
ROC train: 0.967830	val: 0.837226	test: 0.834819
PRC train: 0.820369	val: 0.516000	test: 0.447461

Epoch: 89
Loss: 0.12369874570858536
ROC train: 0.966562	val: 0.832935	test: 0.829297
PRC train: 0.814607	val: 0.498614	test: 0.427377

Epoch: 90
Loss: 0.12586991119679591
ROC train: 0.967784	val: 0.835501	test: 0.826647
PRC train: 0.811924	val: 0.496514	test: 0.419844

Epoch: 91
Loss: 0.12374428814640663
ROC train: 0.968072	val: 0.826947	test: 0.832519
PRC train: 0.816875	val: 0.503152	test: 0.444323

Epoch: 92
Loss: 0.12181427823639784
ROC train: 0.967986	val: 0.837798	test: 0.826949
PRC train: 0.822953	val: 0.499776	test: 0.424774

Epoch: 93
Loss: 0.12221856207188958
ROC train: 0.971047	val: 0.823926	test: 0.830260
PRC train: 0.829964	val: 0.504346	test: 0.443543

Epoch: 94
Loss: 0.12213803992505692
ROC train: 0.970144	val: 0.825160	test: 0.818200
PRC train: 0.819766	val: 0.430935	test: 0.437761

Epoch: 95
Loss: 0.11999981789928735
ROC train: 0.973243	val: 0.825125	test: 0.813432
PRC train: 0.838860	val: 0.424456	test: 0.441826

Epoch: 96
Loss: 0.11865382938135553
ROC train: 0.973472	val: 0.823903	test: 0.811338
PRC train: 0.841848	val: 0.437282	test: 0.435430

Epoch: 97
Loss: 0.11817152237001297
ROC train: 0.972826	val: 0.830414	test: 0.815782
PRC train: 0.834563	val: 0.433294	test: 0.434636

Epoch: 98
Loss: 0.11674961340135288
ROC train: 0.974931	val: 0.820717	test: 0.810199
PRC train: 0.847010	val: 0.426097	test: 0.441223

Epoch: 99
Loss: 0.11866503955158675
ROC train: 0.975318	val: 0.824031	test: 0.816525
PRC train: 0.849055	val: 0.426867	test: 0.446810

Epoch: 100
Loss: 0.11612160152271318
ROC train: 0.976256	val: 0.821682	test: 0.811707
PRC train: 0.853320	val: 0.421877	test: 0.426575

Epoch: 101
Loss: 0.11753029202550219
ROC train: 0.975880	val: 0.823677	test: 0.810179
PRC train: 0.853433	val: 0.437415	test: 0.432260

Epoch: 102
Loss: 0.11578533103323536
ROC train: 0.969551	val: 0.816805	test: 0.808607
PRC train: 0.810239	val: 0.411729	test: 0.427674

Epoch: 103
Loss: 0.11481495707624834
ROC train: 0.977071	val: 0.816216	test: 0.808494
PRC train: 0.859904	val: 0.418921	test: 0.419695

Epoch: 104
Loss: 0.11254094560166843
ROC train: 0.977247	val: 0.826063	test: 0.820475
PRC train: 0.855622	val: 0.425877	test: 0.434460

Epoch: 105
Loss: 0.11529071480037775
ROC train: 0.977949	val: 0.826081	test: 0.816117
PRC train: 0.863322	val: 0.438873	test: 0.436953

Epoch: 106
Loss: 0.10969400803014333
ROC train: 0.980219	val: 0.819420	test: 0.805935
PRC train: 0.870796	val: 0.425950	test: 0.432521

Epoch: 107
Loss: 0.11265119812454971
ROC train: 0.979078	val: 0.827914	test: 0.820106
PRC train: 0.869518	val: 0.441206	test: 0.449937

Epoch: 108
Loss: 0.11139638061087409
ROC train: 0.978616	val: 0.824457	test: 0.815155
PRC train: 0.863577	val: 0.438643	test: 0.438455

Epoch: 109
Loss: 0.1099890638217353
ROC train: 0.980579	val: 0.824815	test: 0.808303
PRC train: 0.874917	val: 0.431543	test: 0.438403

Epoch: 110
Loss: 0.11104721718213947
ROC train: 0.980948	val: 0.823681	test: 0.810913
PRC train: 0.879289	val: 0.437622	test: 0.435423

Epoch: 111
Loss: 0.109348765449124
ROC train: 0.981269	val: 0.822847	test: 0.809758
PRC train: 0.879961	val: 0.441317	test: 0.445972

Epoch: 112
Loss: 0.11170433761770186
ROC train: 0.980604	val: 0.820108	test: 0.807031
PRC train: 0.874725	val: 0.430712	test: 0.432791

Epoch: 113
Loss: 0.10935674077729313
ROC train: 0.981072	val: 0.821159	test: 0.812130
PRC train: 0.878346	val: 0.439065	test: 0.442028

Epoch: 114
Loss: 0.10700592337124278
ROC train: 0.982304	val: 0.820603	test: 0.808116
PRC train: 0.877942	val: 0.420131	test: 0.426708

Epoch: 115
Loss: 0.10844539080388409
ROC train: 0.981093	val: 0.826096	test: 0.812057
PRC train: 0.875628	val: 0.432923	test: 0.415134

Epoch: 116
Loss: 0.10836977423135198
ROC train: 0.983825	val: 0.821333	test: 0.811823
PRC train: 0.893292	val: 0.443434	test: 0.424460

Epoch: 117
Loss: 0.10471316100756588
ROC train: 0.982718	val: 0.814835	test: 0.812465
PRC train: 0.879508	val: 0.416662	test: 0.430976

Epoch: 118
Loss: 0.10671143854162157
ROC train: 0.984107	val: 0.824356	test: 0.811457
PRC train: 0.891974	val: 0.426353	test: 0.422463

Epoch: 119
Loss: 0.10593034102714016
ROC train: 0.984503	val: 0.816852	test: 0.811786
PRC train: 0.894629	val: 0.432526	test: 0.432606

Epoch: 120
Loss: 0.1039136533442265
ROC train: 0.984370	val: 0.824364	test: 0.813865
PRC train: 0.895966	val: 0.444513	test: 0.432787

Early stopping
Best (ROC):	 train: 0.944357	val: 0.837718	test: 0.825642
Best (PRC):	 train: 0.719495	val: 0.434776	test: 0.451242

ROC train: 0.968854	val: 0.836659	test: 0.825023
PRC train: 0.818227	val: 0.445607	test: 0.444899

Epoch: 95
Loss: 0.12030084323349959
ROC train: 0.972729	val: 0.831494	test: 0.821445
PRC train: 0.838031	val: 0.439798	test: 0.435527

Epoch: 96
Loss: 0.1174375195654909
ROC train: 0.973346	val: 0.831201	test: 0.824964
PRC train: 0.837335	val: 0.441929	test: 0.443352

Epoch: 97
Loss: 0.11875420383870793
ROC train: 0.972906	val: 0.832366	test: 0.823392
PRC train: 0.835959	val: 0.450389	test: 0.444315

Epoch: 98
Loss: 0.11875071285094957
ROC train: 0.974566	val: 0.825301	test: 0.821016
PRC train: 0.847075	val: 0.433732	test: 0.439925

Epoch: 99
Loss: 0.12156649465205073
ROC train: 0.975792	val: 0.823395	test: 0.822669
PRC train: 0.849396	val: 0.441981	test: 0.448313

Epoch: 100
Loss: 0.11911378889192764
ROC train: 0.974780	val: 0.833912	test: 0.827254
PRC train: 0.846550	val: 0.446566	test: 0.426415

Epoch: 101
Loss: 0.1166497118291655
ROC train: 0.971992	val: 0.831933	test: 0.825302
PRC train: 0.830472	val: 0.435985	test: 0.430042

Epoch: 102
Loss: 0.11655158408883541
ROC train: 0.977903	val: 0.832572	test: 0.824992
PRC train: 0.860578	val: 0.445677	test: 0.431454

Epoch: 103
Loss: 0.11306087707405763
ROC train: 0.977399	val: 0.830714	test: 0.824747
PRC train: 0.854595	val: 0.450021	test: 0.431003

Epoch: 104
Loss: 0.11362607238518595
ROC train: 0.975539	val: 0.826294	test: 0.819468
PRC train: 0.847470	val: 0.435399	test: 0.432590

Epoch: 105
Loss: 0.11352787127015715
ROC train: 0.977680	val: 0.829200	test: 0.821899
PRC train: 0.861302	val: 0.442139	test: 0.443022

Epoch: 106
Loss: 0.1155836403822055
ROC train: 0.979115	val: 0.829364	test: 0.817286
PRC train: 0.866936	val: 0.442124	test: 0.430084

Epoch: 107
Loss: 0.11427939781210152
ROC train: 0.978636	val: 0.827552	test: 0.823272
PRC train: 0.867396	val: 0.445210	test: 0.433170

Epoch: 108
Loss: 0.11299083796602695
ROC train: 0.978586	val: 0.824833	test: 0.818166
PRC train: 0.863138	val: 0.429034	test: 0.424817

Epoch: 109
Loss: 0.11304542241772308
ROC train: 0.979813	val: 0.828061	test: 0.816244
PRC train: 0.871705	val: 0.453864	test: 0.437531

Epoch: 110
Loss: 0.110671254997701
ROC train: 0.981515	val: 0.826627	test: 0.815679
PRC train: 0.879881	val: 0.439260	test: 0.428870

Epoch: 111
Loss: 0.11143019400574827
ROC train: 0.978299	val: 0.829620	test: 0.814396
PRC train: 0.861834	val: 0.444997	test: 0.437719

Epoch: 112
Loss: 0.11078107512578235
ROC train: 0.980475	val: 0.822686	test: 0.817290
PRC train: 0.873840	val: 0.435121	test: 0.426964

Epoch: 113
Loss: 0.10888911567100885
ROC train: 0.981461	val: 0.815134	test: 0.811347
PRC train: 0.879159	val: 0.434211	test: 0.430249

Epoch: 114
Loss: 0.10823691309965908
ROC train: 0.981686	val: 0.831857	test: 0.824995
PRC train: 0.879738	val: 0.448008	test: 0.433530

Epoch: 115
Loss: 0.10402497375070874
ROC train: 0.982539	val: 0.831976	test: 0.818452
PRC train: 0.883173	val: 0.447209	test: 0.421509

Epoch: 116
Loss: 0.10321726058106058
ROC train: 0.983880	val: 0.827024	test: 0.815904
PRC train: 0.890968	val: 0.438744	test: 0.431951

Epoch: 117
Loss: 0.1088072538409472
ROC train: 0.983426	val: 0.826418	test: 0.818357
PRC train: 0.889465	val: 0.434723	test: 0.433195

Epoch: 118
Loss: 0.10508173101921056
ROC train: 0.983166	val: 0.827304	test: 0.821268
PRC train: 0.888593	val: 0.432600	test: 0.425357

Epoch: 119
Loss: 0.10444071929035968
ROC train: 0.984460	val: 0.832586	test: 0.819715
PRC train: 0.897567	val: 0.457772	test: 0.442369

Epoch: 120
Loss: 0.1042016978228333
ROC train: 0.985360	val: 0.826963	test: 0.812762
PRC train: 0.900178	val: 0.445178	test: 0.426498

Early stopping
Best (ROC):	 train: 0.941217	val: 0.841561	test: 0.834142
Best (PRC):	 train: 0.709739	val: 0.439475	test: 0.444894

ROC train: 0.971885	val: 0.829118	test: 0.815995
PRC train: 0.828291	val: 0.439504	test: 0.435263

Epoch: 95
Loss: 0.12505719171932478
ROC train: 0.971110	val: 0.830353	test: 0.812357
PRC train: 0.820588	val: 0.439928	test: 0.429390

Epoch: 96
Loss: 0.11835993137170651
ROC train: 0.972156	val: 0.833012	test: 0.814167
PRC train: 0.832389	val: 0.442296	test: 0.420379

Epoch: 97
Loss: 0.11943834589321072
ROC train: 0.973352	val: 0.832906	test: 0.817011
PRC train: 0.836760	val: 0.436626	test: 0.427722

Epoch: 98
Loss: 0.1174818893400426
ROC train: 0.974626	val: 0.829335	test: 0.814682
PRC train: 0.844936	val: 0.427216	test: 0.426452

Epoch: 99
Loss: 0.11753831283201428
ROC train: 0.973512	val: 0.832925	test: 0.818307
PRC train: 0.840491	val: 0.434691	test: 0.443223

Epoch: 100
Loss: 0.1199529964084406
ROC train: 0.976084	val: 0.832148	test: 0.819111
PRC train: 0.853187	val: 0.432998	test: 0.442482

Epoch: 101
Loss: 0.1162884996241076
ROC train: 0.973088	val: 0.828293	test: 0.817991
PRC train: 0.831834	val: 0.426732	test: 0.429013

Epoch: 102
Loss: 0.11664770285437252
ROC train: 0.975661	val: 0.824789	test: 0.813381
PRC train: 0.846085	val: 0.424673	test: 0.425656

Epoch: 103
Loss: 0.11840516308020436
ROC train: 0.976013	val: 0.832274	test: 0.814807
PRC train: 0.849988	val: 0.434382	test: 0.426580

Epoch: 104
Loss: 0.11344922083146775
ROC train: 0.977316	val: 0.827290	test: 0.814467
PRC train: 0.857003	val: 0.432129	test: 0.430468

Epoch: 105
Loss: 0.11470363395784461
ROC train: 0.975517	val: 0.832243	test: 0.819223
PRC train: 0.845015	val: 0.422110	test: 0.423975

Epoch: 106
Loss: 0.11538483170707786
ROC train: 0.978361	val: 0.828358	test: 0.818021
PRC train: 0.864665	val: 0.424735	test: 0.435694

Epoch: 107
Loss: 0.11026170344847018
ROC train: 0.978593	val: 0.829584	test: 0.817215
PRC train: 0.865366	val: 0.425365	test: 0.432483

Epoch: 108
Loss: 0.1115333337803474
ROC train: 0.978860	val: 0.826318	test: 0.815504
PRC train: 0.867545	val: 0.422937	test: 0.431367

Epoch: 109
Loss: 0.11061786780026352
ROC train: 0.979926	val: 0.826167	test: 0.813829
PRC train: 0.872322	val: 0.424141	test: 0.436436

Epoch: 110
Loss: 0.11051003996391166
ROC train: 0.980950	val: 0.830151	test: 0.814309
PRC train: 0.873897	val: 0.432133	test: 0.420186

Epoch: 111
Loss: 0.10832849674924942
ROC train: 0.979619	val: 0.825403	test: 0.812255
PRC train: 0.867367	val: 0.417969	test: 0.425200

Epoch: 112
Loss: 0.10970787681031197
ROC train: 0.980942	val: 0.826526	test: 0.808786
PRC train: 0.877935	val: 0.423659	test: 0.429108

Epoch: 113
Loss: 0.10835147158694564
ROC train: 0.978449	val: 0.813723	test: 0.805190
PRC train: 0.862272	val: 0.407130	test: 0.412336

Epoch: 114
Loss: 0.11144413698143628
ROC train: 0.981564	val: 0.828240	test: 0.812769
PRC train: 0.877086	val: 0.425549	test: 0.427668

Epoch: 115
Loss: 0.10927423531280216
ROC train: 0.978237	val: 0.825117	test: 0.809723
PRC train: 0.858391	val: 0.424103	test: 0.422584

Epoch: 116
Loss: 0.10701141011365238
ROC train: 0.982692	val: 0.829618	test: 0.811959
PRC train: 0.883992	val: 0.437652	test: 0.435241

Epoch: 117
Loss: 0.10665680028578102
ROC train: 0.981663	val: 0.817528	test: 0.805798
PRC train: 0.879455	val: 0.421603	test: 0.416843

Epoch: 118
Loss: 0.10738346093227846
ROC train: 0.983195	val: 0.817970	test: 0.807489
PRC train: 0.887834	val: 0.400460	test: 0.408665

Epoch: 119
Loss: 0.1067707162977199
ROC train: 0.982382	val: 0.829845	test: 0.816513
PRC train: 0.879970	val: 0.421157	test: 0.423119

Epoch: 120
Loss: 0.10525731528766291
ROC train: 0.983929	val: 0.826826	test: 0.806937
PRC train: 0.895376	val: 0.435406	test: 0.424223

Early stopping
Best (ROC):	 train: 0.937987	val: 0.838041	test: 0.818760
Best (PRC):	 train: 0.704529	val: 0.432267	test: 0.430874
All runs completed.

ROC train: 0.972270	val: 0.823924	test: 0.819935
PRC train: 0.827654	val: 0.446487	test: 0.451391

Epoch: 95
Loss: 0.11762359541745061
ROC train: 0.972360	val: 0.827571	test: 0.819403
PRC train: 0.833656	val: 0.452369	test: 0.461406

Epoch: 96
Loss: 0.11688438587796651
ROC train: 0.971719	val: 0.823813	test: 0.816188
PRC train: 0.823124	val: 0.427464	test: 0.443786

Epoch: 97
Loss: 0.12021552841460656
ROC train: 0.971297	val: 0.831250	test: 0.825102
PRC train: 0.829147	val: 0.464972	test: 0.466445

Epoch: 98
Loss: 0.118071201885884
ROC train: 0.974614	val: 0.826501	test: 0.818818
PRC train: 0.843908	val: 0.448380	test: 0.459895

Epoch: 99
Loss: 0.11723439469845777
ROC train: 0.972831	val: 0.836267	test: 0.819384
PRC train: 0.833420	val: 0.447741	test: 0.467328

Epoch: 100
Loss: 0.11504431652591754
ROC train: 0.974483	val: 0.835593	test: 0.824116
PRC train: 0.840150	val: 0.453561	test: 0.461168

Epoch: 101
Loss: 0.1159540093650895
ROC train: 0.972675	val: 0.822961	test: 0.813857
PRC train: 0.831250	val: 0.440716	test: 0.443269

Epoch: 102
Loss: 0.11692114639206545
ROC train: 0.974746	val: 0.826920	test: 0.817828
PRC train: 0.845522	val: 0.444127	test: 0.461561

Epoch: 103
Loss: 0.11402770602688705
ROC train: 0.973398	val: 0.827547	test: 0.815768
PRC train: 0.834949	val: 0.458556	test: 0.455733

Epoch: 104
Loss: 0.11365019384952896
ROC train: 0.978102	val: 0.824563	test: 0.818006
PRC train: 0.865009	val: 0.459437	test: 0.459023

Epoch: 105
Loss: 0.1126475898421294
ROC train: 0.975432	val: 0.826653	test: 0.820904
PRC train: 0.848241	val: 0.434823	test: 0.467835

Epoch: 106
Loss: 0.10924555591200956
ROC train: 0.977884	val: 0.827308	test: 0.818498
PRC train: 0.862150	val: 0.443106	test: 0.464342

Epoch: 107
Loss: 0.11046809958394287
ROC train: 0.976652	val: 0.816795	test: 0.813152
PRC train: 0.854745	val: 0.438007	test: 0.455976

Epoch: 108
Loss: 0.11128119136464404
ROC train: 0.977882	val: 0.819869	test: 0.807727
PRC train: 0.855477	val: 0.433093	test: 0.459946

Epoch: 109
Loss: 0.10926144076757849
ROC train: 0.978543	val: 0.821803	test: 0.816379
PRC train: 0.866192	val: 0.435004	test: 0.467642

Epoch: 110
Loss: 0.10914277156058366
ROC train: 0.978927	val: 0.825433	test: 0.819035
PRC train: 0.864944	val: 0.424952	test: 0.466627

Epoch: 111
Loss: 0.1071837661314994
ROC train: 0.980500	val: 0.820329	test: 0.813378
PRC train: 0.872735	val: 0.452255	test: 0.456113

Epoch: 112
Loss: 0.1085336187476266
ROC train: 0.981955	val: 0.824513	test: 0.816288
PRC train: 0.883675	val: 0.470649	test: 0.463830

Epoch: 113
Loss: 0.10737986569575497
ROC train: 0.980461	val: 0.827259	test: 0.812089
PRC train: 0.873155	val: 0.448313	test: 0.452334

Epoch: 114
Loss: 0.10793688200915791
ROC train: 0.980443	val: 0.826504	test: 0.812422
PRC train: 0.871812	val: 0.451884	test: 0.446288

Epoch: 115
Loss: 0.10702670043698351
ROC train: 0.981727	val: 0.820946	test: 0.809740
PRC train: 0.878474	val: 0.448501	test: 0.456015

Epoch: 116
Loss: 0.10772576386201545
ROC train: 0.981796	val: 0.824407	test: 0.810415
PRC train: 0.879573	val: 0.443417	test: 0.455403

Epoch: 117
Loss: 0.10642122256675662
ROC train: 0.981825	val: 0.829614	test: 0.810443
PRC train: 0.876873	val: 0.444765	test: 0.462609

Epoch: 118
Loss: 0.10506800234647207
ROC train: 0.982833	val: 0.814838	test: 0.805670
PRC train: 0.884199	val: 0.430540	test: 0.459980

Epoch: 119
Loss: 0.10384981070161489
ROC train: 0.983194	val: 0.832666	test: 0.813682
PRC train: 0.886990	val: 0.442064	test: 0.460444

Epoch: 120
Loss: 0.10381024714320476
ROC train: 0.984881	val: 0.819234	test: 0.808181
PRC train: 0.892879	val: 0.435728	test: 0.459180

Early stopping
Best (ROC):	 train: 0.949168	val: 0.845274	test: 0.833164
Best (PRC):	 train: 0.733628	val: 0.469327	test: 0.447544

ROC train: 0.970768	val: 0.837711	test: 0.821910
PRC train: 0.822314	val: 0.460454	test: 0.466550

Epoch: 95
Loss: 0.12147751745771593
ROC train: 0.970410	val: 0.831014	test: 0.827645
PRC train: 0.823762	val: 0.451827	test: 0.459118

Epoch: 96
Loss: 0.11875400968345975
ROC train: 0.970407	val: 0.827554	test: 0.820638
PRC train: 0.825441	val: 0.442011	test: 0.451845

Epoch: 97
Loss: 0.11824243444346964
ROC train: 0.970562	val: 0.827372	test: 0.816084
PRC train: 0.823626	val: 0.442276	test: 0.454254

Epoch: 98
Loss: 0.12005684269839838
ROC train: 0.972573	val: 0.833361	test: 0.825464
PRC train: 0.835518	val: 0.459742	test: 0.463853

Epoch: 99
Loss: 0.11945210624788709
ROC train: 0.973053	val: 0.832491	test: 0.817454
PRC train: 0.838591	val: 0.447395	test: 0.449278

Epoch: 100
Loss: 0.11966847383286326
ROC train: 0.973329	val: 0.830618	test: 0.822426
PRC train: 0.832269	val: 0.441011	test: 0.466810

Epoch: 101
Loss: 0.11766936489309064
ROC train: 0.974798	val: 0.827026	test: 0.822595
PRC train: 0.849317	val: 0.450578	test: 0.457833

Epoch: 102
Loss: 0.11392362591251257
ROC train: 0.974592	val: 0.831411	test: 0.823323
PRC train: 0.847882	val: 0.457863	test: 0.460560

Epoch: 103
Loss: 0.11647517390981298
ROC train: 0.973581	val: 0.831384	test: 0.824508
PRC train: 0.837938	val: 0.462301	test: 0.448029

Epoch: 104
Loss: 0.11680246752311993
ROC train: 0.975162	val: 0.828156	test: 0.818586
PRC train: 0.848014	val: 0.443467	test: 0.451028

Epoch: 105
Loss: 0.11356411679665651
ROC train: 0.976235	val: 0.830092	test: 0.819565
PRC train: 0.852705	val: 0.472877	test: 0.455726

Epoch: 106
Loss: 0.11363536707308328
ROC train: 0.976714	val: 0.819802	test: 0.814122
PRC train: 0.851143	val: 0.442963	test: 0.455904

Epoch: 107
Loss: 0.11335466328371363
ROC train: 0.975299	val: 0.833001	test: 0.817318
PRC train: 0.846420	val: 0.459992	test: 0.459989

Epoch: 108
Loss: 0.11206765865907244
ROC train: 0.977621	val: 0.829722	test: 0.821813
PRC train: 0.863304	val: 0.466789	test: 0.444341

Epoch: 109
Loss: 0.10966248799144139
ROC train: 0.978165	val: 0.827928	test: 0.818927
PRC train: 0.864483	val: 0.467167	test: 0.458422

Epoch: 110
Loss: 0.11182609560283016
ROC train: 0.979980	val: 0.822706	test: 0.816791
PRC train: 0.872767	val: 0.456069	test: 0.444456

Epoch: 111
Loss: 0.11132130151991661
ROC train: 0.977017	val: 0.830312	test: 0.824001
PRC train: 0.854111	val: 0.441130	test: 0.460956

Epoch: 112
Loss: 0.11110334266562029
ROC train: 0.979383	val: 0.826775	test: 0.817333
PRC train: 0.868677	val: 0.455711	test: 0.442050

Epoch: 113
Loss: 0.10990497595235862
ROC train: 0.979545	val: 0.827500	test: 0.812027
PRC train: 0.870946	val: 0.455790	test: 0.448222

Epoch: 114
Loss: 0.10856373530984156
ROC train: 0.979288	val: 0.810908	test: 0.812391
PRC train: 0.860122	val: 0.425150	test: 0.438240

Epoch: 115
Loss: 0.109069103160706
ROC train: 0.980920	val: 0.819183	test: 0.814888
PRC train: 0.879073	val: 0.445771	test: 0.450402

Epoch: 116
Loss: 0.10726894051567845
ROC train: 0.981722	val: 0.824568	test: 0.812831
PRC train: 0.874016	val: 0.448040	test: 0.448415

Epoch: 117
Loss: 0.1081019714990635
ROC train: 0.982132	val: 0.821424	test: 0.816854
PRC train: 0.883181	val: 0.460017	test: 0.444175

Epoch: 118
Loss: 0.10614889119474062
ROC train: 0.981350	val: 0.824674	test: 0.814004
PRC train: 0.876609	val: 0.447007	test: 0.446416

Epoch: 119
Loss: 0.10602409800285782
ROC train: 0.983134	val: 0.815521	test: 0.808713
PRC train: 0.888587	val: 0.450574	test: 0.441006

Epoch: 120
Loss: 0.10556104499935146
ROC train: 0.983679	val: 0.820294	test: 0.813352
PRC train: 0.889215	val: 0.448245	test: 0.443842

Early stopping
Best (ROC):	 train: 0.956521	val: 0.841432	test: 0.828669
Best (PRC):	 train: 0.764193	val: 0.454427	test: 0.454869

ROC train: 0.972398	val: 0.825105	test: 0.829114
PRC train: 0.833186	val: 0.426839	test: 0.439981

Epoch: 95
Loss: 0.11934712850181536
ROC train: 0.970778	val: 0.837940	test: 0.825979
PRC train: 0.821804	val: 0.457013	test: 0.452502

Epoch: 96
Loss: 0.11871456295195242
ROC train: 0.974568	val: 0.832139	test: 0.828784
PRC train: 0.844938	val: 0.442805	test: 0.457284

Epoch: 97
Loss: 0.11871987970492476
ROC train: 0.974600	val: 0.836994	test: 0.823111
PRC train: 0.837526	val: 0.456697	test: 0.448065

Epoch: 98
Loss: 0.1159960620511678
ROC train: 0.975100	val: 0.824999	test: 0.828214
PRC train: 0.843034	val: 0.441999	test: 0.448704

Epoch: 99
Loss: 0.11678342735891514
ROC train: 0.975487	val: 0.832834	test: 0.829296
PRC train: 0.846180	val: 0.445132	test: 0.457947

Epoch: 100
Loss: 0.11576993234593652
ROC train: 0.975390	val: 0.829728	test: 0.830570
PRC train: 0.846446	val: 0.435349	test: 0.453822

Epoch: 101
Loss: 0.1152449331477141
ROC train: 0.977058	val: 0.828387	test: 0.822560
PRC train: 0.854850	val: 0.439074	test: 0.444722

Epoch: 102
Loss: 0.1150804179512749
ROC train: 0.976434	val: 0.825549	test: 0.823800
PRC train: 0.854385	val: 0.449212	test: 0.447709

Epoch: 103
Loss: 0.11250407039064338
ROC train: 0.977219	val: 0.831416	test: 0.827350
PRC train: 0.853268	val: 0.452235	test: 0.458973

Epoch: 104
Loss: 0.11524928469433728
ROC train: 0.978991	val: 0.824234	test: 0.819231
PRC train: 0.861027	val: 0.444606	test: 0.435365

Epoch: 105
Loss: 0.11193297958027125
ROC train: 0.978489	val: 0.835111	test: 0.819949
PRC train: 0.858235	val: 0.462873	test: 0.452571

Epoch: 106
Loss: 0.11005804343887632
ROC train: 0.979828	val: 0.829321	test: 0.825957
PRC train: 0.865586	val: 0.444578	test: 0.454862

Epoch: 107
Loss: 0.1123035324200614
ROC train: 0.980623	val: 0.832179	test: 0.826995
PRC train: 0.874900	val: 0.458258	test: 0.454035

Epoch: 108
Loss: 0.11078940014337632
ROC train: 0.980741	val: 0.829891	test: 0.822584
PRC train: 0.873582	val: 0.443904	test: 0.437948

Epoch: 109
Loss: 0.1098140469292806
ROC train: 0.980137	val: 0.826735	test: 0.823194
PRC train: 0.870130	val: 0.448198	test: 0.447610

Epoch: 110
Loss: 0.10978725939250861
ROC train: 0.979982	val: 0.825896	test: 0.825472
PRC train: 0.869004	val: 0.452727	test: 0.438993

Epoch: 111
Loss: 0.10926457948367342
ROC train: 0.980761	val: 0.830192	test: 0.820144
PRC train: 0.874646	val: 0.439072	test: 0.441416

Epoch: 112
Loss: 0.10832182462687588
ROC train: 0.981124	val: 0.830831	test: 0.822800
PRC train: 0.877515	val: 0.438114	test: 0.434173

Epoch: 113
Loss: 0.10774653518274815
ROC train: 0.979738	val: 0.836159	test: 0.821152
PRC train: 0.868321	val: 0.460894	test: 0.445153

Epoch: 114
Loss: 0.10799545069329324
ROC train: 0.980314	val: 0.829122	test: 0.821439
PRC train: 0.866913	val: 0.448787	test: 0.455274

Epoch: 115
Loss: 0.10434362605971087
ROC train: 0.982496	val: 0.830130	test: 0.823491
PRC train: 0.885451	val: 0.453586	test: 0.445481

Epoch: 116
Loss: 0.10125942049905884
ROC train: 0.983714	val: 0.826423	test: 0.822873
PRC train: 0.887331	val: 0.453059	test: 0.444292

Epoch: 117
Loss: 0.1050429492580326
ROC train: 0.983776	val: 0.828723	test: 0.825226
PRC train: 0.888463	val: 0.453506	test: 0.457608

Epoch: 118
Loss: 0.10413525298009653
ROC train: 0.982819	val: 0.825741	test: 0.822307
PRC train: 0.878674	val: 0.435211	test: 0.444168

Epoch: 119
Loss: 0.10281315538495846
ROC train: 0.984170	val: 0.827264	test: 0.814452
PRC train: 0.890811	val: 0.455518	test: 0.433448

Epoch: 120
Loss: 0.10440892443077497
ROC train: 0.983570	val: 0.831764	test: 0.817374
PRC train: 0.888278	val: 0.446232	test: 0.433376

Early stopping
Best (ROC):	 train: 0.954354	val: 0.843228	test: 0.825710
Best (PRC):	 train: 0.754898	val: 0.453043	test: 0.443825
All runs completed.

ROC train: 0.971053	val: 0.831917	test: 0.823699
PRC train: 0.827293	val: 0.494979	test: 0.452852

Epoch: 95
Loss: 0.12218514709608082
ROC train: 0.970618	val: 0.828204	test: 0.825854
PRC train: 0.828721	val: 0.493756	test: 0.459186

Epoch: 96
Loss: 0.12046016697228795
ROC train: 0.970258	val: 0.839421	test: 0.826428
PRC train: 0.824804	val: 0.498945	test: 0.442777

Epoch: 97
Loss: 0.11914499105022536
ROC train: 0.972649	val: 0.829538	test: 0.826050
PRC train: 0.835059	val: 0.494002	test: 0.443854

Epoch: 98
Loss: 0.11723465436550068
ROC train: 0.973960	val: 0.833435	test: 0.829809
PRC train: 0.843695	val: 0.498241	test: 0.451450

Epoch: 99
Loss: 0.11777652405169396
ROC train: 0.972498	val: 0.832922	test: 0.828074
PRC train: 0.834800	val: 0.495157	test: 0.453638

Epoch: 100
Loss: 0.11713247174208143
ROC train: 0.974466	val: 0.830700	test: 0.822078
PRC train: 0.845820	val: 0.498385	test: 0.442968

Epoch: 101
Loss: 0.1166452628092032
ROC train: 0.974433	val: 0.833480	test: 0.825790
PRC train: 0.844587	val: 0.500247	test: 0.462623

Epoch: 102
Loss: 0.11463370602350943
ROC train: 0.975391	val: 0.827853	test: 0.822801
PRC train: 0.849671	val: 0.502328	test: 0.453823

Epoch: 103
Loss: 0.11605950052764287
ROC train: 0.975592	val: 0.825388	test: 0.826929
PRC train: 0.855015	val: 0.486931	test: 0.460734

Epoch: 104
Loss: 0.11481825936560423
ROC train: 0.976794	val: 0.828727	test: 0.818674
PRC train: 0.857111	val: 0.483122	test: 0.442482

Epoch: 105
Loss: 0.11600605991274512
ROC train: 0.973957	val: 0.825709	test: 0.816374
PRC train: 0.840196	val: 0.478498	test: 0.439373

Epoch: 106
Loss: 0.11288684189361067
ROC train: 0.976166	val: 0.827799	test: 0.824792
PRC train: 0.852504	val: 0.480416	test: 0.449063

Epoch: 107
Loss: 0.11240648867758775
ROC train: 0.977027	val: 0.826555	test: 0.819444
PRC train: 0.858564	val: 0.487572	test: 0.431797

Epoch: 108
Loss: 0.11246773229202495
ROC train: 0.977824	val: 0.826799	test: 0.821555
PRC train: 0.862882	val: 0.481878	test: 0.441140

Epoch: 109
Loss: 0.11209988687160828
ROC train: 0.977587	val: 0.828971	test: 0.820330
PRC train: 0.860462	val: 0.472223	test: 0.430805

Epoch: 110
Loss: 0.11200059578520614
ROC train: 0.977726	val: 0.828847	test: 0.820171
PRC train: 0.861517	val: 0.484758	test: 0.444443

Epoch: 111
Loss: 0.11123233847121979
ROC train: 0.980076	val: 0.824537	test: 0.817815
PRC train: 0.873023	val: 0.479450	test: 0.433910

Epoch: 112
Loss: 0.11189199827997472
ROC train: 0.980392	val: 0.830216	test: 0.824135
PRC train: 0.876854	val: 0.483840	test: 0.448665

Epoch: 113
Loss: 0.10907770273997634
ROC train: 0.980995	val: 0.824657	test: 0.819871
PRC train: 0.879242	val: 0.488221	test: 0.445077

Epoch: 114
Loss: 0.10701328012510634
ROC train: 0.980040	val: 0.831398	test: 0.820884
PRC train: 0.873258	val: 0.489995	test: 0.437445

Epoch: 115
Loss: 0.10759257866103869
ROC train: 0.981318	val: 0.828048	test: 0.823007
PRC train: 0.883418	val: 0.489438	test: 0.452975

Epoch: 116
Loss: 0.10789294906250589
ROC train: 0.981422	val: 0.823547	test: 0.817286
PRC train: 0.881505	val: 0.484903	test: 0.455549

Epoch: 117
Loss: 0.10809807681500251
ROC train: 0.981431	val: 0.821109	test: 0.817947
PRC train: 0.880384	val: 0.487804	test: 0.453396

Epoch: 118
Loss: 0.10794819677153739
ROC train: 0.980875	val: 0.825932	test: 0.822587
PRC train: 0.874449	val: 0.479121	test: 0.445873

Epoch: 119
Loss: 0.10614634256232466
ROC train: 0.980686	val: 0.823826	test: 0.816821
PRC train: 0.878440	val: 0.478429	test: 0.446376

Epoch: 120
Loss: 0.1056652693118364
ROC train: 0.981710	val: 0.832599	test: 0.827295
PRC train: 0.883071	val: 0.497813	test: 0.433883

Early stopping
Best (ROC):	 train: 0.944256	val: 0.844173	test: 0.830376
Best (PRC):	 train: 0.725427	val: 0.506871	test: 0.435194

ROC train: 0.969816	val: 0.833913	test: 0.829190
PRC train: 0.826152	val: 0.510873	test: 0.436157

Epoch: 95
Loss: 0.12260325090888745
ROC train: 0.963232	val: 0.823906	test: 0.819455
PRC train: 0.796922	val: 0.482431	test: 0.408678

Epoch: 96
Loss: 0.12162853102863767
ROC train: 0.971553	val: 0.831950	test: 0.824106
PRC train: 0.832141	val: 0.500839	test: 0.449706

Epoch: 97
Loss: 0.12066285078678188
ROC train: 0.972440	val: 0.827792	test: 0.827647
PRC train: 0.840016	val: 0.509140	test: 0.421404

Epoch: 98
Loss: 0.11956894834173293
ROC train: 0.971317	val: 0.835356	test: 0.824456
PRC train: 0.836571	val: 0.500717	test: 0.434651

Epoch: 99
Loss: 0.12076316739158542
ROC train: 0.965907	val: 0.829448	test: 0.828091
PRC train: 0.807555	val: 0.489632	test: 0.420234

Epoch: 100
Loss: 0.11706502965960883
ROC train: 0.973700	val: 0.834835	test: 0.827069
PRC train: 0.846927	val: 0.514929	test: 0.437328

Epoch: 101
Loss: 0.11995166655043432
ROC train: 0.972639	val: 0.825639	test: 0.831743
PRC train: 0.845151	val: 0.487542	test: 0.446114

Epoch: 102
Loss: 0.11521921627780415
ROC train: 0.973817	val: 0.835517	test: 0.833314
PRC train: 0.847303	val: 0.510782	test: 0.455264

Epoch: 103
Loss: 0.11606326435474486
ROC train: 0.974647	val: 0.829812	test: 0.821172
PRC train: 0.847401	val: 0.497126	test: 0.417242

Epoch: 104
Loss: 0.11516291375068635
ROC train: 0.974693	val: 0.820075	test: 0.823375
PRC train: 0.847474	val: 0.486077	test: 0.438904

Epoch: 105
Loss: 0.11603649698856115
ROC train: 0.975371	val: 0.834520	test: 0.828881
PRC train: 0.849636	val: 0.495058	test: 0.442129

Epoch: 106
Loss: 0.11234642662196141
ROC train: 0.976819	val: 0.829156	test: 0.837343
PRC train: 0.861610	val: 0.501034	test: 0.440131

Epoch: 107
Loss: 0.1123981739206386
ROC train: 0.974418	val: 0.830647	test: 0.815588
PRC train: 0.846977	val: 0.496152	test: 0.437066

Epoch: 108
Loss: 0.11379979968781381
ROC train: 0.976700	val: 0.819448	test: 0.810758
PRC train: 0.853885	val: 0.486365	test: 0.416395

Epoch: 109
Loss: 0.11449623645138278
ROC train: 0.978394	val: 0.834596	test: 0.820464
PRC train: 0.866179	val: 0.501954	test: 0.435169

Epoch: 110
Loss: 0.11153797469504788
ROC train: 0.978505	val: 0.831425	test: 0.829920
PRC train: 0.869941	val: 0.507994	test: 0.449042

Epoch: 111
Loss: 0.10911987758816345
ROC train: 0.978903	val: 0.832198	test: 0.832093
PRC train: 0.871391	val: 0.504069	test: 0.425090

Epoch: 112
Loss: 0.11039701631154732
ROC train: 0.979616	val: 0.828643	test: 0.825085
PRC train: 0.873606	val: 0.503659	test: 0.429243

Epoch: 113
Loss: 0.10957633335712727
ROC train: 0.980654	val: 0.829690	test: 0.823809
PRC train: 0.882585	val: 0.505369	test: 0.429807

Epoch: 114
Loss: 0.11016240061873381
ROC train: 0.979590	val: 0.830321	test: 0.827728
PRC train: 0.875003	val: 0.496881	test: 0.434980

Epoch: 115
Loss: 0.10835760880281638
ROC train: 0.980849	val: 0.823651	test: 0.818894
PRC train: 0.883729	val: 0.481498	test: 0.440899

Epoch: 116
Loss: 0.10826151402136946
ROC train: 0.980012	val: 0.828250	test: 0.822473
PRC train: 0.874663	val: 0.482757	test: 0.429252

Epoch: 117
Loss: 0.10548309087109903
ROC train: 0.980273	val: 0.828241	test: 0.823541
PRC train: 0.874864	val: 0.488630	test: 0.435536

Epoch: 118
Loss: 0.10744303235125546
ROC train: 0.980352	val: 0.831574	test: 0.831793
PRC train: 0.879110	val: 0.489874	test: 0.436972

Epoch: 119
Loss: 0.10483694640450082
ROC train: 0.980901	val: 0.825599	test: 0.820747
PRC train: 0.877469	val: 0.494441	test: 0.431396

Epoch: 120
Loss: 0.10741794614484798
ROC train: 0.981432	val: 0.830751	test: 0.815534
PRC train: 0.883287	val: 0.498239	test: 0.428548

Early stopping
Best (ROC):	 train: 0.952037	val: 0.844568	test: 0.835614
Best (PRC):	 train: 0.759122	val: 0.522282	test: 0.452583

ROC train: 0.970217	val: 0.834587	test: 0.831913
PRC train: 0.830223	val: 0.502393	test: 0.432816

Epoch: 95
Loss: 0.11987951888264467
ROC train: 0.971488	val: 0.832351	test: 0.822683
PRC train: 0.831109	val: 0.487505	test: 0.441134

Epoch: 96
Loss: 0.11821629039020813
ROC train: 0.973308	val: 0.840282	test: 0.827787
PRC train: 0.843648	val: 0.499015	test: 0.431619

Epoch: 97
Loss: 0.11828934890581344
ROC train: 0.972715	val: 0.823943	test: 0.817380
PRC train: 0.837313	val: 0.474874	test: 0.409045

Epoch: 98
Loss: 0.12021472870436734
ROC train: 0.973438	val: 0.837466	test: 0.826183
PRC train: 0.846565	val: 0.506666	test: 0.443593

Epoch: 99
Loss: 0.11690808737424394
ROC train: 0.973485	val: 0.838111	test: 0.822823
PRC train: 0.838194	val: 0.504674	test: 0.439940

Epoch: 100
Loss: 0.11899016666547231
ROC train: 0.973438	val: 0.841516	test: 0.822618
PRC train: 0.840779	val: 0.495915	test: 0.431724

Epoch: 101
Loss: 0.11451616655337776
ROC train: 0.976018	val: 0.839013	test: 0.824413
PRC train: 0.858078	val: 0.519054	test: 0.437367

Epoch: 102
Loss: 0.11618558008602699
ROC train: 0.975459	val: 0.836144	test: 0.821290
PRC train: 0.852573	val: 0.500912	test: 0.435095

Epoch: 103
Loss: 0.11449460908437686
ROC train: 0.974703	val: 0.841712	test: 0.824791
PRC train: 0.849871	val: 0.501996	test: 0.428058

Epoch: 104
Loss: 0.11567608829496984
ROC train: 0.974681	val: 0.840599	test: 0.826095
PRC train: 0.847554	val: 0.493045	test: 0.438066

Epoch: 105
Loss: 0.11528314225085136
ROC train: 0.976592	val: 0.838642	test: 0.832171
PRC train: 0.860084	val: 0.501931	test: 0.439726

Epoch: 106
Loss: 0.11470554750701428
ROC train: 0.978350	val: 0.831020	test: 0.826027
PRC train: 0.868326	val: 0.497842	test: 0.437842

Epoch: 107
Loss: 0.1128289826973995
ROC train: 0.977067	val: 0.833540	test: 0.828104
PRC train: 0.864640	val: 0.498990	test: 0.427378

Epoch: 108
Loss: 0.112249713090564
ROC train: 0.977611	val: 0.826696	test: 0.818966
PRC train: 0.859420	val: 0.480245	test: 0.425291

Epoch: 109
Loss: 0.1102903302781647
ROC train: 0.977604	val: 0.839371	test: 0.824581
PRC train: 0.861578	val: 0.493407	test: 0.425917

Epoch: 110
Loss: 0.10977496520505417
ROC train: 0.978817	val: 0.837196	test: 0.835412
PRC train: 0.864163	val: 0.494267	test: 0.437801

Epoch: 111
Loss: 0.1087221779746096
ROC train: 0.979179	val: 0.834808	test: 0.823084
PRC train: 0.869043	val: 0.499877	test: 0.431881

Epoch: 112
Loss: 0.10977505964405199
ROC train: 0.979381	val: 0.836658	test: 0.825171
PRC train: 0.871623	val: 0.508595	test: 0.452499

Epoch: 113
Loss: 0.10927844648049799
ROC train: 0.977361	val: 0.834102	test: 0.824780
PRC train: 0.860896	val: 0.493027	test: 0.427979

Epoch: 114
Loss: 0.10809198249764718
ROC train: 0.981293	val: 0.834327	test: 0.820694
PRC train: 0.879024	val: 0.504465	test: 0.423410

Epoch: 115
Loss: 0.10607636315511382
ROC train: 0.982291	val: 0.830503	test: 0.825499
PRC train: 0.886141	val: 0.489261	test: 0.441446

Epoch: 116
Loss: 0.10447305991332759
ROC train: 0.981257	val: 0.834400	test: 0.819732
PRC train: 0.884118	val: 0.489640	test: 0.434527

Epoch: 117
Loss: 0.10546730079894026
ROC train: 0.982013	val: 0.825149	test: 0.818421
PRC train: 0.882568	val: 0.485119	test: 0.441553

Epoch: 118
Loss: 0.10602340600267564
ROC train: 0.981602	val: 0.833157	test: 0.813168
PRC train: 0.884570	val: 0.501479	test: 0.433956

Epoch: 119
Loss: 0.10535624521806713
ROC train: 0.982415	val: 0.828272	test: 0.822927
PRC train: 0.887498	val: 0.487191	test: 0.438928

Epoch: 120
Loss: 0.10482797365123214
ROC train: 0.981102	val: 0.832126	test: 0.824263
PRC train: 0.885538	val: 0.487830	test: 0.432833

Epoch: 121
Loss: 0.10626477073801194
ROC train: 0.983196	val: 0.830912	test: 0.825432
PRC train: 0.891357	val: 0.497965	test: 0.435617

Epoch: 122
Loss: 0.10438698424737343
ROC train: 0.984395	val: 0.831744	test: 0.824786
PRC train: 0.899936	val: 0.488563	test: 0.447180

Early stopping
Best (ROC):	 train: 0.966587	val: 0.846362	test: 0.829927
Best (PRC):	 train: 0.812307	val: 0.503709	test: 0.447372
All runs completed.
