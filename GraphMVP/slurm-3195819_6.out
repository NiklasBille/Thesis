>>> Starting run for dataset: lipo
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running SCAFF configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.6.yml on cuda:0
Running SCAFF configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.7.yml on cuda:1
Running SCAFF configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.8.yml on cuda:2
Starting process for seed 1: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.7.yml --runseed 1 --device cuda:1
Starting process for seed 2: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.7.yml --runseed 2 --device cuda:1
Starting process for seed 1: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.8.yml --runseed 1 --device cuda:2
Starting process for seed 3: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.7.yml --runseed 3 --device cuda:1
Starting process for seed 2: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.8.yml --runseed 2 --device cuda:2
Starting process for seed 3: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.8.yml --runseed 3 --device cuda:2
Starting process for seed 1: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.6.yml --runseed 1 --device cuda:0
Starting process for seed 2: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.6.yml --runseed 2 --device cuda:0
Starting process for seed 3: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.6.yml --runseed 3 --device cuda:0
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/lipo/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/lipo/scaff/train_prop=0.6/lipophilicity_scaff_3_20-05_14-43-15  ]
[ Using Seed :  3  ]
[ Using device :  cuda:0  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.331988286972046
RMSE train: 2.572645	val: 2.618837	test: 2.595855
MAE train: 2.335633	val: 2.363878	test: 2.346578

Epoch: 2
Loss: 3.862143611907959
RMSE train: 2.256505	val: 2.310479	test: 2.291326
MAE train: 2.024492	val: 2.063746	test: 2.050700

Epoch: 3
Loss: 2.731559491157532
RMSE train: 1.865670	val: 1.925926	test: 1.892471
MAE train: 1.651016	val: 1.687388	test: 1.660924

Epoch: 4
Loss: 1.9196642756462097
RMSE train: 1.390574	val: 1.480288	test: 1.462288
MAE train: 1.184857	val: 1.259063	test: 1.240608

Epoch: 5
Loss: 1.3099475026130676
RMSE train: 1.105982	val: 1.199373	test: 1.188122
MAE train: 0.918131	val: 0.996676	test: 0.983000

Epoch: 6
Loss: 0.9431305289268493
RMSE train: 0.874452	val: 1.015673	test: 0.980198
MAE train: 0.702036	val: 0.820864	test: 0.793560

Epoch: 7
Loss: 0.7958860397338867
RMSE train: 0.804863	val: 0.942932	test: 0.889404
MAE train: 0.628266	val: 0.742452	test: 0.708759

Epoch: 8
Loss: 0.760169368982315
RMSE train: 0.794407	val: 0.963215	test: 0.909507
MAE train: 0.618908	val: 0.750128	test: 0.723639

Epoch: 9
Loss: 0.7204231381416321
RMSE train: 0.755721	val: 0.924005	test: 0.875562
MAE train: 0.588277	val: 0.721371	test: 0.698369

Epoch: 10
Loss: 0.6910943984985352
RMSE train: 0.780538	val: 0.952301	test: 0.904363
MAE train: 0.607190	val: 0.739455	test: 0.722188

Epoch: 11
Loss: 0.700413566827774
RMSE train: 0.742387	val: 0.927215	test: 0.894908
MAE train: 0.583147	val: 0.728151	test: 0.718189

Epoch: 12
Loss: 0.6373320758342743
RMSE train: 0.742197	val: 0.911169	test: 0.873442
MAE train: 0.581068	val: 0.711151	test: 0.694588

Epoch: 13
Loss: 0.6393284976482392
RMSE train: 0.711571	val: 0.912608	test: 0.871287
MAE train: 0.554095	val: 0.709535	test: 0.695005

Epoch: 14
Loss: 0.6208556711673736
RMSE train: 0.733918	val: 0.924693	test: 0.877811
MAE train: 0.573906	val: 0.715370	test: 0.699682

Epoch: 15
Loss: 0.6201747626066207
RMSE train: 0.695783	val: 0.902844	test: 0.875916
MAE train: 0.546769	val: 0.707077	test: 0.691262

Epoch: 16
Loss: 0.5913595199584961
RMSE train: 0.698786	val: 0.907957	test: 0.871863
MAE train: 0.548599	val: 0.716658	test: 0.690783

Epoch: 17
Loss: 0.5663207292556762
RMSE train: 0.667212	val: 0.891353	test: 0.849613
MAE train: 0.520528	val: 0.701391	test: 0.670452

Epoch: 18
Loss: 0.5605267345905304
RMSE train: 0.675154	val: 0.891863	test: 0.841214
MAE train: 0.522933	val: 0.694593	test: 0.665823

Epoch: 19
Loss: 0.5782726228237152
RMSE train: 0.667434	val: 0.883706	test: 0.838196
MAE train: 0.521027	val: 0.693534	test: 0.660765

Epoch: 20
Loss: 0.5602756977081299
RMSE train: 0.663791	val: 0.896878	test: 0.865186
MAE train: 0.516443	val: 0.698325	test: 0.687324

Epoch: 21
Loss: 0.5371577441692352
RMSE train: 0.658019	val: 0.885282	test: 0.840688
MAE train: 0.511259	val: 0.691879	test: 0.666207

Epoch: 22
Loss: 0.5046232432126999
RMSE train: 0.648853	val: 0.880921	test: 0.859812
MAE train: 0.501381	val: 0.684401	test: 0.679450Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/lipo/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/lipo/scaff/train_prop=0.6/lipophilicity_scaff_2_20-05_14-43-15  ]
[ Using Seed :  2  ]
[ Using device :  cuda:0  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 4.95924859046936
RMSE train: 2.204718	val: 2.266251	test: 2.264343
MAE train: 1.968791	val: 2.014485	test: 2.014918

Epoch: 2
Loss: 3.579381537437439
RMSE train: 1.966479	val: 2.038096	test: 2.020479
MAE train: 1.726246	val: 1.786589	test: 1.764987

Epoch: 3
Loss: 2.5145836353302
RMSE train: 1.830697	val: 1.905137	test: 1.877556
MAE train: 1.619727	val: 1.665282	test: 1.643299

Epoch: 4
Loss: 1.7113474130630493
RMSE train: 1.488584	val: 1.581050	test: 1.564918
MAE train: 1.283119	val: 1.355222	test: 1.338167

Epoch: 5
Loss: 1.1763261556625366
RMSE train: 1.016987	val: 1.144721	test: 1.116899
MAE train: 0.836984	val: 0.944117	test: 0.927329

Epoch: 6
Loss: 0.8864812016487121
RMSE train: 0.909471	val: 1.063159	test: 1.017177
MAE train: 0.736049	val: 0.861685	test: 0.826613

Epoch: 7
Loss: 0.7910928785800934
RMSE train: 0.802255	val: 0.969658	test: 0.933756
MAE train: 0.626063	val: 0.753260	test: 0.743554

Epoch: 8
Loss: 0.7332555115222931
RMSE train: 0.747254	val: 0.938021	test: 0.882398
MAE train: 0.579492	val: 0.731876	test: 0.702522

Epoch: 9
Loss: 0.7483771085739136
RMSE train: 0.785178	val: 0.964586	test: 0.935330
MAE train: 0.617563	val: 0.747542	test: 0.750754

Epoch: 10
Loss: 0.7082436680793762
RMSE train: 0.723855	val: 0.921289	test: 0.875759
MAE train: 0.565120	val: 0.719618	test: 0.694484

Epoch: 11
Loss: 0.6501911222934723
RMSE train: 0.739392	val: 0.938393	test: 0.888753
MAE train: 0.576192	val: 0.732595	test: 0.708350

Epoch: 12
Loss: 0.6652213633060455
RMSE train: 0.698542	val: 0.896615	test: 0.849323
MAE train: 0.546943	val: 0.705083	test: 0.672786

Epoch: 13
Loss: 0.6694934725761413
RMSE train: 0.713987	val: 0.918329	test: 0.860031
MAE train: 0.556375	val: 0.714263	test: 0.682139

Epoch: 14
Loss: 0.6150133013725281
RMSE train: 0.703055	val: 0.903062	test: 0.866130
MAE train: 0.550699	val: 0.706905	test: 0.687525

Epoch: 15
Loss: 0.601858127117157
RMSE train: 0.676534	val: 0.907249	test: 0.854864
MAE train: 0.528958	val: 0.701868	test: 0.677941

Epoch: 16
Loss: 0.5823720455169678
RMSE train: 0.707345	val: 0.902558	test: 0.870284
MAE train: 0.552203	val: 0.692874	test: 0.686511

Epoch: 17
Loss: 0.5828243017196655
RMSE train: 0.675642	val: 0.899029	test: 0.855397
MAE train: 0.527486	val: 0.698239	test: 0.680406

Epoch: 18
Loss: 0.5605388104915618
RMSE train: 0.651958	val: 0.885189	test: 0.842680
MAE train: 0.511190	val: 0.685075	test: 0.668675

Epoch: 19
Loss: 0.5628779709339142
RMSE train: 0.664061	val: 0.900583	test: 0.857996
MAE train: 0.524914	val: 0.702290	test: 0.679317

Epoch: 20
Loss: 0.527672803401947
RMSE train: 0.656199	val: 0.894779	test: 0.858064
MAE train: 0.515920	val: 0.696060	test: 0.675210

Epoch: 21
Loss: 0.5328890889883041
RMSE train: 0.635679	val: 0.891986	test: 0.849377
MAE train: 0.494163	val: 0.688971	test: 0.669333

Epoch: 22
Loss: 0.5069068044424057
RMSE train: 0.637608	val: 0.881696	test: 0.840235
MAE train: 0.497995	val: 0.683044	test: 0.659672Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/lipo/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/lipo/scaff/train_prop=0.6/lipophilicity_scaff_1_20-05_14-43-15  ]
[ Using Seed :  1  ]
[ Using device :  cuda:0  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.120080947875977
RMSE train: 2.399547	val: 2.463587	test: 2.466978
MAE train: 2.155627	val: 2.204735	test: 2.210647

Epoch: 2
Loss: 3.6898688316345214
RMSE train: 2.009608	val: 2.063540	test: 2.043052
MAE train: 1.772225	val: 1.809425	test: 1.786672

Epoch: 3
Loss: 2.594132137298584
RMSE train: 1.795011	val: 1.868439	test: 1.851339
MAE train: 1.578418	val: 1.628924	test: 1.613993

Epoch: 4
Loss: 1.743308424949646
RMSE train: 1.300153	val: 1.375736	test: 1.358747
MAE train: 1.099804	val: 1.164037	test: 1.141540

Epoch: 5
Loss: 1.2054681777954102
RMSE train: 0.942941	val: 1.059636	test: 1.043829
MAE train: 0.775034	val: 0.872043	test: 0.854960

Epoch: 6
Loss: 0.9254516065120697
RMSE train: 0.838745	val: 0.981018	test: 0.956916
MAE train: 0.674101	val: 0.784603	test: 0.775609

Epoch: 7
Loss: 0.8100196659564972
RMSE train: 0.777573	val: 0.941147	test: 0.893341
MAE train: 0.618396	val: 0.745533	test: 0.722035

Epoch: 8
Loss: 0.7788284182548523
RMSE train: 0.760777	val: 0.934093	test: 0.883661
MAE train: 0.603324	val: 0.734618	test: 0.710171

Epoch: 9
Loss: 0.770147728919983
RMSE train: 0.772486	val: 0.940625	test: 0.904775
MAE train: 0.611334	val: 0.744790	test: 0.723026

Epoch: 10
Loss: 0.7013321995735169
RMSE train: 0.742824	val: 0.920367	test: 0.882137
MAE train: 0.582676	val: 0.723734	test: 0.707587

Epoch: 11
Loss: 0.68436678647995
RMSE train: 0.718893	val: 0.905415	test: 0.874555
MAE train: 0.564914	val: 0.712028	test: 0.698906

Epoch: 12
Loss: 0.66654332280159
RMSE train: 0.757894	val: 0.943887	test: 0.896153
MAE train: 0.607069	val: 0.755379	test: 0.721304

Epoch: 13
Loss: 0.6608351290225982
RMSE train: 0.744312	val: 0.927801	test: 0.906668
MAE train: 0.595341	val: 0.741263	test: 0.720210

Epoch: 14
Loss: 0.6403419375419617
RMSE train: 0.724651	val: 0.912613	test: 0.889630
MAE train: 0.576587	val: 0.722003	test: 0.704917

Epoch: 15
Loss: 0.6219066023826599
RMSE train: 0.691493	val: 0.905812	test: 0.866423
MAE train: 0.545693	val: 0.716514	test: 0.690293

Epoch: 16
Loss: 0.5989477068185807
RMSE train: 0.697597	val: 0.902717	test: 0.871451
MAE train: 0.550402	val: 0.707916	test: 0.692721

Epoch: 17
Loss: 0.596368032693863
RMSE train: 0.709660	val: 0.914154	test: 0.884836
MAE train: 0.567157	val: 0.727604	test: 0.702564

Epoch: 18
Loss: 0.5774243533611297
RMSE train: 0.717481	val: 0.907788	test: 0.874356
MAE train: 0.569814	val: 0.717123	test: 0.698104

Epoch: 19
Loss: 0.5683980762958527
RMSE train: 0.672837	val: 0.887081	test: 0.871706
MAE train: 0.530472	val: 0.699148	test: 0.684722

Epoch: 20
Loss: 0.5430615842342377
RMSE train: 0.654378	val: 0.883343	test: 0.856805
MAE train: 0.513844	val: 0.697997	test: 0.676068

Epoch: 21
Loss: 0.5165663063526154
RMSE train: 0.638561	val: 0.875963	test: 0.854162
MAE train: 0.504081	val: 0.690847	test: 0.674963

Epoch: 22
Loss: 0.5416034698486328
RMSE train: 0.643219	val: 0.872530	test: 0.858625
MAE train: 0.508935	val: 0.687136	test: 0.678843Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/lipo/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/lipo/scaff/train_prop=0.7/lipophilicity_scaff_2_20-05_14-43-15  ]
[ Using Seed :  2  ]
[ Using device :  cuda:1  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 4.949148019154866
RMSE train: 2.135588	val: 2.160579	test: 2.183007
MAE train: 1.893363	val: 1.913069	test: 1.923842

Epoch: 2
Loss: 3.323519984881083
RMSE train: 1.897333	val: 1.939055	test: 1.950030
MAE train: 1.668286	val: 1.696637	test: 1.698746

Epoch: 3
Loss: 2.1925466060638428
RMSE train: 1.649289	val: 1.693543	test: 1.695159
MAE train: 1.439447	val: 1.464263	test: 1.459982

Epoch: 4
Loss: 1.3781478703022003
RMSE train: 0.993793	val: 1.097004	test: 1.059371
MAE train: 0.815525	val: 0.887531	test: 0.880881

Epoch: 5
Loss: 0.9848445504903793
RMSE train: 0.872362	val: 1.009284	test: 0.954743
MAE train: 0.702430	val: 0.801329	test: 0.785446

Epoch: 6
Loss: 0.8307592670122782
RMSE train: 0.821920	val: 0.975955	test: 0.889163
MAE train: 0.639566	val: 0.753129	test: 0.716919

Epoch: 7
Loss: 0.8078716993331909
RMSE train: 0.801443	val: 0.960105	test: 0.867007
MAE train: 0.626634	val: 0.748331	test: 0.703407

Epoch: 8
Loss: 0.7578839063644409
RMSE train: 0.819511	val: 0.983853	test: 0.895708
MAE train: 0.636565	val: 0.755220	test: 0.722511

Epoch: 9
Loss: 0.7070980245868365
RMSE train: 0.755239	val: 0.932510	test: 0.852750
MAE train: 0.591134	val: 0.713478	test: 0.681757

Epoch: 10
Loss: 0.6918031622966131
RMSE train: 0.749416	val: 0.941396	test: 0.846916
MAE train: 0.583303	val: 0.725876	test: 0.690318

Epoch: 11
Loss: 0.6630296508471171
RMSE train: 0.742559	val: 0.924768	test: 0.853422
MAE train: 0.579919	val: 0.706954	test: 0.694065

Epoch: 12
Loss: 0.6251517608761787
RMSE train: 0.728010	val: 0.914540	test: 0.834672
MAE train: 0.566738	val: 0.694610	test: 0.671331

Epoch: 13
Loss: 0.6352160423994064
RMSE train: 0.697534	val: 0.891831	test: 0.816504
MAE train: 0.541525	val: 0.680653	test: 0.658219

Epoch: 14
Loss: 0.6306035468975703
RMSE train: 0.697805	val: 0.910351	test: 0.821476
MAE train: 0.542223	val: 0.691244	test: 0.663188

Epoch: 15
Loss: 0.5863975311319033
RMSE train: 0.695950	val: 0.906990	test: 0.829720
MAE train: 0.535347	val: 0.687579	test: 0.670138

Epoch: 16
Loss: 0.5920164734125137
RMSE train: 0.694586	val: 0.901448	test: 0.829013
MAE train: 0.542287	val: 0.683315	test: 0.664817

Epoch: 17
Loss: 0.5833064094185829
RMSE train: 0.693680	val: 0.894779	test: 0.817278
MAE train: 0.538197	val: 0.683381	test: 0.661866

Epoch: 18
Loss: 0.5611863931020101
RMSE train: 0.693066	val: 0.900502	test: 0.841373
MAE train: 0.540586	val: 0.685784	test: 0.685760

Epoch: 19
Loss: 0.574465237557888
RMSE train: 0.677053	val: 0.880798	test: 0.831472
MAE train: 0.530027	val: 0.671118	test: 0.671240

Epoch: 20
Loss: 0.5618434697389603
RMSE train: 0.678035	val: 0.892114	test: 0.817805
MAE train: 0.524563	val: 0.677795	test: 0.660114

Epoch: 21
Loss: 0.5502060006062189
RMSE train: 0.658666	val: 0.866970	test: 0.801642
MAE train: 0.515997	val: 0.662019	test: 0.645425

Epoch: 22
Loss: 0.559616374472777
RMSE train: 0.636942	val: 0.849910	test: 0.786538
MAE train: 0.492900	val: 0.642155	test: 0.629137Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/lipo/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/lipo/scaff/train_prop=0.7/lipophilicity_scaff_1_20-05_14-43-15  ]
[ Using Seed :  1  ]
[ Using device :  cuda:1  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.052663465340932
RMSE train: 2.220206	val: 2.227854	test: 2.274555
MAE train: 1.975134	val: 1.975055	test: 2.016994

Epoch: 2
Loss: 3.390886386235555
RMSE train: 1.847003	val: 1.870096	test: 1.889221
MAE train: 1.614135	val: 1.626777	test: 1.639530

Epoch: 3
Loss: 2.1770917773246765
RMSE train: 1.470193	val: 1.521378	test: 1.523584
MAE train: 1.263546	val: 1.306010	test: 1.297822

Epoch: 4
Loss: 1.3628111084302266
RMSE train: 1.097683	val: 1.179105	test: 1.158252
MAE train: 0.909693	val: 0.971546	test: 0.960929

Epoch: 5
Loss: 0.9923178454240164
RMSE train: 0.872630	val: 0.991616	test: 0.943607
MAE train: 0.700634	val: 0.787908	test: 0.765102

Epoch: 6
Loss: 0.8278696238994598
RMSE train: 0.819202	val: 0.941280	test: 0.865055
MAE train: 0.644917	val: 0.733202	test: 0.693497

Epoch: 7
Loss: 0.790983572602272
RMSE train: 0.790012	val: 0.940755	test: 0.875011
MAE train: 0.621147	val: 0.730190	test: 0.702327

Epoch: 8
Loss: 0.7645586133003235
RMSE train: 0.782543	val: 0.947994	test: 0.856537
MAE train: 0.611859	val: 0.731582	test: 0.696330

Epoch: 9
Loss: 0.7389552394549052
RMSE train: 0.771945	val: 0.925866	test: 0.848485
MAE train: 0.604175	val: 0.718819	test: 0.686628

Epoch: 10
Loss: 0.6913700501124064
RMSE train: 0.747453	val: 0.916950	test: 0.830840
MAE train: 0.588626	val: 0.710020	test: 0.668077

Epoch: 11
Loss: 0.6626272449890772
RMSE train: 0.746732	val: 0.922932	test: 0.832812
MAE train: 0.579370	val: 0.715191	test: 0.669001

Epoch: 12
Loss: 0.6515286862850189
RMSE train: 0.748514	val: 0.923954	test: 0.856843
MAE train: 0.580289	val: 0.716862	test: 0.691321

Epoch: 13
Loss: 0.6858648012081782
RMSE train: 0.738184	val: 0.939623	test: 0.858781
MAE train: 0.572307	val: 0.729377	test: 0.693440

Epoch: 14
Loss: 0.6263170590003332
RMSE train: 0.693939	val: 0.896793	test: 0.819247
MAE train: 0.539851	val: 0.693209	test: 0.656852

Epoch: 15
Loss: 0.6238154768943787
RMSE train: 0.733546	val: 0.923810	test: 0.872401
MAE train: 0.573416	val: 0.719992	test: 0.701752

Epoch: 16
Loss: 0.6291590680678686
RMSE train: 0.695847	val: 0.887609	test: 0.833374
MAE train: 0.549320	val: 0.691722	test: 0.665846

Epoch: 17
Loss: 0.6026498725016912
RMSE train: 0.683095	val: 0.880084	test: 0.810583
MAE train: 0.538179	val: 0.682872	test: 0.651359

Epoch: 18
Loss: 0.5921249041954676
RMSE train: 0.670104	val: 0.894034	test: 0.806084
MAE train: 0.523085	val: 0.684157	test: 0.648260

Epoch: 19
Loss: 0.5691305349270502
RMSE train: 0.696637	val: 0.890023	test: 0.824475
MAE train: 0.543529	val: 0.688945	test: 0.661931

Epoch: 20
Loss: 0.563728985687097
RMSE train: 0.669288	val: 0.890237	test: 0.814033
MAE train: 0.527643	val: 0.686679	test: 0.652712

Epoch: 21
Loss: 0.53926253815492
RMSE train: 0.657898	val: 0.876972	test: 0.809309
MAE train: 0.515656	val: 0.675558	test: 0.649322

Epoch: 22
Loss: 0.5529684200882912
RMSE train: 0.659754	val: 0.875562	test: 0.810829
MAE train: 0.518299	val: 0.668824	test: 0.649304Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/lipo/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/lipo/scaff/train_prop=0.7/lipophilicity_scaff_3_20-05_14-43-15  ]
[ Using Seed :  3  ]
[ Using device :  cuda:1  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.250796834627788
RMSE train: 2.420255	val: 2.436731	test: 2.467468
MAE train: 2.182908	val: 2.195685	test: 2.218902

Epoch: 2
Loss: 3.5590349038441977
RMSE train: 1.932731	val: 1.959095	test: 1.970536
MAE train: 1.705075	val: 1.727787	test: 1.722652

Epoch: 3
Loss: 2.374442825714747
RMSE train: 1.486411	val: 1.537112	test: 1.541729
MAE train: 1.270446	val: 1.313484	test: 1.313873

Epoch: 4
Loss: 1.484452615181605
RMSE train: 1.098050	val: 1.184069	test: 1.172001
MAE train: 0.905696	val: 0.973920	test: 0.975990

Epoch: 5
Loss: 1.0527057250340779
RMSE train: 0.879422	val: 0.989398	test: 0.945646
MAE train: 0.700435	val: 0.782367	test: 0.772204

Epoch: 6
Loss: 0.8627041081587473
RMSE train: 0.838516	val: 0.987098	test: 0.902902
MAE train: 0.649711	val: 0.761739	test: 0.732472

Epoch: 7
Loss: 0.793279692530632
RMSE train: 0.810744	val: 0.959776	test: 0.877048
MAE train: 0.628619	val: 0.744307	test: 0.714471

Epoch: 8
Loss: 0.7610223342974981
RMSE train: 0.837342	val: 0.988063	test: 0.900252
MAE train: 0.651238	val: 0.760020	test: 0.723617

Epoch: 9
Loss: 0.7295794238646826
RMSE train: 0.799781	val: 0.955609	test: 0.866833
MAE train: 0.621328	val: 0.737815	test: 0.698340

Epoch: 10
Loss: 0.6868465145428976
RMSE train: 0.755964	val: 0.923972	test: 0.847747
MAE train: 0.585795	val: 0.710329	test: 0.683606

Epoch: 11
Loss: 0.6978060652812322
RMSE train: 0.736650	val: 0.918068	test: 0.847846
MAE train: 0.573048	val: 0.705372	test: 0.682586

Epoch: 12
Loss: 0.6757268160581589
RMSE train: 0.718081	val: 0.906198	test: 0.825567
MAE train: 0.557383	val: 0.699660	test: 0.666614

Epoch: 13
Loss: 0.6570424685875574
RMSE train: 0.779795	val: 0.955854	test: 0.885928
MAE train: 0.608182	val: 0.733815	test: 0.718467

Epoch: 14
Loss: 0.6348149279753367
RMSE train: 0.704439	val: 0.900365	test: 0.822325
MAE train: 0.548925	val: 0.689254	test: 0.653986

Epoch: 15
Loss: 0.6116073876619339
RMSE train: 0.713874	val: 0.910355	test: 0.825204
MAE train: 0.555249	val: 0.695681	test: 0.664815

Epoch: 16
Loss: 0.6077046742041906
RMSE train: 0.699408	val: 0.896114	test: 0.813995
MAE train: 0.547713	val: 0.686538	test: 0.654188

Epoch: 17
Loss: 0.5975945393244425
RMSE train: 0.746198	val: 0.935853	test: 0.857771
MAE train: 0.579809	val: 0.716054	test: 0.687616

Epoch: 18
Loss: 0.5938244412342707
RMSE train: 0.683651	val: 0.887971	test: 0.811526
MAE train: 0.530338	val: 0.676758	test: 0.646941

Epoch: 19
Loss: 0.5753099570671717
RMSE train: 0.735068	val: 0.938865	test: 0.866490
MAE train: 0.570681	val: 0.719400	test: 0.702211

Epoch: 20
Loss: 0.5692133332292239
RMSE train: 0.710011	val: 0.930109	test: 0.834741
MAE train: 0.551135	val: 0.706938	test: 0.673799

Epoch: 21
Loss: 0.5535141800840696
RMSE train: 0.718481	val: 0.904028	test: 0.819651
MAE train: 0.551653	val: 0.685953	test: 0.654911

Epoch: 22
Loss: 0.5676614542802175
RMSE train: 0.688184	val: 0.909587	test: 0.838130
MAE train: 0.535980	val: 0.686477	test: 0.675466Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/lipo/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/lipo/scaff/train_prop=0.8/lipophilicity_scaff_1_20-05_14-43-15  ]
[ Using Seed :  1  ]
[ Using device :  cuda:2  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.03231498173305
RMSE train: 2.099776	val: 2.086841	test: 2.175180
MAE train: 1.849227	val: 1.805158	test: 1.924739

Epoch: 2
Loss: 3.116341437612261
RMSE train: 1.778798	val: 1.782927	test: 1.865787
MAE train: 1.552953	val: 1.525190	test: 1.633187

Epoch: 3
Loss: 1.8705382602555412
RMSE train: 1.286573	val: 1.289502	test: 1.310145
MAE train: 1.081255	val: 1.074881	test: 1.105839

Epoch: 4
Loss: 1.1765534664903368
RMSE train: 0.896707	val: 0.985280	test: 0.929500
MAE train: 0.715150	val: 0.790962	test: 0.743348

Epoch: 5
Loss: 0.8959811542715345
RMSE train: 0.850383	val: 0.933906	test: 0.867331
MAE train: 0.668328	val: 0.752386	test: 0.694233

Epoch: 6
Loss: 0.8386617856366294
RMSE train: 0.956307	val: 1.024466	test: 0.936409
MAE train: 0.748371	val: 0.815510	test: 0.754361

Epoch: 7
Loss: 0.8491974898747036
RMSE train: 0.838036	val: 0.918671	test: 0.843906
MAE train: 0.655287	val: 0.734969	test: 0.678506

Epoch: 8
Loss: 0.8105361887386867
RMSE train: 0.805101	val: 0.895221	test: 0.838777
MAE train: 0.627877	val: 0.706382	test: 0.672722

Epoch: 9
Loss: 0.747055355991636
RMSE train: 0.845795	val: 0.950522	test: 0.869098
MAE train: 0.660819	val: 0.760436	test: 0.702612

Epoch: 10
Loss: 0.7535380252770015
RMSE train: 0.772408	val: 0.876632	test: 0.810675
MAE train: 0.606880	val: 0.694587	test: 0.647782

Epoch: 11
Loss: 0.6946067299161639
RMSE train: 0.826162	val: 0.927604	test: 0.861280
MAE train: 0.646670	val: 0.735449	test: 0.691669

Epoch: 12
Loss: 0.7251993886062077
RMSE train: 0.761475	val: 0.861375	test: 0.813640
MAE train: 0.593851	val: 0.680954	test: 0.651121

Epoch: 13
Loss: 0.6815939545631409
RMSE train: 0.739570	val: 0.865471	test: 0.803530
MAE train: 0.582055	val: 0.677709	test: 0.642917

Epoch: 14
Loss: 0.6553196438721248
RMSE train: 0.726742	val: 0.848543	test: 0.801103
MAE train: 0.565035	val: 0.666168	test: 0.646982

Epoch: 15
Loss: 0.6693842240742275
RMSE train: 0.739311	val: 0.878947	test: 0.818180
MAE train: 0.577599	val: 0.692223	test: 0.659402

Epoch: 16
Loss: 0.6412961568151202
RMSE train: 0.757955	val: 0.893699	test: 0.832704
MAE train: 0.591693	val: 0.700149	test: 0.673606

Epoch: 17
Loss: 0.6286021854196276
RMSE train: 0.711662	val: 0.849649	test: 0.798637
MAE train: 0.554976	val: 0.667453	test: 0.634783

Epoch: 18
Loss: 0.6216799978699002
RMSE train: 0.745355	val: 0.877217	test: 0.818728
MAE train: 0.583018	val: 0.682199	test: 0.665359

Epoch: 19
Loss: 0.6206862500735691
RMSE train: 0.670865	val: 0.818355	test: 0.782379
MAE train: 0.519955	val: 0.629017	test: 0.617612

Epoch: 20
Loss: 0.6055569840329034
RMSE train: 0.724630	val: 0.877295	test: 0.827995
MAE train: 0.564754	val: 0.689635	test: 0.668561

Epoch: 21
Loss: 0.5945478975772858
RMSE train: 0.671664	val: 0.815444	test: 0.794416
MAE train: 0.524804	val: 0.633578	test: 0.627674

Epoch: 22
Loss: 0.5882596118109567
RMSE train: 0.721690	val: 0.857438	test: 0.804914
MAE train: 0.564557	val: 0.673860	test: 0.648243Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/lipo/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/lipo/scaff/train_prop=0.8/lipophilicity_scaff_3_20-05_14-43-15  ]
[ Using Seed :  3  ]
[ Using device :  cuda:2  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 5.188265289579119
RMSE train: 2.418213	val: 2.365260	test: 2.474815
MAE train: 2.188282	val: 2.111732	test: 2.244365

Epoch: 2
Loss: 3.230360380240849
RMSE train: 1.906765	val: 1.865507	test: 1.967423
MAE train: 1.684441	val: 1.629273	test: 1.734022

Epoch: 3
Loss: 2.010392044271742
RMSE train: 1.308543	val: 1.338793	test: 1.369609
MAE train: 1.101720	val: 1.116118	test: 1.154287

Epoch: 4
Loss: 1.2616893010480064
RMSE train: 1.150309	val: 1.176959	test: 1.206470
MAE train: 0.966179	val: 0.977313	test: 1.020039

Epoch: 5
Loss: 0.9812290966510773
RMSE train: 0.867065	val: 0.966767	test: 0.922625
MAE train: 0.689609	val: 0.765246	test: 0.746489

Epoch: 6
Loss: 0.8336960843631199
RMSE train: 0.840543	val: 0.942480	test: 0.859681
MAE train: 0.649189	val: 0.756240	test: 0.694308

Epoch: 7
Loss: 0.8464122286864689
RMSE train: 0.823523	val: 0.946181	test: 0.880016
MAE train: 0.640657	val: 0.754331	test: 0.710003

Epoch: 8
Loss: 0.7697262338229588
RMSE train: 0.850840	val: 0.960664	test: 0.874786
MAE train: 0.657785	val: 0.768647	test: 0.704195

Epoch: 9
Loss: 0.7396248217139926
RMSE train: 0.786361	val: 0.907616	test: 0.859662
MAE train: 0.616395	val: 0.724109	test: 0.691700

Epoch: 10
Loss: 0.7135269216128758
RMSE train: 0.802446	val: 0.935264	test: 0.874273
MAE train: 0.622533	val: 0.740865	test: 0.702701

Epoch: 11
Loss: 0.68072195989745
RMSE train: 0.751862	val: 0.887235	test: 0.846793
MAE train: 0.592408	val: 0.707739	test: 0.678943

Epoch: 12
Loss: 0.697271500314985
RMSE train: 0.777768	val: 0.911572	test: 0.849510
MAE train: 0.608192	val: 0.729144	test: 0.682770

Epoch: 13
Loss: 0.7081634189401355
RMSE train: 0.759729	val: 0.932701	test: 0.851663
MAE train: 0.591659	val: 0.739322	test: 0.688653

Epoch: 14
Loss: 0.6780403128692082
RMSE train: 0.729221	val: 0.887189	test: 0.817155
MAE train: 0.571515	val: 0.702118	test: 0.659807

Epoch: 15
Loss: 0.6630868784018925
RMSE train: 0.717204	val: 0.883309	test: 0.825094
MAE train: 0.560749	val: 0.700514	test: 0.663228

Epoch: 16
Loss: 0.6441349344594138
RMSE train: 0.736348	val: 0.884464	test: 0.827745
MAE train: 0.577554	val: 0.692853	test: 0.673404

Epoch: 17
Loss: 0.6181711001055581
RMSE train: 0.745007	val: 0.907064	test: 0.844089
MAE train: 0.576769	val: 0.719290	test: 0.684105

Epoch: 18
Loss: 0.6409177035093307
RMSE train: 0.717398	val: 0.880928	test: 0.838656
MAE train: 0.558374	val: 0.699738	test: 0.672208

Epoch: 19
Loss: 0.5968878652368274
RMSE train: 0.771709	val: 0.891081	test: 0.841948
MAE train: 0.604157	val: 0.702157	test: 0.685576

Epoch: 20
Loss: 0.6152947757925306
RMSE train: 0.689579	val: 0.855116	test: 0.819002
MAE train: 0.538527	val: 0.667427	test: 0.659859

Epoch: 21
Loss: 0.6031950903790337
RMSE train: 0.686592	val: 0.869239	test: 0.819228
MAE train: 0.534671	val: 0.677025	test: 0.658725

Epoch: 22
Loss: 0.5749296588557107
RMSE train: 0.697684	val: 0.853748	test: 0.815361
MAE train: 0.546065	val: 0.673647	test: 0.657239Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/lipo/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: lipophilicity
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/lipo/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/lipo/scaff/train_prop=0.8/lipophilicity_scaff_2_20-05_14-43-15  ]
[ Using Seed :  2  ]
[ Using device :  cuda:2  ]
Dataset: lipophilicity
Data: Data(edge_attr=[247798, 3], edge_index=[2, 247798], id=[4200], x=[113568, 9], y=[4200])
dataset_folder: ../datasets/molecule_datasets/lipophilicity
MoleculeDatasetComplete(4200)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 4.744161946432931
RMSE train: 2.049001	val: 2.043035	test: 2.149569
MAE train: 1.804683	val: 1.776899	test: 1.909364

Epoch: 2
Loss: 3.0444063459123885
RMSE train: 1.876877	val: 1.866623	test: 1.967853
MAE train: 1.647457	val: 1.618922	test: 1.743408

Epoch: 3
Loss: 1.907634164605822
RMSE train: 1.311344	val: 1.341705	test: 1.378028
MAE train: 1.104028	val: 1.128437	test: 1.167193

Epoch: 4
Loss: 1.1858138952936446
RMSE train: 1.074357	val: 1.139713	test: 1.136921
MAE train: 0.885041	val: 0.940480	test: 0.946963

Epoch: 5
Loss: 0.9132293952362878
RMSE train: 0.869852	val: 0.973035	test: 0.887337
MAE train: 0.688291	val: 0.777569	test: 0.708680

Epoch: 6
Loss: 0.8686694375106266
RMSE train: 0.862805	val: 0.958073	test: 0.865368
MAE train: 0.674376	val: 0.756701	test: 0.687875

Epoch: 7
Loss: 0.8256212856088366
RMSE train: 0.850429	val: 0.958116	test: 0.868159
MAE train: 0.661982	val: 0.759468	test: 0.689631

Epoch: 8
Loss: 0.7915201442582267
RMSE train: 0.812792	val: 0.935766	test: 0.840884
MAE train: 0.634284	val: 0.743706	test: 0.682617

Epoch: 9
Loss: 0.7562703107084546
RMSE train: 0.834113	val: 0.925406	test: 0.841029
MAE train: 0.646384	val: 0.738545	test: 0.679663

Epoch: 10
Loss: 0.7301038163048881
RMSE train: 0.771675	val: 0.897888	test: 0.829067
MAE train: 0.601544	val: 0.703234	test: 0.666842

Epoch: 11
Loss: 0.7106562725135258
RMSE train: 0.793559	val: 0.912418	test: 0.834388
MAE train: 0.618047	val: 0.727968	test: 0.671444

Epoch: 12
Loss: 0.7207929151398795
RMSE train: 0.760638	val: 0.904311	test: 0.831921
MAE train: 0.594349	val: 0.714355	test: 0.664294

Epoch: 13
Loss: 0.6833036712237767
RMSE train: 0.784032	val: 0.901711	test: 0.813648
MAE train: 0.607655	val: 0.707944	test: 0.658079

Epoch: 14
Loss: 0.6608114668301174
RMSE train: 0.735986	val: 0.886420	test: 0.811892
MAE train: 0.575002	val: 0.698259	test: 0.652950

Epoch: 15
Loss: 0.630874902009964
RMSE train: 0.761712	val: 0.881087	test: 0.811471
MAE train: 0.590026	val: 0.692281	test: 0.648608

Epoch: 16
Loss: 0.6227905622550419
RMSE train: 0.762615	val: 0.925889	test: 0.838462
MAE train: 0.596960	val: 0.731974	test: 0.680170

Epoch: 17
Loss: 0.6107752280575889
RMSE train: 0.702197	val: 0.845360	test: 0.788218
MAE train: 0.541194	val: 0.662814	test: 0.632633

Epoch: 18
Loss: 0.5993820301124028
RMSE train: 0.729612	val: 0.888313	test: 0.812916
MAE train: 0.569101	val: 0.696641	test: 0.658263

Epoch: 19
Loss: 0.5700773916074208
RMSE train: 0.705010	val: 0.878434	test: 0.814611
MAE train: 0.547359	val: 0.682619	test: 0.664989

Epoch: 20
Loss: 0.6261833735874721
RMSE train: 0.714250	val: 0.879216	test: 0.803417
MAE train: 0.560287	val: 0.683327	test: 0.652251

Epoch: 21
Loss: 0.5696771272591182
RMSE train: 0.671885	val: 0.835036	test: 0.773427
MAE train: 0.524341	val: 0.644623	test: 0.622833

Epoch: 22
Loss: 0.5649856158665248
RMSE train: 0.688201	val: 0.860950	test: 0.799177
MAE train: 0.536253	val: 0.674037	test: 0.639775

Epoch: 23
Loss: 0.5230136483907699
RMSE train: 0.653060	val: 0.881809	test: 0.844435
MAE train: 0.509715	val: 0.690955	test: 0.669070

Epoch: 24
Loss: 0.49930538833141325
RMSE train: 0.630048	val: 0.872581	test: 0.839202
MAE train: 0.493918	val: 0.685445	test: 0.663204

Epoch: 25
Loss: 0.4861282020807266
RMSE train: 0.656353	val: 0.894036	test: 0.855966
MAE train: 0.515215	val: 0.699030	test: 0.682997

Epoch: 26
Loss: 0.5008992791175843
RMSE train: 0.621032	val: 0.871711	test: 0.845752
MAE train: 0.485737	val: 0.678708	test: 0.664205

Epoch: 27
Loss: 0.5062132865190506
RMSE train: 0.672607	val: 0.908700	test: 0.875361
MAE train: 0.523705	val: 0.702326	test: 0.690787

Epoch: 28
Loss: 0.47808317840099335
RMSE train: 0.602887	val: 0.877662	test: 0.849966
MAE train: 0.467654	val: 0.679485	test: 0.662297

Epoch: 29
Loss: 0.48644045889377596
RMSE train: 0.607077	val: 0.868375	test: 0.827025
MAE train: 0.468826	val: 0.668020	test: 0.652196

Epoch: 30
Loss: 0.4672247588634491
RMSE train: 0.650276	val: 0.905138	test: 0.868997
MAE train: 0.503267	val: 0.694086	test: 0.685750

Epoch: 31
Loss: 0.46084013283252717
RMSE train: 0.620057	val: 0.886757	test: 0.851205
MAE train: 0.481458	val: 0.681664	test: 0.671212

Epoch: 32
Loss: 0.4393700689077377
RMSE train: 0.605626	val: 0.867362	test: 0.830303
MAE train: 0.469814	val: 0.667420	test: 0.647318

Epoch: 33
Loss: 0.4620892763137817
RMSE train: 0.581138	val: 0.874290	test: 0.839331
MAE train: 0.449102	val: 0.680323	test: 0.656276

Epoch: 34
Loss: 0.4483547508716583
RMSE train: 0.589889	val: 0.858560	test: 0.830313
MAE train: 0.460296	val: 0.673541	test: 0.651551

Epoch: 35
Loss: 0.43403632640838624
RMSE train: 0.611675	val: 0.869570	test: 0.832656
MAE train: 0.480909	val: 0.680725	test: 0.655988

Epoch: 36
Loss: 0.4305650144815445
RMSE train: 0.580660	val: 0.868021	test: 0.832949
MAE train: 0.451910	val: 0.670828	test: 0.659450

Epoch: 37
Loss: 0.4307353913784027
RMSE train: 0.594806	val: 0.861696	test: 0.841911
MAE train: 0.460668	val: 0.667965	test: 0.660063

Epoch: 38
Loss: 0.4268174171447754
RMSE train: 0.605448	val: 0.875229	test: 0.842133
MAE train: 0.471846	val: 0.672566	test: 0.665403

Epoch: 39
Loss: 0.42529167234897614
RMSE train: 0.587555	val: 0.857275	test: 0.835939
MAE train: 0.452094	val: 0.657882	test: 0.655474

Epoch: 40
Loss: 0.42465846836566923
RMSE train: 0.579389	val: 0.868058	test: 0.846575
MAE train: 0.447971	val: 0.669832	test: 0.662474

Epoch: 41
Loss: 0.4298635572195053
RMSE train: 0.567764	val: 0.850953	test: 0.826829
MAE train: 0.440841	val: 0.660101	test: 0.650329

Epoch: 42
Loss: 0.42055035531520846
RMSE train: 0.572266	val: 0.874764	test: 0.847801
MAE train: 0.439235	val: 0.671101	test: 0.665491

Epoch: 43
Loss: 0.41400821805000304
RMSE train: 0.569287	val: 0.841493	test: 0.819723
MAE train: 0.443641	val: 0.647133	test: 0.643142

Epoch: 44
Loss: 0.38645835518836974
RMSE train: 0.567780	val: 0.862712	test: 0.841980
MAE train: 0.440026	val: 0.665548	test: 0.656125

Epoch: 45
Loss: 0.36795651316642763
RMSE train: 0.550981	val: 0.840167	test: 0.826284
MAE train: 0.428850	val: 0.647402	test: 0.643411

Epoch: 46
Loss: 0.3903494834899902
RMSE train: 0.564936	val: 0.855351	test: 0.825808
MAE train: 0.437424	val: 0.657377	test: 0.648041

Epoch: 47
Loss: 0.38232489824295046
RMSE train: 0.562400	val: 0.866395	test: 0.838529
MAE train: 0.435035	val: 0.670506	test: 0.654170

Epoch: 48
Loss: 0.38441343903541564
RMSE train: 0.561496	val: 0.863654	test: 0.850612
MAE train: 0.434023	val: 0.669954	test: 0.662572

Epoch: 49
Loss: 0.3838809937238693
RMSE train: 0.576390	val: 0.872050	test: 0.860330
MAE train: 0.445696	val: 0.671790	test: 0.667947

Epoch: 50
Loss: 0.3818418323993683
RMSE train: 0.530521	val: 0.841186	test: 0.817471
MAE train: 0.409603	val: 0.644230	test: 0.637983

Epoch: 51
Loss: 0.37680964469909667
RMSE train: 0.562280	val: 0.868291	test: 0.844364
MAE train: 0.435391	val: 0.663733	test: 0.657251

Epoch: 52
Loss: 0.37565348744392396
RMSE train: 0.578596	val: 0.870740	test: 0.847627
MAE train: 0.450789	val: 0.670038	test: 0.662399

Epoch: 53
Loss: 0.36311717331409454
RMSE train: 0.536713	val: 0.847106	test: 0.827478
MAE train: 0.417876	val: 0.650042	test: 0.648701

Epoch: 54
Loss: 0.36180796325206754
RMSE train: 0.535751	val: 0.854868	test: 0.836770
MAE train: 0.414463	val: 0.650219	test: 0.650953

Epoch: 55
Loss: 0.3491808533668518
RMSE train: 0.561230	val: 0.877547	test: 0.859396
MAE train: 0.435223	val: 0.673106	test: 0.670928

Epoch: 56
Loss: 0.36594362258911134
RMSE train: 0.544985	val: 0.865950	test: 0.852463
MAE train: 0.421449	val: 0.662892	test: 0.666201

Epoch: 57
Loss: 0.3587892144918442
RMSE train: 0.527412	val: 0.850172	test: 0.836260
MAE train: 0.408860	val: 0.649995	test: 0.653363

Epoch: 58
Loss: 0.3476901650428772
RMSE train: 0.537641	val: 0.859287	test: 0.840244
MAE train: 0.415697	val: 0.658176	test: 0.654609

Epoch: 59
Loss: 0.3599854975938797
RMSE train: 0.530484	val: 0.841395	test: 0.836155
MAE train: 0.412440	val: 0.649920	test: 0.652532

Epoch: 60
Loss: 0.3467976599931717
RMSE train: 0.534599	val: 0.848584	test: 0.834552
MAE train: 0.412435	val: 0.652505	test: 0.652843

Epoch: 61
Loss: 0.3345037043094635
RMSE train: 0.512707	val: 0.848732	test: 0.845176
MAE train: 0.396193	val: 0.651967	test: 0.659027

Epoch: 62
Loss: 0.32606315314769746
RMSE train: 0.503369	val: 0.821208	test: 0.810486
MAE train: 0.389578	val: 0.628052	test: 0.629080

Epoch: 63
Loss: 0.32813418209552764
RMSE train: 0.535932	val: 0.853225	test: 0.847784
MAE train: 0.417184	val: 0.654452	test: 0.660951

Epoch: 64
Loss: 0.3223431706428528
RMSE train: 0.522847	val: 0.852828	test: 0.834465
MAE train: 0.405453	val: 0.653771	test: 0.653603

Epoch: 65
Loss: 0.33827440440654755
RMSE train: 0.538348	val: 0.849557	test: 0.839682
MAE train: 0.418790	val: 0.651883	test: 0.654781

Epoch: 66
Loss: 0.33224405348300934
RMSE train: 0.501835	val: 0.840231	test: 0.824704
MAE train: 0.391805	val: 0.645960	test: 0.642114

Epoch: 67
Loss: 0.33581864535808564
RMSE train: 0.513310	val: 0.841519	test: 0.833021
MAE train: 0.398122	val: 0.646582	test: 0.652810

Epoch: 68
Loss: 0.31475943624973296
RMSE train: 0.521129	val: 0.854164	test: 0.838012
MAE train: 0.406960	val: 0.655091	test: 0.656057

Epoch: 69
Loss: 0.31838861405849456
RMSE train: 0.499327	val: 0.850103	test: 0.832823
MAE train: 0.387300	val: 0.646694	test: 0.646497

Epoch: 70
Loss: 0.3136663347482681
RMSE train: 0.514057	val: 0.846848	test: 0.828565
MAE train: 0.396946	val: 0.649570	test: 0.643698

Epoch: 71
Loss: 0.3152871072292328
RMSE train: 0.512167	val: 0.846057	test: 0.831052
MAE train: 0.394005	val: 0.641828	test: 0.648612

Epoch: 72
Loss: 0.3138800263404846
RMSE train: 0.512323	val: 0.850415	test: 0.837662
MAE train: 0.395989	val: 0.650174	test: 0.650245

Epoch: 73
Loss: 0.3111685961484909
RMSE train: 0.525700	val: 0.840549	test: 0.818674
MAE train: 0.407736	val: 0.642139	test: 0.637049

Epoch: 74
Loss: 0.30957527458667755
RMSE train: 0.497193	val: 0.836544	test: 0.829662
MAE train: 0.385630	val: 0.641131	test: 0.643273

Epoch: 75
Loss: 0.2973886951804161
RMSE train: 0.484233	val: 0.829205	test: 0.811820
MAE train: 0.377550	val: 0.635519	test: 0.627735

Epoch: 76
Loss: 0.29195544868707657
RMSE train: 0.508634	val: 0.862712	test: 0.839593
MAE train: 0.394824	val: 0.659464	test: 0.654462

Epoch: 77
Loss: 0.2941602811217308
RMSE train: 0.528823	val: 0.861341	test: 0.842660
MAE train: 0.409136	val: 0.662238	test: 0.654598

Epoch: 78
Loss: 0.2973552763462067
RMSE train: 0.513156	val: 0.862857	test: 0.849526
MAE train: 0.398517	val: 0.658232	test: 0.661185

Epoch: 79
Loss: 0.2850185036659241
RMSE train: 0.543678	val: 0.868092	test: 0.859735
MAE train: 0.425116	val: 0.665335	test: 0.676380

Epoch: 80
Loss: 0.288901598751545
RMSE train: 0.537050	val: 0.862617	test: 0.863482
MAE train: 0.417007	val: 0.663805	test: 0.670338

Epoch: 81
Loss: 0.29054324626922606
RMSE train: 0.486389	val: 0.830707	test: 0.824520
MAE train: 0.379960	val: 0.637971	test: 0.639394

Epoch: 82
Loss: 0.2806075602769852
RMSE train: 0.483865	val: 0.816646	test: 0.812480
MAE train: 0.378348	val: 0.623911	test: 0.631612

Epoch: 83
Loss: 0.2850951224565506
RMSE train: 0.499069	val: 0.843458	test: 0.825560

Epoch: 23
Loss: 0.5133966565132141
RMSE train: 0.625994	val: 0.873167	test: 0.843585
MAE train: 0.490508	val: 0.679242	test: 0.663719

Epoch: 24
Loss: 0.5038625001907349
RMSE train: 0.632134	val: 0.876742	test: 0.837203
MAE train: 0.493039	val: 0.676821	test: 0.656102

Epoch: 25
Loss: 0.49880461394786835
RMSE train: 0.636086	val: 0.875536	test: 0.826936
MAE train: 0.499737	val: 0.674122	test: 0.649078

Epoch: 26
Loss: 0.49186077117919924
RMSE train: 0.617387	val: 0.859671	test: 0.829112
MAE train: 0.482885	val: 0.668274	test: 0.652954

Epoch: 27
Loss: 0.4824299544095993
RMSE train: 0.609774	val: 0.871635	test: 0.827995
MAE train: 0.476337	val: 0.668557	test: 0.650172

Epoch: 28
Loss: 0.4720050603151321
RMSE train: 0.596327	val: 0.862761	test: 0.830399
MAE train: 0.467198	val: 0.673403	test: 0.647737

Epoch: 29
Loss: 0.46846903860569
RMSE train: 0.611541	val: 0.878388	test: 0.836950
MAE train: 0.475486	val: 0.674536	test: 0.654436

Epoch: 30
Loss: 0.4471920758485794
RMSE train: 0.622453	val: 0.875874	test: 0.840865
MAE train: 0.483161	val: 0.671128	test: 0.661962

Epoch: 31
Loss: 0.4589898258447647
RMSE train: 0.610354	val: 0.874983	test: 0.832474
MAE train: 0.476545	val: 0.670465	test: 0.650263

Epoch: 32
Loss: 0.44074037969112395
RMSE train: 0.577542	val: 0.862086	test: 0.837508
MAE train: 0.451693	val: 0.666377	test: 0.655084

Epoch: 33
Loss: 0.4472556680440903
RMSE train: 0.599504	val: 0.855086	test: 0.831278
MAE train: 0.468894	val: 0.656997	test: 0.653793

Epoch: 34
Loss: 0.4442068040370941
RMSE train: 0.579536	val: 0.865915	test: 0.833575
MAE train: 0.454750	val: 0.669888	test: 0.657295

Epoch: 35
Loss: 0.4341021001338959
RMSE train: 0.571463	val: 0.846547	test: 0.816693
MAE train: 0.443307	val: 0.649985	test: 0.638349

Epoch: 36
Loss: 0.4245331883430481
RMSE train: 0.573728	val: 0.854768	test: 0.835540
MAE train: 0.450232	val: 0.665536	test: 0.650707

Epoch: 37
Loss: 0.41959222555160525
RMSE train: 0.567723	val: 0.851244	test: 0.824519
MAE train: 0.445514	val: 0.662011	test: 0.642450

Epoch: 38
Loss: 0.4151355177164078
RMSE train: 0.595043	val: 0.876137	test: 0.838443
MAE train: 0.466252	val: 0.670978	test: 0.652300

Epoch: 39
Loss: 0.41501230001449585
RMSE train: 0.568357	val: 0.846850	test: 0.837750
MAE train: 0.446683	val: 0.655998	test: 0.651623

Epoch: 40
Loss: 0.41681677401065825
RMSE train: 0.558056	val: 0.855505	test: 0.835439
MAE train: 0.439262	val: 0.664699	test: 0.649838

Epoch: 41
Loss: 0.39661865234375
RMSE train: 0.559881	val: 0.843043	test: 0.826746
MAE train: 0.438337	val: 0.654397	test: 0.644858

Epoch: 42
Loss: 0.4003447353839874
RMSE train: 0.549095	val: 0.842375	test: 0.814688
MAE train: 0.428440	val: 0.651547	test: 0.638237

Epoch: 43
Loss: 0.381728807091713
RMSE train: 0.550792	val: 0.834198	test: 0.813756
MAE train: 0.431044	val: 0.639510	test: 0.633956

Epoch: 44
Loss: 0.3767629057168961
RMSE train: 0.545433	val: 0.850635	test: 0.829279
MAE train: 0.424579	val: 0.655040	test: 0.641345

Epoch: 45
Loss: 0.39019213914871215
RMSE train: 0.543760	val: 0.834846	test: 0.817114
MAE train: 0.426174	val: 0.644098	test: 0.633435

Epoch: 46
Loss: 0.38370799124240873
RMSE train: 0.546064	val: 0.851057	test: 0.830141
MAE train: 0.427245	val: 0.655080	test: 0.648394

Epoch: 47
Loss: 0.3864263355731964
RMSE train: 0.547306	val: 0.854005	test: 0.825487
MAE train: 0.432631	val: 0.661697	test: 0.646577

Epoch: 48
Loss: 0.3783363074064255
RMSE train: 0.534494	val: 0.852773	test: 0.821445
MAE train: 0.417889	val: 0.656231	test: 0.638593

Epoch: 49
Loss: 0.3753606528043747
RMSE train: 0.523183	val: 0.838335	test: 0.831557
MAE train: 0.408637	val: 0.645444	test: 0.644565

Epoch: 50
Loss: 0.3857844412326813
RMSE train: 0.564933	val: 0.861891	test: 0.834065
MAE train: 0.440109	val: 0.658801	test: 0.655164

Epoch: 51
Loss: 0.3747892498970032
RMSE train: 0.537542	val: 0.853535	test: 0.834893
MAE train: 0.418388	val: 0.649917	test: 0.652445

Epoch: 52
Loss: 0.3700805902481079
RMSE train: 0.519682	val: 0.845596	test: 0.822733
MAE train: 0.407168	val: 0.643619	test: 0.638915

Epoch: 53
Loss: 0.36102503836154937
RMSE train: 0.526563	val: 0.838832	test: 0.818184
MAE train: 0.413255	val: 0.645566	test: 0.633371

Epoch: 54
Loss: 0.35941472053527834
RMSE train: 0.533680	val: 0.842525	test: 0.816587
MAE train: 0.418305	val: 0.651887	test: 0.640655

Epoch: 55
Loss: 0.3439536452293396
RMSE train: 0.554893	val: 0.856032	test: 0.824486
MAE train: 0.431647	val: 0.655512	test: 0.646721

Epoch: 56
Loss: 0.3628942877054214
RMSE train: 0.523852	val: 0.835949	test: 0.822220
MAE train: 0.411480	val: 0.647047	test: 0.639195

Epoch: 57
Loss: 0.33797973990440366
RMSE train: 0.514338	val: 0.831923	test: 0.815755
MAE train: 0.404201	val: 0.642320	test: 0.634043

Epoch: 58
Loss: 0.35052388310432436
RMSE train: 0.505198	val: 0.811120	test: 0.801072
MAE train: 0.395657	val: 0.628336	test: 0.622120

Epoch: 59
Loss: 0.34829945862293243
RMSE train: 0.523593	val: 0.831903	test: 0.815921
MAE train: 0.411034	val: 0.642463	test: 0.637532

Epoch: 60
Loss: 0.3320053696632385
RMSE train: 0.508175	val: 0.828944	test: 0.814451
MAE train: 0.393700	val: 0.638973	test: 0.633940

Epoch: 61
Loss: 0.34153689742088317
RMSE train: 0.508716	val: 0.817365	test: 0.818046
MAE train: 0.396061	val: 0.629653	test: 0.638934

Epoch: 62
Loss: 0.31207630336284636
RMSE train: 0.521456	val: 0.848764	test: 0.823604
MAE train: 0.406967	val: 0.650038	test: 0.641214

Epoch: 63
Loss: 0.3283269703388214
RMSE train: 0.517307	val: 0.834256	test: 0.822832
MAE train: 0.406468	val: 0.644634	test: 0.642233

Epoch: 64
Loss: 0.33071437776088713
RMSE train: 0.499081	val: 0.819001	test: 0.809823
MAE train: 0.392863	val: 0.632081	test: 0.630154

Epoch: 65
Loss: 0.33285446763038634
RMSE train: 0.498531	val: 0.835906	test: 0.817359
MAE train: 0.390981	val: 0.643626	test: 0.634843

Epoch: 66
Loss: 0.31847286522388457
RMSE train: 0.510078	val: 0.840712	test: 0.816900
MAE train: 0.398767	val: 0.644873	test: 0.636004

Epoch: 67
Loss: 0.3184714257717133
RMSE train: 0.490301	val: 0.834477	test: 0.821303
MAE train: 0.384568	val: 0.640259	test: 0.638568

Epoch: 68
Loss: 0.29997189044952394
RMSE train: 0.500578	val: 0.827025	test: 0.816701
MAE train: 0.395054	val: 0.639483	test: 0.637999

Epoch: 69
Loss: 0.31550338566303254
RMSE train: 0.485893	val: 0.810035	test: 0.801056
MAE train: 0.381012	val: 0.621147	test: 0.626847

Epoch: 70
Loss: 0.30581592917442324
RMSE train: 0.491051	val: 0.825385	test: 0.822998
MAE train: 0.379683	val: 0.629022	test: 0.644604

Epoch: 71
Loss: 0.30417544543743136
RMSE train: 0.480416	val: 0.804629	test: 0.809085
MAE train: 0.376044	val: 0.616740	test: 0.629441

Epoch: 72
Loss: 0.31786830723285675
RMSE train: 0.488580	val: 0.827461	test: 0.810112
MAE train: 0.382153	val: 0.623479	test: 0.629029

Epoch: 73
Loss: 0.30965258181095123
RMSE train: 0.479343	val: 0.805736	test: 0.803336
MAE train: 0.375414	val: 0.618547	test: 0.624693

Epoch: 74
Loss: 0.292529296875
RMSE train: 0.468547	val: 0.798817	test: 0.803511
MAE train: 0.367288	val: 0.609498	test: 0.624262

Epoch: 75
Loss: 0.3130342364311218
RMSE train: 0.475097	val: 0.820795	test: 0.815828
MAE train: 0.369800	val: 0.623884	test: 0.632756

Epoch: 76
Loss: 0.30694136917591097
RMSE train: 0.479654	val: 0.806766	test: 0.801620
MAE train: 0.375935	val: 0.618581	test: 0.628029

Epoch: 77
Loss: 0.30668900310993197
RMSE train: 0.488400	val: 0.806780	test: 0.807763
MAE train: 0.378275	val: 0.622515	test: 0.633309

Epoch: 78
Loss: 0.2988672435283661
RMSE train: 0.480926	val: 0.808142	test: 0.801310
MAE train: 0.376682	val: 0.621882	test: 0.625101

Epoch: 79
Loss: 0.29700678735971453
RMSE train: 0.483216	val: 0.824873	test: 0.820361
MAE train: 0.376794	val: 0.636308	test: 0.641774

Epoch: 80
Loss: 0.28152055740356446
RMSE train: 0.467641	val: 0.809307	test: 0.798453
MAE train: 0.367682	val: 0.620177	test: 0.620352

Epoch: 81
Loss: 0.28706676959991456
RMSE train: 0.475180	val: 0.818359	test: 0.812699
MAE train: 0.369076	val: 0.626410	test: 0.634091

Epoch: 82
Loss: 0.28631226569414137
RMSE train: 0.467413	val: 0.796764	test: 0.807044
MAE train: 0.369140	val: 0.620135	test: 0.625285

Epoch: 83
Loss: 0.2853652164340019
RMSE train: 0.514432	val: 0.832303	test: 0.821595

Epoch: 23
Loss: 0.5292264193296432
RMSE train: 0.631615	val: 0.870314	test: 0.851317
MAE train: 0.496046	val: 0.682798	test: 0.670508

Epoch: 24
Loss: 0.5338706612586975
RMSE train: 0.641417	val: 0.878205	test: 0.872929
MAE train: 0.508401	val: 0.695816	test: 0.685319

Epoch: 25
Loss: 0.5074759274721146
RMSE train: 0.644425	val: 0.874868	test: 0.866507
MAE train: 0.510247	val: 0.684582	test: 0.684633

Epoch: 26
Loss: 0.47312265932559966
RMSE train: 0.648243	val: 0.891790	test: 0.870490
MAE train: 0.515233	val: 0.707926	test: 0.690452

Epoch: 27
Loss: 0.5099701881408691
RMSE train: 0.626297	val: 0.862110	test: 0.857371
MAE train: 0.495670	val: 0.681722	test: 0.675917

Epoch: 28
Loss: 0.470823860168457
RMSE train: 0.616426	val: 0.859890	test: 0.855441
MAE train: 0.486094	val: 0.676759	test: 0.672540

Epoch: 29
Loss: 0.4609740853309631
RMSE train: 0.592772	val: 0.851071	test: 0.833798
MAE train: 0.462532	val: 0.667006	test: 0.657622

Epoch: 30
Loss: 0.4511078506708145
RMSE train: 0.590161	val: 0.848596	test: 0.839971
MAE train: 0.458193	val: 0.657848	test: 0.658908

Epoch: 31
Loss: 0.46363539099693296
RMSE train: 0.592382	val: 0.860453	test: 0.836911
MAE train: 0.466076	val: 0.676238	test: 0.657557

Epoch: 32
Loss: 0.4554128676652908
RMSE train: 0.600627	val: 0.876241	test: 0.848647
MAE train: 0.472010	val: 0.679823	test: 0.667377

Epoch: 33
Loss: 0.4468029260635376
RMSE train: 0.592267	val: 0.867359	test: 0.850955
MAE train: 0.468987	val: 0.678152	test: 0.665827

Epoch: 34
Loss: 0.4470984280109406
RMSE train: 0.583768	val: 0.837893	test: 0.838587
MAE train: 0.457093	val: 0.652128	test: 0.651197

Epoch: 35
Loss: 0.4519773632287979
RMSE train: 0.587064	val: 0.851270	test: 0.832973
MAE train: 0.461458	val: 0.665421	test: 0.652651

Epoch: 36
Loss: 0.43279111087322236
RMSE train: 0.629508	val: 0.891891	test: 0.868614
MAE train: 0.505400	val: 0.709232	test: 0.686410

Epoch: 37
Loss: 0.43043949007987975
RMSE train: 0.596191	val: 0.854320	test: 0.866085
MAE train: 0.467085	val: 0.664504	test: 0.674602

Epoch: 38
Loss: 0.45095056891441343
RMSE train: 0.616086	val: 0.878441	test: 0.864189
MAE train: 0.492919	val: 0.694251	test: 0.684861

Epoch: 39
Loss: 0.4083263486623764
RMSE train: 0.563454	val: 0.841971	test: 0.830639
MAE train: 0.442425	val: 0.653296	test: 0.650593

Epoch: 40
Loss: 0.4028216153383255
RMSE train: 0.567425	val: 0.853847	test: 0.844721
MAE train: 0.445419	val: 0.666628	test: 0.663750

Epoch: 41
Loss: 0.4105666160583496
RMSE train: 0.586695	val: 0.867932	test: 0.862322
MAE train: 0.465556	val: 0.681775	test: 0.676356

Epoch: 42
Loss: 0.4066198945045471
RMSE train: 0.555516	val: 0.848697	test: 0.838199
MAE train: 0.436129	val: 0.660389	test: 0.654475

Epoch: 43
Loss: 0.40938314497470857
RMSE train: 0.553065	val: 0.840566	test: 0.833092
MAE train: 0.434058	val: 0.650554	test: 0.647410

Epoch: 44
Loss: 0.3867966502904892
RMSE train: 0.562328	val: 0.831695	test: 0.825168
MAE train: 0.439502	val: 0.646220	test: 0.646701

Epoch: 45
Loss: 0.39935205280780794
RMSE train: 0.561876	val: 0.852974	test: 0.846269
MAE train: 0.439984	val: 0.662263	test: 0.658147

Epoch: 46
Loss: 0.39293333888053894
RMSE train: 0.550026	val: 0.849151	test: 0.843076
MAE train: 0.431669	val: 0.660616	test: 0.660039

Epoch: 47
Loss: 0.38368953466415406
RMSE train: 0.549529	val: 0.842810	test: 0.846219
MAE train: 0.430690	val: 0.654558	test: 0.654012

Epoch: 48
Loss: 0.39036925137043
RMSE train: 0.565374	val: 0.856485	test: 0.851182
MAE train: 0.449216	val: 0.668852	test: 0.666325

Epoch: 49
Loss: 0.38865094184875487
RMSE train: 0.554361	val: 0.849392	test: 0.856035
MAE train: 0.436825	val: 0.662229	test: 0.662080

Epoch: 50
Loss: 0.3666451334953308
RMSE train: 0.545411	val: 0.844108	test: 0.843827
MAE train: 0.428661	val: 0.657162	test: 0.654980

Epoch: 51
Loss: 0.3811022639274597
RMSE train: 0.521842	val: 0.829569	test: 0.828238
MAE train: 0.406936	val: 0.639330	test: 0.642256

Epoch: 52
Loss: 0.36925676465034485
RMSE train: 0.514433	val: 0.826550	test: 0.820131
MAE train: 0.399606	val: 0.633518	test: 0.636940

Epoch: 53
Loss: 0.36319693326950075
RMSE train: 0.537173	val: 0.826715	test: 0.825657
MAE train: 0.420820	val: 0.640246	test: 0.644327

Epoch: 54
Loss: 0.36977542042732237
RMSE train: 0.568755	val: 0.865628	test: 0.855249
MAE train: 0.453228	val: 0.675794	test: 0.672416

Epoch: 55
Loss: 0.3513775408267975
RMSE train: 0.527428	val: 0.840006	test: 0.828531
MAE train: 0.412553	val: 0.650317	test: 0.638304

Epoch: 56
Loss: 0.3564784646034241
RMSE train: 0.518168	val: 0.836773	test: 0.822386
MAE train: 0.403726	val: 0.646150	test: 0.638776

Epoch: 57
Loss: 0.3551199585199356
RMSE train: 0.512380	val: 0.837528	test: 0.827932
MAE train: 0.396158	val: 0.642978	test: 0.641618

Epoch: 58
Loss: 0.3451978325843811
RMSE train: 0.533260	val: 0.837723	test: 0.843758
MAE train: 0.417632	val: 0.647493	test: 0.653525

Epoch: 59
Loss: 0.34380240738391876
RMSE train: 0.509913	val: 0.835146	test: 0.836137
MAE train: 0.396918	val: 0.637822	test: 0.645319

Epoch: 60
Loss: 0.3499856859445572
RMSE train: 0.515283	val: 0.826744	test: 0.819520
MAE train: 0.402558	val: 0.630666	test: 0.634405

Epoch: 61
Loss: 0.34239670932292937
RMSE train: 0.498170	val: 0.826832	test: 0.821768
MAE train: 0.387113	val: 0.637007	test: 0.632960

Epoch: 62
Loss: 0.32261266708374026
RMSE train: 0.513794	val: 0.836205	test: 0.822573
MAE train: 0.400655	val: 0.640686	test: 0.640420

Epoch: 63
Loss: 0.3422487944364548
RMSE train: 0.507569	val: 0.830992	test: 0.822300
MAE train: 0.395647	val: 0.636598	test: 0.637000

Epoch: 64
Loss: 0.321957066655159
RMSE train: 0.509061	val: 0.832712	test: 0.837490
MAE train: 0.396888	val: 0.640647	test: 0.649555

Epoch: 65
Loss: 0.31653172969818116
RMSE train: 0.537855	val: 0.852777	test: 0.840561
MAE train: 0.429482	val: 0.666076	test: 0.658592

Epoch: 66
Loss: 0.3200163751840591
RMSE train: 0.502528	val: 0.826344	test: 0.816750
MAE train: 0.393365	val: 0.631808	test: 0.632387

Epoch: 67
Loss: 0.32468844950199127
RMSE train: 0.502912	val: 0.832130	test: 0.817480
MAE train: 0.394963	val: 0.641054	test: 0.638215

Epoch: 68
Loss: 0.3228192299604416
RMSE train: 0.510289	val: 0.826146	test: 0.830663
MAE train: 0.397176	val: 0.634745	test: 0.646151

Epoch: 69
Loss: 0.3201581031084061
RMSE train: 0.503671	val: 0.824625	test: 0.821949
MAE train: 0.394208	val: 0.636134	test: 0.633917

Epoch: 70
Loss: 0.3218932390213013
RMSE train: 0.518878	val: 0.839057	test: 0.825396
MAE train: 0.408435	val: 0.640418	test: 0.644880

Epoch: 71
Loss: 0.32806660830974577
RMSE train: 0.500933	val: 0.824744	test: 0.827911
MAE train: 0.390273	val: 0.627400	test: 0.640643

Epoch: 72
Loss: 0.3133772760629654
RMSE train: 0.490962	val: 0.829978	test: 0.834497
MAE train: 0.379572	val: 0.633317	test: 0.648557

Epoch: 73
Loss: 0.3121083855628967
RMSE train: 0.492174	val: 0.843983	test: 0.822355
MAE train: 0.386869	val: 0.643828	test: 0.637612

Epoch: 74
Loss: 0.3088880628347397
RMSE train: 0.475064	val: 0.830665	test: 0.822939
MAE train: 0.368264	val: 0.635714	test: 0.636815

Epoch: 75
Loss: 0.3083167463541031
RMSE train: 0.471158	val: 0.823128	test: 0.813509
MAE train: 0.365932	val: 0.626704	test: 0.628313

Epoch: 76
Loss: 0.312772211432457
RMSE train: 0.512586	val: 0.835809	test: 0.833216
MAE train: 0.405778	val: 0.644640	test: 0.648158

Epoch: 77
Loss: 0.3032839924097061
RMSE train: 0.482185	val: 0.818949	test: 0.814929
MAE train: 0.379331	val: 0.626544	test: 0.631396

Epoch: 78
Loss: 0.2959141880273819
RMSE train: 0.483925	val: 0.818432	test: 0.812830
MAE train: 0.377831	val: 0.625153	test: 0.632554

Epoch: 79
Loss: 0.307965150475502
RMSE train: 0.491756	val: 0.814520	test: 0.816234
MAE train: 0.385115	val: 0.625407	test: 0.633060

Epoch: 80
Loss: 0.3048711270093918
RMSE train: 0.488144	val: 0.835439	test: 0.825864
MAE train: 0.378509	val: 0.632884	test: 0.640680

Epoch: 81
Loss: 0.29665686190128326
RMSE train: 0.471142	val: 0.815447	test: 0.810390
MAE train: 0.367294	val: 0.627109	test: 0.624870

Epoch: 82
Loss: 0.29520302414894106
RMSE train: 0.483614	val: 0.831870	test: 0.817179
MAE train: 0.377890	val: 0.631979	test: 0.636811

Epoch: 83
Loss: 0.2978673666715622
RMSE train: 0.461766	val: 0.813327	test: 0.803743

Epoch: 23
Loss: 0.5234428495168686
RMSE train: 0.668273	val: 0.886581	test: 0.829705
MAE train: 0.520021	val: 0.668332	test: 0.666684

Epoch: 24
Loss: 0.5097399155298868
RMSE train: 0.635637	val: 0.862353	test: 0.790933
MAE train: 0.492746	val: 0.652577	test: 0.639675

Epoch: 25
Loss: 0.5085627560814222
RMSE train: 0.622490	val: 0.851801	test: 0.782712
MAE train: 0.482627	val: 0.639964	test: 0.629698

Epoch: 26
Loss: 0.4971485535303752
RMSE train: 0.628534	val: 0.874461	test: 0.804933
MAE train: 0.485777	val: 0.658913	test: 0.653861

Epoch: 27
Loss: 0.5020845929781595
RMSE train: 0.620275	val: 0.865647	test: 0.787145
MAE train: 0.479707	val: 0.650430	test: 0.636886

Epoch: 28
Loss: 0.4801411305864652
RMSE train: 0.627286	val: 0.855371	test: 0.785781
MAE train: 0.489290	val: 0.645215	test: 0.634423

Epoch: 29
Loss: 0.4827975357572238
RMSE train: 0.614819	val: 0.854176	test: 0.776172
MAE train: 0.477797	val: 0.643279	test: 0.626400

Epoch: 30
Loss: 0.4683929632107417
RMSE train: 0.608262	val: 0.843481	test: 0.799808
MAE train: 0.470766	val: 0.636417	test: 0.643879

Epoch: 31
Loss: 0.4796031340956688
RMSE train: 0.619839	val: 0.851244	test: 0.800746
MAE train: 0.482865	val: 0.648503	test: 0.646625

Epoch: 32
Loss: 0.46495796491702396
RMSE train: 0.603333	val: 0.841156	test: 0.777235
MAE train: 0.471588	val: 0.632892	test: 0.620920

Epoch: 33
Loss: 0.44659065703550976
RMSE train: 0.593932	val: 0.837083	test: 0.771840
MAE train: 0.460409	val: 0.629972	test: 0.618234

Epoch: 34
Loss: 0.44746192296346027
RMSE train: 0.612125	val: 0.854519	test: 0.798797
MAE train: 0.475325	val: 0.641203	test: 0.640899

Epoch: 35
Loss: 0.4476378684242566
RMSE train: 0.607538	val: 0.850025	test: 0.799128
MAE train: 0.472032	val: 0.642463	test: 0.640408

Epoch: 36
Loss: 0.4393020073572795
RMSE train: 0.582567	val: 0.836656	test: 0.770597
MAE train: 0.449122	val: 0.631602	test: 0.614502

Epoch: 37
Loss: 0.4307193234562874
RMSE train: 0.600096	val: 0.846786	test: 0.781415
MAE train: 0.464612	val: 0.634523	test: 0.631068

Epoch: 38
Loss: 0.42794112861156464
RMSE train: 0.581120	val: 0.827819	test: 0.770809
MAE train: 0.450942	val: 0.624463	test: 0.621107

Epoch: 39
Loss: 0.41733147700627643
RMSE train: 0.581698	val: 0.831600	test: 0.768714
MAE train: 0.447718	val: 0.624076	test: 0.614487

Epoch: 40
Loss: 0.42166463285684586
RMSE train: 0.574200	val: 0.821809	test: 0.758087
MAE train: 0.446973	val: 0.620460	test: 0.609048

Epoch: 41
Loss: 0.40768930315971375
RMSE train: 0.557460	val: 0.808313	test: 0.745223
MAE train: 0.431231	val: 0.610934	test: 0.593314

Epoch: 42
Loss: 0.4196228136618932
RMSE train: 0.562825	val: 0.807718	test: 0.752496
MAE train: 0.435764	val: 0.609828	test: 0.601332

Epoch: 43
Loss: 0.3980280285080274
RMSE train: 0.564416	val: 0.824416	test: 0.773346
MAE train: 0.436519	val: 0.617801	test: 0.612900

Epoch: 44
Loss: 0.4053143536051114
RMSE train: 0.548321	val: 0.804931	test: 0.758399
MAE train: 0.426651	val: 0.604544	test: 0.603556

Epoch: 45
Loss: 0.39190611988306046
RMSE train: 0.566962	val: 0.814120	test: 0.756697
MAE train: 0.441270	val: 0.616946	test: 0.602456

Epoch: 46
Loss: 0.38872576504945755
RMSE train: 0.566364	val: 0.818954	test: 0.757282
MAE train: 0.440367	val: 0.613759	test: 0.605801

Epoch: 47
Loss: 0.3881662239631017
RMSE train: 0.545270	val: 0.807400	test: 0.750907
MAE train: 0.422404	val: 0.604018	test: 0.595043

Epoch: 48
Loss: 0.39207133650779724
RMSE train: 0.551508	val: 0.795617	test: 0.750975
MAE train: 0.429315	val: 0.599298	test: 0.596866

Epoch: 49
Loss: 0.37835770348707837
RMSE train: 0.553959	val: 0.808093	test: 0.750909
MAE train: 0.432616	val: 0.607503	test: 0.596269

Epoch: 50
Loss: 0.38808371623357135
RMSE train: 0.557765	val: 0.820250	test: 0.765025
MAE train: 0.433412	val: 0.613017	test: 0.611774

Epoch: 51
Loss: 0.386633222301801
RMSE train: 0.561954	val: 0.829358	test: 0.764317
MAE train: 0.436339	val: 0.623990	test: 0.613483

Epoch: 52
Loss: 0.381353738407294
RMSE train: 0.586084	val: 0.838173	test: 0.779173
MAE train: 0.455473	val: 0.634753	test: 0.625644

Epoch: 53
Loss: 0.37763433406750363
RMSE train: 0.550344	val: 0.822283	test: 0.764573
MAE train: 0.426637	val: 0.617824	test: 0.607515

Epoch: 54
Loss: 0.3649017736315727
RMSE train: 0.536283	val: 0.803294	test: 0.752640
MAE train: 0.417186	val: 0.603081	test: 0.603215

Epoch: 55
Loss: 0.36884577572345734
RMSE train: 0.561144	val: 0.836473	test: 0.781410
MAE train: 0.434960	val: 0.630557	test: 0.624014

Epoch: 56
Loss: 0.35338980207840603
RMSE train: 0.525557	val: 0.801752	test: 0.743349
MAE train: 0.407936	val: 0.603405	test: 0.590896

Epoch: 57
Loss: 0.36074663201967877
RMSE train: 0.546103	val: 0.820683	test: 0.759277
MAE train: 0.425677	val: 0.615377	test: 0.604224

Epoch: 58
Loss: 0.3549039090673129
RMSE train: 0.543286	val: 0.801364	test: 0.749337
MAE train: 0.422572	val: 0.598091	test: 0.595264

Epoch: 59
Loss: 0.3474074527621269
RMSE train: 0.518656	val: 0.792225	test: 0.733655
MAE train: 0.400825	val: 0.594162	test: 0.585098

Epoch: 60
Loss: 0.3390182579557101
RMSE train: 0.558137	val: 0.818985	test: 0.758001
MAE train: 0.435068	val: 0.617929	test: 0.605930

Epoch: 61
Loss: 0.3412950783967972
RMSE train: 0.540306	val: 0.822296	test: 0.760969
MAE train: 0.420610	val: 0.615039	test: 0.605398

Epoch: 62
Loss: 0.33508048703273136
RMSE train: 0.519969	val: 0.800665	test: 0.752878
MAE train: 0.405467	val: 0.599976	test: 0.599767

Epoch: 63
Loss: 0.35138807942469913
RMSE train: 0.520631	val: 0.791104	test: 0.742815
MAE train: 0.406350	val: 0.589990	test: 0.589193

Epoch: 64
Loss: 0.32516471048196155
RMSE train: 0.523968	val: 0.809423	test: 0.748887
MAE train: 0.406842	val: 0.605394	test: 0.599811

Epoch: 65
Loss: 0.3402835850914319
RMSE train: 0.532368	val: 0.806827	test: 0.764612
MAE train: 0.415040	val: 0.602610	test: 0.605921

Epoch: 66
Loss: 0.3337260037660599
RMSE train: 0.521619	val: 0.808731	test: 0.760251
MAE train: 0.403746	val: 0.600827	test: 0.603897

Epoch: 67
Loss: 0.3310621778170268
RMSE train: 0.534432	val: 0.816754	test: 0.761789
MAE train: 0.416208	val: 0.611045	test: 0.604544

Epoch: 68
Loss: 0.32377347846825916
RMSE train: 0.528913	val: 0.805006	test: 0.753639
MAE train: 0.412123	val: 0.601024	test: 0.594988

Epoch: 69
Loss: 0.32714032133420307
RMSE train: 0.509809	val: 0.801833	test: 0.752252
MAE train: 0.394391	val: 0.597761	test: 0.593184

Epoch: 70
Loss: 0.3219997708996137
RMSE train: 0.506225	val: 0.811385	test: 0.750481
MAE train: 0.392137	val: 0.608518	test: 0.595624

Epoch: 71
Loss: 0.3122028037905693
RMSE train: 0.488533	val: 0.773415	test: 0.742186
MAE train: 0.378930	val: 0.575208	test: 0.585604

Epoch: 72
Loss: 0.3202689116199811
RMSE train: 0.514248	val: 0.804883	test: 0.759130
MAE train: 0.400115	val: 0.606447	test: 0.600246

Epoch: 73
Loss: 0.3230298310518265
RMSE train: 0.493379	val: 0.787197	test: 0.733825
MAE train: 0.381422	val: 0.584827	test: 0.577474

Epoch: 74
Loss: 0.31544849773248035
RMSE train: 0.511705	val: 0.799828	test: 0.758108
MAE train: 0.395466	val: 0.595144	test: 0.596892

Epoch: 75
Loss: 0.32016358027855557
RMSE train: 0.509339	val: 0.812467	test: 0.764832
MAE train: 0.395910	val: 0.605547	test: 0.599016

Epoch: 76
Loss: 0.30963945388793945
RMSE train: 0.514293	val: 0.805147	test: 0.758843
MAE train: 0.400892	val: 0.602590	test: 0.602182

Epoch: 77
Loss: 0.30236205955346424
RMSE train: 0.495702	val: 0.789641	test: 0.752712
MAE train: 0.384962	val: 0.588628	test: 0.593437

Epoch: 78
Loss: 0.30954258888959885
RMSE train: 0.501383	val: 0.779467	test: 0.730954
MAE train: 0.391306	val: 0.582780	test: 0.580754

Epoch: 79
Loss: 0.2930758198102315
RMSE train: 0.483266	val: 0.770957	test: 0.730034
MAE train: 0.375392	val: 0.574314	test: 0.575372

Epoch: 80
Loss: 0.30470942084987956
RMSE train: 0.495721	val: 0.781679	test: 0.740468
MAE train: 0.385033	val: 0.581265	test: 0.587315

Epoch: 81
Loss: 0.30183156703909236
RMSE train: 0.500581	val: 0.801386	test: 0.753426
MAE train: 0.389885	val: 0.599272	test: 0.592974

Epoch: 82
Loss: 0.2873627319931984
RMSE train: 0.512334	val: 0.804247	test: 0.767701
MAE train: 0.399536	val: 0.605236	test: 0.604663

Epoch: 83
Loss: 0.28082093099753064
RMSE train: 0.483168	val: 0.778134	test: 0.735079

Epoch: 23
Loss: 0.5118802040815353
RMSE train: 0.678282	val: 0.875113	test: 0.811029
MAE train: 0.526115	val: 0.672333	test: 0.650908

Epoch: 24
Loss: 0.5141837497552236
RMSE train: 0.650673	val: 0.870825	test: 0.807376
MAE train: 0.511976	val: 0.667557	test: 0.649530

Epoch: 25
Loss: 0.4969621499379476
RMSE train: 0.646394	val: 0.863897	test: 0.788504
MAE train: 0.502728	val: 0.657330	test: 0.628390

Epoch: 26
Loss: 0.4916170760989189
RMSE train: 0.622272	val: 0.855613	test: 0.784482
MAE train: 0.490046	val: 0.653456	test: 0.627549

Epoch: 27
Loss: 0.4791002149383227
RMSE train: 0.611978	val: 0.856123	test: 0.781173
MAE train: 0.481519	val: 0.649580	test: 0.622039

Epoch: 28
Loss: 0.49578632414340973
RMSE train: 0.647110	val: 0.877367	test: 0.813131
MAE train: 0.499439	val: 0.654247	test: 0.650855

Epoch: 29
Loss: 0.4877711931864421
RMSE train: 0.610361	val: 0.857489	test: 0.783300
MAE train: 0.479226	val: 0.652906	test: 0.627387

Epoch: 30
Loss: 0.48484813670317334
RMSE train: 0.625822	val: 0.841491	test: 0.782987
MAE train: 0.494445	val: 0.649131	test: 0.626530

Epoch: 31
Loss: 0.4780142605304718
RMSE train: 0.637820	val: 0.865725	test: 0.806121
MAE train: 0.496439	val: 0.663497	test: 0.649155

Epoch: 32
Loss: 0.4613656426469485
RMSE train: 0.602384	val: 0.847931	test: 0.790645
MAE train: 0.467234	val: 0.644565	test: 0.631974

Epoch: 33
Loss: 0.45943620552619296
RMSE train: 0.623445	val: 0.865567	test: 0.798411
MAE train: 0.487442	val: 0.658892	test: 0.638658

Epoch: 34
Loss: 0.45562805235385895
RMSE train: 0.607991	val: 0.851992	test: 0.779856
MAE train: 0.477877	val: 0.648175	test: 0.628579

Epoch: 35
Loss: 0.44749870151281357
RMSE train: 0.599405	val: 0.839473	test: 0.786185
MAE train: 0.470113	val: 0.635848	test: 0.626163

Epoch: 36
Loss: 0.4272390479842822
RMSE train: 0.589651	val: 0.842547	test: 0.772595
MAE train: 0.462806	val: 0.633073	test: 0.620432

Epoch: 37
Loss: 0.4244656910498937
RMSE train: 0.586963	val: 0.844602	test: 0.768284
MAE train: 0.460513	val: 0.636784	test: 0.618478

Epoch: 38
Loss: 0.4387304112315178
RMSE train: 0.577649	val: 0.839142	test: 0.775843
MAE train: 0.452054	val: 0.627753	test: 0.621993

Epoch: 39
Loss: 0.43505879243214923
RMSE train: 0.592274	val: 0.851552	test: 0.780476
MAE train: 0.465035	val: 0.645223	test: 0.629948

Epoch: 40
Loss: 0.4277823194861412
RMSE train: 0.576949	val: 0.819659	test: 0.766994
MAE train: 0.453071	val: 0.621508	test: 0.614700

Epoch: 41
Loss: 0.41408921778202057
RMSE train: 0.580367	val: 0.831158	test: 0.770511
MAE train: 0.454147	val: 0.628739	test: 0.616457

Epoch: 42
Loss: 0.40037600447734195
RMSE train: 0.579318	val: 0.833203	test: 0.781814
MAE train: 0.451119	val: 0.630161	test: 0.624922

Epoch: 43
Loss: 0.3973025878270467
RMSE train: 0.594419	val: 0.856677	test: 0.799694
MAE train: 0.462849	val: 0.644614	test: 0.638356

Epoch: 44
Loss: 0.39814835290114087
RMSE train: 0.587959	val: 0.841895	test: 0.784444
MAE train: 0.457917	val: 0.634062	test: 0.627680

Epoch: 45
Loss: 0.4004446119070053
RMSE train: 0.565814	val: 0.811512	test: 0.765267
MAE train: 0.443675	val: 0.618186	test: 0.607314

Epoch: 46
Loss: 0.4039428134759267
RMSE train: 0.547470	val: 0.821288	test: 0.761150
MAE train: 0.426008	val: 0.614505	test: 0.608116

Epoch: 47
Loss: 0.3895643452803294
RMSE train: 0.611747	val: 0.847435	test: 0.783332
MAE train: 0.481134	val: 0.644512	test: 0.623887

Epoch: 48
Loss: 0.40373025586207706
RMSE train: 0.584457	val: 0.829654	test: 0.788712
MAE train: 0.457779	val: 0.631537	test: 0.625954

Epoch: 49
Loss: 0.39251405000686646
RMSE train: 0.563197	val: 0.827459	test: 0.769419
MAE train: 0.436512	val: 0.626607	test: 0.607972

Epoch: 50
Loss: 0.38733792801698047
RMSE train: 0.551764	val: 0.821744	test: 0.766680
MAE train: 0.430816	val: 0.617735	test: 0.608910

Epoch: 51
Loss: 0.4042944560448329
RMSE train: 0.552414	val: 0.815123	test: 0.757712
MAE train: 0.432499	val: 0.612293	test: 0.601841

Epoch: 52
Loss: 0.37296419590711594
RMSE train: 0.528136	val: 0.805385	test: 0.751598
MAE train: 0.411168	val: 0.604966	test: 0.593689

Epoch: 53
Loss: 0.36993346114953357
RMSE train: 0.532865	val: 0.805295	test: 0.762741
MAE train: 0.411572	val: 0.605150	test: 0.606305

Epoch: 54
Loss: 0.3703687662879626
RMSE train: 0.537349	val: 0.810364	test: 0.767155
MAE train: 0.418228	val: 0.611743	test: 0.609361

Epoch: 55
Loss: 0.34485698988040286
RMSE train: 0.526056	val: 0.796943	test: 0.740926
MAE train: 0.409999	val: 0.593905	test: 0.589028

Epoch: 56
Loss: 0.3606603766481082
RMSE train: 0.546951	val: 0.816009	test: 0.775175
MAE train: 0.424431	val: 0.614389	test: 0.616465

Epoch: 57
Loss: 0.36758777250846225
RMSE train: 0.552969	val: 0.823361	test: 0.785055
MAE train: 0.430469	val: 0.621989	test: 0.623498

Epoch: 58
Loss: 0.3457906097173691
RMSE train: 0.531332	val: 0.813886	test: 0.768844
MAE train: 0.412144	val: 0.608217	test: 0.612100

Epoch: 59
Loss: 0.34697795907656354
RMSE train: 0.536831	val: 0.823326	test: 0.782854
MAE train: 0.419431	val: 0.616290	test: 0.623998

Epoch: 60
Loss: 0.35588424901167554
RMSE train: 0.526329	val: 0.815405	test: 0.767890
MAE train: 0.410463	val: 0.609731	test: 0.607190

Epoch: 61
Loss: 0.3691086818774541
RMSE train: 0.529500	val: 0.807814	test: 0.766219
MAE train: 0.413302	val: 0.606067	test: 0.604023

Epoch: 62
Loss: 0.36342046906550723
RMSE train: 0.528111	val: 0.826349	test: 0.769212
MAE train: 0.406689	val: 0.609906	test: 0.605375

Epoch: 63
Loss: 0.3412917082508405
RMSE train: 0.540454	val: 0.831070	test: 0.775210
MAE train: 0.419997	val: 0.617077	test: 0.615396

Epoch: 64
Loss: 0.3327639549970627
RMSE train: 0.523439	val: 0.799099	test: 0.756224
MAE train: 0.408933	val: 0.607177	test: 0.599739

Epoch: 65
Loss: 0.3454349959890048
RMSE train: 0.527637	val: 0.808859	test: 0.751522
MAE train: 0.409300	val: 0.608763	test: 0.597982

Epoch: 66
Loss: 0.34302597244580585
RMSE train: 0.521230	val: 0.812431	test: 0.781606
MAE train: 0.405541	val: 0.611052	test: 0.619128

Epoch: 67
Loss: 0.34042496979236603
RMSE train: 0.524168	val: 0.806115	test: 0.756676
MAE train: 0.410913	val: 0.608004	test: 0.600629

Epoch: 68
Loss: 0.31459446251392365
RMSE train: 0.512620	val: 0.809011	test: 0.753436
MAE train: 0.398500	val: 0.607559	test: 0.596884

Epoch: 69
Loss: 0.31794912243882817
RMSE train: 0.523294	val: 0.828459	test: 0.764718
MAE train: 0.408339	val: 0.625675	test: 0.608407

Epoch: 70
Loss: 0.33078276614348096
RMSE train: 0.513184	val: 0.804865	test: 0.752898
MAE train: 0.398435	val: 0.600215	test: 0.594626

Epoch: 71
Loss: 0.3317649265130361
RMSE train: 0.495372	val: 0.795284	test: 0.761161
MAE train: 0.384568	val: 0.598175	test: 0.597693

Epoch: 72
Loss: 0.30069098869959515
RMSE train: 0.498505	val: 0.785362	test: 0.744498
MAE train: 0.387276	val: 0.589218	test: 0.589081

Epoch: 73
Loss: 0.32477229336897534
RMSE train: 0.511704	val: 0.808192	test: 0.771662
MAE train: 0.393499	val: 0.604697	test: 0.610471

Epoch: 74
Loss: 0.31707218041022617
RMSE train: 0.509852	val: 0.817328	test: 0.773042
MAE train: 0.395929	val: 0.612306	test: 0.609015

Epoch: 75
Loss: 0.31921232988437015
RMSE train: 0.500386	val: 0.812342	test: 0.765201
MAE train: 0.385866	val: 0.603783	test: 0.604391

Epoch: 76
Loss: 0.3201793506741524
RMSE train: 0.502266	val: 0.806253	test: 0.763188
MAE train: 0.389843	val: 0.606603	test: 0.601633

Epoch: 77
Loss: 0.31331346184015274
RMSE train: 0.504086	val: 0.801664	test: 0.760545
MAE train: 0.392019	val: 0.606262	test: 0.598974

Epoch: 78
Loss: 0.32064611961444217
RMSE train: 0.503675	val: 0.799270	test: 0.753652
MAE train: 0.392557	val: 0.600848	test: 0.598029

Epoch: 79
Loss: 0.2966032053033511
RMSE train: 0.494541	val: 0.798197	test: 0.754614
MAE train: 0.383905	val: 0.599384	test: 0.594606

Epoch: 80
Loss: 0.31036489456892014
RMSE train: 0.499735	val: 0.788626	test: 0.747922
MAE train: 0.390425	val: 0.590210	test: 0.589275

Epoch: 81
Loss: 0.2961639737089475
RMSE train: 0.486766	val: 0.809178	test: 0.763190
MAE train: 0.374320	val: 0.602877	test: 0.603522

Epoch: 82
Loss: 0.3079110359152158
RMSE train: 0.473186	val: 0.784635	test: 0.742996
MAE train: 0.366729	val: 0.588481	test: 0.582291

Epoch: 83
Loss: 0.30477970838546753
RMSE train: 0.501226	val: 0.804454	test: 0.752199

Epoch: 23
Loss: 0.5265375450253487
RMSE train: 0.672982	val: 0.880487	test: 0.799556
MAE train: 0.523308	val: 0.669991	test: 0.645343

Epoch: 24
Loss: 0.5310554628570875
RMSE train: 0.661510	val: 0.887666	test: 0.799108
MAE train: 0.515802	val: 0.673235	test: 0.643761

Epoch: 25
Loss: 0.5320707907279333
RMSE train: 0.657147	val: 0.893379	test: 0.817329
MAE train: 0.509560	val: 0.675718	test: 0.659004

Epoch: 26
Loss: 0.5205482269326845
RMSE train: 0.629273	val: 0.854818	test: 0.780892
MAE train: 0.489602	val: 0.643796	test: 0.620074

Epoch: 27
Loss: 0.5110842883586884
RMSE train: 0.644356	val: 0.881313	test: 0.797143
MAE train: 0.501968	val: 0.665512	test: 0.640444

Epoch: 28
Loss: 0.5075641795992851
RMSE train: 0.649885	val: 0.878971	test: 0.804747
MAE train: 0.507842	val: 0.666391	test: 0.642444

Epoch: 29
Loss: 0.48589663704236347
RMSE train: 0.624601	val: 0.867043	test: 0.789305
MAE train: 0.485339	val: 0.657079	test: 0.627361

Epoch: 30
Loss: 0.502304695546627
RMSE train: 0.633068	val: 0.865278	test: 0.807440
MAE train: 0.491224	val: 0.655495	test: 0.650313

Epoch: 31
Loss: 0.4904561812678973
RMSE train: 0.624943	val: 0.869206	test: 0.793675
MAE train: 0.486502	val: 0.653032	test: 0.635640

Epoch: 32
Loss: 0.4744265675544739
RMSE train: 0.695647	val: 0.924169	test: 0.861225
MAE train: 0.541289	val: 0.702023	test: 0.691777

Epoch: 33
Loss: 0.44648252179225284
RMSE train: 0.620384	val: 0.880789	test: 0.810296
MAE train: 0.479494	val: 0.653866	test: 0.649609

Epoch: 34
Loss: 0.4572785447041194
RMSE train: 0.615254	val: 0.874088	test: 0.809281
MAE train: 0.477601	val: 0.654658	test: 0.648593

Epoch: 35
Loss: 0.4342634901404381
RMSE train: 0.603134	val: 0.859704	test: 0.783850
MAE train: 0.469101	val: 0.646270	test: 0.624580

Epoch: 36
Loss: 0.45277708520491916
RMSE train: 0.605239	val: 0.868901	test: 0.802926
MAE train: 0.468438	val: 0.650056	test: 0.640846

Epoch: 37
Loss: 0.43332814425230026
RMSE train: 0.638389	val: 0.872622	test: 0.815021
MAE train: 0.496202	val: 0.657685	test: 0.656931

Epoch: 38
Loss: 0.4416270429889361
RMSE train: 0.583553	val: 0.851708	test: 0.771708
MAE train: 0.452369	val: 0.636019	test: 0.615943

Epoch: 39
Loss: 0.4223347132404645
RMSE train: 0.653806	val: 0.897952	test: 0.838503
MAE train: 0.509459	val: 0.675382	test: 0.676282

Epoch: 40
Loss: 0.4221208567420642
RMSE train: 0.598486	val: 0.846425	test: 0.783572
MAE train: 0.464414	val: 0.636012	test: 0.626436

Epoch: 41
Loss: 0.4260027805964152
RMSE train: 0.601334	val: 0.864076	test: 0.785601
MAE train: 0.467839	val: 0.646139	test: 0.626711

Epoch: 42
Loss: 0.4008701244990031
RMSE train: 0.658231	val: 0.909821	test: 0.852619
MAE train: 0.516932	val: 0.686657	test: 0.682044

Epoch: 43
Loss: 0.4214685137073199
RMSE train: 0.604584	val: 0.868978	test: 0.820902
MAE train: 0.470622	val: 0.651154	test: 0.657123

Epoch: 44
Loss: 0.4106793055931727
RMSE train: 0.605939	val: 0.858267	test: 0.802224
MAE train: 0.471979	val: 0.643434	test: 0.645504

Epoch: 45
Loss: 0.4067313199241956
RMSE train: 0.622712	val: 0.863046	test: 0.790723
MAE train: 0.483406	val: 0.643798	test: 0.627269

Epoch: 46
Loss: 0.3935592323541641
RMSE train: 0.584390	val: 0.870431	test: 0.791709
MAE train: 0.453205	val: 0.641865	test: 0.633685

Epoch: 47
Loss: 0.38844161480665207
RMSE train: 0.558864	val: 0.822469	test: 0.767798
MAE train: 0.434234	val: 0.610168	test: 0.608879

Epoch: 48
Loss: 0.4121282547712326
RMSE train: 0.584417	val: 0.855368	test: 0.811809
MAE train: 0.454882	val: 0.646652	test: 0.650177

Epoch: 49
Loss: 0.3719503805041313
RMSE train: 0.570408	val: 0.837388	test: 0.777412
MAE train: 0.443605	val: 0.626295	test: 0.616671

Epoch: 50
Loss: 0.39009564618269604
RMSE train: 0.569127	val: 0.832269	test: 0.781368
MAE train: 0.441415	val: 0.619765	test: 0.626306

Epoch: 51
Loss: 0.37746469924847287
RMSE train: 0.570709	val: 0.858703	test: 0.784293
MAE train: 0.441220	val: 0.635244	test: 0.624954

Epoch: 52
Loss: 0.38944276918967563
RMSE train: 0.576025	val: 0.858888	test: 0.803645
MAE train: 0.445158	val: 0.636079	test: 0.641930

Epoch: 53
Loss: 0.37563250213861465
RMSE train: 0.613019	val: 0.873550	test: 0.821275
MAE train: 0.477786	val: 0.659749	test: 0.657772

Epoch: 54
Loss: 0.3726111923654874
RMSE train: 0.567537	val: 0.839676	test: 0.778856
MAE train: 0.441355	val: 0.628235	test: 0.623822

Epoch: 55
Loss: 0.365864892800649
RMSE train: 0.569616	val: 0.840146	test: 0.784060
MAE train: 0.443360	val: 0.630155	test: 0.623926

Epoch: 56
Loss: 0.3740648503104846
RMSE train: 0.594934	val: 0.870577	test: 0.802212
MAE train: 0.463933	val: 0.656493	test: 0.635324

Epoch: 57
Loss: 0.360093226035436
RMSE train: 0.582828	val: 0.857382	test: 0.788085
MAE train: 0.454013	val: 0.645671	test: 0.627878

Epoch: 58
Loss: 0.3569807435075442
RMSE train: 0.580145	val: 0.853992	test: 0.801961
MAE train: 0.451118	val: 0.644805	test: 0.642550

Epoch: 59
Loss: 0.34748904903729755
RMSE train: 0.551781	val: 0.821596	test: 0.776206
MAE train: 0.429643	val: 0.614063	test: 0.620273

Epoch: 60
Loss: 0.3359465226531029
RMSE train: 0.564475	val: 0.844562	test: 0.795879
MAE train: 0.434627	val: 0.627402	test: 0.636094

Epoch: 61
Loss: 0.3297681560118993
RMSE train: 0.563260	val: 0.840414	test: 0.782537
MAE train: 0.435638	val: 0.630385	test: 0.622887

Epoch: 62
Loss: 0.34757063289483386
RMSE train: 0.546693	val: 0.835459	test: 0.779199
MAE train: 0.424922	val: 0.630850	test: 0.623657

Epoch: 63
Loss: 0.346697136759758
RMSE train: 0.569860	val: 0.844474	test: 0.791565
MAE train: 0.444044	val: 0.636273	test: 0.632160

Epoch: 64
Loss: 0.3534526030222575
RMSE train: 0.598227	val: 0.859737	test: 0.805594
MAE train: 0.464368	val: 0.649781	test: 0.648067

Epoch: 65
Loss: 0.33737319707870483
RMSE train: 0.508033	val: 0.800487	test: 0.740059
MAE train: 0.393834	val: 0.591595	test: 0.587734

Epoch: 66
Loss: 0.3498913248380025
RMSE train: 0.541481	val: 0.828543	test: 0.773402
MAE train: 0.420418	val: 0.618580	test: 0.614956

Epoch: 67
Loss: 0.32807624340057373
RMSE train: 0.558945	val: 0.837440	test: 0.788656
MAE train: 0.432371	val: 0.623457	test: 0.624790

Epoch: 68
Loss: 0.34807585924863815
RMSE train: 0.564425	val: 0.849058	test: 0.785397
MAE train: 0.440896	val: 0.637810	test: 0.624512

Epoch: 69
Loss: 0.3235561400651932
RMSE train: 0.580931	val: 0.890826	test: 0.832385
MAE train: 0.455969	val: 0.668805	test: 0.663780

Epoch: 70
Loss: 0.3420447458823522
RMSE train: 0.570788	val: 0.865443	test: 0.812488
MAE train: 0.444965	val: 0.651617	test: 0.652583

Epoch: 71
Loss: 0.32110897327462834
RMSE train: 0.507211	val: 0.796834	test: 0.754312
MAE train: 0.392781	val: 0.592576	test: 0.594495

Epoch: 72
Loss: 0.3256764734784762
RMSE train: 0.542297	val: 0.833508	test: 0.781652
MAE train: 0.420177	val: 0.628132	test: 0.622141

Epoch: 73
Loss: 0.3108345853785674
RMSE train: 0.593108	val: 0.864722	test: 0.813149
MAE train: 0.460672	val: 0.658202	test: 0.646150

Epoch: 74
Loss: 0.31266481180985767
RMSE train: 0.592887	val: 0.877620	test: 0.828041
MAE train: 0.461858	val: 0.665832	test: 0.657680

Epoch: 75
Loss: 0.319033145904541
RMSE train: 0.527337	val: 0.829103	test: 0.779845
MAE train: 0.408901	val: 0.617453	test: 0.618143

Epoch: 76
Loss: 0.321245402097702
RMSE train: 0.574719	val: 0.844057	test: 0.803587
MAE train: 0.448680	val: 0.645406	test: 0.636787

Epoch: 77
Loss: 0.3152317429582278
RMSE train: 0.499817	val: 0.786633	test: 0.743727
MAE train: 0.387291	val: 0.588322	test: 0.585257

Epoch: 78
Loss: 0.30499516675869626
RMSE train: 0.551175	val: 0.831385	test: 0.778998
MAE train: 0.428319	val: 0.622298	test: 0.616794

Epoch: 79
Loss: 0.30509646733601886
RMSE train: 0.497095	val: 0.802778	test: 0.761243
MAE train: 0.382272	val: 0.592764	test: 0.596654

Epoch: 80
Loss: 0.31208979338407516
RMSE train: 0.512294	val: 0.807085	test: 0.762336
MAE train: 0.397275	val: 0.606574	test: 0.605878

Epoch: 81
Loss: 0.30459411938985187
RMSE train: 0.512959	val: 0.821354	test: 0.769759
MAE train: 0.398622	val: 0.609061	test: 0.606606

Epoch: 82
Loss: 0.30130478491385776
RMSE train: 0.539650	val: 0.823267	test: 0.774219
MAE train: 0.419008	val: 0.620535	test: 0.612555

Epoch: 83
Loss: 0.2923488070567449
RMSE train: 0.509172	val: 0.824334	test: 0.764750

Epoch: 23
Loss: 0.5624220882143293
RMSE train: 0.679980	val: 0.838901	test: 0.807214
MAE train: 0.530948	val: 0.652232	test: 0.646756

Epoch: 24
Loss: 0.564747514469283
RMSE train: 0.677026	val: 0.837006	test: 0.810657
MAE train: 0.534877	val: 0.656450	test: 0.656256

Epoch: 25
Loss: 0.5521465710231236
RMSE train: 0.668462	val: 0.838272	test: 0.805877
MAE train: 0.525070	val: 0.654927	test: 0.654813

Epoch: 26
Loss: 0.516388652580125
RMSE train: 0.657345	val: 0.831348	test: 0.799580
MAE train: 0.514364	val: 0.646946	test: 0.642266

Epoch: 27
Loss: 0.547503507563046
RMSE train: 0.695053	val: 0.873133	test: 0.818445
MAE train: 0.539981	val: 0.679786	test: 0.662738

Epoch: 28
Loss: 0.5230013025658471
RMSE train: 0.653899	val: 0.841267	test: 0.795421
MAE train: 0.509554	val: 0.655622	test: 0.637559

Epoch: 29
Loss: 0.4995255172252655
RMSE train: 0.656454	val: 0.833206	test: 0.811584
MAE train: 0.514899	val: 0.651311	test: 0.653029

Epoch: 30
Loss: 0.4923726235117231
RMSE train: 0.646317	val: 0.828961	test: 0.807442
MAE train: 0.504753	val: 0.642237	test: 0.649497

Epoch: 31
Loss: 0.504055746964046
RMSE train: 0.672401	val: 0.856710	test: 0.820427
MAE train: 0.520473	val: 0.659769	test: 0.662592

Epoch: 32
Loss: 0.49286114956651417
RMSE train: 0.652340	val: 0.841155	test: 0.804601
MAE train: 0.505447	val: 0.648796	test: 0.653393

Epoch: 33
Loss: 0.5039080211094448
RMSE train: 0.628201	val: 0.823642	test: 0.796940
MAE train: 0.486085	val: 0.635364	test: 0.644533

Epoch: 34
Loss: 0.49954521656036377
RMSE train: 0.648759	val: 0.840023	test: 0.812902
MAE train: 0.503140	val: 0.646965	test: 0.652426

Epoch: 35
Loss: 0.5027678736618587
RMSE train: 0.647153	val: 0.837663	test: 0.807671
MAE train: 0.504321	val: 0.646275	test: 0.643604

Epoch: 36
Loss: 0.4628383708851678
RMSE train: 0.617435	val: 0.813707	test: 0.780835
MAE train: 0.478659	val: 0.620756	test: 0.625595

Epoch: 37
Loss: 0.4675265976360866
RMSE train: 0.635978	val: 0.830946	test: 0.805758
MAE train: 0.497198	val: 0.631631	test: 0.647474

Epoch: 38
Loss: 0.4772054638181414
RMSE train: 0.620440	val: 0.823425	test: 0.791774
MAE train: 0.482973	val: 0.629925	test: 0.638263

Epoch: 39
Loss: 0.4684765807219914
RMSE train: 0.613212	val: 0.813914	test: 0.792342
MAE train: 0.476435	val: 0.623192	test: 0.638297

Epoch: 40
Loss: 0.4364769948380334
RMSE train: 0.620468	val: 0.814255	test: 0.779621
MAE train: 0.483779	val: 0.622622	test: 0.630880

Epoch: 41
Loss: 0.45474845809595926
RMSE train: 0.604430	val: 0.817229	test: 0.785526
MAE train: 0.467432	val: 0.628929	test: 0.630579

Epoch: 42
Loss: 0.46023682398455484
RMSE train: 0.632500	val: 0.825692	test: 0.800222
MAE train: 0.489910	val: 0.637740	test: 0.640211

Epoch: 43
Loss: 0.42845763691834043
RMSE train: 0.584833	val: 0.798968	test: 0.772051
MAE train: 0.451784	val: 0.610133	test: 0.614446

Epoch: 44
Loss: 0.42688801671777454
RMSE train: 0.600983	val: 0.819439	test: 0.784825
MAE train: 0.470464	val: 0.628093	test: 0.627178

Epoch: 45
Loss: 0.4236556078706469
RMSE train: 0.593224	val: 0.799887	test: 0.782528
MAE train: 0.458927	val: 0.608914	test: 0.633911

Epoch: 46
Loss: 0.41814467523779186
RMSE train: 0.603692	val: 0.818839	test: 0.785927
MAE train: 0.465502	val: 0.629057	test: 0.629644

Epoch: 47
Loss: 0.4311937264033726
RMSE train: 0.600473	val: 0.790984	test: 0.781283
MAE train: 0.470695	val: 0.613258	test: 0.622944

Epoch: 48
Loss: 0.4356613670076643
RMSE train: 0.606685	val: 0.828289	test: 0.797127
MAE train: 0.472630	val: 0.638994	test: 0.638711

Epoch: 49
Loss: 0.4382002353668213
RMSE train: 0.610221	val: 0.815356	test: 0.788391
MAE train: 0.479635	val: 0.620113	test: 0.625863

Epoch: 50
Loss: 0.42725133683000294
RMSE train: 0.582801	val: 0.790842	test: 0.771285
MAE train: 0.455421	val: 0.601315	test: 0.607358

Epoch: 51
Loss: 0.4070420265197754
RMSE train: 0.604364	val: 0.799455	test: 0.792049
MAE train: 0.472664	val: 0.604129	test: 0.637684

Epoch: 52
Loss: 0.40510134186063496
RMSE train: 0.594601	val: 0.823233	test: 0.776502
MAE train: 0.460530	val: 0.632058	test: 0.623218

Epoch: 53
Loss: 0.392957295690264
RMSE train: 0.576527	val: 0.811806	test: 0.779160
MAE train: 0.446205	val: 0.622283	test: 0.625397

Epoch: 54
Loss: 0.3894574280296053
RMSE train: 0.564283	val: 0.786023	test: 0.762496
MAE train: 0.440077	val: 0.600579	test: 0.612201

Epoch: 55
Loss: 0.40681288072041105
RMSE train: 0.585780	val: 0.790798	test: 0.776810
MAE train: 0.458179	val: 0.605711	test: 0.630879

Epoch: 56
Loss: 0.3881137328488486
RMSE train: 0.577366	val: 0.786799	test: 0.777472
MAE train: 0.452510	val: 0.601104	test: 0.617360

Epoch: 57
Loss: 0.38454153495175497
RMSE train: 0.565128	val: 0.790460	test: 0.763493
MAE train: 0.438450	val: 0.597193	test: 0.605690

Epoch: 58
Loss: 0.37288365406649454
RMSE train: 0.589034	val: 0.827807	test: 0.795369
MAE train: 0.458141	val: 0.634658	test: 0.632188

Epoch: 59
Loss: 0.3694371495928083
RMSE train: 0.565055	val: 0.792796	test: 0.768621
MAE train: 0.439610	val: 0.603939	test: 0.610759

Epoch: 60
Loss: 0.3804721917424883
RMSE train: 0.551976	val: 0.789527	test: 0.763802
MAE train: 0.431257	val: 0.602123	test: 0.607569

Epoch: 61
Loss: 0.37656635684626444
RMSE train: 0.570057	val: 0.790363	test: 0.770462
MAE train: 0.445285	val: 0.600568	test: 0.617122

Epoch: 62
Loss: 0.37562412662165506
RMSE train: 0.548717	val: 0.796644	test: 0.764042
MAE train: 0.425113	val: 0.605117	test: 0.608963

Epoch: 63
Loss: 0.4091520096574511
RMSE train: 0.569004	val: 0.782656	test: 0.775845
MAE train: 0.443462	val: 0.598571	test: 0.619894

Epoch: 64
Loss: 0.3790854236909321
RMSE train: 0.598033	val: 0.806796	test: 0.777554
MAE train: 0.473117	val: 0.616998	test: 0.627536

Epoch: 65
Loss: 0.3605682296412332
RMSE train: 0.565614	val: 0.810326	test: 0.761877
MAE train: 0.437611	val: 0.626669	test: 0.598998

Epoch: 66
Loss: 0.3897396858249392
RMSE train: 0.531733	val: 0.795108	test: 0.748992
MAE train: 0.408656	val: 0.594381	test: 0.586722

Epoch: 67
Loss: 0.3762057508741106
RMSE train: 0.565786	val: 0.787266	test: 0.769464
MAE train: 0.442176	val: 0.595962	test: 0.615384

Epoch: 68
Loss: 0.3725885621138981
RMSE train: 0.539223	val: 0.791979	test: 0.754991
MAE train: 0.418135	val: 0.589568	test: 0.599267

Epoch: 69
Loss: 0.37758259900978636
RMSE train: 0.529583	val: 0.776746	test: 0.754286
MAE train: 0.410182	val: 0.584224	test: 0.599219

Epoch: 70
Loss: 0.39778207881110056
RMSE train: 0.543849	val: 0.791869	test: 0.759145
MAE train: 0.421449	val: 0.600280	test: 0.604458

Epoch: 71
Loss: 0.3898227406399591
RMSE train: 0.563950	val: 0.824566	test: 0.773137
MAE train: 0.436313	val: 0.618743	test: 0.618078

Epoch: 72
Loss: 0.33988235145807266
RMSE train: 0.557686	val: 0.813740	test: 0.760143
MAE train: 0.434481	val: 0.611375	test: 0.608297

Epoch: 73
Loss: 0.34762679253305706
RMSE train: 0.530185	val: 0.802788	test: 0.771969
MAE train: 0.410271	val: 0.606249	test: 0.609528

Epoch: 74
Loss: 0.36348635384014677
RMSE train: 0.533081	val: 0.779454	test: 0.752811
MAE train: 0.413096	val: 0.585165	test: 0.596171

Epoch: 75
Loss: 0.35480726616723196
RMSE train: 0.534739	val: 0.790964	test: 0.750247
MAE train: 0.417066	val: 0.593441	test: 0.593344

Epoch: 76
Loss: 0.34425291631902966
RMSE train: 0.523741	val: 0.783303	test: 0.750776
MAE train: 0.405925	val: 0.588138	test: 0.596199

Epoch: 77
Loss: 0.3417279635156904
RMSE train: 0.527847	val: 0.780307	test: 0.752152
MAE train: 0.406653	val: 0.593648	test: 0.592637

Epoch: 78
Loss: 0.33482763171195984
RMSE train: 0.535087	val: 0.793218	test: 0.766935
MAE train: 0.414499	val: 0.591707	test: 0.606922

Epoch: 79
Loss: 0.3537742495536804
RMSE train: 0.517180	val: 0.794804	test: 0.766722
MAE train: 0.401677	val: 0.591056	test: 0.607141

Epoch: 80
Loss: 0.32913810653345926
RMSE train: 0.527445	val: 0.785787	test: 0.760835
MAE train: 0.405414	val: 0.593208	test: 0.596077

Epoch: 81
Loss: 0.32026869697230204
RMSE train: 0.514731	val: 0.786416	test: 0.750656
MAE train: 0.398341	val: 0.592270	test: 0.592313

Epoch: 82
Loss: 0.31949480942317415
RMSE train: 0.524536	val: 0.806459	test: 0.761560
MAE train: 0.405241	val: 0.606692	test: 0.604199

Epoch: 83
Loss: 0.32905541360378265
RMSE train: 0.511274	val: 0.790061	test: 0.752001

Epoch: 23
Loss: 0.5983345274414335
RMSE train: 0.678894	val: 0.816205	test: 0.786930
MAE train: 0.530083	val: 0.633417	test: 0.629107

Epoch: 24
Loss: 0.5475778409412929
RMSE train: 0.677098	val: 0.825918	test: 0.777647
MAE train: 0.528521	val: 0.648403	test: 0.620342

Epoch: 25
Loss: 0.5343524558203561
RMSE train: 0.688899	val: 0.833561	test: 0.792973
MAE train: 0.535946	val: 0.646623	test: 0.638841

Epoch: 26
Loss: 0.5570936650037766
RMSE train: 0.670842	val: 0.829467	test: 0.781958
MAE train: 0.519970	val: 0.648344	test: 0.631887

Epoch: 27
Loss: 0.5229640092168536
RMSE train: 0.638370	val: 0.806859	test: 0.773656
MAE train: 0.491043	val: 0.622277	test: 0.611466

Epoch: 28
Loss: 0.5276861169508525
RMSE train: 0.661230	val: 0.829694	test: 0.780032
MAE train: 0.509987	val: 0.642004	test: 0.621981

Epoch: 29
Loss: 0.5098860902445657
RMSE train: 0.643529	val: 0.811391	test: 0.772829
MAE train: 0.495786	val: 0.622112	test: 0.620639

Epoch: 30
Loss: 0.5003000604254859
RMSE train: 0.681858	val: 0.849739	test: 0.794811
MAE train: 0.525856	val: 0.661494	test: 0.641066

Epoch: 31
Loss: 0.49184501809733255
RMSE train: 0.628628	val: 0.801362	test: 0.771061
MAE train: 0.484313	val: 0.617284	test: 0.617050

Epoch: 32
Loss: 0.49278109414236887
RMSE train: 0.664409	val: 0.836463	test: 0.801077
MAE train: 0.516965	val: 0.647977	test: 0.645554

Epoch: 33
Loss: 0.4921420557158334
RMSE train: 0.622586	val: 0.789322	test: 0.780450
MAE train: 0.486234	val: 0.607948	test: 0.626197

Epoch: 34
Loss: 0.4919527236904417
RMSE train: 0.652731	val: 0.848213	test: 0.801179
MAE train: 0.506100	val: 0.659120	test: 0.641536

Epoch: 35
Loss: 0.4761846363544464
RMSE train: 0.628906	val: 0.804186	test: 0.791088
MAE train: 0.488698	val: 0.614459	test: 0.630809

Epoch: 36
Loss: 0.49542304234845297
RMSE train: 0.632444	val: 0.810384	test: 0.789824
MAE train: 0.488422	val: 0.623875	test: 0.636021

Epoch: 37
Loss: 0.505023119705064
RMSE train: 0.616202	val: 0.790675	test: 0.770122
MAE train: 0.476634	val: 0.609428	test: 0.615868

Epoch: 38
Loss: 0.46890924658094135
RMSE train: 0.614415	val: 0.789865	test: 0.773123
MAE train: 0.473699	val: 0.607954	test: 0.617158

Epoch: 39
Loss: 0.47362526825496126
RMSE train: 0.639341	val: 0.825631	test: 0.795068
MAE train: 0.499291	val: 0.636910	test: 0.626584

Epoch: 40
Loss: 0.4409951184477125
RMSE train: 0.594153	val: 0.781182	test: 0.761798
MAE train: 0.458275	val: 0.594670	test: 0.607476

Epoch: 41
Loss: 0.4793030491897038
RMSE train: 0.604010	val: 0.796530	test: 0.783914
MAE train: 0.465800	val: 0.609567	test: 0.627103

Epoch: 42
Loss: 0.43777577579021454
RMSE train: 0.590292	val: 0.783233	test: 0.757584
MAE train: 0.454987	val: 0.600699	test: 0.608807

Epoch: 43
Loss: 0.43394053620951517
RMSE train: 0.616311	val: 0.796694	test: 0.776640
MAE train: 0.475369	val: 0.605506	test: 0.624181

Epoch: 44
Loss: 0.42943369703633444
RMSE train: 0.596115	val: 0.791953	test: 0.767829
MAE train: 0.460371	val: 0.603565	test: 0.615170

Epoch: 45
Loss: 0.44169419365269796
RMSE train: 0.592210	val: 0.780880	test: 0.765625
MAE train: 0.461725	val: 0.595557	test: 0.608974

Epoch: 46
Loss: 0.4350333384105137
RMSE train: 0.600259	val: 0.798891	test: 0.775920
MAE train: 0.467696	val: 0.615562	test: 0.608007

Epoch: 47
Loss: 0.41624176289354053
RMSE train: 0.592325	val: 0.800096	test: 0.767323
MAE train: 0.457359	val: 0.611944	test: 0.607893

Epoch: 48
Loss: 0.40304394917828695
RMSE train: 0.604055	val: 0.813913	test: 0.769322
MAE train: 0.467518	val: 0.623944	test: 0.612456

Epoch: 49
Loss: 0.41427854129246305
RMSE train: 0.569194	val: 0.782503	test: 0.762443
MAE train: 0.439919	val: 0.596609	test: 0.600920

Epoch: 50
Loss: 0.40969324324812206
RMSE train: 0.571496	val: 0.779636	test: 0.769879
MAE train: 0.443528	val: 0.597811	test: 0.614378

Epoch: 51
Loss: 0.3927199436085565
RMSE train: 0.585262	val: 0.786905	test: 0.774451
MAE train: 0.451334	val: 0.597802	test: 0.614588

Epoch: 52
Loss: 0.39898037697587696
RMSE train: 0.574406	val: 0.766092	test: 0.758320
MAE train: 0.443302	val: 0.587552	test: 0.599073

Epoch: 53
Loss: 0.4043607307331903
RMSE train: 0.570465	val: 0.769877	test: 0.767000
MAE train: 0.442752	val: 0.588940	test: 0.605961

Epoch: 54
Loss: 0.40027261844703127
RMSE train: 0.610551	val: 0.823181	test: 0.800564
MAE train: 0.470826	val: 0.624451	test: 0.644486

Epoch: 55
Loss: 0.3987542284386499
RMSE train: 0.566105	val: 0.783518	test: 0.762386
MAE train: 0.441016	val: 0.607669	test: 0.595115

Epoch: 56
Loss: 0.404085174202919
RMSE train: 0.562092	val: 0.764503	test: 0.752493
MAE train: 0.437535	val: 0.589508	test: 0.590786

Epoch: 57
Loss: 0.3791940999882562
RMSE train: 0.560813	val: 0.784803	test: 0.765392
MAE train: 0.433582	val: 0.595527	test: 0.600726

Epoch: 58
Loss: 0.3872724792787007
RMSE train: 0.555279	val: 0.766504	test: 0.748481
MAE train: 0.430455	val: 0.585556	test: 0.589679

Epoch: 59
Loss: 0.3966280094214848
RMSE train: 0.568914	val: 0.790521	test: 0.760086
MAE train: 0.439595	val: 0.606066	test: 0.599081

Epoch: 60
Loss: 0.3816155620983669
RMSE train: 0.553874	val: 0.771413	test: 0.760629
MAE train: 0.424734	val: 0.587794	test: 0.601644

Epoch: 61
Loss: 0.3920388328177588
RMSE train: 0.552801	val: 0.767056	test: 0.760959
MAE train: 0.428991	val: 0.584852	test: 0.605535

Epoch: 62
Loss: 0.3864688553980419
RMSE train: 0.539176	val: 0.756731	test: 0.752187
MAE train: 0.416232	val: 0.580367	test: 0.596030

Epoch: 63
Loss: 0.3744085452386311
RMSE train: 0.551493	val: 0.776576	test: 0.773823
MAE train: 0.424099	val: 0.584368	test: 0.611946

Epoch: 64
Loss: 0.3683086207934788
RMSE train: 0.551900	val: 0.767394	test: 0.754665
MAE train: 0.428532	val: 0.583653	test: 0.594072

Epoch: 65
Loss: 0.38628350624016355
RMSE train: 0.588152	val: 0.807761	test: 0.783322
MAE train: 0.453282	val: 0.608282	test: 0.624914

Epoch: 66
Loss: 0.3608128662620272
RMSE train: 0.546618	val: 0.786479	test: 0.754196
MAE train: 0.422358	val: 0.593639	test: 0.593962

Epoch: 67
Loss: 0.33937186002731323
RMSE train: 0.542741	val: 0.778104	test: 0.764408
MAE train: 0.418587	val: 0.595697	test: 0.595122

Epoch: 68
Loss: 0.3527087015765054
RMSE train: 0.534385	val: 0.771534	test: 0.762510
MAE train: 0.412768	val: 0.580227	test: 0.598479

Epoch: 69
Loss: 0.34658530141626087
RMSE train: 0.516789	val: 0.758827	test: 0.757047
MAE train: 0.398651	val: 0.573965	test: 0.592034

Epoch: 70
Loss: 0.34457020248685566
RMSE train: 0.524257	val: 0.770344	test: 0.753583
MAE train: 0.404139	val: 0.583546	test: 0.592895

Epoch: 71
Loss: 0.36670627125671934
RMSE train: 0.512385	val: 0.759656	test: 0.750954
MAE train: 0.397272	val: 0.576270	test: 0.585614

Epoch: 72
Loss: 0.36213414796761106
RMSE train: 0.532793	val: 0.774332	test: 0.774697
MAE train: 0.411042	val: 0.589744	test: 0.604677

Epoch: 73
Loss: 0.3428160867520741
RMSE train: 0.535011	val: 0.784004	test: 0.771657
MAE train: 0.409975	val: 0.597273	test: 0.591603

Epoch: 74
Loss: 0.3382658234664372
RMSE train: 0.519500	val: 0.767294	test: 0.758028
MAE train: 0.400279	val: 0.577252	test: 0.590894

Epoch: 75
Loss: 0.3257843873330525
RMSE train: 0.497068	val: 0.749567	test: 0.741660
MAE train: 0.379767	val: 0.568356	test: 0.576078

Epoch: 76
Loss: 0.33609652732099804
RMSE train: 0.532899	val: 0.779484	test: 0.751132
MAE train: 0.413826	val: 0.596934	test: 0.589836

Epoch: 77
Loss: 0.3415316641330719
RMSE train: 0.517865	val: 0.758615	test: 0.761160
MAE train: 0.399614	val: 0.577422	test: 0.589322

Epoch: 78
Loss: 0.3182272176657404
RMSE train: 0.514896	val: 0.760727	test: 0.742220
MAE train: 0.397078	val: 0.584291	test: 0.579607

Epoch: 79
Loss: 0.32806646611009327
RMSE train: 0.514289	val: 0.762442	test: 0.760853
MAE train: 0.398375	val: 0.574814	test: 0.592334

Epoch: 80
Loss: 0.34050086566380094
RMSE train: 0.532386	val: 0.782294	test: 0.763905
MAE train: 0.411351	val: 0.592976	test: 0.602112

Epoch: 81
Loss: 0.3361199306590216
RMSE train: 0.518813	val: 0.760924	test: 0.761897
MAE train: 0.402676	val: 0.583903	test: 0.598888

Epoch: 82
Loss: 0.33124651653426035
RMSE train: 0.511850	val: 0.755111	test: 0.758064
MAE train: 0.395141	val: 0.576539	test: 0.586264

Epoch: 83
Loss: 0.3150150967495782
RMSE train: 0.518559	val: 0.769018	test: 0.759893

Epoch: 23
Loss: 0.5393422756876264
RMSE train: 0.680450	val: 0.848482	test: 0.785664
MAE train: 0.530089	val: 0.663151	test: 0.631793

Epoch: 24
Loss: 0.5395183222634452
RMSE train: 0.694730	val: 0.856160	test: 0.791439
MAE train: 0.537427	val: 0.673192	test: 0.633373

Epoch: 25
Loss: 0.5574497388941901
RMSE train: 0.697021	val: 0.880415	test: 0.806762
MAE train: 0.543388	val: 0.682050	test: 0.653614

Epoch: 26
Loss: 0.5201194733381271
RMSE train: 0.673267	val: 0.868802	test: 0.795356
MAE train: 0.522449	val: 0.671034	test: 0.634514

Epoch: 27
Loss: 0.5142714253493718
RMSE train: 0.671744	val: 0.857163	test: 0.785751
MAE train: 0.522980	val: 0.658142	test: 0.636336

Epoch: 28
Loss: 0.5177082163946969
RMSE train: 0.673945	val: 0.873657	test: 0.808279
MAE train: 0.525226	val: 0.664251	test: 0.655411

Epoch: 29
Loss: 0.4962563876594816
RMSE train: 0.652400	val: 0.829450	test: 0.777402
MAE train: 0.506021	val: 0.635792	test: 0.622584

Epoch: 30
Loss: 0.5438610485621861
RMSE train: 0.643067	val: 0.844557	test: 0.773758
MAE train: 0.500063	val: 0.645160	test: 0.629950

Epoch: 31
Loss: 0.5540145401443753
RMSE train: 0.675998	val: 0.839914	test: 0.784245
MAE train: 0.527474	val: 0.647932	test: 0.633954

Epoch: 32
Loss: 0.4909554549625942
RMSE train: 0.676788	val: 0.858075	test: 0.803785
MAE train: 0.525676	val: 0.669546	test: 0.643631

Epoch: 33
Loss: 0.5216759102685111
RMSE train: 0.632674	val: 0.804925	test: 0.765099
MAE train: 0.492642	val: 0.619958	test: 0.615468

Epoch: 34
Loss: 0.501855051943234
RMSE train: 0.641059	val: 0.836197	test: 0.769588
MAE train: 0.500414	val: 0.641966	test: 0.617320

Epoch: 35
Loss: 0.48216909383024487
RMSE train: 0.636096	val: 0.836543	test: 0.763680
MAE train: 0.495527	val: 0.635699	test: 0.610786

Epoch: 36
Loss: 0.46028830323900494
RMSE train: 0.635393	val: 0.836628	test: 0.765744
MAE train: 0.492728	val: 0.632004	test: 0.612984

Epoch: 37
Loss: 0.49014400584357126
RMSE train: 0.613957	val: 0.825935	test: 0.755973
MAE train: 0.477009	val: 0.620653	test: 0.609765

Epoch: 38
Loss: 0.46861158524240765
RMSE train: 0.622013	val: 0.805567	test: 0.767066
MAE train: 0.486720	val: 0.621765	test: 0.614408

Epoch: 39
Loss: 0.45366195482867105
RMSE train: 0.669730	val: 0.874077	test: 0.815680
MAE train: 0.519861	val: 0.668907	test: 0.654990

Epoch: 40
Loss: 0.46697812633855
RMSE train: 0.624342	val: 0.828638	test: 0.767637
MAE train: 0.486539	val: 0.623046	test: 0.614573

Epoch: 41
Loss: 0.4409728923014232
RMSE train: 0.621442	val: 0.820647	test: 0.767590
MAE train: 0.485614	val: 0.628055	test: 0.620366

Epoch: 42
Loss: 0.42289696633815765
RMSE train: 0.605064	val: 0.794670	test: 0.770249
MAE train: 0.472759	val: 0.605857	test: 0.612125

Epoch: 43
Loss: 0.43406604656151365
RMSE train: 0.616020	val: 0.805474	test: 0.778194
MAE train: 0.481706	val: 0.618795	test: 0.626454

Epoch: 44
Loss: 0.4155967043978827
RMSE train: 0.599426	val: 0.793354	test: 0.764397
MAE train: 0.465012	val: 0.610053	test: 0.619578

Epoch: 45
Loss: 0.4400451268468584
RMSE train: 0.625775	val: 0.807926	test: 0.772064
MAE train: 0.486486	val: 0.625357	test: 0.626329

Epoch: 46
Loss: 0.42413065688950674
RMSE train: 0.611246	val: 0.815091	test: 0.766610
MAE train: 0.475027	val: 0.632727	test: 0.620071

Epoch: 47
Loss: 0.41600597756249563
RMSE train: 0.608568	val: 0.813783	test: 0.772640
MAE train: 0.473645	val: 0.618355	test: 0.624893

Epoch: 48
Loss: 0.4350335342543466
RMSE train: 0.611767	val: 0.835256	test: 0.777871
MAE train: 0.477415	val: 0.642413	test: 0.627570

Epoch: 49
Loss: 0.3985227921179363
RMSE train: 0.585646	val: 0.804769	test: 0.755334
MAE train: 0.456333	val: 0.608391	test: 0.613539

Epoch: 50
Loss: 0.4081229716539383
RMSE train: 0.589938	val: 0.808939	test: 0.763332
MAE train: 0.459969	val: 0.623050	test: 0.600122

Epoch: 51
Loss: 0.4086039385625294
RMSE train: 0.596454	val: 0.804631	test: 0.767207
MAE train: 0.462745	val: 0.613008	test: 0.611013

Epoch: 52
Loss: 0.3986542139734541
RMSE train: 0.585511	val: 0.804215	test: 0.757414
MAE train: 0.456341	val: 0.611959	test: 0.607483

Epoch: 53
Loss: 0.39756317010947634
RMSE train: 0.577197	val: 0.798195	test: 0.743319
MAE train: 0.447409	val: 0.613029	test: 0.594181

Epoch: 54
Loss: 0.4000134382929121
RMSE train: 0.588799	val: 0.795776	test: 0.757183
MAE train: 0.458508	val: 0.610058	test: 0.609591

Epoch: 55
Loss: 0.3826671157564436
RMSE train: 0.560218	val: 0.775528	test: 0.740201
MAE train: 0.432619	val: 0.596743	test: 0.593657

Epoch: 56
Loss: 0.39512115929807934
RMSE train: 0.583225	val: 0.794317	test: 0.754071
MAE train: 0.458055	val: 0.613027	test: 0.607469

Epoch: 57
Loss: 0.39408111572265625
RMSE train: 0.581046	val: 0.794032	test: 0.763125
MAE train: 0.451428	val: 0.609759	test: 0.605219

Epoch: 58
Loss: 0.3903981936829431
RMSE train: 0.581916	val: 0.789148	test: 0.748313
MAE train: 0.455603	val: 0.607110	test: 0.600393

Epoch: 59
Loss: 0.384757684809821
RMSE train: 0.574657	val: 0.789014	test: 0.754223
MAE train: 0.444727	val: 0.604711	test: 0.613462

Epoch: 60
Loss: 0.3758520760706493
RMSE train: 0.551530	val: 0.787141	test: 0.750332
MAE train: 0.423218	val: 0.602617	test: 0.600525

Epoch: 61
Loss: 0.360582702926227
RMSE train: 0.562782	val: 0.794254	test: 0.762831
MAE train: 0.435611	val: 0.609989	test: 0.613516

Epoch: 62
Loss: 0.3754218689032963
RMSE train: 0.574624	val: 0.805008	test: 0.765196
MAE train: 0.448091	val: 0.620889	test: 0.617080

Epoch: 63
Loss: 0.3623062478644507
RMSE train: 0.548483	val: 0.786388	test: 0.748663
MAE train: 0.423665	val: 0.599823	test: 0.600792

Epoch: 64
Loss: 0.3626849225589207
RMSE train: 0.559091	val: 0.795917	test: 0.741571
MAE train: 0.436911	val: 0.613391	test: 0.592660

Epoch: 65
Loss: 0.3494052929537637
RMSE train: 0.552289	val: 0.785469	test: 0.743322
MAE train: 0.430590	val: 0.595441	test: 0.600620

Epoch: 66
Loss: 0.3551940641232899
RMSE train: 0.549725	val: 0.800944	test: 0.747991
MAE train: 0.427399	val: 0.605146	test: 0.594004

Epoch: 67
Loss: 0.351201451250485
RMSE train: 0.544294	val: 0.793456	test: 0.749099
MAE train: 0.425888	val: 0.602747	test: 0.599624

Epoch: 68
Loss: 0.3584820734603064
RMSE train: 0.575540	val: 0.819131	test: 0.755056
MAE train: 0.446845	val: 0.624746	test: 0.606647

Epoch: 69
Loss: 0.3700313802276339
RMSE train: 0.558926	val: 0.797411	test: 0.745948
MAE train: 0.437297	val: 0.610635	test: 0.593274

Epoch: 70
Loss: 0.35056533558028086
RMSE train: 0.541242	val: 0.788615	test: 0.740523
MAE train: 0.418188	val: 0.599870	test: 0.581656

Epoch: 71
Loss: 0.36590246856212616
RMSE train: 0.530273	val: 0.776180	test: 0.739116
MAE train: 0.413380	val: 0.586478	test: 0.583386

Epoch: 72
Loss: 0.34565928152629305
RMSE train: 0.553257	val: 0.779793	test: 0.746065
MAE train: 0.430507	val: 0.598449	test: 0.598379

Epoch: 73
Loss: 0.355052045413426
RMSE train: 0.543661	val: 0.797397	test: 0.763749
MAE train: 0.422170	val: 0.608632	test: 0.612879

Epoch: 74
Loss: 0.3467577759708677
RMSE train: 0.541500	val: 0.796652	test: 0.750633
MAE train: 0.420738	val: 0.597381	test: 0.599438

Epoch: 75
Loss: 0.3359399735927582
RMSE train: 0.524851	val: 0.776600	test: 0.745871
MAE train: 0.405882	val: 0.586155	test: 0.596638

Epoch: 76
Loss: 0.3421685759510313
RMSE train: 0.549322	val: 0.793327	test: 0.749016
MAE train: 0.425492	val: 0.606498	test: 0.604298

Epoch: 77
Loss: 0.341203436255455
RMSE train: 0.552519	val: 0.791140	test: 0.763549
MAE train: 0.426876	val: 0.603606	test: 0.609786

Epoch: 78
Loss: 0.32913135630743845
RMSE train: 0.529285	val: 0.767429	test: 0.741309
MAE train: 0.411983	val: 0.584492	test: 0.589872

Epoch: 79
Loss: 0.32928467861243654
RMSE train: 0.538464	val: 0.783771	test: 0.753082
MAE train: 0.417966	val: 0.599573	test: 0.589868

Epoch: 80
Loss: 0.3250110713498933
RMSE train: 0.518849	val: 0.771509	test: 0.742646
MAE train: 0.404337	val: 0.589562	test: 0.593594

Epoch: 81
Loss: 0.3201524998460497
RMSE train: 0.518967	val: 0.782472	test: 0.746174
MAE train: 0.403823	val: 0.598106	test: 0.589324

Epoch: 82
Loss: 0.3180630419935499
RMSE train: 0.520708	val: 0.792456	test: 0.755727
MAE train: 0.405353	val: 0.602286	test: 0.607222

Epoch: 83
Loss: 0.3185683403696333
RMSE train: 0.513400	val: 0.777696	test: 0.739721
MAE train: 0.394265	val: 0.615284	test: 0.605719

Epoch: 84
Loss: 0.28749631221095723
RMSE train: 0.516687	val: 0.828265	test: 0.792247
MAE train: 0.403556	val: 0.624126	test: 0.632942

Epoch: 85
Loss: 0.2909214322765668
RMSE train: 0.516851	val: 0.828710	test: 0.791059
MAE train: 0.399512	val: 0.622168	test: 0.629796

Epoch: 86
Loss: 0.2826400858660539
RMSE train: 0.505975	val: 0.813303	test: 0.771325
MAE train: 0.392231	val: 0.606043	test: 0.614366

Epoch: 87
Loss: 0.2929345940550168
RMSE train: 0.503051	val: 0.806735	test: 0.753267
MAE train: 0.389363	val: 0.598503	test: 0.593127

Epoch: 88
Loss: 0.30596474806467694
RMSE train: 0.511322	val: 0.828911	test: 0.775099
MAE train: 0.395727	val: 0.616886	test: 0.609385

Epoch: 89
Loss: 0.2897307549913724
RMSE train: 0.517814	val: 0.832001	test: 0.774007
MAE train: 0.399729	val: 0.621055	test: 0.610011

Epoch: 90
Loss: 0.29472529888153076
RMSE train: 0.512437	val: 0.843349	test: 0.782541
MAE train: 0.396909	val: 0.630443	test: 0.621258

Epoch: 91
Loss: 0.2989150136709213
RMSE train: 0.506940	val: 0.825501	test: 0.773328
MAE train: 0.394194	val: 0.616141	test: 0.612298

Epoch: 92
Loss: 0.2827954366803169
RMSE train: 0.487405	val: 0.799217	test: 0.769297
MAE train: 0.375551	val: 0.595017	test: 0.610509

Epoch: 93
Loss: 0.2863481268286705
RMSE train: 0.482029	val: 0.805447	test: 0.749116
MAE train: 0.373791	val: 0.600564	test: 0.597250

Epoch: 94
Loss: 0.2808087654411793
RMSE train: 0.484558	val: 0.800624	test: 0.742045
MAE train: 0.374126	val: 0.592195	test: 0.588157

Epoch: 95
Loss: 0.2722136874993642
RMSE train: 0.479431	val: 0.792459	test: 0.736268
MAE train: 0.369241	val: 0.585382	test: 0.585445

Epoch: 96
Loss: 0.26622314378619194
RMSE train: 0.464328	val: 0.805873	test: 0.740681
MAE train: 0.358233	val: 0.593461	test: 0.585125

Epoch: 97
Loss: 0.27450481802225113
RMSE train: 0.471813	val: 0.804339	test: 0.750996
MAE train: 0.365189	val: 0.598120	test: 0.593357

Epoch: 98
Loss: 0.26843081042170525
RMSE train: 0.510298	val: 0.836838	test: 0.807040
MAE train: 0.393411	val: 0.632336	test: 0.637866

Epoch: 99
Loss: 0.2753911626835664
RMSE train: 0.501373	val: 0.836301	test: 0.788931
MAE train: 0.389212	val: 0.625810	test: 0.622006

Epoch: 100
Loss: 0.2713694969813029
RMSE train: 0.495301	val: 0.811888	test: 0.755004
MAE train: 0.383241	val: 0.603581	test: 0.598753

Epoch: 101
Loss: 0.2814380054672559
RMSE train: 0.446721	val: 0.792773	test: 0.743359
MAE train: 0.342708	val: 0.585579	test: 0.592145

Epoch: 102
Loss: 0.2665262135366599
RMSE train: 0.485858	val: 0.827428	test: 0.768039
MAE train: 0.375265	val: 0.620549	test: 0.608697

Epoch: 103
Loss: 0.2595100738108158
RMSE train: 0.465454	val: 0.805874	test: 0.751831
MAE train: 0.358991	val: 0.603905	test: 0.596940

Epoch: 104
Loss: 0.26427024354537326
RMSE train: 0.464293	val: 0.817256	test: 0.756998
MAE train: 0.359611	val: 0.616076	test: 0.599195

Epoch: 105
Loss: 0.26929116000731784
RMSE train: 0.487797	val: 0.836575	test: 0.759230
MAE train: 0.377508	val: 0.629988	test: 0.603557

Epoch: 106
Loss: 0.2621812770764033
RMSE train: 0.483855	val: 0.832595	test: 0.785226
MAE train: 0.375617	val: 0.626441	test: 0.625603

Epoch: 107
Loss: 0.26904776071508724
RMSE train: 0.463635	val: 0.806840	test: 0.754585
MAE train: 0.359309	val: 0.604894	test: 0.599307

Epoch: 108
Loss: 0.2536414861679077
RMSE train: 0.469167	val: 0.821168	test: 0.764206
MAE train: 0.363569	val: 0.611081	test: 0.604007

Epoch: 109
Loss: 0.2581673798461755
RMSE train: 0.513698	val: 0.846283	test: 0.783037
MAE train: 0.398697	val: 0.640816	test: 0.624237

Epoch: 110
Loss: 0.26349887748559314
RMSE train: 0.482692	val: 0.816042	test: 0.742787
MAE train: 0.374277	val: 0.613276	test: 0.592839

Epoch: 111
Loss: 0.25287962953249615
RMSE train: 0.458216	val: 0.805830	test: 0.750180
MAE train: 0.352359	val: 0.604104	test: 0.594662

Epoch: 112
Loss: 0.24880554775396982
RMSE train: 0.498652	val: 0.851345	test: 0.799150
MAE train: 0.389397	val: 0.646472	test: 0.634196

Epoch: 113
Loss: 0.24902291720112166
RMSE train: 0.469446	val: 0.816927	test: 0.767097
MAE train: 0.362418	val: 0.612546	test: 0.608434

Epoch: 114
Loss: 0.25786122555534047
RMSE train: 0.487334	val: 0.820763	test: 0.758445
MAE train: 0.377901	val: 0.615564	test: 0.601578

Epoch: 115
Loss: 0.25356778626640636
RMSE train: 0.473542	val: 0.817264	test: 0.755680
MAE train: 0.362682	val: 0.613033	test: 0.601308

Epoch: 116
Loss: 0.2489120177924633
RMSE train: 0.439805	val: 0.794156	test: 0.737338
MAE train: 0.339396	val: 0.593420	test: 0.582699

Epoch: 117
Loss: 0.25899652515848476
RMSE train: 0.472220	val: 0.814865	test: 0.761240
MAE train: 0.364614	val: 0.610573	test: 0.607289

Epoch: 118
Loss: 0.25688328842322034
RMSE train: 0.457726	val: 0.791401	test: 0.735881
MAE train: 0.350864	val: 0.590939	test: 0.582175

Epoch: 119
Loss: 0.24646212657292685
RMSE train: 0.472985	val: 0.825821	test: 0.767442
MAE train: 0.366267	val: 0.624705	test: 0.611293

Epoch: 120
Loss: 0.24661706387996674
RMSE train: 0.443967	val: 0.790791	test: 0.736790
MAE train: 0.342593	val: 0.593060	test: 0.585697

Epoch: 121
Loss: 0.23491780708233514
RMSE train: 0.454768	val: 0.806535	test: 0.752158
MAE train: 0.351822	val: 0.605559	test: 0.600060

Early stopping
Best (RMSE):	 train: 0.499817	val: 0.786633	test: 0.743727
Best (MAE):	 train: 0.387291	val: 0.588322	test: 0.585257

MAE train: 0.389847	val: 0.607304	test: 0.595002

Epoch: 84
Loss: 0.3003692875305812
RMSE train: 0.501724	val: 0.799858	test: 0.747344
MAE train: 0.391478	val: 0.600549	test: 0.596725

Epoch: 85
Loss: 0.295709565281868
RMSE train: 0.465917	val: 0.790301	test: 0.744583
MAE train: 0.362372	val: 0.594601	test: 0.584501

Epoch: 86
Loss: 0.2920602795978387
RMSE train: 0.514749	val: 0.821435	test: 0.794565
MAE train: 0.400247	val: 0.625276	test: 0.625650

Epoch: 87
Loss: 0.29074857383966446
RMSE train: 0.476682	val: 0.760316	test: 0.731144
MAE train: 0.371680	val: 0.578134	test: 0.572385

Epoch: 88
Loss: 0.2917582392692566
RMSE train: 0.467976	val: 0.783348	test: 0.742424
MAE train: 0.364003	val: 0.587081	test: 0.582862

Epoch: 89
Loss: 0.27878036846717197
RMSE train: 0.468950	val: 0.784632	test: 0.752332
MAE train: 0.362237	val: 0.590805	test: 0.587884

Epoch: 90
Loss: 0.2907690703868866
RMSE train: 0.455555	val: 0.775024	test: 0.752140
MAE train: 0.351472	val: 0.587234	test: 0.586814

Epoch: 91
Loss: 0.28728487715125084
RMSE train: 0.483404	val: 0.778252	test: 0.748555
MAE train: 0.376498	val: 0.592964	test: 0.590318

Epoch: 92
Loss: 0.2682698145508766
RMSE train: 0.466177	val: 0.778598	test: 0.735767
MAE train: 0.360539	val: 0.585032	test: 0.583964

Epoch: 93
Loss: 0.2741120420396328
RMSE train: 0.478885	val: 0.796308	test: 0.761207
MAE train: 0.371381	val: 0.601813	test: 0.600228

Epoch: 94
Loss: 0.26967325682441395
RMSE train: 0.455204	val: 0.767150	test: 0.737052
MAE train: 0.354183	val: 0.580004	test: 0.580529

Epoch: 95
Loss: 0.26968640089035034
RMSE train: 0.451255	val: 0.773285	test: 0.726039
MAE train: 0.349732	val: 0.583856	test: 0.575346

Epoch: 96
Loss: 0.2823233778278033
RMSE train: 0.478801	val: 0.789118	test: 0.746478
MAE train: 0.372457	val: 0.595565	test: 0.589963

Epoch: 97
Loss: 0.2666567787528038
RMSE train: 0.459197	val: 0.763452	test: 0.731215
MAE train: 0.356586	val: 0.577093	test: 0.572064

Epoch: 98
Loss: 0.27707841495672864
RMSE train: 0.496103	val: 0.810302	test: 0.765524
MAE train: 0.386503	val: 0.612815	test: 0.606825

Epoch: 99
Loss: 0.26930928726991016
RMSE train: 0.471189	val: 0.800744	test: 0.744781
MAE train: 0.367022	val: 0.608640	test: 0.590557

Epoch: 100
Loss: 0.2549922751883666
RMSE train: 0.469915	val: 0.808737	test: 0.765043
MAE train: 0.365844	val: 0.612759	test: 0.605552

Epoch: 101
Loss: 0.25829939047495526
RMSE train: 0.459309	val: 0.778847	test: 0.734352
MAE train: 0.358390	val: 0.592087	test: 0.578070

Epoch: 102
Loss: 0.2536496917406718
RMSE train: 0.453448	val: 0.796600	test: 0.736539
MAE train: 0.349663	val: 0.594978	test: 0.585235

Epoch: 103
Loss: 0.25518543273210526
RMSE train: 0.452898	val: 0.778846	test: 0.742381
MAE train: 0.349405	val: 0.587622	test: 0.581950

Epoch: 104
Loss: 0.25771746039390564
RMSE train: 0.455878	val: 0.785349	test: 0.732740
MAE train: 0.352470	val: 0.589815	test: 0.583718

Epoch: 105
Loss: 0.25297506029407185
RMSE train: 0.448739	val: 0.780561	test: 0.734206
MAE train: 0.348263	val: 0.592962	test: 0.581332

Epoch: 106
Loss: 0.25860392674803734
RMSE train: 0.442504	val: 0.771307	test: 0.736038
MAE train: 0.342094	val: 0.583016	test: 0.574546

Epoch: 107
Loss: 0.24848726019263268
RMSE train: 0.450354	val: 0.778517	test: 0.737957
MAE train: 0.349763	val: 0.589594	test: 0.587729

Epoch: 108
Loss: 0.2718113660812378
RMSE train: 0.473582	val: 0.782808	test: 0.755899
MAE train: 0.370460	val: 0.599162	test: 0.598471

Epoch: 109
Loss: 0.26016823450724286
RMSE train: 0.457428	val: 0.797326	test: 0.753802
MAE train: 0.355661	val: 0.599714	test: 0.596999

Epoch: 110
Loss: 0.24612775817513466
RMSE train: 0.462479	val: 0.804934	test: 0.760968
MAE train: 0.358478	val: 0.603549	test: 0.602601

Epoch: 111
Loss: 0.25265226016441983
RMSE train: 0.449630	val: 0.763719	test: 0.731703
MAE train: 0.351239	val: 0.579923	test: 0.578040

Epoch: 112
Loss: 0.24167813981572786
RMSE train: 0.441895	val: 0.767734	test: 0.736859
MAE train: 0.344174	val: 0.576831	test: 0.579208

Epoch: 113
Loss: 0.24900820602973303
RMSE train: 0.458269	val: 0.772065	test: 0.749883
MAE train: 0.354651	val: 0.580358	test: 0.596903

Epoch: 114
Loss: 0.2503304419418176
RMSE train: 0.445059	val: 0.768779	test: 0.734198
MAE train: 0.343775	val: 0.583575	test: 0.575746

Epoch: 115
Loss: 0.25405317669113475
RMSE train: 0.477138	val: 0.816828	test: 0.780383
MAE train: 0.369478	val: 0.618617	test: 0.613417

Epoch: 116
Loss: 0.228603175530831
RMSE train: 0.453825	val: 0.797529	test: 0.744701
MAE train: 0.350863	val: 0.601629	test: 0.587710

Epoch: 117
Loss: 0.2381070132056872
RMSE train: 0.428338	val: 0.778559	test: 0.739483
MAE train: 0.331990	val: 0.586839	test: 0.587368

Epoch: 118
Loss: 0.24217993393540382
RMSE train: 0.434789	val: 0.771867	test: 0.738413
MAE train: 0.335866	val: 0.587161	test: 0.583759

Epoch: 119
Loss: 0.2319261667629083
RMSE train: 0.440793	val: 0.774900	test: 0.744649
MAE train: 0.342401	val: 0.583290	test: 0.590113

Epoch: 120
Loss: 0.23905376344919205
RMSE train: 0.445360	val: 0.777258	test: 0.741981
MAE train: 0.345348	val: 0.587476	test: 0.587347

Epoch: 121
Loss: 0.2439958080649376
RMSE train: 0.449203	val: 0.790684	test: 0.754084
MAE train: 0.347532	val: 0.600293	test: 0.598488

Epoch: 122
Loss: 0.2523178681731224
RMSE train: 0.432285	val: 0.780870	test: 0.730279
MAE train: 0.334967	val: 0.587324	test: 0.582314

Early stopping
Best (RMSE):	 train: 0.476682	val: 0.760316	test: 0.731144
Best (MAE):	 train: 0.371680	val: 0.578134	test: 0.572385

MAE train: 0.388392	val: 0.640615	test: 0.642984

Epoch: 84
Loss: 0.28659113943576814
RMSE train: 0.499538	val: 0.838967	test: 0.835093
MAE train: 0.386464	val: 0.640671	test: 0.645799

Epoch: 85
Loss: 0.28578590750694277
RMSE train: 0.497272	val: 0.839369	test: 0.824454
MAE train: 0.384262	val: 0.638880	test: 0.643750

Epoch: 86
Loss: 0.27922452986240387
RMSE train: 0.497961	val: 0.843589	test: 0.843673
MAE train: 0.389760	val: 0.644791	test: 0.654059

Epoch: 87
Loss: 0.2849347651004791
RMSE train: 0.465514	val: 0.823746	test: 0.815051
MAE train: 0.360946	val: 0.630311	test: 0.630475

Epoch: 88
Loss: 0.2870491683483124
RMSE train: 0.491344	val: 0.850431	test: 0.847062
MAE train: 0.382994	val: 0.645082	test: 0.654459

Epoch: 89
Loss: 0.276717945933342
RMSE train: 0.519072	val: 0.867545	test: 0.868198
MAE train: 0.403939	val: 0.662998	test: 0.676617

Epoch: 90
Loss: 0.28265248239040375
RMSE train: 0.518905	val: 0.872186	test: 0.860098
MAE train: 0.403278	val: 0.664703	test: 0.668878

Epoch: 91
Loss: 0.27874841541051865
RMSE train: 0.522160	val: 0.860737	test: 0.863221
MAE train: 0.408876	val: 0.656936	test: 0.671432

Epoch: 92
Loss: 0.2651332750916481
RMSE train: 0.496226	val: 0.846973	test: 0.860045
MAE train: 0.386563	val: 0.646403	test: 0.672608

Epoch: 93
Loss: 0.2640859141945839
RMSE train: 0.464936	val: 0.820159	test: 0.808264
MAE train: 0.362105	val: 0.621759	test: 0.629595

Epoch: 94
Loss: 0.26534267961978913
RMSE train: 0.463950	val: 0.824164	test: 0.816456
MAE train: 0.359540	val: 0.625295	test: 0.631334

Epoch: 95
Loss: 0.26045399755239484
RMSE train: 0.445093	val: 0.817529	test: 0.816859
MAE train: 0.347220	val: 0.622941	test: 0.635869

Epoch: 96
Loss: 0.26634738594293594
RMSE train: 0.469299	val: 0.828761	test: 0.834729
MAE train: 0.361919	val: 0.631051	test: 0.652696

Epoch: 97
Loss: 0.2677394673228264
RMSE train: 0.467647	val: 0.828647	test: 0.827350
MAE train: 0.365276	val: 0.632627	test: 0.644357

Epoch: 98
Loss: 0.2554618865251541
RMSE train: 0.464858	val: 0.834095	test: 0.835897
MAE train: 0.362434	val: 0.637601	test: 0.650457

Epoch: 99
Loss: 0.26356803327798844
RMSE train: 0.470076	val: 0.834116	test: 0.823926
MAE train: 0.365440	val: 0.634514	test: 0.641721

Epoch: 100
Loss: 0.25166127979755404
RMSE train: 0.493245	val: 0.836976	test: 0.835879
MAE train: 0.386799	val: 0.636772	test: 0.652546

Epoch: 101
Loss: 0.2474810153245926
RMSE train: 0.477911	val: 0.850130	test: 0.839862
MAE train: 0.371711	val: 0.645071	test: 0.650292

Epoch: 102
Loss: 0.26176239401102064
RMSE train: 0.473290	val: 0.841229	test: 0.828290
MAE train: 0.367877	val: 0.637812	test: 0.644336

Epoch: 103
Loss: 0.2593521073460579
RMSE train: 0.467585	val: 0.830898	test: 0.822077
MAE train: 0.363890	val: 0.629875	test: 0.638221

Epoch: 104
Loss: 0.25639639496803285
RMSE train: 0.472343	val: 0.840050	test: 0.834282
MAE train: 0.366288	val: 0.638786	test: 0.648932

Epoch: 105
Loss: 0.2522414535284042
RMSE train: 0.490960	val: 0.844307	test: 0.834507
MAE train: 0.381933	val: 0.641859	test: 0.651346

Epoch: 106
Loss: 0.24735051691532134
RMSE train: 0.475677	val: 0.847739	test: 0.840750
MAE train: 0.370865	val: 0.644365	test: 0.652149

Epoch: 107
Loss: 0.25372119545936583
RMSE train: 0.480343	val: 0.855190	test: 0.845008
MAE train: 0.374250	val: 0.652062	test: 0.658582

Epoch: 108
Loss: 0.2511377438902855
RMSE train: 0.475106	val: 0.853770	test: 0.843926
MAE train: 0.368475	val: 0.650441	test: 0.653030

Epoch: 109
Loss: 0.24378535896539688
RMSE train: 0.477898	val: 0.847803	test: 0.850225
MAE train: 0.372481	val: 0.646603	test: 0.661748

Epoch: 110
Loss: 0.25448748022317885
RMSE train: 0.509221	val: 0.873368	test: 0.857579
MAE train: 0.396071	val: 0.663245	test: 0.669545

Epoch: 111
Loss: 0.24360306710004806
RMSE train: 0.475331	val: 0.833384	test: 0.820441
MAE train: 0.369271	val: 0.630864	test: 0.638313

Epoch: 112
Loss: 0.24869645833969117
RMSE train: 0.465851	val: 0.844488	test: 0.843151
MAE train: 0.363971	val: 0.641670	test: 0.653859

Epoch: 113
Loss: 0.24494570642709732
RMSE train: 0.433670	val: 0.816798	test: 0.806005
MAE train: 0.336937	val: 0.616817	test: 0.624254

Epoch: 114
Loss: 0.23877387642860412
RMSE train: 0.452948	val: 0.819766	test: 0.823792
MAE train: 0.349519	val: 0.625758	test: 0.635781

Epoch: 115
Loss: 0.2436741217970848
RMSE train: 0.433328	val: 0.816668	test: 0.806875
MAE train: 0.335187	val: 0.617050	test: 0.626321

Epoch: 116
Loss: 0.2376616671681404
RMSE train: 0.427759	val: 0.815756	test: 0.804339
MAE train: 0.331089	val: 0.620056	test: 0.624606

Epoch: 117
Loss: 0.24561179280281067
RMSE train: 0.462004	val: 0.833358	test: 0.836797
MAE train: 0.357956	val: 0.636916	test: 0.647244

Epoch: 118
Loss: 0.24031440019607545
RMSE train: 0.502137	val: 0.872993	test: 0.869658
MAE train: 0.392174	val: 0.662436	test: 0.675378

Epoch: 119
Loss: 0.23266639560461044
RMSE train: 0.464460	val: 0.847159	test: 0.841859
MAE train: 0.360613	val: 0.646685	test: 0.654223

Epoch: 120
Loss: 0.22989367544651032
RMSE train: 0.428062	val: 0.811808	test: 0.801481
MAE train: 0.334800	val: 0.620745	test: 0.622702

Epoch: 121
Loss: 0.2431802824139595
RMSE train: 0.429292	val: 0.826928	test: 0.823636
MAE train: 0.331486	val: 0.625361	test: 0.641338

Epoch: 122
Loss: 0.22405097931623458
RMSE train: 0.443009	val: 0.808293	test: 0.797464
MAE train: 0.341982	val: 0.615398	test: 0.623818

Epoch: 123
Loss: 0.22972748130559922
RMSE train: 0.456072	val: 0.836576	test: 0.834955
MAE train: 0.357036	val: 0.636433	test: 0.652186

Epoch: 124
Loss: 0.2287972390651703
RMSE train: 0.441460	val: 0.836985	test: 0.823648
MAE train: 0.343618	val: 0.634732	test: 0.640411

Epoch: 125
Loss: 0.22863287180662156
RMSE train: 0.454391	val: 0.837210	test: 0.814295
MAE train: 0.353511	val: 0.639024	test: 0.636597

Epoch: 126
Loss: 0.222844260931015
RMSE train: 0.447366	val: 0.844781	test: 0.836307
MAE train: 0.347888	val: 0.643873	test: 0.650874

Epoch: 127
Loss: 0.21761514842510224
RMSE train: 0.432226	val: 0.833024	test: 0.829967
MAE train: 0.335244	val: 0.632988	test: 0.644077

Epoch: 128
Loss: 0.2179763287305832
RMSE train: 0.434110	val: 0.834703	test: 0.832035
MAE train: 0.337490	val: 0.633259	test: 0.645673

Epoch: 129
Loss: 0.22449135929346084
RMSE train: 0.430414	val: 0.841314	test: 0.835822
MAE train: 0.333223	val: 0.637749	test: 0.646197

Epoch: 130
Loss: 0.22793666571378707
RMSE train: 0.441416	val: 0.830049	test: 0.824653
MAE train: 0.342894	val: 0.630844	test: 0.640714

Epoch: 131
Loss: 0.23408025205135347
RMSE train: 0.428947	val: 0.839317	test: 0.842219
MAE train: 0.333799	val: 0.633073	test: 0.651061

Epoch: 132
Loss: 0.22205527126789093
RMSE train: 0.442217	val: 0.842664	test: 0.833989
MAE train: 0.344677	val: 0.636993	test: 0.648306

Epoch: 133
Loss: 0.22766696214675902
RMSE train: 0.427188	val: 0.832310	test: 0.829733
MAE train: 0.333156	val: 0.628350	test: 0.645846

Epoch: 134
Loss: 0.22595594078302383
RMSE train: 0.431099	val: 0.837067	test: 0.829084
MAE train: 0.335352	val: 0.630315	test: 0.645347

Epoch: 135
Loss: 0.21689437180757523
RMSE train: 0.408890	val: 0.804888	test: 0.800601
MAE train: 0.318481	val: 0.612201	test: 0.619126

Epoch: 136
Loss: 0.22757222056388854
RMSE train: 0.413549	val: 0.820153	test: 0.815862
MAE train: 0.320553	val: 0.619370	test: 0.629462

Epoch: 137
Loss: 0.208880253136158
RMSE train: 0.436810	val: 0.836771	test: 0.845306
MAE train: 0.339641	val: 0.636344	test: 0.654654

Epoch: 138
Loss: 0.2021634042263031
RMSE train: 0.415842	val: 0.824659	test: 0.819305
MAE train: 0.322788	val: 0.624537	test: 0.633129

Epoch: 139
Loss: 0.22112472206354142
RMSE train: 0.419804	val: 0.824665	test: 0.827576
MAE train: 0.325922	val: 0.624224	test: 0.638468

Epoch: 140
Loss: 0.22785944044589995
RMSE train: 0.419388	val: 0.825173	test: 0.807249
MAE train: 0.322456	val: 0.624322	test: 0.626486

Epoch: 141
Loss: 0.22148308902978897
RMSE train: 0.415588	val: 0.821306	test: 0.804767
MAE train: 0.321208	val: 0.616561	test: 0.623577

Epoch: 142
Loss: 0.20685291886329651
RMSE train: 0.416416	val: 0.828008	test: 0.822714
MAE train: 0.322763	val: 0.624261	test: 0.634294

Epoch: 143
Loss: 0.20943441092967988
RMSE train: 0.416097	val: 0.821936	test: 0.825307
MAE train: 0.320950	val: 0.619619	test: 0.634789
MAE train: 0.408050	val: 0.644873	test: 0.643811

Epoch: 84
Loss: 0.28949895352125166
RMSE train: 0.483347	val: 0.808323	test: 0.816880
MAE train: 0.384030	val: 0.630941	test: 0.635967

Epoch: 85
Loss: 0.2897506654262543
RMSE train: 0.486052	val: 0.820024	test: 0.818752
MAE train: 0.379275	val: 0.632337	test: 0.638501

Epoch: 86
Loss: 0.2906803026795387
RMSE train: 0.452235	val: 0.802701	test: 0.813216
MAE train: 0.350277	val: 0.618259	test: 0.628476

Epoch: 87
Loss: 0.28522342145442964
RMSE train: 0.469956	val: 0.814131	test: 0.800390
MAE train: 0.366737	val: 0.624497	test: 0.623371

Epoch: 88
Loss: 0.2744423374533653
RMSE train: 0.469610	val: 0.805081	test: 0.803778
MAE train: 0.367820	val: 0.618540	test: 0.626300

Epoch: 89
Loss: 0.26496042162179945
RMSE train: 0.467643	val: 0.790307	test: 0.806304
MAE train: 0.365824	val: 0.610821	test: 0.627533

Epoch: 90
Loss: 0.2842125356197357
RMSE train: 0.477972	val: 0.818322	test: 0.813272
MAE train: 0.374023	val: 0.626403	test: 0.634418

Epoch: 91
Loss: 0.26488949805498124
RMSE train: 0.465158	val: 0.808520	test: 0.809792
MAE train: 0.360828	val: 0.619307	test: 0.628627

Epoch: 92
Loss: 0.2799243852496147
RMSE train: 0.458476	val: 0.790264	test: 0.806870
MAE train: 0.355769	val: 0.607920	test: 0.628857

Epoch: 93
Loss: 0.27210636585950854
RMSE train: 0.463814	val: 0.802664	test: 0.803958
MAE train: 0.360440	val: 0.620647	test: 0.630955

Epoch: 94
Loss: 0.26752039790153503
RMSE train: 0.475999	val: 0.816281	test: 0.814977
MAE train: 0.377402	val: 0.633911	test: 0.637961

Epoch: 95
Loss: 0.26450797319412234
RMSE train: 0.455137	val: 0.794838	test: 0.807495
MAE train: 0.356617	val: 0.616005	test: 0.628775

Epoch: 96
Loss: 0.2672564506530762
RMSE train: 0.446276	val: 0.813816	test: 0.809284
MAE train: 0.347778	val: 0.623133	test: 0.628224

Epoch: 97
Loss: 0.26387780606746675
RMSE train: 0.454476	val: 0.811485	test: 0.819344
MAE train: 0.353481	val: 0.625473	test: 0.634561

Epoch: 98
Loss: 0.25811342895030975
RMSE train: 0.460647	val: 0.794035	test: 0.811189
MAE train: 0.362337	val: 0.616651	test: 0.630998

Epoch: 99
Loss: 0.276142355799675
RMSE train: 0.442065	val: 0.790215	test: 0.802391
MAE train: 0.343046	val: 0.607250	test: 0.619612

Epoch: 100
Loss: 0.2559801936149597
RMSE train: 0.449193	val: 0.802431	test: 0.805108
MAE train: 0.350922	val: 0.622904	test: 0.627663

Epoch: 101
Loss: 0.26260299533605574
RMSE train: 0.433697	val: 0.788766	test: 0.805236
MAE train: 0.338684	val: 0.606208	test: 0.624651

Epoch: 102
Loss: 0.24569450318813324
RMSE train: 0.442677	val: 0.793412	test: 0.807359
MAE train: 0.346705	val: 0.611356	test: 0.628486

Epoch: 103
Loss: 0.2570976987481117
RMSE train: 0.435069	val: 0.786126	test: 0.800030
MAE train: 0.339868	val: 0.606941	test: 0.622644

Epoch: 104
Loss: 0.25312609523534774
RMSE train: 0.447933	val: 0.810632	test: 0.813772
MAE train: 0.350245	val: 0.620528	test: 0.637264

Epoch: 105
Loss: 0.25183665603399275
RMSE train: 0.446503	val: 0.789622	test: 0.803745
MAE train: 0.352778	val: 0.615356	test: 0.626968

Epoch: 106
Loss: 0.25392296314239504
RMSE train: 0.447600	val: 0.790828	test: 0.798880
MAE train: 0.352301	val: 0.615545	test: 0.626950

Epoch: 107
Loss: 0.24312458485364913
RMSE train: 0.441748	val: 0.796437	test: 0.795222
MAE train: 0.345117	val: 0.610411	test: 0.622269

Epoch: 108
Loss: 0.24830190241336822
RMSE train: 0.435494	val: 0.789601	test: 0.788404
MAE train: 0.341400	val: 0.609150	test: 0.617253

Epoch: 109
Loss: 0.2621886312961578
RMSE train: 0.450685	val: 0.802408	test: 0.795602
MAE train: 0.351965	val: 0.618851	test: 0.625276

Epoch: 110
Loss: 0.24631654918193818
RMSE train: 0.425875	val: 0.785943	test: 0.794516
MAE train: 0.333171	val: 0.606074	test: 0.617183

Epoch: 111
Loss: 0.24964826852083205
RMSE train: 0.444247	val: 0.807842	test: 0.811883
MAE train: 0.349358	val: 0.622424	test: 0.631488

Epoch: 112
Loss: 0.24136044681072236
RMSE train: 0.430383	val: 0.798951	test: 0.799318
MAE train: 0.335618	val: 0.614567	test: 0.619924

Epoch: 113
Loss: 0.24674020260572432
RMSE train: 0.438712	val: 0.807631	test: 0.808144
MAE train: 0.343883	val: 0.622555	test: 0.629948

Epoch: 114
Loss: 0.26518028825521467
RMSE train: 0.460748	val: 0.804283	test: 0.807744
MAE train: 0.366439	val: 0.627077	test: 0.632843

Epoch: 115
Loss: 0.23990405201911927
RMSE train: 0.423358	val: 0.789732	test: 0.795816
MAE train: 0.330348	val: 0.606422	test: 0.619994

Epoch: 116
Loss: 0.235663940012455
RMSE train: 0.449900	val: 0.806463	test: 0.799341
MAE train: 0.352763	val: 0.622372	test: 0.623610

Epoch: 117
Loss: 0.23279245644807817
RMSE train: 0.419418	val: 0.800455	test: 0.805328
MAE train: 0.325675	val: 0.618773	test: 0.624550

Epoch: 118
Loss: 0.2469998300075531
RMSE train: 0.443126	val: 0.796961	test: 0.802598
MAE train: 0.348715	val: 0.616917	test: 0.628538

Epoch: 119
Loss: 0.23888201862573624
RMSE train: 0.433110	val: 0.791844	test: 0.792397
MAE train: 0.341851	val: 0.612213	test: 0.619089

Epoch: 120
Loss: 0.22873373627662658
RMSE train: 0.429406	val: 0.802170	test: 0.799933
MAE train: 0.336661	val: 0.619580	test: 0.621464

Epoch: 121
Loss: 0.23137514889240265
RMSE train: 0.424364	val: 0.803935	test: 0.800947
MAE train: 0.331299	val: 0.619181	test: 0.620928

Epoch: 122
Loss: 0.22457759231328964
RMSE train: 0.408125	val: 0.793688	test: 0.801540
MAE train: 0.315880	val: 0.606872	test: 0.620095

Epoch: 123
Loss: 0.23092904388904573
RMSE train: 0.430609	val: 0.784402	test: 0.796129
MAE train: 0.336455	val: 0.604343	test: 0.620944

Epoch: 124
Loss: 0.2356181561946869
RMSE train: 0.426170	val: 0.799953	test: 0.807387
MAE train: 0.334363	val: 0.613132	test: 0.626995

Epoch: 125
Loss: 0.22682625949382781
RMSE train: 0.418699	val: 0.796980	test: 0.805415
MAE train: 0.326523	val: 0.614228	test: 0.626078

Epoch: 126
Loss: 0.2149599090218544
RMSE train: 0.417208	val: 0.773759	test: 0.791795
MAE train: 0.328343	val: 0.600768	test: 0.613719

Epoch: 127
Loss: 0.22624737918376922
RMSE train: 0.439212	val: 0.789384	test: 0.797946
MAE train: 0.347013	val: 0.611328	test: 0.621165

Epoch: 128
Loss: 0.22974031716585158
RMSE train: 0.419346	val: 0.781837	test: 0.788689
MAE train: 0.328305	val: 0.603431	test: 0.612238

Epoch: 129
Loss: 0.21458773463964462
RMSE train: 0.396391	val: 0.779918	test: 0.787508
MAE train: 0.308562	val: 0.598748	test: 0.610260

Epoch: 130
Loss: 0.2219617858529091
RMSE train: 0.446704	val: 0.806303	test: 0.808663
MAE train: 0.348827	val: 0.621856	test: 0.626215

Epoch: 131
Loss: 0.2180556073784828
RMSE train: 0.398388	val: 0.785408	test: 0.794097
MAE train: 0.309610	val: 0.607149	test: 0.615413

Epoch: 132
Loss: 0.21866275668144225
RMSE train: 0.417462	val: 0.786368	test: 0.805767
MAE train: 0.324751	val: 0.606132	test: 0.622903

Epoch: 133
Loss: 0.2260081335902214
RMSE train: 0.416453	val: 0.790061	test: 0.804942
MAE train: 0.325652	val: 0.613028	test: 0.624015

Epoch: 134
Loss: 0.22310459315776826
RMSE train: 0.407209	val: 0.796682	test: 0.800706
MAE train: 0.318161	val: 0.611818	test: 0.618744

Epoch: 135
Loss: 0.22119814157485962
RMSE train: 0.406134	val: 0.775361	test: 0.794455
MAE train: 0.317113	val: 0.597682	test: 0.613499

Epoch: 136
Loss: 0.21625687181949615
RMSE train: 0.404027	val: 0.777049	test: 0.794199
MAE train: 0.318907	val: 0.602325	test: 0.614323

Epoch: 137
Loss: 0.2180696904659271
RMSE train: 0.414230	val: 0.778895	test: 0.792139
MAE train: 0.324789	val: 0.598772	test: 0.615641

Epoch: 138
Loss: 0.20894587486982347
RMSE train: 0.417646	val: 0.783712	test: 0.800860
MAE train: 0.328450	val: 0.605394	test: 0.620972

Epoch: 139
Loss: 0.2109274074435234
RMSE train: 0.395543	val: 0.772621	test: 0.801414
MAE train: 0.306434	val: 0.596734	test: 0.615888

Epoch: 140
Loss: 0.20589784681797027
RMSE train: 0.397814	val: 0.766202	test: 0.792594
MAE train: 0.310293	val: 0.593723	test: 0.613473

Epoch: 141
Loss: 0.20396820306777955
RMSE train: 0.419051	val: 0.788068	test: 0.797499
MAE train: 0.329728	val: 0.612671	test: 0.621467

Epoch: 142
Loss: 0.2139856055378914
RMSE train: 0.396153	val: 0.773742	test: 0.795136
MAE train: 0.308994	val: 0.596808	test: 0.611191

Epoch: 143
Loss: 0.2238881304860115
RMSE train: 0.404405	val: 0.784037	test: 0.792492
MAE train: 0.315020	val: 0.601125	test: 0.613149
MAE train: 0.359960	val: 0.619664	test: 0.624609

Epoch: 84
Loss: 0.2962561175227165
RMSE train: 0.483156	val: 0.815946	test: 0.808282
MAE train: 0.377678	val: 0.623972	test: 0.629496

Epoch: 85
Loss: 0.3010043233633041
RMSE train: 0.469378	val: 0.826483	test: 0.828462
MAE train: 0.362298	val: 0.631881	test: 0.646336

Epoch: 86
Loss: 0.29449148178100587
RMSE train: 0.474063	val: 0.822607	test: 0.831529
MAE train: 0.367792	val: 0.630840	test: 0.640536

Epoch: 87
Loss: 0.29600802063941956
RMSE train: 0.464455	val: 0.819462	test: 0.823864
MAE train: 0.359936	val: 0.624949	test: 0.638620

Epoch: 88
Loss: 0.27436650097370147
RMSE train: 0.497762	val: 0.827748	test: 0.817046
MAE train: 0.392537	val: 0.640168	test: 0.636958

Epoch: 89
Loss: 0.27429900467395785
RMSE train: 0.472026	val: 0.807521	test: 0.818956
MAE train: 0.365928	val: 0.618234	test: 0.634491

Epoch: 90
Loss: 0.28495114147663114
RMSE train: 0.460518	val: 0.809787	test: 0.824105
MAE train: 0.357041	val: 0.617890	test: 0.634464

Epoch: 91
Loss: 0.2694493100047112
RMSE train: 0.450111	val: 0.799329	test: 0.804160
MAE train: 0.349078	val: 0.613899	test: 0.619044

Epoch: 92
Loss: 0.2820633798837662
RMSE train: 0.462953	val: 0.838288	test: 0.832687
MAE train: 0.359800	val: 0.636586	test: 0.637126

Epoch: 93
Loss: 0.2704185500741005
RMSE train: 0.482918	val: 0.816542	test: 0.825504
MAE train: 0.376006	val: 0.622479	test: 0.640364

Epoch: 94
Loss: 0.27261129319667815
RMSE train: 0.477024	val: 0.814333	test: 0.811703
MAE train: 0.373651	val: 0.622145	test: 0.627864

Epoch: 95
Loss: 0.28413262963294983
RMSE train: 0.471124	val: 0.806884	test: 0.804176
MAE train: 0.371180	val: 0.619251	test: 0.626783

Epoch: 96
Loss: 0.2764863848686218
RMSE train: 0.459502	val: 0.810943	test: 0.813122
MAE train: 0.355368	val: 0.617988	test: 0.624020

Epoch: 97
Loss: 0.28205015808343886
RMSE train: 0.461988	val: 0.805607	test: 0.821323
MAE train: 0.358715	val: 0.615862	test: 0.635269

Epoch: 98
Loss: 0.2637004092335701
RMSE train: 0.462952	val: 0.822691	test: 0.820103
MAE train: 0.360114	val: 0.627480	test: 0.634957

Epoch: 99
Loss: 0.26179835945367813
RMSE train: 0.452799	val: 0.796555	test: 0.807915
MAE train: 0.350134	val: 0.609803	test: 0.624844

Epoch: 100
Loss: 0.2685713186860085
RMSE train: 0.458128	val: 0.801606	test: 0.814218
MAE train: 0.354328	val: 0.611060	test: 0.632172

Epoch: 101
Loss: 0.26257405877113343
RMSE train: 0.437531	val: 0.794545	test: 0.806804
MAE train: 0.338632	val: 0.608137	test: 0.621378

Epoch: 102
Loss: 0.25323697477579116
RMSE train: 0.470217	val: 0.815385	test: 0.817506
MAE train: 0.368457	val: 0.623914	test: 0.635162

Epoch: 103
Loss: 0.263609853386879
RMSE train: 0.470922	val: 0.794718	test: 0.812357
MAE train: 0.367410	val: 0.614444	test: 0.626418

Epoch: 104
Loss: 0.26252256482839587
RMSE train: 0.453127	val: 0.811512	test: 0.821817
MAE train: 0.352197	val: 0.622320	test: 0.634776

Epoch: 105
Loss: 0.2484829843044281
RMSE train: 0.446738	val: 0.797542	test: 0.807652
MAE train: 0.348442	val: 0.608635	test: 0.624442

Epoch: 106
Loss: 0.2519043296575546
RMSE train: 0.453589	val: 0.821483	test: 0.833194
MAE train: 0.353158	val: 0.629358	test: 0.644456

Epoch: 107
Loss: 0.2646757706999779
RMSE train: 0.441639	val: 0.796636	test: 0.802158
MAE train: 0.344835	val: 0.612506	test: 0.623005

Epoch: 108
Loss: 0.24922049790620804
RMSE train: 0.443269	val: 0.799096	test: 0.803276
MAE train: 0.346801	val: 0.608037	test: 0.623229

Epoch: 109
Loss: 0.2508043721318245
RMSE train: 0.438275	val: 0.797028	test: 0.810096
MAE train: 0.338966	val: 0.609561	test: 0.625786

Epoch: 110
Loss: 0.24296594858169557
RMSE train: 0.470371	val: 0.819146	test: 0.834627
MAE train: 0.365810	val: 0.619871	test: 0.645943

Epoch: 111
Loss: 0.2550120860338211
RMSE train: 0.440881	val: 0.801410	test: 0.808831
MAE train: 0.344102	val: 0.613783	test: 0.627262

Epoch: 112
Loss: 0.2475418820977211
RMSE train: 0.434564	val: 0.796414	test: 0.795012
MAE train: 0.335462	val: 0.601873	test: 0.616683

Epoch: 113
Loss: 0.24938607215881348
RMSE train: 0.436049	val: 0.810392	test: 0.813807
MAE train: 0.337946	val: 0.616822	test: 0.629422

Epoch: 114
Loss: 0.24578596502542496
RMSE train: 0.429475	val: 0.808724	test: 0.816334
MAE train: 0.331717	val: 0.615198	test: 0.628343

Epoch: 115
Loss: 0.2454705059528351
RMSE train: 0.433601	val: 0.806817	test: 0.810766
MAE train: 0.336778	val: 0.615011	test: 0.627285

Epoch: 116
Loss: 0.237745301425457
RMSE train: 0.442799	val: 0.799905	test: 0.807933
MAE train: 0.343553	val: 0.610112	test: 0.619434

Epoch: 117
Loss: 0.2429721400141716
RMSE train: 0.426673	val: 0.798024	test: 0.811948
MAE train: 0.329523	val: 0.604753	test: 0.623124

Epoch: 118
Loss: 0.2426660418510437
RMSE train: 0.414144	val: 0.799111	test: 0.806263
MAE train: 0.318304	val: 0.606599	test: 0.622323

Epoch: 119
Loss: 0.24452368766069413
RMSE train: 0.454219	val: 0.834135	test: 0.815807
MAE train: 0.352882	val: 0.632614	test: 0.634253

Epoch: 120
Loss: 0.2346440702676773
RMSE train: 0.429704	val: 0.798874	test: 0.803498
MAE train: 0.333784	val: 0.607070	test: 0.619638

Epoch: 121
Loss: 0.23814293444156648
RMSE train: 0.442206	val: 0.811369	test: 0.816437
MAE train: 0.345036	val: 0.616660	test: 0.627312

Epoch: 122
Loss: 0.2361339032649994
RMSE train: 0.434015	val: 0.805943	test: 0.816006
MAE train: 0.337615	val: 0.611260	test: 0.630644

Epoch: 123
Loss: 0.22564149051904678
RMSE train: 0.419163	val: 0.797302	test: 0.796795
MAE train: 0.326882	val: 0.605676	test: 0.615968

Epoch: 124
Loss: 0.23949477076530457
RMSE train: 0.428820	val: 0.810139	test: 0.824116
MAE train: 0.331933	val: 0.614360	test: 0.635756

Epoch: 125
Loss: 0.23405727595090867
RMSE train: 0.435108	val: 0.826120	test: 0.836474
MAE train: 0.336358	val: 0.623190	test: 0.645075

Epoch: 126
Loss: 0.2311977967619896
RMSE train: 0.415822	val: 0.809255	test: 0.816906
MAE train: 0.323118	val: 0.614046	test: 0.629647

Epoch: 127
Loss: 0.22043072879314424
RMSE train: 0.418119	val: 0.799028	test: 0.805681
MAE train: 0.326014	val: 0.604510	test: 0.621308

Epoch: 128
Loss: 0.22746849358081817
RMSE train: 0.422817	val: 0.789490	test: 0.805390
MAE train: 0.330400	val: 0.605397	test: 0.617531

Epoch: 129
Loss: 0.2350905045866966
RMSE train: 0.420079	val: 0.794770	test: 0.805633
MAE train: 0.327012	val: 0.607205	test: 0.621601

Epoch: 130
Loss: 0.22764255851507187
RMSE train: 0.420891	val: 0.803709	test: 0.828803
MAE train: 0.324678	val: 0.615874	test: 0.636951

Epoch: 131
Loss: 0.22834842056035995
RMSE train: 0.427406	val: 0.818054	test: 0.816603
MAE train: 0.332131	val: 0.622462	test: 0.630363

Epoch: 132
Loss: 0.22302168607711792
RMSE train: 0.409051	val: 0.788944	test: 0.799473
MAE train: 0.316837	val: 0.602706	test: 0.617703

Epoch: 133
Loss: 0.23374147117137908
RMSE train: 0.420121	val: 0.789247	test: 0.797549
MAE train: 0.327585	val: 0.604557	test: 0.617505

Epoch: 134
Loss: 0.2285737857222557
RMSE train: 0.407359	val: 0.788175	test: 0.810902
MAE train: 0.315860	val: 0.601071	test: 0.619731

Epoch: 135
Loss: 0.22060638070106506
RMSE train: 0.407781	val: 0.800946	test: 0.804805
MAE train: 0.317626	val: 0.606048	test: 0.618942

Epoch: 136
Loss: 0.22665528655052186
RMSE train: 0.409808	val: 0.794312	test: 0.807924
MAE train: 0.317600	val: 0.603498	test: 0.616565

Epoch: 137
Loss: 0.22646875828504562
RMSE train: 0.409842	val: 0.808870	test: 0.814320
MAE train: 0.318386	val: 0.616930	test: 0.625691

Epoch: 138
Loss: 0.21664530634880066
RMSE train: 0.411322	val: 0.801931	test: 0.818181
MAE train: 0.319040	val: 0.608476	test: 0.627325

Epoch: 139
Loss: 0.21618456840515138
RMSE train: 0.410806	val: 0.794459	test: 0.804407
MAE train: 0.319965	val: 0.606297	test: 0.619734

Epoch: 140
Loss: 0.21144600808620453
RMSE train: 0.410466	val: 0.805138	test: 0.823544
MAE train: 0.318159	val: 0.611115	test: 0.632089

Epoch: 141
Loss: 0.21228881031274796
RMSE train: 0.409642	val: 0.808538	test: 0.810484
MAE train: 0.316894	val: 0.613911	test: 0.626519

Epoch: 142
Loss: 0.21323004513978958
RMSE train: 0.404392	val: 0.807053	test: 0.822576
MAE train: 0.312265	val: 0.611817	test: 0.628246

Epoch: 143
Loss: 0.21936339139938354
RMSE train: 0.438945	val: 0.818644	test: 0.838741
MAE train: 0.343483	val: 0.619845	test: 0.648029
MAE train: 0.373883	val: 0.578487	test: 0.581647

Epoch: 84
Loss: 0.28813614944616955
RMSE train: 0.473528	val: 0.780421	test: 0.727931
MAE train: 0.366593	val: 0.575503	test: 0.579275

Epoch: 85
Loss: 0.30049193774660427
RMSE train: 0.496144	val: 0.796983	test: 0.751869
MAE train: 0.386917	val: 0.597787	test: 0.595265

Epoch: 86
Loss: 0.29129406437277794
RMSE train: 0.480341	val: 0.794236	test: 0.738704
MAE train: 0.372956	val: 0.590817	test: 0.581474

Epoch: 87
Loss: 0.28690194835265476
RMSE train: 0.484608	val: 0.777930	test: 0.731872
MAE train: 0.376965	val: 0.577260	test: 0.576007

Epoch: 88
Loss: 0.2719728698333104
RMSE train: 0.473805	val: 0.791603	test: 0.760744
MAE train: 0.366393	val: 0.592549	test: 0.603600

Epoch: 89
Loss: 0.2878703760604064
RMSE train: 0.487830	val: 0.796913	test: 0.749189
MAE train: 0.377189	val: 0.598600	test: 0.589687

Epoch: 90
Loss: 0.29219670842091244
RMSE train: 0.475424	val: 0.787382	test: 0.749604
MAE train: 0.368116	val: 0.588667	test: 0.588896

Epoch: 91
Loss: 0.28894802431265515
RMSE train: 0.467886	val: 0.794551	test: 0.742607
MAE train: 0.363145	val: 0.587630	test: 0.583223

Epoch: 92
Loss: 0.2850702380140622
RMSE train: 0.478980	val: 0.793059	test: 0.745705
MAE train: 0.370327	val: 0.591809	test: 0.583805

Epoch: 93
Loss: 0.2772887858251731
RMSE train: 0.486122	val: 0.801850	test: 0.764868
MAE train: 0.378295	val: 0.600947	test: 0.599569

Epoch: 94
Loss: 0.2645491448541482
RMSE train: 0.463838	val: 0.782519	test: 0.743761
MAE train: 0.357964	val: 0.580523	test: 0.582357

Epoch: 95
Loss: 0.278337253878514
RMSE train: 0.464225	val: 0.788544	test: 0.747614
MAE train: 0.360299	val: 0.588498	test: 0.584328

Epoch: 96
Loss: 0.2799106165766716
RMSE train: 0.459213	val: 0.778559	test: 0.740675
MAE train: 0.357207	val: 0.578976	test: 0.580325

Epoch: 97
Loss: 0.2596110726396243
RMSE train: 0.512109	val: 0.826655	test: 0.784776
MAE train: 0.398163	val: 0.615062	test: 0.618776

Epoch: 98
Loss: 0.26939985528588295
RMSE train: 0.453581	val: 0.773168	test: 0.726992
MAE train: 0.351284	val: 0.573839	test: 0.571044

Epoch: 99
Loss: 0.26687385017673176
RMSE train: 0.452771	val: 0.774801	test: 0.731508
MAE train: 0.348333	val: 0.572141	test: 0.574383

Epoch: 100
Loss: 0.2596079061428706
RMSE train: 0.451133	val: 0.767371	test: 0.720301
MAE train: 0.347876	val: 0.573532	test: 0.567229

Epoch: 101
Loss: 0.2585120735069116
RMSE train: 0.474234	val: 0.805162	test: 0.760551
MAE train: 0.366994	val: 0.600188	test: 0.596382

Epoch: 102
Loss: 0.26822130754590034
RMSE train: 0.449616	val: 0.788489	test: 0.744377
MAE train: 0.345093	val: 0.585039	test: 0.579452

Epoch: 103
Loss: 0.2572801212469737
RMSE train: 0.462924	val: 0.779199	test: 0.754555
MAE train: 0.360056	val: 0.582958	test: 0.593093

Epoch: 104
Loss: 0.2521010749042034
RMSE train: 0.469540	val: 0.786155	test: 0.762506
MAE train: 0.365536	val: 0.589208	test: 0.594935

Epoch: 105
Loss: 0.2580662506322066
RMSE train: 0.476958	val: 0.807220	test: 0.771828
MAE train: 0.370609	val: 0.607263	test: 0.605329

Epoch: 106
Loss: 0.24857906873027483
RMSE train: 0.457772	val: 0.792451	test: 0.756788
MAE train: 0.353800	val: 0.587102	test: 0.591004

Epoch: 107
Loss: 0.26351478571693104
RMSE train: 0.439539	val: 0.767823	test: 0.728404
MAE train: 0.340677	val: 0.569060	test: 0.570691

Epoch: 108
Loss: 0.25281816472609836
RMSE train: 0.457388	val: 0.798626	test: 0.766887
MAE train: 0.353670	val: 0.594556	test: 0.596938

Epoch: 109
Loss: 0.25433843831221264
RMSE train: 0.460424	val: 0.775000	test: 0.744201
MAE train: 0.356080	val: 0.578146	test: 0.587467

Epoch: 110
Loss: 0.26311814164121944
RMSE train: 0.440602	val: 0.789368	test: 0.743685
MAE train: 0.340263	val: 0.587434	test: 0.581281

Epoch: 111
Loss: 0.24533951779206595
RMSE train: 0.459677	val: 0.793027	test: 0.743244
MAE train: 0.356781	val: 0.590682	test: 0.586075

Epoch: 112
Loss: 0.27164895584185916
RMSE train: 0.436855	val: 0.774835	test: 0.727980
MAE train: 0.337640	val: 0.577781	test: 0.569851

Epoch: 113
Loss: 0.25526229416330654
RMSE train: 0.447802	val: 0.773298	test: 0.744298
MAE train: 0.344650	val: 0.576524	test: 0.581590

Epoch: 114
Loss: 0.24049572398265204
RMSE train: 0.440167	val: 0.764797	test: 0.741961
MAE train: 0.340743	val: 0.569597	test: 0.586528

Epoch: 115
Loss: 0.23888308927416801
RMSE train: 0.460278	val: 0.797197	test: 0.771996
MAE train: 0.357820	val: 0.598295	test: 0.601094

Epoch: 116
Loss: 0.23782598972320557
RMSE train: 0.429746	val: 0.766888	test: 0.737903
MAE train: 0.331540	val: 0.574219	test: 0.577034

Epoch: 117
Loss: 0.23496957619984946
RMSE train: 0.451791	val: 0.785605	test: 0.747948
MAE train: 0.350929	val: 0.584729	test: 0.585776

Epoch: 118
Loss: 0.2516941726207733
RMSE train: 0.444408	val: 0.773196	test: 0.724840
MAE train: 0.343733	val: 0.568311	test: 0.571237

Epoch: 119
Loss: 0.2359346312781175
RMSE train: 0.429234	val: 0.759024	test: 0.726648
MAE train: 0.330815	val: 0.558004	test: 0.566648

Epoch: 120
Loss: 0.23183507348100343
RMSE train: 0.447346	val: 0.768276	test: 0.742156
MAE train: 0.345052	val: 0.572261	test: 0.581269

Epoch: 121
Loss: 0.2372527817885081
RMSE train: 0.461307	val: 0.797644	test: 0.758788
MAE train: 0.359113	val: 0.589623	test: 0.597136

Epoch: 122
Loss: 0.23571919525663057
RMSE train: 0.423137	val: 0.774951	test: 0.731528
MAE train: 0.325654	val: 0.569992	test: 0.573083

Epoch: 123
Loss: 0.22674149150649706
RMSE train: 0.430438	val: 0.761068	test: 0.724193
MAE train: 0.335337	val: 0.564891	test: 0.571309

Epoch: 124
Loss: 0.231968575467666
RMSE train: 0.430507	val: 0.769308	test: 0.735954
MAE train: 0.333811	val: 0.572412	test: 0.574334

Epoch: 125
Loss: 0.23367461065451303
RMSE train: 0.430651	val: 0.761846	test: 0.733176
MAE train: 0.334589	val: 0.566618	test: 0.578892

Epoch: 126
Loss: 0.22869539881745973
RMSE train: 0.427716	val: 0.762282	test: 0.735910
MAE train: 0.329466	val: 0.571218	test: 0.574667

Epoch: 127
Loss: 0.2233771594862143
RMSE train: 0.434332	val: 0.780426	test: 0.747264
MAE train: 0.334817	val: 0.581596	test: 0.585300

Epoch: 128
Loss: 0.24065207441647848
RMSE train: 0.426838	val: 0.773147	test: 0.735749
MAE train: 0.327876	val: 0.573718	test: 0.576661

Epoch: 129
Loss: 0.24049125984311104
RMSE train: 0.439739	val: 0.781336	test: 0.732032
MAE train: 0.340299	val: 0.583915	test: 0.579094

Epoch: 130
Loss: 0.2390597201883793
RMSE train: 0.435098	val: 0.779533	test: 0.738233
MAE train: 0.333899	val: 0.579493	test: 0.571374

Epoch: 131
Loss: 0.23064474513133368
RMSE train: 0.413043	val: 0.756409	test: 0.712343
MAE train: 0.317571	val: 0.563140	test: 0.558396

Epoch: 132
Loss: 0.22415384898583093
RMSE train: 0.438564	val: 0.777028	test: 0.744539
MAE train: 0.339230	val: 0.581309	test: 0.580442

Epoch: 133
Loss: 0.22316719343264899
RMSE train: 0.418911	val: 0.755677	test: 0.730047
MAE train: 0.323437	val: 0.569367	test: 0.573256

Epoch: 134
Loss: 0.21441858013470969
RMSE train: 0.410239	val: 0.774358	test: 0.725346
MAE train: 0.314764	val: 0.581296	test: 0.568144

Epoch: 135
Loss: 0.22492013374964395
RMSE train: 0.410834	val: 0.769743	test: 0.730038
MAE train: 0.316768	val: 0.572354	test: 0.569992

Epoch: 136
Loss: 0.22035669287045798
RMSE train: 0.415804	val: 0.779003	test: 0.730071
MAE train: 0.320580	val: 0.583288	test: 0.564920

Epoch: 137
Loss: 0.2148622733851274
RMSE train: 0.426297	val: 0.768642	test: 0.732007
MAE train: 0.332199	val: 0.576758	test: 0.569966

Epoch: 138
Loss: 0.21471751481294632
RMSE train: 0.416162	val: 0.766093	test: 0.725923
MAE train: 0.320946	val: 0.572267	test: 0.564701

Epoch: 139
Loss: 0.22922080382704735
RMSE train: 0.437919	val: 0.798678	test: 0.759359
MAE train: 0.341071	val: 0.599690	test: 0.591271

Epoch: 140
Loss: 0.21546865751345953
RMSE train: 0.421262	val: 0.773796	test: 0.739023
MAE train: 0.325995	val: 0.578954	test: 0.578114

Epoch: 141
Loss: 0.20953717082738876
RMSE train: 0.403035	val: 0.771066	test: 0.728374
MAE train: 0.310669	val: 0.575044	test: 0.567860

Epoch: 142
Loss: 0.22015657151738802
RMSE train: 0.402770	val: 0.764885	test: 0.729126
MAE train: 0.311281	val: 0.572294	test: 0.570547

Epoch: 143
Loss: 0.21708924447496733
RMSE train: 0.430721	val: 0.777641	test: 0.736394
MAE train: 0.333840	val: 0.578529	test: 0.575580
MAE train: 0.405213	val: 0.593562	test: 0.588621

Epoch: 84
Loss: 0.3103056454232761
RMSE train: 0.504846	val: 0.769200	test: 0.757446
MAE train: 0.386549	val: 0.579893	test: 0.583364

Epoch: 85
Loss: 0.31352630044732777
RMSE train: 0.538573	val: 0.788530	test: 0.768202
MAE train: 0.417431	val: 0.600068	test: 0.600161

Epoch: 86
Loss: 0.31353230348655153
RMSE train: 0.494172	val: 0.758493	test: 0.757126
MAE train: 0.381655	val: 0.582893	test: 0.583999

Epoch: 87
Loss: 0.3086406652416502
RMSE train: 0.490258	val: 0.748613	test: 0.745111
MAE train: 0.377875	val: 0.568083	test: 0.580138

Epoch: 88
Loss: 0.3118025298629488
RMSE train: 0.481769	val: 0.751203	test: 0.751090
MAE train: 0.372078	val: 0.568608	test: 0.584590

Epoch: 89
Loss: 0.30220078357628416
RMSE train: 0.476279	val: 0.751598	test: 0.742922
MAE train: 0.363778	val: 0.564927	test: 0.574540

Epoch: 90
Loss: 0.3043304809502193
RMSE train: 0.493441	val: 0.754213	test: 0.748116
MAE train: 0.381142	val: 0.577254	test: 0.584206

Epoch: 91
Loss: 0.3106789248330252
RMSE train: 0.521209	val: 0.780423	test: 0.748706
MAE train: 0.402272	val: 0.587111	test: 0.582516

Epoch: 92
Loss: 0.33039092059646336
RMSE train: 0.479307	val: 0.728137	test: 0.743186
MAE train: 0.370995	val: 0.554456	test: 0.580805

Epoch: 93
Loss: 0.3012040640626635
RMSE train: 0.540781	val: 0.772865	test: 0.779461
MAE train: 0.418234	val: 0.584370	test: 0.612447

Epoch: 94
Loss: 0.3347613960504532
RMSE train: 0.498047	val: 0.763408	test: 0.742161
MAE train: 0.384788	val: 0.574284	test: 0.574029

Epoch: 95
Loss: 0.2980397769383022
RMSE train: 0.513982	val: 0.783608	test: 0.772133
MAE train: 0.396922	val: 0.589272	test: 0.597544

Epoch: 96
Loss: 0.30023140140942167
RMSE train: 0.493172	val: 0.772372	test: 0.758156
MAE train: 0.381415	val: 0.583751	test: 0.581253

Epoch: 97
Loss: 0.30145001837185453
RMSE train: 0.478470	val: 0.761378	test: 0.738517
MAE train: 0.368323	val: 0.572440	test: 0.564773

Epoch: 98
Loss: 0.28406284217323574
RMSE train: 0.500871	val: 0.767529	test: 0.754902
MAE train: 0.387102	val: 0.578707	test: 0.587123

Epoch: 99
Loss: 0.2831576679434095
RMSE train: 0.470223	val: 0.744914	test: 0.754956
MAE train: 0.363190	val: 0.564999	test: 0.585965

Epoch: 100
Loss: 0.2968431402529989
RMSE train: 0.477276	val: 0.764551	test: 0.744052
MAE train: 0.368324	val: 0.578531	test: 0.575394

Epoch: 101
Loss: 0.29680124670267105
RMSE train: 0.537582	val: 0.820444	test: 0.795610
MAE train: 0.413224	val: 0.621488	test: 0.623857

Epoch: 102
Loss: 0.28345961230141775
RMSE train: 0.476188	val: 0.745810	test: 0.766726
MAE train: 0.367183	val: 0.570107	test: 0.587003

Epoch: 103
Loss: 0.2874739776764597
RMSE train: 0.473297	val: 0.747494	test: 0.732993
MAE train: 0.366668	val: 0.566397	test: 0.574276

Epoch: 104
Loss: 0.30150037578174044
RMSE train: 0.481607	val: 0.750309	test: 0.733346
MAE train: 0.372066	val: 0.568605	test: 0.567350

Epoch: 105
Loss: 0.2883651448147638
RMSE train: 0.486435	val: 0.768463	test: 0.745424
MAE train: 0.376680	val: 0.572719	test: 0.582846

Epoch: 106
Loss: 0.2721085569688252
RMSE train: 0.464821	val: 0.746579	test: 0.729971
MAE train: 0.357866	val: 0.562554	test: 0.566167

Epoch: 107
Loss: 0.2974899634718895
RMSE train: 0.519260	val: 0.799123	test: 0.768121
MAE train: 0.402367	val: 0.598924	test: 0.602153

Epoch: 108
Loss: 0.2774070863212858
RMSE train: 0.477289	val: 0.761387	test: 0.744899
MAE train: 0.366561	val: 0.571484	test: 0.578249

Epoch: 109
Loss: 0.2796252401811736
RMSE train: 0.473443	val: 0.764393	test: 0.743970
MAE train: 0.364287	val: 0.573353	test: 0.574867

Epoch: 110
Loss: 0.26627698647124426
RMSE train: 0.457018	val: 0.743027	test: 0.738410
MAE train: 0.350090	val: 0.558087	test: 0.575514

Epoch: 111
Loss: 0.2708280107804707
RMSE train: 0.469467	val: 0.752439	test: 0.740339
MAE train: 0.362470	val: 0.565559	test: 0.578780

Epoch: 112
Loss: 0.2832351667540414
RMSE train: 0.462719	val: 0.756652	test: 0.734112
MAE train: 0.357104	val: 0.569282	test: 0.573347

Epoch: 113
Loss: 0.2890638474907194
RMSE train: 0.482943	val: 0.761546	test: 0.749668
MAE train: 0.372314	val: 0.576238	test: 0.586772

Epoch: 114
Loss: 0.28666810052735464
RMSE train: 0.479760	val: 0.771314	test: 0.753446
MAE train: 0.370826	val: 0.583369	test: 0.583380

Epoch: 115
Loss: 0.2810884958931378
RMSE train: 0.461810	val: 0.747244	test: 0.744100
MAE train: 0.356676	val: 0.565466	test: 0.579803

Epoch: 116
Loss: 0.2714062545980726
RMSE train: 0.461399	val: 0.761018	test: 0.743970
MAE train: 0.355181	val: 0.576017	test: 0.578693

Epoch: 117
Loss: 0.27958721986838747
RMSE train: 0.461802	val: 0.743904	test: 0.737392
MAE train: 0.355411	val: 0.565234	test: 0.574378

Epoch: 118
Loss: 0.2556478923984936
RMSE train: 0.460400	val: 0.754718	test: 0.739633
MAE train: 0.354823	val: 0.569082	test: 0.575085

Epoch: 119
Loss: 0.25621465912887026
RMSE train: 0.461237	val: 0.759787	test: 0.749331
MAE train: 0.354935	val: 0.570195	test: 0.575573

Epoch: 120
Loss: 0.29327697839055744
RMSE train: 0.476323	val: 0.764126	test: 0.746857
MAE train: 0.370837	val: 0.578800	test: 0.580061

Epoch: 121
Loss: 0.2923500750746046
RMSE train: 0.474624	val: 0.761074	test: 0.747239
MAE train: 0.369367	val: 0.584828	test: 0.579848

Epoch: 122
Loss: 0.2628244683146477
RMSE train: 0.440233	val: 0.749309	test: 0.755661
MAE train: 0.338368	val: 0.569885	test: 0.575450

Epoch: 123
Loss: 0.2596373440963881
RMSE train: 0.455737	val: 0.751499	test: 0.743150
MAE train: 0.349594	val: 0.569044	test: 0.571916

Epoch: 124
Loss: 0.2640248430626733
RMSE train: 0.465338	val: 0.760478	test: 0.750458
MAE train: 0.358666	val: 0.575165	test: 0.584552

Epoch: 125
Loss: 0.2655332897390638
RMSE train: 0.455551	val: 0.764204	test: 0.741464
MAE train: 0.352018	val: 0.581252	test: 0.570387

Epoch: 126
Loss: 0.2645691047821726
RMSE train: 0.442987	val: 0.747243	test: 0.740962
MAE train: 0.341853	val: 0.570096	test: 0.572143

Epoch: 127
Loss: 0.2500615982072694
RMSE train: 0.462004	val: 0.768021	test: 0.754803
MAE train: 0.356656	val: 0.578085	test: 0.586096

Early stopping
Best (RMSE):	 train: 0.479307	val: 0.728137	test: 0.743186
Best (MAE):	 train: 0.370995	val: 0.554456	test: 0.580805


Epoch: 144
Loss: 0.20071845203638078
RMSE train: 0.451030	val: 0.846146	test: 0.853503
MAE train: 0.353816	val: 0.639084	test: 0.662303

Epoch: 145
Loss: 0.22258771806955338
RMSE train: 0.432344	val: 0.826315	test: 0.821967
MAE train: 0.336546	val: 0.625807	test: 0.633201

Epoch: 146
Loss: 0.20769682079553603
RMSE train: 0.402058	val: 0.828090	test: 0.824281
MAE train: 0.311181	val: 0.622919	test: 0.637574

Epoch: 147
Loss: 0.21722475439310074
RMSE train: 0.426982	val: 0.826387	test: 0.824634
MAE train: 0.331389	val: 0.621893	test: 0.639146

Epoch: 148
Loss: 0.21098177433013915
RMSE train: 0.400863	val: 0.818573	test: 0.813024
MAE train: 0.310804	val: 0.617714	test: 0.626711

Epoch: 149
Loss: 0.2051162302494049
RMSE train: 0.389006	val: 0.805673	test: 0.794446
MAE train: 0.300460	val: 0.610064	test: 0.614622

Epoch: 150
Loss: 0.19442834407091142
RMSE train: 0.438250	val: 0.845854	test: 0.839498
MAE train: 0.339698	val: 0.639777	test: 0.648274

Epoch: 151
Loss: 0.19925965666770934
RMSE train: 0.392918	val: 0.819180	test: 0.810202
MAE train: 0.303969	val: 0.616657	test: 0.624853

Epoch: 152
Loss: 0.20181695520877838
RMSE train: 0.402665	val: 0.827432	test: 0.822537
MAE train: 0.311580	val: 0.627031	test: 0.632924

Epoch: 153
Loss: 0.2007150322198868
RMSE train: 0.400486	val: 0.810874	test: 0.817303
MAE train: 0.307908	val: 0.613713	test: 0.628902

Epoch: 154
Loss: 0.20817998349666594
RMSE train: 0.436930	val: 0.844183	test: 0.841077
MAE train: 0.341793	val: 0.637110	test: 0.650042

Epoch: 155
Loss: 0.19580587297677993
RMSE train: 0.428765	val: 0.827924	test: 0.825095
MAE train: 0.333718	val: 0.628025	test: 0.632533

Epoch: 156
Loss: 0.19350371062755584
RMSE train: 0.418541	val: 0.824468	test: 0.824346
MAE train: 0.325258	val: 0.622093	test: 0.632436

Epoch: 157
Loss: 0.18967090994119645
RMSE train: 0.408102	val: 0.806545	test: 0.811508
MAE train: 0.315945	val: 0.609748	test: 0.623719

Epoch: 158
Loss: 0.19237957894802094
RMSE train: 0.424688	val: 0.840673	test: 0.844328
MAE train: 0.332178	val: 0.636929	test: 0.644994

Epoch: 159
Loss: 0.1926291063427925
RMSE train: 0.406491	val: 0.822367	test: 0.819474
MAE train: 0.316865	val: 0.623046	test: 0.629592

Epoch: 160
Loss: 0.20445702224969864
RMSE train: 0.408883	val: 0.827560	test: 0.828485
MAE train: 0.318968	val: 0.626576	test: 0.638019

Epoch: 161
Loss: 0.19606851041316986
RMSE train: 0.421073	val: 0.825405	test: 0.827467
MAE train: 0.327660	val: 0.623623	test: 0.639334

Epoch: 162
Loss: 0.19050590693950653
RMSE train: 0.401073	val: 0.817246	test: 0.817855
MAE train: 0.311814	val: 0.619379	test: 0.629649

Epoch: 163
Loss: 0.18950696289539337
RMSE train: 0.397628	val: 0.829314	test: 0.821501
MAE train: 0.307638	val: 0.629958	test: 0.633062

Epoch: 164
Loss: 0.19236528873443604
RMSE train: 0.429464	val: 0.846527	test: 0.853063
MAE train: 0.337569	val: 0.644226	test: 0.656589

Epoch: 165
Loss: 0.1931459903717041
RMSE train: 0.414057	val: 0.846270	test: 0.848168
MAE train: 0.322804	val: 0.638535	test: 0.651405

Epoch: 166
Loss: 0.1816182866692543
RMSE train: 0.426280	val: 0.851861	test: 0.847739
MAE train: 0.333904	val: 0.644649	test: 0.653728

Epoch: 167
Loss: 0.18436476439237595
RMSE train: 0.423068	val: 0.837884	test: 0.835811
MAE train: 0.330430	val: 0.635919	test: 0.645713

Epoch: 168
Loss: 0.19060790836811065
RMSE train: 0.389419	val: 0.817574	test: 0.824274
MAE train: 0.300444	val: 0.616216	test: 0.636052

Epoch: 169
Loss: 0.18577890545129777
RMSE train: 0.395841	val: 0.818134	test: 0.821958
MAE train: 0.307299	val: 0.616785	test: 0.632579

Epoch: 170
Loss: 0.19877952635288237
RMSE train: 0.414482	val: 0.835593	test: 0.838016
MAE train: 0.320834	val: 0.635477	test: 0.651979

Early stopping
Best (RMSE):	 train: 0.408890	val: 0.804888	test: 0.800601
Best (MAE):	 train: 0.318481	val: 0.612201	test: 0.619126

MAE train: 0.394623	val: 0.599419	test: 0.595816

Epoch: 84
Loss: 0.32463317683764864
RMSE train: 0.515888	val: 0.787015	test: 0.747444
MAE train: 0.398324	val: 0.595171	test: 0.590500

Epoch: 85
Loss: 0.32724024781158995
RMSE train: 0.519087	val: 0.808044	test: 0.770072
MAE train: 0.399427	val: 0.618543	test: 0.597943

Epoch: 86
Loss: 0.32150176593235563
RMSE train: 0.524198	val: 0.800290	test: 0.762593
MAE train: 0.406794	val: 0.607838	test: 0.602357

Epoch: 87
Loss: 0.3439579967941557
RMSE train: 0.506102	val: 0.774506	test: 0.757359
MAE train: 0.392270	val: 0.586049	test: 0.600064

Epoch: 88
Loss: 0.33948062147412983
RMSE train: 0.537600	val: 0.806838	test: 0.774576
MAE train: 0.417334	val: 0.612919	test: 0.613948

Epoch: 89
Loss: 0.3048772918326514
RMSE train: 0.509586	val: 0.787607	test: 0.752845
MAE train: 0.395654	val: 0.598218	test: 0.591224

Epoch: 90
Loss: 0.3100151057754244
RMSE train: 0.493311	val: 0.772165	test: 0.752700
MAE train: 0.383405	val: 0.583329	test: 0.597430

Epoch: 91
Loss: 0.3219407945871353
RMSE train: 0.511835	val: 0.772466	test: 0.749863
MAE train: 0.397207	val: 0.587463	test: 0.596664

Epoch: 92
Loss: 0.3316951330218996
RMSE train: 0.506634	val: 0.784099	test: 0.755588
MAE train: 0.394843	val: 0.590830	test: 0.598151

Epoch: 93
Loss: 0.30497387264456066
RMSE train: 0.482611	val: 0.758195	test: 0.744709
MAE train: 0.373067	val: 0.575345	test: 0.585501

Epoch: 94
Loss: 0.34836783153670176
RMSE train: 0.505408	val: 0.772635	test: 0.738212
MAE train: 0.389671	val: 0.584738	test: 0.581315

Epoch: 95
Loss: 0.32669927392687115
RMSE train: 0.500199	val: 0.761661	test: 0.747238
MAE train: 0.386391	val: 0.577542	test: 0.595263

Epoch: 96
Loss: 0.302730313369206
RMSE train: 0.532190	val: 0.814502	test: 0.777823
MAE train: 0.412940	val: 0.621602	test: 0.610668

Epoch: 97
Loss: 0.3199825701969011
RMSE train: 0.498706	val: 0.777141	test: 0.746257
MAE train: 0.384243	val: 0.580706	test: 0.585898

Epoch: 98
Loss: 0.3191438892057964
RMSE train: 0.495527	val: 0.772123	test: 0.746123
MAE train: 0.383701	val: 0.583725	test: 0.596500

Epoch: 99
Loss: 0.293812449489321
RMSE train: 0.482598	val: 0.771062	test: 0.745585
MAE train: 0.372741	val: 0.586888	test: 0.586536

Epoch: 100
Loss: 0.3029837948935373
RMSE train: 0.495229	val: 0.777788	test: 0.748174
MAE train: 0.383667	val: 0.586927	test: 0.595371

Epoch: 101
Loss: 0.2945838911192758
RMSE train: 0.512889	val: 0.802873	test: 0.756683
MAE train: 0.395585	val: 0.611756	test: 0.596185

Epoch: 102
Loss: 0.29014710017613005
RMSE train: 0.511673	val: 0.774556	test: 0.749550
MAE train: 0.397748	val: 0.587664	test: 0.590675

Epoch: 103
Loss: 0.29971498463835033
RMSE train: 0.495928	val: 0.782629	test: 0.758940
MAE train: 0.382380	val: 0.594095	test: 0.600300

Epoch: 104
Loss: 0.2960960247686931
RMSE train: 0.486875	val: 0.782783	test: 0.748504
MAE train: 0.375443	val: 0.598314	test: 0.586192

Epoch: 105
Loss: 0.2834893177662577
RMSE train: 0.531465	val: 0.812835	test: 0.776160
MAE train: 0.415148	val: 0.620257	test: 0.610779

Epoch: 106
Loss: 0.3016891841377531
RMSE train: 0.470410	val: 0.765367	test: 0.738397
MAE train: 0.364408	val: 0.581410	test: 0.581119

Epoch: 107
Loss: 0.28770781947033747
RMSE train: 0.482297	val: 0.771046	test: 0.745931
MAE train: 0.372365	val: 0.585342	test: 0.587574

Epoch: 108
Loss: 0.29179894498416353
RMSE train: 0.484249	val: 0.768093	test: 0.748794
MAE train: 0.373566	val: 0.578237	test: 0.586887

Epoch: 109
Loss: 0.2793324685522488
RMSE train: 0.486569	val: 0.789626	test: 0.759540
MAE train: 0.376971	val: 0.596423	test: 0.605504

Epoch: 110
Loss: 0.2904978192278317
RMSE train: 0.487244	val: 0.783635	test: 0.750149
MAE train: 0.376255	val: 0.592433	test: 0.592521

Epoch: 111
Loss: 0.28859507611819674
RMSE train: 0.464571	val: 0.761145	test: 0.736996
MAE train: 0.355980	val: 0.573096	test: 0.577548

Epoch: 112
Loss: 0.28296529608113424
RMSE train: 0.500146	val: 0.790960	test: 0.751864
MAE train: 0.387805	val: 0.596939	test: 0.598841

Epoch: 113
Loss: 0.2868151015469006
RMSE train: 0.477300	val: 0.768902	test: 0.740127
MAE train: 0.366975	val: 0.573710	test: 0.586797

Epoch: 114
Loss: 0.28590068008218494
RMSE train: 0.482973	val: 0.775441	test: 0.756766
MAE train: 0.374400	val: 0.591446	test: 0.596861

Epoch: 115
Loss: 0.27162235443081173
RMSE train: 0.483218	val: 0.796926	test: 0.744415
MAE train: 0.374146	val: 0.602663	test: 0.584418

Epoch: 116
Loss: 0.27995582137789043
RMSE train: 0.476089	val: 0.762999	test: 0.731205
MAE train: 0.368011	val: 0.575892	test: 0.581728

Epoch: 117
Loss: 0.2731962076255253
RMSE train: 0.474035	val: 0.778629	test: 0.742437
MAE train: 0.365094	val: 0.589739	test: 0.578586

Epoch: 118
Loss: 0.28317517042160034
RMSE train: 0.497766	val: 0.805397	test: 0.757888
MAE train: 0.386262	val: 0.607649	test: 0.600542

Epoch: 119
Loss: 0.27336619049310684
RMSE train: 0.452906	val: 0.760690	test: 0.735630
MAE train: 0.348780	val: 0.577301	test: 0.575386

Epoch: 120
Loss: 0.2774388226015227
RMSE train: 0.473957	val: 0.767098	test: 0.747360
MAE train: 0.364170	val: 0.584317	test: 0.587304

Epoch: 121
Loss: 0.27490832443748203
RMSE train: 0.506914	val: 0.796160	test: 0.772444
MAE train: 0.394398	val: 0.612250	test: 0.604238

Epoch: 122
Loss: 0.2730386810643332
RMSE train: 0.476658	val: 0.786397	test: 0.774519
MAE train: 0.369254	val: 0.593171	test: 0.602779

Epoch: 123
Loss: 0.26404560783079695
RMSE train: 0.484140	val: 0.771710	test: 0.743830
MAE train: 0.374711	val: 0.588200	test: 0.587068

Epoch: 124
Loss: 0.26239496895245146
RMSE train: 0.459969	val: 0.753404	test: 0.738878
MAE train: 0.355665	val: 0.571898	test: 0.584419

Epoch: 125
Loss: 0.26676606812647413
RMSE train: 0.476979	val: 0.775246	test: 0.749309
MAE train: 0.368558	val: 0.586416	test: 0.589303

Epoch: 126
Loss: 0.2605786823800632
RMSE train: 0.457609	val: 0.779573	test: 0.767108
MAE train: 0.352400	val: 0.592335	test: 0.597963

Epoch: 127
Loss: 0.27464235786880764
RMSE train: 0.480604	val: 0.778721	test: 0.747475
MAE train: 0.375508	val: 0.589280	test: 0.587815

Epoch: 128
Loss: 0.26586756110191345
RMSE train: 0.459977	val: 0.782552	test: 0.749754
MAE train: 0.355314	val: 0.587334	test: 0.590068

Epoch: 129
Loss: 0.2656085395387241
RMSE train: 0.443468	val: 0.766019	test: 0.742877
MAE train: 0.342622	val: 0.581203	test: 0.583016

Epoch: 130
Loss: 0.2765305233853204
RMSE train: 0.472614	val: 0.766521	test: 0.748641
MAE train: 0.369284	val: 0.578767	test: 0.585979

Epoch: 131
Loss: 0.2663721133555685
RMSE train: 0.471971	val: 0.798311	test: 0.757131
MAE train: 0.364297	val: 0.607285	test: 0.593409

Epoch: 132
Loss: 0.2542329100625856
RMSE train: 0.445472	val: 0.774194	test: 0.741469
MAE train: 0.342503	val: 0.583386	test: 0.581132

Epoch: 133
Loss: 0.2615187806742532
RMSE train: 0.459364	val: 0.764242	test: 0.734989
MAE train: 0.359084	val: 0.586242	test: 0.582828

Epoch: 134
Loss: 0.25223032385110855
RMSE train: 0.460438	val: 0.764034	test: 0.739560
MAE train: 0.357152	val: 0.581689	test: 0.580540

Epoch: 135
Loss: 0.25437773977007183
RMSE train: 0.438619	val: 0.755934	test: 0.732598
MAE train: 0.339568	val: 0.572045	test: 0.576206

Epoch: 136
Loss: 0.24949346163443156
RMSE train: 0.476536	val: 0.785997	test: 0.763135
MAE train: 0.370643	val: 0.598635	test: 0.591204

Epoch: 137
Loss: 0.24835182300635747
RMSE train: 0.487679	val: 0.822405	test: 0.777954
MAE train: 0.379731	val: 0.625367	test: 0.608507

Epoch: 138
Loss: 0.24958446941205434
RMSE train: 0.442081	val: 0.764878	test: 0.744611
MAE train: 0.343025	val: 0.575583	test: 0.591329

Epoch: 139
Loss: 0.25302909101758686
RMSE train: 0.447390	val: 0.771407	test: 0.752439
MAE train: 0.346194	val: 0.580836	test: 0.595161

Epoch: 140
Loss: 0.2653299846819469
RMSE train: 0.448861	val: 0.756595	test: 0.735369
MAE train: 0.349722	val: 0.574268	test: 0.579948

Epoch: 141
Loss: 0.2592273439679827
RMSE train: 0.481551	val: 0.799198	test: 0.774632
MAE train: 0.372322	val: 0.609673	test: 0.606377

Epoch: 142
Loss: 0.2560913105096136
RMSE train: 0.470073	val: 0.790083	test: 0.749831
MAE train: 0.365582	val: 0.602076	test: 0.594239

Epoch: 143
Loss: 0.2521928144352777
RMSE train: 0.440972	val: 0.773375	test: 0.739683
MAE train: 0.339028	val: 0.591601	test: 0.577420

Epoch: 144
Loss: 0.21340505182743072
RMSE train: 0.401900	val: 0.780213	test: 0.784536
MAE train: 0.314874	val: 0.600212	test: 0.608770

Epoch: 145
Loss: 0.20254534035921096
RMSE train: 0.396094	val: 0.774921	test: 0.785220
MAE train: 0.310110	val: 0.595705	test: 0.605049

Epoch: 146
Loss: 0.1998518005013466
RMSE train: 0.396415	val: 0.780696	test: 0.794031
MAE train: 0.310035	val: 0.597642	test: 0.613436

Epoch: 147
Loss: 0.2009006917476654
RMSE train: 0.403909	val: 0.790942	test: 0.814380
MAE train: 0.315895	val: 0.603970	test: 0.629776

Epoch: 148
Loss: 0.2091584622859955
RMSE train: 0.401692	val: 0.788186	test: 0.806775
MAE train: 0.313524	val: 0.603232	test: 0.624289

Epoch: 149
Loss: 0.19865550845861435
RMSE train: 0.379986	val: 0.779999	test: 0.800257
MAE train: 0.295441	val: 0.598067	test: 0.621951

Epoch: 150
Loss: 0.1914399102330208
RMSE train: 0.392174	val: 0.783499	test: 0.801940
MAE train: 0.306556	val: 0.600845	test: 0.619664

Epoch: 151
Loss: 0.19617816656827927
RMSE train: 0.389473	val: 0.774643	test: 0.787595
MAE train: 0.304863	val: 0.595029	test: 0.606550

Epoch: 152
Loss: 0.20190523862838744
RMSE train: 0.377702	val: 0.782618	test: 0.792060
MAE train: 0.294524	val: 0.597651	test: 0.610415

Epoch: 153
Loss: 0.20576337575912476
RMSE train: 0.389924	val: 0.777319	test: 0.785721
MAE train: 0.304893	val: 0.596001	test: 0.609598

Epoch: 154
Loss: 0.20945118069648744
RMSE train: 0.403186	val: 0.775576	test: 0.789313
MAE train: 0.319236	val: 0.599295	test: 0.613104

Epoch: 155
Loss: 0.20145927220582963
RMSE train: 0.396296	val: 0.778605	test: 0.790887
MAE train: 0.312255	val: 0.596942	test: 0.611160

Epoch: 156
Loss: 0.20520692318677902
RMSE train: 0.393868	val: 0.779067	test: 0.797051
MAE train: 0.308310	val: 0.597208	test: 0.615881

Epoch: 157
Loss: 0.2070993199944496
RMSE train: 0.412312	val: 0.779598	test: 0.783879
MAE train: 0.325790	val: 0.601193	test: 0.614322

Epoch: 158
Loss: 0.20054543018341064
RMSE train: 0.439439	val: 0.830462	test: 0.825342
MAE train: 0.342216	val: 0.637866	test: 0.642860

Epoch: 159
Loss: 0.20013306438922882
RMSE train: 0.396514	val: 0.786411	test: 0.802817
MAE train: 0.308834	val: 0.603557	test: 0.620701

Epoch: 160
Loss: 0.2099214807152748
RMSE train: 0.388666	val: 0.773894	test: 0.785618
MAE train: 0.303712	val: 0.591687	test: 0.607768

Epoch: 161
Loss: 0.20067581236362458
RMSE train: 0.386589	val: 0.803806	test: 0.795857
MAE train: 0.300497	val: 0.615102	test: 0.616293

Epoch: 162
Loss: 0.18766607344150543
RMSE train: 0.382117	val: 0.785004	test: 0.781802
MAE train: 0.299657	val: 0.600966	test: 0.604271

Epoch: 163
Loss: 0.19187078773975372
RMSE train: 0.374826	val: 0.779550	test: 0.782488
MAE train: 0.289661	val: 0.593574	test: 0.606196

Epoch: 164
Loss: 0.19797335267066957
RMSE train: 0.384261	val: 0.783545	test: 0.792056
MAE train: 0.299748	val: 0.599128	test: 0.609282

Epoch: 165
Loss: 0.1970706507563591
RMSE train: 0.390135	val: 0.792239	test: 0.796106
MAE train: 0.304924	val: 0.604902	test: 0.613607

Epoch: 166
Loss: 0.2105354845523834
RMSE train: 0.385201	val: 0.782577	test: 0.795671
MAE train: 0.303642	val: 0.600972	test: 0.613526

Epoch: 167
Loss: 0.19456181526184083
RMSE train: 0.381386	val: 0.779788	test: 0.797350
MAE train: 0.299452	val: 0.597752	test: 0.614359

Epoch: 168
Loss: 0.18653787970542907
RMSE train: 0.392042	val: 0.801669	test: 0.798093
MAE train: 0.308077	val: 0.613955	test: 0.617424

Epoch: 169
Loss: 0.18807295858860015
RMSE train: 0.372054	val: 0.778780	test: 0.784117
MAE train: 0.289801	val: 0.594959	test: 0.605455

Epoch: 170
Loss: 0.18742599040269853
RMSE train: 0.387290	val: 0.787614	test: 0.787425
MAE train: 0.302604	val: 0.603927	test: 0.609618

Epoch: 171
Loss: 0.1915009006857872
RMSE train: 0.374228	val: 0.788746	test: 0.790038
MAE train: 0.292169	val: 0.603023	test: 0.611307

Epoch: 172
Loss: 0.18847678154706954
RMSE train: 0.375256	val: 0.779621	test: 0.781532
MAE train: 0.293502	val: 0.597571	test: 0.605269

Epoch: 173
Loss: 0.1842914879322052
RMSE train: 0.377396	val: 0.795764	test: 0.808713
MAE train: 0.293109	val: 0.609188	test: 0.625718

Epoch: 174
Loss: 0.19361962080001832
RMSE train: 0.382126	val: 0.795496	test: 0.797745
MAE train: 0.298247	val: 0.610151	test: 0.616031

Epoch: 175
Loss: 0.19121979176998138
RMSE train: 0.386682	val: 0.783903	test: 0.794346
MAE train: 0.302376	val: 0.602620	test: 0.612506

Early stopping
Best (RMSE):	 train: 0.397814	val: 0.766202	test: 0.792594
Best (MAE):	 train: 0.310293	val: 0.593723	test: 0.613473


Epoch: 144
Loss: 0.21809904277324677
RMSE train: 0.407497	val: 0.770813	test: 0.731398
MAE train: 0.314792	val: 0.571552	test: 0.576353

Epoch: 145
Loss: 0.21560660501321158
RMSE train: 0.425821	val: 0.782617	test: 0.739048
MAE train: 0.330794	val: 0.586383	test: 0.576111

Epoch: 146
Loss: 0.22779353708028793
RMSE train: 0.417189	val: 0.784260	test: 0.743414
MAE train: 0.322604	val: 0.588887	test: 0.576290

Epoch: 147
Loss: 0.21359750628471375
RMSE train: 0.403981	val: 0.771590	test: 0.733777
MAE train: 0.312127	val: 0.573016	test: 0.571377

Epoch: 148
Loss: 0.2123773085574309
RMSE train: 0.410065	val: 0.766562	test: 0.730650
MAE train: 0.319640	val: 0.571878	test: 0.571931

Epoch: 149
Loss: 0.20710640152295431
RMSE train: 0.411446	val: 0.775087	test: 0.735314
MAE train: 0.318757	val: 0.579686	test: 0.575623

Epoch: 150
Loss: 0.20118106653292975
RMSE train: 0.410912	val: 0.780118	test: 0.750821
MAE train: 0.318788	val: 0.580858	test: 0.584606

Epoch: 151
Loss: 0.19936179493864378
RMSE train: 0.399285	val: 0.761476	test: 0.731340
MAE train: 0.307800	val: 0.569457	test: 0.573709

Epoch: 152
Loss: 0.21139873564243317
RMSE train: 0.414076	val: 0.781590	test: 0.746272
MAE train: 0.318704	val: 0.585993	test: 0.583839

Epoch: 153
Loss: 0.21617856745918593
RMSE train: 0.402416	val: 0.759254	test: 0.732849
MAE train: 0.311093	val: 0.566946	test: 0.578957

Epoch: 154
Loss: 0.21169756973783174
RMSE train: 0.422310	val: 0.779374	test: 0.757827
MAE train: 0.326951	val: 0.591867	test: 0.592217

Epoch: 155
Loss: 0.20862878238161406
RMSE train: 0.401664	val: 0.757567	test: 0.726885
MAE train: 0.310145	val: 0.566483	test: 0.572271

Epoch: 156
Loss: 0.20217169697086015
RMSE train: 0.391371	val: 0.770016	test: 0.732789
MAE train: 0.300627	val: 0.567416	test: 0.573848

Epoch: 157
Loss: 0.20417352641622225
RMSE train: 0.411703	val: 0.772033	test: 0.741663
MAE train: 0.319349	val: 0.570897	test: 0.579537

Epoch: 158
Loss: 0.1968204453587532
RMSE train: 0.387026	val: 0.776001	test: 0.739030
MAE train: 0.298117	val: 0.578092	test: 0.578058

Epoch: 159
Loss: 0.19415739675362906
RMSE train: 0.421739	val: 0.794451	test: 0.748763
MAE train: 0.328543	val: 0.590223	test: 0.588477

Epoch: 160
Loss: 0.19517597183585167
RMSE train: 0.386068	val: 0.763027	test: 0.732641
MAE train: 0.298291	val: 0.570731	test: 0.571133

Epoch: 161
Loss: 0.20299116522073746
RMSE train: 0.389651	val: 0.774706	test: 0.730084
MAE train: 0.302129	val: 0.573866	test: 0.573809

Epoch: 162
Loss: 0.1944722111026446
RMSE train: 0.385377	val: 0.770124	test: 0.735787
MAE train: 0.297821	val: 0.573148	test: 0.573561

Epoch: 163
Loss: 0.19792628909150758
RMSE train: 0.393418	val: 0.773403	test: 0.728205
MAE train: 0.303421	val: 0.573412	test: 0.569850

Epoch: 164
Loss: 0.1973772073785464
RMSE train: 0.394920	val: 0.762057	test: 0.721745
MAE train: 0.306302	val: 0.566906	test: 0.566392

Epoch: 165
Loss: 0.2054007239639759
RMSE train: 0.388278	val: 0.770253	test: 0.726252
MAE train: 0.299620	val: 0.566638	test: 0.572570

Epoch: 166
Loss: 0.1953160067399343
RMSE train: 0.389620	val: 0.765781	test: 0.730916
MAE train: 0.303136	val: 0.572494	test: 0.572615

Epoch: 167
Loss: 0.1926589123904705
RMSE train: 0.408325	val: 0.777332	test: 0.744604
MAE train: 0.318170	val: 0.580703	test: 0.590208

Epoch: 168
Loss: 0.18051686882972717
RMSE train: 0.378396	val: 0.770419	test: 0.726824
MAE train: 0.293202	val: 0.574175	test: 0.567785

Early stopping
Best (RMSE):	 train: 0.418911	val: 0.755677	test: 0.730047
Best (MAE):	 train: 0.323437	val: 0.569367	test: 0.573256
All runs completed.

MAE train: 0.398087	val: 0.590158	test: 0.591431

Epoch: 84
Loss: 0.31806786358356476
RMSE train: 0.549505	val: 0.809655	test: 0.761421
MAE train: 0.429143	val: 0.618155	test: 0.608310

Epoch: 85
Loss: 0.2923425648893629
RMSE train: 0.524503	val: 0.796973	test: 0.762032
MAE train: 0.407658	val: 0.605222	test: 0.604810

Epoch: 86
Loss: 0.31500300765037537
RMSE train: 0.519354	val: 0.764701	test: 0.739082
MAE train: 0.405277	val: 0.587787	test: 0.595353

Epoch: 87
Loss: 0.3059204859392984
RMSE train: 0.507903	val: 0.769513	test: 0.739499
MAE train: 0.393318	val: 0.588630	test: 0.586119

Epoch: 88
Loss: 0.3138822934457234
RMSE train: 0.519990	val: 0.776556	test: 0.742045
MAE train: 0.404068	val: 0.597648	test: 0.600029

Epoch: 89
Loss: 0.3104699434978621
RMSE train: 0.496446	val: 0.766905	test: 0.736131
MAE train: 0.386357	val: 0.583287	test: 0.585169

Epoch: 90
Loss: 0.3158563950232097
RMSE train: 0.507715	val: 0.759517	test: 0.739897
MAE train: 0.397906	val: 0.581235	test: 0.588117

Epoch: 91
Loss: 0.30285702858652386
RMSE train: 0.498732	val: 0.768129	test: 0.738466
MAE train: 0.387658	val: 0.582027	test: 0.585493

Epoch: 92
Loss: 0.3018527328968048
RMSE train: 0.502016	val: 0.767950	test: 0.743951
MAE train: 0.391035	val: 0.579207	test: 0.590446

Epoch: 93
Loss: 0.2817398863179343
RMSE train: 0.499911	val: 0.772975	test: 0.731846
MAE train: 0.389460	val: 0.578454	test: 0.575128

Epoch: 94
Loss: 0.3000598485980715
RMSE train: 0.498726	val: 0.769343	test: 0.734254
MAE train: 0.388574	val: 0.586480	test: 0.586921

Epoch: 95
Loss: 0.30754998326301575
RMSE train: 0.523692	val: 0.782238	test: 0.739295
MAE train: 0.409045	val: 0.600405	test: 0.588141

Epoch: 96
Loss: 0.32246888322489603
RMSE train: 0.493239	val: 0.767501	test: 0.736547
MAE train: 0.383228	val: 0.584391	test: 0.588440

Epoch: 97
Loss: 0.2862046275820051
RMSE train: 0.508651	val: 0.780145	test: 0.745431
MAE train: 0.394886	val: 0.587065	test: 0.587928

Epoch: 98
Loss: 0.29428944843155996
RMSE train: 0.501487	val: 0.776978	test: 0.731014
MAE train: 0.388451	val: 0.589395	test: 0.581210

Epoch: 99
Loss: 0.319562309554645
RMSE train: 0.502337	val: 0.763622	test: 0.741791
MAE train: 0.390459	val: 0.578326	test: 0.587819

Epoch: 100
Loss: 0.3187042645045689
RMSE train: 0.505500	val: 0.765642	test: 0.743045
MAE train: 0.395170	val: 0.586735	test: 0.588515

Epoch: 101
Loss: 0.3132899765457426
RMSE train: 0.519308	val: 0.784650	test: 0.760325
MAE train: 0.401624	val: 0.599446	test: 0.603909

Epoch: 102
Loss: 0.28433844447135925
RMSE train: 0.528107	val: 0.795902	test: 0.760510
MAE train: 0.410470	val: 0.600711	test: 0.602668

Epoch: 103
Loss: 0.3038165995052883
RMSE train: 0.477425	val: 0.759672	test: 0.743314
MAE train: 0.370128	val: 0.574736	test: 0.583911

Epoch: 104
Loss: 0.31220642903021406
RMSE train: 0.501839	val: 0.778310	test: 0.740259
MAE train: 0.392572	val: 0.591053	test: 0.592212

Epoch: 105
Loss: 0.2879589657698359
RMSE train: 0.492211	val: 0.777559	test: 0.744852
MAE train: 0.384862	val: 0.588332	test: 0.588877

Epoch: 106
Loss: 0.2979011535644531
RMSE train: 0.476621	val: 0.769489	test: 0.741275
MAE train: 0.370519	val: 0.579255	test: 0.583838

Epoch: 107
Loss: 0.2770610547491482
RMSE train: 0.488848	val: 0.769071	test: 0.738970
MAE train: 0.381750	val: 0.584020	test: 0.587466

Epoch: 108
Loss: 0.29393999704292845
RMSE train: 0.491673	val: 0.788550	test: 0.747543
MAE train: 0.381713	val: 0.599365	test: 0.588843

Epoch: 109
Loss: 0.2764151266642979
RMSE train: 0.477804	val: 0.773148	test: 0.734042
MAE train: 0.371425	val: 0.585875	test: 0.581334

Epoch: 110
Loss: 0.28142726314919336
RMSE train: 0.496999	val: 0.786505	test: 0.740371
MAE train: 0.387991	val: 0.596237	test: 0.586848

Epoch: 111
Loss: 0.2729963447366442
RMSE train: 0.464510	val: 0.753002	test: 0.726969
MAE train: 0.358999	val: 0.571403	test: 0.574141

Epoch: 112
Loss: 0.27150547397988184
RMSE train: 0.474413	val: 0.779354	test: 0.738574
MAE train: 0.367307	val: 0.589386	test: 0.582519

Epoch: 113
Loss: 0.2804159341113908
RMSE train: 0.494563	val: 0.764876	test: 0.736932
MAE train: 0.385685	val: 0.587203	test: 0.589253

Epoch: 114
Loss: 0.2689484975167683
RMSE train: 0.466743	val: 0.758862	test: 0.733389
MAE train: 0.363999	val: 0.573650	test: 0.575291

Epoch: 115
Loss: 0.2856200209685734
RMSE train: 0.471964	val: 0.757122	test: 0.723842
MAE train: 0.367700	val: 0.573969	test: 0.576282

Epoch: 116
Loss: 0.25857794497694286
RMSE train: 0.488642	val: 0.767629	test: 0.737637
MAE train: 0.381177	val: 0.584489	test: 0.588325

Epoch: 117
Loss: 0.2724589726754597
RMSE train: 0.485153	val: 0.779199	test: 0.745828
MAE train: 0.376261	val: 0.594909	test: 0.587442

Epoch: 118
Loss: 0.25036684423685074
RMSE train: 0.486161	val: 0.771097	test: 0.741300
MAE train: 0.380186	val: 0.589899	test: 0.582327

Epoch: 119
Loss: 0.2580460011959076
RMSE train: 0.461403	val: 0.761186	test: 0.722907
MAE train: 0.358005	val: 0.579459	test: 0.570987

Epoch: 120
Loss: 0.266425932092326
RMSE train: 0.481727	val: 0.778643	test: 0.743003
MAE train: 0.376418	val: 0.593402	test: 0.588105

Epoch: 121
Loss: 0.2616164279835565
RMSE train: 0.468306	val: 0.751792	test: 0.733946
MAE train: 0.365815	val: 0.573400	test: 0.587726

Epoch: 122
Loss: 0.27235269865819384
RMSE train: 0.469093	val: 0.754415	test: 0.723106
MAE train: 0.367066	val: 0.575634	test: 0.574617

Epoch: 123
Loss: 0.25571716151067186
RMSE train: 0.463001	val: 0.759692	test: 0.724010
MAE train: 0.362226	val: 0.585313	test: 0.577865

Epoch: 124
Loss: 0.25857860169240404
RMSE train: 0.472130	val: 0.769041	test: 0.737469
MAE train: 0.365858	val: 0.588551	test: 0.584901

Epoch: 125
Loss: 0.25277052181107657
RMSE train: 0.468851	val: 0.775712	test: 0.738297
MAE train: 0.365334	val: 0.591284	test: 0.586624

Epoch: 126
Loss: 0.2698685271399362
RMSE train: 0.465790	val: 0.764223	test: 0.739179
MAE train: 0.362859	val: 0.576626	test: 0.585025

Epoch: 127
Loss: 0.26663790217467714
RMSE train: 0.459207	val: 0.776755	test: 0.743655
MAE train: 0.355621	val: 0.585785	test: 0.583198

Epoch: 128
Loss: 0.2643511784928186
RMSE train: 0.476198	val: 0.787253	test: 0.744789
MAE train: 0.371553	val: 0.594460	test: 0.583308

Epoch: 129
Loss: 0.2885377768959318
RMSE train: 0.457010	val: 0.771467	test: 0.728175
MAE train: 0.353903	val: 0.585422	test: 0.576629

Epoch: 130
Loss: 0.26380580557244165
RMSE train: 0.459627	val: 0.760982	test: 0.724984
MAE train: 0.358009	val: 0.581056	test: 0.571534

Epoch: 131
Loss: 0.2632924062865121
RMSE train: 0.471214	val: 0.776332	test: 0.753095
MAE train: 0.365461	val: 0.586233	test: 0.586846

Epoch: 132
Loss: 0.25052095630339216
RMSE train: 0.466653	val: 0.761832	test: 0.743894
MAE train: 0.363235	val: 0.581798	test: 0.588898

Epoch: 133
Loss: 0.25827540350811823
RMSE train: 0.441621	val: 0.755390	test: 0.730259
MAE train: 0.343434	val: 0.574153	test: 0.575438

Epoch: 134
Loss: 0.25599352589675356
RMSE train: 0.468413	val: 0.770748	test: 0.735219
MAE train: 0.363439	val: 0.578252	test: 0.580963

Epoch: 135
Loss: 0.24818065123898642
RMSE train: 0.448980	val: 0.762504	test: 0.726732
MAE train: 0.348768	val: 0.581034	test: 0.576418

Epoch: 136
Loss: 0.25150305352040697
RMSE train: 0.487480	val: 0.781697	test: 0.753521
MAE train: 0.380910	val: 0.591662	test: 0.597834

Epoch: 137
Loss: 0.25674211978912354
RMSE train: 0.434843	val: 0.755307	test: 0.739522
MAE train: 0.335910	val: 0.570117	test: 0.582984

Epoch: 138
Loss: 0.2557791354400771
RMSE train: 0.455658	val: 0.760489	test: 0.736319
MAE train: 0.356043	val: 0.581477	test: 0.583644

Epoch: 139
Loss: 0.2569415122270584
RMSE train: 0.437192	val: 0.763328	test: 0.739235
MAE train: 0.337455	val: 0.573155	test: 0.578363

Epoch: 140
Loss: 0.24505352228879929
RMSE train: 0.452960	val: 0.757003	test: 0.733818
MAE train: 0.354361	val: 0.577791	test: 0.584020

Epoch: 141
Loss: 0.24121059051581792
RMSE train: 0.440965	val: 0.757301	test: 0.725107
MAE train: 0.342252	val: 0.577159	test: 0.574380

Epoch: 142
Loss: 0.2310123581971441
RMSE train: 0.446074	val: 0.772366	test: 0.726318
MAE train: 0.347129	val: 0.584562	test: 0.577426

Epoch: 143
Loss: 0.24080384841987065
RMSE train: 0.435841	val: 0.777161	test: 0.732265
MAE train: 0.337013	val: 0.588986	test: 0.576359

Epoch: 144
Loss: 0.2158554196357727
RMSE train: 0.397666	val: 0.794306	test: 0.809681
MAE train: 0.307761	val: 0.599872	test: 0.619783

Epoch: 145
Loss: 0.21125753819942475
RMSE train: 0.410411	val: 0.794279	test: 0.802459
MAE train: 0.320587	val: 0.605369	test: 0.617458

Epoch: 146
Loss: 0.21760929226875306
RMSE train: 0.409304	val: 0.807191	test: 0.807802
MAE train: 0.321008	val: 0.617617	test: 0.621654

Epoch: 147
Loss: 0.21351980715990065
RMSE train: 0.398054	val: 0.798568	test: 0.812932
MAE train: 0.310316	val: 0.606415	test: 0.626135

Epoch: 148
Loss: 0.21429118067026137
RMSE train: 0.397725	val: 0.778727	test: 0.798900
MAE train: 0.310391	val: 0.596431	test: 0.616664

Epoch: 149
Loss: 0.19804681837558746
RMSE train: 0.398301	val: 0.804736	test: 0.820987
MAE train: 0.309449	val: 0.610144	test: 0.632921

Epoch: 150
Loss: 0.20846460312604903
RMSE train: 0.393551	val: 0.795361	test: 0.807639
MAE train: 0.306881	val: 0.599086	test: 0.618994

Epoch: 151
Loss: 0.20123290568590163
RMSE train: 0.391196	val: 0.799963	test: 0.809252
MAE train: 0.303633	val: 0.605336	test: 0.618392

Epoch: 152
Loss: 0.21352221965789794
RMSE train: 0.399201	val: 0.782906	test: 0.797241
MAE train: 0.313552	val: 0.599226	test: 0.615161

Epoch: 153
Loss: 0.2030415028333664
RMSE train: 0.389142	val: 0.783944	test: 0.797882
MAE train: 0.304044	val: 0.593895	test: 0.611566

Epoch: 154
Loss: 0.1990142822265625
RMSE train: 0.391083	val: 0.800366	test: 0.808925
MAE train: 0.304199	val: 0.604428	test: 0.620867

Epoch: 155
Loss: 0.20149696469306946
RMSE train: 0.394367	val: 0.784927	test: 0.798924
MAE train: 0.307746	val: 0.597042	test: 0.615767

Epoch: 156
Loss: 0.18865637928247453
RMSE train: 0.379876	val: 0.785640	test: 0.807466
MAE train: 0.295165	val: 0.598065	test: 0.616195

Epoch: 157
Loss: 0.20049162954092026
RMSE train: 0.388699	val: 0.792829	test: 0.805465
MAE train: 0.302131	val: 0.603545	test: 0.620317

Epoch: 158
Loss: 0.1932064414024353
RMSE train: 0.394753	val: 0.812214	test: 0.833692
MAE train: 0.305879	val: 0.613967	test: 0.636808

Epoch: 159
Loss: 0.19864711463451384
RMSE train: 0.383553	val: 0.787526	test: 0.814248
MAE train: 0.297209	val: 0.599262	test: 0.619130

Epoch: 160
Loss: 0.19911192059516908
RMSE train: 0.394678	val: 0.794365	test: 0.817101
MAE train: 0.306986	val: 0.602111	test: 0.626891

Epoch: 161
Loss: 0.19631221443414687
RMSE train: 0.388566	val: 0.792671	test: 0.811385
MAE train: 0.301749	val: 0.610302	test: 0.619268

Epoch: 162
Loss: 0.2005114495754242
RMSE train: 0.380799	val: 0.797641	test: 0.811362
MAE train: 0.295264	val: 0.608707	test: 0.618924

Epoch: 163
Loss: 0.20332444608211517
RMSE train: 0.407425	val: 0.789123	test: 0.816040
MAE train: 0.317063	val: 0.603372	test: 0.620507

Epoch: 164
Loss: 0.210298053920269
RMSE train: 0.409043	val: 0.788588	test: 0.806071
MAE train: 0.320984	val: 0.607871	test: 0.621708

Epoch: 165
Loss: 0.1851791709661484
RMSE train: 0.389522	val: 0.781870	test: 0.793863
MAE train: 0.305788	val: 0.602018	test: 0.610232

Epoch: 166
Loss: 0.1957089126110077
RMSE train: 0.393566	val: 0.801937	test: 0.817703
MAE train: 0.308361	val: 0.606796	test: 0.628143

Epoch: 167
Loss: 0.19460911750793458
RMSE train: 0.399664	val: 0.794738	test: 0.817595
MAE train: 0.312518	val: 0.606924	test: 0.622093

Epoch: 168
Loss: 0.20144710540771485
RMSE train: 0.375764	val: 0.794783	test: 0.812599
MAE train: 0.292558	val: 0.604477	test: 0.619365

Epoch: 169
Loss: 0.19158191829919816
RMSE train: 0.378231	val: 0.791192	test: 0.810991
MAE train: 0.294606	val: 0.598237	test: 0.622170

Epoch: 170
Loss: 0.18918177783489226
RMSE train: 0.385768	val: 0.796438	test: 0.807377
MAE train: 0.299473	val: 0.601046	test: 0.618454

Epoch: 171
Loss: 0.1885388970375061
RMSE train: 0.373223	val: 0.801229	test: 0.817640
MAE train: 0.290006	val: 0.605402	test: 0.621903

Epoch: 172
Loss: 0.1886751264333725
RMSE train: 0.405335	val: 0.795992	test: 0.811769
MAE train: 0.319448	val: 0.610707	test: 0.623803

Epoch: 173
Loss: 0.19364870190620423
RMSE train: 0.375491	val: 0.792904	test: 0.811688
MAE train: 0.292231	val: 0.600252	test: 0.620001

Epoch: 174
Loss: 0.1870422676205635
RMSE train: 0.387682	val: 0.793669	test: 0.810427
MAE train: 0.302100	val: 0.602178	test: 0.625298

Epoch: 175
Loss: 0.1914423406124115
RMSE train: 0.376392	val: 0.800642	test: 0.821505
MAE train: 0.291315	val: 0.606620	test: 0.631960

Epoch: 176
Loss: 0.1850810870528221
RMSE train: 0.372687	val: 0.803176	test: 0.821918
MAE train: 0.289296	val: 0.608163	test: 0.629935

Epoch: 177
Loss: 0.18134166449308395
RMSE train: 0.384085	val: 0.781416	test: 0.796263
MAE train: 0.303090	val: 0.596537	test: 0.611311

Epoch: 178
Loss: 0.18441010862588883
RMSE train: 0.380093	val: 0.813301	test: 0.830542
MAE train: 0.295632	val: 0.612055	test: 0.634117

Epoch: 179
Loss: 0.1901320368051529
RMSE train: 0.373511	val: 0.794647	test: 0.811691
MAE train: 0.291884	val: 0.601138	test: 0.613913

Epoch: 180
Loss: 0.1917628288269043
RMSE train: 0.359160	val: 0.787837	test: 0.804173
MAE train: 0.278072	val: 0.594895	test: 0.613706

Epoch: 181
Loss: 0.1900892123579979
RMSE train: 0.407851	val: 0.817473	test: 0.813536
MAE train: 0.318195	val: 0.619200	test: 0.630115

Epoch: 182
Loss: 0.18289831280708313
RMSE train: 0.364763	val: 0.795321	test: 0.808232
MAE train: 0.285913	val: 0.601948	test: 0.615301

Epoch: 183
Loss: 0.17856086641550065
RMSE train: 0.361957	val: 0.800206	test: 0.808556
MAE train: 0.280797	val: 0.602521	test: 0.619339

Early stopping
Best (RMSE):	 train: 0.397725	val: 0.778727	test: 0.798900
Best (MAE):	 train: 0.310391	val: 0.596431	test: 0.616664
All runs completed.


Epoch: 144
Loss: 0.2439912791763033
RMSE train: 0.450545	val: 0.790670	test: 0.744021
MAE train: 0.347022	val: 0.599391	test: 0.582080

Epoch: 145
Loss: 0.23472086872373307
RMSE train: 0.445480	val: 0.774108	test: 0.741222
MAE train: 0.346501	val: 0.586737	test: 0.583386

Epoch: 146
Loss: 0.23437929579189845
RMSE train: 0.460489	val: 0.782831	test: 0.751419
MAE train: 0.356738	val: 0.594480	test: 0.580796

Epoch: 147
Loss: 0.25382707587310244
RMSE train: 0.439989	val: 0.763608	test: 0.728119
MAE train: 0.341044	val: 0.580877	test: 0.572239

Epoch: 148
Loss: 0.2530567539589746
RMSE train: 0.475913	val: 0.807660	test: 0.762849
MAE train: 0.369740	val: 0.614669	test: 0.598003

Epoch: 149
Loss: 0.25509196519851685
RMSE train: 0.448220	val: 0.781570	test: 0.744896
MAE train: 0.346006	val: 0.594915	test: 0.587788

Epoch: 150
Loss: 0.24747509402888163
RMSE train: 0.436565	val: 0.756595	test: 0.721355
MAE train: 0.337829	val: 0.579048	test: 0.565743

Epoch: 151
Loss: 0.2664580802832331
RMSE train: 0.450041	val: 0.778998	test: 0.735826
MAE train: 0.349836	val: 0.593685	test: 0.575418

Epoch: 152
Loss: 0.24643706104585103
RMSE train: 0.423165	val: 0.774968	test: 0.727789
MAE train: 0.326396	val: 0.594369	test: 0.571317

Epoch: 153
Loss: 0.22407918210540498
RMSE train: 0.462318	val: 0.791067	test: 0.744354
MAE train: 0.360764	val: 0.603046	test: 0.584548

Epoch: 154
Loss: 0.2260024600795337
RMSE train: 0.434695	val: 0.774818	test: 0.734752
MAE train: 0.335108	val: 0.583986	test: 0.579408

Epoch: 155
Loss: 0.22524360780205047
RMSE train: 0.449968	val: 0.772294	test: 0.713437
MAE train: 0.351100	val: 0.587096	test: 0.565886

Epoch: 156
Loss: 0.22423151774065836
RMSE train: 0.419430	val: 0.768286	test: 0.717526
MAE train: 0.323573	val: 0.583445	test: 0.557626

Epoch: 157
Loss: 0.22845457707132613
RMSE train: 0.427177	val: 0.768997	test: 0.729887
MAE train: 0.331426	val: 0.584930	test: 0.576014

Epoch: 158
Loss: 0.23173866633858
RMSE train: 0.427882	val: 0.767938	test: 0.738354
MAE train: 0.331027	val: 0.580713	test: 0.574870

Epoch: 159
Loss: 0.22475602477788925
RMSE train: 0.442101	val: 0.785926	test: 0.736990
MAE train: 0.341909	val: 0.596754	test: 0.579311

Early stopping
Best (RMSE):	 train: 0.459969	val: 0.753404	test: 0.738878
Best (MAE):	 train: 0.355665	val: 0.571898	test: 0.584419


Epoch: 144
Loss: 0.24969421646424703
RMSE train: 0.445058	val: 0.766739	test: 0.727074
MAE train: 0.345280	val: 0.586035	test: 0.572076

Epoch: 145
Loss: 0.24986139897789275
RMSE train: 0.437489	val: 0.747283	test: 0.725300
MAE train: 0.338283	val: 0.571531	test: 0.568313

Epoch: 146
Loss: 0.23593495254005706
RMSE train: 0.478197	val: 0.787891	test: 0.747962
MAE train: 0.372059	val: 0.596405	test: 0.591013

Epoch: 147
Loss: 0.238355030970914
RMSE train: 0.450550	val: 0.785501	test: 0.744720
MAE train: 0.350297	val: 0.595522	test: 0.582342

Epoch: 148
Loss: 0.22817820417029516
RMSE train: 0.438785	val: 0.767998	test: 0.725299
MAE train: 0.340738	val: 0.585097	test: 0.570394

Epoch: 149
Loss: 0.2386176117828914
RMSE train: 0.432413	val: 0.763252	test: 0.732771
MAE train: 0.336037	val: 0.582862	test: 0.576589

Epoch: 150
Loss: 0.23591823556593486
RMSE train: 0.456292	val: 0.777032	test: 0.742687
MAE train: 0.354774	val: 0.593596	test: 0.589164

Epoch: 151
Loss: 0.24175536313227244
RMSE train: 0.439463	val: 0.772989	test: 0.732402
MAE train: 0.341545	val: 0.587991	test: 0.580331

Epoch: 152
Loss: 0.23985451140574046
RMSE train: 0.442882	val: 0.773765	test: 0.747995
MAE train: 0.342468	val: 0.581921	test: 0.580781

Epoch: 153
Loss: 0.23330211426530564
RMSE train: 0.439588	val: 0.774670	test: 0.727483
MAE train: 0.342400	val: 0.590845	test: 0.575945

Epoch: 154
Loss: 0.21954475662537984
RMSE train: 0.451548	val: 0.787383	test: 0.754645
MAE train: 0.351845	val: 0.591687	test: 0.593083

Epoch: 155
Loss: 0.22794153967073985
RMSE train: 0.437028	val: 0.773666	test: 0.725773
MAE train: 0.338291	val: 0.587361	test: 0.575925

Epoch: 156
Loss: 0.24329497239419393
RMSE train: 0.435885	val: 0.775415	test: 0.731513
MAE train: 0.339117	val: 0.588091	test: 0.580115

Epoch: 157
Loss: 0.23721134662628174
RMSE train: 0.427966	val: 0.755153	test: 0.738241
MAE train: 0.332393	val: 0.573876	test: 0.579985

Epoch: 158
Loss: 0.22795829602650233
RMSE train: 0.430322	val: 0.760003	test: 0.730155
MAE train: 0.337042	val: 0.582341	test: 0.582249

Epoch: 159
Loss: 0.2202215056334223
RMSE train: 0.425043	val: 0.764260	test: 0.733297
MAE train: 0.330091	val: 0.578392	test: 0.575244

Epoch: 160
Loss: 0.21456799975463323
RMSE train: 0.431889	val: 0.762474	test: 0.724139
MAE train: 0.336331	val: 0.582243	test: 0.576756

Epoch: 161
Loss: 0.22683881861822947
RMSE train: 0.424745	val: 0.762037	test: 0.724658
MAE train: 0.329656	val: 0.579580	test: 0.574324

Epoch: 162
Loss: 0.21143601728337152
RMSE train: 0.429945	val: 0.769579	test: 0.732091
MAE train: 0.334172	val: 0.583623	test: 0.581115

Epoch: 163
Loss: 0.21842894703149796
RMSE train: 0.426142	val: 0.759522	test: 0.737674
MAE train: 0.329812	val: 0.575149	test: 0.584231

Epoch: 164
Loss: 0.21711178443261556
RMSE train: 0.439628	val: 0.758850	test: 0.731651
MAE train: 0.340555	val: 0.578608	test: 0.580706

Epoch: 165
Loss: 0.21578063922269003
RMSE train: 0.441995	val: 0.758093	test: 0.728598
MAE train: 0.343010	val: 0.579135	test: 0.569400

Epoch: 166
Loss: 0.21773891576698848
RMSE train: 0.427191	val: 0.755229	test: 0.725761
MAE train: 0.334106	val: 0.579809	test: 0.578843

Epoch: 167
Loss: 0.22778178645031794
RMSE train: 0.462783	val: 0.791336	test: 0.747993
MAE train: 0.361688	val: 0.601604	test: 0.588030

Epoch: 168
Loss: 0.21407532053334372
RMSE train: 0.440262	val: 0.785489	test: 0.735150
MAE train: 0.340723	val: 0.596121	test: 0.577532

Epoch: 169
Loss: 0.21438414922782353
RMSE train: 0.435512	val: 0.780055	test: 0.733076
MAE train: 0.338666	val: 0.586074	test: 0.576143

Epoch: 170
Loss: 0.22355064111096518
RMSE train: 0.431218	val: 0.771983	test: 0.723365
MAE train: 0.335988	val: 0.583105	test: 0.565153

Epoch: 171
Loss: 0.2250168781195368
RMSE train: 0.412651	val: 0.759545	test: 0.721109
MAE train: 0.320497	val: 0.572760	test: 0.565623

Epoch: 172
Loss: 0.22476078889199666
RMSE train: 0.420321	val: 0.747588	test: 0.723031
MAE train: 0.327491	val: 0.571306	test: 0.569065

Epoch: 173
Loss: 0.22455224075487681
RMSE train: 0.451132	val: 0.791129	test: 0.757666
MAE train: 0.353459	val: 0.598400	test: 0.593713

Epoch: 174
Loss: 0.21189315723521368
RMSE train: 0.434853	val: 0.764245	test: 0.749167
MAE train: 0.336366	val: 0.579658	test: 0.595248

Epoch: 175
Loss: 0.214308357664517
RMSE train: 0.412501	val: 0.750863	test: 0.740053
MAE train: 0.321063	val: 0.570955	test: 0.588687

Epoch: 176
Loss: 0.20254026991980417
RMSE train: 0.443233	val: 0.786395	test: 0.754872
MAE train: 0.346841	val: 0.594185	test: 0.593340

Epoch: 177
Loss: 0.20482037855046137
RMSE train: 0.439377	val: 0.777700	test: 0.742240
MAE train: 0.343262	val: 0.590403	test: 0.588348

Epoch: 178
Loss: 0.20886973717382976
RMSE train: 0.413044	val: 0.769851	test: 0.737660
MAE train: 0.319385	val: 0.582728	test: 0.578771

Epoch: 179
Loss: 0.21547759430749075
RMSE train: 0.433026	val: 0.768592	test: 0.737731
MAE train: 0.338612	val: 0.585079	test: 0.584735

Epoch: 180
Loss: 0.2059722446969577
RMSE train: 0.410157	val: 0.759344	test: 0.730870
MAE train: 0.317351	val: 0.576413	test: 0.577800

Early stopping
Best (RMSE):	 train: 0.437489	val: 0.747283	test: 0.725300
Best (MAE):	 train: 0.338283	val: 0.571531	test: 0.568313
All runs completed.
