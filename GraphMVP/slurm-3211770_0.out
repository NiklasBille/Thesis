>>> Starting run for dataset: esol
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running RANDOM configs_static_noise_experiments/GraphCL/esol/noise=0.0.yml on cuda:0
Running RANDOM configs_static_noise_experiments/GraphCL/esol/noise=0.05.yml on cuda:1
Running RANDOM configs_static_noise_experiments/GraphCL/esol/noise=0.1.yml on cuda:2
Running RANDOM configs_static_noise_experiments/GraphCL/esol/noise=0.2.yml on cuda:3
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.0.yml --runseed 4 --device cuda:0
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.0.yml --runseed 5 --device cuda:0
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.1.yml --runseed 4 --device cuda:2
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.2.yml --runseed 4 --device cuda:3
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.0.yml --runseed 6 --device cuda:0
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.1.yml --runseed 5 --device cuda:2
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.2.yml --runseed 5 --device cuda:3
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.1.yml --runseed 6 --device cuda:2
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.2.yml --runseed 6 --device cuda:3
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.05.yml --runseed 4 --device cuda:1
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.05.yml --runseed 5 --device cuda:1
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.05.yml --runseed 6 --device cuda:1
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/esol/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/esol/noise=0.0/esol_scaff_5_26-05_11-28-50  ]
[ Using Seed :  5  ]
[ Using device :  cuda:0  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 11.87495231628418
RMSE train: 3.308964	val: 4.025411	test: 4.215745
MAE train: 2.688106	val: 3.547050	test: 3.693266

Epoch: 2
Loss: 10.539841175079346
RMSE train: 3.240281	val: 4.202743	test: 4.389304
MAE train: 2.645422	val: 3.831739	test: 3.983328

Epoch: 3
Loss: 9.718994140625
RMSE train: 3.123156	val: 4.181913	test: 4.399183
MAE train: 2.558639	val: 3.869480	test: 4.046994

Epoch: 4
Loss: 8.411359786987305
RMSE train: 2.975570	val: 4.043477	test: 4.317422
MAE train: 2.444288	val: 3.760524	test: 4.002115

Epoch: 5
Loss: 8.028965473175049
RMSE train: 2.775604	val: 3.712279	test: 4.033384
MAE train: 2.296070	val: 3.414523	test: 3.734318

Epoch: 6
Loss: 6.959194302558899
RMSE train: 2.655590	val: 3.529872	test: 3.864150
MAE train: 2.213378	val: 3.214741	test: 3.571740

Epoch: 7
Loss: 6.353060722351074
RMSE train: 2.525396	val: 3.365573	test: 3.681084
MAE train: 2.096032	val: 3.055326	test: 3.374643

Epoch: 8
Loss: 5.761744856834412
RMSE train: 2.303391	val: 3.128789	test: 3.378543
MAE train: 1.880777	val: 2.834053	test: 3.048912

Epoch: 9
Loss: 5.452953934669495
RMSE train: 2.250790	val: 3.097118	test: 3.298541
MAE train: 1.846522	val: 2.797861	test: 2.974206

Epoch: 10
Loss: 4.791606545448303
RMSE train: 2.292729	val: 3.168123	test: 3.334912
MAE train: 1.929805	val: 2.869791	test: 3.029906

Epoch: 11
Loss: 4.294748902320862
RMSE train: 2.277694	val: 3.211742	test: 3.330909
MAE train: 1.924619	val: 2.907798	test: 3.037319

Epoch: 12
Loss: 3.5597976446151733
RMSE train: 2.017042	val: 2.887791	test: 2.949052
MAE train: 1.672052	val: 2.580002	test: 2.622134

Epoch: 13
Loss: 3.1356324553489685
RMSE train: 1.750520	val: 2.452586	test: 2.510927
MAE train: 1.416173	val: 2.138076	test: 2.150804

Epoch: 14
Loss: 2.7717151641845703
RMSE train: 1.677159	val: 2.432313	test: 2.539234
MAE train: 1.358521	val: 2.122693	test: 2.194333

Epoch: 15
Loss: 2.4982985854148865
RMSE train: 1.452918	val: 2.190135	test: 2.339451
MAE train: 1.120875	val: 1.878189	test: 1.983947

Epoch: 16
Loss: 2.1407076120376587
RMSE train: 1.230224	val: 1.849219	test: 1.999707
MAE train: 0.909011	val: 1.560086	test: 1.617647

Epoch: 17
Loss: 1.8149844408035278
RMSE train: 1.168178	val: 1.841020	test: 1.972993
MAE train: 0.870317	val: 1.539534	test: 1.598357

Epoch: 18
Loss: 1.6830550730228424
RMSE train: 0.986454	val: 1.642303	test: 1.776484
MAE train: 0.714874	val: 1.357093	test: 1.399960

Epoch: 19
Loss: 1.4244571924209595
RMSE train: 0.882350	val: 1.509737	test: 1.641770
MAE train: 0.637307	val: 1.221486	test: 1.293478

Epoch: 20
Loss: 1.28079654276371
RMSE train: 0.800131	val: 1.365235	test: 1.528661
MAE train: 0.588995	val: 1.095917	test: 1.194689

Epoch: 21
Loss: 1.1153299659490585
RMSE train: 0.803621	val: 1.254245	test: 1.436382
MAE train: 0.605276	val: 1.018465	test: 1.122963

Epoch: 22
Loss: 1.0094527155160904
RMSE train: 0.832952	val: 1.388033	test: 1.517618
MAE train: 0.628307	val: 1.154728	test: 1.178182

Epoch: 23
Loss: 0.9597460180521011Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/esol/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/esol/noise=0.0/esol_scaff_6_26-05_11-28-50  ]
[ Using Seed :  6  ]
[ Using device :  cuda:0  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 11.326308965682983
RMSE train: 3.088088	val: 4.009775	test: 4.129580
MAE train: 2.509817	val: 3.563234	test: 3.603750

Epoch: 2
Loss: 10.51162075996399
RMSE train: 2.989585	val: 4.025507	test: 4.136922
MAE train: 2.422407	val: 3.637051	test: 3.690705

Epoch: 3
Loss: 8.911194682121277
RMSE train: 2.881495	val: 3.965656	test: 4.093876
MAE train: 2.338734	val: 3.603064	test: 3.712616

Epoch: 4
Loss: 8.119824290275574
RMSE train: 2.919488	val: 4.082573	test: 4.193741
MAE train: 2.423760	val: 3.802014	test: 3.875167

Epoch: 5
Loss: 7.408233404159546
RMSE train: 2.985528	val: 4.153175	test: 4.187175
MAE train: 2.522822	val: 3.890295	test: 3.871270

Epoch: 6
Loss: 6.963456153869629
RMSE train: 2.731808	val: 3.679136	test: 3.811848
MAE train: 2.291885	val: 3.386594	test: 3.461403

Epoch: 7
Loss: 5.989760041236877
RMSE train: 2.427444	val: 3.315067	test: 3.556347
MAE train: 1.992732	val: 2.998799	test: 3.196001

Epoch: 8
Loss: 5.396767973899841
RMSE train: 2.289902	val: 3.222633	test: 3.448244
MAE train: 1.851498	val: 2.910873	test: 3.085293

Epoch: 9
Loss: 4.974522352218628
RMSE train: 2.283120	val: 3.131106	test: 3.346199
MAE train: 1.859352	val: 2.835303	test: 2.967984

Epoch: 10
Loss: 4.324721813201904
RMSE train: 2.082446	val: 2.878579	test: 3.133032
MAE train: 1.685899	val: 2.585652	test: 2.771562

Epoch: 11
Loss: 3.918157398700714
RMSE train: 1.885227	val: 2.633435	test: 2.868043
MAE train: 1.530326	val: 2.346917	test: 2.544870

Epoch: 12
Loss: 3.439252257347107
RMSE train: 1.761184	val: 2.508420	test: 2.712452
MAE train: 1.425742	val: 2.234056	test: 2.408230

Epoch: 13
Loss: 2.960019111633301
RMSE train: 1.663037	val: 2.341277	test: 2.528819
MAE train: 1.329473	val: 2.085633	test: 2.216535

Epoch: 14
Loss: 2.566601276397705
RMSE train: 1.532499	val: 2.201942	test: 2.330622
MAE train: 1.221769	val: 1.927677	test: 2.007316

Epoch: 15
Loss: 2.1666102707386017
RMSE train: 1.360394	val: 2.008884	test: 2.124595
MAE train: 1.070156	val: 1.708184	test: 1.786296

Epoch: 16
Loss: 1.8688917458057404
RMSE train: 1.250708	val: 1.780596	test: 1.918981
MAE train: 0.969028	val: 1.509421	test: 1.589512

Epoch: 17
Loss: 1.5839594006538391
RMSE train: 1.056704	val: 1.491782	test: 1.686790
MAE train: 0.796155	val: 1.234525	test: 1.339568

Epoch: 18
Loss: 1.4404206275939941
RMSE train: 0.916884	val: 1.392574	test: 1.597549
MAE train: 0.676090	val: 1.131608	test: 1.273646

Epoch: 19
Loss: 1.2855519652366638
RMSE train: 0.876431	val: 1.315439	test: 1.489093
MAE train: 0.670789	val: 1.066827	test: 1.182460

Epoch: 20
Loss: 1.2550096809864044
RMSE train: 0.821977	val: 1.257985	test: 1.393985
MAE train: 0.638026	val: 1.002302	test: 1.123941

Epoch: 21
Loss: 1.0941429138183594
RMSE train: 0.768673	val: 1.247132	test: 1.348041
MAE train: 0.594331	val: 1.007040	test: 1.066262

Epoch: 22
Loss: 0.9269135743379593
RMSE train: 0.702193	val: 1.138490	test: 1.275694
MAE train: 0.537080	val: 0.923518	test: 0.995499

Epoch: 23
Loss: 0.8791170716285706Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/esol/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/esol/noise=0.0/esol_scaff_4_26-05_11-28-50  ]
[ Using Seed :  4  ]
[ Using device :  cuda:0  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 12.281308650970459
RMSE train: 3.254966	val: 4.243796	test: 4.376427
MAE train: 2.638792	val: 3.778740	test: 3.869685

Epoch: 2
Loss: 10.7481107711792
RMSE train: 3.177014	val: 4.180480	test: 4.332639
MAE train: 2.595764	val: 3.727067	test: 3.886897

Epoch: 3
Loss: 9.836762428283691
RMSE train: 3.159062	val: 4.254108	test: 4.444381
MAE train: 2.619821	val: 3.903158	test: 4.081402

Epoch: 4
Loss: 9.007363319396973
RMSE train: 3.119249	val: 4.210022	test: 4.441934
MAE train: 2.608183	val: 3.885603	test: 4.116655

Epoch: 5
Loss: 8.200872659683228
RMSE train: 3.073604	val: 4.168892	test: 4.412702
MAE train: 2.607722	val: 3.872652	test: 4.115743

Epoch: 6
Loss: 7.226111650466919
RMSE train: 3.046971	val: 4.150254	test: 4.441432
MAE train: 2.602146	val: 3.847700	test: 4.163460

Epoch: 7
Loss: 6.6834938526153564
RMSE train: 2.755545	val: 3.864152	test: 4.098693
MAE train: 2.313372	val: 3.571390	test: 3.811092

Epoch: 8
Loss: 5.871170163154602
RMSE train: 2.395489	val: 3.339106	test: 3.526528
MAE train: 1.960228	val: 3.030170	test: 3.188605

Epoch: 9
Loss: 5.335938811302185
RMSE train: 2.243328	val: 3.051719	test: 3.293367
MAE train: 1.820701	val: 2.723042	test: 2.937893

Epoch: 10
Loss: 5.032687306404114
RMSE train: 2.201452	val: 3.075003	test: 3.333102
MAE train: 1.802558	val: 2.749792	test: 3.001789

Epoch: 11
Loss: 4.416207313537598
RMSE train: 2.159140	val: 3.015729	test: 3.214012
MAE train: 1.790878	val: 2.714164	test: 2.891529

Epoch: 12
Loss: 3.8436588644981384
RMSE train: 1.984136	val: 2.828365	test: 2.968842
MAE train: 1.622529	val: 2.536009	test: 2.626706

Epoch: 13
Loss: 3.425124704837799
RMSE train: 1.740522	val: 2.493240	test: 2.625626
MAE train: 1.388062	val: 2.199971	test: 2.271974

Epoch: 14
Loss: 2.945213735103607
RMSE train: 1.617758	val: 2.340179	test: 2.443670
MAE train: 1.265845	val: 2.040904	test: 2.090156

Epoch: 15
Loss: 2.492448389530182
RMSE train: 1.438254	val: 2.166185	test: 2.240757
MAE train: 1.097362	val: 1.899205	test: 1.915713

Epoch: 16
Loss: 2.2167381644248962
RMSE train: 1.244034	val: 1.936580	test: 2.005793
MAE train: 0.912285	val: 1.663893	test: 1.676350

Epoch: 17
Loss: 1.9102890491485596
RMSE train: 1.109199	val: 1.709257	test: 1.779423
MAE train: 0.813612	val: 1.425253	test: 1.420115

Epoch: 18
Loss: 1.7369819283485413
RMSE train: 1.009202	val: 1.613116	test: 1.676878
MAE train: 0.742656	val: 1.346564	test: 1.325732

Epoch: 19
Loss: 1.5408432483673096
RMSE train: 0.943569	val: 1.601752	test: 1.636323
MAE train: 0.698352	val: 1.338078	test: 1.293316

Epoch: 20
Loss: 1.280878633260727
RMSE train: 0.858671	val: 1.549228	test: 1.554759
MAE train: 0.628703	val: 1.229537	test: 1.232581

Epoch: 21
Loss: 1.146217405796051
RMSE train: 0.840482	val: 1.296997	test: 1.405007
MAE train: 0.633169	val: 1.038602	test: 1.109644

Epoch: 22
Loss: 1.1025197952985764
RMSE train: 0.828230	val: 1.250075	test: 1.381562
MAE train: 0.623580	val: 1.004728	test: 1.095135

Epoch: 23
Loss: 1.0462318807840347Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:3
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/esol/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/esol/noise=0.2/esol_scaff_4_26-05_11-28-50  ]
[ Using Seed :  4  ]
[ Using device :  cuda:3  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 11.973120927810669
RMSE train: 3.526608	val: 4.184263	test: 4.303649
MAE train: 2.956624	val: 3.720930	test: 3.745725

Epoch: 2
Loss: 11.099747657775879
RMSE train: 3.417157	val: 4.124406	test: 4.199615
MAE train: 2.857038	val: 3.710554	test: 3.670092

Epoch: 3
Loss: 10.456371545791626
RMSE train: 3.311845	val: 4.096848	test: 4.122173
MAE train: 2.763991	val: 3.719443	test: 3.616837

Epoch: 4
Loss: 9.458964586257935
RMSE train: 3.189907	val: 4.042879	test: 4.040327
MAE train: 2.661411	val: 3.674275	test: 3.546231

Epoch: 5
Loss: 8.624881029129028
RMSE train: 3.067290	val: 4.055800	test: 4.001706
MAE train: 2.553238	val: 3.697121	test: 3.529168

Epoch: 6
Loss: 7.828106999397278
RMSE train: 2.902749	val: 4.090984	test: 3.970481
MAE train: 2.387994	val: 3.744509	test: 3.524181

Epoch: 7
Loss: 7.394005060195923
RMSE train: 2.687555	val: 3.958483	test: 3.812027
MAE train: 2.169552	val: 3.613312	test: 3.380644

Epoch: 8
Loss: 6.824995160102844
RMSE train: 2.465631	val: 3.653672	test: 3.537630
MAE train: 1.954035	val: 3.304224	test: 3.109305

Epoch: 9
Loss: 5.928390979766846
RMSE train: 2.215163	val: 3.291844	test: 3.202513
MAE train: 1.731140	val: 2.947466	test: 2.785881

Epoch: 10
Loss: 5.490443229675293
RMSE train: 2.096310	val: 3.139828	test: 3.057420
MAE train: 1.631325	val: 2.808967	test: 2.657211

Epoch: 11
Loss: 5.036127209663391
RMSE train: 1.981078	val: 2.948319	test: 2.892106
MAE train: 1.546464	val: 2.621440	test: 2.514280

Epoch: 12
Loss: 4.429086267948151
RMSE train: 1.921956	val: 2.960735	test: 2.907224
MAE train: 1.497679	val: 2.644778	test: 2.535718

Epoch: 13
Loss: 4.113785922527313
RMSE train: 1.867153	val: 2.960936	test: 2.906823
MAE train: 1.444714	val: 2.639808	test: 2.536741

Epoch: 14
Loss: 3.646572709083557
RMSE train: 1.718485	val: 2.774876	test: 2.738731
MAE train: 1.322181	val: 2.448741	test: 2.367149

Epoch: 15
Loss: 3.3289307355880737
RMSE train: 1.578867	val: 2.569419	test: 2.545777
MAE train: 1.218845	val: 2.213851	test: 2.169854

Epoch: 16
Loss: 2.9639288783073425
RMSE train: 1.462595	val: 2.369828	test: 2.376290
MAE train: 1.127505	val: 2.011588	test: 1.966119

Epoch: 17
Loss: 2.7772101759910583
RMSE train: 1.361631	val: 2.131667	test: 2.163428
MAE train: 1.053352	val: 1.769477	test: 1.741194

Epoch: 18
Loss: 2.6208771467208862
RMSE train: 1.234476	val: 1.977924	test: 1.995316
MAE train: 0.952687	val: 1.570149	test: 1.557270

Epoch: 19
Loss: 2.2566849291324615
RMSE train: 1.163844	val: 1.907415	test: 1.880812
MAE train: 0.913758	val: 1.450544	test: 1.458631

Epoch: 20
Loss: 1.9450689554214478
RMSE train: 1.172886	val: 1.860109	test: 1.814310
MAE train: 0.928350	val: 1.422380	test: 1.431576

Epoch: 21
Loss: 1.9074598550796509
RMSE train: 1.136487	val: 1.925369	test: 1.777418
MAE train: 0.901039	val: 1.440117	test: 1.408753

Epoch: 22
Loss: 1.6509276926517487
RMSE train: 1.115097	val: 1.872799	test: 1.742990Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/esol/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/esol/noise=0.05/esol_scaff_4_26-05_11-28-50  ]
[ Using Seed :  4  ]
[ Using device :  cuda:1  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 11.965245008468628
RMSE train: 3.370179	val: 4.223440	test: 4.330080
MAE train: 2.791012	val: 3.768833	test: 3.797022

Epoch: 2
Loss: 10.83295726776123
RMSE train: 3.281721	val: 4.373132	test: 4.393771
MAE train: 2.720444	val: 4.003024	test: 3.937211

Epoch: 3
Loss: 9.980424880981445
RMSE train: 3.271853	val: 4.667286	test: 4.620247
MAE train: 2.725890	val: 4.358138	test: 4.245172

Epoch: 4
Loss: 8.93431568145752
RMSE train: 3.228948	val: 4.682019	test: 4.651309
MAE train: 2.688718	val: 4.388613	test: 4.323659

Epoch: 5
Loss: 8.119067072868347
RMSE train: 3.083709	val: 4.408386	test: 4.425459
MAE train: 2.554754	val: 4.113559	test: 4.111621

Epoch: 6
Loss: 7.320236802101135
RMSE train: 2.832151	val: 3.956720	test: 4.045135
MAE train: 2.333002	val: 3.672708	test: 3.722545

Epoch: 7
Loss: 6.762190341949463
RMSE train: 2.610716	val: 3.602883	test: 3.737804
MAE train: 2.131842	val: 3.311000	test: 3.413139

Epoch: 8
Loss: 6.263597011566162
RMSE train: 2.438687	val: 3.359534	test: 3.508449
MAE train: 1.974402	val: 3.057553	test: 3.179654

Epoch: 9
Loss: 5.42902934551239
RMSE train: 2.278550	val: 3.100974	test: 3.249519
MAE train: 1.839231	val: 2.815453	test: 2.910849

Epoch: 10
Loss: 4.859499454498291
RMSE train: 2.165651	val: 2.894147	test: 3.068923
MAE train: 1.739058	val: 2.621440	test: 2.717552

Epoch: 11
Loss: 4.5918614864349365
RMSE train: 2.074328	val: 2.758704	test: 2.935704
MAE train: 1.668767	val: 2.478346	test: 2.588562

Epoch: 12
Loss: 4.01893824338913
RMSE train: 1.937931	val: 2.591105	test: 2.762377
MAE train: 1.543866	val: 2.307902	test: 2.415259

Epoch: 13
Loss: 3.6514065265655518
RMSE train: 1.768703	val: 2.268521	test: 2.453732
MAE train: 1.384088	val: 1.994659	test: 2.094462

Epoch: 14
Loss: 3.176911413669586
RMSE train: 1.643571	val: 2.078111	test: 2.260270
MAE train: 1.261596	val: 1.816002	test: 1.895243

Epoch: 15
Loss: 2.8183454275131226
RMSE train: 1.506609	val: 1.897294	test: 2.061993
MAE train: 1.151022	val: 1.658467	test: 1.709407

Epoch: 16
Loss: 2.570497155189514
RMSE train: 1.368512	val: 1.817603	test: 1.897254
MAE train: 1.042448	val: 1.555891	test: 1.512876

Epoch: 17
Loss: 2.2523149847984314
RMSE train: 1.228278	val: 1.705547	test: 1.701791
MAE train: 0.928186	val: 1.414901	test: 1.332885

Epoch: 18
Loss: 2.0984691381454468
RMSE train: 1.161668	val: 1.644112	test: 1.619852
MAE train: 0.878785	val: 1.336763	test: 1.262201

Epoch: 19
Loss: 1.7041184902191162
RMSE train: 1.099424	val: 1.640840	test: 1.610398
MAE train: 0.842455	val: 1.329784	test: 1.269672

Epoch: 20
Loss: 1.604570060968399
RMSE train: 1.008798	val: 1.596254	test: 1.562176
MAE train: 0.775210	val: 1.268868	test: 1.252653

Epoch: 21
Loss: 1.4178293943405151
RMSE train: 0.942059	val: 1.520305	test: 1.445278
MAE train: 0.717096	val: 1.184199	test: 1.173774

Epoch: 22
Loss: 1.2760663330554962
RMSE train: 0.917814	val: 1.471000	test: 1.378558Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/esol/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/esol/noise=0.05/esol_scaff_6_26-05_11-28-50  ]
[ Using Seed :  6  ]
[ Using device :  cuda:1  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 11.760957479476929
RMSE train: 3.056721	val: 3.933390	test: 4.089010
MAE train: 2.471632	val: 3.443592	test: 3.517721

Epoch: 2
Loss: 10.750683307647705
RMSE train: 2.946171	val: 3.829240	test: 3.982256
MAE train: 2.385855	val: 3.367695	test: 3.454849

Epoch: 3
Loss: 9.590412378311157
RMSE train: 2.842791	val: 3.789694	test: 3.892573
MAE train: 2.300114	val: 3.393361	test: 3.421482

Epoch: 4
Loss: 8.714649200439453
RMSE train: 2.788981	val: 3.752282	test: 3.801572
MAE train: 2.260359	val: 3.422669	test: 3.392225

Epoch: 5
Loss: 7.864333868026733
RMSE train: 2.641535	val: 3.592237	test: 3.618233
MAE train: 2.137387	val: 3.276675	test: 3.235899

Epoch: 6
Loss: 7.040446996688843
RMSE train: 2.474985	val: 3.383658	test: 3.468078
MAE train: 1.988554	val: 3.064988	test: 3.087456

Epoch: 7
Loss: 6.717394590377808
RMSE train: 2.364397	val: 3.221745	test: 3.402061
MAE train: 1.901141	val: 2.881539	test: 3.018130

Epoch: 8
Loss: 5.859408140182495
RMSE train: 2.284317	val: 3.108898	test: 3.317790
MAE train: 1.845848	val: 2.765667	test: 2.928399

Epoch: 9
Loss: 5.384709715843201
RMSE train: 2.276575	val: 3.052529	test: 3.276466
MAE train: 1.848074	val: 2.716108	test: 2.894061

Epoch: 10
Loss: 4.80259895324707
RMSE train: 2.145799	val: 2.905291	test: 3.132511
MAE train: 1.724933	val: 2.554907	test: 2.744072

Epoch: 11
Loss: 4.241963863372803
RMSE train: 1.946822	val: 2.707775	test: 2.947723
MAE train: 1.553492	val: 2.338820	test: 2.553968

Epoch: 12
Loss: 3.716370940208435
RMSE train: 1.774680	val: 2.484834	test: 2.724926
MAE train: 1.396374	val: 2.135970	test: 2.315248

Epoch: 13
Loss: 3.3669223189353943
RMSE train: 1.660835	val: 2.279526	test: 2.523183
MAE train: 1.302836	val: 1.967617	test: 2.109799

Epoch: 14
Loss: 2.9584737420082092
RMSE train: 1.559343	val: 2.190808	test: 2.395663
MAE train: 1.216206	val: 1.878622	test: 1.994710

Epoch: 15
Loss: 2.55434513092041
RMSE train: 1.445831	val: 2.184735	test: 2.384958
MAE train: 1.126475	val: 1.839353	test: 1.977099

Epoch: 16
Loss: 2.2413538694381714
RMSE train: 1.226072	val: 1.923667	test: 2.100057
MAE train: 0.927301	val: 1.574370	test: 1.705068

Epoch: 17
Loss: 1.9758816063404083
RMSE train: 1.162671	val: 1.773080	test: 1.925965
MAE train: 0.876943	val: 1.413465	test: 1.527405

Epoch: 18
Loss: 1.7968925833702087
RMSE train: 1.133908	val: 1.830480	test: 1.947366
MAE train: 0.853332	val: 1.459652	test: 1.543708

Epoch: 19
Loss: 1.598888874053955
RMSE train: 1.036952	val: 1.717328	test: 1.819850
MAE train: 0.773995	val: 1.382405	test: 1.442771

Epoch: 20
Loss: 1.41781547665596
RMSE train: 1.008247	val: 1.617980	test: 1.696772
MAE train: 0.764561	val: 1.325614	test: 1.353132

Epoch: 21
Loss: 1.2577460706233978
RMSE train: 0.921613	val: 1.523070	test: 1.571433
MAE train: 0.711061	val: 1.206458	test: 1.241071

Epoch: 22
Loss: 1.1372427940368652
RMSE train: 0.828685	val: 1.582180	test: 1.555656Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:3
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/esol/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/esol/noise=0.2/esol_scaff_5_26-05_11-28-50  ]
[ Using Seed :  5  ]
[ Using device :  cuda:3  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 12.47814679145813
RMSE train: 3.563705	val: 4.243529	test: 4.329041
MAE train: 2.986265	val: 3.847852	test: 3.818971

Epoch: 2
Loss: 11.296763896942139
RMSE train: 3.371844	val: 4.038899	test: 4.112192
MAE train: 2.806779	val: 3.627602	test: 3.597134

Epoch: 3
Loss: 10.77920913696289
RMSE train: 3.245210	val: 3.834359	test: 3.897303
MAE train: 2.688797	val: 3.384276	test: 3.357195

Epoch: 4
Loss: 9.74013090133667
RMSE train: 3.153647	val: 3.737777	test: 3.774097
MAE train: 2.605743	val: 3.259317	test: 3.211358

Epoch: 5
Loss: 8.743637800216675
RMSE train: 3.090576	val: 3.773243	test: 3.793555
MAE train: 2.558519	val: 3.292662	test: 3.269802

Epoch: 6
Loss: 8.031988263130188
RMSE train: 2.977510	val: 3.729292	test: 3.760160
MAE train: 2.457732	val: 3.267241	test: 3.290002

Epoch: 7
Loss: 7.382208585739136
RMSE train: 2.789878	val: 3.548651	test: 3.618833
MAE train: 2.279645	val: 3.126288	test: 3.206867

Epoch: 8
Loss: 6.7150784730911255
RMSE train: 2.539447	val: 3.255903	test: 3.356598
MAE train: 2.044009	val: 2.880414	test: 2.970451

Epoch: 9
Loss: 6.088836431503296
RMSE train: 2.316228	val: 2.859431	test: 2.961568
MAE train: 1.830327	val: 2.508022	test: 2.578362

Epoch: 10
Loss: 5.6570258140563965
RMSE train: 2.167454	val: 2.579585	test: 2.675563
MAE train: 1.689163	val: 2.236816	test: 2.278822

Epoch: 11
Loss: 5.347357869148254
RMSE train: 2.098170	val: 2.585725	test: 2.710883
MAE train: 1.635022	val: 2.241792	test: 2.309174

Epoch: 12
Loss: 4.718473553657532
RMSE train: 2.022645	val: 2.665670	test: 2.860472
MAE train: 1.577488	val: 2.311634	test: 2.443964

Epoch: 13
Loss: 4.1231929063797
RMSE train: 1.875132	val: 2.542907	test: 2.768417
MAE train: 1.438304	val: 2.200407	test: 2.345540

Epoch: 14
Loss: 3.7193708419799805
RMSE train: 1.717138	val: 2.535148	test: 2.737557
MAE train: 1.302307	val: 2.167297	test: 2.272773

Epoch: 15
Loss: 3.3157036304473877
RMSE train: 1.530479	val: 2.420670	test: 2.586383
MAE train: 1.149477	val: 2.032387	test: 2.106969

Epoch: 16
Loss: 3.092704713344574
RMSE train: 1.409910	val: 2.087864	test: 2.210960
MAE train: 1.058387	val: 1.729243	test: 1.792211

Epoch: 17
Loss: 2.686962068080902
RMSE train: 1.324236	val: 1.871531	test: 1.919577
MAE train: 0.988404	val: 1.516845	test: 1.538701

Epoch: 18
Loss: 2.3906558752059937
RMSE train: 1.266152	val: 1.770226	test: 1.837272
MAE train: 0.949284	val: 1.447685	test: 1.445271

Epoch: 19
Loss: 2.2103400826454163
RMSE train: 1.221016	val: 1.767880	test: 1.865507
MAE train: 0.918897	val: 1.494459	test: 1.485735

Epoch: 20
Loss: 2.0258816182613373
RMSE train: 1.127245	val: 1.605922	test: 1.640051
MAE train: 0.847322	val: 1.293744	test: 1.310208

Epoch: 21
Loss: 1.9412192404270172
RMSE train: 1.059545	val: 1.696607	test: 1.639869
MAE train: 0.806395	val: 1.296145	test: 1.312280

Epoch: 22
Loss: 1.6520767211914062
RMSE train: 1.035983	val: 1.810590	test: 1.769051Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/esol/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/esol/noise=0.1/esol_scaff_5_26-05_11-28-50  ]
[ Using Seed :  5  ]
[ Using device :  cuda:2  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 12.378717184066772
RMSE train: 3.383316	val: 4.228852	test: 4.327464
MAE train: 2.803481	val: 3.823775	test: 3.829735

Epoch: 2
Loss: 11.002730131149292
RMSE train: 3.233053	val: 4.135127	test: 4.217602
MAE train: 2.657369	val: 3.736253	test: 3.736740

Epoch: 3
Loss: 10.379984378814697
RMSE train: 3.149487	val: 4.069340	test: 4.134390
MAE train: 2.587460	val: 3.684420	test: 3.668131

Epoch: 4
Loss: 9.353675842285156
RMSE train: 3.082168	val: 4.012323	test: 4.060797
MAE train: 2.534576	val: 3.617284	test: 3.623429

Epoch: 5
Loss: 8.384191989898682
RMSE train: 2.975335	val: 3.993738	test: 4.048362
MAE train: 2.439895	val: 3.619495	test: 3.661861

Epoch: 6
Loss: 7.708222508430481
RMSE train: 2.739521	val: 3.775826	test: 3.842154
MAE train: 2.216954	val: 3.414499	test: 3.481204

Epoch: 7
Loss: 7.056572675704956
RMSE train: 2.548320	val: 3.553435	test: 3.630129
MAE train: 2.038730	val: 3.200923	test: 3.267218

Epoch: 8
Loss: 6.515003204345703
RMSE train: 2.425570	val: 3.251409	test: 3.369786
MAE train: 1.927322	val: 2.886888	test: 2.986824

Epoch: 9
Loss: 5.882880806922913
RMSE train: 2.329793	val: 3.078675	test: 3.220224
MAE train: 1.836575	val: 2.709493	test: 2.825532

Epoch: 10
Loss: 5.4344565868377686
RMSE train: 2.211249	val: 2.875628	test: 3.050338
MAE train: 1.731040	val: 2.496955	test: 2.658387

Epoch: 11
Loss: 5.037763357162476
RMSE train: 2.117689	val: 2.730394	test: 2.949247
MAE train: 1.661180	val: 2.350152	test: 2.553999

Epoch: 12
Loss: 4.505912780761719
RMSE train: 1.964803	val: 2.498403	test: 2.741693
MAE train: 1.531667	val: 2.135674	test: 2.331189

Epoch: 13
Loss: 3.7981691360473633
RMSE train: 1.754197	val: 2.192760	test: 2.443647
MAE train: 1.346988	val: 1.872674	test: 2.017812

Epoch: 14
Loss: 3.605772078037262
RMSE train: 1.616121	val: 2.043359	test: 2.269521
MAE train: 1.234706	val: 1.741308	test: 1.855261

Epoch: 15
Loss: 3.0470290184020996
RMSE train: 1.468822	val: 1.855182	test: 2.027970
MAE train: 1.118584	val: 1.570585	test: 1.630439

Epoch: 16
Loss: 2.7831186056137085
RMSE train: 1.368690	val: 1.638868	test: 1.785917
MAE train: 1.046052	val: 1.373698	test: 1.409057

Epoch: 17
Loss: 2.569255530834198
RMSE train: 1.284118	val: 1.575464	test: 1.678811
MAE train: 0.983795	val: 1.273638	test: 1.287956

Epoch: 18
Loss: 2.255229890346527
RMSE train: 1.217689	val: 1.587155	test: 1.645644
MAE train: 0.938182	val: 1.273190	test: 1.292209

Epoch: 19
Loss: 2.0674386024475098
RMSE train: 1.158297	val: 1.530496	test: 1.628826
MAE train: 0.893838	val: 1.209534	test: 1.312649

Epoch: 20
Loss: 1.9049025774002075
RMSE train: 1.111175	val: 1.646089	test: 1.687905
MAE train: 0.874208	val: 1.289981	test: 1.398933

Epoch: 21
Loss: 1.8053887486457825
RMSE train: 1.074934	val: 1.502387	test: 1.602786
MAE train: 0.849211	val: 1.164828	test: 1.318737

Epoch: 22
Loss: 1.5600759387016296
RMSE train: 1.019326	val: 1.468606	test: 1.572574Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:3
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/esol/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/esol/noise=0.2/esol_scaff_6_26-05_11-28-50  ]
[ Using Seed :  6  ]
[ Using device :  cuda:3  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 11.828298091888428
RMSE train: 3.202633	val: 4.073537	test: 4.181688
MAE train: 2.616392	val: 3.636151	test: 3.635137

Epoch: 2
Loss: 10.994696378707886
RMSE train: 3.230276	val: 4.015394	test: 4.119302
MAE train: 2.661899	val: 3.607057	test: 3.596249

Epoch: 3
Loss: 9.897665977478027
RMSE train: 3.179282	val: 3.944006	test: 4.041014
MAE train: 2.620992	val: 3.558844	test: 3.533258

Epoch: 4
Loss: 9.258762836456299
RMSE train: 3.076700	val: 3.800236	test: 3.910550
MAE train: 2.527773	val: 3.412705	test: 3.406892

Epoch: 5
Loss: 8.402979373931885
RMSE train: 2.960755	val: 3.700097	test: 3.798104
MAE train: 2.426556	val: 3.321164	test: 3.308242

Epoch: 6
Loss: 7.573057055473328
RMSE train: 2.818964	val: 3.642928	test: 3.725953
MAE train: 2.300236	val: 3.283735	test: 3.271206

Epoch: 7
Loss: 7.359965801239014
RMSE train: 2.620566	val: 3.451294	test: 3.526367
MAE train: 2.110863	val: 3.103845	test: 3.092539

Epoch: 8
Loss: 6.3574219942092896
RMSE train: 2.387130	val: 3.060604	test: 3.117755
MAE train: 1.894181	val: 2.703700	test: 2.675401

Epoch: 9
Loss: 5.907517194747925
RMSE train: 2.208299	val: 2.799952	test: 2.839752
MAE train: 1.736514	val: 2.417583	test: 2.376840

Epoch: 10
Loss: 5.271846532821655
RMSE train: 2.030593	val: 2.599174	test: 2.623350
MAE train: 1.592011	val: 2.205995	test: 2.163681

Epoch: 11
Loss: 4.7229931354522705
RMSE train: 1.927325	val: 2.527515	test: 2.550754
MAE train: 1.506496	val: 2.134872	test: 2.107875

Epoch: 12
Loss: 4.23787796497345
RMSE train: 1.834957	val: 2.517086	test: 2.510809
MAE train: 1.426258	val: 2.111016	test: 2.060228

Epoch: 13
Loss: 3.991593658924103
RMSE train: 1.692940	val: 2.197237	test: 2.199138
MAE train: 1.310634	val: 1.802882	test: 1.756182

Epoch: 14
Loss: 3.563951253890991
RMSE train: 1.595982	val: 1.893628	test: 1.996211
MAE train: 1.242655	val: 1.529986	test: 1.573041

Epoch: 15
Loss: 3.185388922691345
RMSE train: 1.523182	val: 2.068295	test: 2.086180
MAE train: 1.176149	val: 1.687832	test: 1.664576

Epoch: 16
Loss: 2.8738365173339844
RMSE train: 1.430257	val: 2.207177	test: 2.092000
MAE train: 1.104055	val: 1.789022	test: 1.709764

Epoch: 17
Loss: 2.689160168170929
RMSE train: 1.317131	val: 1.890203	test: 1.845452
MAE train: 1.022790	val: 1.505286	test: 1.464016

Epoch: 18
Loss: 2.3854355216026306
RMSE train: 1.262147	val: 1.993917	test: 1.851239
MAE train: 0.974017	val: 1.601780	test: 1.457935

Epoch: 19
Loss: 2.1782203912734985
RMSE train: 1.183431	val: 2.353045	test: 1.963185
MAE train: 0.912228	val: 1.819856	test: 1.540966

Epoch: 20
Loss: 1.926501840353012
RMSE train: 1.124604	val: 2.160545	test: 1.860453
MAE train: 0.870882	val: 1.702655	test: 1.461158

Epoch: 21
Loss: 1.6855349242687225
RMSE train: 1.089264	val: 1.820354	test: 1.769048
MAE train: 0.844403	val: 1.441720	test: 1.403432

Epoch: 22
Loss: 1.7224069833755493
RMSE train: 1.019670	val: 1.811724	test: 1.696375Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/esol/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/esol/noise=0.1/esol_scaff_6_26-05_11-28-50  ]
[ Using Seed :  6  ]
[ Using device :  cuda:2  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 11.832475900650024
RMSE train: 3.229621	val: 4.136375	test: 4.230604
MAE train: 2.641537	val: 3.709397	test: 3.691520

Epoch: 2
Loss: 10.849493980407715
RMSE train: 3.112051	val: 4.001528	test: 4.108940
MAE train: 2.540511	val: 3.589908	test: 3.589456

Epoch: 3
Loss: 9.798974514007568
RMSE train: 2.986241	val: 3.849107	test: 3.967076
MAE train: 2.429805	val: 3.436159	test: 3.478615

Epoch: 4
Loss: 9.00258183479309
RMSE train: 2.908810	val: 3.865395	test: 3.945186
MAE train: 2.368456	val: 3.475050	test: 3.508355

Epoch: 5
Loss: 8.08835482597351
RMSE train: 2.852660	val: 3.931261	test: 4.000130
MAE train: 2.320239	val: 3.596689	test: 3.621376

Epoch: 6
Loss: 7.30984091758728
RMSE train: 2.738724	val: 3.875866	test: 3.960622
MAE train: 2.222258	val: 3.581670	test: 3.604640

Epoch: 7
Loss: 6.978040099143982
RMSE train: 2.529006	val: 3.610897	test: 3.743453
MAE train: 2.019958	val: 3.315677	test: 3.366065

Epoch: 8
Loss: 6.071454763412476
RMSE train: 2.363968	val: 3.343229	test: 3.495787
MAE train: 1.864361	val: 3.046736	test: 3.093552

Epoch: 9
Loss: 5.645848989486694
RMSE train: 2.238664	val: 3.067924	test: 3.204529
MAE train: 1.762785	val: 2.772469	test: 2.805588

Epoch: 10
Loss: 4.924505829811096
RMSE train: 2.134638	val: 2.872702	test: 3.012150
MAE train: 1.690866	val: 2.587231	test: 2.615342

Epoch: 11
Loss: 4.4537153244018555
RMSE train: 2.057768	val: 2.813602	test: 2.967076
MAE train: 1.643344	val: 2.515103	test: 2.576156

Epoch: 12
Loss: 3.9224772453308105
RMSE train: 1.929089	val: 2.612058	test: 2.792260
MAE train: 1.517255	val: 2.315077	test: 2.401664

Epoch: 13
Loss: 3.6320138573646545
RMSE train: 1.799942	val: 2.312449	test: 2.542178
MAE train: 1.385566	val: 2.013725	test: 2.128560

Epoch: 14
Loss: 3.199754059314728
RMSE train: 1.659993	val: 2.152880	test: 2.328002
MAE train: 1.265759	val: 1.863385	test: 1.940791

Epoch: 15
Loss: 2.908125162124634
RMSE train: 1.504832	val: 2.086770	test: 2.192511
MAE train: 1.153049	val: 1.760043	test: 1.812761

Epoch: 16
Loss: 2.537006974220276
RMSE train: 1.347839	val: 1.880746	test: 1.944205
MAE train: 1.023024	val: 1.528384	test: 1.561366

Epoch: 17
Loss: 2.187560111284256
RMSE train: 1.214712	val: 1.680180	test: 1.704731
MAE train: 0.917073	val: 1.316091	test: 1.342595

Epoch: 18
Loss: 2.0476089119911194
RMSE train: 1.153078	val: 1.658359	test: 1.645138
MAE train: 0.867674	val: 1.293331	test: 1.288781

Epoch: 19
Loss: 1.8119627833366394
RMSE train: 1.105307	val: 1.638874	test: 1.614462
MAE train: 0.836222	val: 1.284411	test: 1.275551

Epoch: 20
Loss: 1.5732908248901367
RMSE train: 1.104166	val: 1.699772	test: 1.659263
MAE train: 0.839595	val: 1.353165	test: 1.335925

Epoch: 21
Loss: 1.5476422309875488
RMSE train: 1.055179	val: 1.751794	test: 1.683615
MAE train: 0.803708	val: 1.372235	test: 1.356425

Epoch: 22
Loss: 1.3048902302980423
RMSE train: 0.953192	val: 1.711407	test: 1.638071Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/esol/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/esol/noise=0.05/esol_scaff_5_26-05_11-28-50  ]
[ Using Seed :  5  ]
[ Using device :  cuda:1  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 12.359910011291504
RMSE train: 3.303987	val: 4.248282	test: 4.346525
MAE train: 2.722857	val: 3.849359	test: 3.857349

Epoch: 2
Loss: 10.97838020324707
RMSE train: 3.130610	val: 4.144967	test: 4.228595
MAE train: 2.548885	val: 3.753681	test: 3.767444

Epoch: 3
Loss: 10.357000827789307
RMSE train: 3.049221	val: 4.094652	test: 4.174540
MAE train: 2.487717	val: 3.712277	test: 3.745666

Epoch: 4
Loss: 9.298229217529297
RMSE train: 2.967993	val: 4.057738	test: 4.142094
MAE train: 2.429642	val: 3.672469	test: 3.756262

Epoch: 5
Loss: 8.34849464893341
RMSE train: 2.897739	val: 4.004561	test: 4.107354
MAE train: 2.376664	val: 3.662364	test: 3.775444

Epoch: 6
Loss: 7.6858919858932495
RMSE train: 2.626410	val: 3.660525	test: 3.746420
MAE train: 2.116859	val: 3.325907	test: 3.395762

Epoch: 7
Loss: 6.959133267402649
RMSE train: 2.414931	val: 3.326732	test: 3.404758
MAE train: 1.915396	val: 2.983768	test: 3.033767

Epoch: 8
Loss: 6.332446575164795
RMSE train: 2.354563	val: 3.162493	test: 3.264759
MAE train: 1.868200	val: 2.821702	test: 2.887001

Epoch: 9
Loss: 5.7731053829193115
RMSE train: 2.306676	val: 3.078853	test: 3.208078
MAE train: 1.827382	val: 2.750287	test: 2.837745

Epoch: 10
Loss: 5.309067487716675
RMSE train: 2.203976	val: 2.965589	test: 3.141477
MAE train: 1.735521	val: 2.649888	test: 2.780959

Epoch: 11
Loss: 4.917519807815552
RMSE train: 2.093553	val: 2.821409	test: 3.056188
MAE train: 1.642697	val: 2.507803	test: 2.696232

Epoch: 12
Loss: 4.341407775878906
RMSE train: 1.942410	val: 2.604723	test: 2.852307
MAE train: 1.514968	val: 2.275371	test: 2.464135

Epoch: 13
Loss: 3.6756948828697205
RMSE train: 1.762349	val: 2.365131	test: 2.587259
MAE train: 1.349294	val: 2.051946	test: 2.190887

Epoch: 14
Loss: 3.389257550239563
RMSE train: 1.588720	val: 2.252949	test: 2.459498
MAE train: 1.193648	val: 1.928203	test: 2.058608

Epoch: 15
Loss: 2.924619436264038
RMSE train: 1.405234	val: 2.010950	test: 2.179983
MAE train: 1.057785	val: 1.675875	test: 1.777479

Epoch: 16
Loss: 2.5736064314842224
RMSE train: 1.241067	val: 1.710744	test: 1.846432
MAE train: 0.926085	val: 1.404390	test: 1.439068

Epoch: 17
Loss: 2.299055576324463
RMSE train: 1.154702	val: 1.525122	test: 1.635045
MAE train: 0.867257	val: 1.226981	test: 1.265014

Epoch: 18
Loss: 2.0432236790657043
RMSE train: 1.075202	val: 1.517208	test: 1.599947
MAE train: 0.806494	val: 1.233044	test: 1.223948

Epoch: 19
Loss: 1.8115205764770508
RMSE train: 1.043317	val: 1.448013	test: 1.552199
MAE train: 0.781123	val: 1.165561	test: 1.200822

Epoch: 20
Loss: 1.6067750751972198
RMSE train: 0.961038	val: 1.385419	test: 1.513518
MAE train: 0.726272	val: 1.095728	test: 1.204899

Epoch: 21
Loss: 1.4765090644359589
RMSE train: 0.902508	val: 1.364157	test: 1.476697
MAE train: 0.697351	val: 1.071771	test: 1.184434

Epoch: 22
Loss: 1.3171000182628632
RMSE train: 0.868575	val: 1.342905	test: 1.463324Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/esol/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/static-noise/GraphCL/esol/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/esol/noise=0.1/esol_scaff_4_26-05_11-28-50  ]
[ Using Seed :  4  ]
[ Using device :  cuda:2  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 12.054703950881958
RMSE train: 3.469102	val: 4.239783	test: 4.340688
MAE train: 2.900391	val: 3.789959	test: 3.801432

Epoch: 2
Loss: 10.970850706100464
RMSE train: 3.335131	val: 4.287173	test: 4.319921
MAE train: 2.777110	val: 3.895404	test: 3.817934

Epoch: 3
Loss: 10.301815509796143
RMSE train: 3.275112	val: 4.362770	test: 4.345444
MAE train: 2.737833	val: 4.015827	test: 3.896876

Epoch: 4
Loss: 9.233338117599487
RMSE train: 3.265549	val: 4.518585	test: 4.443714
MAE train: 2.743932	val: 4.205173	test: 4.062165

Epoch: 5
Loss: 8.304108738899231
RMSE train: 3.225823	val: 4.644075	test: 4.552748
MAE train: 2.706676	val: 4.340073	test: 4.237412

Epoch: 6
Loss: 7.594396352767944
RMSE train: 3.079540	val: 4.405397	test: 4.412418
MAE train: 2.584022	val: 4.121795	test: 4.147824

Epoch: 7
Loss: 7.03664231300354
RMSE train: 2.791216	val: 3.816023	test: 3.920962
MAE train: 2.310457	val: 3.510842	test: 3.612855

Epoch: 8
Loss: 6.399065017700195
RMSE train: 2.441254	val: 3.211164	test: 3.346333
MAE train: 1.963397	val: 2.890842	test: 3.003879

Epoch: 9
Loss: 5.562453508377075
RMSE train: 2.188393	val: 2.851873	test: 2.959864
MAE train: 1.725781	val: 2.542464	test: 2.596938

Epoch: 10
Loss: 5.135242581367493
RMSE train: 2.063771	val: 2.706312	test: 2.806167
MAE train: 1.619069	val: 2.408224	test: 2.434529

Epoch: 11
Loss: 4.726752996444702
RMSE train: 1.938112	val: 2.684371	test: 2.782463
MAE train: 1.517552	val: 2.385156	test: 2.419898

Epoch: 12
Loss: 4.138107657432556
RMSE train: 1.796767	val: 2.567376	test: 2.670648
MAE train: 1.407758	val: 2.264633	test: 2.307960

Epoch: 13
Loss: 3.7603574991226196
RMSE train: 1.631144	val: 2.307970	test: 2.417854
MAE train: 1.256849	val: 1.989397	test: 2.043566

Epoch: 14
Loss: 3.372282385826111
RMSE train: 1.493145	val: 2.024204	test: 2.178623
MAE train: 1.135088	val: 1.683946	test: 1.782119

Epoch: 15
Loss: 3.0065183639526367
RMSE train: 1.405238	val: 1.818737	test: 2.047263
MAE train: 1.068658	val: 1.471796	test: 1.662052

Epoch: 16
Loss: 2.6482380032539368
RMSE train: 1.287221	val: 1.688238	test: 1.927497
MAE train: 0.991203	val: 1.332872	test: 1.553814

Epoch: 17
Loss: 2.4135347604751587
RMSE train: 1.193054	val: 1.572463	test: 1.812036
MAE train: 0.925520	val: 1.243950	test: 1.470039

Epoch: 18
Loss: 2.2148095965385437
RMSE train: 1.124389	val: 1.431081	test: 1.678891
MAE train: 0.880583	val: 1.110221	test: 1.365330

Epoch: 19
Loss: 1.8733812868595123
RMSE train: 1.078203	val: 1.468146	test: 1.688031
MAE train: 0.846490	val: 1.134055	test: 1.390735

Epoch: 20
Loss: 1.6516770422458649
RMSE train: 1.042720	val: 1.511635	test: 1.723547
MAE train: 0.820356	val: 1.176764	test: 1.409512

Epoch: 21
Loss: 1.6066049933433533
RMSE train: 0.996932	val: 1.477040	test: 1.661816
MAE train: 0.787198	val: 1.148225	test: 1.349492

Epoch: 22
Loss: 1.4324801862239838
RMSE train: 0.979977	val: 1.392296	test: 1.570602
RMSE train: 0.767196	val: 1.333340	test: 1.446964
MAE train: 0.582875	val: 1.077995	test: 1.128541

Epoch: 24
Loss: 0.8614475131034851
RMSE train: 0.643862	val: 1.230736	test: 1.310573
MAE train: 0.493224	val: 0.959622	test: 1.048077

Epoch: 25
Loss: 0.8239956051111221
RMSE train: 0.641190	val: 1.228826	test: 1.317193
MAE train: 0.492432	val: 0.970866	test: 1.035861

Epoch: 26
Loss: 0.8267416208982468
RMSE train: 0.662693	val: 1.184759	test: 1.335559
MAE train: 0.503558	val: 0.948541	test: 1.048252

Epoch: 27
Loss: 0.7767573595046997
RMSE train: 0.633079	val: 1.219291	test: 1.265035
MAE train: 0.479614	val: 0.970964	test: 1.006207

Epoch: 28
Loss: 0.7543692141771317
RMSE train: 0.620982	val: 1.087925	test: 1.245735
MAE train: 0.465778	val: 0.877624	test: 0.985514

Epoch: 29
Loss: 0.7264504432678223
RMSE train: 0.646098	val: 1.053493	test: 1.284413
MAE train: 0.496386	val: 0.826345	test: 1.027643

Epoch: 30
Loss: 0.7135741114616394
RMSE train: 0.667593	val: 1.257389	test: 1.321043
MAE train: 0.517646	val: 1.002317	test: 1.002089

Epoch: 31
Loss: 0.6892226487398148
RMSE train: 0.713932	val: 1.448995	test: 1.383264
MAE train: 0.564839	val: 1.145741	test: 1.096146

Epoch: 32
Loss: 0.7088122069835663
RMSE train: 0.626372	val: 1.176150	test: 1.258740
MAE train: 0.494521	val: 0.935270	test: 1.001903

Epoch: 33
Loss: 0.6670144647359848
RMSE train: 0.610601	val: 1.161418	test: 1.247862
MAE train: 0.477737	val: 0.916947	test: 0.981221

Epoch: 34
Loss: 0.6853226125240326
RMSE train: 0.606320	val: 1.174015	test: 1.249453
MAE train: 0.468895	val: 0.935793	test: 0.974854

Epoch: 35
Loss: 0.6629369109869003
RMSE train: 0.619480	val: 1.153528	test: 1.270036
MAE train: 0.487280	val: 0.883253	test: 1.002533

Epoch: 36
Loss: 0.6656860560178757
RMSE train: 0.595025	val: 1.141986	test: 1.198540
MAE train: 0.471264	val: 0.905431	test: 0.937843

Epoch: 37
Loss: 0.6356311589479446
RMSE train: 0.622066	val: 1.076944	test: 1.178285
MAE train: 0.493830	val: 0.850076	test: 0.932943

Epoch: 38
Loss: 0.6381528228521347
RMSE train: 0.613083	val: 1.138482	test: 1.225046
MAE train: 0.474256	val: 0.910624	test: 0.984107

Epoch: 39
Loss: 0.626420870423317
RMSE train: 0.619386	val: 1.310505	test: 1.300413
MAE train: 0.477517	val: 1.007721	test: 1.031775

Epoch: 40
Loss: 0.5982196629047394
RMSE train: 0.574166	val: 1.158302	test: 1.231276
MAE train: 0.427461	val: 0.931379	test: 0.952110

Epoch: 41
Loss: 0.5521633476018906
RMSE train: 0.564999	val: 1.195571	test: 1.223381
MAE train: 0.424416	val: 0.951142	test: 0.938876

Epoch: 42
Loss: 0.6146834939718246
RMSE train: 0.592390	val: 1.176487	test: 1.244092
MAE train: 0.445439	val: 0.947191	test: 0.957085

Epoch: 43
Loss: 0.5744010284543037
RMSE train: 0.588934	val: 1.089475	test: 1.220197
MAE train: 0.445559	val: 0.871545	test: 0.952185

Epoch: 44
Loss: 0.5716526508331299
RMSE train: 0.565778	val: 1.224876	test: 1.212959
MAE train: 0.429945	val: 0.957065	test: 0.947428

Epoch: 45
Loss: 0.5439624860882759
RMSE train: 0.636636	val: 1.340620	test: 1.283572
MAE train: 0.491168	val: 1.043415	test: 1.009318

Epoch: 46
Loss: 0.5607215911149979
RMSE train: 0.522505	val: 1.247135	test: 1.213546
MAE train: 0.397690	val: 0.990490	test: 0.974949

Epoch: 47
Loss: 0.5411104187369347
RMSE train: 0.537188	val: 1.168148	test: 1.185519
MAE train: 0.404228	val: 0.947026	test: 0.927451

Epoch: 48
Loss: 0.537021093070507
RMSE train: 0.560176	val: 1.149032	test: 1.173760
MAE train: 0.422170	val: 0.930294	test: 0.906814

Epoch: 49
Loss: 0.5569813549518585
RMSE train: 0.524068	val: 1.071616	test: 1.137916
MAE train: 0.398283	val: 0.860647	test: 0.889976

Epoch: 50
Loss: 0.47698575258255005
RMSE train: 0.530758	val: 1.112587	test: 1.163352
MAE train: 0.401229	val: 0.903247	test: 0.910848

Epoch: 51
Loss: 0.49931202828884125
RMSE train: 0.624994	val: 1.222796	test: 1.284862
MAE train: 0.474199	val: 0.989443	test: 0.969262

Epoch: 52
Loss: 0.5895180255174637
RMSE train: 0.542123	val: 1.173414	test: 1.212624
MAE train: 0.406143	val: 0.923971	test: 0.944797

Epoch: 53
Loss: 0.5341535806655884
RMSE train: 0.565238	val: 1.270297	test: 1.265979
MAE train: 0.427369	val: 0.984979	test: 1.002419

Epoch: 54
Loss: 0.5349365249276161
RMSE train: 0.614913	val: 1.278765	test: 1.296145
MAE train: 0.467467	val: 1.001634	test: 1.013432

Epoch: 55
Loss: 0.5310648456215858
RMSE train: 0.535305	val: 1.125086	test: 1.198235
MAE train: 0.407845	val: 0.916251	test: 0.948997

Epoch: 56
Loss: 0.4923819229006767
RMSE train: 0.525396	val: 1.239875	test: 1.229883
MAE train: 0.401411	val: 0.989735	test: 0.974337

Epoch: 57
Loss: 0.4530428722500801
RMSE train: 0.563549	val: 1.231607	test: 1.237777
MAE train: 0.422376	val: 0.982711	test: 0.961146

Epoch: 58
Loss: 0.4665493965148926
RMSE train: 0.588126	val: 1.222880	test: 1.254558
MAE train: 0.433116	val: 0.990294	test: 0.954487

Epoch: 59
Loss: 0.4642699211835861
RMSE train: 0.601111	val: 1.209933	test: 1.228010
MAE train: 0.445395	val: 0.979586	test: 0.933382

Epoch: 60
Loss: 0.4771908074617386
RMSE train: 0.637389	val: 1.375593	test: 1.327311
MAE train: 0.472085	val: 1.069946	test: 1.025020

Epoch: 61
Loss: 0.4473070874810219
RMSE train: 0.568838	val: 1.171234	test: 1.201620
MAE train: 0.417927	val: 0.954702	test: 0.913192

Epoch: 62
Loss: 0.4486667588353157
RMSE train: 0.537697	val: 1.200385	test: 1.169406
MAE train: 0.403068	val: 0.935144	test: 0.887554

Epoch: 63
Loss: 0.4914369583129883
RMSE train: 0.505518	val: 1.211647	test: 1.145017
MAE train: 0.380362	val: 0.946069	test: 0.879046

Epoch: 64
Loss: 0.4330579414963722
RMSE train: 0.543983	val: 1.130687	test: 1.140155
MAE train: 0.415978	val: 0.905356	test: 0.895352

Epoch: 65
Loss: 0.44438615441322327
RMSE train: 0.584286	val: 1.374712	test: 1.286289
MAE train: 0.457616	val: 1.071525	test: 1.003227

Epoch: 66
Loss: 0.3962784856557846
RMSE train: 0.497535	val: 1.154306	test: 1.166272
MAE train: 0.377806	val: 0.903237	test: 0.943305

Epoch: 67
Loss: 0.562496542930603
RMSE train: 0.526190	val: 1.136881	test: 1.168902
MAE train: 0.389870	val: 0.924283	test: 0.914964

Epoch: 68
Loss: 0.42407549917697906
RMSE train: 0.633287	val: 1.408091	test: 1.363097
MAE train: 0.474531	val: 1.079655	test: 1.033436

Epoch: 69
Loss: 0.4234793707728386
RMSE train: 0.534020	val: 1.137872	test: 1.166302
MAE train: 0.399456	val: 0.945438	test: 0.914878

Epoch: 70
Loss: 0.4218266159296036
RMSE train: 0.533434	val: 1.185013	test: 1.191666
MAE train: 0.398438	val: 0.967462	test: 0.922075

Epoch: 71
Loss: 0.4327414333820343
RMSE train: 0.555035	val: 1.260751	test: 1.234957
MAE train: 0.420051	val: 0.999043	test: 0.954523

Epoch: 72
Loss: 0.4462580308318138
RMSE train: 0.529718	val: 1.097803	test: 1.161494
MAE train: 0.392159	val: 0.894738	test: 0.909783

Epoch: 73
Loss: 0.42061683535575867
RMSE train: 0.514971	val: 1.109145	test: 1.166565
MAE train: 0.384403	val: 0.916418	test: 0.910233

Epoch: 74
Loss: 0.39214058965444565
RMSE train: 0.501226	val: 1.178996	test: 1.209101
MAE train: 0.384385	val: 0.956082	test: 0.950784

Epoch: 75
Loss: 0.4126804992556572
RMSE train: 0.504965	val: 1.161150	test: 1.218728
MAE train: 0.395419	val: 0.933483	test: 0.966674

Epoch: 76
Loss: 0.3737085461616516
RMSE train: 0.515771	val: 1.134307	test: 1.167846
MAE train: 0.397830	val: 0.919926	test: 0.929493

Epoch: 77
Loss: 0.4250084236264229
RMSE train: 0.517862	val: 1.193130	test: 1.166242
MAE train: 0.395512	val: 0.962335	test: 0.906344

Epoch: 78
Loss: 0.381137453019619
RMSE train: 0.514504	val: 1.165975	test: 1.148466
MAE train: 0.385878	val: 0.935508	test: 0.897623

Epoch: 79
Loss: 0.3653132691979408
RMSE train: 0.504946	val: 1.167929	test: 1.149086
MAE train: 0.387992	val: 0.921226	test: 0.909971

Epoch: 80
Loss: 0.42516186833381653
RMSE train: 0.480215	val: 1.184552	test: 1.157901
MAE train: 0.367198	val: 0.950100	test: 0.909492

Epoch: 81
Loss: 0.38695313036441803
RMSE train: 0.523063	val: 1.237373	test: 1.193229
MAE train: 0.398967	val: 0.987088	test: 0.930988

Epoch: 82
Loss: 0.37138593196868896
RMSE train: 0.514804	val: 1.217734	test: 1.189718
MAE train: 0.387788	val: 0.961466	test: 0.939545

Epoch: 83
Loss: 0.42743298411369324
RMSE train: 0.516639	val: 1.253486	test: 1.220925
MAE train: 0.385172	val: 0.992102	test: 0.952025
RMSE train: 0.714227	val: 1.190325	test: 1.321880
MAE train: 0.543644	val: 0.953408	test: 1.051749

Epoch: 24
Loss: 0.9077479988336563
RMSE train: 0.691767	val: 1.158345	test: 1.313892
MAE train: 0.517796	val: 0.919288	test: 1.063160

Epoch: 25
Loss: 0.8349318355321884
RMSE train: 0.716583	val: 1.191191	test: 1.333757
MAE train: 0.530536	val: 0.972507	test: 1.020941

Epoch: 26
Loss: 0.7639099359512329
RMSE train: 0.659907	val: 1.149864	test: 1.274739
MAE train: 0.502000	val: 0.929025	test: 0.975718

Epoch: 27
Loss: 0.8017374873161316
RMSE train: 0.665357	val: 1.093173	test: 1.233647
MAE train: 0.521905	val: 0.887002	test: 0.962500

Epoch: 28
Loss: 0.8022713512182236
RMSE train: 0.674315	val: 1.138527	test: 1.246012
MAE train: 0.514383	val: 0.930706	test: 0.965105

Epoch: 29
Loss: 0.7133951932191849
RMSE train: 0.686790	val: 1.288411	test: 1.318049
MAE train: 0.515172	val: 1.040648	test: 1.034305

Epoch: 30
Loss: 0.7411277294158936
RMSE train: 0.610157	val: 1.083868	test: 1.159772
MAE train: 0.461449	val: 0.902289	test: 0.902644

Epoch: 31
Loss: 0.689606711268425
RMSE train: 0.618365	val: 1.069667	test: 1.147941
MAE train: 0.471689	val: 0.895701	test: 0.883676

Epoch: 32
Loss: 0.6559922099113464
RMSE train: 0.684349	val: 1.222808	test: 1.240521
MAE train: 0.521295	val: 0.980998	test: 0.959392

Epoch: 33
Loss: 0.6681874841451645
RMSE train: 0.667085	val: 1.081374	test: 1.195774
MAE train: 0.506685	val: 0.895518	test: 0.922753

Epoch: 34
Loss: 0.6831779032945633
RMSE train: 0.627256	val: 1.100570	test: 1.185254
MAE train: 0.474173	val: 0.891840	test: 0.908962

Epoch: 35
Loss: 0.7025410234928131
RMSE train: 0.666166	val: 1.278467	test: 1.260542
MAE train: 0.498160	val: 1.002568	test: 0.982189

Epoch: 36
Loss: 0.5883803516626358
RMSE train: 0.666124	val: 1.132280	test: 1.189449
MAE train: 0.499476	val: 0.908982	test: 0.909203

Epoch: 37
Loss: 0.5941230207681656
RMSE train: 0.696132	val: 1.169918	test: 1.223806
MAE train: 0.518790	val: 0.953472	test: 0.905326

Epoch: 38
Loss: 0.604867696762085
RMSE train: 0.717603	val: 1.190398	test: 1.216565
MAE train: 0.528112	val: 0.955695	test: 0.919577

Epoch: 39
Loss: 0.5800051391124725
RMSE train: 0.627281	val: 1.096088	test: 1.153129
MAE train: 0.473208	val: 0.899163	test: 0.883156

Epoch: 40
Loss: 0.5738821178674698
RMSE train: 0.602441	val: 1.092048	test: 1.147574
MAE train: 0.459860	val: 0.897478	test: 0.886543

Epoch: 41
Loss: 0.6137583926320076
RMSE train: 0.612257	val: 1.098235	test: 1.150342
MAE train: 0.461050	val: 0.898676	test: 0.894898

Epoch: 42
Loss: 0.5738403499126434
RMSE train: 0.594804	val: 1.082909	test: 1.130682
MAE train: 0.454846	val: 0.897875	test: 0.859891

Epoch: 43
Loss: 0.5637504011392593
RMSE train: 0.587105	val: 1.166683	test: 1.186288
MAE train: 0.445177	val: 0.960730	test: 0.929608

Epoch: 44
Loss: 0.5523285567760468
RMSE train: 0.588881	val: 1.082813	test: 1.166484
MAE train: 0.445896	val: 0.900087	test: 0.888658

Epoch: 45
Loss: 0.6213972419500351
RMSE train: 0.600671	val: 0.946629	test: 1.111597
MAE train: 0.468619	val: 0.799515	test: 0.884176

Epoch: 46
Loss: 0.5449347645044327
RMSE train: 0.583884	val: 1.268904	test: 1.208742
MAE train: 0.443212	val: 1.017246	test: 0.969427

Epoch: 47
Loss: 0.5466794818639755
RMSE train: 0.596295	val: 1.209830	test: 1.172672
MAE train: 0.441754	val: 0.983084	test: 0.871244

Epoch: 48
Loss: 0.5561518445611
RMSE train: 0.602137	val: 1.225262	test: 1.155540
MAE train: 0.448136	val: 0.998480	test: 0.855489

Epoch: 49
Loss: 0.5518398880958557
RMSE train: 0.544433	val: 1.053860	test: 1.069170
MAE train: 0.418484	val: 0.865386	test: 0.822968

Epoch: 50
Loss: 0.525219015777111
RMSE train: 0.627654	val: 1.240082	test: 1.227411
MAE train: 0.468002	val: 0.974456	test: 0.920736

Epoch: 51
Loss: 0.511392317712307
RMSE train: 0.648586	val: 1.205183	test: 1.250418
MAE train: 0.485407	val: 0.980339	test: 0.940563

Epoch: 52
Loss: 0.4745913967490196
RMSE train: 0.630252	val: 1.077559	test: 1.169790
MAE train: 0.479931	val: 0.896529	test: 0.912688

Epoch: 53
Loss: 0.47185493260622025
RMSE train: 0.585216	val: 1.181516	test: 1.184678
MAE train: 0.436098	val: 0.948439	test: 0.918946

Epoch: 54
Loss: 0.5167293921113014
RMSE train: 0.611728	val: 1.245469	test: 1.226451
MAE train: 0.447504	val: 0.984619	test: 0.925010

Epoch: 55
Loss: 0.46152420341968536
RMSE train: 0.554781	val: 1.089298	test: 1.102952
MAE train: 0.415468	val: 0.898884	test: 0.846621

Epoch: 56
Loss: 0.4718823730945587
RMSE train: 0.550800	val: 1.071842	test: 1.108192
MAE train: 0.411189	val: 0.879758	test: 0.871726

Epoch: 57
Loss: 0.503517746925354
RMSE train: 0.566201	val: 1.182715	test: 1.178103
MAE train: 0.417415	val: 0.955688	test: 0.913512

Epoch: 58
Loss: 0.5203016027808189
RMSE train: 0.565562	val: 1.081693	test: 1.131087
MAE train: 0.428023	val: 0.901684	test: 0.887447

Epoch: 59
Loss: 0.4880944564938545
RMSE train: 0.560527	val: 1.176299	test: 1.150825
MAE train: 0.433920	val: 0.963377	test: 0.916882

Epoch: 60
Loss: 0.4729785695672035
RMSE train: 0.567527	val: 1.011260	test: 1.070594
MAE train: 0.428034	val: 0.855435	test: 0.840728

Epoch: 61
Loss: 0.48104898631572723
RMSE train: 0.585064	val: 1.173527	test: 1.135030
MAE train: 0.437544	val: 0.955287	test: 0.871447

Epoch: 62
Loss: 0.47608400881290436
RMSE train: 0.528941	val: 1.088420	test: 1.085916
MAE train: 0.402799	val: 0.896438	test: 0.866563

Epoch: 63
Loss: 0.5015768706798553
RMSE train: 0.583423	val: 1.191875	test: 1.205297
MAE train: 0.428205	val: 0.964716	test: 0.944807

Epoch: 64
Loss: 0.4511822387576103
RMSE train: 0.588897	val: 1.148381	test: 1.191166
MAE train: 0.436376	val: 0.959971	test: 0.916695

Epoch: 65
Loss: 0.4440400078892708
RMSE train: 0.563208	val: 1.122859	test: 1.169259
MAE train: 0.418754	val: 0.925924	test: 0.914701

Epoch: 66
Loss: 0.43142344057559967
RMSE train: 0.569322	val: 1.115970	test: 1.148107
MAE train: 0.420543	val: 0.922793	test: 0.880922

Epoch: 67
Loss: 0.3953155130147934
RMSE train: 0.562548	val: 1.089684	test: 1.144156
MAE train: 0.414378	val: 0.912385	test: 0.869075

Epoch: 68
Loss: 0.40685758739709854
RMSE train: 0.566927	val: 1.139648	test: 1.174567
MAE train: 0.425921	val: 0.929244	test: 0.908891

Epoch: 69
Loss: 0.4148327633738518
RMSE train: 0.527744	val: 1.101336	test: 1.169846
MAE train: 0.400855	val: 0.914508	test: 0.928145

Epoch: 70
Loss: 0.4616842344403267
RMSE train: 0.548855	val: 1.122870	test: 1.168585
MAE train: 0.421007	val: 0.943402	test: 0.916984

Epoch: 71
Loss: 0.4508593901991844
RMSE train: 0.583994	val: 1.259805	test: 1.248656
MAE train: 0.443456	val: 1.016477	test: 0.964113

Epoch: 72
Loss: 0.4099217802286148
RMSE train: 0.561790	val: 1.039724	test: 1.139416
MAE train: 0.429889	val: 0.878521	test: 0.901494

Epoch: 73
Loss: 0.367474302649498
RMSE train: 0.522786	val: 1.103260	test: 1.132457
MAE train: 0.396173	val: 0.924985	test: 0.882077

Epoch: 74
Loss: 0.39863815903663635
RMSE train: 0.504907	val: 1.113685	test: 1.129508
MAE train: 0.386083	val: 0.921457	test: 0.894532

Epoch: 75
Loss: 0.4034051224589348
RMSE train: 0.535688	val: 1.031715	test: 1.104902
MAE train: 0.408020	val: 0.875122	test: 0.880152

Epoch: 76
Loss: 0.4147487208247185
RMSE train: 0.616292	val: 1.192424	test: 1.205043
MAE train: 0.466022	val: 0.986870	test: 0.914872

Epoch: 77
Loss: 0.41874658316373825
RMSE train: 0.523817	val: 1.067908	test: 1.130742
MAE train: 0.393725	val: 0.890234	test: 0.889074

Epoch: 78
Loss: 0.49021797627210617
RMSE train: 0.517694	val: 1.044890	test: 1.116121
MAE train: 0.395005	val: 0.875596	test: 0.902585

Epoch: 79
Loss: 0.42691001296043396
RMSE train: 0.542820	val: 1.289600	test: 1.265176
MAE train: 0.422519	val: 1.039117	test: 1.001722

Epoch: 80
Loss: 0.38240718841552734
RMSE train: 0.533516	val: 1.087402	test: 1.114333
MAE train: 0.417381	val: 0.912134	test: 0.896496

Epoch: 81
Loss: 0.4205690547823906
RMSE train: 0.520007	val: 1.168695	test: 1.135066
MAE train: 0.400224	val: 0.963113	test: 0.888774

Epoch: 82
Loss: 0.3914360925555229
RMSE train: 0.529236	val: 1.195865	test: 1.162195
MAE train: 0.397548	val: 0.981315	test: 0.904448

Epoch: 83
Loss: 0.3880593553185463
RMSE train: 0.546567	val: 1.028447	test: 1.104261
MAE train: 0.411973	val: 0.880627	test: 0.876266
RMSE train: 0.868413	val: 1.515614	test: 1.593799
MAE train: 0.654034	val: 1.194358	test: 1.262931

Epoch: 24
Loss: 0.8884284943342209
RMSE train: 0.808747	val: 1.338769	test: 1.463846
MAE train: 0.598293	val: 1.092910	test: 1.165680

Epoch: 25
Loss: 0.9388928413391113
RMSE train: 0.775487	val: 1.258996	test: 1.382371
MAE train: 0.576609	val: 1.021513	test: 1.091347

Epoch: 26
Loss: 0.885062575340271
RMSE train: 0.730576	val: 1.322861	test: 1.367457
MAE train: 0.558190	val: 1.037728	test: 1.072657

Epoch: 27
Loss: 0.7643976658582687
RMSE train: 0.644866	val: 1.203617	test: 1.251991
MAE train: 0.490403	val: 0.953387	test: 0.969052

Epoch: 28
Loss: 0.7025818079710007
RMSE train: 0.677612	val: 1.276769	test: 1.271329
MAE train: 0.522034	val: 1.015346	test: 0.982024

Epoch: 29
Loss: 0.7171950936317444
RMSE train: 0.671404	val: 1.240559	test: 1.243665
MAE train: 0.511105	val: 0.978460	test: 0.966283

Epoch: 30
Loss: 0.8201724290847778
RMSE train: 0.626116	val: 1.179784	test: 1.213915
MAE train: 0.478184	val: 0.933879	test: 0.964872

Epoch: 31
Loss: 0.7690776586532593
RMSE train: 0.771855	val: 1.332441	test: 1.356541
MAE train: 0.590061	val: 1.059285	test: 1.067475

Epoch: 32
Loss: 0.7639182955026627
RMSE train: 0.666113	val: 1.172902	test: 1.245392
MAE train: 0.523009	val: 0.958882	test: 0.964716

Epoch: 33
Loss: 0.6865196526050568
RMSE train: 0.666101	val: 1.267935	test: 1.271244
MAE train: 0.520219	val: 1.013604	test: 1.012664

Epoch: 34
Loss: 0.6505762934684753
RMSE train: 0.667342	val: 1.284018	test: 1.300573
MAE train: 0.513018	val: 1.033939	test: 1.025253

Epoch: 35
Loss: 0.6450052857398987
RMSE train: 0.670369	val: 1.255477	test: 1.307718
MAE train: 0.512575	val: 1.044203	test: 1.025026

Epoch: 36
Loss: 0.6781442612409592
RMSE train: 0.654173	val: 1.271346	test: 1.333141
MAE train: 0.506149	val: 1.048817	test: 1.053446

Epoch: 37
Loss: 0.6360247135162354
RMSE train: 0.598456	val: 1.268911	test: 1.356957
MAE train: 0.464809	val: 1.020106	test: 1.079475

Epoch: 38
Loss: 0.6400850713253021
RMSE train: 0.652922	val: 1.289229	test: 1.291042
MAE train: 0.502133	val: 1.037342	test: 0.999476

Epoch: 39
Loss: 0.6010531038045883
RMSE train: 0.651945	val: 1.219945	test: 1.231993
MAE train: 0.493428	val: 0.982421	test: 0.938536

Epoch: 40
Loss: 0.6580698043107986
RMSE train: 0.627812	val: 1.161489	test: 1.205077
MAE train: 0.481408	val: 0.926628	test: 0.940779

Epoch: 41
Loss: 0.6329522728919983
RMSE train: 0.647225	val: 1.359039	test: 1.354135
MAE train: 0.491571	val: 1.081192	test: 1.049369

Epoch: 42
Loss: 0.5831503868103027
RMSE train: 0.643379	val: 1.321241	test: 1.328522
MAE train: 0.487054	val: 1.080390	test: 1.031620

Epoch: 43
Loss: 0.5969651266932487
RMSE train: 0.644289	val: 1.270918	test: 1.314614
MAE train: 0.487087	val: 1.027692	test: 1.029225

Epoch: 44
Loss: 0.5699456334114075
RMSE train: 0.651438	val: 1.324893	test: 1.349114
MAE train: 0.486791	val: 1.049512	test: 1.060899

Epoch: 45
Loss: 0.6354620605707169
RMSE train: 0.673507	val: 1.330154	test: 1.330275
MAE train: 0.511505	val: 1.072399	test: 1.020948

Epoch: 46
Loss: 0.6109948605298996
RMSE train: 0.623018	val: 1.166108	test: 1.201032
MAE train: 0.474947	val: 0.959530	test: 0.928226

Epoch: 47
Loss: 0.5334711074829102
RMSE train: 0.660985	val: 1.296522	test: 1.266349
MAE train: 0.503795	val: 1.057111	test: 0.973235

Epoch: 48
Loss: 0.6190198063850403
RMSE train: 0.699193	val: 1.374867	test: 1.313933
MAE train: 0.537245	val: 1.119087	test: 0.987377

Epoch: 49
Loss: 0.5981759577989578
RMSE train: 0.558449	val: 1.133449	test: 1.135040
MAE train: 0.414677	val: 0.926503	test: 0.879049

Epoch: 50
Loss: 0.5180762484669685
RMSE train: 0.532993	val: 1.110513	test: 1.136041
MAE train: 0.398145	val: 0.892134	test: 0.905999

Epoch: 51
Loss: 0.5036018192768097
RMSE train: 0.638720	val: 1.278491	test: 1.282569
MAE train: 0.482295	val: 1.035648	test: 1.006735

Epoch: 52
Loss: 0.5247832983732224
RMSE train: 0.613922	val: 1.272943	test: 1.348245
MAE train: 0.459573	val: 1.037523	test: 1.047747

Epoch: 53
Loss: 0.5030111819505692
RMSE train: 0.627375	val: 1.236994	test: 1.287588
MAE train: 0.470693	val: 1.031370	test: 1.006826

Epoch: 54
Loss: 0.5684517025947571
RMSE train: 0.595634	val: 1.161451	test: 1.206632
MAE train: 0.447705	val: 0.961431	test: 0.942187

Epoch: 55
Loss: 0.5149331390857697
RMSE train: 0.604772	val: 1.237315	test: 1.216369
MAE train: 0.457681	val: 1.001378	test: 0.954634

Epoch: 56
Loss: 0.4784412682056427
RMSE train: 0.602274	val: 1.193832	test: 1.234701
MAE train: 0.450905	val: 0.991526	test: 0.948590

Epoch: 57
Loss: 0.45340920984745026
RMSE train: 0.584281	val: 1.229786	test: 1.237530
MAE train: 0.443354	val: 1.004674	test: 0.958072

Epoch: 58
Loss: 0.4924791529774666
RMSE train: 0.669780	val: 1.404390	test: 1.376839
MAE train: 0.507748	val: 1.139984	test: 1.074994

Epoch: 59
Loss: 0.5019395053386688
RMSE train: 0.661604	val: 1.316168	test: 1.327596
MAE train: 0.498426	val: 1.092440	test: 1.021999

Epoch: 60
Loss: 0.5073952749371529
RMSE train: 0.642031	val: 1.257929	test: 1.296292
MAE train: 0.486544	val: 1.036143	test: 0.993756

Epoch: 61
Loss: 0.45601996779441833
RMSE train: 0.625144	val: 1.217709	test: 1.252497
MAE train: 0.473462	val: 0.995657	test: 0.958675

Epoch: 62
Loss: 0.4650482162833214
RMSE train: 0.585917	val: 1.209334	test: 1.241733
MAE train: 0.441515	val: 0.977315	test: 0.961780

Epoch: 63
Loss: 0.4645610675215721
RMSE train: 0.650188	val: 1.266990	test: 1.267772
MAE train: 0.482500	val: 1.024760	test: 0.967401

Epoch: 64
Loss: 0.45615530014038086
RMSE train: 0.654986	val: 1.312477	test: 1.250442
MAE train: 0.492180	val: 1.049695	test: 0.949535

Epoch: 65
Loss: 0.45453252643346786
RMSE train: 0.565678	val: 1.260073	test: 1.194542
MAE train: 0.427149	val: 1.021362	test: 0.916764

Epoch: 66
Loss: 0.43541552871465683
RMSE train: 0.618033	val: 1.203971	test: 1.159400
MAE train: 0.471978	val: 0.975138	test: 0.875059

Epoch: 67
Loss: 0.5488724559545517
RMSE train: 0.651555	val: 1.311105	test: 1.215340
MAE train: 0.497075	val: 1.042351	test: 0.926683

Epoch: 68
Loss: 0.4563645273447037
RMSE train: 0.581276	val: 1.222084	test: 1.171539
MAE train: 0.436381	val: 0.982591	test: 0.917517

Epoch: 69
Loss: 0.47223614901304245
RMSE train: 0.583683	val: 1.247438	test: 1.200425
MAE train: 0.436750	val: 1.001631	test: 0.931626

Epoch: 70
Loss: 0.4148617535829544
RMSE train: 0.633933	val: 1.385582	test: 1.259230
MAE train: 0.477004	val: 1.105252	test: 0.994135

Epoch: 71
Loss: 0.42732952535152435
RMSE train: 0.578603	val: 1.160803	test: 1.124790
MAE train: 0.431501	val: 0.958079	test: 0.869884

Epoch: 72
Loss: 0.4039810970425606
RMSE train: 0.601863	val: 1.198849	test: 1.147898
MAE train: 0.450114	val: 0.994099	test: 0.883559

Epoch: 73
Loss: 0.43505050987005234
RMSE train: 0.603770	val: 1.289240	test: 1.212607
MAE train: 0.451581	val: 1.050474	test: 0.929852

Epoch: 74
Loss: 0.455520935356617
RMSE train: 0.578452	val: 1.173938	test: 1.179396
MAE train: 0.429666	val: 0.961104	test: 0.933641

Epoch: 75
Loss: 0.4462796747684479
RMSE train: 0.587615	val: 1.273454	test: 1.252005
MAE train: 0.436313	val: 1.022852	test: 0.966989

Epoch: 76
Loss: 0.41850055754184723
RMSE train: 0.600132	val: 1.233802	test: 1.209268
MAE train: 0.448263	val: 0.996734	test: 0.933821

Epoch: 77
Loss: 0.39814741909503937
RMSE train: 0.562105	val: 1.086882	test: 1.127829
MAE train: 0.416687	val: 0.893628	test: 0.883421

Epoch: 78
Loss: 0.392814464867115
RMSE train: 0.565765	val: 1.201285	test: 1.147570
MAE train: 0.423920	val: 0.964779	test: 0.883207

Epoch: 79
Loss: 0.41946684569120407
RMSE train: 0.583254	val: 1.237704	test: 1.187454
MAE train: 0.441127	val: 1.000182	test: 0.913400

Epoch: 80
Loss: 0.3498740717768669
RMSE train: 0.519119	val: 1.141786	test: 1.143957
MAE train: 0.385686	val: 0.944092	test: 0.907202

Epoch: 81
Loss: 0.3940013647079468
RMSE train: 0.554765	val: 1.231060	test: 1.209245
MAE train: 0.417580	val: 1.014206	test: 0.945627

Epoch: 82
Loss: 0.41754476726055145
RMSE train: 0.595258	val: 1.305929	test: 1.280098
MAE train: 0.450867	val: 1.044504	test: 1.005962

Epoch: 83
Loss: 0.3994349092245102
RMSE train: 0.528157	val: 1.121469	test: 1.166288
MAE train: 0.392344	val: 0.935971	test: 0.932338
MAE train: 0.873263	val: 1.388971	test: 1.369884

Epoch: 23
Loss: 1.7386094331741333
RMSE train: 1.064400	val: 1.931735	test: 1.746538
MAE train: 0.834181	val: 1.400333	test: 1.354947

Epoch: 24
Loss: 1.5970352292060852
RMSE train: 1.032324	val: 2.108612	test: 1.842514
MAE train: 0.810854	val: 1.484243	test: 1.418726

Epoch: 25
Loss: 1.4357884228229523
RMSE train: 0.985610	val: 1.965749	test: 1.790699
MAE train: 0.776740	val: 1.444820	test: 1.395185

Epoch: 26
Loss: 1.4351155161857605
RMSE train: 0.944727	val: 1.953533	test: 1.794371
MAE train: 0.743181	val: 1.473119	test: 1.409072

Epoch: 27
Loss: 1.4459520280361176
RMSE train: 0.973038	val: 1.923289	test: 1.758467
MAE train: 0.772016	val: 1.450922	test: 1.370889

Epoch: 28
Loss: 1.4032423198223114
RMSE train: 1.004943	val: 2.098175	test: 1.805797
MAE train: 0.804199	val: 1.545608	test: 1.402272

Epoch: 29
Loss: 1.3760835230350494
RMSE train: 0.925654	val: 2.041457	test: 1.827572
MAE train: 0.737731	val: 1.566049	test: 1.434185

Epoch: 30
Loss: 1.3153136670589447
RMSE train: 0.877635	val: 2.061230	test: 1.818162
MAE train: 0.702355	val: 1.541380	test: 1.435935

Epoch: 31
Loss: 1.2246741950511932
RMSE train: 0.890251	val: 2.009857	test: 1.783573
MAE train: 0.698045	val: 1.496416	test: 1.383162

Epoch: 32
Loss: 1.1791497766971588
RMSE train: 0.854177	val: 1.690349	test: 1.679688
MAE train: 0.665785	val: 1.317879	test: 1.298669

Epoch: 33
Loss: 1.2103695273399353
RMSE train: 0.827741	val: 1.662450	test: 1.667186
MAE train: 0.639393	val: 1.289765	test: 1.290465

Epoch: 34
Loss: 1.1321635246276855
RMSE train: 0.834893	val: 1.789492	test: 1.789334
MAE train: 0.656142	val: 1.406111	test: 1.404672

Epoch: 35
Loss: 1.14044651389122
RMSE train: 0.770155	val: 1.677353	test: 1.667849
MAE train: 0.604901	val: 1.327626	test: 1.294962

Epoch: 36
Loss: 1.099709391593933
RMSE train: 0.761055	val: 1.537054	test: 1.588464
MAE train: 0.595129	val: 1.210265	test: 1.226134

Epoch: 37
Loss: 1.0741887986660004
RMSE train: 0.749482	val: 1.510613	test: 1.587837
MAE train: 0.583690	val: 1.172396	test: 1.232043

Epoch: 38
Loss: 0.9754080772399902
RMSE train: 0.778325	val: 1.697962	test: 1.747401
MAE train: 0.608945	val: 1.301965	test: 1.366114

Epoch: 39
Loss: 0.9763645082712173
RMSE train: 0.760135	val: 1.907489	test: 1.940150
MAE train: 0.600144	val: 1.539345	test: 1.546445

Epoch: 40
Loss: 0.9876295775175095
RMSE train: 0.713775	val: 1.842185	test: 1.881387
MAE train: 0.561101	val: 1.470664	test: 1.484514

Epoch: 41
Loss: 0.9818696081638336
RMSE train: 0.676174	val: 1.686568	test: 1.739901
MAE train: 0.530001	val: 1.317722	test: 1.359881

Epoch: 42
Loss: 0.9010225385427475
RMSE train: 0.697659	val: 1.686227	test: 1.721781
MAE train: 0.549622	val: 1.341372	test: 1.365309

Epoch: 43
Loss: 0.8771293014287949
RMSE train: 0.722667	val: 1.644453	test: 1.657579
MAE train: 0.571044	val: 1.294023	test: 1.287152

Epoch: 44
Loss: 0.8122444599866867
RMSE train: 0.704217	val: 1.675938	test: 1.679967
MAE train: 0.551388	val: 1.339313	test: 1.328414

Epoch: 45
Loss: 0.7728055119514465
RMSE train: 0.664649	val: 1.660872	test: 1.681328
MAE train: 0.524319	val: 1.333969	test: 1.343418

Epoch: 46
Loss: 0.7907899022102356
RMSE train: 0.622202	val: 1.544244	test: 1.610450
MAE train: 0.494220	val: 1.212812	test: 1.245972

Epoch: 47
Loss: 0.7379073202610016
RMSE train: 0.631494	val: 1.532336	test: 1.605615
MAE train: 0.504666	val: 1.199547	test: 1.223297

Epoch: 48
Loss: 0.7776178568601608
RMSE train: 0.631376	val: 1.578771	test: 1.625927
MAE train: 0.504909	val: 1.237691	test: 1.251007

Epoch: 49
Loss: 0.7047076374292374
RMSE train: 0.642527	val: 1.623509	test: 1.658123
MAE train: 0.513043	val: 1.262134	test: 1.286601

Epoch: 50
Loss: 0.7348304688930511
RMSE train: 0.622796	val: 1.660590	test: 1.682321
MAE train: 0.493292	val: 1.296731	test: 1.302123

Epoch: 51
Loss: 0.6833222210407257
RMSE train: 0.649985	val: 1.682919	test: 1.689149
MAE train: 0.501503	val: 1.318067	test: 1.319034

Epoch: 52
Loss: 0.6580618172883987
RMSE train: 0.681286	val: 1.742942	test: 1.741576
MAE train: 0.531540	val: 1.403487	test: 1.390204

Epoch: 53
Loss: 0.7136248648166656
RMSE train: 0.660619	val: 1.699942	test: 1.713679
MAE train: 0.518736	val: 1.366876	test: 1.347450

Epoch: 54
Loss: 0.6565698534250259
RMSE train: 0.570565	val: 1.503687	test: 1.586165
MAE train: 0.456557	val: 1.165298	test: 1.198731

Epoch: 55
Loss: 0.6729625910520554
RMSE train: 0.609444	val: 1.521121	test: 1.610839
MAE train: 0.484272	val: 1.150712	test: 1.226125

Epoch: 56
Loss: 0.6409095078706741
RMSE train: 0.619631	val: 1.657842	test: 1.684136
MAE train: 0.487059	val: 1.304828	test: 1.314454

Epoch: 57
Loss: 0.6710563600063324
RMSE train: 0.610809	val: 1.544513	test: 1.552507
MAE train: 0.481998	val: 1.209568	test: 1.185335

Epoch: 58
Loss: 0.6495967656373978
RMSE train: 0.583645	val: 1.549243	test: 1.569304
MAE train: 0.466013	val: 1.219444	test: 1.250568

Epoch: 59
Loss: 0.5964769423007965
RMSE train: 0.635441	val: 1.588420	test: 1.619896
MAE train: 0.496392	val: 1.239991	test: 1.226172

Epoch: 60
Loss: 0.6452194601297379
RMSE train: 0.732451	val: 1.705544	test: 1.743286
MAE train: 0.571874	val: 1.346067	test: 1.336384

Epoch: 61
Loss: 0.5849563926458359
RMSE train: 0.580013	val: 1.594779	test: 1.627393
MAE train: 0.449241	val: 1.225919	test: 1.248726

Epoch: 62
Loss: 0.5970312505960464
RMSE train: 0.519756	val: 1.506785	test: 1.566224
MAE train: 0.403448	val: 1.161223	test: 1.199983

Epoch: 63
Loss: 0.6309875696897507
RMSE train: 0.617869	val: 1.640380	test: 1.658729
MAE train: 0.479470	val: 1.274978	test: 1.278260

Epoch: 64
Loss: 0.5633981451392174
RMSE train: 0.642100	val: 1.734534	test: 1.670777
MAE train: 0.502725	val: 1.330234	test: 1.297749

Epoch: 65
Loss: 0.6353820860385895
RMSE train: 0.504877	val: 1.522812	test: 1.583669
MAE train: 0.399147	val: 1.166486	test: 1.232528

Epoch: 66
Loss: 0.5323130190372467
RMSE train: 0.541894	val: 1.536244	test: 1.603436
MAE train: 0.427944	val: 1.169606	test: 1.232321

Epoch: 67
Loss: 0.5731366202235222
RMSE train: 0.584146	val: 1.573138	test: 1.601446
MAE train: 0.469161	val: 1.184158	test: 1.228657

Epoch: 68
Loss: 0.5583990067243576
RMSE train: 0.553160	val: 1.535155	test: 1.597353
MAE train: 0.441350	val: 1.159416	test: 1.220436

Epoch: 69
Loss: 0.5433248355984688
RMSE train: 0.533479	val: 1.520545	test: 1.614679
MAE train: 0.421824	val: 1.181862	test: 1.237602

Epoch: 70
Loss: 0.5741737559437752
RMSE train: 0.547478	val: 1.545304	test: 1.614580
MAE train: 0.428407	val: 1.192484	test: 1.235828

Epoch: 71
Loss: 0.553569421172142
RMSE train: 0.550922	val: 1.484718	test: 1.551318
MAE train: 0.425646	val: 1.124634	test: 1.193928

Epoch: 72
Loss: 0.5069293975830078
RMSE train: 0.547598	val: 1.464862	test: 1.543492
MAE train: 0.429251	val: 1.135110	test: 1.169206

Epoch: 73
Loss: 0.5078456550836563
RMSE train: 0.544957	val: 1.444481	test: 1.535107
MAE train: 0.427326	val: 1.134786	test: 1.178661

Epoch: 74
Loss: 0.5032234564423561
RMSE train: 0.551318	val: 1.472125	test: 1.545685
MAE train: 0.429007	val: 1.117503	test: 1.173291

Epoch: 75
Loss: 0.5304577052593231
RMSE train: 0.486097	val: 1.337493	test: 1.476845
MAE train: 0.371062	val: 1.025571	test: 1.127813

Epoch: 76
Loss: 0.4725178852677345
RMSE train: 0.516142	val: 1.367101	test: 1.483399
MAE train: 0.395541	val: 1.067999	test: 1.145135

Epoch: 77
Loss: 0.5370679050683975
RMSE train: 0.575195	val: 1.487762	test: 1.577524
MAE train: 0.435983	val: 1.131436	test: 1.199329

Epoch: 78
Loss: 0.47413384169340134
RMSE train: 0.548152	val: 1.431219	test: 1.506512
MAE train: 0.425613	val: 1.128985	test: 1.133029

Epoch: 79
Loss: 0.43018000572919846
RMSE train: 0.474712	val: 1.305676	test: 1.434923
MAE train: 0.375668	val: 1.042951	test: 1.075540

Epoch: 80
Loss: 0.4971531331539154
RMSE train: 0.468116	val: 1.387922	test: 1.490080
MAE train: 0.364745	val: 1.074042	test: 1.118259

Epoch: 81
Loss: 0.4531349837779999
RMSE train: 0.582975	val: 1.729781	test: 1.795820
MAE train: 0.453954	val: 1.362518	test: 1.385414

Epoch: 82
Loss: 0.3856099247932434
RMSE train: 0.616113	val: 1.789164	test: 1.845074
MAE train: 0.468306	val: 1.451958	test: 1.460772

Epoch: 83
Loss: 0.47821972519159317
RMSE train: 0.474041	val: 1.514211	test: 1.610193
MAE train: 0.793643	val: 1.420879	test: 1.404855

Epoch: 23
Loss: 1.5954008996486664
RMSE train: 1.018181	val: 1.821259	test: 1.858733
MAE train: 0.771398	val: 1.479469	test: 1.473988

Epoch: 24
Loss: 1.5120902061462402
RMSE train: 0.991864	val: 2.113508	test: 2.057270
MAE train: 0.760835	val: 1.645127	test: 1.595924

Epoch: 25
Loss: 1.3636605441570282
RMSE train: 1.019087	val: 1.977481	test: 1.930653
MAE train: 0.795393	val: 1.530754	test: 1.494097

Epoch: 26
Loss: 1.5073502957820892
RMSE train: 0.966555	val: 1.723067	test: 1.782061
MAE train: 0.759565	val: 1.380068	test: 1.411405

Epoch: 27
Loss: 1.4174581468105316
RMSE train: 0.925758	val: 1.926704	test: 2.003612
MAE train: 0.719758	val: 1.535950	test: 1.571870

Epoch: 28
Loss: 1.265811800956726
RMSE train: 0.896032	val: 2.040835	test: 2.107256
MAE train: 0.698485	val: 1.597274	test: 1.658324

Epoch: 29
Loss: 1.2281584739685059
RMSE train: 0.900029	val: 1.851429	test: 1.852407
MAE train: 0.714000	val: 1.451806	test: 1.482877

Epoch: 30
Loss: 1.173297107219696
RMSE train: 0.911813	val: 1.600484	test: 1.606123
MAE train: 0.728934	val: 1.242019	test: 1.264825

Epoch: 31
Loss: 1.1473964154720306
RMSE train: 0.870189	val: 1.699109	test: 1.695226
MAE train: 0.690782	val: 1.328964	test: 1.313738

Epoch: 32
Loss: 1.195463478565216
RMSE train: 0.835886	val: 1.991343	test: 2.017524
MAE train: 0.645424	val: 1.562411	test: 1.535315

Epoch: 33
Loss: 1.0640070140361786
RMSE train: 0.809998	val: 1.954111	test: 2.003389
MAE train: 0.617813	val: 1.556731	test: 1.541500

Epoch: 34
Loss: 1.0942751318216324
RMSE train: 0.843672	val: 1.957863	test: 1.968838
MAE train: 0.649409	val: 1.544522	test: 1.489779

Epoch: 35
Loss: 1.1326858699321747
RMSE train: 0.819438	val: 2.029569	test: 2.035867
MAE train: 0.628075	val: 1.594489	test: 1.556683

Epoch: 36
Loss: 0.9359076172113419
RMSE train: 0.777888	val: 1.983125	test: 2.047963
MAE train: 0.593645	val: 1.595980	test: 1.603885

Epoch: 37
Loss: 1.0445609539747238
RMSE train: 0.781522	val: 2.170581	test: 2.226147
MAE train: 0.594287	val: 1.747099	test: 1.764316

Epoch: 38
Loss: 0.9407298266887665
RMSE train: 0.856112	val: 2.236899	test: 2.274012
MAE train: 0.647937	val: 1.783922	test: 1.797803

Epoch: 39
Loss: 1.0149765312671661
RMSE train: 0.787081	val: 1.888896	test: 1.944162
MAE train: 0.595892	val: 1.513645	test: 1.515898

Epoch: 40
Loss: 0.9021227061748505
RMSE train: 0.761600	val: 1.912038	test: 1.956014
MAE train: 0.574507	val: 1.523809	test: 1.536612

Epoch: 41
Loss: 0.835024282336235
RMSE train: 0.785116	val: 1.932703	test: 1.967780
MAE train: 0.599772	val: 1.556451	test: 1.530332

Epoch: 42
Loss: 0.8711836934089661
RMSE train: 0.784806	val: 1.775307	test: 1.818448
MAE train: 0.599638	val: 1.438613	test: 1.384389

Epoch: 43
Loss: 0.866880014538765
RMSE train: 0.782031	val: 1.944962	test: 2.012976
MAE train: 0.600625	val: 1.548968	test: 1.509741

Epoch: 44
Loss: 0.8614952117204666
RMSE train: 0.829513	val: 2.295600	test: 2.359838
MAE train: 0.632254	val: 1.801427	test: 1.817211

Epoch: 45
Loss: 0.7901196181774139
RMSE train: 0.849786	val: 2.182484	test: 2.237581
MAE train: 0.646203	val: 1.780060	test: 1.758488

Epoch: 46
Loss: 0.7993144989013672
RMSE train: 0.775784	val: 2.079961	test: 2.114033
MAE train: 0.589276	val: 1.690209	test: 1.656929

Epoch: 47
Loss: 0.687426432967186
RMSE train: 0.762524	val: 2.035562	test: 2.056809
MAE train: 0.578263	val: 1.638186	test: 1.627090

Epoch: 48
Loss: 0.7083475738763809
RMSE train: 0.793417	val: 1.944890	test: 1.951112
MAE train: 0.603007	val: 1.595089	test: 1.521563

Epoch: 49
Loss: 0.7375537306070328
RMSE train: 0.761572	val: 1.796466	test: 1.819739
MAE train: 0.594008	val: 1.481171	test: 1.403429

Epoch: 50
Loss: 0.7560268640518188
RMSE train: 0.710749	val: 1.768372	test: 1.811484
MAE train: 0.559169	val: 1.413015	test: 1.380447

Epoch: 51
Loss: 0.6635093241930008
RMSE train: 0.728976	val: 2.175754	test: 2.204711
MAE train: 0.556960	val: 1.713388	test: 1.714285

Epoch: 52
Loss: 0.704586386680603
RMSE train: 0.731198	val: 2.039872	test: 2.094070
MAE train: 0.559232	val: 1.650413	test: 1.636070

Epoch: 53
Loss: 0.7148598283529282
RMSE train: 0.697075	val: 1.890388	test: 1.946365
MAE train: 0.539003	val: 1.557770	test: 1.514401

Epoch: 54
Loss: 0.6144856512546539
RMSE train: 0.715601	val: 2.156179	test: 2.216350
MAE train: 0.560576	val: 1.759015	test: 1.717241

Epoch: 55
Loss: 0.7367267310619354
RMSE train: 0.816789	val: 2.292648	test: 2.336185
MAE train: 0.630949	val: 1.880403	test: 1.834206

Epoch: 56
Loss: 0.6674452722072601
RMSE train: 0.742188	val: 1.956018	test: 1.990061
MAE train: 0.570702	val: 1.613138	test: 1.547206

Epoch: 57
Loss: 0.68714939057827
RMSE train: 0.693107	val: 1.987184	test: 1.995557
MAE train: 0.525259	val: 1.621886	test: 1.561700

Epoch: 58
Loss: 0.6085406988859177
RMSE train: 0.676948	val: 2.057069	test: 2.061507
MAE train: 0.519035	val: 1.675201	test: 1.623459

Epoch: 59
Loss: 0.5925376117229462
RMSE train: 0.634749	val: 1.910333	test: 1.949315
MAE train: 0.493367	val: 1.559971	test: 1.517322

Epoch: 60
Loss: 0.5449551641941071
RMSE train: 0.682546	val: 1.934455	test: 1.991291
MAE train: 0.531011	val: 1.552002	test: 1.531467

Epoch: 61
Loss: 0.5396503210067749
RMSE train: 0.713497	val: 1.937767	test: 1.984905
MAE train: 0.544366	val: 1.559231	test: 1.548951

Epoch: 62
Loss: 0.602241575717926
RMSE train: 0.681212	val: 1.816950	test: 1.871277
MAE train: 0.523624	val: 1.492808	test: 1.459676

Epoch: 63
Loss: 0.5564488023519516
RMSE train: 0.728432	val: 1.881726	test: 1.953685
MAE train: 0.555457	val: 1.546102	test: 1.504062

Epoch: 64
Loss: 0.519228920340538
RMSE train: 0.712930	val: 2.021176	test: 2.102811
MAE train: 0.542258	val: 1.634062	test: 1.616627

Epoch: 65
Loss: 0.5388796702027321
RMSE train: 0.708227	val: 2.001476	test: 2.077824
MAE train: 0.536773	val: 1.615009	test: 1.598804

Epoch: 66
Loss: 0.5372254103422165
RMSE train: 0.714942	val: 1.975549	test: 2.034983
MAE train: 0.547841	val: 1.630198	test: 1.566655

Epoch: 67
Loss: 0.493771955370903
RMSE train: 0.634008	val: 1.921484	test: 1.992686
MAE train: 0.490847	val: 1.581598	test: 1.542315

Epoch: 68
Loss: 0.5287684574723244
RMSE train: 0.625445	val: 1.970440	test: 2.041785
MAE train: 0.484575	val: 1.631748	test: 1.572959

Epoch: 69
Loss: 0.5400931611657143
RMSE train: 0.604254	val: 1.890138	test: 1.952579
MAE train: 0.466621	val: 1.563507	test: 1.506389

Epoch: 70
Loss: 0.49726030230522156
RMSE train: 0.564132	val: 1.719799	test: 1.813432
MAE train: 0.442902	val: 1.402794	test: 1.415812

Epoch: 71
Loss: 0.5068318322300911
RMSE train: 0.585302	val: 1.658700	test: 1.757657
MAE train: 0.458129	val: 1.356921	test: 1.352448

Epoch: 72
Loss: 0.45032016187906265
RMSE train: 0.645088	val: 1.869790	test: 1.957655
MAE train: 0.498623	val: 1.525730	test: 1.498555

Epoch: 73
Loss: 0.4773477390408516
RMSE train: 0.712944	val: 2.231630	test: 2.312497
MAE train: 0.531337	val: 1.809218	test: 1.812977

Epoch: 74
Loss: 0.4928368553519249
RMSE train: 0.642942	val: 2.162457	test: 2.246567
MAE train: 0.487398	val: 1.758404	test: 1.757630

Epoch: 75
Loss: 0.44982126355171204
RMSE train: 0.647796	val: 1.988402	test: 2.075361
MAE train: 0.501821	val: 1.636211	test: 1.596664

Epoch: 76
Loss: 0.44978997111320496
RMSE train: 0.621609	val: 1.808346	test: 1.907104
MAE train: 0.478842	val: 1.461953	test: 1.469757

Epoch: 77
Loss: 0.45805929601192474
RMSE train: 0.711138	val: 1.820301	test: 1.897233
MAE train: 0.535294	val: 1.484060	test: 1.448261

Epoch: 78
Loss: 0.5000188127160072
RMSE train: 0.714096	val: 1.728976	test: 1.811934
MAE train: 0.547205	val: 1.430715	test: 1.378171

Epoch: 79
Loss: 0.4135177657008171
RMSE train: 0.662300	val: 1.843242	test: 1.933903
MAE train: 0.512807	val: 1.501778	test: 1.472719

Epoch: 80
Loss: 0.4608238488435745
RMSE train: 0.678672	val: 2.023580	test: 2.102035
MAE train: 0.521342	val: 1.642464	test: 1.611959

Epoch: 81
Loss: 0.42785274237394333
RMSE train: 0.675202	val: 1.886726	test: 1.955485
MAE train: 0.515741	val: 1.551255	test: 1.497692

Epoch: 82
Loss: 0.44480475783348083
RMSE train: 0.599949	val: 1.844782	test: 1.935109
MAE train: 0.460465	val: 1.497555	test: 1.497852

Epoch: 83
Loss: 0.4382179155945778
RMSE train: 0.579709	val: 2.040972	test: 2.148846
MAE train: 0.701522	val: 1.152662	test: 1.130299

Epoch: 23
Loss: 1.2479838728904724
RMSE train: 0.877349	val: 1.465804	test: 1.359262
MAE train: 0.685721	val: 1.133285	test: 1.123266

Epoch: 24
Loss: 1.1475948989391327
RMSE train: 0.849164	val: 1.525975	test: 1.451342
MAE train: 0.673245	val: 1.173303	test: 1.198115

Epoch: 25
Loss: 1.0673363506793976
RMSE train: 0.834002	val: 1.560310	test: 1.526071
MAE train: 0.659796	val: 1.219066	test: 1.241285

Epoch: 26
Loss: 1.0374746024608612
RMSE train: 0.789000	val: 1.520307	test: 1.452679
MAE train: 0.623947	val: 1.202213	test: 1.182202

Epoch: 27
Loss: 1.0718892514705658
RMSE train: 0.799513	val: 1.485799	test: 1.403484
MAE train: 0.623737	val: 1.189997	test: 1.129166

Epoch: 28
Loss: 1.0770877301692963
RMSE train: 0.799614	val: 1.526328	test: 1.421025
MAE train: 0.618378	val: 1.218820	test: 1.147258

Epoch: 29
Loss: 1.081092283129692
RMSE train: 0.804255	val: 1.504268	test: 1.403846
MAE train: 0.622200	val: 1.212817	test: 1.137050

Epoch: 30
Loss: 0.9122401177883148
RMSE train: 0.828065	val: 1.442954	test: 1.374069
MAE train: 0.636446	val: 1.151966	test: 1.114086

Epoch: 31
Loss: 0.9685454666614532
RMSE train: 0.792422	val: 1.259274	test: 1.261234
MAE train: 0.613958	val: 1.044114	test: 1.016512

Epoch: 32
Loss: 0.8935915678739548
RMSE train: 0.765785	val: 1.286320	test: 1.300975
MAE train: 0.592292	val: 1.054469	test: 1.047324

Epoch: 33
Loss: 0.9662088751792908
RMSE train: 0.788925	val: 1.367389	test: 1.363070
MAE train: 0.602129	val: 1.089257	test: 1.087305

Epoch: 34
Loss: 0.9225616455078125
RMSE train: 0.720526	val: 1.240573	test: 1.247385
MAE train: 0.564792	val: 1.015561	test: 0.998262

Epoch: 35
Loss: 0.862902820110321
RMSE train: 0.703969	val: 1.217223	test: 1.254490
MAE train: 0.557500	val: 1.004326	test: 1.021456

Epoch: 36
Loss: 0.8271310776472092
RMSE train: 0.671575	val: 1.359102	test: 1.350273
MAE train: 0.531897	val: 1.097635	test: 1.101083

Epoch: 37
Loss: 0.7720443904399872
RMSE train: 0.648442	val: 1.257798	test: 1.250988
MAE train: 0.506684	val: 1.009036	test: 1.026007

Epoch: 38
Loss: 0.7447272539138794
RMSE train: 0.694356	val: 1.226236	test: 1.234040
MAE train: 0.541860	val: 0.986087	test: 0.972746

Epoch: 39
Loss: 0.7689452916383743
RMSE train: 0.652296	val: 1.201014	test: 1.222539
MAE train: 0.523738	val: 0.970505	test: 0.989896

Epoch: 40
Loss: 0.8181875497102737
RMSE train: 0.634574	val: 1.246116	test: 1.265174
MAE train: 0.504905	val: 1.011245	test: 1.024628

Epoch: 41
Loss: 0.6738896667957306
RMSE train: 0.682485	val: 1.259527	test: 1.257144
MAE train: 0.524555	val: 1.030556	test: 1.020186

Epoch: 42
Loss: 0.6992094367742538
RMSE train: 0.727318	val: 1.384711	test: 1.315125
MAE train: 0.554008	val: 1.106174	test: 1.036766

Epoch: 43
Loss: 0.6529834419488907
RMSE train: 0.686650	val: 1.317307	test: 1.251093
MAE train: 0.520171	val: 1.052423	test: 0.976173

Epoch: 44
Loss: 0.5717972964048386
RMSE train: 0.678476	val: 1.246737	test: 1.223742
MAE train: 0.516482	val: 1.016532	test: 0.949064

Epoch: 45
Loss: 0.6424058526754379
RMSE train: 0.670119	val: 1.290835	test: 1.236975
MAE train: 0.510741	val: 1.029271	test: 0.959016

Epoch: 46
Loss: 0.6101064085960388
RMSE train: 0.663618	val: 1.312531	test: 1.244103
MAE train: 0.510782	val: 1.045703	test: 0.986842

Epoch: 47
Loss: 0.6178959459066391
RMSE train: 0.665303	val: 1.265858	test: 1.223076
MAE train: 0.515180	val: 1.011839	test: 0.986800

Epoch: 48
Loss: 0.6261047273874283
RMSE train: 0.590523	val: 1.215070	test: 1.173045
MAE train: 0.459121	val: 0.965406	test: 0.969379

Epoch: 49
Loss: 0.5424085706472397
RMSE train: 0.594530	val: 1.171178	test: 1.144204
MAE train: 0.458370	val: 0.942145	test: 0.920424

Epoch: 50
Loss: 0.596215158700943
RMSE train: 0.676842	val: 1.258036	test: 1.220717
MAE train: 0.513398	val: 0.978318	test: 0.973453

Epoch: 51
Loss: 0.581849217414856
RMSE train: 0.678974	val: 1.245619	test: 1.240217
MAE train: 0.518240	val: 0.985417	test: 0.964789

Epoch: 52
Loss: 0.5875362306833267
RMSE train: 0.598962	val: 1.209078	test: 1.223701
MAE train: 0.461320	val: 0.969229	test: 0.965612

Epoch: 53
Loss: 0.6061670184135437
RMSE train: 0.679975	val: 1.288801	test: 1.332850
MAE train: 0.525170	val: 1.033388	test: 1.039841

Epoch: 54
Loss: 0.6026289314031601
RMSE train: 0.600546	val: 1.159060	test: 1.223191
MAE train: 0.466523	val: 0.936758	test: 0.963307

Epoch: 55
Loss: 0.5346818417310715
RMSE train: 0.621748	val: 1.237739	test: 1.245883
MAE train: 0.467431	val: 0.968050	test: 1.009981

Epoch: 56
Loss: 0.49541516602039337
RMSE train: 0.642365	val: 1.258428	test: 1.269947
MAE train: 0.488084	val: 1.012969	test: 0.972742

Epoch: 57
Loss: 0.5083968564867973
RMSE train: 0.598813	val: 1.254087	test: 1.255562
MAE train: 0.458983	val: 0.998559	test: 0.980691

Epoch: 58
Loss: 0.5525361448526382
RMSE train: 0.542144	val: 1.161520	test: 1.207887
MAE train: 0.421315	val: 0.924260	test: 0.982271

Epoch: 59
Loss: 0.47798916697502136
RMSE train: 0.617429	val: 1.229224	test: 1.273539
MAE train: 0.474196	val: 0.963639	test: 1.027153

Epoch: 60
Loss: 0.5133580267429352
RMSE train: 0.601300	val: 1.217828	test: 1.250856
MAE train: 0.470168	val: 0.979967	test: 0.998841

Epoch: 61
Loss: 0.48071980476379395
RMSE train: 0.541023	val: 1.188639	test: 1.195563
MAE train: 0.421492	val: 0.956101	test: 0.982871

Epoch: 62
Loss: 0.4736420437693596
RMSE train: 0.542322	val: 1.174715	test: 1.172857
MAE train: 0.422614	val: 0.944669	test: 0.953586

Epoch: 63
Loss: 0.48616327345371246
RMSE train: 0.620364	val: 1.227202	test: 1.218242
MAE train: 0.486059	val: 0.996138	test: 0.947969

Epoch: 64
Loss: 0.4817233085632324
RMSE train: 0.707949	val: 1.257479	test: 1.245958
MAE train: 0.534896	val: 0.994210	test: 0.971591

Epoch: 65
Loss: 0.4573412761092186
RMSE train: 0.715645	val: 1.317833	test: 1.316705
MAE train: 0.549893	val: 1.082709	test: 1.008408

Epoch: 66
Loss: 0.46436014026403427
RMSE train: 0.676089	val: 1.279713	test: 1.248469
MAE train: 0.497802	val: 1.018273	test: 0.977386

Epoch: 67
Loss: 0.48127971589565277
RMSE train: 0.557616	val: 1.206472	test: 1.192209
MAE train: 0.430766	val: 0.968661	test: 0.960419

Epoch: 68
Loss: 0.485366128385067
RMSE train: 0.633803	val: 1.302718	test: 1.302773
MAE train: 0.493311	val: 1.062901	test: 1.019565

Epoch: 69
Loss: 0.40634047985076904
RMSE train: 0.577214	val: 1.275406	test: 1.221321
MAE train: 0.461458	val: 0.986846	test: 0.998053

Epoch: 70
Loss: 0.5440801829099655
RMSE train: 0.572836	val: 1.179401	test: 1.142219
MAE train: 0.463037	val: 0.955888	test: 0.906696

Epoch: 71
Loss: 0.4849345237016678
RMSE train: 0.553101	val: 1.183500	test: 1.144006
MAE train: 0.431623	val: 0.949463	test: 0.909792

Epoch: 72
Loss: 0.43020640313625336
RMSE train: 0.526233	val: 1.255645	test: 1.194417
MAE train: 0.410554	val: 0.975108	test: 0.961015

Epoch: 73
Loss: 0.43142881989479065
RMSE train: 0.491654	val: 1.231638	test: 1.204875
MAE train: 0.384561	val: 0.964553	test: 0.979593

Epoch: 74
Loss: 0.3867974504828453
RMSE train: 0.579503	val: 1.228246	test: 1.251321
MAE train: 0.448040	val: 0.994610	test: 1.007130

Epoch: 75
Loss: 0.40998724848032
RMSE train: 0.552719	val: 1.216887	test: 1.222174
MAE train: 0.424241	val: 0.978990	test: 0.980799

Epoch: 76
Loss: 0.3652403727173805
RMSE train: 0.696983	val: 1.410023	test: 1.408434
MAE train: 0.532109	val: 1.160099	test: 1.089649

Epoch: 77
Loss: 0.48050767183303833
RMSE train: 0.617096	val: 1.351301	test: 1.331269
MAE train: 0.471506	val: 1.056410	test: 1.038558

Epoch: 78
Loss: 0.40722647309303284
RMSE train: 0.556338	val: 1.190343	test: 1.214346
MAE train: 0.432678	val: 0.954258	test: 0.941561

Epoch: 79
Loss: 0.3916441351175308
RMSE train: 0.530597	val: 1.171030	test: 1.186408
MAE train: 0.413998	val: 0.920241	test: 0.931280

Epoch: 80
Loss: 0.3699057325720787
RMSE train: 0.475773	val: 1.229696	test: 1.195580
MAE train: 0.371994	val: 0.959352	test: 0.961095

Epoch: 81
Loss: 0.40384432673454285
RMSE train: 0.545324	val: 1.223613	test: 1.235122
MAE train: 0.422702	val: 0.995265	test: 0.971005

Epoch: 82
Loss: 0.3632989227771759
RMSE train: 0.596956	val: 1.251518	test: 1.279847
MAE train: 0.448867	val: 1.029680	test: 1.007427

Epoch: 83
Loss: 0.3884260058403015
RMSE train: 0.499412	val: 1.221466	test: 1.248436
MAE train: 0.629344	val: 1.250872	test: 1.255261

Epoch: 23
Loss: 1.190737634897232
RMSE train: 0.866880	val: 1.399188	test: 1.478862
MAE train: 0.662484	val: 1.141591	test: 1.157185

Epoch: 24
Loss: 1.0825145691633224
RMSE train: 0.832107	val: 1.372389	test: 1.436294
MAE train: 0.645851	val: 1.101366	test: 1.124397

Epoch: 25
Loss: 1.0951193124055862
RMSE train: 0.774531	val: 1.629351	test: 1.505560
MAE train: 0.598185	val: 1.266338	test: 1.212764

Epoch: 26
Loss: 0.9656699746847153
RMSE train: 0.783855	val: 1.556950	test: 1.473774
MAE train: 0.608851	val: 1.219483	test: 1.175225

Epoch: 27
Loss: 1.0050864517688751
RMSE train: 0.745780	val: 1.460507	test: 1.400615
MAE train: 0.574248	val: 1.156574	test: 1.125485

Epoch: 28
Loss: 0.9241699576377869
RMSE train: 0.728868	val: 1.502117	test: 1.434949
MAE train: 0.559370	val: 1.205977	test: 1.163184

Epoch: 29
Loss: 0.864486813545227
RMSE train: 0.731065	val: 1.528497	test: 1.467189
MAE train: 0.558395	val: 1.221232	test: 1.191597

Epoch: 30
Loss: 0.8681147694587708
RMSE train: 0.705586	val: 1.490020	test: 1.490309
MAE train: 0.541723	val: 1.189104	test: 1.186334

Epoch: 31
Loss: 0.8272460997104645
RMSE train: 0.677566	val: 1.335042	test: 1.360245
MAE train: 0.519002	val: 1.077157	test: 1.102261

Epoch: 32
Loss: 0.9001945406198502
RMSE train: 0.675155	val: 1.249259	test: 1.310994
MAE train: 0.520599	val: 1.022897	test: 1.056521

Epoch: 33
Loss: 0.8588916659355164
RMSE train: 0.633786	val: 1.337503	test: 1.337447
MAE train: 0.494074	val: 1.076659	test: 1.118838

Epoch: 34
Loss: 0.8164187967777252
RMSE train: 0.629157	val: 1.419733	test: 1.364135
MAE train: 0.486547	val: 1.132839	test: 1.127924

Epoch: 35
Loss: 0.7487587332725525
RMSE train: 0.675249	val: 1.359089	test: 1.330549
MAE train: 0.525561	val: 1.078799	test: 1.065200

Epoch: 36
Loss: 0.7921461313962936
RMSE train: 0.687550	val: 1.522691	test: 1.398928
MAE train: 0.529534	val: 1.206996	test: 1.115797

Epoch: 37
Loss: 0.7728105038404465
RMSE train: 0.692799	val: 1.558706	test: 1.429402
MAE train: 0.534589	val: 1.239267	test: 1.144163

Epoch: 38
Loss: 0.7347715646028519
RMSE train: 0.653989	val: 1.454758	test: 1.344366
MAE train: 0.503983	val: 1.157785	test: 1.079106

Epoch: 39
Loss: 0.70265793800354
RMSE train: 0.664005	val: 1.463437	test: 1.350716
MAE train: 0.511049	val: 1.161682	test: 1.107943

Epoch: 40
Loss: 0.7434705942869186
RMSE train: 0.649449	val: 1.378005	test: 1.360700
MAE train: 0.503273	val: 1.112480	test: 1.111475

Epoch: 41
Loss: 0.6757020205259323
RMSE train: 0.638208	val: 1.377129	test: 1.397691
MAE train: 0.494969	val: 1.124687	test: 1.125887

Epoch: 42
Loss: 0.6511406749486923
RMSE train: 0.734712	val: 1.583938	test: 1.572660
MAE train: 0.571401	val: 1.271834	test: 1.268039

Epoch: 43
Loss: 0.5991473495960236
RMSE train: 0.651362	val: 1.373063	test: 1.412950
MAE train: 0.502322	val: 1.116085	test: 1.134655

Epoch: 44
Loss: 0.6805173605680466
RMSE train: 0.621970	val: 1.428506	test: 1.453756
MAE train: 0.478295	val: 1.144180	test: 1.168552

Epoch: 45
Loss: 0.5854910910129547
RMSE train: 0.664753	val: 1.465200	test: 1.506397
MAE train: 0.512952	val: 1.184321	test: 1.178752

Epoch: 46
Loss: 0.6206134855747223
RMSE train: 0.596834	val: 1.382555	test: 1.408114
MAE train: 0.461968	val: 1.115781	test: 1.122214

Epoch: 47
Loss: 0.6360434740781784
RMSE train: 0.603917	val: 1.356429	test: 1.425664
MAE train: 0.465849	val: 1.123815	test: 1.127393

Epoch: 48
Loss: 0.6172059029340744
RMSE train: 0.639788	val: 1.477387	test: 1.482350
MAE train: 0.492001	val: 1.199592	test: 1.189525

Epoch: 49
Loss: 0.5837033241987228
RMSE train: 0.629191	val: 1.462781	test: 1.471430
MAE train: 0.484223	val: 1.183693	test: 1.183923

Epoch: 50
Loss: 0.6220365464687347
RMSE train: 0.572600	val: 1.355836	test: 1.393852
MAE train: 0.441449	val: 1.089099	test: 1.124341

Epoch: 51
Loss: 0.5399703830480576
RMSE train: 0.638597	val: 1.518169	test: 1.510736
MAE train: 0.494096	val: 1.204311	test: 1.224419

Epoch: 52
Loss: 0.5219142585992813
RMSE train: 0.645055	val: 1.403725	test: 1.473170
MAE train: 0.498351	val: 1.120144	test: 1.173173

Epoch: 53
Loss: 0.5447663813829422
RMSE train: 0.625414	val: 1.425329	test: 1.478671
MAE train: 0.482303	val: 1.145817	test: 1.180929

Epoch: 54
Loss: 0.5568899065256119
RMSE train: 0.638016	val: 1.558906	test: 1.532644
MAE train: 0.495319	val: 1.238646	test: 1.232385

Epoch: 55
Loss: 0.49951498955488205
RMSE train: 0.583729	val: 1.416374	test: 1.401239
MAE train: 0.449139	val: 1.136772	test: 1.116384

Epoch: 56
Loss: 0.539807640016079
RMSE train: 0.600286	val: 1.359811	test: 1.379849
MAE train: 0.462871	val: 1.089004	test: 1.101895

Epoch: 57
Loss: 0.5133254826068878
RMSE train: 0.684417	val: 1.588544	test: 1.624226
MAE train: 0.522742	val: 1.254267	test: 1.299560

Epoch: 58
Loss: 0.481269508600235
RMSE train: 0.686763	val: 1.471608	test: 1.547865
MAE train: 0.527981	val: 1.204881	test: 1.222712

Epoch: 59
Loss: 0.566057562828064
RMSE train: 0.627897	val: 1.439279	test: 1.422152
MAE train: 0.479806	val: 1.155293	test: 1.115162

Epoch: 60
Loss: 0.5308488383889198
RMSE train: 0.621157	val: 1.521846	test: 1.459646
MAE train: 0.478629	val: 1.217975	test: 1.168534

Epoch: 61
Loss: 0.48789507150650024
RMSE train: 0.606699	val: 1.395857	test: 1.414496
MAE train: 0.470634	val: 1.121582	test: 1.106189

Epoch: 62
Loss: 0.49294211715459824
RMSE train: 0.595716	val: 1.404862	test: 1.394501
MAE train: 0.461940	val: 1.111143	test: 1.101940

Epoch: 63
Loss: 0.49990401417016983
RMSE train: 0.645582	val: 1.353982	test: 1.394396
MAE train: 0.499556	val: 1.065561	test: 1.097878

Epoch: 64
Loss: 0.4906368553638458
RMSE train: 0.549959	val: 1.201916	test: 1.329425
MAE train: 0.425380	val: 0.997679	test: 1.052405

Epoch: 65
Loss: 0.4475639835000038
RMSE train: 0.571228	val: 1.420858	test: 1.500105
MAE train: 0.441371	val: 1.130940	test: 1.187633

Epoch: 66
Loss: 0.48032014071941376
RMSE train: 0.618863	val: 1.518154	test: 1.542956
MAE train: 0.482727	val: 1.208005	test: 1.225705

Epoch: 67
Loss: 0.4493647739291191
RMSE train: 0.545080	val: 1.302028	test: 1.364618
MAE train: 0.422220	val: 1.063150	test: 1.059344

Epoch: 68
Loss: 0.4718031510710716
RMSE train: 0.583738	val: 1.444499	test: 1.456561
MAE train: 0.456560	val: 1.140429	test: 1.117114

Epoch: 69
Loss: 0.4551321789622307
RMSE train: 0.635189	val: 1.649326	test: 1.593928
MAE train: 0.488337	val: 1.295903	test: 1.237743

Epoch: 70
Loss: 0.4099850654602051
RMSE train: 0.639366	val: 1.410453	test: 1.467106
MAE train: 0.486267	val: 1.153000	test: 1.123880

Epoch: 71
Loss: 0.4754486456513405
RMSE train: 0.609524	val: 1.551658	test: 1.492451
MAE train: 0.469682	val: 1.241666	test: 1.171701

Epoch: 72
Loss: 0.44120706617832184
RMSE train: 0.652409	val: 1.575314	test: 1.585297
MAE train: 0.508233	val: 1.276690	test: 1.249954

Epoch: 73
Loss: 0.4130648821592331
RMSE train: 0.656758	val: 1.567139	test: 1.594445
MAE train: 0.509269	val: 1.303252	test: 1.253081

Epoch: 74
Loss: 0.40477973967790604
RMSE train: 0.537812	val: 1.461578	test: 1.440875
MAE train: 0.420560	val: 1.152333	test: 1.144510

Epoch: 75
Loss: 0.4042806699872017
RMSE train: 0.542936	val: 1.436225	test: 1.422585
MAE train: 0.424712	val: 1.134316	test: 1.116132

Epoch: 76
Loss: 0.39502549171447754
RMSE train: 0.566377	val: 1.497810	test: 1.482278
MAE train: 0.441791	val: 1.184356	test: 1.154579

Epoch: 77
Loss: 0.3961854577064514
RMSE train: 0.508986	val: 1.427661	test: 1.426983
MAE train: 0.396071	val: 1.145169	test: 1.115258

Epoch: 78
Loss: 0.3540003001689911
RMSE train: 0.547691	val: 1.458782	test: 1.468542
MAE train: 0.427861	val: 1.166874	test: 1.153576

Epoch: 79
Loss: 0.3935418725013733
RMSE train: 0.538683	val: 1.445555	test: 1.462335
MAE train: 0.423627	val: 1.160409	test: 1.149061

Epoch: 80
Loss: 0.4099242314696312
RMSE train: 0.522354	val: 1.388484	test: 1.445915
MAE train: 0.409677	val: 1.119301	test: 1.140065

Epoch: 81
Loss: 0.35510628670454025
RMSE train: 0.570995	val: 1.406839	test: 1.508736
MAE train: 0.442101	val: 1.159046	test: 1.181317

Epoch: 82
Loss: 0.3745209723711014
RMSE train: 0.601672	val: 1.451724	test: 1.561425
MAE train: 0.463492	val: 1.179444	test: 1.221764

Epoch: 83
Loss: 0.3183448389172554
RMSE train: 0.557192	val: 1.454299	test: 1.563666
MAE train: 0.796976	val: 1.396175	test: 1.353638

Epoch: 23
Loss: 1.6777042150497437
RMSE train: 1.042167	val: 2.250094	test: 1.917229
MAE train: 0.822204	val: 1.643946	test: 1.512474

Epoch: 24
Loss: 1.5311721861362457
RMSE train: 0.978329	val: 1.949504	test: 1.786738
MAE train: 0.770433	val: 1.434444	test: 1.402191

Epoch: 25
Loss: 1.5145952105522156
RMSE train: 0.937336	val: 1.653182	test: 1.630236
MAE train: 0.731268	val: 1.252172	test: 1.317397

Epoch: 26
Loss: 1.362037569284439
RMSE train: 0.933180	val: 1.929902	test: 1.808483
MAE train: 0.725815	val: 1.538903	test: 1.433339

Epoch: 27
Loss: 1.406716912984848
RMSE train: 0.882244	val: 1.944018	test: 1.663092
MAE train: 0.689184	val: 1.484576	test: 1.310245

Epoch: 28
Loss: 1.2849026024341583
RMSE train: 0.908613	val: 1.931358	test: 1.682980
MAE train: 0.717399	val: 1.456287	test: 1.345353

Epoch: 29
Loss: 1.1575557738542557
RMSE train: 0.888379	val: 2.009144	test: 1.696958
MAE train: 0.693844	val: 1.504589	test: 1.371911

Epoch: 30
Loss: 1.1900666058063507
RMSE train: 0.835665	val: 1.908120	test: 1.666800
MAE train: 0.652057	val: 1.455675	test: 1.349905

Epoch: 31
Loss: 1.0342051982879639
RMSE train: 0.809296	val: 1.791418	test: 1.599310
MAE train: 0.631507	val: 1.370223	test: 1.288831

Epoch: 32
Loss: 1.2065271735191345
RMSE train: 0.853843	val: 1.720119	test: 1.630143
MAE train: 0.655965	val: 1.308492	test: 1.265317

Epoch: 33
Loss: 1.076160192489624
RMSE train: 0.847471	val: 1.664459	test: 1.674773
MAE train: 0.648761	val: 1.269321	test: 1.285395

Epoch: 34
Loss: 1.100132405757904
RMSE train: 0.788567	val: 1.602591	test: 1.601381
MAE train: 0.605254	val: 1.209523	test: 1.269725

Epoch: 35
Loss: 0.9429793059825897
RMSE train: 0.793350	val: 1.692826	test: 1.644893
MAE train: 0.609059	val: 1.292767	test: 1.321584

Epoch: 36
Loss: 1.0447815656661987
RMSE train: 0.834690	val: 1.666142	test: 1.645169
MAE train: 0.639589	val: 1.270285	test: 1.313853

Epoch: 37
Loss: 0.9381515383720398
RMSE train: 0.885457	val: 1.619016	test: 1.619812
MAE train: 0.680453	val: 1.225889	test: 1.310448

Epoch: 38
Loss: 0.938623309135437
RMSE train: 0.927233	val: 1.645786	test: 1.640604
MAE train: 0.718862	val: 1.258465	test: 1.321163

Epoch: 39
Loss: 0.9163257777690887
RMSE train: 0.968599	val: 1.736384	test: 1.725743
MAE train: 0.760172	val: 1.340419	test: 1.351432

Epoch: 40
Loss: 0.8575214445590973
RMSE train: 0.895137	val: 1.697715	test: 1.718785
MAE train: 0.704292	val: 1.307489	test: 1.327674

Epoch: 41
Loss: 0.8699114620685577
RMSE train: 0.794549	val: 1.573736	test: 1.609163
MAE train: 0.625750	val: 1.186585	test: 1.289519

Epoch: 42
Loss: 0.8224658817052841
RMSE train: 0.773588	val: 1.614908	test: 1.608534
MAE train: 0.603455	val: 1.225298	test: 1.296573

Epoch: 43
Loss: 0.8064364939928055
RMSE train: 0.785538	val: 1.641289	test: 1.607587
MAE train: 0.612759	val: 1.247087	test: 1.277565

Epoch: 44
Loss: 0.8468400239944458
RMSE train: 0.782784	val: 1.638751	test: 1.617000
MAE train: 0.614745	val: 1.260382	test: 1.263493

Epoch: 45
Loss: 0.719801738858223
RMSE train: 0.764371	val: 1.739808	test: 1.666984
MAE train: 0.600504	val: 1.328907	test: 1.301423

Epoch: 46
Loss: 0.7344069480895996
RMSE train: 0.786479	val: 1.862450	test: 1.751119
MAE train: 0.604283	val: 1.432513	test: 1.396671

Epoch: 47
Loss: 0.7810640931129456
RMSE train: 0.834131	val: 1.735422	test: 1.723875
MAE train: 0.641242	val: 1.351466	test: 1.333129

Epoch: 48
Loss: 0.8155393302440643
RMSE train: 0.798839	val: 1.659919	test: 1.678058
MAE train: 0.619199	val: 1.295342	test: 1.299756

Epoch: 49
Loss: 0.6848295629024506
RMSE train: 0.798964	val: 1.711049	test: 1.684164
MAE train: 0.623034	val: 1.341796	test: 1.321733

Epoch: 50
Loss: 0.7497793138027191
RMSE train: 0.718175	val: 1.541349	test: 1.576551
MAE train: 0.562108	val: 1.203967	test: 1.263460

Epoch: 51
Loss: 0.6440048962831497
RMSE train: 0.698819	val: 1.529442	test: 1.573340
MAE train: 0.545414	val: 1.188903	test: 1.247943

Epoch: 52
Loss: 0.710542842745781
RMSE train: 0.691889	val: 1.635590	test: 1.643353
MAE train: 0.537867	val: 1.243879	test: 1.301412

Epoch: 53
Loss: 0.688416913151741
RMSE train: 0.762218	val: 1.695461	test: 1.710608
MAE train: 0.598675	val: 1.330019	test: 1.320797

Epoch: 54
Loss: 0.702247366309166
RMSE train: 0.821277	val: 1.736453	test: 1.755384
MAE train: 0.645687	val: 1.405764	test: 1.357524

Epoch: 55
Loss: 0.5891026556491852
RMSE train: 0.725179	val: 1.537504	test: 1.535340
MAE train: 0.576084	val: 1.193380	test: 1.233365

Epoch: 56
Loss: 0.6096780300140381
RMSE train: 0.634981	val: 1.530146	test: 1.544569
MAE train: 0.502230	val: 1.202650	test: 1.310368

Epoch: 57
Loss: 0.6381547152996063
RMSE train: 0.783812	val: 1.626840	test: 1.634745
MAE train: 0.611639	val: 1.247770	test: 1.313902

Epoch: 58
Loss: 0.6121052950620651
RMSE train: 0.752298	val: 1.697203	test: 1.706422
MAE train: 0.588450	val: 1.335390	test: 1.318219

Epoch: 59
Loss: 0.6125348508358002
RMSE train: 0.610325	val: 1.575130	test: 1.594493
MAE train: 0.476178	val: 1.205770	test: 1.282958

Epoch: 60
Loss: 0.5806021243333817
RMSE train: 0.633890	val: 1.554826	test: 1.584911
MAE train: 0.496000	val: 1.195142	test: 1.275028

Epoch: 61
Loss: 0.6033261269330978
RMSE train: 0.728665	val: 1.657913	test: 1.684178
MAE train: 0.573995	val: 1.317701	test: 1.317185

Epoch: 62
Loss: 0.5827977359294891
RMSE train: 0.624198	val: 1.691891	test: 1.687188
MAE train: 0.493969	val: 1.326988	test: 1.331681

Epoch: 63
Loss: 0.5555610656738281
RMSE train: 0.629732	val: 1.726720	test: 1.713966
MAE train: 0.500454	val: 1.335671	test: 1.355501

Epoch: 64
Loss: 0.5105108991265297
RMSE train: 0.655011	val: 1.589548	test: 1.605480
MAE train: 0.519266	val: 1.271177	test: 1.248363

Epoch: 65
Loss: 0.5187018439173698
RMSE train: 0.702564	val: 1.733261	test: 1.729428
MAE train: 0.551416	val: 1.387379	test: 1.325324

Epoch: 66
Loss: 0.5896701365709305
RMSE train: 0.630040	val: 1.732657	test: 1.702992
MAE train: 0.494808	val: 1.337177	test: 1.335995

Epoch: 67
Loss: 0.5578085780143738
RMSE train: 0.615032	val: 1.696933	test: 1.676381
MAE train: 0.481743	val: 1.293770	test: 1.351236

Epoch: 68
Loss: 0.5232409462332726
RMSE train: 0.682635	val: 1.655529	test: 1.662621
MAE train: 0.531290	val: 1.300519	test: 1.315397

Epoch: 69
Loss: 0.4910013899207115
RMSE train: 0.596387	val: 1.586014	test: 1.592130
MAE train: 0.463401	val: 1.248562	test: 1.263368

Epoch: 70
Loss: 0.48616814613342285
RMSE train: 0.525034	val: 1.555593	test: 1.563077
MAE train: 0.411789	val: 1.216085	test: 1.253662

Epoch: 71
Loss: 0.5140023231506348
RMSE train: 0.613194	val: 1.656577	test: 1.675526
MAE train: 0.481635	val: 1.269853	test: 1.340824

Epoch: 72
Loss: 0.4994712918996811
RMSE train: 0.602571	val: 1.545762	test: 1.571784
MAE train: 0.471831	val: 1.207386	test: 1.261287

Epoch: 73
Loss: 0.4675157889723778
RMSE train: 0.557327	val: 1.471553	test: 1.512693
MAE train: 0.438540	val: 1.138455	test: 1.258200

Epoch: 74
Loss: 0.48422251641750336
RMSE train: 0.589315	val: 1.457127	test: 1.509535
MAE train: 0.461983	val: 1.145538	test: 1.249527

Epoch: 75
Loss: 0.44366512447595596
RMSE train: 0.673458	val: 1.382428	test: 1.454011
MAE train: 0.523792	val: 1.105010	test: 1.179027

Epoch: 76
Loss: 0.49291905760765076
RMSE train: 0.700502	val: 1.439914	test: 1.532787
MAE train: 0.550037	val: 1.142419	test: 1.255271

Epoch: 77
Loss: 0.4309583380818367
RMSE train: 0.695200	val: 1.467260	test: 1.544823
MAE train: 0.550920	val: 1.171595	test: 1.253775

Epoch: 78
Loss: 0.4209442511200905
RMSE train: 0.637081	val: 1.416736	test: 1.520163
MAE train: 0.497345	val: 1.123905	test: 1.239999

Epoch: 79
Loss: 0.4260157570242882
RMSE train: 0.572326	val: 1.409204	test: 1.529725
MAE train: 0.437340	val: 1.079806	test: 1.263225

Epoch: 80
Loss: 0.4791955202817917
RMSE train: 0.635177	val: 1.537303	test: 1.594893
MAE train: 0.476542	val: 1.194132	test: 1.240939

Epoch: 81
Loss: 0.3854702040553093
RMSE train: 0.529552	val: 1.501756	test: 1.539954
MAE train: 0.403778	val: 1.200543	test: 1.214998

Epoch: 82
Loss: 0.45386505126953125
RMSE train: 0.481091	val: 1.447434	test: 1.512984
MAE train: 0.370352	val: 1.114413	test: 1.240577

Epoch: 83
Loss: 0.39318323880434036
RMSE train: 0.575964	val: 1.470846	test: 1.524909
MAE train: 0.792460	val: 1.133917	test: 1.286519

Epoch: 23
Loss: 1.5478383898735046
RMSE train: 0.990089	val: 1.495023	test: 1.548374
MAE train: 0.769006	val: 1.166109	test: 1.263231

Epoch: 24
Loss: 1.3118847608566284
RMSE train: 0.958950	val: 1.706708	test: 1.699246
MAE train: 0.747583	val: 1.351329	test: 1.439114

Epoch: 25
Loss: 1.2460942566394806
RMSE train: 0.941608	val: 1.618730	test: 1.652325
MAE train: 0.728068	val: 1.275245	test: 1.409794

Epoch: 26
Loss: 1.3294662833213806
RMSE train: 0.890459	val: 1.591081	test: 1.600773
MAE train: 0.686917	val: 1.254570	test: 1.356682

Epoch: 27
Loss: 1.2003339529037476
RMSE train: 0.885400	val: 1.391049	test: 1.451674
MAE train: 0.689592	val: 1.102383	test: 1.191401

Epoch: 28
Loss: 1.206368774175644
RMSE train: 0.931459	val: 1.530415	test: 1.593457
MAE train: 0.730915	val: 1.216733	test: 1.308285

Epoch: 29
Loss: 1.2122951447963715
RMSE train: 0.862335	val: 1.819834	test: 1.744565
MAE train: 0.667668	val: 1.450324	test: 1.468725

Epoch: 30
Loss: 1.0891730934381485
RMSE train: 0.874333	val: 1.502599	test: 1.529805
MAE train: 0.674881	val: 1.178720	test: 1.292112

Epoch: 31
Loss: 1.027597814798355
RMSE train: 0.810982	val: 1.488132	test: 1.519796
MAE train: 0.621872	val: 1.165685	test: 1.255252

Epoch: 32
Loss: 1.1052640676498413
RMSE train: 0.760184	val: 1.558597	test: 1.595225
MAE train: 0.583937	val: 1.239254	test: 1.328288

Epoch: 33
Loss: 1.0034681558609009
RMSE train: 0.786520	val: 1.469301	test: 1.561822
MAE train: 0.618963	val: 1.187319	test: 1.312937

Epoch: 34
Loss: 1.0331080704927444
RMSE train: 0.768612	val: 1.709339	test: 1.728703
MAE train: 0.605997	val: 1.368371	test: 1.461060

Epoch: 35
Loss: 1.0103101432323456
RMSE train: 0.729636	val: 1.769258	test: 1.705478
MAE train: 0.573468	val: 1.405522	test: 1.450615

Epoch: 36
Loss: 0.9179558455944061
RMSE train: 0.762728	val: 1.706508	test: 1.720440
MAE train: 0.600956	val: 1.357456	test: 1.454585

Epoch: 37
Loss: 1.0182607173919678
RMSE train: 0.708489	val: 1.926559	test: 1.776207
MAE train: 0.556041	val: 1.529905	test: 1.496914

Epoch: 38
Loss: 0.8714238256216049
RMSE train: 0.709107	val: 1.822886	test: 1.658973
MAE train: 0.559651	val: 1.453653	test: 1.411753

Epoch: 39
Loss: 0.9067245572805405
RMSE train: 0.668136	val: 1.629116	test: 1.606605
MAE train: 0.526219	val: 1.311220	test: 1.353376

Epoch: 40
Loss: 0.9198726564645767
RMSE train: 0.676503	val: 1.555156	test: 1.563983
MAE train: 0.523636	val: 1.253258	test: 1.324669

Epoch: 41
Loss: 0.7319414913654327
RMSE train: 0.723612	val: 1.546438	test: 1.483343
MAE train: 0.550493	val: 1.227067	test: 1.242701

Epoch: 42
Loss: 0.8916792869567871
RMSE train: 0.748870	val: 1.562329	test: 1.502464
MAE train: 0.573210	val: 1.236605	test: 1.255241

Epoch: 43
Loss: 0.8062213659286499
RMSE train: 0.691339	val: 1.343395	test: 1.385371
MAE train: 0.528261	val: 1.073256	test: 1.166197

Epoch: 44
Loss: 0.8353484272956848
RMSE train: 0.679480	val: 1.556833	test: 1.563570
MAE train: 0.523472	val: 1.243983	test: 1.308869

Epoch: 45
Loss: 0.7591884285211563
RMSE train: 0.712988	val: 1.474589	test: 1.482290
MAE train: 0.553503	val: 1.137718	test: 1.205119

Epoch: 46
Loss: 0.7648296505212784
RMSE train: 0.713042	val: 1.573210	test: 1.586803
MAE train: 0.549746	val: 1.215179	test: 1.292623

Epoch: 47
Loss: 0.6795200705528259
RMSE train: 0.621393	val: 1.555221	test: 1.578213
MAE train: 0.483964	val: 1.234634	test: 1.314319

Epoch: 48
Loss: 0.6354483962059021
RMSE train: 0.659450	val: 1.479765	test: 1.455224
MAE train: 0.516829	val: 1.155960	test: 1.176380

Epoch: 49
Loss: 0.5786183625459671
RMSE train: 0.647988	val: 1.483598	test: 1.439944
MAE train: 0.507199	val: 1.165642	test: 1.155686

Epoch: 50
Loss: 0.660465657711029
RMSE train: 0.619009	val: 1.574421	test: 1.525222
MAE train: 0.482444	val: 1.256275	test: 1.274021

Epoch: 51
Loss: 0.6357434689998627
RMSE train: 0.639554	val: 1.454894	test: 1.439124
MAE train: 0.499393	val: 1.147605	test: 1.187778

Epoch: 52
Loss: 0.5529632270336151
RMSE train: 0.617439	val: 1.304332	test: 1.348356
MAE train: 0.481332	val: 1.034283	test: 1.067227

Epoch: 53
Loss: 0.6123293191194534
RMSE train: 0.541702	val: 1.406081	test: 1.442528
MAE train: 0.426101	val: 1.118793	test: 1.205179

Epoch: 54
Loss: 0.5177523121237755
RMSE train: 0.568323	val: 1.290494	test: 1.365663
MAE train: 0.442660	val: 1.041051	test: 1.126462

Epoch: 55
Loss: 0.7274034321308136
RMSE train: 0.612145	val: 1.444889	test: 1.474267
MAE train: 0.477380	val: 1.139423	test: 1.191908

Epoch: 56
Loss: 0.5723685920238495
RMSE train: 0.575537	val: 1.455150	test: 1.520417
MAE train: 0.452829	val: 1.156379	test: 1.240581

Epoch: 57
Loss: 0.6239063590764999
RMSE train: 0.548105	val: 1.582973	test: 1.611212
MAE train: 0.431293	val: 1.258391	test: 1.328621

Epoch: 58
Loss: 0.532621294260025
RMSE train: 0.527010	val: 1.505043	test: 1.509898
MAE train: 0.415587	val: 1.208903	test: 1.260046

Epoch: 59
Loss: 0.5747832804918289
RMSE train: 0.516234	val: 1.512635	test: 1.562798
MAE train: 0.409274	val: 1.222640	test: 1.315464

Epoch: 60
Loss: 0.4986013397574425
RMSE train: 0.557843	val: 1.406016	test: 1.512305
MAE train: 0.440205	val: 1.124895	test: 1.258653

Epoch: 61
Loss: 0.5284972861409187
RMSE train: 0.553452	val: 1.311266	test: 1.387577
MAE train: 0.441926	val: 1.051920	test: 1.145004

Epoch: 62
Loss: 0.5767042636871338
RMSE train: 0.503966	val: 1.467310	test: 1.558047
MAE train: 0.401376	val: 1.180514	test: 1.310567

Epoch: 63
Loss: 0.525465838611126
RMSE train: 0.484438	val: 1.396828	test: 1.492009
MAE train: 0.388204	val: 1.132955	test: 1.255909

Epoch: 64
Loss: 0.5475145056843758
RMSE train: 0.511417	val: 1.284219	test: 1.442237
MAE train: 0.408894	val: 1.044949	test: 1.191946

Epoch: 65
Loss: 0.4943891167640686
RMSE train: 0.523544	val: 1.423621	test: 1.474621
MAE train: 0.417859	val: 1.144573	test: 1.228284

Epoch: 66
Loss: 0.5080792456865311
RMSE train: 0.513193	val: 1.331893	test: 1.387435
MAE train: 0.413714	val: 1.084184	test: 1.159227

Epoch: 67
Loss: 0.45389020442962646
RMSE train: 0.486289	val: 1.330254	test: 1.456448
MAE train: 0.384237	val: 1.080701	test: 1.204882

Epoch: 68
Loss: 0.44612523168325424
RMSE train: 0.523596	val: 1.389108	test: 1.479284
MAE train: 0.417105	val: 1.101473	test: 1.213053

Epoch: 69
Loss: 0.4682948887348175
RMSE train: 0.582835	val: 1.428187	test: 1.483454
MAE train: 0.468793	val: 1.119807	test: 1.191469

Epoch: 70
Loss: 0.43797945976257324
RMSE train: 0.518545	val: 1.366414	test: 1.466258
MAE train: 0.413277	val: 1.099126	test: 1.208961

Epoch: 71
Loss: 0.5012975260615349
RMSE train: 0.504517	val: 1.439426	test: 1.495085
MAE train: 0.392770	val: 1.165635	test: 1.231331

Epoch: 72
Loss: 0.46920429170131683
RMSE train: 0.515870	val: 1.370330	test: 1.390604
MAE train: 0.394159	val: 1.090292	test: 1.136821

Epoch: 73
Loss: 0.4688183665275574
RMSE train: 0.495716	val: 1.500923	test: 1.482842
MAE train: 0.393711	val: 1.195408	test: 1.227276

Epoch: 74
Loss: 0.4774671494960785
RMSE train: 0.490484	val: 1.493874	test: 1.495983
MAE train: 0.396008	val: 1.204906	test: 1.242295

Epoch: 75
Loss: 0.4602692276239395
RMSE train: 0.513351	val: 1.314838	test: 1.410869
MAE train: 0.409625	val: 1.051295	test: 1.173518

Epoch: 76
Loss: 0.4149060323834419
RMSE train: 0.523905	val: 1.394501	test: 1.470529
MAE train: 0.413330	val: 1.091485	test: 1.183728

Epoch: 77
Loss: 0.39780842512845993
RMSE train: 0.631001	val: 1.657439	test: 1.683189
MAE train: 0.486599	val: 1.282212	test: 1.358577

Epoch: 78
Loss: 0.43218134343624115
RMSE train: 0.496980	val: 1.243435	test: 1.365088
MAE train: 0.390188	val: 1.009878	test: 1.125882

Epoch: 79
Loss: 0.36764174699783325
RMSE train: 0.475778	val: 1.262080	test: 1.401163
MAE train: 0.374092	val: 1.024195	test: 1.172246

Epoch: 80
Loss: 0.41525691747665405
RMSE train: 0.503984	val: 1.414549	test: 1.474577
MAE train: 0.395611	val: 1.127381	test: 1.226938

Epoch: 81
Loss: 0.44647593796253204
RMSE train: 0.561446	val: 1.212258	test: 1.324385
MAE train: 0.435240	val: 0.982694	test: 1.063573

Epoch: 82
Loss: 0.42656557261943817
RMSE train: 0.480795	val: 1.349381	test: 1.465296
MAE train: 0.376036	val: 1.087520	test: 1.222535

Epoch: 83
Loss: 0.416074201464653
RMSE train: 0.570200	val: 1.387647	test: 1.493185
MAE train: 0.725599	val: 1.344027	test: 1.313352

Epoch: 23
Loss: 1.3802713751792908
RMSE train: 0.961093	val: 1.641603	test: 1.638591
MAE train: 0.737696	val: 1.334083	test: 1.327328

Epoch: 24
Loss: 1.267797738313675
RMSE train: 0.869498	val: 1.504481	test: 1.459898
MAE train: 0.672265	val: 1.221366	test: 1.164518

Epoch: 25
Loss: 1.2206833064556122
RMSE train: 0.884373	val: 1.511013	test: 1.455981
MAE train: 0.697230	val: 1.224592	test: 1.154440

Epoch: 26
Loss: 1.066579818725586
RMSE train: 0.833169	val: 1.599758	test: 1.501037
MAE train: 0.654917	val: 1.256513	test: 1.203152

Epoch: 27
Loss: 1.0939429104328156
RMSE train: 0.883791	val: 1.682724	test: 1.623292
MAE train: 0.692967	val: 1.324415	test: 1.318189

Epoch: 28
Loss: 1.0098093301057816
RMSE train: 0.871610	val: 1.746110	test: 1.671178
MAE train: 0.685007	val: 1.380610	test: 1.359639

Epoch: 29
Loss: 0.9864509552717209
RMSE train: 0.857634	val: 1.738083	test: 1.665804
MAE train: 0.671901	val: 1.384771	test: 1.341802

Epoch: 30
Loss: 0.9513130038976669
RMSE train: 0.757522	val: 1.563335	test: 1.501082
MAE train: 0.592163	val: 1.256758	test: 1.179029

Epoch: 31
Loss: 0.9110386818647385
RMSE train: 0.730260	val: 1.524305	test: 1.472990
MAE train: 0.567765	val: 1.218117	test: 1.149323

Epoch: 32
Loss: 0.9488924145698547
RMSE train: 0.753568	val: 1.539070	test: 1.527574
MAE train: 0.584642	val: 1.234696	test: 1.194844

Epoch: 33
Loss: 0.9252286106348038
RMSE train: 0.686734	val: 1.530219	test: 1.507865
MAE train: 0.534693	val: 1.237343	test: 1.194218

Epoch: 34
Loss: 0.9016115069389343
RMSE train: 0.685475	val: 1.627317	test: 1.585955
MAE train: 0.537075	val: 1.286491	test: 1.302222

Epoch: 35
Loss: 0.8284337818622589
RMSE train: 0.684138	val: 1.526263	test: 1.495977
MAE train: 0.529778	val: 1.208924	test: 1.206337

Epoch: 36
Loss: 0.8419413566589355
RMSE train: 0.695861	val: 1.529362	test: 1.493982
MAE train: 0.545562	val: 1.210862	test: 1.189078

Epoch: 37
Loss: 0.8825821727514267
RMSE train: 0.698262	val: 1.724894	test: 1.605394
MAE train: 0.546467	val: 1.352465	test: 1.284102

Epoch: 38
Loss: 0.7847005426883698
RMSE train: 0.675940	val: 1.567093	test: 1.522638
MAE train: 0.529792	val: 1.236855	test: 1.186226

Epoch: 39
Loss: 0.7562952786684036
RMSE train: 0.811508	val: 1.741996	test: 1.732547
MAE train: 0.639734	val: 1.360777	test: 1.373567

Epoch: 40
Loss: 0.7407005280256271
RMSE train: 0.759409	val: 1.732367	test: 1.709571
MAE train: 0.599802	val: 1.351440	test: 1.353050

Epoch: 41
Loss: 0.7689293026924133
RMSE train: 0.725762	val: 1.699049	test: 1.665886
MAE train: 0.571060	val: 1.334468	test: 1.323707

Epoch: 42
Loss: 0.731912836432457
RMSE train: 0.751441	val: 1.663219	test: 1.643977
MAE train: 0.588815	val: 1.307275	test: 1.297496

Epoch: 43
Loss: 0.6801499426364899
RMSE train: 0.629014	val: 1.477667	test: 1.462274
MAE train: 0.489135	val: 1.182940	test: 1.161649

Epoch: 44
Loss: 0.728643462061882
RMSE train: 0.581986	val: 1.473749	test: 1.437098
MAE train: 0.454581	val: 1.185492	test: 1.147552

Epoch: 45
Loss: 0.6359328180551529
RMSE train: 0.653880	val: 1.614917	test: 1.577100
MAE train: 0.514836	val: 1.284666	test: 1.215641

Epoch: 46
Loss: 0.6822660118341446
RMSE train: 0.646882	val: 1.576118	test: 1.527072
MAE train: 0.511973	val: 1.247591	test: 1.170747

Epoch: 47
Loss: 0.7194532454013824
RMSE train: 0.653125	val: 1.528905	test: 1.502254
MAE train: 0.516995	val: 1.229726	test: 1.177313

Epoch: 48
Loss: 0.6782386600971222
RMSE train: 0.728058	val: 1.638770	test: 1.610558
MAE train: 0.574642	val: 1.302686	test: 1.268428

Epoch: 49
Loss: 0.6127148270606995
RMSE train: 0.862367	val: 1.881759	test: 1.883465
MAE train: 0.685936	val: 1.523286	test: 1.535780

Epoch: 50
Loss: 0.6969395577907562
RMSE train: 0.669068	val: 1.605359	test: 1.585122
MAE train: 0.524454	val: 1.276633	test: 1.222463

Epoch: 51
Loss: 0.5541733950376511
RMSE train: 0.588492	val: 1.503000	test: 1.441620
MAE train: 0.458202	val: 1.183059	test: 1.138173

Epoch: 52
Loss: 0.5561006292700768
RMSE train: 0.623478	val: 1.648822	test: 1.602328
MAE train: 0.495456	val: 1.310419	test: 1.249686

Epoch: 53
Loss: 0.5870830863714218
RMSE train: 0.653926	val: 1.696110	test: 1.648559
MAE train: 0.518330	val: 1.365127	test: 1.286693

Epoch: 54
Loss: 0.5420353412628174
RMSE train: 0.536893	val: 1.565312	test: 1.487724
MAE train: 0.420679	val: 1.266343	test: 1.158336

Epoch: 55
Loss: 0.5202365666627884
RMSE train: 0.574319	val: 1.589169	test: 1.537770
MAE train: 0.452036	val: 1.302790	test: 1.216882

Epoch: 56
Loss: 0.5159626230597496
RMSE train: 0.656150	val: 1.682164	test: 1.643244
MAE train: 0.513750	val: 1.352009	test: 1.302583

Epoch: 57
Loss: 0.540270633995533
RMSE train: 0.637215	val: 1.641763	test: 1.614246
MAE train: 0.501736	val: 1.295728	test: 1.269594

Epoch: 58
Loss: 0.5143798589706421
RMSE train: 0.739012	val: 1.780844	test: 1.768350
MAE train: 0.574290	val: 1.418466	test: 1.408387

Epoch: 59
Loss: 0.5626145005226135
RMSE train: 0.657693	val: 1.640491	test: 1.602401
MAE train: 0.512641	val: 1.291864	test: 1.260949

Epoch: 60
Loss: 0.5984133630990982
RMSE train: 0.517787	val: 1.529923	test: 1.467115
MAE train: 0.407224	val: 1.235234	test: 1.157858

Epoch: 61
Loss: 0.5318177789449692
RMSE train: 0.578055	val: 1.584992	test: 1.541583
MAE train: 0.456350	val: 1.282379	test: 1.221778

Epoch: 62
Loss: 0.49255049228668213
RMSE train: 0.632801	val: 1.661563	test: 1.609185
MAE train: 0.496585	val: 1.326436	test: 1.264298

Epoch: 63
Loss: 0.5344448089599609
RMSE train: 0.585966	val: 1.499026	test: 1.469321
MAE train: 0.459623	val: 1.230648	test: 1.150068

Epoch: 64
Loss: 0.49343205988407135
RMSE train: 0.554838	val: 1.429383	test: 1.461162
MAE train: 0.437770	val: 1.188987	test: 1.157801

Epoch: 65
Loss: 0.4704308807849884
RMSE train: 0.624514	val: 1.568085	test: 1.589545
MAE train: 0.486811	val: 1.246375	test: 1.218419

Epoch: 66
Loss: 0.4664066284894943
RMSE train: 0.581180	val: 1.584523	test: 1.588643
MAE train: 0.450999	val: 1.244800	test: 1.228036

Epoch: 67
Loss: 0.4782553017139435
RMSE train: 0.550400	val: 1.504624	test: 1.526872
MAE train: 0.425896	val: 1.213167	test: 1.191415

Epoch: 68
Loss: 0.44215135276317596
RMSE train: 0.620824	val: 1.600373	test: 1.573529
MAE train: 0.480359	val: 1.250034	test: 1.222667

Epoch: 69
Loss: 0.42048027366399765
RMSE train: 0.531153	val: 1.532559	test: 1.508796
MAE train: 0.417696	val: 1.229938	test: 1.176449

Epoch: 70
Loss: 0.44599274545907974
RMSE train: 0.505771	val: 1.468119	test: 1.462674
MAE train: 0.393805	val: 1.204618	test: 1.144876

Epoch: 71
Loss: 0.5043665245175362
RMSE train: 0.641469	val: 1.613821	test: 1.549027
MAE train: 0.493984	val: 1.251074	test: 1.202056

Epoch: 72
Loss: 0.46462568640708923
RMSE train: 0.558547	val: 1.533593	test: 1.493744
MAE train: 0.438132	val: 1.220732	test: 1.146666

Epoch: 73
Loss: 0.429237999022007
RMSE train: 0.534162	val: 1.501335	test: 1.487639
MAE train: 0.422152	val: 1.221819	test: 1.165534

Epoch: 74
Loss: 0.4155218154191971
RMSE train: 0.546811	val: 1.598501	test: 1.556247
MAE train: 0.434142	val: 1.277987	test: 1.221432

Epoch: 75
Loss: 0.43401844799518585
RMSE train: 0.673238	val: 1.677538	test: 1.662376
MAE train: 0.541172	val: 1.347336	test: 1.280025

Epoch: 76
Loss: 0.4189240485429764
RMSE train: 0.651513	val: 1.644663	test: 1.638672
MAE train: 0.520132	val: 1.323177	test: 1.256985

Epoch: 77
Loss: 0.3914400413632393
RMSE train: 0.604304	val: 1.645816	test: 1.635139
MAE train: 0.481203	val: 1.322877	test: 1.260645

Epoch: 78
Loss: 0.3690567836165428
RMSE train: 0.577686	val: 1.618425	test: 1.604671
MAE train: 0.454143	val: 1.299413	test: 1.240962

Epoch: 79
Loss: 0.39598118513822556
RMSE train: 0.489351	val: 1.497731	test: 1.494489
MAE train: 0.386358	val: 1.220842	test: 1.161446

Epoch: 80
Loss: 0.44768041372299194
RMSE train: 0.513816	val: 1.519426	test: 1.524138
MAE train: 0.403435	val: 1.237862	test: 1.189388

Epoch: 81
Loss: 0.361150398850441
RMSE train: 0.533662	val: 1.544435	test: 1.532118
MAE train: 0.412598	val: 1.273166	test: 1.192871

Epoch: 82
Loss: 0.36700862646102905
RMSE train: 0.449052	val: 1.490223	test: 1.443354
MAE train: 0.354156	val: 1.246251	test: 1.127396

Epoch: 83
Loss: 0.3716423809528351
RMSE train: 0.463151	val: 1.550538	test: 1.473109
MAE train: 0.675835	val: 1.063335	test: 1.162923

Epoch: 23
Loss: 1.3236452341079712
RMSE train: 0.905101	val: 1.305655	test: 1.453774
MAE train: 0.718143	val: 1.044932	test: 1.152827

Epoch: 24
Loss: 1.1385955214500427
RMSE train: 0.872741	val: 1.524282	test: 1.638953
MAE train: 0.694989	val: 1.198208	test: 1.357917

Epoch: 25
Loss: 1.048926204442978
RMSE train: 0.864608	val: 1.433333	test: 1.593914
MAE train: 0.691357	val: 1.143486	test: 1.315797

Epoch: 26
Loss: 1.105603814125061
RMSE train: 0.853986	val: 1.391210	test: 1.577437
MAE train: 0.680969	val: 1.092024	test: 1.304614

Epoch: 27
Loss: 1.0351078808307648
RMSE train: 0.833678	val: 1.382592	test: 1.533336
MAE train: 0.658391	val: 1.076891	test: 1.253211

Epoch: 28
Loss: 1.0451221615076065
RMSE train: 0.840838	val: 1.341150	test: 1.519296
MAE train: 0.665741	val: 1.067992	test: 1.220641

Epoch: 29
Loss: 1.0327695310115814
RMSE train: 0.811060	val: 1.494249	test: 1.632156
MAE train: 0.630993	val: 1.159928	test: 1.317143

Epoch: 30
Loss: 0.9739945828914642
RMSE train: 0.757252	val: 1.403806	test: 1.617582
MAE train: 0.594539	val: 1.098611	test: 1.302752

Epoch: 31
Loss: 0.8874576091766357
RMSE train: 0.736198	val: 1.360704	test: 1.588543
MAE train: 0.578952	val: 1.072440	test: 1.271665

Epoch: 32
Loss: 0.9954672455787659
RMSE train: 0.699044	val: 1.451516	test: 1.639579
MAE train: 0.543456	val: 1.146809	test: 1.345095

Epoch: 33
Loss: 0.8916192352771759
RMSE train: 0.724933	val: 1.306261	test: 1.453870
MAE train: 0.567223	val: 1.039369	test: 1.129337

Epoch: 34
Loss: 0.8445143401622772
RMSE train: 0.756517	val: 1.568830	test: 1.701048
MAE train: 0.597511	val: 1.238261	test: 1.345446

Epoch: 35
Loss: 0.9019741266965866
RMSE train: 0.694314	val: 1.352837	test: 1.617268
MAE train: 0.547187	val: 1.054276	test: 1.259487

Epoch: 36
Loss: 0.8353573679924011
RMSE train: 0.710041	val: 1.254981	test: 1.517173
MAE train: 0.555410	val: 1.012086	test: 1.194117

Epoch: 37
Loss: 0.8497034460306168
RMSE train: 0.658523	val: 1.390498	test: 1.635806
MAE train: 0.506757	val: 1.091646	test: 1.322117

Epoch: 38
Loss: 0.7582900822162628
RMSE train: 0.634895	val: 1.335001	test: 1.595431
MAE train: 0.492840	val: 1.058017	test: 1.273768

Epoch: 39
Loss: 0.7386753857135773
RMSE train: 0.623445	val: 1.324605	test: 1.631058
MAE train: 0.478904	val: 1.037709	test: 1.274861

Epoch: 40
Loss: 0.7966646701097488
RMSE train: 0.660244	val: 1.407304	test: 1.649755
MAE train: 0.509548	val: 1.076846	test: 1.296798

Epoch: 41
Loss: 0.705694705247879
RMSE train: 0.654207	val: 1.364918	test: 1.539993
MAE train: 0.498917	val: 1.062354	test: 1.207681

Epoch: 42
Loss: 0.8003610521554947
RMSE train: 0.624954	val: 1.320071	test: 1.486939
MAE train: 0.478259	val: 1.035590	test: 1.172768

Epoch: 43
Loss: 0.7483873069286346
RMSE train: 0.663817	val: 1.293242	test: 1.427902
MAE train: 0.517163	val: 1.036907	test: 1.127343

Epoch: 44
Loss: 0.7432021796703339
RMSE train: 0.666264	val: 1.419140	test: 1.568280
MAE train: 0.515547	val: 1.118857	test: 1.235962

Epoch: 45
Loss: 0.7193442434072495
RMSE train: 0.643092	val: 1.305297	test: 1.472081
MAE train: 0.494698	val: 1.047128	test: 1.142482

Epoch: 46
Loss: 0.693497508764267
RMSE train: 0.623868	val: 1.322087	test: 1.522779
MAE train: 0.486629	val: 1.042772	test: 1.187121

Epoch: 47
Loss: 0.6443136930465698
RMSE train: 0.585972	val: 1.251142	test: 1.462285
MAE train: 0.457220	val: 1.006494	test: 1.137019

Epoch: 48
Loss: 0.6000319123268127
RMSE train: 0.588324	val: 1.255817	test: 1.486541
MAE train: 0.457729	val: 1.018847	test: 1.157970

Epoch: 49
Loss: 0.5893534272909164
RMSE train: 0.603819	val: 1.338537	test: 1.522217
MAE train: 0.470360	val: 1.048627	test: 1.178955

Epoch: 50
Loss: 0.5888625830411911
RMSE train: 0.597884	val: 1.248278	test: 1.421663
MAE train: 0.468327	val: 0.994708	test: 1.100696

Epoch: 51
Loss: 0.5939134210348129
RMSE train: 0.591156	val: 1.255668	test: 1.401947
MAE train: 0.464492	val: 0.990336	test: 1.085235

Epoch: 52
Loss: 0.5275492742657661
RMSE train: 0.600278	val: 1.306038	test: 1.449849
MAE train: 0.472572	val: 1.030152	test: 1.108985

Epoch: 53
Loss: 0.5518927574157715
RMSE train: 0.579490	val: 1.348352	test: 1.497918
MAE train: 0.453446	val: 1.088888	test: 1.153415

Epoch: 54
Loss: 0.5178241431713104
RMSE train: 0.534934	val: 1.253651	test: 1.427583
MAE train: 0.417723	val: 1.031650	test: 1.102588

Epoch: 55
Loss: 0.6447377800941467
RMSE train: 0.564775	val: 1.313276	test: 1.496173
MAE train: 0.442275	val: 1.041479	test: 1.146869

Epoch: 56
Loss: 0.5525413081049919
RMSE train: 0.586037	val: 1.309399	test: 1.486388
MAE train: 0.460955	val: 1.072774	test: 1.136543

Epoch: 57
Loss: 0.6236310005187988
RMSE train: 0.631452	val: 1.388571	test: 1.543004
MAE train: 0.492263	val: 1.122400	test: 1.185034

Epoch: 58
Loss: 0.5180188864469528
RMSE train: 0.526815	val: 1.263422	test: 1.414835
MAE train: 0.404646	val: 1.030523	test: 1.095589

Epoch: 59
Loss: 0.5336568206548691
RMSE train: 0.518129	val: 1.255028	test: 1.434644
MAE train: 0.405795	val: 1.000283	test: 1.121558

Epoch: 60
Loss: 0.481177382171154
RMSE train: 0.642273	val: 1.317266	test: 1.468011
MAE train: 0.486522	val: 1.041047	test: 1.120045

Epoch: 61
Loss: 0.5080442577600479
RMSE train: 0.609965	val: 1.325501	test: 1.437348
MAE train: 0.467636	val: 1.075879	test: 1.113667

Epoch: 62
Loss: 0.5230570957064629
RMSE train: 0.511140	val: 1.285441	test: 1.412650
MAE train: 0.396074	val: 1.003153	test: 1.096217

Epoch: 63
Loss: 0.5023194998502731
RMSE train: 0.564568	val: 1.220899	test: 1.323939
MAE train: 0.434912	val: 0.977874	test: 1.009362

Epoch: 64
Loss: 0.5077891796827316
RMSE train: 0.498632	val: 1.198996	test: 1.295541
MAE train: 0.393684	val: 0.949630	test: 0.995834

Epoch: 65
Loss: 0.5089173018932343
RMSE train: 0.465706	val: 1.199505	test: 1.306343
MAE train: 0.365688	val: 0.967435	test: 1.006823

Epoch: 66
Loss: 0.4295666590332985
RMSE train: 0.539234	val: 1.251747	test: 1.363008
MAE train: 0.422699	val: 1.008014	test: 1.051375

Epoch: 67
Loss: 0.4768042638897896
RMSE train: 0.545548	val: 1.295744	test: 1.451700
MAE train: 0.424280	val: 1.023375	test: 1.120306

Epoch: 68
Loss: 0.48450665920972824
RMSE train: 0.518598	val: 1.228694	test: 1.387438
MAE train: 0.404212	val: 1.001561	test: 1.075562

Epoch: 69
Loss: 0.5282695889472961
RMSE train: 0.505140	val: 1.215429	test: 1.365357
MAE train: 0.393646	val: 0.977370	test: 1.061201

Epoch: 70
Loss: 0.3913528770208359
RMSE train: 0.485655	val: 1.244885	test: 1.406395
MAE train: 0.376567	val: 0.984972	test: 1.095429

Epoch: 71
Loss: 0.45270927250385284
RMSE train: 0.506227	val: 1.276491	test: 1.423127
MAE train: 0.391280	val: 1.030383	test: 1.102350

Epoch: 72
Loss: 0.44413629174232483
RMSE train: 0.536788	val: 1.367407	test: 1.490760
MAE train: 0.412211	val: 1.088196	test: 1.143736

Epoch: 73
Loss: 0.41354769468307495
RMSE train: 0.540893	val: 1.400744	test: 1.498972
MAE train: 0.416254	val: 1.080355	test: 1.147053

Epoch: 74
Loss: 0.435178741812706
RMSE train: 0.515950	val: 1.212042	test: 1.322155
MAE train: 0.405069	val: 0.974907	test: 1.010209

Epoch: 75
Loss: 0.41669975221157074
RMSE train: 0.468125	val: 1.209457	test: 1.295110
MAE train: 0.375689	val: 0.942617	test: 1.035881

Epoch: 76
Loss: 0.4142631068825722
RMSE train: 0.451686	val: 1.248284	test: 1.323724
MAE train: 0.355989	val: 0.973747	test: 1.035154

Epoch: 77
Loss: 0.3869231939315796
RMSE train: 0.547967	val: 1.338502	test: 1.414176
MAE train: 0.426730	val: 1.072578	test: 1.087367

Epoch: 78
Loss: 0.46187061816453934
RMSE train: 0.463547	val: 1.224320	test: 1.358753
MAE train: 0.370773	val: 0.971108	test: 1.064208

Epoch: 79
Loss: 0.3772134333848953
RMSE train: 0.496868	val: 1.257465	test: 1.378631
MAE train: 0.383391	val: 1.020784	test: 1.053142

Epoch: 80
Loss: 0.3967585414648056
RMSE train: 0.550739	val: 1.382940	test: 1.494830
MAE train: 0.417189	val: 1.069360	test: 1.131848

Epoch: 81
Loss: 0.4125804156064987
RMSE train: 0.530846	val: 1.219757	test: 1.357135
MAE train: 0.410054	val: 0.985367	test: 1.052285

Epoch: 82
Loss: 0.4160551279783249
RMSE train: 0.432923	val: 1.192309	test: 1.348288
MAE train: 0.343901	val: 0.946763	test: 1.066341

Epoch: 83
Loss: 0.41734642535448074
RMSE train: 0.526826	val: 1.251734	test: 1.384261
MAE train: 0.780693	val: 1.082916	test: 1.290506

Epoch: 23
Loss: 1.40563103556633
RMSE train: 0.988235	val: 1.407300	test: 1.559608
MAE train: 0.794331	val: 1.123184	test: 1.287068

Epoch: 24
Loss: 1.3948040902614594
RMSE train: 0.950557	val: 1.456832	test: 1.602029
MAE train: 0.761407	val: 1.142029	test: 1.314334

Epoch: 25
Loss: 1.232638269662857
RMSE train: 0.883333	val: 1.433725	test: 1.540900
MAE train: 0.707666	val: 1.107868	test: 1.247848

Epoch: 26
Loss: 1.1699632108211517
RMSE train: 0.877628	val: 1.478024	test: 1.522560
MAE train: 0.701448	val: 1.130536	test: 1.244621

Epoch: 27
Loss: 1.1721932590007782
RMSE train: 0.879274	val: 1.435681	test: 1.539568
MAE train: 0.701721	val: 1.110798	test: 1.261832

Epoch: 28
Loss: 1.169231116771698
RMSE train: 0.847509	val: 1.427134	test: 1.501952
MAE train: 0.678157	val: 1.116062	test: 1.228149

Epoch: 29
Loss: 1.141074389219284
RMSE train: 0.791412	val: 1.423646	test: 1.513957
MAE train: 0.630084	val: 1.106570	test: 1.216257

Epoch: 30
Loss: 0.9940522760152817
RMSE train: 0.812464	val: 1.479214	test: 1.517027
MAE train: 0.644901	val: 1.131160	test: 1.231667

Epoch: 31
Loss: 1.0016613453626633
RMSE train: 0.804957	val: 1.458141	test: 1.519974
MAE train: 0.647174	val: 1.175242	test: 1.223432

Epoch: 32
Loss: 0.9657062292098999
RMSE train: 0.773694	val: 1.326398	test: 1.468897
MAE train: 0.613795	val: 1.068817	test: 1.190190

Epoch: 33
Loss: 1.0203310996294022
RMSE train: 0.785411	val: 1.379570	test: 1.435586
MAE train: 0.614571	val: 1.064212	test: 1.146612

Epoch: 34
Loss: 0.8463005572557449
RMSE train: 0.747939	val: 1.368055	test: 1.398006
MAE train: 0.596311	val: 1.047719	test: 1.108210

Epoch: 35
Loss: 1.0288407057523727
RMSE train: 0.709418	val: 1.383777	test: 1.444210
MAE train: 0.571561	val: 1.078181	test: 1.172782

Epoch: 36
Loss: 0.8615428954362869
RMSE train: 0.738746	val: 1.519597	test: 1.584907
MAE train: 0.582980	val: 1.147334	test: 1.286891

Epoch: 37
Loss: 0.9331420212984085
RMSE train: 0.768028	val: 1.581559	test: 1.626340
MAE train: 0.605043	val: 1.232391	test: 1.275400

Epoch: 38
Loss: 0.818558394908905
RMSE train: 0.690614	val: 1.528161	test: 1.555675
MAE train: 0.538616	val: 1.162967	test: 1.232613

Epoch: 39
Loss: 0.9194299876689911
RMSE train: 0.660236	val: 1.429293	test: 1.459349
MAE train: 0.526887	val: 1.115718	test: 1.166219

Epoch: 40
Loss: 0.8302303105592728
RMSE train: 0.680353	val: 1.434176	test: 1.465863
MAE train: 0.539957	val: 1.119790	test: 1.173069

Epoch: 41
Loss: 0.8059584349393845
RMSE train: 0.667736	val: 1.395063	test: 1.469327
MAE train: 0.529802	val: 1.101077	test: 1.199022

Epoch: 42
Loss: 0.8436622321605682
RMSE train: 0.631133	val: 1.412519	test: 1.472255
MAE train: 0.501572	val: 1.094015	test: 1.193490

Epoch: 43
Loss: 0.7124455720186234
RMSE train: 0.659902	val: 1.430213	test: 1.470368
MAE train: 0.522845	val: 1.125593	test: 1.164062

Epoch: 44
Loss: 0.7196965515613556
RMSE train: 0.632626	val: 1.323710	test: 1.393860
MAE train: 0.497548	val: 1.048369	test: 1.103670

Epoch: 45
Loss: 0.7405982464551926
RMSE train: 0.663018	val: 1.347654	test: 1.400453
MAE train: 0.517351	val: 1.057775	test: 1.109986

Epoch: 46
Loss: 0.6541355848312378
RMSE train: 0.614047	val: 1.368386	test: 1.444273
MAE train: 0.482497	val: 1.071622	test: 1.173939

Epoch: 47
Loss: 0.6636748909950256
RMSE train: 0.594412	val: 1.354727	test: 1.441839
MAE train: 0.471919	val: 1.076448	test: 1.170499

Epoch: 48
Loss: 0.7094114869832993
RMSE train: 0.573604	val: 1.342919	test: 1.415485
MAE train: 0.456181	val: 1.068029	test: 1.155672

Epoch: 49
Loss: 0.5516651719808578
RMSE train: 0.591157	val: 1.328904	test: 1.358835
MAE train: 0.466360	val: 1.059950	test: 1.093150

Epoch: 50
Loss: 0.636180967092514
RMSE train: 0.623659	val: 1.391472	test: 1.406458
MAE train: 0.492423	val: 1.089232	test: 1.121531

Epoch: 51
Loss: 0.5859670490026474
RMSE train: 0.623055	val: 1.420369	test: 1.448298
MAE train: 0.496610	val: 1.099460	test: 1.166001

Epoch: 52
Loss: 0.6381014436483383
RMSE train: 0.614199	val: 1.406191	test: 1.449411
MAE train: 0.489424	val: 1.096370	test: 1.133827

Epoch: 53
Loss: 0.6718223243951797
RMSE train: 0.653187	val: 1.420647	test: 1.469775
MAE train: 0.523982	val: 1.134575	test: 1.129011

Epoch: 54
Loss: 0.6669123470783234
RMSE train: 0.556542	val: 1.294141	test: 1.369821
MAE train: 0.441203	val: 1.026088	test: 1.060270

Epoch: 55
Loss: 0.5577453374862671
RMSE train: 0.594295	val: 1.374244	test: 1.430240
MAE train: 0.461198	val: 1.066796	test: 1.110847

Epoch: 56
Loss: 0.5913389101624489
RMSE train: 0.632932	val: 1.431633	test: 1.462963
MAE train: 0.491265	val: 1.128452	test: 1.135383

Epoch: 57
Loss: 0.5720632523298264
RMSE train: 0.551549	val: 1.344671	test: 1.381383
MAE train: 0.436735	val: 1.054292	test: 1.098522

Epoch: 58
Loss: 0.5676885098218918
RMSE train: 0.557938	val: 1.313108	test: 1.389524
MAE train: 0.447199	val: 1.046631	test: 1.121285

Epoch: 59
Loss: 0.5355992540717125
RMSE train: 0.597213	val: 1.382065	test: 1.431726
MAE train: 0.473799	val: 1.071451	test: 1.133930

Epoch: 60
Loss: 0.5544654577970505
RMSE train: 0.558694	val: 1.406042	test: 1.450346
MAE train: 0.449368	val: 1.090930	test: 1.162658

Epoch: 61
Loss: 0.4991037994623184
RMSE train: 0.528322	val: 1.348547	test: 1.407210
MAE train: 0.426910	val: 1.077329	test: 1.142349

Epoch: 62
Loss: 0.5255110710859299
RMSE train: 0.545333	val: 1.395407	test: 1.420860
MAE train: 0.443348	val: 1.116391	test: 1.104664

Epoch: 63
Loss: 0.5208343863487244
RMSE train: 0.585928	val: 1.420048	test: 1.443502
MAE train: 0.472600	val: 1.115001	test: 1.125709

Epoch: 64
Loss: 0.5014211684465408
RMSE train: 0.523207	val: 1.257565	test: 1.322459
MAE train: 0.417901	val: 0.983714	test: 1.060382

Epoch: 65
Loss: 0.49198513478040695
RMSE train: 0.506843	val: 1.344909	test: 1.377413
MAE train: 0.402910	val: 1.062149	test: 1.082490

Epoch: 66
Loss: 0.4987178072333336
RMSE train: 0.589482	val: 1.410439	test: 1.424000
MAE train: 0.469975	val: 1.095504	test: 1.121129

Epoch: 67
Loss: 0.5084856003522873
RMSE train: 0.525429	val: 1.341456	test: 1.443652
MAE train: 0.423236	val: 1.086009	test: 1.169802

Epoch: 68
Loss: 0.49301573634147644
RMSE train: 0.517579	val: 1.374642	test: 1.424721
MAE train: 0.414618	val: 1.094147	test: 1.144368

Epoch: 69
Loss: 0.4462699815630913
RMSE train: 0.614098	val: 1.503495	test: 1.533861
MAE train: 0.494877	val: 1.167722	test: 1.236981

Epoch: 70
Loss: 0.5359411686658859
RMSE train: 0.508736	val: 1.358572	test: 1.408027
MAE train: 0.408529	val: 1.069113	test: 1.131954

Epoch: 71
Loss: 0.5206546187400818
RMSE train: 0.510941	val: 1.374576	test: 1.423874
MAE train: 0.406744	val: 1.083403	test: 1.123956

Epoch: 72
Loss: 0.4671008661389351
RMSE train: 0.599693	val: 1.577538	test: 1.579933
MAE train: 0.470639	val: 1.293705	test: 1.248610

Epoch: 73
Loss: 0.428835429251194
RMSE train: 0.458482	val: 1.351455	test: 1.405577
MAE train: 0.366645	val: 1.047823	test: 1.113276

Epoch: 74
Loss: 0.41636212170124054
RMSE train: 0.494724	val: 1.307974	test: 1.400576
MAE train: 0.395051	val: 1.027926	test: 1.115148

Epoch: 75
Loss: 0.4105413183569908
RMSE train: 0.516400	val: 1.382817	test: 1.443014
MAE train: 0.412697	val: 1.103150	test: 1.127519

Epoch: 76
Loss: 0.3788411095738411
RMSE train: 0.503390	val: 1.429091	test: 1.464502
MAE train: 0.404929	val: 1.135151	test: 1.151274

Epoch: 77
Loss: 0.4979083463549614
RMSE train: 0.484093	val: 1.390539	test: 1.445260
MAE train: 0.386779	val: 1.101297	test: 1.148355

Epoch: 78
Loss: 0.45105310529470444
RMSE train: 0.560225	val: 1.420572	test: 1.456242
MAE train: 0.437327	val: 1.152409	test: 1.134077

Epoch: 79
Loss: 0.402640201151371
RMSE train: 0.484507	val: 1.339009	test: 1.386289
MAE train: 0.383795	val: 1.057237	test: 1.113972

Epoch: 80
Loss: 0.41042251139879227
RMSE train: 0.429905	val: 1.263114	test: 1.320406
MAE train: 0.344472	val: 1.007500	test: 1.063241

Epoch: 81
Loss: 0.42304594069719315
RMSE train: 0.535748	val: 1.446389	test: 1.462141
MAE train: 0.425875	val: 1.142511	test: 1.139193

Epoch: 82
Loss: 0.42106103152036667
RMSE train: 0.584472	val: 1.443169	test: 1.461767
MAE train: 0.459695	val: 1.153297	test: 1.117660

Epoch: 83
Loss: 0.42123979330062866
RMSE train: 0.448002	val: 1.280655	test: 1.401229

Epoch: 84
Loss: 0.3768051043152809
RMSE train: 0.491323	val: 1.171824	test: 1.178041
MAE train: 0.372492	val: 0.947973	test: 0.921315

Epoch: 85
Loss: 0.398053914308548
RMSE train: 0.511131	val: 1.266804	test: 1.249706
MAE train: 0.396196	val: 1.001319	test: 0.968988

Epoch: 86
Loss: 0.34759747982025146
RMSE train: 0.515089	val: 1.149706	test: 1.152122
MAE train: 0.401745	val: 0.938211	test: 0.893631

Epoch: 87
Loss: 0.34174392372369766
RMSE train: 0.477508	val: 1.075833	test: 1.110257
MAE train: 0.365837	val: 0.874119	test: 0.873149

Epoch: 88
Loss: 0.3479437530040741
RMSE train: 0.477632	val: 1.211969	test: 1.176444
MAE train: 0.364438	val: 0.973788	test: 0.920469

Epoch: 89
Loss: 0.4185546785593033
RMSE train: 0.528901	val: 1.144172	test: 1.188106
MAE train: 0.393058	val: 0.936001	test: 0.899283

Epoch: 90
Loss: 0.41034480184316635
RMSE train: 0.467706	val: 1.142537	test: 1.121382
MAE train: 0.355354	val: 0.900502	test: 0.880133

Epoch: 91
Loss: 0.38928135484457016
RMSE train: 0.502510	val: 1.249845	test: 1.165125
MAE train: 0.385164	val: 0.974127	test: 0.913716

Epoch: 92
Loss: 0.35627531260252
RMSE train: 0.497867	val: 1.108811	test: 1.136653
MAE train: 0.380791	val: 0.884816	test: 0.891292

Epoch: 93
Loss: 0.42929893732070923
RMSE train: 0.536678	val: 1.351099	test: 1.226293
MAE train: 0.417049	val: 1.052006	test: 0.965888

Epoch: 94
Loss: 0.3563162609934807
RMSE train: 0.497707	val: 1.201688	test: 1.184205
MAE train: 0.374743	val: 0.960448	test: 0.930935

Epoch: 95
Loss: 0.39611366391181946
RMSE train: 0.495202	val: 1.187430	test: 1.192098
MAE train: 0.374584	val: 0.964856	test: 0.922888

Epoch: 96
Loss: 0.38662347197532654
RMSE train: 0.553446	val: 1.342167	test: 1.237514
MAE train: 0.439017	val: 1.058086	test: 0.962017

Epoch: 97
Loss: 0.36785146594047546
RMSE train: 0.494405	val: 1.073743	test: 1.112258
MAE train: 0.389756	val: 0.853373	test: 0.878996

Epoch: 98
Loss: 0.41142649948596954
RMSE train: 0.487699	val: 1.093181	test: 1.083485
MAE train: 0.378597	val: 0.879171	test: 0.864749

Epoch: 99
Loss: 0.3555963933467865
RMSE train: 0.498324	val: 1.166597	test: 1.117504
MAE train: 0.389570	val: 0.929163	test: 0.888610

Epoch: 100
Loss: 0.38227705657482147
RMSE train: 0.513302	val: 1.136492	test: 1.197874
MAE train: 0.411593	val: 0.900901	test: 0.950257

Epoch: 101
Loss: 0.3811713978648186
RMSE train: 0.440766	val: 1.207817	test: 1.203611
MAE train: 0.343179	val: 0.955513	test: 0.952814

Epoch: 102
Loss: 0.37009475380182266
RMSE train: 0.460970	val: 1.230422	test: 1.208704
MAE train: 0.357582	val: 0.980464	test: 0.946535

Epoch: 103
Loss: 0.34160009771585464
RMSE train: 0.464051	val: 1.095412	test: 1.154769
MAE train: 0.352339	val: 0.893459	test: 0.919050

Epoch: 104
Loss: 0.33109617233276367
RMSE train: 0.490797	val: 1.210649	test: 1.178507
MAE train: 0.374375	val: 0.964031	test: 0.904403

Epoch: 105
Loss: 0.3139013648033142
RMSE train: 0.495033	val: 1.302002	test: 1.198412
MAE train: 0.379499	val: 1.009768	test: 0.924889

Epoch: 106
Loss: 0.38581259548664093
RMSE train: 0.493918	val: 1.190304	test: 1.155769
MAE train: 0.372732	val: 0.929903	test: 0.894375

Epoch: 107
Loss: 0.35982344299554825
RMSE train: 0.499890	val: 1.327438	test: 1.171101
MAE train: 0.386210	val: 1.021690	test: 0.917409

Epoch: 108
Loss: 0.38636037707328796
RMSE train: 0.496401	val: 1.200106	test: 1.152210
MAE train: 0.374477	val: 0.931882	test: 0.896211

Epoch: 109
Loss: 0.36793480813503265
RMSE train: 0.485133	val: 1.158518	test: 1.154209
MAE train: 0.368345	val: 0.925289	test: 0.904642

Epoch: 110
Loss: 0.34102240204811096
RMSE train: 0.605107	val: 1.340487	test: 1.308104
MAE train: 0.448374	val: 1.055018	test: 1.001333

Epoch: 111
Loss: 0.3467615768313408
RMSE train: 0.489423	val: 1.179797	test: 1.189723
MAE train: 0.368217	val: 0.940617	test: 0.941369

Epoch: 112
Loss: 0.40159307420253754
RMSE train: 0.446924	val: 1.137374	test: 1.179941
MAE train: 0.344964	val: 0.905352	test: 0.938425

Epoch: 113
Loss: 0.33621596544981003
RMSE train: 0.511221	val: 1.207278	test: 1.177920
MAE train: 0.397043	val: 0.954791	test: 0.927519

Epoch: 114
Loss: 0.3281739130616188
RMSE train: 0.484147	val: 1.058506	test: 1.115833
MAE train: 0.372372	val: 0.865767	test: 0.884513

Epoch: 115
Loss: 0.31801795214414597
RMSE train: 0.474239	val: 1.146018	test: 1.139914
MAE train: 0.363264	val: 0.910827	test: 0.885963

Epoch: 116
Loss: 0.2874038815498352
RMSE train: 0.480633	val: 1.168996	test: 1.140237
MAE train: 0.369719	val: 0.930128	test: 0.886609

Epoch: 117
Loss: 0.38407740741968155
RMSE train: 0.482999	val: 1.069842	test: 1.120542
MAE train: 0.362113	val: 0.869682	test: 0.868429

Epoch: 118
Loss: 0.3023603558540344
RMSE train: 0.484624	val: 1.101783	test: 1.112700
MAE train: 0.362691	val: 0.896139	test: 0.854458

Epoch: 119
Loss: 0.28367719054222107
RMSE train: 0.485870	val: 1.259051	test: 1.156042
MAE train: 0.378158	val: 0.989840	test: 0.897158

Epoch: 120
Loss: 0.30430037528276443
RMSE train: 0.441144	val: 1.121114	test: 1.108878
MAE train: 0.333714	val: 0.895289	test: 0.882595

Epoch: 121
Loss: 0.35058699548244476
RMSE train: 0.470737	val: 1.095879	test: 1.131461
MAE train: 0.353803	val: 0.904952	test: 0.867960

Early stopping
Best (RMSE):	 train: 0.646098	val: 1.053493	test: 1.284413
Best (MAE):	 train: 0.496386	val: 0.826345	test: 1.027643


Epoch: 84
Loss: 0.39234647154808044
RMSE train: 0.550792	val: 1.098778	test: 1.146539
MAE train: 0.416668	val: 0.923610	test: 0.881285

Epoch: 85
Loss: 0.36948513239622116
RMSE train: 0.539583	val: 1.124111	test: 1.180912
MAE train: 0.411510	val: 0.943176	test: 0.915226

Epoch: 86
Loss: 0.42179250717163086
RMSE train: 0.553900	val: 1.097040	test: 1.150100
MAE train: 0.425258	val: 0.921980	test: 0.894354

Epoch: 87
Loss: 0.3911818414926529
RMSE train: 0.565610	val: 1.065794	test: 1.127080
MAE train: 0.432795	val: 0.905209	test: 0.871990

Epoch: 88
Loss: 0.41743801534175873
RMSE train: 0.565800	val: 1.152935	test: 1.149236
MAE train: 0.426937	val: 0.952528	test: 0.885319

Epoch: 89
Loss: 0.3835241124033928
RMSE train: 0.549444	val: 1.158980	test: 1.157804
MAE train: 0.413857	val: 0.971151	test: 0.883346

Epoch: 90
Loss: 0.3842861130833626
RMSE train: 0.552558	val: 1.176314	test: 1.174305
MAE train: 0.421195	val: 0.971433	test: 0.905702

Epoch: 91
Loss: 0.37425363063812256
RMSE train: 0.563213	val: 1.113732	test: 1.162667
MAE train: 0.429544	val: 0.930174	test: 0.899525

Epoch: 92
Loss: 0.35217173397541046
RMSE train: 0.558717	val: 1.059556	test: 1.146304
MAE train: 0.424669	val: 0.913158	test: 0.891777

Epoch: 93
Loss: 0.34652695059776306
RMSE train: 0.503418	val: 1.076316	test: 1.141334
MAE train: 0.384158	val: 0.921004	test: 0.895117

Epoch: 94
Loss: 0.35604195296764374
RMSE train: 0.529750	val: 1.201007	test: 1.221505
MAE train: 0.404997	val: 0.987855	test: 0.959100

Epoch: 95
Loss: 0.37141959369182587
RMSE train: 0.533700	val: 1.142199	test: 1.180943
MAE train: 0.415657	val: 0.962397	test: 0.939863

Epoch: 96
Loss: 0.35566144436597824
RMSE train: 0.509515	val: 1.055407	test: 1.148285
MAE train: 0.399632	val: 0.877394	test: 0.938893

Epoch: 97
Loss: 0.3808518126606941
RMSE train: 0.536773	val: 1.054764	test: 1.126797
MAE train: 0.416203	val: 0.901540	test: 0.884404

Epoch: 98
Loss: 0.3921421244740486
RMSE train: 0.533080	val: 1.085668	test: 1.162084
MAE train: 0.414847	val: 0.914698	test: 0.923805

Epoch: 99
Loss: 0.384720616042614
RMSE train: 0.520644	val: 1.111772	test: 1.187080
MAE train: 0.402126	val: 0.940056	test: 0.950137

Epoch: 100
Loss: 0.33304058015346527
RMSE train: 0.545091	val: 1.121352	test: 1.148903
MAE train: 0.420652	val: 0.949682	test: 0.874994

Epoch: 101
Loss: 0.36765747517347336
RMSE train: 0.497405	val: 1.094093	test: 1.122631
MAE train: 0.380268	val: 0.919226	test: 0.868631

Epoch: 102
Loss: 0.35759133845567703
RMSE train: 0.511607	val: 1.116374	test: 1.163022
MAE train: 0.391978	val: 0.931786	test: 0.925564

Epoch: 103
Loss: 0.3339391499757767
RMSE train: 0.510964	val: 1.145450	test: 1.178382
MAE train: 0.394698	val: 0.953220	test: 0.926757

Epoch: 104
Loss: 0.3011506162583828
RMSE train: 0.468400	val: 1.116899	test: 1.157395
MAE train: 0.362444	val: 0.934296	test: 0.935568

Epoch: 105
Loss: 0.3423352539539337
RMSE train: 0.486190	val: 1.127355	test: 1.147700
MAE train: 0.377274	val: 0.953933	test: 0.909276

Epoch: 106
Loss: 0.32115644589066505
RMSE train: 0.483334	val: 1.065761	test: 1.100623
MAE train: 0.371942	val: 0.916003	test: 0.876859

Epoch: 107
Loss: 0.35538066178560257
RMSE train: 0.501675	val: 1.063278	test: 1.096896
MAE train: 0.385304	val: 0.922749	test: 0.860486

Epoch: 108
Loss: 0.3316483721137047
RMSE train: 0.561372	val: 1.271827	test: 1.257230
MAE train: 0.430507	val: 1.039331	test: 0.978560

Epoch: 109
Loss: 0.36150704324245453
RMSE train: 0.568524	val: 1.103701	test: 1.183743
MAE train: 0.435880	val: 0.935142	test: 0.930036

Epoch: 110
Loss: 0.3595893308520317
RMSE train: 0.524410	val: 1.009528	test: 1.134994
MAE train: 0.408698	val: 0.846924	test: 0.916024

Epoch: 111
Loss: 0.32444512099027634
RMSE train: 0.542849	val: 1.108038	test: 1.145507
MAE train: 0.430683	val: 0.919042	test: 0.907270

Epoch: 112
Loss: 0.3523879870772362
RMSE train: 0.482129	val: 1.077229	test: 1.139565
MAE train: 0.371941	val: 0.895957	test: 0.909590

Epoch: 113
Loss: 0.4802789241075516
RMSE train: 0.567788	val: 1.246860	test: 1.257231
MAE train: 0.417213	val: 0.988997	test: 0.974005

Epoch: 114
Loss: 0.3600528836250305
RMSE train: 0.558384	val: 1.208064	test: 1.226906
MAE train: 0.412941	val: 1.000175	test: 0.942606

Epoch: 115
Loss: 0.3783247545361519
RMSE train: 0.505570	val: 1.135992	test: 1.173077
MAE train: 0.387686	val: 0.954463	test: 0.917053

Epoch: 116
Loss: 0.311834491789341
RMSE train: 0.512781	val: 1.150055	test: 1.187834
MAE train: 0.395242	val: 0.968839	test: 0.916945

Epoch: 117
Loss: 0.33744699507951736
RMSE train: 0.515888	val: 1.108567	test: 1.148869
MAE train: 0.399102	val: 0.938868	test: 0.893034

Epoch: 118
Loss: 0.32844845950603485
RMSE train: 0.482320	val: 1.055415	test: 1.146377
MAE train: 0.367784	val: 0.887027	test: 0.916508

Epoch: 119
Loss: 0.31669338792562485
RMSE train: 0.510928	val: 1.075684	test: 1.171196
MAE train: 0.382662	val: 0.924162	test: 0.910797

Epoch: 120
Loss: 0.30251678079366684
RMSE train: 0.493884	val: 1.064315	test: 1.140972
MAE train: 0.374828	val: 0.926839	test: 0.893615

Epoch: 121
Loss: 0.3037397116422653
RMSE train: 0.470148	val: 1.091731	test: 1.178390
MAE train: 0.363603	val: 0.922915	test: 0.940852

Early stopping
Best (RMSE):	 train: 0.600671	val: 0.946629	test: 1.111597
Best (MAE):	 train: 0.468619	val: 0.799515	test: 0.884176

MAE train: 0.438235	val: 1.175612	test: 1.225006

Epoch: 84
Loss: 0.3680715411901474
RMSE train: 0.592076	val: 1.502657	test: 1.526634
MAE train: 0.471407	val: 1.179025	test: 1.192228

Epoch: 85
Loss: 0.3632609695196152
RMSE train: 0.641699	val: 1.523310	test: 1.531422
MAE train: 0.512835	val: 1.213518	test: 1.162229

Epoch: 86
Loss: 0.39933203160762787
RMSE train: 0.546640	val: 1.395359	test: 1.421278
MAE train: 0.428231	val: 1.082108	test: 1.078229

Epoch: 87
Loss: 0.3898932486772537
RMSE train: 0.566312	val: 1.330726	test: 1.405518
MAE train: 0.432755	val: 1.072921	test: 1.043652

Epoch: 88
Loss: 0.3868357911705971
RMSE train: 0.572341	val: 1.359055	test: 1.407463
MAE train: 0.446241	val: 1.087847	test: 1.077310

Epoch: 89
Loss: 0.35623641312122345
RMSE train: 0.490773	val: 1.530173	test: 1.467989
MAE train: 0.384968	val: 1.215248	test: 1.201314

Epoch: 90
Loss: 0.3452150672674179
RMSE train: 0.468351	val: 1.228141	test: 1.307963
MAE train: 0.356365	val: 1.002043	test: 1.029927

Epoch: 91
Loss: 0.3509325087070465
RMSE train: 0.465082	val: 1.265154	test: 1.347525
MAE train: 0.365300	val: 1.004095	test: 1.060416

Epoch: 92
Loss: 0.3660901412367821
RMSE train: 0.520848	val: 1.463209	test: 1.496654
MAE train: 0.406442	val: 1.160540	test: 1.183081

Epoch: 93
Loss: 0.3197469413280487
RMSE train: 0.482735	val: 1.294287	test: 1.365026
MAE train: 0.369984	val: 1.047485	test: 1.062543

Epoch: 94
Loss: 0.37448911368846893
RMSE train: 0.538236	val: 1.445255	test: 1.451413
MAE train: 0.410423	val: 1.133343	test: 1.142723

Epoch: 95
Loss: 0.3748897314071655
RMSE train: 0.491750	val: 1.311248	test: 1.377184
MAE train: 0.380010	val: 1.037667	test: 1.089919

Epoch: 96
Loss: 0.3741542175412178
RMSE train: 0.557430	val: 1.321378	test: 1.424420
MAE train: 0.439344	val: 1.081021	test: 1.094238

Epoch: 97
Loss: 0.31465862691402435
RMSE train: 0.602094	val: 1.472796	test: 1.529901
MAE train: 0.475155	val: 1.167520	test: 1.201661

Epoch: 98
Loss: 0.3178108483552933
RMSE train: 0.480171	val: 1.289348	test: 1.376762
MAE train: 0.374935	val: 1.022955	test: 1.076615

Epoch: 99
Loss: 0.3516105115413666
RMSE train: 0.496685	val: 1.325718	test: 1.391010
MAE train: 0.383417	val: 1.057967	test: 1.075298

Epoch: 100
Loss: 0.3098083585500717
RMSE train: 0.556407	val: 1.475658	test: 1.497429
MAE train: 0.430484	val: 1.152951	test: 1.160105

Epoch: 101
Loss: 0.38638970255851746
RMSE train: 0.552727	val: 1.419717	test: 1.470888
MAE train: 0.426698	val: 1.123453	test: 1.136334

Epoch: 102
Loss: 0.31613223254680634
RMSE train: 0.477591	val: 1.270896	test: 1.367140
MAE train: 0.377394	val: 1.026504	test: 1.061333

Epoch: 103
Loss: 0.35263705253601074
RMSE train: 0.604151	val: 1.485888	test: 1.546768
MAE train: 0.469861	val: 1.212826	test: 1.188030

Epoch: 104
Loss: 0.2932651489973068
RMSE train: 0.497601	val: 1.333479	test: 1.390400
MAE train: 0.379631	val: 1.103084	test: 1.061976

Epoch: 105
Loss: 0.32099413126707077
RMSE train: 0.393389	val: 1.255150	test: 1.299537
MAE train: 0.301921	val: 1.032825	test: 1.014183

Epoch: 106
Loss: 0.2896818444132805
RMSE train: 0.459897	val: 1.356352	test: 1.349853
MAE train: 0.363044	val: 1.089936	test: 1.048099

Epoch: 107
Loss: 0.289326973259449
RMSE train: 0.442597	val: 1.238510	test: 1.273724
MAE train: 0.348709	val: 1.013631	test: 0.985049

Epoch: 108
Loss: 0.2840150520205498
RMSE train: 0.406394	val: 1.207542	test: 1.263973
MAE train: 0.315488	val: 0.972753	test: 0.993142

Epoch: 109
Loss: 0.2917941063642502
RMSE train: 0.439164	val: 1.255498	test: 1.344068
MAE train: 0.339024	val: 1.012446	test: 1.037448

Epoch: 110
Loss: 0.2869991213083267
RMSE train: 0.568532	val: 1.390543	test: 1.470311
MAE train: 0.425206	val: 1.147782	test: 1.115722

Epoch: 111
Loss: 0.29369303584098816
RMSE train: 0.468149	val: 1.440843	test: 1.505210
MAE train: 0.356958	val: 1.123741	test: 1.196548

Epoch: 112
Loss: 0.3118237033486366
RMSE train: 0.521792	val: 1.410615	test: 1.472654
MAE train: 0.406421	val: 1.140422	test: 1.145674

Epoch: 113
Loss: 0.29727931320667267
RMSE train: 0.516372	val: 1.369004	test: 1.430389
MAE train: 0.407200	val: 1.129839	test: 1.092971

Epoch: 114
Loss: 0.2811174616217613
RMSE train: 0.505752	val: 1.485824	test: 1.498395
MAE train: 0.399360	val: 1.190065	test: 1.181290

Epoch: 115
Loss: 0.2599126063287258
RMSE train: 0.494554	val: 1.312986	test: 1.410186
MAE train: 0.390102	val: 1.056015	test: 1.073906

Epoch: 116
Loss: 0.33788638934493065
RMSE train: 0.475209	val: 1.381043	test: 1.422199
MAE train: 0.371097	val: 1.095695	test: 1.099898

Epoch: 117
Loss: 0.28191961720585823
RMSE train: 0.545983	val: 1.431027	test: 1.418324
MAE train: 0.423295	val: 1.125008	test: 1.093468

Epoch: 118
Loss: 0.2846081331372261
RMSE train: 0.376348	val: 1.255645	test: 1.270846
MAE train: 0.292872	val: 0.993858	test: 0.997029

Epoch: 119
Loss: 0.27652980759739876
RMSE train: 0.447205	val: 1.315795	test: 1.331929
MAE train: 0.350221	val: 1.061510	test: 1.025923

Epoch: 120
Loss: 0.28626222908496857
RMSE train: 0.457122	val: 1.354647	test: 1.361035
MAE train: 0.357182	val: 1.091260	test: 1.060418

Epoch: 121
Loss: 0.2581913247704506
RMSE train: 0.372127	val: 1.362125	test: 1.348126
MAE train: 0.292220	val: 1.084522	test: 1.072339

Early stopping
Best (RMSE):	 train: 0.549959	val: 1.201916	test: 1.329425
Best (MAE):	 train: 0.425380	val: 0.997679	test: 1.052405

MAE train: 0.440922	val: 1.126414	test: 1.239853

Epoch: 84
Loss: 0.4019513204693794
RMSE train: 0.577969	val: 1.496613	test: 1.529344
MAE train: 0.448986	val: 1.151811	test: 1.223778

Epoch: 85
Loss: 0.4621826037764549
RMSE train: 0.598396	val: 1.564929	test: 1.575833
MAE train: 0.471525	val: 1.212696	test: 1.220808

Epoch: 86
Loss: 0.43402089923620224
RMSE train: 0.621934	val: 1.608465	test: 1.633252
MAE train: 0.484162	val: 1.232194	test: 1.263259

Epoch: 87
Loss: 0.43562203645706177
RMSE train: 0.581608	val: 1.546326	test: 1.586848
MAE train: 0.452532	val: 1.184974	test: 1.239764

Epoch: 88
Loss: 0.4084974080324173
RMSE train: 0.573507	val: 1.622872	test: 1.637190
MAE train: 0.444476	val: 1.245636	test: 1.293821

Epoch: 89
Loss: 0.409243680536747
RMSE train: 0.528556	val: 1.566539	test: 1.570825
MAE train: 0.411923	val: 1.210704	test: 1.233045

Epoch: 90
Loss: 0.3526839464902878
RMSE train: 0.505249	val: 1.408814	test: 1.473461
MAE train: 0.391236	val: 1.106523	test: 1.174162

Epoch: 91
Loss: 0.3616850972175598
RMSE train: 0.519804	val: 1.517517	test: 1.558458
MAE train: 0.398412	val: 1.164625	test: 1.229947

Epoch: 92
Loss: 0.38322367519140244
RMSE train: 0.607624	val: 1.698805	test: 1.694710
MAE train: 0.464055	val: 1.317268	test: 1.314982

Epoch: 93
Loss: 0.363776758313179
RMSE train: 0.507797	val: 1.533261	test: 1.553659
MAE train: 0.390600	val: 1.207357	test: 1.231879

Epoch: 94
Loss: 0.3895794227719307
RMSE train: 0.516494	val: 1.508795	test: 1.539762
MAE train: 0.392111	val: 1.180979	test: 1.215660

Epoch: 95
Loss: 0.40923990309238434
RMSE train: 0.623043	val: 1.667652	test: 1.682431
MAE train: 0.463502	val: 1.293983	test: 1.301273

Epoch: 96
Loss: 0.3989664912223816
RMSE train: 0.544938	val: 1.601587	test: 1.614827
MAE train: 0.418825	val: 1.273825	test: 1.254411

Epoch: 97
Loss: 0.37550798803567886
RMSE train: 0.520333	val: 1.589854	test: 1.579166
MAE train: 0.402532	val: 1.234258	test: 1.241570

Epoch: 98
Loss: 0.344649076461792
RMSE train: 0.569676	val: 1.536328	test: 1.533357
MAE train: 0.434649	val: 1.189253	test: 1.217747

Epoch: 99
Loss: 0.34591687470674515
RMSE train: 0.541856	val: 1.487245	test: 1.483826
MAE train: 0.408605	val: 1.153113	test: 1.184267

Epoch: 100
Loss: 0.33906154334545135
RMSE train: 0.475235	val: 1.587303	test: 1.557909
MAE train: 0.358544	val: 1.217736	test: 1.274981

Epoch: 101
Loss: 0.4083656296133995
RMSE train: 0.631334	val: 1.613197	test: 1.595873
MAE train: 0.467660	val: 1.297190	test: 1.231674

Epoch: 102
Loss: 0.33939627557992935
RMSE train: 0.551147	val: 1.497792	test: 1.518029
MAE train: 0.415133	val: 1.201334	test: 1.194973

Epoch: 103
Loss: 0.33055589348077774
RMSE train: 0.523646	val: 1.627706	test: 1.634508
MAE train: 0.393717	val: 1.249707	test: 1.294353

Epoch: 104
Loss: 0.3461602181196213
RMSE train: 0.548521	val: 1.654521	test: 1.650325
MAE train: 0.405657	val: 1.305136	test: 1.272686

Epoch: 105
Loss: 0.31252121925354004
RMSE train: 0.479289	val: 1.561700	test: 1.564268
MAE train: 0.359482	val: 1.249712	test: 1.222653

Epoch: 106
Loss: 0.28116095811128616
RMSE train: 0.440254	val: 1.570896	test: 1.558658
MAE train: 0.335224	val: 1.225221	test: 1.253727

Epoch: 107
Loss: 0.32398655265569687
RMSE train: 0.444636	val: 1.493510	test: 1.504722
MAE train: 0.338588	val: 1.167339	test: 1.217209

Epoch: 108
Loss: 0.31342224031686783
RMSE train: 0.479130	val: 1.488062	test: 1.501780
MAE train: 0.364872	val: 1.209366	test: 1.188360

Epoch: 109
Loss: 0.30648406594991684
RMSE train: 0.521587	val: 1.499344	test: 1.517969
MAE train: 0.391445	val: 1.189687	test: 1.177195

Epoch: 110
Loss: 0.3306928128004074
RMSE train: 0.583103	val: 1.490177	test: 1.530861
MAE train: 0.434779	val: 1.174412	test: 1.184906

Epoch: 111
Loss: 0.3316285088658333
RMSE train: 0.516767	val: 1.443235	test: 1.498122
MAE train: 0.395899	val: 1.123382	test: 1.214120

Epoch: 112
Loss: 0.32025742530822754
RMSE train: 0.562307	val: 1.463465	test: 1.503751
MAE train: 0.438435	val: 1.143205	test: 1.174847

Epoch: 113
Loss: 0.30220698565244675
RMSE train: 0.562419	val: 1.551205	test: 1.584938
MAE train: 0.439691	val: 1.252201	test: 1.208431

Epoch: 114
Loss: 0.2808639407157898
RMSE train: 0.504775	val: 1.573045	test: 1.611458
MAE train: 0.393294	val: 1.222246	test: 1.251286

Epoch: 115
Loss: 0.2972557470202446
RMSE train: 0.544774	val: 1.563317	test: 1.619270
MAE train: 0.418567	val: 1.234788	test: 1.243555

Epoch: 116
Loss: 0.32658669352531433
RMSE train: 0.566157	val: 1.539339	test: 1.590226
MAE train: 0.435243	val: 1.245588	test: 1.223214

Epoch: 117
Loss: 0.3087402582168579
RMSE train: 0.649114	val: 1.646151	test: 1.664708
MAE train: 0.491321	val: 1.326057	test: 1.272914

Epoch: 118
Loss: 0.2986818924546242
RMSE train: 0.518215	val: 1.475514	test: 1.508956
MAE train: 0.400668	val: 1.172702	test: 1.180321

Epoch: 119
Loss: 0.27068132907152176
RMSE train: 0.493916	val: 1.507557	test: 1.520161
MAE train: 0.383416	val: 1.177156	test: 1.181483

Epoch: 120
Loss: 0.32695272564888
RMSE train: 0.587317	val: 1.695772	test: 1.698271
MAE train: 0.441932	val: 1.383226	test: 1.301467

Epoch: 121
Loss: 0.25830765441060066
RMSE train: 0.463280	val: 1.558760	test: 1.572115
MAE train: 0.347998	val: 1.205890	test: 1.227726

Early stopping
Best (RMSE):	 train: 0.673458	val: 1.382428	test: 1.454011
Best (MAE):	 train: 0.523792	val: 1.105010	test: 1.179027


Epoch: 84
Loss: 0.3523980900645256
RMSE train: 0.531164	val: 1.199915	test: 1.185109
MAE train: 0.399428	val: 0.982548	test: 0.947469

Epoch: 85
Loss: 0.4379260241985321
RMSE train: 0.582994	val: 1.260477	test: 1.243350
MAE train: 0.434954	val: 1.034573	test: 0.973237

Epoch: 86
Loss: 0.35772527754306793
RMSE train: 0.542197	val: 1.164039	test: 1.168729
MAE train: 0.403061	val: 0.971102	test: 0.920238

Epoch: 87
Loss: 0.38110917806625366
RMSE train: 0.597234	val: 1.237436	test: 1.241193
MAE train: 0.440368	val: 1.024159	test: 0.964780

Epoch: 88
Loss: 0.40508056432008743
RMSE train: 0.560447	val: 1.194231	test: 1.206980
MAE train: 0.416438	val: 1.002665	test: 0.943419

Epoch: 89
Loss: 0.399064801633358
RMSE train: 0.506295	val: 1.120848	test: 1.153046
MAE train: 0.377441	val: 0.941402	test: 0.916589

Epoch: 90
Loss: 0.4643387943506241
RMSE train: 0.610024	val: 1.284261	test: 1.248743
MAE train: 0.468017	val: 1.057581	test: 0.968124

Epoch: 91
Loss: 0.3645590841770172
RMSE train: 0.583434	val: 1.242245	test: 1.202457
MAE train: 0.445876	val: 1.020776	test: 0.937185

Epoch: 92
Loss: 0.3955689072608948
RMSE train: 0.546743	val: 1.148256	test: 1.160057
MAE train: 0.410707	val: 0.960008	test: 0.913884

Epoch: 93
Loss: 0.3684639632701874
RMSE train: 0.533922	val: 1.217177	test: 1.182976
MAE train: 0.404641	val: 0.978319	test: 0.942815

Epoch: 94
Loss: 0.35371679812669754
RMSE train: 0.566597	val: 1.181311	test: 1.179056
MAE train: 0.427024	val: 0.966779	test: 0.928189

Epoch: 95
Loss: 0.3589204102754593
RMSE train: 0.568148	val: 1.152692	test: 1.185947
MAE train: 0.428228	val: 0.958203	test: 0.942548

Epoch: 96
Loss: 0.36832675337791443
RMSE train: 0.543952	val: 1.248391	test: 1.247079
MAE train: 0.415644	val: 1.013631	test: 0.979841

Epoch: 97
Loss: 0.38342271000146866
RMSE train: 0.587887	val: 1.196794	test: 1.209916
MAE train: 0.441115	val: 1.009378	test: 0.933093

Epoch: 98
Loss: 0.33702319115400314
RMSE train: 0.566919	val: 1.185082	test: 1.187349
MAE train: 0.424608	val: 0.980339	test: 0.913735

Epoch: 99
Loss: 0.34495116025209427
RMSE train: 0.529638	val: 1.187611	test: 1.181842
MAE train: 0.400025	val: 0.974242	test: 0.906436

Epoch: 100
Loss: 0.334009513258934
RMSE train: 0.517520	val: 1.140906	test: 1.152264
MAE train: 0.390068	val: 0.937366	test: 0.894406

Epoch: 101
Loss: 0.33309271186590195
RMSE train: 0.526854	val: 1.179864	test: 1.160469
MAE train: 0.394092	val: 0.962618	test: 0.900038

Epoch: 102
Loss: 0.35862214863300323
RMSE train: 0.563293	val: 1.226045	test: 1.210587
MAE train: 0.423358	val: 1.014193	test: 0.930116

Epoch: 103
Loss: 0.4143085777759552
RMSE train: 0.536481	val: 1.182300	test: 1.198457
MAE train: 0.407321	val: 0.972264	test: 0.929373

Epoch: 104
Loss: 0.3726583942770958
RMSE train: 0.533013	val: 1.135920	test: 1.168194
MAE train: 0.408483	val: 0.959130	test: 0.906026

Epoch: 105
Loss: 0.3808128610253334
RMSE train: 0.528070	val: 1.113517	test: 1.145369
MAE train: 0.405994	val: 0.939855	test: 0.886783

Epoch: 106
Loss: 0.3256033957004547
RMSE train: 0.539620	val: 1.118702	test: 1.162269
MAE train: 0.415530	val: 0.950140	test: 0.899613

Epoch: 107
Loss: 0.3485240191221237
RMSE train: 0.499110	val: 1.091188	test: 1.187079
MAE train: 0.380780	val: 0.908346	test: 0.942654

Epoch: 108
Loss: 0.33661341667175293
RMSE train: 0.595475	val: 1.207105	test: 1.254900
MAE train: 0.446745	val: 1.007821	test: 0.966286

Epoch: 109
Loss: 0.32362258434295654
RMSE train: 0.611248	val: 1.265022	test: 1.297097
MAE train: 0.458061	val: 1.042328	test: 0.999567

Epoch: 110
Loss: 0.3448389172554016
RMSE train: 0.529121	val: 1.111184	test: 1.179082
MAE train: 0.399780	val: 0.931639	test: 0.927192

Epoch: 111
Loss: 0.3362683802843094
RMSE train: 0.495279	val: 1.076332	test: 1.143148
MAE train: 0.371780	val: 0.900629	test: 0.905751

Epoch: 112
Loss: 0.30008435249328613
RMSE train: 0.545047	val: 1.163531	test: 1.187410
MAE train: 0.413688	val: 0.970443	test: 0.915996

Epoch: 113
Loss: 0.2900537773966789
RMSE train: 0.558694	val: 1.253308	test: 1.273553
MAE train: 0.425096	val: 1.031209	test: 0.983829

Epoch: 114
Loss: 0.31617918610572815
RMSE train: 0.523114	val: 1.213259	test: 1.250667
MAE train: 0.393564	val: 0.997965	test: 0.978937

Epoch: 115
Loss: 0.33883146941661835
RMSE train: 0.562612	val: 1.198869	test: 1.237377
MAE train: 0.421145	val: 0.984699	test: 0.962787

Epoch: 116
Loss: 0.3132268115878105
RMSE train: 0.526968	val: 1.206811	test: 1.192827
MAE train: 0.398923	val: 0.972451	test: 0.925909

Epoch: 117
Loss: 0.2874826714396477
RMSE train: 0.519765	val: 1.130290	test: 1.133340
MAE train: 0.398348	val: 0.939079	test: 0.863529

Epoch: 118
Loss: 0.302118256688118
RMSE train: 0.516416	val: 1.180122	test: 1.117384
MAE train: 0.399322	val: 0.957892	test: 0.862150

Epoch: 119
Loss: 0.30884677171707153
RMSE train: 0.502833	val: 1.123848	test: 1.106800
MAE train: 0.391758	val: 0.936502	test: 0.868834

Epoch: 120
Loss: 0.3348575234413147
RMSE train: 0.511215	val: 1.205015	test: 1.197343
MAE train: 0.395266	val: 0.979311	test: 0.936963

Epoch: 121
Loss: 0.3263687640428543
RMSE train: 0.504774	val: 1.181606	test: 1.218456
MAE train: 0.382117	val: 0.982433	test: 0.970490

Epoch: 122
Loss: 0.33339257538318634
RMSE train: 0.577508	val: 1.348997	test: 1.328422
MAE train: 0.440180	val: 1.075333	test: 1.022872

Epoch: 123
Loss: 0.32960397750139236
RMSE train: 0.500465	val: 1.145291	test: 1.160199
MAE train: 0.383173	val: 0.954676	test: 0.920832

Epoch: 124
Loss: 0.30933113768696785
RMSE train: 0.530945	val: 1.203479	test: 1.147601
MAE train: 0.409113	val: 0.996585	test: 0.875491

Epoch: 125
Loss: 0.29959362745285034
RMSE train: 0.522639	val: 1.166915	test: 1.112968
MAE train: 0.406744	val: 0.959872	test: 0.881776

Epoch: 126
Loss: 0.27792587876319885
RMSE train: 0.488876	val: 1.144568	test: 1.181955
MAE train: 0.387837	val: 0.936762	test: 0.948444

Epoch: 127
Loss: 0.2719600722193718
RMSE train: 0.500544	val: 1.218978	test: 1.200487
MAE train: 0.389274	val: 1.001827	test: 0.954323

Epoch: 128
Loss: 0.2993176057934761
RMSE train: 0.519244	val: 1.191890	test: 1.174439
MAE train: 0.404338	val: 0.992623	test: 0.927339

Epoch: 129
Loss: 0.2917478680610657
RMSE train: 0.500289	val: 1.167458	test: 1.147401
MAE train: 0.387761	val: 0.962395	test: 0.918303

Epoch: 130
Loss: 0.28627918660640717
RMSE train: 0.516666	val: 1.210552	test: 1.156604
MAE train: 0.399256	val: 0.997896	test: 0.894514

Epoch: 131
Loss: 0.29154057055711746
RMSE train: 0.540711	val: 1.163467	test: 1.155245
MAE train: 0.411331	val: 0.989976	test: 0.876595

Epoch: 132
Loss: 0.30258092284202576
RMSE train: 0.463720	val: 1.133216	test: 1.109253
MAE train: 0.358186	val: 0.944582	test: 0.865812

Epoch: 133
Loss: 0.32643719762563705
RMSE train: 0.498252	val: 1.159665	test: 1.138378
MAE train: 0.382281	val: 0.977870	test: 0.891195

Epoch: 134
Loss: 0.29939771443605423
RMSE train: 0.499175	val: 1.160222	test: 1.138573
MAE train: 0.389607	val: 0.972849	test: 0.896689

Epoch: 135
Loss: 0.3272690996527672
RMSE train: 0.451444	val: 1.129355	test: 1.118145
MAE train: 0.346843	val: 0.933791	test: 0.887769

Epoch: 136
Loss: 0.3172011971473694
RMSE train: 0.512431	val: 1.187215	test: 1.154034
MAE train: 0.391525	val: 0.973432	test: 0.887772

Epoch: 137
Loss: 0.29280316084623337
RMSE train: 0.486290	val: 1.154202	test: 1.153039
MAE train: 0.367686	val: 0.965625	test: 0.884753

Epoch: 138
Loss: 0.27888020500540733
RMSE train: 0.533169	val: 1.236561	test: 1.210071
MAE train: 0.406097	val: 1.009897	test: 0.933379

Epoch: 139
Loss: 0.31475382298231125
RMSE train: 0.489448	val: 1.109434	test: 1.158585
MAE train: 0.370142	val: 0.934469	test: 0.915565

Epoch: 140
Loss: 0.29875780642032623
RMSE train: 0.560505	val: 1.233070	test: 1.222952
MAE train: 0.432979	val: 1.033370	test: 0.937014

Epoch: 141
Loss: 0.27769866585731506
RMSE train: 0.496610	val: 1.211797	test: 1.200173
MAE train: 0.388787	val: 0.991489	test: 0.932501

Epoch: 142
Loss: 0.24112701043486595
RMSE train: 0.483290	val: 1.136801	test: 1.156238
MAE train: 0.369610	val: 0.940416	test: 0.911160

Epoch: 143
Loss: 0.3071567043662071
RMSE train: 0.565523	val: 1.170573	test: 1.186730
MAE train: 0.428228	val: 0.977503	test: 0.903412

Epoch: 144
Loss: 0.24724717810750008
RMSE train: 0.533222	val: 1.182962	test: 1.181370
MAE train: 0.404472	val: 0.963264	test: 0.903918

Epoch: 145
Loss: 0.303688608109951
RMSE train: 0.533294	val: 1.199787	test: 1.179279
MAE train: 0.403785	val: 0.972716	test: 0.903903

Epoch: 146
Loss: 0.28209035471081734
RMSE train: 0.538696	val: 1.159303	test: 1.163522
MAE train: 0.417266	val: 0.972882	test: 0.897793

Early stopping
Best (RMSE):	 train: 0.495279	val: 1.076332	test: 1.143148
Best (MAE):	 train: 0.371780	val: 0.900629	test: 0.905751
All runs completed.

MAE train: 0.365349	val: 1.160949	test: 1.204152

Epoch: 84
Loss: 0.4419514238834381
RMSE train: 0.514292	val: 1.515564	test: 1.595498
MAE train: 0.391673	val: 1.167865	test: 1.181535

Epoch: 85
Loss: 0.49467378854751587
RMSE train: 0.530399	val: 1.483983	test: 1.543271
MAE train: 0.412780	val: 1.174218	test: 1.161098

Epoch: 86
Loss: 0.43035341054201126
RMSE train: 0.522987	val: 1.383041	test: 1.458686
MAE train: 0.408515	val: 1.073680	test: 1.095933

Epoch: 87
Loss: 0.43587175756692886
RMSE train: 0.461276	val: 1.410567	test: 1.484045
MAE train: 0.360523	val: 1.093088	test: 1.128953

Epoch: 88
Loss: 0.439378947019577
RMSE train: 0.519366	val: 1.505851	test: 1.584369
MAE train: 0.404015	val: 1.172212	test: 1.191593

Epoch: 89
Loss: 0.4122217446565628
RMSE train: 0.465954	val: 1.381705	test: 1.517222
MAE train: 0.368979	val: 1.068507	test: 1.124855

Epoch: 90
Loss: 0.41717012971639633
RMSE train: 0.443440	val: 1.318899	test: 1.498208
MAE train: 0.343384	val: 1.024066	test: 1.127560

Epoch: 91
Loss: 0.41128741949796677
RMSE train: 0.497490	val: 1.378051	test: 1.487783
MAE train: 0.384510	val: 1.094233	test: 1.139525

Epoch: 92
Loss: 0.3626222759485245
RMSE train: 0.512454	val: 1.359675	test: 1.458122
MAE train: 0.393800	val: 1.071058	test: 1.110658

Epoch: 93
Loss: 0.42504172027111053
RMSE train: 0.485288	val: 1.393898	test: 1.498463
MAE train: 0.369475	val: 1.080492	test: 1.134053

Epoch: 94
Loss: 0.3806433454155922
RMSE train: 0.524675	val: 1.441521	test: 1.545879
MAE train: 0.394922	val: 1.114586	test: 1.177332

Epoch: 95
Loss: 0.37446751445531845
RMSE train: 0.473864	val: 1.377086	test: 1.494703
MAE train: 0.359936	val: 1.075280	test: 1.140688

Epoch: 96
Loss: 0.3981745168566704
RMSE train: 0.420201	val: 1.304256	test: 1.454443
MAE train: 0.324958	val: 1.014512	test: 1.102296

Epoch: 97
Loss: 0.40514855831861496
RMSE train: 0.496142	val: 1.350811	test: 1.467384
MAE train: 0.387099	val: 1.059901	test: 1.101850

Epoch: 98
Loss: 0.37289663404226303
RMSE train: 0.486305	val: 1.356645	test: 1.477645
MAE train: 0.384188	val: 1.068611	test: 1.108565

Epoch: 99
Loss: 0.3436729907989502
RMSE train: 0.472803	val: 1.332903	test: 1.451318
MAE train: 0.372055	val: 1.051513	test: 1.096594

Epoch: 100
Loss: 0.38697029650211334
RMSE train: 0.511582	val: 1.328893	test: 1.434991
MAE train: 0.399709	val: 1.051522	test: 1.080448

Epoch: 101
Loss: 0.35339685529470444
RMSE train: 0.495157	val: 1.392532	test: 1.491346
MAE train: 0.376280	val: 1.092738	test: 1.112533

Epoch: 102
Loss: 0.33632203936576843
RMSE train: 0.508078	val: 1.464561	test: 1.562291
MAE train: 0.374938	val: 1.130916	test: 1.155790

Epoch: 103
Loss: 0.3661525771021843
RMSE train: 0.510643	val: 1.471347	test: 1.565628
MAE train: 0.378492	val: 1.129359	test: 1.159221

Epoch: 104
Loss: 0.3753902465105057
RMSE train: 0.481130	val: 1.409816	test: 1.510503
MAE train: 0.365621	val: 1.092266	test: 1.133122

Epoch: 105
Loss: 0.3821929469704628
RMSE train: 0.466284	val: 1.393674	test: 1.488967
MAE train: 0.359713	val: 1.076314	test: 1.112985

Epoch: 106
Loss: 0.3437563627958298
RMSE train: 0.442701	val: 1.409936	test: 1.529912
MAE train: 0.339332	val: 1.086112	test: 1.156360

Epoch: 107
Loss: 0.30263379216194153
RMSE train: 0.449403	val: 1.399598	test: 1.500853
MAE train: 0.349169	val: 1.092200	test: 1.110832

Epoch: 108
Loss: 0.380049966275692
RMSE train: 0.465815	val: 1.465542	test: 1.565684
MAE train: 0.355729	val: 1.136373	test: 1.153980

Epoch: 109
Loss: 0.37075353413820267
RMSE train: 0.511068	val: 1.502206	test: 1.599472
MAE train: 0.386745	val: 1.164237	test: 1.178353

Epoch: 110
Loss: 0.3715579882264137
RMSE train: 0.478596	val: 1.426547	test: 1.516146
MAE train: 0.358943	val: 1.113436	test: 1.133688

Epoch: 111
Loss: 0.3378392234444618
RMSE train: 0.440179	val: 1.340967	test: 1.460732
MAE train: 0.337481	val: 1.048031	test: 1.091689

Epoch: 112
Loss: 0.33939773589372635
RMSE train: 0.505294	val: 1.464690	test: 1.569325
MAE train: 0.387209	val: 1.109369	test: 1.176851

Epoch: 113
Loss: 0.31203844398260117
RMSE train: 0.542232	val: 1.453681	test: 1.563081
MAE train: 0.419528	val: 1.137484	test: 1.168159

Epoch: 114
Loss: 0.30452048033475876
RMSE train: 0.466919	val: 1.363954	test: 1.507757
MAE train: 0.362149	val: 1.036752	test: 1.141140

Epoch: 115
Loss: 0.30892421305179596
RMSE train: 0.494729	val: 1.368856	test: 1.508397
MAE train: 0.378467	val: 1.039768	test: 1.135781

Epoch: 116
Loss: 0.27991408854722977
RMSE train: 0.527347	val: 1.396280	test: 1.521274
MAE train: 0.403592	val: 1.059964	test: 1.132982

Epoch: 117
Loss: 0.2953411117196083
RMSE train: 0.476323	val: 1.353579	test: 1.481186
MAE train: 0.363808	val: 1.040271	test: 1.100285

Epoch: 118
Loss: 0.2814217582345009
RMSE train: 0.441046	val: 1.313054	test: 1.438208
MAE train: 0.337735	val: 1.025488	test: 1.078911

Epoch: 119
Loss: 0.2896192669868469
RMSE train: 0.477037	val: 1.374400	test: 1.458875
MAE train: 0.361608	val: 1.053069	test: 1.111486

Epoch: 120
Loss: 0.29835206270217896
RMSE train: 0.452740	val: 1.443958	test: 1.497651
MAE train: 0.349990	val: 1.094995	test: 1.159161

Epoch: 121
Loss: 0.3347175270318985
RMSE train: 0.395209	val: 1.298227	test: 1.419710
MAE train: 0.320686	val: 1.024686	test: 1.088104

Epoch: 122
Loss: 0.294320248067379
RMSE train: 0.428368	val: 1.389123	test: 1.493373
MAE train: 0.334842	val: 1.067236	test: 1.166415

Epoch: 123
Loss: 0.30102046579122543
RMSE train: 0.490517	val: 1.409306	test: 1.517822
MAE train: 0.378570	val: 1.074843	test: 1.131916

Epoch: 124
Loss: 0.2960118502378464
RMSE train: 0.401525	val: 1.299812	test: 1.449787
MAE train: 0.319429	val: 1.010267	test: 1.090458

Epoch: 125
Loss: 0.29356613010168076
RMSE train: 0.382542	val: 1.404649	test: 1.527965
MAE train: 0.296136	val: 1.079778	test: 1.189582

Epoch: 126
Loss: 0.2683461681008339
RMSE train: 0.402718	val: 1.339143	test: 1.468946
MAE train: 0.318243	val: 1.020175	test: 1.118937

Epoch: 127
Loss: 0.2906423918902874
RMSE train: 0.410665	val: 1.365034	test: 1.490490
MAE train: 0.321393	val: 1.026865	test: 1.128620

Epoch: 128
Loss: 0.28372326493263245
RMSE train: 0.440325	val: 1.399171	test: 1.527554
MAE train: 0.339534	val: 1.052237	test: 1.145839

Epoch: 129
Loss: 0.2800300046801567
RMSE train: 0.518852	val: 1.403615	test: 1.532328
MAE train: 0.396958	val: 1.083403	test: 1.130157

Epoch: 130
Loss: 0.2838832214474678
RMSE train: 0.465863	val: 1.405103	test: 1.540243
MAE train: 0.361489	val: 1.062333	test: 1.150899

Epoch: 131
Loss: 0.2899925634264946
RMSE train: 0.393993	val: 1.357326	test: 1.503604
MAE train: 0.310188	val: 1.037201	test: 1.119088

Epoch: 132
Loss: 0.26027219742536545
RMSE train: 0.414442	val: 1.303215	test: 1.450414
MAE train: 0.327379	val: 1.006059	test: 1.083181

Epoch: 133
Loss: 0.3074120283126831
RMSE train: 0.462864	val: 1.404936	test: 1.518514
MAE train: 0.358182	val: 1.046766	test: 1.147090

Epoch: 134
Loss: 0.279524989426136
RMSE train: 0.449760	val: 1.410173	test: 1.525174
MAE train: 0.349983	val: 1.054560	test: 1.154802

Epoch: 135
Loss: 0.27701564133167267
RMSE train: 0.421611	val: 1.286749	test: 1.444925
MAE train: 0.328635	val: 0.976890	test: 1.077849

Epoch: 136
Loss: 0.3077955096960068
RMSE train: 0.380832	val: 1.304990	test: 1.456660
MAE train: 0.297906	val: 0.991571	test: 1.102302

Epoch: 137
Loss: 0.2746385931968689
RMSE train: 0.412333	val: 1.344059	test: 1.488261
MAE train: 0.317126	val: 1.017523	test: 1.134519

Epoch: 138
Loss: 0.27692941948771477
RMSE train: 0.516254	val: 1.346785	test: 1.469413
MAE train: 0.386702	val: 1.023019	test: 1.087876

Epoch: 139
Loss: 0.25649021938443184
RMSE train: 0.427102	val: 1.350112	test: 1.492753
MAE train: 0.318508	val: 1.014678	test: 1.126721

Epoch: 140
Loss: 0.30591896921396255
RMSE train: 0.429884	val: 1.356850	test: 1.512474
MAE train: 0.325187	val: 1.014921	test: 1.104399

Epoch: 141
Loss: 0.28128063678741455
RMSE train: 0.485614	val: 1.489586	test: 1.630414
MAE train: 0.361199	val: 1.124365	test: 1.192627

Epoch: 142
Loss: 0.2536752186715603
RMSE train: 0.527138	val: 1.633358	test: 1.748568
MAE train: 0.389502	val: 1.252802	test: 1.296780

Epoch: 143
Loss: 0.2844807207584381
RMSE train: 0.426878	val: 1.458456	test: 1.586146
MAE train: 0.321406	val: 1.108333	test: 1.170375
MAE train: 0.377545	val: 0.967512	test: 1.000856

Epoch: 84
Loss: 0.3480728566646576
RMSE train: 0.546875	val: 1.245186	test: 1.264316
MAE train: 0.416844	val: 1.011056	test: 0.987952

Epoch: 85
Loss: 0.38445043563842773
RMSE train: 0.533767	val: 1.157180	test: 1.194340
MAE train: 0.414931	val: 0.960821	test: 0.950727

Epoch: 86
Loss: 0.3760332316160202
RMSE train: 0.521097	val: 1.200786	test: 1.217550
MAE train: 0.404773	val: 0.956207	test: 0.979258

Epoch: 87
Loss: 0.3676569387316704
RMSE train: 0.498447	val: 1.149791	test: 1.212208
MAE train: 0.390240	val: 0.939267	test: 0.954667

Epoch: 88
Loss: 0.3336266502737999
RMSE train: 0.460869	val: 1.165332	test: 1.179573
MAE train: 0.366021	val: 0.926338	test: 0.943371

Epoch: 89
Loss: 0.32448993623256683
RMSE train: 0.431171	val: 1.155022	test: 1.192376
MAE train: 0.347406	val: 0.925070	test: 0.974452

Epoch: 90
Loss: 0.3540499657392502
RMSE train: 0.487914	val: 1.167199	test: 1.204910
MAE train: 0.382192	val: 0.962230	test: 0.963087

Epoch: 91
Loss: 0.32480697333812714
RMSE train: 0.477839	val: 1.175730	test: 1.212924
MAE train: 0.378530	val: 0.965378	test: 0.965888

Epoch: 92
Loss: 0.3187127634882927
RMSE train: 0.441252	val: 1.175968	test: 1.237834
MAE train: 0.349520	val: 0.945897	test: 0.997368

Epoch: 93
Loss: 0.41664254665374756
RMSE train: 0.546305	val: 1.203915	test: 1.281197
MAE train: 0.420116	val: 1.002254	test: 1.010160

Epoch: 94
Loss: 0.3586834594607353
RMSE train: 0.475274	val: 1.125860	test: 1.216291
MAE train: 0.376431	val: 0.924928	test: 0.978259

Epoch: 95
Loss: 0.3167670890688896
RMSE train: 0.450743	val: 1.197541	test: 1.317235
MAE train: 0.353479	val: 0.968394	test: 1.048591

Epoch: 96
Loss: 0.3426998481154442
RMSE train: 0.505173	val: 1.246956	test: 1.324478
MAE train: 0.382387	val: 1.023367	test: 1.043052

Epoch: 97
Loss: 0.3548211604356766
RMSE train: 0.490443	val: 1.190567	test: 1.305461
MAE train: 0.383187	val: 0.969753	test: 1.035138

Epoch: 98
Loss: 0.3412711024284363
RMSE train: 0.454640	val: 1.246972	test: 1.399585
MAE train: 0.353197	val: 0.992456	test: 1.104569

Epoch: 99
Loss: 0.3349989950656891
RMSE train: 0.495452	val: 1.184573	test: 1.228479
MAE train: 0.389643	val: 0.983992	test: 0.962968

Epoch: 100
Loss: 0.32875245809555054
RMSE train: 0.495129	val: 1.215144	test: 1.236158
MAE train: 0.389775	val: 1.000667	test: 0.969589

Epoch: 101
Loss: 0.33705584704875946
RMSE train: 0.432946	val: 1.159560	test: 1.196585
MAE train: 0.342374	val: 0.952613	test: 0.954223

Epoch: 102
Loss: 0.30563079565763474
RMSE train: 0.517581	val: 1.152826	test: 1.182023
MAE train: 0.405065	val: 0.944339	test: 0.927114

Epoch: 103
Loss: 0.326939232647419
RMSE train: 0.601293	val: 1.255252	test: 1.255562
MAE train: 0.455431	val: 1.006328	test: 0.974168

Epoch: 104
Loss: 0.32357074320316315
RMSE train: 0.477320	val: 1.204338	test: 1.197112
MAE train: 0.367444	val: 0.932530	test: 0.954747

Epoch: 105
Loss: 0.3186812251806259
RMSE train: 0.538305	val: 1.214136	test: 1.231844
MAE train: 0.421125	val: 0.966729	test: 0.961398

Epoch: 106
Loss: 0.3027665466070175
RMSE train: 0.619658	val: 1.332084	test: 1.346725
MAE train: 0.468235	val: 1.050649	test: 1.046398

Epoch: 107
Loss: 0.2732687368988991
RMSE train: 0.553780	val: 1.228707	test: 1.278178
MAE train: 0.425485	val: 1.007772	test: 1.004524

Epoch: 108
Loss: 0.3111370876431465
RMSE train: 0.487923	val: 1.220142	test: 1.262565
MAE train: 0.375900	val: 0.995596	test: 0.994348

Epoch: 109
Loss: 0.32734350115060806
RMSE train: 0.497984	val: 1.176887	test: 1.218375
MAE train: 0.385336	val: 0.965806	test: 0.952048

Epoch: 110
Loss: 0.3089121952652931
RMSE train: 0.521433	val: 1.124158	test: 1.190623
MAE train: 0.404999	val: 0.926815	test: 0.923019

Epoch: 111
Loss: 0.27395017445087433
RMSE train: 0.397313	val: 1.123629	test: 1.176331
MAE train: 0.314564	val: 0.906746	test: 0.950616

Epoch: 112
Loss: 0.31885434687137604
RMSE train: 0.405281	val: 1.117410	test: 1.187737
MAE train: 0.325118	val: 0.892811	test: 0.951720

Epoch: 113
Loss: 0.2944064438343048
RMSE train: 0.505402	val: 1.200787	test: 1.239583
MAE train: 0.402448	val: 0.972873	test: 0.985029

Epoch: 114
Loss: 0.2636079490184784
RMSE train: 0.522698	val: 1.265825	test: 1.268295
MAE train: 0.402497	val: 0.986535	test: 1.006452

Epoch: 115
Loss: 0.279879804700613
RMSE train: 0.459365	val: 1.191036	test: 1.198780
MAE train: 0.357676	val: 0.969886	test: 0.949019

Epoch: 116
Loss: 0.2682344913482666
RMSE train: 0.475253	val: 1.215599	test: 1.210614
MAE train: 0.373213	val: 0.986108	test: 0.946854

Epoch: 117
Loss: 0.2507418990135193
RMSE train: 0.435139	val: 1.163199	test: 1.180091
MAE train: 0.347272	val: 0.936469	test: 0.931974

Epoch: 118
Loss: 0.28128328546881676
RMSE train: 0.427618	val: 1.159498	test: 1.184052
MAE train: 0.341339	val: 0.948295	test: 0.943010

Epoch: 119
Loss: 0.24538179114460945
RMSE train: 0.466705	val: 1.215758	test: 1.219546
MAE train: 0.365772	val: 0.997531	test: 0.972785

Epoch: 120
Loss: 0.2671574577689171
RMSE train: 0.464677	val: 1.223137	test: 1.224944
MAE train: 0.362853	val: 0.985961	test: 0.965463

Epoch: 121
Loss: 0.3062684163451195
RMSE train: 0.431538	val: 1.151858	test: 1.178426
MAE train: 0.343119	val: 0.935204	test: 0.935220

Epoch: 122
Loss: 0.23684535920619965
RMSE train: 0.450281	val: 1.122259	test: 1.175072
MAE train: 0.356401	val: 0.917632	test: 0.924844

Epoch: 123
Loss: 0.24966759979724884
RMSE train: 0.492535	val: 1.204042	test: 1.225213
MAE train: 0.384801	val: 0.972355	test: 0.952991

Epoch: 124
Loss: 0.25273197516798973
RMSE train: 0.475399	val: 1.237520	test: 1.240649
MAE train: 0.375166	val: 0.982465	test: 0.965384

Epoch: 125
Loss: 0.25391727685928345
RMSE train: 0.450018	val: 1.195552	test: 1.203153
MAE train: 0.355942	val: 0.966622	test: 0.938701

Epoch: 126
Loss: 0.22170714661478996
RMSE train: 0.433351	val: 1.154481	test: 1.180647
MAE train: 0.342981	val: 0.932557	test: 0.950236

Epoch: 127
Loss: 0.2546854801476002
RMSE train: 0.450635	val: 1.188891	test: 1.186983
MAE train: 0.356594	val: 0.981228	test: 0.932515

Epoch: 128
Loss: 0.2378932423889637
RMSE train: 0.453850	val: 1.178282	test: 1.166916
MAE train: 0.365780	val: 0.965121	test: 0.925774

Epoch: 129
Loss: 0.2612627036869526
RMSE train: 0.447298	val: 1.164101	test: 1.155738
MAE train: 0.366222	val: 0.943823	test: 0.925124

Epoch: 130
Loss: 0.25136420130729675
RMSE train: 0.464453	val: 1.191558	test: 1.196524
MAE train: 0.376504	val: 0.949033	test: 0.976389

Epoch: 131
Loss: 0.24783764779567719
RMSE train: 0.450299	val: 1.183887	test: 1.194264
MAE train: 0.363165	val: 0.941836	test: 0.958933

Epoch: 132
Loss: 0.25334345549345016
RMSE train: 0.460225	val: 1.221828	test: 1.209295
MAE train: 0.358714	val: 0.994842	test: 0.951276

Epoch: 133
Loss: 0.26898567378520966
RMSE train: 0.487342	val: 1.266336	test: 1.238145
MAE train: 0.373194	val: 1.040229	test: 0.968735

Epoch: 134
Loss: 0.24201565980911255
RMSE train: 0.434891	val: 1.197029	test: 1.182013
MAE train: 0.342337	val: 0.994235	test: 0.921759

Epoch: 135
Loss: 0.21894653514027596
RMSE train: 0.391556	val: 1.135517	test: 1.136231
MAE train: 0.317430	val: 0.922975	test: 0.887987

Epoch: 136
Loss: 0.2553042955696583
RMSE train: 0.434515	val: 1.193811	test: 1.183071
MAE train: 0.340439	val: 0.966976	test: 0.919338

Epoch: 137
Loss: 0.25614871457219124
RMSE train: 0.437674	val: 1.152964	test: 1.150786
MAE train: 0.344033	val: 0.956645	test: 0.906007

Epoch: 138
Loss: 0.2289249487221241
RMSE train: 0.451982	val: 1.157764	test: 1.148390
MAE train: 0.362953	val: 0.961362	test: 0.917709

Epoch: 139
Loss: 0.24545027315616608
RMSE train: 0.392608	val: 1.171105	test: 1.168335
MAE train: 0.320527	val: 0.956447	test: 0.947786

Epoch: 140
Loss: 0.24792950972914696
RMSE train: 0.418003	val: 1.186129	test: 1.167866
MAE train: 0.328799	val: 0.970124	test: 0.924975

Epoch: 141
Loss: 0.23958981037139893
RMSE train: 0.424816	val: 1.199847	test: 1.184896
MAE train: 0.337991	val: 0.954316	test: 0.931649

Epoch: 142
Loss: 0.24332844838500023
RMSE train: 0.456000	val: 1.192638	test: 1.184808
MAE train: 0.363010	val: 0.957070	test: 0.920479

Epoch: 143
Loss: 0.25997018441557884
RMSE train: 0.483079	val: 1.223106	test: 1.214583
MAE train: 0.379251	val: 0.998011	test: 0.935047
MAE train: 0.446797	val: 1.652556	test: 1.682596

Epoch: 84
Loss: 0.40308670699596405
RMSE train: 0.585363	val: 2.001013	test: 2.112947
MAE train: 0.456001	val: 1.629201	test: 1.646240

Epoch: 85
Loss: 0.4334527850151062
RMSE train: 0.580295	val: 1.989413	test: 2.108242
MAE train: 0.444766	val: 1.596546	test: 1.641891

Epoch: 86
Loss: 0.39425595849752426
RMSE train: 0.540135	val: 1.908159	test: 2.023055
MAE train: 0.414231	val: 1.506768	test: 1.584508

Epoch: 87
Loss: 0.353532075881958
RMSE train: 0.607218	val: 1.935777	test: 2.027578
MAE train: 0.467447	val: 1.521655	test: 1.570223

Epoch: 88
Loss: 0.3688522055745125
RMSE train: 0.623880	val: 1.928709	test: 2.014853
MAE train: 0.479687	val: 1.537523	test: 1.549669

Epoch: 89
Loss: 0.4034017100930214
RMSE train: 0.589301	val: 2.136229	test: 2.232476
MAE train: 0.441988	val: 1.692979	test: 1.754935

Epoch: 90
Loss: 0.4247453659772873
RMSE train: 0.495295	val: 2.057515	test: 2.174497
MAE train: 0.382618	val: 1.630534	test: 1.718494

Epoch: 91
Loss: 0.3600399047136307
RMSE train: 0.530695	val: 1.885495	test: 1.986656
MAE train: 0.418332	val: 1.523625	test: 1.549360

Epoch: 92
Loss: 0.3707190081477165
RMSE train: 0.568854	val: 1.869015	test: 1.968661
MAE train: 0.437336	val: 1.483599	test: 1.546969

Epoch: 93
Loss: 0.37153148651123047
RMSE train: 0.560420	val: 1.779158	test: 1.855359
MAE train: 0.430143	val: 1.438665	test: 1.424865

Epoch: 94
Loss: 0.3758537843823433
RMSE train: 0.595086	val: 1.819547	test: 1.887792
MAE train: 0.455882	val: 1.499354	test: 1.445682

Epoch: 95
Loss: 0.37689296901226044
RMSE train: 0.516848	val: 1.764634	test: 1.873400
MAE train: 0.404340	val: 1.427453	test: 1.468051

Epoch: 96
Loss: 0.3799389600753784
RMSE train: 0.554215	val: 1.876008	test: 2.007653
MAE train: 0.425215	val: 1.499612	test: 1.589497

Epoch: 97
Loss: 0.37662310898303986
RMSE train: 0.603362	val: 2.004731	test: 2.133772
MAE train: 0.456867	val: 1.605883	test: 1.686299

Epoch: 98
Loss: 0.36624566465616226
RMSE train: 0.641851	val: 2.163414	test: 2.260464
MAE train: 0.475774	val: 1.737311	test: 1.777233

Epoch: 99
Loss: 0.3427819684147835
RMSE train: 0.566111	val: 2.018931	test: 2.107408
MAE train: 0.425912	val: 1.622416	test: 1.631619

Epoch: 100
Loss: 0.3730427324771881
RMSE train: 0.533080	val: 1.913095	test: 2.013593
MAE train: 0.415190	val: 1.514146	test: 1.580232

Epoch: 101
Loss: 0.37225380539894104
RMSE train: 0.568066	val: 1.938454	test: 2.049410
MAE train: 0.436995	val: 1.531914	test: 1.621294

Epoch: 102
Loss: 0.3020031452178955
RMSE train: 0.664478	val: 1.988797	test: 2.057628
MAE train: 0.505524	val: 1.634345	test: 1.600135

Epoch: 103
Loss: 0.40046218037605286
RMSE train: 0.539956	val: 1.823868	test: 1.949515
MAE train: 0.416866	val: 1.458013	test: 1.533740

Epoch: 104
Loss: 0.32600729167461395
RMSE train: 0.564890	val: 1.807000	test: 1.897853
MAE train: 0.427163	val: 1.464966	test: 1.475177

Epoch: 105
Loss: 0.3157602623105049
RMSE train: 0.581659	val: 1.827007	test: 1.915478
MAE train: 0.439738	val: 1.476361	test: 1.475098

Epoch: 106
Loss: 0.31779175251722336
RMSE train: 0.616290	val: 1.892453	test: 1.976421
MAE train: 0.467114	val: 1.525202	test: 1.512359

Epoch: 107
Loss: 0.37089455127716064
RMSE train: 0.622203	val: 1.918754	test: 1.988949
MAE train: 0.475139	val: 1.568457	test: 1.514917

Epoch: 108
Loss: 0.31987278163433075
RMSE train: 0.553241	val: 1.822551	test: 1.882844
MAE train: 0.424638	val: 1.487839	test: 1.426227

Epoch: 109
Loss: 0.35734041780233383
RMSE train: 0.525932	val: 1.904844	test: 1.982293
MAE train: 0.403627	val: 1.489652	test: 1.542137

Epoch: 110
Loss: 0.2836121693253517
RMSE train: 0.553228	val: 1.726760	test: 1.792558
MAE train: 0.421928	val: 1.378089	test: 1.362300

Epoch: 111
Loss: 0.35758310556411743
RMSE train: 0.546856	val: 1.919414	test: 2.005454
MAE train: 0.422595	val: 1.536181	test: 1.541483

Epoch: 112
Loss: 0.33038971573114395
RMSE train: 0.643057	val: 2.334061	test: 2.407379
MAE train: 0.480993	val: 1.872164	test: 1.887538

Epoch: 113
Loss: 0.3307150900363922
RMSE train: 0.609570	val: 2.024496	test: 2.089695
MAE train: 0.459669	val: 1.653688	test: 1.614605

Epoch: 114
Loss: 0.32538405805826187
RMSE train: 0.535954	val: 1.848966	test: 1.931140
MAE train: 0.407794	val: 1.467622	test: 1.500948

Epoch: 115
Loss: 0.3239918500185013
RMSE train: 0.610662	val: 2.001855	test: 2.081433
MAE train: 0.450325	val: 1.566589	test: 1.659784

Epoch: 116
Loss: 0.2920420542359352
RMSE train: 0.546019	val: 1.571531	test: 1.653438
MAE train: 0.421375	val: 1.265241	test: 1.280251

Epoch: 117
Loss: 0.30238086730241776
RMSE train: 0.544122	val: 1.809276	test: 1.942398
MAE train: 0.413959	val: 1.401251	test: 1.540273

Epoch: 118
Loss: 0.35268982499837875
RMSE train: 0.590085	val: 1.891203	test: 2.000267
MAE train: 0.450640	val: 1.507054	test: 1.564766

Epoch: 119
Loss: 0.2839558869600296
RMSE train: 0.576236	val: 1.996726	test: 2.085595
MAE train: 0.443804	val: 1.619811	test: 1.622986

Epoch: 120
Loss: 0.27191295102238655
RMSE train: 0.613746	val: 2.314405	test: 2.400406
MAE train: 0.468772	val: 1.843225	test: 1.904905

Epoch: 121
Loss: 0.29237230867147446
RMSE train: 0.482904	val: 1.984614	test: 2.082337
MAE train: 0.375727	val: 1.570550	test: 1.660808

Epoch: 122
Loss: 0.2702070251107216
RMSE train: 0.477920	val: 1.844821	test: 1.933501
MAE train: 0.373088	val: 1.475849	test: 1.527503

Epoch: 123
Loss: 0.29916568845510483
RMSE train: 0.550629	val: 2.033240	test: 2.105694
MAE train: 0.415862	val: 1.620872	test: 1.664821

Epoch: 124
Loss: 0.31547967344522476
RMSE train: 0.491664	val: 1.941551	test: 2.028234
MAE train: 0.376392	val: 1.543186	test: 1.592762

Epoch: 125
Loss: 0.26865432411432266
RMSE train: 0.500895	val: 1.927255	test: 2.009103
MAE train: 0.381843	val: 1.570535	test: 1.565066

Epoch: 126
Loss: 0.31894514709711075
RMSE train: 0.502694	val: 1.987659	test: 2.080703
MAE train: 0.383153	val: 1.602811	test: 1.631158

Epoch: 127
Loss: 0.3033393770456314
RMSE train: 0.515533	val: 2.055780	test: 2.153317
MAE train: 0.391005	val: 1.665703	test: 1.690769

Epoch: 128
Loss: 0.2558180019259453
RMSE train: 0.528508	val: 1.869806	test: 1.952105
MAE train: 0.406453	val: 1.533799	test: 1.512727

Epoch: 129
Loss: 0.2671445645391941
RMSE train: 0.470775	val: 1.825117	test: 1.910627
MAE train: 0.367293	val: 1.476970	test: 1.477961

Epoch: 130
Loss: 0.2587374150753021
RMSE train: 0.492459	val: 1.723302	test: 1.782538
MAE train: 0.374930	val: 1.388659	test: 1.376210

Epoch: 131
Loss: 0.272657111287117
RMSE train: 0.523570	val: 1.541712	test: 1.589918
MAE train: 0.393036	val: 1.253545	test: 1.207104

Epoch: 132
Loss: 0.26766514405608177
RMSE train: 0.598625	val: 1.897041	test: 1.939296
MAE train: 0.439044	val: 1.514515	test: 1.489132

Epoch: 133
Loss: 0.2962987869977951
RMSE train: 0.518601	val: 1.753170	test: 1.807917
MAE train: 0.393685	val: 1.398512	test: 1.377938

Epoch: 134
Loss: 0.27590738609433174
RMSE train: 0.457556	val: 1.711752	test: 1.776276
MAE train: 0.355205	val: 1.372285	test: 1.365882

Epoch: 135
Loss: 0.28915996104478836
RMSE train: 0.573436	val: 1.986669	test: 2.054397
MAE train: 0.427712	val: 1.579387	test: 1.628465

Epoch: 136
Loss: 0.2789470851421356
RMSE train: 0.488527	val: 1.763931	test: 1.825905
MAE train: 0.379299	val: 1.418921	test: 1.409666

Epoch: 137
Loss: 0.26974834129214287
RMSE train: 0.471659	val: 1.835060	test: 1.904297
MAE train: 0.359731	val: 1.462485	test: 1.485848

Epoch: 138
Loss: 0.2668137326836586
RMSE train: 0.599734	val: 1.877152	test: 1.915496
MAE train: 0.441140	val: 1.519545	test: 1.457816

Epoch: 139
Loss: 0.2700151577591896
RMSE train: 0.591483	val: 1.874770	test: 1.916323
MAE train: 0.435273	val: 1.506938	test: 1.448414

Epoch: 140
Loss: 0.25032248347997665
RMSE train: 0.495951	val: 1.848938	test: 1.903889
MAE train: 0.366180	val: 1.465908	test: 1.466842

Epoch: 141
Loss: 0.2707274667918682
RMSE train: 0.529723	val: 1.725102	test: 1.765156
MAE train: 0.392935	val: 1.395197	test: 1.330729

Epoch: 142
Loss: 0.2684464380145073
RMSE train: 0.485164	val: 1.715343	test: 1.782924
MAE train: 0.374376	val: 1.361139	test: 1.380398

Epoch: 143
Loss: 0.26312271505594254
RMSE train: 0.424766	val: 1.828097	test: 1.936468
MAE train: 0.329802	val: 1.458330	test: 1.523866
MAE train: 0.409168	val: 1.018285	test: 1.055476

Epoch: 84
Loss: 0.381234772503376
RMSE train: 0.548075	val: 1.252451	test: 1.392134
MAE train: 0.418068	val: 1.026164	test: 1.066962

Epoch: 85
Loss: 0.40172959864139557
RMSE train: 0.470262	val: 1.214527	test: 1.400412
MAE train: 0.365469	val: 0.967768	test: 1.090796

Epoch: 86
Loss: 0.35638415068387985
RMSE train: 0.498463	val: 1.269956	test: 1.449321
MAE train: 0.388816	val: 1.008073	test: 1.123555

Epoch: 87
Loss: 0.34312529116868973
RMSE train: 0.538707	val: 1.324272	test: 1.488804
MAE train: 0.405889	val: 1.062781	test: 1.147468

Epoch: 88
Loss: 0.3280504047870636
RMSE train: 0.531096	val: 1.318151	test: 1.513985
MAE train: 0.409865	val: 1.040617	test: 1.165527

Epoch: 89
Loss: 0.32934414595365524
RMSE train: 0.442535	val: 1.264969	test: 1.456905
MAE train: 0.348093	val: 0.982366	test: 1.132770

Epoch: 90
Loss: 0.4162861108779907
RMSE train: 0.451293	val: 1.252439	test: 1.410675
MAE train: 0.356308	val: 0.975092	test: 1.087467

Epoch: 91
Loss: 0.3553415611386299
RMSE train: 0.537092	val: 1.291460	test: 1.412438
MAE train: 0.414759	val: 1.023434	test: 1.062463

Epoch: 92
Loss: 0.35025521367788315
RMSE train: 0.476376	val: 1.241574	test: 1.367066
MAE train: 0.370574	val: 0.980432	test: 1.052173

Epoch: 93
Loss: 0.3736551031470299
RMSE train: 0.385935	val: 1.183237	test: 1.326713
MAE train: 0.300378	val: 0.957664	test: 1.026902

Epoch: 94
Loss: 0.34690623730421066
RMSE train: 0.550561	val: 1.340791	test: 1.448871
MAE train: 0.420202	val: 1.076030	test: 1.088233

Epoch: 95
Loss: 0.32958506792783737
RMSE train: 0.515481	val: 1.287174	test: 1.397646
MAE train: 0.399182	val: 1.041344	test: 1.044913

Epoch: 96
Loss: 0.3528183028101921
RMSE train: 0.387039	val: 1.206227	test: 1.333477
MAE train: 0.306687	val: 0.955964	test: 1.015256

Epoch: 97
Loss: 0.37186622619628906
RMSE train: 0.431017	val: 1.200374	test: 1.319798
MAE train: 0.326000	val: 0.983960	test: 1.011737

Epoch: 98
Loss: 0.3067694418132305
RMSE train: 0.465014	val: 1.249166	test: 1.376591
MAE train: 0.367215	val: 1.003433	test: 1.054903

Epoch: 99
Loss: 0.31317437440156937
RMSE train: 0.383179	val: 1.183865	test: 1.327754
MAE train: 0.301637	val: 0.950683	test: 1.033868

Epoch: 100
Loss: 0.3303765505552292
RMSE train: 0.425954	val: 1.191430	test: 1.313459
MAE train: 0.331407	val: 0.960885	test: 1.004005

Epoch: 101
Loss: 0.3349953442811966
RMSE train: 0.547170	val: 1.299297	test: 1.410328
MAE train: 0.406271	val: 1.065122	test: 1.060629

Epoch: 102
Loss: 0.30544572323560715
RMSE train: 0.463795	val: 1.280654	test: 1.431016
MAE train: 0.355642	val: 1.028208	test: 1.090271

Epoch: 103
Loss: 0.3662968575954437
RMSE train: 0.482990	val: 1.287786	test: 1.425495
MAE train: 0.370426	val: 1.042065	test: 1.088833

Epoch: 104
Loss: 0.30410219728946686
RMSE train: 0.491111	val: 1.349754	test: 1.477075
MAE train: 0.387694	val: 1.075267	test: 1.120602

Epoch: 105
Loss: 0.2984362915158272
RMSE train: 0.435700	val: 1.290683	test: 1.427791
MAE train: 0.338884	val: 1.042703	test: 1.077965

Epoch: 106
Loss: 0.309709157794714
RMSE train: 0.487556	val: 1.306587	test: 1.436059
MAE train: 0.372195	val: 1.063901	test: 1.087815

Epoch: 107
Loss: 0.3732925355434418
RMSE train: 0.523903	val: 1.317137	test: 1.456007
MAE train: 0.399500	val: 1.031452	test: 1.102763

Epoch: 108
Loss: 0.2892703637480736
RMSE train: 0.406691	val: 1.116826	test: 1.283047
MAE train: 0.320340	val: 0.904992	test: 0.974432

Epoch: 109
Loss: 0.35105787217617035
RMSE train: 0.436860	val: 1.215029	test: 1.373237
MAE train: 0.347069	val: 0.958797	test: 1.043403

Epoch: 110
Loss: 0.29712317883968353
RMSE train: 0.525152	val: 1.198438	test: 1.348816
MAE train: 0.385751	val: 0.992354	test: 1.052389

Epoch: 111
Loss: 0.36300455778837204
RMSE train: 0.400430	val: 1.158072	test: 1.338115
MAE train: 0.307049	val: 0.955953	test: 1.037846

Epoch: 112
Loss: 0.3256729319691658
RMSE train: 0.400969	val: 1.206198	test: 1.334755
MAE train: 0.311477	val: 0.981619	test: 1.026936

Epoch: 113
Loss: 0.30169590562582016
RMSE train: 0.498837	val: 1.186145	test: 1.276277
MAE train: 0.376945	val: 0.964564	test: 0.993641

Epoch: 114
Loss: 0.3115488588809967
RMSE train: 0.406959	val: 1.283986	test: 1.364924
MAE train: 0.322747	val: 1.011347	test: 1.063869

Epoch: 115
Loss: 0.2710767202079296
RMSE train: 0.392566	val: 1.187040	test: 1.281068
MAE train: 0.299369	val: 0.972081	test: 0.986424

Epoch: 116
Loss: 0.33027739077806473
RMSE train: 0.468652	val: 1.252671	test: 1.347309
MAE train: 0.353132	val: 1.023115	test: 1.033186

Epoch: 117
Loss: 0.2743709310889244
RMSE train: 0.472084	val: 1.312011	test: 1.419440
MAE train: 0.363456	val: 1.018634	test: 1.074697

Epoch: 118
Loss: 0.352298840880394
RMSE train: 0.424474	val: 1.159055	test: 1.290743
MAE train: 0.321920	val: 0.944355	test: 0.999608

Epoch: 119
Loss: 0.25386248528957367
RMSE train: 0.534012	val: 1.319013	test: 1.442500
MAE train: 0.408612	val: 1.055674	test: 1.103371

Epoch: 120
Loss: 0.2573893181979656
RMSE train: 0.487673	val: 1.261769	test: 1.392175
MAE train: 0.381240	val: 1.017893	test: 1.068136

Epoch: 121
Loss: 0.2692740373313427
RMSE train: 0.413339	val: 1.205276	test: 1.344128
MAE train: 0.323827	val: 0.963098	test: 1.038848

Epoch: 122
Loss: 0.24162045493721962
RMSE train: 0.474228	val: 1.279931	test: 1.409222
MAE train: 0.363155	val: 1.025684	test: 1.082048

Epoch: 123
Loss: 0.2614053189754486
RMSE train: 0.451009	val: 1.213155	test: 1.345132
MAE train: 0.348870	val: 0.980685	test: 1.031514

Epoch: 124
Loss: 0.2781776189804077
RMSE train: 0.368481	val: 1.214093	test: 1.340079
MAE train: 0.289949	val: 0.957161	test: 1.026440

Epoch: 125
Loss: 0.24799257516860962
RMSE train: 0.422780	val: 1.252429	test: 1.373785
MAE train: 0.327567	val: 1.007183	test: 1.052589

Epoch: 126
Loss: 0.296654611825943
RMSE train: 0.378753	val: 1.186661	test: 1.290627
MAE train: 0.297038	val: 0.932417	test: 1.008063

Epoch: 127
Loss: 0.3105328902602196
RMSE train: 0.370392	val: 1.252687	test: 1.323357
MAE train: 0.293738	val: 0.981555	test: 1.024968

Epoch: 128
Loss: 0.22716398537158966
RMSE train: 0.402757	val: 1.234665	test: 1.335659
MAE train: 0.315611	val: 0.979197	test: 1.025887

Epoch: 129
Loss: 0.2704082913696766
RMSE train: 0.408049	val: 1.248841	test: 1.367728
MAE train: 0.311742	val: 1.010732	test: 1.057780

Epoch: 130
Loss: 0.22513258457183838
RMSE train: 0.400144	val: 1.314625	test: 1.443197
MAE train: 0.310675	val: 1.024309	test: 1.110323

Epoch: 131
Loss: 0.24393047019839287
RMSE train: 0.411657	val: 1.271973	test: 1.375575
MAE train: 0.321489	val: 1.022760	test: 1.059564

Epoch: 132
Loss: 0.21598662808537483
RMSE train: 0.460043	val: 1.248305	test: 1.352611
MAE train: 0.352301	val: 1.007454	test: 1.027978

Epoch: 133
Loss: 0.26587749272584915
RMSE train: 0.401526	val: 1.195464	test: 1.312199
MAE train: 0.314541	val: 0.963638	test: 1.009798

Epoch: 134
Loss: 0.274249866604805
RMSE train: 0.339394	val: 1.176030	test: 1.307772
MAE train: 0.268992	val: 0.943099	test: 1.014324

Epoch: 135
Loss: 0.2662174701690674
RMSE train: 0.441543	val: 1.290851	test: 1.436673
MAE train: 0.340341	val: 1.042605	test: 1.105762

Epoch: 136
Loss: 0.23335427790880203
RMSE train: 0.406149	val: 1.285946	test: 1.438562
MAE train: 0.310370	val: 1.030569	test: 1.104843

Epoch: 137
Loss: 0.2556471712887287
RMSE train: 0.340564	val: 1.246477	test: 1.390635
MAE train: 0.272312	val: 0.973937	test: 1.073885

Epoch: 138
Loss: 0.2600318677723408
RMSE train: 0.420655	val: 1.210491	test: 1.340022
MAE train: 0.319141	val: 0.982268	test: 1.031431

Epoch: 139
Loss: 0.2550179027020931
RMSE train: 0.511114	val: 1.355507	test: 1.476669
MAE train: 0.379576	val: 1.079147	test: 1.124877

Epoch: 140
Loss: 0.23297712206840515
RMSE train: 0.432836	val: 1.267505	test: 1.426614
MAE train: 0.341881	val: 1.005500	test: 1.094223

Epoch: 141
Loss: 0.25297026336193085
RMSE train: 0.361404	val: 1.226568	test: 1.384738
MAE train: 0.286257	val: 0.968780	test: 1.064730

Epoch: 142
Loss: 0.23446547612547874
RMSE train: 0.333149	val: 1.195915	test: 1.318384
MAE train: 0.263938	val: 0.943698	test: 1.014278

Epoch: 143
Loss: 0.22814250737428665
RMSE train: 0.349392	val: 1.160915	test: 1.261508
MAE train: 0.275899	val: 0.930055	test: 0.963246

Early stopping
Best (RMSE):	 train: 0.406691	val: 1.116826	test: 1.283047
Best (MAE):	 train: 0.320340	val: 0.904992	test: 0.974432


Epoch: 144
Loss: 0.24830468371510506
RMSE train: 0.458762	val: 1.226079	test: 1.210196
MAE train: 0.362225	val: 0.956722	test: 0.956329

Epoch: 145
Loss: 0.23141994699835777
RMSE train: 0.444865	val: 1.136989	test: 1.162507
MAE train: 0.355858	val: 0.904429	test: 0.924644

Epoch: 146
Loss: 0.22954369336366653
RMSE train: 0.431641	val: 1.168803	test: 1.181094
MAE train: 0.343190	val: 0.934094	test: 0.927491

Epoch: 147
Loss: 0.19725293666124344
RMSE train: 0.450613	val: 1.171554	test: 1.193711
MAE train: 0.359091	val: 0.938421	test: 0.934812

Early stopping
Best (RMSE):	 train: 0.405281	val: 1.117410	test: 1.187737
Best (MAE):	 train: 0.325118	val: 0.892811	test: 0.951720

MAE train: 0.441245	val: 1.101207	test: 1.229202

Epoch: 84
Loss: 0.4070739075541496
RMSE train: 0.649017	val: 1.406867	test: 1.476584
MAE train: 0.504092	val: 1.125094	test: 1.140214

Epoch: 85
Loss: 0.44827308505773544
RMSE train: 0.546801	val: 1.417210	test: 1.484138
MAE train: 0.424448	val: 1.125003	test: 1.174340

Epoch: 86
Loss: 0.36252231895923615
RMSE train: 0.461460	val: 1.367507	test: 1.448678
MAE train: 0.366868	val: 1.107528	test: 1.189404

Epoch: 87
Loss: 0.3816477656364441
RMSE train: 0.571398	val: 1.339512	test: 1.386424
MAE train: 0.452736	val: 1.072052	test: 1.073061

Epoch: 88
Loss: 0.3583732768893242
RMSE train: 0.524294	val: 1.364960	test: 1.444684
MAE train: 0.415628	val: 1.079159	test: 1.153024

Epoch: 89
Loss: 0.3769754022359848
RMSE train: 0.447618	val: 1.561649	test: 1.686095
MAE train: 0.353486	val: 1.254854	test: 1.369058

Epoch: 90
Loss: 0.4351901486515999
RMSE train: 0.444775	val: 1.485308	test: 1.539740
MAE train: 0.351020	val: 1.189223	test: 1.257423

Epoch: 91
Loss: 0.34463196992874146
RMSE train: 0.536985	val: 1.402597	test: 1.403595
MAE train: 0.418774	val: 1.122222	test: 1.078273

Epoch: 92
Loss: 0.3910527527332306
RMSE train: 0.451275	val: 1.492332	test: 1.507487
MAE train: 0.358260	val: 1.200259	test: 1.230286

Epoch: 93
Loss: 0.368816040456295
RMSE train: 0.414141	val: 1.353844	test: 1.463748
MAE train: 0.328137	val: 1.092542	test: 1.194375

Epoch: 94
Loss: 0.334424264729023
RMSE train: 0.485586	val: 1.304574	test: 1.374994
MAE train: 0.380689	val: 1.046656	test: 1.101424

Epoch: 95
Loss: 0.3240819647908211
RMSE train: 0.430900	val: 1.320520	test: 1.435484
MAE train: 0.342237	val: 1.052620	test: 1.171259

Epoch: 96
Loss: 0.3286251872777939
RMSE train: 0.425239	val: 1.386592	test: 1.561723
MAE train: 0.336505	val: 1.119106	test: 1.269429

Epoch: 97
Loss: 0.38250938802957535
RMSE train: 0.472366	val: 1.352732	test: 1.475405
MAE train: 0.375026	val: 1.094117	test: 1.173393

Epoch: 98
Loss: 0.3419594466686249
RMSE train: 0.483989	val: 1.399323	test: 1.509046
MAE train: 0.390390	val: 1.125690	test: 1.209698

Epoch: 99
Loss: 0.3215196803212166
RMSE train: 0.425812	val: 1.280723	test: 1.411470
MAE train: 0.343149	val: 1.046309	test: 1.149319

Epoch: 100
Loss: 0.33587899804115295
RMSE train: 0.462358	val: 1.352742	test: 1.427637
MAE train: 0.361103	val: 1.068882	test: 1.141730

Epoch: 101
Loss: 0.3367408961057663
RMSE train: 0.478417	val: 1.275050	test: 1.361267
MAE train: 0.374877	val: 1.023079	test: 1.091482

Epoch: 102
Loss: 0.32530613988637924
RMSE train: 0.455299	val: 1.392593	test: 1.505451
MAE train: 0.358799	val: 1.118456	test: 1.217980

Epoch: 103
Loss: 0.373608723282814
RMSE train: 0.494113	val: 1.355545	test: 1.437371
MAE train: 0.389101	val: 1.074673	test: 1.161949

Epoch: 104
Loss: 0.3091675341129303
RMSE train: 0.447306	val: 1.267298	test: 1.348720
MAE train: 0.359739	val: 1.017312	test: 1.096498

Epoch: 105
Loss: 0.31653060764074326
RMSE train: 0.436978	val: 1.408555	test: 1.466156
MAE train: 0.345315	val: 1.130540	test: 1.197088

Epoch: 106
Loss: 0.29884717613458633
RMSE train: 0.563365	val: 1.433009	test: 1.475109
MAE train: 0.436705	val: 1.123304	test: 1.126051

Epoch: 107
Loss: 0.3447793126106262
RMSE train: 0.506987	val: 1.381214	test: 1.455875
MAE train: 0.394271	val: 1.094389	test: 1.161301

Epoch: 108
Loss: 0.2761511728167534
RMSE train: 0.419425	val: 1.376884	test: 1.488765
MAE train: 0.336231	val: 1.107122	test: 1.218143

Epoch: 109
Loss: 0.3366606831550598
RMSE train: 0.479018	val: 1.357502	test: 1.452282
MAE train: 0.381141	val: 1.080525	test: 1.162432

Epoch: 110
Loss: 0.3007681891322136
RMSE train: 0.510189	val: 1.224299	test: 1.348432
MAE train: 0.392950	val: 0.986186	test: 1.097461

Epoch: 111
Loss: 0.33817846328020096
RMSE train: 0.459599	val: 1.288612	test: 1.445225
MAE train: 0.351092	val: 1.053092	test: 1.195836

Epoch: 112
Loss: 0.31423216313123703
RMSE train: 0.520779	val: 1.329265	test: 1.429727
MAE train: 0.410564	val: 1.042229	test: 1.158975

Epoch: 113
Loss: 0.3071647435426712
RMSE train: 0.459711	val: 1.167231	test: 1.306633
MAE train: 0.365443	val: 0.950137	test: 1.064791

Epoch: 114
Loss: 0.3245956227183342
RMSE train: 0.452272	val: 1.689422	test: 1.871766
MAE train: 0.363672	val: 1.362465	test: 1.477351

Epoch: 115
Loss: 0.31660817563533783
RMSE train: 0.405756	val: 1.347163	test: 1.517281
MAE train: 0.328734	val: 1.072428	test: 1.227908

Epoch: 116
Loss: 0.33465951681137085
RMSE train: 0.456580	val: 1.262600	test: 1.430748
MAE train: 0.377578	val: 1.014332	test: 1.161360

Epoch: 117
Loss: 0.24892360344529152
RMSE train: 0.470842	val: 1.654354	test: 1.810300
MAE train: 0.383560	val: 1.314195	test: 1.434753

Epoch: 118
Loss: 0.3235040716826916
RMSE train: 0.434382	val: 1.239482	test: 1.364093
MAE train: 0.350205	val: 1.009319	test: 1.130918

Epoch: 119
Loss: 0.2714296951889992
RMSE train: 0.477267	val: 1.308789	test: 1.394783
MAE train: 0.380303	val: 1.025599	test: 1.109872

Epoch: 120
Loss: 0.2713194414973259
RMSE train: 0.467298	val: 1.368758	test: 1.460424
MAE train: 0.372863	val: 1.084275	test: 1.170983

Epoch: 121
Loss: 0.24554410949349403
RMSE train: 0.426092	val: 1.324069	test: 1.468421
MAE train: 0.336226	val: 1.082789	test: 1.214875

Epoch: 122
Loss: 0.24839653447270393
RMSE train: 0.455235	val: 1.239229	test: 1.368234
MAE train: 0.355903	val: 1.002044	test: 1.095203

Epoch: 123
Loss: 0.25868142768740654
RMSE train: 0.458149	val: 1.334768	test: 1.443523
MAE train: 0.362727	val: 1.057544	test: 1.158919

Epoch: 124
Loss: 0.3042580187320709
RMSE train: 0.403789	val: 1.253235	test: 1.387068
MAE train: 0.318655	val: 1.007093	test: 1.141538

Epoch: 125
Loss: 0.24281563237309456
RMSE train: 0.439334	val: 1.283215	test: 1.400433
MAE train: 0.344907	val: 1.020902	test: 1.122674

Epoch: 126
Loss: 0.2755422070622444
RMSE train: 0.446696	val: 1.340992	test: 1.471491
MAE train: 0.351139	val: 1.065013	test: 1.197269

Epoch: 127
Loss: 0.27810806781053543
RMSE train: 0.424243	val: 1.316985	test: 1.479946
MAE train: 0.334672	val: 1.051332	test: 1.209455

Epoch: 128
Loss: 0.22486011311411858
RMSE train: 0.402924	val: 1.350401	test: 1.512376
MAE train: 0.319519	val: 1.074358	test: 1.233641

Epoch: 129
Loss: 0.2514817826449871
RMSE train: 0.446383	val: 1.399919	test: 1.545610
MAE train: 0.351622	val: 1.100344	test: 1.246200

Epoch: 130
Loss: 0.22717470675706863
RMSE train: 0.439829	val: 1.329832	test: 1.434305
MAE train: 0.343767	val: 1.047092	test: 1.158637

Epoch: 131
Loss: 0.2511182427406311
RMSE train: 0.499269	val: 1.499986	test: 1.583143
MAE train: 0.377205	val: 1.180061	test: 1.254054

Epoch: 132
Loss: 0.23469405621290207
RMSE train: 0.496652	val: 1.413889	test: 1.514123
MAE train: 0.383758	val: 1.142888	test: 1.201649

Epoch: 133
Loss: 0.2489689327776432
RMSE train: 0.432801	val: 1.308583	test: 1.449337
MAE train: 0.340278	val: 1.066920	test: 1.168255

Epoch: 134
Loss: 0.2730353772640228
RMSE train: 0.419074	val: 1.391678	test: 1.538616
MAE train: 0.326776	val: 1.125764	test: 1.249338

Epoch: 135
Loss: 0.2894541434943676
RMSE train: 0.475206	val: 1.372492	test: 1.480168
MAE train: 0.362359	val: 1.100654	test: 1.195866

Epoch: 136
Loss: 0.24457909911870956
RMSE train: 0.434793	val: 1.348222	test: 1.425393
MAE train: 0.343634	val: 1.072939	test: 1.145659

Epoch: 137
Loss: 0.263644240796566
RMSE train: 0.402588	val: 1.500715	test: 1.568287
MAE train: 0.319699	val: 1.187756	test: 1.255890

Epoch: 138
Loss: 0.23934636637568474
RMSE train: 0.459770	val: 1.334889	test: 1.433904
MAE train: 0.350025	val: 1.105893	test: 1.148724

Epoch: 139
Loss: 0.265808317810297
RMSE train: 0.446876	val: 1.394078	test: 1.515608
MAE train: 0.350176	val: 1.116171	test: 1.205474

Epoch: 140
Loss: 0.23952338844537735
RMSE train: 0.415382	val: 1.457668	test: 1.601750
MAE train: 0.325219	val: 1.136756	test: 1.279303

Epoch: 141
Loss: 0.2587185949087143
RMSE train: 0.423230	val: 1.382986	test: 1.492187
MAE train: 0.330780	val: 1.078382	test: 1.207341

Epoch: 142
Loss: 0.22552098706364632
RMSE train: 0.420902	val: 1.456945	test: 1.559550
MAE train: 0.324698	val: 1.160501	test: 1.274704

Epoch: 143
Loss: 0.25104132294654846
RMSE train: 0.397433	val: 1.308813	test: 1.385646
MAE train: 0.311640	val: 1.021747	test: 1.118960
MAE train: 0.365064	val: 1.277968	test: 1.142603

Epoch: 84
Loss: 0.3636191710829735
RMSE train: 0.509863	val: 1.589835	test: 1.521193
MAE train: 0.402260	val: 1.288935	test: 1.166008

Epoch: 85
Loss: 0.3656674548983574
RMSE train: 0.486026	val: 1.564947	test: 1.496508
MAE train: 0.380186	val: 1.235591	test: 1.142066

Epoch: 86
Loss: 0.3944673463702202
RMSE train: 0.467789	val: 1.506608	test: 1.477855
MAE train: 0.368726	val: 1.213881	test: 1.117305

Epoch: 87
Loss: 0.3789735585451126
RMSE train: 0.504276	val: 1.563861	test: 1.554473
MAE train: 0.395989	val: 1.250486	test: 1.165480

Epoch: 88
Loss: 0.3868388906121254
RMSE train: 0.519846	val: 1.550065	test: 1.541231
MAE train: 0.400263	val: 1.226731	test: 1.164071

Epoch: 89
Loss: 0.34877389669418335
RMSE train: 0.467308	val: 1.515647	test: 1.509198
MAE train: 0.367510	val: 1.202548	test: 1.161116

Epoch: 90
Loss: 0.3260049857199192
RMSE train: 0.593689	val: 1.631654	test: 1.637721
MAE train: 0.460760	val: 1.308277	test: 1.245488

Epoch: 91
Loss: 0.34075527638196945
RMSE train: 0.585739	val: 1.582457	test: 1.575332
MAE train: 0.458595	val: 1.256374	test: 1.197356

Epoch: 92
Loss: 0.35148636996746063
RMSE train: 0.515651	val: 1.565313	test: 1.531637
MAE train: 0.407759	val: 1.225642	test: 1.190145

Epoch: 93
Loss: 0.3053518012166023
RMSE train: 0.574106	val: 1.592472	test: 1.575841
MAE train: 0.447729	val: 1.270034	test: 1.211933

Epoch: 94
Loss: 0.3719230145215988
RMSE train: 0.538778	val: 1.535922	test: 1.511358
MAE train: 0.409715	val: 1.214061	test: 1.151122

Epoch: 95
Loss: 0.36907654255628586
RMSE train: 0.472938	val: 1.556563	test: 1.492165
MAE train: 0.365830	val: 1.233294	test: 1.164945

Epoch: 96
Loss: 0.3801684081554413
RMSE train: 0.475040	val: 1.506426	test: 1.495800
MAE train: 0.367814	val: 1.201260	test: 1.139530

Epoch: 97
Loss: 0.32329177111387253
RMSE train: 0.415746	val: 1.482854	test: 1.442372
MAE train: 0.327885	val: 1.175259	test: 1.112209

Epoch: 98
Loss: 0.3316345736384392
RMSE train: 0.391930	val: 1.452371	test: 1.393804
MAE train: 0.308177	val: 1.164975	test: 1.087800

Epoch: 99
Loss: 0.35845357924699783
RMSE train: 0.438242	val: 1.477482	test: 1.427830
MAE train: 0.340782	val: 1.203506	test: 1.099814

Epoch: 100
Loss: 0.31140172481536865
RMSE train: 0.463239	val: 1.615169	test: 1.516343
MAE train: 0.359697	val: 1.250475	test: 1.169167

Epoch: 101
Loss: 0.36757243424654007
RMSE train: 0.533087	val: 1.616594	test: 1.589989
MAE train: 0.413795	val: 1.307518	test: 1.216100

Epoch: 102
Loss: 0.2879963517189026
RMSE train: 0.503431	val: 1.571935	test: 1.559933
MAE train: 0.396195	val: 1.270864	test: 1.195546

Epoch: 103
Loss: 0.3250143751502037
RMSE train: 0.535060	val: 1.641639	test: 1.626162
MAE train: 0.408941	val: 1.303714	test: 1.231842

Epoch: 104
Loss: 0.30843187123537064
RMSE train: 0.514777	val: 1.578334	test: 1.563024
MAE train: 0.390443	val: 1.277417	test: 1.164465

Epoch: 105
Loss: 0.30075571313500404
RMSE train: 0.404593	val: 1.497406	test: 1.465720
MAE train: 0.310785	val: 1.222450	test: 1.119125

Epoch: 106
Loss: 0.28693628311157227
RMSE train: 0.393226	val: 1.411424	test: 1.386332
MAE train: 0.309118	val: 1.166932	test: 1.078475

Epoch: 107
Loss: 0.2831061854958534
RMSE train: 0.472401	val: 1.392023	test: 1.407261
MAE train: 0.365066	val: 1.147715	test: 1.094966

Epoch: 108
Loss: 0.30105236172676086
RMSE train: 0.426076	val: 1.442943	test: 1.428660
MAE train: 0.334832	val: 1.177743	test: 1.092134

Epoch: 109
Loss: 0.3077634871006012
RMSE train: 0.418337	val: 1.449954	test: 1.422655
MAE train: 0.326123	val: 1.165474	test: 1.077053

Epoch: 110
Loss: 0.2796650268137455
RMSE train: 0.555224	val: 1.584390	test: 1.587370
MAE train: 0.421368	val: 1.278691	test: 1.185160

Epoch: 111
Loss: 0.296219602227211
RMSE train: 0.469202	val: 1.490258	test: 1.470192
MAE train: 0.362849	val: 1.183251	test: 1.109730

Epoch: 112
Loss: 0.28523213416337967
RMSE train: 0.470710	val: 1.533761	test: 1.504496
MAE train: 0.368571	val: 1.219549	test: 1.144866

Epoch: 113
Loss: 0.29882100224494934
RMSE train: 0.531631	val: 1.630545	test: 1.610341
MAE train: 0.411228	val: 1.321440	test: 1.213520

Epoch: 114
Loss: 0.2760705463588238
RMSE train: 0.400367	val: 1.525705	test: 1.465087
MAE train: 0.312482	val: 1.213626	test: 1.126833

Epoch: 115
Loss: 0.2644989341497421
RMSE train: 0.368025	val: 1.387171	test: 1.392835
MAE train: 0.292254	val: 1.158912	test: 1.074825

Epoch: 116
Loss: 0.3006894588470459
RMSE train: 0.402947	val: 1.436549	test: 1.427588
MAE train: 0.316447	val: 1.178080	test: 1.082207

Epoch: 117
Loss: 0.2660961486399174
RMSE train: 0.456936	val: 1.497654	test: 1.448636
MAE train: 0.353565	val: 1.201655	test: 1.089408

Epoch: 118
Loss: 0.29592347145080566
RMSE train: 0.376747	val: 1.425384	test: 1.392765
MAE train: 0.296342	val: 1.177786	test: 1.062204

Epoch: 119
Loss: 0.313940592110157
RMSE train: 0.496579	val: 1.538866	test: 1.491207
MAE train: 0.378357	val: 1.227904	test: 1.112094

Epoch: 120
Loss: 0.3006058782339096
RMSE train: 0.493882	val: 1.547070	test: 1.507008
MAE train: 0.379970	val: 1.251997	test: 1.125792

Epoch: 121
Loss: 0.24585526064038277
RMSE train: 0.417667	val: 1.516205	test: 1.461808
MAE train: 0.328262	val: 1.200815	test: 1.118761

Epoch: 122
Loss: 0.34483660012483597
RMSE train: 0.525806	val: 1.615749	test: 1.578327
MAE train: 0.407411	val: 1.307997	test: 1.193120

Epoch: 123
Loss: 0.2816813811659813
RMSE train: 0.587367	val: 1.656956	test: 1.636527
MAE train: 0.450562	val: 1.347301	test: 1.242073

Epoch: 124
Loss: 0.2851686179637909
RMSE train: 0.435805	val: 1.490465	test: 1.462570
MAE train: 0.345005	val: 1.209166	test: 1.108835

Epoch: 125
Loss: 0.25682757049798965
RMSE train: 0.424653	val: 1.512890	test: 1.471785
MAE train: 0.329969	val: 1.212181	test: 1.117524

Epoch: 126
Loss: 0.2699187472462654
RMSE train: 0.393428	val: 1.458357	test: 1.418389
MAE train: 0.306779	val: 1.192167	test: 1.110890

Epoch: 127
Loss: 0.24328604340553284
RMSE train: 0.395466	val: 1.496096	test: 1.453808
MAE train: 0.303375	val: 1.233567	test: 1.124829

Epoch: 128
Loss: 0.2485116869211197
RMSE train: 0.384346	val: 1.473436	test: 1.436011
MAE train: 0.288247	val: 1.187232	test: 1.111169

Epoch: 129
Loss: 0.24543123692274094
RMSE train: 0.459432	val: 1.501503	test: 1.491803
MAE train: 0.342154	val: 1.219791	test: 1.115110

Epoch: 130
Loss: 0.2667333260178566
RMSE train: 0.484911	val: 1.556697	test: 1.544086
MAE train: 0.370186	val: 1.260402	test: 1.152987

Epoch: 131
Loss: 0.25573118031024933
RMSE train: 0.417163	val: 1.494327	test: 1.479662
MAE train: 0.327350	val: 1.213334	test: 1.131491

Epoch: 132
Loss: 0.23855029791593552
RMSE train: 0.372399	val: 1.413336	test: 1.416109
MAE train: 0.292161	val: 1.171683	test: 1.113483

Epoch: 133
Loss: 0.2564290650188923
RMSE train: 0.471679	val: 1.553062	test: 1.520807
MAE train: 0.353859	val: 1.237621	test: 1.162594

Epoch: 134
Loss: 0.24352533742785454
RMSE train: 0.437807	val: 1.522232	test: 1.497001
MAE train: 0.330060	val: 1.234380	test: 1.133063

Epoch: 135
Loss: 0.22187745198607445
RMSE train: 0.356034	val: 1.443541	test: 1.418856
MAE train: 0.277888	val: 1.181859	test: 1.093273

Epoch: 136
Loss: 0.25249509513378143
RMSE train: 0.468312	val: 1.556432	test: 1.525306
MAE train: 0.361237	val: 1.250729	test: 1.148832

Epoch: 137
Loss: 0.2640387788414955
RMSE train: 0.426399	val: 1.525620	test: 1.482879
MAE train: 0.331746	val: 1.239954	test: 1.126779

Epoch: 138
Loss: 0.25473034381866455
RMSE train: 0.413898	val: 1.534902	test: 1.500344
MAE train: 0.325902	val: 1.245852	test: 1.142315

Epoch: 139
Loss: 0.24356068298220634
RMSE train: 0.481308	val: 1.582800	test: 1.549035
MAE train: 0.376307	val: 1.279362	test: 1.176935

Epoch: 140
Loss: 0.21903495118021965
RMSE train: 0.373236	val: 1.479188	test: 1.431408
MAE train: 0.290799	val: 1.205131	test: 1.108578

Epoch: 141
Loss: 0.24978327751159668
RMSE train: 0.424188	val: 1.582090	test: 1.539745
MAE train: 0.327696	val: 1.279720	test: 1.180368

Epoch: 142
Loss: 0.21291923895478249
RMSE train: 0.411887	val: 1.571713	test: 1.531499
MAE train: 0.321939	val: 1.277056	test: 1.182951

Epoch: 143
Loss: 0.2305569015443325
RMSE train: 0.341980	val: 1.447495	test: 1.409613
MAE train: 0.266270	val: 1.202478	test: 1.104010All runs completed.

MAE train: 0.355361	val: 1.016121	test: 1.132778

Epoch: 84
Loss: 0.3940054923295975
RMSE train: 0.552017	val: 1.430737	test: 1.476573
MAE train: 0.426153	val: 1.143775	test: 1.162869

Epoch: 85
Loss: 0.42901889979839325
RMSE train: 0.536137	val: 1.356954	test: 1.406137
MAE train: 0.417273	val: 1.089773	test: 1.095292

Epoch: 86
Loss: 0.3888245150446892
RMSE train: 0.500277	val: 1.331214	test: 1.432479
MAE train: 0.398661	val: 1.049702	test: 1.171628

Epoch: 87
Loss: 0.40010736137628555
RMSE train: 0.537906	val: 1.357788	test: 1.412095
MAE train: 0.417432	val: 1.064932	test: 1.106136

Epoch: 88
Loss: 0.4026791676878929
RMSE train: 0.505906	val: 1.349454	test: 1.404595
MAE train: 0.404376	val: 1.060284	test: 1.116343

Epoch: 89
Loss: 0.3842057064175606
RMSE train: 0.421972	val: 1.352161	test: 1.476857
MAE train: 0.340939	val: 1.074968	test: 1.215395

Epoch: 90
Loss: 0.3692021146416664
RMSE train: 0.486447	val: 1.353805	test: 1.404894
MAE train: 0.380459	val: 1.090260	test: 1.091613

Epoch: 91
Loss: 0.4133273661136627
RMSE train: 0.491994	val: 1.345872	test: 1.386870
MAE train: 0.386389	val: 1.089295	test: 1.098882

Epoch: 92
Loss: 0.38129645586013794
RMSE train: 0.444917	val: 1.362268	test: 1.432624
MAE train: 0.357572	val: 1.074299	test: 1.162614

Epoch: 93
Loss: 0.44768691807985306
RMSE train: 0.507570	val: 1.322483	test: 1.365314
MAE train: 0.398773	val: 1.062767	test: 1.070723

Epoch: 94
Loss: 0.36686379462480545
RMSE train: 0.477266	val: 1.335810	test: 1.380254
MAE train: 0.379800	val: 1.052821	test: 1.089063

Epoch: 95
Loss: 0.33605844527482986
RMSE train: 0.429084	val: 1.344119	test: 1.440491
MAE train: 0.343640	val: 1.056954	test: 1.179702

Epoch: 96
Loss: 0.41058499366045
RMSE train: 0.477174	val: 1.358970	test: 1.426781
MAE train: 0.377905	val: 1.071781	test: 1.124612

Epoch: 97
Loss: 0.3498312495648861
RMSE train: 0.592811	val: 1.471032	test: 1.512999
MAE train: 0.468011	val: 1.166047	test: 1.167348

Epoch: 98
Loss: 0.37298084050416946
RMSE train: 0.395873	val: 1.312671	test: 1.379906
MAE train: 0.318445	val: 1.042693	test: 1.095093

Epoch: 99
Loss: 0.34328557550907135
RMSE train: 0.437802	val: 1.292851	test: 1.361685
MAE train: 0.354972	val: 1.042209	test: 1.075915

Epoch: 100
Loss: 0.3856751248240471
RMSE train: 0.487795	val: 1.375113	test: 1.421867
MAE train: 0.387085	val: 1.077678	test: 1.116165

Epoch: 101
Loss: 0.36588089168071747
RMSE train: 0.442105	val: 1.369962	test: 1.409277
MAE train: 0.350135	val: 1.071142	test: 1.110665

Epoch: 102
Loss: 0.3225830867886543
RMSE train: 0.400962	val: 1.306387	test: 1.354413
MAE train: 0.314760	val: 1.047492	test: 1.065021

Epoch: 103
Loss: 0.3458543047308922
RMSE train: 0.482674	val: 1.337287	test: 1.369689
MAE train: 0.374008	val: 1.058669	test: 1.069168

Epoch: 104
Loss: 0.32552485913038254
RMSE train: 0.450851	val: 1.301745	test: 1.355964
MAE train: 0.352839	val: 1.018708	test: 1.076991

Epoch: 105
Loss: 0.3241172134876251
RMSE train: 0.437482	val: 1.336657	test: 1.382101
MAE train: 0.350149	val: 1.042637	test: 1.098067

Epoch: 106
Loss: 0.34352296590805054
RMSE train: 0.442029	val: 1.380397	test: 1.426564
MAE train: 0.354635	val: 1.091026	test: 1.112965

Epoch: 107
Loss: 0.29667892307043076
RMSE train: 0.477480	val: 1.406867	test: 1.451460
MAE train: 0.377466	val: 1.120519	test: 1.111510

Epoch: 108
Loss: 0.30538035929203033
RMSE train: 0.470451	val: 1.458749	test: 1.497542
MAE train: 0.367103	val: 1.137184	test: 1.171833

Epoch: 109
Loss: 0.3411629945039749
RMSE train: 0.434477	val: 1.295487	test: 1.348664
MAE train: 0.341990	val: 1.026598	test: 1.043648

Epoch: 110
Loss: 0.3454645574092865
RMSE train: 0.434340	val: 1.319357	test: 1.353716
MAE train: 0.336729	val: 1.037281	test: 1.040829

Epoch: 111
Loss: 0.283029280602932
RMSE train: 0.436693	val: 1.402529	test: 1.411787
MAE train: 0.342218	val: 1.079292	test: 1.118181

Epoch: 112
Loss: 0.2813396789133549
RMSE train: 0.404944	val: 1.325805	test: 1.373723
MAE train: 0.319216	val: 1.059965	test: 1.074721

Epoch: 113
Loss: 0.3019174113869667
RMSE train: 0.432599	val: 1.367683	test: 1.417640
MAE train: 0.344784	val: 1.067040	test: 1.117502

Epoch: 114
Loss: 0.3134568780660629
RMSE train: 0.526479	val: 1.454775	test: 1.477079
MAE train: 0.407084	val: 1.124178	test: 1.162680

Epoch: 115
Loss: 0.2844066210091114
RMSE train: 0.486354	val: 1.386575	test: 1.420920
MAE train: 0.378133	val: 1.127812	test: 1.097105

Epoch: 116
Loss: 0.26040324568748474
RMSE train: 0.426707	val: 1.400136	test: 1.428702
MAE train: 0.338885	val: 1.094468	test: 1.122294

Epoch: 117
Loss: 0.31720705330371857
RMSE train: 0.419833	val: 1.352334	test: 1.379032
MAE train: 0.330851	val: 1.076585	test: 1.071342

Epoch: 118
Loss: 0.318882018327713
RMSE train: 0.476522	val: 1.323391	test: 1.354502
MAE train: 0.374951	val: 1.073517	test: 1.053196

Epoch: 119
Loss: 0.29450222849845886
RMSE train: 0.445300	val: 1.384241	test: 1.406586
MAE train: 0.352838	val: 1.075819	test: 1.132902

Epoch: 120
Loss: 0.2769267335534096
RMSE train: 0.388100	val: 1.248430	test: 1.332754
MAE train: 0.316102	val: 0.995921	test: 1.064852

Epoch: 121
Loss: 0.29380568116903305
RMSE train: 0.412359	val: 1.297653	test: 1.358133
MAE train: 0.330551	val: 1.045495	test: 1.066187

Epoch: 122
Loss: 0.2758270278573036
RMSE train: 0.444972	val: 1.309686	test: 1.390544
MAE train: 0.348095	val: 1.026588	test: 1.110077

Epoch: 123
Loss: 0.2676272764801979
RMSE train: 0.434025	val: 1.242161	test: 1.327639
MAE train: 0.337994	val: 0.986593	test: 1.033710

Epoch: 124
Loss: 0.2664651796221733
RMSE train: 0.468221	val: 1.355704	test: 1.422521
MAE train: 0.361020	val: 1.064105	test: 1.124408

Epoch: 125
Loss: 0.2851812094449997
RMSE train: 0.437432	val: 1.365712	test: 1.431748
MAE train: 0.343658	val: 1.065803	test: 1.148714

Epoch: 126
Loss: 0.27084583044052124
RMSE train: 0.420120	val: 1.269915	test: 1.356270
MAE train: 0.336994	val: 1.007663	test: 1.078317

Epoch: 127
Loss: 0.2697392888367176
RMSE train: 0.453679	val: 1.359165	test: 1.404314
MAE train: 0.357388	val: 1.076385	test: 1.099724

Epoch: 128
Loss: 0.25711222365498543
RMSE train: 0.467036	val: 1.361241	test: 1.398159
MAE train: 0.365234	val: 1.091268	test: 1.069356

Epoch: 129
Loss: 0.27316273003816605
RMSE train: 0.438969	val: 1.287199	test: 1.350021
MAE train: 0.345349	val: 1.031046	test: 1.030447

Epoch: 130
Loss: 0.27103259414434433
RMSE train: 0.481641	val: 1.369050	test: 1.419921
MAE train: 0.373126	val: 1.064262	test: 1.109861

Epoch: 131
Loss: 0.2712690606713295
RMSE train: 0.419505	val: 1.358236	test: 1.399437
MAE train: 0.334290	val: 1.076486	test: 1.078575

Epoch: 132
Loss: 0.2604721076786518
RMSE train: 0.379381	val: 1.287810	test: 1.351168
MAE train: 0.305845	val: 1.026702	test: 1.066133

Epoch: 133
Loss: 0.3033773824572563
RMSE train: 0.429070	val: 1.341907	test: 1.389157
MAE train: 0.335797	val: 1.055835	test: 1.081845

Epoch: 134
Loss: 0.22777358815073967
RMSE train: 0.412038	val: 1.314566	test: 1.377969
MAE train: 0.321471	val: 1.039480	test: 1.067752

Epoch: 135
Loss: 0.25054556876420975
RMSE train: 0.390886	val: 1.228934	test: 1.323609
MAE train: 0.308683	val: 0.981368	test: 1.039631

Epoch: 136
Loss: 0.25784285366535187
RMSE train: 0.419749	val: 1.380025	test: 1.445886
MAE train: 0.330411	val: 1.076988	test: 1.163802

Epoch: 137
Loss: 0.2712922729551792
RMSE train: 0.443250	val: 1.317860	test: 1.396355
MAE train: 0.344895	val: 1.037884	test: 1.093388

Epoch: 138
Loss: 0.24842768907546997
RMSE train: 0.485775	val: 1.327520	test: 1.399245
MAE train: 0.370319	val: 1.064179	test: 1.067303

Epoch: 139
Loss: 0.2832724377512932
RMSE train: 0.407614	val: 1.318396	test: 1.428215
MAE train: 0.319264	val: 1.036257	test: 1.145327

Epoch: 140
Loss: 0.2736516259610653
RMSE train: 0.462559	val: 1.315106	test: 1.380495
MAE train: 0.362493	val: 1.064639	test: 1.060355

Epoch: 141
Loss: 0.29972995817661285
RMSE train: 0.461413	val: 1.367537	test: 1.401752
MAE train: 0.361415	val: 1.074729	test: 1.061261

Epoch: 142
Loss: 0.23351005837321281
RMSE train: 0.392667	val: 1.387042	test: 1.402837
MAE train: 0.303717	val: 1.068893	test: 1.108920

Epoch: 143
Loss: 0.2720932215452194
RMSE train: 0.421391	val: 1.297601	test: 1.344546
MAE train: 0.327614	val: 1.030822	test: 1.023247

Epoch: 144
Loss: 0.23694439232349396
RMSE train: 0.494298	val: 1.420607	test: 1.448911
MAE train: 0.378974	val: 1.116086	test: 1.155722

Epoch: 145
Loss: 0.23047971352934837
RMSE train: 0.498782	val: 1.531768	test: 1.494211
MAE train: 0.382761	val: 1.204597	test: 1.196899

Epoch: 146
Loss: 0.2295725718140602
RMSE train: 0.430621	val: 1.500952	test: 1.457489
MAE train: 0.337607	val: 1.189767	test: 1.167679

Epoch: 147
Loss: 0.2730390653014183
RMSE train: 0.498837	val: 1.396541	test: 1.445558
MAE train: 0.376836	val: 1.113484	test: 1.120834

Epoch: 148
Loss: 0.2267940491437912
RMSE train: 0.421708	val: 1.462448	test: 1.470184
MAE train: 0.326393	val: 1.169825	test: 1.203657

Early stopping
Best (RMSE):	 train: 0.459711	val: 1.167231	test: 1.306633
Best (MAE):	 train: 0.365443	val: 0.950137	test: 1.064791


Epoch: 144
Loss: 0.1987658552825451
RMSE train: 0.364467	val: 1.468076	test: 1.410135
MAE train: 0.285323	val: 1.199973	test: 1.089072

Epoch: 145
Loss: 0.27309633418917656
RMSE train: 0.374023	val: 1.469524	test: 1.421733
MAE train: 0.290058	val: 1.200151	test: 1.088394

Epoch: 146
Loss: 0.22962597757577896
RMSE train: 0.320560	val: 1.426351	test: 1.380524
MAE train: 0.250860	val: 1.165793	test: 1.059601

Epoch: 147
Loss: 0.224280446767807
RMSE train: 0.454383	val: 1.567849	test: 1.514546
MAE train: 0.352402	val: 1.257788	test: 1.146125

Epoch: 148
Loss: 0.20211782678961754
RMSE train: 0.427068	val: 1.600706	test: 1.558963
MAE train: 0.319598	val: 1.296788	test: 1.181793

Epoch: 149
Loss: 0.29641974717378616
RMSE train: 0.350441	val: 1.503277	test: 1.450037
MAE train: 0.271512	val: 1.232310	test: 1.132947

Epoch: 150
Loss: 0.23436186835169792
RMSE train: 0.432768	val: 1.489571	test: 1.465964
MAE train: 0.325142	val: 1.215579	test: 1.134575

Early stopping
Best (RMSE):	 train: 0.368025	val: 1.387171	test: 1.392835
Best (MAE):	 train: 0.292254	val: 1.158912	test: 1.074825


Epoch: 144
Loss: 0.2460612915456295
RMSE train: 0.538149	val: 1.939936	test: 1.969189
MAE train: 0.407381	val: 1.595463	test: 1.531637

Epoch: 145
Loss: 0.2673565559089184
RMSE train: 0.506885	val: 1.783006	test: 1.854488
MAE train: 0.398194	val: 1.419105	test: 1.447893

Epoch: 146
Loss: 0.2511965222656727
RMSE train: 0.513474	val: 1.761049	test: 1.838628
MAE train: 0.391524	val: 1.397793	test: 1.440815

Epoch: 147
Loss: 0.3074032589793205
RMSE train: 0.614408	val: 1.859186	test: 1.901260
MAE train: 0.454181	val: 1.514047	test: 1.445294

Epoch: 148
Loss: 0.25301535055041313
RMSE train: 0.545217	val: 1.931186	test: 2.000182
MAE train: 0.406174	val: 1.551145	test: 1.551519

Epoch: 149
Loss: 0.2503829151391983
RMSE train: 0.573692	val: 1.888168	test: 1.941235
MAE train: 0.433515	val: 1.531323	test: 1.486518

Epoch: 150
Loss: 0.2729727402329445
RMSE train: 0.559428	val: 1.685211	test: 1.724409
MAE train: 0.430412	val: 1.378559	test: 1.303537

Epoch: 151
Loss: 0.24676990881562233
RMSE train: 0.483609	val: 1.840057	test: 1.901069
MAE train: 0.372576	val: 1.470019	test: 1.483570

Epoch: 152
Loss: 0.23450055345892906
RMSE train: 0.523608	val: 1.852057	test: 1.911216
MAE train: 0.398025	val: 1.489882	test: 1.482286

Epoch: 153
Loss: 0.22336497157812119
RMSE train: 0.487455	val: 1.650939	test: 1.712876
MAE train: 0.372938	val: 1.342008	test: 1.313904

Epoch: 154
Loss: 0.2521219439804554
RMSE train: 0.486870	val: 1.935490	test: 1.998212
MAE train: 0.371903	val: 1.572354	test: 1.551125

Epoch: 155
Loss: 0.21855682134628296
RMSE train: 0.453723	val: 1.903373	test: 1.973894
MAE train: 0.349625	val: 1.544860	test: 1.535731

Epoch: 156
Loss: 0.2700044922530651
RMSE train: 0.452923	val: 1.717160	test: 1.783476
MAE train: 0.341407	val: 1.407985	test: 1.357973

Epoch: 157
Loss: 0.254523828625679
RMSE train: 0.512917	val: 1.826171	test: 1.870815
MAE train: 0.386294	val: 1.491836	test: 1.413470

Epoch: 158
Loss: 0.24521898105740547
RMSE train: 0.412964	val: 1.770126	test: 1.892694
MAE train: 0.321197	val: 1.382965	test: 1.441449

Epoch: 159
Loss: 0.23954414576292038
RMSE train: 0.502908	val: 1.569700	test: 1.636811
MAE train: 0.386286	val: 1.263176	test: 1.238501

Epoch: 160
Loss: 0.26588018238544464
RMSE train: 0.590937	val: 1.746762	test: 1.808980
MAE train: 0.440142	val: 1.416421	test: 1.365836

Epoch: 161
Loss: 0.23481854423880577
RMSE train: 0.591536	val: 1.984152	test: 2.082299
MAE train: 0.443390	val: 1.591144	test: 1.613695

Epoch: 162
Loss: 0.28462090343236923
RMSE train: 0.585334	val: 1.907408	test: 1.986119
MAE train: 0.441583	val: 1.558197	test: 1.513556

Epoch: 163
Loss: 0.221792072057724
RMSE train: 0.512715	val: 2.013183	test: 2.111544
MAE train: 0.387101	val: 1.619867	test: 1.635447

Epoch: 164
Loss: 0.21701868996024132
RMSE train: 0.538445	val: 2.006692	test: 2.096840
MAE train: 0.400922	val: 1.606470	test: 1.630590

Epoch: 165
Loss: 0.23299750685691833
RMSE train: 0.538942	val: 1.834108	test: 1.886377
MAE train: 0.403392	val: 1.470169	test: 1.450573

Epoch: 166
Loss: 0.21535971015691757
RMSE train: 0.440453	val: 1.714523	test: 1.779960
MAE train: 0.349035	val: 1.370229	test: 1.363796

Early stopping
Best (RMSE):	 train: 0.523570	val: 1.541712	test: 1.589918
Best (MAE):	 train: 0.393036	val: 1.253545	test: 1.207104


Epoch: 144
Loss: 0.25906145572662354
RMSE train: 0.448171	val: 1.422114	test: 1.534304
MAE train: 0.339210	val: 1.067532	test: 1.137511

Epoch: 145
Loss: 0.25779516994953156
RMSE train: 0.412269	val: 1.368197	test: 1.489268
MAE train: 0.319339	val: 1.041879	test: 1.101722

Epoch: 146
Loss: 0.25534723699092865
RMSE train: 0.347171	val: 1.317591	test: 1.461281
MAE train: 0.274849	val: 0.999226	test: 1.104602

Epoch: 147
Loss: 0.24435363337397575
RMSE train: 0.428213	val: 1.383253	test: 1.500816
MAE train: 0.334487	val: 1.028418	test: 1.145162

Epoch: 148
Loss: 0.31672443449497223
RMSE train: 0.496410	val: 1.409036	test: 1.523334
MAE train: 0.388133	val: 1.066519	test: 1.128677

Epoch: 149
Loss: 0.26075729355216026
RMSE train: 0.398173	val: 1.295167	test: 1.454738
MAE train: 0.322503	val: 0.978276	test: 1.102170

Epoch: 150
Loss: 0.3396749384701252
RMSE train: 0.386532	val: 1.340031	test: 1.490027
MAE train: 0.300844	val: 1.015479	test: 1.106955

Epoch: 151
Loss: 0.22923579066991806
RMSE train: 0.534799	val: 1.517142	test: 1.625070
MAE train: 0.401309	val: 1.179284	test: 1.218278

Epoch: 152
Loss: 0.22876079753041267
RMSE train: 0.443765	val: 1.385616	test: 1.522109
MAE train: 0.342019	val: 1.058850	test: 1.119320

Epoch: 153
Loss: 0.2688363194465637
RMSE train: 0.386441	val: 1.324152	test: 1.478732
MAE train: 0.305755	val: 1.013423	test: 1.097480

Epoch: 154
Loss: 0.23687110468745232
RMSE train: 0.483793	val: 1.395361	test: 1.511574
MAE train: 0.373409	val: 1.067088	test: 1.122094

Epoch: 155
Loss: 0.235972810536623
RMSE train: 0.433293	val: 1.341731	test: 1.450253
MAE train: 0.344316	val: 1.038715	test: 1.095331

Epoch: 156
Loss: 0.2433139979839325
RMSE train: 0.384578	val: 1.261368	test: 1.398323
MAE train: 0.314460	val: 1.005653	test: 1.062924

Epoch: 157
Loss: 0.22930171713232994
RMSE train: 0.387571	val: 1.291853	test: 1.415169
MAE train: 0.304518	val: 1.012008	test: 1.068271

Epoch: 158
Loss: 0.23441587015986443
RMSE train: 0.441885	val: 1.358346	test: 1.470274
MAE train: 0.331472	val: 1.053867	test: 1.099519

Epoch: 159
Loss: 0.2514296844601631
RMSE train: 0.427795	val: 1.347353	test: 1.468465
MAE train: 0.319259	val: 1.046959	test: 1.107493

Epoch: 160
Loss: 0.25237206742167473
RMSE train: 0.381931	val: 1.351557	test: 1.466248
MAE train: 0.300094	val: 1.033042	test: 1.129498

Epoch: 161
Loss: 0.28896741569042206
RMSE train: 0.423726	val: 1.375724	test: 1.487702
MAE train: 0.329525	val: 1.051931	test: 1.108535

Epoch: 162
Loss: 0.23070170357823372
RMSE train: 0.443906	val: 1.343905	test: 1.482899
MAE train: 0.338006	val: 1.055630	test: 1.113752

Epoch: 163
Loss: 0.24215716868638992
RMSE train: 0.408748	val: 1.340695	test: 1.445571
MAE train: 0.315956	val: 1.010279	test: 1.073529

Epoch: 164
Loss: 0.24686765298247337
RMSE train: 0.434903	val: 1.219840	test: 1.360444
MAE train: 0.360445	val: 0.973107	test: 1.031861

Epoch: 165
Loss: 0.25353503972291946
RMSE train: 0.388242	val: 1.264062	test: 1.385160
MAE train: 0.310116	val: 0.985151	test: 1.047272

Epoch: 166
Loss: 0.2161116972565651
RMSE train: 0.391372	val: 1.335971	test: 1.436394
MAE train: 0.309087	val: 1.019546	test: 1.083140

Epoch: 167
Loss: 0.23475417122244835
RMSE train: 0.375698	val: 1.308956	test: 1.422273
MAE train: 0.293466	val: 1.003206	test: 1.082891

Epoch: 168
Loss: 0.20962268114089966
RMSE train: 0.407275	val: 1.283984	test: 1.410529
MAE train: 0.330833	val: 1.005237	test: 1.066295

Epoch: 169
Loss: 0.2734251581132412
RMSE train: 0.447351	val: 1.451466	test: 1.560867
MAE train: 0.348321	val: 1.136814	test: 1.175374

Epoch: 170
Loss: 0.2117713838815689
RMSE train: 0.418106	val: 1.469409	test: 1.554492
MAE train: 0.332131	val: 1.149610	test: 1.178627

Epoch: 171
Loss: 0.21465856581926346
RMSE train: 0.396351	val: 1.377563	test: 1.477331
MAE train: 0.320601	val: 1.074104	test: 1.114779

Epoch: 172
Loss: 0.18835613876581192
RMSE train: 0.434307	val: 1.405895	test: 1.493014
MAE train: 0.342140	val: 1.091144	test: 1.128360

Epoch: 173
Loss: 0.2157779149711132
RMSE train: 0.429478	val: 1.427953	test: 1.533040
MAE train: 0.328865	val: 1.127434	test: 1.164129

Epoch: 174
Loss: 0.2261049747467041
RMSE train: 0.422671	val: 1.375012	test: 1.472784
MAE train: 0.333057	val: 1.071362	test: 1.132402

Epoch: 175
Loss: 0.2509354017674923
RMSE train: 0.411178	val: 1.420030	test: 1.478954
MAE train: 0.323565	val: 1.101967	test: 1.158539

Epoch: 176
Loss: 0.24726203456521034
RMSE train: 0.398551	val: 1.432937	test: 1.496838
MAE train: 0.302785	val: 1.105719	test: 1.156225

Epoch: 177
Loss: 0.25656695663928986
RMSE train: 0.482452	val: 1.510196	test: 1.602351
MAE train: 0.366108	val: 1.200189	test: 1.215106

Epoch: 178
Loss: 0.22973109781742096
RMSE train: 0.503167	val: 1.554600	test: 1.654091
MAE train: 0.382945	val: 1.223167	test: 1.255219

Epoch: 179
Loss: 0.23594188317656517
RMSE train: 0.389705	val: 1.294304	test: 1.434308
MAE train: 0.307020	val: 0.998709	test: 1.089837

Epoch: 180
Loss: 0.22154241055250168
RMSE train: 0.418872	val: 1.331256	test: 1.470159
MAE train: 0.323037	val: 1.044180	test: 1.114686

Epoch: 181
Loss: 0.21606994792819023
RMSE train: 0.444086	val: 1.413796	test: 1.507761
MAE train: 0.342662	val: 1.106121	test: 1.136555

Epoch: 182
Loss: 0.20269129052758217
RMSE train: 0.402488	val: 1.416677	test: 1.474307
MAE train: 0.319786	val: 1.088937	test: 1.124385

Epoch: 183
Loss: 0.2122201956808567
RMSE train: 0.409153	val: 1.379958	test: 1.440072
MAE train: 0.328235	val: 1.072758	test: 1.117983

Epoch: 184
Loss: 0.20378482341766357
RMSE train: 0.415904	val: 1.393196	test: 1.442473
MAE train: 0.323389	val: 1.070974	test: 1.109279

Epoch: 185
Loss: 0.20990639179944992
RMSE train: 0.398687	val: 1.410762	test: 1.464910
MAE train: 0.310499	val: 1.084672	test: 1.125866

Epoch: 186
Loss: 0.20233247056603432
RMSE train: 0.408956	val: 1.397488	test: 1.502784
MAE train: 0.318376	val: 1.059821	test: 1.139489

Epoch: 187
Loss: 0.24492176622152328
RMSE train: 0.450634	val: 1.438267	test: 1.555530
MAE train: 0.341175	val: 1.090645	test: 1.167187

Epoch: 188
Loss: 0.22671091929078102
RMSE train: 0.444787	val: 1.401599	test: 1.549266
MAE train: 0.335193	val: 1.112961	test: 1.175375

Epoch: 189
Loss: 0.24044129997491837
RMSE train: 0.419620	val: 1.406765	test: 1.486867
MAE train: 0.340632	val: 1.084432	test: 1.129548

Epoch: 190
Loss: 0.22499417513608932
RMSE train: 0.424296	val: 1.378420	test: 1.474376
MAE train: 0.346420	val: 1.078049	test: 1.160332

Epoch: 191
Loss: 0.30056072771549225
RMSE train: 0.438630	val: 1.270999	test: 1.382612
MAE train: 0.361223	val: 1.014326	test: 1.072216

Epoch: 192
Loss: 0.2685094550251961
RMSE train: 0.441841	val: 1.321774	test: 1.394660
MAE train: 0.361909	val: 1.031821	test: 1.088430

Epoch: 193
Loss: 0.20102820172905922
RMSE train: 0.437411	val: 1.388847	test: 1.439528
MAE train: 0.364539	val: 1.088655	test: 1.146229

Epoch: 194
Loss: 0.21408408135175705
RMSE train: 0.438631	val: 1.240587	test: 1.364052
MAE train: 0.362882	val: 1.004368	test: 1.080269

Epoch: 195
Loss: 0.21144504845142365
RMSE train: 0.410822	val: 1.276706	test: 1.399912
MAE train: 0.322899	val: 1.009061	test: 1.066758

Epoch: 196
Loss: 0.22869546338915825
RMSE train: 0.403927	val: 1.287447	test: 1.423518
MAE train: 0.310001	val: 1.010817	test: 1.080644

Epoch: 197
Loss: 0.20767875388264656
RMSE train: 0.396057	val: 1.297791	test: 1.438987
MAE train: 0.307000	val: 1.005902	test: 1.117351

Epoch: 198
Loss: 0.23123674467206
RMSE train: 0.423398	val: 1.304247	test: 1.447423
MAE train: 0.321530	val: 1.011709	test: 1.110371

Epoch: 199
Loss: 0.1977756842970848
RMSE train: 0.405780	val: 1.347075	test: 1.484787
MAE train: 0.306358	val: 1.070879	test: 1.124267

Early stopping
Best (RMSE):	 train: 0.434903	val: 1.219840	test: 1.360444
Best (MAE):	 train: 0.360445	val: 0.973107	test: 1.031861
All runs completed.


Epoch: 144
Loss: 0.24636508524417877
RMSE train: 0.482733	val: 1.409589	test: 1.422642
MAE train: 0.371932	val: 1.079321	test: 1.099035

Epoch: 145
Loss: 0.242923803627491
RMSE train: 0.369530	val: 1.223216	test: 1.310509
MAE train: 0.296104	val: 0.963582	test: 1.035467

Epoch: 146
Loss: 0.2542690336704254
RMSE train: 0.356636	val: 1.222674	test: 1.326158
MAE train: 0.284955	val: 0.958416	test: 1.045298

Epoch: 147
Loss: 0.23966265842318535
RMSE train: 0.548264	val: 1.374646	test: 1.442162
MAE train: 0.417110	val: 1.072938	test: 1.105848

Epoch: 148
Loss: 0.30377088487148285
RMSE train: 0.458275	val: 1.279058	test: 1.361058
MAE train: 0.360297	val: 1.020918	test: 1.040203

Epoch: 149
Loss: 0.24479719996452332
RMSE train: 0.353139	val: 1.220387	test: 1.320916
MAE train: 0.283925	val: 0.961866	test: 1.068774

Epoch: 150
Loss: 0.3237723372876644
RMSE train: 0.427145	val: 1.326387	test: 1.384321
MAE train: 0.331834	val: 1.032618	test: 1.108881

Epoch: 151
Loss: 0.21147197112441063
RMSE train: 0.491042	val: 1.289193	test: 1.346933
MAE train: 0.382701	val: 1.035366	test: 1.042775

Epoch: 152
Loss: 0.24243061989545822
RMSE train: 0.411739	val: 1.233261	test: 1.308264
MAE train: 0.329767	val: 0.973560	test: 1.060089

Epoch: 153
Loss: 0.2567579485476017
RMSE train: 0.374770	val: 1.269732	test: 1.331655
MAE train: 0.295742	val: 0.990207	test: 1.067259

Epoch: 154
Loss: 0.2192482426762581
RMSE train: 0.457838	val: 1.269291	test: 1.325453
MAE train: 0.352075	val: 1.020248	test: 1.028645

Epoch: 155
Loss: 0.2231963388621807
RMSE train: 0.412841	val: 1.277912	test: 1.341982
MAE train: 0.314798	val: 0.997069	test: 1.050390

Epoch: 156
Loss: 0.249803077429533
RMSE train: 0.369600	val: 1.303851	test: 1.372471
MAE train: 0.291246	val: 1.015101	test: 1.073715

Epoch: 157
Loss: 0.2334589548408985
RMSE train: 0.417177	val: 1.336073	test: 1.418254
MAE train: 0.332595	val: 1.045398	test: 1.105575

Epoch: 158
Loss: 0.22742849588394165
RMSE train: 0.465933	val: 1.348678	test: 1.416966
MAE train: 0.363990	val: 1.081787	test: 1.070574

Epoch: 159
Loss: 0.23277417570352554
RMSE train: 0.382389	val: 1.276045	test: 1.350749
MAE train: 0.302857	val: 1.016175	test: 1.043844

Epoch: 160
Loss: 0.2357897162437439
RMSE train: 0.407573	val: 1.426318	test: 1.464721
MAE train: 0.326768	val: 1.128069	test: 1.173671

Epoch: 161
Loss: 0.2770213857293129
RMSE train: 0.462436	val: 1.323333	test: 1.378796
MAE train: 0.362675	val: 1.062353	test: 1.037113

Epoch: 162
Loss: 0.21028363704681396
RMSE train: 0.390347	val: 1.266745	test: 1.323108
MAE train: 0.307783	val: 1.007429	test: 1.016995

Epoch: 163
Loss: 0.23359601944684982
RMSE train: 0.357123	val: 1.322384	test: 1.357494
MAE train: 0.280334	val: 1.033729	test: 1.088413

Epoch: 164
Loss: 0.21330104768276215
RMSE train: 0.396793	val: 1.237883	test: 1.278585
MAE train: 0.319924	val: 0.983697	test: 0.994022

Epoch: 165
Loss: 0.24020738154649734
RMSE train: 0.364233	val: 1.305043	test: 1.320652
MAE train: 0.289213	val: 1.004280	test: 1.038974

Epoch: 166
Loss: 0.221102025359869
RMSE train: 0.394625	val: 1.272768	test: 1.319084
MAE train: 0.307710	val: 0.997506	test: 1.004279

Epoch: 167
Loss: 0.23565636947751045
RMSE train: 0.414345	val: 1.268627	test: 1.325015
MAE train: 0.322612	val: 0.983349	test: 1.029424

Epoch: 168
Loss: 0.2078835479915142
RMSE train: 0.433011	val: 1.286110	test: 1.341793
MAE train: 0.336008	val: 0.994202	test: 1.043309

Epoch: 169
Loss: 0.23921168595552444
RMSE train: 0.462999	val: 1.326937	test: 1.381673
MAE train: 0.358554	val: 1.052731	test: 1.048829

Epoch: 170
Loss: 0.1904461905360222
RMSE train: 0.433020	val: 1.352491	test: 1.392624
MAE train: 0.335919	val: 1.059205	test: 1.089964

Epoch: 171
Loss: 0.23328755795955658
RMSE train: 0.395545	val: 1.274285	test: 1.343841
MAE train: 0.312408	val: 1.013911	test: 1.049590

Epoch: 172
Loss: 0.2164713591337204
RMSE train: 0.481559	val: 1.365203	test: 1.416102
MAE train: 0.365181	val: 1.075807	test: 1.094080

Epoch: 173
Loss: 0.19127946719527245
RMSE train: 0.392423	val: 1.289272	test: 1.345434
MAE train: 0.307557	val: 1.035563	test: 1.062191

Epoch: 174
Loss: 0.20900370925664902
RMSE train: 0.399110	val: 1.385667	test: 1.416866
MAE train: 0.300783	val: 1.083926	test: 1.111525

Epoch: 175
Loss: 0.2572580724954605
RMSE train: 0.421556	val: 1.367218	test: 1.410489
MAE train: 0.324538	val: 1.075549	test: 1.105647

Epoch: 176
Loss: 0.1993517056107521
RMSE train: 0.387694	val: 1.304733	test: 1.372529
MAE train: 0.309368	val: 1.041061	test: 1.067030

Epoch: 177
Loss: 0.21688834577798843
RMSE train: 0.410648	val: 1.381842	test: 1.426005
MAE train: 0.324859	val: 1.102126	test: 1.087892

Epoch: 178
Loss: 0.18781306967139244
RMSE train: 0.403374	val: 1.333820	test: 1.378154
MAE train: 0.324065	val: 1.066405	test: 1.036308

Epoch: 179
Loss: 0.20766832306981087
RMSE train: 0.395183	val: 1.212695	test: 1.291422
MAE train: 0.319385	val: 0.967617	test: 1.025517

Epoch: 180
Loss: 0.20010314136743546
RMSE train: 0.437682	val: 1.264434	test: 1.324171
MAE train: 0.346932	val: 0.997512	test: 1.017336

Epoch: 181
Loss: 0.19925964623689651
RMSE train: 0.364874	val: 1.267381	test: 1.350591
MAE train: 0.296118	val: 0.991706	test: 1.065363

Epoch: 182
Loss: 0.18158897012472153
RMSE train: 0.386945	val: 1.232223	test: 1.318579
MAE train: 0.314449	val: 0.979774	test: 1.019361

Epoch: 183
Loss: 0.20226243138313293
RMSE train: 0.446466	val: 1.259119	test: 1.337097
MAE train: 0.351552	val: 0.993431	test: 1.033686

Epoch: 184
Loss: 0.2266797423362732
RMSE train: 0.387817	val: 1.226556	test: 1.312752
MAE train: 0.303127	val: 0.969983	test: 1.033168

Epoch: 185
Loss: 0.1818593107163906
RMSE train: 0.340040	val: 1.345080	test: 1.410267
MAE train: 0.264760	val: 1.056988	test: 1.113922

Epoch: 186
Loss: 0.20413298159837723
RMSE train: 0.380098	val: 1.388779	test: 1.433147
MAE train: 0.301485	val: 1.089274	test: 1.122515

Epoch: 187
Loss: 0.2218070738017559
RMSE train: 0.413130	val: 1.281840	test: 1.331174
MAE train: 0.328384	val: 1.021445	test: 1.015658

Epoch: 188
Loss: 0.20248138159513474
RMSE train: 0.355009	val: 1.257301	test: 1.311468
MAE train: 0.285477	val: 1.000437	test: 1.014864

Epoch: 189
Loss: 0.2226402387022972
RMSE train: 0.371861	val: 1.376078	test: 1.411010
MAE train: 0.293228	val: 1.073209	test: 1.117269

Epoch: 190
Loss: 0.17886226251721382
RMSE train: 0.388953	val: 1.290823	test: 1.351836
MAE train: 0.307947	val: 1.029833	test: 1.058986

Epoch: 191
Loss: 0.2689337395131588
RMSE train: 0.404629	val: 1.314694	test: 1.365236
MAE train: 0.313396	val: 1.050007	test: 1.070310

Epoch: 192
Loss: 0.23765171319246292
RMSE train: 0.400922	val: 1.306710	test: 1.388808
MAE train: 0.309049	val: 1.021306	test: 1.112606

Epoch: 193
Loss: 0.17958903312683105
RMSE train: 0.390439	val: 1.232581	test: 1.314226
MAE train: 0.312710	val: 0.992139	test: 1.023369

Epoch: 194
Loss: 0.1960018090903759
RMSE train: 0.458699	val: 1.331812	test: 1.393639
MAE train: 0.355233	val: 1.069150	test: 1.052665

Epoch: 195
Loss: 0.18992722779512405
RMSE train: 0.383664	val: 1.300182	test: 1.356790
MAE train: 0.296742	val: 1.021784	test: 1.064057

Epoch: 196
Loss: 0.18887203186750412
RMSE train: 0.392686	val: 1.298377	test: 1.340502
MAE train: 0.302145	val: 1.020738	test: 1.058103

Epoch: 197
Loss: 0.2066185548901558
RMSE train: 0.465672	val: 1.356922	test: 1.374422
MAE train: 0.357610	val: 1.079560	test: 1.057402

Epoch: 198
Loss: 0.1896715648472309
RMSE train: 0.373443	val: 1.273944	test: 1.324151
MAE train: 0.290137	val: 1.010669	test: 1.043184

Epoch: 199
Loss: 0.18797172233462334
RMSE train: 0.336940	val: 1.293570	test: 1.336745
MAE train: 0.260990	val: 1.023429	test: 1.058969

Epoch: 200
Loss: 0.17267464846372604
RMSE train: 0.449358	val: 1.357616	test: 1.364227
MAE train: 0.343052	val: 1.064612	test: 1.061928

Epoch: 201
Loss: 0.18448356911540031
RMSE train: 0.392576	val: 1.245946	test: 1.289439
MAE train: 0.307957	val: 0.986921	test: 1.019382

Epoch: 202
Loss: 0.1970759481191635
RMSE train: 0.336189	val: 1.299521	test: 1.333043
MAE train: 0.265467	val: 1.016995	test: 1.072735

Epoch: 203
Loss: 0.17711816728115082
RMSE train: 0.359451	val: 1.295657	test: 1.322949
MAE train: 0.294263	val: 1.032031	test: 1.047312

Epoch: 204
Loss: 0.1794028915464878
RMSE train: 0.350308	val: 1.242898	test: 1.307845
MAE train: 0.284909	val: 0.999927	test: 1.054430

Epoch: 205
Loss: 0.18743612244725227
RMSE train: 0.373151	val: 1.324895	test: 1.374673
MAE train: 0.295830	val: 1.054234	test: 1.075431

Epoch: 206
Loss: 0.1807081587612629
RMSE train: 0.414882	val: 1.368907	test: 1.420221
MAE train: 0.325177	val: 1.092090	test: 1.094597

Epoch: 207
Loss: 0.188804741948843
RMSE train: 0.360994	val: 1.253085	test: 1.337744
MAE train: 0.287338	val: 1.012062	test: 1.035440

Epoch: 208
Loss: 0.19348913431167603
RMSE train: 0.388879	val: 1.309657	test: 1.380828
MAE train: 0.304864	val: 1.051425	test: 1.070800

Epoch: 209
Loss: 0.19222494959831238
RMSE train: 0.452228	val: 1.377412	test: 1.415616
MAE train: 0.353913	val: 1.079949	test: 1.113086

Epoch: 210
Loss: 0.21586060151457787
RMSE train: 0.404488	val: 1.231699	test: 1.311619
MAE train: 0.324256	val: 0.997555	test: 1.041243

Epoch: 211
Loss: 0.17591562122106552
RMSE train: 0.338195	val: 1.306112	test: 1.358814
MAE train: 0.266436	val: 1.048269	test: 1.074649

Epoch: 212
Loss: 0.20444338768720627
RMSE train: 0.391826	val: 1.388289	test: 1.415683
MAE train: 0.304620	val: 1.103105	test: 1.112672

Epoch: 213
Loss: 0.17399649322032928
RMSE train: 0.385578	val: 1.280237	test: 1.325525
MAE train: 0.311815	val: 1.038075	test: 1.017703

Epoch: 214
Loss: 0.21523390337824821
RMSE train: 0.386646	val: 1.347007	test: 1.375339
MAE train: 0.301261	val: 1.076872	test: 1.057568

Early stopping
Best (RMSE):	 train: 0.395183	val: 1.212695	test: 1.291422
Best (MAE):	 train: 0.319385	val: 0.967617	test: 1.025517
All runs completed.
