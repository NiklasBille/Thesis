>>> Starting run for dataset: freesolv
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running RANDOM configs_split_experiments/GraphCL/freesolv/random/train_prop=0.6.yml on cuda:0
Running RANDOM configs_split_experiments/GraphCL/freesolv/random/train_prop=0.7.yml on cuda:1
Running RANDOM configs_split_experiments/GraphCL/freesolv/random/train_prop=0.8.yml on cuda:2
Starting process for seed 1: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/freesolv/random/train_prop=0.8.yml --runseed 1 --device cuda:2
Starting process for seed 2: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/freesolv/random/train_prop=0.8.yml --runseed 2 --device cuda:2
Starting process for seed 3: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/freesolv/random/train_prop=0.8.yml --runseed 3 --device cuda:2
Starting process for seed 1: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/freesolv/random/train_prop=0.7.yml --runseed 1 --device cuda:1
Starting process for seed 2: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/freesolv/random/train_prop=0.7.yml --runseed 2 --device cuda:1
Starting process for seed 1: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/freesolv/random/train_prop=0.6.yml --runseed 1 --device cuda:0
Starting process for seed 3: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/freesolv/random/train_prop=0.7.yml --runseed 3 --device cuda:1
Starting process for seed 2: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/freesolv/random/train_prop=0.6.yml --runseed 2 --device cuda:0
Starting process for seed 3: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphCL/freesolv/random/train_prop=0.6.yml --runseed 3 --device cuda:0
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/freesolv/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/freesolv/random/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/freesolv/random/train_prop=0.6/freesolv_random_3_20-05_14-43-15  ]
[ Using Seed :  3  ]
[ Using device :  cuda:0  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 28.11850070953369
RMSE train: 5.433971	val: 5.534480	test: 5.478949
MAE train: 4.333568	val: 4.352888	test: 4.433642

Epoch: 2
Loss: 26.606029510498047
RMSE train: 5.480117	val: 5.568968	test: 5.549511
MAE train: 4.429998	val: 4.451817	test: 4.526962

Epoch: 3
Loss: 26.786145210266113
RMSE train: 5.462120	val: 5.545304	test: 5.564854
MAE train: 4.470294	val: 4.479948	test: 4.547305

Epoch: 4
Loss: 24.965872764587402
RMSE train: 5.335317	val: 5.425563	test: 5.494447
MAE train: 4.417746	val: 4.410946	test: 4.502480

Epoch: 5
Loss: 23.5557804107666
RMSE train: 5.190091	val: 5.283490	test: 5.397503
MAE train: 4.382362	val: 4.351783	test: 4.496141

Epoch: 6
Loss: 21.29963207244873
RMSE train: 4.980430	val: 5.069941	test: 5.190128
MAE train: 4.303308	val: 4.232590	test: 4.427850

Epoch: 7
Loss: 19.556161880493164
RMSE train: 4.732262	val: 4.760696	test: 4.888069
MAE train: 4.142220	val: 4.037525	test: 4.250669

Epoch: 8
Loss: 18.60530185699463
RMSE train: 4.454077	val: 4.396962	test: 4.597282
MAE train: 3.935858	val: 3.783624	test: 4.029867

Epoch: 9
Loss: 17.153719425201416
RMSE train: 4.221521	val: 4.095938	test: 4.427908
MAE train: 3.741837	val: 3.581858	test: 3.907957

Epoch: 10
Loss: 17.24605703353882
RMSE train: 4.061937	val: 3.881134	test: 4.318332
MAE train: 3.604455	val: 3.441179	test: 3.844504

Epoch: 11
Loss: 15.317255020141602
RMSE train: 3.909064	val: 3.710676	test: 4.195182
MAE train: 3.459234	val: 3.320709	test: 3.745955

Epoch: 12
Loss: 16.539379596710205
RMSE train: 3.788061	val: 3.569149	test: 4.074252
MAE train: 3.342331	val: 3.193744	test: 3.629248

Epoch: 13
Loss: 13.781476497650146
RMSE train: 3.703633	val: 3.478959	test: 3.995519
MAE train: 3.274405	val: 3.104759	test: 3.541661

Epoch: 14
Loss: 14.240087985992432
RMSE train: 3.654337	val: 3.464594	test: 3.954365
MAE train: 3.223332	val: 3.084699	test: 3.478995

Epoch: 15
Loss: 14.862192153930664
RMSE train: 3.650872	val: 3.484146	test: 3.936707
MAE train: 3.220810	val: 3.095348	test: 3.464360

Epoch: 16
Loss: 12.518022060394287
RMSE train: 3.641251	val: 3.495557	test: 3.895646
MAE train: 3.221136	val: 3.089719	test: 3.432903

Epoch: 17
Loss: 12.649929523468018
RMSE train: 3.643981	val: 3.505319	test: 3.871989
MAE train: 3.230308	val: 3.083975	test: 3.427066

Epoch: 18
Loss: 14.533809661865234
RMSE train: 3.664203	val: 3.525376	test: 3.878309
MAE train: 3.253623	val: 3.097411	test: 3.445702

Epoch: 19
Loss: 10.983627796173096
RMSE train: 3.651757	val: 3.518900	test: 3.857087
MAE train: 3.239449	val: 3.080907	test: 3.424317

Epoch: 20
Loss: 11.413408756256104
RMSE train: 3.661050	val: 3.543952	test: 3.883515
MAE train: 3.252572	val: 3.113702	test: 3.451587

Epoch: 21
Loss: 11.898359298706055
RMSE train: 3.624674	val: 3.544878	test: 3.881417
MAE train: 3.209262	val: 3.118251	test: 3.437358

Epoch: 22
Loss: 10.577859878540039
RMSE train: 3.574248	val: 3.503190	test: 3.840653
MAE train: 3.159782	val: 3.080539	test: 3.395999Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/freesolv/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/freesolv/random/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/freesolv/random/train_prop=0.6/freesolv_random_2_20-05_14-43-15  ]
[ Using Seed :  2  ]
[ Using device :  cuda:0  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 29.826568603515625
RMSE train: 5.453080	val: 5.569342	test: 5.433602
MAE train: 4.328555	val: 4.316433	test: 4.334590

Epoch: 2
Loss: 28.40335750579834
RMSE train: 5.547631	val: 5.624107	test: 5.572327
MAE train: 4.460819	val: 4.444000	test: 4.540685

Epoch: 3
Loss: 25.132441520690918
RMSE train: 5.553177	val: 5.592636	test: 5.626542
MAE train: 4.519836	val: 4.499568	test: 4.630763

Epoch: 4
Loss: 26.098430633544922
RMSE train: 5.448768	val: 5.444203	test: 5.585534
MAE train: 4.477463	val: 4.430275	test: 4.599348

Epoch: 5
Loss: 24.716395378112793
RMSE train: 5.314704	val: 5.291305	test: 5.530933
MAE train: 4.430777	val: 4.346299	test: 4.564408

Epoch: 6
Loss: 21.14754009246826
RMSE train: 5.098957	val: 5.082960	test: 5.397761
MAE train: 4.332818	val: 4.233739	test: 4.492207

Epoch: 7
Loss: 20.441699028015137
RMSE train: 4.843837	val: 4.831623	test: 5.152146
MAE train: 4.194280	val: 4.101346	test: 4.395697

Epoch: 8
Loss: 18.452777862548828
RMSE train: 4.637733	val: 4.511910	test: 4.872605
MAE train: 4.065422	val: 3.930102	test: 4.282809

Epoch: 9
Loss: 17.977669715881348
RMSE train: 4.508188	val: 4.190738	test: 4.624694
MAE train: 3.996345	val: 3.733503	test: 4.119507

Epoch: 10
Loss: 17.400349617004395
RMSE train: 4.339396	val: 3.935460	test: 4.425217
MAE train: 3.870945	val: 3.543969	test: 3.974229

Epoch: 11
Loss: 16.80088996887207
RMSE train: 4.091930	val: 3.734383	test: 4.227422
MAE train: 3.663761	val: 3.370259	test: 3.823031

Epoch: 12
Loss: 14.60220718383789
RMSE train: 3.907705	val: 3.585909	test: 4.069265
MAE train: 3.482488	val: 3.212028	test: 3.676057

Epoch: 13
Loss: 14.467396259307861
RMSE train: 3.769365	val: 3.482592	test: 3.939958
MAE train: 3.335921	val: 3.103524	test: 3.544726

Epoch: 14
Loss: 13.985682964324951
RMSE train: 3.668509	val: 3.438631	test: 3.865602
MAE train: 3.236182	val: 3.064414	test: 3.454434

Epoch: 15
Loss: 13.378962993621826
RMSE train: 3.619305	val: 3.447937	test: 3.846899
MAE train: 3.174298	val: 3.062200	test: 3.403564

Epoch: 16
Loss: 13.29468059539795
RMSE train: 3.636140	val: 3.528246	test: 3.883995
MAE train: 3.175641	val: 3.103242	test: 3.388104

Epoch: 17
Loss: 13.724318027496338
RMSE train: 3.712688	val: 3.624552	test: 3.947707
MAE train: 3.269557	val: 3.186194	test: 3.447964

Epoch: 18
Loss: 12.068802833557129
RMSE train: 3.769519	val: 3.664278	test: 3.983211
MAE train: 3.351158	val: 3.237348	test: 3.495295

Epoch: 19
Loss: 12.50312852859497
RMSE train: 3.850592	val: 3.719524	test: 4.031854
MAE train: 3.461115	val: 3.324109	test: 3.595911

Epoch: 20
Loss: 11.589743614196777
RMSE train: 3.893910	val: 3.745024	test: 4.064429
MAE train: 3.507976	val: 3.364284	test: 3.653503

Epoch: 21
Loss: 10.628294467926025
RMSE train: 3.869544	val: 3.702324	test: 4.042087
MAE train: 3.485082	val: 3.326659	test: 3.639780

Epoch: 22
Loss: 11.01359224319458
RMSE train: 3.819949	val: 3.653543	test: 3.996623
MAE train: 3.438824	val: 3.283396	test: 3.599852Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/freesolv/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/freesolv/random/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/freesolv/random/train_prop=0.6/freesolv_random_1_20-05_14-43-15  ]
[ Using Seed :  1  ]
[ Using device :  cuda:0  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 27.411758422851562
RMSE train: 5.802792	val: 5.809787	test: 5.747762
MAE train: 4.645721	val: 4.564909	test: 4.733631

Epoch: 2
Loss: 27.303133964538574
RMSE train: 5.896855	val: 5.902582	test: 5.852217
MAE train: 4.778732	val: 4.706566	test: 4.864467

Epoch: 3
Loss: 25.68403720855713
RMSE train: 5.991541	val: 6.010048	test: 5.955797
MAE train: 4.930508	val: 4.856530	test: 4.989230

Epoch: 4
Loss: 25.295942306518555
RMSE train: 6.063725	val: 6.063425	test: 6.046927
MAE train: 5.076055	val: 4.944621	test: 5.090270

Epoch: 5
Loss: 24.84537410736084
RMSE train: 6.094430	val: 6.027355	test: 6.101252
MAE train: 5.170063	val: 4.950387	test: 5.165338

Epoch: 6
Loss: 24.857791900634766
RMSE train: 6.028913	val: 5.908915	test: 6.066582
MAE train: 5.164933	val: 4.891147	test: 5.164514

Epoch: 7
Loss: 23.740422248840332
RMSE train: 5.870718	val: 5.693477	test: 5.949996
MAE train: 5.089113	val: 4.771858	test: 5.099186

Epoch: 8
Loss: 20.05092144012451
RMSE train: 5.582601	val: 5.335274	test: 5.743013
MAE train: 4.908603	val: 4.570032	test: 4.962211

Epoch: 9
Loss: 18.44515037536621
RMSE train: 5.215417	val: 4.921701	test: 5.477966
MAE train: 4.624783	val: 4.309683	test: 4.799094

Epoch: 10
Loss: 17.699400901794434
RMSE train: 4.942574	val: 4.591006	test: 5.270131
MAE train: 4.391858	val: 4.057221	test: 4.664535

Epoch: 11
Loss: 17.403618812561035
RMSE train: 4.675730	val: 4.320273	test: 4.996040
MAE train: 4.167856	val: 3.824751	test: 4.467707

Epoch: 12
Loss: 15.401864051818848
RMSE train: 4.389399	val: 4.032870	test: 4.650852
MAE train: 3.921948	val: 3.594176	test: 4.188782

Epoch: 13
Loss: 14.132096290588379
RMSE train: 4.065430	val: 3.701734	test: 4.305019
MAE train: 3.624328	val: 3.312675	test: 3.885458

Epoch: 14
Loss: 15.25191068649292
RMSE train: 3.772453	val: 3.449382	test: 4.000513
MAE train: 3.339916	val: 3.077851	test: 3.599257

Epoch: 15
Loss: 13.580140113830566
RMSE train: 3.563990	val: 3.313094	test: 3.776281
MAE train: 3.118010	val: 2.906921	test: 3.370456

Epoch: 16
Loss: 14.349104881286621
RMSE train: 3.493672	val: 3.300450	test: 3.702028
MAE train: 3.043048	val: 2.846798	test: 3.283241

Epoch: 17
Loss: 12.516348361968994
RMSE train: 3.473804	val: 3.338161	test: 3.691223
MAE train: 3.016829	val: 2.849580	test: 3.242708

Epoch: 18
Loss: 12.924999713897705
RMSE train: 3.506674	val: 3.397279	test: 3.730724
MAE train: 3.044693	val: 2.899033	test: 3.255667

Epoch: 19
Loss: 12.111924648284912
RMSE train: 3.489275	val: 3.415431	test: 3.743703
MAE train: 3.033001	val: 2.931242	test: 3.256750

Epoch: 20
Loss: 11.27519416809082
RMSE train: 3.444408	val: 3.392876	test: 3.726304
MAE train: 2.990379	val: 2.920723	test: 3.234047

Epoch: 21
Loss: 10.245562076568604
RMSE train: 3.381719	val: 3.336181	test: 3.679064
MAE train: 2.924873	val: 2.875215	test: 3.189853

Epoch: 22
Loss: 10.235855102539062
RMSE train: 3.312184	val: 3.256781	test: 3.605094
MAE train: 2.866441	val: 2.807723	test: 3.136468Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/freesolv/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/freesolv/random/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/freesolv/random/train_prop=0.7/freesolv_random_2_20-05_14-43-15  ]
[ Using Seed :  2  ]
[ Using device :  cuda:1  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 28.274720191955566
RMSE train: 5.268478	val: 6.070784	test: 5.336069
MAE train: 4.118920	val: 4.807114	test: 4.193338

Epoch: 2
Loss: 26.859121322631836
RMSE train: 5.403484	val: 6.148424	test: 5.476148
MAE train: 4.323750	val: 4.991686	test: 4.423447

Epoch: 3
Loss: 26.07303524017334
RMSE train: 5.375192	val: 6.071972	test: 5.488418
MAE train: 4.339335	val: 4.975733	test: 4.440971

Epoch: 4
Loss: 24.441987991333008
RMSE train: 5.234341	val: 5.891813	test: 5.427714
MAE train: 4.269688	val: 4.851936	test: 4.405710

Epoch: 5
Loss: 22.687856674194336
RMSE train: 5.036400	val: 5.694841	test: 5.328299
MAE train: 4.168575	val: 4.732020	test: 4.340604

Epoch: 6
Loss: 21.20771312713623
RMSE train: 4.793600	val: 5.483505	test: 5.168711
MAE train: 4.044813	val: 4.598663	test: 4.255957

Epoch: 7
Loss: 19.285490036010742
RMSE train: 4.524323	val: 5.232558	test: 4.875564
MAE train: 3.880840	val: 4.451158	test: 4.126400

Epoch: 8
Loss: 17.640982151031494
RMSE train: 4.339959	val: 4.961022	test: 4.596900
MAE train: 3.768481	val: 4.316834	test: 4.030125

Epoch: 9
Loss: 16.959732055664062
RMSE train: 4.159217	val: 4.632136	test: 4.353787
MAE train: 3.657209	val: 4.100412	test: 3.897606

Epoch: 10
Loss: 15.82040548324585
RMSE train: 3.907423	val: 4.294312	test: 4.074438
MAE train: 3.461489	val: 3.804256	test: 3.708610

Epoch: 11
Loss: 14.712436199188232
RMSE train: 3.684929	val: 4.010590	test: 3.820723
MAE train: 3.265721	val: 3.537396	test: 3.456874

Epoch: 12
Loss: 14.668659687042236
RMSE train: 3.532957	val: 3.836076	test: 3.663820
MAE train: 3.129659	val: 3.343618	test: 3.274288

Epoch: 13
Loss: 13.719358444213867
RMSE train: 3.452515	val: 3.776270	test: 3.593818
MAE train: 3.056958	val: 3.251913	test: 3.189339

Epoch: 14
Loss: 13.72761869430542
RMSE train: 3.415173	val: 3.759894	test: 3.567335
MAE train: 3.008204	val: 3.208973	test: 3.133133

Epoch: 15
Loss: 12.680493831634521
RMSE train: 3.334739	val: 3.685635	test: 3.504208
MAE train: 2.928378	val: 3.125176	test: 3.076140

Epoch: 16
Loss: 12.278843879699707
RMSE train: 3.333203	val: 3.667560	test: 3.505809
MAE train: 2.940265	val: 3.132093	test: 3.087495

Epoch: 17
Loss: 11.902138233184814
RMSE train: 3.383683	val: 3.703949	test: 3.555925
MAE train: 2.990540	val: 3.180470	test: 3.144531

Epoch: 18
Loss: 11.636353492736816
RMSE train: 3.454887	val: 3.792377	test: 3.630281
MAE train: 3.059842	val: 3.257616	test: 3.215454

Epoch: 19
Loss: 10.895378589630127
RMSE train: 3.494140	val: 3.842283	test: 3.688084
MAE train: 3.089579	val: 3.300228	test: 3.262171

Epoch: 20
Loss: 10.151577949523926
RMSE train: 3.510806	val: 3.886117	test: 3.713806
MAE train: 3.099518	val: 3.330424	test: 3.281151

Epoch: 21
Loss: 9.822755813598633
RMSE train: 3.473245	val: 3.867983	test: 3.674695
MAE train: 3.057753	val: 3.299227	test: 3.244572

Epoch: 22
Loss: 9.150298118591309
RMSE train: 3.434299	val: 3.807446	test: 3.630399
MAE train: 3.029630	val: 3.239708	test: 3.216227Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/freesolv/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/freesolv/random/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/freesolv/random/train_prop=0.8/freesolv_random_2_20-05_14-43-15  ]
[ Using Seed :  2  ]
[ Using device :  cuda:2  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 30.05059242248535
RMSE train: 5.436090	val: 5.485369	test: 5.302645
MAE train: 4.267004	val: 4.616285	test: 3.966423

Epoch: 2
Loss: 28.485139846801758
RMSE train: 5.460605	val: 5.564464	test: 5.357014
MAE train: 4.343026	val: 4.769675	test: 4.075077

Epoch: 3
Loss: 27.276488304138184
RMSE train: 5.381576	val: 5.555858	test: 5.359569
MAE train: 4.320269	val: 4.780915	test: 4.118597

Epoch: 4
Loss: 25.67294216156006
RMSE train: 5.280888	val: 5.495297	test: 5.362564
MAE train: 4.295159	val: 4.732704	test: 4.150388

Epoch: 5
Loss: 23.671759605407715
RMSE train: 5.157472	val: 5.419741	test: 5.369161
MAE train: 4.262852	val: 4.661550	test: 4.188388

Epoch: 6
Loss: 22.20554542541504
RMSE train: 5.024133	val: 5.332341	test: 5.349175
MAE train: 4.227592	val: 4.585683	test: 4.229696

Epoch: 7
Loss: 20.35123634338379
RMSE train: 4.847428	val: 5.244938	test: 5.210690
MAE train: 4.178000	val: 4.588652	test: 4.255051

Epoch: 8
Loss: 18.94551658630371
RMSE train: 4.552343	val: 5.040297	test: 4.761507
MAE train: 4.016688	val: 4.464232	test: 4.152365

Epoch: 9
Loss: 17.44613790512085
RMSE train: 4.285529	val: 4.807263	test: 4.273631
MAE train: 3.831126	val: 4.269893	test: 3.949273

Epoch: 10
Loss: 16.038718223571777
RMSE train: 4.088923	val: 4.620194	test: 3.941528
MAE train: 3.672369	val: 4.113669	test: 3.664339

Epoch: 11
Loss: 16.62174129486084
RMSE train: 3.852143	val: 4.433237	test: 3.651357
MAE train: 3.448780	val: 3.938269	test: 3.397784

Epoch: 12
Loss: 15.013031482696533
RMSE train: 3.678945	val: 4.323602	test: 3.480058
MAE train: 3.261833	val: 3.823660	test: 3.240439

Epoch: 13
Loss: 14.820266723632812
RMSE train: 3.582436	val: 4.290737	test: 3.400716
MAE train: 3.156608	val: 3.771404	test: 3.134539

Epoch: 14
Loss: 13.86426067352295
RMSE train: 3.539096	val: 4.254281	test: 3.395359
MAE train: 3.107847	val: 3.718046	test: 3.133220

Epoch: 15
Loss: 13.401831150054932
RMSE train: 3.577230	val: 4.273416	test: 3.479297
MAE train: 3.171165	val: 3.728815	test: 3.210325

Epoch: 16
Loss: 12.609661102294922
RMSE train: 3.657560	val: 4.334256	test: 3.572589
MAE train: 3.255086	val: 3.755284	test: 3.273175

Epoch: 17
Loss: 12.049787044525146
RMSE train: 3.713313	val: 4.359089	test: 3.638706
MAE train: 3.295851	val: 3.738252	test: 3.311969

Epoch: 18
Loss: 11.470555305480957
RMSE train: 3.718589	val: 4.320667	test: 3.640742
MAE train: 3.291232	val: 3.686692	test: 3.293087

Epoch: 19
Loss: 11.096192836761475
RMSE train: 3.706723	val: 4.278072	test: 3.612692
MAE train: 3.280825	val: 3.666574	test: 3.257265

Epoch: 20
Loss: 10.938271522521973
RMSE train: 3.676189	val: 4.241052	test: 3.555409
MAE train: 3.262424	val: 3.682897	test: 3.210025

Epoch: 21
Loss: 10.495677471160889
RMSE train: 3.603125	val: 4.197708	test: 3.457899
MAE train: 3.202370	val: 3.686961	test: 3.138476

Epoch: 22
Loss: 9.909059047698975
RMSE train: 3.535986	val: 4.189567	test: 3.366744
MAE train: 3.146009	val: 3.716502	test: 3.070009Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/freesolv/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/freesolv/random/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/freesolv/random/train_prop=0.7/freesolv_random_3_20-05_14-43-15  ]
[ Using Seed :  3  ]
[ Using device :  cuda:1  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 27.79173183441162
RMSE train: 5.313430	val: 6.018094	test: 5.441958
MAE train: 4.218083	val: 4.806523	test: 4.358827

Epoch: 2
Loss: 26.60332679748535
RMSE train: 5.425534	val: 6.028648	test: 5.576286
MAE train: 4.381959	val: 4.871444	test: 4.544505

Epoch: 3
Loss: 24.922157287597656
RMSE train: 5.451687	val: 6.019047	test: 5.628101
MAE train: 4.472431	val: 4.945658	test: 4.629046

Epoch: 4
Loss: 23.90529727935791
RMSE train: 5.352585	val: 5.907516	test: 5.600141
MAE train: 4.454444	val: 4.909276	test: 4.614132

Epoch: 5
Loss: 21.763154983520508
RMSE train: 5.229426	val: 5.756295	test: 5.539335
MAE train: 4.423147	val: 4.843946	test: 4.567969

Epoch: 6
Loss: 20.2308988571167
RMSE train: 5.074876	val: 5.569388	test: 5.431498
MAE train: 4.392255	val: 4.764448	test: 4.535444

Epoch: 7
Loss: 18.43502426147461
RMSE train: 4.836086	val: 5.280472	test: 5.132127
MAE train: 4.255230	val: 4.583958	test: 4.404086

Epoch: 8
Loss: 17.2779541015625
RMSE train: 4.549833	val: 4.965324	test: 4.733916
MAE train: 4.032914	val: 4.368762	test: 4.192700

Epoch: 9
Loss: 16.8285870552063
RMSE train: 4.312125	val: 4.700723	test: 4.435145
MAE train: 3.840825	val: 4.167239	test: 4.001153

Epoch: 10
Loss: 15.630102634429932
RMSE train: 4.090818	val: 4.456932	test: 4.202054
MAE train: 3.657926	val: 3.949461	test: 3.804012

Epoch: 11
Loss: 15.486795425415039
RMSE train: 3.904993	val: 4.273461	test: 4.049352
MAE train: 3.513206	val: 3.777799	test: 3.638919

Epoch: 12
Loss: 14.01430892944336
RMSE train: 3.772933	val: 4.115625	test: 3.977203
MAE train: 3.400604	val: 3.616353	test: 3.564030

Epoch: 13
Loss: 13.240327835083008
RMSE train: 3.684061	val: 3.988839	test: 3.936992
MAE train: 3.321726	val: 3.497292	test: 3.520846

Epoch: 14
Loss: 12.785361766815186
RMSE train: 3.639570	val: 3.912941	test: 3.906883
MAE train: 3.280788	val: 3.423009	test: 3.491792

Epoch: 15
Loss: 12.70281457901001
RMSE train: 3.620965	val: 3.874628	test: 3.877444
MAE train: 3.262530	val: 3.380542	test: 3.465267

Epoch: 16
Loss: 12.339916229248047
RMSE train: 3.585526	val: 3.846966	test: 3.831339
MAE train: 3.217684	val: 3.326526	test: 3.422684

Epoch: 17
Loss: 11.447162628173828
RMSE train: 3.571450	val: 3.876901	test: 3.793873
MAE train: 3.195173	val: 3.333554	test: 3.393504

Epoch: 18
Loss: 10.649085998535156
RMSE train: 3.585702	val: 3.925384	test: 3.781176
MAE train: 3.206111	val: 3.366308	test: 3.397030

Epoch: 19
Loss: 10.536796569824219
RMSE train: 3.607343	val: 3.979974	test: 3.777850
MAE train: 3.219416	val: 3.410441	test: 3.409142

Epoch: 20
Loss: 10.077538013458252
RMSE train: 3.607552	val: 4.017491	test: 3.762162
MAE train: 3.214021	val: 3.438700	test: 3.402337

Epoch: 21
Loss: 9.6308274269104
RMSE train: 3.573316	val: 4.014841	test: 3.717571
MAE train: 3.171343	val: 3.419550	test: 3.359014

Epoch: 22
Loss: 9.155327320098877
RMSE train: 3.511906	val: 3.977010	test: 3.662225
MAE train: 3.101857	val: 3.365644	test: 3.299676Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/freesolv/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/freesolv/random/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/freesolv/random/train_prop=0.7/freesolv_random_1_20-05_14-43-15  ]
[ Using Seed :  1  ]
[ Using device :  cuda:1  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 28.114261627197266
RMSE train: 5.528772	val: 6.179131	test: 5.564176
MAE train: 4.376777	val: 4.917987	test: 4.514204

Epoch: 2
Loss: 27.48133373260498
RMSE train: 5.539159	val: 6.171989	test: 5.584564
MAE train: 4.417257	val: 4.952097	test: 4.562725

Epoch: 3
Loss: 25.47844409942627
RMSE train: 5.576061	val: 6.191500	test: 5.635659
MAE train: 4.504194	val: 5.036013	test: 4.639494

Epoch: 4
Loss: 24.669790267944336
RMSE train: 5.627842	val: 6.174904	test: 5.691815
MAE train: 4.629607	val: 5.100662	test: 4.719236

Epoch: 5
Loss: 22.902140617370605
RMSE train: 5.595717	val: 6.070294	test: 5.682256
MAE train: 4.683188	val: 5.098206	test: 4.721555

Epoch: 6
Loss: 21.786601066589355
RMSE train: 5.441188	val: 5.866512	test: 5.590707
MAE train: 4.631121	val: 5.000703	test: 4.681537

Epoch: 7
Loss: 20.283141136169434
RMSE train: 5.201711	val: 5.599858	test: 5.391906
MAE train: 4.505874	val: 4.843097	test: 4.597887

Epoch: 8
Loss: 18.511377334594727
RMSE train: 4.897460	val: 5.244626	test: 5.106355
MAE train: 4.302056	val: 4.631669	test: 4.459990

Epoch: 9
Loss: 17.229973793029785
RMSE train: 4.569519	val: 4.888336	test: 4.724318
MAE train: 4.042785	val: 4.339096	test: 4.215075

Epoch: 10
Loss: 15.904561996459961
RMSE train: 4.272063	val: 4.550759	test: 4.336323
MAE train: 3.804644	val: 4.049233	test: 3.891219

Epoch: 11
Loss: 15.590091705322266
RMSE train: 3.954639	val: 4.142769	test: 3.956808
MAE train: 3.511597	val: 3.675478	test: 3.558818

Epoch: 12
Loss: 14.169633388519287
RMSE train: 3.671114	val: 3.816531	test: 3.702227
MAE train: 3.238901	val: 3.331015	test: 3.341136

Epoch: 13
Loss: 14.356489658355713
RMSE train: 3.433369	val: 3.582579	test: 3.532449
MAE train: 3.015752	val: 3.056590	test: 3.151935

Epoch: 14
Loss: 12.97557258605957
RMSE train: 3.320192	val: 3.513853	test: 3.448263
MAE train: 2.914423	val: 2.974481	test: 3.042750

Epoch: 15
Loss: 13.341464042663574
RMSE train: 3.335214	val: 3.537951	test: 3.460291
MAE train: 2.933617	val: 2.998069	test: 3.066454

Epoch: 16
Loss: 12.077615737915039
RMSE train: 3.391112	val: 3.597295	test: 3.490994
MAE train: 2.991268	val: 3.051354	test: 3.114714

Epoch: 17
Loss: 11.794021129608154
RMSE train: 3.487184	val: 3.696387	test: 3.562938
MAE train: 3.101088	val: 3.142322	test: 3.208380

Epoch: 18
Loss: 11.23702621459961
RMSE train: 3.539630	val: 3.745614	test: 3.614536
MAE train: 3.161991	val: 3.195520	test: 3.272587

Epoch: 19
Loss: 10.849640846252441
RMSE train: 3.526358	val: 3.723229	test: 3.625910
MAE train: 3.145151	val: 3.178328	test: 3.292310

Epoch: 20
Loss: 10.39579439163208
RMSE train: 3.495804	val: 3.697272	test: 3.633044
MAE train: 3.109414	val: 3.159809	test: 3.296143

Epoch: 21
Loss: 9.9192476272583
RMSE train: 3.469513	val: 3.687446	test: 3.656390
MAE train: 3.081534	val: 3.151877	test: 3.305395

Epoch: 22
Loss: 9.458138465881348
RMSE train: 3.420388	val: 3.670241	test: 3.649896
MAE train: 3.026837	val: 3.122858	test: 3.270371Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/freesolv/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/freesolv/random/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/freesolv/random/train_prop=0.8/freesolv_random_1_20-05_14-43-15  ]
[ Using Seed :  1  ]
[ Using device :  cuda:2  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 29.768390655517578
RMSE train: 5.679106	val: 5.828235	test: 5.434784
MAE train: 4.500072	val: 5.020410	test: 4.180042

Epoch: 2
Loss: 28.38847255706787
RMSE train: 5.798048	val: 5.918554	test: 5.613668
MAE train: 4.667305	val: 5.150727	test: 4.424648

Epoch: 3
Loss: 27.24520778656006
RMSE train: 5.910050	val: 5.994949	test: 5.773205
MAE train: 4.850890	val: 5.240750	test: 4.641559

Epoch: 4
Loss: 26.03291130065918
RMSE train: 5.917954	val: 5.963902	test: 5.853543
MAE train: 4.918961	val: 5.206455	test: 4.756461

Epoch: 5
Loss: 24.779783248901367
RMSE train: 5.831363	val: 5.864394	test: 5.858494
MAE train: 4.894273	val: 5.098018	test: 4.779773

Epoch: 6
Loss: 23.021368980407715
RMSE train: 5.584019	val: 5.607180	test: 5.751176
MAE train: 4.735509	val: 4.882173	test: 4.697273

Epoch: 7
Loss: 21.373841285705566
RMSE train: 5.290287	val: 5.340302	test: 5.578687
MAE train: 4.546505	val: 4.668379	test: 4.590803

Epoch: 8
Loss: 19.708672523498535
RMSE train: 4.963283	val: 5.115194	test: 5.304213
MAE train: 4.335444	val: 4.505989	test: 4.434967

Epoch: 9
Loss: 18.579943656921387
RMSE train: 4.559047	val: 4.881930	test: 4.896431
MAE train: 4.034465	val: 4.328160	test: 4.221174

Epoch: 10
Loss: 16.792611122131348
RMSE train: 4.193185	val: 4.681679	test: 4.415244
MAE train: 3.729368	val: 4.161081	test: 3.951364

Epoch: 11
Loss: 15.49440860748291
RMSE train: 3.971747	val: 4.530949	test: 4.037534
MAE train: 3.505463	val: 3.984997	test: 3.651519

Epoch: 12
Loss: 14.930592060089111
RMSE train: 3.816861	val: 4.361108	test: 3.827412
MAE train: 3.331656	val: 3.758296	test: 3.419750

Epoch: 13
Loss: 14.966724395751953
RMSE train: 3.586802	val: 4.149324	test: 3.562314
MAE train: 3.104650	val: 3.592374	test: 3.168257

Epoch: 14
Loss: 13.846417427062988
RMSE train: 3.464488	val: 4.098387	test: 3.438146
MAE train: 2.993222	val: 3.537208	test: 3.054841

Epoch: 15
Loss: 13.16173791885376
RMSE train: 3.450584	val: 4.089450	test: 3.451911
MAE train: 3.007087	val: 3.541753	test: 3.141685

Epoch: 16
Loss: 12.545299053192139
RMSE train: 3.482454	val: 4.102150	test: 3.522633
MAE train: 3.054999	val: 3.570932	test: 3.220992

Epoch: 17
Loss: 11.952871799468994
RMSE train: 3.474974	val: 4.097468	test: 3.560204
MAE train: 3.071312	val: 3.586154	test: 3.241961

Epoch: 18
Loss: 12.06021499633789
RMSE train: 3.471800	val: 4.090246	test: 3.573723
MAE train: 3.088776	val: 3.578932	test: 3.256150

Epoch: 19
Loss: 11.21848726272583
RMSE train: 3.474710	val: 4.093135	test: 3.580236
MAE train: 3.101776	val: 3.565217	test: 3.246644

Epoch: 20
Loss: 10.625511646270752
RMSE train: 3.426328	val: 4.055702	test: 3.528463
MAE train: 3.049782	val: 3.514060	test: 3.185305

Epoch: 21
Loss: 10.382667064666748
RMSE train: 3.370604	val: 4.027231	test: 3.451872
MAE train: 2.979859	val: 3.468446	test: 3.101815

Epoch: 22
Loss: 9.877973079681396
RMSE train: 3.286700	val: 3.973280	test: 3.357071
MAE train: 2.880918	val: 3.409412	test: 3.008112Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/freesolv/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: freesolv
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_regression.pth
  output_model_dir: ../runs/split/GraphCL/freesolv/random/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/freesolv/random/train_prop=0.8/freesolv_random_3_20-05_14-43-15  ]
[ Using Seed :  3  ]
[ Using device :  cuda:2  ]
Dataset: freesolv
Data: Data(edge_attr=[10770, 3], edge_index=[2, 10770], id=[639], x=[5597, 9], y=[639])
dataset_folder: ../datasets/molecule_datasets/freesolv
MoleculeDatasetComplete(639)
randomly split
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 28.948448181152344
RMSE train: 5.512489	val: 5.691261	test: 5.409157
MAE train: 4.386339	val: 4.859802	test: 4.114658

Epoch: 2
Loss: 27.369647979736328
RMSE train: 5.594735	val: 5.778535	test: 5.558050
MAE train: 4.533424	val: 4.967706	test: 4.329901

Epoch: 3
Loss: 25.875277519226074
RMSE train: 5.612812	val: 5.801836	test: 5.649714
MAE train: 4.630173	val: 4.991633	test: 4.460560

Epoch: 4
Loss: 24.058289527893066
RMSE train: 5.489660	val: 5.690627	test: 5.625682
MAE train: 4.592533	val: 4.876416	test: 4.468820

Epoch: 5
Loss: 22.449478149414062
RMSE train: 5.258104	val: 5.510891	test: 5.476044
MAE train: 4.476410	val: 4.742758	test: 4.415908

Epoch: 6
Loss: 20.64988422393799
RMSE train: 4.917041	val: 5.256341	test: 5.098528
MAE train: 4.263995	val: 4.544647	test: 4.231799

Epoch: 7
Loss: 18.837852478027344
RMSE train: 4.556761	val: 4.994906	test: 4.537397
MAE train: 4.021000	val: 4.362816	test: 3.965192

Epoch: 8
Loss: 17.667224884033203
RMSE train: 4.205291	val: 4.746793	test: 4.150256
MAE train: 3.741432	val: 4.184387	test: 3.721359

Epoch: 9
Loss: 16.34318447113037
RMSE train: 4.000941	val: 4.647014	test: 3.907623
MAE train: 3.562013	val: 4.114311	test: 3.565408

Epoch: 10
Loss: 16.04951286315918
RMSE train: 3.882153	val: 4.613190	test: 3.752102
MAE train: 3.465528	val: 4.064878	test: 3.450083

Epoch: 11
Loss: 14.942681312561035
RMSE train: 3.757858	val: 4.557979	test: 3.583772
MAE train: 3.364737	val: 3.962545	test: 3.301042

Epoch: 12
Loss: 14.394351482391357
RMSE train: 3.639613	val: 4.453966	test: 3.449052
MAE train: 3.240572	val: 3.832968	test: 3.175603

Epoch: 13
Loss: 14.061854839324951
RMSE train: 3.557442	val: 4.329569	test: 3.408309
MAE train: 3.152526	val: 3.691980	test: 3.138949

Epoch: 14
Loss: 12.97150468826294
RMSE train: 3.568788	val: 4.277437	test: 3.474687
MAE train: 3.158879	val: 3.612090	test: 3.201190

Epoch: 15
Loss: 12.861310482025146
RMSE train: 3.593223	val: 4.255364	test: 3.546273
MAE train: 3.177669	val: 3.574101	test: 3.257162

Epoch: 16
Loss: 12.16767168045044
RMSE train: 3.636876	val: 4.290073	test: 3.624137
MAE train: 3.219148	val: 3.613155	test: 3.318295

Epoch: 17
Loss: 11.647121906280518
RMSE train: 3.747613	val: 4.391255	test: 3.735784
MAE train: 3.338205	val: 3.734744	test: 3.436968

Epoch: 18
Loss: 11.1538987159729
RMSE train: 3.826084	val: 4.472195	test: 3.830862
MAE train: 3.418961	val: 3.859559	test: 3.506691

Epoch: 19
Loss: 10.543936252593994
RMSE train: 3.828222	val: 4.481693	test: 3.829073
MAE train: 3.429987	val: 3.918897	test: 3.494744

Epoch: 20
Loss: 10.291041851043701
RMSE train: 3.799224	val: 4.465583	test: 3.802457
MAE train: 3.408229	val: 3.941220	test: 3.445401

Epoch: 21
Loss: 9.77856159210205
RMSE train: 3.722462	val: 4.403921	test: 3.706959
MAE train: 3.335033	val: 3.895918	test: 3.347293

Epoch: 22
Loss: 9.418582439422607
RMSE train: 3.677649	val: 4.352294	test: 3.643336
MAE train: 3.291823	val: 3.852698	test: 3.281366

Epoch: 23
Loss: 9.355401992797852
RMSE train: 3.497542	val: 3.382138	test: 3.770166
MAE train: 3.101669	val: 2.983097	test: 3.341630

Epoch: 24
Loss: 10.237532615661621
RMSE train: 3.460247	val: 3.325857	test: 3.745396
MAE train: 3.071348	val: 2.941263	test: 3.315823

Epoch: 25
Loss: 9.283833980560303
RMSE train: 3.434072	val: 3.287243	test: 3.723685
MAE train: 3.050069	val: 2.913352	test: 3.293017

Epoch: 26
Loss: 8.533418655395508
RMSE train: 3.382722	val: 3.270109	test: 3.681800
MAE train: 3.003790	val: 2.903667	test: 3.247510

Epoch: 27
Loss: 8.867853879928589
RMSE train: 3.377708	val: 3.281042	test: 3.672645
MAE train: 3.000380	val: 2.915266	test: 3.238594

Epoch: 28
Loss: 8.134334564208984
RMSE train: 3.403413	val: 3.305141	test: 3.690329
MAE train: 3.026934	val: 2.933824	test: 3.249405

Epoch: 29
Loss: 8.318161010742188
RMSE train: 3.452522	val: 3.342631	test: 3.720813
MAE train: 3.076551	val: 2.963163	test: 3.278445

Epoch: 30
Loss: 7.60442328453064
RMSE train: 3.426542	val: 3.331505	test: 3.675381
MAE train: 3.055328	val: 2.938115	test: 3.234129

Epoch: 31
Loss: 7.32275915145874
RMSE train: 3.408574	val: 3.332826	test: 3.630073
MAE train: 3.043518	val: 2.928669	test: 3.194086

Epoch: 32
Loss: 6.143615484237671
RMSE train: 3.321208	val: 3.268367	test: 3.531802
MAE train: 2.950801	val: 2.845289	test: 3.087554

Epoch: 33
Loss: 6.13078236579895
RMSE train: 3.209534	val: 3.180218	test: 3.411851
MAE train: 2.838986	val: 2.743627	test: 2.970617

Epoch: 34
Loss: 6.602488994598389
RMSE train: 3.085627	val: 3.080267	test: 3.283842
MAE train: 2.716486	val: 2.637942	test: 2.851897

Epoch: 35
Loss: 6.391510009765625
RMSE train: 2.969666	val: 2.985405	test: 3.160243
MAE train: 2.596490	val: 2.537384	test: 2.737042

Epoch: 36
Loss: 5.039614915847778
RMSE train: 2.838600	val: 2.861732	test: 3.021087
MAE train: 2.466022	val: 2.416744	test: 2.599261

Epoch: 37
Loss: 4.6599462032318115
RMSE train: 2.741705	val: 2.754573	test: 2.911859
MAE train: 2.376167	val: 2.319687	test: 2.496718

Epoch: 38
Loss: 4.511524677276611
RMSE train: 2.673008	val: 2.657185	test: 2.843991
MAE train: 2.315016	val: 2.240181	test: 2.434677

Epoch: 39
Loss: 4.4546449184417725
RMSE train: 2.572393	val: 2.559183	test: 2.756566
MAE train: 2.219005	val: 2.142821	test: 2.335504

Epoch: 40
Loss: 4.377350568771362
RMSE train: 2.470835	val: 2.475356	test: 2.680334
MAE train: 2.114336	val: 2.055623	test: 2.244141

Epoch: 41
Loss: 3.780966877937317
RMSE train: 2.304921	val: 2.332967	test: 2.551838
MAE train: 1.937282	val: 1.914979	test: 2.092868

Epoch: 42
Loss: 3.6884865760803223
RMSE train: 2.179081	val: 2.228593	test: 2.453800
MAE train: 1.791569	val: 1.807367	test: 1.976253

Epoch: 43
Loss: 3.7732244729995728
RMSE train: 2.087172	val: 2.133904	test: 2.362692
MAE train: 1.702463	val: 1.728267	test: 1.899546

Epoch: 44
Loss: 3.494343638420105
RMSE train: 2.070706	val: 2.116203	test: 2.313320
MAE train: 1.695272	val: 1.708587	test: 1.874095

Epoch: 45
Loss: 3.2980117797851562
RMSE train: 2.050999	val: 2.095069	test: 2.263779
MAE train: 1.684304	val: 1.687541	test: 1.840050

Epoch: 46
Loss: 2.8038601875305176
RMSE train: 1.989422	val: 2.041642	test: 2.201739
MAE train: 1.618825	val: 1.624447	test: 1.776753

Epoch: 47
Loss: 3.2284799814224243
RMSE train: 1.939634	val: 2.016603	test: 2.166145
MAE train: 1.562778	val: 1.582150	test: 1.736726

Epoch: 48
Loss: 2.8280142545700073
RMSE train: 1.849778	val: 1.960029	test: 2.097496
MAE train: 1.471993	val: 1.512517	test: 1.649683

Epoch: 49
Loss: 2.644751191139221
RMSE train: 1.756413	val: 1.890175	test: 2.021549
MAE train: 1.383027	val: 1.444772	test: 1.551417

Epoch: 50
Loss: 2.4295846819877625
RMSE train: 1.687922	val: 1.825152	test: 1.960083
MAE train: 1.315736	val: 1.383525	test: 1.481101

Epoch: 51
Loss: 2.6346014738082886
RMSE train: 1.650588	val: 1.791636	test: 1.917986
MAE train: 1.274364	val: 1.345727	test: 1.436227

Epoch: 52
Loss: 2.6353172063827515
RMSE train: 1.600532	val: 1.761837	test: 1.853041
MAE train: 1.227086	val: 1.310047	test: 1.378244

Epoch: 53
Loss: 2.021304666996002
RMSE train: 1.504716	val: 1.679119	test: 1.769056
MAE train: 1.142210	val: 1.249360	test: 1.315929

Epoch: 54
Loss: 2.7479583621025085
RMSE train: 1.506944	val: 1.673818	test: 1.765659
MAE train: 1.155763	val: 1.250979	test: 1.343565

Epoch: 55
Loss: 2.2097913026809692
RMSE train: 1.485711	val: 1.644555	test: 1.759403
MAE train: 1.146032	val: 1.234361	test: 1.340569

Epoch: 56
Loss: 1.6690922379493713
RMSE train: 1.453182	val: 1.585766	test: 1.745886
MAE train: 1.122684	val: 1.201690	test: 1.326836

Epoch: 57
Loss: 1.9848571419715881
RMSE train: 1.429935	val: 1.545275	test: 1.731995
MAE train: 1.086318	val: 1.163236	test: 1.297961

Epoch: 58
Loss: 1.8387739658355713
RMSE train: 1.364712	val: 1.466844	test: 1.686049
MAE train: 1.010575	val: 1.087172	test: 1.235416

Epoch: 59
Loss: 1.8697911500930786
RMSE train: 1.302708	val: 1.407734	test: 1.626665
MAE train: 0.952522	val: 1.032712	test: 1.165420

Epoch: 60
Loss: 1.838029384613037
RMSE train: 1.224270	val: 1.354495	test: 1.557751
MAE train: 0.895993	val: 0.991110	test: 1.100725

Epoch: 61
Loss: 1.8993427157402039
RMSE train: 1.192409	val: 1.318273	test: 1.534163
MAE train: 0.875276	val: 0.984092	test: 1.106368

Epoch: 62
Loss: 1.5687731504440308
RMSE train: 1.204682	val: 1.332681	test: 1.542757
MAE train: 0.898174	val: 1.013363	test: 1.145545

Epoch: 63
Loss: 1.5454100966453552
RMSE train: 1.237003	val: 1.368158	test: 1.565836
MAE train: 0.943503	val: 1.056450	test: 1.190424

Epoch: 64
Loss: 1.8145374655723572
RMSE train: 1.304978	val: 1.451073	test: 1.625613
MAE train: 1.008657	val: 1.119489	test: 1.260505

Epoch: 65
Loss: 1.524834930896759
RMSE train: 1.311233	val: 1.469533	test: 1.639607
MAE train: 1.009747	val: 1.128526	test: 1.266235

Epoch: 66
Loss: 1.703324317932129
RMSE train: 1.320473	val: 1.462146	test: 1.664137
MAE train: 1.002341	val: 1.119959	test: 1.235250

Epoch: 67
Loss: 1.5134274363517761
RMSE train: 1.300809	val: 1.422581	test: 1.675061
MAE train: 0.966807	val: 1.085352	test: 1.177885

Epoch: 68
Loss: 1.6502952575683594
RMSE train: 1.235655	val: 1.340428	test: 1.628736
MAE train: 0.903204	val: 1.010079	test: 1.113612

Epoch: 69
Loss: 1.3864365816116333
RMSE train: 1.154218	val: 1.285941	test: 1.556444
MAE train: 0.833751	val: 0.970160	test: 1.052549

Epoch: 70
Loss: 1.204827070236206
RMSE train: 1.063308	val: 1.250668	test: 1.477858
MAE train: 0.765565	val: 0.945568	test: 1.014400

Epoch: 71
Loss: 1.3021062016487122
RMSE train: 0.989785	val: 1.227268	test: 1.416702
MAE train: 0.724755	val: 0.929603	test: 0.993013

Epoch: 72
Loss: 1.4526446461677551
RMSE train: 0.932159	val: 1.191852	test: 1.372237
MAE train: 0.680533	val: 0.885496	test: 0.955188

Epoch: 73
Loss: 1.5501940250396729
RMSE train: 0.949311	val: 1.187132	test: 1.393678
MAE train: 0.682077	val: 0.873359	test: 0.952582

Epoch: 74
Loss: 1.3648106455802917
RMSE train: 1.002893	val: 1.242300	test: 1.450998
MAE train: 0.722225	val: 0.914636	test: 0.969313

Epoch: 75
Loss: 1.2608797550201416
RMSE train: 1.050912	val: 1.282580	test: 1.497006
MAE train: 0.774723	val: 0.961447	test: 1.018856

Epoch: 76
Loss: 1.509050190448761
RMSE train: 1.084299	val: 1.309727	test: 1.509640
MAE train: 0.795118	val: 0.984820	test: 1.055883

Epoch: 77
Loss: 1.1644567251205444
RMSE train: 1.132183	val: 1.352454	test: 1.526187
MAE train: 0.825130	val: 1.021832	test: 1.088269

Epoch: 78
Loss: 1.3822826147079468
RMSE train: 1.094756	val: 1.334741	test: 1.486274
MAE train: 0.803935	val: 1.011415	test: 1.054549

Epoch: 79
Loss: 1.2931898832321167
RMSE train: 1.009890	val: 1.278894	test: 1.396852
MAE train: 0.752802	val: 0.959914	test: 0.974111

Epoch: 80
Loss: 1.3659934401512146
RMSE train: 0.956467	val: 1.228174	test: 1.343619
MAE train: 0.711898	val: 0.913748	test: 0.921084

Epoch: 81
Loss: 1.3845228552818298
RMSE train: 0.932515	val: 1.187218	test: 1.330664
MAE train: 0.686637	val: 0.874612	test: 0.912042

Epoch: 82
Loss: 1.4895517230033875
RMSE train: 0.926429	val: 1.196753	test: 1.355840
MAE train: 0.673821	val: 0.870109	test: 0.933730

Epoch: 83
Loss: 1.2303746342658997
RMSE train: 0.898145	val: 1.193378	test: 1.379904
MAE train: 0.653912	val: 0.864914	test: 0.949895

Epoch: 23
Loss: 9.860552787780762
RMSE train: 3.717716	val: 3.578984	test: 3.904237
MAE train: 3.337923	val: 3.203540	test: 3.493100

Epoch: 24
Loss: 10.41481876373291
RMSE train: 3.663268	val: 3.576465	test: 3.866704
MAE train: 3.275893	val: 3.182387	test: 3.439115

Epoch: 25
Loss: 9.028260231018066
RMSE train: 3.614540	val: 3.577315	test: 3.833965
MAE train: 3.215256	val: 3.157680	test: 3.381852

Epoch: 26
Loss: 8.903471946716309
RMSE train: 3.584860	val: 3.579773	test: 3.798871
MAE train: 3.172688	val: 3.137616	test: 3.331338

Epoch: 27
Loss: 8.40229082107544
RMSE train: 3.536871	val: 3.546911	test: 3.733336
MAE train: 3.122961	val: 3.100080	test: 3.268865

Epoch: 28
Loss: 8.651408672332764
RMSE train: 3.474491	val: 3.484683	test: 3.651367
MAE train: 3.071159	val: 3.046100	test: 3.204211

Epoch: 29
Loss: 7.5589258670806885
RMSE train: 3.385828	val: 3.374241	test: 3.541959
MAE train: 2.992671	val: 2.949048	test: 3.113344

Epoch: 30
Loss: 7.901275873184204
RMSE train: 3.339534	val: 3.291192	test: 3.479097
MAE train: 2.958859	val: 2.887498	test: 3.070653

Epoch: 31
Loss: 7.280879020690918
RMSE train: 3.283215	val: 3.193978	test: 3.410704
MAE train: 2.912375	val: 2.807839	test: 3.020523

Epoch: 32
Loss: 8.076605319976807
RMSE train: 3.254226	val: 3.141876	test: 3.370980
MAE train: 2.890917	val: 2.763019	test: 2.992986

Epoch: 33
Loss: 6.581366062164307
RMSE train: 3.148447	val: 3.052971	test: 3.280925
MAE train: 2.779104	val: 2.672860	test: 2.899230

Epoch: 34
Loss: 5.911969184875488
RMSE train: 3.089313	val: 3.032134	test: 3.239431
MAE train: 2.711106	val: 2.646275	test: 2.847386

Epoch: 35
Loss: 6.087393045425415
RMSE train: 3.009215	val: 2.990698	test: 3.174144
MAE train: 2.625727	val: 2.592086	test: 2.782828

Epoch: 36
Loss: 5.533250570297241
RMSE train: 2.931927	val: 2.933223	test: 3.114240
MAE train: 2.555945	val: 2.521412	test: 2.724173

Epoch: 37
Loss: 4.9942626953125
RMSE train: 2.887288	val: 2.899022	test: 3.073257
MAE train: 2.520137	val: 2.474767	test: 2.679853

Epoch: 38
Loss: 4.969686508178711
RMSE train: 2.853452	val: 2.862665	test: 3.025982
MAE train: 2.493393	val: 2.427951	test: 2.625174

Epoch: 39
Loss: 4.970301389694214
RMSE train: 2.767399	val: 2.780292	test: 2.942222
MAE train: 2.412389	val: 2.349085	test: 2.538570

Epoch: 40
Loss: 4.653378486633301
RMSE train: 2.693672	val: 2.714438	test: 2.873329
MAE train: 2.343948	val: 2.292051	test: 2.467836

Epoch: 41
Loss: 4.338820457458496
RMSE train: 2.591545	val: 2.612136	test: 2.783364
MAE train: 2.237645	val: 2.202017	test: 2.379179

Epoch: 42
Loss: 3.967334747314453
RMSE train: 2.481029	val: 2.491036	test: 2.685762
MAE train: 2.117637	val: 2.085050	test: 2.277206

Epoch: 43
Loss: 3.5941728353500366
RMSE train: 2.379919	val: 2.389292	test: 2.598874
MAE train: 2.002857	val: 1.974141	test: 2.178007

Epoch: 44
Loss: 3.7728887796401978
RMSE train: 2.294445	val: 2.286681	test: 2.512471
MAE train: 1.907090	val: 1.865646	test: 2.080889

Epoch: 45
Loss: 3.2902004718780518
RMSE train: 2.184549	val: 2.185036	test: 2.398232
MAE train: 1.800707	val: 1.762740	test: 1.971455

Epoch: 46
Loss: 3.0814777612686157
RMSE train: 2.107864	val: 2.121606	test: 2.308376
MAE train: 1.728282	val: 1.691799	test: 1.880156

Epoch: 47
Loss: 3.073447346687317
RMSE train: 2.071062	val: 2.095441	test: 2.247518
MAE train: 1.698195	val: 1.656447	test: 1.827632

Epoch: 48
Loss: 2.779604434967041
RMSE train: 2.006014	val: 2.062941	test: 2.169899
MAE train: 1.641403	val: 1.614280	test: 1.758250

Epoch: 49
Loss: 2.3603529930114746
RMSE train: 1.915347	val: 2.014684	test: 2.085634
MAE train: 1.545498	val: 1.559796	test: 1.668015

Epoch: 50
Loss: 2.5362950563430786
RMSE train: 1.818541	val: 1.940709	test: 2.000153
MAE train: 1.431870	val: 1.468644	test: 1.555140

Epoch: 51
Loss: 2.260696530342102
RMSE train: 1.753411	val: 1.872972	test: 1.947796
MAE train: 1.364390	val: 1.391980	test: 1.484230

Epoch: 52
Loss: 2.1484785079956055
RMSE train: 1.727565	val: 1.806145	test: 1.929232
MAE train: 1.365107	val: 1.371220	test: 1.487525

Epoch: 53
Loss: 2.36781108379364
RMSE train: 1.719512	val: 1.758622	test: 1.909221
MAE train: 1.370315	val: 1.353360	test: 1.486681

Epoch: 54
Loss: 1.9988141655921936
RMSE train: 1.599513	val: 1.627089	test: 1.807414
MAE train: 1.247358	val: 1.235201	test: 1.367628

Epoch: 55
Loss: 2.073413074016571
RMSE train: 1.445913	val: 1.479128	test: 1.698840
MAE train: 1.092999	val: 1.102589	test: 1.233821

Epoch: 56
Loss: 1.7661600708961487
RMSE train: 1.315286	val: 1.365627	test: 1.621135
MAE train: 0.970187	val: 1.007205	test: 1.123765

Epoch: 57
Loss: 2.068491220474243
RMSE train: 1.282228	val: 1.361064	test: 1.600427
MAE train: 0.942538	val: 0.999539	test: 1.097790

Epoch: 58
Loss: 1.9644151329994202
RMSE train: 1.289417	val: 1.410254	test: 1.606564
MAE train: 0.947251	val: 1.028425	test: 1.115345

Epoch: 59
Loss: 1.5334593057632446
RMSE train: 1.284813	val: 1.434795	test: 1.607462
MAE train: 0.940348	val: 1.030597	test: 1.119453

Epoch: 60
Loss: 1.733717679977417
RMSE train: 1.268652	val: 1.444604	test: 1.592370
MAE train: 0.921164	val: 1.022209	test: 1.109123

Epoch: 61
Loss: 1.8981395363807678
RMSE train: 1.223839	val: 1.453040	test: 1.552485
MAE train: 0.884114	val: 1.017752	test: 1.082704

Epoch: 62
Loss: 1.6260972023010254
RMSE train: 1.174505	val: 1.451281	test: 1.503229
MAE train: 0.846155	val: 1.011223	test: 1.046594

Epoch: 63
Loss: 1.7786969542503357
RMSE train: 1.144649	val: 1.445143	test: 1.475098
MAE train: 0.824330	val: 1.009753	test: 1.017963

Epoch: 64
Loss: 1.6072531938552856
RMSE train: 1.109564	val: 1.410862	test: 1.453795
MAE train: 0.805191	val: 0.991911	test: 0.997625

Epoch: 65
Loss: 1.808691918849945
RMSE train: 1.147184	val: 1.420059	test: 1.484040
MAE train: 0.841462	val: 1.011756	test: 1.047063

Epoch: 66
Loss: 1.4010071754455566
RMSE train: 1.154278	val: 1.429025	test: 1.495652
MAE train: 0.849247	val: 1.029392	test: 1.072527

Epoch: 67
Loss: 1.4606441855430603
RMSE train: 1.141762	val: 1.402948	test: 1.484408
MAE train: 0.841543	val: 1.021986	test: 1.070017

Epoch: 68
Loss: 1.643774926662445
RMSE train: 1.154726	val: 1.392659	test: 1.476087
MAE train: 0.856992	val: 1.027936	test: 1.065993

Epoch: 69
Loss: 1.8235963582992554
RMSE train: 1.188053	val: 1.399343	test: 1.475021
MAE train: 0.881122	val: 1.024571	test: 1.053373

Epoch: 70
Loss: 1.4042696356773376
RMSE train: 1.160540	val: 1.356637	test: 1.440501
MAE train: 0.855927	val: 0.995929	test: 1.022676

Epoch: 71
Loss: 1.158418595790863
RMSE train: 1.155166	val: 1.321352	test: 1.432280
MAE train: 0.855590	val: 0.977111	test: 1.029168

Epoch: 72
Loss: 1.4935829043388367
RMSE train: 1.174979	val: 1.318480	test: 1.450953
MAE train: 0.871520	val: 0.976147	test: 1.048704

Epoch: 73
Loss: 1.4143693447113037
RMSE train: 1.144409	val: 1.283208	test: 1.441923
MAE train: 0.831170	val: 0.939553	test: 1.010386

Epoch: 74
Loss: 1.34864741563797
RMSE train: 1.161273	val: 1.298704	test: 1.471300
MAE train: 0.830052	val: 0.937270	test: 1.027325

Epoch: 75
Loss: 1.435043752193451
RMSE train: 1.175608	val: 1.327238	test: 1.496434
MAE train: 0.843346	val: 0.957633	test: 1.048950

Epoch: 76
Loss: 1.3811892867088318
RMSE train: 1.138929	val: 1.323582	test: 1.468790
MAE train: 0.814888	val: 0.955643	test: 1.030538

Epoch: 77
Loss: 1.2525319457054138
RMSE train: 1.112332	val: 1.324555	test: 1.441246
MAE train: 0.803154	val: 0.963280	test: 1.015631

Epoch: 78
Loss: 1.3250150084495544
RMSE train: 1.123899	val: 1.335407	test: 1.439656
MAE train: 0.814586	val: 0.969250	test: 1.018428

Epoch: 79
Loss: 1.4524239301681519
RMSE train: 1.154536	val: 1.373726	test: 1.466284
MAE train: 0.836464	val: 0.984484	test: 1.022424

Epoch: 80
Loss: 1.2480790615081787
RMSE train: 1.116551	val: 1.383474	test: 1.462233
MAE train: 0.804801	val: 0.977297	test: 1.002781

Epoch: 81
Loss: 1.3686789870262146
RMSE train: 1.045732	val: 1.384214	test: 1.421756
MAE train: 0.757670	val: 0.965594	test: 0.965872

Epoch: 82
Loss: 1.2584378123283386
RMSE train: 1.009630	val: 1.385247	test: 1.404431
MAE train: 0.729361	val: 0.944587	test: 0.945631

Epoch: 83
Loss: 1.2657601237297058
RMSE train: 0.940161	val: 1.330788	test: 1.365172
MAE train: 0.684087	val: 0.903656	test: 0.918822

Epoch: 23
Loss: 9.493342399597168
RMSE train: 3.249185	val: 3.193112	test: 3.534715
MAE train: 2.808216	val: 2.745749	test: 3.072144

Epoch: 24
Loss: 9.34674072265625
RMSE train: 3.255713	val: 3.194715	test: 3.551592
MAE train: 2.835054	val: 2.761685	test: 3.101124

Epoch: 25
Loss: 8.76680850982666
RMSE train: 3.262739	val: 3.198843	test: 3.578818
MAE train: 2.857062	val: 2.769594	test: 3.127441

Epoch: 26
Loss: 8.866742610931396
RMSE train: 3.251805	val: 3.190040	test: 3.578437
MAE train: 2.852823	val: 2.756352	test: 3.117411

Epoch: 27
Loss: 8.324198484420776
RMSE train: 3.210414	val: 3.167008	test: 3.545781
MAE train: 2.807147	val: 2.725854	test: 3.064383

Epoch: 28
Loss: 7.913419485092163
RMSE train: 3.123912	val: 3.094491	test: 3.458204
MAE train: 2.717194	val: 2.654062	test: 2.968694

Epoch: 29
Loss: 7.549448013305664
RMSE train: 3.020491	val: 2.985157	test: 3.354161
MAE train: 2.618593	val: 2.553299	test: 2.863356

Epoch: 30
Loss: 7.697111368179321
RMSE train: 2.955264	val: 2.909382	test: 3.282719
MAE train: 2.556754	val: 2.475531	test: 2.794963

Epoch: 31
Loss: 6.3545472621917725
RMSE train: 2.861432	val: 2.813021	test: 3.205121
MAE train: 2.467553	val: 2.383718	test: 2.722113

Epoch: 32
Loss: 6.4791295528411865
RMSE train: 2.798998	val: 2.747440	test: 3.141437
MAE train: 2.399600	val: 2.307205	test: 2.660215

Epoch: 33
Loss: 6.179177522659302
RMSE train: 2.695046	val: 2.659253	test: 3.028805
MAE train: 2.287706	val: 2.200082	test: 2.546268

Epoch: 34
Loss: 5.7795493602752686
RMSE train: 2.606769	val: 2.612288	test: 2.923839
MAE train: 2.198152	val: 2.128357	test: 2.438116

Epoch: 35
Loss: 5.923228979110718
RMSE train: 2.526627	val: 2.581379	test: 2.841253
MAE train: 2.120478	val: 2.084603	test: 2.349833

Epoch: 36
Loss: 5.634782314300537
RMSE train: 2.441367	val: 2.522175	test: 2.760445
MAE train: 2.048346	val: 2.036547	test: 2.275967

Epoch: 37
Loss: 4.7954065799713135
RMSE train: 2.368571	val: 2.457838	test: 2.697824
MAE train: 1.982861	val: 1.990119	test: 2.209377

Epoch: 38
Loss: 4.522030472755432
RMSE train: 2.283789	val: 2.380609	test: 2.648248
MAE train: 1.902443	val: 1.934894	test: 2.165573

Epoch: 39
Loss: 4.8767619132995605
RMSE train: 2.262327	val: 2.354344	test: 2.630300
MAE train: 1.883962	val: 1.921673	test: 2.162986

Epoch: 40
Loss: 3.913406252861023
RMSE train: 2.196996	val: 2.302181	test: 2.575137
MAE train: 1.810378	val: 1.863152	test: 2.106740

Epoch: 41
Loss: 4.758749485015869
RMSE train: 2.119794	val: 2.236469	test: 2.493352
MAE train: 1.721872	val: 1.798469	test: 2.028570

Epoch: 42
Loss: 3.800963282585144
RMSE train: 2.005634	val: 2.129735	test: 2.359268
MAE train: 1.594539	val: 1.694573	test: 1.901538

Epoch: 43
Loss: 3.800075054168701
RMSE train: 1.918129	val: 2.037965	test: 2.240204
MAE train: 1.512871	val: 1.600764	test: 1.793236

Epoch: 44
Loss: 3.488294243812561
RMSE train: 1.875755	val: 1.972286	test: 2.161206
MAE train: 1.494258	val: 1.556844	test: 1.733533

Epoch: 45
Loss: 3.3452776670455933
RMSE train: 1.894310	val: 1.981445	test: 2.151352
MAE train: 1.526945	val: 1.569562	test: 1.733821

Epoch: 46
Loss: 3.2356293201446533
RMSE train: 1.957466	val: 2.022739	test: 2.186733
MAE train: 1.581614	val: 1.591618	test: 1.747314

Epoch: 47
Loss: 3.1636719703674316
RMSE train: 1.950940	val: 2.015337	test: 2.186922
MAE train: 1.563344	val: 1.571236	test: 1.710003

Epoch: 48
Loss: 3.116654634475708
RMSE train: 1.868418	val: 1.954434	test: 2.169591
MAE train: 1.459299	val: 1.495716	test: 1.628261

Epoch: 49
Loss: 3.2973649501800537
RMSE train: 1.765199	val: 1.855532	test: 2.119735
MAE train: 1.343871	val: 1.406529	test: 1.531719

Epoch: 50
Loss: 2.5261436700820923
RMSE train: 1.667822	val: 1.767701	test: 2.064776
MAE train: 1.265816	val: 1.348373	test: 1.495680

Epoch: 51
Loss: 2.5533108711242676
RMSE train: 1.577960	val: 1.710362	test: 1.995257
MAE train: 1.200118	val: 1.300935	test: 1.470494

Epoch: 52
Loss: 2.487725615501404
RMSE train: 1.535989	val: 1.673602	test: 1.925688
MAE train: 1.190150	val: 1.286969	test: 1.455355

Epoch: 53
Loss: 2.3736919164657593
RMSE train: 1.518644	val: 1.665213	test: 1.875296
MAE train: 1.203814	val: 1.289322	test: 1.449284

Epoch: 54
Loss: 2.326176404953003
RMSE train: 1.543505	val: 1.682757	test: 1.870867
MAE train: 1.224087	val: 1.304639	test: 1.443985

Epoch: 55
Loss: 2.188840866088867
RMSE train: 1.488942	val: 1.699164	test: 1.830594
MAE train: 1.160678	val: 1.291503	test: 1.375348

Epoch: 56
Loss: 2.180596709251404
RMSE train: 1.467713	val: 1.734176	test: 1.822378
MAE train: 1.117475	val: 1.265995	test: 1.321935

Epoch: 57
Loss: 2.056555211544037
RMSE train: 1.468486	val: 1.766976	test: 1.828210
MAE train: 1.110126	val: 1.261919	test: 1.280186

Epoch: 58
Loss: 2.0441585779190063
RMSE train: 1.431650	val: 1.757159	test: 1.816253
MAE train: 1.080914	val: 1.252587	test: 1.264432

Epoch: 59
Loss: 1.7584730982780457
RMSE train: 1.342022	val: 1.696302	test: 1.777296
MAE train: 1.006797	val: 1.207391	test: 1.233710

Epoch: 60
Loss: 1.6417675614356995
RMSE train: 1.252986	val: 1.617649	test: 1.716841
MAE train: 0.930679	val: 1.145313	test: 1.179200

Epoch: 61
Loss: 1.6544103622436523
RMSE train: 1.159241	val: 1.533045	test: 1.634130
MAE train: 0.850957	val: 1.073054	test: 1.099808

Epoch: 62
Loss: 1.939376711845398
RMSE train: 1.069717	val: 1.434707	test: 1.535995
MAE train: 0.780555	val: 0.994967	test: 1.021310

Epoch: 63
Loss: 1.6682632565498352
RMSE train: 1.028624	val: 1.406053	test: 1.495304
MAE train: 0.749364	val: 0.963291	test: 0.988521

Epoch: 64
Loss: 1.5516998767852783
RMSE train: 1.025154	val: 1.418263	test: 1.500303
MAE train: 0.753328	val: 0.964268	test: 0.986853

Epoch: 65
Loss: 1.4927819967269897
RMSE train: 1.003322	val: 1.414154	test: 1.492532
MAE train: 0.741440	val: 0.967122	test: 0.972358

Epoch: 66
Loss: 1.512146234512329
RMSE train: 0.999279	val: 1.408747	test: 1.491769
MAE train: 0.738750	val: 0.973504	test: 0.980202

Epoch: 67
Loss: 1.289460301399231
RMSE train: 0.953475	val: 1.405950	test: 1.484965
MAE train: 0.699092	val: 0.963251	test: 0.974308

Epoch: 68
Loss: 1.3266944289207458
RMSE train: 0.885237	val: 1.400211	test: 1.476084
MAE train: 0.654133	val: 0.948104	test: 0.967710

Epoch: 69
Loss: 1.6423859000205994
RMSE train: 0.836301	val: 1.403552	test: 1.489440
MAE train: 0.626018	val: 0.923264	test: 0.964283

Epoch: 70
Loss: 1.2954916954040527
RMSE train: 0.836514	val: 1.428506	test: 1.531578
MAE train: 0.625279	val: 0.917033	test: 0.985577

Epoch: 71
Loss: 1.3286769390106201
RMSE train: 0.864230	val: 1.448046	test: 1.553987
MAE train: 0.645710	val: 0.930977	test: 1.000723

Epoch: 72
Loss: 1.4278549551963806
RMSE train: 0.848934	val: 1.462796	test: 1.519941
MAE train: 0.632434	val: 0.933222	test: 0.965167

Epoch: 73
Loss: 1.236257553100586
RMSE train: 0.831012	val: 1.460298	test: 1.463707
MAE train: 0.619057	val: 0.949069	test: 0.930834

Epoch: 74
Loss: 1.4606654047966003
RMSE train: 0.838490	val: 1.480891	test: 1.431289
MAE train: 0.623571	val: 0.976759	test: 0.913364

Epoch: 75
Loss: 1.4976380467414856
RMSE train: 0.806847	val: 1.462071	test: 1.392722
MAE train: 0.601328	val: 0.961477	test: 0.876666

Epoch: 76
Loss: 1.3712498545646667
RMSE train: 0.801226	val: 1.389303	test: 1.345453
MAE train: 0.607063	val: 0.925536	test: 0.857643

Epoch: 77
Loss: 1.349716305732727
RMSE train: 0.813133	val: 1.368727	test: 1.344553
MAE train: 0.620237	val: 0.912082	test: 0.884014

Epoch: 78
Loss: 1.0071741342544556
RMSE train: 0.887384	val: 1.390465	test: 1.402495
MAE train: 0.672441	val: 0.934416	test: 0.948235

Epoch: 79
Loss: 1.2627718448638916
RMSE train: 0.901798	val: 1.410272	test: 1.435160
MAE train: 0.680059	val: 0.941318	test: 0.971616

Epoch: 80
Loss: 1.2607367038726807
RMSE train: 0.851745	val: 1.410806	test: 1.415914
MAE train: 0.633660	val: 0.933711	test: 0.944739

Epoch: 81
Loss: 1.2920573949813843
RMSE train: 0.812302	val: 1.408748	test: 1.392854
MAE train: 0.601132	val: 0.934275	test: 0.921109

Epoch: 82
Loss: 1.6212644577026367
RMSE train: 0.821201	val: 1.369544	test: 1.343464
MAE train: 0.621256	val: 0.934347	test: 0.912163

Epoch: 83
Loss: 1.3149284720420837
RMSE train: 0.816348	val: 1.336526	test: 1.305539
MAE train: 0.623393	val: 0.938431	test: 0.901961

Epoch: 23
Loss: 9.369055271148682
RMSE train: 3.388697	val: 3.730084	test: 3.566124
MAE train: 2.998381	val: 3.177491	test: 3.178023

Epoch: 24
Loss: 8.48123550415039
RMSE train: 3.352633	val: 3.665229	test: 3.494401
MAE train: 2.975836	val: 3.129457	test: 3.135103

Epoch: 25
Loss: 8.568965911865234
RMSE train: 3.298760	val: 3.597337	test: 3.394871
MAE train: 2.934824	val: 3.069041	test: 3.067970

Epoch: 26
Loss: 8.174497127532959
RMSE train: 3.252388	val: 3.515918	test: 3.341060
MAE train: 2.893526	val: 2.993551	test: 3.022725

Epoch: 27
Loss: 7.818782091140747
RMSE train: 3.244695	val: 3.499806	test: 3.345534
MAE train: 2.887403	val: 2.978151	test: 3.021866

Epoch: 28
Loss: 7.405244588851929
RMSE train: 3.214107	val: 3.475994	test: 3.349532
MAE train: 2.853634	val: 2.947900	test: 2.997193

Epoch: 29
Loss: 7.181801795959473
RMSE train: 3.184757	val: 3.503330	test: 3.331759
MAE train: 2.812631	val: 2.952503	test: 2.953013

Epoch: 30
Loss: 6.928938388824463
RMSE train: 3.119468	val: 3.482395	test: 3.270718
MAE train: 2.741922	val: 2.916475	test: 2.884596

Epoch: 31
Loss: 6.076162576675415
RMSE train: 3.044224	val: 3.408729	test: 3.196182
MAE train: 2.672820	val: 2.848936	test: 2.813137

Epoch: 32
Loss: 5.961083889007568
RMSE train: 2.945620	val: 3.291136	test: 3.091503
MAE train: 2.584358	val: 2.742119	test: 2.716721

Epoch: 33
Loss: 5.57328462600708
RMSE train: 2.820044	val: 3.161332	test: 2.964507
MAE train: 2.463788	val: 2.612148	test: 2.601904

Epoch: 34
Loss: 5.319291591644287
RMSE train: 2.663514	val: 3.031539	test: 2.809490
MAE train: 2.302046	val: 2.477361	test: 2.449565

Epoch: 35
Loss: 5.104413032531738
RMSE train: 2.561635	val: 2.938939	test: 2.702402
MAE train: 2.197232	val: 2.393202	test: 2.350461

Epoch: 36
Loss: 4.603182792663574
RMSE train: 2.510441	val: 2.866413	test: 2.644489
MAE train: 2.163677	val: 2.354674	test: 2.321550

Epoch: 37
Loss: 4.365526914596558
RMSE train: 2.438636	val: 2.796010	test: 2.562848
MAE train: 2.095370	val: 2.294955	test: 2.254022

Epoch: 38
Loss: 4.054293870925903
RMSE train: 2.307938	val: 2.690065	test: 2.432153
MAE train: 1.958424	val: 2.171538	test: 2.123724

Epoch: 39
Loss: 4.538588523864746
RMSE train: 2.191096	val: 2.605795	test: 2.316892
MAE train: 1.826996	val: 2.054713	test: 1.995397

Epoch: 40
Loss: 3.600434422492981
RMSE train: 2.041943	val: 2.492861	test: 2.200311
MAE train: 1.661531	val: 1.930949	test: 1.854707

Epoch: 41
Loss: 3.7454808950424194
RMSE train: 1.955728	val: 2.420357	test: 2.128557
MAE train: 1.580942	val: 1.869447	test: 1.780782

Epoch: 42
Loss: 3.4091265201568604
RMSE train: 1.897297	val: 2.350492	test: 2.074145
MAE train: 1.529830	val: 1.813584	test: 1.733979

Epoch: 43
Loss: 3.195712447166443
RMSE train: 1.852215	val: 2.292756	test: 2.041760
MAE train: 1.501398	val: 1.776123	test: 1.710085

Epoch: 44
Loss: 2.826984405517578
RMSE train: 1.806534	val: 2.248676	test: 1.995021
MAE train: 1.468207	val: 1.742500	test: 1.674924

Epoch: 45
Loss: 2.8110289573669434
RMSE train: 1.773357	val: 2.226006	test: 1.959361
MAE train: 1.438827	val: 1.721572	test: 1.641402

Epoch: 46
Loss: 2.897478461265564
RMSE train: 1.758717	val: 2.210440	test: 1.930885
MAE train: 1.430620	val: 1.706063	test: 1.619410

Epoch: 47
Loss: 2.5343676805496216
RMSE train: 1.691254	val: 2.166556	test: 1.865146
MAE train: 1.355992	val: 1.649836	test: 1.545220

Epoch: 48
Loss: 2.7375630140304565
RMSE train: 1.634634	val: 2.123763	test: 1.797777
MAE train: 1.288049	val: 1.587025	test: 1.478217

Epoch: 49
Loss: 2.4536064863204956
RMSE train: 1.547146	val: 2.037005	test: 1.706231
MAE train: 1.197516	val: 1.491599	test: 1.386761

Epoch: 50
Loss: 2.388000011444092
RMSE train: 1.458961	val: 1.942717	test: 1.600166
MAE train: 1.111472	val: 1.385375	test: 1.281756

Epoch: 51
Loss: 2.017321765422821
RMSE train: 1.316363	val: 1.825904	test: 1.457826
MAE train: 0.987240	val: 1.263820	test: 1.143061

Epoch: 52
Loss: 2.0946145057678223
RMSE train: 1.210985	val: 1.728999	test: 1.371945
MAE train: 0.889912	val: 1.179099	test: 1.045361

Epoch: 53
Loss: 1.9444963932037354
RMSE train: 1.174742	val: 1.693499	test: 1.357336
MAE train: 0.855933	val: 1.156008	test: 1.045092

Epoch: 54
Loss: 1.8389999866485596
RMSE train: 1.192004	val: 1.656844	test: 1.353788
MAE train: 0.885053	val: 1.146459	test: 1.067841

Epoch: 55
Loss: 2.019058406352997
RMSE train: 1.212886	val: 1.678374	test: 1.364572
MAE train: 0.930332	val: 1.182539	test: 1.094898

Epoch: 56
Loss: 1.758545458316803
RMSE train: 1.207534	val: 1.701275	test: 1.377243
MAE train: 0.925891	val: 1.194832	test: 1.095898

Epoch: 57
Loss: 1.7693572044372559
RMSE train: 1.208756	val: 1.748917	test: 1.401990
MAE train: 0.907164	val: 1.211689	test: 1.070404

Epoch: 58
Loss: 1.903594732284546
RMSE train: 1.188407	val: 1.758308	test: 1.387625
MAE train: 0.873990	val: 1.203295	test: 1.017739

Epoch: 59
Loss: 1.6667844653129578
RMSE train: 1.111909	val: 1.703774	test: 1.325223
MAE train: 0.817895	val: 1.156968	test: 0.964111

Epoch: 60
Loss: 1.371052086353302
RMSE train: 1.058136	val: 1.624757	test: 1.304122
MAE train: 0.787517	val: 1.099475	test: 0.927977

Epoch: 61
Loss: 1.3537145853042603
RMSE train: 0.989598	val: 1.536191	test: 1.284598
MAE train: 0.740316	val: 1.039065	test: 0.890835

Epoch: 62
Loss: 1.4286130666732788
RMSE train: 0.924132	val: 1.463867	test: 1.271601
MAE train: 0.675140	val: 0.957489	test: 0.826529

Epoch: 63
Loss: 1.5530401468276978
RMSE train: 0.917907	val: 1.447677	test: 1.288996
MAE train: 0.665217	val: 0.934459	test: 0.849235

Epoch: 64
Loss: 1.193355679512024
RMSE train: 0.963748	val: 1.452980	test: 1.347335
MAE train: 0.697445	val: 0.938256	test: 0.904319

Epoch: 65
Loss: 1.422278106212616
RMSE train: 0.986534	val: 1.449744	test: 1.371941
MAE train: 0.715583	val: 0.935857	test: 0.906995

Epoch: 66
Loss: 1.2405349016189575
RMSE train: 1.013751	val: 1.460393	test: 1.377755
MAE train: 0.740892	val: 0.950789	test: 0.908247

Epoch: 67
Loss: 1.2592600584030151
RMSE train: 1.019964	val: 1.481434	test: 1.369098
MAE train: 0.739950	val: 0.966188	test: 0.904759

Epoch: 68
Loss: 1.4576557874679565
RMSE train: 0.968732	val: 1.497680	test: 1.347597
MAE train: 0.705025	val: 0.976617	test: 0.893121

Epoch: 69
Loss: 1.3364994525909424
RMSE train: 0.892777	val: 1.488315	test: 1.336756
MAE train: 0.651267	val: 0.964858	test: 0.869788

Epoch: 70
Loss: 1.2740845680236816
RMSE train: 0.851359	val: 1.495478	test: 1.329726
MAE train: 0.621423	val: 0.967672	test: 0.867742

Epoch: 71
Loss: 1.333849847316742
RMSE train: 0.834455	val: 1.496461	test: 1.303189
MAE train: 0.612596	val: 0.977196	test: 0.851690

Epoch: 72
Loss: 1.307137131690979
RMSE train: 0.826783	val: 1.497105	test: 1.273223
MAE train: 0.608004	val: 0.976027	test: 0.829932

Epoch: 73
Loss: 1.2899554371833801
RMSE train: 0.842612	val: 1.507810	test: 1.270486
MAE train: 0.623577	val: 0.974540	test: 0.831803

Epoch: 74
Loss: 1.2369563579559326
RMSE train: 0.850648	val: 1.533419	test: 1.271604
MAE train: 0.632013	val: 0.980282	test: 0.845779

Epoch: 75
Loss: 1.2161179780960083
RMSE train: 0.839963	val: 1.519230	test: 1.259390
MAE train: 0.626048	val: 0.971521	test: 0.837478

Epoch: 76
Loss: 1.228119432926178
RMSE train: 0.822437	val: 1.478726	test: 1.232937
MAE train: 0.609828	val: 0.951822	test: 0.824939

Epoch: 77
Loss: 1.3690431714057922
RMSE train: 0.795353	val: 1.421648	test: 1.190924
MAE train: 0.574216	val: 0.915672	test: 0.780570

Epoch: 78
Loss: 1.209095537662506
RMSE train: 0.783192	val: 1.394560	test: 1.173093
MAE train: 0.559534	val: 0.906035	test: 0.782125

Epoch: 79
Loss: 1.0617918372154236
RMSE train: 0.803725	val: 1.432130	test: 1.185026
MAE train: 0.577315	val: 0.926122	test: 0.809186

Epoch: 80
Loss: 1.5281344652175903
RMSE train: 0.824596	val: 1.465378	test: 1.214588
MAE train: 0.605044	val: 0.951890	test: 0.837678

Epoch: 81
Loss: 0.9580422937870026
RMSE train: 0.872306	val: 1.479263	test: 1.242744
MAE train: 0.655406	val: 0.972649	test: 0.869064

Epoch: 82
Loss: 1.0988324284553528
RMSE train: 0.869577	val: 1.445021	test: 1.232511
MAE train: 0.660470	val: 0.954158	test: 0.865353

Epoch: 83
Loss: 1.0986009240150452
RMSE train: 0.839724	val: 1.430473	test: 1.195778
MAE train: 0.639352	val: 0.940756	test: 0.827089

Epoch: 23
Loss: 8.533294677734375
RMSE train: 3.445230	val: 3.904176	test: 3.623958
MAE train: 3.036303	val: 3.283068	test: 3.255415

Epoch: 24
Loss: 8.062013626098633
RMSE train: 3.395850	val: 3.832492	test: 3.608928
MAE train: 2.990966	val: 3.215388	test: 3.220821

Epoch: 25
Loss: 8.323686122894287
RMSE train: 3.343156	val: 3.739721	test: 3.579714
MAE train: 2.944781	val: 3.147005	test: 3.177882

Epoch: 26
Loss: 7.625049591064453
RMSE train: 3.249109	val: 3.643034	test: 3.488816
MAE train: 2.871283	val: 3.067026	test: 3.104359

Epoch: 27
Loss: 6.9430646896362305
RMSE train: 3.139214	val: 3.536950	test: 3.365504
MAE train: 2.771958	val: 2.964125	test: 2.992448

Epoch: 28
Loss: 7.10753059387207
RMSE train: 3.072161	val: 3.446591	test: 3.290754
MAE train: 2.708884	val: 2.885322	test: 2.922743

Epoch: 29
Loss: 6.709501504898071
RMSE train: 2.969061	val: 3.337576	test: 3.189079
MAE train: 2.609588	val: 2.793925	test: 2.822958

Epoch: 30
Loss: 6.479239225387573
RMSE train: 2.842025	val: 3.213047	test: 3.060486
MAE train: 2.486501	val: 2.682888	test: 2.697527

Epoch: 31
Loss: 6.028931140899658
RMSE train: 2.735472	val: 3.101383	test: 2.940682
MAE train: 2.381013	val: 2.579691	test: 2.582717

Epoch: 32
Loss: 5.5678229331970215
RMSE train: 2.679870	val: 3.045061	test: 2.866620
MAE train: 2.327576	val: 2.525254	test: 2.512741

Epoch: 33
Loss: 5.216973543167114
RMSE train: 2.647285	val: 3.014859	test: 2.807633
MAE train: 2.290960	val: 2.489961	test: 2.457368

Epoch: 34
Loss: 4.940815687179565
RMSE train: 2.605462	val: 2.971133	test: 2.747270
MAE train: 2.246999	val: 2.444395	test: 2.399618

Epoch: 35
Loss: 4.7249836921691895
RMSE train: 2.503035	val: 2.902732	test: 2.626973
MAE train: 2.151464	val: 2.365388	test: 2.293672

Epoch: 36
Loss: 4.410843133926392
RMSE train: 2.419837	val: 2.831491	test: 2.531386
MAE train: 2.078792	val: 2.302390	test: 2.217136

Epoch: 37
Loss: 4.380081653594971
RMSE train: 2.370855	val: 2.759428	test: 2.490602
MAE train: 2.040763	val: 2.238659	test: 2.184165

Epoch: 38
Loss: 4.211395502090454
RMSE train: 2.340357	val: 2.715337	test: 2.478104
MAE train: 2.020601	val: 2.201448	test: 2.175840

Epoch: 39
Loss: 3.8461161851882935
RMSE train: 2.292343	val: 2.666605	test: 2.471370
MAE train: 1.980965	val: 2.163163	test: 2.159076

Epoch: 40
Loss: 3.8077235221862793
RMSE train: 2.247395	val: 2.645758	test: 2.440271
MAE train: 1.937760	val: 2.143810	test: 2.115481

Epoch: 41
Loss: 3.519157648086548
RMSE train: 2.205404	val: 2.625171	test: 2.399091
MAE train: 1.891033	val: 2.122068	test: 2.062772

Epoch: 42
Loss: 3.104798913002014
RMSE train: 2.117720	val: 2.531025	test: 2.304913
MAE train: 1.800293	val: 2.032696	test: 1.975788

Epoch: 43
Loss: 2.9484658241271973
RMSE train: 2.048833	val: 2.448906	test: 2.216729
MAE train: 1.722299	val: 1.944825	test: 1.885762

Epoch: 44
Loss: 2.7369813919067383
RMSE train: 2.016253	val: 2.386398	test: 2.195984
MAE train: 1.679219	val: 1.884625	test: 1.850652

Epoch: 45
Loss: 2.6154433488845825
RMSE train: 1.985081	val: 2.366986	test: 2.163091
MAE train: 1.648098	val: 1.852853	test: 1.823617

Epoch: 46
Loss: 2.5232272148132324
RMSE train: 1.950100	val: 2.355508	test: 2.164459
MAE train: 1.607863	val: 1.830139	test: 1.802829

Epoch: 47
Loss: 2.541961908340454
RMSE train: 1.918809	val: 2.340966	test: 2.159017
MAE train: 1.562288	val: 1.799353	test: 1.772338

Epoch: 48
Loss: 2.456080913543701
RMSE train: 1.850808	val: 2.320455	test: 2.113618
MAE train: 1.478407	val: 1.758994	test: 1.697519

Epoch: 49
Loss: 2.307058811187744
RMSE train: 1.785524	val: 2.271850	test: 2.042921
MAE train: 1.414991	val: 1.706824	test: 1.626515

Epoch: 50
Loss: 2.191922426223755
RMSE train: 1.687285	val: 2.178926	test: 1.904003
MAE train: 1.330889	val: 1.619045	test: 1.524785

Epoch: 51
Loss: 2.0173537731170654
RMSE train: 1.566647	val: 2.052204	test: 1.753893
MAE train: 1.220851	val: 1.498392	test: 1.406007

Epoch: 52
Loss: 1.7934424877166748
RMSE train: 1.443398	val: 1.935427	test: 1.619430
MAE train: 1.103237	val: 1.384066	test: 1.283009

Epoch: 53
Loss: 2.2803611755371094
RMSE train: 1.342715	val: 1.825289	test: 1.526606
MAE train: 1.006509	val: 1.286271	test: 1.186238

Epoch: 54
Loss: 2.1074845790863037
RMSE train: 1.273668	val: 1.769019	test: 1.452034
MAE train: 0.936214	val: 1.222076	test: 1.091948

Epoch: 55
Loss: 1.9078627228736877
RMSE train: 1.222005	val: 1.709399	test: 1.393024
MAE train: 0.895357	val: 1.171966	test: 1.049277

Epoch: 56
Loss: 1.5545703172683716
RMSE train: 1.248626	val: 1.717429	test: 1.434703
MAE train: 0.921248	val: 1.190492	test: 1.086447

Epoch: 57
Loss: 2.0216349363327026
RMSE train: 1.285089	val: 1.725801	test: 1.480439
MAE train: 0.966675	val: 1.219244	test: 1.132294

Epoch: 58
Loss: 1.7144050002098083
RMSE train: 1.284110	val: 1.724969	test: 1.477338
MAE train: 0.962810	val: 1.214171	test: 1.117990

Epoch: 59
Loss: 1.4734310507774353
RMSE train: 1.227194	val: 1.750925	test: 1.430803
MAE train: 0.904015	val: 1.213602	test: 1.061116

Epoch: 60
Loss: 1.4055779576301575
RMSE train: 1.190330	val: 1.787154	test: 1.419501
MAE train: 0.868374	val: 1.223749	test: 1.029305

Epoch: 61
Loss: 1.4362938404083252
RMSE train: 1.135361	val: 1.752889	test: 1.398101
MAE train: 0.823913	val: 1.191314	test: 1.008498

Epoch: 62
Loss: 1.4936911463737488
RMSE train: 1.072991	val: 1.674279	test: 1.377847
MAE train: 0.785146	val: 1.138145	test: 1.000255

Epoch: 63
Loss: 1.6140270829200745
RMSE train: 1.069045	val: 1.665035	test: 1.387089
MAE train: 0.780886	val: 1.126675	test: 1.006641

Epoch: 64
Loss: 1.3068339228630066
RMSE train: 1.023974	val: 1.661105	test: 1.362743
MAE train: 0.740002	val: 1.102843	test: 0.972427

Epoch: 65
Loss: 1.3815467357635498
RMSE train: 1.001377	val: 1.673771	test: 1.334940
MAE train: 0.716259	val: 1.097029	test: 0.941946

Epoch: 66
Loss: 1.5255146026611328
RMSE train: 1.023462	val: 1.720707	test: 1.321779
MAE train: 0.734050	val: 1.127613	test: 0.944673

Epoch: 67
Loss: 1.404902696609497
RMSE train: 1.057497	val: 1.738105	test: 1.314936
MAE train: 0.760421	val: 1.143192	test: 0.959043

Epoch: 68
Loss: 1.4159704446792603
RMSE train: 1.043913	val: 1.701633	test: 1.296147
MAE train: 0.758426	val: 1.122789	test: 0.960252

Epoch: 69
Loss: 1.246252715587616
RMSE train: 1.022180	val: 1.649752	test: 1.281842
MAE train: 0.752609	val: 1.089984	test: 0.961877

Epoch: 70
Loss: 1.4189015626907349
RMSE train: 1.035326	val: 1.620175	test: 1.300097
MAE train: 0.770702	val: 1.080906	test: 0.968023

Epoch: 71
Loss: 1.2725515961647034
RMSE train: 1.060557	val: 1.613358	test: 1.338058
MAE train: 0.790154	val: 1.077458	test: 0.970256

Epoch: 72
Loss: 1.2863634824752808
RMSE train: 1.072419	val: 1.660688	test: 1.356959
MAE train: 0.794924	val: 1.105382	test: 0.968563

Epoch: 73
Loss: 1.2879981994628906
RMSE train: 1.050422	val: 1.683489	test: 1.350431
MAE train: 0.776642	val: 1.122950	test: 0.967153

Epoch: 74
Loss: 1.3530113101005554
RMSE train: 1.023709	val: 1.701528	test: 1.317021
MAE train: 0.761925	val: 1.134970	test: 0.963388

Epoch: 75
Loss: 1.2652066946029663
RMSE train: 0.997302	val: 1.670561	test: 1.285645
MAE train: 0.746360	val: 1.112836	test: 0.959615

Epoch: 76
Loss: 1.4350616335868835
RMSE train: 1.021178	val: 1.674184	test: 1.278773
MAE train: 0.769213	val: 1.112712	test: 0.964768

Epoch: 77
Loss: 1.1545143723487854
RMSE train: 1.035422	val: 1.670350	test: 1.263528
MAE train: 0.775423	val: 1.107452	test: 0.958366

Epoch: 78
Loss: 1.3116022944450378
RMSE train: 1.061687	val: 1.683344	test: 1.254861
MAE train: 0.795644	val: 1.114546	test: 0.956932

Epoch: 79
Loss: 1.1109209060668945
RMSE train: 1.055914	val: 1.676667	test: 1.239936
MAE train: 0.786498	val: 1.102431	test: 0.931057

Epoch: 80
Loss: 1.128742277622223
RMSE train: 1.049353	val: 1.672306	test: 1.239851
MAE train: 0.779696	val: 1.093967	test: 0.922144

Epoch: 81
Loss: 1.3299351930618286
RMSE train: 1.058463	val: 1.689778	test: 1.267483
MAE train: 0.787489	val: 1.107478	test: 0.930496

Epoch: 82
Loss: 1.0892364978790283
RMSE train: 1.050978	val: 1.681419	test: 1.275094
MAE train: 0.780660	val: 1.101041	test: 0.937371

Epoch: 83
Loss: 1.12983900308609
RMSE train: 1.034002	val: 1.669968	test: 1.269847
MAE train: 0.765021	val: 1.090398	test: 0.931443

Epoch: 23
Loss: 9.151881217956543
RMSE train: 3.352475	val: 3.614358	test: 3.613763
MAE train: 2.958013	val: 3.062060	test: 3.217247

Epoch: 24
Loss: 8.773650646209717
RMSE train: 3.281256	val: 3.547112	test: 3.559576
MAE train: 2.890407	val: 2.997606	test: 3.156981

Epoch: 25
Loss: 8.543002605438232
RMSE train: 3.220736	val: 3.502939	test: 3.490824
MAE train: 2.843329	val: 2.955049	test: 3.103539

Epoch: 26
Loss: 8.121376752853394
RMSE train: 3.154099	val: 3.446926	test: 3.403079
MAE train: 2.786043	val: 2.896483	test: 3.028129

Epoch: 27
Loss: 8.296263694763184
RMSE train: 3.082576	val: 3.378764	test: 3.309929
MAE train: 2.716103	val: 2.823524	test: 2.940731

Epoch: 28
Loss: 7.345867395401001
RMSE train: 2.992714	val: 3.275155	test: 3.191159
MAE train: 2.630325	val: 2.727588	test: 2.834325

Epoch: 29
Loss: 6.981386423110962
RMSE train: 2.888811	val: 3.163997	test: 3.057970
MAE train: 2.529229	val: 2.618794	test: 2.719854

Epoch: 30
Loss: 6.574442386627197
RMSE train: 2.812117	val: 3.087926	test: 2.982148
MAE train: 2.451343	val: 2.543549	test: 2.651443

Epoch: 31
Loss: 5.886942148208618
RMSE train: 2.753026	val: 3.061668	test: 2.926645
MAE train: 2.389509	val: 2.509693	test: 2.596001

Epoch: 32
Loss: 6.0149900913238525
RMSE train: 2.737332	val: 3.072042	test: 2.927819
MAE train: 2.370118	val: 2.514971	test: 2.581082

Epoch: 33
Loss: 5.894263744354248
RMSE train: 2.723634	val: 3.091116	test: 2.922611
MAE train: 2.356492	val: 2.526503	test: 2.570147

Epoch: 34
Loss: 5.799988269805908
RMSE train: 2.683615	val: 3.070551	test: 2.875506
MAE train: 2.328570	val: 2.507943	test: 2.540113

Epoch: 35
Loss: 4.905458211898804
RMSE train: 2.594848	val: 2.963596	test: 2.789611
MAE train: 2.258304	val: 2.418009	test: 2.472596

Epoch: 36
Loss: 4.874615907669067
RMSE train: 2.462834	val: 2.835319	test: 2.662060
MAE train: 2.131713	val: 2.283736	test: 2.348037

Epoch: 37
Loss: 4.710537075996399
RMSE train: 2.335792	val: 2.736676	test: 2.526216
MAE train: 2.002198	val: 2.162459	test: 2.213164

Epoch: 38
Loss: 4.553716421127319
RMSE train: 2.262685	val: 2.665171	test: 2.461656
MAE train: 1.930592	val: 2.086625	test: 2.134495

Epoch: 39
Loss: 3.9734668731689453
RMSE train: 2.232659	val: 2.621778	test: 2.455816
MAE train: 1.903383	val: 2.054738	test: 2.100678

Epoch: 40
Loss: 4.433304786682129
RMSE train: 2.234208	val: 2.586721	test: 2.468241
MAE train: 1.902026	val: 2.042224	test: 2.097747

Epoch: 41
Loss: 3.953531265258789
RMSE train: 2.177488	val: 2.521383	test: 2.422099
MAE train: 1.838759	val: 1.987241	test: 2.041716

Epoch: 42
Loss: 3.3163281679153442
RMSE train: 2.095718	val: 2.444143	test: 2.356293
MAE train: 1.739261	val: 1.902366	test: 1.951442

Epoch: 43
Loss: 3.2059110403060913
RMSE train: 1.973392	val: 2.335268	test: 2.247902
MAE train: 1.618563	val: 1.798305	test: 1.845953

Epoch: 44
Loss: 3.308518648147583
RMSE train: 1.820624	val: 2.230402	test: 2.099975
MAE train: 1.467246	val: 1.678007	test: 1.704366

Epoch: 45
Loss: 2.9475903511047363
RMSE train: 1.740112	val: 2.163039	test: 1.988642
MAE train: 1.397214	val: 1.592529	test: 1.607470

Epoch: 46
Loss: 2.594156265258789
RMSE train: 1.643137	val: 2.081853	test: 1.878337
MAE train: 1.312736	val: 1.505310	test: 1.521242

Epoch: 47
Loss: 2.7589796781539917
RMSE train: 1.573831	val: 2.017142	test: 1.791692
MAE train: 1.252945	val: 1.440417	test: 1.458850

Epoch: 48
Loss: 2.7874046564102173
RMSE train: 1.562777	val: 2.009016	test: 1.777186
MAE train: 1.237089	val: 1.422631	test: 1.430987

Epoch: 49
Loss: 2.3097665905952454
RMSE train: 1.562967	val: 2.013521	test: 1.778326
MAE train: 1.237686	val: 1.433304	test: 1.420459

Epoch: 50
Loss: 2.210782051086426
RMSE train: 1.541692	val: 1.997716	test: 1.755958
MAE train: 1.219637	val: 1.430333	test: 1.393015

Epoch: 51
Loss: 1.9932023286819458
RMSE train: 1.457435	val: 1.952801	test: 1.684429
MAE train: 1.132220	val: 1.376825	test: 1.319383

Epoch: 52
Loss: 1.8720791935920715
RMSE train: 1.365463	val: 1.895708	test: 1.610472
MAE train: 1.038293	val: 1.308949	test: 1.242301

Epoch: 53
Loss: 1.8333863019943237
RMSE train: 1.321879	val: 1.896233	test: 1.583048
MAE train: 0.995556	val: 1.301660	test: 1.214937

Epoch: 54
Loss: 1.97410249710083
RMSE train: 1.282581	val: 1.868951	test: 1.535978
MAE train: 0.971637	val: 1.281770	test: 1.182500

Epoch: 55
Loss: 1.7412748336791992
RMSE train: 1.256147	val: 1.846239	test: 1.518300
MAE train: 0.948165	val: 1.252390	test: 1.150721

Epoch: 56
Loss: 1.6358157396316528
RMSE train: 1.245067	val: 1.832053	test: 1.519501
MAE train: 0.940759	val: 1.237985	test: 1.142462

Epoch: 57
Loss: 1.7292287349700928
RMSE train: 1.243150	val: 1.800269	test: 1.553683
MAE train: 0.933359	val: 1.212819	test: 1.135583

Epoch: 58
Loss: 1.6112918257713318
RMSE train: 1.292036	val: 1.835709	test: 1.598320
MAE train: 0.955833	val: 1.229763	test: 1.147831

Epoch: 59
Loss: 1.724072515964508
RMSE train: 1.324399	val: 1.838515	test: 1.628059
MAE train: 0.951480	val: 1.209053	test: 1.129406

Epoch: 60
Loss: 1.596916913986206
RMSE train: 1.303248	val: 1.823794	test: 1.602508
MAE train: 0.923044	val: 1.187768	test: 1.101435

Epoch: 61
Loss: 1.639628291130066
RMSE train: 1.199260	val: 1.719309	test: 1.460984
MAE train: 0.856610	val: 1.107016	test: 1.028274

Epoch: 62
Loss: 1.666826844215393
RMSE train: 1.059853	val: 1.592647	test: 1.267567
MAE train: 0.763875	val: 1.019350	test: 0.945648

Epoch: 63
Loss: 1.5099757313728333
RMSE train: 1.019303	val: 1.532058	test: 1.210260
MAE train: 0.730806	val: 0.977223	test: 0.922331

Epoch: 64
Loss: 1.5499181151390076
RMSE train: 0.998395	val: 1.512953	test: 1.182770
MAE train: 0.714828	val: 0.965882	test: 0.895481

Epoch: 65
Loss: 1.481597661972046
RMSE train: 0.968334	val: 1.523398	test: 1.175925
MAE train: 0.707451	val: 0.980012	test: 0.883535

Epoch: 66
Loss: 1.2886892557144165
RMSE train: 0.957872	val: 1.540209	test: 1.182480
MAE train: 0.707614	val: 0.991688	test: 0.885003

Epoch: 67
Loss: 1.3036752939224243
RMSE train: 0.958246	val: 1.552312	test: 1.170798
MAE train: 0.702142	val: 0.992436	test: 0.866025

Epoch: 68
Loss: 1.3801432847976685
RMSE train: 0.976272	val: 1.595553	test: 1.185055
MAE train: 0.713989	val: 1.010554	test: 0.864638

Epoch: 69
Loss: 1.4586238861083984
RMSE train: 0.990591	val: 1.617998	test: 1.191441
MAE train: 0.715933	val: 1.030231	test: 0.873401

Epoch: 70
Loss: 1.3626295924186707
RMSE train: 0.978782	val: 1.623762	test: 1.186631
MAE train: 0.700039	val: 1.034753	test: 0.876294

Epoch: 71
Loss: 1.3102422952651978
RMSE train: 0.923203	val: 1.542549	test: 1.145003
MAE train: 0.659527	val: 0.994294	test: 0.841421

Epoch: 72
Loss: 1.3755847811698914
RMSE train: 0.907621	val: 1.482262	test: 1.115049
MAE train: 0.647772	val: 0.949554	test: 0.817516

Epoch: 73
Loss: 1.219273567199707
RMSE train: 0.884619	val: 1.431464	test: 1.083679
MAE train: 0.635885	val: 0.913255	test: 0.811678

Epoch: 74
Loss: 1.2299829125404358
RMSE train: 0.914200	val: 1.450297	test: 1.106112
MAE train: 0.671348	val: 0.938946	test: 0.843835

Epoch: 75
Loss: 1.1994074583053589
RMSE train: 0.869384	val: 1.398224	test: 1.089637
MAE train: 0.639185	val: 0.901760	test: 0.816384

Epoch: 76
Loss: 1.236290454864502
RMSE train: 0.826240	val: 1.373381	test: 1.090532
MAE train: 0.601726	val: 0.883149	test: 0.789348

Epoch: 77
Loss: 1.361014485359192
RMSE train: 0.820254	val: 1.402405	test: 1.112448
MAE train: 0.589405	val: 0.909460	test: 0.782800

Epoch: 78
Loss: 0.9908220171928406
RMSE train: 0.828287	val: 1.445710	test: 1.141932
MAE train: 0.597453	val: 0.942218	test: 0.818722

Epoch: 79
Loss: 1.3391557931900024
RMSE train: 0.859209	val: 1.486527	test: 1.175991
MAE train: 0.612543	val: 0.968210	test: 0.845389

Epoch: 80
Loss: 1.1993669867515564
RMSE train: 0.877944	val: 1.501165	test: 1.183271
MAE train: 0.623716	val: 0.975225	test: 0.847807

Epoch: 81
Loss: 1.057026207447052
RMSE train: 0.876844	val: 1.518821	test: 1.176553
MAE train: 0.627900	val: 0.985864	test: 0.858549

Epoch: 82
Loss: 1.2544053196907043
RMSE train: 0.888194	val: 1.511345	test: 1.164643
MAE train: 0.646538	val: 0.987009	test: 0.859746

Epoch: 83
Loss: 1.1574424505233765
RMSE train: 0.863572	val: 1.441258	test: 1.120347
MAE train: 0.634638	val: 0.934957	test: 0.848583

Epoch: 23
Loss: 9.441236734390259
RMSE train: 3.492918	val: 4.177852	test: 3.338919
MAE train: 3.122581	val: 3.734291	test: 3.056364

Epoch: 24
Loss: 9.1804838180542
RMSE train: 3.463411	val: 4.160187	test: 3.337049
MAE train: 3.106789	val: 3.732412	test: 3.061371

Epoch: 25
Loss: 8.764984607696533
RMSE train: 3.449639	val: 4.152731	test: 3.331949
MAE train: 3.101755	val: 3.730788	test: 3.060670

Epoch: 26
Loss: 8.280216693878174
RMSE train: 3.424797	val: 4.126983	test: 3.295186
MAE train: 3.084771	val: 3.691403	test: 3.035236

Epoch: 27
Loss: 7.934966325759888
RMSE train: 3.421171	val: 4.097903	test: 3.279919
MAE train: 3.082476	val: 3.644113	test: 3.013761

Epoch: 28
Loss: 7.81780481338501
RMSE train: 3.432892	val: 4.050258	test: 3.285176
MAE train: 3.093738	val: 3.587728	test: 3.012876

Epoch: 29
Loss: 7.4189066886901855
RMSE train: 3.419078	val: 3.997782	test: 3.282937
MAE train: 3.075580	val: 3.540528	test: 3.007664

Epoch: 30
Loss: 7.003873586654663
RMSE train: 3.370725	val: 3.935306	test: 3.267136
MAE train: 3.025452	val: 3.471391	test: 2.996100

Epoch: 31
Loss: 6.574889183044434
RMSE train: 3.278293	val: 3.848440	test: 3.221572
MAE train: 2.933648	val: 3.373266	test: 2.938338

Epoch: 32
Loss: 6.210570812225342
RMSE train: 3.140463	val: 3.722626	test: 3.124070
MAE train: 2.792932	val: 3.235156	test: 2.819565

Epoch: 33
Loss: 5.888077735900879
RMSE train: 2.976099	val: 3.568388	test: 2.981619
MAE train: 2.628237	val: 3.072683	test: 2.671753

Epoch: 34
Loss: 5.5332300662994385
RMSE train: 2.821582	val: 3.429691	test: 2.822600
MAE train: 2.472539	val: 2.923901	test: 2.522667

Epoch: 35
Loss: 5.355604410171509
RMSE train: 2.699542	val: 3.321106	test: 2.677432
MAE train: 2.351558	val: 2.800407	test: 2.409134

Epoch: 36
Loss: 4.899030685424805
RMSE train: 2.628486	val: 3.236123	test: 2.575118
MAE train: 2.296352	val: 2.726974	test: 2.346401

Epoch: 37
Loss: 4.6836628913879395
RMSE train: 2.596013	val: 3.183804	test: 2.512896
MAE train: 2.277349	val: 2.689936	test: 2.298500

Epoch: 38
Loss: 4.525771379470825
RMSE train: 2.563106	val: 3.133552	test: 2.452981
MAE train: 2.244954	val: 2.640555	test: 2.235354

Epoch: 39
Loss: 4.1772119998931885
RMSE train: 2.496369	val: 3.065255	test: 2.363211
MAE train: 2.173322	val: 2.560016	test: 2.140065

Epoch: 40
Loss: 3.953155040740967
RMSE train: 2.391236	val: 2.962710	test: 2.254451
MAE train: 2.062617	val: 2.448835	test: 2.026844

Epoch: 41
Loss: 3.8686949014663696
RMSE train: 2.251404	val: 2.825174	test: 2.141095
MAE train: 1.918869	val: 2.292559	test: 1.905133

Epoch: 42
Loss: 3.4716298580169678
RMSE train: 2.105459	val: 2.689448	test: 2.012260
MAE train: 1.773318	val: 2.132537	test: 1.781605

Epoch: 43
Loss: 3.289381265640259
RMSE train: 1.971331	val: 2.578187	test: 1.897558
MAE train: 1.629917	val: 1.984511	test: 1.663105

Epoch: 44
Loss: 3.084608793258667
RMSE train: 1.891279	val: 2.496611	test: 1.799349
MAE train: 1.535202	val: 1.882643	test: 1.561233

Epoch: 45
Loss: 2.7862128019332886
RMSE train: 1.800033	val: 2.390859	test: 1.705092
MAE train: 1.449329	val: 1.787295	test: 1.474611

Epoch: 46
Loss: 2.8317830562591553
RMSE train: 1.736537	val: 2.288218	test: 1.657151
MAE train: 1.408151	val: 1.717898	test: 1.448600

Epoch: 47
Loss: 2.6279983520507812
RMSE train: 1.675391	val: 2.209010	test: 1.600507
MAE train: 1.351986	val: 1.655535	test: 1.390996

Epoch: 48
Loss: 2.5647692680358887
RMSE train: 1.614678	val: 2.158299	test: 1.524880
MAE train: 1.286472	val: 1.611462	test: 1.314174

Epoch: 49
Loss: 2.2461087703704834
RMSE train: 1.586334	val: 2.137299	test: 1.473397
MAE train: 1.246394	val: 1.583311	test: 1.249139

Epoch: 50
Loss: 2.1613634824752808
RMSE train: 1.570761	val: 2.132042	test: 1.457311
MAE train: 1.224631	val: 1.562346	test: 1.222951

Epoch: 51
Loss: 2.0678284764289856
RMSE train: 1.548150	val: 2.114428	test: 1.446098
MAE train: 1.208050	val: 1.545801	test: 1.221316

Epoch: 52
Loss: 2.151520609855652
RMSE train: 1.496714	val: 2.049993	test: 1.429462
MAE train: 1.171328	val: 1.498889	test: 1.213452

Epoch: 53
Loss: 1.7550424933433533
RMSE train: 1.409351	val: 1.947419	test: 1.376319
MAE train: 1.095905	val: 1.406618	test: 1.163749

Epoch: 54
Loss: 1.9660236239433289
RMSE train: 1.314513	val: 1.842524	test: 1.288762
MAE train: 1.008013	val: 1.302299	test: 1.071768

Epoch: 55
Loss: 1.8084987998008728
RMSE train: 1.242158	val: 1.792466	test: 1.214188
MAE train: 0.937058	val: 1.242403	test: 0.994946

Epoch: 56
Loss: 1.7294804453849792
RMSE train: 1.205853	val: 1.805963	test: 1.158369
MAE train: 0.905451	val: 1.218868	test: 0.935424

Epoch: 57
Loss: 1.6662334203720093
RMSE train: 1.229969	val: 1.859778	test: 1.146696
MAE train: 0.928547	val: 1.241906	test: 0.896048

Epoch: 58
Loss: 1.831686794757843
RMSE train: 1.225908	val: 1.823247	test: 1.157491
MAE train: 0.927211	val: 1.209373	test: 0.915115

Epoch: 59
Loss: 1.6439691185951233
RMSE train: 1.228086	val: 1.785373	test: 1.210306
MAE train: 0.935731	val: 1.209331	test: 0.975600

Epoch: 60
Loss: 1.4423680305480957
RMSE train: 1.187946	val: 1.722508	test: 1.206738
MAE train: 0.905546	val: 1.165354	test: 0.974283

Epoch: 61
Loss: 1.4660075902938843
RMSE train: 1.113696	val: 1.692766	test: 1.127889
MAE train: 0.832331	val: 1.136360	test: 0.893848

Epoch: 62
Loss: 1.5465684533119202
RMSE train: 1.085737	val: 1.664646	test: 1.107193
MAE train: 0.802493	val: 1.094531	test: 0.867056

Epoch: 63
Loss: 1.4170904159545898
RMSE train: 1.040452	val: 1.613870	test: 1.066888
MAE train: 0.760582	val: 1.050897	test: 0.827000

Epoch: 64
Loss: 1.5620505809783936
RMSE train: 1.025681	val: 1.612030	test: 1.038710
MAE train: 0.749412	val: 1.062409	test: 0.797813

Epoch: 65
Loss: 1.407723605632782
RMSE train: 1.019744	val: 1.642807	test: 1.000789
MAE train: 0.746030	val: 1.088513	test: 0.747459

Epoch: 66
Loss: 1.6234774589538574
RMSE train: 1.018318	val: 1.660117	test: 0.984620
MAE train: 0.737801	val: 1.090389	test: 0.730465

Epoch: 67
Loss: 1.4452187418937683
RMSE train: 0.998687	val: 1.613215	test: 0.958206
MAE train: 0.723978	val: 1.045365	test: 0.716369

Epoch: 68
Loss: 1.4344423413276672
RMSE train: 1.019455	val: 1.596526	test: 1.056005
MAE train: 0.753644	val: 1.038972	test: 0.811643

Epoch: 69
Loss: 1.3039811849594116
RMSE train: 1.023010	val: 1.567258	test: 1.145840
MAE train: 0.773939	val: 1.029836	test: 0.884701

Epoch: 70
Loss: 1.450568974018097
RMSE train: 0.993053	val: 1.563587	test: 1.114139
MAE train: 0.746437	val: 1.013996	test: 0.864648

Epoch: 71
Loss: 1.2570096254348755
RMSE train: 0.923173	val: 1.555653	test: 1.028490
MAE train: 0.674970	val: 0.975717	test: 0.768360

Epoch: 72
Loss: 1.3152326345443726
RMSE train: 0.888764	val: 1.532446	test: 0.978611
MAE train: 0.645709	val: 0.960627	test: 0.722769

Epoch: 73
Loss: 1.5417127013206482
RMSE train: 0.914559	val: 1.559918	test: 0.962506
MAE train: 0.656303	val: 0.986586	test: 0.700557

Epoch: 74
Loss: 1.3386901021003723
RMSE train: 0.927763	val: 1.551352	test: 0.957417
MAE train: 0.679860	val: 0.967966	test: 0.743001

Epoch: 75
Loss: 1.339305341243744
RMSE train: 0.922235	val: 1.559570	test: 0.910345
MAE train: 0.677293	val: 0.982314	test: 0.692962

Epoch: 76
Loss: 1.3027095198631287
RMSE train: 0.918700	val: 1.538463	test: 0.924187
MAE train: 0.676200	val: 0.957879	test: 0.689210

Epoch: 77
Loss: 1.4471490979194641
RMSE train: 0.917650	val: 1.536813	test: 1.035584
MAE train: 0.675748	val: 0.948649	test: 0.714486

Epoch: 78
Loss: 1.1702309846878052
RMSE train: 0.903294	val: 1.523159	test: 1.116347
MAE train: 0.663121	val: 0.954500	test: 0.734719

Epoch: 79
Loss: 1.408419907093048
RMSE train: 0.868187	val: 1.532181	test: 1.139195
MAE train: 0.632016	val: 0.970684	test: 0.725754

Epoch: 80
Loss: 1.1685732007026672
RMSE train: 0.873696	val: 1.595774	test: 1.119056
MAE train: 0.645382	val: 1.025949	test: 0.754512

Epoch: 81
Loss: 1.3160873055458069
RMSE train: 0.896899	val: 1.652860	test: 1.080644
MAE train: 0.668873	val: 1.079037	test: 0.775309

Epoch: 82
Loss: 1.2186528444290161
RMSE train: 0.914261	val: 1.682701	test: 1.095548
MAE train: 0.667494	val: 1.082118	test: 0.764677

Epoch: 83
Loss: 1.254130780696869
RMSE train: 0.939665	val: 1.660667	test: 1.104528
MAE train: 0.688832	val: 1.052034	test: 0.796953

Epoch: 23
Loss: 9.585485458374023
RMSE train: 3.230373	val: 3.927530	test: 3.289495
MAE train: 2.828000	val: 3.372296	test: 2.949532

Epoch: 24
Loss: 8.937244415283203
RMSE train: 3.155599	val: 3.875769	test: 3.218506
MAE train: 2.755210	val: 3.325190	test: 2.870010

Epoch: 25
Loss: 8.873514175415039
RMSE train: 3.085239	val: 3.818409	test: 3.144715
MAE train: 2.688227	val: 3.275191	test: 2.783816

Epoch: 26
Loss: 8.608129024505615
RMSE train: 3.032792	val: 3.756976	test: 3.092276
MAE train: 2.636940	val: 3.219985	test: 2.714810

Epoch: 27
Loss: 7.810962438583374
RMSE train: 2.951952	val: 3.671852	test: 3.016657
MAE train: 2.551980	val: 3.128684	test: 2.635363

Epoch: 28
Loss: 7.442831754684448
RMSE train: 2.860263	val: 3.580377	test: 2.907457
MAE train: 2.454575	val: 3.017329	test: 2.527052

Epoch: 29
Loss: 7.400073766708374
RMSE train: 2.762278	val: 3.479963	test: 2.814740
MAE train: 2.352138	val: 2.894590	test: 2.423833

Epoch: 30
Loss: 7.18373441696167
RMSE train: 2.659976	val: 3.375596	test: 2.705038
MAE train: 2.239562	val: 2.776430	test: 2.306529

Epoch: 31
Loss: 6.833017349243164
RMSE train: 2.576623	val: 3.287618	test: 2.617378
MAE train: 2.151143	val: 2.672533	test: 2.218180

Epoch: 32
Loss: 6.381819009780884
RMSE train: 2.541524	val: 3.219757	test: 2.606986
MAE train: 2.127990	val: 2.618327	test: 2.195274

Epoch: 33
Loss: 6.2027997970581055
RMSE train: 2.502138	val: 3.152640	test: 2.589062
MAE train: 2.107123	val: 2.573413	test: 2.183566

Epoch: 34
Loss: 5.665769815444946
RMSE train: 2.479350	val: 3.090089	test: 2.579750
MAE train: 2.109455	val: 2.551722	test: 2.192049

Epoch: 35
Loss: 5.326848745346069
RMSE train: 2.432538	val: 3.016877	test: 2.546542
MAE train: 2.077368	val: 2.504628	test: 2.174083

Epoch: 36
Loss: 5.297321319580078
RMSE train: 2.323237	val: 2.909739	test: 2.440239
MAE train: 1.971610	val: 2.383739	test: 2.094518

Epoch: 37
Loss: 4.979439735412598
RMSE train: 2.197416	val: 2.816004	test: 2.300918
MAE train: 1.844040	val: 2.250531	test: 1.985488

Epoch: 38
Loss: 4.698843240737915
RMSE train: 2.094582	val: 2.739398	test: 2.180187
MAE train: 1.735972	val: 2.134555	test: 1.874116

Epoch: 39
Loss: 4.323702454566956
RMSE train: 2.009725	val: 2.666716	test: 2.083614
MAE train: 1.638449	val: 2.027825	test: 1.771155

Epoch: 40
Loss: 4.185880661010742
RMSE train: 1.959660	val: 2.601394	test: 2.022473
MAE train: 1.586405	val: 1.949645	test: 1.711137

Epoch: 41
Loss: 3.854548692703247
RMSE train: 1.950885	val: 2.561495	test: 2.001605
MAE train: 1.589430	val: 1.914039	test: 1.699141

Epoch: 42
Loss: 3.6754276752471924
RMSE train: 1.941529	val: 2.511660	test: 1.993884
MAE train: 1.606658	val: 1.905826	test: 1.721264

Epoch: 43
Loss: 3.5789926052093506
RMSE train: 1.888409	val: 2.435891	test: 1.948744
MAE train: 1.558257	val: 1.852186	test: 1.684592

Epoch: 44
Loss: 3.298514246940613
RMSE train: 1.850968	val: 2.395917	test: 1.963087
MAE train: 1.520743	val: 1.824909	test: 1.670973

Epoch: 45
Loss: 3.161586880683899
RMSE train: 1.789869	val: 2.352785	test: 1.926792
MAE train: 1.458542	val: 1.784064	test: 1.626689

Epoch: 46
Loss: 2.8862743377685547
RMSE train: 1.730847	val: 2.300443	test: 1.912562
MAE train: 1.403104	val: 1.743920	test: 1.582382

Epoch: 47
Loss: 2.6561156511306763
RMSE train: 1.665436	val: 2.228330	test: 1.877732
MAE train: 1.338815	val: 1.677398	test: 1.516728

Epoch: 48
Loss: 2.819011688232422
RMSE train: 1.579976	val: 2.162847	test: 1.798326
MAE train: 1.246746	val: 1.587198	test: 1.412989

Epoch: 49
Loss: 2.579985797405243
RMSE train: 1.472442	val: 2.064529	test: 1.687206
MAE train: 1.136310	val: 1.461763	test: 1.290390

Epoch: 50
Loss: 2.213866353034973
RMSE train: 1.401340	val: 1.984615	test: 1.565953
MAE train: 1.084707	val: 1.387401	test: 1.222540

Epoch: 51
Loss: 2.2931571006774902
RMSE train: 1.354533	val: 1.934477	test: 1.507962
MAE train: 1.047318	val: 1.336332	test: 1.185613

Epoch: 52
Loss: 2.109783947467804
RMSE train: 1.333641	val: 1.908811	test: 1.517093
MAE train: 1.028058	val: 1.310552	test: 1.175857

Epoch: 53
Loss: 1.992610514163971
RMSE train: 1.314650	val: 1.899821	test: 1.512428
MAE train: 1.008471	val: 1.302020	test: 1.155957

Epoch: 54
Loss: 1.6555436849594116
RMSE train: 1.284282	val: 1.865820	test: 1.508544
MAE train: 0.973025	val: 1.288160	test: 1.120862

Epoch: 55
Loss: 1.8628147840499878
RMSE train: 1.253184	val: 1.827153	test: 1.505633
MAE train: 0.941523	val: 1.259812	test: 1.104186

Epoch: 56
Loss: 1.8711237907409668
RMSE train: 1.230043	val: 1.801021	test: 1.515769
MAE train: 0.913941	val: 1.230753	test: 1.083580

Epoch: 57
Loss: 1.8503682613372803
RMSE train: 1.187702	val: 1.743886	test: 1.472900
MAE train: 0.895734	val: 1.181835	test: 1.075939

Epoch: 58
Loss: 1.7987589836120605
RMSE train: 1.135141	val: 1.683846	test: 1.416812
MAE train: 0.863431	val: 1.131784	test: 1.036548

Epoch: 59
Loss: 1.7945800423622131
RMSE train: 1.031542	val: 1.601544	test: 1.301679
MAE train: 0.771662	val: 1.044530	test: 0.930206

Epoch: 60
Loss: 1.6758134961128235
RMSE train: 0.998139	val: 1.563385	test: 1.264350
MAE train: 0.731118	val: 1.027120	test: 0.880207

Epoch: 61
Loss: 1.8928548693656921
RMSE train: 1.059670	val: 1.612139	test: 1.351530
MAE train: 0.773817	val: 1.061897	test: 0.880617

Epoch: 62
Loss: 1.5053248405456543
RMSE train: 1.107825	val: 1.656404	test: 1.392796
MAE train: 0.810606	val: 1.088314	test: 0.906018

Epoch: 63
Loss: 1.4727003574371338
RMSE train: 1.117148	val: 1.694013	test: 1.394373
MAE train: 0.820274	val: 1.112692	test: 0.930904

Epoch: 64
Loss: 1.31475430727005
RMSE train: 1.088182	val: 1.659136	test: 1.380669
MAE train: 0.809542	val: 1.090762	test: 0.967820

Epoch: 65
Loss: 1.6346733570098877
RMSE train: 1.065536	val: 1.610797	test: 1.383360
MAE train: 0.799196	val: 1.063079	test: 0.991793

Epoch: 66
Loss: 1.3919973373413086
RMSE train: 1.049183	val: 1.605122	test: 1.333750
MAE train: 0.788992	val: 1.065041	test: 0.984736

Epoch: 67
Loss: 1.3480696082115173
RMSE train: 1.053100	val: 1.630344	test: 1.334372
MAE train: 0.775822	val: 1.052408	test: 0.927675

Epoch: 68
Loss: 1.3928865790367126
RMSE train: 1.083614	val: 1.661687	test: 1.384636
MAE train: 0.788307	val: 1.070576	test: 0.889371

Epoch: 69
Loss: 1.549617886543274
RMSE train: 1.065288	val: 1.589125	test: 1.408530
MAE train: 0.752275	val: 1.025827	test: 0.847462

Epoch: 70
Loss: 1.5244537591934204
RMSE train: 0.971897	val: 1.466646	test: 1.344543
MAE train: 0.678087	val: 0.922626	test: 0.805677

Epoch: 71
Loss: 1.3682087063789368
RMSE train: 0.905771	val: 1.369906	test: 1.317019
MAE train: 0.631705	val: 0.860859	test: 0.816644

Epoch: 72
Loss: 1.4253441095352173
RMSE train: 0.845613	val: 1.334763	test: 1.247131
MAE train: 0.592016	val: 0.838872	test: 0.801471

Epoch: 73
Loss: 1.3915086388587952
RMSE train: 0.831848	val: 1.367236	test: 1.180057
MAE train: 0.602302	val: 0.881373	test: 0.809735

Epoch: 74
Loss: 1.4504885077476501
RMSE train: 0.853024	val: 1.436308	test: 1.115121
MAE train: 0.626994	val: 0.953412	test: 0.803744

Epoch: 75
Loss: 1.4175617098808289
RMSE train: 0.855624	val: 1.426650	test: 1.115057
MAE train: 0.626350	val: 0.934117	test: 0.793764

Epoch: 76
Loss: 1.3350287675857544
RMSE train: 0.865122	val: 1.413328	test: 1.116540
MAE train: 0.636042	val: 0.922917	test: 0.804902

Epoch: 77
Loss: 1.2285006046295166
RMSE train: 0.842281	val: 1.400077	test: 1.131952
MAE train: 0.631858	val: 0.915916	test: 0.791342

Epoch: 78
Loss: 1.1369062662124634
RMSE train: 0.840142	val: 1.400179	test: 1.159753
MAE train: 0.635338	val: 0.909950	test: 0.826602

Epoch: 79
Loss: 1.5592588782310486
RMSE train: 0.850990	val: 1.415322	test: 1.204296
MAE train: 0.635959	val: 0.916002	test: 0.846350

Epoch: 80
Loss: 1.2629526257514954
RMSE train: 0.826483	val: 1.389697	test: 1.189449
MAE train: 0.601858	val: 0.872455	test: 0.794151

Epoch: 81
Loss: 1.247641146183014
RMSE train: 0.808943	val: 1.391247	test: 1.146376
MAE train: 0.582785	val: 0.856299	test: 0.755598

Epoch: 82
Loss: 1.2989433407783508
RMSE train: 0.826719	val: 1.412974	test: 1.169877
MAE train: 0.598432	val: 0.876096	test: 0.760369

Epoch: 83
Loss: 1.1661743521690369
RMSE train: 0.826099	val: 1.422972	test: 1.162770
MAE train: 0.602301	val: 0.904524	test: 0.777785

Epoch: 23
Loss: 9.057281970977783
RMSE train: 3.624568	val: 4.296528	test: 3.562424
MAE train: 3.237468	val: 3.800620	test: 3.201792

Epoch: 24
Loss: 8.723525047302246
RMSE train: 3.546240	val: 4.197378	test: 3.470339
MAE train: 3.167267	val: 3.708639	test: 3.118528

Epoch: 25
Loss: 8.507232189178467
RMSE train: 3.423043	val: 4.051966	test: 3.338879
MAE train: 3.051055	val: 3.557881	test: 3.012520

Epoch: 26
Loss: 7.979248523712158
RMSE train: 3.343469	val: 3.947577	test: 3.259821
MAE train: 2.981663	val: 3.451688	test: 2.950643

Epoch: 27
Loss: 7.286051273345947
RMSE train: 3.257653	val: 3.847914	test: 3.194309
MAE train: 2.903371	val: 3.348202	test: 2.891722

Epoch: 28
Loss: 7.173828601837158
RMSE train: 3.183499	val: 3.768369	test: 3.140897
MAE train: 2.830897	val: 3.256380	test: 2.832774

Epoch: 29
Loss: 7.105518579483032
RMSE train: 3.132436	val: 3.719396	test: 3.118613
MAE train: 2.779697	val: 3.202804	test: 2.791761

Epoch: 30
Loss: 6.5401082038879395
RMSE train: 3.074028	val: 3.675548	test: 3.109645
MAE train: 2.712708	val: 3.151275	test: 2.739687

Epoch: 31
Loss: 6.157069683074951
RMSE train: 2.986626	val: 3.610390	test: 3.035174
MAE train: 2.606097	val: 3.067484	test: 2.635241

Epoch: 32
Loss: 5.91628360748291
RMSE train: 2.861778	val: 3.496819	test: 2.907088
MAE train: 2.483193	val: 2.958631	test: 2.519588

Epoch: 33
Loss: 5.691501617431641
RMSE train: 2.775578	val: 3.376523	test: 2.832347
MAE train: 2.408444	val: 2.857233	test: 2.448958

Epoch: 34
Loss: 5.1540868282318115
RMSE train: 2.697023	val: 3.262699	test: 2.737668
MAE train: 2.337578	val: 2.761739	test: 2.382138

Epoch: 35
Loss: 4.836066246032715
RMSE train: 2.641153	val: 3.173172	test: 2.692588
MAE train: 2.285929	val: 2.683963	test: 2.345766

Epoch: 36
Loss: 4.62590479850769
RMSE train: 2.578085	val: 3.109884	test: 2.632980
MAE train: 2.226331	val: 2.615520	test: 2.285884

Epoch: 37
Loss: 4.736857652664185
RMSE train: 2.503486	val: 3.047058	test: 2.582267
MAE train: 2.140326	val: 2.532463	test: 2.194939

Epoch: 38
Loss: 4.022395372390747
RMSE train: 2.424542	val: 2.984387	test: 2.505532
MAE train: 2.043191	val: 2.446919	test: 2.085899

Epoch: 39
Loss: 4.026891589164734
RMSE train: 2.374873	val: 2.936903	test: 2.462603
MAE train: 1.980902	val: 2.376845	test: 2.023352

Epoch: 40
Loss: 3.97273325920105
RMSE train: 2.305775	val: 2.836565	test: 2.389205
MAE train: 1.937512	val: 2.290103	test: 1.988439

Epoch: 41
Loss: 3.5004135370254517
RMSE train: 2.223588	val: 2.719396	test: 2.299124
MAE train: 1.878098	val: 2.187175	test: 1.943929

Epoch: 42
Loss: 3.3424627780914307
RMSE train: 2.120480	val: 2.600626	test: 2.180751
MAE train: 1.785505	val: 2.076271	test: 1.857875

Epoch: 43
Loss: 3.1058050394058228
RMSE train: 2.034748	val: 2.521270	test: 2.086302
MAE train: 1.702188	val: 2.008702	test: 1.769474

Epoch: 44
Loss: 2.972623109817505
RMSE train: 1.957285	val: 2.473858	test: 2.023357
MAE train: 1.615511	val: 1.947794	test: 1.680010

Epoch: 45
Loss: 2.7102437019348145
RMSE train: 1.901248	val: 2.423823	test: 1.985045
MAE train: 1.560831	val: 1.892801	test: 1.626273

Epoch: 46
Loss: 2.5733126401901245
RMSE train: 1.827821	val: 2.366530	test: 1.942003
MAE train: 1.490135	val: 1.823211	test: 1.561933

Epoch: 47
Loss: 2.3478291034698486
RMSE train: 1.790140	val: 2.323689	test: 1.950483
MAE train: 1.464844	val: 1.780661	test: 1.555135

Epoch: 48
Loss: 2.370753526687622
RMSE train: 1.699603	val: 2.228730	test: 1.879589
MAE train: 1.379878	val: 1.676880	test: 1.486446

Epoch: 49
Loss: 2.092385768890381
RMSE train: 1.603482	val: 2.138641	test: 1.776476
MAE train: 1.274577	val: 1.566896	test: 1.375868

Epoch: 50
Loss: 2.064317464828491
RMSE train: 1.534696	val: 2.073991	test: 1.645655
MAE train: 1.183154	val: 1.482655	test: 1.259066

Epoch: 51
Loss: 1.8524487018585205
RMSE train: 1.468949	val: 1.993021	test: 1.597673
MAE train: 1.131118	val: 1.407098	test: 1.215334

Epoch: 52
Loss: 2.1013278365135193
RMSE train: 1.414537	val: 1.931218	test: 1.572688
MAE train: 1.093298	val: 1.356478	test: 1.203151

Epoch: 53
Loss: 1.839998185634613
RMSE train: 1.380271	val: 1.888368	test: 1.539778
MAE train: 1.069799	val: 1.324189	test: 1.194226

Epoch: 54
Loss: 1.537801742553711
RMSE train: 1.338742	val: 1.840820	test: 1.525381
MAE train: 1.031497	val: 1.287173	test: 1.159187

Epoch: 55
Loss: 1.62401682138443
RMSE train: 1.286130	val: 1.790180	test: 1.480746
MAE train: 0.983856	val: 1.242188	test: 1.101953

Epoch: 56
Loss: 1.5646823644638062
RMSE train: 1.226032	val: 1.769376	test: 1.359997
MAE train: 0.925956	val: 1.225962	test: 1.017983

Epoch: 57
Loss: 1.5219039916992188
RMSE train: 1.200965	val: 1.748513	test: 1.316576
MAE train: 0.900280	val: 1.203817	test: 0.980899

Epoch: 58
Loss: 1.5441237688064575
RMSE train: 1.223011	val: 1.748470	test: 1.300572
MAE train: 0.913243	val: 1.189414	test: 0.980856

Epoch: 59
Loss: 1.4671414494514465
RMSE train: 1.268041	val: 1.763169	test: 1.309758
MAE train: 0.944522	val: 1.182487	test: 1.004958

Epoch: 60
Loss: 1.7136733531951904
RMSE train: 1.245998	val: 1.718492	test: 1.330634
MAE train: 0.936257	val: 1.150872	test: 1.037801

Epoch: 61
Loss: 1.4414144158363342
RMSE train: 1.153774	val: 1.654485	test: 1.309039
MAE train: 0.873237	val: 1.104168	test: 1.020238

Epoch: 62
Loss: 1.3306291103363037
RMSE train: 1.092389	val: 1.640330	test: 1.288156
MAE train: 0.827142	val: 1.081136	test: 0.984893

Epoch: 63
Loss: 1.5374127626419067
RMSE train: 1.054519	val: 1.637611	test: 1.315754
MAE train: 0.789934	val: 1.074672	test: 0.959805

Epoch: 64
Loss: 1.594479262828827
RMSE train: 1.032079	val: 1.630746	test: 1.324471
MAE train: 0.762628	val: 1.078927	test: 0.918670

Epoch: 65
Loss: 1.498399019241333
RMSE train: 1.059723	val: 1.653829	test: 1.311651
MAE train: 0.780598	val: 1.109138	test: 0.907779

Epoch: 66
Loss: 1.44096040725708
RMSE train: 1.050302	val: 1.649624	test: 1.219331
MAE train: 0.769350	val: 1.092639	test: 0.890832

Epoch: 67
Loss: 1.4616692662239075
RMSE train: 1.063723	val: 1.672597	test: 1.201032
MAE train: 0.786515	val: 1.103457	test: 0.930099

Epoch: 68
Loss: 1.3390304446220398
RMSE train: 1.085515	val: 1.675640	test: 1.231891
MAE train: 0.819244	val: 1.091648	test: 0.997224

Epoch: 69
Loss: 1.2491152882575989
RMSE train: 1.074001	val: 1.660016	test: 1.211525
MAE train: 0.812086	val: 1.062279	test: 0.982185

Epoch: 70
Loss: 1.3079458475112915
RMSE train: 1.069524	val: 1.658762	test: 1.229062
MAE train: 0.801771	val: 1.076145	test: 0.953723

Epoch: 71
Loss: 1.3059183359146118
RMSE train: 1.057192	val: 1.681766	test: 1.230102
MAE train: 0.791157	val: 1.071050	test: 0.899731

Epoch: 72
Loss: 1.2808071374893188
RMSE train: 1.056294	val: 1.722973	test: 1.227009
MAE train: 0.794248	val: 1.093429	test: 0.876615

Epoch: 73
Loss: 1.3919812440872192
RMSE train: 1.037814	val: 1.742184	test: 1.184909
MAE train: 0.777096	val: 1.109101	test: 0.849125

Epoch: 74
Loss: 1.1611890196800232
RMSE train: 0.973636	val: 1.702534	test: 1.122805
MAE train: 0.724906	val: 1.080269	test: 0.812025

Epoch: 75
Loss: 1.3522732853889465
RMSE train: 0.927267	val: 1.630852	test: 1.099907
MAE train: 0.691070	val: 1.031418	test: 0.796335

Epoch: 76
Loss: 1.3226165771484375
RMSE train: 0.933997	val: 1.618852	test: 1.090204
MAE train: 0.700699	val: 1.025032	test: 0.801489

Epoch: 77
Loss: 1.3288561701774597
RMSE train: 0.956761	val: 1.637184	test: 1.104092
MAE train: 0.714969	val: 1.020887	test: 0.824804

Epoch: 78
Loss: 1.3603671789169312
RMSE train: 0.980123	val: 1.664979	test: 1.127227
MAE train: 0.732266	val: 1.020220	test: 0.840249

Epoch: 79
Loss: 1.2576721906661987
RMSE train: 0.964166	val: 1.658811	test: 1.111605
MAE train: 0.716691	val: 0.989977	test: 0.823164

Epoch: 80
Loss: 1.1505802869796753
RMSE train: 0.923454	val: 1.628998	test: 1.081838
MAE train: 0.678097	val: 0.964866	test: 0.792586

Epoch: 81
Loss: 1.6523353457450867
RMSE train: 0.893842	val: 1.610633	test: 1.094559
MAE train: 0.659239	val: 0.978878	test: 0.798928

Epoch: 82
Loss: 1.5695228576660156
RMSE train: 0.887913	val: 1.594563	test: 1.055832
MAE train: 0.660182	val: 0.997040	test: 0.784070

Epoch: 83
Loss: 1.2921175956726074
RMSE train: 0.887182	val: 1.559477	test: 1.065321
MAE train: 0.658460	val: 0.985275	test: 0.800407

Epoch: 84
Loss: 1.4111101031303406
RMSE train: 0.916805	val: 1.224414	test: 1.433170
MAE train: 0.674312	val: 0.886667	test: 0.987485

Epoch: 85
Loss: 1.433120608329773
RMSE train: 0.972331	val: 1.290054	test: 1.488948
MAE train: 0.727103	val: 0.944060	test: 1.032250

Epoch: 86
Loss: 1.3005993366241455
RMSE train: 1.001044	val: 1.334068	test: 1.508288
MAE train: 0.750320	val: 0.971243	test: 1.034570

Epoch: 87
Loss: 1.3923348188400269
RMSE train: 1.012653	val: 1.342221	test: 1.491301
MAE train: 0.757779	val: 0.973133	test: 1.015411

Epoch: 88
Loss: 1.1415587067604065
RMSE train: 0.987337	val: 1.302769	test: 1.449429
MAE train: 0.731218	val: 0.934249	test: 0.977393

Epoch: 89
Loss: 1.4433500170707703
RMSE train: 0.934403	val: 1.251708	test: 1.400672
MAE train: 0.683691	val: 0.894188	test: 0.933201

Epoch: 90
Loss: 1.3981274366378784
RMSE train: 0.902632	val: 1.231834	test: 1.361341
MAE train: 0.661345	val: 0.886581	test: 0.902388

Epoch: 91
Loss: 1.2989585399627686
RMSE train: 0.913194	val: 1.236808	test: 1.374354
MAE train: 0.678503	val: 0.894907	test: 0.920733

Epoch: 92
Loss: 1.1014010310173035
RMSE train: 0.922555	val: 1.247818	test: 1.395897
MAE train: 0.686765	val: 0.900913	test: 0.933978

Epoch: 93
Loss: 1.2047278881072998
RMSE train: 0.918348	val: 1.227668	test: 1.394345
MAE train: 0.686668	val: 0.897612	test: 0.941580

Epoch: 94
Loss: 1.5867052674293518
RMSE train: 0.934123	val: 1.220856	test: 1.390867
MAE train: 0.701601	val: 0.898769	test: 0.954072

Epoch: 95
Loss: 1.2170727849006653
RMSE train: 0.949897	val: 1.222735	test: 1.398006
MAE train: 0.711409	val: 0.907547	test: 0.964039

Epoch: 96
Loss: 1.1682904064655304
RMSE train: 0.977125	val: 1.238025	test: 1.416427
MAE train: 0.731997	val: 0.925773	test: 0.989207

Epoch: 97
Loss: 1.4672343730926514
RMSE train: 1.043343	val: 1.298952	test: 1.460073
MAE train: 0.784684	val: 0.974596	test: 1.035434

Epoch: 98
Loss: 1.106650948524475
RMSE train: 1.029715	val: 1.317491	test: 1.457244
MAE train: 0.770512	val: 0.982366	test: 1.015451

Epoch: 99
Loss: 1.13881915807724
RMSE train: 1.000267	val: 1.323544	test: 1.448157
MAE train: 0.750028	val: 0.976468	test: 0.992669

Epoch: 100
Loss: 1.6415191888809204
RMSE train: 0.961124	val: 1.279451	test: 1.426734
MAE train: 0.722238	val: 0.947851	test: 0.983634

Epoch: 101
Loss: 1.1850652694702148
RMSE train: 0.883634	val: 1.198541	test: 1.386716
MAE train: 0.663854	val: 0.891170	test: 0.948017

Epoch: 102
Loss: 1.1727957725524902
RMSE train: 0.829644	val: 1.141482	test: 1.357981
MAE train: 0.622203	val: 0.849648	test: 0.916692

Epoch: 103
Loss: 1.0020419359207153
RMSE train: 0.819069	val: 1.131022	test: 1.350108
MAE train: 0.617937	val: 0.842002	test: 0.905831

Epoch: 104
Loss: 1.043066918849945
RMSE train: 0.871466	val: 1.150611	test: 1.370096
MAE train: 0.655044	val: 0.850532	test: 0.924860

Epoch: 105
Loss: 1.2938141226768494
RMSE train: 0.946819	val: 1.192423	test: 1.409272
MAE train: 0.705651	val: 0.866158	test: 0.957125

Epoch: 106
Loss: 1.1864213347434998
RMSE train: 0.957366	val: 1.194527	test: 1.420963
MAE train: 0.712564	val: 0.869788	test: 0.981662

Epoch: 107
Loss: 1.1176281571388245
RMSE train: 0.950061	val: 1.189346	test: 1.424328
MAE train: 0.708779	val: 0.876186	test: 0.990170

Epoch: 108
Loss: 0.8571905195713043
RMSE train: 0.956524	val: 1.223212	test: 1.437031
MAE train: 0.717595	val: 0.907985	test: 1.001567

Epoch: 109
Loss: 1.0692766308784485
RMSE train: 0.977531	val: 1.256277	test: 1.451220
MAE train: 0.742258	val: 0.936599	test: 1.022086

Epoch: 110
Loss: 0.9785396158695221
RMSE train: 0.987629	val: 1.280797	test: 1.456613
MAE train: 0.756395	val: 0.954727	test: 1.050310

Epoch: 111
Loss: 1.0856900215148926
RMSE train: 0.926921	val: 1.259132	test: 1.415670
MAE train: 0.707973	val: 0.931521	test: 1.005216

Epoch: 112
Loss: 1.1395834386348724
RMSE train: 0.849214	val: 1.231620	test: 1.375148
MAE train: 0.646550	val: 0.896694	test: 0.944970

Epoch: 113
Loss: 1.0911108255386353
RMSE train: 0.824182	val: 1.220774	test: 1.355028
MAE train: 0.626528	val: 0.876694	test: 0.913749

Epoch: 114
Loss: 1.4298157095909119
RMSE train: 0.849978	val: 1.212720	test: 1.351396
MAE train: 0.647186	val: 0.877757	test: 0.905774

Epoch: 115
Loss: 0.9656740725040436
RMSE train: 0.862286	val: 1.212999	test: 1.349374
MAE train: 0.654426	val: 0.882828	test: 0.899924

Epoch: 116
Loss: 1.4172919392585754
RMSE train: 0.875050	val: 1.220519	test: 1.356244
MAE train: 0.661768	val: 0.887243	test: 0.900712

Epoch: 117
Loss: 0.9947158992290497
RMSE train: 0.918809	val: 1.248958	test: 1.377013
MAE train: 0.690158	val: 0.905661	test: 0.920189

Epoch: 118
Loss: 1.094167321920395
RMSE train: 0.942727	val: 1.252550	test: 1.389327
MAE train: 0.703962	val: 0.905279	test: 0.930469

Epoch: 119
Loss: 1.0092830955982208
RMSE train: 0.956400	val: 1.249724	test: 1.404424
MAE train: 0.712562	val: 0.902483	test: 0.948675

Epoch: 120
Loss: 1.1073782444000244
RMSE train: 0.996797	val: 1.255219	test: 1.425878
MAE train: 0.742652	val: 0.902545	test: 0.978733

Epoch: 121
Loss: 1.0886072516441345
RMSE train: 1.027640	val: 1.249853	test: 1.462601
MAE train: 0.760932	val: 0.902986	test: 1.013770

Epoch: 122
Loss: 1.7325872778892517
RMSE train: 1.059642	val: 1.258528	test: 1.498771
MAE train: 0.782448	val: 0.909527	test: 1.041281

Epoch: 123
Loss: 1.1436212062835693
RMSE train: 1.070687	val: 1.252778	test: 1.517833
MAE train: 0.794544	val: 0.920473	test: 1.059619

Epoch: 124
Loss: 1.070203959941864
RMSE train: 1.071125	val: 1.256068	test: 1.532202
MAE train: 0.807608	val: 0.929332	test: 1.061499

Epoch: 125
Loss: 1.009559452533722
RMSE train: 1.048840	val: 1.231022	test: 1.517839
MAE train: 0.795100	val: 0.910114	test: 1.037747

Epoch: 126
Loss: 1.0714762210845947
RMSE train: 1.011662	val: 1.185441	test: 1.471951
MAE train: 0.759200	val: 0.869528	test: 0.991469

Epoch: 127
Loss: 1.2435451745986938
RMSE train: 0.923918	val: 1.147226	test: 1.401002
MAE train: 0.690460	val: 0.838523	test: 0.930306

Epoch: 128
Loss: 0.9489932358264923
RMSE train: 0.826622	val: 1.125772	test: 1.345525
MAE train: 0.615137	val: 0.811918	test: 0.893280

Epoch: 129
Loss: 0.8746077716350555
RMSE train: 0.780654	val: 1.111547	test: 1.329161
MAE train: 0.577609	val: 0.792826	test: 0.879755

Epoch: 130
Loss: 1.0893108248710632
RMSE train: 0.754199	val: 1.099424	test: 1.308608
MAE train: 0.556342	val: 0.775729	test: 0.854780

Epoch: 131
Loss: 0.768264502286911
RMSE train: 0.752096	val: 1.083448	test: 1.280661
MAE train: 0.557949	val: 0.786794	test: 0.858132

Epoch: 132
Loss: 0.8978393375873566
RMSE train: 0.780725	val: 1.124937	test: 1.299128
MAE train: 0.586892	val: 0.827913	test: 0.893714

Epoch: 133
Loss: 1.3156697154045105
RMSE train: 0.774500	val: 1.155282	test: 1.315991
MAE train: 0.572716	val: 0.837490	test: 0.878272

Epoch: 134
Loss: 1.1819894909858704
RMSE train: 0.822466	val: 1.204399	test: 1.354481
MAE train: 0.606796	val: 0.863880	test: 0.893607

Epoch: 135
Loss: 0.9315457940101624
RMSE train: 0.813001	val: 1.201349	test: 1.371786
MAE train: 0.610806	val: 0.862371	test: 0.914798

Epoch: 136
Loss: 0.9209524989128113
RMSE train: 0.829292	val: 1.188846	test: 1.386397
MAE train: 0.629336	val: 0.864550	test: 0.942120

Epoch: 137
Loss: 1.102451503276825
RMSE train: 0.881301	val: 1.186684	test: 1.396972
MAE train: 0.673423	val: 0.886251	test: 0.962844

Epoch: 138
Loss: 0.9549920558929443
RMSE train: 0.938641	val: 1.182004	test: 1.391993
MAE train: 0.715956	val: 0.891823	test: 0.971050

Epoch: 139
Loss: 0.9517830312252045
RMSE train: 0.962124	val: 1.177367	test: 1.392936
MAE train: 0.726390	val: 0.890040	test: 0.968596

Epoch: 140
Loss: 1.0184240341186523
RMSE train: 0.971809	val: 1.198110	test: 1.403192
MAE train: 0.739448	val: 0.905928	test: 0.976978

Epoch: 141
Loss: 1.210878849029541
RMSE train: 0.988084	val: 1.275769	test: 1.419121
MAE train: 0.750529	val: 0.937683	test: 0.975296

Epoch: 142
Loss: 0.9225698411464691
RMSE train: 0.969671	val: 1.309751	test: 1.411910
MAE train: 0.724362	val: 0.938838	test: 0.949903

Epoch: 143
Loss: 0.806082546710968
RMSE train: 0.915918	val: 1.289890	test: 1.386422
MAE train: 0.665994	val: 0.908659	test: 0.912688

Epoch: 144
Loss: 0.9596405625343323
RMSE train: 0.844615	val: 1.246761	test: 1.357056

Epoch: 84
Loss: 1.3612995743751526
RMSE train: 0.894723	val: 1.292543	test: 1.345301
MAE train: 0.660723	val: 0.879141	test: 0.911101

Epoch: 85
Loss: 1.5694493055343628
RMSE train: 0.901888	val: 1.293572	test: 1.360166
MAE train: 0.680557	val: 0.902900	test: 0.945580

Epoch: 86
Loss: 1.4870757460594177
RMSE train: 0.929233	val: 1.302618	test: 1.374399
MAE train: 0.702578	val: 0.916397	test: 0.964497

Epoch: 87
Loss: 1.540000081062317
RMSE train: 0.974852	val: 1.315499	test: 1.380489
MAE train: 0.729827	val: 0.927777	test: 0.953472

Epoch: 88
Loss: 1.369933307170868
RMSE train: 1.026415	val: 1.338696	test: 1.388022
MAE train: 0.754223	val: 0.940761	test: 0.955287

Epoch: 89
Loss: 1.2452178597450256
RMSE train: 1.089238	val: 1.365662	test: 1.403846
MAE train: 0.786097	val: 0.946903	test: 0.972722

Epoch: 90
Loss: 1.461507797241211
RMSE train: 1.084283	val: 1.335596	test: 1.405094
MAE train: 0.768574	val: 0.925111	test: 0.959894

Epoch: 91
Loss: 1.1410174369812012
RMSE train: 0.997204	val: 1.259177	test: 1.365256
MAE train: 0.705575	val: 0.877050	test: 0.917874

Epoch: 92
Loss: 1.1773388385772705
RMSE train: 0.933621	val: 1.241688	test: 1.356738
MAE train: 0.665008	val: 0.858717	test: 0.903752

Epoch: 93
Loss: 1.2557798624038696
RMSE train: 0.893770	val: 1.243595	test: 1.359750
MAE train: 0.641094	val: 0.863019	test: 0.900632

Epoch: 94
Loss: 1.251958429813385
RMSE train: 0.902444	val: 1.265225	test: 1.373407
MAE train: 0.654339	val: 0.890592	test: 0.911845

Epoch: 95
Loss: 1.226823627948761
RMSE train: 0.948500	val: 1.298674	test: 1.402190
MAE train: 0.685519	val: 0.920387	test: 0.934968

Epoch: 96
Loss: 1.129587709903717
RMSE train: 0.971268	val: 1.348630	test: 1.424758
MAE train: 0.696623	val: 0.943064	test: 0.939267

Epoch: 97
Loss: 1.1665410101413727
RMSE train: 0.977239	val: 1.404294	test: 1.448494
MAE train: 0.700563	val: 0.967295	test: 0.948900

Epoch: 98
Loss: 1.2527568936347961
RMSE train: 0.973294	val: 1.421109	test: 1.448097
MAE train: 0.698489	val: 0.969212	test: 0.944377

Epoch: 99
Loss: 1.2222155034542084
RMSE train: 0.944913	val: 1.404272	test: 1.426243
MAE train: 0.672557	val: 0.953208	test: 0.915748

Epoch: 100
Loss: 0.9566337764263153
RMSE train: 0.895780	val: 1.359056	test: 1.408802
MAE train: 0.645457	val: 0.929650	test: 0.906377

Epoch: 101
Loss: 1.1438316106796265
RMSE train: 0.878705	val: 1.340296	test: 1.417467
MAE train: 0.639417	val: 0.916943	test: 0.912533

Epoch: 102
Loss: 0.9919052124023438
RMSE train: 0.914726	val: 1.345263	test: 1.457877
MAE train: 0.662517	val: 0.919245	test: 0.933774

Epoch: 103
Loss: 1.3406563997268677
RMSE train: 1.000842	val: 1.370859	test: 1.521282
MAE train: 0.736677	val: 0.952912	test: 0.996809

Epoch: 104
Loss: 1.0396104454994202
RMSE train: 1.055357	val: 1.381050	test: 1.544880
MAE train: 0.788210	val: 0.974388	test: 1.045709

Epoch: 105
Loss: 1.0470741391181946
RMSE train: 1.030451	val: 1.370332	test: 1.515461
MAE train: 0.765802	val: 0.961480	test: 1.025450

Epoch: 106
Loss: 1.0757596492767334
RMSE train: 0.981714	val: 1.342491	test: 1.458515
MAE train: 0.719413	val: 0.931196	test: 0.963511

Epoch: 107
Loss: 1.2105368375778198
RMSE train: 0.946890	val: 1.326271	test: 1.413790
MAE train: 0.684191	val: 0.912616	test: 0.923038

Epoch: 108
Loss: 1.0208863019943237
RMSE train: 0.973781	val: 1.361164	test: 1.397320
MAE train: 0.704289	val: 0.932280	test: 0.929901

Epoch: 109
Loss: 1.171499252319336
RMSE train: 0.963598	val: 1.358968	test: 1.363989
MAE train: 0.692430	val: 0.927658	test: 0.911279

Epoch: 110
Loss: 1.1509323120117188
RMSE train: 0.937569	val: 1.350120	test: 1.342393
MAE train: 0.670507	val: 0.908923	test: 0.878003

Epoch: 111
Loss: 0.8858726918697357
RMSE train: 0.893727	val: 1.315237	test: 1.321149
MAE train: 0.649372	val: 0.882398	test: 0.859363

Epoch: 112
Loss: 1.1322214603424072
RMSE train: 0.859432	val: 1.293081	test: 1.312182
MAE train: 0.632640	val: 0.878439	test: 0.852586

Epoch: 113
Loss: 0.9751411378383636
RMSE train: 0.816495	val: 1.270955	test: 1.312999
MAE train: 0.605740	val: 0.871116	test: 0.846022

Epoch: 114
Loss: 0.9226651787757874
RMSE train: 0.784653	val: 1.249183	test: 1.326092
MAE train: 0.584939	val: 0.855339	test: 0.839315

Epoch: 115
Loss: 1.0198589861392975
RMSE train: 0.784898	val: 1.235978	test: 1.332496
MAE train: 0.589300	val: 0.845588	test: 0.848109

Epoch: 116
Loss: 1.0291276574134827
RMSE train: 0.811687	val: 1.217180	test: 1.333762
MAE train: 0.611776	val: 0.844546	test: 0.867282

Epoch: 117
Loss: 1.1774267852306366
RMSE train: 0.866973	val: 1.214835	test: 1.340572
MAE train: 0.647219	val: 0.861708	test: 0.886105

Epoch: 118
Loss: 1.036165565252304
RMSE train: 0.893838	val: 1.212266	test: 1.350080
MAE train: 0.662630	val: 0.865665	test: 0.893835

Epoch: 119
Loss: 1.0731847882270813
RMSE train: 0.975135	val: 1.237715	test: 1.385112
MAE train: 0.717426	val: 0.886786	test: 0.917630

Epoch: 120
Loss: 1.0685555934906006
RMSE train: 1.011787	val: 1.240933	test: 1.405684
MAE train: 0.743105	val: 0.888647	test: 0.926043

Epoch: 121
Loss: 1.094580054283142
RMSE train: 0.974961	val: 1.217336	test: 1.397663
MAE train: 0.721372	val: 0.873154	test: 0.928266

Epoch: 122
Loss: 1.112704873085022
RMSE train: 0.973942	val: 1.229460	test: 1.408730
MAE train: 0.713969	val: 0.879201	test: 0.938795

Epoch: 123
Loss: 1.0964497923851013
RMSE train: 0.993643	val: 1.259407	test: 1.433241
MAE train: 0.725653	val: 0.893153	test: 0.949578

Epoch: 124
Loss: 0.9436798393726349
RMSE train: 1.043520	val: 1.312918	test: 1.485832
MAE train: 0.764300	val: 0.924523	test: 0.977146

Epoch: 125
Loss: 1.0844833850860596
RMSE train: 1.082954	val: 1.384797	test: 1.533673
MAE train: 0.788062	val: 0.966799	test: 0.999352

Epoch: 126
Loss: 0.9988882541656494
RMSE train: 1.027256	val: 1.353901	test: 1.507313
MAE train: 0.735680	val: 0.942196	test: 0.969811

Epoch: 127
Loss: 1.0224361419677734
RMSE train: 0.916523	val: 1.295831	test: 1.438350
MAE train: 0.656706	val: 0.899747	test: 0.940514

Epoch: 128
Loss: 1.056787759065628
RMSE train: 0.882437	val: 1.281480	test: 1.406278
MAE train: 0.624691	val: 0.871744	test: 0.920527

Epoch: 129
Loss: 1.0576332211494446
RMSE train: 0.878353	val: 1.258006	test: 1.390277
MAE train: 0.629991	val: 0.866048	test: 0.926744

Epoch: 130
Loss: 0.9821021854877472
RMSE train: 0.947405	val: 1.287814	test: 1.421962
MAE train: 0.696374	val: 0.905749	test: 0.981984

Epoch: 131
Loss: 1.2713908553123474
RMSE train: 1.018046	val: 1.311729	test: 1.467159
MAE train: 0.759478	val: 0.942529	test: 1.016387

Epoch: 132
Loss: 0.8823239207267761
RMSE train: 1.093995	val: 1.341932	test: 1.513249
MAE train: 0.802021	val: 0.964705	test: 1.040605

Epoch: 133
Loss: 0.8778676688671112
RMSE train: 1.051258	val: 1.298904	test: 1.492610
MAE train: 0.768812	val: 0.938858	test: 1.014655

Epoch: 134
Loss: 0.9101901650428772
RMSE train: 0.975541	val: 1.255986	test: 1.457572
MAE train: 0.720841	val: 0.907763	test: 0.978293

Epoch: 135
Loss: 0.9479705989360809
RMSE train: 0.875881	val: 1.228677	test: 1.422107
MAE train: 0.652516	val: 0.871837	test: 0.931150

Epoch: 136
Loss: 1.0070651471614838
RMSE train: 0.809342	val: 1.215020	test: 1.414176
MAE train: 0.598974	val: 0.839953	test: 0.911309

Epoch: 137
Loss: 1.0644117593765259
RMSE train: 0.788118	val: 1.210958	test: 1.420562
MAE train: 0.579587	val: 0.825508	test: 0.908772

Epoch: 138
Loss: 0.9462448358535767
RMSE train: 0.752321	val: 1.193542	test: 1.411266
MAE train: 0.549826	val: 0.813092	test: 0.894067

Epoch: 139
Loss: 0.8151268064975739
RMSE train: 0.757439	val: 1.195477	test: 1.417981
MAE train: 0.558753	val: 0.823338	test: 0.901843

Epoch: 140
Loss: 0.8517192304134369
RMSE train: 0.818983	val: 1.215400	test: 1.449960
MAE train: 0.606405	val: 0.853224	test: 0.933530

Epoch: 141
Loss: 0.9479198455810547
RMSE train: 0.907084	val: 1.223527	test: 1.493505
MAE train: 0.661257	val: 0.869990	test: 0.968405

Epoch: 142
Loss: 0.8684729337692261
RMSE train: 0.957392	val: 1.239844	test: 1.515293
MAE train: 0.691640	val: 0.878782	test: 0.988321

Epoch: 143
Loss: 0.9103753864765167
RMSE train: 0.911647	val: 1.209765	test: 1.492166
MAE train: 0.664745	val: 0.855414	test: 0.974562

Epoch: 144
Loss: 0.8553781509399414
RMSE train: 0.844568	val: 1.183738	test: 1.454929

Epoch: 84
Loss: 1.5874398946762085
RMSE train: 0.789168	val: 1.314936	test: 1.296688
MAE train: 0.593076	val: 0.907773	test: 0.872162

Epoch: 85
Loss: 1.2948148250579834
RMSE train: 0.813453	val: 1.325630	test: 1.326360
MAE train: 0.622875	val: 0.907237	test: 0.910368

Epoch: 86
Loss: 1.8755107522010803
RMSE train: 0.865356	val: 1.338046	test: 1.358983
MAE train: 0.670474	val: 0.933669	test: 0.949609

Epoch: 87
Loss: 1.3702289462089539
RMSE train: 0.838340	val: 1.336619	test: 1.330060
MAE train: 0.638699	val: 0.916212	test: 0.925083

Epoch: 88
Loss: 1.2063229084014893
RMSE train: 0.828738	val: 1.329028	test: 1.287615
MAE train: 0.596802	val: 0.904919	test: 0.875102

Epoch: 89
Loss: 1.3076425194740295
RMSE train: 0.802222	val: 1.327242	test: 1.274732
MAE train: 0.584290	val: 0.892271	test: 0.849993

Epoch: 90
Loss: 1.081802487373352
RMSE train: 0.781300	val: 1.356765	test: 1.298098
MAE train: 0.585743	val: 0.906337	test: 0.855494

Epoch: 91
Loss: 1.2553721070289612
RMSE train: 0.770053	val: 1.349898	test: 1.326887
MAE train: 0.584233	val: 0.898704	test: 0.880505

Epoch: 92
Loss: 1.218487560749054
RMSE train: 0.800206	val: 1.299593	test: 1.361685
MAE train: 0.606938	val: 0.878616	test: 0.916876

Epoch: 93
Loss: 1.2249404191970825
RMSE train: 0.812864	val: 1.252258	test: 1.395432
MAE train: 0.616327	val: 0.863320	test: 0.941857

Epoch: 94
Loss: 1.095400631427765
RMSE train: 0.816252	val: 1.221528	test: 1.406982
MAE train: 0.617185	val: 0.862792	test: 0.945032

Epoch: 95
Loss: 1.2641281485557556
RMSE train: 0.800385	val: 1.230907	test: 1.385124
MAE train: 0.604595	val: 0.875673	test: 0.920375

Epoch: 96
Loss: 1.3176284432411194
RMSE train: 0.796725	val: 1.263014	test: 1.357280
MAE train: 0.604842	val: 0.898158	test: 0.901772

Epoch: 97
Loss: 1.0335505902767181
RMSE train: 0.827465	val: 1.310451	test: 1.349786
MAE train: 0.632923	val: 0.935953	test: 0.893606

Epoch: 98
Loss: 1.23745858669281
RMSE train: 0.865552	val: 1.347365	test: 1.372442
MAE train: 0.656178	val: 0.960219	test: 0.908378

Epoch: 99
Loss: 1.162130743265152
RMSE train: 0.898196	val: 1.377587	test: 1.402217
MAE train: 0.669117	val: 0.980537	test: 0.931056

Epoch: 100
Loss: 1.3081565499305725
RMSE train: 0.871659	val: 1.396243	test: 1.405256
MAE train: 0.637295	val: 0.990593	test: 0.904888

Epoch: 101
Loss: 1.4217724204063416
RMSE train: 0.869829	val: 1.402324	test: 1.414699
MAE train: 0.624655	val: 0.989929	test: 0.894525

Epoch: 102
Loss: 1.3174758553504944
RMSE train: 0.842994	val: 1.387087	test: 1.394830
MAE train: 0.610319	val: 0.974660	test: 0.882393

Epoch: 103
Loss: 1.1326625645160675
RMSE train: 0.814642	val: 1.369353	test: 1.376891
MAE train: 0.599114	val: 0.949777	test: 0.872324

Epoch: 104
Loss: 1.0669925212860107
RMSE train: 0.776844	val: 1.331367	test: 1.349764
MAE train: 0.583820	val: 0.909990	test: 0.861467

Epoch: 105
Loss: 1.0083552300930023
RMSE train: 0.774295	val: 1.298701	test: 1.338164
MAE train: 0.587803	val: 0.879438	test: 0.869599

Epoch: 106
Loss: 1.1798877716064453
RMSE train: 0.789013	val: 1.287530	test: 1.352225
MAE train: 0.601891	val: 0.875695	test: 0.892335

Epoch: 107
Loss: 1.3178101778030396
RMSE train: 0.755673	val: 1.280279	test: 1.352866
MAE train: 0.578881	val: 0.869928	test: 0.885822

Epoch: 108
Loss: 1.2293258905410767
RMSE train: 0.701247	val: 1.288328	test: 1.348220
MAE train: 0.536726	val: 0.867864	test: 0.870140

Epoch: 109
Loss: 1.1294918060302734
RMSE train: 0.665516	val: 1.306663	test: 1.348449
MAE train: 0.507865	val: 0.868585	test: 0.859753

Epoch: 110
Loss: 1.1037623286247253
RMSE train: 0.660462	val: 1.292005	test: 1.356187
MAE train: 0.498948	val: 0.860322	test: 0.863799

Epoch: 111
Loss: 0.952379047870636
RMSE train: 0.681498	val: 1.257854	test: 1.358541
MAE train: 0.516133	val: 0.853836	test: 0.875375

Epoch: 112
Loss: 1.1589428782463074
RMSE train: 0.708341	val: 1.261202	test: 1.365549
MAE train: 0.543248	val: 0.861352	test: 0.891195

Epoch: 113
Loss: 0.97684246301651
RMSE train: 0.749444	val: 1.260760	test: 1.367224
MAE train: 0.581516	val: 0.878120	test: 0.895779

Epoch: 114
Loss: 1.2428359985351562
RMSE train: 0.745314	val: 1.249420	test: 1.313803
MAE train: 0.577499	val: 0.878260	test: 0.859673

Epoch: 115
Loss: 0.8928969502449036
RMSE train: 0.733738	val: 1.259249	test: 1.287099
MAE train: 0.560579	val: 0.878284	test: 0.826851

Epoch: 116
Loss: 1.004646360874176
RMSE train: 0.719133	val: 1.321310	test: 1.307214
MAE train: 0.544637	val: 0.901158	test: 0.833205

Epoch: 117
Loss: 1.0734271109104156
RMSE train: 0.741986	val: 1.349482	test: 1.330807
MAE train: 0.556352	val: 0.919857	test: 0.857516

Epoch: 118
Loss: 0.8343966007232666
RMSE train: 0.714622	val: 1.332468	test: 1.336105
MAE train: 0.529644	val: 0.895703	test: 0.857169

Epoch: 119
Loss: 0.7616209089756012
RMSE train: 0.725134	val: 1.292680	test: 1.346414
MAE train: 0.529456	val: 0.876297	test: 0.861657

Epoch: 120
Loss: 0.9411789774894714
RMSE train: 0.718698	val: 1.257594	test: 1.339460
MAE train: 0.523389	val: 0.859692	test: 0.854772

Epoch: 121
Loss: 1.0055671632289886
RMSE train: 0.701305	val: 1.201448	test: 1.333072
MAE train: 0.515046	val: 0.821856	test: 0.854509

Epoch: 122
Loss: 0.9205417037010193
RMSE train: 0.701723	val: 1.191639	test: 1.351768
MAE train: 0.512696	val: 0.805329	test: 0.868357

Epoch: 123
Loss: 0.9038616120815277
RMSE train: 0.702951	val: 1.215532	test: 1.368180
MAE train: 0.518450	val: 0.826998	test: 0.888538

Epoch: 124
Loss: 0.988880455493927
RMSE train: 0.683285	val: 1.220099	test: 1.337628
MAE train: 0.507246	val: 0.841792	test: 0.886200

Epoch: 125
Loss: 1.2083300650119781
RMSE train: 0.673302	val: 1.217557	test: 1.343968
MAE train: 0.506511	val: 0.842229	test: 0.892487

Epoch: 126
Loss: 0.8547371923923492
RMSE train: 0.688666	val: 1.231505	test: 1.398419
MAE train: 0.529997	val: 0.845986	test: 0.911659

Epoch: 127
Loss: 1.3813341557979584
RMSE train: 0.712676	val: 1.266904	test: 1.454102
MAE train: 0.553006	val: 0.864082	test: 0.942136

Epoch: 128
Loss: 0.923614114522934
RMSE train: 0.676951	val: 1.277846	test: 1.464566
MAE train: 0.526556	val: 0.854847	test: 0.945684

Epoch: 129
Loss: 0.9481195211410522
RMSE train: 0.668768	val: 1.288474	test: 1.432317
MAE train: 0.508059	val: 0.854444	test: 0.929658

Epoch: 130
Loss: 0.8817370533943176
RMSE train: 0.682444	val: 1.270368	test: 1.384215
MAE train: 0.511746	val: 0.859226	test: 0.921802

Epoch: 131
Loss: 0.8891618549823761
RMSE train: 0.700986	val: 1.250111	test: 1.357150
MAE train: 0.529948	val: 0.865463	test: 0.917856

Epoch: 132
Loss: 1.0548759400844574
RMSE train: 0.699575	val: 1.230087	test: 1.358626
MAE train: 0.538227	val: 0.861212	test: 0.910529

Epoch: 133
Loss: 0.9645707011222839
RMSE train: 0.729166	val: 1.228596	test: 1.374669
MAE train: 0.559042	val: 0.862247	test: 0.918395

Epoch: 134
Loss: 0.8505953848361969
RMSE train: 0.733596	val: 1.225521	test: 1.361396
MAE train: 0.553669	val: 0.858910	test: 0.900917

Epoch: 135
Loss: 1.0236566662788391
RMSE train: 0.685825	val: 1.192230	test: 1.308154
MAE train: 0.514948	val: 0.833876	test: 0.841236

Epoch: 136
Loss: 0.8560006022453308
RMSE train: 0.665211	val: 1.178808	test: 1.291088
MAE train: 0.493402	val: 0.822769	test: 0.807549

Epoch: 137
Loss: 1.041211575269699
RMSE train: 0.654327	val: 1.203518	test: 1.310174
MAE train: 0.483668	val: 0.822490	test: 0.809897

Epoch: 138
Loss: 0.9663679897785187
RMSE train: 0.681963	val: 1.241731	test: 1.356429
MAE train: 0.505211	val: 0.839700	test: 0.848966

Epoch: 139
Loss: 1.1399687230587006
RMSE train: 0.731024	val: 1.301276	test: 1.432730
MAE train: 0.547652	val: 0.872687	test: 0.915584

Epoch: 140
Loss: 0.7376160323619843
RMSE train: 0.771465	val: 1.369310	test: 1.496201
MAE train: 0.584221	val: 0.900817	test: 0.963851

Epoch: 141
Loss: 1.0249672532081604
RMSE train: 0.773570	val: 1.389997	test: 1.510164
MAE train: 0.580874	val: 0.910586	test: 0.969231

Epoch: 142
Loss: 0.7530768811702728
RMSE train: 0.742992	val: 1.378866	test: 1.483231
MAE train: 0.555339	val: 0.900725	test: 0.945790

Epoch: 143
Loss: 0.9140240848064423
RMSE train: 0.728075	val: 1.368996	test: 1.447674
MAE train: 0.548215	val: 0.904961	test: 0.927268

Epoch: 144
Loss: 0.9479906558990479
RMSE train: 0.734728	val: 1.366001	test: 1.414526
MAE train: 0.607037	val: 0.867443	test: 0.888112

Epoch: 145
Loss: 1.1331100463867188
RMSE train: 0.786827	val: 1.198729	test: 1.337536
MAE train: 0.566649	val: 0.827217	test: 0.877078

Epoch: 146
Loss: 1.2468193769454956
RMSE train: 0.720048	val: 1.144058	test: 1.316994
MAE train: 0.522388	val: 0.781876	test: 0.859657

Epoch: 147
Loss: 0.7160930931568146
RMSE train: 0.741067	val: 1.138717	test: 1.325974
MAE train: 0.547789	val: 0.796525	test: 0.874048

Epoch: 148
Loss: 1.083812952041626
RMSE train: 0.799372	val: 1.154277	test: 1.344533
MAE train: 0.597241	val: 0.830402	test: 0.903537

Epoch: 149
Loss: 0.9290956258773804
RMSE train: 0.831113	val: 1.177042	test: 1.364017
MAE train: 0.624198	val: 0.841681	test: 0.911888

Epoch: 150
Loss: 0.8079812824726105
RMSE train: 0.882933	val: 1.217983	test: 1.396056
MAE train: 0.661285	val: 0.863257	test: 0.931203

Epoch: 151
Loss: 0.9283595979213715
RMSE train: 0.883735	val: 1.228674	test: 1.405853
MAE train: 0.660321	val: 0.861425	test: 0.944448

Epoch: 152
Loss: 1.0065453052520752
RMSE train: 0.803793	val: 1.183922	test: 1.369714
MAE train: 0.599834	val: 0.825662	test: 0.920378

Epoch: 153
Loss: 0.8907080590724945
RMSE train: 0.738642	val: 1.139084	test: 1.329020
MAE train: 0.547425	val: 0.786534	test: 0.886503

Epoch: 154
Loss: 0.8785094618797302
RMSE train: 0.716086	val: 1.115439	test: 1.296437
MAE train: 0.526653	val: 0.780766	test: 0.865233

Epoch: 155
Loss: 0.7094560265541077
RMSE train: 0.717432	val: 1.118758	test: 1.290483
MAE train: 0.528095	val: 0.787730	test: 0.858495

Epoch: 156
Loss: 1.0247940123081207
RMSE train: 0.754037	val: 1.129357	test: 1.302198
MAE train: 0.557596	val: 0.800506	test: 0.872991

Epoch: 157
Loss: 1.0347745418548584
RMSE train: 0.832990	val: 1.176636	test: 1.342434
MAE train: 0.618445	val: 0.836791	test: 0.905249

Epoch: 158
Loss: 0.8519754707813263
RMSE train: 0.893270	val: 1.225160	test: 1.386508
MAE train: 0.661717	val: 0.864868	test: 0.932911

Epoch: 159
Loss: 0.9140475392341614
RMSE train: 0.893120	val: 1.210020	test: 1.392767
MAE train: 0.663468	val: 0.852042	test: 0.939583

Epoch: 160
Loss: 0.9973081946372986
RMSE train: 0.841851	val: 1.167066	test: 1.365863
MAE train: 0.623727	val: 0.817255	test: 0.919169

Epoch: 161
Loss: 1.0148707926273346
RMSE train: 0.773231	val: 1.110376	test: 1.316445
MAE train: 0.577564	val: 0.792969	test: 0.886877

Epoch: 162
Loss: 0.8604738414287567
RMSE train: 0.722908	val: 1.097066	test: 1.288460
MAE train: 0.542641	val: 0.795789	test: 0.864694

Epoch: 163
Loss: 0.9303634762763977
RMSE train: 0.732499	val: 1.114294	test: 1.286072
MAE train: 0.545483	val: 0.813533	test: 0.862383

Epoch: 164
Loss: 1.3359098732471466
RMSE train: 0.732103	val: 1.127175	test: 1.277920
MAE train: 0.538802	val: 0.813799	test: 0.849897

Epoch: 165
Loss: 0.8297071754932404
RMSE train: 0.805028	val: 1.182584	test: 1.305156
MAE train: 0.587811	val: 0.832254	test: 0.869697

Epoch: 166
Loss: 0.8592406213283539
RMSE train: 0.856540	val: 1.227498	test: 1.328784
MAE train: 0.632109	val: 0.859688	test: 0.895327

Early stopping
Best (RMSE):	 train: 0.752096	val: 1.083448	test: 1.280661
Best (MAE):	 train: 0.557949	val: 0.786794	test: 0.858132


Epoch: 84
Loss: 1.2340999245643616
RMSE train: 0.850853	val: 1.391070	test: 1.105239
MAE train: 0.622476	val: 0.890858	test: 0.837513

Epoch: 85
Loss: 1.2584694623947144
RMSE train: 0.811831	val: 1.387072	test: 1.106892
MAE train: 0.599070	val: 0.886509	test: 0.829919

Epoch: 86
Loss: 1.2424037456512451
RMSE train: 0.815577	val: 1.411889	test: 1.121456
MAE train: 0.603549	val: 0.911418	test: 0.824752

Epoch: 87
Loss: 1.1919981837272644
RMSE train: 0.850072	val: 1.454946	test: 1.153135
MAE train: 0.638293	val: 0.953178	test: 0.837354

Epoch: 88
Loss: 1.112617313861847
RMSE train: 0.825007	val: 1.415987	test: 1.138339
MAE train: 0.620633	val: 0.931867	test: 0.826331

Epoch: 89
Loss: 1.338326096534729
RMSE train: 0.807428	val: 1.349298	test: 1.137142
MAE train: 0.610969	val: 0.884319	test: 0.813177

Epoch: 90
Loss: 1.078026831150055
RMSE train: 0.770855	val: 1.298227	test: 1.089989
MAE train: 0.588904	val: 0.840062	test: 0.796408

Epoch: 91
Loss: 1.0578771233558655
RMSE train: 0.746275	val: 1.262142	test: 1.074964
MAE train: 0.563826	val: 0.812156	test: 0.779022

Epoch: 92
Loss: 1.251413345336914
RMSE train: 0.718681	val: 1.234950	test: 1.067402
MAE train: 0.535530	val: 0.787639	test: 0.753541

Epoch: 93
Loss: 1.1078515648841858
RMSE train: 0.706502	val: 1.225002	test: 1.044852
MAE train: 0.520280	val: 0.777166	test: 0.732755

Epoch: 94
Loss: 0.9232875108718872
RMSE train: 0.742516	val: 1.296807	test: 1.070644
MAE train: 0.550672	val: 0.839671	test: 0.751538

Epoch: 95
Loss: 1.0385969281196594
RMSE train: 0.795003	val: 1.383208	test: 1.100609
MAE train: 0.597306	val: 0.910306	test: 0.785996

Epoch: 96
Loss: 1.042491614818573
RMSE train: 0.830148	val: 1.426619	test: 1.113560
MAE train: 0.630527	val: 0.933644	test: 0.777516

Epoch: 97
Loss: 1.076943039894104
RMSE train: 0.818270	val: 1.385596	test: 1.084557
MAE train: 0.621023	val: 0.895128	test: 0.754613

Epoch: 98
Loss: 1.2305797934532166
RMSE train: 0.861648	val: 1.400934	test: 1.116917
MAE train: 0.651313	val: 0.902258	test: 0.785358

Epoch: 99
Loss: 0.9821093380451202
RMSE train: 0.887195	val: 1.416787	test: 1.143014
MAE train: 0.664705	val: 0.913608	test: 0.815131

Epoch: 100
Loss: 1.097194492816925
RMSE train: 0.904140	val: 1.482495	test: 1.173515
MAE train: 0.673049	val: 0.959207	test: 0.865685

Epoch: 101
Loss: 0.9924289882183075
RMSE train: 0.924356	val: 1.532704	test: 1.216513
MAE train: 0.687019	val: 1.001852	test: 0.902416

Epoch: 102
Loss: 1.1155320703983307
RMSE train: 0.876186	val: 1.494295	test: 1.191482
MAE train: 0.648557	val: 0.981326	test: 0.883003

Epoch: 103
Loss: 0.8659267127513885
RMSE train: 0.827146	val: 1.402393	test: 1.169281
MAE train: 0.611194	val: 0.921097	test: 0.856164

Epoch: 104
Loss: 1.0380228161811829
RMSE train: 0.808429	val: 1.348167	test: 1.188483
MAE train: 0.596718	val: 0.892757	test: 0.858521

Epoch: 105
Loss: 1.0438861846923828
RMSE train: 0.816488	val: 1.368143	test: 1.224871
MAE train: 0.597224	val: 0.890104	test: 0.866054

Epoch: 106
Loss: 1.0972986817359924
RMSE train: 0.857687	val: 1.405577	test: 1.258833
MAE train: 0.637242	val: 0.917129	test: 0.906581

Epoch: 107
Loss: 0.9403221905231476
RMSE train: 0.864049	val: 1.404246	test: 1.235770
MAE train: 0.645334	val: 0.903000	test: 0.901759

Epoch: 108
Loss: 1.0846566259860992
RMSE train: 0.820454	val: 1.332802	test: 1.213817
MAE train: 0.612062	val: 0.837296	test: 0.840963

Epoch: 109
Loss: 1.1398621201515198
RMSE train: 0.850440	val: 1.322396	test: 1.260963
MAE train: 0.634445	val: 0.827970	test: 0.836357

Epoch: 110
Loss: 0.8510648906230927
RMSE train: 0.846729	val: 1.339551	test: 1.269524
MAE train: 0.626852	val: 0.836574	test: 0.835785

Epoch: 111
Loss: 0.9521868228912354
RMSE train: 0.809768	val: 1.389416	test: 1.223188
MAE train: 0.601158	val: 0.872868	test: 0.837064

Epoch: 112
Loss: 0.9411215782165527
RMSE train: 0.783970	val: 1.424700	test: 1.175390
MAE train: 0.580383	val: 0.903170	test: 0.837505

Epoch: 113
Loss: 1.0145799815654755
RMSE train: 0.787817	val: 1.403957	test: 1.158516
MAE train: 0.596959	val: 0.895415	test: 0.836242

Epoch: 114
Loss: 0.877299040555954
RMSE train: 0.802475	val: 1.355420	test: 1.164455
MAE train: 0.618104	val: 0.872673	test: 0.840217

Epoch: 115
Loss: 1.142932653427124
RMSE train: 0.838621	val: 1.345117	test: 1.148483
MAE train: 0.639683	val: 0.861319	test: 0.842809

Epoch: 116
Loss: 0.9072762429714203
RMSE train: 0.855961	val: 1.362796	test: 1.109262
MAE train: 0.644195	val: 0.861272	test: 0.826987

Epoch: 117
Loss: 0.8886910080909729
RMSE train: 0.855615	val: 1.422998	test: 1.111373
MAE train: 0.648133	val: 0.903483	test: 0.827084

Epoch: 118
Loss: 1.0833984017372131
RMSE train: 0.836084	val: 1.427121	test: 1.118510
MAE train: 0.636390	val: 0.909537	test: 0.817509

Epoch: 119
Loss: 0.8659816980361938
RMSE train: 0.804292	val: 1.412040	test: 1.112664
MAE train: 0.613355	val: 0.902347	test: 0.799689

Epoch: 120
Loss: 0.8846658170223236
RMSE train: 0.756081	val: 1.357841	test: 1.099983
MAE train: 0.570953	val: 0.866597	test: 0.784491

Epoch: 121
Loss: 0.9641507565975189
RMSE train: 0.701547	val: 1.285802	test: 1.064060
MAE train: 0.523297	val: 0.814765	test: 0.752343

Epoch: 122
Loss: 0.8405523300170898
RMSE train: 0.699804	val: 1.277316	test: 1.050373
MAE train: 0.514076	val: 0.814639	test: 0.747462

Epoch: 123
Loss: 0.8802597522735596
RMSE train: 0.734529	val: 1.348082	test: 1.076305
MAE train: 0.549624	val: 0.877429	test: 0.779292

Epoch: 124
Loss: 0.9501010179519653
RMSE train: 0.748125	val: 1.404435	test: 1.117004
MAE train: 0.565804	val: 0.920534	test: 0.808920

Epoch: 125
Loss: 0.8935241997241974
RMSE train: 0.724656	val: 1.383098	test: 1.139440
MAE train: 0.546807	val: 0.898949	test: 0.796075

Epoch: 126
Loss: 0.8136198818683624
RMSE train: 0.701890	val: 1.348705	test: 1.155966
MAE train: 0.527941	val: 0.863377	test: 0.777670

Epoch: 127
Loss: 0.8274498581886292
RMSE train: 0.682298	val: 1.317788	test: 1.146360
MAE train: 0.516476	val: 0.836258	test: 0.768296

Epoch: 128
Loss: 0.9027302265167236
RMSE train: 0.691899	val: 1.321811	test: 1.114482
MAE train: 0.532023	val: 0.838339	test: 0.784903

Early stopping
Best (RMSE):	 train: 0.706502	val: 1.225002	test: 1.044852
Best (MAE):	 train: 0.520280	val: 0.777166	test: 0.732755


Epoch: 84
Loss: 1.154778778553009
RMSE train: 0.811303	val: 1.429918	test: 1.179217
MAE train: 0.610667	val: 0.941190	test: 0.803702

Epoch: 85
Loss: 1.2620820999145508
RMSE train: 0.813457	val: 1.442430	test: 1.200769
MAE train: 0.602095	val: 0.943843	test: 0.812391

Epoch: 86
Loss: 1.0412564277648926
RMSE train: 0.824671	val: 1.448504	test: 1.238805
MAE train: 0.607307	val: 0.950574	test: 0.834589

Epoch: 87
Loss: 1.1635635495185852
RMSE train: 0.821599	val: 1.428018	test: 1.251072
MAE train: 0.612551	val: 0.953875	test: 0.840381

Epoch: 88
Loss: 0.9971458911895752
RMSE train: 0.797419	val: 1.411028	test: 1.241179
MAE train: 0.591880	val: 0.943200	test: 0.817415

Epoch: 89
Loss: 1.1574371457099915
RMSE train: 0.762226	val: 1.383137	test: 1.237354
MAE train: 0.560702	val: 0.914694	test: 0.800754

Epoch: 90
Loss: 1.106644630432129
RMSE train: 0.751053	val: 1.350094	test: 1.238487
MAE train: 0.555884	val: 0.888854	test: 0.801690

Epoch: 91
Loss: 1.4044743776321411
RMSE train: 0.794970	val: 1.332425	test: 1.250895
MAE train: 0.596370	val: 0.887725	test: 0.828324

Epoch: 92
Loss: 1.1656622886657715
RMSE train: 0.846060	val: 1.362492	test: 1.287241
MAE train: 0.634343	val: 0.906157	test: 0.855982

Epoch: 93
Loss: 1.1533434987068176
RMSE train: 0.870668	val: 1.435085	test: 1.266360
MAE train: 0.654595	val: 0.949903	test: 0.861289

Epoch: 94
Loss: 0.9855533242225647
RMSE train: 0.893725	val: 1.487620	test: 1.260399
MAE train: 0.668337	val: 0.992010	test: 0.868719

Epoch: 95
Loss: 1.2183889150619507
RMSE train: 0.866464	val: 1.488171	test: 1.254864
MAE train: 0.645116	val: 0.983856	test: 0.854638

Epoch: 96
Loss: 1.0271363258361816
RMSE train: 0.817462	val: 1.456735	test: 1.242545
MAE train: 0.603237	val: 0.950708	test: 0.839026

Epoch: 97
Loss: 1.0320327281951904
RMSE train: 0.803149	val: 1.446081	test: 1.255196
MAE train: 0.592516	val: 0.945510	test: 0.851081

Epoch: 98
Loss: 1.0370869636535645
RMSE train: 0.812246	val: 1.466648	test: 1.271344
MAE train: 0.600319	val: 0.954794	test: 0.869486

Epoch: 99
Loss: 1.069083034992218
RMSE train: 0.823029	val: 1.469236	test: 1.269285
MAE train: 0.613119	val: 0.965456	test: 0.872322

Epoch: 100
Loss: 1.116782784461975
RMSE train: 0.810504	val: 1.448519	test: 1.245237
MAE train: 0.602961	val: 0.956083	test: 0.849924

Epoch: 101
Loss: 1.1290528178215027
RMSE train: 0.751499	val: 1.421791	test: 1.203748
MAE train: 0.563045	val: 0.925010	test: 0.793082

Epoch: 102
Loss: 0.9336706697940826
RMSE train: 0.698974	val: 1.367064	test: 1.188986
MAE train: 0.530760	val: 0.891088	test: 0.772367

Epoch: 103
Loss: 0.8992187976837158
RMSE train: 0.703509	val: 1.331268	test: 1.174769
MAE train: 0.536770	val: 0.886687	test: 0.763257

Epoch: 104
Loss: 0.9258238971233368
RMSE train: 0.692579	val: 1.309060	test: 1.151181
MAE train: 0.533639	val: 0.884300	test: 0.752987

Epoch: 105
Loss: 1.0331341624259949
RMSE train: 0.687504	val: 1.310904	test: 1.126205
MAE train: 0.529750	val: 0.889228	test: 0.744138

Epoch: 106
Loss: 0.9534108936786652
RMSE train: 0.711419	val: 1.339293	test: 1.142113
MAE train: 0.537372	val: 0.906350	test: 0.764238

Epoch: 107
Loss: 1.0390452146530151
RMSE train: 0.723745	val: 1.356171	test: 1.179622
MAE train: 0.543502	val: 0.912045	test: 0.787305

Epoch: 108
Loss: 0.8593396842479706
RMSE train: 0.713873	val: 1.334468	test: 1.195485
MAE train: 0.539000	val: 0.890462	test: 0.785127

Epoch: 109
Loss: 0.9579907655715942
RMSE train: 0.702021	val: 1.304243	test: 1.202568
MAE train: 0.532546	val: 0.863874	test: 0.764207

Epoch: 110
Loss: 0.9927901327610016
RMSE train: 0.713345	val: 1.293582	test: 1.207546
MAE train: 0.539254	val: 0.862498	test: 0.771801

Epoch: 111
Loss: 1.027857095003128
RMSE train: 0.748175	val: 1.302980	test: 1.216428
MAE train: 0.564859	val: 0.873341	test: 0.808127

Epoch: 112
Loss: 1.0410988330841064
RMSE train: 0.773496	val: 1.305275	test: 1.194241
MAE train: 0.590310	val: 0.872234	test: 0.820320

Epoch: 113
Loss: 0.9512571692466736
RMSE train: 0.822680	val: 1.321333	test: 1.205158
MAE train: 0.637353	val: 0.889407	test: 0.869462

Epoch: 114
Loss: 0.8076272904872894
RMSE train: 0.862077	val: 1.336533	test: 1.236689
MAE train: 0.668117	val: 0.906350	test: 0.896285

Epoch: 115
Loss: 0.9165688157081604
RMSE train: 0.831785	val: 1.330322	test: 1.240803
MAE train: 0.639862	val: 0.892790	test: 0.889524

Epoch: 116
Loss: 0.8849331140518188
RMSE train: 0.780773	val: 1.355138	test: 1.237531
MAE train: 0.603744	val: 0.902473	test: 0.863180

Epoch: 117
Loss: 0.8515377640724182
RMSE train: 0.740984	val: 1.387290	test: 1.232954
MAE train: 0.574514	val: 0.919348	test: 0.846474

Epoch: 118
Loss: 1.0167067646980286
RMSE train: 0.672709	val: 1.353071	test: 1.212389
MAE train: 0.499736	val: 0.869511	test: 0.805093

Epoch: 119
Loss: 0.953526109457016
RMSE train: 0.733171	val: 1.334496	test: 1.250538
MAE train: 0.536553	val: 0.876625	test: 0.811230

Epoch: 120
Loss: 1.0193790793418884
RMSE train: 0.784054	val: 1.325118	test: 1.262106
MAE train: 0.561164	val: 0.880540	test: 0.819725

Epoch: 121
Loss: 1.000606745481491
RMSE train: 0.767762	val: 1.345471	test: 1.237366
MAE train: 0.552999	val: 0.884470	test: 0.802904

Epoch: 122
Loss: 1.1667228937149048
RMSE train: 0.791570	val: 1.385838	test: 1.260891
MAE train: 0.587310	val: 0.898613	test: 0.852282

Epoch: 123
Loss: 1.046581655740738
RMSE train: 0.803249	val: 1.353734	test: 1.244961
MAE train: 0.600890	val: 0.895391	test: 0.851986

Epoch: 124
Loss: 1.0609262585639954
RMSE train: 0.824687	val: 1.324837	test: 1.234862
MAE train: 0.625627	val: 0.887646	test: 0.861736

Epoch: 125
Loss: 1.1394110321998596
RMSE train: 0.891560	val: 1.338847	test: 1.243431
MAE train: 0.676459	val: 0.912707	test: 0.910662

Epoch: 126
Loss: 1.0030258893966675
RMSE train: 0.911892	val: 1.379560	test: 1.233486
MAE train: 0.684998	val: 0.935647	test: 0.914775

Epoch: 127
Loss: 0.9925054311752319
RMSE train: 0.828549	val: 1.390936	test: 1.193383
MAE train: 0.611787	val: 0.916388	test: 0.855831

Epoch: 128
Loss: 0.7985182404518127
RMSE train: 0.768785	val: 1.410587	test: 1.194597
MAE train: 0.563876	val: 0.911463	test: 0.833658

Epoch: 129
Loss: 0.839088499546051
RMSE train: 0.754049	val: 1.378294	test: 1.187178
MAE train: 0.554278	val: 0.892714	test: 0.824749

Epoch: 130
Loss: 0.8322736024856567
RMSE train: 0.721653	val: 1.307081	test: 1.160025
MAE train: 0.534666	val: 0.835073	test: 0.794169

Epoch: 131
Loss: 1.006511926651001
RMSE train: 0.721861	val: 1.267809	test: 1.175474
MAE train: 0.539946	val: 0.812324	test: 0.801593

Epoch: 132
Loss: 0.9052449464797974
RMSE train: 0.766176	val: 1.265812	test: 1.217656
MAE train: 0.573529	val: 0.821089	test: 0.836910

Epoch: 133
Loss: 0.8999023139476776
RMSE train: 0.803642	val: 1.299483	test: 1.250515
MAE train: 0.600794	val: 0.849017	test: 0.865438

Epoch: 134
Loss: 0.9460865259170532
RMSE train: 0.764806	val: 1.302173	test: 1.226789
MAE train: 0.577851	val: 0.848600	test: 0.854558

Epoch: 135
Loss: 0.8028102517127991
RMSE train: 0.718831	val: 1.304077	test: 1.197967
MAE train: 0.543248	val: 0.849895	test: 0.825719

Epoch: 136
Loss: 0.7867509424686432
RMSE train: 0.701435	val: 1.324135	test: 1.207605
MAE train: 0.525262	val: 0.866308	test: 0.810833

Epoch: 137
Loss: 0.7182295918464661
RMSE train: 0.655248	val: 1.291588	test: 1.188424
MAE train: 0.479747	val: 0.831091	test: 0.771754

Epoch: 138
Loss: 0.8496943116188049
RMSE train: 0.642129	val: 1.250053	test: 1.168019
MAE train: 0.473557	val: 0.803604	test: 0.745187

Epoch: 139
Loss: 0.8358871042728424
RMSE train: 0.669810	val: 1.253518	test: 1.159288
MAE train: 0.497989	val: 0.813269	test: 0.762371

Epoch: 140
Loss: 0.9555393159389496
RMSE train: 0.674347	val: 1.256956	test: 1.178866
MAE train: 0.503914	val: 0.815489	test: 0.778127

Epoch: 141
Loss: 0.7733824849128723
RMSE train: 0.665374	val: 1.249168	test: 1.201740
MAE train: 0.494258	val: 0.802360	test: 0.794639

Epoch: 142
Loss: 0.8225216567516327
RMSE train: 0.685375	val: 1.253994	test: 1.228582
MAE train: 0.511030	val: 0.799559	test: 0.820591

Epoch: 143
Loss: 0.690829873085022
RMSE train: 0.697794	val: 1.242308	test: 1.229941
MAE train: 0.524656	val: 0.799550	test: 0.826644

Epoch: 144
Loss: 0.8464373052120209
RMSE train: 0.710507	val: 1.237519	test: 1.223990
MAE train: 0.550189	val: 0.930041	test: 0.907577

Epoch: 145
Loss: 0.8734310567378998
RMSE train: 0.748038	val: 1.336573	test: 1.384262
MAE train: 0.559941	val: 0.929762	test: 0.883427

Epoch: 146
Loss: 0.8306021094322205
RMSE train: 0.759230	val: 1.320104	test: 1.372630
MAE train: 0.572817	val: 0.920660	test: 0.870294

Epoch: 147
Loss: 0.6815842688083649
RMSE train: 0.763517	val: 1.312875	test: 1.384484
MAE train: 0.575368	val: 0.914994	test: 0.884348

Epoch: 148
Loss: 0.7982243001461029
RMSE train: 0.720545	val: 1.268684	test: 1.368623
MAE train: 0.538643	val: 0.878295	test: 0.867305

Epoch: 149
Loss: 0.7434430420398712
RMSE train: 0.654620	val: 1.226217	test: 1.343507
MAE train: 0.480627	val: 0.849698	test: 0.841370

Epoch: 150
Loss: 0.831312894821167
RMSE train: 0.669077	val: 1.222734	test: 1.351238
MAE train: 0.497836	val: 0.849557	test: 0.870203

Epoch: 151
Loss: 0.996185302734375
RMSE train: 0.675689	val: 1.235378	test: 1.375870
MAE train: 0.502007	val: 0.848524	test: 0.882489

Epoch: 152
Loss: 0.7173219323158264
RMSE train: 0.686145	val: 1.274287	test: 1.410611
MAE train: 0.517399	val: 0.851875	test: 0.900661

Epoch: 153
Loss: 0.9078041911125183
RMSE train: 0.675499	val: 1.308260	test: 1.426795
MAE train: 0.518474	val: 0.863798	test: 0.916008

Epoch: 154
Loss: 0.7982203662395477
RMSE train: 0.680310	val: 1.306003	test: 1.409749
MAE train: 0.522226	val: 0.877676	test: 0.905388

Epoch: 155
Loss: 0.6832218170166016
RMSE train: 0.670593	val: 1.289341	test: 1.393224
MAE train: 0.508654	val: 0.870975	test: 0.890259

Epoch: 156
Loss: 0.9688006937503815
RMSE train: 0.648542	val: 1.259491	test: 1.364325
MAE train: 0.490792	val: 0.855466	test: 0.869869

Epoch: 157
Loss: 0.7763131260871887
RMSE train: 0.690231	val: 1.277088	test: 1.374226
MAE train: 0.510920	val: 0.868912	test: 0.883400

Epoch: 158
Loss: 0.875038355588913
RMSE train: 0.724736	val: 1.290830	test: 1.379042
MAE train: 0.529511	val: 0.874043	test: 0.886070

Epoch: 159
Loss: 0.9076718091964722
RMSE train: 0.778729	val: 1.305500	test: 1.394618
MAE train: 0.564363	val: 0.893491	test: 0.900585

Epoch: 160
Loss: 0.6380318403244019
RMSE train: 0.822368	val: 1.314118	test: 1.412906
MAE train: 0.601754	val: 0.910583	test: 0.907066

Epoch: 161
Loss: 0.8080857396125793
RMSE train: 0.820817	val: 1.285998	test: 1.413235
MAE train: 0.607309	val: 0.898873	test: 0.912778

Epoch: 162
Loss: 0.7272143065929413
RMSE train: 0.765188	val: 1.211488	test: 1.386511
MAE train: 0.568358	val: 0.843389	test: 0.895706

Epoch: 163
Loss: 0.8148903846740723
RMSE train: 0.747744	val: 1.183977	test: 1.383194
MAE train: 0.557494	val: 0.817440	test: 0.890250

Epoch: 164
Loss: 1.0435273051261902
RMSE train: 0.759787	val: 1.210262	test: 1.411435
MAE train: 0.571025	val: 0.827713	test: 0.906535

Epoch: 165
Loss: 0.8364753127098083
RMSE train: 0.748324	val: 1.239026	test: 1.432893
MAE train: 0.560655	val: 0.837117	test: 0.928726

Epoch: 166
Loss: 0.7296642363071442
RMSE train: 0.713434	val: 1.240078	test: 1.426859
MAE train: 0.523326	val: 0.831790	test: 0.924437

Epoch: 167
Loss: 0.6441826224327087
RMSE train: 0.680819	val: 1.242927	test: 1.419440
MAE train: 0.491582	val: 0.829555	test: 0.908554

Epoch: 168
Loss: 0.8971817493438721
RMSE train: 0.671113	val: 1.254459	test: 1.406206
MAE train: 0.489225	val: 0.853410	test: 0.900638

Epoch: 169
Loss: 1.5588866174221039
RMSE train: 0.654197	val: 1.261083	test: 1.416546
MAE train: 0.500131	val: 0.849256	test: 0.890636

Epoch: 170
Loss: 0.8213737308979034
RMSE train: 0.705290	val: 1.275478	test: 1.428070
MAE train: 0.540363	val: 0.881154	test: 0.908598

Epoch: 171
Loss: 0.7460471391677856
RMSE train: 0.703544	val: 1.287373	test: 1.430242
MAE train: 0.548168	val: 0.885481	test: 0.924000

Early stopping
Best (RMSE):	 train: 0.665211	val: 1.178808	test: 1.291088
Best (MAE):	 train: 0.493402	val: 0.822769	test: 0.807549


Epoch: 84
Loss: 1.035977691411972
RMSE train: 0.980096	val: 1.597270	test: 1.230603
MAE train: 0.724244	val: 1.029934	test: 0.902208

Epoch: 85
Loss: 1.1009750962257385
RMSE train: 0.914362	val: 1.482322	test: 1.182960
MAE train: 0.680024	val: 0.946953	test: 0.860869

Epoch: 86
Loss: 1.0358318090438843
RMSE train: 0.882512	val: 1.441654	test: 1.169450
MAE train: 0.656973	val: 0.921468	test: 0.849022

Epoch: 87
Loss: 1.1233077645301819
RMSE train: 0.884261	val: 1.443437	test: 1.191818
MAE train: 0.655339	val: 0.914718	test: 0.867773

Epoch: 88
Loss: 1.3206650614738464
RMSE train: 0.879467	val: 1.451551	test: 1.187719
MAE train: 0.644414	val: 0.910418	test: 0.848983

Epoch: 89
Loss: 0.986873060464859
RMSE train: 0.880512	val: 1.468798	test: 1.154687
MAE train: 0.641359	val: 0.920508	test: 0.820264

Epoch: 90
Loss: 1.0455456376075745
RMSE train: 0.885823	val: 1.474023	test: 1.137904
MAE train: 0.648759	val: 0.930708	test: 0.800811

Epoch: 91
Loss: 1.1086767315864563
RMSE train: 0.899832	val: 1.473963	test: 1.131389
MAE train: 0.660116	val: 0.935433	test: 0.802156

Epoch: 92
Loss: 1.1448027193546295
RMSE train: 0.861958	val: 1.441781	test: 1.108817
MAE train: 0.631951	val: 0.915431	test: 0.789798

Epoch: 93
Loss: 1.0324207544326782
RMSE train: 0.806012	val: 1.390589	test: 1.089745
MAE train: 0.588680	val: 0.885803	test: 0.769183

Epoch: 94
Loss: 1.0140169858932495
RMSE train: 0.794127	val: 1.387413	test: 1.108346
MAE train: 0.582621	val: 0.888645	test: 0.781179

Epoch: 95
Loss: 0.9918763637542725
RMSE train: 0.798188	val: 1.405366	test: 1.140871
MAE train: 0.584215	val: 0.905833	test: 0.800937

Epoch: 96
Loss: 1.2277361154556274
RMSE train: 0.827822	val: 1.440570	test: 1.179753
MAE train: 0.609250	val: 0.932761	test: 0.823182

Epoch: 97
Loss: 1.1176488399505615
RMSE train: 0.864263	val: 1.478665	test: 1.215074
MAE train: 0.644240	val: 0.961256	test: 0.842356

Epoch: 98
Loss: 0.9715236723423004
RMSE train: 0.899629	val: 1.490995	test: 1.223269
MAE train: 0.672000	val: 0.972019	test: 0.860722

Epoch: 99
Loss: 1.0954639911651611
RMSE train: 0.943920	val: 1.498181	test: 1.213939
MAE train: 0.709222	val: 0.980780	test: 0.865659

Epoch: 100
Loss: 1.0051454603672028
RMSE train: 0.966398	val: 1.522343	test: 1.212909
MAE train: 0.725728	val: 1.002187	test: 0.878883

Epoch: 101
Loss: 1.1579535007476807
RMSE train: 0.996986	val: 1.558308	test: 1.230956
MAE train: 0.744609	val: 1.037198	test: 0.913303

Epoch: 102
Loss: 0.9307374954223633
RMSE train: 0.975013	val: 1.557163	test: 1.224552
MAE train: 0.734943	val: 1.036676	test: 0.905649

Epoch: 103
Loss: 1.028353750705719
RMSE train: 0.956614	val: 1.572473	test: 1.215545
MAE train: 0.719077	val: 1.035955	test: 0.882374

Epoch: 104
Loss: 0.9400974810123444
RMSE train: 0.932676	val: 1.573593	test: 1.198054
MAE train: 0.690274	val: 1.013884	test: 0.838743

Epoch: 105
Loss: 1.1020058393478394
RMSE train: 0.930577	val: 1.564888	test: 1.195122
MAE train: 0.688238	val: 1.008827	test: 0.836790

Epoch: 106
Loss: 0.9434685409069061
RMSE train: 0.912707	val: 1.495531	test: 1.184889
MAE train: 0.681465	val: 0.978272	test: 0.822592

Epoch: 107
Loss: 1.2184280157089233
RMSE train: 0.891538	val: 1.439303	test: 1.196408
MAE train: 0.660800	val: 0.956819	test: 0.818603

Epoch: 108
Loss: 1.0042710304260254
RMSE train: 0.855052	val: 1.420335	test: 1.175644
MAE train: 0.633499	val: 0.937746	test: 0.815887

Epoch: 109
Loss: 0.9154238402843475
RMSE train: 0.828936	val: 1.424898	test: 1.168466
MAE train: 0.619862	val: 0.933271	test: 0.828621

Epoch: 110
Loss: 1.1073763966560364
RMSE train: 0.811934	val: 1.412502	test: 1.164743
MAE train: 0.614676	val: 0.919719	test: 0.837036

Epoch: 111
Loss: 1.0127889811992645
RMSE train: 0.814405	val: 1.441868	test: 1.182133
MAE train: 0.618189	val: 0.943058	test: 0.848817

Epoch: 112
Loss: 0.947073221206665
RMSE train: 0.839760	val: 1.453268	test: 1.179634
MAE train: 0.637290	val: 0.949253	test: 0.853399

Epoch: 113
Loss: 0.9269299507141113
RMSE train: 0.874579	val: 1.471302	test: 1.166781
MAE train: 0.661187	val: 0.963684	test: 0.852907

Epoch: 114
Loss: 0.9095960855484009
RMSE train: 0.944288	val: 1.507115	test: 1.184333
MAE train: 0.708477	val: 0.988348	test: 0.862448

Epoch: 115
Loss: 0.9673012793064117
RMSE train: 0.960880	val: 1.520251	test: 1.181817
MAE train: 0.712466	val: 0.990012	test: 0.857439

Epoch: 116
Loss: 0.8920457661151886
RMSE train: 0.920521	val: 1.486551	test: 1.162189
MAE train: 0.678339	val: 0.961536	test: 0.830410

Epoch: 117
Loss: 0.9294878840446472
RMSE train: 0.887449	val: 1.475960	test: 1.154740
MAE train: 0.651716	val: 0.949875	test: 0.809601

Epoch: 118
Loss: 0.8745645582675934
RMSE train: 0.855121	val: 1.487097	test: 1.139604
MAE train: 0.628034	val: 0.953186	test: 0.796315

Epoch: 119
Loss: 0.9850258529186249
RMSE train: 0.840949	val: 1.487191	test: 1.125920
MAE train: 0.612531	val: 0.947321	test: 0.782236

Epoch: 120
Loss: 1.0003207921981812
RMSE train: 0.836021	val: 1.469791	test: 1.128555
MAE train: 0.607437	val: 0.932882	test: 0.766223

Epoch: 121
Loss: 0.9621658325195312
RMSE train: 0.857346	val: 1.484872	test: 1.155875
MAE train: 0.625448	val: 0.942555	test: 0.773562

Epoch: 122
Loss: 0.9144594073295593
RMSE train: 0.858339	val: 1.480265	test: 1.190832
MAE train: 0.628845	val: 0.938989	test: 0.800331

Epoch: 123
Loss: 1.006140112876892
RMSE train: 0.868357	val: 1.490288	test: 1.223569
MAE train: 0.638739	val: 0.948231	test: 0.820541

Epoch: 124
Loss: 0.928231418132782
RMSE train: 0.835782	val: 1.463551	test: 1.214887
MAE train: 0.610219	val: 0.928522	test: 0.812731

Epoch: 125
Loss: 0.7892393469810486
RMSE train: 0.802476	val: 1.436937	test: 1.190617
MAE train: 0.581093	val: 0.910832	test: 0.799874

Epoch: 126
Loss: 0.7752236127853394
RMSE train: 0.756339	val: 1.393918	test: 1.162641
MAE train: 0.539846	val: 0.881624	test: 0.784567

Epoch: 127
Loss: 0.7674781680107117
RMSE train: 0.708225	val: 1.365479	test: 1.141185
MAE train: 0.503979	val: 0.868817	test: 0.778162

Epoch: 128
Loss: 0.831125020980835
RMSE train: 0.719983	val: 1.383933	test: 1.119514
MAE train: 0.514301	val: 0.878193	test: 0.768238

Epoch: 129
Loss: 1.0038257241249084
RMSE train: 0.775975	val: 1.410666	test: 1.106062
MAE train: 0.559457	val: 0.894798	test: 0.772617

Epoch: 130
Loss: 0.7739423513412476
RMSE train: 0.783286	val: 1.387057	test: 1.121139
MAE train: 0.563496	val: 0.871061	test: 0.769306

Epoch: 131
Loss: 0.8515610992908478
RMSE train: 0.777997	val: 1.383291	test: 1.132577
MAE train: 0.561903	val: 0.869107	test: 0.775307

Epoch: 132
Loss: 1.012769103050232
RMSE train: 0.749155	val: 1.363449	test: 1.161118
MAE train: 0.544778	val: 0.861251	test: 0.779048

Epoch: 133
Loss: 0.9174132645130157
RMSE train: 0.750490	val: 1.387664	test: 1.208705
MAE train: 0.547890	val: 0.882913	test: 0.795345

Epoch: 134
Loss: 0.8730598390102386
RMSE train: 0.736435	val: 1.351247	test: 1.203864
MAE train: 0.536513	val: 0.856667	test: 0.774516

Epoch: 135
Loss: 0.7513867020606995
RMSE train: 0.723343	val: 1.327534	test: 1.199906
MAE train: 0.525200	val: 0.842339	test: 0.771301

Epoch: 136
Loss: 0.6506419777870178
RMSE train: 0.706899	val: 1.316359	test: 1.183990
MAE train: 0.508256	val: 0.838148	test: 0.771770

Epoch: 137
Loss: 0.7899142503738403
RMSE train: 0.686337	val: 1.304468	test: 1.169620
MAE train: 0.492976	val: 0.827154	test: 0.770895

Epoch: 138
Loss: 0.8576233685016632
RMSE train: 0.713040	val: 1.333934	test: 1.169680
MAE train: 0.515047	val: 0.834696	test: 0.792936

Epoch: 139
Loss: 0.8417999744415283
RMSE train: 0.731060	val: 1.310301	test: 1.142660
MAE train: 0.536661	val: 0.819266	test: 0.776707

Epoch: 140
Loss: 0.7597510814666748
RMSE train: 0.745557	val: 1.291155	test: 1.129672
MAE train: 0.558240	val: 0.817463	test: 0.774992

Epoch: 141
Loss: 0.7515172064304352
RMSE train: 0.755539	val: 1.296390	test: 1.123450
MAE train: 0.567557	val: 0.825049	test: 0.772995

Epoch: 142
Loss: 0.758167177438736
RMSE train: 0.766051	val: 1.328394	test: 1.132196
MAE train: 0.574666	val: 0.846129	test: 0.776713

Epoch: 143
Loss: 0.8996835947036743
RMSE train: 0.796464	val: 1.365585	test: 1.160485
MAE train: 0.603216	val: 0.873188	test: 0.801084

Epoch: 144
Loss: 0.8313803672790527
RMSE train: 0.817980	val: 1.368149	test: 1.174801

Epoch: 84
Loss: 1.1313125491142273
RMSE train: 0.810694	val: 1.357541	test: 1.153002
MAE train: 0.597120	val: 0.875085	test: 0.775095

Epoch: 85
Loss: 1.3329001069068909
RMSE train: 0.785413	val: 1.286396	test: 1.141079
MAE train: 0.587388	val: 0.836723	test: 0.768972

Epoch: 86
Loss: 1.1926730871200562
RMSE train: 0.788677	val: 1.258740	test: 1.142648
MAE train: 0.590044	val: 0.817404	test: 0.762270

Epoch: 87
Loss: 1.132764756679535
RMSE train: 0.800252	val: 1.319005	test: 1.105925
MAE train: 0.601707	val: 0.866817	test: 0.759238

Epoch: 88
Loss: 1.1168665289878845
RMSE train: 0.792449	val: 1.344576	test: 1.062524
MAE train: 0.578609	val: 0.866795	test: 0.715887

Epoch: 89
Loss: 0.9581742882728577
RMSE train: 0.831724	val: 1.419423	test: 1.087015
MAE train: 0.602774	val: 0.904905	test: 0.733395

Epoch: 90
Loss: 1.1178924441337585
RMSE train: 0.842907	val: 1.437128	test: 1.102956
MAE train: 0.605440	val: 0.900371	test: 0.729123

Epoch: 91
Loss: 1.1454130411148071
RMSE train: 0.831119	val: 1.431504	test: 1.077277
MAE train: 0.593028	val: 0.886793	test: 0.708358

Epoch: 92
Loss: 1.0181781649589539
RMSE train: 0.833534	val: 1.425133	test: 1.101179
MAE train: 0.592553	val: 0.857045	test: 0.712537

Epoch: 93
Loss: 0.9957163631916046
RMSE train: 0.818369	val: 1.399079	test: 1.126330
MAE train: 0.586557	val: 0.845462	test: 0.719341

Epoch: 94
Loss: 1.2504713535308838
RMSE train: 0.828889	val: 1.352203	test: 1.172589
MAE train: 0.601404	val: 0.833064	test: 0.750263

Epoch: 95
Loss: 1.0293269753456116
RMSE train: 0.817882	val: 1.355408	test: 1.167176
MAE train: 0.592395	val: 0.819469	test: 0.732054

Epoch: 96
Loss: 1.1044614911079407
RMSE train: 0.776549	val: 1.350299	test: 1.125971
MAE train: 0.566314	val: 0.800748	test: 0.722881

Epoch: 97
Loss: 1.0930256843566895
RMSE train: 0.766925	val: 1.342961	test: 1.067776
MAE train: 0.562506	val: 0.816534	test: 0.723998

Epoch: 98
Loss: 1.086194634437561
RMSE train: 0.782943	val: 1.343859	test: 1.060325
MAE train: 0.578552	val: 0.811021	test: 0.728647

Epoch: 99
Loss: 1.075743019580841
RMSE train: 0.794898	val: 1.325294	test: 1.081376
MAE train: 0.589886	val: 0.783806	test: 0.728606

Epoch: 100
Loss: 0.8879783749580383
RMSE train: 0.824926	val: 1.348661	test: 1.141795
MAE train: 0.605393	val: 0.806117	test: 0.758080

Epoch: 101
Loss: 1.1042965054512024
RMSE train: 0.884973	val: 1.395930	test: 1.243774
MAE train: 0.635174	val: 0.848617	test: 0.789271

Epoch: 102
Loss: 1.1046565175056458
RMSE train: 0.893590	val: 1.430429	test: 1.255319
MAE train: 0.637034	val: 0.860943	test: 0.772956

Epoch: 103
Loss: 1.1767913103103638
RMSE train: 0.850535	val: 1.424556	test: 1.232121
MAE train: 0.602163	val: 0.845287	test: 0.758485

Epoch: 104
Loss: 1.2279680371284485
RMSE train: 0.813049	val: 1.381947	test: 1.228651
MAE train: 0.588624	val: 0.830053	test: 0.779027

Epoch: 105
Loss: 1.2327613830566406
RMSE train: 0.741834	val: 1.256686	test: 1.203014
MAE train: 0.539895	val: 0.762319	test: 0.742155

Epoch: 106
Loss: 0.9756927490234375
RMSE train: 0.732445	val: 1.240152	test: 1.150026
MAE train: 0.529245	val: 0.769964	test: 0.710342

Epoch: 107
Loss: 1.0985054969787598
RMSE train: 0.736822	val: 1.307608	test: 1.098075
MAE train: 0.548665	val: 0.803649	test: 0.693641

Epoch: 108
Loss: 1.4975030422210693
RMSE train: 0.754199	val: 1.369577	test: 1.065033
MAE train: 0.571465	val: 0.842711	test: 0.694788

Epoch: 109
Loss: 1.1454573273658752
RMSE train: 0.776428	val: 1.341934	test: 1.111737
MAE train: 0.581473	val: 0.805916	test: 0.698644

Epoch: 110
Loss: 1.0201429724693298
RMSE train: 0.810468	val: 1.284692	test: 1.209059
MAE train: 0.590319	val: 0.762267	test: 0.721636

Epoch: 111
Loss: 1.005767822265625
RMSE train: 0.816208	val: 1.268986	test: 1.213267
MAE train: 0.593543	val: 0.761878	test: 0.730188

Epoch: 112
Loss: 0.9640650451183319
RMSE train: 0.766806	val: 1.265080	test: 1.124741
MAE train: 0.555021	val: 0.754472	test: 0.701280

Epoch: 113
Loss: 0.955514669418335
RMSE train: 0.776828	val: 1.342009	test: 1.077656
MAE train: 0.573108	val: 0.828891	test: 0.708412

Epoch: 114
Loss: 0.9812285304069519
RMSE train: 0.811170	val: 1.435288	test: 1.101474
MAE train: 0.602973	val: 0.903618	test: 0.729824

Epoch: 115
Loss: 1.107656478881836
RMSE train: 0.804538	val: 1.417200	test: 1.171629
MAE train: 0.599009	val: 0.871010	test: 0.750021

Epoch: 116
Loss: 0.7376857995986938
RMSE train: 0.799130	val: 1.365786	test: 1.211508
MAE train: 0.590409	val: 0.827728	test: 0.785354

Epoch: 117
Loss: 0.9347904324531555
RMSE train: 0.794735	val: 1.331447	test: 1.222717
MAE train: 0.588065	val: 0.796286	test: 0.787655

Epoch: 118
Loss: 1.0320534408092499
RMSE train: 0.798916	val: 1.323322	test: 1.234421
MAE train: 0.597644	val: 0.804928	test: 0.777069

Epoch: 119
Loss: 0.8986104428768158
RMSE train: 0.778488	val: 1.336297	test: 1.190394
MAE train: 0.583220	val: 0.830001	test: 0.766395

Epoch: 120
Loss: 0.8708528876304626
RMSE train: 0.755458	val: 1.356378	test: 1.138839
MAE train: 0.566931	val: 0.834203	test: 0.729637

Epoch: 121
Loss: 0.7828935086727142
RMSE train: 0.752790	val: 1.392514	test: 1.084242
MAE train: 0.562842	val: 0.844454	test: 0.689815

Epoch: 122
Loss: 0.9153119325637817
RMSE train: 0.768956	val: 1.405324	test: 1.087223
MAE train: 0.569711	val: 0.856098	test: 0.696645

Epoch: 123
Loss: 0.9385239481925964
RMSE train: 0.749999	val: 1.392579	test: 1.085821
MAE train: 0.556072	val: 0.849945	test: 0.708508

Epoch: 124
Loss: 0.8978140354156494
RMSE train: 0.703338	val: 1.345890	test: 1.093759
MAE train: 0.521519	val: 0.816898	test: 0.711826

Epoch: 125
Loss: 0.9541770219802856
RMSE train: 0.698359	val: 1.348564	test: 1.110496
MAE train: 0.520801	val: 0.809059	test: 0.738051

Epoch: 126
Loss: 0.8462975919246674
RMSE train: 0.694026	val: 1.320188	test: 1.117768
MAE train: 0.511693	val: 0.782893	test: 0.731421

Epoch: 127
Loss: 1.0090596675872803
RMSE train: 0.674181	val: 1.317060	test: 1.066441
MAE train: 0.491911	val: 0.780232	test: 0.691968

Epoch: 128
Loss: 1.000081479549408
RMSE train: 0.679053	val: 1.308330	test: 1.079637
MAE train: 0.492316	val: 0.780795	test: 0.679217

Epoch: 129
Loss: 0.871753454208374
RMSE train: 0.682850	val: 1.326934	test: 1.072641
MAE train: 0.498951	val: 0.799442	test: 0.671596

Epoch: 130
Loss: 0.942714512348175
RMSE train: 0.689745	val: 1.334369	test: 1.077069
MAE train: 0.514409	val: 0.812997	test: 0.668721

Epoch: 131
Loss: 0.9054912328720093
RMSE train: 0.689938	val: 1.313823	test: 1.118077
MAE train: 0.514870	val: 0.803994	test: 0.683299

Epoch: 132
Loss: 0.8600496053695679
RMSE train: 0.720664	val: 1.299657	test: 1.211664
MAE train: 0.526127	val: 0.801286	test: 0.713682

Epoch: 133
Loss: 0.9007203876972198
RMSE train: 0.733716	val: 1.283292	test: 1.249601
MAE train: 0.533522	val: 0.788655	test: 0.727818

Epoch: 134
Loss: 0.9233112037181854
RMSE train: 0.709039	val: 1.273885	test: 1.216700
MAE train: 0.518017	val: 0.792369	test: 0.718809

Epoch: 135
Loss: 0.8068772554397583
RMSE train: 0.696539	val: 1.264356	test: 1.181755
MAE train: 0.515990	val: 0.799226	test: 0.725473

Epoch: 136
Loss: 0.8729133307933807
RMSE train: 0.670446	val: 1.259608	test: 1.123593
MAE train: 0.505330	val: 0.804618	test: 0.709297

Epoch: 137
Loss: 0.7894993424415588
RMSE train: 0.668959	val: 1.285222	test: 1.064454
MAE train: 0.508094	val: 0.824045	test: 0.696541

Epoch: 138
Loss: 0.8597721755504608
RMSE train: 0.655854	val: 1.291522	test: 1.015156
MAE train: 0.499316	val: 0.835761	test: 0.687989

Epoch: 139
Loss: 0.8586017191410065
RMSE train: 0.655898	val: 1.293968	test: 1.018454
MAE train: 0.500816	val: 0.833388	test: 0.683649

Epoch: 140
Loss: 0.8276307582855225
RMSE train: 0.653343	val: 1.282927	test: 1.003175
MAE train: 0.496483	val: 0.820187	test: 0.664511

Epoch: 141
Loss: 0.8289855718612671
RMSE train: 0.671634	val: 1.287764	test: 1.029258
MAE train: 0.508895	val: 0.807259	test: 0.658690

Early stopping
Best (RMSE):	 train: 0.732445	val: 1.240152	test: 1.150026
Best (MAE):	 train: 0.529245	val: 0.769964	test: 0.710342


Epoch: 84
Loss: 1.1692786812782288
RMSE train: 0.968129	val: 1.645551	test: 1.121556
MAE train: 0.717110	val: 1.043312	test: 0.845205

Epoch: 85
Loss: 1.2963314652442932
RMSE train: 0.966502	val: 1.653048	test: 1.095097
MAE train: 0.710483	val: 1.035535	test: 0.837721

Epoch: 86
Loss: 1.1401093006134033
RMSE train: 0.923537	val: 1.645954	test: 1.004311
MAE train: 0.674659	val: 1.010655	test: 0.767358

Epoch: 87
Loss: 1.3391066193580627
RMSE train: 0.897655	val: 1.601877	test: 0.936687
MAE train: 0.649918	val: 1.008337	test: 0.693753

Epoch: 88
Loss: 1.219322919845581
RMSE train: 0.857944	val: 1.515500	test: 0.947458
MAE train: 0.619473	val: 0.942756	test: 0.693153

Epoch: 89
Loss: 1.0266824960708618
RMSE train: 0.863495	val: 1.467271	test: 1.015502
MAE train: 0.630075	val: 0.915935	test: 0.746769

Epoch: 90
Loss: 1.088840126991272
RMSE train: 0.885228	val: 1.500445	test: 1.049129
MAE train: 0.650047	val: 0.954403	test: 0.783101

Epoch: 91
Loss: 1.192887306213379
RMSE train: 0.915397	val: 1.536951	test: 1.057952
MAE train: 0.677344	val: 0.984434	test: 0.793959

Epoch: 92
Loss: 1.1211494207382202
RMSE train: 0.956337	val: 1.577236	test: 1.059776
MAE train: 0.705426	val: 0.990651	test: 0.791623

Epoch: 93
Loss: 0.9629587531089783
RMSE train: 0.973583	val: 1.610682	test: 1.021830
MAE train: 0.717903	val: 1.006922	test: 0.781075

Epoch: 94
Loss: 1.2525563836097717
RMSE train: 0.960275	val: 1.631801	test: 1.007820
MAE train: 0.707565	val: 1.027892	test: 0.763531

Epoch: 95
Loss: 1.088333249092102
RMSE train: 0.957658	val: 1.636893	test: 1.014660
MAE train: 0.701650	val: 1.025585	test: 0.775416

Epoch: 96
Loss: 0.9976315498352051
RMSE train: 0.926368	val: 1.629244	test: 0.971277
MAE train: 0.678695	val: 0.997910	test: 0.737417

Epoch: 97
Loss: 1.0761706233024597
RMSE train: 0.910962	val: 1.592279	test: 0.956710
MAE train: 0.666765	val: 0.964047	test: 0.739373

Epoch: 98
Loss: 0.8712737560272217
RMSE train: 0.919786	val: 1.596811	test: 0.983713
MAE train: 0.666339	val: 0.965840	test: 0.736159

Epoch: 99
Loss: 1.0200642943382263
RMSE train: 0.896255	val: 1.613135	test: 0.976906
MAE train: 0.652265	val: 0.957756	test: 0.703217

Epoch: 100
Loss: 0.8853563666343689
RMSE train: 0.874212	val: 1.618355	test: 0.978650
MAE train: 0.647983	val: 0.949677	test: 0.715018

Epoch: 101
Loss: 0.9831801354885101
RMSE train: 0.862554	val: 1.622445	test: 1.006764
MAE train: 0.645452	val: 0.954078	test: 0.729252

Epoch: 102
Loss: 0.913398265838623
RMSE train: 0.825593	val: 1.590483	test: 1.019512
MAE train: 0.612735	val: 0.927606	test: 0.729240

Epoch: 103
Loss: 1.089758813381195
RMSE train: 0.806722	val: 1.544376	test: 1.073934
MAE train: 0.593523	val: 0.902920	test: 0.732361

Epoch: 104
Loss: 0.93656787276268
RMSE train: 0.807606	val: 1.550661	test: 1.080475
MAE train: 0.579287	val: 0.926194	test: 0.731391

Epoch: 105
Loss: 1.1484188437461853
RMSE train: 0.824253	val: 1.561233	test: 1.048811
MAE train: 0.587682	val: 0.947550	test: 0.724748

Epoch: 106
Loss: 1.1049189567565918
RMSE train: 0.803535	val: 1.561771	test: 0.958899
MAE train: 0.577369	val: 0.947873	test: 0.687846

Epoch: 107
Loss: 1.013487696647644
RMSE train: 0.805419	val: 1.585414	test: 0.903979
MAE train: 0.582315	val: 0.958043	test: 0.662956

Epoch: 108
Loss: 1.5010239481925964
RMSE train: 0.835138	val: 1.605595	test: 0.909198
MAE train: 0.605822	val: 0.947010	test: 0.661118

Epoch: 109
Loss: 0.9226807355880737
RMSE train: 0.847528	val: 1.626427	test: 0.927151
MAE train: 0.620195	val: 0.929054	test: 0.666537

Epoch: 110
Loss: 0.9275762736797333
RMSE train: 0.844642	val: 1.640775	test: 0.974416
MAE train: 0.616192	val: 0.928284	test: 0.672499

Epoch: 111
Loss: 0.9457858800888062
RMSE train: 0.819715	val: 1.597581	test: 1.019664
MAE train: 0.596630	val: 0.888018	test: 0.677360

Epoch: 112
Loss: 1.1887292861938477
RMSE train: 0.796350	val: 1.525419	test: 1.018958
MAE train: 0.581158	val: 0.856027	test: 0.700986

Epoch: 113
Loss: 0.9538397192955017
RMSE train: 0.755270	val: 1.462097	test: 1.008273
MAE train: 0.551732	val: 0.824215	test: 0.700392

Epoch: 114
Loss: 0.9310224056243896
RMSE train: 0.703446	val: 1.425220	test: 0.936609
MAE train: 0.513180	val: 0.835924	test: 0.679489

Epoch: 115
Loss: 0.9592307209968567
RMSE train: 0.710475	val: 1.440256	test: 0.909113
MAE train: 0.520152	val: 0.869051	test: 0.683139

Epoch: 116
Loss: 1.125819742679596
RMSE train: 0.737142	val: 1.494526	test: 0.945292
MAE train: 0.547421	val: 0.908135	test: 0.711481

Epoch: 117
Loss: 0.9644961357116699
RMSE train: 0.832195	val: 1.558870	test: 1.078244
MAE train: 0.620259	val: 0.941194	test: 0.778604

Epoch: 118
Loss: 1.0713676512241364
RMSE train: 0.905857	val: 1.610771	test: 1.160785
MAE train: 0.673874	val: 0.971220	test: 0.795940

Epoch: 119
Loss: 1.02237930893898
RMSE train: 0.882558	val: 1.647568	test: 1.112047
MAE train: 0.664078	val: 0.996372	test: 0.757947

Epoch: 120
Loss: 0.9477289617061615
RMSE train: 0.849053	val: 1.687782	test: 1.072097
MAE train: 0.638801	val: 1.023096	test: 0.743315

Epoch: 121
Loss: 1.0062264800071716
RMSE train: 0.796907	val: 1.653801	test: 1.047419
MAE train: 0.587272	val: 0.986595	test: 0.721429

Epoch: 122
Loss: 0.7980977296829224
RMSE train: 0.761595	val: 1.620753	test: 1.005514
MAE train: 0.554930	val: 0.954917	test: 0.699550

Epoch: 123
Loss: 0.9578591883182526
RMSE train: 0.748752	val: 1.578531	test: 1.024527
MAE train: 0.540526	val: 0.918245	test: 0.697590

Epoch: 124
Loss: 0.9885662198066711
RMSE train: 0.737699	val: 1.545481	test: 1.012779
MAE train: 0.526635	val: 0.893239	test: 0.689512

Epoch: 125
Loss: 0.9953685402870178
RMSE train: 0.752580	val: 1.532151	test: 1.004076
MAE train: 0.542890	val: 0.887205	test: 0.690712

Epoch: 126
Loss: 0.9711317121982574
RMSE train: 0.719913	val: 1.491759	test: 0.968239
MAE train: 0.533293	val: 0.856581	test: 0.673982

Epoch: 127
Loss: 0.8053662478923798
RMSE train: 0.698728	val: 1.486584	test: 0.914971
MAE train: 0.523947	val: 0.845968	test: 0.663009

Epoch: 128
Loss: 0.9049952626228333
RMSE train: 0.715026	val: 1.518391	test: 0.893931
MAE train: 0.546630	val: 0.887316	test: 0.651555

Epoch: 129
Loss: 0.8872248530387878
RMSE train: 0.728475	val: 1.549177	test: 0.896668
MAE train: 0.563197	val: 0.920806	test: 0.672183

Epoch: 130
Loss: 0.9336822032928467
RMSE train: 0.699897	val: 1.521840	test: 0.876863
MAE train: 0.536298	val: 0.877745	test: 0.651931

Epoch: 131
Loss: 0.850399523973465
RMSE train: 0.695726	val: 1.495142	test: 0.902085
MAE train: 0.523693	val: 0.835448	test: 0.647034

Epoch: 132
Loss: 0.9562622606754303
RMSE train: 0.722539	val: 1.500606	test: 0.931477
MAE train: 0.539608	val: 0.845595	test: 0.661269

Epoch: 133
Loss: 0.9078978002071381
RMSE train: 0.758966	val: 1.525064	test: 0.971555
MAE train: 0.567719	val: 0.870083	test: 0.667642

Epoch: 134
Loss: 0.8829376697540283
RMSE train: 0.780209	val: 1.529979	test: 1.032553
MAE train: 0.576239	val: 0.882260	test: 0.676469

Epoch: 135
Loss: 0.8610777854919434
RMSE train: 0.780516	val: 1.529985	test: 1.064729
MAE train: 0.576020	val: 0.881738	test: 0.684643

Epoch: 136
Loss: 0.9043907523155212
RMSE train: 0.762252	val: 1.549480	test: 0.995767
MAE train: 0.570188	val: 0.899066	test: 0.682944

Epoch: 137
Loss: 0.8307955265045166
RMSE train: 0.731194	val: 1.558366	test: 0.885882
MAE train: 0.543702	val: 0.908942	test: 0.650188

Epoch: 138
Loss: 0.908254474401474
RMSE train: 0.728373	val: 1.542732	test: 0.893964
MAE train: 0.537451	val: 0.873822	test: 0.649542

Epoch: 139
Loss: 0.7900567948818207
RMSE train: 0.735722	val: 1.510298	test: 0.913710
MAE train: 0.537140	val: 0.840567	test: 0.653912

Epoch: 140
Loss: 0.9288887679576874
RMSE train: 0.773653	val: 1.528027	test: 0.963940
MAE train: 0.568813	val: 0.867217	test: 0.703205

Epoch: 141
Loss: 0.8623257577419281
RMSE train: 0.835482	val: 1.586670	test: 1.054808
MAE train: 0.617629	val: 0.920785	test: 0.769490

Epoch: 142
Loss: 0.7545482218265533
RMSE train: 0.903279	val: 1.646415	test: 1.140211
MAE train: 0.667991	val: 0.976784	test: 0.807938

Epoch: 143
Loss: 0.7805854082107544
RMSE train: 0.859446	val: 1.627552	test: 1.093282
MAE train: 0.633366	val: 0.958720	test: 0.776670

Epoch: 144
Loss: 0.8480665981769562
RMSE train: 0.780025	val: 1.585625	test: 1.013300

Epoch: 84
Loss: 1.1476683616638184
RMSE train: 0.848051	val: 1.513112	test: 1.058549
MAE train: 0.612744	val: 0.949305	test: 0.791738

Epoch: 85
Loss: 1.1922935545444489
RMSE train: 0.837365	val: 1.485374	test: 1.077311
MAE train: 0.605358	val: 0.919501	test: 0.784449

Epoch: 86
Loss: 1.1077468395233154
RMSE train: 0.882132	val: 1.523656	test: 1.089189
MAE train: 0.648999	val: 0.944839	test: 0.793067

Epoch: 87
Loss: 1.0182313323020935
RMSE train: 0.946378	val: 1.570747	test: 1.106859
MAE train: 0.708519	val: 0.988883	test: 0.828526

Epoch: 88
Loss: 1.1053273677825928
RMSE train: 0.965335	val: 1.586583	test: 1.136743
MAE train: 0.727984	val: 1.007134	test: 0.844500

Epoch: 89
Loss: 1.1764039993286133
RMSE train: 0.964080	val: 1.580431	test: 1.261647
MAE train: 0.732576	val: 1.014118	test: 0.870702

Epoch: 90
Loss: 1.107431799173355
RMSE train: 0.978420	val: 1.583264	test: 1.312368
MAE train: 0.737510	val: 1.016333	test: 0.867600

Epoch: 91
Loss: 1.1143314242362976
RMSE train: 0.983152	val: 1.577270	test: 1.318155
MAE train: 0.731081	val: 1.019875	test: 0.873250

Epoch: 92
Loss: 1.2914438843727112
RMSE train: 0.958980	val: 1.600159	test: 1.202449
MAE train: 0.702500	val: 1.055631	test: 0.851034

Epoch: 93
Loss: 1.1332535445690155
RMSE train: 0.925818	val: 1.592724	test: 1.172826
MAE train: 0.684769	val: 1.060699	test: 0.851903

Epoch: 94
Loss: 1.1632338166236877
RMSE train: 0.937421	val: 1.593254	test: 1.152603
MAE train: 0.713065	val: 1.046675	test: 0.864762

Epoch: 95
Loss: 1.1077789664268494
RMSE train: 0.991901	val: 1.650432	test: 1.152430
MAE train: 0.766141	val: 1.092430	test: 0.881746

Epoch: 96
Loss: 1.1350153684616089
RMSE train: 1.039154	val: 1.707151	test: 1.207220
MAE train: 0.801485	val: 1.126690	test: 0.917222

Epoch: 97
Loss: 0.9719482958316803
RMSE train: 1.029544	val: 1.699273	test: 1.219970
MAE train: 0.787627	val: 1.096342	test: 0.907312

Epoch: 98
Loss: 1.1117087006568909
RMSE train: 0.995652	val: 1.637377	test: 1.210756
MAE train: 0.749019	val: 1.056649	test: 0.858081

Epoch: 99
Loss: 0.9083662629127502
RMSE train: 0.940898	val: 1.564619	test: 1.176377
MAE train: 0.691516	val: 1.007957	test: 0.805680

Epoch: 100
Loss: 1.2254303097724915
RMSE train: 0.853933	val: 1.499898	test: 1.090383
MAE train: 0.626331	val: 0.956874	test: 0.770175

Epoch: 101
Loss: 0.9409729242324829
RMSE train: 0.821160	val: 1.473088	test: 1.043637
MAE train: 0.606256	val: 0.959290	test: 0.753488

Epoch: 102
Loss: 1.1390456557273865
RMSE train: 0.833545	val: 1.488136	test: 1.067344
MAE train: 0.612407	val: 0.982939	test: 0.741230

Epoch: 103
Loss: 1.112846553325653
RMSE train: 0.846434	val: 1.527691	test: 1.066604
MAE train: 0.619182	val: 1.016866	test: 0.725560

Epoch: 104
Loss: 1.1095251142978668
RMSE train: 0.819202	val: 1.530092	test: 1.020221
MAE train: 0.600645	val: 1.010564	test: 0.705116

Epoch: 105
Loss: 0.9169854521751404
RMSE train: 0.825761	val: 1.558509	test: 1.000937
MAE train: 0.610351	val: 1.003561	test: 0.703364

Epoch: 106
Loss: 0.943941205739975
RMSE train: 0.824476	val: 1.569120	test: 0.994049
MAE train: 0.618635	val: 0.984450	test: 0.720353

Epoch: 107
Loss: 0.9205843210220337
RMSE train: 0.821805	val: 1.563698	test: 0.998993
MAE train: 0.619532	val: 0.962515	test: 0.724306

Epoch: 108
Loss: 0.9743366241455078
RMSE train: 0.847369	val: 1.586923	test: 1.003111
MAE train: 0.634899	val: 0.966390	test: 0.726478

Epoch: 109
Loss: 1.1635624170303345
RMSE train: 0.846456	val: 1.581602	test: 0.982768
MAE train: 0.624570	val: 0.947683	test: 0.715197

Epoch: 110
Loss: 0.8515631854534149
RMSE train: 0.833858	val: 1.555731	test: 0.953648
MAE train: 0.611155	val: 0.941345	test: 0.702636

Epoch: 111
Loss: 0.7845936417579651
RMSE train: 0.848310	val: 1.556561	test: 0.960370
MAE train: 0.625162	val: 0.974407	test: 0.725767

Epoch: 112
Loss: 0.9649063944816589
RMSE train: 0.835578	val: 1.541532	test: 0.952711
MAE train: 0.616971	val: 0.987095	test: 0.733633

Epoch: 113
Loss: 0.920465737581253
RMSE train: 0.766480	val: 1.524272	test: 0.919234
MAE train: 0.565833	val: 0.990091	test: 0.713255

Epoch: 114
Loss: 1.1685312986373901
RMSE train: 0.716757	val: 1.480586	test: 0.901808
MAE train: 0.525243	val: 0.953091	test: 0.688875

Epoch: 115
Loss: 0.872079998254776
RMSE train: 0.698915	val: 1.472251	test: 0.912337
MAE train: 0.511415	val: 0.939688	test: 0.678371

Epoch: 116
Loss: 0.9031178057193756
RMSE train: 0.710162	val: 1.507237	test: 0.913608
MAE train: 0.518412	val: 0.959171	test: 0.674187

Epoch: 117
Loss: 0.8712436258792877
RMSE train: 0.769076	val: 1.561216	test: 0.937555
MAE train: 0.565876	val: 1.000736	test: 0.701121

Epoch: 118
Loss: 0.9269393682479858
RMSE train: 0.833257	val: 1.577196	test: 0.983230
MAE train: 0.624036	val: 1.018439	test: 0.733452

Epoch: 119
Loss: 0.8953005075454712
RMSE train: 0.846892	val: 1.568370	test: 1.000704
MAE train: 0.641279	val: 1.009023	test: 0.741971

Epoch: 120
Loss: 0.8876687586307526
RMSE train: 0.813940	val: 1.531023	test: 0.963377
MAE train: 0.622656	val: 0.970727	test: 0.726741

Epoch: 121
Loss: 0.916569173336029
RMSE train: 0.799162	val: 1.540838	test: 0.924682
MAE train: 0.605511	val: 0.958808	test: 0.700749

Epoch: 122
Loss: 0.946549117565155
RMSE train: 0.805335	val: 1.585066	test: 0.918023
MAE train: 0.602797	val: 0.985572	test: 0.689911

Epoch: 123
Loss: 0.895173966884613
RMSE train: 0.867720	val: 1.645976	test: 0.981708
MAE train: 0.647821	val: 1.027783	test: 0.715236

Epoch: 124
Loss: 0.9223006963729858
RMSE train: 0.871340	val: 1.648583	test: 1.015259
MAE train: 0.640753	val: 1.023155	test: 0.716746

Epoch: 125
Loss: 0.8842914700508118
RMSE train: 0.835996	val: 1.640631	test: 1.002220
MAE train: 0.612221	val: 1.002445	test: 0.705872

Epoch: 126
Loss: 0.8205564618110657
RMSE train: 0.779592	val: 1.606834	test: 0.967749
MAE train: 0.570917	val: 0.966794	test: 0.689866

Epoch: 127
Loss: 0.7821006774902344
RMSE train: 0.793324	val: 1.604876	test: 0.985429
MAE train: 0.585245	val: 0.964252	test: 0.710192

Epoch: 128
Loss: 0.8062589764595032
RMSE train: 0.826341	val: 1.626347	test: 1.010954
MAE train: 0.615235	val: 1.000231	test: 0.746205

Epoch: 129
Loss: 0.9508579969406128
RMSE train: 0.870997	val: 1.655793	test: 1.031263
MAE train: 0.646204	val: 1.041665	test: 0.765081

Epoch: 130
Loss: 1.020230770111084
RMSE train: 0.840733	val: 1.612157	test: 1.023887
MAE train: 0.620397	val: 1.005044	test: 0.756233

Epoch: 131
Loss: 0.9105028510093689
RMSE train: 0.787357	val: 1.555063	test: 1.009624
MAE train: 0.574483	val: 0.942374	test: 0.728684

Epoch: 132
Loss: 0.8684252798557281
RMSE train: 0.724598	val: 1.521678	test: 0.963429
MAE train: 0.523865	val: 0.926543	test: 0.674381

Epoch: 133
Loss: 0.8618815243244171
RMSE train: 0.710694	val: 1.508104	test: 0.964610
MAE train: 0.517927	val: 0.925088	test: 0.664379

Epoch: 134
Loss: 0.7818873226642609
RMSE train: 0.710905	val: 1.506064	test: 0.987626
MAE train: 0.522155	val: 0.930330	test: 0.684453

Epoch: 135
Loss: 1.000381350517273
RMSE train: 0.731382	val: 1.524109	test: 1.003504
MAE train: 0.539794	val: 0.944893	test: 0.705849

Epoch: 136
Loss: 0.8982784748077393
RMSE train: 0.722023	val: 1.499172	test: 0.977655
MAE train: 0.542988	val: 0.920113	test: 0.706033

Epoch: 137
Loss: 1.0279299020767212
RMSE train: 0.735552	val: 1.494845	test: 0.939469
MAE train: 0.558237	val: 0.925698	test: 0.712964

Epoch: 138
Loss: 0.8504023849964142
RMSE train: 0.771832	val: 1.527153	test: 0.948263
MAE train: 0.585506	val: 0.954051	test: 0.720685

Epoch: 139
Loss: 0.7681918740272522
RMSE train: 0.797961	val: 1.580075	test: 0.948778
MAE train: 0.609204	val: 0.983565	test: 0.713165

Epoch: 140
Loss: 1.0110975503921509
RMSE train: 0.839441	val: 1.629418	test: 1.018727
MAE train: 0.631612	val: 1.021431	test: 0.747460

Epoch: 141
Loss: 1.1167124509811401
RMSE train: 0.883459	val: 1.626829	test: 1.159129
MAE train: 0.653888	val: 1.039010	test: 0.785949

Epoch: 142
Loss: 0.8418292105197906
RMSE train: 0.887727	val: 1.633064	test: 1.219215
MAE train: 0.650868	val: 1.037369	test: 0.791692

Epoch: 143
Loss: 0.8844881355762482
RMSE train: 0.849905	val: 1.621935	test: 1.177404
MAE train: 0.628141	val: 1.017891	test: 0.773933

Epoch: 144
Loss: 0.7931472361087799
RMSE train: 0.777138	val: 1.592762	test: 1.074359
MAE train: 0.576325	val: 0.927868	test: 0.728355

Epoch: 145
Loss: 0.7937639653682709
RMSE train: 0.711573	val: 1.546833	test: 0.936819
MAE train: 0.522571	val: 0.902226	test: 0.688783

Epoch: 146
Loss: 0.7622516453266144
RMSE train: 0.673826	val: 1.545434	test: 0.912609
MAE train: 0.488847	val: 0.901288	test: 0.673080

Epoch: 147
Loss: 0.771080732345581
RMSE train: 0.655109	val: 1.557441	test: 0.909557
MAE train: 0.477378	val: 0.904324	test: 0.663311

Epoch: 148
Loss: 0.8230051398277283
RMSE train: 0.655456	val: 1.550665	test: 0.954587
MAE train: 0.487820	val: 0.898675	test: 0.667243

Epoch: 149
Loss: 0.8638602495193481
RMSE train: 0.696040	val: 1.524260	test: 1.074398
MAE train: 0.521947	val: 0.880353	test: 0.706395

Early stopping
Best (RMSE):	 train: 0.703446	val: 1.425220	test: 0.936609
Best (MAE):	 train: 0.513180	val: 0.835924	test: 0.679489

MAE train: 0.573789	val: 0.983171	test: 0.731785

Epoch: 145
Loss: 0.862706184387207
RMSE train: 0.735633	val: 1.592664	test: 0.976393
MAE train: 0.541169	val: 0.968549	test: 0.691671

Epoch: 146
Loss: 0.7587968111038208
RMSE train: 0.734371	val: 1.615487	test: 0.938812
MAE train: 0.535099	val: 0.995428	test: 0.673408

Epoch: 147
Loss: 0.7369999587535858
RMSE train: 0.738189	val: 1.626201	test: 0.919316
MAE train: 0.532341	val: 0.997401	test: 0.656637

Epoch: 148
Loss: 0.755222737789154
RMSE train: 0.720754	val: 1.603561	test: 0.912768
MAE train: 0.519360	val: 0.956548	test: 0.648730

Epoch: 149
Loss: 0.7500509321689606
RMSE train: 0.741695	val: 1.577193	test: 0.967110
MAE train: 0.540244	val: 0.914384	test: 0.667868

Epoch: 150
Loss: 0.7931875288486481
RMSE train: 0.774986	val: 1.566723	test: 1.029238
MAE train: 0.570433	val: 0.908681	test: 0.695643

Early stopping
Best (RMSE):	 train: 0.698915	val: 1.472251	test: 0.912337
Best (MAE):	 train: 0.511415	val: 0.939688	test: 0.678371

MAE train: 0.619750	val: 0.829952	test: 0.947068

Epoch: 145
Loss: 0.8719998002052307
RMSE train: 0.809222	val: 1.184960	test: 1.425860
MAE train: 0.592325	val: 0.822866	test: 0.915862

Epoch: 146
Loss: 0.9327401220798492
RMSE train: 0.775529	val: 1.195484	test: 1.407497
MAE train: 0.567188	val: 0.831780	test: 0.887885

Epoch: 147
Loss: 0.8023201525211334
RMSE train: 0.764952	val: 1.214276	test: 1.409030
MAE train: 0.565428	val: 0.836518	test: 0.871180

Epoch: 148
Loss: 0.816813588142395
RMSE train: 0.763143	val: 1.205881	test: 1.431312
MAE train: 0.569943	val: 0.826509	test: 0.879715

Epoch: 149
Loss: 0.9303670525550842
RMSE train: 0.808240	val: 1.216765	test: 1.476338
MAE train: 0.606777	val: 0.838087	test: 0.915223

Epoch: 150
Loss: 0.8220275938510895
RMSE train: 0.884773	val: 1.247448	test: 1.533909
MAE train: 0.652252	val: 0.863451	test: 0.961591

Epoch: 151
Loss: 0.7806411683559418
RMSE train: 0.906055	val: 1.262513	test: 1.557545
MAE train: 0.654461	val: 0.871007	test: 0.980097

Epoch: 152
Loss: 1.0409374833106995
RMSE train: 0.851843	val: 1.216091	test: 1.509156
MAE train: 0.611715	val: 0.832067	test: 0.942481

Epoch: 153
Loss: 1.0080598294734955
RMSE train: 0.735270	val: 1.148976	test: 1.421640
MAE train: 0.538041	val: 0.780387	test: 0.871570

Epoch: 154
Loss: 0.889433741569519
RMSE train: 0.709098	val: 1.139703	test: 1.388157
MAE train: 0.526364	val: 0.784486	test: 0.849640

Epoch: 155
Loss: 0.7132377922534943
RMSE train: 0.715466	val: 1.132489	test: 1.379211
MAE train: 0.525885	val: 0.769345	test: 0.845967

Epoch: 156
Loss: 0.9045220911502838
RMSE train: 0.741990	val: 1.127310	test: 1.382055
MAE train: 0.544191	val: 0.758212	test: 0.851092

Epoch: 157
Loss: 0.9603142440319061
RMSE train: 0.747510	val: 1.114747	test: 1.362500
MAE train: 0.535728	val: 0.762880	test: 0.846903

Epoch: 158
Loss: 1.0738588273525238
RMSE train: 0.809870	val: 1.140393	test: 1.375388
MAE train: 0.572869	val: 0.781664	test: 0.860651

Epoch: 159
Loss: 0.8449966907501221
RMSE train: 0.779156	val: 1.112655	test: 1.362984
MAE train: 0.552103	val: 0.774596	test: 0.858740

Epoch: 160
Loss: 0.8119164705276489
RMSE train: 0.781034	val: 1.097790	test: 1.365325
MAE train: 0.548214	val: 0.762793	test: 0.858207

Epoch: 161
Loss: 0.8304445445537567
RMSE train: 0.816842	val: 1.113025	test: 1.382657
MAE train: 0.588922	val: 0.779520	test: 0.888044

Epoch: 162
Loss: 0.8317318558692932
RMSE train: 0.831717	val: 1.121493	test: 1.394720
MAE train: 0.621915	val: 0.795990	test: 0.913455

Epoch: 163
Loss: 0.8000332117080688
RMSE train: 0.835476	val: 1.133109	test: 1.383787
MAE train: 0.628368	val: 0.802835	test: 0.913510

Epoch: 164
Loss: 0.8139634430408478
RMSE train: 0.846681	val: 1.146543	test: 1.378910
MAE train: 0.632493	val: 0.809581	test: 0.911833

Epoch: 165
Loss: 0.7560911178588867
RMSE train: 0.853873	val: 1.163318	test: 1.366572
MAE train: 0.627895	val: 0.810649	test: 0.897153

Epoch: 166
Loss: 0.9859249591827393
RMSE train: 0.849293	val: 1.172909	test: 1.356416
MAE train: 0.617970	val: 0.809654	test: 0.885015

Epoch: 167
Loss: 0.8486951589584351
RMSE train: 0.880893	val: 1.194289	test: 1.372828
MAE train: 0.640371	val: 0.819084	test: 0.888266

Epoch: 168
Loss: 0.8676337003707886
RMSE train: 0.889489	val: 1.198535	test: 1.400279
MAE train: 0.645601	val: 0.824433	test: 0.898862

Epoch: 169
Loss: 0.7874083518981934
RMSE train: 0.868509	val: 1.167347	test: 1.408703
MAE train: 0.638329	val: 0.814676	test: 0.912548

Epoch: 170
Loss: 0.8969335556030273
RMSE train: 0.791298	val: 1.094711	test: 1.382625
MAE train: 0.590166	val: 0.777905	test: 0.871021

Epoch: 171
Loss: 0.6956754326820374
RMSE train: 0.767041	val: 1.074127	test: 1.364838
MAE train: 0.583065	val: 0.766008	test: 0.843173

Epoch: 172
Loss: 0.7241232693195343
RMSE train: 0.799923	val: 1.110400	test: 1.378576
MAE train: 0.608156	val: 0.795762	test: 0.863417

Epoch: 173
Loss: 1.0945019721984863
RMSE train: 0.855269	val: 1.195821	test: 1.403453
MAE train: 0.643839	val: 0.852628	test: 0.897039

Epoch: 174
Loss: 0.6449806988239288
RMSE train: 0.943477	val: 1.307497	test: 1.435043
MAE train: 0.696831	val: 0.905037	test: 0.927649

Epoch: 175
Loss: 0.9838428497314453
RMSE train: 0.932452	val: 1.303331	test: 1.419299
MAE train: 0.674948	val: 0.896985	test: 0.916539

Epoch: 176
Loss: 0.726496696472168
RMSE train: 0.863585	val: 1.266911	test: 1.396396
MAE train: 0.624103	val: 0.872169	test: 0.902377

Epoch: 177
Loss: 0.8693971931934357
RMSE train: 0.774932	val: 1.217937	test: 1.368470
MAE train: 0.564707	val: 0.839991	test: 0.880160

Epoch: 178
Loss: 0.7525261342525482
RMSE train: 0.710105	val: 1.143852	test: 1.341980
MAE train: 0.519080	val: 0.793415	test: 0.860077

Epoch: 179
Loss: 0.713449239730835
RMSE train: 0.690234	val: 1.100482	test: 1.323050
MAE train: 0.507635	val: 0.762301	test: 0.842813

Epoch: 180
Loss: 0.6298279166221619
RMSE train: 0.717611	val: 1.092307	test: 1.318708
MAE train: 0.533888	val: 0.764353	test: 0.849409

Epoch: 181
Loss: 0.656881183385849
RMSE train: 0.750863	val: 1.106514	test: 1.331472
MAE train: 0.552360	val: 0.776471	test: 0.851691

Epoch: 182
Loss: 0.8104249835014343
RMSE train: 0.774114	val: 1.133769	test: 1.347970
MAE train: 0.568514	val: 0.792359	test: 0.858746

Epoch: 183
Loss: 0.8066456317901611
RMSE train: 0.799467	val: 1.169820	test: 1.355494
MAE train: 0.574333	val: 0.816156	test: 0.854836

Epoch: 184
Loss: 0.8896355628967285
RMSE train: 0.759126	val: 1.147869	test: 1.340705
MAE train: 0.542814	val: 0.804047	test: 0.844596

Epoch: 185
Loss: 0.6656246781349182
RMSE train: 0.742135	val: 1.125501	test: 1.312718
MAE train: 0.526697	val: 0.791417	test: 0.832130

Epoch: 186
Loss: 0.8106953799724579
RMSE train: 0.732758	val: 1.114295	test: 1.293572
MAE train: 0.521054	val: 0.787811	test: 0.825326

Epoch: 187
Loss: 0.9453021883964539
RMSE train: 0.769746	val: 1.128254	test: 1.302173
MAE train: 0.548202	val: 0.793733	test: 0.834365

Epoch: 188
Loss: 0.7348293960094452
RMSE train: 0.803553	val: 1.150518	test: 1.343740
MAE train: 0.579622	val: 0.816748	test: 0.868030

Epoch: 189
Loss: 0.8533448874950409
RMSE train: 0.809958	val: 1.159708	test: 1.367546
MAE train: 0.593948	val: 0.828159	test: 0.884671

Epoch: 190
Loss: 0.6999581456184387
RMSE train: 0.791728	val: 1.154183	test: 1.368027
MAE train: 0.586915	val: 0.821217	test: 0.881005

Epoch: 191
Loss: 0.7591316998004913
RMSE train: 0.784633	val: 1.154523	test: 1.360297
MAE train: 0.578728	val: 0.809160	test: 0.876338

Epoch: 192
Loss: 0.6695433557033539
RMSE train: 0.807087	val: 1.181999	test: 1.378450
MAE train: 0.595766	val: 0.821165	test: 0.887304

Epoch: 193
Loss: 0.7247657477855682
RMSE train: 0.814306	val: 1.192604	test: 1.404682
MAE train: 0.598399	val: 0.825094	test: 0.903891

Epoch: 194
Loss: 0.7405617535114288
RMSE train: 0.827939	val: 1.204523	test: 1.422861
MAE train: 0.599354	val: 0.832406	test: 0.917971

Epoch: 195
Loss: 0.8878169655799866
RMSE train: 0.773912	val: 1.148168	test: 1.369633
MAE train: 0.559389	val: 0.795222	test: 0.880672

Epoch: 196
Loss: 0.5791751742362976
RMSE train: 0.710951	val: 1.096739	test: 1.319156
MAE train: 0.519336	val: 0.770172	test: 0.848817

Epoch: 197
Loss: 0.7557628750801086
RMSE train: 0.670529	val: 1.069992	test: 1.288035
MAE train: 0.499716	val: 0.759678	test: 0.827254

Epoch: 198
Loss: 0.7965191602706909
RMSE train: 0.674233	val: 1.076781	test: 1.278543
MAE train: 0.507048	val: 0.772937	test: 0.825853

Epoch: 199
Loss: 0.8140797019004822
RMSE train: 0.706356	val: 1.098836	test: 1.294757
MAE train: 0.527535	val: 0.787600	test: 0.827813

Epoch: 200
Loss: 0.7133879363536835
RMSE train: 0.732663	val: 1.120188	test: 1.330537
MAE train: 0.548762	val: 0.789108	test: 0.846209

Epoch: 201
Loss: 0.6995261609554291
RMSE train: 0.742809	val: 1.138121	test: 1.381100
MAE train: 0.556023	val: 0.786515	test: 0.868517

Epoch: 202
Loss: 0.8925477564334869
RMSE train: 0.710869	val: 1.124902	test: 1.378321
MAE train: 0.533384	val: 0.769299	test: 0.861624

Epoch: 203
Loss: 0.8077733218669891
RMSE train: 0.668480	val: 1.111161	test: 1.344946
MAE train: 0.501845	val: 0.753468	test: 0.835794

Epoch: 204
Loss: 0.661652147769928
RMSE train: 0.669488	val: 1.127320	test: 1.318042
MAE train: 0.502391	val: 0.776502	test: 0.818762All runs completed.

MAE train: 0.533135	val: 0.801715	test: 0.820959

Epoch: 145
Loss: 0.9486163258552551
RMSE train: 0.713738	val: 1.248175	test: 1.205047
MAE train: 0.540265	val: 0.809529	test: 0.811363

Epoch: 146
Loss: 0.7178823053836823
RMSE train: 0.712741	val: 1.262246	test: 1.192871
MAE train: 0.543853	val: 0.814004	test: 0.804857

Epoch: 147
Loss: 0.816346138715744
RMSE train: 0.720829	val: 1.270703	test: 1.178265
MAE train: 0.552758	val: 0.814232	test: 0.791114

Epoch: 148
Loss: 0.8191474080085754
RMSE train: 0.715094	val: 1.273108	test: 1.185385
MAE train: 0.550371	val: 0.808388	test: 0.789811

Epoch: 149
Loss: 0.7255052030086517
RMSE train: 0.730964	val: 1.328624	test: 1.228824
MAE train: 0.560212	val: 0.839336	test: 0.814614

Epoch: 150
Loss: 0.7208905816078186
RMSE train: 0.750205	val: 1.353569	test: 1.260992
MAE train: 0.568176	val: 0.846783	test: 0.815678

Epoch: 151
Loss: 0.6633691489696503
RMSE train: 0.737660	val: 1.354573	test: 1.256412
MAE train: 0.550006	val: 0.852132	test: 0.803190

Epoch: 152
Loss: 0.6963610351085663
RMSE train: 0.711173	val: 1.337354	test: 1.241166
MAE train: 0.526325	val: 0.856509	test: 0.793975

Epoch: 153
Loss: 0.9501886069774628
RMSE train: 0.659277	val: 1.307753	test: 1.193112
MAE train: 0.490697	val: 0.853654	test: 0.775041

Epoch: 154
Loss: 0.717354953289032
RMSE train: 0.621204	val: 1.287858	test: 1.158428
MAE train: 0.456313	val: 0.842708	test: 0.757515

Epoch: 155
Loss: 0.7122003138065338
RMSE train: 0.652310	val: 1.313649	test: 1.156010
MAE train: 0.487019	val: 0.864250	test: 0.779365

Epoch: 156
Loss: 0.7465263903141022
RMSE train: 0.669598	val: 1.307060	test: 1.144927
MAE train: 0.503870	val: 0.865901	test: 0.776952

Epoch: 157
Loss: 0.7492031455039978
RMSE train: 0.676200	val: 1.306585	test: 1.130909
MAE train: 0.505720	val: 0.866536	test: 0.772083

Epoch: 158
Loss: 0.7625410556793213
RMSE train: 0.726825	val: 1.319618	test: 1.149518
MAE train: 0.548857	val: 0.875199	test: 0.797511

Epoch: 159
Loss: 0.837584525346756
RMSE train: 0.746943	val: 1.312504	test: 1.175222
MAE train: 0.562823	val: 0.867270	test: 0.816359

Epoch: 160
Loss: 0.8548146188259125
RMSE train: 0.747958	val: 1.280321	test: 1.186967
MAE train: 0.561438	val: 0.849709	test: 0.823475

Epoch: 161
Loss: 0.6969835460186005
RMSE train: 0.710305	val: 1.262344	test: 1.163100
MAE train: 0.532920	val: 0.828018	test: 0.805581

Epoch: 162
Loss: 0.6323724687099457
RMSE train: 0.701057	val: 1.281645	test: 1.140293
MAE train: 0.528406	val: 0.835539	test: 0.796181

Epoch: 163
Loss: 0.7813307344913483
RMSE train: 0.699542	val: 1.320282	test: 1.104504
MAE train: 0.526773	val: 0.860476	test: 0.768208

Epoch: 164
Loss: 0.6553020775318146
RMSE train: 0.688381	val: 1.308464	test: 1.070288
MAE train: 0.520556	val: 0.855036	test: 0.744584

Epoch: 165
Loss: 0.8224627673625946
RMSE train: 0.665105	val: 1.272624	test: 1.047310
MAE train: 0.502412	val: 0.828684	test: 0.714983

Epoch: 166
Loss: 0.7529793083667755
RMSE train: 0.681997	val: 1.256207	test: 1.045145
MAE train: 0.518797	val: 0.819398	test: 0.723612

Epoch: 167
Loss: 1.065204381942749
RMSE train: 0.686080	val: 1.258437	test: 1.075998
MAE train: 0.529149	val: 0.816805	test: 0.747991

Epoch: 168
Loss: 0.7206062376499176
RMSE train: 0.690544	val: 1.276950	test: 1.111823
MAE train: 0.538671	val: 0.827803	test: 0.775531

Epoch: 169
Loss: 0.7513276040554047
RMSE train: 0.686336	val: 1.336839	test: 1.143788
MAE train: 0.532330	val: 0.854500	test: 0.796164

Epoch: 170
Loss: 0.7561969459056854
RMSE train: 0.703047	val: 1.375620	test: 1.168330
MAE train: 0.540690	val: 0.868899	test: 0.816768

Epoch: 171
Loss: 0.6945019662380219
RMSE train: 0.710965	val: 1.416028	test: 1.189093
MAE train: 0.542004	val: 0.885745	test: 0.826941

Epoch: 172
Loss: 0.6066762804985046
RMSE train: 0.722853	val: 1.430135	test: 1.201049
MAE train: 0.542042	val: 0.901356	test: 0.832533

Epoch: 173
Loss: 0.8069494068622589
RMSE train: 0.757259	val: 1.454020	test: 1.217422
MAE train: 0.555805	val: 0.933645	test: 0.845041

Epoch: 174
Loss: 0.8699122965335846
RMSE train: 0.740663	val: 1.416978	test: 1.183049
MAE train: 0.541940	val: 0.924788	test: 0.828462

Epoch: 175
Loss: 0.612144410610199
RMSE train: 0.696388	val: 1.386438	test: 1.125025
MAE train: 0.513148	val: 0.905825	test: 0.787487

Epoch: 176
Loss: 0.8611955940723419
RMSE train: 0.642663	val: 1.332129	test: 1.096327
MAE train: 0.484222	val: 0.858217	test: 0.757835

Epoch: 177
Loss: 0.7505017518997192
RMSE train: 0.640768	val: 1.274795	test: 1.093926
MAE train: 0.487339	val: 0.819098	test: 0.746450

Epoch: 178
Loss: 0.7351461052894592
RMSE train: 0.625117	val: 1.251163	test: 1.107004
MAE train: 0.472580	val: 0.816454	test: 0.743382

Epoch: 179
Loss: 0.7455158531665802
RMSE train: 0.621145	val: 1.273783	test: 1.114291
MAE train: 0.465385	val: 0.841012	test: 0.755361

Early stopping
Best (RMSE):	 train: 0.710507	val: 1.237519	test: 1.223990
Best (MAE):	 train: 0.533135	val: 0.801715	test: 0.820959

MAE train: 0.624136	val: 0.881321	test: 0.823345

Epoch: 145
Loss: 0.9627588093280792
RMSE train: 0.810834	val: 1.358070	test: 1.167187
MAE train: 0.618570	val: 0.874506	test: 0.819133

Epoch: 146
Loss: 0.8004329204559326
RMSE train: 0.806987	val: 1.320369	test: 1.154048
MAE train: 0.613758	val: 0.847789	test: 0.800153

Epoch: 147
Loss: 0.827740490436554
RMSE train: 0.803995	val: 1.287088	test: 1.143673
MAE train: 0.605652	val: 0.828642	test: 0.782386

Epoch: 148
Loss: 0.9406701028347015
RMSE train: 0.835580	val: 1.321455	test: 1.154114
MAE train: 0.626225	val: 0.856060	test: 0.799651

Epoch: 149
Loss: 0.9533478617668152
RMSE train: 0.841654	val: 1.391287	test: 1.169254
MAE train: 0.636758	val: 0.896132	test: 0.813399

Epoch: 150
Loss: 0.8858266472816467
RMSE train: 0.825019	val: 1.419929	test: 1.189423
MAE train: 0.628237	val: 0.914148	test: 0.821468

Epoch: 151
Loss: 0.8990588188171387
RMSE train: 0.809022	val: 1.410020	test: 1.180171
MAE train: 0.611087	val: 0.901704	test: 0.811988

Epoch: 152
Loss: 0.9195922315120697
RMSE train: 0.800539	val: 1.402386	test: 1.167296
MAE train: 0.601462	val: 0.896775	test: 0.809968

Epoch: 153
Loss: 0.8406715393066406
RMSE train: 0.810026	val: 1.385319	test: 1.151322
MAE train: 0.608673	val: 0.891753	test: 0.812139

Epoch: 154
Loss: 0.8580120503902435
RMSE train: 0.812402	val: 1.364133	test: 1.140742
MAE train: 0.600742	val: 0.878808	test: 0.792080

Epoch: 155
Loss: 0.8135021030902863
RMSE train: 0.786546	val: 1.306270	test: 1.121589
MAE train: 0.578542	val: 0.844670	test: 0.765956

Epoch: 156
Loss: 0.7959893643856049
RMSE train: 0.724853	val: 1.242427	test: 1.103191
MAE train: 0.534050	val: 0.800839	test: 0.741357

Epoch: 157
Loss: 0.7928128838539124
RMSE train: 0.711785	val: 1.254918	test: 1.118166
MAE train: 0.514398	val: 0.799392	test: 0.757938

Epoch: 158
Loss: 0.7468702793121338
RMSE train: 0.756690	val: 1.298733	test: 1.161957
MAE train: 0.552742	val: 0.834306	test: 0.796905

Epoch: 159
Loss: 0.6998055279254913
RMSE train: 0.798020	val: 1.333930	test: 1.178490
MAE train: 0.590789	val: 0.854843	test: 0.809404

Epoch: 160
Loss: 0.7547423541545868
RMSE train: 0.825893	val: 1.356299	test: 1.161419
MAE train: 0.609276	val: 0.853021	test: 0.809090

Epoch: 161
Loss: 0.7554957866668701
RMSE train: 0.838345	val: 1.362518	test: 1.147684
MAE train: 0.616571	val: 0.852391	test: 0.805199

Epoch: 162
Loss: 0.8126235008239746
RMSE train: 0.811836	val: 1.325926	test: 1.121330
MAE train: 0.598900	val: 0.834192	test: 0.784301

Epoch: 163
Loss: 0.9314177930355072
RMSE train: 0.777584	val: 1.333848	test: 1.103828
MAE train: 0.579144	val: 0.850669	test: 0.770140

Epoch: 164
Loss: 0.6475928723812103
RMSE train: 0.718246	val: 1.311900	test: 1.085405
MAE train: 0.534441	val: 0.841756	test: 0.746245

Epoch: 165
Loss: 0.9743502140045166
RMSE train: 0.696098	val: 1.308585	test: 1.089944
MAE train: 0.516438	val: 0.838306	test: 0.740975

Epoch: 166
Loss: 0.6463190317153931
RMSE train: 0.678866	val: 1.291334	test: 1.103824
MAE train: 0.500746	val: 0.814888	test: 0.743807

Epoch: 167
Loss: 0.7735368013381958
RMSE train: 0.663381	val: 1.268217	test: 1.102650
MAE train: 0.487230	val: 0.786068	test: 0.733736

Epoch: 168
Loss: 0.8463446497917175
RMSE train: 0.711926	val: 1.290278	test: 1.125612
MAE train: 0.532228	val: 0.817392	test: 0.748387

Epoch: 169
Loss: 0.6602760255336761
RMSE train: 0.760623	val: 1.317615	test: 1.152946
MAE train: 0.576129	val: 0.847642	test: 0.780459

Epoch: 170
Loss: 0.8066650331020355
RMSE train: 0.817062	val: 1.364119	test: 1.187427
MAE train: 0.622359	val: 0.887347	test: 0.818785

Epoch: 171
Loss: 0.7710235118865967
RMSE train: 0.858082	val: 1.405512	test: 1.208964
MAE train: 0.647624	val: 0.915764	test: 0.833138

Epoch: 172
Loss: 0.6082025468349457
RMSE train: 0.862214	val: 1.413195	test: 1.219866
MAE train: 0.637660	val: 0.913112	test: 0.842951

Epoch: 173
Loss: 0.7903936505317688
RMSE train: 0.842500	val: 1.384662	test: 1.207755
MAE train: 0.611662	val: 0.889931	test: 0.839155

Epoch: 174
Loss: 0.7438958883285522
RMSE train: 0.806712	val: 1.343454	test: 1.187067
MAE train: 0.583983	val: 0.859915	test: 0.831009

Epoch: 175
Loss: 0.681533008813858
RMSE train: 0.795938	val: 1.336567	test: 1.177756
MAE train: 0.584804	val: 0.869869	test: 0.825241

Epoch: 176
Loss: 0.6531363725662231
RMSE train: 0.791290	val: 1.315466	test: 1.160541
MAE train: 0.592411	val: 0.867891	test: 0.837636

Epoch: 177
Loss: 0.7544702887535095
RMSE train: 0.797842	val: 1.312496	test: 1.156435
MAE train: 0.594950	val: 0.860759	test: 0.841547

Epoch: 178
Loss: 0.7930154204368591
RMSE train: 0.788991	val: 1.319052	test: 1.162724
MAE train: 0.585351	val: 0.856800	test: 0.830920

Epoch: 179
Loss: 0.6836507618427277
RMSE train: 0.778276	val: 1.340445	test: 1.161003
MAE train: 0.586530	val: 0.861477	test: 0.808925

Epoch: 180
Loss: 0.8583052754402161
RMSE train: 0.739312	val: 1.296503	test: 1.138232
MAE train: 0.557621	val: 0.827659	test: 0.791551

Epoch: 181
Loss: 0.5944601893424988
RMSE train: 0.703362	val: 1.225792	test: 1.101237
MAE train: 0.520080	val: 0.770335	test: 0.753031

Epoch: 182
Loss: 0.6929153203964233
RMSE train: 0.710100	val: 1.220988	test: 1.097795
MAE train: 0.518622	val: 0.776713	test: 0.748358

Epoch: 183
Loss: 0.715768575668335
RMSE train: 0.745962	val: 1.274654	test: 1.120932
MAE train: 0.544953	val: 0.814650	test: 0.770916

Epoch: 184
Loss: 0.6381089389324188
RMSE train: 0.759874	val: 1.308873	test: 1.136690
MAE train: 0.553415	val: 0.832444	test: 0.774591

Epoch: 185
Loss: 0.5737030506134033
RMSE train: 0.751090	val: 1.328527	test: 1.162810
MAE train: 0.549079	val: 0.836377	test: 0.780340

Epoch: 186
Loss: 0.7526521384716034
RMSE train: 0.738403	val: 1.361417	test: 1.188479
MAE train: 0.542951	val: 0.859821	test: 0.805765

Epoch: 187
Loss: 0.7295670509338379
RMSE train: 0.726920	val: 1.370679	test: 1.193443
MAE train: 0.534425	val: 0.867896	test: 0.822460

Epoch: 188
Loss: 0.7343054711818695
RMSE train: 0.669882	val: 1.283470	test: 1.155574
MAE train: 0.487816	val: 0.796968	test: 0.781946

Epoch: 189
Loss: 0.766766756772995
RMSE train: 0.659075	val: 1.209521	test: 1.111545
MAE train: 0.478395	val: 0.744586	test: 0.743401

Epoch: 190
Loss: 0.8232068717479706
RMSE train: 0.664739	val: 1.200038	test: 1.089912
MAE train: 0.481813	val: 0.739127	test: 0.730009

Epoch: 191
Loss: 0.6973714828491211
RMSE train: 0.651437	val: 1.206311	test: 1.092839
MAE train: 0.471824	val: 0.744636	test: 0.724447

Epoch: 192
Loss: 0.7030525505542755
RMSE train: 0.650206	val: 1.224576	test: 1.122606
MAE train: 0.475226	val: 0.749848	test: 0.727410

Epoch: 193
Loss: 0.6122677028179169
RMSE train: 0.677241	val: 1.272406	test: 1.163482
MAE train: 0.499513	val: 0.786761	test: 0.762801

Epoch: 194
Loss: 0.7902989089488983
RMSE train: 0.655680	val: 1.245341	test: 1.155369
MAE train: 0.489268	val: 0.775719	test: 0.755140

Epoch: 195
Loss: 0.680862545967102
RMSE train: 0.610063	val: 1.181016	test: 1.138168
MAE train: 0.456895	val: 0.738013	test: 0.734792

Epoch: 196
Loss: 0.6950651705265045
RMSE train: 0.595756	val: 1.126182	test: 1.126620
MAE train: 0.449183	val: 0.709315	test: 0.711568

Epoch: 197
Loss: 0.5390055775642395
RMSE train: 0.604801	val: 1.140543	test: 1.109140
MAE train: 0.455644	val: 0.714125	test: 0.705994

Epoch: 198
Loss: 0.9344129264354706
RMSE train: 0.651307	val: 1.211399	test: 1.091700
MAE train: 0.486403	val: 0.759559	test: 0.715098

Epoch: 199
Loss: 0.728765606880188
RMSE train: 0.685317	val: 1.276736	test: 1.112530
MAE train: 0.519461	val: 0.804491	test: 0.744670

Epoch: 200
Loss: 0.6190873682498932
RMSE train: 0.712618	val: 1.316935	test: 1.145797
MAE train: 0.543705	val: 0.836342	test: 0.767223

Epoch: 201
Loss: 0.6510244607925415
RMSE train: 0.678952	val: 1.283521	test: 1.149953
MAE train: 0.511036	val: 0.807620	test: 0.762557

Epoch: 202
Loss: 0.6638493835926056
RMSE train: 0.591649	val: 1.201914	test: 1.123808
MAE train: 0.439137	val: 0.751049	test: 0.723353

Epoch: 203
Loss: 0.706565648317337
RMSE train: 0.555208	val: 1.159063	test: 1.104198
MAE train: 0.412867	val: 0.722072	test: 0.710413

Epoch: 204
Loss: 0.6121196150779724
RMSE train: 0.576072	val: 1.177555	test: 1.095096
MAE train: 0.429012	val: 0.732187	test: 0.714806

Epoch: 205
Loss: 0.8037199974060059
RMSE train: 0.707440	val: 1.158667	test: 1.308033
MAE train: 0.530218	val: 0.804157	test: 0.819247

Epoch: 206
Loss: 0.6618916094303131
RMSE train: 0.770769	val: 1.206946	test: 1.323811
MAE train: 0.569834	val: 0.820202	test: 0.848664

Epoch: 207
Loss: 0.8578771948814392
RMSE train: 0.807963	val: 1.228303	test: 1.344501
MAE train: 0.586651	val: 0.821204	test: 0.869252

Epoch: 208
Loss: 0.7519402503967285
RMSE train: 0.749823	val: 1.189858	test: 1.338689
MAE train: 0.539779	val: 0.794593	test: 0.848995

Epoch: 209
Loss: 0.7703734934329987
RMSE train: 0.679421	val: 1.140378	test: 1.316804
MAE train: 0.500610	val: 0.775673	test: 0.827764

Epoch: 210
Loss: 0.712430328130722
RMSE train: 0.651605	val: 1.109467	test: 1.299108
MAE train: 0.491456	val: 0.770738	test: 0.817616

Epoch: 211
Loss: 0.6045459508895874
RMSE train: 0.644695	val: 1.079319	test: 1.296295
MAE train: 0.490947	val: 0.757736	test: 0.815893

Epoch: 212
Loss: 0.7058722972869873
RMSE train: 0.654214	val: 1.068119	test: 1.310948
MAE train: 0.496113	val: 0.750197	test: 0.825761

Epoch: 213
Loss: 0.7029585242271423
RMSE train: 0.690686	val: 1.075550	test: 1.330120
MAE train: 0.515321	val: 0.752824	test: 0.836292

Epoch: 214
Loss: 0.6024826169013977
RMSE train: 0.727753	val: 1.096597	test: 1.352573
MAE train: 0.531901	val: 0.767133	test: 0.852801

Epoch: 215
Loss: 0.6480868756771088
RMSE train: 0.754859	val: 1.127188	test: 1.367584
MAE train: 0.541118	val: 0.778546	test: 0.863102

Epoch: 216
Loss: 0.5635503083467484
RMSE train: 0.790945	val: 1.159891	test: 1.378317
MAE train: 0.554825	val: 0.791554	test: 0.868638

Epoch: 217
Loss: 0.5573514103889465
RMSE train: 0.803948	val: 1.178876	test: 1.375476
MAE train: 0.556548	val: 0.805112	test: 0.869570

Epoch: 218
Loss: 0.7069569528102875
RMSE train: 0.765811	val: 1.154504	test: 1.358296
MAE train: 0.533978	val: 0.794243	test: 0.858078

Epoch: 219
Loss: 0.6760159432888031
RMSE train: 0.694419	val: 1.103256	test: 1.328348
MAE train: 0.498474	val: 0.769125	test: 0.833729

Epoch: 220
Loss: 0.5474165678024292
RMSE train: 0.648891	val: 1.070739	test: 1.313549
MAE train: 0.478863	val: 0.743869	test: 0.823103

Epoch: 221
Loss: 0.6182555556297302
RMSE train: 0.657296	val: 1.073171	test: 1.335180
MAE train: 0.487600	val: 0.739326	test: 0.835055

Epoch: 222
Loss: 0.8804950416088104
RMSE train: 0.688721	val: 1.094724	test: 1.361546
MAE train: 0.513748	val: 0.760032	test: 0.858399

Epoch: 223
Loss: 0.6229191422462463
RMSE train: 0.692400	val: 1.107393	test: 1.371446
MAE train: 0.517304	val: 0.769946	test: 0.866135

Epoch: 224
Loss: 0.8886167407035828
RMSE train: 0.699829	val: 1.131711	test: 1.374147
MAE train: 0.516865	val: 0.784739	test: 0.870335

Epoch: 225
Loss: 0.5217899084091187
RMSE train: 0.735194	val: 1.168905	test: 1.360449
MAE train: 0.535097	val: 0.807424	test: 0.872642

Epoch: 226
Loss: 0.5779158473014832
RMSE train: 0.731489	val: 1.189291	test: 1.347424
MAE train: 0.527869	val: 0.816540	test: 0.869763

Epoch: 227
Loss: 0.6454122960567474
RMSE train: 0.683536	val: 1.174106	test: 1.332562
MAE train: 0.495345	val: 0.805332	test: 0.855182

Epoch: 228
Loss: 0.6353392601013184
RMSE train: 0.622361	val: 1.145487	test: 1.318939
MAE train: 0.455821	val: 0.789180	test: 0.843027

Epoch: 229
Loss: 0.847222775220871
RMSE train: 0.606424	val: 1.136825	test: 1.318467
MAE train: 0.449102	val: 0.782565	test: 0.845776

Epoch: 230
Loss: 0.8072674572467804
RMSE train: 0.640150	val: 1.141901	test: 1.353218
MAE train: 0.485827	val: 0.803743	test: 0.873112

Epoch: 231
Loss: 0.872695654630661
RMSE train: 0.692728	val: 1.158064	test: 1.377780
MAE train: 0.529779	val: 0.825544	test: 0.893127

Epoch: 232
Loss: 0.7225308418273926
RMSE train: 0.694196	val: 1.171225	test: 1.397265
MAE train: 0.534138	val: 0.838703	test: 0.901983

Epoch: 233
Loss: 0.6852463185787201
RMSE train: 0.645883	val: 1.128714	test: 1.379786
MAE train: 0.499471	val: 0.794796	test: 0.871399

Epoch: 234
Loss: 0.7694190740585327
RMSE train: 0.592619	val: 1.073123	test: 1.334251
MAE train: 0.460500	val: 0.744469	test: 0.827915

Epoch: 235
Loss: 0.5539348423480988
RMSE train: 0.602124	val: 1.045977	test: 1.309434
MAE train: 0.459729	val: 0.726077	test: 0.823167

Epoch: 236
Loss: 0.5992944538593292
RMSE train: 0.671504	val: 1.076147	test: 1.319176
MAE train: 0.493782	val: 0.742564	test: 0.844087

Epoch: 237
Loss: 0.6622770428657532
RMSE train: 0.756731	val: 1.123065	test: 1.342163
MAE train: 0.555996	val: 0.778830	test: 0.871746

Epoch: 238
Loss: 0.5725742280483246
RMSE train: 0.764071	val: 1.116834	test: 1.346531
MAE train: 0.573272	val: 0.781974	test: 0.884255

Epoch: 239
Loss: 0.7151450514793396
RMSE train: 0.714546	val: 1.074695	test: 1.325445
MAE train: 0.545245	val: 0.766334	test: 0.870809

Epoch: 240
Loss: 0.6546687483787537
RMSE train: 0.704241	val: 1.072638	test: 1.324737
MAE train: 0.539129	val: 0.765289	test: 0.871310

Epoch: 241
Loss: 0.5608125627040863
RMSE train: 0.680195	val: 1.078410	test: 1.346934
MAE train: 0.515948	val: 0.764696	test: 0.871579

Epoch: 242
Loss: 0.8313857316970825
RMSE train: 0.649390	val: 1.100195	test: 1.366487
MAE train: 0.478673	val: 0.764943	test: 0.877313

Epoch: 243
Loss: 0.6992541551589966
RMSE train: 0.639258	val: 1.135832	test: 1.365465
MAE train: 0.464103	val: 0.780468	test: 0.871310

Epoch: 244
Loss: 0.7281427681446075
RMSE train: 0.668259	val: 1.163377	test: 1.357701
MAE train: 0.481299	val: 0.803824	test: 0.873513

Epoch: 245
Loss: 0.8622993230819702
RMSE train: 0.714993	val: 1.181943	test: 1.349672
MAE train: 0.515992	val: 0.825984	test: 0.878025

Epoch: 246
Loss: 0.6201353669166565
RMSE train: 0.694548	val: 1.152467	test: 1.327585
MAE train: 0.504592	val: 0.804104	test: 0.861439

Epoch: 247
Loss: 0.6603032648563385
RMSE train: 0.696430	val: 1.126065	test: 1.331220
MAE train: 0.502088	val: 0.775542	test: 0.853702

Epoch: 248
Loss: 0.5948445796966553
RMSE train: 0.701107	val: 1.113154	test: 1.351369
MAE train: 0.505260	val: 0.767588	test: 0.869148

Epoch: 249
Loss: 0.5582796186208725
RMSE train: 0.635705	val: 1.058489	test: 1.313716
MAE train: 0.462215	val: 0.723609	test: 0.834041

Epoch: 250
Loss: 0.6590367257595062
RMSE train: 0.572541	val: 1.004937	test: 1.259519
MAE train: 0.419797	val: 0.678581	test: 0.788986

Epoch: 251
Loss: 0.5246909260749817
RMSE train: 0.608506	val: 1.011190	test: 1.252605
MAE train: 0.444546	val: 0.706833	test: 0.785829

Epoch: 252
Loss: 0.6416531503200531
RMSE train: 0.645846	val: 1.025526	test: 1.263597
MAE train: 0.475647	val: 0.728514	test: 0.797275

Epoch: 253
Loss: 0.754938393831253
RMSE train: 0.721242	val: 1.042435	test: 1.287489
MAE train: 0.538205	val: 0.747537	test: 0.830332

Epoch: 254
Loss: 0.6406185626983643
RMSE train: 0.714193	val: 1.040335	test: 1.304607
MAE train: 0.537213	val: 0.740526	test: 0.841427

Epoch: 255
Loss: 0.5577459633350372
RMSE train: 0.704141	val: 1.045087	test: 1.317226
MAE train: 0.528402	val: 0.742922	test: 0.849343

Epoch: 256
Loss: 0.6312391459941864
RMSE train: 0.681230	val: 1.066994	test: 1.328424
MAE train: 0.508885	val: 0.752025	test: 0.851223

Epoch: 257
Loss: 0.6080368161201477
RMSE train: 0.644316	val: 1.086774	test: 1.323869
MAE train: 0.479603	val: 0.760555	test: 0.848503

Epoch: 258
Loss: 0.49968625605106354
RMSE train: 0.607175	val: 1.100920	test: 1.328830
MAE train: 0.449732	val: 0.759755	test: 0.838750

Epoch: 259
Loss: 0.702051043510437
RMSE train: 0.595639	val: 1.097277	test: 1.330914
MAE train: 0.435098	val: 0.754061	test: 0.831526

Epoch: 260
Loss: 0.5361873209476471
RMSE train: 0.628206	val: 1.077406	test: 1.330220
MAE train: 0.451854	val: 0.751586	test: 0.835553

Epoch: 261
Loss: 0.6054207384586334
RMSE train: 0.703308	val: 1.088758	test: 1.340201
MAE train: 0.501364	val: 0.771420	test: 0.846068

Epoch: 262
Loss: 0.5038557797670364
RMSE train: 0.788391	val: 1.128914	test: 1.365186
MAE train: 0.569607	val: 0.804012	test: 0.873434

Epoch: 263
Loss: 1.093465805053711
RMSE train: 0.846300	val: 1.161890	test: 1.375955
MAE train: 0.618310	val: 0.824027	test: 0.894507

Epoch: 264
Loss: 0.5656560659408569
RMSE train: 0.771611	val: 1.094597	test: 1.328951
MAE train: 0.568074	val: 0.790317	test: 0.867214

Epoch: 265
Loss: 0.6116370260715485


Epoch: 205
Loss: 0.6242287755012512
RMSE train: 0.611096	val: 1.235768	test: 1.098602
MAE train: 0.447919	val: 0.769293	test: 0.727632

Epoch: 206
Loss: 0.7669336497783661
RMSE train: 0.624938	val: 1.253581	test: 1.106842
MAE train: 0.462120	val: 0.776649	test: 0.738081

Epoch: 207
Loss: 0.5854424238204956
RMSE train: 0.605940	val: 1.240353	test: 1.123313
MAE train: 0.449845	val: 0.765440	test: 0.742063

Epoch: 208
Loss: 0.6874305605888367
RMSE train: 0.584805	val: 1.206967	test: 1.131482
MAE train: 0.428534	val: 0.740670	test: 0.717006

Epoch: 209
Loss: 0.6693315207958221
RMSE train: 0.602534	val: 1.184582	test: 1.152923
MAE train: 0.435464	val: 0.746970	test: 0.712408

Epoch: 210
Loss: 0.6176433265209198
RMSE train: 0.622595	val: 1.222205	test: 1.174646
MAE train: 0.446140	val: 0.773436	test: 0.738055

Epoch: 211
Loss: 0.5438561886548996
RMSE train: 0.667840	val: 1.301553	test: 1.204320
MAE train: 0.483866	val: 0.819158	test: 0.778206

Epoch: 212
Loss: 0.6094538271427155
RMSE train: 0.715305	val: 1.366149	test: 1.225907
MAE train: 0.518896	val: 0.868009	test: 0.813864

Epoch: 213
Loss: 0.5933014899492264
RMSE train: 0.721003	val: 1.373697	test: 1.203146
MAE train: 0.521651	val: 0.874659	test: 0.802414

Epoch: 214
Loss: 0.5746188461780548
RMSE train: 0.672000	val: 1.295421	test: 1.153831
MAE train: 0.487488	val: 0.819143	test: 0.752014

Epoch: 215
Loss: 0.5645893216133118
RMSE train: 0.639013	val: 1.197982	test: 1.132322
MAE train: 0.470145	val: 0.763931	test: 0.723127

Epoch: 216
Loss: 0.6240031272172928
RMSE train: 0.615166	val: 1.168875	test: 1.107748
MAE train: 0.450640	val: 0.737732	test: 0.714216

Epoch: 217
Loss: 0.559472918510437
RMSE train: 0.612492	val: 1.218058	test: 1.093033
MAE train: 0.443996	val: 0.743144	test: 0.703463

Epoch: 218
Loss: 0.7569794654846191
RMSE train: 0.667548	val: 1.305989	test: 1.133166
MAE train: 0.492322	val: 0.805645	test: 0.762599

Epoch: 219
Loss: 0.6350144743919373
RMSE train: 0.686136	val: 1.304491	test: 1.134716
MAE train: 0.518005	val: 0.810619	test: 0.764263

Epoch: 220
Loss: 0.5590839087963104
RMSE train: 0.680860	val: 1.259651	test: 1.108735
MAE train: 0.517799	val: 0.783044	test: 0.734234

Epoch: 221
Loss: 0.6036655008792877
RMSE train: 0.691640	val: 1.251215	test: 1.111585
MAE train: 0.526843	val: 0.788263	test: 0.731743

Epoch: 222
Loss: 0.5797625780105591
RMSE train: 0.678072	val: 1.262647	test: 1.127027
MAE train: 0.515402	val: 0.798571	test: 0.739145

Epoch: 223
Loss: 0.6160222291946411
RMSE train: 0.658929	val: 1.290034	test: 1.141359
MAE train: 0.495630	val: 0.812646	test: 0.759774

Epoch: 224
Loss: 0.6170186400413513
RMSE train: 0.665948	val: 1.319693	test: 1.148909
MAE train: 0.493622	val: 0.832535	test: 0.779559

Epoch: 225
Loss: 0.4916917532682419
RMSE train: 0.690105	val: 1.355517	test: 1.158504
MAE train: 0.505614	val: 0.850265	test: 0.793853

Epoch: 226
Loss: 0.604229211807251
RMSE train: 0.700436	val: 1.379105	test: 1.168768
MAE train: 0.510134	val: 0.854631	test: 0.797794

Epoch: 227
Loss: 0.5979837477207184
RMSE train: 0.702721	val: 1.363550	test: 1.171920
MAE train: 0.516141	val: 0.840081	test: 0.791324

Epoch: 228
Loss: 0.622425377368927
RMSE train: 0.726263	val: 1.349907	test: 1.153036
MAE train: 0.538420	val: 0.836514	test: 0.786885

Epoch: 229
Loss: 0.5744795799255371
RMSE train: 0.773240	val: 1.366526	test: 1.159066
MAE train: 0.578927	val: 0.860157	test: 0.794746

Epoch: 230
Loss: 0.5074696242809296
RMSE train: 0.789464	val: 1.360863	test: 1.163550
MAE train: 0.586575	val: 0.858273	test: 0.782204

Epoch: 231
Loss: 0.5752617716789246
RMSE train: 0.762259	val: 1.309572	test: 1.160431
MAE train: 0.562091	val: 0.827736	test: 0.772570

Early stopping
Best (RMSE):	 train: 0.595756	val: 1.126182	test: 1.126620
Best (MAE):	 train: 0.449183	val: 0.709315	test: 0.711568
All runs completed.
RMSE train: 0.688866	val: 1.033882	test: 1.290867
MAE train: 0.515426	val: 0.752072	test: 0.845249

Epoch: 266
Loss: 0.7122862339019775
RMSE train: 0.603626	val: 0.991445	test: 1.263048
MAE train: 0.454237	val: 0.717502	test: 0.817720

Epoch: 267
Loss: 0.7000059485435486
RMSE train: 0.620776	val: 1.010990	test: 1.271596
MAE train: 0.465342	val: 0.726731	test: 0.824287

Epoch: 268
Loss: 0.5167846530675888
RMSE train: 0.733709	val: 1.099490	test: 1.333857
MAE train: 0.542524	val: 0.776212	test: 0.875512

Epoch: 269
Loss: 0.555239737033844
RMSE train: 0.789231	val: 1.140470	test: 1.363310
MAE train: 0.574325	val: 0.795861	test: 0.894527

Epoch: 270
Loss: 0.6189223527908325
RMSE train: 0.725729	val: 1.079434	test: 1.320152
MAE train: 0.526129	val: 0.760748	test: 0.845744

Epoch: 271
Loss: 0.568712592124939
RMSE train: 0.625789	val: 1.021803	test: 1.277633
MAE train: 0.463937	val: 0.721463	test: 0.809949

Epoch: 272
Loss: 0.5932301580905914
RMSE train: 0.571264	val: 1.012481	test: 1.276083
MAE train: 0.421481	val: 0.708485	test: 0.799116

Epoch: 273
Loss: 0.5201868563890457
RMSE train: 0.566052	val: 1.031085	test: 1.308896
MAE train: 0.424074	val: 0.716126	test: 0.823138

Epoch: 274
Loss: 0.9331646263599396
RMSE train: 0.611978	val: 1.071552	test: 1.369762
MAE train: 0.463516	val: 0.747710	test: 0.881341

Epoch: 275
Loss: 0.6101142019033432
RMSE train: 0.619990	val: 1.094613	test: 1.395192
MAE train: 0.473659	val: 0.758783	test: 0.902218

Epoch: 276
Loss: 0.5795864164829254
RMSE train: 0.569154	val: 1.084684	test: 1.364153
MAE train: 0.424647	val: 0.742679	test: 0.863984

Epoch: 277
Loss: 0.5816341042518616
RMSE train: 0.587985	val: 1.108037	test: 1.345163
MAE train: 0.432134	val: 0.761200	test: 0.852739

Epoch: 278
Loss: 0.5588552355766296
RMSE train: 0.623575	val: 1.123424	test: 1.344112
MAE train: 0.456799	val: 0.770208	test: 0.858638

Epoch: 279
Loss: 0.5063549876213074
RMSE train: 0.621271	val: 1.100876	test: 1.337007
MAE train: 0.461703	val: 0.754963	test: 0.854111

Epoch: 280
Loss: 0.5291427224874496
RMSE train: 0.607951	val: 1.065252	test: 1.335626
MAE train: 0.453397	val: 0.728507	test: 0.851355

Epoch: 281
Loss: 0.5808038413524628
RMSE train: 0.623938	val: 1.052326	test: 1.333011
MAE train: 0.466540	val: 0.721595	test: 0.857954

Epoch: 282
Loss: 0.5320664942264557
RMSE train: 0.627670	val: 1.051137	test: 1.329884
MAE train: 0.471611	val: 0.726624	test: 0.857746

Epoch: 283
Loss: 0.6219159960746765
RMSE train: 0.625777	val: 1.060041	test: 1.334852
MAE train: 0.473104	val: 0.739576	test: 0.859847

Epoch: 284
Loss: 0.5676874965429306
RMSE train: 0.624124	val: 1.060539	test: 1.344033
MAE train: 0.469383	val: 0.747330	test: 0.862446

Epoch: 285
Loss: 0.547679603099823
RMSE train: 0.615739	val: 1.051421	test: 1.359434
MAE train: 0.464182	val: 0.736582	test: 0.875249

Epoch: 286
Loss: 0.5138652622699738
RMSE train: 0.620593	val: 1.045219	test: 1.365663
MAE train: 0.463751	val: 0.728098	test: 0.874464

Epoch: 287
Loss: 0.5124683976173401
RMSE train: 0.623921	val: 1.038931	test: 1.349545
MAE train: 0.465097	val: 0.732409	test: 0.860700

Epoch: 288
Loss: 0.5296369045972824
RMSE train: 0.637453	val: 1.047346	test: 1.341530
MAE train: 0.472534	val: 0.741483	test: 0.852521

Epoch: 289
Loss: 0.749464213848114
RMSE train: 0.648349	val: 1.063006	test: 1.335780
MAE train: 0.480731	val: 0.747745	test: 0.850273

Epoch: 290
Loss: 0.4627419710159302
RMSE train: 0.702238	val: 1.097489	test: 1.338831
MAE train: 0.520363	val: 0.765940	test: 0.862764

Epoch: 291
Loss: 0.48744460940361023
RMSE train: 0.719199	val: 1.097293	test: 1.338230
MAE train: 0.528356	val: 0.762847	test: 0.863024

Epoch: 292
Loss: 0.6051381826400757
RMSE train: 0.680245	val: 1.064305	test: 1.323359
MAE train: 0.499292	val: 0.745729	test: 0.846715

Epoch: 293
Loss: 0.5352323353290558
RMSE train: 0.664944	val: 1.047769	test: 1.326794
MAE train: 0.493619	val: 0.745463	test: 0.850594

Epoch: 294
Loss: 0.6572354584932327
RMSE train: 0.672953	val: 1.045298	test: 1.342436
MAE train: 0.504033	val: 0.751219	test: 0.867140

Epoch: 295
Loss: 0.4793183505535126
RMSE train: 0.673883	val: 1.034074	test: 1.366765
MAE train: 0.510886	val: 0.744890	test: 0.888892

Epoch: 296
Loss: 0.5668944120407104
RMSE train: 0.691286	val: 1.054048	test: 1.386122
MAE train: 0.525666	val: 0.763248	test: 0.901187

Epoch: 297
Loss: 0.514728456735611
RMSE train: 0.692298	val: 1.063639	test: 1.394207
MAE train: 0.518808	val: 0.762753	test: 0.904032

Epoch: 298
Loss: 0.5157046765089035
RMSE train: 0.690364	val: 1.056840	test: 1.371373
MAE train: 0.511707	val: 0.748389	test: 0.885581

Epoch: 299
Loss: 0.43885529041290283
RMSE train: 0.663219	val: 1.035894	test: 1.331269
MAE train: 0.489439	val: 0.725672	test: 0.849344

Epoch: 300
Loss: 0.5835169553756714
RMSE train: 0.656386	val: 1.035617	test: 1.313933
MAE train: 0.483642	val: 0.724679	test: 0.838453

Epoch: 301
Loss: 0.5030050426721573
RMSE train: 0.619607	val: 1.027160	test: 1.304478
MAE train: 0.460741	val: 0.713059	test: 0.829208

Early stopping
Best (RMSE):	 train: 0.603626	val: 0.991445	test: 1.263048
Best (MAE):	 train: 0.454237	val: 0.717502	test: 0.817720
All runs completed.
