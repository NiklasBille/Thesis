>>> Starting run for dataset: bbbp
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running SCAFF configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.6.yml on cuda:0
Running SCAFF configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.7.yml on cuda:1
Running SCAFF configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.8.yml on cuda:2
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.8.yml --runseed 4 --device cuda:2
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.8.yml --runseed 5 --device cuda:2
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.8.yml --runseed 6 --device cuda:2
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.7.yml --runseed 4 --device cuda:1
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.6.yml --runseed 4 --device cuda:0
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.7.yml --runseed 5 --device cuda:1
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.6.yml --runseed 5 --device cuda:0
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.7.yml --runseed 6 --device cuda:1
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.6.yml --runseed 6 --device cuda:0
[10:23:10] [10:23:10] WARNING: not removing hydrogen atom without neighborsWARNING: not removing hydrogen atom without neighbors

[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] [10:23:10] WARNING: not removing hydrogen atom without neighborsWARNING: not removing hydrogen atom without neighbors

[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] [10:23:10] WARNING: not removing hydrogen atom without neighborsWARNING: not removing hydrogen atom without neighbors

[10:23:10] WARNING: not removing hydrogen atom without neighbors[10:23:10] 
WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] [10:23:10] WARNING: not removing hydrogen atom without neighborsWARNING: not removing hydrogen atom without neighbors

[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
[10:23:10] WARNING: not removing hydrogen atom without neighbors
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/bbbp/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/bbbp/scaff/train_prop=0.6/bbbp_scaff_4_26-05_10-23-09  ]
[ Using Seed :  4  ]
[ Using device :  cuda:0  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6481186001078636
0 is invalid
0
Some target is missing!
Missing ratio: 1.000000
Traceback (most recent call last):
  File "molecule_finetune.py", line 263, in <module>
    val_result, val_target, val_pred, val_loss = eval(model, device, val_loader, compute_loss=True)
  File "molecule_finetune.py", line 107, in eval
    return {'ROC': sum(roc_list) / len(roc_list), 'PRC': sum(prc_list) / len(prc_list)}, y_true, y_scores, total_loss/len(loader)
ZeroDivisionError: division by zero
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/bbbp/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/bbbp/scaff/train_prop=0.6/bbbp_scaff_5_26-05_10-23-09  ]
[ Using Seed :  5  ]
[ Using device :  cuda:0  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6107692257925936
0 is invalid
0
Some target is missing!
Missing ratio: 1.000000
Traceback (most recent call last):
  File "molecule_finetune.py", line 263, in <module>
    val_result, val_target, val_pred, val_loss = eval(model, device, val_loader, compute_loss=True)
  File "molecule_finetune.py", line 107, in eval
    return {'ROC': sum(roc_list) / len(roc_list), 'PRC': sum(prc_list) / len(prc_list)}, y_true, y_scores, total_loss/len(loader)
ZeroDivisionError: division by zero
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/bbbp/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/bbbp/scaff/train_prop=0.6/bbbp_scaff_6_26-05_10-23-09  ]
[ Using Seed :  6  ]
[ Using device :  cuda:0  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6297664877888856
0 is invalid
0
Some target is missing!
Missing ratio: 1.000000
Traceback (most recent call last):
  File "molecule_finetune.py", line 263, in <module>
    val_result, val_target, val_pred, val_loss = eval(model, device, val_loader, compute_loss=True)
  File "molecule_finetune.py", line 107, in eval
    return {'ROC': sum(roc_list) / len(roc_list), 'PRC': sum(prc_list) / len(prc_list)}, y_true, y_scores, total_loss/len(loader)
ZeroDivisionError: division by zero
All runs completed.
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/bbbp/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/bbbp/scaff/train_prop=0.7/bbbp_scaff_5_26-05_10-23-09  ]
[ Using Seed :  5  ]
[ Using device :  cuda:1  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6005202981721682
ROC train: 0.806819	val: 0.847264	test: 0.732124
PRC train: 0.939356	val: 0.971223	test: 0.642798

Epoch: 2
Loss: 0.4971936545441338
ROC train: 0.830662	val: 0.838828	test: 0.731900
PRC train: 0.946417	val: 0.974902	test: 0.637307

Epoch: 3
Loss: 0.4214884811521009
ROC train: 0.867936	val: 0.904429	test: 0.763441
PRC train: 0.962472	val: 0.986441	test: 0.683560

Epoch: 4
Loss: 0.36898193744056407
ROC train: 0.874026	val: 0.928405	test: 0.764830
PRC train: 0.964958	val: 0.990259	test: 0.681536

Epoch: 5
Loss: 0.33653216671156577
ROC train: 0.894557	val: 0.937618	test: 0.764875
PRC train: 0.970459	val: 0.991035	test: 0.659915

Epoch: 6
Loss: 0.3085795355605247
ROC train: 0.909340	val: 0.934621	test: 0.759946
PRC train: 0.973955	val: 0.990817	test: 0.661725

Epoch: 7
Loss: 0.2972491943007311
ROC train: 0.919733	val: 0.927406	test: 0.760394
PRC train: 0.976794	val: 0.988516	test: 0.661656

Epoch: 8
Loss: 0.2807374750287565
ROC train: 0.931875	val: 0.915307	test: 0.757168
PRC train: 0.980386	val: 0.983819	test: 0.656196

Epoch: 9
Loss: 0.26861652537523834
ROC train: 0.938585	val: 0.919636	test: 0.768324
PRC train: 0.982701	val: 0.985238	test: 0.673310

Epoch: 10
Loss: 0.2690760815784226
ROC train: 0.942611	val: 0.901876	test: 0.773163
PRC train: 0.984216	val: 0.983634	test: 0.674346

Epoch: 11
Loss: 0.2661073875299818
ROC train: 0.942224	val: 0.917749	test: 0.779749
PRC train: 0.983698	val: 0.986056	test: 0.690993

Epoch: 12
Loss: 0.24379285835630338
ROC train: 0.949656	val: 0.901543	test: 0.774014
PRC train: 0.986298	val: 0.978957	test: 0.701004

Epoch: 13
Loss: 0.24436287144136148
ROC train: 0.954781	val: 0.913420	test: 0.777867
PRC train: 0.987201	val: 0.986824	test: 0.701363

Epoch: 14
Loss: 0.23085016167371064
ROC train: 0.954841	val: 0.904207	test: 0.775851
PRC train: 0.987306	val: 0.985130	test: 0.688175

Epoch: 15
Loss: 0.22733697026187127
ROC train: 0.960387	val: 0.892219	test: 0.783109
PRC train: 0.989372	val: 0.977027	test: 0.708199

Epoch: 16
Loss: 0.21952497115649236
ROC train: 0.962631	val: 0.888223	test: 0.767921
PRC train: 0.990046	val: 0.980415	test: 0.699893

Epoch: 17
Loss: 0.20914298806406517
ROC train: 0.965675	val: 0.907981	test: 0.781676
PRC train: 0.990734	val: 0.985935	test: 0.698772

Epoch: 18
Loss: 0.20815092404363558
ROC train: 0.964592	val: 0.920413	test: 0.784005
PRC train: 0.990619	val: 0.987753	test: 0.688255

Epoch: 19
Loss: 0.20915524092358048
ROC train: 0.969725	val: 0.916528	test: 0.783244
PRC train: 0.992152	val: 0.986072	test: 0.703772

Epoch: 20
Loss: 0.2141314596059067
ROC train: 0.967133	val: 0.925186	test: 0.780690
PRC train: 0.991484	val: 0.988579	test: 0.714912

Epoch: 21
Loss: 0.1988039224136471
ROC train: 0.968535	val: 0.924409	test: 0.777061
PRC train: 0.991759	val: 0.988912	test: 0.713551

Epoch: 22
Loss: 0.1956598074660961
ROC train: 0.973840	val: 0.924187	test: 0.781541
PRC train: 0.993243	val: 0.989556	test: 0.722766

Epoch: 23
Loss: 0.18509943492545342
ROC train: 0.974634	val: 0.926518	test: 0.780152
PRC train: 0.993173	val: 0.990191	test: 0.728232

Epoch: 24
Loss: 0.18406410485131353
ROC train: 0.975714	val: 0.922189	test: 0.783602
PRC train: 0.993746	val: 0.988826	test: 0.711997

Epoch: 25
Loss: 0.19200047630348624
ROC train: 0.976740	val: 0.908203	test: 0.772357
PRC train: 0.994257	val: 0.986590	test: 0.690864

Epoch: 26
Loss: 0.19109175235273426
ROC train: 0.975811	val: 0.918415	test: 0.777688
PRC train: 0.994229	val: 0.986459	test: 0.686280

Epoch: 27
Loss: 0.17414726651304888
ROC train: 0.980438	val: 0.909868	test: 0.782885
PRC train: 0.995315	val: 0.983908	test: 0.695732

Epoch: 28
Loss: 0.17742963709714854
ROC train: 0.980801	val: 0.922189	test: 0.780152
PRC train: 0.995427	val: 0.988198	test: 0.686090

Epoch: 29
Loss: 0.18989308768568894
ROC train: 0.978334	val: 0.943723	test: 0.790591
PRC train: 0.994625	val: 0.992089	test: 0.738935

Epoch: 30
Loss: 0.16299078142640588
ROC train: 0.980581	val: 0.923632	test: 0.778987
PRC train: 0.995315	val: 0.987870	test: 0.715271

Epoch: 31
Loss: 0.16687108228516379
ROC train: 0.982219	val: 0.934066	test: 0.784409
PRC train: 0.995627	val: 0.989457	test: 0.730036

Epoch: 32
Loss: 0.16459837553431642
ROC train: 0.979207	val: 0.934510	test: 0.778539
PRC train: 0.995034	val: 0.990008	test: 0.709315

Epoch: 33
Loss: 0.15630517353731738Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/bbbp/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/bbbp/scaff/train_prop=0.7/bbbp_scaff_6_26-05_10-23-09  ]
[ Using Seed :  6  ]
[ Using device :  cuda:1  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6263894159421259
ROC train: 0.800969	val: 0.849817	test: 0.733333
PRC train: 0.941973	val: 0.975224	test: 0.634549

Epoch: 2
Loss: 0.5117757670819634
ROC train: 0.833098	val: 0.835054	test: 0.726344
PRC train: 0.952206	val: 0.973984	test: 0.650278

Epoch: 3
Loss: 0.42581704765195627
ROC train: 0.860164	val: 0.889666	test: 0.758244
PRC train: 0.960067	val: 0.984099	test: 0.675409

Epoch: 4
Loss: 0.36860048491386904
ROC train: 0.881438	val: 0.925297	test: 0.768369
PRC train: 0.966756	val: 0.989699	test: 0.681405

Epoch: 5
Loss: 0.3351179600730116
ROC train: 0.902197	val: 0.938062	test: 0.758468
PRC train: 0.972324	val: 0.991829	test: 0.669205

Epoch: 6
Loss: 0.3279039724882023
ROC train: 0.917959	val: 0.934288	test: 0.766711
PRC train: 0.976691	val: 0.991049	test: 0.676152

Epoch: 7
Loss: 0.29731114683209553
ROC train: 0.921989	val: 0.932068	test: 0.772849
PRC train: 0.977144	val: 0.990390	test: 0.688738

Epoch: 8
Loss: 0.2828998777045489
ROC train: 0.931038	val: 0.916972	test: 0.770833
PRC train: 0.979373	val: 0.985239	test: 0.685820

Epoch: 9
Loss: 0.27358952654521285
ROC train: 0.936789	val: 0.911089	test: 0.764158
PRC train: 0.980800	val: 0.981771	test: 0.682034

Epoch: 10
Loss: 0.2610544999984747
ROC train: 0.944529	val: 0.894994	test: 0.771371
PRC train: 0.983241	val: 0.973134	test: 0.709851

Epoch: 11
Loss: 0.23719090027071132
ROC train: 0.949071	val: 0.888667	test: 0.773746
PRC train: 0.984862	val: 0.965254	test: 0.710185

Epoch: 12
Loss: 0.2357326474872746
ROC train: 0.946375	val: 0.893773	test: 0.766935
PRC train: 0.985011	val: 0.974816	test: 0.674713

Epoch: 13
Loss: 0.23799292459196453
ROC train: 0.953164	val: 0.888556	test: 0.776792
PRC train: 0.987447	val: 0.971468	test: 0.691264

Epoch: 14
Loss: 0.2319154954516697
ROC train: 0.956418	val: 0.884227	test: 0.790726
PRC train: 0.988400	val: 0.965217	test: 0.744235

Epoch: 15
Loss: 0.22753820232510036
ROC train: 0.955769	val: 0.881119	test: 0.791891
PRC train: 0.987676	val: 0.968472	test: 0.750656

Epoch: 16
Loss: 0.22226811505696464
ROC train: 0.961178	val: 0.891442	test: 0.787186
PRC train: 0.989095	val: 0.974952	test: 0.733864

Epoch: 17
Loss: 0.20605283895968854
ROC train: 0.967415	val: 0.905872	test: 0.782572
PRC train: 0.990698	val: 0.981364	test: 0.719975

Epoch: 18
Loss: 0.2109645589829807
ROC train: 0.964979	val: 0.904096	test: 0.773880
PRC train: 0.990588	val: 0.981399	test: 0.685970

Epoch: 19
Loss: 0.21157172694417034
ROC train: 0.970650	val: 0.893773	test: 0.781676
PRC train: 0.992302	val: 0.973272	test: 0.697441

Epoch: 20
Loss: 0.20119669894058057
ROC train: 0.966260	val: 0.899212	test: 0.787231
PRC train: 0.991462	val: 0.971544	test: 0.711435

Epoch: 21
Loss: 0.2034637409611307
ROC train: 0.972169	val: 0.894217	test: 0.769444
PRC train: 0.992821	val: 0.975184	test: 0.689492

Epoch: 22
Loss: 0.18324829955761557
ROC train: 0.972606	val: 0.905872	test: 0.783020
PRC train: 0.992883	val: 0.980344	test: 0.693638

Epoch: 23
Loss: 0.19882429203350105
ROC train: 0.971246	val: 0.905872	test: 0.791801
PRC train: 0.992724	val: 0.981419	test: 0.733204

Epoch: 24
Loss: 0.19777160253114276
ROC train: 0.977846	val: 0.888112	test: 0.775314
PRC train: 0.994387	val: 0.976973	test: 0.719951

Epoch: 25
Loss: 0.1811417343822549
ROC train: 0.973822	val: 0.888667	test: 0.783468
PRC train: 0.993339	val: 0.977074	test: 0.717845

Epoch: 26
Loss: 0.1857766291237344
ROC train: 0.977938	val: 0.905206	test: 0.786649
PRC train: 0.994517	val: 0.980428	test: 0.714162

Epoch: 27
Loss: 0.17353330228003463
ROC train: 0.978943	val: 0.906982	test: 0.778271
PRC train: 0.994807	val: 0.978739	test: 0.695746

Epoch: 28
Loss: 0.16828855197002737
ROC train: 0.981239	val: 0.900433	test: 0.777733
PRC train: 0.995390	val: 0.976534	test: 0.694750

Epoch: 29
Loss: 0.17896985402438273
ROC train: 0.975839	val: 0.899434	test: 0.779301
PRC train: 0.994077	val: 0.981816	test: 0.705800

Epoch: 30
Loss: 0.18326697357777755
ROC train: 0.979576	val: 0.901432	test: 0.782706
PRC train: 0.994859	val: 0.981624	test: 0.732122

Epoch: 31
Loss: 0.17858536567283814
ROC train: 0.982359	val: 0.912310	test: 0.784319
PRC train: 0.995774	val: 0.978903	test: 0.699682

Epoch: 32
Loss: 0.1840613925320139
ROC train: 0.985192	val: 0.897214	test: 0.767249
PRC train: 0.996542	val: 0.975001	test: 0.665491

Epoch: 33
Loss: 0.17188080630371796Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/bbbp/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/bbbp/scaff/train_prop=0.7/bbbp_scaff_4_26-05_10-23-09  ]
[ Using Seed :  4  ]
[ Using device :  cuda:1  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6451957300045476
ROC train: 0.795193	val: 0.856477	test: 0.724686
PRC train: 0.937539	val: 0.977520	test: 0.639418

Epoch: 2
Loss: 0.5293976704307154
ROC train: 0.846029	val: 0.840382	test: 0.733020
PRC train: 0.955455	val: 0.972947	test: 0.659025

Epoch: 3
Loss: 0.4489219891753011
ROC train: 0.869653	val: 0.871684	test: 0.746192
PRC train: 0.964075	val: 0.975009	test: 0.664958

Epoch: 4
Loss: 0.3839744550825841
ROC train: 0.884591	val: 0.883117	test: 0.758378
PRC train: 0.968001	val: 0.975662	test: 0.661389

Epoch: 5
Loss: 0.3463789136036628
ROC train: 0.901475	val: 0.885559	test: 0.754256
PRC train: 0.972323	val: 0.973652	test: 0.646168

Epoch: 6
Loss: 0.3282234018059307
ROC train: 0.913368	val: 0.896770	test: 0.757527
PRC train: 0.975817	val: 0.977186	test: 0.654605

Epoch: 7
Loss: 0.2971468532230374
ROC train: 0.927295	val: 0.901099	test: 0.757930
PRC train: 0.979905	val: 0.976914	test: 0.662743

Epoch: 8
Loss: 0.29009854716715217
ROC train: 0.935622	val: 0.903319	test: 0.764337
PRC train: 0.981988	val: 0.971885	test: 0.675712

Epoch: 9
Loss: 0.29108992145452833
ROC train: 0.943011	val: 0.893884	test: 0.771998
PRC train: 0.983926	val: 0.962019	test: 0.695752

Epoch: 10
Loss: 0.2621190507300197
ROC train: 0.941858	val: 0.904096	test: 0.763217
PRC train: 0.983952	val: 0.963164	test: 0.681153

Epoch: 11
Loss: 0.24615055239516057
ROC train: 0.950946	val: 0.886225	test: 0.760349
PRC train: 0.986663	val: 0.957814	test: 0.673997

Epoch: 12
Loss: 0.2513647237149163
ROC train: 0.947601	val: 0.889333	test: 0.770789
PRC train: 0.985543	val: 0.957495	test: 0.688562

Epoch: 13
Loss: 0.2294669328330559
ROC train: 0.954169	val: 0.883783	test: 0.759498
PRC train: 0.987363	val: 0.958325	test: 0.679137

Epoch: 14
Loss: 0.2336833937472483
ROC train: 0.956933	val: 0.901765	test: 0.764785
PRC train: 0.988322	val: 0.963234	test: 0.684209

Epoch: 15
Loss: 0.22237738737485213
ROC train: 0.959174	val: 0.902320	test: 0.761111
PRC train: 0.989142	val: 0.976075	test: 0.669691

Epoch: 16
Loss: 0.229507609092527
ROC train: 0.959006	val: 0.917527	test: 0.757885
PRC train: 0.989167	val: 0.984914	test: 0.641567

Epoch: 17
Loss: 0.21356770362017197
ROC train: 0.961603	val: 0.913087	test: 0.758468
PRC train: 0.989522	val: 0.978744	test: 0.641917

Epoch: 18
Loss: 0.21691134514102683
ROC train: 0.966283	val: 0.920857	test: 0.771550
PRC train: 0.990875	val: 0.984361	test: 0.700797

Epoch: 19
Loss: 0.2014179805494057
ROC train: 0.968348	val: 0.925741	test: 0.775358
PRC train: 0.991322	val: 0.987017	test: 0.689835

Epoch: 20
Loss: 0.20277304013644024
ROC train: 0.969946	val: 0.927184	test: 0.770341
PRC train: 0.991912	val: 0.988013	test: 0.667964

Epoch: 21
Loss: 0.19846627513081205
ROC train: 0.970229	val: 0.918415	test: 0.766667
PRC train: 0.992463	val: 0.985369	test: 0.644949

Epoch: 22
Loss: 0.19771731901480796
ROC train: 0.973942	val: 0.910201	test: 0.761604
PRC train: 0.993410	val: 0.984922	test: 0.639010

Epoch: 23
Loss: 0.18230440750551638
ROC train: 0.972105	val: 0.913420	test: 0.770878
PRC train: 0.993120	val: 0.984296	test: 0.656757

Epoch: 24
Loss: 0.18087696534131448
ROC train: 0.976715	val: 0.915529	test: 0.765188
PRC train: 0.994271	val: 0.985198	test: 0.653514

Epoch: 25
Loss: 0.1885297950889435
ROC train: 0.970653	val: 0.933511	test: 0.765905
PRC train: 0.992650	val: 0.989748	test: 0.644437

Epoch: 26
Loss: 0.16888038696622235
ROC train: 0.973349	val: 0.899878	test: 0.746685
PRC train: 0.993512	val: 0.984713	test: 0.618873

Epoch: 27
Loss: 0.18754356931985552
ROC train: 0.977012	val: 0.894106	test: 0.763844
PRC train: 0.994302	val: 0.981314	test: 0.638706

Epoch: 28
Loss: 0.18587968484257653
ROC train: 0.975966	val: 0.901432	test: 0.764113
PRC train: 0.993980	val: 0.981448	test: 0.641195

Epoch: 29
Loss: 0.1910346081810997
ROC train: 0.975629	val: 0.894883	test: 0.768145
PRC train: 0.993989	val: 0.980516	test: 0.681492

Epoch: 30
Loss: 0.18931818413605864
ROC train: 0.978334	val: 0.897436	test: 0.770206
PRC train: 0.994710	val: 0.979801	test: 0.678303

Epoch: 31
Loss: 0.17454326587590163
ROC train: 0.983071	val: 0.894550	test: 0.751927
PRC train: 0.995975	val: 0.978504	test: 0.641898

Epoch: 32
Loss: 0.16255908959323104
ROC train: 0.982045	val: 0.894772	test: 0.749059
PRC train: 0.995663	val: 0.980115	test: 0.637410

Epoch: 33
Loss: 0.15801522089764003Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/bbbp/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/bbbp/scaff/train_prop=0.8/bbbp_scaff_5_26-05_10-23-09  ]
[ Using Seed :  5  ]
[ Using device :  cuda:2  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6023088401857705
ROC train: 0.799293	val: 0.876644	test: 0.615934
PRC train: 0.945432	val: 0.804434	test: 0.668191

Epoch: 2
Loss: 0.4683826191086168
ROC train: 0.844444	val: 0.889290	test: 0.637153
PRC train: 0.960163	val: 0.830658	test: 0.681119

Epoch: 3
Loss: 0.3990918626224002
ROC train: 0.860287	val: 0.908160	test: 0.656636
PRC train: 0.964831	val: 0.833700	test: 0.704068

Epoch: 4
Loss: 0.3453099304284586
ROC train: 0.877881	val: 0.909164	test: 0.638117
PRC train: 0.969635	val: 0.799758	test: 0.684830

Epoch: 5
Loss: 0.3175954232724882
ROC train: 0.893299	val: 0.905651	test: 0.650463
PRC train: 0.973607	val: 0.785056	test: 0.696100

Epoch: 6
Loss: 0.2954812199661023
ROC train: 0.918034	val: 0.892502	test: 0.663194
PRC train: 0.979654	val: 0.749922	test: 0.709641

Epoch: 7
Loss: 0.27832679801060506
ROC train: 0.931719	val: 0.893807	test: 0.670718
PRC train: 0.982700	val: 0.775837	test: 0.726923

Epoch: 8
Loss: 0.25156367512152084
ROC train: 0.936108	val: 0.897521	test: 0.684799
PRC train: 0.983520	val: 0.779170	test: 0.728418

Epoch: 9
Loss: 0.23888342252428815
ROC train: 0.946434	val: 0.891097	test: 0.679591
PRC train: 0.986430	val: 0.773722	test: 0.731477

Epoch: 10
Loss: 0.241859763425401
ROC train: 0.942639	val: 0.888989	test: 0.669367
PRC train: 0.985074	val: 0.763498	test: 0.709670

Epoch: 11
Loss: 0.23514501719890912
ROC train: 0.955340	val: 0.901435	test: 0.679302
PRC train: 0.988761	val: 0.783733	test: 0.717900

Epoch: 12
Loss: 0.22953813519847488
ROC train: 0.953260	val: 0.885777	test: 0.684896
PRC train: 0.987990	val: 0.775450	test: 0.729698

Epoch: 13
Loss: 0.21808715210505042
ROC train: 0.955745	val: 0.886781	test: 0.685185
PRC train: 0.988628	val: 0.792819	test: 0.754124

Epoch: 14
Loss: 0.20242931009006207
ROC train: 0.960615	val: 0.888588	test: 0.694541
PRC train: 0.990447	val: 0.790072	test: 0.765043

Epoch: 15
Loss: 0.2120852766166969
ROC train: 0.959889	val: 0.900632	test: 0.709394
PRC train: 0.990487	val: 0.780694	test: 0.771792

Epoch: 16
Loss: 0.20924041981525796
ROC train: 0.964347	val: 0.880056	test: 0.685571
PRC train: 0.991968	val: 0.745040	test: 0.727904

Epoch: 17
Loss: 0.19267187157447957
ROC train: 0.964629	val: 0.898223	test: 0.684992
PRC train: 0.991588	val: 0.794305	test: 0.713904

Epoch: 18
Loss: 0.18488559028235216
ROC train: 0.966912	val: 0.889591	test: 0.702257
PRC train: 0.992241	val: 0.799934	test: 0.748648

Epoch: 19
Loss: 0.1867877809010508
ROC train: 0.969400	val: 0.886781	test: 0.714120
PRC train: 0.992776	val: 0.785710	test: 0.758004

Epoch: 20
Loss: 0.1847148984133335
ROC train: 0.973224	val: 0.886380	test: 0.697145
PRC train: 0.994360	val: 0.781905	test: 0.720805

Epoch: 21
Loss: 0.1852045231660289
ROC train: 0.970562	val: 0.875841	test: 0.705440
PRC train: 0.993555	val: 0.780928	test: 0.752645

Epoch: 22
Loss: 0.1783798597327849
ROC train: 0.972854	val: 0.879855	test: 0.688368
PRC train: 0.993947	val: 0.795107	test: 0.736365

Epoch: 23
Loss: 0.17235656050802492
ROC train: 0.973097	val: 0.878149	test: 0.692130
PRC train: 0.994002	val: 0.786626	test: 0.740365

Epoch: 24
Loss: 0.17904377901541027
ROC train: 0.972406	val: 0.909064	test: 0.716049
PRC train: 0.993106	val: 0.845052	test: 0.771497

Epoch: 25
Loss: 0.17870462014403793
ROC train: 0.978198	val: 0.900231	test: 0.701678
PRC train: 0.995207	val: 0.828344	test: 0.738641

Epoch: 26
Loss: 0.16751104576153586
ROC train: 0.977338	val: 0.889190	test: 0.710069
PRC train: 0.995069	val: 0.784469	test: 0.725800

Epoch: 27
Loss: 0.16039100410628457
ROC train: 0.976577	val: 0.892502	test: 0.701003
PRC train: 0.994997	val: 0.781054	test: 0.695945

Epoch: 28
Loss: 0.16636797746614249
ROC train: 0.980085	val: 0.888186	test: 0.694444
PRC train: 0.995812	val: 0.796692	test: 0.728974

Epoch: 29
Loss: 0.1609376134680413
ROC train: 0.979322	val: 0.888688	test: 0.699942
PRC train: 0.995684	val: 0.797307	test: 0.742902

Epoch: 30
Loss: 0.15937187397719316
ROC train: 0.980463	val: 0.884372	test: 0.709298
PRC train: 0.995904	val: 0.773862	test: 0.737428

Epoch: 31
Loss: 0.16158780494830385
ROC train: 0.979907	val: 0.908562	test: 0.712481
PRC train: 0.995762	val: 0.818583	test: 0.746218

Epoch: 32
Loss: 0.15813773820237972
ROC train: 0.982228	val: 0.894911	test: 0.697627
PRC train: 0.996219	val: 0.804101	test: 0.724588

Epoch: 33
Loss: 0.15656148902470965Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/bbbp/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/bbbp/scaff/train_prop=0.8/bbbp_scaff_4_26-05_10-23-09  ]
[ Using Seed :  4  ]
[ Using device :  cuda:2  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.632292277412354
ROC train: 0.816487	val: 0.884171	test: 0.633102
PRC train: 0.951424	val: 0.818877	test: 0.692136

Epoch: 2
Loss: 0.5074885992739666
ROC train: 0.854989	val: 0.894108	test: 0.620563
PRC train: 0.963158	val: 0.812850	test: 0.676639

Epoch: 3
Loss: 0.4172052372064393
ROC train: 0.880353	val: 0.902539	test: 0.657793
PRC train: 0.969408	val: 0.808078	test: 0.700203

Epoch: 4
Loss: 0.3638596536013929
ROC train: 0.900118	val: 0.909666	test: 0.666281
PRC train: 0.975716	val: 0.817490	test: 0.714064

Epoch: 5
Loss: 0.31247651586659925
ROC train: 0.907365	val: 0.915889	test: 0.662037
PRC train: 0.977316	val: 0.835005	test: 0.706186

Epoch: 6
Loss: 0.3151110655691222
ROC train: 0.921530	val: 0.915588	test: 0.672068
PRC train: 0.980578	val: 0.838795	test: 0.709736

Epoch: 7
Loss: 0.2850035200852209
ROC train: 0.934620	val: 0.913380	test: 0.676312
PRC train: 0.983647	val: 0.818892	test: 0.719131

Epoch: 8
Loss: 0.2614139177864142
ROC train: 0.941028	val: 0.902640	test: 0.670621
PRC train: 0.985312	val: 0.791216	test: 0.714540

Epoch: 9
Loss: 0.24549707924878192
ROC train: 0.945775	val: 0.896015	test: 0.669464
PRC train: 0.986665	val: 0.760147	test: 0.713309

Epoch: 10
Loss: 0.23486157289326384
ROC train: 0.949325	val: 0.892502	test: 0.687596
PRC train: 0.987771	val: 0.753521	test: 0.728251

Epoch: 11
Loss: 0.23313683947570205
ROC train: 0.949697	val: 0.905952	test: 0.693962
PRC train: 0.987749	val: 0.778011	test: 0.739055

Epoch: 12
Loss: 0.21791828165604454
ROC train: 0.952323	val: 0.899729	test: 0.692033
PRC train: 0.988647	val: 0.772554	test: 0.719699

Epoch: 13
Loss: 0.2100893413944611
ROC train: 0.959749	val: 0.902841	test: 0.694444
PRC train: 0.990257	val: 0.793265	test: 0.728938

Epoch: 14
Loss: 0.21331105288694396
ROC train: 0.963720	val: 0.903643	test: 0.703414
PRC train: 0.991078	val: 0.805511	test: 0.745263

Epoch: 15
Loss: 0.2093771670715015
ROC train: 0.961045	val: 0.893707	test: 0.697242
PRC train: 0.990899	val: 0.787341	test: 0.704573

Epoch: 16
Loss: 0.20637071706563265
ROC train: 0.964535	val: 0.890495	test: 0.696277
PRC train: 0.991598	val: 0.779393	test: 0.738223

Epoch: 17
Loss: 0.2094800859840487
ROC train: 0.961874	val: 0.883670	test: 0.695312
PRC train: 0.990722	val: 0.778425	test: 0.731018

Epoch: 18
Loss: 0.19728276494889238
ROC train: 0.965183	val: 0.888588	test: 0.708140
PRC train: 0.991826	val: 0.781531	test: 0.745814

Epoch: 19
Loss: 0.19308123514808287
ROC train: 0.967741	val: 0.894008	test: 0.710262
PRC train: 0.992498	val: 0.778519	test: 0.739221

Epoch: 20
Loss: 0.1956730880935828
ROC train: 0.969207	val: 0.884874	test: 0.695409
PRC train: 0.992777	val: 0.766724	test: 0.716577

Epoch: 21
Loss: 0.18279840150531476
ROC train: 0.967153	val: 0.895313	test: 0.697917
PRC train: 0.992023	val: 0.793870	test: 0.717766

Epoch: 22
Loss: 0.17705439995733832
ROC train: 0.971806	val: 0.893807	test: 0.687789
PRC train: 0.993187	val: 0.790858	test: 0.707063

Epoch: 23
Loss: 0.17165067276702345
ROC train: 0.973491	val: 0.889591	test: 0.697627
PRC train: 0.993707	val: 0.784948	test: 0.711041

Epoch: 24
Loss: 0.1834042318217291
ROC train: 0.973830	val: 0.894610	test: 0.712963
PRC train: 0.993608	val: 0.807668	test: 0.745669

Epoch: 25
Loss: 0.16754635576051427
ROC train: 0.976887	val: 0.901636	test: 0.711034
PRC train: 0.994509	val: 0.853583	test: 0.761894

Epoch: 26
Loss: 0.16741494036197982
ROC train: 0.978648	val: 0.908160	test: 0.713831
PRC train: 0.995285	val: 0.829191	test: 0.738204

Epoch: 27
Loss: 0.16802062944556226
ROC train: 0.979681	val: 0.900733	test: 0.714120
PRC train: 0.995307	val: 0.818740	test: 0.732059

Epoch: 28
Loss: 0.1609970532298867
ROC train: 0.981618	val: 0.899428	test: 0.716242
PRC train: 0.995842	val: 0.809637	test: 0.741758

Epoch: 29
Loss: 0.16084228083639893
ROC train: 0.982022	val: 0.890394	test: 0.708623
PRC train: 0.996145	val: 0.805052	test: 0.719615

Epoch: 30
Loss: 0.16395028820117533
ROC train: 0.975435	val: 0.890194	test: 0.724441
PRC train: 0.994712	val: 0.802193	test: 0.743850

Epoch: 31
Loss: 0.1622600641457245
ROC train: 0.980145	val: 0.888086	test: 0.709298
PRC train: 0.995739	val: 0.813057	test: 0.726522

Epoch: 32
Loss: 0.1715385776652337
ROC train: 0.978936	val: 0.898324	test: 0.694927
PRC train: 0.995631	val: 0.816358	test: 0.698145

Epoch: 33
Loss: 0.16852025167968435Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/bbbp/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/bbbp/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/bbbp/scaff/train_prop=0.8/bbbp_scaff_6_26-05_10-23-09  ]
[ Using Seed :  6  ]
[ Using device :  cuda:2  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6084008445814657
ROC train: 0.803898	val: 0.865803	test: 0.629147
PRC train: 0.950437	val: 0.796339	test: 0.668508

Epoch: 2
Loss: 0.480563540085141
ROC train: 0.841295	val: 0.880357	test: 0.632234
PRC train: 0.959613	val: 0.822975	test: 0.682190

Epoch: 3
Loss: 0.3928438500855326
ROC train: 0.868799	val: 0.911372	test: 0.638503
PRC train: 0.966684	val: 0.860078	test: 0.689353

Epoch: 4
Loss: 0.3567776712492522
ROC train: 0.891035	val: 0.903844	test: 0.645448
PRC train: 0.972166	val: 0.811091	test: 0.687379

Epoch: 5
Loss: 0.30653199060473424
ROC train: 0.908973	val: 0.904346	test: 0.645833
PRC train: 0.976494	val: 0.816038	test: 0.684151

Epoch: 6
Loss: 0.27997950280846845
ROC train: 0.918189	val: 0.911774	test: 0.657697
PRC train: 0.979020	val: 0.835246	test: 0.693076

Epoch: 7
Loss: 0.27581857194965764
ROC train: 0.932403	val: 0.921911	test: 0.670718
PRC train: 0.982706	val: 0.858214	test: 0.708700

Epoch: 8
Loss: 0.2512675157195941
ROC train: 0.938922	val: 0.903041	test: 0.672840
PRC train: 0.984389	val: 0.834154	test: 0.719083

Epoch: 9
Loss: 0.2587349979149623
ROC train: 0.943922	val: 0.907859	test: 0.692901
PRC train: 0.985710	val: 0.834228	test: 0.739350

Epoch: 10
Loss: 0.2451781509090576
ROC train: 0.952481	val: 0.897521	test: 0.709491
PRC train: 0.989018	val: 0.794315	test: 0.762556

Epoch: 11
Loss: 0.22627331528703126
ROC train: 0.953246	val: 0.879956	test: 0.703318
PRC train: 0.989043	val: 0.802666	test: 0.755361

Epoch: 12
Loss: 0.22190320290098325
ROC train: 0.956668	val: 0.889491	test: 0.716435
PRC train: 0.989665	val: 0.803043	test: 0.762120

Epoch: 13
Loss: 0.21968186068956505
ROC train: 0.956869	val: 0.889190	test: 0.723958
PRC train: 0.989822	val: 0.773564	test: 0.770429

Epoch: 14
Loss: 0.22302663948473694
ROC train: 0.960383	val: 0.877647	test: 0.706019
PRC train: 0.990784	val: 0.767569	test: 0.743269

Epoch: 15
Loss: 0.2043740968720557
ROC train: 0.965400	val: 0.900130	test: 0.710938
PRC train: 0.991568	val: 0.839258	test: 0.762326

Epoch: 16
Loss: 0.1940821340451159
ROC train: 0.964096	val: 0.892803	test: 0.726563
PRC train: 0.991478	val: 0.828463	test: 0.779680

Epoch: 17
Loss: 0.1929474053142616
ROC train: 0.968396	val: 0.894209	test: 0.716242
PRC train: 0.992787	val: 0.827730	test: 0.762197

Epoch: 18
Loss: 0.19227888751807284
ROC train: 0.970071	val: 0.882867	test: 0.724826
PRC train: 0.993102	val: 0.804908	test: 0.765684

Epoch: 19
Loss: 0.19804438130531563
ROC train: 0.964268	val: 0.888688	test: 0.745853
PRC train: 0.991580	val: 0.806945	test: 0.782126

Epoch: 20
Loss: 0.18207947050841625
ROC train: 0.972136	val: 0.872026	test: 0.734279
PRC train: 0.993712	val: 0.792703	test: 0.769361

Epoch: 21
Loss: 0.17578039157209693
ROC train: 0.971594	val: 0.890997	test: 0.732832
PRC train: 0.993607	val: 0.831507	test: 0.778443

Epoch: 22
Loss: 0.18264549612239728
ROC train: 0.974879	val: 0.892502	test: 0.728877
PRC train: 0.994299	val: 0.814877	test: 0.785085

Epoch: 23
Loss: 0.17617660915500638
ROC train: 0.975264	val: 0.882164	test: 0.729167
PRC train: 0.994370	val: 0.792412	test: 0.773202

Epoch: 24
Loss: 0.18363595332945856
ROC train: 0.978378	val: 0.873733	test: 0.713542
PRC train: 0.995236	val: 0.765592	test: 0.753003

Epoch: 25
Loss: 0.1749875741372664
ROC train: 0.976871	val: 0.889290	test: 0.715085
PRC train: 0.994901	val: 0.787218	test: 0.759656

Epoch: 26
Loss: 0.1679605115675093
ROC train: 0.977237	val: 0.886279	test: 0.728974
PRC train: 0.995036	val: 0.797097	test: 0.762370

Epoch: 27
Loss: 0.16405031122013924
ROC train: 0.977804	val: 0.882164	test: 0.730421
PRC train: 0.995180	val: 0.800359	test: 0.783428

Epoch: 28
Loss: 0.16370060491370633
ROC train: 0.977543	val: 0.881060	test: 0.717303
PRC train: 0.995040	val: 0.803550	test: 0.753745

Epoch: 29
Loss: 0.1644301858293398
ROC train: 0.978193	val: 0.889190	test: 0.717496
PRC train: 0.995299	val: 0.808676	test: 0.743112

Epoch: 30
Loss: 0.15176264803383493
ROC train: 0.980801	val: 0.875841	test: 0.717689
PRC train: 0.995921	val: 0.781395	test: 0.741866

Epoch: 31
Loss: 0.14987726327007672
ROC train: 0.983137	val: 0.877848	test: 0.714796
PRC train: 0.996449	val: 0.783365	test: 0.732714

Epoch: 32
Loss: 0.14738472740714026
ROC train: 0.982011	val: 0.886881	test: 0.711323
PRC train: 0.996086	val: 0.790306	test: 0.714303

Epoch: 33
Loss: 0.14946537533346257
ROC train: 0.982328	val: 0.904540	test: 0.769803
PRC train: 0.995686	val: 0.982412	test: 0.660146

Epoch: 34
Loss: 0.17044929383423477
ROC train: 0.983699	val: 0.895105	test: 0.763441
PRC train: 0.996080	val: 0.981225	test: 0.652377

Epoch: 35
Loss: 0.16169353951789014
ROC train: 0.987176	val: 0.892996	test: 0.769579
PRC train: 0.996935	val: 0.980456	test: 0.651326

Epoch: 36
Loss: 0.16637510896010493
ROC train: 0.987054	val: 0.877123	test: 0.768190
PRC train: 0.996892	val: 0.974757	test: 0.659631

Epoch: 37
Loss: 0.15482905478990397
ROC train: 0.988114	val: 0.854479	test: 0.760797
PRC train: 0.997221	val: 0.964152	test: 0.658931

Epoch: 38
Loss: 0.16441731839134985
ROC train: 0.988888	val: 0.872239	test: 0.759857
PRC train: 0.997438	val: 0.967201	test: 0.669607

Epoch: 39
Loss: 0.14778765426974252
ROC train: 0.989955	val: 0.878455	test: 0.761201
PRC train: 0.997669	val: 0.968733	test: 0.669348

Epoch: 40
Loss: 0.17385034208475658
ROC train: 0.988593	val: 0.888001	test: 0.753808
PRC train: 0.997373	val: 0.969532	test: 0.643068

Epoch: 41
Loss: 0.16234385113770897
ROC train: 0.989510	val: 0.878122	test: 0.749597
PRC train: 0.997596	val: 0.968754	test: 0.628253

Epoch: 42
Loss: 0.15041395016765777
ROC train: 0.991133	val: 0.849040	test: 0.746909
PRC train: 0.997961	val: 0.966272	test: 0.627955

Epoch: 43
Loss: 0.14700585650274667
ROC train: 0.989725	val: 0.877012	test: 0.761066
PRC train: 0.997635	val: 0.976487	test: 0.644569

Epoch: 44
Loss: 0.14664866483899175
ROC train: 0.989745	val: 0.858364	test: 0.745296
PRC train: 0.997675	val: 0.971016	test: 0.610720

Epoch: 45
Loss: 0.14644860182413405
ROC train: 0.992466	val: 0.884227	test: 0.769176
PRC train: 0.998288	val: 0.970945	test: 0.654861

Epoch: 46
Loss: 0.1484663725405035
ROC train: 0.990688	val: 0.876901	test: 0.773925
PRC train: 0.997868	val: 0.967924	test: 0.685884

Epoch: 47
Loss: 0.13111281106149078
ROC train: 0.991754	val: 0.848041	test: 0.757751
PRC train: 0.998152	val: 0.965914	test: 0.668840

Epoch: 48
Loss: 0.13923408115559768
ROC train: 0.993977	val: 0.847375	test: 0.757841
PRC train: 0.998643	val: 0.968256	test: 0.639299

Epoch: 49
Loss: 0.12334282427877152
ROC train: 0.993659	val: 0.842824	test: 0.758871
PRC train: 0.998568	val: 0.968306	test: 0.630345

Epoch: 50
Loss: 0.13739932993286016
ROC train: 0.994094	val: 0.856588	test: 0.755108
PRC train: 0.998674	val: 0.971535	test: 0.641806

Epoch: 51
Loss: 0.14482275842630918
ROC train: 0.991378	val: 0.878344	test: 0.763172
PRC train: 0.998040	val: 0.976792	test: 0.638345

Epoch: 52
Loss: 0.14017913818131408
ROC train: 0.992194	val: 0.890776	test: 0.764919
PRC train: 0.998228	val: 0.977294	test: 0.638983

Epoch: 53
Loss: 0.13035959742905392
ROC train: 0.993456	val: 0.897658	test: 0.768190
PRC train: 0.998529	val: 0.979146	test: 0.653257

Epoch: 54
Loss: 0.12276975229596121
ROC train: 0.992799	val: 0.902098	test: 0.758289
PRC train: 0.998379	val: 0.981407	test: 0.656355

Epoch: 55
Loss: 0.13612718456452713
ROC train: 0.991906	val: 0.907426	test: 0.757841
PRC train: 0.998119	val: 0.981848	test: 0.678689

Epoch: 56
Loss: 0.1239629025047489
ROC train: 0.993173	val: 0.904540	test: 0.759050
PRC train: 0.998471	val: 0.980571	test: 0.658735

Epoch: 57
Loss: 0.130240840740573
ROC train: 0.992827	val: 0.895882	test: 0.763844
PRC train: 0.998371	val: 0.979630	test: 0.681884

Epoch: 58
Loss: 0.1141601783543202
ROC train: 0.994424	val: 0.878011	test: 0.757258
PRC train: 0.998734	val: 0.972214	test: 0.681663

Epoch: 59
Loss: 0.14156343832007723
ROC train: 0.994443	val: 0.843046	test: 0.738530
PRC train: 0.998752	val: 0.960254	test: 0.627963

Epoch: 60
Loss: 0.11969817004974359
ROC train: 0.994658	val: 0.861805	test: 0.749328
PRC train: 0.998803	val: 0.964336	test: 0.652554

Epoch: 61
Loss: 0.11606043203581505
ROC train: 0.993659	val: 0.868909	test: 0.759229
PRC train: 0.998542	val: 0.967291	test: 0.669557

Epoch: 62
Loss: 0.12223931912455359
ROC train: 0.995580	val: 0.821290	test: 0.740591
PRC train: 0.999014	val: 0.959683	test: 0.647343

Epoch: 63
Loss: 0.10663950359114899
ROC train: 0.994875	val: 0.875791	test: 0.751703
PRC train: 0.998835	val: 0.976933	test: 0.631685

Epoch: 64
Loss: 0.11755873545642768
ROC train: 0.996561	val: 0.871240	test: 0.750403
PRC train: 0.999224	val: 0.974440	test: 0.630583

Epoch: 65
Loss: 0.11806135513375972
ROC train: 0.994376	val: 0.877345	test: 0.754973
PRC train: 0.998737	val: 0.974581	test: 0.643050

Epoch: 66
Loss: 0.12417176734225865
ROC train: 0.995131	val: 0.879787	test: 0.754122
PRC train: 0.998892	val: 0.976416	test: 0.665988

Epoch: 67
Loss: 0.11302894815817814
ROC train: 0.995636	val: 0.881230	test: 0.758557
PRC train: 0.998992	val: 0.967707	test: 0.683776

Epoch: 68
Loss: 0.11204018091430128
ROC train: 0.995895	val: 0.853591	test: 0.738396
PRC train: 0.999085	val: 0.963798	test: 0.638164

Epoch: 69
Loss: 0.10093097317141103
ROC train: 0.996050	val: 0.841270	test: 0.737410
PRC train: 0.999109	val: 0.965778	test: 0.630627

Epoch: 70
Loss: 0.11075261158233551
ROC train: 0.996614	val: 0.875569	test: 0.752912
PRC train: 0.999241	val: 0.976984	test: 0.650754

Epoch: 71
Loss: 0.11345199843189592
ROC train: 0.997161	val: 0.891331	test: 0.762142
PRC train: 0.999365	val: 0.977045	test: 0.647731

Epoch: 72
Loss: 0.11152983831404344
ROC train: 0.997291	val: 0.880564	test: 0.753629
PRC train: 0.999399	val: 0.972185	test: 0.645989

Epoch: 73
Loss: 0.09869857689856736
ROC train: 0.996015	val: 0.862915	test: 0.747177
PRC train: 0.999116	val: 0.970834	test: 0.626193

Epoch: 74
Loss: 0.10216437527490473
ROC train: 0.997054	val: 0.870130	test: 0.749194
PRC train: 0.999340	val: 0.970134	test: 0.625606

Epoch: 75
Loss: 0.09544358890848154
ROC train: 0.997052	val: 0.868243	test: 0.748566
PRC train: 0.999345	val: 0.969540	test: 0.631788

Epoch: 76
Loss: 0.11737720204857251
ROC train: 0.997118	val: 0.849595	test: 0.753853
PRC train: 0.999362	val: 0.968560	test: 0.632073

Epoch: 77
Loss: 0.10214680669451108
ROC train: 0.996053	val: 0.844711	test: 0.741577
PRC train: 0.999119	val: 0.971614	test: 0.616433

Epoch: 78
Loss: 0.10428527245129326
ROC train: 0.996502	val: 0.875569	test: 0.743996
PRC train: 0.999223	val: 0.976881	test: 0.628823

Epoch: 79
Loss: 0.10746332387556634
ROC train: 0.997185	val: 0.885892	test: 0.758244
PRC train: 0.999370	val: 0.975812	test: 0.650149

Epoch: 80
Loss: 0.1031829837835553
ROC train: 0.995772	val: 0.861139	test: 0.752330
PRC train: 0.999052	val: 0.968976	test: 0.637664

Epoch: 81
Loss: 0.10600105102563512
ROC train: 0.997225	val: 0.856366	test: 0.756810
PRC train: 0.999383	val: 0.960805	test: 0.647638

Epoch: 82
Loss: 0.10317525316908932
ROC train: 0.997681	val: 0.864802	test: 0.761066
PRC train: 0.999480	val: 0.959675	test: 0.662299

Epoch: 83
Loss: 0.09879970375868073
ROC train: 0.995620	val: 0.821068	test: 0.731541
PRC train: 0.999008	val: 0.955872	test: 0.624867

Epoch: 84
Loss: 0.11634517262564985
ROC train: 0.997131	val: 0.854479	test: 0.747805
PRC train: 0.999360	val: 0.968061	test: 0.624817

Epoch: 85
Loss: 0.09798834515437777
ROC train: 0.997540	val: 0.818737	test: 0.740188
PRC train: 0.999455	val: 0.960266	test: 0.604258

Epoch: 86
Loss: 0.09325076098453912
ROC train: 0.997228	val: 0.815629	test: 0.751658
PRC train: 0.999384	val: 0.963573	test: 0.623566

Epoch: 87
Loss: 0.09495712693284807
ROC train: 0.997045	val: 0.831391	test: 0.753271
PRC train: 0.999340	val: 0.968611	test: 0.632343

Epoch: 88
Loss: 0.10043389540997223
ROC train: 0.997976	val: 0.830170	test: 0.743548
PRC train: 0.999549	val: 0.964248	test: 0.628822

Epoch: 89
Loss: 0.09424945112992045
ROC train: 0.997091	val: 0.831835	test: 0.732572
PRC train: 0.999343	val: 0.958272	test: 0.626904

Epoch: 90
Loss: 0.09502262513416028
ROC train: 0.997260	val: 0.817405	test: 0.724507
PRC train: 0.999386	val: 0.953929	test: 0.620370

Epoch: 91
Loss: 0.08948916079048345
ROC train: 0.997764	val: 0.840493	test: 0.751344
PRC train: 0.999500	val: 0.962732	test: 0.658755

Epoch: 92
Loss: 0.09038387798902948
ROC train: 0.998194	val: 0.866134	test: 0.759364
PRC train: 0.999599	val: 0.967793	test: 0.645834

Epoch: 93
Loss: 0.08216204501010879
ROC train: 0.997815	val: 0.866467	test: 0.753898
PRC train: 0.999513	val: 0.967737	test: 0.639564

ROC train: 0.986100	val: 0.883117	test: 0.750627
PRC train: 0.996766	val: 0.972145	test: 0.649112

Epoch: 34
Loss: 0.1756762238410975
ROC train: 0.984721	val: 0.904984	test: 0.773790
PRC train: 0.996437	val: 0.977249	test: 0.678759

Epoch: 35
Loss: 0.1605396369779173
ROC train: 0.985909	val: 0.906649	test: 0.770744
PRC train: 0.996711	val: 0.985094	test: 0.678544

Epoch: 36
Loss: 0.16764060173954218
ROC train: 0.985466	val: 0.903763	test: 0.777823
PRC train: 0.996636	val: 0.982560	test: 0.704654

Epoch: 37
Loss: 0.15386012096752952
ROC train: 0.985787	val: 0.911533	test: 0.784050
PRC train: 0.996688	val: 0.980112	test: 0.718367

Epoch: 38
Loss: 0.15776116464529172
ROC train: 0.988455	val: 0.906427	test: 0.771192
PRC train: 0.997350	val: 0.977905	test: 0.695107

Epoch: 39
Loss: 0.15394023768530918
ROC train: 0.986697	val: 0.903874	test: 0.770923
PRC train: 0.996948	val: 0.981081	test: 0.693417

Epoch: 40
Loss: 0.15075754753362205
ROC train: 0.989228	val: 0.910867	test: 0.781048
PRC train: 0.997515	val: 0.983932	test: 0.728130

Epoch: 41
Loss: 0.15946525313579038
ROC train: 0.988308	val: 0.916972	test: 0.781541
PRC train: 0.997286	val: 0.984492	test: 0.727037

Epoch: 42
Loss: 0.13178737676559837
ROC train: 0.989831	val: 0.918970	test: 0.781362
PRC train: 0.997624	val: 0.981654	test: 0.714312

Epoch: 43
Loss: 0.1432478016030439
ROC train: 0.988534	val: 0.919636	test: 0.777778
PRC train: 0.997315	val: 0.984944	test: 0.695053

Epoch: 44
Loss: 0.1345077065016729
ROC train: 0.991113	val: 0.919636	test: 0.774552
PRC train: 0.997963	val: 0.984251	test: 0.700771

Epoch: 45
Loss: 0.13471094948628173
ROC train: 0.989534	val: 0.924298	test: 0.784453
PRC train: 0.997578	val: 0.983177	test: 0.701353

Epoch: 46
Loss: 0.139385668512627
ROC train: 0.989437	val: 0.916084	test: 0.788351
PRC train: 0.997524	val: 0.979502	test: 0.722062

Epoch: 47
Loss: 0.1349932250569774
ROC train: 0.991599	val: 0.894661	test: 0.776927
PRC train: 0.998077	val: 0.970922	test: 0.725519

Epoch: 48
Loss: 0.14571309948315922
ROC train: 0.992393	val: 0.905872	test: 0.771595
PRC train: 0.998275	val: 0.969242	test: 0.697053

Epoch: 49
Loss: 0.14405243648380686
ROC train: 0.992655	val: 0.905650	test: 0.781452
PRC train: 0.998348	val: 0.968552	test: 0.717474

Epoch: 50
Loss: 0.12991428531719104
ROC train: 0.992321	val: 0.892774	test: 0.768100
PRC train: 0.998254	val: 0.971424	test: 0.698998

Epoch: 51
Loss: 0.13149513878133134
ROC train: 0.991469	val: 0.905650	test: 0.770027
PRC train: 0.998061	val: 0.980268	test: 0.695748

Epoch: 52
Loss: 0.12231281820582467
ROC train: 0.992690	val: 0.912421	test: 0.782706
PRC train: 0.998312	val: 0.975734	test: 0.715642

Epoch: 53
Loss: 0.12157772155768037
ROC train: 0.992753	val: 0.908092	test: 0.768324
PRC train: 0.998329	val: 0.974165	test: 0.700301

Epoch: 54
Loss: 0.1255643693102443
ROC train: 0.990088	val: 0.891109	test: 0.760529
PRC train: 0.997673	val: 0.976946	test: 0.677644

Epoch: 55
Loss: 0.11740372992682568
ROC train: 0.987964	val: 0.883561	test: 0.767428
PRC train: 0.997167	val: 0.974483	test: 0.687704

Epoch: 56
Loss: 0.1238603639244904
ROC train: 0.992463	val: 0.882340	test: 0.773208
PRC train: 0.998274	val: 0.968752	test: 0.700097

Epoch: 57
Loss: 0.12523482946986167
ROC train: 0.995490	val: 0.895882	test: 0.775806
PRC train: 0.998978	val: 0.974437	test: 0.712537

Epoch: 58
Loss: 0.1086515647282902
ROC train: 0.994649	val: 0.904318	test: 0.779749
PRC train: 0.998799	val: 0.977000	test: 0.714091

Epoch: 59
Loss: 0.12410945853948696
ROC train: 0.994755	val: 0.901987	test: 0.774507
PRC train: 0.998826	val: 0.975097	test: 0.708075

Epoch: 60
Loss: 0.11797256979060232
ROC train: 0.995224	val: 0.903430	test: 0.780869
PRC train: 0.998929	val: 0.977406	test: 0.718384

Epoch: 61
Loss: 0.13015131718290435
ROC train: 0.995506	val: 0.893884	test: 0.781093
PRC train: 0.998995	val: 0.974956	test: 0.711221

Epoch: 62
Loss: 0.12503943895551647
ROC train: 0.994063	val: 0.889444	test: 0.781765
PRC train: 0.998631	val: 0.972265	test: 0.720453

Epoch: 63
Loss: 0.10848536707594918
ROC train: 0.994135	val: 0.894217	test: 0.767563
PRC train: 0.998655	val: 0.972011	test: 0.702325

Epoch: 64
Loss: 0.12833852349859395
ROC train: 0.996078	val: 0.898657	test: 0.752957
PRC train: 0.999123	val: 0.969427	test: 0.674090

Epoch: 65
Loss: 0.11675444287414143
ROC train: 0.996437	val: 0.902209	test: 0.762948
PRC train: 0.999207	val: 0.973224	test: 0.674336

Epoch: 66
Loss: 0.09990866278825894
ROC train: 0.994906	val: 0.908314	test: 0.770520
PRC train: 0.998854	val: 0.982398	test: 0.668208

Epoch: 67
Loss: 0.1080488031327868
ROC train: 0.995157	val: 0.903763	test: 0.773701
PRC train: 0.998918	val: 0.979041	test: 0.675190

Epoch: 68
Loss: 0.1054097188947553
ROC train: 0.997274	val: 0.896548	test: 0.770520
PRC train: 0.999395	val: 0.969923	test: 0.683846

Epoch: 69
Loss: 0.09928994996341782
ROC train: 0.997073	val: 0.897325	test: 0.772536
PRC train: 0.999348	val: 0.968756	test: 0.693709

Epoch: 70
Loss: 0.11114712385178109
ROC train: 0.996918	val: 0.902653	test: 0.768593
PRC train: 0.999306	val: 0.971872	test: 0.694154

Epoch: 71
Loss: 0.10134713741171918
ROC train: 0.997012	val: 0.903763	test: 0.769803
PRC train: 0.999333	val: 0.974854	test: 0.675695

Epoch: 72
Loss: 0.10464053722017247
ROC train: 0.996516	val: 0.904096	test: 0.772357
PRC train: 0.999220	val: 0.975473	test: 0.688583

Epoch: 73
Loss: 0.1157684363418527
ROC train: 0.997665	val: 0.894661	test: 0.757392
PRC train: 0.999481	val: 0.969752	test: 0.667658

Epoch: 74
Loss: 0.11120963449042208
ROC train: 0.997123	val: 0.888556	test: 0.749821
PRC train: 0.999364	val: 0.967594	test: 0.643350

Epoch: 75
Loss: 0.11573200219660464
ROC train: 0.997124	val: 0.878344	test: 0.746057
PRC train: 0.999361	val: 0.970466	test: 0.632884

Epoch: 76
Loss: 0.08972824111965107
ROC train: 0.997261	val: 0.894772	test: 0.765412
PRC train: 0.999395	val: 0.975810	test: 0.663041

Epoch: 77
Loss: 0.10358581004615379
ROC train: 0.997354	val: 0.909313	test: 0.783289
PRC train: 0.999410	val: 0.978309	test: 0.701996

Epoch: 78
Loss: 0.11677474800620918
ROC train: 0.997752	val: 0.900433	test: 0.766219
PRC train: 0.999499	val: 0.979785	test: 0.688875

Epoch: 79
Loss: 0.10318400802067605
ROC train: 0.995510	val: 0.906871	test: 0.761873
PRC train: 0.999017	val: 0.986138	test: 0.667924

Epoch: 80
Loss: 0.10827040960060845
ROC train: 0.997105	val: 0.902320	test: 0.773656
PRC train: 0.999349	val: 0.977026	test: 0.678097

Epoch: 81
Loss: 0.09841559530766325
ROC train: 0.996940	val: 0.895438	test: 0.775941
PRC train: 0.999313	val: 0.970610	test: 0.707934

Epoch: 82
Loss: 0.09693254847693938
ROC train: 0.997550	val: 0.868465	test: 0.761156
PRC train: 0.999457	val: 0.969935	test: 0.689162

Epoch: 83
Loss: 0.09728583457792273
ROC train: 0.996974	val: 0.878566	test: 0.760797
PRC train: 0.999327	val: 0.974333	test: 0.670254

Epoch: 84
Loss: 0.11037221977606643
ROC train: 0.997243	val: 0.888556	test: 0.763710
PRC train: 0.999389	val: 0.971585	test: 0.676681

Epoch: 85
Loss: 0.0968593371680216
ROC train: 0.997940	val: 0.884338	test: 0.754525
PRC train: 0.999535	val: 0.969462	test: 0.670847

Epoch: 86
Loss: 0.09431220309632637
ROC train: 0.998097	val: 0.869575	test: 0.756900
PRC train: 0.999574	val: 0.967033	test: 0.665195

Epoch: 87
Loss: 0.09059456295452006
ROC train: 0.997863	val: 0.871684	test: 0.755242
PRC train: 0.999521	val: 0.966891	test: 0.663472

Epoch: 88
Loss: 0.10791104862799604
ROC train: 0.998387	val: 0.890998	test: 0.760887
PRC train: 0.999643	val: 0.968847	test: 0.670502

Epoch: 89
Loss: 0.08638827790405901
ROC train: 0.998708	val: 0.892996	test: 0.755376
PRC train: 0.999712	val: 0.966987	test: 0.657657

Epoch: 90
Loss: 0.09038849010419887
ROC train: 0.998522	val: 0.893884	test: 0.763306
PRC train: 0.999671	val: 0.968244	test: 0.671164

Epoch: 91
Loss: 0.08260633550698067
ROC train: 0.998495	val: 0.890998	test: 0.769982
PRC train: 0.999664	val: 0.972181	test: 0.693336

Epoch: 92
Loss: 0.0911032734577165
ROC train: 0.998608	val: 0.897880	test: 0.766263
PRC train: 0.999689	val: 0.976813	test: 0.674730

Epoch: 93
Loss: 0.0842032572403187
ROC train: 0.998430	val: 0.904984	test: 0.763844
PRC train: 0.999651	val: 0.977771	test: 0.666125

Epoch: 94
Loss: 0.08830990854339284
ROC train: 0.982794	val: 0.941836	test: 0.774821
PRC train: 0.995871	val: 0.991825	test: 0.679480

Epoch: 34
Loss: 0.15668575302272147
ROC train: 0.983183	val: 0.940504	test: 0.777016
PRC train: 0.995976	val: 0.991566	test: 0.698850

Epoch: 35
Loss: 0.17222943844436586
ROC train: 0.980300	val: 0.932623	test: 0.774059
PRC train: 0.995379	val: 0.989368	test: 0.683056

Epoch: 36
Loss: 0.15480787682100458
ROC train: 0.987033	val: 0.906538	test: 0.782079
PRC train: 0.997001	val: 0.981731	test: 0.699224

Epoch: 37
Loss: 0.16564561930306898
ROC train: 0.988017	val: 0.904540	test: 0.779794
PRC train: 0.997234	val: 0.981363	test: 0.700485

Epoch: 38
Loss: 0.16339816841142837
ROC train: 0.988066	val: 0.903985	test: 0.776344
PRC train: 0.997269	val: 0.981045	test: 0.691495

Epoch: 39
Loss: 0.1787679286698435
ROC train: 0.989136	val: 0.900433	test: 0.784364
PRC train: 0.997548	val: 0.976283	test: 0.713306

Epoch: 40
Loss: 0.1541430383539317
ROC train: 0.985415	val: 0.927850	test: 0.790278
PRC train: 0.996612	val: 0.989204	test: 0.720654

Epoch: 41
Loss: 0.15868561060582187
ROC train: 0.988203	val: 0.915085	test: 0.775806
PRC train: 0.997307	val: 0.986420	test: 0.696815

Epoch: 42
Loss: 0.14325057370520394
ROC train: 0.988844	val: 0.911200	test: 0.779794
PRC train: 0.997449	val: 0.983972	test: 0.671467

Epoch: 43
Loss: 0.15113658075308292
ROC train: 0.987962	val: 0.910867	test: 0.774014
PRC train: 0.997223	val: 0.985739	test: 0.649501

Epoch: 44
Loss: 0.1434153570624389
ROC train: 0.989014	val: 0.916750	test: 0.773073
PRC train: 0.997474	val: 0.986967	test: 0.657103

Epoch: 45
Loss: 0.14003830589929078
ROC train: 0.990831	val: 0.921634	test: 0.777195
PRC train: 0.997913	val: 0.987657	test: 0.650461

Epoch: 46
Loss: 0.14668460611041914
ROC train: 0.990948	val: 0.929515	test: 0.772894
PRC train: 0.997936	val: 0.989520	test: 0.666693

Epoch: 47
Loss: 0.1406789616333128
ROC train: 0.991298	val: 0.925519	test: 0.780376
PRC train: 0.998010	val: 0.987886	test: 0.682050

Epoch: 48
Loss: 0.1492723316614151
ROC train: 0.991841	val: 0.907981	test: 0.780869
PRC train: 0.998157	val: 0.981151	test: 0.696245

Epoch: 49
Loss: 0.1352214748250333
ROC train: 0.992347	val: 0.906982	test: 0.786111
PRC train: 0.998280	val: 0.979795	test: 0.712361

Epoch: 50
Loss: 0.1382238746310319
ROC train: 0.991723	val: 0.910201	test: 0.780063
PRC train: 0.998162	val: 0.982682	test: 0.715031

Epoch: 51
Loss: 0.1447298869267631
ROC train: 0.993097	val: 0.911977	test: 0.779973
PRC train: 0.998457	val: 0.983036	test: 0.702901

Epoch: 52
Loss: 0.14654829391291915
ROC train: 0.993380	val: 0.920968	test: 0.787769
PRC train: 0.998528	val: 0.986726	test: 0.702603

Epoch: 53
Loss: 0.13057364402222024
ROC train: 0.992850	val: 0.925297	test: 0.784095
PRC train: 0.998404	val: 0.988325	test: 0.699192

Epoch: 54
Loss: 0.11804115544411774
ROC train: 0.994259	val: 0.914641	test: 0.772043
PRC train: 0.998720	val: 0.985080	test: 0.679857

Epoch: 55
Loss: 0.12984691035808046
ROC train: 0.994158	val: 0.910756	test: 0.769982
PRC train: 0.998702	val: 0.983994	test: 0.673427

Epoch: 56
Loss: 0.12234731680203322
ROC train: 0.994516	val: 0.910534	test: 0.775806
PRC train: 0.998787	val: 0.985038	test: 0.687397

Epoch: 57
Loss: 0.1274731625204582
ROC train: 0.993680	val: 0.904984	test: 0.775806
PRC train: 0.998584	val: 0.982155	test: 0.704692

Epoch: 58
Loss: 0.11648819710079221
ROC train: 0.993934	val: 0.899434	test: 0.772894
PRC train: 0.998630	val: 0.975655	test: 0.696968

Epoch: 59
Loss: 0.11871461500198459
ROC train: 0.992926	val: 0.895105	test: 0.767608
PRC train: 0.998411	val: 0.975900	test: 0.670316

Epoch: 60
Loss: 0.1085928199309869
ROC train: 0.994687	val: 0.888445	test: 0.766487
PRC train: 0.998816	val: 0.975819	test: 0.687144

Epoch: 61
Loss: 0.11819004922626768
ROC train: 0.995318	val: 0.893773	test: 0.770161
PRC train: 0.998964	val: 0.975364	test: 0.695157

Epoch: 62
Loss: 0.10749381414563781
ROC train: 0.996464	val: 0.906649	test: 0.779346
PRC train: 0.999217	val: 0.977935	test: 0.712958

Epoch: 63
Loss: 0.12043360240701101
ROC train: 0.996427	val: 0.918082	test: 0.776837
PRC train: 0.999202	val: 0.985067	test: 0.687920

Epoch: 64
Loss: 0.11182668979330335
ROC train: 0.996811	val: 0.914863	test: 0.768593
PRC train: 0.999289	val: 0.983579	test: 0.669485

Epoch: 65
Loss: 0.10292324120206621
ROC train: 0.996963	val: 0.898546	test: 0.758737
PRC train: 0.999330	val: 0.973480	test: 0.649583

Epoch: 66
Loss: 0.11424601360089348
ROC train: 0.996414	val: 0.896992	test: 0.758961
PRC train: 0.999209	val: 0.974325	test: 0.649645

Epoch: 67
Loss: 0.1129342654483918
ROC train: 0.996043	val: 0.912421	test: 0.769176
PRC train: 0.999117	val: 0.982860	test: 0.649673

Epoch: 68
Loss: 0.11895454214766633
ROC train: 0.995060	val: 0.917527	test: 0.770296
PRC train: 0.998905	val: 0.986469	test: 0.648730

Epoch: 69
Loss: 0.1058595875908454
ROC train: 0.996594	val: 0.916528	test: 0.762679
PRC train: 0.999247	val: 0.986316	test: 0.644724

Epoch: 70
Loss: 0.1256237075633554
ROC train: 0.996019	val: 0.911644	test: 0.776210
PRC train: 0.999110	val: 0.983628	test: 0.676638

Epoch: 71
Loss: 0.10683591427161633
ROC train: 0.994054	val: 0.900988	test: 0.765815
PRC train: 0.998650	val: 0.977601	test: 0.672803

Epoch: 72
Loss: 0.11622632721815516
ROC train: 0.996933	val: 0.903985	test: 0.776613
PRC train: 0.999306	val: 0.979755	test: 0.679786

Epoch: 73
Loss: 0.11406454449477345
ROC train: 0.996281	val: 0.893662	test: 0.751837
PRC train: 0.999168	val: 0.982154	test: 0.622225

Epoch: 74
Loss: 0.09986073447669168
ROC train: 0.996172	val: 0.888223	test: 0.741846
PRC train: 0.999147	val: 0.981262	test: 0.621836

Epoch: 75
Loss: 0.10591465532224398
ROC train: 0.997362	val: 0.900433	test: 0.763396
PRC train: 0.999410	val: 0.981215	test: 0.660031

Epoch: 76
Loss: 0.09645017300884536
ROC train: 0.997843	val: 0.901210	test: 0.760842
PRC train: 0.999516	val: 0.982708	test: 0.651550

Epoch: 77
Loss: 0.09374361462017351
ROC train: 0.997502	val: 0.898435	test: 0.764023
PRC train: 0.999443	val: 0.983281	test: 0.638012

Epoch: 78
Loss: 0.09038950587958568
ROC train: 0.997648	val: 0.897436	test: 0.769624
PRC train: 0.999473	val: 0.977772	test: 0.664887

Epoch: 79
Loss: 0.09323465741976615
ROC train: 0.997882	val: 0.894106	test: 0.761514
PRC train: 0.999527	val: 0.973647	test: 0.653469

Epoch: 80
Loss: 0.09471957388777003
ROC train: 0.997995	val: 0.895660	test: 0.760573
PRC train: 0.999553	val: 0.974922	test: 0.635465

Epoch: 81
Loss: 0.09823415110645368
ROC train: 0.997622	val: 0.905317	test: 0.765726
PRC train: 0.999468	val: 0.978453	test: 0.649031

Epoch: 82
Loss: 0.09103749199243273
ROC train: 0.998135	val: 0.909535	test: 0.760215
PRC train: 0.999586	val: 0.979217	test: 0.648192

Epoch: 83
Loss: 0.09190301719429443
ROC train: 0.998243	val: 0.903541	test: 0.753002
PRC train: 0.999608	val: 0.978900	test: 0.622046

Epoch: 84
Loss: 0.09983238316503351
ROC train: 0.998112	val: 0.898102	test: 0.750000
PRC train: 0.999582	val: 0.978512	test: 0.625051

Epoch: 85
Loss: 0.09766777341535808
ROC train: 0.998169	val: 0.897103	test: 0.757796
PRC train: 0.999593	val: 0.977368	test: 0.640956

Epoch: 86
Loss: 0.10119054082295979
ROC train: 0.998229	val: 0.896659	test: 0.766219
PRC train: 0.999604	val: 0.977829	test: 0.650942

Epoch: 87
Loss: 0.09281326174057809
ROC train: 0.998322	val: 0.892219	test: 0.756944
PRC train: 0.999626	val: 0.979852	test: 0.625960

Epoch: 88
Loss: 0.08087077218876905
ROC train: 0.997925	val: 0.888889	test: 0.756720
PRC train: 0.999539	val: 0.982218	test: 0.637497

Epoch: 89
Loss: 0.09256922867541495
ROC train: 0.997258	val: 0.904540	test: 0.770296
PRC train: 0.999388	val: 0.986460	test: 0.665551

Epoch: 90
Loss: 0.109046275662749
ROC train: 0.997981	val: 0.892108	test: 0.760081
PRC train: 0.999550	val: 0.980347	test: 0.656924

Epoch: 91
Loss: 0.0942933947018829
ROC train: 0.998619	val: 0.896326	test: 0.745116
PRC train: 0.999690	val: 0.978790	test: 0.630293

Epoch: 92
Loss: 0.08363772272598803
ROC train: 0.998716	val: 0.906982	test: 0.756183
PRC train: 0.999713	val: 0.981357	test: 0.641073

Epoch: 93
Loss: 0.08627317335654837
ROC train: 0.998667	val: 0.919969	test: 0.759050
PRC train: 0.999705	val: 0.986731	test: 0.648736

Epoch: 94
Loss: 0.08623309109478289
ROC train: 0.976003	val: 0.877447	test: 0.707562
PRC train: 0.994839	val: 0.784651	test: 0.740790

Epoch: 34
Loss: 0.14807666637175582
ROC train: 0.983338	val: 0.874435	test: 0.715181
PRC train: 0.996511	val: 0.774954	test: 0.753202

Epoch: 35
Loss: 0.15195570831297966
ROC train: 0.984527	val: 0.882666	test: 0.711613
PRC train: 0.996742	val: 0.789586	test: 0.766645

Epoch: 36
Loss: 0.15374132441557142
ROC train: 0.984127	val: 0.866707	test: 0.711130
PRC train: 0.996510	val: 0.777995	test: 0.769796

Epoch: 37
Loss: 0.14232846280616457
ROC train: 0.983129	val: 0.873331	test: 0.709394
PRC train: 0.996358	val: 0.757883	test: 0.748164

Epoch: 38
Loss: 0.14535633928877076
ROC train: 0.984137	val: 0.863896	test: 0.707369
PRC train: 0.996773	val: 0.780201	test: 0.757712

Epoch: 39
Loss: 0.16150095477819254
ROC train: 0.986147	val: 0.885276	test: 0.713349
PRC train: 0.997203	val: 0.827195	test: 0.775570

Epoch: 40
Loss: 0.16267677388256832
ROC train: 0.986095	val: 0.893104	test: 0.726466
PRC train: 0.997224	val: 0.839797	test: 0.788056

Epoch: 41
Loss: 0.14653928230208849
ROC train: 0.983740	val: 0.864499	test: 0.723573
PRC train: 0.996782	val: 0.803852	test: 0.777522

Epoch: 42
Loss: 0.13871214123853542
ROC train: 0.987224	val: 0.878751	test: 0.707755
PRC train: 0.997448	val: 0.816784	test: 0.739303

Epoch: 43
Loss: 0.13880722518928126
ROC train: 0.988769	val: 0.866004	test: 0.712481
PRC train: 0.997785	val: 0.806154	test: 0.735918

Epoch: 44
Loss: 0.13787051200196743
ROC train: 0.986095	val: 0.867510	test: 0.708816
PRC train: 0.997258	val: 0.804898	test: 0.735680

Epoch: 45
Loss: 0.1322558951995412
ROC train: 0.990664	val: 0.870922	test: 0.714699
PRC train: 0.998170	val: 0.800754	test: 0.734188

Epoch: 46
Loss: 0.135424586250011
ROC train: 0.990173	val: 0.863796	test: 0.716532
PRC train: 0.998068	val: 0.798554	test: 0.728355

Epoch: 47
Loss: 0.13317432339935667
ROC train: 0.990622	val: 0.865502	test: 0.715374
PRC train: 0.998147	val: 0.803521	test: 0.729233

Epoch: 48
Loss: 0.14005543402309234
ROC train: 0.991167	val: 0.848439	test: 0.716049
PRC train: 0.998280	val: 0.759497	test: 0.731754

Epoch: 49
Loss: 0.1316181518710511
ROC train: 0.990609	val: 0.888086	test: 0.710841
PRC train: 0.998169	val: 0.826310	test: 0.742211

Epoch: 50
Loss: 0.12490987724168777
ROC train: 0.988934	val: 0.876342	test: 0.696663
PRC train: 0.997831	val: 0.815725	test: 0.719693

Epoch: 51
Loss: 0.14072733612138924
ROC train: 0.992759	val: 0.868012	test: 0.696470
PRC train: 0.998587	val: 0.794007	test: 0.704063

Epoch: 52
Loss: 0.12597512385593926
ROC train: 0.992344	val: 0.882365	test: 0.716435
PRC train: 0.998522	val: 0.813893	test: 0.734038

Epoch: 53
Loss: 0.12326841963574282
ROC train: 0.991998	val: 0.861186	test: 0.708816
PRC train: 0.998443	val: 0.788774	test: 0.728732

Epoch: 54
Loss: 0.12399184879773809
ROC train: 0.990435	val: 0.857874	test: 0.706597
PRC train: 0.998140	val: 0.780677	test: 0.733433

Epoch: 55
Loss: 0.11770908377418872
ROC train: 0.993340	val: 0.859681	test: 0.705440
PRC train: 0.998723	val: 0.767889	test: 0.745933

Epoch: 56
Loss: 0.11542003861247573
ROC train: 0.993680	val: 0.840209	test: 0.693576
PRC train: 0.998789	val: 0.730959	test: 0.713727

Epoch: 57
Loss: 0.11016342635359223
ROC train: 0.992793	val: 0.846733	test: 0.698785
PRC train: 0.998610	val: 0.731147	test: 0.702607

Epoch: 58
Loss: 0.11791868027527531
ROC train: 0.993100	val: 0.870521	test: 0.715953
PRC train: 0.998659	val: 0.739051	test: 0.716424

Epoch: 59
Loss: 0.11861811735987181
ROC train: 0.994292	val: 0.826960	test: 0.690394
PRC train: 0.998897	val: 0.702580	test: 0.703327

Epoch: 60
Loss: 0.11739083656283301
ROC train: 0.993271	val: 0.855164	test: 0.705536
PRC train: 0.998707	val: 0.749640	test: 0.713350

Epoch: 61
Loss: 0.12055344147215297
ROC train: 0.991860	val: 0.855967	test: 0.710938
PRC train: 0.998414	val: 0.763108	test: 0.722508

Epoch: 62
Loss: 0.11529118589698088
ROC train: 0.994849	val: 0.852856	test: 0.705729
PRC train: 0.999014	val: 0.735450	test: 0.700634

Epoch: 63
Loss: 0.11506357825573477
ROC train: 0.994141	val: 0.860785	test: 0.700328
PRC train: 0.998880	val: 0.730097	test: 0.690006

Epoch: 64
Loss: 0.12148703759475583
ROC train: 0.994729	val: 0.869116	test: 0.717014
PRC train: 0.998988	val: 0.754557	test: 0.726162

Epoch: 65
Loss: 0.11748174046599955
ROC train: 0.993553	val: 0.859480	test: 0.712095
PRC train: 0.998763	val: 0.749848	test: 0.736338

Epoch: 66
Loss: 0.10949634385246586
ROC train: 0.994678	val: 0.867510	test: 0.711323
PRC train: 0.998974	val: 0.747992	test: 0.742382

Epoch: 67
Loss: 0.11438698456307253
ROC train: 0.993908	val: 0.846733	test: 0.716917
PRC train: 0.998825	val: 0.710530	test: 0.733396

Epoch: 68
Loss: 0.11495677907604218
ROC train: 0.995093	val: 0.847837	test: 0.721065
PRC train: 0.999057	val: 0.709532	test: 0.729614

Epoch: 69
Loss: 0.10867887773507645
ROC train: 0.994896	val: 0.863997	test: 0.741127
PRC train: 0.999013	val: 0.747939	test: 0.742432

Epoch: 70
Loss: 0.11660376566588711
ROC train: 0.996110	val: 0.862893	test: 0.720583
PRC train: 0.999260	val: 0.754372	test: 0.729091

Epoch: 71
Loss: 0.09879206756891439
ROC train: 0.995269	val: 0.860986	test: 0.717785
PRC train: 0.999102	val: 0.756578	test: 0.720362

Epoch: 72
Loss: 0.11162711768449969
ROC train: 0.996430	val: 0.860685	test: 0.718268
PRC train: 0.999325	val: 0.762370	test: 0.756454

Epoch: 73
Loss: 0.10453949310950991
ROC train: 0.996370	val: 0.845227	test: 0.710648
PRC train: 0.999320	val: 0.721734	test: 0.742158

Epoch: 74
Loss: 0.11511955202910418
ROC train: 0.996639	val: 0.851149	test: 0.701871
PRC train: 0.999370	val: 0.717626	test: 0.732588

Epoch: 75
Loss: 0.10842451185181545
ROC train: 0.995977	val: 0.845830	test: 0.697917
PRC train: 0.999237	val: 0.709523	test: 0.713498

Epoch: 76
Loss: 0.09943970444838794
ROC train: 0.996033	val: 0.829871	test: 0.689333
PRC train: 0.999249	val: 0.674859	test: 0.692610

Epoch: 77
Loss: 0.08972035423860213
ROC train: 0.996848	val: 0.862190	test: 0.714699
PRC train: 0.999400	val: 0.727764	test: 0.728778

Epoch: 78
Loss: 0.08802485806224888
ROC train: 0.996928	val: 0.876242	test: 0.724633
PRC train: 0.999413	val: 0.761569	test: 0.733344

Epoch: 79
Loss: 0.0914672447075433
ROC train: 0.996946	val: 0.863896	test: 0.717593
PRC train: 0.999419	val: 0.745135	test: 0.717086

Epoch: 80
Loss: 0.09288097078519517
ROC train: 0.996976	val: 0.863093	test: 0.693287
PRC train: 0.999427	val: 0.743090	test: 0.700074

Epoch: 81
Loss: 0.10198152732902144
ROC train: 0.997371	val: 0.869718	test: 0.704765
PRC train: 0.999503	val: 0.759059	test: 0.719937

Epoch: 82
Loss: 0.10435457092987213
ROC train: 0.996692	val: 0.865803	test: 0.724537
PRC train: 0.999370	val: 0.755917	test: 0.731675

Epoch: 83
Loss: 0.09377235323972667
ROC train: 0.997206	val: 0.868714	test: 0.714699
PRC train: 0.999471	val: 0.764436	test: 0.719001

Epoch: 84
Loss: 0.10014962449011029
ROC train: 0.997303	val: 0.851751	test: 0.694348
PRC train: 0.999489	val: 0.739943	test: 0.708394

Epoch: 85
Loss: 0.09728217668198737
ROC train: 0.996074	val: 0.857071	test: 0.717110
PRC train: 0.999257	val: 0.750422	test: 0.732161

Epoch: 86
Loss: 0.09446738135666742
ROC train: 0.996056	val: 0.852253	test: 0.715181
PRC train: 0.999251	val: 0.736593	test: 0.718092

Epoch: 87
Loss: 0.09036938444078234
ROC train: 0.997629	val: 0.868112	test: 0.698206
PRC train: 0.999552	val: 0.752625	test: 0.704074

Epoch: 88
Loss: 0.08456801865543288
ROC train: 0.995841	val: 0.870621	test: 0.708333
PRC train: 0.999198	val: 0.768043	test: 0.737716

Epoch: 89
Loss: 0.10021394647798611
ROC train: 0.996580	val: 0.873231	test: 0.699846
PRC train: 0.999342	val: 0.756616	test: 0.723498

Epoch: 90
Loss: 0.09664300031940234
ROC train: 0.997578	val: 0.860082	test: 0.698399
PRC train: 0.999536	val: 0.714956	test: 0.711061

Epoch: 91
Loss: 0.09236218705112671
ROC train: 0.997836	val: 0.857673	test: 0.698785
PRC train: 0.999588	val: 0.749688	test: 0.712970

Epoch: 92
Loss: 0.09153399125817799
ROC train: 0.997611	val: 0.853759	test: 0.712963
PRC train: 0.999547	val: 0.758416	test: 0.731709

Epoch: 93
Loss: 0.09173814997711334
ROC train: 0.997777	val: 0.863194	test: 0.713927
PRC train: 0.999580	val: 0.768685	test: 0.740526
ROC train: 0.984081	val: 0.894811	test: 0.708816
PRC train: 0.996789	val: 0.817856	test: 0.727378

Epoch: 34
Loss: 0.16593682176054952
ROC train: 0.983357	val: 0.898826	test: 0.715664
PRC train: 0.996560	val: 0.847522	test: 0.762629

Epoch: 35
Loss: 0.15078877896644818
ROC train: 0.982824	val: 0.906855	test: 0.722126
PRC train: 0.996487	val: 0.837766	test: 0.754202

Epoch: 36
Loss: 0.1458227045440347
ROC train: 0.984165	val: 0.892101	test: 0.703704
PRC train: 0.996867	val: 0.773468	test: 0.714453

Epoch: 37
Loss: 0.15194816978109266
ROC train: 0.986303	val: 0.883368	test: 0.684414
PRC train: 0.997277	val: 0.797486	test: 0.719574

Epoch: 38
Loss: 0.14853272264577597
ROC train: 0.984789	val: 0.881261	test: 0.664448
PRC train: 0.996976	val: 0.799248	test: 0.698019

Epoch: 39
Loss: 0.15095241008089122
ROC train: 0.988344	val: 0.897621	test: 0.701003
PRC train: 0.997657	val: 0.825174	test: 0.746234

Epoch: 40
Loss: 0.138749665107374
ROC train: 0.989514	val: 0.898525	test: 0.710069
PRC train: 0.997895	val: 0.828223	test: 0.756188

Epoch: 41
Loss: 0.1311497853456039
ROC train: 0.988791	val: 0.901034	test: 0.700810
PRC train: 0.997763	val: 0.814784	test: 0.727535

Epoch: 42
Loss: 0.13900334987849028
ROC train: 0.990378	val: 0.902439	test: 0.694734
PRC train: 0.998107	val: 0.799178	test: 0.720846

Epoch: 43
Loss: 0.1333724694540657
ROC train: 0.990324	val: 0.900733	test: 0.700810
PRC train: 0.998114	val: 0.811362	test: 0.726777

Epoch: 44
Loss: 0.14379449328382893
ROC train: 0.989351	val: 0.888387	test: 0.701389
PRC train: 0.997922	val: 0.789990	test: 0.725043

Epoch: 45
Loss: 0.13624068649863263
ROC train: 0.990782	val: 0.892803	test: 0.700810
PRC train: 0.998197	val: 0.789890	test: 0.728902

Epoch: 46
Loss: 0.13368855106431157
ROC train: 0.989167	val: 0.864599	test: 0.670621
PRC train: 0.997867	val: 0.756702	test: 0.690747

Epoch: 47
Loss: 0.13700414340062786
ROC train: 0.988634	val: 0.876041	test: 0.655961
PRC train: 0.997755	val: 0.785217	test: 0.664789

Epoch: 48
Loss: 0.1276539575013654
ROC train: 0.991460	val: 0.895012	test: 0.676698
PRC train: 0.998340	val: 0.806650	test: 0.707261

Epoch: 49
Loss: 0.15151033622199656
ROC train: 0.991172	val: 0.883870	test: 0.686053
PRC train: 0.998290	val: 0.798360	test: 0.716187

Epoch: 50
Loss: 0.12467965413252746
ROC train: 0.989205	val: 0.888186	test: 0.668210
PRC train: 0.997890	val: 0.789972	test: 0.685004

Epoch: 51
Loss: 0.13771763815526147
ROC train: 0.991961	val: 0.892302	test: 0.682002
PRC train: 0.998423	val: 0.785338	test: 0.702531

Epoch: 52
Loss: 0.1285375076314774
ROC train: 0.993543	val: 0.881662	test: 0.693480
PRC train: 0.998730	val: 0.781236	test: 0.721607

Epoch: 53
Loss: 0.12128884114478793
ROC train: 0.993301	val: 0.888487	test: 0.690586
PRC train: 0.998686	val: 0.798035	test: 0.713675

Epoch: 54
Loss: 0.11096934895591579
ROC train: 0.994119	val: 0.889893	test: 0.677662
PRC train: 0.998858	val: 0.802796	test: 0.700051

Epoch: 55
Loss: 0.10722927096592387
ROC train: 0.994434	val: 0.887584	test: 0.672840
PRC train: 0.998927	val: 0.800975	test: 0.685761

Epoch: 56
Loss: 0.10760864651189368
ROC train: 0.994684	val: 0.883870	test: 0.678048
PRC train: 0.998973	val: 0.778251	test: 0.693120

Epoch: 57
Loss: 0.11626052506802277
ROC train: 0.995490	val: 0.884573	test: 0.667052
PRC train: 0.999136	val: 0.772083	test: 0.668497

Epoch: 58
Loss: 0.10784405066892593
ROC train: 0.995538	val: 0.882766	test: 0.670910
PRC train: 0.999142	val: 0.785598	test: 0.686594

Epoch: 59
Loss: 0.12293876175949682
ROC train: 0.993601	val: 0.897320	test: 0.694734
PRC train: 0.998750	val: 0.803129	test: 0.696102

Epoch: 60
Loss: 0.10426767166808734
ROC train: 0.994588	val: 0.887785	test: 0.679205
PRC train: 0.998942	val: 0.795933	test: 0.680092

Epoch: 61
Loss: 0.11020603140909399
ROC train: 0.995124	val: 0.875038	test: 0.674286
PRC train: 0.999046	val: 0.764749	test: 0.674866

Epoch: 62
Loss: 0.10917406502299427
ROC train: 0.995751	val: 0.877848	test: 0.693480
PRC train: 0.999173	val: 0.750959	test: 0.700509

Epoch: 63
Loss: 0.1152633423504247
ROC train: 0.996103	val: 0.875439	test: 0.690586
PRC train: 0.999247	val: 0.736913	test: 0.704300

Epoch: 64
Loss: 0.12448002793986458
ROC train: 0.994227	val: 0.855365	test: 0.652199
PRC train: 0.998882	val: 0.714207	test: 0.646088

Epoch: 65
Loss: 0.10797676087640226
ROC train: 0.994105	val: 0.881963	test: 0.667438
PRC train: 0.998836	val: 0.756974	test: 0.673977

Epoch: 66
Loss: 0.10805716661304705
ROC train: 0.995568	val: 0.905450	test: 0.683353
PRC train: 0.999131	val: 0.805576	test: 0.698201

Epoch: 67
Loss: 0.10767154560078311
ROC train: 0.996091	val: 0.893004	test: 0.677373
PRC train: 0.999248	val: 0.794363	test: 0.683157

Epoch: 68
Loss: 0.10139845423498957
ROC train: 0.996869	val: 0.890595	test: 0.687982
PRC train: 0.999403	val: 0.796809	test: 0.693805

Epoch: 69
Loss: 0.09795781357672668
ROC train: 0.996584	val: 0.886881	test: 0.681520
PRC train: 0.999344	val: 0.779257	test: 0.697630

Epoch: 70
Loss: 0.09605295191976593
ROC train: 0.996246	val: 0.886179	test: 0.669271
PRC train: 0.999275	val: 0.785150	test: 0.684061

Epoch: 71
Loss: 0.10585139927724499
ROC train: 0.996441	val: 0.889190	test: 0.682581
PRC train: 0.999321	val: 0.792525	test: 0.698109

Epoch: 72
Loss: 0.09464054898316399
ROC train: 0.995956	val: 0.886580	test: 0.660687
PRC train: 0.999227	val: 0.777614	test: 0.672323

Epoch: 73
Loss: 0.10832787226550804
ROC train: 0.996827	val: 0.885276	test: 0.659626
PRC train: 0.999400	val: 0.786297	test: 0.665470

Epoch: 74
Loss: 0.08383015450427692
ROC train: 0.996613	val: 0.891900	test: 0.677662
PRC train: 0.999353	val: 0.780636	test: 0.681865

Epoch: 75
Loss: 0.09724234234669167
ROC train: 0.997425	val: 0.886179	test: 0.662905
PRC train: 0.999510	val: 0.765220	test: 0.671571

Epoch: 76
Loss: 0.09611462066488459
ROC train: 0.997608	val: 0.885577	test: 0.667824
PRC train: 0.999544	val: 0.764649	test: 0.686278

Epoch: 77
Loss: 0.09307204052848846
ROC train: 0.997059	val: 0.895513	test: 0.663580
PRC train: 0.999440	val: 0.799934	test: 0.693225

Epoch: 78
Loss: 0.10517693513613276
ROC train: 0.997275	val: 0.899829	test: 0.674961
PRC train: 0.999477	val: 0.803017	test: 0.699558

Epoch: 79
Loss: 0.09654138725435275
ROC train: 0.997505	val: 0.885978	test: 0.671682
PRC train: 0.999524	val: 0.790928	test: 0.686779

Epoch: 80
Loss: 0.0915419635708995
ROC train: 0.995619	val: 0.864800	test: 0.655093
PRC train: 0.999119	val: 0.757446	test: 0.661261

Epoch: 81
Loss: 0.09394587217584435
ROC train: 0.996359	val: 0.884272	test: 0.666667
PRC train: 0.999256	val: 0.794805	test: 0.680246

Epoch: 82
Loss: 0.08759985071301828
ROC train: 0.997904	val: 0.898525	test: 0.679109
PRC train: 0.999600	val: 0.819539	test: 0.696534

Epoch: 83
Loss: 0.0982775077346749
ROC train: 0.996345	val: 0.886681	test: 0.652199
PRC train: 0.999301	val: 0.793080	test: 0.647982

Epoch: 84
Loss: 0.09328204539486504
ROC train: 0.997725	val: 0.879855	test: 0.647666
PRC train: 0.999568	val: 0.777449	test: 0.653714

Epoch: 85
Loss: 0.07888211922866027
ROC train: 0.997600	val: 0.890194	test: 0.654225
PRC train: 0.999540	val: 0.792882	test: 0.667536

Epoch: 86
Loss: 0.09954941039292615
ROC train: 0.996671	val: 0.880960	test: 0.638214
PRC train: 0.999365	val: 0.784633	test: 0.630976

Epoch: 87
Loss: 0.08815415454581088
ROC train: 0.998143	val: 0.893707	test: 0.665220
PRC train: 0.999648	val: 0.792946	test: 0.673907

Epoch: 88
Loss: 0.0898497362058437
ROC train: 0.998045	val: 0.877246	test: 0.667245
PRC train: 0.999628	val: 0.761200	test: 0.677407

Epoch: 89
Loss: 0.07365619205106125
ROC train: 0.997669	val: 0.853357	test: 0.645158
PRC train: 0.999557	val: 0.736459	test: 0.639042

Epoch: 90
Loss: 0.08148924524141751
ROC train: 0.998060	val: 0.865001	test: 0.649016
PRC train: 0.999632	val: 0.741098	test: 0.646628

Epoch: 91
Loss: 0.08715584464578874
ROC train: 0.997754	val: 0.872629	test: 0.675347
PRC train: 0.999576	val: 0.744939	test: 0.691665

Epoch: 92
Loss: 0.10514658326061967
ROC train: 0.997838	val: 0.857473	test: 0.680556
PRC train: 0.999586	val: 0.721345	test: 0.714184

Epoch: 93
Loss: 0.08292714359266687
ROC train: 0.997704	val: 0.857272	test: 0.658565
PRC train: 0.999566	val: 0.718434	test: 0.671127

Epoch: 94
Loss: 0.08803163421311964
ROC train: 0.984176	val: 0.893305	test: 0.723765
PRC train: 0.996529	val: 0.807491	test: 0.760706

Epoch: 34
Loss: 0.15648738210947138
ROC train: 0.983420	val: 0.885978	test: 0.725309
PRC train: 0.996403	val: 0.808786	test: 0.777743

Epoch: 35
Loss: 0.14885888436976144
ROC train: 0.985599	val: 0.858777	test: 0.718557
PRC train: 0.996987	val: 0.763116	test: 0.755744

Epoch: 36
Loss: 0.15778341848811261
ROC train: 0.987460	val: 0.865904	test: 0.718750
PRC train: 0.997389	val: 0.757156	test: 0.762417

Epoch: 37
Loss: 0.14343489270511764
ROC train: 0.986437	val: 0.845127	test: 0.712191
PRC train: 0.997191	val: 0.734478	test: 0.753318

Epoch: 38
Loss: 0.16510661636550533
ROC train: 0.987447	val: 0.873331	test: 0.722994
PRC train: 0.997384	val: 0.781832	test: 0.762633

Epoch: 39
Loss: 0.142956590148365
ROC train: 0.988828	val: 0.869316	test: 0.716917
PRC train: 0.997760	val: 0.769614	test: 0.734274

Epoch: 40
Loss: 0.14810497207176704
ROC train: 0.989354	val: 0.876744	test: 0.728588
PRC train: 0.997838	val: 0.804498	test: 0.780703

Epoch: 41
Loss: 0.13939105119449682
ROC train: 0.990045	val: 0.877246	test: 0.720679
PRC train: 0.998033	val: 0.801840	test: 0.763055

Epoch: 42
Loss: 0.13495484481381817
ROC train: 0.990584	val: 0.874335	test: 0.716532
PRC train: 0.998150	val: 0.793435	test: 0.729847

Epoch: 43
Loss: 0.138100446810181
ROC train: 0.989870	val: 0.863495	test: 0.712191
PRC train: 0.997993	val: 0.776830	test: 0.752414

Epoch: 44
Loss: 0.13223789274863754
ROC train: 0.989195	val: 0.860584	test: 0.707079
PRC train: 0.997834	val: 0.768638	test: 0.733883

Epoch: 45
Loss: 0.1329743999324539
ROC train: 0.988657	val: 0.865402	test: 0.710745
PRC train: 0.997725	val: 0.771861	test: 0.742748

Epoch: 46
Loss: 0.1197264831628475
ROC train: 0.991198	val: 0.860082	test: 0.716725
PRC train: 0.998260	val: 0.760267	test: 0.765422

Epoch: 47
Loss: 0.13171472777895638
ROC train: 0.992015	val: 0.864398	test: 0.724730
PRC train: 0.998426	val: 0.752374	test: 0.759727

Epoch: 48
Loss: 0.14508775638409624
ROC train: 0.992135	val: 0.857874	test: 0.713831
PRC train: 0.998471	val: 0.735364	test: 0.737483

Epoch: 49
Loss: 0.123092132773436
ROC train: 0.992261	val: 0.871725	test: 0.697242
PRC train: 0.998475	val: 0.753123	test: 0.720452

Epoch: 50
Loss: 0.13585879342247048
ROC train: 0.993269	val: 0.858777	test: 0.703029
PRC train: 0.998682	val: 0.747039	test: 0.726757

Epoch: 51
Loss: 0.12755512190754673
ROC train: 0.991546	val: 0.863696	test: 0.709394
PRC train: 0.998346	val: 0.753356	test: 0.731384

Epoch: 52
Loss: 0.10867669764982056
ROC train: 0.992602	val: 0.877547	test: 0.711613
PRC train: 0.998547	val: 0.780237	test: 0.729671

Epoch: 53
Loss: 0.12464607248556067
ROC train: 0.992744	val: 0.867209	test: 0.692226
PRC train: 0.998576	val: 0.765873	test: 0.704589

Epoch: 54
Loss: 0.1183428399491091
ROC train: 0.993466	val: 0.859279	test: 0.703221
PRC train: 0.998732	val: 0.747740	test: 0.723979

Epoch: 55
Loss: 0.12770909442435577
ROC train: 0.994012	val: 0.863294	test: 0.702064
PRC train: 0.998824	val: 0.758036	test: 0.721314

Epoch: 56
Loss: 0.10955153889633147
ROC train: 0.994214	val: 0.857172	test: 0.695216
PRC train: 0.998856	val: 0.757352	test: 0.714110

Epoch: 57
Loss: 0.10308773193057275
ROC train: 0.993096	val: 0.854361	test: 0.699556
PRC train: 0.998644	val: 0.751629	test: 0.735056

Epoch: 58
Loss: 0.10602453822176629
ROC train: 0.993795	val: 0.861086	test: 0.693962
PRC train: 0.998790	val: 0.742832	test: 0.707476

Epoch: 59
Loss: 0.11473652483667943
ROC train: 0.994858	val: 0.866707	test: 0.686246
PRC train: 0.998996	val: 0.738416	test: 0.692590

Epoch: 60
Loss: 0.101861411016303
ROC train: 0.995980	val: 0.873131	test: 0.697820
PRC train: 0.999223	val: 0.764690	test: 0.718728

Epoch: 61
Loss: 0.10588688933270965
ROC train: 0.995718	val: 0.868614	test: 0.683160
PRC train: 0.999173	val: 0.764625	test: 0.703997

Epoch: 62
Loss: 0.11011845243905352
ROC train: 0.995914	val: 0.866406	test: 0.702353
PRC train: 0.999213	val: 0.764452	test: 0.715223

Epoch: 63
Loss: 0.10519573316600428
ROC train: 0.995417	val: 0.858677	test: 0.703511
PRC train: 0.999123	val: 0.743573	test: 0.720814

Epoch: 64
Loss: 0.10051292674891689
ROC train: 0.994490	val: 0.828365	test: 0.690104
PRC train: 0.998937	val: 0.700572	test: 0.709214

Epoch: 65
Loss: 0.09591883841359512
ROC train: 0.996279	val: 0.855064	test: 0.685089
PRC train: 0.999290	val: 0.743862	test: 0.718403

Epoch: 66
Loss: 0.1082844604677763
ROC train: 0.995988	val: 0.863294	test: 0.705826
PRC train: 0.999229	val: 0.752434	test: 0.732209

Epoch: 67
Loss: 0.113714582047501
ROC train: 0.996747	val: 0.868212	test: 0.711998
PRC train: 0.999382	val: 0.781924	test: 0.723772

Epoch: 68
Loss: 0.10468458919516942
ROC train: 0.996998	val: 0.879655	test: 0.701389
PRC train: 0.999425	val: 0.803086	test: 0.721673

Epoch: 69
Loss: 0.11715804027949324
ROC train: 0.995980	val: 0.857874	test: 0.695216
PRC train: 0.999231	val: 0.745210	test: 0.703832

Epoch: 70
Loss: 0.10651805297771112
ROC train: 0.995013	val: 0.843220	test: 0.692323
PRC train: 0.999042	val: 0.717789	test: 0.707353

Epoch: 71
Loss: 0.09883214414277086
ROC train: 0.997200	val: 0.866606	test: 0.711709
PRC train: 0.999467	val: 0.777523	test: 0.750690

Epoch: 72
Loss: 0.11070560252314467
ROC train: 0.997322	val: 0.859982	test: 0.712481
PRC train: 0.999490	val: 0.776866	test: 0.754562

Epoch: 73
Loss: 0.09421793799980462
ROC train: 0.997625	val: 0.859982	test: 0.718846
PRC train: 0.999551	val: 0.749895	test: 0.742828

Epoch: 74
Loss: 0.09048622625920114
ROC train: 0.997838	val: 0.866807	test: 0.715760
PRC train: 0.999592	val: 0.767545	test: 0.737558

Epoch: 75
Loss: 0.08754607666251071
ROC train: 0.998073	val: 0.864499	test: 0.703221
PRC train: 0.999635	val: 0.770731	test: 0.720437

Epoch: 76
Loss: 0.08327320529773266
ROC train: 0.997646	val: 0.872227	test: 0.712577
PRC train: 0.999551	val: 0.775696	test: 0.718429

Epoch: 77
Loss: 0.07953920361179304
ROC train: 0.996817	val: 0.853157	test: 0.717496
PRC train: 0.999391	val: 0.762524	test: 0.736174

Epoch: 78
Loss: 0.1036991913153179
ROC train: 0.996885	val: 0.856168	test: 0.711420
PRC train: 0.999405	val: 0.759490	test: 0.725609

Epoch: 79
Loss: 0.08543522852837111
ROC train: 0.996843	val: 0.873231	test: 0.708719
PRC train: 0.999400	val: 0.787484	test: 0.725656

Epoch: 80
Loss: 0.08906476562965145
ROC train: 0.997225	val: 0.874335	test: 0.705536
PRC train: 0.999476	val: 0.788592	test: 0.724309

Epoch: 81
Loss: 0.09058050068625471
ROC train: 0.997656	val: 0.862491	test: 0.704379
PRC train: 0.999556	val: 0.762966	test: 0.705846

Epoch: 82
Loss: 0.1173023718205249
ROC train: 0.998199	val: 0.865001	test: 0.698978
PRC train: 0.999657	val: 0.753903	test: 0.694982

Epoch: 83
Loss: 0.1004330578181927
ROC train: 0.997026	val: 0.862491	test: 0.699363
PRC train: 0.999434	val: 0.759275	test: 0.696800

Epoch: 84
Loss: 0.08748755622325821
ROC train: 0.997592	val: 0.873632	test: 0.709780
PRC train: 0.999539	val: 0.764588	test: 0.724203

Epoch: 85
Loss: 0.09283140070359275
ROC train: 0.997768	val: 0.863294	test: 0.690490
PRC train: 0.999576	val: 0.762534	test: 0.686401

Epoch: 86
Loss: 0.09590347940409087
ROC train: 0.998031	val: 0.851551	test: 0.665027
PRC train: 0.999627	val: 0.740335	test: 0.664487

Epoch: 87
Loss: 0.09046281307287178
ROC train: 0.997151	val: 0.850246	test: 0.686053
PRC train: 0.999459	val: 0.745621	test: 0.689230

Epoch: 88
Loss: 0.07625978806383742
ROC train: 0.996960	val: 0.865302	test: 0.690490
PRC train: 0.999416	val: 0.758750	test: 0.700177

Epoch: 89
Loss: 0.11131939362259007
ROC train: 0.998069	val: 0.840811	test: 0.687982
PRC train: 0.999635	val: 0.711425	test: 0.700260

Epoch: 90
Loss: 0.07871330205349354
ROC train: 0.995485	val: 0.819030	test: 0.707465
PRC train: 0.999139	val: 0.695793	test: 0.730657

Epoch: 91
Loss: 0.0878580948017237
ROC train: 0.997990	val: 0.847937	test: 0.713735
PRC train: 0.999618	val: 0.725502	test: 0.735409

Epoch: 92
Loss: 0.07579473093186766
ROC train: 0.998774	val: 0.861186	test: 0.707465
PRC train: 0.999767	val: 0.755287	test: 0.720120

Epoch: 93
Loss: 0.08800890197521559
ROC train: 0.998819	val: 0.860785	test: 0.713252
PRC train: 0.999775	val: 0.749140	test: 0.720534

Epoch: 94
Loss: 0.08095445865362948
ROC train: 0.998682	val: 0.904762	test: 0.765681
PRC train: 0.999705	val: 0.979117	test: 0.671745

Epoch: 95
Loss: 0.09220372176654584
ROC train: 0.998606	val: 0.876568	test: 0.753898
PRC train: 0.999688	val: 0.975303	test: 0.654202

Epoch: 96
Loss: 0.08579880851678166
ROC train: 0.998497	val: 0.886003	test: 0.750672
PRC train: 0.999665	val: 0.973663	test: 0.653666

Epoch: 97
Loss: 0.08057105968266769
ROC train: 0.998631	val: 0.902986	test: 0.760797
PRC train: 0.999697	val: 0.977845	test: 0.658065

Epoch: 98
Loss: 0.08817669752171721
ROC train: 0.998561	val: 0.901210	test: 0.758737
PRC train: 0.999680	val: 0.976788	test: 0.670507

Epoch: 99
Loss: 0.0902811846554558
ROC train: 0.997929	val: 0.901432	test: 0.765367
PRC train: 0.999533	val: 0.974628	test: 0.651165

Epoch: 100
Loss: 0.07536493142162605
ROC train: 0.998571	val: 0.903097	test: 0.768638
PRC train: 0.999682	val: 0.977747	test: 0.659393

Epoch: 101
Loss: 0.08279409789832708
ROC train: 0.998759	val: 0.898324	test: 0.774955
PRC train: 0.999722	val: 0.977929	test: 0.668322

Epoch: 102
Loss: 0.07068204422865987
ROC train: 0.998759	val: 0.904429	test: 0.775941
PRC train: 0.999723	val: 0.977044	test: 0.680915

Epoch: 103
Loss: 0.07800071207297478
ROC train: 0.998715	val: 0.872905	test: 0.758916
PRC train: 0.999713	val: 0.964473	test: 0.647901

Epoch: 104
Loss: 0.093605147318842
ROC train: 0.998624	val: 0.869686	test: 0.762007
PRC train: 0.999690	val: 0.957748	test: 0.676026

Epoch: 105
Loss: 0.0888516443587632
ROC train: 0.999113	val: 0.858808	test: 0.760529
PRC train: 0.999801	val: 0.956348	test: 0.667404

Epoch: 106
Loss: 0.07367875069383938
ROC train: 0.998960	val: 0.864358	test: 0.762634
PRC train: 0.999769	val: 0.960941	test: 0.664368

Epoch: 107
Loss: 0.08952633528149695
ROC train: 0.999071	val: 0.870352	test: 0.769624
PRC train: 0.999793	val: 0.967371	test: 0.674390

Epoch: 108
Loss: 0.07302033020171907
ROC train: 0.998812	val: 0.866578	test: 0.772984
PRC train: 0.999734	val: 0.972458	test: 0.666835

Epoch: 109
Loss: 0.06997544899595526
ROC train: 0.999056	val: 0.872683	test: 0.772581
PRC train: 0.999789	val: 0.970568	test: 0.667823

Epoch: 110
Loss: 0.07337596459955882
ROC train: 0.998931	val: 0.875902	test: 0.758244
PRC train: 0.999761	val: 0.972141	test: 0.649087

Epoch: 111
Loss: 0.0753673937728553
ROC train: 0.998108	val: 0.870463	test: 0.752688
PRC train: 0.999581	val: 0.969062	test: 0.636103

Epoch: 112
Loss: 0.07466321212771378
ROC train: 0.998609	val: 0.869353	test: 0.761559
PRC train: 0.999692	val: 0.966209	test: 0.667826

Epoch: 113
Loss: 0.07872061052480876
ROC train: 0.999016	val: 0.829948	test: 0.756720
PRC train: 0.999778	val: 0.953602	test: 0.667978

Epoch: 114
Loss: 0.08501254335029217
ROC train: 0.998823	val: 0.806083	test: 0.755600
PRC train: 0.999738	val: 0.948375	test: 0.662014

Epoch: 115
Loss: 0.0783557081009977
ROC train: 0.998975	val: 0.825952	test: 0.760618
PRC train: 0.999771	val: 0.953021	test: 0.672337

Epoch: 116
Loss: 0.07316686758426492
ROC train: 0.999277	val: 0.846376	test: 0.760260
PRC train: 0.999836	val: 0.958476	test: 0.663061

Epoch: 117
Loss: 0.07358638000835115
ROC train: 0.999263	val: 0.846931	test: 0.752867
PRC train: 0.999833	val: 0.961257	test: 0.657405

Epoch: 118
Loss: 0.06767735549891807
ROC train: 0.999122	val: 0.846265	test: 0.753091
PRC train: 0.999803	val: 0.960771	test: 0.665799

Epoch: 119
Loss: 0.07180336240694933
ROC train: 0.999135	val: 0.868243	test: 0.762321
PRC train: 0.999806	val: 0.963099	test: 0.661510

Epoch: 120
Loss: 0.05976962889093145
ROC train: 0.999166	val: 0.878899	test: 0.762231
PRC train: 0.999811	val: 0.963801	test: 0.663238

Early stopping
Best (ROC):	 train: 0.902197	val: 0.938062	test: 0.758468
Best (PRC):	 train: 0.972324	val: 0.991829	test: 0.669205

Epoch: 94
Loss: 0.1004222640007847
ROC train: 0.998509	val: 0.839827	test: 0.742249
PRC train: 0.999666	val: 0.957878	test: 0.638856

Epoch: 95
Loss: 0.09198940179010733
ROC train: 0.998311	val: 0.807526	test: 0.739830
PRC train: 0.999623	val: 0.952528	test: 0.631932

Epoch: 96
Loss: 0.07933061322880389
ROC train: 0.998037	val: 0.829282	test: 0.751478
PRC train: 0.999563	val: 0.961677	test: 0.635336

Epoch: 97
Loss: 0.09634951046343772
ROC train: 0.998528	val: 0.870463	test: 0.756048
PRC train: 0.999670	val: 0.970346	test: 0.645396

Epoch: 98
Loss: 0.07862585359233527
ROC train: 0.998463	val: 0.870685	test: 0.755287
PRC train: 0.999658	val: 0.973972	test: 0.633379

Epoch: 99
Loss: 0.0805193850402704
ROC train: 0.997606	val: 0.832834	test: 0.748611
PRC train: 0.999471	val: 0.969725	test: 0.615221

Epoch: 100
Loss: 0.08188278207022508
ROC train: 0.997866	val: 0.840604	test: 0.751927
PRC train: 0.999522	val: 0.971588	test: 0.612297

Epoch: 101
Loss: 0.0749546520506653
ROC train: 0.998931	val: 0.876346	test: 0.755421
PRC train: 0.999762	val: 0.975136	test: 0.619512

Epoch: 102
Loss: 0.08545017542572157
ROC train: 0.998932	val: 0.865468	test: 0.744713
PRC train: 0.999762	val: 0.969806	test: 0.605884

Epoch: 103
Loss: 0.08935535945982061
ROC train: 0.998100	val: 0.814963	test: 0.738889
PRC train: 0.999577	val: 0.962779	test: 0.598749

Epoch: 104
Loss: 0.09470052456282418
ROC train: 0.998418	val: 0.833167	test: 0.748611
PRC train: 0.999648	val: 0.963050	test: 0.611811

Epoch: 105
Loss: 0.0883505862843126
ROC train: 0.998314	val: 0.857920	test: 0.757079
PRC train: 0.999624	val: 0.968797	test: 0.642154

Epoch: 106
Loss: 0.07323950146506769
ROC train: 0.997815	val: 0.852925	test: 0.742473
PRC train: 0.999512	val: 0.970059	test: 0.608792

Epoch: 107
Loss: 0.08437430625402494
ROC train: 0.998517	val: 0.862027	test: 0.741219
PRC train: 0.999670	val: 0.973840	test: 0.608719

Epoch: 108
Loss: 0.0732717698656057
ROC train: 0.998682	val: 0.861361	test: 0.735036
PRC train: 0.999707	val: 0.974497	test: 0.609470

Epoch: 109
Loss: 0.07939993821939458
ROC train: 0.998954	val: 0.841159	test: 0.735260
PRC train: 0.999767	val: 0.970110	test: 0.618759

Epoch: 110
Loss: 0.06993834489050219
ROC train: 0.998311	val: 0.824731	test: 0.735573
PRC train: 0.999625	val: 0.966514	test: 0.606093

Epoch: 111
Loss: 0.08984089967922466
ROC train: 0.998687	val: 0.822511	test: 0.738754
PRC train: 0.999708	val: 0.964261	test: 0.605806

Epoch: 112
Loss: 0.07904978076336114
ROC train: 0.998700	val: 0.848818	test: 0.737724
PRC train: 0.999711	val: 0.969207	test: 0.592271

Epoch: 113
Loss: 0.0723591555507294
ROC train: 0.998939	val: 0.839383	test: 0.732392
PRC train: 0.999763	val: 0.970269	test: 0.591432

Epoch: 114
Loss: 0.07749644358561504
ROC train: 0.998683	val: 0.833056	test: 0.740188
PRC train: 0.999705	val: 0.969682	test: 0.607084

Epoch: 115
Loss: 0.06793603515495795
ROC train: 0.998611	val: 0.828061	test: 0.741174
PRC train: 0.999691	val: 0.968832	test: 0.605608

Epoch: 116
Loss: 0.06555859992913478
ROC train: 0.998907	val: 0.825286	test: 0.734812
PRC train: 0.999756	val: 0.967779	test: 0.594915

Epoch: 117
Loss: 0.07132009437629895
ROC train: 0.999087	val: 0.837385	test: 0.732258
PRC train: 0.999796	val: 0.971289	test: 0.590355

Epoch: 118
Loss: 0.07924424627287881
ROC train: 0.999224	val: 0.837385	test: 0.742159
PRC train: 0.999827	val: 0.971391	test: 0.594769

Epoch: 119
Loss: 0.06855401453805342
ROC train: 0.998751	val: 0.829504	test: 0.742966
PRC train: 0.999721	val: 0.969303	test: 0.604102

Epoch: 120
Loss: 0.06811536477225706
ROC train: 0.998479	val: 0.824620	test: 0.734543
PRC train: 0.999660	val: 0.966757	test: 0.598040

Early stopping
Best (ROC):	 train: 0.970653	val: 0.933511	test: 0.765905
Best (PRC):	 train: 0.992650	val: 0.989748	test: 0.644437

ROC train: 0.998154	val: 0.921745	test: 0.761246
PRC train: 0.999590	val: 0.987993	test: 0.650384

Epoch: 95
Loss: 0.08815595427114598
ROC train: 0.998601	val: 0.918082	test: 0.764830
PRC train: 0.999688	val: 0.984809	test: 0.645049

Epoch: 96
Loss: 0.08138820845890891
ROC train: 0.998800	val: 0.905095	test: 0.759588
PRC train: 0.999733	val: 0.978814	test: 0.630368

Epoch: 97
Loss: 0.08058626886794121
ROC train: 0.998891	val: 0.899101	test: 0.765502
PRC train: 0.999753	val: 0.979949	test: 0.640965

Epoch: 98
Loss: 0.07411890270875392
ROC train: 0.999066	val: 0.898435	test: 0.754570
PRC train: 0.999791	val: 0.982613	test: 0.634605

Epoch: 99
Loss: 0.07020319576058857
ROC train: 0.998812	val: 0.899212	test: 0.764606
PRC train: 0.999734	val: 0.980433	test: 0.642385

Epoch: 100
Loss: 0.0767322723318065
ROC train: 0.998598	val: 0.897991	test: 0.775403
PRC train: 0.999684	val: 0.978814	test: 0.652690

Epoch: 101
Loss: 0.07836779500066245
ROC train: 0.998555	val: 0.897769	test: 0.769713
PRC train: 0.999678	val: 0.980979	test: 0.648001

Epoch: 102
Loss: 0.08037265891820666
ROC train: 0.998838	val: 0.891442	test: 0.759901
PRC train: 0.999739	val: 0.979179	test: 0.639142

Epoch: 103
Loss: 0.0877238712675575
ROC train: 0.999071	val: 0.893551	test: 0.759364
PRC train: 0.999791	val: 0.977578	test: 0.643547

Epoch: 104
Loss: 0.09440893936411994
ROC train: 0.998690	val: 0.897214	test: 0.766443
PRC train: 0.999706	val: 0.978775	test: 0.651008

Epoch: 105
Loss: 0.08825734158719933
ROC train: 0.998454	val: 0.896104	test: 0.759677
PRC train: 0.999653	val: 0.979852	test: 0.652678

Epoch: 106
Loss: 0.07575564325119395
ROC train: 0.998565	val: 0.902986	test: 0.750538
PRC train: 0.999680	val: 0.982524	test: 0.627333

Epoch: 107
Loss: 0.0813511023166729
ROC train: 0.999006	val: 0.911311	test: 0.749686
PRC train: 0.999777	val: 0.983556	test: 0.618166

Epoch: 108
Loss: 0.07646726451317497
ROC train: 0.999008	val: 0.913198	test: 0.753987
PRC train: 0.999779	val: 0.981663	test: 0.628144

Epoch: 109
Loss: 0.08090753249463101
ROC train: 0.999168	val: 0.913198	test: 0.756810
PRC train: 0.999813	val: 0.979951	test: 0.629635

Epoch: 110
Loss: 0.07299948611872413
ROC train: 0.998903	val: 0.901099	test: 0.739158
PRC train: 0.999754	val: 0.980965	test: 0.599389

Epoch: 111
Loss: 0.07904516539069965
ROC train: 0.999267	val: 0.910312	test: 0.753181
PRC train: 0.999835	val: 0.984829	test: 0.627920

Epoch: 112
Loss: 0.0794841752450709
ROC train: 0.999169	val: 0.906760	test: 0.749283
PRC train: 0.999814	val: 0.984253	test: 0.621940

Epoch: 113
Loss: 0.07391422997299359
ROC train: 0.999028	val: 0.911977	test: 0.757079
PRC train: 0.999784	val: 0.985613	test: 0.634099

Epoch: 114
Loss: 0.07179920688547697
ROC train: 0.999413	val: 0.909202	test: 0.755556
PRC train: 0.999868	val: 0.983040	test: 0.640396

Epoch: 115
Loss: 0.07569995652516795
ROC train: 0.997940	val: 0.895105	test: 0.746640
PRC train: 0.999547	val: 0.980990	test: 0.630661

Epoch: 116
Loss: 0.07411852662246692
ROC train: 0.999069	val: 0.892552	test: 0.746505
PRC train: 0.999789	val: 0.976177	test: 0.644410

Epoch: 117
Loss: 0.0698595127602185
ROC train: 0.998957	val: 0.888556	test: 0.747984
PRC train: 0.999766	val: 0.973500	test: 0.658858

Epoch: 118
Loss: 0.07516168322771868
ROC train: 0.999156	val: 0.888445	test: 0.740860
PRC train: 0.999811	val: 0.973245	test: 0.636249

Epoch: 119
Loss: 0.05777455635874507
ROC train: 0.998710	val: 0.891553	test: 0.739337
PRC train: 0.999717	val: 0.975920	test: 0.618821

Epoch: 120
Loss: 0.07648655958295687
ROC train: 0.998662	val: 0.893551	test: 0.751882
PRC train: 0.999701	val: 0.975659	test: 0.638681

Early stopping
Best (ROC):	 train: 0.978334	val: 0.943723	test: 0.790591
Best (PRC):	 train: 0.994625	val: 0.992089	test: 0.738935
All runs completed.


Epoch: 94
Loss: 0.07314558630597139
ROC train: 0.997938	val: 0.859982	test: 0.708237
PRC train: 0.999609	val: 0.763389	test: 0.729898

Epoch: 95
Loss: 0.0899242324954348
ROC train: 0.998425	val: 0.869216	test: 0.703221
PRC train: 0.999702	val: 0.770076	test: 0.722863

Epoch: 96
Loss: 0.0804007215258914
ROC train: 0.998213	val: 0.856369	test: 0.700617
PRC train: 0.999661	val: 0.738080	test: 0.719271

Epoch: 97
Loss: 0.07324400999591107
ROC train: 0.997987	val: 0.846331	test: 0.704090
PRC train: 0.999618	val: 0.719369	test: 0.720952

Epoch: 98
Loss: 0.06612775193247455
ROC train: 0.997914	val: 0.837699	test: 0.685957
PRC train: 0.999604	val: 0.709641	test: 0.679874

Epoch: 99
Loss: 0.07593898276053541
ROC train: 0.998207	val: 0.855465	test: 0.689622
PRC train: 0.999659	val: 0.738619	test: 0.679797

Epoch: 100
Loss: 0.0793513079216722
ROC train: 0.998134	val: 0.869216	test: 0.700521
PRC train: 0.999645	val: 0.771723	test: 0.708414

Epoch: 101
Loss: 0.08600981358369213
ROC train: 0.998400	val: 0.835090	test: 0.693287
PRC train: 0.999697	val: 0.701042	test: 0.692930

Epoch: 102
Loss: 0.08378142555253894
ROC train: 0.998472	val: 0.833082	test: 0.692130
PRC train: 0.999710	val: 0.679708	test: 0.697071

Epoch: 103
Loss: 0.09498914974830473
ROC train: 0.998595	val: 0.840410	test: 0.690394
PRC train: 0.999733	val: 0.689465	test: 0.703441

Epoch: 104
Loss: 0.07897974361151212
ROC train: 0.998694	val: 0.863595	test: 0.705150
PRC train: 0.999751	val: 0.723169	test: 0.712210

Epoch: 105
Loss: 0.06627342796244118
ROC train: 0.998611	val: 0.880458	test: 0.709394
PRC train: 0.999735	val: 0.768462	test: 0.704435

Epoch: 106
Loss: 0.06724823588233744
ROC train: 0.998840	val: 0.867510	test: 0.702257
PRC train: 0.999779	val: 0.762793	test: 0.700943

Epoch: 107
Loss: 0.08548566365679515
ROC train: 0.998599	val: 0.868915	test: 0.710455
PRC train: 0.999734	val: 0.763153	test: 0.710759

Epoch: 108
Loss: 0.07127309541826021
ROC train: 0.998210	val: 0.858476	test: 0.704282
PRC train: 0.999656	val: 0.742788	test: 0.709379

Epoch: 109
Loss: 0.08950018781632933
ROC train: 0.998391	val: 0.847737	test: 0.689815
PRC train: 0.999695	val: 0.743218	test: 0.708862

Epoch: 110
Loss: 0.08820360832423456
ROC train: 0.998297	val: 0.865703	test: 0.676119
PRC train: 0.999676	val: 0.765431	test: 0.679391

Epoch: 111
Loss: 0.07543406107481299
ROC train: 0.998249	val: 0.856268	test: 0.674865
PRC train: 0.999668	val: 0.769059	test: 0.687220

Epoch: 112
Loss: 0.08345313594080532
ROC train: 0.998802	val: 0.864398	test: 0.700907
PRC train: 0.999771	val: 0.768106	test: 0.715694

Epoch: 113
Loss: 0.07640084326349664
ROC train: 0.998203	val: 0.850346	test: 0.699074
PRC train: 0.999656	val: 0.745230	test: 0.712872

Epoch: 114
Loss: 0.08123523498111188
ROC train: 0.998811	val: 0.857172	test: 0.692805
PRC train: 0.999772	val: 0.771538	test: 0.700243

Epoch: 115
Loss: 0.06418207434143028
ROC train: 0.998749	val: 0.861488	test: 0.679880
PRC train: 0.999761	val: 0.778886	test: 0.673875

Epoch: 116
Loss: 0.07942862304586737
ROC train: 0.999182	val: 0.866707	test: 0.695891
PRC train: 0.999843	val: 0.780700	test: 0.704567

Epoch: 117
Loss: 0.063203090279954
ROC train: 0.998840	val: 0.863595	test: 0.702160
PRC train: 0.999779	val: 0.756280	test: 0.719089

Epoch: 118
Loss: 0.06757747038280691
ROC train: 0.999070	val: 0.867610	test: 0.692805
PRC train: 0.999823	val: 0.755624	test: 0.703107

Epoch: 119
Loss: 0.06553013023124261
ROC train: 0.999224	val: 0.857071	test: 0.695891
PRC train: 0.999852	val: 0.736178	test: 0.713334

Epoch: 120
Loss: 0.07727079155376182
ROC train: 0.998031	val: 0.842317	test: 0.662519
PRC train: 0.999624	val: 0.730411	test: 0.671264

Early stopping
Best (ROC):	 train: 0.877881	val: 0.909164	test: 0.638117
Best (PRC):	 train: 0.969635	val: 0.799758	test: 0.684830

ROC train: 0.998402	val: 0.857874	test: 0.634163
PRC train: 0.999697	val: 0.735853	test: 0.650670

Epoch: 95
Loss: 0.08111611177823276
ROC train: 0.998897	val: 0.866305	test: 0.663387
PRC train: 0.999790	val: 0.739680	test: 0.683795

Epoch: 96
Loss: 0.06797684068030897
ROC train: 0.998360	val: 0.860183	test: 0.655093
PRC train: 0.999692	val: 0.737543	test: 0.662747

Epoch: 97
Loss: 0.07726219165119574
ROC train: 0.998933	val: 0.871224	test: 0.652778
PRC train: 0.999798	val: 0.749921	test: 0.653165

Epoch: 98
Loss: 0.08442540644815366
ROC train: 0.998747	val: 0.872026	test: 0.642168
PRC train: 0.999761	val: 0.755357	test: 0.639572

Epoch: 99
Loss: 0.09899171970173318
ROC train: 0.998398	val: 0.871123	test: 0.664545
PRC train: 0.999696	val: 0.761816	test: 0.671973

Epoch: 100
Loss: 0.078563967756185
ROC train: 0.997214	val: 0.872328	test: 0.635706
PRC train: 0.999472	val: 0.781614	test: 0.665683

Epoch: 101
Loss: 0.08581349894337043
ROC train: 0.997916	val: 0.854662	test: 0.640721
PRC train: 0.999603	val: 0.722848	test: 0.672819

Epoch: 102
Loss: 0.07373114451102772
ROC train: 0.997879	val: 0.869818	test: 0.671103
PRC train: 0.999599	val: 0.733369	test: 0.685351

Epoch: 103
Loss: 0.08192933809674156
ROC train: 0.998694	val: 0.884473	test: 0.683546
PRC train: 0.999753	val: 0.758786	test: 0.685671

Epoch: 104
Loss: 0.07085289965489697
ROC train: 0.999042	val: 0.887183	test: 0.674672
PRC train: 0.999817	val: 0.767538	test: 0.684360

Epoch: 105
Loss: 0.0672016590018508
ROC train: 0.999050	val: 0.871525	test: 0.678241
PRC train: 0.999819	val: 0.737302	test: 0.686023

Epoch: 106
Loss: 0.07420323482711262
ROC train: 0.998534	val: 0.871224	test: 0.692901
PRC train: 0.999719	val: 0.734657	test: 0.694286

Epoch: 107
Loss: 0.07612830286778667
ROC train: 0.998799	val: 0.874335	test: 0.687596
PRC train: 0.999772	val: 0.739915	test: 0.701206

Epoch: 108
Loss: 0.06578477994298444
ROC train: 0.998628	val: 0.862290	test: 0.662326
PRC train: 0.999740	val: 0.719262	test: 0.657392

Epoch: 109
Loss: 0.07221236638672068
ROC train: 0.998328	val: 0.853659	test: 0.635127
PRC train: 0.999684	val: 0.710663	test: 0.632901

Epoch: 110
Loss: 0.07157556107261029
ROC train: 0.998603	val: 0.860082	test: 0.657311
PRC train: 0.999735	val: 0.726976	test: 0.674865

Epoch: 111
Loss: 0.07439708039876931
ROC train: 0.998378	val: 0.865101	test: 0.661941
PRC train: 0.999692	val: 0.739171	test: 0.687413

Epoch: 112
Loss: 0.07052457913625475
ROC train: 0.998463	val: 0.851651	test: 0.648630
PRC train: 0.999708	val: 0.724485	test: 0.666231

Epoch: 113
Loss: 0.07202560723887548
ROC train: 0.998801	val: 0.859179	test: 0.657022
PRC train: 0.999770	val: 0.736734	test: 0.670265

Epoch: 114
Loss: 0.07434593625341453
ROC train: 0.998940	val: 0.875138	test: 0.674383
PRC train: 0.999798	val: 0.751578	test: 0.686929

Epoch: 115
Loss: 0.058560531692737325
ROC train: 0.999175	val: 0.872829	test: 0.652103
PRC train: 0.999843	val: 0.757915	test: 0.664143

Epoch: 116
Loss: 0.09850518171738061
ROC train: 0.999188	val: 0.868313	test: 0.651524
PRC train: 0.999845	val: 0.769528	test: 0.665643

Epoch: 117
Loss: 0.0643821869636035
ROC train: 0.994612	val: 0.845528	test: 0.636381
PRC train: 0.998624	val: 0.743475	test: 0.635477

Epoch: 118
Loss: 0.08697413411475244
ROC train: 0.999071	val: 0.872127	test: 0.660108
PRC train: 0.999823	val: 0.772502	test: 0.654299

Epoch: 119
Loss: 0.06773158945000908
ROC train: 0.998857	val: 0.874034	test: 0.657793
PRC train: 0.999782	val: 0.772789	test: 0.670364

Epoch: 120
Loss: 0.059876971022710605
ROC train: 0.998874	val: 0.871123	test: 0.660494
PRC train: 0.999784	val: 0.746165	test: 0.667566

Early stopping
Best (ROC):	 train: 0.907365	val: 0.915889	test: 0.662037
Best (PRC):	 train: 0.977316	val: 0.835005	test: 0.706186

ROC train: 0.997538	val: 0.834488	test: 0.715471
PRC train: 0.999534	val: 0.710082	test: 0.733002

Epoch: 95
Loss: 0.09134847871005831
ROC train: 0.998813	val: 0.848238	test: 0.688657
PRC train: 0.999773	val: 0.724459	test: 0.701153

Epoch: 96
Loss: 0.08483149131722348
ROC train: 0.998146	val: 0.843019	test: 0.686246
PRC train: 0.999646	val: 0.720573	test: 0.683586

Epoch: 97
Loss: 0.08071835872581581
ROC train: 0.997193	val: 0.832882	test: 0.696663
PRC train: 0.999468	val: 0.723552	test: 0.693696

Epoch: 98
Loss: 0.08870671601949985
ROC train: 0.998421	val: 0.847335	test: 0.702932
PRC train: 0.999701	val: 0.725120	test: 0.710685

Epoch: 99
Loss: 0.07671693093600884
ROC train: 0.998157	val: 0.854763	test: 0.711902
PRC train: 0.999650	val: 0.718967	test: 0.716830

Epoch: 100
Loss: 0.07474155778015887
ROC train: 0.993985	val: 0.843421	test: 0.714313
PRC train: 0.998828	val: 0.694284	test: 0.742377

Epoch: 101
Loss: 0.081288953388871
ROC train: 0.998998	val: 0.873833	test: 0.690779
PRC train: 0.999808	val: 0.765483	test: 0.708930

Epoch: 102
Loss: 0.09651479037879289
ROC train: 0.998879	val: 0.858978	test: 0.671393
PRC train: 0.999786	val: 0.743155	test: 0.672943

Epoch: 103
Loss: 0.07938492422982428
ROC train: 0.998899	val: 0.846030	test: 0.676601
PRC train: 0.999790	val: 0.725110	test: 0.692344

Epoch: 104
Loss: 0.0710068174696244
ROC train: 0.998802	val: 0.843421	test: 0.668113
PRC train: 0.999772	val: 0.726710	test: 0.680122

Epoch: 105
Loss: 0.07233612294992289
ROC train: 0.998641	val: 0.849443	test: 0.676408
PRC train: 0.999742	val: 0.736145	test: 0.681684

Epoch: 106
Loss: 0.06666259637984111
ROC train: 0.998520	val: 0.861789	test: 0.691069
PRC train: 0.999717	val: 0.750440	test: 0.695149

Epoch: 107
Loss: 0.06827455403776027
ROC train: 0.999057	val: 0.861588	test: 0.677951
PRC train: 0.999820	val: 0.740987	test: 0.676551

Epoch: 108
Loss: 0.07440206601528518
ROC train: 0.999226	val: 0.858175	test: 0.679880
PRC train: 0.999852	val: 0.739085	test: 0.695104

Epoch: 109
Loss: 0.06728181491509207
ROC train: 0.999149	val: 0.861889	test: 0.679012
PRC train: 0.999837	val: 0.749992	test: 0.698425

Epoch: 110
Loss: 0.07406099523833307
ROC train: 0.999216	val: 0.861387	test: 0.683063
PRC train: 0.999849	val: 0.749532	test: 0.711782

Epoch: 111
Loss: 0.06557452848536272
ROC train: 0.998832	val: 0.852655	test: 0.695312
PRC train: 0.999777	val: 0.749068	test: 0.719426

Epoch: 112
Loss: 0.08295676411727174
ROC train: 0.998745	val: 0.861387	test: 0.697724
PRC train: 0.999760	val: 0.765717	test: 0.715475

Epoch: 113
Loss: 0.055644580776539186
ROC train: 0.999153	val: 0.861488	test: 0.699653
PRC train: 0.999837	val: 0.763849	test: 0.723025

Epoch: 114
Loss: 0.05360334325879463
ROC train: 0.999085	val: 0.870119	test: 0.701678
PRC train: 0.999825	val: 0.772973	test: 0.714516

Epoch: 115
Loss: 0.06273562645169699
ROC train: 0.998961	val: 0.866406	test: 0.708526
PRC train: 0.999802	val: 0.768120	test: 0.721279

Epoch: 116
Loss: 0.0653789228538496
ROC train: 0.999199	val: 0.866506	test: 0.703704
PRC train: 0.999849	val: 0.768635	test: 0.721421

Epoch: 117
Loss: 0.06873107764409585
ROC train: 0.999213	val: 0.858777	test: 0.675154
PRC train: 0.999850	val: 0.762591	test: 0.697131

Epoch: 118
Loss: 0.07973253329604334
ROC train: 0.998975	val: 0.860484	test: 0.674576
PRC train: 0.999804	val: 0.761453	test: 0.687581

Epoch: 119
Loss: 0.06157513892300105
ROC train: 0.998526	val: 0.862190	test: 0.697242
PRC train: 0.999722	val: 0.769947	test: 0.693595

Epoch: 120
Loss: 0.06315627649837821
ROC train: 0.999028	val: 0.863294	test: 0.693962
PRC train: 0.999815	val: 0.764307	test: 0.689815

Early stopping
Best (ROC):	 train: 0.932403	val: 0.921911	test: 0.670718
Best (PRC):	 train: 0.982706	val: 0.858214	test: 0.708700
All runs completed.
