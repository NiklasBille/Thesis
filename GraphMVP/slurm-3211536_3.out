>>> Starting run for dataset: bbbp
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running RANDOM configs_split_experiments/GraphCL/bbbp/random/train_prop=0.6.yml on cuda:0
Running RANDOM configs_split_experiments/GraphCL/bbbp/random/train_prop=0.7.yml on cuda:1
Running RANDOM configs_split_experiments/GraphCL/bbbp/random/train_prop=0.8.yml on cuda:2
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/bbbp/random/train_prop=0.8.yml --runseed 4 --device cuda:2
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/bbbp/random/train_prop=0.8.yml --runseed 5 --device cuda:2
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/bbbp/random/train_prop=0.8.yml --runseed 6 --device cuda:2
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/bbbp/random/train_prop=0.7.yml --runseed 4 --device cuda:1
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/bbbp/random/train_prop=0.6.yml --runseed 4 --device cuda:0
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/bbbp/random/train_prop=0.7.yml --runseed 5 --device cuda:1
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/bbbp/random/train_prop=0.6.yml --runseed 5 --device cuda:0
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/bbbp/random/train_prop=0.7.yml --runseed 6 --device cuda:1
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphCL/bbbp/random/train_prop=0.6.yml --runseed 6 --device cuda:0
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/bbbp/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/bbbp/random/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/bbbp/random/train_prop=0.6/bbbp_random_6_26-05_09-45-14  ]
[ Using Seed :  6  ]
[ Using device :  cuda:0  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
randomly split
Data(edge_attr=[58, 2], edge_index=[2, 58], id=[1], x=[27, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6234204794885054
ROC train: 0.812796	val: 0.764649	test: 0.773082
PRC train: 0.926580	val: 0.903894	test: 0.913301

Epoch: 2
Loss: 0.5200208749451631
ROC train: 0.839974	val: 0.792043	test: 0.773968
PRC train: 0.936361	val: 0.916280	test: 0.909974

Epoch: 3
Loss: 0.45108782812651754
ROC train: 0.864387	val: 0.821715	test: 0.791128
PRC train: 0.948928	val: 0.921225	test: 0.916341

Epoch: 4
Loss: 0.40142385525847696
ROC train: 0.880472	val: 0.839848	test: 0.810224
PRC train: 0.956277	val: 0.929979	test: 0.924285

Epoch: 5
Loss: 0.37877678021083494
ROC train: 0.886639	val: 0.844997	test: 0.812422
PRC train: 0.957668	val: 0.934161	test: 0.921933

Epoch: 6
Loss: 0.3501961391437279
ROC train: 0.900414	val: 0.855023	test: 0.828204
PRC train: 0.962815	val: 0.937709	test: 0.928805

Epoch: 7
Loss: 0.33973791229315675
ROC train: 0.914120	val: 0.869876	test: 0.843887
PRC train: 0.968139	val: 0.943942	test: 0.937150

Epoch: 8
Loss: 0.3281486203294698
ROC train: 0.920378	val: 0.872153	test: 0.843986
PRC train: 0.970904	val: 0.944815	test: 0.930395

Epoch: 9
Loss: 0.3139687693597968
ROC train: 0.927790	val: 0.881313	test: 0.853534
PRC train: 0.974282	val: 0.946546	test: 0.934769

Epoch: 10
Loss: 0.30772389755146146
ROC train: 0.934969	val: 0.891527	test: 0.866592
PRC train: 0.977155	val: 0.948822	test: 0.941853

Epoch: 11
Loss: 0.28370266982009607
ROC train: 0.939951	val: 0.899344	test: 0.873384
PRC train: 0.979129	val: 0.951156	test: 0.948542

Epoch: 12
Loss: 0.28459055552815415
ROC train: 0.947389	val: 0.898562	test: 0.868003
PRC train: 0.982031	val: 0.950894	test: 0.945294

Epoch: 13
Loss: 0.2848534009083579
ROC train: 0.952604	val: 0.897951	test: 0.872433
PRC train: 0.983663	val: 0.949496	test: 0.948227

Epoch: 14
Loss: 0.2753068462225639
ROC train: 0.955337	val: 0.895028	test: 0.874434
PRC train: 0.984772	val: 0.948980	test: 0.946896

Epoch: 15
Loss: 0.2604598360637744
ROC train: 0.958242	val: 0.893736	test: 0.872990
PRC train: 0.985976	val: 0.952047	test: 0.944919

Epoch: 16
Loss: 0.2575869748034573
ROC train: 0.960291	val: 0.892665	test: 0.883555
PRC train: 0.986559	val: 0.951090	test: 0.952818

Epoch: 17
Loss: 0.2538652783459947
ROC train: 0.966817	val: 0.897815	test: 0.886804
PRC train: 0.989024	val: 0.953927	test: 0.956066

Epoch: 18
Loss: 0.2362615818944481
ROC train: 0.966300	val: 0.887431	test: 0.885262
PRC train: 0.988970	val: 0.951424	test: 0.951137

Epoch: 19
Loss: 0.24467050729868775
ROC train: 0.969460	val: 0.891425	test: 0.895794
PRC train: 0.990070	val: 0.952136	test: 0.959825

Epoch: 20
Loss: 0.23759252649661033
ROC train: 0.972498	val: 0.891221	test: 0.896614
PRC train: 0.991135	val: 0.952985	test: 0.961223

Epoch: 21
Loss: 0.23414932868104188
ROC train: 0.972851	val: 0.891527	test: 0.901536
PRC train: 0.991111	val: 0.952182	test: 0.963306

Epoch: 22
Loss: 0.23543984803062395
ROC train: 0.974054	val: 0.893226	test: 0.901634
PRC train: 0.991643	val: 0.952575	test: 0.961960

Epoch: 23
Loss: 0.23388735817842346
ROC train: 0.975198	val: 0.890949	test: 0.889199
PRC train: 0.991885	val: 0.952269	test: 0.956210

Epoch: 24
Loss: 0.22734772850335877
ROC train: 0.976310	val: 0.891595	test: 0.889264
PRC train: 0.992181	val: 0.950110	test: 0.957845

Epoch: 25
Loss: 0.21546302040750595
ROC train: 0.974564	val: 0.886870	test: 0.876731
PRC train: 0.991614	val: 0.949895	test: 0.949471

Epoch: 26
Loss: 0.2139505078207004
ROC train: 0.980362	val: 0.896965	test: 0.897303
PRC train: 0.993742	val: 0.954331	test: 0.962424

Epoch: 27
Loss: 0.21114816928344443
ROC train: 0.979530	val: 0.896625	test: 0.904554
PRC train: 0.993387	val: 0.955459	test: 0.966203

Epoch: 28
Loss: 0.20895344140687655
ROC train: 0.982299	val: 0.894790	test: 0.908590
PRC train: 0.994392	val: 0.954033	test: 0.967365

Epoch: 29
Loss: 0.1940279855302121
ROC train: 0.981957	val: 0.895860	test: 0.905309
PRC train: 0.994269	val: 0.953753	test: 0.964435

Epoch: 30
Loss: 0.19780853084379665
ROC train: 0.983582	val: 0.894008	test: 0.905440
PRC train: 0.994846	val: 0.953188	test: 0.966012

Epoch: 31
Loss: 0.19979508230822854
ROC train: 0.983114	val: 0.894824	test: 0.900551
PRC train: 0.994715	val: 0.953945	test: 0.963692

Epoch: 32
Loss: 0.1795829682318198
ROC train: 0.985002	val: 0.900058	test: 0.901536
PRC train: 0.995259	val: 0.956179	test: 0.963439

Epoch: 33
Loss: 0.1810356753184179Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/bbbp/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/bbbp/random/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/bbbp/random/train_prop=0.6/bbbp_random_5_26-05_09-45-14  ]
[ Using Seed :  5  ]
[ Using device :  cuda:0  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
randomly split
Data(edge_attr=[58, 2], edge_index=[2, 58], id=[1], x=[27, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6076175852487304
ROC train: 0.794243	val: 0.767538	test: 0.741715
PRC train: 0.902792	val: 0.896168	test: 0.867534

Epoch: 2
Loss: 0.49046038578831413
ROC train: 0.845288	val: 0.815665	test: 0.769506
PRC train: 0.929495	val: 0.925705	test: 0.888733

Epoch: 3
Loss: 0.4294264170307188
ROC train: 0.866312	val: 0.833271	test: 0.782925
PRC train: 0.945636	val: 0.930104	test: 0.897521

Epoch: 4
Loss: 0.3957332890279577
ROC train: 0.881667	val: 0.837010	test: 0.792605
PRC train: 0.956099	val: 0.929590	test: 0.901016

Epoch: 5
Loss: 0.3742718585098735
ROC train: 0.887672	val: 0.838743	test: 0.805958
PRC train: 0.958349	val: 0.929997	test: 0.903974

Epoch: 6
Loss: 0.36066011232709744
ROC train: 0.899589	val: 0.846662	test: 0.822626
PRC train: 0.963143	val: 0.932002	test: 0.918210

Epoch: 7
Loss: 0.34162013906294614
ROC train: 0.914616	val: 0.858796	test: 0.847070
PRC train: 0.968922	val: 0.935851	test: 0.933702

Epoch: 8
Loss: 0.3280536593780726
ROC train: 0.922628	val: 0.863504	test: 0.859013
PRC train: 0.972561	val: 0.936019	test: 0.939613

Epoch: 9
Loss: 0.32348006559038583
ROC train: 0.930939	val: 0.866885	test: 0.860161
PRC train: 0.975791	val: 0.937474	test: 0.939850

Epoch: 10
Loss: 0.30477677437711803
ROC train: 0.939959	val: 0.874431	test: 0.864656
PRC train: 0.979669	val: 0.942402	test: 0.941063

Epoch: 11
Loss: 0.28452743670757663
ROC train: 0.945989	val: 0.878747	test: 0.870070
PRC train: 0.981959	val: 0.944123	test: 0.942106

Epoch: 12
Loss: 0.29253909855682275
ROC train: 0.948389	val: 0.878577	test: 0.872925
PRC train: 0.982576	val: 0.941902	test: 0.943760

Epoch: 13
Loss: 0.28288942392640337
ROC train: 0.954352	val: 0.884287	test: 0.881127
PRC train: 0.984930	val: 0.946780	test: 0.951513

Epoch: 14
Loss: 0.2763400196315702
ROC train: 0.958639	val: 0.887278	test: 0.893202
PRC train: 0.986434	val: 0.941064	test: 0.958265

Epoch: 15
Loss: 0.2704372343049678
ROC train: 0.953909	val: 0.874465	test: 0.888543
PRC train: 0.984156	val: 0.936366	test: 0.957145

Epoch: 16
Loss: 0.27427298097538333
ROC train: 0.961174	val: 0.883302	test: 0.886180
PRC train: 0.987119	val: 0.941627	test: 0.957083

Epoch: 17
Loss: 0.25907558370561434
ROC train: 0.963418	val: 0.885749	test: 0.883654
PRC train: 0.988009	val: 0.943239	test: 0.955796

Epoch: 18
Loss: 0.25525248882101303
ROC train: 0.963750	val: 0.885409	test: 0.885458
PRC train: 0.987952	val: 0.941585	test: 0.955853

Epoch: 19
Loss: 0.2528067844797973
ROC train: 0.966296	val: 0.889555	test: 0.889724
PRC train: 0.988937	val: 0.940371	test: 0.957462

Epoch: 20
Loss: 0.2442204108654086
ROC train: 0.967750	val: 0.887431	test: 0.885852
PRC train: 0.989480	val: 0.940534	test: 0.954084

Epoch: 21
Loss: 0.250685120983032
ROC train: 0.969226	val: 0.884899	test: 0.883424
PRC train: 0.990164	val: 0.940024	test: 0.953321

Epoch: 22
Loss: 0.24022823022279577
ROC train: 0.969345	val: 0.890065	test: 0.886377
PRC train: 0.990307	val: 0.940893	test: 0.956901

Epoch: 23
Loss: 0.22690233172519064
ROC train: 0.971303	val: 0.892003	test: 0.898222
PRC train: 0.990830	val: 0.941243	test: 0.962073

Epoch: 24
Loss: 0.23383723372970983
ROC train: 0.976650	val: 0.896319	test: 0.903537
PRC train: 0.992593	val: 0.947672	test: 0.965240

Epoch: 25
Loss: 0.21466586566356294
ROC train: 0.975919	val: 0.885647	test: 0.887985
PRC train: 0.992175	val: 0.947266	test: 0.953220

Epoch: 26
Loss: 0.21030027279335534
ROC train: 0.977441	val: 0.890235	test: 0.890314
PRC train: 0.992962	val: 0.947726	test: 0.955504

Epoch: 27
Loss: 0.21343803616180504
ROC train: 0.978943	val: 0.891289	test: 0.892644
PRC train: 0.993358	val: 0.946069	test: 0.957399

Epoch: 28
Loss: 0.21546006526420558
ROC train: 0.982690	val: 0.893770	test: 0.888739
PRC train: 0.994585	val: 0.946330	test: 0.958259

Epoch: 29
Loss: 0.19411318803840877
ROC train: 0.983248	val: 0.894093	test: 0.893136
PRC train: 0.994727	val: 0.947049	test: 0.958719

Epoch: 30
Loss: 0.2210289456096109
ROC train: 0.983680	val: 0.889216	test: 0.891791
PRC train: 0.994916	val: 0.944817	test: 0.956690

Epoch: 31
Loss: 0.19273734189356023
ROC train: 0.982601	val: 0.886666	test: 0.884244
PRC train: 0.994639	val: 0.943532	test: 0.953315

Epoch: 32
Loss: 0.20560786844865392
ROC train: 0.983673	val: 0.886463	test: 0.893497
PRC train: 0.994888	val: 0.940775	test: 0.957934

Epoch: 33
Loss: 0.1967989659088659Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/bbbp/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/bbbp/random/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphCL/bbbp/random/train_prop=0.6/bbbp_random_4_26-05_09-45-14  ]
[ Using Seed :  4  ]
[ Using device :  cuda:0  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
randomly split
Data(edge_attr=[58, 2], edge_index=[2, 58], id=[1], x=[27, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6288606872877158
ROC train: 0.819463	val: 0.770971	test: 0.754774
PRC train: 0.927622	val: 0.908542	test: 0.889564

Epoch: 2
Loss: 0.5149395735375034
ROC train: 0.831677	val: 0.776035	test: 0.753560
PRC train: 0.918549	val: 0.894969	test: 0.866098

Epoch: 3
Loss: 0.4485037230019991
ROC train: 0.858678	val: 0.803497	test: 0.775543
PRC train: 0.934201	val: 0.911167	test: 0.881899

Epoch: 4
Loss: 0.40868473548060547
ROC train: 0.884550	val: 0.837010	test: 0.797067
PRC train: 0.954690	val: 0.933612	test: 0.912070

Epoch: 5
Loss: 0.3860160851217176
ROC train: 0.896308	val: 0.842414	test: 0.806024
PRC train: 0.959284	val: 0.932676	test: 0.919720

Epoch: 6
Loss: 0.3686278630698022
ROC train: 0.901171	val: 0.848498	test: 0.813702
PRC train: 0.962563	val: 0.929111	test: 0.925511

Epoch: 7
Loss: 0.3413347636001874
ROC train: 0.910357	val: 0.851098	test: 0.831452
PRC train: 0.966613	val: 0.932386	test: 0.929972

Epoch: 8
Loss: 0.34373964471770274
ROC train: 0.922749	val: 0.862229	test: 0.853009
PRC train: 0.969819	val: 0.933463	test: 0.939550

Epoch: 9
Loss: 0.3105231852836813
ROC train: 0.932130	val: 0.875688	test: 0.863935
PRC train: 0.973482	val: 0.944248	test: 0.946364

Epoch: 10
Loss: 0.3164157611469168
ROC train: 0.938846	val: 0.879359	test: 0.866625
PRC train: 0.977348	val: 0.943438	test: 0.945607

Epoch: 11
Loss: 0.30184323732300417
ROC train: 0.940757	val: 0.880719	test: 0.876173
PRC train: 0.979284	val: 0.943274	test: 0.949109

Epoch: 12
Loss: 0.2820436821463029
ROC train: 0.944545	val: 0.886361	test: 0.886771
PRC train: 0.980273	val: 0.944240	test: 0.958488

Epoch: 13
Loss: 0.2776614121413735
ROC train: 0.950732	val: 0.889148	test: 0.890872
PRC train: 0.982666	val: 0.946667	test: 0.960052

Epoch: 14
Loss: 0.27580029327904504
ROC train: 0.951867	val: 0.881840	test: 0.884408
PRC train: 0.983702	val: 0.942733	test: 0.955626

Epoch: 15
Loss: 0.2746517111227846
ROC train: 0.960046	val: 0.896931	test: 0.893169
PRC train: 0.986926	val: 0.952467	test: 0.959476

Epoch: 16
Loss: 0.2649514946412389
ROC train: 0.958644	val: 0.895401	test: 0.884277
PRC train: 0.986456	val: 0.951534	test: 0.954604

Epoch: 17
Loss: 0.2587611382558861
ROC train: 0.962946	val: 0.901451	test: 0.887657
PRC train: 0.988092	val: 0.957833	test: 0.957517

Epoch: 18
Loss: 0.26421415720581026
ROC train: 0.966478	val: 0.897424	test: 0.888838
PRC train: 0.989376	val: 0.955631	test: 0.956277

Epoch: 19
Loss: 0.25168208186790764
ROC train: 0.968421	val: 0.899242	test: 0.887197
PRC train: 0.989888	val: 0.955113	test: 0.956321

Epoch: 20
Loss: 0.248146896193686
ROC train: 0.968462	val: 0.897305	test: 0.881784
PRC train: 0.989674	val: 0.952993	test: 0.954351

Epoch: 21
Loss: 0.22945048270966956
ROC train: 0.970588	val: 0.895537	test: 0.887329
PRC train: 0.990494	val: 0.951750	test: 0.956030

Epoch: 22
Loss: 0.23731947674644735
ROC train: 0.972099	val: 0.893260	test: 0.888280
PRC train: 0.991166	val: 0.953917	test: 0.957566

Epoch: 23
Loss: 0.2205612208024185
ROC train: 0.972252	val: 0.900058	test: 0.895367
PRC train: 0.991300	val: 0.956820	test: 0.961542

Epoch: 24
Loss: 0.22925373192493398
ROC train: 0.973871	val: 0.897492	test: 0.889953
PRC train: 0.991737	val: 0.953749	test: 0.957244

Epoch: 25
Loss: 0.22718330235713088
ROC train: 0.976369	val: 0.896693	test: 0.894055
PRC train: 0.992521	val: 0.956004	test: 0.959580

Epoch: 26
Loss: 0.22566932526641326
ROC train: 0.979470	val: 0.899208	test: 0.896319
PRC train: 0.993601	val: 0.959346	test: 0.960854

Epoch: 27
Loss: 0.21603053540751987
ROC train: 0.979582	val: 0.903898	test: 0.901864
PRC train: 0.993557	val: 0.961717	test: 0.964094

Epoch: 28
Loss: 0.21683132534268085
ROC train: 0.980453	val: 0.902539	test: 0.903799
PRC train: 0.993795	val: 0.959897	test: 0.963455

Epoch: 29
Loss: 0.21262871502577768
ROC train: 0.978804	val: 0.893175	test: 0.899042
PRC train: 0.993173	val: 0.954337	test: 0.961179

Epoch: 30
Loss: 0.22091094435498487
ROC train: 0.979046	val: 0.895469	test: 0.894120
PRC train: 0.993366	val: 0.954391	test: 0.960384

Epoch: 31
Loss: 0.21648296987432034
ROC train: 0.982176	val: 0.897985	test: 0.887854
PRC train: 0.994394	val: 0.958511	test: 0.956976

Epoch: 32
Loss: 0.19977599269803287
ROC train: 0.984227	val: 0.898018	test: 0.890314
PRC train: 0.995079	val: 0.959160	test: 0.958419

Epoch: 33
Loss: 0.2005155085267695Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/bbbp/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/bbbp/random/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/bbbp/random/train_prop=0.7/bbbp_random_5_26-05_09-45-14  ]
[ Using Seed :  5  ]
[ Using device :  cuda:1  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
randomly split
Data(edge_attr=[58, 2], edge_index=[2, 58], id=[1], x=[27, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6025327996457946
ROC train: 0.819217	val: 0.764435	test: 0.797046
PRC train: 0.919904	val: 0.891210	test: 0.907844

Epoch: 2
Loss: 0.48097549279466517
ROC train: 0.849401	val: 0.776957	test: 0.806932
PRC train: 0.937912	val: 0.907185	test: 0.918899

Epoch: 3
Loss: 0.4311009596136411
ROC train: 0.869499	val: 0.784029	test: 0.821941
PRC train: 0.948435	val: 0.906549	test: 0.929656

Epoch: 4
Loss: 0.3968181047355929
ROC train: 0.882902	val: 0.807072	test: 0.835805
PRC train: 0.954065	val: 0.916594	test: 0.938754

Epoch: 5
Loss: 0.3838556919515012
ROC train: 0.895440	val: 0.819101	test: 0.860759
PRC train: 0.960618	val: 0.925628	test: 0.948547

Epoch: 6
Loss: 0.36702938297842586
ROC train: 0.906135	val: 0.835971	test: 0.868716
PRC train: 0.964323	val: 0.930666	test: 0.951461

Epoch: 7
Loss: 0.34927667161007264
ROC train: 0.917819	val: 0.844435	test: 0.879867
PRC train: 0.968188	val: 0.933273	test: 0.955192

Epoch: 8
Loss: 0.3260722986324785
ROC train: 0.926993	val: 0.845652	test: 0.888547
PRC train: 0.972929	val: 0.930122	test: 0.958091

Epoch: 9
Loss: 0.3312760692913498
ROC train: 0.928535	val: 0.866000	test: 0.887462
PRC train: 0.973249	val: 0.939926	test: 0.959735

Epoch: 10
Loss: 0.32383687074206896
ROC train: 0.938337	val: 0.871159	test: 0.903737
PRC train: 0.977457	val: 0.938927	test: 0.964670

Epoch: 11
Loss: 0.2960425722827185
ROC train: 0.944136	val: 0.880319	test: 0.911995
PRC train: 0.979714	val: 0.946800	test: 0.968428

Epoch: 12
Loss: 0.2813229691504613
ROC train: 0.942774	val: 0.874928	test: 0.908318
PRC train: 0.979399	val: 0.945415	test: 0.967011

Epoch: 13
Loss: 0.27359316764555736
ROC train: 0.946174	val: 0.880000	test: 0.907414
PRC train: 0.981122	val: 0.944951	test: 0.967238

Epoch: 14
Loss: 0.27690529656969703
ROC train: 0.952567	val: 0.890000	test: 0.916034
PRC train: 0.983773	val: 0.950980	test: 0.971258

Epoch: 15
Loss: 0.2665874003191038
ROC train: 0.956406	val: 0.891913	test: 0.916637
PRC train: 0.984924	val: 0.953051	test: 0.970866

Epoch: 16
Loss: 0.26528122890586414
ROC train: 0.957636	val: 0.886174	test: 0.909042
PRC train: 0.985043	val: 0.951017	test: 0.966880

Epoch: 17
Loss: 0.2603228151479993
ROC train: 0.960976	val: 0.893072	test: 0.911935
PRC train: 0.986212	val: 0.957618	test: 0.969322

Epoch: 18
Loss: 0.25666664350837554
ROC train: 0.955366	val: 0.883855	test: 0.896986
PRC train: 0.984644	val: 0.953238	test: 0.964031

Epoch: 19
Loss: 0.2622337599598285
ROC train: 0.962864	val: 0.899275	test: 0.906691
PRC train: 0.987179	val: 0.957451	test: 0.967198

Epoch: 20
Loss: 0.24488777058125433
ROC train: 0.966070	val: 0.900203	test: 0.914587
PRC train: 0.988355	val: 0.954048	test: 0.969943

Epoch: 21
Loss: 0.243535687088871
ROC train: 0.964572	val: 0.889420	test: 0.905666
PRC train: 0.987948	val: 0.948246	test: 0.964836

Epoch: 22
Loss: 0.24712204859896217
ROC train: 0.967070	val: 0.890870	test: 0.904641
PRC train: 0.988988	val: 0.948172	test: 0.964331

Epoch: 23
Loss: 0.24473182633612164
ROC train: 0.969879	val: 0.886232	test: 0.907474
PRC train: 0.990050	val: 0.945547	test: 0.967161

Epoch: 24
Loss: 0.24459440018514592
ROC train: 0.971030	val: 0.890000	test: 0.907595
PRC train: 0.990453	val: 0.946029	test: 0.967170

Epoch: 25
Loss: 0.24258351049660856
ROC train: 0.974461	val: 0.896174	test: 0.910428
PRC train: 0.991625	val: 0.951056	test: 0.968632

Epoch: 26
Loss: 0.2329808910936796
ROC train: 0.974795	val: 0.898232	test: 0.917058
PRC train: 0.991774	val: 0.950486	test: 0.970515

Epoch: 27
Loss: 0.23897734745447274
ROC train: 0.976992	val: 0.898522	test: 0.923146
PRC train: 0.992599	val: 0.952865	test: 0.973714

Epoch: 28
Loss: 0.20521181370607286
ROC train: 0.979184	val: 0.893739	test: 0.926703
PRC train: 0.993314	val: 0.952174	test: 0.976320

Epoch: 29
Loss: 0.21259390599553832
ROC train: 0.979850	val: 0.891623	test: 0.922242
PRC train: 0.993400	val: 0.948880	test: 0.973912

Epoch: 30
Loss: 0.2156652811774377
ROC train: 0.978031	val: 0.890812	test: 0.918143
PRC train: 0.992781	val: 0.947521	test: 0.971301

Epoch: 31
Loss: 0.21659833904263306
ROC train: 0.977011	val: 0.885304	test: 0.915552
PRC train: 0.992463	val: 0.947558	test: 0.968742

Epoch: 32
Loss: 0.19198268595537538
ROC train: 0.980103	val: 0.890870	test: 0.904762
PRC train: 0.993304	val: 0.953134	test: 0.964054

Epoch: 33
Loss: 0.19483067320485636Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/bbbp/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/bbbp/random/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/bbbp/random/train_prop=0.7/bbbp_random_4_26-05_09-45-14  ]
[ Using Seed :  4  ]
[ Using device :  cuda:1  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
randomly split
Data(edge_attr=[58, 2], edge_index=[2, 58], id=[1], x=[27, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.622415274800449
ROC train: 0.813659	val: 0.728261	test: 0.787101
PRC train: 0.923788	val: 0.887326	test: 0.908598

Epoch: 2
Loss: 0.5107096836371714
ROC train: 0.846475	val: 0.762058	test: 0.817902
PRC train: 0.937098	val: 0.903722	test: 0.925097

Epoch: 3
Loss: 0.45149384134830034
ROC train: 0.870240	val: 0.782580	test: 0.841772
PRC train: 0.948008	val: 0.909012	test: 0.937710

Epoch: 4
Loss: 0.39673913282561846
ROC train: 0.884689	val: 0.807391	test: 0.859132
PRC train: 0.953896	val: 0.921675	test: 0.945168

Epoch: 5
Loss: 0.3757260785468299
ROC train: 0.899906	val: 0.818058	test: 0.871187
PRC train: 0.960801	val: 0.925184	test: 0.950627

Epoch: 6
Loss: 0.3518366304152873
ROC train: 0.913371	val: 0.847565	test: 0.887764
PRC train: 0.965928	val: 0.937137	test: 0.958237

Epoch: 7
Loss: 0.33772753731824356
ROC train: 0.912782	val: 0.840609	test: 0.890536
PRC train: 0.965604	val: 0.928430	test: 0.957147

Epoch: 8
Loss: 0.31527850226540566
ROC train: 0.926900	val: 0.865449	test: 0.901808
PRC train: 0.971801	val: 0.937933	test: 0.962237

Epoch: 9
Loss: 0.32703577749771345
ROC train: 0.930535	val: 0.859333	test: 0.903556
PRC train: 0.972695	val: 0.932429	test: 0.963927

Epoch: 10
Loss: 0.30718037717626084
ROC train: 0.933063	val: 0.853478	test: 0.900904
PRC train: 0.974141	val: 0.930515	test: 0.961063

Epoch: 11
Loss: 0.3050365490144366
ROC train: 0.939080	val: 0.869855	test: 0.899759
PRC train: 0.976819	val: 0.939943	test: 0.960915

Epoch: 12
Loss: 0.2938071429677758
ROC train: 0.943404	val: 0.879710	test: 0.908499
PRC train: 0.978633	val: 0.942649	test: 0.966124

Epoch: 13
Loss: 0.2901154582743765
ROC train: 0.943108	val: 0.869507	test: 0.911573
PRC train: 0.978438	val: 0.938395	test: 0.966885

Epoch: 14
Loss: 0.2720700466677626
ROC train: 0.950048	val: 0.880145	test: 0.923327
PRC train: 0.981608	val: 0.947386	test: 0.973165

Epoch: 15
Loss: 0.28121786120162673
ROC train: 0.951474	val: 0.874464	test: 0.919771
PRC train: 0.982221	val: 0.945378	test: 0.971345

Epoch: 16
Loss: 0.27042921829930117
ROC train: 0.953679	val: 0.882696	test: 0.919108
PRC train: 0.983051	val: 0.950015	test: 0.971350

Epoch: 17
Loss: 0.2626963827252808
ROC train: 0.957110	val: 0.889304	test: 0.918023
PRC train: 0.984366	val: 0.955425	test: 0.971449

Epoch: 18
Loss: 0.2469382372972936
ROC train: 0.958610	val: 0.889188	test: 0.913201
PRC train: 0.985357	val: 0.953229	test: 0.969386

Epoch: 19
Loss: 0.26521790679079355
ROC train: 0.959837	val: 0.891942	test: 0.917541
PRC train: 0.985814	val: 0.951747	test: 0.970866

Epoch: 20
Loss: 0.2579361167817911
ROC train: 0.959275	val: 0.877507	test: 0.919289
PRC train: 0.985197	val: 0.947964	test: 0.971816

Epoch: 21
Loss: 0.23223239199347687
ROC train: 0.961670	val: 0.881188	test: 0.912477
PRC train: 0.986444	val: 0.948139	test: 0.969262

Epoch: 22
Loss: 0.24879665852621827
ROC train: 0.964948	val: 0.887449	test: 0.907715
PRC train: 0.987651	val: 0.953705	test: 0.966771

Epoch: 23
Loss: 0.24525898017569572
ROC train: 0.962905	val: 0.892116	test: 0.903858
PRC train: 0.986652	val: 0.956930	test: 0.966570

Epoch: 24
Loss: 0.240689181402629
ROC train: 0.969954	val: 0.894406	test: 0.910066
PRC train: 0.989867	val: 0.953838	test: 0.967099

Epoch: 25
Loss: 0.22670440652392143
ROC train: 0.967200	val: 0.876377	test: 0.894274
PRC train: 0.988948	val: 0.941832	test: 0.955903

Epoch: 26
Loss: 0.2414312686553092
ROC train: 0.970550	val: 0.884319	test: 0.898071
PRC train: 0.990046	val: 0.944265	test: 0.958668

Epoch: 27
Loss: 0.2212192246685247
ROC train: 0.974681	val: 0.894609	test: 0.909162
PRC train: 0.991555	val: 0.951595	test: 0.966008

Epoch: 28
Loss: 0.21505246286144455
ROC train: 0.977481	val: 0.898464	test: 0.914165
PRC train: 0.992578	val: 0.956991	test: 0.969581

Epoch: 29
Loss: 0.23396719865746318
ROC train: 0.975462	val: 0.889014	test: 0.903677
PRC train: 0.991900	val: 0.951522	test: 0.965035

Epoch: 30
Loss: 0.20560628238762857
ROC train: 0.977734	val: 0.897884	test: 0.907595
PRC train: 0.992329	val: 0.957784	test: 0.967832

Epoch: 31
Loss: 0.20048762439122814
ROC train: 0.976793	val: 0.892145	test: 0.912236
PRC train: 0.992212	val: 0.956379	test: 0.967789

Epoch: 32
Loss: 0.20316042076447652
ROC train: 0.981214	val: 0.902116	test: 0.914045
PRC train: 0.993825	val: 0.959754	test: 0.969827

Epoch: 33
Loss: 0.2023945088641713Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/bbbp/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/bbbp/random/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphCL/bbbp/random/train_prop=0.7/bbbp_random_6_26-05_09-45-14  ]
[ Using Seed :  6  ]
[ Using device :  cuda:1  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
randomly split
Data(edge_attr=[58, 2], edge_index=[2, 58], id=[1], x=[27, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6184975092300936
ROC train: 0.817582	val: 0.748899	test: 0.799759
PRC train: 0.924455	val: 0.905446	test: 0.924770

Epoch: 2
Loss: 0.5004497133731675
ROC train: 0.844172	val: 0.773478	test: 0.811392
PRC train: 0.935471	val: 0.911701	test: 0.926989

Epoch: 3
Loss: 0.44505462574371907
ROC train: 0.872081	val: 0.813478	test: 0.832550
PRC train: 0.948681	val: 0.923161	test: 0.934249

Epoch: 4
Loss: 0.3967818601773451
ROC train: 0.890668	val: 0.816029	test: 0.846534
PRC train: 0.958150	val: 0.921346	test: 0.938984

Epoch: 5
Loss: 0.367284069870432
ROC train: 0.900342	val: 0.831188	test: 0.857263
PRC train: 0.960843	val: 0.927816	test: 0.943239

Epoch: 6
Loss: 0.3624241265902035
ROC train: 0.912560	val: 0.845304	test: 0.878963
PRC train: 0.966358	val: 0.935409	test: 0.954928

Epoch: 7
Loss: 0.3299125606456525
ROC train: 0.922035	val: 0.860290	test: 0.892526
PRC train: 0.970366	val: 0.941685	test: 0.961848

Epoch: 8
Loss: 0.31899302840742544
ROC train: 0.927785	val: 0.860609	test: 0.896323
PRC train: 0.972333	val: 0.941139	test: 0.961871

Epoch: 9
Loss: 0.3155836371929525
ROC train: 0.936959	val: 0.871101	test: 0.902833
PRC train: 0.976298	val: 0.945187	test: 0.964609

Epoch: 10
Loss: 0.2902101824672367
ROC train: 0.941309	val: 0.871710	test: 0.903376
PRC train: 0.977856	val: 0.945883	test: 0.965361

Epoch: 11
Loss: 0.2952000252941067
ROC train: 0.947031	val: 0.875855	test: 0.913261
PRC train: 0.980842	val: 0.949057	test: 0.970647

Epoch: 12
Loss: 0.2757019018479593
ROC train: 0.949683	val: 0.873130	test: 0.916878
PRC train: 0.981733	val: 0.944004	test: 0.971020

Epoch: 13
Loss: 0.27353685431170943
ROC train: 0.953737	val: 0.882232	test: 0.915913
PRC train: 0.983650	val: 0.948178	test: 0.969401

Epoch: 14
Loss: 0.25905194318876906
ROC train: 0.956974	val: 0.886754	test: 0.909102
PRC train: 0.985193	val: 0.949407	test: 0.966325

Epoch: 15
Loss: 0.27450441653736374
ROC train: 0.956213	val: 0.873246	test: 0.913683
PRC train: 0.984710	val: 0.940006	test: 0.968619

Epoch: 16
Loss: 0.25438037014515524
ROC train: 0.961056	val: 0.876667	test: 0.912297
PRC train: 0.986554	val: 0.942794	test: 0.968576

Epoch: 17
Loss: 0.2435224185606518
ROC train: 0.958708	val: 0.883391	test: 0.911392
PRC train: 0.985383	val: 0.945810	test: 0.967763

Epoch: 18
Loss: 0.261869778171606
ROC train: 0.962971	val: 0.886725	test: 0.911091
PRC train: 0.987149	val: 0.951205	test: 0.968151

Epoch: 19
Loss: 0.24618279023535936
ROC train: 0.964783	val: 0.895739	test: 0.909464
PRC train: 0.987593	val: 0.952572	test: 0.966748

Epoch: 20
Loss: 0.2386848681119922
ROC train: 0.967125	val: 0.894580	test: 0.909584
PRC train: 0.988509	val: 0.949436	test: 0.966663

Epoch: 21
Loss: 0.2270377245021635
ROC train: 0.969085	val: 0.890174	test: 0.916878
PRC train: 0.989500	val: 0.952113	test: 0.970647

Epoch: 22
Loss: 0.23031216357611836
ROC train: 0.971109	val: 0.895391	test: 0.919590
PRC train: 0.990285	val: 0.953897	test: 0.969951

Epoch: 23
Loss: 0.2127645986893771
ROC train: 0.974114	val: 0.900725	test: 0.918565
PRC train: 0.991297	val: 0.955825	test: 0.970276

Epoch: 24
Loss: 0.23657983633882074
ROC train: 0.974897	val: 0.902870	test: 0.912055
PRC train: 0.991673	val: 0.959923	test: 0.968515

Epoch: 25
Loss: 0.2111769817697152
ROC train: 0.975173	val: 0.891565	test: 0.909825
PRC train: 0.991871	val: 0.952507	test: 0.966597

Epoch: 26
Loss: 0.22373338653827454
ROC train: 0.977353	val: 0.885043	test: 0.907655
PRC train: 0.992717	val: 0.946727	test: 0.966238

Epoch: 27
Loss: 0.20755092953832144
ROC train: 0.979035	val: 0.895449	test: 0.909705
PRC train: 0.993203	val: 0.951933	test: 0.969221

Epoch: 28
Loss: 0.21947220339892023
ROC train: 0.976477	val: 0.899159	test: 0.908981
PRC train: 0.992414	val: 0.949391	test: 0.967140

Epoch: 29
Loss: 0.21670229755387638
ROC train: 0.976790	val: 0.882000	test: 0.898493
PRC train: 0.992575	val: 0.942392	test: 0.962250

Epoch: 30
Loss: 0.20970384285728927
ROC train: 0.974193	val: 0.883333	test: 0.889391
PRC train: 0.991606	val: 0.949255	test: 0.956097

Epoch: 31
Loss: 0.2093861003732327
ROC train: 0.976059	val: 0.897188	test: 0.902230
PRC train: 0.992155	val: 0.956187	test: 0.964369

Epoch: 32
Loss: 0.19939866853564917
ROC train: 0.979187	val: 0.890986	test: 0.897348
PRC train: 0.993257	val: 0.951220	test: 0.963669

Epoch: 33
Loss: 0.18977442407793824
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/bbbp/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/bbbp/random/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/bbbp/random/train_prop=0.8/bbbp_random_4_26-05_09-45-14  ]
[ Using Seed :  4  ]
[ Using device :  cuda:2  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
randomly split
Data(edge_attr=[58, 2], edge_index=[2, 58], id=[1], x=[27, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6245616638859913
ROC train: 0.809175	val: 0.710310	test: 0.802459
PRC train: 0.921064	val: 0.871443	test: 0.902812

Epoch: 2
Loss: 0.4963576067249212
ROC train: 0.850520	val: 0.722900	test: 0.841837
PRC train: 0.938976	val: 0.884606	test: 0.931014

Epoch: 3
Loss: 0.4342186525371875
ROC train: 0.877512	val: 0.738007	test: 0.861722
PRC train: 0.951296	val: 0.890563	test: 0.942374

Epoch: 4
Loss: 0.3924385291364197
ROC train: 0.896266	val: 0.772992	test: 0.878990
PRC train: 0.959197	val: 0.901910	test: 0.948012

Epoch: 5
Loss: 0.3563297489703138
ROC train: 0.899531	val: 0.793400	test: 0.878990
PRC train: 0.959670	val: 0.917981	test: 0.944100

Epoch: 6
Loss: 0.3539461542055654
ROC train: 0.917007	val: 0.817387	test: 0.896913
PRC train: 0.967229	val: 0.933142	test: 0.955820

Epoch: 7
Loss: 0.3350133337182087
ROC train: 0.923359	val: 0.807978	test: 0.896782
PRC train: 0.970164	val: 0.924588	test: 0.956813

Epoch: 8
Loss: 0.3230097834798317
ROC train: 0.931254	val: 0.819772	test: 0.899267
PRC train: 0.973086	val: 0.933616	test: 0.958237

Epoch: 9
Loss: 0.32212940962590414
ROC train: 0.935998	val: 0.833289	test: 0.902015
PRC train: 0.974797	val: 0.933515	test: 0.949720

Epoch: 10
Loss: 0.30230305035920596
ROC train: 0.940240	val: 0.853697	test: 0.908817
PRC train: 0.976572	val: 0.946777	test: 0.956878

Epoch: 11
Loss: 0.29237781280836395
ROC train: 0.948211	val: 0.874238	test: 0.927002
PRC train: 0.980360	val: 0.952075	test: 0.966772

Epoch: 12
Loss: 0.29837679146182683
ROC train: 0.949386	val: 0.874371	test: 0.925301
PRC train: 0.980887	val: 0.957021	test: 0.969059

Epoch: 13
Loss: 0.2738783882446475
ROC train: 0.951124	val: 0.864166	test: 0.919806
PRC train: 0.981278	val: 0.951814	test: 0.967601

Epoch: 14
Loss: 0.2652737086322735
ROC train: 0.950489	val: 0.862709	test: 0.918367
PRC train: 0.981565	val: 0.943861	test: 0.964341

Epoch: 15
Loss: 0.2638020689164657
ROC train: 0.955984	val: 0.865624	test: 0.921507
PRC train: 0.983539	val: 0.949104	test: 0.965419

Epoch: 16
Loss: 0.2584153567421393
ROC train: 0.956656	val: 0.874901	test: 0.921900
PRC train: 0.984108	val: 0.956921	test: 0.967308

Epoch: 17
Loss: 0.26058127885818744
ROC train: 0.963436	val: 0.891996	test: 0.938514
PRC train: 0.986579	val: 0.964606	test: 0.974260

Epoch: 18
Loss: 0.25870143414982366
ROC train: 0.965085	val: 0.898092	test: 0.931973
PRC train: 0.987229	val: 0.966676	test: 0.972853

Epoch: 19
Loss: 0.25104765735424495
ROC train: 0.961399	val: 0.885502	test: 0.912088
PRC train: 0.985425	val: 0.961309	test: 0.962209

Epoch: 20
Loss: 0.26366237631263106
ROC train: 0.964590	val: 0.888948	test: 0.919021
PRC train: 0.987227	val: 0.963049	test: 0.969498

Epoch: 21
Loss: 0.24205022158324585
ROC train: 0.964519	val: 0.871853	test: 0.909210
PRC train: 0.987655	val: 0.952527	test: 0.963293

Epoch: 22
Loss: 0.2618717008645437
ROC train: 0.968720	val: 0.888815	test: 0.917452
PRC train: 0.988781	val: 0.962790	test: 0.967863

Epoch: 23
Loss: 0.25116483551391305
ROC train: 0.965520	val: 0.868937	test: 0.913265
PRC train: 0.987517	val: 0.950049	test: 0.963910

Epoch: 24
Loss: 0.22445280488297767
ROC train: 0.969704	val: 0.886165	test: 0.915489
PRC train: 0.989568	val: 0.950513	test: 0.964615

Epoch: 25
Loss: 0.21681670692284855
ROC train: 0.971837	val: 0.892128	test: 0.921245
PRC train: 0.990115	val: 0.958871	test: 0.964500

Epoch: 26
Loss: 0.20941147511719452
ROC train: 0.973426	val: 0.892128	test: 0.933543
PRC train: 0.990280	val: 0.960703	test: 0.971128

Epoch: 27
Loss: 0.2092321727023408
ROC train: 0.975140	val: 0.893188	test: 0.922946
PRC train: 0.991502	val: 0.964642	test: 0.970039

Epoch: 28
Loss: 0.2102627532150913
ROC train: 0.976601	val: 0.886430	test: 0.922292
PRC train: 0.992315	val: 0.961937	test: 0.963258

Epoch: 29
Loss: 0.22029409608027348
ROC train: 0.975240	val: 0.890140	test: 0.922554
PRC train: 0.991715	val: 0.961151	test: 0.957075

Epoch: 30
Loss: 0.22007333722520042
ROC train: 0.976718	val: 0.916777	test: 0.925824
PRC train: 0.992362	val: 0.972497	test: 0.968743

Epoch: 31
Loss: 0.21735521295251645
ROC train: 0.977483	val: 0.903127	test: 0.920068
PRC train: 0.992638	val: 0.967987	test: 0.965771

Epoch: 32
Loss: 0.20139083755990755
ROC train: 0.974934	val: 0.881262	test: 0.909602
PRC train: 0.991567	val: 0.960139	test: 0.954031

Epoch: 33
Loss: 0.23345607741832847Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/bbbp/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/bbbp/random/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/bbbp/random/train_prop=0.8/bbbp_random_5_26-05_09-45-14  ]
[ Using Seed :  5  ]
[ Using device :  cuda:2  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
randomly split
Data(edge_attr=[58, 2], edge_index=[2, 58], id=[1], x=[27, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.5913203723527672
ROC train: 0.824230	val: 0.717996	test: 0.821298
PRC train: 0.926028	val: 0.887931	test: 0.914707

Epoch: 2
Loss: 0.47045146607584226
ROC train: 0.836567	val: 0.709250	test: 0.825092
PRC train: 0.925579	val: 0.857270	test: 0.910143

Epoch: 3
Loss: 0.41086692252230933
ROC train: 0.872227	val: 0.728995	test: 0.845631
PRC train: 0.948881	val: 0.882627	test: 0.932859

Epoch: 4
Loss: 0.38374467663567813
ROC train: 0.887999	val: 0.737212	test: 0.859105
PRC train: 0.956383	val: 0.886748	test: 0.940213

Epoch: 5
Loss: 0.361248922638313
ROC train: 0.902861	val: 0.753777	test: 0.878467
PRC train: 0.962571	val: 0.895427	test: 0.949154

Epoch: 6
Loss: 0.36486800286963206
ROC train: 0.912479	val: 0.786774	test: 0.884746
PRC train: 0.965612	val: 0.917770	test: 0.951088

Epoch: 7
Loss: 0.3312677028554131
ROC train: 0.923459	val: 0.801749	test: 0.904500
PRC train: 0.969049	val: 0.923503	test: 0.960096

Epoch: 8
Loss: 0.3207763319562326
ROC train: 0.930971	val: 0.825603	test: 0.916405
PRC train: 0.972066	val: 0.928598	test: 0.963418

Epoch: 9
Loss: 0.30238801034101914
ROC train: 0.935309	val: 0.835277	test: 0.922161
PRC train: 0.974562	val: 0.937536	test: 0.964153

Epoch: 10
Loss: 0.3039311455632142
ROC train: 0.938434	val: 0.846806	test: 0.932234
PRC train: 0.976603	val: 0.944451	test: 0.972471

Epoch: 11
Loss: 0.30025829996024556
ROC train: 0.944270	val: 0.838325	test: 0.932104
PRC train: 0.978012	val: 0.938686	test: 0.970564

Epoch: 12
Loss: 0.27836165789819184
ROC train: 0.942183	val: 0.827723	test: 0.927656
PRC train: 0.977069	val: 0.928461	test: 0.970937

Epoch: 13
Loss: 0.2872767067654345
ROC train: 0.948126	val: 0.840710	test: 0.924778
PRC train: 0.979523	val: 0.939208	test: 0.966705

Epoch: 14
Loss: 0.26390888447794897
ROC train: 0.949016	val: 0.850649	test: 0.923993
PRC train: 0.980768	val: 0.943027	test: 0.965290

Epoch: 15
Loss: 0.274836817110993
ROC train: 0.951304	val: 0.846409	test: 0.936028
PRC train: 0.982098	val: 0.939104	test: 0.972724

Epoch: 16
Loss: 0.26096926670872084
ROC train: 0.954608	val: 0.868275	test: 0.936944
PRC train: 0.982779	val: 0.953512	test: 0.973199

Epoch: 17
Loss: 0.26493192481825634
ROC train: 0.955970	val: 0.869732	test: 0.927525
PRC train: 0.982940	val: 0.953319	test: 0.968594

Epoch: 18
Loss: 0.26974420998299925
ROC train: 0.959740	val: 0.853167	test: 0.927002
PRC train: 0.984769	val: 0.949636	test: 0.970069

Epoch: 19
Loss: 0.243506223358772
ROC train: 0.961578	val: 0.858071	test: 0.926086
PRC train: 0.985448	val: 0.949543	test: 0.968636

Epoch: 20
Loss: 0.27057804014417325
ROC train: 0.964677	val: 0.862046	test: 0.927263
PRC train: 0.986848	val: 0.947732	test: 0.970204

Epoch: 21
Loss: 0.26390060944424515
ROC train: 0.964208	val: 0.884177	test: 0.924647
PRC train: 0.986432	val: 0.959434	test: 0.969167

Epoch: 22
Loss: 0.24517050192661394
ROC train: 0.963276	val: 0.870527	test: 0.918106
PRC train: 0.986318	val: 0.947987	test: 0.954646

Epoch: 23
Loss: 0.26408909095984057
ROC train: 0.965734	val: 0.878214	test: 0.921638
PRC train: 0.987256	val: 0.954471	test: 0.965279

Epoch: 24
Loss: 0.23399007952173984
ROC train: 0.963482	val: 0.879936	test: 0.928964
PRC train: 0.986749	val: 0.958374	test: 0.972783

Epoch: 25
Loss: 0.2333106877904893
ROC train: 0.967153	val: 0.879539	test: 0.933150
PRC train: 0.988223	val: 0.956420	test: 0.974094

Epoch: 26
Loss: 0.22956447713344943
ROC train: 0.971678	val: 0.883117	test: 0.936421
PRC train: 0.990324	val: 0.957373	test: 0.976094

Epoch: 27
Loss: 0.23798573415495264
ROC train: 0.974636	val: 0.898357	test: 0.935897
PRC train: 0.991019	val: 0.962502	test: 0.975757

Epoch: 28
Loss: 0.22894592111947967
ROC train: 0.970457	val: 0.891333	test: 0.931450
PRC train: 0.989648	val: 0.960551	test: 0.973217

Epoch: 29
Loss: 0.22205572443697638
ROC train: 0.970955	val: 0.884707	test: 0.939168
PRC train: 0.990227	val: 0.960017	test: 0.976309

Epoch: 30
Loss: 0.22488693609546884
ROC train: 0.975859	val: 0.898489	test: 0.929880
PRC train: 0.991891	val: 0.965800	test: 0.975049

Epoch: 31
Loss: 0.2109446863741862
ROC train: 0.976671	val: 0.892658	test: 0.941130
PRC train: 0.992303	val: 0.965033	test: 0.979547

Epoch: 32
Loss: 0.221274060504689
ROC train: 0.975369	val: 0.886165	test: 0.928179
PRC train: 0.991849	val: 0.961389	test: 0.971997

Epoch: 33
Loss: 0.2184514852155088Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphCL/bbbp/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/split/GraphCL/bbbp/random/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphCL/bbbp/random/train_prop=0.8/bbbp_random_6_26-05_09-45-14  ]
[ Using Seed :  6  ]
[ Using device :  cuda:2  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
randomly split
Data(edge_attr=[58, 2], edge_index=[2, 58], id=[1], x=[27, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6082752924093539
ROC train: 0.820444	val: 0.719056	test: 0.811094
PRC train: 0.927656	val: 0.892458	test: 0.919322

Epoch: 2
Loss: 0.476036903845265
ROC train: 0.845942	val: 0.722502	test: 0.823391
PRC train: 0.937244	val: 0.882057	test: 0.920549

Epoch: 3
Loss: 0.4179220652498382
ROC train: 0.880230	val: 0.743175	test: 0.851779
PRC train: 0.953144	val: 0.894167	test: 0.931224

Epoch: 4
Loss: 0.3718158086103108
ROC train: 0.888525	val: 0.759608	test: 0.870094
PRC train: 0.955904	val: 0.899557	test: 0.944885

Epoch: 5
Loss: 0.3516514475799202
ROC train: 0.898522	val: 0.788232	test: 0.895343
PRC train: 0.960398	val: 0.913003	test: 0.957642

Epoch: 6
Loss: 0.3447136310705675
ROC train: 0.919285	val: 0.811556	test: 0.912873
PRC train: 0.968075	val: 0.920309	test: 0.962700

Epoch: 7
Loss: 0.329280486389591
ROC train: 0.923392	val: 0.817652	test: 0.908425
PRC train: 0.968578	val: 0.923904	test: 0.958972

Epoch: 8
Loss: 0.3151589182504989
ROC train: 0.921389	val: 0.818844	test: 0.898090
PRC train: 0.967370	val: 0.927069	test: 0.953800

Epoch: 9
Loss: 0.2895337640677961
ROC train: 0.934644	val: 0.833157	test: 0.906724
PRC train: 0.971787	val: 0.938630	test: 0.954882

Epoch: 10
Loss: 0.29885481582287693
ROC train: 0.940441	val: 0.839518	test: 0.924254
PRC train: 0.976498	val: 0.938547	test: 0.965407

Epoch: 11
Loss: 0.2983047717563948
ROC train: 0.943580	val: 0.846144	test: 0.927786
PRC train: 0.978046	val: 0.942139	test: 0.968684

Epoch: 12
Loss: 0.28685288719542296
ROC train: 0.945266	val: 0.849589	test: 0.921245
PRC train: 0.979121	val: 0.940553	test: 0.965839

Epoch: 13
Loss: 0.2729471889334723
ROC train: 0.944784	val: 0.834349	test: 0.919545
PRC train: 0.978998	val: 0.929670	test: 0.965403

Epoch: 14
Loss: 0.2606080944536881
ROC train: 0.951831	val: 0.847601	test: 0.926609
PRC train: 0.981731	val: 0.939871	test: 0.969866

Epoch: 15
Loss: 0.2576823918539034
ROC train: 0.954229	val: 0.856878	test: 0.929880
PRC train: 0.982549	val: 0.944857	test: 0.972620

Epoch: 16
Loss: 0.2616407953787953
ROC train: 0.958527	val: 0.863371	test: 0.923469
PRC train: 0.984431	val: 0.947008	test: 0.970026

Epoch: 17
Loss: 0.24237370027660982
ROC train: 0.960911	val: 0.872250	test: 0.920460
PRC train: 0.985506	val: 0.950868	test: 0.967107

Epoch: 18
Loss: 0.25341356547926247
ROC train: 0.962712	val: 0.858733	test: 0.924908
PRC train: 0.986346	val: 0.947089	test: 0.970744

Epoch: 19
Loss: 0.24992107464497493
ROC train: 0.959035	val: 0.855288	test: 0.940476
PRC train: 0.984855	val: 0.942088	test: 0.978196

Epoch: 20
Loss: 0.25194441202693946
ROC train: 0.962626	val: 0.869865	test: 0.939299
PRC train: 0.985902	val: 0.950577	test: 0.977762

Epoch: 21
Loss: 0.24219526370467168
ROC train: 0.963383	val: 0.872913	test: 0.928571
PRC train: 0.986486	val: 0.953096	test: 0.973746

Epoch: 22
Loss: 0.26605794688926976
ROC train: 0.965777	val: 0.887358	test: 0.930403
PRC train: 0.988076	val: 0.957936	test: 0.969980

Epoch: 23
Loss: 0.2340042909745807
ROC train: 0.964682	val: 0.893321	test: 0.931057
PRC train: 0.987795	val: 0.964506	test: 0.976274

Epoch: 24
Loss: 0.22538817600012875
ROC train: 0.969083	val: 0.884310	test: 0.937729
PRC train: 0.989288	val: 0.958295	test: 0.977500

Epoch: 25
Loss: 0.23581556581423166
ROC train: 0.971741	val: 0.875166	test: 0.942700
PRC train: 0.990469	val: 0.953474	test: 0.979670

Epoch: 26
Loss: 0.22776386998713202
ROC train: 0.971915	val: 0.886562	test: 0.932758
PRC train: 0.990445	val: 0.957434	test: 0.975959

Epoch: 27
Loss: 0.23032988782045088
ROC train: 0.973272	val: 0.883647	test: 0.919676
PRC train: 0.990727	val: 0.956280	test: 0.969447

Epoch: 28
Loss: 0.21367041175387186
ROC train: 0.973339	val: 0.875431	test: 0.910518
PRC train: 0.990832	val: 0.953846	test: 0.962969

Epoch: 29
Loss: 0.23005429167059474
ROC train: 0.975088	val: 0.889345	test: 0.920460
PRC train: 0.991606	val: 0.961400	test: 0.969395

Epoch: 30
Loss: 0.2072790372379277
ROC train: 0.974436	val: 0.877816	test: 0.919021
PRC train: 0.991411	val: 0.956675	test: 0.965575

Epoch: 31
Loss: 0.21392715014544766
ROC train: 0.974188	val: 0.891996	test: 0.917713
PRC train: 0.991410	val: 0.959167	test: 0.968677

Epoch: 32
Loss: 0.1975814758671963
ROC train: 0.974521	val: 0.899417	test: 0.939822
PRC train: 0.991620	val: 0.962491	test: 0.978799

Epoch: 33
Loss: 0.20951199766821252

ROC train: 0.987085	val: 0.901961	test: 0.907376
PRC train: 0.995913	val: 0.956602	test: 0.968167

Epoch: 34
Loss: 0.19004608640968876
ROC train: 0.985845	val: 0.894586	test: 0.901240
PRC train: 0.995572	val: 0.953806	test: 0.965052

Epoch: 35
Loss: 0.19196916184426255
ROC train: 0.986106	val: 0.891935	test: 0.898451
PRC train: 0.995599	val: 0.953002	test: 0.964348

Epoch: 36
Loss: 0.16707488545745078
ROC train: 0.988302	val: 0.894144	test: 0.901043
PRC train: 0.996261	val: 0.954491	test: 0.965337

Epoch: 37
Loss: 0.17982053591550326
ROC train: 0.988901	val: 0.894586	test: 0.897106
PRC train: 0.996499	val: 0.955670	test: 0.961733

Epoch: 38
Loss: 0.1771056436690137
ROC train: 0.989953	val: 0.889963	test: 0.909312
PRC train: 0.996894	val: 0.949616	test: 0.967335

Epoch: 39
Loss: 0.1810894265767555
ROC train: 0.987689	val: 0.890575	test: 0.906293
PRC train: 0.996156	val: 0.946457	test: 0.966277

Epoch: 40
Loss: 0.18435184436119822
ROC train: 0.987217	val: 0.890983	test: 0.902815
PRC train: 0.996057	val: 0.948512	test: 0.960307

Epoch: 41
Loss: 0.16320204737160787
ROC train: 0.986424	val: 0.893736	test: 0.894842
PRC train: 0.995896	val: 0.952528	test: 0.958774

Epoch: 42
Loss: 0.17123812544357708
ROC train: 0.990009	val: 0.894008	test: 0.891135
PRC train: 0.996922	val: 0.955149	test: 0.957444

Epoch: 43
Loss: 0.16605393805404756
ROC train: 0.991196	val: 0.893141	test: 0.900617
PRC train: 0.997309	val: 0.956950	test: 0.963444

Epoch: 44
Loss: 0.15689353035064185
ROC train: 0.992153	val: 0.888876	test: 0.908163
PRC train: 0.997611	val: 0.954423	test: 0.967862

Epoch: 45
Loss: 0.16107957194981265
ROC train: 0.992734	val: 0.886598	test: 0.905407
PRC train: 0.997777	val: 0.952559	test: 0.965356

Epoch: 46
Loss: 0.16774007734483232
ROC train: 0.992158	val: 0.889793	test: 0.897565
PRC train: 0.997643	val: 0.952334	test: 0.961261

Epoch: 47
Loss: 0.16413029239876137
ROC train: 0.993227	val: 0.890201	test: 0.898976
PRC train: 0.997948	val: 0.953770	test: 0.960933

Epoch: 48
Loss: 0.14228362659720445
ROC train: 0.994048	val: 0.900296	test: 0.904423
PRC train: 0.998184	val: 0.959117	test: 0.966217

Epoch: 49
Loss: 0.1440815149317698
ROC train: 0.993865	val: 0.901417	test: 0.906096
PRC train: 0.998142	val: 0.959844	test: 0.967303

Epoch: 50
Loss: 0.1691162033088654
ROC train: 0.993586	val: 0.893498	test: 0.902323
PRC train: 0.998041	val: 0.956203	test: 0.965243

Epoch: 51
Loss: 0.1434575337341598
ROC train: 0.991740	val: 0.896149	test: 0.901503
PRC train: 0.997518	val: 0.956234	test: 0.964747

Epoch: 52
Loss: 0.14479570625623792
ROC train: 0.994338	val: 0.902811	test: 0.910821
PRC train: 0.998275	val: 0.960210	test: 0.966722

Epoch: 53
Loss: 0.12897892882553263
ROC train: 0.994723	val: 0.897951	test: 0.912199
PRC train: 0.998383	val: 0.958521	test: 0.967377

Epoch: 54
Loss: 0.1381633458313458
ROC train: 0.992756	val: 0.889114	test: 0.901634
PRC train: 0.997745	val: 0.956725	test: 0.961666

Epoch: 55
Loss: 0.1327075823409834
ROC train: 0.994865	val: 0.891459	test: 0.904718
PRC train: 0.998414	val: 0.953813	test: 0.963834

Epoch: 56
Loss: 0.15725793750475786
ROC train: 0.994993	val: 0.894416	test: 0.911051
PRC train: 0.998484	val: 0.953496	test: 0.967794

Epoch: 57
Loss: 0.14265429504247354
ROC train: 0.995198	val: 0.886123	test: 0.908754
PRC train: 0.998556	val: 0.953986	test: 0.965171

Epoch: 58
Loss: 0.12908795464105444
ROC train: 0.994569	val: 0.887618	test: 0.904193
PRC train: 0.998377	val: 0.954673	test: 0.964238

Epoch: 59
Loss: 0.1407002799920693
ROC train: 0.995250	val: 0.893600	test: 0.899108
PRC train: 0.998550	val: 0.955946	test: 0.961803

Epoch: 60
Loss: 0.11846347672023327
ROC train: 0.994265	val: 0.893566	test: 0.901175
PRC train: 0.998261	val: 0.955624	test: 0.964563

Epoch: 61
Loss: 0.128079173582404
ROC train: 0.995257	val: 0.895062	test: 0.907868
PRC train: 0.998581	val: 0.955599	test: 0.968552

Epoch: 62
Loss: 0.1327057121956084
ROC train: 0.996521	val: 0.900330	test: 0.910066
PRC train: 0.998953	val: 0.958781	test: 0.969462

Epoch: 63
Loss: 0.1387676908020982
ROC train: 0.995875	val: 0.897747	test: 0.904882
PRC train: 0.998739	val: 0.956009	test: 0.967026

Epoch: 64
Loss: 0.1387995298984263
ROC train: 0.993854	val: 0.886564	test: 0.901601
PRC train: 0.998120	val: 0.953148	test: 0.963026

Epoch: 65
Loss: 0.14115340216626787
ROC train: 0.994494	val: 0.889725	test: 0.907015
PRC train: 0.998324	val: 0.953621	test: 0.966675

Epoch: 66
Loss: 0.13574848175080909
ROC train: 0.996166	val: 0.880719	test: 0.896712
PRC train: 0.998835	val: 0.948667	test: 0.958790

Epoch: 67
Loss: 0.12921211222732953
ROC train: 0.996389	val: 0.890966	test: 0.895302
PRC train: 0.998899	val: 0.957048	test: 0.961158

Epoch: 68
Loss: 0.12321152619320679
ROC train: 0.996244	val: 0.896353	test: 0.899862
PRC train: 0.998870	val: 0.958234	test: 0.964767

Epoch: 69
Loss: 0.13411345736267757
ROC train: 0.996050	val: 0.894246	test: 0.900748
PRC train: 0.998820	val: 0.956933	test: 0.965656

Epoch: 70
Loss: 0.1304937073055601
ROC train: 0.996369	val: 0.891323	test: 0.909640
PRC train: 0.998915	val: 0.956816	test: 0.968356

Epoch: 71
Loss: 0.1161370723871056
ROC train: 0.996229	val: 0.887958	test: 0.903537
PRC train: 0.998880	val: 0.954253	test: 0.966678

Epoch: 72
Loss: 0.13292483840482439
ROC train: 0.995594	val: 0.882622	test: 0.896253
PRC train: 0.998671	val: 0.952645	test: 0.963213

Epoch: 73
Loss: 0.12944792852970313
ROC train: 0.996542	val: 0.889827	test: 0.898419
PRC train: 0.998950	val: 0.956288	test: 0.964335

Epoch: 74
Loss: 0.11150734562405898
ROC train: 0.997242	val: 0.894076	test: 0.905309
PRC train: 0.999166	val: 0.955629	test: 0.967000

Epoch: 75
Loss: 0.11155534854482958
ROC train: 0.997973	val: 0.889080	test: 0.904882
PRC train: 0.999389	val: 0.952597	test: 0.966535

Epoch: 76
Loss: 0.09788862197448818
ROC train: 0.997915	val: 0.888808	test: 0.906457
PRC train: 0.999375	val: 0.951762	test: 0.967546

Epoch: 77
Loss: 0.10839902767835272
ROC train: 0.996875	val: 0.876640	test: 0.899928
PRC train: 0.999057	val: 0.945830	test: 0.964591

Epoch: 78
Loss: 0.11176384316687323
ROC train: 0.997212	val: 0.876776	test: 0.899633
PRC train: 0.999151	val: 0.948256	test: 0.962798

Epoch: 79
Loss: 0.11374095058094383
ROC train: 0.996921	val: 0.885511	test: 0.902782
PRC train: 0.999075	val: 0.953512	test: 0.965615

Epoch: 80
Loss: 0.11673824534702455
ROC train: 0.997744	val: 0.889997	test: 0.915939
PRC train: 0.999312	val: 0.953309	test: 0.970676

Epoch: 81
Loss: 0.10507541252055463
ROC train: 0.998068	val: 0.892886	test: 0.916563
PRC train: 0.999414	val: 0.955087	test: 0.971665

Epoch: 82
Loss: 0.11009416787435686
ROC train: 0.997930	val: 0.889589	test: 0.904653
PRC train: 0.999375	val: 0.954725	test: 0.966778

Epoch: 83
Loss: 0.11002274428256545
ROC train: 0.998135	val: 0.880990	test: 0.896351
PRC train: 0.999433	val: 0.949058	test: 0.961726

Epoch: 84
Loss: 0.10412072306382592
ROC train: 0.998068	val: 0.884423	test: 0.897894
PRC train: 0.999410	val: 0.951543	test: 0.964947

Epoch: 85
Loss: 0.10167361496037078
ROC train: 0.998042	val: 0.886734	test: 0.903242
PRC train: 0.999400	val: 0.954998	test: 0.967277

Epoch: 86
Loss: 0.1081129523415919
ROC train: 0.997962	val: 0.885715	test: 0.901011
PRC train: 0.999378	val: 0.955854	test: 0.963798

Epoch: 87
Loss: 0.11267986162524055
ROC train: 0.998183	val: 0.885307	test: 0.894022
PRC train: 0.999451	val: 0.952031	test: 0.958935

Epoch: 88
Loss: 0.09898479076599245
ROC train: 0.998397	val: 0.887754	test: 0.897631
PRC train: 0.999517	val: 0.951391	test: 0.961961

Epoch: 89
Loss: 0.10627855974243201
ROC train: 0.998410	val: 0.891561	test: 0.905506
PRC train: 0.999518	val: 0.955210	test: 0.966726

Epoch: 90
Loss: 0.09185510125258374
ROC train: 0.998630	val: 0.885647	test: 0.900453
PRC train: 0.999587	val: 0.953331	test: 0.964618

Epoch: 91
Loss: 0.10574019270841699
ROC train: 0.998399	val: 0.884151	test: 0.888838
PRC train: 0.999512	val: 0.952418	test: 0.958289

Epoch: 92
Loss: 0.10601339829484417
ROC train: 0.998034	val: 0.883336	test: 0.881029
PRC train: 0.999401	val: 0.951334	test: 0.954875

Epoch: 93
Loss: 0.09736136747103322
ROC train: 0.998381	val: 0.882350	test: 0.887427
PRC train: 0.999513	val: 0.951223	test: 0.957483

Epoch: 94
Loss: 0.09832798352624945
ROC train: 0.984618	val: 0.893158	test: 0.897598
PRC train: 0.995206	val: 0.956859	test: 0.961372

Epoch: 34
Loss: 0.18751724308986897
ROC train: 0.987146	val: 0.895605	test: 0.897467
PRC train: 0.995973	val: 0.958160	test: 0.959875

Epoch: 35
Loss: 0.18644354321133977
ROC train: 0.988160	val: 0.898120	test: 0.899272
PRC train: 0.996309	val: 0.957506	test: 0.959534

Epoch: 36
Loss: 0.18604185034490123
ROC train: 0.988211	val: 0.890371	test: 0.898681
PRC train: 0.996314	val: 0.952708	test: 0.957605

Epoch: 37
Loss: 0.17598239503994473
ROC train: 0.989165	val: 0.890643	test: 0.893792
PRC train: 0.996650	val: 0.954714	test: 0.956315

Epoch: 38
Loss: 0.19021545712327415
ROC train: 0.988801	val: 0.894790	test: 0.896614
PRC train: 0.996503	val: 0.956418	test: 0.959290

Epoch: 39
Loss: 0.1725381974958856
ROC train: 0.989160	val: 0.892207	test: 0.894711
PRC train: 0.996638	val: 0.955947	test: 0.958040

Epoch: 40
Loss: 0.18655329349687497
ROC train: 0.989543	val: 0.893498	test: 0.898484
PRC train: 0.996720	val: 0.955468	test: 0.960503

Epoch: 41
Loss: 0.1699633989398846
ROC train: 0.990104	val: 0.895809	test: 0.900387
PRC train: 0.996882	val: 0.955756	test: 0.962107

Epoch: 42
Loss: 0.16198742661933366
ROC train: 0.989655	val: 0.895435	test: 0.901667
PRC train: 0.996765	val: 0.955564	test: 0.962193

Epoch: 43
Loss: 0.17839869983515932
ROC train: 0.989089	val: 0.890541	test: 0.896351
PRC train: 0.996643	val: 0.954300	test: 0.957987

Epoch: 44
Loss: 0.16747371606316003
ROC train: 0.991522	val: 0.898392	test: 0.902979
PRC train: 0.997377	val: 0.955749	test: 0.959913

Epoch: 45
Loss: 0.1556935541121837
ROC train: 0.991125	val: 0.894416	test: 0.898976
PRC train: 0.997252	val: 0.953982	test: 0.957904

Epoch: 46
Loss: 0.16824590243665277
ROC train: 0.993102	val: 0.897679	test: 0.901437
PRC train: 0.997902	val: 0.956803	test: 0.958313

Epoch: 47
Loss: 0.1518686082862435
ROC train: 0.992358	val: 0.898800	test: 0.898123
PRC train: 0.997625	val: 0.958015	test: 0.958001

Epoch: 48
Loss: 0.174188326535553
ROC train: 0.993028	val: 0.899786	test: 0.902323
PRC train: 0.997841	val: 0.959348	test: 0.962094

Epoch: 49
Loss: 0.1497223106152052
ROC train: 0.993379	val: 0.895639	test: 0.908295
PRC train: 0.997961	val: 0.958531	test: 0.960925

Epoch: 50
Loss: 0.16942809694014754
ROC train: 0.993495	val: 0.897611	test: 0.907573
PRC train: 0.997998	val: 0.957591	test: 0.960650

Epoch: 51
Loss: 0.1551558396298139
ROC train: 0.993878	val: 0.894450	test: 0.906260
PRC train: 0.998094	val: 0.954426	test: 0.962449

Epoch: 52
Loss: 0.1638765971153498
ROC train: 0.992168	val: 0.884015	test: 0.899140
PRC train: 0.997603	val: 0.949465	test: 0.961824

Epoch: 53
Loss: 0.14417878866092698
ROC train: 0.993365	val: 0.889725	test: 0.895302
PRC train: 0.997953	val: 0.953833	test: 0.959113

Epoch: 54
Loss: 0.14486546969201836
ROC train: 0.993739	val: 0.890575	test: 0.893005
PRC train: 0.998070	val: 0.955315	test: 0.957262

Epoch: 55
Loss: 0.13446072044082713
ROC train: 0.992877	val: 0.887924	test: 0.890675
PRC train: 0.997825	val: 0.954339	test: 0.955011

Epoch: 56
Loss: 0.15188163138638186
ROC train: 0.995410	val: 0.891969	test: 0.898287
PRC train: 0.998599	val: 0.955683	test: 0.959795

Epoch: 57
Loss: 0.14803047920279072
ROC train: 0.993417	val: 0.890847	test: 0.892217
PRC train: 0.997931	val: 0.955641	test: 0.957278

Epoch: 58
Loss: 0.13873677161860257
ROC train: 0.995164	val: 0.886904	test: 0.904587
PRC train: 0.998503	val: 0.952915	test: 0.958799

Epoch: 59
Loss: 0.11995345783485509
ROC train: 0.993426	val: 0.885613	test: 0.889330
PRC train: 0.997996	val: 0.953544	test: 0.951789

Epoch: 60
Loss: 0.1351269714646411
ROC train: 0.994902	val: 0.889250	test: 0.888182
PRC train: 0.998455	val: 0.955864	test: 0.955905

Epoch: 61
Loss: 0.146546392998115
ROC train: 0.993370	val: 0.881398	test: 0.885688
PRC train: 0.997963	val: 0.953551	test: 0.957565

Epoch: 62
Loss: 0.14121246528890474
ROC train: 0.994156	val: 0.882350	test: 0.890544
PRC train: 0.998218	val: 0.954638	test: 0.958337

Epoch: 63
Loss: 0.14124405663415623
ROC train: 0.994753	val: 0.875620	test: 0.884343
PRC train: 0.998394	val: 0.951500	test: 0.952693

Epoch: 64
Loss: 0.12716963544554857
ROC train: 0.995527	val: 0.880769	test: 0.881521
PRC train: 0.998634	val: 0.953187	test: 0.951949

Epoch: 65
Loss: 0.14523539371518493
ROC train: 0.996222	val: 0.887346	test: 0.882473
PRC train: 0.998833	val: 0.956039	test: 0.953617

Epoch: 66
Loss: 0.12155796086112707
ROC train: 0.995924	val: 0.887584	test: 0.890938
PRC train: 0.998719	val: 0.954954	test: 0.957675

Epoch: 67
Loss: 0.1370562983606071
ROC train: 0.996585	val: 0.891017	test: 0.894153
PRC train: 0.998932	val: 0.956028	test: 0.958446

Epoch: 68
Loss: 0.13752046986949815
ROC train: 0.996927	val: 0.891153	test: 0.901536
PRC train: 0.999040	val: 0.954945	test: 0.961039

Epoch: 69
Loss: 0.11311079490550957
ROC train: 0.996890	val: 0.893124	test: 0.903176
PRC train: 0.999029	val: 0.956403	test: 0.962335

Epoch: 70
Loss: 0.11221491556151189
ROC train: 0.997467	val: 0.893396	test: 0.903996
PRC train: 0.999213	val: 0.956905	test: 0.962739

Epoch: 71
Loss: 0.13298770187070363
ROC train: 0.996892	val: 0.887448	test: 0.894580
PRC train: 0.999043	val: 0.955735	test: 0.957851

Epoch: 72
Loss: 0.1205102151308112
ROC train: 0.995187	val: 0.896795	test: 0.891627
PRC train: 0.998463	val: 0.961115	test: 0.957904

Epoch: 73
Loss: 0.1209161648346635
ROC train: 0.997718	val: 0.897186	test: 0.892152
PRC train: 0.999295	val: 0.959836	test: 0.956912

Epoch: 74
Loss: 0.11666175901758546
ROC train: 0.997418	val: 0.897203	test: 0.886246
PRC train: 0.999200	val: 0.958693	test: 0.953861

Epoch: 75
Loss: 0.11264118187981449
ROC train: 0.997807	val: 0.894144	test: 0.883588
PRC train: 0.999330	val: 0.958216	test: 0.953083

Epoch: 76
Loss: 0.1108491510356249
ROC train: 0.997013	val: 0.889216	test: 0.888018
PRC train: 0.999051	val: 0.955807	test: 0.953574

Epoch: 77
Loss: 0.12122389025484784
ROC train: 0.997740	val: 0.890711	test: 0.898484
PRC train: 0.999303	val: 0.955035	test: 0.958795

Epoch: 78
Loss: 0.13028950721219473
ROC train: 0.997122	val: 0.893906	test: 0.899239
PRC train: 0.999112	val: 0.956511	test: 0.958396

Epoch: 79
Loss: 0.12397106130446107
ROC train: 0.997108	val: 0.888672	test: 0.890249
PRC train: 0.999121	val: 0.954753	test: 0.953183

Epoch: 80
Loss: 0.09627192631860751
ROC train: 0.995019	val: 0.883675	test: 0.884966
PRC train: 0.998479	val: 0.953392	test: 0.953573

Epoch: 81
Loss: 0.12596049608044751
ROC train: 0.997666	val: 0.886564	test: 0.892316
PRC train: 0.999285	val: 0.955097	test: 0.958098

Epoch: 82
Loss: 0.10481978243000067
ROC train: 0.996624	val: 0.888570	test: 0.899797
PRC train: 0.998931	val: 0.955877	test: 0.962765

Epoch: 83
Loss: 0.1091016808396725
ROC train: 0.996758	val: 0.883675	test: 0.897172
PRC train: 0.998999	val: 0.952258	test: 0.958463

Epoch: 84
Loss: 0.13275862564572105
ROC train: 0.997242	val: 0.879971	test: 0.894908
PRC train: 0.999125	val: 0.947268	test: 0.955995

Epoch: 85
Loss: 0.12014194351430144
ROC train: 0.997755	val: 0.887856	test: 0.904882
PRC train: 0.999308	val: 0.953225	test: 0.963887

Epoch: 86
Loss: 0.1216386706394326
ROC train: 0.997595	val: 0.887958	test: 0.897828
PRC train: 0.999253	val: 0.953635	test: 0.960329

Epoch: 87
Loss: 0.11146956337415992
ROC train: 0.998101	val: 0.882112	test: 0.884671
PRC train: 0.999415	val: 0.950331	test: 0.954427

Epoch: 88
Loss: 0.10248490946658997
ROC train: 0.998366	val: 0.892308	test: 0.884244
PRC train: 0.999500	val: 0.957190	test: 0.956328

Epoch: 89
Loss: 0.11592771393902132
ROC train: 0.998448	val: 0.896727	test: 0.878043
PRC train: 0.999527	val: 0.960890	test: 0.954467

Epoch: 90
Loss: 0.11013947438985354
ROC train: 0.998388	val: 0.889046	test: 0.883621
PRC train: 0.999507	val: 0.957914	test: 0.955085

Epoch: 91
Loss: 0.10111236348793436
ROC train: 0.998381	val: 0.876164	test: 0.890183
PRC train: 0.999503	val: 0.949736	test: 0.955884

Epoch: 92
Loss: 0.10070024068127077
ROC train: 0.998671	val: 0.882826	test: 0.896319
PRC train: 0.999596	val: 0.948949	test: 0.959170

Epoch: 93
Loss: 0.10353684076943726
ROC train: 0.998466	val: 0.889691	test: 0.901929
PRC train: 0.999532	val: 0.953239	test: 0.963189

Epoch: 94
Loss: 0.10504847765998182
ROC train: 0.981970	val: 0.887958	test: 0.893694
PRC train: 0.994331	val: 0.940403	test: 0.959191

Epoch: 34
Loss: 0.18319913667212567
ROC train: 0.985307	val: 0.891799	test: 0.904521
PRC train: 0.995457	val: 0.945054	test: 0.966875

Epoch: 35
Loss: 0.19442944281165112
ROC train: 0.986869	val: 0.890745	test: 0.897008
PRC train: 0.995951	val: 0.944175	test: 0.961986

Epoch: 36
Loss: 0.18547164491827042
ROC train: 0.986785	val: 0.892037	test: 0.891496
PRC train: 0.995915	val: 0.946764	test: 0.958163

Epoch: 37
Loss: 0.1940619921932299
ROC train: 0.985627	val: 0.893634	test: 0.879618
PRC train: 0.995617	val: 0.948900	test: 0.954898

Epoch: 38
Loss: 0.17283705340304917
ROC train: 0.987055	val: 0.889861	test: 0.899304
PRC train: 0.995889	val: 0.947612	test: 0.962908

Epoch: 39
Loss: 0.18603035403055018
ROC train: 0.988010	val: 0.893192	test: 0.908393
PRC train: 0.996219	val: 0.946793	test: 0.965413

Epoch: 40
Loss: 0.18808282258461212
ROC train: 0.989313	val: 0.898766	test: 0.911805
PRC train: 0.996653	val: 0.952568	test: 0.968101

Epoch: 41
Loss: 0.18041800994436216
ROC train: 0.989690	val: 0.889487	test: 0.897467
PRC train: 0.996825	val: 0.948781	test: 0.959284

Epoch: 42
Loss: 0.17893771244418483
ROC train: 0.990131	val: 0.886395	test: 0.887427
PRC train: 0.997002	val: 0.949022	test: 0.954732

Epoch: 43
Loss: 0.16426746892953842
ROC train: 0.990176	val: 0.889657	test: 0.887525
PRC train: 0.997011	val: 0.950662	test: 0.956263

Epoch: 44
Loss: 0.16530262469688378
ROC train: 0.992369	val: 0.894382	test: 0.890971
PRC train: 0.997676	val: 0.952459	test: 0.958671

Epoch: 45
Loss: 0.16502340876636942
ROC train: 0.991159	val: 0.892886	test: 0.876239
PRC train: 0.997329	val: 0.952202	test: 0.952167

Epoch: 46
Loss: 0.16031668912010227
ROC train: 0.990623	val: 0.894875	test: 0.870103
PRC train: 0.997084	val: 0.952530	test: 0.944867

Epoch: 47
Loss: 0.15863468969150107
ROC train: 0.992004	val: 0.890864	test: 0.883162
PRC train: 0.997550	val: 0.951829	test: 0.951380

Epoch: 48
Loss: 0.15707517047026137
ROC train: 0.993964	val: 0.892224	test: 0.899698
PRC train: 0.998160	val: 0.953618	test: 0.962420

Epoch: 49
Loss: 0.16101687740687937
ROC train: 0.993638	val: 0.898630	test: 0.901404
PRC train: 0.998050	val: 0.957058	test: 0.963180

Epoch: 50
Loss: 0.14983088806818148
ROC train: 0.993646	val: 0.899004	test: 0.901503
PRC train: 0.998038	val: 0.956768	test: 0.963236

Epoch: 51
Loss: 0.14413771279091425
ROC train: 0.993724	val: 0.889861	test: 0.895137
PRC train: 0.998069	val: 0.953004	test: 0.959982

Epoch: 52
Loss: 0.13728568539523794
ROC train: 0.993683	val: 0.888774	test: 0.893825
PRC train: 0.998037	val: 0.951668	test: 0.958835

Epoch: 53
Loss: 0.15606088343986718
ROC train: 0.994468	val: 0.895231	test: 0.904981
PRC train: 0.998318	val: 0.954345	test: 0.964537

Epoch: 54
Loss: 0.14926103169760271
ROC train: 0.995002	val: 0.893668	test: 0.904817
PRC train: 0.998460	val: 0.955197	test: 0.964888

Epoch: 55
Loss: 0.12632720799632155
ROC train: 0.996041	val: 0.891901	test: 0.904357
PRC train: 0.998812	val: 0.953586	test: 0.964399

Epoch: 56
Loss: 0.13342511302198348
ROC train: 0.996624	val: 0.888842	test: 0.903373
PRC train: 0.998991	val: 0.952866	test: 0.964144

Epoch: 57
Loss: 0.13694702937829387
ROC train: 0.996534	val: 0.891357	test: 0.904423
PRC train: 0.998952	val: 0.955554	test: 0.964442

Epoch: 58
Loss: 0.14159962880734867
ROC train: 0.994721	val: 0.890592	test: 0.897500
PRC train: 0.998395	val: 0.955088	test: 0.961267

Epoch: 59
Loss: 0.13442594539044497
ROC train: 0.995015	val: 0.895197	test: 0.894809
PRC train: 0.998487	val: 0.954875	test: 0.960417

Epoch: 60
Loss: 0.12888743185724852
ROC train: 0.995827	val: 0.893872	test: 0.889953
PRC train: 0.998726	val: 0.953366	test: 0.956984

Epoch: 61
Loss: 0.13222862694199863
ROC train: 0.996631	val: 0.887754	test: 0.889264
PRC train: 0.998986	val: 0.952109	test: 0.954994

Epoch: 62
Loss: 0.12951966294391015
ROC train: 0.997115	val: 0.889250	test: 0.889757
PRC train: 0.999122	val: 0.955877	test: 0.957616

Epoch: 63
Loss: 0.14130294265852153
ROC train: 0.995339	val: 0.888451	test: 0.880045
PRC train: 0.998574	val: 0.956300	test: 0.952870

Epoch: 64
Loss: 0.13141532143431361
ROC train: 0.994398	val: 0.884355	test: 0.881357
PRC train: 0.998253	val: 0.955629	test: 0.954504

Epoch: 65
Loss: 0.13937517355784396
ROC train: 0.995600	val: 0.896149	test: 0.894514
PRC train: 0.998640	val: 0.957025	test: 0.959444

Epoch: 66
Loss: 0.1402653399003702
ROC train: 0.996056	val: 0.894620	test: 0.891200
PRC train: 0.998782	val: 0.954756	test: 0.957022

Epoch: 67
Loss: 0.1244521944665447
ROC train: 0.995645	val: 0.881874	test: 0.877321
PRC train: 0.998687	val: 0.952040	test: 0.951559

Epoch: 68
Loss: 0.12489841725956889
ROC train: 0.996964	val: 0.887278	test: 0.879093
PRC train: 0.999080	val: 0.951327	test: 0.950241

Epoch: 69
Loss: 0.12528999275017982
ROC train: 0.996765	val: 0.886870	test: 0.876895
PRC train: 0.999013	val: 0.950483	test: 0.947181

Epoch: 70
Loss: 0.11821252288400035
ROC train: 0.996199	val: 0.882622	test: 0.876895
PRC train: 0.998826	val: 0.950387	test: 0.947367

Epoch: 71
Loss: 0.11838568397945344
ROC train: 0.997964	val: 0.887448	test: 0.887099
PRC train: 0.999380	val: 0.953158	test: 0.951888

Epoch: 72
Loss: 0.11327224739295676
ROC train: 0.997634	val: 0.884457	test: 0.882702
PRC train: 0.999284	val: 0.950844	test: 0.950843

Epoch: 73
Loss: 0.12338626120079599
ROC train: 0.996966	val: 0.878305	test: 0.875517
PRC train: 0.999069	val: 0.945673	test: 0.943987

Epoch: 74
Loss: 0.12442943950174148
ROC train: 0.997331	val: 0.879733	test: 0.886311
PRC train: 0.999180	val: 0.946703	test: 0.949503

Epoch: 75
Loss: 0.10515532831205383
ROC train: 0.997197	val: 0.878815	test: 0.888707
PRC train: 0.999148	val: 0.948122	test: 0.955432

Epoch: 76
Loss: 0.1206849854866007
ROC train: 0.997778	val: 0.880345	test: 0.890183
PRC train: 0.999332	val: 0.950907	test: 0.959041

Epoch: 77
Loss: 0.10782594550830563
ROC train: 0.998038	val: 0.885817	test: 0.880865
PRC train: 0.999401	val: 0.951148	test: 0.952900

Epoch: 78
Loss: 0.11300713266722691
ROC train: 0.997975	val: 0.890269	test: 0.877846
PRC train: 0.999381	val: 0.952181	test: 0.949008

Epoch: 79
Loss: 0.10116790806892458
ROC train: 0.997871	val: 0.887363	test: 0.881423
PRC train: 0.999358	val: 0.951730	test: 0.947969

Epoch: 80
Loss: 0.10273961586984173
ROC train: 0.998517	val: 0.883098	test: 0.886640
PRC train: 0.999550	val: 0.950887	test: 0.950849

Epoch: 81
Loss: 0.1149104527887949
ROC train: 0.997229	val: 0.880345	test: 0.884638
PRC train: 0.999104	val: 0.948579	test: 0.950948

Epoch: 82
Loss: 0.10355173150302752
ROC train: 0.997964	val: 0.883641	test: 0.878371
PRC train: 0.999366	val: 0.950364	test: 0.948250

Epoch: 83
Loss: 0.10904153657473144
ROC train: 0.998079	val: 0.879631	test: 0.876993
PRC train: 0.999417	val: 0.949622	test: 0.943396

Epoch: 84
Loss: 0.10547560675039633
ROC train: 0.998168	val: 0.884355	test: 0.890806
PRC train: 0.999436	val: 0.953260	test: 0.956061

Epoch: 85
Loss: 0.09624478582620259
ROC train: 0.998403	val: 0.884831	test: 0.892283
PRC train: 0.999509	val: 0.953495	test: 0.958014

Epoch: 86
Loss: 0.09168100602608734
ROC train: 0.998429	val: 0.886463	test: 0.888838
PRC train: 0.999525	val: 0.952817	test: 0.956997

Epoch: 87
Loss: 0.114305876538606
ROC train: 0.998820	val: 0.888230	test: 0.896155
PRC train: 0.999641	val: 0.951463	test: 0.959469

Epoch: 88
Loss: 0.10261861803517432
ROC train: 0.998431	val: 0.885579	test: 0.896745
PRC train: 0.999523	val: 0.950910	test: 0.958188

Epoch: 89
Loss: 0.10620718981687753
ROC train: 0.998505	val: 0.879631	test: 0.888411
PRC train: 0.999547	val: 0.948839	test: 0.954385

Epoch: 90
Loss: 0.09458877315862493
ROC train: 0.999067	val: 0.886734	test: 0.879946
PRC train: 0.999721	val: 0.949527	test: 0.946907

Epoch: 91
Loss: 0.09965195050573555
ROC train: 0.998947	val: 0.887584	test: 0.876993
PRC train: 0.999682	val: 0.953249	test: 0.948890

Epoch: 92
Loss: 0.09290867175020803
ROC train: 0.998396	val: 0.878050	test: 0.870037
PRC train: 0.999514	val: 0.951732	test: 0.947599

Epoch: 93
Loss: 0.097170706980227
ROC train: 0.998489	val: 0.879937	test: 0.868889
PRC train: 0.999553	val: 0.951752	test: 0.945852

Epoch: 94
Loss: 0.10324719017831767
ROC train: 0.980172	val: 0.888493	test: 0.911513
PRC train: 0.993534	val: 0.947606	test: 0.965097

Epoch: 34
Loss: 0.20149174051333005
ROC train: 0.982845	val: 0.899449	test: 0.923388
PRC train: 0.994420	val: 0.952100	test: 0.972818

Epoch: 35
Loss: 0.18535272595441818
ROC train: 0.982829	val: 0.901130	test: 0.927486
PRC train: 0.994523	val: 0.955640	test: 0.976437

Epoch: 36
Loss: 0.20281763111491613
ROC train: 0.984000	val: 0.903391	test: 0.924171
PRC train: 0.994866	val: 0.953538	test: 0.972714

Epoch: 37
Loss: 0.20477534270880549
ROC train: 0.983849	val: 0.895855	test: 0.915371
PRC train: 0.994651	val: 0.949353	test: 0.967493

Epoch: 38
Loss: 0.19465412171646002
ROC train: 0.983132	val: 0.882348	test: 0.913562
PRC train: 0.994612	val: 0.945938	test: 0.967669

Epoch: 39
Loss: 0.20452946202727174
ROC train: 0.984612	val: 0.886522	test: 0.915130
PRC train: 0.995023	val: 0.947252	test: 0.970674

Epoch: 40
Loss: 0.17599199158438497
ROC train: 0.984890	val: 0.882870	test: 0.912417
PRC train: 0.995122	val: 0.944785	test: 0.967661

Epoch: 41
Loss: 0.18285299356384113
ROC train: 0.984383	val: 0.887507	test: 0.918324
PRC train: 0.995047	val: 0.948089	test: 0.969323

Epoch: 42
Loss: 0.18646036964998033
ROC train: 0.987882	val: 0.902000	test: 0.921941
PRC train: 0.996169	val: 0.955927	test: 0.972856

Epoch: 43
Loss: 0.17803098418869803
ROC train: 0.985456	val: 0.881246	test: 0.906389
PRC train: 0.995414	val: 0.947268	test: 0.966814

Epoch: 44
Loss: 0.17404056320809314
ROC train: 0.989150	val: 0.890406	test: 0.912297
PRC train: 0.996561	val: 0.950486	test: 0.969494

Epoch: 45
Loss: 0.1790479712586044
ROC train: 0.988958	val: 0.904551	test: 0.907836
PRC train: 0.996538	val: 0.959809	test: 0.966232

Epoch: 46
Loss: 0.17392392111106517
ROC train: 0.989563	val: 0.900667	test: 0.914587
PRC train: 0.996696	val: 0.955795	test: 0.969165

Epoch: 47
Loss: 0.1777350130478601
ROC train: 0.989785	val: 0.898116	test: 0.919349
PRC train: 0.996783	val: 0.951060	test: 0.972212

Epoch: 48
Loss: 0.15973558063771884
ROC train: 0.990504	val: 0.897420	test: 0.921760
PRC train: 0.997008	val: 0.954068	test: 0.974537

Epoch: 49
Loss: 0.15870112951743665
ROC train: 0.991147	val: 0.893188	test: 0.922483
PRC train: 0.997265	val: 0.953426	test: 0.973343

Epoch: 50
Loss: 0.16530304992610192
ROC train: 0.990010	val: 0.906638	test: 0.913743
PRC train: 0.996865	val: 0.957986	test: 0.970140

Epoch: 51
Loss: 0.1588402714534116
ROC train: 0.988566	val: 0.893768	test: 0.898312
PRC train: 0.996407	val: 0.953668	test: 0.963556

Epoch: 52
Loss: 0.16022347995927408
ROC train: 0.992594	val: 0.894058	test: 0.910368
PRC train: 0.997704	val: 0.954754	test: 0.969242

Epoch: 53
Loss: 0.15405320126574007
ROC train: 0.993793	val: 0.900319	test: 0.922303
PRC train: 0.998091	val: 0.957574	test: 0.974109

Epoch: 54
Loss: 0.1596713940792612
ROC train: 0.993953	val: 0.900899	test: 0.915371
PRC train: 0.998114	val: 0.956388	test: 0.969582

Epoch: 55
Loss: 0.15456695199194523
ROC train: 0.992796	val: 0.891217	test: 0.902532
PRC train: 0.997728	val: 0.952951	test: 0.963293

Epoch: 56
Loss: 0.16793901400958564
ROC train: 0.992834	val: 0.892609	test: 0.906751
PRC train: 0.997770	val: 0.951640	test: 0.965806

Epoch: 57
Loss: 0.16804343157297083
ROC train: 0.991801	val: 0.892783	test: 0.910006
PRC train: 0.997334	val: 0.951204	test: 0.966868

Epoch: 58
Loss: 0.14943896867447154
ROC train: 0.989725	val: 0.872261	test: 0.904039
PRC train: 0.996383	val: 0.943652	test: 0.964176

Epoch: 59
Loss: 0.16545111502039175
ROC train: 0.992437	val: 0.882174	test: 0.910127
PRC train: 0.997551	val: 0.944626	test: 0.966761

Epoch: 60
Loss: 0.14327822817033986
ROC train: 0.994543	val: 0.898290	test: 0.921398
PRC train: 0.998317	val: 0.957351	test: 0.974165

Epoch: 61
Loss: 0.15462990012912547
ROC train: 0.993639	val: 0.888899	test: 0.918807
PRC train: 0.997988	val: 0.952700	test: 0.973160

Epoch: 62
Loss: 0.13967755643480453
ROC train: 0.993322	val: 0.886580	test: 0.914708
PRC train: 0.997903	val: 0.951285	test: 0.969391

Epoch: 63
Loss: 0.14955285762268808
ROC train: 0.993516	val: 0.889826	test: 0.909946
PRC train: 0.997946	val: 0.954264	test: 0.965662

Epoch: 64
Loss: 0.1330626859186171
ROC train: 0.995390	val: 0.889014	test: 0.911814
PRC train: 0.998558	val: 0.954064	test: 0.966554

Epoch: 65
Loss: 0.15367912464683
ROC train: 0.993672	val: 0.879043	test: 0.908921
PRC train: 0.997997	val: 0.948924	test: 0.964761

Epoch: 66
Loss: 0.14791196590617614
ROC train: 0.995532	val: 0.889130	test: 0.911332
PRC train: 0.998607	val: 0.952427	test: 0.965201

Epoch: 67
Loss: 0.13707518856713766
ROC train: 0.995168	val: 0.887333	test: 0.909403
PRC train: 0.998480	val: 0.953557	test: 0.966929

Epoch: 68
Loss: 0.1297320010881511
ROC train: 0.995499	val: 0.877884	test: 0.898674
PRC train: 0.998614	val: 0.951560	test: 0.964960

Epoch: 69
Loss: 0.1368318773212893
ROC train: 0.996603	val: 0.883014	test: 0.910006
PRC train: 0.998963	val: 0.950845	test: 0.969358

Epoch: 70
Loss: 0.14022557256315596
ROC train: 0.995414	val: 0.888377	test: 0.912658
PRC train: 0.998594	val: 0.951302	test: 0.968027

Epoch: 71
Loss: 0.11699337256011126
ROC train: 0.993355	val: 0.887797	test: 0.904702
PRC train: 0.997858	val: 0.951353	test: 0.964809

Epoch: 72
Loss: 0.12494089777838657
ROC train: 0.994511	val: 0.874116	test: 0.901507
PRC train: 0.998302	val: 0.944876	test: 0.961538

Epoch: 73
Loss: 0.14252084695892608
ROC train: 0.997251	val: 0.885884	test: 0.908318
PRC train: 0.999161	val: 0.952441	test: 0.967839

Epoch: 74
Loss: 0.14819492347566438
ROC train: 0.997376	val: 0.893246	test: 0.916034
PRC train: 0.999191	val: 0.953542	test: 0.970238

Epoch: 75
Loss: 0.12047927121645903
ROC train: 0.992142	val: 0.898116	test: 0.898734
PRC train: 0.997087	val: 0.953077	test: 0.956802

Epoch: 76
Loss: 0.1351474382135495
ROC train: 0.995280	val: 0.892406	test: 0.905003
PRC train: 0.998480	val: 0.948656	test: 0.962320

Epoch: 77
Loss: 0.12067071154976117
ROC train: 0.996921	val: 0.889188	test: 0.913623
PRC train: 0.999038	val: 0.950449	test: 0.971199

Epoch: 78
Loss: 0.10913404941979532
ROC train: 0.997081	val: 0.876029	test: 0.901627
PRC train: 0.999094	val: 0.944362	test: 0.967095

Epoch: 79
Loss: 0.11396804223442288
ROC train: 0.997088	val: 0.880609	test: 0.900723
PRC train: 0.999102	val: 0.946683	test: 0.965641

Epoch: 80
Loss: 0.12197906555813193
ROC train: 0.997231	val: 0.891043	test: 0.905365
PRC train: 0.999152	val: 0.950088	test: 0.966486

Epoch: 81
Loss: 0.12403304754274469
ROC train: 0.997702	val: 0.886406	test: 0.911392
PRC train: 0.999295	val: 0.948203	test: 0.969392

Epoch: 82
Loss: 0.10832108468412631
ROC train: 0.997658	val: 0.883507	test: 0.912297
PRC train: 0.999291	val: 0.951713	test: 0.970971

Epoch: 83
Loss: 0.12659519378653294
ROC train: 0.995762	val: 0.877681	test: 0.903074
PRC train: 0.998482	val: 0.947649	test: 0.966910

Epoch: 84
Loss: 0.1275736346598198
ROC train: 0.992794	val: 0.886116	test: 0.887884
PRC train: 0.996431	val: 0.949127	test: 0.957788

Epoch: 85
Loss: 0.1437462078464474
ROC train: 0.994910	val: 0.885130	test: 0.896383
PRC train: 0.998199	val: 0.947451	test: 0.961783

Epoch: 86
Loss: 0.11544302155934934
ROC train: 0.995906	val: 0.884551	test: 0.909464
PRC train: 0.998728	val: 0.951060	test: 0.969184

Epoch: 87
Loss: 0.11503033607037826
ROC train: 0.997885	val: 0.896899	test: 0.920494
PRC train: 0.999350	val: 0.952715	test: 0.973240

Epoch: 88
Loss: 0.10559417642168424
ROC train: 0.998167	val: 0.894638	test: 0.912116
PRC train: 0.999434	val: 0.953824	test: 0.969801

Epoch: 89
Loss: 0.111427496156017
ROC train: 0.998310	val: 0.886000	test: 0.908740
PRC train: 0.999493	val: 0.949856	test: 0.967769

Epoch: 90
Loss: 0.09996478111441803
ROC train: 0.998192	val: 0.889623	test: 0.916576
PRC train: 0.999456	val: 0.950360	test: 0.971562

Epoch: 91
Loss: 0.10586605061095962
ROC train: 0.998054	val: 0.888319	test: 0.916817
PRC train: 0.999408	val: 0.950973	test: 0.971634

Epoch: 92
Loss: 0.11545794329900066
ROC train: 0.998017	val: 0.890870	test: 0.905546
PRC train: 0.999391	val: 0.954188	test: 0.967009

Epoch: 93
Loss: 0.10286818978304903
ROC train: 0.997924	val: 0.884493	test: 0.898553
PRC train: 0.999371	val: 0.950425	test: 0.961446

Epoch: 94
Loss: 0.12270587373188169
ROC train: 0.981295	val: 0.887913	test: 0.903918
PRC train: 0.993864	val: 0.951696	test: 0.965159

Epoch: 34
Loss: 0.20160144394408874
ROC train: 0.980091	val: 0.891768	test: 0.899337
PRC train: 0.993532	val: 0.954395	test: 0.960128

Epoch: 35
Loss: 0.1917327466013127
ROC train: 0.983095	val: 0.891014	test: 0.913442
PRC train: 0.994589	val: 0.955243	test: 0.968575

Epoch: 36
Loss: 0.19955506468197273
ROC train: 0.983625	val: 0.893826	test: 0.911935
PRC train: 0.994640	val: 0.956415	test: 0.969389

Epoch: 37
Loss: 0.1958558918366544
ROC train: 0.983282	val: 0.890116	test: 0.912779
PRC train: 0.994558	val: 0.954887	test: 0.968822

Epoch: 38
Loss: 0.1782649830402324
ROC train: 0.984427	val: 0.896841	test: 0.914647
PRC train: 0.994889	val: 0.957233	test: 0.970452

Epoch: 39
Loss: 0.1816715862633241
ROC train: 0.986753	val: 0.890638	test: 0.913743
PRC train: 0.995751	val: 0.951358	test: 0.969260

Epoch: 40
Loss: 0.1887079228136067
ROC train: 0.986857	val: 0.883333	test: 0.912176
PRC train: 0.995825	val: 0.947250	test: 0.967860

Epoch: 41
Loss: 0.18542870803194314
ROC train: 0.985849	val: 0.879797	test: 0.909584
PRC train: 0.995528	val: 0.949195	test: 0.968492

Epoch: 42
Loss: 0.17888195071192492
ROC train: 0.986265	val: 0.892087	test: 0.923749
PRC train: 0.995703	val: 0.955052	test: 0.974303

Epoch: 43
Loss: 0.17621902222648667
ROC train: 0.987169	val: 0.893826	test: 0.926281
PRC train: 0.995946	val: 0.955595	test: 0.975800

Epoch: 44
Loss: 0.1709401457752274
ROC train: 0.985972	val: 0.891594	test: 0.916275
PRC train: 0.995638	val: 0.956045	test: 0.970948

Epoch: 45
Loss: 0.16408884372270657
ROC train: 0.989903	val: 0.905072	test: 0.923146
PRC train: 0.996825	val: 0.962312	test: 0.974511

Epoch: 46
Loss: 0.16876979795472377
ROC train: 0.989759	val: 0.900725	test: 0.922242
PRC train: 0.996781	val: 0.958641	test: 0.974721

Epoch: 47
Loss: 0.1625152696997909
ROC train: 0.990141	val: 0.893391	test: 0.913502
PRC train: 0.996913	val: 0.954631	test: 0.970323

Epoch: 48
Loss: 0.16182737838739486
ROC train: 0.989080	val: 0.889768	test: 0.919771
PRC train: 0.996605	val: 0.956328	test: 0.971597

Epoch: 49
Loss: 0.16457544577819566
ROC train: 0.991506	val: 0.904406	test: 0.924352
PRC train: 0.997401	val: 0.962912	test: 0.975313

Epoch: 50
Loss: 0.18027169448330663
ROC train: 0.990952	val: 0.897826	test: 0.916275
PRC train: 0.997245	val: 0.960778	test: 0.972508

Epoch: 51
Loss: 0.16167789545304337
ROC train: 0.988769	val: 0.891449	test: 0.910669
PRC train: 0.996468	val: 0.958049	test: 0.967063

Epoch: 52
Loss: 0.16610483343346366
ROC train: 0.990026	val: 0.902290	test: 0.921218
PRC train: 0.996923	val: 0.963648	test: 0.973035

Epoch: 53
Loss: 0.16699533984688072
ROC train: 0.991010	val: 0.902522	test: 0.920072
PRC train: 0.997243	val: 0.962535	test: 0.972566

Epoch: 54
Loss: 0.15756958405624202
ROC train: 0.989835	val: 0.911275	test: 0.924533
PRC train: 0.996793	val: 0.964435	test: 0.975076

Epoch: 55
Loss: 0.15526453788514363
ROC train: 0.990385	val: 0.906261	test: 0.930018
PRC train: 0.996975	val: 0.962911	test: 0.976154

Epoch: 56
Loss: 0.16260593570670642
ROC train: 0.992844	val: 0.897768	test: 0.925919
PRC train: 0.997800	val: 0.960015	test: 0.973781

Epoch: 57
Loss: 0.16562032382731148
ROC train: 0.992538	val: 0.887159	test: 0.909162
PRC train: 0.997759	val: 0.956141	test: 0.966939

Epoch: 58
Loss: 0.15336895350816135
ROC train: 0.993849	val: 0.889275	test: 0.915612
PRC train: 0.998120	val: 0.954965	test: 0.971279

Epoch: 59
Loss: 0.15420465109310372
ROC train: 0.994048	val: 0.891942	test: 0.922122
PRC train: 0.998180	val: 0.955765	test: 0.974846

Epoch: 60
Loss: 0.15537083219511855
ROC train: 0.994311	val: 0.881594	test: 0.915732
PRC train: 0.998297	val: 0.951944	test: 0.972043

Epoch: 61
Loss: 0.16665964615569656
ROC train: 0.993659	val: 0.880087	test: 0.907233
PRC train: 0.998085	val: 0.949960	test: 0.966346

Epoch: 62
Loss: 0.1555500322973452
ROC train: 0.992201	val: 0.889536	test: 0.911513
PRC train: 0.997593	val: 0.957344	test: 0.969693

Epoch: 63
Loss: 0.1428921489270029
ROC train: 0.993248	val: 0.886928	test: 0.911814
PRC train: 0.997936	val: 0.956126	test: 0.970132

Epoch: 64
Loss: 0.1310083911179006
ROC train: 0.993679	val: 0.895913	test: 0.910066
PRC train: 0.998042	val: 0.959176	test: 0.970027

Epoch: 65
Loss: 0.12597526303790033
ROC train: 0.994338	val: 0.899275	test: 0.918505
PRC train: 0.998203	val: 0.960824	test: 0.973060

Epoch: 66
Loss: 0.12958936820187364
ROC train: 0.992722	val: 0.894812	test: 0.909825
PRC train: 0.997680	val: 0.959515	test: 0.968625

Epoch: 67
Loss: 0.14027573516990302
ROC train: 0.991753	val: 0.899913	test: 0.913140
PRC train: 0.997353	val: 0.962589	test: 0.971177

Epoch: 68
Loss: 0.14072576953418126
ROC train: 0.993613	val: 0.895710	test: 0.922845
PRC train: 0.998024	val: 0.959757	test: 0.973924

Epoch: 69
Loss: 0.13850701965461348
ROC train: 0.993135	val: 0.889072	test: 0.923629
PRC train: 0.997880	val: 0.956009	test: 0.975100

Epoch: 70
Loss: 0.11983384051285863
ROC train: 0.993922	val: 0.888957	test: 0.916456
PRC train: 0.998128	val: 0.956741	test: 0.972052

Epoch: 71
Loss: 0.13270291966084413
ROC train: 0.994913	val: 0.894696	test: 0.915491
PRC train: 0.998402	val: 0.959360	test: 0.970891

Epoch: 72
Loss: 0.12609786990887312
ROC train: 0.995349	val: 0.889478	test: 0.912658
PRC train: 0.998561	val: 0.956103	test: 0.969925

Epoch: 73
Loss: 0.14154090135529512
ROC train: 0.995795	val: 0.885072	test: 0.907957
PRC train: 0.998711	val: 0.953306	test: 0.968137

Epoch: 74
Loss: 0.12913562930968783
ROC train: 0.996257	val: 0.884261	test: 0.913140
PRC train: 0.998864	val: 0.952444	test: 0.969757

Epoch: 75
Loss: 0.12549311066155142
ROC train: 0.996051	val: 0.895304	test: 0.907233
PRC train: 0.998781	val: 0.958549	test: 0.966993

Epoch: 76
Loss: 0.11331193990047471
ROC train: 0.995880	val: 0.893536	test: 0.906570
PRC train: 0.998684	val: 0.957634	test: 0.966840

Epoch: 77
Loss: 0.13245008167265712
ROC train: 0.995064	val: 0.886928	test: 0.906751
PRC train: 0.998396	val: 0.954996	test: 0.967222

Epoch: 78
Loss: 0.12256951767677611
ROC train: 0.996042	val: 0.885362	test: 0.912297
PRC train: 0.998793	val: 0.951212	test: 0.967594

Epoch: 79
Loss: 0.1380053783093154
ROC train: 0.996581	val: 0.900957	test: 0.919168
PRC train: 0.998952	val: 0.957594	test: 0.972662

Epoch: 80
Loss: 0.11652232866245771
ROC train: 0.996909	val: 0.900551	test: 0.925136
PRC train: 0.999062	val: 0.957349	test: 0.975568

Epoch: 81
Loss: 0.1235385765646669
ROC train: 0.997611	val: 0.903884	test: 0.924834
PRC train: 0.999276	val: 0.958822	test: 0.975656

Epoch: 82
Loss: 0.11330731683434093
ROC train: 0.997624	val: 0.893246	test: 0.917722
PRC train: 0.999281	val: 0.956253	test: 0.971545

Epoch: 83
Loss: 0.11530217966296309
ROC train: 0.997001	val: 0.888319	test: 0.913442
PRC train: 0.999102	val: 0.954942	test: 0.969578

Epoch: 84
Loss: 0.12340931051765429
ROC train: 0.996499	val: 0.888986	test: 0.908258
PRC train: 0.998938	val: 0.956198	test: 0.967551

Epoch: 85
Loss: 0.1319683423195437
ROC train: 0.996902	val: 0.890348	test: 0.906932
PRC train: 0.999056	val: 0.954870	test: 0.965551

Epoch: 86
Loss: 0.1142699581772489
ROC train: 0.997748	val: 0.894058	test: 0.922363
PRC train: 0.999315	val: 0.957583	test: 0.973867

Epoch: 87
Loss: 0.12237729516664599
ROC train: 0.997429	val: 0.891623	test: 0.919650
PRC train: 0.999217	val: 0.956053	test: 0.972884

Epoch: 88
Loss: 0.12272348836833656
ROC train: 0.997110	val: 0.889217	test: 0.909524
PRC train: 0.999125	val: 0.953828	test: 0.966713

Epoch: 89
Loss: 0.10971627114292344
ROC train: 0.997257	val: 0.892551	test: 0.901869
PRC train: 0.999181	val: 0.956383	test: 0.965315

Epoch: 90
Loss: 0.10860734220330287
ROC train: 0.997717	val: 0.890290	test: 0.904762
PRC train: 0.999311	val: 0.955065	test: 0.964059

Epoch: 91
Loss: 0.11229944220667143
ROC train: 0.997024	val: 0.879681	test: 0.895720
PRC train: 0.999107	val: 0.950814	test: 0.959753

Epoch: 92
Loss: 0.10842517578321087
ROC train: 0.997283	val: 0.879043	test: 0.909222
PRC train: 0.999187	val: 0.951531	test: 0.968243

Epoch: 93
Loss: 0.09958635027910749
ROC train: 0.997568	val: 0.875971	test: 0.914045
PRC train: 0.999249	val: 0.948824	test: 0.968159

Epoch: 94
Loss: 0.10893191146147473ROC train: 0.981944	val: 0.885246	test: 0.898855
PRC train: 0.994267	val: 0.943266	test: 0.963876

Epoch: 34
Loss: 0.1925699754425838
ROC train: 0.982384	val: 0.886145	test: 0.906631
PRC train: 0.994472	val: 0.946850	test: 0.965796

Epoch: 35
Loss: 0.2207854832184102
ROC train: 0.983177	val: 0.893246	test: 0.923629
PRC train: 0.994749	val: 0.953478	test: 0.972100

Epoch: 36
Loss: 0.19335108543478544
ROC train: 0.984718	val: 0.892435	test: 0.919228
PRC train: 0.995246	val: 0.952904	test: 0.969233

Epoch: 37
Loss: 0.20672748048313117
ROC train: 0.984511	val: 0.896377	test: 0.907595
PRC train: 0.995073	val: 0.956659	test: 0.967823

Epoch: 38
Loss: 0.1742113283245301
ROC train: 0.985618	val: 0.895478	test: 0.906088
PRC train: 0.995326	val: 0.952582	test: 0.964582

Epoch: 39
Loss: 0.16947614011470857
ROC train: 0.984677	val: 0.897913	test: 0.909524
PRC train: 0.995020	val: 0.955281	test: 0.967254

Epoch: 40
Loss: 0.19090550961874023
ROC train: 0.986258	val: 0.891797	test: 0.908077
PRC train: 0.995619	val: 0.951535	test: 0.968584

Epoch: 41
Loss: 0.18628607144774112
ROC train: 0.988655	val: 0.894754	test: 0.897951
PRC train: 0.996452	val: 0.946817	test: 0.960203

Epoch: 42
Loss: 0.18474413483678823
ROC train: 0.988336	val: 0.899072	test: 0.898975
PRC train: 0.996381	val: 0.952376	test: 0.961247

Epoch: 43
Loss: 0.16509513298187786
ROC train: 0.987232	val: 0.895739	test: 0.896866
PRC train: 0.996026	val: 0.952361	test: 0.958048

Epoch: 44
Loss: 0.1797884497899117
ROC train: 0.985956	val: 0.898145	test: 0.904400
PRC train: 0.995544	val: 0.953263	test: 0.965054

Epoch: 45
Loss: 0.18495713590785356
ROC train: 0.988922	val: 0.899391	test: 0.917782
PRC train: 0.996545	val: 0.956819	test: 0.971101

Epoch: 46
Loss: 0.18130303251840338
ROC train: 0.988449	val: 0.901536	test: 0.905003
PRC train: 0.996398	val: 0.958066	test: 0.963927

Epoch: 47
Loss: 0.1654242542943228
ROC train: 0.988685	val: 0.885014	test: 0.897830
PRC train: 0.996437	val: 0.945601	test: 0.959466

Epoch: 48
Loss: 0.1702740506568945
ROC train: 0.988616	val: 0.892754	test: 0.907836
PRC train: 0.996356	val: 0.949251	test: 0.965144

Epoch: 49
Loss: 0.16994741400562816
ROC train: 0.991904	val: 0.892493	test: 0.895539
PRC train: 0.997404	val: 0.950265	test: 0.962208

Epoch: 50
Loss: 0.1642911354808309
ROC train: 0.991875	val: 0.894406	test: 0.895419
PRC train: 0.997464	val: 0.950541	test: 0.959922

Epoch: 51
Loss: 0.18415059733763964
ROC train: 0.990804	val: 0.899913	test: 0.897348
PRC train: 0.997158	val: 0.957438	test: 0.961502

Epoch: 52
Loss: 0.16156060631925237
ROC train: 0.991367	val: 0.892029	test: 0.898855
PRC train: 0.997328	val: 0.950961	test: 0.959217

Epoch: 53
Loss: 0.1539077246308467
ROC train: 0.991051	val: 0.887739	test: 0.898915
PRC train: 0.997258	val: 0.951414	test: 0.959201

Epoch: 54
Loss: 0.15909565511114226
ROC train: 0.993571	val: 0.893246	test: 0.914045
PRC train: 0.998028	val: 0.954614	test: 0.970108

Epoch: 55
Loss: 0.16630498547646796
ROC train: 0.992755	val: 0.896174	test: 0.917360
PRC train: 0.997798	val: 0.956268	test: 0.972235

Epoch: 56
Loss: 0.14284260602336377
ROC train: 0.992970	val: 0.897768	test: 0.915732
PRC train: 0.997837	val: 0.958970	test: 0.974192

Epoch: 57
Loss: 0.15664806800756806
ROC train: 0.993501	val: 0.906145	test: 0.910669
PRC train: 0.997947	val: 0.961581	test: 0.970781

Epoch: 58
Loss: 0.14725642167510106
ROC train: 0.991086	val: 0.892957	test: 0.891983
PRC train: 0.997199	val: 0.955633	test: 0.959913

Epoch: 59
Loss: 0.15198885381454974
ROC train: 0.993220	val: 0.896957	test: 0.885353
PRC train: 0.997908	val: 0.956257	test: 0.959395

Epoch: 60
Loss: 0.14326155103011012
ROC train: 0.994338	val: 0.900435	test: 0.903737
PRC train: 0.998297	val: 0.958104	test: 0.965629

Epoch: 61
Loss: 0.1465140042208228
ROC train: 0.995412	val: 0.900551	test: 0.907233
PRC train: 0.998617	val: 0.957731	test: 0.968065

Epoch: 62
Loss: 0.13549012438862937
ROC train: 0.994128	val: 0.894812	test: 0.905967
PRC train: 0.998190	val: 0.954719	test: 0.966033

Epoch: 63
Loss: 0.15328587452279988
ROC train: 0.994467	val: 0.902290	test: 0.913382
PRC train: 0.998290	val: 0.958281	test: 0.970561

Epoch: 64
Loss: 0.14258521734992252
ROC train: 0.994982	val: 0.892667	test: 0.914165
PRC train: 0.998479	val: 0.954796	test: 0.970833

Epoch: 65
Loss: 0.14983380815443145
ROC train: 0.995116	val: 0.899391	test: 0.916456
PRC train: 0.998494	val: 0.958277	test: 0.971905

Epoch: 66
Loss: 0.12662049305608655
ROC train: 0.994809	val: 0.899275	test: 0.911392
PRC train: 0.998360	val: 0.957901	test: 0.970235

Epoch: 67
Loss: 0.14337651856788305
ROC train: 0.993815	val: 0.891797	test: 0.914045
PRC train: 0.998014	val: 0.954105	test: 0.970844

Epoch: 68
Loss: 0.1421116766735565
ROC train: 0.995402	val: 0.892319	test: 0.907113
PRC train: 0.998592	val: 0.953451	test: 0.969540

Epoch: 69
Loss: 0.13746289661860137
ROC train: 0.994417	val: 0.891159	test: 0.901869
PRC train: 0.998318	val: 0.952984	test: 0.965924

Epoch: 70
Loss: 0.13438168757338065
ROC train: 0.994935	val: 0.893942	test: 0.895238
PRC train: 0.998465	val: 0.954415	test: 0.964148

Epoch: 71
Loss: 0.13472439953930213
ROC train: 0.996216	val: 0.890928	test: 0.898071
PRC train: 0.998856	val: 0.952239	test: 0.964711

Epoch: 72
Loss: 0.11892487356987985
ROC train: 0.995849	val: 0.892841	test: 0.906088
PRC train: 0.998715	val: 0.954655	test: 0.966793

Epoch: 73
Loss: 0.1299561195159208
ROC train: 0.996179	val: 0.898754	test: 0.905063
PRC train: 0.998846	val: 0.957793	test: 0.967076

Epoch: 74
Loss: 0.12893529374048462
ROC train: 0.996392	val: 0.891333	test: 0.905063
PRC train: 0.998905	val: 0.950899	test: 0.965204

Epoch: 75
Loss: 0.1264982515385175
ROC train: 0.993398	val: 0.895971	test: 0.899216
PRC train: 0.997981	val: 0.952195	test: 0.965377

Epoch: 76
Loss: 0.11973004601704303
ROC train: 0.996173	val: 0.898522	test: 0.903436
PRC train: 0.998853	val: 0.956780	test: 0.967429

Epoch: 77
Loss: 0.12744204501101095
ROC train: 0.996439	val: 0.900841	test: 0.907535
PRC train: 0.998882	val: 0.958979	test: 0.969103

Epoch: 78
Loss: 0.11219228787213494
ROC train: 0.996894	val: 0.894145	test: 0.902411
PRC train: 0.999069	val: 0.955113	test: 0.968002

Epoch: 79
Loss: 0.1365010173377329
ROC train: 0.995606	val: 0.900087	test: 0.896745
PRC train: 0.998675	val: 0.958619	test: 0.965292

Epoch: 80
Loss: 0.13850431997898915
ROC train: 0.996417	val: 0.896957	test: 0.903134
PRC train: 0.998923	val: 0.954201	test: 0.965879

Epoch: 81
Loss: 0.12420975794213289
ROC train: 0.995653	val: 0.895275	test: 0.908077
PRC train: 0.998675	val: 0.952821	test: 0.968823

Epoch: 82
Loss: 0.1314452159938063
ROC train: 0.996721	val: 0.907565	test: 0.904219
PRC train: 0.998982	val: 0.956700	test: 0.968582

Epoch: 83
Loss: 0.12055958540542323
ROC train: 0.997521	val: 0.898783	test: 0.908981
PRC train: 0.999247	val: 0.953268	test: 0.968507

Epoch: 84
Loss: 0.11720783626628839
ROC train: 0.996021	val: 0.898261	test: 0.912357
PRC train: 0.998771	val: 0.954647	test: 0.971210

Epoch: 85
Loss: 0.11343274586461392
ROC train: 0.997455	val: 0.895681	test: 0.902833
PRC train: 0.999243	val: 0.953962	test: 0.967212

Epoch: 86
Loss: 0.11294718831281202
ROC train: 0.996751	val: 0.898029	test: 0.885533
PRC train: 0.999022	val: 0.956537	test: 0.958457

Epoch: 87
Loss: 0.11650318744444617
ROC train: 0.997959	val: 0.906522	test: 0.903496
PRC train: 0.999396	val: 0.961168	test: 0.968450

Epoch: 88
Loss: 0.1280199732747712
ROC train: 0.996869	val: 0.898116	test: 0.912176
PRC train: 0.999048	val: 0.955236	test: 0.971520

Epoch: 89
Loss: 0.12018104029893655
ROC train: 0.998011	val: 0.890000	test: 0.902291
PRC train: 0.999400	val: 0.948676	test: 0.965371

Epoch: 90
Loss: 0.10525271060552688
ROC train: 0.997798	val: 0.897536	test: 0.901145
PRC train: 0.999341	val: 0.955049	test: 0.963418

Epoch: 91
Loss: 0.10816937243527981
ROC train: 0.997751	val: 0.895507	test: 0.897830
PRC train: 0.999333	val: 0.957049	test: 0.964828

Epoch: 92
Loss: 0.11869494222446075
ROC train: 0.995631	val: 0.889884	test: 0.900422
PRC train: 0.998697	val: 0.945538	test: 0.965015

Epoch: 93
Loss: 0.12366276120003944
ROC train: 0.995776	val: 0.893014	test: 0.895298
PRC train: 0.998713	val: 0.946458	test: 0.962949

Epoch: 94
Loss: 0.1188038705116626
ROC train: 0.998861	val: 0.889284	test: 0.901404
PRC train: 0.999656	val: 0.953990	test: 0.965422

Epoch: 95
Loss: 0.08796584249875225
ROC train: 0.998840	val: 0.893532	test: 0.907245
PRC train: 0.999650	val: 0.956327	test: 0.968600

Epoch: 96
Loss: 0.09993401349560324
ROC train: 0.998239	val: 0.891918	test: 0.905637
PRC train: 0.999472	val: 0.954795	test: 0.968782

Epoch: 97
Loss: 0.10215558643316598
ROC train: 0.997854	val: 0.886666	test: 0.899928
PRC train: 0.999360	val: 0.951143	test: 0.965791

Epoch: 98
Loss: 0.08024363242740387
ROC train: 0.998734	val: 0.884933	test: 0.899797
PRC train: 0.999618	val: 0.948508	test: 0.964365

Epoch: 99
Loss: 0.10120447112768476
ROC train: 0.998907	val: 0.879529	test: 0.906391
PRC train: 0.999669	val: 0.941578	test: 0.966811

Epoch: 100
Loss: 0.0988997204686756
ROC train: 0.998813	val: 0.886649	test: 0.908491
PRC train: 0.999644	val: 0.949252	test: 0.969482

Epoch: 101
Loss: 0.09602529494234818
ROC train: 0.998656	val: 0.888026	test: 0.908721
PRC train: 0.999596	val: 0.952496	test: 0.968910

Epoch: 102
Loss: 0.09714043324669135
ROC train: 0.998276	val: 0.881772	test: 0.896614
PRC train: 0.999473	val: 0.947766	test: 0.961106

Epoch: 103
Loss: 0.09627399279191322
ROC train: 0.998894	val: 0.886480	test: 0.899698
PRC train: 0.999664	val: 0.950764	test: 0.964053

Epoch: 104
Loss: 0.09463226476190709
ROC train: 0.999021	val: 0.880769	test: 0.902553
PRC train: 0.999708	val: 0.947208	test: 0.966554

Epoch: 105
Loss: 0.090831025251044
ROC train: 0.998537	val: 0.880481	test: 0.895498
PRC train: 0.999557	val: 0.947946	test: 0.962588

Epoch: 106
Loss: 0.08618373399163992
ROC train: 0.998643	val: 0.874567	test: 0.892545
PRC train: 0.999594	val: 0.946654	test: 0.962451

Epoch: 107
Loss: 0.08474738220429301
ROC train: 0.998799	val: 0.874703	test: 0.883162
PRC train: 0.999642	val: 0.948213	test: 0.957121

Epoch: 108
Loss: 0.07565773553137975
ROC train: 0.999330	val: 0.890133	test: 0.885458
PRC train: 0.999799	val: 0.953763	test: 0.955638

Epoch: 109
Loss: 0.08581606699140912
ROC train: 0.999322	val: 0.893702	test: 0.903603
PRC train: 0.999796	val: 0.953264	test: 0.965795

Epoch: 110
Loss: 0.0854644101698035
ROC train: 0.999036	val: 0.888536	test: 0.902848
PRC train: 0.999710	val: 0.951622	test: 0.967431

Epoch: 111
Loss: 0.07820208060996381
ROC train: 0.999233	val: 0.886123	test: 0.899698
PRC train: 0.999767	val: 0.951326	test: 0.964362

Epoch: 112
Loss: 0.10367031415222154
ROC train: 0.999110	val: 0.882860	test: 0.901798
PRC train: 0.999734	val: 0.950125	test: 0.964868

Epoch: 113
Loss: 0.09674474495013982
ROC train: 0.998974	val: 0.877014	test: 0.897303
PRC train: 0.999695	val: 0.944547	test: 0.963094

Epoch: 114
Loss: 0.0803446112891729
ROC train: 0.999041	val: 0.883183	test: 0.893891
PRC train: 0.999710	val: 0.947288	test: 0.961652

Epoch: 115
Loss: 0.08668991779374191
ROC train: 0.999518	val: 0.885103	test: 0.892152
PRC train: 0.999854	val: 0.949196	test: 0.960190

Epoch: 116
Loss: 0.0915613167302769
ROC train: 0.999568	val: 0.887397	test: 0.899173
PRC train: 0.999868	val: 0.950553	test: 0.964528

Epoch: 117
Loss: 0.07438244931732689
ROC train: 0.999244	val: 0.888196	test: 0.905178
PRC train: 0.999768	val: 0.950829	test: 0.968826

Epoch: 118
Loss: 0.08113579394659495
ROC train: 0.999486	val: 0.886395	test: 0.908065
PRC train: 0.999842	val: 0.948877	test: 0.969590

Epoch: 119
Loss: 0.07664834518723995
ROC train: 0.999676	val: 0.885443	test: 0.901765
PRC train: 0.999900	val: 0.949757	test: 0.965919

Epoch: 120
Loss: 0.0723433241093645
ROC train: 0.999576	val: 0.881024	test: 0.886836
PRC train: 0.999870	val: 0.948745	test: 0.960232

Early stopping
Best (ROC):	 train: 0.994338	val: 0.902811	test: 0.910821
Best (PRC):	 train: 0.998275	val: 0.960210	test: 0.966722

ROC train: 0.973838	val: 0.871190	test: 0.907509
PRC train: 0.991198	val: 0.955567	test: 0.949517

Epoch: 34
Loss: 0.20575633307694746
ROC train: 0.978097	val: 0.898887	test: 0.928310
PRC train: 0.992697	val: 0.964467	test: 0.970112

Epoch: 35
Loss: 0.20308940988519622
ROC train: 0.979763	val: 0.897429	test: 0.932104
PRC train: 0.993345	val: 0.963392	test: 0.972398

Epoch: 36
Loss: 0.1987916431921159
ROC train: 0.980226	val: 0.885370	test: 0.918498
PRC train: 0.993714	val: 0.958177	test: 0.967211

Epoch: 37
Loss: 0.19668324881299612
ROC train: 0.981911	val: 0.902200	test: 0.910649
PRC train: 0.994041	val: 0.966137	test: 0.963484

Epoch: 38
Loss: 0.19457290136042873
ROC train: 0.981164	val: 0.893056	test: 0.905939
PRC train: 0.993563	val: 0.957575	test: 0.956479

Epoch: 39
Loss: 0.20791781359275557
ROC train: 0.983023	val: 0.903260	test: 0.912350
PRC train: 0.994491	val: 0.964499	test: 0.962359

Epoch: 40
Loss: 0.19612484134284033
ROC train: 0.984123	val: 0.913862	test: 0.918629
PRC train: 0.994916	val: 0.969685	test: 0.968829

Epoch: 41
Loss: 0.17743142086803004
ROC train: 0.984720	val: 0.915187	test: 0.922161
PRC train: 0.995055	val: 0.970571	test: 0.964755

Epoch: 42
Loss: 0.17384830747343677
ROC train: 0.985268	val: 0.912404	test: 0.910126
PRC train: 0.995388	val: 0.969917	test: 0.954097

Epoch: 43
Loss: 0.17820607453505127
ROC train: 0.983619	val: 0.887225	test: 0.908032
PRC train: 0.994886	val: 0.958538	test: 0.955376

Epoch: 44
Loss: 0.17481542597445518
ROC train: 0.986827	val: 0.895839	test: 0.921507
PRC train: 0.995844	val: 0.960955	test: 0.963892

Epoch: 45
Loss: 0.18691518110694985
ROC train: 0.986415	val: 0.916247	test: 0.918891
PRC train: 0.995734	val: 0.968914	test: 0.966352

Epoch: 46
Loss: 0.1737977807847524
ROC train: 0.988219	val: 0.915982	test: 0.921115
PRC train: 0.996367	val: 0.971208	test: 0.965186

Epoch: 47
Loss: 0.17680019944056108
ROC train: 0.989469	val: 0.911079	test: 0.914443
PRC train: 0.996743	val: 0.968526	test: 0.955169

Epoch: 48
Loss: 0.16368715370749023
ROC train: 0.988665	val: 0.910946	test: 0.906724
PRC train: 0.996468	val: 0.968893	test: 0.958438

Epoch: 49
Loss: 0.18758968530321266
ROC train: 0.988903	val: 0.903393	test: 0.917975
PRC train: 0.996557	val: 0.965514	test: 0.966719

Epoch: 50
Loss: 0.16730764823530198
ROC train: 0.988879	val: 0.914259	test: 0.920460
PRC train: 0.996477	val: 0.969954	test: 0.965999

Epoch: 51
Loss: 0.18158880995355298
ROC train: 0.988236	val: 0.911609	test: 0.908032
PRC train: 0.996348	val: 0.968778	test: 0.958943

Epoch: 52
Loss: 0.18311215298407665
ROC train: 0.988842	val: 0.916114	test: 0.912219
PRC train: 0.996380	val: 0.971490	test: 0.961440

Epoch: 53
Loss: 0.16754576647109845
ROC train: 0.987900	val: 0.914392	test: 0.918367
PRC train: 0.995990	val: 0.969988	test: 0.964985

Epoch: 54
Loss: 0.14861484363674665
ROC train: 0.989376	val: 0.895441	test: 0.921376
PRC train: 0.996718	val: 0.965274	test: 0.969414

Epoch: 55
Loss: 0.1686392307900561
ROC train: 0.990659	val: 0.900345	test: 0.927002
PRC train: 0.997120	val: 0.964864	test: 0.965906

Epoch: 56
Loss: 0.18505494941521924
ROC train: 0.990864	val: 0.906440	test: 0.913658
PRC train: 0.997205	val: 0.967617	test: 0.967433

Epoch: 57
Loss: 0.1626956421936785
ROC train: 0.990840	val: 0.899947	test: 0.908425
PRC train: 0.997242	val: 0.963786	test: 0.965750

Epoch: 58
Loss: 0.17092779718664858
ROC train: 0.990897	val: 0.891068	test: 0.922946
PRC train: 0.997270	val: 0.958923	test: 0.972588

Epoch: 59
Loss: 0.1651861149725555
ROC train: 0.992041	val: 0.905778	test: 0.935505
PRC train: 0.997600	val: 0.966190	test: 0.977401

Epoch: 60
Loss: 0.17039139128418923
ROC train: 0.991495	val: 0.912404	test: 0.938252
PRC train: 0.997377	val: 0.969203	test: 0.975698

Epoch: 61
Loss: 0.15368263509722116
ROC train: 0.992738	val: 0.909356	test: 0.919545
PRC train: 0.997800	val: 0.967962	test: 0.966492

Epoch: 62
Loss: 0.1506885440758063
ROC train: 0.992093	val: 0.903790	test: 0.923208
PRC train: 0.997558	val: 0.962545	test: 0.961621

Epoch: 63
Loss: 0.1564052532936023
ROC train: 0.990737	val: 0.911211	test: 0.923731
PRC train: 0.997161	val: 0.967462	test: 0.972529

Epoch: 64
Loss: 0.14762233770249278
ROC train: 0.993243	val: 0.913994	test: 0.925039
PRC train: 0.997959	val: 0.967714	test: 0.973596

Epoch: 65
Loss: 0.14380589050607492
ROC train: 0.992907	val: 0.915054	test: 0.924123
PRC train: 0.997847	val: 0.967928	test: 0.971779

Epoch: 66
Loss: 0.14886803618525604
ROC train: 0.992653	val: 0.904983	test: 0.917713
PRC train: 0.997790	val: 0.966196	test: 0.970480

Epoch: 67
Loss: 0.15197411682028214
ROC train: 0.992332	val: 0.889743	test: 0.922030
PRC train: 0.997643	val: 0.955560	test: 0.971194

Epoch: 68
Loss: 0.15277211959693157
ROC train: 0.993372	val: 0.904983	test: 0.924778
PRC train: 0.997959	val: 0.967316	test: 0.968320

Epoch: 69
Loss: 0.15924797252736886
ROC train: 0.993739	val: 0.920090	test: 0.928571
PRC train: 0.998090	val: 0.973609	test: 0.975654

Epoch: 70
Loss: 0.1451481414948244
ROC train: 0.992852	val: 0.912006	test: 0.922946
PRC train: 0.997749	val: 0.968597	test: 0.972685

Epoch: 71
Loss: 0.128347220506919
ROC train: 0.994462	val: 0.912536	test: 0.923993
PRC train: 0.998262	val: 0.966422	test: 0.971371

Epoch: 72
Loss: 0.13491281564811794
ROC train: 0.995121	val: 0.914657	test: 0.925432
PRC train: 0.998498	val: 0.965704	test: 0.970551

Epoch: 73
Loss: 0.152281589290247
ROC train: 0.993287	val: 0.906838	test: 0.917321
PRC train: 0.997933	val: 0.964866	test: 0.965974

Epoch: 74
Loss: 0.13080287064300303
ROC train: 0.995384	val: 0.915717	test: 0.932627
PRC train: 0.998589	val: 0.969566	test: 0.974049

Epoch: 75
Loss: 0.13272259724404628
ROC train: 0.995012	val: 0.921018	test: 0.927786
PRC train: 0.998480	val: 0.971340	test: 0.971894

Epoch: 76
Loss: 0.12291440475995029
ROC train: 0.994941	val: 0.917307	test: 0.913396
PRC train: 0.998456	val: 0.969560	test: 0.964660

Epoch: 77
Loss: 0.1311196119020997
ROC train: 0.995226	val: 0.917970	test: 0.917975
PRC train: 0.998549	val: 0.971488	test: 0.970218

Epoch: 78
Loss: 0.14557061090292836
ROC train: 0.994909	val: 0.907766	test: 0.931580
PRC train: 0.998452	val: 0.969397	test: 0.974346

Epoch: 79
Loss: 0.12426810571793304
ROC train: 0.995590	val: 0.904188	test: 0.931319
PRC train: 0.998675	val: 0.967671	test: 0.974473

Epoch: 80
Loss: 0.13929978079884747
ROC train: 0.995499	val: 0.899417	test: 0.919021
PRC train: 0.998629	val: 0.962389	test: 0.969116

Epoch: 81
Loss: 0.1381078314800685
ROC train: 0.994483	val: 0.890140	test: 0.911303
PRC train: 0.998337	val: 0.957955	test: 0.965339

Epoch: 82
Loss: 0.1301770839993913
ROC train: 0.994535	val: 0.902730	test: 0.921115
PRC train: 0.998308	val: 0.961559	test: 0.967866

Epoch: 83
Loss: 0.11547155904750664
ROC train: 0.996815	val: 0.917440	test: 0.933281
PRC train: 0.999038	val: 0.969483	test: 0.975045

Epoch: 84
Loss: 0.12378636791376792
ROC train: 0.997028	val: 0.916777	test: 0.929749
PRC train: 0.999101	val: 0.968864	test: 0.974451

Epoch: 85
Loss: 0.11775982017014221
ROC train: 0.996438	val: 0.893453	test: 0.913004
PRC train: 0.998899	val: 0.961878	test: 0.965348

Epoch: 86
Loss: 0.13961944385289912
ROC train: 0.997144	val: 0.898887	test: 0.919414
PRC train: 0.999131	val: 0.964115	test: 0.968232

Epoch: 87
Loss: 0.12635574541998112
ROC train: 0.996791	val: 0.914922	test: 0.917582
PRC train: 0.999021	val: 0.970322	test: 0.965783

Epoch: 88
Loss: 0.1269890711676421
ROC train: 0.997524	val: 0.915982	test: 0.915751
PRC train: 0.999252	val: 0.971163	test: 0.965662

Epoch: 89
Loss: 0.12796606762337914
ROC train: 0.996276	val: 0.894381	test: 0.919021
PRC train: 0.998877	val: 0.962948	test: 0.961538

Epoch: 90
Loss: 0.12071843713232393
ROC train: 0.996595	val: 0.901272	test: 0.916536
PRC train: 0.998945	val: 0.964079	test: 0.965953

Epoch: 91
Loss: 0.1408652502112133
ROC train: 0.996912	val: 0.920090	test: 0.923469
PRC train: 0.999063	val: 0.972289	test: 0.969793

Epoch: 92
Loss: 0.11054945637746316
ROC train: 0.995751	val: 0.913199	test: 0.925039
PRC train: 0.998699	val: 0.971168	test: 0.973444

Epoch: 93
Loss: 0.11967567357592247
ROC train: 0.996041	val: 0.907766	test: 0.926871
PRC train: 0.998803	val: 0.967064	test: 0.974162

Epoch: 94
Loss: 0.1226445399632037
ROC train: 0.998533	val: 0.893617	test: 0.911772
PRC train: 0.999554	val: 0.956615	test: 0.967672

Epoch: 95
Loss: 0.10382048074835562
ROC train: 0.998511	val: 0.887006	test: 0.903767
PRC train: 0.999539	val: 0.953633	test: 0.960393

Epoch: 96
Loss: 0.1025463804019529
ROC train: 0.998757	val: 0.884627	test: 0.891331
PRC train: 0.999620	val: 0.954224	test: 0.955533

Epoch: 97
Loss: 0.0983910321665041
ROC train: 0.998496	val: 0.886055	test: 0.891430
PRC train: 0.999541	val: 0.954137	test: 0.955158

Epoch: 98
Loss: 0.09652844340789635
ROC train: 0.998094	val: 0.886632	test: 0.885885
PRC train: 0.999424	val: 0.953156	test: 0.951657

Epoch: 99
Loss: 0.11049785286810512
ROC train: 0.998712	val: 0.883234	test: 0.885163
PRC train: 0.999604	val: 0.951078	test: 0.952394

Epoch: 100
Loss: 0.09992829437935205
ROC train: 0.998139	val: 0.883709	test: 0.890150
PRC train: 0.999429	val: 0.950275	test: 0.955985

Epoch: 101
Loss: 0.10413387583374167
ROC train: 0.998868	val: 0.891289	test: 0.894514
PRC train: 0.999652	val: 0.954834	test: 0.956016

Epoch: 102
Loss: 0.09027548860954202
ROC train: 0.999196	val: 0.891629	test: 0.882637
PRC train: 0.999752	val: 0.955449	test: 0.949206

Epoch: 103
Loss: 0.08598153999244557
ROC train: 0.998950	val: 0.885681	test: 0.872958
PRC train: 0.999675	val: 0.953223	test: 0.943116

Epoch: 104
Loss: 0.08448196812648759
ROC train: 0.998865	val: 0.885137	test: 0.867347
PRC train: 0.999654	val: 0.952643	test: 0.939321

Epoch: 105
Loss: 0.09366074638807274
ROC train: 0.999257	val: 0.891323	test: 0.869480
PRC train: 0.999775	val: 0.956134	test: 0.940885

Epoch: 106
Loss: 0.09095591926334938
ROC train: 0.999274	val: 0.888910	test: 0.880406
PRC train: 0.999778	val: 0.956945	test: 0.948750

Epoch: 107
Loss: 0.08706131436863793
ROC train: 0.999058	val: 0.891289	test: 0.878699
PRC train: 0.999712	val: 0.958742	test: 0.950975

Epoch: 108
Loss: 0.10160852870770867
ROC train: 0.999244	val: 0.890643	test: 0.883883
PRC train: 0.999770	val: 0.956783	test: 0.952772

Epoch: 109
Loss: 0.10254968427399662
ROC train: 0.999432	val: 0.889114	test: 0.889166
PRC train: 0.999827	val: 0.954880	test: 0.956065

Epoch: 110
Loss: 0.08017874906158155
ROC train: 0.999084	val: 0.891391	test: 0.892053
PRC train: 0.999725	val: 0.956735	test: 0.960460

Epoch: 111
Loss: 0.08346852883729247
ROC train: 0.999298	val: 0.891306	test: 0.892381
PRC train: 0.999787	val: 0.956854	test: 0.959402

Epoch: 112
Loss: 0.1022431121684912
ROC train: 0.999211	val: 0.885647	test: 0.882965
PRC train: 0.999762	val: 0.952424	test: 0.950046

Epoch: 113
Loss: 0.07831105547543835
ROC train: 0.999255	val: 0.884270	test: 0.879913
PRC train: 0.999773	val: 0.950049	test: 0.947742

Epoch: 114
Loss: 0.08038792266647968
ROC train: 0.998930	val: 0.882979	test: 0.883719
PRC train: 0.999669	val: 0.950330	test: 0.951479

Epoch: 115
Loss: 0.10548235496981981
ROC train: 0.998878	val: 0.884661	test: 0.885032
PRC train: 0.999656	val: 0.952900	test: 0.954110

Epoch: 116
Loss: 0.07175258527676895
ROC train: 0.999356	val: 0.886870	test: 0.883654
PRC train: 0.999804	val: 0.952618	test: 0.954275

Epoch: 117
Loss: 0.08851436935542135
ROC train: 0.998083	val: 0.881296	test: 0.883523
PRC train: 0.999415	val: 0.948878	test: 0.951331

Epoch: 118
Loss: 0.09111902427904639
ROC train: 0.999123	val: 0.887363	test: 0.896351
PRC train: 0.999732	val: 0.952350	test: 0.960191

Epoch: 119
Loss: 0.07854021540312693
ROC train: 0.998846	val: 0.881296	test: 0.894252
PRC train: 0.999642	val: 0.947896	test: 0.957660

Epoch: 120
Loss: 0.07906809834410129
ROC train: 0.999103	val: 0.880311	test: 0.891397
PRC train: 0.999722	val: 0.948368	test: 0.954659

Early stopping
Best (ROC):	 train: 0.979582	val: 0.903898	test: 0.901864
Best (PRC):	 train: 0.993557	val: 0.961717	test: 0.964094

ROC train: 0.974193	val: 0.870660	test: 0.924908
PRC train: 0.991413	val: 0.951510	test: 0.968609

Epoch: 34
Loss: 0.20549623435516975
ROC train: 0.979326	val: 0.888550	test: 0.929487
PRC train: 0.993295	val: 0.960461	test: 0.972495

Epoch: 35
Loss: 0.19987315316541635
ROC train: 0.980446	val: 0.901802	test: 0.926609
PRC train: 0.993736	val: 0.964589	test: 0.972173

Epoch: 36
Loss: 0.19769732623065778
ROC train: 0.981190	val: 0.898092	test: 0.935636
PRC train: 0.993938	val: 0.963035	test: 0.975621

Epoch: 37
Loss: 0.19539939600408993
ROC train: 0.981839	val: 0.917970	test: 0.939430
PRC train: 0.994059	val: 0.970978	test: 0.977510

Epoch: 38
Loss: 0.2084511186957288
ROC train: 0.982527	val: 0.914657	test: 0.937075
PRC train: 0.994260	val: 0.970747	test: 0.976604

Epoch: 39
Loss: 0.21076070065170496
ROC train: 0.979365	val: 0.887490	test: 0.937991
PRC train: 0.993254	val: 0.960329	test: 0.977534

Epoch: 40
Loss: 0.1924664471539977
ROC train: 0.980657	val: 0.908296	test: 0.941654
PRC train: 0.993746	val: 0.969147	test: 0.979205

Epoch: 41
Loss: 0.20826147750809568
ROC train: 0.981669	val: 0.902995	test: 0.922554
PRC train: 0.994034	val: 0.967526	test: 0.969956

Epoch: 42
Loss: 0.18964838752333973
ROC train: 0.982141	val: 0.905380	test: 0.922030
PRC train: 0.994156	val: 0.968724	test: 0.970210

Epoch: 43
Loss: 0.18713481524168898
ROC train: 0.982327	val: 0.900742	test: 0.930141
PRC train: 0.994318	val: 0.965800	test: 0.974605

Epoch: 44
Loss: 0.19064491284675653
ROC train: 0.983091	val: 0.926716	test: 0.933412
PRC train: 0.994419	val: 0.976545	test: 0.977100

Epoch: 45
Loss: 0.1916883974257106
ROC train: 0.983777	val: 0.902995	test: 0.932758
PRC train: 0.994738	val: 0.968366	test: 0.976923

Epoch: 46
Loss: 0.19274619408951302
ROC train: 0.984564	val: 0.889610	test: 0.931973
PRC train: 0.995130	val: 0.962492	test: 0.974942

Epoch: 47
Loss: 0.17642570113965148
ROC train: 0.986016	val: 0.902995	test: 0.911957
PRC train: 0.995423	val: 0.965499	test: 0.966332

Epoch: 48
Loss: 0.18746944146874225
ROC train: 0.987814	val: 0.921680	test: 0.923731
PRC train: 0.996228	val: 0.972190	test: 0.972952

Epoch: 49
Loss: 0.17218655518156314
ROC train: 0.987839	val: 0.913332	test: 0.920984
PRC train: 0.996232	val: 0.970243	test: 0.971556

Epoch: 50
Loss: 0.1765360653370603
ROC train: 0.988077	val: 0.900212	test: 0.926217
PRC train: 0.996212	val: 0.964648	test: 0.973108

Epoch: 51
Loss: 0.18102030427585145
ROC train: 0.988253	val: 0.913862	test: 0.930272
PRC train: 0.996253	val: 0.970322	test: 0.973660

Epoch: 52
Loss: 0.15932602363702944
ROC train: 0.988356	val: 0.935330	test: 0.930665
PRC train: 0.996262	val: 0.979621	test: 0.975276

Epoch: 53
Loss: 0.16275700652698225
ROC train: 0.987659	val: 0.915849	test: 0.937206
PRC train: 0.996049	val: 0.973049	test: 0.978328

Epoch: 54
Loss: 0.14835794298084595
ROC train: 0.988031	val: 0.902067	test: 0.927002
PRC train: 0.996139	val: 0.967788	test: 0.971391

Epoch: 55
Loss: 0.1855850068849157
ROC train: 0.989004	val: 0.902597	test: 0.923077
PRC train: 0.996447	val: 0.967481	test: 0.967969

Epoch: 56
Loss: 0.16176165114388968
ROC train: 0.989950	val: 0.917175	test: 0.933281
PRC train: 0.996873	val: 0.971571	test: 0.976659

Epoch: 57
Loss: 0.15230965558462523
ROC train: 0.990677	val: 0.903525	test: 0.930010
PRC train: 0.997065	val: 0.966311	test: 0.975562

Epoch: 58
Loss: 0.16090644364170179
ROC train: 0.988964	val: 0.918500	test: 0.917713
PRC train: 0.996390	val: 0.974259	test: 0.970025

Epoch: 59
Loss: 0.16240732881685901
ROC train: 0.988590	val: 0.916645	test: 0.921900
PRC train: 0.996400	val: 0.973587	test: 0.971756

Epoch: 60
Loss: 0.15305260462484063
ROC train: 0.991135	val: 0.912271	test: 0.926740
PRC train: 0.997247	val: 0.970732	test: 0.973982

Epoch: 61
Loss: 0.14605837440662758
ROC train: 0.992055	val: 0.905910	test: 0.931450
PRC train: 0.997524	val: 0.966304	test: 0.974330

Epoch: 62
Loss: 0.155815295810839
ROC train: 0.991986	val: 0.906706	test: 0.932365
PRC train: 0.997517	val: 0.967907	test: 0.973337

Epoch: 63
Loss: 0.1576628200687155
ROC train: 0.992279	val: 0.917572	test: 0.944793
PRC train: 0.997605	val: 0.973959	test: 0.979823

Epoch: 64
Loss: 0.13750372112600848
ROC train: 0.992066	val: 0.919295	test: 0.937598
PRC train: 0.997497	val: 0.973495	test: 0.977389

Epoch: 65
Loss: 0.1600065877745949
ROC train: 0.992683	val: 0.901007	test: 0.943223
PRC train: 0.997736	val: 0.965696	test: 0.979885

Epoch: 66
Loss: 0.14179752497766748
ROC train: 0.990964	val: 0.916645	test: 0.938383
PRC train: 0.997139	val: 0.972969	test: 0.979123

Epoch: 67
Loss: 0.1430620527692708
ROC train: 0.993773	val: 0.910814	test: 0.923600
PRC train: 0.998065	val: 0.969692	test: 0.972393

Epoch: 68
Loss: 0.1358703212322863
ROC train: 0.993020	val: 0.890008	test: 0.929487
PRC train: 0.997785	val: 0.962066	test: 0.973885

Epoch: 69
Loss: 0.15435415247907505
ROC train: 0.993060	val: 0.890140	test: 0.936552
PRC train: 0.997894	val: 0.961990	test: 0.975051

Epoch: 70
Loss: 0.14567986910597083
ROC train: 0.994737	val: 0.928571	test: 0.937991
PRC train: 0.998400	val: 0.975758	test: 0.977892

Epoch: 71
Loss: 0.14811192486514504
ROC train: 0.992052	val: 0.934932	test: 0.918629
PRC train: 0.997520	val: 0.978726	test: 0.969295

Epoch: 72
Loss: 0.1503997340136149
ROC train: 0.991390	val: 0.909356	test: 0.917975
PRC train: 0.997254	val: 0.968009	test: 0.970615

Epoch: 73
Loss: 0.15082205930699505
ROC train: 0.994715	val: 0.916512	test: 0.919676
PRC train: 0.998384	val: 0.971919	test: 0.969787

Epoch: 74
Loss: 0.1383115311324486
ROC train: 0.994852	val: 0.902200	test: 0.916667
PRC train: 0.998443	val: 0.968149	test: 0.970010

Epoch: 75
Loss: 0.14126951714916033
ROC train: 0.996069	val: 0.920223	test: 0.927002
PRC train: 0.998824	val: 0.974700	test: 0.974853

Epoch: 76
Loss: 0.13420512579971294
ROC train: 0.993806	val: 0.914789	test: 0.925432
PRC train: 0.998124	val: 0.972180	test: 0.972532

Epoch: 77
Loss: 0.1497325131536513
ROC train: 0.992436	val: 0.912669	test: 0.915489
PRC train: 0.997633	val: 0.970792	test: 0.968987

Epoch: 78
Loss: 0.1353800060472322
ROC train: 0.993452	val: 0.905380	test: 0.929226
PRC train: 0.997974	val: 0.967244	test: 0.975247

Epoch: 79
Loss: 0.13472982364953678
ROC train: 0.994772	val: 0.902067	test: 0.930665
PRC train: 0.998435	val: 0.967020	test: 0.975646

Epoch: 80
Loss: 0.14456841225210249
ROC train: 0.995329	val: 0.907633	test: 0.925039
PRC train: 0.998571	val: 0.966032	test: 0.971690

Epoch: 81
Loss: 0.1471351719148736
ROC train: 0.993642	val: 0.902995	test: 0.913396
PRC train: 0.998022	val: 0.964952	test: 0.965067

Epoch: 82
Loss: 0.1376422045557369
ROC train: 0.994416	val: 0.901140	test: 0.919806
PRC train: 0.998209	val: 0.963324	test: 0.970373

Epoch: 83
Loss: 0.1348929029602102
ROC train: 0.994661	val: 0.914392	test: 0.935374
PRC train: 0.998329	val: 0.970299	test: 0.978034

Epoch: 84
Loss: 0.1296405402929253
ROC train: 0.996070	val: 0.906838	test: 0.934720
PRC train: 0.998822	val: 0.967729	test: 0.977158

Epoch: 85
Loss: 0.13400486465379577
ROC train: 0.996948	val: 0.910814	test: 0.930272
PRC train: 0.999089	val: 0.969483	test: 0.974308

Epoch: 86
Loss: 0.13281526452788972
ROC train: 0.996271	val: 0.906971	test: 0.927132
PRC train: 0.998885	val: 0.968025	test: 0.973730

Epoch: 87
Loss: 0.12922224072985428
ROC train: 0.995586	val: 0.901670	test: 0.921376
PRC train: 0.998645	val: 0.966566	test: 0.972443

Epoch: 88
Loss: 0.1509839878272059
ROC train: 0.996596	val: 0.896899	test: 0.916667
PRC train: 0.998967	val: 0.965168	test: 0.967818

Epoch: 89
Loss: 0.11932301241423338
ROC train: 0.994460	val: 0.884310	test: 0.925693
PRC train: 0.998208	val: 0.961214	test: 0.974568

Epoch: 90
Loss: 0.12938182793303224
ROC train: 0.996947	val: 0.901670	test: 0.934720
PRC train: 0.999084	val: 0.966438	test: 0.976940

Epoch: 91
Loss: 0.12985738812523043
ROC train: 0.994974	val: 0.880732	test: 0.929356
PRC train: 0.998467	val: 0.957701	test: 0.971294

Epoch: 92
Loss: 0.12573808655108462
ROC train: 0.995564	val: 0.894249	test: 0.926609
PRC train: 0.998658	val: 0.965498	test: 0.969788

Epoch: 93
Loss: 0.12076648200100247
ROC train: 0.995872	val: 0.898224	test: 0.925824
PRC train: 0.998767	val: 0.966835	test: 0.972315

Epoch: 94
Loss: 0.14157062636632362ROC train: 0.978508	val: 0.901140	test: 0.942962
PRC train: 0.992862	val: 0.963971	test: 0.980584

Epoch: 34
Loss: 0.20232377634705573
ROC train: 0.980886	val: 0.891068	test: 0.927786
PRC train: 0.993673	val: 0.962018	test: 0.973114

Epoch: 35
Loss: 0.2116422051541627
ROC train: 0.980013	val: 0.882587	test: 0.920853
PRC train: 0.993386	val: 0.957788	test: 0.967785

Epoch: 36
Loss: 0.20701561206294622
ROC train: 0.980744	val: 0.918632	test: 0.915751
PRC train: 0.993583	val: 0.971678	test: 0.968097

Epoch: 37
Loss: 0.20653838368307792
ROC train: 0.980258	val: 0.913199	test: 0.932365
PRC train: 0.993492	val: 0.969010	test: 0.974914

Epoch: 38
Loss: 0.21234653482075144
ROC train: 0.982168	val: 0.919958	test: 0.936944
PRC train: 0.994106	val: 0.972879	test: 0.978990

Epoch: 39
Loss: 0.2012115443540636
ROC train: 0.984617	val: 0.915584	test: 0.936290
PRC train: 0.994925	val: 0.969624	test: 0.978509

Epoch: 40
Loss: 0.19596704392352907
ROC train: 0.981314	val: 0.889213	test: 0.929618
PRC train: 0.993734	val: 0.958573	test: 0.974836

Epoch: 41
Loss: 0.19420627948921818
ROC train: 0.982714	val: 0.889213	test: 0.923339
PRC train: 0.994172	val: 0.961709	test: 0.974287

Epoch: 42
Loss: 0.19191121673189387
ROC train: 0.983060	val: 0.906043	test: 0.936552
PRC train: 0.994210	val: 0.968214	test: 0.978703

Epoch: 43
Loss: 0.20626899659990466
ROC train: 0.985557	val: 0.905380	test: 0.932758
PRC train: 0.995371	val: 0.966669	test: 0.976583

Epoch: 44
Loss: 0.19131151539542937
ROC train: 0.985456	val: 0.911211	test: 0.929487
PRC train: 0.995391	val: 0.968134	test: 0.974764

Epoch: 45
Loss: 0.1815018527111485
ROC train: 0.984954	val: 0.897562	test: 0.925563
PRC train: 0.995229	val: 0.961884	test: 0.970487

Epoch: 46
Loss: 0.17482332940161724
ROC train: 0.986871	val: 0.910681	test: 0.929618
PRC train: 0.995815	val: 0.969077	test: 0.975914

Epoch: 47
Loss: 0.16168343498957347
ROC train: 0.986581	val: 0.911874	test: 0.933019
PRC train: 0.995685	val: 0.969814	test: 0.977207

Epoch: 48
Loss: 0.1812759986055061
ROC train: 0.985804	val: 0.901935	test: 0.942046
PRC train: 0.995472	val: 0.966564	test: 0.980549

Epoch: 49
Loss: 0.16625939352820462
ROC train: 0.983438	val: 0.904718	test: 0.930272
PRC train: 0.994580	val: 0.968751	test: 0.975786

Epoch: 50
Loss: 0.18211768934935013
ROC train: 0.987585	val: 0.891863	test: 0.925039
PRC train: 0.995998	val: 0.961150	test: 0.971877

Epoch: 51
Loss: 0.18798568147414613
ROC train: 0.985682	val: 0.894779	test: 0.924123
PRC train: 0.995308	val: 0.961431	test: 0.967495

Epoch: 52
Loss: 0.1831293860265982
ROC train: 0.988849	val: 0.920090	test: 0.928702
PRC train: 0.996515	val: 0.973647	test: 0.975162

Epoch: 53
Loss: 0.1699629681832008
ROC train: 0.987644	val: 0.900212	test: 0.929487
PRC train: 0.996109	val: 0.966056	test: 0.975785

Epoch: 54
Loss: 0.18073341392467795
ROC train: 0.988465	val: 0.893851	test: 0.945578
PRC train: 0.996316	val: 0.964376	test: 0.981440

Epoch: 55
Loss: 0.1523654376011833
ROC train: 0.990969	val: 0.902862	test: 0.942308
PRC train: 0.997154	val: 0.967943	test: 0.979818

Epoch: 56
Loss: 0.18497443858525414
ROC train: 0.990404	val: 0.902862	test: 0.925039
PRC train: 0.996986	val: 0.968545	test: 0.973305

Epoch: 57
Loss: 0.17388050014639053
ROC train: 0.991128	val: 0.899152	test: 0.925955
PRC train: 0.997224	val: 0.965869	test: 0.974666

Epoch: 58
Loss: 0.14988897877091903
ROC train: 0.991429	val: 0.913862	test: 0.932627
PRC train: 0.997319	val: 0.972456	test: 0.977683

Epoch: 59
Loss: 0.15768879367798222
ROC train: 0.992235	val: 0.923933	test: 0.935767
PRC train: 0.997587	val: 0.975129	test: 0.978405

Epoch: 60
Loss: 0.15829857330278432
ROC train: 0.991362	val: 0.895839	test: 0.934066
PRC train: 0.997292	val: 0.961701	test: 0.976897

Epoch: 61
Loss: 0.14964975637435657
ROC train: 0.992752	val: 0.907236	test: 0.941654
PRC train: 0.997770	val: 0.966577	test: 0.979887

Epoch: 62
Loss: 0.14811349140066463
ROC train: 0.991822	val: 0.911476	test: 0.929749
PRC train: 0.997446	val: 0.968749	test: 0.975052

Epoch: 63
Loss: 0.16235467451804617
ROC train: 0.991833	val: 0.907766	test: 0.938514
PRC train: 0.997431	val: 0.967987	test: 0.979305

Epoch: 64
Loss: 0.15671468107798994
ROC train: 0.990483	val: 0.884707	test: 0.930795
PRC train: 0.996979	val: 0.956675	test: 0.977459

Epoch: 65
Loss: 0.17094709982074785
ROC train: 0.993111	val: 0.903127	test: 0.936290
PRC train: 0.997848	val: 0.965630	test: 0.978515

Epoch: 66
Loss: 0.18185709108957265
ROC train: 0.993523	val: 0.908428	test: 0.931580
PRC train: 0.997960	val: 0.967105	test: 0.977330

Epoch: 67
Loss: 0.1551534712600888
ROC train: 0.992472	val: 0.907766	test: 0.926871
PRC train: 0.997626	val: 0.968076	test: 0.975923

Epoch: 68
Loss: 0.14933415370687378
ROC train: 0.992224	val: 0.910151	test: 0.937860
PRC train: 0.997588	val: 0.969012	test: 0.979002

Epoch: 69
Loss: 0.15595449655124138
ROC train: 0.993227	val: 0.908826	test: 0.938121
PRC train: 0.997872	val: 0.969379	test: 0.980140

Epoch: 70
Loss: 0.1547640098679475
ROC train: 0.994764	val: 0.898489	test: 0.944532
PRC train: 0.998379	val: 0.965281	test: 0.981726

Epoch: 71
Loss: 0.1598838424741169
ROC train: 0.994730	val: 0.899682	test: 0.941392
PRC train: 0.998395	val: 0.964463	test: 0.979868

Epoch: 72
Loss: 0.1426827373526057
ROC train: 0.994400	val: 0.900345	test: 0.930534
PRC train: 0.998293	val: 0.965753	test: 0.976329

Epoch: 73
Loss: 0.1346543341851495
ROC train: 0.993703	val: 0.880466	test: 0.927263
PRC train: 0.998056	val: 0.957215	test: 0.974786

Epoch: 74
Loss: 0.1392728412063923
ROC train: 0.995074	val: 0.892128	test: 0.929226
PRC train: 0.998489	val: 0.964396	test: 0.975395

Epoch: 75
Loss: 0.14209991933259042
ROC train: 0.994715	val: 0.874238	test: 0.915620
PRC train: 0.998368	val: 0.954892	test: 0.970010

Epoch: 76
Loss: 0.13241148685563106
ROC train: 0.996406	val: 0.891466	test: 0.932365
PRC train: 0.998892	val: 0.960938	test: 0.977056

Epoch: 77
Loss: 0.14658460587206876
ROC train: 0.995587	val: 0.893453	test: 0.941392
PRC train: 0.998633	val: 0.962885	test: 0.980644

Epoch: 78
Loss: 0.1338885126583762
ROC train: 0.994822	val: 0.901802	test: 0.930403
PRC train: 0.998368	val: 0.967056	test: 0.976406

Epoch: 79
Loss: 0.12948357081634823
ROC train: 0.993706	val: 0.914127	test: 0.912611
PRC train: 0.998048	val: 0.972761	test: 0.967727

Epoch: 80
Loss: 0.12952894917698385
ROC train: 0.996161	val: 0.909091	test: 0.926478
PRC train: 0.998819	val: 0.971182	test: 0.973463

Epoch: 81
Loss: 0.12588972580343802
ROC train: 0.995813	val: 0.900345	test: 0.933412
PRC train: 0.998580	val: 0.967793	test: 0.977706

Epoch: 82
Loss: 0.12389022842463919
ROC train: 0.995843	val: 0.889213	test: 0.931973
PRC train: 0.998708	val: 0.961526	test: 0.976368

Epoch: 83
Loss: 0.1382404996740587
ROC train: 0.996803	val: 0.885767	test: 0.916143
PRC train: 0.999026	val: 0.959554	test: 0.966885

Epoch: 84
Loss: 0.12711695931695188
ROC train: 0.996169	val: 0.879009	test: 0.910649
PRC train: 0.998820	val: 0.956115	test: 0.964765

Epoch: 85
Loss: 0.12565689813236353
ROC train: 0.996543	val: 0.876093	test: 0.926609
PRC train: 0.998925	val: 0.956209	test: 0.973987

Epoch: 86
Loss: 0.13350078941200097
ROC train: 0.995299	val: 0.901405	test: 0.934066
PRC train: 0.998510	val: 0.968297	test: 0.977326

Epoch: 87
Loss: 0.1520062945338383
ROC train: 0.996648	val: 0.909886	test: 0.926347
PRC train: 0.998946	val: 0.970452	test: 0.973966

Epoch: 88
Loss: 0.13545512707626084
ROC train: 0.994619	val: 0.885767	test: 0.937206
PRC train: 0.998337	val: 0.959061	test: 0.977693

Epoch: 89
Loss: 0.13756947025750296
ROC train: 0.995816	val: 0.897297	test: 0.934851
PRC train: 0.998706	val: 0.963425	test: 0.975763

Epoch: 90
Loss: 0.12266830565271346
ROC train: 0.996532	val: 0.899152	test: 0.916797
PRC train: 0.998935	val: 0.964268	test: 0.964985

Epoch: 91
Loss: 0.13083912174637405
ROC train: 0.996013	val: 0.891466	test: 0.922554
PRC train: 0.998785	val: 0.961201	test: 0.972060

Epoch: 92
Loss: 0.12004922237083836
ROC train: 0.996835	val: 0.897032	test: 0.938514
PRC train: 0.999029	val: 0.965769	test: 0.979101

Epoch: 93
Loss: 0.11023097003986872
ROC train: 0.995193	val: 0.912669	test: 0.947541
PRC train: 0.998469	val: 0.969427	test: 0.982468

Epoch: 94
Loss: 0.12492570642764896
ROC train: 0.998004	val: 0.892899	test: 0.913864
PRC train: 0.999395	val: 0.956607	test: 0.969799

Epoch: 95
Loss: 0.11555219839151305
ROC train: 0.998069	val: 0.894290	test: 0.910428
PRC train: 0.999417	val: 0.961637	test: 0.970794

Epoch: 96
Loss: 0.105793311293778
ROC train: 0.997899	val: 0.893971	test: 0.910910
PRC train: 0.999358	val: 0.957914	test: 0.970182

Epoch: 97
Loss: 0.10716189491615546
ROC train: 0.997880	val: 0.897130	test: 0.911091
PRC train: 0.999352	val: 0.959902	test: 0.969511

Epoch: 98
Loss: 0.10845599521367848
ROC train: 0.997447	val: 0.891449	test: 0.900542
PRC train: 0.999229	val: 0.958317	test: 0.965806

Epoch: 99
Loss: 0.10326748220996047
ROC train: 0.998345	val: 0.892754	test: 0.902230
PRC train: 0.999497	val: 0.958941	test: 0.964406

Epoch: 100
Loss: 0.11441354281071543
ROC train: 0.998474	val: 0.891623	test: 0.903315
PRC train: 0.999535	val: 0.958350	test: 0.968192

Epoch: 101
Loss: 0.10523703714697714
ROC train: 0.998198	val: 0.882638	test: 0.902954
PRC train: 0.999452	val: 0.950531	test: 0.967658

Epoch: 102
Loss: 0.10960584994557228
ROC train: 0.997303	val: 0.874928	test: 0.888427
PRC train: 0.999185	val: 0.949076	test: 0.960036

Epoch: 103
Loss: 0.09346185264034722
ROC train: 0.998555	val: 0.877159	test: 0.877939
PRC train: 0.999559	val: 0.948722	test: 0.957965

Epoch: 104
Loss: 0.0916694782393898
ROC train: 0.998480	val: 0.878116	test: 0.887161
PRC train: 0.999530	val: 0.949811	test: 0.962335

Epoch: 105
Loss: 0.08419128030875382
ROC train: 0.999140	val: 0.884725	test: 0.908800
PRC train: 0.999738	val: 0.952680	test: 0.971351

Epoch: 106
Loss: 0.10472573902188688
ROC train: 0.999182	val: 0.884609	test: 0.911875
PRC train: 0.999751	val: 0.949440	test: 0.971427

Epoch: 107
Loss: 0.10161159186722347
ROC train: 0.998585	val: 0.873652	test: 0.910006
PRC train: 0.999567	val: 0.942794	test: 0.969784

Epoch: 108
Loss: 0.09550835692831072
ROC train: 0.998693	val: 0.878928	test: 0.911091
PRC train: 0.999596	val: 0.945047	test: 0.970746

Epoch: 109
Loss: 0.1088234667210195
ROC train: 0.998389	val: 0.882667	test: 0.911332
PRC train: 0.999478	val: 0.949516	test: 0.970866

Epoch: 110
Loss: 0.09584636071553954
ROC train: 0.998799	val: 0.875623	test: 0.910066
PRC train: 0.999631	val: 0.946254	test: 0.970525

Epoch: 111
Loss: 0.08980651729036558
ROC train: 0.999334	val: 0.883739	test: 0.900060
PRC train: 0.999797	val: 0.949126	test: 0.966383

Epoch: 112
Loss: 0.09394880773371578
ROC train: 0.999388	val: 0.884319	test: 0.902954
PRC train: 0.999812	val: 0.947555	test: 0.967438

Epoch: 113
Loss: 0.08653238430374673
ROC train: 0.999529	val: 0.886116	test: 0.908137
PRC train: 0.999856	val: 0.947243	test: 0.968409

Epoch: 114
Loss: 0.0856879820892944
ROC train: 0.999277	val: 0.888783	test: 0.915913
PRC train: 0.999777	val: 0.950826	test: 0.972642

Epoch: 115
Loss: 0.09712203108218691
ROC train: 0.999228	val: 0.867159	test: 0.902954
PRC train: 0.999766	val: 0.937650	test: 0.965398

Epoch: 116
Loss: 0.08501070040625931
ROC train: 0.999000	val: 0.863623	test: 0.896263
PRC train: 0.999697	val: 0.937437	test: 0.963193

Epoch: 117
Loss: 0.09185895298050249
ROC train: 0.998992	val: 0.869710	test: 0.896624
PRC train: 0.999697	val: 0.942194	test: 0.963607

Epoch: 118
Loss: 0.08694307676105224
ROC train: 0.999115	val: 0.875333	test: 0.901145
PRC train: 0.999728	val: 0.945349	test: 0.966152

Epoch: 119
Loss: 0.0845879370038063
ROC train: 0.998929	val: 0.879942	test: 0.906751
PRC train: 0.999669	val: 0.950587	test: 0.969515

Epoch: 120
Loss: 0.0978631137983737
ROC train: 0.999570	val: 0.876841	test: 0.903014
PRC train: 0.999868	val: 0.941794	test: 0.965928

Early stopping
Best (ROC):	 train: 0.990010	val: 0.906638	test: 0.913743
Best (PRC):	 train: 0.996865	val: 0.957986	test: 0.970140

ROC train: 0.998203	val: 0.887217	test: 0.920856
PRC train: 0.999457	val: 0.955308	test: 0.972372

Epoch: 95
Loss: 0.0983090370161274
ROC train: 0.998418	val: 0.886464	test: 0.906631
PRC train: 0.999525	val: 0.952717	test: 0.965015

Epoch: 96
Loss: 0.10637954674493084
ROC train: 0.997376	val: 0.882986	test: 0.911212
PRC train: 0.999219	val: 0.953094	test: 0.967659

Epoch: 97
Loss: 0.10605376052753103
ROC train: 0.995594	val: 0.886841	test: 0.922845
PRC train: 0.998676	val: 0.955896	test: 0.973523

Epoch: 98
Loss: 0.09593082475233701
ROC train: 0.998184	val: 0.882696	test: 0.917661
PRC train: 0.999451	val: 0.952709	test: 0.970569

Epoch: 99
Loss: 0.11135743128256516
ROC train: 0.998696	val: 0.886522	test: 0.914045
PRC train: 0.999605	val: 0.954167	test: 0.970608

Epoch: 100
Loss: 0.08968540158199982
ROC train: 0.998826	val: 0.887536	test: 0.905847
PRC train: 0.999648	val: 0.955361	test: 0.967241

Epoch: 101
Loss: 0.0960425587414624
ROC train: 0.997757	val: 0.878348	test: 0.901386
PRC train: 0.999312	val: 0.949995	test: 0.964363

Epoch: 102
Loss: 0.09944050296031842
ROC train: 0.997620	val: 0.872551	test: 0.898674
PRC train: 0.999231	val: 0.949615	test: 0.963831

Epoch: 103
Loss: 0.1176872241697247
ROC train: 0.998282	val: 0.887188	test: 0.893852
PRC train: 0.999474	val: 0.956739	test: 0.962885

Epoch: 104
Loss: 0.10213929352751198
ROC train: 0.998595	val: 0.885623	test: 0.900482
PRC train: 0.999573	val: 0.955173	test: 0.965186

Epoch: 105
Loss: 0.0893997124449223
ROC train: 0.997662	val: 0.880783	test: 0.913924
PRC train: 0.999307	val: 0.949645	test: 0.970554

Epoch: 106
Loss: 0.10776340277175399
ROC train: 0.998544	val: 0.887391	test: 0.914888
PRC train: 0.999566	val: 0.953909	test: 0.971602

Epoch: 107
Loss: 0.09705268867657739
ROC train: 0.998621	val: 0.882406	test: 0.915009
PRC train: 0.999588	val: 0.952260	test: 0.971105

Epoch: 108
Loss: 0.10340119187220886
ROC train: 0.998403	val: 0.882000	test: 0.909042
PRC train: 0.999518	val: 0.952946	test: 0.968839

Epoch: 109
Loss: 0.10520078772018719
ROC train: 0.998628	val: 0.878348	test: 0.903496
PRC train: 0.999589	val: 0.949848	test: 0.965744

Epoch: 110
Loss: 0.09914058764025023
ROC train: 0.998693	val: 0.881797	test: 0.914346
PRC train: 0.999603	val: 0.949588	test: 0.968606

Epoch: 111
Loss: 0.09715400719660879
ROC train: 0.998352	val: 0.880377	test: 0.907052
PRC train: 0.999507	val: 0.948609	test: 0.966974

Epoch: 112
Loss: 0.10388092698514378
ROC train: 0.998403	val: 0.879913	test: 0.908379
PRC train: 0.999513	val: 0.948412	test: 0.967509

Epoch: 113
Loss: 0.1024192810073754
ROC train: 0.998652	val: 0.880899	test: 0.902652
PRC train: 0.999607	val: 0.950911	test: 0.964553

Epoch: 114
Loss: 0.0990752468793845
ROC train: 0.998726	val: 0.884986	test: 0.894093
PRC train: 0.999619	val: 0.952114	test: 0.961580

Epoch: 115
Loss: 0.08725201630172723
ROC train: 0.999071	val: 0.885942	test: 0.902411
PRC train: 0.999718	val: 0.953023	test: 0.964157

Epoch: 116
Loss: 0.08666747109566303
ROC train: 0.998306	val: 0.876725	test: 0.916637
PRC train: 0.999491	val: 0.945795	test: 0.968872

Epoch: 117
Loss: 0.08931078255194026
ROC train: 0.998940	val: 0.898261	test: 0.922363
PRC train: 0.999682	val: 0.958460	test: 0.973861

Epoch: 118
Loss: 0.0856180712964708
ROC train: 0.998562	val: 0.900319	test: 0.910187
PRC train: 0.999566	val: 0.961116	test: 0.968633

Epoch: 119
Loss: 0.08030271603961057
ROC train: 0.999165	val: 0.889797	test: 0.913502
PRC train: 0.999747	val: 0.955757	test: 0.970750

Epoch: 120
Loss: 0.08381093864169616
ROC train: 0.999193	val: 0.883739	test: 0.915612
PRC train: 0.999755	val: 0.952426	test: 0.971446

Early stopping
Best (ROC):	 train: 0.989835	val: 0.911275	test: 0.924533
Best (PRC):	 train: 0.996793	val: 0.964435	test: 0.975076

ROC train: 0.997926	val: 0.891536	test: 0.894876
PRC train: 0.999380	val: 0.942063	test: 0.963492

Epoch: 95
Loss: 0.09720420705421064
ROC train: 0.998261	val: 0.889942	test: 0.895961
PRC train: 0.999482	val: 0.943022	test: 0.963689

Epoch: 96
Loss: 0.11603562333506855
ROC train: 0.998367	val: 0.896174	test: 0.904822
PRC train: 0.999509	val: 0.946386	test: 0.967190

Epoch: 97
Loss: 0.1210104522323719
ROC train: 0.997603	val: 0.900145	test: 0.907414
PRC train: 0.999274	val: 0.948657	test: 0.968333

Epoch: 98
Loss: 0.1014813262604246
ROC train: 0.998148	val: 0.903391	test: 0.910066
PRC train: 0.999443	val: 0.954139	test: 0.969503

Epoch: 99
Loss: 0.10483497082247724
ROC train: 0.998581	val: 0.893710	test: 0.898975
PRC train: 0.999575	val: 0.950456	test: 0.964305

Epoch: 100
Loss: 0.0968571151010779
ROC train: 0.998389	val: 0.883217	test: 0.886799
PRC train: 0.999512	val: 0.945138	test: 0.958414

Epoch: 101
Loss: 0.10906417208963044
ROC train: 0.998274	val: 0.880899	test: 0.880892
PRC train: 0.999476	val: 0.946497	test: 0.955320

Epoch: 102
Loss: 0.09662682585220402
ROC train: 0.997310	val: 0.888638	test: 0.881977
PRC train: 0.999178	val: 0.952885	test: 0.957832

Epoch: 103
Loss: 0.11267264242341136
ROC train: 0.997726	val: 0.890406	test: 0.893972
PRC train: 0.999311	val: 0.952220	test: 0.963513

Epoch: 104
Loss: 0.118423682815317
ROC train: 0.998337	val: 0.892957	test: 0.899458
PRC train: 0.999504	val: 0.952301	test: 0.965439

Epoch: 105
Loss: 0.09055851808542081
ROC train: 0.998121	val: 0.886174	test: 0.904702
PRC train: 0.999445	val: 0.947614	test: 0.966617

Epoch: 106
Loss: 0.092425174687093
ROC train: 0.998480	val: 0.890319	test: 0.908379
PRC train: 0.999548	val: 0.947025	test: 0.967748

Epoch: 107
Loss: 0.09651709443272215
ROC train: 0.998889	val: 0.890493	test: 0.902291
PRC train: 0.999669	val: 0.947568	test: 0.966321

Epoch: 108
Loss: 0.10431768299859669
ROC train: 0.998422	val: 0.887101	test: 0.903737
PRC train: 0.999526	val: 0.946008	test: 0.966557

Epoch: 109
Loss: 0.0880508663383231
ROC train: 0.999030	val: 0.886174	test: 0.900362
PRC train: 0.999710	val: 0.944056	test: 0.965070

Epoch: 110
Loss: 0.10852849685254744
ROC train: 0.998970	val: 0.892638	test: 0.898252
PRC train: 0.999693	val: 0.947550	test: 0.964833

Epoch: 111
Loss: 0.09268775479194492
ROC train: 0.999214	val: 0.892377	test: 0.907655
PRC train: 0.999764	val: 0.950217	test: 0.968390

Epoch: 112
Loss: 0.09153355442070543
ROC train: 0.998702	val: 0.890870	test: 0.906450
PRC train: 0.999600	val: 0.951606	test: 0.969351

Epoch: 113
Loss: 0.09316247120011938
ROC train: 0.999165	val: 0.869478	test: 0.901748
PRC train: 0.999747	val: 0.938184	test: 0.964081

Epoch: 114
Loss: 0.10061813136733046
ROC train: 0.999028	val: 0.868145	test: 0.899458
PRC train: 0.999705	val: 0.941462	test: 0.964675

Epoch: 115
Loss: 0.07787937154526882
ROC train: 0.999133	val: 0.871275	test: 0.891441
PRC train: 0.999735	val: 0.942777	test: 0.959608

Epoch: 116
Loss: 0.10360179408118486
ROC train: 0.998891	val: 0.872261	test: 0.893490
PRC train: 0.999663	val: 0.944097	test: 0.961026

Epoch: 117
Loss: 0.09550147629258082
ROC train: 0.998925	val: 0.882580	test: 0.898312
PRC train: 0.999672	val: 0.952551	test: 0.964287

Epoch: 118
Loss: 0.10474578811120532
ROC train: 0.999080	val: 0.900870	test: 0.905847
PRC train: 0.999724	val: 0.959118	test: 0.968511

Epoch: 119
Loss: 0.08656620463382465
ROC train: 0.997599	val: 0.899739	test: 0.909464
PRC train: 0.999275	val: 0.956297	test: 0.969637

Epoch: 120
Loss: 0.0899181761752718
ROC train: 0.998984	val: 0.892377	test: 0.911151
PRC train: 0.999694	val: 0.953502	test: 0.969577

Early stopping
Best (ROC):	 train: 0.996721	val: 0.907565	test: 0.904219
Best (PRC):	 train: 0.998982	val: 0.956700	test: 0.968582
All runs completed.

ROC train: 0.998984	val: 0.883608	test: 0.871809
PRC train: 0.999695	val: 0.952153	test: 0.944851

Epoch: 95
Loss: 0.09234068295968464
ROC train: 0.999356	val: 0.885137	test: 0.881784
PRC train: 0.999804	val: 0.952624	test: 0.951039

Epoch: 96
Loss: 0.07993278932373515
ROC train: 0.999360	val: 0.894858	test: 0.889953
PRC train: 0.999803	val: 0.954594	test: 0.956775

Epoch: 97
Loss: 0.09400596365023021
ROC train: 0.999490	val: 0.890643	test: 0.891889
PRC train: 0.999843	val: 0.954326	test: 0.955110

Epoch: 98
Loss: 0.09847558601408304
ROC train: 0.999401	val: 0.891051	test: 0.887460
PRC train: 0.999816	val: 0.955777	test: 0.952262

Epoch: 99
Loss: 0.10654540081895436
ROC train: 0.999334	val: 0.891153	test: 0.877223
PRC train: 0.999795	val: 0.955024	test: 0.950407

Epoch: 100
Loss: 0.0844017487337678
ROC train: 0.999274	val: 0.895945	test: 0.873679
PRC train: 0.999778	val: 0.953545	test: 0.950181

Epoch: 101
Loss: 0.10135084459575523
ROC train: 0.999337	val: 0.898596	test: 0.881882
PRC train: 0.999798	val: 0.954330	test: 0.952903

Epoch: 102
Loss: 0.08070151947587838
ROC train: 0.999336	val: 0.896149	test: 0.888444
PRC train: 0.999797	val: 0.957440	test: 0.952827

Epoch: 103
Loss: 0.10587155178257115
ROC train: 0.999159	val: 0.888638	test: 0.884540
PRC train: 0.999743	val: 0.957501	test: 0.950190

Epoch: 104
Loss: 0.08594952497074285
ROC train: 0.997971	val: 0.875467	test: 0.858882
PRC train: 0.999373	val: 0.952170	test: 0.940176

Epoch: 105
Loss: 0.08709755424272463
ROC train: 0.998807	val: 0.885647	test: 0.858127
PRC train: 0.999632	val: 0.954207	test: 0.935792

Epoch: 106
Loss: 0.08833868913952574
ROC train: 0.999174	val: 0.891663	test: 0.870595
PRC train: 0.999747	val: 0.956205	test: 0.937508

Epoch: 107
Loss: 0.09475733706185355
ROC train: 0.999430	val: 0.887516	test: 0.878667
PRC train: 0.999824	val: 0.954670	test: 0.947902

Epoch: 108
Loss: 0.08364723615953322
ROC train: 0.999460	val: 0.888111	test: 0.884048
PRC train: 0.999831	val: 0.954992	test: 0.956516

Epoch: 109
Loss: 0.08499422434787741
ROC train: 0.999524	val: 0.890881	test: 0.887132
PRC train: 0.999854	val: 0.955541	test: 0.958408

Epoch: 110
Loss: 0.0832849063587793
ROC train: 0.999661	val: 0.891595	test: 0.886377
PRC train: 0.999897	val: 0.956432	test: 0.955636

Epoch: 111
Loss: 0.07893723201001115
ROC train: 0.999644	val: 0.890575	test: 0.885065
PRC train: 0.999892	val: 0.954883	test: 0.952935

Epoch: 112
Loss: 0.08067807436945675
ROC train: 0.999378	val: 0.890269	test: 0.875254
PRC train: 0.999812	val: 0.954806	test: 0.950407

Epoch: 113
Loss: 0.07835683081165361
ROC train: 0.999509	val: 0.887210	test: 0.875090
PRC train: 0.999850	val: 0.954545	test: 0.950275

Epoch: 114
Loss: 0.08450338912841866
ROC train: 0.999389	val: 0.885307	test: 0.884737
PRC train: 0.999813	val: 0.953429	test: 0.955248

Epoch: 115
Loss: 0.08325923442000159
ROC train: 0.998798	val: 0.883030	test: 0.886869
PRC train: 0.999622	val: 0.951196	test: 0.955975

Epoch: 116
Loss: 0.08093739370431635
ROC train: 0.999635	val: 0.895062	test: 0.893661
PRC train: 0.999888	val: 0.955229	test: 0.959511

Epoch: 117
Loss: 0.07493688742191829
ROC train: 0.999600	val: 0.899174	test: 0.895728
PRC train: 0.999878	val: 0.956637	test: 0.958871

Epoch: 118
Loss: 0.06488940968328151
ROC train: 0.999345	val: 0.891765	test: 0.892250
PRC train: 0.999802	val: 0.956181	test: 0.956704

Epoch: 119
Loss: 0.07662698112029113
ROC train: 0.999507	val: 0.887652	test: 0.890347
PRC train: 0.999849	val: 0.955104	test: 0.956484

Epoch: 120
Loss: 0.07734897770974407
ROC train: 0.999531	val: 0.888196	test: 0.892808
PRC train: 0.999855	val: 0.953015	test: 0.960210

Epoch: 121
Loss: 0.10122885318635863
ROC train: 0.999639	val: 0.890065	test: 0.892316
PRC train: 0.999888	val: 0.952115	test: 0.955618

Epoch: 122
Loss: 0.06945461872547902
ROC train: 0.999412	val: 0.888026	test: 0.882604
PRC train: 0.999820	val: 0.950066	test: 0.946672

Epoch: 123
Loss: 0.07433110820120703
ROC train: 0.999706	val: 0.892614	test: 0.881160
PRC train: 0.999910	val: 0.956500	test: 0.949653

Epoch: 124
Loss: 0.07441833825985789
ROC train: 0.999806	val: 0.887822	test: 0.882177
PRC train: 0.999941	val: 0.955419	test: 0.951291

Epoch: 125
Loss: 0.07531613687431389
ROC train: 0.999751	val: 0.885817	test: 0.869939
PRC train: 0.999925	val: 0.955034	test: 0.945661

Epoch: 126
Loss: 0.07454205764562107
ROC train: 0.999836	val: 0.885749	test: 0.860949
PRC train: 0.999949	val: 0.953495	test: 0.941453

Epoch: 127
Loss: 0.07259637376492857
ROC train: 0.999365	val: 0.888434	test: 0.871776
PRC train: 0.999801	val: 0.955303	test: 0.948055

Epoch: 128
Loss: 0.07477989969042405
ROC train: 0.999591	val: 0.889776	test: 0.876567
PRC train: 0.999871	val: 0.956202	test: 0.951225

Epoch: 129
Loss: 0.08157570684318743
ROC train: 0.999630	val: 0.891544	test: 0.867445
PRC train: 0.999888	val: 0.956029	test: 0.945179

Epoch: 130
Loss: 0.06768571879254062
ROC train: 0.999471	val: 0.886089	test: 0.857930
PRC train: 0.999841	val: 0.952681	test: 0.938383

Epoch: 131
Loss: 0.08527596395645895
ROC train: 0.999840	val: 0.883675	test: 0.862885
PRC train: 0.999951	val: 0.953137	test: 0.938628

Epoch: 132
Loss: 0.07517602849712605
ROC train: 0.999907	val: 0.888468	test: 0.869906
PRC train: 0.999971	val: 0.955674	test: 0.942382

Epoch: 133
Loss: 0.06544889359986436
ROC train: 0.999101	val: 0.888706	test: 0.877518
PRC train: 0.999731	val: 0.956107	test: 0.952683

Epoch: 134
Loss: 0.08687768368689222
ROC train: 0.999706	val: 0.892580	test: 0.883227
PRC train: 0.999909	val: 0.957548	test: 0.950330

Epoch: 135
Loss: 0.06024437847547348
ROC train: 0.999810	val: 0.891051	test: 0.883883
PRC train: 0.999941	val: 0.956952	test: 0.950213

Epoch: 136
Loss: 0.07505875141502223
ROC train: 0.999512	val: 0.875654	test: 0.877453
PRC train: 0.999847	val: 0.949696	test: 0.950667

Epoch: 137
Loss: 0.06683715874435357
ROC train: 0.999343	val: 0.869400	test: 0.873745
PRC train: 0.999793	val: 0.946483	test: 0.950625

Epoch: 138
Loss: 0.06648531438876906
ROC train: 0.999706	val: 0.874754	test: 0.880931
PRC train: 0.999909	val: 0.949352	test: 0.954873

Epoch: 139
Loss: 0.06757348858965556
ROC train: 0.999773	val: 0.875722	test: 0.879454
PRC train: 0.999930	val: 0.950135	test: 0.954543

Epoch: 140
Loss: 0.05929258759786339
ROC train: 0.999445	val: 0.881432	test: 0.883621
PRC train: 0.999836	val: 0.951966	test: 0.957822

Epoch: 141
Loss: 0.07436143129796495
ROC train: 0.999561	val: 0.885749	test: 0.888313
PRC train: 0.999872	val: 0.952719	test: 0.958938

Epoch: 142
Loss: 0.07673765987532462
ROC train: 0.999509	val: 0.884117	test: 0.891692
PRC train: 0.999851	val: 0.948431	test: 0.957344

Epoch: 143
Loss: 0.07395125612207654
ROC train: 0.998544	val: 0.878509	test: 0.881915
PRC train: 0.999563	val: 0.946512	test: 0.948767

Epoch: 144
Loss: 0.06547313518157476
ROC train: 0.999698	val: 0.884236	test: 0.871153
PRC train: 0.999907	val: 0.952557	test: 0.941191

Epoch: 145
Loss: 0.07430359344928147
ROC train: 0.999738	val: 0.880617	test: 0.870037
PRC train: 0.999923	val: 0.951803	test: 0.942299

Epoch: 146
Loss: 0.0663262462810148
ROC train: 0.999836	val: 0.883879	test: 0.879454
PRC train: 0.999950	val: 0.954557	test: 0.952117

Epoch: 147
Loss: 0.07803139452583482
ROC train: 0.999870	val: 0.882044	test: 0.881259
PRC train: 0.999959	val: 0.954358	test: 0.954377

Epoch: 148
Loss: 0.07745796817756057
ROC train: 0.999926	val: 0.881602	test: 0.882899
PRC train: 0.999976	val: 0.952566	test: 0.953233

Epoch: 149
Loss: 0.054221358259088116
ROC train: 0.999739	val: 0.880600	test: 0.882735
PRC train: 0.999920	val: 0.951096	test: 0.952722

Epoch: 150
Loss: 0.06115950427203907
ROC train: 0.999955	val: 0.880141	test: 0.888543
PRC train: 0.999985	val: 0.950172	test: 0.954890

Epoch: 151
Loss: 0.05652260606668837
ROC train: 0.999948	val: 0.884151	test: 0.887361
PRC train: 0.999983	val: 0.952566	test: 0.956189

Epoch: 152
Loss: 0.053563902307527334
ROC train: 0.999853	val: 0.885426	test: 0.887361
PRC train: 0.999955	val: 0.952021	test: 0.957144

Early stopping
Best (ROC):	 train: 0.999600	val: 0.899174	test: 0.895728
Best (PRC):	 train: 0.999878	val: 0.956637	test: 0.958871
All runs completed.

ROC train: 0.996833	val: 0.898754	test: 0.916143
PRC train: 0.999047	val: 0.965184	test: 0.966938

Epoch: 95
Loss: 0.11123352080449421
ROC train: 0.995594	val: 0.891466	test: 0.930665
PRC train: 0.998689	val: 0.962900	test: 0.974795

Epoch: 96
Loss: 0.13458255638999436
ROC train: 0.997038	val: 0.906175	test: 0.929487
PRC train: 0.999108	val: 0.969530	test: 0.974146

Epoch: 97
Loss: 0.11193357350407515
ROC train: 0.997106	val: 0.904585	test: 0.937598
PRC train: 0.999119	val: 0.967212	test: 0.977923

Epoch: 98
Loss: 0.09984775195169422
ROC train: 0.997301	val: 0.894779	test: 0.948718
PRC train: 0.999182	val: 0.963102	test: 0.981528

Epoch: 99
Loss: 0.10301348612696883
ROC train: 0.997567	val: 0.895574	test: 0.937598
PRC train: 0.999274	val: 0.963906	test: 0.976106

Epoch: 100
Loss: 0.10952531416453333
ROC train: 0.998075	val: 0.900080	test: 0.940607
PRC train: 0.999419	val: 0.964892	test: 0.978604

Epoch: 101
Loss: 0.11218893349386706
ROC train: 0.997428	val: 0.898622	test: 0.940999
PRC train: 0.999227	val: 0.965730	test: 0.979421

Epoch: 102
Loss: 0.12461705013426559
ROC train: 0.997700	val: 0.898754	test: 0.936944
PRC train: 0.999308	val: 0.966520	test: 0.977686

Epoch: 103
Loss: 0.106924152741525
ROC train: 0.998036	val: 0.891466	test: 0.924385
PRC train: 0.999410	val: 0.962549	test: 0.969405

Epoch: 104
Loss: 0.10553789727580985
ROC train: 0.998172	val: 0.887623	test: 0.915489
PRC train: 0.999447	val: 0.960011	test: 0.964087

Epoch: 105
Loss: 0.10932391070707281
ROC train: 0.998399	val: 0.897562	test: 0.919414
PRC train: 0.999517	val: 0.963433	test: 0.968506

Epoch: 106
Loss: 0.11088041065146907
ROC train: 0.998037	val: 0.902465	test: 0.937075
PRC train: 0.999417	val: 0.965777	test: 0.978852

Epoch: 107
Loss: 0.11758410383429865
ROC train: 0.997827	val: 0.902730	test: 0.939822
PRC train: 0.999347	val: 0.967513	test: 0.980036

Epoch: 108
Loss: 0.10314964486846555
ROC train: 0.998030	val: 0.895044	test: 0.928310
PRC train: 0.999406	val: 0.964715	test: 0.976266

Epoch: 109
Loss: 0.11719275943226186
ROC train: 0.998383	val: 0.907501	test: 0.925955
PRC train: 0.999510	val: 0.968567	test: 0.974024

Epoch: 110
Loss: 0.1060113476243628
ROC train: 0.997910	val: 0.901007	test: 0.919545
PRC train: 0.999364	val: 0.965761	test: 0.971202

Epoch: 111
Loss: 0.11169719221759015
ROC train: 0.998483	val: 0.905380	test: 0.925693
PRC train: 0.999550	val: 0.968063	test: 0.974576

Epoch: 112
Loss: 0.12209702237508648
ROC train: 0.998342	val: 0.904453	test: 0.924516
PRC train: 0.999504	val: 0.967452	test: 0.972152

Epoch: 113
Loss: 0.10029020912818713
ROC train: 0.997544	val: 0.910946	test: 0.915228
PRC train: 0.999271	val: 0.971974	test: 0.970086

Epoch: 114
Loss: 0.1001009693468466
ROC train: 0.997936	val: 0.909223	test: 0.929226
PRC train: 0.999377	val: 0.969791	test: 0.973921

Epoch: 115
Loss: 0.11176252620245011
ROC train: 0.998688	val: 0.902067	test: 0.925955
PRC train: 0.999606	val: 0.967905	test: 0.972368

Epoch: 116
Loss: 0.0987608139577705
ROC train: 0.998049	val: 0.888550	test: 0.928179
PRC train: 0.999416	val: 0.960941	test: 0.975677

Epoch: 117
Loss: 0.10517078879944457
ROC train: 0.998141	val: 0.882057	test: 0.923600
PRC train: 0.999436	val: 0.957304	test: 0.973073

Epoch: 118
Loss: 0.0874452555621474
ROC train: 0.997581	val: 0.894249	test: 0.930665
PRC train: 0.999248	val: 0.962437	test: 0.977063

Epoch: 119
Loss: 0.10771966992406667
ROC train: 0.998722	val: 0.904983	test: 0.940999
PRC train: 0.999614	val: 0.966358	test: 0.981015

Epoch: 120
Loss: 0.09362637726528956
ROC train: 0.998873	val: 0.908163	test: 0.936421
PRC train: 0.999658	val: 0.970905	test: 0.979475

Early stopping
Best (ROC):	 train: 0.988356	val: 0.935330	test: 0.930665
Best (PRC):	 train: 0.996262	val: 0.979621	test: 0.975276

ROC train: 0.996796	val: 0.912404	test: 0.943485
PRC train: 0.998998	val: 0.970626	test: 0.981307

Epoch: 95
Loss: 0.11809604318447389
ROC train: 0.997362	val: 0.894116	test: 0.933673
PRC train: 0.999188	val: 0.964054	test: 0.976660

Epoch: 96
Loss: 0.11189084051085936
ROC train: 0.997259	val: 0.880069	test: 0.938383
PRC train: 0.999163	val: 0.955374	test: 0.978042

Epoch: 97
Loss: 0.12617996570945486
ROC train: 0.997420	val: 0.892658	test: 0.930665
PRC train: 0.999210	val: 0.961210	test: 0.976092

Epoch: 98
Loss: 0.12219061157131603
ROC train: 0.997194	val: 0.893056	test: 0.929487
PRC train: 0.999136	val: 0.964629	test: 0.974552

Epoch: 99
Loss: 0.11871418134095872
ROC train: 0.997208	val: 0.881262	test: 0.931057
PRC train: 0.999151	val: 0.957269	test: 0.974584

Epoch: 100
Loss: 0.10320482330973713
ROC train: 0.997574	val: 0.891201	test: 0.930403
PRC train: 0.999253	val: 0.960002	test: 0.976303

Epoch: 101
Loss: 0.10898289911112737
ROC train: 0.998019	val: 0.891068	test: 0.927917
PRC train: 0.999386	val: 0.961509	test: 0.975500

Epoch: 102
Loss: 0.10826750535176677
ROC train: 0.997503	val: 0.883514	test: 0.925824
PRC train: 0.999219	val: 0.959985	test: 0.974987

Epoch: 103
Loss: 0.10694521358232796
ROC train: 0.997948	val: 0.877949	test: 0.914050
PRC train: 0.999372	val: 0.955180	test: 0.969021

Epoch: 104
Loss: 0.10544192309945706
ROC train: 0.997609	val: 0.904320	test: 0.904762
PRC train: 0.999275	val: 0.965607	test: 0.962302

Epoch: 105
Loss: 0.13264190663883202
ROC train: 0.997738	val: 0.894514	test: 0.921115
PRC train: 0.999315	val: 0.965410	test: 0.972252

Epoch: 106
Loss: 0.12922955416377752
ROC train: 0.997434	val: 0.887888	test: 0.926609
PRC train: 0.999222	val: 0.956285	test: 0.970184

Epoch: 107
Loss: 0.114913051583918
ROC train: 0.998210	val: 0.886695	test: 0.923862
PRC train: 0.999457	val: 0.957505	test: 0.970651

Epoch: 108
Loss: 0.12391988746169742
ROC train: 0.998022	val: 0.887755	test: 0.915489
PRC train: 0.999402	val: 0.960191	test: 0.969101

Epoch: 109
Loss: 0.11647617288233447
ROC train: 0.997319	val: 0.902332	test: 0.917321
PRC train: 0.999169	val: 0.967250	test: 0.968638

Epoch: 110
Loss: 0.11058818049876909
ROC train: 0.996818	val: 0.916777	test: 0.917452
PRC train: 0.999009	val: 0.973328	test: 0.969939

Epoch: 111
Loss: 0.09906065534005774
ROC train: 0.997327	val: 0.901935	test: 0.914835
PRC train: 0.999159	val: 0.966441	test: 0.968701

Epoch: 112
Loss: 0.09628696597888241
ROC train: 0.998492	val: 0.887623	test: 0.918629
PRC train: 0.999536	val: 0.959766	test: 0.970507

Epoch: 113
Loss: 0.10688297737699896
ROC train: 0.998872	val: 0.898754	test: 0.915228
PRC train: 0.999657	val: 0.965231	test: 0.965476

Epoch: 114
Loss: 0.10510881943440532
ROC train: 0.998690	val: 0.899682	test: 0.922423
PRC train: 0.999602	val: 0.963977	test: 0.971802

Epoch: 115
Loss: 0.11419727653737669
ROC train: 0.998532	val: 0.890406	test: 0.923731
PRC train: 0.999558	val: 0.960523	test: 0.973943

Epoch: 116
Loss: 0.11210252435726577
ROC train: 0.998385	val: 0.888550	test: 0.911172
PRC train: 0.999495	val: 0.962105	test: 0.966920

Epoch: 117
Loss: 0.11774126678530868
ROC train: 0.998307	val: 0.901537	test: 0.916928
PRC train: 0.999480	val: 0.967032	test: 0.969477

Epoch: 118
Loss: 0.11563086921179289
ROC train: 0.998473	val: 0.902995	test: 0.923469
PRC train: 0.999540	val: 0.966122	test: 0.973240

Epoch: 119
Loss: 0.09680495559200401
ROC train: 0.997627	val: 0.902862	test: 0.918106
PRC train: 0.999270	val: 0.967988	test: 0.971136

Epoch: 120
Loss: 0.08294472401667904
ROC train: 0.997562	val: 0.893586	test: 0.915097
PRC train: 0.999266	val: 0.965600	test: 0.969871

Early stopping
Best (ROC):	 train: 0.992235	val: 0.923933	test: 0.935767
Best (PRC):	 train: 0.997587	val: 0.975129	test: 0.978405

ROC train: 0.996435	val: 0.914392	test: 0.926740
PRC train: 0.998922	val: 0.969068	test: 0.972577

Epoch: 95
Loss: 0.11732438925916613
ROC train: 0.997159	val: 0.914922	test: 0.926347
PRC train: 0.999136	val: 0.970401	test: 0.972783

Epoch: 96
Loss: 0.13004553906763344
ROC train: 0.997773	val: 0.906043	test: 0.927786
PRC train: 0.999310	val: 0.965457	test: 0.970461

Epoch: 97
Loss: 0.11367307212285944
ROC train: 0.997374	val: 0.907368	test: 0.929880
PRC train: 0.999188	val: 0.967265	test: 0.972991

Epoch: 98
Loss: 0.1118857715963358
ROC train: 0.997693	val: 0.911079	test: 0.924908
PRC train: 0.999296	val: 0.967050	test: 0.970507

Epoch: 99
Loss: 0.1159909415033251
ROC train: 0.998375	val: 0.914922	test: 0.923993
PRC train: 0.999508	val: 0.968729	test: 0.971001

Epoch: 100
Loss: 0.11389583815872073
ROC train: 0.998498	val: 0.911741	test: 0.929356
PRC train: 0.999544	val: 0.968810	test: 0.974398

Epoch: 101
Loss: 0.1125909363388506
ROC train: 0.997314	val: 0.896634	test: 0.928702
PRC train: 0.999185	val: 0.960493	test: 0.974314

Epoch: 102
Loss: 0.11114879238366182
ROC train: 0.997835	val: 0.906838	test: 0.925955
PRC train: 0.999345	val: 0.967437	test: 0.973644

Epoch: 103
Loss: 0.10892475037034609
ROC train: 0.998087	val: 0.920355	test: 0.927132
PRC train: 0.999426	val: 0.972028	test: 0.974044

Epoch: 104
Loss: 0.10378731216244448
ROC train: 0.998459	val: 0.908826	test: 0.925170
PRC train: 0.999539	val: 0.964490	test: 0.973520

Epoch: 105
Loss: 0.09710346061302368
ROC train: 0.998573	val: 0.901007	test: 0.920068
PRC train: 0.999569	val: 0.960911	test: 0.965012

Epoch: 106
Loss: 0.09758380486233652
ROC train: 0.998616	val: 0.898887	test: 0.916274
PRC train: 0.999581	val: 0.960448	test: 0.960498

Epoch: 107
Loss: 0.09959976745840915
ROC train: 0.998099	val: 0.901670	test: 0.919806
PRC train: 0.999420	val: 0.963514	test: 0.962550

Epoch: 108
Loss: 0.09428062467398245
ROC train: 0.997817	val: 0.906573	test: 0.924516
PRC train: 0.999335	val: 0.965246	test: 0.970379

Epoch: 109
Loss: 0.11970543586452252
ROC train: 0.997408	val: 0.901935	test: 0.916405
PRC train: 0.999208	val: 0.963450	test: 0.969125

Epoch: 110
Loss: 0.11389321022530388
ROC train: 0.997213	val: 0.904055	test: 0.922946
PRC train: 0.999168	val: 0.965713	test: 0.970773

Epoch: 111
Loss: 0.09619176624753768
ROC train: 0.997782	val: 0.908031	test: 0.923600
PRC train: 0.999333	val: 0.968732	test: 0.971545

Epoch: 112
Loss: 0.11355137760807911
ROC train: 0.998251	val: 0.903923	test: 0.922946
PRC train: 0.999468	val: 0.966464	test: 0.971209

Epoch: 113
Loss: 0.11890232870677567
ROC train: 0.998090	val: 0.903260	test: 0.921115
PRC train: 0.999416	val: 0.964646	test: 0.969004

Epoch: 114
Loss: 0.11774090706915157
ROC train: 0.998000	val: 0.898092	test: 0.911434
PRC train: 0.999400	val: 0.961368	test: 0.967184

Epoch: 115
Loss: 0.11112628605117561
ROC train: 0.997246	val: 0.894116	test: 0.898613
PRC train: 0.999152	val: 0.956532	test: 0.959672

Epoch: 116
Loss: 0.11007063377407118
ROC train: 0.998285	val: 0.902862	test: 0.912480
PRC train: 0.999487	val: 0.964176	test: 0.964052

Epoch: 117
Loss: 0.09702712220246265
ROC train: 0.998472	val: 0.919428	test: 0.927132
PRC train: 0.999542	val: 0.972793	test: 0.973961

Epoch: 118
Loss: 0.09842206301175073
ROC train: 0.998462	val: 0.924993	test: 0.929618
PRC train: 0.999534	val: 0.974978	test: 0.975997

Epoch: 119
Loss: 0.09605442342379
ROC train: 0.998063	val: 0.916777	test: 0.929749
PRC train: 0.999423	val: 0.973050	test: 0.974290

Epoch: 120
Loss: 0.10147923341575424
ROC train: 0.998400	val: 0.920355	test: 0.924908
PRC train: 0.999512	val: 0.971991	test: 0.971447

Epoch: 121
Loss: 0.09478730620593012
ROC train: 0.998093	val: 0.910284	test: 0.923862
PRC train: 0.999424	val: 0.967747	test: 0.971442

Epoch: 122
Loss: 0.09059117188783442
ROC train: 0.998443	val: 0.908826	test: 0.929749
PRC train: 0.999533	val: 0.968685	test: 0.974328

Epoch: 123
Loss: 0.08799166051628607
ROC train: 0.998921	val: 0.906308	test: 0.928310
PRC train: 0.999673	val: 0.966984	test: 0.972511

Epoch: 124
Loss: 0.09564737614636996
ROC train: 0.999132	val: 0.927114	test: 0.928441
PRC train: 0.999737	val: 0.976620	test: 0.973613

Epoch: 125
Loss: 0.08434919815731134
ROC train: 0.997780	val: 0.916777	test: 0.909995
PRC train: 0.999320	val: 0.973060	test: 0.966608

Epoch: 126
Loss: 0.08860696725153229
ROC train: 0.998038	val: 0.903393	test: 0.911695
PRC train: 0.999411	val: 0.966560	test: 0.967512

Epoch: 127
Loss: 0.11372416282152861
ROC train: 0.998932	val: 0.901670	test: 0.923993
PRC train: 0.999677	val: 0.964009	test: 0.971573

Epoch: 128
Loss: 0.08504432263767636
ROC train: 0.998869	val: 0.890936	test: 0.923208
PRC train: 0.999656	val: 0.961419	test: 0.971472

Epoch: 129
Loss: 0.09680409090189827
ROC train: 0.999206	val: 0.897032	test: 0.924647
PRC train: 0.999758	val: 0.963373	test: 0.972560

Epoch: 130
Loss: 0.09565628676723373
ROC train: 0.999322	val: 0.893586	test: 0.926217
PRC train: 0.999797	val: 0.962920	test: 0.974565

Epoch: 131
Loss: 0.10271245299631773
ROC train: 0.998892	val: 0.895971	test: 0.918106
PRC train: 0.999671	val: 0.961749	test: 0.971773

Epoch: 132
Loss: 0.08189548556995048
ROC train: 0.999028	val: 0.906971	test: 0.923731
PRC train: 0.999707	val: 0.964085	test: 0.972601

Epoch: 133
Loss: 0.0902500814062543
ROC train: 0.998821	val: 0.895971	test: 0.919283
PRC train: 0.999642	val: 0.959778	test: 0.971016

Epoch: 134
Loss: 0.07659720298758095
ROC train: 0.999166	val: 0.890936	test: 0.918237
PRC train: 0.999746	val: 0.955277	test: 0.967485

Epoch: 135
Loss: 0.09031947120797335
ROC train: 0.998571	val: 0.891201	test: 0.920330
PRC train: 0.999558	val: 0.957239	test: 0.971836

Epoch: 136
Loss: 0.07896278801782224
ROC train: 0.999120	val: 0.917970	test: 0.916274
PRC train: 0.999732	val: 0.970606	test: 0.968345

Epoch: 137
Loss: 0.08213704222420522
ROC train: 0.998953	val: 0.921680	test: 0.922684
PRC train: 0.999679	val: 0.971558	test: 0.969369

Epoch: 138
Loss: 0.08113791389353511
ROC train: 0.999196	val: 0.912669	test: 0.925824
PRC train: 0.999756	val: 0.970338	test: 0.972186

Epoch: 139
Loss: 0.09600215341076922
ROC train: 0.998564	val: 0.893321	test: 0.930534
PRC train: 0.999560	val: 0.959182	test: 0.974342

Epoch: 140
Loss: 0.08230207734484267
ROC train: 0.998762	val: 0.897429	test: 0.930665
PRC train: 0.999620	val: 0.960231	test: 0.973958

Epoch: 141
Loss: 0.09320410362599894
ROC train: 0.999184	val: 0.906838	test: 0.920722
PRC train: 0.999744	val: 0.966144	test: 0.969928

Epoch: 142
Loss: 0.10151361597398385
ROC train: 0.998936	val: 0.913862	test: 0.919545
PRC train: 0.999678	val: 0.969929	test: 0.970854

Epoch: 143
Loss: 0.0851883263765258
ROC train: 0.998999	val: 0.909488	test: 0.925563
PRC train: 0.999699	val: 0.969698	test: 0.973577

Epoch: 144
Loss: 0.07360371995102657
ROC train: 0.999058	val: 0.911211	test: 0.931842
PRC train: 0.999713	val: 0.968664	test: 0.974386

Epoch: 145
Loss: 0.08979315007243797
ROC train: 0.999263	val: 0.917837	test: 0.937075
PRC train: 0.999774	val: 0.969778	test: 0.976537

Epoch: 146
Loss: 0.091841477819209
ROC train: 0.999012	val: 0.917705	test: 0.917452
PRC train: 0.999697	val: 0.969919	test: 0.968436

Epoch: 147
Loss: 0.09260449272640324
ROC train: 0.998422	val: 0.906971	test: 0.901361
PRC train: 0.999496	val: 0.965286	test: 0.948672

Epoch: 148
Loss: 0.08225495185050495
ROC train: 0.998871	val: 0.916247	test: 0.900706
PRC train: 0.999652	val: 0.970205	test: 0.949071

Epoch: 149
Loss: 0.07909468243780902
ROC train: 0.999499	val: 0.917705	test: 0.916274
PRC train: 0.999847	val: 0.969833	test: 0.966582

Epoch: 150
Loss: 0.07104530919252905
ROC train: 0.999412	val: 0.916247	test: 0.920984
PRC train: 0.999820	val: 0.968311	test: 0.970932

Epoch: 151
Loss: 0.08310565226653563
ROC train: 0.999545	val: 0.907103	test: 0.925170
PRC train: 0.999862	val: 0.961745	test: 0.969902

Epoch: 152
Loss: 0.06694870662459022
ROC train: 0.999377	val: 0.911079	test: 0.916405
PRC train: 0.999809	val: 0.967165	test: 0.964230

Epoch: 153
Loss: 0.07480323756107081
ROC train: 0.999430	val: 0.912139	test: 0.907378
PRC train: 0.999824	val: 0.968003	test: 0.961450

Epoch: 154
Loss: 0.0709368494434854
ROC train: 0.999626	val: 0.915982	test: 0.911303
PRC train: 0.999886	val: 0.970375	test: 0.962653

Epoch: 155
Loss: 0.08347159359989507
ROC train: 0.999609	val: 0.913199	test: 0.915620
PRC train: 0.999881	val: 0.966775	test: 0.965177

Epoch: 156
Loss: 0.07572487046445578
ROC train: 0.999220	val: 0.906440	test: 0.927656
PRC train: 0.999761	val: 0.964804	test: 0.973781

Epoch: 157
Loss: 0.11148167302145048
ROC train: 0.999395	val: 0.907501	test: 0.923077
PRC train: 0.999816	val: 0.967209	test: 0.969061

Epoch: 158
Loss: 0.08474609982638524
ROC train: 0.999311	val: 0.910416	test: 0.927263
PRC train: 0.999794	val: 0.968810	test: 0.974363

Epoch: 159
Loss: 0.07534661989383278
ROC train: 0.999100	val: 0.917175	test: 0.924123
PRC train: 0.999730	val: 0.971554	test: 0.973129

Early stopping
Best (ROC):	 train: 0.999132	val: 0.927114	test: 0.928441
Best (PRC):	 train: 0.999737	val: 0.976620	test: 0.973613
All runs completed.
