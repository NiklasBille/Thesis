>>> Starting run for dataset: bbbp
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running RANDOM configs_split_experiments/GraphMVP/bbbp/random/train_prop=0.6.yml on cuda:0
Running RANDOM configs_split_experiments/GraphMVP/bbbp/random/train_prop=0.7.yml on cuda:1
Running RANDOM configs_split_experiments/GraphMVP/bbbp/random/train_prop=0.8.yml on cuda:2
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphMVP/bbbp/random/train_prop=0.8.yml --runseed 4 --device cuda:2
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphMVP/bbbp/random/train_prop=0.7.yml --runseed 4 --device cuda:1
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphMVP/bbbp/random/train_prop=0.8.yml --runseed 5 --device cuda:2
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphMVP/bbbp/random/train_prop=0.7.yml --runseed 5 --device cuda:1
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphMVP/bbbp/random/train_prop=0.8.yml --runseed 6 --device cuda:2
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphMVP/bbbp/random/train_prop=0.7.yml --runseed 6 --device cuda:1
Starting process for seed 4: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphMVP/bbbp/random/train_prop=0.6.yml --runseed 4 --device cuda:0
Starting process for seed 5: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphMVP/bbbp/random/train_prop=0.6.yml --runseed 5 --device cuda:0
Starting process for seed 6: python molecule_finetune.py --config /workspace/configs_split_experiments/GraphMVP/bbbp/random/train_prop=0.6.yml --runseed 6 --device cuda:0
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/bbbp/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/split/GraphMVP/bbbp/random/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/bbbp/random/train_prop=0.6/bbbp_random_6_26-05_11-06-56  ]
[ Using Seed :  6  ]
[ Using device :  cuda:0  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
randomly split
Data(edge_attr=[58, 2], edge_index=[2, 58], id=[1], x=[27, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6540722501004287
ROC train: 0.784921	val: 0.782493	test: 0.737384
PRC train: 0.911870	val: 0.917648	test: 0.885723

Epoch: 2
Loss: 0.55871844587141
ROC train: 0.824338	val: 0.815801	test: 0.768882
PRC train: 0.929517	val: 0.928953	test: 0.901314

Epoch: 3
Loss: 0.4806893105683345
ROC train: 0.847185	val: 0.834834	test: 0.799199
PRC train: 0.938303	val: 0.935361	test: 0.919469

Epoch: 4
Loss: 0.4306046100329608
ROC train: 0.862823	val: 0.845983	test: 0.822396
PRC train: 0.940756	val: 0.934920	test: 0.920089

Epoch: 5
Loss: 0.38766942642279856
ROC train: 0.885211	val: 0.860632	test: 0.852779
PRC train: 0.953479	val: 0.940313	test: 0.943509

Epoch: 6
Loss: 0.3657851211658126
ROC train: 0.900140	val: 0.868755	test: 0.871678
PRC train: 0.961975	val: 0.943810	test: 0.950560

Epoch: 7
Loss: 0.34955664357021793
ROC train: 0.912066	val: 0.885341	test: 0.877715
PRC train: 0.968354	val: 0.950946	test: 0.950195

Epoch: 8
Loss: 0.33500087455458044
ROC train: 0.917458	val: 0.884661	test: 0.872826
PRC train: 0.970796	val: 0.947218	test: 0.944545

Epoch: 9
Loss: 0.3221918635860278
ROC train: 0.922439	val: 0.890337	test: 0.877190
PRC train: 0.971985	val: 0.948937	test: 0.945762

Epoch: 10
Loss: 0.3256618135235599
ROC train: 0.929454	val: 0.893464	test: 0.881357
PRC train: 0.975320	val: 0.948094	test: 0.951550

Epoch: 11
Loss: 0.30919816906676995
ROC train: 0.933213	val: 0.891221	test: 0.874172
PRC train: 0.976975	val: 0.948802	test: 0.947913

Epoch: 12
Loss: 0.29535716383138955
ROC train: 0.937035	val: 0.897305	test: 0.882965
PRC train: 0.978561	val: 0.950742	test: 0.953087

Epoch: 13
Loss: 0.2969995369049933
ROC train: 0.937104	val: 0.895775	test: 0.879093
PRC train: 0.978642	val: 0.950020	test: 0.951102

Epoch: 14
Loss: 0.2821631749396193
ROC train: 0.942488	val: 0.901043	test: 0.883687
PRC train: 0.980607	val: 0.952789	test: 0.952856

Epoch: 15
Loss: 0.27720073813352275
ROC train: 0.941422	val: 0.891969	test: 0.867708
PRC train: 0.980194	val: 0.947732	test: 0.942860

Epoch: 16
Loss: 0.27873050649223496
ROC train: 0.945036	val: 0.892580	test: 0.876206
PRC train: 0.980772	val: 0.947120	test: 0.946934

Epoch: 17
Loss: 0.26317565921043296
ROC train: 0.949861	val: 0.897237	test: 0.878535
PRC train: 0.982443	val: 0.951180	test: 0.948658

Epoch: 18
Loss: 0.25410131914785306
ROC train: 0.949427	val: 0.889487	test: 0.879323
PRC train: 0.982749	val: 0.948092	test: 0.948119

Epoch: 19
Loss: 0.27553641488514463
ROC train: 0.954894	val: 0.904340	test: 0.891496
PRC train: 0.984839	val: 0.953368	test: 0.957834

Epoch: 20
Loss: 0.2602730525275743
ROC train: 0.953522	val: 0.898256	test: 0.866888
PRC train: 0.984281	val: 0.949965	test: 0.938958

Epoch: 21
Loss: 0.24887051639048954
ROC train: 0.960035	val: 0.912328	test: 0.884376
PRC train: 0.986965	val: 0.955646	test: 0.953924

Epoch: 22
Loss: 0.25564477742013375
ROC train: 0.961758	val: 0.903593	test: 0.885918
PRC train: 0.987591	val: 0.952443	test: 0.954312

Epoch: 23
Loss: 0.2398085923195926
ROC train: 0.963292	val: 0.903389	test: 0.887591
PRC train: 0.988115	val: 0.953360	test: 0.955184

Epoch: 24
Loss: 0.2430039597491594
ROC train: 0.964494	val: 0.902063	test: 0.882604
PRC train: 0.988609	val: 0.954320	test: 0.951088

Epoch: 25
Loss: 0.23876217947687986
ROC train: 0.965105	val: 0.894688	test: 0.874697
PRC train: 0.988777	val: 0.949841	test: 0.945589

Epoch: 26
Loss: 0.22739222841218001
ROC train: 0.966395	val: 0.903729	test: 0.885032
PRC train: 0.988874	val: 0.955210	test: 0.954522

Epoch: 27
Loss: 0.21894266487387015
ROC train: 0.968608	val: 0.900941	test: 0.880537
PRC train: 0.989772	val: 0.953178	test: 0.950686

Epoch: 28
Loss: 0.23663084478577945
ROC train: 0.966795	val: 0.896421	test: 0.870858
PRC train: 0.989118	val: 0.952140	test: 0.946734

Epoch: 29
Loss: 0.2080434499211671
ROC train: 0.975055	val: 0.900975	test: 0.885754
PRC train: 0.991937	val: 0.953562	test: 0.954842

Epoch: 30
Loss: 0.21961137410915138
ROC train: 0.972647	val: 0.890745	test: 0.868692
PRC train: 0.991024	val: 0.948609	test: 0.944861

Epoch: 31
Loss: 0.22519796034353598
ROC train: 0.970962	val: 0.887448	test: 0.870267
PRC train: 0.990398	val: 0.947880	test: 0.945861

Epoch: 32
Loss: 0.22182783459851318
ROC train: 0.976414	val: 0.894314	test: 0.903537
PRC train: 0.992459	val: 0.947918	test: 0.962163

Epoch: 33
Loss: 0.21078086582092764Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/bbbp/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/split/GraphMVP/bbbp/random/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/bbbp/random/train_prop=0.6/bbbp_random_4_26-05_11-06-56  ]
[ Using Seed :  4  ]
[ Using device :  cuda:0  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
randomly split
Data(edge_attr=[58, 2], edge_index=[2, 58], id=[1], x=[27, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6365115218895063
ROC train: 0.800995	val: 0.797176	test: 0.755004
PRC train: 0.922143	val: 0.921656	test: 0.903271

Epoch: 2
Loss: 0.5416047589718594
ROC train: 0.824598	val: 0.821885	test: 0.774985
PRC train: 0.929262	val: 0.927824	test: 0.904323

Epoch: 3
Loss: 0.4662247672562721
ROC train: 0.840987	val: 0.832557	test: 0.803694
PRC train: 0.932319	val: 0.930150	test: 0.905045

Epoch: 4
Loss: 0.408604503272511
ROC train: 0.851399	val: 0.842380	test: 0.815506
PRC train: 0.938203	val: 0.939105	test: 0.916780

Epoch: 5
Loss: 0.3881833694774118
ROC train: 0.880759	val: 0.867327	test: 0.861080
PRC train: 0.952989	val: 0.948073	test: 0.945539

Epoch: 6
Loss: 0.35871628798088795
ROC train: 0.904557	val: 0.878543	test: 0.884408
PRC train: 0.964429	val: 0.948083	test: 0.953320

Epoch: 7
Loss: 0.3420530389474532
ROC train: 0.917307	val: 0.885409	test: 0.891824
PRC train: 0.970329	val: 0.950843	test: 0.956658

Epoch: 8
Loss: 0.32410837118126123
ROC train: 0.922707	val: 0.890048	test: 0.895498
PRC train: 0.972701	val: 0.953070	test: 0.960429

Epoch: 9
Loss: 0.31179830381930185
ROC train: 0.925712	val: 0.894008	test: 0.896483
PRC train: 0.973048	val: 0.954657	test: 0.961571

Epoch: 10
Loss: 0.300741754569808
ROC train: 0.933452	val: 0.897713	test: 0.894777
PRC train: 0.976801	val: 0.955212	test: 0.960124

Epoch: 11
Loss: 0.3058670857019751
ROC train: 0.936638	val: 0.898902	test: 0.894186
PRC train: 0.978046	val: 0.954957	test: 0.960755

Epoch: 12
Loss: 0.2844101211724415
ROC train: 0.940190	val: 0.898154	test: 0.891135
PRC train: 0.979347	val: 0.953397	test: 0.959821

Epoch: 13
Loss: 0.2767074187511713
ROC train: 0.941098	val: 0.894008	test: 0.884146
PRC train: 0.979847	val: 0.949824	test: 0.953946

Epoch: 14
Loss: 0.2804063439831216
ROC train: 0.945710	val: 0.896251	test: 0.883851
PRC train: 0.981365	val: 0.951163	test: 0.953616

Epoch: 15
Loss: 0.26897054785661717
ROC train: 0.948576	val: 0.900160	test: 0.885458
PRC train: 0.982343	val: 0.953651	test: 0.954538

Epoch: 16
Loss: 0.28809171520120713
ROC train: 0.951795	val: 0.902811	test: 0.897401
PRC train: 0.983429	val: 0.954940	test: 0.960945

Epoch: 17
Loss: 0.2618529610918401
ROC train: 0.952493	val: 0.900092	test: 0.898878
PRC train: 0.983635	val: 0.953607	test: 0.961945

Epoch: 18
Loss: 0.25371450931325457
ROC train: 0.954045	val: 0.893668	test: 0.888477
PRC train: 0.984283	val: 0.948427	test: 0.955496

Epoch: 19
Loss: 0.25631615646630496
ROC train: 0.958884	val: 0.899616	test: 0.896778
PRC train: 0.985927	val: 0.951665	test: 0.960955

Epoch: 20
Loss: 0.2480618098596684
ROC train: 0.959804	val: 0.896897	test: 0.893267
PRC train: 0.986267	val: 0.950765	test: 0.958825

Epoch: 21
Loss: 0.23628895450756754
ROC train: 0.957524	val: 0.898800	test: 0.888805
PRC train: 0.985621	val: 0.953710	test: 0.957805

Epoch: 22
Loss: 0.2546849434141861
ROC train: 0.964911	val: 0.899718	test: 0.892545
PRC train: 0.988211	val: 0.951881	test: 0.957675

Epoch: 23
Loss: 0.24765496314928398
ROC train: 0.966650	val: 0.904612	test: 0.897762
PRC train: 0.988895	val: 0.956762	test: 0.961938

Epoch: 24
Loss: 0.23066776295560015
ROC train: 0.968124	val: 0.901995	test: 0.893169
PRC train: 0.989463	val: 0.954486	test: 0.957946

Epoch: 25
Loss: 0.24089865292074553
ROC train: 0.969676	val: 0.902403	test: 0.892086
PRC train: 0.990038	val: 0.952691	test: 0.958396

Epoch: 26
Loss: 0.21455307362764825
ROC train: 0.970517	val: 0.905734	test: 0.893530
PRC train: 0.990318	val: 0.953244	test: 0.960621

Epoch: 27
Loss: 0.20773337128682257
ROC train: 0.968198	val: 0.903729	test: 0.897172
PRC train: 0.989450	val: 0.953698	test: 0.962933

Epoch: 28
Loss: 0.22237642058474774
ROC train: 0.971167	val: 0.897849	test: 0.892677
PRC train: 0.990160	val: 0.948415	test: 0.957766

Epoch: 29
Loss: 0.2253997230675747
ROC train: 0.974324	val: 0.898324	test: 0.899108
PRC train: 0.991266	val: 0.946693	test: 0.960879

Epoch: 30
Loss: 0.21413758507381492
ROC train: 0.974056	val: 0.900568	test: 0.905473
PRC train: 0.991388	val: 0.949622	test: 0.965726

Epoch: 31
Loss: 0.22215787001186177
ROC train: 0.972557	val: 0.894790	test: 0.892283
PRC train: 0.990522	val: 0.948759	test: 0.959232

Epoch: 32
Loss: 0.22378994790498288
ROC train: 0.975885	val: 0.902488	test: 0.894908
PRC train: 0.991879	val: 0.950864	test: 0.961871

Epoch: 33
Loss: 0.21910369031212684Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/bbbp/random/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/split/GraphMVP/bbbp/random/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/bbbp/random/train_prop=0.6/bbbp_random_5_26-05_11-06-56  ]
[ Using Seed :  5  ]
[ Using device :  cuda:0  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
randomly split
Data(edge_attr=[58, 2], edge_index=[2, 58], id=[1], x=[27, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6655528824730939
ROC train: 0.784819	val: 0.794932	test: 0.758449
PRC train: 0.907998	val: 0.922250	test: 0.887876

Epoch: 2
Loss: 0.5628439264674415
ROC train: 0.829216	val: 0.830688	test: 0.784828
PRC train: 0.926918	val: 0.931566	test: 0.896217

Epoch: 3
Loss: 0.47794726951770306
ROC train: 0.846803	val: 0.845167	test: 0.802546
PRC train: 0.935620	val: 0.937387	test: 0.908142

Epoch: 4
Loss: 0.42491331165022855
ROC train: 0.848060	val: 0.846662	test: 0.815802
PRC train: 0.935257	val: 0.937049	test: 0.916200

Epoch: 5
Loss: 0.3834357148930992
ROC train: 0.859473	val: 0.847546	test: 0.840213
PRC train: 0.938376	val: 0.935334	test: 0.924742

Epoch: 6
Loss: 0.36443224830433685
ROC train: 0.885993	val: 0.859680	test: 0.871186
PRC train: 0.953037	val: 0.936561	test: 0.945996

Epoch: 7
Loss: 0.3480695031367676
ROC train: 0.906601	val: 0.869468	test: 0.887755
PRC train: 0.964533	val: 0.937736	test: 0.956702

Epoch: 8
Loss: 0.35179843201553035
ROC train: 0.915669	val: 0.875110	test: 0.886672
PRC train: 0.969360	val: 0.938405	test: 0.955065

Epoch: 9
Loss: 0.3329538003667034
ROC train: 0.919747	val: 0.880651	test: 0.892217
PRC train: 0.971232	val: 0.940830	test: 0.957894

Epoch: 10
Loss: 0.3164678806359018
ROC train: 0.922407	val: 0.884695	test: 0.888214
PRC train: 0.972323	val: 0.944011	test: 0.957092

Epoch: 11
Loss: 0.30170418084368994
ROC train: 0.927045	val: 0.887822	test: 0.888739
PRC train: 0.973904	val: 0.946739	test: 0.958731

Epoch: 12
Loss: 0.3054326605989278
ROC train: 0.927708	val: 0.886564	test: 0.878207
PRC train: 0.974250	val: 0.947472	test: 0.953062

Epoch: 13
Loss: 0.29832044786030976
ROC train: 0.936871	val: 0.897662	test: 0.886968
PRC train: 0.977880	val: 0.954829	test: 0.956457

Epoch: 14
Loss: 0.28525994065979693
ROC train: 0.942259	val: 0.902097	test: 0.890183
PRC train: 0.980049	val: 0.956438	test: 0.957439

Epoch: 15
Loss: 0.2822446908572601
ROC train: 0.943244	val: 0.898630	test: 0.888674
PRC train: 0.980502	val: 0.952380	test: 0.955066

Epoch: 16
Loss: 0.27107289676634855
ROC train: 0.947696	val: 0.902896	test: 0.890938
PRC train: 0.982264	val: 0.952873	test: 0.956898

Epoch: 17
Loss: 0.277927219275948
ROC train: 0.947854	val: 0.899701	test: 0.892316
PRC train: 0.982263	val: 0.952368	test: 0.957650

Epoch: 18
Loss: 0.26176070829754605
ROC train: 0.953621	val: 0.904850	test: 0.897861
PRC train: 0.984222	val: 0.953185	test: 0.960551

Epoch: 19
Loss: 0.25732570850274245
ROC train: 0.952843	val: 0.903083	test: 0.895892
PRC train: 0.983894	val: 0.951161	test: 0.959292

Epoch: 20
Loss: 0.2600961664477954
ROC train: 0.950816	val: 0.898460	test: 0.886935
PRC train: 0.982996	val: 0.951741	test: 0.954370

Epoch: 21
Loss: 0.2572819143767324
ROC train: 0.957079	val: 0.907739	test: 0.892709
PRC train: 0.985356	val: 0.955140	test: 0.957675

Epoch: 22
Loss: 0.2553629009474826
ROC train: 0.961103	val: 0.907059	test: 0.890085
PRC train: 0.987035	val: 0.953262	test: 0.956897

Epoch: 23
Loss: 0.2511001482491061
ROC train: 0.957734	val: 0.892852	test: 0.879290
PRC train: 0.985640	val: 0.945689	test: 0.950184

Epoch: 24
Loss: 0.23935876821831256
ROC train: 0.963869	val: 0.899021	test: 0.894022
PRC train: 0.987582	val: 0.950098	test: 0.960024

Epoch: 25
Loss: 0.23533096100657844
ROC train: 0.964647	val: 0.889929	test: 0.892414
PRC train: 0.987939	val: 0.941517	test: 0.958055

Epoch: 26
Loss: 0.22529002139551313
ROC train: 0.968228	val: 0.903898	test: 0.899797
PRC train: 0.989381	val: 0.949030	test: 0.962778

Epoch: 27
Loss: 0.2349545224475375
ROC train: 0.966705	val: 0.895571	test: 0.887230
PRC train: 0.989134	val: 0.944769	test: 0.957008

Epoch: 28
Loss: 0.237618701245255
ROC train: 0.971764	val: 0.901315	test: 0.901601
PRC train: 0.990954	val: 0.944982	test: 0.962875

Epoch: 29
Loss: 0.21150703004139743
ROC train: 0.972645	val: 0.904187	test: 0.910230
PRC train: 0.991213	val: 0.949868	test: 0.966620

Epoch: 30
Loss: 0.22213215633744712
ROC train: 0.975118	val: 0.900228	test: 0.901732
PRC train: 0.991910	val: 0.950011	test: 0.961826

Epoch: 31
Loss: 0.20758427363333815
ROC train: 0.977214	val: 0.905088	test: 0.901732
PRC train: 0.992725	val: 0.951333	test: 0.962140

Epoch: 32
Loss: 0.2101576188395971
ROC train: 0.978523	val: 0.899140	test: 0.904685
PRC train: 0.993184	val: 0.947674	test: 0.962536

Epoch: 33
Loss: 0.20856001268882368Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/bbbp/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/split/GraphMVP/bbbp/random/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/bbbp/random/train_prop=0.7/bbbp_random_6_26-05_11-06-56  ]
[ Using Seed :  6  ]
[ Using device :  cuda:1  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
randomly split
Data(edge_attr=[58, 2], edge_index=[2, 58], id=[1], x=[27, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6493363094144321
ROC train: 0.796026	val: 0.760029	test: 0.772514
PRC train: 0.917777	val: 0.894073	test: 0.908586

Epoch: 2
Loss: 0.5426586869878401
ROC train: 0.824737	val: 0.783565	test: 0.796624
PRC train: 0.930121	val: 0.908361	test: 0.919551

Epoch: 3
Loss: 0.4731165168366816
ROC train: 0.851160	val: 0.812319	test: 0.827306
PRC train: 0.937741	val: 0.922198	test: 0.926724

Epoch: 4
Loss: 0.41900488743605147
ROC train: 0.870811	val: 0.840029	test: 0.874985
PRC train: 0.946476	val: 0.934961	test: 0.953411

Epoch: 5
Loss: 0.376264418473419
ROC train: 0.890346	val: 0.867449	test: 0.900482
PRC train: 0.955604	val: 0.944888	test: 0.964180

Epoch: 6
Loss: 0.37491372766300096
ROC train: 0.902626	val: 0.862058	test: 0.903617
PRC train: 0.962732	val: 0.941205	test: 0.963899

Epoch: 7
Loss: 0.34441007889206743
ROC train: 0.913617	val: 0.874000	test: 0.904159
PRC train: 0.967587	val: 0.948415	test: 0.965866

Epoch: 8
Loss: 0.3226428008623204
ROC train: 0.917165	val: 0.855768	test: 0.907776
PRC train: 0.968765	val: 0.934868	test: 0.964736

Epoch: 9
Loss: 0.32540592363504234
ROC train: 0.927308	val: 0.878203	test: 0.911935
PRC train: 0.973125	val: 0.945986	test: 0.966444

Epoch: 10
Loss: 0.31411465177110315
ROC train: 0.930053	val: 0.878928	test: 0.907173
PRC train: 0.974007	val: 0.945274	test: 0.965702

Epoch: 11
Loss: 0.30060854055628067
ROC train: 0.930676	val: 0.891101	test: 0.912055
PRC train: 0.974028	val: 0.950234	test: 0.969489

Epoch: 12
Loss: 0.29252416378251916
ROC train: 0.934247	val: 0.896493	test: 0.916215
PRC train: 0.975216	val: 0.951813	test: 0.971420

Epoch: 13
Loss: 0.2935148979302687
ROC train: 0.938306	val: 0.881536	test: 0.907535
PRC train: 0.976489	val: 0.942221	test: 0.966360

Epoch: 14
Loss: 0.2850500155820169
ROC train: 0.944264	val: 0.884319	test: 0.905606
PRC train: 0.978576	val: 0.946788	test: 0.965951

Epoch: 15
Loss: 0.27435800172637387
ROC train: 0.944759	val: 0.868435	test: 0.908680
PRC train: 0.978954	val: 0.942599	test: 0.965813

Epoch: 16
Loss: 0.27706270263472155
ROC train: 0.945817	val: 0.864841	test: 0.905787
PRC train: 0.979537	val: 0.937895	test: 0.964522

Epoch: 17
Loss: 0.26840400337294384
ROC train: 0.949852	val: 0.881304	test: 0.911212
PRC train: 0.981251	val: 0.946699	test: 0.967694

Epoch: 18
Loss: 0.2680292001627378
ROC train: 0.952011	val: 0.874928	test: 0.913984
PRC train: 0.982054	val: 0.940206	test: 0.968843

Epoch: 19
Loss: 0.2536458502941164
ROC train: 0.954589	val: 0.878348	test: 0.910307
PRC train: 0.983525	val: 0.937400	test: 0.967839

Epoch: 20
Loss: 0.2530820563592082
ROC train: 0.958388	val: 0.889536	test: 0.918264
PRC train: 0.985050	val: 0.942512	test: 0.970955

Epoch: 21
Loss: 0.24820250614569495
ROC train: 0.958514	val: 0.891971	test: 0.921881
PRC train: 0.984711	val: 0.946530	test: 0.972340

Epoch: 22
Loss: 0.2551905816777334
ROC train: 0.960281	val: 0.888551	test: 0.921941
PRC train: 0.985308	val: 0.945165	test: 0.972458

Epoch: 23
Loss: 0.23562347958396915
ROC train: 0.962478	val: 0.880464	test: 0.912477
PRC train: 0.986113	val: 0.943714	test: 0.968651

Epoch: 24
Loss: 0.24841114991648072
ROC train: 0.963582	val: 0.884783	test: 0.912417
PRC train: 0.987041	val: 0.946776	test: 0.968758

Epoch: 25
Loss: 0.24154507940451622
ROC train: 0.964633	val: 0.883797	test: 0.923026
PRC train: 0.987825	val: 0.941502	test: 0.973451

Epoch: 26
Loss: 0.23232677449023886
ROC train: 0.968351	val: 0.879739	test: 0.912297
PRC train: 0.989405	val: 0.935239	test: 0.967357

Epoch: 27
Loss: 0.2360902712903996
ROC train: 0.969007	val: 0.886464	test: 0.914768
PRC train: 0.989638	val: 0.941937	test: 0.968874

Epoch: 28
Loss: 0.2180792027524752
ROC train: 0.971662	val: 0.889826	test: 0.917902
PRC train: 0.990579	val: 0.942964	test: 0.969538

Epoch: 29
Loss: 0.22845390200385776
ROC train: 0.969833	val: 0.880783	test: 0.911814
PRC train: 0.990113	val: 0.941264	test: 0.968259

Epoch: 30
Loss: 0.251296853718556
ROC train: 0.970635	val: 0.874522	test: 0.916034
PRC train: 0.989932	val: 0.935701	test: 0.968792

Epoch: 31
Loss: 0.22965869151379906
ROC train: 0.966762	val: 0.867623	test: 0.899578
PRC train: 0.988674	val: 0.936424	test: 0.961769

Epoch: 32
Loss: 0.2296211725540717
ROC train: 0.964458	val: 0.855275	test: 0.907414
PRC train: 0.987913	val: 0.932661	test: 0.967644

Epoch: 33
Loss: 0.22789145934327593Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/bbbp/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/split/GraphMVP/bbbp/random/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/bbbp/random/train_prop=0.7/bbbp_random_4_26-05_11-06-56  ]
[ Using Seed :  4  ]
[ Using device :  cuda:1  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
randomly split
Data(edge_attr=[58, 2], edge_index=[2, 58], id=[1], x=[27, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6271121693803922
ROC train: 0.805134	val: 0.778377	test: 0.793550
PRC train: 0.922710	val: 0.904367	test: 0.918236

Epoch: 2
Loss: 0.5146478521875052
ROC train: 0.829625	val: 0.792957	test: 0.808499
PRC train: 0.931017	val: 0.909222	test: 0.919838

Epoch: 3
Loss: 0.46113871104923393
ROC train: 0.853228	val: 0.829478	test: 0.840205
PRC train: 0.937693	val: 0.923336	test: 0.923638

Epoch: 4
Loss: 0.3935615432861462
ROC train: 0.881711	val: 0.841217	test: 0.888246
PRC train: 0.954025	val: 0.940239	test: 0.959389

Epoch: 5
Loss: 0.36028939663172505
ROC train: 0.904571	val: 0.856377	test: 0.920374
PRC train: 0.963324	val: 0.942388	test: 0.970066

Epoch: 6
Loss: 0.3416717730134071
ROC train: 0.915743	val: 0.878348	test: 0.918867
PRC train: 0.968713	val: 0.954223	test: 0.971048

Epoch: 7
Loss: 0.3171243389718014
ROC train: 0.918611	val: 0.878232	test: 0.922242
PRC train: 0.969920	val: 0.953141	test: 0.973014

Epoch: 8
Loss: 0.31364025002546353
ROC train: 0.925131	val: 0.878464	test: 0.927366
PRC train: 0.972586	val: 0.949720	test: 0.975357

Epoch: 9
Loss: 0.31641663124178315
ROC train: 0.927381	val: 0.869275	test: 0.919048
PRC train: 0.973332	val: 0.942384	test: 0.972448

Epoch: 10
Loss: 0.29566945370352776
ROC train: 0.933561	val: 0.874580	test: 0.915431
PRC train: 0.975440	val: 0.949121	test: 0.970011

Epoch: 11
Loss: 0.3123236969067743
ROC train: 0.935868	val: 0.867072	test: 0.912417
PRC train: 0.976302	val: 0.945229	test: 0.967995

Epoch: 12
Loss: 0.2900238002687972
ROC train: 0.942389	val: 0.879855	test: 0.914467
PRC train: 0.978821	val: 0.949453	test: 0.969354

Epoch: 13
Loss: 0.2865450829729924
ROC train: 0.941711	val: 0.861739	test: 0.903617
PRC train: 0.978116	val: 0.939528	test: 0.965039

Epoch: 14
Loss: 0.2673880509003416
ROC train: 0.942999	val: 0.878870	test: 0.913683
PRC train: 0.978368	val: 0.949604	test: 0.970815

Epoch: 15
Loss: 0.27669413974589
ROC train: 0.944686	val: 0.861275	test: 0.907233
PRC train: 0.979440	val: 0.942147	test: 0.965878

Epoch: 16
Loss: 0.2668885528248632
ROC train: 0.950847	val: 0.886580	test: 0.913743
PRC train: 0.982134	val: 0.950301	test: 0.969348

Epoch: 17
Loss: 0.2647349013292675
ROC train: 0.952979	val: 0.875043	test: 0.903255
PRC train: 0.981910	val: 0.948193	test: 0.963819

Epoch: 18
Loss: 0.2532095361154411
ROC train: 0.956509	val: 0.884609	test: 0.904219
PRC train: 0.983341	val: 0.951159	test: 0.964454

Epoch: 19
Loss: 0.26577295661393696
ROC train: 0.955247	val: 0.875797	test: 0.909825
PRC train: 0.983255	val: 0.943554	test: 0.966567

Epoch: 20
Loss: 0.2531266586513432
ROC train: 0.953102	val: 0.870232	test: 0.913502
PRC train: 0.982852	val: 0.946292	test: 0.969894

Epoch: 21
Loss: 0.24831850869138475
ROC train: 0.962486	val: 0.882203	test: 0.920253
PRC train: 0.986097	val: 0.948839	test: 0.971679

Epoch: 22
Loss: 0.24252547731873678
ROC train: 0.965006	val: 0.875797	test: 0.910488
PRC train: 0.986617	val: 0.943746	test: 0.966433

Epoch: 23
Loss: 0.23060209331924256
ROC train: 0.964799	val: 0.877188	test: 0.915009
PRC train: 0.986836	val: 0.943215	test: 0.969840

Epoch: 24
Loss: 0.23826443346342074
ROC train: 0.967460	val: 0.872435	test: 0.908379
PRC train: 0.988476	val: 0.939487	test: 0.965878

Epoch: 25
Loss: 0.2315678070804077
ROC train: 0.967388	val: 0.861884	test: 0.903978
PRC train: 0.988111	val: 0.935414	test: 0.962459

Epoch: 26
Loss: 0.22763795834578515
ROC train: 0.970704	val: 0.874609	test: 0.917963
PRC train: 0.989465	val: 0.932555	test: 0.969859

Epoch: 27
Loss: 0.23411292800021796
ROC train: 0.968429	val: 0.863913	test: 0.898734
PRC train: 0.988625	val: 0.937610	test: 0.958734

Epoch: 28
Loss: 0.21191524274905874
ROC train: 0.968678	val: 0.863826	test: 0.893249
PRC train: 0.988981	val: 0.938182	test: 0.955020

Epoch: 29
Loss: 0.23119777920158513
ROC train: 0.975886	val: 0.875507	test: 0.905425
PRC train: 0.991931	val: 0.940351	test: 0.960685

Epoch: 30
Loss: 0.21060724234407413
ROC train: 0.973106	val: 0.865159	test: 0.901266
PRC train: 0.991116	val: 0.935523	test: 0.958956

Epoch: 31
Loss: 0.20827116605680118
ROC train: 0.978743	val: 0.868609	test: 0.901748
PRC train: 0.992925	val: 0.936676	test: 0.958621

Epoch: 32
Loss: 0.22760017619653336
ROC train: 0.976767	val: 0.874000	test: 0.905907
PRC train: 0.991969	val: 0.940157	test: 0.962264

Epoch: 33
Loss: 0.20849595250276265Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/bbbp/random/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/split/GraphMVP/bbbp/random/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/bbbp/random/train_prop=0.7/bbbp_random_5_26-05_11-06-56  ]
[ Using Seed :  5  ]
[ Using device :  cuda:1  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
randomly split
Data(edge_attr=[58, 2], edge_index=[2, 58], id=[1], x=[27, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6532764817860767
ROC train: 0.781432	val: 0.776058	test: 0.767812
PRC train: 0.909195	val: 0.900861	test: 0.897548

Epoch: 2
Loss: 0.5488267863532876
ROC train: 0.827583	val: 0.796551	test: 0.809343
PRC train: 0.927409	val: 0.903229	test: 0.915087

Epoch: 3
Loss: 0.4682483721885126
ROC train: 0.851455	val: 0.826638	test: 0.838156
PRC train: 0.935995	val: 0.916698	test: 0.924790

Epoch: 4
Loss: 0.40704761624882185
ROC train: 0.864366	val: 0.835971	test: 0.871127
PRC train: 0.943425	val: 0.926901	test: 0.947277

Epoch: 5
Loss: 0.3800942903911779
ROC train: 0.891816	val: 0.851391	test: 0.908318
PRC train: 0.956953	val: 0.938450	test: 0.967191

Epoch: 6
Loss: 0.3574635560423571
ROC train: 0.907640	val: 0.866377	test: 0.915431
PRC train: 0.964188	val: 0.943635	test: 0.969302

Epoch: 7
Loss: 0.35022983701372185
ROC train: 0.915743	val: 0.870464	test: 0.920133
PRC train: 0.967279	val: 0.942307	test: 0.971086

Epoch: 8
Loss: 0.32127568772961485
ROC train: 0.923103	val: 0.871971	test: 0.927908
PRC train: 0.970153	val: 0.943421	test: 0.974489

Epoch: 9
Loss: 0.3297589940866636
ROC train: 0.925989	val: 0.876000	test: 0.913261
PRC train: 0.972174	val: 0.949445	test: 0.969681

Epoch: 10
Loss: 0.3158816192923208
ROC train: 0.927438	val: 0.864319	test: 0.914949
PRC train: 0.973128	val: 0.942694	test: 0.969183

Epoch: 11
Loss: 0.2996233083713166
ROC train: 0.930515	val: 0.872087	test: 0.919952
PRC train: 0.974215	val: 0.944604	test: 0.971068

Epoch: 12
Loss: 0.30229495040066107
ROC train: 0.937260	val: 0.872725	test: 0.922182
PRC train: 0.976094	val: 0.944894	test: 0.972487

Epoch: 13
Loss: 0.2801590268225645
ROC train: 0.940180	val: 0.868029	test: 0.913683
PRC train: 0.977380	val: 0.942732	test: 0.969038

Epoch: 14
Loss: 0.2867564533472127
ROC train: 0.943390	val: 0.876000	test: 0.921278
PRC train: 0.978583	val: 0.947509	test: 0.973296

Epoch: 15
Loss: 0.27301531672644
ROC train: 0.943697	val: 0.876493	test: 0.922785
PRC train: 0.978639	val: 0.945315	test: 0.973393

Epoch: 16
Loss: 0.25438202600662874
ROC train: 0.947653	val: 0.888667	test: 0.915853
PRC train: 0.979636	val: 0.951641	test: 0.970730

Epoch: 17
Loss: 0.2685143716331628
ROC train: 0.949283	val: 0.870783	test: 0.913683
PRC train: 0.980319	val: 0.941304	test: 0.969106

Epoch: 18
Loss: 0.26369395686749125
ROC train: 0.954000	val: 0.878986	test: 0.926582
PRC train: 0.983085	val: 0.946344	test: 0.974908

Epoch: 19
Loss: 0.25246098927997684
ROC train: 0.957154	val: 0.883449	test: 0.927969
PRC train: 0.983992	val: 0.947840	test: 0.975487

Epoch: 20
Loss: 0.2604220746667028
ROC train: 0.958222	val: 0.879826	test: 0.925859
PRC train: 0.984294	val: 0.948348	test: 0.974021

Epoch: 21
Loss: 0.24874638648825084
ROC train: 0.959004	val: 0.889014	test: 0.921941
PRC train: 0.984643	val: 0.953646	test: 0.971383

Epoch: 22
Loss: 0.25763676867871615
ROC train: 0.961231	val: 0.879971	test: 0.919409
PRC train: 0.985783	val: 0.950636	test: 0.971071

Epoch: 23
Loss: 0.23923922169201428
ROC train: 0.965804	val: 0.883275	test: 0.916817
PRC train: 0.987182	val: 0.949027	test: 0.969820

Epoch: 24
Loss: 0.2319108108283565
ROC train: 0.966647	val: 0.878029	test: 0.912176
PRC train: 0.987450	val: 0.946392	test: 0.968133

Epoch: 25
Loss: 0.23055589509706442
ROC train: 0.964469	val: 0.880087	test: 0.920796
PRC train: 0.986961	val: 0.947061	test: 0.972676

Epoch: 26
Loss: 0.23007789568546458
ROC train: 0.969493	val: 0.886058	test: 0.913562
PRC train: 0.989053	val: 0.951320	test: 0.969425

Epoch: 27
Loss: 0.23783425742643752
ROC train: 0.962341	val: 0.854638	test: 0.896866
PRC train: 0.986771	val: 0.937258	test: 0.960342

Epoch: 28
Loss: 0.22920492967454167
ROC train: 0.967666	val: 0.876551	test: 0.917661
PRC train: 0.988717	val: 0.946636	test: 0.971948

Epoch: 29
Loss: 0.21698679359978054
ROC train: 0.971330	val: 0.890493	test: 0.920675
PRC train: 0.989654	val: 0.948435	test: 0.973441

Epoch: 30
Loss: 0.23161079799066933
ROC train: 0.972070	val: 0.894116	test: 0.926884
PRC train: 0.990129	val: 0.949008	test: 0.975678

Epoch: 31
Loss: 0.2126357057737881
ROC train: 0.973910	val: 0.891449	test: 0.931103
PRC train: 0.991023	val: 0.945560	test: 0.976832

Epoch: 32
Loss: 0.21790173233015517
ROC train: 0.973343	val: 0.879391	test: 0.903978
PRC train: 0.991146	val: 0.947963	test: 0.964239

Epoch: 33
Loss: 0.213120520366985Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/bbbp/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/split/GraphMVP/bbbp/random/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/bbbp/random/train_prop=0.8/bbbp_random_4_26-05_11-06-56  ]
[ Using Seed :  4  ]
[ Using device :  cuda:2  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
randomly split
Data(edge_attr=[58, 2], edge_index=[2, 58], id=[1], x=[27, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6185717943196707
ROC train: 0.818160	val: 0.726478	test: 0.803898
PRC train: 0.927567	val: 0.887727	test: 0.921625

Epoch: 2
Loss: 0.5080259961791169
ROC train: 0.836117	val: 0.742248	test: 0.822344
PRC train: 0.931724	val: 0.882015	test: 0.917930

Epoch: 3
Loss: 0.43161379168908104
ROC train: 0.862192	val: 0.780281	test: 0.862376
PRC train: 0.945540	val: 0.913637	test: 0.937974

Epoch: 4
Loss: 0.3876993703205464
ROC train: 0.880999	val: 0.804930	test: 0.879906
PRC train: 0.954201	val: 0.926108	test: 0.945778

Epoch: 5
Loss: 0.35157008879190155
ROC train: 0.911290	val: 0.852505	test: 0.924778
PRC train: 0.965837	val: 0.945752	test: 0.970517

Epoch: 6
Loss: 0.3387609544232693
ROC train: 0.919755	val: 0.857673	test: 0.924778
PRC train: 0.969275	val: 0.948159	test: 0.970814

Epoch: 7
Loss: 0.3294186764500843
ROC train: 0.919817	val: 0.854890	test: 0.917059
PRC train: 0.969880	val: 0.947652	test: 0.966593

Epoch: 8
Loss: 0.3125176367814766
ROC train: 0.924682	val: 0.856878	test: 0.913527
PRC train: 0.971664	val: 0.947996	test: 0.961378

Epoch: 9
Loss: 0.3176642588678483
ROC train: 0.914626	val: 0.835409	test: 0.897436
PRC train: 0.966164	val: 0.934880	test: 0.949999

Epoch: 10
Loss: 0.2990329823635095
ROC train: 0.935439	val: 0.862046	test: 0.920984
PRC train: 0.975778	val: 0.949105	test: 0.966187

Epoch: 11
Loss: 0.31048573292461457
ROC train: 0.936471	val: 0.869732	test: 0.926086
PRC train: 0.975867	val: 0.951537	test: 0.970270

Epoch: 12
Loss: 0.29098139493122827
ROC train: 0.935530	val: 0.861781	test: 0.919937
PRC train: 0.974930	val: 0.948776	test: 0.965876

Epoch: 13
Loss: 0.27774135867782623
ROC train: 0.943594	val: 0.866552	test: 0.927525
PRC train: 0.978630	val: 0.952522	test: 0.969102

Epoch: 14
Loss: 0.271537258592678
ROC train: 0.939641	val: 0.852770	test: 0.928571
PRC train: 0.977287	val: 0.945529	test: 0.969846

Epoch: 15
Loss: 0.28936526264923385
ROC train: 0.943935	val: 0.858733	test: 0.919937
PRC train: 0.978696	val: 0.949144	test: 0.965069

Epoch: 16
Loss: 0.2733220982133657
ROC train: 0.947221	val: 0.869467	test: 0.922292
PRC train: 0.980394	val: 0.952279	test: 0.968990

Epoch: 17
Loss: 0.2811401367241895
ROC train: 0.947491	val: 0.881924	test: 0.935897
PRC train: 0.980132	val: 0.958449	test: 0.976350

Epoch: 18
Loss: 0.2684626847577577
ROC train: 0.944648	val: 0.867612	test: 0.927656
PRC train: 0.979087	val: 0.951011	test: 0.972789

Epoch: 19
Loss: 0.26018547376257467
ROC train: 0.949538	val: 0.871455	test: 0.924385
PRC train: 0.980863	val: 0.951416	test: 0.971603

Epoch: 20
Loss: 0.269502784889631
ROC train: 0.954468	val: 0.878744	test: 0.930795
PRC train: 0.982493	val: 0.955108	test: 0.973100

Epoch: 21
Loss: 0.24903942206388271
ROC train: 0.955854	val: 0.858203	test: 0.920591
PRC train: 0.983483	val: 0.946633	test: 0.964934

Epoch: 22
Loss: 0.2552277283750433
ROC train: 0.958971	val: 0.874238	test: 0.919021
PRC train: 0.984204	val: 0.953508	test: 0.966185

Epoch: 23
Loss: 0.2558442774853142
ROC train: 0.961264	val: 0.867745	test: 0.912480
PRC train: 0.985171	val: 0.951553	test: 0.962555

Epoch: 24
Loss: 0.23518433891726234
ROC train: 0.961311	val: 0.876358	test: 0.919806
PRC train: 0.985569	val: 0.954651	test: 0.968284

Epoch: 25
Loss: 0.25115116598038917
ROC train: 0.962121	val: 0.874768	test: 0.930272
PRC train: 0.985883	val: 0.954425	test: 0.972317

Epoch: 26
Loss: 0.23370459574868638
ROC train: 0.966310	val: 0.873443	test: 0.926478
PRC train: 0.987471	val: 0.952326	test: 0.969536

Epoch: 27
Loss: 0.23642809887574545
ROC train: 0.965712	val: 0.877816	test: 0.916667
PRC train: 0.987265	val: 0.955253	test: 0.966644

Epoch: 28
Loss: 0.22002298254804403
ROC train: 0.965378	val: 0.870660	test: 0.931188
PRC train: 0.987498	val: 0.952018	test: 0.971770

Epoch: 29
Loss: 0.24494914729775052
ROC train: 0.968101	val: 0.871058	test: 0.914181
PRC train: 0.987896	val: 0.951804	test: 0.965462

Epoch: 30
Loss: 0.23061359392805492
ROC train: 0.963134	val: 0.855288	test: 0.904108
PRC train: 0.986048	val: 0.942394	test: 0.955129

Epoch: 31
Loss: 0.22520414600681796
ROC train: 0.969830	val: 0.877418	test: 0.921507
PRC train: 0.988732	val: 0.953246	test: 0.965988

Epoch: 32
Loss: 0.2221317811609857
ROC train: 0.969552	val: 0.866022	test: 0.910649
PRC train: 0.988958	val: 0.949510	test: 0.961292

Epoch: 33
Loss: 0.22976858392107782Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/bbbp/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/split/GraphMVP/bbbp/random/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/bbbp/random/train_prop=0.8/bbbp_random_5_26-05_11-06-56  ]
[ Using Seed :  5  ]
[ Using device :  cuda:2  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
randomly split
Data(edge_attr=[58, 2], edge_index=[2, 58], id=[1], x=[27, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6467183668540925
ROC train: 0.809107	val: 0.755102	test: 0.791732
PRC train: 0.920211	val: 0.891364	test: 0.893954

Epoch: 2
Loss: 0.5248258933126777
ROC train: 0.836500	val: 0.755765	test: 0.827054
PRC train: 0.929576	val: 0.878583	test: 0.913682

Epoch: 3
Loss: 0.42943344990012006
ROC train: 0.861255	val: 0.777763	test: 0.850863
PRC train: 0.941421	val: 0.888948	test: 0.929532

Epoch: 4
Loss: 0.4063360207260171
ROC train: 0.873195	val: 0.814736	test: 0.891026
PRC train: 0.944867	val: 0.919213	test: 0.950923

Epoch: 5
Loss: 0.36373125871704776
ROC train: 0.893994	val: 0.832096	test: 0.904631
PRC train: 0.956965	val: 0.930839	test: 0.960691

Epoch: 6
Loss: 0.34836430864325507
ROC train: 0.909406	val: 0.854625	test: 0.916928
PRC train: 0.964467	val: 0.946491	test: 0.967409

Epoch: 7
Loss: 0.32946930882405384
ROC train: 0.917013	val: 0.850252	test: 0.913658
PRC train: 0.967933	val: 0.943778	test: 0.964249

Epoch: 8
Loss: 0.3202011200014433
ROC train: 0.924932	val: 0.864431	test: 0.919021
PRC train: 0.971983	val: 0.951298	test: 0.967233

Epoch: 9
Loss: 0.3012550659574202
ROC train: 0.927396	val: 0.869997	test: 0.920984
PRC train: 0.972782	val: 0.954122	test: 0.968458

Epoch: 10
Loss: 0.2892149849959327
ROC train: 0.927570	val: 0.854492	test: 0.914966
PRC train: 0.971713	val: 0.945815	test: 0.965681

Epoch: 11
Loss: 0.29586059621374455
ROC train: 0.933307	val: 0.865757	test: 0.932496
PRC train: 0.974252	val: 0.950393	test: 0.973117

Epoch: 12
Loss: 0.29214735770274824
ROC train: 0.938930	val: 0.854095	test: 0.932104
PRC train: 0.976911	val: 0.945679	test: 0.973152

Epoch: 13
Loss: 0.2748865680305515
ROC train: 0.941828	val: 0.856613	test: 0.925563
PRC train: 0.977951	val: 0.945290	test: 0.967488

Epoch: 14
Loss: 0.277261581583218
ROC train: 0.944845	val: 0.867214	test: 0.927525
PRC train: 0.977958	val: 0.948579	test: 0.969985

Epoch: 15
Loss: 0.2612868128846439
ROC train: 0.945327	val: 0.860456	test: 0.931711
PRC train: 0.978859	val: 0.949449	test: 0.973409

Epoch: 16
Loss: 0.25979619723560676
ROC train: 0.948561	val: 0.852505	test: 0.928048
PRC train: 0.980470	val: 0.945267	test: 0.969980

Epoch: 17
Loss: 0.27720798614491277
ROC train: 0.951443	val: 0.857938	test: 0.913789
PRC train: 0.981886	val: 0.944410	test: 0.960870

Epoch: 18
Loss: 0.26044799579812244
ROC train: 0.955202	val: 0.858601	test: 0.916143
PRC train: 0.983186	val: 0.945162	test: 0.964535

Epoch: 19
Loss: 0.25359666448029644
ROC train: 0.956012	val: 0.871720	test: 0.926871
PRC train: 0.982242	val: 0.950486	test: 0.969942

Epoch: 20
Loss: 0.2639221932933277
ROC train: 0.947146	val: 0.858336	test: 0.915620
PRC train: 0.979178	val: 0.946507	test: 0.964590

Epoch: 21
Loss: 0.26967104707914585
ROC train: 0.959770	val: 0.877021	test: 0.927394
PRC train: 0.984410	val: 0.953348	test: 0.969302

Epoch: 22
Loss: 0.24393317611550125
ROC train: 0.958180	val: 0.865227	test: 0.910780
PRC train: 0.983930	val: 0.947506	test: 0.961656

Epoch: 23
Loss: 0.25722769427445696
ROC train: 0.958012	val: 0.851842	test: 0.905154
PRC train: 0.984000	val: 0.942217	test: 0.959790

Epoch: 24
Loss: 0.24085384954829717
ROC train: 0.963473	val: 0.884177	test: 0.925824
PRC train: 0.985976	val: 0.956170	test: 0.968991

Epoch: 25
Loss: 0.23397840271032627
ROC train: 0.960407	val: 0.882852	test: 0.932496
PRC train: 0.984327	val: 0.954342	test: 0.973654

Epoch: 26
Loss: 0.2440983431767571
ROC train: 0.963267	val: 0.891333	test: 0.931450
PRC train: 0.985685	val: 0.953522	test: 0.971324

Epoch: 27
Loss: 0.24883175536801014
ROC train: 0.959795	val: 0.855023	test: 0.908294
PRC train: 0.985721	val: 0.940616	test: 0.958666

Epoch: 28
Loss: 0.2432656004032175
ROC train: 0.964764	val: 0.865227	test: 0.920199
PRC train: 0.987302	val: 0.944690	test: 0.963778

Epoch: 29
Loss: 0.22999033852682862
ROC train: 0.967339	val: 0.898092	test: 0.935636
PRC train: 0.987680	val: 0.961146	test: 0.974693

Epoch: 30
Loss: 0.2406659978976604
ROC train: 0.966763	val: 0.884972	test: 0.918237
PRC train: 0.987219	val: 0.955929	test: 0.966666

Epoch: 31
Loss: 0.22105365294421667
ROC train: 0.971166	val: 0.879539	test: 0.918891
PRC train: 0.989170	val: 0.947480	test: 0.962307

Epoch: 32
Loss: 0.22227030235921116
ROC train: 0.973077	val: 0.886430	test: 0.914050
PRC train: 0.989761	val: 0.954045	test: 0.963095

Epoch: 33
Loss: 0.2277510901211015Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/bbbp/random/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: bbbp
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: random
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_classification.pth
  output_model_dir: ../runs/split/GraphMVP/bbbp/random/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/bbbp/random/train_prop=0.8/bbbp_random_6_26-05_11-06-56  ]
[ Using Seed :  6  ]
[ Using device :  cuda:2  ]
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
randomly split
Data(edge_attr=[58, 2], edge_index=[2, 58], id=[1], x=[27, 2], y=[1])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6405920046402753
ROC train: 0.803727	val: 0.709117	test: 0.794872
PRC train: 0.920772	val: 0.869725	test: 0.910124

Epoch: 2
Loss: 0.509749413088839
ROC train: 0.837733	val: 0.741850	test: 0.820774
PRC train: 0.935116	val: 0.898964	test: 0.923869

Epoch: 3
Loss: 0.4449628975287357
ROC train: 0.859850	val: 0.788232	test: 0.852826
PRC train: 0.938699	val: 0.900915	test: 0.930437

Epoch: 4
Loss: 0.3944484145209407
ROC train: 0.879964	val: 0.816061	test: 0.883569
PRC train: 0.949114	val: 0.929761	test: 0.950774

Epoch: 5
Loss: 0.3673264674025189
ROC train: 0.903955	val: 0.831699	test: 0.900968
PRC train: 0.963971	val: 0.937922	test: 0.955217

Epoch: 6
Loss: 0.3494552616083733
ROC train: 0.911592	val: 0.857540	test: 0.909210
PRC train: 0.966507	val: 0.948510	test: 0.960995

Epoch: 7
Loss: 0.3346345432967217
ROC train: 0.917889	val: 0.861384	test: 0.918367
PRC train: 0.968289	val: 0.944860	test: 0.963479

Epoch: 8
Loss: 0.31902500789555965
ROC train: 0.917582	val: 0.871985	test: 0.910780
PRC train: 0.967371	val: 0.954719	test: 0.961919

Epoch: 9
Loss: 0.3021411497318101
ROC train: 0.930124	val: 0.866817	test: 0.925563
PRC train: 0.973702	val: 0.953812	test: 0.970687

Epoch: 10
Loss: 0.31062201255815186
ROC train: 0.934513	val: 0.856215	test: 0.922684
PRC train: 0.975779	val: 0.947506	test: 0.969033

Epoch: 11
Loss: 0.30317944898150395
ROC train: 0.936977	val: 0.865094	test: 0.913134
PRC train: 0.976075	val: 0.949586	test: 0.959022

Epoch: 12
Loss: 0.29960959159494394
ROC train: 0.937662	val: 0.873973	test: 0.914574
PRC train: 0.976566	val: 0.952499	test: 0.958977

Epoch: 13
Loss: 0.27360662794429186
ROC train: 0.942541	val: 0.876756	test: 0.925301
PRC train: 0.977671	val: 0.955183	test: 0.968070

Epoch: 14
Loss: 0.2813613686080603
ROC train: 0.945219	val: 0.866684	test: 0.920460
PRC train: 0.978908	val: 0.949855	test: 0.965024

Epoch: 15
Loss: 0.26946778045530584
ROC train: 0.948182	val: 0.863371	test: 0.909995
PRC train: 0.980238	val: 0.947194	test: 0.958378

Epoch: 16
Loss: 0.2704976745176128
ROC train: 0.946961	val: 0.873575	test: 0.913396
PRC train: 0.979709	val: 0.951587	test: 0.962890

Epoch: 17
Loss: 0.2619800187059789
ROC train: 0.950025	val: 0.883249	test: 0.917975
PRC train: 0.980654	val: 0.957176	test: 0.964634

Epoch: 18
Loss: 0.2613363492777915
ROC train: 0.951161	val: 0.871455	test: 0.923993
PRC train: 0.981916	val: 0.949734	test: 0.968897

Epoch: 19
Loss: 0.2588722934758363
ROC train: 0.953246	val: 0.884575	test: 0.933673
PRC train: 0.982521	val: 0.957662	test: 0.974015

Epoch: 20
Loss: 0.24482042969708231
ROC train: 0.958335	val: 0.895971	test: 0.918237
PRC train: 0.984200	val: 0.960905	test: 0.963849

Epoch: 21
Loss: 0.2528523045996521
ROC train: 0.958713	val: 0.874238	test: 0.914704
PRC train: 0.985182	val: 0.950709	test: 0.963473

Epoch: 22
Loss: 0.27512311298538594
ROC train: 0.961434	val: 0.890140	test: 0.927917
PRC train: 0.986427	val: 0.959626	test: 0.970724

Epoch: 23
Loss: 0.23854149734345612
ROC train: 0.960025	val: 0.871190	test: 0.914835
PRC train: 0.985935	val: 0.950021	test: 0.966004

Epoch: 24
Loss: 0.24894099376125328
ROC train: 0.961873	val: 0.875033	test: 0.901884
PRC train: 0.985621	val: 0.950909	test: 0.952404

Epoch: 25
Loss: 0.24364920432743206
ROC train: 0.964966	val: 0.889345	test: 0.918891
PRC train: 0.986810	val: 0.958349	test: 0.961829

Epoch: 26
Loss: 0.2374987646337486
ROC train: 0.965261	val: 0.884840	test: 0.926086
PRC train: 0.987722	val: 0.955730	test: 0.967280

Epoch: 27
Loss: 0.252003468664702
ROC train: 0.966087	val: 0.884045	test: 0.930010
PRC train: 0.987990	val: 0.953996	test: 0.973052

Epoch: 28
Loss: 0.22563180564000815
ROC train: 0.962331	val: 0.875166	test: 0.919414
PRC train: 0.986752	val: 0.952355	test: 0.968470

Epoch: 29
Loss: 0.2336637633966725
ROC train: 0.966594	val: 0.889610	test: 0.929356
PRC train: 0.988039	val: 0.957226	test: 0.972563

Epoch: 30
Loss: 0.2266328521303704
ROC train: 0.967228	val: 0.898754	test: 0.923077
PRC train: 0.987948	val: 0.962061	test: 0.967445

Epoch: 31
Loss: 0.23449965327356606
ROC train: 0.969480	val: 0.906175	test: 0.924254
PRC train: 0.989073	val: 0.965518	test: 0.966342

Epoch: 32
Loss: 0.23128267028322474
ROC train: 0.970524	val: 0.897827	test: 0.918237
PRC train: 0.989331	val: 0.960995	test: 0.960229

Epoch: 33
Loss: 0.24061873524217198
ROC train: 0.978606	val: 0.892716	test: 0.898386
PRC train: 0.993262	val: 0.945464	test: 0.958689

Epoch: 34
Loss: 0.20436749703138216
ROC train: 0.978755	val: 0.898579	test: 0.891692
PRC train: 0.993279	val: 0.950258	test: 0.955100

Epoch: 35
Loss: 0.19198789071040576
ROC train: 0.975561	val: 0.892037	test: 0.885065
PRC train: 0.992141	val: 0.943921	test: 0.950813

Epoch: 36
Loss: 0.2064101290233386
ROC train: 0.974616	val: 0.887346	test: 0.879946
PRC train: 0.991798	val: 0.938161	test: 0.945085

Epoch: 37
Loss: 0.20223095290629542
ROC train: 0.984737	val: 0.893124	test: 0.905702
PRC train: 0.995189	val: 0.939645	test: 0.962386

Epoch: 38
Loss: 0.18047635083354335
ROC train: 0.983219	val: 0.891884	test: 0.914135
PRC train: 0.994538	val: 0.936593	test: 0.969005

Epoch: 39
Loss: 0.19283985621972896
ROC train: 0.982273	val: 0.890303	test: 0.909246
PRC train: 0.994254	val: 0.938612	test: 0.966334

Epoch: 40
Loss: 0.19041851381426886
ROC train: 0.984481	val: 0.893940	test: 0.901667
PRC train: 0.994987	val: 0.942691	test: 0.962336

Epoch: 41
Loss: 0.1890133884016782
ROC train: 0.986065	val: 0.893498	test: 0.897467
PRC train: 0.995523	val: 0.942051	test: 0.961470

Epoch: 42
Loss: 0.1818599388770151
ROC train: 0.986167	val: 0.896166	test: 0.908229
PRC train: 0.995613	val: 0.941841	test: 0.965580

Epoch: 43
Loss: 0.18834148094413655
ROC train: 0.987470	val: 0.893226	test: 0.903701
PRC train: 0.996100	val: 0.941815	test: 0.961354

Epoch: 44
Loss: 0.17372718945288007
ROC train: 0.981819	val: 0.887822	test: 0.884835
PRC train: 0.994215	val: 0.946766	test: 0.949052

Epoch: 45
Loss: 0.18043157376973035
ROC train: 0.985318	val: 0.891459	test: 0.892152
PRC train: 0.995323	val: 0.946051	test: 0.953261

Epoch: 46
Loss: 0.17604958287635214
ROC train: 0.988285	val: 0.889385	test: 0.892775
PRC train: 0.996267	val: 0.946794	test: 0.958132

Epoch: 47
Loss: 0.17249271021121157
ROC train: 0.988732	val: 0.896047	test: 0.893694
PRC train: 0.996340	val: 0.948949	test: 0.959364

Epoch: 48
Loss: 0.16389947837830202
ROC train: 0.985448	val: 0.892852	test: 0.890446
PRC train: 0.995245	val: 0.946658	test: 0.958170

Epoch: 49
Loss: 0.1725061734467322
ROC train: 0.989417	val: 0.893532	test: 0.901732
PRC train: 0.996570	val: 0.946228	test: 0.961939

Epoch: 50
Loss: 0.17191744419538477
ROC train: 0.989796	val: 0.899140	test: 0.898714
PRC train: 0.996781	val: 0.951909	test: 0.957971

Epoch: 51
Loss: 0.15457824005869059
ROC train: 0.986856	val: 0.898154	test: 0.880340
PRC train: 0.995826	val: 0.953517	test: 0.948784

Epoch: 52
Loss: 0.1657608267359918
ROC train: 0.990009	val: 0.895843	test: 0.894613
PRC train: 0.996841	val: 0.949818	test: 0.957752

Epoch: 53
Loss: 0.16571634544139005
ROC train: 0.990031	val: 0.897135	test: 0.903373
PRC train: 0.996821	val: 0.950220	test: 0.962581

Epoch: 54
Loss: 0.16443107520755
ROC train: 0.990943	val: 0.902471	test: 0.899567
PRC train: 0.997057	val: 0.953556	test: 0.962365

Epoch: 55
Loss: 0.16070606427822112
ROC train: 0.992335	val: 0.902471	test: 0.896089
PRC train: 0.997562	val: 0.956531	test: 0.959107

Epoch: 56
Loss: 0.1585344845930463
ROC train: 0.985653	val: 0.884423	test: 0.883555
PRC train: 0.995448	val: 0.947914	test: 0.951526

Epoch: 57
Loss: 0.16851620669687387
ROC train: 0.991323	val: 0.901978	test: 0.888543
PRC train: 0.997250	val: 0.957895	test: 0.955631

Epoch: 58
Loss: 0.14543072506424276
ROC train: 0.987835	val: 0.887958	test: 0.876435
PRC train: 0.996016	val: 0.951814	test: 0.951282

Epoch: 59
Loss: 0.15051290512941423
ROC train: 0.993180	val: 0.899446	test: 0.901700
PRC train: 0.997871	val: 0.952928	test: 0.963233

Epoch: 60
Loss: 0.15078360929538875
ROC train: 0.990431	val: 0.890711	test: 0.894284
PRC train: 0.997006	val: 0.951948	test: 0.961451

Epoch: 61
Loss: 0.15946694976153422
ROC train: 0.993206	val: 0.884797	test: 0.899665
PRC train: 0.997807	val: 0.945707	test: 0.961783

Epoch: 62
Loss: 0.13356241830241983
ROC train: 0.993955	val: 0.889250	test: 0.897894
PRC train: 0.998023	val: 0.946268	test: 0.961093

Epoch: 63
Loss: 0.1543161822212675
ROC train: 0.993162	val: 0.891255	test: 0.898615
PRC train: 0.997753	val: 0.945425	test: 0.960726

Epoch: 64
Loss: 0.140542142805617
ROC train: 0.994077	val: 0.889521	test: 0.893825
PRC train: 0.998097	val: 0.943691	test: 0.958597

Epoch: 65
Loss: 0.14459653913279452
ROC train: 0.993117	val: 0.893702	test: 0.895794
PRC train: 0.997868	val: 0.946602	test: 0.961143

Epoch: 66
Loss: 0.15783543381148044
ROC train: 0.993214	val: 0.890728	test: 0.908295
PRC train: 0.997887	val: 0.947576	test: 0.966411

Epoch: 67
Loss: 0.15515082974515154
ROC train: 0.993247	val: 0.882588	test: 0.887985
PRC train: 0.997856	val: 0.942639	test: 0.952473

Epoch: 68
Loss: 0.1349457234219437
ROC train: 0.995170	val: 0.890133	test: 0.886410
PRC train: 0.998461	val: 0.944482	test: 0.953935

Epoch: 69
Loss: 0.15873553970931714
ROC train: 0.994599	val: 0.893566	test: 0.892906
PRC train: 0.998327	val: 0.951501	test: 0.959086

Epoch: 70
Loss: 0.13139447435803883
ROC train: 0.994351	val: 0.891697	test: 0.896483
PRC train: 0.998239	val: 0.951273	test: 0.958275

Epoch: 71
Loss: 0.13838745089647023
ROC train: 0.992983	val: 0.890235	test: 0.881226
PRC train: 0.997742	val: 0.953617	test: 0.952044

Epoch: 72
Loss: 0.1450540355973399
ROC train: 0.995525	val: 0.890337	test: 0.887230
PRC train: 0.998572	val: 0.953054	test: 0.953178

Epoch: 73
Loss: 0.13007515078201554
ROC train: 0.995015	val: 0.882180	test: 0.879749
PRC train: 0.998385	val: 0.948196	test: 0.947887

Epoch: 74
Loss: 0.13261845930716568
ROC train: 0.995561	val: 0.887040	test: 0.888313
PRC train: 0.998591	val: 0.947079	test: 0.955488

Epoch: 75
Loss: 0.133150125377633
ROC train: 0.995522	val: 0.898834	test: 0.895498
PRC train: 0.998584	val: 0.952067	test: 0.959127

Epoch: 76
Loss: 0.1312955251418087
ROC train: 0.994455	val: 0.893056	test: 0.883982
PRC train: 0.998236	val: 0.951703	test: 0.951908

Epoch: 77
Loss: 0.12188619167411378
ROC train: 0.995745	val: 0.880549	test: 0.886738
PRC train: 0.998628	val: 0.945258	test: 0.953056

Epoch: 78
Loss: 0.1339061212565877
ROC train: 0.995291	val: 0.888536	test: 0.902061
PRC train: 0.998517	val: 0.950087	test: 0.962370

Epoch: 79
Loss: 0.11769745099377879
ROC train: 0.995347	val: 0.892682	test: 0.891922
PRC train: 0.998487	val: 0.951192	test: 0.956878

Epoch: 80
Loss: 0.13910170347319942
ROC train: 0.996525	val: 0.896013	test: 0.900781
PRC train: 0.998884	val: 0.951059	test: 0.962112

Epoch: 81
Loss: 0.12224406927782434
ROC train: 0.996348	val: 0.893668	test: 0.892119
PRC train: 0.998857	val: 0.954684	test: 0.960065

Epoch: 82
Loss: 0.11490813108460865
ROC train: 0.997016	val: 0.892648	test: 0.890544
PRC train: 0.999020	val: 0.952269	test: 0.956109

Epoch: 83
Loss: 0.12604683678631162
ROC train: 0.996758	val: 0.884491	test: 0.886804
PRC train: 0.998943	val: 0.950607	test: 0.954509

Epoch: 84
Loss: 0.11932175942505616
ROC train: 0.997186	val: 0.889589	test: 0.889363
PRC train: 0.999103	val: 0.950167	test: 0.955183

Epoch: 85
Loss: 0.10818396727216395
ROC train: 0.996531	val: 0.893464	test: 0.886082
PRC train: 0.998911	val: 0.954278	test: 0.951848

Epoch: 86
Loss: 0.129088391088625
ROC train: 0.997709	val: 0.889453	test: 0.889921
PRC train: 0.999284	val: 0.954322	test: 0.955481

Epoch: 87
Loss: 0.10666728507304639
ROC train: 0.997283	val: 0.887312	test: 0.888641
PRC train: 0.999151	val: 0.951374	test: 0.956085

Epoch: 88
Loss: 0.11311326932263524
ROC train: 0.996966	val: 0.892207	test: 0.891233
PRC train: 0.999044	val: 0.952234	test: 0.957536

Epoch: 89
Loss: 0.12209381107098045
ROC train: 0.996936	val: 0.891085	test: 0.893136
PRC train: 0.999020	val: 0.949891	test: 0.959286

Epoch: 90
Loss: 0.10499385568801782
ROC train: 0.997331	val: 0.887516	test: 0.888444
PRC train: 0.999150	val: 0.942255	test: 0.955355

Epoch: 91
Loss: 0.10977483072766292
ROC train: 0.997763	val: 0.885851	test: 0.887099
PRC train: 0.999297	val: 0.944260	test: 0.954331

Epoch: 92
Loss: 0.11067124870253986
ROC train: 0.997039	val: 0.891051	test: 0.882276
PRC train: 0.999073	val: 0.955079	test: 0.952326

Epoch: 93
Loss: 0.10244501949974352
ROC train: 0.996199	val: 0.885749	test: 0.872531
PRC train: 0.998818	val: 0.952665	test: 0.945559

Epoch: 94
Loss: 0.10296224292372731
ROC train: 0.972725	val: 0.897271	test: 0.911904
PRC train: 0.991143	val: 0.947851	test: 0.965594

Epoch: 34
Loss: 0.2315250672810003
ROC train: 0.976991	val: 0.895622	test: 0.908754
PRC train: 0.992705	val: 0.944713	test: 0.964737

Epoch: 35
Loss: 0.20345668076472512
ROC train: 0.979734	val: 0.889182	test: 0.889330
PRC train: 0.993579	val: 0.942991	test: 0.954557

Epoch: 36
Loss: 0.19637979801470118
ROC train: 0.979062	val: 0.885613	test: 0.879421
PRC train: 0.993265	val: 0.943411	test: 0.950382

Epoch: 37
Loss: 0.19991313220342347
ROC train: 0.981473	val: 0.891629	test: 0.890511
PRC train: 0.994213	val: 0.946275	test: 0.955501

Epoch: 38
Loss: 0.19837274314122597
ROC train: 0.981000	val: 0.893872	test: 0.891791
PRC train: 0.993810	val: 0.947208	test: 0.956370

Epoch: 39
Loss: 0.20042516166134172
ROC train: 0.973540	val: 0.888128	test: 0.872203
PRC train: 0.990987	val: 0.947250	test: 0.947154

Epoch: 40
Loss: 0.2036633232708569
ROC train: 0.982121	val: 0.893498	test: 0.878831
PRC train: 0.994386	val: 0.949390	test: 0.949943

Epoch: 41
Loss: 0.18474809818020124
ROC train: 0.982230	val: 0.899106	test: 0.891364
PRC train: 0.994531	val: 0.950125	test: 0.957352

Epoch: 42
Loss: 0.20537169267367697
ROC train: 0.981990	val: 0.892376	test: 0.887165
PRC train: 0.994350	val: 0.946201	test: 0.951320

Epoch: 43
Loss: 0.18124762560447044
ROC train: 0.984399	val: 0.888332	test: 0.882702
PRC train: 0.995151	val: 0.945766	test: 0.952187

Epoch: 44
Loss: 0.183678327015682
ROC train: 0.983682	val: 0.885885	test: 0.901896
PRC train: 0.994843	val: 0.944166	test: 0.964492

Epoch: 45
Loss: 0.1847750531405317
ROC train: 0.984641	val: 0.886157	test: 0.904653
PRC train: 0.995187	val: 0.940828	test: 0.962038

Epoch: 46
Loss: 0.17924135605284713
ROC train: 0.981138	val: 0.886598	test: 0.902061
PRC train: 0.993793	val: 0.942154	test: 0.962129

Epoch: 47
Loss: 0.17562542833675335
ROC train: 0.987191	val: 0.893107	test: 0.905276
PRC train: 0.995994	val: 0.945002	test: 0.963461

Epoch: 48
Loss: 0.16307418924352804
ROC train: 0.987366	val: 0.884083	test: 0.886804
PRC train: 0.996015	val: 0.945321	test: 0.950333

Epoch: 49
Loss: 0.1623586812832507
ROC train: 0.988159	val: 0.884916	test: 0.894678
PRC train: 0.996164	val: 0.944967	test: 0.956859

Epoch: 50
Loss: 0.17845903514908096
ROC train: 0.988218	val: 0.882520	test: 0.885163
PRC train: 0.996273	val: 0.942649	test: 0.951754

Epoch: 51
Loss: 0.18045994729833778
ROC train: 0.988047	val: 0.876470	test: 0.877453
PRC train: 0.996245	val: 0.938575	test: 0.950337

Epoch: 52
Loss: 0.18046925383495122
ROC train: 0.988847	val: 0.877320	test: 0.882899
PRC train: 0.996538	val: 0.943451	test: 0.955082

Epoch: 53
Loss: 0.1704604198987585
ROC train: 0.990632	val: 0.886649	test: 0.905637
PRC train: 0.997030	val: 0.945794	test: 0.964735

Epoch: 54
Loss: 0.15058998105226953
ROC train: 0.990243	val: 0.889148	test: 0.905538
PRC train: 0.996955	val: 0.946682	test: 0.963264

Epoch: 55
Loss: 0.1599364918001807
ROC train: 0.990842	val: 0.887142	test: 0.896351
PRC train: 0.997106	val: 0.946578	test: 0.959484

Epoch: 56
Loss: 0.16116884472852366
ROC train: 0.990306	val: 0.883608	test: 0.893136
PRC train: 0.996939	val: 0.946269	test: 0.958895

Epoch: 57
Loss: 0.16250691451861665
ROC train: 0.993147	val: 0.880549	test: 0.896614
PRC train: 0.997888	val: 0.941806	test: 0.959870

Epoch: 58
Loss: 0.13684904081289723
ROC train: 0.992830	val: 0.883641	test: 0.889560
PRC train: 0.997722	val: 0.943038	test: 0.953798

Epoch: 59
Loss: 0.15683094708211034
ROC train: 0.992045	val: 0.887414	test: 0.897598
PRC train: 0.997482	val: 0.945782	test: 0.959304

Epoch: 60
Loss: 0.13925254654192107
ROC train: 0.991494	val: 0.886870	test: 0.900387
PRC train: 0.997344	val: 0.944362	test: 0.962077

Epoch: 61
Loss: 0.1461133505750924
ROC train: 0.992637	val: 0.883064	test: 0.891627
PRC train: 0.997738	val: 0.941457	test: 0.956260

Epoch: 62
Loss: 0.14266598116731535
ROC train: 0.993322	val: 0.879937	test: 0.894711
PRC train: 0.997926	val: 0.938234	test: 0.958152

Epoch: 63
Loss: 0.15111661806183316
ROC train: 0.992525	val: 0.872120	test: 0.898287
PRC train: 0.997699	val: 0.934069	test: 0.961058

Epoch: 64
Loss: 0.16058081852790485
ROC train: 0.990813	val: 0.871780	test: 0.893070
PRC train: 0.997108	val: 0.937338	test: 0.958042

Epoch: 65
Loss: 0.15443511809423394
ROC train: 0.994606	val: 0.882894	test: 0.897008
PRC train: 0.998384	val: 0.942497	test: 0.959829

Epoch: 66
Loss: 0.14018525259024092
ROC train: 0.993534	val: 0.874397	test: 0.884080
PRC train: 0.998013	val: 0.938970	test: 0.950983

Epoch: 67
Loss: 0.14289251554860805
ROC train: 0.994814	val: 0.884389	test: 0.893169
PRC train: 0.998388	val: 0.944820	test: 0.957342

Epoch: 68
Loss: 0.12573095888322783
ROC train: 0.994915	val: 0.893192	test: 0.899009
PRC train: 0.998444	val: 0.950159	test: 0.960316

Epoch: 69
Loss: 0.14351430520257025
ROC train: 0.995995	val: 0.887176	test: 0.892053
PRC train: 0.998757	val: 0.947900	test: 0.957698

Epoch: 70
Loss: 0.13189856247923903
ROC train: 0.993070	val: 0.876929	test: 0.891791
PRC train: 0.997750	val: 0.938015	test: 0.957062

Epoch: 71
Loss: 0.12605413391057138
ROC train: 0.993204	val: 0.870012	test: 0.880635
PRC train: 0.997866	val: 0.934367	test: 0.953196

Epoch: 72
Loss: 0.14178105660999626
ROC train: 0.994334	val: 0.873411	test: 0.880438
PRC train: 0.998215	val: 0.936707	test: 0.950223

Epoch: 73
Loss: 0.14013376383742843
ROC train: 0.994792	val: 0.877286	test: 0.894744
PRC train: 0.998376	val: 0.939857	test: 0.957510

Epoch: 74
Loss: 0.1249718700653764
ROC train: 0.995055	val: 0.877354	test: 0.896876
PRC train: 0.998504	val: 0.942007	test: 0.961389

Epoch: 75
Loss: 0.11829348036670248
ROC train: 0.996296	val: 0.875943	test: 0.888182
PRC train: 0.998870	val: 0.938808	test: 0.955540

Epoch: 76
Loss: 0.12814086617999873
ROC train: 0.996341	val: 0.879053	test: 0.888543
PRC train: 0.998875	val: 0.935595	test: 0.956636

Epoch: 77
Loss: 0.13622884039220517
ROC train: 0.995980	val: 0.877575	test: 0.883096
PRC train: 0.998743	val: 0.938986	test: 0.955254

Epoch: 78
Loss: 0.1354692243345919
ROC train: 0.994746	val: 0.877320	test: 0.887230
PRC train: 0.998320	val: 0.939095	test: 0.956143

Epoch: 79
Loss: 0.13837550601186327
ROC train: 0.995043	val: 0.877745	test: 0.889625
PRC train: 0.998442	val: 0.937954	test: 0.951623

Epoch: 80
Loss: 0.11650258067765454
ROC train: 0.994889	val: 0.869672	test: 0.880799
PRC train: 0.998404	val: 0.928909	test: 0.945785

Epoch: 81
Loss: 0.11824404010137543
ROC train: 0.996013	val: 0.869400	test: 0.891594
PRC train: 0.998763	val: 0.929810	test: 0.957085

Epoch: 82
Loss: 0.12282578599468344
ROC train: 0.996307	val: 0.872086	test: 0.900912
PRC train: 0.998877	val: 0.931905	test: 0.962178

Epoch: 83
Loss: 0.12889479611122315
ROC train: 0.994046	val: 0.876062	test: 0.889592
PRC train: 0.998175	val: 0.939211	test: 0.957137

Epoch: 84
Loss: 0.12448783223951736
ROC train: 0.995555	val: 0.880141	test: 0.889428
PRC train: 0.998646	val: 0.941487	test: 0.957817

Epoch: 85
Loss: 0.12219222486442405
ROC train: 0.995879	val: 0.878135	test: 0.894908
PRC train: 0.998737	val: 0.937884	test: 0.960643

Epoch: 86
Loss: 0.12449010297077287
ROC train: 0.996169	val: 0.869638	test: 0.899140
PRC train: 0.998801	val: 0.930595	test: 0.961167

Epoch: 87
Loss: 0.12091666968116263
ROC train: 0.995767	val: 0.855329	test: 0.876960
PRC train: 0.998680	val: 0.922542	test: 0.947337

Epoch: 88
Loss: 0.125868301762106
ROC train: 0.995857	val: 0.864472	test: 0.866067
PRC train: 0.998681	val: 0.928890	test: 0.942771

Epoch: 89
Loss: 0.11941051637588229
ROC train: 0.996058	val: 0.869485	test: 0.877420
PRC train: 0.998752	val: 0.938662	test: 0.949932

Epoch: 90
Loss: 0.10833672825808197
ROC train: 0.996929	val: 0.870862	test: 0.879717
PRC train: 0.999041	val: 0.939048	test: 0.949808

Epoch: 91
Loss: 0.11822598533389124
ROC train: 0.996717	val: 0.873785	test: 0.872465
PRC train: 0.998970	val: 0.939294	test: 0.945740

Epoch: 92
Loss: 0.12038661933856598
ROC train: 0.997994	val: 0.877422	test: 0.889888
PRC train: 0.999400	val: 0.937508	test: 0.955161

Epoch: 93
Loss: 0.11477368841018638
ROC train: 0.996062	val: 0.874431	test: 0.893792
PRC train: 0.998801	val: 0.938880	test: 0.957004

Epoch: 94
Loss: 0.0926955330613772
ROC train: 0.977337	val: 0.904884	test: 0.891692
PRC train: 0.992591	val: 0.953927	test: 0.960546

Epoch: 34
Loss: 0.2049166941319717
ROC train: 0.968261	val: 0.887074	test: 0.864886
PRC train: 0.989468	val: 0.944380	test: 0.946433

Epoch: 35
Loss: 0.20189881992137684
ROC train: 0.978722	val: 0.897407	test: 0.887657
PRC train: 0.993077	val: 0.946283	test: 0.957217

Epoch: 36
Loss: 0.18854028692688496
ROC train: 0.976749	val: 0.896897	test: 0.899436
PRC train: 0.992249	val: 0.947981	test: 0.962229

Epoch: 37
Loss: 0.19863125198067347
ROC train: 0.977910	val: 0.893566	test: 0.892841
PRC train: 0.992777	val: 0.947935	test: 0.958524

Epoch: 38
Loss: 0.20838534555276098
ROC train: 0.981201	val: 0.899208	test: 0.889691
PRC train: 0.993904	val: 0.950095	test: 0.956669

Epoch: 39
Loss: 0.18911956859808482
ROC train: 0.980797	val: 0.895571	test: 0.886836
PRC train: 0.993669	val: 0.949017	test: 0.954528

Epoch: 40
Loss: 0.20532934448828324
ROC train: 0.983831	val: 0.894484	test: 0.892316
PRC train: 0.994682	val: 0.948206	test: 0.958456

Epoch: 41
Loss: 0.18663616798992044
ROC train: 0.983085	val: 0.889385	test: 0.887624
PRC train: 0.994463	val: 0.947019	test: 0.956030

Epoch: 42
Loss: 0.185248963642336
ROC train: 0.980464	val: 0.887856	test: 0.876796
PRC train: 0.993710	val: 0.947753	test: 0.951993

Epoch: 43
Loss: 0.18585799457718352
ROC train: 0.973704	val: 0.878883	test: 0.868331
PRC train: 0.991411	val: 0.943346	test: 0.946851

Epoch: 44
Loss: 0.18550784200160092
ROC train: 0.982925	val: 0.893498	test: 0.891791
PRC train: 0.994441	val: 0.949162	test: 0.958859

Epoch: 45
Loss: 0.17285205154477162
ROC train: 0.986416	val: 0.902369	test: 0.903799
PRC train: 0.995513	val: 0.953531	test: 0.965641

Epoch: 46
Loss: 0.16878643448550165
ROC train: 0.986763	val: 0.898562	test: 0.896187
PRC train: 0.995587	val: 0.952473	test: 0.961402

Epoch: 47
Loss: 0.1916388488351347
ROC train: 0.983018	val: 0.899990	test: 0.901273
PRC train: 0.994223	val: 0.952287	test: 0.964465

Epoch: 48
Loss: 0.18376606220132433
ROC train: 0.987131	val: 0.900500	test: 0.892972
PRC train: 0.995645	val: 0.950831	test: 0.958913

Epoch: 49
Loss: 0.16755273635069562
ROC train: 0.986053	val: 0.901689	test: 0.887066
PRC train: 0.995463	val: 0.954189	test: 0.955770

Epoch: 50
Loss: 0.1671262133021675
ROC train: 0.989072	val: 0.896965	test: 0.889592
PRC train: 0.996516	val: 0.951815	test: 0.956021

Epoch: 51
Loss: 0.174456602671404
ROC train: 0.984832	val: 0.884899	test: 0.877321
PRC train: 0.995063	val: 0.945849	test: 0.948838

Epoch: 52
Loss: 0.17009226352662224
ROC train: 0.989592	val: 0.892716	test: 0.894120
PRC train: 0.996592	val: 0.946070	test: 0.957646

Epoch: 53
Loss: 0.16584946773531503
ROC train: 0.987420	val: 0.886700	test: 0.888969
PRC train: 0.995808	val: 0.941668	test: 0.952800

Epoch: 54
Loss: 0.1587545714885063
ROC train: 0.992218	val: 0.901383	test: 0.902651
PRC train: 0.997472	val: 0.948785	test: 0.964199

Epoch: 55
Loss: 0.15588334487109157
ROC train: 0.991282	val: 0.899072	test: 0.898451
PRC train: 0.997246	val: 0.949411	test: 0.962364

Epoch: 56
Loss: 0.16734954482712988
ROC train: 0.988581	val: 0.886938	test: 0.877321
PRC train: 0.996308	val: 0.946260	test: 0.950505

Epoch: 57
Loss: 0.16842941978472062
ROC train: 0.990314	val: 0.895333	test: 0.882932
PRC train: 0.996875	val: 0.951877	test: 0.955028

Epoch: 58
Loss: 0.15160607010903213
ROC train: 0.990381	val: 0.896251	test: 0.888739
PRC train: 0.996896	val: 0.949943	test: 0.959090

Epoch: 59
Loss: 0.1538539275898962
ROC train: 0.992393	val: 0.896285	test: 0.898615
PRC train: 0.997537	val: 0.947547	test: 0.962035

Epoch: 60
Loss: 0.16708531888781253
ROC train: 0.992492	val: 0.891527	test: 0.889921
PRC train: 0.997564	val: 0.945556	test: 0.957616

Epoch: 61
Loss: 0.15112331956533334
ROC train: 0.994055	val: 0.890779	test: 0.894613
PRC train: 0.998090	val: 0.944087	test: 0.960176

Epoch: 62
Loss: 0.14046554695981728
ROC train: 0.990936	val: 0.888604	test: 0.880373
PRC train: 0.997029	val: 0.943202	test: 0.951293

Epoch: 63
Loss: 0.1496887780807569
ROC train: 0.991371	val: 0.888434	test: 0.882013
PRC train: 0.997230	val: 0.947127	test: 0.953606

Epoch: 64
Loss: 0.14173705084469673
ROC train: 0.992691	val: 0.885919	test: 0.885229
PRC train: 0.997675	val: 0.943794	test: 0.955429

Epoch: 65
Loss: 0.14012964120276314
ROC train: 0.992294	val: 0.885137	test: 0.893989
PRC train: 0.997489	val: 0.938705	test: 0.959391

Epoch: 66
Loss: 0.13831233993404046
ROC train: 0.987948	val: 0.885749	test: 0.900125
PRC train: 0.995957	val: 0.941238	test: 0.963458

Epoch: 67
Loss: 0.14148104913750198
ROC train: 0.992127	val: 0.892241	test: 0.898615
PRC train: 0.997372	val: 0.947278	test: 0.961414

Epoch: 68
Loss: 0.1409435373041075
ROC train: 0.993949	val: 0.892478	test: 0.884244
PRC train: 0.998038	val: 0.946143	test: 0.953362

Epoch: 69
Loss: 0.1398900041174083
ROC train: 0.994993	val: 0.890507	test: 0.882473
PRC train: 0.998367	val: 0.947956	test: 0.954249

Epoch: 70
Loss: 0.15506344922931686
ROC train: 0.995094	val: 0.888094	test: 0.880799
PRC train: 0.998442	val: 0.948440	test: 0.949582

Epoch: 71
Loss: 0.14736461108425666
ROC train: 0.993586	val: 0.883777	test: 0.871153
PRC train: 0.997936	val: 0.946409	test: 0.943886

Epoch: 72
Loss: 0.14500920223737293
ROC train: 0.993082	val: 0.882282	test: 0.878863
PRC train: 0.997824	val: 0.943358	test: 0.953031

Epoch: 73
Loss: 0.13376946406081058
ROC train: 0.992702	val: 0.885001	test: 0.866559
PRC train: 0.997722	val: 0.946217	test: 0.946141

Epoch: 74
Loss: 0.14690960431119943
ROC train: 0.993275	val: 0.889114	test: 0.875714
PRC train: 0.997888	val: 0.949571	test: 0.952539

Epoch: 75
Loss: 0.13779205127974634
ROC train: 0.992082	val: 0.876708	test: 0.862097
PRC train: 0.997433	val: 0.945700	test: 0.940991

Epoch: 76
Loss: 0.13690415992076524
ROC train: 0.994427	val: 0.885137	test: 0.881423
PRC train: 0.998156	val: 0.944992	test: 0.949381

Epoch: 77
Loss: 0.13605057693335706
ROC train: 0.993653	val: 0.889589	test: 0.901306
PRC train: 0.997911	val: 0.943722	test: 0.963299

Epoch: 78
Loss: 0.1392148330177941
ROC train: 0.991624	val: 0.894110	test: 0.901339
PRC train: 0.997228	val: 0.945163	test: 0.964174

Epoch: 79
Loss: 0.12767704735621105
ROC train: 0.995855	val: 0.897645	test: 0.902389
PRC train: 0.998678	val: 0.949853	test: 0.964770

Epoch: 80
Loss: 0.11840078983649842
ROC train: 0.996706	val: 0.889793	test: 0.898025
PRC train: 0.998949	val: 0.943247	test: 0.962049

Epoch: 81
Loss: 0.10666057974261747
ROC train: 0.996672	val: 0.888485	test: 0.883916
PRC train: 0.998939	val: 0.943521	test: 0.951115

Epoch: 82
Loss: 0.1287933354540634
ROC train: 0.997281	val: 0.890575	test: 0.887329
PRC train: 0.999140	val: 0.945555	test: 0.954780

Epoch: 83
Loss: 0.11573930337073397
ROC train: 0.995842	val: 0.883845	test: 0.890938
PRC train: 0.998666	val: 0.941068	test: 0.958004

Epoch: 84
Loss: 0.13553719620167395
ROC train: 0.994829	val: 0.886666	test: 0.891069
PRC train: 0.998268	val: 0.942310	test: 0.958123

Epoch: 85
Loss: 0.1382541438887532
ROC train: 0.994204	val: 0.891867	test: 0.888510
PRC train: 0.998148	val: 0.951525	test: 0.959302

Epoch: 86
Loss: 0.12750834042970097
ROC train: 0.995194	val: 0.886513	test: 0.869184
PRC train: 0.998470	val: 0.943575	test: 0.943501

Epoch: 87
Loss: 0.13529144779969302
ROC train: 0.996847	val: 0.882928	test: 0.876600
PRC train: 0.998951	val: 0.938786	test: 0.945288

Epoch: 88
Loss: 0.12023824421567308
ROC train: 0.995507	val: 0.889284	test: 0.885885
PRC train: 0.998565	val: 0.945726	test: 0.954861

Epoch: 89
Loss: 0.11920500459471661
ROC train: 0.995451	val: 0.893974	test: 0.887394
PRC train: 0.998584	val: 0.948984	test: 0.956473

Epoch: 90
Loss: 0.11227760842277919
ROC train: 0.997055	val: 0.886089	test: 0.882112
PRC train: 0.999061	val: 0.942810	test: 0.952091

Epoch: 91
Loss: 0.10444287968518129
ROC train: 0.997082	val: 0.889385	test: 0.885786
PRC train: 0.999060	val: 0.945171	test: 0.955132

Epoch: 92
Loss: 0.11832075343342119
ROC train: 0.997217	val: 0.895537	test: 0.890282
PRC train: 0.999134	val: 0.948708	test: 0.958282

Epoch: 93
Loss: 0.10901300721037407
ROC train: 0.998224	val: 0.890303	test: 0.884835
PRC train: 0.999439	val: 0.945141	test: 0.954379

Epoch: 94
Loss: 0.10260705250291775
ROC train: 0.969482	val: 0.877362	test: 0.924473
PRC train: 0.989977	val: 0.939720	test: 0.975925

Epoch: 34
Loss: 0.2146686630033348
ROC train: 0.974980	val: 0.874580	test: 0.922242
PRC train: 0.991779	val: 0.936662	test: 0.973754

Epoch: 35
Loss: 0.2189249695279084
ROC train: 0.975423	val: 0.871507	test: 0.914406
PRC train: 0.991733	val: 0.940336	test: 0.968511

Epoch: 36
Loss: 0.20371449572450062
ROC train: 0.977034	val: 0.888261	test: 0.907836
PRC train: 0.992230	val: 0.945824	test: 0.961640

Epoch: 37
Loss: 0.20668568230792514
ROC train: 0.978305	val: 0.891333	test: 0.914225
PRC train: 0.992772	val: 0.949043	test: 0.966743

Epoch: 38
Loss: 0.19945410251902052
ROC train: 0.976941	val: 0.884957	test: 0.924653
PRC train: 0.992456	val: 0.940446	test: 0.972221

Epoch: 39
Loss: 0.2109259871324086
ROC train: 0.976419	val: 0.869391	test: 0.920434
PRC train: 0.992452	val: 0.932020	test: 0.971905

Epoch: 40
Loss: 0.20116179135040393
ROC train: 0.982526	val: 0.883623	test: 0.925196
PRC train: 0.994487	val: 0.940819	test: 0.974019

Epoch: 41
Loss: 0.20944389158939944
ROC train: 0.983008	val: 0.889130	test: 0.918324
PRC train: 0.994711	val: 0.944145	test: 0.969975

Epoch: 42
Loss: 0.19188314185025454
ROC train: 0.980950	val: 0.886870	test: 0.921941
PRC train: 0.993910	val: 0.945521	test: 0.972946

Epoch: 43
Loss: 0.19989187990579257
ROC train: 0.982182	val: 0.877420	test: 0.916516
PRC train: 0.994313	val: 0.932581	test: 0.968731

Epoch: 44
Loss: 0.2155086399691574
ROC train: 0.982543	val: 0.877014	test: 0.915310
PRC train: 0.994451	val: 0.930699	test: 0.969508

Epoch: 45
Loss: 0.20754874269582746
ROC train: 0.981625	val: 0.883043	test: 0.913020
PRC train: 0.994251	val: 0.937958	test: 0.967785

Epoch: 46
Loss: 0.19847224358542795
ROC train: 0.982310	val: 0.894058	test: 0.917722
PRC train: 0.994207	val: 0.941466	test: 0.968201

Epoch: 47
Loss: 0.18038583755312484
ROC train: 0.980744	val: 0.870464	test: 0.901266
PRC train: 0.993736	val: 0.937092	test: 0.961308

Epoch: 48
Loss: 0.18774638048744605
ROC train: 0.985105	val: 0.876435	test: 0.915190
PRC train: 0.995282	val: 0.939915	test: 0.967758

Epoch: 49
Loss: 0.18468034202316308
ROC train: 0.985874	val: 0.892667	test: 0.920976
PRC train: 0.995566	val: 0.946971	test: 0.970585

Epoch: 50
Loss: 0.18193718707676812
ROC train: 0.986819	val: 0.886058	test: 0.914647
PRC train: 0.995887	val: 0.944538	test: 0.968532

Epoch: 51
Loss: 0.16814779055298512
ROC train: 0.987494	val: 0.871855	test: 0.911031
PRC train: 0.996118	val: 0.939621	test: 0.969010

Epoch: 52
Loss: 0.20260720688421788
ROC train: 0.986936	val: 0.872783	test: 0.916697
PRC train: 0.995883	val: 0.938657	test: 0.968986

Epoch: 53
Loss: 0.17780481884324403
ROC train: 0.986923	val: 0.870174	test: 0.920313
PRC train: 0.995988	val: 0.938663	test: 0.972369

Epoch: 54
Loss: 0.18840216286438674
ROC train: 0.987920	val: 0.880493	test: 0.925558
PRC train: 0.996265	val: 0.939673	test: 0.973817

Epoch: 55
Loss: 0.15997364223005942
ROC train: 0.988950	val: 0.884667	test: 0.921459
PRC train: 0.996567	val: 0.937894	test: 0.972202

Epoch: 56
Loss: 0.17526740583093262
ROC train: 0.988361	val: 0.889652	test: 0.917480
PRC train: 0.996337	val: 0.935203	test: 0.970537

Epoch: 57
Loss: 0.16517039998174057
ROC train: 0.986886	val: 0.878580	test: 0.910609
PRC train: 0.995864	val: 0.935768	test: 0.966854

Epoch: 58
Loss: 0.18966696388976265
ROC train: 0.989522	val: 0.881652	test: 0.908921
PRC train: 0.996762	val: 0.937196	test: 0.967861

Epoch: 59
Loss: 0.17643235810759003
ROC train: 0.988424	val: 0.887333	test: 0.917661
PRC train: 0.996439	val: 0.942133	test: 0.972203

Epoch: 60
Loss: 0.1654415617214728
ROC train: 0.988335	val: 0.887565	test: 0.917541
PRC train: 0.996356	val: 0.942055	test: 0.967474

Epoch: 61
Loss: 0.16264643269113727
ROC train: 0.989514	val: 0.883159	test: 0.913562
PRC train: 0.996636	val: 0.936460	test: 0.967693

Epoch: 62
Loss: 0.15961431734248893
ROC train: 0.986920	val: 0.862058	test: 0.899638
PRC train: 0.995909	val: 0.924284	test: 0.960764

Epoch: 63
Loss: 0.16861873899233992
ROC train: 0.988375	val: 0.890464	test: 0.926341
PRC train: 0.996369	val: 0.940467	test: 0.974805

Epoch: 64
Loss: 0.15238914497539555
ROC train: 0.990266	val: 0.888203	test: 0.920374
PRC train: 0.996959	val: 0.938405	test: 0.972751

Epoch: 65
Loss: 0.17412685013100862
ROC train: 0.991703	val: 0.888841	test: 0.922845
PRC train: 0.997445	val: 0.944403	test: 0.973184

Epoch: 66
Loss: 0.16714315370945387
ROC train: 0.990813	val: 0.887739	test: 0.916878
PRC train: 0.997140	val: 0.942880	test: 0.968561

Epoch: 67
Loss: 0.15758350455133505
ROC train: 0.992473	val: 0.883391	test: 0.917842
PRC train: 0.997673	val: 0.941161	test: 0.969980

Epoch: 68
Loss: 0.15536244709725125
ROC train: 0.992585	val: 0.871565	test: 0.917360
PRC train: 0.997757	val: 0.936504	test: 0.971649

Epoch: 69
Loss: 0.15908928774734307
ROC train: 0.993585	val: 0.870058	test: 0.915732
PRC train: 0.998038	val: 0.937023	test: 0.970542

Epoch: 70
Loss: 0.1582224431902416
ROC train: 0.983300	val: 0.878754	test: 0.907173
PRC train: 0.994537	val: 0.942898	test: 0.967272

Epoch: 71
Loss: 0.15916724078023073
ROC train: 0.990451	val: 0.876116	test: 0.902170
PRC train: 0.996973	val: 0.941122	test: 0.962880

Epoch: 72
Loss: 0.14087409466975023
ROC train: 0.991870	val: 0.861391	test: 0.909283
PRC train: 0.997442	val: 0.935727	test: 0.967538

Epoch: 73
Loss: 0.15431476757430507
ROC train: 0.991888	val: 0.857826	test: 0.912899
PRC train: 0.997471	val: 0.936201	test: 0.970434

Epoch: 74
Loss: 0.16321850472577856
ROC train: 0.993623	val: 0.871043	test: 0.911995
PRC train: 0.998046	val: 0.942259	test: 0.968833

Epoch: 75
Loss: 0.15969321320474922
ROC train: 0.993194	val: 0.879391	test: 0.912960
PRC train: 0.997906	val: 0.941440	test: 0.967843

Epoch: 76
Loss: 0.1335654992892857
ROC train: 0.992697	val: 0.873043	test: 0.905244
PRC train: 0.997676	val: 0.943560	test: 0.965711

Epoch: 77
Loss: 0.14983529335904847
ROC train: 0.993144	val: 0.877101	test: 0.901869
PRC train: 0.997815	val: 0.947423	test: 0.965342

Epoch: 78
Loss: 0.14089101821072134
ROC train: 0.994497	val: 0.869710	test: 0.905666
PRC train: 0.998329	val: 0.936720	test: 0.965246

Epoch: 79
Loss: 0.14812629297299698
ROC train: 0.988238	val: 0.891362	test: 0.908499
PRC train: 0.996273	val: 0.946902	test: 0.968188

Epoch: 80
Loss: 0.14739789071296203
ROC train: 0.994563	val: 0.876319	test: 0.906269
PRC train: 0.998328	val: 0.941223	test: 0.966119

Epoch: 81
Loss: 0.13052853387769828
ROC train: 0.994771	val: 0.883565	test: 0.910729
PRC train: 0.998392	val: 0.944626	test: 0.967334

Epoch: 82
Loss: 0.15747330692356545
ROC train: 0.995009	val: 0.877942	test: 0.912598
PRC train: 0.998455	val: 0.941606	test: 0.968590

Epoch: 83
Loss: 0.12640724648116666
ROC train: 0.994111	val: 0.881623	test: 0.926341
PRC train: 0.998205	val: 0.943416	test: 0.975209

Epoch: 84
Loss: 0.13366486008781095
ROC train: 0.995719	val: 0.882116	test: 0.920735
PRC train: 0.998699	val: 0.944122	test: 0.973036

Epoch: 85
Loss: 0.13436190620306873
ROC train: 0.991968	val: 0.875217	test: 0.905726
PRC train: 0.997485	val: 0.943509	test: 0.963963

Epoch: 86
Loss: 0.137347622039652
ROC train: 0.995036	val: 0.873304	test: 0.913382
PRC train: 0.998493	val: 0.937450	test: 0.969482

Epoch: 87
Loss: 0.150075283834349
ROC train: 0.994616	val: 0.884145	test: 0.932369
PRC train: 0.998352	val: 0.944924	test: 0.977381

Epoch: 88
Loss: 0.13987238595440446
ROC train: 0.989399	val: 0.851623	test: 0.897589
PRC train: 0.996732	val: 0.934176	test: 0.959371

Epoch: 89
Loss: 0.1408309342543227
ROC train: 0.993489	val: 0.866000	test: 0.888547
PRC train: 0.997991	val: 0.942197	test: 0.958648

Epoch: 90
Loss: 0.12923244341801607
ROC train: 0.994272	val: 0.884232	test: 0.914346
PRC train: 0.998232	val: 0.945451	test: 0.971170

Epoch: 91
Loss: 0.11973233427704671
ROC train: 0.988444	val: 0.877594	test: 0.915793
PRC train: 0.996356	val: 0.937487	test: 0.970471

Epoch: 92
Loss: 0.12184481760670958
ROC train: 0.995557	val: 0.895855	test: 0.907896
PRC train: 0.998645	val: 0.953658	test: 0.969302

Epoch: 93
Loss: 0.14004550142894798
ROC train: 0.996583	val: 0.895159	test: 0.909403
PRC train: 0.998961	val: 0.954534	test: 0.969850

Epoch: 94
Loss: 0.12833039361980247
ROC train: 0.975547	val: 0.878812	test: 0.909222
PRC train: 0.991365	val: 0.940226	test: 0.964898

Epoch: 34
Loss: 0.22140615478794756
ROC train: 0.978735	val: 0.864841	test: 0.907655
PRC train: 0.992808	val: 0.933407	test: 0.961614

Epoch: 35
Loss: 0.21627878746108722
ROC train: 0.975910	val: 0.884087	test: 0.915491
PRC train: 0.991383	val: 0.946601	test: 0.966651

Epoch: 36
Loss: 0.19918827341645481
ROC train: 0.978269	val: 0.883101	test: 0.925316
PRC train: 0.992483	val: 0.942241	test: 0.971967

Epoch: 37
Loss: 0.1966598269023224
ROC train: 0.976927	val: 0.859971	test: 0.905847
PRC train: 0.991997	val: 0.936749	test: 0.961928

Epoch: 38
Loss: 0.20027255514321451
ROC train: 0.980406	val: 0.862986	test: 0.913743
PRC train: 0.993325	val: 0.938588	test: 0.967378

Epoch: 39
Loss: 0.18515920301392721
ROC train: 0.983612	val: 0.878551	test: 0.918686
PRC train: 0.994648	val: 0.946953	test: 0.969956

Epoch: 40
Loss: 0.19148129009422954
ROC train: 0.981461	val: 0.878638	test: 0.918686
PRC train: 0.993956	val: 0.945182	test: 0.970624

Epoch: 41
Loss: 0.1931530642295167
ROC train: 0.982466	val: 0.875043	test: 0.910970
PRC train: 0.994065	val: 0.946221	test: 0.964057

Epoch: 42
Loss: 0.1938289676875652
ROC train: 0.980817	val: 0.871681	test: 0.903074
PRC train: 0.993068	val: 0.943294	test: 0.960311

Epoch: 43
Loss: 0.19834178178392156
ROC train: 0.983666	val: 0.881159	test: 0.915793
PRC train: 0.994470	val: 0.944457	test: 0.969271

Epoch: 44
Loss: 0.18948066405681874
ROC train: 0.982695	val: 0.864087	test: 0.911935
PRC train: 0.994306	val: 0.936669	test: 0.966971

Epoch: 45
Loss: 0.19156383171936528
ROC train: 0.977787	val: 0.879913	test: 0.917782
PRC train: 0.992914	val: 0.945309	test: 0.971685

Epoch: 46
Loss: 0.1796410083631975
ROC train: 0.984581	val: 0.865884	test: 0.908921
PRC train: 0.994896	val: 0.931162	test: 0.963925

Epoch: 47
Loss: 0.17922850567760204
ROC train: 0.985244	val: 0.865188	test: 0.908921
PRC train: 0.995069	val: 0.930205	test: 0.965013

Epoch: 48
Loss: 0.16676886093977283
ROC train: 0.984882	val: 0.869246	test: 0.916215
PRC train: 0.994894	val: 0.937699	test: 0.968219

Epoch: 49
Loss: 0.18986651556163703
ROC train: 0.985418	val: 0.878464	test: 0.919228
PRC train: 0.995137	val: 0.944263	test: 0.970636

Epoch: 50
Loss: 0.17488241565618187
ROC train: 0.982633	val: 0.846058	test: 0.893550
PRC train: 0.994248	val: 0.927480	test: 0.956479

Epoch: 51
Loss: 0.18280306453201314
ROC train: 0.987880	val: 0.857420	test: 0.911875
PRC train: 0.996219	val: 0.922617	test: 0.963820

Epoch: 52
Loss: 0.18669931026585176
ROC train: 0.983408	val: 0.864319	test: 0.919831
PRC train: 0.994680	val: 0.928930	test: 0.969383

Epoch: 53
Loss: 0.18030049041319277
ROC train: 0.985731	val: 0.881072	test: 0.910850
PRC train: 0.995128	val: 0.944905	test: 0.966762

Epoch: 54
Loss: 0.17289894961497185
ROC train: 0.985844	val: 0.881449	test: 0.900422
PRC train: 0.994859	val: 0.946534	test: 0.962003

Epoch: 55
Loss: 0.16396509710107757
ROC train: 0.989741	val: 0.868580	test: 0.905124
PRC train: 0.996703	val: 0.941056	test: 0.964406

Epoch: 56
Loss: 0.17002897670592002
ROC train: 0.987939	val: 0.855971	test: 0.908198
PRC train: 0.996191	val: 0.932647	test: 0.963827

Epoch: 57
Loss: 0.18426921928643117
ROC train: 0.984348	val: 0.834348	test: 0.882640
PRC train: 0.995018	val: 0.919297	test: 0.947146

Epoch: 58
Loss: 0.18240246719908035
ROC train: 0.989251	val: 0.852551	test: 0.884087
PRC train: 0.996373	val: 0.926660	test: 0.951857

Epoch: 59
Loss: 0.16548071195672268
ROC train: 0.990141	val: 0.881942	test: 0.916275
PRC train: 0.996776	val: 0.943141	test: 0.971344

Epoch: 60
Loss: 0.16290632115129824
ROC train: 0.984746	val: 0.893246	test: 0.930319
PRC train: 0.995000	val: 0.947421	test: 0.977517

Epoch: 61
Loss: 0.1594832423535765
ROC train: 0.988877	val: 0.891797	test: 0.925558
PRC train: 0.996365	val: 0.948417	test: 0.973681

Epoch: 62
Loss: 0.16993086094115492
ROC train: 0.989613	val: 0.866174	test: 0.907595
PRC train: 0.996450	val: 0.941094	test: 0.963920

Epoch: 63
Loss: 0.16586527842392698
ROC train: 0.983008	val: 0.849768	test: 0.878360
PRC train: 0.993883	val: 0.932467	test: 0.945259

Epoch: 64
Loss: 0.16935715282418404
ROC train: 0.991727	val: 0.877072	test: 0.913562
PRC train: 0.997333	val: 0.939733	test: 0.968547

Epoch: 65
Loss: 0.15800920418225353
ROC train: 0.989473	val: 0.887855	test: 0.914467
PRC train: 0.996606	val: 0.950694	test: 0.967663

Epoch: 66
Loss: 0.15519687436259683
ROC train: 0.990166	val: 0.880841	test: 0.910970
PRC train: 0.996897	val: 0.947764	test: 0.967819

Epoch: 67
Loss: 0.15591345875747512
ROC train: 0.990325	val: 0.865942	test: 0.900784
PRC train: 0.996895	val: 0.942409	test: 0.963246

Epoch: 68
Loss: 0.1446265330166189
ROC train: 0.993601	val: 0.872145	test: 0.913140
PRC train: 0.997996	val: 0.940286	test: 0.967524

Epoch: 69
Loss: 0.15190739452784743
ROC train: 0.992266	val: 0.871101	test: 0.915069
PRC train: 0.997535	val: 0.934255	test: 0.968895

Epoch: 70
Loss: 0.14426355424286672
ROC train: 0.992097	val: 0.887623	test: 0.922303
PRC train: 0.997462	val: 0.943773	test: 0.970983

Epoch: 71
Loss: 0.14085120482747202
ROC train: 0.992192	val: 0.890580	test: 0.915732
PRC train: 0.997483	val: 0.949855	test: 0.965071

Epoch: 72
Loss: 0.1331922477470228
ROC train: 0.993686	val: 0.878928	test: 0.915009
PRC train: 0.997998	val: 0.943233	test: 0.967395

Epoch: 73
Loss: 0.1503042351127498
ROC train: 0.993770	val: 0.872609	test: 0.910428
PRC train: 0.998068	val: 0.939682	test: 0.964932

Epoch: 74
Loss: 0.13734943681773615
ROC train: 0.994294	val: 0.883159	test: 0.905666
PRC train: 0.998175	val: 0.947661	test: 0.960021

Epoch: 75
Loss: 0.13727869244902366
ROC train: 0.991445	val: 0.858696	test: 0.887583
PRC train: 0.997290	val: 0.934239	test: 0.948198

Epoch: 76
Loss: 0.1426762667352132
ROC train: 0.994113	val: 0.858696	test: 0.902291
PRC train: 0.998168	val: 0.930337	test: 0.960012

Epoch: 77
Loss: 0.142303017001172
ROC train: 0.995026	val: 0.869884	test: 0.903376
PRC train: 0.998473	val: 0.936924	test: 0.962914

Epoch: 78
Loss: 0.1506525190127886
ROC train: 0.990981	val: 0.881362	test: 0.903677
PRC train: 0.997063	val: 0.944172	test: 0.964639

Epoch: 79
Loss: 0.1384633963395413
ROC train: 0.993252	val: 0.886580	test: 0.919168
PRC train: 0.997842	val: 0.950477	test: 0.973405

Epoch: 80
Loss: 0.14446462069648205
ROC train: 0.994854	val: 0.868609	test: 0.914105
PRC train: 0.998377	val: 0.941906	test: 0.969829

Epoch: 81
Loss: 0.14495167809697507
ROC train: 0.991629	val: 0.834435	test: 0.880350
PRC train: 0.997311	val: 0.922728	test: 0.953472

Epoch: 82
Loss: 0.13611250081543488
ROC train: 0.993352	val: 0.853942	test: 0.889391
PRC train: 0.997960	val: 0.930664	test: 0.956514

Epoch: 83
Loss: 0.13176729198032194
ROC train: 0.995553	val: 0.870580	test: 0.892285
PRC train: 0.998641	val: 0.942619	test: 0.956990

Epoch: 84
Loss: 0.13552101742401568
ROC train: 0.995742	val: 0.896812	test: 0.916034
PRC train: 0.998661	val: 0.958947	test: 0.966917

Epoch: 85
Loss: 0.13030534958931647
ROC train: 0.995399	val: 0.882058	test: 0.902592
PRC train: 0.998530	val: 0.952963	test: 0.959678

Epoch: 86
Loss: 0.13876325250441815
ROC train: 0.995727	val: 0.862928	test: 0.899638
PRC train: 0.998682	val: 0.939303	test: 0.960202

Epoch: 87
Loss: 0.13480700967066375
ROC train: 0.996970	val: 0.879797	test: 0.914888
PRC train: 0.999064	val: 0.944656	test: 0.969502

Epoch: 88
Loss: 0.11317652653672423
ROC train: 0.997054	val: 0.878638	test: 0.913442
PRC train: 0.999100	val: 0.944376	test: 0.969481

Epoch: 89
Loss: 0.13431499546433115
ROC train: 0.997570	val: 0.869565	test: 0.901206
PRC train: 0.999251	val: 0.940281	test: 0.962629

Epoch: 90
Loss: 0.11835690547340955
ROC train: 0.997196	val: 0.872667	test: 0.902471
PRC train: 0.999144	val: 0.943487	test: 0.963920

Epoch: 91
Loss: 0.1408110272345851
ROC train: 0.996040	val: 0.882348	test: 0.920072
PRC train: 0.998776	val: 0.948007	test: 0.972455

Epoch: 92
Loss: 0.12790340371261869
ROC train: 0.995182	val: 0.878870	test: 0.916998
PRC train: 0.998516	val: 0.947464	test: 0.971947

Epoch: 93
Loss: 0.11467307746432108
ROC train: 0.996406	val: 0.878058	test: 0.923629
PRC train: 0.998912	val: 0.944945	test: 0.973481

Epoch: 94
Loss: 0.1086137087797111
ROC train: 0.978267	val: 0.881884	test: 0.919409
PRC train: 0.992708	val: 0.948601	test: 0.971372

Epoch: 34
Loss: 0.2029341891095251
ROC train: 0.975448	val: 0.885536	test: 0.928210
PRC train: 0.991374	val: 0.946007	test: 0.974169

Epoch: 35
Loss: 0.20622745294084868
ROC train: 0.979410	val: 0.891333	test: 0.915552
PRC train: 0.992786	val: 0.954359	test: 0.967000

Epoch: 36
Loss: 0.22210297628875844
ROC train: 0.968760	val: 0.848232	test: 0.882158
PRC train: 0.989610	val: 0.929821	test: 0.949338

Epoch: 37
Loss: 0.19829754715096728
ROC train: 0.978505	val: 0.864435	test: 0.905184
PRC train: 0.992762	val: 0.937680	test: 0.963769

Epoch: 38
Loss: 0.21099610303087127
ROC train: 0.974111	val: 0.891159	test: 0.932248
PRC train: 0.991239	val: 0.943888	test: 0.977960

Epoch: 39
Loss: 0.2118833117352014
ROC train: 0.982595	val: 0.884435	test: 0.927667
PRC train: 0.994372	val: 0.945659	test: 0.975629

Epoch: 40
Loss: 0.1908740363587317
ROC train: 0.975903	val: 0.854348	test: 0.897830
PRC train: 0.992129	val: 0.936344	test: 0.959618

Epoch: 41
Loss: 0.21468863811758454
ROC train: 0.979923	val: 0.881884	test: 0.918505
PRC train: 0.993415	val: 0.947220	test: 0.970160

Epoch: 42
Loss: 0.19372040735433935
ROC train: 0.981611	val: 0.888319	test: 0.924533
PRC train: 0.993866	val: 0.949570	test: 0.973917

Epoch: 43
Loss: 0.1875524582131818
ROC train: 0.983453	val: 0.888725	test: 0.917480
PRC train: 0.994336	val: 0.951416	test: 0.970274

Epoch: 44
Loss: 0.19234125374378772
ROC train: 0.979424	val: 0.856957	test: 0.895118
PRC train: 0.993183	val: 0.936199	test: 0.958500

Epoch: 45
Loss: 0.17921983143867282
ROC train: 0.982732	val: 0.869855	test: 0.919952
PRC train: 0.994418	val: 0.936435	test: 0.971405

Epoch: 46
Loss: 0.19529190721779024
ROC train: 0.983937	val: 0.890696	test: 0.933153
PRC train: 0.994802	val: 0.946236	test: 0.977847

Epoch: 47
Loss: 0.18650670982069872
ROC train: 0.984874	val: 0.867391	test: 0.916817
PRC train: 0.995003	val: 0.932175	test: 0.969156

Epoch: 48
Loss: 0.17303718495712284
ROC train: 0.985886	val: 0.873826	test: 0.916637
PRC train: 0.995557	val: 0.940003	test: 0.970062

Epoch: 49
Loss: 0.18197568274099854
ROC train: 0.987560	val: 0.883797	test: 0.923448
PRC train: 0.996050	val: 0.947469	test: 0.971987

Epoch: 50
Loss: 0.1677943457103754
ROC train: 0.983282	val: 0.896203	test: 0.935142
PRC train: 0.994434	val: 0.949647	test: 0.977119

Epoch: 51
Loss: 0.1828715833140491
ROC train: 0.987898	val: 0.888000	test: 0.923327
PRC train: 0.995904	val: 0.943968	test: 0.971952

Epoch: 52
Loss: 0.17876990523591366
ROC train: 0.988817	val: 0.870522	test: 0.906088
PRC train: 0.996468	val: 0.942718	test: 0.963614

Epoch: 53
Loss: 0.1707116988108451
ROC train: 0.988128	val: 0.870406	test: 0.914406
PRC train: 0.996331	val: 0.939890	test: 0.966923

Epoch: 54
Loss: 0.16331920004700307
ROC train: 0.988857	val: 0.877246	test: 0.916637
PRC train: 0.996482	val: 0.941639	test: 0.969064

Epoch: 55
Loss: 0.18586533933329621
ROC train: 0.990734	val: 0.883855	test: 0.920012
PRC train: 0.997070	val: 0.945094	test: 0.970191

Epoch: 56
Loss: 0.15883978444845592
ROC train: 0.987953	val: 0.865942	test: 0.905365
PRC train: 0.996187	val: 0.938649	test: 0.961990

Epoch: 57
Loss: 0.17410458833454126
ROC train: 0.990139	val: 0.871913	test: 0.913803
PRC train: 0.996778	val: 0.938416	test: 0.967393

Epoch: 58
Loss: 0.14955923449102013
ROC train: 0.991648	val: 0.878580	test: 0.919892
PRC train: 0.997345	val: 0.935610	test: 0.969836

Epoch: 59
Loss: 0.16875005835149368
ROC train: 0.985937	val: 0.878464	test: 0.911573
PRC train: 0.995415	val: 0.940767	test: 0.967969

Epoch: 60
Loss: 0.1686027384854167
ROC train: 0.991252	val: 0.880899	test: 0.916335
PRC train: 0.997214	val: 0.940933	test: 0.967629

Epoch: 61
Loss: 0.15783289195431333
ROC train: 0.991588	val: 0.874319	test: 0.911814
PRC train: 0.997335	val: 0.939302	test: 0.965740

Epoch: 62
Loss: 0.1411515158031522
ROC train: 0.992264	val: 0.875275	test: 0.921037
PRC train: 0.997514	val: 0.942010	test: 0.972051

Epoch: 63
Loss: 0.1401652809686436
ROC train: 0.987813	val: 0.857188	test: 0.907776
PRC train: 0.996092	val: 0.934561	test: 0.965889

Epoch: 64
Loss: 0.16652780177218632
ROC train: 0.991936	val: 0.856261	test: 0.902652
PRC train: 0.997474	val: 0.932115	test: 0.960955

Epoch: 65
Loss: 0.15154885466449608
ROC train: 0.991927	val: 0.885768	test: 0.926763
PRC train: 0.997422	val: 0.941158	test: 0.974041

Epoch: 66
Loss: 0.15401293523002327
ROC train: 0.992870	val: 0.882522	test: 0.924834
PRC train: 0.997744	val: 0.942132	test: 0.971979

Epoch: 67
Loss: 0.14494789145517326
ROC train: 0.993374	val: 0.873826	test: 0.914105
PRC train: 0.997915	val: 0.939213	test: 0.967092

Epoch: 68
Loss: 0.16211496894342398
ROC train: 0.991334	val: 0.868667	test: 0.901507
PRC train: 0.997227	val: 0.942776	test: 0.963010

Epoch: 69
Loss: 0.14633956173726995
ROC train: 0.994730	val: 0.877478	test: 0.923026
PRC train: 0.998372	val: 0.943081	test: 0.973060

Epoch: 70
Loss: 0.15673125461477921
ROC train: 0.990922	val: 0.886116	test: 0.928813
PRC train: 0.997143	val: 0.941452	test: 0.976223

Epoch: 71
Loss: 0.14382123372055253
ROC train: 0.992479	val: 0.869014	test: 0.915552
PRC train: 0.997612	val: 0.938247	test: 0.969397

Epoch: 72
Loss: 0.14129487642375022
ROC train: 0.995458	val: 0.877188	test: 0.914888
PRC train: 0.998591	val: 0.942981	test: 0.967119

Epoch: 73
Loss: 0.14477351292642307
ROC train: 0.994172	val: 0.878638	test: 0.913321
PRC train: 0.998164	val: 0.944038	test: 0.965528

Epoch: 74
Loss: 0.15166313107806198
ROC train: 0.991967	val: 0.868319	test: 0.919771
PRC train: 0.997479	val: 0.941227	test: 0.971113

Epoch: 75
Loss: 0.15047794774119658
ROC train: 0.990185	val: 0.880493	test: 0.919048
PRC train: 0.996974	val: 0.948562	test: 0.973288

Epoch: 76
Loss: 0.1537030752003685
ROC train: 0.991855	val: 0.856551	test: 0.886438
PRC train: 0.997427	val: 0.939146	test: 0.956006

Epoch: 77
Loss: 0.12731592163801472
ROC train: 0.996477	val: 0.872087	test: 0.924473
PRC train: 0.998938	val: 0.941455	test: 0.972270

Epoch: 78
Loss: 0.12890954546313235
ROC train: 0.993583	val: 0.887971	test: 0.924895
PRC train: 0.998039	val: 0.943857	test: 0.971442

Epoch: 79
Loss: 0.13535795921625424
ROC train: 0.994190	val: 0.882580	test: 0.906751
PRC train: 0.998229	val: 0.947009	test: 0.959515

Epoch: 80
Loss: 0.14283991253348946
ROC train: 0.996028	val: 0.867652	test: 0.903315
PRC train: 0.998783	val: 0.942192	test: 0.962797

Epoch: 81
Loss: 0.14139404502990724
ROC train: 0.996075	val: 0.863043	test: 0.905063
PRC train: 0.998806	val: 0.932809	test: 0.958938

Epoch: 82
Loss: 0.13586422766956013
ROC train: 0.995286	val: 0.873362	test: 0.911453
PRC train: 0.998559	val: 0.938643	test: 0.965881

Epoch: 83
Loss: 0.14232448273091228
ROC train: 0.995961	val: 0.871449	test: 0.908198
PRC train: 0.998765	val: 0.939147	test: 0.963895

Epoch: 84
Loss: 0.13441158715284549
ROC train: 0.996518	val: 0.857826	test: 0.898553
PRC train: 0.998954	val: 0.932225	test: 0.958147

Epoch: 85
Loss: 0.11847283775028505
ROC train: 0.996318	val: 0.870087	test: 0.903797
PRC train: 0.998887	val: 0.942063	test: 0.962960

Epoch: 86
Loss: 0.13930557990213335
ROC train: 0.996505	val: 0.856435	test: 0.908740
PRC train: 0.998943	val: 0.928119	test: 0.963241

Epoch: 87
Loss: 0.12185108133704699
ROC train: 0.996598	val: 0.866174	test: 0.916395
PRC train: 0.998972	val: 0.932256	test: 0.966535

Epoch: 88
Loss: 0.11625390849529617
ROC train: 0.996188	val: 0.877333	test: 0.921640
PRC train: 0.998858	val: 0.938299	test: 0.970987

Epoch: 89
Loss: 0.1240867581897971
ROC train: 0.995442	val: 0.860319	test: 0.882098
PRC train: 0.998580	val: 0.941427	test: 0.951496

Epoch: 90
Loss: 0.11606189933884932
ROC train: 0.997950	val: 0.891391	test: 0.910970
PRC train: 0.999378	val: 0.954615	test: 0.966134

Epoch: 91
Loss: 0.11657117018668604
ROC train: 0.997474	val: 0.883043	test: 0.915552
PRC train: 0.999237	val: 0.945219	test: 0.967767

Epoch: 92
Loss: 0.1193026317860152
ROC train: 0.993565	val: 0.863710	test: 0.877939
PRC train: 0.998093	val: 0.939667	test: 0.951994

Epoch: 93
Loss: 0.11902902560454004
ROC train: 0.997717	val: 0.865536	test: 0.885775
PRC train: 0.999315	val: 0.937208	test: 0.953046

Epoch: 94
Loss: 0.1078958730237551
ROC train: 0.971593	val: 0.891466	test: 0.920722
PRC train: 0.989812	val: 0.956954	test: 0.965612

Epoch: 34
Loss: 0.2253869823795604
ROC train: 0.970390	val: 0.889478	test: 0.937075
PRC train: 0.989370	val: 0.959747	test: 0.976689

Epoch: 35
Loss: 0.2119532008505223
ROC train: 0.970628	val: 0.886960	test: 0.920330
PRC train: 0.989693	val: 0.953471	test: 0.964611

Epoch: 36
Loss: 0.2162693064329548
ROC train: 0.975470	val: 0.891068	test: 0.905678
PRC train: 0.991586	val: 0.958933	test: 0.958534

Epoch: 37
Loss: 0.21180830517668894
ROC train: 0.969533	val: 0.861649	test: 0.887363
PRC train: 0.989293	val: 0.946517	test: 0.942594

Epoch: 38
Loss: 0.2082776348904718
ROC train: 0.973872	val: 0.877153	test: 0.907509
PRC train: 0.990667	val: 0.953108	test: 0.956940

Epoch: 39
Loss: 0.22118326253672066
ROC train: 0.977946	val: 0.896899	test: 0.921900
PRC train: 0.992224	val: 0.962523	test: 0.968513

Epoch: 40
Loss: 0.2123688587472043
ROC train: 0.979123	val: 0.895441	test: 0.922030
PRC train: 0.992800	val: 0.959724	test: 0.969745

Epoch: 41
Loss: 0.19754869620807197
ROC train: 0.978035	val: 0.888683	test: 0.919806
PRC train: 0.992469	val: 0.953930	test: 0.965590

Epoch: 42
Loss: 0.19553726568603888
ROC train: 0.981124	val: 0.886297	test: 0.906593
PRC train: 0.993846	val: 0.956034	test: 0.959659

Epoch: 43
Loss: 0.1912029077330349
ROC train: 0.979938	val: 0.883779	test: 0.919283
PRC train: 0.993333	val: 0.957495	test: 0.969431

Epoch: 44
Loss: 0.19124792744056254
ROC train: 0.971205	val: 0.850252	test: 0.880429
PRC train: 0.990566	val: 0.940972	test: 0.941464

Epoch: 45
Loss: 0.18274077569040695
ROC train: 0.982808	val: 0.890406	test: 0.899006
PRC train: 0.994396	val: 0.957759	test: 0.952103

Epoch: 46
Loss: 0.20680901359116574
ROC train: 0.981878	val: 0.900212	test: 0.913658
PRC train: 0.993806	val: 0.960963	test: 0.963928

Epoch: 47
Loss: 0.19137894324486565
ROC train: 0.979809	val: 0.896766	test: 0.920460
PRC train: 0.993111	val: 0.960827	test: 0.970704

Epoch: 48
Loss: 0.1896519087546756
ROC train: 0.981778	val: 0.906043	test: 0.928702
PRC train: 0.993827	val: 0.965068	test: 0.972483

Epoch: 49
Loss: 0.18996781557795456
ROC train: 0.982341	val: 0.893586	test: 0.912350
PRC train: 0.994106	val: 0.959166	test: 0.963756

Epoch: 50
Loss: 0.17967806315990245
ROC train: 0.981452	val: 0.885370	test: 0.899398
PRC train: 0.994066	val: 0.951026	test: 0.951023

Epoch: 51
Loss: 0.1952006838902847
ROC train: 0.983782	val: 0.898622	test: 0.914966
PRC train: 0.994554	val: 0.961013	test: 0.964572

Epoch: 52
Loss: 0.18286655265198995
ROC train: 0.985026	val: 0.897032	test: 0.913396
PRC train: 0.994934	val: 0.962400	test: 0.965895

Epoch: 53
Loss: 0.18308652702827008
ROC train: 0.985534	val: 0.909886	test: 0.905678
PRC train: 0.995238	val: 0.966894	test: 0.961432

Epoch: 54
Loss: 0.18462035487323875
ROC train: 0.985939	val: 0.905380	test: 0.897436
PRC train: 0.995469	val: 0.963257	test: 0.959076

Epoch: 55
Loss: 0.16853252211581266
ROC train: 0.985022	val: 0.909488	test: 0.909210
PRC train: 0.995142	val: 0.966398	test: 0.966221

Epoch: 56
Loss: 0.18070505563062042
ROC train: 0.987637	val: 0.904320	test: 0.909471
PRC train: 0.995965	val: 0.964125	test: 0.965273

Epoch: 57
Loss: 0.1766338514265742
ROC train: 0.984393	val: 0.870660	test: 0.882653
PRC train: 0.995081	val: 0.941044	test: 0.944997

Epoch: 58
Loss: 0.17879191868607028
ROC train: 0.983004	val: 0.888948	test: 0.909733
PRC train: 0.994364	val: 0.956567	test: 0.962951

Epoch: 59
Loss: 0.1782129260373267
ROC train: 0.986482	val: 0.905778	test: 0.911303
PRC train: 0.995479	val: 0.963702	test: 0.961728

Epoch: 60
Loss: 0.18456147130672185
ROC train: 0.986536	val: 0.905248	test: 0.920068
PRC train: 0.995443	val: 0.965430	test: 0.969845

Epoch: 61
Loss: 0.1620556575497658
ROC train: 0.989301	val: 0.893984	test: 0.898613
PRC train: 0.996563	val: 0.960869	test: 0.959675

Epoch: 62
Loss: 0.17037365496500173
ROC train: 0.988783	val: 0.898224	test: 0.899137
PRC train: 0.996409	val: 0.963180	test: 0.956536

Epoch: 63
Loss: 0.1578153519546381
ROC train: 0.991014	val: 0.906043	test: 0.905285
PRC train: 0.997182	val: 0.964184	test: 0.961778

Epoch: 64
Loss: 0.15548448473846513
ROC train: 0.990546	val: 0.900477	test: 0.902407
PRC train: 0.997078	val: 0.961123	test: 0.960474

Epoch: 65
Loss: 0.1612728025892089
ROC train: 0.989288	val: 0.907236	test: 0.914835
PRC train: 0.996592	val: 0.965406	test: 0.964853

Epoch: 66
Loss: 0.1457880263236881
ROC train: 0.987244	val: 0.883117	test: 0.885400
PRC train: 0.996047	val: 0.955462	test: 0.951780

Epoch: 67
Loss: 0.16196942234034264
ROC train: 0.989701	val: 0.878876	test: 0.888409
PRC train: 0.996799	val: 0.950805	test: 0.951351

Epoch: 68
Loss: 0.13293817281930537
ROC train: 0.987634	val: 0.916512	test: 0.927263
PRC train: 0.995935	val: 0.971663	test: 0.973472

Epoch: 69
Loss: 0.15931154029594283
ROC train: 0.987571	val: 0.904453	test: 0.916274
PRC train: 0.995911	val: 0.964833	test: 0.968823

Epoch: 70
Loss: 0.1826728369092056
ROC train: 0.990767	val: 0.899549	test: 0.897436
PRC train: 0.997140	val: 0.960886	test: 0.955315

Epoch: 71
Loss: 0.1449667708457516
ROC train: 0.990704	val: 0.893586	test: 0.890895
PRC train: 0.997080	val: 0.962555	test: 0.956424

Epoch: 72
Loss: 0.1522900305332426
ROC train: 0.989133	val: 0.907501	test: 0.917190
PRC train: 0.996448	val: 0.965144	test: 0.968567

Epoch: 73
Loss: 0.1673878373450915
ROC train: 0.988905	val: 0.906971	test: 0.922423
PRC train: 0.996290	val: 0.964806	test: 0.971299

Epoch: 74
Loss: 0.15551521171009527
ROC train: 0.992188	val: 0.893453	test: 0.900968
PRC train: 0.997569	val: 0.959967	test: 0.961553

Epoch: 75
Loss: 0.1493316437319578
ROC train: 0.991462	val: 0.901007	test: 0.904500
PRC train: 0.997239	val: 0.962080	test: 0.957734

Epoch: 76
Loss: 0.14846401960762795
ROC train: 0.992954	val: 0.890803	test: 0.906724
PRC train: 0.997738	val: 0.959373	test: 0.961643

Epoch: 77
Loss: 0.1312375349303008
ROC train: 0.993574	val: 0.905248	test: 0.916667
PRC train: 0.997950	val: 0.966579	test: 0.967548

Epoch: 78
Loss: 0.14947011397571064
ROC train: 0.993124	val: 0.904850	test: 0.902930
PRC train: 0.997880	val: 0.964521	test: 0.958384

Epoch: 79
Loss: 0.15106931245831562
ROC train: 0.992484	val: 0.887623	test: 0.915751
PRC train: 0.997678	val: 0.954242	test: 0.965278

Epoch: 80
Loss: 0.14282584208088472
ROC train: 0.991743	val: 0.907898	test: 0.920984
PRC train: 0.997446	val: 0.963224	test: 0.967825

Epoch: 81
Loss: 0.150682115886635
ROC train: 0.993339	val: 0.899152	test: 0.909602
PRC train: 0.997981	val: 0.959457	test: 0.964755

Epoch: 82
Loss: 0.15797785573560358
ROC train: 0.993467	val: 0.908428	test: 0.927786
PRC train: 0.997948	val: 0.962537	test: 0.968023

Epoch: 83
Loss: 0.13182570019529757
ROC train: 0.994170	val: 0.909223	test: 0.909471
PRC train: 0.998223	val: 0.966010	test: 0.962155

Epoch: 84
Loss: 0.14120675388326814
ROC train: 0.995725	val: 0.908428	test: 0.895474
PRC train: 0.998659	val: 0.964765	test: 0.954621

Epoch: 85
Loss: 0.1327766069621003
ROC train: 0.994770	val: 0.908428	test: 0.908032
PRC train: 0.998336	val: 0.964255	test: 0.956715

Epoch: 86
Loss: 0.129315585815602
ROC train: 0.994823	val: 0.899284	test: 0.904631
PRC train: 0.998387	val: 0.959474	test: 0.953934

Epoch: 87
Loss: 0.13852994385849646
ROC train: 0.995624	val: 0.900477	test: 0.892857
PRC train: 0.998661	val: 0.962362	test: 0.948825

Epoch: 88
Loss: 0.12251962148816184
ROC train: 0.993268	val: 0.886960	test: 0.885008
PRC train: 0.997913	val: 0.953820	test: 0.939102

Epoch: 89
Loss: 0.13690809110555668
ROC train: 0.995799	val: 0.900345	test: 0.894165
PRC train: 0.998702	val: 0.961530	test: 0.945647

Epoch: 90
Loss: 0.13155688447608924
ROC train: 0.994823	val: 0.904453	test: 0.904239
PRC train: 0.998325	val: 0.965746	test: 0.960897

Epoch: 91
Loss: 0.13443606020958074
ROC train: 0.992719	val: 0.891466	test: 0.922423
PRC train: 0.997706	val: 0.959858	test: 0.971182

Epoch: 92
Loss: 0.1340231630251007
ROC train: 0.994487	val: 0.894911	test: 0.916405
PRC train: 0.998256	val: 0.957337	test: 0.964090

Epoch: 93
Loss: 0.12277728456546076
ROC train: 0.993532	val: 0.880466	test: 0.885008
PRC train: 0.998040	val: 0.952038	test: 0.944754

Epoch: 94
Loss: 0.12433021375976881
ROC train: 0.970071	val: 0.899947	test: 0.920984
PRC train: 0.989257	val: 0.963899	test: 0.968553

Epoch: 34
Loss: 0.22077671082936753
ROC train: 0.971217	val: 0.899682	test: 0.927263
PRC train: 0.989806	val: 0.963941	test: 0.972842

Epoch: 35
Loss: 0.21620282107703853
ROC train: 0.974199	val: 0.907633	test: 0.920591
PRC train: 0.990577	val: 0.966724	test: 0.967361

Epoch: 36
Loss: 0.21677880884657427
ROC train: 0.971043	val: 0.896104	test: 0.920722
PRC train: 0.989995	val: 0.957658	test: 0.970052

Epoch: 37
Loss: 0.21249181501815156
ROC train: 0.972351	val: 0.895309	test: 0.919021
PRC train: 0.990623	val: 0.962724	test: 0.968889

Epoch: 38
Loss: 0.21161041714888182
ROC train: 0.972348	val: 0.890406	test: 0.903454
PRC train: 0.990409	val: 0.956264	test: 0.953980

Epoch: 39
Loss: 0.21886991034392375
ROC train: 0.977488	val: 0.895441	test: 0.918106
PRC train: 0.992088	val: 0.956234	test: 0.959340

Epoch: 40
Loss: 0.20981498706013427
ROC train: 0.975029	val: 0.892923	test: 0.917975
PRC train: 0.991137	val: 0.957476	test: 0.964300

Epoch: 41
Loss: 0.20651259401012245
ROC train: 0.975280	val: 0.914789	test: 0.929356
PRC train: 0.991215	val: 0.968289	test: 0.972424

Epoch: 42
Loss: 0.2175455036852511
ROC train: 0.976541	val: 0.899549	test: 0.918106
PRC train: 0.992036	val: 0.958893	test: 0.968985

Epoch: 43
Loss: 0.1898737924141966
ROC train: 0.977616	val: 0.890008	test: 0.907509
PRC train: 0.992400	val: 0.953638	test: 0.958804

Epoch: 44
Loss: 0.21292530772768561
ROC train: 0.978717	val: 0.893321	test: 0.905285
PRC train: 0.992768	val: 0.954662	test: 0.955077

Epoch: 45
Loss: 0.19690002551042235
ROC train: 0.976439	val: 0.883514	test: 0.911957
PRC train: 0.991765	val: 0.952956	test: 0.961429

Epoch: 46
Loss: 0.20061864092136192
ROC train: 0.977969	val: 0.880732	test: 0.911041
PRC train: 0.992441	val: 0.954476	test: 0.961018

Epoch: 47
Loss: 0.19543816685325535
ROC train: 0.981610	val: 0.898489	test: 0.922815
PRC train: 0.993446	val: 0.961337	test: 0.968437

Epoch: 48
Loss: 0.1915595764064554
ROC train: 0.978087	val: 0.896501	test: 0.919283
PRC train: 0.992134	val: 0.958637	test: 0.966194

Epoch: 49
Loss: 0.2031933755525511
ROC train: 0.980972	val: 0.902465	test: 0.910256
PRC train: 0.993541	val: 0.962945	test: 0.964543

Epoch: 50
Loss: 0.20559705095865402
ROC train: 0.979334	val: 0.874503	test: 0.909864
PRC train: 0.993172	val: 0.946775	test: 0.964991

Epoch: 51
Loss: 0.18641118462601652
ROC train: 0.982086	val: 0.903658	test: 0.919021
PRC train: 0.993808	val: 0.963066	test: 0.966812

Epoch: 52
Loss: 0.18741850031512006
ROC train: 0.980270	val: 0.908428	test: 0.925693
PRC train: 0.992961	val: 0.965260	test: 0.971264

Epoch: 53
Loss: 0.18537148180982438
ROC train: 0.982767	val: 0.896766	test: 0.911434
PRC train: 0.994085	val: 0.958858	test: 0.964760

Epoch: 54
Loss: 0.19058673984424362
ROC train: 0.984510	val: 0.904453	test: 0.912742
PRC train: 0.994952	val: 0.964269	test: 0.965479

Epoch: 55
Loss: 0.17626141628873288
ROC train: 0.983578	val: 0.895706	test: 0.897174
PRC train: 0.994378	val: 0.957847	test: 0.952093

Epoch: 56
Loss: 0.19246446421252358
ROC train: 0.984586	val: 0.893851	test: 0.906201
PRC train: 0.995012	val: 0.957771	test: 0.961214

Epoch: 57
Loss: 0.19546695434867112
ROC train: 0.982536	val: 0.888550	test: 0.913134
PRC train: 0.994476	val: 0.954366	test: 0.965435

Epoch: 58
Loss: 0.18674230909056602
ROC train: 0.981668	val: 0.897164	test: 0.919283
PRC train: 0.993896	val: 0.958654	test: 0.966534

Epoch: 59
Loss: 0.17857710851961847
ROC train: 0.983854	val: 0.893719	test: 0.919545
PRC train: 0.994573	val: 0.957931	test: 0.966486

Epoch: 60
Loss: 0.1863924794513403
ROC train: 0.986099	val: 0.894911	test: 0.915751
PRC train: 0.995354	val: 0.958655	test: 0.960401

Epoch: 61
Loss: 0.18099176567099867
ROC train: 0.985325	val: 0.891068	test: 0.916536
PRC train: 0.995126	val: 0.953796	test: 0.962871

Epoch: 62
Loss: 0.1729658542846551
ROC train: 0.988030	val: 0.900345	test: 0.921115
PRC train: 0.996072	val: 0.958594	test: 0.965334

Epoch: 63
Loss: 0.16947139660364674
ROC train: 0.988904	val: 0.895441	test: 0.920722
PRC train: 0.996309	val: 0.958115	test: 0.967118

Epoch: 64
Loss: 0.17285669487987423
ROC train: 0.985364	val: 0.890538	test: 0.919414
PRC train: 0.995039	val: 0.957550	test: 0.969244

Epoch: 65
Loss: 0.1850922255411174
ROC train: 0.986318	val: 0.894116	test: 0.914704
PRC train: 0.995553	val: 0.953694	test: 0.964334

Epoch: 66
Loss: 0.1843203171346229
ROC train: 0.987655	val: 0.901007	test: 0.916928
PRC train: 0.995885	val: 0.957797	test: 0.967539

Epoch: 67
Loss: 0.16816023410683997
ROC train: 0.985103	val: 0.886562	test: 0.895997
PRC train: 0.995106	val: 0.952941	test: 0.954611

Epoch: 68
Loss: 0.15714705224626788
ROC train: 0.987769	val: 0.894646	test: 0.917059
PRC train: 0.995879	val: 0.957794	test: 0.964411

Epoch: 69
Loss: 0.1651331037702032
ROC train: 0.989108	val: 0.905380	test: 0.899660
PRC train: 0.996294	val: 0.962235	test: 0.958016

Epoch: 70
Loss: 0.15826727056838247
ROC train: 0.989455	val: 0.899284	test: 0.904369
PRC train: 0.996497	val: 0.957683	test: 0.957417

Epoch: 71
Loss: 0.15821258949312367
ROC train: 0.990376	val: 0.898357	test: 0.911826
PRC train: 0.996912	val: 0.959656	test: 0.965356

Epoch: 72
Loss: 0.16063132283058265
ROC train: 0.990408	val: 0.900610	test: 0.912480
PRC train: 0.996844	val: 0.959295	test: 0.964133

Epoch: 73
Loss: 0.150798491247087
ROC train: 0.987726	val: 0.889080	test: 0.903323
PRC train: 0.996068	val: 0.951290	test: 0.954471

Epoch: 74
Loss: 0.16666476473022626
ROC train: 0.984515	val: 0.891996	test: 0.893642
PRC train: 0.994965	val: 0.954823	test: 0.956781

Epoch: 75
Loss: 0.18626961647568246
ROC train: 0.989348	val: 0.886032	test: 0.887232
PRC train: 0.996535	val: 0.947947	test: 0.948521

Epoch: 76
Loss: 0.1480886722034989
ROC train: 0.984911	val: 0.887092	test: 0.915882
PRC train: 0.994793	val: 0.951907	test: 0.960776

Epoch: 77
Loss: 0.16831596725746503
ROC train: 0.990065	val: 0.896501	test: 0.921900
PRC train: 0.996725	val: 0.955746	test: 0.967530

Epoch: 78
Loss: 0.15960101764308415
ROC train: 0.992093	val: 0.894514	test: 0.914181
PRC train: 0.997437	val: 0.958543	test: 0.964721

Epoch: 79
Loss: 0.1636178684308838
ROC train: 0.992185	val: 0.902067	test: 0.909210
PRC train: 0.997528	val: 0.962569	test: 0.959603

Epoch: 80
Loss: 0.15922791132528427
ROC train: 0.981918	val: 0.864697	test: 0.891287
PRC train: 0.994059	val: 0.937906	test: 0.952553

Epoch: 81
Loss: 0.15871030094823876
ROC train: 0.991856	val: 0.881792	test: 0.911172
PRC train: 0.997427	val: 0.951769	test: 0.961037

Epoch: 82
Loss: 0.15243204055262227
ROC train: 0.990211	val: 0.893719	test: 0.915097
PRC train: 0.996838	val: 0.956846	test: 0.964873

Epoch: 83
Loss: 0.16860452244553978
ROC train: 0.990637	val: 0.894911	test: 0.912480
PRC train: 0.996925	val: 0.957627	test: 0.966243

Epoch: 84
Loss: 0.14310632473273577
ROC train: 0.991018	val: 0.889478	test: 0.908294
PRC train: 0.997103	val: 0.952070	test: 0.955431

Epoch: 85
Loss: 0.15515491234942652
ROC train: 0.993112	val: 0.888683	test: 0.911303
PRC train: 0.997829	val: 0.952446	test: 0.961559

Epoch: 86
Loss: 0.1522002575745473
ROC train: 0.992988	val: 0.885900	test: 0.917321
PRC train: 0.997757	val: 0.952297	test: 0.967831

Epoch: 87
Loss: 0.14694803402419487
ROC train: 0.992410	val: 0.896766	test: 0.905154
PRC train: 0.997578	val: 0.962733	test: 0.961998

Epoch: 88
Loss: 0.14158988754611446
ROC train: 0.991528	val: 0.887490	test: 0.904762
PRC train: 0.997214	val: 0.953375	test: 0.958432

Epoch: 89
Loss: 0.1567900403913136
ROC train: 0.993991	val: 0.894646	test: 0.904108
PRC train: 0.998120	val: 0.956070	test: 0.963681

Epoch: 90
Loss: 0.14231211037723326
ROC train: 0.993687	val: 0.883912	test: 0.905024
PRC train: 0.998017	val: 0.949057	test: 0.962713

Epoch: 91
Loss: 0.13164618735505498
ROC train: 0.992586	val: 0.874901	test: 0.906070
PRC train: 0.997698	val: 0.943193	test: 0.956541

Epoch: 92
Loss: 0.14875618371951033
ROC train: 0.994701	val: 0.891068	test: 0.908687
PRC train: 0.998360	val: 0.955246	test: 0.959137

Epoch: 93
Loss: 0.1172825978790591
ROC train: 0.994379	val: 0.903127	test: 0.922946
PRC train: 0.998259	val: 0.959808	test: 0.963866

Epoch: 94
Loss: 0.13315890466978003
ROC train: 0.971591	val: 0.878744	test: 0.925170
PRC train: 0.989481	val: 0.950317	test: 0.968838

Epoch: 34
Loss: 0.22207493577237888
ROC train: 0.974774	val: 0.891068	test: 0.925563
PRC train: 0.990281	val: 0.953984	test: 0.966807

Epoch: 35
Loss: 0.21421547766206936
ROC train: 0.971277	val: 0.878611	test: 0.920199
PRC train: 0.988648	val: 0.949107	test: 0.963693

Epoch: 36
Loss: 0.21713010271478364
ROC train: 0.975275	val: 0.870925	test: 0.916667
PRC train: 0.990323	val: 0.943862	test: 0.960732

Epoch: 37
Loss: 0.21450633374453112
ROC train: 0.975848	val: 0.877418	test: 0.925955
PRC train: 0.991056	val: 0.949394	test: 0.969157

Epoch: 38
Loss: 0.20976427601774125
ROC train: 0.977085	val: 0.883117	test: 0.925824
PRC train: 0.991467	val: 0.948106	test: 0.965921

Epoch: 39
Loss: 0.20377150346566547
ROC train: 0.978483	val: 0.888948	test: 0.913919
PRC train: 0.992328	val: 0.952896	test: 0.962841

Epoch: 40
Loss: 0.21383750301561708
ROC train: 0.977726	val: 0.898092	test: 0.919937
PRC train: 0.992029	val: 0.957358	test: 0.964107

Epoch: 41
Loss: 0.21234340024854012
ROC train: 0.977522	val: 0.894779	test: 0.935113
PRC train: 0.991385	val: 0.958741	test: 0.973089

Epoch: 42
Loss: 0.18848688357642548
ROC train: 0.972857	val: 0.877418	test: 0.900576
PRC train: 0.990591	val: 0.948724	test: 0.956984

Epoch: 43
Loss: 0.20454003374072505
ROC train: 0.974029	val: 0.879141	test: 0.908032
PRC train: 0.991388	val: 0.950699	test: 0.960433

Epoch: 44
Loss: 0.18212996538927093
ROC train: 0.978379	val: 0.888550	test: 0.912480
PRC train: 0.993074	val: 0.953395	test: 0.964564

Epoch: 45
Loss: 0.1976646121746171
ROC train: 0.978445	val: 0.878876	test: 0.918760
PRC train: 0.992824	val: 0.947367	test: 0.966168

Epoch: 46
Loss: 0.20212677703529797
ROC train: 0.978585	val: 0.895441	test: 0.928441
PRC train: 0.992593	val: 0.956308	test: 0.968784

Epoch: 47
Loss: 0.1898102866099998
ROC train: 0.975142	val: 0.855288	test: 0.890502
PRC train: 0.991776	val: 0.931368	test: 0.934584

Epoch: 48
Loss: 0.1983866558699417
ROC train: 0.983123	val: 0.888550	test: 0.922030
PRC train: 0.994513	val: 0.953561	test: 0.962269

Epoch: 49
Loss: 0.19169257498037262
ROC train: 0.977856	val: 0.897694	test: 0.925824
PRC train: 0.992432	val: 0.962613	test: 0.969047

Epoch: 50
Loss: 0.18371927220818882
ROC train: 0.979567	val: 0.881659	test: 0.924123
PRC train: 0.992853	val: 0.950663	test: 0.965966

Epoch: 51
Loss: 0.17217683411458595
ROC train: 0.983072	val: 0.890008	test: 0.927525
PRC train: 0.994362	val: 0.949744	test: 0.962077

Epoch: 52
Loss: 0.17655730980562515
ROC train: 0.981542	val: 0.874371	test: 0.906332
PRC train: 0.994024	val: 0.940704	test: 0.950509

Epoch: 53
Loss: 0.17739525644406998
ROC train: 0.983970	val: 0.892393	test: 0.922030
PRC train: 0.994359	val: 0.952533	test: 0.965376

Epoch: 54
Loss: 0.18390467186757622
ROC train: 0.984629	val: 0.895706	test: 0.909733
PRC train: 0.994895	val: 0.955071	test: 0.960411

Epoch: 55
Loss: 0.19373870824335993
ROC train: 0.986364	val: 0.879406	test: 0.906593
PRC train: 0.995577	val: 0.942543	test: 0.953548

Epoch: 56
Loss: 0.16459446225017674
ROC train: 0.987108	val: 0.872648	test: 0.903977
PRC train: 0.995705	val: 0.939425	test: 0.951304

Epoch: 57
Loss: 0.1637792144594072
ROC train: 0.985420	val: 0.895441	test: 0.910649
PRC train: 0.995085	val: 0.955757	test: 0.956388

Epoch: 58
Loss: 0.1729077415722491
ROC train: 0.987958	val: 0.891598	test: 0.912088
PRC train: 0.995997	val: 0.946975	test: 0.955581

Epoch: 59
Loss: 0.17171258825114352
ROC train: 0.988847	val: 0.900080	test: 0.919283
PRC train: 0.996489	val: 0.953076	test: 0.959706

Epoch: 60
Loss: 0.16814655457255806
ROC train: 0.988610	val: 0.901802	test: 0.923600
PRC train: 0.996342	val: 0.954475	test: 0.964136

Epoch: 61
Loss: 0.16662886169113136
ROC train: 0.989289	val: 0.898887	test: 0.906855
PRC train: 0.996385	val: 0.950994	test: 0.942947

Epoch: 62
Loss: 0.16407834296148488
ROC train: 0.986688	val: 0.889478	test: 0.903454
PRC train: 0.995506	val: 0.946104	test: 0.947476

Epoch: 63
Loss: 0.1565291119434345
ROC train: 0.989031	val: 0.893188	test: 0.912350
PRC train: 0.996402	val: 0.951286	test: 0.956389

Epoch: 64
Loss: 0.15405464550955658
ROC train: 0.988792	val: 0.898224	test: 0.910126
PRC train: 0.996348	val: 0.952044	test: 0.954850

Epoch: 65
Loss: 0.16954818104651542
ROC train: 0.990088	val: 0.904055	test: 0.912219
PRC train: 0.996776	val: 0.955436	test: 0.958235

Epoch: 66
Loss: 0.18245927739352824
ROC train: 0.990360	val: 0.900212	test: 0.909471
PRC train: 0.996936	val: 0.953024	test: 0.955985

Epoch: 67
Loss: 0.17715804403593224
ROC train: 0.989618	val: 0.884840	test: 0.911172
PRC train: 0.996793	val: 0.941828	test: 0.951644

Epoch: 68
Loss: 0.15785732659429702
ROC train: 0.988628	val: 0.906308	test: 0.908163
PRC train: 0.996380	val: 0.957352	test: 0.955094

Epoch: 69
Loss: 0.1719105639579493
ROC train: 0.990405	val: 0.899019	test: 0.911565
PRC train: 0.996958	val: 0.950861	test: 0.951010

Epoch: 70
Loss: 0.15321257148966955
ROC train: 0.989560	val: 0.896634	test: 0.913265
PRC train: 0.996699	val: 0.952735	test: 0.955942

Epoch: 71
Loss: 0.164381881961601
ROC train: 0.988651	val: 0.881394	test: 0.896389
PRC train: 0.996446	val: 0.945591	test: 0.952002

Epoch: 72
Loss: 0.14172195109466817
ROC train: 0.991036	val: 0.902465	test: 0.909210
PRC train: 0.997168	val: 0.958274	test: 0.956013

Epoch: 73
Loss: 0.15637534150700177
ROC train: 0.989536	val: 0.902332	test: 0.915751
PRC train: 0.996672	val: 0.958440	test: 0.958673

Epoch: 74
Loss: 0.14530441862313
ROC train: 0.993208	val: 0.883647	test: 0.899660
PRC train: 0.997925	val: 0.945300	test: 0.953189

Epoch: 75
Loss: 0.14477313160790595
ROC train: 0.993516	val: 0.887623	test: 0.909733
PRC train: 0.998017	val: 0.947170	test: 0.954889

Epoch: 76
Loss: 0.14014588380890736
ROC train: 0.989349	val: 0.911211	test: 0.921638
PRC train: 0.996370	val: 0.961294	test: 0.965566

Epoch: 77
Loss: 0.1611260387543764
ROC train: 0.989699	val: 0.905115	test: 0.909995
PRC train: 0.996596	val: 0.960830	test: 0.957877

Epoch: 78
Loss: 0.16369739857224053
ROC train: 0.992162	val: 0.876623	test: 0.894819
PRC train: 0.997581	val: 0.943106	test: 0.950263

Epoch: 79
Loss: 0.16025763601156504
ROC train: 0.992427	val: 0.893586	test: 0.910387
PRC train: 0.997607	val: 0.954159	test: 0.959343

Epoch: 80
Loss: 0.15911317419887597
ROC train: 0.991537	val: 0.904850	test: 0.900183
PRC train: 0.997205	val: 0.957351	test: 0.951448

Epoch: 81
Loss: 0.14782250504369823
ROC train: 0.992128	val: 0.884575	test: 0.897828
PRC train: 0.997517	val: 0.946146	test: 0.955939

Epoch: 82
Loss: 0.1480800004525267
ROC train: 0.988182	val: 0.882719	test: 0.905024
PRC train: 0.996242	val: 0.948612	test: 0.958701

Epoch: 83
Loss: 0.144539566758526
ROC train: 0.993665	val: 0.904718	test: 0.906070
PRC train: 0.998011	val: 0.949536	test: 0.958063

Epoch: 84
Loss: 0.138750579377934
ROC train: 0.992176	val: 0.876623	test: 0.890764
PRC train: 0.997584	val: 0.934532	test: 0.946828

Epoch: 85
Loss: 0.1452811064412242
ROC train: 0.989022	val: 0.897429	test: 0.916143
PRC train: 0.996544	val: 0.953367	test: 0.961013

Epoch: 86
Loss: 0.13712781680283317
ROC train: 0.992722	val: 0.896634	test: 0.916667
PRC train: 0.997683	val: 0.951373	test: 0.959744

Epoch: 87
Loss: 0.14005256334556668
ROC train: 0.993271	val: 0.877418	test: 0.904369
PRC train: 0.997893	val: 0.940689	test: 0.953348

Epoch: 88
Loss: 0.15725319726016712
ROC train: 0.994661	val: 0.900875	test: 0.911565
PRC train: 0.998361	val: 0.955826	test: 0.958936

Epoch: 89
Loss: 0.14268567536662508
ROC train: 0.994159	val: 0.896766	test: 0.903715
PRC train: 0.998168	val: 0.956297	test: 0.956339

Epoch: 90
Loss: 0.12235538446429207
ROC train: 0.994183	val: 0.901670	test: 0.905547
PRC train: 0.998166	val: 0.954316	test: 0.951122

Epoch: 91
Loss: 0.1388220349507344
ROC train: 0.995987	val: 0.912404	test: 0.910256
PRC train: 0.998770	val: 0.959170	test: 0.953048

Epoch: 92
Loss: 0.12716491069768843
ROC train: 0.991187	val: 0.900742	test: 0.900183
PRC train: 0.997218	val: 0.953004	test: 0.948667

Epoch: 93
Loss: 0.12949161685414132
ROC train: 0.993574	val: 0.890008	test: 0.914050
PRC train: 0.997984	val: 0.950442	test: 0.958872

Epoch: 94
Loss: 0.14045124548566432
ROC train: 0.996534	val: 0.885443	test: 0.871350
PRC train: 0.998923	val: 0.951642	test: 0.944001

Epoch: 95
Loss: 0.11534638223196451
ROC train: 0.998258	val: 0.894875	test: 0.900879
PRC train: 0.999459	val: 0.953424	test: 0.960840

Epoch: 96
Loss: 0.10338422646840147
ROC train: 0.997878	val: 0.895333	test: 0.901175
PRC train: 0.999335	val: 0.949550	test: 0.959048

Epoch: 97
Loss: 0.11162178709699347
ROC train: 0.997485	val: 0.884763	test: 0.892152
PRC train: 0.999216	val: 0.942792	test: 0.956095

Epoch: 98
Loss: 0.10935308823571266
ROC train: 0.998407	val: 0.885919	test: 0.891200
PRC train: 0.999493	val: 0.945588	test: 0.958251

Epoch: 99
Loss: 0.1108812847935002
ROC train: 0.995656	val: 0.892105	test: 0.876632
PRC train: 0.998619	val: 0.955061	test: 0.951572

Epoch: 100
Loss: 0.09763465148432363
ROC train: 0.995548	val: 0.890813	test: 0.882473
PRC train: 0.998570	val: 0.954752	test: 0.953435

Epoch: 101
Loss: 0.10722352546922494
ROC train: 0.997753	val: 0.894722	test: 0.903701
PRC train: 0.999287	val: 0.954784	test: 0.962674

Epoch: 102
Loss: 0.10614998082308194
ROC train: 0.997647	val: 0.889895	test: 0.892906
PRC train: 0.999232	val: 0.951375	test: 0.955617

Epoch: 103
Loss: 0.10041200696853236
ROC train: 0.998070	val: 0.890507	test: 0.884376
PRC train: 0.999381	val: 0.953669	test: 0.952027

Epoch: 104
Loss: 0.10061139325115187
ROC train: 0.998842	val: 0.893804	test: 0.881620
PRC train: 0.999627	val: 0.956210	test: 0.950831

Epoch: 105
Loss: 0.09930192547107333
ROC train: 0.998589	val: 0.898698	test: 0.888083
PRC train: 0.999566	val: 0.957789	test: 0.955428

Epoch: 106
Loss: 0.09080201368849745
ROC train: 0.998893	val: 0.890609	test: 0.895958
PRC train: 0.999639	val: 0.951206	test: 0.959989

Epoch: 107
Loss: 0.09617875197883144
ROC train: 0.998241	val: 0.887040	test: 0.891397
PRC train: 0.999414	val: 0.950186	test: 0.957695

Epoch: 108
Loss: 0.07853979882619475
ROC train: 0.998591	val: 0.888944	test: 0.888608
PRC train: 0.999540	val: 0.952372	test: 0.955659

Epoch: 109
Loss: 0.09955255699795273
ROC train: 0.998639	val: 0.884559	test: 0.883063
PRC train: 0.999571	val: 0.950716	test: 0.952114

Epoch: 110
Loss: 0.07771686867375563
ROC train: 0.996445	val: 0.878475	test: 0.870169
PRC train: 0.998913	val: 0.945509	test: 0.944844

Epoch: 111
Loss: 0.08149296123372433
ROC train: 0.998530	val: 0.889793	test: 0.881127
PRC train: 0.999534	val: 0.949569	test: 0.950190

Epoch: 112
Loss: 0.10334524123702574
ROC train: 0.998827	val: 0.897917	test: 0.895466
PRC train: 0.999634	val: 0.955949	test: 0.956296

Epoch: 113
Loss: 0.09685113282952593
ROC train: 0.998997	val: 0.894110	test: 0.897500
PRC train: 0.999682	val: 0.951907	test: 0.956914

Epoch: 114
Loss: 0.09236154942547767
ROC train: 0.998660	val: 0.888128	test: 0.895334
PRC train: 0.999574	val: 0.946271	test: 0.954905

Epoch: 115
Loss: 0.07874194887617704
ROC train: 0.998042	val: 0.889046	test: 0.898222
PRC train: 0.999406	val: 0.948687	test: 0.957927

Epoch: 116
Loss: 0.08489526225588898
ROC train: 0.998952	val: 0.884933	test: 0.885655
PRC train: 0.999675	val: 0.949831	test: 0.954257

Epoch: 117
Loss: 0.08514005265587798
ROC train: 0.999034	val: 0.893804	test: 0.881620
PRC train: 0.999701	val: 0.954107	test: 0.951621

Epoch: 118
Loss: 0.08123428785635899
ROC train: 0.999397	val: 0.891204	test: 0.889297
PRC train: 0.999812	val: 0.953375	test: 0.955409

Epoch: 119
Loss: 0.08186630955980422
ROC train: 0.999267	val: 0.887873	test: 0.901634
PRC train: 0.999773	val: 0.952309	test: 0.961021

Epoch: 120
Loss: 0.0737145440540616
ROC train: 0.999207	val: 0.891119	test: 0.898583
PRC train: 0.999753	val: 0.951201	test: 0.957146

Early stopping
Best (ROC):	 train: 0.957079	val: 0.907739	test: 0.892709
Best (PRC):	 train: 0.985356	val: 0.955140	test: 0.957675

ROC train: 0.997199	val: 0.883760	test: 0.877059
PRC train: 0.999105	val: 0.941793	test: 0.951584

Epoch: 95
Loss: 0.12218083484953945
ROC train: 0.998120	val: 0.890881	test: 0.879126
PRC train: 0.999401	val: 0.947038	test: 0.953043

Epoch: 96
Loss: 0.10236307082799356
ROC train: 0.998379	val: 0.895707	test: 0.880012
PRC train: 0.999494	val: 0.951634	test: 0.952811

Epoch: 97
Loss: 0.10473585515881405
ROC train: 0.997480	val: 0.891901	test: 0.884048
PRC train: 0.999217	val: 0.947993	test: 0.955343

Epoch: 98
Loss: 0.10517629704694373
ROC train: 0.998072	val: 0.896183	test: 0.883391
PRC train: 0.999379	val: 0.951424	test: 0.955343

Epoch: 99
Loss: 0.10839893841390813
ROC train: 0.998528	val: 0.890167	test: 0.873909
PRC train: 0.999527	val: 0.948818	test: 0.949297

Epoch: 100
Loss: 0.09553022869455967
ROC train: 0.996918	val: 0.885103	test: 0.874434
PRC train: 0.999034	val: 0.946620	test: 0.949895

Epoch: 101
Loss: 0.09285007193321523
ROC train: 0.998667	val: 0.887380	test: 0.871284
PRC train: 0.999583	val: 0.946500	test: 0.945413

Epoch: 102
Loss: 0.09864133993217906
ROC train: 0.995084	val: 0.885103	test: 0.846775
PRC train: 0.998467	val: 0.944915	test: 0.930240

Epoch: 103
Loss: 0.09688478387341373
ROC train: 0.998474	val: 0.889589	test: 0.883785
PRC train: 0.999524	val: 0.948241	test: 0.954690

Epoch: 104
Loss: 0.12764702368840092
ROC train: 0.998120	val: 0.886429	test: 0.876731
PRC train: 0.999408	val: 0.944111	test: 0.948674

Epoch: 105
Loss: 0.11429541280016717
ROC train: 0.998228	val: 0.891357	test: 0.882473
PRC train: 0.999443	val: 0.948313	test: 0.954143

Epoch: 106
Loss: 0.10641598734246058
ROC train: 0.996583	val: 0.892274	test: 0.889560
PRC train: 0.998923	val: 0.950587	test: 0.959179

Epoch: 107
Loss: 0.0971076670080123
ROC train: 0.998803	val: 0.890507	test: 0.887165
PRC train: 0.999630	val: 0.950393	test: 0.954198

Epoch: 108
Loss: 0.09345771820091205
ROC train: 0.998932	val: 0.893124	test: 0.879093
PRC train: 0.999668	val: 0.950430	test: 0.949007

Epoch: 109
Loss: 0.0779058195000347
ROC train: 0.998580	val: 0.888230	test: 0.878667
PRC train: 0.999560	val: 0.948343	test: 0.949569

Epoch: 110
Loss: 0.09379234921395738
ROC train: 0.998293	val: 0.882877	test: 0.880996
PRC train: 0.999469	val: 0.943643	test: 0.950708

Epoch: 111
Loss: 0.10204929916072585
ROC train: 0.998738	val: 0.883675	test: 0.875451
PRC train: 0.999609	val: 0.943463	test: 0.945853

Epoch: 112
Loss: 0.10080021543068378
ROC train: 0.997517	val: 0.888264	test: 0.889428
PRC train: 0.999221	val: 0.949162	test: 0.958340

Epoch: 113
Loss: 0.10221986468796156
ROC train: 0.998219	val: 0.893838	test: 0.877846
PRC train: 0.999434	val: 0.953607	test: 0.950166

Epoch: 114
Loss: 0.08752114754396835
ROC train: 0.996184	val: 0.885409	test: 0.855666
PRC train: 0.998803	val: 0.948716	test: 0.940798

Epoch: 115
Loss: 0.10142457023528668
ROC train: 0.998755	val: 0.892665	test: 0.889822
PRC train: 0.999614	val: 0.952771	test: 0.957897

Epoch: 116
Loss: 0.08143886819567532
ROC train: 0.992175	val: 0.886649	test: 0.897861
PRC train: 0.997521	val: 0.949151	test: 0.962290

Epoch: 117
Loss: 0.09440282922977092
ROC train: 0.996367	val: 0.884321	test: 0.888674
PRC train: 0.998875	val: 0.945543	test: 0.955805

Epoch: 118
Loss: 0.10488890785397835
ROC train: 0.999131	val: 0.890235	test: 0.879093
PRC train: 0.999731	val: 0.949695	test: 0.949588

Epoch: 119
Loss: 0.07347967403361498
ROC train: 0.997504	val: 0.879257	test: 0.844740
PRC train: 0.999238	val: 0.946437	test: 0.933226

Epoch: 120
Loss: 0.08868391681643457
ROC train: 0.995648	val: 0.875178	test: 0.840606
PRC train: 0.998682	val: 0.945675	test: 0.933010

Early stopping
Best (ROC):	 train: 0.970517	val: 0.905734	test: 0.893530
Best (PRC):	 train: 0.990318	val: 0.953244	test: 0.960621

ROC train: 0.998195	val: 0.867361	test: 0.892611
PRC train: 0.999445	val: 0.936807	test: 0.956391

Epoch: 95
Loss: 0.10190373288876615
ROC train: 0.997990	val: 0.866240	test: 0.889625
PRC train: 0.999375	val: 0.934025	test: 0.956310

Epoch: 96
Loss: 0.11424121165616064
ROC train: 0.997320	val: 0.868857	test: 0.899239
PRC train: 0.999168	val: 0.937271	test: 0.961588

Epoch: 97
Loss: 0.10399941837257747
ROC train: 0.997714	val: 0.871729	test: 0.897762
PRC train: 0.999301	val: 0.935589	test: 0.961565

Epoch: 98
Loss: 0.1065257458756621
ROC train: 0.997807	val: 0.868449	test: 0.898222
PRC train: 0.999331	val: 0.930287	test: 0.961543

Epoch: 99
Loss: 0.09947077239951967
ROC train: 0.998764	val: 0.877864	test: 0.895859
PRC train: 0.999623	val: 0.939894	test: 0.960511

Epoch: 100
Loss: 0.10315339499840226
ROC train: 0.998168	val: 0.872103	test: 0.887689
PRC train: 0.999438	val: 0.943107	test: 0.956908

Epoch: 101
Loss: 0.11850803435822999
ROC train: 0.997740	val: 0.871610	test: 0.888018
PRC train: 0.999261	val: 0.938288	test: 0.953850

Epoch: 102
Loss: 0.11102478415611125
ROC train: 0.997949	val: 0.867191	test: 0.893825
PRC train: 0.999348	val: 0.935926	test: 0.957095

Epoch: 103
Loss: 0.08583188465575396
ROC train: 0.997255	val: 0.863826	test: 0.883030
PRC train: 0.999144	val: 0.933513	test: 0.953250

Epoch: 104
Loss: 0.10412019706982356
ROC train: 0.998496	val: 0.872357	test: 0.883785
PRC train: 0.999529	val: 0.936348	test: 0.952665

Epoch: 105
Loss: 0.10308066646072538
ROC train: 0.998317	val: 0.873173	test: 0.874828
PRC train: 0.999479	val: 0.936552	test: 0.945940

Epoch: 106
Loss: 0.11061631223859685
ROC train: 0.998347	val: 0.874227	test: 0.873154
PRC train: 0.999481	val: 0.935435	test: 0.945080

Epoch: 107
Loss: 0.0865935143110972
ROC train: 0.997921	val: 0.869434	test: 0.879651
PRC train: 0.999342	val: 0.935997	test: 0.948811

Epoch: 108
Loss: 0.09852799301282941
ROC train: 0.998617	val: 0.866647	test: 0.883424
PRC train: 0.999573	val: 0.932611	test: 0.952405

Epoch: 109
Loss: 0.08432466304582542
ROC train: 0.998880	val: 0.872018	test: 0.882144
PRC train: 0.999651	val: 0.936741	test: 0.952348

Epoch: 110
Loss: 0.08749179650933356
ROC train: 0.999092	val: 0.874907	test: 0.872400
PRC train: 0.999720	val: 0.940362	test: 0.945832

Epoch: 111
Loss: 0.0916579701232761
ROC train: 0.999002	val: 0.870998	test: 0.876468
PRC train: 0.999696	val: 0.941381	test: 0.949097

Epoch: 112
Loss: 0.10288757582681193
ROC train: 0.998896	val: 0.868755	test: 0.891561
PRC train: 0.999660	val: 0.937094	test: 0.957652

Epoch: 113
Loss: 0.09612070875109777
ROC train: 0.998798	val: 0.870182	test: 0.896647
PRC train: 0.999632	val: 0.933833	test: 0.959295

Epoch: 114
Loss: 0.08791883123321811
ROC train: 0.998947	val: 0.875178	test: 0.904357
PRC train: 0.999678	val: 0.938812	test: 0.963166

Epoch: 115
Loss: 0.09348921855478122
ROC train: 0.999349	val: 0.877337	test: 0.896614
PRC train: 0.999800	val: 0.943450	test: 0.960881

Epoch: 116
Loss: 0.09537446227270305
ROC train: 0.996720	val: 0.872629	test: 0.896351
PRC train: 0.998955	val: 0.943265	test: 0.961982

Epoch: 117
Loss: 0.08844324269795537
ROC train: 0.998630	val: 0.871270	test: 0.901962
PRC train: 0.999579	val: 0.935950	test: 0.962611

Epoch: 118
Loss: 0.07926252366105699
ROC train: 0.999148	val: 0.876334	test: 0.893366
PRC train: 0.999741	val: 0.937494	test: 0.954175

Epoch: 119
Loss: 0.09461817903295908
ROC train: 0.998459	val: 0.870352	test: 0.876862
PRC train: 0.999524	val: 0.936267	test: 0.942913

Epoch: 120
Loss: 0.09094616456661242
ROC train: 0.997807	val: 0.851353	test: 0.871087
PRC train: 0.999335	val: 0.930145	test: 0.945018

Early stopping
Best (ROC):	 train: 0.960035	val: 0.912328	test: 0.884376
Best (PRC):	 train: 0.986965	val: 0.955646	test: 0.953924
All runs completed.

ROC train: 0.996650	val: 0.879507	test: 0.918083
PRC train: 0.998965	val: 0.947552	test: 0.970324

Epoch: 95
Loss: 0.11824623523364902
ROC train: 0.997296	val: 0.877942	test: 0.908981
PRC train: 0.999167	val: 0.946340	test: 0.965150

Epoch: 96
Loss: 0.12499533036070477
ROC train: 0.997828	val: 0.878029	test: 0.900181
PRC train: 0.999336	val: 0.943828	test: 0.960033

Epoch: 97
Loss: 0.13741141878347532
ROC train: 0.997496	val: 0.878464	test: 0.914105
PRC train: 0.999232	val: 0.946266	test: 0.970475

Epoch: 98
Loss: 0.11201919256655303
ROC train: 0.993024	val: 0.854986	test: 0.894696
PRC train: 0.997829	val: 0.934216	test: 0.960286

Epoch: 99
Loss: 0.13303201393979883
ROC train: 0.997176	val: 0.869188	test: 0.904039
PRC train: 0.999126	val: 0.936917	test: 0.963819

Epoch: 100
Loss: 0.11969489020416113
ROC train: 0.989666	val: 0.869594	test: 0.899578
PRC train: 0.996723	val: 0.941170	test: 0.964049

Epoch: 101
Loss: 0.10550497465703246
ROC train: 0.994361	val: 0.867391	test: 0.902652
PRC train: 0.998287	val: 0.939800	test: 0.966004

Epoch: 102
Loss: 0.12779505904223595
ROC train: 0.997904	val: 0.887739	test: 0.926944
PRC train: 0.999360	val: 0.950341	test: 0.975182

Epoch: 103
Loss: 0.10362526911856791
ROC train: 0.996283	val: 0.878986	test: 0.917480
PRC train: 0.998866	val: 0.945702	test: 0.970329

Epoch: 104
Loss: 0.10360126527630069
ROC train: 0.997502	val: 0.871913	test: 0.906932
PRC train: 0.999230	val: 0.938075	test: 0.964085

Epoch: 105
Loss: 0.12185916925338426
ROC train: 0.997939	val: 0.882638	test: 0.919289
PRC train: 0.999367	val: 0.944497	test: 0.970767

Epoch: 106
Loss: 0.11630993974552854
ROC train: 0.997384	val: 0.887623	test: 0.924111
PRC train: 0.999198	val: 0.948953	test: 0.972331

Epoch: 107
Loss: 0.12010479473859144
ROC train: 0.996559	val: 0.887449	test: 0.914708
PRC train: 0.998925	val: 0.952043	test: 0.964161

Epoch: 108
Loss: 0.10642430524931384
ROC train: 0.997843	val: 0.871855	test: 0.901989
PRC train: 0.999345	val: 0.944504	test: 0.960981

Epoch: 109
Loss: 0.10628179226492317
ROC train: 0.998436	val: 0.864841	test: 0.904099
PRC train: 0.999525	val: 0.938931	test: 0.963090

Epoch: 110
Loss: 0.10975520389990218
ROC train: 0.998417	val: 0.868551	test: 0.909705
PRC train: 0.999513	val: 0.936085	test: 0.964679

Epoch: 111
Loss: 0.11396038967043672
ROC train: 0.998573	val: 0.880348	test: 0.913442
PRC train: 0.999560	val: 0.945888	test: 0.968565

Epoch: 112
Loss: 0.09255206873821686
ROC train: 0.998340	val: 0.882754	test: 0.911573
PRC train: 0.999498	val: 0.946663	test: 0.965004

Epoch: 113
Loss: 0.10783912153358251
ROC train: 0.998937	val: 0.871043	test: 0.892405
PRC train: 0.999674	val: 0.944921	test: 0.955790

Epoch: 114
Loss: 0.10304602011140784
ROC train: 0.998795	val: 0.876899	test: 0.900422
PRC train: 0.999633	val: 0.947036	test: 0.962910

Epoch: 115
Loss: 0.10134359066782865
ROC train: 0.998608	val: 0.886348	test: 0.910970
PRC train: 0.999580	val: 0.948125	test: 0.968663

Epoch: 116
Loss: 0.10711425467573167
ROC train: 0.998110	val: 0.873710	test: 0.901206
PRC train: 0.999428	val: 0.941233	test: 0.963975

Epoch: 117
Loss: 0.09734560439337747
ROC train: 0.998949	val: 0.873072	test: 0.905365
PRC train: 0.999680	val: 0.943655	test: 0.964739

Epoch: 118
Loss: 0.09715400849981769
ROC train: 0.998633	val: 0.889710	test: 0.916034
PRC train: 0.999584	val: 0.953628	test: 0.970569

Epoch: 119
Loss: 0.10594689137640421
ROC train: 0.998754	val: 0.891739	test: 0.920072
PRC train: 0.999620	val: 0.953061	test: 0.973559

Epoch: 120
Loss: 0.10124373851623021
ROC train: 0.998737	val: 0.869826	test: 0.899458
PRC train: 0.999615	val: 0.940506	test: 0.961765

Early stopping
Best (ROC):	 train: 0.995742	val: 0.896812	test: 0.916034
Best (PRC):	 train: 0.998661	val: 0.958947	test: 0.966917

ROC train: 0.997570	val: 0.877420	test: 0.907294
PRC train: 0.999263	val: 0.942153	test: 0.965072

Epoch: 95
Loss: 0.11580266007430846
ROC train: 0.996479	val: 0.882928	test: 0.908620
PRC train: 0.998934	val: 0.946852	test: 0.964228

Epoch: 96
Loss: 0.11936771027530874
ROC train: 0.997728	val: 0.882174	test: 0.904943
PRC train: 0.999299	val: 0.949898	test: 0.965913

Epoch: 97
Loss: 0.10900923779463345
ROC train: 0.997684	val: 0.881942	test: 0.906992
PRC train: 0.999300	val: 0.944240	test: 0.964999

Epoch: 98
Loss: 0.10748711241410987
ROC train: 0.998241	val: 0.886580	test: 0.913020
PRC train: 0.999470	val: 0.948966	test: 0.967981

Epoch: 99
Loss: 0.11193433250037625
ROC train: 0.998714	val: 0.865942	test: 0.903978
PRC train: 0.999608	val: 0.936687	test: 0.960126

Epoch: 100
Loss: 0.10944930199770507
ROC train: 0.997557	val: 0.857942	test: 0.896383
PRC train: 0.999263	val: 0.935194	test: 0.958458

Epoch: 101
Loss: 0.1087654203767906
ROC train: 0.998463	val: 0.867913	test: 0.905124
PRC train: 0.999529	val: 0.939577	test: 0.962861

Epoch: 102
Loss: 0.09488900873470774
ROC train: 0.995971	val: 0.874087	test: 0.912538
PRC train: 0.998769	val: 0.941711	test: 0.969148

Epoch: 103
Loss: 0.10280774463389292
ROC train: 0.997259	val: 0.869246	test: 0.912297
PRC train: 0.999167	val: 0.940088	test: 0.967380

Epoch: 104
Loss: 0.1107153135775507
ROC train: 0.997836	val: 0.859536	test: 0.886558
PRC train: 0.999334	val: 0.936159	test: 0.954372

Epoch: 105
Loss: 0.09782065447890809
ROC train: 0.998540	val: 0.865130	test: 0.898433
PRC train: 0.999551	val: 0.938260	test: 0.959487

Epoch: 106
Loss: 0.11050635466705966
ROC train: 0.998739	val: 0.863391	test: 0.896926
PRC train: 0.999619	val: 0.939063	test: 0.956504

Epoch: 107
Loss: 0.10797115128785235
ROC train: 0.996716	val: 0.855681	test: 0.882278
PRC train: 0.999014	val: 0.937714	test: 0.951300

Epoch: 108
Loss: 0.10856302147718844
ROC train: 0.996392	val: 0.855072	test: 0.882459
PRC train: 0.998900	val: 0.933465	test: 0.946423

Epoch: 109
Loss: 0.11731332400434373
ROC train: 0.998255	val: 0.882899	test: 0.903978
PRC train: 0.999461	val: 0.948049	test: 0.960705

Epoch: 110
Loss: 0.10667338592780758
ROC train: 0.998611	val: 0.874754	test: 0.892646
PRC train: 0.999575	val: 0.944834	test: 0.953171

Epoch: 111
Loss: 0.10164868403236582
ROC train: 0.998184	val: 0.861826	test: 0.898373
PRC train: 0.999455	val: 0.934305	test: 0.955198

Epoch: 112
Loss: 0.10651435386479412
ROC train: 0.998485	val: 0.866899	test: 0.906510
PRC train: 0.999542	val: 0.936064	test: 0.962563

Epoch: 113
Loss: 0.10322322810191713
ROC train: 0.998484	val: 0.870000	test: 0.895178
PRC train: 0.999542	val: 0.939225	test: 0.959551

Epoch: 114
Loss: 0.12101306887090846
ROC train: 0.998136	val: 0.861652	test: 0.893068
PRC train: 0.999432	val: 0.932023	test: 0.953074

Epoch: 115
Loss: 0.10869874948139503
ROC train: 0.998728	val: 0.855043	test: 0.899940
PRC train: 0.999612	val: 0.928769	test: 0.957454

Epoch: 116
Loss: 0.09189988239141371
ROC train: 0.998760	val: 0.874290	test: 0.911091
PRC train: 0.999621	val: 0.937444	test: 0.965709

Epoch: 117
Loss: 0.12564793081211575
ROC train: 0.999132	val: 0.866812	test: 0.897167
PRC train: 0.999738	val: 0.939278	test: 0.959202

Epoch: 118
Loss: 0.07618348088213361
ROC train: 0.998365	val: 0.850812	test: 0.870524
PRC train: 0.999503	val: 0.933853	test: 0.945402

Epoch: 119
Loss: 0.0873602063911862
ROC train: 0.996658	val: 0.845623	test: 0.876793
PRC train: 0.998948	val: 0.928443	test: 0.946055

Epoch: 120
Loss: 0.10036864435457112
ROC train: 0.997617	val: 0.874609	test: 0.898674
PRC train: 0.999277	val: 0.943051	test: 0.961947

Early stopping
Best (ROC):	 train: 0.983282	val: 0.896203	test: 0.935142
Best (PRC):	 train: 0.994434	val: 0.949647	test: 0.977119

ROC train: 0.996601	val: 0.890803	test: 0.885531
PRC train: 0.998976	val: 0.958790	test: 0.948097

Epoch: 95
Loss: 0.1420791029031828
ROC train: 0.994603	val: 0.875828	test: 0.920330
PRC train: 0.998319	val: 0.948806	test: 0.966483

Epoch: 96
Loss: 0.15576499509714567
ROC train: 0.987995	val: 0.861384	test: 0.908948
PRC train: 0.995620	val: 0.936268	test: 0.964314

Epoch: 97
Loss: 0.14383916964256266
ROC train: 0.989513	val: 0.879141	test: 0.890502
PRC train: 0.996536	val: 0.952429	test: 0.955270

Epoch: 98
Loss: 0.1468960177415998
ROC train: 0.995807	val: 0.897164	test: 0.903323
PRC train: 0.998691	val: 0.959208	test: 0.957815

Epoch: 99
Loss: 0.13619565772840325
ROC train: 0.995823	val: 0.900875	test: 0.910387
PRC train: 0.998703	val: 0.962967	test: 0.962249

Epoch: 100
Loss: 0.12527470418156492
ROC train: 0.996609	val: 0.876491	test: 0.877551
PRC train: 0.998973	val: 0.948620	test: 0.946811

Epoch: 101
Loss: 0.12859324295570398
ROC train: 0.997838	val: 0.890803	test: 0.905939
PRC train: 0.999341	val: 0.955290	test: 0.962416

Epoch: 102
Loss: 0.10373923525887219
ROC train: 0.996383	val: 0.899019	test: 0.921900
PRC train: 0.998874	val: 0.960494	test: 0.968015

Epoch: 103
Loss: 0.12077464474617906
ROC train: 0.996071	val: 0.880732	test: 0.892726
PRC train: 0.998791	val: 0.951643	test: 0.952867

Epoch: 104
Loss: 0.11254625684794557
ROC train: 0.997369	val: 0.894514	test: 0.906463
PRC train: 0.999206	val: 0.958923	test: 0.962542

Epoch: 105
Loss: 0.11910458607464405
ROC train: 0.996606	val: 0.899549	test: 0.911172
PRC train: 0.998957	val: 0.963494	test: 0.964681

Epoch: 106
Loss: 0.12388114754922062
ROC train: 0.997792	val: 0.888550	test: 0.898090
PRC train: 0.999326	val: 0.955839	test: 0.959491

Epoch: 107
Loss: 0.11887005861591073
ROC train: 0.997604	val: 0.889743	test: 0.884223
PRC train: 0.999270	val: 0.956317	test: 0.950964

Epoch: 108
Loss: 0.12051685147873507
ROC train: 0.997030	val: 0.898622	test: 0.915228
PRC train: 0.999071	val: 0.960975	test: 0.963568

Epoch: 109
Loss: 0.11822814225577793
ROC train: 0.996483	val: 0.903658	test: 0.917059
PRC train: 0.998923	val: 0.963548	test: 0.964756

Epoch: 110
Loss: 0.10952533493844867
ROC train: 0.997906	val: 0.903127	test: 0.902407
PRC train: 0.999367	val: 0.961852	test: 0.959476

Epoch: 111
Loss: 0.11552304830076224
ROC train: 0.997104	val: 0.902995	test: 0.909864
PRC train: 0.999122	val: 0.964153	test: 0.962669

Epoch: 112
Loss: 0.12076725174993282
ROC train: 0.998040	val: 0.906706	test: 0.920853
PRC train: 0.999406	val: 0.965677	test: 0.969135

Epoch: 113
Loss: 0.13414110997069395
ROC train: 0.998344	val: 0.905645	test: 0.907640
PRC train: 0.999501	val: 0.962507	test: 0.959650

Epoch: 114
Loss: 0.09860910286018142
ROC train: 0.996888	val: 0.902200	test: 0.903323
PRC train: 0.999048	val: 0.962952	test: 0.957931

Epoch: 115
Loss: 0.11796739925193798
ROC train: 0.997606	val: 0.907633	test: 0.916143
PRC train: 0.999248	val: 0.964841	test: 0.961315

Epoch: 116
Loss: 0.10769257699592859
ROC train: 0.997449	val: 0.903127	test: 0.907117
PRC train: 0.999223	val: 0.962335	test: 0.958158

Epoch: 117
Loss: 0.11362564712967545
ROC train: 0.998031	val: 0.904055	test: 0.898090
PRC train: 0.999403	val: 0.964167	test: 0.956780

Epoch: 118
Loss: 0.09357003718105449
ROC train: 0.997672	val: 0.910416	test: 0.909864
PRC train: 0.999278	val: 0.963723	test: 0.960435

Epoch: 119
Loss: 0.10406364202756554
ROC train: 0.998418	val: 0.899549	test: 0.905939
PRC train: 0.999513	val: 0.958150	test: 0.955180

Epoch: 120
Loss: 0.09055145316661027
ROC train: 0.997948	val: 0.888020	test: 0.894819
PRC train: 0.999387	val: 0.956039	test: 0.951701

Early stopping
Best (ROC):	 train: 0.987634	val: 0.916512	test: 0.927263
Best (PRC):	 train: 0.995935	val: 0.971663	test: 0.973472

ROC train: 0.994925	val: 0.902465	test: 0.907378
PRC train: 0.998423	val: 0.958316	test: 0.958667

Epoch: 95
Loss: 0.13906758158222884
ROC train: 0.994775	val: 0.906043	test: 0.902407
PRC train: 0.998322	val: 0.964780	test: 0.958139

Epoch: 96
Loss: 0.13618210592313093
ROC train: 0.994260	val: 0.904850	test: 0.903585
PRC train: 0.998211	val: 0.963577	test: 0.959950

Epoch: 97
Loss: 0.13354756298941386
ROC train: 0.995294	val: 0.896766	test: 0.912480
PRC train: 0.998551	val: 0.956080	test: 0.960321

Epoch: 98
Loss: 0.14822880011093106
ROC train: 0.995692	val: 0.896369	test: 0.906855
PRC train: 0.998647	val: 0.958648	test: 0.955879

Epoch: 99
Loss: 0.1525588825937346
ROC train: 0.994662	val: 0.891068	test: 0.890241
PRC train: 0.998347	val: 0.955170	test: 0.951164

Epoch: 100
Loss: 0.1454650004859634
ROC train: 0.995809	val: 0.895309	test: 0.900968
PRC train: 0.998715	val: 0.955296	test: 0.956271

Epoch: 101
Loss: 0.13435799343003013
ROC train: 0.996227	val: 0.891466	test: 0.905939
PRC train: 0.998837	val: 0.953498	test: 0.958733

Epoch: 102
Loss: 0.13667535389135912
ROC train: 0.996311	val: 0.898092	test: 0.902930
PRC train: 0.998849	val: 0.960193	test: 0.955347

Epoch: 103
Loss: 0.13044395794957397
ROC train: 0.996186	val: 0.899284	test: 0.895997
PRC train: 0.998833	val: 0.959060	test: 0.956630

Epoch: 104
Loss: 0.1175680310411723
ROC train: 0.995891	val: 0.881527	test: 0.885008
PRC train: 0.998711	val: 0.948530	test: 0.944885

Epoch: 105
Loss: 0.12588938034492508
ROC train: 0.996125	val: 0.900212	test: 0.901099
PRC train: 0.998719	val: 0.959136	test: 0.960721

Epoch: 106
Loss: 0.12845987656827096
ROC train: 0.995491	val: 0.901405	test: 0.901753
PRC train: 0.998474	val: 0.959427	test: 0.953682

Epoch: 107
Loss: 0.1254542647783613
ROC train: 0.996081	val: 0.887888	test: 0.901361
PRC train: 0.998785	val: 0.951700	test: 0.958700

Epoch: 108
Loss: 0.12584198513751246
ROC train: 0.996342	val: 0.908031	test: 0.918760
PRC train: 0.998855	val: 0.965858	test: 0.970004

Epoch: 109
Loss: 0.1298872901986482
ROC train: 0.996197	val: 0.904320	test: 0.933281
PRC train: 0.998838	val: 0.956939	test: 0.975383

Epoch: 110
Loss: 0.11156437808796094
ROC train: 0.997553	val: 0.910284	test: 0.917582
PRC train: 0.999237	val: 0.964566	test: 0.967019

Epoch: 111
Loss: 0.11113487832254716
ROC train: 0.996576	val: 0.902067	test: 0.907902
PRC train: 0.998882	val: 0.959476	test: 0.960594

Epoch: 112
Loss: 0.11124403887441815
ROC train: 0.996993	val: 0.899814	test: 0.912873
PRC train: 0.999086	val: 0.959940	test: 0.959759

Epoch: 113
Loss: 0.1230309815320493
ROC train: 0.997839	val: 0.897562	test: 0.911695
PRC train: 0.999341	val: 0.960516	test: 0.965324

Epoch: 114
Loss: 0.10414663652296029
ROC train: 0.997964	val: 0.887358	test: 0.899137
PRC train: 0.999374	val: 0.952321	test: 0.954235

Epoch: 115
Loss: 0.12173559843004701
ROC train: 0.997529	val: 0.878479	test: 0.887363
PRC train: 0.999248	val: 0.947518	test: 0.950850

Epoch: 116
Loss: 0.13985710167136814
ROC train: 0.998084	val: 0.895441	test: 0.910911
PRC train: 0.999404	val: 0.958506	test: 0.963133

Epoch: 117
Loss: 0.1288301461018549
ROC train: 0.997886	val: 0.892923	test: 0.905808
PRC train: 0.999363	val: 0.957010	test: 0.963010

Epoch: 118
Loss: 0.11569980498505254
ROC train: 0.997861	val: 0.893719	test: 0.913919
PRC train: 0.999348	val: 0.954516	test: 0.965029

Epoch: 119
Loss: 0.10017435887873564
ROC train: 0.997659	val: 0.888418	test: 0.915751
PRC train: 0.999299	val: 0.954533	test: 0.967039

Epoch: 120
Loss: 0.09746543775109416
ROC train: 0.998244	val: 0.875298	test: 0.902669
PRC train: 0.999461	val: 0.948066	test: 0.960704

Early stopping
Best (ROC):	 train: 0.975280	val: 0.914789	test: 0.929356
Best (PRC):	 train: 0.991215	val: 0.968289	test: 0.972424

ROC train: 0.995710	val: 0.903260	test: 0.905678
PRC train: 0.998686	val: 0.959396	test: 0.957337

Epoch: 95
Loss: 0.12225190522042972
ROC train: 0.994381	val: 0.901405	test: 0.905416
PRC train: 0.998249	val: 0.954973	test: 0.949329

Epoch: 96
Loss: 0.13513710656196573
ROC train: 0.995283	val: 0.889345	test: 0.881214
PRC train: 0.998561	val: 0.947607	test: 0.941550

Epoch: 97
Loss: 0.1329691453428679
ROC train: 0.996513	val: 0.889345	test: 0.906332
PRC train: 0.998922	val: 0.946029	test: 0.945487

Epoch: 98
Loss: 0.1233448611418305
ROC train: 0.996325	val: 0.888153	test: 0.905678
PRC train: 0.998866	val: 0.951046	test: 0.949734

Epoch: 99
Loss: 0.11665045717352139
ROC train: 0.996466	val: 0.883779	test: 0.894950
PRC train: 0.998892	val: 0.951320	test: 0.947963

Epoch: 100
Loss: 0.12835147502047112
ROC train: 0.996837	val: 0.896236	test: 0.904762
PRC train: 0.999019	val: 0.955421	test: 0.951611

Epoch: 101
Loss: 0.12280600586930855
ROC train: 0.995564	val: 0.898357	test: 0.902276
PRC train: 0.998646	val: 0.954139	test: 0.945641

Epoch: 102
Loss: 0.1251906929383359
ROC train: 0.996772	val: 0.904055	test: 0.905939
PRC train: 0.999019	val: 0.959668	test: 0.952915

Epoch: 103
Loss: 0.12813837951074702
ROC train: 0.995961	val: 0.902995	test: 0.900314
PRC train: 0.998771	val: 0.957742	test: 0.949947

Epoch: 104
Loss: 0.11362420945270743
ROC train: 0.996855	val: 0.895839	test: 0.896782
PRC train: 0.999032	val: 0.950387	test: 0.948884

Epoch: 105
Loss: 0.12206457377036438
ROC train: 0.996599	val: 0.894779	test: 0.902930
PRC train: 0.998956	val: 0.951672	test: 0.951165

Epoch: 106
Loss: 0.12491063706492647
ROC train: 0.996595	val: 0.893984	test: 0.907117
PRC train: 0.998947	val: 0.953222	test: 0.954468

Epoch: 107
Loss: 0.12210449556234806
ROC train: 0.993607	val: 0.856348	test: 0.891811
PRC train: 0.998031	val: 0.936698	test: 0.951146

Epoch: 108
Loss: 0.10952705916622826
ROC train: 0.996783	val: 0.901802	test: 0.908948
PRC train: 0.999013	val: 0.957135	test: 0.956096

Epoch: 109
Loss: 0.12479705344518635
ROC train: 0.996839	val: 0.892791	test: 0.916013
PRC train: 0.999032	val: 0.949795	test: 0.958380

Epoch: 110
Loss: 0.12489831917159767
ROC train: 0.997467	val: 0.882057	test: 0.909341
PRC train: 0.999226	val: 0.945787	test: 0.954292

Epoch: 111
Loss: 0.11270651955453759
ROC train: 0.996964	val: 0.900610	test: 0.908948
PRC train: 0.999055	val: 0.956094	test: 0.955896

Epoch: 112
Loss: 0.12507644647873112
ROC train: 0.996555	val: 0.905380	test: 0.908425
PRC train: 0.998867	val: 0.957459	test: 0.948926

Epoch: 113
Loss: 0.11036321131000096
ROC train: 0.996701	val: 0.910019	test: 0.910780
PRC train: 0.998997	val: 0.958688	test: 0.955114

Epoch: 114
Loss: 0.1136711049926756
ROC train: 0.997957	val: 0.911079	test: 0.895735
PRC train: 0.999377	val: 0.959885	test: 0.947228

Epoch: 115
Loss: 0.11991122873121511
ROC train: 0.995220	val: 0.873973	test: 0.898613
PRC train: 0.998555	val: 0.944813	test: 0.952483

Epoch: 116
Loss: 0.10850652618981911
ROC train: 0.997675	val: 0.899814	test: 0.920591
PRC train: 0.999292	val: 0.957702	test: 0.957884

Epoch: 117
Loss: 0.11116775482122232
ROC train: 0.998076	val: 0.893056	test: 0.911957
PRC train: 0.999413	val: 0.953286	test: 0.953363

Epoch: 118
Loss: 0.12028854465043348
ROC train: 0.996292	val: 0.884972	test: 0.895474
PRC train: 0.998870	val: 0.950059	test: 0.949740

Epoch: 119
Loss: 0.11889570154194073
ROC train: 0.997219	val: 0.910681	test: 0.918498
PRC train: 0.999161	val: 0.965047	test: 0.959802

Epoch: 120
Loss: 0.10714920251730035
ROC train: 0.998104	val: 0.906971	test: 0.897305
PRC train: 0.999421	val: 0.962461	test: 0.944702

Epoch: 121
Loss: 0.10974172217772662
ROC train: 0.997792	val: 0.890273	test: 0.883700
PRC train: 0.999327	val: 0.956172	test: 0.946103

Epoch: 122
Loss: 0.12016526772703236
ROC train: 0.998294	val: 0.889213	test: 0.894165
PRC train: 0.999480	val: 0.949284	test: 0.952413

Epoch: 123
Loss: 0.11199205605469968
ROC train: 0.996484	val: 0.860588	test: 0.901361
PRC train: 0.998923	val: 0.932543	test: 0.957004

Epoch: 124
Loss: 0.11721775317639342
ROC train: 0.997857	val: 0.899947	test: 0.913658
PRC train: 0.999332	val: 0.954175	test: 0.960215

Epoch: 125
Loss: 0.10988858472089269
ROC train: 0.994897	val: 0.888418	test: 0.913134
PRC train: 0.998414	val: 0.947913	test: 0.959819

Epoch: 126
Loss: 0.09264858023448282
ROC train: 0.997891	val: 0.894646	test: 0.911303
PRC train: 0.999366	val: 0.950020	test: 0.956310

Early stopping
Best (ROC):	 train: 0.995987	val: 0.912404	test: 0.910256
Best (PRC):	 train: 0.998770	val: 0.959170	test: 0.953048
All runs completed.

ROC train: 0.995971	val: 0.884957	test: 0.916275
PRC train: 0.998786	val: 0.947773	test: 0.971928

Epoch: 95
Loss: 0.1301533563820359
ROC train: 0.996475	val: 0.878928	test: 0.910609
PRC train: 0.998927	val: 0.943156	test: 0.967744

Epoch: 96
Loss: 0.12129947043225575
ROC train: 0.996317	val: 0.873681	test: 0.908077
PRC train: 0.998858	val: 0.941155	test: 0.966160

Epoch: 97
Loss: 0.12642856208082073
ROC train: 0.992411	val: 0.861942	test: 0.894093
PRC train: 0.997606	val: 0.937683	test: 0.960147

Epoch: 98
Loss: 0.12984676941994186
ROC train: 0.997107	val: 0.875971	test: 0.909705
PRC train: 0.999109	val: 0.940676	test: 0.967598

Epoch: 99
Loss: 0.11138796944506135
ROC train: 0.996421	val: 0.882464	test: 0.920976
PRC train: 0.998917	val: 0.942235	test: 0.973508

Epoch: 100
Loss: 0.11764262004823968
ROC train: 0.996843	val: 0.889304	test: 0.923267
PRC train: 0.999043	val: 0.948321	test: 0.974432

Epoch: 101
Loss: 0.12193336893982838
ROC train: 0.997057	val: 0.870174	test: 0.910187
PRC train: 0.999117	val: 0.938086	test: 0.967092

Epoch: 102
Loss: 0.11418233152524299
ROC train: 0.997820	val: 0.872551	test: 0.904521
PRC train: 0.999344	val: 0.938198	test: 0.964592

Epoch: 103
Loss: 0.12416906959996081
ROC train: 0.998144	val: 0.877594	test: 0.906570
PRC train: 0.999441	val: 0.940666	test: 0.966229

Epoch: 104
Loss: 0.11356473930702306
ROC train: 0.997633	val: 0.871507	test: 0.905967
PRC train: 0.999281	val: 0.937820	test: 0.965882

Epoch: 105
Loss: 0.11102094290886899
ROC train: 0.996985	val: 0.867449	test: 0.899578
PRC train: 0.999074	val: 0.938951	test: 0.965055

Epoch: 106
Loss: 0.1169077664607185
ROC train: 0.996770	val: 0.874609	test: 0.908258
PRC train: 0.999000	val: 0.940137	test: 0.967491

Epoch: 107
Loss: 0.11031126942764963
ROC train: 0.998181	val: 0.879101	test: 0.918505
PRC train: 0.999446	val: 0.943171	test: 0.969942

Epoch: 108
Loss: 0.12374318163950086
ROC train: 0.997629	val: 0.876261	test: 0.912658
PRC train: 0.999285	val: 0.940834	test: 0.967633

Epoch: 109
Loss: 0.11380551488470785
ROC train: 0.997625	val: 0.848841	test: 0.898975
PRC train: 0.999287	val: 0.928545	test: 0.961092

Epoch: 110
Loss: 0.1135427086793452
ROC train: 0.996201	val: 0.857275	test: 0.894997
PRC train: 0.998850	val: 0.939998	test: 0.963488

Epoch: 111
Loss: 0.1137153648712058
ROC train: 0.997904	val: 0.891768	test: 0.907474
PRC train: 0.999362	val: 0.951586	test: 0.968045

Epoch: 112
Loss: 0.10846660754566066
ROC train: 0.998721	val: 0.892261	test: 0.905967
PRC train: 0.999611	val: 0.951471	test: 0.968286

Epoch: 113
Loss: 0.12891036517349844
ROC train: 0.995001	val: 0.862174	test: 0.890838
PRC train: 0.998475	val: 0.939750	test: 0.961195

Epoch: 114
Loss: 0.10303432272347542
ROC train: 0.998413	val: 0.863971	test: 0.904099
PRC train: 0.999519	val: 0.934334	test: 0.965344

Epoch: 115
Loss: 0.11156914686050083
ROC train: 0.997110	val: 0.865710	test: 0.895238
PRC train: 0.999133	val: 0.938984	test: 0.963944

Epoch: 116
Loss: 0.10862530669726866
ROC train: 0.996307	val: 0.882348	test: 0.900663
PRC train: 0.998880	val: 0.947605	test: 0.965919

Epoch: 117
Loss: 0.1000274396601271
ROC train: 0.996486	val: 0.900783	test: 0.908620
PRC train: 0.998936	val: 0.955468	test: 0.968594

Epoch: 118
Loss: 0.11626417804733108
ROC train: 0.998140	val: 0.882522	test: 0.903134
PRC train: 0.999441	val: 0.944758	test: 0.966719

Epoch: 119
Loss: 0.10419328193676507
ROC train: 0.996253	val: 0.864493	test: 0.901748
PRC train: 0.998897	val: 0.934179	test: 0.965919

Epoch: 120
Loss: 0.10940395868625308
ROC train: 0.997683	val: 0.901652	test: 0.920675
PRC train: 0.999293	val: 0.957699	test: 0.973561

Epoch: 121
Loss: 0.12086757303037227
ROC train: 0.998195	val: 0.887275	test: 0.900301
PRC train: 0.999447	val: 0.954955	test: 0.965947

Epoch: 122
Loss: 0.0986327691425824
ROC train: 0.998009	val: 0.872725	test: 0.884810
PRC train: 0.999385	val: 0.944225	test: 0.957599

Epoch: 123
Loss: 0.10514411961592014
ROC train: 0.998140	val: 0.884609	test: 0.916456
PRC train: 0.999445	val: 0.946708	test: 0.972281

Epoch: 124
Loss: 0.10018586047328469
ROC train: 0.997858	val: 0.887855	test: 0.926221
PRC train: 0.999346	val: 0.944915	test: 0.975690

Epoch: 125
Loss: 0.08709038157812421
ROC train: 0.998639	val: 0.877246	test: 0.914406
PRC train: 0.999582	val: 0.945259	test: 0.971734

Epoch: 126
Loss: 0.09937118343216568
ROC train: 0.998888	val: 0.877304	test: 0.907294
PRC train: 0.999657	val: 0.946899	test: 0.969227

Epoch: 127
Loss: 0.09774927623907141
ROC train: 0.998895	val: 0.882464	test: 0.909825
PRC train: 0.999664	val: 0.946362	test: 0.968677

Epoch: 128
Loss: 0.09252614561063562
ROC train: 0.998650	val: 0.878464	test: 0.912719
PRC train: 0.999589	val: 0.942151	test: 0.969141

Epoch: 129
Loss: 0.08089446357932968
ROC train: 0.998714	val: 0.872638	test: 0.908137
PRC train: 0.999609	val: 0.944604	test: 0.969298

Epoch: 130
Loss: 0.10177811688706691
ROC train: 0.998923	val: 0.878174	test: 0.914888
PRC train: 0.999667	val: 0.947107	test: 0.971621

Epoch: 131
Loss: 0.08697726186694056
ROC train: 0.999126	val: 0.880435	test: 0.902351
PRC train: 0.999734	val: 0.945479	test: 0.965714

Epoch: 132
Loss: 0.09338839069704201
ROC train: 0.999132	val: 0.885217	test: 0.898493
PRC train: 0.999731	val: 0.945716	test: 0.963995

Epoch: 133
Loss: 0.08860037020455531
ROC train: 0.999107	val: 0.888377	test: 0.907354
PRC train: 0.999726	val: 0.949141	test: 0.967517

Epoch: 134
Loss: 0.10428996868307366
ROC train: 0.999034	val: 0.874174	test: 0.910669
PRC train: 0.999707	val: 0.944762	test: 0.967280

Epoch: 135
Loss: 0.08750534663499127
ROC train: 0.998841	val: 0.875913	test: 0.903858
PRC train: 0.999646	val: 0.945400	test: 0.966101

Epoch: 136
Loss: 0.092706802689779
ROC train: 0.999221	val: 0.878464	test: 0.915431
PRC train: 0.999762	val: 0.945079	test: 0.970741

Epoch: 137
Loss: 0.09167658162980087
ROC train: 0.999008	val: 0.898986	test: 0.915973
PRC train: 0.999695	val: 0.953529	test: 0.970592

Epoch: 138
Loss: 0.08023759505256592
ROC train: 0.999427	val: 0.879652	test: 0.906510
PRC train: 0.999825	val: 0.943763	test: 0.967100

Epoch: 139
Loss: 0.08740668399553486
ROC train: 0.999225	val: 0.875739	test: 0.911814
PRC train: 0.999763	val: 0.944573	test: 0.968704

Epoch: 140
Loss: 0.09180123644880639
ROC train: 0.999480	val: 0.880957	test: 0.922966
PRC train: 0.999841	val: 0.945883	test: 0.972816

Epoch: 141
Loss: 0.07742252657526898
ROC train: 0.998266	val: 0.872841	test: 0.920434
PRC train: 0.999472	val: 0.939805	test: 0.971910

Epoch: 142
Loss: 0.09123344059349864
ROC train: 0.999049	val: 0.848783	test: 0.902471
PRC train: 0.999703	val: 0.930742	test: 0.964059

Epoch: 143
Loss: 0.07693997438425242
ROC train: 0.998981	val: 0.863333	test: 0.905907
PRC train: 0.999686	val: 0.934776	test: 0.966324

Epoch: 144
Loss: 0.0796897905439234
ROC train: 0.999466	val: 0.874290	test: 0.912899
PRC train: 0.999836	val: 0.939962	test: 0.970236

Epoch: 145
Loss: 0.08842591371167352
ROC train: 0.999438	val: 0.879507	test: 0.911754
PRC train: 0.999830	val: 0.938516	test: 0.970144

Epoch: 146
Loss: 0.08173428454677076
ROC train: 0.999463	val: 0.884029	test: 0.921760
PRC train: 0.999835	val: 0.946502	test: 0.973875

Epoch: 147
Loss: 0.09695837264293823
ROC train: 0.998877	val: 0.876899	test: 0.916516
PRC train: 0.999659	val: 0.946260	test: 0.972010

Epoch: 148
Loss: 0.0944386733226421
ROC train: 0.999123	val: 0.887217	test: 0.911634
PRC train: 0.999730	val: 0.948466	test: 0.968776

Epoch: 149
Loss: 0.08363698749916533
ROC train: 0.999273	val: 0.864029	test: 0.883605
PRC train: 0.999777	val: 0.940142	test: 0.959045

Epoch: 150
Loss: 0.08484670684723972
ROC train: 0.999430	val: 0.862696	test: 0.878843
PRC train: 0.999827	val: 0.937808	test: 0.953600

Epoch: 151
Loss: 0.08396555594608505
ROC train: 0.998996	val: 0.885101	test: 0.910127
PRC train: 0.999693	val: 0.944666	test: 0.966515

Epoch: 152
Loss: 0.09808561184741742
ROC train: 0.999329	val: 0.865942	test: 0.911513
PRC train: 0.999794	val: 0.936701	test: 0.968343

Epoch: 153
Loss: 0.08585393855775476
ROC train: 0.999470	val: 0.873449	test: 0.903797
PRC train: 0.999838	val: 0.936780	test: 0.963661

Epoch: 154
Loss: 0.09101141609565866
ROC train: 0.999510	val: 0.884087	test: 0.912357
PRC train: 0.999850	val: 0.941844	test: 0.969180

Epoch: 155
Loss: 0.08528797823136665
ROC train: 0.999154	val: 0.878406	test: 0.920193
PRC train: 0.999743	val: 0.943541	test: 0.974976

Early stopping
Best (ROC):	 train: 0.997683	val: 0.901652	test: 0.920675
Best (PRC):	 train: 0.999293	val: 0.957699	test: 0.973561
All runs completed.
