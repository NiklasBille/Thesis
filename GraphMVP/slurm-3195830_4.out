>>> Starting run for dataset: clintox
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running RANDOM configs_static_noise_experiments/GraphCL/clintox/noise=0.0.yml on cuda:0
Running RANDOM configs_static_noise_experiments/GraphCL/clintox/noise=0.05.yml on cuda:1
Running RANDOM configs_static_noise_experiments/GraphCL/clintox/noise=0.1.yml on cuda:2
Running RANDOM configs_static_noise_experiments/GraphCL/clintox/noise=0.2.yml on cuda:3
Starting process for seed 1: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.1.yml --runseed 1 --device cuda:2
Starting process for seed 2: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.1.yml --runseed 2 --device cuda:2
Starting process for seed 1: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.05.yml --runseed 1 --device cuda:1
Starting process for seed 3: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.1.yml --runseed 3 --device cuda:2
Starting process for seed 2: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.05.yml --runseed 2 --device cuda:1
Starting process for seed 3: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.05.yml --runseed 3 --device cuda:1
Starting process for seed 1: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.2.yml --runseed 1 --device cuda:3
Starting process for seed 2: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.2.yml --runseed 2 --device cuda:3
Starting process for seed 3: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.2.yml --runseed 3 --device cuda:3
Starting process for seed 1: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.0.yml --runseed 1 --device cuda:0
Starting process for seed 2: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.0.yml --runseed 2 --device cuda:0
Starting process for seed 3: python molecule_finetune.py --config /workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.0.yml --runseed 3 --device cuda:0
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: clintox
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/clintox/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/clintox/noise=0.0/clintox_scaff_3_20-05_14-43-32  ]
[ Using Seed :  3  ]
[ Using device :  cuda:0  ]
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.6452876614223053
ROC train: 0.558418	val: 0.664886	test: 0.424960
PRC train: 0.521849	val: 0.562355	test: 0.489732

Epoch: 2
Loss: 0.5666552210089848
ROC train: 0.620260	val: 0.699388	test: 0.463030
PRC train: 0.539020	val: 0.572612	test: 0.494475

Epoch: 3
Loss: 0.4999276015147357
ROC train: 0.674229	val: 0.765881	test: 0.508025
PRC train: 0.553221	val: 0.551072	test: 0.503421

Epoch: 4
Loss: 0.4485974692863298
ROC train: 0.704120	val: 0.788371	test: 0.527113
PRC train: 0.562329	val: 0.556600	test: 0.506356

Epoch: 5
Loss: 0.39776030804807777
ROC train: 0.738030	val: 0.836634	test: 0.549898
PRC train: 0.574332	val: 0.562893	test: 0.510805

Epoch: 6
Loss: 0.3620420477333461
ROC train: 0.777431	val: 0.865741	test: 0.585125
PRC train: 0.591639	val: 0.571626	test: 0.519724

Epoch: 7
Loss: 0.3291444755119176
ROC train: 0.809975	val: 0.868064	test: 0.615120
PRC train: 0.611234	val: 0.573309	test: 0.530043

Epoch: 8
Loss: 0.30527774274600394
ROC train: 0.827116	val: 0.855539	test: 0.628361
PRC train: 0.620534	val: 0.574415	test: 0.531769

Epoch: 9
Loss: 0.28049566040785656
ROC train: 0.840760	val: 0.874969	test: 0.670521
PRC train: 0.641293	val: 0.583815	test: 0.553057

Epoch: 10
Loss: 0.2614153430594771
ROC train: 0.859917	val: 0.876705	test: 0.609446
PRC train: 0.649780	val: 0.576299	test: 0.533380

Epoch: 11
Loss: 0.25477107499641305
ROC train: 0.874572	val: 0.900330	test: 0.637729
PRC train: 0.669306	val: 0.596898	test: 0.542833

Epoch: 12
Loss: 0.23050513718298835
ROC train: 0.889867	val: 0.907685	test: 0.649320
PRC train: 0.710728	val: 0.665301	test: 0.547004

Epoch: 13
Loss: 0.22730276141303393
ROC train: 0.903078	val: 0.903103	test: 0.661424
PRC train: 0.720272	val: 0.652960	test: 0.547705

Epoch: 14
Loss: 0.21862115019189043
ROC train: 0.910390	val: 0.898545	test: 0.681772
PRC train: 0.725496	val: 0.623826	test: 0.553305

Epoch: 15
Loss: 0.21392918563658941
ROC train: 0.914882	val: 0.904164	test: 0.671878
PRC train: 0.730837	val: 0.643124	test: 0.548910

Epoch: 16
Loss: 0.20248241751460566
ROC train: 0.922190	val: 0.861696	test: 0.675075
PRC train: 0.745663	val: 0.603614	test: 0.547953

Epoch: 17
Loss: 0.1999175415349672
ROC train: 0.921935	val: 0.819640	test: 0.691828
PRC train: 0.745789	val: 0.603643	test: 0.552089

Epoch: 18
Loss: 0.1906988182768201
ROC train: 0.926182	val: 0.809575	test: 0.695115
PRC train: 0.753046	val: 0.601906	test: 0.550203

Epoch: 19
Loss: 0.2094038129932271
ROC train: 0.932385	val: 0.789946	test: 0.673585
PRC train: 0.777074	val: 0.581181	test: 0.544156

Epoch: 20
Loss: 0.1835167833364234
ROC train: 0.938517	val: 0.806304	test: 0.713052
PRC train: 0.797312	val: 0.612508	test: 0.566162

Epoch: 21
Loss: 0.18396274739882304
ROC train: 0.939075	val: 0.808964	test: 0.716836
PRC train: 0.772431	val: 0.687356	test: 0.562629

Epoch: 22
Loss: 0.18969503689342837
ROC train: 0.931204	val: 0.787774	test: 0.722095
PRC train: 0.758925	val: 0.654925	test: 0.557682

Epoch: 23
Loss: 0.18968196289467348
ROC train: 0.936961	val: 0.878103	test: 0.755772
PRC train: 0.766867	val: 0.622148	test: 0.577422

Epoch: 24
Loss: 0.18144801062152519
ROC train: 0.940598	val: 0.894999	test: 0.758271
PRC train: 0.783677	val: 0.651297	test: 0.573624

Epoch: 25
Loss: 0.18081317761610238
ROC train: 0.948636	val: 0.865393	test: 0.753288
PRC train: 0.806879	val: 0.615004	test: 0.577669

Epoch: 26
Loss: 0.16491156115579444
ROC train: 0.944843	val: 0.814070	test: 0.764531
PRC train: 0.797385	val: 0.580139	test: 0.593230

Epoch: 27
Loss: 0.176545890876542
ROC train: 0.951379	val: 0.812710	test: 0.759747
PRC train: 0.808605	val: 0.577791	test: 0.571081

Epoch: 28
Loss: 0.1681339095824829
ROC train: 0.958442	val: 0.787598	test: 0.785643
PRC train: 0.828417	val: 0.597113	test: 0.585089

Epoch: 29
Loss: 0.15130338696167822
ROC train: 0.955945	val: 0.802695	test: 0.761345
PRC train: 0.818681	val: 0.620278	test: 0.570091

Epoch: 30
Loss: 0.16265934018835385
ROC train: 0.960752	val: 0.782952	test: 0.803255
PRC train: 0.842338	val: 0.615049	test: 0.623887

Epoch: 31
Loss: 0.16672602623810004
ROC train: 0.964452	val: 0.751175	test: 0.780870
PRC train: 0.845741	val: 0.582229	test: 0.581646

Epoch: 32
Loss: 0.1568591219111164
ROC train: 0.965521	val: 0.800597	test: 0.800289
PRC train: 0.847651	val: 0.621725	test: 0.584141

Epoch: 33
Loss: 0.16362023106619344Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: clintox
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/clintox/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/clintox/noise=0.0/clintox_scaff_1_20-05_14-43-32  ]
[ Using Seed :  1  ]
[ Using device :  cuda:0  ]
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.6492420128011533
ROC train: 0.602145	val: 0.656786	test: 0.448295
PRC train: 0.529963	val: 0.537691	test: 0.497425

Epoch: 2
Loss: 0.5619746119890617
ROC train: 0.643872	val: 0.717485	test: 0.469298
PRC train: 0.537512	val: 0.614587	test: 0.495904

Epoch: 3
Loss: 0.4918640045536673
ROC train: 0.680898	val: 0.768942	test: 0.513307
PRC train: 0.549970	val: 0.590735	test: 0.503189

Epoch: 4
Loss: 0.4401748840786759
ROC train: 0.730107	val: 0.784439	test: 0.541976
PRC train: 0.569439	val: 0.623299	test: 0.506868

Epoch: 5
Loss: 0.39007278519171074
ROC train: 0.781719	val: 0.821126	test: 0.572919
PRC train: 0.588625	val: 0.627868	test: 0.519050

Epoch: 6
Loss: 0.34853908815185547
ROC train: 0.774113	val: 0.829567	test: 0.570457
PRC train: 0.584302	val: 0.570698	test: 0.517603

Epoch: 7
Loss: 0.32126952787835683
ROC train: 0.816838	val: 0.850395	test: 0.580938
PRC train: 0.611449	val: 0.579313	test: 0.517747

Epoch: 8
Loss: 0.2944584512565747
ROC train: 0.839583	val: 0.861383	test: 0.606798
PRC train: 0.635411	val: 0.583565	test: 0.528316

Epoch: 9
Loss: 0.27091309277204806
ROC train: 0.843363	val: 0.834349	test: 0.584610
PRC train: 0.631467	val: 0.575933	test: 0.524318

Epoch: 10
Loss: 0.25809252487081913
ROC train: 0.860155	val: 0.846399	test: 0.616958
PRC train: 0.652664	val: 0.602872	test: 0.529563

Epoch: 11
Loss: 0.244248161009825
ROC train: 0.875261	val: 0.850145	test: 0.598139
PRC train: 0.670129	val: 0.614213	test: 0.523501

Epoch: 12
Loss: 0.23578498393461594
ROC train: 0.881329	val: 0.871673	test: 0.597583
PRC train: 0.672768	val: 0.596362	test: 0.522753

Epoch: 13
Loss: 0.2212093264733516
ROC train: 0.895612	val: 0.830990	test: 0.633879
PRC train: 0.706970	val: 0.631110	test: 0.535719

Epoch: 14
Loss: 0.2194129341473941
ROC train: 0.894054	val: 0.831078	test: 0.605035
PRC train: 0.680932	val: 0.599445	test: 0.529590

Epoch: 15
Loss: 0.20996901076784455
ROC train: 0.917123	val: 0.809438	test: 0.654841
PRC train: 0.755133	val: 0.636657	test: 0.539337

Epoch: 16
Loss: 0.2040078935531499
ROC train: 0.914721	val: 0.803120	test: 0.713489
PRC train: 0.761809	val: 0.622764	test: 0.549652

Epoch: 17
Loss: 0.1995211174736035
ROC train: 0.917541	val: 0.848610	test: 0.722383
PRC train: 0.743201	val: 0.624680	test: 0.552172

Epoch: 18
Loss: 0.1872416383575673
ROC train: 0.930894	val: 0.846600	test: 0.744275
PRC train: 0.775484	val: 0.648695	test: 0.565313

Epoch: 19
Loss: 0.1922787051104307
ROC train: 0.918327	val: 0.801510	test: 0.694536
PRC train: 0.764109	val: 0.659924	test: 0.541145

Epoch: 20
Loss: 0.18510657061653885
ROC train: 0.934861	val: 0.839856	test: 0.729678
PRC train: 0.774687	val: 0.622843	test: 0.553308

Epoch: 21
Loss: 0.18138274403802954
ROC train: 0.933171	val: 0.901704	test: 0.670230
PRC train: 0.783786	val: 0.638557	test: 0.539123

Epoch: 22
Loss: 0.18913999722712188
ROC train: 0.934519	val: 0.891752	test: 0.699011
PRC train: 0.779262	val: 0.630557	test: 0.546717

Epoch: 23
Loss: 0.1740346966303323
ROC train: 0.941945	val: 0.890852	test: 0.712820
PRC train: 0.789598	val: 0.645131	test: 0.553564

Epoch: 24
Loss: 0.16925489445848982
ROC train: 0.948023	val: 0.808827	test: 0.717486
PRC train: 0.812200	val: 0.626873	test: 0.552036

Epoch: 25
Loss: 0.16623536340250006
ROC train: 0.939782	val: 0.764911	test: 0.673801
PRC train: 0.783662	val: 0.578093	test: 0.539422

Epoch: 26
Loss: 0.16441476893538942
ROC train: 0.947587	val: 0.859212	test: 0.710516
PRC train: 0.807569	val: 0.603772	test: 0.549620

Epoch: 27
Loss: 0.17644540804033612
ROC train: 0.950750	val: 0.811262	test: 0.748503
PRC train: 0.813133	val: 0.606935	test: 0.556408

Epoch: 28
Loss: 0.1640451178037207
ROC train: 0.952218	val: 0.753884	test: 0.759272
PRC train: 0.817957	val: 0.609166	test: 0.562250

Epoch: 29
Loss: 0.17078663558140328
ROC train: 0.953437	val: 0.796215	test: 0.750438
PRC train: 0.827120	val: 0.614670	test: 0.553268

Epoch: 30
Loss: 0.16547414144308398
ROC train: 0.953361	val: 0.830041	test: 0.753837
PRC train: 0.826199	val: 0.623795	test: 0.557324

Epoch: 31
Loss: 0.16162812567068313
ROC train: 0.954956	val: 0.794004	test: 0.751637
PRC train: 0.834166	val: 0.622757	test: 0.557414

Epoch: 32
Loss: 0.15460068130881646
ROC train: 0.961011	val: 0.768780	test: 0.788464
PRC train: 0.845651	val: 0.632880	test: 0.576258

Epoch: 33
Loss: 0.15315235318515574Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.0.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:0
  input_data_dir: 
  dataset: clintox
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/clintox/noise=0.0
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/clintox/noise=0.0/clintox_scaff_2_20-05_14-43-32  ]
[ Using Seed :  2  ]
[ Using device :  cuda:0  ]
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.6852141968133106
ROC train: 0.509043	val: 0.518043	test: 0.384835
PRC train: 0.509915	val: 0.518287	test: 0.483766

Epoch: 2
Loss: 0.5958012944841421
ROC train: 0.600573	val: 0.622071	test: 0.459459
PRC train: 0.528758	val: 0.528635	test: 0.494952

Epoch: 3
Loss: 0.528895966757183
ROC train: 0.656404	val: 0.722053	test: 0.488202
PRC train: 0.542438	val: 0.543379	test: 0.499855

Epoch: 4
Loss: 0.4632050140232101
ROC train: 0.697380	val: 0.747053	test: 0.494186
PRC train: 0.557542	val: 0.550874	test: 0.503681

Epoch: 5
Loss: 0.4125963221159588
ROC train: 0.739860	val: 0.781529	test: 0.491463
PRC train: 0.570615	val: 0.556657	test: 0.505456

Epoch: 6
Loss: 0.3738722486840008
ROC train: 0.766117	val: 0.827244	test: 0.545927
PRC train: 0.579994	val: 0.563473	test: 0.519696

Epoch: 7
Loss: 0.3478808071521947
ROC train: 0.792834	val: 0.840106	test: 0.564988
PRC train: 0.593277	val: 0.562759	test: 0.523650

Epoch: 8
Loss: 0.31491289535815803
ROC train: 0.817235	val: 0.831728	test: 0.546932
PRC train: 0.618883	val: 0.565612	test: 0.517903

Epoch: 9
Loss: 0.2962569215078502
ROC train: 0.832816	val: 0.832814	test: 0.533410
PRC train: 0.641708	val: 0.556989	test: 0.515656

Epoch: 10
Loss: 0.2742936436966215
ROC train: 0.844233	val: 0.860796	test: 0.575783
PRC train: 0.639333	val: 0.567835	test: 0.525438

Epoch: 11
Loss: 0.25598680074022573
ROC train: 0.854772	val: 0.878940	test: 0.617219
PRC train: 0.658568	val: 0.655325	test: 0.534517

Epoch: 12
Loss: 0.24182028069316647
ROC train: 0.855477	val: 0.839469	test: 0.591165
PRC train: 0.653403	val: 0.568729	test: 0.528407

Epoch: 13
Loss: 0.24020122909635133
ROC train: 0.876106	val: 0.857163	test: 0.604594
PRC train: 0.688692	val: 0.642318	test: 0.539920

Epoch: 14
Loss: 0.2280153296180359
ROC train: 0.892480	val: 0.871511	test: 0.608172
PRC train: 0.720828	val: 0.587322	test: 0.541924

Epoch: 15
Loss: 0.22004002040222076
ROC train: 0.901421	val: 0.888206	test: 0.613133
PRC train: 0.723576	val: 0.588193	test: 0.533637

Epoch: 16
Loss: 0.21152084270451033
ROC train: 0.906612	val: 0.887419	test: 0.653694
PRC train: 0.719577	val: 0.648215	test: 0.540505

Epoch: 17
Loss: 0.20357520391193473
ROC train: 0.912376	val: 0.887669	test: 0.709245
PRC train: 0.735678	val: 0.640787	test: 0.566428

Epoch: 18
Loss: 0.2019483616298537
ROC train: 0.912723	val: 0.849421	test: 0.711516
PRC train: 0.723673	val: 0.642817	test: 0.560398

Epoch: 19
Loss: 0.19124242677550787
ROC train: 0.921772	val: 0.892838	test: 0.716040
PRC train: 0.729900	val: 0.596721	test: 0.596935

Epoch: 20
Loss: 0.18881818412288057
ROC train: 0.929552	val: 0.885683	test: 0.733827
PRC train: 0.757837	val: 0.591661	test: 0.572751

Epoch: 21
Loss: 0.18136172166518244
ROC train: 0.929275	val: 0.859212	test: 0.735926
PRC train: 0.765646	val: 0.626217	test: 0.553907

Epoch: 22
Loss: 0.1838091059134353
ROC train: 0.930887	val: 0.840531	test: 0.727970
PRC train: 0.766421	val: 0.639515	test: 0.558468

Epoch: 23
Loss: 0.19001144094137998
ROC train: 0.934479	val: 0.839895	test: 0.710474
PRC train: 0.783073	val: 0.592288	test: 0.548752

Epoch: 24
Loss: 0.1719432106751952
ROC train: 0.942347	val: 0.820852	test: 0.744701
PRC train: 0.812305	val: 0.564848	test: 0.560871

Epoch: 25
Loss: 0.17284895969655778
ROC train: 0.946739	val: 0.795853	test: 0.760497
PRC train: 0.816211	val: 0.557199	test: 0.568434

Epoch: 26
Loss: 0.16778463163977173
ROC train: 0.947835	val: 0.848747	test: 0.778673
PRC train: 0.813548	val: 0.567971	test: 0.578108

Epoch: 27
Loss: 0.17067451128131572
ROC train: 0.947971	val: 0.857901	test: 0.756138
PRC train: 0.816342	val: 0.580418	test: 0.558401

Epoch: 28
Loss: 0.16640990691465612
ROC train: 0.950971	val: 0.846325	test: 0.799785
PRC train: 0.813933	val: 0.597074	test: 0.581074

Epoch: 29
Loss: 0.1706937838256657
ROC train: 0.950969	val: 0.876156	test: 0.745444
PRC train: 0.815360	val: 0.601152	test: 0.559113

Epoch: 30
Loss: 0.17693596693572577
ROC train: 0.949228	val: 0.843054	test: 0.714139
PRC train: 0.804476	val: 0.558982	test: 0.548354

Epoch: 31
Loss: 0.1573394060900409
ROC train: 0.954063	val: 0.862683	test: 0.687043
PRC train: 0.828963	val: 0.607098	test: 0.551389

Epoch: 32
Loss: 0.16751711719656073
ROC train: 0.955009	val: 0.861671	test: 0.721546
PRC train: 0.835217	val: 0.618448	test: 0.551969

Epoch: 33
Loss: 0.16259193417708623Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: clintox
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/clintox/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/clintox/noise=0.05/clintox_scaff_3_20-05_14-43-32  ]
[ Using Seed :  3  ]
[ Using device :  cuda:1  ]
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.6369016399179499
ROC train: 0.573175	val: 0.551188	test: 0.526441
PRC train: 0.528757	val: 0.523341	test: 0.517082

Epoch: 2
Loss: 0.5582282801669655
ROC train: 0.622340	val: 0.649582	test: 0.482117
PRC train: 0.538909	val: 0.545144	test: 0.505249

Epoch: 3
Loss: 0.49507976432361156
ROC train: 0.672671	val: 0.718947	test: 0.477071
PRC train: 0.549545	val: 0.553266	test: 0.501605

Epoch: 4
Loss: 0.44451492664900927
ROC train: 0.723834	val: 0.802807	test: 0.531386
PRC train: 0.568124	val: 0.554996	test: 0.511242

Epoch: 5
Loss: 0.3966051304816654
ROC train: 0.775555	val: 0.827332	test: 0.553671
PRC train: 0.608051	val: 0.628315	test: 0.520007

Epoch: 6
Loss: 0.35368931696277567
ROC train: 0.780519	val: 0.790346	test: 0.538292
PRC train: 0.609870	val: 0.617878	test: 0.512283

Epoch: 7
Loss: 0.3186662267840869
ROC train: 0.828938	val: 0.853891	test: 0.572283
PRC train: 0.642387	val: 0.566896	test: 0.524681

Epoch: 8
Loss: 0.30291179955563774
ROC train: 0.846060	val: 0.848522	test: 0.558616
PRC train: 0.657473	val: 0.564458	test: 0.519676

Epoch: 9
Loss: 0.2690208799174284
ROC train: 0.864858	val: 0.801510	test: 0.564701
PRC train: 0.701470	val: 0.555229	test: 0.523776

Epoch: 10
Loss: 0.25046925382320057
ROC train: 0.891850	val: 0.803071	test: 0.580441
PRC train: 0.753680	val: 0.552422	test: 0.529086

Epoch: 11
Loss: 0.22851881637855892
ROC train: 0.904814	val: 0.779157	test: 0.593870
PRC train: 0.779889	val: 0.548118	test: 0.532929

Epoch: 12
Loss: 0.22174734369315519
ROC train: 0.927112	val: 0.780893	test: 0.620353
PRC train: 0.826028	val: 0.550724	test: 0.535059

Epoch: 13
Loss: 0.21524925707133075
ROC train: 0.942562	val: 0.816642	test: 0.654882
PRC train: 0.849589	val: 0.660434	test: 0.552607

Epoch: 14
Loss: 0.20904024021222384
ROC train: 0.927929	val: 0.852018	test: 0.640240
PRC train: 0.830973	val: 0.615977	test: 0.533740

Epoch: 15
Loss: 0.20233437834640253
ROC train: 0.942461	val: 0.731394	test: 0.628126
PRC train: 0.835834	val: 0.633051	test: 0.544064

Epoch: 16
Loss: 0.18849619940029594
ROC train: 0.952154	val: 0.769504	test: 0.659185
PRC train: 0.881308	val: 0.568425	test: 0.538708

Epoch: 17
Loss: 0.16841846326479845
ROC train: 0.954288	val: 0.736626	test: 0.645562
PRC train: 0.896187	val: 0.549645	test: 0.536222

Epoch: 18
Loss: 0.16578862341679726
ROC train: 0.959619	val: 0.635695	test: 0.679498
PRC train: 0.902665	val: 0.544851	test: 0.542818

Epoch: 19
Loss: 0.1623244082930336
ROC train: 0.961754	val: 0.600557	test: 0.688332
PRC train: 0.916160	val: 0.524835	test: 0.542367

Epoch: 20
Loss: 0.16375262845144845
ROC train: 0.973950	val: 0.677326	test: 0.687726
PRC train: 0.930968	val: 0.545179	test: 0.544647

Epoch: 21
Loss: 0.13779357015122706
ROC train: 0.973632	val: 0.720792	test: 0.706619
PRC train: 0.923364	val: 0.571544	test: 0.543438

Epoch: 22
Loss: 0.15787809968492209
ROC train: 0.977090	val: 0.766907	test: 0.688705
PRC train: 0.925300	val: 0.579611	test: 0.540233

Epoch: 23
Loss: 0.1389177952264155
ROC train: 0.981825	val: 0.701812	test: 0.723652
PRC train: 0.948067	val: 0.682395	test: 0.551720

Epoch: 24
Loss: 0.13376538588795858
ROC train: 0.987139	val: 0.676476	test: 0.756702
PRC train: 0.953617	val: 0.659728	test: 0.563195

Epoch: 25
Loss: 0.134954610391467
ROC train: 0.991746	val: 0.642986	test: 0.722931
PRC train: 0.967816	val: 0.603138	test: 0.551948

Epoch: 26
Loss: 0.1391075370553331
ROC train: 0.992182	val: 0.646645	test: 0.747820
PRC train: 0.967111	val: 0.582426	test: 0.560972

Epoch: 27
Loss: 0.11888005651711675
ROC train: 0.992836	val: 0.682882	test: 0.732453
PRC train: 0.968785	val: 0.654475	test: 0.560269

Epoch: 28
Loss: 0.10536898440696907
ROC train: 0.995309	val: 0.724376	test: 0.715823
PRC train: 0.979750	val: 0.597699	test: 0.554785

Epoch: 29
Loss: 0.10989826895934758
ROC train: 0.994949	val: 0.722204	test: 0.692003
PRC train: 0.978186	val: 0.579632	test: 0.547743

Epoch: 30
Loss: 0.11130771760194506
ROC train: 0.992598	val: 0.724190	test: 0.694838
PRC train: 0.970593	val: 0.556545	test: 0.552596

Epoch: 31
Loss: 0.09806735083520221
ROC train: 0.991526	val: 0.694721	test: 0.635276
PRC train: 0.965162	val: 0.542394	test: 0.549472

Epoch: 32
Loss: 0.10644606870847004
ROC train: 0.993654	val: 0.744343	test: 0.640796Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: clintox
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/clintox/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/clintox/noise=0.05/clintox_scaff_1_20-05_14-43-32  ]
[ Using Seed :  1  ]
[ Using device :  cuda:1  ]
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.6691363203295949
ROC train: 0.576306	val: 0.519220	test: 0.448646
PRC train: 0.529485	val: 0.510640	test: 0.493182

Epoch: 2
Loss: 0.5915608661775424
ROC train: 0.643655	val: 0.593431	test: 0.491280
PRC train: 0.544827	val: 0.515505	test: 0.496032

Epoch: 3
Loss: 0.5217049757499878
ROC train: 0.675429	val: 0.654864	test: 0.494295
PRC train: 0.555308	val: 0.522351	test: 0.496623

Epoch: 4
Loss: 0.45804441720023037
ROC train: 0.709932	val: 0.777534	test: 0.527561
PRC train: 0.568349	val: 0.544952	test: 0.501892

Epoch: 5
Loss: 0.41275619929039103
ROC train: 0.751540	val: 0.798000	test: 0.528248
PRC train: 0.590914	val: 0.548839	test: 0.502134

Epoch: 6
Loss: 0.3669684238169382
ROC train: 0.799312	val: 0.785764	test: 0.546686
PRC train: 0.639253	val: 0.544778	test: 0.507297

Epoch: 7
Loss: 0.33030770088999395
ROC train: 0.820811	val: 0.815145	test: 0.550507
PRC train: 0.649784	val: 0.561264	test: 0.524913

Epoch: 8
Loss: 0.29978307918353875
ROC train: 0.840169	val: 0.838570	test: 0.526411
PRC train: 0.660140	val: 0.574730	test: 0.514140

Epoch: 9
Loss: 0.28338146448432877
ROC train: 0.858793	val: 0.834711	test: 0.520602
PRC train: 0.694383	val: 0.587562	test: 0.507919

Epoch: 10
Loss: 0.2592516229461949
ROC train: 0.891684	val: 0.853392	test: 0.539215
PRC train: 0.735012	val: 0.635332	test: 0.516373

Epoch: 11
Loss: 0.23731538644001487
ROC train: 0.910828	val: 0.844277	test: 0.565724
PRC train: 0.762117	val: 0.638628	test: 0.520658

Epoch: 12
Loss: 0.22378015230767717
ROC train: 0.919059	val: 0.773201	test: 0.586186
PRC train: 0.778289	val: 0.552975	test: 0.527875

Epoch: 13
Loss: 0.2099983079053906
ROC train: 0.924009	val: 0.770404	test: 0.598565
PRC train: 0.797299	val: 0.552009	test: 0.526990

Epoch: 14
Loss: 0.20291355445585588
ROC train: 0.946208	val: 0.789622	test: 0.562139
PRC train: 0.844725	val: 0.624212	test: 0.515078

Epoch: 15
Loss: 0.19819853396691087
ROC train: 0.952574	val: 0.787788	test: 0.569034
PRC train: 0.862584	val: 0.623026	test: 0.515923

Epoch: 16
Loss: 0.17655665641679208
ROC train: 0.963252	val: 0.860111	test: 0.562624
PRC train: 0.882115	val: 0.583757	test: 0.516065

Epoch: 17
Loss: 0.1840587150588309
ROC train: 0.970145	val: 0.827258	test: 0.570001
PRC train: 0.894162	val: 0.657451	test: 0.513860

Epoch: 18
Loss: 0.16079419864837788
ROC train: 0.971766	val: 0.845201	test: 0.609831
PRC train: 0.898791	val: 0.646950	test: 0.525170

Epoch: 19
Loss: 0.15687263451621325
ROC train: 0.971652	val: 0.809389	test: 0.598229
PRC train: 0.910585	val: 0.630330	test: 0.516982

Epoch: 20
Loss: 0.15111446437468076
ROC train: 0.977167	val: 0.798063	test: 0.636508
PRC train: 0.925861	val: 0.602178	test: 0.527363

Epoch: 21
Loss: 0.14814262519045973
ROC train: 0.977368	val: 0.784527	test: 0.683909
PRC train: 0.928876	val: 0.622454	test: 0.541042

Epoch: 22
Loss: 0.13578439431958508
ROC train: 0.980154	val: 0.790859	test: 0.651617
PRC train: 0.941421	val: 0.689592	test: 0.533220

Epoch: 23
Loss: 0.1414473090057488
ROC train: 0.981570	val: 0.816969	test: 0.679061
PRC train: 0.942266	val: 0.569751	test: 0.539787

Epoch: 24
Loss: 0.13200389680518473
ROC train: 0.992027	val: 0.827869	test: 0.653616
PRC train: 0.964636	val: 0.569480	test: 0.534003

Epoch: 25
Loss: 0.12345793080250148
ROC train: 0.990636	val: 0.809301	test: 0.657807
PRC train: 0.958004	val: 0.583558	test: 0.532857

Epoch: 26
Loss: 0.1147083559626737
ROC train: 0.987992	val: 0.859574	test: 0.635078
PRC train: 0.952288	val: 0.598735	test: 0.528545

Epoch: 27
Loss: 0.11876669303960322
ROC train: 0.995311	val: 0.844090	test: 0.642784
PRC train: 0.978637	val: 0.617461	test: 0.532737

Epoch: 28
Loss: 0.11047200871059104
ROC train: 0.993978	val: 0.863544	test: 0.679341
PRC train: 0.973500	val: 0.643489	test: 0.541194

Epoch: 29
Loss: 0.1062637747753177
ROC train: 0.990301	val: 0.837498	test: 0.647968
PRC train: 0.968404	val: 0.659086	test: 0.533516

Epoch: 30
Loss: 0.10226175445496746
ROC train: 0.994220	val: 0.851220	test: 0.641248
PRC train: 0.977732	val: 0.678748	test: 0.530821

Epoch: 31
Loss: 0.09477328509800134
ROC train: 0.994710	val: 0.827806	test: 0.700998
PRC train: 0.976994	val: 0.646791	test: 0.545788

Epoch: 32
Loss: 0.09537308271428169
ROC train: 0.998052	val: 0.783167	test: 0.698436Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: clintox
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/clintox/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/clintox/noise=0.1/clintox_scaff_3_20-05_14-43-32  ]
[ Using Seed :  3  ]
[ Using device :  cuda:2  ]
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.6404732446703152
ROC train: 0.536779	val: 0.512202	test: 0.513367
PRC train: 0.521095	val: 0.533769	test: 0.513289

Epoch: 2
Loss: 0.5630603104380428
ROC train: 0.576928	val: 0.629654	test: 0.487175
PRC train: 0.530605	val: 0.542552	test: 0.508160

Epoch: 3
Loss: 0.5019668576251002
ROC train: 0.623832	val: 0.692999	test: 0.479331
PRC train: 0.544749	val: 0.548775	test: 0.505315

Epoch: 4
Loss: 0.452261537405788
ROC train: 0.693290	val: 0.798537	test: 0.501156
PRC train: 0.563757	val: 0.563985	test: 0.509061

Epoch: 5
Loss: 0.4000141012689987
ROC train: 0.755606	val: 0.818417	test: 0.500734
PRC train: 0.601762	val: 0.582007	test: 0.512684

Epoch: 6
Loss: 0.3610636653880689
ROC train: 0.771662	val: 0.785764	test: 0.507241
PRC train: 0.616614	val: 0.616821	test: 0.511341

Epoch: 7
Loss: 0.32806593266368445
ROC train: 0.813307	val: 0.806567	test: 0.536309
PRC train: 0.645230	val: 0.580963	test: 0.516487

Epoch: 8
Loss: 0.3074565467009693
ROC train: 0.841723	val: 0.819678	test: 0.568350
PRC train: 0.682827	val: 0.575166	test: 0.520758

Epoch: 9
Loss: 0.27782419342551073
ROC train: 0.865729	val: 0.832065	test: 0.581954
PRC train: 0.717934	val: 0.591469	test: 0.524297

Epoch: 10
Loss: 0.2590833399407773
ROC train: 0.882475	val: 0.850546	test: 0.599831
PRC train: 0.740401	val: 0.633085	test: 0.533276

Epoch: 11
Loss: 0.23680631167679644
ROC train: 0.887909	val: 0.839557	test: 0.621772
PRC train: 0.762968	val: 0.626658	test: 0.532753

Epoch: 12
Loss: 0.22449988318715297
ROC train: 0.916080	val: 0.778584	test: 0.634670
PRC train: 0.816295	val: 0.654544	test: 0.536305

Epoch: 13
Loss: 0.20690512390179797
ROC train: 0.936893	val: 0.770667	test: 0.648371
PRC train: 0.831663	val: 0.626582	test: 0.538544

Epoch: 14
Loss: 0.2040218965501388
ROC train: 0.945863	val: 0.728336	test: 0.626203
PRC train: 0.863403	val: 0.578223	test: 0.533619

Epoch: 15
Loss: 0.18653546550190514
ROC train: 0.961850	val: 0.709954	test: 0.670593
PRC train: 0.897115	val: 0.618144	test: 0.543136

Epoch: 16
Loss: 0.19424300862225435
ROC train: 0.966042	val: 0.726601	test: 0.655229
PRC train: 0.920897	val: 0.589630	test: 0.537181

Epoch: 17
Loss: 0.17421595523927777
ROC train: 0.972678	val: 0.739799	test: 0.644050
PRC train: 0.925218	val: 0.628807	test: 0.535534

Epoch: 18
Loss: 0.15618324323697275
ROC train: 0.984126	val: 0.753698	test: 0.653915
PRC train: 0.944508	val: 0.571292	test: 0.543337

Epoch: 19
Loss: 0.14809839073972103
ROC train: 0.980719	val: 0.673770	test: 0.710370
PRC train: 0.935852	val: 0.553746	test: 0.556608

Epoch: 20
Loss: 0.15096270462859715
ROC train: 0.983476	val: 0.609961	test: 0.677215
PRC train: 0.939058	val: 0.536920	test: 0.556616

Epoch: 21
Loss: 0.13908675232352244
ROC train: 0.987956	val: 0.577171	test: 0.662207
PRC train: 0.956077	val: 0.539859	test: 0.545498

Epoch: 22
Loss: 0.14444597555276845
ROC train: 0.991369	val: 0.638481	test: 0.647224
PRC train: 0.968420	val: 0.606946	test: 0.547211

Epoch: 23
Loss: 0.1190585486684653
ROC train: 0.992883	val: 0.608900	test: 0.659891
PRC train: 0.969709	val: 0.581455	test: 0.559397

Epoch: 24
Loss: 0.11702500310422144
ROC train: 0.994607	val: 0.536176	test: 0.676281
PRC train: 0.970605	val: 0.518705	test: 0.561510

Epoch: 25
Loss: 0.11202916447163198
ROC train: 0.996791	val: 0.670098	test: 0.700673
PRC train: 0.981683	val: 0.541947	test: 0.556938

Epoch: 26
Loss: 0.11537512352898602
ROC train: 0.996076	val: 0.706159	test: 0.668456
PRC train: 0.980047	val: 0.551885	test: 0.552058

Epoch: 27
Loss: 0.09489996025815611
ROC train: 0.997218	val: 0.724728	test: 0.638525
PRC train: 0.985683	val: 0.573064	test: 0.555263

Epoch: 28
Loss: 0.1047084203800543
ROC train: 0.996678	val: 0.618402	test: 0.635537
PRC train: 0.984292	val: 0.603258	test: 0.562818

Epoch: 29
Loss: 0.09907467031519995
ROC train: 0.998765	val: 0.631576	test: 0.650452
PRC train: 0.992964	val: 0.552002	test: 0.557816

Epoch: 30
Loss: 0.09678864974967002
ROC train: 0.998759	val: 0.636095	test: 0.689478
PRC train: 0.992493	val: 0.546046	test: 0.555987

Epoch: 31
Loss: 0.08134215162507803
ROC train: 0.999030	val: 0.651255	test: 0.689004
PRC train: 0.994159	val: 0.532928	test: 0.556223

Epoch: 32
Loss: 0.08679996418162612
ROC train: 0.999405	val: 0.638692	test: 0.697267Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: clintox
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/clintox/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/clintox/noise=0.1/clintox_scaff_2_20-05_14-43-32  ]
[ Using Seed :  2  ]
[ Using device :  cuda:2  ]
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.6319685821354746
ROC train: 0.555366	val: 0.619305	test: 0.469134
PRC train: 0.518408	val: 0.538216	test: 0.505144

Epoch: 2
Loss: 0.5578885351514742
ROC train: 0.599187	val: 0.622032	test: 0.445209
PRC train: 0.537106	val: 0.523429	test: 0.500542

Epoch: 3
Loss: 0.4978208441589305
ROC train: 0.638864	val: 0.700776	test: 0.454685
PRC train: 0.547901	val: 0.536143	test: 0.500598

Epoch: 4
Loss: 0.445167025006294
ROC train: 0.669146	val: 0.736489	test: 0.474090
PRC train: 0.559438	val: 0.541775	test: 0.501097

Epoch: 5
Loss: 0.3965785583787039
ROC train: 0.709964	val: 0.762212	test: 0.489200
PRC train: 0.588754	val: 0.556311	test: 0.501442

Epoch: 6
Loss: 0.3591940792634415
ROC train: 0.757330	val: 0.801384	test: 0.499281
PRC train: 0.625576	val: 0.569088	test: 0.503731

Epoch: 7
Loss: 0.32213860985773596
ROC train: 0.781865	val: 0.822212	test: 0.516844
PRC train: 0.651249	val: 0.560969	test: 0.504199

Epoch: 8
Loss: 0.30532439641186443
ROC train: 0.806996	val: 0.802445	test: 0.495146
PRC train: 0.684160	val: 0.561150	test: 0.501787

Epoch: 9
Loss: 0.27854905719770284
ROC train: 0.832978	val: 0.814020	test: 0.484866
PRC train: 0.709284	val: 0.576774	test: 0.502293

Epoch: 10
Loss: 0.26614006326989176
ROC train: 0.852477	val: 0.785813	test: 0.461644
PRC train: 0.737171	val: 0.622421	test: 0.496535

Epoch: 11
Loss: 0.24979144979194462
ROC train: 0.889094	val: 0.822887	test: 0.494096
PRC train: 0.756219	val: 0.630448	test: 0.504172

Epoch: 12
Loss: 0.22998032828579634
ROC train: 0.912965	val: 0.824422	test: 0.484501
PRC train: 0.784675	val: 0.591535	test: 0.504577

Epoch: 13
Loss: 0.22094693521405717
ROC train: 0.921127	val: 0.796327	test: 0.415846
PRC train: 0.816383	val: 0.584630	test: 0.496111

Epoch: 14
Loss: 0.2149596340291013
ROC train: 0.931334	val: 0.814695	test: 0.492486
PRC train: 0.830309	val: 0.616273	test: 0.507225

Epoch: 15
Loss: 0.20001081849424662
ROC train: 0.946373	val: 0.812822	test: 0.522197
PRC train: 0.865375	val: 0.632083	test: 0.510626

Epoch: 16
Loss: 0.1968442864081757
ROC train: 0.958605	val: 0.813047	test: 0.490006
PRC train: 0.884119	val: 0.643036	test: 0.506901

Epoch: 17
Loss: 0.18819035571456347
ROC train: 0.957902	val: 0.833376	test: 0.501556
PRC train: 0.883443	val: 0.629447	test: 0.511363

Epoch: 18
Loss: 0.18194204450429524
ROC train: 0.969726	val: 0.825272	test: 0.499494
PRC train: 0.912577	val: 0.595853	test: 0.512729

Epoch: 19
Loss: 0.17234523548543215
ROC train: 0.975204	val: 0.772403	test: 0.524390
PRC train: 0.924907	val: 0.630658	test: 0.516768

Epoch: 20
Loss: 0.1513569337334933
ROC train: 0.977979	val: 0.795354	test: 0.518850
PRC train: 0.930495	val: 0.594561	test: 0.513854

Epoch: 21
Loss: 0.1523965902858496
ROC train: 0.980425	val: 0.844414	test: 0.527576
PRC train: 0.940957	val: 0.587417	test: 0.512772

Epoch: 22
Loss: 0.1438791904948718
ROC train: 0.982319	val: 0.782517	test: 0.518458
PRC train: 0.947836	val: 0.616818	test: 0.514975

Epoch: 23
Loss: 0.14428301674744992
ROC train: 0.984639	val: 0.736029	test: 0.499695
PRC train: 0.957346	val: 0.595607	test: 0.512130

Epoch: 24
Loss: 0.1385554843731953
ROC train: 0.989493	val: 0.797926	test: 0.476458
PRC train: 0.971319	val: 0.564535	test: 0.507390

Epoch: 25
Loss: 0.1360184241944261
ROC train: 0.986704	val: 0.730322	test: 0.495740
PRC train: 0.955086	val: 0.547242	test: 0.514169

Epoch: 26
Loss: 0.11294492705143641
ROC train: 0.992085	val: 0.731559	test: 0.494784
PRC train: 0.971968	val: 0.569387	test: 0.508964

Epoch: 27
Loss: 0.12116549255552023
ROC train: 0.991260	val: 0.738601	test: 0.543354
PRC train: 0.970939	val: 0.563726	test: 0.517186

Epoch: 28
Loss: 0.1144366841050785
ROC train: 0.990875	val: 0.733007	test: 0.587938
PRC train: 0.968027	val: 0.564496	test: 0.529099

Epoch: 29
Loss: 0.11445419131909074
ROC train: 0.992847	val: 0.718272	test: 0.570610
PRC train: 0.972227	val: 0.547206	test: 0.523087

Epoch: 30
Loss: 0.11164467584621038
ROC train: 0.993875	val: 0.700515	test: 0.618235
PRC train: 0.974495	val: 0.535151	test: 0.542130

Epoch: 31
Loss: 0.10353426696053174
ROC train: 0.995813	val: 0.736176	test: 0.628574
PRC train: 0.982369	val: 0.552874	test: 0.547620

Epoch: 32
Loss: 0.10964711507488215
ROC train: 0.993826	val: 0.802782	test: 0.592846Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:3
  input_data_dir: 
  dataset: clintox
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/clintox/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/clintox/noise=0.2/clintox_scaff_2_20-05_14-43-32  ]
[ Using Seed :  2  ]
[ Using device :  cuda:3  ]
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.6361531079536658
ROC train: 0.548590	val: 0.588399	test: 0.481288
PRC train: 0.518205	val: 0.539188	test: 0.507528

Epoch: 2
Loss: 0.5642357703522879
ROC train: 0.615360	val: 0.616174	test: 0.467837
PRC train: 0.544517	val: 0.524635	test: 0.506090

Epoch: 3
Loss: 0.5027921858106229
ROC train: 0.658213	val: 0.674989	test: 0.494070
PRC train: 0.558525	val: 0.539473	test: 0.510557

Epoch: 4
Loss: 0.4476527073758203
ROC train: 0.710040	val: 0.709754	test: 0.510311
PRC train: 0.576531	val: 0.566908	test: 0.510230

Epoch: 5
Loss: 0.40441395388408496
ROC train: 0.758281	val: 0.708805	test: 0.515597
PRC train: 0.597257	val: 0.608051	test: 0.511015

Epoch: 6
Loss: 0.3610505768263474
ROC train: 0.789700	val: 0.709480	test: 0.533362
PRC train: 0.631474	val: 0.607631	test: 0.514575

Epoch: 7
Loss: 0.32790340829249304
ROC train: 0.814276	val: 0.746754	test: 0.556109
PRC train: 0.665218	val: 0.611672	test: 0.521496

Epoch: 8
Loss: 0.3040455866193729
ROC train: 0.824230	val: 0.766995	test: 0.563942
PRC train: 0.688932	val: 0.573346	test: 0.526794

Epoch: 9
Loss: 0.2767431066658567
ROC train: 0.854304	val: 0.734293	test: 0.557439
PRC train: 0.733184	val: 0.608550	test: 0.521550

Epoch: 10
Loss: 0.26281291319319344
ROC train: 0.881059	val: 0.685008	test: 0.547227
PRC train: 0.780398	val: 0.602783	test: 0.519042

Epoch: 11
Loss: 0.25064792024197147
ROC train: 0.898542	val: 0.708345	test: 0.572836
PRC train: 0.815645	val: 0.570347	test: 0.534818

Epoch: 12
Loss: 0.22800349539393264
ROC train: 0.902340	val: 0.701700	test: 0.590949
PRC train: 0.824595	val: 0.540473	test: 0.546968

Epoch: 13
Loss: 0.21410332135364393
ROC train: 0.913357	val: 0.726587	test: 0.583179
PRC train: 0.833039	val: 0.544846	test: 0.543203

Epoch: 14
Loss: 0.20040161120733085
ROC train: 0.933044	val: 0.760002	test: 0.585465
PRC train: 0.863857	val: 0.556212	test: 0.543446

Epoch: 15
Loss: 0.19286899386121142
ROC train: 0.949764	val: 0.719745	test: 0.590874
PRC train: 0.887606	val: 0.579658	test: 0.541949

Epoch: 16
Loss: 0.18903029411938257
ROC train: 0.962321	val: 0.654252	test: 0.587064
PRC train: 0.906628	val: 0.601431	test: 0.548793

Epoch: 17
Loss: 0.16921063540474182
ROC train: 0.966231	val: 0.726537	test: 0.594034
PRC train: 0.919188	val: 0.545146	test: 0.542410

Epoch: 18
Loss: 0.16698855702776605
ROC train: 0.973710	val: 0.698793	test: 0.621903
PRC train: 0.932935	val: 0.604236	test: 0.543885

Epoch: 19
Loss: 0.15209330484933337
ROC train: 0.966810	val: 0.751326	test: 0.650497
PRC train: 0.921207	val: 0.616381	test: 0.553316

Epoch: 20
Loss: 0.1424874440568349
ROC train: 0.973798	val: 0.679238	test: 0.626202
PRC train: 0.938098	val: 0.610453	test: 0.566707

Epoch: 21
Loss: 0.14030901486667718
ROC train: 0.984901	val: 0.600909	test: 0.602554
PRC train: 0.958475	val: 0.593868	test: 0.542393

Epoch: 22
Loss: 0.13236441075717836
ROC train: 0.981494	val: 0.617998	test: 0.604003
PRC train: 0.949418	val: 0.527047	test: 0.545003

Epoch: 23
Loss: 0.12719660085290815
ROC train: 0.980773	val: 0.594699	test: 0.619580
PRC train: 0.941617	val: 0.540533	test: 0.538960

Epoch: 24
Loss: 0.12304834431532305
ROC train: 0.987474	val: 0.619161	test: 0.577733
PRC train: 0.958011	val: 0.533863	test: 0.547675

Epoch: 25
Loss: 0.11289746900359558
ROC train: 0.992046	val: 0.661094	test: 0.552912
PRC train: 0.969826	val: 0.560179	test: 0.544072

Epoch: 26
Loss: 0.10756921626800806
ROC train: 0.994166	val: 0.640491	test: 0.579433
PRC train: 0.975764	val: 0.527822	test: 0.546792

Epoch: 27
Loss: 0.11217207370603632
ROC train: 0.996270	val: 0.619449	test: 0.605314
PRC train: 0.981836	val: 0.522360	test: 0.542472

Epoch: 28
Loss: 0.10359801962230189
ROC train: 0.993834	val: 0.635245	test: 0.628298
PRC train: 0.968414	val: 0.529230	test: 0.546202

Epoch: 29
Loss: 0.09540715598087743
ROC train: 0.995452	val: 0.647558	test: 0.599988
PRC train: 0.976366	val: 0.538483	test: 0.542738

Epoch: 30
Loss: 0.08936721508066875
ROC train: 0.995706	val: 0.646996	test: 0.583205
PRC train: 0.979794	val: 0.533134	test: 0.539950

Epoch: 31
Loss: 0.08029141506061402
ROC train: 0.995009	val: 0.653501	test: 0.559919
PRC train: 0.977500	val: 0.536661	test: 0.534211

Epoch: 32
Loss: 0.08725536088887764
ROC train: 0.996940	val: 0.600206	test: 0.556322Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.1.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:2
  input_data_dir: 
  dataset: clintox
  num_workers: 0
  noise_level: 0.1
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/clintox/noise=0.1
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/clintox/noise=0.1/clintox_scaff_1_20-05_14-43-32  ]
[ Using Seed :  1  ]
[ Using device :  cuda:2  ]
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.670073287920913
ROC train: 0.558560	val: 0.482234	test: 0.436943
PRC train: 0.530660	val: 0.508280	test: 0.492748

Epoch: 2
Loss: 0.5923043393382528
ROC train: 0.605472	val: 0.613946	test: 0.460987
PRC train: 0.543035	val: 0.517819	test: 0.492911

Epoch: 3
Loss: 0.5234890534856842
ROC train: 0.638395	val: 0.687830	test: 0.471483
PRC train: 0.546392	val: 0.525911	test: 0.493388

Epoch: 4
Loss: 0.46212362411539915
ROC train: 0.685833	val: 0.771915	test: 0.499143
PRC train: 0.567745	val: 0.542636	test: 0.499190

Epoch: 5
Loss: 0.42014996254332504
ROC train: 0.716231	val: 0.787261	test: 0.498493
PRC train: 0.584684	val: 0.548265	test: 0.500469

Epoch: 6
Loss: 0.3740581435863398
ROC train: 0.769490	val: 0.821625	test: 0.520311
PRC train: 0.624831	val: 0.553547	test: 0.507186

Epoch: 7
Loss: 0.33656440822735856
ROC train: 0.802730	val: 0.829430	test: 0.546010
PRC train: 0.643255	val: 0.555001	test: 0.509295

Epoch: 8
Loss: 0.31141371058852074
ROC train: 0.814551	val: 0.825072	test: 0.527445
PRC train: 0.665157	val: 0.555399	test: 0.507327

Epoch: 9
Loss: 0.28905166205023786
ROC train: 0.830666	val: 0.833513	test: 0.537490
PRC train: 0.686406	val: 0.574486	test: 0.510236

Epoch: 10
Loss: 0.264213992242056
ROC train: 0.855103	val: 0.862982	test: 0.578675
PRC train: 0.712728	val: 0.581990	test: 0.521473

Epoch: 11
Loss: 0.24957935627536337
ROC train: 0.875372	val: 0.791558	test: 0.563935
PRC train: 0.743448	val: 0.576860	test: 0.519188

Epoch: 12
Loss: 0.24027061991250007
ROC train: 0.886239	val: 0.682685	test: 0.542999
PRC train: 0.750356	val: 0.608741	test: 0.513413

Epoch: 13
Loss: 0.23210476139898767
ROC train: 0.906657	val: 0.665266	test: 0.569669
PRC train: 0.784872	val: 0.607780	test: 0.522304

Epoch: 14
Loss: 0.2167248129900706
ROC train: 0.912842	val: 0.709381	test: 0.594942
PRC train: 0.790451	val: 0.565468	test: 0.526388

Epoch: 15
Loss: 0.20200574333591045
ROC train: 0.924146	val: 0.683795	test: 0.575604
PRC train: 0.823537	val: 0.618655	test: 0.521030

Epoch: 16
Loss: 0.18555255101192025
ROC train: 0.937190	val: 0.690901	test: 0.574443
PRC train: 0.849715	val: 0.608471	test: 0.519406

Epoch: 17
Loss: 0.1860613653399532
ROC train: 0.954760	val: 0.728899	test: 0.605221
PRC train: 0.882528	val: 0.583163	test: 0.535558

Epoch: 18
Loss: 0.16780037020258778
ROC train: 0.964563	val: 0.735892	test: 0.616801
PRC train: 0.900740	val: 0.586812	test: 0.544692

Epoch: 19
Loss: 0.17991082261011435
ROC train: 0.968187	val: 0.720683	test: 0.599320
PRC train: 0.904603	val: 0.611087	test: 0.536484

Epoch: 20
Loss: 0.15517741679523406
ROC train: 0.972551	val: 0.784140	test: 0.602192
PRC train: 0.916832	val: 0.546537	test: 0.535796

Epoch: 21
Loss: 0.15739845438627004
ROC train: 0.971477	val: 0.776936	test: 0.573524
PRC train: 0.913829	val: 0.554800	test: 0.526970

Epoch: 22
Loss: 0.15331993376183786
ROC train: 0.978112	val: 0.698544	test: 0.596515
PRC train: 0.945350	val: 0.609671	test: 0.529723

Epoch: 23
Loss: 0.14789495480206188
ROC train: 0.979747	val: 0.700104	test: 0.639661
PRC train: 0.944919	val: 0.551500	test: 0.536864

Epoch: 24
Loss: 0.15637583500568178
ROC train: 0.986611	val: 0.718834	test: 0.638507
PRC train: 0.956119	val: 0.565054	test: 0.536496

Epoch: 25
Loss: 0.1426959316131171
ROC train: 0.986115	val: 0.742147	test: 0.600399
PRC train: 0.952204	val: 0.539342	test: 0.526479

Epoch: 26
Loss: 0.118060022898416
ROC train: 0.987190	val: 0.804406	test: 0.630207
PRC train: 0.950675	val: 0.553733	test: 0.534262

Epoch: 27
Loss: 0.11763211690679758
ROC train: 0.995297	val: 0.787974	test: 0.658513
PRC train: 0.980992	val: 0.561155	test: 0.537805

Epoch: 28
Loss: 0.10730027431559208
ROC train: 0.996525	val: 0.752000	test: 0.671665
PRC train: 0.986208	val: 0.570720	test: 0.541750

Epoch: 29
Loss: 0.09992401375710255
ROC train: 0.996197	val: 0.776374	test: 0.680940
PRC train: 0.988065	val: 0.541795	test: 0.545100

Epoch: 30
Loss: 0.09236076912041909
ROC train: 0.997866	val: 0.791720	test: 0.692370
PRC train: 0.993428	val: 0.550710	test: 0.549728

Epoch: 31
Loss: 0.09706476028492397
ROC train: 0.997384	val: 0.796801	test: 0.707841
PRC train: 0.989839	val: 0.546293	test: 0.560376

Epoch: 32
Loss: 0.09784455678170409
ROC train: 0.997031	val: 0.785050	test: 0.710908Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 3
  multiple_seeds: [1, 2, 3]
  device: cuda:3
  input_data_dir: 
  dataset: clintox
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/clintox/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/clintox/noise=0.2/clintox_scaff_3_20-05_14-43-32  ]
[ Using Seed :  3  ]
[ Using device :  cuda:3  ]
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.6423825680509679
ROC train: 0.527741	val: 0.481721	test: 0.543245
PRC train: 0.516685	val: 0.516258	test: 0.520816

Epoch: 2
Loss: 0.5682256480938782
ROC train: 0.580387	val: 0.566432	test: 0.572093
PRC train: 0.526031	val: 0.525795	test: 0.528657

Epoch: 3
Loss: 0.5057256498358913
ROC train: 0.651245	val: 0.647421	test: 0.566224
PRC train: 0.546639	val: 0.537505	test: 0.525685

Epoch: 4
Loss: 0.45403366800834527
ROC train: 0.703827	val: 0.707846	test: 0.540459
PRC train: 0.562817	val: 0.543145	test: 0.518551

Epoch: 5
Loss: 0.40579394577123135
ROC train: 0.763228	val: 0.768682	test: 0.547978
PRC train: 0.610321	val: 0.568772	test: 0.522353

Epoch: 6
Loss: 0.36447079615088934
ROC train: 0.791176	val: 0.764662	test: 0.586589
PRC train: 0.629797	val: 0.577853	test: 0.529076

Epoch: 7
Loss: 0.32726884546145035
ROC train: 0.817492	val: 0.711518	test: 0.601478
PRC train: 0.647361	val: 0.576994	test: 0.533045

Epoch: 8
Loss: 0.30406908479818834
ROC train: 0.839468	val: 0.667926	test: 0.612165
PRC train: 0.693772	val: 0.615237	test: 0.541467

Epoch: 9
Loss: 0.2775877185990601
ROC train: 0.879902	val: 0.673246	test: 0.649940
PRC train: 0.754648	val: 0.635308	test: 0.549358

Epoch: 10
Loss: 0.2641403767899888
ROC train: 0.901011	val: 0.707459	test: 0.713780
PRC train: 0.791725	val: 0.600517	test: 0.562938

Epoch: 11
Loss: 0.2366577463618374
ROC train: 0.914417	val: 0.688142	test: 0.744984
PRC train: 0.804770	val: 0.637546	test: 0.566836

Epoch: 12
Loss: 0.21694288490190386
ROC train: 0.926376	val: 0.629591	test: 0.697225
PRC train: 0.816603	val: 0.644042	test: 0.549411

Epoch: 13
Loss: 0.20805824222931277
ROC train: 0.950647	val: 0.620500	test: 0.701760
PRC train: 0.861854	val: 0.672323	test: 0.562333

Epoch: 14
Loss: 0.1957430851334665
ROC train: 0.963880	val: 0.647846	test: 0.723073
PRC train: 0.888032	val: 0.674862	test: 0.570310

Epoch: 15
Loss: 0.19001319855773607
ROC train: 0.972660	val: 0.648858	test: 0.776902
PRC train: 0.904902	val: 0.645274	test: 0.584898

Epoch: 16
Loss: 0.1669794484523673
ROC train: 0.976771	val: 0.581978	test: 0.746333
PRC train: 0.922904	val: 0.537675	test: 0.568785

Epoch: 17
Loss: 0.17236057163000326
ROC train: 0.980588	val: 0.612147	test: 0.765446
PRC train: 0.926859	val: 0.526100	test: 0.592447

Epoch: 18
Loss: 0.15301291222047717
ROC train: 0.982141	val: 0.620538	test: 0.740849
PRC train: 0.945490	val: 0.551837	test: 0.570575

Epoch: 19
Loss: 0.13809825631384934
ROC train: 0.985515	val: 0.639381	test: 0.747095
PRC train: 0.954989	val: 0.561119	test: 0.566332

Epoch: 20
Loss: 0.1372079144498755
ROC train: 0.991694	val: 0.644350	test: 0.759597
PRC train: 0.966541	val: 0.603844	test: 0.569844

Epoch: 21
Loss: 0.11923478827188246
ROC train: 0.992438	val: 0.616592	test: 0.731340
PRC train: 0.969893	val: 0.542898	test: 0.557049

Epoch: 22
Loss: 0.12843622611289024
ROC train: 0.996154	val: 0.638056	test: 0.813389
PRC train: 0.977811	val: 0.595598	test: 0.580490

Epoch: 23
Loss: 0.10739644783499543
ROC train: 0.996519	val: 0.652492	test: 0.786768
PRC train: 0.981180	val: 0.548391	test: 0.579557

Epoch: 24
Loss: 0.11219277754781846
ROC train: 0.996343	val: 0.661920	test: 0.739083
PRC train: 0.980362	val: 0.540582	test: 0.569351

Epoch: 25
Loss: 0.09733526712842731
ROC train: 0.996792	val: 0.688553	test: 0.794623
PRC train: 0.979187	val: 0.541287	test: 0.588724

Epoch: 26
Loss: 0.11539406288143354
ROC train: 0.998826	val: 0.682235	test: 0.773425
PRC train: 0.990884	val: 0.565526	test: 0.566285

Epoch: 27
Loss: 0.1005949926594972
ROC train: 0.998890	val: 0.604117	test: 0.707542
PRC train: 0.995350	val: 0.547836	test: 0.547710

Epoch: 28
Loss: 0.08911935426182549
ROC train: 0.998523	val: 0.566906	test: 0.737260
PRC train: 0.993392	val: 0.542061	test: 0.555048

Epoch: 29
Loss: 0.08551209639813609
ROC train: 0.998233	val: 0.614332	test: 0.788542
PRC train: 0.990412	val: 0.531837	test: 0.568738

Epoch: 30
Loss: 0.07844222259319698
ROC train: 0.999318	val: 0.635435	test: 0.761072
PRC train: 0.996126	val: 0.537825	test: 0.566458

Epoch: 31
Loss: 0.08079779954902488
ROC train: 0.998638	val: 0.616704	test: 0.707030
PRC train: 0.992741	val: 0.518442	test: 0.561433

Epoch: 32
Loss: 0.06906780552077496
ROC train: 0.998877	val: 0.647846	test: 0.793652Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.05.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 2
  multiple_seeds: [1, 2, 3]
  device: cuda:1
  input_data_dir: 
  dataset: clintox
  num_workers: 0
  noise_level: 0.05
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/clintox/noise=0.05
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/clintox/noise=0.05/clintox_scaff_2_20-05_14-43-32  ]
[ Using Seed :  2  ]
[ Using device :  cuda:1  ]
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.6316371756611826
ROC train: 0.554063	val: 0.615036	test: 0.461009
PRC train: 0.519356	val: 0.531279	test: 0.505813

Epoch: 2
Loss: 0.5527244895436467
ROC train: 0.621416	val: 0.661028	test: 0.475394
PRC train: 0.535327	val: 0.536374	test: 0.503164

Epoch: 3
Loss: 0.4893444911095549
ROC train: 0.684255	val: 0.755293	test: 0.488912
PRC train: 0.551865	val: 0.549399	test: 0.506035

Epoch: 4
Loss: 0.43837691942405366
ROC train: 0.748072	val: 0.806739	test: 0.534490
PRC train: 0.580806	val: 0.564717	test: 0.513427

Epoch: 5
Loss: 0.3861256471446498
ROC train: 0.792212	val: 0.785275	test: 0.558407
PRC train: 0.623922	val: 0.581322	test: 0.517707

Epoch: 6
Loss: 0.35244815352922165
ROC train: 0.819379	val: 0.836609	test: 0.576007
PRC train: 0.652541	val: 0.576420	test: 0.525458

Epoch: 7
Loss: 0.3172696716382287
ROC train: 0.836837	val: 0.885033	test: 0.583740
PRC train: 0.658178	val: 0.585567	test: 0.527708

Epoch: 8
Loss: 0.2940448705752847
ROC train: 0.852438	val: 0.877516	test: 0.545427
PRC train: 0.683562	val: 0.580951	test: 0.516220

Epoch: 9
Loss: 0.26909713508981753
ROC train: 0.874348	val: 0.866278	test: 0.532435
PRC train: 0.724983	val: 0.586527	test: 0.512337

Epoch: 10
Loss: 0.2604115873781438
ROC train: 0.890692	val: 0.834662	test: 0.531199
PRC train: 0.747842	val: 0.585738	test: 0.512363

Epoch: 11
Loss: 0.24304632408425383
ROC train: 0.910175	val: 0.833488	test: 0.554197
PRC train: 0.799074	val: 0.576455	test: 0.518793

Epoch: 12
Loss: 0.21761181266630483
ROC train: 0.921084	val: 0.802958	test: 0.547500
PRC train: 0.826712	val: 0.574542	test: 0.516313

Epoch: 13
Loss: 0.21975358607879808
ROC train: 0.928484	val: 0.708658	test: 0.500405
PRC train: 0.834210	val: 0.570525	test: 0.506345

Epoch: 14
Loss: 0.2042349685068577
ROC train: 0.938618	val: 0.772041	test: 0.527800
PRC train: 0.852191	val: 0.626858	test: 0.516792

Epoch: 15
Loss: 0.19973876258580928
ROC train: 0.943585	val: 0.800512	test: 0.519530
PRC train: 0.859863	val: 0.626294	test: 0.512057

Epoch: 16
Loss: 0.1958957165876877
ROC train: 0.955761	val: 0.779234	test: 0.515668
PRC train: 0.877717	val: 0.580819	test: 0.513267

Epoch: 17
Loss: 0.18099106572567977
ROC train: 0.956096	val: 0.743496	test: 0.539779
PRC train: 0.876482	val: 0.560345	test: 0.517401

Epoch: 18
Loss: 0.17677385279352387
ROC train: 0.960839	val: 0.667476	test: 0.496857
PRC train: 0.898402	val: 0.602374	test: 0.510376

Epoch: 19
Loss: 0.16942044056351105
ROC train: 0.967100	val: 0.715513	test: 0.530530
PRC train: 0.912582	val: 0.558409	test: 0.516018

Epoch: 20
Loss: 0.168044993423011
ROC train: 0.976083	val: 0.715738	test: 0.583254
PRC train: 0.931582	val: 0.609801	test: 0.525294

Epoch: 21
Loss: 0.15310023394566802
ROC train: 0.977699	val: 0.710706	test: 0.594213
PRC train: 0.939520	val: 0.605949	test: 0.524787

Epoch: 22
Loss: 0.14831547097661515
ROC train: 0.974886	val: 0.796590	test: 0.629885
PRC train: 0.928143	val: 0.564131	test: 0.540430

Epoch: 23
Loss: 0.14573911641483678
ROC train: 0.983520	val: 0.776051	test: 0.578017
PRC train: 0.949324	val: 0.615753	test: 0.521783

Epoch: 24
Loss: 0.13134214027104613
ROC train: 0.987200	val: 0.815971	test: 0.598079
PRC train: 0.959367	val: 0.574523	test: 0.525335

Epoch: 25
Loss: 0.1384189710009983
ROC train: 0.987348	val: 0.808704	test: 0.615643
PRC train: 0.946930	val: 0.585660	test: 0.529394

Epoch: 26
Loss: 0.11734286481223508
ROC train: 0.989935	val: 0.818743	test: 0.569011
PRC train: 0.960907	val: 0.635177	test: 0.520984

Epoch: 27
Loss: 0.12432690465758145
ROC train: 0.991660	val: 0.842056	test: 0.609386
PRC train: 0.967986	val: 0.645518	test: 0.532113

Epoch: 28
Loss: 0.12293195337778753
ROC train: 0.992646	val: 0.824412	test: 0.655926
PRC train: 0.972645	val: 0.566467	test: 0.545106

Epoch: 29
Loss: 0.12596865594675483
ROC train: 0.993754	val: 0.740400	test: 0.640740
PRC train: 0.980750	val: 0.611663	test: 0.539187

Epoch: 30
Loss: 0.11119762056261925
ROC train: 0.990553	val: 0.787363	test: 0.661628
PRC train: 0.969082	val: 0.585127	test: 0.550950

Epoch: 31
Loss: 0.11228814670416092
ROC train: 0.994991	val: 0.825223	test: 0.698241
PRC train: 0.978297	val: 0.570837	test: 0.551079

Epoch: 32
Loss: 0.11864167979149491
ROC train: 0.995159	val: 0.798439	test: 0.683236Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_static_noise_experiments/GraphCL/clintox/noise=0.2.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 1
  multiple_seeds: [1, 2, 3]
  device: cuda:3
  input_data_dir: 
  dataset: clintox
  num_workers: 0
  noise_level: 0.2
  dynamic_noise: False
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphCL_classification.pth
  output_model_dir: ../runs/static-noise/GraphCL/clintox/noise=0.2
  verbose: False
[ Logs to :  ../runs/static-noise/GraphCL/clintox/noise=0.2/clintox_scaff_1_20-05_14-43-32  ]
[ Using Seed :  1  ]
[ Using device :  cuda:3  ]
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
  (molecule_model): GNN(
    (x_embedding1): Embedding(120, 300)
    (x_embedding2): Embedding(3, 300)
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): ReLU()
          (2): Linear(in_features=600, out_features=300, bias=True)
        )
        (edge_embedding1): Embedding(6, 300)
        (edge_embedding2): Embedding(3, 300)
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.669417224556642
ROC train: 0.581556	val: 0.432623	test: 0.471741
PRC train: 0.522059	val: 0.504742	test: 0.498166

Epoch: 2
Loss: 0.5952775917506743
ROC train: 0.611245	val: 0.488324	test: 0.448257
PRC train: 0.535463	val: 0.509672	test: 0.494174

Epoch: 3
Loss: 0.5264409455049133
ROC train: 0.640645	val: 0.566193	test: 0.481232
PRC train: 0.547281	val: 0.517507	test: 0.499512

Epoch: 4
Loss: 0.4710453333781304
ROC train: 0.688439	val: 0.738239	test: 0.516430
PRC train: 0.562425	val: 0.532123	test: 0.504408

Epoch: 5
Loss: 0.4212431602574749
ROC train: 0.705590	val: 0.837983	test: 0.506087
PRC train: 0.564110	val: 0.556553	test: 0.502196

Epoch: 6
Loss: 0.38353282094127455
ROC train: 0.763448	val: 0.837020	test: 0.529910
PRC train: 0.601093	val: 0.564118	test: 0.506675

Epoch: 7
Loss: 0.33976136055420814
ROC train: 0.792709	val: 0.852029	test: 0.539305
PRC train: 0.625432	val: 0.578456	test: 0.511338

Epoch: 8
Loss: 0.31282038514175925
ROC train: 0.797682	val: 0.866939	test: 0.524364
PRC train: 0.645146	val: 0.576722	test: 0.515156

Epoch: 9
Loss: 0.29515393733305373
ROC train: 0.837557	val: 0.857349	test: 0.519616
PRC train: 0.675981	val: 0.571705	test: 0.517216

Epoch: 10
Loss: 0.27224795829489085
ROC train: 0.876037	val: 0.773275	test: 0.582776
PRC train: 0.701338	val: 0.582320	test: 0.570070

Epoch: 11
Loss: 0.2555999495613542
ROC train: 0.891976	val: 0.833548	test: 0.537329
PRC train: 0.736926	val: 0.568393	test: 0.528912

Epoch: 12
Loss: 0.24305312300095494
ROC train: 0.910736	val: 0.794841	test: 0.514768
PRC train: 0.764427	val: 0.555346	test: 0.554628

Epoch: 13
Loss: 0.23161115053929376
ROC train: 0.930331	val: 0.739199	test: 0.527684
PRC train: 0.786783	val: 0.543945	test: 0.556017

Epoch: 14
Loss: 0.21820938225779987
ROC train: 0.946609	val: 0.679487	test: 0.506916
PRC train: 0.819962	val: 0.530676	test: 0.552332

Epoch: 15
Loss: 0.20632480916720608
ROC train: 0.959372	val: 0.606127	test: 0.531819
PRC train: 0.844213	val: 0.517386	test: 0.515157

Epoch: 16
Loss: 0.18641902539194186
ROC train: 0.963782	val: 0.673605	test: 0.540227
PRC train: 0.856986	val: 0.538993	test: 0.515329

Epoch: 17
Loss: 0.17976733076599183
ROC train: 0.974856	val: 0.712963	test: 0.530833
PRC train: 0.902180	val: 0.553243	test: 0.516615

Epoch: 18
Loss: 0.16591623631149
ROC train: 0.982292	val: 0.757890	test: 0.529608
PRC train: 0.932254	val: 0.552604	test: 0.515357

Epoch: 19
Loss: 0.1720761681882218
ROC train: 0.980055	val: 0.760325	test: 0.483126
PRC train: 0.930118	val: 0.555328	test: 0.505144

Epoch: 20
Loss: 0.15541830009915672
ROC train: 0.978419	val: 0.710678	test: 0.482301
PRC train: 0.915736	val: 0.535917	test: 0.506705

Epoch: 21
Loss: 0.14730764815390512
ROC train: 0.990734	val: 0.791931	test: 0.543511
PRC train: 0.959262	val: 0.545450	test: 0.523632

Epoch: 22
Loss: 0.12746895517044762
ROC train: 0.993051	val: 0.784014	test: 0.550981
PRC train: 0.968656	val: 0.548315	test: 0.541156

Epoch: 23
Loss: 0.13315541162987082
ROC train: 0.994924	val: 0.816530	test: 0.541425
PRC train: 0.973097	val: 0.554806	test: 0.520156

Epoch: 24
Loss: 0.1276636668547157
ROC train: 0.995903	val: 0.749474	test: 0.506173
PRC train: 0.977758	val: 0.555085	test: 0.515821

Epoch: 25
Loss: 0.12498303482795407
ROC train: 0.993279	val: 0.749924	test: 0.462283
PRC train: 0.975893	val: 0.555842	test: 0.510139

Epoch: 26
Loss: 0.12160726469482415
ROC train: 0.994021	val: 0.729721	test: 0.500215
PRC train: 0.979176	val: 0.541371	test: 0.518916

Epoch: 27
Loss: 0.10652671412380137
ROC train: 0.994347	val: 0.699841	test: 0.508959
PRC train: 0.971447	val: 0.533447	test: 0.515084

Epoch: 28
Loss: 0.1152984563390721
ROC train: 0.996835	val: 0.745615	test: 0.529421
PRC train: 0.985097	val: 0.542864	test: 0.522253

Epoch: 29
Loss: 0.09352533779293101
ROC train: 0.998325	val: 0.753719	test: 0.526385
PRC train: 0.990865	val: 0.549440	test: 0.520361

Epoch: 30
Loss: 0.08254470237958986
ROC train: 0.998701	val: 0.718132	test: 0.518003
PRC train: 0.993641	val: 0.539810	test: 0.514335

Epoch: 31
Loss: 0.08834518892328133
ROC train: 0.999587	val: 0.702136	test: 0.517166
PRC train: 0.997911	val: 0.549308	test: 0.514556

Epoch: 32
Loss: 0.08188708168430897
ROC train: 0.999485	val: 0.750335	test: 0.520864
ROC train: 0.963170	val: 0.837920	test: 0.783824
PRC train: 0.835609	val: 0.633697	test: 0.585848

Epoch: 34
Loss: 0.1521813119028385
ROC train: 0.964747	val: 0.822623	test: 0.803942
PRC train: 0.841381	val: 0.629521	test: 0.627500

Epoch: 35
Loss: 0.15309825421180653
ROC train: 0.967857	val: 0.796738	test: 0.776003
PRC train: 0.859145	val: 0.610202	test: 0.583089

Epoch: 36
Loss: 0.15102648246596456
ROC train: 0.968446	val: 0.800709	test: 0.807090
PRC train: 0.863856	val: 0.605466	test: 0.591058

Epoch: 37
Loss: 0.14600432411579667
ROC train: 0.967595	val: 0.820451	test: 0.804929
PRC train: 0.858736	val: 0.620874	test: 0.593031

Epoch: 38
Loss: 0.14804648318770558
ROC train: 0.966060	val: 0.808352	test: 0.793917
PRC train: 0.842198	val: 0.589666	test: 0.594583

Epoch: 39
Loss: 0.14868572720322154
ROC train: 0.962913	val: 0.769244	test: 0.801294
PRC train: 0.853108	val: 0.592353	test: 0.602162

Epoch: 40
Loss: 0.14274787059977506
ROC train: 0.964217	val: 0.765698	test: 0.800644
PRC train: 0.860209	val: 0.601292	test: 0.589477

Epoch: 41
Loss: 0.15558193734794845
ROC train: 0.967510	val: 0.784952	test: 0.815940
PRC train: 0.859491	val: 0.614710	test: 0.596690

Epoch: 42
Loss: 0.14007684425379652
ROC train: 0.971133	val: 0.796763	test: 0.833855
PRC train: 0.868246	val: 0.624786	test: 0.593928

Epoch: 43
Loss: 0.1410736748147464
ROC train: 0.975056	val: 0.787535	test: 0.831774
PRC train: 0.880997	val: 0.622895	test: 0.591570

Epoch: 44
Loss: 0.13192097802686825
ROC train: 0.975247	val: 0.801296	test: 0.813647
PRC train: 0.884588	val: 0.644868	test: 0.600836

Epoch: 45
Loss: 0.1318567134558081
ROC train: 0.975746	val: 0.836022	test: 0.835495
PRC train: 0.885429	val: 0.659250	test: 0.614654

Epoch: 46
Loss: 0.1364860954694799
ROC train: 0.974014	val: 0.821713	test: 0.823990
PRC train: 0.884031	val: 0.653054	test: 0.597308

Epoch: 47
Loss: 0.12903845338972414
ROC train: 0.973513	val: 0.819028	test: 0.825140
PRC train: 0.875684	val: 0.641778	test: 0.595094

Epoch: 48
Loss: 0.1391639216398412
ROC train: 0.974382	val: 0.828007	test: 0.824946
PRC train: 0.876366	val: 0.663417	test: 0.597663

Epoch: 49
Loss: 0.12205927657825452
ROC train: 0.975615	val: 0.774849	test: 0.813124
PRC train: 0.883644	val: 0.669160	test: 0.604493

Epoch: 50
Loss: 0.13915329508395854
ROC train: 0.975950	val: 0.785300	test: 0.786529
PRC train: 0.884700	val: 0.680569	test: 0.587933

Epoch: 51
Loss: 0.1315904859427456
ROC train: 0.976389	val: 0.793217	test: 0.815447
PRC train: 0.887261	val: 0.679547	test: 0.587087

Epoch: 52
Loss: 0.14833775529690454
ROC train: 0.976488	val: 0.795389	test: 0.844403
PRC train: 0.885897	val: 0.688696	test: 0.603230

Epoch: 53
Loss: 0.13496317879885736
ROC train: 0.975100	val: 0.847622	test: 0.832110
PRC train: 0.883693	val: 0.668718	test: 0.599316

Epoch: 54
Loss: 0.13070166291888322
ROC train: 0.972389	val: 0.851456	test: 0.830411
PRC train: 0.878579	val: 0.662286	test: 0.597351

Epoch: 55
Loss: 0.13291322812449208
ROC train: 0.976134	val: 0.840330	test: 0.840242
PRC train: 0.881889	val: 0.641170	test: 0.601804

Epoch: 56
Loss: 0.14273521323123528
ROC train: 0.977285	val: 0.803570	test: 0.825144
PRC train: 0.890485	val: 0.586848	test: 0.598923

Epoch: 57
Loss: 0.14060038935534194
ROC train: 0.978097	val: 0.809189	test: 0.825701
PRC train: 0.892175	val: 0.616481	test: 0.607109

Epoch: 58
Loss: 0.1289688522390348
ROC train: 0.978854	val: 0.816143	test: 0.845214
PRC train: 0.894117	val: 0.635865	test: 0.613155

Epoch: 59
Loss: 0.12449391993060868
ROC train: 0.979227	val: 0.859036	test: 0.825152
PRC train: 0.897597	val: 0.631773	test: 0.601862

Epoch: 60
Loss: 0.12404364326509269
ROC train: 0.979851	val: 0.870523	test: 0.812403
PRC train: 0.902501	val: 0.689478	test: 0.601303

Epoch: 61
Loss: 0.131647757651184
ROC train: 0.977445	val: 0.872534	test: 0.766721
PRC train: 0.897740	val: 0.676962	test: 0.585918

Epoch: 62
Loss: 0.12619010482455462
ROC train: 0.976039	val: 0.859085	test: 0.740495
PRC train: 0.892938	val: 0.644973	test: 0.573097

Epoch: 63
Loss: 0.12923620484622433
ROC train: 0.979703	val: 0.852654	test: 0.771823
PRC train: 0.902229	val: 0.643637	test: 0.588618

Epoch: 64
Loss: 0.12219947592462421
ROC train: 0.979506	val: 0.813957	test: 0.803278
PRC train: 0.896911	val: 0.626336	test: 0.617948

Epoch: 65
Loss: 0.12529052108287514
ROC train: 0.979223	val: 0.807614	test: 0.830060
PRC train: 0.901725	val: 0.624477	test: 0.636818

Epoch: 66
Loss: 0.12283729744720337
ROC train: 0.979993	val: 0.810211	test: 0.828749
PRC train: 0.903896	val: 0.634785	test: 0.628130

Epoch: 67
Loss: 0.11879363372296552
ROC train: 0.980262	val: 0.785936	test: 0.826026
PRC train: 0.904728	val: 0.611219	test: 0.616083

Epoch: 68
Loss: 0.11485890474183454
ROC train: 0.980216	val: 0.822936	test: 0.838281
PRC train: 0.902258	val: 0.652881	test: 0.616232

Epoch: 69
Loss: 0.12056836428273363
ROC train: 0.978526	val: 0.791681	test: 0.825940
PRC train: 0.894724	val: 0.598423	test: 0.607707

Epoch: 70
Loss: 0.1395857214241168
ROC train: 0.977321	val: 0.751273	test: 0.791500
PRC train: 0.890910	val: 0.574355	test: 0.590750

Epoch: 71
Loss: 0.12252871808665275
ROC train: 0.981774	val: 0.740485	test: 0.814846
PRC train: 0.905930	val: 0.591920	test: 0.617626

Epoch: 72
Loss: 0.12004471821434914
ROC train: 0.982103	val: 0.750749	test: 0.823116
PRC train: 0.905226	val: 0.587174	test: 0.616967

Epoch: 73
Loss: 0.12094105307472265
ROC train: 0.981522	val: 0.800734	test: 0.837344
PRC train: 0.900793	val: 0.620969	test: 0.615973

Epoch: 74
Loss: 0.12871950937857302
ROC train: 0.981726	val: 0.808314	test: 0.830011
PRC train: 0.903926	val: 0.627968	test: 0.609977

Epoch: 75
Loss: 0.12698987308332904
ROC train: 0.981382	val: 0.770340	test: 0.822503
PRC train: 0.901765	val: 0.628654	test: 0.651721

Epoch: 76
Loss: 0.12455321090864917
ROC train: 0.981476	val: 0.744867	test: 0.842379
PRC train: 0.903650	val: 0.650550	test: 0.643512

Epoch: 77
Loss: 0.11456434147813369
ROC train: 0.983307	val: 0.769504	test: 0.856468
PRC train: 0.908950	val: 0.645197	test: 0.617894

Epoch: 78
Loss: 0.10760730910437369
ROC train: 0.982048	val: 0.782678	test: 0.837493
PRC train: 0.900853	val: 0.623786	test: 0.603965

Epoch: 79
Loss: 0.12218902520770376
ROC train: 0.982660	val: 0.847148	test: 0.852221
PRC train: 0.905879	val: 0.631473	test: 0.605307

Epoch: 80
Loss: 0.11740544252107987
ROC train: 0.982959	val: 0.845436	test: 0.859210
PRC train: 0.908576	val: 0.673406	test: 0.625523

Epoch: 81
Loss: 0.1022953147474956
ROC train: 0.983363	val: 0.800284	test: 0.837530
PRC train: 0.911277	val: 0.678529	test: 0.618981

Epoch: 82
Loss: 0.11237289543279783
ROC train: 0.983055	val: 0.796538	test: 0.835032
PRC train: 0.912422	val: 0.673041	test: 0.611919

Epoch: 83
Loss: 0.10672130055866132
ROC train: 0.983645	val: 0.785437	test: 0.858672
PRC train: 0.912864	val: 0.652352	test: 0.614127

Epoch: 84
Loss: 0.11234178240502406
ROC train: 0.984323	val: 0.782415	test: 0.880677
PRC train: 0.913520	val: 0.647830	test: 0.628027

Epoch: 85
Loss: 0.11177648432146459
ROC train: 0.985098	val: 0.783789	test: 0.883725
PRC train: 0.920573	val: 0.639966	test: 0.642992

Epoch: 86
Loss: 0.10428273231541726
ROC train: 0.984452	val: 0.787060	test: 0.883501
PRC train: 0.917477	val: 0.625638	test: 0.638389

Epoch: 87
Loss: 0.1065587765029878
ROC train: 0.983014	val: 0.765558	test: 0.877080
PRC train: 0.906102	val: 0.620889	test: 0.647724

Epoch: 88
Loss: 0.10457927768550782
ROC train: 0.981981	val: 0.766457	test: 0.855930
PRC train: 0.899388	val: 0.636270	test: 0.635242

Epoch: 89
Loss: 0.11212815769924094
ROC train: 0.984385	val: 0.809737	test: 0.856569
PRC train: 0.914866	val: 0.647328	test: 0.626744

Epoch: 90
Loss: 0.11264875982111835
ROC train: 0.983651	val: 0.812784	test: 0.864189
PRC train: 0.912448	val: 0.669280	test: 0.626537

Epoch: 91
Loss: 0.1155826906160586
ROC train: 0.984823	val: 0.786112	test: 0.863151
PRC train: 0.920332	val: 0.646459	test: 0.633132

Epoch: 92
Loss: 0.09948172531420799
ROC train: 0.984392	val: 0.763411	test: 0.853144
PRC train: 0.922662	val: 0.632849	test: 0.630711

Epoch: 93
Loss: 0.1009606139188525
ROC train: 0.984279	val: 0.770878	test: 0.865743
PRC train: 0.918982	val: 0.643612	test: 0.647844

Epoch: 94
Loss: 0.10689007868062288
ROC train: 0.963558	val: 0.725501	test: 0.814771
PRC train: 0.847327	val: 0.610556	test: 0.590132

Epoch: 34
Loss: 0.15155708815714433
ROC train: 0.961449	val: 0.728835	test: 0.780713
PRC train: 0.838511	val: 0.616409	test: 0.573700

Epoch: 35
Loss: 0.1520866243522368
ROC train: 0.962482	val: 0.817991	test: 0.792049
PRC train: 0.845237	val: 0.624308	test: 0.573929

Epoch: 36
Loss: 0.15106492493299425
ROC train: 0.959534	val: 0.841367	test: 0.829645
PRC train: 0.842096	val: 0.603880	test: 0.596241

Epoch: 37
Loss: 0.15002917387487036
ROC train: 0.963568	val: 0.826720	test: 0.810487
PRC train: 0.855399	val: 0.640936	test: 0.585703

Epoch: 38
Loss: 0.14520800428489888
ROC train: 0.967657	val: 0.847299	test: 0.787302
PRC train: 0.863049	val: 0.648644	test: 0.581589

Epoch: 39
Loss: 0.15121281852261853
ROC train: 0.969250	val: 0.870474	test: 0.765742
PRC train: 0.867389	val: 0.652016	test: 0.569799

Epoch: 40
Loss: 0.13713462517616334
ROC train: 0.969515	val: 0.872734	test: 0.790499
PRC train: 0.870780	val: 0.634899	test: 0.585164

Epoch: 41
Loss: 0.1385479510656884
ROC train: 0.969868	val: 0.881238	test: 0.774916
PRC train: 0.875772	val: 0.631888	test: 0.585718

Epoch: 42
Loss: 0.13941287759029816
ROC train: 0.967759	val: 0.888055	test: 0.775222
PRC train: 0.872474	val: 0.620931	test: 0.580676

Epoch: 43
Loss: 0.147439782134854
ROC train: 0.970205	val: 0.882461	test: 0.785266
PRC train: 0.869876	val: 0.626627	test: 0.589687

Epoch: 44
Loss: 0.14575558904353977
ROC train: 0.972185	val: 0.850708	test: 0.805803
PRC train: 0.864784	val: 0.601467	test: 0.599457

Epoch: 45
Loss: 0.1437537692801583
ROC train: 0.972391	val: 0.821101	test: 0.812948
PRC train: 0.863942	val: 0.589442	test: 0.597339

Epoch: 46
Loss: 0.13407027719547435
ROC train: 0.973435	val: 0.798762	test: 0.842379
PRC train: 0.871245	val: 0.596719	test: 0.631445

Epoch: 47
Loss: 0.14118907503637593
ROC train: 0.975145	val: 0.766833	test: 0.853009
PRC train: 0.872503	val: 0.612501	test: 0.637695

Epoch: 48
Loss: 0.14420893214501201
ROC train: 0.974783	val: 0.778683	test: 0.855546
PRC train: 0.872451	val: 0.611243	test: 0.634250

Epoch: 49
Loss: 0.1365858995157873
ROC train: 0.975521	val: 0.807477	test: 0.874596
PRC train: 0.881677	val: 0.627376	test: 0.671570

Epoch: 50
Loss: 0.13471291974374383
ROC train: 0.975176	val: 0.858987	test: 0.834583
PRC train: 0.886485	val: 0.628272	test: 0.631669

Epoch: 51
Loss: 0.13181214682587886
ROC train: 0.977794	val: 0.810787	test: 0.840130
PRC train: 0.889756	val: 0.627155	test: 0.652814

Epoch: 52
Loss: 0.12694337446068754
ROC train: 0.979599	val: 0.768682	test: 0.840873
PRC train: 0.896928	val: 0.615962	test: 0.648430

Epoch: 53
Loss: 0.13631957762017216
ROC train: 0.979040	val: 0.804269	test: 0.823116
PRC train: 0.893651	val: 0.613510	test: 0.637792

Epoch: 54
Loss: 0.13740724057949485
ROC train: 0.976433	val: 0.852830	test: 0.821760
PRC train: 0.886647	val: 0.646030	test: 0.609635

Epoch: 55
Loss: 0.12952506503351424
ROC train: 0.979905	val: 0.829518	test: 0.832010
PRC train: 0.899249	val: 0.639425	test: 0.663230

Epoch: 56
Loss: 0.12357095751114748
ROC train: 0.980037	val: 0.789011	test: 0.828599
PRC train: 0.897092	val: 0.622588	test: 0.680126

Epoch: 57
Loss: 0.12602514314444754
ROC train: 0.978852	val: 0.814895	test: 0.817569
PRC train: 0.894382	val: 0.620417	test: 0.667517

Epoch: 58
Loss: 0.14143257268363646
ROC train: 0.978637	val: 0.817155	test: 0.826945
PRC train: 0.891921	val: 0.636433	test: 0.644941

Epoch: 59
Loss: 0.13254665653390943
ROC train: 0.978946	val: 0.814470	test: 0.826052
PRC train: 0.897381	val: 0.660708	test: 0.660510

Epoch: 60
Loss: 0.12125457004879489
ROC train: 0.980821	val: 0.824036	test: 0.825040
PRC train: 0.901859	val: 0.638872	test: 0.672121

Epoch: 61
Loss: 0.12648061138172645
ROC train: 0.980518	val: 0.802146	test: 0.831935
PRC train: 0.900156	val: 0.651566	test: 0.668214

Epoch: 62
Loss: 0.10673266629390929
ROC train: 0.982828	val: 0.764686	test: 0.831535
PRC train: 0.905567	val: 0.598774	test: 0.665665

Epoch: 63
Loss: 0.12028681049571528
ROC train: 0.981899	val: 0.759454	test: 0.839955
PRC train: 0.902257	val: 0.592450	test: 0.673255

Epoch: 64
Loss: 0.12651427608395505
ROC train: 0.981194	val: 0.786175	test: 0.852322
PRC train: 0.900830	val: 0.596503	test: 0.676888

Epoch: 65
Loss: 0.12385623797915077
ROC train: 0.983273	val: 0.797775	test: 0.856719
PRC train: 0.911211	val: 0.608940	test: 0.697577

Epoch: 66
Loss: 0.12636796521242352
ROC train: 0.982572	val: 0.820564	test: 0.861615
PRC train: 0.910430	val: 0.638277	test: 0.694140

Epoch: 67
Loss: 0.1194827745155655
ROC train: 0.982293	val: 0.804993	test: 0.849117
PRC train: 0.905826	val: 0.639021	test: 0.694737

Epoch: 68
Loss: 0.11118848223629296
ROC train: 0.981853	val: 0.807066	test: 0.826526
PRC train: 0.906352	val: 0.583740	test: 0.662466

Epoch: 69
Loss: 0.1169995905563126
ROC train: 0.982867	val: 0.856165	test: 0.821080
PRC train: 0.905419	val: 0.632235	test: 0.648296

Epoch: 70
Loss: 0.11775877231096854
ROC train: 0.983071	val: 0.785001	test: 0.825689
PRC train: 0.904517	val: 0.599418	test: 0.649058

Epoch: 71
Loss: 0.11984012544341102
ROC train: 0.983176	val: 0.716473	test: 0.829425
PRC train: 0.908431	val: 0.572624	test: 0.652476

Epoch: 72
Loss: 0.11397027278097396
ROC train: 0.984741	val: 0.749126	test: 0.851579
PRC train: 0.914927	val: 0.623305	test: 0.681532

Epoch: 73
Loss: 0.11440611716022805
ROC train: 0.984947	val: 0.785001	test: 0.831423
PRC train: 0.917439	val: 0.598338	test: 0.669246

Epoch: 74
Loss: 0.11552961521560919
ROC train: 0.984750	val: 0.784390	test: 0.816602
PRC train: 0.920120	val: 0.606664	test: 0.666966

Epoch: 75
Loss: 0.11367674283582516
ROC train: 0.981548	val: 0.815644	test: 0.823691
PRC train: 0.910250	val: 0.620156	test: 0.655611

Epoch: 76
Loss: 0.1251708825187509
ROC train: 0.983314	val: 0.829068	test: 0.829387
PRC train: 0.912373	val: 0.626062	test: 0.656731

Epoch: 77
Loss: 0.10620088707672368
ROC train: 0.983342	val: 0.810538	test: 0.860080
PRC train: 0.910598	val: 0.618252	test: 0.674863

Epoch: 78
Loss: 0.10640381907847515
ROC train: 0.984621	val: 0.838746	test: 0.851978
PRC train: 0.914989	val: 0.622958	test: 0.679751

Epoch: 79
Loss: 0.11581125498008353
ROC train: 0.984852	val: 0.832564	test: 0.832234
PRC train: 0.916744	val: 0.613346	test: 0.652827

Epoch: 80
Loss: 0.11440093931696307
ROC train: 0.985381	val: 0.806416	test: 0.841553
PRC train: 0.918544	val: 0.605827	test: 0.670556

Epoch: 81
Loss: 0.10280177118375715
ROC train: 0.984495	val: 0.787099	test: 0.838580
PRC train: 0.918451	val: 0.608187	test: 0.667692

Epoch: 82
Loss: 0.10166503800881606
ROC train: 0.985471	val: 0.788747	test: 0.832884
PRC train: 0.919040	val: 0.612556	test: 0.661125

Epoch: 83
Loss: 0.11076371190855658
ROC train: 0.986429	val: 0.775323	test: 0.858317
PRC train: 0.924433	val: 0.603971	test: 0.681539

Epoch: 84
Loss: 0.10766716104586962
ROC train: 0.986168	val: 0.788522	test: 0.861403
PRC train: 0.922202	val: 0.599909	test: 0.674240

Epoch: 85
Loss: 0.09888575633912855
ROC train: 0.985200	val: 0.796939	test: 0.856338
PRC train: 0.919555	val: 0.595072	test: 0.679414

Epoch: 86
Loss: 0.1070012554045312
ROC train: 0.984971	val: 0.825346	test: 0.848755
PRC train: 0.918763	val: 0.591045	test: 0.670219

Epoch: 87
Loss: 0.11571812872822347
ROC train: 0.985034	val: 0.768819	test: 0.853346
PRC train: 0.917907	val: 0.576473	test: 0.672859

Epoch: 88
Loss: 0.10270367982403719
ROC train: 0.986012	val: 0.777123	test: 0.863464
PRC train: 0.916975	val: 0.575319	test: 0.686058

Epoch: 89
Loss: 0.10022375001072806
ROC train: 0.986727	val: 0.776722	test: 0.830224
PRC train: 0.924499	val: 0.579397	test: 0.648888

Epoch: 90
Loss: 0.10788833194627387
ROC train: 0.986396	val: 0.763548	test: 0.828838
PRC train: 0.924328	val: 0.622140	test: 0.655096

Epoch: 91
Loss: 0.11747697950650124
ROC train: 0.986228	val: 0.721354	test: 0.849177
PRC train: 0.923319	val: 0.619032	test: 0.655773

Epoch: 92
Loss: 0.11130780373556107
ROC train: 0.985481	val: 0.708204	test: 0.867864
PRC train: 0.920790	val: 0.625224	test: 0.671755

Epoch: 93
Loss: 0.10731888069328392
ROC train: 0.985948	val: 0.744505	test: 0.836832
PRC train: 0.922953	val: 0.597977	test: 0.650690

Epoch: 94
Loss: 0.10918563446290457
ROC train: 0.956613	val: 0.835137	test: 0.750901
PRC train: 0.826595	val: 0.598318	test: 0.557697

Epoch: 34
Loss: 0.1615201555429113
ROC train: 0.952612	val: 0.811462	test: 0.747390
PRC train: 0.821515	val: 0.595979	test: 0.562027

Epoch: 35
Loss: 0.15948194522841283
ROC train: 0.961063	val: 0.768320	test: 0.778281
PRC train: 0.839105	val: 0.590714	test: 0.570357

Epoch: 36
Loss: 0.1551928573889244
ROC train: 0.964175	val: 0.825859	test: 0.764816
PRC train: 0.849900	val: 0.615325	test: 0.568176

Epoch: 37
Loss: 0.15730898212148997
ROC train: 0.964864	val: 0.845064	test: 0.757558
PRC train: 0.857034	val: 0.667452	test: 0.567472

Epoch: 38
Loss: 0.153389599866491
ROC train: 0.960986	val: 0.859686	test: 0.744854
PRC train: 0.844085	val: 0.663960	test: 0.568316

Epoch: 39
Loss: 0.15136984218829327
ROC train: 0.964390	val: 0.837459	test: 0.742968
PRC train: 0.857203	val: 0.613905	test: 0.561237

Epoch: 40
Loss: 0.15810456530440936
ROC train: 0.958601	val: 0.827933	test: 0.723993
PRC train: 0.839608	val: 0.579335	test: 0.556319

Epoch: 41
Loss: 0.15705708720713024
ROC train: 0.961942	val: 0.863256	test: 0.728408
PRC train: 0.837666	val: 0.610057	test: 0.555266

Epoch: 42
Loss: 0.14765279877778456
ROC train: 0.965182	val: 0.823923	test: 0.745190
PRC train: 0.842733	val: 0.589901	test: 0.559728

Epoch: 43
Loss: 0.15961383833898415
ROC train: 0.967775	val: 0.806929	test: 0.751537
PRC train: 0.855722	val: 0.580034	test: 0.571159

Epoch: 44
Loss: 0.15500153215693965
ROC train: 0.970790	val: 0.803144	test: 0.795370
PRC train: 0.866579	val: 0.615004	test: 0.604942

Epoch: 45
Loss: 0.14712851052929204
ROC train: 0.971055	val: 0.779807	test: 0.803465
PRC train: 0.862290	val: 0.584427	test: 0.594406

Epoch: 46
Loss: 0.1434105946103192
ROC train: 0.970393	val: 0.765523	test: 0.800342
PRC train: 0.860623	val: 0.566330	test: 0.591436

Epoch: 47
Loss: 0.13174680203227235
ROC train: 0.973804	val: 0.775387	test: 0.813632
PRC train: 0.873370	val: 0.609521	test: 0.609080

Epoch: 48
Loss: 0.13849504265092827
ROC train: 0.973783	val: 0.774076	test: 0.796719
PRC train: 0.873447	val: 0.634330	test: 0.597479

Epoch: 49
Loss: 0.1471667854397182
ROC train: 0.972465	val: 0.805580	test: 0.798930
PRC train: 0.874532	val: 0.636764	test: 0.587648

Epoch: 50
Loss: 0.14185772555490436
ROC train: 0.973045	val: 0.855539	test: 0.809598
PRC train: 0.869365	val: 0.617491	test: 0.599792

Epoch: 51
Loss: 0.13258439123236748
ROC train: 0.972039	val: 0.836335	test: 0.817311
PRC train: 0.870754	val: 0.604183	test: 0.599088

Epoch: 52
Loss: 0.13281000088692238
ROC train: 0.968654	val: 0.817967	test: 0.781441
PRC train: 0.869580	val: 0.606383	test: 0.589687

Epoch: 53
Loss: 0.14074020096872503
ROC train: 0.970211	val: 0.849334	test: 0.780130
PRC train: 0.868963	val: 0.640234	test: 0.589235

Epoch: 54
Loss: 0.13246915025953648
ROC train: 0.974365	val: 0.848248	test: 0.785139
PRC train: 0.878042	val: 0.633537	test: 0.591844

Epoch: 55
Loss: 0.13369179072461304
ROC train: 0.977748	val: 0.816544	test: 0.782379
PRC train: 0.885876	val: 0.592366	test: 0.589664

Epoch: 56
Loss: 0.1300860218875014
ROC train: 0.978223	val: 0.783465	test: 0.813120
PRC train: 0.887236	val: 0.581734	test: 0.608817

Epoch: 57
Loss: 0.13695837396211122
ROC train: 0.977008	val: 0.816294	test: 0.832495
PRC train: 0.887922	val: 0.589356	test: 0.620349

Epoch: 58
Loss: 0.1327289715682708
ROC train: 0.979071	val: 0.809663	test: 0.834169
PRC train: 0.894213	val: 0.599772	test: 0.629588

Epoch: 59
Loss: 0.1176796528216271
ROC train: 0.978774	val: 0.824622	test: 0.809135
PRC train: 0.894084	val: 0.608005	test: 0.611982

Epoch: 60
Loss: 0.1344365595262326
ROC train: 0.980710	val: 0.810025	test: 0.809172
PRC train: 0.901025	val: 0.608667	test: 0.609693

Epoch: 61
Loss: 0.122137489494699
ROC train: 0.980037	val: 0.786625	test: 0.812295
PRC train: 0.896052	val: 0.594382	test: 0.602515

Epoch: 62
Loss: 0.11977547147213481
ROC train: 0.980149	val: 0.775636	test: 0.824737
PRC train: 0.895168	val: 0.603918	test: 0.613239

Epoch: 63
Loss: 0.13563622957390153
ROC train: 0.980042	val: 0.777397	test: 0.815724
PRC train: 0.898627	val: 0.599490	test: 0.628241

Epoch: 64
Loss: 0.1118295630603551
ROC train: 0.979998	val: 0.775186	test: 0.837329
PRC train: 0.895186	val: 0.598517	test: 0.672086

Epoch: 65
Loss: 0.12560941425846464
ROC train: 0.979212	val: 0.769205	test: 0.820333
PRC train: 0.894079	val: 0.592286	test: 0.667124

Epoch: 66
Loss: 0.12876012815517876
ROC train: 0.979487	val: 0.762637	test: 0.798411
PRC train: 0.891892	val: 0.588433	test: 0.659874

Epoch: 67
Loss: 0.146463632501554
ROC train: 0.980436	val: 0.799012	test: 0.778692
PRC train: 0.897161	val: 0.630288	test: 0.665739

Epoch: 68
Loss: 0.13719043800367367
ROC train: 0.978247	val: 0.778658	test: 0.766751
PRC train: 0.889631	val: 0.600795	test: 0.625450

Epoch: 69
Loss: 0.1258378478175684
ROC train: 0.978758	val: 0.816480	test: 0.792621
PRC train: 0.896725	val: 0.631228	test: 0.634036

Epoch: 70
Loss: 0.11786792894964557
ROC train: 0.980483	val: 0.830491	test: 0.821409
PRC train: 0.901459	val: 0.626119	test: 0.629995

Epoch: 71
Loss: 0.1301889322974987
ROC train: 0.981568	val: 0.782766	test: 0.810610
PRC train: 0.904581	val: 0.603894	test: 0.612413

Epoch: 72
Loss: 0.1305503145588373
ROC train: 0.980589	val: 0.813159	test: 0.810760
PRC train: 0.896962	val: 0.639683	test: 0.605645

Epoch: 73
Loss: 0.12278987341886327
ROC train: 0.978415	val: 0.821713	test: 0.803595
PRC train: 0.890018	val: 0.603160	test: 0.613536

Epoch: 74
Loss: 0.12605968249605823
ROC train: 0.981515	val: 0.798112	test: 0.812646
PRC train: 0.902643	val: 0.627413	test: 0.623861

Epoch: 75
Loss: 0.1291244997595732
ROC train: 0.982726	val: 0.776673	test: 0.800316
PRC train: 0.909850	val: 0.613747	test: 0.611638

Epoch: 76
Loss: 0.11745185022863462
ROC train: 0.981532	val: 0.799848	test: 0.797436
PRC train: 0.902037	val: 0.638045	test: 0.619973

Epoch: 77
Loss: 0.11854271992373469
ROC train: 0.981371	val: 0.832227	test: 0.784751
PRC train: 0.902964	val: 0.630653	test: 0.606218

Epoch: 78
Loss: 0.12293202045892
ROC train: 0.981244	val: 0.825571	test: 0.784676
PRC train: 0.905406	val: 0.626030	test: 0.610191

Epoch: 79
Loss: 0.1239569405807928
ROC train: 0.984582	val: 0.779357	test: 0.828054
PRC train: 0.916103	val: 0.607434	test: 0.671762

Epoch: 80
Loss: 0.12025652866499792
ROC train: 0.982890	val: 0.773289	test: 0.838722
PRC train: 0.910107	val: 0.607112	test: 0.684405

Epoch: 81
Loss: 0.12285286759871986
ROC train: 0.983684	val: 0.779133	test: 0.820303
PRC train: 0.916501	val: 0.611021	test: 0.657390

Epoch: 82
Loss: 0.12066123341211836
ROC train: 0.983590	val: 0.777260	test: 0.822089
PRC train: 0.913630	val: 0.604718	test: 0.652605

Epoch: 83
Loss: 0.11606298127083152
ROC train: 0.984450	val: 0.783627	test: 0.819803
PRC train: 0.914912	val: 0.608401	test: 0.657404

Epoch: 84
Loss: 0.12054103241673805
ROC train: 0.984401	val: 0.790033	test: 0.800790
PRC train: 0.919898	val: 0.602074	test: 0.638402

Epoch: 85
Loss: 0.11896147662594606
ROC train: 0.984216	val: 0.780218	test: 0.807036
PRC train: 0.919407	val: 0.593701	test: 0.672839

Epoch: 86
Loss: 0.12202774313701115
ROC train: 0.984386	val: 0.790146	test: 0.811970
PRC train: 0.917721	val: 0.635889	test: 0.691627

Epoch: 87
Loss: 0.10584472692103668
ROC train: 0.983419	val: 0.811736	test: 0.772122
PRC train: 0.912355	val: 0.639722	test: 0.646097

Epoch: 88
Loss: 0.12213039241713512
ROC train: 0.984303	val: 0.806142	test: 0.746558
PRC train: 0.914169	val: 0.648893	test: 0.624802

Epoch: 89
Loss: 0.1082774676225681
ROC train: 0.983199	val: 0.786175	test: 0.759568
PRC train: 0.911822	val: 0.654751	test: 0.620350

Epoch: 90
Loss: 0.12343763159610954
ROC train: 0.985069	val: 0.801496	test: 0.796325
PRC train: 0.920583	val: 0.649458	test: 0.662088

Epoch: 91
Loss: 0.10941439250501113
ROC train: 0.985644	val: 0.797863	test: 0.793477
PRC train: 0.925676	val: 0.598733	test: 0.660540

Epoch: 92
Loss: 0.10438139074114046
ROC train: 0.984934	val: 0.780668	test: 0.818510
PRC train: 0.917535	val: 0.601391	test: 0.663247

Epoch: 93
Loss: 0.10421502623522252
ROC train: 0.984759	val: 0.761151	test: 0.825200
PRC train: 0.915311	val: 0.588572	test: 0.671479

Epoch: 94
Loss: 0.1083697314665358
PRC train: 0.972699	val: 0.567753	test: 0.549470

Epoch: 33
Loss: 0.10319688530201723
ROC train: 0.995149	val: 0.746241	test: 0.683730
PRC train: 0.979278	val: 0.596774	test: 0.544931

Epoch: 34
Loss: 0.1027420045690313
ROC train: 0.994509	val: 0.753982	test: 0.738620
PRC train: 0.976299	val: 0.666112	test: 0.563128

Epoch: 35
Loss: 0.08191163942824445
ROC train: 0.996286	val: 0.772688	test: 0.713698
PRC train: 0.983596	val: 0.607671	test: 0.557979

Epoch: 36
Loss: 0.08909563731806507
ROC train: 0.996717	val: 0.774311	test: 0.704543
PRC train: 0.985415	val: 0.573576	test: 0.558562

Epoch: 37
Loss: 0.08116074274060502
ROC train: 0.999127	val: 0.730009	test: 0.705880
PRC train: 0.994444	val: 0.540125	test: 0.565217

Epoch: 38
Loss: 0.09025411900138836
ROC train: 0.999433	val: 0.602645	test: 0.693938
PRC train: 0.995914	val: 0.517035	test: 0.556513

Epoch: 39
Loss: 0.08684714061656426
ROC train: 0.998653	val: 0.670235	test: 0.669181
PRC train: 0.991774	val: 0.525747	test: 0.553993

Epoch: 40
Loss: 0.08243722602742623
ROC train: 0.995471	val: 0.693610	test: 0.651154
PRC train: 0.979481	val: 0.530265	test: 0.544769

Epoch: 41
Loss: 0.08558949965532492
ROC train: 0.999045	val: 0.745218	test: 0.713377
PRC train: 0.996067	val: 0.555044	test: 0.584304

Epoch: 42
Loss: 0.07700400177108521
ROC train: 0.997287	val: 0.731682	test: 0.698163
PRC train: 0.989494	val: 0.572432	test: 0.551830

Epoch: 43
Loss: 0.06624651235291049
ROC train: 0.998344	val: 0.721593	test: 0.698204
PRC train: 0.992762	val: 0.536903	test: 0.551473

Epoch: 44
Loss: 0.0685836577395235
ROC train: 0.998853	val: 0.755469	test: 0.718072
PRC train: 0.994437	val: 0.565950	test: 0.561033

Epoch: 45
Loss: 0.06286998132236196
ROC train: 0.999073	val: 0.718458	test: 0.706728
PRC train: 0.995686	val: 0.555011	test: 0.563941

Epoch: 46
Loss: 0.06448161499022705
ROC train: 0.999098	val: 0.681185	test: 0.692919
PRC train: 0.994996	val: 0.550856	test: 0.558703

Epoch: 47
Loss: 0.06505065286260862
ROC train: 0.998135	val: 0.637181	test: 0.696942
PRC train: 0.991070	val: 0.534430	test: 0.561283

Epoch: 48
Loss: 0.06029466438474797
ROC train: 0.999171	val: 0.668636	test: 0.699721
PRC train: 0.994953	val: 0.539664	test: 0.550526

Epoch: 49
Loss: 0.06511320068326565
ROC train: 0.999532	val: 0.636844	test: 0.712749
PRC train: 0.996972	val: 0.545254	test: 0.553013

Epoch: 50
Loss: 0.060595266490037825
ROC train: 0.999231	val: 0.646371	test: 0.715468
PRC train: 0.994292	val: 0.618107	test: 0.554318

Epoch: 51
Loss: 0.05933702562547592
ROC train: 0.999656	val: 0.680534	test: 0.716499
PRC train: 0.997627	val: 0.574745	test: 0.552857

Epoch: 52
Loss: 0.05565975884097479
ROC train: 0.999804	val: 0.628129	test: 0.720369
PRC train: 0.998919	val: 0.519637	test: 0.563717

Epoch: 53
Loss: 0.05538636023317158
ROC train: 0.999690	val: 0.661843	test: 0.732524
PRC train: 0.997434	val: 0.530876	test: 0.569275

Epoch: 54
Loss: 0.050826180913744565
ROC train: 0.999681	val: 0.684744	test: 0.685273
PRC train: 0.997172	val: 0.531011	test: 0.555204

Epoch: 55
Loss: 0.047754920585470685
ROC train: 0.999639	val: 0.676915	test: 0.672700
PRC train: 0.997104	val: 0.530196	test: 0.553466

Epoch: 56
Loss: 0.04446832331406885
ROC train: 0.999714	val: 0.704536	test: 0.666604
PRC train: 0.997223	val: 0.554175	test: 0.545744

Epoch: 57
Loss: 0.044618628417655085
ROC train: 0.999821	val: 0.646497	test: 0.626330
PRC train: 0.998586	val: 0.548449	test: 0.536713

Epoch: 58
Loss: 0.04344854183107179
ROC train: 0.999796	val: 0.570027	test: 0.626879
PRC train: 0.998658	val: 0.536560	test: 0.540315

Epoch: 59
Loss: 0.04137072612176239
ROC train: 0.999891	val: 0.632824	test: 0.674511
PRC train: 0.999369	val: 0.552187	test: 0.549695

Epoch: 60
Loss: 0.045467027637123326
ROC train: 0.999959	val: 0.670372	test: 0.685078
PRC train: 0.999728	val: 0.568333	test: 0.552577

Epoch: 61
Loss: 0.03989397982729505
ROC train: 0.999985	val: 0.654639	test: 0.668826
PRC train: 0.999892	val: 0.554288	test: 0.557036

Epoch: 62
Loss: 0.049030433742385734
ROC train: 0.999937	val: 0.670235	test: 0.664504
PRC train: 0.999680	val: 0.527936	test: 0.558546

Epoch: 63
Loss: 0.04684487341654212
ROC train: 1.000000	val: 0.674954	test: 0.645406
PRC train: 1.000000	val: 0.539861	test: 0.545764

Epoch: 64
Loss: 0.03541714127475355
ROC train: 0.999897	val: 0.674592	test: 0.608329
PRC train: 0.999671	val: 0.543868	test: 0.527686

Epoch: 65
Loss: 0.032577696222543966
ROC train: 0.999830	val: 0.655750	test: 0.621784
PRC train: 0.999602	val: 0.542135	test: 0.534255

Epoch: 66
Loss: 0.04257690046676142
ROC train: 0.999989	val: 0.677663	test: 0.666111
PRC train: 0.999999	val: 0.552857	test: 0.545616

Epoch: 67
Loss: 0.04230329663552409
ROC train: 0.999994	val: 0.686305	test: 0.676872
PRC train: 1.000000	val: 0.580951	test: 0.551756

Epoch: 68
Loss: 0.026272386053454055
ROC train: 0.999994	val: 0.694134	test: 0.689826
PRC train: 1.000000	val: 0.590209	test: 0.553103

Epoch: 69
Loss: 0.029092704769927737
ROC train: 1.000000	val: 0.702887	test: 0.694585
PRC train: 1.000000	val: 0.537750	test: 0.552228

Epoch: 70
Loss: 0.040225528157443874
ROC train: 1.000000	val: 0.739736	test: 0.700681
PRC train: 1.000000	val: 0.544382	test: 0.552159

Epoch: 71
Loss: 0.031856698500073055
ROC train: 1.000000	val: 0.764198	test: 0.696508
PRC train: 1.000000	val: 0.570433	test: 0.550760

Epoch: 72
Loss: 0.023781559822364018
ROC train: 0.999983	val: 0.774487	test: 0.699026
PRC train: 0.999999	val: 0.605465	test: 0.555437

Epoch: 73
Loss: 0.03181634587920379
ROC train: 0.999977	val: 0.764535	test: 0.705940
PRC train: 0.999998	val: 0.600759	test: 0.557928

Epoch: 74
Loss: 0.026416654933130897
ROC train: 0.999945	val: 0.770154	test: 0.695496
PRC train: 0.999598	val: 0.593258	test: 0.548067

Epoch: 75
Loss: 0.036273330091947456
ROC train: 0.999983	val: 0.744905	test: 0.717732
PRC train: 0.999999	val: 0.621914	test: 0.556996

Epoch: 76
Loss: 0.03306006554871182
ROC train: 0.999994	val: 0.700726	test: 0.731400
PRC train: 1.000000	val: 0.606165	test: 0.571304

Epoch: 77
Loss: 0.03227627407734705
ROC train: 0.999972	val: 0.717485	test: 0.723354
PRC train: 0.999998	val: 0.552748	test: 0.565244

Epoch: 78
Loss: 0.028205572833786115
ROC train: 0.999994	val: 0.726038	test: 0.685915
PRC train: 1.000000	val: 0.543415	test: 0.549090

Epoch: 79
Loss: 0.03137017927670842
ROC train: 0.999995	val: 0.713314	test: 0.678807
PRC train: 0.999946	val: 0.541217	test: 0.545770

Epoch: 80
Loss: 0.04608923796902113
ROC train: 1.000000	val: 0.729897	test: 0.694047
PRC train: 1.000000	val: 0.547341	test: 0.554951

Epoch: 81
Loss: 0.0330136906999141
ROC train: 0.999752	val: 0.725427	test: 0.703871
PRC train: 0.998872	val: 0.539118	test: 0.565915

Epoch: 82
Loss: 0.037950022251985016
ROC train: 0.999994	val: 0.701426	test: 0.695459
PRC train: 1.000000	val: 0.560559	test: 0.568439

Epoch: 83
Loss: 0.030677471289559433
ROC train: 0.999961	val: 0.721779	test: 0.627103
PRC train: 0.999944	val: 0.583524	test: 0.536805

Epoch: 84
Loss: 0.024865449552851016
ROC train: 0.999943	val: 0.745556	test: 0.599977
PRC train: 0.999996	val: 0.623289	test: 0.530900

Epoch: 85
Loss: 0.02384620413809535
ROC train: 0.999949	val: 0.741785	test: 0.607541
PRC train: 0.999611	val: 0.616424	test: 0.531531

Epoch: 86
Loss: 0.03206516238410499
ROC train: 1.000000	val: 0.767020	test: 0.619984
PRC train: 1.000000	val: 0.593002	test: 0.532597

Epoch: 87
Loss: 0.025133496533951073
ROC train: 0.999994	val: 0.765596	test: 0.641219
PRC train: 1.000000	val: 0.622279	test: 0.537170

Epoch: 88
Loss: 0.025974036775552184
ROC train: 1.000000	val: 0.760153	test: 0.633812
PRC train: 1.000000	val: 0.583386	test: 0.536532

Epoch: 89
Loss: 0.021880515064318366
ROC train: 0.999977	val: 0.729735	test: 0.614937
PRC train: 0.999998	val: 0.569866	test: 0.536725

Epoch: 90
Loss: 0.03537529516157985
ROC train: 0.999983	val: 0.772864	test: 0.630215
PRC train: 0.999999	val: 0.588327	test: 0.543047

Epoch: 91
Loss: 0.020606772058982443
ROC train: 0.999995	val: 0.774624	test: 0.640333
PRC train: 0.999946	val: 0.579057	test: 0.545754

Epoch: 92
Loss: 0.024063853500372257
ROC train: 0.999986	val: 0.752173	test: 0.663511
PRC train: 0.999837	val: 0.585562	test: 0.554479

Epoch: 93
Loss: 0.024544808778915135
PRC train: 0.989623	val: 0.600481	test: 0.546272

Epoch: 33
Loss: 0.08593359562917913
ROC train: 0.998481	val: 0.797290	test: 0.704259
PRC train: 0.992169	val: 0.631190	test: 0.552403

Epoch: 34
Loss: 0.0838180548307889
ROC train: 0.998305	val: 0.838633	test: 0.708454
PRC train: 0.989863	val: 0.573058	test: 0.551046

Epoch: 35
Loss: 0.07693602355249929
ROC train: 0.997191	val: 0.810988	test: 0.679001
PRC train: 0.986242	val: 0.583820	test: 0.541347

Epoch: 36
Loss: 0.08785909875150646
ROC train: 0.998490	val: 0.785514	test: 0.659237
PRC train: 0.991128	val: 0.557368	test: 0.535466

Epoch: 37
Loss: 0.087556299297186
ROC train: 0.997015	val: 0.836247	test: 0.687708
PRC train: 0.985140	val: 0.592176	test: 0.541141

Epoch: 38
Loss: 0.07282448280640963
ROC train: 0.995098	val: 0.810974	test: 0.663238
PRC train: 0.978116	val: 0.559804	test: 0.535529

Epoch: 39
Loss: 0.07452314248694629
ROC train: 0.996534	val: 0.795863	test: 0.684197
PRC train: 0.982861	val: 0.562818	test: 0.540632

Epoch: 40
Loss: 0.08380380400898911
ROC train: 0.999199	val: 0.815194	test: 0.700684
PRC train: 0.994329	val: 0.654923	test: 0.549256

Epoch: 41
Loss: 0.06555267057101968
ROC train: 0.995507	val: 0.716062	test: 0.744567
PRC train: 0.985026	val: 0.614883	test: 0.569312

Epoch: 42
Loss: 0.07874754639354119
ROC train: 0.998736	val: 0.677115	test: 0.734687
PRC train: 0.992682	val: 0.604375	test: 0.561023

Epoch: 43
Loss: 0.07526200532957225
ROC train: 0.999274	val: 0.678113	test: 0.732703
PRC train: 0.995546	val: 0.532576	test: 0.553041

Epoch: 44
Loss: 0.08352078728193417
ROC train: 0.999663	val: 0.777260	test: 0.706198
PRC train: 0.997105	val: 0.594257	test: 0.547137

Epoch: 45
Loss: 0.055886881987167056
ROC train: 0.999447	val: 0.804494	test: 0.692844
PRC train: 0.995602	val: 0.578001	test: 0.543167

Epoch: 46
Loss: 0.05764273862436751
ROC train: 0.999397	val: 0.814309	test: 0.715786
PRC train: 0.996027	val: 0.642081	test: 0.550690

Epoch: 47
Loss: 0.047771037572978126
ROC train: 0.999510	val: 0.740636	test: 0.727212
PRC train: 0.997121	val: 0.626127	test: 0.554236

Epoch: 48
Loss: 0.06122084876136572
ROC train: 0.998761	val: 0.774237	test: 0.745676
PRC train: 0.992711	val: 0.634210	test: 0.558211

Epoch: 49
Loss: 0.06009366938467311
ROC train: 0.999614	val: 0.810475	test: 0.703232
PRC train: 0.998164	val: 0.574346	test: 0.553316

Epoch: 50
Loss: 0.0701890770112156
ROC train: 0.999127	val: 0.801046	test: 0.723694
PRC train: 0.996198	val: 0.573584	test: 0.559702

Epoch: 51
Loss: 0.058641012549827363
ROC train: 0.999206	val: 0.759116	test: 0.742008
PRC train: 0.995938	val: 0.623013	test: 0.571398

Epoch: 52
Loss: 0.05912360232069898
ROC train: 0.999580	val: 0.758392	test: 0.701020
PRC train: 0.997124	val: 0.617996	test: 0.557769

Epoch: 53
Loss: 0.03992916011202881
ROC train: 0.999924	val: 0.785201	test: 0.673682
PRC train: 0.999424	val: 0.651864	test: 0.547296

Epoch: 54
Loss: 0.04797075970314606
ROC train: 0.999651	val: 0.808103	test: 0.705095
PRC train: 0.998596	val: 0.667254	test: 0.553864

Epoch: 55
Loss: 0.04709642494694268
ROC train: 0.999739	val: 0.821326	test: 0.713212
PRC train: 0.998008	val: 0.589573	test: 0.560683

Epoch: 56
Loss: 0.05048500707599483
ROC train: 0.999803	val: 0.841230	test: 0.715723
PRC train: 0.998639	val: 0.642432	test: 0.563512

Epoch: 57
Loss: 0.03850064394321105
ROC train: 0.999562	val: 0.818005	test: 0.706616
PRC train: 0.999346	val: 0.694830	test: 0.557677

Epoch: 58
Loss: 0.0632200989236646
ROC train: 0.999788	val: 0.833513	test: 0.715424
PRC train: 0.998561	val: 0.660136	test: 0.560877

Epoch: 59
Loss: 0.03990105621854314
ROC train: 0.999855	val: 0.806142	test: 0.703370
PRC train: 0.999206	val: 0.568549	test: 0.554441

Epoch: 60
Loss: 0.040868036406100525
ROC train: 0.999854	val: 0.812010	test: 0.693539
PRC train: 0.999027	val: 0.561913	test: 0.547947

Epoch: 61
Loss: 0.033701347465790094
ROC train: 0.999852	val: 0.824422	test: 0.714763
PRC train: 0.999090	val: 0.634731	test: 0.551682

Epoch: 62
Loss: 0.035034259519002214
ROC train: 0.999876	val: 0.803731	test: 0.711188
PRC train: 0.999083	val: 0.629971	test: 0.553963

Epoch: 63
Loss: 0.04319014040854615
ROC train: 0.999980	val: 0.785201	test: 0.698847
PRC train: 0.999842	val: 0.642273	test: 0.550580

Epoch: 64
Loss: 0.04437837280763649
ROC train: 0.998008	val: 0.839455	test: 0.703934
PRC train: 0.992746	val: 0.659514	test: 0.552067

Epoch: 65
Loss: 0.04070459988786345
ROC train: 0.999845	val: 0.863232	test: 0.708065
PRC train: 0.999447	val: 0.644759	test: 0.553365

Epoch: 66
Loss: 0.03851149725340965
ROC train: 0.999994	val: 0.863207	test: 0.702406
PRC train: 1.000000	val: 0.651221	test: 0.547344

Epoch: 67
Loss: 0.03365317468906488
ROC train: 0.999977	val: 0.841792	test: 0.690203
PRC train: 0.999998	val: 0.582339	test: 0.544161

Epoch: 68
Loss: 0.03137697043378178
ROC train: 0.999858	val: 0.791495	test: 0.708566
PRC train: 0.999990	val: 0.620847	test: 0.546864

Epoch: 69
Loss: 0.037437350488066554
ROC train: 0.999989	val: 0.785289	test: 0.749654
PRC train: 0.999999	val: 0.621972	test: 0.559891

Epoch: 70
Loss: 0.0400262976948212
ROC train: 0.999938	val: 0.781093	test: 0.757786
PRC train: 0.999995	val: 0.617488	test: 0.565864

Epoch: 71
Loss: 0.03631631774878581
ROC train: 0.999994	val: 0.795765	test: 0.720698
PRC train: 1.000000	val: 0.628057	test: 0.567079

Epoch: 72
Loss: 0.03115192620995008
ROC train: 0.999825	val: 0.826819	test: 0.714538
PRC train: 0.998666	val: 0.640931	test: 0.570332

Epoch: 73
Loss: 0.028658883439354372
ROC train: 0.999903	val: 0.817229	test: 0.744417
PRC train: 0.999123	val: 0.636326	test: 0.566941

Epoch: 74
Loss: 0.027099097747853212
ROC train: 1.000000	val: 0.807228	test: 0.725554
PRC train: 1.000000	val: 0.629662	test: 0.557477

Epoch: 75
Loss: 0.020111989921567652
ROC train: 0.999947	val: 0.825870	test: 0.740857
PRC train: 0.999465	val: 0.633875	test: 0.559907

Epoch: 76
Loss: 0.027698971110616737
ROC train: 0.999947	val: 0.814969	test: 0.746916
PRC train: 0.999465	val: 0.626744	test: 0.561845

Epoch: 77
Loss: 0.028575059036504147
ROC train: 0.999989	val: 0.785725	test: 0.701932
PRC train: 0.999999	val: 0.577137	test: 0.558210

Epoch: 78
Loss: 0.025913725889334792
ROC train: 1.000000	val: 0.838032	test: 0.683394
PRC train: 1.000000	val: 0.640980	test: 0.556300

Epoch: 79
Loss: 0.035753022569513314
ROC train: 0.999981	val: 0.842653	test: 0.679797
PRC train: 0.999782	val: 0.711214	test: 0.552700

Epoch: 80
Loss: 0.02400063235433191
ROC train: 0.999981	val: 0.858337	test: 0.689564
PRC train: 0.999782	val: 0.646155	test: 0.555574

Epoch: 81
Loss: 0.03686568580580356
ROC train: 1.000000	val: 0.851906	test: 0.727690
PRC train: 1.000000	val: 0.657511	test: 0.560036

Epoch: 82
Loss: 0.025109882576675974
ROC train: 1.000000	val: 0.824535	test: 0.714994
PRC train: 1.000000	val: 0.648444	test: 0.556364

Epoch: 83
Loss: 0.02897501354419314
ROC train: 0.999969	val: 0.848322	test: 0.693064
PRC train: 0.999785	val: 0.611102	test: 0.545012

Epoch: 84
Loss: 0.02671336559095621
ROC train: 0.999841	val: 0.843588	test: 0.674115
PRC train: 0.998629	val: 0.604784	test: 0.538833

Epoch: 85
Loss: 0.027335050953287095
ROC train: 1.000000	val: 0.828330	test: 0.697550
PRC train: 1.000000	val: 0.649640	test: 0.553003

Epoch: 86
Loss: 0.02324897422858487
ROC train: 0.999994	val: 0.824060	test: 0.712503
PRC train: 1.000000	val: 0.639167	test: 0.567114

Epoch: 87
Loss: 0.025791730063364826
ROC train: 1.000000	val: 0.824310	test: 0.710090
PRC train: 1.000000	val: 0.634393	test: 0.570459

Epoch: 88
Loss: 0.025266546387839078
ROC train: 0.999986	val: 0.844214	test: 0.711375
PRC train: 0.999842	val: 0.585473	test: 0.559427

Epoch: 89
Loss: 0.025837191643399177
ROC train: 0.999979	val: 0.846761	test: 0.695660
PRC train: 0.999891	val: 0.576638	test: 0.555783

Epoch: 90
Loss: 0.021477738073363624
ROC train: 1.000000	val: 0.831777	test: 0.680708
PRC train: 1.000000	val: 0.565036	test: 0.551775

Epoch: 91
Loss: 0.023967747243826398
ROC train: 0.999968	val: 0.837108	test: 0.694219
PRC train: 0.999890	val: 0.576410	test: 0.558007

Epoch: 92
Loss: 0.027878631462259818
ROC train: 0.999989	val: 0.836883	test: 0.699848
PRC train: 0.999999	val: 0.587997	test: 0.565142

Epoch: 93
Loss: 0.02233099295584629
PRC train: 0.995407	val: 0.556499	test: 0.563156

Epoch: 33
Loss: 0.07699844029620975
ROC train: 0.999552	val: 0.707821	test: 0.703837
PRC train: 0.997435	val: 0.550853	test: 0.566077

Epoch: 34
Loss: 0.07212945538387082
ROC train: 0.999359	val: 0.674631	test: 0.693987
PRC train: 0.996378	val: 0.602693	test: 0.553889

Epoch: 35
Loss: 0.05961026539388288
ROC train: 0.999328	val: 0.590655	test: 0.686991
PRC train: 0.995973	val: 0.533077	test: 0.555460

Epoch: 36
Loss: 0.06479201898307693
ROC train: 0.999775	val: 0.578693	test: 0.665718
PRC train: 0.998882	val: 0.538957	test: 0.555847

Epoch: 37
Loss: 0.06386729288022228
ROC train: 0.999615	val: 0.564408	test: 0.648420
PRC train: 0.999562	val: 0.538024	test: 0.551563

Epoch: 38
Loss: 0.06318267096626833
ROC train: 0.999715	val: 0.587808	test: 0.635851
PRC train: 0.999719	val: 0.594558	test: 0.566816

Epoch: 39
Loss: 0.0540998769882855
ROC train: 0.999674	val: 0.563259	test: 0.627943
PRC train: 0.999454	val: 0.592789	test: 0.558206

Epoch: 40
Loss: 0.06528772694903454
ROC train: 0.999772	val: 0.572599	test: 0.620760
PRC train: 0.999409	val: 0.538135	test: 0.544128

Epoch: 41
Loss: 0.04519346678671804
ROC train: 0.999772	val: 0.610622	test: 0.641073
PRC train: 0.998418	val: 0.530981	test: 0.545716

Epoch: 42
Loss: 0.04626727798943717
ROC train: 0.999833	val: 0.601731	test: 0.689591
PRC train: 0.999132	val: 0.543657	test: 0.560055

Epoch: 43
Loss: 0.04982879079351179
ROC train: 0.999841	val: 0.612695	test: 0.710254
PRC train: 0.999034	val: 0.543447	test: 0.563513

Epoch: 44
Loss: 0.05875045243092673
ROC train: 0.999955	val: 0.653177	test: 0.721296
PRC train: 0.999997	val: 0.560646	test: 0.560586

Epoch: 45
Loss: 0.04452091658786157
ROC train: 0.999666	val: 0.735266	test: 0.732699
PRC train: 0.998404	val: 0.570326	test: 0.560537

Epoch: 46
Loss: 0.06889869829385338
ROC train: 0.999863	val: 0.778785	test: 0.723473
PRC train: 0.999082	val: 0.564591	test: 0.568432

Epoch: 47
Loss: 0.06760956763535579
ROC train: 0.999924	val: 0.734180	test: 0.735635
PRC train: 0.999788	val: 0.557565	test: 0.571652

Epoch: 48
Loss: 0.045390978312345086
ROC train: 0.999837	val: 0.641465	test: 0.719159
PRC train: 0.998664	val: 0.537380	test: 0.578024

Epoch: 49
Loss: 0.04433623293941798
ROC train: 0.999828	val: 0.581514	test: 0.703157
PRC train: 0.998592	val: 0.539833	test: 0.567786

Epoch: 50
Loss: 0.035003694562701936
ROC train: 0.999952	val: 0.597398	test: 0.706093
PRC train: 0.999483	val: 0.557151	test: 0.567616

Epoch: 51
Loss: 0.03346034558151377
ROC train: 1.000000	val: 0.612695	test: 0.702010
PRC train: 1.000000	val: 0.572150	test: 0.556806

Epoch: 52
Loss: 0.02838130656909541
ROC train: 0.999994	val: 0.598821	test: 0.737637
PRC train: 1.000000	val: 0.554910	test: 0.560084

Epoch: 53
Loss: 0.034852695271647784
ROC train: 0.999916	val: 0.596723	test: 0.732464
PRC train: 0.999569	val: 0.539512	test: 0.562138

Epoch: 54
Loss: 0.03531832007092396
ROC train: 0.999975	val: 0.638467	test: 0.686449
PRC train: 0.999781	val: 0.533570	test: 0.566889

Epoch: 55
Loss: 0.03605070223873409
ROC train: 0.999984	val: 0.705586	test: 0.691238
PRC train: 0.999945	val: 0.551535	test: 0.563652

Epoch: 56
Loss: 0.032095834964628304
ROC train: 0.999990	val: 0.692373	test: 0.734197
PRC train: 0.999894	val: 0.605432	test: 0.576732

Epoch: 57
Loss: 0.03538777148938886
ROC train: 0.999989	val: 0.657559	test: 0.723518
PRC train: 0.999999	val: 0.561371	test: 0.583550

Epoch: 58
Loss: 0.03588368690208556
ROC train: 1.000000	val: 0.674768	test: 0.718184
PRC train: 1.000000	val: 0.542236	test: 0.577987

Epoch: 59
Loss: 0.041455590583566945
ROC train: 1.000000	val: 0.700603	test: 0.739333
PRC train: 1.000000	val: 0.556021	test: 0.573924

Epoch: 60
Loss: 0.02869978424674501
ROC train: 0.999974	val: 0.737340	test: 0.724280
PRC train: 0.999841	val: 0.568743	test: 0.571628

Epoch: 61
Loss: 0.03163944058134725
ROC train: 0.999990	val: 0.703288	test: 0.686942
PRC train: 0.999892	val: 0.549763	test: 0.566184

Epoch: 62
Loss: 0.03149823069785144
ROC train: 0.999967	val: 0.634897	test: 0.694305
PRC train: 0.999944	val: 0.537260	test: 0.560256

Epoch: 63
Loss: 0.033768966158905296
ROC train: 0.999986	val: 0.615130	test: 0.744003
PRC train: 0.999842	val: 0.556661	test: 0.568209

Epoch: 64
Loss: 0.03150374457359671
ROC train: 0.999931	val: 0.613145	test: 0.763015
PRC train: 0.999420	val: 0.598485	test: 0.570902

Epoch: 65
Loss: 0.020951513342016358
ROC train: 0.999921	val: 0.632711	test: 0.765021
PRC train: 0.999324	val: 0.559838	test: 0.577813

Epoch: 66
Loss: 0.024049263188119698
ROC train: 0.999976	val: 0.707856	test: 0.764778
PRC train: 0.999742	val: 0.608493	test: 0.583508

Epoch: 67
Loss: 0.030285966132146395
ROC train: 0.999981	val: 0.782703	test: 0.753005
PRC train: 0.999792	val: 0.622066	test: 0.574030

Epoch: 68
Loss: 0.02373277696297301
ROC train: 0.999929	val: 0.740010	test: 0.751873
PRC train: 0.999507	val: 0.616164	test: 0.576823

Epoch: 69
Loss: 0.035415554524194016
ROC train: 0.999953	val: 0.714488	test: 0.735882
PRC train: 0.999741	val: 0.613697	test: 0.569658

Epoch: 70
Loss: 0.023075020757289018
ROC train: 0.999985	val: 0.730508	test: 0.721654
PRC train: 0.999892	val: 0.611249	test: 0.564405

Epoch: 71
Loss: 0.030612428608044605
ROC train: 0.999995	val: 0.751150	test: 0.717288
PRC train: 0.999946	val: 0.577818	test: 0.565943

Epoch: 72
Loss: 0.026348045112660857
ROC train: 1.000000	val: 0.671345	test: 0.730122
PRC train: 1.000000	val: 0.587926	test: 0.564893

Epoch: 73
Loss: 0.019397503535528684
ROC train: 1.000000	val: 0.597732	test: 0.765611
PRC train: 1.000000	val: 0.600029	test: 0.578664

Epoch: 74
Loss: 0.02090459760351509
ROC train: 0.999990	val: 0.560160	test: 0.763852
PRC train: 0.999892	val: 0.595113	test: 0.581380

Epoch: 75
Loss: 0.031010440449158693
ROC train: 0.999981	val: 0.553043	test: 0.741175
PRC train: 0.999792	val: 0.550891	test: 0.573271

Epoch: 76
Loss: 0.021533114657807925
ROC train: 0.999962	val: 0.582214	test: 0.734985
PRC train: 0.999599	val: 0.539335	test: 0.599933

Epoch: 77
Loss: 0.028789717703194828
ROC train: 0.999981	val: 0.538333	test: 0.705069
PRC train: 0.999786	val: 0.535614	test: 0.596725

Epoch: 78
Loss: 0.03164979163698384
ROC train: 1.000000	val: 0.614705	test: 0.697382
PRC train: 1.000000	val: 0.544213	test: 0.570202

Epoch: 79
Loss: 0.02650863781267227
ROC train: 1.000000	val: 0.621586	test: 0.703807
PRC train: 1.000000	val: 0.528897	test: 0.572868

Epoch: 80
Loss: 0.029682157335418645
ROC train: 1.000000	val: 0.614955	test: 0.725980
PRC train: 1.000000	val: 0.530787	test: 0.576776

Epoch: 81
Loss: 0.025520707239615824
ROC train: 1.000000	val: 0.637719	test: 0.739158
PRC train: 1.000000	val: 0.532534	test: 0.567431

Epoch: 82
Loss: 0.020448750022768167
ROC train: 1.000000	val: 0.669560	test: 0.747738
PRC train: 1.000000	val: 0.561008	test: 0.590074

Epoch: 83
Loss: 0.016232907715197715
ROC train: 1.000000	val: 0.635508	test: 0.731747
PRC train: 1.000000	val: 0.598715	test: 0.577374

Epoch: 84
Loss: 0.016848341025493964
ROC train: 1.000000	val: 0.597848	test: 0.715697
PRC train: 1.000000	val: 0.526298	test: 0.574893

Epoch: 85
Loss: 0.01905424257923366
ROC train: 1.000000	val: 0.639704	test: 0.721531
PRC train: 1.000000	val: 0.543766	test: 0.589426

Epoch: 86
Loss: 0.016005263619364017
ROC train: 1.000000	val: 0.600308	test: 0.733062
PRC train: 1.000000	val: 0.532742	test: 0.583910

Epoch: 87
Loss: 0.01687487665832335
ROC train: 0.999986	val: 0.585573	test: 0.723511
PRC train: 0.999842	val: 0.517284	test: 0.574601

Epoch: 88
Loss: 0.01682418735372739
ROC train: 1.000000	val: 0.642663	test: 0.735098
PRC train: 1.000000	val: 0.527446	test: 0.571344

Epoch: 89
Loss: 0.011992711566380286
ROC train: 1.000000	val: 0.753135	test: 0.756411
PRC train: 1.000000	val: 0.570462	test: 0.576323

Epoch: 90
Loss: 0.014796809629135687
ROC train: 1.000000	val: 0.754172	test: 0.762444
PRC train: 1.000000	val: 0.570774	test: 0.570519

Epoch: 91
Loss: 0.014842178755877983
ROC train: 1.000000	val: 0.648170	test: 0.768301
PRC train: 1.000000	val: 0.538179	test: 0.584398

Epoch: 92
Loss: 0.015323168019947129
ROC train: 1.000000	val: 0.627953	test: 0.768752
PRC train: 1.000000	val: 0.543422	test: 0.596382

Epoch: 93
Loss: 0.013208656034719227
PRC train: 0.981388	val: 0.548298	test: 0.536663

Epoch: 33
Loss: 0.08822172119210592
ROC train: 0.998232	val: 0.597746	test: 0.549864
PRC train: 0.987980	val: 0.536784	test: 0.541892

Epoch: 34
Loss: 0.07549713004375039
ROC train: 0.997838	val: 0.604901	test: 0.588801
PRC train: 0.985533	val: 0.521746	test: 0.538524

Epoch: 35
Loss: 0.0817793470556085
ROC train: 0.997147	val: 0.615264	test: 0.584979
PRC train: 0.983032	val: 0.532277	test: 0.549289

Epoch: 36
Loss: 0.0803041654141302
ROC train: 0.997696	val: 0.631239	test: 0.601781
PRC train: 0.990041	val: 0.545167	test: 0.560526

Epoch: 37
Loss: 0.07173788778543051
ROC train: 0.998216	val: 0.584312	test: 0.634353
PRC train: 0.991669	val: 0.519511	test: 0.554011

Epoch: 38
Loss: 0.06320465676381304
ROC train: 0.998533	val: 0.632785	test: 0.650623
PRC train: 0.991077	val: 0.518914	test: 0.571249

Epoch: 39
Loss: 0.07089892004039124
ROC train: 0.997792	val: 0.624481	test: 0.655526
PRC train: 0.990859	val: 0.520349	test: 0.569276

Epoch: 40
Loss: 0.05230304121864343
ROC train: 0.999041	val: 0.583676	test: 0.671922
PRC train: 0.994614	val: 0.517376	test: 0.582079

Epoch: 41
Loss: 0.06079418751685399
ROC train: 0.999142	val: 0.550622	test: 0.664358
PRC train: 0.996462	val: 0.512875	test: 0.576227

Epoch: 42
Loss: 0.05404358887738882
ROC train: 0.999752	val: 0.558301	test: 0.636358
PRC train: 0.998153	val: 0.515010	test: 0.570358

Epoch: 43
Loss: 0.05227819749560829
ROC train: 0.999770	val: 0.556814	test: 0.625552
PRC train: 0.997888	val: 0.524745	test: 0.563265

Epoch: 44
Loss: 0.057897911534731304
ROC train: 0.999792	val: 0.547449	test: 0.597758
PRC train: 0.998196	val: 0.516655	test: 0.566898

Epoch: 45
Loss: 0.050883172802108724
ROC train: 0.999358	val: 0.513935	test: 0.608994
PRC train: 0.994924	val: 0.504190	test: 0.537024

Epoch: 46
Loss: 0.05117250801676473
ROC train: 0.999522	val: 0.499851	test: 0.607981
PRC train: 0.996010	val: 0.503103	test: 0.529457

Epoch: 47
Loss: 0.037118410356139095
ROC train: 0.999837	val: 0.526860	test: 0.628555
PRC train: 0.998344	val: 0.511097	test: 0.535989

Epoch: 48
Loss: 0.049377243312258
ROC train: 0.999813	val: 0.537152	test: 0.610409
PRC train: 0.998420	val: 0.547272	test: 0.528794

Epoch: 49
Loss: 0.03875932410282249
ROC train: 0.999717	val: 0.528360	test: 0.617843
PRC train: 0.997508	val: 0.533169	test: 0.531790

Epoch: 50
Loss: 0.04442166868817972
ROC train: 0.999914	val: 0.513187	test: 0.639418
PRC train: 0.999045	val: 0.515660	test: 0.542089

Epoch: 51
Loss: 0.05501060288613948
ROC train: 0.999793	val: 0.529668	test: 0.642297
PRC train: 0.997891	val: 0.507890	test: 0.553097

Epoch: 52
Loss: 0.03216717043252335
ROC train: 0.998481	val: 0.547136	test: 0.602068
PRC train: 0.992908	val: 0.512450	test: 0.543917

Epoch: 53
Loss: 0.04997582242541211
ROC train: 0.999584	val: 0.567627	test: 0.623304
PRC train: 0.997091	val: 0.519133	test: 0.549137

Epoch: 54
Loss: 0.040999204316783854
ROC train: 0.999821	val: 0.536074	test: 0.653171
PRC train: 0.998818	val: 0.507819	test: 0.561806

Epoch: 55
Loss: 0.03688601937305579
ROC train: 0.999700	val: 0.516645	test: 0.657818
PRC train: 0.998665	val: 0.508155	test: 0.560892

Epoch: 56
Loss: 0.042634185979679703
ROC train: 0.999900	val: 0.526185	test: 0.666300
PRC train: 0.999833	val: 0.508947	test: 0.566152

Epoch: 57
Loss: 0.04036367103151904
ROC train: 0.999952	val: 0.527721	test: 0.656794
PRC train: 0.999840	val: 0.511012	test: 0.567498

Epoch: 58
Loss: 0.03470978060316265
ROC train: 0.999989	val: 0.516757	test: 0.624178
PRC train: 0.999999	val: 0.516323	test: 0.552912

Epoch: 59
Loss: 0.03585699478920763
ROC train: 0.999994	val: 0.551620	test: 0.609326
PRC train: 1.000000	val: 0.517352	test: 0.543741

Epoch: 60
Loss: 0.02652208083771313
ROC train: 0.999955	val: 0.536612	test: 0.608751
PRC train: 0.999634	val: 0.519871	test: 0.538910

Epoch: 61
Loss: 0.026637715526450202
ROC train: 0.999989	val: 0.494906	test: 0.632496
PRC train: 0.999999	val: 0.517479	test: 0.537844

Epoch: 62
Loss: 0.04211929562957995
ROC train: 1.000000	val: 0.490897	test: 0.657743
PRC train: 1.000000	val: 0.504459	test: 0.543348

Epoch: 63
Loss: 0.03429108743705166
ROC train: 0.999985	val: 0.546201	test: 0.639436
PRC train: 0.999893	val: 0.505647	test: 0.547019

Epoch: 64
Loss: 0.03153312112492211
ROC train: 0.999961	val: 0.542480	test: 0.625026
PRC train: 0.999646	val: 0.512319	test: 0.542967

Epoch: 65
Loss: 0.02520195829966254
ROC train: 0.999636	val: 0.569465	test: 0.618623
PRC train: 0.997713	val: 0.524101	test: 0.536448

Epoch: 66
Loss: 0.035303170181290845
ROC train: 0.999648	val: 0.605691	test: 0.632597
PRC train: 0.997894	val: 0.533613	test: 0.551094

Epoch: 67
Loss: 0.032132670360609114
ROC train: 0.999846	val: 0.600849	test: 0.630860
PRC train: 0.998728	val: 0.552700	test: 0.544067

Epoch: 68
Loss: 0.02743674933410639
ROC train: 0.999986	val: 0.571380	test: 0.630348
PRC train: 0.999842	val: 0.550540	test: 0.538575

Epoch: 69
Loss: 0.031180068723983
ROC train: 0.999994	val: 0.568941	test: 0.628936
PRC train: 1.000000	val: 0.530644	test: 0.540068

Epoch: 70
Loss: 0.02516296397778709
ROC train: 0.999978	val: 0.607810	test: 0.637475
PRC train: 0.999945	val: 0.538767	test: 0.550749

Epoch: 71
Loss: 0.03611084507144543
ROC train: 1.000000	val: 0.597883	test: 0.628062
PRC train: 1.000000	val: 0.543135	test: 0.552421

Epoch: 72
Loss: 0.024648638955345754
ROC train: 0.999995	val: 0.582288	test: 0.637281
PRC train: 0.999946	val: 0.527729	test: 0.554059

Epoch: 73
Loss: 0.029183642363371138
ROC train: 1.000000	val: 0.528807	test: 0.635507
PRC train: 1.000000	val: 0.511844	test: 0.553324

Epoch: 74
Loss: 0.02282272976800159
ROC train: 1.000000	val: 0.484104	test: 0.634319
PRC train: 1.000000	val: 0.504121	test: 0.539467

Epoch: 75
Loss: 0.020247697875258465
ROC train: 1.000000	val: 0.485840	test: 0.637266
PRC train: 1.000000	val: 0.504162	test: 0.539380

Epoch: 76
Loss: 0.02334738533799976
ROC train: 0.999990	val: 0.451363	test: 0.626486
PRC train: 0.999894	val: 0.501790	test: 0.540299

Epoch: 77
Loss: 0.022218527274762687
ROC train: 0.999995	val: 0.436379	test: 0.618844
PRC train: 0.999946	val: 0.500316	test: 0.549955

Epoch: 78
Loss: 0.0204600007888756
ROC train: 1.000000	val: 0.465736	test: 0.624428
PRC train: 1.000000	val: 0.502637	test: 0.564731

Epoch: 79
Loss: 0.01920681890654012
ROC train: 1.000000	val: 0.484055	test: 0.631835
PRC train: 1.000000	val: 0.504216	test: 0.570788

Epoch: 80
Loss: 0.018227065842798245
ROC train: 1.000000	val: 0.503171	test: 0.639055
PRC train: 1.000000	val: 0.506440	test: 0.578256

Epoch: 81
Loss: 0.012744215107632222
ROC train: 0.999990	val: 0.513735	test: 0.650773
PRC train: 0.999946	val: 0.508558	test: 0.586295

Epoch: 82
Loss: 0.016968636995921098
ROC train: 0.999990	val: 0.546363	test: 0.653459
PRC train: 0.999892	val: 0.511425	test: 0.579970

Epoch: 83
Loss: 0.02043448103920587
ROC train: 1.000000	val: 0.564169	test: 0.638331
PRC train: 1.000000	val: 0.512778	test: 0.557454

Epoch: 84
Loss: 0.020928990821916568
ROC train: 1.000000	val: 0.534078	test: 0.652708
PRC train: 1.000000	val: 0.516041	test: 0.554527

Epoch: 85
Loss: 0.01709449008170367
ROC train: 1.000000	val: 0.501288	test: 0.662326
PRC train: 1.000000	val: 0.508186	test: 0.551375

Epoch: 86
Loss: 0.021808310387002323
ROC train: 0.999989	val: 0.524737	test: 0.663439
PRC train: 0.999999	val: 0.504772	test: 0.574183

Epoch: 87
Loss: 0.01935695476024046
ROC train: 1.000000	val: 0.529843	test: 0.637430
PRC train: 1.000000	val: 0.507480	test: 0.565684

Epoch: 88
Loss: 0.01818116874606501
ROC train: 1.000000	val: 0.519530	test: 0.625164
PRC train: 1.000000	val: 0.506962	test: 0.560136

Epoch: 89
Loss: 0.019131581940944564
ROC train: 1.000000	val: 0.496990	test: 0.618044
PRC train: 1.000000	val: 0.504903	test: 0.562655

Epoch: 90
Loss: 0.024904841714888725
ROC train: 1.000000	val: 0.505118	test: 0.618668
PRC train: 1.000000	val: 0.508696	test: 0.564340

Epoch: 91
Loss: 0.019417911165190073
ROC train: 0.999990	val: 0.525584	test: 0.612098
PRC train: 0.999894	val: 0.516556	test: 0.566506

Epoch: 92
Loss: 0.030482764839994604
ROC train: 0.999995	val: 0.573921	test: 0.634771
PRC train: 0.999946	val: 0.519747	test: 0.566580

Epoch: 93
Loss: 0.01715544815337353
PRC train: 0.981045	val: 0.600485	test: 0.529090

Epoch: 33
Loss: 0.09883399561100903
ROC train: 0.997280	val: 0.804768	test: 0.600272
PRC train: 0.986323	val: 0.647266	test: 0.529761

Epoch: 34
Loss: 0.09436095560562062
ROC train: 0.998909	val: 0.816681	test: 0.628208
PRC train: 0.995895	val: 0.596557	test: 0.531442

Epoch: 35
Loss: 0.08533115317793666
ROC train: 0.998354	val: 0.810201	test: 0.588472
PRC train: 0.995230	val: 0.592871	test: 0.524129

Epoch: 36
Loss: 0.0870750676947886
ROC train: 0.999311	val: 0.789109	test: 0.594508
PRC train: 0.996726	val: 0.559913	test: 0.531325

Epoch: 37
Loss: 0.08794558552298701
ROC train: 0.998327	val: 0.792107	test: 0.598580
PRC train: 0.993415	val: 0.554387	test: 0.528798

Epoch: 38
Loss: 0.07856734583515487
ROC train: 0.998759	val: 0.765934	test: 0.600317
PRC train: 0.993375	val: 0.559872	test: 0.533537

Epoch: 39
Loss: 0.07304008240325874
ROC train: 0.999527	val: 0.817243	test: 0.587848
PRC train: 0.998364	val: 0.563842	test: 0.526254

Epoch: 40
Loss: 0.07853044959712456
ROC train: 0.998270	val: 0.818255	test: 0.567110
PRC train: 0.992890	val: 0.570340	test: 0.520869

Epoch: 41
Loss: 0.06884690615883052
ROC train: 0.999911	val: 0.779220	test: 0.602188
PRC train: 0.999888	val: 0.583497	test: 0.529190

Epoch: 42
Loss: 0.05723643244442761
ROC train: 0.999746	val: 0.804944	test: 0.608060
PRC train: 0.999567	val: 0.565376	test: 0.528259

Epoch: 43
Loss: 0.06236227293560147
ROC train: 0.999721	val: 0.786776	test: 0.637980
PRC train: 0.999676	val: 0.567772	test: 0.539995

Epoch: 44
Loss: 0.06627679856306942
ROC train: 0.999945	val: 0.771279	test: 0.654131
PRC train: 0.999890	val: 0.557498	test: 0.543119

Epoch: 45
Loss: 0.06968437669960943
ROC train: 0.999833	val: 0.786937	test: 0.635343
PRC train: 0.999112	val: 0.565572	test: 0.533957

Epoch: 46
Loss: 0.05107247465039218
ROC train: 0.999926	val: 0.818329	test: 0.632246
PRC train: 0.999331	val: 0.568520	test: 0.538599

Epoch: 47
Loss: 0.04888193865920471
ROC train: 0.999952	val: 0.821351	test: 0.649884
PRC train: 0.999482	val: 0.563865	test: 0.544244

Epoch: 48
Loss: 0.05032086503872142
ROC train: 0.999990	val: 0.801310	test: 0.602390
PRC train: 0.999946	val: 0.567142	test: 0.530973

Epoch: 49
Loss: 0.05370259958132558
ROC train: 0.999746	val: 0.810201	test: 0.647960
PRC train: 0.998084	val: 0.567126	test: 0.540687

Epoch: 50
Loss: 0.04590606162015628
ROC train: 0.999803	val: 0.804518	test: 0.704535
PRC train: 0.998386	val: 0.569649	test: 0.556351

Epoch: 51
Loss: 0.04008702492230992
ROC train: 0.999832	val: 0.817468	test: 0.659715
PRC train: 0.998411	val: 0.569458	test: 0.544072

Epoch: 52
Loss: 0.04658802824181628
ROC train: 0.999962	val: 0.833288	test: 0.634857
PRC train: 0.999577	val: 0.576271	test: 0.538905

Epoch: 53
Loss: 0.053645361279704304
ROC train: 0.999962	val: 0.780619	test: 0.634356
PRC train: 0.999578	val: 0.573329	test: 0.539693

Epoch: 54
Loss: 0.04855731634916181
ROC train: 0.999910	val: 0.752549	test: 0.680401
PRC train: 0.999940	val: 0.551591	test: 0.550086

Epoch: 55
Loss: 0.03701433606433969
ROC train: 1.000000	val: 0.759928	test: 0.708282
PRC train: 1.000000	val: 0.546693	test: 0.559051

Epoch: 56
Loss: 0.04701932775519008
ROC train: 1.000000	val: 0.763038	test: 0.694741
PRC train: 1.000000	val: 0.545107	test: 0.558599

Epoch: 57
Loss: 0.041045756591014164
ROC train: 0.999995	val: 0.766109	test: 0.625437
PRC train: 0.999946	val: 0.543998	test: 0.537741

Epoch: 58
Loss: 0.03941235163267606
ROC train: 1.000000	val: 0.757117	test: 0.578943
PRC train: 1.000000	val: 0.549252	test: 0.531204

Epoch: 59
Loss: 0.031204657479878096
ROC train: 0.999990	val: 0.782042	test: 0.570610
PRC train: 0.999892	val: 0.555819	test: 0.531881

Epoch: 60
Loss: 0.027937575248602957
ROC train: 0.999990	val: 0.781730	test: 0.582253
PRC train: 0.999892	val: 0.552853	test: 0.539193

Epoch: 61
Loss: 0.036383078915827836
ROC train: 0.999990	val: 0.786762	test: 0.564040
PRC train: 0.999894	val: 0.558497	test: 0.528207

Epoch: 62
Loss: 0.02890003138564986
ROC train: 0.999986	val: 0.783778	test: 0.562803
PRC train: 0.999839	val: 0.558452	test: 0.525002

Epoch: 63
Loss: 0.0353038766725849
ROC train: 1.000000	val: 0.788673	test: 0.589809
PRC train: 1.000000	val: 0.556872	test: 0.528241

Epoch: 64
Loss: 0.027540540954552172
ROC train: 0.999977	val: 0.775513	test: 0.588237
PRC train: 0.999998	val: 0.567338	test: 0.527504

Epoch: 65
Loss: 0.028720013730396926
ROC train: 0.999977	val: 0.780183	test: 0.590885
PRC train: 0.999998	val: 0.562115	test: 0.528013

Epoch: 66
Loss: 0.030628657349908366
ROC train: 1.000000	val: 0.768980	test: 0.598311
PRC train: 1.000000	val: 0.554936	test: 0.531766

Epoch: 67
Loss: 0.028839923828808362
ROC train: 1.000000	val: 0.746416	test: 0.601191
PRC train: 1.000000	val: 0.545843	test: 0.531985

Epoch: 68
Loss: 0.027862990598217376
ROC train: 1.000000	val: 0.724703	test: 0.614544
PRC train: 1.000000	val: 0.540650	test: 0.537143

Epoch: 69
Loss: 0.03773990659283476
ROC train: 1.000000	val: 0.759879	test: 0.596992
PRC train: 1.000000	val: 0.549514	test: 0.536398

Epoch: 70
Loss: 0.02231884159606386
ROC train: 0.999995	val: 0.756769	test: 0.626497
PRC train: 0.999946	val: 0.548459	test: 0.546216

Epoch: 71
Loss: 0.026407298238991943
ROC train: 0.999995	val: 0.768369	test: 0.626161
PRC train: 0.999946	val: 0.547174	test: 0.545724

Epoch: 72
Loss: 0.023319321425768987
ROC train: 1.000000	val: 0.769992	test: 0.598079
PRC train: 1.000000	val: 0.545255	test: 0.533875

Epoch: 73
Loss: 0.026953126656581873
ROC train: 1.000000	val: 0.740973	test: 0.605886
PRC train: 1.000000	val: 0.541232	test: 0.532869

Epoch: 74
Loss: 0.024234561663489106
ROC train: 0.999995	val: 0.721993	test: 0.587299
PRC train: 0.999946	val: 0.540348	test: 0.533817

Epoch: 75
Loss: 0.02314587939885202
ROC train: 0.999990	val: 0.703014	test: 0.586900
PRC train: 0.999892	val: 0.552178	test: 0.534748

Epoch: 76
Loss: 0.02745004266601026
ROC train: 0.999994	val: 0.701429	test: 0.587523
PRC train: 1.000000	val: 0.544235	test: 0.534008

Epoch: 77
Loss: 0.0233320590348956
ROC train: 0.999989	val: 0.756783	test: 0.628896
PRC train: 0.999999	val: 0.550622	test: 0.538671

Epoch: 78
Loss: 0.020197607918402093
ROC train: 1.000000	val: 0.815120	test: 0.643956
PRC train: 1.000000	val: 0.557662	test: 0.555103

Epoch: 79
Loss: 0.0295849248264869
ROC train: 1.000000	val: 0.823136	test: 0.642776
PRC train: 1.000000	val: 0.566460	test: 0.554857

Epoch: 80
Loss: 0.026738894891799266
ROC train: 0.999983	val: 0.763737	test: 0.649357
PRC train: 0.999999	val: 0.568544	test: 0.552133

Epoch: 81
Loss: 0.02241973856312677
ROC train: 0.999989	val: 0.746107	test: 0.643949
PRC train: 0.999999	val: 0.572852	test: 0.546153

Epoch: 82
Loss: 0.02205393055968856
ROC train: 1.000000	val: 0.742410	test: 0.634581
PRC train: 1.000000	val: 0.569691	test: 0.543115

Epoch: 83
Loss: 0.026101906513383438
ROC train: 1.000000	val: 0.771454	test: 0.649040
PRC train: 1.000000	val: 0.569142	test: 0.558958

Epoch: 84
Loss: 0.021772590567228963
ROC train: 1.000000	val: 0.762476	test: 0.665348
PRC train: 1.000000	val: 0.553795	test: 0.568435

Epoch: 85
Loss: 0.015257337874301271
ROC train: 0.999995	val: 0.747218	test: 0.657429
PRC train: 0.999946	val: 0.549149	test: 0.557052

Epoch: 86
Loss: 0.019519712525316164
ROC train: 1.000000	val: 0.712354	test: 0.637729
PRC train: 1.000000	val: 0.546415	test: 0.561790

Epoch: 87
Loss: 0.026185661640845313
ROC train: 1.000000	val: 0.722868	test: 0.599629
PRC train: 1.000000	val: 0.550481	test: 0.564191

Epoch: 88
Loss: 0.01611295177771392
ROC train: 1.000000	val: 0.762152	test: 0.593970
PRC train: 1.000000	val: 0.554363	test: 0.547848

Epoch: 89
Loss: 0.02484849434338829
ROC train: 1.000000	val: 0.747404	test: 0.610910
PRC train: 1.000000	val: 0.557838	test: 0.589151

Epoch: 90
Loss: 0.021351263618634878
ROC train: 1.000000	val: 0.738464	test: 0.605819
PRC train: 1.000000	val: 0.556350	test: 0.569606

Epoch: 91
Loss: 0.015120014469980844
ROC train: 0.999965	val: 0.758744	test: 0.593321
PRC train: 0.999725	val: 0.575541	test: 0.548233

Epoch: 92
Loss: 0.023208063341680525
ROC train: 0.999995	val: 0.764475	test: 0.572571
PRC train: 0.999946	val: 0.567269	test: 0.528484

Epoch: 93
Loss: 0.019878026972078154
PRC train: 0.983434	val: 0.577354	test: 0.541040

Epoch: 33
Loss: 0.11289657540889735
ROC train: 0.991388	val: 0.820377	test: 0.637031
PRC train: 0.976036	val: 0.594756	test: 0.547402

Epoch: 34
Loss: 0.10054858013713275
ROC train: 0.995101	val: 0.829742	test: 0.679034
PRC train: 0.979813	val: 0.592905	test: 0.544972

Epoch: 35
Loss: 0.1069624933536367
ROC train: 0.994671	val: 0.761502	test: 0.611922
PRC train: 0.986683	val: 0.630125	test: 0.529092

Epoch: 36
Loss: 0.09182882484747185
ROC train: 0.991767	val: 0.807790	test: 0.636919
PRC train: 0.975880	val: 0.594634	test: 0.541657

Epoch: 37
Loss: 0.09007683995754404
ROC train: 0.996966	val: 0.780844	test: 0.625720
PRC train: 0.989401	val: 0.637189	test: 0.536622

Epoch: 38
Loss: 0.08786045563170948
ROC train: 0.998424	val: 0.765048	test: 0.627338
PRC train: 0.995662	val: 0.595641	test: 0.540503

Epoch: 39
Loss: 0.08359832530708773
ROC train: 0.998034	val: 0.772315	test: 0.672546
PRC train: 0.991859	val: 0.588296	test: 0.557855

Epoch: 40
Loss: 0.08931787700625278
ROC train: 0.998972	val: 0.734718	test: 0.665337
PRC train: 0.996234	val: 0.560842	test: 0.542499

Epoch: 41
Loss: 0.09044432374624019
ROC train: 0.999170	val: 0.765723	test: 0.670271
PRC train: 0.997233	val: 0.568258	test: 0.538783

Epoch: 42
Loss: 0.058275682977067686
ROC train: 0.998602	val: 0.772428	test: 0.671033
PRC train: 0.994716	val: 0.555147	test: 0.539019

Epoch: 43
Loss: 0.0755428380059352
ROC train: 0.998981	val: 0.748353	test: 0.675318
PRC train: 0.996860	val: 0.556608	test: 0.539574

Epoch: 44
Loss: 0.0777839681449166
ROC train: 0.998963	val: 0.737839	test: 0.660690
PRC train: 0.995105	val: 0.572258	test: 0.539319

Epoch: 45
Loss: 0.060513970146490056
ROC train: 0.998828	val: 0.751312	test: 0.667261
PRC train: 0.994410	val: 0.593418	test: 0.545644

Epoch: 46
Loss: 0.07069784341718821
ROC train: 0.999234	val: 0.736914	test: 0.687536
PRC train: 0.996249	val: 0.563104	test: 0.549553

Epoch: 47
Loss: 0.0574782999991417
ROC train: 0.999088	val: 0.756994	test: 0.691484
PRC train: 0.996716	val: 0.578392	test: 0.559672

Epoch: 48
Loss: 0.06769515096673051
ROC train: 0.998899	val: 0.771117	test: 0.708696
PRC train: 0.995496	val: 0.585484	test: 0.566991

Epoch: 49
Loss: 0.06474117048452852
ROC train: 0.999244	val: 0.747042	test: 0.730895
PRC train: 0.996465	val: 0.626153	test: 0.579123

Epoch: 50
Loss: 0.06905579442280178
ROC train: 0.999457	val: 0.747429	test: 0.713369
PRC train: 0.996693	val: 0.628416	test: 0.566661

Epoch: 51
Loss: 0.046548389234571816
ROC train: 0.999282	val: 0.764124	test: 0.678978
PRC train: 0.996437	val: 0.635931	test: 0.555106

Epoch: 52
Loss: 0.04070417663385689
ROC train: 0.999693	val: 0.766447	test: 0.671926
PRC train: 0.998496	val: 0.609213	test: 0.549241

Epoch: 53
Loss: 0.058979977706293304
ROC train: 0.999757	val: 0.769943	test: 0.678959
PRC train: 0.998896	val: 0.599048	test: 0.543557

Epoch: 54
Loss: 0.04762257298243077
ROC train: 0.999776	val: 0.725353	test: 0.699653
PRC train: 0.998536	val: 0.567862	test: 0.544734

Epoch: 55
Loss: 0.05177723742632491
ROC train: 0.999586	val: 0.744919	test: 0.716992
PRC train: 0.996645	val: 0.577537	test: 0.555016

Epoch: 56
Loss: 0.04581300201335624
ROC train: 0.999865	val: 0.743271	test: 0.682751
PRC train: 0.999547	val: 0.576778	test: 0.591468

Epoch: 57
Loss: 0.051422972467976245
ROC train: 0.999965	val: 0.760466	test: 0.682063
PRC train: 0.999732	val: 0.562640	test: 0.559531

Epoch: 58
Loss: 0.048126961051012905
ROC train: 0.999956	val: 0.788673	test: 0.699141
PRC train: 0.999568	val: 0.559585	test: 0.567623

Epoch: 59
Loss: 0.03729713515972489
ROC train: 0.999610	val: 0.792507	test: 0.724724
PRC train: 0.998513	val: 0.562550	test: 0.550875

Epoch: 60
Loss: 0.035552942363186645
ROC train: 0.999952	val: 0.819204	test: 0.732557
PRC train: 0.999840	val: 0.569932	test: 0.561292

Epoch: 61
Loss: 0.04261804789575833
ROC train: 0.999983	val: 0.827820	test: 0.726147
PRC train: 0.999999	val: 0.586427	test: 0.555714

Epoch: 62
Loss: 0.0344892495780689
ROC train: 0.999636	val: 0.836324	test: 0.711091
PRC train: 0.999654	val: 0.612125	test: 0.544936

Epoch: 63
Loss: 0.04639473138966819
ROC train: 0.999955	val: 0.807105	test: 0.699832
PRC train: 0.999997	val: 0.586073	test: 0.543131

Epoch: 64
Loss: 0.04321451646281289
ROC train: 0.999952	val: 0.740337	test: 0.687610
PRC train: 0.999784	val: 0.554384	test: 0.564179

Epoch: 65
Loss: 0.03798358301129189
ROC train: 0.999931	val: 0.746518	test: 0.701188
PRC train: 0.999723	val: 0.559053	test: 0.601549

Epoch: 66
Loss: 0.03756591069495945
ROC train: 0.999859	val: 0.801647	test: 0.707460
PRC train: 0.999579	val: 0.566364	test: 0.571400

Epoch: 67
Loss: 0.03193051061296789
ROC train: 0.999909	val: 0.837709	test: 0.704512
PRC train: 0.999678	val: 0.583207	test: 0.592833

Epoch: 68
Loss: 0.032917498153746665
ROC train: 0.999977	val: 0.823761	test: 0.704318
PRC train: 0.999998	val: 0.587672	test: 0.567901

Epoch: 69
Loss: 0.03235522357241217
ROC train: 0.999960	val: 0.831478	test: 0.704262
PRC train: 0.999997	val: 0.651223	test: 0.569707

Epoch: 70
Loss: 0.0293279849920567
ROC train: 0.999990	val: 0.821751	test: 0.707923
PRC train: 0.999894	val: 0.595519	test: 0.563179

Epoch: 71
Loss: 0.03879039623553536
ROC train: 0.999995	val: 0.816856	test: 0.727922
PRC train: 0.999946	val: 0.618032	test: 0.568711

Epoch: 72
Loss: 0.028799588215340656
ROC train: 1.000000	val: 0.759791	test: 0.720276
PRC train: 1.000000	val: 0.687591	test: 0.551079

Epoch: 73
Loss: 0.02912210503863174
ROC train: 0.999858	val: 0.730048	test: 0.685847
PRC train: 0.999990	val: 0.627659	test: 0.541431

Epoch: 74
Loss: 0.035142720693450105
ROC train: 0.999962	val: 0.761639	test: 0.698734
PRC train: 0.999892	val: 0.576869	test: 0.554706

Epoch: 75
Loss: 0.026538788721115904
ROC train: 0.999979	val: 0.767620	test: 0.710407
PRC train: 0.999891	val: 0.593037	test: 0.556714

Epoch: 76
Loss: 0.03438537643564314
ROC train: 0.999977	val: 0.753561	test: 0.706446
PRC train: 0.999998	val: 0.635484	test: 0.545632

Epoch: 77
Loss: 0.02369997212397827
ROC train: 0.999960	val: 0.762539	test: 0.705958
PRC train: 0.999997	val: 0.588902	test: 0.544185

Epoch: 78
Loss: 0.022875667472819022
ROC train: 0.999977	val: 0.784903	test: 0.695716
PRC train: 0.999998	val: 0.597633	test: 0.544068

Epoch: 79
Loss: 0.027251012521549073
ROC train: 0.999943	val: 0.784927	test: 0.726708
PRC train: 0.999996	val: 0.606178	test: 0.549499

Epoch: 80
Loss: 0.029911659330629496
ROC train: 1.000000	val: 0.790184	test: 0.750094
PRC train: 1.000000	val: 0.578542	test: 0.559051

Epoch: 81
Loss: 0.024780926395173763
ROC train: 0.999963	val: 0.805506	test: 0.750345
PRC train: 0.999836	val: 0.578363	test: 0.564423

Epoch: 82
Loss: 0.027736923121575708
ROC train: 1.000000	val: 0.824573	test: 0.737376
PRC train: 1.000000	val: 0.583852	test: 0.564259

Epoch: 83
Loss: 0.027477016982870937
ROC train: 1.000000	val: 0.825972	test: 0.715539
PRC train: 1.000000	val: 0.577815	test: 0.554387

Epoch: 84
Loss: 0.024776594441129476
ROC train: 1.000000	val: 0.804806	test: 0.713216
PRC train: 1.000000	val: 0.592964	test: 0.550386

Epoch: 85
Loss: 0.017142338114046238
ROC train: 1.000000	val: 0.774389	test: 0.699063
PRC train: 1.000000	val: 0.632661	test: 0.543878

Epoch: 86
Loss: 0.02866182485931267
ROC train: 1.000000	val: 0.776237	test: 0.705185
PRC train: 1.000000	val: 0.586925	test: 0.546112

Epoch: 87
Loss: 0.03352713063835021
ROC train: 0.999979	val: 0.813022	test: 0.723648
PRC train: 0.999891	val: 0.574507	test: 0.555823

Epoch: 88
Loss: 0.03156088756475345
ROC train: 1.000000	val: 0.803158	test: 0.725359
PRC train: 1.000000	val: 0.597316	test: 0.555742

Epoch: 89
Loss: 0.02072834913984839
ROC train: 1.000000	val: 0.813335	test: 0.716376
PRC train: 1.000000	val: 0.598103	test: 0.546851

Epoch: 90
Loss: 0.019560523407632775
ROC train: 1.000000	val: 0.811487	test: 0.703885
PRC train: 1.000000	val: 0.601017	test: 0.545446

Epoch: 91
Loss: 0.023916979623809613
ROC train: 1.000000	val: 0.793368	test: 0.705099
PRC train: 1.000000	val: 0.583939	test: 0.551075

Epoch: 92
Loss: 0.02954765453756092
ROC train: 0.999966	val: 0.763649	test: 0.714478
PRC train: 0.999693	val: 0.604396	test: 0.545308

Epoch: 93
Loss: 0.024518398464599495
PRC train: 0.994170	val: 0.530026	test: 0.580041

Epoch: 33
Loss: 0.0734124215979963
ROC train: 0.998752	val: 0.640667	test: 0.759601
PRC train: 0.993421	val: 0.532409	test: 0.568220

Epoch: 34
Loss: 0.06460222938750113
ROC train: 0.994940	val: 0.672582	test: 0.758577
PRC train: 0.984012	val: 0.525256	test: 0.571352

Epoch: 35
Loss: 0.06331867890853203
ROC train: 0.999231	val: 0.687517	test: 0.767859
PRC train: 0.997132	val: 0.545482	test: 0.572351

Epoch: 36
Loss: 0.07090970490180118
ROC train: 0.999649	val: 0.630353	test: 0.772842
PRC train: 0.999537	val: 0.547822	test: 0.570102

Epoch: 37
Loss: 0.06079043265405617
ROC train: 0.999134	val: 0.536689	test: 0.709843
PRC train: 0.995437	val: 0.504439	test: 0.553900

Epoch: 38
Loss: 0.05531459287756788
ROC train: 0.998312	val: 0.522228	test: 0.703459
PRC train: 0.993391	val: 0.501573	test: 0.550852

Epoch: 39
Loss: 0.04965574064483006
ROC train: 0.999678	val: 0.546465	test: 0.676278
PRC train: 0.997201	val: 0.507364	test: 0.544418

Epoch: 40
Loss: 0.052486449269930444
ROC train: 0.999177	val: 0.603368	test: 0.733622
PRC train: 0.997198	val: 0.521790	test: 0.557335

Epoch: 41
Loss: 0.0538130328458943
ROC train: 0.998390	val: 0.712678	test: 0.759713
PRC train: 0.993495	val: 0.538750	test: 0.567772

Epoch: 42
Loss: 0.06406761662079559
ROC train: 0.997569	val: 0.668899	test: 0.771531
PRC train: 0.987879	val: 0.521550	test: 0.595057

Epoch: 43
Loss: 0.05605819144853795
ROC train: 0.999522	val: 0.636770	test: 0.760101
PRC train: 0.997097	val: 0.524240	test: 0.575664

Epoch: 44
Loss: 0.0632310183200433
ROC train: 0.999730	val: 0.649807	test: 0.745070
PRC train: 0.999135	val: 0.519946	test: 0.562368

Epoch: 45
Loss: 0.04232552634921232
ROC train: 0.999687	val: 0.591631	test: 0.704584
PRC train: 0.998511	val: 0.512587	test: 0.552444

Epoch: 46
Loss: 0.038507354372905186
ROC train: 0.999281	val: 0.529583	test: 0.696889
PRC train: 0.996287	val: 0.506262	test: 0.551193

Epoch: 47
Loss: 0.038093117898512566
ROC train: 0.999711	val: 0.632226	test: 0.763597
PRC train: 0.999248	val: 0.518021	test: 0.571581

Epoch: 48
Loss: 0.047940142714871306
ROC train: 0.999732	val: 0.636784	test: 0.770056
PRC train: 0.999720	val: 0.515938	test: 0.578036

Epoch: 49
Loss: 0.042598913491059535
ROC train: 0.999714	val: 0.575748	test: 0.735116
PRC train: 0.999773	val: 0.506587	test: 0.567950

Epoch: 50
Loss: 0.03591991896962754
ROC train: 0.999701	val: 0.606102	test: 0.765745
PRC train: 0.999874	val: 0.514678	test: 0.584607

Epoch: 51
Loss: 0.03464545956771407
ROC train: 0.999665	val: 0.653778	test: 0.739479
PRC train: 0.999977	val: 0.524163	test: 0.567874

Epoch: 52
Loss: 0.03356043693014968
ROC train: 0.999716	val: 0.653641	test: 0.738741
PRC train: 0.999980	val: 0.520912	test: 0.566183

Epoch: 53
Loss: 0.03523120033343981
ROC train: 0.999768	val: 0.587871	test: 0.753057
PRC train: 0.999325	val: 0.507882	test: 0.576119

Epoch: 54
Loss: 0.044355247744689456
ROC train: 0.999680	val: 0.494882	test: 0.702421
PRC train: 0.997645	val: 0.500239	test: 0.560462

Epoch: 55
Loss: 0.03711544062482246
ROC train: 0.999917	val: 0.458370	test: 0.662573
PRC train: 0.999227	val: 0.498310	test: 0.551067

Epoch: 56
Loss: 0.03466996470000106
ROC train: 0.999885	val: 0.532718	test: 0.732729
PRC train: 0.998901	val: 0.501604	test: 0.569197

Epoch: 57
Loss: 0.034028219611285715
ROC train: 0.999928	val: 0.511328	test: 0.699851
PRC train: 0.999238	val: 0.500249	test: 0.596290

Epoch: 58
Loss: 0.03556687399512989
ROC train: 0.999874	val: 0.485991	test: 0.676378
PRC train: 0.998745	val: 0.500641	test: 0.558407

Epoch: 59
Loss: 0.03238726022951075
ROC train: 0.999894	val: 0.614895	test: 0.733330
PRC train: 0.998924	val: 0.516699	test: 0.564476

Epoch: 60
Loss: 0.032185345637320874
ROC train: 0.999933	val: 0.633885	test: 0.760688
PRC train: 0.999289	val: 0.542318	test: 0.566829

Epoch: 61
Loss: 0.027926927502177297
ROC train: 1.000000	val: 0.616989	test: 0.739576
PRC train: 1.000000	val: 0.524831	test: 0.563265

Epoch: 62
Loss: 0.029668342092320953
ROC train: 0.999977	val: 0.586048	test: 0.735840
PRC train: 0.999998	val: 0.510240	test: 0.572532

Epoch: 63
Loss: 0.028008151533846115
ROC train: 1.000000	val: 0.598059	test: 0.754592
PRC train: 1.000000	val: 0.514361	test: 0.591226

Epoch: 64
Loss: 0.028400392957760028
ROC train: 1.000000	val: 0.637304	test: 0.774691
PRC train: 1.000000	val: 0.531551	test: 0.594406

Epoch: 65
Loss: 0.027227304747038227
ROC train: 0.999990	val: 0.587408	test: 0.771131
PRC train: 0.999894	val: 0.515104	test: 0.606628

Epoch: 66
Loss: 0.027500244350568283
ROC train: 0.999891	val: 0.534963	test: 0.755264
PRC train: 0.999388	val: 0.508922	test: 0.587526

Epoch: 67
Loss: 0.031067477188093835
ROC train: 0.999927	val: 0.548612	test: 0.750524
PRC train: 0.999293	val: 0.505349	test: 0.575854

Epoch: 68
Loss: 0.03178651835998304
ROC train: 1.000000	val: 0.616754	test: 0.767344
PRC train: 1.000000	val: 0.513356	test: 0.579029

Epoch: 69
Loss: 0.027636098453007497
ROC train: 0.999989	val: 0.627841	test: 0.767598
PRC train: 0.999999	val: 0.520934	test: 0.584255

Epoch: 70
Loss: 0.026997939689080756
ROC train: 1.000000	val: 0.644624	test: 0.769533
PRC train: 1.000000	val: 0.524435	test: 0.584121

Epoch: 71
Loss: 0.020946413304502465
ROC train: 1.000000	val: 0.620313	test: 0.762073
PRC train: 1.000000	val: 0.520565	test: 0.585253

Epoch: 72
Loss: 0.02098488209142556
ROC train: 1.000000	val: 0.519319	test: 0.734817
PRC train: 1.000000	val: 0.506560	test: 0.574334

Epoch: 73
Loss: 0.02578138039690866
ROC train: 0.999994	val: 0.502936	test: 0.753893
PRC train: 1.000000	val: 0.500018	test: 0.586302

Epoch: 74
Loss: 0.022125120697659695
ROC train: 1.000000	val: 0.534166	test: 0.767747
PRC train: 1.000000	val: 0.502259	test: 0.585682

Epoch: 75
Loss: 0.021921937913946588
ROC train: 1.000000	val: 0.476738	test: 0.745580
PRC train: 1.000000	val: 0.497975	test: 0.579347

Epoch: 76
Loss: 0.027020716153414847
ROC train: 1.000000	val: 0.484006	test: 0.739067
PRC train: 1.000000	val: 0.498184	test: 0.584006

Epoch: 77
Loss: 0.020923709914960315
ROC train: 0.999480	val: 0.488050	test: 0.725520
PRC train: 0.998643	val: 0.512843	test: 0.564966

Epoch: 78
Loss: 0.02722897939197471
ROC train: 0.999876	val: 0.558090	test: 0.725400
PRC train: 0.999580	val: 0.515447	test: 0.565052

Epoch: 79
Loss: 0.028926102947582254
ROC train: 0.999981	val: 0.570417	test: 0.704512
PRC train: 0.999792	val: 0.512178	test: 0.560513

Epoch: 80
Loss: 0.027649997541886785
ROC train: 0.999976	val: 0.584203	test: 0.702010
PRC train: 0.999742	val: 0.510345	test: 0.555152

Epoch: 81
Loss: 0.015597996809578674
ROC train: 0.999949	val: 0.546166	test: 0.714116
PRC train: 0.999996	val: 0.505959	test: 0.565674

Epoch: 82
Loss: 0.025197718870311813
ROC train: 0.999955	val: 0.508393	test: 0.708352
PRC train: 0.999997	val: 0.501469	test: 0.598688

Epoch: 83
Loss: 0.01865256314743791
ROC train: 1.000000	val: 0.548239	test: 0.723525
PRC train: 1.000000	val: 0.503831	test: 0.576690

Epoch: 84
Loss: 0.0158047354100659
ROC train: 1.000000	val: 0.581528	test: 0.741152
PRC train: 1.000000	val: 0.508111	test: 0.582418

Epoch: 85
Loss: 0.021724064396135175
ROC train: 0.999989	val: 0.573787	test: 0.765024
PRC train: 0.999999	val: 0.506475	test: 0.600219

Epoch: 86
Loss: 0.016260260295745906
ROC train: 0.999989	val: 0.602219	test: 0.755380
PRC train: 0.999999	val: 0.511078	test: 0.591776

Epoch: 87
Loss: 0.021456397759041613
ROC train: 1.000000	val: 0.641841	test: 0.726730
PRC train: 1.000000	val: 0.522538	test: 0.576894

Epoch: 88
Loss: 0.013584795678230724
ROC train: 1.000000	val: 0.670136	test: 0.718191
PRC train: 1.000000	val: 0.525930	test: 0.571509

Epoch: 89
Loss: 0.015356420005924127
ROC train: 1.000000	val: 0.661667	test: 0.736942
PRC train: 1.000000	val: 0.525929	test: 0.585131

Epoch: 90
Loss: 0.01648521542601481
ROC train: 0.999995	val: 0.617689	test: 0.728560
PRC train: 0.999946	val: 0.520498	test: 0.586124

Epoch: 91
Loss: 0.014569930043096684
ROC train: 1.000000	val: 0.641426	test: 0.748353
PRC train: 1.000000	val: 0.528199	test: 0.585641

Epoch: 92
Loss: 0.018428478481245673
ROC train: 1.000000	val: 0.663129	test: 0.764968
PRC train: 1.000000	val: 0.541371	test: 0.576798

Epoch: 93
Loss: 0.010215539568950546
PRC train: 0.984291	val: 0.547901	test: 0.560608

Epoch: 33
Loss: 0.08212133855677665
ROC train: 0.998734	val: 0.736240	test: 0.699022
PRC train: 0.992194	val: 0.538242	test: 0.556389

Epoch: 34
Loss: 0.08031383584362548
ROC train: 0.998451	val: 0.713363	test: 0.698185
PRC train: 0.990374	val: 0.535274	test: 0.554851

Epoch: 35
Loss: 0.09652225762075106
ROC train: 0.997069	val: 0.732343	test: 0.696374
PRC train: 0.986295	val: 0.559659	test: 0.550695

Epoch: 36
Loss: 0.09191912964872999
ROC train: 0.999189	val: 0.750588	test: 0.666682
PRC train: 0.995636	val: 0.538373	test: 0.542689

Epoch: 37
Loss: 0.08000675025120993
ROC train: 0.999261	val: 0.810274	test: 0.667407
PRC train: 0.994730	val: 0.568168	test: 0.546270

Epoch: 38
Loss: 0.05556866365878541
ROC train: 0.999127	val: 0.830603	test: 0.634891
PRC train: 0.993295	val: 0.557519	test: 0.539096

Epoch: 39
Loss: 0.08126063671346699
ROC train: 0.999384	val: 0.773601	test: 0.629968
PRC train: 0.995760	val: 0.540856	test: 0.543752

Epoch: 40
Loss: 0.0651689211451373
ROC train: 0.999540	val: 0.804694	test: 0.693139
PRC train: 0.995655	val: 0.551895	test: 0.553319

Epoch: 41
Loss: 0.06706428573545753
ROC train: 0.999369	val: 0.810225	test: 0.719809
PRC train: 0.996293	val: 0.551717	test: 0.557550

Epoch: 42
Loss: 0.05786230468150057
ROC train: 0.999200	val: 0.808626	test: 0.696837
PRC train: 0.995838	val: 0.550049	test: 0.550055

Epoch: 43
Loss: 0.05053828008980883
ROC train: 0.999674	val: 0.831640	test: 0.693326
PRC train: 0.997399	val: 0.555827	test: 0.548409

Epoch: 44
Loss: 0.0580282126789761
ROC train: 0.999736	val: 0.835650	test: 0.690009
PRC train: 0.997925	val: 0.560084	test: 0.544080

Epoch: 45
Loss: 0.04916254367608182
ROC train: 0.999568	val: 0.754499	test: 0.673600
PRC train: 0.996666	val: 0.609188	test: 0.538903

Epoch: 46
Loss: 0.05487658150579827
ROC train: 0.999501	val: 0.804508	test: 0.690108
PRC train: 0.996513	val: 0.565543	test: 0.544041

Epoch: 47
Loss: 0.047258162907996636
ROC train: 0.999097	val: 0.848673	test: 0.696822
PRC train: 0.994543	val: 0.579909	test: 0.546276

Epoch: 48
Loss: 0.06187296451862705
ROC train: 0.999555	val: 0.856639	test: 0.709896
PRC train: 0.996545	val: 0.592592	test: 0.551081

Epoch: 49
Loss: 0.03684833704228101
ROC train: 0.999944	val: 0.822089	test: 0.708678
PRC train: 0.999626	val: 0.568774	test: 0.551588

Epoch: 50
Loss: 0.05086064217587869
ROC train: 0.999876	val: 0.826221	test: 0.709365
PRC train: 0.999255	val: 0.555400	test: 0.551909

Epoch: 51
Loss: 0.053807605684254835
ROC train: 0.999289	val: 0.798014	test: 0.701469
PRC train: 0.995324	val: 0.545304	test: 0.551005

Epoch: 52
Loss: 0.053373525531643276
ROC train: 0.999297	val: 0.807491	test: 0.698320
PRC train: 0.996699	val: 0.548405	test: 0.551923

Epoch: 53
Loss: 0.047321499453987524
ROC train: 0.999774	val: 0.822974	test: 0.701080
PRC train: 0.998726	val: 0.554667	test: 0.549406

Epoch: 54
Loss: 0.0495107560894688
ROC train: 0.999911	val: 0.834599	test: 0.678796
PRC train: 0.999194	val: 0.559978	test: 0.540878

Epoch: 55
Loss: 0.051140851959343946
ROC train: 0.999787	val: 0.818954	test: 0.679132
PRC train: 0.998598	val: 0.549718	test: 0.544944

Epoch: 56
Loss: 0.038394716846114854
ROC train: 0.999548	val: 0.801510	test: 0.680645
PRC train: 0.998057	val: 0.552945	test: 0.551216

Epoch: 57
Loss: 0.04347680801684593
ROC train: 0.999657	val: 0.802884	test: 0.661486
PRC train: 0.998542	val: 0.550500	test: 0.541150

Epoch: 58
Loss: 0.03847409882018296
ROC train: 0.999913	val: 0.831566	test: 0.633730
PRC train: 0.999462	val: 0.560693	test: 0.532048

Epoch: 59
Loss: 0.03654156040425521
ROC train: 1.000000	val: 0.826309	test: 0.643198
PRC train: 1.000000	val: 0.558307	test: 0.534014

Epoch: 60
Loss: 0.03364205621458759
ROC train: 1.000000	val: 0.803721	test: 0.660694
PRC train: 1.000000	val: 0.545035	test: 0.538686

Epoch: 61
Loss: 0.03212668706210862
ROC train: 1.000000	val: 0.831914	test: 0.672375
PRC train: 1.000000	val: 0.557086	test: 0.539995

Epoch: 62
Loss: 0.03491025760662751
ROC train: 0.999966	val: 0.819815	test: 0.682131
PRC train: 0.999693	val: 0.549363	test: 0.541945

Epoch: 63
Loss: 0.03461308147534888
ROC train: 0.999986	val: 0.796527	test: 0.690726
PRC train: 0.999842	val: 0.546415	test: 0.544076

Epoch: 64
Loss: 0.03006598499488395
ROC train: 0.999990	val: 0.789682	test: 0.684638
PRC train: 0.999894	val: 0.561881	test: 0.545164

Epoch: 65
Loss: 0.0316827789864237
ROC train: 1.000000	val: 0.848223	test: 0.680506
PRC train: 1.000000	val: 0.574067	test: 0.543829

Epoch: 66
Loss: 0.029465275943463376
ROC train: 1.000000	val: 0.829268	test: 0.668128
PRC train: 1.000000	val: 0.554957	test: 0.542822

Epoch: 67
Loss: 0.03541447736722912
ROC train: 0.999990	val: 0.817654	test: 0.641966
PRC train: 0.999946	val: 0.553789	test: 0.535138

Epoch: 68
Loss: 0.02173770672266305
ROC train: 0.999995	val: 0.829405	test: 0.636938
PRC train: 0.999946	val: 0.556549	test: 0.532505

Epoch: 69
Loss: 0.0239892834272583
ROC train: 1.000000	val: 0.832828	test: 0.650441
PRC train: 1.000000	val: 0.557598	test: 0.535255

Epoch: 70
Loss: 0.027056135384012452
ROC train: 0.999995	val: 0.859549	test: 0.658547
PRC train: 0.999946	val: 0.573024	test: 0.538673

Epoch: 71
Loss: 0.023447351359417182
ROC train: 1.000000	val: 0.847949	test: 0.662319
PRC train: 1.000000	val: 0.572080	test: 0.542446

Epoch: 72
Loss: 0.022348864419118177
ROC train: 1.000000	val: 0.843665	test: 0.663257
PRC train: 1.000000	val: 0.560217	test: 0.539399

Epoch: 73
Loss: 0.018159208413362594
ROC train: 0.999973	val: 0.830080	test: 0.661882
PRC train: 0.999892	val: 0.554577	test: 0.538388

Epoch: 74
Loss: 0.029474559291174124
ROC train: 0.999986	val: 0.819678	test: 0.657979
PRC train: 0.999837	val: 0.561617	test: 0.538907

Epoch: 75
Loss: 0.01954077527037614
ROC train: 0.999983	val: 0.788199	test: 0.666510
PRC train: 0.999999	val: 0.554856	test: 0.544302

Epoch: 76
Loss: 0.024076263471191257
ROC train: 1.000000	val: 0.769356	test: 0.667653
PRC train: 1.000000	val: 0.550424	test: 0.544650

Epoch: 77
Loss: 0.023998178351951505
ROC train: 1.000000	val: 0.798038	test: 0.678859
PRC train: 1.000000	val: 0.558875	test: 0.546179

Epoch: 78
Loss: 0.023052576528306577
ROC train: 1.000000	val: 0.795916	test: 0.703168
PRC train: 1.000000	val: 0.566362	test: 0.554756

Epoch: 79
Loss: 0.02229494299576284
ROC train: 1.000000	val: 0.796815	test: 0.694742
PRC train: 1.000000	val: 0.558122	test: 0.545529

Epoch: 80
Loss: 0.024400976976911716
ROC train: 0.999990	val: 0.792258	test: 0.669838
PRC train: 0.999894	val: 0.553309	test: 0.538886

Epoch: 81
Loss: 0.025446760716933116
ROC train: 0.999994	val: 0.823449	test: 0.676872
PRC train: 1.000000	val: 0.557635	test: 0.544566

Epoch: 82
Loss: 0.018312424754944767
ROC train: 1.000000	val: 0.815394	test: 0.689325
PRC train: 1.000000	val: 0.555217	test: 0.548535

Epoch: 83
Loss: 0.027738388261803977
ROC train: 0.999972	val: 0.811786	test: 0.701193
PRC train: 0.999998	val: 0.550755	test: 0.548697

Epoch: 84
Loss: 0.028612567597423783
ROC train: 0.999995	val: 0.798425	test: 0.711610
PRC train: 0.999946	val: 0.548265	test: 0.550955

Epoch: 85
Loss: 0.02372154298898773
ROC train: 1.000000	val: 0.800273	test: 0.701331
PRC train: 1.000000	val: 0.548559	test: 0.549334

Epoch: 86
Loss: 0.022727878764180105
ROC train: 1.000000	val: 0.818504	test: 0.685616
PRC train: 1.000000	val: 0.554327	test: 0.545694

Epoch: 87
Loss: 0.018704649878435366
ROC train: 0.999995	val: 0.804332	test: 0.667728
PRC train: 0.999946	val: 0.548261	test: 0.540049

Epoch: 88
Loss: 0.02207128570743309
ROC train: 0.999983	val: 0.811849	test: 0.678396
PRC train: 0.999999	val: 0.550946	test: 0.544927

Epoch: 89
Loss: 0.034114741153892805
ROC train: 0.999972	val: 0.804019	test: 0.692788
PRC train: 0.999998	val: 0.549856	test: 0.550493

Epoch: 90
Loss: 0.022954251932641785
ROC train: 0.999928	val: 0.779094	test: 0.669140
PRC train: 0.999888	val: 0.551434	test: 0.545221

Epoch: 91
Loss: 0.027752038170679116
ROC train: 0.999977	val: 0.767181	test: 0.680719
PRC train: 0.999998	val: 0.555910	test: 0.547709

Epoch: 92
Loss: 0.031783423278697154
ROC train: 0.999966	val: 0.795589	test: 0.697558
PRC train: 0.999998	val: 0.557761	test: 0.555525

Epoch: 93
Loss: 0.02374013097483666
PRC train: 0.998465	val: 0.559696	test: 0.516613

Epoch: 33
Loss: 0.06832553599762818
ROC train: 0.999289	val: 0.833411	test: 0.525510
PRC train: 0.998290	val: 0.567287	test: 0.517403

Epoch: 34
Loss: 0.07975192030064622
ROC train: 0.999354	val: 0.798098	test: 0.543705
PRC train: 0.996759	val: 0.560343	test: 0.520944

Epoch: 35
Loss: 0.08191253886849074
ROC train: 0.999218	val: 0.756393	test: 0.561425
PRC train: 0.996381	val: 0.541635	test: 0.528131

Epoch: 36
Loss: 0.06267009960112908
ROC train: 0.998430	val: 0.654014	test: 0.553043
PRC train: 0.993009	val: 0.548300	test: 0.519693

Epoch: 37
Loss: 0.07293670574184861
ROC train: 0.999660	val: 0.707495	test: 0.571044
PRC train: 0.998078	val: 0.535284	test: 0.526923

Epoch: 38
Loss: 0.07526154298346673
ROC train: 0.999571	val: 0.757553	test: 0.596552
PRC train: 0.998112	val: 0.549216	test: 0.533065

Epoch: 39
Loss: 0.06488219563630293
ROC train: 0.999339	val: 0.776395	test: 0.592529
PRC train: 0.998120	val: 0.550799	test: 0.530507

Epoch: 40
Loss: 0.06796818319470081
ROC train: 0.999686	val: 0.770340	test: 0.557959
PRC train: 0.997325	val: 0.543672	test: 0.520661

Epoch: 41
Loss: 0.06105347625576117
ROC train: 0.999522	val: 0.758104	test: 0.544986
PRC train: 0.996601	val: 0.541744	test: 0.521481

Epoch: 42
Loss: 0.054521839018882676
ROC train: 0.998769	val: 0.719397	test: 0.527935
PRC train: 0.991900	val: 0.533700	test: 0.516258

Epoch: 43
Loss: 0.062362375150887286
ROC train: 0.999649	val: 0.725314	test: 0.530333
PRC train: 0.997622	val: 0.538179	test: 0.523357

Epoch: 44
Loss: 0.04595360613636783
ROC train: 0.999863	val: 0.730171	test: 0.548583
PRC train: 0.998811	val: 0.537189	test: 0.526735

Epoch: 45
Loss: 0.05126314544018351
ROC train: 0.999974	val: 0.732093	test: 0.533044
PRC train: 0.999839	val: 0.542827	test: 0.533983

Epoch: 46
Loss: 0.049307056545936055
ROC train: 0.999986	val: 0.734514	test: 0.535080
PRC train: 0.999839	val: 0.548689	test: 0.534762

Epoch: 47
Loss: 0.043820764816490596
ROC train: 0.999751	val: 0.762349	test: 0.583654
PRC train: 0.999929	val: 0.544171	test: 0.534179

Epoch: 48
Loss: 0.05081278890202292
ROC train: 0.999898	val: 0.773436	test: 0.608419
PRC train: 0.999993	val: 0.547497	test: 0.547953

Epoch: 49
Loss: 0.040689547022624326
ROC train: 0.999995	val: 0.723589	test: 0.576340
PRC train: 0.999946	val: 0.535043	test: 0.541645

Epoch: 50
Loss: 0.04346409974715364
ROC train: 0.999923	val: 0.669296	test: 0.554492
PRC train: 0.999835	val: 0.526830	test: 0.537750

Epoch: 51
Loss: 0.05515445078044634
ROC train: 0.999994	val: 0.699602	test: 0.547829
PRC train: 1.000000	val: 0.533356	test: 0.528204

Epoch: 52
Loss: 0.04355401581698873
ROC train: 0.999591	val: 0.759152	test: 0.556510
PRC train: 0.999655	val: 0.545513	test: 0.522362

Epoch: 53
Loss: 0.03907838730782335
ROC train: 0.999729	val: 0.732156	test: 0.548583
PRC train: 0.999873	val: 0.536455	test: 0.521811

Epoch: 54
Loss: 0.04031221189344384
ROC train: 0.999921	val: 0.786400	test: 0.548184
PRC train: 0.999994	val: 0.546221	test: 0.558418

Epoch: 55
Loss: 0.039578546740985834
ROC train: 0.999989	val: 0.798200	test: 0.539551
PRC train: 0.999999	val: 0.546019	test: 0.533695

Epoch: 56
Loss: 0.03829023329233465
ROC train: 0.999972	val: 0.786062	test: 0.541494
PRC train: 0.999998	val: 0.550072	test: 0.534709

Epoch: 57
Loss: 0.037746800889774876
ROC train: 0.999852	val: 0.730846	test: 0.535192
PRC train: 0.999989	val: 0.544444	test: 0.533470

Epoch: 58
Loss: 0.035740734192669016
ROC train: 0.999983	val: 0.738123	test: 0.549327
PRC train: 0.999999	val: 0.547539	test: 0.524991

Epoch: 59
Loss: 0.030195422283394547
ROC train: 0.999957	val: 0.721291	test: 0.543106
PRC train: 0.999539	val: 0.542760	test: 0.520630

Epoch: 60
Loss: 0.030056660749796748
ROC train: 0.999918	val: 0.688202	test: 0.540889
PRC train: 0.999149	val: 0.532397	test: 0.517471

Epoch: 61
Loss: 0.027033972489894413
ROC train: 0.999966	val: 0.701738	test: 0.546641
PRC train: 0.999633	val: 0.535181	test: 0.516475

Epoch: 62
Loss: 0.03313378152915853
ROC train: 0.999990	val: 0.727321	test: 0.552419
PRC train: 0.999894	val: 0.550260	test: 0.524137

Epoch: 63
Loss: 0.034802857541444306
ROC train: 1.000000	val: 0.728084	test: 0.552606
PRC train: 1.000000	val: 0.537393	test: 0.518540

Epoch: 64
Loss: 0.030945181850425392
ROC train: 0.999995	val: 0.690574	test: 0.536828
PRC train: 0.999946	val: 0.531323	test: 0.518057

Epoch: 65
Loss: 0.022629345985270336
ROC train: 1.000000	val: 0.727947	test: 0.559046
PRC train: 1.000000	val: 0.538857	test: 0.521672

Epoch: 66
Loss: 0.02399952849179987
ROC train: 1.000000	val: 0.753132	test: 0.565336
PRC train: 1.000000	val: 0.546367	test: 0.522170

Epoch: 67
Loss: 0.025525307748332858
ROC train: 1.000000	val: 0.724039	test: 0.542362
PRC train: 1.000000	val: 0.540222	test: 0.520922

Epoch: 68
Loss: 0.024469972725568438
ROC train: 0.999994	val: 0.748975	test: 0.554429
PRC train: 1.000000	val: 0.542372	test: 0.522019

Epoch: 69
Loss: 0.03288868772178814
ROC train: 0.999961	val: 0.683008	test: 0.520614
PRC train: 0.999944	val: 0.544443	test: 0.515555

Epoch: 70
Loss: 0.026812065773143122
ROC train: 0.999995	val: 0.654351	test: 0.489091
PRC train: 0.999946	val: 0.539884	test: 0.508922

Epoch: 71
Loss: 0.0302011869060489
ROC train: 0.999990	val: 0.690999	test: 0.480478
PRC train: 0.999892	val: 0.536337	test: 0.507946

Epoch: 72
Loss: 0.03228306463840166
ROC train: 0.999995	val: 0.745766	test: 0.513169
PRC train: 0.999946	val: 0.544570	test: 0.513909

Epoch: 73
Loss: 0.026641072927667563
ROC train: 1.000000	val: 0.749811	test: 0.541694
PRC train: 1.000000	val: 0.544423	test: 0.518493

Epoch: 74
Loss: 0.01909927099703928
ROC train: 1.000000	val: 0.721579	test: 0.567906
PRC train: 1.000000	val: 0.541305	test: 0.523456

Epoch: 75
Loss: 0.0218824403110362
ROC train: 1.000000	val: 0.751772	test: 0.564197
PRC train: 1.000000	val: 0.543534	test: 0.522490

Epoch: 76
Loss: 0.024573175518188384
ROC train: 1.000000	val: 0.773612	test: 0.554392
PRC train: 1.000000	val: 0.547420	test: 0.518397

Epoch: 77
Loss: 0.021069542628416926
ROC train: 1.000000	val: 0.771876	test: 0.545296
PRC train: 1.000000	val: 0.549280	test: 0.513608

Epoch: 78
Loss: 0.020105126365380744
ROC train: 1.000000	val: 0.749874	test: 0.530893
PRC train: 1.000000	val: 0.545522	test: 0.513007

Epoch: 79
Loss: 0.020357164147453127
ROC train: 1.000000	val: 0.757865	test: 0.532305
PRC train: 1.000000	val: 0.547002	test: 0.514285

Epoch: 80
Loss: 0.016653955298568786
ROC train: 1.000000	val: 0.770702	test: 0.528671
PRC train: 1.000000	val: 0.548944	test: 0.512312

Epoch: 81
Loss: 0.02515439351297268
ROC train: 1.000000	val: 0.776771	test: 0.537053
PRC train: 1.000000	val: 0.547472	test: 0.513251

Epoch: 82
Loss: 0.023618100235869032
ROC train: 1.000000	val: 0.727634	test: 0.563147
PRC train: 1.000000	val: 0.538971	test: 0.517796

Epoch: 83
Loss: 0.014751234730372733
ROC train: 0.999994	val: 0.714034	test: 0.569363
PRC train: 1.000000	val: 0.539143	test: 0.520399

Epoch: 84
Loss: 0.019221712223322626
ROC train: 1.000000	val: 0.649727	test: 0.544811
PRC train: 1.000000	val: 0.531147	test: 0.516523

Epoch: 85
Loss: 0.017919251278640358
ROC train: 1.000000	val: 0.618047	test: 0.521738
PRC train: 1.000000	val: 0.525941	test: 0.511560

Epoch: 86
Loss: 0.015751673501824742
ROC train: 0.999981	val: 0.652587	test: 0.502938
PRC train: 0.999792	val: 0.530100	test: 0.506190

Epoch: 87
Loss: 0.023427605350269134
ROC train: 1.000000	val: 0.640200	test: 0.515530
PRC train: 1.000000	val: 0.530669	test: 0.509176

Epoch: 88
Loss: 0.02082368719969662
ROC train: 0.999994	val: 0.654035	test: 0.519388
PRC train: 1.000000	val: 0.533043	test: 0.512710

Epoch: 89
Loss: 0.013165762527349027
ROC train: 0.999989	val: 0.695329	test: 0.534740
PRC train: 0.999999	val: 0.540802	test: 0.517609

Epoch: 90
Loss: 0.022172949451091294
ROC train: 1.000000	val: 0.683504	test: 0.543623
PRC train: 1.000000	val: 0.537440	test: 0.519230

Epoch: 91
Loss: 0.017188750754873215
ROC train: 1.000000	val: 0.625690	test: 0.529145
PRC train: 1.000000	val: 0.527343	test: 0.515056

Epoch: 92
Loss: 0.0147999606197677
ROC train: 1.000000	val: 0.590939	test: 0.521432
PRC train: 1.000000	val: 0.520218	test: 0.513766

Epoch: 93
Loss: 0.012431703013658476
ROC train: 0.982438	val: 0.784151	test: 0.844500
PRC train: 0.907273	val: 0.633332	test: 0.642285

Epoch: 95
Loss: 0.1104502672823046
ROC train: 0.982194	val: 0.777632	test: 0.844863
PRC train: 0.906595	val: 0.624372	test: 0.644152

Epoch: 96
Loss: 0.10741170436566741
ROC train: 0.985105	val: 0.767930	test: 0.859154
PRC train: 0.918132	val: 0.653365	test: 0.650509

Epoch: 97
Loss: 0.09687808786231233
ROC train: 0.985908	val: 0.761587	test: 0.867887
PRC train: 0.918521	val: 0.644244	test: 0.633806

Epoch: 98
Loss: 0.11209883507331353
ROC train: 0.983983	val: 0.778869	test: 0.868287
PRC train: 0.911711	val: 0.637886	test: 0.633682

Epoch: 99
Loss: 0.107216530166648
ROC train: 0.984993	val: 0.788322	test: 0.857443
PRC train: 0.922216	val: 0.640198	test: 0.637918

Epoch: 100
Loss: 0.11514200590351087
ROC train: 0.985073	val: 0.782752	test: 0.858786
PRC train: 0.924477	val: 0.666760	test: 0.641299

Epoch: 101
Loss: 0.11088266770657276
ROC train: 0.983913	val: 0.784713	test: 0.856793
PRC train: 0.917868	val: 0.648404	test: 0.649385

Epoch: 102
Loss: 0.10523983285276377
ROC train: 0.984685	val: 0.790806	test: 0.855493
PRC train: 0.922701	val: 0.649495	test: 0.618421

Epoch: 103
Loss: 0.10017812439448921
ROC train: 0.985539	val: 0.776683	test: 0.862314
PRC train: 0.925027	val: 0.643631	test: 0.623617

Epoch: 104
Loss: 0.11126480288140353
ROC train: 0.985671	val: 0.759127	test: 0.875881
PRC train: 0.924116	val: 0.650311	test: 0.645782

Epoch: 105
Loss: 0.11049744058287085
ROC train: 0.986872	val: 0.775710	test: 0.875193
PRC train: 0.931470	val: 0.639709	test: 0.653009

Epoch: 106
Loss: 0.10399799510559267
ROC train: 0.985554	val: 0.793941	test: 0.869971
PRC train: 0.923527	val: 0.651800	test: 0.651874

Epoch: 107
Loss: 0.09788251289873057
ROC train: 0.986127	val: 0.786112	test: 0.868160
PRC train: 0.923890	val: 0.656331	test: 0.660070

Epoch: 108
Loss: 0.09502365138561493
ROC train: 0.984396	val: 0.773387	test: 0.869109
PRC train: 0.917499	val: 0.653030	test: 0.664489

Epoch: 109
Loss: 0.10015623338570505
ROC train: 0.986264	val: 0.795951	test: 0.849072
PRC train: 0.928361	val: 0.645948	test: 0.626556

Epoch: 110
Loss: 0.10356179422536269
ROC train: 0.987450	val: 0.810461	test: 0.857230
PRC train: 0.932910	val: 0.663455	test: 0.631829

Epoch: 111
Loss: 0.10767952622623427
ROC train: 0.987773	val: 0.804754	test: 0.866867
PRC train: 0.934044	val: 0.647538	test: 0.635901

Epoch: 112
Loss: 0.09171010821011832
ROC train: 0.986641	val: 0.803106	test: 0.854313
PRC train: 0.931376	val: 0.634039	test: 0.636242

Epoch: 113
Loss: 0.0919835769807904
ROC train: 0.986732	val: 0.788821	test: 0.871309
PRC train: 0.929305	val: 0.626782	test: 0.661132

Epoch: 114
Loss: 0.0890305032807107
ROC train: 0.986880	val: 0.768942	test: 0.862889
PRC train: 0.931419	val: 0.618959	test: 0.653093

Epoch: 115
Loss: 0.09294738570536822
ROC train: 0.986726	val: 0.777608	test: 0.846269
PRC train: 0.931580	val: 0.629030	test: 0.632367

Epoch: 116
Loss: 0.09574198126793818
ROC train: 0.988007	val: 0.788122	test: 0.860278
PRC train: 0.936269	val: 0.646617	test: 0.631411

Epoch: 117
Loss: 0.0934521014973285
ROC train: 0.987374	val: 0.773050	test: 0.885462
PRC train: 0.932038	val: 0.647934	test: 0.652790

Epoch: 118
Loss: 0.10333971431879493
ROC train: 0.986944	val: 0.764135	test: 0.891158
PRC train: 0.931571	val: 0.644549	test: 0.677743

Epoch: 119
Loss: 0.0895329220252441
ROC train: 0.986095	val: 0.794191	test: 0.875063
PRC train: 0.927348	val: 0.637475	test: 0.664422

Epoch: 120
Loss: 0.09741436922573651
ROC train: 0.984837	val: 0.809749	test: 0.864051
PRC train: 0.921430	val: 0.672627	test: 0.660481

Early stopping
Best (ROC):	 train: 0.889867	val: 0.907685	test: 0.649320
Best (PRC):	 train: 0.710728	val: 0.665301	test: 0.547004

ROC train: 0.984868	val: 0.759390	test: 0.837318
PRC train: 0.918997	val: 0.586926	test: 0.679879

Epoch: 95
Loss: 0.11051372644156872
ROC train: 0.984832	val: 0.764286	test: 0.848878
PRC train: 0.917492	val: 0.594357	test: 0.686200

Epoch: 96
Loss: 0.10145791241365887
ROC train: 0.985369	val: 0.791769	test: 0.824281
PRC train: 0.920962	val: 0.612637	test: 0.659741

Epoch: 97
Loss: 0.10564012684157213
ROC train: 0.985525	val: 0.803007	test: 0.811540
PRC train: 0.920926	val: 0.606531	test: 0.683592

Epoch: 98
Loss: 0.10761536828912384
ROC train: 0.985371	val: 0.812734	test: 0.822227
PRC train: 0.917535	val: 0.651924	test: 0.647654

Epoch: 99
Loss: 0.09710785498571581
ROC train: 0.985412	val: 0.826183	test: 0.816744
PRC train: 0.917443	val: 0.667169	test: 0.629988

Epoch: 100
Loss: 0.10578151042823994
ROC train: 0.985723	val: 0.825821	test: 0.792621
PRC train: 0.919167	val: 0.675132	test: 0.619016

Epoch: 101
Loss: 0.11713226696254249
ROC train: 0.984796	val: 0.809238	test: 0.817117
PRC train: 0.915279	val: 0.681799	test: 0.677688

Epoch: 102
Loss: 0.11694221559130313
ROC train: 0.985422	val: 0.783016	test: 0.851258
PRC train: 0.917753	val: 0.642601	test: 0.712911

Epoch: 103
Loss: 0.10081155751831225
ROC train: 0.985493	val: 0.770491	test: 0.837179
PRC train: 0.918896	val: 0.634550	test: 0.692297

Epoch: 104
Loss: 0.11030702932159404
ROC train: 0.986364	val: 0.784189	test: 0.849259
PRC train: 0.924013	val: 0.597817	test: 0.682861

Epoch: 105
Loss: 0.10699950113991505
ROC train: 0.986080	val: 0.805017	test: 0.838423
PRC train: 0.921393	val: 0.602197	test: 0.682532

Epoch: 106
Loss: 0.10170713211774536
ROC train: 0.986871	val: 0.798674	test: 0.814895
PRC train: 0.928940	val: 0.637576	test: 0.653108

Epoch: 107
Loss: 0.09728240941115067
ROC train: 0.986014	val: 0.798200	test: 0.802116
PRC train: 0.927329	val: 0.596558	test: 0.656864

Epoch: 108
Loss: 0.10103164508712599
ROC train: 0.987074	val: 0.812710	test: 0.799998
PRC train: 0.932383	val: 0.608139	test: 0.661771

Epoch: 109
Loss: 0.10445402296594657
ROC train: 0.987866	val: 0.805604	test: 0.814969
PRC train: 0.934627	val: 0.674474	test: 0.675399

Epoch: 110
Loss: 0.09797680544191484
ROC train: 0.987777	val: 0.786087	test: 0.821278
PRC train: 0.931449	val: 0.631696	test: 0.684582

Epoch: 111
Loss: 0.09383206800997508
ROC train: 0.986531	val: 0.786674	test: 0.839872
PRC train: 0.925234	val: 0.653089	test: 0.675803

Epoch: 112
Loss: 0.10141092708563841
ROC train: 0.987532	val: 0.825958	test: 0.842726
PRC train: 0.930739	val: 0.652190	test: 0.667838

Epoch: 113
Loss: 0.09942015006014834
ROC train: 0.987449	val: 0.854341	test: 0.835450
PRC train: 0.932885	val: 0.630264	test: 0.679710

Epoch: 114
Loss: 0.1075989399371194
ROC train: 0.988218	val: 0.840692	test: 0.853682
PRC train: 0.937677	val: 0.646367	test: 0.691442

Epoch: 115
Loss: 0.10262854894153124
ROC train: 0.986122	val: 0.816505	test: 0.845337
PRC train: 0.929245	val: 0.649914	test: 0.691494

Epoch: 116
Loss: 0.10067194753408656
ROC train: 0.986104	val: 0.784776	test: 0.838991
PRC train: 0.929635	val: 0.615914	test: 0.688070

Epoch: 117
Loss: 0.10177431855818542
ROC train: 0.986472	val: 0.790782	test: 0.855281
PRC train: 0.925700	val: 0.596163	test: 0.696535

Epoch: 118
Loss: 0.10323656894643192
ROC train: 0.986571	val: 0.823498	test: 0.842289
PRC train: 0.929329	val: 0.591835	test: 0.678742

Epoch: 119
Loss: 0.09557385852017775
ROC train: 0.986416	val: 0.830741	test: 0.836118
PRC train: 0.930239	val: 0.593507	test: 0.646830

Epoch: 120
Loss: 0.10208870438355362
ROC train: 0.987706	val: 0.788273	test: 0.854007
PRC train: 0.935431	val: 0.635348	test: 0.685889

Early stopping
Best (ROC):	 train: 0.921772	val: 0.892838	test: 0.716040
Best (PRC):	 train: 0.729900	val: 0.596721	test: 0.596935

ROC train: 0.985086	val: 0.780306	test: 0.808574
PRC train: 0.918797	val: 0.570091	test: 0.633240

Epoch: 95
Loss: 0.10492978462762881
ROC train: 0.986928	val: 0.795540	test: 0.843496
PRC train: 0.928366	val: 0.594247	test: 0.655692

Epoch: 96
Loss: 0.10001182650713072
ROC train: 0.986666	val: 0.795926	test: 0.850828
PRC train: 0.923264	val: 0.607454	test: 0.670351

Epoch: 97
Loss: 0.10979533221712719
ROC train: 0.986780	val: 0.757879	test: 0.858093
PRC train: 0.925927	val: 0.586690	test: 0.667795

Epoch: 98
Loss: 0.10547266127661814
ROC train: 0.987145	val: 0.765435	test: 0.857581
PRC train: 0.928305	val: 0.588919	test: 0.654982

Epoch: 99
Loss: 0.10303544759077114
ROC train: 0.987257	val: 0.804944	test: 0.853420
PRC train: 0.928605	val: 0.592097	test: 0.651342

Epoch: 100
Loss: 0.10174218450678633
ROC train: 0.986818	val: 0.824260	test: 0.838692
PRC train: 0.925212	val: 0.631271	test: 0.655032

Epoch: 101
Loss: 0.09492540979909243
ROC train: 0.987273	val: 0.818779	test: 0.849310
PRC train: 0.929652	val: 0.632344	test: 0.671228

Epoch: 102
Loss: 0.1006081335361377
ROC train: 0.987404	val: 0.818167	test: 0.860790
PRC train: 0.928716	val: 0.644090	test: 0.677995

Epoch: 103
Loss: 0.10652790586016267
ROC train: 0.987767	val: 0.785563	test: 0.874794
PRC train: 0.931423	val: 0.636350	test: 0.681077

Epoch: 104
Loss: 0.10296116043746824
ROC train: 0.986440	val: 0.749150	test: 0.876680
PRC train: 0.923997	val: 0.636167	test: 0.667460

Epoch: 105
Loss: 0.10229982739316486
ROC train: 0.984928	val: 0.748926	test: 0.888786
PRC train: 0.920926	val: 0.603783	test: 0.668298

Epoch: 106
Loss: 0.09621098949372257
ROC train: 0.986760	val: 0.757517	test: 0.860891
PRC train: 0.929808	val: 0.591955	test: 0.656276

Epoch: 107
Loss: 0.11640991519453459
ROC train: 0.986333	val: 0.746118	test: 0.830385
PRC train: 0.927552	val: 0.585688	test: 0.644170

Epoch: 108
Loss: 0.09520005640742724
ROC train: 0.984770	val: 0.762451	test: 0.800742
PRC train: 0.921235	val: 0.595558	test: 0.636974

Epoch: 109
Loss: 0.10035322698953694
ROC train: 0.985437	val: 0.793930	test: 0.828472
PRC train: 0.920320	val: 0.641239	test: 0.647189

Epoch: 110
Loss: 0.0989802554822791
ROC train: 0.987125	val: 0.841342	test: 0.859378
PRC train: 0.925452	val: 0.605639	test: 0.668470

Epoch: 111
Loss: 0.09762342331682339
ROC train: 0.987029	val: 0.861696	test: 0.848385
PRC train: 0.927501	val: 0.599639	test: 0.652763

Epoch: 112
Loss: 0.10773328875367642
ROC train: 0.987263	val: 0.868264	test: 0.843051
PRC train: 0.928719	val: 0.608727	test: 0.657899

Epoch: 113
Loss: 0.11225017503579056
ROC train: 0.987503	val: 0.855965	test: 0.825525
PRC train: 0.930567	val: 0.610513	test: 0.644968

Epoch: 114
Loss: 0.10188369488586249
ROC train: 0.987154	val: 0.832902	test: 0.817393
PRC train: 0.930389	val: 0.610243	test: 0.641261

Epoch: 115
Loss: 0.09953167995515584
ROC train: 0.987360	val: 0.825209	test: 0.818992
PRC train: 0.930741	val: 0.611856	test: 0.640391

Epoch: 116
Loss: 0.09950528940929079
ROC train: 0.987501	val: 0.819253	test: 0.850484
PRC train: 0.931414	val: 0.623322	test: 0.653589

Epoch: 117
Loss: 0.0914678443434033
ROC train: 0.988034	val: 0.786263	test: 0.836387
PRC train: 0.935057	val: 0.599104	test: 0.646309

Epoch: 118
Loss: 0.094834143750435
ROC train: 0.987941	val: 0.802621	test: 0.827587
PRC train: 0.936935	val: 0.597242	test: 0.649690

Epoch: 119
Loss: 0.10222304380173505
ROC train: 0.988006	val: 0.815033	test: 0.825301
PRC train: 0.934834	val: 0.600304	test: 0.646565

Epoch: 120
Loss: 0.08891085886211883
ROC train: 0.987335	val: 0.826158	test: 0.808287
PRC train: 0.931124	val: 0.608854	test: 0.639184

Early stopping
Best (ROC):	 train: 0.933171	val: 0.901704	test: 0.670230
Best (PRC):	 train: 0.783786	val: 0.638557	test: 0.539123
All runs completed.

ROC train: 0.999966	val: 0.719084	test: 0.654849
PRC train: 0.999669	val: 0.542593	test: 0.555981

Epoch: 94
Loss: 0.017669221545449126
ROC train: 0.999994	val: 0.750163	test: 0.666716
PRC train: 1.000000	val: 0.593020	test: 0.555273

Epoch: 95
Loss: 0.02247956682991325
ROC train: 1.000000	val: 0.774287	test: 0.677746
PRC train: 1.000000	val: 0.616054	test: 0.558742

Epoch: 96
Loss: 0.013530082298229112
ROC train: 1.000000	val: 0.740523	test: 0.657209
PRC train: 1.000000	val: 0.568479	test: 0.544118

Epoch: 97
Loss: 0.02243138512551809
ROC train: 1.000000	val: 0.731633	test: 0.660975
PRC train: 1.000000	val: 0.550422	test: 0.544326

Epoch: 98
Loss: 0.017682399721961468
ROC train: 0.999989	val: 0.731070	test: 0.653549
PRC train: 0.999999	val: 0.618965	test: 0.546454

Epoch: 99
Loss: 0.019817638289021715
ROC train: 1.000000	val: 0.758691	test: 0.629927
PRC train: 1.000000	val: 0.590313	test: 0.538620

Epoch: 100
Loss: 0.025333255246180776
ROC train: 1.000000	val: 0.751874	test: 0.627529
PRC train: 1.000000	val: 0.579486	test: 0.539698

Epoch: 101
Loss: 0.012851878444713721
ROC train: 1.000000	val: 0.761239	test: 0.624862
PRC train: 1.000000	val: 0.601366	test: 0.537062

Epoch: 102
Loss: 0.020291262610671434
ROC train: 1.000000	val: 0.773563	test: 0.645623
PRC train: 1.000000	val: 0.630291	test: 0.547096

Epoch: 103
Loss: 0.016116818001503695
ROC train: 1.000000	val: 0.774424	test: 0.678258
PRC train: 1.000000	val: 0.686902	test: 0.559382

Epoch: 104
Loss: 0.024489167928240956
ROC train: 1.000000	val: 0.766594	test: 0.680869
PRC train: 1.000000	val: 0.662069	test: 0.567841

Epoch: 105
Loss: 0.027311849342631544
ROC train: 1.000000	val: 0.686143	test: 0.681444
PRC train: 1.000000	val: 0.556944	test: 0.563042

Epoch: 106
Loss: 0.032556557810634164
ROC train: 1.000000	val: 0.723016	test: 0.689352
PRC train: 1.000000	val: 0.550862	test: 0.570297

Epoch: 107
Loss: 0.017536798841785863
ROC train: 0.999995	val: 0.724777	test: 0.689157
PRC train: 0.999946	val: 0.542848	test: 0.573572

Epoch: 108
Loss: 0.024798737113607635
ROC train: 0.999957	val: 0.721256	test: 0.672812
PRC train: 0.999840	val: 0.543343	test: 0.560177

Epoch: 109
Loss: 0.027148480722844166
ROC train: 1.000000	val: 0.753258	test: 0.679214
PRC train: 1.000000	val: 0.586086	test: 0.574222

Epoch: 110
Loss: 0.020961875579500744
ROC train: 1.000000	val: 0.742020	test: 0.682606
PRC train: 1.000000	val: 0.598982	test: 0.570168

Epoch: 111
Loss: 0.018816380926834214
ROC train: 0.999957	val: 0.716160	test: 0.657508
PRC train: 0.999519	val: 0.545234	test: 0.546439

Epoch: 112
Loss: 0.021481475393868698
ROC train: 0.999901	val: 0.697255	test: 0.662293
PRC train: 0.999082	val: 0.560511	test: 0.546462

Epoch: 113
Loss: 0.022626437654438575
ROC train: 0.999989	val: 0.670783	test: 0.698301
PRC train: 0.999999	val: 0.586121	test: 0.569682

Epoch: 114
Loss: 0.015634478643087042
ROC train: 1.000000	val: 0.648033	test: 0.687652
PRC train: 1.000000	val: 0.610628	test: 0.561350

Epoch: 115
Loss: 0.017865869960956137
ROC train: 1.000000	val: 0.689801	test: 0.668666
PRC train: 1.000000	val: 0.561970	test: 0.551192

Epoch: 116
Loss: 0.018776315436768533
ROC train: 1.000000	val: 0.691537	test: 0.643493
PRC train: 1.000000	val: 0.555682	test: 0.543805

Epoch: 117
Loss: 0.025689346604063013
ROC train: 1.000000	val: 0.721333	test: 0.639784
PRC train: 1.000000	val: 0.545446	test: 0.543189

Epoch: 118
Loss: 0.024859626007520085
ROC train: 0.999994	val: 0.705126	test: 0.650265
PRC train: 1.000000	val: 0.530939	test: 0.543372

Epoch: 119
Loss: 0.02145527132347578
ROC train: 1.000000	val: 0.710717	test: 0.665024
PRC train: 1.000000	val: 0.538219	test: 0.546685

Epoch: 120
Loss: 0.018243027535952104
ROC train: 1.000000	val: 0.727124	test: 0.697084
PRC train: 1.000000	val: 0.556228	test: 0.558739

Early stopping
Best (ROC):	 train: 0.828938	val: 0.853891	test: 0.572283
Best (PRC):	 train: 0.642387	val: 0.566896	test: 0.524681

ROC train: 1.000000	val: 0.652865	test: 0.749665
PRC train: 1.000000	val: 0.545888	test: 0.580007

Epoch: 94
Loss: 0.01658231134145593
ROC train: 1.000000	val: 0.709368	test: 0.669386
PRC train: 1.000000	val: 0.607500	test: 0.555537

Epoch: 95
Loss: 0.01767579917142018
ROC train: 1.000000	val: 0.745991	test: 0.662659
PRC train: 1.000000	val: 0.618853	test: 0.552593

Epoch: 96
Loss: 0.025359347286624128
ROC train: 1.000000	val: 0.762399	test: 0.681309
PRC train: 1.000000	val: 0.565223	test: 0.562381

Epoch: 97
Loss: 0.01748641657464869
ROC train: 1.000000	val: 0.774037	test: 0.700034
PRC train: 1.000000	val: 0.548878	test: 0.566897

Epoch: 98
Loss: 0.015940402705284025
ROC train: 1.000000	val: 0.764222	test: 0.736185
PRC train: 1.000000	val: 0.547137	test: 0.591594

Epoch: 99
Loss: 0.016701081672073147
ROC train: 1.000000	val: 0.764261	test: 0.729270
PRC train: 1.000000	val: 0.544649	test: 0.585189

Epoch: 100
Loss: 0.009521563121534751
ROC train: 1.000000	val: 0.736366	test: 0.710688
PRC train: 1.000000	val: 0.556017	test: 0.595969

Epoch: 101
Loss: 0.018024874654212247
ROC train: 0.999995	val: 0.676440	test: 0.702855
PRC train: 0.999946	val: 0.562907	test: 0.587061

Epoch: 102
Loss: 0.01068533823448779
ROC train: 1.000000	val: 0.639704	test: 0.690663
PRC train: 1.000000	val: 0.558803	test: 0.584992

Epoch: 103
Loss: 0.013376561026514596
ROC train: 1.000000	val: 0.606313	test: 0.714800
PRC train: 1.000000	val: 0.598411	test: 0.593515

Epoch: 104
Loss: 0.016692997777787877
ROC train: 1.000000	val: 0.566330	test: 0.750554
PRC train: 1.000000	val: 0.552663	test: 0.606554

Epoch: 105
Loss: 0.012399827869871092
ROC train: 1.000000	val: 0.526821	test: 0.747170
PRC train: 1.000000	val: 0.549036	test: 0.604544

Epoch: 106
Loss: 0.01010681248996188
ROC train: 1.000000	val: 0.509402	test: 0.728968
PRC train: 1.000000	val: 0.526440	test: 0.591806

Epoch: 107
Loss: 0.012570473517327374
ROC train: 1.000000	val: 0.569377	test: 0.734589
PRC train: 1.000000	val: 0.539895	test: 0.593464

Epoch: 108
Loss: 0.02129811769875472
ROC train: 1.000000	val: 0.619038	test: 0.716193
PRC train: 1.000000	val: 0.536454	test: 0.579984

Epoch: 109
Loss: 0.011313358826927421
ROC train: 1.000000	val: 0.678700	test: 0.715185
PRC train: 1.000000	val: 0.541579	test: 0.581080

Epoch: 110
Loss: 0.013860948984116456
ROC train: 1.000000	val: 0.729897	test: 0.691963
PRC train: 1.000000	val: 0.539274	test: 0.578112

Epoch: 111
Loss: 0.01672258263609172
ROC train: 1.000000	val: 0.688040	test: 0.654737
PRC train: 1.000000	val: 0.530599	test: 0.567489

Epoch: 112
Loss: 0.029858730281049232
ROC train: 0.999985	val: 0.700002	test: 0.646418
PRC train: 0.999893	val: 0.578652	test: 0.575054

Epoch: 113
Loss: 0.021417691131402176
ROC train: 1.000000	val: 0.771377	test: 0.639078
PRC train: 1.000000	val: 0.691733	test: 0.557795

Epoch: 114
Loss: 0.024107659315952677
ROC train: 1.000000	val: 0.808489	test: 0.663739
PRC train: 1.000000	val: 0.623767	test: 0.563966

Epoch: 115
Loss: 0.013889677073917816
ROC train: 0.999994	val: 0.678289	test: 0.688989
PRC train: 1.000000	val: 0.610289	test: 0.579181

Epoch: 116
Loss: 0.02486926448075984
ROC train: 0.999990	val: 0.689390	test: 0.703254
PRC train: 0.999894	val: 0.606786	test: 0.606614

Epoch: 117
Loss: 0.026758205540015366
ROC train: 1.000000	val: 0.713078	test: 0.701999
PRC train: 1.000000	val: 0.608235	test: 0.578771

Epoch: 118
Loss: 0.02909213646253857
ROC train: 1.000000	val: 0.712291	test: 0.698996
PRC train: 1.000000	val: 0.552341	test: 0.572133

Epoch: 119
Loss: 0.025756051693695502
ROC train: 1.000000	val: 0.637269	test: 0.712126
PRC train: 1.000000	val: 0.531890	test: 0.581691

Epoch: 120
Loss: 0.034017794591756766
ROC train: 1.000000	val: 0.605189	test: 0.747484
PRC train: 1.000000	val: 0.598073	test: 0.590968

Early stopping
Best (ROC):	 train: 0.882475	val: 0.850546	test: 0.599831
Best (PRC):	 train: 0.740401	val: 0.633085	test: 0.533276

ROC train: 1.000000	val: 0.824647	test: 0.677174
PRC train: 1.000000	val: 0.579567	test: 0.551447

Epoch: 94
Loss: 0.0262518760525522
ROC train: 1.000000	val: 0.855789	test: 0.690778
PRC train: 1.000000	val: 0.605147	test: 0.546282

Epoch: 95
Loss: 0.021047285525086508
ROC train: 0.999995	val: 0.851593	test: 0.694626
PRC train: 0.999946	val: 0.580942	test: 0.547884

Epoch: 96
Loss: 0.019495274106895465
ROC train: 1.000000	val: 0.836971	test: 0.716485
PRC train: 1.000000	val: 0.654050	test: 0.558647

Epoch: 97
Loss: 0.021021450432736642
ROC train: 1.000000	val: 0.828530	test: 0.688110
PRC train: 1.000000	val: 0.661139	test: 0.559550

Epoch: 98
Loss: 0.022836061095304838
ROC train: 1.000000	val: 0.837895	test: 0.677847
PRC train: 1.000000	val: 0.661888	test: 0.555045

Epoch: 99
Loss: 0.015862274632811153
ROC train: 1.000000	val: 0.852068	test: 0.684724
PRC train: 1.000000	val: 0.643423	test: 0.549219

Epoch: 100
Loss: 0.014547152648166422
ROC train: 1.000000	val: 0.845050	test: 0.677428
PRC train: 1.000000	val: 0.640134	test: 0.548864

Epoch: 101
Loss: 0.013978467850236478
ROC train: 1.000000	val: 0.841416	test: 0.678441
PRC train: 1.000000	val: 0.654022	test: 0.547661

Epoch: 102
Loss: 0.0212158329754898
ROC train: 1.000000	val: 0.833787	test: 0.705929
PRC train: 1.000000	val: 0.655225	test: 0.561919

Epoch: 103
Loss: 0.01962987999965386
ROC train: 0.999995	val: 0.822711	test: 0.716298
PRC train: 0.999946	val: 0.638222	test: 0.573440

Epoch: 104
Loss: 0.02029071402579497
ROC train: 1.000000	val: 0.826594	test: 0.725580
PRC train: 1.000000	val: 0.648532	test: 0.578108

Epoch: 105
Loss: 0.04013114842313581
ROC train: 1.000000	val: 0.837133	test: 0.721755
PRC train: 1.000000	val: 0.653766	test: 0.582026

Epoch: 106
Loss: 0.01937361298551083
ROC train: 0.998254	val: 0.818202	test: 0.683379
PRC train: 0.996324	val: 0.644507	test: 0.560738

Epoch: 107
Loss: 0.029974865739354928
ROC train: 0.997613	val: 0.785138	test: 0.705559
PRC train: 0.994565	val: 0.650030	test: 0.549495

Epoch: 108
Loss: 0.03922532840699241
ROC train: 0.999863	val: 0.780356	test: 0.708678
PRC train: 0.999675	val: 0.665133	test: 0.560228

Epoch: 109
Loss: 0.023877423958643736
ROC train: 0.999984	val: 0.826295	test: 0.688679
PRC train: 0.999945	val: 0.668797	test: 0.552566

Epoch: 110
Loss: 0.015364349384666845
ROC train: 0.999994	val: 0.828955	test: 0.699198
PRC train: 1.000000	val: 0.625490	test: 0.556128

Epoch: 111
Loss: 0.028180128937816396
ROC train: 1.000000	val: 0.832701	test: 0.711625
PRC train: 1.000000	val: 0.680840	test: 0.554606

Epoch: 112
Loss: 0.023902445552606174
ROC train: 0.999983	val: 0.804631	test: 0.718251
PRC train: 0.999999	val: 0.652012	test: 0.556751

Epoch: 113
Loss: 0.028166429902318775
ROC train: 0.999968	val: 0.815918	test: 0.761992
PRC train: 0.999892	val: 0.612095	test: 0.591037

Epoch: 114
Loss: 0.024282015642849014
ROC train: 0.999508	val: 0.782341	test: 0.773272
PRC train: 0.998108	val: 0.605627	test: 0.601242

Epoch: 115
Loss: 0.023003115944241963
ROC train: 1.000000	val: 0.827694	test: 0.744727
PRC train: 1.000000	val: 0.571910	test: 0.578261

Epoch: 116
Loss: 0.019111564944873705
ROC train: 1.000000	val: 0.853104	test: 0.750424
PRC train: 1.000000	val: 0.582800	test: 0.577013

Epoch: 117
Loss: 0.01988862358953687
ROC train: 0.999938	val: 0.842341	test: 0.737145
PRC train: 0.999357	val: 0.600098	test: 0.570480

Epoch: 118
Loss: 0.01748277122789583
ROC train: 1.000000	val: 0.839406	test: 0.703292
PRC train: 1.000000	val: 0.620282	test: 0.554322

Epoch: 119
Loss: 0.02507083285129084
ROC train: 0.999994	val: 0.820065	test: 0.693124
PRC train: 1.000000	val: 0.594991	test: 0.554424

Epoch: 120
Loss: 0.018763371835292527
ROC train: 1.000000	val: 0.806528	test: 0.716548
PRC train: 1.000000	val: 0.604942	test: 0.561801

Early stopping
Best (ROC):	 train: 0.993978	val: 0.863544	test: 0.679341
Best (PRC):	 train: 0.973500	val: 0.643489	test: 0.541194

ROC train: 0.999981	val: 0.573509	test: 0.639093
PRC train: 0.999782	val: 0.514507	test: 0.561111

Epoch: 94
Loss: 0.017758084924761004
ROC train: 0.999994	val: 0.532841	test: 0.654594
PRC train: 1.000000	val: 0.514692	test: 0.560160

Epoch: 95
Loss: 0.021448688660975563
ROC train: 1.000000	val: 0.530318	test: 0.641816
PRC train: 1.000000	val: 0.502043	test: 0.557015

Epoch: 96
Loss: 0.018433474649152813
ROC train: 0.999990	val: 0.506531	test: 0.615228
PRC train: 0.999894	val: 0.510446	test: 0.552926

Epoch: 97
Loss: 0.015838135770799636
ROC train: 1.000000	val: 0.515808	test: 0.611911
PRC train: 1.000000	val: 0.523394	test: 0.548670

Epoch: 98
Loss: 0.018853575360904727
ROC train: 1.000000	val: 0.570649	test: 0.640116
PRC train: 1.000000	val: 0.522103	test: 0.553501

Epoch: 99
Loss: 0.016648485344742368
ROC train: 1.000000	val: 0.597996	test: 0.636444
PRC train: 1.000000	val: 0.525421	test: 0.550505

Epoch: 100
Loss: 0.011496865538463224
ROC train: 1.000000	val: 0.594724	test: 0.642178
PRC train: 1.000000	val: 0.515930	test: 0.548575

Epoch: 101
Loss: 0.018778636276209758
ROC train: 1.000000	val: 0.586547	test: 0.652297
PRC train: 1.000000	val: 0.511688	test: 0.560824

Epoch: 102
Loss: 0.012314045605899365
ROC train: 1.000000	val: 0.572536	test: 0.650911
PRC train: 1.000000	val: 0.513193	test: 0.570325

Epoch: 103
Loss: 0.020524409618820374
ROC train: 1.000000	val: 0.562496	test: 0.664276
PRC train: 1.000000	val: 0.509718	test: 0.568061

Epoch: 104
Loss: 0.014926491232022499
ROC train: 1.000000	val: 0.611022	test: 0.662464
PRC train: 1.000000	val: 0.524031	test: 0.574652

Epoch: 105
Loss: 0.014322111114242444
ROC train: 1.000000	val: 0.594928	test: 0.669834
PRC train: 1.000000	val: 0.521520	test: 0.567315

Epoch: 106
Loss: 0.018074956938424036
ROC train: 1.000000	val: 0.626154	test: 0.664862
PRC train: 1.000000	val: 0.520103	test: 0.573643

Epoch: 107
Loss: 0.011161195778958282
ROC train: 1.000000	val: 0.643486	test: 0.660242
PRC train: 1.000000	val: 0.524731	test: 0.566614

Epoch: 108
Loss: 0.009373414619314582
ROC train: 1.000000	val: 0.641813	test: 0.646926
PRC train: 1.000000	val: 0.524274	test: 0.558055

Epoch: 109
Loss: 0.01091571475681615
ROC train: 1.000000	val: 0.628315	test: 0.645439
PRC train: 1.000000	val: 0.520083	test: 0.557384

Epoch: 110
Loss: 0.012461030699852608
ROC train: 1.000000	val: 0.614979	test: 0.650754
PRC train: 1.000000	val: 0.518088	test: 0.568125

Epoch: 111
Loss: 0.011721355831526739
ROC train: 1.000000	val: 0.582913	test: 0.643078
PRC train: 1.000000	val: 0.515354	test: 0.563643

Epoch: 112
Loss: 0.009594232159772117
ROC train: 1.000000	val: 0.576507	test: 0.630587
PRC train: 1.000000	val: 0.515268	test: 0.561509

Epoch: 113
Loss: 0.008823774954767829
ROC train: 1.000000	val: 0.591765	test: 0.630763
PRC train: 1.000000	val: 0.517022	test: 0.566424

Epoch: 114
Loss: 0.008949085221218444
ROC train: 1.000000	val: 0.616701	test: 0.641244
PRC train: 1.000000	val: 0.526570	test: 0.575211

Epoch: 115
Loss: 0.012970620588015833
ROC train: 1.000000	val: 0.641549	test: 0.633224
PRC train: 1.000000	val: 0.538810	test: 0.577848

Epoch: 116
Loss: 0.014755109298759459
ROC train: 1.000000	val: 0.616276	test: 0.639246
PRC train: 1.000000	val: 0.529395	test: 0.585218

Epoch: 117
Loss: 0.009553196998880704
ROC train: 1.000000	val: 0.608260	test: 0.638895
PRC train: 1.000000	val: 0.530815	test: 0.576048

Epoch: 118
Loss: 0.011456621507983803
ROC train: 1.000000	val: 0.610520	test: 0.644815
PRC train: 1.000000	val: 0.527496	test: 0.583320

Epoch: 119
Loss: 0.006792305084171997
ROC train: 1.000000	val: 0.618237	test: 0.646182
PRC train: 1.000000	val: 0.520990	test: 0.579923

Epoch: 120
Loss: 0.014750026858219092
ROC train: 1.000000	val: 0.616364	test: 0.642548
PRC train: 1.000000	val: 0.519035	test: 0.564254

Early stopping
Best (ROC):	 train: 0.824230	val: 0.766995	test: 0.563942
Best (PRC):	 train: 0.688932	val: 0.573346	test: 0.526794

ROC train: 1.000000	val: 0.795765	test: 0.702418
PRC train: 1.000000	val: 0.558908	test: 0.554252

Epoch: 94
Loss: 0.023072165214489244
ROC train: 0.999912	val: 0.765547	test: 0.696609
PRC train: 0.999834	val: 0.545635	test: 0.552757

Epoch: 95
Loss: 0.021711772783941798
ROC train: 0.999869	val: 0.785490	test: 0.693636
PRC train: 0.999666	val: 0.547568	test: 0.553970

Epoch: 96
Loss: 0.03182865690703901
ROC train: 0.999972	val: 0.829767	test: 0.706179
PRC train: 0.999945	val: 0.556865	test: 0.560143

Epoch: 97
Loss: 0.024744880994957547
ROC train: 0.999962	val: 0.805942	test: 0.709302
PRC train: 0.999892	val: 0.551562	test: 0.560471

Epoch: 98
Loss: 0.021316190760422606
ROC train: 1.000000	val: 0.734954	test: 0.700834
PRC train: 1.000000	val: 0.544587	test: 0.560631

Epoch: 99
Loss: 0.028856907558760452
ROC train: 0.999994	val: 0.711978	test: 0.680297
PRC train: 1.000000	val: 0.547038	test: 0.549827

Epoch: 100
Loss: 0.024217389983689525
ROC train: 0.999994	val: 0.736640	test: 0.697210
PRC train: 1.000000	val: 0.555834	test: 0.557279

Epoch: 101
Loss: 0.01594975567765841
ROC train: 1.000000	val: 0.735417	test: 0.705854
PRC train: 1.000000	val: 0.556341	test: 0.557868

Epoch: 102
Loss: 0.019058658679117005
ROC train: 1.000000	val: 0.769244	test: 0.704442
PRC train: 1.000000	val: 0.558188	test: 0.557865

Epoch: 103
Loss: 0.021837100484461042
ROC train: 1.000000	val: 0.780756	test: 0.693412
PRC train: 1.000000	val: 0.556969	test: 0.551449

Epoch: 104
Loss: 0.014199665588408692
ROC train: 1.000000	val: 0.785862	test: 0.694854
PRC train: 1.000000	val: 0.557146	test: 0.546903

Epoch: 105
Loss: 0.010364809903670513
ROC train: 1.000000	val: 0.767157	test: 0.693199
PRC train: 1.000000	val: 0.549811	test: 0.546828

Epoch: 106
Loss: 0.013655229538309211
ROC train: 1.000000	val: 0.736190	test: 0.694555
PRC train: 1.000000	val: 0.532847	test: 0.548030

Epoch: 107
Loss: 0.018012680229196516
ROC train: 1.000000	val: 0.727363	test: 0.698533
PRC train: 1.000000	val: 0.530033	test: 0.549743

Epoch: 108
Loss: 0.012234258007779364
ROC train: 0.999994	val: 0.741922	test: 0.695123
PRC train: 1.000000	val: 0.538480	test: 0.550707

Epoch: 109
Loss: 0.012196714053266744
ROC train: 1.000000	val: 0.767533	test: 0.701107
PRC train: 1.000000	val: 0.547774	test: 0.557801

Epoch: 110
Loss: 0.014617435911089858
ROC train: 1.000000	val: 0.769943	test: 0.709563
PRC train: 1.000000	val: 0.553462	test: 0.560783

Epoch: 111
Loss: 0.011754046909801776
ROC train: 1.000000	val: 0.750563	test: 0.714361
PRC train: 1.000000	val: 0.594633	test: 0.574872

Epoch: 112
Loss: 0.016283547358063033
ROC train: 1.000000	val: 0.759542	test: 0.705742
PRC train: 1.000000	val: 0.575075	test: 0.565460

Epoch: 113
Loss: 0.015950848782763878
ROC train: 1.000000	val: 0.791260	test: 0.691776
PRC train: 1.000000	val: 0.563494	test: 0.552780

Epoch: 114
Loss: 0.013472676046988184
ROC train: 1.000000	val: 0.817594	test: 0.680122
PRC train: 1.000000	val: 0.586418	test: 0.547403

Epoch: 115
Loss: 0.014634584703812148
ROC train: 1.000000	val: 0.818905	test: 0.674351
PRC train: 1.000000	val: 0.597615	test: 0.545324

Epoch: 116
Loss: 0.010890049620027245
ROC train: 1.000000	val: 0.822314	test: 0.679759
PRC train: 1.000000	val: 0.591451	test: 0.547441

Epoch: 117
Loss: 0.009134907215847969
ROC train: 1.000000	val: 0.811311	test: 0.692975
PRC train: 1.000000	val: 0.578139	test: 0.550937

Epoch: 118
Loss: 0.011390124108621048
ROC train: 0.999994	val: 0.764858	test: 0.693311
PRC train: 1.000000	val: 0.554340	test: 0.550043

Epoch: 119
Loss: 0.013454182611504311
ROC train: 0.999989	val: 0.756017	test: 0.694110
PRC train: 0.999999	val: 0.548495	test: 0.551810

Epoch: 120
Loss: 0.009176973948434562
ROC train: 1.000000	val: 0.748026	test: 0.697196
PRC train: 1.000000	val: 0.562638	test: 0.552648

Early stopping
Best (ROC):	 train: 0.855103	val: 0.862982	test: 0.578675
Best (PRC):	 train: 0.712728	val: 0.581990	test: 0.521473

ROC train: 0.999994	val: 0.788336	test: 0.758338
PRC train: 1.000000	val: 0.599964	test: 0.558692

Epoch: 94
Loss: 0.028789942412018227
ROC train: 0.999972	val: 0.782580	test: 0.759957
PRC train: 0.999945	val: 0.584429	test: 0.567158

Epoch: 95
Loss: 0.028084239208947988
ROC train: 1.000000	val: 0.787187	test: 0.750808
PRC train: 1.000000	val: 0.591219	test: 0.582662

Epoch: 96
Loss: 0.01613316123393418
ROC train: 0.999961	val: 0.773401	test: 0.710089
PRC train: 0.999646	val: 0.593864	test: 0.573527

Epoch: 97
Loss: 0.02877663729453761
ROC train: 1.000000	val: 0.773826	test: 0.725247
PRC train: 1.000000	val: 0.593145	test: 0.556381

Epoch: 98
Loss: 0.01511222149174185
ROC train: 1.000000	val: 0.772902	test: 0.704535
PRC train: 1.000000	val: 0.577085	test: 0.544908

Epoch: 99
Loss: 0.02336601983487693
ROC train: 1.000000	val: 0.814421	test: 0.711143
PRC train: 1.000000	val: 0.573835	test: 0.545235

Epoch: 100
Loss: 0.02035431263713899
ROC train: 0.999994	val: 0.826084	test: 0.711180
PRC train: 1.000000	val: 0.576547	test: 0.544980

Epoch: 101
Loss: 0.02821449566370654
ROC train: 1.000000	val: 0.827708	test: 0.671295
PRC train: 1.000000	val: 0.586731	test: 0.542327

Epoch: 102
Loss: 0.019595901934938136
ROC train: 1.000000	val: 0.805843	test: 0.694905
PRC train: 1.000000	val: 0.584251	test: 0.551818

Epoch: 103
Loss: 0.023281659392066424
ROC train: 1.000000	val: 0.792282	test: 0.728822
PRC train: 1.000000	val: 0.581777	test: 0.558589

Epoch: 104
Loss: 0.017994583478045695
ROC train: 1.000000	val: 0.755883	test: 0.712431
PRC train: 1.000000	val: 0.588654	test: 0.555407

Epoch: 105
Loss: 0.01313880544463376
ROC train: 1.000000	val: 0.770168	test: 0.700927
PRC train: 1.000000	val: 0.646544	test: 0.556247

Epoch: 106
Loss: 0.018513052277795837
ROC train: 1.000000	val: 0.768970	test: 0.716193
PRC train: 1.000000	val: 0.647942	test: 0.565163

Epoch: 107
Loss: 0.016448662125743255
ROC train: 1.000000	val: 0.761140	test: 0.722214
PRC train: 1.000000	val: 0.593151	test: 0.575528

Epoch: 108
Loss: 0.023020622225640098
ROC train: 1.000000	val: 0.754260	test: 0.741264
PRC train: 1.000000	val: 0.643878	test: 0.564176

Epoch: 109
Loss: 0.015524612172363455
ROC train: 0.999984	val: 0.771279	test: 0.750632
PRC train: 0.999945	val: 0.605266	test: 0.560964

Epoch: 110
Loss: 0.019251016904124618
ROC train: 0.999985	val: 0.813521	test: 0.749732
PRC train: 0.999893	val: 0.588247	test: 0.579239

Epoch: 111
Loss: 0.015621110978987473
ROC train: 1.000000	val: 0.805168	test: 0.753867
PRC train: 1.000000	val: 0.596037	test: 0.604308

Epoch: 112
Loss: 0.019577549476533434
ROC train: 1.000000	val: 0.773689	test: 0.763086
PRC train: 1.000000	val: 0.590801	test: 0.581320

Epoch: 113
Loss: 0.016567305713438903
ROC train: 1.000000	val: 0.755096	test: 0.756553
PRC train: 1.000000	val: 0.602446	test: 0.571516

Epoch: 114
Loss: 0.013178444916925797
ROC train: 1.000000	val: 0.755209	test: 0.731619
PRC train: 1.000000	val: 0.645769	test: 0.556181

Epoch: 115
Loss: 0.017274596015709683
ROC train: 0.999986	val: 0.743545	test: 0.720414
PRC train: 0.999842	val: 0.686309	test: 0.552086

Epoch: 116
Loss: 0.012465196906757417
ROC train: 1.000000	val: 0.754197	test: 0.735217
PRC train: 1.000000	val: 0.645821	test: 0.559088

Epoch: 117
Loss: 0.014268409480635685
ROC train: 1.000000	val: 0.737202	test: 0.731818
PRC train: 1.000000	val: 0.578374	test: 0.555085

Epoch: 118
Loss: 0.014413234804429919
ROC train: 1.000000	val: 0.742322	test: 0.699339
PRC train: 1.000000	val: 0.566599	test: 0.548995

Epoch: 119
Loss: 0.01706468439458348
ROC train: 1.000000	val: 0.752524	test: 0.707135
PRC train: 1.000000	val: 0.556862	test: 0.550340

Epoch: 120
Loss: 0.01389798478942654
ROC train: 0.999994	val: 0.778047	test: 0.733293
PRC train: 1.000000	val: 0.553768	test: 0.559314

Early stopping
Best (ROC):	 train: 0.836837	val: 0.885033	test: 0.583740
Best (PRC):	 train: 0.658178	val: 0.585567	test: 0.527708

ROC train: 1.000000	val: 0.666713	test: 0.760377
PRC train: 1.000000	val: 0.542763	test: 0.573664

Epoch: 94
Loss: 0.017918310716914805
ROC train: 0.999994	val: 0.658248	test: 0.761007
PRC train: 1.000000	val: 0.519793	test: 0.572307

Epoch: 95
Loss: 0.01741400328156574
ROC train: 0.999989	val: 0.573112	test: 0.754704
PRC train: 0.999999	val: 0.507018	test: 0.580723

Epoch: 96
Loss: 0.011203654404657976
ROC train: 1.000000	val: 0.574922	test: 0.784612
PRC train: 1.000000	val: 0.512237	test: 0.590756

Epoch: 97
Loss: 0.011671639102528848
ROC train: 1.000000	val: 0.567043	test: 0.762876
PRC train: 1.000000	val: 0.512609	test: 0.582959

Epoch: 98
Loss: 0.012440307409451477
ROC train: 1.000000	val: 0.570853	test: 0.753228
PRC train: 1.000000	val: 0.508943	test: 0.574622

Epoch: 99
Loss: 0.010897747373962407
ROC train: 1.000000	val: 0.577582	test: 0.762533
PRC train: 1.000000	val: 0.509077	test: 0.569747

Epoch: 100
Loss: 0.023722140913908875
ROC train: 1.000000	val: 0.612020	test: 0.793749
PRC train: 1.000000	val: 0.514788	test: 0.583002

Epoch: 101
Loss: 0.01225980305208126
ROC train: 1.000000	val: 0.639880	test: 0.770732
PRC train: 1.000000	val: 0.516396	test: 0.568673

Epoch: 102
Loss: 0.015394098741043772
ROC train: 1.000000	val: 0.595353	test: 0.753688
PRC train: 1.000000	val: 0.509835	test: 0.571942

Epoch: 103
Loss: 0.013150642257888572
ROC train: 1.000000	val: 0.566221	test: 0.741798
PRC train: 1.000000	val: 0.504138	test: 0.570863

Epoch: 104
Loss: 0.019271180433089792
ROC train: 1.000000	val: 0.554421	test: 0.732292
PRC train: 1.000000	val: 0.504747	test: 0.567258

Epoch: 105
Loss: 0.009578032629787226
ROC train: 1.000000	val: 0.598649	test: 0.729151
PRC train: 1.000000	val: 0.511616	test: 0.566504

Epoch: 106
Loss: 0.00937047615639607
ROC train: 1.000000	val: 0.647122	test: 0.748708
PRC train: 1.000000	val: 0.519618	test: 0.577686

Epoch: 107
Loss: 0.009775560314606404
ROC train: 1.000000	val: 0.637669	test: 0.761550
PRC train: 1.000000	val: 0.518210	test: 0.583757

Epoch: 108
Loss: 0.007689792688888858
ROC train: 1.000000	val: 0.624196	test: 0.753542
PRC train: 1.000000	val: 0.512595	test: 0.582723

Epoch: 109
Loss: 0.007620113098575477
ROC train: 1.000000	val: 0.643239	test: 0.752380
PRC train: 1.000000	val: 0.517917	test: 0.576909

Epoch: 110
Loss: 0.008376091537621403
ROC train: 1.000000	val: 0.659759	test: 0.772861
PRC train: 1.000000	val: 0.521161	test: 0.585576

Epoch: 111
Loss: 0.011879320019058625
ROC train: 1.000000	val: 0.653515	test: 0.787675
PRC train: 1.000000	val: 0.518578	test: 0.593341

Epoch: 112
Loss: 0.00830776062021243
ROC train: 1.000000	val: 0.640678	test: 0.798825
PRC train: 1.000000	val: 0.519637	test: 0.601791

Epoch: 113
Loss: 0.007579954617688153
ROC train: 1.000000	val: 0.637631	test: 0.790368
PRC train: 1.000000	val: 0.518315	test: 0.591859

Epoch: 114
Loss: 0.010677606229474501
ROC train: 1.000000	val: 0.651280	test: 0.782905
PRC train: 1.000000	val: 0.518605	test: 0.592454

Epoch: 115
Loss: 0.00605182429206385
ROC train: 1.000000	val: 0.665564	test: 0.779102
PRC train: 1.000000	val: 0.524200	test: 0.592160

Epoch: 116
Loss: 0.007623868329254965
ROC train: 1.000000	val: 0.678464	test: 0.775479
PRC train: 1.000000	val: 0.528172	test: 0.586518

Epoch: 117
Loss: 0.005351701899954747
ROC train: 1.000000	val: 0.685957	test: 0.777478
PRC train: 1.000000	val: 0.526285	test: 0.586508

Epoch: 118
Loss: 0.006915785945025012
ROC train: 1.000000	val: 0.642477	test: 0.761450
PRC train: 1.000000	val: 0.519920	test: 0.583114

Epoch: 119
Loss: 0.021894282837444223
ROC train: 1.000000	val: 0.578355	test: 0.762566
PRC train: 1.000000	val: 0.511129	test: 0.589457

Epoch: 120
Loss: 0.012991822776163378
ROC train: 1.000000	val: 0.570888	test: 0.771505
PRC train: 1.000000	val: 0.512563	test: 0.588707

Early stopping
Best (ROC):	 train: 0.763228	val: 0.768682	test: 0.547978
Best (PRC):	 train: 0.610321	val: 0.568772	test: 0.522353

ROC train: 1.000000	val: 0.608946	test: 0.552644
PRC train: 1.000000	val: 0.523247	test: 0.517415

Epoch: 94
Loss: 0.012815968173447365
ROC train: 1.000000	val: 0.615900	test: 0.569818
PRC train: 1.000000	val: 0.525160	test: 0.520118

Epoch: 95
Loss: 0.015458569915578386
ROC train: 1.000000	val: 0.638401	test: 0.576027
PRC train: 1.000000	val: 0.528013	test: 0.521354

Epoch: 96
Loss: 0.014101148440649303
ROC train: 1.000000	val: 0.639712	test: 0.570573
PRC train: 1.000000	val: 0.528227	test: 0.520552

Epoch: 97
Loss: 0.017330944313099072
ROC train: 1.000000	val: 0.643071	test: 0.546133
PRC train: 1.000000	val: 0.531982	test: 0.517332

Epoch: 98
Loss: 0.012764097138071534
ROC train: 0.999995	val: 0.667346	test: 0.534879
PRC train: 0.999946	val: 0.537108	test: 0.516109

Epoch: 99
Loss: 0.011148404175712967
ROC train: 1.000000	val: 0.674476	test: 0.535304
PRC train: 1.000000	val: 0.538873	test: 0.516204

Epoch: 100
Loss: 0.013501914008518925
ROC train: 1.000000	val: 0.660915	test: 0.532749
PRC train: 1.000000	val: 0.536450	test: 0.515838

Epoch: 101
Loss: 0.012419086697143525
ROC train: 1.000000	val: 0.651550	test: 0.514816
PRC train: 1.000000	val: 0.534357	test: 0.513237

Epoch: 102
Loss: 0.011542725676130104
ROC train: 0.999957	val: 0.640014	test: 0.523837
PRC train: 0.999534	val: 0.531641	test: 0.512314

Epoch: 103
Loss: 0.017611624659027176
ROC train: 1.000000	val: 0.669919	test: 0.547160
PRC train: 1.000000	val: 0.532468	test: 0.517481

Epoch: 104
Loss: 0.01015997351489831
ROC train: 1.000000	val: 0.709652	test: 0.550208
PRC train: 1.000000	val: 0.539872	test: 0.521228

Epoch: 105
Loss: 0.009120355622601346
ROC train: 1.000000	val: 0.697040	test: 0.544187
PRC train: 1.000000	val: 0.541014	test: 0.518525

Epoch: 106
Loss: 0.010947722077291997
ROC train: 1.000000	val: 0.695954	test: 0.533231
PRC train: 1.000000	val: 0.538465	test: 0.514486

Epoch: 107
Loss: 0.01124340303913
ROC train: 1.000000	val: 0.702673	test: 0.524050
PRC train: 1.000000	val: 0.539393	test: 0.510989

Epoch: 108
Loss: 0.008539818851335071
ROC train: 1.000000	val: 0.727535	test: 0.533956
PRC train: 1.000000	val: 0.541605	test: 0.510709

Epoch: 109
Loss: 0.006434739266381359
ROC train: 1.000000	val: 0.722978	test: 0.537041
PRC train: 1.000000	val: 0.541900	test: 0.508917

Epoch: 110
Loss: 0.011894143850144872
ROC train: 1.000000	val: 0.734152	test: 0.552868
PRC train: 1.000000	val: 0.545127	test: 0.516206

Epoch: 111
Loss: 0.009047439862280376
ROC train: 1.000000	val: 0.712024	test: 0.559027
PRC train: 1.000000	val: 0.541275	test: 0.519126

Epoch: 112
Loss: 0.011296237468805932
ROC train: 1.000000	val: 0.655732	test: 0.561126
PRC train: 1.000000	val: 0.534014	test: 0.519728

Epoch: 113
Loss: 0.012983164983247425
ROC train: 1.000000	val: 0.658255	test: 0.570689
PRC train: 1.000000	val: 0.538645	test: 0.521327

Epoch: 114
Loss: 0.014018389824645802
ROC train: 1.000000	val: 0.728808	test: 0.578178
PRC train: 1.000000	val: 0.547796	test: 0.523499

Epoch: 115
Loss: 0.009755395147144245
ROC train: 1.000000	val: 0.771950	test: 0.566823
PRC train: 1.000000	val: 0.550798	test: 0.523078

Epoch: 116
Loss: 0.01299518472206244
ROC train: 1.000000	val: 0.772312	test: 0.549622
PRC train: 1.000000	val: 0.549827	test: 0.518891

Epoch: 117
Loss: 0.0194090087511365
ROC train: 1.000000	val: 0.750634	test: 0.539004
PRC train: 1.000000	val: 0.548325	test: 0.514623

Epoch: 118
Loss: 0.00997152000502519
ROC train: 1.000000	val: 0.741181	test: 0.546424
PRC train: 1.000000	val: 0.547749	test: 0.515564

Epoch: 119
Loss: 0.017826288072679088
ROC train: 1.000000	val: 0.754917	test: 0.542252
PRC train: 1.000000	val: 0.546896	test: 0.511764

Epoch: 120
Loss: 0.012638173724798554
ROC train: 1.000000	val: 0.757553	test: 0.539790
PRC train: 1.000000	val: 0.544108	test: 0.510676

Early stopping
Best (ROC):	 train: 0.797682	val: 0.866939	test: 0.524364
Best (PRC):	 train: 0.645146	val: 0.576722	test: 0.515156
All runs completed.
All runs completed.

ROC train: 1.000000	val: 0.741922	test: 0.627536
PRC train: 1.000000	val: 0.553153	test: 0.549645

Epoch: 94
Loss: 0.017562016720907073
ROC train: 1.000000	val: 0.702800	test: 0.638010
PRC train: 1.000000	val: 0.553400	test: 0.550399

Epoch: 95
Loss: 0.01696196266641118
ROC train: 0.999995	val: 0.693073	test: 0.636829
PRC train: 0.999946	val: 0.560998	test: 0.542994

Epoch: 96
Loss: 0.01128053501167181
ROC train: 0.999962	val: 0.698056	test: 0.621963
PRC train: 0.999592	val: 0.570020	test: 0.535764

Epoch: 97
Loss: 0.01631993211196691
ROC train: 0.999971	val: 0.701527	test: 0.608785
PRC train: 0.999694	val: 0.566291	test: 0.535516

Epoch: 98
Loss: 0.017018956715287865
ROC train: 1.000000	val: 0.735853	test: 0.618216
PRC train: 1.000000	val: 0.559860	test: 0.557728

Epoch: 99
Loss: 0.014246463675122145
ROC train: 1.000000	val: 0.788610	test: 0.598890
PRC train: 1.000000	val: 0.562993	test: 0.529367

Epoch: 100
Loss: 0.019377356914491387
ROC train: 1.000000	val: 0.815345	test: 0.598277
PRC train: 1.000000	val: 0.582636	test: 0.528493

Epoch: 101
Loss: 0.014768070314662702
ROC train: 1.000000	val: 0.799325	test: 0.604724
PRC train: 1.000000	val: 0.582756	test: 0.533847

Epoch: 102
Loss: 0.024575833721647893
ROC train: 1.000000	val: 0.779783	test: 0.611896
PRC train: 1.000000	val: 0.579779	test: 0.540424

Epoch: 103
Loss: 0.016281092429244887
ROC train: 1.000000	val: 0.729138	test: 0.645361
PRC train: 1.000000	val: 0.584776	test: 0.558019

Epoch: 104
Loss: 0.028465456455867795
ROC train: 1.000000	val: 0.718711	test: 0.651998
PRC train: 1.000000	val: 0.573942	test: 0.573468

Epoch: 105
Loss: 0.014708128191856473
ROC train: 1.000000	val: 0.745345	test: 0.657739
PRC train: 1.000000	val: 0.571304	test: 0.583345

Epoch: 106
Loss: 0.01680431472975304
ROC train: 1.000000	val: 0.771753	test: 0.651942
PRC train: 1.000000	val: 0.560714	test: 0.569484

Epoch: 107
Loss: 0.02227200825699237
ROC train: 1.000000	val: 0.789148	test: 0.643224
PRC train: 1.000000	val: 0.554389	test: 0.560855

Epoch: 108
Loss: 0.020822254718745176
ROC train: 0.999976	val: 0.765909	test: 0.634767
PRC train: 0.999726	val: 0.552534	test: 0.552796

Epoch: 109
Loss: 0.0199957459338772
ROC train: 1.000000	val: 0.761689	test: 0.622575
PRC train: 1.000000	val: 0.551526	test: 0.550328

Epoch: 110
Loss: 0.01776967818151092
ROC train: 1.000000	val: 0.764261	test: 0.618041
PRC train: 1.000000	val: 0.552454	test: 0.549710

Epoch: 111
Loss: 0.01187177685075206
ROC train: 1.000000	val: 0.770017	test: 0.611246
PRC train: 1.000000	val: 0.557813	test: 0.546258

Epoch: 112
Loss: 0.01907048989795826
ROC train: 1.000000	val: 0.786600	test: 0.632982
PRC train: 1.000000	val: 0.552236	test: 0.557024

Epoch: 113
Loss: 0.01675551964735867
ROC train: 1.000000	val: 0.825410	test: 0.650669
PRC train: 1.000000	val: 0.557937	test: 0.557361

Epoch: 114
Loss: 0.017324232517180436
ROC train: 1.000000	val: 0.816157	test: 0.636516
PRC train: 1.000000	val: 0.555437	test: 0.562299

Epoch: 115
Loss: 0.016180921076921033
ROC train: 1.000000	val: 0.811448	test: 0.621765
PRC train: 1.000000	val: 0.564381	test: 0.566101

Epoch: 116
Loss: 0.016317381940725424
ROC train: 1.000000	val: 0.844526	test: 0.607175
PRC train: 1.000000	val: 0.572874	test: 0.557607

Epoch: 117
Loss: 0.011289096105703238
ROC train: 1.000000	val: 0.851020	test: 0.611246
PRC train: 1.000000	val: 0.575286	test: 0.556042

Epoch: 118
Loss: 0.014298540048140865
ROC train: 1.000000	val: 0.838384	test: 0.618941
PRC train: 1.000000	val: 0.570306	test: 0.561476

Epoch: 119
Loss: 0.01655059949520816
ROC train: 1.000000	val: 0.828231	test: 0.620528
PRC train: 1.000000	val: 0.569688	test: 0.558851

Epoch: 120
Loss: 0.011123817229891923
ROC train: 1.000000	val: 0.808602	test: 0.623995
PRC train: 1.000000	val: 0.560935	test: 0.582392

Epoch: 121
Loss: 0.007093814903842201
ROC train: 1.000000	val: 0.788434	test: 0.623539
PRC train: 1.000000	val: 0.559375	test: 0.594553

Epoch: 122
Loss: 0.010642250930827168
ROC train: 1.000000	val: 0.800934	test: 0.609961
PRC train: 1.000000	val: 0.562379	test: 0.574116

Epoch: 123
Loss: 0.008891019563879322
ROC train: 1.000000	val: 0.829068	test: 0.599943
PRC train: 1.000000	val: 0.573415	test: 0.554861

Epoch: 124
Loss: 0.01671311978055437
ROC train: 1.000000	val: 0.819703	test: 0.612210
PRC train: 1.000000	val: 0.571263	test: 0.574886

Epoch: 125
Loss: 0.008957737935236083
ROC train: 1.000000	val: 0.807878	test: 0.617092
PRC train: 1.000000	val: 0.587676	test: 0.562444

Epoch: 126
Loss: 0.008622854813994971
ROC train: 1.000000	val: 0.800498	test: 0.613032
PRC train: 1.000000	val: 0.607191	test: 0.550385

Epoch: 127
Loss: 0.006646915948912052
ROC train: 1.000000	val: 0.782267	test: 0.610458
PRC train: 1.000000	val: 0.579821	test: 0.554088

Epoch: 128
Loss: 0.011405344226946841
ROC train: 1.000000	val: 0.725726	test: 0.628615
PRC train: 1.000000	val: 0.546929	test: 0.608410

Epoch: 129
Loss: 0.00892816962315128
ROC train: 1.000000	val: 0.730259	test: 0.625511
PRC train: 1.000000	val: 0.537504	test: 0.599918

Epoch: 130
Loss: 0.012806854105627107
ROC train: 1.000000	val: 0.801110	test: 0.628858
PRC train: 1.000000	val: 0.556022	test: 0.562392

Epoch: 131
Loss: 0.021772988521562217
ROC train: 0.999971	val: 0.807066	test: 0.628970
PRC train: 0.999694	val: 0.561011	test: 0.554830

Epoch: 132
Loss: 0.010602244796503745
ROC train: 1.000000	val: 0.759053	test: 0.646298
PRC train: 1.000000	val: 0.559084	test: 0.576199

Epoch: 133
Loss: 0.019309110297145042
ROC train: 1.000000	val: 0.765733	test: 0.647998
PRC train: 1.000000	val: 0.559988	test: 0.571106

Epoch: 134
Loss: 0.013776825175831417
ROC train: 1.000000	val: 0.806929	test: 0.614320
PRC train: 1.000000	val: 0.563236	test: 0.542214

Epoch: 135
Loss: 0.01279778753251316
ROC train: 1.000000	val: 0.806866	test: 0.609293
PRC train: 1.000000	val: 0.557794	test: 0.542416

Epoch: 136
Loss: 0.011284281209740918
ROC train: 1.000000	val: 0.752784	test: 0.611997
PRC train: 1.000000	val: 0.551505	test: 0.555973

Epoch: 137
Loss: 0.022359550565147903
ROC train: 1.000000	val: 0.718870	test: 0.598206
PRC train: 1.000000	val: 0.557831	test: 0.551980

Epoch: 138
Loss: 0.005310857152694789
ROC train: 1.000000	val: 0.698091	test: 0.601295
PRC train: 1.000000	val: 0.544886	test: 0.560449

Epoch: 139
Loss: 0.009126590172819807
ROC train: 1.000000	val: 0.705896	test: 0.595506
PRC train: 1.000000	val: 0.546042	test: 0.539927

Epoch: 140
Loss: 0.008511735087703141
ROC train: 1.000000	val: 0.713352	test: 0.582925
PRC train: 1.000000	val: 0.542491	test: 0.530837

Epoch: 141
Loss: 0.009148424287438514
ROC train: 1.000000	val: 0.712477	test: 0.598666
PRC train: 1.000000	val: 0.539154	test: 0.549820

Epoch: 142
Loss: 0.007141159422679555
ROC train: 1.000000	val: 0.741497	test: 0.598931
PRC train: 1.000000	val: 0.550169	test: 0.556711

Epoch: 143
Loss: 0.006544368087827157
ROC train: 1.000000	val: 0.763699	test: 0.603671
PRC train: 1.000000	val: 0.552997	test: 0.557032

Epoch: 144
Loss: 0.007926155776326394
ROC train: 1.000000	val: 0.760178	test: 0.609218
PRC train: 1.000000	val: 0.551366	test: 0.553088

Epoch: 145
Loss: 0.007334951176128907
ROC train: 1.000000	val: 0.775973	test: 0.618687
PRC train: 1.000000	val: 0.559051	test: 0.560229

Epoch: 146
Loss: 0.00799378597367147
ROC train: 1.000000	val: 0.781680	test: 0.621029
PRC train: 1.000000	val: 0.568480	test: 0.557834

Epoch: 147
Loss: 0.008623852765316364
ROC train: 1.000000	val: 0.776648	test: 0.620935
PRC train: 1.000000	val: 0.568306	test: 0.553945

Epoch: 148
Loss: 0.01336146440328928
ROC train: 1.000000	val: 0.785989	test: 0.604145
PRC train: 1.000000	val: 0.575184	test: 0.552229

Epoch: 149
Loss: 0.0082789372234378
ROC train: 1.000000	val: 0.783915	test: 0.595696
PRC train: 1.000000	val: 0.569151	test: 0.554841

Epoch: 150
Loss: 0.01090821648100532
ROC train: 1.000000	val: 0.765161	test: 0.597650
PRC train: 1.000000	val: 0.570940	test: 0.538335

Epoch: 151
Loss: 0.015025471333035847
ROC train: 1.000000	val: 0.765884	test: 0.590123
PRC train: 1.000000	val: 0.559320	test: 0.532247

Epoch: 152
Loss: 0.008688743852449058
ROC train: 1.000000	val: 0.728973	test: 0.581506
PRC train: 1.000000	val: 0.560567	test: 0.544936

Early stopping
Best (ROC):	 train: 1.000000	val: 0.851020	test: 0.611246
Best (PRC):	 train: 1.000000	val: 0.575286	test: 0.556042
All runs completed.
