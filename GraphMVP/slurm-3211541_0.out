>>> Starting run for dataset: esol
FATAL:   Image file already exists: "graphmvp.sif" - will not overwrite
Running SCAFF configs_split_experiments/GraphMVP/esol/scaff/train_prop=0.6.yml on cuda:0
Running SCAFF configs_split_experiments/GraphMVP/esol/scaff/train_prop=0.7.yml on cuda:1
Running SCAFF configs_split_experiments/GraphMVP/esol/scaff/train_prop=0.8.yml on cuda:2
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/esol/scaff/train_prop=0.6.yml --runseed 4 --device cuda:0
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/esol/scaff/train_prop=0.6.yml --runseed 5 --device cuda:0
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/esol/scaff/train_prop=0.6.yml --runseed 6 --device cuda:0
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/esol/scaff/train_prop=0.7.yml --runseed 4 --device cuda:1
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/esol/scaff/train_prop=0.7.yml --runseed 5 --device cuda:1
Starting process for seed 4: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/esol/scaff/train_prop=0.8.yml --runseed 4 --device cuda:2
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/esol/scaff/train_prop=0.7.yml --runseed 6 --device cuda:1
Starting process for seed 5: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/esol/scaff/train_prop=0.8.yml --runseed 5 --device cuda:2
Starting process for seed 6: python molecule_finetune_regression.py --config /workspace/configs_split_experiments/GraphMVP/esol/scaff/train_prop=0.8.yml --runseed 6 --device cuda:2
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.
Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/esol/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/esol/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/esol/scaff/train_prop=0.6/esol_scaff_5_26-05_11-09-14  ]
[ Using Seed :  5  ]
[ Using device :  cuda:0  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 11.73400592803955
RMSE train: 3.388216	val: 3.924506	test: 4.322428
MAE train: 2.859152	val: 3.504460	test: 3.961557

Epoch: 2
Loss: 10.527972539265951
RMSE train: 3.316665	val: 4.005572	test: 4.383638
MAE train: 2.827438	val: 3.632368	test: 4.087808

Epoch: 3
Loss: 9.819085121154785
RMSE train: 3.230743	val: 4.149330	test: 4.548866
MAE train: 2.773799	val: 3.791338	test: 4.283512

Epoch: 4
Loss: 8.713401794433594
RMSE train: 3.094274	val: 4.221538	test: 4.629329
MAE train: 2.658756	val: 3.861301	test: 4.368134

Epoch: 5
Loss: 8.269421577453613
RMSE train: 2.911560	val: 4.155390	test: 4.502450
MAE train: 2.499464	val: 3.796706	test: 4.238987

Epoch: 6
Loss: 7.46702766418457
RMSE train: 2.681780	val: 3.955544	test: 4.241993
MAE train: 2.279473	val: 3.593870	test: 3.963487

Epoch: 7
Loss: 7.073835213979085
RMSE train: 2.553591	val: 3.727029	test: 4.007177
MAE train: 2.145916	val: 3.356084	test: 3.724247

Epoch: 8
Loss: 6.684888998667399
RMSE train: 2.444456	val: 3.481416	test: 3.748966
MAE train: 2.034432	val: 3.098640	test: 3.457567

Epoch: 9
Loss: 6.230504194895427
RMSE train: 2.383230	val: 3.350581	test: 3.606031
MAE train: 2.002426	val: 2.966689	test: 3.317597

Epoch: 10
Loss: 5.519042332967122
RMSE train: 2.368986	val: 3.433775	test: 3.684772
MAE train: 2.019229	val: 3.060376	test: 3.413028

Epoch: 11
Loss: 5.085006395975749
RMSE train: 2.404403	val: 3.576657	test: 3.852051
MAE train: 2.039163	val: 3.201715	test: 3.584180

Epoch: 12
Loss: 4.809381326039632
RMSE train: 2.379492	val: 3.544842	test: 3.806953
MAE train: 2.017125	val: 3.179140	test: 3.538391

Epoch: 13
Loss: 4.417659282684326
RMSE train: 2.280057	val: 3.362500	test: 3.587671
MAE train: 1.944583	val: 3.003185	test: 3.313350

Epoch: 14
Loss: 4.126743237177531
RMSE train: 2.153090	val: 3.200234	test: 3.384983
MAE train: 1.825294	val: 2.843322	test: 3.103062

Epoch: 15
Loss: 3.6469476222991943
RMSE train: 2.019902	val: 3.107397	test: 3.263169
MAE train: 1.695023	val: 2.752840	test: 2.971600

Epoch: 16
Loss: 3.198197523752848
RMSE train: 1.922200	val: 3.007871	test: 3.140211
MAE train: 1.609518	val: 2.656976	test: 2.835542

Epoch: 17
Loss: 2.9207372665405273
RMSE train: 1.816555	val: 2.800431	test: 2.902001
MAE train: 1.535301	val: 2.450573	test: 2.580309

Epoch: 18
Loss: 2.6017319361368814
RMSE train: 1.694260	val: 2.611258	test: 2.711437
MAE train: 1.423839	val: 2.259497	test: 2.392542

Epoch: 19
Loss: 2.211327234903971
RMSE train: 1.636108	val: 2.549097	test: 2.680566
MAE train: 1.334221	val: 2.188723	test: 2.359349

Epoch: 20
Loss: 1.9877151648203533
RMSE train: 1.535648	val: 2.466340	test: 2.591368
MAE train: 1.231702	val: 2.121723	test: 2.265719

Epoch: 21
Loss: 1.7978320121765137
RMSE train: 1.426371	val: 2.353924	test: 2.491855
MAE train: 1.152414	val: 2.032892	test: 2.140187

Epoch: 22
Loss: 1.5770095984141033
RMSE train: 1.349879	val: 2.216120	test: 2.398255
MAE train: 1.106811	val: 1.914999	test: 2.004770

Epoch: 23
Loss: 1.309446930885315Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/esol/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/esol/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/esol/scaff/train_prop=0.6/esol_scaff_4_26-05_11-09-14  ]
[ Using Seed :  4  ]
[ Using device :  cuda:0  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 11.269967714945475
RMSE train: 3.500190	val: 4.019687	test: 4.447308
MAE train: 2.932165	val: 3.539066	test: 4.039599

Epoch: 2
Loss: 9.922117869059244
RMSE train: 3.393391	val: 4.084819	test: 4.497795
MAE train: 2.905929	val: 3.672741	test: 4.169967

Epoch: 3
Loss: 9.03685728708903
RMSE train: 3.257462	val: 4.195184	test: 4.600527
MAE train: 2.836809	val: 3.829578	test: 4.322944

Epoch: 4
Loss: 8.15794579188029
RMSE train: 3.098854	val: 4.275421	test: 4.679762
MAE train: 2.690441	val: 3.937144	test: 4.424892

Epoch: 5
Loss: 7.506622314453125
RMSE train: 2.899933	val: 4.202254	test: 4.606479
MAE train: 2.479773	val: 3.880976	test: 4.357762

Epoch: 6
Loss: 7.181263287862142
RMSE train: 2.670083	val: 3.953035	test: 4.340567
MAE train: 2.251818	val: 3.628852	test: 4.083048

Epoch: 7
Loss: 6.507969697316487
RMSE train: 2.447936	val: 3.548891	test: 3.886642
MAE train: 2.042985	val: 3.205321	test: 3.602128

Epoch: 8
Loss: 6.380821228027344
RMSE train: 2.332508	val: 3.300049	test: 3.603596
MAE train: 1.916876	val: 2.929229	test: 3.301975

Epoch: 9
Loss: 5.413331588109334
RMSE train: 2.257425	val: 3.221477	test: 3.502093
MAE train: 1.858676	val: 2.836391	test: 3.201131

Epoch: 10
Loss: 5.17642068862915
RMSE train: 2.231745	val: 3.236566	test: 3.508024
MAE train: 1.866198	val: 2.844823	test: 3.207401

Epoch: 11
Loss: 4.783761342366536
RMSE train: 2.215425	val: 3.265140	test: 3.532963
MAE train: 1.867158	val: 2.875002	test: 3.232538

Epoch: 12
Loss: 4.2893892129262285
RMSE train: 2.160336	val: 3.187039	test: 3.412491
MAE train: 1.828928	val: 2.814975	test: 3.121487

Epoch: 13
Loss: 3.9527103900909424
RMSE train: 2.124048	val: 3.070743	test: 3.259835
MAE train: 1.802499	val: 2.712046	test: 2.967533

Epoch: 14
Loss: 3.4011964003245034
RMSE train: 2.025481	val: 2.978214	test: 3.149943
MAE train: 1.720174	val: 2.621937	test: 2.858102

Epoch: 15
Loss: 3.200610876083374
RMSE train: 1.893646	val: 2.811781	test: 2.953708
MAE train: 1.597315	val: 2.467817	test: 2.656899

Epoch: 16
Loss: 2.942014137903849
RMSE train: 1.765392	val: 2.632051	test: 2.740285
MAE train: 1.483886	val: 2.307728	test: 2.433456

Epoch: 17
Loss: 2.598962942759196
RMSE train: 1.677836	val: 2.564952	test: 2.664031
MAE train: 1.398478	val: 2.245813	test: 2.347857

Epoch: 18
Loss: 2.3029867808024087
RMSE train: 1.583542	val: 2.473171	test: 2.564423
MAE train: 1.318660	val: 2.149127	test: 2.235379

Epoch: 19
Loss: 2.0656211773554483
RMSE train: 1.509159	val: 2.316131	test: 2.394862
MAE train: 1.261627	val: 1.990605	test: 2.059984

Epoch: 20
Loss: 1.7849542299906414
RMSE train: 1.488427	val: 2.301556	test: 2.390366
MAE train: 1.214561	val: 1.987223	test: 2.052521

Epoch: 21
Loss: 1.5632591644922893
RMSE train: 1.354115	val: 2.139007	test: 2.201595
MAE train: 1.100698	val: 1.851494	test: 1.877555

Epoch: 22
Loss: 1.2344120343526204
RMSE train: 1.173427	val: 2.047640	test: 2.130149
MAE train: 0.940405	val: 1.772089	test: 1.807300

Epoch: 23
Loss: 1.1156831979751587Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/esol/scaff/train_prop=0.6.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:0
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.6
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/esol/scaff/train_prop=0.6
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/esol/scaff/train_prop=0.6/esol_scaff_6_26-05_11-09-14  ]
[ Using Seed :  6  ]
[ Using device :  cuda:0  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 11.526843706766764
RMSE train: 3.264050	val: 3.881581	test: 4.322713
MAE train: 2.686505	val: 3.430292	test: 3.905079

Epoch: 2
Loss: 10.281072298685709
RMSE train: 3.114914	val: 3.922873	test: 4.336962
MAE train: 2.552092	val: 3.514477	test: 3.988841

Epoch: 3
Loss: 9.298592249552408
RMSE train: 2.982174	val: 4.003219	test: 4.414677
MAE train: 2.433364	val: 3.604440	test: 4.113788

Epoch: 4
Loss: 8.460473696390787
RMSE train: 2.775125	val: 3.991479	test: 4.426815
MAE train: 2.236944	val: 3.572468	test: 4.140468

Epoch: 5
Loss: 7.897946993509929
RMSE train: 2.486063	val: 3.730203	test: 4.126827
MAE train: 1.978667	val: 3.317647	test: 3.839729

Epoch: 6
Loss: 7.45781135559082
RMSE train: 2.249845	val: 3.411173	test: 3.703223
MAE train: 1.791937	val: 3.022672	test: 3.405262

Epoch: 7
Loss: 6.800925095876058
RMSE train: 2.181001	val: 3.233950	test: 3.493068
MAE train: 1.766528	val: 2.851968	test: 3.179932

Epoch: 8
Loss: 6.392380714416504
RMSE train: 2.255973	val: 3.275016	test: 3.565839
MAE train: 1.841660	val: 2.879250	test: 3.254094

Epoch: 9
Loss: 5.807175159454346
RMSE train: 2.334248	val: 3.496008	test: 3.787491
MAE train: 1.906302	val: 3.089184	test: 3.508142

Epoch: 10
Loss: 5.221776644388835
RMSE train: 2.315677	val: 3.513319	test: 3.775101
MAE train: 1.902391	val: 3.103736	test: 3.492431

Epoch: 11
Loss: 4.946794509887695
RMSE train: 2.199837	val: 3.326312	test: 3.541989
MAE train: 1.818746	val: 2.938451	test: 3.244086

Epoch: 12
Loss: 4.569766839345296
RMSE train: 2.062490	val: 3.097349	test: 3.286914
MAE train: 1.713751	val: 2.730219	test: 2.989505

Epoch: 13
Loss: 4.124214172363281
RMSE train: 1.986534	val: 2.982190	test: 3.162576
MAE train: 1.641545	val: 2.620676	test: 2.856814

Epoch: 14
Loss: 3.609823226928711
RMSE train: 1.977959	val: 2.921790	test: 3.114720
MAE train: 1.633105	val: 2.564221	test: 2.812334

Epoch: 15
Loss: 3.2921254634857178
RMSE train: 1.896012	val: 2.889462	test: 3.062133
MAE train: 1.559433	val: 2.544192	test: 2.755771

Epoch: 16
Loss: 3.0006630420684814
RMSE train: 1.741040	val: 2.769769	test: 2.931754
MAE train: 1.434385	val: 2.425704	test: 2.620810

Epoch: 17
Loss: 2.626051346460978
RMSE train: 1.658204	val: 2.667645	test: 2.842779
MAE train: 1.377579	val: 2.325878	test: 2.523511

Epoch: 18
Loss: 2.324365218480428
RMSE train: 1.559060	val: 2.510261	test: 2.677567
MAE train: 1.294971	val: 2.181946	test: 2.347835

Epoch: 19
Loss: 2.1529127756754556
RMSE train: 1.509519	val: 2.382495	test: 2.527050
MAE train: 1.251899	val: 2.068726	test: 2.194986

Epoch: 20
Loss: 1.8134648005167644
RMSE train: 1.386048	val: 2.282776	test: 2.390343
MAE train: 1.112531	val: 1.981841	test: 2.073239

Epoch: 21
Loss: 1.5760046641031902
RMSE train: 1.422899	val: 2.261646	test: 2.365992
MAE train: 1.156344	val: 1.973880	test: 2.041183

Epoch: 22
Loss: 1.378094752629598
RMSE train: 1.160214	val: 2.160352	test: 2.244988
MAE train: 0.944626	val: 1.885320	test: 1.933474

Epoch: 23
Loss: 1.2308452129364014Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/esol/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/esol/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/esol/scaff/train_prop=0.7/esol_scaff_4_26-05_11-09-14  ]
[ Using Seed :  4  ]
[ Using device :  cuda:1  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 10.888020873069763
RMSE train: 3.462541	val: 3.815427	test: 4.403629
MAE train: 2.908217	val: 3.406178	test: 4.005807

Epoch: 2
Loss: 9.295269250869751
RMSE train: 3.154537	val: 3.656026	test: 4.147697
MAE train: 2.686400	val: 3.317392	test: 3.845048

Epoch: 3
Loss: 8.834736347198486
RMSE train: 3.017017	val: 3.661968	test: 4.087787
MAE train: 2.584914	val: 3.339366	test: 3.815997

Epoch: 4
Loss: 7.832800030708313
RMSE train: 2.813172	val: 3.616452	test: 4.036891
MAE train: 2.359552	val: 3.267241	test: 3.767943

Epoch: 5
Loss: 8.728077292442322
RMSE train: 2.632220	val: 3.486722	test: 3.902739
MAE train: 2.166556	val: 3.106969	test: 3.642740

Epoch: 6
Loss: 7.0582191944122314
RMSE train: 2.413028	val: 3.192859	test: 3.562269
MAE train: 1.971984	val: 2.813707	test: 3.298010

Epoch: 7
Loss: 6.321020007133484
RMSE train: 2.335779	val: 3.047847	test: 3.416447
MAE train: 1.907797	val: 2.660803	test: 3.148782

Epoch: 8
Loss: 5.74752676486969
RMSE train: 2.253346	val: 2.937938	test: 3.304079
MAE train: 1.844682	val: 2.555789	test: 3.032111

Epoch: 9
Loss: 4.757561087608337
RMSE train: 2.198980	val: 2.944320	test: 3.298816
MAE train: 1.795433	val: 2.578716	test: 3.037108

Epoch: 10
Loss: 4.353184223175049
RMSE train: 2.146334	val: 2.936344	test: 3.293287
MAE train: 1.750057	val: 2.584637	test: 3.040509

Epoch: 11
Loss: 3.298911988735199
RMSE train: 2.105556	val: 2.885763	test: 3.193258
MAE train: 1.750751	val: 2.547783	test: 2.937564

Epoch: 12
Loss: 3.3052935004234314
RMSE train: 2.023163	val: 2.746360	test: 2.998272
MAE train: 1.660065	val: 2.405868	test: 2.727254

Epoch: 13
Loss: 3.1646448373794556
RMSE train: 1.687789	val: 2.405156	test: 2.564144
MAE train: 1.357885	val: 2.093878	test: 2.274698

Epoch: 14
Loss: 2.3366895616054535
RMSE train: 1.467111	val: 2.036691	test: 2.196316
MAE train: 1.183552	val: 1.741764	test: 1.866959

Epoch: 15
Loss: 2.218555450439453
RMSE train: 1.468143	val: 2.059545	test: 2.171749
MAE train: 1.202262	val: 1.772295	test: 1.862506

Epoch: 16
Loss: 2.286029100418091
RMSE train: 1.530463	val: 2.204627	test: 2.349140
MAE train: 1.243231	val: 1.923424	test: 2.063656

Epoch: 17
Loss: 1.8545623123645782
RMSE train: 1.465925	val: 2.134360	test: 2.283956
MAE train: 1.207278	val: 1.851092	test: 1.995187

Epoch: 18
Loss: 1.505934476852417
RMSE train: 1.430794	val: 2.057604	test: 2.216126
MAE train: 1.183655	val: 1.779114	test: 1.929021

Epoch: 19
Loss: 1.1464342027902603
RMSE train: 1.386404	val: 1.981264	test: 2.157353
MAE train: 1.127431	val: 1.695536	test: 1.868523

Epoch: 20
Loss: 1.1490903794765472
RMSE train: 1.284457	val: 1.894002	test: 2.053630
MAE train: 1.010260	val: 1.604158	test: 1.761437

Epoch: 21
Loss: 0.8327108919620514
RMSE train: 1.172507	val: 1.916544	test: 2.067194
MAE train: 0.901404	val: 1.625982	test: 1.768694

Epoch: 22
Loss: 1.17158842086792
RMSE train: 0.973443	val: 1.605672	test: 1.699215
MAE train: 0.769496	val: 1.334841	test: 1.388639

Epoch: 23
Loss: 0.7961781471967697Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/esol/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/esol/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/esol/scaff/train_prop=0.7/esol_scaff_6_26-05_11-09-14  ]
[ Using Seed :  6  ]
[ Using device :  cuda:1  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 11.646475076675415
RMSE train: 3.279125	val: 3.745861	test: 4.348518
MAE train: 2.718151	val: 3.327055	test: 3.942989

Epoch: 2
Loss: 9.703110098838806
RMSE train: 3.127633	val: 3.745446	test: 4.333399
MAE train: 2.563124	val: 3.366025	test: 4.011247

Epoch: 3
Loss: 9.215723752975464
RMSE train: 2.947354	val: 3.726055	test: 4.299676
MAE train: 2.375687	val: 3.348874	test: 4.015503

Epoch: 4
Loss: 8.208719253540039
RMSE train: 2.610782	val: 3.393114	test: 3.881118
MAE train: 2.086615	val: 3.007554	test: 3.596209

Epoch: 5
Loss: 7.728313684463501
RMSE train: 2.371323	val: 3.237276	test: 3.656095
MAE train: 1.894714	val: 2.852833	test: 3.376069

Epoch: 6
Loss: 8.747894525527954
RMSE train: 2.371972	val: 3.319182	test: 3.719681
MAE train: 1.912795	val: 2.944406	test: 3.453341

Epoch: 7
Loss: 6.347273826599121
RMSE train: 2.174033	val: 3.111438	test: 3.471857
MAE train: 1.714475	val: 2.769233	test: 3.201318

Epoch: 8
Loss: 5.590328216552734
RMSE train: 2.131636	val: 3.009466	test: 3.349249
MAE train: 1.681918	val: 2.679324	test: 3.073250

Epoch: 9
Loss: 4.774737358093262
RMSE train: 2.096812	val: 2.903202	test: 3.197006
MAE train: 1.672098	val: 2.563096	test: 2.922194

Epoch: 10
Loss: 5.063458442687988
RMSE train: 2.156696	val: 2.985015	test: 3.264957
MAE train: 1.766264	val: 2.626174	test: 2.994505

Epoch: 11
Loss: 5.047403573989868
RMSE train: 2.098000	val: 3.029351	test: 3.287421
MAE train: 1.686047	val: 2.655844	test: 3.020130

Epoch: 12
Loss: 3.2110812962055206
RMSE train: 1.820546	val: 2.633610	test: 2.796623
MAE train: 1.424687	val: 2.259289	test: 2.503328

Epoch: 13
Loss: 3.25988507270813
RMSE train: 1.708755	val: 2.382536	test: 2.571276
MAE train: 1.360681	val: 2.041691	test: 2.276971

Epoch: 14
Loss: 2.8200610876083374
RMSE train: 1.649620	val: 2.494947	test: 2.734071
MAE train: 1.318575	val: 2.190174	test: 2.454431

Epoch: 15
Loss: 2.345861494541168
RMSE train: 1.457028	val: 2.296103	test: 2.508017
MAE train: 1.147620	val: 2.008964	test: 2.222927

Epoch: 16
Loss: 2.280729830265045
RMSE train: 1.288280	val: 1.909149	test: 2.046985
MAE train: 0.998611	val: 1.610220	test: 1.740138

Epoch: 17
Loss: 1.7561680972576141
RMSE train: 1.219921	val: 1.760384	test: 1.880875
MAE train: 0.954380	val: 1.481208	test: 1.588401

Epoch: 18
Loss: 1.3772504478693008
RMSE train: 1.135473	val: 1.734718	test: 1.912068
MAE train: 0.868416	val: 1.460930	test: 1.583063

Epoch: 19
Loss: 1.5868585109710693
RMSE train: 1.001787	val: 1.672325	test: 1.806165
MAE train: 0.726414	val: 1.385500	test: 1.490054

Epoch: 20
Loss: 1.4451203048229218
RMSE train: 0.912821	val: 1.494563	test: 1.634521
MAE train: 0.664741	val: 1.250835	test: 1.348048

Epoch: 21
Loss: 1.145340919494629
RMSE train: 0.799613	val: 1.309643	test: 1.461148
MAE train: 0.579008	val: 1.069251	test: 1.166062

Epoch: 22
Loss: 0.8547172099351883
RMSE train: 0.689630	val: 1.118403	test: 1.285411
MAE train: 0.538250	val: 0.893424	test: 1.006663

Epoch: 23
Loss: 1.0896932482719421Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/esol/scaff/train_prop=0.7.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:1
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.7
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/esol/scaff/train_prop=0.7
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/esol/scaff/train_prop=0.7/esol_scaff_5_26-05_11-09-14  ]
[ Using Seed :  5  ]
[ Using device :  cuda:1  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 12.723279476165771
RMSE train: 3.382740	val: 3.787171	test: 4.325805
MAE train: 2.871743	val: 3.424019	test: 3.986876

Epoch: 2
Loss: 10.097431540489197
RMSE train: 3.183922	val: 3.741079	test: 4.252466
MAE train: 2.689020	val: 3.420409	test: 3.971354

Epoch: 3
Loss: 11.490971326828003
RMSE train: 3.058057	val: 3.799393	test: 4.266313
MAE train: 2.574194	val: 3.465632	test: 3.986309

Epoch: 4
Loss: 8.673985242843628
RMSE train: 2.776929	val: 3.645650	test: 4.010714
MAE train: 2.300327	val: 3.296953	test: 3.721654

Epoch: 5
Loss: 8.648349165916443
RMSE train: 2.559876	val: 3.406385	test: 3.729054
MAE train: 2.106938	val: 3.048133	test: 3.423536

Epoch: 6
Loss: 6.866507887840271
RMSE train: 2.413574	val: 3.197180	test: 3.507073
MAE train: 2.001538	val: 2.834743	test: 3.212293

Epoch: 7
Loss: 6.9163655042648315
RMSE train: 2.386800	val: 3.012719	test: 3.312688
MAE train: 1.997027	val: 2.638132	test: 3.024945

Epoch: 8
Loss: 5.487554728984833
RMSE train: 2.327279	val: 2.912295	test: 3.199256
MAE train: 1.947779	val: 2.548011	test: 2.911557

Epoch: 9
Loss: 5.624186873435974
RMSE train: 2.313897	val: 2.997845	test: 3.341911
MAE train: 1.920777	val: 2.635667	test: 3.057847

Epoch: 10
Loss: 5.5385578870773315
RMSE train: 2.258159	val: 2.988019	test: 3.314084
MAE train: 1.875225	val: 2.632784	test: 3.036639

Epoch: 11
Loss: 4.151753902435303
RMSE train: 2.155248	val: 2.876233	test: 3.187933
MAE train: 1.792207	val: 2.535350	test: 2.904445

Epoch: 12
Loss: 3.5993568301200867
RMSE train: 2.118033	val: 2.800917	test: 3.072424
MAE train: 1.791043	val: 2.489406	test: 2.790624

Epoch: 13
Loss: 3.2606820464134216
RMSE train: 1.996640	val: 2.629141	test: 2.828963
MAE train: 1.684644	val: 2.307333	test: 2.516313

Epoch: 14
Loss: 4.520355939865112
RMSE train: 1.930251	val: 2.543779	test: 2.710001
MAE train: 1.616865	val: 2.217333	test: 2.394765

Epoch: 15
Loss: 3.0258629322052
RMSE train: 1.888772	val: 2.594136	test: 2.806759
MAE train: 1.552061	val: 2.285068	test: 2.517682

Epoch: 16
Loss: 2.3585097789764404
RMSE train: 1.737635	val: 2.362030	test: 2.563193
MAE train: 1.413118	val: 2.063815	test: 2.262970

Epoch: 17
Loss: 1.983191728591919
RMSE train: 1.538880	val: 1.993159	test: 2.146084
MAE train: 1.255411	val: 1.706052	test: 1.809602

Epoch: 18
Loss: 1.8358217775821686
RMSE train: 1.492191	val: 1.932579	test: 2.086521
MAE train: 1.201696	val: 1.639990	test: 1.760582

Epoch: 19
Loss: 1.5918531119823456
RMSE train: 1.347921	val: 1.840928	test: 1.986623
MAE train: 1.080529	val: 1.564394	test: 1.662898

Epoch: 20
Loss: 1.3535144627094269
RMSE train: 1.178280	val: 1.713253	test: 1.869839
MAE train: 0.944797	val: 1.440052	test: 1.551635

Epoch: 21
Loss: 1.0213493406772614
RMSE train: 1.062414	val: 1.551061	test: 1.680458
MAE train: 0.819012	val: 1.289836	test: 1.365590

Epoch: 22
Loss: 1.0177695155143738
RMSE train: 0.851292	val: 1.233683	test: 1.350474
MAE train: 0.641949	val: 0.983715	test: 1.057885

Epoch: 23
Loss: 0.7076829001307487Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/esol/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 5
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/esol/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/esol/scaff/train_prop=0.8/esol_scaff_5_26-05_11-09-14  ]
[ Using Seed :  5  ]
[ Using device :  cuda:2  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 12.268514156341553
RMSE train: 3.371666	val: 4.108941	test: 4.192206
MAE train: 2.880489	val: 3.808610	test: 3.844076

Epoch: 2
Loss: 10.680920362472534
RMSE train: 3.264387	val: 4.247989	test: 4.303020
MAE train: 2.805074	val: 4.003232	test: 3.999463

Epoch: 3
Loss: 9.73368763923645
RMSE train: 3.164715	val: 4.312113	test: 4.392778
MAE train: 2.699874	val: 4.049075	test: 4.103237

Epoch: 4
Loss: 8.378468990325928
RMSE train: 2.951708	val: 4.129206	test: 4.242769
MAE train: 2.469119	val: 3.862387	test: 3.957757

Epoch: 5
Loss: 8.077178001403809
RMSE train: 2.568952	val: 3.516584	test: 3.651125
MAE train: 2.125640	val: 3.226129	test: 3.321829

Epoch: 6
Loss: 7.084179997444153
RMSE train: 2.446264	val: 3.299110	test: 3.437969
MAE train: 2.031882	val: 3.035272	test: 3.131238

Epoch: 7
Loss: 6.549328446388245
RMSE train: 2.403322	val: 3.280912	test: 3.410007
MAE train: 1.992931	val: 3.021807	test: 3.115508

Epoch: 8
Loss: 5.852324724197388
RMSE train: 2.308150	val: 3.226779	test: 3.353537
MAE train: 1.930983	val: 2.981132	test: 3.070388

Epoch: 9
Loss: 5.514123797416687
RMSE train: 2.261544	val: 3.188032	test: 3.344771
MAE train: 1.887651	val: 2.946898	test: 3.051768

Epoch: 10
Loss: 4.841324806213379
RMSE train: 2.120562	val: 2.963504	test: 3.153338
MAE train: 1.767605	val: 2.709398	test: 2.851935

Epoch: 11
Loss: 4.504686594009399
RMSE train: 2.009320	val: 2.781699	test: 2.959498
MAE train: 1.680291	val: 2.523700	test: 2.678036

Epoch: 12
Loss: 3.6538814902305603
RMSE train: 1.852620	val: 2.562242	test: 2.689321
MAE train: 1.533808	val: 2.273576	test: 2.398779

Epoch: 13
Loss: 3.2032113075256348
RMSE train: 1.833895	val: 2.552391	test: 2.657013
MAE train: 1.502183	val: 2.280419	test: 2.376526

Epoch: 14
Loss: 2.7897773385047913
RMSE train: 1.776678	val: 2.538433	test: 2.654098
MAE train: 1.463076	val: 2.287218	test: 2.378925

Epoch: 15
Loss: 2.4019711315631866
RMSE train: 1.720171	val: 2.428460	test: 2.563282
MAE train: 1.435418	val: 2.183685	test: 2.282780

Epoch: 16
Loss: 1.9772946536540985
RMSE train: 1.671464	val: 2.322615	test: 2.477817
MAE train: 1.393022	val: 2.069355	test: 2.191988

Epoch: 17
Loss: 1.6819444298744202
RMSE train: 1.534906	val: 2.088636	test: 2.232089
MAE train: 1.289786	val: 1.819906	test: 1.939488

Epoch: 18
Loss: 1.4479670226573944
RMSE train: 1.499847	val: 2.093309	test: 2.258492
MAE train: 1.245685	val: 1.832738	test: 1.967413

Epoch: 19
Loss: 1.298204094171524
RMSE train: 1.331031	val: 1.937471	test: 2.079714
MAE train: 1.073220	val: 1.673854	test: 1.773237

Epoch: 20
Loss: 1.0541525334119797
RMSE train: 1.212538	val: 1.905812	test: 1.953294
MAE train: 0.958199	val: 1.613787	test: 1.650506

Epoch: 21
Loss: 0.8867056220769882
RMSE train: 1.122326	val: 1.740967	test: 1.786317
MAE train: 0.882330	val: 1.498253	test: 1.505006

Epoch: 22
Loss: 0.785625621676445
RMSE train: 1.001319	val: 1.581240	test: 1.636263
MAE train: 0.769530	val: 1.336672	test: 1.341511

Epoch: 23
Loss: 0.6717066913843155Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/esol/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 6
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/esol/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/esol/scaff/train_prop=0.8/esol_scaff_6_26-05_11-09-14  ]
[ Using Seed :  6  ]
[ Using device :  cuda:2  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 12.013458013534546
RMSE train: 3.287682	val: 4.152115	test: 4.194597
MAE train: 2.737701	val: 3.788513	test: 3.798093

Epoch: 2
Loss: 10.944598913192749
RMSE train: 3.095787	val: 4.088737	test: 4.153629
MAE train: 2.549691	val: 3.801227	test: 3.835444

Epoch: 3
Loss: 9.109915018081665
RMSE train: 2.883200	val: 4.058418	test: 4.147577
MAE train: 2.310732	val: 3.761665	test: 3.828793

Epoch: 4
Loss: 8.202440738677979
RMSE train: 2.489250	val: 3.521661	test: 3.637938
MAE train: 1.965742	val: 3.238238	test: 3.324934

Epoch: 5
Loss: 7.653791069984436
RMSE train: 2.171879	val: 3.026155	test: 3.153345
MAE train: 1.710896	val: 2.746861	test: 2.819273

Epoch: 6
Loss: 7.194060802459717
RMSE train: 2.232255	val: 3.237094	test: 3.313913
MAE train: 1.761500	val: 2.961276	test: 2.980866

Epoch: 7
Loss: 6.1782402992248535
RMSE train: 2.355769	val: 3.517893	test: 3.577491
MAE train: 1.878212	val: 3.273308	test: 3.270049

Epoch: 8
Loss: 5.613225221633911
RMSE train: 2.164437	val: 3.137299	test: 3.234335
MAE train: 1.723826	val: 2.882625	test: 2.921837

Epoch: 9
Loss: 5.131393313407898
RMSE train: 2.069188	val: 2.890788	test: 3.041962
MAE train: 1.682722	val: 2.658793	test: 2.725894

Epoch: 10
Loss: 4.49596381187439
RMSE train: 2.044311	val: 2.925718	test: 3.104390
MAE train: 1.683540	val: 2.710586	test: 2.810066

Epoch: 11
Loss: 4.084007501602173
RMSE train: 1.886172	val: 2.771793	test: 2.941574
MAE train: 1.531827	val: 2.557698	test: 2.652847

Epoch: 12
Loss: 3.5803803205490112
RMSE train: 1.687997	val: 2.479065	test: 2.641922
MAE train: 1.356758	val: 2.249629	test: 2.336424

Epoch: 13
Loss: 3.121469736099243
RMSE train: 1.650857	val: 2.401808	test: 2.589877
MAE train: 1.347163	val: 2.170985	test: 2.287708

Epoch: 14
Loss: 2.5807560086250305
RMSE train: 1.595363	val: 2.329615	test: 2.550008
MAE train: 1.283721	val: 2.098376	test: 2.238047

Epoch: 15
Loss: 2.1307758688926697
RMSE train: 1.326212	val: 1.994280	test: 2.196154
MAE train: 1.018551	val: 1.749399	test: 1.888704

Epoch: 16
Loss: 1.7470726370811462
RMSE train: 1.186121	val: 1.754050	test: 1.864420
MAE train: 0.952498	val: 1.489107	test: 1.571232

Epoch: 17
Loss: 1.4714477211236954
RMSE train: 1.214475	val: 1.848171	test: 1.968296
MAE train: 0.964259	val: 1.616031	test: 1.679801

Epoch: 18
Loss: 1.3158641159534454
RMSE train: 1.166992	val: 1.739674	test: 1.897685
MAE train: 0.906321	val: 1.530259	test: 1.603851

Epoch: 19
Loss: 1.1134635657072067
RMSE train: 0.974519	val: 1.520416	test: 1.606858
MAE train: 0.765131	val: 1.276646	test: 1.319827

Epoch: 20
Loss: 0.9637902230024338
RMSE train: 0.943303	val: 1.576540	test: 1.633059
MAE train: 0.719104	val: 1.300080	test: 1.330279

Epoch: 21
Loss: 0.8634165078401566
RMSE train: 0.897460	val: 1.413345	test: 1.544715
MAE train: 0.679283	val: 1.186279	test: 1.239488

Epoch: 22
Loss: 0.7467236667871475
RMSE train: 0.884408	val: 1.429280	test: 1.583298
MAE train: 0.662045	val: 1.218873	test: 1.297964

Epoch: 23
Loss: 0.7400853186845779Arguments:
  config: <_io.TextIOWrapper name='/workspace/configs_split_experiments/GraphMVP/esol/scaff/train_prop=0.8.yml' mode='r' encoding='UTF-8'>
  seed: 42
  runseed: 4
  multiple_seeds: [4, 5, 6]
  device: cuda:2
  input_data_dir: 
  dataset: esol
  num_workers: 0
  noise_level: 0.0
  dynamic_noise: True
  train_prop: 0.8
  split: scaffold
  batch_size: 256
  epochs: 1000
  lr: 0.001
  lr_scale: 1
  decay: 0
  patience: 35
  minimum_epochs: 120
  gnn_type: gin
  num_layer: 5
  emb_dim: 300
  dropout_ratio: 0.5
  graph_pooling: mean
  JK: last
  gnn_lr_scale: 1
  model_3d: schnet
  mask_rate: 0.15
  mask_edge: 0
  csize: 3
  contextpred_neg_samples: 1
  num_filters: 128
  num_interactions: 6
  num_gaussians: 51
  cutoff: 10
  readout: mean
  schnet_lr_scale: 1
  CL_neg_samples: 1
  CL_similarity_metric: InfoNCE_dot_prod
  T: 0.1
  normalize: False
  SSL_masking_ratio: 0
  AE_model: AE
  AE_loss: l2
  detach_target: True
  beta: 1
  alpha_1: 1
  alpha_2: 1
  SSL_2D_mode: AM
  alpha_3: 0.1
  gamma_joao: 0.1
  gamma_joaov2: 0.1
  eval_train: True
  input_model_file: ../weights/pretrained/GraphMVP_regression.pth
  output_model_dir: ../runs/split/GraphMVP/esol/scaff/train_prop=0.8
  verbose: False
[ Logs to :  ../runs/split/GraphMVP/esol/scaff/train_prop=0.8/esol_scaff_4_26-05_11-09-14  ]
[ Using Seed :  4  ]
[ Using device :  cuda:2  ]
Dataset: esol
Data: Data(edge_attr=[30856, 3], edge_index=[2, 30856], id=[1127], x=[14990, 9], y=[1127])
dataset_folder: ../datasets/molecule_datasets/esol
MoleculeDatasetComplete(1127)
split via scaffold
GNN_graphpredComplete(
  (molecule_model): GNNComplete(
    (atom_encoder): AtomEncoder(
      (atom_embedding_list): ModuleList(
        (0): Embedding(119, 300)
        (1): Embedding(4, 300)
        (2): Embedding(12, 300)
        (3): Embedding(12, 300)
        (4): Embedding(10, 300)
        (5): Embedding(6, 300)
        (6): Embedding(6, 300)
        (7): Embedding(2, 300)
        (8): Embedding(2, 300)
      )
    )
    (gnns): ModuleList(
      (0): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (1): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (2): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (3): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
      (4): GINConv(
        (mlp): Sequential(
          (0): Linear(in_features=300, out_features=600, bias=True)
          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=600, out_features=300, bias=True)
        )
        (bond_encoder): BondEncoder(
          (bond_embedding_list): ModuleList(
            (0): Embedding(5, 300)
            (1): Embedding(6, 300)
            (2): Embedding(2, 300)
          )
        )
      )
    )
    (batch_norms): ModuleList(
      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 11.848088026046753
RMSE train: 3.504576	val: 4.285747	test: 4.350268
MAE train: 2.964553	val: 3.952734	test: 3.940195

Epoch: 2
Loss: 10.08765172958374
RMSE train: 3.315892	val: 4.227947	test: 4.245822
MAE train: 2.848445	val: 3.984440	test: 3.928128

Epoch: 3
Loss: 9.043644189834595
RMSE train: 3.099782	val: 4.140201	test: 4.169711
MAE train: 2.652134	val: 3.887557	test: 3.859052

Epoch: 4
Loss: 8.083170413970947
RMSE train: 2.759258	val: 3.834074	test: 3.934427
MAE train: 2.297518	val: 3.570854	test: 3.637893

Epoch: 5
Loss: 7.473030090332031
RMSE train: 2.434667	val: 3.342627	test: 3.484265
MAE train: 1.999563	val: 3.051750	test: 3.154873

Epoch: 6
Loss: 6.657284736633301
RMSE train: 2.339550	val: 3.217603	test: 3.369009
MAE train: 1.928589	val: 2.931550	test: 3.077041

Epoch: 7
Loss: 6.0293344259262085
RMSE train: 2.293735	val: 3.198188	test: 3.338912
MAE train: 1.889954	val: 2.926944	test: 3.057253

Epoch: 8
Loss: 5.2479119300842285
RMSE train: 2.307419	val: 3.219504	test: 3.350232
MAE train: 1.940861	val: 2.977057	test: 3.079748

Epoch: 9
Loss: 4.728583812713623
RMSE train: 2.249205	val: 3.183123	test: 3.308529
MAE train: 1.879691	val: 2.933996	test: 3.045629

Epoch: 10
Loss: 4.398612380027771
RMSE train: 2.121392	val: 2.970950	test: 3.113532
MAE train: 1.771525	val: 2.733204	test: 2.840854

Epoch: 11
Loss: 3.8338574171066284
RMSE train: 1.952933	val: 2.706222	test: 2.859247
MAE train: 1.627555	val: 2.456439	test: 2.571501

Epoch: 12
Loss: 3.3442503809928894
RMSE train: 1.937895	val: 2.765534	test: 2.902710
MAE train: 1.586394	val: 2.513540	test: 2.627088

Epoch: 13
Loss: 2.931743323802948
RMSE train: 1.832908	val: 2.560813	test: 2.681394
MAE train: 1.530611	val: 2.312210	test: 2.401269

Epoch: 14
Loss: 2.408468544483185
RMSE train: 1.667217	val: 2.378036	test: 2.526891
MAE train: 1.378582	val: 2.129485	test: 2.251263

Epoch: 15
Loss: 2.019256204366684
RMSE train: 1.512862	val: 2.177929	test: 2.331778
MAE train: 1.229805	val: 1.938289	test: 2.055208

Epoch: 16
Loss: 1.6755878925323486
RMSE train: 1.385450	val: 2.015435	test: 2.142869
MAE train: 1.125892	val: 1.764148	test: 1.874252

Epoch: 17
Loss: 1.4184665083885193
RMSE train: 1.245169	val: 1.804923	test: 1.936678
MAE train: 1.008323	val: 1.555921	test: 1.649443

Epoch: 18
Loss: 1.2064135074615479
RMSE train: 1.110042	val: 1.651449	test: 1.793852
MAE train: 0.891552	val: 1.415549	test: 1.498164

Epoch: 19
Loss: 1.0777216851711273
RMSE train: 1.090262	val: 1.706265	test: 1.812652
MAE train: 0.861902	val: 1.455454	test: 1.538028

Epoch: 20
Loss: 0.8806338012218475
RMSE train: 1.024884	val: 1.596227	test: 1.664208
MAE train: 0.782247	val: 1.330082	test: 1.374193

Epoch: 21
Loss: 0.7976509034633636
RMSE train: 0.915791	val: 1.454051	test: 1.545752
MAE train: 0.693600	val: 1.208069	test: 1.248159

Epoch: 22
Loss: 0.7033711075782776
RMSE train: 0.904024	val: 1.492872	test: 1.550639
MAE train: 0.684053	val: 1.241483	test: 1.265031

Epoch: 23
Loss: 0.6686849743127823
RMSE train: 1.312409	val: 2.140411	test: 2.321993
MAE train: 1.074467	val: 1.842342	test: 1.934946

Epoch: 24
Loss: 1.1668918927510579
RMSE train: 1.198746	val: 1.991445	test: 2.145982
MAE train: 0.978261	val: 1.711305	test: 1.779429

Epoch: 25
Loss: 0.982967217763265
RMSE train: 1.063752	val: 1.780100	test: 1.976516
MAE train: 0.844722	val: 1.521602	test: 1.583971

Epoch: 26
Loss: 0.8543984889984131
RMSE train: 0.956847	val: 1.656275	test: 2.103578
MAE train: 0.753908	val: 1.369041	test: 1.495963

Epoch: 27
Loss: 0.7746727267901102
RMSE train: 0.791701	val: 1.564521	test: 1.923394
MAE train: 0.610976	val: 1.305908	test: 1.420470

Epoch: 28
Loss: 0.7418959736824036
RMSE train: 0.784455	val: 1.629113	test: 1.842528
MAE train: 0.588583	val: 1.362383	test: 1.426232

Epoch: 29
Loss: 0.6599459648132324
RMSE train: 0.842845	val: 1.615822	test: 1.795741
MAE train: 0.627953	val: 1.353631	test: 1.376756

Epoch: 30
Loss: 0.6184406181176504
RMSE train: 0.707336	val: 1.459103	test: 1.824650
MAE train: 0.545968	val: 1.185904	test: 1.268020

Epoch: 31
Loss: 0.6297889351844788
RMSE train: 0.693439	val: 1.514381	test: 1.884919
MAE train: 0.542292	val: 1.240849	test: 1.316432

Epoch: 32
Loss: 0.4506991505622864
RMSE train: 0.748241	val: 1.553015	test: 1.810707
MAE train: 0.553186	val: 1.291657	test: 1.320544

Epoch: 33
Loss: 0.5057399968306223
RMSE train: 0.647541	val: 1.457716	test: 1.652880
MAE train: 0.470774	val: 1.206584	test: 1.212696

Epoch: 34
Loss: 0.5035911401112875
RMSE train: 0.580587	val: 1.409791	test: 1.602298
MAE train: 0.412044	val: 1.153079	test: 1.168924

Epoch: 35
Loss: 0.41035550832748413
RMSE train: 0.641098	val: 1.444291	test: 1.702695
MAE train: 0.473133	val: 1.195486	test: 1.223312

Epoch: 36
Loss: 0.5001932382583618
RMSE train: 0.538643	val: 1.375538	test: 1.736279
MAE train: 0.397534	val: 1.117734	test: 1.203413

Epoch: 37
Loss: 0.4897720217704773
RMSE train: 0.528892	val: 1.328723	test: 1.629900
MAE train: 0.396839	val: 1.063145	test: 1.138144

Epoch: 38
Loss: 0.47873804966608685
RMSE train: 0.583952	val: 1.385609	test: 1.607439
MAE train: 0.423214	val: 1.128133	test: 1.160618

Epoch: 39
Loss: 0.4236636261145274
RMSE train: 0.548641	val: 1.374150	test: 1.608833
MAE train: 0.393530	val: 1.110842	test: 1.134120

Epoch: 40
Loss: 0.4156324068705241
RMSE train: 0.487660	val: 1.367942	test: 1.610661
MAE train: 0.354033	val: 1.101619	test: 1.129771

Epoch: 41
Loss: 0.39824599027633667
RMSE train: 0.466950	val: 1.371409	test: 1.695879
MAE train: 0.339563	val: 1.089016	test: 1.150599

Epoch: 42
Loss: 0.4162714680035909
RMSE train: 0.476396	val: 1.379187	test: 1.863324
MAE train: 0.347407	val: 1.095098	test: 1.216116

Epoch: 43
Loss: 0.4215256671110789
RMSE train: 0.518684	val: 1.325388	test: 1.649184
MAE train: 0.370189	val: 1.056263	test: 1.118569

Epoch: 44
Loss: 0.39278167486190796
RMSE train: 0.541038	val: 1.331742	test: 1.550119
MAE train: 0.374927	val: 1.075533	test: 1.103347

Epoch: 45
Loss: 0.403422753016154
RMSE train: 0.468754	val: 1.334094	test: 1.623109
MAE train: 0.332962	val: 1.067098	test: 1.129518

Epoch: 46
Loss: 0.4101848800977071
RMSE train: 0.478655	val: 1.381850	test: 1.855123
MAE train: 0.345971	val: 1.093142	test: 1.225658

Epoch: 47
Loss: 0.41288907329241437
RMSE train: 0.457462	val: 1.336986	test: 1.635104
MAE train: 0.327718	val: 1.066486	test: 1.136470

Epoch: 48
Loss: 0.38665133714675903
RMSE train: 0.541719	val: 1.404602	test: 1.504299
MAE train: 0.378396	val: 1.151820	test: 1.149051

Epoch: 49
Loss: 0.4973617096741994
RMSE train: 0.562297	val: 1.362359	test: 1.475085
MAE train: 0.387361	val: 1.118350	test: 1.113467

Epoch: 50
Loss: 0.3832404911518097
RMSE train: 0.475166	val: 1.323441	test: 1.547029
MAE train: 0.333368	val: 1.068321	test: 1.103532

Epoch: 51
Loss: 0.3676743805408478
RMSE train: 0.508904	val: 1.378660	test: 1.650305
MAE train: 0.365174	val: 1.108994	test: 1.167492

Epoch: 52
Loss: 0.4212349057197571
RMSE train: 0.534122	val: 1.391733	test: 1.680523
MAE train: 0.390658	val: 1.119270	test: 1.181391

Epoch: 53
Loss: 0.36494508385658264
RMSE train: 0.546362	val: 1.387224	test: 1.650886
MAE train: 0.399612	val: 1.119359	test: 1.162731

Epoch: 54
Loss: 0.39002955953280133
RMSE train: 0.611648	val: 1.432113	test: 1.612604
MAE train: 0.435665	val: 1.177163	test: 1.200141

Epoch: 55
Loss: 0.37767818570137024
RMSE train: 0.583783	val: 1.437923	test: 1.611129
MAE train: 0.408491	val: 1.185406	test: 1.207956

Epoch: 56
Loss: 0.36532407999038696
RMSE train: 0.463283	val: 1.376727	test: 1.654541
MAE train: 0.330010	val: 1.096560	test: 1.161246

Epoch: 57
Loss: 0.35224778453509015
RMSE train: 0.460041	val: 1.353686	test: 1.753091
MAE train: 0.328068	val: 1.062308	test: 1.171913

Epoch: 58
Loss: 0.3654571771621704
RMSE train: 0.495982	val: 1.363107	test: 1.703792
MAE train: 0.347066	val: 1.081461	test: 1.177712

Epoch: 59
Loss: 0.3232332170009613
RMSE train: 0.439934	val: 1.330050	test: 1.549003
MAE train: 0.312639	val: 1.065695	test: 1.116545

Epoch: 60
Loss: 0.3711978892485301
RMSE train: 0.446830	val: 1.316999	test: 1.588406
MAE train: 0.318730	val: 1.060900	test: 1.132528

Epoch: 61
Loss: 0.37660297751426697
RMSE train: 0.469832	val: 1.279709	test: 1.608538
MAE train: 0.338535	val: 1.025345	test: 1.120871

Epoch: 62
Loss: 0.32890598972638446
RMSE train: 0.443958	val: 1.287304	test: 1.609257
MAE train: 0.319099	val: 1.028069	test: 1.129382

Epoch: 63
Loss: 0.3330669601758321
RMSE train: 0.447807	val: 1.351469	test: 1.676707
MAE train: 0.319224	val: 1.073337	test: 1.173818

Epoch: 64
Loss: 0.3412282069524129
RMSE train: 0.421434	val: 1.327642	test: 1.611424
MAE train: 0.298077	val: 1.054631	test: 1.128979

Epoch: 65
Loss: 0.3299794594446818
RMSE train: 0.398118	val: 1.297890	test: 1.565600
MAE train: 0.283396	val: 1.024889	test: 1.089995

Epoch: 66
Loss: 0.3279128472010295
RMSE train: 0.479472	val: 1.347185	test: 1.555093
MAE train: 0.336584	val: 1.082879	test: 1.113907

Epoch: 67
Loss: 0.312626580397288
RMSE train: 0.424857	val: 1.348674	test: 1.573427
MAE train: 0.303538	val: 1.084078	test: 1.120979

Epoch: 68
Loss: 0.35711191097895306
RMSE train: 0.454614	val: 1.396086	test: 1.653040
MAE train: 0.334204	val: 1.121025	test: 1.175309

Epoch: 69
Loss: 0.3772725462913513
RMSE train: 0.483942	val: 1.373233	test: 1.615093
MAE train: 0.340264	val: 1.099378	test: 1.151997

Epoch: 70
Loss: 0.2762352178494136
RMSE train: 0.517013	val: 1.372680	test: 1.544894
MAE train: 0.363635	val: 1.102701	test: 1.134984

Epoch: 71
Loss: 0.31797950466473895
RMSE train: 0.532000	val: 1.389865	test: 1.603136
MAE train: 0.365966	val: 1.124005	test: 1.169832

Epoch: 72
Loss: 0.33307496706644696
RMSE train: 0.512828	val: 1.366164	test: 1.611644
MAE train: 0.354915	val: 1.096748	test: 1.154785

Epoch: 73
Loss: 0.2767738203207652
RMSE train: 0.472694	val: 1.369237	test: 1.548568
MAE train: 0.336095	val: 1.102735	test: 1.126943

Epoch: 74
Loss: 0.3198019862174988
RMSE train: 0.467383	val: 1.362450	test: 1.532964
MAE train: 0.334589	val: 1.096913	test: 1.119851

Epoch: 75
Loss: 0.33961254358291626
RMSE train: 0.503372	val: 1.363063	test: 1.577625
MAE train: 0.360435	val: 1.095276	test: 1.142888

Epoch: 76
Loss: 0.2731188436349233
RMSE train: 0.482811	val: 1.356621	test: 1.653611
MAE train: 0.345084	val: 1.082686	test: 1.167003

Epoch: 77
Loss: 0.28234844903151196
RMSE train: 0.468262	val: 1.354344	test: 1.677536
MAE train: 0.334331	val: 1.078677	test: 1.166591

Epoch: 78
Loss: 0.36441382269064587
RMSE train: 0.510242	val: 1.359666	test: 1.680814
MAE train: 0.352318	val: 1.086986	test: 1.183780

Epoch: 79
Loss: 0.28688910106817883
RMSE train: 0.544993	val: 1.365827	test: 1.666138
MAE train: 0.364587	val: 1.099670	test: 1.200886

Epoch: 80
Loss: 0.30652859807014465
RMSE train: 0.461332	val: 1.346194	test: 1.549334
MAE train: 0.319188	val: 1.079920	test: 1.135712

Epoch: 81
Loss: 0.2665311396121979
RMSE train: 0.446474	val: 1.324062	test: 1.620550
MAE train: 0.315381	val: 1.054774	test: 1.153109

Epoch: 82
Loss: 0.2897161195675532
RMSE train: 0.453616	val: 1.328738	test: 1.627750
MAE train: 0.324723	val: 1.073662	test: 1.171138

Epoch: 83
Loss: 0.31354742248853046
RMSE train: 0.460345	val: 1.330042	test: 1.482166
MAE train: 0.330043	val: 1.088140	test: 1.123575
RMSE train: 1.047397	val: 1.882111	test: 2.013582
MAE train: 0.823690	val: 1.601649	test: 1.657281

Epoch: 24
Loss: 0.9457318385442098
RMSE train: 0.985297	val: 1.670848	test: 1.819404
MAE train: 0.767239	val: 1.384367	test: 1.425651

Epoch: 25
Loss: 0.8222065965334574
RMSE train: 0.925519	val: 1.606122	test: 1.695450
MAE train: 0.700491	val: 1.328033	test: 1.342852

Epoch: 26
Loss: 0.7536689639091492
RMSE train: 0.853905	val: 1.668016	test: 1.858920
MAE train: 0.639544	val: 1.403771	test: 1.435693

Epoch: 27
Loss: 0.7270788153012594
RMSE train: 0.777881	val: 1.658518	test: 1.926522
MAE train: 0.580155	val: 1.391778	test: 1.440197

Epoch: 28
Loss: 0.6075701614220937
RMSE train: 0.819476	val: 1.521588	test: 1.704892
MAE train: 0.612641	val: 1.249164	test: 1.276906

Epoch: 29
Loss: 0.5905730624993643
RMSE train: 0.825475	val: 1.553675	test: 1.679247
MAE train: 0.590887	val: 1.278677	test: 1.283220

Epoch: 30
Loss: 0.5743162234624227
RMSE train: 0.630408	val: 1.489095	test: 1.661139
MAE train: 0.455826	val: 1.215518	test: 1.242186

Epoch: 31
Loss: 0.5012254317601522
RMSE train: 0.552129	val: 1.419057	test: 1.788651
MAE train: 0.393970	val: 1.128972	test: 1.211706

Epoch: 32
Loss: 0.47132210930188495
RMSE train: 0.581676	val: 1.432214	test: 1.813364
MAE train: 0.414634	val: 1.149426	test: 1.244297

Epoch: 33
Loss: 0.4909754196802775
RMSE train: 0.509027	val: 1.337040	test: 1.640603
MAE train: 0.356367	val: 1.086074	test: 1.150420

Epoch: 34
Loss: 0.5399686396121979
RMSE train: 0.533962	val: 1.321987	test: 1.532638
MAE train: 0.370314	val: 1.078652	test: 1.102953

Epoch: 35
Loss: 0.4223240613937378
RMSE train: 0.494614	val: 1.300700	test: 1.498992
MAE train: 0.349540	val: 1.059561	test: 1.076729

Epoch: 36
Loss: 0.43637682000796
RMSE train: 0.519210	val: 1.351020	test: 1.620105
MAE train: 0.370568	val: 1.097221	test: 1.144766

Epoch: 37
Loss: 0.4611216684182485
RMSE train: 0.495339	val: 1.351185	test: 1.748457
MAE train: 0.360047	val: 1.096767	test: 1.188270

Epoch: 38
Loss: 0.4603910644849141
RMSE train: 0.494184	val: 1.361404	test: 1.776265
MAE train: 0.356743	val: 1.096443	test: 1.184130

Epoch: 39
Loss: 0.4122484127680461
RMSE train: 0.513039	val: 1.376381	test: 1.675659
MAE train: 0.367759	val: 1.101052	test: 1.142449

Epoch: 40
Loss: 0.41690685351689655
RMSE train: 0.492780	val: 1.395189	test: 1.768152
MAE train: 0.356727	val: 1.104249	test: 1.171469

Epoch: 41
Loss: 0.3814917902151744
RMSE train: 0.505437	val: 1.417504	test: 1.864309
MAE train: 0.369369	val: 1.112757	test: 1.219603

Epoch: 42
Loss: 0.3923044999440511
RMSE train: 0.491703	val: 1.398959	test: 1.695672
MAE train: 0.355853	val: 1.111716	test: 1.185080

Epoch: 43
Loss: 0.47649943828582764
RMSE train: 0.463002	val: 1.357459	test: 1.631934
MAE train: 0.325322	val: 1.076321	test: 1.142724

Epoch: 44
Loss: 0.3813264270623525
RMSE train: 0.467018	val: 1.398559	test: 1.770475
MAE train: 0.330750	val: 1.106778	test: 1.201377

Epoch: 45
Loss: 0.409572829802831
RMSE train: 0.528850	val: 1.440516	test: 1.790785
MAE train: 0.385759	val: 1.145366	test: 1.230526

Epoch: 46
Loss: 0.39768292506535846
RMSE train: 0.469996	val: 1.378547	test: 1.656799
MAE train: 0.333981	val: 1.083321	test: 1.136905

Epoch: 47
Loss: 0.40902114907900494
RMSE train: 0.482393	val: 1.353527	test: 1.643218
MAE train: 0.349600	val: 1.060408	test: 1.114643

Epoch: 48
Loss: 0.404219130674998
RMSE train: 0.556934	val: 1.434696	test: 1.689875
MAE train: 0.408193	val: 1.145348	test: 1.190840

Epoch: 49
Loss: 0.39830369750658673
RMSE train: 0.479840	val: 1.394833	test: 1.662355
MAE train: 0.344814	val: 1.114632	test: 1.154220

Epoch: 50
Loss: 0.35300663113594055
RMSE train: 0.405071	val: 1.365490	test: 1.845925
MAE train: 0.294936	val: 1.047481	test: 1.160644

Epoch: 51
Loss: 0.36647870143254596
RMSE train: 0.437815	val: 1.388653	test: 1.912056
MAE train: 0.322983	val: 1.075537	test: 1.203332

Epoch: 52
Loss: 0.3583337863286336
RMSE train: 0.551461	val: 1.438222	test: 1.749482
MAE train: 0.408746	val: 1.139095	test: 1.202571

Epoch: 53
Loss: 0.3746268649895986
RMSE train: 0.529908	val: 1.425573	test: 1.648518
MAE train: 0.383389	val: 1.151403	test: 1.184129

Epoch: 54
Loss: 0.3390459616978963
RMSE train: 0.499737	val: 1.373790	test: 1.651593
MAE train: 0.362188	val: 1.097631	test: 1.141140

Epoch: 55
Loss: 0.36332590381304425
RMSE train: 0.553790	val: 1.360414	test: 1.610572
MAE train: 0.395848	val: 1.074682	test: 1.137721

Epoch: 56
Loss: 0.34107962250709534
RMSE train: 0.563867	val: 1.404681	test: 1.641570
MAE train: 0.401813	val: 1.115363	test: 1.178235

Epoch: 57
Loss: 0.3457929293314616
RMSE train: 0.508130	val: 1.412887	test: 1.647307
MAE train: 0.362496	val: 1.128685	test: 1.177295

Epoch: 58
Loss: 0.2847351133823395
RMSE train: 0.439611	val: 1.364057	test: 1.652195
MAE train: 0.315894	val: 1.081593	test: 1.145769

Epoch: 59
Loss: 0.36711304386456806
RMSE train: 0.464573	val: 1.355017	test: 1.649547
MAE train: 0.331420	val: 1.074739	test: 1.153213

Epoch: 60
Loss: 0.37926963965098065
RMSE train: 0.539862	val: 1.365992	test: 1.608957
MAE train: 0.378881	val: 1.085870	test: 1.168495

Epoch: 61
Loss: 0.35175156593322754
RMSE train: 0.439110	val: 1.309667	test: 1.546841
MAE train: 0.313846	val: 1.041469	test: 1.105559

Epoch: 62
Loss: 0.28887367248535156
RMSE train: 0.374641	val: 1.306576	test: 1.687931
MAE train: 0.276272	val: 1.026237	test: 1.143005

Epoch: 63
Loss: 0.3208616077899933
RMSE train: 0.387706	val: 1.334994	test: 1.717998
MAE train: 0.282567	val: 1.053577	test: 1.171766

Epoch: 64
Loss: 0.3378281792004903
RMSE train: 0.477173	val: 1.382778	test: 1.688075
MAE train: 0.342190	val: 1.092825	test: 1.187922

Epoch: 65
Loss: 0.2845918635527293
RMSE train: 0.421196	val: 1.359998	test: 1.734186
MAE train: 0.304067	val: 1.055772	test: 1.157077

Epoch: 66
Loss: 0.2893221378326416
RMSE train: 0.401057	val: 1.377629	test: 1.886017
MAE train: 0.294540	val: 1.049543	test: 1.185345

Epoch: 67
Loss: 0.3086923360824585
RMSE train: 0.444384	val: 1.406826	test: 1.909111
MAE train: 0.326241	val: 1.080834	test: 1.225886

Epoch: 68
Loss: 0.2892398585875829
RMSE train: 0.486054	val: 1.399964	test: 1.848509
MAE train: 0.345980	val: 1.091846	test: 1.225588

Epoch: 69
Loss: 0.2998347083727519
RMSE train: 0.464642	val: 1.340399	test: 1.663477
MAE train: 0.321546	val: 1.055820	test: 1.139003

Epoch: 70
Loss: 0.31794236103693646
RMSE train: 0.399934	val: 1.289154	test: 1.637496
MAE train: 0.287340	val: 1.004477	test: 1.098093

Epoch: 71
Loss: 0.3232404987017314
RMSE train: 0.425085	val: 1.332523	test: 1.687419
MAE train: 0.312118	val: 1.049319	test: 1.143713

Epoch: 72
Loss: 0.32036255796750385
RMSE train: 0.569540	val: 1.455380	test: 1.721401
MAE train: 0.419268	val: 1.158395	test: 1.228158

Epoch: 73
Loss: 0.2810089786847432
RMSE train: 0.577471	val: 1.443337	test: 1.665799
MAE train: 0.417958	val: 1.143069	test: 1.188498

Epoch: 74
Loss: 0.3033870855967204
RMSE train: 0.546962	val: 1.393886	test: 1.579210
MAE train: 0.392822	val: 1.094642	test: 1.104956

Epoch: 75
Loss: 0.293509582678477
RMSE train: 0.555793	val: 1.431976	test: 1.689446
MAE train: 0.396204	val: 1.127935	test: 1.174934

Epoch: 76
Loss: 0.2994578182697296
RMSE train: 0.440184	val: 1.343110	test: 1.607208
MAE train: 0.319489	val: 1.071225	test: 1.127704

Epoch: 77
Loss: 0.31116753816604614
RMSE train: 0.457034	val: 1.361194	test: 1.661069
MAE train: 0.325801	val: 1.081578	test: 1.178371

Epoch: 78
Loss: 0.3141357998053233
RMSE train: 0.529840	val: 1.412513	test: 1.713578
MAE train: 0.392929	val: 1.125038	test: 1.214266

Epoch: 79
Loss: 0.3233564992745717
RMSE train: 0.377343	val: 1.311089	test: 1.588825
MAE train: 0.273819	val: 1.016237	test: 1.091564

Epoch: 80
Loss: 0.3019310136636098
RMSE train: 0.373437	val: 1.314484	test: 1.657210
MAE train: 0.269567	val: 1.006446	test: 1.126208

Epoch: 81
Loss: 0.2671586771806081
RMSE train: 0.408673	val: 1.356345	test: 1.761573
MAE train: 0.294948	val: 1.054389	test: 1.193730

Epoch: 82
Loss: 0.3086185157299042
RMSE train: 0.371415	val: 1.360915	test: 1.752014
MAE train: 0.275134	val: 1.060167	test: 1.180509

Epoch: 83
Loss: 0.2953815956910451
RMSE train: 0.379796	val: 1.340077	test: 1.640095
MAE train: 0.277034	val: 1.054005	test: 1.136151
RMSE train: 1.086318	val: 2.131251	test: 2.219056
MAE train: 0.861305	val: 1.854679	test: 1.898353

Epoch: 24
Loss: 1.0482410391171773
RMSE train: 1.049838	val: 1.998781	test: 2.079841
MAE train: 0.838478	val: 1.726659	test: 1.743942

Epoch: 25
Loss: 0.9404623111089071
RMSE train: 0.899392	val: 1.794455	test: 1.851169
MAE train: 0.694611	val: 1.517212	test: 1.522908

Epoch: 26
Loss: 0.7746474146842957
RMSE train: 0.835947	val: 1.732696	test: 1.814348
MAE train: 0.638975	val: 1.466981	test: 1.463532

Epoch: 27
Loss: 0.6767117778460184
RMSE train: 0.829784	val: 1.713565	test: 1.849698
MAE train: 0.643417	val: 1.443511	test: 1.453655

Epoch: 28
Loss: 0.6266632278760275
RMSE train: 0.726628	val: 1.622522	test: 1.735281
MAE train: 0.538588	val: 1.346742	test: 1.359858

Epoch: 29
Loss: 0.5941134691238403
RMSE train: 0.692991	val: 1.581658	test: 1.741710
MAE train: 0.511684	val: 1.304739	test: 1.326459

Epoch: 30
Loss: 0.5482516090075175
RMSE train: 0.719893	val: 1.649106	test: 1.962524
MAE train: 0.547898	val: 1.349961	test: 1.395542

Epoch: 31
Loss: 0.5667001207669576
RMSE train: 0.632001	val: 1.471752	test: 1.738563
MAE train: 0.478018	val: 1.182363	test: 1.230954

Epoch: 32
Loss: 0.5105222364266714
RMSE train: 0.594613	val: 1.480399	test: 1.724635
MAE train: 0.431048	val: 1.204391	test: 1.242192

Epoch: 33
Loss: 0.5203449328740438
RMSE train: 0.593525	val: 1.456014	test: 1.836349
MAE train: 0.444064	val: 1.163602	test: 1.220419

Epoch: 34
Loss: 0.4039915104707082
RMSE train: 0.466100	val: 1.382031	test: 1.941591
MAE train: 0.343598	val: 1.080817	test: 1.209729

Epoch: 35
Loss: 0.4991247554620107
RMSE train: 0.504322	val: 1.368236	test: 1.876330
MAE train: 0.365796	val: 1.074725	test: 1.195308

Epoch: 36
Loss: 0.4124642113844554
RMSE train: 0.582735	val: 1.408797	test: 1.828949
MAE train: 0.418158	val: 1.114375	test: 1.208556

Epoch: 37
Loss: 0.4446787436803182
RMSE train: 0.468419	val: 1.389661	test: 1.917477
MAE train: 0.335523	val: 1.080653	test: 1.183417

Epoch: 38
Loss: 0.4288799266020457
RMSE train: 0.463220	val: 1.446099	test: 2.117192
MAE train: 0.341815	val: 1.114873	test: 1.254135

Epoch: 39
Loss: 0.4524346590042114
RMSE train: 0.512131	val: 1.440512	test: 1.925955
MAE train: 0.368735	val: 1.149126	test: 1.246377

Epoch: 40
Loss: 0.4256449341773987
RMSE train: 0.510868	val: 1.483446	test: 1.906684
MAE train: 0.361407	val: 1.190927	test: 1.279887

Epoch: 41
Loss: 0.43357113003730774
RMSE train: 0.516112	val: 1.460599	test: 1.922004
MAE train: 0.375670	val: 1.157942	test: 1.240952

Epoch: 42
Loss: 0.3871269424756368
RMSE train: 0.530343	val: 1.464644	test: 1.970661
MAE train: 0.389757	val: 1.143816	test: 1.245510

Epoch: 43
Loss: 0.3699674606323242
RMSE train: 0.496644	val: 1.474655	test: 1.937220
MAE train: 0.357177	val: 1.153915	test: 1.241887

Epoch: 44
Loss: 0.3711535135904948
RMSE train: 0.449825	val: 1.498457	test: 1.986152
MAE train: 0.323659	val: 1.165524	test: 1.242533

Epoch: 45
Loss: 0.41959430774052936
RMSE train: 0.505504	val: 1.517935	test: 1.888861
MAE train: 0.359237	val: 1.201076	test: 1.240280

Epoch: 46
Loss: 0.41517223914464313
RMSE train: 0.627678	val: 1.552346	test: 1.893057
MAE train: 0.441319	val: 1.245044	test: 1.270814

Epoch: 47
Loss: 0.42780139048894245
RMSE train: 0.472595	val: 1.453988	test: 1.853237
MAE train: 0.346039	val: 1.125075	test: 1.208110

Epoch: 48
Loss: 0.42127497990926105
RMSE train: 0.544749	val: 1.491720	test: 1.861507
MAE train: 0.398563	val: 1.184538	test: 1.235830

Epoch: 49
Loss: 0.38937945167223614
RMSE train: 0.618972	val: 1.584333	test: 1.819432
MAE train: 0.451918	val: 1.291847	test: 1.293698

Epoch: 50
Loss: 0.3818580408891042
RMSE train: 0.522768	val: 1.556510	test: 1.795862
MAE train: 0.381081	val: 1.259664	test: 1.265014

Epoch: 51
Loss: 0.3443868160247803
RMSE train: 0.493773	val: 1.507923	test: 1.767646
MAE train: 0.352667	val: 1.199565	test: 1.218045

Epoch: 52
Loss: 0.38010525703430176
RMSE train: 0.501673	val: 1.515423	test: 1.801872
MAE train: 0.351882	val: 1.204424	test: 1.230715

Epoch: 53
Loss: 0.30594555536905926
RMSE train: 0.513655	val: 1.505448	test: 1.668942
MAE train: 0.355861	val: 1.215032	test: 1.210566

Epoch: 54
Loss: 0.3510276675224304
RMSE train: 0.568067	val: 1.513603	test: 1.734927
MAE train: 0.400577	val: 1.217581	test: 1.229256

Epoch: 55
Loss: 0.34108930826187134
RMSE train: 0.577622	val: 1.523240	test: 1.770166
MAE train: 0.412107	val: 1.221382	test: 1.242099

Epoch: 56
Loss: 0.36889609694480896
RMSE train: 0.485328	val: 1.493349	test: 1.779761
MAE train: 0.348103	val: 1.183643	test: 1.224598

Epoch: 57
Loss: 0.32926151156425476
RMSE train: 0.468424	val: 1.541451	test: 1.838181
MAE train: 0.341866	val: 1.221789	test: 1.260694

Epoch: 58
Loss: 0.34663451711336773
RMSE train: 0.522418	val: 1.551421	test: 1.842423
MAE train: 0.370203	val: 1.242135	test: 1.282790

Epoch: 59
Loss: 0.33377204338709515
RMSE train: 0.490125	val: 1.454963	test: 1.719502
MAE train: 0.340728	val: 1.168018	test: 1.209364

Epoch: 60
Loss: 0.38088921705881756
RMSE train: 0.443521	val: 1.516503	test: 1.708896
MAE train: 0.314660	val: 1.212549	test: 1.222636

Epoch: 61
Loss: 0.34695632259051007
RMSE train: 0.451459	val: 1.558433	test: 1.708007
MAE train: 0.325302	val: 1.247421	test: 1.239848

Epoch: 62
Loss: 0.40302756428718567
RMSE train: 0.420825	val: 1.482167	test: 1.748701
MAE train: 0.299347	val: 1.168089	test: 1.205240

Epoch: 63
Loss: 0.3015701671441396
RMSE train: 0.396493	val: 1.496814	test: 1.881977
MAE train: 0.289581	val: 1.147379	test: 1.231805

Epoch: 64
Loss: 0.29282185435295105
RMSE train: 0.468263	val: 1.532632	test: 1.883463
MAE train: 0.342880	val: 1.186726	test: 1.256258

Epoch: 65
Loss: 0.3237374226252238
RMSE train: 0.460564	val: 1.467382	test: 1.711665
MAE train: 0.327828	val: 1.155146	test: 1.198847

Epoch: 66
Loss: 0.31650227308273315
RMSE train: 0.420421	val: 1.415778	test: 1.631819
MAE train: 0.299545	val: 1.115216	test: 1.156077

Epoch: 67
Loss: 0.28036737938721973
RMSE train: 0.407679	val: 1.456474	test: 1.727913
MAE train: 0.288945	val: 1.137843	test: 1.185196

Epoch: 68
Loss: 0.34950711329778034
RMSE train: 0.444581	val: 1.484905	test: 1.771510
MAE train: 0.312928	val: 1.169818	test: 1.216652

Epoch: 69
Loss: 0.3102969725926717
RMSE train: 0.435981	val: 1.505482	test: 1.795318
MAE train: 0.311660	val: 1.192351	test: 1.234895

Epoch: 70
Loss: 0.313255250453949
RMSE train: 0.501209	val: 1.508870	test: 1.870383
MAE train: 0.355332	val: 1.174086	test: 1.241618

Epoch: 71
Loss: 0.3113454778989156
RMSE train: 0.404404	val: 1.507154	test: 1.923469
MAE train: 0.287941	val: 1.138004	test: 1.235908

Epoch: 72
Loss: 0.30569236477216083
RMSE train: 0.418065	val: 1.532886	test: 1.941755
MAE train: 0.294083	val: 1.162730	test: 1.256876

Epoch: 73
Loss: 0.302444060643514
RMSE train: 0.511329	val: 1.579682	test: 1.888443
MAE train: 0.361732	val: 1.238063	test: 1.287095

Epoch: 74
Loss: 0.29351983467737836
RMSE train: 0.448586	val: 1.525224	test: 1.707017
MAE train: 0.316042	val: 1.220752	test: 1.229599

Epoch: 75
Loss: 0.31549544135729474
RMSE train: 0.463171	val: 1.544760	test: 1.767824
MAE train: 0.326176	val: 1.225595	test: 1.243969

Epoch: 76
Loss: 0.3644530375798543
RMSE train: 0.525991	val: 1.544295	test: 1.762531
MAE train: 0.373259	val: 1.227785	test: 1.258314

Epoch: 77
Loss: 0.31692931056022644
RMSE train: 0.541633	val: 1.549364	test: 1.802768
MAE train: 0.380730	val: 1.225682	test: 1.272217

Epoch: 78
Loss: 0.2983296811580658
RMSE train: 0.462517	val: 1.499689	test: 1.770129
MAE train: 0.322938	val: 1.173310	test: 1.239564

Epoch: 79
Loss: 0.29410170515378314
RMSE train: 0.419386	val: 1.513137	test: 1.811796
MAE train: 0.299217	val: 1.166486	test: 1.240404

Epoch: 80
Loss: 0.29211562871932983
RMSE train: 0.448926	val: 1.587050	test: 2.007753
MAE train: 0.318558	val: 1.201563	test: 1.299329

Epoch: 81
Loss: 0.2987978259722392
RMSE train: 0.419787	val: 1.522314	test: 1.884274
MAE train: 0.292122	val: 1.152851	test: 1.240611

Epoch: 82
Loss: 0.2490376482407252
RMSE train: 0.451237	val: 1.498216	test: 1.825484
MAE train: 0.304719	val: 1.154766	test: 1.233510

Epoch: 83
Loss: 0.331540713707606
RMSE train: 0.441692	val: 1.569500	test: 2.182741
MAE train: 0.304217	val: 1.152752	test: 1.315625
RMSE train: 0.694078	val: 1.214454	test: 1.376728
MAE train: 0.537148	val: 0.974296	test: 1.071063

Epoch: 24
Loss: 0.7811556607484818
RMSE train: 0.720343	val: 1.361515	test: 1.544268
MAE train: 0.528589	val: 1.111639	test: 1.190092

Epoch: 25
Loss: 0.7625413686037064
RMSE train: 0.708390	val: 1.275173	test: 1.461642
MAE train: 0.504756	val: 1.007570	test: 1.125805

Epoch: 26
Loss: 0.9324763864278793
RMSE train: 0.742747	val: 1.286987	test: 1.503073
MAE train: 0.530706	val: 1.015320	test: 1.146929

Epoch: 27
Loss: 0.7917362302541733
RMSE train: 0.632608	val: 1.175618	test: 1.362325
MAE train: 0.467857	val: 0.917029	test: 1.051977

Epoch: 28
Loss: 0.7393708974123001
RMSE train: 0.681498	val: 1.177115	test: 1.322240
MAE train: 0.506176	val: 0.906406	test: 1.044108

Epoch: 29
Loss: 0.7058498412370682
RMSE train: 0.684031	val: 1.251058	test: 1.406389
MAE train: 0.509403	val: 0.975768	test: 1.081954

Epoch: 30
Loss: 0.6789928078651428
RMSE train: 0.647003	val: 1.276074	test: 1.472366
MAE train: 0.482474	val: 1.012911	test: 1.119381

Epoch: 31
Loss: 0.6652373820543289
RMSE train: 0.779654	val: 1.398541	test: 1.575781
MAE train: 0.585406	val: 1.127188	test: 1.248516

Epoch: 32
Loss: 0.6864495128393173
RMSE train: 0.797737	val: 1.403952	test: 1.562048
MAE train: 0.582616	val: 1.127915	test: 1.250811

Epoch: 33
Loss: 0.5545169711112976
RMSE train: 0.687154	val: 1.181805	test: 1.305704
MAE train: 0.512515	val: 0.904909	test: 1.022671

Epoch: 34
Loss: 0.6212143152952194
RMSE train: 0.613890	val: 1.208552	test: 1.318847
MAE train: 0.456826	val: 0.964965	test: 1.055487

Epoch: 35
Loss: 0.5257135555148125
RMSE train: 0.616077	val: 1.216508	test: 1.330921
MAE train: 0.462598	val: 0.959261	test: 1.065180

Epoch: 36
Loss: 0.8662890046834946
RMSE train: 0.566208	val: 1.105770	test: 1.225282
MAE train: 0.417634	val: 0.856020	test: 0.969953

Epoch: 37
Loss: 0.5451409816741943
RMSE train: 0.615939	val: 1.161226	test: 1.273407
MAE train: 0.442054	val: 0.907330	test: 1.010375

Epoch: 38
Loss: 0.6079607307910919
RMSE train: 0.619854	val: 1.133327	test: 1.235255
MAE train: 0.449925	val: 0.892925	test: 0.981286

Epoch: 39
Loss: 0.5777158886194229
RMSE train: 0.611973	val: 1.141541	test: 1.243913
MAE train: 0.442053	val: 0.902846	test: 0.989374

Epoch: 40
Loss: 0.6201487481594086
RMSE train: 0.575156	val: 1.144195	test: 1.282611
MAE train: 0.428184	val: 0.910865	test: 1.008777

Epoch: 41
Loss: 0.6227193772792816
RMSE train: 0.558402	val: 1.173935	test: 1.356115
MAE train: 0.424339	val: 0.917560	test: 1.064300

Epoch: 42
Loss: 0.6349154114723206
RMSE train: 0.535440	val: 1.122740	test: 1.259921
MAE train: 0.393224	val: 0.841425	test: 1.001279

Epoch: 43
Loss: 0.6772568374872208
RMSE train: 0.661435	val: 1.193957	test: 1.284122
MAE train: 0.481032	val: 0.924316	test: 1.020769

Epoch: 44
Loss: 0.6513628661632538
RMSE train: 0.761980	val: 1.229151	test: 1.326282
MAE train: 0.594121	val: 0.977206	test: 1.026430

Epoch: 45
Loss: 0.753935232758522
RMSE train: 0.588435	val: 1.086775	test: 1.197923
MAE train: 0.468566	val: 0.824548	test: 0.952076

Epoch: 46
Loss: 0.6657398045063019
RMSE train: 0.581343	val: 1.129561	test: 1.325079
MAE train: 0.452053	val: 0.861052	test: 1.040608

Epoch: 47
Loss: 0.6735129803419113
RMSE train: 0.570964	val: 1.123091	test: 1.183821
MAE train: 0.422650	val: 0.849376	test: 0.960124

Epoch: 48
Loss: 0.6360879242420197
RMSE train: 0.744071	val: 1.219126	test: 1.252960
MAE train: 0.538943	val: 0.946773	test: 1.029461

Epoch: 49
Loss: 0.7797234654426575
RMSE train: 0.551603	val: 1.189774	test: 1.228744
MAE train: 0.396623	val: 0.898408	test: 0.993290

Epoch: 50
Loss: 0.6325949728488922
RMSE train: 0.543342	val: 1.226684	test: 1.303746
MAE train: 0.400074	val: 0.935268	test: 1.044704

Epoch: 51
Loss: 0.7176104709506035
RMSE train: 0.598576	val: 1.210727	test: 1.290998
MAE train: 0.442444	val: 0.952017	test: 1.022869

Epoch: 52
Loss: 0.6265454441308975
RMSE train: 0.694390	val: 1.294472	test: 1.382701
MAE train: 0.507696	val: 1.029000	test: 1.105601

Epoch: 53
Loss: 0.8001937493681908
RMSE train: 0.659351	val: 1.263282	test: 1.322881
MAE train: 0.475796	val: 0.994954	test: 1.049918

Epoch: 54
Loss: 0.4771408215165138
RMSE train: 0.652938	val: 1.176578	test: 1.235870
MAE train: 0.477023	val: 0.930235	test: 0.990240

Epoch: 55
Loss: 0.5610813423991203
RMSE train: 0.552227	val: 1.086953	test: 1.127249
MAE train: 0.397345	val: 0.842876	test: 0.897462

Epoch: 56
Loss: 0.491773821413517
RMSE train: 0.555870	val: 1.128042	test: 1.180346
MAE train: 0.406013	val: 0.868747	test: 0.918587

Epoch: 57
Loss: 0.6004916504025459
RMSE train: 0.696117	val: 1.309663	test: 1.367883
MAE train: 0.521470	val: 1.047349	test: 1.090449

Epoch: 58
Loss: 0.6041625440120697
RMSE train: 0.653138	val: 1.274739	test: 1.336846
MAE train: 0.481359	val: 1.021053	test: 1.062037

Epoch: 59
Loss: 0.5519495904445648
RMSE train: 0.551872	val: 1.097135	test: 1.126137
MAE train: 0.387815	val: 0.871649	test: 0.882145

Epoch: 60
Loss: 0.6838040873408318
RMSE train: 0.488962	val: 1.007784	test: 1.040475
MAE train: 0.355137	val: 0.788876	test: 0.839057

Epoch: 61
Loss: 0.4345845356583595
RMSE train: 0.521299	val: 1.071030	test: 1.154060
MAE train: 0.382929	val: 0.819879	test: 0.935037

Epoch: 62
Loss: 0.6060295552015305
RMSE train: 0.542877	val: 1.111046	test: 1.144867
MAE train: 0.414566	val: 0.853351	test: 0.896613

Epoch: 63
Loss: 0.7759343087673187
RMSE train: 0.501703	val: 1.074916	test: 1.100581
MAE train: 0.367665	val: 0.822807	test: 0.864278

Epoch: 64
Loss: 0.8652403950691223
RMSE train: 0.599514	val: 1.113397	test: 1.146303
MAE train: 0.439633	val: 0.877179	test: 0.901782

Epoch: 65
Loss: 0.5294046700000763
RMSE train: 0.585466	val: 1.135269	test: 1.187317
MAE train: 0.436897	val: 0.886926	test: 0.931619

Epoch: 66
Loss: 0.5228321701288223
RMSE train: 0.663361	val: 1.241758	test: 1.302793
MAE train: 0.498476	val: 0.971593	test: 1.029318

Epoch: 67
Loss: 0.49185624718666077
RMSE train: 0.685858	val: 1.209262	test: 1.262866
MAE train: 0.508467	val: 0.950041	test: 0.984620

Epoch: 68
Loss: 0.6360322684049606
RMSE train: 0.707805	val: 1.199531	test: 1.269571
MAE train: 0.510756	val: 0.942960	test: 0.990157

Epoch: 69
Loss: 0.6035274490714073
RMSE train: 0.606695	val: 1.182132	test: 1.234099
MAE train: 0.443615	val: 0.927412	test: 0.966091

Epoch: 70
Loss: 0.4025239124894142
RMSE train: 0.543510	val: 1.137145	test: 1.189181
MAE train: 0.392296	val: 0.887533	test: 0.931593

Epoch: 71
Loss: 0.6714605316519737
RMSE train: 0.481430	val: 1.097414	test: 1.178927
MAE train: 0.357005	val: 0.848183	test: 0.941677

Epoch: 72
Loss: 0.40191376954317093
RMSE train: 0.512982	val: 1.142638	test: 1.182061
MAE train: 0.380755	val: 0.871396	test: 0.934256

Epoch: 73
Loss: 0.423462338745594
RMSE train: 0.522431	val: 1.129821	test: 1.149438
MAE train: 0.380328	val: 0.874887	test: 0.905904

Epoch: 74
Loss: 0.4634392485022545
RMSE train: 0.541503	val: 1.130881	test: 1.141466
MAE train: 0.378607	val: 0.890647	test: 0.897731

Epoch: 75
Loss: 0.4559481665492058
RMSE train: 0.640375	val: 1.223715	test: 1.273082
MAE train: 0.448473	val: 0.970890	test: 1.005307

Epoch: 76
Loss: 0.423989474773407
RMSE train: 0.675242	val: 1.256103	test: 1.327290
MAE train: 0.476574	val: 0.994199	test: 1.052800

Epoch: 77
Loss: 1.0989026799798012
RMSE train: 0.580355	val: 1.189803	test: 1.256206
MAE train: 0.416948	val: 0.932698	test: 0.988018

Epoch: 78
Loss: 0.4253317639231682
RMSE train: 0.479521	val: 1.105606	test: 1.160743
MAE train: 0.353530	val: 0.843637	test: 0.920365

Epoch: 79
Loss: 0.47909392416477203
RMSE train: 0.493809	val: 1.101964	test: 1.148793
MAE train: 0.360152	val: 0.842661	test: 0.910427

Epoch: 80
Loss: 0.7371247112751007
RMSE train: 0.532063	val: 1.096077	test: 1.147825
MAE train: 0.385105	val: 0.831991	test: 0.912068

Epoch: 81
Loss: 0.43754518777132034
RMSE train: 0.556413	val: 1.112126	test: 1.160601
MAE train: 0.402590	val: 0.860262	test: 0.917313

Epoch: 82
Loss: 0.5837442502379417
RMSE train: 0.467383	val: 1.048763	test: 1.095838
MAE train: 0.355435	val: 0.781487	test: 0.877488

Epoch: 83
Loss: 0.5231760814785957
RMSE train: 0.470447	val: 1.062597	test: 1.108554
MAE train: 0.346862	val: 0.824254	test: 0.878480
RMSE train: 0.933100	val: 1.575874	test: 1.686264
MAE train: 0.727587	val: 1.291639	test: 1.377901

Epoch: 24
Loss: 0.8801540583372116
RMSE train: 0.763477	val: 1.397554	test: 1.438950
MAE train: 0.586889	val: 1.133299	test: 1.162680

Epoch: 25
Loss: 0.6993922144174576
RMSE train: 0.690844	val: 1.363366	test: 1.375113
MAE train: 0.523604	val: 1.071918	test: 1.105391

Epoch: 26
Loss: 0.6166593730449677
RMSE train: 0.714455	val: 1.358300	test: 1.398632
MAE train: 0.531281	val: 1.085264	test: 1.117000

Epoch: 27
Loss: 0.7356347143650055
RMSE train: 0.709665	val: 1.268386	test: 1.304674
MAE train: 0.522280	val: 1.000099	test: 1.025244

Epoch: 28
Loss: 0.7613252326846123
RMSE train: 0.743402	val: 1.277966	test: 1.311349
MAE train: 0.532834	val: 1.000446	test: 1.020510

Epoch: 29
Loss: 0.6377323269844055
RMSE train: 0.608581	val: 1.219220	test: 1.254982
MAE train: 0.459559	val: 0.940128	test: 0.996473

Epoch: 30
Loss: 0.7977935820817947
RMSE train: 0.645954	val: 1.214071	test: 1.266557
MAE train: 0.487671	val: 0.928728	test: 0.990197

Epoch: 31
Loss: 0.8148466646671295
RMSE train: 0.803366	val: 1.507453	test: 1.557928
MAE train: 0.580870	val: 1.224085	test: 1.234165

Epoch: 32
Loss: 0.8357199430465698
RMSE train: 0.819180	val: 1.541223	test: 1.587075
MAE train: 0.600701	val: 1.265979	test: 1.256767

Epoch: 33
Loss: 0.76444873213768
RMSE train: 0.720997	val: 1.142807	test: 1.225673
MAE train: 0.547487	val: 0.880618	test: 0.937005

Epoch: 34
Loss: 0.6661924719810486
RMSE train: 0.741203	val: 1.261845	test: 1.360872
MAE train: 0.574747	val: 0.998843	test: 1.067576

Epoch: 35
Loss: 0.7257803529500961
RMSE train: 0.696249	val: 1.262163	test: 1.350204
MAE train: 0.525552	val: 0.991131	test: 1.066915

Epoch: 36
Loss: 0.568248026072979
RMSE train: 0.598984	val: 1.139342	test: 1.195740
MAE train: 0.436949	val: 0.875281	test: 0.951915

Epoch: 37
Loss: 0.6313688233494759
RMSE train: 0.606205	val: 1.128754	test: 1.169727
MAE train: 0.437350	val: 0.871134	test: 0.920738

Epoch: 38
Loss: 0.6664686352014542
RMSE train: 0.677855	val: 1.249037	test: 1.294273
MAE train: 0.485261	val: 0.992607	test: 1.031533

Epoch: 39
Loss: 0.6459374949336052
RMSE train: 0.678741	val: 1.287062	test: 1.290756
MAE train: 0.499086	val: 0.992805	test: 1.032460

Epoch: 40
Loss: 0.7033569142222404
RMSE train: 0.617334	val: 1.222075	test: 1.199420
MAE train: 0.437306	val: 0.917950	test: 0.949682

Epoch: 41
Loss: 0.47640398889780045
RMSE train: 0.604513	val: 1.223238	test: 1.210560
MAE train: 0.439710	val: 0.936311	test: 0.967304

Epoch: 42
Loss: 0.5268451273441315
RMSE train: 0.657232	val: 1.278371	test: 1.291086
MAE train: 0.482561	val: 1.003983	test: 1.037897

Epoch: 43
Loss: 0.7060973793268204
RMSE train: 0.661256	val: 1.287238	test: 1.298705
MAE train: 0.480146	val: 1.000216	test: 1.040396

Epoch: 44
Loss: 0.7277943715453148
RMSE train: 0.583767	val: 1.234357	test: 1.240020
MAE train: 0.420455	val: 0.938547	test: 0.994939

Epoch: 45
Loss: 0.5449303984642029
RMSE train: 0.495617	val: 1.182111	test: 1.175294
MAE train: 0.363983	val: 0.886931	test: 0.947325

Epoch: 46
Loss: 0.6854442358016968
RMSE train: 0.493910	val: 1.159509	test: 1.154521
MAE train: 0.360658	val: 0.890919	test: 0.910257

Epoch: 47
Loss: 0.9592671319842339
RMSE train: 0.574603	val: 1.222137	test: 1.220046
MAE train: 0.413119	val: 0.947715	test: 0.977066

Epoch: 48
Loss: 0.6246546059846878
RMSE train: 0.608370	val: 1.248157	test: 1.254623
MAE train: 0.435353	val: 0.945313	test: 1.009267

Epoch: 49
Loss: 0.7090562582015991
RMSE train: 0.618812	val: 1.157686	test: 1.174961
MAE train: 0.441384	val: 0.879778	test: 0.942084

Epoch: 50
Loss: 0.6333666443824768
RMSE train: 0.595890	val: 1.206612	test: 1.218053
MAE train: 0.438911	val: 0.935898	test: 0.977512

Epoch: 51
Loss: 0.6057302579283714
RMSE train: 0.567287	val: 1.231234	test: 1.250874
MAE train: 0.419719	val: 0.960673	test: 0.988326

Epoch: 52
Loss: 0.4544445127248764
RMSE train: 0.632711	val: 1.298706	test: 1.341823
MAE train: 0.480329	val: 1.034824	test: 1.067071

Epoch: 53
Loss: 0.6552238315343857
RMSE train: 0.600375	val: 1.203813	test: 1.253512
MAE train: 0.442488	val: 0.953100	test: 0.991888

Epoch: 54
Loss: 0.5506736040115356
RMSE train: 0.604794	val: 1.172408	test: 1.206983
MAE train: 0.441519	val: 0.908753	test: 0.953420

Epoch: 55
Loss: 0.5198337286710739
RMSE train: 0.644840	val: 1.278011	test: 1.311751
MAE train: 0.479517	val: 0.997702	test: 1.050345

Epoch: 56
Loss: 0.6089418083429337
RMSE train: 0.516562	val: 1.213768	test: 1.243958
MAE train: 0.385492	val: 0.931359	test: 1.003699

Epoch: 57
Loss: 0.5550646260380745
RMSE train: 0.539531	val: 1.183744	test: 1.287595
MAE train: 0.403810	val: 0.887611	test: 1.001556

Epoch: 58
Loss: 0.5514537617564201
RMSE train: 0.562140	val: 1.127839	test: 1.181248
MAE train: 0.407729	val: 0.886342	test: 0.934424

Epoch: 59
Loss: 0.8970151096582413
RMSE train: 0.657402	val: 1.255959	test: 1.333548
MAE train: 0.474401	val: 0.991150	test: 1.055201

Epoch: 60
Loss: 0.4546019732952118
RMSE train: 0.638528	val: 1.236113	test: 1.290098
MAE train: 0.460989	val: 0.974227	test: 1.023041

Epoch: 61
Loss: 0.47450263798236847
RMSE train: 0.586248	val: 1.190299	test: 1.229039
MAE train: 0.424620	val: 0.922033	test: 0.970782

Epoch: 62
Loss: 0.45999472588300705
RMSE train: 0.575328	val: 1.157194	test: 1.180174
MAE train: 0.423397	val: 0.891952	test: 0.937111

Epoch: 63
Loss: 0.48110131919384
RMSE train: 0.665249	val: 1.184241	test: 1.203169
MAE train: 0.484853	val: 0.930492	test: 0.957570

Epoch: 64
Loss: 0.4834192991256714
RMSE train: 0.615436	val: 1.188642	test: 1.206892
MAE train: 0.448347	val: 0.948482	test: 0.948538

Epoch: 65
Loss: 0.43497905135154724
RMSE train: 0.531874	val: 1.148762	test: 1.162878
MAE train: 0.387630	val: 0.904453	test: 0.922225

Epoch: 66
Loss: 0.48127229511737823
RMSE train: 0.534397	val: 1.126941	test: 1.141132
MAE train: 0.386620	val: 0.884676	test: 0.903961

Epoch: 67
Loss: 0.45721253007650375
RMSE train: 0.559250	val: 1.136483	test: 1.151646
MAE train: 0.412888	val: 0.891245	test: 0.918434

Epoch: 68
Loss: 0.569098711013794
RMSE train: 0.561141	val: 1.149454	test: 1.160859
MAE train: 0.419687	val: 0.901935	test: 0.920865

Epoch: 69
Loss: 0.4237094447016716
RMSE train: 0.530311	val: 1.133854	test: 1.152938
MAE train: 0.395013	val: 0.898395	test: 0.909411

Epoch: 70
Loss: 0.6583466082811356
RMSE train: 0.531396	val: 1.148700	test: 1.148056
MAE train: 0.395239	val: 0.898013	test: 0.903654

Epoch: 71
Loss: 0.42454271018505096
RMSE train: 0.484781	val: 1.086647	test: 1.073404
MAE train: 0.358797	val: 0.834656	test: 0.844261

Epoch: 72
Loss: 0.6473023146390915
RMSE train: 0.488837	val: 1.081656	test: 1.088555
MAE train: 0.362254	val: 0.828804	test: 0.847887

Epoch: 73
Loss: 0.44851014018058777
RMSE train: 0.521655	val: 1.137947	test: 1.160781
MAE train: 0.380577	val: 0.882726	test: 0.918244

Epoch: 74
Loss: 0.4889756292104721
RMSE train: 0.572289	val: 1.212543	test: 1.239933
MAE train: 0.408777	val: 0.936796	test: 0.998204

Epoch: 75
Loss: 0.5363468155264854
RMSE train: 0.563372	val: 1.196730	test: 1.227590
MAE train: 0.395744	val: 0.918293	test: 0.996388

Epoch: 76
Loss: 0.4036332443356514
RMSE train: 0.544640	val: 1.147640	test: 1.139735
MAE train: 0.385905	val: 0.890943	test: 0.923604

Epoch: 77
Loss: 0.46194881200790405
RMSE train: 0.498828	val: 1.131816	test: 1.096240
MAE train: 0.349958	val: 0.875420	test: 0.860686

Epoch: 78
Loss: 0.6893900409340858
RMSE train: 0.508449	val: 1.156966	test: 1.134403
MAE train: 0.364735	val: 0.889500	test: 0.890981

Epoch: 79
Loss: 0.5153010934591293
RMSE train: 0.575203	val: 1.205236	test: 1.207461
MAE train: 0.414471	val: 0.928746	test: 0.955585

Epoch: 80
Loss: 0.5890288203954697
RMSE train: 0.612994	val: 1.201656	test: 1.207951
MAE train: 0.450922	val: 0.954871	test: 0.963313

Epoch: 81
Loss: 0.43567781895399094
RMSE train: 0.707790	val: 1.285684	test: 1.331896
MAE train: 0.521488	val: 1.021587	test: 1.062358

Epoch: 82
Loss: 0.47490110248327255
RMSE train: 0.592046	val: 1.231451	test: 1.280075
MAE train: 0.431549	val: 0.976061	test: 1.018483

Epoch: 83
Loss: 0.6694408059120178
RMSE train: 0.551542	val: 1.164957	test: 1.198021
MAE train: 0.416022	val: 0.911632	test: 0.960795
RMSE train: 0.767250	val: 1.235747	test: 1.411489
MAE train: 0.573778	val: 0.966072	test: 1.088555

Epoch: 24
Loss: 0.9596510827541351
RMSE train: 0.817135	val: 1.308805	test: 1.446800
MAE train: 0.613204	val: 1.072313	test: 1.135929

Epoch: 25
Loss: 1.2686454504728317
RMSE train: 0.821849	val: 1.300479	test: 1.416031
MAE train: 0.601281	val: 1.062659	test: 1.134555

Epoch: 26
Loss: 0.8225841522216797
RMSE train: 0.734205	val: 1.260431	test: 1.341701
MAE train: 0.542013	val: 1.021370	test: 1.067501

Epoch: 27
Loss: 0.9812883138656616
RMSE train: 0.812123	val: 1.251860	test: 1.322884
MAE train: 0.605433	val: 1.010854	test: 1.052450

Epoch: 28
Loss: 0.6713304966688156
RMSE train: 0.701682	val: 1.149181	test: 1.256317
MAE train: 0.520509	val: 0.911571	test: 1.000640

Epoch: 29
Loss: 0.788859024643898
RMSE train: 0.688295	val: 1.238800	test: 1.359564
MAE train: 0.506960	val: 0.976425	test: 1.094684

Epoch: 30
Loss: 0.6568242609500885
RMSE train: 0.639904	val: 1.164856	test: 1.264295
MAE train: 0.465676	val: 0.890100	test: 1.007655

Epoch: 31
Loss: 0.8190007954835892
RMSE train: 0.597467	val: 1.146120	test: 1.231569
MAE train: 0.435846	val: 0.860554	test: 0.965797

Epoch: 32
Loss: 0.6638243347406387
RMSE train: 0.699712	val: 1.322975	test: 1.386361
MAE train: 0.530413	val: 1.050458	test: 1.099083

Epoch: 33
Loss: 0.7915859371423721
RMSE train: 0.706102	val: 1.361784	test: 1.430206
MAE train: 0.540464	val: 1.089471	test: 1.146579

Epoch: 34
Loss: 0.7172441780567169
RMSE train: 0.637963	val: 1.263458	test: 1.345127
MAE train: 0.469493	val: 0.980259	test: 1.075155

Epoch: 35
Loss: 0.5173884332180023
RMSE train: 0.667522	val: 1.250615	test: 1.310011
MAE train: 0.498867	val: 0.977165	test: 1.050517

Epoch: 36
Loss: 0.5499365329742432
RMSE train: 0.602285	val: 1.192391	test: 1.237529
MAE train: 0.438865	val: 0.920044	test: 0.990457

Epoch: 37
Loss: 0.7440296337008476
RMSE train: 0.577539	val: 1.182201	test: 1.197784
MAE train: 0.424308	val: 0.885806	test: 0.951105

Epoch: 38
Loss: 0.5513061285018921
RMSE train: 0.590083	val: 1.217743	test: 1.226884
MAE train: 0.433414	val: 0.886501	test: 0.964283

Epoch: 39
Loss: 0.6686091795563698
RMSE train: 0.582765	val: 1.246675	test: 1.325197
MAE train: 0.444558	val: 0.898089	test: 1.045313

Epoch: 40
Loss: 0.5418147817254066
RMSE train: 0.586358	val: 1.199257	test: 1.232909
MAE train: 0.441968	val: 0.910622	test: 0.985720

Epoch: 41
Loss: 0.5662822723388672
RMSE train: 0.716279	val: 1.256521	test: 1.332222
MAE train: 0.546516	val: 0.981544	test: 1.062106

Epoch: 42
Loss: 0.608879916369915
RMSE train: 0.626502	val: 1.155791	test: 1.193750
MAE train: 0.466628	val: 0.896356	test: 0.941870

Epoch: 43
Loss: 0.6040001735091209
RMSE train: 0.716473	val: 1.266245	test: 1.305678
MAE train: 0.524194	val: 1.003434	test: 1.036012

Epoch: 44
Loss: 0.5237822160124779
RMSE train: 0.660169	val: 1.222480	test: 1.269495
MAE train: 0.481769	val: 0.957116	test: 1.006519

Epoch: 45
Loss: 0.5887686163187027
RMSE train: 0.696297	val: 1.322133	test: 1.371260
MAE train: 0.519138	val: 1.051324	test: 1.098932

Epoch: 46
Loss: 0.5275173783302307
RMSE train: 0.784606	val: 1.397159	test: 1.468363
MAE train: 0.594232	val: 1.127198	test: 1.184147

Epoch: 47
Loss: 0.6681860685348511
RMSE train: 0.642787	val: 1.229237	test: 1.347218
MAE train: 0.464338	val: 0.975895	test: 1.073420

Epoch: 48
Loss: 0.5016286000609398
RMSE train: 0.522723	val: 1.123642	test: 1.182634
MAE train: 0.372770	val: 0.869946	test: 0.939240

Epoch: 49
Loss: 0.400937732309103
RMSE train: 0.552382	val: 1.190213	test: 1.228009
MAE train: 0.407676	val: 0.927645	test: 0.977911

Epoch: 50
Loss: 0.5064228773117065
RMSE train: 0.616942	val: 1.266139	test: 1.317536
MAE train: 0.448713	val: 0.999987	test: 1.050882

Epoch: 51
Loss: 0.7064715549349785
RMSE train: 0.569690	val: 1.220691	test: 1.271684
MAE train: 0.405847	val: 0.956069	test: 1.004015

Epoch: 52
Loss: 0.5754396468400955
RMSE train: 0.597288	val: 1.269247	test: 1.319675
MAE train: 0.437759	val: 1.020807	test: 1.039381

Epoch: 53
Loss: 0.612477645277977
RMSE train: 0.562185	val: 1.172824	test: 1.224051
MAE train: 0.401471	val: 0.932214	test: 0.965408

Epoch: 54
Loss: 0.47996337711811066
RMSE train: 0.616397	val: 1.222462	test: 1.273458
MAE train: 0.456694	val: 0.969764	test: 1.018089

Epoch: 55
Loss: 0.6364864036440849
RMSE train: 0.501017	val: 1.115593	test: 1.209824
MAE train: 0.377250	val: 0.836219	test: 0.982665

Epoch: 56
Loss: 0.5766789019107819
RMSE train: 0.560355	val: 1.093299	test: 1.260751
MAE train: 0.428336	val: 0.802483	test: 0.991787

Epoch: 57
Loss: 0.7692356705665588
RMSE train: 0.523307	val: 1.178795	test: 1.335735
MAE train: 0.380626	val: 0.894476	test: 1.067092

Epoch: 58
Loss: 0.6772434711456299
RMSE train: 0.514463	val: 1.163651	test: 1.312865
MAE train: 0.373040	val: 0.881269	test: 1.047179

Epoch: 59
Loss: 0.6597557961940765
RMSE train: 0.610600	val: 1.201372	test: 1.337736
MAE train: 0.441445	val: 0.925238	test: 1.047408

Epoch: 60
Loss: 0.49899056553840637
RMSE train: 0.646935	val: 1.182137	test: 1.283302
MAE train: 0.469978	val: 0.901661	test: 1.014296

Epoch: 61
Loss: 0.465903639793396
RMSE train: 0.530151	val: 1.093446	test: 1.176509
MAE train: 0.389035	val: 0.834874	test: 0.940408

Epoch: 62
Loss: 0.4165831506252289
RMSE train: 0.468933	val: 1.087304	test: 1.228731
MAE train: 0.352605	val: 0.821415	test: 0.978688

Epoch: 63
Loss: 0.5365031883120537
RMSE train: 0.518666	val: 1.134476	test: 1.247571
MAE train: 0.383075	val: 0.870790	test: 0.995956

Epoch: 64
Loss: 0.4637501910328865
RMSE train: 0.564187	val: 1.132503	test: 1.240378
MAE train: 0.422875	val: 0.876153	test: 0.999555

Epoch: 65
Loss: 0.5083307847380638
RMSE train: 0.569084	val: 1.138753	test: 1.203370
MAE train: 0.414004	val: 0.905963	test: 0.966690

Epoch: 66
Loss: 0.4655119478702545
RMSE train: 0.618123	val: 1.217524	test: 1.278803
MAE train: 0.446715	val: 0.971847	test: 1.024021

Epoch: 67
Loss: 0.4379434958100319
RMSE train: 0.529723	val: 1.153338	test: 1.198270
MAE train: 0.382497	val: 0.909166	test: 0.956236

Epoch: 68
Loss: 0.4151662811636925
RMSE train: 0.486261	val: 1.114112	test: 1.208003
MAE train: 0.356330	val: 0.857245	test: 0.967609

Epoch: 69
Loss: 0.8038237988948822
RMSE train: 0.517719	val: 1.078295	test: 1.175163
MAE train: 0.369982	val: 0.840727	test: 0.942511

Epoch: 70
Loss: 0.49004678428173065
RMSE train: 0.615971	val: 1.196420	test: 1.283582
MAE train: 0.449190	val: 0.955599	test: 1.020392

Epoch: 71
Loss: 0.47330452501773834
RMSE train: 0.591479	val: 1.126094	test: 1.200812
MAE train: 0.429836	val: 0.890540	test: 0.947196

Epoch: 72
Loss: 0.4393041208386421
RMSE train: 0.479645	val: 1.032059	test: 1.143373
MAE train: 0.367798	val: 0.753498	test: 0.913126

Epoch: 73
Loss: 0.45524127036333084
RMSE train: 0.483971	val: 1.074612	test: 1.185430
MAE train: 0.367668	val: 0.769462	test: 0.946430

Epoch: 74
Loss: 0.610342837870121
RMSE train: 0.511273	val: 1.123734	test: 1.187046
MAE train: 0.384697	val: 0.859217	test: 0.944859

Epoch: 75
Loss: 0.4474445730447769
RMSE train: 0.560376	val: 1.199433	test: 1.265474
MAE train: 0.405030	val: 0.925062	test: 0.986869

Epoch: 76
Loss: 0.4518294036388397
RMSE train: 0.502933	val: 1.086430	test: 1.123737
MAE train: 0.372792	val: 0.839739	test: 0.876751

Epoch: 77
Loss: 0.5293606445193291
RMSE train: 0.462297	val: 1.018710	test: 1.087443
MAE train: 0.334310	val: 0.765304	test: 0.867462

Epoch: 78
Loss: 0.3995952382683754
RMSE train: 0.465456	val: 1.057035	test: 1.180369
MAE train: 0.334842	val: 0.771070	test: 0.949972

Epoch: 79
Loss: 0.3366900309920311
RMSE train: 0.518089	val: 1.101498	test: 1.215395
MAE train: 0.367343	val: 0.856285	test: 0.963564

Epoch: 80
Loss: 0.44109439104795456
RMSE train: 0.512232	val: 1.082833	test: 1.186720
MAE train: 0.368300	val: 0.862097	test: 0.937150

Epoch: 81
Loss: 0.36624157428741455
RMSE train: 0.479224	val: 1.047035	test: 1.152684
MAE train: 0.343328	val: 0.827778	test: 0.916534

Epoch: 82
Loss: 0.4196728318929672
RMSE train: 0.457694	val: 1.016014	test: 1.098700
MAE train: 0.324757	val: 0.796502	test: 0.882625

Epoch: 83
Loss: 0.3886343911290169
RMSE train: 0.448776	val: 1.003849	test: 1.090752
MAE train: 0.330527	val: 0.749289	test: 0.864292
RMSE train: 0.685194	val: 1.234222	test: 1.330621
MAE train: 0.503237	val: 1.025583	test: 1.037665

Epoch: 24
Loss: 0.638629674911499
RMSE train: 0.616435	val: 1.208846	test: 1.210425
MAE train: 0.463634	val: 0.986712	test: 0.921634

Epoch: 25
Loss: 0.6160839051008224
RMSE train: 0.650013	val: 1.228431	test: 1.299814
MAE train: 0.474936	val: 1.008682	test: 0.999671

Epoch: 26
Loss: 0.5970361977815628
RMSE train: 0.642466	val: 1.114599	test: 1.222568
MAE train: 0.464242	val: 0.938073	test: 0.942943

Epoch: 27
Loss: 0.5718629956245422
RMSE train: 0.676451	val: 1.347130	test: 1.279265
MAE train: 0.508446	val: 1.090448	test: 1.015455

Epoch: 28
Loss: 0.5636232197284698
RMSE train: 0.722071	val: 1.384462	test: 1.316837
MAE train: 0.534732	val: 1.129468	test: 1.039743

Epoch: 29
Loss: 0.5327185764908791
RMSE train: 0.743673	val: 1.355773	test: 1.349515
MAE train: 0.550307	val: 1.134529	test: 1.055300

Epoch: 30
Loss: 0.566823273897171
RMSE train: 0.548481	val: 1.109942	test: 1.112633
MAE train: 0.405600	val: 0.919236	test: 0.873957

Epoch: 31
Loss: 0.5635622143745422
RMSE train: 0.523200	val: 1.149514	test: 1.118390
MAE train: 0.390259	val: 0.937329	test: 0.875756

Epoch: 32
Loss: 0.5002964660525322
RMSE train: 0.575952	val: 1.085639	test: 1.127792
MAE train: 0.422099	val: 0.901995	test: 0.872964

Epoch: 33
Loss: 0.48129483312368393
RMSE train: 0.526133	val: 1.039693	test: 1.108687
MAE train: 0.378146	val: 0.874332	test: 0.838866

Epoch: 34
Loss: 0.48060590773820877
RMSE train: 0.542413	val: 1.197420	test: 1.133763
MAE train: 0.397119	val: 0.968790	test: 0.854179

Epoch: 35
Loss: 0.5916124880313873
RMSE train: 0.490084	val: 1.070815	test: 1.104745
MAE train: 0.366389	val: 0.888754	test: 0.837380

Epoch: 36
Loss: 0.41965217143297195
RMSE train: 0.515585	val: 1.099547	test: 1.130983
MAE train: 0.386261	val: 0.912825	test: 0.857147

Epoch: 37
Loss: 0.43637991696596146
RMSE train: 0.536410	val: 1.136551	test: 1.154826
MAE train: 0.396226	val: 0.926133	test: 0.883991

Epoch: 38
Loss: 0.40384070575237274
RMSE train: 0.494313	val: 1.029050	test: 1.092377
MAE train: 0.355120	val: 0.844363	test: 0.842219

Epoch: 39
Loss: 0.46020546555519104
RMSE train: 0.544595	val: 1.092120	test: 1.156639
MAE train: 0.392616	val: 0.894207	test: 0.897524

Epoch: 40
Loss: 0.44354598969221115
RMSE train: 0.519355	val: 1.125899	test: 1.158043
MAE train: 0.376532	val: 0.906071	test: 0.897944

Epoch: 41
Loss: 0.46591177582740784
RMSE train: 0.507154	val: 1.109615	test: 1.125293
MAE train: 0.367583	val: 0.890169	test: 0.865855

Epoch: 42
Loss: 0.413505494594574
RMSE train: 0.524220	val: 1.127899	test: 1.124266
MAE train: 0.385088	val: 0.906224	test: 0.871030

Epoch: 43
Loss: 0.3899565190076828
RMSE train: 0.510612	val: 1.068864	test: 1.096516
MAE train: 0.370708	val: 0.862568	test: 0.847138

Epoch: 44
Loss: 0.40705590695142746
RMSE train: 0.534340	val: 1.134693	test: 1.131412
MAE train: 0.390115	val: 0.905267	test: 0.881362

Epoch: 45
Loss: 0.4027582257986069
RMSE train: 0.497277	val: 1.146587	test: 1.123590
MAE train: 0.366746	val: 0.909321	test: 0.875391

Epoch: 46
Loss: 0.4140344634652138
RMSE train: 0.463859	val: 1.143035	test: 1.119015
MAE train: 0.339589	val: 0.899443	test: 0.874438

Epoch: 47
Loss: 0.4135196954011917
RMSE train: 0.439468	val: 1.088329	test: 1.102151
MAE train: 0.326922	val: 0.874674	test: 0.865465

Epoch: 48
Loss: 0.37245114892721176
RMSE train: 0.465969	val: 1.144424	test: 1.120445
MAE train: 0.354855	val: 0.913407	test: 0.865358

Epoch: 49
Loss: 0.4091833606362343
RMSE train: 0.442601	val: 1.039970	test: 1.106252
MAE train: 0.337886	val: 0.852455	test: 0.849659

Epoch: 50
Loss: 0.3883367106318474
RMSE train: 0.530327	val: 1.160591	test: 1.147768
MAE train: 0.389311	val: 0.940524	test: 0.887408

Epoch: 51
Loss: 0.3857523277401924
RMSE train: 0.480089	val: 1.125733	test: 1.135086
MAE train: 0.349932	val: 0.924592	test: 0.863272

Epoch: 52
Loss: 0.38729412853717804
RMSE train: 0.469389	val: 1.157293	test: 1.134701
MAE train: 0.336286	val: 0.927084	test: 0.877894

Epoch: 53
Loss: 0.3385164961218834
RMSE train: 0.478182	val: 1.069688	test: 1.122368
MAE train: 0.340789	val: 0.858543	test: 0.881190

Epoch: 54
Loss: 0.3786485195159912
RMSE train: 0.489681	val: 1.067627	test: 1.131922
MAE train: 0.347698	val: 0.889139	test: 0.871062

Epoch: 55
Loss: 0.367729552090168
RMSE train: 0.558115	val: 1.126685	test: 1.162067
MAE train: 0.411713	val: 0.946638	test: 0.900000

Epoch: 56
Loss: 0.3503692075610161
RMSE train: 0.447367	val: 1.086788	test: 1.097443
MAE train: 0.329251	val: 0.892609	test: 0.842658

Epoch: 57
Loss: 0.3559047728776932
RMSE train: 0.502144	val: 1.203407	test: 1.169992
MAE train: 0.374619	val: 0.977004	test: 0.916188

Epoch: 58
Loss: 0.37520408630371094
RMSE train: 0.459544	val: 1.048768	test: 1.094342
MAE train: 0.338203	val: 0.874180	test: 0.835830

Epoch: 59
Loss: 0.3352629914879799
RMSE train: 0.441802	val: 1.022253	test: 1.065113
MAE train: 0.318856	val: 0.842513	test: 0.815719

Epoch: 60
Loss: 0.3248535767197609
RMSE train: 0.542069	val: 1.176437	test: 1.161916
MAE train: 0.392780	val: 0.940824	test: 0.914554

Epoch: 61
Loss: 0.35017332434654236
RMSE train: 0.474788	val: 1.092699	test: 1.155498
MAE train: 0.356289	val: 0.900247	test: 0.880959

Epoch: 62
Loss: 0.3570166975259781
RMSE train: 0.507202	val: 1.126191	test: 1.146550
MAE train: 0.369047	val: 0.917580	test: 0.894078

Epoch: 63
Loss: 0.39968911558389664
RMSE train: 0.488642	val: 1.060184	test: 1.115707
MAE train: 0.348465	val: 0.871880	test: 0.856240

Epoch: 64
Loss: 0.40633808821439743
RMSE train: 0.475725	val: 1.027438	test: 1.120084
MAE train: 0.342430	val: 0.856529	test: 0.840489

Epoch: 65
Loss: 0.3392692059278488
RMSE train: 0.561013	val: 1.234111	test: 1.188626
MAE train: 0.404102	val: 1.000752	test: 0.924044

Epoch: 66
Loss: 0.32327721267938614
RMSE train: 0.462837	val: 1.073387	test: 1.106762
MAE train: 0.337087	val: 0.882828	test: 0.836384

Epoch: 67
Loss: 0.35452108830213547
RMSE train: 0.495243	val: 1.118022	test: 1.127629
MAE train: 0.358242	val: 0.917662	test: 0.866121

Epoch: 68
Loss: 0.33404378592967987
RMSE train: 0.493079	val: 1.107521	test: 1.119101
MAE train: 0.359513	val: 0.907392	test: 0.850080

Epoch: 69
Loss: 0.3102487549185753
RMSE train: 0.406775	val: 1.084789	test: 1.100721
MAE train: 0.297410	val: 0.873527	test: 0.845345

Epoch: 70
Loss: 0.3442020118236542
RMSE train: 0.511096	val: 1.157050	test: 1.143409
MAE train: 0.360949	val: 0.930451	test: 0.887204

Epoch: 71
Loss: 0.3846842050552368
RMSE train: 0.419879	val: 1.012497	test: 1.074407
MAE train: 0.303171	val: 0.830631	test: 0.829278

Epoch: 72
Loss: 0.3205438405275345
RMSE train: 0.417450	val: 0.952199	test: 1.065676
MAE train: 0.301270	val: 0.807673	test: 0.820448

Epoch: 73
Loss: 0.3336811661720276
RMSE train: 0.536526	val: 1.158181	test: 1.195111
MAE train: 0.403162	val: 0.942853	test: 0.941910

Epoch: 74
Loss: 0.31379593163728714
RMSE train: 0.421752	val: 1.037414	test: 1.144240
MAE train: 0.317118	val: 0.861096	test: 0.869583

Epoch: 75
Loss: 0.3239217698574066
RMSE train: 0.481361	val: 1.016573	test: 1.140762
MAE train: 0.342502	val: 0.857932	test: 0.868928

Epoch: 76
Loss: 0.31379105895757675
RMSE train: 0.499497	val: 1.138015	test: 1.175242
MAE train: 0.365906	val: 0.932249	test: 0.908299

Epoch: 77
Loss: 0.32627487927675247
RMSE train: 0.389714	val: 1.086978	test: 1.074403
MAE train: 0.291750	val: 0.871769	test: 0.807673

Epoch: 78
Loss: 0.3299253433942795
RMSE train: 0.390139	val: 0.986357	test: 1.064246
MAE train: 0.285824	val: 0.818231	test: 0.816011

Epoch: 79
Loss: 0.3169870898127556
RMSE train: 0.402904	val: 1.041580	test: 1.090707
MAE train: 0.291348	val: 0.859169	test: 0.840432

Epoch: 80
Loss: 0.33276626467704773
RMSE train: 0.415864	val: 1.032725	test: 1.101060
MAE train: 0.299065	val: 0.858496	test: 0.846249

Epoch: 81
Loss: 0.29517849534749985
RMSE train: 0.426495	val: 1.037915	test: 1.103155
MAE train: 0.301873	val: 0.856362	test: 0.859958

Epoch: 82
Loss: 0.31527668982744217
RMSE train: 0.407015	val: 1.159037	test: 1.187356
MAE train: 0.295099	val: 0.920284	test: 0.922600

Epoch: 83
Loss: 0.2963481768965721
RMSE train: 0.388506	val: 1.082011	test: 1.146852
MAE train: 0.293467	val: 0.871407	test: 0.880474
RMSE train: 0.826582	val: 1.471603	test: 1.492741
MAE train: 0.626202	val: 1.200572	test: 1.186701

Epoch: 24
Loss: 0.6514603197574615
RMSE train: 0.684911	val: 1.134689	test: 1.210351
MAE train: 0.514214	val: 0.943104	test: 0.923544

Epoch: 25
Loss: 0.5960458815097809
RMSE train: 0.776236	val: 1.279454	test: 1.377980
MAE train: 0.572568	val: 1.048538	test: 1.087340

Epoch: 26
Loss: 0.5447699502110481
RMSE train: 0.628916	val: 1.156170	test: 1.207041
MAE train: 0.455612	val: 0.934927	test: 0.933166

Epoch: 27
Loss: 0.6141938418149948
RMSE train: 0.694516	val: 1.313008	test: 1.294150
MAE train: 0.520189	val: 1.058223	test: 1.002342

Epoch: 28
Loss: 0.5342512130737305
RMSE train: 0.787107	val: 1.289650	test: 1.349435
MAE train: 0.579013	val: 1.075569	test: 1.053299

Epoch: 29
Loss: 0.5306269079446793
RMSE train: 0.618207	val: 1.207903	test: 1.164254
MAE train: 0.457768	val: 0.971512	test: 0.911948

Epoch: 30
Loss: 0.489237017929554
RMSE train: 0.675906	val: 1.406904	test: 1.290318
MAE train: 0.510496	val: 1.114963	test: 1.027496

Epoch: 31
Loss: 0.5237320810556412
RMSE train: 0.544936	val: 1.155230	test: 1.142852
MAE train: 0.395833	val: 0.946732	test: 0.868165

Epoch: 32
Loss: 0.4812147915363312
RMSE train: 0.509326	val: 1.101623	test: 1.102785
MAE train: 0.374612	val: 0.894042	test: 0.851454

Epoch: 33
Loss: 0.4870154410600662
RMSE train: 0.651587	val: 1.314395	test: 1.293267
MAE train: 0.485898	val: 1.054172	test: 1.025505

Epoch: 34
Loss: 0.45449715107679367
RMSE train: 0.583808	val: 1.143977	test: 1.206978
MAE train: 0.425738	val: 0.938545	test: 0.929153

Epoch: 35
Loss: 0.4934875890612602
RMSE train: 0.550598	val: 1.150840	test: 1.183117
MAE train: 0.406735	val: 0.938049	test: 0.908817

Epoch: 36
Loss: 0.47449204325675964
RMSE train: 0.538815	val: 1.170570	test: 1.160950
MAE train: 0.390811	val: 0.957346	test: 0.899964

Epoch: 37
Loss: 0.4534979313611984
RMSE train: 0.532040	val: 1.054010	test: 1.102516
MAE train: 0.377466	val: 0.867238	test: 0.841505

Epoch: 38
Loss: 0.46676047146320343
RMSE train: 0.577028	val: 1.149221	test: 1.172382
MAE train: 0.414680	val: 0.932905	test: 0.907103

Epoch: 39
Loss: 0.4402904659509659
RMSE train: 0.659425	val: 1.291221	test: 1.301493
MAE train: 0.495522	val: 1.042018	test: 1.015994

Epoch: 40
Loss: 0.4535802751779556
RMSE train: 0.578721	val: 1.071108	test: 1.148310
MAE train: 0.428536	val: 0.888415	test: 0.873903

Epoch: 41
Loss: 0.43474871665239334
RMSE train: 0.647185	val: 1.203477	test: 1.205253
MAE train: 0.486169	val: 0.968184	test: 0.942309

Epoch: 42
Loss: 0.45792099833488464
RMSE train: 0.632463	val: 1.302519	test: 1.265482
MAE train: 0.471803	val: 1.034729	test: 1.001436

Epoch: 43
Loss: 0.4488120302557945
RMSE train: 0.553223	val: 1.072382	test: 1.109703
MAE train: 0.411334	val: 0.869192	test: 0.860779

Epoch: 44
Loss: 0.42889145761728287
RMSE train: 0.528073	val: 1.148596	test: 1.142675
MAE train: 0.383901	val: 0.921970	test: 0.872857

Epoch: 45
Loss: 0.5052260756492615
RMSE train: 0.522138	val: 1.169635	test: 1.173572
MAE train: 0.377855	val: 0.932564	test: 0.900499

Epoch: 46
Loss: 0.42357875406742096
RMSE train: 0.479624	val: 1.135459	test: 1.116327
MAE train: 0.356909	val: 0.895945	test: 0.867578

Epoch: 47
Loss: 0.458615206182003
RMSE train: 0.507850	val: 1.069384	test: 1.129485
MAE train: 0.364849	val: 0.878419	test: 0.872196

Epoch: 48
Loss: 0.4548061713576317
RMSE train: 0.625932	val: 1.185414	test: 1.229338
MAE train: 0.452848	val: 0.954996	test: 0.964020

Epoch: 49
Loss: 0.4390347972512245
RMSE train: 0.521595	val: 1.037950	test: 1.123365
MAE train: 0.373931	val: 0.858020	test: 0.860008

Epoch: 50
Loss: 0.3820113018155098
RMSE train: 0.595422	val: 1.046535	test: 1.141508
MAE train: 0.432819	val: 0.855546	test: 0.878485

Epoch: 51
Loss: 0.3881646916270256
RMSE train: 0.565579	val: 1.109062	test: 1.156642
MAE train: 0.404952	val: 0.901579	test: 0.893135

Epoch: 52
Loss: 0.3915324956178665
RMSE train: 0.498256	val: 1.124227	test: 1.149600
MAE train: 0.355121	val: 0.892474	test: 0.885998

Epoch: 53
Loss: 0.3876183554530144
RMSE train: 0.471833	val: 1.029569	test: 1.125983
MAE train: 0.337119	val: 0.836931	test: 0.866163

Epoch: 54
Loss: 0.37947311997413635
RMSE train: 0.508731	val: 1.090637	test: 1.137934
MAE train: 0.363550	val: 0.885071	test: 0.881247

Epoch: 55
Loss: 0.3625832870602608
RMSE train: 0.429518	val: 1.084000	test: 1.140757
MAE train: 0.316699	val: 0.871410	test: 0.866813

Epoch: 56
Loss: 0.3508754074573517
RMSE train: 0.473709	val: 1.206494	test: 1.188166
MAE train: 0.345346	val: 0.955537	test: 0.923813

Epoch: 57
Loss: 0.3271386921405792
RMSE train: 0.461941	val: 1.113461	test: 1.119414
MAE train: 0.339440	val: 0.892065	test: 0.873259

Epoch: 58
Loss: 0.37070122361183167
RMSE train: 0.497294	val: 1.069444	test: 1.094612
MAE train: 0.356699	val: 0.870997	test: 0.851733

Epoch: 59
Loss: 0.37053554505109787
RMSE train: 0.495694	val: 1.089165	test: 1.098428
MAE train: 0.353360	val: 0.887008	test: 0.845832

Epoch: 60
Loss: 0.3612046241760254
RMSE train: 0.453613	val: 1.098751	test: 1.080668
MAE train: 0.332412	val: 0.885136	test: 0.830713

Epoch: 61
Loss: 0.36365488171577454
RMSE train: 0.443383	val: 1.055047	test: 1.055165
MAE train: 0.319492	val: 0.863249	test: 0.791580

Epoch: 62
Loss: 0.34680846333503723
RMSE train: 0.441215	val: 1.030162	test: 1.046964
MAE train: 0.321059	val: 0.839935	test: 0.783506

Epoch: 63
Loss: 0.39536329358816147
RMSE train: 0.522649	val: 1.090485	test: 1.102503
MAE train: 0.380361	val: 0.885272	test: 0.851043

Epoch: 64
Loss: 0.32443802803754807
RMSE train: 0.545449	val: 1.101471	test: 1.130209
MAE train: 0.397665	val: 0.910100	test: 0.866170

Epoch: 65
Loss: 0.337789885699749
RMSE train: 0.579197	val: 1.167280	test: 1.172664
MAE train: 0.419952	val: 0.940642	test: 0.914927

Epoch: 66
Loss: 0.3154606819152832
RMSE train: 0.533476	val: 1.183177	test: 1.158049
MAE train: 0.390973	val: 0.964890	test: 0.892564

Epoch: 67
Loss: 0.3749156668782234
RMSE train: 0.488071	val: 1.154209	test: 1.130242
MAE train: 0.362545	val: 0.907633	test: 0.856256

Epoch: 68
Loss: 0.36767738312482834
RMSE train: 0.474196	val: 1.090620	test: 1.103104
MAE train: 0.339787	val: 0.877240	test: 0.827596

Epoch: 69
Loss: 0.33765164762735367
RMSE train: 0.450436	val: 1.115532	test: 1.114203
MAE train: 0.331634	val: 0.892287	test: 0.819024

Epoch: 70
Loss: 0.3536125719547272
RMSE train: 0.420756	val: 1.179023	test: 1.085349
MAE train: 0.303624	val: 0.911683	test: 0.828455

Epoch: 71
Loss: 0.34991125017404556
RMSE train: 0.451667	val: 1.024987	test: 1.086113
MAE train: 0.318241	val: 0.842484	test: 0.815983

Epoch: 72
Loss: 0.3472508415579796
RMSE train: 0.457136	val: 1.123706	test: 1.115203
MAE train: 0.323723	val: 0.906437	test: 0.849504

Epoch: 73
Loss: 0.31577426940202713
RMSE train: 0.482267	val: 1.234059	test: 1.177043
MAE train: 0.354978	val: 0.978192	test: 0.912956

Epoch: 74
Loss: 0.3159855306148529
RMSE train: 0.515606	val: 1.171625	test: 1.175392
MAE train: 0.376215	val: 0.953212	test: 0.900614

Epoch: 75
Loss: 0.3339818865060806
RMSE train: 0.534701	val: 1.106913	test: 1.139249
MAE train: 0.384259	val: 0.887090	test: 0.876365

Epoch: 76
Loss: 0.30897194892168045
RMSE train: 0.419113	val: 1.050572	test: 1.088908
MAE train: 0.302802	val: 0.837695	test: 0.807237

Epoch: 77
Loss: 0.33023621141910553
RMSE train: 0.491783	val: 1.119369	test: 1.129508
MAE train: 0.359711	val: 0.901901	test: 0.876255

Epoch: 78
Loss: 0.3103535920381546
RMSE train: 0.467401	val: 1.065059	test: 1.094538
MAE train: 0.332501	val: 0.842004	test: 0.843497

Epoch: 79
Loss: 0.2891725152730942
RMSE train: 0.439054	val: 1.024693	test: 1.076456
MAE train: 0.315007	val: 0.826131	test: 0.799787

Epoch: 80
Loss: 0.32561831921339035
RMSE train: 0.482068	val: 1.184219	test: 1.130130
MAE train: 0.360891	val: 0.950815	test: 0.885633

Epoch: 81
Loss: 0.29296253621578217
RMSE train: 0.429980	val: 1.064590	test: 1.109208
MAE train: 0.315717	val: 0.862584	test: 0.813427

Epoch: 82
Loss: 0.30766355991363525
RMSE train: 0.468438	val: 1.102637	test: 1.115225
MAE train: 0.336681	val: 0.895815	test: 0.838192

Epoch: 83
Loss: 0.32413821667432785
RMSE train: 0.428962	val: 1.082795	test: 1.082065
MAE train: 0.310921	val: 0.885141	test: 0.818310
RMSE train: 0.888649	val: 1.494227	test: 1.502926
MAE train: 0.681070	val: 1.214091	test: 1.214177

Epoch: 24
Loss: 0.6636833399534225
RMSE train: 0.882624	val: 1.414980	test: 1.482688
MAE train: 0.663671	val: 1.176288	test: 1.193531

Epoch: 25
Loss: 0.6135902106761932
RMSE train: 0.838666	val: 1.287059	test: 1.371595
MAE train: 0.619649	val: 1.040148	test: 1.067294

Epoch: 26
Loss: 0.5754363387823105
RMSE train: 0.674763	val: 1.371166	test: 1.325631
MAE train: 0.488158	val: 1.084001	test: 1.026674

Epoch: 27
Loss: 0.5492372289299965
RMSE train: 0.679133	val: 1.316322	test: 1.280288
MAE train: 0.494723	val: 1.033334	test: 0.982188

Epoch: 28
Loss: 0.5873052626848221
RMSE train: 0.662625	val: 1.152963	test: 1.193011
MAE train: 0.474861	val: 0.924444	test: 0.909561

Epoch: 29
Loss: 0.5238503515720367
RMSE train: 0.700098	val: 1.418447	test: 1.322199
MAE train: 0.517446	val: 1.140523	test: 1.043862

Epoch: 30
Loss: 0.536556139588356
RMSE train: 0.585675	val: 1.297139	test: 1.186813
MAE train: 0.429891	val: 1.020937	test: 0.929315

Epoch: 31
Loss: 0.4637346416711807
RMSE train: 0.674436	val: 1.227906	test: 1.162988
MAE train: 0.483829	val: 0.962442	test: 0.907985

Epoch: 32
Loss: 0.46966134011745453
RMSE train: 0.547174	val: 1.177632	test: 1.128620
MAE train: 0.394854	val: 0.926524	test: 0.877851

Epoch: 33
Loss: 0.4925527200102806
RMSE train: 0.610702	val: 1.316578	test: 1.225215
MAE train: 0.445887	val: 1.022267	test: 0.977903

Epoch: 34
Loss: 0.5171602964401245
RMSE train: 0.697947	val: 1.251339	test: 1.219951
MAE train: 0.522876	val: 0.993298	test: 0.958350

Epoch: 35
Loss: 0.4516138434410095
RMSE train: 0.732302	val: 1.420166	test: 1.340215
MAE train: 0.546100	val: 1.144174	test: 1.085830

Epoch: 36
Loss: 0.4688068628311157
RMSE train: 0.631060	val: 1.260391	test: 1.180016
MAE train: 0.469733	val: 0.989345	test: 0.928622

Epoch: 37
Loss: 0.48140598833560944
RMSE train: 0.583267	val: 1.062388	test: 1.071124
MAE train: 0.439906	val: 0.862929	test: 0.804398

Epoch: 38
Loss: 0.4389621242880821
RMSE train: 0.661223	val: 1.218841	test: 1.217341
MAE train: 0.490298	val: 0.992594	test: 0.942802

Epoch: 39
Loss: 0.4393817186355591
RMSE train: 0.536074	val: 1.108448	test: 1.077887
MAE train: 0.389837	val: 0.891154	test: 0.806746

Epoch: 40
Loss: 0.4374612271785736
RMSE train: 0.507472	val: 1.137541	test: 1.084135
MAE train: 0.369363	val: 0.891672	test: 0.820922

Epoch: 41
Loss: 0.44335300475358963
RMSE train: 0.601686	val: 1.245084	test: 1.177330
MAE train: 0.438660	val: 0.983748	test: 0.924554

Epoch: 42
Loss: 0.4302210882306099
RMSE train: 0.558004	val: 1.159055	test: 1.140489
MAE train: 0.404882	val: 0.919669	test: 0.862835

Epoch: 43
Loss: 0.4169602394104004
RMSE train: 0.550723	val: 1.236851	test: 1.137612
MAE train: 0.410371	val: 0.961912	test: 0.886533

Epoch: 44
Loss: 0.4342064931988716
RMSE train: 0.493745	val: 1.119839	test: 1.106798
MAE train: 0.366713	val: 0.889146	test: 0.839700

Epoch: 45
Loss: 0.4405948966741562
RMSE train: 0.583230	val: 1.213895	test: 1.163996
MAE train: 0.432343	val: 0.959398	test: 0.908350

Epoch: 46
Loss: 0.49315714091062546
RMSE train: 0.547149	val: 1.102922	test: 1.102288
MAE train: 0.400929	val: 0.890613	test: 0.844406

Epoch: 47
Loss: 0.41192489862442017
RMSE train: 0.558969	val: 1.162891	test: 1.147451
MAE train: 0.406758	val: 0.926741	test: 0.885743

Epoch: 48
Loss: 0.44624821841716766
RMSE train: 0.563561	val: 1.188610	test: 1.160585
MAE train: 0.413065	val: 0.956938	test: 0.899374

Epoch: 49
Loss: 0.415498748421669
RMSE train: 0.454391	val: 1.086556	test: 1.059638
MAE train: 0.337127	val: 0.883331	test: 0.801236

Epoch: 50
Loss: 0.3613891452550888
RMSE train: 0.462172	val: 1.104752	test: 1.074867
MAE train: 0.345182	val: 0.889621	test: 0.816080

Epoch: 51
Loss: 0.3869744539260864
RMSE train: 0.525976	val: 1.058630	test: 1.077158
MAE train: 0.376188	val: 0.869253	test: 0.816178

Epoch: 52
Loss: 0.38950496166944504
RMSE train: 0.500792	val: 1.034677	test: 1.055994
MAE train: 0.355407	val: 0.851426	test: 0.800898

Epoch: 53
Loss: 0.4013936296105385
RMSE train: 0.480316	val: 1.047579	test: 1.074541
MAE train: 0.347004	val: 0.855862	test: 0.808190

Epoch: 54
Loss: 0.39048516005277634
RMSE train: 0.566027	val: 1.152204	test: 1.150370
MAE train: 0.413643	val: 0.933366	test: 0.887258

Epoch: 55
Loss: 0.37531641870737076
RMSE train: 0.550170	val: 1.134122	test: 1.106912
MAE train: 0.399529	val: 0.915890	test: 0.839641

Epoch: 56
Loss: 0.3620544746518135
RMSE train: 0.515374	val: 1.074787	test: 1.103087
MAE train: 0.366519	val: 0.881747	test: 0.813605

Epoch: 57
Loss: 0.35462161898612976
RMSE train: 0.572742	val: 1.187706	test: 1.172150
MAE train: 0.411878	val: 0.938870	test: 0.893624

Epoch: 58
Loss: 0.3851110562682152
RMSE train: 0.542083	val: 1.133029	test: 1.152715
MAE train: 0.391440	val: 0.895128	test: 0.882807

Epoch: 59
Loss: 0.3603895753622055
RMSE train: 0.613705	val: 1.143703	test: 1.193541
MAE train: 0.451845	val: 0.939479	test: 0.911381

Epoch: 60
Loss: 0.3377336487174034
RMSE train: 0.720666	val: 1.311682	test: 1.300644
MAE train: 0.534998	val: 1.069054	test: 1.027803

Epoch: 61
Loss: 0.34678662568330765
RMSE train: 0.462168	val: 1.115621	test: 1.109609
MAE train: 0.337117	val: 0.898857	test: 0.840427

Epoch: 62
Loss: 0.34738559275865555
RMSE train: 0.439244	val: 1.150883	test: 1.131409
MAE train: 0.316666	val: 0.917192	test: 0.855418

Epoch: 63
Loss: 0.3465765118598938
RMSE train: 0.549266	val: 1.135868	test: 1.136611
MAE train: 0.389911	val: 0.923076	test: 0.878890

Epoch: 64
Loss: 0.3709182068705559
RMSE train: 0.495484	val: 1.109198	test: 1.097207
MAE train: 0.354377	val: 0.887187	test: 0.828870

Epoch: 65
Loss: 0.31357933580875397
RMSE train: 0.516932	val: 1.146900	test: 1.132803
MAE train: 0.375243	val: 0.920029	test: 0.864762

Epoch: 66
Loss: 0.35213299095630646
RMSE train: 0.499581	val: 1.085788	test: 1.102122
MAE train: 0.365541	val: 0.882998	test: 0.829545

Epoch: 67
Loss: 0.4291499927639961
RMSE train: 0.482387	val: 1.103699	test: 1.081299
MAE train: 0.355193	val: 0.871342	test: 0.815972

Epoch: 68
Loss: 0.31367968767881393
RMSE train: 0.496616	val: 1.076730	test: 1.100039
MAE train: 0.359638	val: 0.859582	test: 0.839704

Epoch: 69
Loss: 0.3564816862344742
RMSE train: 0.489827	val: 1.101827	test: 1.134526
MAE train: 0.354162	val: 0.883092	test: 0.858186

Epoch: 70
Loss: 0.32711412012577057
RMSE train: 0.472524	val: 1.083274	test: 1.103917
MAE train: 0.343817	val: 0.872258	test: 0.839994

Epoch: 71
Loss: 0.32574988156557083
RMSE train: 0.458333	val: 1.033509	test: 1.089376
MAE train: 0.330664	val: 0.835782	test: 0.803284

Epoch: 72
Loss: 0.3400994949042797
RMSE train: 0.492983	val: 1.124688	test: 1.141255
MAE train: 0.351907	val: 0.899824	test: 0.845985

Epoch: 73
Loss: 0.32315464317798615
RMSE train: 0.589850	val: 1.254208	test: 1.225064
MAE train: 0.421438	val: 1.004254	test: 0.972770

Epoch: 74
Loss: 0.3035225421190262
RMSE train: 0.462330	val: 1.038340	test: 1.078129
MAE train: 0.332064	val: 0.853293	test: 0.812085

Epoch: 75
Loss: 0.3484344109892845
RMSE train: 0.531295	val: 1.208713	test: 1.172736
MAE train: 0.378834	val: 0.954941	test: 0.918862

Epoch: 76
Loss: 0.31385713070631027
RMSE train: 0.491960	val: 1.129461	test: 1.114493
MAE train: 0.353626	val: 0.891494	test: 0.861433

Epoch: 77
Loss: 0.2892223186790943
RMSE train: 0.450205	val: 1.025545	test: 1.044953
MAE train: 0.323033	val: 0.820062	test: 0.786742

Epoch: 78
Loss: 0.32818903774023056
RMSE train: 0.540015	val: 1.107850	test: 1.117474
MAE train: 0.386700	val: 0.894905	test: 0.854980

Epoch: 79
Loss: 0.3112626075744629
RMSE train: 0.442487	val: 1.103285	test: 1.106385
MAE train: 0.320334	val: 0.876591	test: 0.820302

Epoch: 80
Loss: 0.3011341467499733
RMSE train: 0.411505	val: 1.003544	test: 1.064068
MAE train: 0.294532	val: 0.816911	test: 0.780783

Epoch: 81
Loss: 0.35400207340717316
RMSE train: 0.471417	val: 1.042817	test: 1.090931
MAE train: 0.339674	val: 0.848364	test: 0.821811

Epoch: 82
Loss: 0.31430432200431824
RMSE train: 0.437368	val: 1.096440	test: 1.099593
MAE train: 0.318334	val: 0.882357	test: 0.818167

Epoch: 83
Loss: 0.32991723716259
RMSE train: 0.409897	val: 1.032601	test: 1.064821
MAE train: 0.299852	val: 0.839563	test: 0.784887

Epoch: 84
Loss: 0.3131263355414073
RMSE train: 0.422621	val: 1.348416	test: 1.581347
MAE train: 0.304146	val: 1.073089	test: 1.133312

Epoch: 85
Loss: 0.2723962863286336
RMSE train: 0.410687	val: 1.281158	test: 1.521625
MAE train: 0.287648	val: 1.012603	test: 1.093512

Epoch: 86
Loss: 0.2547647953033447
RMSE train: 0.371392	val: 1.271220	test: 1.584445
MAE train: 0.264240	val: 0.997750	test: 1.109197

Epoch: 87
Loss: 0.2763783633708954
RMSE train: 0.376193	val: 1.333060	test: 1.736526
MAE train: 0.271373	val: 1.044577	test: 1.170768

Epoch: 88
Loss: 0.2969820549090703
RMSE train: 0.415202	val: 1.370510	test: 1.765329
MAE train: 0.299393	val: 1.080442	test: 1.204703

Epoch: 89
Loss: 0.2726804514726003
RMSE train: 0.360172	val: 1.319171	test: 1.833789
MAE train: 0.259045	val: 1.018214	test: 1.188272

Epoch: 90
Loss: 0.32212849458058673
RMSE train: 0.370387	val: 1.314974	test: 1.884532
MAE train: 0.269795	val: 1.013222	test: 1.197851

Epoch: 91
Loss: 0.2449604719877243
RMSE train: 0.434870	val: 1.391943	test: 2.011756
MAE train: 0.324168	val: 1.071444	test: 1.267342

Epoch: 92
Loss: 0.26690126458803815
RMSE train: 0.372327	val: 1.392639	test: 2.048240
MAE train: 0.271288	val: 1.050990	test: 1.255659

Epoch: 93
Loss: 0.28505921363830566
RMSE train: 0.343542	val: 1.371451	test: 2.016750
MAE train: 0.245959	val: 1.015179	test: 1.222402

Epoch: 94
Loss: 0.2820454736550649
RMSE train: 0.451519	val: 1.363278	test: 1.846300
MAE train: 0.317812	val: 1.045270	test: 1.211071

Epoch: 95
Loss: 0.3110105097293854
RMSE train: 0.449853	val: 1.320330	test: 1.645452
MAE train: 0.308434	val: 1.023707	test: 1.137558

Epoch: 96
Loss: 0.25498555103937787
RMSE train: 0.381689	val: 1.288555	test: 1.627998
MAE train: 0.284943	val: 0.976741	test: 1.093010

Epoch: 97
Loss: 0.3060247798760732
RMSE train: 0.370835	val: 1.337032	test: 1.853759
MAE train: 0.275038	val: 1.019531	test: 1.195246

Epoch: 98
Loss: 0.2574079732100169
RMSE train: 0.404481	val: 1.370175	test: 1.923133
MAE train: 0.304333	val: 1.044474	test: 1.219578

Epoch: 99
Loss: 0.23928321401278177
RMSE train: 0.403118	val: 1.308907	test: 1.715102
MAE train: 0.296266	val: 1.019446	test: 1.138201

Epoch: 100
Loss: 0.27187328537305194
RMSE train: 0.464412	val: 1.312641	test: 1.613260
MAE train: 0.330544	val: 1.034342	test: 1.117080

Epoch: 101
Loss: 0.2694459656874339
RMSE train: 0.522408	val: 1.368076	test: 1.643574
MAE train: 0.366421	val: 1.068399	test: 1.142487

Epoch: 102
Loss: 0.3177482982476552
RMSE train: 0.423258	val: 1.348181	test: 1.663116
MAE train: 0.304301	val: 1.041928	test: 1.117353

Epoch: 103
Loss: 0.2442057877779007
RMSE train: 0.434708	val: 1.371445	test: 1.721218
MAE train: 0.314518	val: 1.059705	test: 1.159099

Epoch: 104
Loss: 0.25784532725811005
RMSE train: 0.464314	val: 1.376781	test: 1.713279
MAE train: 0.339000	val: 1.069344	test: 1.177105

Epoch: 105
Loss: 0.22742851078510284
RMSE train: 0.422676	val: 1.322346	test: 1.618799
MAE train: 0.305193	val: 1.032132	test: 1.121254

Epoch: 106
Loss: 0.23300703366597494
RMSE train: 0.353184	val: 1.292286	test: 1.624513
MAE train: 0.253822	val: 0.998738	test: 1.107199

Epoch: 107
Loss: 0.23892344534397125
RMSE train: 0.359440	val: 1.315317	test: 1.658749
MAE train: 0.259655	val: 1.014874	test: 1.134959

Epoch: 108
Loss: 0.27857252955436707
RMSE train: 0.372167	val: 1.360107	test: 1.830374
MAE train: 0.270844	val: 1.027069	test: 1.198201

Epoch: 109
Loss: 0.2545337527990341
RMSE train: 0.369933	val: 1.350933	test: 1.815661
MAE train: 0.269589	val: 1.016024	test: 1.175139

Epoch: 110
Loss: 0.26499536633491516
RMSE train: 0.363792	val: 1.332129	test: 1.748830
MAE train: 0.262582	val: 1.012256	test: 1.138087

Epoch: 111
Loss: 0.22040991485118866
RMSE train: 0.389556	val: 1.324149	test: 1.694318
MAE train: 0.278660	val: 1.027344	test: 1.142967

Epoch: 112
Loss: 0.2575593441724777
RMSE train: 0.371368	val: 1.307592	test: 1.709423
MAE train: 0.268081	val: 1.005871	test: 1.134189

Epoch: 113
Loss: 0.23005387683709463
RMSE train: 0.384395	val: 1.299037	test: 1.686657
MAE train: 0.278050	val: 1.001798	test: 1.131789

Epoch: 114
Loss: 0.23308329284191132
RMSE train: 0.401245	val: 1.314395	test: 1.615917
MAE train: 0.290694	val: 1.032280	test: 1.122838

Epoch: 115
Loss: 0.24735511342684427
RMSE train: 0.370223	val: 1.306090	test: 1.600378
MAE train: 0.268857	val: 1.026933	test: 1.108856

Epoch: 116
Loss: 0.24654118220011392
RMSE train: 0.434640	val: 1.305107	test: 1.656787
MAE train: 0.310114	val: 1.026827	test: 1.160387

Epoch: 117
Loss: 0.2283183087905248
RMSE train: 0.414968	val: 1.317763	test: 1.693242
MAE train: 0.296817	val: 1.025043	test: 1.175926

Epoch: 118
Loss: 0.23820599416891733
RMSE train: 0.346769	val: 1.317445	test: 1.645669
MAE train: 0.254928	val: 1.021985	test: 1.146615

Epoch: 119
Loss: 0.21911540627479553
RMSE train: 0.326648	val: 1.329744	test: 1.768865
MAE train: 0.239878	val: 1.022830	test: 1.191979

Epoch: 120
Loss: 0.22344600657622019
RMSE train: 0.373553	val: 1.352540	test: 1.784423
MAE train: 0.275626	val: 1.049207	test: 1.222249

Epoch: 121
Loss: 0.3013518850008647
RMSE train: 0.347181	val: 1.325836	test: 1.708352
MAE train: 0.255349	val: 1.021495	test: 1.181612

Early stopping
Best (RMSE):	 train: 0.371392	val: 1.271220	test: 1.584445
Best (MAE):	 train: 0.264240	val: 0.997750	test: 1.109197


Epoch: 84
Loss: 0.29166219631830853
RMSE train: 0.508321	val: 1.338760	test: 1.466437
MAE train: 0.352697	val: 1.095502	test: 1.123591

Epoch: 85
Loss: 0.29288620750109357
RMSE train: 0.462787	val: 1.310436	test: 1.488087
MAE train: 0.324355	val: 1.054914	test: 1.112494

Epoch: 86
Loss: 0.29633856813112897
RMSE train: 0.474479	val: 1.365445	test: 1.508824
MAE train: 0.334016	val: 1.110585	test: 1.139276

Epoch: 87
Loss: 0.3081321716308594
RMSE train: 0.520930	val: 1.437602	test: 1.582225
MAE train: 0.368429	val: 1.185865	test: 1.198103

Epoch: 88
Loss: 0.30574845274289447
RMSE train: 0.457281	val: 1.360413	test: 1.625226
MAE train: 0.321335	val: 1.081643	test: 1.180936

Epoch: 89
Loss: 0.31270599365234375
RMSE train: 0.469837	val: 1.345667	test: 1.530339
MAE train: 0.327572	val: 1.073989	test: 1.128681

Epoch: 90
Loss: 0.2897433936595917
RMSE train: 0.559758	val: 1.455432	test: 1.609937
MAE train: 0.394992	val: 1.208650	test: 1.220691

Epoch: 91
Loss: 0.2752388020356496
RMSE train: 0.520150	val: 1.423113	test: 1.629325
MAE train: 0.373198	val: 1.170606	test: 1.209181

Epoch: 92
Loss: 0.21802175045013428
RMSE train: 0.417046	val: 1.328627	test: 1.550909
MAE train: 0.293651	val: 1.066883	test: 1.114652

Epoch: 93
Loss: 0.30595750610033673
RMSE train: 0.468339	val: 1.337672	test: 1.573157
MAE train: 0.324645	val: 1.079819	test: 1.141311

Epoch: 94
Loss: 0.3170005679130554
RMSE train: 0.471006	val: 1.378181	test: 1.608601
MAE train: 0.329304	val: 1.122971	test: 1.193299

Epoch: 95
Loss: 0.26898851493994397
RMSE train: 0.354804	val: 1.291784	test: 1.684265
MAE train: 0.253837	val: 1.020647	test: 1.168504

Epoch: 96
Loss: 0.25021150211493176
RMSE train: 0.363266	val: 1.289851	test: 1.721084
MAE train: 0.262496	val: 1.015649	test: 1.181736

Epoch: 97
Loss: 0.29105378687381744
RMSE train: 0.413618	val: 1.294984	test: 1.611423
MAE train: 0.293214	val: 1.026756	test: 1.152384

Epoch: 98
Loss: 0.26362480719884235
RMSE train: 0.455020	val: 1.323136	test: 1.516435
MAE train: 0.318101	val: 1.068836	test: 1.133741

Epoch: 99
Loss: 0.28705944617589313
RMSE train: 0.394707	val: 1.297886	test: 1.522189
MAE train: 0.280360	val: 1.040894	test: 1.120152

Epoch: 100
Loss: 0.33376290400822956
RMSE train: 0.359818	val: 1.270331	test: 1.552815
MAE train: 0.255490	val: 1.001571	test: 1.116368

Epoch: 101
Loss: 0.241790180404981
RMSE train: 0.407787	val: 1.289788	test: 1.546829
MAE train: 0.288174	val: 1.023097	test: 1.121998

Epoch: 102
Loss: 0.2767785390218099
RMSE train: 0.421804	val: 1.340964	test: 1.528261
MAE train: 0.303627	val: 1.076815	test: 1.127761

Epoch: 103
Loss: 0.2479158639907837
RMSE train: 0.435379	val: 1.365537	test: 1.577981
MAE train: 0.315573	val: 1.103339	test: 1.158267

Epoch: 104
Loss: 0.26992567380269367
RMSE train: 0.378330	val: 1.303838	test: 1.587407
MAE train: 0.273755	val: 1.042053	test: 1.131018

Epoch: 105
Loss: 0.26138624052206677
RMSE train: 0.411144	val: 1.310123	test: 1.534251
MAE train: 0.295252	val: 1.046047	test: 1.113453

Epoch: 106
Loss: 0.2700945734977722
RMSE train: 0.553473	val: 1.417352	test: 1.595394
MAE train: 0.394776	val: 1.161376	test: 1.200737

Epoch: 107
Loss: 0.24555642902851105
RMSE train: 0.430339	val: 1.352062	test: 1.513043
MAE train: 0.307661	val: 1.093435	test: 1.129301

Epoch: 108
Loss: 0.3002648601929347
RMSE train: 0.377163	val: 1.304653	test: 1.495129
MAE train: 0.269888	val: 1.043573	test: 1.095667

Epoch: 109
Loss: 0.2213365932305654
RMSE train: 0.475833	val: 1.366778	test: 1.595441
MAE train: 0.336017	val: 1.109412	test: 1.180789

Epoch: 110
Loss: 0.2747495820124944
RMSE train: 0.513921	val: 1.364821	test: 1.568323
MAE train: 0.353492	val: 1.110474	test: 1.176004

Epoch: 111
Loss: 0.27022390564282733
RMSE train: 0.394403	val: 1.296328	test: 1.506129
MAE train: 0.277406	val: 1.027447	test: 1.097009

Epoch: 112
Loss: 0.23403465747833252
RMSE train: 0.385671	val: 1.334960	test: 1.540522
MAE train: 0.274669	val: 1.064895	test: 1.117732

Epoch: 113
Loss: 0.22657658656438193
RMSE train: 0.452011	val: 1.384536	test: 1.580694
MAE train: 0.325656	val: 1.125186	test: 1.167336

Epoch: 114
Loss: 0.26335221032301587
RMSE train: 0.520623	val: 1.391200	test: 1.602349
MAE train: 0.362186	val: 1.131033	test: 1.186889

Epoch: 115
Loss: 0.2658849855264028
RMSE train: 0.404028	val: 1.330900	test: 1.542103
MAE train: 0.290445	val: 1.058706	test: 1.130178

Epoch: 116
Loss: 0.26153476039568585
RMSE train: 0.414423	val: 1.365384	test: 1.594183
MAE train: 0.300006	val: 1.095197	test: 1.164836

Epoch: 117
Loss: 0.35099607706069946
RMSE train: 0.478751	val: 1.393103	test: 1.677960
MAE train: 0.353509	val: 1.121416	test: 1.207585

Epoch: 118
Loss: 0.22785601516564688
RMSE train: 0.348115	val: 1.322200	test: 1.593738
MAE train: 0.251903	val: 1.036328	test: 1.139857

Epoch: 119
Loss: 0.23102633158365884
RMSE train: 0.334317	val: 1.311337	test: 1.586614
MAE train: 0.243249	val: 1.022389	test: 1.143913

Epoch: 120
Loss: 0.2407377908627192
RMSE train: 0.392713	val: 1.326728	test: 1.581010
MAE train: 0.275035	val: 1.062594	test: 1.164280

Epoch: 121
Loss: 0.24254812796910605
RMSE train: 0.402102	val: 1.330041	test: 1.548404
MAE train: 0.281728	val: 1.064829	test: 1.148982

Epoch: 122
Loss: 0.23986592392126718
RMSE train: 0.416202	val: 1.354832	test: 1.534188
MAE train: 0.292984	val: 1.093895	test: 1.151414

Epoch: 123
Loss: 0.23132838805516562
RMSE train: 0.408643	val: 1.369289	test: 1.665281
MAE train: 0.294275	val: 1.095148	test: 1.202159

Epoch: 124
Loss: 0.2411146561304728
RMSE train: 0.400927	val: 1.359544	test: 1.611053
MAE train: 0.283685	val: 1.090847	test: 1.183444

Epoch: 125
Loss: 0.24410965541998544
RMSE train: 0.428045	val: 1.368004	test: 1.543892
MAE train: 0.299086	val: 1.115193	test: 1.165459

Epoch: 126
Loss: 0.21226724982261658
RMSE train: 0.501743	val: 1.418917	test: 1.571391
MAE train: 0.354602	val: 1.173053	test: 1.206046

Epoch: 127
Loss: 0.2024782051642736
RMSE train: 0.467086	val: 1.341486	test: 1.533145
MAE train: 0.328567	val: 1.076857	test: 1.130154

Epoch: 128
Loss: 0.251702348391215
RMSE train: 0.374342	val: 1.283193	test: 1.435365
MAE train: 0.264724	val: 1.017454	test: 1.068273

Epoch: 129
Loss: 0.23316475749015808
RMSE train: 0.397508	val: 1.331469	test: 1.590769
MAE train: 0.283489	val: 1.071014	test: 1.166023

Epoch: 130
Loss: 0.23398770888646445
RMSE train: 0.466583	val: 1.372569	test: 1.714795
MAE train: 0.325523	val: 1.113630	test: 1.243098

Epoch: 131
Loss: 0.23767255743344626
RMSE train: 0.376324	val: 1.320664	test: 1.572665
MAE train: 0.260689	val: 1.062994	test: 1.146063

Epoch: 132
Loss: 0.2562738060951233
RMSE train: 0.356597	val: 1.331717	test: 1.453682
MAE train: 0.257909	val: 1.062936	test: 1.090547

Epoch: 133
Loss: 0.24695680538813272
RMSE train: 0.342076	val: 1.312532	test: 1.588862
MAE train: 0.251798	val: 1.032880	test: 1.117021

Epoch: 134
Loss: 0.2624006470044454
RMSE train: 0.364331	val: 1.309344	test: 1.658729
MAE train: 0.268851	val: 1.031101	test: 1.157806

Epoch: 135
Loss: 0.26126814881960553
RMSE train: 0.354487	val: 1.322215	test: 1.530379
MAE train: 0.256945	val: 1.044566	test: 1.119299

Early stopping
Best (RMSE):	 train: 0.359818	val: 1.270331	test: 1.552815
Best (MAE):	 train: 0.255490	val: 1.001571	test: 1.116368


Epoch: 84
Loss: 0.41672755032777786
RMSE train: 0.518154	val: 1.087056	test: 1.137104
MAE train: 0.377967	val: 0.855802	test: 0.903958

Epoch: 85
Loss: 1.0172344520688057
RMSE train: 0.494261	val: 1.066131	test: 1.115515
MAE train: 0.372572	val: 0.809818	test: 0.886061

Epoch: 86
Loss: 0.47281555086374283
RMSE train: 0.518006	val: 1.103034	test: 1.161613
MAE train: 0.373398	val: 0.860367	test: 0.912229

Epoch: 87
Loss: 0.4534267485141754
RMSE train: 0.525565	val: 1.168634	test: 1.213484
MAE train: 0.394465	val: 0.906945	test: 0.948123

Epoch: 88
Loss: 0.3699110671877861
RMSE train: 0.463269	val: 1.129542	test: 1.156849
MAE train: 0.335841	val: 0.863952	test: 0.904883

Epoch: 89
Loss: 0.45125366747379303
RMSE train: 0.482225	val: 1.104969	test: 1.155513
MAE train: 0.354141	val: 0.837874	test: 0.904229

Epoch: 90
Loss: 0.3685346394777298
RMSE train: 0.534229	val: 1.154723	test: 1.217467
MAE train: 0.384822	val: 0.892987	test: 0.951972

Epoch: 91
Loss: 0.4276801571249962
RMSE train: 0.534639	val: 1.134155	test: 1.191945
MAE train: 0.385261	val: 0.875282	test: 0.937568

Epoch: 92
Loss: 0.3612346798181534
RMSE train: 0.531147	val: 1.068348	test: 1.111122
MAE train: 0.381512	val: 0.813392	test: 0.874592

Epoch: 93
Loss: 0.3662768602371216
RMSE train: 0.491930	val: 1.030195	test: 1.070608
MAE train: 0.353737	val: 0.786816	test: 0.841978

Epoch: 94
Loss: 0.6608938351273537
RMSE train: 0.466773	val: 1.030637	test: 1.062505
MAE train: 0.343088	val: 0.792640	test: 0.836957

Epoch: 95
Loss: 0.40357183665037155
RMSE train: 0.448848	val: 1.010166	test: 1.033448
MAE train: 0.341643	val: 0.761593	test: 0.811173

Epoch: 96
Loss: 0.448837511241436
RMSE train: 0.470497	val: 1.064318	test: 1.065608
MAE train: 0.348563	val: 0.825485	test: 0.841185

Epoch: 97
Loss: 0.6385945305228233
RMSE train: 0.608877	val: 1.151035	test: 1.170471
MAE train: 0.428901	val: 0.905173	test: 0.922712

Epoch: 98
Loss: 0.5454067885875702
RMSE train: 0.650384	val: 1.245239	test: 1.277958
MAE train: 0.464754	val: 0.992396	test: 1.015330

Epoch: 99
Loss: 0.5011984556913376
RMSE train: 0.547433	val: 1.199351	test: 1.232712
MAE train: 0.414484	val: 0.926928	test: 0.973720

Epoch: 100
Loss: 0.4353048875927925
RMSE train: 0.461652	val: 1.128689	test: 1.156986
MAE train: 0.342285	val: 0.880070	test: 0.909593

Epoch: 101
Loss: 0.45185374468564987
RMSE train: 0.577528	val: 1.257973	test: 1.300131
MAE train: 0.407327	val: 1.010958	test: 1.024676

Epoch: 102
Loss: 0.6701188087463379
RMSE train: 0.575138	val: 1.251839	test: 1.270747
MAE train: 0.408116	val: 0.999935	test: 1.005847

Epoch: 103
Loss: 0.4646955356001854
RMSE train: 0.474247	val: 1.091949	test: 1.092980
MAE train: 0.349259	val: 0.827736	test: 0.871509

Epoch: 104
Loss: 0.4154019355773926
RMSE train: 0.474850	val: 1.052249	test: 1.075486
MAE train: 0.357742	val: 0.805220	test: 0.862609

Epoch: 105
Loss: 0.7012446746230125
RMSE train: 0.448523	val: 1.042664	test: 1.070237
MAE train: 0.330527	val: 0.793959	test: 0.854145

Epoch: 106
Loss: 0.4362083822488785
RMSE train: 0.435517	val: 1.037777	test: 1.083566
MAE train: 0.326325	val: 0.789056	test: 0.869891

Epoch: 107
Loss: 0.45410706102848053
RMSE train: 0.475288	val: 1.081890	test: 1.131728
MAE train: 0.353879	val: 0.829544	test: 0.903979

Epoch: 108
Loss: 0.4495554268360138
RMSE train: 0.457993	val: 1.077860	test: 1.125427
MAE train: 0.345608	val: 0.813825	test: 0.896941

Epoch: 109
Loss: 0.36832084506750107
RMSE train: 0.459389	val: 1.053392	test: 1.085741
MAE train: 0.339739	val: 0.790949	test: 0.862722

Epoch: 110
Loss: 0.35337018966674805
RMSE train: 0.523361	val: 1.080667	test: 1.090182
MAE train: 0.379202	val: 0.829554	test: 0.855973

Epoch: 111
Loss: 0.38225870579481125
RMSE train: 0.480925	val: 1.100449	test: 1.105498
MAE train: 0.342298	val: 0.847194	test: 0.878668

Epoch: 112
Loss: 0.43894969671964645
RMSE train: 0.445872	val: 1.086294	test: 1.102461
MAE train: 0.331822	val: 0.823484	test: 0.884155

Epoch: 113
Loss: 0.4442035108804703
RMSE train: 0.404788	val: 1.057581	test: 1.061182
MAE train: 0.301387	val: 0.805508	test: 0.850519

Epoch: 114
Loss: 0.35944007337093353
RMSE train: 0.440356	val: 1.097629	test: 1.100377
MAE train: 0.324048	val: 0.846142	test: 0.876074

Epoch: 115
Loss: 0.3816848695278168
RMSE train: 0.438406	val: 1.079284	test: 1.077822
MAE train: 0.325825	val: 0.814639	test: 0.860004

Epoch: 116
Loss: 0.48263970017433167
RMSE train: 0.451518	val: 1.114890	test: 1.122062
MAE train: 0.330181	val: 0.850382	test: 0.884687

Epoch: 117
Loss: 0.4059862568974495
RMSE train: 0.427025	val: 1.048842	test: 1.052520
MAE train: 0.313691	val: 0.793907	test: 0.835377

Epoch: 118
Loss: 0.335391603410244
RMSE train: 0.469471	val: 1.031088	test: 1.045062
MAE train: 0.338779	val: 0.801024	test: 0.828594

Epoch: 119
Loss: 0.3925686404109001
RMSE train: 0.504009	val: 1.089280	test: 1.125242
MAE train: 0.359076	val: 0.848863	test: 0.887253

Epoch: 120
Loss: 0.37441910058259964
RMSE train: 0.494748	val: 1.124886	test: 1.169895
MAE train: 0.354442	val: 0.877009	test: 0.927545

Epoch: 121
Loss: 0.3550097420811653
RMSE train: 0.465865	val: 1.084472	test: 1.125149
MAE train: 0.338827	val: 0.851000	test: 0.881837

Early stopping
Best (RMSE):	 train: 0.488962	val: 1.007784	test: 1.040475
Best (MAE):	 train: 0.355137	val: 0.788876	test: 0.839057


Epoch: 84
Loss: 0.2825882037480672
RMSE train: 0.435633	val: 1.482132	test: 1.928270
MAE train: 0.299578	val: 1.131024	test: 1.241195

Epoch: 85
Loss: 0.2863908608754476
RMSE train: 0.463871	val: 1.436725	test: 1.619129
MAE train: 0.308560	val: 1.156731	test: 1.200206

Epoch: 86
Loss: 0.29085959990819293
RMSE train: 0.478844	val: 1.457643	test: 1.684455
MAE train: 0.327771	val: 1.180356	test: 1.231200

Epoch: 87
Loss: 0.37122848629951477
RMSE train: 0.452951	val: 1.451218	test: 1.781216
MAE train: 0.315022	val: 1.149771	test: 1.223366

Epoch: 88
Loss: 0.2575121223926544
RMSE train: 0.417678	val: 1.385585	test: 1.636604
MAE train: 0.304659	val: 1.090564	test: 1.155419

Epoch: 89
Loss: 0.3571951985359192
RMSE train: 0.439311	val: 1.456477	test: 1.612041
MAE train: 0.312540	val: 1.167601	test: 1.191740

Epoch: 90
Loss: 0.2624438951412837
RMSE train: 0.471479	val: 1.521409	test: 1.775784
MAE train: 0.337796	val: 1.197567	test: 1.244728

Epoch: 91
Loss: 0.2682260324557622
RMSE train: 0.364759	val: 1.502135	test: 1.923679
MAE train: 0.259937	val: 1.146836	test: 1.262989

Epoch: 92
Loss: 0.2850210765997569
RMSE train: 0.383739	val: 1.470172	test: 1.803162
MAE train: 0.273256	val: 1.137267	test: 1.217130

Epoch: 93
Loss: 0.2637140800555547
RMSE train: 0.433642	val: 1.522466	test: 1.797611
MAE train: 0.307904	val: 1.213195	test: 1.263443

Epoch: 94
Loss: 0.2726595600446065
RMSE train: 0.411096	val: 1.569661	test: 1.929437
MAE train: 0.306033	val: 1.218771	test: 1.290128

Epoch: 95
Loss: 0.28594766060511273
RMSE train: 0.395068	val: 1.496907	test: 1.875639
MAE train: 0.277797	val: 1.148019	test: 1.244976

Epoch: 96
Loss: 0.2659854044516881
RMSE train: 0.416697	val: 1.460273	test: 1.896887
MAE train: 0.287660	val: 1.111254	test: 1.229395

Epoch: 97
Loss: 0.25336222847302753
RMSE train: 0.377772	val: 1.478231	test: 1.879391
MAE train: 0.266496	val: 1.126856	test: 1.235154

Epoch: 98
Loss: 0.288310706615448
RMSE train: 0.371788	val: 1.531971	test: 1.937483
MAE train: 0.267527	val: 1.173325	test: 1.285839

Epoch: 99
Loss: 0.2593679229418437
RMSE train: 0.400943	val: 1.540653	test: 1.976594
MAE train: 0.285915	val: 1.190622	test: 1.315566

Epoch: 100
Loss: 0.2860395908355713
RMSE train: 0.422021	val: 1.431969	test: 1.739054
MAE train: 0.295379	val: 1.140939	test: 1.240943

Epoch: 101
Loss: 0.2397767553726832
RMSE train: 0.433283	val: 1.405061	test: 1.722816
MAE train: 0.297187	val: 1.120542	test: 1.225455

Epoch: 102
Loss: 0.2645200391610463
RMSE train: 0.390849	val: 1.413713	test: 1.739329
MAE train: 0.271408	val: 1.118894	test: 1.221799

Epoch: 103
Loss: 0.2817208170890808
RMSE train: 0.399864	val: 1.450094	test: 1.728805
MAE train: 0.282388	val: 1.156596	test: 1.238425

Epoch: 104
Loss: 0.24803032477696738
RMSE train: 0.419320	val: 1.445361	test: 1.743148
MAE train: 0.297086	val: 1.152120	test: 1.238243

Epoch: 105
Loss: 0.24841939409573874
RMSE train: 0.419040	val: 1.399297	test: 1.776241
MAE train: 0.290087	val: 1.097624	test: 1.213305

Epoch: 106
Loss: 0.27440563837687176
RMSE train: 0.416833	val: 1.364080	test: 1.677237
MAE train: 0.287909	val: 1.079833	test: 1.180108

Epoch: 107
Loss: 0.27341146767139435
RMSE train: 0.434023	val: 1.394506	test: 1.651115
MAE train: 0.301995	val: 1.123399	test: 1.201902

Epoch: 108
Loss: 0.24207139511903128
RMSE train: 0.451771	val: 1.459648	test: 1.723603
MAE train: 0.324663	val: 1.173997	test: 1.251036

Epoch: 109
Loss: 0.22955045600732168
RMSE train: 0.443608	val: 1.456581	test: 1.727591
MAE train: 0.311475	val: 1.166078	test: 1.242030

Epoch: 110
Loss: 0.23703956107298532
RMSE train: 0.416986	val: 1.425549	test: 1.643805
MAE train: 0.283240	val: 1.140464	test: 1.198374

Epoch: 111
Loss: 0.27382062872250873
RMSE train: 0.394814	val: 1.423560	test: 1.699606
MAE train: 0.280237	val: 1.118736	test: 1.201503

Epoch: 112
Loss: 0.27868103484312695
RMSE train: 0.445789	val: 1.457183	test: 1.858181
MAE train: 0.303439	val: 1.134932	test: 1.257135

Epoch: 113
Loss: 0.241450106104215
RMSE train: 0.387342	val: 1.447207	test: 1.814957
MAE train: 0.269787	val: 1.123039	test: 1.243379

Epoch: 114
Loss: 0.25203018883864087
RMSE train: 0.402041	val: 1.441623	test: 1.649357
MAE train: 0.275244	val: 1.160097	test: 1.227369

Epoch: 115
Loss: 0.2571551899115245
RMSE train: 0.528551	val: 1.489723	test: 1.688800
MAE train: 0.359574	val: 1.222678	test: 1.281271

Epoch: 116
Loss: 0.24755580723285675
RMSE train: 0.377315	val: 1.427514	test: 1.652495
MAE train: 0.262316	val: 1.134204	test: 1.209922

Epoch: 117
Loss: 0.2457120418548584
RMSE train: 0.362480	val: 1.463583	test: 1.821686
MAE train: 0.254872	val: 1.126767	test: 1.249262

Epoch: 118
Loss: 0.2935810685157776
RMSE train: 0.402586	val: 1.447254	test: 1.728564
MAE train: 0.273796	val: 1.146279	test: 1.236586

Epoch: 119
Loss: 0.2599407931168874
RMSE train: 0.371422	val: 1.404832	test: 1.582596
MAE train: 0.256032	val: 1.142745	test: 1.195046

Epoch: 120
Loss: 0.26308536529541016
RMSE train: 0.356632	val: 1.399930	test: 1.642122
MAE train: 0.247655	val: 1.125161	test: 1.203437

Epoch: 121
Loss: 0.2228941967089971
RMSE train: 0.422025	val: 1.428284	test: 1.724460
MAE train: 0.290013	val: 1.134514	test: 1.231661

Epoch: 122
Loss: 0.23438227673371634
RMSE train: 0.364087	val: 1.390982	test: 1.626897
MAE train: 0.251770	val: 1.101291	test: 1.190844

Epoch: 123
Loss: 0.2336998631556829
RMSE train: 0.353637	val: 1.441960	test: 1.806161
MAE train: 0.251110	val: 1.114356	test: 1.239215

Epoch: 124
Loss: 0.22707271575927734
RMSE train: 0.411218	val: 1.475683	test: 1.840680
MAE train: 0.291282	val: 1.157882	test: 1.272707

Epoch: 125
Loss: 0.2458015779654185
RMSE train: 0.375017	val: 1.413649	test: 1.627928
MAE train: 0.261166	val: 1.134099	test: 1.207248

Epoch: 126
Loss: 0.23222021261850992
RMSE train: 0.372426	val: 1.411336	test: 1.610025
MAE train: 0.259313	val: 1.127906	test: 1.202078

Epoch: 127
Loss: 0.23118379215399423
RMSE train: 0.370765	val: 1.463276	test: 1.781425
MAE train: 0.260344	val: 1.145938	test: 1.249871

Epoch: 128
Loss: 0.1929649362961451
RMSE train: 0.375904	val: 1.452107	test: 1.789784
MAE train: 0.262307	val: 1.131908	test: 1.245146

Epoch: 129
Loss: 0.22036276757717133
RMSE train: 0.356250	val: 1.420080	test: 1.705751
MAE train: 0.248375	val: 1.106916	test: 1.205212

Epoch: 130
Loss: 0.28842010100682575
RMSE train: 0.393457	val: 1.420912	test: 1.631965
MAE train: 0.268349	val: 1.138913	test: 1.203234

Epoch: 131
Loss: 0.25312620401382446
RMSE train: 0.404458	val: 1.419042	test: 1.616193
MAE train: 0.277932	val: 1.142430	test: 1.207748

Epoch: 132
Loss: 0.1838286022345225
RMSE train: 0.445880	val: 1.389454	test: 1.602930
MAE train: 0.303679	val: 1.115893	test: 1.193558

Epoch: 133
Loss: 0.266387403011322
RMSE train: 0.414813	val: 1.387374	test: 1.655994
MAE train: 0.283171	val: 1.100860	test: 1.192298

Epoch: 134
Loss: 0.20513448615868887
RMSE train: 0.367693	val: 1.413829	test: 1.694363
MAE train: 0.262645	val: 1.119925	test: 1.214656

Epoch: 135
Loss: 0.19750113785266876
RMSE train: 0.447388	val: 1.501863	test: 1.772069
MAE train: 0.325826	val: 1.200898	test: 1.290862

Epoch: 136
Loss: 0.206220010916392
RMSE train: 0.405051	val: 1.466134	test: 1.681568
MAE train: 0.286058	val: 1.186601	test: 1.253790

Epoch: 137
Loss: 0.22101639211177826
RMSE train: 0.335818	val: 1.425218	test: 1.706976
MAE train: 0.235445	val: 1.106590	test: 1.215696

Epoch: 138
Loss: 0.19786549607912698
RMSE train: 0.354977	val: 1.441528	test: 1.741852
MAE train: 0.245988	val: 1.112839	test: 1.243927

Epoch: 139
Loss: 0.2135302573442459
RMSE train: 0.339267	val: 1.412458	test: 1.621136
MAE train: 0.235006	val: 1.122594	test: 1.208093

Epoch: 140
Loss: 0.21828839679559073
RMSE train: 0.352518	val: 1.426913	test: 1.622139
MAE train: 0.245589	val: 1.140817	test: 1.217447

Epoch: 141
Loss: 0.24873886009057364
RMSE train: 0.457946	val: 1.448539	test: 1.633584
MAE train: 0.313718	val: 1.185126	test: 1.247406

Early stopping
Best (RMSE):	 train: 0.416833	val: 1.364080	test: 1.677237
Best (MAE):	 train: 0.287909	val: 1.079833	test: 1.180108
All runs completed.


Epoch: 84
Loss: 0.3006768152117729
RMSE train: 0.466017	val: 1.155797	test: 1.164679
MAE train: 0.342660	val: 0.934083	test: 0.901204

Epoch: 85
Loss: 0.25844717770814896
RMSE train: 0.429069	val: 1.027818	test: 1.120113
MAE train: 0.308964	val: 0.869474	test: 0.853024

Epoch: 86
Loss: 0.3250410929322243
RMSE train: 0.398109	val: 1.040208	test: 1.115552
MAE train: 0.289017	val: 0.854921	test: 0.856352

Epoch: 87
Loss: 0.2963367849588394
RMSE train: 0.418499	val: 1.098645	test: 1.135886
MAE train: 0.303671	val: 0.892496	test: 0.873941

Epoch: 88
Loss: 0.2843589186668396
RMSE train: 0.428613	val: 1.082457	test: 1.150566
MAE train: 0.304883	val: 0.892014	test: 0.879975

Epoch: 89
Loss: 0.30316122621297836
RMSE train: 0.443780	val: 1.044884	test: 1.138096
MAE train: 0.313416	val: 0.883593	test: 0.866256

Epoch: 90
Loss: 0.29215239733457565
RMSE train: 0.416702	val: 1.101661	test: 1.120870
MAE train: 0.301228	val: 0.892434	test: 0.864855

Epoch: 91
Loss: 0.29669734090566635
RMSE train: 0.389206	val: 1.141305	test: 1.165336
MAE train: 0.285413	val: 0.895659	test: 0.880151

Epoch: 92
Loss: 0.2791355028748512
RMSE train: 0.437344	val: 0.994190	test: 1.120931
MAE train: 0.314866	val: 0.840014	test: 0.853833

Epoch: 93
Loss: 0.2919175401329994
RMSE train: 0.470781	val: 1.034793	test: 1.151643
MAE train: 0.338558	val: 0.879477	test: 0.881316

Epoch: 94
Loss: 0.2659202478826046
RMSE train: 0.437277	val: 1.126624	test: 1.173616
MAE train: 0.320824	val: 0.913147	test: 0.903446

Epoch: 95
Loss: 0.25574443489313126
RMSE train: 0.399284	val: 1.091895	test: 1.151534
MAE train: 0.293898	val: 0.876235	test: 0.872495

Epoch: 96
Loss: 0.27099067717790604
RMSE train: 0.456811	val: 1.090273	test: 1.184749
MAE train: 0.330543	val: 0.902941	test: 0.913332

Epoch: 97
Loss: 0.2900707833468914
RMSE train: 0.432864	val: 1.050457	test: 1.170216
MAE train: 0.311819	val: 0.881840	test: 0.890906

Epoch: 98
Loss: 0.25488677620887756
RMSE train: 0.486218	val: 1.116978	test: 1.216905
MAE train: 0.348400	val: 0.918714	test: 0.937193

Epoch: 99
Loss: 0.24023902788758278
RMSE train: 0.429735	val: 1.078267	test: 1.174698
MAE train: 0.305287	val: 0.882277	test: 0.893836

Epoch: 100
Loss: 0.25789955258369446
RMSE train: 0.456083	val: 1.159457	test: 1.174503
MAE train: 0.326420	val: 0.928664	test: 0.905174

Epoch: 101
Loss: 0.28737279400229454
RMSE train: 0.410043	val: 0.999859	test: 1.109174
MAE train: 0.287041	val: 0.843758	test: 0.831469

Epoch: 102
Loss: 0.2529519572854042
RMSE train: 0.384237	val: 1.011070	test: 1.112180
MAE train: 0.281456	val: 0.842171	test: 0.830957

Epoch: 103
Loss: 0.24506177008152008
RMSE train: 0.433667	val: 1.149255	test: 1.127638
MAE train: 0.316176	val: 0.904266	test: 0.858808

Epoch: 104
Loss: 0.2508474737405777
RMSE train: 0.382473	val: 1.119555	test: 1.112817
MAE train: 0.282357	val: 0.880840	test: 0.837431

Epoch: 105
Loss: 0.26775140687823296
RMSE train: 0.404241	val: 0.976866	test: 1.080000
MAE train: 0.299824	val: 0.823564	test: 0.827048

Epoch: 106
Loss: 0.27163074165582657
RMSE train: 0.406835	val: 0.969831	test: 1.056067
MAE train: 0.297205	val: 0.830618	test: 0.801563

Epoch: 107
Loss: 0.252550158649683
RMSE train: 0.371106	val: 0.988885	test: 1.071667
MAE train: 0.274510	val: 0.828642	test: 0.818769

Epoch: 108
Loss: 0.24591320753097534
RMSE train: 0.398482	val: 1.064407	test: 1.141499
MAE train: 0.287670	val: 0.870186	test: 0.874016

Epoch: 109
Loss: 0.2722729593515396
RMSE train: 0.385081	val: 1.045753	test: 1.173335
MAE train: 0.284417	val: 0.876320	test: 0.894171

Epoch: 110
Loss: 0.3080626204609871
RMSE train: 0.372077	val: 1.084191	test: 1.169053
MAE train: 0.269340	val: 0.885791	test: 0.899268

Epoch: 111
Loss: 0.2413046546280384
RMSE train: 0.408473	val: 1.042214	test: 1.115978
MAE train: 0.290936	val: 0.865653	test: 0.851326

Epoch: 112
Loss: 0.2682398594915867
RMSE train: 0.367035	val: 0.983576	test: 1.085666
MAE train: 0.268851	val: 0.814305	test: 0.826036

Epoch: 113
Loss: 0.37623489648103714
RMSE train: 0.408094	val: 1.156435	test: 1.145284
MAE train: 0.298768	val: 0.899495	test: 0.873912

Epoch: 114
Loss: 0.2404986508190632
RMSE train: 0.362756	val: 1.087025	test: 1.117383
MAE train: 0.268333	val: 0.861021	test: 0.846185

Epoch: 115
Loss: 0.2620695009827614
RMSE train: 0.480693	val: 1.139222	test: 1.187651
MAE train: 0.351831	val: 0.932486	test: 0.919886

Epoch: 116
Loss: 0.2710811570286751
RMSE train: 0.398183	val: 1.045095	test: 1.121943
MAE train: 0.290524	val: 0.856553	test: 0.863649

Epoch: 117
Loss: 0.2600599490106106
RMSE train: 0.378077	val: 1.009745	test: 1.100155
MAE train: 0.277131	val: 0.848621	test: 0.841733

Epoch: 118
Loss: 0.2330666109919548
RMSE train: 0.408474	val: 1.080610	test: 1.121137
MAE train: 0.294727	val: 0.885612	test: 0.861335

Epoch: 119
Loss: 0.2628604620695114
RMSE train: 0.387199	val: 1.044220	test: 1.141484
MAE train: 0.284957	val: 0.856153	test: 0.878684

Epoch: 120
Loss: 0.23148415237665176
RMSE train: 0.421696	val: 1.012756	test: 1.135969
MAE train: 0.304127	val: 0.850107	test: 0.867402

Epoch: 121
Loss: 0.24989204108715057
RMSE train: 0.446882	val: 1.197228	test: 1.180154
MAE train: 0.326099	val: 0.954670	test: 0.915962

Early stopping
Best (RMSE):	 train: 0.417450	val: 0.952199	test: 1.065676
Best (MAE):	 train: 0.301270	val: 0.807673	test: 0.820448


Epoch: 84
Loss: 0.5478307455778122
RMSE train: 0.477150	val: 1.155149	test: 1.162009
MAE train: 0.353573	val: 0.869927	test: 0.925422

Epoch: 85
Loss: 0.6453403457999229
RMSE train: 0.619845	val: 1.216708	test: 1.222253
MAE train: 0.431798	val: 0.925025	test: 0.955709

Epoch: 86
Loss: 0.4702991023659706
RMSE train: 0.566906	val: 1.140702	test: 1.119742
MAE train: 0.399219	val: 0.874245	test: 0.878379

Epoch: 87
Loss: 0.4356241747736931
RMSE train: 0.523934	val: 1.124610	test: 1.103486
MAE train: 0.385834	val: 0.860636	test: 0.866663

Epoch: 88
Loss: 0.6039028614759445
RMSE train: 0.493824	val: 1.172203	test: 1.161474
MAE train: 0.357304	val: 0.911342	test: 0.921029

Epoch: 89
Loss: 0.4550180286169052
RMSE train: 0.441393	val: 1.151293	test: 1.133323
MAE train: 0.324961	val: 0.878193	test: 0.896966

Epoch: 90
Loss: 0.32566137611866
RMSE train: 0.478590	val: 1.105266	test: 1.109747
MAE train: 0.361297	val: 0.806942	test: 0.868858

Epoch: 91
Loss: 0.8633638396859169
RMSE train: 0.459818	val: 1.092515	test: 1.106363
MAE train: 0.343269	val: 0.808600	test: 0.873350

Epoch: 92
Loss: 0.42352569103240967
RMSE train: 0.445208	val: 1.160004	test: 1.174714
MAE train: 0.326951	val: 0.885036	test: 0.936793

Epoch: 93
Loss: 0.38844946399331093
RMSE train: 0.569730	val: 1.292119	test: 1.304122
MAE train: 0.420932	val: 1.007810	test: 1.034202

Epoch: 94
Loss: 0.38128137215971947
RMSE train: 0.509679	val: 1.189073	test: 1.198488
MAE train: 0.370088	val: 0.922558	test: 0.943067

Epoch: 95
Loss: 0.43114884942770004
RMSE train: 0.470466	val: 1.107394	test: 1.100441
MAE train: 0.345688	val: 0.844599	test: 0.862734

Epoch: 96
Loss: 0.36886634677648544
RMSE train: 0.473352	val: 1.102654	test: 1.089277
MAE train: 0.341051	val: 0.841849	test: 0.851621

Epoch: 97
Loss: 0.3673665001988411
RMSE train: 0.483859	val: 1.096007	test: 1.095012
MAE train: 0.351300	val: 0.842122	test: 0.854362

Epoch: 98
Loss: 0.4352070093154907
RMSE train: 0.509388	val: 1.123291	test: 1.137790
MAE train: 0.371944	val: 0.855131	test: 0.891452

Epoch: 99
Loss: 0.3850030153989792
RMSE train: 0.500853	val: 1.087289	test: 1.085582
MAE train: 0.365451	val: 0.817394	test: 0.855204

Epoch: 100
Loss: 0.4082372859120369
RMSE train: 0.441317	val: 1.082525	test: 1.092399
MAE train: 0.325170	val: 0.804075	test: 0.865254

Epoch: 101
Loss: 0.5660252273082733
RMSE train: 0.430895	val: 1.088988	test: 1.151062
MAE train: 0.326001	val: 0.822330	test: 0.914452

Epoch: 102
Loss: 0.3520388603210449
RMSE train: 0.465620	val: 1.080551	test: 1.128964
MAE train: 0.342453	val: 0.811572	test: 0.887198

Epoch: 103
Loss: 0.355056120082736
RMSE train: 0.498730	val: 1.088672	test: 1.134295
MAE train: 0.361754	val: 0.840479	test: 0.891076

Epoch: 104
Loss: 0.44340283423662186
RMSE train: 0.499431	val: 1.087337	test: 1.124544
MAE train: 0.356047	val: 0.843814	test: 0.879117

Epoch: 105
Loss: 0.4051264598965645
RMSE train: 0.477593	val: 1.088035	test: 1.120609
MAE train: 0.339676	val: 0.851102	test: 0.868555

Epoch: 106
Loss: 0.3501949980854988
RMSE train: 0.460480	val: 1.079829	test: 1.118056
MAE train: 0.336506	val: 0.822319	test: 0.875551

Epoch: 107
Loss: 0.43634314835071564
RMSE train: 0.508256	val: 1.115306	test: 1.152401
MAE train: 0.361391	val: 0.859199	test: 0.901873

Epoch: 108
Loss: 0.33085352182388306
RMSE train: 0.521427	val: 1.126052	test: 1.156481
MAE train: 0.367698	val: 0.873691	test: 0.904653

Epoch: 109
Loss: 0.4703410193324089
RMSE train: 0.486163	val: 1.114163	test: 1.148007
MAE train: 0.344503	val: 0.864852	test: 0.898725

Epoch: 110
Loss: 0.4546444043517113
RMSE train: 0.432966	val: 1.120979	test: 1.157309
MAE train: 0.316656	val: 0.850374	test: 0.916727

Epoch: 111
Loss: 0.3058956414461136
RMSE train: 0.497246	val: 1.191737	test: 1.234501
MAE train: 0.365564	val: 0.919441	test: 0.982466

Epoch: 112
Loss: 0.3963029012084007
RMSE train: 0.468776	val: 1.069398	test: 1.100201
MAE train: 0.346835	val: 0.806042	test: 0.861755

Epoch: 113
Loss: 0.37504514306783676
RMSE train: 0.481308	val: 1.036968	test: 1.068083
MAE train: 0.356643	val: 0.782153	test: 0.838310

Epoch: 114
Loss: 0.38898900151252747
RMSE train: 0.507384	val: 1.100522	test: 1.111625
MAE train: 0.363870	val: 0.851294	test: 0.871412

Epoch: 115
Loss: 0.35211072117090225
RMSE train: 0.501669	val: 1.089967	test: 1.086350
MAE train: 0.356787	val: 0.845374	test: 0.850946

Epoch: 116
Loss: 0.3433360978960991
RMSE train: 0.462341	val: 1.054933	test: 1.057396
MAE train: 0.341415	val: 0.804340	test: 0.832775

Epoch: 117
Loss: 0.4125029444694519
RMSE train: 0.458268	val: 1.062954	test: 1.078549
MAE train: 0.343293	val: 0.795219	test: 0.853199

Epoch: 118
Loss: 0.3470422625541687
RMSE train: 0.462581	val: 1.149007	test: 1.162876
MAE train: 0.338074	val: 0.836950	test: 0.931791

Epoch: 119
Loss: 0.3346172794699669
RMSE train: 0.450459	val: 1.079575	test: 1.065441
MAE train: 0.326884	val: 0.802584	test: 0.844930

Epoch: 120
Loss: 0.3727291077375412
RMSE train: 0.461065	val: 1.079505	test: 1.064325
MAE train: 0.330485	val: 0.824278	test: 0.837400

Epoch: 121
Loss: 0.44971421360969543
RMSE train: 0.473422	val: 1.135218	test: 1.141384
MAE train: 0.341857	val: 0.882322	test: 0.898486

Epoch: 122
Loss: 0.543203204870224
RMSE train: 0.477375	val: 1.149868	test: 1.170463
MAE train: 0.347787	val: 0.889068	test: 0.931973

Epoch: 123
Loss: 0.4247922897338867
RMSE train: 0.651412	val: 1.244485	test: 1.308182
MAE train: 0.461629	val: 0.964048	test: 1.037622

Epoch: 124
Loss: 0.4240155965089798
RMSE train: 0.539098	val: 1.142608	test: 1.158809
MAE train: 0.384336	val: 0.894667	test: 0.910063

Epoch: 125
Loss: 0.3242640495300293
RMSE train: 0.465498	val: 1.089720	test: 1.076062
MAE train: 0.349192	val: 0.823707	test: 0.841427

Epoch: 126
Loss: 0.3846346586942673
RMSE train: 0.444928	val: 1.076906	test: 1.083767
MAE train: 0.325273	val: 0.807494	test: 0.851197

Epoch: 127
Loss: 0.3643355816602707
RMSE train: 0.511621	val: 1.169184	test: 1.192660
MAE train: 0.363513	val: 0.898694	test: 0.948501

Epoch: 128
Loss: 0.37334680557250977
RMSE train: 0.487526	val: 1.122452	test: 1.135043
MAE train: 0.347225	val: 0.865227	test: 0.898965

Epoch: 129
Loss: 0.4958280399441719
RMSE train: 0.427620	val: 1.063839	test: 1.061760
MAE train: 0.310179	val: 0.817878	test: 0.830546

Epoch: 130
Loss: 0.32685205340385437
RMSE train: 0.419444	val: 1.070474	test: 1.103027
MAE train: 0.313698	val: 0.817047	test: 0.863085

Epoch: 131
Loss: 0.6160036996006966
RMSE train: 0.414632	val: 1.128554	test: 1.144234
MAE train: 0.308146	val: 0.831204	test: 0.906979

Epoch: 132
Loss: 0.3103543743491173
RMSE train: 0.478301	val: 1.109681	test: 1.116703
MAE train: 0.349653	val: 0.836545	test: 0.876252

Epoch: 133
Loss: 0.3936927393078804
RMSE train: 0.418249	val: 1.072005	test: 1.088661
MAE train: 0.312608	val: 0.803978	test: 0.846573

Epoch: 134
Loss: 0.33256424218416214
RMSE train: 0.413936	val: 1.064660	test: 1.069791
MAE train: 0.310175	val: 0.792326	test: 0.836450

Epoch: 135
Loss: 0.372232049703598
RMSE train: 0.440343	val: 1.103248	test: 1.099328
MAE train: 0.325699	val: 0.832524	test: 0.857624

Epoch: 136
Loss: 0.39182429760694504
RMSE train: 0.486526	val: 1.144854	test: 1.126731
MAE train: 0.356801	val: 0.887723	test: 0.882943

Epoch: 137
Loss: 0.3402528390288353
RMSE train: 0.463225	val: 1.076149	test: 1.071428
MAE train: 0.335173	val: 0.825506	test: 0.827112

Epoch: 138
Loss: 0.40250956267118454
RMSE train: 0.431472	val: 1.043870	test: 1.045551
MAE train: 0.322617	val: 0.786265	test: 0.818512

Epoch: 139
Loss: 0.3237542137503624
RMSE train: 0.393905	val: 1.081093	test: 1.082984
MAE train: 0.293214	val: 0.806346	test: 0.852548

Epoch: 140
Loss: 0.32001732289791107
RMSE train: 0.424626	val: 1.136426	test: 1.134689
MAE train: 0.310864	val: 0.859531	test: 0.880305

Epoch: 141
Loss: 0.34536128491163254
RMSE train: 0.457054	val: 1.116752	test: 1.142631
MAE train: 0.331883	val: 0.855358	test: 0.875553

Epoch: 142
Loss: 0.319643959403038
RMSE train: 0.427813	val: 1.065611	test: 1.095526
MAE train: 0.317942	val: 0.804692	test: 0.847966

Epoch: 143
Loss: 0.5777845233678818
RMSE train: 0.400441	val: 1.050164	test: 1.062688
MAE train: 0.299474	val: 0.780089	test: 0.837622

Epoch: 144
Loss: 0.37057044357061386

Epoch: 84
Loss: 0.8088927716016769
RMSE train: 0.450222	val: 1.045226	test: 1.120050
MAE train: 0.346871	val: 0.787760	test: 0.896028

Epoch: 85
Loss: 0.5381995961070061
RMSE train: 0.506176	val: 1.141358	test: 1.217323
MAE train: 0.389962	val: 0.861151	test: 0.991316

Epoch: 86
Loss: 0.41453684866428375
RMSE train: 0.465796	val: 1.112862	test: 1.209661
MAE train: 0.357969	val: 0.817880	test: 0.964418

Epoch: 87
Loss: 0.46657322347164154
RMSE train: 0.476697	val: 1.078714	test: 1.159726
MAE train: 0.357613	val: 0.814917	test: 0.926548

Epoch: 88
Loss: 0.4400077983736992
RMSE train: 0.513180	val: 1.119024	test: 1.176774
MAE train: 0.374963	val: 0.862652	test: 0.946131

Epoch: 89
Loss: 0.5303443148732185
RMSE train: 0.479613	val: 1.074834	test: 1.121473
MAE train: 0.351112	val: 0.827225	test: 0.901294

Epoch: 90
Loss: 0.4330621808767319
RMSE train: 0.452268	val: 1.044897	test: 1.099929
MAE train: 0.332532	val: 0.795262	test: 0.890113

Epoch: 91
Loss: 0.3543540760874748
RMSE train: 0.523235	val: 1.102396	test: 1.178491
MAE train: 0.370745	val: 0.862906	test: 0.943132

Epoch: 92
Loss: 0.5111077800393105
RMSE train: 0.590258	val: 1.179442	test: 1.263484
MAE train: 0.427868	val: 0.932833	test: 1.011784

Epoch: 93
Loss: 0.48871880024671555
RMSE train: 0.569375	val: 1.154960	test: 1.229085
MAE train: 0.412855	val: 0.905970	test: 0.980936

Epoch: 94
Loss: 0.6387665942311287
RMSE train: 0.528718	val: 1.089299	test: 1.139252
MAE train: 0.381720	val: 0.851963	test: 0.901725

Epoch: 95
Loss: 0.42334508895874023
RMSE train: 0.558386	val: 1.085138	test: 1.139991
MAE train: 0.401360	val: 0.839886	test: 0.908785

Epoch: 96
Loss: 0.4523851275444031
RMSE train: 0.481404	val: 1.059886	test: 1.067002
MAE train: 0.354888	val: 0.800806	test: 0.870238

Epoch: 97
Loss: 0.644064411520958
RMSE train: 0.448224	val: 1.053117	test: 1.068721
MAE train: 0.325594	val: 0.799978	test: 0.868354

Epoch: 98
Loss: 0.39293377101421356
RMSE train: 0.450922	val: 1.031995	test: 1.098090
MAE train: 0.334485	val: 0.801007	test: 0.873368

Epoch: 99
Loss: 0.3597370833158493
RMSE train: 0.557412	val: 1.136738	test: 1.218011
MAE train: 0.403998	val: 0.894704	test: 0.963755

Epoch: 100
Loss: 0.4090868607163429
RMSE train: 0.593693	val: 1.178129	test: 1.262977
MAE train: 0.425986	val: 0.937114	test: 0.993476

Epoch: 101
Loss: 0.3832956925034523
RMSE train: 0.569292	val: 1.110070	test: 1.171620
MAE train: 0.397706	val: 0.869311	test: 0.926393

Epoch: 102
Loss: 0.47859327495098114
RMSE train: 0.498848	val: 1.051764	test: 1.092126
MAE train: 0.351073	val: 0.797216	test: 0.865169

Epoch: 103
Loss: 0.37266334891319275
RMSE train: 0.492292	val: 1.133831	test: 1.178716
MAE train: 0.361106	val: 0.861767	test: 0.943251

Epoch: 104
Loss: 0.4311469718813896
RMSE train: 0.589219	val: 1.213829	test: 1.255831
MAE train: 0.440531	val: 0.967923	test: 0.989952

Epoch: 105
Loss: 0.4039217680692673
RMSE train: 0.517522	val: 1.104545	test: 1.145569
MAE train: 0.376449	val: 0.875913	test: 0.894923

Epoch: 106
Loss: 0.35669805109500885
RMSE train: 0.475002	val: 1.059864	test: 1.107106
MAE train: 0.339530	val: 0.832632	test: 0.860604

Epoch: 107
Loss: 0.39380625635385513
RMSE train: 0.461844	val: 1.056649	test: 1.101760
MAE train: 0.328964	val: 0.821557	test: 0.867721

Epoch: 108
Loss: 0.3413648456335068
RMSE train: 0.470268	val: 1.102480	test: 1.153213
MAE train: 0.338428	val: 0.843280	test: 0.920292

Epoch: 109
Loss: 0.6333758383989334
RMSE train: 0.426733	val: 1.039346	test: 1.083161
MAE train: 0.305377	val: 0.795595	test: 0.855463

Epoch: 110
Loss: 0.3597795441746712
RMSE train: 0.407231	val: 0.998283	test: 1.058344
MAE train: 0.299435	val: 0.752853	test: 0.841426

Epoch: 111
Loss: 0.8168989941477776
RMSE train: 0.416572	val: 1.003480	test: 1.068314
MAE train: 0.314460	val: 0.748585	test: 0.843476

Epoch: 112
Loss: 0.44080228358507156
RMSE train: 0.408132	val: 1.008636	test: 1.103692
MAE train: 0.298451	val: 0.745866	test: 0.877605

Epoch: 113
Loss: 0.5987515076994896
RMSE train: 0.475856	val: 1.131296	test: 1.284816
MAE train: 0.339597	val: 0.846984	test: 1.022674

Epoch: 114
Loss: 0.5730580613017082
RMSE train: 0.481669	val: 1.121503	test: 1.258847
MAE train: 0.357607	val: 0.847540	test: 0.986241

Epoch: 115
Loss: 0.5006014034152031
RMSE train: 0.529218	val: 1.127768	test: 1.225996
MAE train: 0.389338	val: 0.878607	test: 0.969870

Epoch: 116
Loss: 0.3523983582854271
RMSE train: 0.479703	val: 1.014360	test: 1.075816
MAE train: 0.342982	val: 0.804989	test: 0.848113

Epoch: 117
Loss: 0.43487706780433655
RMSE train: 0.462524	val: 1.002499	test: 1.048939
MAE train: 0.339012	val: 0.779500	test: 0.830212

Epoch: 118
Loss: 0.3313494026660919
RMSE train: 0.437293	val: 1.075509	test: 1.198040
MAE train: 0.322808	val: 0.794244	test: 0.946469

Epoch: 119
Loss: 0.34771714359521866
RMSE train: 0.445211	val: 1.074734	test: 1.158559
MAE train: 0.326449	val: 0.810346	test: 0.924263

Epoch: 120
Loss: 0.3602501228451729
RMSE train: 0.571266	val: 1.189756	test: 1.268942
MAE train: 0.418098	val: 0.938105	test: 1.006607

Epoch: 121
Loss: 0.31799546629190445
RMSE train: 0.544640	val: 1.124781	test: 1.192591
MAE train: 0.388727	val: 0.888324	test: 0.938502

Epoch: 122
Loss: 0.33998262137174606
RMSE train: 0.526945	val: 1.101637	test: 1.160057
MAE train: 0.371389	val: 0.870403	test: 0.913999

Epoch: 123
Loss: 0.40223361551761627
RMSE train: 0.486739	val: 1.137775	test: 1.178799
MAE train: 0.350672	val: 0.894566	test: 0.935615

Epoch: 124
Loss: 0.38749922066926956
RMSE train: 0.580381	val: 1.253441	test: 1.315650
MAE train: 0.434313	val: 0.974438	test: 1.052164

Epoch: 125
Loss: 0.4516546055674553
RMSE train: 0.421232	val: 1.032936	test: 1.065407
MAE train: 0.310610	val: 0.783722	test: 0.847733

Epoch: 126
Loss: 0.3286125585436821
RMSE train: 0.475623	val: 1.021046	test: 1.035295
MAE train: 0.359823	val: 0.770250	test: 0.813610

Epoch: 127
Loss: 0.414385125041008
RMSE train: 0.452534	val: 1.015026	test: 1.052892
MAE train: 0.332306	val: 0.788170	test: 0.841731

Epoch: 128
Loss: 0.31247542425990105
RMSE train: 0.493457	val: 1.085037	test: 1.144986
MAE train: 0.368683	val: 0.851174	test: 0.914485

Epoch: 129
Loss: 0.37555738538503647
RMSE train: 0.456274	val: 1.043809	test: 1.108820
MAE train: 0.328870	val: 0.803589	test: 0.882536

Epoch: 130
Loss: 0.3139076232910156
RMSE train: 0.414493	val: 1.007446	test: 1.070954
MAE train: 0.306036	val: 0.759796	test: 0.852823

Epoch: 131
Loss: 0.4093012586236
RMSE train: 0.412359	val: 0.986245	test: 1.046100
MAE train: 0.307579	val: 0.752718	test: 0.832396

Epoch: 132
Loss: 0.46964118629693985
RMSE train: 0.459209	val: 1.047093	test: 1.118283
MAE train: 0.337985	val: 0.825175	test: 0.888129

Epoch: 133
Loss: 0.3302326649427414
RMSE train: 0.532240	val: 1.123262	test: 1.205057
MAE train: 0.393026	val: 0.882413	test: 0.953765

Epoch: 134
Loss: 0.29969705641269684
RMSE train: 0.513658	val: 1.048631	test: 1.143491
MAE train: 0.370460	val: 0.803540	test: 0.891542

Epoch: 135
Loss: 0.34070585668087006
RMSE train: 0.413462	val: 0.965019	test: 1.075497
MAE train: 0.306636	val: 0.730948	test: 0.845489

Epoch: 136
Loss: 0.315876841545105
RMSE train: 0.391031	val: 0.961317	test: 1.051550
MAE train: 0.289406	val: 0.729364	test: 0.829136

Epoch: 137
Loss: 0.36927109211683273
RMSE train: 0.424146	val: 0.979151	test: 1.072661
MAE train: 0.307510	val: 0.748284	test: 0.847409

Epoch: 138
Loss: 0.4036921337246895
RMSE train: 0.457362	val: 1.026343	test: 1.111496
MAE train: 0.328897	val: 0.788722	test: 0.871544

Epoch: 139
Loss: 0.32028209418058395
RMSE train: 0.436502	val: 0.999087	test: 1.076226
MAE train: 0.314711	val: 0.756242	test: 0.853395

Epoch: 140
Loss: 0.3043200746178627
RMSE train: 0.422281	val: 1.007503	test: 1.114371
MAE train: 0.315723	val: 0.762791	test: 0.881797

Epoch: 141
Loss: 0.3359518423676491
RMSE train: 0.430067	val: 1.003187	test: 1.114062
MAE train: 0.321439	val: 0.764956	test: 0.887657

Epoch: 142
Loss: 0.3168317526578903
RMSE train: 0.462877	val: 1.036391	test: 1.132280
MAE train: 0.343497	val: 0.803760	test: 0.893120

Epoch: 143
Loss: 0.4633142501115799
RMSE train: 0.468481	val: 1.046891	test: 1.124855
MAE train: 0.348559	val: 0.816114	test: 0.882305

Epoch: 144
Loss: 0.34324514120817184
RMSE train: 0.399729	val: 1.073433	test: 1.084865
MAE train: 0.301789	val: 0.795049	test: 0.858333

Epoch: 145
Loss: 0.3946763575077057
RMSE train: 0.465692	val: 1.083676	test: 1.091182
MAE train: 0.334829	val: 0.817217	test: 0.860149

Epoch: 146
Loss: 0.26350241154432297
RMSE train: 0.471748	val: 1.103926	test: 1.110529
MAE train: 0.339344	val: 0.837836	test: 0.875505

Epoch: 147
Loss: 0.31829701364040375
RMSE train: 0.467234	val: 1.093489	test: 1.091323
MAE train: 0.337801	val: 0.841792	test: 0.857559

Epoch: 148
Loss: 0.35169675201177597
RMSE train: 0.483209	val: 1.077355	test: 1.086328
MAE train: 0.341461	val: 0.835032	test: 0.855949

Early stopping
Best (RMSE):	 train: 0.481308	val: 1.036968	test: 1.068083
Best (MAE):	 train: 0.356643	val: 0.782153	test: 0.838310


Epoch: 84
Loss: 0.2822389081120491
RMSE train: 0.417244	val: 1.070443	test: 1.079791
MAE train: 0.301694	val: 0.862896	test: 0.800497

Epoch: 85
Loss: 0.327273964881897
RMSE train: 0.491259	val: 1.126194	test: 1.141437
MAE train: 0.352407	val: 0.913245	test: 0.872960

Epoch: 86
Loss: 0.2844161093235016
RMSE train: 0.462147	val: 1.107051	test: 1.110517
MAE train: 0.336336	val: 0.894265	test: 0.845462

Epoch: 87
Loss: 0.3140467181801796
RMSE train: 0.480399	val: 1.008101	test: 1.054757
MAE train: 0.343443	val: 0.828883	test: 0.790028

Epoch: 88
Loss: 0.3101714700460434
RMSE train: 0.493510	val: 1.087106	test: 1.102988
MAE train: 0.350889	val: 0.896853	test: 0.829670

Epoch: 89
Loss: 0.30769363045692444
RMSE train: 0.464587	val: 1.124680	test: 1.092795
MAE train: 0.332134	val: 0.912443	test: 0.820711

Epoch: 90
Loss: 0.31682875007390976
RMSE train: 0.511079	val: 1.116362	test: 1.110538
MAE train: 0.363553	val: 0.911896	test: 0.841471

Epoch: 91
Loss: 0.2858586832880974
RMSE train: 0.447474	val: 1.076059	test: 1.076157
MAE train: 0.325400	val: 0.876498	test: 0.811196

Epoch: 92
Loss: 0.3197973668575287
RMSE train: 0.480590	val: 1.118829	test: 1.108683
MAE train: 0.349173	val: 0.900250	test: 0.849750

Epoch: 93
Loss: 0.27093852311372757
RMSE train: 0.460999	val: 1.140261	test: 1.132794
MAE train: 0.334948	val: 0.906455	test: 0.862880

Epoch: 94
Loss: 0.29083752632141113
RMSE train: 0.473380	val: 1.094132	test: 1.094008
MAE train: 0.336354	val: 0.873289	test: 0.831273

Epoch: 95
Loss: 0.3135475590825081
RMSE train: 0.465670	val: 1.007746	test: 1.057726
MAE train: 0.326930	val: 0.836194	test: 0.785861

Epoch: 96
Loss: 0.28493088483810425
RMSE train: 0.454752	val: 1.154176	test: 1.115813
MAE train: 0.331843	val: 0.924051	test: 0.845991

Epoch: 97
Loss: 0.3224252797663212
RMSE train: 0.467198	val: 1.129654	test: 1.112059
MAE train: 0.337974	val: 0.901025	test: 0.841760

Epoch: 98
Loss: 0.2717115692794323
RMSE train: 0.529529	val: 1.121970	test: 1.114372
MAE train: 0.380032	val: 0.905222	test: 0.852646

Epoch: 99
Loss: 0.24403796717524529
RMSE train: 0.446961	val: 1.027941	test: 1.056996
MAE train: 0.317389	val: 0.835669	test: 0.788639

Epoch: 100
Loss: 0.29137787222862244
RMSE train: 0.472871	val: 1.135601	test: 1.118366
MAE train: 0.340431	val: 0.900301	test: 0.851218

Epoch: 101
Loss: 0.237423874437809
RMSE train: 0.477674	val: 1.148710	test: 1.119470
MAE train: 0.347463	val: 0.910395	test: 0.851004

Epoch: 102
Loss: 0.2796969339251518
RMSE train: 0.388104	val: 0.991278	test: 1.031150
MAE train: 0.276337	val: 0.812283	test: 0.759749

Epoch: 103
Loss: 0.3184257969260216
RMSE train: 0.378722	val: 1.089705	test: 1.090135
MAE train: 0.275437	val: 0.873538	test: 0.811977

Epoch: 104
Loss: 0.30954888463020325
RMSE train: 0.425106	val: 1.202860	test: 1.146025
MAE train: 0.313249	val: 0.951773	test: 0.866622

Epoch: 105
Loss: 0.272135891020298
RMSE train: 0.386543	val: 1.066415	test: 1.060676
MAE train: 0.283397	val: 0.854712	test: 0.781187

Epoch: 106
Loss: 0.2533407434821129
RMSE train: 0.429948	val: 1.139844	test: 1.097884
MAE train: 0.307409	val: 0.895663	test: 0.828747

Epoch: 107
Loss: 0.26213570684194565
RMSE train: 0.451427	val: 1.150193	test: 1.125381
MAE train: 0.323158	val: 0.894111	test: 0.853216

Epoch: 108
Loss: 0.2586032971739769
RMSE train: 0.561602	val: 1.269869	test: 1.217908
MAE train: 0.403113	val: 0.990422	test: 0.949585

Epoch: 109
Loss: 0.2558416873216629
RMSE train: 0.501114	val: 1.118469	test: 1.134980
MAE train: 0.362016	val: 0.895607	test: 0.860762

Epoch: 110
Loss: 0.2611916549503803
RMSE train: 0.455984	val: 1.128401	test: 1.099247
MAE train: 0.329024	val: 0.896847	test: 0.826376

Epoch: 111
Loss: 0.26998578757047653
RMSE train: 0.400377	val: 1.112891	test: 1.070134
MAE train: 0.286119	val: 0.883867	test: 0.791418

Epoch: 112
Loss: 0.23794548958539963
RMSE train: 0.400735	val: 1.037847	test: 1.045762
MAE train: 0.282098	val: 0.843858	test: 0.778514

Epoch: 113
Loss: 0.2268410138785839
RMSE train: 0.414828	val: 1.088766	test: 1.065247
MAE train: 0.303029	val: 0.879121	test: 0.787593

Epoch: 114
Loss: 0.26177018880844116
RMSE train: 0.373275	val: 1.065742	test: 1.073145
MAE train: 0.274975	val: 0.863191	test: 0.782090

Epoch: 115
Loss: 0.26646389439702034
RMSE train: 0.499015	val: 1.139837	test: 1.115970
MAE train: 0.360620	val: 0.921559	test: 0.840219

Epoch: 116
Loss: 0.23720403760671616
RMSE train: 0.477875	val: 1.040002	test: 1.102119
MAE train: 0.339713	val: 0.854969	test: 0.821738

Epoch: 117
Loss: 0.2754724770784378
RMSE train: 0.560356	val: 1.200267	test: 1.187253
MAE train: 0.396174	val: 0.955908	test: 0.910748

Epoch: 118
Loss: 0.275169525295496
RMSE train: 0.478560	val: 1.089246	test: 1.134975
MAE train: 0.344456	val: 0.878822	test: 0.841996

Epoch: 119
Loss: 0.27202608063817024
RMSE train: 0.452123	val: 1.065643	test: 1.097203
MAE train: 0.320528	val: 0.863649	test: 0.814387

Epoch: 120
Loss: 0.2540198490023613
RMSE train: 0.422185	val: 1.117992	test: 1.085177
MAE train: 0.300904	val: 0.892432	test: 0.819699

Epoch: 121
Loss: 0.24821360781788826
RMSE train: 0.363988	val: 0.998422	test: 1.069225
MAE train: 0.267176	val: 0.821143	test: 0.796933

Epoch: 122
Loss: 0.2389768585562706
RMSE train: 0.419837	val: 1.126585	test: 1.094056
MAE train: 0.299521	val: 0.897770	test: 0.826199

Epoch: 123
Loss: 0.26302994042634964
RMSE train: 0.367707	val: 0.996520	test: 1.098167
MAE train: 0.278239	val: 0.818051	test: 0.817502

Epoch: 124
Loss: 0.26277323439717293
RMSE train: 0.487488	val: 1.189434	test: 1.173109
MAE train: 0.355226	val: 0.956380	test: 0.900815

Epoch: 125
Loss: 0.25250231847167015
RMSE train: 0.373480	val: 1.057987	test: 1.087099
MAE train: 0.269694	val: 0.862003	test: 0.805875

Epoch: 126
Loss: 0.24984750524163246
RMSE train: 0.339086	val: 1.066676	test: 1.108165
MAE train: 0.251195	val: 0.876772	test: 0.809637

Epoch: 127
Loss: 0.24009445682168007
RMSE train: 0.514480	val: 1.183830	test: 1.158218
MAE train: 0.371056	val: 0.949367	test: 0.895906

Epoch: 128
Loss: 0.26253504678606987
RMSE train: 0.365458	val: 1.008974	test: 1.054151
MAE train: 0.264325	val: 0.824594	test: 0.768396

Epoch: 129
Loss: 0.25733282789587975
RMSE train: 0.404632	val: 1.107921	test: 1.097882
MAE train: 0.288761	val: 0.879051	test: 0.804182

Epoch: 130
Loss: 0.24171064794063568
RMSE train: 0.474927	val: 1.127738	test: 1.105107
MAE train: 0.334376	val: 0.901504	test: 0.844543

Epoch: 131
Loss: 0.26581157743930817
RMSE train: 0.407690	val: 0.963559	test: 1.077491
MAE train: 0.288390	val: 0.800488	test: 0.793551

Epoch: 132
Loss: 0.2402312308549881
RMSE train: 0.445413	val: 1.140342	test: 1.128794
MAE train: 0.325793	val: 0.913565	test: 0.866109

Epoch: 133
Loss: 0.27183353528380394
RMSE train: 0.389938	val: 1.041146	test: 1.082995
MAE train: 0.279718	val: 0.839803	test: 0.809799

Epoch: 134
Loss: 0.26016633212566376
RMSE train: 0.388705	val: 0.976832	test: 1.065136
MAE train: 0.279439	val: 0.802962	test: 0.791679

Epoch: 135
Loss: 0.29014116898179054
RMSE train: 0.403320	val: 1.153477	test: 1.118170
MAE train: 0.285896	val: 0.911778	test: 0.848495

Epoch: 136
Loss: 0.25100818648934364
RMSE train: 0.389408	val: 1.042844	test: 1.099779
MAE train: 0.276131	val: 0.850608	test: 0.829338

Epoch: 137
Loss: 0.24298450350761414
RMSE train: 0.401942	val: 1.099862	test: 1.146001
MAE train: 0.288456	val: 0.889357	test: 0.884055

Epoch: 138
Loss: 0.22719602659344673
RMSE train: 0.422505	val: 1.111666	test: 1.105700
MAE train: 0.303326	val: 0.894657	test: 0.840851

Epoch: 139
Loss: 0.25781167298555374
RMSE train: 0.394655	val: 1.050532	test: 1.086356
MAE train: 0.284222	val: 0.859459	test: 0.815721

Epoch: 140
Loss: 0.25065727159380913
RMSE train: 0.473341	val: 1.100271	test: 1.132145
MAE train: 0.339041	val: 0.897917	test: 0.860464

Epoch: 141
Loss: 0.23035290092229843
RMSE train: 0.404226	val: 1.123593	test: 1.102481
MAE train: 0.289831	val: 0.896429	test: 0.834658

Epoch: 142
Loss: 0.19364561513066292
RMSE train: 0.396459	val: 1.075849	test: 1.069996
MAE train: 0.277644	val: 0.861194	test: 0.804580

Epoch: 143
Loss: 0.23427743837237358
RMSE train: 0.426218	val: 1.100415	test: 1.083281
MAE train: 0.299463	val: 0.877990	test: 0.809933

Epoch: 144
Loss: 0.22377732023596764

Epoch: 84
Loss: 0.3117533326148987
RMSE train: 0.408640	val: 1.033971	test: 1.065376
MAE train: 0.300942	val: 0.841507	test: 0.799902

Epoch: 85
Loss: 0.27238548547029495
RMSE train: 0.465801	val: 1.115934	test: 1.114876
MAE train: 0.338424	val: 0.900668	test: 0.853453

Epoch: 86
Loss: 0.27671629190444946
RMSE train: 0.412247	val: 1.080040	test: 1.092912
MAE train: 0.300749	val: 0.873909	test: 0.817504

Epoch: 87
Loss: 0.300887867808342
RMSE train: 0.414637	val: 1.080910	test: 1.070820
MAE train: 0.300683	val: 0.883715	test: 0.814297

Epoch: 88
Loss: 0.28315597027540207
RMSE train: 0.446493	val: 1.096346	test: 1.088932
MAE train: 0.323629	val: 0.888802	test: 0.839791

Epoch: 89
Loss: 0.30080337822437286
RMSE train: 0.397400	val: 1.037660	test: 1.062441
MAE train: 0.287443	val: 0.836150	test: 0.800419

Epoch: 90
Loss: 0.29411185532808304
RMSE train: 0.456024	val: 1.155582	test: 1.121756
MAE train: 0.329405	val: 0.934640	test: 0.866446

Epoch: 91
Loss: 0.28866516053676605
RMSE train: 0.471719	val: 1.167405	test: 1.142936
MAE train: 0.340148	val: 0.950268	test: 0.887461

Epoch: 92
Loss: 0.2631353288888931
RMSE train: 0.403294	val: 1.082564	test: 1.074128
MAE train: 0.292042	val: 0.872553	test: 0.822116

Epoch: 93
Loss: 0.29589633643627167
RMSE train: 0.457213	val: 1.121405	test: 1.108282
MAE train: 0.325270	val: 0.900273	test: 0.858935

Epoch: 94
Loss: 0.2923055589199066
RMSE train: 0.413818	val: 1.056373	test: 1.118526
MAE train: 0.302339	val: 0.862253	test: 0.830961

Epoch: 95
Loss: 0.29460377246141434
RMSE train: 0.440199	val: 1.103841	test: 1.142799
MAE train: 0.323405	val: 0.909366	test: 0.869558

Epoch: 96
Loss: 0.2728344723582268
RMSE train: 0.483187	val: 1.104354	test: 1.154535
MAE train: 0.357324	val: 0.909639	test: 0.886193

Epoch: 97
Loss: 0.26892340183258057
RMSE train: 0.410898	val: 0.992259	test: 1.101920
MAE train: 0.301168	val: 0.814870	test: 0.826177

Epoch: 98
Loss: 0.3412560075521469
RMSE train: 0.442650	val: 1.094732	test: 1.132518
MAE train: 0.323985	val: 0.877918	test: 0.876097

Epoch: 99
Loss: 0.26689470559358597
RMSE train: 0.459234	val: 1.161793	test: 1.148730
MAE train: 0.336116	val: 0.924283	test: 0.898980

Epoch: 100
Loss: 0.3023627996444702
RMSE train: 0.390215	val: 1.049049	test: 1.096473
MAE train: 0.281640	val: 0.835793	test: 0.819927

Epoch: 101
Loss: 0.24941008538007736
RMSE train: 0.403867	val: 1.122539	test: 1.127330
MAE train: 0.296587	val: 0.896855	test: 0.856237

Epoch: 102
Loss: 0.30387087538838387
RMSE train: 0.377851	val: 1.092204	test: 1.111543
MAE train: 0.275684	val: 0.874196	test: 0.835620

Epoch: 103
Loss: 0.2776103541254997
RMSE train: 0.418935	val: 1.138433	test: 1.105296
MAE train: 0.310817	val: 0.912371	test: 0.839134

Epoch: 104
Loss: 0.2583119310438633
RMSE train: 0.484851	val: 1.173339	test: 1.145067
MAE train: 0.347500	val: 0.967943	test: 0.859149

Epoch: 105
Loss: 0.2764929160475731
RMSE train: 0.383378	val: 1.072672	test: 1.061528
MAE train: 0.275172	val: 0.861496	test: 0.801572

Epoch: 106
Loss: 0.2665250599384308
RMSE train: 0.416025	val: 1.007244	test: 1.061637
MAE train: 0.300152	val: 0.831719	test: 0.796878

Epoch: 107
Loss: 0.2843538783490658
RMSE train: 0.387417	val: 1.029481	test: 1.064789
MAE train: 0.287719	val: 0.849753	test: 0.782054

Epoch: 108
Loss: 0.30918312817811966
RMSE train: 0.374500	val: 1.089008	test: 1.076417
MAE train: 0.282461	val: 0.872865	test: 0.804731

Epoch: 109
Loss: 0.27820611745119095
RMSE train: 0.439816	val: 1.115436	test: 1.111691
MAE train: 0.320848	val: 0.911842	test: 0.841390

Epoch: 110
Loss: 0.2609441578388214
RMSE train: 0.512469	val: 1.135690	test: 1.163787
MAE train: 0.365039	val: 0.938813	test: 0.884011

Epoch: 111
Loss: 0.2956034727394581
RMSE train: 0.403900	val: 1.130793	test: 1.120104
MAE train: 0.288776	val: 0.901673	test: 0.847280

Epoch: 112
Loss: 0.2794996276497841
RMSE train: 0.391215	val: 1.128702	test: 1.117719
MAE train: 0.284734	val: 0.895097	test: 0.854127

Epoch: 113
Loss: 0.25900285691022873
RMSE train: 0.434106	val: 1.121726	test: 1.114978
MAE train: 0.308291	val: 0.895696	test: 0.850000

Epoch: 114
Loss: 0.23316209390759468
RMSE train: 0.434258	val: 1.086630	test: 1.084858
MAE train: 0.308611	val: 0.869145	test: 0.823101

Epoch: 115
Loss: 0.26354678720235825
RMSE train: 0.414142	val: 1.151088	test: 1.119927
MAE train: 0.301381	val: 0.905900	test: 0.850549

Epoch: 116
Loss: 0.24123656004667282
RMSE train: 0.412203	val: 1.099233	test: 1.124049
MAE train: 0.300775	val: 0.885104	test: 0.840731

Epoch: 117
Loss: 0.36394623667001724
RMSE train: 0.390068	val: 1.107830	test: 1.120968
MAE train: 0.287819	val: 0.874648	test: 0.839856

Epoch: 118
Loss: 0.2231665402650833
RMSE train: 0.454625	val: 1.084359	test: 1.149623
MAE train: 0.323556	val: 0.887305	test: 0.863980

Epoch: 119
Loss: 0.27325406670570374
RMSE train: 0.425342	val: 1.065430	test: 1.105877
MAE train: 0.302921	val: 0.864828	test: 0.839842

Epoch: 120
Loss: 0.24050098657608032
RMSE train: 0.358602	val: 1.028982	test: 1.086624
MAE train: 0.261054	val: 0.823644	test: 0.815749

Epoch: 121
Loss: 0.2914502024650574
RMSE train: 0.427962	val: 1.102740	test: 1.141030
MAE train: 0.309909	val: 0.895664	test: 0.871073

Epoch: 122
Loss: 0.24008532986044884
RMSE train: 0.362065	val: 0.982574	test: 1.083660
MAE train: 0.265055	val: 0.812671	test: 0.817668

Epoch: 123
Loss: 0.22938450425863266
RMSE train: 0.356253	val: 1.061073	test: 1.106284
MAE train: 0.259262	val: 0.856751	test: 0.834964

Epoch: 124
Loss: 0.22390112280845642
RMSE train: 0.381279	val: 1.085218	test: 1.155607
MAE train: 0.279183	val: 0.872750	test: 0.881141

Epoch: 125
Loss: 0.21864265203475952
RMSE train: 0.390118	val: 1.053944	test: 1.122898
MAE train: 0.281994	val: 0.847280	test: 0.853438

Epoch: 126
Loss: 0.265336737036705
RMSE train: 0.389235	val: 1.100737	test: 1.118531
MAE train: 0.279812	val: 0.872554	test: 0.858933

Epoch: 127
Loss: 0.24543913453817368
RMSE train: 0.404782	val: 1.092729	test: 1.124125
MAE train: 0.290425	val: 0.870728	test: 0.854996

Epoch: 128
Loss: 0.2553895451128483
RMSE train: 0.345193	val: 1.090819	test: 1.095764
MAE train: 0.251350	val: 0.858941	test: 0.831061

Epoch: 129
Loss: 0.26332662627100945
RMSE train: 0.345635	val: 1.051456	test: 1.095182
MAE train: 0.252082	val: 0.837158	test: 0.819826

Epoch: 130
Loss: 0.2706739194691181
RMSE train: 0.413172	val: 1.077654	test: 1.114051
MAE train: 0.297518	val: 0.882726	test: 0.835344

Epoch: 131
Loss: 0.2503749057650566
RMSE train: 0.342365	val: 1.098008	test: 1.127818
MAE train: 0.261202	val: 0.873829	test: 0.852262

Epoch: 132
Loss: 0.2625623643398285
RMSE train: 0.357636	val: 1.108069	test: 1.112039
MAE train: 0.260809	val: 0.875481	test: 0.827083

Epoch: 133
Loss: 0.23074055463075638
RMSE train: 0.473855	val: 1.165709	test: 1.141830
MAE train: 0.339350	val: 0.934594	test: 0.873784

Epoch: 134
Loss: 0.2645837627351284
RMSE train: 0.361174	val: 1.005453	test: 1.077189
MAE train: 0.267016	val: 0.806945	test: 0.810529

Epoch: 135
Loss: 0.22994079813361168
RMSE train: 0.357526	val: 1.086903	test: 1.103216
MAE train: 0.262395	val: 0.862734	test: 0.827884

Epoch: 136
Loss: 0.23110265284776688
RMSE train: 0.389182	val: 1.143331	test: 1.152546
MAE train: 0.284120	val: 0.905552	test: 0.878831

Epoch: 137
Loss: 0.2366427406668663
RMSE train: 0.375147	val: 1.040349	test: 1.125520
MAE train: 0.276959	val: 0.846680	test: 0.840644

Epoch: 138
Loss: 0.2574644424021244
RMSE train: 0.423984	val: 1.159632	test: 1.158590
MAE train: 0.308848	val: 0.922323	test: 0.883213

Epoch: 139
Loss: 0.23181740567088127
RMSE train: 0.379296	val: 1.019936	test: 1.090916
MAE train: 0.274769	val: 0.835477	test: 0.811000

Epoch: 140
Loss: 0.2316090129315853
RMSE train: 0.405189	val: 1.051290	test: 1.105435
MAE train: 0.289551	val: 0.856732	test: 0.831777

Epoch: 141
Loss: 0.23035933077335358
RMSE train: 0.393561	val: 1.074678	test: 1.108467
MAE train: 0.280809	val: 0.865331	test: 0.834306

Epoch: 142
Loss: 0.24153634160757065
RMSE train: 0.437492	val: 1.117815	test: 1.135679
MAE train: 0.315216	val: 0.904601	test: 0.857053

Epoch: 143
Loss: 0.2189367488026619
RMSE train: 0.410324	val: 1.124972	test: 1.104132
MAE train: 0.298621	val: 0.897565	test: 0.835288

Epoch: 144
Loss: 0.26424382999539375
RMSE train: 0.566085	val: 1.100787	test: 1.188370
MAE train: 0.413782	val: 0.877463	test: 0.928107

Epoch: 145
Loss: 0.38949761539697647
RMSE train: 0.524015	val: 1.094056	test: 1.176955
MAE train: 0.376273	val: 0.855687	test: 0.917056

Epoch: 146
Loss: 0.3723416402935982
RMSE train: 0.592009	val: 1.185066	test: 1.262625
MAE train: 0.429243	val: 0.935768	test: 0.996287

Epoch: 147
Loss: 0.3782934695482254
RMSE train: 0.529028	val: 1.097922	test: 1.159668
MAE train: 0.376820	val: 0.847124	test: 0.908562

Epoch: 148
Loss: 0.2979206144809723
RMSE train: 0.434824	val: 0.984876	test: 1.031548
MAE train: 0.310561	val: 0.753493	test: 0.823656

Epoch: 149
Loss: 0.47652600705623627
RMSE train: 0.417229	val: 1.015722	test: 1.064933
MAE train: 0.300135	val: 0.770177	test: 0.836496

Epoch: 150
Loss: 0.5360790640115738
RMSE train: 0.550406	val: 1.162291	test: 1.192175
MAE train: 0.409919	val: 0.906247	test: 0.937023

Epoch: 151
Loss: 0.33182621747255325
RMSE train: 0.621986	val: 1.122444	test: 1.156284
MAE train: 0.450518	val: 0.885590	test: 0.909428

Epoch: 152
Loss: 0.6121948733925819
RMSE train: 0.437670	val: 1.021810	test: 1.096822
MAE train: 0.313426	val: 0.776569	test: 0.854335

Epoch: 153
Loss: 0.48775266110897064
RMSE train: 0.477661	val: 1.069492	test: 1.189454
MAE train: 0.363576	val: 0.820055	test: 0.937032

Epoch: 154
Loss: 0.3134354203939438
RMSE train: 0.445542	val: 1.018347	test: 1.146983
MAE train: 0.336577	val: 0.786700	test: 0.911993

Epoch: 155
Loss: 0.37285116314888
RMSE train: 0.428616	val: 1.037957	test: 1.103187
MAE train: 0.318110	val: 0.796977	test: 0.879158

Epoch: 156
Loss: 0.49271364510059357
RMSE train: 0.498527	val: 1.049434	test: 1.108155
MAE train: 0.357965	val: 0.829021	test: 0.875546

Epoch: 157
Loss: 0.35220514982938766
RMSE train: 0.449451	val: 1.008917	test: 1.058195
MAE train: 0.313917	val: 0.787866	test: 0.841640

Epoch: 158
Loss: 0.6029508784413338
RMSE train: 0.398428	val: 1.015874	test: 1.059713
MAE train: 0.294600	val: 0.771092	test: 0.848930

Epoch: 159
Loss: 0.3610652834177017
RMSE train: 0.430048	val: 1.094831	test: 1.164736
MAE train: 0.321964	val: 0.823726	test: 0.936417

Epoch: 160
Loss: 0.41417231410741806
RMSE train: 0.415221	val: 1.072777	test: 1.156022
MAE train: 0.307775	val: 0.807218	test: 0.913184

Epoch: 161
Loss: 0.41557513177394867
RMSE train: 0.430256	val: 1.062703	test: 1.151495
MAE train: 0.314670	val: 0.822331	test: 0.899770

Epoch: 162
Loss: 0.47394224256277084
RMSE train: 0.476764	val: 1.107497	test: 1.193396
MAE train: 0.337687	val: 0.863412	test: 0.929400

Epoch: 163
Loss: 0.3949962481856346
RMSE train: 0.632220	val: 1.214308	test: 1.292262
MAE train: 0.461056	val: 0.963167	test: 1.015627

Epoch: 164
Loss: 0.32607748359441757
RMSE train: 0.492029	val: 1.091502	test: 1.149749
MAE train: 0.365059	val: 0.848302	test: 0.893891

Epoch: 165
Loss: 0.3109675347805023
RMSE train: 0.393278	val: 0.978708	test: 1.057138
MAE train: 0.290614	val: 0.743904	test: 0.824653

Epoch: 166
Loss: 0.4113804064691067
RMSE train: 0.413059	val: 0.967862	test: 1.052849
MAE train: 0.303993	val: 0.742318	test: 0.822526

Epoch: 167
Loss: 0.28779027611017227
RMSE train: 0.433062	val: 1.072463	test: 1.133591
MAE train: 0.315460	val: 0.832345	test: 0.886644

Epoch: 168
Loss: 0.2316578347235918
RMSE train: 0.519119	val: 1.191794	test: 1.257231
MAE train: 0.377916	val: 0.936442	test: 0.995718

Epoch: 169
Loss: 0.3464396595954895
RMSE train: 0.466142	val: 1.088228	test: 1.148534
MAE train: 0.332578	val: 0.851923	test: 0.903315

Epoch: 170
Loss: 0.24883145093917847
RMSE train: 0.412500	val: 0.992010	test: 1.041741
MAE train: 0.292844	val: 0.768700	test: 0.817575

Epoch: 171
Loss: 0.3436863571405411
RMSE train: 0.417259	val: 0.992705	test: 1.058690
MAE train: 0.298938	val: 0.766465	test: 0.825798

Early stopping
Best (RMSE):	 train: 0.391031	val: 0.961317	test: 1.051550
Best (MAE):	 train: 0.289406	val: 0.729364	test: 0.829136
All runs completed.

RMSE train: 0.384706	val: 1.036543	test: 1.090478
MAE train: 0.274680	val: 0.834625	test: 0.817612

Epoch: 145
Loss: 0.24991775304079056
RMSE train: 0.455063	val: 1.128742	test: 1.146727
MAE train: 0.322092	val: 0.907615	test: 0.873088

Epoch: 146
Loss: 0.24785588681697845
RMSE train: 0.366565	val: 1.048623	test: 1.087020
MAE train: 0.263762	val: 0.852256	test: 0.816790

Epoch: 147
Loss: 0.23996519669890404
RMSE train: 0.395070	val: 1.128978	test: 1.129371
MAE train: 0.283518	val: 0.904716	test: 0.862857

Epoch: 148
Loss: 0.24686073511838913
RMSE train: 0.357361	val: 1.107184	test: 1.139608
MAE train: 0.265637	val: 0.884226	test: 0.862860

Epoch: 149
Loss: 0.23089443519711494
RMSE train: 0.361817	val: 1.099673	test: 1.116775
MAE train: 0.266329	val: 0.880834	test: 0.843141

Epoch: 150
Loss: 0.21694231033325195
RMSE train: 0.480496	val: 1.148378	test: 1.177281
MAE train: 0.348305	val: 0.940612	test: 0.889524

Epoch: 151
Loss: 0.24557027220726013
RMSE train: 0.383346	val: 1.000099	test: 1.113331
MAE train: 0.277393	val: 0.826141	test: 0.825398

Epoch: 152
Loss: 0.2539013959467411
RMSE train: 0.445200	val: 1.109600	test: 1.149409
MAE train: 0.318886	val: 0.900545	test: 0.869760

Epoch: 153
Loss: 0.27898014709353447
RMSE train: 0.319929	val: 1.030777	test: 1.098606
MAE train: 0.235041	val: 0.816969	test: 0.826600

Epoch: 154
Loss: 0.22064250707626343
RMSE train: 0.320188	val: 1.052111	test: 1.115776
MAE train: 0.242014	val: 0.822909	test: 0.846406

Epoch: 155
Loss: 0.21286707371473312
RMSE train: 0.357004	val: 1.010917	test: 1.090497
MAE train: 0.261907	val: 0.823256	test: 0.814898

Epoch: 156
Loss: 0.20903977006673813
RMSE train: 0.347976	val: 1.059147	test: 1.110011
MAE train: 0.252830	val: 0.851330	test: 0.832412

Epoch: 157
Loss: 0.21704909950494766
RMSE train: 0.335905	val: 1.116502	test: 1.164661
MAE train: 0.243489	val: 0.890960	test: 0.875515

Early stopping
Best (RMSE):	 train: 0.362065	val: 0.982574	test: 1.083660
Best (MAE):	 train: 0.265055	val: 0.812671	test: 0.817668

RMSE train: 0.392764	val: 1.113089	test: 1.087337
MAE train: 0.284523	val: 0.886459	test: 0.805714

Epoch: 145
Loss: 0.2636893056333065
RMSE train: 0.371641	val: 1.162455	test: 1.106670
MAE train: 0.274135	val: 0.915260	test: 0.833133

Epoch: 146
Loss: 0.20757895335555077
RMSE train: 0.378253	val: 1.118242	test: 1.098839
MAE train: 0.274781	val: 0.886864	test: 0.829129

Epoch: 147
Loss: 0.21942467987537384
RMSE train: 0.470845	val: 1.220358	test: 1.145674
MAE train: 0.338013	val: 0.966528	test: 0.893856

Epoch: 148
Loss: 0.21613415703177452
RMSE train: 0.424929	val: 1.058862	test: 1.082186
MAE train: 0.301468	val: 0.881297	test: 0.810739

Epoch: 149
Loss: 0.21176951378583908
RMSE train: 0.428910	val: 1.094230	test: 1.105342
MAE train: 0.310763	val: 0.907921	test: 0.838973

Epoch: 150
Loss: 0.24007460102438927
RMSE train: 0.457363	val: 1.162830	test: 1.156851
MAE train: 0.333138	val: 0.945928	test: 0.899292

Epoch: 151
Loss: 0.21970390528440475
RMSE train: 0.392030	val: 1.033947	test: 1.085765
MAE train: 0.280549	val: 0.848705	test: 0.815586

Epoch: 152
Loss: 0.22244232147932053
RMSE train: 0.399124	val: 1.042901	test: 1.068269
MAE train: 0.282225	val: 0.851688	test: 0.798513

Epoch: 153
Loss: 0.2186100296676159
RMSE train: 0.431957	val: 1.121244	test: 1.084275
MAE train: 0.311368	val: 0.902095	test: 0.812554

Epoch: 154
Loss: 0.22424090653657913
RMSE train: 0.384718	val: 1.045106	test: 1.088749
MAE train: 0.275915	val: 0.854795	test: 0.814098

Epoch: 155
Loss: 0.2126832902431488
RMSE train: 0.392781	val: 1.117583	test: 1.110073
MAE train: 0.281906	val: 0.894097	test: 0.833303

Epoch: 156
Loss: 0.21784774959087372
RMSE train: 0.413871	val: 1.131096	test: 1.110345
MAE train: 0.293576	val: 0.901986	test: 0.839166

Epoch: 157
Loss: 0.2221771404147148
RMSE train: 0.372738	val: 1.081178	test: 1.086937
MAE train: 0.260479	val: 0.855372	test: 0.817245

Epoch: 158
Loss: 0.26750217005610466
RMSE train: 0.366769	val: 1.045433	test: 1.109756
MAE train: 0.265009	val: 0.846930	test: 0.832364

Epoch: 159
Loss: 0.2324015013873577
RMSE train: 0.348462	val: 1.113103	test: 1.118848
MAE train: 0.253541	val: 0.897405	test: 0.836853

Epoch: 160
Loss: 0.2309647686779499
RMSE train: 0.385813	val: 1.142019	test: 1.140771
MAE train: 0.274167	val: 0.926861	test: 0.860103

Epoch: 161
Loss: 0.21469788625836372
RMSE train: 0.340501	val: 1.033738	test: 1.091195
MAE train: 0.246026	val: 0.857839	test: 0.820855

Epoch: 162
Loss: 0.18604812026023865
RMSE train: 0.342123	val: 1.155826	test: 1.135190
MAE train: 0.242877	val: 0.926355	test: 0.843837

Epoch: 163
Loss: 0.20377151295542717
RMSE train: 0.368609	val: 1.090266	test: 1.082404
MAE train: 0.258687	val: 0.896602	test: 0.796767

Epoch: 164
Loss: 0.2295674942433834
RMSE train: 0.380235	val: 1.043493	test: 1.081562
MAE train: 0.270519	val: 0.862921	test: 0.795602

Epoch: 165
Loss: 0.19466182962059975
RMSE train: 0.337102	val: 1.155467	test: 1.149842
MAE train: 0.249651	val: 0.923550	test: 0.854708

Epoch: 166
Loss: 0.2206270433962345
RMSE train: 0.352758	val: 1.072670	test: 1.110650
MAE train: 0.258186	val: 0.878037	test: 0.826367

Early stopping
Best (RMSE):	 train: 0.407690	val: 0.963559	test: 1.077491
Best (MAE):	 train: 0.288390	val: 0.800488	test: 0.793551
All runs completed.
